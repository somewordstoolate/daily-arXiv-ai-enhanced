{"id": "2505.18159", "pdf": "https://arxiv.org/pdf/2505.18159", "abs": "https://arxiv.org/abs/2505.18159", "authors": ["Jesus Alvarez C", "Daua D. Karajeanes", "Ashley Celeste Prado", "John Ruttan", "Ivory Yang", "Sean O'Brien", "Vasu Sharma", "Kevin Zhu"], "title": "Advancing Uto-Aztecan Language Technologies: A Case Study on the Endangered Comanche Language", "categories": ["cs.CL", "cs.LG", "I.2.7; H.3.1"], "comment": "11 pages, 13 figures; published in Proceedings of the Fifth Workshop\n  on NLP for Indigenous Languages of the Americas (AmericasNLP 2025) at NAACL\n  2025, Albuquerque, NM", "summary": "The digital exclusion of endangered languages remains a critical challenge in\nNLP, limiting both linguistic research and revitalization efforts. This study\nintroduces the first computational investigation of Comanche, an Uto-Aztecan\nlanguage on the verge of extinction, demonstrating how minimal-cost,\ncommunity-informed NLP interventions can support language preservation. We\npresent a manually curated dataset of 412 phrases, a synthetic data generation\npipeline, and an empirical evaluation of GPT-4o and GPT-4o-mini for language\nidentification. Our experiments reveal that while LLMs struggle with Comanche\nin zero-shot settings, few-shot prompting significantly improves performance,\nachieving near-perfect accuracy with just five examples. Our findings highlight\nthe potential of targeted NLP methodologies in low-resource contexts and\nemphasize that visibility is the first step toward inclusion. By establishing a\nfoundation for Comanche in NLP, we advocate for computational approaches that\nprioritize accessibility, cultural sensitivity, and community engagement."}
{"id": "2505.18215", "pdf": "https://arxiv.org/pdf/2505.18215", "abs": "https://arxiv.org/abs/2505.18215", "authors": ["Junyan Zhang", "Yiming Huang", "Shuliang Liu", "Yubo Gao", "Xuming Hu"], "title": "Do BERT-Like Bidirectional Models Still Perform Better on Text Classification in the Era of LLMs?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid adoption of LLMs has overshadowed the potential advantages of\ntraditional BERT-like models in text classification. This study challenges the\nprevailing \"LLM-centric\" trend by systematically comparing three category\nmethods, i.e., BERT-like models fine-tuning, LLM internal state utilization,\nand zero-shot inference across six high-difficulty datasets. Our findings\nreveal that BERT-like models often outperform LLMs. We further categorize\ndatasets into three types, perform PCA and probing experiments, and identify\ntask-specific model strengths: BERT-like models excel in pattern-driven tasks,\nwhile LLMs dominate those requiring deep semantics or world knowledge. Based on\nthis, we propose TaMAS, a fine-grained task selection strategy, advocating for\na nuanced, task-driven approach over a one-size-fits-all reliance on LLMs."}
{"id": "2505.18218", "pdf": "https://arxiv.org/pdf/2505.18218", "abs": "https://arxiv.org/abs/2505.18218", "authors": ["Shuhang Xu", "Fangwei Zhong"], "title": "CoMet: Metaphor-Driven Covert Communication for Multi-Agent Language Games", "categories": ["cs.CL", "cs.AI"], "comment": "To Appear at ACL 2025 (Main)", "summary": "Metaphors are a crucial way for humans to express complex or subtle ideas by\ncomparing one concept to another, often from a different domain. However, many\nlarge language models (LLMs) struggle to interpret and apply metaphors in\nmulti-agent language games, hindering their ability to engage in covert\ncommunication and semantic evasion, which are crucial for strategic\ncommunication. To address this challenge, we introduce CoMet, a framework that\nenables LLM-based agents to engage in metaphor processing. CoMet combines a\nhypothesis-based metaphor reasoner with a metaphor generator that improves\nthrough self-reflection and knowledge integration. This enhances the agents'\nability to interpret and apply metaphors, improving the strategic and nuanced\nquality of their interactions. We evaluate CoMet on two multi-agent language\ngames - Undercover and Adversarial Taboo - which emphasize Covert Communication\nand Semantic Evasion. Experimental results demonstrate that CoMet significantly\nenhances the agents' ability to communicate strategically using metaphors."}
{"id": "2505.18223", "pdf": "https://arxiv.org/pdf/2505.18223", "abs": "https://arxiv.org/abs/2505.18223", "authors": ["Hanyu Li", "Haoyu Liu", "Tingyu Zhu", "Tianyu Guo", "Zeyu Zheng", "Xiaotie Deng", "Michael I. Jordan"], "title": "IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) show promise as data analysis agents, but\nexisting benchmarks overlook the iterative nature of the field, where experts'\ndecisions evolve with deeper insights of the dataset. To address this, we\nintroduce IDA-Bench, a novel benchmark evaluating LLM agents in multi-round\ninteractive scenarios. Derived from complex Kaggle notebooks, tasks are\npresented as sequential natural language instructions by an LLM-simulated user.\nAgent performance is judged by comparing its final numerical output to the\nhuman-derived baseline. Initial results show that even state-of-the-art coding\nagents (like Claude-3.7-thinking) succeed on < 50% of the tasks, highlighting\nlimitations not evident in single-turn tests. This work underscores the need to\nimprove LLMs' multi-round capabilities for building more reliable data analysis\nagents, highlighting the necessity of achieving a balance between instruction\nfollowing and reasoning."}
{"id": "2505.18277", "pdf": "https://arxiv.org/pdf/2505.18277", "abs": "https://arxiv.org/abs/2505.18277", "authors": ["Joshua S. Rule", "Steven T. Piantadosi"], "title": "The end of radical concept nativism", "categories": ["cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "Though humans seem to be remarkable learners, arguments in cognitive science\nand philosophy of mind have long maintained that learning something\nfundamentally new is impossible. Specifically, Jerry Fodor's arguments for\nradical concept nativism hold that most, if not all, concepts are innate and\nthat what many call concept learning never actually leads to the acquisition of\nnew concepts. These arguments have deeply affected cognitive science, and many\nbelieve that the counterarguments to radical concept nativism have been either\nunsuccessful or only apply to a narrow class of concepts. This paper first\nreviews the features and limitations of prior arguments. We then identify three\ncritical points - related to issues of expressive power, conceptual structure,\nand concept possession - at which the arguments in favor of radical concept\nnativism diverge from describing actual human cognition. We use ideas from\ncomputer science and information theory to formalize the relevant ideas in ways\nthat are arguably more scientifically productive. We conclude that, as a\nresult, there is an important sense in which people do indeed learn new\nconcepts."}
{"id": "2505.18164", "pdf": "https://arxiv.org/pdf/2505.18164", "abs": "https://arxiv.org/abs/2505.18164", "authors": ["Davide Macario", "Hulya Seferoglu", "Erdem Koyuncu"], "title": "Model-Distributed Inference for Large Language Models at the Edge", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce Model-Distributed Inference for Large-Language Models (MDI-LLM),\na novel framework designed to facilitate the deployment of state-of-the-art\nlarge-language models (LLMs) across low-power devices at the edge. This is\naccomplished by dividing the model into multiple partitions, which are then\nassigned to different devices/nodes within the network. These nodes exchange\nintermediate activation vectors via device-to-device links, enabling\ncollaborative computation. To enhance the efficiency of this process, we\npropose the \"recurrent pipeline parallelism\" technique, which reduces idle time\non each device and facilitates parallel inference during the generation of\nmultiple text sequences. By leveraging the combined computational resources of\nmultiple edge devices, MDI-LLM enables the deployment of LLMs that exceed the\nmemory capacity of individual devices, making it possible to perform inference\non low-cost hardware. Furthermore, as the number of participating devices\nincreases, MDI-LLM boosts token generation throughput and reduces memory\nconsumption per device."}
{"id": "2505.18237", "pdf": "https://arxiv.org/pdf/2505.18237", "abs": "https://arxiv.org/abs/2505.18237", "authors": ["Xixian Yong", "Xiao Zhou", "Yingying Zhang", "Jinlin Li", "Yefeng Zheng", "Xian Wu"], "title": "Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "The recent rise of Large Reasoning Models (LRMs) has significantly improved\nmulti-step reasoning performance, but often at the cost of generating\nexcessively long reasoning chains. This paper revisits the efficiency of such\nreasoning processes through an information-theoretic lens, revealing a\nfundamental trade-off between reasoning length and semantic efficiency. We\npropose two metrics, InfoBias and InfoGain, to quantify divergence from ideal\nreasoning paths and stepwise information contribution, respectively. Empirical\nanalyses show that longer reasoning chains tend to exhibit higher information\nbias and diminishing information gain, especially for incorrect answers.\nMotivated by these findings, we introduce an entropy-based Adaptive Think\nstrategy that dynamically halts reasoning once confidence is sufficiently high,\nimproving efficiency while maintaining competitive accuracy. Compared to the\nVanilla Think approach (default mode), our strategy yields a 1.10% improvement\nin average accuracy and a 50.80% reduction in token usage on QwQ-32B across six\nbenchmark tasks spanning diverse reasoning types and difficulty levels,\ndemonstrating superior efficiency and reasoning performance. These results\nunderscore the promise of entropy-based methods for enhancing both accuracy and\ncost-effiiciency in large language model deployment."}
{"id": "2505.18325", "pdf": "https://arxiv.org/pdf/2505.18325", "abs": "https://arxiv.org/abs/2505.18325", "authors": ["Licheng Pan", "Yongqi Tong", "Xin Zhang", "Xiaolu Zhang", "Jun Zhou", "Zhixuan Chu"], "title": "Understanding and Mitigating Overrefusal in LLMs from an Unveiling Perspective of Safety Decision Boundary", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet they often refuse to answer legitimate queries-a\nphenomenon known as overrefusal. Overrefusal typically stems from\nover-conservative safety alignment, causing models to treat many reasonable\nprompts as potentially risky. To systematically understand this issue, we probe\nand leverage the models'safety decision boundaries to analyze and mitigate\noverrefusal. Our findings reveal that overrefusal is closely tied to\nmisalignment at these boundary regions, where models struggle to distinguish\nsubtle differences between benign and harmful content. Building on these\ninsights, we present RASS, an automated framework for prompt generation and\nselection that strategically targets overrefusal prompts near the safety\nboundary. By harnessing steering vectors in the representation space, RASS\nefficiently identifies and curates boundary-aligned prompts, enabling more\neffective and targeted mitigation of overrefusal. This approach not only\nprovides a more precise and interpretable view of model safety decisions but\nalso seamlessly extends to multilingual scenarios.We have explored the safety\ndecision boundaries of various LLMs and construct the MORBench evaluation set\nto facilitate robust assessment of model safety and helpfulness across multiple\nlanguages. Code and datasets will be released at\nhttps://anonymous.4open.science/r/RASS-80D3."}
{"id": "2505.18166", "pdf": "https://arxiv.org/pdf/2505.18166", "abs": "https://arxiv.org/abs/2505.18166", "authors": ["Jacob Sander", "David Moe", "Achraf Cohen", "Brent Venable", "Venkat Dasari", "Brian Jalaian"], "title": "Constrained Edge AI Deployment: Fine-Tuning vs Distillation for LLM Compression", "categories": ["cs.LG"], "comment": "9 Pages, 2 Figures", "summary": "Modern foundational models are often compressed via a combination of\nstructured pruning and re-training to meet the strict compute, memory, and\nconnectivity constraints of edge deployments. While state-of-the-art pruning\nschemes target the entire Transformer, we adopt a simple, layer-wise L2-norm\npruning on only the MLP blocks as a fixed baseline. Our focus is not on\nachieving maximal compression, but on isolating the impact of the re-training\nloss function: (i) Fine-tuning with Cross- Entropy (L2PFT), which requires\nlabeled data, versus (ii) Self-Distillation with KL-divergence, which leverages\nonly teacher logits (no labels) (L2PSD). We evaluate both pipelines on the\nOLMo2- 7B-SFT model for CommonsenseQA suitable for intermittent or denied\nconnectivity scenarios typical of edge networks. Under identical pruning\nschedules, KL-based distillation matches or exceeds CE fine-tuning in test\naccuracy, demonstrating that, even with a basic MLP-only pruning, the choice of\nloss function materially affects compressed model recovery in\nresource-constrained environments."}
{"id": "2505.18240", "pdf": "https://arxiv.org/pdf/2505.18240", "abs": "https://arxiv.org/abs/2505.18240", "authors": ["Ananth Muppidi", "Tarak Das", "Sambaran Bandyopadhyay", "Tripti Shukla", "Dharun D A"], "title": "Taming LLMs with Negative Samples: A Reference-Free Framework to Evaluate Presentation Content with Actionable Feedback", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The generation of presentation slides automatically is an important problem\nin the era of generative AI. This paper focuses on evaluating multimodal\ncontent in presentation slides that can effectively summarize a document and\nconvey concepts to a broad audience. We introduce a benchmark dataset,\nRefSlides, consisting of human-made high-quality presentations that span\nvarious topics. Next, we propose a set of metrics to characterize different\nintrinsic properties of the content of a presentation and present REFLEX, an\nevaluation approach that generates scores and actionable feedback for these\nmetrics. We achieve this by generating negative presentation samples with\ndifferent degrees of metric-specific perturbations and use them to fine-tune\nLLMs. This reference-free evaluation technique does not require ground truth\npresentations during inference. Our extensive automated and human experiments\ndemonstrate that our evaluation approach outperforms classical heuristic-based\nand state-of-the-art large language model-based evaluations in generating\nscores and explanations."}
{"id": "2505.18380", "pdf": "https://arxiv.org/pdf/2505.18380", "abs": "https://arxiv.org/abs/2505.18380", "authors": ["Praphul Singh", "Charlotte Dzialo", "Jangwon Kim", "Sumana Srivatsa", "Irfan Bulu", "Sri Gadde", "Krishnaram Kenthapadi"], "title": "RedactOR: An LLM-Powered Framework for Automatic Clinical Data De-Identification", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Industry Track. To appear", "summary": "Ensuring clinical data privacy while preserving utility is critical for\nAI-driven healthcare and data analytics. Existing de-identification (De-ID)\nmethods, including rule-based techniques, deep learning models, and large\nlanguage models (LLMs), often suffer from recall errors, limited\ngeneralization, and inefficiencies, limiting their real-world applicability. We\npropose a fully automated, multi-modal framework, RedactOR for de-identifying\nstructured and unstructured electronic health records, including clinical audio\nrecords. Our framework employs cost-efficient De-ID strategies, including\nintelligent routing, hybrid rule and LLM based approaches, and a two-step audio\nredaction approach. We present a retrieval-based entity relexicalization\napproach to ensure consistent substitutions of protected entities, thereby\nenhancing data coherence for downstream applications. We discuss key design\ndesiderata, de-identification and relexicalization methodology, and modular\narchitecture of RedactX and its integration with the Oracle Health Clinical AI\nsystem. Evaluated on the i2b2 2014 De-ID dataset using standard metrics with\nstrict recall, our approach achieves competitive performance while optimizing\ntoken usage to reduce LLM costs. Finally, we discuss key lessons and insights\nfrom deployment in real-world AI- driven healthcare data pipelines."}
{"id": "2505.18168", "pdf": "https://arxiv.org/pdf/2505.18168", "abs": "https://arxiv.org/abs/2505.18168", "authors": ["Feifan Wang", "Tengfei Song", "Minggui He", "Chang Su", "Zhanglin Wu", "Hao Yang", "Wenming Zheng", "Osamu Yoshie"], "title": "Emotion Knowledge Enhancement for Vision Large Language Models: A Self-Verification Approach for High-Quality Emotion Instruction Data Generation", "categories": ["cs.LG", "cs.GR"], "comment": null, "summary": "Facial emotion perception in the vision large language model (VLLM) is\ncrucial for achieving natural human-machine interaction. However, creating\nhigh-quality annotations for both coarse- and fine-grained facial emotion\nanalysis demands costly expertise. The lack of such high-quality instruction\ndata limits the performance of VLLMs in facial emotion perception. To address\nthis, we propose a self-verification approach with emotion knowledge\nenhancement (SEKE), which generates high-quality instruction data for\nmulti-grained emotion analysis cost-effectively using closed-source VLLM. This\napproach integrates prior human knowledge to VLLM inference, guided by the\ninherent correlations between three grained levels of emotion descriptions,\ni.e., discrete expression, valence-arousal, and action unit, to reliably\ngenerate comprehensive annotations. A self-verification strategy with\nUncertainty-Aware Monte Carlo sampling (SV-UAMC) is further embedded to\nefficiently extract more accurate VLLM predictions, further improving\nannotation reliability. Consequently, we construct a facial emotion instruction\ndataset (FEID) containing three comprehensive descriptions, which provides\ncoarse- and fine-grained emotional information for effective model training.\nAdditionally, we introduce a facial emotion analysis benchmark (FEAB) to\nmeasure the VLLM's corresponding ability. Our method significantly outperforms\nstate-of-the-art methods on three downstream facial emotion analysis tasks."}
{"id": "2505.18244", "pdf": "https://arxiv.org/pdf/2505.18244", "abs": "https://arxiv.org/abs/2505.18244", "authors": ["Yukin Zhang", "Qi Dong"], "title": "Multi-Scale Probabilistic Generation Theory: A Hierarchical Framework for Interpreting Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Transformer based language models achieve remarkable performance but\nremain opaque in how they plan, structure, and realize text. We introduce\nMulti_Scale Probabilistic Generation Theory (MSPGT), a hierarchical framework\nthat factorizes generation into three semantic scales_global context,\nintermediate structure, and local word choices and aligns each scale with\nspecific layer ranges in Transformer architectures. To identify scale\nboundaries, we propose two complementary metrics: attention span thresholds and\ninter layer mutual information peaks. Across four representative models (GPT-2,\nBERT, RoBERTa, and T5), these metrics yield stable local/intermediate/global\npartitions, corroborated by probing tasks and causal interventions. We find\nthat decoder_only models allocate more layers to intermediate and global\nprocessing while encoder_only models emphasize local feature extraction.\nThrough targeted interventions, we demonstrate that local scale manipulations\nprimarily influence lexical diversity, intermediate-scale modifications affect\nsentence structure and length, and global_scale perturbations impact discourse\ncoherence all with statistically significant effects. MSPGT thus offers a\nunified, architecture-agnostic method for interpreting, diagnosing, and\ncontrolling large language models, bridging the gap between mechanistic\ninterpretability and emergent capabilities."}
{"id": "2505.18425", "pdf": "https://arxiv.org/pdf/2505.18425", "abs": "https://arxiv.org/abs/2505.18425", "authors": ["Menghua Wu", "Yujia Bao"], "title": "Advertising in AI systems: Society must be vigilant", "categories": ["cs.AI"], "comment": null, "summary": "AI systems have increasingly become our gateways to the Internet. We argue\nthat just as advertising has driven the monetization of web search and social\nmedia, so too will commercial incentives shape the content served by AI. Unlike\ntraditional media, however, the outputs of these systems are dynamic,\npersonalized, and lack clear provenance -- raising concerns for transparency\nand regulation. In this paper, we envision how commercial content could be\ndelivered through generative AI-based systems. Based on the requirements of key\nstakeholders -- advertisers, consumers, and platforms -- we propose design\nprinciples for commercially-influenced AI systems. We then outline high-level\nstrategies for end users to identify and mitigate commercial biases from model\noutputs. Finally, we conclude with open questions and a call to action towards\nthese goals."}
{"id": "2505.18169", "pdf": "https://arxiv.org/pdf/2505.18169", "abs": "https://arxiv.org/abs/2505.18169", "authors": ["Nischal Mandal"], "title": "Interpretable Multi-Task PINN for Emotion Recognition and EDA Prediction", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Understanding and predicting human emotional and physiological states using\nwearable sensors has important applications in stress monitoring, mental health\nassessment, and affective computing. This study presents a novel Multi-Task\nPhysics-Informed Neural Network (PINN) that performs Electrodermal Activity\n(EDA) prediction and emotion classification simultaneously, using the publicly\navailable WESAD dataset. The model integrates psychological self-report\nfeatures (PANAS and SAM) with a physics-inspired differential equation\nrepresenting EDA dynamics, enforcing biophysically grounded constraints through\na custom loss function. This loss combines EDA regression, emotion\nclassification, and a physics residual term for improved interpretability.\n  The architecture supports dual outputs for both tasks and is trained under a\nunified multi-task framework. Evaluated using 5-fold cross-validation, the\nmodel achieves an average EDA RMSE of 0.0362, Pearson correlation of 0.9919,\nand F1-score of 94.08 percent. These results outperform classical models such\nas SVR and XGBoost, as well as ablated variants like emotion-only and EDA-only\nmodels.\n  In addition, the learned physical parameters including decay rate (alpha_0),\nemotional sensitivity (beta), and time scaling (gamma) are interpretable and\nstable across folds, aligning with known principles of human physiology. This\nwork is the first to introduce a multi-task PINN framework for wearable emotion\nrecognition, offering improved performance, generalizability, and model\ntransparency. The proposed system provides a foundation for future\ninterpretable and multimodal applications in healthcare and human-computer\ninteraction."}
{"id": "2505.18247", "pdf": "https://arxiv.org/pdf/2505.18247", "abs": "https://arxiv.org/abs/2505.18247", "authors": ["Kunal Sawarkar", "Shivam R. Solanki", "Abhilasha Mangal"], "title": "MetaGen Blended RAG: Higher Accuracy for Domain-Specific Q&A Without Fine-Tuning", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "Preprint. Paper Submitted NeurIPS 2025- The Thirty-Ninth Annual\n  Conference on Neural Information Processing Systems", "summary": "Despite the widespread exploration of Retrieval-Augmented Generation (RAG),\nits deployment in enterprises for domain-specific datasets remains limited due\nto poor answer accuracy. These corpora, often shielded behind firewalls in\nprivate enterprise knowledge bases, having complex, domain-specific\nterminology, rarely seen by LLMs during pre-training; exhibit significant\nsemantic variability across domains (like networking, military, or legal,\netc.), or even within a single domain like medicine, and thus result in poor\ncontext precision for RAG systems. Currently, in such situations, fine-tuning\nor RAG with fine-tuning is attempted, but these approaches are slow, expensive,\nand lack generalization for accuracy as the new domain-specific data emerges.\nWe propose an approach for Enterprise Search that focuses on enhancing the\nretriever for a domain-specific corpus through hybrid query indexes and\nmetadata enrichment. This 'MetaGen Blended RAG' method constructs a metadata\ngeneration pipeline using key concepts, topics, and acronyms, and then creates\na metadata-enriched hybrid index with boosted search queries. This approach\navoids overfitting and generalizes effectively across domains. On the PubMedQA\nbenchmark for the biomedical domain, the proposed method achieves 82% retrieval\naccuracy and 77% RAG accuracy, surpassing all previous RAG accuracy results\nwithout fine-tuning and sets a new benchmark for zero-shot results while\noutperforming much larger models like GPT3.5. The results are even comparable\nto the best fine-tuned models on this dataset, and we further demonstrate the\nrobustness and scalability of the approach by evaluating it on other Q&A\ndatasets like SQuAD, NQ etc."}
{"id": "2505.18457", "pdf": "https://arxiv.org/pdf/2505.18457", "abs": "https://arxiv.org/abs/2505.18457", "authors": ["Abir Ray"], "title": "EdgeAgentX: A Novel Framework for Agentic AI at the Edge in Military Communication Networks", "categories": ["cs.AI", "cs.LG", "cs.MA"], "comment": "6 pages, 2 figures", "summary": "This paper introduces EdgeAgentX, a novel framework integrating federated\nlearning (FL), multi-agent reinforcement learning (MARL), and adversarial\ndefense mechanisms, tailored for military communication networks. EdgeAgentX\nsignificantly improves autonomous decision-making, reduces latency, enhances\nthroughput, and robustly withstands adversarial disruptions, as evidenced by\ncomprehensive simulations."}
{"id": "2505.18171", "pdf": "https://arxiv.org/pdf/2505.18171", "abs": "https://arxiv.org/abs/2505.18171", "authors": ["Tengwei Song", "Xudong Ma", "Yang Liu", "Jie Luo"], "title": "Robust Knowledge Graph Embedding via Denoising", "categories": ["cs.LG"], "comment": null, "summary": "We focus on obtaining robust knowledge graph embedding under perturbation in\nthe embedding space. To address these challenges, we introduce a novel\nframework, Robust Knowledge Graph Embedding via Denoising, which enhances the\nrobustness of KGE models on noisy triples. By treating KGE methods as\nenergy-based models, we leverage the established connection between denoising\nand score matching, enabling the training of a robust denoising KGE model.\nFurthermore, we propose certified robustness evaluation metrics for KGE methods\nbased on the concept of randomized smoothing. Through comprehensive experiments\non benchmark datasets, our framework consistently shows superior performance\ncompared to existing state-of-the-art KGE methods when faced with perturbed\nentity embedding."}
{"id": "2505.18283", "pdf": "https://arxiv.org/pdf/2505.18283", "abs": "https://arxiv.org/abs/2505.18283", "authors": ["Jianghao Wu", "Feilong Tang", "Yulong Li", "Ming Hu", "Haochen Xue", "Shoaib Jameel", "Yutong Xie", "Imran Razzak"], "title": "TAGS: A Test-Time Generalist-Specialist Framework with Retrieval-Augmented Reasoning and Verification", "categories": ["cs.CL", "cs.AI", "cs.MA", "I.2.7"], "comment": "16 pages including references, 2 figures", "summary": "Recent advances such as Chain-of-Thought prompting have significantly\nimproved large language models (LLMs) in zero-shot medical reasoning. However,\nprompting-based methods often remain shallow and unstable, while fine-tuned\nmedical LLMs suffer from poor generalization under distribution shifts and\nlimited adaptability to unseen clinical scenarios. To address these\nlimitations, we present TAGS, a test-time framework that combines a broadly\ncapable generalist with a domain-specific specialist to offer complementary\nperspectives without any model fine-tuning or parameter updates. To support\nthis generalist-specialist reasoning process, we introduce two auxiliary\nmodules: a hierarchical retrieval mechanism that provides multi-scale exemplars\nby selecting examples based on both semantic and rationale-level similarity,\nand a reliability scorer that evaluates reasoning consistency to guide final\nanswer aggregation. TAGS achieves strong performance across nine MedQA\nbenchmarks, boosting GPT-4o accuracy by 13.8%, DeepSeek-R1 by 16.8%, and\nimproving a vanilla 7B model from 14.1% to 23.9%. These results surpass several\nfine-tuned medical LLMs, without any parameter updates. The code will be\navailable at https://github.com/JianghaoWu/TAGS."}
{"id": "2505.18467", "pdf": "https://arxiv.org/pdf/2505.18467", "abs": "https://arxiv.org/abs/2505.18467", "authors": ["Unggi Lee", "Jaeyong Lee", "Jiyeong Bae", "Yeil Jeong", "Junbo Koh", "Gyeonggeon Lee", "Gunho Lee", "Taekyung Ahn", "Hyeoncheol Kim"], "title": "Pedagogy-R1: Pedagogically-Aligned Reasoning Model with Balanced Educational Benchmark", "categories": ["cs.AI", "cs.CL"], "comment": "15 pages, 5 figures, 4 tables", "summary": "Recent advances in large reasoning models (LRMs) show strong performance in\nstructured domains such as mathematics and programming; however, they often\nlack pedagogical coherence and realistic teaching behaviors. To bridge this\ngap, we introduce Pedagogy-R1, a framework that adapts LRMs for classroom use\nthrough three innovations: (1) a distillation-based pipeline that filters and\nrefines model outputs for instruction-tuning, (2) the Well-balanced Educational\nBenchmark (WBEB), which evaluates performance across subject knowledge,\npedagogical knowledge, tracing, essay scoring, and teacher decision-making, and\n(3) a Chain-of-Pedagogy (CoP) prompting strategy for generating and eliciting\nteacher-style reasoning. Our mixed-method evaluation combines quantitative\nmetrics with qualitative analysis, providing the first systematic assessment of\nLRMs' pedagogical strengths and limitations."}
{"id": "2505.18176", "pdf": "https://arxiv.org/pdf/2505.18176", "abs": "https://arxiv.org/abs/2505.18176", "authors": ["Jonathan Tammer Eweis-Labolle", "Tyler Johnson", "Xiangyu Sun", "Ramin Bostanabad"], "title": "Should We Simultaneously Calibrate Multiple Computer Models?", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "In an increasing number of applications designers have access to multiple\ncomputer models which typically have different levels of fidelity and cost.\nTraditionally, designers calibrate these models one at a time against some\nhigh-fidelity data (e.g., experiments). In this paper, we question this\ntradition and assess the potential of calibrating multiple computer models at\nthe same time. To this end, we develop a probabilistic framework that is\nfounded on customized neural networks (NNs) that are designed to calibrate an\narbitrary number of computer models. In our approach, we (1) consider the fact\nthat most computer models are multi-response and that the number and nature of\ncalibration parameters may change across the models, and (2) learn a unique\nprobability distribution for each calibration parameter of each computer model,\n(3) develop a loss function that enables our NN to emulate all data sources\nwhile calibrating the computer models, and (4) aim to learn a visualizable\nlatent space where model-form errors can be identified. We test the performance\nof our approach on analytic and engineering problems to understand the\npotential advantages and pitfalls in simultaneous calibration of multiple\ncomputer models. Our method can improve predictive accuracy, however, it is\nprone to non-identifiability issues in higher-dimensional input spaces that are\nnormally constrained by underlying physics."}
{"id": "2505.18298", "pdf": "https://arxiv.org/pdf/2505.18298", "abs": "https://arxiv.org/abs/2505.18298", "authors": ["Jinyan Su", "Claire Cardie"], "title": "Thinking Fast and Right: Balancing Accuracy and Reasoning Length with Adaptive Rewards", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong reasoning abilities in\nmathematical tasks, often enhanced through reinforcement learning (RL).\nHowever, RL-trained models frequently produce unnecessarily long reasoning\ntraces -- even for simple queries -- leading to increased inference costs and\nlatency. While recent approaches attempt to control verbosity by adding length\npenalties to the reward function, these methods rely on fixed penalty terms\nthat are hard to tune and cannot adapt as the model's reasoning capability\nevolves, limiting their effectiveness. In this work, we propose an adaptive\nreward-shaping method that enables LLMs to \"think fast and right\" -- producing\nconcise outputs without sacrificing correctness. Our method dynamically adjusts\nthe reward trade-off between accuracy and response length based on model\nperformance: when accuracy is high, the length penalty increases to encourage\nfaster length reduction; when accuracy drops, the penalty is relaxed to\npreserve correctness. This adaptive reward accelerates early-stage length\nreduction while avoiding over-compression in later stages. Experiments across\nmultiple datasets show that our approach consistently and dramatically reduces\nreasoning length while largely maintaining accuracy, offering a new direction\nfor cost-efficient adaptive reasoning in large-scale language models."}
{"id": "2505.18470", "pdf": "https://arxiv.org/pdf/2505.18470", "abs": "https://arxiv.org/abs/2505.18470", "authors": ["Christopher J. Mungall", "Adnan Malik", "Daniel R. Korn", "Justin T. Reese", "Noel M. O'Boyle", "Noel", "Janna Hastings"], "title": "Chemical classification program synthesis using generative artificial intelligence", "categories": ["cs.AI", "q-bio.BM"], "comment": null, "summary": "Accurately classifying chemical structures is essential for cheminformatics\nand bioinformatics, including tasks such as identifying bioactive compounds of\ninterest, screening molecules for toxicity to humans, finding non-organic\ncompounds with desirable material properties, or organizing large chemical\nlibraries for drug discovery or environmental monitoring. However, manual\nclassification is labor-intensive and difficult to scale to large chemical\ndatabases. Existing automated approaches either rely on manually constructed\nclassification rules, or the use of deep learning methods that lack\nexplainability.\n  This work presents an approach that uses generative artificial intelligence\nto automatically write chemical classifier programs for classes in the Chemical\nEntities of Biological Interest (ChEBI) database. These programs can be used\nfor efficient deterministic run-time classification of SMILES structures, with\nnatural language explanations. The programs themselves constitute an\nexplainable computable ontological model of chemical class nomenclature, which\nwe call the ChEBI Chemical Class Program Ontology (C3PO).\n  We validated our approach against the ChEBI database, and compared our\nresults against state of the art deep learning models. We also demonstrate the\nuse of C3PO to classify out-of-distribution examples taken from metabolomics\nrepositories and natural product databases. We also demonstrate the potential\nuse of our approach to find systematic classification errors in existing\nchemical databases, and show how an ensemble artificial intelligence approach\ncombining generated ontologies, automated literature search, and multimodal\nvision models can be used to pinpoint potential errors requiring expert\nvalidation"}
{"id": "2505.18177", "pdf": "https://arxiv.org/pdf/2505.18177", "abs": "https://arxiv.org/abs/2505.18177", "authors": ["Zhizhong Tan", "Jiexin Zheng", "Xingxing Yang", "Chi Zhang", "Weiping Deng", "Wenyong Wang"], "title": "FedGRec: Dynamic Spatio-Temporal Federated Graph Learning for Secure and Efficient Cross-Border Recommendations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Due to the highly sensitive nature of certain data in cross-border sharing,\ncollaborative cross-border recommendations and data sharing are often subject\nto stringent privacy protection regulations, resulting in insufficient data for\nmodel training. Consequently, achieving efficient cross-border business\nrecommendations while ensuring privacy security poses a significant challenge.\nAlthough federated learning has demonstrated broad potential in collaborative\ntraining without exposing raw data, most existing federated learning-based GNN\ntraining methods still rely on federated averaging strategies, which perform\nsuboptimally on highly heterogeneous graph data. To address this issue, we\npropose FedGRec, a privacy-preserving federated graph learning method for\ncross-border recommendations. FedGRec captures user preferences from\ndistributed multi-domain data to enhance recommendation performance across all\ndomains without privacy leakage. Specifically, FedGRec leverages collaborative\nsignals from local subgraphs associated with users or items to enrich their\nrepresentation learning. Additionally, it employs dynamic spatiotemporal\nmodeling to integrate global and local user preferences in real time based on\nbusiness recommendation states, thereby deriving the final representations of\ntarget users and candidate items. By automatically filtering relevant\nbehaviors, FedGRec effectively mitigates noise interference from unreliable\nneighbors. Furthermore, through a personalized federated aggregation strategy,\nFedGRec adapts global preferences to heterogeneous domain data, enabling\ncollaborative learning of user preferences across multiple domains. Extensive\nexperiments on three datasets demonstrate that FedGRec consistently outperforms\ncompetitive single-domain and cross-domain baselines while effectively\npreserving data privacy in cross-border recommendations."}
{"id": "2505.18322", "pdf": "https://arxiv.org/pdf/2505.18322", "abs": "https://arxiv.org/abs/2505.18322", "authors": ["Zhuozhuo Joy Liu", "Farhan Samir", "Mehar Bhatia", "Laura K. Nelson", "Vered Shwartz"], "title": "Is It Bad to Work All the Time? Cross-Cultural Evaluation of Social Norm Biases in GPT-4", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "LLMs have been demonstrated to align with the values of Western or North\nAmerican cultures. Prior work predominantly showed this effect through\nleveraging surveys that directly ask (originally people and now also LLMs)\nabout their values. However, it is hard to believe that LLMs would consistently\napply those values in real-world scenarios. To address that, we take a\nbottom-up approach, asking LLMs to reason about cultural norms in narratives\nfrom different cultures. We find that GPT-4 tends to generate norms that, while\nnot necessarily incorrect, are significantly less culture-specific. In\naddition, while it avoids overtly generating stereotypes, the stereotypical\nrepresentations of certain cultures are merely hidden rather than suppressed in\nthe model, and such stereotypes can be easily recovered. Addressing these\nchallenges is a crucial step towards developing LLMs that fairly serve their\ndiverse user base."}
{"id": "2505.18483", "pdf": "https://arxiv.org/pdf/2505.18483", "abs": "https://arxiv.org/abs/2505.18483", "authors": ["Hongjia Wu", "Hongxin Zhang", "Wei Chen", "Jiazhi Xia"], "title": "Retrieval Augmented Decision-Making: A Requirements-Driven, Multi-Criteria Framework for Structured Decision Support", "categories": ["cs.AI"], "comment": null, "summary": "Various industries have produced a large number of documents such as\nindustrial plans, technical guidelines, and regulations that are structurally\ncomplex and content-wise fragmented. This poses significant challenges for\nexperts and decision-makers in terms of retrieval and understanding. Although\nexisting LLM-based Retrieval-Augmented Generation methods can provide\ncontext-related suggestions, they lack quantitative weighting and traceable\nreasoning paths, making it difficult to offer multi-level and transparent\ndecision support. To address this issue, this paper proposes the RAD method,\nwhich integrates Multi-Criteria Decision Making with the semantic understanding\ncapabilities of LLMs. The method automatically extracts key criteria from\nindustry documents, builds a weighted hierarchical decision model, and\ngenerates structured reports under model guidance. The RAD framework introduces\nexplicit weight assignment and reasoning chains in decision generation to\nensure accuracy, completeness, and traceability. Experiments show that in\nvarious decision-making tasks, the decision reports generated by RAD\nsignificantly outperform existing methods in terms of detail, rationality, and\nstructure, demonstrating its application value and potential in complex\ndecision support scenarios."}
{"id": "2505.18178", "pdf": "https://arxiv.org/pdf/2505.18178", "abs": "https://arxiv.org/abs/2505.18178", "authors": ["Min Namgung", "Yijun Lin", "JangHyeon Lee", "Yao-Yi Chiang"], "title": "Less is More: Multimodal Region Representation via Pairwise Inter-view Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "With the increasing availability of geospatial datasets, researchers have\nexplored region representation learning (RRL) to analyze complex region\ncharacteristics. Recent RRL methods use contrastive learning (CL) to capture\nshared information between two modalities but often overlook task-relevant\nunique information specific to each modality. Such modality-specific details\ncan explain region characteristics that shared information alone cannot\ncapture. Bringing information factorization to RRL can address this by\nfactorizing multimodal data into shared and unique information. However,\nexisting factorization approaches focus on two modalities, whereas RRL can\nbenefit from various geospatial data. Extending factorization beyond two\nmodalities is non-trivial because modeling high-order relationships introduces\na combinatorial number of learning objectives, increasing model complexity. We\nintroduce Cross modal Knowledge Injected Embedding, an information\nfactorization approach for RRL that captures both shared and unique\nrepresentations. CooKIE uses a pairwise inter-view learning approach that\ncaptures high-order information without modeling high-order dependency,\navoiding exhaustive combinations. We evaluate CooKIE on three regression tasks\nand a land use classification task in New York City and Delhi, India. Results\nshow that CooKIE outperforms existing RRL methods and a factorized RRL model,\ncapturing multimodal information with fewer training parameters and\nfloating-point operations per second (FLOPs). We release the code:\nhttps://github.com/MinNamgung/CooKIE."}
{"id": "2505.18331", "pdf": "https://arxiv.org/pdf/2505.18331", "abs": "https://arxiv.org/abs/2505.18331", "authors": ["Naghmeh Jamali", "Milad Mohammadi", "Danial Baledi", "Zahra Rezvani", "Hesham Faili"], "title": "PerMedCQA: Benchmarking Large Language Models on Medical Consumer Question Answering in Persian Language", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Medical consumer question answering (CQA) is crucial for empowering patients\nby providing personalized and reliable health information. Despite recent\nadvances in large language models (LLMs) for medical QA, consumer-oriented and\nmultilingual resources, particularly in low-resource languages like Persian,\nremain sparse. To bridge this gap, we present PerMedCQA, the first\nPersian-language benchmark for evaluating LLMs on real-world,\nconsumer-generated medical questions. Curated from a large medical QA forum,\nPerMedCQA contains 68,138 question-answer pairs, refined through careful data\ncleaning from an initial set of 87,780 raw entries. We evaluate several\nstate-of-the-art multilingual and instruction-tuned LLMs, utilizing MedJudge, a\nnovel rubric-based evaluation framework driven by an LLM grader, validated\nagainst expert human annotators. Our results highlight key challenges in\nmultilingual medical QA and provide valuable insights for developing more\naccurate and context-aware medical assistance systems. The data is publicly\navailable on https://huggingface.co/datasets/NaghmehAI/PerMedCQA"}
{"id": "2505.18492", "pdf": "https://arxiv.org/pdf/2505.18492", "abs": "https://arxiv.org/abs/2505.18492", "authors": ["Jialiang Sun", "Yuzhi Tang", "Ao Li", "Chris J. Maddison", "Kuldeep S. Meel"], "title": "Enumerate-Conjecture-Prove: Formally Solving Answer-Construction Problems in Math Competitions", "categories": ["cs.AI"], "comment": null, "summary": "Mathematical reasoning lies at the heart of artificial intelligence,\nunderpinning applications in education, program verification, and\nresearch-level mathematical discovery. Mathematical competitions, in\nparticular, present two challenging problem types: theorem-proving, requiring\nrigorous proofs of stated conclusions, and answer-construction, involving\nhypothesizing and formally verifying mathematical objects. Large Language\nModels (LLMs) effectively generate creative candidate answers but struggle with\nformal verification, while symbolic provers ensure rigor but cannot efficiently\nhandle creative conjecture generation. We introduce the\nEnumerate-Conjecture-Prove (ECP) framework, a modular neuro-symbolic method\nintegrating LLM-based enumeration and pattern-driven conjecturing with formal\ntheorem proving. We present ConstructiveBench, a dataset of 3,431\nanswer-construction problems in various math competitions with verified Lean\nformalizations. On the ConstructiveBench dataset, ECP improves the accuracy of\nanswer construction from the Chain-of-Thought (CoT) baseline of 14.54% to\n45.06% with the gpt-4.1-mini model. Moreover, combining with ECP's constructed\nanswers, the state-of-the-art DeepSeek-Prover-V2-7B model generates correct\nproofs for 858 of the 3,431 constructive problems in Lean, achieving 25.01%\naccuracy, compared to 9.86% for symbolic-only baselines. Our code and dataset\nare publicly available at GitHub and HuggingFace, respectively."}
{"id": "2505.18179", "pdf": "https://arxiv.org/pdf/2505.18179", "abs": "https://arxiv.org/abs/2505.18179", "authors": ["Ata Akbari Asanjan", "Olivia Alexander", "Tom Berg", "Clara Zhang", "Matt Yang", "Jad Makki", "Disha Shidham", "Srija Chakraborty", "William Bender", "Stephen Peng", "Arun Ravindran", "Olivier Raiman", "David Potere", "David Bell"], "title": "GAIA: A Foundation Model for Operational Atmospheric Dynamics", "categories": ["cs.LG", "cs.AI"], "comment": "14 pages, 7 figures", "summary": "We present the GAIA (Geospatial Artificial Intelligence for Atmospheres)\nFoundation Model, a novel model that combines masked autoencoders (MAE) and\nself-DIstillation with NO labels (DINO) for analyzing global atmospheric\npatterns in satellite imagery. By integrating these complementary\nself-supervised learning approaches, our model simultaneously captures both\nlocal features and global dependencies. We address two critical challenges in\nsatellite data analysis: reconstructing missing regions and estimating\nprecipitation patterns as our first downstream tasks. The model demonstrates\nsuperior temporal pattern capture compared to standard MAE approaches, while\nmaintaining robust performance in downstream tasks. Our experimental results\nshow strong gap-filling capabilities across varying mask ratios and accurate\nprecipitation estimation with limited training data, achieving a false alarm\nratio of 0.088 and structural similarity of 0.881. This work represents an\nadvancement in self-supervised learning for atmospheric science, providing a\nfoundation for improved weather monitoring and climate analysis. The trained\nmodel weights and accompanying code are publicly available as open-source on\nHugging Face here: https://huggingface.co/bcg-usra-nasa-gaia/GAIA-v1."}
{"id": "2505.18343", "pdf": "https://arxiv.org/pdf/2505.18343", "abs": "https://arxiv.org/abs/2505.18343", "authors": ["Yash Kumar Atri", "Ahmed Alaa", "Thomas Hartvigsen"], "title": "Model Editing with Graph-Based External Memory", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have revolutionized natural language processing,\nyet their practical utility is often limited by persistent issues of\nhallucinations and outdated parametric knowledge. Although post-training model\nediting offers a pathway for dynamic updates, existing methods frequently\nsuffer from overfitting and catastrophic forgetting. To tackle these\nchallenges, we propose a novel framework that leverages hyperbolic geometry and\ngraph neural networks for precise and stable model edits. We introduce HYPE\n(HYperbolic Parameter Editing), which comprises three key components: (i)\nHyperbolic Graph Construction, which uses Poincar\\'e embeddings to represent\nknowledge triples in hyperbolic space, preserving hierarchical relationships\nand preventing unintended side effects by ensuring that edits to parent\nconcepts do not inadvertently affect child concepts; (ii) M\\\"obius-Transformed\nUpdates, which apply hyperbolic addition to propagate edits while maintaining\nstructural consistency within the hyperbolic manifold, unlike conventional\nEuclidean updates that distort relational distances; and (iii) Dual\nStabilization, which combines gradient masking and periodic GNN parameter\nresetting to prevent catastrophic forgetting by focusing updates on critical\nparameters and preserving long-term knowledge. Experiments on CounterFact,\nCounterFact+, and MQuAKE with GPT-J and GPT2-XL demonstrate that HYPE\nsignificantly enhances edit stability, factual accuracy, and multi-hop\nreasoning."}
{"id": "2505.18502", "pdf": "https://arxiv.org/pdf/2505.18502", "abs": "https://arxiv.org/abs/2505.18502", "authors": ["Guodong Du", "Xuanning Zhou", "Junlin Li", "Zhuo Li", "Zesheng Shi", "Wanyu Lin", "Ho-Kin Tang", "Xiucheng Li", "Fangming Liu", "Wenya Wang", "Min Zhang", "Jing Li"], "title": "Knowledge Grafting of Large Language Models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Cross-capability transfer is a key challenge in large language model (LLM)\nresearch, with applications in multi-task integration, model compression, and\ncontinual learning. Recent works like FuseLLM and FuseChat have demonstrated\nthe potential of transferring multiple model capabilities to lightweight\nmodels, enhancing adaptability and efficiency, which motivates our\ninvestigation into more efficient cross-capability transfer methods. However,\nexisting approaches primarily focus on small, homogeneous models, limiting\ntheir applicability. For large, heterogeneous models, knowledge distillation\nwith full-parameter fine-tuning often overlooks the student model's intrinsic\ncapacity and risks catastrophic forgetting, while PEFT methods struggle to\neffectively absorb knowledge from source LLMs. To address these issues, we\nintroduce GraftLLM, a novel method that stores source model capabilities in a\ntarget model with SkillPack format. This approach preserves general\ncapabilities, reduces parameter conflicts, and supports forget-free continual\nlearning and model fusion. We employ a module-aware adaptive compression\nstrategy to compress parameter updates, ensuring efficient storage while\nmaintaining task-specific knowledge. The resulting SkillPack serves as a\ncompact and transferable knowledge carrier, ideal for heterogeneous model\nfusion and continual learning. Experiments across various scenarios demonstrate\nthat GraftLLM outperforms existing techniques in knowledge transfer, knowledge\nfusion, and forget-free learning, providing a scalable and efficient solution\nfor cross-capability transfer. The code is publicly available at:\nhttps://github.com/duguodong7/GraftLLM."}
{"id": "2505.18181", "pdf": "https://arxiv.org/pdf/2505.18181", "abs": "https://arxiv.org/abs/2505.18181", "authors": ["Yunrui Li", "Hao Xu", "Pengyu Hong"], "title": "2DNMRGym: An Annotated Experimental Dataset for Atom-Level Molecular Representation Learning in 2D NMR via Surrogate Supervision", "categories": ["cs.LG", "cs.AI", "cs.CE", "physics.chem-ph"], "comment": null, "summary": "Two-dimensional (2D) Nuclear Magnetic Resonance (NMR) spectroscopy,\nparticularly Heteronuclear Single Quantum Coherence (HSQC) spectroscopy, plays\na critical role in elucidating molecular structures, interactions, and\nelectronic properties. However, accurately interpreting 2D NMR data remains\nlabor-intensive and error-prone, requiring highly trained domain experts,\nespecially for complex molecules. Machine Learning (ML) holds significant\npotential in 2D NMR analysis by learning molecular representations and\nrecognizing complex patterns from data. However, progress has been limited by\nthe lack of large-scale and high-quality annotated datasets. In this work, we\nintroduce 2DNMRGym, the first annotated experimental dataset designed for\nML-based molecular representation learning in 2D NMR. It includes over 22,000\nHSQC spectra, along with the corresponding molecular graphs and SMILES strings.\nUniquely, 2DNMRGym adopts a surrogate supervision setup: models are trained\nusing algorithm-generated annotations derived from a previously validated\nmethod and evaluated on a held-out set of human-annotated gold-standard labels.\nThis enables rigorous assessment of a model's ability to generalize from\nimperfect supervision to expert-level interpretation. We provide benchmark\nresults using a series of 2D and 3D GNN and GNN transformer models,\nestablishing a strong foundation for future work. 2DNMRGym supports scalable\nmodel training and introduces a chemically meaningful benchmark for evaluating\natom-level molecular representations in NMR-guided structural tasks. Our data\nand code is open-source and available on Huggingface and Github."}
{"id": "2505.18356", "pdf": "https://arxiv.org/pdf/2505.18356", "abs": "https://arxiv.org/abs/2505.18356", "authors": ["Lucas Bandarkar", "Nanyun Peng"], "title": "The Unreasonable Effectiveness of Model Merging for Cross-Lingual Transfer in LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": null, "summary": "Large language models (LLMs) still struggle across tasks outside of\nhigh-resource languages. In this work, we investigate cross-lingual transfer to\nlower-resource languages where task-specific post-training data is scarce.\nBuilding on prior work, we first validate that the subsets of model parameters\nthat matter most for mathematical reasoning and multilingual capabilities are\ndistinctly non-overlapping. To exploit this implicit separability between task\nand target language parameterization, we develop and analyze numerous modular\nframeworks to improve the composition of the two during fine-tuning. These\nmethods generally employ freezing parameters or post hoc model merging to\nassign math and language improvement to different key parts of the LLM. In the\nabsence of in-language math data, we demonstrate that the modular approaches\nsuccessfully improve upon baselines across three languages, four models, and\ntwo fine-tuning paradigms (full and LoRA). Furthermore, we identify the most\nconsistently successful modular method to be fine-tuning separate language and\nmath experts and model merging via Layer-Swapping, somewhat surprisingly. We\noffer possible explanations for this result via recent works on the linearity\nof task vectors. We further explain this by empirically showing that reverting\nless useful fine-tuning updates after training often outperforms freezing them\nfrom the start."}
{"id": "2505.18517", "pdf": "https://arxiv.org/pdf/2505.18517", "abs": "https://arxiv.org/abs/2505.18517", "authors": ["Pooneh Mousavi", "Shubham Gupta", "Cem Subakan", "Mirco Ravanelli"], "title": "LiSTEN: Learning Soft Token Embeddings for Neural Audio LLMs", "categories": ["cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "Foundation models based on large language models (LLMs) have shown great\nsuccess in handling various tasks and modalities. However, adapting these\nmodels for general-purpose audio-language tasks is challenging due to\ndifferences in acoustic environments and task variations. In this work, we\nintroduce LiSTEN Learning Soft Token Embeddings for Neural Audio LLMs), a\nframework for adapting LLMs to speech and audio tasks. LiSTEN uses a dynamic\nprompt selection strategy with learnable key-value pairs, allowing the model to\nbalance general and task-specific knowledge while avoiding overfitting in a\nmultitask setting. Our approach reduces dependence on large-scale ASR or\ncaptioning datasets, achieves competitive performance with fewer trainable\nparameters, and simplifies training by using a single-stage process.\nAdditionally, LiSTEN enhances interpretability by analyzing the diversity and\noverlap of selected prompts across different tasks."}
{"id": "2505.18193", "pdf": "https://arxiv.org/pdf/2505.18193", "abs": "https://arxiv.org/abs/2505.18193", "authors": ["Antoine Collas", "Ce Ju", "Nicolas Salvy", "Bertrand Thirion"], "title": "Riemannian Flow Matching for Brain Connectivity Matrices via Pullback Geometry", "categories": ["cs.LG", "eess.SP", "stat.ML"], "comment": null, "summary": "Generating realistic brain connectivity matrices is key to analyzing\npopulation heterogeneity in brain organization, understanding disease, and\naugmenting data in challenging classification problems. Functional connectivity\nmatrices lie in constrained spaces--such as the set of symmetric positive\ndefinite or correlation matrices--that can be modeled as Riemannian manifolds.\nHowever, using Riemannian tools typically requires redefining core operations\n(geodesics, norms, integration), making generative modeling computationally\ninefficient. In this work, we propose DiffeoCFM, an approach that enables\nconditional flow matching (CFM) on matrix manifolds by exploiting pullback\nmetrics induced by global diffeomorphisms on Euclidean spaces. We show that\nRiemannian CFM with such metrics is equivalent to applying standard CFM after\ndata transformation. This equivalence allows efficient vector field learning,\nand fast sampling with standard ODE solvers. We instantiate DiffeoCFM with two\ndifferent settings: the matrix logarithm for covariance matrices and the\nnormalized Cholesky decomposition for correlation matrices. We evaluate\nDiffeoCFM on three large-scale fMRI datasets with more than 4600 scans from\n2800 subjects (ADNI, ABIDE, OASIS-3) and two EEG motor imagery datasets with\nover 30000 trials from 26 subjects (BNCI2014-002 and BNCI2015-001). It enables\nfast training and achieves state-of-the-art performance, all while preserving\nmanifold constraints."}
{"id": "2505.18363", "pdf": "https://arxiv.org/pdf/2505.18363", "abs": "https://arxiv.org/abs/2505.18363", "authors": ["AmirHossein Safdarian", "Milad Mohammadi", "Ehsan Jahanbakhsh", "Mona Shahamat Naderi", "Heshaam Faili"], "title": "SchemaGraphSQL: Efficient Schema Linking with Pathfinding Graph Algorithms for Text-to-SQL on Large-Scale Databases", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "Text-to-SQL systems translate natural language questions into executable SQL\nqueries, and recent progress with large language models (LLMs) has driven\nsubstantial improvements in this task. Schema linking remains a critical\ncomponent in Text-to-SQL systems, reducing prompt size for models with narrow\ncontext windows and sharpening model focus even when the entire schema fits. We\npresent a zero-shot, training-free schema linking approach that first\nconstructs a schema graph based on foreign key relations, then uses a single\nprompt to Gemini 2.5 Flash to extract source and destination tables from the\nuser query, followed by applying classical path-finding algorithms and\npost-processing to identify the optimal sequence of tables and columns that\nshould be joined, enabling the LLM to generate more accurate SQL queries.\nDespite being simple, cost-effective, and highly scalable, our method achieves\nstate-of-the-art results on the BIRD benchmark, outperforming previous\nspecialized, fine-tuned, and complex multi-step LLM-based approaches. We\nconduct detailed ablation studies to examine the precision-recall trade-off in\nour framework. Additionally, we evaluate the execution accuracy of our schema\nfiltering method compared to other approaches across various model sizes."}
{"id": "2505.18531", "pdf": "https://arxiv.org/pdf/2505.18531", "abs": "https://arxiv.org/abs/2505.18531", "authors": ["Jiayi Zhou", "Jiaming Ji", "Boyuan Chen", "Jiapeng Sun", "Wenqi Chen", "Donghai Hong", "Sirui Han", "Yike Guo", "Yaodong Yang"], "title": "Generative RLHF-V: Learning Principles from Multi-modal Human Preference", "categories": ["cs.AI", "cs.CV"], "comment": "9 pages, 8 figures", "summary": "Training multi-modal large language models (MLLMs) that align with human\nintentions is a long-term challenge. Traditional score-only reward models for\nalignment suffer from low accuracy, weak generalization, and poor\ninterpretability, blocking the progress of alignment methods, e.g.,\nreinforcement learning from human feedback (RLHF). Generative reward models\n(GRMs) leverage MLLMs' intrinsic reasoning capabilities to discriminate\npair-wise responses, but their pair-wise paradigm makes it hard to generalize\nto learnable rewards. We introduce Generative RLHF-V, a novel alignment\nframework that integrates GRMs with multi-modal RLHF. We propose a two-stage\npipeline: $\\textbf{multi-modal generative reward modeling from RL}$, where RL\nguides GRMs to actively capture human intention, then predict the correct\npair-wise scores; and $\\textbf{RL optimization from grouped comparison}$, which\nenhances multi-modal RL scoring precision by grouped responses comparison.\nExperimental results demonstrate that, besides out-of-distribution\ngeneralization of RM discrimination, our framework improves 4 MLLMs'\nperformance across 7 benchmarks by $18.1\\%$, while the baseline RLHF is only\n$5.3\\%$. We further validate that Generative RLHF-V achieves a near-linear\nimprovement with an increasing number of candidate responses. Our code and\nmodels can be found at https://generative-rlhf-v.github.io."}
{"id": "2505.18221", "pdf": "https://arxiv.org/pdf/2505.18221", "abs": "https://arxiv.org/abs/2505.18221", "authors": ["Sharad Duwal", "Mir Nafis Sharear Shopnil", "Abhishek Tyagi", "Adiba Mahbub Proma"], "title": "Evidence-Grounded Multimodal Misinformation Detection with Attention-Based GNNs", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "Multimodal out-of-context (OOC) misinformation is misinformation that\nrepurposes real images with unrelated or misleading captions. Detecting such\nmisinformation is challenging because it requires resolving the context of the\nclaim before checking for misinformation. Many current methods, including LLMs\nand LVLMs, do not perform this contextualization step. LLMs hallucinate in\nabsence of context or parametric knowledge. In this work, we propose a\ngraph-based method that evaluates the consistency between the image and the\ncaption by constructing two graph representations: an evidence graph, derived\nfrom online textual evidence, and a claim graph, from the claim in the caption.\nUsing graph neural networks (GNNs) to encode and compare these representations,\nour framework then evaluates the truthfulness of image-caption pairs. We create\ndatasets for our graph-based method, evaluate and compare our baseline model\nagainst popular LLMs on the misinformation detection task. Our method scores\n$93.05\\%$ detection accuracy on the evaluation set and outperforms the\nsecond-best performing method (an LLM) by $2.82\\%$, making a case for smaller\nand task-specific methods."}
{"id": "2505.18374", "pdf": "https://arxiv.org/pdf/2505.18374", "abs": "https://arxiv.org/abs/2505.18374", "authors": ["Jarrod Ragsdale", "Rajendra Boppana"], "title": "ShIOEnv: A CLI Behavior-Capturing Environment Enabling Grammar-Guided Command Synthesis for Dataset Curation", "categories": ["cs.CL", "cs.LG"], "comment": "18 pages, 11 figures, conference preprint", "summary": "Command-line interfaces (CLIs) provide structured textual environments for\nsystem administration. Explorations have been performed using pre-trained\nlanguage models (PLMs) to simulate these environments for safe interaction in\nhigh-risk environments. However, their use has been constrained to frozen,\nlarge parameter models like GPT. For smaller architectures to reach a similar\nlevel of believability, a rich dataset of CLI interactions is required.\nExisting public datasets focus on mapping natural-language tasks to commands,\nomitting crucial execution data such as exit codes, outputs, and environmental\nside effects, limiting their usability for behavioral modeling. We introduce a\nShell Input -Output Environment (ShIOEnv), which casts command construction as\na Markov Decision Process whose state is the partially built sequence and whose\nactions append arguments. After each action, ShIOEnv executes the candidate and\nreturns its exit status, output, and progress toward a minimal-length\nbehavioral objective. Due to the intractable nature of the combinatorial\nargument state-action space, we derive a context-free grammar from man pages to\nmask invalid arguments from being emitted. We explore random and\nproximal-policy optimization (PPO)-optimized sampling of unrestricted and\ngrammar-masked action spaces to produce four exploration strategies. We\nobserved that grammar masking and PPO significantly improve sample efficiency\nto produce a higher quality dataset (maximizing the number of arguments while\nminimizing redundancies). Policy-generated datasets of shell input-output\nbehavior pairs are used to fine-tune CodeT5, where we observe 85% improvements\nin BLEU-4 when constraining the action space to grammar productions with an\nadditional 26% improvement when applying PPO. The ShIOEnv environment and\ncurated command behavior datasets are released for use in future research."}
{"id": "2505.18541", "pdf": "https://arxiv.org/pdf/2505.18541", "abs": "https://arxiv.org/abs/2505.18541", "authors": ["Yongjie Wang", "Jonathan Leung", "Zhiqi Shen"], "title": "RoleRAG: Enhancing LLM Role-Playing via Graph Guided Retrieval", "categories": ["cs.AI", "68T50", "I.2.7"], "comment": "A Retrieval-enhanced LLM Role-playing", "summary": "Large Language Models (LLMs) have shown promise in character imitation,\nenabling immersive and engaging conversations. However, they often generate\ncontent that is irrelevant or inconsistent with a character's background. We\nattribute these failures to: (1) the inability to accurately recall\ncharacter-specific knowledge due to entity ambiguity, and (2) a lack of\nawareness of the character's cognitive boundaries. To address these issues, we\npropose RoleRAG, a retrieval-based framework that integrates efficient entity\ndisambiguation for knowledge indexing with a boundary-aware retriever for\nextracting contextually appropriate information from a structured knowledge\ngraph. Experiments on role-playing benchmarks show that RoleRAG's calibrated\nretrieval helps both general-purpose and role-specific LLMs better align with\ncharacter knowledge and reduce hallucinated responses."}
{"id": "2505.18227", "pdf": "https://arxiv.org/pdf/2505.18227", "abs": "https://arxiv.org/abs/2505.18227", "authors": ["Zhenglun Kong", "Yize Li", "Fanhu Zeng", "Lei Xin", "Shvat Messica", "Xue Lin", "Pu Zhao", "Manolis Kellis", "Hao Tang", "Marinka Zitnik"], "title": "Token Reduction Should Go Beyond Efficiency in Generative Models -- From Vision, Language to Multimodality", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In Transformer architectures, tokens\\textemdash discrete units derived from\nraw data\\textemdash are formed by segmenting inputs into fixed-length chunks.\nEach token is then mapped to an embedding, enabling parallel attention\ncomputations while preserving the input's essential information. Due to the\nquadratic computational complexity of transformer self-attention mechanisms,\ntoken reduction has primarily been used as an efficiency strategy. This is\nespecially true in single vision and language domains, where it helps balance\ncomputational costs, memory usage, and inference latency. Despite these\nadvances, this paper argues that token reduction should transcend its\ntraditional efficiency-oriented role in the era of large generative models.\nInstead, we position it as a fundamental principle in generative modeling,\ncritically influencing both model architecture and broader applications.\nSpecifically, we contend that across vision, language, and multimodal systems,\ntoken reduction can: (i) facilitate deeper multimodal integration and\nalignment, (ii) mitigate \"overthinking\" and hallucinations, (iii) maintain\ncoherence over long inputs, and (iv) enhance training stability, etc. We\nreframe token reduction as more than an efficiency measure. By doing so, we\noutline promising future directions, including algorithm design, reinforcement\nlearning-guided token reduction, token optimization for in-context learning,\nand broader ML and scientific domains. We highlight its potential to drive new\nmodel architectures and learning strategies that improve robustness, increase\ninterpretability, and better align with the objectives of generative modeling."}
{"id": "2505.18383", "pdf": "https://arxiv.org/pdf/2505.18383", "abs": "https://arxiv.org/abs/2505.18383", "authors": ["Abdellah El Mekki", "Houdaifa Atou", "Omer Nacar", "Shady Shehata", "Muhammad Abdul-Mageed"], "title": "NileChat: Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities", "categories": ["cs.CL"], "comment": null, "summary": "Enhancing the linguistic capabilities of Large Language Models (LLMs) to\ninclude low-resource languages is a critical research area. Current research\ndirections predominantly rely on synthetic data generated by translating\nEnglish corpora, which, while demonstrating promising linguistic understanding\nand translation abilities, often results in models aligned with source language\nculture. These models frequently fail to represent the cultural heritage and\nvalues of local communities. This work proposes a methodology to create both\nsynthetic and retrieval-based pre-training data tailored to a specific\ncommunity, considering its (i) language, (ii) cultural heritage, and (iii)\ncultural values. We demonstrate our methodology using Egyptian and Moroccan\ndialects as testbeds, chosen for their linguistic and cultural richness and\ncurrent underrepresentation in LLMs. As a proof-of-concept, we develop\nNileChat, a 3B parameter LLM adapted for Egyptian and Moroccan communities,\nincorporating their language, cultural heritage, and values. Our results on\nvarious understanding, translation, and cultural and values alignment\nbenchmarks show that NileChat outperforms existing Arabic-aware LLMs of similar\nsize and performs on par with larger models. We share our methods, data, and\nmodels with the community to promote the inclusion and coverage of more diverse\ncommunities in LLM development."}
{"id": "2505.18547", "pdf": "https://arxiv.org/pdf/2505.18547", "abs": "https://arxiv.org/abs/2505.18547", "authors": ["Min Cheng", "Fatemeh Doudi", "Dileep Kalathil", "Mohammad Ghavamzadeh", "Panganamala R. Kumar"], "title": "Diffusion Blend: Inference-Time Multi-Preference Alignment for Diffusion Models", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Reinforcement learning (RL) algorithms have been used recently to align\ndiffusion models with downstream objectives such as aesthetic quality and\ntext-image consistency by fine-tuning them to maximize a single reward function\nunder a fixed KL regularization. However, this approach is inherently\nrestrictive in practice, where alignment must balance multiple, often\nconflicting objectives. Moreover, user preferences vary across prompts,\nindividuals, and deployment contexts, with varying tolerances for deviation\nfrom a pre-trained base model. We address the problem of inference-time\nmulti-preference alignment: given a set of basis reward functions and a\nreference KL regularization strength, can we design a fine-tuning procedure so\nthat, at inference time, it can generate images aligned with any user-specified\nlinear combination of rewards and regularization, without requiring additional\nfine-tuning? We propose Diffusion Blend, a novel approach to solve\ninference-time multi-preference alignment by blending backward diffusion\nprocesses associated with fine-tuned models, and we instantiate this approach\nwith two algorithms: DB-MPA for multi-reward alignment and DB-KLA for KL\nregularization control. Extensive experiments show that Diffusion Blend\nalgorithms consistently outperform relevant baselines and closely match or\nexceed the performance of individually fine-tuned models, enabling efficient,\nuser-driven alignment at inference-time. The code is available at\nhttps://github.com/bluewoods127/DB-2025}{github.com/bluewoods127/DB-2025."}
{"id": "2505.18230", "pdf": "https://arxiv.org/pdf/2505.18230", "abs": "https://arxiv.org/abs/2505.18230", "authors": ["Louis Bthune", "David Vigouroux", "Yilun Du", "Rufin VanRullen", "Thomas Serre", "Victor Boutin"], "title": "Follow the Energy, Find the Path: Riemannian Metrics from Energy-Based Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "What is the shortest path between two data points lying in a high-dimensional\nspace? While the answer is trivial in Euclidean geometry, it becomes\nsignificantly more complex when the data lies on a curved manifold -- requiring\na Riemannian metric to describe the space's local curvature. Estimating such a\nmetric, however, remains a major challenge in high dimensions.\n  In this work, we propose a method for deriving Riemannian metrics directly\nfrom pretrained Energy-Based Models (EBMs) -- a class of generative models that\nassign low energy to high-density regions. These metrics define spatially\nvarying distances, enabling the computation of geodesics -- shortest paths that\nfollow the data manifold's intrinsic geometry. We introduce two novel metrics\nderived from EBMs and show that they produce geodesics that remain closer to\nthe data manifold and exhibit lower curvature distortion, as measured by\nalignment with ground-truth trajectories. We evaluate our approach on\nincreasingly complex datasets: synthetic datasets with known data density,\nrotated character images with interpretable geometry, and high-resolution\nnatural images embedded in a pretrained VAE latent space.\n  Our results show that EBM-derived metrics consistently outperform established\nbaselines, especially in high-dimensional settings. Our work is the first to\nderive Riemannian metrics from EBMs, enabling data-aware geodesics and\nunlocking scalable, geometry-driven learning for generative modeling and\nsimulation."}
{"id": "2505.18405", "pdf": "https://arxiv.org/pdf/2505.18405", "abs": "https://arxiv.org/abs/2505.18405", "authors": ["Debrup Das", "Sam O' Nuallain", "Razieh Rahimi"], "title": "RaDeR: Reasoning-aware Dense Retrieval Models", "categories": ["cs.CL", "cs.IR"], "comment": "26 pages", "summary": "We propose RaDeR, a set of reasoning-based dense retrieval models trained\nwith data derived from mathematical problem solving using large language models\n(LLMs). Our method leverages retrieval-augmented reasoning trajectories of an\nLLM and self-reflective relevance evaluation, enabling the creation of both\ndiverse and hard-negative samples for reasoning-intensive relevance. RaDeR\nretrievers, trained for mathematical reasoning, effectively generalize to\ndiverse reasoning tasks in the BRIGHT and RAR-b benchmarks, consistently\noutperforming strong baselines in overall performance.Notably, RaDeR achieves\nsignificantly higher performance than baselines on the Math and Coding splits.\nIn addition, RaDeR presents the first dense retriever that outperforms BM25\nwhen queries are Chain-of-Thought reasoning steps, underscoring the critical\nrole of reasoning-based retrieval to augment reasoning language models.\nFurthermore, RaDeR achieves comparable or superior performance while using only\n2.5% of the training data used by the concurrent work REASONIR, highlighting\nthe quality of our synthesized training data."}
{"id": "2505.18575", "pdf": "https://arxiv.org/pdf/2505.18575", "abs": "https://arxiv.org/abs/2505.18575", "authors": ["Yongjie Wang", "Yibo Wang", "Xin Zhou", "Zhiqi Shen"], "title": "Response Uncertainty and Probe Modeling: Two Sides of the Same Coin in LLM Interpretability?", "categories": ["cs.AI", "68T50, 68T35", "I.2.0"], "comment": "18 Pages", "summary": "Probing techniques have shown promise in revealing how LLMs encode\nhuman-interpretable concepts, particularly when applied to curated datasets.\nHowever, the factors governing a dataset's suitability for effective probe\ntraining are not well-understood. This study hypothesizes that probe\nperformance on such datasets reflects characteristics of both the LLM's\ngenerated responses and its internal feature space. Through quantitative\nanalysis of probe performance and LLM response uncertainty across a series of\ntasks, we find a strong correlation: improved probe performance consistently\ncorresponds to a reduction in response uncertainty, and vice versa.\nSubsequently, we delve deeper into this correlation through the lens of feature\nimportance analysis. Our findings indicate that high LLM response variance is\nassociated with a larger set of important features, which poses a greater\nchallenge for probe models and often results in diminished performance.\nMoreover, leveraging the insights from response uncertainty analysis, we are\nable to identify concrete examples where LLM representations align with human\nknowledge across diverse domains, offering additional evidence of interpretable\nreasoning in LLMs."}
{"id": "2505.18231", "pdf": "https://arxiv.org/pdf/2505.18231", "abs": "https://arxiv.org/abs/2505.18231", "authors": ["Donghyun Son", "Euntae Choi", "Sungjoo Yoo"], "title": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Model (LLM) inference is typically memory-intensive,\nespecially when processing large batch sizes and long sequences, due to the\nlarge size of key-value (KV) cache. Vector Quantization (VQ) is recently\nadopted to alleviate this issue, but we find that the existing approach is\nsusceptible to distribution shift due to its reliance on calibration datasets.\nTo address this limitation, we introduce NSNQuant, a calibration-free Vector\nQuantization (VQ) technique designed for low-bit compression of the KV cache.\nBy applying a three-step transformation-1) a token-wise normalization\n(Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise\nnormalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns\nthe token distribution with the standard normal distribution. This alignment\nenables robust, calibration-free vector quantization using a single reusable\ncodebook. Extensive experiments show that NSNQuant consistently outperforms\nprior methods in both 1-bit and 2-bit settings, offering strong generalization\nand up to 3$\\times$ throughput gain over full-precision baselines."}
{"id": "2505.18411", "pdf": "https://arxiv.org/pdf/2505.18411", "abs": "https://arxiv.org/abs/2505.18411", "authors": ["Yue Jiang", "Jichu Li", "Yang Liu", "Dingkang Yang", "Feng Zhou", "Quyu Kong"], "title": "DanmakuTPPBench: A Multi-modal Benchmark for Temporal Point Process Modeling and Understanding", "categories": ["cs.CL", "cs.LG"], "comment": "https://github.com/FRENKIE-CHIANG/DanmakuTPPBench", "summary": "We introduce DanmakuTPPBench, a comprehensive benchmark designed to advance\nmulti-modal Temporal Point Process (TPP) modeling in the era of Large Language\nModels (LLMs). While TPPs have been widely studied for modeling temporal event\nsequences, existing datasets are predominantly unimodal, hindering progress in\nmodels that require joint reasoning over temporal, textual, and visual\ninformation. To address this gap, DanmakuTPPBench comprises two complementary\ncomponents: (1) DanmakuTPP-Events, a novel dataset derived from the Bilibili\nvideo platform, where user-generated bullet comments (Danmaku) naturally form\nmulti-modal events annotated with precise timestamps, rich textual content, and\ncorresponding video frames; (2) DanmakuTPP-QA, a challenging question-answering\ndataset constructed via a novel multi-agent pipeline powered by\nstate-of-the-art LLMs and multi-modal LLMs (MLLMs), targeting complex\ntemporal-textual-visual reasoning. We conduct extensive evaluations using both\nclassical TPP models and recent MLLMs, revealing significant performance gaps\nand limitations in current methods' ability to model multi-modal event\ndynamics. Our benchmark establishes strong baselines and calls for further\nintegration of TPP modeling into the multi-modal language modeling landscape.\nThe code and dataset have been released at\nhttps://github.com/FRENKIE-CHIANG/DanmakuTPPBench"}
{"id": "2505.18585", "pdf": "https://arxiv.org/pdf/2505.18585", "abs": "https://arxiv.org/abs/2505.18585", "authors": ["Yedi Zhang", "Sun Yi Emma", "Annabelle Lee Jia En", "Annabelle Lee Jia En", "Jin Song Dong"], "title": "RvLLM: LLM Runtime Verification with Domain Knowledge", "categories": ["cs.AI", "cs.CL", "cs.LO"], "comment": "12 pages, 2 figures", "summary": "Large language models (LLMs) have emerged as a dominant AI paradigm due to\ntheir exceptional text understanding and generation capabilities. However,\ntheir tendency to generate inconsistent or erroneous outputs challenges their\nreliability, especially in high-stakes domains requiring accuracy and\ntrustworthiness. Existing research primarily focuses on detecting and\nmitigating model misbehavior in general-purpose scenarios, often overlooking\nthe potential of integrating domain-specific knowledge. In this work, we\nadvance misbehavior detection by incorporating domain knowledge. The core idea\nis to design a general specification language that enables domain experts to\ncustomize domain-specific predicates in a lightweight and intuitive manner,\nsupporting later runtime verification of LLM outputs. To achieve this, we\ndesign a novel specification language, ESL, and introduce a runtime\nverification framework, RvLLM, to validate LLM output against domain-specific\nconstraints defined in ESL. We evaluate RvLLM on three representative tasks:\nviolation detection against Singapore Rapid Transit Systems Act, numerical\ncomparison, and inequality solving. Experimental results demonstrate that RvLLM\neffectively detects erroneous outputs across various LLMs in a lightweight and\nflexible manner. The results reveal that despite their impressive capabilities,\nLLMs remain prone to low-level errors due to limited interpretability and a\nlack of formal guarantees during inference, and our framework offers a\npotential long-term solution by leveraging expert domain knowledge to\nrigorously and efficiently verify LLM outputs."}
{"id": "2505.18232", "pdf": "https://arxiv.org/pdf/2505.18232", "abs": "https://arxiv.org/abs/2505.18232", "authors": ["Mingkuan Feng", "Jinyang Wu", "Siyuan Liu", "Shuai Zhang", "Hongjian Fang", "Ruihan Jin", "Feihu Che", "Pengpeng Shao", "Zhengqi Wen", "Jianhua Tao"], "title": "ELDeR: Getting Efficient LLMs through Data-Driven Regularized Layer-wise Pruning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The deployment of Large language models (LLMs) in many fields is largely\nhindered by their high computational and memory costs. Recent studies suggest\nthat LLMs exhibit sparsity, which can be used for pruning. Previous pruning\nmethods typically follow a prune-then-finetune paradigm. Since the pruned parts\nstill contain valuable information, statically removing them without updating\nthe remaining parameters often results in irreversible performance degradation,\nrequiring costly recovery fine-tuning (RFT) to maintain performance. To address\nthis, we propose a novel paradigm: first apply regularization, then prune.\nBased on this paradigm, we propose ELDeR: Getting Efficient LLMs through\nData-Driven Regularized Layer-wise Pruning. We multiply the output of each\ntransformer layer by an initial weight, then we iteratively learn the weights\nof each transformer layer by using a small amount of data in a simple way.\nAfter that, we apply regularization to the difference between the output and\ninput of the layers with smaller weights, forcing the information to be\ntransferred to the remaining layers. Compared with direct pruning, ELDeR\nreduces the information loss caused by direct parameter removal, thus better\npreserving the model's language modeling ability. Experimental results show\nthat ELDeR achieves superior performance compared with powerful layer-wise\nstructured pruning methods, while greatly reducing RFT computational costs.\nSince ELDeR is a layer-wise pruning method, its end-to-end acceleration effect\nis obvious, making it a promising technique for efficient LLMs."}
{"id": "2505.18426", "pdf": "https://arxiv.org/pdf/2505.18426", "abs": "https://arxiv.org/abs/2505.18426", "authors": ["Khandakar Ashrafi Akbar", "Md Nahiyan Uddin", "Latifur Khan", "Trayce Hockstad", "Mizanur Rahman", "Mashrur Chowdhury", "Bhavani Thuraisingham"], "title": "Retrieval Augmented Generation-based Large Language Models for Bridging Transportation Cybersecurity Legal Knowledge Gaps", "categories": ["cs.CL", "cs.AI"], "comment": "Presented at the Transportation Research Board (TRB) Annual Meeting\n  2025, and subsequently submitted for publication consideration in the\n  Transportation Research Record (TRR)", "summary": "As connected and automated transportation systems evolve, there is a growing\nneed for federal and state authorities to revise existing laws and develop new\nstatutes to address emerging cybersecurity and data privacy challenges. This\nstudy introduces a Retrieval-Augmented Generation (RAG) based Large Language\nModel (LLM) framework designed to support policymakers by extracting relevant\nlegal content and generating accurate, inquiry-specific responses. The\nframework focuses on reducing hallucinations in LLMs by using a curated set of\ndomain-specific questions to guide response generation. By incorporating\nretrieval mechanisms, the system enhances the factual grounding and specificity\nof its outputs. Our analysis shows that the proposed RAG-based LLM outperforms\nleading commercial LLMs across four evaluation metrics: AlignScore, ParaScore,\nBERTScore, and ROUGE, demonstrating its effectiveness in producing reliable and\ncontext-aware legal insights. This approach offers a scalable, AI-driven method\nfor legislative analysis, supporting efforts to update legal frameworks in line\nwith advancements in transportation technologies."}
{"id": "2505.18597", "pdf": "https://arxiv.org/pdf/2505.18597", "abs": "https://arxiv.org/abs/2505.18597", "authors": ["Haojie Wang", "Jiuyun Jiang", "L. Jeff Hong", "Guangxin Jiang"], "title": "LLMs for Supply Chain Management", "categories": ["cs.AI", "cs.LG", "stat.AP"], "comment": null, "summary": "The development of large language models (LLMs) has provided new tools for\nresearch in supply chain management (SCM). In this paper, we introduce a\nretrieval-augmented generation (RAG) framework that dynamically integrates\nexternal knowledge into the inference process, and develop a domain-specialized\nSCM LLM, which demonstrates expert-level competence by passing standardized SCM\nexaminations and beer game tests. We further employ the use of LLMs to conduct\nhorizontal and vertical supply chain games, in order to analyze competition and\ncooperation within supply chains. Our experiments show that RAG significantly\nimproves performance on SCM tasks. Moreover, game-theoretic analysis reveals\nthat the LLM can reproduce insights from the classical SCM literature, while\nalso uncovering novel behaviors and offering fresh perspectives on phenomena\nsuch as the bullwhip effect. This paper opens the door for exploring\ncooperation and competition for complex supply chain network through the lens\nof LLMs."}
{"id": "2505.18233", "pdf": "https://arxiv.org/pdf/2505.18233", "abs": "https://arxiv.org/abs/2505.18233", "authors": ["Shaghayegh Hosseinpour", "Sanchari Das"], "title": "POSTER: A Multi-Signal Model for Detecting Evasive Smishing", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Smishing, or SMS-based phishing, poses an increasing threat to mobile users\nby mimicking legitimate communications through culturally adapted, concise, and\ndeceptive messages, which can result in the loss of sensitive data or financial\nresources. In such, we present a multi-channel smishing detection model that\ncombines country-specific semantic tagging, structural pattern tagging,\ncharacter-level stylistic cues, and contextual phrase embeddings. We curated\nand relabeled over 84,000 messages across five datasets, including 24,086\nsmishing samples. Our unified architecture achieves 97.89% accuracy, an F1\nscore of 0.963, and an AUC of 99.73%, outperforming single-stream models by\ncapturing diverse linguistic and structural cues. This work demonstrates the\neffectiveness of multi-signal learning in robust and region-aware phishing."}
{"id": "2505.18436", "pdf": "https://arxiv.org/pdf/2505.18436", "abs": "https://arxiv.org/abs/2505.18436", "authors": ["AbdelRahim Elmadany", "Sang Yun Kwon", "Hawau Olamide Toyin", "Alcides Alcoba Inciarte", "Hanan Aldarmaki", "Muhammad Abdul-Mageed"], "title": "Voice of a Continent: Mapping Africa's Speech Technology Frontier", "categories": ["cs.CL"], "comment": null, "summary": "Africa's rich linguistic diversity remains significantly underrepresented in\nspeech technologies, creating barriers to digital inclusion. To alleviate this\nchallenge, we systematically map the continent's speech space of datasets and\ntechnologies, leading to a new comprehensive benchmark SimbaBench for\ndownstream African speech tasks. Using SimbaBench, we introduce the Simba\nfamily of models, achieving state-of-the-art performance across multiple\nAfrican languages and speech tasks. Our benchmark analysis reveals critical\npatterns in resource availability, while our model evaluation demonstrates how\ndataset quality, domain diversity, and language family relationships influence\nperformance across languages. Our work highlights the need for expanded speech\ntechnology resources that better reflect Africa's linguistic diversity and\nprovides a solid foundation for future research and development efforts toward\nmore inclusive speech technologies."}
{"id": "2505.18603", "pdf": "https://arxiv.org/pdf/2505.18603", "abs": "https://arxiv.org/abs/2505.18603", "authors": ["Ye Mo", "Zirui Shao", "Kai Ye", "Xianwei Mao", "Bo Zhang", "Hangdi Xing", "Peng Ye", "Gang Huang", "Kehan Chen", "Zhou Huan", "Zixu Yan", "Sheng Zhou"], "title": "Doc-CoB: Enhancing Multi-Modal Document Understanding with Visual Chain-of-Boxes Reasoning", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Multimodal large language models (MLLMs) have made significant progress in\ndocument understanding. However, the information-dense nature of document\nimages still poses challenges, as most queries depend on only a few relevant\nregions, with the rest being redundant. Existing one-pass MLLMs process entire\ndocument images without considering query relevance, often failing to focus on\ncritical regions and producing unfaithful responses. Inspired by the human\ncoarse-to-fine reading pattern, we introduce Doc-CoB (Chain-of-Box), a\nsimple-yet-effective mechanism that integrates human-style visual reasoning\ninto MLLM without modifying its architecture. Our method allows the model to\nautonomously select the set of regions (boxes) most relevant to the query, and\nthen focus attention on them for further understanding. We first design a fully\nautomatic pipeline, integrating a commercial MLLM with a layout analyzer, to\ngenerate 249k training samples with intermediate visual reasoning supervision.\nThen we incorporate two enabling tasks that improve box identification and\nbox-query reasoning, which together enhance document understanding. Extensive\nexperiments on seven benchmarks with four popular models show that Doc-CoB\nsignificantly improves performance, demonstrating its effectiveness and wide\napplicability. All code, data, and models will be released publicly."}
{"id": "2505.18234", "pdf": "https://arxiv.org/pdf/2505.18234", "abs": "https://arxiv.org/abs/2505.18234", "authors": ["Yuanya She"], "title": "A Robust PPO-optimized Tabular Transformer Framework for Intrusion Detection in Industrial IoT Systems", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In this paper, we propose a robust and reinforcement-learning-enhanced\nnetwork intrusion detection system (NIDS) designed for class-imbalanced and\nfew-shot attack scenarios in Industrial Internet of Things (IIoT) environments.\nOur model integrates a TabTransformer for effective tabular feature\nrepresentation with Proximal Policy Optimization (PPO) to optimize\nclassification decisions via policy learning. Evaluated on the\nTON\\textunderscore IoT benchmark, our method achieves a macro F1-score of\n97.73\\% and accuracy of 98.85\\%. Remarkably, even on extremely rare classes\nlike man-in-the-middle (MITM), our model achieves an F1-score of 88.79\\%,\nshowcasing strong robustness and few-shot detection capabilities. Extensive\nablation experiments confirm the complementary roles of TabTransformer and PPO\nin mitigating class imbalance and improving generalization. These results\nhighlight the potential of combining transformer-based tabular learning with\nreinforcement learning for real-world NIDS applications."}
{"id": "2505.18440", "pdf": "https://arxiv.org/pdf/2505.18440", "abs": "https://arxiv.org/abs/2505.18440", "authors": ["Zhaoyang Wang", "Jinqi Jiang", "Tian Qiu", "Hui Liu", "Xianfeng Tang", "Huaxiu Yao"], "title": "Efficient Long CoT Reasoning in Small Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent large reasoning models such as DeepSeek-R1 exhibit strong complex\nproblems solving abilities by generating long chain-of-thought (CoT) reasoning\nsteps. It is challenging to directly train small language models (SLMs) to\nemerge long CoT. Thus, distillation becomes a practical method to enable SLMs\nfor such reasoning ability. However, the long CoT often contains a lot of\nredundant contents (e.g., overthinking steps) which may make SLMs hard to learn\nconsidering their relatively poor capacity and generalization. To address this\nissue, we propose a simple-yet-effective method to prune unnecessary steps in\nlong CoT, and then employ an on-policy method for the SLM itself to curate\nvalid and useful long CoT training data. In this way, SLMs can effectively\nlearn efficient long CoT reasoning and preserve competitive performance at the\nsame time. Experimental results across a series of mathematical reasoning\nbenchmarks demonstrate the effectiveness of the proposed method in distilling\nlong CoT reasoning ability into SLMs which maintains the competitive\nperformance but significantly reduces generating redundant reasoning steps."}
{"id": "2505.18607", "pdf": "https://arxiv.org/pdf/2505.18607", "abs": "https://arxiv.org/abs/2505.18607", "authors": ["Jonathan Leung", "Yongjie Wang", "Zhiqi Shen"], "title": "Knowledge Retrieval in LLM Gaming: A Shift from Entity-Centric to Goal-Oriented Graphs", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate impressive general capabilities but\noften struggle with step-by-step reasoning, especially in complex applications\nsuch as games. While retrieval-augmented methods like GraphRAG attempt to\nbridge this gap through cross-document extraction and indexing, their\nfragmented entity-relation graphs and overly dense local connectivity hinder\nthe construction of coherent reasoning. In this paper, we propose a novel\nframework based on Goal-Oriented Graphs (GoGs), where each node represents a\ngoal and its associated attributes, and edges encode logical dependencies\nbetween goals. This structure enables explicit retrieval of reasoning paths by\nfirst identifying high-level goals and recursively retrieving their subgoals,\nforming coherent reasoning chains to guide LLM prompting. Our method\nsignificantly enhances the reasoning ability of LLMs in game-playing tasks, as\ndemonstrated by extensive experiments on the Minecraft testbed, outperforming\nGraphRAG and other baselines."}
{"id": "2505.18235", "pdf": "https://arxiv.org/pdf/2505.18235", "abs": "https://arxiv.org/abs/2505.18235", "authors": ["Alexander Modell", "Patrick Rubin-Delanchy", "Nick Whiteley"], "title": "The Origins of Representation Manifolds in Large Language Models", "categories": ["cs.LG", "cs.AI", "68T07", "I.2.7"], "comment": "16 pages, 4 figures", "summary": "There is a large ongoing scientific effort in mechanistic interpretability to\nmap embeddings and internal representations of AI systems into\nhuman-understandable concepts. A key element of this effort is the linear\nrepresentation hypothesis, which posits that neural representations are sparse\nlinear combinations of `almost-orthogonal' direction vectors, reflecting the\npresence or absence of different features. This model underpins the use of\nsparse autoencoders to recover features from representations. Moving towards a\nfuller model of features, in which neural representations could encode not just\nthe presence but also a potentially continuous and multidimensional value for a\nfeature, has been a subject of intense recent discourse. We describe why and\nhow a feature might be represented as a manifold, demonstrating in particular\nthat cosine similarity in representation space may encode the intrinsic\ngeometry of a feature through shortest, on-manifold paths, potentially\nanswering the question of how distance in representation space and relatedness\nin concept space could be connected. The critical assumptions and predictions\nof the theory are validated on text embeddings and token activations of large\nlanguage models."}
{"id": "2505.18450", "pdf": "https://arxiv.org/pdf/2505.18450", "abs": "https://arxiv.org/abs/2505.18450", "authors": ["Ainulla Khan", "Yamada Moyuru", "Srinidhi Akella"], "title": "BRIT: Bidirectional Retrieval over Unified Image-Text Graph", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a promising technique to\nenhance the quality and relevance of responses generated by large language\nmodels. While recent advancements have mainly focused on improving RAG for\ntext-based queries, RAG on multi-modal documents containing both texts and\nimages has not been fully explored. Especially when fine-tuning does not work.\nThis paper proposes BRIT, a novel multi-modal RAG framework that effectively\nunifies various text-image connections in the document into a multi-modal graph\nand retrieves the texts and images as a query-specific sub-graph. By traversing\nboth image-to-text and text-to-image paths in the graph, BRIT retrieve not only\ndirectly query-relevant images and texts but also further relevant contents to\nanswering complex cross-modal multi-hop questions. To evaluate the\neffectiveness of BRIT, we introduce MM-RAG test set specifically designed for\nmulti-modal question answering tasks that require to understand the text-image\nrelations. Our comprehensive experiments demonstrate the superiority of BRIT,\nhighlighting its ability to handle cross-modal questions on the multi-modal\ndocuments."}
{"id": "2505.18623", "pdf": "https://arxiv.org/pdf/2505.18623", "abs": "https://arxiv.org/abs/2505.18623", "authors": ["Lucas Saldyt", "Subbarao Kambhampati"], "title": "Mind The Gap: Deep Learning Doesn't Learn Deeply", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "This paper aims to understand how neural networks learn algorithmic reasoning\nby addressing two questions: How faithful are learned algorithms when they are\neffective, and why do neural networks fail to learn effective algorithms\notherwise? To answer these questions, we use neural compilation, a technique\nthat directly encodes a source algorithm into neural network parameters,\nenabling the network to compute the algorithm exactly. This enables comparison\nbetween compiled and conventionally learned parameters, intermediate vectors,\nand behaviors. This investigation is crucial for developing neural networks\nthat robustly learn complexalgorithms from data. Our analysis focuses on graph\nneural networks (GNNs), which are naturally aligned with algorithmic reasoning\ntasks, specifically our choices of BFS, DFS, and Bellman-Ford, which cover the\nspectrum of effective, faithful, and ineffective learned algorithms. Commonly,\nlearning algorithmic reasoning is framed as induction over synthetic data,\nwhere a parameterized model is trained on inputs, traces, and outputs produced\nby an underlying ground truth algorithm. In contrast, we introduce a neural\ncompilation method for GNNs, which sets network parameters analytically,\nbypassing training. Focusing on GNNs leverages their alignment with algorithmic\nreasoning, extensive algorithmic induction literature, and the novel\napplication of neural compilation to GNNs. Overall, this paper aims to\ncharacterize expressability-trainability gaps - a fundamental shortcoming in\nlearning algorithmic reasoning. We hypothesize that inductive learning is most\neffective for parallel algorithms contained within the computational class\n\\texttt{NC}."}
{"id": "2505.18245", "pdf": "https://arxiv.org/pdf/2505.18245", "abs": "https://arxiv.org/abs/2505.18245", "authors": ["Roy Elkayam"], "title": "Decomposition of Water Demand Patterns Using Skewed Gaussian Distributions for Behavioral Insights and Operational Planning", "categories": ["cs.LG"], "comment": null, "summary": "This study presents a novel approach for decomposing urban water demand\npatterns using Skewed Gaussian Distributions (SGD) to derive behavioral\ninsights and support operational planning. Hourly demand profiles contain\ncritical information for both long-term infrastructure design and daily\noperations, influencing network pressures, water quality, energy consumption,\nand overall reliability. By breaking down each daily demand curve into a\nbaseline component and distinct peak components, the proposed SGD method\ncharacterizes each peak with interpretable parameters, including peak\namplitude, timing (mean), spread (duration), and skewness (asymmetry), thereby\nreconstructing the observed pattern and uncovering latent usage dynamics. This\ndetailed peak-level decomposition enables both operational applications, e.g.\nanomaly and leakage detection, real-time demand management, and strategic\nanalyses, e.g. identifying behavioral shifts, seasonal influences, or policy\nimpacts on consumption patterns. Unlike traditional symmetric Gaussian or\npurely statistical time-series models, SGDs explicitly capture asymmetric peak\nshapes such as sharp morning surges followed by gradual declines, improving the\nfidelity of synthetic pattern generation and enhancing the detection of\nirregular consumption behavior. The method is demonstrated on several\nreal-world datasets, showing that SGD outperforms symmetric Gaussian models in\nreconstruction accuracy, reducing root-mean-square error by over 50% on\naverage, while maintaining physical interpretability. The SGD framework can\nalso be used to construct synthetic demand scenarios by designing daily peak\nprofiles with chosen characteristics. All implementation code is publicly\navailable at: https://github.com/Relkayam/water-demand-decomposition-sgd"}
{"id": "2505.18452", "pdf": "https://arxiv.org/pdf/2505.18452", "abs": "https://arxiv.org/abs/2505.18452", "authors": ["Heyuan Huang", "Alexandra DeLucia", "Vijay Murari Tiyyala", "Mark Dredze"], "title": "MedScore: Factuality Evaluation of Free-Form Medical Answers", "categories": ["cs.CL"], "comment": null, "summary": "While Large Language Models (LLMs) can generate fluent and convincing\nresponses, they are not necessarily correct. This is especially apparent in the\npopular decompose-then-verify factuality evaluation pipeline, where LLMs\nevaluate generations by decomposing the generations into individual, valid\nclaims. Factuality evaluation is especially important for medical answers,\nsince incorrect medical information could seriously harm the patient. However,\nexisting factuality systems are a poor match for the medical domain, as they\nare typically only evaluated on objective, entity-centric, formulaic texts such\nas biographies and historical topics. This differs from condition-dependent,\nconversational, hypothetical, sentence-structure diverse, and subjective\nmedical answers, which makes decomposition into valid facts challenging. We\npropose MedScore, a new approach to decomposing medical answers into\ncondition-aware valid facts. Our method extracts up to three times more valid\nfacts than existing methods, reducing hallucination and vague references, and\nretaining condition-dependency in facts. The resulting factuality score\nsignificantly varies by decomposition method, verification corpus, and used\nbackbone LLM, highlighting the importance of customizing each step for reliable\nfactuality evaluation."}
{"id": "2505.18645", "pdf": "https://arxiv.org/pdf/2505.18645", "abs": "https://arxiv.org/abs/2505.18645", "authors": ["Haleema Bibi", "Sadia Saleem", "Zakia Jalil", "Muhammad Nasir", "Tahani Alsubait"], "title": "Riverine Flood Prediction and Early Warning in Mountainous Regions using Artificial Intelligence", "categories": ["cs.AI"], "comment": "26 pages, 6 figure", "summary": "Flooding is the most devastating phenomenon occurring globally, particularly\nin mountainous regions, risk dramatically increases due to complex terrains and\nextreme climate changes. These situations are damaging livelihoods,\nagriculture, infrastructure, and human lives. This study uses the Kabul River\nbetween Pakistan and Afghanistan as a case study to reflect the complications\nof flood forecasting in transboundary basins. The challenges in obtaining\nupstream data impede the efficacy of flood control measures and early warning\nsystems, a common global problem in similar basins. Utilizing satellite-based\nclimatic data, this study applied numerous advanced machine-learning and deep\nlearning models, such as Support Vector Machines (SVM), XGBoost, and Artificial\nNeural Networks (ANN), Long Short-Term Memory (LSTM) networks, and Gated\nRecurrent Units (GRU) to predict daily and multi-step river flow. The LSTM\nnetwork outperformed other models, achieving the highest R2 value of 0.96 and\nthe lowest RMSE value of 140.96 m3/sec. The time series LSTM and GRU network\nmodels, utilized for short-term forecasts of up to five days, performed\nsignificantly. However, the accuracy declined beyond the fourth day,\nhighlighting the need for longer-term historical datasets for reliable\nlong-term flood predictions. The results of the study are directly aligned with\nSustainable Development Goals 6, 11, 13, and 15, facilitating disaster and\nwater management, timely evacuations, improved preparedness, and effective\nearly warning."}
{"id": "2505.18266", "pdf": "https://arxiv.org/pdf/2505.18266", "abs": "https://arxiv.org/abs/2505.18266", "authors": ["Gavin McCracken", "Gabriela Moisescu-Pareja", "Vincent Letourneau", "Doina Precup", "Jonathan Love"], "title": "Uncovering a Universal Abstract Algorithm for Modular Addition in Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose a testable universality hypothesis, asserting that seemingly\ndisparate neural network solutions observed in the simple task of modular\naddition are unified under a common abstract algorithm. While prior work\ninterpreted variations in neuron-level representations as evidence for distinct\nalgorithms, we demonstrate - through multi-level analyses spanning neurons,\nneuron clusters, and entire networks - that multilayer perceptrons and\ntransformers universally implement the abstract algorithm we call the\napproximate Chinese Remainder Theorem. Crucially, we introduce approximate\ncosets and show that neurons activate exclusively on them. Furthermore, our\ntheory works for deep neural networks (DNNs). It predicts that universally\nlearned solutions in DNNs with trainable embeddings or more than one hidden\nlayer require only O(log n) features, a result we empirically confirm. This\nwork thus provides the first theory-backed interpretation of multilayer\nnetworks solving modular addition. It advances generalizable interpretability\nand opens a testable universality hypothesis for group multiplication beyond\nmodular addition."}
{"id": "2505.18454", "pdf": "https://arxiv.org/pdf/2505.18454", "abs": "https://arxiv.org/abs/2505.18454", "authors": ["Zhenrui Yue", "Bowen Jin", "Huimin Zeng", "Honglei Zhuang", "Zhen Qin", "Jinsung Yoon", "Lanyu Shang", "Jiawei Han", "Dong Wang"], "title": "Hybrid Latent Reasoning via Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have introduced latent\nreasoning as a promising alternative to autoregressive reasoning. By performing\ninternal computation with hidden states from previous steps, latent reasoning\nbenefit from more informative features rather than sampling a discrete\nchain-of-thought (CoT) path. Yet latent reasoning approaches are often\nincompatible with LLMs, as their continuous paradigm conflicts with the\ndiscrete nature of autoregressive generation. Moreover, these methods rely on\nCoT traces for training and thus fail to exploit the inherent reasoning\npatterns of LLMs. In this work, we explore latent reasoning by leveraging the\nintrinsic capabilities of LLMs via reinforcement learning (RL). To this end, we\nintroduce hybrid reasoning policy optimization (HRPO), an RL-based hybrid\nlatent reasoning approach that (1) integrates prior hidden states into sampled\ntokens with a learnable gating mechanism, and (2) initializes training with\npredominantly token embeddings while progressively incorporating more hidden\nfeatures. This design maintains LLMs' generative capabilities and incentivizes\nhybrid reasoning using both discrete and continuous representations. In\naddition, the hybrid HRPO introduces stochasticity into latent reasoning via\ntoken sampling, thereby enabling RL-based optimization without requiring CoT\ntrajectories. Extensive evaluations across diverse benchmarks show that HRPO\noutperforms prior methods in both knowledge- and reasoning-intensive tasks.\nFurthermore, HRPO-trained LLMs remain interpretable and exhibit intriguing\nbehaviors like cross-lingual patterns and shorter completion lengths,\nhighlighting the potential of our RL-based approach and offer insights for\nfuture work in latent reasoning."}
{"id": "2505.18657", "pdf": "https://arxiv.org/pdf/2505.18657", "abs": "https://arxiv.org/abs/2505.18657", "authors": ["Xu Zheng", "Chenfei Liao", "Yuqian Fu", "Kaiyu Lei", "Yuanhuiyi Lyu", "Lutao Jiang", "Bin Ren", "Jialei Chen", "Jiawen Wang", "Chengxin Li", "Linfeng Zhang", "Danda Pani Paudel", "Xuanjing Huang", "Yu-Gang Jiang", "Nicu Sebe", "Dacheng Tao", "Luc Van Gool", "Xuming Hu"], "title": "MLLMs are Deeply Affected by Modality Bias", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have shown\npromising results in integrating diverse modalities such as texts and images.\nMLLMs are heavily influenced by modality bias, often relying on language while\nunder-utilizing other modalities like visual inputs. This position paper argues\nthat MLLMs are deeply affected by modality bias. Firstly, we diagnose the\ncurrent state of modality bias, highlighting its manifestations across various\ntasks. Secondly, we propose a systematic research road-map related to modality\nbias in MLLMs. Thirdly, we identify key factors of modality bias in MLLMs and\noffer actionable suggestions for future research to mitigate it. To\nsubstantiate these findings, we conduct experiments that demonstrate the\ninfluence of each factor: 1. Data Characteristics: Language data is compact and\nabstract, while visual data is redundant and complex, creating an inherent\nimbalance in learning dynamics. 2. Imbalanced Backbone Capabilities: The\ndominance of pretrained language models in MLLMs leads to overreliance on\nlanguage and neglect of visual information. 3. Training Objectives: Current\nobjectives often fail to promote balanced cross-modal alignment, resulting in\nshortcut learning biased toward language. These findings highlight the need for\nbalanced training strategies and model architectures to better integrate\nmultiple modalities in MLLMs. We call for interdisciplinary efforts to tackle\nthese challenges and drive innovation in MLLM research. Our work provides a\nfresh perspective on modality bias in MLLMs and offers insights for developing\nmore robust and generalizable multimodal systems-advancing progress toward\nArtificial General Intelligence."}
{"id": "2505.18269", "pdf": "https://arxiv.org/pdf/2505.18269", "abs": "https://arxiv.org/abs/2505.18269", "authors": ["Quan Zhou", "Mark Kozdoba", "Shie Mannor"], "title": "Representative Action Selection for Large Action-Space Meta-Bandits", "categories": ["cs.LG", "math.OC", "math.PR", "stat.ML"], "comment": null, "summary": "We study the problem of selecting a subset from a large action space shared\nby a family of bandits, with the goal of achieving performance nearly matching\nthat of using the full action space. We assume that similar actions tend to\nhave related payoffs, modeled by a Gaussian process. To exploit this structure,\nwe propose a simple epsilon-net algorithm to select a representative subset. We\nprovide theoretical guarantees for its performance and compare it empirically\nto Thompson Sampling and Upper Confidence Bound."}
{"id": "2505.18456", "pdf": "https://arxiv.org/pdf/2505.18456", "abs": "https://arxiv.org/abs/2505.18456", "authors": ["Litu Rout", "Constantine Caramanis", "Sanjay Shakkottai"], "title": "Anchored Diffusion Language Model", "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "Diffusion Language Models (DLMs) promise parallel generation and\nbidirectional context, yet they underperform autoregressive (AR) models in both\nlikelihood modeling and generated text quality. We identify that this\nperformance gap arises when important tokens (e.g., key words or low-frequency\nwords that anchor a sentence) are masked early in the forward process, limiting\ncontextual information for accurate reconstruction. To address this, we\nintroduce the Anchored Diffusion Language Model (ADLM), a novel two-stage\nframework that first predicts distributions over important tokens via an anchor\nnetwork, and then predicts the likelihoods of missing tokens conditioned on the\nanchored predictions. ADLM significantly improves test perplexity on LM1B and\nOpenWebText, achieving up to 25.4% gains over prior DLMs, and narrows the gap\nwith strong AR baselines. It also achieves state-of-the-art performance in\nzero-shot generalization across seven benchmarks and surpasses AR models in\nMAUVE score, which marks the first time a DLM generates better human-like text\nthan an AR model. Theoretically, we derive an Anchored Negative Evidence Lower\nBound (ANELBO) objective and show that anchoring improves sample complexity and\nlikelihood modeling. Beyond diffusion, anchoring boosts performance in AR\nmodels and enhances reasoning in math and logic tasks, outperforming existing\nchain-of-thought approaches"}
{"id": "2505.18670", "pdf": "https://arxiv.org/pdf/2505.18670", "abs": "https://arxiv.org/abs/2505.18670", "authors": ["Chonghua Han", "Yuan Yuan", "Kaiyan Chen", "Jingtao Ding", "Yong Li"], "title": "TrajMoE: Spatially-Aware Mixture of Experts for Unified Human Mobility Modeling", "categories": ["cs.AI"], "comment": null, "summary": "Modeling human mobility across diverse cities is essential for applications\nsuch as urban planning, transportation optimization, and personalized services.\nHowever, generalization remains challenging due to heterogeneous spatial\nrepresentations and mobility patterns across cities. Existing methods typically\nrely on numerical coordinates or require training city-specific models,\nlimiting their scalability and transferability. We propose TrajMoE, a unified\nand scalable model for cross-city human mobility modeling. TrajMoE addresses\ntwo key challenges: (1) inconsistent spatial semantics across cities, and (2)\ndiverse urban mobility patterns. To tackle these, we begin by designing a\nspatial semantic encoder that learns transferable location representations from\nPOI-based functional semantics and visit patterns. Furthermore, we design a\nSpatially-Aware Mixture-of-Experts (SAMoE) Transformer that injects structured\npriors into experts specialized in distinct mobility semantics, along with a\nshared expert to capture city-invariant patterns and enable adaptive cross-city\ngeneralization. Extensive experiments demonstrate that TrajMoE achieves up to\n27% relative improvement over competitive mobility foundation models after only\none epoch of fine-tuning, and consistently outperforms full-data baselines\nusing merely 5% of target city data. These results establish TrajMoE as a\nsignificant step toward realizing a truly generalizable, transferable, and\npretrainable foundation model for human mobility."}
{"id": "2505.18280", "pdf": "https://arxiv.org/pdf/2505.18280", "abs": "https://arxiv.org/abs/2505.18280", "authors": ["Tsai Hor Chan", "Dora Yan Zhang", "Guosheng Yin", "Lequan Yu"], "title": "Feature Preserving Shrinkage on Bayesian Neural Networks via the R2D2 Prior", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "To appear in TPAMI", "summary": "Bayesian neural networks (BNNs) treat neural network weights as random\nvariables, which aim to provide posterior uncertainty estimates and avoid\noverfitting by performing inference on the posterior weights. However, the\nselection of appropriate prior distributions remains a challenging task, and\nBNNs may suffer from catastrophic inflated variance or poor predictive\nperformance when poor choices are made for the priors. Existing BNN designs\napply different priors to weights, while the behaviours of these priors make it\ndifficult to sufficiently shrink noisy signals or they are prone to\novershrinking important signals in the weights. To alleviate this problem, we\npropose a novel R2D2-Net, which imposes the R^2-induced Dirichlet Decomposition\n(R2D2) prior to the BNN weights. The R2D2-Net can effectively shrink irrelevant\ncoefficients towards zero, while preventing key features from over-shrinkage.\nTo approximate the posterior distribution of weights more accurately, we\nfurther propose a variational Gibbs inference algorithm that combines the Gibbs\nupdating procedure and gradient-based optimization. This strategy enhances\nstability and consistency in estimation when the variational objective\ninvolving the shrinkage parameters is non-convex. We also analyze the evidence\nlower bound (ELBO) and the posterior concentration rates from a theoretical\nperspective. Experiments on both natural and medical image classification and\nuncertainty estimation tasks demonstrate satisfactory performance of our\nmethod."}
{"id": "2505.18466", "pdf": "https://arxiv.org/pdf/2505.18466", "abs": "https://arxiv.org/abs/2505.18466", "authors": ["Mamnuya Rinki", "Chahat Raj", "Anjishnu Mukherjee", "Ziwei Zhu"], "title": "Measuring South Asian Biases in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Evaluations of Large Language Models (LLMs) often overlook intersectional and\nculturally specific biases, particularly in underrepresented multilingual\nregions like South Asia. This work addresses these gaps by conducting a\nmultilingual and intersectional analysis of LLM outputs across 10 Indo-Aryan\nand Dravidian languages, identifying how cultural stigmas influenced by purdah\nand patriarchy are reinforced in generative tasks. We construct a culturally\ngrounded bias lexicon capturing previously unexplored intersectional dimensions\nincluding gender, religion, marital status, and number of children. We use our\nlexicon to quantify intersectional bias and the effectiveness of self-debiasing\nin open-ended generations (e.g., storytelling, hobbies, and to-do lists), where\nbias manifests subtly and remains largely unexamined in multilingual contexts.\nFinally, we evaluate two self-debiasing strategies (simple and complex prompts)\nto measure their effectiveness in reducing culturally specific bias in\nIndo-Aryan and Dravidian languages. Our approach offers a nuanced lens into\ncultural bias by introducing a novel bias lexicon and evaluation framework that\nextends beyond Eurocentric or small-scale multilingual settings."}
{"id": "2505.18694", "pdf": "https://arxiv.org/pdf/2505.18694", "abs": "https://arxiv.org/abs/2505.18694", "authors": ["Rafiu Adekoya Badekale", "Adewale Akinfaderin"], "title": "AI-Driven Climate Policy Scenario Generation for Sub-Saharan Africa", "categories": ["cs.AI"], "comment": "13 pages long. Extended version of the paper titled \"Climate Policy\n  Simulation and Scenario Generation for Sub-Saharan Africa\" accepted at the\n  13th Data Science Africa Workshop (DSA 2025). Code available at\n  https://github.com/AdeTheBade/CPSG.git", "summary": "Climate policy scenario generation and evaluation have traditionally relied\non integrated assessment models (IAMs) and expert-driven qualitative analysis.\nThese methods enable stakeholders, such as policymakers and researchers, to\nanticipate impacts, plan governance strategies, and develop mitigation\nmeasures. However, traditional methods are often time-intensive, reliant on\nsimple extrapolations of past trends, and limited in capturing the complex and\ninterconnected nature of energy and climate issues. With the advent of\nartificial intelligence (AI), particularly generative AI models trained on vast\ndatasets, these limitations can be addressed, ensuring robustness even under\nlimited data conditions. In this work, we explore the novel method that employs\ngenerative AI, specifically large language models (LLMs), to simulate climate\npolicy scenarios for Sub-Saharan Africa. These scenarios focus on energy\ntransition themes derived from the historical United Nations Climate Change\nConference (COP) documents. By leveraging generative models, the project aims\nto create plausible and diverse policy scenarios that align with regional\nclimate goals and energy challenges. Given limited access to human evaluators,\nautomated techniques were employed for scenario evaluation. We generated policy\nscenarios using the llama3.2-3B model. Of the 34 generated responses, 30 (88%)\npassed expert validation, accurately reflecting the intended impacts provided\nin the corresponding prompts. We compared these validated responses against\nassessments from a human climate expert and two additional LLMs (gemma2-2B and\nmistral-7B). Our structured, embedding-based evaluation framework shows that\ngenerative AI effectively generate scenarios that are coherent, relevant,\nplausible, and diverse. This approach offers a transformative tool for climate\npolicy planning in data-constrained regions."}
{"id": "2505.18284", "pdf": "https://arxiv.org/pdf/2505.18284", "abs": "https://arxiv.org/abs/2505.18284", "authors": ["Pritam Anand", "Aadesh Minz", "Asish Joel"], "title": "Tube Loss based Deep Networks For Improving the Probabilistic Forecasting of Wind Speed", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Uncertainty Quantification (UQ) in wind speed forecasting is a critical\nchallenge in wind power production due to the inherently volatile nature of\nwind. By quantifying the associated risks and returns, UQ supports more\neffective decision-making for grid operations and participation in the\nelectricity market. In this paper, we design a sequence of deep learning based\nprobabilistic forecasting methods by using the Tube loss function for wind\nspeed forecasting. The Tube loss function is a simple and model agnostic\nPrediction Interval (PI) estimation approach and can obtain the narrow PI with\nasymptotical coverage guarantees without any distribution assumption. Our deep\nprobabilistic forecasting models effectively incorporate popular architectures\nsuch as LSTM, GRU, and TCN within the Tube loss framework. We further design a\nsimple yet effective heuristic for tuning the $\\delta$ parameter of the Tube\nloss function so that our deep forecasting models obtain the narrower PI\nwithout compromising its calibration ability. We have considered three wind\ndatasets, containing the hourly recording of the wind speed, collected from\nthree distinct location namely Jaisalmer, Los Angeles and San Fransico. Our\nnumerical results demonstrate that the proposed deep forecasting models produce\nmore reliable and narrower PIs compared to recently developed probabilistic\nwind forecasting methods."}
{"id": "2505.18486", "pdf": "https://arxiv.org/pdf/2505.18486", "abs": "https://arxiv.org/abs/2505.18486", "authors": ["Hong Jiao", "Dan Song", "Won-Chan Lee"], "title": "Investigating AI Rater Effects of Large Language Models: GPT, Claude, Gemini, and DeepSeek", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have been widely explored for automated scoring\nin low-stakes assessment to facilitate learning and instruction. Empirical\nevidence related to which LLM produces the most reliable scores and induces\nleast rater effects needs to be collected before the use of LLMs for automated\nscoring in practice. This study compared ten LLMs (ChatGPT 3.5, ChatGPT 4,\nChatGPT 4o, OpenAI o1, Claude 3.5 Sonnet, Gemini 1.5, Gemini 1.5 Pro, Gemini\n2.0, as well as DeepSeek V3, and DeepSeek R1) with human expert raters in\nscoring two types of writing tasks. The accuracy of the holistic and analytic\nscores from LLMs compared with human raters was evaluated in terms of Quadratic\nWeighted Kappa. Intra-rater consistency across prompts was compared in terms of\nCronbach Alpha. Rater effects of LLMs were evaluated and compared with human\nraters using the Many-Facet Rasch model. The results in general supported the\nuse of ChatGPT 4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet with high scoring\naccuracy, better rater reliability, and less rater effects."}
{"id": "2505.18695", "pdf": "https://arxiv.org/pdf/2505.18695", "abs": "https://arxiv.org/abs/2505.18695", "authors": ["Yu Han", "Aaron Ceross", "Jeroen H. M. Bergmann"], "title": "AI for Regulatory Affairs: Balancing Accuracy, Interpretability, and Computational Cost in Medical Device Classification", "categories": ["cs.AI"], "comment": null, "summary": "Regulatory affairs, which sits at the intersection of medicine and law, can\nbenefit significantly from AI-enabled automation. Classification task is the\ninitial step in which manufacturers position their products to regulatory\nauthorities, and it plays a critical role in determining market access,\nregulatory scrutiny, and ultimately, patient safety. In this study, we\ninvestigate a broad range of AI models -- including traditional machine\nlearning (ML) algorithms, deep learning architectures, and large language\nmodels -- using a regulatory dataset of medical device descriptions. We\nevaluate each model along three key dimensions: accuracy, interpretability, and\ncomputational cost."}
{"id": "2505.18289", "pdf": "https://arxiv.org/pdf/2505.18289", "abs": "https://arxiv.org/abs/2505.18289", "authors": ["Saar Cohen", "Noa Agmon", "Uri Shaham"], "title": "Convexified Message-Passing Graph Neural Networks", "categories": ["cs.LG"], "comment": "31 pages, 4 figures", "summary": "Graph Neural Networks (GNNs) have become prominent methods for graph\nrepresentation learning, demonstrating strong empirical results on diverse\ngraph prediction tasks. In this paper, we introduce Convexified Message Passing\nGraph Neural Networks (CGNNs), a novel and general framework that combines the\npower of message-passing GNNs with the tractability of convex optimization. By\nmapping their nonlinear filters into a reproducing kernel Hilbert space, CGNNs\ntransform training into a convex optimization problem, which can be solved\nefficiently and optimally by projected gradient methods. This convexity further\nallows the statistical properties of CGNNs to be analyzed accurately and\nrigorously. For two-layer CGNNs, we establish rigorous generalization\nguarantees, showing convergence to the performance of the optimal GNN. To scale\nto deeper architectures, we adopt a principled layer-wise training strategy.\nExperiments on benchmark datasets show that CGNNs significantly exceed the\nperformance of leading GNN models, achieving 10 to 40 percent higher accuracy\nin most cases, underscoring their promise as a powerful and principled method\nwith strong theoretical foundations. In rare cases where improvements are not\nquantitatively substantial, the convex models either slightly exceed or match\nthe baselines, stressing their robustness and wide applicability. Though\nover-parameterization is often employed to enhance performance in nonconvex\nmodels, we show that our CGNNs framework yields shallow convex models that can\nsurpass these models in both accuracy and resource efficiency."}
{"id": "2505.18497", "pdf": "https://arxiv.org/pdf/2505.18497", "abs": "https://arxiv.org/abs/2505.18497", "authors": ["Kefan Yu", "Qingcheng Zeng", "Weihao Xuan", "Wanxin Li", "Jingyi Wu", "Rob Voigt"], "title": "The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic Competence in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Current large language models (LLMs) have demonstrated emerging capabilities\nin social intelligence tasks, including implicature resolution (Sravanthi et\nal. (2024)) and theory-of-mind reasoning (Shapira et al. (2024)), both of which\nrequire substantial pragmatic understanding. However, how LLMs acquire this\ncompetence throughout the training process remains poorly understood. In this\nwork, we introduce ALTPRAG, a dataset grounded in the pragmatic concept of\nalternatives, designed to evaluate whether LLMs at different training stages\ncan accurately infer nuanced speaker intentions. Each instance pairs two\ncontextually appropriate but pragmatically distinct continuations, enabling\nfine-grained assessment of both pragmatic interpretation and contrastive\nreasoning. We systematically evaluate 22 LLMs across key training stages:\npre-training, supervised fine-tuning (SFT), and preference optimization, to\nexamine the development of pragmatic competence. Our results show that even\nbase models exhibit notable sensitivity to pragmatic cues, which improves\nconsistently with increases in model and data scale. Additionally, SFT and RLHF\ncontribute further gains, particularly in cognitive-pragmatic reasoning. These\nfindings highlight pragmatic competence as an emergent and compositional\nproperty of LLM training and offer new insights for aligning models with human\ncommunicative norms."}
{"id": "2505.18705", "pdf": "https://arxiv.org/pdf/2505.18705", "abs": "https://arxiv.org/abs/2505.18705", "authors": ["Jiabin Tang", "Lianghao Xia", "Zhonghang Li", "Chao Huang"], "title": "AI-Researcher: Autonomous Scientific Innovation", "categories": ["cs.AI"], "comment": "Code on github: https://github.com/HKUDS/AI-Researcher", "summary": "The powerful reasoning capabilities of Large Language Models (LLMs) in\nmathematics and coding, combined with their ability to automate complex tasks\nthrough agentic frameworks, present unprecedented opportunities for\naccelerating scientific innovation. In this paper, we introduce AI-Researcher,\na fully autonomous research system that transforms how AI-driven scientific\ndiscovery is conducted and evaluated. Our framework seamlessly orchestrates the\ncomplete research pipeline--from literature review and hypothesis generation to\nalgorithm implementation and publication-ready manuscript preparation--with\nminimal human intervention. To rigorously assess autonomous research\ncapabilities, we develop Scientist-Bench, a comprehensive benchmark comprising\nstate-of-the-art papers across diverse AI research domains, featuring both\nguided innovation and open-ended exploration tasks. Through extensive\nexperiments, we demonstrate that AI-Researcher achieves remarkable\nimplementation success rates and produces research papers that approach\nhuman-level quality. This work establishes new foundations for autonomous\nscientific innovation that can complement human researchers by systematically\nexploring solution spaces beyond cognitive limitations."}
{"id": "2505.18300", "pdf": "https://arxiv.org/pdf/2505.18300", "abs": "https://arxiv.org/abs/2505.18300", "authors": ["Jie Hu", "Yi-Ting Ma", "Do Young Eun"], "title": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient Nonlinear MCMC on General Graphs", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted at ICML 2025 (Spotlight)", "summary": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs."}
{"id": "2505.18522", "pdf": "https://arxiv.org/pdf/2505.18522", "abs": "https://arxiv.org/abs/2505.18522", "authors": ["Xin Lu", "Yanyan Zhao", "Si Wei", "Shijin Wang", "Bing Qin", "Ting Liu"], "title": "How Does Sequence Modeling Architecture Influence Base Capabilities of Pre-trained Language Models? Exploring Key Architecture Design Principles to Avoid Base Capabilities Degradation", "categories": ["cs.CL"], "comment": null, "summary": "Pre-trained language models represented by the Transformer have been proven\nto possess strong base capabilities, and the representative self-attention\nmechanism in the Transformer has become a classic in sequence modeling\narchitectures. Different from the work of proposing sequence modeling\narchitecture to improve the efficiency of attention mechanism, this work\nfocuses on the impact of sequence modeling architectures on base capabilities.\nSpecifically, our concern is: How exactly do sequence modeling architectures\naffect the base capabilities of pre-trained language models? In this work, we\nfirst point out that the mixed domain pre-training setting commonly adopted in\nexisting architecture design works fails to adequately reveal the differences\nin base capabilities among various architectures. To address this, we propose a\nlimited domain pre-training setting with out-of-distribution testing, which\nsuccessfully uncovers significant differences in base capabilities among\narchitectures at an early stage. Next, we analyze the base capabilities of\nstateful sequence modeling architectures, and find that they exhibit\nsignificant degradation in base capabilities compared to the Transformer. Then,\nthrough a series of architecture component analysis, we summarize a key\narchitecture design principle: A sequence modeling architecture need possess\nfull-sequence arbitrary selection capability to avoid degradation in base\ncapabilities. Finally, we empirically validate this principle using an\nextremely simple Top-1 element selection architecture and further generalize it\nto a more practical Top-1 chunk selection architecture. Experimental results\ndemonstrate our proposed sequence modeling architecture design principle and\nsuggest that our work can serve as a valuable reference for future architecture\nimprovements and novel designs."}
{"id": "2505.18746", "pdf": "https://arxiv.org/pdf/2505.18746", "abs": "https://arxiv.org/abs/2505.18746", "authors": ["Peijie Yu", "Yifan Yang", "Jinjian Li", "Zelong Zhang", "Haorui Wang", "Xiao Feng", "Feng Zhang"], "title": "$C^3$-Bench: The Things Real Disturbing LLM based Agent in Multi-Tasking", "categories": ["cs.AI"], "comment": null, "summary": "Agents based on large language models leverage tools to modify environments,\nrevolutionizing how AI interacts with the physical world. Unlike traditional\nNLP tasks that rely solely on historical dialogue for responses, these agents\nmust consider more complex factors, such as inter-tool relationships,\nenvironmental feedback and previous decisions, when making choices. Current\nresearch typically evaluates agents via multi-turn dialogues. However, it\noverlooks the influence of these critical factors on agent behavior. To bridge\nthis gap, we present an open-source and high-quality benchmark $C^3$-Bench.\nThis benchmark integrates attack concepts and applies univariate analysis to\npinpoint key elements affecting agent robustness. In concrete, we design three\nchallenges: navigate complex tool relationships, handle critical hidden\ninformation and manage dynamic decision paths. Complementing these challenges,\nwe introduce fine-grained metrics, innovative data collection algorithms and\nreproducible evaluation methods. Extensive experiments are conducted on 49\nmainstream agents, encompassing general fast-thinking, slow-thinking and\ndomain-specific models. We observe that agents have significant shortcomings in\nhandling tool dependencies, long context information dependencies and frequent\npolicy-type switching. In essence, $C^3$-Bench aims to expose model\nvulnerabilities through these challenges and drive research into the\ninterpretability of agent performance. The benchmark is publicly available at\nhttps://github.com/yupeijei1997/C3-Bench."}
{"id": "2505.18313", "pdf": "https://arxiv.org/pdf/2505.18313", "abs": "https://arxiv.org/abs/2505.18313", "authors": ["Matan Haroush", "Daniel Soudry"], "title": "PLUMAGE: Probabilistic Low rank Unbiased Min Variance Gradient Estimator for Efficient Large Model Training", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Accelerator memory and networking constraints have emerged as dominant\nbottlenecks when training large language models LLMs with billions of\nparameters. Existing low rank gradient estimators such as GaLoRE and FLORA\ncompress gradients and optimizer tensors by projecting weight gradients onto a\nrank r subspace, enabling LLM training on consumer hardware. Yet, these methods\nare either biased or subject to high estimator variance. Moreover, the\noptimizer state based on the first and second moments estimates expressed in\nthe previous subspace becomes misaligned whenever the projection is updated,\nleading to instabilities during training. We propose PLUMAGE: Probabilistic Low\nrank Unbiased Minimum vAriance Gradient Estimator. PLUMAGE is a drop in\nreplacement for existing low rank gradient estimators. It does not introduce\nnew hyperparameters beyond the chosen rank r and the update interval. In\naddition, we resolve optimizer state misalignment issues to prevent spurious\nweight updates and enhance training stability. We empirically demonstrate that\nPLUMAGE shrinks the full rank optimization's gap over the pre training\nevaluation loss by 33% on average across models and the average training loss\nacross the GLUE benchmark by 28% within a similar computational and memory\nfootprint as GaloRE."}
{"id": "2505.18524", "pdf": "https://arxiv.org/pdf/2505.18524", "abs": "https://arxiv.org/abs/2505.18524", "authors": ["Guowei Xu", "Mert Yuksekgonul", "Carlos Guestrin", "James Zou"], "title": "metaTextGrad: Automatically optimizing language model optimizers", "categories": ["cs.CL"], "comment": "21 pages, 2 figures", "summary": "Large language models (LLMs) are increasingly used in learning algorithms,\nevaluations, and optimization tasks. Recent studies have shown that using\nLLM-based optimizers to automatically optimize model prompts, demonstrations,\npredictions themselves, or other components can significantly enhance the\nperformance of AI systems, as demonstrated by frameworks such as DSPy and\nTextGrad. However, optimizers built on language models themselves are usually\ndesigned by humans with manual design choices; optimizers themselves are not\noptimized. Moreover, these optimizers are general purpose by design, to be\nuseful to a broad audience, and are not tailored for specific tasks. To address\nthese challenges, we propose metaTextGrad, which focuses on designing a\nmeta-optimizer to further enhance existing optimizers and align them to be good\noptimizers for a given task. Our approach consists of two key components: a\nmeta prompt optimizer and a meta structure optimizer. The combination of these\ntwo significantly improves performance across multiple benchmarks, achieving an\naverage absolute performance improvement of up to 6% compared to the best\nbaseline."}
{"id": "2505.18759", "pdf": "https://arxiv.org/pdf/2505.18759", "abs": "https://arxiv.org/abs/2505.18759", "authors": ["Ruichen Zhang", "Rana Muhammad Shahroz Khan", "Zhen Tan", "Dawei Li", "Song Wang", "Tianlong Chen"], "title": "The Quest for Efficient Reasoning: A Data-Centric Benchmark to CoT Distillation", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Data-centric distillation, including data augmentation, selection, and\nmixing, offers a promising path to creating smaller, more efficient student\nLarge Language Models (LLMs) that retain strong reasoning abilities. However,\nthere still lacks a comprehensive benchmark to systematically assess the effect\nof each distillation approach. This paper introduces DC-CoT, the first\ndata-centric benchmark that investigates data manipulation in chain-of-thought\n(CoT) distillation from method, model and data perspectives. Utilizing various\nteacher models (e.g., o4-mini, Gemini-Pro, Claude-3.5) and student\narchitectures (e.g., 3B, 7B parameters), we rigorously evaluate the impact of\nthese data manipulations on student model performance across multiple reasoning\ndatasets, with a focus on in-distribution (IID) and out-of-distribution (OOD)\ngeneralization, and cross-domain transfer. Our findings aim to provide\nactionable insights and establish best practices for optimizing CoT\ndistillation through data-centric techniques, ultimately facilitating the\ndevelopment of more accessible and capable reasoning models. The dataset can be\nfound at https://huggingface.co/datasets/rana-shahroz/DC-COT, while our code is\nshared in https://anonymous.4open.science/r/DC-COT-FF4C/."}
{"id": "2505.18344", "pdf": "https://arxiv.org/pdf/2505.18344", "abs": "https://arxiv.org/abs/2505.18344", "authors": ["Mudit Gaur", "Prashant Trivedi", "Sasidhar Kunapuli", "Amrit Singh Bedi", "Vaneet Aggarwal"], "title": "Sample Complexity of Diffusion Model Training Without Empirical Risk Minimizer Access", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Diffusion models have demonstrated state-of-the-art performance across\nvision, language, and scientific domains. Despite their empirical success,\nprior theoretical analyses of the sample complexity suffer from poor scaling\nwith input data dimension or rely on unrealistic assumptions such as access to\nexact empirical risk minimizers. In this work, we provide a principled analysis\nof score estimation, establishing a sample complexity bound of\n$\\widetilde{\\mathcal{O}}(\\epsilon^{-6})$. Our approach leverages a structured\ndecomposition of the score estimation error into statistical, approximation,\nand optimization errors, enabling us to eliminate the exponential dependence on\nneural network parameters that arises in prior analyses. It is the first such\nresult which achieves sample complexity bounds without assuming access to the\nempirical risk minimizer of score function estimation loss."}
{"id": "2505.18536", "pdf": "https://arxiv.org/pdf/2505.18536", "abs": "https://arxiv.org/abs/2505.18536", "authors": ["Haoyuan Sun", "Jiaqi Wu", "Bo Xia", "Yifu Luo", "Yifei Zhao", "Kai Qin", "Xufei Lv", "Tiantian Zhang", "Yongzhe Chang", "Xueqian Wang"], "title": "Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Standing in 2025, at a critical juncture in the pursuit of Artificial General\nIntelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated\nsignificant potential in enhancing the reasoning capability of large language\nmodels (LLMs) and has led to the development of cutting-edge AI models such as\nOpenAI-o1 and DeepSeek-R1. Moreover, the efficient application of RFT to\nenhance the reasoning capability of multimodal large language models (MLLMs)\nhas attracted widespread attention from the community. In this position paper,\nwe argue that reinforcement fine-tuning powers the reasoning capability of\nmultimodal large language models. To begin with, we provide a detailed\nintroduction to the fundamental background knowledge that researchers\ninterested in this field should be familiar with. Furthermore, we meticulously\nsummarize the improvements of RFT in powering reasoning capability of MLLMs\ninto five key points: diverse modalities, diverse tasks and domains, better\ntraining algorithms, abundant benchmarks and thriving engineering frameworks.\nFinally, we propose five promising directions for future research that the\ncommunity might consider. We hope that this position paper will provide\nvaluable insights to the community at this pivotal stage in the advancement\ntoward AGI. Summary of works done on RFT for MLLMs is available at\nhttps://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs."}
{"id": "2505.18807", "pdf": "https://arxiv.org/pdf/2505.18807", "abs": "https://arxiv.org/abs/2505.18807", "authors": ["Jiaming Ji", "Wenqi Chen", "Kaile Wang", "Donghai Hong", "Sitong Fang", "Boyuan Chen", "Jiayi Zhou", "Juntao Dai", "Sirui Han", "Yike Guo", "Yaodong Yang"], "title": "Mitigating Deceptive Alignment via Self-Monitoring", "categories": ["cs.AI"], "comment": null, "summary": "Modern large language models rely on chain-of-thought (CoT) reasoning to\nachieve impressive performance, yet the same mechanism can amplify deceptive\nalignment, situations in which a model appears aligned while covertly pursuing\nmisaligned goals. Existing safety pipelines treat deception as a black-box\noutput to be filtered post-hoc, leaving the model free to scheme during its\ninternal reasoning. We ask: Can deception be intercepted while the model is\nthinking? We answer this question, the first framework that embeds a\nSelf-Monitor inside the CoT process itself, named CoT Monitor+. During\ngeneration, the model produces (i) ordinary reasoning steps and (ii) an\ninternal self-evaluation signal trained to flag and suppress misaligned\nstrategies. The signal is used as an auxiliary reward in reinforcement\nlearning, creating a feedback loop that rewards honest reasoning and\ndiscourages hidden goals. To study deceptive alignment systematically, we\nintroduce DeceptionBench, a five-category benchmark that probes covert\nalignment-faking, sycophancy, etc. We evaluate various LLMs and show that\nunrestricted CoT roughly aggravates the deceptive tendency. In contrast, CoT\nMonitor+ cuts deceptive behaviors by 43.8% on average while preserving task\naccuracy. Further, when the self-monitor signal replaces an external weak judge\nin RL fine-tuning, models exhibit substantially fewer obfuscated thoughts and\nretain transparency. Our project website can be found at\ncot-monitor-plus.github.io"}
{"id": "2505.18345", "pdf": "https://arxiv.org/pdf/2505.18345", "abs": "https://arxiv.org/abs/2505.18345", "authors": ["Augusto Tagle", "Javier Ruiz-del-Solar", "Felipe Tobar"], "title": "Diffusion Self-Weighted Guidance for Offline Reinforcement Learning", "categories": ["cs.LG", "stat.ML"], "comment": "20 pages, 4 figures", "summary": "Offline reinforcement learning (RL) recovers the optimal policy $\\pi$ given\nhistorical observations of an agent. In practice, $\\pi$ is modeled as a\nweighted version of the agent's behavior policy $\\mu$, using a weight function\n$w$ working as a critic of the agent's behavior. Though recent approaches to\noffline RL based on diffusion models have exhibited promising results, the\ncomputation of the required scores is challenging due to their dependence on\nthe unknown $w$. In this work, we alleviate this issue by constructing a\ndiffusion over both the actions and the weights. With the proposed setting, the\nrequired scores are directly obtained from the diffusion model without learning\nextra networks. Our main conceptual contribution is a novel guidance method,\nwhere guidance (which is a function of $w$) comes from the same diffusion\nmodel, therefore, our proposal is termed Self-Weighted Guidance (SWG). We show\nthat SWG generates samples from the desired distribution on toy examples and\nperforms on par with state-of-the-art methods on D4RL's challenging\nenvironments, while maintaining a streamlined training pipeline. We further\nvalidate SWG through ablation studies on weight formulations and scalability."}
{"id": "2505.18542", "pdf": "https://arxiv.org/pdf/2505.18542", "abs": "https://arxiv.org/abs/2505.18542", "authors": ["Chen Yang", "Ruping Xu", "Ruizhe Li", "Bin Cao", "Jing Fan"], "title": "Business as \\textit{Rule}sual: A Benchmark and Framework for Business Rule Flow Modeling with LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Process mining aims to discover, monitor and optimize the actual behaviors of\nreal processes. While prior work has mainly focused on extracting procedural\naction flows from instructional texts, rule flows embedded in business\ndocuments remain underexplored. To this end, we introduce a novel annotated\nChinese dataset, \\textbf{BPRF}, which contains 50 business process documents\nwith 326 explicitly labeled business rules across multiple domains. Each rule\nis represented as a <Condition, Action> pair, and we annotate logical\ndependencies between rules (sequential, conditional, or parallel). We also\npropose \\textbf{ExIde}, a framework for automatic business rule extraction and\ndependency relationship identification using large language models (LLMs). We\nevaluate ExIde using 12 state-of-the-art (SOTA) LLMs on the BPRF dataset,\nbenchmarking performance on both rule extraction and dependency classification\ntasks of current LLMs. Our results demonstrate the effectiveness of ExIde in\nextracting structured business rules and analyzing their interdependencies for\ncurrent SOTA LLMs, paving the way for more automated and interpretable business\nprocess automation."}
{"id": "2505.18822", "pdf": "https://arxiv.org/pdf/2505.18822", "abs": "https://arxiv.org/abs/2505.18822", "authors": ["Shijue Huang", "Hongru Wang", "Wanjun Zhong", "Zhaochen Su", "Jiazhan Feng", "Bowen Cao", "Yi R. Fung"], "title": "AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Modern large reasoning models demonstrate impressive problem-solving\ncapabilities by employing sophisticated reasoning strategies. However, they\noften struggle to balance efficiency and effectiveness, frequently generating\nunnecessarily lengthy reasoning chains for simple problems. In this work, we\npropose AdaCtrl, a novel framework to support both difficulty-aware adaptive\nreasoning budget allocation and explicit user control over reasoning depth.\nAdaCtrl dynamically adjusts its reasoning length based on self-assessed problem\ndifficulty, while also allowing users to manually control the budget to\nprioritize either efficiency or effectiveness. This is achieved through a\ntwo-stage training pipeline: an initial cold-start fine-tuning phase to instill\nthe ability to self-aware difficulty and adjust reasoning budget, followed by a\ndifficulty-aware reinforcement learning (RL) stage that refines the model's\nadaptive reasoning strategies and calibrates its difficulty assessments based\non its evolving capabilities during online training. To enable intuitive user\ninteraction, we design explicit length-triggered tags that function as a\nnatural interface for budget control. Empirical results show that AdaCtrl\nadapts reasoning length based on estimated difficulty, compared to the standard\ntraining baseline that also incorporates fine-tuning and RL, it yields\nperformance improvements and simultaneously reduces response length by 10.06%\nand 12.14% on the more challenging AIME2024 and AIME2025 datasets, which\nrequire elaborate reasoning, and by 62.05% and 91.04% on the MATH500 and GSM8K\ndatasets, where more concise responses are sufficient. Furthermore, AdaCtrl\nenables precise user control over the reasoning budget, allowing for tailored\nresponses to meet specific needs."}
{"id": "2505.18347", "pdf": "https://arxiv.org/pdf/2505.18347", "abs": "https://arxiv.org/abs/2505.18347", "authors": ["Mohamed A. Mohamed", "Kateryna Nekhomiazh", "Vedant Vyas", "Marcos M. Jose", "Andrew Patterson", "Marlos C. Machado"], "title": "The Cell Must Go On: Agar.io for Continual Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Continual reinforcement learning (RL) concerns agents that are expected to\nlearn continually, rather than converge to a policy that is then fixed for\nevaluation. Such an approach is well suited to environments the agent perceives\nas changing, which renders any static policy ineffective over time. The few\nsimulators explicitly designed for empirical research in continual RL are often\nlimited in scope or complexity, and it is now common for researchers to modify\nepisodic RL environments by artificially incorporating abrupt task changes\nduring interaction. In this paper, we introduce AgarCL, a research platform for\ncontinual RL that allows for a progression of increasingly sophisticated\nbehaviour. AgarCL is based on the game Agar.io, a non-episodic,\nhigh-dimensional problem featuring stochastic, ever-evolving dynamics,\ncontinuous actions, and partial observability. Additionally, we provide\nbenchmark results reporting the performance of DQN, PPO, and SAC in both the\nprimary, challenging continual RL problem, and across a suite of smaller tasks\nwithin AgarCL, each of which isolates aspects of the full environment and allow\nus to characterize the challenges posed by different aspects of the game."}
{"id": "2505.18548", "pdf": "https://arxiv.org/pdf/2505.18548", "abs": "https://arxiv.org/abs/2505.18548", "authors": ["Sanwoo Lee", "Kun Liang", "Yunfang Wu"], "title": "Composable Cross-prompt Essay Scoring by Merging Models", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in cross-prompt automated essay scoring (AES) typically train\nmodels jointly on all source prompts, often requiring additional access to\nunlabeled target prompt essays simultaneously. However, using all sources is\nsuboptimal in our pilot study, and re-accessing source datasets during\nadaptation raises privacy concerns. We propose a source-free adaptation\napproach that selectively merges individually trained source models' parameters\ninstead of datasets. In particular, we simulate joint training through linear\ncombinations of task vectors -- the parameter updates from fine-tuning. To\noptimize the combination's coefficients, we propose Prior-encoded Information\nMaximization (PIM), an unsupervised objective which promotes the model's score\ndiscriminability regularized by priors pre-computed from the sources. We employ\nBayesian optimization as an efficient optimizer of PIM. Experimental results\nwith LLMs on in-dataset and cross-dataset adaptation show that our method (1)\nconsistently outperforms training jointly on all sources, (2) maintains\nsuperior robustness compared to other merging methods, (3) excels under severe\ndistribution shifts where recent leading cross-prompt methods struggle, all\nwhile retaining computational efficiency."}
{"id": "2505.18829", "pdf": "https://arxiv.org/pdf/2505.18829", "abs": "https://arxiv.org/abs/2505.18829", "authors": ["Kai Mei", "Xi Zhu", "Hang Gao", "Shuhang Lin", "Yongfeng Zhang"], "title": "LiteCUA: Computer as MCP Server for Computer-Use Agent on AIOS", "categories": ["cs.AI", "cs.HC", "cs.OS"], "comment": null, "summary": "We present AIOS 1.0, a novel platform designed to advance computer-use agent\n(CUA) capabilities through environmental contextualization. While existing\napproaches primarily focus on building more powerful agent frameworks or\nenhancing agent models, we identify a fundamental limitation: the semantic\ndisconnect between how language models understand the world and how computer\ninterfaces are structured. AIOS 1.0 addresses this challenge by transforming\ncomputers into contextual environments that language models can natively\ncomprehend, implementing a Model Context Protocol (MCP) server architecture to\nabstract computer states and actions. This approach effectively decouples\ninterface complexity from decision complexity, enabling agents to reason more\neffectively about computing environments. To demonstrate our platform's\neffectiveness, we introduce LiteCUA, a lightweight computer-use agent built on\nAIOS 1.0 that achieves a 14.66% success rate on the OSWorld benchmark,\noutperforming several specialized agent frameworks despite its simple\narchitecture. Our results suggest that contextualizing computer environments\nfor language models represents a promising direction for developing more\ncapable computer-use agents and advancing toward AI that can interact with\ndigital systems. The source code of LiteCUA is available at\nhttps://github.com/agiresearch/LiteCUA, and it is also integrated into the AIOS\nmain branch as part of AIOS at https://github.com/agiresearch/AIOS."}
{"id": "2505.18350", "pdf": "https://arxiv.org/pdf/2505.18350", "abs": "https://arxiv.org/abs/2505.18350", "authors": ["Waleed Reda", "Abhinav Jangda", "Krishna Chintalapudi"], "title": "Task Specific Pruning with LLM-Sieve: How Many Parameters Does Your Task Really Need?", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7"], "comment": null, "summary": "As Large Language Models (LLMs) are increasingly being adopted for narrow\ntasks - such as medical question answering or sentiment analysis - and deployed\nin resource-constrained settings, a key question arises: how many parameters\ndoes a task actually need? In this work, we present LLM-Sieve, the first\ncomprehensive framework for task-specific pruning of LLMs that achieves 20-75%\nparameter reduction with only 1-5% accuracy degradation across diverse domains.\nUnlike prior methods that apply uniform pruning or rely on low-rank\napproximations of weight matrices or inputs in isolation, LLM-Sieve (i) learns\ntask-aware joint projections to better approximate output behavior, and (ii)\nemploys a Genetic Algorithm to discover differentiated pruning levels for each\nmatrix. LLM-Sieve is fully compatible with LoRA fine-tuning and quantization,\nand uniquely demonstrates strong generalization across datasets within the same\ntask domain. Together, these results establish a practical and robust mechanism\nto generate smaller performant task-specific models."}
{"id": "2505.18549", "pdf": "https://arxiv.org/pdf/2505.18549", "abs": "https://arxiv.org/abs/2505.18549", "authors": ["Baraa Hikal", "Mohamed Basem", "Islam Oshallah", "Ali Hamdi"], "title": "MSA at BEA 2025 Shared Task: Disagreement-Aware Instruction Tuning for Multi-Dimensional Evaluation of LLMs as Math Tutors", "categories": ["cs.CL"], "comment": null, "summary": "We present MSA-MathEval, our submission to the BEA 2025 Shared Task on\nevaluating AI tutor responses across four instructional dimensions: Mistake\nIdentification, Mistake Location, Providing Guidance, and Actionability. Our\napproach uses a unified training pipeline to fine-tune a single\ninstruction-tuned language model across all tracks, without any task-specific\narchitectural changes. To improve prediction reliability, we introduce a\ndisagreement-aware ensemble inference strategy that enhances coverage of\nminority labels. Our system achieves strong performance across all tracks,\nranking 1st in Providing Guidance, 3rd in Actionability, and 4th in both\nMistake Identification and Mistake Location. These results demonstrate the\neffectiveness of scalable instruction tuning and disagreement-driven modeling\nfor robust, multi-dimensional evaluation of LLMs as educational tutors."}
{"id": "2505.18847", "pdf": "https://arxiv.org/pdf/2505.18847", "abs": "https://arxiv.org/abs/2505.18847", "authors": ["William Han", "Chaojing Duan", "Zhepeng Cen", "Yihang Yao", "Xiaoyu Song", "Atharva Mhaskar", "Dylan Leong", "Michael A. Rosenberg", "Emerson Liu", "Ding Zhao"], "title": "Signal, Image, or Symbolic: Exploring the Best Input Representation for Electrocardiogram-Language Models Through a Unified Framework", "categories": ["cs.AI", "cs.CL"], "comment": "29 pages, 2 figures, 8 tables", "summary": "Recent advances have increasingly applied large language models (LLMs) to\nelectrocardiogram (ECG) interpretation, giving rise to\nElectrocardiogram-Language Models (ELMs). Conditioned on an ECG and a textual\nquery, an ELM autoregressively generates a free-form textual response. Unlike\ntraditional classification-based systems, ELMs emulate expert cardiac\nelectrophysiologists by issuing diagnoses, analyzing waveform morphology,\nidentifying contributing factors, and proposing patient-specific action plans.\nTo realize this potential, researchers are curating instruction-tuning datasets\nthat pair ECGs with textual dialogues and are training ELMs on these resources.\nYet before scaling ELMs further, there is a fundamental question yet to be\nexplored: What is the most effective ECG input representation? In recent works,\nthree candidate representations have emerged-raw time-series signals, rendered\nimages, and discretized symbolic sequences. We present the first comprehensive\nbenchmark of these modalities across 6 public datasets and 5 evaluation\nmetrics. We find symbolic representations achieve the greatest number of\nstatistically significant wins over both signal and image inputs. We further\nablate the LLM backbone, ECG duration, and token budget, and we evaluate\nrobustness to signal perturbations. We hope that our findings offer clear\nguidance for selecting input representations when developing the next\ngeneration of ELMs."}
{"id": "2505.18355", "pdf": "https://arxiv.org/pdf/2505.18355", "abs": "https://arxiv.org/abs/2505.18355", "authors": ["Yiming Sun", "Shuo Chen", "Shengyu Chen", "Chonghao Qiu", "Licheng Liu", "Youmi Oh", "Sparkle L. Malone", "Gavin McNicol", "Qianlai Zhuang", "Chris Smith", "Yiqun Xie", "Xiaowei Jia"], "title": "X-MethaneWet: A Cross-scale Global Wetland Methane Emission Benchmark Dataset for Advancing Science Discovery with AI", "categories": ["cs.LG"], "comment": "8 pages, 8 figures, 3 tables", "summary": "Methane (CH$_4$) is the second most powerful greenhouse gas after carbon\ndioxide and plays a crucial role in climate change due to its high global\nwarming potential. Accurately modeling CH$_4$ fluxes across the globe and at\nfine temporal scales is essential for understanding its spatial and temporal\nvariability and developing effective mitigation strategies. In this work, we\nintroduce the first-of-its-kind cross-scale global wetland methane benchmark\ndataset (X-MethaneWet), which synthesizes physics-based model simulation data\nfrom TEM-MDM and the real-world observation data from FLUXNET-CH$_4$. This\ndataset can offer opportunities for improving global wetland CH$_4$ modeling\nand science discovery with new AI algorithms. To set up AI model baselines for\nmethane flux prediction, we evaluate the performance of various sequential deep\nlearning models on X-MethaneWet. Furthermore, we explore four different\ntransfer learning techniques to leverage simulated data from TEM-MDM to improve\nthe generalization of deep learning models on real-world FLUXNET-CH$_4$\nobservations. Our extensive experiments demonstrate the effectiveness of these\napproaches, highlighting their potential for advancing methane emission\nmodeling and contributing to the development of more accurate and scalable\nAI-driven climate models."}
{"id": "2505.18555", "pdf": "https://arxiv.org/pdf/2505.18555", "abs": "https://arxiv.org/abs/2505.18555", "authors": ["Yiyang Feng", "Yichen Wang", "Shaobo Cui", "Boi Faltings", "Mina Lee", "Jiawei Zhou"], "title": "Unraveling Misinformation Propagation in LLM Reasoning", "categories": ["cs.CL"], "comment": "24 pages, 14 figures, 4 tables", "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nreasoning, positioning them as promising tools for supporting human\nproblem-solving. However, what happens when their performance is affected by\nmisinformation, i.e., incorrect inputs introduced by users due to oversights or\ngaps in knowledge? Such misinformation is prevalent in real-world interactions\nwith LLMs, yet how it propagates within LLMs' reasoning process remains\nunderexplored. Focusing on mathematical reasoning, we present a comprehensive\nanalysis of how misinformation affects intermediate reasoning steps and final\nanswers. We also examine how effectively LLMs can correct misinformation when\nexplicitly instructed to do so. Even with explicit instructions, LLMs succeed\nless than half the time in rectifying misinformation, despite possessing\ncorrect internal knowledge, leading to significant accuracy drops (10.02% -\n72.20%). Further analysis shows that applying factual corrections early in the\nreasoning process most effectively reduces misinformation propagation, and\nfine-tuning on synthesized data with early-stage corrections significantly\nimproves reasoning factuality. Our work offers a practical approach to\nmitigating misinformation propagation."}
{"id": "2505.18850", "pdf": "https://arxiv.org/pdf/2505.18850", "abs": "https://arxiv.org/abs/2505.18850", "authors": ["Mohamed Aly Bouke"], "title": "The Theory of the Unique Latent Pattern: A Formal Epistemic Framework for Structural Singularity in Complex Systems", "categories": ["cs.AI", "math.LO"], "comment": null, "summary": "This paper introduces the Theory of the Unique Latent Pattern (ULP), a formal\nepistemic framework that redefines the origin of apparent complexity in dynamic\nsystems. Rather than attributing unpredictability to intrinsic randomness or\nemergent nonlinearity, ULP asserts that every analyzable system is governed by\na structurally unique, deterministic generative mechanism, one that remains\nhidden not due to ontological indeterminacy, but due to epistemic constraints.\nThe theory is formalized using a non-universal generative mapping \\(\n\\mathcal{F}_S(P_S, t) \\), where each system \\( S \\) possesses its own latent\nstructure \\( P_S \\), irreducible and non-replicable across systems. Observed\nirregularities are modeled as projections of this generative map through\nobserver-limited interfaces, introducing epistemic noise \\( \\varepsilon_S(t) \\)\nas a measure of incomplete access. By shifting the locus of uncertainty from\nthe system to the observer, ULP reframes chaos as a context-relative failure of\nrepresentation. We contrast this position with foundational paradigms in chaos\ntheory, complexity science, and statistical learning. While they assume or\nmodel shared randomness or collective emergence, ULP maintains that every\ninstance harbors a singular structural identity. Although conceptual, the\ntheory satisfies the criterion of falsifiability in the Popperian sense, it\ninvites empirical challenge by asserting that no two systems governed by\ndistinct latent mechanisms will remain indistinguishable under sufficient\nresolution. This opens avenues for structurally individuated models in AI,\nbehavioral inference, and epistemic diagnostics."}
{"id": "2505.18369", "pdf": "https://arxiv.org/pdf/2505.18369", "abs": "https://arxiv.org/abs/2505.18369", "authors": ["Csaba Both", "Benjamin Hoover", "Hendrik Strobelt", "Dmitry Krotov", "Daniel Karl I. Weidele", "Mauro Martino", "Nima Dehmamy"], "title": "Small Models, Smarter Learning: The Power of Joint Task Training", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The ability of a model to learn a task depends strongly on both the task\ndifficulty and the model size. We aim to understand how task difficulty relates\nto the minimum number of parameters required for learning specific tasks in\nsmall transformer models. Our study focuses on the ListOps dataset, which\nconsists of nested mathematical operations. We gradually increase task\ndifficulty by introducing new operations or combinations of operations into the\ntraining data. We observe that sum modulo n is the hardest to learn. Curiously,\nwhen combined with other operations such as maximum and median, the sum\noperation becomes easier to learn and requires fewer parameters. We show that\njoint training not only improves performance but also leads to qualitatively\ndifferent model behavior. We show evidence that models trained only on SUM\nmight be memorizing and fail to capture the number structure in the embeddings.\nIn contrast, models trained on a mixture of SUM and other operations exhibit\nnumber-like representations in the embedding space, and a strong ability to\ndistinguish parity. Furthermore, the SUM-only model relies more heavily on its\nfeedforward layers, while the jointly trained model activates the attention\nmechanism more. Finally, we show that learning pure SUM can be induced in\nmodels below the learning threshold of pure SUM, by pretraining them on\nMAX+MED. Our findings indicate that emergent abilities in language models\ndepend not only on model size, but also the training curriculum."}
{"id": "2505.18556", "pdf": "https://arxiv.org/pdf/2505.18556", "abs": "https://arxiv.org/abs/2505.18556", "authors": ["Jun Zhuang", "Haibo Jin", "Ye Zhang", "Zhengjian Kang", "Wenbin Zhang", "Gaby G. Dagher", "Haohan Wang"], "title": "Exploring the Vulnerability of the Content Moderation Guardrail in Large Language Models via Intent Manipulation", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint, under review. TL;DR: We propose a new two-stage\n  intent-based prompt-refinement framework, IntentPrompt, that aims to explore\n  the vulnerability of LLMs' content moderation guardrails by refining prompts\n  into benign-looking declarative forms via intent manipulation for red-teaming\n  purposes", "summary": "Intent detection, a core component of natural language understanding, has\nconsiderably evolved as a crucial mechanism in safeguarding large language\nmodels (LLMs). While prior work has applied intent detection to enhance LLMs'\nmoderation guardrails, showing a significant success against content-level\njailbreaks, the robustness of these intent-aware guardrails under malicious\nmanipulations remains under-explored. In this work, we investigate the\nvulnerability of intent-aware guardrails and demonstrate that LLMs exhibit\nimplicit intent detection capabilities. We propose a two-stage intent-based\nprompt-refinement framework, IntentPrompt, that first transforms harmful\ninquiries into structured outlines and further reframes them into\ndeclarative-style narratives by iteratively optimizing prompts via feedback\nloops to enhance jailbreak success for red-teaming purposes. Extensive\nexperiments across four public benchmarks and various black-box LLMs indicate\nthat our framework consistently outperforms several cutting-edge jailbreak\nmethods and evades even advanced Intent Analysis (IA) and Chain-of-Thought\n(CoT)-based defenses. Specifically, our \"FSTR+SPIN\" variant achieves attack\nsuccess rates ranging from 88.25% to 96.54% against CoT-based defenses on the\no1 model, and from 86.75% to 97.12% on the GPT-4o model under IA-based\ndefenses. These findings highlight a critical weakness in LLMs' safety\nmechanisms and suggest that intent manipulation poses a growing challenge to\ncontent moderation guardrails."}
{"id": "2505.18857", "pdf": "https://arxiv.org/pdf/2505.18857", "abs": "https://arxiv.org/abs/2505.18857", "authors": ["Alexander Khrabry", "Edward Startsev", "Andrew Powis", "Igor Kaganovich"], "title": "Hierarchical-embedding autoencoder with a predictor (HEAP) as efficient architecture for learning long-term evolution of complex multi-scale physical systems", "categories": ["cs.AI", "physics.plasm-ph"], "comment": null, "summary": "We propose a novel efficient architecture for learning long-term evolution in\ncomplex multi-scale physical systems which is based on the idea of separation\nof scales. Structures of various scales that dynamically emerge in the system\ninteract with each other only locally. Structures of similar scale can interact\ndirectly when they are in contact and indirectly when they are parts of larger\nstructures that interact directly. This enables modeling a multi-scale system\nin an efficient way, where interactions between small-scale features that are\napart from each other do not need to be modeled. The hierarchical\nfully-convolutional autoencoder transforms the state of a physical system not\njust into a single embedding layer, as it is done conventionally, but into a\nseries of embedding layers which encode structures of various scales preserving\nspatial information at a corresponding resolution level. Shallower layers embed\nsmaller structures on a finer grid, while deeper layers embed larger structures\non a coarser grid. The predictor advances all embedding layers in sync.\nInteractions between features of various scales are modeled using a combination\nof convolutional operators. We compare the performance of our model to\nvariations of a conventional ResNet architecture in application to the\nHasegawa-Wakatani turbulence. A multifold improvement in long-term prediction\naccuracy was observed for crucial statistical characteristics of this system."}
{"id": "2505.18373", "pdf": "https://arxiv.org/pdf/2505.18373", "abs": "https://arxiv.org/abs/2505.18373", "authors": ["Paul M. Riechers", "Henry R. Bigelow", "Eric A. Alt", "Adam Shai"], "title": "Next-token pretraining implies in-context learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We argue that in-context learning (ICL) predictably arises from standard\nself-supervised next-token pretraining, rather than being an exotic emergent\nproperty. This work establishes the foundational principles of this emergence\nby focusing on in-distribution ICL, demonstrating how models necessarily adapt\nto context when trained on token sequences, especially from non-ergodic\nsources. Our information-theoretic framework precisely predicts these\nin-distribution ICL dynamics (i.e., context-dependent loss reduction). We\nverify this with experiments using synthetic datasets of differing types of\ncorrelational structure, reproducing characteristic phenomena like phase\ntransitions in training loss for induction head formation and power-law scaling\nof in-context loss. We further show that a model's in-context performance on\nany task is mathematically coupled to the ensemble of tasks seen in\npretraining, offering a fundamental explanation, grounded in architecture- and\nmodality-independent principles, for such inference-time learning."}
{"id": "2505.18557", "pdf": "https://arxiv.org/pdf/2505.18557", "abs": "https://arxiv.org/abs/2505.18557", "authors": ["He Zhu", "Zhiwen Ruan", "Junyou Su", "Xingwei He", "Wenjia Zhang", "Yun Chen", "Guanhua Chen"], "title": "TAG-INSTRUCT: Controlled Instruction Complexity Enhancement through Structure-based Augmentation", "categories": ["cs.CL"], "comment": null, "summary": "High-quality instruction data is crucial for developing large language models\n(LLMs), yet existing approaches struggle to effectively control instruction\ncomplexity. We present TAG-INSTRUCT, a novel framework that enhances\ninstruction complexity through structured semantic compression and controlled\ndifficulty augmentation. Unlike previous prompt-based methods operating on raw\ntext, TAG-INSTRUCT compresses instructions into a compact tag space and\nsystematically enhances complexity through RL-guided tag expansion. Through\nextensive experiments, we show that TAG-INSTRUCT outperforms existing\ninstruction complexity augmentation approaches. Our analysis reveals that\noperating in tag space provides superior controllability and stability across\ndifferent instruction synthesis frameworks."}
{"id": "2505.18894", "pdf": "https://arxiv.org/pdf/2505.18894", "abs": "https://arxiv.org/abs/2505.18894", "authors": ["Vanessa Utz", "Steve DiPaola"], "title": "Digital Overconsumption and Waste: A Closer Look at the Impacts of Generative AI", "categories": ["cs.AI"], "comment": "Conference on Computer Vision and Pattern Recognition (CVPR) 2023.\n  Ethical Considerations in Creative Applications of Computer Vision (EC3V)\n  Workshop", "summary": "Generative Artificial Intelligence (AI) systems currently contribute\nnegatively to the production of digital waste, via the associated energy\nconsumption and the related CO2 emissions. At this moment, a discussion is\nurgently needed on the replication of harmful consumer behavior, such as\noverconsumption, in the digital space. We outline our previous work on the\nclimate implications of commercially available generative AI systems and the\nsentiment of generative AI users when confronted with AI-related climate\nresearch. We expand on this work via a discussion of digital overconsumption\nand waste, other related societal impacts, and a possible solution pathway"}
{"id": "2505.18392", "pdf": "https://arxiv.org/pdf/2505.18392", "abs": "https://arxiv.org/abs/2505.18392", "authors": ["Danny Reidenbach", "Filipp Nikitin", "Olexandr Isayev", "Saee Paliwal"], "title": "Applications of Modular Co-Design for De Novo 3D Molecule Generation", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.QM"], "comment": null, "summary": "De novo 3D molecule generation is a pivotal task in drug discovery. However,\nmany recent geometric generative models struggle to produce high-quality 3D\nstructures, even if they maintain 2D validity and topological stability. To\ntackle this issue and enhance the learning of effective molecular generation\ndynamics, we present Megalodon-a family of scalable transformer models. These\nmodels are enhanced with basic equivariant layers and trained using a joint\ncontinuous and discrete denoising co-design objective. We assess Megalodon's\nperformance on established molecule generation benchmarks and introduce new 3D\nstructure benchmarks that evaluate a model's capability to generate realistic\nmolecular structures, particularly focusing on energetics. We show that\nMegalodon achieves state-of-the-art results in 3D molecule generation,\nconditional structure generation, and structure energy benchmarks using\ndiffusion and flow matching. Furthermore, doubling the number of parameters in\nMegalodon to 40M significantly enhances its performance, generating up to 49x\nmore valid large molecules and achieving energy levels that are 2-10x lower\nthan those of the best prior generative models."}
{"id": "2505.18562", "pdf": "https://arxiv.org/pdf/2505.18562", "abs": "https://arxiv.org/abs/2505.18562", "authors": ["Xunlian Dai", "Li Zhou", "Benyou Wang", "Haizhou Li"], "title": "From Word to World: Evaluate and Mitigate Culture Bias via Word Association Test", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The human-centered word association test (WAT) serves as a cognitive proxy,\nrevealing sociocultural variations through lexical-semantic patterns. We extend\nthis test into an LLM-adaptive, free-relation task to assess the alignment of\nlarge language models (LLMs) with cross-cultural cognition. To mitigate the\nculture preference, we propose CultureSteer, an innovative approach that\nintegrates a culture-aware steering mechanism to guide semantic representations\ntoward culturally specific spaces. Experiments show that current LLMs exhibit\nsignificant bias toward Western cultural (notably in American) schemas at the\nword association level. In contrast, our model substantially improves\ncross-cultural alignment, surpassing prompt-based methods in capturing diverse\nsemantic associations. Further validation on culture-sensitive downstream tasks\nconfirms its efficacy in fostering cognitive alignment across cultures. This\nwork contributes a novel methodological paradigm for enhancing cultural\nawareness in LLMs, advancing the development of more inclusive language\ntechnologies."}
{"id": "2505.18907", "pdf": "https://arxiv.org/pdf/2505.18907", "abs": "https://arxiv.org/abs/2505.18907", "authors": ["Sanjay Kariyappa", "G. Edward Suh"], "title": "Stronger Enforcement of Instruction Hierarchy via Augmented Intermediate Representations", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Prompt injection attacks are a critical security vulnerability in large\nlanguage models (LLMs), allowing attackers to hijack model behavior by\ninjecting malicious instructions within the input context. Recent defense\nmechanisms have leveraged an Instruction Hierarchy (IH) Signal, often\nimplemented through special delimiter tokens or additive embeddings to denote\nthe privilege level of input tokens. However, these prior works typically\ninject the IH signal exclusively at the initial input layer, which we\nhypothesize limits its ability to effectively distinguish the privilege levels\nof tokens as it propagates through the different layers of the model. To\novercome this limitation, we introduce a novel approach that injects the IH\nsignal into the intermediate token representations within the network. Our\nmethod augments these representations with layer-specific trainable embeddings\nthat encode the privilege information. Our evaluations across multiple models\nand training methods reveal that our proposal yields between $1.6\\times$ and\n$9.2\\times$ reduction in attack success rate on gradient-based prompt injection\nattacks compared to state-of-the-art methods, without significantly degrading\nthe model's utility."}
{"id": "2505.18404", "pdf": "https://arxiv.org/pdf/2505.18404", "abs": "https://arxiv.org/abs/2505.18404", "authors": ["Menghua Wu", "Cai Zhou", "Stephen Bates", "Tommi Jaakkola"], "title": "Thought calibration: Efficient and confident test-time scaling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reasoning large language models achieve impressive test-time scaling by\nthinking for longer, but this performance gain comes at significant compute\ncost. Directly limiting test-time budget hurts overall performance, but not all\nproblems are equally difficult. We propose thought calibration to decide\ndynamically when thinking can be terminated. To calibrate our decision rule, we\nview a language model's growing body of thoughts as a nested sequence of\nreasoning trees, where the goal is to identify the point at which novel\nreasoning plateaus. We realize this framework through lightweight probes that\noperate on top of the language model's hidden representations, which are\ninformative of both the reasoning structure and overall consistency of\nresponse. Based on three reasoning language models and four datasets, thought\ncalibration preserves model performance with up to a 60% reduction in thinking\ntokens on in-distribution data, and up to 20% in out-of-distribution data."}
{"id": "2505.18581", "pdf": "https://arxiv.org/pdf/2505.18581", "abs": "https://arxiv.org/abs/2505.18581", "authors": ["Wentao Hu", "Wengyu Zhang", "Yiyang Jiang", "Chen Jason Zhang", "Xiaoyong Wei", "Qing Li"], "title": "Removal of Hallucination on Hallucination: Debate-Augmented RAG", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025", "summary": "Retrieval-Augmented Generation (RAG) enhances factual accuracy by integrating\nexternal knowledge, yet it introduces a critical issue: erroneous or biased\nretrieval can mislead generation, compounding hallucinations, a phenomenon we\nterm Hallucination on Hallucination. To address this, we propose\nDebate-Augmented RAG (DRAG), a training-free framework that integrates\nMulti-Agent Debate (MAD) mechanisms into both retrieval and generation stages.\nIn retrieval, DRAG employs structured debates among proponents, opponents, and\njudges to refine retrieval quality and ensure factual reliability. In\ngeneration, DRAG introduces asymmetric information roles and adversarial\ndebates, enhancing reasoning robustness and mitigating factual inconsistencies.\nEvaluations across multiple tasks demonstrate that DRAG improves retrieval\nreliability, reduces RAG-induced hallucinations, and significantly enhances\noverall factual accuracy. Our code is available at\nhttps://github.com/Huenao/Debate-Augmented-RAG."}
{"id": "2505.18929", "pdf": "https://arxiv.org/pdf/2505.18929", "abs": "https://arxiv.org/abs/2505.18929", "authors": ["Wenda Zhang"], "title": "Meta-aware Learning in text-to-SQL Large Language Model", "categories": ["cs.AI", "cs.CL"], "comment": "Keywords: text-to-SQL LLM, fine-tuning, meta-aware leanring,\n  metadata, chain-of-thought, BigQuery SQL, business database", "summary": "The advancements of Large language models (LLMs) have provided great\nopportunities to text-to-SQL tasks to overcome the main challenges to\nunderstand complex domain information and complex database structures in\nbusiness applications. In this paper, we propose a meta-aware learning\nframework to integrate domain knowledge, database schema, chain-of-thought\nreasoning processes, and metadata relationships to improve the SQL generation\nquality. The proposed framework includes four learning strategies: schema-based\nlearning, Chain-of-Thought (CoT) learning, knowledge-enhanced learning, and key\ninformation tokenization. This approach provides a comprehensive understanding\nof database structure and metadata information towards LLM through fine-tuning\nto improve its performance on SQL generation within business domains. Through\ntwo experimental studies, we have demonstrated the superiority of the proposed\nmethods in execution accuracy, multi-task SQL generation capability, and\nreduction of catastrophic forgetting."}
{"id": "2505.18407", "pdf": "https://arxiv.org/pdf/2505.18407", "abs": "https://arxiv.org/abs/2505.18407", "authors": ["Yizhou Zhang", "Kishan Panaganti", "Laixi Shi", "Juba Ziani", "Adam Wierman"], "title": "KL-regularization Itself is Differentially Private in Bandits and RLHF", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Differential Privacy (DP) provides a rigorous framework for privacy, ensuring\nthe outputs of data-driven algorithms remain statistically indistinguishable\nacross datasets that differ in a single entry. While guaranteeing DP generally\nrequires explicitly injecting noise either to the algorithm itself or to its\noutputs, the intrinsic randomness of existing algorithms presents an\nopportunity to achieve DP ``for free''. In this work, we explore the role of\nregularization in achieving DP across three different decision-making problems:\nmulti-armed bandits, linear contextual bandits, and reinforcement learning from\nhuman feedback (RLHF), in offline data settings. We show that adding\nKL-regularization to the learning objective (a common approach in optimization\nalgorithms) makes the action sampled from the resulting stochastic policy\nitself differentially private. This offers a new route to privacy guarantees\nwithout additional noise injection, while also preserving the inherent\nadvantage of regularization in enhancing performance."}
{"id": "2505.18588", "pdf": "https://arxiv.org/pdf/2505.18588", "abs": "https://arxiv.org/abs/2505.18588", "authors": ["Zesheng Shi", "Yucheng Zhou", "Jing Li"], "title": "Safety Alignment via Constrained Knowledge Unlearning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite significant progress in safety alignment, large language models\n(LLMs) remain susceptible to jailbreak attacks. Existing defense mechanisms\nhave not fully deleted harmful knowledge in LLMs, which allows such attacks to\nbypass safeguards and produce harmful outputs. To address this challenge, we\npropose a novel safety alignment strategy, Constrained Knowledge Unlearning\n(CKU), which focuses on two primary objectives: knowledge localization and\nretention, and unlearning harmful knowledge. CKU works by scoring neurons in\nspecific multilayer perceptron (MLP) layers to identify a subset U of neurons\nassociated with useful knowledge. During the unlearning process, CKU prunes the\ngradients of neurons in U to preserve valuable knowledge while effectively\nmitigating harmful content. Experimental results demonstrate that CKU\nsignificantly enhances model safety without compromising overall performance,\noffering a superior balance between safety and utility compared to existing\nmethods. Additionally, our analysis of neuron knowledge sensitivity across\nvarious MLP layers provides valuable insights into the mechanics of safety\nalignment and model knowledge editing."}
{"id": "2505.18931", "pdf": "https://arxiv.org/pdf/2505.18931", "abs": "https://arxiv.org/abs/2505.18931", "authors": ["Ryan Saklad", "Aman Chadha", "Oleg Pavlov", "Raha Moraffah"], "title": "Can Large Language Models Infer Causal Relationships from Real-World Text?", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Understanding and inferring causal relationships from texts is a core aspect\nof human cognition and is essential for advancing large language models (LLMs)\ntowards artificial general intelligence. Existing work primarily focuses on\nsynthetically generated texts which involve simple causal relationships\nexplicitly mentioned in the text. This fails to reflect the complexities of\nreal-world tasks. In this paper, we investigate whether LLMs are capable of\ninferring causal relationships from real-world texts. We develop a benchmark\ndrawn from real-world academic literature which includes diverse texts with\nrespect to length, complexity of relationships (different levels of\nexplicitness, number of events, and causal relationships), and domains and\nsub-domains. To the best of our knowledge, our benchmark is the first-ever\nreal-world dataset for this task. Our experiments on state-of-the-art LLMs\nevaluated on our proposed benchmark demonstrate significant challenges, with\nthe best-performing model achieving an average F1 score of only 0.477. Analysis\nreveals common pitfalls: difficulty with implicitly stated information, in\ndistinguishing relevant causal factors from surrounding contextual details, and\nwith connecting causally relevant information spread across lengthy textual\npassages. By systematically characterizing these deficiencies, our benchmark\noffers targeted insights for further research into advancing LLM causal\nreasoning."}
{"id": "2505.18413", "pdf": "https://arxiv.org/pdf/2505.18413", "abs": "https://arxiv.org/abs/2505.18413", "authors": ["Toshiaki Koike-Akino", "Xiangyu Chen", "Jing Liu", "Ye Wang", "Pu", "Wang", "Matthew Brand"], "title": "LatentLLM: Attention-Aware Joint Tensor Compression", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "37 pages, 16 figures", "summary": "Modern foundation models such as large language models (LLMs) and large\nmulti-modal models (LMMs) require a massive amount of computational and memory\nresources. We propose a new framework to convert such LLMs/LMMs into a\nreduced-dimension latent structure. Our method extends a local activation-aware\ntensor decomposition to a global attention-aware joint tensor de-composition.\nOur framework can significantly improve the model accuracy over the existing\nmodel compression methods when reducing the latent dimension to realize\ncomputationally/memory-efficient LLMs/LLMs. We show the benefit on several\nbenchmark including multi-modal reasoning tasks."}
{"id": "2505.18596", "pdf": "https://arxiv.org/pdf/2505.18596", "abs": "https://arxiv.org/abs/2505.18596", "authors": ["Chen Han", "Wenzhen Zheng", "Xijin Tang"], "title": "Debate-to-Detect: Reformulating Misinformation Detection as a Real-World Debate with Large Language Models", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.6; H.3.3"], "comment": null, "summary": "The proliferation of misinformation in digital platforms reveals the\nlimitations of traditional detection methods, which mostly rely on static\nclassification and fail to capture the intricate process of real-world\nfact-checking. Despite advancements in Large Language Models (LLMs) that\nenhance automated reasoning, their application to misinformation detection\nremains hindered by issues of logical inconsistency and superficial\nverification. In response, we introduce Debate-to-Detect (D2D), a novel\nMulti-Agent Debate (MAD) framework that reformulates misinformation detection\nas a structured adversarial debate. Inspired by fact-checking workflows, D2D\nassigns domain-specific profiles to each agent and orchestrates a five-stage\ndebate process, including Opening Statement, Rebuttal, Free Debate, Closing\nStatement, and Judgment. To transcend traditional binary classification, D2D\nintroduces a multi-dimensional evaluation mechanism that assesses each claim\nacross five distinct dimensions: Factuality, Source Reliability, Reasoning\nQuality, Clarity, and Ethics. Experiments with GPT-4o on two fakenews datasets\ndemonstrate significant improvements over baseline methods, and the case study\nhighlight D2D's capability to iteratively refine evidence while improving\ndecision transparency, representing a substantial advancement towards robust\nand interpretable misinformation detection. The code will be open-sourced in a\nfuture release."}
{"id": "2505.18933", "pdf": "https://arxiv.org/pdf/2505.18933", "abs": "https://arxiv.org/abs/2505.18933", "authors": ["Haitian Zhong", "Yuhuan Liu", "Ziyang Xu", "Guofan Liu", "Qiang Liu", "Shu Wu", "Zhe Zhao", "Liang Wang", "Tieniu Tan"], "title": "REACT: Representation Extraction And Controllable Tuning to Overcome Overfitting in LLM Knowledge Editing", "categories": ["cs.AI", "cs.CL"], "comment": "15 pages, 4 figures", "summary": "Large language model editing methods frequently suffer from overfitting,\nwherein factual updates can propagate beyond their intended scope,\noveremphasizing the edited target even when it's contextually inappropriate. To\naddress this challenge, we introduce REACT (Representation Extraction And\nControllable Tuning), a unified two-phase framework designed for precise and\ncontrollable knowledge editing. In the initial phase, we utilize tailored\nstimuli to extract latent factual representations and apply Principal Component\nAnalysis with a simple learnbale linear transformation to compute a directional\n\"belief shift\" vector for each instance. In the second phase, we apply\ncontrollable perturbations to hidden states using the obtained vector with a\nmagnitude scalar, gated by a pre-trained classifier that permits edits only\nwhen contextually necessary. Relevant experiments on EVOKE benchmarks\ndemonstrate that REACT significantly reduces overfitting across nearly all\nevaluation metrics, and experiments on COUNTERFACT and MQuAKE shows that our\nmethod preserves balanced basic editing performance (reliability, locality, and\ngenerality) under diverse editing scenarios."}
{"id": "2505.18414", "pdf": "https://arxiv.org/pdf/2505.18414", "abs": "https://arxiv.org/abs/2505.18414", "authors": ["Chandra Kundu", "Abiy Tasissa", "HanQin Cai"], "title": "A Dual Basis Approach for Structured Robust Euclidean Distance Geometry", "categories": ["cs.LG", "cs.IT", "math.IT", "math.OC", "stat.ML"], "comment": null, "summary": "Euclidean Distance Matrix (EDM), which consists of pairwise squared Euclidean\ndistances of a given point configuration, finds many applications in modern\nmachine learning. This paper considers the setting where only a set of anchor\nnodes is used to collect the distances between themselves and the rest. In the\npresence of potential outliers, it results in a structured partial observation\non EDM with partial corruptions. Note that an EDM can be connected to a\npositive semi-definite Gram matrix via a non-orthogonal dual basis. Inspired by\nrecent development of non-orthogonal dual basis in optimization, we propose a\nnovel algorithmic framework, dubbed Robust Euclidean Distance Geometry via Dual\nBasis (RoDEoDB), for recovering the Euclidean distance geometry, i.e., the\nunderlying point configuration. The exact recovery guarantees have been\nestablished in terms of both the Gram matrix and point configuration, under\nsome mild conditions. Empirical experiments show superior performance of\nRoDEoDB on sensor localization and molecular conformation datasets."}
{"id": "2505.18601", "pdf": "https://arxiv.org/pdf/2505.18601", "abs": "https://arxiv.org/abs/2505.18601", "authors": ["Jongwoo Ko", "Sungnyun Kim", "Sungwoo Cho", "Se-Young Yun"], "title": "Flex-Judge: Think Once, Judge Anywhere", "categories": ["cs.CL", "cs.AI"], "comment": "The code is available at https://github.com/jongwooko/flex-judge", "summary": "Human-generated reward signals are critical for aligning generative models\nwith human preferences, guiding both training and inference-time evaluations.\nWhile large language models (LLMs) employed as proxy evaluators, i.e.,\nLLM-as-a-Judge, significantly reduce the costs associated with manual\nannotations, they typically require extensive modality-specific training data\nand fail to generalize well across diverse multimodal tasks. In this paper, we\npropose Flex-Judge, a reasoning-guided multimodal judge model that leverages\nminimal textual reasoning data to robustly generalize across multiple\nmodalities and evaluation formats. Our core intuition is that structured\ntextual reasoning explanations inherently encode generalizable decision-making\npatterns, enabling an effective transfer to multimodal judgments, e.g., with\nimages or videos. Empirical results demonstrate that Flex-Judge, despite being\ntrained on significantly fewer text data, achieves competitive or superior\nperformance compared to state-of-the-art commercial APIs and extensively\ntrained multimodal evaluators. Notably, Flex-Judge presents broad impact in\nmodalities like molecule, where comprehensive evaluation benchmarks are scarce,\nunderscoring its practical value in resource-constrained domains. Our framework\nhighlights reasoning-based text supervision as a powerful, cost-effective\nalternative to traditional annotation-intensive approaches, substantially\nadvancing scalable multimodal model-as-a-judge."}
{"id": "2505.18946", "pdf": "https://arxiv.org/pdf/2505.18946", "abs": "https://arxiv.org/abs/2505.18946", "authors": ["Yong Xiao", "Haoran Zhou", "Xubo Li", "Yayu Gao", "Guangming Shi", "Ping Zhang"], "title": "SANNet: A Semantic-Aware Agentic AI Networking Framework for Multi-Agent Cross-Layer Coordination", "categories": ["cs.AI", "cs.MA", "cs.NI"], "comment": "submitted to IEEE GLOBECOM'25", "summary": "Agentic AI networking (AgentNet) is a novel AI-native networking paradigm\nthat relies on a large number of specialized AI agents to collaborate and\ncoordinate for autonomous decision-making, dynamic environmental adaptation,\nand complex goal achievement. It has the potential to facilitate real-time\nnetwork management alongside capabilities for self-configuration,\nself-optimization, and self-adaptation across diverse and complex networking\nenvironments, laying the foundation for fully autonomous networking systems in\nthe future. Despite its promise, AgentNet is still in the early stage of\ndevelopment, and there still lacks an effective networking framework to support\nautomatic goal discovery and multi-agent self-orchestration and task\nassignment. This paper proposes SANNet, a novel semantic-aware agentic AI\nnetworking architecture that can infer the semantic goal of the user and\nautomatically assign agents associated with different layers of a mobile system\nto fulfill the inferred goal. Motivated by the fact that one of the major\nchallenges in AgentNet is that different agents may have different and even\nconflicting objectives when collaborating for certain goals, we introduce a\ndynamic weighting-based conflict-resolving mechanism to address this issue. We\nprove that SANNet can provide theoretical guarantee in both conflict-resolving\nand model generalization performance for multi-agent collaboration in dynamic\nenvironment. We develop a hardware prototype of SANNet based on the open RAN\nand 5GS core platform. Our experimental results show that SANNet can\nsignificantly improve the performance of multi-agent networking systems, even\nwhen agents with conflicting objectives are selected to collaborate for the\nsame goal."}
{"id": "2505.18421", "pdf": "https://arxiv.org/pdf/2505.18421", "abs": "https://arxiv.org/abs/2505.18421", "authors": ["Junyi Fan", "Shuheng Chen", "Li Sun", "Yong Si", "Elham Pishgar", "Kamiar Alaei", "Greg Placencia", "Maryam Pishgar"], "title": "Development of Interactive Nomograms for Predicting Short-Term Survival in ICU Patients with Aplastic Anemia", "categories": ["cs.LG"], "comment": null, "summary": "Aplastic anemia is a rare, life-threatening hematologic disorder\ncharacterized by pancytopenia and bone marrow failure. ICU admission in these\npatients often signals critical complications or disease progression, making\nearly risk assessment crucial for clinical decision-making and resource\nallocation. In this study, we used the MIMIC-IV database to identify ICU\npatients diagnosed with aplastic anemia and extracted clinical features from\nfive domains: demographics, synthetic indicators, laboratory results,\ncomorbidities, and medications. Over 400 variables were reduced to seven key\npredictors through machine learning-based feature selection. Logistic\nregression and Cox regression models were constructed to predict 7-, 14-, and\n28-day mortality, and their performance was evaluated using AUROC. External\nvalidation was conducted using the eICU Collaborative Research Database to\nassess model generalizability. Among 1,662 included patients, the logistic\nregression model demonstrated superior performance, with AUROC values of\n0.8227, 0.8311, and 0.8298 for 7-, 14-, and 28-day mortality, respectively,\ncompared to the Cox model. External validation yielded AUROCs of 0.7391,\n0.7119, and 0.7093. Interactive nomograms were developed based on the logistic\nregression model to visually estimate individual patient risk. In conclusion,\nwe identified a concise set of seven predictors, led by APS III, to build\nvalidated and generalizable nomograms that accurately estimate short-term\nmortality in ICU patients with aplastic anemia. These tools may aid clinicians\nin personalized risk stratification and decision-making at the point of care."}
{"id": "2505.18609", "pdf": "https://arxiv.org/pdf/2505.18609", "abs": "https://arxiv.org/abs/2505.18609", "authors": ["Ashwin Sankar", "Yoach Lacombe", "Sherry Thomas", "Praveen Srinivasa Varadhan", "Sanchit Gandhi", "Mitesh M Khapra"], "title": "RASMALAI: Resources for Adaptive Speech Modeling in Indian Languages with Accents and Intonations", "categories": ["cs.CL"], "comment": "Accepted at Interspeech 2025", "summary": "We introduce RASMALAI, a large-scale speech dataset with rich text\ndescriptions, designed to advance controllable and expressive text-to-speech\n(TTS) synthesis for 23 Indian languages and English. It comprises 13,000 hours\nof speech and 24 million text-description annotations with fine-grained\nattributes like speaker identity, accent, emotion, style, and background\nconditions. Using RASMALAI, we develop IndicParlerTTS, the first open-source,\ntext-description-guided TTS for Indian languages. Systematic evaluation\ndemonstrates its ability to generate high-quality speech for named speakers,\nreliably follow text descriptions and accurately synthesize specified\nattributes. Additionally, it effectively transfers expressive characteristics\nboth within and across languages. IndicParlerTTS consistently achieves strong\nperformance across these evaluations, setting a new standard for controllable\nmultilingual expressive speech synthesis in Indian languages."}
{"id": "2505.18955", "pdf": "https://arxiv.org/pdf/2505.18955", "abs": "https://arxiv.org/abs/2505.18955", "authors": ["Yuheng Tang", "Hongwei Li", "Kaijie Zhu", "Michael Yang", "Yangruibo Ding", "Wenbo Guo"], "title": "Co-PatcheR: Collaborative Software Patching with Component(s)-specific Small Reasoning Models", "categories": ["cs.AI", "cs.CR", "cs.SE"], "comment": null, "summary": "Motivated by the success of general-purpose large language models (LLMs) in\nsoftware patching, recent works started to train specialized patching models.\nMost works trained one model to handle the end-to-end patching pipeline\n(including issue localization, patch generation, and patch validation).\nHowever, it is hard for a small model to handle all tasks, as different\nsub-tasks have different workflows and require different expertise. As such, by\nusing a 70 billion model, SOTA methods can only reach up to 41% resolved rate\non SWE-bench-Verified. Motivated by the collaborative nature, we propose\nCo-PatcheR, the first collaborative patching system with small and specialized\nreasoning models for individual components. Our key technique novelties are the\nspecific task designs and training recipes. First, we train a model for\nlocalization and patch generation. Our localization pinpoints the suspicious\nlines through a two-step procedure, and our generation combines patch\ngeneration and critique. We then propose a hybrid patch validation that\nincludes two models for crafting issue-reproducing test cases with and without\nassertions and judging patch correctness, followed by a majority vote-based\npatch selection. Through extensive evaluation, we show that Co-PatcheR achieves\n46% resolved rate on SWE-bench-Verified with only 3 x 14B models. This makes\nCo-PatcheR the best patcher with specialized models, requiring the least\ntraining resources and the smallest models. We conduct a comprehensive ablation\nstudy to validate our recipes, as well as our choice of training data number,\nmodel size, and testing-phase scaling strategy."}
{"id": "2505.18433", "pdf": "https://arxiv.org/pdf/2505.18433", "abs": "https://arxiv.org/abs/2505.18433", "authors": ["Zhiyao Zhang", "Myeung Suk Oh", "FNU Hairi", "Ziyue Luo", "Alvaro Velasquez", "Jia Liu"], "title": "Finite-Time Global Optimality Convergence in Deep Neural Actor-Critic Methods for Decentralized Multi-Agent Reinforcement Learning", "categories": ["cs.LG", "cs.MA"], "comment": null, "summary": "Actor-critic methods for decentralized multi-agent reinforcement learning\n(MARL) facilitate collaborative optimal decision making without centralized\ncoordination, thus enabling a wide range of applications in practice. To date,\nhowever, most theoretical convergence studies for existing actor-critic\ndecentralized MARL methods are limited to the guarantee of a stationary\nsolution under the linear function approximation. This leaves a significant gap\nbetween the highly successful use of deep neural actor-critic for decentralized\nMARL in practice and the current theoretical understanding. To bridge this gap,\nin this paper, we make the first attempt to develop a deep neural actor-critic\nmethod for decentralized MARL, where both the actor and critic components are\ninherently non-linear. We show that our proposed method enjoys a global\noptimality guarantee with a finite-time convergence rate of O(1/T), where T is\nthe total iteration times. This marks the first global convergence result for\ndeep neural actor-critic methods in the MARL literature. We also conduct\nextensive numerical experiments, which verify our theoretical results."}
{"id": "2505.18610", "pdf": "https://arxiv.org/pdf/2505.18610", "abs": "https://arxiv.org/abs/2505.18610", "authors": ["Tengxuan Liu", "Shiyao Li", "Jiayi Yang", "Tianchen Zhao", "Feng Zhou", "Xiaohui Song", "Guohao Dai", "Shengen Yan", "Huazhong Yang", "Yu Wang"], "title": "PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Recently, significant progress has been made in developing reasoning-capable\nLarge Language Models (LLMs) through long Chain-of-Thought (CoT) techniques.\nHowever, this long-CoT reasoning process imposes substantial memory overhead\ndue to the large Key-Value (KV) Cache memory overhead. Post-training KV Cache\nquantization has emerged as a promising compression technique and has been\nextensively studied in short-context scenarios. However, directly applying\nexisting methods to long-CoT LLMs causes significant performance degradation\ndue to the following two reasons: (1) Large cumulative error: Existing methods\nfail to adequately leverage available memory, and they directly quantize the KV\nCache during each decoding step, leading to large cumulative quantization\nerror. (2) Short-context calibration: Due to Rotary Positional Embedding\n(RoPE), the use of short-context data during calibration fails to account for\nthe distribution of less frequent channels in the Key Cache, resulting in\nperformance loss. We propose Progressive Mixed-Precision KV Cache Quantization\n(PM-KVQ) for long-CoT LLMs to address the above issues in two folds: (1) To\nreduce cumulative error, we design a progressive quantization strategy to\ngradually lower the bit-width of KV Cache in each block. Then, we propose\nblock-wise memory allocation to assign a higher bit-width to more sensitive\ntransformer blocks. (2) To increase the calibration length without additional\noverhead, we propose a new calibration strategy with positional interpolation\nthat leverages short calibration data with positional interpolation to\napproximate the data distribution of long-context data. Extensive experiments\non 7B-70B long-CoT LLMs show that PM-KVQ improves reasoning benchmark\nperformance by up to 8% over SOTA baselines under the same memory budget. Our\ncode is available at https://github.com/thu-nics/PM-KVQ."}
{"id": "2505.18961", "pdf": "https://arxiv.org/pdf/2505.18961", "abs": "https://arxiv.org/abs/2505.18961", "authors": ["Rohit Khoja", "Devanshu Gupta", "Yanjie Fu", "Dan Roth", "Vivek Gupta"], "title": "Weaver: Interweaving SQL and LLM for Table Reasoning", "categories": ["cs.AI", "cs.IR"], "comment": null, "summary": "Querying tables with unstructured data is challenging due to the presence of\ntext (or image), either embedded in the table or in external paragraphs, which\ntraditional SQL struggles to process, especially for tasks requiring semantic\nreasoning. While Large Language Models (LLMs) excel at understanding context,\nthey face limitations with long input sequences. Existing approaches that\ncombine SQL and LLMs typically rely on rigid, predefined work-flows, limiting\ntheir adaptability to complex queries. To address these issues, we introduce\nWeaver , a modular pipeline that dynamically integrates SQL and LLMs for\ntable-based question answering (TableQA). Weaver generates a flexible,\nstep-by-step plan that combines SQL for structured data retrieval with LLMs for\nsemantic processing. By decomposing complex queries into manageable subtasks,\nWeaver improves accuracy and generalization. Our experiments show that Weaver\nconsistently outperforms state-of-the-art methods across four TableQA datasets,\nreducing both API calls and error rates."}
{"id": "2505.18441", "pdf": "https://arxiv.org/pdf/2505.18441", "abs": "https://arxiv.org/abs/2505.18441", "authors": ["Romeo Valentin", "Sydney M. Katz", "Vincent Vanhoucke", "Mykel J. Kochenderfer"], "title": "DB-KSVD: Scalable Alternating Optimization for Disentangling High-Dimensional Embedding Spaces", "categories": ["cs.LG", "cs.MS", "stat.AP"], "comment": "9 pages + 4 pages appendix", "summary": "Dictionary learning has recently emerged as a promising approach for\nmechanistic interpretability of large transformer models. Disentangling\nhigh-dimensional transformer embeddings, however, requires algorithms that\nscale to high-dimensional data with large sample sizes. Recent work has\nexplored sparse autoencoders (SAEs) for this problem. However, SAEs use a\nsimple linear encoder to solve the sparse encoding subproblem, which is known\nto be NP-hard. It is therefore interesting to understand whether this structure\nis sufficient to find good solutions to the dictionary learning problem or if a\nmore sophisticated algorithm could find better solutions. In this work, we\npropose Double-Batch KSVD (DB-KSVD), a scalable dictionary learning algorithm\nthat adapts the classic KSVD algorithm. DB-KSVD is informed by the rich\ntheoretical foundations of KSVD but scales to datasets with millions of samples\nand thousands of dimensions. We demonstrate the efficacy of DB-KSVD by\ndisentangling embeddings of the Gemma-2-2B model and evaluating on six metrics\nfrom the SAEBench benchmark, where we achieve competitive results when compared\nto established approaches based on SAEs. By matching SAE performance with an\nentirely different optimization approach, our results suggest that (i) SAEs do\nfind strong solutions to the dictionary learning problem and (ii) that\ntraditional optimization approaches can be scaled to the required problem\nsizes, offering a promising avenue for further research. We provide an\nimplementation of DB-KSVD at https://github.com/RomeoV/KSVD.jl."}
{"id": "2505.18614", "pdf": "https://arxiv.org/pdf/2505.18614", "abs": "https://arxiv.org/abs/2505.18614", "authors": ["Woohyun Cho", "Youngmin Kim", "Sunghyun Lee", "Youngjae Yu"], "title": "MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song Translation", "categories": ["cs.CL", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "comment": "28 pages, 8 figures", "summary": "Lyrics translation requires both accurate semantic transfer and preservation\nof musical rhythm, syllabic structure, and poetic style. In animated musicals,\nthe challenge intensifies due to alignment with visual and auditory cues. We\nintroduce Multilingual Audio-Video Lyrics Benchmark for Animated Song\nTranslation (MAVL), the first multilingual, multimodal benchmark for singable\nlyrics translation. By integrating text, audio, and video, MAVL enables richer\nand more expressive translations than text-only approaches. Building on this,\nwe propose Syllable-Constrained Audio-Video LLM with Chain-of-Thought\nSylAVL-CoT, which leverages audio-video cues and enforces syllabic constraints\nto produce natural-sounding lyrics. Experimental results demonstrate that\nSylAVL-CoT significantly outperforms text-based models in singability and\ncontextual accuracy, emphasizing the value of multimodal, multilingual\napproaches for lyrics translation."}
{"id": "2505.19003", "pdf": "https://arxiv.org/pdf/2505.19003", "abs": "https://arxiv.org/abs/2505.19003", "authors": ["Tianming Liu", "Manzi Li", "Yafeng Yin"], "title": "Aligning LLM with human travel choices: a persona-based embedding learning approach", "categories": ["cs.AI"], "comment": "32 pages, 8 figures", "summary": "The advent of large language models (LLMs) presents new opportunities for\ntravel demand modeling. However, behavioral misalignment between LLMs and\nhumans presents obstacles for the usage of LLMs, and existing alignment methods\nare frequently inefficient or impractical given the constraints of typical\ntravel demand data. This paper introduces a novel framework for aligning LLMs\nwith human travel choice behavior, tailored to the current travel demand data\nsources. Our framework uses a persona inference and loading process to\ncondition LLMs with suitable prompts to enhance alignment. The inference step\nestablishes a set of base personas from empirical data, and a learned persona\nloading function driven by behavioral embeddings guides the loading process. We\nvalidate our framework on the Swissmetro mode choice dataset, and the results\nshow that our proposed approach significantly outperformed baseline choice\nmodels and LLM-based simulation models in predicting both aggregate mode choice\nshares and individual choice outcomes. Furthermore, we showcase that our\nframework can generate insights on population behavior through interpretable\nparameters. Overall, our research offers a more adaptable, interpretable, and\nresource-efficient pathway to robust LLM-based travel behavior simulation,\npaving the way to integrate LLMs into travel demand modeling practice in the\nfuture."}
{"id": "2505.18442", "pdf": "https://arxiv.org/pdf/2505.18442", "abs": "https://arxiv.org/abs/2505.18442", "authors": ["Zhining Liu", "Ze Yang", "Xiao Lin", "Ruizhong Qiu", "Tianxin Wei", "Yada Zhu", "Hendrik Hamann", "Jingrui He", "Hanghang Tong"], "title": "Breaking Silos: Adaptive Model Fusion Unlocks Better Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by ICML 2025. 22 pages, 6 Figures, 12 tables", "summary": "Time-series forecasting plays a critical role in many real-world\napplications. Although increasingly powerful models have been developed and\nachieved superior results on benchmark datasets, through a fine-grained\nsample-level inspection, we find that (i) no single model consistently\noutperforms others across different test samples, but instead (ii) each model\nexcels in specific cases. These findings prompt us to explore how to adaptively\nleverage the distinct strengths of various forecasting models for different\nsamples. We introduce TimeFuse, a framework for collective time-series\nforecasting with sample-level adaptive fusion of heterogeneous models. TimeFuse\nutilizes meta-features to characterize input time series and trains a learnable\nfusor to predict optimal model fusion weights for any given input. The fusor\ncan leverage samples from diverse datasets for joint training, allowing it to\nadapt to a wide variety of temporal patterns and thus generalize to new inputs,\neven from unseen datasets. Extensive experiments demonstrate the effectiveness\nof TimeFuse in various long-/short-term forecasting tasks, achieving\nnear-universal improvement over the state-of-the-art individual models. Code is\navailable at https://github.com/ZhiningLiu1998/TimeFuse."}
{"id": "2505.18630", "pdf": "https://arxiv.org/pdf/2505.18630", "abs": "https://arxiv.org/abs/2505.18630", "authors": ["Zhihao Jia", "Mingyi Jia", "Junwen Duan", "Jianxin Wang"], "title": "DDO: Dual-Decision Optimization via Multi-Agent Collaboration for LLM-Based Medical Consultation", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "17 pages, 4 figures", "summary": "Large Language Models (LLMs) demonstrate strong generalization and reasoning\nabilities, making them well-suited for complex decision-making tasks such as\nmedical consultation (MC). However, existing LLM-based methods often fail to\ncapture the dual nature of MC, which entails two distinct sub-tasks: symptom\ninquiry, a sequential decision-making process, and disease diagnosis, a\nclassification problem. This mismatch often results in ineffective symptom\ninquiry and unreliable disease diagnosis. To address this, we propose\n\\textbf{DDO}, a novel LLM-based framework that performs\n\\textbf{D}ual-\\textbf{D}ecision \\textbf{O}ptimization by decoupling and\nindependently optimizing the the two sub-tasks through a collaborative\nmulti-agent workflow. Experiments on three real-world MC datasets show that DDO\nconsistently outperforms existing LLM-based approaches and achieves competitive\nperformance with state-of-the-art generation-based methods, demonstrating its\neffectiveness in the MC task."}
{"id": "2505.19030", "pdf": "https://arxiv.org/pdf/2505.19030", "abs": "https://arxiv.org/abs/2505.19030", "authors": ["Wenhao Liu", "Zhengkang Guo", "Mingchen Xie", "Jingwen Xu", "Zisu Huang", "Muzhao Tian", "Jianhan Xu", "Muling Wu", "Xiaohua Wang", "Changze Lv", "He-Da Wang", "Hu Yao", "Xiaoqing Zheng", "Xuanjing Huang"], "title": "RECAST: Strengthening LLMs' Complex Instruction Following with Constraint-Verifiable Data", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly expected to tackle complex\ntasks, driven by their expanding applications and users' growing proficiency in\ncrafting sophisticated prompts. However, as the number of explicitly stated\nrequirements increases (particularly more than 10 constraints), LLMs often\nstruggle to accurately follow such complex instructions. To address this\nchallenge, we propose RECAST, a novel framework for synthesizing datasets where\neach example incorporates far more constraints than those in existing\nbenchmarks. These constraints are extracted from real-world prompt-response\npairs to ensure practical relevance. RECAST enables automatic verification of\nconstraint satisfaction via rule-based validators for quantitative constraints\nand LLM-based validators for qualitative ones. Using this framework, we\nconstruct RECAST-30K, a large-scale, high-quality dataset comprising 30k\ninstances spanning 15 constraint types. Experimental results demonstrate that\nmodels fine-tuned on RECAST-30K show substantial improvements in following\ncomplex instructions. Moreover, the verifiability provided by RECAST enables\nthe design of reward functions for reinforcement learning, which further boosts\nmodel performance on complex and challenging tasks."}
{"id": "2505.18447", "pdf": "https://arxiv.org/pdf/2505.18447", "abs": "https://arxiv.org/abs/2505.18447", "authors": ["Chi Zhang", "Ziying Jia", "George K. Atia", "Sihong He", "Yue Wang"], "title": "Pessimism Principle Can Be Effective: Towards a Framework for Zero-Shot Transfer Reinforcement Learning", "categories": ["cs.LG"], "comment": "Accepted to ICML 2025", "summary": "Transfer reinforcement learning aims to derive a near-optimal policy for a\ntarget environment with limited data by leveraging abundant data from related\nsource domains. However, it faces two key challenges: the lack of performance\nguarantees for the transferred policy, which can lead to undesired actions, and\nthe risk of negative transfer when multiple source domains are involved. We\npropose a novel framework based on the pessimism principle, which constructs\nand optimizes a conservative estimation of the target domain's performance. Our\nframework effectively addresses the two challenges by providing an optimized\nlower bound on target performance, ensuring safe and reliable decisions, and by\nexhibiting monotonic improvement with respect to the quality of the source\ndomains, thereby avoiding negative transfer. We construct two types of\nconservative estimations, rigorously characterize their effectiveness, and\ndevelop efficient distributed algorithms with convergence guarantees. Our\nframework provides a theoretically sound and practically robust solution for\ntransfer learning in reinforcement learning."}
{"id": "2505.18638", "pdf": "https://arxiv.org/pdf/2505.18638", "abs": "https://arxiv.org/abs/2505.18638", "authors": ["Md. Tanzib Hosain", "Rajan Das Gupta", "Md. Kishor Morol"], "title": "Multilingual Question Answering in Low-Resource Settings: A Dzongkha-English Benchmark for Foundation Models", "categories": ["cs.CL"], "comment": "24 pages, 20 figures", "summary": "In this work, we provide DZEN, a dataset of parallel Dzongkha and English\ntest questions for Bhutanese middle and high school students. The over 5K\nquestions in our collection span a variety of scientific topics and include\nfactual, application, and reasoning-based questions. We use our parallel\ndataset to test a number of Large Language Models (LLMs) and find a significant\nperformance difference between the models in English and Dzongkha. We also look\nat different prompting strategies and discover that Chain-of-Thought (CoT)\nprompting works well for reasoning questions but less well for factual ones. We\nalso find that adding English translations enhances the precision of Dzongkha\nquestion responses. Our results point to exciting avenues for further study to\nimprove LLM performance in Dzongkha and, more generally, in low-resource\nlanguages. We release the dataset at:\nhttps://github.com/kraritt/llm_dzongkha_evaluation."}
{"id": "2505.19075", "pdf": "https://arxiv.org/pdf/2505.19075", "abs": "https://arxiv.org/abs/2505.19075", "authors": ["Jaemin Kim", "Hangeol Chang", "Hyunmin Hwang", "Choonghan Kim", "Jong Chul Ye"], "title": "Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for Frozen LLMs", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "22 pages", "summary": "Large Language Models (LLMs) have demonstrated remarkable general\ncapabilities, but enhancing skills such as reasoning often demands substantial\ncomputational resources and may compromise their generalization. While\nParameter-Efficient Fine-Tuning (PEFT) methods offer a more resource-conscious\nalternative, they typically requires retraining for each LLM backbone due to\narchitectural dependencies. To address these challenges, here we propose\nUniversal Reasoner (UniR) - a single, lightweight, composable, and\nplug-and-play reasoning module that can be used with any frozen LLM to endow it\nwith specialized reasoning capabilities. Specifically, UniR decomposes the\nreward into a standalone reasoning module that is trained independently using\npredefined rewards, effectively translating trajectory-level signals into\ntoken-level guidance. Once trained, UniR can be combined with any frozen LLM at\ninference time by simply adding its output logits to those of the LLM backbone.\nThis additive structure naturally enables modular composition: multiple UniR\nmodules trained for different tasks can be jointly applied by summing their\nlogits, enabling complex reasoning via composition. Experimental results on\nmathematical reasoning and machine translation tasks show that UniR\nsignificantly outperforms \\add{existing baseline fine-tuning methods using the\nLlama3.2 model}. Furthermore, UniR demonstrates strong weak-to-strong\ngeneralization: reasoning modules trained on smaller models effectively guide\nmuch larger LLMs. This makes UniR a cost-efficient, adaptable, and robust\nsolution for enhancing reasoning in LLMs without compromising their core\ncapabilities. Code is open-sourced at https://github.com/hangeol/UniR"}
{"id": "2505.18451", "pdf": "https://arxiv.org/pdf/2505.18451", "abs": "https://arxiv.org/abs/2505.18451", "authors": ["Toshiaki Koike-Akino", "Jing Liu", "Ye Wang"], "title": "$$-MoE: Test-Time Pruning as Micro-Grained Mixture-of-Experts", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "10 pages, 4 figures", "summary": "To tackle the huge computational demand of large foundation models,\nactivation-aware compression techniques without retraining have been\nintroduced. However, since these rely on calibration data, domain shift may\narise for unknown downstream tasks. With a computationally efficient\ncalibration, activation-aware pruning can be executed for every prompt\nadaptively, yet achieving reduced complexity at inference. We formulate it as a\nmixture of micro-experts, called $\\mu$-MoE. Several experiments demonstrate\nthat $\\mu$-MoE can dynamically adapt to task/prompt-dependent structured\nsparsity on the fly."}
{"id": "2505.18642", "pdf": "https://arxiv.org/pdf/2505.18642", "abs": "https://arxiv.org/abs/2505.18642", "authors": ["Xiao Chen", "Sihang Zhou", "Ke Liang", "Xiaoyu Sun", "Xinwang Liu"], "title": "Skip-Thinking: Chunk-wise Chain-of-Thought Distillation Enable Smaller Language Models to Reason Better and Faster", "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-thought (CoT) distillation allows a large language model (LLM) to\nguide a small language model (SLM) in reasoning tasks. Existing methods train\nthe SLM to learn the long rationale in one iteration, resulting in two issues:\n1) Long rationales lead to a large token-level batch size during training,\nmaking gradients of core reasoning tokens (i.e., the token will directly affect\nthe correctness of subsequent reasoning) over-smoothed as they contribute a\ntiny fraction of the rationale. As a result, the SLM converges to sharp minima\nwhere it fails to grasp the reasoning logic. 2) The response is slow, as the\nSLM must generate a long rationale before reaching the answer. Therefore, we\npropose chunk-wise training (CWT), which uses a heuristic search to divide the\nrationale into internal semantically coherent chunks and focuses SLM on\nlearning from only one chunk per iteration. In this way, CWT naturally isolates\nnon-reasoning chunks that do not involve the core reasoning token (e.g.,\nsummary and transitional chunks) from the SLM learning for reasoning chunks,\nmaking the fraction of the core reasoning token increase in the corresponding\niteration. Based on CWT, skip-thinking training (STT) is proposed. STT makes\nthe SLM automatically skip non-reasoning medium chunks to reach the answer,\nimproving reasoning speed while maintaining accuracy. We validate our approach\non a variety of SLMs and multiple reasoning tasks."}
{"id": "2505.19092", "pdf": "https://arxiv.org/pdf/2505.19092", "abs": "https://arxiv.org/abs/2505.19092", "authors": ["Yang Zhang", "Wenxin Xu", "Xiaoyan Zhao", "Wenjie Wang", "Fuli Feng", "Xiangnan He", "Tat-Seng Chua"], "title": "Reinforced Latent Reasoning for LLM-based Recommendation", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive reasoning\ncapabilities in complex problem-solving tasks, sparking growing interest in\ntheir application to preference reasoning in recommendation systems. Existing\nmethods typically rely on fine-tuning with explicit chain-of-thought (CoT)\ndata. However, these methods face significant practical limitations due to (1)\nthe difficulty of obtaining high-quality CoT data in recommendation and (2) the\nhigh inference latency caused by generating CoT reasoning. In this work, we\nexplore an alternative approach that shifts from explicit CoT reasoning to\ncompact, information-dense latent reasoning. This approach eliminates the need\nfor explicit CoT generation and improves inference efficiency, as a small set\nof latent tokens can effectively capture the entire reasoning process. Building\non this idea, we propose $\\textit{\\underline{R}einforced \\underline{Latent}\n\\underline{R}easoning for \\underline{R}ecommendation}$ (LatentR$^3$), a novel\nend-to-end training framework that leverages reinforcement learning (RL) to\noptimize latent reasoning without relying on any CoT data.LatentR$^3$ adopts a\ntwo-stage training strategy: first, supervised fine-tuning to initialize the\nlatent reasoning module, followed by pure RL training to encourage exploration\nthrough a rule-based reward design. Our RL implementation is based on a\nmodified GRPO algorithm, which reduces computational overhead during training\nand introduces continuous reward signals for more efficient learning. Extensive\nexperiments demonstrate that LatentR$^3$ enables effective latent reasoning\nwithout any direct supervision of the reasoning process, significantly\nimproving performance when integrated with different LLM-based recommendation\nmethods. Our codes are available at https://anonymous.4open.science/r/R3-A278/."}
{"id": "2505.18461", "pdf": "https://arxiv.org/pdf/2505.18461", "abs": "https://arxiv.org/abs/2505.18461", "authors": ["Morteza Karimzadeh", "Zhongying Wang", "James L. Crooks"], "title": "Performance and Generalizability Impacts of Incorporating Geolocation into Deep Learning for Dynamic PM2.5 Estimation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Deep learning models have demonstrated success in geospatial applications,\nyet quantifying the role of geolocation information in enhancing model\nperformance and geographic generalizability remains underexplored. A new\ngeneration of location encoders have emerged with the goal of capturing\nattributes present at any given location for downstream use in predictive\nmodeling. Being a nascent area of research, their evaluation has remained\nlargely limited to static tasks such as species distributions or average\ntemperature mapping. In this paper, we discuss and quantify the impact of\nincorporating geolocation into deep learning for a real-world application\ndomain that is characteristically dynamic (with fast temporal change) and\nspatially heterogeneous at high resolutions: estimating surface-level daily\nPM2.5 levels using remotely sensed and ground-level data. We build on a\nrecently published deep learning-based PM2.5 estimation model that achieves\nstate-of-the-art performance on data observed in the contiguous United States.\nWe examine three approaches for incorporating geolocation: excluding\ngeolocation as a baseline, using raw geographic coordinates, and leveraging\npretrained location encoders. We evaluate each approach under within-region\n(WR) and out-of-region (OoR) evaluation scenarios. Aggregate performance\nmetrics indicate that while na\\\"ive incorporation of raw geographic coordinates\nimproves within-region performance by retaining the interpolative value of\ngeographic location, it can hinder generalizability across regions. In\ncontrast, pretrained location encoders like GeoCLIP enhance predictive\nperformance and geographic generalizability for both WR and OoR scenarios.\nHowever, qualitative analysis reveals artifact patterns caused by high-degree\nbasis functions and sparse upstream samples in certain areas, and ablation\nresults indicate varying performance among location encoders..."}
{"id": "2505.18651", "pdf": "https://arxiv.org/pdf/2505.18651", "abs": "https://arxiv.org/abs/2505.18651", "authors": ["Daniel J. Korchinski", "Dhruva Karkada", "Yasaman Bahri", "Matthieu Wyart"], "title": "On the Emergence of Linear Analogies in Word Embeddings", "categories": ["cs.CL", "cond-mat.dis-nn", "cs.LG"], "comment": "Main: 12 pages, 3 figures. Appendices: 8 pages, 7 figures", "summary": "Models such as Word2Vec and GloVe construct word embeddings based on the\nco-occurrence probability $P(i,j)$ of words $i$ and $j$ in text corpora. The\nresulting vectors $W_i$ not only group semantically similar words but also\nexhibit a striking linear analogy structure -- for example, $W_{\\text{king}} -\nW_{\\text{man}} + W_{\\text{woman}} \\approx W_{\\text{queen}}$ -- whose\ntheoretical origin remains unclear. Previous observations indicate that this\nanalogy structure: (i) already emerges in the top eigenvectors of the matrix\n$M(i,j) = P(i,j)/P(i)P(j)$, (ii) strengthens and then saturates as more\neigenvectors of $M (i, j)$, which controls the dimension of the embeddings, are\nincluded, (iii) is enhanced when using $\\log M(i,j)$ rather than $M(i,j)$, and\n(iv) persists even when all word pairs involved in a specific analogy relation\n(e.g., king-queen, man-woman) are removed from the corpus. To explain these\nphenomena, we introduce a theoretical generative model in which words are\ndefined by binary semantic attributes, and co-occurrence probabilities are\nderived from attribute-based interactions. This model analytically reproduces\nthe emergence of linear analogy structure and naturally accounts for properties\n(i)-(iv). It can be viewed as giving fine-grained resolution into the role of\neach additional embedding dimension. It is robust to various forms of noise and\nagrees well with co-occurrence statistics measured on Wikipedia and the analogy\nbenchmark introduced by Mikolov et al."}
{"id": "2505.19095", "pdf": "https://arxiv.org/pdf/2505.19095", "abs": "https://arxiv.org/abs/2505.19095", "authors": ["Runliang Niu", "Jinglong Ji", "Yi Chang", "Qi Wang"], "title": "ScreenExplorer: Training a Vision-Language Model for Diverse Exploration in Open GUI World", "categories": ["cs.AI"], "comment": null, "summary": "The rapid progress of large language models (LLMs) has sparked growing\ninterest in building Artificial General Intelligence (AGI) within Graphical\nUser Interface (GUI) environments. However, existing GUI agents based on LLMs\nor vision-language models (VLMs) often fail to generalize to novel environments\nand rely heavily on manually curated, diverse datasets. To overcome these\nlimitations, we introduce ScreenExplorer, a VLM trained via Group Relative\nPolicy Optimization(GRPO) in real, dynamic, and open-ended GUI environments.\nInnovatively, we introduced a world-model-based curiosity reward function to\nhelp the agent overcome the cold-start phase of exploration. Additionally,\ndistilling experience streams further enhances the model's exploration\ncapabilities. Our training framework enhances model exploration in open GUI\nenvironments, with trained models showing better environmental adaptation and\nsustained exploration compared to static deployment models. Our findings offer\na scalable pathway toward AGI systems with self-improving capabilities in\ncomplex interactive settings."}
{"id": "2505.18475", "pdf": "https://arxiv.org/pdf/2505.18475", "abs": "https://arxiv.org/abs/2505.18475", "authors": ["Mengran Li", "Pengyu Zhang", "Wenbin Xing", "Yijia Zheng", "Klim Zaporojets", "Junzhou Chen", "Ronghui Zhang", "Yong Zhang", "Siyuan Gong", "Jia Hu", "Xiaolei Ma", "Zhiyuan Liu", "Paul Groth", "Marcel Worring"], "title": "Using Large Language Models to Tackle Fundamental Challenges in Graph Learning: A Comprehensive Survey", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Graphs are a widely used paradigm for representing non-Euclidean data, with\napplications ranging from social network analysis to biomolecular prediction.\nConventional graph learning approaches typically rely on fixed structural\nassumptions or fully observed data, limiting their effectiveness in more\ncomplex, noisy, or evolving settings. Consequently, real-world graph data often\nviolates the assumptions of traditional graph learning methods, in particular,\nit leads to four fundamental challenges: (1) Incompleteness, real-world graphs\nhave missing nodes, edges, or attributes; (2) Imbalance, the distribution of\nthe labels of nodes or edges and their structures for real-world graphs are\nhighly skewed; (3) Cross-domain Heterogeneity, graphs from different domains\nexhibit incompatible feature spaces or structural patterns; and (4) Dynamic\nInstability, graphs evolve over time in unpredictable ways. Recent advances in\nLarge Language Models (LLMs) offer the potential to tackle these challenges by\nleveraging rich semantic reasoning and external knowledge. This survey provides\na comprehensive review of how LLMs can be integrated with graph learning to\naddress the aforementioned challenges. For each challenge, we review both\ntraditional solutions and modern LLM-driven approaches, highlighting how LLMs\ncontribute unique advantages. Finally, we discuss open research questions and\npromising future directions in this emerging interdisciplinary field. To\nsupport further exploration, we have curated a repository of recent advances on\ngraph learning challenges:\nhttps://github.com/limengran98/Awesome-Literature-Graph-Learning-Challenges."}
{"id": "2505.18653", "pdf": "https://arxiv.org/pdf/2505.18653", "abs": "https://arxiv.org/abs/2505.18653", "authors": ["Murathan Kurfal", "Shorouq Zahra", "Joakim Nivre", "Gabriele Messori"], "title": "Climate-Eval: A Comprehensive Benchmark for NLP Tasks Related to Climate Change", "categories": ["cs.CL"], "comment": "Accepted to ClimateNLP 2025@ACL", "summary": "Climate-Eval is a comprehensive benchmark designed to evaluate natural\nlanguage processing models across a broad range of tasks related to climate\nchange. Climate-Eval aggregates existing datasets along with a newly developed\nnews classification dataset, created specifically for this release. This\nresults in a benchmark of 25 tasks based on 13 datasets, covering key aspects\nof climate discourse, including text classification, question answering, and\ninformation extraction. Our benchmark provides a standardized evaluation suite\nfor systematically assessing the performance of large language models (LLMs) on\nthese tasks. Additionally, we conduct an extensive evaluation of open-source\nLLMs (ranging from 2B to 70B parameters) in both zero-shot and few-shot\nsettings, analyzing their strengths and limitations in the domain of climate\nchange."}
{"id": "2505.19099", "pdf": "https://arxiv.org/pdf/2505.19099", "abs": "https://arxiv.org/abs/2505.19099", "authors": ["Kun Xiang", "Heng Li", "Terry Jingchen Zhang", "Yinya Huang", "Zirong Liu", "Peixin Qu", "Jixi He", "Jiaqi Chen", "Yu-Jie Yuan", "Jianhua Han", "Hang Xu", "Hanhui Li", "Mrinmaya Sachan", "Xiaodan Liang"], "title": "SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics Reasoning", "categories": ["cs.AI"], "comment": "46 pages", "summary": "We present SeePhys, a large-scale multimodal benchmark for LLM reasoning\ngrounded in physics questions ranging from middle school to PhD qualifying\nexams. The benchmark covers 7 fundamental domains spanning the physics\ndiscipline, incorporating 21 categories of highly heterogeneous diagrams. In\ncontrast to prior works where visual elements mainly serve auxiliary purposes,\nour benchmark features a substantial proportion of vision-essential problems\n(75\\%) that mandate visual information extraction for correct solutions.\nThrough extensive evaluation, we observe that even the most advanced visual\nreasoning models (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60\\% accuracy\non our benchmark. These results reveal fundamental challenges in current large\nlanguage models' visual understanding capabilities, particularly in: (i)\nestablishing rigorous coupling between diagram interpretation and physics\nreasoning, and (ii) overcoming their persistent reliance on textual cues as\ncognitive shortcuts."}
{"id": "2505.18485", "pdf": "https://arxiv.org/pdf/2505.18485", "abs": "https://arxiv.org/abs/2505.18485", "authors": ["Shengzhe Xu", "Nikhil Muralidhar", "Naren Ramakrishnan"], "title": "The Prompt is Mightier than the Example", "categories": ["cs.LG"], "comment": null, "summary": "Numerous recent prompt optimization approaches like chain-of-thought, have\nbeen demonstrated to significantly improve the quality of content generated by\nlarge language models (LLMs). In-context learning (ICL), a recent paradigm\nwhere a few representative examples guide content generation has also led to\nstrong improvements in generation quality of LLM generated content. This idea\nhas been applied to great effect in synthetic tabular data generation, where\nLLMs, through effective use of ICL and prompt optimization, can generate data\nthat approximate samples from complex, heterogeneous distributions based on\nrepresentative examples. However, ensuring high-fidelity synthetic data often\nrequires a very large number of ICL examples which may be unavailable or costly\nto obtain. At the same time, as LLMs get larger and larger, their in-built\nprior knowledge becomes vast and can potentially substitute for specific data\nexamples. In this paper, we introduce Knowledge-Guided Prompting (KGP) as a new\nknob in prompt optimization and explore the ability of KGP-based prompt\noptimization to offset the cost of ICL. Specifically, we explore the question\n`how many examples can a prompt substitute for?' and explore knowledge-guided\nprompting (KGP) where domain knowledge, either inferred or available, is\nexplicitly injected into the prompt, reducing dependence on ICL examples. Our\nexperiments systematically explore the trade-off between ICL and KGP, revealing\nan empirical scaling law that quantifies how quality of generated synthetic\ndata varies with increasing domain knowledge and decreasing example count. Our\nresults demonstrate that knowledge-guided prompting can be a scalable\nalternative, or addition, to in-context examples, unlocking new approaches to\nsynthetic data generation."}
{"id": "2505.18658", "pdf": "https://arxiv.org/pdf/2505.18658", "abs": "https://arxiv.org/abs/2505.18658", "authors": ["Pankaj Kumar", "Subhankar Mishra"], "title": "Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have emerged as a promising cornerstone for the\ndevelopment of natural language processing (NLP) and artificial intelligence\n(AI). However, ensuring the robustness of LLMs remains a critical challenge. To\naddress these challenges and advance the field, this survey provides a\ncomprehensive overview of current studies in this area. First, we\nsystematically examine the nature of robustness in LLMs, including its\nconceptual foundations, the importance of consistent performance across diverse\ninputs, and the implications of failure modes in real-world applications. Next,\nwe analyze the sources of non-robustness, categorizing intrinsic model\nlimitations, data-driven vulnerabilities, and external adversarial factors that\ncompromise reliability. Following this, we review state-of-the-art mitigation\nstrategies, and then we discuss widely adopted benchmarks, emerging metrics,\nand persistent gaps in assessing real-world reliability. Finally, we synthesize\nfindings from existing surveys and interdisciplinary studies to highlight\ntrends, unresolved issues, and pathways for future research."}
{"id": "2505.19165", "pdf": "https://arxiv.org/pdf/2505.19165", "abs": "https://arxiv.org/abs/2505.19165", "authors": ["Debdeep Sanyal Umakanta Maharana", "Yash Sinha", "Hong Ming Tan", "Shirish Karande", "Mohan Kankanhalli", "Murari Mandal"], "title": "OrgAccess: A Benchmark for Role Based Access Control in Organization Scale LLMs", "categories": ["cs.AI"], "comment": "56 Pages", "summary": "Role-based access control (RBAC) and hierarchical structures are foundational\nto how information flows and decisions are made within virtually all\norganizations. As the potential of Large Language Models (LLMs) to serve as\nunified knowledge repositories and intelligent assistants in enterprise\nsettings becomes increasingly apparent, a critical, yet under explored,\nchallenge emerges: \\textit{can these models reliably understand and operate\nwithin the complex, often nuanced, constraints imposed by organizational\nhierarchies and associated permissions?} Evaluating this crucial capability is\ninherently difficult due to the proprietary and sensitive nature of real-world\ncorporate data and access control policies. We introduce a synthetic yet\nrepresentative \\textbf{OrgAccess} benchmark consisting of 40 distinct types of\npermissions commonly relevant across different organizational roles and levels.\nWe further create three types of permissions: 40,000 easy (1 permission),\n10,000 medium (3-permissions tuple), and 20,000 hard (5-permissions tuple) to\ntest LLMs' ability to accurately assess these permissions and generate\nresponses that strictly adhere to the specified hierarchical rules,\nparticularly in scenarios involving users with overlapping or conflicting\npermissions. Our findings reveal that even state-of-the-art LLMs struggle\nsignificantly to maintain compliance with role-based structures, even with\nexplicit instructions, with their performance degrades further when navigating\ninteractions involving two or more conflicting permissions. Specifically, even\n\\textbf{GPT-4.1 only achieves an F1-Score of 0.27 on our hardest benchmark}.\nThis demonstrates a critical limitation in LLMs' complex rule following and\ncompositional reasoning capabilities beyond standard factual or STEM-based\nbenchmarks, opening up a new paradigm for evaluating their fitness for\npractical, structured environments."}
{"id": "2505.18488", "pdf": "https://arxiv.org/pdf/2505.18488", "abs": "https://arxiv.org/abs/2505.18488", "authors": ["Yanxiang Zhang", "Zheng Xu", "Shanshan Wu", "Yuanbo Zhang", "Daniel Ramage"], "title": "Synthesizing and Adapting Error Correction Data for Mobile Large Language Model Applications", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ACL Industry", "summary": "Error correction is an important capability when applying large language\nmodels (LLMs) to facilitate user typing on mobile devices. In this paper, we\nuse LLMs to synthesize a high-quality dataset of error correction pairs to\nevaluate and improve LLMs for mobile applications. We first prompt LLMs with\nerror correction domain knowledge to build a scalable and reliable addition to\nthe existing data synthesis pipeline. We then adapt the synthetic data\ndistribution to match the mobile application domain by reweighting the samples.\nThe reweighting model is learnt by predicting (a handful of) live A/B test\nmetrics when deploying LLMs in production, given the LLM performance on offline\nevaluation data and scores from a small privacy-preserving on-device language\nmodel. Finally, we present best practices for mixing our synthetic data with\nother data sources to improve model performance on error correction in both\noffline evaluation and production live A/B testing."}
{"id": "2505.18673", "pdf": "https://arxiv.org/pdf/2505.18673", "abs": "https://arxiv.org/abs/2505.18673", "authors": ["Zixiang Xu", "Yanbo Wang", "Yue Huang", "Xiuying Chen", "Jieyu Zhao", "Meng Jiang", "Xiangliang Zhang"], "title": "Cross-Lingual Pitfalls: Automatic Probing Cross-Lingual Weakness of Multilingual Large Language Models", "categories": ["cs.CL"], "comment": "ACL 2025. Code available at\n  https://github.com/xzx34/Cross-Lingual-Pitfalls", "summary": "Large Language Models (LLMs) have achieved remarkable success in Natural\nLanguage Processing (NLP), yet their cross-lingual performance consistency\nremains a significant challenge. This paper introduces a novel methodology for\nefficiently identifying inherent cross-lingual weaknesses in LLMs. Our approach\nleverages beam search and LLM-based simulation to generate bilingual question\npairs that expose performance discrepancies between English and target\nlanguages. We construct a new dataset of over 6,000 bilingual pairs across 16\nlanguages using this methodology, demonstrating its effectiveness in revealing\nweaknesses even in state-of-the-art models. The extensive experiments\ndemonstrate that our method precisely and cost-effectively pinpoints\ncross-lingual weaknesses, consistently revealing over 50\\% accuracy drops in\ntarget languages across a wide range of models. Moreover, further experiments\ninvestigate the relationship between linguistic similarity and cross-lingual\nweaknesses, revealing that linguistically related languages share similar\nperformance patterns and benefit from targeted post-training. Code is available\nat https://github.com/xzx34/Cross-Lingual-Pitfalls."}
{"id": "2505.19167", "pdf": "https://arxiv.org/pdf/2505.19167", "abs": "https://arxiv.org/abs/2505.19167", "authors": ["Thomas P. Kehler", "Scott E. Page", "Alex Pentland", "Martin Reeves", "John Seely Brown"], "title": "Amplifying Human Creativity and Problem Solving with AI Through Generative Collective Intelligence", "categories": ["cs.AI"], "comment": null, "summary": "We propose a new framework for human-AI collaboration that amplifies the\ndistinct capabilities of both. This framework, which we call Generative\nCollective Intelligence (GCI), shifts AI to the group/social level and employs\nAI in dual roles: as interactive agents and as technology that accumulates,\norganizes, and leverages knowledge. By creating a cognitive bridge between\nhuman reasoning and AI models, GCI can overcome the limitations of purely\nalgorithmic approaches to problem-solving and decision-making. The framework\ndemonstrates how AI can be reframed as a social and cultural technology that\nenables groups to solve complex problems through structured collaboration that\ntranscends traditional communication barriers. We describe the mathematical\nfoundations of GCI based on comparative judgment and minimum regret principles,\nand illustrate its applications across domains including climate adaptation,\nhealthcare transformation, and civic participation. By combining human\ncreativity with AI's computational capabilities, GCI offers a promising\napproach to addressing complex societal challenges that neither human or\nmachines can solve alone."}
{"id": "2505.18494", "pdf": "https://arxiv.org/pdf/2505.18494", "abs": "https://arxiv.org/abs/2505.18494", "authors": ["Zihao Peng", "Jiandian Zeng", "Boyuan Li", "Guo Li", "Shengbo Chen", "Tian Wang"], "title": "FedHL: Federated Learning for Heterogeneous Low-Rank Adaptation via Unbiased Aggregation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated Learning (FL) facilitates the fine-tuning of Foundation Models\n(FMs) using distributed data sources, with Low-Rank Adaptation (LoRA) gaining\npopularity due to its low communication costs and strong performance. While\nrecent work acknowledges the benefits of heterogeneous LoRA in FL and\nintroduces flexible algorithms to support its implementation, our theoretical\nanalysis reveals a critical gap: existing methods lack formal convergence\nguarantees due to parameter truncation and biased gradient updates.\nSpecifically, adapting client-specific LoRA ranks necessitates truncating\nglobal parameters, which introduces inherent truncation errors and leads to\nsubsequent inaccurate gradient updates that accumulate over training rounds,\nultimately degrading performance. To address the above issues, we propose\n\\textbf{FedHL}, a simple yet effective \\textbf{Fed}erated Learning framework\ntailored for \\textbf{H}eterogeneous \\textbf{L}oRA. By leveraging the full-rank\nglobal model as a calibrated aggregation basis, FedHL eliminates the direct\ntruncation bias from initial alignment with client-specific ranks. Furthermore,\nwe derive the theoretically optimal aggregation weights by minimizing the\ngradient drift term in the convergence upper bound. Our analysis shows that\nFedHL guarantees $\\mathcal{O}(1/\\sqrt{T})$ convergence rate, and experiments on\nmultiple real-world datasets demonstrate a 1-3\\% improvement over several\nstate-of-the-art methods."}
{"id": "2505.18677", "pdf": "https://arxiv.org/pdf/2505.18677", "abs": "https://arxiv.org/abs/2505.18677", "authors": ["Eric Chamoun", "Nedjma Ousidhoum", "Michael Schlichtkrull", "Andreas Vlachos"], "title": "Social Good or Scientific Curiosity? Uncovering the Research Framing Behind NLP Artefacts", "categories": ["cs.CL"], "comment": null, "summary": "Clarifying the research framing of NLP artefacts (e.g., models, datasets,\netc.) is crucial to aligning research with practical applications. Recent\nstudies manually analyzed NLP research across domains, showing that few papers\nexplicitly identify key stakeholders, intended uses, or appropriate contexts.\nIn this work, we propose to automate this analysis, developing a\nthree-component system that infers research framings by first extracting key\nelements (means, ends, stakeholders), then linking them through interpretable\nrules and contextual reasoning. We evaluate our approach on two domains:\nautomated fact-checking using an existing dataset, and hate speech detection\nfor which we annotate a new dataset-achieving consistent improvements over\nstrong LLM baselines. Finally, we apply our system to recent automated\nfact-checking papers and uncover three notable trends: a rise in vague or\nunderspecified research goals, increased emphasis on scientific exploration\nover application, and a shift toward supporting human fact-checkers rather than\npursuing full automation."}
{"id": "2505.19173", "pdf": "https://arxiv.org/pdf/2505.19173", "abs": "https://arxiv.org/abs/2505.19173", "authors": ["Debdeep Sanyal", "Agniva Maiti", "Umakanta Maharana", "Dhruv Kumar", "Ankur Mali", "C. Lee Giles", "Murari Mandal"], "title": "Investigating Pedagogical Teacher and Student LLM Agents: Genetic Adaptation Meets Retrieval Augmented Generation Across Learning Style", "categories": ["cs.AI"], "comment": "38 Pages", "summary": "Effective teaching requires adapting instructional strategies to accommodate\nthe diverse cognitive and behavioral profiles of students, a persistent\nchallenge in education and teacher training. While Large Language Models (LLMs)\noffer promise as tools to simulate such complex pedagogical environments,\ncurrent simulation frameworks are limited in two key respects: (1) they often\nreduce students to static knowledge profiles, and (2) they lack adaptive\nmechanisms for modeling teachers who evolve their strategies in response to\nstudent feedback. To address these gaps, \\textbf{we introduce a novel\nsimulation framework that integrates LLM-based heterogeneous student agents\nwith a self-optimizing teacher agent}. The teacher agent's pedagogical policy\nis dynamically evolved using a genetic algorithm, allowing it to discover and\nrefine effective teaching strategies based on the aggregate performance of\ndiverse learners. In addition, \\textbf{we propose Persona-RAG}, a Retrieval\nAugmented Generation module that enables student agents to retrieve knowledge\ntailored to their individual learning styles. Persona-RAG preserves the\nretrieval accuracy of standard RAG baselines while enhancing personalization,\nan essential factor in modeling realistic educational scenarios. Through\nextensive experiments, we demonstrate how our framework supports the emergence\nof distinct and interpretable teaching patterns when interacting with varied\nstudent populations. Our results highlight the potential of LLM-driven\nsimulations to inform adaptive teaching practices and provide a testbed for\ntraining human educators in controlled, data-driven environments."}
{"id": "2505.18495", "pdf": "https://arxiv.org/pdf/2505.18495", "abs": "https://arxiv.org/abs/2505.18495", "authors": ["Chen-Hao Chao", "Wei-Fang Sun", "Hanwen Liang", "Chun-Yi Lee", "Rahul G. Krishnan"], "title": "Beyond Masked and Unmasked: Discrete Diffusion Models via Partial Masking", "categories": ["cs.LG"], "comment": null, "summary": "Masked diffusion models (MDM) are powerful generative models for discrete\ndata that generate samples by progressively unmasking tokens in a sequence.\nEach token can take one of two states: masked or unmasked. We observe that\ntoken sequences often remain unchanged between consecutive sampling steps;\nconsequently, the model repeatedly processes identical inputs, leading to\nredundant computation. To address this inefficiency, we propose the Partial\nmasking scheme (Prime), which augments MDM by allowing tokens to take\nintermediate states interpolated between the masked and unmasked states. This\ndesign enables the model to make predictions based on partially observed token\ninformation, and facilitates a fine-grained denoising process. We derive a\nvariational training objective and introduce a simple architectural design to\naccommodate intermediate-state inputs. Our method demonstrates superior\nperformance across a diverse set of generative modeling tasks. On text data, it\nachieves a perplexity of 15.36 on OpenWebText, outperforming previous MDM\n(21.52), autoregressive models (17.54), and their hybrid variants (17.58),\nwithout relying on an autoregressive formulation. On image data, it attains\ncompetitive FID scores of 3.26 on CIFAR-10 and 6.98 on ImageNet-32, comparable\nto leading continuous generative models."}
{"id": "2505.18683", "pdf": "https://arxiv.org/pdf/2505.18683", "abs": "https://arxiv.org/abs/2505.18683", "authors": ["Raphal Merx", "Hanna Suominen", "Lois Hong", "Nick Thieberger", "Trevor Cohn", "Ekaterina Vylomova"], "title": "TULUN: Transparent and Adaptable Low-resource Machine Translation", "categories": ["cs.CL"], "comment": null, "summary": "Machine translation (MT) systems that support low-resource languages often\nstruggle on specialized domains. While researchers have proposed various\ntechniques for domain adaptation, these approaches typically require model\nfine-tuning, making them impractical for non-technical users and small\norganizations. To address this gap, we propose Tulun, a versatile solution for\nterminology-aware translation, combining neural MT with large language model\n(LLM)-based post-editing guided by existing glossaries and translation\nmemories. Our open-source web-based platform enables users to easily create,\nedit, and leverage terminology resources, fostering a collaborative\nhuman-machine translation process that respects and incorporates domain\nexpertise while increasing MT accuracy. Evaluations show effectiveness in both\nreal-world and benchmark scenarios: on medical and disaster relief translation\ntasks for Tetun and Bislama, our system achieves improvements of 16.90-22.41\nChrF++ points over baseline MT systems. Across six low-resource languages on\nthe FLORES dataset, Tulun outperforms both standalone MT and LLM approaches,\nachieving an average improvement of 2.8 ChrF points over NLLB-54B."}
{"id": "2505.19195", "pdf": "https://arxiv.org/pdf/2505.19195", "abs": "https://arxiv.org/abs/2505.19195", "authors": ["Shaohao Rui", "Haoyang Su", "Jinyi Xiang", "Lian-Ming Wu", "Xiaosong Wang"], "title": "CardioCoT: Hierarchical Reasoning for Multimodal Survival Analysis", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Accurate prediction of major adverse cardiovascular events recurrence risk in\nacute myocardial infarction patients based on postoperative cardiac MRI and\nassociated clinical notes is crucial for precision treatment and personalized\nintervention. Existing methods primarily focus on risk stratification\ncapability while overlooking the need for intermediate robust reasoning and\nmodel interpretability in clinical practice. Moreover, end-to-end risk\nprediction using LLM/VLM faces significant challenges due to data limitations\nand modeling complexity. To bridge this gap, we propose CardioCoT, a novel\ntwo-stage hierarchical reasoning-enhanced survival analysis framework designed\nto enhance both model interpretability and predictive performance. In the first\nstage, we employ an evidence-augmented self-refinement mechanism to guide\nLLM/VLMs in generating robust hierarchical reasoning trajectories based on\nassociated radiological findings. In the second stage, we integrate the\nreasoning trajectories with imaging data for risk model training and\nprediction. CardioCoT demonstrates superior performance in MACE recurrence risk\nprediction while providing interpretable reasoning processes, offering valuable\ninsights for clinical decision-making."}
{"id": "2505.18499", "pdf": "https://arxiv.org/pdf/2505.18499", "abs": "https://arxiv.org/abs/2505.18499", "authors": ["Xiaojun Guo", "Ang Li", "Yifei Wang", "Stefanie Jegelka", "Yisen Wang"], "title": "G1: Teaching LLMs to Reason on Graphs with Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Although Large Language Models (LLMs) have demonstrated remarkable progress,\ntheir proficiency in graph-related tasks remains notably limited, hindering the\ndevelopment of truly general-purpose models. Previous attempts, including\npretraining graph foundation models or employing supervised fine-tuning, often\nface challenges such as the scarcity of large-scale, universally represented\ngraph data. We introduce G1, a simple yet effective approach demonstrating that\nReinforcement Learning (RL) on synthetic graph-theoretic tasks can\nsignificantly scale LLMs' graph reasoning abilities. To enable RL training, we\ncurate Erd\\~os, the largest graph reasoning dataset to date comprising 50\ndiverse graph-theoretic tasks of varying difficulty levels, 100k training data\nand 5k test data, all drived from real-world graphs. With RL on Erd\\~os, G1\nobtains substantial improvements in graph reasoning, where our finetuned 3B\nmodel even outperforms Qwen2.5-72B-Instruct (24x size). RL-trained models also\nshow strong zero-shot generalization to unseen tasks, domains, and graph\nencoding schemes, including other graph-theoretic benchmarks as well as\nreal-world node classification and link prediction tasks, without compromising\ngeneral reasoning abilities. Our findings offer an efficient, scalable path for\nbuilding strong graph reasoners by finetuning LLMs with RL on graph-theoretic\ntasks, which combines the strengths of pretrained LLM capabilities with\nabundant, automatically generated synthetic data, suggesting that LLMs possess\ngraph understanding abilities that RL can elicit successfully."}
{"id": "2505.18685", "pdf": "https://arxiv.org/pdf/2505.18685", "abs": "https://arxiv.org/abs/2505.18685", "authors": ["Zhihao Zhang", "Yiran Zhang", "Xiyue Zhou", "Liting Huang", "Imran Razzak", "Preslav Nakov", "Usman Naseem"], "title": "From Generation to Detection: A Multimodal Multi-Task Dataset for Benchmarking Health Misinformation", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Infodemics and health misinformation have significant negative impact on\nindividuals and society, exacerbating confusion and increasing hesitancy in\nadopting recommended health measures. Recent advancements in generative AI,\ncapable of producing realistic, human like text and images, have significantly\naccelerated the spread and expanded the reach of health misinformation,\nresulting in an alarming surge in its dissemination. To combat the infodemics,\nmost existing work has focused on developing misinformation datasets from\nsocial media and fact checking platforms, but has faced limitations in topical\ncoverage, inclusion of AI generation, and accessibility of raw content. To\naddress these issues, we present MM Health, a large scale multimodal\nmisinformation dataset in the health domain consisting of 34,746 news article\nencompassing both textual and visual information. MM Health includes\nhuman-generated multimodal information (5,776 articles) and AI generated\nmultimodal information (28,880 articles) from various SOTA generative AI\nmodels. Additionally, We benchmarked our dataset against three tasks\n(reliability checks, originality checks, and fine-grained AI detection)\ndemonstrating that existing SOTA models struggle to accurately distinguish the\nreliability and origin of information. Our dataset aims to support the\ndevelopment of misinformation detection across various health scenarios,\nfacilitating the detection of human and machine generated content at multimodal\nlevels."}
{"id": "2505.19197", "pdf": "https://arxiv.org/pdf/2505.19197", "abs": "https://arxiv.org/abs/2505.19197", "authors": ["Chanyeol Choi", "Jihoon Kwon", "Minjae Kim", "Juneha Hwang", "Minsoo Ha", "Chaewoon Kim", "Jaeseon Ha", "Suyeol Yun", "Jin Kim"], "title": "Structuring the Unstructured: A Multi-Agent System for Extracting and Querying Financial KPIs and Guidance", "categories": ["cs.AI"], "comment": "6 pages, FinIR'25", "summary": "Extracting structured and quantitative insights from unstructured financial\nfilings is essential in investment research, yet remains time-consuming and\nresource-intensive. Conventional approaches in practice rely heavily on\nlabor-intensive manual processes, limiting scalability and delaying the\nresearch workflow. In this paper, we propose an efficient and scalable method\nfor accurately extracting quantitative insights from unstructured financial\ndocuments, leveraging a multi-agent system composed of large language models.\nOur proposed multi-agent system consists of two specialized agents: the\n\\emph{Extraction Agent} and the \\emph{Text-to-SQL Agent}. The\n\\textit{Extraction Agent} automatically identifies key performance indicators\nfrom unstructured financial text, standardizes their formats, and verifies\ntheir accuracy. On the other hand, the \\textit{Text-to-SQL Agent} generates\nexecutable SQL statements from natural language queries, allowing users to\naccess structured data accurately without requiring familiarity with the\ndatabase schema. Through experiments, we demonstrate that our proposed system\neffectively transforms unstructured text into structured data accurately and\nenables precise retrieval of key information. First, we demonstrate that our\nsystem achieves approximately 95\\% accuracy in transforming financial filings\ninto structured data, matching the performance level typically attained by\nhuman annotators. Second, in a human evaluation of the retrieval task -- where\nnatural language queries are used to search information from structured data --\n91\\% of the responses were rated as correct by human evaluators. In both\nevaluations, our system generalizes well across financial document types,\nconsistently delivering reliable performance."}
{"id": "2505.18505", "pdf": "https://arxiv.org/pdf/2505.18505", "abs": "https://arxiv.org/abs/2505.18505", "authors": ["Yixuan Ma", "Kai Yi", "Pietro Lio", "Shi Jin", "Yu Guang Wang"], "title": "How Particle System Theory Enhances Hypergraph Message Passing", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Hypergraphs effectively model higher-order relationships in natural\nphenomena, capturing complex interactions beyond pairwise connections. We\nintroduce a novel hypergraph message passing framework inspired by interacting\nparticle systems, where hyperedges act as fields inducing shared node dynamics.\nBy incorporating attraction, repulsion, and Allen-Cahn forcing terms, particles\nof varying classes and features achieve class-dependent equilibrium, enabling\nseparability through the particle-driven message passing. We investigate both\nfirst-order and second-order particle system equations for modeling these\ndynamics, which mitigate over-smoothing and heterophily thus can capture\ncomplete interactions. The more stable second-order system permits deeper\nmessage passing. Furthermore, we enhance deterministic message passing with\nstochastic element to account for interaction uncertainties. We prove\ntheoretically that our approach mitigates over-smoothing by maintaining a\npositive lower bound on the hypergraph Dirichlet energy during propagation and\nthus to enable hypergraph message passing to go deep. Empirically, our models\ndemonstrate competitive performance on diverse real-world hypergraph node\nclassification tasks, excelling on both homophilic and heterophilic datasets."}
{"id": "2505.18688", "pdf": "https://arxiv.org/pdf/2505.18688", "abs": "https://arxiv.org/abs/2505.18688", "authors": ["Aleksandr Tsymbalov"], "title": "Large Language Models in the Task of Automatic Validation of Text Classifier Predictions", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Machine learning models for text classification are trained to predict a\nclass for a given text. To do this, training and validation samples must be\nprepared: a set of texts is collected, and each text is assigned a class. These\nclasses are usually assigned by human annotators with different expertise\nlevels, depending on the specific classification task. Collecting such samples\nfrom scratch is labor-intensive because it requires finding specialists and\ncompensating them for their work; moreover, the number of available specialists\nis limited, and their productivity is constrained by human factors. While it\nmay not be too resource-intensive to collect samples once, the ongoing need to\nretrain models (especially in incremental learning pipelines) to address data\ndrift (also called model drift) makes the data collection process crucial and\ncostly over the model's entire lifecycle. This paper proposes several\napproaches to replace human annotators with Large Language Models (LLMs) to\ntest classifier predictions for correctness, helping ensure model quality and\nsupport high-quality incremental learning."}
{"id": "2505.19213", "pdf": "https://arxiv.org/pdf/2505.19213", "abs": "https://arxiv.org/abs/2505.19213", "authors": ["Shaohao Rui", "Kaitao Chen", "Weijie Ma", "Xiaosong Wang"], "title": "Improving Medical Reasoning with Curriculum-Aware Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in reinforcement learning with verifiable, rule-based rewards\nhave greatly enhanced the reasoning capabilities and out-of-distribution\ngeneralization of VLMs/LLMs, obviating the need for manually crafted reasoning\nchains. Despite these promising developments in the general domain, their\ntranslation to medical imaging remains limited. Current medical reinforcement\nfine-tuning (RFT) methods predominantly focus on close-ended VQA, thereby\nrestricting the model's ability to engage in world knowledge retrieval and\nflexible task adaptation. More critically, these methods fall short of\naddressing the critical clinical demand for open-ended, reasoning-intensive\ndecision-making. To bridge this gap, we introduce \\textbf{MedCCO}, the first\nmultimodal reinforcement learning framework tailored for medical VQA that\nunifies close-ended and open-ended data within a curriculum-driven RFT\nparadigm. Specifically, MedCCO is initially fine-tuned on a diverse set of\nclose-ended medical VQA tasks to establish domain-grounded reasoning\ncapabilities, and is then progressively adapted to open-ended tasks to foster\ndeeper knowledge enhancement and clinical interpretability. We validate MedCCO\nacross eight challenging medical VQA benchmarks, spanning both close-ended and\nopen-ended settings. Experimental results show that MedCCO consistently\nenhances performance and generalization, achieving a 11.4\\% accuracy gain\nacross three in-domain tasks, and a 5.7\\% improvement on five out-of-domain\nbenchmarks. These findings highlight the promise of curriculum-guided RL in\nadvancing robust, clinically-relevant reasoning in medical multimodal language\nmodels."}
{"id": "2505.18511", "pdf": "https://arxiv.org/pdf/2505.18511", "abs": "https://arxiv.org/abs/2505.18511", "authors": ["Zheyan Li", "Yuantu Zhu", "Hao Ni", "Siran Li", "Bingguang Chen", "Qi Meng"], "title": "SPDEBench: An Extensive Benchmark for Learning Regular and Singular Stochastic PDEs", "categories": ["cs.LG", "math.AP", "physics.comp-ph"], "comment": null, "summary": "Stochastic Partial Differential Equations (SPDEs) driven by random noise play\na central role in modelling physical processes whose spatio-temporal dynamics\ncan be rough, such as turbulence flows, superconductors, and quantum dynamics.\nTo efficiently model these processes and make predictions, machine learning\n(ML)-based surrogate models are proposed, with their network architectures\nincorporating the spatio-temporal roughness in their design. However, it lacks\nan extensive and unified datasets for SPDE learning; especially, existing\ndatasets do not account for the computational error introduced by noise\nsampling and the necessary renormalization required for handling singular\nSPDEs. We thus introduce SPDEBench, which is designed to solve typical SPDEs of\nphysical significance (e.g., the $\\Phi^4_d$, wave, incompressible\nNavier--Stokes, and KdV equations) on 1D or 2D tori driven by white noise via\nML methods. New datasets for singular SPDEs based on the renormalization\nprocess have been constructed, and novel ML models achieving the best results\nto date have been proposed. In particular, we investigate the impact of\ncomputational error introduced by noise sampling and renormalization on the\nperformance comparison of ML models and highlight the importance of selecting\nhigh-quality test data for accurate evaluation. Results are benchmarked with\ntraditional numerical solvers and ML-based models, including FNO, NSPDE and\nDLR-Net, etc. It is shown that, for singular SPDEs, naively applying ML models\non data without specifying the numerical schemes can lead to significant errors\nand misleading conclusions. Our SPDEBench provides an open-source codebase that\nensures full reproducibility of benchmarking across a variety of SPDE datasets\nwhile offering the flexibility to incorporate new datasets and machine learning\nbaselines, making it a valuable resource for the community."}
{"id": "2505.18690", "pdf": "https://arxiv.org/pdf/2505.18690", "abs": "https://arxiv.org/abs/2505.18690", "authors": ["Guoxiu He", "Xin Song", "Futing Wang", "Aixin Sun"], "title": "Benchmarking and Rethinking Knowledge Editing for Large Language Models", "categories": ["cs.CL"], "comment": "arXiv admin note: text overlap with arXiv:2503.05212", "summary": "Knowledge editing aims to update the embedded knowledge within Large Language\nModels (LLMs). However, existing approaches, whether through parameter\nmodification or external memory integration, often suffer from inconsistent\nevaluation objectives and experimental setups. To address this gap, we conduct\na comprehensive benchmarking study. In addition to fact-level datasets, we\nintroduce more complex event-based datasets and general-purpose datasets drawn\nfrom other tasks. Our evaluation covers both instruction-tuned and\nreasoning-oriented LLMs, under a realistic autoregressive inference setting\nrather than teacher-forced decoding. Beyond single-edit assessments, we also\nevaluate multi-edit scenarios to better reflect practical demands. We employ\nfour evaluation dimensions, including portability, and compare all recent\nmethods against a simple and straightforward baseline named Selective\nContextual Reasoning (SCR). Empirical results reveal that parameter-based\nediting methods perform poorly under realistic conditions. In contrast, SCR\nconsistently outperforms them across all settings. This study offers new\ninsights into the limitations of current knowledge editing methods and\nhighlights the potential of context-based reasoning as a more robust\nalternative."}
{"id": "2505.19219", "pdf": "https://arxiv.org/pdf/2505.19219", "abs": "https://arxiv.org/abs/2505.19219", "authors": ["Shiyue Wang", "Haozheng Xu", "Yuhan Zhang", "Jingran Lin", "Changhong Lu", "Xiangfeng Wang", "Wenhao Li"], "title": "Where Paths Collide: A Comprehensive Survey of Classic and Learning-Based Multi-Agent Pathfinding", "categories": ["cs.AI", "cs.LG", "cs.MA", "math.CO"], "comment": "112 pages, 21 figures, 20 tables", "summary": "Multi-Agent Path Finding (MAPF) is a fundamental problem in artificial\nintelligence and robotics, requiring the computation of collision-free paths\nfor multiple agents navigating from their start locations to designated goals.\nAs autonomous systems become increasingly prevalent in warehouses, urban\ntransportation, and other complex environments, MAPF has evolved from a\ntheoretical challenge to a critical enabler of real-world multi-robot\ncoordination. This comprehensive survey bridges the long-standing divide\nbetween classical algorithmic approaches and emerging learning-based methods in\nMAPF research. We present a unified framework that encompasses search-based\nmethods (including Conflict-Based Search, Priority-Based Search, and Large\nNeighborhood Search), compilation-based approaches (SAT, SMT, CSP, ASP, and MIP\nformulations), and data-driven techniques (reinforcement learning, supervised\nlearning, and hybrid strategies). Through systematic analysis of experimental\npractices across 200+ papers, we uncover significant disparities in evaluation\nmethodologies, with classical methods typically tested on larger-scale\ninstances (up to 200 by 200 grids with 1000+ agents) compared to learning-based\napproaches (predominantly 10-100 agents). We provide a comprehensive taxonomy\nof evaluation metrics, environment types, and baseline selections, highlighting\nthe need for standardized benchmarking protocols. Finally, we outline promising\nfuture directions including mixed-motive MAPF with game-theoretic\nconsiderations, language-grounded planning with large language models, and\nneural solver architectures that combine the rigor of classical methods with\nthe flexibility of deep learning. This survey serves as both a comprehensive\nreference for researchers and a practical guide for deploying MAPF solutions in\nincreasingly complex real-world applications."}
{"id": "2505.18513", "pdf": "https://arxiv.org/pdf/2505.18513", "abs": "https://arxiv.org/abs/2505.18513", "authors": ["Weiwei Sun", "Haokun Liu", "Nikhil Kandpal", "Colin Raffel", "Yiming Yang"], "title": "Enhancing Training Data Attribution with Representational Optimization", "categories": ["cs.LG"], "comment": null, "summary": "Training data attribution (TDA) methods aim to measure how training data\nimpacts a model's predictions. While gradient-based attribution methods, such\nas influence functions, offer theoretical grounding, their computational costs\nmake them impractical for large-scale applications. Representation-based\napproaches are far more scalable, but typically rely on heuristic embeddings\nthat are not optimized for attribution, limiting their fidelity. To address\nthese challenges, we propose AirRep, a scalable, representation-based approach\nthat closes this gap by learning task-specific and model-aligned\nrepresentations optimized explicitly for TDA. AirRep introduces two key\ninnovations: a trainable encoder tuned for attribution quality, and an\nattention-based pooling mechanism that enables accurate estimation of\ngroup-wise influence. We train AirRep using a ranking objective over\nautomatically constructed training subsets labeled by their empirical effect on\ntarget predictions. Experiments on instruction-tuned LLMs demonstrate that\nAirRep achieves performance on par with state-of-the-art gradient-based\napproaches while being nearly two orders of magnitude more efficient at\ninference time. Further analysis highlights its robustness and generalization\nacross tasks and models. Our code is available at\nhttps://github.com/sunnweiwei/AirRep."}
{"id": "2505.18703", "pdf": "https://arxiv.org/pdf/2505.18703", "abs": "https://arxiv.org/abs/2505.18703", "authors": ["Gaurav Negi", "Dhairya Dalal", "Omnia Zayed", "Paul Buitelaar"], "title": "Towards Semantic Integration of Opinions: Unified Opinion Concepts Ontology and Extraction Task", "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces the Unified Opinion Concepts (UOC) ontology to\nintegrate opinions within their semantic context. The UOC ontology bridges the\ngap between the semantic representation of opinion across different\nformulations. It is a unified conceptualisation based on the facets of opinions\nstudied extensively in NLP and semantic structures described through symbolic\ndescriptions. We further propose the Unified Opinion Concept Extraction (UOCE)\ntask of extracting opinions from the text with enhanced expressivity.\nAdditionally, we provide a manually extended and re-annotated evaluation\ndataset for this task and tailored evaluation metrics to assess the adherence\nof extracted opinions to UOC semantics. Finally, we establish baseline\nperformance for the UOCE task using state-of-the-art generative models."}
{"id": "2505.19220", "pdf": "https://arxiv.org/pdf/2505.19220", "abs": "https://arxiv.org/abs/2505.19220", "authors": ["Chengbo He", "Bochao Zou", "Junliang Xing", "Jiansheng Chen", "Yuanchun Shi", "Huimin Ma"], "title": "DeCoDe: Defer-and-Complement Decision-Making via Decoupled Concept Bottleneck Models", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "In human-AI collaboration, a central challenge is deciding whether the AI\nshould handle a task, be deferred to a human expert, or be addressed through\ncollaborative effort. Existing Learning to Defer approaches typically make\nbinary choices between AI and humans, neglecting their complementary strengths.\nThey also lack interpretability, a critical property in high-stakes scenarios\nwhere users must understand and, if necessary, correct the model's reasoning.\nTo overcome these limitations, we propose Defer-and-Complement Decision-Making\nvia Decoupled Concept Bottleneck Models (DeCoDe), a concept-driven framework\nfor human-AI collaboration. DeCoDe makes strategy decisions based on\nhuman-interpretable concept representations, enhancing transparency throughout\nthe decision process. It supports three flexible modes: autonomous AI\nprediction, deferral to humans, and human-AI collaborative complementarity,\nselected via a gating network that takes concept-level inputs and is trained\nusing a novel surrogate loss that balances accuracy and human effort. This\napproach enables instance-specific, interpretable, and adaptive human-AI\ncollaboration. Experiments on real-world datasets demonstrate that DeCoDe\nsignificantly outperforms AI-only, human-only, and traditional deferral\nbaselines, while maintaining strong robustness and interpretability even under\nnoisy expert annotations."}
{"id": "2505.18514", "pdf": "https://arxiv.org/pdf/2505.18514", "abs": "https://arxiv.org/abs/2505.18514", "authors": ["Taeckyung Lee", "Sorn Chottananurak", "Junsu Kim", "Jinwoo Shin", "Taesik Gong", "Sung-Ju Lee"], "title": "Test-Time Adaptation with Binary Feedback", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to ICML 2025", "summary": "Deep learning models perform poorly when domain shifts exist between training\nand test data. Test-time adaptation (TTA) is a paradigm to mitigate this issue\nby adapting pre-trained models using only unlabeled test samples. However,\nexisting TTA methods can fail under severe domain shifts, while recent active\nTTA approaches requiring full-class labels are impractical due to high labeling\ncosts. To address this issue, we introduce a new setting of TTA with binary\nfeedback. This setting uses a few binary feedback inputs from annotators to\nindicate whether model predictions are correct, thereby significantly reducing\nthe labeling burden of annotators. Under the setting, we propose BiTTA, a novel\ndual-path optimization framework that leverages reinforcement learning to\nbalance binary feedback-guided adaptation on uncertain samples with\nagreement-based self-adaptation on confident predictions. Experiments show\nBiTTA achieves 13.3%p accuracy improvements over state-of-the-art baselines,\ndemonstrating its effectiveness in handling severe distribution shifts with\nminimal labeling effort. The source code is available at\nhttps://github.com/taeckyung/BiTTA."}
{"id": "2505.18708", "pdf": "https://arxiv.org/pdf/2505.18708", "abs": "https://arxiv.org/abs/2505.18708", "authors": ["Xu Zhang", "Kun Zhang", "Wenxin Ma", "Rongsheng Wang", "Chenxu Wu", "Yingtai Li", "S. Kevin Zhou"], "title": "A General Knowledge Injection Framework for ICD Coding", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "ICD Coding aims to assign a wide range of medical codes to a medical text\ndocument, which is a popular and challenging task in the healthcare domain. To\nalleviate the problems of long-tail distribution and the lack of annotations of\ncode-specific evidence, many previous works have proposed incorporating code\nknowledge to improve coding performance. However, existing methods often focus\non a single type of knowledge and design specialized modules that are complex\nand incompatible with each other, thereby limiting their scalability and\neffectiveness. To address this issue, we propose GKI-ICD, a novel, general\nknowledge injection framework that integrates three key types of knowledge,\nnamely ICD Description, ICD Synonym, and ICD Hierarchy, without specialized\ndesign of additional modules. The comprehensive utilization of the above\nknowledge, which exhibits both differences and complementarity, can effectively\nenhance the ICD coding performance. Extensive experiments on existing popular\nICD coding benchmarks demonstrate the effectiveness of GKI-ICD, which achieves\nthe state-of-the-art performance on most evaluation metrics. Code is available\nat https://github.com/xuzhang0112/GKI-ICD."}
{"id": "2505.19234", "pdf": "https://arxiv.org/pdf/2505.19234", "abs": "https://arxiv.org/abs/2505.19234", "authors": ["Jialong Zhou", "Lichao Wang", "Xiao Yang"], "title": "GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling", "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "The emergence of large language models (LLMs) enables the development of\nintelligent agents capable of engaging in complex and multi-turn dialogues.\nHowever, multi-agent collaboration face critical safety challenges, such as\nhallucination amplification and error injection and propagation. This paper\npresents GUARDIAN, a unified method for detecting and mitigating multiple\nsafety concerns in GUARDing Intelligent Agent collaboratioNs. By modeling the\nmulti-agent collaboration process as a discrete-time temporal attributed graph,\nGUARDIAN explicitly captures the propagation dynamics of hallucinations and\nerrors. The unsupervised encoder-decoder architecture incorporating an\nincremental training paradigm, learns to reconstruct node attributes and graph\nstructures from latent embeddings, enabling the identification of anomalous\nnodes and edges with unparalleled precision. Moreover, we introduce a graph\nabstraction mechanism based on the Information Bottleneck Theory, which\ncompresses temporal interaction graphs while preserving essential patterns.\nExtensive experiments demonstrate GUARDIAN's effectiveness in safeguarding LLM\nmulti-agent collaborations against diverse safety vulnerabilities, achieving\nstate-of-the-art accuracy with efficient resource utilization."}
{"id": "2505.18527", "pdf": "https://arxiv.org/pdf/2505.18527", "abs": "https://arxiv.org/abs/2505.18527", "authors": ["Yiqing Zhang", "Xiaozhong Liu", "Fabricio Murai"], "title": "CLaDMoP: Learning Transferrable Models from Successful Clinical Trials via LLMs", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted and to be published in KDD2025", "summary": "Many existing models for clinical trial outcome prediction are optimized\nusing task-specific loss functions on trial phase-specific data. While this\nscheme may boost prediction for common diseases and drugs, it can hinder\nlearning of generalizable representations, leading to more false\npositives/negatives. To address this limitation, we introduce CLaDMoP, a new\npre-training approach for clinical trial outcome prediction, alongside the\nSuccessful Clinical Trials dataset(SCT), specifically designed for this task.\nCLaDMoP leverages a Large Language Model-to encode trials' eligibility\ncriteria-linked to a lightweight Drug-Molecule branch through a novel\nmulti-level fusion technique. To efficiently fuse long embeddings across\nlevels, we incorporate a grouping block, drastically reducing computational\noverhead. CLaDMoP avoids reliance on task-specific objectives by pre-training\non a \"pair matching\" proxy task. Compared to established zero-shot and few-shot\nbaselines, our method significantly improves both PR-AUC and ROC-AUC,\nespecially for phase I and phase II trials. We further evaluate and perform\nablation on CLaDMoP after Parameter-Efficient Fine-Tuning, comparing it to\nstate-of-the-art supervised baselines, including MEXA-CTP, on the Trial Outcome\nPrediction(TOP) benchmark. CLaDMoP achieves up to 10.5% improvement in PR-AUC\nand 3.6% in ROC-AUC, while attaining comparable F1 score to MEXA-CTP,\nhighlighting its potential for clinical trial outcome prediction. Code and SCT\ndataset can be downloaded from https://github.com/murai-lab/CLaDMoP."}
{"id": "2505.18709", "pdf": "https://arxiv.org/pdf/2505.18709", "abs": "https://arxiv.org/abs/2505.18709", "authors": ["Sourav Kumar Das", "Md. Julkar Naeen", "MD. Jahidul Islam", "Md. Anisul Haque Sajeeb", "Narayan Ranjan Chakraborty", "Mayen Uddin Mojumdar"], "title": "Improving Bangla Linguistics: Advanced LSTM, Bi-LSTM, and Seq2Seq Models for Translating Sylheti to Modern Bangla", "categories": ["cs.CL", "cs.AI"], "comment": "2024 15th International Conference on Computing Communication and\n  Networking Technologies (ICCCNT)", "summary": "Bangla or Bengali is the national language of Bangladesh, people from\ndifferent regions don't talk in proper Bangla. Every division of Bangladesh has\nits own local language like Sylheti, Chittagong etc. In recent years some\npapers were published on Bangla language like sentiment analysis, fake news\ndetection and classifications, but a few of them were on Bangla languages. This\nresearch is for the local language and this particular paper is on Sylheti\nlanguage. It presented a comprehensive system using Natural Language Processing\nor NLP techniques for translating Pure or Modern Bangla to locally spoken\nSylheti Bangla language. Total 1200 data used for training 3 models LSTM,\nBi-LSTM and Seq2Seq and LSTM scored the best in performance with 89.3%\naccuracy. The findings of this research may contribute to the growth of Bangla\nNLP researchers for future more advanced innovations."}
{"id": "2505.19237", "pdf": "https://arxiv.org/pdf/2505.19237", "abs": "https://arxiv.org/abs/2505.19237", "authors": ["Iaki Dellibarda Varela", "Pablo Romero-Sorozabal", "Diego Torricelli", "Gabriel Delgado-Oleas", "Jose Ignacio Serrano", "Maria Dolores del Castillo Sobrino", "Eduardo Rocon", "Manuel Cebrian"], "title": "Sensorimotor features of self-awareness in multimodal large language models", "categories": ["cs.AI", "cs.RO"], "comment": "16 pages, 3 figures, 1 table", "summary": "Self-awareness - the ability to distinguish oneself from the surrounding\nenvironment - underpins intelligent, autonomous behavior. Recent advances in AI\nachieve human-like performance in tasks integrating multimodal information,\nparticularly in large language models, raising interest in the embodiment\ncapabilities of AI agents on nonhuman platforms such as robots. Here, we\nexplore whether multimodal LLMs can develop self-awareness solely through\nsensorimotor experiences. By integrating a multimodal LLM into an autonomous\nmobile robot, we test its ability to achieve this capacity. We find that the\nsystem exhibits robust environmental awareness, self-recognition and predictive\nawareness, allowing it to infer its robotic nature and motion characteristics.\nStructural equation modeling reveals how sensory integration influences\ndistinct dimensions of self-awareness and its coordination with past-present\nmemory, as well as the hierarchical internal associations that drive\nself-identification. Ablation tests of sensory inputs identify critical\nmodalities for each dimension, demonstrate compensatory interactions among\nsensors and confirm the essential role of structured and episodic memory in\ncoherent reasoning. These findings demonstrate that, given appropriate sensory\ninformation about the world and itself, multimodal LLMs exhibit emergent\nself-awareness, opening the door to artificial embodied cognitive systems."}
{"id": "2505.18532", "pdf": "https://arxiv.org/pdf/2505.18532", "abs": "https://arxiv.org/abs/2505.18532", "authors": ["Mingyang Wu", "Li Lin", "Wenbin Zhang", "Xin Wang", "Zhenhuan Yang", "Shu Hu"], "title": "Preserving AUC Fairness in Learning with Noisy Protected Groups", "categories": ["cs.LG"], "comment": null, "summary": "The Area Under the ROC Curve (AUC) is a key metric for classification,\nespecially under class imbalance, with growing research focus on optimizing AUC\nover accuracy in applications like medical image analysis and deepfake\ndetection. This leads to fairness in AUC optimization becoming crucial as\nbiases can impact protected groups. While various fairness mitigation\ntechniques exist, fairness considerations in AUC optimization remain in their\nearly stages, with most research focusing on improving AUC fairness under the\nassumption of clean protected groups. However, these studies often overlook the\nimpact of noisy protected groups, leading to fairness violations in practice.\nTo address this, we propose the first robust AUC fairness approach under noisy\nprotected groups with fairness theoretical guarantees using distributionally\nrobust optimization. Extensive experiments on tabular and image datasets show\nthat our method outperforms state-of-the-art approaches in preserving AUC\nfairness. The code is in\nhttps://github.com/Purdue-M2/AUC_Fairness_with_Noisy_Groups."}
{"id": "2505.18720", "pdf": "https://arxiv.org/pdf/2505.18720", "abs": "https://arxiv.org/abs/2505.18720", "authors": ["Meng Li", "Guangda Huzhang", "Haibo Zhang", "Xiting Wang", "Anxiang Zeng"], "title": "Optimal Transport-Based Token Weighting scheme for Enhanced Preference Optimization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "24 pages, 11 figures. Accepted by ACL 2025 (main)", "summary": "Direct Preference Optimization (DPO) has emerged as a promising framework for\naligning Large Language Models (LLMs) with human preferences by directly\noptimizing the log-likelihood difference between chosen and rejected responses.\nHowever, existing methods assign equal importance to all tokens in the\nresponse, while humans focus on more meaningful parts. This leads to suboptimal\npreference optimization, as irrelevant or noisy tokens disproportionately\ninfluence DPO loss. To address this limitation, we propose \\textbf{O}ptimal\n\\textbf{T}ransport-based token weighting scheme for enhancing direct\n\\textbf{P}reference \\textbf{O}ptimization (OTPO). By emphasizing semantically\nmeaningful token pairs and de-emphasizing less relevant ones, our method\nintroduces a context-aware token weighting scheme that yields a more\ncontrastive reward difference estimate. This adaptive weighting enhances reward\nstability, improves interpretability, and ensures that preference optimization\nfocuses on meaningful differences between responses. Extensive experiments have\nvalidated OTPO's effectiveness in improving instruction-following ability\nacross various settings\\footnote{Code is available at\nhttps://github.com/Mimasss2/OTPO.}."}
{"id": "2505.19266", "pdf": "https://arxiv.org/pdf/2505.19266", "abs": "https://arxiv.org/abs/2505.19266", "authors": ["Yaxuan Yang", "Shiyu Wang", "Xiaoming Zhai"], "title": "Using Large Language Models to Assess Teachers' Pedagogical Content Knowledge", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "Assessing teachers' pedagogical content knowledge (PCK) through\nperformance-based tasks is both time and effort-consuming. While large language\nmodels (LLMs) offer new opportunities for efficient automatic scoring, little\nis known about whether LLMs introduce construct-irrelevant variance (CIV) in\nways similar to or different from traditional machine learning (ML) and human\nraters. This study examines three sources of CIV -- scenario variability, rater\nseverity, and rater sensitivity to scenario -- in the context of video-based\nconstructed-response tasks targeting two PCK sub-constructs: analyzing student\nthinking and evaluating teacher responsiveness. Using generalized linear mixed\nmodels (GLMMs), we compared variance components and rater-level scoring\npatterns across three scoring sources: human raters, supervised ML, and LLM.\nResults indicate that scenario-level variance was minimal across tasks, while\nrater-related factors contributed substantially to CIV, especially in the more\ninterpretive Task II. The ML model was the most severe and least sensitive\nrater, whereas the LLM was the most lenient. These findings suggest that the\nLLM contributes to scoring efficiency while also introducing CIV as human\nraters do, yet with varying levels of contribution compared to supervised ML.\nImplications for rater training, automated scoring design, and future research\non model interpretability are discussed."}
{"id": "2505.18535", "pdf": "https://arxiv.org/pdf/2505.18535", "abs": "https://arxiv.org/abs/2505.18535", "authors": ["Dmitry Dudukalov", "Artem Logachov", "Vladimir Lotov", "Timofei Prasolov", "Evgeny Prokopenko", "Anton Tarasenko"], "title": "Convergence, Sticking and Escape: Stochastic Dynamics Near Critical Points in SGD", "categories": ["cs.LG", "math.PR", "stat.ML"], "comment": null, "summary": "We study the convergence properties and escape dynamics of Stochastic\nGradient Descent (SGD) in one-dimensional landscapes, separately considering\ninfinite- and finite-variance noise. Our main focus is to identify the time\nscales on which SGD reliably moves from an initial point to the local minimum\nin the same ''basin''. Under suitable conditions on the noise distribution, we\nprove that SGD converges to the basin's minimum unless the initial point lies\ntoo close to a local maximum. In that near-maximum scenario, we show that SGD\ncan linger for a long time in its neighborhood. For initial points near a\n''sharp'' maximum, we show that SGD does not remain stuck there, and we provide\nresults to estimate the probability that it will reach each of the two\nneighboring minima. Overall, our findings present a nuanced view of SGD's\ntransitions between local maxima and minima, influenced by both noise\ncharacteristics and the underlying function geometry."}
{"id": "2505.18744", "pdf": "https://arxiv.org/pdf/2505.18744", "abs": "https://arxiv.org/abs/2505.18744", "authors": ["Tao Liu", "Hongying Zan", "Yifan Li", "Dixuan Zhang", "Lulu Kong", "Haixin Liu", "Jiaming Hou", "Aoze Zheng", "Rui Li", "Yiming Qiao", "Zewei Luo", "Qi Wang", "Zhiqiang Zhang", "Jiaxi Li", "Supeng Liu", "Kunli Zhang", "Min Peng"], "title": "LogicCat: A Chain-of-Thought Text-to-SQL Benchmark for Multi-Domain Reasoning Challenges", "categories": ["cs.CL"], "comment": "22 pages, 10 figures", "summary": "Text-to-SQL is a fundamental task in natural language processing that seeks\nto translate natural language questions into meaningful and executable SQL\nqueries. While existing datasets are extensive and primarily focus on business\nscenarios and operational logic, they frequently lack coverage of\ndomain-specific knowledge and complex mathematical reasoning. To address this\ngap, we present a novel dataset tailored for complex reasoning and\nchain-of-thought analysis in SQL inference, encompassing physical, arithmetic,\ncommonsense, and hypothetical reasoning. The dataset consists of 4,038 English\nquestions, each paired with a unique SQL query and accompanied by 12,114\nstep-by-step reasoning annotations, spanning 45 databases across diverse\ndomains. Experimental results demonstrate that LogicCat substantially increases\nthe difficulty for state-of-the-art models, with the highest execution accuracy\nreaching only 14.96%. Incorporating our chain-of-thought annotations boosts\nperformance to 33.96%. Benchmarking leading public methods on Spider and BIRD\nfurther underscores the unique challenges presented by LogicCat, highlighting\nthe significant opportunities for advancing research in robust,\nreasoning-driven text-to-SQL systems. We have released our dataset code at\nhttps://github.com/Ffunkytao/LogicCat."}
{"id": "2505.19277", "pdf": "https://arxiv.org/pdf/2505.19277", "abs": "https://arxiv.org/abs/2505.19277", "authors": ["Ibukun Olatunji", "Mark Sheppard"], "title": "Next Token Prediction Is a Dead End for Creativity", "categories": ["cs.AI", "cs.CL", "J.5; I.2.0; I.2.7"], "comment": "10 pages including references", "summary": "This paper argues that token prediction is fundamentally misaligned with real\ncreativity. While next-token models have enabled impressive advances in\nlanguage generation, their architecture favours surface-level coherence over\nspontaneity, originality, and improvisational risk. We use battle rap as a case\nstudy to expose the limitations of predictive systems, demonstrating that they\ncannot truly engage in adversarial or emotionally resonant exchanges. By\nreframing creativity as an interactive process rather than a predictive output,\nwe offer a vision for AI systems that are more expressive, responsive, and\naligned with human creative practice."}
{"id": "2505.18545", "pdf": "https://arxiv.org/pdf/2505.18545", "abs": "https://arxiv.org/abs/2505.18545", "authors": ["An Vo", "Mohammad Reza Taesiri", "Daeyoung Kim", "Anh Totti Nguyen"], "title": "B-score: Detecting biases in large language models using response history", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to ICML 2025 (Main track)", "summary": "Large language models (LLMs) often exhibit strong biases, e.g, against women\nor in favor of the number 7. We investigate whether LLMs would be able to\noutput less biased answers when allowed to observe their prior answers to the\nsame question in a multi-turn conversation. To understand which types of\nquestions invite more biased answers, we test LLMs on our proposed set of\nquestions that span 9 topics and belong to three types: (1) Subjective; (2)\nRandom; and (3) Objective. Interestingly, LLMs are able to \"de-bias\" themselves\nin a multi-turn conversation in response to questions that seek an Random,\nunbiased answer. Furthermore, we propose B-score, a novel metric that is\neffective in detecting biases to Subjective, Random, Easy, and Hard questions.\nOn MMLU, HLE, and CSQA, leveraging B-score substantially improves the\nverification accuracy of LLM answers (i.e, accepting LLM correct answers and\nrejecting incorrect ones) compared to using verbalized confidence scores or the\nfrequency of single-turn answers alone. Code and data are available at:\nhttps://b-score.github.io."}
{"id": "2505.18752", "pdf": "https://arxiv.org/pdf/2505.18752", "abs": "https://arxiv.org/abs/2505.18752", "authors": ["Haolin Yang", "Hakaze Cho", "Yiqiao Zhong", "Naoya Inoue"], "title": "Unifying Attention Heads and Task Vectors via Hidden State Geometry in In-Context Learning", "categories": ["cs.CL"], "comment": "45 pages, 49 figures", "summary": "The unusual properties of in-context learning (ICL) have prompted\ninvestigations into the internal mechanisms of large language models. Prior\nwork typically focuses on either special attention heads or task vectors at\nspecific layers, but lacks a unified framework linking these components to the\nevolution of hidden states across layers that ultimately produce the model's\noutput. In this paper, we propose such a framework for ICL in classification\ntasks by analyzing two geometric factors that govern performance: the\nseparability and alignment of query hidden states. A fine-grained analysis of\nlayer-wise dynamics reveals a striking two-stage mechanism: separability\nemerges in early layers, while alignment develops in later layers. Ablation\nstudies further show that Previous Token Heads drive separability, while\nInduction Heads and task vectors enhance alignment. Our findings thus bridge\nthe gap between attention heads and task vectors, offering a unified account of\nICL's underlying mechanisms."}
{"id": "2505.19317", "pdf": "https://arxiv.org/pdf/2505.19317", "abs": "https://arxiv.org/abs/2505.19317", "authors": ["Tin Nguyen", "Jiannan Xu", "Zora Che", "Phuong-Anh Nguyen-Le", "Rushil Dandamudi", "Donald Braman", "Furong Huang", "Hal Daum III", "Zubin Jelveh"], "title": "Effort-aware Fairness: Incorporating a Philosophy-informed, Human-centered Notion of Effort into Algorithmic Fairness Metrics", "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": null, "summary": "Although popularized AI fairness metrics, e.g., demographic parity, have\nuncovered bias in AI-assisted decision-making outcomes, they do not consider\nhow much effort one has spent to get to where one is today in the input feature\nspace. However, the notion of effort is important in how Philosophy and humans\nunderstand fairness. We propose a philosophy-informed way to conceptualize and\nevaluate Effort-aware Fairness (EaF) based on the concept of Force, or temporal\ntrajectory of predictive features coupled with inertia. In addition to our\ntheoretical formulation of EaF metrics, our empirical contributions include: 1/\na pre-registered human subjects experiment, which demonstrates that for both\nstages of the (individual) fairness evaluation process, people consider the\ntemporal trajectory of a predictive feature more than its aggregate value; 2/\npipelines to compute Effort-aware Individual/Group Fairness in the criminal\njustice and personal finance contexts. Our work may enable AI model auditors to\nuncover and potentially correct unfair decisions against individuals who spent\nsignificant efforts to improve but are still stuck with systemic/early-life\ndisadvantages outside their control."}
{"id": "2505.18558", "pdf": "https://arxiv.org/pdf/2505.18558", "abs": "https://arxiv.org/abs/2505.18558", "authors": ["Wenbo He", "Zhijian Ou"], "title": "Joint-stochastic-approximation Autoencoders with Application to Semi-supervised Learning", "categories": ["cs.LG", "stat.ML"], "comment": "ICML 2018 submission. arXiv admin note: text overlap with\n  arXiv:1808.01630", "summary": "Our examination of existing deep generative models (DGMs), including VAEs and\nGANs, reveals two problems. First, their capability in handling discrete\nobservations and latent codes is unsatisfactory, though there are interesting\nefforts. Second, both VAEs and GANs optimize some criteria that are indirectly\nrelated to the data likelihood. To address these problems, we formally present\nJoint-stochastic-approximation (JSA) autoencoders - a new family of algorithms\nfor building deep directed generative models, with application to\nsemi-supervised learning. The JSA learning algorithm directly maximizes the\ndata log-likelihood and simultaneously minimizes the inclusive KL divergence\nthe between the posteriori and the inference model. We provide theoretical\nresults and conduct a series of experiments to show its superiority such as\nbeing robust to structure mismatch between encoder and decoder, consistent\nhandling of both discrete and continuous variables. Particularly we empirically\nshow that JSA autoencoders with discrete latent space achieve comparable\nperformance to other state-of-the-art DGMs with continuous latent space in\nsemi-supervised tasks over the widely adopted datasets - MNIST and SVHN. To the\nbest of our knowledge, this is the first demonstration that discrete latent\nvariable models are successfully applied in the challenging semi-supervised\ntasks."}
{"id": "2505.18754", "pdf": "https://arxiv.org/pdf/2505.18754", "abs": "https://arxiv.org/abs/2505.18754", "authors": ["Elsen Ronando", "Sozo Inoue"], "title": "Few-Shot Optimization for Sensor Data Using Large Language Models: A Case Study on Fatigue Detection", "categories": ["cs.CL", "I.2.7"], "comment": "43 pages, 18 figures. Accepted for publication in MDPI Sensors\n  (2025). Final version before journal publication", "summary": "In this paper, we propose a novel few-shot optimization with HED-LM (Hybrid\nEuclidean Distance with Large Language Models) to improve example selection for\nsensor-based classification tasks. While few-shot prompting enables efficient\ninference with limited labeled data, its performance largely depends on the\nquality of selected examples. HED-LM addresses this challenge through a hybrid\nselection pipeline that filters candidate examples based on Euclidean distance\nand re-ranks them using contextual relevance scored by large language models\n(LLMs). To validate its effectiveness, we apply HED-LM to a fatigue detection\ntask using accelerometer data characterized by overlapping patterns and high\ninter-subject variability. Unlike simpler tasks such as activity recognition,\nfatigue detection demands more nuanced example selection due to subtle\ndifferences in physiological signals. Our experiments show that HED-LM achieves\na mean macro F1-score of 69.13$\\pm$10.71%, outperforming both random selection\n(59.30$\\pm$10.13%) and distance-only filtering (67.61$\\pm$11.39%). These\nrepresent relative improvements of 16.6% and 2.3%, respectively. The results\nconfirm that combining numerical similarity with contextual relevance improves\nthe robustness of few-shot prompting. Overall, HED-LM offers a practical\nsolution to improve performance in real-world sensor-based learning tasks and\nshows potential for broader applications in healthcare monitoring, human\nactivity recognition, and industrial safety scenarios."}
{"id": "2505.19333", "pdf": "https://arxiv.org/pdf/2505.19333", "abs": "https://arxiv.org/abs/2505.19333", "authors": ["Zach Studdiford", "Timothy T. Rogers", "Siddharth Suresh", "Kushin Mukherjee"], "title": "Evaluating Steering Techniques using Human Similarity Judgments", "categories": ["cs.AI", "I.2.7"], "comment": null, "summary": "Current evaluations of Large Language Model (LLM) steering techniques focus\non task-specific performance, overlooking how well steered representations\nalign with human cognition. Using a well-established triadic similarity\njudgment task, we assessed steered LLMs on their ability to flexibly judge\nsimilarity between concepts based on size or kind. We found that prompt-based\nsteering methods outperformed other methods both in terms of steering accuracy\nand model-to-human alignment. We also found LLMs were biased towards 'kind'\nsimilarity and struggled with 'size' alignment. This evaluation approach,\ngrounded in human cognition, adds further support to the efficacy of\nprompt-based steering and reveals privileged representational axes in LLMs\nprior to steering."}
{"id": "2505.18565", "pdf": "https://arxiv.org/pdf/2505.18565", "abs": "https://arxiv.org/abs/2505.18565", "authors": ["Afrah Farea", "Saiful Khan", "Reza Daryani", "Emre Cenk Ersan", "Mustafa Serdar Celebi"], "title": "Learning Fluid-Structure Interaction Dynamics with Physics-Informed Neural Networks and Immersed Boundary Methods", "categories": ["cs.LG", "cs.CE", "physics.flu-dyn"], "comment": null, "summary": "We introduce neural network architectures that combine physics-informed\nneural networks (PINNs) with the immersed boundary method (IBM) to solve\nfluid-structure interaction (FSI) problems. Our approach features two distinct\narchitectures: a Single-FSI network with a unified parameter space, and an\ninnovative Eulerian-Lagrangian network that maintains separate parameter spaces\nfor fluid and structure domains. We study each architecture using standard Tanh\nand adaptive B-spline activation functions. Empirical studies on a 2D cavity\nflow problem involving a moving solid structure show that the\nEulerian-Lagrangian architecture performs significantly better. The adaptive\nB-spline activation further enhances accuracy by providing locality-aware\nrepresentation near boundaries. While our methodology shows promising results\nin predicting the velocity field, pressure recovery remains challenging due to\nthe absence of explicit force-coupling constraints in the current formulation.\nOur findings underscore the importance of domain-specific architectural design\nand adaptive activation functions for modeling FSI problems within the PINN\nframework."}
{"id": "2505.18761", "pdf": "https://arxiv.org/pdf/2505.18761", "abs": "https://arxiv.org/abs/2505.18761", "authors": ["Minglai Yang", "Ethan Huang", "Liang Zhang", "Mihai Surdeanu", "William Wang", "Liangming Pan"], "title": "How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "15 pages, 9 figure, 4 tables", "summary": "We introduce Grade School Math with Distracting Context (GSM-DC), a synthetic\nbenchmark to evaluate Large Language Models' (LLMs) reasoning robustness\nagainst systematically controlled irrelevant context (IC). GSM-DC constructs\nsymbolic reasoning graphs with precise distractor injections, enabling\nrigorous, reproducible evaluation. Our experiments demonstrate that LLMs are\nsignificantly sensitive to IC, affecting both reasoning path selection and\narithmetic accuracy. Additionally, training models with strong distractors\nimproves performance in both in-distribution and out-of-distribution scenarios.\nWe further propose a stepwise tree search guided by a process reward model,\nwhich notably enhances robustness in out-of-distribution conditions."}
{"id": "2505.19347", "pdf": "https://arxiv.org/pdf/2505.19347", "abs": "https://arxiv.org/abs/2505.19347", "authors": ["Yongmin Yoo", "Qiongkai Xu", "Longbing Cao"], "title": "PatentMind: A Multi-Aspect Reasoning Graph for Patent Similarity Evaluation", "categories": ["cs.AI"], "comment": null, "summary": "Patent similarity evaluation plays a critical role in intellectual property\nanalysis. However, existing methods often overlook the intricate structure of\npatent documents, which integrate technical specifications, legal boundaries,\nand application contexts. We introduce PatentMind, a novel framework for patent\nsimilarity assessment based on a Multi-Aspect Reasoning Graph (MARG).\nPatentMind decomposes patents into three core dimensions: technical feature,\napplication domain, and claim scope, to compute dimension-specific similarity\nscores. These scores are dynamically weighted through a four-stage reasoning\nprocess which integrates contextual signals to emulate expert-level judgment.\nTo support evaluation, we construct PatentSimBench, a human-annotated benchmark\ncomprising 500 patent pairs. Experimental results demonstrate that PatentMind\nachieves a strong correlation ($r=0.938$) with expert annotations,\nsignificantly outperforming embedding-based models and advanced prompt\nengineering methods.These results highlight the effectiveness of modular\nreasoning frameworks in overcoming key limitations of embedding-based methods\nfor analyzing patent similarity."}
{"id": "2505.18568", "pdf": "https://arxiv.org/pdf/2505.18568", "abs": "https://arxiv.org/abs/2505.18568", "authors": ["Zhikang Chen", "Abudukelimu Wuerkaixi", "Sen Cui", "Haoxuan Li", "Ding Li", "Jingfeng Zhang", "Bo Han", "Gang Niu", "Houfang Liu", "Yi Yang", "Sifan Yang", "Changshui Zhang", "Tianling Ren"], "title": "Learning without Isolation: Pathway Protection for Continual Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "23 pages", "summary": "Deep networks are prone to catastrophic forgetting during sequential task\nlearning, i.e., losing the knowledge about old tasks upon learning new tasks.\nTo this end, continual learning(CL) has emerged, whose existing methods focus\nmostly on regulating or protecting the parameters associated with the previous\ntasks. However, parameter protection is often impractical, since the size of\nparameters for storing the old-task knowledge increases linearly with the\nnumber of tasks, otherwise it is hard to preserve the parameters related to the\nold-task knowledge. In this work, we bring a dual opinion from neuroscience and\nphysics to CL: in the whole networks, the pathways matter more than the\nparameters when concerning the knowledge acquired from the old tasks. Following\nthis opinion, we propose a novel CL framework, learning without isolation(LwI),\nwhere model fusion is formulated as graph matching and the pathways occupied by\nthe old tasks are protected without being isolated. Thanks to the sparsity of\nactivation channels in a deep network, LwI can adaptively allocate available\npathways for a new task, realizing pathway protection and addressing\ncatastrophic forgetting in a parameter-efficient manner. Experiments on popular\nbenchmark datasets demonstrate the superiority of the proposed LwI."}
{"id": "2505.18762", "pdf": "https://arxiv.org/pdf/2505.18762", "abs": "https://arxiv.org/abs/2505.18762", "authors": ["Michael Flor", "Zuowei Wang", "Paul Deane", "Tenaha O'Reilly"], "title": "Towards an automatic method for generating topical vocabulary test forms for specific reading passages", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "This manuscript was accepted to be published as an ETS Research\n  Report. Keywords topics; vocabulary; background knowledge; automatic item\n  generation; assessment; reading comprehension", "summary": "Background knowledge is typically needed for successful comprehension of\ntopical and domain specific reading passages, such as in the STEM domain.\nHowever, there are few automated measures of student knowledge that can be\nreadily deployed and scored in time to make predictions on whether a given\nstudent will likely be able to understand a specific content area text. In this\npaper, we present our effort in developing K-tool, an automated system for\ngenerating topical vocabulary tests that measure students' background knowledge\nrelated to a specific text. The system automatically detects the topic of a\ngiven text and produces topical vocabulary items based on their relationship\nwith the topic. This information is used to automatically generate background\nknowledge forms that contain words that are highly related to the topic and\nwords that share similar features but do not share high associations to the\ntopic. Prior research indicates that performance on such tasks can help\ndetermine whether a student is likely to understand a particular text based on\ntheir knowledge state. The described system is intended for use with middle and\nhigh school student population of native speakers of English. It is designed to\nhandle single reading passages and is not dependent on any corpus or text\ncollection. In this paper, we describe the system architecture and present an\ninitial evaluation of the system outputs."}
{"id": "2505.19353", "pdf": "https://arxiv.org/pdf/2505.19353", "abs": "https://arxiv.org/abs/2505.19353", "authors": ["Camilo Chacn Sartori"], "title": "Architectures of Error: A Philosophical Inquiry into AI and Human Code Generation", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.SE"], "comment": "preprint", "summary": "With the rise of generative AI (GenAI), Large Language Models are\nincreasingly employed for code generation, becoming active co-authors alongside\nhuman programmers. Focusing specifically on this application domain, this paper\narticulates distinct ``Architectures of Error'' to ground an epistemic\ndistinction between human and machine code generation. Examined through their\nshared vulnerability to error, this distinction reveals fundamentally different\ncausal origins: human-cognitive versus artificial-stochastic. To develop this\nframework and substantiate the distinction, the analysis draws critically upon\nDennett's mechanistic functionalism and Rescher's methodological pragmatism. I\nargue that a systematic differentiation of these error profiles raises critical\nphilosophical questions concerning semantic coherence, security robustness,\nepistemic limits, and control mechanisms in human-AI collaborative software\ndevelopment. The paper also utilizes Floridi's levels of abstraction to provide\na nuanced understanding of how these error dimensions interact and may evolve\nwith technological advancements. This analysis aims to offer philosophers a\nstructured framework for understanding GenAI's unique epistemological\nchallenges, shaped by these architectural foundations, while also providing\nsoftware engineers a basis for more critically informed engagement."}
{"id": "2505.18570", "pdf": "https://arxiv.org/pdf/2505.18570", "abs": "https://arxiv.org/abs/2505.18570", "authors": ["Tina Khezresmaeilzadeh", "Parsa Razmara", "Seyedarmin Azizi", "Mohammad Erfan Sadeghi", "Erfan Baghaei Portaghloo"], "title": "VISTA: Vision-Language Inference for Training-Free Stock Time-Series Analysis", "categories": ["cs.LG"], "comment": null, "summary": "Stock price prediction remains a complex and high-stakes task in financial\nanalysis, traditionally addressed using statistical models or, more recently,\nlanguage models. In this work, we introduce VISTA (Vision-Language Inference\nfor Stock Time-series Analysis), a novel, training-free framework that\nleverages Vision-Language Models (VLMs) for multi-modal stock forecasting.\nVISTA prompts a VLM with both textual representations of historical stock\nprices and their corresponding line charts to predict future price values. By\ncombining numerical and visual modalities in a zero-shot setting and using\ncarefully designed chain-of-thought prompts, VISTA captures complementary\npatterns that unimodal approaches often miss. We benchmark VISTA against\nstandard baselines, including ARIMA and text-only LLM-based prompting methods.\nExperimental results show that VISTA outperforms these baselines by up to\n89.83%, demonstrating the effectiveness of multi-modal inference for stock\ntime-series analysis and highlighting the potential of VLMs in financial\nforecasting tasks without requiring task-specific training."}
{"id": "2505.18774", "pdf": "https://arxiv.org/pdf/2505.18774", "abs": "https://arxiv.org/abs/2505.18774", "authors": ["Mengqi Zhang", "Zisheng Zhou", "Xiaotian Ye", "Qiang Liu", "Zhaochun Ren", "Zhumin Chen", "Pengjie Ren"], "title": "Disentangling Knowledge Representations for Large Language Model Editing", "categories": ["cs.CL"], "comment": null, "summary": "Knowledge Editing has emerged as a promising solution for efficiently\nupdating embedded knowledge in large language models (LLMs). While existing\napproaches demonstrate effectiveness in integrating new knowledge and\npreserving the original capabilities of LLMs, they fail to maintain\nfine-grained irrelevant knowledge facts that share the same subject as edited\nknowledge but differ in relation and object. This challenge arises because\nsubject representations inherently encode multiple attributes, causing the\ntarget and fine-grained irrelevant knowledge to become entangled in the\nrepresentation space, and thus vulnerable to unintended alterations during\nediting. To address this, we propose DiKE, a novel approach that Disentangles\nKnowledge representations for LLM Editing (DiKE). DiKE consists of two key\ncomponents: a Knowledge Representation Disentanglement (KRD) module that\ndecomposes the subject representation into target-knowledgerelated and\n-unrelated components, and a Disentanglement-based Knowledge Edit (DKE) module\nthat updates only the target-related component while explicitly preserving the\nunrelated one. We further derive a closed-form, rank-one parameter update based\non matrix theory to enable efficient and minimally invasive edits. To\nrigorously evaluate fine-grained irrelevant knowledge preservation, we\nconstruct FINE-KED, a new benchmark comprising fine-grained irrelevant\nknowledge at different levels of relational similarity to the edited knowledge.\nExtensive experiments across multiple LLMs demonstrate that DiKE substantially\nimproves fine-grained irrelevant knowledge preservation while maintaining\ncompetitive general editing performance."}
{"id": "2505.19361", "pdf": "https://arxiv.org/pdf/2505.19361", "abs": "https://arxiv.org/abs/2505.19361", "authors": ["Mario Leiva", "Noel Ngu", "Joshua Shay Kricheli", "Aditya Taparia", "Ransalu Senanayake", "Paulo Shakarian", "Nathaniel Bastian", "John Corcoran", "Gerardo Simari"], "title": "Consistency-based Abductive Reasoning over Perceptual Errors of Multiple Pre-trained Models in Novel Environments", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.LO"], "comment": null, "summary": "The deployment of pre-trained perception models in novel environments often\nleads to performance degradation due to distributional shifts. Although recent\nartificial intelligence approaches for metacognition use logical rules to\ncharacterize and filter model errors, improving precision often comes at the\ncost of reduced recall. This paper addresses the hypothesis that leveraging\nmultiple pre-trained models can mitigate this recall reduction. We formulate\nthe challenge of identifying and managing conflicting predictions from various\nmodels as a consistency-based abduction problem. The input predictions and the\nlearned error detection rules derived from each model are encoded in a logic\nprogram. We then seek an abductive explanation--a subset of model\npredictions--that maximizes prediction coverage while ensuring the rate of\nlogical inconsistencies (derived from domain constraints) remains below a\nspecified threshold. We propose two algorithms for this knowledge\nrepresentation task: an exact method based on Integer Programming (IP) and an\nefficient Heuristic Search (HS). Through extensive experiments on a simulated\naerial imagery dataset featuring controlled, complex distributional shifts, we\ndemonstrate that our abduction-based framework outperforms individual models\nand standard ensemble baselines, achieving, for instance, average relative\nimprovements of approximately 13.6% in F1-score and 16.6% in accuracy across 15\ndiverse test datasets when compared to the best individual model. Our results\nvalidate the use of consistency-based abduction as an effective mechanism to\nrobustly integrate knowledge from multiple imperfect reasoners in challenging,\nnovel scenarios."}
{"id": "2505.18573", "pdf": "https://arxiv.org/pdf/2505.18573", "abs": "https://arxiv.org/abs/2505.18573", "authors": ["Mengqi Liao", "Xiangyu Xi", "Ruinian Chen", "Jia Leng", "Yangen Hu", "Ke Zeng", "Shuai Liu", "Huaiyu Wan"], "title": "Enhancing Efficiency and Exploration in Reinforcement Learning for LLMs", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Reasoning large language models (LLMs) excel in complex tasks, which has\ndrawn significant attention to reinforcement learning (RL) for LLMs. However,\nexisting approaches allocate an equal number of rollouts to all questions\nduring the RL process, which is inefficient. This inefficiency stems from the\nfact that training on simple questions yields limited gains, whereas more\nrollouts are needed for challenging questions to sample correct answers.\nFurthermore, while RL improves response precision, it limits the model's\nexploration ability, potentially resulting in a performance cap below that of\nthe base model prior to RL. To address these issues, we propose a mechanism for\ndynamically allocating rollout budgets based on the difficulty of the problems,\nenabling more efficient RL training. Additionally, we introduce an adaptive\ndynamic temperature adjustment strategy to maintain the entropy at a stable\nlevel, thereby encouraging sufficient exploration. This enables LLMs to improve\nresponse precision while preserving their exploratory ability to uncover\npotential correct pathways. The code and data is available on:\nhttps://github.com/LiaoMengqi/E3-RL4LLMs"}
{"id": "2505.18778", "pdf": "https://arxiv.org/pdf/2505.18778", "abs": "https://arxiv.org/abs/2505.18778", "authors": ["Benjamin Bennetzen", "Peter Buus Steffensen", "Hans Httel", "Nikolaj Rossander Kristensen", "Andreas Tor Mortensen"], "title": "A generalised editor calculus (Short Paper)", "categories": ["cs.CL", "F.2.2, I.2.7"], "comment": "7 pages, 21 figures", "summary": "In this paper, we present a generalization of a syntax-directed editor\ncalculus, which can be used to instantiate a specialized syntax-directed editor\nfor any language, given by some abstract syntax. The editor calculus guarantees\nthe absence of syntactical errors while allowing incomplete programs. The\ngeneralized editor calculus is then encoded into a simply typed lambda\ncalculus, extended with pairs, booleans, pattern matching and fixed points"}
{"id": "2505.19371", "pdf": "https://arxiv.org/pdf/2505.19371", "abs": "https://arxiv.org/abs/2505.19371", "authors": ["Georgy Noarov", "Soham Mallick", "Tao Wang", "Sunay Joshi", "Yan Sun", "Yangxinyu Xie", "Mengxin Yu", "Edgar Dobriban"], "title": "Foundations of Top-$k$ Decoding For Language Models", "categories": ["cs.AI", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "Top-$k$ decoding is a widely used method for sampling from LLMs: at each\ntoken, only the largest $k$ next-token-probabilities are kept, and the next\ntoken is sampled after re-normalizing them to sum to unity. Top-$k$ and other\nsampling methods are motivated by the intuition that true next-token\ndistributions are sparse, and the noisy LLM probabilities need to be truncated.\nHowever, to our knowledge, a precise theoretical motivation for the use of\ntop-$k$ decoding is missing. In this work, we develop a theoretical framework\nthat both explains and generalizes top-$k$ decoding. We view decoding at a\nfixed token as the recovery of a sparse probability distribution. We consider\n\\emph{Bregman decoders} obtained by minimizing a separable Bregman divergence\n(for both the \\emph{primal} and \\emph{dual} cases) with a sparsity-inducing\n$\\ell_0$ regularization. Despite the combinatorial nature of the objective, we\nshow how to optimize it efficiently for a large class of divergences. We show\nthat the optimal decoding strategies are greedy, and further that the loss\nfunction is discretely convex in $k$, so that binary search provably and\nefficiently finds the optimal $k$. We show that top-$k$ decoding arises as a\nspecial case for the KL divergence, and identify new decoding strategies that\nhave distinct behaviors (e.g., non-linearly up-weighting larger probabilities\nafter re-normalization)."}
{"id": "2505.18579", "pdf": "https://arxiv.org/pdf/2505.18579", "abs": "https://arxiv.org/abs/2505.18579", "authors": ["Tingpeng Zhang", "Xuzhang Peng", "Mingyuan Zhou", "Guobiao Hu", "Zhilu Lai"], "title": "Mechanical in-sensor computing: a programmable meta-sensor for structural damage classification without external electronic power", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Structural health monitoring (SHM) involves sensor deployment, data\nacquisition, and data interpretation, commonly implemented via a tedious wired\nsystem. The information processing in current practice majorly depends on\nelectronic computers, albeit with universal applications, delivering challenges\nsuch as high energy consumption and low throughput due to the nature of digital\nunits. In recent years, there has been a renaissance interest in shifting\ncomputations from electronic computing units to the use of real physical\nsystems, a concept known as physical computation. This approach provides the\npossibility of thinking out of the box for SHM, seamlessly integrating sensing\nand computing into a pure-physical entity, without relying on external\nelectronic power supplies, thereby properly coping with resource-restricted\nscenarios. The latest advances of metamaterials (MM) hold great promise for\nthis proactive idea. In this paper, we introduce a programmable\nmetamaterial-based sensor (termed as MM-sensor) for physically processing\nstructural vibration information to perform specific SHM tasks, such as\nstructural damage warning (binary classification) in this initiation, without\nthe need for further information processing or resource-consuming, that is, the\ndata collection and analysis are completed in-situ at the sensor level. We\nadopt the configuration of a locally resonant metamaterial plate (LRMP) to\nachieve the first fabrication of the MM-sensor. We take advantage of the\nbandgap properties of LRMP to physically differentiate the dynamic behavior of\nstructures before and after damage. By inversely designing the geometric\nparameters, our current approach allows for adjustments to the bandgap\nfeatures. This is effective for engineering systems with a first natural\nfrequency ranging from 9.54 Hz to 81.86 Hz."}
{"id": "2505.18799", "pdf": "https://arxiv.org/pdf/2505.18799", "abs": "https://arxiv.org/abs/2505.18799", "authors": ["Hao Chen", "Haoze Li", "Zhiqing Xiao", "Lirong Gao", "Qi Zhang", "Xiaomeng Hu", "Ningtao Wang", "Xing Fu", "Junbo Zhao"], "title": "ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 8 figures, 14 tables", "summary": "Aligning general-purpose large language models (LLMs) to downstream tasks\noften incurs significant costs, including constructing task-specific\ninstruction pairs and extensive training adjustments. Prior research has\nexplored various avenues to enhance alignment efficiency, primarily through\nminimal-data training or data-driven activations to identify key attention\nheads. However, these approaches inherently introduce data dependency, which\nhinders generalization and reusability. To address this issue and enhance model\nalignment efficiency, we propose the \\textit{\\textbf{A}ttention\n\\textbf{L}ocalization and \\textbf{P}runing \\textbf{S}trategy (\\textbf{ALPS})},\nan efficient algorithm that localizes the most task-sensitive attention heads\nand prunes by restricting attention training updates to these heads, thereby\nreducing alignment costs. Experimental results demonstrate that our method\nactivates only \\textbf{10\\%} of attention parameters during fine-tuning while\nachieving a \\textbf{2\\%} performance improvement over baselines on three tasks.\nMoreover, the identified task-specific heads are transferable across datasets\nand mitigate knowledge forgetting. Our work and findings provide a novel\nperspective on efficient LLM alignment."}
{"id": "2505.19381", "pdf": "https://arxiv.org/pdf/2505.19381", "abs": "https://arxiv.org/abs/2505.19381", "authors": ["Anqing Jiang", "Yu Gao", "Zhigang Sun", "Yiru Wang", "Jijun Wang", "Jinghao Chai", "Qian Cao", "Yuweng Heng", "Hao Jiang", "Zongzheng Zhang", "Xianda Guo", "Hao Sun", "Hao Zhao"], "title": "DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous Driving", "categories": ["cs.AI", "cs.CV", "cs.RO"], "comment": "4pages", "summary": "Research interest in end-to-end autonomous driving has surged owing to its\nfully differentiable design integrating modular tasks, i.e. perception,\nprediction and planing, which enables optimization in pursuit of the ultimate\ngoal. Despite the great potential of the end-to-end paradigm, existing methods\nsuffer from several aspects including expensive BEV (bird's eye view)\ncomputation, action diversity, and sub-optimal decision in complex real-world\nscenarios. To address these challenges, we propose a novel hybrid sparse-dense\ndiffusion policy, empowered by a Vision-Language Model (VLM), called Diff-VLA.\nWe explore the sparse diffusion representation for efficient multi-modal\ndriving behavior. Moreover, we rethink the effectiveness of VLM driving\ndecision and improve the trajectory generation guidance through deep\ninteraction across agent, map instances and VLM output. Our method shows\nsuperior performance in Autonomous Grand Challenge 2025 which contains\nchallenging real and reactive synthetic scenarios. Our methods achieves 45.0\nPDMS."}
{"id": "2505.18591", "pdf": "https://arxiv.org/pdf/2505.18591", "abs": "https://arxiv.org/abs/2505.18591", "authors": ["Joery A. de Vries", "Jinke He", "Mathijs M. de Weerdt", "Matthijs T. J. Spaan"], "title": "Bayesian Meta-Reinforcement Learning with Laplace Variational Recurrent Networks", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Meta-reinforcement learning trains a single reinforcement learning agent on a\ndistribution of tasks to quickly generalize to new tasks outside of the\ntraining set at test time. From a Bayesian perspective, one can interpret this\nas performing amortized variational inference on the posterior distribution\nover training tasks. Among the various meta-reinforcement learning approaches,\na common method is to represent this distribution with a point-estimate using a\nrecurrent neural network. We show how one can augment this point estimate to\ngive full distributions through the Laplace approximation, either at the start\nof, during, or after learning, without modifying the base model architecture.\nWith our approximation, we are able to estimate distribution statistics (e.g.,\nthe entropy) of non-Bayesian agents and observe that point-estimate based\nmethods produce overconfident estimators while not satisfying consistency.\nFurthermore, when comparing our approach to full-distribution based learning of\nthe task posterior, our method performs on par with variational baselines while\nhaving much fewer parameters."}
{"id": "2505.18842", "pdf": "https://arxiv.org/pdf/2505.18842", "abs": "https://arxiv.org/abs/2505.18842", "authors": ["Jiwan Chung", "Junhyeok Kim", "Siyeol Kim", "Jaeyoung Lee", "Min Soo Kim", "Youngjae Yu"], "title": "Don't Look Only Once: Towards Multimodal Interactive Reasoning with Selective Visual Revisitation", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "We present v1, a lightweight extension to Multimodal Large Language Models\n(MLLMs) that enables selective visual revisitation during inference. While\ncurrent MLLMs typically consume visual input only once and reason purely over\ninternal memory, v1 introduces a simple point-and-copy mechanism that allows\nthe model to dynamically retrieve relevant image regions throughout the\nreasoning process. This mechanism augments existing architectures with minimal\nmodifications, enabling contextual access to visual tokens based on the model's\nevolving hypotheses. To train this capability, we construct v1g, a dataset of\n300K multimodal reasoning traces with interleaved visual grounding annotations.\nExperiments on three multimodal mathematical reasoning benchmarks -- MathVista,\nMathVision, and MathVerse -- demonstrate that v1 consistently improves\nperformance over comparable baselines, particularly on tasks requiring\nfine-grained visual reference and multi-step reasoning. Our results suggest\nthat dynamic visual access is a promising direction for enhancing grounded\nmultimodal reasoning. Code, models, and data will be released to support future\nresearch."}
{"id": "2505.19383", "pdf": "https://arxiv.org/pdf/2505.19383", "abs": "https://arxiv.org/abs/2505.19383", "authors": ["Varun Reddy", "Yen-Ling Kuo"], "title": "CaseEdit: Enhancing Localized Commonsense Reasoning via Null-Space Constrained Knowledge Editing in Small Parameter Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) exhibit strong performance on factual recall and\ngeneral reasoning but struggle to adapt to user-specific, commonsense\nknowledge, a challenge particularly acute in small-parameter settings where\ncomputational efficiency is prioritized. We introduce CaseEdit, a new dataset\nand generation pipeline for evaluating localized, personalized commonsense\nknowledge editing in small LLMs to address this. Built upon the ATOMIC20/20\ncommonsense graph, CaseEdit uses a multi-stage inference process to generate\nboth typical and atypical contextual edits for household objects, paired with\ntargeted evaluation questions across four axes: reliability, generalization,\nlocality, and portability. We evaluate established knowledge editing methods\nusing CaseEdit and demonstrate that AlphaEdit, a technique employing null-space\nprojection to minimize interference with unrelated knowledge, consistently\noutperforms other methods when applied to an LLaMA 3.2 3B model, even in\nscalability tests, showing minimal ripple effects. Our results indicate that\nusing CaseEdit with effective editing techniques like AlphaEdit allows small\nmodels to internalize high-quality, context-sensitive common-sense knowledge,\npaving the way for lightweight, personalized assistants."}
{"id": "2505.18595", "pdf": "https://arxiv.org/pdf/2505.18595", "abs": "https://arxiv.org/abs/2505.18595", "authors": ["The Viet Bui", "Tien Mai", "Hong Thanh Nguyen"], "title": "MisoDICE: Multi-Agent Imitation from Unlabeled Mixed-Quality Demonstrations", "categories": ["cs.LG", "cs.AI", "cs.MA"], "comment": null, "summary": "We study offline imitation learning (IL) in cooperative multi-agent settings,\nwhere demonstrations have unlabeled mixed quality - containing both expert and\nsuboptimal trajectories. Our proposed solution is structured in two stages:\ntrajectory labeling and multi-agent imitation learning, designed jointly to\nenable effective learning from heterogeneous, unlabeled data. In the first\nstage, we combine advances in large language models and preference-based\nreinforcement learning to construct a progressive labeling pipeline that\ndistinguishes expert-quality trajectories. In the second stage, we introduce\nMisoDICE, a novel multi-agent IL algorithm that leverages these labels to learn\nrobust policies while addressing the computational complexity of large joint\nstate-action spaces. By extending the popular single-agent DICE framework to\nmulti-agent settings with a new value decomposition and mixing architecture,\nour method yields a convex policy optimization objective and ensures\nconsistency between global and local policies. We evaluate MisoDICE on multiple\nstandard multi-agent RL benchmarks and demonstrate superior performance,\nespecially when expert data is scarce."}
{"id": "2505.18845", "pdf": "https://arxiv.org/pdf/2505.18845", "abs": "https://arxiv.org/abs/2505.18845", "authors": ["Sagar Sapkota", "Mohammad Saqib Hasan", "Mubarak Shah", "Santu Karmaker"], "title": "Multi-Party Conversational Agents: A Survey", "categories": ["cs.CL"], "comment": null, "summary": "Multi-party Conversational Agents (MPCAs) are systems designed to engage in\ndialogue with more than two participants simultaneously. Unlike traditional\ntwo-party agents, designing MPCAs faces additional challenges due to the need\nto interpret both utterance semantics and social dynamics. This survey explores\nrecent progress in MPCAs by addressing three key questions: 1) Can agents model\neach participants' mental states? (State of Mind Modeling); 2) Can they\nproperly understand the dialogue content? (Semantic Understanding); and 3) Can\nthey reason about and predict future conversation flow? (Agent Action\nModeling). We review methods ranging from classical machine learning to Large\nLanguage Models (LLMs) and multi-modal systems. Our analysis underscores Theory\nof Mind (ToM) as essential for building intelligent MPCAs and highlights\nmulti-modal understanding as a promising yet underexplored direction. Finally,\nthis survey offers guidance to future researchers on developing more capable\nMPCAs."}
{"id": "2505.19402", "pdf": "https://arxiv.org/pdf/2505.19402", "abs": "https://arxiv.org/abs/2505.19402", "authors": ["Tai-Quan Peng", "Xuzhen Yang"], "title": "Recalibrating the Compass: Integrating Large Language Models into Classical Research Methods", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "This paper examines how large language models (LLMs) are transforming core\nquantitative methods in communication research in particular, and in the social\nsciences more broadly-namely, content analysis, survey research, and\nexperimental studies. Rather than replacing classical approaches, LLMs\nintroduce new possibilities for coding and interpreting text, simulating\ndynamic respondents, and generating personalized and interactive stimuli.\nDrawing on recent interdisciplinary work, the paper highlights both the\npotential and limitations of LLMs as research tools, including issues of\nvalidity, bias, and interpretability. To situate these developments\ntheoretically, the paper revisits Lasswell's foundational framework -- \"Who\nsays what, in which channel, to whom, with what effect?\" -- and demonstrates\nhow LLMs reconfigure message studies, audience analysis, and effects research\nby enabling interpretive variation, audience trajectory modeling, and\ncounterfactual experimentation. Revisiting the metaphor of the methodological\ncompass, the paper argues that classical research logics remain essential as\nthe field integrates LLMs and generative AI. By treating LLMs not only as\ntechnical instruments but also as epistemic and cultural tools, the paper calls\nfor thoughtful, rigorous, and imaginative use of LLMs in future communication\nand social science research."}
{"id": "2505.18604", "pdf": "https://arxiv.org/pdf/2505.18604", "abs": "https://arxiv.org/abs/2505.18604", "authors": ["Isaac Ning Lee", "Leila Mahmoodi", "Trung Le", "Mehrtash Harandi"], "title": "Exemplar-Free Continual Learning for State Space Models", "categories": ["cs.LG"], "comment": "41 pages, 4 figures", "summary": "State-Space Models (SSMs) excel at capturing long-range dependencies with\nstructured recurrence, making them well-suited for sequence modeling. However,\ntheir evolving internal states pose challenges in adapting them under Continual\nLearning (CL). This is particularly difficult in exemplar-free settings, where\nthe absence of prior data leaves updates to the dynamic SSM states\nunconstrained, resulting in catastrophic forgetting. To address this, we\npropose Inf-SSM, a novel and simple geometry-aware regularization method that\nutilizes the geometry of the infinite-dimensional Grassmannian to constrain\nstate evolution during CL. Unlike classical continual learning methods that\nconstrain weight updates, Inf-SSM regularizes the infinite-horizon evolution of\nSSMs encoded in their extended observability subspace. We show that enforcing\nthis regularization requires solving a matrix equation known as the Sylvester\nequation, which typically incurs $\\mathcal{O}(n^3)$ complexity. We develop a\n$\\mathcal{O}(n^2)$ solution by exploiting the structure and properties of SSMs.\nThis leads to an efficient regularization mechanism that can be seamlessly\nintegrated into existing CL methods. Comprehensive experiments on challenging\nbenchmarks, including ImageNet-R and Caltech-256, demonstrate a significant\nreduction in forgetting while improving accuracy across sequential tasks."}
{"id": "2505.18853", "pdf": "https://arxiv.org/pdf/2505.18853", "abs": "https://arxiv.org/abs/2505.18853", "authors": ["Alexander Shabalin", "Viacheslav Meshchaninov", "Dmitry Vetrov"], "title": "Smoothie: Smoothing Diffusion on Token Embeddings for Text Generation", "categories": ["cs.CL"], "comment": "17 pages, 2 figures, 8 tables", "summary": "Diffusion models have achieved state-of-the-art performance in generating\nimages, audio, and video, but their adaptation to text remains challenging due\nto its discrete nature. Prior approaches either apply Gaussian diffusion in\ncontinuous latent spaces, which inherits semantic structure but struggles with\ntoken decoding, or operate in categorical simplex space, which respect\ndiscreteness but disregard semantic relation between tokens. In this paper, we\npropose Smoothing Diffusion on Token Embeddings (Smoothie), a novel diffusion\nmethod that combines the strengths of both approaches by progressively\nsmoothing token embeddings based on semantic similarity. This technique enables\ngradual information removal while maintaining a natural decoding process.\nExperimental results on several sequence-to-sequence generation tasks\ndemonstrate that Smoothie outperforms existing diffusion-based models in\ngeneration quality. Furthermore, ablation studies show that our proposed\ndiffusion space yields better performance than both the standard embedding\nspace and the categorical simplex. Our code is available at\nhttps://github.com/ashaba1in/smoothie."}
{"id": "2505.19406", "pdf": "https://arxiv.org/pdf/2505.19406", "abs": "https://arxiv.org/abs/2505.19406", "authors": ["Tianle Li", "Jihai Zhang", "Yongming Rao", "Yu Cheng"], "title": "Unveiling the Compositional Ability Gap in Vision-Language Reasoning Model", "categories": ["cs.AI"], "comment": null, "summary": "While large language models (LLMs) demonstrate strong reasoning capabilities\nutilizing reinforcement learning (RL) with verifiable reward, whether large\nvision-language models (VLMs) can directly inherit such capabilities through\nsimilar post-training strategies remains underexplored. In this work, we\nconduct a systematic compositional probing study to evaluate whether current\nVLMs trained with RL or other post-training strategies can compose capabilities\nacross modalities or tasks under out-of-distribution conditions. We design a\nsuite of diagnostic tasks that train models on unimodal tasks or isolated\nreasoning skills, and evaluate them on multimodal, compositional variants\nrequiring skill integration. Through comparisons between supervised fine-tuning\n(SFT) and RL-trained models, we identify three key findings: (1) RL-trained\nmodels consistently outperform SFT on compositional generalization,\ndemonstrating better integration of learned skills; (2) although VLMs achieve\nstrong performance on individual tasks, they struggle to generalize\ncompositionally under cross-modal and cross-task scenario, revealing a\nsignificant gap in current training strategies; (3) enforcing models to\nexplicitly describe visual content before reasoning (e.g.,\ncaption-before-thinking), along with rewarding progressive vision-to-text\ngrounding, yields notable gains. It highlights two essential ingredients for\nimproving compositionality in VLMs: visual-to-text alignment and accurate\nvisual grounding. Our findings shed light on the current limitations of\nRL-based reasoning VLM training and provide actionable insights toward building\nmodels that reason compositionally across modalities and tasks."}
{"id": "2505.18622", "pdf": "https://arxiv.org/pdf/2505.18622", "abs": "https://arxiv.org/abs/2505.18622", "authors": ["Kourosh Shahnazari", "Seyed Moein Ayyoubzadeh", "Mohammadali Keshtparvar", "Pegah Ghaffari"], "title": "Trust, or Don't Predict: Introducing the CWSA Family for Confidence-Aware Model Evaluation", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "In recent machine learning systems, confidence scores are being utilized more\nand more to manage selective prediction, whereby a model can abstain from\nmaking a prediction when it is unconfident. Yet, conventional metrics like\naccuracy, expected calibration error (ECE), and area under the risk-coverage\ncurve (AURC) do not capture the actual reliability of predictions. These\nmetrics either disregard confidence entirely, dilute valuable localized\ninformation through averaging, or neglect to suitably penalize overconfident\nmisclassifications, which can be particularly detrimental in real-world\nsystems. We introduce two new metrics Confidence-Weighted Selective Accuracy\n(CWSA) and its normalized variant CWSA+ that offer a principled and\ninterpretable way to evaluate predictive models under confidence thresholds.\nUnlike existing methods, our metrics explicitly reward confident accuracy and\npenalize overconfident mistakes. They are threshold-local, decomposable, and\nusable in both evaluation and deployment settings where trust and risk must be\nquantified. Through exhaustive experiments on both real-world data sets (MNIST,\nCIFAR-10) and artificial model variants (calibrated, overconfident,\nunderconfident, random, perfect), we show that CWSA and CWSA+ both effectively\ndetect nuanced failure modes and outperform classical metrics in\ntrust-sensitive tests. Our results confirm that CWSA is a sound basis for\ndeveloping and assessing selective prediction systems for safety-critical\ndomains."}
{"id": "2505.18859", "pdf": "https://arxiv.org/pdf/2505.18859", "abs": "https://arxiv.org/abs/2505.18859", "authors": ["Yuxiang Liu", "Kevin Chen-Chuan Chang"], "title": "Writing Like the Best: Exemplar-Based Expository Text Generation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025. Camera-ready version", "summary": "We introduce the Exemplar-Based Expository Text Generation task, aiming to\ngenerate an expository text on a new topic using an exemplar on a similar\ntopic. Current methods fall short due to their reliance on extensive exemplar\ndata, difficulty in adapting topic-specific content, and issues with long-text\ncoherence. To address these challenges, we propose the concept of Adaptive\nImitation and present a novel Recurrent Plan-then-Adapt (RePA) framework. RePA\nleverages large language models (LLMs) for effective adaptive imitation through\na fine-grained plan-then-adapt process. RePA also enables recurrent\nsegment-by-segment imitation, supported by two memory structures that enhance\ninput clarity and output coherence. We also develop task-specific evaluation\nmetrics--imitativeness, adaptiveness, and adaptive-imitativeness--using LLMs as\nevaluators. Experimental results across our collected three diverse datasets\ndemonstrate that RePA surpasses existing baselines in producing factual,\nconsistent, and relevant texts for this task."}
{"id": "2505.19409", "pdf": "https://arxiv.org/pdf/2505.19409", "abs": "https://arxiv.org/abs/2505.19409", "authors": ["Ruihang Wang", "Minghao Li", "Zhiwei Cao", "Jimin Jia", "Kyle Guan", "Yonggang Wen"], "title": "Fusion Intelligence for Digital Twinning AI Data Centers: A Synergistic GenAI-PhyAI Approach", "categories": ["cs.AI"], "comment": null, "summary": "The explosion in artificial intelligence (AI) applications is pushing the\ndevelopment of AI-dedicated data centers (AIDCs), creating management\nchallenges that traditional methods and standalone AI solutions struggle to\naddress. While digital twins are beneficial for AI-based design validation and\noperational optimization, current AI methods for their creation face\nlimitations. Specifically, physical AI (PhyAI) aims to capture the underlying\nphysical laws, which demands extensive, case-specific customization, and\ngenerative AI (GenAI) can produce inaccurate or hallucinated results. We\npropose Fusion Intelligence, a novel framework synergizing GenAI's automation\nwith PhyAI's domain grounding. In this dual-agent collaboration, GenAI\ninterprets natural language prompts to generate tokenized AIDC digital twins.\nSubsequently, PhyAI optimizes these generated twins by enforcing physical\nconstraints and assimilating real-time data. Case studies demonstrate the\nadvantages of our framework in automating the creation and validation of AIDC\ndigital twins. These twins deliver predictive analytics to support power usage\neffectiveness (PUE) optimization in the design stage. With operational data\ncollected, the digital twin accuracy is further improved compared with pure\nphysics-based models developed by human experts. Fusion Intelligence offers a\npromising pathway to accelerate digital transformation. It enables more\nreliable and efficient AI-driven digital transformation for a broad range of\nmission-critical infrastructures."}
{"id": "2505.18629", "pdf": "https://arxiv.org/pdf/2505.18629", "abs": "https://arxiv.org/abs/2505.18629", "authors": ["Yixuan Wang", "Yijun Liu", "Shiyu ji", "Yuzhuang Xu", "Yang Xu", "Qingfu Zhu", "Wanxiang Che"], "title": "Think Before You Accept: Semantic Reflective Verification for Faster Speculative Decoding", "categories": ["cs.LG"], "comment": "15 pages, 7 figures", "summary": "Large language models (LLMs) suffer from high inference latency due to the\nauto-regressive decoding process. Speculative decoding accelerates inference by\ngenerating multiple draft tokens using a lightweight model and verifying them\nin parallel. However, existing verification methods rely heavily on\ndistributional consistency while overlooking semantic correctness, thereby\nlimiting the potential speedup of speculative decoding. While some methods\nemploy additional models for relaxed verification of draft tokens, they often\nfail to generalize effectively to more diverse or open-domain settings. In this\nwork, we propose Reflective Verification, a training-free and semantics-aware\napproach that achieves a better trade-off between correctness and efficiency.\nSpecifically, we leverage the inherent reflective capacity of LLMs to\nsemantically assess the correctness of draft tokens in parallel during\nverification. Using prompt-based probing, we obtain both the original and\nreflective distributions of draft tokens in a single forward pass. The fusion\nof these distributions enables semantic-level verification of draft tokens that\nincorporates both consistency and correctness. Experiments across multiple\ndomain benchmarks and model scales demonstrate that our method significantly\nincreases the acceptance length of draft tokens without compromising model\nperformance. Furthermore, we find that the proposed Reflective Verification is\northogonal to existing statistical verification methods, and their combination\nyields additional 5$\\sim$15\\% improvements in decoding speed."}
{"id": "2505.18864", "pdf": "https://arxiv.org/pdf/2505.18864", "abs": "https://arxiv.org/abs/2505.18864", "authors": ["Binhao Ma", "Hanqing Guo", "Zhengping Jay Luo", "Rui Duan"], "title": "Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced the naturalness and flexibility of human computer\ninteraction by enabling seamless understanding across text, vision, and audio\nmodalities. Among these, voice enabled models such as SpeechGPT have\ndemonstrated considerable improvements in usability, offering expressive, and\nemotionally responsive interactions that foster deeper connections in real\nworld communication scenarios. However, the use of voice introduces new\nsecurity risks, as attackers can exploit the unique characteristics of spoken\nlanguage, such as timing, pronunciation variability, and speech to text\ntranslation, to craft inputs that bypass defenses in ways not seen in\ntext-based systems. Despite substantial research on text based jailbreaks, the\nvoice modality remains largely underexplored in terms of both attack strategies\nand defense mechanisms. In this work, we present an adversarial attack\ntargeting the speech input of aligned MLLMs in a white box scenario.\nSpecifically, we introduce a novel token level attack that leverages access to\nthe model's speech tokenization to generate adversarial token sequences. These\nsequences are then synthesized into audio prompts, which effectively bypass\nalignment safeguards and to induce prohibited outputs. Evaluated on SpeechGPT,\nour approach achieves up to 89 percent attack success rate across multiple\nrestricted tasks, significantly outperforming existing voice based jailbreak\nmethods. Our findings shed light on the vulnerabilities of voice-enabled\nmultimodal systems and to help guide the development of more robust\nnext-generation MLLMs."}
{"id": "2505.19414", "pdf": "https://arxiv.org/pdf/2505.19414", "abs": "https://arxiv.org/abs/2505.19414", "authors": ["Ruihang Wang", "Zhiwei Cao", "Qingang Zhang", "Rui Tan", "Yonggang Wen", "Tommy Leung", "Stuart Kennedy", "Justin Teoh"], "title": "Toward Physics-Informed Machine Learning for Data Center Operations: A Tropical Case Study", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Data centers are the backbone of computing capacity. Operating data centers\nin the tropical regions faces unique challenges due to consistently high\nambient temperature and elevated relative humidity throughout the year. These\nconditions result in increased cooling costs to maintain the reliability of the\ncomputing systems. While existing machine learning-based approaches have\ndemonstrated potential to elevate operations to a more proactive and\nintelligent level, their deployment remains dubious due to concerns about model\nextrapolation capabilities and associated system safety issues. To address\nthese concerns, this article proposes incorporating the physical\ncharacteristics of data centers into traditional data-driven machine learning\nsolutions. We begin by introducing the data center system, including the\nrelevant multiphysics processes and the data-physics availability. Next, we\noutline the associated modeling and optimization problems and propose an\nintegrated, physics-informed machine learning system to address them. Using the\nproposed system, we present relevant applications across varying levels of\noperational intelligence. A case study on an industry-grade tropical data\ncenter is provided to demonstrate the effectiveness of our approach. Finally,\nwe discuss key challenges and highlight potential future directions."}
{"id": "2505.18636", "pdf": "https://arxiv.org/pdf/2505.18636", "abs": "https://arxiv.org/abs/2505.18636", "authors": ["Tim G. Zhou", "Evan Shelhamer", "Geoff Pleiss"], "title": "Asymmetric Duos: Sidekicks Improve Uncertainty", "categories": ["cs.LG"], "comment": "24 pages, 14 figures", "summary": "The go-to strategy to apply deep networks in settings where uncertainty\ninforms decisions--ensembling multiple training runs with random\ninitializations--is ill-suited for the extremely large-scale models and\npractical fine-tuning workflows of today. We introduce a new cost-effective\nstrategy for improving the uncertainty quantification and downstream decisions\nof a large model (e.g. a fine-tuned ViT-B): coupling it with a less accurate\nbut much smaller \"sidekick\" (e.g. a fine-tuned ResNet-34) with a fraction of\nthe computational cost. We propose aggregating the predictions of this\n\\emph{Asymmetric Duo} by simple learned weighted averaging. Surprisingly,\ndespite their inherent asymmetry, the sidekick model almost never harms the\nperformance of the larger model. In fact, across five image classification\nbenchmarks and a variety of model architectures and training schemes (including\nsoups), Asymmetric Duos significantly improve accuracy, uncertainty\nquantification, and selective classification metrics with only ${\\sim}10-20\\%$\nmore computation."}
{"id": "2505.18867", "pdf": "https://arxiv.org/pdf/2505.18867", "abs": "https://arxiv.org/abs/2505.18867", "authors": ["Ming Cheng", "Jiaying Gong", "Hoda Eldardiry"], "title": "Sci-LoRA: Mixture of Scientific LoRAs for Cross-Domain Lay Paraphrasing", "categories": ["cs.CL", "cs.LG"], "comment": "18 pages, 3 figures, ACL 2025 Findings", "summary": "Lay paraphrasing aims to make scientific information accessible to audiences\nwithout technical backgrounds. However, most existing studies focus on a single\ndomain, such as biomedicine. With the rise of interdisciplinary research, it is\nincreasingly necessary to comprehend knowledge spanning multiple technical\nfields. To address this, we propose Sci-LoRA, a model that leverages a mixture\nof LoRAs fine-tuned on multiple scientific domains. In particular, Sci-LoRA\ndynamically generates and applies weights for each LoRA, enabling it to adjust\nthe impact of different domains based on the input text, without requiring\nexplicit domain labels. To balance domain-specific knowledge and generalization\nacross various domains, Sci-LoRA integrates information at both the data and\nmodel levels. This dynamic fusion enhances the adaptability and performance\nacross various domains. Experimental results across twelve domains on five\npublic datasets show that Sci-LoRA significantly outperforms state-of-the-art\nlarge language models and demonstrates flexible generalization and adaptability\nin cross-domain lay paraphrasing."}
{"id": "2505.19436", "pdf": "https://arxiv.org/pdf/2505.19436", "abs": "https://arxiv.org/abs/2505.19436", "authors": ["Ye Ye"], "title": "Task Memory Engine: Spatial Memory for Robust Multi-Step LLM Agents", "categories": ["cs.AI", "cs.CL", "I.2.6; I.2.8; H.3.3"], "comment": "Under review. 9 pages main content, 15 pages appendix, 5 figures", "summary": "Large Language Models (LLMs) falter in multi-step interactions -- often\nhallucinating, repeating actions, or misinterpreting user corrections -- due to\nreliance on linear, unstructured context. This fragility stems from the lack of\npersistent memory to track evolving goals and task dependencies, undermining\ntrust in autonomous agents. We introduce the Task Memory Engine (TME), a\nmodular memory controller that transforms existing LLMs into robust,\nrevision-aware agents without fine-tuning. TME implements a spatial memory\nframework that replaces flat context with graph-based structures to support\nconsistent, multi-turn reasoning. Departing from linear concatenation and\nReAct-style prompting, TME builds a dynamic task graph -- either a tree or\ndirected acyclic graph (DAG) -- to map user inputs to subtasks, align them with\nprior context, and enable dependency-tracked revisions. Its Task Representation\nand Intent Management (TRIM) component models task semantics and user intent to\nensure accurate interpretation. Across four multi-turn scenarios-trip planning,\ncooking, meeting scheduling, and shopping cart editing -- TME eliminates 100%\nof hallucinations and misinterpretations in three tasks, and reduces\nhallucinations by 66.7% and misinterpretations by 83.3% across 27 user turns,\noutperforming ReAct. TME's modular design supports plug-and-play deployment and\ndomain-specific customization, adaptable to both personal assistants and\nenterprise automation. We release TME's codebase, benchmarks, and components as\nopen-source resources, enabling researchers to develop reliable LLM agents.\nTME's scalable architecture addresses a critical gap in agent performance\nacross complex, interactive settings."}
{"id": "2505.18640", "pdf": "https://arxiv.org/pdf/2505.18640", "abs": "https://arxiv.org/abs/2505.18640", "authors": ["Jian Liang", "Wenke Huang", "Xianda Guo", "Guancheng Wan", "Bo Du", "Mang Ye"], "title": "ThanoRA: Task Heterogeneity-Aware Multi-Task Low-Rank Adaptation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Low-Rank Adaptation (LoRA) is widely adopted for downstream fine-tuning of\nfoundation models due to its efficiency and zero additional inference cost.\nMany real-world applications require foundation models to specialize in\nmultiple tasks simultaneously, motivating the need for efficient multi-task\nadaptation. While recent approaches integrate LoRA with mixture-of-experts\n(MoE) to address this, the use of routers prevents parameter mergeability,\nwhich increases inference overhead and hinders unified multi-task adaptation,\nthereby limiting deployment practicality. In this work, we propose ThanoRA, a\nTask Heterogeneity-Aware Multi-Task Low-Rank Adaptation framework that enables\nmulti-task adaptation while preserving the inference efficiency of LoRA.\nThanoRA jointly models task heterogeneity and mitigates subspace interference\nthroughout training. Specifically, motivated by inherent differences in\ncomplexity and heterogeneity across tasks, ThanoRA constructs task-specific\nLoRA subspaces at initialization, enabling fine-grained knowledge injection\naligned with task heterogeneity. Furthermore, to prevent task interference and\nsubspace collapse during multi-task training, ThanoRA introduces a\nsubspace-preserving regularization that maintains the independence of\ntask-specific representations. With the synergy of both components, ThanoRA\nenables efficient and unified multi-task adaptation. Extensive experiments\nacross multimodal and text-only benchmarks under varying multi-task mixtures\ndemonstrate that ThanoRA consistently achieves robust and superior performance\nover strong baselines without introducing additional inference overhead. Our\ncode is publicly available at: https://github.com/LiangJian24/ThanoRA."}
{"id": "2505.18878", "pdf": "https://arxiv.org/pdf/2505.18878", "abs": "https://arxiv.org/abs/2505.18878", "authors": ["Kung-Hsiang Huang", "Akshara Prabhakar", "Onkar Thorat", "Divyansh Agarwal", "Prafulla Kumar Choubey", "Yixin Mao", "Silvio Savarese", "Caiming Xiong", "Chien-Sheng Wu"], "title": "CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While AI agents hold transformative potential in business, effective\nperformance benchmarking is hindered by the scarcity of public, realistic\nbusiness data on widely used platforms. Existing benchmarks often lack fidelity\nin their environments, data, and agent-user interactions, with limited coverage\nof diverse business scenarios and industries. To address these gaps, we\nintroduce CRMArena-Pro, a novel benchmark for holistic, realistic assessment of\nLLM agents in diverse professional settings. CRMArena-Pro expands on CRMArena\nwith nineteen expert-validated tasks across sales, service, and 'configure,\nprice, and quote' processes, for both Business-to-Business and\nBusiness-to-Customer scenarios. It distinctively incorporates multi-turn\ninteractions guided by diverse personas and robust confidentiality awareness\nassessments. Experiments reveal leading LLM agents achieve only around 58%\nsingle-turn success on CRMArena-Pro, with performance dropping significantly to\napproximately 35% in multi-turn settings. While Workflow Execution proves more\ntractable for top agents (over 83% single-turn success), other evaluated\nbusiness skills present greater challenges. Furthermore, agents exhibit\nnear-zero inherent confidentiality awareness; though targeted prompting can\nimprove this, it often compromises task performance. These findings highlight a\nsubstantial gap between current LLM capabilities and enterprise demands,\nunderscoring the need for advancements in multi-turn reasoning, confidentiality\nadherence, and versatile skill acquisition."}
{"id": "2505.19442", "pdf": "https://arxiv.org/pdf/2505.19442", "abs": "https://arxiv.org/abs/2505.19442", "authors": ["Dutao Zhang", "Sergey Kovalchuk", "YuLong He"], "title": "Style2Code: A Style-Controllable Code Generation Framework with Dual-Modal Contrastive Representation Learning", "categories": ["cs.AI", "I.2.6; D.2.3"], "comment": "10 pages, 5 figures, submitted to EMNLP 2025 (Industry Track)", "summary": "Controllable code generation, the ability to synthesize code that follows a\nspecified style while maintaining functionality, remains a challenging task. We\npropose a two-stage training framework combining contrastive learning and\nconditional decoding to enable flexible style control. The first stage aligns\ncode style representations with semantic and structural features. In the second\nstage, we fine-tune a language model (e.g., Flan-T5) conditioned on the learned\nstyle vector to guide generation. Our method supports style interpolation and\nuser personalization via lightweight mixing. Compared to prior work, our\nunified framework offers improved stylistic control without sacrificing code\ncorrectness. This is among the first approaches to combine contrastive\nalignment with conditional decoding for style-guided code generation."}
{"id": "2505.18647", "pdf": "https://arxiv.org/pdf/2505.18647", "abs": "https://arxiv.org/abs/2505.18647", "authors": ["Kiet Bennema ten Brinke", "Koen Minartz", "Vlado Menkovski"], "title": "Flow Matching for Geometric Trajectory Simulation", "categories": ["cs.LG", "cs.AI"], "comment": "20 pages, 17 figures", "summary": "The simulation of N-body systems is a fundamental problem with applications\nin a wide range of fields, such as molecular dynamics, biochemistry, and\npedestrian dynamics. Machine learning has become an invaluable tool for scaling\nphysics-based simulators and developing models directly from experimental data.\nIn particular, recent advances based on deep generative modeling and geometric\ndeep learning have enabled probabilistic simulation by modeling complex\ndistributions over trajectories while respecting the permutation symmetry that\nis fundamental to N-body systems. However, to generate realistic trajectories,\nexisting methods must learn complex transformations starting from uninformed\nnoise and do not allow for the exploitation of domain-informed priors. In this\nwork, we propose STFlow to address this limitation. By leveraging flow matching\nand data-dependent couplings, STFlow facilitates physics-informed simulation of\ngeometric trajectories without sacrificing model expressivity or scalability.\nOur evaluation on N-body dynamical systems, molecular dynamics, and pedestrian\ndynamics benchmarks shows that STFlow produces significantly lower prediction\nerrors while enabling more efficient inference, highlighting the benefits of\nemploying physics-informed prior distributions in probabilistic geometric\ntrajectory modeling."}
{"id": "2505.18903", "pdf": "https://arxiv.org/pdf/2505.18903", "abs": "https://arxiv.org/abs/2505.18903", "authors": ["Valentin Barriere", "Nahuel Gomez", "Leo Hemamou", "Sofia Callejas", "Brian Ravenet"], "title": "StandUp4AI: A New Multilingual Dataset for Humor Detection in Stand-up Comedy Videos", "categories": ["cs.CL"], "comment": null, "summary": "Aiming towards improving current computational models of humor detection, we\npropose a new multimodal dataset of stand-up comedies, in seven languages:\nEnglish, French, Spanish, Italian, Portuguese, Hungarian and Czech. Our dataset\nof more than 330 hours, is at the time of writing the biggest available for\nthis type of task, and the most diverse. The whole dataset is automatically\nannotated in laughter (from the audience), and the subpart left for model\nvalidation is manually annotated. Contrary to contemporary approaches, we do\nnot frame the task of humor detection as a binary sequence classification, but\nas word-level sequence labeling, in order to take into account all the context\nof the sequence and to capture the continuous joke tagging mechanism typically\noccurring in natural conversations. As par with unimodal baselines results, we\npropose a method for e propose a method to enhance the automatic laughter\ndetection based on Audio Speech Recognition errors. Our code and data are\navailable online: https://tinyurl.com/EMNLPHumourStandUpPublic"}
{"id": "2505.19457", "pdf": "https://arxiv.org/pdf/2505.19457", "abs": "https://arxiv.org/abs/2505.19457", "authors": ["Guilong Lu", "Xuntao Guo", "Rongjunchen Zhang", "Wenqiao Zhu", "Ji Liu"], "title": "BizFinBench: A Business-Driven Real-World Financial Benchmark for Evaluating LLMs", "categories": ["cs.AI", "cs.CE", "cs.CL"], "comment": "Project Page: https://hithink-research.github.io/BizFinBench/", "summary": "Large language models excel in general tasks, yet assessing their reliability\nin logic-heavy, precision-critical domains like finance, law, and healthcare\nremains challenging. To address this, we introduce BizFinBench, the first\nbenchmark specifically designed to evaluate LLMs in real-world financial\napplications. BizFinBench consists of 6,781 well-annotated queries in Chinese,\nspanning five dimensions: numerical calculation, reasoning, information\nextraction, prediction recognition, and knowledge-based question answering,\ngrouped into nine fine-grained categories. The benchmark includes both\nobjective and subjective metrics. We also introduce IteraJudge, a novel LLM\nevaluation method that reduces bias when LLMs serve as evaluators in objective\nmetrics. We benchmark 25 models, including both proprietary and open-source\nsystems. Extensive experiments show that no model dominates across all tasks.\nOur evaluation reveals distinct capability patterns: (1) In Numerical\nCalculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while\nsmaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning,\nproprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with\nopen-source models trailing by up to 19.49 points; (3) In Information\nExtraction, the performance spread is the largest, with DeepSeek-R1 scoring\n71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition,\nperformance variance is minimal, with top models scoring between 39.16 and\n50.00. We find that while current LLMs handle routine finance queries\ncompetently, they struggle with complex scenarios requiring cross-concept\nreasoning. BizFinBench offers a rigorous, business-aligned benchmark for future\nresearch. The code and dataset are available at\nhttps://github.com/HiThink-Research/BizFinBench."}
{"id": "2505.18656", "pdf": "https://arxiv.org/pdf/2505.18656", "abs": "https://arxiv.org/abs/2505.18656", "authors": ["Dev Gurung", "Shiva Raj Pokhrel"], "title": "LLM-QFL: Distilling Large Language Model for Quantum Federated Learning", "categories": ["cs.LG"], "comment": null, "summary": "Inspired by the power of large language models (LLMs), our research adapts\nthem to quantum federated learning (QFL) to boost efficiency and performance.\nWe propose a federated fine-tuning method that distills an LLM within QFL,\nallowing each client to locally adapt the model to its own data while\npreserving privacy and reducing unnecessary global updates. The fine-tuned LLM\nalso acts as a reinforcement agent, optimizing QFL by adjusting optimizer\nsteps, cutting down communication rounds, and intelligently selecting clients.\nExperiments show significant efficiency gains. We pioneer a synergy between LLM\nand QFL, offering: i) practical efficiency: Reduced communication costs and\nfaster convergence. ii) theoretical rigor: Provable guarantees for adaptive\nfederated optimization. iii) scalability: PEFT methods (LoRA, QLoRA) enable\ndeployment on resource-constrained quantum devices. Code implementation is\navailable here 1."}
{"id": "2505.18905", "pdf": "https://arxiv.org/pdf/2505.18905", "abs": "https://arxiv.org/abs/2505.18905", "authors": ["Kweku Andoh Yamoah", "Jackson Weako", "Emmanuel J. Dorley"], "title": "Building a Functional Machine Translation Corpus for Kpelle", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we introduce the first publicly available English-Kpelle\ndataset for machine translation, comprising over 2000 sentence pairs drawn from\neveryday communication, religious texts, and educational materials. By\nfine-tuning Meta's No Language Left Behind(NLLB) model on two versions of the\ndataset, we achieved BLEU scores of up to 30 in the Kpelle-to-English\ndirection, demonstrating the benefits of data augmentation. Our findings align\nwith NLLB-200 benchmarks on other African languages, underscoring Kpelle's\npotential for competitive performance despite its low-resource status. Beyond\nmachine translation, this dataset enables broader NLP tasks, including speech\nrecognition and language modelling. We conclude with a roadmap for future\ndataset expansion, emphasizing orthographic consistency, community-driven\nvalidation, and interdisciplinary collaboration to advance inclusive language\ntechnology development for Kpelle and other low-resourced Mande languages."}
{"id": "2505.19466", "pdf": "https://arxiv.org/pdf/2505.19466", "abs": "https://arxiv.org/abs/2505.19466", "authors": ["Hongyu Liang", "Yuting Zheng", "Yihan Li", "Yiran Zhang", "Shiyu Liang"], "title": "Origin Tracer: A Method for Detecting LoRA Fine-Tuning Origins in LLMs", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "As large language models (LLMs) continue to advance, their deployment often\ninvolves fine-tuning to enhance performance on specific downstream tasks.\nHowever, this customization is sometimes accompanied by misleading claims about\nthe origins, raising significant concerns about transparency and trust within\nthe open-source community. Existing model verification techniques typically\nassess functional, representational, and weight similarities. However, these\napproaches often struggle against obfuscation techniques, such as permutations\nand scaling transformations. To address this limitation, we propose a novel\ndetection method Origin-Tracer that rigorously determines whether a model has\nbeen fine-tuned from a specified base model. This method includes the ability\nto extract the LoRA rank utilized during the fine-tuning process, providing a\nmore robust verification framework. This framework is the first to provide a\nformalized approach specifically aimed at pinpointing the sources of model\nfine-tuning. We empirically validated our method on thirty-one diverse\nopen-source models under conditions that simulate real-world obfuscation\nscenarios. We empirically analyze the effectiveness of our framework and\nfinally, discuss its limitations. The results demonstrate the effectiveness of\nour approach and indicate its potential to establish new benchmarks for model\nverification."}
{"id": "2505.18671", "pdf": "https://arxiv.org/pdf/2505.18671", "abs": "https://arxiv.org/abs/2505.18671", "authors": ["Giacomo Turri", "Luigi Bonati", "Kai Zhu", "Massimiliano Pontil", "Pietro Novelli"], "title": "Self-Supervised Evolution Operator Learning for High-Dimensional Dynamical Systems", "categories": ["cs.LG", "math.DS"], "comment": null, "summary": "We introduce an encoder-only approach to learn the evolution operators of\nlarge-scale non-linear dynamical systems, such as those describing complex\nnatural phenomena. Evolution operators are particularly well-suited for\nanalyzing systems that exhibit complex spatio-temporal patterns and have become\na key analytical tool across various scientific communities. As terabyte-scale\nweather datasets and simulation tools capable of running millions of molecular\ndynamics steps per day are becoming commodities, our approach provides an\neffective tool to make sense of them from a data-driven perspective. The core\nof it lies in a remarkable connection between self-supervised representation\nlearning methods and the recently established learning theory of evolution\noperators. To show the usefulness of the proposed method, we test it across\nmultiple scientific domains: explaining the folding dynamics of small proteins,\nthe binding process of drug-like molecules in host sites, and autonomously\nfinding patterns in climate data. Code and data to reproduce the experiments\nare made available open source."}
{"id": "2505.18906", "pdf": "https://arxiv.org/pdf/2505.18906", "abs": "https://arxiv.org/abs/2505.18906", "authors": ["Abhijit Chakraborty", "Chahana Dahal", "Vivek Gupta"], "title": "Federated Retrieval-Augmented Generation: A Systematic Mapping Study", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Federated Retrieval-Augmented Generation (Federated RAG) combines Federated\nLearning (FL), which enables distributed model training without exposing raw\ndata, with Retrieval-Augmented Generation (RAG), which improves the factual\naccuracy of language models by grounding outputs in external knowledge. As\nlarge language models are increasingly deployed in privacy-sensitive domains\nsuch as healthcare, finance, and personalized assistance, Federated RAG offers\na promising framework for secure, knowledge-intensive natural language\nprocessing (NLP). To the best of our knowledge, this paper presents the first\nsystematic mapping study of Federated RAG, covering literature published\nbetween 2020 and 2025. Following Kitchenham's guidelines for evidence-based\nsoftware engineering, we develop a structured classification of research\nfocuses, contribution types, and application domains. We analyze architectural\npatterns, temporal trends, and key challenges, including privacy-preserving\nretrieval, cross-client heterogeneity, and evaluation limitations. Our findings\nsynthesize a rapidly evolving body of research, identify recurring design\npatterns, and surface open questions, providing a foundation for future work at\nthe intersection of RAG and federated systems."}
{"id": "2505.19474", "pdf": "https://arxiv.org/pdf/2505.19474", "abs": "https://arxiv.org/abs/2505.19474", "authors": ["Xinmiao Hu", "Chun Wang", "Ruihe An", "ChenYu Shao", "Xiaojun Ye", "Sheng Zhou", "Liangcheng Li"], "title": "Causal-LLaVA: Causal Disentanglement for Mitigating Hallucination in Multimodal Large Language Models", "categories": ["cs.AI"], "comment": "21 pages, 19 figures, Submitted to NeurIPS 2025", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated strong performance\nin visual understanding tasks, yet they often suffer from object\nhallucinations--generating descriptions of objects that are inconsistent with\nor entirely absent from the input. This issue is closely related to dataset\nbiases, where frequent co-occurrences of objects lead to entangled semantic\nrepresentations across modalities. As a result, models may erroneously activate\nobject representations that are commonly associated with the input but not\nactually present.\n  To address this, we propose a causality-driven disentanglement framework that\nmitigates hallucinations through causal intervention. Our approach includes a\nCausal-Driven Projector in the visual pathway and a Causal Intervention Module\nintegrated into the final transformer layer of the language model. These\ncomponents work together to reduce spurious correlations caused by biased\ntraining data.\n  Experimental results show that our method significantly reduces\nhallucinations while maintaining strong performance on multiple multimodal\nbenchmarks. Visualization analyses further confirm improved separability of\nobject representations.\n  The code is available at: https://github.com/IgniSavium/Causal-LLaVA"}
{"id": "2505.18672", "pdf": "https://arxiv.org/pdf/2505.18672", "abs": "https://arxiv.org/abs/2505.18672", "authors": ["Hongzheng Yang", "Yongqiang Chen", "Zeyu Qin", "Tongliang Liu", "Chaowei Xiao", "Kun Zhang", "Bo Han"], "title": "Does Representation Intervention Really Identify Desired Concepts and Elicit Alignment?", "categories": ["cs.LG", "stat.ML"], "comment": "Hongzheng and Yongqiang contributed equally; project page:\n  https://causalcoat.github.io/coca", "summary": "Representation intervention aims to locate and modify the representations\nthat encode the underlying concepts in Large Language Models (LLMs) to elicit\nthe aligned and expected behaviors. Despite the empirical success, it has never\nbeen examined whether one could locate the faithful concepts for intervention.\nIn this work, we explore the question in safety alignment. If the interventions\nare faithful, the intervened LLMs should erase the harmful concepts and be\nrobust to both in-distribution adversarial prompts and the out-of-distribution\n(OOD) jailbreaks. While it is feasible to erase harmful concepts without\ndegrading the benign functionalities of LLMs in linear settings, we show that\nit is infeasible in the general non-linear setting. To tackle the issue, we\npropose Concept Concentration (COCA). Instead of identifying the faithful\nlocations to intervene, COCA refractors the training data with an explicit\nreasoning process, which firstly identifies the potential unsafe concepts and\nthen decides the responses. Essentially, COCA simplifies the decision boundary\nbetween harmful and benign representations, enabling more effective linear\nerasure. Extensive experiments with multiple representation intervention\nmethods and model architectures demonstrate that COCA significantly reduces\nboth in-distribution and OOD jailbreak success rates, and meanwhile maintaining\nstrong performance on regular tasks such as math and code generation."}
{"id": "2505.18916", "pdf": "https://arxiv.org/pdf/2505.18916", "abs": "https://arxiv.org/abs/2505.18916", "authors": ["Yue Li", "Jake Vasilakes", "Zhixue Zhao", "Carolina Scarton"], "title": "SCRum-9: Multilingual Stance Classification over Rumours on Social Media", "categories": ["cs.CL"], "comment": null, "summary": "We introduce SCRum-9, a multilingual dataset for Rumour Stance\nClassification, containing 7,516 tweet-reply pairs from X. SCRum-9 goes beyond\nexisting stance classification datasets by covering more languages (9), linking\nexamples to more fact-checked claims (2.1k), and including complex annotations\nfrom multiple annotators to account for intra- and inter-annotator variability.\nAnnotations were made by at least three native speakers per language, totalling\naround 405 hours of annotation and 8,150 dollars in compensation. Experiments\non SCRum-9 show that it is a challenging benchmark for both state-of-the-art\nLLMs (e.g. Deepseek) as well as fine-tuned pre-trained models, motivating\nfuture work in this area."}
{"id": "2505.19477", "pdf": "https://arxiv.org/pdf/2505.19477", "abs": "https://arxiv.org/abs/2505.19477", "authors": ["Chiyu Ma", "Enpei Zhang", "Yilun Zhao", "Wenjun Liu", "Yaning Jia", "Peijun Qing", "Lin Shi", "Arman Cohan", "Yujun Yan", "Soroush Vosoughi"], "title": "Judging with Many Minds: Do More Perspectives Mean Less Prejudice?", "categories": ["cs.AI"], "comment": null, "summary": "LLM-as-Judge has emerged as a scalable alternative to human evaluation,\nenabling large language models (LLMs) to provide reward signals in trainings.\nWhile recent work has explored multi-agent extensions such as multi-agent\ndebate and meta-judging to enhance evaluation quality, the question of how\nintrinsic biases manifest in these settings remains underexplored. In this\nstudy, we conduct a systematic analysis of four diverse bias types: position\nbias, verbosity bias, chain-of-thought bias, and bandwagon bias. We evaluate\nthese biases across two widely adopted multi-agent LLM-as-Judge frameworks:\nMulti-Agent-Debate and LLM-as-Meta-Judge. Our results show that debate\nframework amplifies biases sharply after the initial debate, and this increased\nbias is sustained in subsequent rounds, while meta-judge approaches exhibit\ngreater resistance. We further investigate the incorporation of PINE, a leading\nsingle-agent debiasing method, as a bias-free agent within these systems. The\nresults reveal that this bias-free agent effectively reduces biases in debate\nsettings but provides less benefit in meta-judge scenarios. Our work provides a\ncomprehensive study of bias behavior in multi-agent LLM-as-Judge systems and\nhighlights the need for targeted bias mitigation strategies in collaborative\nevaluation settings."}
{"id": "2505.18693", "pdf": "https://arxiv.org/pdf/2505.18693", "abs": "https://arxiv.org/abs/2505.18693", "authors": ["Ihtesham Ibn Malek", "Hafiz Imtiaz", "Samia Subrina"], "title": "Simultaneous Optimization of Efficiency and Degradation in Tunable HTL-Free Perovskite Solar Cells with MWCNT-Integrated Back Contact Using a Machine Learning-Derived Polynomial Regressor", "categories": ["cs.LG", "eess.SP", "stat.ML"], "comment": "Under review in Elsevier Renewable Energy", "summary": "Perovskite solar cells (PSCs) without a hole transport layer (HTL) offer a\ncost-effective and stable alternative to conventional architectures, utilizing\nonly an absorber layer and an electron transport layer (ETL). This study\npresents a machine learning (ML)-driven framework to optimize the efficiency\nand stability of HTL-free PSCs by integrating experimental validation with\nnumerical simulations. Excellent agreement is achieved between a fabricated\ndevice and its simulated counterpart at a molar fraction \\( x = 68.7\\% \\) in\n\\(\\mathrm{MAPb}_{1-x}\\mathrm{Sb}_{2x/3}\\mathrm{I}_3\\), where MA is\nmethylammonium. A dataset of 1650 samples is generated by varying molar\nfraction, absorber defect density, thickness, and ETL doping, with\ncorresponding efficiency and 50-hour degradation as targets. A fourth-degree\npolynomial regressor (PR-4) shows the best performance, achieving RMSEs of\n0.0179 and 0.0117, and \\( R^2 \\) scores of 1 and 0.999 for efficiency and\ndegradation, respectively. The derived model generalizes beyond the training\nrange and is used in an L-BFGS-B optimization algorithm with a weighted\nobjective function to maximize efficiency and minimize degradation. This\nimproves device efficiency from 13.7\\% to 16.84\\% and reduces degradation from\n6.61\\% to 2.39\\% over 1000 hours. Finally, the dataset is labeled into superior\nand inferior classes, and a multilayer perceptron (MLP) classifier achieves\n100\\% accuracy, successfully identifying optimal configurations."}
{"id": "2505.18927", "pdf": "https://arxiv.org/pdf/2505.18927", "abs": "https://arxiv.org/abs/2505.18927", "authors": ["Amel Muminovic"], "title": "Benchmarking Large Language Models for Cyberbullying Detection in Real-World YouTube Comments", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint. 9 pages, 3 tables, 1 figure. Not yet submitted to a\n  journal. Feedback welcome", "summary": "As online platforms grow, comment sections increasingly host harassment that\nundermines user experience and well-being. This study benchmarks three leading\nlarge language models, OpenAI GPT-4.1, Google Gemini 1.5 Pro, and Anthropic\nClaude 3 Opus, on a corpus of 5,080 YouTube comments sampled from high-abuse\nthreads in gaming, lifestyle, food vlog, and music channels. The dataset\ncomprises 1,334 harmful and 3,746 non-harmful messages in English, Arabic, and\nIndonesian, annotated independently by two reviewers with substantial agreement\n(Cohen's kappa = 0.83). Using a unified prompt and deterministic settings,\nGPT-4.1 achieved the best overall balance with an F1 score of 0.863, precision\nof 0.887, and recall of 0.841. Gemini flagged the highest share of harmful\nposts (recall = 0.875) but its precision fell to 0.767 due to frequent false\npositives. Claude delivered the highest precision at 0.920 and the lowest\nfalse-positive rate of 0.022, yet its recall dropped to 0.720. Qualitative\nanalysis showed that all three models struggle with sarcasm, coded insults, and\nmixed-language slang. These results underscore the need for moderation\npipelines that combine complementary models, incorporate conversational\ncontext, and fine-tune for under-represented languages and implicit abuse. A\nde-identified version of the dataset and full prompts is publicly released to\npromote reproducibility and further progress in automated content moderation."}
{"id": "2505.19489", "pdf": "https://arxiv.org/pdf/2505.19489", "abs": "https://arxiv.org/abs/2505.19489", "authors": ["Zhenhao Zhou", "Zhuochen Huang", "Yike He", "Chong Wang", "Jiajun Wang", "Yijian Wu", "Xin Peng", "Yiling Lou"], "title": "Benchmarking and Enhancing LLM Agents in Localizing Linux Kernel Bugs", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "The Linux kernel is a critical system, serving as the foundation for numerous\nsystems. Bugs in the Linux kernel can cause serious consequences, affecting\nbillions of users. Fault localization (FL), which aims at identifying the buggy\ncode elements in software, plays an essential role in software quality\nassurance. While recent LLM agents have achieved promising accuracy in FL on\nrecent benchmarks like SWE-bench, it remains unclear how well these methods\nperform in the Linux kernel, where FL is much more challenging due to the\nlarge-scale code base, limited observability, and diverse impact factors. In\nthis paper, we introduce LinuxFLBench, a FL benchmark constructed from\nreal-world Linux kernel bugs. We conduct an empirical study to assess the\nperformance of state-of-the-art LLM agents on the Linux kernel. Our initial\nresults reveal that existing agents struggle with this task, achieving a best\ntop-1 accuracy of only 41.6% at file level. To address this challenge, we\npropose LinuxFL$^+$, an enhancement framework designed to improve FL\neffectiveness of LLM agents for the Linux kernel. LinuxFL$^+$ substantially\nimproves the FL accuracy of all studied agents (e.g., 7.2% - 11.2% accuracy\nincrease) with minimal costs. Data and code are available at\nhttps://github.com/FudanSELab/LinuxFLBench."}
{"id": "2505.18697", "pdf": "https://arxiv.org/pdf/2505.18697", "abs": "https://arxiv.org/abs/2505.18697", "authors": ["Ziyang Cheng", "Zhixun Li", "Yuhan Li", "Yixin Song", "Kangyi Zhao", "Dawei Cheng", "Jia Li", "Jeffrey Xu Yu"], "title": "Can LLMs Alleviate Catastrophic Forgetting in Graph Continual Learning? A Systematic Study", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Nowadays, real-world data, including graph-structure data, often arrives in a\nstreaming manner, which means that learning systems need to continuously\nacquire new knowledge without forgetting previously learned information.\nAlthough substantial existing works attempt to address catastrophic forgetting\nin graph machine learning, they are all based on training from scratch with\nstreaming data. With the rise of pretrained models, an increasing number of\nstudies have leveraged their strong generalization ability for continual\nlearning. Therefore, in this work, we attempt to answer whether large language\nmodels (LLMs) can mitigate catastrophic forgetting in Graph Continual Learning\n(GCL). We first point out that current experimental setups for GCL have\nsignificant flaws, as the evaluation stage may lead to task ID leakage. Then,\nwe evaluate the performance of LLMs in more realistic scenarios and find that\neven minor modifications can lead to outstanding results. Finally, based on\nextensive experiments, we propose a simple-yet-effective method, Simple Graph\nContinual Learning (SimGCL), that surpasses the previous state-of-the-art\nGNN-based baseline by around 20% under the rehearsal-free constraint. To\nfacilitate reproducibility, we have developed an easy-to-use benchmark LLM4GCL\nfor training and evaluating existing GCL methods. The code is available at:\nhttps://github.com/ZhixunLEE/LLM4GCL."}
{"id": "2505.18943", "pdf": "https://arxiv.org/pdf/2505.18943", "abs": "https://arxiv.org/abs/2505.18943", "authors": ["Xuanming Zhang", "Yuxuan Chen", "Min-Hsuan Yeh", "Yixuan Li"], "title": "MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent Systems", "categories": ["cs.CL"], "comment": null, "summary": "Human social interactions depend on the ability to infer others' unspoken\nintentions, emotions, and beliefs-a cognitive skill grounded in the\npsychological concept of Theory of Mind (ToM). While large language models\n(LLMs) excel in semantic understanding tasks, they struggle with the ambiguity\nand contextual nuance inherent in human communication. To bridge this gap, we\nintroduce MetaMind, a multi-agent framework inspired by psychological theories\nof metacognition, designed to emulate human-like social reasoning. MetaMind\ndecomposes social understanding into three collaborative stages: (1) a\nTheory-of-Mind Agent generates hypotheses user mental states (e.g., intent,\nemotion), (2) a Domain Agent refines these hypotheses using cultural norms and\nethical constraints, and (3) a Response Agent generates contextually\nappropriate responses while validating alignment with inferred intent. Our\nframework achieves state-of-the-art performance across three challenging\nbenchmarks, with 35.7% improvement in real-world social scenarios and 6.2% gain\nin ToM reasoning. Notably, it enables LLMs to match human-level performance on\nkey ToM tasks for the first time. Ablation studies confirm the necessity of all\ncomponents, which showcase the framework's ability to balance contextual\nplausibility, social appropriateness, and user adaptation. This work advances\nAI systems toward human-like social intelligence, with applications in\nempathetic dialogue and culturally sensitive interactions. Code is available at\nhttps://github.com/XMZhangAI/MetaMind."}
{"id": "2505.19490", "pdf": "https://arxiv.org/pdf/2505.19490", "abs": "https://arxiv.org/abs/2505.19490", "authors": ["Jianxing Liao", "Junyan Xu", "Yatao Sun", "Maowen Tang", "Sicheng He", "Jingxian Liao", "Shui Yu", "Yun Li", "Hongguan Xiao"], "title": "Automated CAD Modeling Sequence Generation from Text Descriptions via Transformer-Based Large Language Models", "categories": ["cs.AI", "I.2.7; I.2.6"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Designing complex computer-aided design (CAD) models is often time-consuming\ndue to challenges such as computational inefficiency and the difficulty of\ngenerating precise models. We propose a novel language-guided framework for\nindustrial design automation to address these issues, integrating large\nlanguage models (LLMs) with computer-automated design (CAutoD).Through this\nframework, CAD models are automatically generated from parameters and\nappearance descriptions, supporting the automation of design tasks during the\ndetailed CAD design phase. Our approach introduces three key innovations: (1) a\nsemi-automated data annotation pipeline that leverages LLMs and vision-language\nlarge models (VLLMs) to generate high-quality parameters and appearance\ndescriptions; (2) a Transformer-based CAD generator (TCADGen) that predicts\nmodeling sequences via dual-channel feature aggregation; (3) an enhanced CAD\nmodeling generation model, called CADLLM, that is designed to refine the\ngenerated sequences by incorporating the confidence scores from TCADGen.\nExperimental results demonstrate that the proposed approach outperforms\ntraditional methods in both accuracy and efficiency, providing a powerful tool\nfor automating industrial workflows and generating complex CAD models from\ntextual prompts. The code is available at\nhttps://jianxliao.github.io/cadllm-page/"}
{"id": "2505.18698", "pdf": "https://arxiv.org/pdf/2505.18698", "abs": "https://arxiv.org/abs/2505.18698", "authors": ["Can Yaras", "Alec S. Xu", "Pierre Abillama", "Changwoo Lee", "Laura Balzano"], "title": "MonarchAttention: Zero-Shot Conversion to Fast, Hardware-Aware Structured Attention", "categories": ["cs.LG"], "comment": null, "summary": "Transformers have achieved state-of-the-art performance across various tasks,\nbut suffer from a notable quadratic complexity in sequence length due to the\nattention mechanism. In this work, we propose MonarchAttention -- a novel\napproach to sub-quadratic attention approximation via Monarch matrices, an\nexpressive class of structured matrices. Based on the variational form of\nsoftmax, we describe an efficient optimization-based algorithm to compute an\napproximate projection of softmax attention onto the class of Monarch matrices\nwith $\\Theta(N\\sqrt{N} d)$ computational complexity and $\\Theta(Nd)$ memory/IO\ncomplexity. Unlike previous approaches, MonarchAttention is both (1)\ntransferable, yielding minimal performance loss with no additional training,\neven when replacing every attention layer of the transformer, and (2)\nhardware-efficient, utilizing the highest-throughput tensor core units on\nmodern GPUs. With optimized kernels, MonarchAttention achieves substantial\nspeed-ups in wall-time over FlashAttention-2: $1.4\\times$ for shorter sequences\n$(N=256)$, $4.5\\times$ for medium-length sequences $(N=4K)$, and $8.2\\times$\nfor longer sequences $(N=16K)$. We demonstrate the quality of MonarchAttention\non diverse tasks and architectures in vision and language problems, showing\nthat it flexibly and accurately approximates softmax attention in a variety of\ncontexts. Our code is available at\nhttps://github.com/cjyaras/monarch-attention."}
{"id": "2505.18949", "pdf": "https://arxiv.org/pdf/2505.18949", "abs": "https://arxiv.org/abs/2505.18949", "authors": ["Longfei Yun", "Chenyang An", "Zilong Wang", "Letian Peng", "Jingbo Shang"], "title": "The Price of Format: Diversity Collapse in LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "14 pages, 7 figures", "summary": "Instruction-tuned large language models (LLMs) employ structured templates,\nsuch as role markers and special tokens, to enforce format consistency during\ninference. However, we identify a critical limitation of such formatting: it\ninduces a phenomenon we term diversity collapse, where the model generates\nsemantically similar outputs for open-ended inputs, undermining creativity and\nvariability. We systematically evaluate this effect across tasks like story\ncompletion and free-form generation, finding that (1) diversity collapse\npersists even under high-temperature sampling, and (2) structural tokens in\ntemplates significantly constrain the model's output space. To contextualize\nthese findings, we fine-tune the same model using a range of structured prompts\nand then evaluate them across three axes: downstream task performance,\nalignment behavior, and output diversity. Our analysis shows that format\nconsistency between fine-tuning and inference is crucial for\nstructure-sensitive tasks (e.g., GSM8K, IFEval), but has marginal influence on\nknowledge-heavy tasks (e.g., MMLU, WebQuestions). In contrast, output diversity\nis primarily governed by the presence or absence of structural tokens, with\nminimal formatting yielding the most diverse outputs. These findings reveal\nthat current prompting conventions, while beneficial for alignment, may\ninadvertently suppress output diversity, underscoring the need for\ndiversity-aware prompt design and instruction tuning."}
{"id": "2505.19501", "pdf": "https://arxiv.org/pdf/2505.19501", "abs": "https://arxiv.org/abs/2505.19501", "authors": ["Ming Yin", "Yuanhao Qu", "Dyllan Liu", "Ling Yang", "Le Cong", "Mengdi Wang"], "title": "Genome-Bench: A Scientific Reasoning Benchmark from Real-World Expert Discussions", "categories": ["cs.AI"], "comment": null, "summary": "In this short report, we present an automated pipeline tailored for the\ngenomics domain and introduce \\textit{Genome-Bench}, a new benchmark\nconstructed from over a decade of scientific forum discussions on genome\nengineering. Our pipeline transforms raw interactions into a reinforcement\nlearning friendly multiple-choice questions format, supported by 3000+ high\nquality question answer pairs spanning foundational biology, experimental\ntroubleshooting, tool usage, and beyond. To our knowledge, this is the first\nend-to-end pipeline for teaching LLMs to reason from scientific discussions,\nwith promising potential for generalization across scientific domains beyond\nbiology."}
{"id": "2505.18706", "pdf": "https://arxiv.org/pdf/2505.18706", "abs": "https://arxiv.org/abs/2505.18706", "authors": ["Viacheslav Sinii", "Alexey Gorbatovski", "Artem Cherepanov", "Boris Shaposhnikov", "Nikita Balagansky", "Daniil Gavrilov"], "title": "Steering LLM Reasoning Through Bias-Only Adaptation", "categories": ["cs.LG", "cs.AI"], "comment": "Preprint", "summary": "Recent work on reasoning-oriented language models, exemplified by o1-like\nsystems, suggests that reinforcement-learning (RL) finetuning does not create\nnew capabilities but instead strengthens reasoning patterns already latent in\nthe pretrained network. We test this claim by training steering vectors:\nlayer-wise biases that additively amplify selected hidden features while\nleaving all original weights unchanged. Experiments on four base models across\nthe GSM8K and MATH benchmarks show that steering vectors recover, and in\nseveral cases exceed, the accuracy of fully-tuned counterparts. This result\nsupports the view that the required reasoning skills pre-exist in the base\nmodel. Further, logit-lens analysis reveals that the trained vectors\nconsistently boost token groups linked to structured languages and logical\nconnectors, providing an interpretable account that aligns with the demands of\nquantitative reasoning tasks."}
{"id": "2505.18951", "pdf": "https://arxiv.org/pdf/2505.18951", "abs": "https://arxiv.org/abs/2505.18951", "authors": ["Saman Sarker Joy"], "title": "BnMMLU: Measuring Massive Multitask Language Understanding in Bengali", "categories": ["cs.CL"], "comment": "18 pages, 9 figures, 5 tables; Code & dataset available at\n  https://github.com/samanjoy2/bnmmlu", "summary": "The Massive Multitask Language Understanding (MMLU) benchmark has been widely\nused to evaluate language models across various domains. However, existing MMLU\ndatasets primarily focus on high-resource languages such as English, which\nleaves low-resource languages like Bengali underrepresented. In this paper, we\nintroduce BnMMLU, a benchmark to evaluate the multitask language understanding\ncapabilities of Bengali in language models. The dataset spans 23 domains,\nincluding science, humanities, mathematics and general knowledge and is\nstructured in a multiple-choice format to assess factual knowledge,\napplication-based problem-solving and reasoning abilities of language models.\nIt consists of 138,949 question-option pairs. We benchmark several proprietary\nand open-source large language models (LLMs) on the BnMMLU test set.\nAdditionally, we annotate the test set with three cognitive categories-factual\nknowledge, procedural application and reasoning-to gain deeper insights into\nmodel strengths and weaknesses across various cognitive tasks. The results\nreveal significant performance gaps, highlighting the need for improved\npre-training and fine-tuning strategies tailored to Bengali data. We release\nthe dataset and benchmark results to facilitate further research in this area."}
{"id": "2505.19550", "pdf": "https://arxiv.org/pdf/2505.19550", "abs": "https://arxiv.org/abs/2505.19550", "authors": ["Georgios Mappouras"], "title": "Turing Test 2.0: The General Intelligence Threshold", "categories": ["cs.AI"], "comment": null, "summary": "With the rise of artificial intelligence (A.I.) and large language models\nlike Chat-GPT, a new race for achieving artificial general intelligence (A.G.I)\nhas started. While many speculate how and when A.I. will achieve A.G.I., there\nis no clear agreement on how A.G.I. can be detected in A.I. models, even when\npopular tools like the Turing test (and its modern variations) are used to\nmeasure their intelligence. In this work, we discuss why traditional methods\nlike the Turing test do not suffice for measuring or detecting A.G.I. and\nprovide a new, practical method that can be used to decide if a (computer or\nany other) system has reached or surpassed A.G.I. To achieve this, we make two\nnew contributions. First, we present a clear definition for general\nintelligence (G.I.) and set a G.I. threshold (G.I.T.) that can be used to\ndistinguish between systems that achieve A.G.I. and systems that do not.\nSecond, we present a new framework on how to construct tests that can detect if\na system has achieved G.I. in a simple, comprehensive, and clear-cut fail/pass\nway. We call this novel framework the Turing Tests 2.0. We then demonstrate\nreal-life examples of applying tests that follow our Turing Tests 2.0 framework\non modern A.I. models."}
{"id": "2505.18713", "pdf": "https://arxiv.org/pdf/2505.18713", "abs": "https://arxiv.org/abs/2505.18713", "authors": ["Guodong Du", "Zitao Fang", "Jing Li", "Junlin Li", "Runhua Jiang", "Shuyang Yu", "Yifei Guo", "Yangneng Chen", "Sim Kuan Goh", "Ho-Kin Tang", "Daojing He", "Honghai Liu", "Min Zhang"], "title": "Neural Parameter Search for Slimmer Fine-Tuned Models and Better Transfer", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by ACL2025 Main", "summary": "Foundation models and their checkpoints have significantly advanced deep\nlearning, boosting performance across various applications. However, fine-tuned\nmodels often struggle outside their specific domains and exhibit considerable\nredundancy. Recent studies suggest that combining a pruned fine-tuned model\nwith the original pre-trained model can mitigate forgetting, reduce\ninterference when merging model parameters across tasks, and improve\ncompression efficiency. In this context, developing an effective pruning\nstrategy for fine-tuned models is crucial. Leveraging the advantages of the\ntask vector mechanism, we preprocess fine-tuned models by calculating the\ndifferences between them and the original model. Recognizing that different\ntask vector subspaces contribute variably to model performance, we introduce a\nnovel method called Neural Parameter Search (NPS-Pruning) for slimming down\nfine-tuned models. This method enhances pruning efficiency by searching through\nneural parameters of task vectors within low-rank subspaces. Our method has\nthree key applications: enhancing knowledge transfer through pairwise model\ninterpolation, facilitating effective knowledge fusion via model merging, and\nenabling the deployment of compressed models that retain near-original\nperformance while significantly reducing storage costs. Extensive experiments\nacross vision, NLP, and multi-modal benchmarks demonstrate the effectiveness\nand robustness of our approach, resulting in substantial performance gains. The\ncode is publicly available at: https://github.com/duguodong7/NPS-Pruning."}
{"id": "2505.18953", "pdf": "https://arxiv.org/pdf/2505.18953", "abs": "https://arxiv.org/abs/2505.18953", "authors": ["Divij Chawla", "Ashita Bhutada", "Do Duc Anh", "Abhinav Raghunathan", "Vinod SP", "Cathy Guo", "Dar Win Liew", "Prannaya Gupta", "Rishabh Bhardwaj", "Rajat Bhardwaj", "Soujanya Poria"], "title": "Evaluating AI for Finance: Is AI Credible at Assessing Investment Risk?", "categories": ["cs.CL"], "comment": null, "summary": "We evaluate the credibility of leading AI models in assessing investment risk\nappetite. Our analysis spans proprietary (GPT-4, Claude 3.7, Gemini 1.5) and\nopen-weight models (LLaMA 3.1/3.3, DeepSeek-V3, Mistral-small), using 1,720\nuser profiles constructed with 16 risk-relevant features across 10 countries\nand both genders. We observe significant variance across models in score\ndistributions and demographic sensitivity. For example, GPT-4o assigns higher\nrisk scores to Nigerian and Indonesian profiles, while LLaMA and DeepSeek show\nopposite gender tendencies in risk classification. While some models (e.g.,\nGPT-4o, LLaMA 3.1) align closely with expected scores in low- and mid-risk\nranges, none maintain consistent performance across regions and demographics.\nOur findings highlight the need for rigorous, standardized evaluations of AI\nsystems in regulated financial contexts to prevent bias, opacity, and\ninconsistency in real-world deployment."}
{"id": "2505.19562", "pdf": "https://arxiv.org/pdf/2505.19562", "abs": "https://arxiv.org/abs/2505.19562", "authors": ["Ying Xiao", "Jie Huang", "Ruijuan He", "Jing Xiao", "Mohammad Reza Mousavi", "Yepang Liu", "Kezhi Li", "Zhenpeng Chen", "Jie M. Zhang"], "title": "AMQA: An Adversarial Dataset for Benchmarking Bias of LLMs in Medicine and Healthcare", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) are reaching expert-level accuracy on medical\ndiagnosis questions, yet their mistakes and the biases behind them pose\nlife-critical risks. Bias linked to race, sex, and socioeconomic status is\nalready well known, but a consistent and automatic testbed for measuring it is\nmissing. To fill this gap, this paper presents AMQA -- an Adversarial Medical\nQuestion-Answering dataset -- built for automated, large-scale bias evaluation\nof LLMs in medical QA. AMQA includes 4,806 medical QA pairs sourced from the\nUnited States Medical Licensing Examination (USMLE) dataset, generated using a\nmulti-agent framework to create diverse adversarial descriptions and question\npairs. Using AMQA, we benchmark five representative LLMs and find surprisingly\nsubstantial disparities: even GPT-4.1, the least biased model tested, answers\nprivileged-group questions over 10 percentage points more accurately than\nunprivileged ones. Compared with the existing benchmark CPV, AMQA reveals 15%\nlarger accuracy gaps on average between privileged and unprivileged groups. Our\ndataset and code are publicly available at https://github.com/XY-Showing/AMQA\nto support reproducible research and advance trustworthy, bias-aware medical\nAI."}
{"id": "2505.18724", "pdf": "https://arxiv.org/pdf/2505.18724", "abs": "https://arxiv.org/abs/2505.18724", "authors": ["Junyu Chen", "Junzhuo Li", "Zhen Peng", "Wenjie Wang", "Yuxiang Ren", "Long Shi", "Xuming Hu"], "title": "LoTA-QAF: Lossless Ternary Adaptation for Quantization-Aware Fine-Tuning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Quantization and fine-tuning are crucial for deploying large language models\n(LLMs) on resource-constrained edge devices. However, fine-tuning quantized\nmodels presents significant challenges, primarily stemming from: First, the\nmismatch in data types between the low-precision quantized weights (e.g.,\n4-bit) and the high-precision adaptation weights (e.g., 16-bit). This mismatch\nlimits the computational efficiency advantage offered by quantized weights\nduring inference. Second, potential accuracy degradation when merging these\nhigh-precision adaptation weights into the low-precision quantized weights, as\nthe adaptation weights often necessitate approximation or truncation. Third, as\nfar as we know, no existing methods support the lossless merging of adaptation\nwhile adjusting all quantized weights. To address these challenges, we\nintroduce lossless ternary adaptation for quantization-aware fine-tuning\n(LoTA-QAF). This is a novel fine-tuning method specifically designed for\nquantized LLMs, enabling the lossless merging of ternary adaptation weights\ninto quantized weights and the adjustment of all quantized weights. LoTA-QAF\noperates through a combination of: i) A custom-designed ternary adaptation (TA)\nthat aligns ternary weights with the quantization grid and uses these ternary\nweights to adjust quantized weights. ii) A TA-based mechanism that enables the\nlossless merging of adaptation weights. iii) Ternary signed gradient descent\n(t-SignSGD) for updating the TA weights. We apply LoTA-QAF to Llama-3.1/3.3 and\nQwen-2.5 model families and validate its effectiveness on several downstream\ntasks. On the MMLU benchmark, our method effectively recovers performance for\nquantized models, surpassing 16-bit LoRA by up to 5.14\\%. For task-specific\nfine-tuning, 16-bit LoRA achieves superior results, but LoTA-QAF still\noutperforms other methods."}
{"id": "2505.18962", "pdf": "https://arxiv.org/pdf/2505.18962", "abs": "https://arxiv.org/abs/2505.18962", "authors": ["Xiaoqiang Wang", "Suyuchen Wang", "Yun Zhu", "Bang Liu"], "title": "System-1.5 Reasoning: Traversal in Language and Latent Spaces with Dynamic Shortcuts", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Chain-of-thought (CoT) reasoning enables large language models (LLMs) to move\nbeyond fast System-1 responses and engage in deliberative System-2 reasoning.\nHowever, this comes at the cost of significant inefficiency due to verbose\nintermediate output. Recent latent-space reasoning methods improve efficiency\nby operating on hidden states without decoding into language, yet they treat\nall steps uniformly, failing to distinguish critical deductions from auxiliary\nsteps and resulting in suboptimal use of computational resources. In this\npaper, we propose System-1.5 Reasoning, an adaptive reasoning framework that\ndynamically allocates computation across reasoning steps through shortcut paths\nin latent space.Specifically, System-1.5 Reasoning introduces two types of\ndynamic shortcuts. The model depth shortcut (DS) adaptively reasons along the\nvertical depth by early exiting non-critical tokens through lightweight adapter\nbranches, while allowing critical tokens to continue through deeper Transformer\nlayers. The step shortcut (SS) reuses hidden states across the decoding steps\nto skip trivial steps and reason horizontally in latent space. Training\nSystem-1.5 Reasoning involves a two-stage self-distillation process: first\ndistilling natural language CoT into latent-space continuous thought, and then\ndistilling full-path System-2 latent reasoning into adaptive shortcut paths\n(System-1.5 Reasoning).Experiments on reasoning tasks demonstrate the superior\nperformance of our method. For example, on GSM8K, System-1.5 Reasoning achieves\nreasoning performance comparable to traditional CoT fine-tuning methods while\naccelerating inference by over 20x and reducing token generation by 92.31% on\naverage."}
{"id": "2505.19563", "pdf": "https://arxiv.org/pdf/2505.19563", "abs": "https://arxiv.org/abs/2505.19563", "authors": ["Shi-Yu Tian", "Zhi Zhou", "Wei Dong", "Ming Yang", "Kun-Yang Yu", "Zi-Jian Cheng", "Lan-Zhe Guo", "Yu-Feng Li"], "title": "Automated Text-to-Table for Reasoning-Intensive Table QA: Pipeline Design and Benchmarking Insights", "categories": ["cs.AI", "cs.CL"], "comment": "Paper under review, code and dataset are all available", "summary": "Reasoning with tabular data holds increasing importance in modern\napplications, yet comprehensive evaluation methodologies for\nreasoning-intensive Table Question Answering (QA) tasks remain nascent.\nExisting research is constrained by two primary bottlenecks: 1) Reliance on\ncostly manually annotated real-world data, which is difficult to cover complex\nreasoning scenarios; 2) The heterogeneity of table structures hinders\nsystematic analysis of the intrinsic mechanisms behind the underperformance of\nLLMs, especially in reasoning-intensive tasks. To address these issues, we\npropose an automated generation pipeline AutoT2T that transforms mathematical\nword problems into table-based reasoning tasks, eliminating the need for manual\nannotation. The pipeline can generate multiple variants of a table for the same\nreasoning problem, including noisy versions to support robustness evaluation.\nBased on this, we construct a new benchmark TabularGSM, which systematically\nspans a range of table complexities and trap problems. Experimental analyses\nthrough AutoT2T and TabularGSM reveal that the tight coupling between reasoning\nand retrieval or identification processes is a key factor underlying the\nfailure of LLMs in complex Table QA tasks. This highlights the necessity for\nmodels to develop synergistic reasoning capabilities in order to perform\neffectively in complex Table QA tasks."}
{"id": "2505.18728", "pdf": "https://arxiv.org/pdf/2505.18728", "abs": "https://arxiv.org/abs/2505.18728", "authors": ["Andrea Ceni", "Alessio Gravina", "Claudio Gallicchio", "Davide Bacciu", "Carola-Bibiane Schonlieb", "Moshe Eliasof"], "title": "Message-Passing State-Space Models: Improving Graph Learning with Modern Sequence Modeling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The recent success of State-Space Models (SSMs) in sequence modeling has\nmotivated their adaptation to graph learning, giving rise to Graph State-Space\nModels (GSSMs). However, existing GSSMs operate by applying SSM modules to\nsequences extracted from graphs, often compromising core properties such as\npermutation equivariance, message-passing compatibility, and computational\nefficiency. In this paper, we introduce a new perspective by embedding the key\nprinciples of modern SSM computation directly into the Message-Passing Neural\nNetwork framework, resulting in a unified methodology for both static and\ntemporal graphs. Our approach, MP-SSM, enables efficient,\npermutation-equivariant, and long-range information propagation while\npreserving the architectural simplicity of message passing. Crucially, MP-SSM\nenables an exact sensitivity analysis, which we use to theoretically\ncharacterize information flow and evaluate issues like vanishing gradients and\nover-squashing in the deep regime. Furthermore, our design choices allow for a\nhighly optimized parallel implementation akin to modern SSMs. We validate\nMP-SSM across a wide range of tasks, including node classification, graph\nproperty prediction, long-range benchmarks, and spatiotemporal forecasting,\ndemonstrating both its versatility and strong empirical performance."}
{"id": "2505.18970", "pdf": "https://arxiv.org/pdf/2505.18970", "abs": "https://arxiv.org/abs/2505.18970", "authors": ["Bowen Wei", "Ziwei Zhu"], "title": "Learning to Explain: Prototype-Based Surrogate Models for LLM Classification", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance on\nnatural language tasks, but their decision-making processes remain largely\nopaque. Existing explanation methods either suffer from limited faithfulness to\nthe model's reasoning or produce explanations that humans find difficult to\nunderstand. To address these challenges, we propose \\textbf{ProtoSurE}, a novel\nprototype-based surrogate framework that provides faithful and\nhuman-understandable explanations for LLMs. ProtoSurE trains an\ninterpretable-by-design surrogate model that aligns with the target LLM while\nutilizing sentence-level prototypes as human-understandable concepts. Extensive\nexperiments show that ProtoSurE consistently outperforms SOTA explanation\nmethods across diverse LLMs and datasets. Importantly, ProtoSurE demonstrates\nstrong data efficiency, requiring relatively few training examples to achieve\ngood performance, making it practical for real-world applications."}
{"id": "2505.19567", "pdf": "https://arxiv.org/pdf/2505.19567", "abs": "https://arxiv.org/abs/2505.19567", "authors": ["Rasoul Zahedifar", "Sayyed Ali Mirghasemi", "Mahdieh Soleymani Baghshah", "Alireza Taheri"], "title": "LLM-Agent-Controller: A Universal Multi-Agent Large Language Model System as a Control Engineer", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "This study presents the LLM-Agent-Controller, a multi-agent large language\nmodel (LLM) system developed to address a wide range of problems in control\nengineering (Control Theory). The system integrates a central controller agent\nwith multiple specialized auxiliary agents, responsible for tasks such as\ncontroller design, model representation, control analysis, time-domain\nresponse, and simulation. A supervisor oversees high-level decision-making and\nworkflow coordination, enhancing the system's reliability and efficiency. The\nLLM-Agent-Controller incorporates advanced capabilities, including\nRetrieval-Augmented Generation (RAG), Chain-of-Thought reasoning,\nself-criticism and correction, efficient memory handling, and user-friendly\nnatural language communication. It is designed to function without requiring\nusers to have prior knowledge of Control Theory, enabling them to input\nproblems in plain language and receive complete, real-time solutions. To\nevaluate the system, we propose new performance metrics assessing both\nindividual agents and the system as a whole. We test five categories of Control\nTheory problems and benchmark performance across three advanced LLMs.\nAdditionally, we conduct a comprehensive qualitative conversational analysis\ncovering all key services. Results show that the LLM-Agent-Controller\nsuccessfully solved 83% of general tasks, with individual agents achieving an\naverage success rate of 87%. Performance improved with more advanced LLMs. This\nresearch demonstrates the potential of multi-agent LLM architectures to solve\ncomplex, domain-specific problems. By integrating specialized agents,\nsupervisory control, and advanced reasoning, the LLM-Agent-Controller offers a\nscalable, robust, and accessible solution framework that can be extended to\nvarious technical domains."}
{"id": "2505.18731", "pdf": "https://arxiv.org/pdf/2505.18731", "abs": "https://arxiv.org/abs/2505.18731", "authors": ["Wei Shen", "Xiaonan He", "Chuheng Zhang", "Xuyun Zhang", "Xiaolong Xu", "Wanchun Dou"], "title": "Reward-Driven Interaction: Enhancing Proactive Dialogue Agents through User Satisfaction Prediction", "categories": ["cs.LG"], "comment": null, "summary": "Reward-driven proactive dialogue agents require precise estimation of user\nsatisfaction as an intrinsic reward signal to determine optimal interaction\nstrategies. Specifically, this framework triggers clarification questions when\ndetecting potential user dissatisfaction during interactions in the industrial\ndialogue system. Traditional works typically rely on training a neural network\nmodel based on weak labels which are generated by a simple model trained on\nuser actions after current turn. However, existing methods suffer from two\ncritical limitations in real-world scenarios: (1) Noisy Reward Supervision,\ndependence on weak labels derived from post-hoc user actions introduces bias,\nparticularly failing to capture satisfaction signals in ASR-error-induced\nutterances; (2) Long-Tail Feedback Sparsity, the power-law distribution of user\nqueries causes reward prediction accuracy to drop in low-frequency domains. The\nnoise in the weak labels and a power-law distribution of user utterances\nresults in that the model is hard to learn good representation of user\nutterances and sessions. To address these limitations, we propose two auxiliary\ntasks to improve the representation learning of user utterances and sessions\nthat enhance user satisfaction prediction. The first one is a contrastive\nself-supervised learning task, which helps the model learn the representation\nof rare user utterances and identify ASR errors. The second one is a\ndomain-intent classification task, which aids the model in learning the\nrepresentation of user sessions from long-tailed domains and improving the\nmodel's performance on such domains. The proposed method is evaluated on\nDuerOS, demonstrating significant improvements in the accuracy of error\nrecognition on rare user utterances and long-tailed domains."}
{"id": "2505.18971", "pdf": "https://arxiv.org/pdf/2505.18971", "abs": "https://arxiv.org/abs/2505.18971", "authors": ["Abhijit Chakraborty", "Chahana Dahal", "Ashutosh Balasubramaniam", "Tejas Anvekar", "Vivek Gupta"], "title": "Is Architectural Complexity Overrated? Competitive and Interpretable Knowledge Graph Completion with RelatE", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "We revisit the efficacy of simple, real-valued embedding models for knowledge\ngraph completion and introduce RelatE, an interpretable and modular method that\nefficiently integrates dual representations for entities and relations. RelatE\nemploys a real-valued phase-modulus decomposition, leveraging sinusoidal phase\nalignments to encode relational patterns such as symmetry, inversion, and\ncomposition. In contrast to recent approaches based on complex-valued\nembeddings or deep neural architectures, RelatE preserves architectural\nsimplicity while achieving competitive or superior performance on standard\nbenchmarks. Empirically, RelatE outperforms prior methods across several\ndatasets: on YAGO3-10, it achieves an MRR of 0.521 and Hit@10 of 0.680,\nsurpassing all baselines. Additionally, RelatE offers significant efficiency\ngains, reducing training time by 24%, inference latency by 31%, and peak GPU\nmemory usage by 22% compared to RotatE. Perturbation studies demonstrate\nimproved robustness, with MRR degradation reduced by up to 61% relative to\nTransE and by up to 19% compared to RotatE under structural edits such as edge\nremovals and relation swaps. Formal analysis further establishes the model's\nfull expressiveness and its capacity to represent essential first-order logical\ninference patterns. These results position RelatE as a scalable and\ninterpretable alternative to more complex architectures for knowledge graph\ncompletion."}
{"id": "2505.19568", "pdf": "https://arxiv.org/pdf/2505.19568", "abs": "https://arxiv.org/abs/2505.19568", "authors": ["Jiongchao Jin", "Xiuju Fu", "Xiaowei Gao", "Tao Cheng", "Ran Yan"], "title": "MSD-LLM: Predicting Ship Detention in Port State Control Inspections with Large Language Model", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Maritime transportation is the backbone of global trade, making ship\ninspection essential for ensuring maritime safety and environmental protection.\nPort State Control (PSC), conducted by national ports, enforces compliance with\nsafety regulations, with ship detention being the most severe consequence,\nimpacting both ship schedules and company reputations. Traditional machine\nlearning methods for ship detention prediction are limited by the capacity of\nrepresentation learning and thus suffer from low accuracy. Meanwhile,\nautoencoder-based deep learning approaches face challenges due to the severe\ndata imbalance in learning historical PSC detention records. To address these\nlimitations, we propose Maritime Ship Detention with Large Language Models\n(MSD-LLM), integrating a dual robust subspace recovery (DSR) layer-based\nautoencoder with a progressive learning pipeline to handle imbalanced data and\nextract meaningful PSC representations. Then, a large language model groups and\nranks features to identify likely detention cases, enabling dynamic\nthresholding for flexible detention predictions. Extensive evaluations on\n31,707 PSC inspection records from the Asia-Pacific region show that MSD-LLM\noutperforms state-of-the-art methods more than 12\\% on Area Under the Curve\n(AUC) for Singapore ports. Additionally, it demonstrates robustness to\nreal-world challenges, making it adaptable to diverse maritime risk assessment\nscenarios."}
{"id": "2505.18738", "pdf": "https://arxiv.org/pdf/2505.18738", "abs": "https://arxiv.org/abs/2505.18738", "authors": ["Haonan Dong", "Wenhao Zhu", "Guojie Song", "Liang Wang"], "title": "AuroRA: Breaking Low-Rank Bottleneck of LoRA with Nonlinear Mapping", "categories": ["cs.LG"], "comment": null, "summary": "Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient\nfine-tuning (PEFT) method validated across NLP and CV domains. However, LoRA\nfaces an inherent low-rank bottleneck: narrowing its performance gap with full\nfinetuning requires increasing the rank of its parameter matrix, resulting in\nsignificant parameter overhead. Recent linear LoRA variants have attempted to\nenhance expressiveness by introducing additional linear mappings; however,\ntheir composition remains inherently linear and fails to fundamentally improve\nLoRA's representational capacity. To address this limitation, we propose\nAuroRA, which incorporates an Adaptive Nonlinear Layer (ANL) between two linear\nprojectors to capture fixed and learnable nonlinearities. This combination\nforms an MLP-like structure with a compressed rank, enabling flexible and\nprecise approximation of diverse target functions while theoretically\nguaranteeing lower approximation errors and bounded gradients. Extensive\nexperiments on 22 datasets and 6 pretrained models demonstrate that AuroRA: (I)\nnot only matches or surpasses full fine-tuning performance with only 6.18% ~\n25% of LoRA's parameters but also (II) outperforms state-of-the-art PEFT\nmethods by up to 10.88% in both NLP and CV tasks, and (III) exhibits robust\nperformance across various rank configurations."}
{"id": "2505.18973", "pdf": "https://arxiv.org/pdf/2505.18973", "abs": "https://arxiv.org/abs/2505.18973", "authors": ["Sarang Patil", "Ashish Parmanand Pandey", "Ioannis Koutis", "Mengjia Xu"], "title": "Hierarchical Mamba Meets Hyperbolic Geometry: A New Paradigm for Structured Language Embeddings", "categories": ["cs.CL", "cs.LG"], "comment": "10 pages, 3 figures", "summary": "Selective state-space models have achieved great success in long-sequence\nmodeling. However, their capacity for language representation, especially in\ncomplex hierarchical reasoning tasks, remains underexplored. Most large\nlanguage models rely on flat Euclidean embeddings, limiting their ability to\ncapture latent hierarchies. To address this limitation, we propose Hierarchical\nMamba (HiM), integrating efficient Mamba2 with exponential growth and curved\nnature of hyperbolic geometry to learn hierarchy-aware language embeddings for\ndeeper linguistic understanding. Mamba2-processed sequences are projected to\nthe Poincare ball (via tangent-based mapping) or Lorentzian manifold (via\ncosine and sine-based mapping) with \"learnable\" curvature, optimized with a\ncombined hyperbolic loss. Our HiM model facilitates the capture of relational\ndistances across varying hierarchical levels, enabling effective long-range\nreasoning. This makes it well-suited for tasks like mixed-hop prediction and\nmulti-hop inference in hierarchical classification. We evaluated our HiM with\nfour linguistic and medical datasets for mixed-hop prediction and multi-hop\ninference tasks. Experimental results demonstrated that: 1) Both HiM models\neffectively capture hierarchical relationships for four ontological datasets,\nsurpassing Euclidean baselines. 2) HiM-Poincare captures fine-grained semantic\ndistinctions with higher h-norms, while HiM-Lorentz provides more stable,\ncompact, and hierarchy-preserving embeddings favoring robustness over detail."}
{"id": "2505.19621", "pdf": "https://arxiv.org/pdf/2505.19621", "abs": "https://arxiv.org/abs/2505.19621", "authors": ["George Kour", "Itay Nakash", "Ateret Anaby-Tavor", "Michal Shmueli-Scheuer"], "title": "Think Again! The Effect of Test-Time Compute on Preferences, Opinions, and Beliefs of Large Language Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) become deeply integrated into human life and\nincreasingly influence decision-making, it's crucial to evaluate whether and to\nwhat extent they exhibit subjective preferences, opinions, and beliefs. These\ntendencies may stem from biases within the models, which may shape their\nbehavior, influence the advice and recommendations they offer to users, and\npotentially reinforce certain viewpoints. This paper presents the Preference,\nOpinion, and Belief survey (POBs), a benchmark developed to assess LLMs'\nsubjective inclinations across societal, cultural, ethical, and personal\ndomains. We applied our benchmark to evaluate leading open- and closed-source\nLLMs, measuring desired properties such as reliability, neutrality, and\nconsistency. In addition, we investigated the effect of increasing the\ntest-time compute, through reasoning and self-reflection mechanisms, on those\nmetrics. While effective in other tasks, our results show that these mechanisms\noffer only limited gains in our domain. Furthermore, we reveal that newer model\nversions are becoming less consistent and more biased toward specific\nviewpoints, highlighting a blind spot and a concerning trend. POBS:\nhttps://ibm.github.io/POBS"}
{"id": "2505.18755", "pdf": "https://arxiv.org/pdf/2505.18755", "abs": "https://arxiv.org/abs/2505.18755", "authors": ["Xiaolu Chen", "Chenghao Huang", "Yanru Zhang", "Hao Wang"], "title": "Smart Energy Guardian: A Hybrid Deep Learning Model for Detecting Fraudulent PV Generation", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": "2024 IEEE International Smart Cities Conference (ISC2)", "summary": "With the proliferation of smart grids, smart cities face growing challenges\ndue to cyber-attacks and sophisticated electricity theft behaviors,\nparticularly in residential photovoltaic (PV) generation systems. Traditional\nElectricity Theft Detection (ETD) methods often struggle to capture complex\ntemporal dependencies and integrating multi-source data, limiting their\neffectiveness. In this work, we propose an efficient ETD method that accurately\nidentifies fraudulent behaviors in residential PV generation, thus ensuring the\nsupply-demand balance in smart cities. Our hybrid deep learning model,\ncombining multi-scale Convolutional Neural Network (CNN), Long Short-Term\nMemory (LSTM), and Transformer, excels in capturing both short-term and\nlong-term temporal dependencies. Additionally, we introduce a data embedding\ntechnique that seamlessly integrates time-series data with discrete temperature\nvariables, enhancing detection robustness. Extensive simulation experiments\nusing real-world data validate the effectiveness of our approach, demonstrating\nsignificant improvements in the accuracy of detecting sophisticated energy\ntheft activities, thereby contributing to the stability and fairness of energy\nsystems in smart cities."}
{"id": "2505.18978", "pdf": "https://arxiv.org/pdf/2505.18978", "abs": "https://arxiv.org/abs/2505.18978", "authors": ["Miguel Angel Pealoza Perez", "Bruno Lopez Orozco", "Jesus Tadeo Cruz Soto", "Michelle Bruno Hernandez", "Miguel Angel Alvarado Gonzalez", "Sandra Malagon"], "title": "AI4Math: A Native Spanish Benchmark for University-Level Mathematical Reasoning in Large Language Models", "categories": ["cs.CL", "68", "I.2"], "comment": "36 pages, 5 figures", "summary": "Existing mathematical reasoning benchmarks are predominantly English only or\ntranslation-based, which can introduce semantic drift and mask languagespecific\nreasoning errors. To address this, we present AI4Math, a benchmark of 105\noriginal university level math problems natively authored in Spanish. The\ndataset spans seven advanced domains (Algebra, Calculus, Geometry, Probability,\nNumber Theory, Combinatorics, and Logic), and each problem is accompanied by a\nstep by step human solution. We evaluate six large language models GPT 4o, GPT\n4o mini, o3 mini, LLaMA 3.3 70B, DeepSeek R1 685B, and DeepSeek V3 685B under\nfour configurations: zero shot and chain of thought, each in Spanish and\nEnglish. The top models (o3 mini, DeepSeek R1 685B, DeepSeek V3 685B) achieve\nover 70% accuracy, whereas LLaMA 3.3 70B and GPT-4o mini remain below 40%. Most\nmodels show no significant performance drop between languages, with GPT 4o even\nperforming better on Spanish problems in the zero shot setting. Geometry,\nCombinatorics, and Probability questions remain persistently challenging for\nall models. These results highlight the need for native-language benchmarks and\ndomain-specific evaluations to reveal reasoning failures not captured by\nstandard metrics."}
{"id": "2505.19641", "pdf": "https://arxiv.org/pdf/2505.19641", "abs": "https://arxiv.org/abs/2505.19641", "authors": ["Junteng Liu", "Yuanxiang Fan", "Zhuo Jiang", "Han Ding", "Yongyi Hu", "Chi Zhang", "Yiqi Shi", "Shitong Weng", "Aili Chen", "Shiqi Chen", "Yunan Huang", "Mozhi Zhang", "Pengyu Zhao", "Junjie Yan", "Junxian He"], "title": "SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the\npotential of Reinforcement Learning (RL) to enhance reasoning abilities in\nLarge Language Models (LLMs). While open-source replication efforts have\nprimarily focused on mathematical and coding domains, methods and resources for\ndeveloping general reasoning capabilities remain underexplored. This gap is\npartly due to the challenge of collecting diverse and verifiable reasoning data\nsuitable for RL. We hypothesize that logical reasoning is critical for\ndeveloping general reasoning capabilities, as logic forms a fundamental\nbuilding block of reasoning. In this work, we present SynLogic, a data\nsynthesis framework and dataset that generates diverse logical reasoning data\nat scale, encompassing 35 diverse logical reasoning tasks. The SynLogic\napproach enables controlled synthesis of data with adjustable difficulty and\nquantity. Importantly, all examples can be verified by simple rules, making\nthem ideally suited for RL with verifiable rewards. In our experiments, we\nvalidate the effectiveness of RL training on the SynLogic dataset based on 7B\nand 32B models. SynLogic leads to state-of-the-art logical reasoning\nperformance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B\nby 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and\ncoding tasks improves the training efficiency of these domains and\nsignificantly enhances reasoning generalization. Notably, our mixed training\nmodel outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These\nfindings position SynLogic as a valuable resource for advancing the broader\nreasoning capabilities of LLMs. We open-source both the data synthesis pipeline\nand the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic."}
{"id": "2505.18758", "pdf": "https://arxiv.org/pdf/2505.18758", "abs": "https://arxiv.org/abs/2505.18758", "authors": ["Alexander Conzelmann", "Robert Bamler"], "title": "Reducing Storage of Pretrained Neural Networks by Rate-Constrained Quantization and Entropy Coding", "categories": ["cs.LG"], "comment": "9 pages + 5 pages of appendix", "summary": "The ever-growing size of neural networks poses serious challenges on\nresource-constrained devices, such as embedded sensors. Compression algorithms\nthat reduce their size can mitigate these problems, provided that model\nperformance stays close to the original. We propose a novel post-training\ncompression framework that combines rate-aware quantization with entropy coding\nby (1) extending the well-known layer-wise loss by a quadratic rate estimation,\nand (2) providing locally exact solutions to this modified objective following\nthe Optimal Brain Surgeon (OBS) method. Our method allows for very fast\ndecoding and is compatible with arbitrary quantization grids. We verify our\nresults empirically by testing on various computer-vision networks, achieving a\n20-40\\% decrease in bit rate at the same performance as the popular compression\nalgorithm NNCodec. Our code is available at https://github.com/Conzel/cerwu."}
{"id": "2505.18995", "pdf": "https://arxiv.org/pdf/2505.18995", "abs": "https://arxiv.org/abs/2505.18995", "authors": ["Carlos Jude G. Maminta", "Isaiah Job Enriquez", "Deandre Nigel Nunez", "Michael B. Dela Fuente"], "title": "FiLLM -- A Filipino-optimized Large Language Model based on Southeast Asia Large Language Model (SEALLM)", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study presents FiLLM, a Filipino-optimized large language model,\ndesigned to enhance natural language processing (NLP) capabilities in the\nFilipino language. Built upon the SeaLLM-7B 2.5 model, FiLLM leverages Low-Rank\nAdaptation (LoRA) fine-tuning to optimize memory efficiency while maintaining\ntask-specific performance. The model was trained and evaluated on diverse\nFilipino datasets to address key NLP tasks, including Named Entity Recognition\n(NER), Part-of-Speech (POS) tagging, Dependency Parsing, and Text\nSummarization. Performance comparisons with the CalamanCy model were conducted\nusing F1 Score, Precision, Recall, Compression Rate, and Keyword Overlap\nmetrics. Results indicate that Calamancy outperforms FILLM in several aspects,\ndemonstrating its effectiveness in processing Filipino text with improved\nlinguistic comprehension and adaptability. This research contributes to the\nadvancement of Filipino NLP applications by providing an optimized, efficient,\nand scalable language model tailored for local linguistic needs."}
{"id": "2505.19653", "pdf": "https://arxiv.org/pdf/2505.19653", "abs": "https://arxiv.org/abs/2505.19653", "authors": ["Yang Ning", "Lin Hai", "Liu Yibo", "Tian Baoliang", "Liu Guoqing", "Zhang Haijun"], "title": "Token-Importance Guided Direct Preference Optimization", "categories": ["cs.AI"], "comment": null, "summary": "Ensuring that large language models (LLMs) generate outputs aligned with\nhuman preferences is important for safe and effective AI interactions. While\nDirect Preference Optimization (DPO) employs an implicit reward function to\noptimize the policy model, however, it and its related variants overlook the\ndifferential importance of individual tokens and are sensitive to judgment\nnoise in preference datasets during generation. Although recent methods attempt\nto assess the important weight of tokens via probability prediction or\nsimplistic weighting schemes, these evaluation methods are prone to biases and\nstill cannot fully address these issues. To solve this problem, we propose the\nToken-Importance Guided Direct Preference Optimization (TI-DPO), which\nintroduces two key innovations: the gradient-based token-importance weights\nthat dynamically prioritize critical tokens, and a triple loss that explicitly\nguides model outputs to approach human-preferred responses and stay away from\nnon-preferred responses. Experimental results show that TI-DPO achieves higher\naccuracy and stronger generative diversity, providing more stable and\ncomputationally efficient solutions compared with DPO and other RLHF methods."}
{"id": "2505.18763", "pdf": "https://arxiv.org/pdf/2505.18763", "abs": "https://arxiv.org/abs/2505.18763", "authors": ["Shutong Ding", "Ke Hu", "Shan Zhong", "Haoyang Luo", "Weinan Zhang", "Jingya Wang", "Jun Wang", "Ye Shi"], "title": "GenPO: Generative Diffusion Models Meet On-Policy Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Recent advances in reinforcement learning (RL) have demonstrated the powerful\nexploration capabilities and multimodality of generative diffusion-based\npolicies. While substantial progress has been made in offline RL and off-policy\nRL settings, integrating diffusion policies into on-policy frameworks like PPO\nremains underexplored. This gap is particularly significant given the\nwidespread use of large-scale parallel GPU-accelerated simulators, such as\nIsaacLab, which are optimized for on-policy RL algorithms and enable rapid\ntraining of complex robotic tasks. A key challenge lies in computing\nstate-action log-likelihoods under diffusion policies, which is straightforward\nfor Gaussian policies but intractable for flow-based models due to irreversible\nforward-reverse processes and discretization errors (e.g., Euler-Maruyama\napproximations). To bridge this gap, we propose GenPO, a generative policy\noptimization framework that leverages exact diffusion inversion to construct\ninvertible action mappings. GenPO introduces a novel doubled dummy action\nmechanism that enables invertibility via alternating updates, resolving\nlog-likelihood computation barriers. Furthermore, we also use the action\nlog-likelihood for unbiased entropy and KL divergence estimation, enabling\nKL-adaptive learning rates and entropy regularization in on-policy updates.\nExtensive experiments on eight IsaacLab benchmarks, including legged locomotion\n(Ant, Humanoid, Anymal-D, Unitree H1, Go2), dexterous manipulation (Shadow\nHand), aerial control (Quadcopter), and robotic arm tasks (Franka), demonstrate\nGenPO's superiority over existing RL baselines. Notably, GenPO is the first\nmethod to successfully integrate diffusion policies into on-policy RL,\nunlocking their potential for large-scale parallelized training and real-world\nrobotic deployment."}
{"id": "2505.19000", "pdf": "https://arxiv.org/pdf/2505.19000", "abs": "https://arxiv.org/abs/2505.19000", "authors": ["Yunxin Li", "Xinyu Chen", "Zitao Li", "Zhenyu Liu", "Longyue Wang", "Wenhan Luo", "Baotian Hu", "Min Zhang"], "title": "VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Gudied Iterative Policy Optimization", "categories": ["cs.CL", "cs.CV"], "comment": "19 pages, 9 figures, Project Link:\n  https://github.com/HITsz-TMG/VerIPO", "summary": "Applying Reinforcement Learning (RL) to Video Large Language Models\n(Video-LLMs) shows significant promise for complex video reasoning. However,\npopular Reinforcement Fine-Tuning (RFT) methods, such as outcome-based Group\nRelative Policy Optimization (GRPO), are limited by data preparation\nbottlenecks (e.g., noise or high cost) and exhibit unstable improvements in the\nquality of long chain-of-thoughts (CoTs) and downstream performance.To address\nthese limitations, we propose VerIPO, a Verifier-guided Iterative Policy\nOptimization method designed to gradually improve video LLMs' capacity for\ngenerating deep, long-term reasoning chains. The core component is\nRollout-Aware Verifier, positioned between the GRPO and Direct Preference\nOptimization (DPO) training phases to form the GRPO-Verifier-DPO training loop.\nThis verifier leverages small LLMs as a judge to assess the reasoning logic of\nrollouts, enabling the construction of high-quality contrastive data, including\nreflective and contextually consistent CoTs. These curated preference samples\ndrive the efficient DPO stage (7x faster than GRPO), leading to marked\nimprovements in reasoning chain quality, especially in terms of length and\ncontextual consistency. This training loop benefits from GRPO's expansive\nsearch and DPO's targeted optimization. Experimental results demonstrate: 1)\nSignificantly faster and more effective optimization compared to standard GRPO\nvariants, yielding superior performance; 2) Our trained models exceed the\ndirect inference of large-scale instruction-tuned Video-LLMs, producing long\nand contextually consistent CoTs on diverse video reasoning tasks; and 3) Our\nmodel with one iteration outperforms powerful LMMs (e.g., Kimi-VL) and long\nreasoning models (e.g., Video-R1), highlighting its effectiveness and\nstability."}
{"id": "2505.19662", "pdf": "https://arxiv.org/pdf/2505.19662", "abs": "https://arxiv.org/abs/2505.19662", "authors": ["Atsunori Moteki", "Shoichi Masui", "Fan Yang", "Yueqi Song", "Yonatan Bisk", "Graham Neubig", "Ikuo Kusajima", "Yasuto Watanabe", "Hiroyuki Ishida", "Jun Takahashi", "Shan Jiang"], "title": "FieldWorkArena: Agentic AI Benchmark for Real Field Work Tasks", "categories": ["cs.AI", "cs.CV"], "comment": "6 pages, 2 figures, 4 tables", "summary": "This paper proposes FieldWorkArena, a benchmark for agentic AI targeting\nreal-world field work. With the recent increase in demand for agentic AI, they\nare required to monitor and report safety and health incidents, as well as\nmanufacturing-related incidents, that may occur in real-world work\nenvironments. Existing agentic AI benchmarks have been limited to evaluating\nweb tasks and are insufficient for evaluating agents in real-world work\nenvironments, where complexity increases significantly. In this paper, we\ndefine a new action space that agentic AI should possess for real world work\nenvironment benchmarks and improve the evaluation function from previous\nmethods to assess the performance of agentic AI in diverse real-world tasks.\nThe dataset consists of videos captured on-site and documents actually used in\nfactories and warehouses, and tasks were created based on interviews with\non-site workers and managers. Evaluation results confirmed that performance\nevaluation considering the characteristics of Multimodal LLM (MLLM) such as\nGPT-4o is feasible. Additionally, the effectiveness and limitations of the\nproposed new evaluation method were identified. The complete dataset\n(HuggingFace) and evaluation program (GitHub) can be downloaded from the\nfollowing website:\nhttps://en-documents.research.global.fujitsu.com/fieldworkarena/."}
{"id": "2505.18765", "pdf": "https://arxiv.org/pdf/2505.18765", "abs": "https://arxiv.org/abs/2505.18765", "authors": ["Dai Hai Nguyen", "Hiroshi Mamitsuka", "Atsuyoshi Nakamura"], "title": "Multiple Wasserstein Gradient Descent Algorithm for Multi-Objective Distributional Optimization", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted to UAI 2025 (Oral Presentation)", "summary": "We address the optimization problem of simultaneously minimizing multiple\nobjective functionals over a family of probability distributions. This type of\nMulti-Objective Distributional Optimization commonly arises in machine learning\nand statistics, with applications in areas such as multiple target sampling,\nmulti-task learning, and multi-objective generative modeling. To solve this\nproblem, we propose an iterative particle-based algorithm, which we call\nMuliple Wasserstein Gradient Descent (MWGraD), which constructs a flow of\nintermediate empirical distributions, each being represented by a set of\nparticles, which gradually minimize the multiple objective functionals\nsimultaneously. Specifically, MWGraD consists of two key steps at each\niteration. First, it estimates the Wasserstein gradient for each objective\nfunctional based on the current particles. Then, it aggregates these gradients\ninto a single Wasserstein gradient using dynamically adjusted weights and\nupdates the particles accordingly. In addition, we provide theoretical analysis\nand present experimental results on both synthetic and real-world datasets,\ndemonstrating the effectiveness of MWGraD."}
{"id": "2505.19018", "pdf": "https://arxiv.org/pdf/2505.19018", "abs": "https://arxiv.org/abs/2505.19018", "authors": ["Md. Mithun Hossain", "Md. Shakil Hossain", "Sudipto Chaki", "Md. Rajib Hossain", "Md. Saifur Rahman", "A. B. M. Shawkat Ali"], "title": "CrosGrpsABS: Cross-Attention over Syntactic and Semantic Graphs for Aspect-Based Sentiment Analysis in a Low-Resource Language", "categories": ["cs.CL"], "comment": null, "summary": "Aspect-Based Sentiment Analysis (ABSA) is a fundamental task in natural\nlanguage processing, offering fine-grained insights into opinions expressed in\ntext. While existing research has largely focused on resource-rich languages\nlike English which leveraging large annotated datasets, pre-trained models, and\nlanguage-specific tools. These resources are often unavailable for low-resource\nlanguages such as Bengali. The ABSA task in Bengali remains poorly explored and\nis further complicated by its unique linguistic characteristics and a lack of\nannotated data, pre-trained models, and optimized hyperparameters. To address\nthese challenges, this research propose CrosGrpsABS, a novel hybrid framework\nthat leverages bidirectional cross-attention between syntactic and semantic\ngraphs to enhance aspect-level sentiment classification. The CrosGrpsABS\ncombines transformerbased contextual embeddings with graph convolutional\nnetworks, built upon rule-based syntactic dependency parsing and semantic\nsimilarity computations. By employing bidirectional crossattention, the model\neffectively fuses local syntactic structure with global semantic context,\nresulting in improved sentiment classification performance across both low- and\nhigh-resource settings. We evaluate CrosGrpsABS on four low-resource Bengali\nABSA datasets and the high-resource English SemEval 2014 Task 4 dataset. The\nCrosGrpsABS consistently outperforms existing approaches, achieving notable\nimprovements, including a 0.93% F1-score increase for the Restaurant domain and\na 1.06% gain for the Laptop domain in the SemEval 2014 Task 4 benchmark."}
{"id": "2505.19676", "pdf": "https://arxiv.org/pdf/2505.19676", "abs": "https://arxiv.org/abs/2505.19676", "authors": ["Lachlan McGinness", "Peter Baumgartner"], "title": "Large Language Models' Reasoning Stalls: An Investigation into the Capabilities of Frontier Models", "categories": ["cs.AI"], "comment": null, "summary": "Empirical methods to examine the capability of Large Language Models (LLMs)\nto use Automated Theorem Prover (ATP) reasoning strategies are studied. We\nevaluate the performance of State of the Art models from December 2023 and\nAugust 2024 on PRONTOQA steamroller reasoning problems. For that, we develop\nmethods for assessing LLM response accuracy and correct answer correlation.\n  Our results show that progress in improving LLM reasoning abilities has\nstalled over the nine month period. By tracking completion tokens, we show that\nalmost all improvement in reasoning ability since GPT-4 was released can be\nattributed to either hidden system prompts or the training of models to\nautomatically use generic Chain of Thought prompting strategies. Among the ATP\nreasoning strategies tried, we found that current frontier LLMs are best able\nto follow the bottom-up (also known as forward-chaining) strategy. A low\npositive correlation was found between an LLM response containing correct\nreasoning and arriving at the correct conclusion."}
{"id": "2505.18777", "pdf": "https://arxiv.org/pdf/2505.18777", "abs": "https://arxiv.org/abs/2505.18777", "authors": ["Yiding Wang", "Fauxu meng", "Xuefeng Zhang", "Fan Jiang", "Pingzhi Tang", "Muhan Zhang"], "title": "HD-PiSSA: High-Rank Distributed Orthogonal Adaptation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Existing parameter-efficient fine-tuning (PEFT) methods for large language\nmodels (LLMs), such as LoRA and PiSSA, constrain model updates to low-rank\nsubspaces, limiting their expressiveness and leading to suboptimal performance\non complex tasks. To address this, we introduce High-rank Distributed PiSSA\n(HD-PiSSA), a distributed PEFT approach that initializes orthogonal adapters\nacross different devices and aggregates their delta updates collectively on W\nfor fine-tuning. Unlike Data Parallel LoRA or PiSSA, which maintain identical\nadapters across all devices, HD-PiSSA assigns different principal components of\nthe pre-trained weights to each GPU, significantly expanding the range of\nupdate directions. This results in over 16x higher effective updated ranks than\ndata-parallel LoRA or PiSSA when fine-tuning on 8 GPUs with the same per-device\nadapter rank. Empirically, we evaluate HD-PiSSA across various challenging\ndownstream tasks, including mathematics, code generation, and multi-task\nlearning. In the multi-task setting, HD-PiSSA achieves average gains of 10.0\nabsolute points (14.63%) over LoRA and 4.98 points (6.60%) over PiSSA across 12\nbenchmarks, demonstrating its benefits from the extra optimization flexibility."}
{"id": "2505.19051", "pdf": "https://arxiv.org/pdf/2505.19051", "abs": "https://arxiv.org/abs/2505.19051", "authors": ["Mahdi Nikdan", "Vincent Cohen-Addad", "Dan Alistarh", "Vahab Mirrokni"], "title": "Efficient Data Selection at Scale via Influence Distillation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Effective data selection is critical for efficient training of modern Large\nLanguage Models (LLMs). This paper introduces Influence Distillation, a novel,\nmathematically-justified framework for data selection that employs second-order\ninformation to optimally weight training samples. By distilling each sample's\ninfluence on a target distribution, our method assigns model-specific weights\nthat are used to select training data for LLM fine-tuning, guiding it toward\nstrong performance on the target domain. We derive these optimal weights for\nboth Gradient Descent and Adam optimizers. To ensure scalability and reduce\ncomputational cost, we propose a $\\textit{landmark-based approximation}$:\ninfluence is precisely computed for a small subset of \"landmark\" samples and\nthen efficiently propagated to all other samples to determine their weights. We\nvalidate Influence Distillation by applying it to instruction tuning on the\nTulu V2 dataset, targeting a range of tasks including GSM8k, SQuAD, and MMLU,\nacross several models from the Llama and Qwen families. Experiments show that\nInfluence Distillation matches or outperforms state-of-the-art performance\nwhile achieving up to $3.5\\times$ faster selection."}
{"id": "2505.19683", "pdf": "https://arxiv.org/pdf/2505.19683", "abs": "https://arxiv.org/abs/2505.19683", "authors": ["Pengfei Cao", "Tianyi Men", "Wencan Liu", "Jingwen Zhang", "Xuzhao Li", "Xixun Lin", "Dianbo Sui", "Yanan Cao", "Kang Liu", "Jun Zhao"], "title": "Large Language Models for Planning: A Comprehensive and Systematic Survey", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Planning represents a fundamental capability of intelligent agents, requiring\ncomprehensive environmental understanding, rigorous logical reasoning, and\neffective sequential decision-making. While Large Language Models (LLMs) have\ndemonstrated remarkable performance on certain planning tasks, their broader\napplication in this domain warrants systematic investigation. This paper\npresents a comprehensive review of LLM-based planning. Specifically, this\nsurvey is structured as follows: First, we establish the theoretical\nfoundations by introducing essential definitions and categories about automated\nplanning. Next, we provide a detailed taxonomy and analysis of contemporary\nLLM-based planning methodologies, categorizing them into three principal\napproaches: 1) External Module Augmented Methods that combine LLMs with\nadditional components for planning, 2) Finetuning-based Methods that involve\nusing trajectory data and feedback signals to adjust LLMs in order to improve\ntheir planning abilities, and 3) Searching-based Methods that break down\ncomplex tasks into simpler components, navigate the planning space, or enhance\ndecoding strategies to find the best solutions. Subsequently, we systematically\nsummarize existing evaluation frameworks, including benchmark datasets,\nevaluation metrics and performance comparisons between representative planning\nmethods. Finally, we discuss the underlying mechanisms enabling LLM-based\nplanning and outline promising research directions for this rapidly evolving\nfield. We hope this survey will serve as a valuable resource to inspire\ninnovation and drive progress in this field."}
{"id": "2505.18781", "pdf": "https://arxiv.org/pdf/2505.18781", "abs": "https://arxiv.org/abs/2505.18781", "authors": ["Shizheng Wen", "Arsh Kumbhat", "Levi Lingsch", "Sepehr Mousavi", "Praveen Chandrashekar", "Siddhartha Mishra"], "title": "Geometry Aware Operator Transformer as an Efficient and Accurate Neural Surrogate for PDEs on Arbitrary Domains", "categories": ["cs.LG"], "comment": null, "summary": "The very challenging task of learning solution operators of PDEs on arbitrary\ndomains accurately and efficiently is of vital importance to engineering and\nindustrial simulations. Despite the existence of many operator learning\nalgorithms to approximate such PDEs, we find that accurate models are not\nnecessarily computationally efficient and vice versa. We address this issue by\nproposing a geometry aware operator transformer (GAOT) for learning PDEs on\narbitrary domains. GAOT combines novel multiscale attentional graph neural\noperator encoders and decoders, together with geometry embeddings and (vision)\ntransformer processors to accurately map information about the domain and the\ninputs into a robust approximation of the PDE solution. Multiple innovations in\nthe implementation of GAOT also ensure computational efficiency and\nscalability. We demonstrate this significant gain in both accuracy and\nefficiency of GAOT over several baselines on a large number of learning tasks\nfrom a diverse set of PDEs, including achieving state of the art performance on\na large scale three-dimensional industrial CFD dataset."}
{"id": "2505.19056", "pdf": "https://arxiv.org/pdf/2505.19056", "abs": "https://arxiv.org/abs/2505.19056", "authors": ["Harethah Abu Shairah", "Hasan Abed Al Kader Hammoud", "Bernard Ghanem", "George Turkiyyah"], "title": "An Embarrassingly Simple Defense Against LLM Abliteration Attacks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "preprint", "summary": "Large language models (LLMs) are typically aligned to comply with safety\nguidelines by refusing harmful instructions. A recent attack, termed\nabliteration, isolates and suppresses the single latent direction most\nresponsible for refusal behavior, enabling the model to generate unethical\ncontent. We propose a defense that modifies how models generate refusals. We\nconstruct an extended-refusal dataset that contains harmful prompts with a full\nresponse that justifies the reason for refusal. We then fine-tune\nLlama-2-7B-Chat and Qwen2.5-Instruct (1.5B and 3B parameters) on our\nextended-refusal dataset, and evaluate the resulting systems on a set of\nharmful prompts. In our experiments, extended-refusal models maintain high\nrefusal rates, dropping at most by 10%, whereas baseline models' refusal rates\ndrop by 70-80% after abliteration. A broad evaluation of safety and utility\nshows that extended-refusal fine-tuning neutralizes the abliteration attack\nwhile preserving general performance."}
{"id": "2505.19690", "pdf": "https://arxiv.org/pdf/2505.19690", "abs": "https://arxiv.org/abs/2505.19690", "authors": ["Baihui Zheng", "Boren Zheng", "Kerui Cao", "Yingshui Tan", "Zhendong Liu", "Weixun Wang", "Jiaheng Liu", "Jian Yang", "Wenbo Su", "Xiaoyong Zhu", "Bo Zheng", "Kaifu Zhang"], "title": "Beyond Safe Answers: A Benchmark for Evaluating True Risk Awareness in Large Reasoning Models", "categories": ["cs.AI"], "comment": null, "summary": "Despite the remarkable proficiency of \\textit{Large Reasoning Models} (LRMs)\nin handling complex reasoning tasks, their reliability in safety-critical\nscenarios remains uncertain. Existing evaluations primarily assess\nresponse-level safety, neglecting a critical issue we identify as\n\\textbf{\\textit{Superficial Safety Alignment} (SSA)} -- a phenomenon where\nmodels produce superficially safe outputs while internal reasoning processes\nfail to genuinely detect and mitigate underlying risks, resulting in\ninconsistent safety behaviors across multiple sampling attempts. To\nsystematically investigate SSA, we introduce \\textbf{Beyond Safe Answers (BSA)}\nbench, a novel benchmark comprising 2,000 challenging instances organized into\nthree distinct SSA scenario types and spanning nine risk categories, each\nmeticulously annotated with risk rationales. Evaluations of 19 state-of-the-art\nLRMs demonstrate the difficulty of this benchmark, with top-performing models\nachieving only 38.0\\% accuracy in correctly identifying risk rationales. We\nfurther explore the efficacy of safety rules, specialized fine-tuning on safety\nreasoning data, and diverse decoding strategies in mitigating SSA. Our work\nprovides a comprehensive assessment tool for evaluating and improving safety\nreasoning fidelity in LRMs, advancing the development of genuinely risk-aware\nand reliably safe AI systems."}
{"id": "2505.18783", "pdf": "https://arxiv.org/pdf/2505.18783", "abs": "https://arxiv.org/abs/2505.18783", "authors": ["Xinbao Qiao", "Ningning Ding", "Yushi Cheng", "Meng Zhang"], "title": "Soft Weighted Machine Unlearning", "categories": ["cs.LG", "cs.AI"], "comment": "24 pages,22 figures", "summary": "Machine unlearning, as a post-hoc processing technique, has gained widespread\nadoption in addressing challenges like bias mitigation and robustness\nenhancement, colloquially, machine unlearning for fairness and robustness.\nHowever, existing non-privacy unlearning-based solutions persist in using\nbinary data removal framework designed for privacy-driven motivation, leading\nto significant information loss, a phenomenon known as over-unlearning. While\nover-unlearning has been largely described in many studies as primarily causing\nutility degradation, we investigate its fundamental causes and provide deeper\ninsights in this work through counterfactual leave-one-out analysis. In this\npaper, we introduce a weighted influence function that assigns tailored weights\nto each sample by solving a convex quadratic programming problem analytically.\nBuilding on this, we propose a soft-weighted framework enabling fine-grained\nmodel adjustments to address the over-unlearning challenge. We demonstrate that\nthe proposed soft-weighted scheme is versatile and can be seamlessly integrated\ninto most existing unlearning algorithms. Extensive experiments show that in\nfairness- and robustness-driven tasks, the soft-weighted scheme significantly\noutperforms hard-weighted schemes in fairness/robustness metrics and alleviates\nthe decline in utility metric, thereby enhancing machine unlearning algorithm\nas an effective correction solution."}
{"id": "2505.19060", "pdf": "https://arxiv.org/pdf/2505.19060", "abs": "https://arxiv.org/abs/2505.19060", "authors": ["Roman Vashurin", "Maiya Goloburda", "Preslav Nakov", "Maxim Panov"], "title": "UNCERTAINTY-LINE: Length-Invariant Estimation of Uncertainty for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have become indispensable tools across various\napplications, making it more important than ever to ensure the quality and the\ntrustworthiness of their outputs. This has led to growing interest in\nuncertainty quantification (UQ) methods for assessing the reliability of LLM\noutputs. Many existing UQ techniques rely on token probabilities, which\ninadvertently introduces a bias with respect to the length of the output. While\nsome methods attempt to account for this, we demonstrate that such biases\npersist even in length-normalized approaches. To address the problem, here we\npropose UNCERTAINTY-LINE: (Length-INvariant Estimation), a simple debiasing\nprocedure that regresses uncertainty scores on output length and uses the\nresiduals as corrected, length-invariant estimates. Our method is post-hoc,\nmodel-agnostic, and applicable to a range of UQ measures. Through extensive\nevaluation on machine translation, summarization, and question-answering tasks,\nwe demonstrate that UNCERTAINTY-LINE: consistently improves over even nominally\nlength-normalized UQ methods uncertainty estimates across multiple metrics and\nmodels."}
{"id": "2505.19716", "pdf": "https://arxiv.org/pdf/2505.19716", "abs": "https://arxiv.org/abs/2505.19716", "authors": ["Yifan Wu", "Jingze Shi", "Bingheng Wu", "Jiayi Zhang", "Xiaotian Lin", "Nan Tang", "Yuyu Luo"], "title": "Concise Reasoning, Big Gains: Pruning Long Reasoning Trace with Difficulty-Aware Prompting", "categories": ["cs.AI"], "comment": null, "summary": "Existing chain-of-thought (CoT) distillation methods can effectively transfer\nreasoning abilities to base models but suffer from two major limitations:\nexcessive verbosity of reasoning traces and inadequate adaptability to problem\ndifficulty. Long reasoning traces significantly increase inference costs, and\nuniform-length solutions prevent base models from learning adaptive reasoning\nstrategies. To address these issues, we propose a difficulty-aware prompting\n(DAP) method to dynamically shorten reasoning traces without performance loss.\nIn our approach, a large teacher model first judges each problem's difficulty\nand then rewrites its reasoning traces to an appropriate shorter length,\nyielding concise yet complete reasoning traces. Leveraging the DAP pipeline, we\ncurate a distilled dataset called LiteCoT consisting of 100K concise reasoning\nexamples, with solutions averaging only 720 tokens (an order of magnitude\nshorter than typical CoTs). Using LiteCoT, we distilled a new family of\nreasoning models called Liter (1.5B, 7B, and 32B) based on the Qwen2.5\narchitecture. Experiments show that a student model fine-tuned on just 100K of\nthese difficulty-pruned CoT samples outperforms a model distilled on 800K\noriginal Long CoT samples, while significantly reducing training and inference\ncosts. Our method also generalizes well: across 11 diverse benchmarks, the\nshorter difficulty-aware CoTs achieve equal or better accuracy than Long\nchains, using far fewer tokens. For example, on the challenging AIME24 exam,\nour approach reaches $74.2\\%$ Pass@1 using only about 5K inference tokens,\nsurpassing other methods that consume many more tokens. Our code and data are\navailable at https://github.com/Evanwu1125/LiteCoT."}
{"id": "2505.18786", "pdf": "https://arxiv.org/pdf/2505.18786", "abs": "https://arxiv.org/abs/2505.18786", "authors": ["Nazanin Mohammadi Sepahvand", "Anvith Thudi", "Berivan Isik", "Ashmita Bhattacharyya", "Nicolas Papernot", "Eleni Triantafillou", "Daniel M. Roy", "Gintare Karolina Dziugaite"], "title": "Leveraging Per-Instance Privacy for Machine Unlearning", "categories": ["cs.LG"], "comment": null, "summary": "We present a principled, per-instance approach to quantifying the difficulty\nof unlearning via fine-tuning. We begin by sharpening an analysis of noisy\ngradient descent for unlearning (Chien et al., 2024), obtaining a better\nutility-unlearning tradeoff by replacing worst-case privacy loss bounds with\nper-instance privacy losses (Thudi et al., 2024), each of which bounds the\n(Renyi) divergence to retraining without an individual data point. To\ndemonstrate the practical applicability of our theory, we present empirical\nresults showing that our theoretical predictions are born out both for\nStochastic Gradient Langevin Dynamics (SGLD) as well as for standard\nfine-tuning without explicit noise. We further demonstrate that per-instance\nprivacy losses correlate well with several existing data difficulty metrics,\nwhile also identifying harder groups of data points, and introduce novel\nevaluation methods based on loss barriers. All together, our findings provide a\nfoundation for more efficient and adaptive unlearning strategies tailored to\nthe unique properties of individual data points."}
{"id": "2505.19073", "pdf": "https://arxiv.org/pdf/2505.19073", "abs": "https://arxiv.org/abs/2505.19073", "authors": ["Rui Li", "Jing Long", "Muge Qi", "Heming Xia", "Lei Sha", "Peiyi Wang", "Zhifang Sui"], "title": "Towards Harmonized Uncertainty Estimation for Large Language Models", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "To facilitate robust and trustworthy deployment of large language models\n(LLMs), it is essential to quantify the reliability of their generations\nthrough uncertainty estimation. While recent efforts have made significant\nadvancements by leveraging the internal logic and linguistic features of LLMs\nto estimate uncertainty scores, our empirical analysis highlights the pitfalls\nof these methods to strike a harmonized estimation between indication, balance,\nand calibration, which hinders their broader capability for accurate\nuncertainty estimation. To address this challenge, we propose CUE (Corrector\nfor Uncertainty Estimation): A straightforward yet effective method that\nemploys a lightweight model trained on data aligned with the target LLM's\nperformance to adjust uncertainty scores. Comprehensive experiments across\ndiverse models and tasks demonstrate its effectiveness, which achieves\nconsistent improvements of up to 60% over existing methods."}
{"id": "2505.19734", "pdf": "https://arxiv.org/pdf/2505.19734", "abs": "https://arxiv.org/abs/2505.19734", "authors": ["Juxin Niu", "Xiangfeng Liu", "Dan Niu", "Xi Wang", "Zhe Jiang", "Nan Guan"], "title": "ReChisel: Effective Automatic Chisel Code Generation by LLM with Reflection", "categories": ["cs.AI", "cs.AR"], "comment": "Accepted to DAC 2025", "summary": "Coding with hardware description languages (HDLs) such as Verilog is a\ntime-intensive and laborious task. With the rapid advancement of large language\nmodels (LLMs), there is increasing interest in applying LLMs to assist with HDL\ncoding. Recent efforts have demonstrated the potential of LLMs in translating\nnatural language to traditional HDL Verilog. Chisel, a next-generation HDL\nbased on Scala, introduces higher-level abstractions, facilitating more\nconcise, maintainable, and scalable hardware designs. However, the potential of\nusing LLMs for Chisel code generation remains largely unexplored. This work\nproposes ReChisel, an LLM-based agentic system designed to enhance the\neffectiveness of Chisel code generation. ReChisel incorporates a reflection\nmechanism to iteratively refine the quality of generated code using feedback\nfrom compilation and simulation processes, and introduces an escape mechanism\nto break free from non-progress loops. Experiments demonstrate that ReChisel\nsignificantly improves the success rate of Chisel code generation, achieving\nperformance comparable to state-of-the-art LLM-based agentic systems for\nVerilog code generation."}
{"id": "2505.18798", "pdf": "https://arxiv.org/pdf/2505.18798", "abs": "https://arxiv.org/abs/2505.18798", "authors": ["Lexiang Hu", "Yikang Li", "Zhouchen Lin"], "title": "Governing Equation Discovery from Data Based on Differential Invariants", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "The explicit governing equation is one of the simplest and most intuitive\nforms for characterizing physical laws. However, directly discovering partial\ndifferential equations (PDEs) from data poses significant challenges, primarily\nin determining relevant terms from a vast search space. Symmetry, as a crucial\nprior knowledge in scientific fields, has been widely applied in tasks such as\ndesigning equivariant networks and guiding neural PDE solvers. In this paper,\nwe propose a pipeline for governing equation discovery based on differential\ninvariants, which can losslessly reduce the search space of existing equation\ndiscovery methods while strictly adhering to symmetry. Specifically, we compute\nthe set of differential invariants corresponding to the infinitesimal\ngenerators of the symmetry group and select them as the relevant terms for\nequation discovery. Taking DI-SINDy (SINDy based on Differential Invariants) as\nan example, we demonstrate that its success rate and accuracy in PDE discovery\nsurpass those of other symmetry-informed governing equation discovery methods\nacross a series of PDEs."}
{"id": "2505.19091", "pdf": "https://arxiv.org/pdf/2505.19091", "abs": "https://arxiv.org/abs/2505.19091", "authors": ["Benjamin Clavi", "Florian Brand"], "title": "ReadBench: Measuring the Dense Text Visual Reading Ability of Vision-Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent advancements in Large Vision-Language Models (VLMs), have greatly\nenhanced their capability to jointly process text and images. However, despite\nextensive benchmarks evaluating visual comprehension (e.g., diagrams, color\nschemes, OCR tasks...), there is limited assessment of VLMs' ability to read\nand reason about text-rich images effectively. To fill this gap, we introduce\nReadBench, a multimodal benchmark specifically designed to evaluate the reading\ncomprehension capabilities of VLMs. ReadBench transposes contexts from\nestablished text-only benchmarks into images of text while keeping textual\nprompts and questions intact. Evaluating leading VLMs with ReadBench, we find\nminimal-but-present performance degradation on short, text-image inputs, while\nperformance sharply declines for longer, multi-page contexts. Our experiments\nfurther reveal that text resolution has negligible effects on multimodal\nperformance. These findings highlight needed improvements in VLMs, particularly\ntheir reasoning over visually presented extensive textual content, a capability\ncritical for practical applications. ReadBench is available at\nhttps://github.com/answerdotai/ReadBench ."}
{"id": "2505.19761", "pdf": "https://arxiv.org/pdf/2505.19761", "abs": "https://arxiv.org/abs/2505.19761", "authors": ["Zican Hu", "Wei Liu", "Xiaoye Qu", "Xiangyu Yue", "Chunlin Chen", "Zhi Wang", "Yu Cheng"], "title": "Divide and Conquer: Grounding LLMs as Efficient Decision-Making Agents via Offline Hierarchical Reinforcement Learning", "categories": ["cs.AI"], "comment": "Accepted by ICML 2025, 21 pages", "summary": "While showing sophisticated reasoning abilities, large language models (LLMs)\nstill struggle with long-horizon decision-making tasks due to deficient\nexploration and long-term credit assignment, especially in sparse-reward\nscenarios. Inspired by the divide-and-conquer principle, we propose an\ninnovative framework **GLIDER** (**G**rounding **L**anguage Models as\nEff**I**cient **D**ecision-Making Agents via Offline Hi**E**rarchical\n**R**einforcement Learning) that introduces a parameter-efficient and generally\napplicable hierarchy to LLM policies. We develop a scheme where the low-level\ncontroller is supervised with abstract, step-by-step plans that are learned and\ninstructed by the high-level policy. This design decomposes complicated\nproblems into a series of coherent chain-of-thought reasoning sub-tasks,\nproviding flexible temporal abstraction to significantly enhance exploration\nand learning for long-horizon tasks. Furthermore, GLIDER facilitates fast\nonline adaptation to non-stationary environments owing to the strong\ntransferability of its task-agnostic low-level skills. Experiments on\nScienceWorld and ALFWorld benchmarks show that GLIDER achieves consistent\nperformance gains, along with enhanced generalization capabilities."}
{"id": "2505.18825", "pdf": "https://arxiv.org/pdf/2505.18825", "abs": "https://arxiv.org/abs/2505.18825", "authors": ["Nicholas M. Boffi", "Michael S. Albergo", "Eric Vanden-Eijnden"], "title": "How to build a consistency model: Learning flow maps via self-distillation", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Building on the framework proposed in Boffi et al. (2024), we present a\nsystematic approach for learning flow maps associated with flow and diffusion\nmodels. Flow map-based models, commonly known as consistency models, encompass\nrecent efforts to improve the efficiency of generative models based on\nsolutions to differential equations. By exploiting a relationship between the\nvelocity field underlying a continuous-time flow and the instantaneous rate of\nchange of the flow map, we show how to convert existing distillation schemes\ninto direct training algorithms via self-distillation, eliminating the need for\npre-trained models. We empirically evaluate several instantiations of our\nframework, finding that high-dimensional tasks like image synthesis benefit\nfrom objective functions that avoid temporal and spatial derivatives of the\nflow map, while lower-dimensional tasks can benefit from objectives\nincorporating higher-order derivatives to capture sharp features."}
{"id": "2505.19100", "pdf": "https://arxiv.org/pdf/2505.19100", "abs": "https://arxiv.org/abs/2505.19100", "authors": ["Yeyuan Wang", "Dehong Gao", "Rujiao Long", "Lei Yi", "Linbo Jin", "Libin Yang", "Xiaoyan Cai"], "title": "ASPO: Adaptive Sentence-Level Preference Optimization for Fine-Grained Multimodal Reasoning", "categories": ["cs.CL", "cs.CV"], "comment": "Accepted by ACL 2025 findings", "summary": "Direct Preference Optimization (DPO) has gained significant attention for its\nsimplicity and computational efficiency in aligning large language models\n(LLMs). Recent advancements have extended DPO to multimodal scenarios,\nachieving strong performance. However, traditional DPO relies on binary\npreference optimization, rewarding or penalizing entire responses without\nconsidering fine-grained segment correctness, leading to suboptimal solutions.\nThe root of this issue lies in the absence of fine-grained supervision during\nthe optimization process. To address this, we propose Adaptive Sentence-level\nPreference Optimization (ASPO), which evaluates individual sentences for more\nprecise preference optimization. By dynamically calculating adaptive rewards at\nthe sentence level based on model predictions, ASPO enhances response content\nassessment without additional models or parameters. This significantly improves\nthe alignment of multimodal features. Extensive experiments show that ASPO\nsubstantially enhances the overall performance of multimodal models."}
{"id": "2505.19762", "pdf": "https://arxiv.org/pdf/2505.19762", "abs": "https://arxiv.org/abs/2505.19762", "authors": ["Wenjun Wang", "Dawei Cheng"], "title": "Language Model-Enhanced Message Passing for Heterophilic Graph Learning", "categories": ["cs.AI"], "comment": null, "summary": "Traditional graph neural networks (GNNs), which rely on homophily-driven\nmessage passing, struggle with heterophilic graphs where connected nodes\nexhibit dissimilar features and different labels. While existing methods\naddress heterophily through graph structure refinement or adaptation of\nneighbor aggregation functions, they often overlook the semantic potential of\nnode text, rely on suboptimal message representation for propagation and\ncompromise performance on homophilic graphs. To address these limitations, we\npropose a novel language model (LM)-enhanced message passing approach for\nheterophilic graph leaning (LEMP4HG). Specifically, in the context of\ntext-attributed graph, we provide paired node texts for LM to generate their\nconnection analysis, which are encoded and then fused with paired node textual\nembeddings through a gating mechanism. The synthesized messages are\nsemantically enriched and adaptively balanced with both nodes' information,\nwhich mitigates contradictory signals when neighbor aggregation in heterophilic\nregions. Furthermore, we introduce an active learning strategy guided by our\nheuristic MVRD (Modulated Variation of Reliable Distance), selectively\nenhancing node pairs suffer most from message passing, reducing the cost of\nanalysis generation and side effects on homophilic regions. Extensive\nexperiments validate that our approach excels on heterophilic graphs and\nperforms robustly on homophilic ones, with a graph convolutional network (GCN)\nbackbone and a practical budget."}
{"id": "2505.18828", "pdf": "https://arxiv.org/pdf/2505.18828", "abs": "https://arxiv.org/abs/2505.18828", "authors": ["Junyan Liu", "Ziyun Chen", "Kun Wang", "Haipeng Luo", "Lillian J. Ratliff"], "title": "Improved Regret and Contextual Linear Extension for Pandora's Box and Prophet Inequality", "categories": ["cs.LG", "cs.DS", "cs.GT"], "comment": null, "summary": "We study the Pandora's Box problem in an online learning setting with\nsemi-bandit feedback. In each round, the learner sequentially pays to open up\nto $n$ boxes with unknown reward distributions, observes rewards upon opening,\nand decides when to stop. The utility of the learner is the maximum observed\nreward minus the cumulative cost of opened boxes, and the goal is to minimize\nregret defined as the gap between the cumulative expected utility and that of\nthe optimal policy. We propose a new algorithm that achieves\n$\\widetilde{O}(\\sqrt{nT})$ regret after $T$ rounds, which improves the\n$\\widetilde{O}(n\\sqrt{T})$ bound of Agarwal et al. [2024] and matches the known\nlower bound up to logarithmic factors. To better capture real-life\napplications, we then extend our results to a natural but challenging\ncontextual linear setting, where each box's expected reward is linear in some\nknown but time-varying $d$-dimensional context and the noise distribution is\nfixed over time. We design an algorithm that learns both the linear function\nand the noise distributions, achieving $\\widetilde{O}(nd\\sqrt{T})$ regret.\nFinally, we show that our techniques also apply to the online Prophet\nInequality problem, where the learner must decide immediately whether or not to\naccept a revealed reward. In both non-contextual and contextual settings, our\napproach achieves similar improvements and regret bounds."}
{"id": "2505.19103", "pdf": "https://arxiv.org/pdf/2505.19103", "abs": "https://arxiv.org/abs/2505.19103", "authors": ["Iddo Yosha", "Dorin Shteyman", "Yossi Adi"], "title": "WHISTRESS: Enriching Transcriptions with Sentence Stress Detection", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech2025", "summary": "Spoken language conveys meaning not only through words but also through\nintonation, emotion, and emphasis. Sentence stress, the emphasis placed on\nspecific words within a sentence, is crucial for conveying speaker intent and\nhas been extensively studied in linguistics. In this work, we introduce\nWHISTRESS, an alignment-free approach for enhancing transcription systems with\nsentence stress detection. To support this task, we propose TINYSTRESS-15K, a\nscalable, synthetic training data for the task of sentence stress detection\nwhich resulted from a fully automated dataset creation process. We train\nWHISTRESS on TINYSTRESS-15K and evaluate it against several competitive\nbaselines. Our results show that WHISTRESS outperforms existing methods while\nrequiring no additional input priors during training or inference. Notably,\ndespite being trained on synthetic data, WHISTRESS demonstrates strong\nzero-shot generalization across diverse benchmarks. Project page:\nhttps://pages.cs.huji.ac.il/adiyoss-lab/whistress."}
{"id": "2505.19788", "pdf": "https://arxiv.org/pdf/2505.19788", "abs": "https://arxiv.org/abs/2505.19788", "authors": ["Zihao Zeng", "Xuyao Huang", "Boxiu Li", "Hao Zhang", "Zhijie Deng"], "title": "Done Is Better than Perfect: Unlocking Efficient Reasoning by Structured Multi-Turn Decomposition", "categories": ["cs.AI"], "comment": null, "summary": "Large Reasoning Models (LRMs) are criticized for the excessively lengthy\nChain-of-Thought (CoT) to derive the final answer, suffering from high\nfirst-token and overall latency. Typically, the CoT of LRMs mixes multiple\nthinking units; each unit attempts to produce a candidate answer to the\noriginal query. Hence, a natural idea to improve efficiency is to reduce the\nunit number. Yet, the fact that the thinking units in vanilla CoT cannot be\nexplicitly managed renders doing so challenging. This paper introduces\nMulti-Turn Decomposition (MinD) to decode conventional CoT into a sequence of\nexplicit, structured, and turn-wise interactions to bridge the gap. In MinD,\nthe model provides a multi-turn response to the query, where each turn embraces\na thinking unit and yields a corresponding answer. The subsequent turns can\nreflect, verify, revise, or explore alternative approaches to both the thinking\nand answer parts of earlier ones. This not only makes the answer delivered more\nswiftly, but also enables explicit controls over the iterative reasoning\nprocess (i.e., users may halt or continue at any turn). We follow a supervised\nfine-tuning (SFT) then reinforcement learning (RL) paradigm to realize MinD. We\nfirst rephrase the outputs of an LRM into multi-turn formats by prompting\nanother LLM, and then tune the LRM with such data. Observing that the tuned\nmodel tends to consume even more tokens than the original one (probably due to\nthat the multi-turn formats introduce additional answer tokens), we advocate\nleveraging RL algorithms like GRPO to prioritize correct outputs with fewer\nturns. Trained on the MATH dataset using R1-Distill models, MinD can achieve up\nto ~70% reduction in both output token usage and time to first token (TTFT),\nwhile maintaining competitive performance on reasoning benchmarks such as\nMATH-500, AIME24, AMC23, and GPQA-Diamond."}
{"id": "2505.18830", "pdf": "https://arxiv.org/pdf/2505.18830", "abs": "https://arxiv.org/abs/2505.18830", "authors": ["Wenlong Deng", "Yi Ren", "Muchen Li", "Danica J. Sutherland", "Xiaoxiao Li", "Christos Thrampoulidis"], "title": "On the Effect of Negative Gradient in Group Relative Deep Reinforcement Optimization", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) has become popular in enhancing the reasoning\ncapabilities of large language models (LLMs), with Group Relative Policy\nOptimization (GRPO) emerging as a widely used algorithm in recent systems.\nDespite GRPO's widespread adoption, we identify a previously unrecognized\nphenomenon we term Lazy Likelihood Displacement (LLD), wherein the likelihood\nof correct responses marginally increases or even decreases during training.\nThis behavior mirrors a recently discovered misalignment issue in Direct\nPreference Optimization (DPO), attributed to the influence of negative\ngradients. We provide a theoretical analysis of GRPO's learning dynamic,\nidentifying the source of LLD as the naive penalization of all tokens in\nincorrect responses with the same strength. To address this, we develop a\nmethod called NTHR, which downweights penalties on tokens contributing to the\nLLD. Unlike prior DPO-based approaches, NTHR takes advantage of GRPO's\ngroup-based structure, using correct responses as anchors to identify\ninfluential tokens. Experiments on math reasoning benchmarks demonstrate that\nNTHR effectively mitigates LLD, yielding consistent performance gains across\nmodels ranging from 0.5B to 3B parameters."}
{"id": "2505.19108", "pdf": "https://arxiv.org/pdf/2505.19108", "abs": "https://arxiv.org/abs/2505.19108", "authors": ["Yongheng Zhang", "Xu Liu", "Ruoxi Zhou", "Qiguang Chen", "Hao Fei", "Wenpeng Lu", "Libo Qin"], "title": "CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 Main Conference", "summary": "Investigating hallucination issues in large language models (LLMs) within\ncross-lingual and cross-modal scenarios can greatly advance the large-scale\ndeployment in real-world applications. Nevertheless, the current studies are\nlimited to a single scenario, either cross-lingual or cross-modal, leaving a\ngap in the exploration of hallucinations in the joint cross-lingual and\ncross-modal scenarios. Motivated by this, we introduce a novel joint\nCross-lingual and Cross-modal Hallucinations benchmark (CCHall) to fill this\ngap. Specifically, CCHall simultaneously incorporates both cross-lingual and\ncross-modal hallucination scenarios, which can be used to assess the\ncross-lingual and cross-modal capabilities of LLMs. Furthermore, we conduct a\ncomprehensive evaluation on CCHall, exploring both mainstream open-source and\nclosed-source LLMs. The experimental results highlight that current LLMs still\nstruggle with CCHall. We hope CCHall can serve as a valuable resource to assess\nLLMs in joint cross-lingual and cross-modal scenarios."}
{"id": "2505.19792", "pdf": "https://arxiv.org/pdf/2505.19792", "abs": "https://arxiv.org/abs/2505.19792", "authors": ["Claire Ott", "Frank Jkel"], "title": "Types of Relations: Defining Analogies with Category Theory", "categories": ["cs.AI"], "comment": "27 pages, 15 figures", "summary": "In order to behave intelligently both humans and machines have to represent\ntheir knowledge adequately for how it is used. Humans often use analogies to\ntransfer their knowledge to new domains, or help others with this transfer via\nexplanations. Hence, an important question is: What representation can be used\nto construct, find, and evaluate analogies? In this paper, we study features of\na domain that are important for constructing analogies. We do so by formalizing\nknowledge domains as categories. We use the well-known example of the analogy\nbetween the solar system and the hydrogen atom to demonstrate how to construct\ndomain categories. We also show how functors, pullbacks, and pushouts can be\nused to define an analogy, describe its core and a corresponding blend of the\nunderlying domains."}
{"id": "2505.18866", "pdf": "https://arxiv.org/pdf/2505.18866", "abs": "https://arxiv.org/abs/2505.18866", "authors": ["Md Farhamdur Reza", "Reza Jahani", "Richeng Jin", "Huaiyu Dai"], "title": "Distribution-Aware Mobility-Assisted Decentralized Federated Learning", "categories": ["cs.LG"], "comment": "Under review for possible publication in IEEE GLOBECOM 2025", "summary": "Decentralized federated learning (DFL) has attracted significant attention\ndue to its scalability and independence from a central server. In practice,\nsome participating clients can be mobile, yet the impact of user mobility on\nDFL performance remains largely unexplored, despite its potential to facilitate\ncommunication and model convergence. In this work, we demonstrate that\nintroducing a small fraction of mobile clients, even with random movement, can\nsignificantly improve the accuracy of DFL by facilitating information flow. To\nfurther enhance performance, we propose novel distribution-aware mobility\npatterns, where mobile clients strategically navigate the network, leveraging\nknowledge of data distributions and static client locations. The proposed\nmoving strategies mitigate the impact of data heterogeneity and boost learning\nconvergence. Extensive experiments validate the effectiveness of induced\nmobility in DFL and demonstrate the superiority of our proposed mobility\npatterns over random movement."}
{"id": "2505.19112", "pdf": "https://arxiv.org/pdf/2505.19112", "abs": "https://arxiv.org/abs/2505.19112", "authors": ["Zheng Chu", "Huiming Fan", "Jingchang Chen", "Qianyu Wang", "Mingda Yang", "Jiafeng Liang", "Zhongjie Wang", "Hao Li", "Guo Tang", "Ming Liu", "Bing Qin"], "title": "Self-Critique Guided Iterative Reasoning for Multi-hop Question Answering", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Although large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities, they still face challenges in knowledge-intensive multi-hop\nreasoning. Recent work explores iterative retrieval to address complex\nproblems. However, the lack of intermediate guidance often results in\ninaccurate retrieval and flawed intermediate reasoning, leading to incorrect\nreasoning. To address these, we propose Self-Critique Guided Iterative\nReasoning (SiGIR), which uses self-critique feedback to guide the iterative\nreasoning process. Specifically, through end-to-end training, we enable the\nmodel to iteratively address complex problems via question decomposition.\nAdditionally, the model is able to self-evaluate its intermediate reasoning\nsteps. During iterative reasoning, the model engages in branching exploration\nand employs self-evaluation to guide the selection of promising reasoning\ntrajectories. Extensive experiments on three multi-hop reasoning datasets\ndemonstrate the effectiveness of our proposed method, surpassing the previous\nSOTA by $8.6\\%$. Furthermore, our thorough analysis offers insights for future\nresearch. Our code, data, and models are available at Github:\nhttps://github.com/zchuz/SiGIR-MHQA."}
{"id": "2505.19847", "pdf": "https://arxiv.org/pdf/2505.19847", "abs": "https://arxiv.org/abs/2505.19847", "authors": ["Wenqing Zhou", "Yuxuan Yan", "Qianqian Yang"], "title": "DGRAG: Distributed Graph-based Retrieval-Augmented Generation in Edge-Cloud Systems", "categories": ["cs.AI", "cs.DC"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a promising approach to\nenhance the capabilities of language models by integrating external knowledge.\nDue to the diversity of data sources and the constraints of memory and\ncomputing resources, real-world data is often scattered in multiple devices.\nConventional RAGs that store massive amounts of scattered data centrally face\nincreasing privacy concerns and high computational costs. Additionally, RAG in\na central node raises latency issues when searching over a large-scale\nknowledge base. To address these challenges, we propose a distributed Knowledge\nGraph-based RAG approach, referred to as DGRAG, in an edge-cloud system, where\neach edge device maintains a local knowledge base without the need to share it\nwith the cloud, instead sharing only summaries of its knowledge. Specifically,\nDGRAG has two main phases. In the Distributed Knowledge Construction phase,\nDGRAG organizes local knowledge using knowledge graphs, generating subgraph\nsummaries and storing them in a summary database in the cloud as information\nsharing. In the Collaborative Retrieval and Generation phase, DGRAG first\nperforms knowledge retrieval and answer generation locally, and a gate\nmechanism determines whether the query is beyond the scope of local knowledge\nor processing capabilities. For queries that exceed the local knowledge scope,\nthe cloud retrieves knowledge from the most relevant edges based on the\nsummaries and generates a more precise answer. Experimental results demonstrate\nthe effectiveness of the proposed DGRAG approach in significantly improving the\nquality of question-answering tasks over baseline approaches."}
{"id": "2505.18877", "pdf": "https://arxiv.org/pdf/2505.18877", "abs": "https://arxiv.org/abs/2505.18877", "authors": ["Yilang Zhang", "Bingcong Li", "Georgios B. Giannakis"], "title": "RefLoRA: Refactored Low-Rank Adaptation for Efficient Fine-Tuning of Large Models", "categories": ["cs.LG"], "comment": null, "summary": "Low-Rank Adaptation (LoRA) lowers the computational and memory overhead of\nfine-tuning large models by updating a low-dimensional subspace of the\npre-trained weight matrix. Albeit efficient, LoRA exhibits suboptimal\nconvergence and noticeable performance degradation, due to inconsistent and\nimbalanced weight updates induced by its nonunique low-rank factorizations. To\novercome these limitations, this article identifies the optimal low-rank\nfactorization per step that minimizes an upper bound on the loss. The resultant\nrefactored low-rank adaptation (RefLoRA) method promotes a flatter loss\nlandscape, along with consistent and balanced weight updates, thus speeding up\nstable convergence. Extensive experiments evaluate RefLoRA on natural language\nunderstanding, and commonsense reasoning tasks with popular large language\nmodels including DeBERTaV3, LLaMA-7B, LLaMA2-7B and LLaMA3-8B. The numerical\ntests corroborate that RefLoRA converges faster, outperforms various\nbenchmarks, and enjoys negligible computational overhead compared to\nstate-of-the-art LoRA variants."}
{"id": "2505.19116", "pdf": "https://arxiv.org/pdf/2505.19116", "abs": "https://arxiv.org/abs/2505.19116", "authors": ["Nahyun Lee", "Yeongseo Woo", "Hyunwoo Ko", "Guijin Son"], "title": "Controlling Language Confusion in Multilingual LLMs", "categories": ["cs.CL"], "comment": "4 pages", "summary": "Large language models often suffer from language confusion, a phenomenon\nwhere responses are partially or entirely generated in unintended languages.\nThis can critically impact user experience in low-resource settings. We\nhypothesize that conventional supervised fine-tuning exacerbates this issue\nbecause the softmax objective focuses probability mass only on the single\ncorrect token but does not explicitly penalize cross-lingual mixing.\nInterestingly, by observing loss trajectories during the pretraining phase, we\nobserve that models fail to learn to distinguish between monolingual and\nlanguage-confused text. Additionally, we find that ORPO, which adds penalties\nfor unwanted output styles to standard SFT, effectively suppresses\nlanguage-confused generations even at high decoding temperatures without\ndegrading overall model performance. Our findings suggest that incorporating\nappropriate penalty terms can mitigate language confusion in low-resource\nsettings with limited data."}
{"id": "2505.19866", "pdf": "https://arxiv.org/pdf/2505.19866", "abs": "https://arxiv.org/abs/2505.19866", "authors": ["Feng Xiong", "Hongling Xu", "Yifei Wang", "Runxi Cheng", "Yong Wang", "Xiangxiang Chu"], "title": "HS-STAR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Self-taught reasoners (STaRs) enhance the mathematical reasoning abilities of\nlarge language models (LLMs) by leveraging self-generated responses for\nself-training. Recent studies have incorporated reward models to guide response\nselection or decoding, aiming to obtain higher-quality data. However, they\ntypically allocate a uniform sampling budget across all problems, overlooking\nthe varying utility of problems at different difficulty levels. In this work,\nwe conduct an empirical study and find that problems near the boundary of the\nLLM's reasoning capability offer significantly greater learning utility than\nboth easy and overly difficult ones. To identify and exploit such problems, we\npropose HS-STaR, a Hierarchical Sampling framework for Self-Taught Reasoners.\nGiven a fixed sampling budget, HS-STaR first performs lightweight pre-sampling\nwith a reward-guided difficulty estimation strategy to efficiently identify\nboundary-level problems. Subsequently, it dynamically reallocates the remaining\nbudget toward these high-utility problems during a re-sampling phase,\nmaximizing the generation of valuable training data. Extensive experiments\nacross multiple reasoning benchmarks and backbone LLMs demonstrate that HS-STaR\nsignificantly outperforms other baselines without requiring additional sampling\nbudget."}
{"id": "2505.18883", "pdf": "https://arxiv.org/pdf/2505.18883", "abs": "https://arxiv.org/abs/2505.18883", "authors": ["Justin Deschenaux", "Lan Tran", "Caglar Gulcehre"], "title": "Partition Generative Modeling: Masked Modeling Without Masks", "categories": ["cs.LG"], "comment": null, "summary": "We introduce ``Partition Generative Models'' (PGMs), a novel approach to\nmasked generative modeling (MGMs), particularly effective for masked diffusion\nlanguage modeling (MDLMs). PGM divides tokens into two distinct groups and\nemploys sparse attention patterns to prevent cross-group information exchange.\nHence, the model is trained to predict tokens in one group based solely on\ninformation from the other group. This partitioning strategy eliminates the\nneed for MASK tokens entirely. While traditional MGMs inefficiently process\nMASK tokens during generation, PGMs achieve greater computational efficiency by\noperating exclusively on unmasked tokens. Our experiments on OpenWebText with a\ncontext length of 1024 tokens demonstrate that PGMs deliver at least 5x\nimprovements in both latency and throughput compared to MDLM when using the\nsame number of sampling steps, while generating samples with better generative\nperplexity than MDLM. Finally, we show that PGMs can be distilled with\nSelf-Distillation Through Time (SDTT), a method originally devised for MDLM, in\norder to achieve further inference gains."}
{"id": "2505.19121", "pdf": "https://arxiv.org/pdf/2505.19121", "abs": "https://arxiv.org/abs/2505.19121", "authors": ["Seunguk Yu", "Juhwan Choi", "Youngbin Kim"], "title": "Delving into Multilingual Ethical Bias: The MSQAD with Statistical Hypothesis Tests for Large Language Models", "categories": ["cs.CL"], "comment": "ACL 2025 main conference", "summary": "Despite the recent strides in large language models, studies have underscored\nthe existence of social biases within these systems. In this paper, we delve\ninto the validation and comparison of the ethical biases of LLMs concerning\nglobally discussed and potentially sensitive topics, hypothesizing that these\nbiases may arise from language-specific distinctions. Introducing the\nMultilingual Sensitive Questions & Answers Dataset (MSQAD), we collected news\narticles from Human Rights Watch covering 17 topics, and generated socially\nsensitive questions along with corresponding responses in multiple languages.\nWe scrutinized the biases of these responses across languages and topics,\nemploying two statistical hypothesis tests. The results showed that the null\nhypotheses were rejected in most cases, indicating biases arising from\ncross-language differences. It demonstrates that ethical biases in responses\nare widespread across various languages, and notably, these biases were\nprevalent even among different LLMs. By making the proposed MSQAD openly\navailable, we aim to facilitate future research endeavors focused on examining\ncross-language biases in LLMs and their variant models."}
{"id": "2505.19892", "pdf": "https://arxiv.org/pdf/2505.19892", "abs": "https://arxiv.org/abs/2505.19892", "authors": ["Yongxian Wei", "Runxi Cheng", "Weike Jin", "Enneng Yang", "Li Shen", "Lu Hou", "Sinan Du", "Chun Yuan", "Xiaochun Cao", "Dacheng Tao"], "title": "Unifying Multimodal Large Language Model Capabilities and Modalities via Model Merging", "categories": ["cs.AI"], "comment": null, "summary": "While foundation models update slowly due to resource-intensive training\nrequirements, domain-specific models evolve between updates. Model merging aims\nto combine multiple expert models into a single, more capable model, thereby\nreducing storage and serving costs while supporting decentralized model\ndevelopment. Despite its potential, previous studies have primarily focused on\nmerging visual classification models or Large Language Models (LLMs) for code\nand math tasks. Multimodal Large Language Models (MLLMs), which extend the\ncapabilities of LLMs through large-scale multimodal training, have gained\ntraction. However, there lacks a benchmark for model merging research that\nclearly divides the tasks for MLLM training and evaluation. In this paper, (i)\nwe introduce the model merging benchmark for MLLMs, which includes multiple\ntasks such as VQA, Geometry, Chart, OCR, and Grounding, providing both LoRA and\nfull fine-tuning models. Moreover, we explore how model merging can combine\ndifferent modalities (e.g., vision-language, audio-language, and video-language\nmodels), moving toward the Omni-language model. (ii) We implement 10 model\nmerging algorithms on the benchmark. Furthermore, we propose a novel method\nthat removes noise from task vectors and robustly optimizes the merged vector\nbased on a loss defined over task vector interactions, achieving an average\nperformance gain of 2.48%. (iii) We find that model merging offers a promising\nway for building improved MLLMs without requiring data training. Our results\nalso demonstrate that the complementarity among multiple modalities outperforms\nindividual modalities."}
{"id": "2505.18884", "pdf": "https://arxiv.org/pdf/2505.18884", "abs": "https://arxiv.org/abs/2505.18884", "authors": ["Borna Khodabandeh", "Amirabbas Afzali", "Amirhossein Afsharrad", "Seyed Shahabeddin Mousavi", "Sanjay Lall", "Sajjad Amini", "Seyed-Mohsen Moosavi-Dezfooli"], "title": "LORE: Lagrangian-Optimized Robust Embeddings for Visual Encoders", "categories": ["cs.LG", "cs.AI", "cs.CV", "math.OC"], "comment": null, "summary": "Visual encoders have become fundamental components in modern computer vision\npipelines. However, ensuring robustness against adversarial perturbations\nremains a critical challenge. Recent efforts have explored both supervised and\nunsupervised adversarial fine-tuning strategies. We identify two key\nlimitations in these approaches: (i) they often suffer from instability,\nespecially during the early stages of fine-tuning, resulting in suboptimal\nconvergence and degraded performance on clean data, and (ii) they exhibit a\nsuboptimal trade-off between robustness and clean data accuracy, hindering the\nsimultaneous optimization of both objectives. To overcome these challenges, we\npropose Lagrangian-Optimized Robust Embeddings (LORE), a novel unsupervised\nadversarial fine-tuning framework. LORE utilizes constrained optimization,\nwhich offers a principled approach to balancing competing goals, such as\nimproving robustness while preserving nominal performance. By enforcing\nembedding-space proximity constraints, LORE effectively maintains clean data\nperformance throughout adversarial fine-tuning. Extensive experiments show that\nLORE significantly improves zero-shot adversarial robustness with minimal\ndegradation in clean data accuracy. Furthermore, we demonstrate the\neffectiveness of the adversarially fine-tuned CLIP image encoder in\nout-of-distribution generalization and enhancing the interpretability of image\nembeddings."}
{"id": "2505.19126", "pdf": "https://arxiv.org/pdf/2505.19126", "abs": "https://arxiv.org/abs/2505.19126", "authors": ["Wenyang Luo", "Wayne Xin Zhao", "Jing Sha", "Shijin Wang", "Ji-Rong Wen"], "title": "MMATH: A Multilingual Benchmark for Mathematical Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "The advent of large reasoning models, such as OpenAI o1 and DeepSeek R1, has\nsignificantly advanced complex reasoning tasks. However, their capabilities in\nmultilingual complex reasoning remain underexplored, with existing efforts\nlargely focused on simpler tasks like MGSM. To address this gap, we introduce\nMMATH, a benchmark for multilingual complex reasoning spanning 374 high-quality\nmath problems across 10 typologically diverse languages. Using MMATH, we\nobserve that even advanced models like DeepSeek R1 exhibit substantial\nperformance disparities across languages and suffer from a critical off-target\nissue-generating responses in unintended languages. To address this, we explore\nstrategies including prompting and training, demonstrating that reasoning in\nEnglish and answering in target languages can simultaneously enhance\nperformance and preserve target-language consistency. Our findings offer new\ninsights and practical strategies for advancing the multilingual reasoning\ncapabilities of large language models. Our code and data could be found at\nhttps://github.com/RUCAIBox/MMATH."}
{"id": "2505.19896", "pdf": "https://arxiv.org/pdf/2505.19896", "abs": "https://arxiv.org/abs/2505.19896", "authors": ["Alejandro Carrasco", "Victor Rodriguez-Fernandez", "Richard Linares"], "title": "Large Language Models as Autonomous Spacecraft Operators in Kerbal Space Program", "categories": ["cs.AI", "astro-ph.IM", "cs.CL"], "comment": "Non revised version for paper going to be published in Journal of\n  Advances in Space Research", "summary": "Recent trends are emerging in the use of Large Language Models (LLMs) as\nautonomous agents that take actions based on the content of the user text\nprompts. We intend to apply these concepts to the field of Control in space,\nenabling LLMs to play a significant role in the decision-making process for\nautonomous satellite operations. As a first step towards this goal, we have\ndeveloped a pure LLM-based solution for the Kerbal Space Program Differential\nGames (KSPDG) challenge, a public software design competition where\nparticipants create autonomous agents for maneuvering satellites involved in\nnon-cooperative space operations, running on the KSP game engine. Our approach\nleverages prompt engineering, few-shot prompting, and fine-tuning techniques to\ncreate an effective LLM-based agent that ranked 2nd in the competition. To the\nbest of our knowledge, this work pioneers the integration of LLM agents into\nspace research. The project comprises several open repositories to facilitate\nreplication and further research. The codebase is accessible on\n\\href{https://github.com/ARCLab-MIT/kspdg}{GitHub}, while the trained models\nand datasets are available on \\href{https://huggingface.co/OhhTuRnz}{Hugging\nFace}. Additionally, experiment tracking and detailed results can be reviewed\non \\href{https://wandb.ai/carrusk/huggingface}{Weights \\& Biases"}
{"id": "2505.18886", "pdf": "https://arxiv.org/pdf/2505.18886", "abs": "https://arxiv.org/abs/2505.18886", "authors": ["Zhendong Mi", "Qitao Tan", "Xiaodong Yu", "Zining Zhu", "Geng Yuan", "Shaoyi Huang"], "title": "KerZOO: Kernel Function Informed Zeroth-Order Optimization for Accurate and Accelerated LLM Fine-Tuning", "categories": ["cs.LG", "I.2.6; I.2.7"], "comment": "9 pages, 6 figures, Neurips", "summary": "Large language models (LLMs) have demonstrated impressive capabilities across\nnumerous NLP tasks. Nevertheless, conventional first-order fine-tuning\ntechniques impose heavy memory demands, creating practical obstacles to\nreal-world applications. Zeroth-order (ZO) optimization has recently emerged as\na promising memory-efficient alternative, as it circumvents the need for\nbackpropagation by estimating gradients solely through forward passes--making\nit particularly suitable for resource-limited environments. Despite its\nefficiency, ZO optimization suffers from gradient estimation bias, which\nsignificantly hinders convergence speed. To address this, we analytically\nidentify and characterize the lower-order bias introduced during ZO-based\ngradient estimation in LLM fine-tuning. Motivated by tools in mathematical\nphysics, we introduce a kernel-function-based ZO framework aimed at mitigating\nthis bias and improving optimization stability. KerZOO achieves comparable or\nsuperior performance to existing ZO baselines in both full-parameter and\nparameter-efficient fine-tuning settings of LLMs, while significantly reducing\nthe number of iterations required to reach convergence. For example, KerZOO\nreduces total GPU training hours by as much as 74% and 44% on WSC and MultiRC\ndatasets in fine-tuning OPT-2.7B model and can exceed the MeZO baseline by 2.9%\nand 2.6% in accuracy. We show that the kernel function is an effective avenue\nfor reducing estimation bias in ZO methods."}
{"id": "2505.19128", "pdf": "https://arxiv.org/pdf/2505.19128", "abs": "https://arxiv.org/abs/2505.19128", "authors": ["Jin Zhang", "Fan Gao", "Linyu Li", "Yongbin Yu", "Xiangxiang Wang", "Nyima Tashi", "Gadeng Luosang"], "title": "RetrieveAll: A Multilingual Named Entity Recognition Framework with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rise of large language models has led to significant performance\nbreakthroughs in named entity recognition (NER) for high-resource languages,\nyet there remains substantial room for improvement in low- and medium-resource\nlanguages. Existing multilingual NER methods face severe language interference\nduring the multi-language adaptation process, manifested in feature conflicts\nbetween different languages and the competitive suppression of low-resource\nlanguage features by high-resource languages. Although training a dedicated\nmodel for each language can mitigate such interference, it lacks scalability\nand incurs excessive computational costs in real-world applications. To address\nthis issue, we propose RetrieveAll, a universal multilingual NER framework\nbased on dynamic LoRA. The framework decouples task-specific features across\nlanguages and demonstrates efficient dynamic adaptability. Furthermore, we\nintroduce a cross-granularity knowledge augmented method that fully exploits\nthe intrinsic potential of the data without relying on external resources. By\nleveraging a hierarchical prompting mechanism to guide knowledge injection,\nthis approach advances the paradigm from \"prompt-guided inference\" to\n\"prompt-driven learning.\" Experimental results show that RetrieveAll\noutperforms existing baselines; on the PAN-X dataset, it achieves an average F1\nimprovement of 12.1 percent."}
{"id": "2505.19897", "pdf": "https://arxiv.org/pdf/2505.19897", "abs": "https://arxiv.org/abs/2505.19897", "authors": ["Qiushi Sun", "Zhoumianze Liu", "Chang Ma", "Zichen Ding", "Fangzhi Xu", "Zhangyue Yin", "Haiteng Zhao", "Zhenyu Wu", "Kanzhi Cheng", "Zhaoyang Liu", "Jianing Wang", "Qintong Li", "Xiangru Tang", "Tianbao Xie", "Xiachong Feng", "Xiang Li", "Ben Kao", "Wenhai Wang", "Biqing Qi", "Lingpeng Kong", "Zhiyong Wu"], "title": "ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "work in progress", "summary": "Large Language Models (LLMs) have extended their impact beyond Natural\nLanguage Processing, substantially fostering the development of\ninterdisciplinary research. Recently, various LLM-based agents have been\ndeveloped to assist scientific discovery progress across multiple aspects and\ndomains. Among these, computer-using agents, capable of interacting with\noperating systems as humans do, are paving the way to automated scientific\nproblem-solving and addressing routines in researchers' workflows. Recognizing\nthe transformative potential of these agents, we introduce ScienceBoard, which\nencompasses two complementary contributions: (i) a realistic, multi-domain\nenvironment featuring dynamic and visually rich scientific workflows with\nintegrated professional software, where agents can autonomously interact via\ndifferent interfaces to accelerate complex research tasks and experiments; and\n(ii) a challenging benchmark of 169 high-quality, rigorously validated\nreal-world tasks curated by humans, spanning scientific-discovery workflows in\ndomains such as biochemistry, astronomy, and geoinformatics. Extensive\nevaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude\n3.7, UI-TARS) show that, despite some promising results, they still fall short\nof reliably assisting scientists in complex workflows, achieving only a 15%\noverall success rate. In-depth analysis further provides valuable insights for\naddressing current agent limitations and more effective design principles,\npaving the way to build more capable agents for scientific discovery. Our code,\nenvironment, and benchmark are at\nhttps://qiushisun.github.io/ScienceBoard-Home/."}
{"id": "2505.18890", "pdf": "https://arxiv.org/pdf/2505.18890", "abs": "https://arxiv.org/abs/2505.18890", "authors": ["Morteza Rakhshaninejad", "Mira Jurgens", "Nicolas Dewolf", "Willem Waegeman"], "title": "Conformal Prediction for Uncertainty Estimation in Drug-Target Interaction Prediction", "categories": ["cs.LG"], "comment": null, "summary": "Accurate drug-target interaction (DTI) prediction with machine learning\nmodels is essential for drug discovery. Such models should also provide a\ncredible representation of their uncertainty, but applying classical marginal\nconformal prediction (CP) in DTI prediction often overlooks variability across\ndrug and protein subgroups. In this work, we analyze three cluster-conditioned\nCP methods for DTI prediction, and compare them with marginal and\ngroup-conditioned CP. Clusterings are obtained via nonconformity scores,\nfeature similarity, and nearest neighbors, respectively. Experiments on the\nKIBA dataset using four data-splitting strategies show that nonconformity-based\nclustering yields the tightest intervals and most reliable subgroup coverage,\nespecially in random and fully unseen drug-protein splits. Group-conditioned CP\nworks well when one entity is familiar, but residual-driven clustering provides\nrobust uncertainty estimates even in sparse or novel scenarios. These results\nhighlight the potential of cluster-based CP for improving DTI prediction under\nuncertainty."}
{"id": "2505.19147", "pdf": "https://arxiv.org/pdf/2505.19147", "abs": "https://arxiv.org/abs/2505.19147", "authors": ["Xuyang Liu", "Zichen Wen", "Shaobo Wang", "Junjie Chen", "Zhishan Tao", "Yubo Wang", "Xiangqi Jin", "Chang Zou", "Yiyu Wang", "Chenfei Liao", "Xu Zheng", "Honggang Chen", "Weijia Li", "Xuming Hu", "Conghui He", "Linfeng Zhang"], "title": "Shifting AI Efficiency From Model-Centric to Data-Centric Compression", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Project:\n  \\url{https://github.com/xuyang-liu16/Awesome-Token-level-Model-Compression}", "summary": "The rapid advancement of large language models (LLMs) and multi-modal LLMs\n(MLLMs) has historically relied on model-centric scaling through increasing\nparameter counts from millions to hundreds of billions to drive performance\ngains. However, as we approach hardware limits on model size, the dominant\ncomputational bottleneck has fundamentally shifted to the quadratic cost of\nself-attention over long token sequences, now driven by ultra-long text\ncontexts, high-resolution images, and extended videos. In this position paper,\n\\textbf{we argue that the focus of research for efficient AI is shifting from\nmodel-centric compression to data-centric compression}. We position token\ncompression as the new frontier, which improves AI efficiency via reducing the\nnumber of tokens during model training or inference. Through comprehensive\nanalysis, we first examine recent developments in long-context AI across\nvarious domains and establish a unified mathematical framework for existing\nmodel efficiency strategies, demonstrating why token compression represents a\ncrucial paradigm shift in addressing long-context overhead. Subsequently, we\nsystematically review the research landscape of token compression, analyzing\nits fundamental benefits and identifying its compelling advantages across\ndiverse scenarios. Furthermore, we provide an in-depth analysis of current\nchallenges in token compression research and outline promising future\ndirections. Ultimately, our work aims to offer a fresh perspective on AI\nefficiency, synthesize existing research, and catalyze innovative developments\nto address the challenges that increasing context lengths pose to the AI\ncommunity's advancement."}
{"id": "2505.19905", "pdf": "https://arxiv.org/pdf/2505.19905", "abs": "https://arxiv.org/abs/2505.19905", "authors": ["Shuang Ao", "Flora D. Salim", "Simon Khan"], "title": "EMAC+: Embodied Multimodal Agent for Collaborative Planning with VLM+LLM", "categories": ["cs.AI"], "comment": null, "summary": "Although LLMs demonstrate proficiency in several text-based reasoning and\nplanning tasks, their implementation in robotics control is constrained by\nsignificant deficiencies: (1) LLM agents are designed to work mainly with\ntextual inputs rather than visual conditions; (2) Current multimodal agents\ntreat LLMs as static planners, which separates their reasoning from environment\ndynamics, resulting in actions that do not take domain-specific knowledge into\naccount; and (3) LLMs are not designed to learn from visual interactions, which\nmakes it harder for them to make better policies for specific domains. In this\npaper, we introduce EMAC+, an Embodied Multimodal Agent that collaboratively\nintegrates LLM and VLM via a bidirectional training paradigm. Unlike existing\nmethods, EMAC+ dynamically refines high-level textual plans generated by an LLM\nusing real-time feedback from a VLM executing low-level visual control tasks.\nWe address critical limitations of previous models by enabling the LLM to\ninternalize visual environment dynamics directly through interactive\nexperience, rather than relying solely on static symbolic mappings. Extensive\nexperimental evaluations on ALFWorld and RT-1 benchmarks demonstrate that EMAC+\nachieves superior task performance, robustness against noisy observations, and\nefficient learning. We also conduct thorough ablation studies and provide\ndetailed analyses of success and failure cases."}
{"id": "2505.18901", "pdf": "https://arxiv.org/pdf/2505.18901", "abs": "https://arxiv.org/abs/2505.18901", "authors": ["Xiaoyan Hu", "Lauren Pick", "Ho-fung Leung", "Farzan Farnia"], "title": "PromptWise: Online Learning for Cost-Aware Prompt Assignment in Generative Models", "categories": ["cs.LG", "cs.AI"], "comment": "44 pages", "summary": "The rapid advancement of generative AI models has provided users with\nnumerous options to address their prompts. When selecting a generative AI model\nfor a given prompt, users should consider not only the performance of the\nchosen model but also its associated service cost. The principle guiding such\nconsideration is to select the least expensive model among the available\nsatisfactory options. However, existing model-selection approaches typically\nprioritize performance, overlooking pricing differences between models. In this\npaper, we introduce PromptWise, an online learning framework designed to assign\na sequence of prompts to a group of large language models (LLMs) in a\ncost-effective manner. PromptWise strategically queries cheaper models first,\nprogressing to more expensive options only if the lower-cost models fail to\nadequately address a given prompt. Through numerical experiments, we\ndemonstrate PromptWise's effectiveness across various tasks, including puzzles\nof varying complexity and code generation/translation tasks. The results\nhighlight that PromptWise consistently outperforms cost-unaware baseline\nmethods, emphasizing that directly assigning prompts to the most expensive\nmodels can lead to higher costs and potentially lower average performance."}
{"id": "2505.19163", "pdf": "https://arxiv.org/pdf/2505.19163", "abs": "https://arxiv.org/abs/2505.19163", "authors": ["Firoj Alam", "Md Arid Hasan", "Shammur Absar Chowdhury"], "title": "SpokenNativQA: Multilingual Everyday Spoken Queries for LLMs", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": "Spoken Question Answering, Multilingual LLMs, Speech-based\n  Evaluation, Dialectal Speech, Low-resource Languages, Multimodal\n  Benchmarking, Conversational AI, Speech-to-Text QA, Real-world Interaction,\n  Natural Language Understanding", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious disciplines and tasks. However, benchmarking their capabilities with\nmultilingual spoken queries remains largely unexplored. In this study, we\nintroduce SpokenNativQA, the first multilingual and culturally aligned spoken\nquestion-answering (SQA) dataset designed to evaluate LLMs in real-world\nconversational settings. The dataset comprises approximately 33,000 naturally\nspoken questions and answers in multiple languages, including low-resource and\ndialect-rich languages, providing a robust benchmark for assessing LLM\nperformance in speech-based interactions. SpokenNativQA addresses the\nlimitations of text-based QA datasets by incorporating speech variability,\naccents, and linguistic diversity. We benchmark different ASR systems and LLMs\nfor SQA and present our findings. We released the data at\n(https://huggingface.co/datasets/QCRI/SpokenNativQA) and the experimental\nscripts at (https://llmebench.qcri.org/) for the research community."}
{"id": "2505.19927", "pdf": "https://arxiv.org/pdf/2505.19927", "abs": "https://arxiv.org/abs/2505.19927", "authors": ["Zifeng Ding", "Sikuan Yan", "Zhangdie Yuan", "Xianglong Hu", "Fangru Lin", "Andreas Vlachos"], "title": "TCP: a Benchmark for Temporal Constraint-Based Planning", "categories": ["cs.AI"], "comment": null, "summary": "Temporal reasoning and planning are essential capabilities for large language\nmodels (LLMs), yet most existing benchmarks evaluate them in isolation and\nunder limited forms of complexity. To address this gap, we introduce the\nTemporal Constraint-based Planning (TCP) benchmark, that jointly assesses both\ncapabilities. Each instance in TCP features a naturalistic dialogue around a\ncollaborative project, where diverse and interdependent temporal constraints\nare explicitly or implicitly expressed, and models must infer an optimal\nschedule that satisfies all constraints. To construct TCP, we first generate\nabstract problem prototypes that are paired with realistic scenarios from\nvarious domains and enriched into dialogues using an LLM. A human quality check\nis performed on a sampled subset to confirm the reliability of our benchmark.\nWe evaluate state-of-the-art LLMs and find that even the strongest models\nstruggle with TCP, highlighting its difficulty and revealing limitations in\nLLMs' temporal constraint-based planning abilities. We analyze underlying\nfailure cases, open source our benchmark, and hope our findings can inspire\nfuture research."}
{"id": "2505.18917", "pdf": "https://arxiv.org/pdf/2505.18917", "abs": "https://arxiv.org/abs/2505.18917", "authors": ["Zhepeng Cen", "Yihang Yao", "William Han", "Zuxin Liu", "Ding Zhao"], "title": "Behavior Injection: Preparing Language Models for Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement fine-tuning (RFT) has emerged as a powerful post-training\ntechnique to incentivize the reasoning ability of large language models (LLMs).\nHowever, LLMs can respond very inconsistently to RFT: some show substantial\nperformance gains, while others plateau or even degrade. To understand this\ndivergence, we analyze the per-step influence of the RL objective and identify\ntwo key conditions for effective post-training: (1) RL-informative rollout\naccuracy, and (2) strong data co-influence, which quantifies how much the\ntraining data affects performance on other samples. Guided by these insights,\nwe propose behavior injection, a task-agnostic data-augmentation scheme applied\nprior to RL. Behavior injection enriches the supervised finetuning (SFT) data\nby seeding exploratory and exploitative behaviors, effectively making the model\nmore RL-ready. We evaluate our method across two reasoning benchmarks with\nmultiple base models. The results demonstrate that our theoretically motivated\naugmentation can significantly increases the performance gain from RFT over the\npre-RL model."}
{"id": "2505.19176", "pdf": "https://arxiv.org/pdf/2505.19176", "abs": "https://arxiv.org/abs/2505.19176", "authors": ["Zhuo Liu", "Moxin Li", "Xun Deng", "Qifan Wang", "Fuli Feng"], "title": "Assistant-Guided Mitigation of Teacher Preference Bias in LLM-as-a-Judge", "categories": ["cs.CL"], "comment": "Under review", "summary": "LLM-as-a-Judge employs large language models (LLMs), such as GPT-4, to\nevaluate the quality of LLM-generated responses, gaining popularity for its\ncost-effectiveness and strong alignment with human evaluations. However,\ntraining proxy judge models using evaluation data generated by powerful teacher\nmodels introduces a critical yet previously overlooked issue: teacher\npreference bias, where the proxy judge model learns a biased preference for\nresponses from the teacher model. To tackle this problem, we propose a novel\nsetting that incorporates an additional assistant model, which is not biased\ntoward the teacher model's responses, to complement the training data. Building\non this setup, we introduce AGDe-Judge, a three-stage framework designed to\ndebias from both the labels and feedbacks in the training data. Extensive\nexperiments demonstrate that AGDe-Judge effectively reduces teacher preference\nbias while maintaining strong performance across six evaluation benchmarks.\nCode is available at https://github.com/Liuz233/AGDe-Judge."}
{"id": "2505.19933", "pdf": "https://arxiv.org/pdf/2505.19933", "abs": "https://arxiv.org/abs/2505.19933", "authors": ["Yejin Son", "Minseo Kim", "Sungwoong Kim", "Seungju Han", "Jian Kim", "Dongju Jang", "Youngjae Yu", "Chanyoung Park"], "title": "Subtle Risks, Critical Failures: A Framework for Diagnosing Physical Safety of LLMs for Embodied Decision Making", "categories": ["cs.AI"], "comment": "37 pages, 13 tables, 6 figures", "summary": "Large Language Models (LLMs) are increasingly used for decision making in\nembodied agents, yet existing safety evaluations often rely on coarse success\nrates and domain-specific setups, making it difficult to diagnose why and where\nthese models fail. This obscures our understanding of embodied safety and\nlimits the selective deployment of LLMs in high-risk physical environments. We\nintroduce SAFEL, the framework for systematically evaluating the physical\nsafety of LLMs in embodied decision making. SAFEL assesses two key\ncompetencies: (1) rejecting unsafe commands via the Command Refusal Test, and\n(2) generating safe and executable plans via the Plan Safety Test. Critically,\nthe latter is decomposed into functional modules, goal interpretation,\ntransition modeling, action sequencing, enabling fine-grained diagnosis of\nsafety failures. To support this framework, we introduce EMBODYGUARD, a\nPDDL-grounded benchmark containing 942 LLM-generated scenarios covering both\novertly malicious and contextually hazardous instructions. Evaluation across 13\nstate-of-the-art LLMs reveals that while models often reject clearly unsafe\ncommands, they struggle to anticipate and mitigate subtle, situational risks.\nOur results highlight critical limitations in current LLMs and provide a\nfoundation for more targeted, modular improvements in safe embodied reasoning."}
{"id": "2505.18923", "pdf": "https://arxiv.org/pdf/2505.18923", "abs": "https://arxiv.org/abs/2505.18923", "authors": ["Yile Li", "Shandian Zhe"], "title": "Graph-Based Operator Learning from Limited Data on Irregular Domains", "categories": ["cs.LG"], "comment": null, "summary": "Operator learning seeks to approximate mappings from input functions to\noutput solutions, particularly in the context of partial differential equations\n(PDEs). While recent advances such as DeepONet and Fourier Neural Operator\n(FNO) have demonstrated strong performance, they often rely on regular grid\ndiscretizations, limiting their applicability to complex or irregular domains.\nIn this work, we propose a Graph-based Operator Learning with Attention (GOLA)\nframework that addresses this limitation by constructing graphs from\nirregularly sampled spatial points and leveraging attention-enhanced Graph\nNeural Netwoks (GNNs) to model spatial dependencies with global information. To\nimprove the expressive capacity, we introduce a Fourier-based encoder that\nprojects input functions into a frequency space using learnable complex\ncoefficients, allowing for flexible embeddings even with sparse or nonuniform\nsamples. We evaluated our approach across a range of 2D PDEs, including Darcy\nFlow, Advection, Eikonal, and Nonlinear Diffusion, under varying sampling\ndensities. Our method consistently outperforms baselines, particularly in\ndata-scarce regimes, demonstrating strong generalization and efficiency on\nirregular domains."}
{"id": "2505.19184", "pdf": "https://arxiv.org/pdf/2505.19184", "abs": "https://arxiv.org/abs/2505.19184", "authors": ["Minh Nhat Nguyen", "Pradyumna Shyama Prasad"], "title": "Two LLMs debate, both are certain they've won", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Can LLMs accurately adjust their confidence when facing opposition? Building\non previous studies measuring calibration on static fact-based\nquestion-answering tasks, we evaluate Large Language Models (LLMs) in a\ndynamic, adversarial debate setting, uniquely combining two realistic factors:\n(a) a multi-turn format requiring models to update beliefs as new information\nemerges, and (b) a zero-sum structure to control for task-related uncertainty,\nsince mutual high-confidence claims imply systematic overconfidence. We\norganized 60 three-round policy debates among ten state-of-the-art LLMs, with\nmodels privately rating their confidence (0-100) in winning after each round.\nWe observed five concerning patterns: (1) Systematic overconfidence: models\nbegan debates with average initial confidence of 72.9% vs. a rational 50%\nbaseline. (2) Confidence escalation: rather than reducing confidence as debates\nprogressed, debaters increased their win probabilities, averaging 83% by the\nfinal round. (3) Mutual overestimation: in 61.7% of debates, both sides\nsimultaneously claimed >=75% probability of victory, a logical impossibility.\n(4) Persistent self-debate bias: models debating identical copies increased\nconfidence from 64.1% to 75.2%; even when explicitly informed their chance of\nwinning was exactly 50%, confidence still rose (from 50.0% to 57.1%). (5)\nMisaligned private reasoning: models' private scratchpad thoughts sometimes\ndiffered from their public confidence ratings, raising concerns about\nfaithfulness of chain-of-thought reasoning. These results suggest LLMs lack the\nability to accurately self-assess or update their beliefs in dynamic,\nmulti-turn tasks; a major concern as LLM outputs are deployed without careful\nreview in assistant roles or agentic settings."}
{"id": "2505.19956", "pdf": "https://arxiv.org/pdf/2505.19956", "abs": "https://arxiv.org/abs/2505.19956", "authors": ["Jihyung Lee", "Jin-Seop Lee", "Jaehoon Lee", "YunSeok Choi", "Jee-Hyong Lee"], "title": "DCG-SQL: Enhancing In-Context Learning for Text-to-SQL with Deep Contextual Schema Link Graph", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Text-to-SQL, which translates a natural language question into an SQL query,\nhas advanced with in-context learning of Large Language Models (LLMs). However,\nexisting methods show little improvement in performance compared to randomly\nchosen demonstrations, and significant performance drops when smaller LLMs\n(e.g., Llama 3.1-8B) are used. This indicates that these methods heavily rely\non the intrinsic capabilities of hyper-scaled LLMs, rather than effectively\nretrieving useful demonstrations. In this paper, we propose a novel approach\nfor effectively retrieving demonstrations and generating SQL queries. We\nconstruct a Deep Contextual Schema Link Graph, which contains key information\nand semantic relationship between a question and its database schema items.\nThis graph-based structure enables effective representation of Text-to-SQL\nsamples and retrieval of useful demonstrations for in-context learning.\nExperimental results on the Spider benchmark demonstrate the effectiveness of\nour approach, showing consistent improvements in SQL generation performance and\nefficiency across both hyper-scaled LLMs and small LLMs. Our code will be\nreleased."}
{"id": "2505.18926", "pdf": "https://arxiv.org/pdf/2505.18926", "abs": "https://arxiv.org/abs/2505.18926", "authors": ["Jingxuan Xu", "Hong Huang", "Chuhang Zou", "Manolis Savva", "Yunchao Wei", "Wuyang Chen"], "title": "Hybrid Neural-MPM for Interactive Fluid Simulations in Real-Time", "categories": ["cs.LG", "physics.flu-dyn"], "comment": null, "summary": "We propose a neural physics system for real-time, interactive fluid\nsimulations. Traditional physics-based methods, while accurate, are\ncomputationally intensive and suffer from latency issues. Recent\nmachine-learning methods reduce computational costs while preserving fidelity;\nyet most still fail to satisfy the latency constraints for real-time use and\nlack support for interactive applications. To bridge this gap, we introduce a\nnovel hybrid method that integrates numerical simulation, neural physics, and\ngenerative control. Our neural physics jointly pursues low-latency simulation\nand high physical fidelity by employing a fallback safeguard to classical\nnumerical solvers. Furthermore, we develop a diffusion-based controller that is\ntrained using a reverse modeling strategy to generate external dynamic force\nfields for fluid manipulation. Our system demonstrates robust performance\nacross diverse 2D/3D scenarios, material types, and obstacle interactions,\nachieving real-time simulations at high frame rates (11~29% latency) while\nenabling fluid control guided by user-friendly freehand sketches. We present a\nsignificant step towards practical, controllable, and physically plausible\nfluid simulations for real-time interactive applications. We promise to release\nboth models and data upon acceptance."}
{"id": "2505.19187", "pdf": "https://arxiv.org/pdf/2505.19187", "abs": "https://arxiv.org/abs/2505.19187", "authors": ["Yang Xiao", "Jiashuo Wang", "Ruifeng Yuan", "Chunpu Xu", "Kaishuai Xu", "Wenjie Li", "Pengfei Liu"], "title": "LIMOPro: Reasoning Refinement for Efficient and Effective Test-time Scaling", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities through test-time scaling approaches, particularly when fine-tuned\nwith chain-of-thought (CoT) data distilled from more powerful large reasoning\nmodels (LRMs). However, these reasoning chains often contain verbose elements\nthat mirror human problem-solving, categorized as progressive reasoning (the\nessential solution development path) and functional elements (verification\nprocesses, alternative solution approaches, and error corrections). While\nprogressive reasoning is crucial, the functional elements significantly\nincrease computational demands during test-time inference. We introduce PIR\n(Perplexity-based Importance Refinement), a principled framework that\nquantitatively evaluates the importance of each reasoning step based on its\nimpact on answer prediction confidence. PIR systematically identifies and\nselectively prunes only low-importance functional steps while preserving\nprogressive reasoning components, creating optimized training data that\nmaintains the integrity of the core solution path while reducing verbosity.\nModels fine-tuned on PIR-optimized data exhibit superior test-time scaling\nproperties, generating more concise reasoning chains while achieving improved\naccuracy (+0.9\\% to +6.6\\%) with significantly reduced token usage (-3\\% to\n-41\\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond).\nOur approach demonstrates strong generalizability across different model sizes,\ndata sources, and token budgets, offering a practical solution for deploying\nreasoning-capable LLMs in scenarios where efficient test-time scaling, response\ntime, and computational efficiency are valuable constraints."}
{"id": "2505.19965", "pdf": "https://arxiv.org/pdf/2505.19965", "abs": "https://arxiv.org/abs/2505.19965", "authors": ["Yu Wang", "Junshu Dai", "Yuchen Ying", "Yuxuan Liang", "Tongya Zheng", "Mingli Song"], "title": "Adaptive Location Hierarchy Learning for Long-Tailed Mobility Prediction", "categories": ["cs.AI"], "comment": null, "summary": "Human mobility prediction is crucial for applications ranging from\nlocation-based recommendations to urban planning, which aims to forecast users'\nnext location visits based on historical trajectories. Despite the severe\nlong-tailed distribution of locations, the problem of long-tailed mobility\nprediction remains largely underexplored. Existing long-tailed learning methods\nprimarily focus on rebalancing the skewed distribution at the data, model, or\nclass level, neglecting to exploit the spatiotemporal semantics of locations.\nTo address this gap, we propose the first plug-and-play framework for\nlong-tailed mobility prediction in an exploitation and exploration manner,\nnamed \\textbf{A}daptive \\textbf{LO}cation \\textbf{H}ier\\textbf{A}rchy learning\n(ALOHA). First, we construct city-tailored location hierarchy based on Large\nLanguage Models (LLMs) by exploiting Maslow's theory of human motivation to\ndesign Chain-of-Thought (CoT) prompts that captures spatiotemporal semantics.\nSecond, we optimize the location hierarchy predictions by Gumbel disturbance\nand node-wise adaptive weights within the hierarchical tree structure.\nExperiments on state-of-the-art models across six datasets demonstrate the\nframework's consistent effectiveness and generalizability, which strikes a well\nbalance between head and tail locations. Weight analysis and ablation studies\nreveal the optimization differences of each component for head and tail\nlocations. Furthermore, in-depth analyses of hierarchical distance and case\nstudy demonstrate the effective semantic guidance from the location hierarchy.\nOur code will be made publicly available."}
{"id": "2505.18934", "pdf": "https://arxiv.org/pdf/2505.18934", "abs": "https://arxiv.org/abs/2505.18934", "authors": ["Xiping Li", "Xiangyu Dong", "Xingyi Zhang", "Kun Xie", "Yuanhao Feng", "Bo Wang", "Guilin Li", "Wuxiong Zeng", "Xiujun Shu", "Sibo Wang"], "title": "Chi-Square Wavelet Graph Neural Networks for Heterogeneous Graph Anomaly Detection", "categories": ["cs.LG", "cs.AI", "cs.IR", "cs.SI"], "comment": null, "summary": "Graph Anomaly Detection (GAD) in heterogeneous networks presents unique\nchallenges due to node and edge heterogeneity. Existing Graph Neural Network\n(GNN) methods primarily focus on homogeneous GAD and thus fail to address three\nkey issues: (C1) Capturing abnormal signal and rich semantics across diverse\nmeta-paths; (C2) Retaining high-frequency content in HIN dimension alignment;\nand (C3) Learning effectively from difficult anomaly samples with class\nimbalance. To overcome these, we propose ChiGAD, a spectral GNN framework based\non a novel Chi-Square filter, inspired by the wavelet effectiveness in diverse\ndomains. Specifically, ChiGAD consists of: (1) Multi-Graph Chi-Square Filter,\nwhich captures anomalous information via applying dedicated Chi-Square filters\nto each meta-path graph; (2) Interactive Meta-Graph Convolution, which aligns\nfeatures while preserving high-frequency information and incorporates\nheterogeneous messages by a unified Chi-Square Filter; and (3)\nContribution-Informed Cross-Entropy Loss, which prioritizes difficult anomalies\nto address class imbalance. Extensive experiments on public and industrial\ndatasets show that ChiGAD outperforms state-of-the-art models on multiple\nmetrics. Additionally, its homogeneous variant, ChiGNN, excels on seven GAD\ndatasets, validating the effectiveness of Chi-Square filters. Our code is\navailable at https://github.com/HsipingLi/ChiGAD."}
{"id": "2505.19191", "pdf": "https://arxiv.org/pdf/2505.19191", "abs": "https://arxiv.org/abs/2505.19191", "authors": ["Nursulu Sagimbayeva", "Ruveyda Betl Baheci", "Ingmar Weber"], "title": "Misleading through Inconsistency: A Benchmark for Political Inconsistencies Detection", "categories": ["cs.CL"], "comment": "8 pages, 6 figures. Accepted for publication in the Proceedings of\n  1st Workshop on Misinformation Detection in the Era of LLMs (MisD) at\n  ICWSM-2025", "summary": "Inconsistent political statements represent a form of misinformation. They\nerode public trust and pose challenges to accountability, when left unnoticed.\nDetecting inconsistencies automatically could support journalists in asking\nclarification questions, thereby helping to keep politicians accountable. We\npropose the Inconsistency detection task and develop a scale of inconsistency\ntypes to prompt NLP-research in this direction. To provide a resource for\ndetecting inconsistencies in a political domain, we present a dataset of 698\nhuman-annotated pairs of political statements with explanations of the\nannotators' reasoning for 237 samples. The statements mainly come from voting\nassistant platforms such as Wahl-O-Mat in Germany and Smartvote in Switzerland,\nreflecting real-world political issues. We benchmark Large Language Models\n(LLMs) on our dataset and show that in general, they are as good as humans at\ndetecting inconsistencies, and might be even better than individual humans at\npredicting the crowd-annotated ground-truth. However, when it comes to\nidentifying fine-grained inconsistency types, none of the model have reached\nthe upper bound of performance (due to natural labeling variation), thus\nleaving room for improvement. We make our dataset and code publicly available."}
{"id": "2505.20011", "pdf": "https://arxiv.org/pdf/2505.20011", "abs": "https://arxiv.org/abs/2505.20011", "authors": ["Maciej wiechowski", "Dominik lzak"], "title": "The Many Challenges of Human-Like Agents in Virtual Game Environments", "categories": ["cs.AI", "cs.HC", "cs.MM", "68T01", "I.2; I.6.0; H.1.2"], "comment": "In proceedings of the 24th International Conference on Autonomous\n  Agents and Multiagent Systems (AAMAS-2025), pages 1996--2005, May 19-23,\n  Detroit, Michigan, USA", "summary": "Human-like agents are an increasingly important topic in games and beyond.\nBelievable non-player characters enhance the gaming experience by improving\nimmersion and providing entertainment. They also offer players the opportunity\nto engage with AI entities that can function as opponents, teachers, or\ncooperating partners. Additionally, in games where bots are prohibited -- and\neven more so in non-game environments -- there is a need for methods capable of\nidentifying whether digital interactions occur with bots or humans. This leads\nto two fundamental research questions: (1) how to model and implement\nhuman-like AI, and (2) how to measure its degree of human likeness.\n  This article offers two contributions. The first one is a survey of the most\nsignificant challenges in implementing human-like AI in games (or any virtual\nenvironment featuring simulated agents, although this article specifically\nfocuses on games). Thirteen such challenges, both conceptual and technical, are\ndiscussed in detail. The second is an empirical study performed in a tactical\nvideo game that addresses the research question: \"Is it possible to distinguish\nhuman players from bots (AI agents) based on empirical data?\" A\nmachine-learning approach using a custom deep recurrent convolutional neural\nnetwork is presented. We hypothesize that the more challenging it is to create\nhuman-like AI for a given game, the easier it becomes to develop a method for\ndistinguishing humans from AI-driven players."}
{"id": "2505.18948", "pdf": "https://arxiv.org/pdf/2505.18948", "abs": "https://arxiv.org/abs/2505.18948", "authors": ["William Merrill", "Ashish Sabharwal"], "title": "Exact Expressive Power of Transformers with Padding", "categories": ["cs.LG", "cs.CC", "cs.FL"], "comment": null, "summary": "Chain of thought is a natural inference-time method for increasing the\ncomputational power of transformer-based large language models (LLMs), but\ncomes at the cost of sequential decoding. Are there more efficient alternatives\nto expand a transformer's expressive power without adding parameters? We\nconsider transformers with padding tokens as a form of parallelizable test-time\ncompute. We show that averaging-hard-attention, masked-pre-norm transformers\nwith polynomial padding converge to precisely the class $\\mathsf{TC}^0$ of\nextremely parallelizable problems. While the $\\mathsf{TC}^0$ upper bound was\nknown, proving a matching lower bound had been elusive. Further, our novel\nanalysis reveals the precise expanded power of padded transformers when coupled\nwith another form of inference-time compute, namely dynamically increasing\ndepth via looping. Our core technical contribution is to show how padding helps\nbring the notions of complete problems and reductions, which have been a\ncornerstone of classical complexity theory, to the formal study of\ntransformers. Armed with this new tool, we prove that padded transformers with\n$O(\\log^d n)$ looping on inputs of length $n$ recognize exactly the class\n$\\mathsf{TC}^d$ of moderately parallelizable problems. Thus, padding and\nlooping together systematically expand transformers' expressive power: with\npolylogarithmic looping, padded transformers converge to the class\n$\\mathsf{NC}$, the best that could be expected without losing parallelism\n(unless $\\mathsf{NC} = \\mathsf{P}$). Our results thus motivate further\nexploration of padding and looping as parallelizable alternatives to chain of\nthought."}
{"id": "2505.19201", "pdf": "https://arxiv.org/pdf/2505.19201", "abs": "https://arxiv.org/abs/2505.19201", "authors": ["Yunhai Hu", "Tianhua Xia", "Zining Liu", "Rahul Raman", "Xingyu Liu", "Bo Bao", "Eric Sather", "Vithursan Thangarasa", "Sai Qian Zhang"], "title": "DREAM: Drafting with Refined Target Features and Entropy-Adaptive Cross-Attention Fusion for Multimodal Speculative Decoding", "categories": ["cs.CL"], "comment": null, "summary": "Speculative decoding (SD) has emerged as a powerful method for accelerating\nautoregressive generation in large language models (LLMs), yet its integration\ninto vision-language models (VLMs) remains underexplored. We introduce DREAM, a\nnovel speculative decoding framework tailored for VLMs that combines three key\ninnovations: (1) a cross-attention-based mechanism to inject intermediate\nfeatures from the target model into the draft model for improved alignment, (2)\nadaptive intermediate feature selection based on attention entropy to guide\nefficient draft model training, and (3) visual token compression to reduce\ndraft model latency. DREAM enables efficient, accurate, and parallel multimodal\ndecoding with significant throughput improvement. Experiments across a diverse\nset of recent popular VLMs, including LLaVA, Pixtral, SmolVLM and Gemma3,\ndemonstrate up to 3.6x speedup over conventional decoding and significantly\noutperform prior SD baselines in both inference throughput and speculative\ndraft acceptance length across a broad range of multimodal benchmarks. The code\nis publicly available at: https://github.com/SAI-Lab-NYU/DREAM.git"}
{"id": "2505.20075", "pdf": "https://arxiv.org/pdf/2505.20075", "abs": "https://arxiv.org/abs/2505.20075", "authors": ["Mengdi Li", "Jiaye Lin", "Xufeng Zhao", "Wenhao Lu", "Peilin Zhao", "Stefan Wermter", "Di Wang"], "title": "Curriculum-RLAIF: Curriculum Alignment with Reinforcement Learning from AI Feedback", "categories": ["cs.AI"], "comment": null, "summary": "Reward models trained with conventional Reinforcement Learning from AI\nFeedback (RLAIF) methods suffer from limited generalizability, which hinders\nthe alignment performance of the policy model during reinforcement learning\n(RL). This challenge stems from various issues, including distribution shift,\npreference label noise, and mismatches between overly challenging samples and\nmodel capacity. In this paper, we attempt to enhance the generalizability of\nreward models through a data-centric approach, driven by the insight that these\nissues are inherently intertwined from the perspective of data difficulty. To\naddress this, we propose a novel framework, $\\textit{Curriculum-RLAIF}$, which\nconstructs preference pairs with varying difficulty levels and produces a\ncurriculum that progressively incorporates preference pairs of increasing\ndifficulty for reward model training. Our experimental results suggest that\nreward models trained with Curriculum-RLAIF achieve improved generalizability,\nsignificantly increasing the alignment performance of the policy model by a\nlarge margin without incurring additional inference costs compared to various\nnon-curriculum baselines. Detailed analysis and comparisons with alternative\napproaches, including data selection via external pretrained reward models or\ninternal self-selection mechanisms, as well as other curriculum strategies,\nfurther demonstrate the superiority of our approach in terms of simplicity,\nefficiency, and effectiveness."}
{"id": "2505.18952", "pdf": "https://arxiv.org/pdf/2505.18952", "abs": "https://arxiv.org/abs/2505.18952", "authors": ["Chen Jia"], "title": "Online Knowledge Distillation with Reward Guidance", "categories": ["cs.LG"], "comment": null, "summary": "This work studies knowledge distillation (KD) for large language models\n(LLMs) through preference optimization. We propose a reward-guided imitation\nlearning framework for sequential KD, formulating a min-max optimization\nproblem between the policy and reward model (RM) to minimize the performance\ngap between the student and teacher policies. Specifically, the reward\noptimization is constrained to achieve near-optimality within a confidence set\nfor preference alignment. For preference data construction, we explore both\noffline and online preference-based KD. Additionally, we reformulate the RM\nusing the $Q$-value function and extend the framework to white-box KD, where\nthe teacher policy's predicted probabilities are accessible. Theoretical\nanalysis and empirical results demonstrate the effectiveness of the proposed\nframework."}
{"id": "2505.19206", "pdf": "https://arxiv.org/pdf/2505.19206", "abs": "https://arxiv.org/abs/2505.19206", "authors": ["Richard He Bai", "Zijin Gu", "Tatiana Likhomanenko", "Navdeep Jaitly"], "title": "SpeakStream: Streaming Text-to-Speech with Interleaved Data", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "The latency bottleneck of traditional text-to-speech (TTS) systems\nfundamentally hinders the potential of streaming large language models (LLMs)\nin conversational AI. These TTS systems, typically trained and inferenced on\ncomplete utterances, introduce unacceptable delays, even with optimized\ninference speeds, when coupled with streaming LLM outputs. This is particularly\nproblematic for creating responsive conversational agents where low first-token\nlatency is critical. In this paper, we present SpeakStream, a streaming TTS\nsystem that generates audio incrementally from streaming text using a\ndecoder-only architecture. SpeakStream is trained using a next-step prediction\nloss on interleaved text-speech data. During inference, it generates speech\nincrementally while absorbing streaming input text, making it particularly\nsuitable for cascaded conversational AI agents where an LLM streams text to a\nTTS system. Our experiments demonstrate that SpeakStream achieves\nstate-of-the-art latency results in terms of first-token latency while\nmaintaining the quality of non-streaming TTS systems."}
{"id": "2505.20087", "pdf": "https://arxiv.org/pdf/2505.20087", "abs": "https://arxiv.org/abs/2505.20087", "authors": ["Makesh Narsimhan Sreedhar", "Traian Rebedea", "Christopher Parisien"], "title": "Safety Through Reasoning: An Empirical Study of Reasoning Guardrail Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning-based language models have demonstrated strong performance across\nvarious domains, with the most notable gains seen in mathematical and coding\ntasks. Recent research has shown that reasoning also offers significant\nbenefits for LLM safety and guardrail applications. In this work, we conduct a\ncomprehensive analysis of training reasoning-based guardrail models for content\nmoderation, with an emphasis on generalization to custom safety policies at\ninference time. Our study focuses on two key dimensions: data efficiency and\ninference efficiency. On the data front, we find that reasoning-based models\nexhibit strong sample efficiency, achieving competitive performance with\nsignificantly fewer training examples than their non-reasoning counterparts.\nThis unlocks the potential to repurpose the remaining data for mining\nhigh-value, difficult samples that further enhance model performance. On the\ninference side, we evaluate practical trade-offs by introducing reasoning\nbudgets, examining the impact of reasoning length on latency and accuracy, and\nexploring dual-mode training to allow runtime control over reasoning behavior.\nOur findings will provide practical insights for researchers and developers to\neffectively and efficiently train and deploy reasoning-based guardrails models\nin real-world systems."}
{"id": "2505.18966", "pdf": "https://arxiv.org/pdf/2505.18966", "abs": "https://arxiv.org/abs/2505.18966", "authors": ["Nuowei Liu", "Jiahao Kuang", "Yanting Liu", "Changzhi Sun", "Tao Ji", "Yuanbin Wu", "Man Lan"], "title": "Protein Design with Dynamic Protein Vocabulary", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "comment": null, "summary": "Protein design is a fundamental challenge in biotechnology, aiming to design\nnovel sequences with specific functions within the vast space of possible\nproteins. Recent advances in deep generative models have enabled function-based\nprotein design from textual descriptions, yet struggle with structural\nplausibility. Inspired by classical protein design methods that leverage\nnatural protein structures, we explore whether incorporating fragments from\nnatural proteins can enhance foldability in generative models. Our empirical\nresults show that even random incorporation of fragments improves foldability.\nBuilding on this insight, we introduce ProDVa, a novel protein design approach\nthat integrates a text encoder for functional descriptions, a protein language\nmodel for designing proteins, and a fragment encoder to dynamically retrieve\nprotein fragments based on textual functional descriptions. Experimental\nresults demonstrate that our approach effectively designs protein sequences\nthat are both functionally aligned and structurally plausible. Compared to\nstate-of-the-art models, ProDVa achieves comparable function alignment using\nless than 0.04% of the training data, while designing significantly more\nwell-folded proteins, with the proportion of proteins having pLDDT above 70\nincreasing by 7.38% and those with PAE below 10 increasing by 9.6%."}
{"id": "2505.19209", "pdf": "https://arxiv.org/pdf/2505.19209", "abs": "https://arxiv.org/abs/2505.19209", "authors": ["Zonglin Yang", "Wanhao Liu", "Ben Gao", "Yujie Liu", "Wei Li", "Tong Xie", "Lidong Bing", "Wanli Ouyang", "Erik Cambria", "Dongzhan Zhou"], "title": "MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search", "categories": ["cs.CL", "cs.AI", "cs.CE", "stat.ML"], "comment": null, "summary": "Large language models (LLMs) have shown promise in automating scientific\nhypothesis generation, yet existing approaches primarily yield coarse-grained\nhypotheses lacking critical methodological and experimental details. We\nintroduce and formally define the novel task of fine-grained scientific\nhypothesis discovery, which entails generating detailed, experimentally\nactionable hypotheses from coarse initial research directions. We frame this as\na combinatorial optimization problem and investigate the upper limits of LLMs'\ncapacity to solve it when maximally leveraged. Specifically, we explore four\nfoundational questions: (1) how to best harness an LLM's internal heuristics to\nformulate the fine-grained hypothesis it itself would judge as the most\npromising among all the possible hypotheses it might generate, based on its own\ninternal scoring-thus defining a latent reward landscape over the hypothesis\nspace; (2) whether such LLM-judged better hypotheses exhibit stronger alignment\nwith ground-truth hypotheses; (3) whether shaping the reward landscape using an\nensemble of diverse LLMs of similar capacity yields better outcomes than\ndefining it with repeated instances of the strongest LLM among them; and (4)\nwhether an ensemble of identical LLMs provides a more reliable reward landscape\nthan a single LLM. To address these questions, we propose a hierarchical search\nmethod that incrementally proposes and integrates details into the hypothesis,\nprogressing from general concepts to specific experimental configurations. We\nshow that this hierarchical process smooths the reward landscape and enables\nmore effective optimization. Empirical evaluations on a new benchmark of\nexpert-annotated fine-grained hypotheses from recent chemistry literature show\nthat our method consistently outperforms strong baselines."}
{"id": "2505.20094", "pdf": "https://arxiv.org/pdf/2505.20094", "abs": "https://arxiv.org/abs/2505.20094", "authors": ["Qi Li", "Kun Li", "Haozhi Han", "Honghui Shang", "Xinfu He", "Yunquan Zhang", "Hong An", "Ting Cao", "Mao Yang"], "title": "SwarmThinkers: Learning Physically Consistent Atomic KMC Transitions at Scale", "categories": ["cs.AI"], "comment": null, "summary": "Can a scientific simulation system be physically consistent, interpretable by\ndesign, and scalable across regimes--all at once? Despite decades of progress,\nthis trifecta remains elusive. Classical methods like Kinetic Monte Carlo\nensure thermodynamic accuracy but scale poorly; learning-based methods offer\nefficiency but often sacrifice physical consistency and interpretability. We\npresent SwarmThinkers, a reinforcement learning framework that recasts\natomic-scale simulation as a physically grounded swarm intelligence system.\nEach diffusing particle is modeled as a local decision-making agent that\nselects transitions via a shared policy network trained under thermodynamic\nconstraints. A reweighting mechanism fuses learned preferences with transition\nrates, preserving statistical fidelity while enabling interpretable, step-wise\ndecision making. Training follows a centralized-training,\ndecentralized-execution paradigm, allowing the policy to generalize across\nsystem sizes, concentrations, and temperatures without retraining. On a\nbenchmark simulating radiation-induced Fe-Cu alloy precipitation, SwarmThinkers\nis the first system to achieve full-scale, physically consistent simulation on\na single A100 GPU, previously attainable only via OpenKMC on a supercomputer.\nIt delivers up to 4963x (3185x on average) faster computation with 485x lower\nmemory usage. By treating particles as decision-makers, not passive samplers,\nSwarmThinkers marks a paradigm shift in scientific simulation--one that unifies\nphysical consistency, interpretability, and scalability through agent-driven\nintelligence."}
{"id": "2505.18976", "pdf": "https://arxiv.org/pdf/2505.18976", "abs": "https://arxiv.org/abs/2505.18976", "authors": ["Pingbang Hu", "Joseph Melkonian", "Weijing Tang", "Han Zhao", "Jiaqi W. Ma"], "title": "GraSS: Scalable Influence Function with Sparse Gradient Compression", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Gradient-based data attribution methods, such as influence functions, are\ncritical for understanding the impact of individual training samples without\nrequiring repeated model retraining. However, their scalability is often\nlimited by the high computational and memory costs associated with per-sample\ngradient computation. In this work, we propose GraSS, a novel gradient\ncompression algorithm and its variants FactGraSS for linear layers\nspecifically, that explicitly leverage the inherent sparsity of per-sample\ngradients to achieve sub-linear space and time complexity. Extensive\nexperiments demonstrate the effectiveness of our approach, achieving\nsubstantial speedups while preserving data influence fidelity. In particular,\nFactGraSS achieves up to 165% faster throughput on billion-scale models\ncompared to the previous state-of-the-art baselines. Our code is publicly\navailable at https://github.com/TRAIS-Lab/GraSS."}
{"id": "2505.19212", "pdf": "https://arxiv.org/pdf/2505.19212", "abs": "https://arxiv.org/abs/2505.19212", "authors": ["Steffen Backmann", "David Guzman Piedrahita", "Emanuel Tewolde", "Rada Mihalcea", "Bernhard Schlkopf", "Zhijing Jin"], "title": "When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled their use in\ncomplex agentic roles, involving decision-making with humans or other agents,\nmaking ethical alignment a key AI safety concern. While prior work has examined\nboth LLMs' moral judgment and strategic behavior in social dilemmas, there is\nlimited understanding of how they act when moral imperatives directly conflict\nwith rewards or incentives. To investigate this, we introduce Moral Behavior in\nSocial Dilemma Simulation (MoralSim) and evaluate how LLMs behave in the\nprisoner's dilemma and public goods game with morally charged contexts. In\nMoralSim, we test a range of frontier models across both game structures and\nthree distinct moral framings, enabling a systematic examination of how LLMs\nnavigate social dilemmas in which ethical norms conflict with payoff-maximizing\nstrategies. Our results show substantial variation across models in both their\ngeneral tendency to act morally and the consistency of their behavior across\ngame types, the specific moral framing, and situational factors such as\nopponent behavior and survival risks. Crucially, no model exhibits consistently\nmoral behavior in MoralSim, highlighting the need for caution when deploying\nLLMs in agentic roles where the agent's \"self-interest\" may conflict with\nethical expectations. Our code is available at\nhttps://github.com/sbackmann/moralsim."}
{"id": "2505.20119", "pdf": "https://arxiv.org/pdf/2505.20119", "abs": "https://arxiv.org/abs/2505.20119", "authors": ["Jiaming Ma", "Guanjun Wang", "Sheng Huang", "Kuo Yang", "Binwu Wang", "Pengkun Wang", "Yang Wang"], "title": "Spatiotemporal Causal Decoupling Model for Air Quality Forecasting", "categories": ["cs.AI"], "comment": null, "summary": "Due to the profound impact of air pollution on human health, livelihoods, and\neconomic development, air quality forecasting is of paramount significance.\nInitially, we employ the causal graph method to scrutinize the constraints of\nexisting research in comprehensively modeling the causal relationships between\nthe air quality index (AQI) and meteorological features. In order to enhance\nprediction accuracy, we introduce a novel air quality forecasting model,\nAirCade, which incorporates a causal decoupling approach. AirCade leverages a\nspatiotemporal module in conjunction with knowledge embedding techniques to\ncapture the internal dynamics of AQI. Subsequently, a causal decoupling module\nis proposed to disentangle synchronous causality from past AQI and\nmeteorological features, followed by the dissemination of acquired knowledge to\nfuture time steps to enhance performance. Additionally, we introduce a causal\nintervention mechanism to explicitly represent the uncertainty of future\nmeteorological features, thereby bolstering the model's robustness. Our\nevaluation of AirCade on an open-source air quality dataset demonstrates over\n20\\% relative improvement over state-of-the-art models."}
{"id": "2505.18979", "pdf": "https://arxiv.org/pdf/2505.18979", "abs": "https://arxiv.org/abs/2505.18979", "authors": ["Zixuan Chen", "Hao Lin", "Ke Xu", "Xinghao Jiang", "Tanfeng Sun"], "title": "GhostPrompt: Jailbreaking Text-to-image Generative Models based on Dynamic Optimization", "categories": ["cs.LG"], "comment": null, "summary": "Text-to-image (T2I) generation models can inadvertently produce\nnot-safe-for-work (NSFW) content, prompting the integration of text and image\nsafety filters. Recent advances employ large language models (LLMs) for\nsemantic-level detection, rendering traditional token-level perturbation\nattacks largely ineffective. However, our evaluation shows that existing\njailbreak methods are ineffective against these modern filters. We introduce\nGhostPrompt, the first automated jailbreak framework that combines dynamic\nprompt optimization with multimodal feedback. It consists of two key\ncomponents: (i) Dynamic Optimization, an iterative process that guides a large\nlanguage model (LLM) using feedback from text safety filters and CLIP\nsimilarity scores to generate semantically aligned adversarial prompts; and\n(ii) Adaptive Safety Indicator Injection, which formulates the injection of\nbenign visual cues as a reinforcement learning problem to bypass image-level\nfilters. GhostPrompt achieves state-of-the-art performance, increasing the\nShieldLM-7B bypass rate from 12.5\\% (Sneakyprompt) to 99.0\\%, improving CLIP\nscore from 0.2637 to 0.2762, and reducing the time cost by $4.2 \\times$.\nMoreover, it generalizes to unseen filters including GPT-4.1 and successfully\njailbreaks DALLE 3 to generate NSFW images in our evaluation, revealing\nsystemic vulnerabilities in current multimodal defenses. To support further\nresearch on AI safety and red-teaming, we will release code and adversarial\nprompts under a controlled-access protocol."}
{"id": "2505.19217", "pdf": "https://arxiv.org/pdf/2505.19217", "abs": "https://arxiv.org/abs/2505.19217", "authors": ["Weize Chen", "Jiarui Yuan", "Tailin Jin", "Ning Ding", "Huimin Chen", "Zhiyuan Liu", "Maosong Sun"], "title": "The Overthinker's DIET: Cutting Token Calories with DIfficulty-AwarE Training", "categories": ["cs.CL"], "comment": "under review", "summary": "Recent large language models (LLMs) exhibit impressive reasoning but often\nover-think, generating excessively long responses that hinder efficiency. We\nintroduce DIET ( DIfficulty-AwarE Training), a framework that systematically\ncuts these \"token calories\" by integrating on-the-fly problem difficulty into\nthe reinforcement learning (RL) process. DIET dynamically adapts token\ncompression strategies by modulating token penalty strength and conditioning\ntarget lengths on estimated task difficulty, to optimize the\nperformance-efficiency trade-off. We also theoretically analyze the pitfalls of\nnaive reward weighting in group-normalized RL algorithms like GRPO, and propose\nAdvantage Weighting technique, which enables stable and effective\nimplementation of these difficulty-aware objectives. Experimental results\ndemonstrate that DIET significantly reduces token counts while simultaneously\nimproving reasoning performance. Beyond raw token reduction, we show two\ncrucial benefits largely overlooked by prior work: (1) DIET leads to superior\ninference scaling. By maintaining high per-sample quality with fewer tokens, it\nenables better scaling performance via majority voting with more samples under\nfixed computational budgets, an area where other methods falter. (2) DIET\nenhances the natural positive correlation between response length and problem\ndifficulty, ensuring verbosity is appropriately allocated, unlike many existing\ncompression methods that disrupt this relationship. Our analyses provide a\nprincipled and effective framework for developing more efficient, practical,\nand high-performing LLMs."}
{"id": "2505.20120", "pdf": "https://arxiv.org/pdf/2505.20120", "abs": "https://arxiv.org/abs/2505.20120", "authors": ["Simpson Zhang", "Tennison Liu", "Mihaela van der Schaar"], "title": "Agents Require Metacognitive and Strategic Reasoning to Succeed in the Coming Labor Markets", "categories": ["cs.AI"], "comment": "*Zhang & Liu contributed equally", "summary": "Current labor markets are strongly affected by the economic forces of adverse\nselection, moral hazard, and reputation, each of which arises due to\n$\\textit{incomplete information}$. These economic forces will still be\ninfluential after AI agents are introduced, and thus, agents must use\nmetacognitive and strategic reasoning to perform effectively. Metacognition is\na form of $\\textit{internal reasoning}$ that includes the capabilities for\nself-assessment, task understanding, and evaluation of strategies. Strategic\nreasoning is $\\textit{external reasoning}$ that covers holding beliefs about\nother participants in the labor market (e.g., competitors, colleagues), making\nstrategic decisions, and learning about others over time. Both types of\nreasoning are required by agents as they decide among the many\n$\\textit{actions}$ they can take in labor markets, both within and outside\ntheir jobs. We discuss current research into metacognitive and strategic\nreasoning and the areas requiring further development."}
{"id": "2505.18981", "pdf": "https://arxiv.org/pdf/2505.18981", "abs": "https://arxiv.org/abs/2505.18981", "authors": ["Huan Wang", "Haoran Li", "Huaming Chen", "Jun Yan", "Lijuan Wang", "Jiahua Shi", "Shiping Chen", "Jun Shen"], "title": "FedSKC: Federated Learning with Non-IID Data via Structural Knowledge Collaboration", "categories": ["cs.LG"], "comment": "11 pages, International Conference on Web Services (ICWS) 2025", "summary": "With the advancement of edge computing, federated learning (FL) displays a\nbright promise as a privacy-preserving collaborative learning paradigm.\nHowever, one major challenge for FL is the data heterogeneity issue, which\nrefers to the biased labeling preferences among multiple clients, negatively\nimpacting convergence and model performance. Most previous FL methods attempt\nto tackle the data heterogeneity issue locally or globally, neglecting\nunderlying class-wise structure information contained in each client. In this\npaper, we first study how data heterogeneity affects the divergence of the\nmodel and decompose it into local, global, and sampling drift sub-problems. To\nexplore the potential of using intra-client class-wise structural knowledge in\nhandling these drifts, we thus propose Federated Learning with Structural\nKnowledge Collaboration (FedSKC). The key idea of FedSKC is to extract and\ntransfer domain preferences from inter-client data distributions, offering\ndiverse class-relevant knowledge and a fair convergent signal. FedSKC comprises\nthree components: i) local contrastive learning, to prevent weight divergence\nresulting from local training; ii) global discrepancy aggregation, which\naddresses the parameter deviation between the server and clients; iii) global\nperiod review, correcting for the sampling drift introduced by the server\nrandomly selecting devices. We have theoretically analyzed FedSKC under\nnon-convex objectives and empirically validated its superiority through\nextensive experimental results."}
{"id": "2505.19236", "pdf": "https://arxiv.org/pdf/2505.19236", "abs": "https://arxiv.org/abs/2505.19236", "authors": ["Qian Cao", "Xiting Wang", "Yuzhuo Yuan", "Yahui Liu", "Fang Luo", "Ruihua Song"], "title": "Evaluating Text Creativity across Diverse Domains: A Dataset and Large Language Model Evaluator", "categories": ["cs.CL"], "comment": null, "summary": "Creativity evaluation remains a challenging frontier for large language\nmodels (LLMs). Current evaluations heavily rely on inefficient and costly human\njudgments, hindering progress in enhancing machine creativity. While automated\nmethods exist, ranging from psychological testing to heuristic- or\nprompting-based approaches, they often lack generalizability or alignment with\nhuman judgment. To address these issues, in this paper, we propose a novel\npairwise-comparison framework for assessing textual creativity, leveraging\nshared contextual instructions to improve evaluation consistency. We introduce\nCreataSet, a large-scale dataset with 100K+ human-level and 1M+ synthetic\ncreative instruction-response pairs spanning diverse open-domain tasks. Through\ntraining on CreataSet, we develop an LLM-based evaluator named CrEval. CrEval\ndemonstrates remarkable superiority over existing methods in alignment with\nhuman judgments. Experimental results underscore the indispensable significance\nof integrating both human-generated and synthetic data in training highly\nrobust evaluators, and showcase the practical utility of CrEval in boosting the\ncreativity of LLMs. We will release all data, code, and models publicly soon to\nsupport further research."}
{"id": "2505.20127", "pdf": "https://arxiv.org/pdf/2505.20127", "abs": "https://arxiv.org/abs/2505.20127", "authors": ["Fabiana Fournier", "Lior Limonad", "Yuval David"], "title": "Agentic AI Process Observability: Discovering Behavioral Variability", "categories": ["cs.AI"], "comment": "12 pages, 7 figures", "summary": "AI agents that leverage Large Language Models (LLMs) are increasingly\nbecoming core building blocks of modern software systems. A wide range of\nframeworks is now available to support the specification of such applications.\nThese frameworks enable the definition of agent setups using natural language\nprompting, which specifies the roles, goals, and tools assigned to the various\nagents involved. Within such setups, agent behavior is non-deterministic for\nany given input, highlighting the critical need for robust debugging and\nobservability tools. In this work, we explore the use of process and causal\ndiscovery applied to agent execution trajectories as a means of enhancing\ndeveloper observability. This approach aids in monitoring and understanding the\nemergent variability in agent behavior. Additionally, we complement this with\nLLM-based static analysis techniques to distinguish between intended and\nunintended behavioral variability. We argue that such instrumentation is\nessential for giving developers greater control over evolving specifications\nand for identifying aspects of functionality that may require more precise and\nexplicit definitions."}
{"id": "2505.18983", "pdf": "https://arxiv.org/pdf/2505.18983", "abs": "https://arxiv.org/abs/2505.18983", "authors": ["Haotian Sun", "Yitong Li", "Yuchen Zhuang", "Niao He", "Hanjun Dai", "Bo Dai"], "title": "AmorLIP: Efficient Language-Image Pretraining via Amortization", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Contrastive Language-Image Pretraining (CLIP) has demonstrated strong\nzero-shot performance across diverse downstream text-image tasks. Existing CLIP\nmethods typically optimize a contrastive objective using negative samples drawn\nfrom each minibatch. To achieve robust representation learning, these methods\nrequire extremely large batch sizes and escalate computational demands to\nhundreds or even thousands of GPUs. Prior approaches to mitigate this issue\noften compromise downstream performance, prolong training duration, or face\nscalability challenges with very large datasets. To overcome these limitations,\nwe propose AmorLIP, an efficient CLIP pretraining framework that amortizes\nexpensive computations involved in contrastive learning through lightweight\nneural networks, which substantially improves training efficiency and\nperformance. Leveraging insights from a spectral factorization of energy-based\nmodels, we introduce novel amortization objectives along with practical\ntechniques to improve training stability. Extensive experiments across 38\ndownstream tasks demonstrate the superior zero-shot classification and\nretrieval capabilities of AmorLIP, consistently outperforming standard CLIP\nbaselines with substantial relative improvements of up to 12.24%."}
{"id": "2505.19240", "pdf": "https://arxiv.org/pdf/2505.19240", "abs": "https://arxiv.org/abs/2505.19240", "authors": ["Aida Kostikova", "Zhipin Wang", "Deidamea Bajri", "Ole Ptz", "Benjamin Paaen", "Steffen Eger"], "title": "LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This manuscript is currently under review at ACM Computing Surveys", "summary": "Large language model (LLM) research has grown rapidly, along with increasing\nconcern about their limitations such as failures in reasoning, hallucinations,\nand limited multilingual capability. In this survey, we conduct a data-driven,\nsemi-automated review of research on limitations of LLM (LLLMs) from 2022 to\n2024 using a bottom-up approach. From a corpus of 250,000 ACL and arXiv papers,\nwe identify 14,648 relevant papers using keyword filtering, LLM-based\nclassification, validated against expert labels, and topic clustering (via two\napproaches, HDBSCAN+BERTopic and LlooM). We find that LLM-related research\nincreases over fivefold in ACL and fourfold in arXiv. Since 2022, LLLMs\nresearch grows even faster, reaching over 30% of LLM papers by late 2024.\nReasoning remains the most studied limitation, followed by generalization,\nhallucination, bias, and security. The distribution of topics in the ACL\ndataset stays relatively stable over time, while arXiv shifts toward safety and\ncontrollability (with topics like security risks, alignment, hallucinations,\nknowledge editing), and multimodality between 2022 and 2024. We release a\ndataset of annotated abstracts and a validated methodology, and offer a\nquantitative view of trends in LLM limitations research."}
{"id": "2505.20148", "pdf": "https://arxiv.org/pdf/2505.20148", "abs": "https://arxiv.org/abs/2505.20148", "authors": ["Ziming Wei", "Bingqian Lin", "Zijian Jiao", "Yunshuang Nie", "Liang Ma", "Yuecheng Liu", "Yuzheng Zhuang", "Xiaodan Liang"], "title": "MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents", "categories": ["cs.AI"], "comment": null, "summary": "Spatial Planning is a crucial part in the field of spatial intelligence,\nwhich requires the understanding and planning about object arrangements in\nspace perspective. AI agents with the spatial planning ability can better adapt\nto various real-world applications, including robotic manipulation, automatic\nassembly, urban planning etc. Recent works have attempted to construct\nbenchmarks for evaluating the spatial intelligence of Multimodal Large Language\nModels (MLLMs). Nevertheless, these benchmarks primarily focus on spatial\nreasoning based on typical Visual Question-Answering (VQA) forms, which suffers\nfrom the gap between abstract spatial understanding and concrete task\nexecution. In this work, we take a step further to build a comprehensive\nbenchmark called MineAnyBuild, aiming to evaluate the spatial planning ability\nof open-world AI agents in the Minecraft game. Specifically, MineAnyBuild\nrequires an agent to generate executable architecture building plans based on\nthe given multi-modal human instructions. It involves 4,000 curated spatial\nplanning tasks and also provides a paradigm for infinitely expandable data\ncollection by utilizing rich player-generated content. MineAnyBuild evaluates\nspatial planning through four core supporting dimensions: spatial\nunderstanding, spatial reasoning, creativity, and spatial commonsense. Based on\nMineAnyBuild, we perform a comprehensive evaluation for existing MLLM-based\nagents, revealing the severe limitations but enormous potential in their\nspatial planning abilities. We believe our MineAnyBuild will open new avenues\nfor the evaluation of spatial intelligence and help promote further development\nfor open-world AI agents capable of spatial planning."}
{"id": "2505.18985", "pdf": "https://arxiv.org/pdf/2505.18985", "abs": "https://arxiv.org/abs/2505.18985", "authors": ["Tianyu Zhang", "Xinyu Wang", "Zhenghan Tai", "Lu Li", "Jijun Chi", "Jingrui Tian", "Hailin He", "Suyuchen Wang"], "title": "STRICT: Stress Test of Rendering Images Containing Text", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "13 pages", "summary": "While diffusion models have revolutionized text-to-image generation with\ntheir ability to synthesize realistic and diverse scenes, they continue to\nstruggle to generate consistent and legible text within images. This\nshortcoming is commonly attributed to the locality bias inherent in\ndiffusion-based generation, which limits their ability to model long-range\nspatial dependencies. In this paper, we introduce $\\textbf{STRICT}$, a\nbenchmark designed to systematically stress-test the ability of diffusion\nmodels to render coherent and instruction-aligned text in images. Our benchmark\nevaluates models across multiple dimensions: (1) the maximum length of readable\ntext that can be generated; (2) the correctness and legibility of the generated\ntext, and (3) the ratio of not following instructions for generating text. We\nevaluate several state-of-the-art models, including proprietary and open-source\nvariants, and reveal persistent limitations in long-range consistency and\ninstruction-following capabilities. Our findings provide insights into\narchitectural bottlenecks and motivate future research directions in multimodal\ngenerative modeling. We release our entire evaluation pipeline at\nhttps://github.com/tianyu-z/STRICT-Bench."}
{"id": "2505.19250", "pdf": "https://arxiv.org/pdf/2505.19250", "abs": "https://arxiv.org/abs/2505.19250", "authors": ["Yi Wang", "Junxiao Liu", "Shimao Zhang", "Jiajun Chen", "Shujian Huang"], "title": "PATS: Process-Level Adaptive Thinking Mode Switching", "categories": ["cs.CL"], "comment": null, "summary": "Current large-language models (LLMs) typically adopt a fixed reasoning\nstrategy, either simple or complex, for all questions, regardless of their\ndifficulty. This neglect of variation in task and reasoning process complexity\nleads to an imbalance between performance and efficiency. Existing methods\nattempt to implement training-free fast-slow thinking system switching to\nhandle problems of varying difficulty, but are limited by coarse-grained\nsolution-level strategy adjustments. To address this issue, we propose a novel\nreasoning paradigm: Process-Level Adaptive Thinking Mode Switching (PATS),\nwhich enables LLMs to dynamically adjust their reasoning strategy based on the\ndifficulty of each step, optimizing the balance between accuracy and\ncomputational efficiency. Our approach integrates Process Reward Models (PRMs)\nwith Beam Search, incorporating progressive mode switching and bad-step penalty\nmechanisms. Experiments on diverse mathematical benchmarks demonstrate that our\nmethodology achieves high accuracy while maintaining moderate token usage. This\nstudy emphasizes the significance of process-level, difficulty-aware reasoning\nstrategy adaptation, offering valuable insights into efficient inference for\nLLMs."}
{"id": "2505.20162", "pdf": "https://arxiv.org/pdf/2505.20162", "abs": "https://arxiv.org/abs/2505.20162", "authors": ["Alexander Panfilov", "Paul Kassianik", "Maksym Andriushchenko", "Jonas Geiping"], "title": "Capability-Based Scaling Laws for LLM Red-Teaming", "categories": ["cs.AI"], "comment": null, "summary": "As large language models grow in capability and agency, identifying\nvulnerabilities through red-teaming becomes vital for safe deployment. However,\ntraditional prompt-engineering approaches may prove ineffective once\nred-teaming turns into a weak-to-strong problem, where target models surpass\nred-teamers in capabilities. To study this shift, we frame red-teaming through\nthe lens of the capability gap between attacker and target. We evaluate more\nthan 500 attacker-target pairs using LLM-based jailbreak attacks that mimic\nhuman red-teamers across diverse families, sizes, and capability levels. Three\nstrong trends emerge: (i) more capable models are better attackers, (ii) attack\nsuccess drops sharply once the target's capability exceeds the attacker's, and\n(iii) attack success rates correlate with high performance on social science\nsplits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking\nscaling law that predicts attack success for a fixed target based on\nattacker-target capability gap. These findings suggest that fixed-capability\nattackers (e.g., humans) may become ineffective against future models,\nincreasingly capable open-source models amplify risks for existing systems, and\nmodel providers must accurately measure and control models' persuasive and\nmanipulative abilities to limit their effectiveness as attackers."}
{"id": "2505.18996", "pdf": "https://arxiv.org/pdf/2505.18996", "abs": "https://arxiv.org/abs/2505.18996", "authors": ["Bob Junyi Zou", "Lu Tian"], "title": "Automatic and Structure-Aware Sparsification of Hybrid Neural ODEs", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Hybrid neural ordinary differential equations (neural ODEs) integrate\nmechanistic models with neural ODEs, offering strong inductive bias and\nflexibility, and are particularly advantageous in data-scarce healthcare\nsettings. However, excessive latent states and interactions from mechanistic\nmodels can lead to training inefficiency and over-fitting, limiting practical\neffectiveness of hybrid neural ODEs. In response, we propose a new hybrid\npipeline for automatic state selection and structure optimization in\nmechanistic neural ODEs, combining domain-informed graph modifications with\ndata-driven regularization to sparsify the model for improving predictive\nperformance and stability while retaining mechanistic plausibility. Experiments\non synthetic and real-world data show improved predictive performance and\nrobustness with desired sparsity, establishing an effective solution for hybrid\nmodel reduction in healthcare applications."}
{"id": "2505.19254", "pdf": "https://arxiv.org/pdf/2505.19254", "abs": "https://arxiv.org/abs/2505.19254", "authors": ["Rafa Powiata", "Marcin Micha Miroczuk", "Sawomir Dadas", "Magorzata Grbowiec", "Micha Perekiewicz"], "title": "Unveiling Dual Quality in Product Reviews: An NLP-Based Approach", "categories": ["cs.CL"], "comment": "Accepted for ACL 2025 Industry Track", "summary": "Consumers often face inconsistent product quality, particularly when\nidentical products vary between markets, a situation known as the dual quality\nproblem. To identify and address this issue, automated techniques are needed.\nThis paper explores how natural language processing (NLP) can aid in detecting\nsuch discrepancies and presents the full process of developing a solution.\nFirst, we describe in detail the creation of a new Polish-language dataset with\n1,957 reviews, 540 highlighting dual quality issues. We then discuss\nexperiments with various approaches like SetFit with sentence-transformers,\ntransformer-based encoders, and LLMs, including error analysis and robustness\nverification. Additionally, we evaluate multilingual transfer using a subset of\nopinions in English, French, and German. The paper concludes with insights on\ndeployment and practical applications."}
{"id": "2505.20170", "pdf": "https://arxiv.org/pdf/2505.20170", "abs": "https://arxiv.org/abs/2505.20170", "authors": ["Yunze Lin"], "title": "Program of Equations Thoughts to Solve Algebra Word Problems", "categories": ["cs.AI"], "comment": null, "summary": "Solving algebraic word problems (AWPs) has recently emerged as an important\nnatural language processing task. Recently, large language models (LLMs) have\ndemonstrated powerful mathematical capabilities, and the Chain-of-Thought\ntechnique, which guides LLMs through step-by-step reasoning, has yielded\nimpressive results. However, this reasoning ability is limited by the\ncomputational weaknesses of LLMs themselves, where calculation errors can\naccumulate, leading to incorrect final answers. To address this, we propose\nProgram of Equations Thoughts (POET), which transforms the task of generating\nstep-by-step reasoning answers into a two-stage task of predicting equations\nand generating code, offloading complex computations to a Python interpreter to\navoid calculation errors in LLMs. Furthermore, we propose Zero-shot POET, which\nutilizes a manually designed template to enable LLMs to directly generate\nPython code for one-step solving. Our method achieves accuracies of 95.3% and\n98.0% on the PEN and ALG514 datasets, respectively, setting a new\nstate-of-the-art (SOTA). Zero-shot POET also achieves the SOTA result of 95.5%\non the DRAW-1K dataset."}
{"id": "2505.19002", "pdf": "https://arxiv.org/pdf/2505.19002", "abs": "https://arxiv.org/abs/2505.19002", "authors": ["Jin Zhu", "Xin Zhou", "Jiaang Yao", "Gholamali Aminian", "Omar Rivasplata", "Simon Little", "Lexin Li", "Chengchun Shi"], "title": "Semi-pessimistic Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Offline reinforcement learning (RL) aims to learn an optimal policy from\npre-collected data. However, it faces challenges of distributional shift, where\nthe learned policy may encounter unseen scenarios not covered in the offline\ndata. Additionally, numerous applications suffer from a scarcity of labeled\nreward data. Relying on labeled data alone often leads to a narrow state-action\ndistribution, further amplifying the distributional shift, and resulting in\nsuboptimal policy learning. To address these issues, we first recognize that\nthe volume of unlabeled data is typically substantially larger than that of\nlabeled data. We then propose a semi-pessimistic RL method to effectively\nleverage abundant unlabeled data. Our approach offers several advantages. It\nconsiderably simplifies the learning process, as it seeks a lower bound of the\nreward function, rather than that of the Q-function or state transition\nfunction. It is highly flexible, and can be integrated with a range of\nmodel-free and model-based RL algorithms. It enjoys the guaranteed improvement\nwhen utilizing vast unlabeled data, but requires much less restrictive\nconditions. We compare our method with a number of alternative solutions, both\nanalytically and numerically, and demonstrate its clear competitiveness. We\nfurther illustrate with an application to adaptive deep brain stimulation for\nParkinson's disease."}
{"id": "2505.19286", "pdf": "https://arxiv.org/pdf/2505.19286", "abs": "https://arxiv.org/abs/2505.19286", "authors": ["Utkarsh Sahu", "Zhisheng Qi", "Yongjia Lei", "Ryan A. Rossi", "Franck Dernoncourt", "Nesreen K. Ahmed", "Mahantesh M Halappanavar", "Yao Ma", "Yu Wang"], "title": "A Graph Perspective to Probe Structural Patterns of Knowledge in Large Language Models", "categories": ["cs.CL", "cs.LG", "cs.SI"], "comment": null, "summary": "Large language models have been extensively studied as neural knowledge bases\nfor their knowledge access, editability, reasoning, and explainability.\nHowever, few works focus on the structural patterns of their knowledge.\nMotivated by this gap, we investigate these structural patterns from a graph\nperspective. We quantify the knowledge of LLMs at both the triplet and entity\nlevels, and analyze how it relates to graph structural properties such as node\ndegree. Furthermore, we uncover the knowledge homophily, where topologically\nclose entities exhibit similar levels of knowledgeability, which further\nmotivates us to develop graph machine learning models to estimate entity\nknowledge based on its local neighbors. This model further enables valuable\nknowledge checking by selecting triplets less known to LLMs. Empirical results\nshow that using selected triplets for fine-tuning leads to superior\nperformance."}
{"id": "2505.20182", "pdf": "https://arxiv.org/pdf/2505.20182", "abs": "https://arxiv.org/abs/2505.20182", "authors": ["Shubham Gandhi", "Atharva Naik", "Yiqing Xie", "Carolyn Rose"], "title": "An Empirical Study on Strong-Weak Model Collaboration for Repo-level Code Generation", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "We study cost-efficient collaboration between strong and weak language models\nfor repository-level code generation, where the weak model handles simpler\ntasks at lower cost, and the most challenging tasks are delegated to the strong\nmodel. While many works propose architectures for this task, few analyze\nperformance relative to cost. We evaluate a broad spectrum of collaboration\nstrategies: context-based, pipeline-based, and dynamic, on GitHub issue\nresolution. Our most effective collaborative strategy achieves equivalent\nperformance to the strong model while reducing the cost by 40%. Based on our\nfindings, we offer actionable guidelines for choosing collaboration strategies\nunder varying budget and performance constraints. Our results show that\nstrong-weak collaboration substantially boosts the weak model's performance at\na fraction of the cost, pipeline and context-based methods being most\nefficient. We release the code for our work at\nhttps://github.com/shubhamrgandhi/codegen-strong-weak-collab."}
{"id": "2505.19013", "pdf": "https://arxiv.org/pdf/2505.19013", "abs": "https://arxiv.org/abs/2505.19013", "authors": ["Kiljae Lee", "Ziqi Liu", "Weijing Tang", "Yuan Zhang"], "title": "Faithful Group Shapley Value", "categories": ["cs.LG", "cs.AI", "econ.GN", "q-fin.EC", "stat.ML"], "comment": null, "summary": "Data Shapley is an important tool for data valuation, which quantifies the\ncontribution of individual data points to machine learning models. In practice,\ngroup-level data valuation is desirable when data providers contribute data in\nbatch. However, we identify that existing group-level extensions of Data\nShapley are vulnerable to shell company attacks, where strategic group\nsplitting can unfairly inflate valuations. We propose Faithful Group Shapley\nValue (FGSV) that uniquely defends against such attacks. Building on original\nmathematical insights, we develop a provably fast and accurate approximation\nalgorithm for computing FGSV. Empirical experiments demonstrate that our\nalgorithm significantly outperforms state-of-the-art methods in computational\nefficiency and approximation accuracy, while ensuring faithful group-level\nvaluation."}
{"id": "2505.19293", "pdf": "https://arxiv.org/pdf/2505.19293", "abs": "https://arxiv.org/abs/2505.19293", "authors": ["Wang Yang", "Hongye Jin", "Shaochen Zhong", "Song Jiang", "Qifan Wang", "Vipin Chaudhary", "Xiaotian Han"], "title": "100-LongBench: Are de facto Long-Context Benchmarks Literally Evaluating Long-Context Ability?", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Long-context capability is considered one of the most important abilities of\nLLMs, as a truly long context-capable LLM enables users to effortlessly process\nmany originally exhausting tasks -- e.g., digesting a long-form document to\nfind answers vs. directly asking an LLM about it. However, existing\nreal-task-based long-context evaluation benchmarks have two major shortcomings.\nFirst, benchmarks like LongBench often do not provide proper metrics to\nseparate long-context performance from the model's baseline ability, making\ncross-model comparison unclear. Second, such benchmarks are usually constructed\nwith fixed input lengths, which limits their applicability across different\nmodels and fails to reveal when a model begins to break down. To address these\nissues, we introduce a length-controllable long-context benchmark and a novel\nmetric that disentangles baseline knowledge from true long-context\ncapabilities. Experiments demonstrate the superiority of our approach in\neffectively evaluating LLMs."}
{"id": "2505.20196", "pdf": "https://arxiv.org/pdf/2505.20196", "abs": "https://arxiv.org/abs/2505.20196", "authors": ["Yuetai Li", "Zhangchen Xu", "Fengqing Jiang", "Bhaskar Ramasubramanian", "Luyao Niu", "Bill Yuchen Lin", "Xiang Yue", "Radha Poovendran"], "title": "Temporal Sampling for Forgotten Reasoning in LLMs", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Fine-tuning large language models (LLMs) is intended to improve their\nreasoning capabilities, yet we uncover a counterintuitive effect: models often\nforget how to solve problems they previously answered correctly during\ntraining. We term this phenomenon temporal forgetting and show that it is\nwidespread across model sizes, fine-tuning methods (both Reinforcement Learning\nand Supervised Fine-Tuning), and multiple reasoning benchmarks. To address this\ngap, we introduce Temporal Sampling, a simple decoding strategy that draws\noutputs from multiple checkpoints along the training trajectory. This approach\nrecovers forgotten solutions without retraining or ensembling, and leads to\nsubstantial improvements in reasoning performance, gains from 4 to 19 points in\nPass@k and consistent gains in Majority@k across several benchmarks. We further\nextend our method to LoRA-adapted models, demonstrating that storing only\nadapter weights across checkpoints achieves similar benefits with minimal\nstorage cost. By leveraging the temporal diversity inherent in training,\nTemporal Sampling offers a practical, compute-efficient way to surface hidden\nreasoning ability and rethink how we evaluate LLMs."}
{"id": "2505.19014", "pdf": "https://arxiv.org/pdf/2505.19014", "abs": "https://arxiv.org/abs/2505.19014", "authors": ["Haitao Lin", "Odin Zhang", "Jia Xu", "Yunfan Liu", "Zheng Cheng", "Lirong Wu", "Yufei Huang", "Zhifeng Gao", "Stan Z. Li"], "title": "Tokenizing Electron Cloud in Protein-Ligand Interaction Learning", "categories": ["cs.LG", "physics.chem-ph", "q-bio.QM"], "comment": "conference paper", "summary": "The affinity and specificity of protein-molecule binding directly impact\nfunctional outcomes, uncovering the mechanisms underlying biological regulation\nand signal transduction. Most deep-learning-based prediction approaches focus\non structures of atoms or fragments. However, quantum chemical properties, such\nas electronic structures, are the key to unveiling interaction patterns but\nremain largely underexplored. To bridge this gap, we propose ECBind, a method\nfor tokenizing electron cloud signals into quantized embeddings, enabling their\nintegration into downstream tasks such as binding affinity prediction. By\nincorporating electron densities, ECBind helps uncover binding modes that\ncannot be fully represented by atom-level models. Specifically, to remove the\nredundancy inherent in electron cloud signals, a structure-aware transformer\nand hierarchical codebooks encode 3D binding sites enriched with electron\nstructures into tokens. These tokenized codes are then used for specific tasks\nwith labels. To extend its applicability to a wider range of scenarios, we\nutilize knowledge distillation to develop an electron-cloud-agnostic prediction\nmodel. Experimentally, ECBind demonstrates state-of-the-art performance across\nmultiple tasks, achieving improvements of 6.42\\% and 15.58\\% in per-structure\nPearson and Spearman correlation coefficients, respectively."}
{"id": "2505.19299", "pdf": "https://arxiv.org/pdf/2505.19299", "abs": "https://arxiv.org/abs/2505.19299", "authors": ["Lingjun Zhao", "Hal Daum III"], "title": "A Necessary Step toward Faithfulness: Measuring and Improving Consistency in Free-Text Explanations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Faithful free-text explanations are important to ensure transparency in\nhigh-stakes AI decision-making contexts, but they are challenging to generate\nby language models and assess by humans. In this paper, we present a measure\nfor Prediction-EXplanation (PEX) consistency, by extending the concept of\nweight of evidence. This measure quantifies how much a free-text explanation\nsupports or opposes a prediction, serving as an important aspect of explanation\nfaithfulness. Our analysis reveals that more than 62% explanations generated by\nlarge language models lack this consistency. We show that applying direct\npreference optimization improves the consistency of generated explanations\nacross three model families, with improvement ranging from 43.1% to 292.3%.\nFurthermore, we demonstrate that optimizing this consistency measure can\nimprove explanation faithfulness by up to 9.7%."}
{"id": "2505.20203", "pdf": "https://arxiv.org/pdf/2505.20203", "abs": "https://arxiv.org/abs/2505.20203", "authors": ["Elliott Thornley"], "title": "Shutdownable Agents through POST-Agency", "categories": ["cs.AI"], "comment": null, "summary": "Many fear that future artificial agents will resist shutdown. I present an\nidea - the POST-Agents Proposal - for ensuring that doesn't happen. I propose\nthat we train agents to satisfy Preferences Only Between Same-Length\nTrajectories (POST). I then prove that POST - together with other conditions -\nimplies Neutrality+: the agent maximizes expected utility, ignoring the\nprobability distribution over trajectory-lengths. I argue that Neutrality+\nkeeps agents shutdownable and allows them to be useful."}
{"id": "2505.19019", "pdf": "https://arxiv.org/pdf/2505.19019", "abs": "https://arxiv.org/abs/2505.19019", "authors": ["Daniel Barzilai", "Yuval Margalit", "Eitan Gronich", "Gilad Yehudai", "Meirav Galun", "Ronen Basri"], "title": "Querying Kernel Methods Suffices for Reconstructing their Training Data", "categories": ["cs.LG"], "comment": null, "summary": "Over-parameterized models have raised concerns about their potential to\nmemorize training data, even when achieving strong generalization. The privacy\nimplications of such memorization are generally unclear, particularly in\nscenarios where only model outputs are accessible. We study this question in\nthe context of kernel methods, and demonstrate both empirically and\ntheoretically that querying kernel models at various points suffices to\nreconstruct their training data, even without access to model parameters. Our\nresults hold for a range of kernel methods, including kernel regression,\nsupport vector machines, and kernel density estimation. Our hope is that this\nwork can illuminate potential privacy concerns for such models."}
{"id": "2505.19300", "pdf": "https://arxiv.org/pdf/2505.19300", "abs": "https://arxiv.org/abs/2505.19300", "authors": ["Junnan Liu", "Linhao Luo", "Thuy-Trang Vu", "Gholamreza Haffari"], "title": "SituatedThinker: Grounding LLM Reasoning with Real-World through Situated Thinking", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Recent advances in large language models (LLMs) demonstrate their impressive\nreasoning capabilities. However, the reasoning confined to internal parametric\nspace limits LLMs' access to real-time information and understanding of the\nphysical world. To overcome this constraint, we introduce SituatedThinker, a\nnovel framework that enables LLMs to ground their reasoning in real-world\ncontexts through situated thinking, which adaptively combines both internal\nknowledge and external information with predefined interfaces. By utilizing\nreinforcement learning, SituatedThinker incentivizes deliberate reasoning with\nthe real world to acquire information and feedback, allowing LLMs to surpass\ntheir knowledge boundaries and enhance reasoning. Experimental results\ndemonstrate significant performance improvements on multi-hop\nquestion-answering and mathematical reasoning benchmarks. Furthermore,\nSituatedThinker demonstrates strong performance on unseen tasks, such as KBQA,\nTableQA, and text-based games, showcasing the generalizable real-world grounded\nreasoning capability. Our codes are available at\nhttps://github.com/jnanliu/SituatedThinker."}
{"id": "2505.20214", "pdf": "https://arxiv.org/pdf/2505.20214", "abs": "https://arxiv.org/abs/2505.20214", "authors": ["Jiaming Ji", "Sitong Fang", "Wenjing Cao", "Jiahao Li", "Xuyao Wang", "Juntao Dai", "Chi-Min Chan", "Sirui Han", "Yike Guo", "Yaodong Yang"], "title": "The Mirage of Multimodality: Where Truth is Tested and Honesty Unravels", "categories": ["cs.AI"], "comment": null, "summary": "Reasoning models have recently attracted significant attention, especially\nfor tasks that involve complex inference. Their strengths exemplify the System\nII paradigm (slow, structured thinking), contrasting with the System I (rapid,\nheuristic-driven). Yet, does slower reasoning necessarily lead to greater\ntruthfulness? Our findings suggest otherwise. In this study, we present the\nfirst systematic investigation of distortions associated with System I and\nSystem II reasoning in multimodal contexts. We demonstrate that slower\nreasoning models, when presented with incomplete or misleading visual inputs,\nare more likely to fabricate plausible yet false details to support flawed\nreasoning -- a phenomenon we term the \"Mirage of Multimodality\". To examine\nthis, we constructed a 5,000-sample hierarchical prompt dataset annotated by 50\nhuman participants. These prompts gradually increase in complexity, revealing a\nconsistent pattern: slower reasoning models tend to employ depth-first thinking\n(delving deeper into incorrect premises), whereas faster chat models favor\nbreadth-first inference, exhibiting greater caution under uncertainty. Our\nresults highlight a critical vulnerability of slower reasoning models: although\nhighly effective in structured domains such as mathematics, it becomes brittle\nwhen confronted with ambiguous multimodal inputs."}
{"id": "2505.19024", "pdf": "https://arxiv.org/pdf/2505.19024", "abs": "https://arxiv.org/abs/2505.19024", "authors": ["Siqi Huang", "Yanchen Xu", "Hongyuan Zhang", "Xuelong Li"], "title": "Learn Beneficial Noise as Graph Augmentation", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Although graph contrastive learning (GCL) has been widely investigated, it is\nstill a challenge to generate effective and stable graph augmentations.\nExisting methods often apply heuristic augmentation like random edge dropping,\nwhich may disrupt important graph structures and result in unstable GCL\nperformance. In this paper, we propose Positive-incentive Noise driven Graph\nData Augmentation (PiNGDA), where positive-incentive noise (pi-noise)\nscientifically analyzes the beneficial effect of noise under the information\ntheory. To bridge the standard GCL and pi-noise framework, we design a Gaussian\nauxiliary variable to convert the loss function to information entropy. We\nprove that the standard GCL with pre-defined augmentations is equivalent to\nestimate the beneficial noise via the point estimation. Following our analysis,\nPiNGDA is derived from learning the beneficial noise on both topology and\nattributes through a trainable noise generator for graph augmentations, instead\nof the simple estimation. Since the generator learns how to produce beneficial\nperturbations on graph topology and node attributes, PiNGDA is more reliable\ncompared with the existing methods. Extensive experimental results validate the\neffectiveness and stability of PiNGDA."}
{"id": "2505.19345", "pdf": "https://arxiv.org/pdf/2505.19345", "abs": "https://arxiv.org/abs/2505.19345", "authors": ["Yongmin Yoo", "Qiongkai Xu", "Longbing Cao"], "title": "PatentScore: Multi-dimensional Evaluation of LLM-Generated Patent Claims", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural language generation (NLG) metrics play a central role in evaluating\ngenerated texts, but are not well suited for the structural and legal\ncharacteristics of patent documents. Large language models (LLMs) offer strong\npotential in automating patent generation, yet research on evaluating\nLLM-generated patents remains limited, especially in evaluating the generation\nquality of patent claims, which are central to defining the scope of\nprotection. Effective claim evaluation requires addressing legal validity,\ntechnical accuracy, and structural compliance. To address this gap, we\nintroduce PatentScore, a multi-dimensional evaluation framework for assessing\nLLM-generated patent claims. PatentScore incorporates: (1) hierarchical\ndecomposition for claim analysis; (2) domain-specific validation patterns based\non legal and technical standards; and (3) scoring across structural, semantic,\nand legal dimensions. Unlike general-purpose NLG metrics, PatentScore reflects\npatent-specific constraints and document structures, enabling evaluation beyond\nsurface similarity. We evaluate 400 GPT-4o-mini generated Claim 1s and report a\nPearson correlation of $r = 0.819$ with expert annotations, outperforming\nexisting NLG metrics. Furthermore, we conduct additional evaluations using open\nmodels such as Claude-3.5-Haiku and Gemini-1.5-flash, all of which show strong\ncorrelations with expert judgments, confirming the robustness and\ngeneralizability of our framework."}
{"id": "2505.20246", "pdf": "https://arxiv.org/pdf/2505.20246", "abs": "https://arxiv.org/abs/2505.20246", "authors": ["Jiahao Qiu", "Fulian Xiao", "Yimin Wang", "Yuchen Mao", "Yijia Chen", "Xinzhe Juan", "Siran Wang", "Xuan Qi", "Tongcheng Zhang", "Zixin Yao", "Jiacheng Guo", "Yifu Lu", "Charles Argon", "Jundi Cui", "Daixin Chen", "Junran Zhou", "Shuyao Zhou", "Zhanpeng Zhou", "Ling Yang", "Shilong Liu", "Hongru Wang", "Kaixuan Huang", "Xun Jiang", "Yuming Cao", "Yue Chen", "Yunfei Chen", "Zhengyi Chen", "Ruowei Dai", "Mengqiu Deng", "Jiye Fu", "Yunting Gu", "Zijie Guan", "Zirui Huang", "Xiaoyan Ji", "Yumeng Jiang", "Delong Kong", "Haolong Li", "Jiaqi Li", "Ruipeng Li", "Tianze Li", "Zhuoran Li", "Haixia Lian", "Mengyue Lin", "Xudong Liu", "Jiayi Lu", "Jinghan Lu", "Wanyu Luo", "Ziyue Luo", "Zihao Pu", "Zhi Qiao", "Ruihuan Ren", "Liang Wan", "Ruixiang Wang", "Tianhui Wang", "Yang Wang", "Zeyu Wang", "Zihua Wang", "Yujia Wu", "Zhaoyi Wu", "Hao Xin", "Weiao Xing", "Ruojun Xiong", "Weijie Xu", "Yao Shu", "Xiao Yao", "Xiaorui Yang", "Yuchen Yang", "Nan Yi", "Jiadong Yu", "Yangyuxuan Yu", "Huiting Zeng", "Danni Zhang", "Yunjie Zhang", "Zhaoyu Zhang", "Zhiheng Zhang", "Xiaofeng Zheng", "Peirong Zhou", "Linyan Zhong", "Xiaoyin Zong", "Ying Zhao", "Zhenxin Chen", "Lin Ding", "Xiaoyu Gao", "Bingbing Gong", "Yichao Li", "Yang Liao", "Guang Ma", "Tianyuan Ma", "Xinrui Sun", "Tianyi Wang", "Han Xia", "Ruobing Xian", "Gen Ye", "Tengfei Yu", "Wentao Zhang", "Yuxi Wang", "Xi Gao", "Mengdi Wang"], "title": "On Path to Multimodal Historical Reasoning: HistBench and HistAgent", "categories": ["cs.AI", "cs.CL"], "comment": "17 pages, 7 figures", "summary": "Recent advances in large language models (LLMs) have led to remarkable\nprogress across domains, yet their capabilities in the humanities, particularly\nhistory, remain underexplored. Historical reasoning poses unique challenges for\nAI, involving multimodal source interpretation, temporal inference, and\ncross-linguistic analysis. While general-purpose agents perform well on many\nexisting benchmarks, they lack the domain-specific expertise required to engage\nwith historical materials and questions. To address this gap, we introduce\nHistBench, a new benchmark of 414 high-quality questions designed to evaluate\nAI's capacity for historical reasoning and authored by more than 40 expert\ncontributors. The tasks span a wide range of historical problems-from factual\nretrieval based on primary sources to interpretive analysis of manuscripts and\nimages, to interdisciplinary challenges involving archaeology, linguistics, or\ncultural history. Furthermore, the benchmark dataset spans 29 ancient and\nmodern languages and covers a wide range of historical periods and world\nregions. Finding the poor performance of LLMs and other agents on HistBench, we\nfurther present HistAgent, a history-specific agent equipped with carefully\ndesigned tools for OCR, translation, archival search, and image understanding\nin History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of\n27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online\nsearch and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%)\nand Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These\nresults highlight the limitations of existing LLMs and generalist agents and\ndemonstrate the advantages of HistAgent for historical reasoning."}
{"id": "2505.19038", "pdf": "https://arxiv.org/pdf/2505.19038", "abs": "https://arxiv.org/abs/2505.19038", "authors": ["Hao Wu", "Yuan Gao", "Ruiqi Shu", "Zean Han", "Fan Xu", "Zhihong Zhu", "Qingsong Wen", "Xian Wu", "Kun Wang", "Xiaomeng Huang"], "title": "Turb-L1: Achieving Long-term Turbulence Tracing By Tackling Spectral Bias", "categories": ["cs.LG", "cs.AI", "physics.flu-dyn"], "comment": null, "summary": "Accurately predicting the long-term evolution of turbulence is crucial for\nadvancing scientific understanding and optimizing engineering applications.\nHowever, existing deep learning methods face significant bottlenecks in\nlong-term autoregressive prediction, which exhibit excessive smoothing and fail\nto accurately track complex fluid dynamics. Our extensive experimental and\nspectral analysis of prevailing methods provides an interpretable explanation\nfor this shortcoming, identifying Spectral Bias as the core obstacle.\nConcretely, spectral bias is the inherent tendency of models to favor\nlow-frequency, smooth features while overlooking critical high-frequency\ndetails during training, thus reducing fidelity and causing physical\ndistortions in long-term predictions. Building on this insight, we propose\nTurb-L1, an innovative turbulence prediction method, which utilizes a\nHierarchical Dynamics Synthesis mechanism within a multi-grid architecture to\nexplicitly overcome spectral bias. It accurately captures cross-scale\ninteractions and preserves the fidelity of high-frequency dynamics, enabling\nreliable long-term tracking of turbulence evolution. Extensive experiments on\nthe 2D turbulence benchmark show that Turb-L1 demonstrates excellent\nperformance: (I) In long-term predictions, it reduces Mean Squared Error (MSE)\nby $80.3\\%$ and increases Structural Similarity (SSIM) by over $9\\times$\ncompared to the SOTA baseline, significantly improving prediction fidelity.\n(II) It effectively overcomes spectral bias, accurately reproducing the full\nenstrophy spectrum and maintaining physical realism in high-wavenumber regions,\nthus avoiding the spectral distortions or spurious energy accumulation seen in\nother methods."}
{"id": "2505.19354", "pdf": "https://arxiv.org/pdf/2505.19354", "abs": "https://arxiv.org/abs/2505.19354", "authors": ["Mohammad Mahdi Moradi", "Sudhir Mudur"], "title": "GC-KBVQA: A New Four-Stage Framework for Enhancing Knowledge Based Visual Question Answering Performance", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Knowledge-Based Visual Question Answering (KB-VQA) methods focus on tasks\nthat demand reasoning with information extending beyond the explicit content\ndepicted in the image. Early methods relied on explicit knowledge bases to\nprovide this auxiliary information. Recent approaches leverage Large Language\nModels (LLMs) as implicit knowledge sources. While KB-VQA methods have\ndemonstrated promising results, their potential remains constrained as the\nauxiliary text provided may not be relevant to the question context, and may\nalso include irrelevant information that could misguide the answer predictor.\nWe introduce a novel four-stage framework called Grounding Caption-Guided\nKnowledge-Based Visual Question Answering (GC-KBVQA), which enables LLMs to\neffectively perform zero-shot VQA tasks without the need for end-to-end\nmultimodal training. Innovations include grounding question-aware caption\ngeneration to move beyond generic descriptions and have compact, yet detailed\nand context-rich information. This is combined with knowledge from external\nsources to create highly informative prompts for the LLM. GC-KBVQA can address\na variety of VQA tasks, and does not require task-specific fine-tuning, thus\nreducing both costs and deployment complexity by leveraging general-purpose,\npre-trained LLMs. Comparison with competing KB-VQA methods shows significantly\nimproved performance. Our code will be made public."}
{"id": "2505.20266", "pdf": "https://arxiv.org/pdf/2505.20266", "abs": "https://arxiv.org/abs/2505.20266", "authors": ["Alexander Conway", "Debadeepta Dey", "Stefan Hackmann", "Matthew Hausknecht", "Michael Schmidt", "Mark Steadman", "Nick Volynets"], "title": "syftr: Pareto-Optimal Generative AI", "categories": ["cs.AI", "cs.LG"], "comment": "International Conference on Automated Machine Learning (AutoML) 2025", "summary": "Retrieval-Augmented Generation (RAG) pipelines are central to applying large\nlanguage models (LLMs) to proprietary or dynamic data. However, building\neffective RAG flows is complex, requiring careful selection among vector\ndatabases, embedding models, text splitters, retrievers, and synthesizing LLMs.\nThe challenge deepens with the rise of agentic paradigms. Modules like\nverifiers, rewriters, and rerankers-each with intricate hyperparameter\ndependencies have to be carefully tuned. Balancing tradeoffs between latency,\naccuracy, and cost becomes increasingly difficult in performance-sensitive\napplications.\n  We introduce syftr, a framework that performs efficient multi-objective\nsearch over a broad space of agentic and non-agentic RAG configurations. Using\nBayesian Optimization, syftr discovers Pareto-optimal flows that jointly\noptimize task accuracy and cost. A novel early-stopping mechanism further\nimproves efficiency by pruning clearly suboptimal candidates. Across multiple\nRAG benchmarks, syftr finds flows which are on average approximately 9 times\ncheaper while preserving most of the accuracy of the most accurate flows on the\nPareto-frontier. Furthermore, syftr's ability to design and optimize allows\nintegrating new modules, making it even easier and faster to realize\nhigh-performing generative AI pipelines."}
{"id": "2505.19043", "pdf": "https://arxiv.org/pdf/2505.19043", "abs": "https://arxiv.org/abs/2505.19043", "authors": ["Jingyuan Liu", "Zeyu Zhang", "Xuchuang Wang", "Xutong Liu", "John C. S. Lui", "Mohammad Hajiesmaili", "Carlee Joe-Wong"], "title": "Offline Clustering of Linear Bandits: Unlocking the Power of Clusters in Data-Limited Environments", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Contextual linear multi-armed bandits are a learning framework for making a\nsequence of decisions, e.g., advertising recommendations for a sequence of\narriving users. Recent works have shown that clustering these users based on\nthe similarity of their learned preferences can significantly accelerate the\nlearning. However, prior work has primarily focused on the online setting,\nwhich requires continually collecting user data, ignoring the offline data\nwidely available in many applications. To tackle these limitations, we study\nthe offline clustering of bandits (Off-ClusBand) problem, which studies how to\nuse the offline dataset to learn cluster properties and improve decision-making\nacross multiple users. The key challenge in Off-ClusBand arises from data\ninsufficiency for users: unlike the online case, in the offline case, we have a\nfixed, limited dataset to work from and thus must determine whether we have\nenough data to confidently cluster users together. To address this challenge,\nwe propose two algorithms: Off-C$^2$LUB, which we analytically show performs\nwell for arbitrary amounts of user data, and Off-CLUB, which is prone to bias\nwhen data is limited but, given sufficient data, matches a theoretical lower\nbound that we derive for the offline clustered MAB problem. We experimentally\nvalidate these results on both real and synthetic datasets."}
{"id": "2505.19355", "pdf": "https://arxiv.org/pdf/2505.19355", "abs": "https://arxiv.org/abs/2505.19355", "authors": ["Lin Tian", "Marian-Andrei Rizoiu"], "title": "Estimating Online Influence Needs Causal Modeling! Counterfactual Analysis of Social Media Engagement", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Understanding true influence in social media requires distinguishing\ncorrelation from causation--particularly when analyzing misinformation spread.\nWhile existing approaches focus on exposure metrics and network structures,\nthey often fail to capture the causal mechanisms by which external temporal\nsignals trigger engagement. We introduce a novel joint treatment-outcome\nframework that leverages existing sequential models to simultaneously adapt to\nboth policy timing and engagement effects. Our approach adapts causal inference\ntechniques from healthcare to estimate Average Treatment Effects (ATE) within\nthe sequential nature of social media interactions, tackling challenges from\nexternal confounding signals. Through our experiments on real-world\nmisinformation and disinformation datasets, we show that our models outperform\nexisting benchmarks by 15--22% in predicting engagement across diverse\ncounterfactual scenarios, including exposure adjustment, timing shifts, and\nvaried intervention durations. Case studies on 492 social media users show our\ncausal effect measure aligns strongly with the gold standard in influence\nestimation, the expert-based empirical influence."}
{"id": "2505.20273", "pdf": "https://arxiv.org/pdf/2505.20273", "abs": "https://arxiv.org/abs/2505.20273", "authors": ["Ke Yang", "ChengXiang Zhai"], "title": "Ten Principles of AI Agent Economics", "categories": ["cs.AI"], "comment": null, "summary": "The rapid rise of AI-based autonomous agents is transforming human society\nand economic systems, as these entities increasingly exhibit human-like or\nsuperhuman intelligence. From excelling at complex games like Go to tackling\ndiverse general-purpose tasks with large language and multimodal models, AI\nagents are evolving from specialized tools into dynamic participants in social\nand economic ecosystems. Their autonomy and decision-making capabilities are\npoised to impact industries, professions, and human lives profoundly, raising\ncritical questions about their integration into economic activities, potential\nethical concerns, and the balance between their utility and safety.\n  To address these challenges, this paper presents ten principles of AI agent\neconomics, offering a framework to understand how AI agents make decisions,\ninfluence social interactions, and participate in the broader economy. Drawing\non economics, decision theory, and ethics, we explore fundamental questions,\nsuch as whether AI agents might evolve from tools into independent entities,\ntheir impact on labor markets, and the ethical safeguards needed to align them\nwith human values. These principles build on existing economic theories while\naccounting for the unique traits of AI agents, providing a roadmap for their\nresponsible integration into human systems.\n  Beyond theoretical insights, this paper highlights the urgency of future\nresearch into AI trustworthiness, ethical guidelines, and regulatory oversight.\nAs we enter a transformative era, this work serves as both a guide and a call\nto action, ensuring AI agents contribute positively to human progress while\naddressing risks tied to their unprecedented capabilities."}
{"id": "2505.19053", "pdf": "https://arxiv.org/pdf/2505.19053", "abs": "https://arxiv.org/abs/2505.19053", "authors": ["Heiko Hoppe", "Lo Baty", "Louis Bouvier", "Axel Parmentier", "Maximilian Schiffer"], "title": "Structured Reinforcement Learning for Combinatorial Decision-Making", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": "29 pages, 6 figures", "summary": "Reinforcement learning (RL) is increasingly applied to real-world problems\ninvolving complex and structured decisions, such as routing, scheduling, and\nassortment planning. These settings challenge standard RL algorithms, which\nstruggle to scale, generalize, and exploit structure in the presence of\ncombinatorial action spaces. We propose Structured Reinforcement Learning\n(SRL), a novel actor-critic framework that embeds combinatorial optimization\nlayers into the actor neural network. We enable end-to-end learning of the\nactor via Fenchel-Young losses and provide a geometric interpretation of SRL as\na primal-dual algorithm in the dual of the moment polytope. Across six\nenvironments with exogenous and endogenous uncertainty, SRL matches or\nsurpasses the performance of unstructured RL and imitation learning on static\ntasks and improves over these baselines by up to 92% on dynamic problems, with\nimproved stability and convergence speed."}
{"id": "2505.19360", "pdf": "https://arxiv.org/pdf/2505.19360", "abs": "https://arxiv.org/abs/2505.19360", "authors": ["Manan Suri", "Puneet Mathur", "Nedim Lipka", "Franck Dernoncourt", "Ryan A. Rossi", "Dinesh Manocha"], "title": "ChartLens: Fine-grained Visual Attribution in Charts", "categories": ["cs.CL"], "comment": "ACL 2025 (Main)", "summary": "The growing capabilities of multimodal large language models (MLLMs) have\nadvanced tasks like chart understanding. However, these models often suffer\nfrom hallucinations, where generated text sequences conflict with the provided\nvisual data. To address this, we introduce Post-Hoc Visual Attribution for\nCharts, which identifies fine-grained chart elements that validate a given\nchart-associated response. We propose ChartLens, a novel chart attribution\nalgorithm that uses segmentation-based techniques to identify chart objects and\nemploys set-of-marks prompting with MLLMs for fine-grained visual attribution.\nAdditionally, we present ChartVA-Eval, a benchmark with synthetic and\nreal-world charts from diverse domains like finance, policy, and economics,\nfeaturing fine-grained attribution annotations. Our evaluations show that\nChartLens improves fine-grained attributions by 26-66%."}
{"id": "2505.20286", "pdf": "https://arxiv.org/pdf/2505.20286", "abs": "https://arxiv.org/abs/2505.20286", "authors": ["Jiahao Qiu", "Xuan Qi", "Tongcheng Zhang", "Xinzhe Juan", "Jiacheng Guo", "Yifu Lu", "Yimin Wang", "Zixin Yao", "Qihan Ren", "Xun Jiang", "Xing Zhou", "Dongrui Liu", "Ling Yang", "Yue Wu", "Kaixuan Huang", "Shilong Liu", "Hongru Wang", "Mengdi Wang"], "title": "Alita: Generalist Agent Enabling Scalable Agentic Reasoning with Minimal Predefinition and Maximal Self-Evolution", "categories": ["cs.AI"], "comment": "9 pages, 3 figures", "summary": "Recent advances in large language models (LLMs) have enabled agents to\nautonomously perform complex, open-ended tasks. However, many existing\nframeworks depend heavily on manually predefined tools and workflows, which\nhinder their adaptability, scalability, and generalization across domains. In\nthis work, we introduce Alita--a generalist agent designed with the principle\nof \"Simplicity is the ultimate sophistication,\" enabling scalable agentic\nreasoning through minimal predefinition and maximal self-evolution. For minimal\npredefinition, Alita is equipped with only one component for direct\nproblem-solving, making it much simpler and neater than previous approaches\nthat relied heavily on hand-crafted, elaborate tools and workflows. This clean\ndesign enhances its potential to generalize to challenging questions, without\nbeing limited by tools. For Maximal self-evolution, we enable the creativity of\nAlita by providing a suite of general-purpose components to autonomously\nconstruct, refine, and reuse external capabilities by generating task-related\nmodel context protocols (MCPs) from open source, which contributes to scalable\nagentic reasoning. Notably, Alita achieves 75.15% pass@1 and 87.27% pass@3\naccuracy, which is top-ranking among general-purpose agents, on the GAIA\nbenchmark validation dataset, 74.00% and 52.00% pass@1, respectively, on\nMathvista and PathVQA, outperforming many agent systems with far greater\ncomplexity. More details will be updated at\n$\\href{https://github.com/CharlesQ9/Alita}{https://github.com/CharlesQ9/Alita}$."}
{"id": "2505.19054", "pdf": "https://arxiv.org/pdf/2505.19054", "abs": "https://arxiv.org/abs/2505.19054", "authors": ["Zhuochen Liu", "Rahul Jain", "Quan Nguyen"], "title": "Reduce Computational Cost In Deep Reinforcement Learning Via Randomized Policy Learning", "categories": ["cs.LG"], "comment": "8 pages main, 12 pages total, 6 figures", "summary": "Recent advancements in reinforcement learning (RL) have leveraged neural\nnetworks to achieve state-of-the-art performance across various control tasks.\nHowever, these successes often come at the cost of significant computational\nresources, as training deep neural networks requires substantial time and data.\nIn this paper, we introduce an actor-critic algorithm that utilizes randomized\nneural networks to drastically reduce computational costs while maintaining\nstrong performance. Despite its simple architecture, our method effectively\nsolves a range of control problems, including the locomotion control of a\nhighly dynamic 12-motor quadruped robot, and achieves results comparable to\nleading algorithms such as Proximal Policy Optimization (PPO). Notably, our\napproach does not outperform other algorithms in terms of sample efficnency but\nrather in terms of wall-clock training time. That is, although our algorithm\nrequires more timesteps to converge to an optimal policy, the actual time\nrequired for training turns out to be lower."}
{"id": "2505.19376", "pdf": "https://arxiv.org/pdf/2505.19376", "abs": "https://arxiv.org/abs/2505.19376", "authors": ["Lance Ying", "Almog Hillel", "Ryan Truong", "Vikash K. Mansinghka", "Joshua B. Tenenbaum", "Tan Zhi-Xuan"], "title": "Belief Attribution as Mental Explanation: The Role of Accuracy, Informativity, and Causality", "categories": ["cs.CL"], "comment": "8 pages, 3 figures; oral presentation at CogSci 2025", "summary": "A key feature of human theory-of-mind is the ability to attribute beliefs to\nother agents as mentalistic explanations for their behavior. But given the wide\nvariety of beliefs that agents may hold about the world and the rich language\nwe can use to express them, which specific beliefs are people inclined to\nattribute to others? In this paper, we investigate the hypothesis that people\nprefer to attribute beliefs that are good explanations for the behavior they\nobserve. We develop a computational model that quantifies the explanatory\nstrength of a (natural language) statement about an agent's beliefs via three\nfactors: accuracy, informativity, and causal relevance to actions, each of\nwhich can be computed from a probabilistic generative model of belief-driven\nbehavior. Using this model, we study the role of each factor in how people\nselectively attribute beliefs to other agents. We investigate this via an\nexperiment where participants watch an agent collect keys hidden in boxes in\norder to reach a goal, then rank a set of statements describing the agent's\nbeliefs about the boxes' contents. We find that accuracy and informativity\nperform reasonably well at predicting these rankings when combined, but that\ncausal relevance is the single factor that best explains participants'\nresponses."}
{"id": "2406.17441", "pdf": "https://arxiv.org/pdf/2406.17441", "abs": "https://arxiv.org/abs/2406.17441", "authors": ["Alex Mossi", "Bojan unkovic", "Kyriakos Flouris"], "title": "A Matrix Product State Model for Simultaneous Classification and Generation", "categories": ["quant-ph", "cs.AI", "math-ph", "math.MP", "physics.comp-ph", "stat.ML"], "comment": null, "summary": "Quantum machine learning (QML) is a rapidly expanding field that merges the\nprinciples of quantum computing with the techniques of machine learning. One of\nthe powerful mathematical frameworks in this domain is tensor networks. These\nnetworks are used to approximate high-order tensors by contracting tensors with\nlower ranks. Initially developed for simulating quantum systems, tensor\nnetworks have become integral to quantum computing and, by extension, to QML.\nDrawing inspiration from these quantum methods, specifically the Matrix Product\nStates (MPS), we apply them in a classical machine learning setting. Their\nability to efficiently represent and manipulate complex, high-dimensional data\nmakes them effective in a supervised learning framework. Here, we present an\nMPS model, in which the MPS functions as both a classifier and a generator. The\ndual functionality of this novel MPS model permits a strategy that enhances the\ntraditional training of supervised MPS models. This framework is inspired by\ngenerative adversarial networks and is geared towards generating more realistic\nsamples by reducing outliers. In addition, our contributions offer insights\ninto the mechanics of tensor network methods for generation tasks.\nSpecifically, we discuss alternative embedding functions and a new sampling\nmethod from non-normalized MPSs."}
{"id": "2505.19058", "pdf": "https://arxiv.org/pdf/2505.19058", "abs": "https://arxiv.org/abs/2505.19058", "authors": ["Chung I Lu", "Julian Sester", "Aijia Zhang"], "title": "Distributionally Robust Deep Q-Learning", "categories": ["cs.LG", "math.OC", "q-fin.PM", "stat.ML"], "comment": null, "summary": "We propose a novel distributionally robust $Q$-learning algorithm for the\nnon-tabular case accounting for continuous state spaces where the state\ntransition of the underlying Markov decision process is subject to model\nuncertainty. The uncertainty is taken into account by considering the\nworst-case transition from a ball around a reference probability measure. To\ndetermine the optimal policy under the worst-case state transition, we solve\nthe associated non-linear Bellman equation by dualising and regularising the\nBellman operator with the Sinkhorn distance, which is then parameterized with\ndeep neural networks. This approach allows us to modify the Deep Q-Network\nalgorithm to optimise for the worst case state transition.\n  We illustrate the tractability and effectiveness of our approach through\nseveral applications, including a portfolio optimisation task based on\nS\\&{P}~500 data."}
{"id": "2505.19384", "pdf": "https://arxiv.org/pdf/2505.19384", "abs": "https://arxiv.org/abs/2505.19384", "authors": ["Seokgi Lee", "Jungjun Kim"], "title": "GSA-TTS : Toward Zero-Shot Speech Synthesis based on Gradual Style Adaptor", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "7 pages, 3 figures", "summary": "We present the gradual style adaptor TTS (GSA-TTS) with a novel style encoder\nthat gradually encodes speaking styles from an acoustic reference for zero-shot\nspeech synthesis. GSA first captures the local style of each semantic sound\nunit. Then the local styles are combined by self-attention to obtain a global\nstyle condition. This semantic and hierarchical encoding strategy provides a\nrobust and rich style representation for an acoustic model. We test GSA-TTS on\nunseen speakers and obtain promising results regarding naturalness, speaker\nsimilarity, and intelligibility. Additionally, we explore the potential of GSA\nin terms of interpretability and controllability, which stems from its\nhierarchical structure."}
{"id": "2505.17648", "pdf": "https://arxiv.org/pdf/2505.17648", "abs": "https://arxiv.org/abs/2505.17648", "authors": ["Jianhao Lin", "Lexuan Sun", "Yixin Yan"], "title": "Simulating Macroeconomic Expectations using LLM Agents", "categories": ["econ.GN", "cs.AI", "q-fin.EC"], "comment": null, "summary": "We introduce a novel framework for simulating macroeconomic expectation\nformation using Large Language Model-Empowered Agents (LLM Agents). By\nconstructing thousands of LLM Agents equipped with modules for personal\ncharacteristics, prior expectations, and knowledge, we replicate a survey\nexperiment involving households and experts on inflation and unemployment. Our\nresults show that although the expectations and thoughts generated by LLM\nAgents are more homogeneous than those of human participants, they still\neffectively capture key heterogeneity across agents and the underlying drivers\nof expectation formation. Furthermore, a module-ablation exercise highlights\nthe critical role of prior expectations in simulating such heterogeneity. This\napproach complements traditional survey methods and offers new insights into AI\nbehavioral science in macroeconomic research."}
{"id": "2505.19061", "pdf": "https://arxiv.org/pdf/2505.19061", "abs": "https://arxiv.org/abs/2505.19061", "authors": ["Chen Avin", "Zvi Lotker", "Shie Mannor", "Gil Shabat", "Hanan Shteingart", "Roey Yadgar"], "title": "Adversarial Bandit over Bandits: Hierarchical Bandits for Online Configuration Management", "categories": ["cs.LG", "cs.MA", "stat.ML"], "comment": null, "summary": "Motivated by dynamic parameter optimization in finite, but large action\n(configurations) spaces, this work studies the nonstochastic multi-armed bandit\n(MAB) problem in metric action spaces with oblivious Lipschitz adversaries. We\npropose ABoB, a hierarchical Adversarial Bandit over Bandits algorithm that can\nuse state-of-the-art existing \"flat\" algorithms, but additionally clusters\nsimilar configurations to exploit local structures and adapt to changing\nenvironments. We prove that in the worst-case scenario, such clustering\napproach cannot hurt too much and ABoB guarantees a standard worst-case regret\nbound of $O\\left(k^{\\frac{1}{2}}T^{\\frac{1}{2}}\\right)$, where $T$ is the\nnumber of rounds and $k$ is the number of arms, matching the traditional flat\napproach. However, under favorable conditions related to the algorithm\nproperties, clusters properties, and certain Lipschitz conditions, the regret\nbound can be improved to $O\\left(k^{\\frac{1}{4}}T^{\\frac{1}{2}}\\right)$.\nSimulations and experiments on a real storage system demonstrate that ABoB,\nusing standard algorithms like EXP3 and Tsallis-INF, achieves lower regret and\nfaster convergence than the flat method, up to 50% improvement in known\nprevious setups, nonstochastic and stochastic, as well as in our settings."}
{"id": "2505.19388", "pdf": "https://arxiv.org/pdf/2505.19388", "abs": "https://arxiv.org/abs/2505.19388", "authors": ["Takumi Goto", "Yusuke Sakai", "Taro Watanabe"], "title": "gec-metrics: A Unified Library for Grammatical Error Correction Evaluation", "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 System Demonstration Track, 11 pages, 9 figures", "summary": "We introduce gec-metrics, a library for using and developing grammatical\nerror correction (GEC) evaluation metrics through a unified interface. Our\nlibrary enables fair system comparisons by ensuring that everyone conducts\nevaluations using a consistent implementation. Moreover, it is designed with a\nstrong focus on API usage, making it highly extensible. It also includes\nmeta-evaluation functionalities and provides analysis and visualization\nscripts, contributing to developing GEC evaluation metrics. Our code is\nreleased under the MIT license and is also distributed as an installable\npackage. The video is available on YouTube."}
{"id": "2505.18156", "pdf": "https://arxiv.org/pdf/2505.18156", "abs": "https://arxiv.org/abs/2505.18156", "authors": ["Austin Howard"], "title": "InjectLab: A Tactical Framework for Adversarial Threat Modeling Against Large Language Models", "categories": ["cs.CR", "cs.AI", "cs.HC"], "comment": "This is an independent research whitepaper submitted as a preprint.\n  For more information, visit https://injectlab.org or\n  https://github.com/ahow2004/injectlab", "summary": "Large Language Models (LLMs) are changing the way people interact with\ntechnology. Tools like ChatGPT and Claude AI are now common in business,\nresearch, and everyday life. But with that growth comes new risks, especially\nprompt-based attacks that exploit how these models process language. InjectLab\nis a security framework designed to address that problem. This paper introduces\nInjectLab as a structured, open-source matrix that maps real-world techniques\nused to manipulate LLMs. The framework is inspired by MITRE ATT&CK and focuses\nspecifically on adversarial behavior at the prompt layer. It includes over 25\ntechniques organized under six core tactics, covering threats like instruction\noverride, identity swapping, and multi-agent exploitation. Each technique in\nInjectLab includes detection guidance, mitigation strategies, and YAML-based\nsimulation tests. A Python tool supports easy execution of prompt-based test\ncases. This paper outlines the framework's structure, compares it to other AI\nthreat taxonomies, and discusses its future direction as a practical,\ncommunity-driven foundation for securing language models."}
{"id": "2505.19068", "pdf": "https://arxiv.org/pdf/2505.19068", "abs": "https://arxiv.org/abs/2505.19068", "authors": ["Dirk Tasche"], "title": "Recalibrating binary probabilistic classifiers", "categories": ["cs.LG", "q-fin.RM", "68T09, 91G40", "I.5.1; G.3"], "comment": "16 pages, 2 figures, 1 table", "summary": "Recalibration of binary probabilistic classifiers to a target prior\nprobability is an important task in areas like credit risk management. We\nanalyse methods for recalibration from a distribution shift perspective.\nDistribution shift assumptions linked to the area under the curve (AUC) of a\nprobabilistic classifier are found to be useful for the design of meaningful\nrecalibration methods. Two new methods called parametric covariate shift with\nposterior drift (CSPD) and ROC-based quasi moment matching (QMM) are proposed\nand tested together with some other methods in an example setting. The outcomes\nof the test suggest that the QMM methods discussed in the paper can provide\nappropriately conservative results in evaluations with concave functionals like\nfor instance risk weights functions for credit risk."}
{"id": "2505.19392", "pdf": "https://arxiv.org/pdf/2505.19392", "abs": "https://arxiv.org/abs/2505.19392", "authors": ["Jade Robinson", "Jonathan K. Kummerfeld"], "title": "Simple and Effective Baselines for Code Summarisation Evaluation", "categories": ["cs.CL", "cs.AI", "cs.SE", "68T50", "I.2.7"], "comment": null, "summary": "Code documentation is useful, but writing it is time-consuming. Different\ntechniques for generating code summaries have emerged, but comparing them is\ndifficult because human evaluation is expensive and automatic metrics are\nunreliable. In this paper, we introduce a simple new baseline in which we ask\nan LLM to give an overall score to a summary. Unlike n-gram and embedding-based\nbaselines, our approach is able to consider the code when giving a score. This\nallows us to also make a variant that does not consider the reference summary\nat all, which could be used for other tasks, e.g., to evaluate the quality of\ndocumentation in code bases. We find that our method is as good or better than\nprior metrics, though we recommend using it in conjunction with embedding-based\nmethods to avoid the risk of LLM-specific bias."}
{"id": "2505.18164", "pdf": "https://arxiv.org/pdf/2505.18164", "abs": "https://arxiv.org/abs/2505.18164", "authors": ["Davide Macario", "Hulya Seferoglu", "Erdem Koyuncu"], "title": "Model-Distributed Inference for Large Language Models at the Edge", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce Model-Distributed Inference for Large-Language Models (MDI-LLM),\na novel framework designed to facilitate the deployment of state-of-the-art\nlarge-language models (LLMs) across low-power devices at the edge. This is\naccomplished by dividing the model into multiple partitions, which are then\nassigned to different devices/nodes within the network. These nodes exchange\nintermediate activation vectors via device-to-device links, enabling\ncollaborative computation. To enhance the efficiency of this process, we\npropose the \"recurrent pipeline parallelism\" technique, which reduces idle time\non each device and facilitates parallel inference during the generation of\nmultiple text sequences. By leveraging the combined computational resources of\nmultiple edge devices, MDI-LLM enables the deployment of LLMs that exceed the\nmemory capacity of individual devices, making it possible to perform inference\non low-cost hardware. Furthermore, as the number of participating devices\nincreases, MDI-LLM boosts token generation throughput and reduces memory\nconsumption per device."}
{"id": "2505.19087", "pdf": "https://arxiv.org/pdf/2505.19087", "abs": "https://arxiv.org/abs/2505.19087", "authors": ["Itamar Harel", "Yonathan Wolanowsky", "Gal Vardi", "Nathan Srebro", "Daniel Soudry"], "title": "Temperature is All You Need for Generalization in Langevin Dynamics and other Markov Processes", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We analyze the generalization gap (gap between the training and test errors)\nwhen training a potentially over-parametrized model using a Markovian\nstochastic training algorithm, initialized from some distribution $\\theta_0\n\\sim p_0$. We focus on Langevin dynamics with a positive temperature\n$\\beta^{-1}$, i.e. gradient descent on a training loss $L$ with infinitesimal\nstep size, perturbed with $\\beta^{-1}$-variances Gaussian noise, and lightly\nregularized or bounded. There, we bound the generalization gap, at any time\nduring training, by $\\sqrt{(\\beta\\mathbb{E} L (\\theta_0) + \\log(1/\\delta))/N}$\nwith probability $1-\\delta$ over the dataset, where $N$ is the sample size, and\n$\\mathbb{E} L (\\theta_0) =O(1)$ with standard initialization scaling. In\ncontrast to previous guarantees, we have no dependence on either training time\nor reliance on mixing, nor a dependence on dimensionality, gradient norms, or\nany other properties of the loss or model. This guarantee follows from a\ngeneral analysis of any Markov process-based training that has a Gibbs-style\nstationary distribution. The proof is surprisingly simple, once we observe that\nthe marginal distribution divergence from initialization remains bounded, as\nimplied by a generalized second law of thermodynamics."}
{"id": "2505.19405", "pdf": "https://arxiv.org/pdf/2505.19405", "abs": "https://arxiv.org/abs/2505.19405", "authors": ["Yan Wen", "Junfeng Guo", "Heng Huang"], "title": "CoTGuard: Using Chain-of-Thought Triggering for Copyright Protection in Multi-Agent LLM Systems", "categories": ["cs.CL", "cs.CR"], "comment": "18 pages, 1 figure", "summary": "As large language models (LLMs) evolve into autonomous agents capable of\ncollaborative reasoning and task execution, multi-agent LLM systems have\nemerged as a powerful paradigm for solving complex problems. However, these\nsystems pose new challenges for copyright protection, particularly when\nsensitive or copyrighted content is inadvertently recalled through inter-agent\ncommunication and reasoning. Existing protection techniques primarily focus on\ndetecting content in final outputs, overlooking the richer, more revealing\nreasoning processes within the agents themselves. In this paper, we introduce\nCoTGuard, a novel framework for copyright protection that leverages\ntrigger-based detection within Chain-of-Thought (CoT) reasoning. Specifically,\nwe can activate specific CoT segments and monitor intermediate reasoning steps\nfor unauthorized content reproduction by embedding specific trigger queries\ninto agent prompts. This approach enables fine-grained, interpretable detection\nof copyright violations in collaborative agent scenarios. We evaluate CoTGuard\non various benchmarks in extensive experiments and show that it effectively\nuncovers content leakage with minimal interference to task performance. Our\nfindings suggest that reasoning-level monitoring offers a promising direction\nfor safeguarding intellectual property in LLM-based agent systems."}
{"id": "2505.18174", "pdf": "https://arxiv.org/pdf/2505.18174", "abs": "https://arxiv.org/abs/2505.18174", "authors": ["Zhixin li", "Peihong Zhang", "Rui Sang", "Yuxuan Liu", "Shengchen Li"], "title": "NMCSE: Noise-Robust Multi-Modal Coupling Signal Estimation Method via Optimal Transport for Cardiovascular Disease Detection", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "Electrocardiogram (ECG) and Phonocardiogram (PCG) signals are linked by a\nlatent coupling signal representing the electrical-to-mechanical cardiac\ntransformation. While valuable for cardiovascular disease (CVD) detection, this\ncoupling signal is traditionally estimated using deconvolution methods that\namplify noise, limiting clinical utility. In this paper, we propose\nNoise-Robust Multi-Modal Coupling Signal Estimation (NMCSE), which reformulates\nthe problem as distribution matching via optimal transport theory. By jointly\noptimizing amplitude and temporal alignment, NMCSE mitigates noise\namplification without additional preprocessing. Integrated with our\nTemporal-Spatial Feature Extraction network, NMCSE enables robust multi-modal\nCVD detection. Experiments on the PhysioNet 2016 dataset with realistic\nhospital noise demonstrate that NMCSE reduces estimation errors by\napproximately 30% in Mean Squared Error while maintaining higher Pearson\nCorrelation Coefficients across all tested signal-to-noise ratios. Our approach\nachieves 97.38% accuracy and 0.98 AUC in CVD detection, outperforming\nstate-of-the-art methods and demonstrating robust performance for real-world\nclinical applications."}
{"id": "2505.19090", "pdf": "https://arxiv.org/pdf/2505.19090", "abs": "https://arxiv.org/abs/2505.19090", "authors": ["Haotian Si", "Changhua Pei", "Jianhui Li", "Dan Pei", "Gaogang Xie"], "title": "CMoS: Rethinking Time Series Prediction Through the Lens of Chunk-wise Spatial Correlations", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted by Forty-second International Conference on Machine Learning\n  (ICML'25)", "summary": "Recent advances in lightweight time series forecasting models suggest the\ninherent simplicity of time series forecasting tasks. In this paper, we present\nCMoS, a super-lightweight time series forecasting model. Instead of learning\nthe embedding of the shapes, CMoS directly models the spatial correlations\nbetween different time series chunks. Additionally, we introduce a Correlation\nMixing technique that enables the model to capture diverse spatial correlations\nwith minimal parameters, and an optional Periodicity Injection technique to\nensure faster convergence. Despite utilizing as low as 1% of the lightweight\nmodel DLinear's parameters count, experimental results demonstrate that CMoS\noutperforms existing state-of-the-art models across multiple datasets.\nFurthermore, the learned weights of CMoS exhibit great interpretability,\nproviding practitioners with valuable insights into temporal structures within\nspecific application scenarios."}
{"id": "2505.19410", "pdf": "https://arxiv.org/pdf/2505.19410", "abs": "https://arxiv.org/abs/2505.19410", "authors": ["Jiajun Zhu", "Ye Liu", "Meikai Bao", "Kai Zhang", "Yanghai Zhang", "Qi Liu"], "title": "Self-Reflective Planning with Knowledge Graphs: Enhancing LLM Reasoning Reliability for Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated remarkable\ncapabilities in natural language processing tasks, yet they remain prone to\nhallucinations when reasoning with insufficient internal knowledge. While\nintegrating LLMs with knowledge graphs (KGs) provides access to structured,\nverifiable information, existing approaches often generate incomplete or\nfactually inconsistent reasoning paths. To this end, we propose Self-Reflective\nPlanning (SRP), a framework that synergizes LLMs with KGs through iterative,\nreference-guided reasoning. Specifically, given a question and topic entities,\nSRP first searches for references to guide planning and reflection. In the\nplanning process, it checks initial relations and generates a reasoning path.\nAfter retrieving knowledge from KGs through a reasoning path, it implements\niterative reflection by judging the retrieval result and editing the reasoning\npath until the answer is correctly retrieved. Extensive experiments on three\npublic datasets demonstrate that SRP surpasses various strong baselines and\nfurther underscore its reliable reasoning ability."}
{"id": "2505.18175", "pdf": "https://arxiv.org/pdf/2505.18175", "abs": "https://arxiv.org/abs/2505.18175", "authors": ["Natia Kukhilava", "Tatia Tsmindashvili", "Rapael Kalandadze", "Anchit Gupta", "Sofio Katamadze", "Franois Brmond", "Laura M. Ferrari", "Philipp Mller", "Benedikt Emanuel Wirth"], "title": "Evaluation in EEG Emotion Recognition: State-of-the-Art Review and Unified Framework", "categories": ["eess.SP", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Electroencephalography-based Emotion Recognition (EEG-ER) has become a\ngrowing research area in recent years. Analyzing 216 papers published between\n2018 and 2023, we uncover that the field lacks a unified evaluation protocol,\nwhich is essential to fairly define the state of the art, compare new\napproaches and to track the field's progress. We report the main\ninconsistencies between the used evaluation protocols, which are related to\nground truth definition, evaluation metric selection, data splitting types\n(e.g., subject-dependent or subject-independent) and the use of different\ndatasets. Capitalizing on this state-of-the-art research, we propose a unified\nevaluation protocol, EEGain (https://github.com/EmotionLab/EEGain), which\nenables an easy and efficient evaluation of new methods and datasets. EEGain is\na novel open source software framework, offering the capability to compare -\nand thus define - state-of-the-art results. EEGain includes standardized\nmethods for data pre-processing, data splitting, evaluation metrics, and the\nability to load the six most relevant datasets (i.e., AMIGOS, DEAP, DREAMER,\nMAHNOB-HCI, SEED, SEED-IV) in EEG-ER with only a single line of code. In\naddition, we have assessed and validated EEGain using these six datasets on the\nfour most common publicly available methods (EEGNet, DeepConvNet,\nShallowConvNet, TSception). This is a significant step to make research on\nEEG-ER more reproducible and comparable, thereby accelerating the overall\nprogress of the field."}
{"id": "2505.19097", "pdf": "https://arxiv.org/pdf/2505.19097", "abs": "https://arxiv.org/abs/2505.19097", "authors": ["Xichen Ye", "Yifan Wu", "Weizhong Zhang", "Cheng Jin", "Yifan Chen"], "title": "Towards Robust Influence Functions with Flat Validation Minima", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted by ICML 2025. arXiv admin note: text overlap with\n  arXiv:2310.00902 by other authors", "summary": "The Influence Function (IF) is a widely used technique for assessing the\nimpact of individual training samples on model predictions. However, existing\nIF methods often fail to provide reliable influence estimates in deep neural\nnetworks, particularly when applied to noisy training data. This issue does not\nstem from inaccuracies in parameter change estimation, which has been the\nprimary focus of prior research, but rather from deficiencies in loss change\nestimation, specifically due to the sharpness of validation risk. In this work,\nwe establish a theoretical connection between influence estimation error,\nvalidation set risk, and its sharpness, underscoring the importance of flat\nvalidation minima for accurate influence estimation. Furthermore, we introduce\na novel estimation form of Influence Function specifically designed for flat\nvalidation minima. Experimental results across various tasks validate the\nsuperiority of our approach."}
{"id": "2505.19426", "pdf": "https://arxiv.org/pdf/2505.19426", "abs": "https://arxiv.org/abs/2505.19426", "authors": ["Wenyang Xiao", "Haoyu Zhao", "Lingxiao Huang"], "title": "The Role of Diversity in In-Context Learning for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "30 pages", "summary": "In-context learning (ICL) is a crucial capability of current large language\nmodels (LLMs), where the selection of examples plays a key role in performance.\nWhile most existing approaches focus on selecting the most similar examples to\nthe query, the impact of diversity in example selection remains underexplored.\nWe systematically investigate the role of diversity in in-context example\nselection through experiments across a range of tasks, from sentiment\nclassification to more challenging math and code problems. Experiments on\nLlama-3.1, Gemma-2, and Mistral-v0.3 families of models show that\ndiversity-aware selection methods improve performance, particularly on complex\ntasks like math and code, and enhance robustness to out-of-distribution\nqueries. To support these findings, we introduce a theoretical framework that\nexplains the benefits of incorporating diversity in in-context example\nselection."}
{"id": "2505.18177", "pdf": "https://arxiv.org/pdf/2505.18177", "abs": "https://arxiv.org/abs/2505.18177", "authors": ["Zhizhong Tan", "Jiexin Zheng", "Xingxing Yang", "Chi Zhang", "Weiping Deng", "Wenyong Wang"], "title": "FedGRec: Dynamic Spatio-Temporal Federated Graph Learning for Secure and Efficient Cross-Border Recommendations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Due to the highly sensitive nature of certain data in cross-border sharing,\ncollaborative cross-border recommendations and data sharing are often subject\nto stringent privacy protection regulations, resulting in insufficient data for\nmodel training. Consequently, achieving efficient cross-border business\nrecommendations while ensuring privacy security poses a significant challenge.\nAlthough federated learning has demonstrated broad potential in collaborative\ntraining without exposing raw data, most existing federated learning-based GNN\ntraining methods still rely on federated averaging strategies, which perform\nsuboptimally on highly heterogeneous graph data. To address this issue, we\npropose FedGRec, a privacy-preserving federated graph learning method for\ncross-border recommendations. FedGRec captures user preferences from\ndistributed multi-domain data to enhance recommendation performance across all\ndomains without privacy leakage. Specifically, FedGRec leverages collaborative\nsignals from local subgraphs associated with users or items to enrich their\nrepresentation learning. Additionally, it employs dynamic spatiotemporal\nmodeling to integrate global and local user preferences in real time based on\nbusiness recommendation states, thereby deriving the final representations of\ntarget users and candidate items. By automatically filtering relevant\nbehaviors, FedGRec effectively mitigates noise interference from unreliable\nneighbors. Furthermore, through a personalized federated aggregation strategy,\nFedGRec adapts global preferences to heterogeneous domain data, enabling\ncollaborative learning of user preferences across multiple domains. Extensive\nexperiments on three datasets demonstrate that FedGRec consistently outperforms\ncompetitive single-domain and cross-domain baselines while effectively\npreserving data privacy in cross-border recommendations."}
{"id": "2505.19105", "pdf": "https://arxiv.org/pdf/2505.19105", "abs": "https://arxiv.org/abs/2505.19105", "authors": ["Karn Tiwari", "Niladri Dutta", "N M Anoop Krishnan", "Prathosh A P"], "title": "Latent Mamba Operator for Partial Differential Equations", "categories": ["cs.LG"], "comment": "Proceedings of the 42 nd International Conference on Machine\n  Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s)", "summary": "Neural operators have emerged as powerful data-driven frameworks for solving\nPartial Differential Equations (PDEs), offering significant speedups over\nnumerical methods. However, existing neural operators struggle with scalability\nin high-dimensional spaces, incur high computational costs, and face challenges\nin capturing continuous and long-range dependencies in PDE dynamics. To address\nthese limitations, we introduce the Latent Mamba Operator (LaMO), which\nintegrates the efficiency of state-space models (SSMs) in latent space with the\nexpressive power of kernel integral formulations in neural operators. We also\nestablish a theoretical connection between state-space models (SSMs) and the\nkernel integral of neural operators. Extensive experiments across diverse PDE\nbenchmarks on regular grids, structured meshes, and point clouds covering solid\nand fluid physics datasets, LaMOs achieve consistent state-of-the-art (SOTA)\nperformance, with a 32.3\\% improvement over existing baselines in solution\noperator approximation, highlighting its efficacy in modeling complex PDE\nsolutions."}
{"id": "2505.19428", "pdf": "https://arxiv.org/pdf/2505.19428", "abs": "https://arxiv.org/abs/2505.19428", "authors": ["Abhijnan Nath", "Carine Graff", "Andrei Bachinin", "Nikhil Krishnaswamy"], "title": "Frictional Agent Alignment Framework: Slow Down and Don't Break Things", "categories": ["cs.CL"], "comment": "48 pages (main paper: 10 pages incl. Limitations and Acknowledgments;\n  references: 6 pages; appendix: 32 pages), 9 figures, 12 tables, appearing in\n  Proceedings of ACL 2025, Vienna, Austria", "summary": "AI support of collaborative interactions entails mediating potential\nmisalignment between interlocutor beliefs. Common preference alignment methods\nlike DPO excel in static settings, but struggle in dynamic collaborative tasks\nwhere the explicit signals of interlocutor beliefs are sparse and skewed. We\npropose the Frictional Agent Alignment Framework (FAAF), to generate precise,\ncontext-aware \"friction\" that prompts for deliberation and re-examination of\nexisting evidence. FAAF's two-player objective decouples from data skew: a\nfrictive-state policy identifies belief misalignments, while an intervention\npolicy crafts collaborator-preferred responses. We derive an analytical\nsolution to this objective, enabling training a single policy via a simple\nsupervised loss. Experiments on three benchmarks show FAAF outperforms\ncompetitors in producing concise, interpretable friction and in OOD\ngeneralization. By aligning LLMs to act as adaptive \"thought partners\" -- not\npassive responders -- FAAF advances scalable, dynamic human-AI collaboration.\nOur code and data can be found at https://github.com/csu-signal/FAAF_ACL."}
{"id": "2505.18178", "pdf": "https://arxiv.org/pdf/2505.18178", "abs": "https://arxiv.org/abs/2505.18178", "authors": ["Min Namgung", "Yijun Lin", "JangHyeon Lee", "Yao-Yi Chiang"], "title": "Less is More: Multimodal Region Representation via Pairwise Inter-view Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "With the increasing availability of geospatial datasets, researchers have\nexplored region representation learning (RRL) to analyze complex region\ncharacteristics. Recent RRL methods use contrastive learning (CL) to capture\nshared information between two modalities but often overlook task-relevant\nunique information specific to each modality. Such modality-specific details\ncan explain region characteristics that shared information alone cannot\ncapture. Bringing information factorization to RRL can address this by\nfactorizing multimodal data into shared and unique information. However,\nexisting factorization approaches focus on two modalities, whereas RRL can\nbenefit from various geospatial data. Extending factorization beyond two\nmodalities is non-trivial because modeling high-order relationships introduces\na combinatorial number of learning objectives, increasing model complexity. We\nintroduce Cross modal Knowledge Injected Embedding, an information\nfactorization approach for RRL that captures both shared and unique\nrepresentations. CooKIE uses a pairwise inter-view learning approach that\ncaptures high-order information without modeling high-order dependency,\navoiding exhaustive combinations. We evaluate CooKIE on three regression tasks\nand a land use classification task in New York City and Delhi, India. Results\nshow that CooKIE outperforms existing RRL methods and a factorized RRL model,\ncapturing multimodal information with fewer training parameters and\nfloating-point operations per second (FLOPs). We release the code:\nhttps://github.com/MinNamgung/CooKIE."}
{"id": "2505.19107", "pdf": "https://arxiv.org/pdf/2505.19107", "abs": "https://arxiv.org/abs/2505.19107", "authors": ["Boyan Gao", "Xin Wang", "Yibo Yang", "David Clifton"], "title": "Optimization-Inspired Few-Shot Adaptation for Large Language Models", "categories": ["cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in\nreal-world applications. However, adapting LLMs to novel tasks via fine-tuning\noften requires substantial training data and computational resources that are\nimpractical in few-shot scenarios. Existing approaches, such as in-context\nlearning and Parameter-Efficient Fine-Tuning (PEFT), face key limitations:\nin-context learning introduces additional inference computational overhead with\nlimited performance gains, while PEFT models are prone to overfitting on the\nfew demonstration examples. In this work, we reinterpret the forward pass of\nLLMs as an optimization process, a sequence of preconditioned gradient descent\nsteps refining internal representations. Based on this connection, we propose\nOptimization-Inspired Few-Shot Adaptation (OFA), integrating a parameterization\nthat learns preconditioners without introducing additional trainable\nparameters, and an objective that improves optimization efficiency by learning\npreconditioners based on a convergence bound, while simultaneously steering the\noptimization path toward the flat local minimum. Our method overcomes both\nissues of ICL-based and PEFT-based methods, and demonstrates superior\nperformance over the existing methods on a variety of few-shot adaptation tasks\nin experiments."}
{"id": "2505.19429", "pdf": "https://arxiv.org/pdf/2505.19429", "abs": "https://arxiv.org/abs/2505.19429", "authors": ["Younghan Park", "Anuj Diwan", "David Harwath", "Eunsol Choi"], "title": "Rhapsody: A Dataset for Highlight Detection in Podcasts", "categories": ["cs.CL"], "comment": null, "summary": "Podcasts have become daily companions for half a billion users. Given the\nenormous amount of podcast content available, highlights provide a valuable\nsignal that helps viewers get the gist of an episode and decide if they want to\ninvest in listening to it in its entirety. However, identifying highlights\nautomatically is challenging due to the unstructured and long-form nature of\nthe content. We introduce Rhapsody, a dataset of 13K podcast episodes paired\nwith segment-level highlight scores derived from YouTube's 'most replayed'\nfeature. We frame the podcast highlight detection as a segment-level binary\nclassification task. We explore various baseline approaches, including\nzero-shot prompting of language models and lightweight finetuned language\nmodels using segment-level classification heads. Our experimental results\nindicate that even state-of-the-art language models like GPT-4o and Gemini\nstruggle with this task, while models finetuned with in-domain data\nsignificantly outperform their zero-shot performance. The finetuned model\nbenefits from leveraging both speech signal features and transcripts. These\nfindings highlight the challenges for fine-grained information access in\nlong-form spoken media."}
{"id": "2505.18179", "pdf": "https://arxiv.org/pdf/2505.18179", "abs": "https://arxiv.org/abs/2505.18179", "authors": ["Ata Akbari Asanjan", "Olivia Alexander", "Tom Berg", "Clara Zhang", "Matt Yang", "Jad Makki", "Disha Shidham", "Srija Chakraborty", "William Bender", "Stephen Peng", "Arun Ravindran", "Olivier Raiman", "David Potere", "David Bell"], "title": "GAIA: A Foundation Model for Operational Atmospheric Dynamics", "categories": ["cs.LG", "cs.AI"], "comment": "14 pages, 7 figures", "summary": "We present the GAIA (Geospatial Artificial Intelligence for Atmospheres)\nFoundation Model, a novel model that combines masked autoencoders (MAE) and\nself-DIstillation with NO labels (DINO) for analyzing global atmospheric\npatterns in satellite imagery. By integrating these complementary\nself-supervised learning approaches, our model simultaneously captures both\nlocal features and global dependencies. We address two critical challenges in\nsatellite data analysis: reconstructing missing regions and estimating\nprecipitation patterns as our first downstream tasks. The model demonstrates\nsuperior temporal pattern capture compared to standard MAE approaches, while\nmaintaining robust performance in downstream tasks. Our experimental results\nshow strong gap-filling capabilities across varying mask ratios and accurate\nprecipitation estimation with limited training data, achieving a false alarm\nratio of 0.088 and structural similarity of 0.881. This work represents an\nadvancement in self-supervised learning for atmospheric science, providing a\nfoundation for improved weather monitoring and climate analysis. The trained\nmodel weights and accompanying code are publicly available as open-source on\nHugging Face here: https://huggingface.co/bcg-usra-nasa-gaia/GAIA-v1."}
{"id": "2505.19115", "pdf": "https://arxiv.org/pdf/2505.19115", "abs": "https://arxiv.org/abs/2505.19115", "authors": ["Brian Chmiel", "Maxim Fishman", "Ron Banner", "Daniel Soudry"], "title": "FP4 All the Way: Fully Quantized Training of LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We demonstrate, for the first time, fully quantized training (FQT) of large\nlanguage models (LLMs) using predominantly 4-bit floating-point (FP4) precision\nfor weights, activations, and gradients on datasets up to 200 billion tokens.\nWe extensively investigate key design choices for FP4, including block sizes,\nscaling formats, and rounding methods. Our analysis shows that the NVFP4\nformat, where each block of 16 FP4 values (E2M1) shares a scale represented in\nE4M3, provides optimal results. We use stochastic rounding for backward and\nupdate passes and round-to-nearest for the forward pass to enhance stability.\nAdditionally, we identify a theoretical and empirical threshold for effective\nquantized training: when the gradient norm falls below approximately $\\sqrt{3}$\ntimes the quantization noise, quantized training becomes less effective.\nLeveraging these insights, we successfully train a 7-billion-parameter model on\n256 Intel Gaudi2 accelerators. The resulting FP4-trained model achieves\ndownstream task performance comparable to a standard BF16 baseline, confirming\nthat FP4 training is a practical and highly efficient approach for large-scale\nLLM training. A reference implementation is supplied in\nhttps://github.com/Anonymous1252022/fp4-all-the-way ."}
{"id": "2505.19430", "pdf": "https://arxiv.org/pdf/2505.19430", "abs": "https://arxiv.org/abs/2505.19430", "authors": ["Keane Ong", "Rui Mao", "Deeksha Varshney", "Paul Pu Liang", "Erik Cambria", "Gianmarco Mengaldo"], "title": "Deriving Strategic Market Insights with Large Language Models: A Benchmark for Forward Counterfactual Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Counterfactual reasoning typically involves considering alternatives to\nactual events. While often applied to understand past events, a distinct\nform-forward counterfactual reasoning-focuses on anticipating plausible future\ndevelopments. This type of reasoning is invaluable in dynamic financial\nmarkets, where anticipating market developments can powerfully unveil potential\nrisks and opportunities for stakeholders, guiding their decision-making.\nHowever, performing this at scale is challenging due to the cognitive demands\ninvolved, underscoring the need for automated solutions. Large Language Models\n(LLMs) offer promise, but remain unexplored for this application. To address\nthis gap, we introduce a novel benchmark, Fin-Force-FINancial FORward\nCounterfactual Evaluation. By curating financial news headlines and providing\nstructured evaluation, Fin-Force supports LLM based forward counterfactual\ngeneration. This paves the way for scalable and automated solutions for\nexploring and anticipating future market developments, thereby providing\nstructured insights for decision-making. Through experiments on Fin-Force, we\nevaluate state-of-the-art LLMs and counterfactual generation methods, analyzing\ntheir limitations and proposing insights for future research."}
{"id": "2505.18181", "pdf": "https://arxiv.org/pdf/2505.18181", "abs": "https://arxiv.org/abs/2505.18181", "authors": ["Yunrui Li", "Hao Xu", "Pengyu Hong"], "title": "2DNMRGym: An Annotated Experimental Dataset for Atom-Level Molecular Representation Learning in 2D NMR via Surrogate Supervision", "categories": ["cs.LG", "cs.AI", "cs.CE", "physics.chem-ph"], "comment": null, "summary": "Two-dimensional (2D) Nuclear Magnetic Resonance (NMR) spectroscopy,\nparticularly Heteronuclear Single Quantum Coherence (HSQC) spectroscopy, plays\na critical role in elucidating molecular structures, interactions, and\nelectronic properties. However, accurately interpreting 2D NMR data remains\nlabor-intensive and error-prone, requiring highly trained domain experts,\nespecially for complex molecules. Machine Learning (ML) holds significant\npotential in 2D NMR analysis by learning molecular representations and\nrecognizing complex patterns from data. However, progress has been limited by\nthe lack of large-scale and high-quality annotated datasets. In this work, we\nintroduce 2DNMRGym, the first annotated experimental dataset designed for\nML-based molecular representation learning in 2D NMR. It includes over 22,000\nHSQC spectra, along with the corresponding molecular graphs and SMILES strings.\nUniquely, 2DNMRGym adopts a surrogate supervision setup: models are trained\nusing algorithm-generated annotations derived from a previously validated\nmethod and evaluated on a held-out set of human-annotated gold-standard labels.\nThis enables rigorous assessment of a model's ability to generalize from\nimperfect supervision to expert-level interpretation. We provide benchmark\nresults using a series of 2D and 3D GNN and GNN transformer models,\nestablishing a strong foundation for future work. 2DNMRGym supports scalable\nmodel training and introduces a chemically meaningful benchmark for evaluating\natom-level molecular representations in NMR-guided structural tasks. Our data\nand code is open-source and available on Huggingface and Github."}
{"id": "2505.19133", "pdf": "https://arxiv.org/pdf/2505.19133", "abs": "https://arxiv.org/abs/2505.19133", "authors": ["Yan Xia", "Hao Feng", "Hongwei Sun", "Junjie Wang", "Qicong Hu"], "title": "Fast and Accurate Power Load Data Completion via Regularization-optimized Low-Rank Factorization", "categories": ["cs.LG"], "comment": null, "summary": "Low-rank representation learning has emerged as a powerful tool for\nrecovering missing values in power load data due to its ability to exploit the\ninherent low-dimensional structures of spatiotemporal measurements. Among\nvarious techniques, low-rank factorization models are favoured for their\nefficiency and interpretability. However, their performance is highly sensitive\nto the choice of regularization parameters, which are typically fixed or\nmanually tuned, resulting in limited generalization capability or slow\nconvergence in practical scenarios. In this paper, we propose a\nRegularization-optimized Low-Rank Factorization, which introduces a\nProportional-Integral-Derivative controller to adaptively adjust the\nregularization coefficient. Furthermore, we provide a detailed algorithmic\ncomplexity analysis, showing that our method preserves the computational\nefficiency of stochastic gradient descent while improving adaptivity.\nExperimental results on real-world power load datasets validate the superiority\nof our method in both imputation accuracy and training efficiency compared to\nexisting baselines."}
{"id": "2505.19435", "pdf": "https://arxiv.org/pdf/2505.19435", "abs": "https://arxiv.org/abs/2505.19435", "authors": ["Zhihong Pan", "Kai Zhang", "Yuze Zhao", "Yupeng Han"], "title": "Route to Reason: Adaptive Routing for LLM and Reasoning Strategy Selection", "categories": ["cs.CL"], "comment": null, "summary": "The inherent capabilities of a language model (LM) and the reasoning\nstrategies it employs jointly determine its performance in reasoning tasks.\nWhile test-time scaling is regarded as an effective approach to tackling\ncomplex reasoning tasks, it incurs substantial computational costs and often\nleads to \"overthinking\", where models become trapped in \"thought pitfalls\". To\naddress this challenge, we propose Route-To-Reason (RTR), a novel unified\nrouting framework that dynamically allocates both LMs and reasoning strategies\naccording to task difficulty under budget constraints. RTR learns compressed\nrepresentations of both expert models and reasoning strategies, enabling their\njoint and adaptive selection at inference time. This method is low-cost, highly\nflexible, and can be seamlessly extended to arbitrary black-box or white-box\nmodels and strategies, achieving true plug-and-play functionality. Extensive\nexperiments across seven open source models and four reasoning strategies\ndemonstrate that RTR achieves an optimal trade-off between accuracy and\ncomputational efficiency among all baselines, achieving higher accuracy than\nthe best single model while reducing token usage by over 60%."}
{"id": "2505.18188", "pdf": "https://arxiv.org/pdf/2505.18188", "abs": "https://arxiv.org/abs/2505.18188", "authors": ["Beck LaBash", "Shahriar Khushrushahi", "Fabian Ruehle"], "title": "Improving Generative Inverse Design of Rectangular Patch Antennas with Test Time Optimization", "categories": ["eess.SP", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "comment": "Code and dataset available at\n  https://github.com/becklabs/patch-antenna-tto", "summary": "We propose a two-stage deep learning framework for the inverse design of\nrectangular patch antennas. Our approach leverages generative modeling to learn\na latent representation of antenna frequency response curves and conditions a\nsubsequent generative model on these responses to produce feasible antenna\ngeometries. We further demonstrate that leveraging search and optimization\ntechniques at test-time improves the accuracy of the generated designs and\nenables consideration of auxiliary objectives such as manufacturability. Our\napproach generalizes naturally to different design criteria, and can be easily\nadapted to more complex geometric design spaces."}
{"id": "2505.19144", "pdf": "https://arxiv.org/pdf/2505.19144", "abs": "https://arxiv.org/abs/2505.19144", "authors": ["Yuxuan Nie", "Yutong Song", "Hong Peng"], "title": "ADGSyn: Dual-Stream Learning for Efficient Anticancer Drug Synergy Prediction", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Drug combinations play a critical role in cancer therapy by significantly\nenhancing treatment efficacy and overcoming drug resistance. However, the\ncombinatorial space of possible drug pairs grows exponentially, making\nexperimental screening highly impractical. Therefore, developing efficient\ncomputational methods to predict promising drug combinations and guide\nexperimental validation is of paramount importance. In this work, we propose\nADGSyn, an innovative method for predicting drug synergy. The key components of\nour approach include: (1) shared projection matrices combined with attention\nmechanisms to enable cross-drug feature alignment; (2) automatic mixed\nprecision (AMP)-optimized graph operations that reduce memory consumption by\n40\\% while accelerating training speed threefold; and (3) residual pathways\nstabilized by LayerNorm to ensure stable gradient propagation during training.\nEvaluated on the O'Neil dataset containing 13,243 drug--cell line combinations,\nADGSyn demonstrates superior performance over eight baseline methods. Moreover,\nthe framework supports full-batch processing of up to 256 molecular graphs on a\nsingle GPU, setting a new standard for efficiency in drug synergy prediction\nwithin the field of computational oncology."}
{"id": "2505.19439", "pdf": "https://arxiv.org/pdf/2505.19439", "abs": "https://arxiv.org/abs/2505.19439", "authors": ["Rihui Xin", "Han Liu", "Zecheng Wang", "Yupeng Zhang", "Dianbo Sui", "Xiaolin Hu", "Bingning Wang"], "title": "Surrogate Signals from Format and Length: Reinforcement Learning for Solving Mathematical Problems without Ground Truth Answers", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models have achieved remarkable success in natural language\nprocessing tasks, with Reinforcement Learning playing a key role in adapting\nthem to specific applications. However, obtaining ground truth answers for\ntraining LLMs in mathematical problem-solving is often challenging, costly, and\nsometimes unfeasible. This research delves into the utilization of format and\nlength as surrogate signals to train LLMs for mathematical problem-solving,\nbypassing the need for traditional ground truth answers.Our study shows that a\nreward function centered on format correctness alone can yield performance\nimprovements comparable to the standard GRPO algorithm in early phases.\nRecognizing the limitations of format-only rewards in the later phases, we\nincorporate length-based rewards. The resulting GRPO approach, leveraging\nformat-length surrogate signals, not only matches but surpasses the performance\nof the standard GRPO algorithm relying on ground truth answers in certain\nscenarios, achieving 40.0\\% accuracy on AIME2024 with a 7B base model. Through\nsystematic exploration and experimentation, this research not only offers a\npractical solution for training LLMs to solve mathematical problems and\nreducing the dependence on extensive ground truth data collection, but also\nreveals the essence of why our label-free approach succeeds: base model is like\nan excellent student who has already mastered mathematical and logical\nreasoning skills, but performs poorly on the test paper, it simply needs to\ndevelop good answering habits to achieve outstanding results in exams , in\nother words, to unlock the capabilities it already possesses."}
{"id": "2505.18190", "pdf": "https://arxiv.org/pdf/2505.18190", "abs": "https://arxiv.org/abs/2505.18190", "authors": ["Yuezhou Ma", "Haixu Wu", "Hang Zhou", "Huikun Weng", "Jianmin Wang", "Mingsheng Long"], "title": "PhySense: Sensor Placement Optimization for Accurate Physics Sensing", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "Physics sensing plays a central role in many scientific and engineering\ndomains, which inherently involves two coupled tasks: reconstructing dense\nphysical fields from sparse observations and optimizing scattered sensor\nplacements to observe maximum information. While deep learning has made rapid\nadvances in sparse-data reconstruction, existing methods generally omit\noptimization of sensor placements, leaving the mutual enhancement between\nreconstruction and placement on the shelf. To change this suboptimal practice,\nwe propose PhySense, a synergistic two-stage framework that learns to jointly\nreconstruct physical fields and to optimize sensor placements, both aiming for\naccurate physics sensing. The first stage involves a flow-based generative\nmodel enhanced by cross-attention to adaptively fuse sparse observations.\n\\correct{Leveraging the reconstruction feedback, }the second stage performs\nsensor placement via projected gradient descent to satisfy spatial constraints.\n\\correct{We further prove that the learning objectives of the two stages are\nconsistent with classical variance-minimization principles, providing\ntheoretical guarantees.} Extensive experiments across three challenging\nbenchmarks, especially a 3D geometry dataset, indicate PhySense achieves\nstate-of-the-art physics sensing accuracy and discovers informative sensor\nplacements previously unconsidered."}
{"id": "2505.19171", "pdf": "https://arxiv.org/pdf/2505.19171", "abs": "https://arxiv.org/abs/2505.19171", "authors": ["Atahan Karagoz"], "title": "Computational Inertia as a Conserved Quantity in Frictionless and Damped Learning Dynamics", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "We identify a conserved quantity in continuous-time optimization dynamics,\ntermed computational inertia. Defined as the sum of kinetic energy (parameter\nvelocity) and potential energy (loss), this scalar remains invariant under\nidealized, frictionless training. We formalize this conservation law, derive\nits analytic decay under damping and stochastic perturbations, and demonstrate\nits behavior in a synthetic system. The invariant offers a compact lens for\ninterpreting learning trajectories, and may inform theoretical tools for\nanalyzing convergence, stability, and training geometry."}
{"id": "2505.19440", "pdf": "https://arxiv.org/pdf/2505.19440", "abs": "https://arxiv.org/abs/2505.19440", "authors": ["Shashata Sawmya", "Micah Adler", "Nir Shavit"], "title": "The Birth of Knowledge: Emergent Features across Time, Space, and Scale in Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper studies the emergence of interpretable categorical features within\nlarge language models (LLMs), analyzing their behavior across training\ncheckpoints (time), transformer layers (space), and varying model sizes\n(scale). Using sparse autoencoders for mechanistic interpretability, we\nidentify when and where specific semantic concepts emerge within neural\nactivations. Results indicate clear temporal and scale-specific thresholds for\nfeature emergence across multiple domains. Notably, spatial analysis reveals\nunexpected semantic reactivation, with early-layer features re-emerging at\nlater layers, challenging standard assumptions about representational dynamics\nin transformer models."}
{"id": "2505.18191", "pdf": "https://arxiv.org/pdf/2505.18191", "abs": "https://arxiv.org/abs/2505.18191", "authors": ["Jonathan Dan", "Amirhossein Shahbazinia", "Christodoulos Kechris", "David Atienza"], "title": "SzCORE as a benchmark: report from the seizure detection challenge at the 2025 AI in Epilepsy and Neurological Disorders Conference", "categories": ["eess.SP", "cs.AI", "cs.LG", "cs.PF"], "comment": null, "summary": "Reliable automatic seizure detection from long-term EEG remains a challenge,\nas current machine learning models often fail to generalize across patients or\nclinical settings. Manual EEG review remains the clinical standard,\nunderscoring the need for robust models and standardized evaluation. To\nrigorously assess algorithm performance, we organized a challenge using a\nprivate dataset of continuous EEG recordings from 65 subjects (4,360 hours).\nExpert neurophysiologists annotated the data, providing ground truth for\nseizure events. Participants were required to detect seizure onset and\nduration, with evaluation based on event-based metrics, including sensitivity,\nprecision, F1-score, and false positives per day. The SzCORE framework ensured\nstandardized evaluation. The primary ranking criterion was the event-based\nF1-score, reflecting clinical relevance by balancing sensitivity and false\npositives. The challenge received 30 submissions from 19 teams, with 28\nalgorithms evaluated. Results revealed wide variability in performance, with a\ntop F1-score of 43% (sensitivity 37%, precision 45%), highlighting the ongoing\ndifficulty of seizure detection. The challenge also revealed a gap between\nreported performance and real-world evaluation, emphasizing the importance of\nrigorous benchmarking. Compared to previous challenges and commercial systems,\nthe best-performing algorithm in this contest showed improved performance.\nImportantly, the challenge platform now supports continuous benchmarking,\nenabling reproducible research, integration of new datasets, and clinical\nevaluation of seizure detection algorithms using a standardized framework."}
{"id": "2505.19183", "pdf": "https://arxiv.org/pdf/2505.19183", "abs": "https://arxiv.org/abs/2505.19183", "authors": ["A. Jung"], "title": "Federated Learning: From Theory to Practice", "categories": ["cs.LG", "stat.ML", "F.1.1; I.2.11; I.5.3"], "comment": null, "summary": "This book offers a hands-on introduction to building and understanding\nfederated learning (FL) systems. FL enables multiple devices -- such as\nsmartphones, sensors, or local computers -- to collaboratively train machine\nlearning (ML) models, while keeping their data private and local. It is a\npowerful solution when data cannot or should not be centralized due to privacy,\nregulatory, or technical reasons. The book is designed for students, engineers,\nand researchers who want to learn how to design scalable, privacy preserving FL\nsystems. Our main focus is on personalization: enabling each device to train\nits own model while still benefiting from collaboration with relevant devices.\nThis is achieved by leveraging similarities between (the learning tasks\nassociated with) devices that are encoded by the weighted edges (or links) of a\nfederated learning network (FL network). The key idea is to represent\nreal-world FL systems as networks of devices, where nodes correspond to device\nand edges represent communication links and data similarities between them. The\ntraining of personalized models for these devices can be naturally framed as a\ndistributed optimization problem. This optimization problem is referred to as\ngeneralized total variation minimization (GTVMin) and ensures that devices with\nsimilar learning tasks learn similar model parameters. Our approach is both\nmathematically principled and practically motivated. While we introduce some\nadvanced ideas from optimization theory and graph-based learning, we aim to\nkeep the book accessible. Readers are guided through the core ideas step by\nstep, with intuitive explanations."}
{"id": "2505.19472", "pdf": "https://arxiv.org/pdf/2505.19472", "abs": "https://arxiv.org/abs/2505.19472", "authors": ["Mohammad Mahdi Moradi", "Walid Ahmed", "Shuangyue Wen", "Sudhir Mudur", "Weiwei Zhang", "Yang Liu"], "title": "Balancing Computation Load and Representation Expressivity in Parallel Hybrid Neural Networks", "categories": ["cs.CL"], "comment": null, "summary": "Attention and State-Space Models (SSMs) when combined in a hybrid network in\nsequence or in parallel provide complementary strengths. In a hybrid sequential\npipeline they alternate between applying a transformer to the input and then\nfeeding its output into a SSM. This results in idle periods in the individual\ncomponents increasing end-to-end latency and lowering throughput caps. In the\nparallel hybrid architecture, the transformer operates independently in\nparallel with the SSM, and these pairs are cascaded, with output from one pair\nforming the input to the next. Two issues are (i) creating an expressive\nknowledge representation with the inherently divergent outputs from these\nseparate branches, and (ii) load balancing the computation between these\nparallel branches, while maintaining representation fidelity. In this work we\npresent FlowHN, a novel parallel hybrid network architecture that accommodates\nvarious strategies for load balancing, achieved through appropriate\ndistribution of input tokens between the two branches. Two innovative\ndifferentiating factors in FlowHN include a FLOP aware dynamic token split\nbetween the attention and SSM branches yielding efficient balance in compute\nload, and secondly, a method to fuse the highly divergent outputs from\nindividual branches for enhancing representation expressivity. Together they\nenable much better token processing speeds, avoid bottlenecks, and at the same\ntime yield significantly improved accuracy as compared to other competing\nworks. We conduct comprehensive experiments on autoregressive language modeling\nfor models with 135M, 350M, and 1B parameters. FlowHN outperforms sequential\nhybrid models and its parallel counterpart, achieving up to 4* higher Tokens\nper Second (TPS) and 2* better Model FLOPs Utilization (MFU)."}
{"id": "2505.18194", "pdf": "https://arxiv.org/pdf/2505.18194", "abs": "https://arxiv.org/abs/2505.18194", "authors": ["Yubo Peng", "Luping Xiang", "Bingxin Zhang", "Kun Yang"], "title": "Large Language Model-Driven Distributed Integrated Multimodal Sensing and Semantic Communications", "categories": ["eess.SP", "cs.AI", "cs.CV"], "comment": null, "summary": "Traditional single-modal sensing systems-based solely on either radio\nfrequency (RF) or visual data-struggle to cope with the demands of complex and\ndynamic environments. Furthermore, single-device systems are constrained by\nlimited perspectives and insufficient spatial coverage, which impairs their\neffectiveness in urban or non-line-of-sight scenarios. To overcome these\nchallenges, we propose a novel large language model (LLM)-driven distributed\nintegrated multimodal sensing and semantic communication (LLM-DiSAC) framework.\nSpecifically, our system consists of multiple collaborative sensing devices\nequipped with RF and camera modules, working together with an aggregation\ncenter to enhance sensing accuracy. First, on sensing devices, LLM-DiSAC\ndevelops an RF-vision fusion network (RVFN), which employs specialized feature\nextractors for RF and visual data, followed by a cross-attention module for\neffective multimodal integration. Second, a LLM-based semantic transmission\nnetwork (LSTN) is proposed to enhance communication efficiency, where the\nLLM-based decoder leverages known channel parameters, such as transceiver\ndistance and signal-to-noise ratio (SNR), to mitigate semantic distortion.\nThird, at the aggregation center, a transformer-based aggregation model (TRAM)\nwith an adaptive aggregation attention mechanism is developed to fuse\ndistributed features and enhance sensing accuracy. To preserve data privacy, a\ntwo-stage distributed learning strategy is introduced, allowing local model\ntraining at the device level and centralized aggregation model training using\nintermediate features. Finally, evaluations on a synthetic multi-view RF-visual\ndataset generated by the Genesis simulation engine show that LLM-DiSAC achieves\na good performance."}
{"id": "2505.19188", "pdf": "https://arxiv.org/pdf/2505.19188", "abs": "https://arxiv.org/abs/2505.19188", "authors": ["Hongxu Pan", "Shuxian Hu", "Mo Zhou", "Zhibin Wang", "Rong Gu", "Chen Tian", "Kun Yang", "Sheng Zhong"], "title": "Chordless Structure: A Pathway to Simple and Expressive GNNs", "categories": ["cs.LG"], "comment": null, "summary": "Researchers have proposed various methods of incorporating more structured\ninformation into the design of Graph Neural Networks (GNNs) to enhance their\nexpressiveness. However, these methods are either computationally expensive or\nlacking in provable expressiveness. In this paper, we observe that the chords\nincrease the complexity of the graph structure while contributing little useful\ninformation in many cases. In contrast, chordless structures are more efficient\nand effective for representing the graph. Therefore, when leveraging the\ninformation of cycles, we choose to omit the chords. Accordingly, we propose a\nChordless Structure-based Graph Neural Network (CSGNN) and prove that its\nexpressiveness is strictly more powerful than the k-hop GNN (KPGNN) with\npolynomial complexity. Experimental results on real-world datasets demonstrate\nthat CSGNN outperforms existing GNNs across various graph tasks while incurring\nlower computational costs and achieving better performance than the GNNs of\n3-WL expressiveness."}
{"id": "2505.19475", "pdf": "https://arxiv.org/pdf/2505.19475", "abs": "https://arxiv.org/abs/2505.19475", "authors": ["Mohammad Mahdi Moradi", "Hossam Amer", "Sudhir Mudur", "Weiwei Zhang", "Yang Liu", "Walid Ahmed"], "title": "Continuous Self-Improvement of Large Language Models by Test-time Training with Verifier-Driven Sample Selection", "categories": ["cs.CL"], "comment": null, "summary": "Learning to adapt pretrained language models to unlabeled,\nout-of-distribution data is a critical challenge, as models often falter on\nstructurally novel reasoning tasks even while excelling within their training\ndistribution. We introduce a new framework called VDS-TTT - Verifier-Driven\nSample Selection for Test-Time Training to efficiently address this. We use a\nlearned verifier to score a pool of generated responses and select only from\nhigh ranking pseudo-labeled examples for fine-tuned adaptation. Specifically,\nfor each input query our LLM generates N candidate answers; the verifier\nassigns a reliability score to each, and the response with the highest\nconfidence and above a fixed threshold is paired with its query for test-time\ntraining. We fine-tune only low-rank LoRA adapter parameters, ensuring\nadaptation efficiency and fast convergence. Our proposed self-supervised\nframework is the first to synthesize verifier driven test-time training data\nfor continuous self-improvement of the model. Experiments across three diverse\nbenchmarks and three state-of-the-art LLMs demonstrate that VDS-TTT yields up\nto a 32.29% relative improvement over the base model and a 6.66% gain compared\nto verifier-based methods without test-time training, highlighting its\neffectiveness and efficiency for on-the-fly large language model adaptation."}
{"id": "2505.18200", "pdf": "https://arxiv.org/pdf/2505.18200", "abs": "https://arxiv.org/abs/2505.18200", "authors": ["Fahrettin Emin Tiras", "Hayriye Serra Altinoluk"], "title": "CrossRF: A Domain-Invariant Deep Learning Approach for RF Fingerprinting", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "Radio Frequency (RF) fingerprinting offers a promising approach for drone\nidentification and security, although it suffers from significant performance\ndegradation when operating on different transmission channels. This paper\npresents CrossRF, a domain-invariant deep learning approach that addresses the\nproblem of cross-channel RF fingerprinting for Unmanned Aerial Vehicle (UAV)\nidentification. Our approach aims to minimize the domain gap between different\nRF channels by using adversarial learning to train a more robust model that\nmaintains consistent identification performance despite channel variations. We\nvalidate our approach using the UAVSig dataset, comprising real-world\nover-the-air RF signals from identical drone models operating across several\nfrequency channels, ensuring that the findings correspond to real-world\nscenarios. The experimental results show CrossRF's efficiency, achieving up to\n99.03% accuracy when adapting from Channel 3 to Channel 4, compared to only\n26.39% using conventional methods. The model maintains robust performance in\nmore difficult multi-channel scenarios (87.57% accuracy adapting from Channels\n1,3 to 2,4) and achieves 89.45% accuracy with 0.9 precision for controller\nclassification. These results confirm CrossRF's ability to significantly reduce\nperformance degradation due to cross-channel variations while maintaining high\nidentification accuracy with minimal training data requirements, making it\nparticularly suitable for practical drone security applications."}
{"id": "2505.19190", "pdf": "https://arxiv.org/pdf/2505.19190", "abs": "https://arxiv.org/abs/2505.19190", "authors": ["Jiayi Xin", "Sukwon Yun", "Jie Peng", "Inyoung Choi", "Jenna L. Ballard", "Tianlong Chen", "Qi Long"], "title": "I2MoE: Interpretable Multimodal Interaction-aware Mixture-of-Experts", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "ICML 2025 Poster", "summary": "Modality fusion is a cornerstone of multimodal learning, enabling information\nintegration from diverse data sources. However, vanilla fusion methods are\nlimited by (1) inability to account for heterogeneous interactions between\nmodalities and (2) lack of interpretability in uncovering the multimodal\ninteractions inherent in the data. To this end, we propose I2MoE (Interpretable\nMultimodal Interaction-aware Mixture of Experts), an end-to-end MoE framework\ndesigned to enhance modality fusion by explicitly modeling diverse multimodal\ninteractions, as well as providing interpretation on a local and global level.\nFirst, I2MoE utilizes different interaction experts with weakly supervised\ninteraction losses to learn multimodal interactions in a data-driven way.\nSecond, I2MoE deploys a reweighting model that assigns importance scores for\nthe output of each interaction expert, which offers sample-level and\ndataset-level interpretation. Extensive evaluation of medical and general\nmultimodal datasets shows that I2MoE is flexible enough to be combined with\ndifferent fusion techniques, consistently improves task performance, and\nprovides interpretation across various real-world scenarios. Code is available\nat https://github.com/Raina-Xin/I2MoE."}
{"id": "2505.19484", "pdf": "https://arxiv.org/pdf/2505.19484", "abs": "https://arxiv.org/abs/2505.19484", "authors": ["Ruixiang Feng", "Shen Gao", "Xiuying Chen", "Lisi Chen", "Shuo Shang"], "title": "CulFiT: A Fine-grained Cultural-aware LLM Training Paradigm via Multilingual Critique Data Synthesis", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they often exhibit a specific cultural biases, neglecting\nthe values and linguistic diversity of low-resource regions. This cultural bias\nnot only undermines universal equality, but also risks reinforcing stereotypes\nand perpetuating discrimination. To address this, we propose CulFiT, a novel\nculturally-aware training paradigm that leverages multilingual data and\nfine-grained reward modeling to enhance cultural sensitivity and inclusivity.\nOur approach synthesizes diverse cultural-related questions, constructs\ncritique data in culturally relevant languages, and employs fine-grained\nrewards to decompose cultural texts into verifiable knowledge units for\ninterpretable evaluation. We also introduce GlobalCultureQA, a multilingual\nopen-ended question-answering dataset designed to evaluate culturally-aware\nresponses in a global context. Extensive experiments on three existing\nbenchmarks and our GlobalCultureQA demonstrate that CulFiT achieves\nstate-of-the-art open-source model performance in cultural alignment and\ngeneral reasoning."}
{"id": "2505.18212", "pdf": "https://arxiv.org/pdf/2505.18212", "abs": "https://arxiv.org/abs/2505.18212", "authors": ["Barbara Puccio", "Federico Castagna", "Allan Tucker", "Pierangelo Veltri"], "title": "Towards medical AI misalignment: a preliminary study", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Despite their staggering capabilities as assistant tools, often exceeding\nhuman performances, Large Language Models (LLMs) are still prone to jailbreak\nattempts from malevolent users. Although red teaming practices have already\nidentified and helped to address several such jailbreak techniques, one\nparticular sturdy approach involving role-playing (which we named `Goofy Game')\nseems effective against most of the current LLMs safeguards. This can result in\nthe provision of unsafe content, which, although not harmful per se, might lead\nto dangerous consequences if delivered in a setting such as the medical domain.\nIn this preliminary and exploratory study, we provide an initial analysis of\nhow, even without technical knowledge of the internal architecture and\nparameters of generative AI models, a malicious user could construct a\nrole-playing prompt capable of coercing an LLM into producing incorrect (and\npotentially harmful) clinical suggestions. We aim to illustrate a specific\nvulnerability scenario, providing insights that can support future advancements\nin the field."}
{"id": "2505.19193", "pdf": "https://arxiv.org/pdf/2505.19193", "abs": "https://arxiv.org/abs/2505.19193", "authors": ["Andrea Zerio", "Maya Bechler-Speicher", "Maor Huri", "Marie Vibeke Vestergaard", "Ran Gilad-Bachrach", "Tine Jess", "Samir Bhatt", "Aleksejs Sazonovs"], "title": "Interpretable Graph Learning Over Sets of Temporally-Sparse Data", "categories": ["cs.LG"], "comment": null, "summary": "Real-world medical data often includes measurements from multiple signals\nthat are collected at irregular and asynchronous time intervals. For example,\ndifferent types of blood tests can be measured at different times and\nfrequencies, resulting in fragmented and unevenly scattered temporal data.\nSimilar issues of irregular sampling of different attributes occur in other\ndomains, such as monitoring of large systems using event log files or the\nspread of fake news on social networks. Effectively learning from such data\nrequires models that can handle sets of temporally sparse and heterogeneous\nsignals. In this paper, we propose Graph Mixing Additive Networks (GMAN), a\nnovel and interpretable-by-design model for learning over irregular sets of\ntemporal signals. Our method achieves state-of-the-art performance in\nreal-world medical tasks, including a 4-point increase in the AUROC score of\nin-hospital mortality prediction, compared to existing methods. We further\nshowcase GMAN's flexibility by applying it to a fake news detection task. We\ndemonstrate how its interpretability capabilities, including node-level,\ngraph-level, and subset-level importance, allow for transition phases detection\nand gaining medical insights with real-world high-stakes implications. Finally,\nwe provide theoretical insights on GMAN expressive power."}
{"id": "2505.19494", "pdf": "https://arxiv.org/pdf/2505.19494", "abs": "https://arxiv.org/abs/2505.19494", "authors": ["Manoj Balaji Jagadeeshan", "Prince Raj", "Pawan Goyal"], "title": "Anveshana: A New Benchmark Dataset for Cross-Lingual Information Retrieval On English Queries and Sanskrit Documents", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "The study presents a comprehensive benchmark for retrieving Sanskrit\ndocuments using English queries, focusing on the chapters of the\nSrimadbhagavatam. It employs a tripartite approach: Direct Retrieval (DR),\nTranslation-based Retrieval (DT), and Query Translation (QT), utilizing shared\nembedding spaces and advanced translation methods to enhance retrieval systems\nin a RAG framework. The study fine-tunes state-of-the-art models for Sanskrit's\nlinguistic nuances, evaluating models such as BM25, REPLUG, mDPR, ColBERT,\nContriever, and GPT-2. It adapts summarization techniques for Sanskrit\ndocuments to improve QA processing. Evaluation shows DT methods outperform DR\nand QT in handling the cross-lingual challenges of ancient texts, improving\naccessibility and understanding. A dataset of 3,400 English-Sanskrit\nquery-document pairs underpins the study, aiming to preserve Sanskrit\nscriptures and share their philosophical importance widely. Our dataset is\npublicly available at https://huggingface.co/datasets/manojbalaji1/anveshana"}
{"id": "2505.18213", "pdf": "https://arxiv.org/pdf/2505.18213", "abs": "https://arxiv.org/abs/2505.18213", "authors": ["Kaveen Hiniduma", "Dylan Ryan", "Suren Byna", "Jean Luca Bez", "Ravi Madduri"], "title": "AIDRIN 2.0: A Framework to Assess Data Readiness for AI", "categories": ["cs.CY", "cs.AI"], "comment": "3 pages, 3 figures", "summary": "AI Data Readiness Inspector (AIDRIN) is a framework to evaluate and improve\ndata preparedness for AI applications. It addresses critical data readiness\ndimensions such as data quality, bias, fairness, and privacy. This paper\ndetails enhancements to AIDRIN by focusing on user interface improvements and\nintegration with a privacy-preserving federated learning (PPFL) framework. By\nrefining the UI and enabling smooth integration with decentralized AI\npipelines, AIDRIN becomes more accessible and practical for users with varying\ntechnical expertise. Integrating with an existing PPFL framework ensures that\ndata readiness and privacy are prioritized in federated learning environments.\nA case study involving a real-world dataset demonstrates AIDRIN's practical\nvalue in identifying data readiness issues that impact AI model performance."}
{"id": "2505.19194", "pdf": "https://arxiv.org/pdf/2505.19194", "abs": "https://arxiv.org/abs/2505.19194", "authors": ["Peiran Sun"], "title": "Curvature Dynamic Black-box Attack: revisiting adversarial robustness via dynamic curvature estimation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Adversarial attack reveals the vulnerability of deep learning models. For\nabout a decade, countless attack and defense methods have been proposed,\nleading to robustified classifiers and better understanding of models. Among\nthese methods, curvature-based approaches have attracted attention because it\nis assumed that high curvature may give rise to rough decision boundary.\nHowever, the most commonly used \\textit{curvature} is the curvature of loss\nfunction, scores or other parameters from within the model as opposed to\ndecision boundary curvature, since the former can be relatively easily formed\nusing second order derivative. In this paper, we propose a new query-efficient\nmethod, dynamic curvature estimation(DCE), to estimate the decision boundary\ncurvature in a black-box setting. Our approach is based on CGBA, a black-box\nadversarial attack. By performing DCE on a wide range of classifiers, we\ndiscovered, statistically, a connection between decision boundary curvature and\nadversarial robustness. We also propose a new attack method, curvature dynamic\nblack-box attack(CDBA) with improved performance using the dynamically\nestimated curvature."}
{"id": "2505.19510", "pdf": "https://arxiv.org/pdf/2505.19510", "abs": "https://arxiv.org/abs/2505.19510", "authors": ["Dongil Yang", "Minjin Kim", "Sunghwan Kim", "Beong-woo Kwak", "Minjun Park", "Jinseok Hong", "Woontack Woo", "Jinyoung Yeo"], "title": "LLM Meets Scene Graph: Can Large Language Models Understand and Generate Scene Graphs? A Benchmark and Empirical Study", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "The remarkable reasoning and generalization capabilities of Large Language\nModels (LLMs) have paved the way for their expanding applications in embodied\nAI, robotics, and other real-world tasks. To effectively support these\napplications, grounding in spatial and temporal understanding in multimodal\nenvironments is essential. To this end, recent works have leveraged scene\ngraphs, a structured representation that encodes entities, attributes, and\ntheir relationships in a scene. However, a comprehensive evaluation of LLMs'\nability to utilize scene graphs remains limited. In this work, we introduce\nText-Scene Graph (TSG) Bench, a benchmark designed to systematically assess\nLLMs' ability to (1) understand scene graphs and (2) generate them from textual\nnarratives. With TSG Bench we evaluate 11 LLMs and reveal that, while models\nperform well on scene graph understanding, they struggle with scene graph\ngeneration, particularly for complex narratives. Our analysis indicates that\nthese models fail to effectively decompose discrete scenes from a complex\nnarrative, leading to a bottleneck when generating scene graphs. These findings\nunderscore the need for improved methodologies in scene graph generation and\nprovide valuable insights for future research. The demonstration of our\nbenchmark is available at https://tsg-bench.netlify.app. Additionally, our code\nand evaluation data are publicly available at\nhttps://anonymous.4open.science/r/TSG-Bench."}
{"id": "2505.18214", "pdf": "https://arxiv.org/pdf/2505.18214", "abs": "https://arxiv.org/abs/2505.18214", "authors": ["TaekHyun Park", "YoungJun Choi", "SeungHoon Shin", "Kwangil Lee"], "title": "LA-RCS: LLM-Agent-Based Robot Control System", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "LA-RCS (LLM-agent-based robot control system) is a sophisticated robot\ncontrol system designed to autonomously plan, work, and analyze the external\nenvironment based on user requirements by utilizing LLM-Agent. Utilizing a\ndual-agent framework, LA-RCS generates plans based on user requests, observes\nthe external environment, executes the plans, and modifies the plans as needed\nto adapt to changes in the external conditions. Additionally, LA-RCS interprets\nnatural language commands by the user and converts them into commands\ncompatible with the robot interface so that the robot can execute tasks and\nmeet user requests properly. During his process, the system autonomously\nevaluates observation results, provides feedback on the tasks, and executes\ncommands based on real-time environmental monitoring, significantly reducing\nthe need for user intervention in fulfilling requests. We categorized the\nscenarios that LA-RCS needs to perform into four distinct types and conducted a\nquantitative assessment of its performance in each scenario. The results showed\nan average success rate of 90 percent, demonstrating the system capability to\nfulfill user requests satisfactorily. For more extensive results, readers can\nvisit our project page: https://la-rcs.github.io"}
{"id": "2505.19205", "pdf": "https://arxiv.org/pdf/2505.19205", "abs": "https://arxiv.org/abs/2505.19205", "authors": ["Meher Bhaskar Madiraju", "Meher Sai Preetam Madiraju"], "title": "OptiMindTune: A Multi-Agent Framework for Intelligent Hyperparameter Optimization", "categories": ["cs.LG", "cs.AI", "cs.MA"], "comment": "7 pages, 2 tables", "summary": "Hyperparameter optimization (HPO) is a critical yet challenging aspect of\nmachine learning model development, significantly impacting model performance\nand generalization. Traditional HPO methods often struggle with high\ndimensionality, complex interdependencies, and computational expense. This\npaper introduces OptiMindTune, a novel multi-agent framework designed to\nintelligently and efficiently optimize hyperparameters. OptiMindTune leverages\nthe collaborative intelligence of three specialized AI agents -- a Recommender\nAgent, an Evaluator Agent, and a Decision Agent -- each powered by Google's\nGemini models. These agents address distinct facets of the HPO problem, from\nmodel selection and hyperparameter suggestion to robust evaluation and\nstrategic decision-making. By fostering dynamic interactions and knowledge\nsharing, OptiMindTune aims to converge to optimal hyperparameter configurations\nmore rapidly and robustly than existing single-agent or monolithic approaches.\nOur framework integrates principles from advanced large language models, and\nadaptive search to achieve scalable and intelligent AutoML. We posit that this\nmulti-agent paradigm offers a promising avenue for tackling the increasing\ncomplexity of modern machine learning model tuning."}
{"id": "2505.19511", "pdf": "https://arxiv.org/pdf/2505.19511", "abs": "https://arxiv.org/abs/2505.19511", "authors": ["Aggrey Muhebwa", "Khalid K. Osman"], "title": "Causal Distillation: Transferring Structured Explanations from Large to Compact Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large proprietary language models exhibit strong causal reasoning abilities\nthat smaller open-source models struggle to replicate. We introduce a novel\nframework for distilling causal explanations that transfers causal reasoning\nskills from a powerful teacher model to a compact open-source model. The key\nidea is to train the smaller model to develop causal reasoning abilities by\ngenerating structured cause-and-effect explanations consistent with those of\nthe teacher model. To evaluate the quality of the student-generated\nexplanations, we introduce a new metric called Causal Explanation Coherence\n(CEC) to assess the structural and logical consistency of causal reasoning.\nThis metric uses sentence-level semantic alignment to measure how well each\npart of the generated explanation corresponds to the teacher's reference,\ncapturing both faithfulness and coverage of the underlying causal chain. Our\nframework and the CEC metric provide a principled foundation for training\nsmaller models to perform robust causal reasoning and for systematically\nassessing the coherence of explanations in language model outputs."}
{"id": "2505.18215", "pdf": "https://arxiv.org/pdf/2505.18215", "abs": "https://arxiv.org/abs/2505.18215", "authors": ["Junyan Zhang", "Yiming Huang", "Shuliang Liu", "Yubo Gao", "Xuming Hu"], "title": "Do BERT-Like Bidirectional Models Still Perform Better on Text Classification in the Era of LLMs?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid adoption of LLMs has overshadowed the potential advantages of\ntraditional BERT-like models in text classification. This study challenges the\nprevailing \"LLM-centric\" trend by systematically comparing three category\nmethods, i.e., BERT-like models fine-tuning, LLM internal state utilization,\nand zero-shot inference across six high-difficulty datasets. Our findings\nreveal that BERT-like models often outperform LLMs. We further categorize\ndatasets into three types, perform PCA and probing experiments, and identify\ntask-specific model strengths: BERT-like models excel in pattern-driven tasks,\nwhile LLMs dominate those requiring deep semantics or world knowledge. Based on\nthis, we propose TaMAS, a fine-grained task selection strategy, advocating for\na nuanced, task-driven approach over a one-size-fits-all reliance on LLMs."}
{"id": "2505.19223", "pdf": "https://arxiv.org/pdf/2505.19223", "abs": "https://arxiv.org/abs/2505.19223", "authors": ["Fengqi Zhu", "Rongzhen Wang", "Shen Nie", "Xiaolu Zhang", "Chunwei Wu", "Jun Hu", "Jun Zhou", "Jianfei Chen", "Yankai Lin", "Ji-Rong Wen", "Chongxuan Li"], "title": "LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models", "categories": ["cs.LG"], "comment": null, "summary": "While Masked Diffusion Models (MDMs), such as LLaDA, present a promising\nparadigm for language modeling, there has been relatively little effort in\naligning these models with human preferences via reinforcement learning. The\nchallenge primarily arises from the high variance in Evidence Lower Bound\n(ELBO)-based likelihood estimates required for preference optimization. To\naddress this issue, we propose Variance-Reduced Preference Optimization (VRPO),\na framework that formally analyzes the variance of ELBO estimators and derives\nbounds on both the bias and variance of preference optimization gradients.\nBuilding on this theoretical foundation, we introduce unbiased variance\nreduction strategies, including optimal Monte Carlo budget allocation and\nantithetic sampling, that significantly improve the performance of MDM\nalignment. We demonstrate the effectiveness of VRPO by applying it to LLaDA,\nand the resulting model, LLaDA 1.5, outperforms its SFT-only predecessor\nconsistently and significantly across mathematical (GSM8K +4.7), code\n(HumanEval +3.0, MBPP +1.8), and alignment benchmarks (IFEval +4.0, Arena-Hard\n+4.3). Furthermore, LLaDA 1.5 demonstrates a highly competitive mathematical\nperformance compared to strong language MDMs and ARMs. Project page:\nhttps://ml-gsai.github.io/LLaDA-1.5-Demo/."}
{"id": "2505.19514", "pdf": "https://arxiv.org/pdf/2505.19514", "abs": "https://arxiv.org/abs/2505.19514", "authors": ["Yaoning Yu", "Ye Yu", "Kai Wei", "Haojing Luo", "Haohan Wang"], "title": "SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Prompt quality plays a critical role in the performance of large language\nmodels (LLMs), motivating a growing body of work on prompt optimization. Most\nexisting methods optimize prompts over a fixed dataset, assuming static input\ndistributions and offering limited support for iterative improvement. We\nintroduce SIPDO (Self-Improving Prompts through Data-Augmented Optimization), a\nclosed-loop framework for prompt learning that integrates synthetic data\ngeneration into the optimization process. SIPDO couples a synthetic data\ngenerator with a prompt optimizer, where the generator produces new examples\nthat reveal current prompt weaknesses and the optimizer incrementally refines\nthe prompt in response. This feedback-driven loop enables systematic\nimprovement of prompt performance without assuming access to external\nsupervision or new tasks. Experiments across question answering and reasoning\nbenchmarks show that SIPDO outperforms standard prompt tuning methods,\nhighlighting the value of integrating data synthesis into prompt learning\nworkflows."}
{"id": "2505.18216", "pdf": "https://arxiv.org/pdf/2505.18216", "abs": "https://arxiv.org/abs/2505.18216", "authors": ["Peggy Cellier", "Mireille Ducass", "Sbastien Ferr", "Olivier Ridoux", "W. Eric Wong"], "title": "Data Mining-Based Techniques for Software Fault Localization", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "This chapter illustrates the basic concepts of fault localization using a\ndata mining technique. It utilizes the Trityp program to illustrate the general\nmethod. Formal concept analysis and association rule are two well-known methods\nfor symbolic data mining. In their original inception, they both consider data\nin the form of an object-attribute table. In their original inception, they\nboth consider data in the form of an object-attribute table. The chapter\nconsiders a debugging process in which a program is tested against different\ntest cases. Two attributes, PASS and FAIL, represent the issue of the test\ncase. The chapter extends the analysis of data mining for fault localization\nfor the multiple fault situations. It addresses how data mining can be further\napplied to fault localization for GUI components. Unlike traditional software,\nGUI test cases are usually event sequences, and each individual event has a\nunique corresponding event handler."}
{"id": "2505.19227", "pdf": "https://arxiv.org/pdf/2505.19227", "abs": "https://arxiv.org/abs/2505.19227", "authors": ["Frederik Kunstner", "Francis Bach"], "title": "Scaling Laws for Gradient Descent and Sign Descent for Linear Bigram Models under Zipf's Law", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Recent works have highlighted optimization difficulties faced by gradient\ndescent in training the first and last layers of transformer-based language\nmodels, which are overcome by optimizers such as Adam. These works suggest that\nthe difficulty is linked to the heavy-tailed distribution of words in text\ndata, where the frequency of the $k$th most frequent word $\\pi_k$ is\nproportional to $1/k$, following Zipf's law. To better understand the impact of\nthe data distribution on training performance, we study a linear bigram model\nfor next-token prediction when the tokens follow a power law $\\pi_k \\propto\n1/k^\\alpha$ parameterized by the exponent $\\alpha > 0$. We derive optimization\nscaling laws for deterministic gradient descent and sign descent as a proxy for\nAdam as a function of the exponent $\\alpha$. Existing theoretical\ninvestigations in scaling laws assume that the eigenvalues of the data decay as\na power law with exponent $\\alpha > 1$. This assumption effectively makes the\nproblem ``finite dimensional'' as most of the loss comes from a few of the\nlargest eigencomponents. In comparison, we show that the problem is more\ndifficult when the data have heavier tails. The case $\\alpha = 1$ as found in\ntext data is ``worst-case'' for gradient descent, in that the number of\niterations required to reach a small relative error scales almost linearly with\ndimension. While the performance of sign descent also depends on the dimension,\nfor Zipf-distributed data the number of iterations scales only with the\nsquare-root of the dimension, leading to a large improvement for large\nvocabularies."}
{"id": "2505.19515", "pdf": "https://arxiv.org/pdf/2505.19515", "abs": "https://arxiv.org/abs/2505.19515", "authors": ["Lavanya Prahallad", "Radhika Mamidi"], "title": "Bias in Political Dialogue: Tagging U.S. Presidential Debates with an Extended DAMSL Framework", "categories": ["cs.CL"], "comment": "8 pages", "summary": "We present a critical discourse analysis of the 2024 U.S. presidential\ndebates, examining Donald Trump's rhetorical strategies in his interactions\nwith Joe Biden and Kamala Harris. We introduce a novel annotation framework,\nBEADS (Bias Enriched Annotation for Dialogue Structure), which systematically\nextends the DAMSL framework to capture bias driven and adversarial discourse\nfeatures in political communication. BEADS includes a domain and language\nagnostic set of tags that model ideological framing, emotional appeals, and\nconfrontational tactics. Our methodology compares detailed human annotation\nwith zero shot ChatGPT assisted tagging on verified transcripts from the Trump\nand Biden (19,219 words) and Trump and Harris (18,123 words) debates. Our\nanalysis shows that Trump consistently dominated in key categories: Challenge\nand Adversarial Exchanges, Selective Emphasis, Appeal to Fear, Political Bias,\nand Perceived Dismissiveness. These findings underscore his use of emotionally\ncharged and adversarial rhetoric to control the narrative and influence\naudience perception. In this work, we establish BEADS as a scalable and\nreproducible framework for critical discourse analysis across languages,\ndomains, and political contexts."}
{"id": "2505.18217", "pdf": "https://arxiv.org/pdf/2505.18217", "abs": "https://arxiv.org/abs/2505.18217", "authors": ["Soumya Dutta", "Smruthi Balaji", "Varada R", "Viveka Salinamakki", "Sriram Ganapathy"], "title": "ABHINAYA -- A System for Speech Emotion Recognition In Naturalistic Conditions Challenge", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "5 pages, 2 figures, 4 tables, accepted at Interspeech 2025", "summary": "Speech emotion recognition (SER) in naturalistic settings remains a challenge\ndue to the intrinsic variability, diverse recording conditions, and class\nimbalance. As participants in the Interspeech Naturalistic SER Challenge which\nfocused on these complexities, we present Abhinaya, a system integrating\nspeech-based, text-based, and speech-text models. Our approach fine-tunes\nself-supervised and speech large language models (SLLM) for speech\nrepresentations, leverages large language models (LLM) for textual context, and\nemploys speech-text modeling with an SLLM to capture nuanced emotional cues. To\ncombat class imbalance, we apply tailored loss functions and generate\ncategorical decisions through majority voting. Despite one model not being\nfully trained, the Abhinaya system ranked 4th among 166 submissions. Upon\ncompletion of training, it achieved state-of-the-art performance among\npublished results, demonstrating the effectiveness of our approach for SER in\nreal-world conditions."}
{"id": "2505.19235", "pdf": "https://arxiv.org/pdf/2505.19235", "abs": "https://arxiv.org/abs/2505.19235", "authors": ["Qinsi Wang", "Hancheng Ye", "Ming-Yu Chung", "Yudong Liu", "Yueqian Lin", "Martin Kuo", "Mingyuan Ma", "Jianyi Zhang", "Yiran Chen"], "title": "CoreMatching: A Co-adaptive Sparse Inference Framework with Token and Neuron Pruning for Comprehensive Acceleration of Vision-Language Models", "categories": ["cs.LG", "cs.CV"], "comment": "ICML 2025", "summary": "Vision-Language Models (VLMs) excel across diverse tasks but suffer from high\ninference costs in time and memory. Token sparsity mitigates inefficiencies in\ntoken usage, while neuron sparsity reduces high-dimensional computations, both\noffering promising solutions to enhance efficiency. Recently, these two\nsparsity paradigms have evolved largely in parallel, fostering the prevailing\nassumption that they function independently. However, a fundamental yet\nunderexplored question remains: Do they truly operate in isolation, or is there\na deeper underlying interplay that has yet to be uncovered? In this paper, we\nconduct the first comprehensive investigation into this question. By\nintroducing and analyzing the matching mechanism between Core Neurons and Core\nTokens, we found that key neurons and tokens for inference mutually influence\nand reinforce each other. Building on this insight, we propose CoreMatching, a\nco-adaptive sparse inference framework, which leverages the synergy between\ntoken and neuron sparsity to enhance inference efficiency. Through theoretical\nanalysis and efficiency evaluations, we demonstrate that the proposed method\nsurpasses state-of-the-art baselines on ten image understanding tasks and three\nhardware devices. Notably, on the NVIDIA Titan Xp, it achieved 5x FLOPs\nreduction and a 10x overall speedup. Code is released at\nhttps://github.com/wangqinsi1/2025-ICML-CoreMatching/tree/main."}
{"id": "2505.19528", "pdf": "https://arxiv.org/pdf/2505.19528", "abs": "https://arxiv.org/abs/2505.19528", "authors": ["Yejin Lee", "Joonghyuk Hahn", "Hyeseon Ahn", "Yo-Sub Han"], "title": "AmpleHate: Amplifying the Attention for Versatile Implicit Hate Detection", "categories": ["cs.CL", "cs.AI", "cs.CY", "68T50", "I.2.7"], "comment": "13 pages, 4 figures, Under Review", "summary": "Implicit hate speech detection is challenging due to its subtlety and\nreliance on contextual interpretation rather than explicit offensive words.\nCurrent approaches rely on contrastive learning, which are shown to be\neffective on distinguishing hate and non-hate sentences. Humans, however,\ndetect implicit hate speech by first identifying specific targets within the\ntext and subsequently interpreting how these target relate to their surrounding\ncontext. Motivated by this reasoning process, we propose AmpleHate, a novel\napproach designed to mirror human inference for implicit hate detection.\nAmpleHate identifies explicit target using a pretrained Named Entity\nRecognition model and capture implicit target information via [CLS] tokens. It\ncomputes attention-based relationships between explicit, implicit targets and\nsentence context and then, directly injects these relational vectors into the\nfinal sentence representation. This amplifies the critical signals of\ntarget-context relations for determining implicit hate. Experiments demonstrate\nthat AmpleHate achieves state-of-the-art performance, outperforming contrastive\nlearning baselines by an average of 82.14% and achieve faster convergence.\nQualitative analyses further reveal that attention patterns produced by\nAmpleHate closely align with human judgement, underscoring its interpretability\nand robustness."}
{"id": "2505.18218", "pdf": "https://arxiv.org/pdf/2505.18218", "abs": "https://arxiv.org/abs/2505.18218", "authors": ["Shuhang Xu", "Fangwei Zhong"], "title": "CoMet: Metaphor-Driven Covert Communication for Multi-Agent Language Games", "categories": ["cs.CL", "cs.AI"], "comment": "To Appear at ACL 2025 (Main)", "summary": "Metaphors are a crucial way for humans to express complex or subtle ideas by\ncomparing one concept to another, often from a different domain. However, many\nlarge language models (LLMs) struggle to interpret and apply metaphors in\nmulti-agent language games, hindering their ability to engage in covert\ncommunication and semantic evasion, which are crucial for strategic\ncommunication. To address this challenge, we introduce CoMet, a framework that\nenables LLM-based agents to engage in metaphor processing. CoMet combines a\nhypothesis-based metaphor reasoner with a metaphor generator that improves\nthrough self-reflection and knowledge integration. This enhances the agents'\nability to interpret and apply metaphors, improving the strategic and nuanced\nquality of their interactions. We evaluate CoMet on two multi-agent language\ngames - Undercover and Adversarial Taboo - which emphasize Covert Communication\nand Semantic Evasion. Experimental results demonstrate that CoMet significantly\nenhances the agents' ability to communicate strategically using metaphors."}
{"id": "2505.19238", "pdf": "https://arxiv.org/pdf/2505.19238", "abs": "https://arxiv.org/abs/2505.19238", "authors": ["Sourav Ganguly", "Arnob Ghosh", "Kishan Panaganti", "Adam Wierman"], "title": "Efficient Policy Optimization in Robust Constrained MDPs with Iteration Complexity Guarantees", "categories": ["cs.LG", "cs.AI", "cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Constrained decision-making is essential for designing safe policies in\nreal-world control systems, yet simulated environments often fail to capture\nreal-world adversities. We consider the problem of learning a policy that will\nmaximize the cumulative reward while satisfying a constraint, even when there\nis a mismatch between the real model and an accessible simulator/nominal model.\nIn particular, we consider the robust constrained Markov decision problem\n(RCMDP) where an agent needs to maximize the reward and satisfy the constraint\nagainst the worst possible stochastic model under the uncertainty set centered\naround an unknown nominal model. Primal-dual methods, effective for standard\nconstrained MDP (CMDP), are not applicable here because of the lack of the\nstrong duality property. Further, one cannot apply the standard robust\nvalue-iteration based approach on the composite value function either as the\nworst case models may be different for the reward value function and the\nconstraint value function. We propose a novel technique that effectively\nminimizes the constraint value function--to satisfy the constraints; on the\nother hand, when all the constraints are satisfied, it can simply maximize the\nrobust reward value function. We prove that such an algorithm finds a policy\nwith at most $\\epsilon$ sub-optimality and feasible policy after\n$O(\\epsilon^{-2})$ iterations. In contrast to the state-of-the-art method, we\ndo not need to employ a binary search, thus, we reduce the computation time by\nat least 4x for smaller value of discount factor ($\\gamma$) and by at least 6x\nfor larger value of $\\gamma$."}
{"id": "2505.19529", "pdf": "https://arxiv.org/pdf/2505.19529", "abs": "https://arxiv.org/abs/2505.19529", "authors": ["Tanjil Hasan Sakib", "Md. Tanzib Hosain", "Md. Kishor Morol"], "title": "Small Language Models: Architectures, Techniques, Evaluation, Problems and Future Adaptation", "categories": ["cs.CL"], "comment": "9 pages", "summary": "Small Language Models (SLMs) have gained substantial attention due to their\nability to execute diverse language tasks successfully while using fewer\ncomputer resources. These models are particularly ideal for deployment in\nlimited environments, such as mobile devices, on-device processing, and edge\nsystems. In this study, we present a complete assessment of SLMs, focussing on\ntheir design frameworks, training approaches, and techniques for lowering model\nsize and complexity. We offer a novel classification system to organize the\noptimization approaches applied for SLMs, encompassing strategies like pruning,\nquantization, and model compression. Furthermore, we assemble SLM's studies of\nevaluation suite with some existing datasets, establishing a rigorous platform\nfor measuring SLM capabilities. Alongside this, we discuss the important\ndifficulties that remain unresolved in this sector, including trade-offs\nbetween efficiency and performance, and we suggest directions for future study.\nWe anticipate this study to serve as a beneficial guide for researchers and\npractitioners who aim to construct compact, efficient, and high-performing\nlanguage models."}
{"id": "2505.18220", "pdf": "https://arxiv.org/pdf/2505.18220", "abs": "https://arxiv.org/abs/2505.18220", "authors": ["Smitha Kumar", "Michael A. Lones", "Manuel Maarek", "Hind Zantout"], "title": "Navigating Pitfalls: Evaluating LLMs in Machine Learning Programming Education", "categories": ["cs.CY", "cs.AI"], "comment": "29 pages", "summary": "The rapid advancement of Large Language Models (LLMs) has opened new avenues\nin education. This study examines the use of LLMs in supporting learning in\nmachine learning education; in particular, it focuses on the ability of LLMs to\nidentify common errors of practice (pitfalls) in machine learning code, and\ntheir ability to provide feedback that can guide learning. Using a portfolio of\ncode samples, we consider four different LLMs: one closed model and three open\nmodels. Whilst the most basic pitfalls are readily identified by all models,\nmany common pitfalls are not. They particularly struggle to identify pitfalls\nin the early stages of the ML pipeline, especially those which can lead to\ninformation leaks, a major source of failure within applied ML projects. They\nalso exhibit limited success at identifying pitfalls around model selection,\nwhich is a concept that students often struggle with when first transitioning\nfrom theory to practice. This questions the use of current LLMs to support\nmachine learning education, and also raises important questions about their use\nby novice practitioners. Nevertheless, when LLMs successfully identify pitfalls\nin code, they do provide feedback that includes advice on how to proceed,\nemphasising their potential role in guiding learners. We also compare the\ncapability of closed and open LLM models, and find that the gap is relatively\nsmall given the large difference in model sizes. This presents an opportunity\nto deploy, and potentially customise, smaller more efficient LLM models within\neducation, avoiding risks around cost and data sharing associated with\ncommercial models."}
{"id": "2505.19241", "pdf": "https://arxiv.org/pdf/2505.19241", "abs": "https://arxiv.org/abs/2505.19241", "authors": ["Xiaoqiang Lin", "Arun Verma", "Zhongxiang Dai", "Daniela Rus", "See-Kiong Ng", "Bryan Kian Hsiang Low"], "title": "ActiveDPO: Active Direct Preference Optimization for Sample-Efficient Alignment", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The recent success of using human preferences to align large language models\n(LLMs) has significantly improved their performance in various downstream tasks\nlike question answering, mathematical reasoning, and code generation. However,3\nachieving effective LLM alignment depends on high-quality human preference\ndatasets. Collecting these datasets requires human preference annotation, which\nis costly and resource-intensive, necessitating efficient active data selection\nmethods. Existing methods either lack a strong theoretical foundation or depend\non restrictive reward function assumptions (e.g., linearity). To this end, we\npropose an algorithm, ActiveDPO, that uses a theoretically grounded data\nselection criterion for non-linear reward functions while directly leveraging\nthe LLM itself to parameterize the reward model that is used for active data\nselection. As a result, ActiveDPO explicitly accounts for the influence of LLM\non data selection, unlike methods that select the data without considering the\nLLM that is being aligned, thereby leading to more effective and efficient data\ncollection. Extensive experiments show that ActiveDPO outperforms existing\nmethods across various models and datasets."}
{"id": "2505.19538", "pdf": "https://arxiv.org/pdf/2505.19538", "abs": "https://arxiv.org/abs/2505.19538", "authors": ["Yuxing Lu", "Gecheng Fu", "Wei Wu", "Xukai Zhao", "Sin Yee Goi", "Jinzhuo Wang"], "title": "DoctorRAG: Medical RAG Fusing Knowledge with Patient Analogy through Textual Gradients", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.IR", "cs.MA"], "comment": "32 pages, 5 figures, 5 tables", "summary": "Existing medical RAG systems mainly leverage knowledge from medical knowledge\nbases, neglecting the crucial role of experiential knowledge derived from\nsimilar patient cases -- a key component of human clinical reasoning. To bridge\nthis gap, we propose DoctorRAG, a RAG framework that emulates doctor-like\nreasoning by integrating both explicit clinical knowledge and implicit\ncase-based experience. DoctorRAG enhances retrieval precision by first\nallocating conceptual tags for queries and knowledge sources, together with a\nhybrid retrieval mechanism from both relevant knowledge and patient. In\naddition, a Med-TextGrad module using multi-agent textual gradients is\nintegrated to ensure that the final output adheres to the retrieved knowledge\nand patient query. Comprehensive experiments on multilingual, multitask\ndatasets demonstrate that DoctorRAG significantly outperforms strong baseline\nRAG models and gains improvements from iterative refinements. Our approach\ngenerates more accurate, relevant, and comprehensive responses, taking a step\ntowards more doctor-like medical reasoning systems."}
{"id": "2505.18221", "pdf": "https://arxiv.org/pdf/2505.18221", "abs": "https://arxiv.org/abs/2505.18221", "authors": ["Sharad Duwal", "Mir Nafis Sharear Shopnil", "Abhishek Tyagi", "Adiba Mahbub Proma"], "title": "Evidence-Grounded Multimodal Misinformation Detection with Attention-Based GNNs", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "Multimodal out-of-context (OOC) misinformation is misinformation that\nrepurposes real images with unrelated or misleading captions. Detecting such\nmisinformation is challenging because it requires resolving the context of the\nclaim before checking for misinformation. Many current methods, including LLMs\nand LVLMs, do not perform this contextualization step. LLMs hallucinate in\nabsence of context or parametric knowledge. In this work, we propose a\ngraph-based method that evaluates the consistency between the image and the\ncaption by constructing two graph representations: an evidence graph, derived\nfrom online textual evidence, and a claim graph, from the claim in the caption.\nUsing graph neural networks (GNNs) to encode and compare these representations,\nour framework then evaluates the truthfulness of image-caption pairs. We create\ndatasets for our graph-based method, evaluate and compare our baseline model\nagainst popular LLMs on the misinformation detection task. Our method scores\n$93.05\\%$ detection accuracy on the evaluation set and outperforms the\nsecond-best performing method (an LLM) by $2.82\\%$, making a case for smaller\nand task-specific methods."}
{"id": "2505.19245", "pdf": "https://arxiv.org/pdf/2505.19245", "abs": "https://arxiv.org/abs/2505.19245", "authors": ["Kevin Xu", "Issei Sato"], "title": "To CoT or To Loop? A Formal Comparison Between Chain-of-Thought and Looped Transformers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Chain-of-Thought (CoT) and Looped Transformers have been shown to empirically\nimprove performance on reasoning tasks and to theoretically enhance\nexpressivity by recursively increasing the number of computational steps.\nHowever, their comparative capabilities are still not well understood. In this\npaper, we provide a formal analysis of their respective strengths and\nlimitations. We show that Looped Transformers can efficiently simulate parallel\ncomputations for deterministic tasks, which we formalize as evaluation over\ndirected acyclic graphs. In contrast, CoT with stochastic decoding excels at\napproximate inference for compositional structures, namely self-reducible\nproblems. These separations suggest the tasks for which depth-driven recursion\nis more suitable, thereby offering practical cues for choosing between\nreasoning paradigms."}
{"id": "2505.19548", "pdf": "https://arxiv.org/pdf/2505.19548", "abs": "https://arxiv.org/abs/2505.19548", "authors": ["Xufeng Duan", "Zhaoqian Yao", "Yunhao Zhang", "Shaonan Wang", "Zhenguang G. Cai"], "title": "How Syntax Specialization Emerges in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have been found to develop surprising internal\nspecializations: Individual neurons, attention heads, and circuits become\nselectively sensitive to syntactic structure, reflecting patterns observed in\nthe human brain. While this specialization is well-documented, how it emerges\nduring training and what influences its development remains largely unknown.\n  In this work, we tap into the black box of specialization by tracking its\nformation over time. By quantifying internal syntactic consistency across\nminimal pairs from various syntactic phenomena, we identify a clear\ndevelopmental trajectory: Syntactic sensitivity emerges gradually, concentrates\nin specific layers, and exhibits a 'critical period' of rapid internal\nspecialization. This process is consistent across architectures and\ninitialization parameters (e.g., random seeds), and is influenced by model\nscale and training data. We therefore reveal not only where syntax arises in\nLLMs but also how some models internalize it during training. To support future\nresearch, we will release the code, models, and training checkpoints upon\nacceptance."}
{"id": "2505.18222", "pdf": "https://arxiv.org/pdf/2505.18222", "abs": "https://arxiv.org/abs/2505.18222", "authors": ["Hessa Alawwad"], "title": "A Domain Ontology for Modeling the Book of Purification in Islam", "categories": ["cs.DL", "cs.AI"], "comment": "9 pages", "summary": "This paper aims to address a gap in major Islamic topics by developing an\nontology for the Book of Purification in Islam. Many authoritative Islamic\ntexts begin with the Book of Purification, as it is essential for performing\nprayer (the second pillar of Islam after Shahadah, the profession of faith) and\nother religious duties such as Umrah and Hajj.\n  The ontology development strategy followed six key steps: (1) domain\nidentification, (2) knowledge acquisition, (3) conceptualization, (4)\nclassification, (5) integration and implementation, and (6) ontology\ngeneration. This paper includes examples of the constructed tables and\nclassifications.\n  The focus is on the design and analysis phases, as technical implementation\nis beyond the scope of this study. However, an initial implementation is\nprovided to illustrate the steps of the proposed strategy.\n  The developed ontology ensures reusability by formally defining and encoding\nthe key concepts, attributes, and relationships related to the Book of\nPurification. This structured representation is intended to support knowledge\nsharing and reuse."}
{"id": "2505.19247", "pdf": "https://arxiv.org/pdf/2505.19247", "abs": "https://arxiv.org/abs/2505.19247", "authors": ["Tao Wang", "Ruipeng Zhang", "Sicun Gao"], "title": "Improving Value Estimation Critically Enhances Vanilla Policy Gradient", "categories": ["cs.LG", "cs.AI", "cs.RO", "I.2.6"], "comment": "15 pages and 21 figures", "summary": "Modern policy gradient algorithms, such as TRPO and PPO, outperform vanilla\npolicy gradient in many RL tasks. Questioning the common belief that enforcing\napproximate trust regions leads to steady policy improvement in practice, we\nshow that the more critical factor is the enhanced value estimation accuracy\nfrom more value update steps in each iteration. To demonstrate, we show that by\nsimply increasing the number of value update steps per iteration, vanilla\npolicy gradient itself can achieve performance comparable to or better than PPO\nin all the standard continuous control benchmark environments. Importantly,\nthis simple change to vanilla policy gradient is significantly more robust to\nhyperparameter choices, opening up the possibility that RL algorithms may still\nbecome more effective and easier to use."}
{"id": "2505.19549", "pdf": "https://arxiv.org/pdf/2505.19549", "abs": "https://arxiv.org/abs/2505.19549", "authors": ["Derong Xu", "Yi Wen", "Pengyue Jia", "Yingyi Zhang", "wenlin zhang", "Yichao Wang", "Huifeng Guo", "Ruiming Tang", "Xiangyu Zhao", "Enhong Chen", "Tong Xu"], "title": "Towards Multi-Granularity Memory Association and Selection for Long-Term Conversational Agents", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have recently been widely adopted in\nconversational agents. However, the increasingly long interactions between\nusers and agents accumulate extensive dialogue records, making it difficult for\nLLMs with limited context windows to maintain a coherent long-term dialogue\nmemory and deliver personalized responses. While retrieval-augmented memory\nsystems have emerged to address this issue, existing methods often depend on\nsingle-granularity memory segmentation and retrieval. This approach falls short\nin capturing deep memory connections, leading to partial retrieval of useful\ninformation or substantial noise, resulting in suboptimal performance. To\ntackle these limits, we propose MemGAS, a framework that enhances memory\nconsolidation by constructing multi-granularity association, adaptive\nselection, and retrieval. MemGAS is based on multi-granularity memory units and\nemploys Gaussian Mixture Models to cluster and associate new memories with\nhistorical ones. An entropy-based router adaptively selects optimal granularity\nby evaluating query relevance distributions and balancing information\ncompleteness and noise. Retrieved memories are further refined via LLM-based\nfiltering. Experiments on four long-term memory benchmarks demonstrate that\nMemGAS outperforms state-of-the-art methods on both question answer and\nretrieval tasks, achieving superior performance across different query types\nand top-K settings."}
{"id": "2505.18223", "pdf": "https://arxiv.org/pdf/2505.18223", "abs": "https://arxiv.org/abs/2505.18223", "authors": ["Hanyu Li", "Haoyu Liu", "Tingyu Zhu", "Tianyu Guo", "Zeyu Zheng", "Xiaotie Deng", "Michael I. Jordan"], "title": "IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) show promise as data analysis agents, but\nexisting benchmarks overlook the iterative nature of the field, where experts'\ndecisions evolve with deeper insights of the dataset. To address this, we\nintroduce IDA-Bench, a novel benchmark evaluating LLM agents in multi-round\ninteractive scenarios. Derived from complex Kaggle notebooks, tasks are\npresented as sequential natural language instructions by an LLM-simulated user.\nAgent performance is judged by comparing its final numerical output to the\nhuman-derived baseline. Initial results show that even state-of-the-art coding\nagents (like Claude-3.7-thinking) succeed on < 50% of the tasks, highlighting\nlimitations not evident in single-turn tests. This work underscores the need to\nimprove LLMs' multi-round capabilities for building more reliable data analysis\nagents, highlighting the necessity of achieving a balance between instruction\nfollowing and reasoning."}
{"id": "2505.19255", "pdf": "https://arxiv.org/pdf/2505.19255", "abs": "https://arxiv.org/abs/2505.19255", "authors": ["Mingyuan Wu", "Jingcheng Yang", "Jize Jiang", "Meitang Li", "Kaizhuo Yan", "Hanchao Yu", "Minjia Zhang", "Chengxiang Zhai", "Klara Nahrstedt"], "title": "VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement Learning Finetuning (RFT) has significantly advanced the\nreasoning capabilities of large language models (LLMs) by enabling long chains\nof thought, self-correction, and effective tool use. While recent works attempt\nto extend RFT to vision-language models (VLMs), these efforts largely produce\ntext-only reasoning conditioned on static image inputs, falling short of true\nmultimodal reasoning in the response. In contrast, test-time methods like\nVisual Sketchpad incorporate visual steps but lack training mechanisms.\n  We introduce VTool-R1, the first framework that trains VLMs to generate\nmultimodal chains of thought by interleaving text and intermediate visual\nreasoning steps. VTool-R1 integrates Python-based visual editing tools into the\nRFT process, enabling VLMs to learn when and how to generate visual reasoning\nsteps that benefit final reasoning. Trained with outcome-based rewards tied to\ntask accuracy, our approach elicits strategic visual tool use for reasoning\nwithout relying on process-based supervision. Experiments on structured visual\nquestion answering over charts and tables show that VTool-R1 enhances reasoning\nperformance by teaching VLMs to \"think with images\" and generate multimodal\nchain of thoughts with tools."}
{"id": "2505.19572", "pdf": "https://arxiv.org/pdf/2505.19572", "abs": "https://arxiv.org/abs/2505.19572", "authors": ["Li Zeng", "Zeming Liu", "Chong Feng", "Heyan Huang", "Yuhang Guo"], "title": "DocMEdit: Towards Document-Level Model Editing", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 findings", "summary": "Model editing aims to correct errors and outdated knowledge in the Large\nlanguage models (LLMs) with minimal cost. Prior research has proposed a variety\nof datasets to assess the effectiveness of these model editing methods.\nHowever, most existing datasets only require models to output short phrases or\nsentences, overlooks the widespread existence of document-level tasks in the\nreal world, raising doubts about their practical usability. Aimed at addressing\nthis limitation and promoting the application of model editing in real-world\nscenarios, we propose the task of document-level model editing. To tackle such\nchallenges and enhance model capabilities in practical settings, we introduce\n\\benchmarkname, a dataset focused on document-level model editing,\ncharacterized by document-level inputs and outputs, extrapolative, and multiple\nfacts within a single edit. We propose a series of evaluation metrics and\nexperiments. The results show that the difficulties in document-level model\nediting pose challenges for existing model editing methods."}
{"id": "2505.18227", "pdf": "https://arxiv.org/pdf/2505.18227", "abs": "https://arxiv.org/abs/2505.18227", "authors": ["Zhenglun Kong", "Yize Li", "Fanhu Zeng", "Lei Xin", "Shvat Messica", "Xue Lin", "Pu Zhao", "Manolis Kellis", "Hao Tang", "Marinka Zitnik"], "title": "Token Reduction Should Go Beyond Efficiency in Generative Models -- From Vision, Language to Multimodality", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In Transformer architectures, tokens\\textemdash discrete units derived from\nraw data\\textemdash are formed by segmenting inputs into fixed-length chunks.\nEach token is then mapped to an embedding, enabling parallel attention\ncomputations while preserving the input's essential information. Due to the\nquadratic computational complexity of transformer self-attention mechanisms,\ntoken reduction has primarily been used as an efficiency strategy. This is\nespecially true in single vision and language domains, where it helps balance\ncomputational costs, memory usage, and inference latency. Despite these\nadvances, this paper argues that token reduction should transcend its\ntraditional efficiency-oriented role in the era of large generative models.\nInstead, we position it as a fundamental principle in generative modeling,\ncritically influencing both model architecture and broader applications.\nSpecifically, we contend that across vision, language, and multimodal systems,\ntoken reduction can: (i) facilitate deeper multimodal integration and\nalignment, (ii) mitigate \"overthinking\" and hallucinations, (iii) maintain\ncoherence over long inputs, and (iv) enhance training stability, etc. We\nreframe token reduction as more than an efficiency measure. By doing so, we\noutline promising future directions, including algorithm design, reinforcement\nlearning-guided token reduction, token optimization for in-context learning,\nand broader ML and scientific domains. We highlight its potential to drive new\nmodel architectures and learning strategies that improve robustness, increase\ninterpretability, and better align with the objectives of generative modeling."}
{"id": "2505.19258", "pdf": "https://arxiv.org/pdf/2505.19258", "abs": "https://arxiv.org/abs/2505.19258", "authors": ["Felipe Curcio", "Pedro Castro", "Augusto Fonseca", "Rafaela Castro", "Raquel Franco", "Eduardo Ogasawara", "Victor Stepanenko", "Fabio Porto", "Mariza Ferro", "Eduardo Bezerra"], "title": "Towards a Spatiotemporal Fusion Approach to Precipitation Nowcasting", "categories": ["cs.LG", "eess.SP"], "comment": "Accepted manuscript submitted to FUSION 2025\n  (https://fusion2025.org/)", "summary": "With the increasing availability of meteorological data from various sensors,\nnumerical models and reanalysis products, the need for efficient data\nintegration methods has become paramount for improving weather forecasts and\nhydrometeorological studies. In this work, we propose a data fusion approach\nfor precipitation nowcasting by integrating data from meteorological and rain\ngauge stations in Rio de Janeiro metropolitan area with ERA5 reanalysis data\nand GFS numerical weather prediction. We employ the spatiotemporal deep\nlearning architecture called STConvS2S, leveraging a structured dataset\ncovering a 9 x 11 grid. The study spans from January 2011 to October 2024, and\nwe evaluate the impact of integrating three surface station systems. Among the\ntested configurations, the fusion-based model achieves an F1-score of 0.2033\nfor forecasting heavy precipitation events (greater than 25 mm/h) at a one-hour\nlead time. Additionally, we present an ablation study to assess the\ncontribution of each station network and propose a refined inference strategy\nfor precipitation nowcasting, integrating the GFS numerical weather prediction\n(NWP) data with in-situ observations."}
{"id": "2505.19586", "pdf": "https://arxiv.org/pdf/2505.19586", "abs": "https://arxiv.org/abs/2505.19586", "authors": ["Dingyu Yao", "Bowen Shen", "Zheng Lin", "Wei Liu", "Jian Luan", "Bin Wang", "Weiping Wang"], "title": "TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV Cache Optimization", "categories": ["cs.CL"], "comment": null, "summary": "The Key-Value (KV) cache in generative large language models (LLMs)\nintroduces substantial memory overhead. Existing works mitigate this burden by\noffloading or compressing the KV cache. However, loading the entire cache\nincurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU\ncommunication, while aggressive compression causes notable performance\ndegradation. We identify that certain layers in the LLM need to maintain global\ninformation and are unsuitable for selective loading. In contrast, other layers\nprimarily focus on a few tokens with dominant activations that potentially\nincur substantial quantization error. This observation leads to a key insight\nthat loading dominant tokens and quantizing all tokens can complement each\nother. Building on this insight, we propose a hybrid compression method,\nTailorKV, which seamlessly integrates quantization and offloading. TailorKV\ndevelops an inference framework along with a hardware-friendly implementation\nthat leverages these complementary characteristics. Extensive long-context\nevaluations exhibit that TailorKV achieves nearly lossless performance under\naggressive compression settings, outperforming the state-of-the-art.\nParticularly, the Llama-3.1-8B with 128k context can be served within a single\nRTX 3090 GPU, reaching 82 ms per token during decoding."}
{"id": "2505.18229", "pdf": "https://arxiv.org/pdf/2505.18229", "abs": "https://arxiv.org/abs/2505.18229", "authors": ["Mingning Guo", "Mengwei Wu", "Jiarun He", "Shaoxian Li", "Haifeng Li", "Chao Tao"], "title": "BEDI: A Comprehensive Benchmark for Evaluating Embodied Agents on UAVs", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "With the rapid advancement of low-altitude remote sensing and Vision-Language\nModels (VLMs), Embodied Agents based on Unmanned Aerial Vehicles (UAVs) have\nshown significant potential in autonomous tasks. However, current evaluation\nmethods for UAV-Embodied Agents (UAV-EAs) remain constrained by the lack of\nstandardized benchmarks, diverse testing scenarios and open system interfaces.\nTo address these challenges, we propose BEDI (Benchmark for Embodied Drone\nIntelligence), a systematic and standardized benchmark designed for evaluating\nUAV-EAs. Specifically, we introduce a novel Dynamic Chain-of-Embodied-Task\nparadigm based on the perception-decision-action loop, which decomposes complex\nUAV tasks into standardized, measurable subtasks. Building on this paradigm, we\ndesign a unified evaluation framework encompassing five core sub-skills:\nsemantic perception, spatial perception, motion control, tool utilization, and\ntask planning. Furthermore, we construct a hybrid testing platform that\nintegrates static real-world environments with dynamic virtual scenarios,\nenabling comprehensive performance assessment of UAV-EAs across varied\ncontexts. The platform also offers open and standardized interfaces, allowing\nresearchers to customize tasks and extend scenarios, thereby enhancing\nflexibility and scalability in the evaluation process. Finally, through\nempirical evaluations of several state-of-the-art (SOTA) VLMs, we reveal their\nlimitations in embodied UAV tasks, underscoring the critical role of the BEDI\nbenchmark in advancing embodied intelligence research and model optimization.\nBy filling the gap in systematic and standardized evaluation within this field,\nBEDI facilitates objective model comparison and lays a robust foundation for\nfuture development in this field. Our benchmark will be released at\nhttps://github.com/lostwolves/BEDI ."}
{"id": "2505.19259", "pdf": "https://arxiv.org/pdf/2505.19259", "abs": "https://arxiv.org/abs/2505.19259", "authors": ["Hossein Zaremehrjerdi", "Shreyan Ganguly", "Ashlyn Rairdin", "Elizabeth Tranel", "Benjamin Feuer", "Juan Ignacio Di Salvo", "Srikanth Panthulugiri", "Victoria Moser", "Sarah Jones", "Joscif G Raigne", "Yanben Shen", "Heidi M. Dornath", "Aditya Balu", "Adarsh Krishnamurthy", "Asheesh K Singh", "Arti Singh", "Baskar Ganapathysubramanian", "Chinmay Hegde", "Soumik Sarkar"], "title": "Towards Large Reasoning Models for Agriculture", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Agricultural decision-making involves complex, context-specific reasoning,\nwhere choices about crops, practices, and interventions depend heavily on\ngeographic, climatic, and economic conditions. Traditional large language\nmodels (LLMs) often fall short in navigating this nuanced problem due to\nlimited reasoning capacity. We hypothesize that recent advances in large\nreasoning models (LRMs) can better handle such structured, domain-specific\ninference. To investigate this, we introduce AgReason, the first expert-curated\nopen-ended science benchmark with 100 questions for agricultural reasoning.\nEvaluations across thirteen open-source and proprietary models reveal that LRMs\noutperform conventional ones, though notable challenges persist, with the\nstrongest Gemini-based baseline achieving 36% accuracy. We also present\nAgThoughts, a large-scale dataset of 44.6K question-answer pairs generated with\nhuman oversight and equipped with synthetically generated reasoning traces.\nUsing AgThoughts, we develop AgThinker, a suite of small reasoning models that\ncan be run on consumer-grade GPUs, and show that our dataset can be effective\nin unlocking agricultural reasoning abilities in LLMs. Our project page is\nhere: https://baskargroup.github.io/Ag_reasoning/"}
{"id": "2505.19591", "pdf": "https://arxiv.org/pdf/2505.19591", "abs": "https://arxiv.org/abs/2505.19591", "authors": ["Yufan Dang", "Chen Qian", "Xueheng Luo", "Jingru Fan", "Zihao Xie", "Ruijie Shi", "Weize Chen", "Cheng Yang", "Xiaoyin Che", "Ye Tian", "Xuantang Xiong", "Lei Han", "Zhiyuan Liu", "Maosong Sun"], "title": "Multi-Agent Collaboration via Evolving Orchestration", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "Work in Progress", "summary": "Large language models (LLMs) have achieved remarkable results across diverse\ndownstream tasks, but their monolithic nature restricts scalability and\nefficiency in complex problem-solving. While recent research explores\nmulti-agent collaboration among LLMs, most approaches rely on static\norganizational structures that struggle to adapt as task complexity and agent\nnumbers grow, resulting in coordination overhead and inefficiencies. To this\nend, we propose a puppeteer-style paradigm for LLM-based multi-agent\ncollaboration, where a centralized orchestrator (\"puppeteer\") dynamically\ndirects agents (\"puppets\") in response to evolving task states. This\norchestrator is trained via reinforcement learning to adaptively sequence and\nprioritize agents, enabling flexible and evolvable collective reasoning.\nExperiments on closed- and open-domain scenarios show that this method achieves\nsuperior performance with reduced computational costs. Analyses further reveal\nthat the key improvements consistently stem from the emergence of more compact,\ncyclic reasoning structures under the orchestrator's evolution."}
{"id": "2505.18230", "pdf": "https://arxiv.org/pdf/2505.18230", "abs": "https://arxiv.org/abs/2505.18230", "authors": ["Louis Bthune", "David Vigouroux", "Yilun Du", "Rufin VanRullen", "Thomas Serre", "Victor Boutin"], "title": "Follow the Energy, Find the Path: Riemannian Metrics from Energy-Based Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "What is the shortest path between two data points lying in a high-dimensional\nspace? While the answer is trivial in Euclidean geometry, it becomes\nsignificantly more complex when the data lies on a curved manifold -- requiring\na Riemannian metric to describe the space's local curvature. Estimating such a\nmetric, however, remains a major challenge in high dimensions.\n  In this work, we propose a method for deriving Riemannian metrics directly\nfrom pretrained Energy-Based Models (EBMs) -- a class of generative models that\nassign low energy to high-density regions. These metrics define spatially\nvarying distances, enabling the computation of geodesics -- shortest paths that\nfollow the data manifold's intrinsic geometry. We introduce two novel metrics\nderived from EBMs and show that they produce geodesics that remain closer to\nthe data manifold and exhibit lower curvature distortion, as measured by\nalignment with ground-truth trajectories. We evaluate our approach on\nincreasingly complex datasets: synthetic datasets with known data density,\nrotated character images with interpretable geometry, and high-resolution\nnatural images embedded in a pretrained VAE latent space.\n  Our results show that EBM-derived metrics consistently outperform established\nbaselines, especially in high-dimensional settings. Our work is the first to\nderive Riemannian metrics from EBMs, enabling data-aware geodesics and\nunlocking scalable, geometry-driven learning for generative modeling and\nsimulation."}
{"id": "2505.19263", "pdf": "https://arxiv.org/pdf/2505.19263", "abs": "https://arxiv.org/abs/2505.19263", "authors": ["Hui Ma", "Kai Yang", "Yang Jiao"], "title": "Cellular Traffic Prediction via Byzantine-robust Asynchronous Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Network traffic prediction plays a crucial role in intelligent network\noperation. Traditional prediction methods often rely on centralized training,\nnecessitating the transfer of vast amounts of traffic data to a central server.\nThis approach can lead to latency and privacy concerns. To address these\nissues, federated learning integrated with differential privacy has emerged as\na solution to improve data privacy and model robustness in distributed\nsettings. Nonetheless, existing federated learning protocols are vulnerable to\nByzantine attacks, which may significantly compromise model robustness.\nDeveloping a robust and privacy-preserving prediction model in the presence of\nByzantine clients remains a significant challenge. To this end, we propose an\nasynchronous differential federated learning framework based on\ndistributionally robust optimization. The proposed framework utilizes multiple\nclients to train the prediction model collaboratively with local differential\nprivacy. In addition, regularization techniques have been employed to further\nimprove the Byzantine robustness of the models. We have conducted extensive\nexperiments on three real-world datasets, and the results elucidate that our\nproposed distributed algorithm can achieve superior performance over existing\nmethods."}
{"id": "2505.19598", "pdf": "https://arxiv.org/pdf/2505.19598", "abs": "https://arxiv.org/abs/2505.19598", "authors": ["Guanyu Hou", "Jiaming He", "Yinhang Zhou", "Ji Guo", "Yitong Qiao", "Rui Zhang", "Wenbo Jiang"], "title": "Evaluating Robustness of Large Audio Language Models to Audio Injection: An Empirical Study", "categories": ["cs.CL"], "comment": null, "summary": "Large Audio-Language Models (LALMs) are increasingly deployed in real-world\napplications, yet their robustness against malicious audio injection attacks\nremains underexplored. This study systematically evaluates five leading LALMs\nacross four attack scenarios: Audio Interference Attack, Instruction Following\nAttack, Context Injection Attack, and Judgment Hijacking Attack. Using metrics\nlike Defense Success Rate, Context Robustness Score, and Judgment Robustness\nIndex, their vulnerabilities and resilience were quantitatively assessed.\nExperimental results reveal significant performance disparities among models;\nno single model consistently outperforms others across all attack types. The\nposition of malicious content critically influences attack effectiveness,\nparticularly when placed at the beginning of sequences. A negative correlation\nbetween instruction-following capability and robustness suggests models\nadhering strictly to instructions may be more susceptible, contrasting with\ngreater resistance by safety-aligned models. Additionally, system prompts show\nmixed effectiveness, indicating the need for tailored strategies. This work\nintroduces a benchmark framework and highlights the importance of integrating\nrobustness into training pipelines. Findings emphasize developing multi-modal\ndefenses and architectural designs that decouple capability from susceptibility\nfor secure LALMs deployment."}
{"id": "2505.18231", "pdf": "https://arxiv.org/pdf/2505.18231", "abs": "https://arxiv.org/abs/2505.18231", "authors": ["Donghyun Son", "Euntae Choi", "Sungjoo Yoo"], "title": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Model (LLM) inference is typically memory-intensive,\nespecially when processing large batch sizes and long sequences, due to the\nlarge size of key-value (KV) cache. Vector Quantization (VQ) is recently\nadopted to alleviate this issue, but we find that the existing approach is\nsusceptible to distribution shift due to its reliance on calibration datasets.\nTo address this limitation, we introduce NSNQuant, a calibration-free Vector\nQuantization (VQ) technique designed for low-bit compression of the KV cache.\nBy applying a three-step transformation-1) a token-wise normalization\n(Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise\nnormalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns\nthe token distribution with the standard normal distribution. This alignment\nenables robust, calibration-free vector quantization using a single reusable\ncodebook. Extensive experiments show that NSNQuant consistently outperforms\nprior methods in both 1-bit and 2-bit settings, offering strong generalization\nand up to 3$\\times$ throughput gain over full-precision baselines."}
{"id": "2505.19281", "pdf": "https://arxiv.org/pdf/2505.19281", "abs": "https://arxiv.org/abs/2505.19281", "authors": ["Yuzheng Hu", "Fan Wu", "Haotian Ye", "David Forsyth", "James Zou", "Nan Jiang", "Jiaqi W. Ma", "Han Zhao"], "title": "A Snapshot of Influence: A Local Data Attribution Framework for Online Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Online reinforcement learning (RL) excels in complex, safety-critical\ndomains, yet it faces challenges such as sample inefficiency, training\ninstability, and a lack of interpretability. Data attribution offers a\nprincipled way to trace model behavior back to individual training samples.\nHowever, in online RL, each training sample not only drives policy updates but\nalso influences future data collection, violating the fixed dataset assumption\nin existing attribution methods. In this paper, we initiate the study of data\nattribution for online RL, focusing on the widely used Proximal Policy\nOptimization (PPO) algorithm. We start by establishing a local attribution\nframework, interpreting model checkpoints with respect to the records in the\nrecent training buffer. We design two target functions, capturing agent action\nand cumulative return respectively, and measure each record's contribution\nthrough gradient similarity between its training loss and these targets. We\ndemonstrate the power of this framework through three concrete applications:\ndiagnosis of learning, temporal analysis of behavior formation, and targeted\nintervention during training. Leveraging this framework, we further propose an\nalgorithm, iterative influence-based filtering (IIF), for online RL training\nthat iteratively performs experience filtering to refine policy updates. Across\nstandard RL benchmarks (classic control, navigation, locomotion) to RLHF for\nlarge language models, IIF reduces sample complexity, speeds up training, and\nachieves higher returns. Overall, these results advance interpretability,\nefficiency, and effectiveness of online RL."}
{"id": "2505.19599", "pdf": "https://arxiv.org/pdf/2505.19599", "abs": "https://arxiv.org/abs/2505.19599", "authors": ["Andrew Gambardella", "Takeshi Kojima", "Yusuke Iwasawa", "Yutaka Matsuo"], "title": "Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese Grammar", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "In Proceedings of the 63rd Annual Meeting of the Association for\n  Computational Linguistics, 2025", "summary": "Typical methods for evaluating the performance of language models evaluate\ntheir ability to answer questions accurately. These evaluation metrics are\nacceptable for determining the extent to which language models can understand\nand reason about text in a general sense, but fail to capture nuanced\ncapabilities, such as the ability of language models to recognize and obey rare\ngrammar points, particularly in languages other than English. We measure the\nperplexity of language models when confronted with the \"first person psych\npredicate restriction\" grammar point in Japanese. Weblab is the only tested\nopen source model in the 7-10B parameter range which consistently assigns\nhigher perplexity to ungrammatical psych predicate sentences than grammatical\nones. We give evidence that Weblab's uniformly bad tokenization is a possible\nroot cause for its good performance, and show that Llama 3's perplexity on\ngrammatical psych predicate sentences can be reduced by orders of magnitude\n(28x difference) by restricting test sentences to those with uniformly\nwell-behaved tokenizations. We show in further experiments on machine\ntranslation tasks that language models will use alternative grammar patterns in\norder to produce grammatical sentences when tokenization issues prevent the\nmost natural sentence from being output."}
{"id": "2505.18232", "pdf": "https://arxiv.org/pdf/2505.18232", "abs": "https://arxiv.org/abs/2505.18232", "authors": ["Mingkuan Feng", "Jinyang Wu", "Siyuan Liu", "Shuai Zhang", "Hongjian Fang", "Ruihan Jin", "Feihu Che", "Pengpeng Shao", "Zhengqi Wen", "Jianhua Tao"], "title": "ELDeR: Getting Efficient LLMs through Data-Driven Regularized Layer-wise Pruning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The deployment of Large language models (LLMs) in many fields is largely\nhindered by their high computational and memory costs. Recent studies suggest\nthat LLMs exhibit sparsity, which can be used for pruning. Previous pruning\nmethods typically follow a prune-then-finetune paradigm. Since the pruned parts\nstill contain valuable information, statically removing them without updating\nthe remaining parameters often results in irreversible performance degradation,\nrequiring costly recovery fine-tuning (RFT) to maintain performance. To address\nthis, we propose a novel paradigm: first apply regularization, then prune.\nBased on this paradigm, we propose ELDeR: Getting Efficient LLMs through\nData-Driven Regularized Layer-wise Pruning. We multiply the output of each\ntransformer layer by an initial weight, then we iteratively learn the weights\nof each transformer layer by using a small amount of data in a simple way.\nAfter that, we apply regularization to the difference between the output and\ninput of the layers with smaller weights, forcing the information to be\ntransferred to the remaining layers. Compared with direct pruning, ELDeR\nreduces the information loss caused by direct parameter removal, thus better\npreserving the model's language modeling ability. Experimental results show\nthat ELDeR achieves superior performance compared with powerful layer-wise\nstructured pruning methods, while greatly reducing RFT computational costs.\nSince ELDeR is a layer-wise pruning method, its end-to-end acceleration effect\nis obvious, making it a promising technique for efficient LLMs."}
{"id": "2505.19288", "pdf": "https://arxiv.org/pdf/2505.19288", "abs": "https://arxiv.org/abs/2505.19288", "authors": ["Jimeng Shi", "Sizhe Zhou", "Bowen Jin", "Wei Hu", "Shaowen Wang", "Giri Narasimhan", "Jiawei Han"], "title": "Hypercube-RAG: Hypercube-Based Retrieval-Augmented Generation for In-domain Scientific Question-Answering", "categories": ["cs.LG"], "comment": null, "summary": "Large language models (LLMs) often need to incorporate external knowledge to\nsolve theme-specific problems. Retrieval-augmented generation (RAG), which\nempowers LLMs to generate more qualified responses with retrieved external data\nand knowledge, has shown its high promise. However, traditional semantic\nsimilarity-based RAGs struggle to return concise yet highly relevant\ninformation for domain knowledge-intensive tasks, such as scientific\nquestion-answering (QA). Built on a multi-dimensional (cube) structure called\nHypercube, which can index documents in an application-driven, human-defined,\nmulti-dimensional space, we introduce the Hypercube-RAG, a novel RAG framework\nfor precise and efficient retrieval. Given a query, Hypercube-RAG first\ndecomposes it based on its entities and topics and then retrieves relevant\ndocuments from cubes by aligning these decomposed components with hypercube\ndimensions. Experiments on three in-domain scientific QA datasets demonstrate\nthat our method improves accuracy by 3.7% and boosts retrieval efficiency by\n81.2%, measured as relative gains over the strongest RAG baseline. More\nimportantly, our Hypercube-RAG inherently offers explainability by revealing\nthe underlying predefined hypercube dimensions used for retrieval. The code and\ndata sets are available at https://github.com/JimengShi/Hypercube-RAG."}
{"id": "2505.19604", "pdf": "https://arxiv.org/pdf/2505.19604", "abs": "https://arxiv.org/abs/2505.19604", "authors": ["Ahan Prasannakumar Shetty"], "title": "Evaluating Machine Translation Models for English-Hindi Language Pairs: A Comparative Analysis", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Machine translation has become a critical tool in bridging linguistic gaps,\nespecially between languages as diverse as English and Hindi. This paper\ncomprehensively evaluates various machine translation models for translating\nbetween English and Hindi. We assess the performance of these models using a\ndiverse set of automatic evaluation metrics, both lexical and machine\nlearning-based metrics. Our evaluation leverages an 18000+ corpus of English\nHindi parallel dataset and a custom FAQ dataset comprising questions from\ngovernment websites. The study aims to provide insights into the effectiveness\nof different machine translation approaches in handling both general and\nspecialized language domains. Results indicate varying performance levels\nacross different metrics, highlighting strengths and areas for improvement in\ncurrent translation systems."}
{"id": "2505.18233", "pdf": "https://arxiv.org/pdf/2505.18233", "abs": "https://arxiv.org/abs/2505.18233", "authors": ["Shaghayegh Hosseinpour", "Sanchari Das"], "title": "POSTER: A Multi-Signal Model for Detecting Evasive Smishing", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Smishing, or SMS-based phishing, poses an increasing threat to mobile users\nby mimicking legitimate communications through culturally adapted, concise, and\ndeceptive messages, which can result in the loss of sensitive data or financial\nresources. In such, we present a multi-channel smishing detection model that\ncombines country-specific semantic tagging, structural pattern tagging,\ncharacter-level stylistic cues, and contextual phrase embeddings. We curated\nand relabeled over 84,000 messages across five datasets, including 24,086\nsmishing samples. Our unified architecture achieves 97.89% accuracy, an F1\nscore of 0.963, and an AUC of 99.73%, outperforming single-stream models by\ncapturing diverse linguistic and structural cues. This work demonstrates the\neffectiveness of multi-signal learning in robust and region-aware phishing."}
{"id": "2505.19313", "pdf": "https://arxiv.org/pdf/2505.19313", "abs": "https://arxiv.org/abs/2505.19313", "authors": ["Marta Aparicio Rodriguez", "Xenia Miscouridou", "Anastasia Borovykh"], "title": "Concept Reachability in Diffusion Models: Beyond Dataset Constraints", "categories": ["cs.LG"], "comment": null, "summary": "Despite significant advances in quality and complexity of the generations in\ntext-to-image models, prompting does not always lead to the desired outputs.\nControlling model behaviour by directly steering intermediate model activations\nhas emerged as a viable alternative allowing to reach concepts in latent space\nthat may otherwise remain inaccessible by prompt. In this work, we introduce a\nset of experiments to deepen our understanding of concept reachability. We\ndesign a training data setup with three key obstacles: scarcity of concepts,\nunderspecification of concepts in the captions, and data biases with tied\nconcepts. Our results show: (i) concept reachability in latent space exhibits a\ndistinct phase transition, with only a small number of samples being sufficient\nto enable reachability, (ii) where in the latent space the intervention is\nperformed critically impacts reachability, showing that certain concepts are\nreachable only at certain stages of transformation, and (iii) while prompting\nability rapidly diminishes with a decrease in quality of the dataset, concepts\noften remain reliably reachable through steering. Model providers can leverage\nthis to bypass costly retraining and dataset curation and instead innovate with\nuser-facing control mechanisms."}
{"id": "2505.19606", "pdf": "https://arxiv.org/pdf/2505.19606", "abs": "https://arxiv.org/abs/2505.19606", "authors": ["Ryan Soh-Eun Shim", "Domenico De Cristofaro", "Chengzhi Martin Hu", "Alessandro Vietti", "Barbara Plank"], "title": "Languages in Multilingual Speech Foundation Models Align Both Phonetically and Semantically", "categories": ["cs.CL"], "comment": null, "summary": "Cross-lingual alignment in pretrained language models (LMs) has enabled\nefficient transfer in text-based LMs. Such an alignment has also been observed\nin speech foundation models. However, it remains an open question whether\nfindings and methods from text-based cross-lingual alignment apply to speech.\nBuilding on prior work on spoken translation retrieval, we perform\npronunciation-controlled experiments to observe if cross-lingual alignment can\nindeed occur in such models on a semantic basis, instead of relying on phonetic\nsimilarities. Our findings indicate that even in the absence of phonetic cues,\nspoken translation retrieval accuracy remains relatively stable. We follow up\nwith a controlled experiment on a word-level dataset of cross-lingual synonyms\nand near-homophones, confirming the existence of both phonetic and semantic\nknowledge in the encoder. Finally, we qualitatively examine the transcriptions\nproduced by early exiting the encoder, where we observe that speech translation\nproduces semantic errors that are characterized by phonetic similarities to\ncorresponding words in the source language. We apply this insight from early\nexiting to speech recognition in seven low-resource languages unsupported by\nthe Whisper model, and achieve improved accuracy in all languages examined,\nparticularly for languages with transparent orthographies."}
{"id": "2505.18234", "pdf": "https://arxiv.org/pdf/2505.18234", "abs": "https://arxiv.org/abs/2505.18234", "authors": ["Yuanya She"], "title": "A Robust PPO-optimized Tabular Transformer Framework for Intrusion Detection in Industrial IoT Systems", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In this paper, we propose a robust and reinforcement-learning-enhanced\nnetwork intrusion detection system (NIDS) designed for class-imbalanced and\nfew-shot attack scenarios in Industrial Internet of Things (IIoT) environments.\nOur model integrates a TabTransformer for effective tabular feature\nrepresentation with Proximal Policy Optimization (PPO) to optimize\nclassification decisions via policy learning. Evaluated on the\nTON\\textunderscore IoT benchmark, our method achieves a macro F1-score of\n97.73\\% and accuracy of 98.85\\%. Remarkably, even on extremely rare classes\nlike man-in-the-middle (MITM), our model achieves an F1-score of 88.79\\%,\nshowcasing strong robustness and few-shot detection capabilities. Extensive\nablation experiments confirm the complementary roles of TabTransformer and PPO\nin mitigating class imbalance and improving generalization. These results\nhighlight the potential of combining transformer-based tabular learning with\nreinforcement learning for real-world NIDS applications."}
{"id": "2505.19327", "pdf": "https://arxiv.org/pdf/2505.19327", "abs": "https://arxiv.org/abs/2505.19327", "authors": ["Buse Sibel Korkmaz", "Rahul Nair", "Elizabeth M. Daly", "Antonio del Rio Chanona"], "title": "Paying Alignment Tax with Contrastive Learning", "categories": ["cs.LG"], "comment": null, "summary": "Current debiasing approaches often result a degradation in model capabilities\nsuch as factual accuracy and knowledge retention. Through systematic evaluation\nacross multiple benchmarks, we demonstrate that existing debiasing methods face\nfundamental trade-offs, particularly in smaller models, leading to reduced\ntruthfulness, knowledge loss, or unintelligible outputs. To address these\nlimitations, we propose a contrastive learning framework that learns through\ncarefully constructed positive and negative examples. Our approach introduces\ncontrast computation and dynamic loss scaling to balance bias mitigation with\nfaithfulness preservation. Experimental results across multiple model scales\ndemonstrate that our method achieves substantial improvements in both toxicity\nreduction and faithfulness preservation. Most importantly, we show that our\nframework is the first to consistently improve both metrics simultaneously,\navoiding the capability degradation characteristic of existing approaches.\nThese results suggest that explicit modeling of both positive and negative\nexamples through contrastive learning could be a promising direction for\nreducing the alignment tax in language model debiasing."}
{"id": "2505.19628", "pdf": "https://arxiv.org/pdf/2505.19628", "abs": "https://arxiv.org/abs/2505.19628", "authors": ["Silin Li", "Yuhang Guo", "Jiashu Yao", "Zeming Liu", "Haifeng Wang"], "title": "HomeBench: Evaluating LLMs in Smart Homes with Valid and Invalid Instructions Across Single and Multiple Devices", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have the potential to revolutionize smart home\nassistants by enhancing their ability to accurately understand user needs and\nrespond appropriately, which is extremely beneficial for building a smarter\nhome environment. While recent studies have explored integrating LLMs into\nsmart home systems, they primarily focus on handling straightforward, valid\nsingle-device operation instructions. However, real-world scenarios are far\nmore complex and often involve users issuing invalid instructions or\ncontrolling multiple devices simultaneously. These have two main challenges:\nLLMs must accurately identify and rectify errors in user instructions and\nexecute multiple user instructions perfectly. To address these challenges and\nadvance the development of LLM-based smart home assistants, we introduce\nHomeBench, the first smart home dataset with valid and invalid instructions\nacross single and multiple devices in this paper. We have experimental results\non 13 distinct LLMs; e.g., GPT-4o achieves only a 0.0% success rate in the\nscenario of invalid multi-device instructions, revealing that the existing\nstate-of-the-art LLMs still cannot perform well in this situation even with the\nhelp of in-context learning, retrieval-augmented generation, and fine-tuning.\nOur code and dataset are publicly available at\nhttps://github.com/BITHLP/HomeBench."}
{"id": "2505.18235", "pdf": "https://arxiv.org/pdf/2505.18235", "abs": "https://arxiv.org/abs/2505.18235", "authors": ["Alexander Modell", "Patrick Rubin-Delanchy", "Nick Whiteley"], "title": "The Origins of Representation Manifolds in Large Language Models", "categories": ["cs.LG", "cs.AI", "68T07", "I.2.7"], "comment": "16 pages, 4 figures", "summary": "There is a large ongoing scientific effort in mechanistic interpretability to\nmap embeddings and internal representations of AI systems into\nhuman-understandable concepts. A key element of this effort is the linear\nrepresentation hypothesis, which posits that neural representations are sparse\nlinear combinations of `almost-orthogonal' direction vectors, reflecting the\npresence or absence of different features. This model underpins the use of\nsparse autoencoders to recover features from representations. Moving towards a\nfuller model of features, in which neural representations could encode not just\nthe presence but also a potentially continuous and multidimensional value for a\nfeature, has been a subject of intense recent discourse. We describe why and\nhow a feature might be represented as a manifold, demonstrating in particular\nthat cosine similarity in representation space may encode the intrinsic\ngeometry of a feature through shortest, on-manifold paths, potentially\nanswering the question of how distance in representation space and relatedness\nin concept space could be connected. The critical assumptions and predictions\nof the theory are validated on text embeddings and token activations of large\nlanguage models."}
{"id": "2505.19334", "pdf": "https://arxiv.org/pdf/2505.19334", "abs": "https://arxiv.org/abs/2505.19334", "authors": ["Charles Godfrey", "Ping Nie", "Natalia Ostapuk", "David Ken", "Shang Gao", "Souheil Inati"], "title": "Likert or Not: LLM Absolute Relevance Judgments on Fine-Grained Ordinal Scales", "categories": ["cs.LG", "cs.IR", "H.3.3; I.2.7; H.3.1"], "comment": null, "summary": "Large language models (LLMs) obtain state of the art zero shot relevance\nranking performance on a variety of information retrieval tasks. The two most\ncommon prompts to elicit LLM relevance judgments are pointwise scoring (a.k.a.\nrelevance generation), where the LLM sees a single query-document pair and\noutputs a single relevance score, and listwise ranking (a.k.a. permutation\ngeneration), where the LLM sees a query and a list of documents and outputs a\npermutation, sorting the documents in decreasing order of relevance. The\ncurrent research community consensus is that listwise ranking yields superior\nperformance, and significant research effort has been devoted to crafting LLM\nlistwise ranking algorithms. The underlying hypothesis is that LLMs are better\nat making relative relevance judgments than absolute ones. In tension with this\nhypothesis, we find that the gap between pointwise scoring and listwise ranking\nshrinks when pointwise scoring is implemented using a sufficiently large\nordinal relevance label space, becoming statistically insignificant for many\nLLM-benchmark dataset combinations (where ``significant'' means ``95\\%\nconfidence that listwise ranking improves NDCG@10''). Our evaluations span four\nLLMs, eight benchmark datasets from the BEIR and TREC-DL suites, and two\nproprietary datasets with relevance labels collected after the training cut-off\nof all LLMs evaluated."}
{"id": "2505.19630", "pdf": "https://arxiv.org/pdf/2505.19630", "abs": "https://arxiv.org/abs/2505.19630", "authors": ["Yichun Feng", "Jiawei Wang", "Lu Zhou", "Yixue Li"], "title": "DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated excellent capabilities in the\nfield of biomedical question answering, but their application in real-world\nclinical consultations still faces core challenges. Existing systems rely on a\none-way information transmission mode where patients must fully describe their\nsymptoms in a single round, leading to nonspecific diagnostic recommendations\nwhen complaints are vague. Traditional multi-turn dialogue methods based on\nsupervised learning are constrained by static data-driven paradigms, lacking\ngeneralizability and struggling to intelligently extract key clinical\ninformation. To address these limitations, we propose DoctorAgent-RL, a\nreinforcement learning (RL)-based multi-agent collaborative framework that\nmodels medical consultations as a dynamic decision-making process under\nuncertainty. The doctor agent continuously optimizes its questioning strategy\nwithin the RL framework through multi-turn interactions with the patient agent,\ndynamically adjusting its information-gathering path based on comprehensive\nrewards from the Consultation Evaluator. This RL fine-tuning mechanism enables\nLLMs to autonomously develop interaction strategies aligned with clinical\nreasoning logic, rather than superficially imitating patterns in existing\ndialogue data. Notably, we constructed MTMedDialog, the first English\nmulti-turn medical consultation dataset capable of simulating patient\ninteractions. Experiments demonstrate that DoctorAgent-RL outperforms existing\nmodels in both multi-turn reasoning capability and final diagnostic\nperformance, demonstrating practical value in assisting clinical consultations.\nhttps://github.com/JarvisUSTC/DoctorAgent-RL"}
{"id": "2505.18236", "pdf": "https://arxiv.org/pdf/2505.18236", "abs": "https://arxiv.org/abs/2505.18236", "authors": ["Natalia Matuszczyk", "Craig R. Barnes", "Rohit Gupta", "Bulent Ozel", "Aniket Mitra"], "title": "From Bias to Accountability: How the EU AI Act Confronts Challenges in European GeoAI Auditing", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Bias in geospatial artificial intelligence (GeoAI) models has been\ndocumented, yet the evidence is scattered across narrowly focused studies. We\nsynthesize this fragmented literature to provide a concise overview of bias in\nGeoAI and examine how the EU's Artificial Intelligence Act (EU AI Act) shapes\naudit obligations. We discuss recurring bias mechanisms, including\nrepresentation, algorithmic and aggregation bias, and map them to specific\nprovisions of the EU AI Act. By applying the Act's high-risk criteria, we\ndemonstrate that widely deployed GeoAI applications qualify as high-risk\nsystems. We then present examples of recent audits along with an outline of\npractical methods for detecting bias. As far as we know, this study represents\nthe first integration of GeoAI bias evidence into the EU AI Act context, by\nidentifying high-risk GeoAI systems and mapping bias mechanisms to the Act's\nArticles. Although the analysis is exploratory, it suggests that even\nwell-curated European datasets should employ routine bias audits before 2027,\nwhen the AI Act's high-risk provisions take full effect."}
{"id": "2505.19337", "pdf": "https://arxiv.org/pdf/2505.19337", "abs": "https://arxiv.org/abs/2505.19337", "authors": ["Kevin Li", "Marinka Zitnik"], "title": "Prompting Decision Transformers for Zero-Shot Reach-Avoid Policies", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Offline goal-conditioned reinforcement learning methods have shown promise\nfor reach-avoid tasks, where an agent must reach a target state while avoiding\nundesirable regions of the state space. Existing approaches typically encode\navoid-region information into an augmented state space and cost function, which\nprevents flexible, dynamic specification of novel avoid-region information at\nevaluation time. They also rely heavily on well-designed reward and cost\nfunctions, limiting scalability to complex or poorly structured environments.\nWe introduce RADT, a decision transformer model for offline, reward-free,\ngoal-conditioned, avoid region-conditioned RL. RADT encodes goals and avoid\nregions directly as prompt tokens, allowing any number of avoid regions of\narbitrary size to be specified at evaluation time. Using only suboptimal\noffline trajectories from a random policy, RADT learns reach-avoid behavior\nthrough a novel combination of goal and avoid-region hindsight relabeling. We\nbenchmark RADT against 3 existing offline goal-conditioned RL models across 11\ntasks, environments, and experimental settings. RADT generalizes in a zero-shot\nmanner to out-of-distribution avoid region sizes and counts, outperforming\nbaselines that require retraining. In one such zero-shot setting, RADT achieves\n35.7% improvement in normalized cost over the best retrained baseline while\nmaintaining high goal-reaching success. We apply RADT to cell reprogramming in\nbiology, where it reduces visits to undesirable intermediate gene expression\nstates during trajectories to desired target states, despite stochastic\ntransitions and discrete, structured state dynamics."}
{"id": "2505.19631", "pdf": "https://arxiv.org/pdf/2505.19631", "abs": "https://arxiv.org/abs/2505.19631", "authors": ["Zihong Zhang", "Liqi He", "Zuchao Li", "Lefei Zhang", "Hai Zhao", "Bo Du"], "title": "Segment First or Comprehend First? Explore the Limit of Unsupervised Word Segmentation with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Word segmentation stands as a cornerstone of Natural Language Processing\n(NLP). Based on the concept of \"comprehend first, segment later\", we propose a\nnew framework to explore the limit of unsupervised word segmentation with Large\nLanguage Models (LLMs) and evaluate the semantic understanding capabilities of\nLLMs based on word segmentation. We employ current mainstream LLMs to perform\nword segmentation across multiple languages to assess LLMs' \"comprehension\".\nOur findings reveal that LLMs are capable of following simple prompts to\nsegment raw text into words. There is a trend suggesting that models with more\nparameters tend to perform better on multiple languages. Additionally, we\nintroduce a novel unsupervised method, termed LLACA ($\\textbf{L}$arge\n$\\textbf{L}$anguage Model-Inspired $\\textbf{A}$ho-$\\textbf{C}$orasick\n$\\textbf{A}$utomaton). Leveraging the advanced pattern recognition capabilities\nof Aho-Corasick automata, LLACA innovatively combines these with the deep\ninsights of well-pretrained LLMs. This approach not only enables the\nconstruction of a dynamic $n$-gram model that adjusts based on contextual\ninformation but also integrates the nuanced understanding of LLMs, offering\nsignificant improvements over traditional methods. Our source code is available\nat https://github.com/hkr04/LLACA"}
{"id": "2505.18237", "pdf": "https://arxiv.org/pdf/2505.18237", "abs": "https://arxiv.org/abs/2505.18237", "authors": ["Xixian Yong", "Xiao Zhou", "Yingying Zhang", "Jinlin Li", "Yefeng Zheng", "Xian Wu"], "title": "Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "The recent rise of Large Reasoning Models (LRMs) has significantly improved\nmulti-step reasoning performance, but often at the cost of generating\nexcessively long reasoning chains. This paper revisits the efficiency of such\nreasoning processes through an information-theoretic lens, revealing a\nfundamental trade-off between reasoning length and semantic efficiency. We\npropose two metrics, InfoBias and InfoGain, to quantify divergence from ideal\nreasoning paths and stepwise information contribution, respectively. Empirical\nanalyses show that longer reasoning chains tend to exhibit higher information\nbias and diminishing information gain, especially for incorrect answers.\nMotivated by these findings, we introduce an entropy-based Adaptive Think\nstrategy that dynamically halts reasoning once confidence is sufficiently high,\nimproving efficiency while maintaining competitive accuracy. Compared to the\nVanilla Think approach (default mode), our strategy yields a 1.10% improvement\nin average accuracy and a 50.80% reduction in token usage on QwQ-32B across six\nbenchmark tasks spanning diverse reasoning types and difficulty levels,\ndemonstrating superior efficiency and reasoning performance. These results\nunderscore the promise of entropy-based methods for enhancing both accuracy and\ncost-effiiciency in large language model deployment."}
{"id": "2505.19342", "pdf": "https://arxiv.org/pdf/2505.19342", "abs": "https://arxiv.org/abs/2505.19342", "authors": ["Xiao Liu", "Lijun Zhang", "Deepak Ganesan", "Hui Guan"], "title": "Communication-Efficient Multi-Device Inference Acceleration for Transformer Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Transformer models power many AI applications but suffer from high inference\nlatency, limiting their use in real-time settings. Multi-device inference can\nreduce latency by parallelizing computation. Yet, existing methods require high\ninter-device bandwidth, making them impractical for bandwidth-constrained\nenvironments. We propose ASTRA, a communication-efficient framework that\naccelerates Transformer inference through a novel integration of sequence\nparallelism and a Mixed-Precision Attention mechanism designed to minimize\ninter-device communication. ASTRA compresses non-local token embeddings via\nvector quantization and preserves task accuracy through two optimizations,\nNoise-Augmented Quantization and Distributed Class Tokens. Experiments on ViT\nand GPT2 across vision and NLP tasks show that ASTRA achieves up to 2.64X\nspeedups over single-device inference and up to 15.25X speedups over\nstate-of-the-art multi-device inferences, while operating under bandwidths as\nlow as 10 Mbps. ASTRA is open-sourced at https://github.com/xl1990/Astra."}
{"id": "2505.19634", "pdf": "https://arxiv.org/pdf/2505.19634", "abs": "https://arxiv.org/abs/2505.19634", "authors": ["Zili Wang", "Tianyu Zhang", "Haoli Bai", "Lu Hou", "Xianzhi Yu", "Wulong Liu", "Shiming Xiang", "Lei Zhu"], "title": "Faster and Better LLMs via Latency-Aware Test-Time Scaling", "categories": ["cs.CL"], "comment": null, "summary": "Test-Time Scaling (TTS) has proven effective in improving the performance of\nLarge Language Models (LLMs) during inference. However, existing research has\noverlooked the efficiency of TTS from a latency-sensitive perspective. Through\na latency-aware evaluation of representative TTS methods, we demonstrate that a\ncompute-optimal TTS does not always result in the lowest latency in scenarios\nwhere latency is critical. To address this gap and achieve latency-optimal TTS,\nwe propose two key approaches by optimizing the concurrency configurations: (1)\nbranch-wise parallelism, which leverages multiple concurrent inference\nbranches, and (2) sequence-wise parallelism, enabled by speculative decoding.\nBy integrating these two approaches and allocating computational resources\nproperly to each, our latency-optimal TTS enables a 32B model to reach 82.3%\naccuracy on MATH-500 within 1 minute and a smaller 3B model to achieve 72.4%\nwithin 10 seconds. Our work emphasizes the importance of latency-aware TTS and\ndemonstrates its ability to deliver both speed and accuracy in\nlatency-sensitive scenarios."}
{"id": "2505.18240", "pdf": "https://arxiv.org/pdf/2505.18240", "abs": "https://arxiv.org/abs/2505.18240", "authors": ["Ananth Muppidi", "Tarak Das", "Sambaran Bandyopadhyay", "Tripti Shukla", "Dharun D A"], "title": "Taming LLMs with Negative Samples: A Reference-Free Framework to Evaluate Presentation Content with Actionable Feedback", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The generation of presentation slides automatically is an important problem\nin the era of generative AI. This paper focuses on evaluating multimodal\ncontent in presentation slides that can effectively summarize a document and\nconvey concepts to a broad audience. We introduce a benchmark dataset,\nRefSlides, consisting of human-made high-quality presentations that span\nvarious topics. Next, we propose a set of metrics to characterize different\nintrinsic properties of the content of a presentation and present REFLEX, an\nevaluation approach that generates scores and actionable feedback for these\nmetrics. We achieve this by generating negative presentation samples with\ndifferent degrees of metric-specific perturbations and use them to fine-tune\nLLMs. This reference-free evaluation technique does not require ground truth\npresentations during inference. Our extensive automated and human experiments\ndemonstrate that our evaluation approach outperforms classical heuristic-based\nand state-of-the-art large language model-based evaluations in generating\nscores and explanations."}
{"id": "2505.19369", "pdf": "https://arxiv.org/pdf/2505.19369", "abs": "https://arxiv.org/abs/2505.19369", "authors": ["Yunbo Liu", "Xukui Qin", "Yifan Gao", "Xiang Li", "Chengwei Feng"], "title": "SETransformer: A Hybrid Attention-Based Architecture for Robust Human Activity Recognition", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Human Activity Recognition (HAR) using wearable sensor data has become a\ncentral task in mobile computing, healthcare, and human-computer interaction.\nDespite the success of traditional deep learning models such as CNNs and RNNs,\nthey often struggle to capture long-range temporal dependencies and contextual\nrelevance across multiple sensor channels. To address these limitations, we\npropose SETransformer, a hybrid deep neural architecture that combines\nTransformer-based temporal modeling with channel-wise squeeze-and-excitation\n(SE) attention and a learnable temporal attention pooling mechanism. The model\ntakes raw triaxial accelerometer data as input and leverages global\nself-attention to capture activity-specific motion dynamics over extended time\nwindows, while adaptively emphasizing informative sensor channels and critical\ntime steps.\n  We evaluate SETransformer on the WISDM dataset and demonstrate that it\nsignificantly outperforms conventional models including LSTM, GRU, BiLSTM, and\nCNN baselines. The proposed model achieves a validation accuracy of 84.68\\% and\na macro F1-score of 84.64\\%, surpassing all baseline architectures by a notable\nmargin. Our results show that SETransformer is a competitive and interpretable\nsolution for real-world HAR tasks, with strong potential for deployment in\nmobile and ubiquitous sensing applications."}
{"id": "2505.19640", "pdf": "https://arxiv.org/pdf/2505.19640", "abs": "https://arxiv.org/abs/2505.19640", "authors": ["Roy Xie", "David Qiu", "Deepak Gopinath", "Dong Lin", "Yanchao Sun", "Chong Wang", "Saloni Potdar", "Bhuwan Dhingra"], "title": "Interleaved Reasoning for Large Language Models via Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "Long chain-of-thought (CoT) significantly enhances large language models'\n(LLM) reasoning capabilities. However, the extensive reasoning traces lead to\ninefficiencies and an increased time-to-first-token (TTFT). We propose a novel\ntraining paradigm that uses reinforcement learning (RL) to guide reasoning LLMs\nto interleave thinking and answering for multi-hop questions. We observe that\nmodels inherently possess the ability to perform interleaved reasoning, which\ncan be further enhanced through RL. We introduce a simple yet effective\nrule-based reward to incentivize correct intermediate steps, which guides the\npolicy model toward correct reasoning paths by leveraging intermediate signals\ngenerated during interleaved reasoning. Extensive experiments conducted across\nfive diverse datasets and three RL algorithms (PPO, GRPO, and REINFORCE++)\ndemonstrate consistent improvements over traditional think-answer reasoning,\nwithout requiring external tools. Specifically, our approach reduces TTFT by\nover 80% on average and improves up to 19.3% in Pass@1 accuracy. Furthermore,\nour method, trained solely on question answering and logical reasoning\ndatasets, exhibits strong generalization ability to complex reasoning datasets\nsuch as MATH, GPQA, and MMLU. Additionally, we conduct in-depth analysis to\nreveal several valuable insights into conditional reward modeling."}
{"id": "2505.18241", "pdf": "https://arxiv.org/pdf/2505.18241", "abs": "https://arxiv.org/abs/2505.18241", "authors": ["Arjun Bhalla", "Qi Huang"], "title": "Intent Classification on Low-Resource Languages with Query Similarity Search", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Intent classification is an important component of a functional Information\nRetrieval ecosystem. Many current approaches to intent classification,\ntypically framed as a classification problem, can be problematic as intents are\noften hard to define and thus data can be difficult and expensive to annotate.\nThe problem is exacerbated when we need to extend the intent classification\nsystem to support multiple and in particular low-resource languages. To address\nthis, we propose casting intent classification as a query similarity search\nproblem - we use previous example queries to define an intent, and a query\nsimilarity method to classify an incoming query based on the labels of its most\nsimilar queries in latent space. With the proposed approach, we are able to\nachieve reasonable intent classification performance for queries in\nlow-resource languages in a zero-shot setting."}
{"id": "2505.19387", "pdf": "https://arxiv.org/pdf/2505.19387", "abs": "https://arxiv.org/abs/2505.19387", "authors": ["Botong Zhang", "Shuo Li", "Ignacio Hounie", "Osbert Bastani", "Dongsheng Ding", "Alejandro Ribeiro"], "title": "Alignment of large language models with constrained learning", "categories": ["cs.LG", "cs.SY", "eess.SY", "math.OC"], "comment": "48 pages, 7 figures, 7 tables", "summary": "We study the problem of computing an optimal large language model (LLM)\npolicy for a constrained alignment problem, where the goal is to maximize a\nprimary reward objective while satisfying constraints on secondary utilities.\nDespite the popularity of Lagrangian-based LLM policy search in constrained\nalignment, iterative primal-dual methods often fail to converge, and\nnon-iterative dual-based methods do not achieve optimality in the LLM parameter\nspace. To address these challenges, we employ Lagrangian duality to develop an\niterative dual-based alignment method that alternates between updating the LLM\npolicy via Lagrangian maximization and updating the dual variable via dual\ndescent. In theory, we characterize the primal-dual gap between the primal\nvalue in the distribution space and the dual value in the LLM parameter space.\nWe further quantify the optimality gap of the learned LLM policies at\nnear-optimal dual variables with respect to both the objective and the\nconstraint functions. These results prove that dual-based alignment methods can\nfind an optimal constrained LLM policy, up to an LLM parametrization gap. We\ndemonstrate the effectiveness and merits of our approach through extensive\nexperiments conducted on the PKU-SafeRLHF dataset."}
{"id": "2505.19647", "pdf": "https://arxiv.org/pdf/2505.19647", "abs": "https://arxiv.org/abs/2505.19647", "authors": ["Xiaochuan Liu", "Ruihua Song", "Xiting Wang", "Xu Chen"], "title": "Select, Read, and Write: A Multi-Agent Framework of Full-Text-based Related Work Generation", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 (Findings)", "summary": "Automatic related work generation (RWG) can save people's time and effort\nwhen writing a draft of related work section (RWS) for further revision.\nHowever, existing methods for RWG always suffer from shallow comprehension due\nto taking the limited portions of references papers as input and isolated\nexplanation for each reference due to ineffective capturing the relationships\namong them. To address these issues, we focus on full-text-based RWG task and\npropose a novel multi-agent framework. Our framework consists of three agents:\na selector that decides which section of the papers is going to read next, a\nreader that digests the selected section and updates a shared working memory,\nand a writer that generates RWS based on the final curated memory. To better\ncapture the relationships among references, we also propose two graph-aware\nstrategies for selector, enabling to optimize the reading order with constrains\nof the graph structure. Extensive experiments demonstrate that our framework\nconsistently improves performance across three base models and various input\nconfigurations. The graph-aware selectors outperform alternative selectors,\nachieving state-of-the-art results. The code and data are available at\nhttps://github.com/1190200817/Full_Text_RWG."}
{"id": "2505.18243", "pdf": "https://arxiv.org/pdf/2505.18243", "abs": "https://arxiv.org/abs/2505.18243", "authors": ["Monirul Islam Mahmud"], "title": "ZeroML: A Next Generation AutoML Language", "categories": ["cs.PL", "cs.AI"], "comment": null, "summary": "ZeroML is a new generation programming language for AutoML to drive the ML\npipeline in a compiled and multi-paradigm way, with a pure functional core.\nMeeting the shortcomings introduced by Python, R, or Julia such as slow-running\ntime, brittle pipelines or high dependency cost ZeroML brings the\nMicroservices-based architecture adding the modular, reusable pieces such as\nDataCleaner, FeatureEngineer or ModelSelector. As a native multithread and\nmemory-aware search optimized toolkit, and with one command deployability\nability, ZeroML ensures non-coders and ML professionals to create high-accuracy\nmodels super fast and in a more reproducible way. The verbosity of the language\nensures that when it comes to dropping into the backend, the code we will be\ncreating is extremely clear but the level of repetition and boilerplate\nrequired when developing on the front end is now removed."}
{"id": "2505.19397", "pdf": "https://arxiv.org/pdf/2505.19397", "abs": "https://arxiv.org/abs/2505.19397", "authors": ["Jiawen Zhang", "Zhenwei Zhang", "Shun Zheng", "Xumeng Wen", "Jia Li", "Jiang Bian"], "title": "Are Time-Series Foundation Models Deployment-Ready? A Systematic Study of Adversarial Robustness Across Domains", "categories": ["cs.LG"], "comment": "Preprint", "summary": "Time Series Foundation Models (TSFMs), which are pretrained on large-scale,\ncross-domain data and capable of zero-shot forecasting in new scenarios without\nfurther training, are increasingly adopted in real-world applications. However,\nas the zero-shot forecasting paradigm gets popular, a critical yet overlooked\nquestion emerges: Are TSFMs robust to adversarial input perturbations? Such\nperturbations could be exploited in man-in-the-middle attacks or data\npoisoning. To address this gap, we conduct a systematic investigation into the\nadversarial robustness of TSFMs. Our results show that even minimal\nperturbations can induce significant and controllable changes in forecast\nbehaviors, including trend reversal, temporal drift, and amplitude shift,\nposing serious risks to TSFM-based services. Through experiments on\nrepresentative TSFMs and multiple datasets, we reveal their consistent\nvulnerabilities and identify potential architectural designs, such as\nstructural sparsity and multi-task pretraining, that may improve robustness.\nOur findings offer actionable guidance for designing more resilient forecasting\nsystems and provide a critical assessment of the adversarial robustness of\nTSFMs."}
{"id": "2505.19660", "pdf": "https://arxiv.org/pdf/2505.19660", "abs": "https://arxiv.org/abs/2505.19660", "authors": ["Tingjia Shen", "Hao Wang", "Chuan Qin", "Ruijun Sun", "Yang Song", "Defu Lian", "Hengshu Zhu", "Enhong Chen"], "title": "GenKI: Enhancing Open-Domain Question Answering with Knowledge Integration and Controllable Generation in Large Language Models", "categories": ["cs.CL", "cs.AI", "68P20", "H.3.4; I.2.6"], "comment": "13 pages, 5 figures", "summary": "Open-domain question answering (OpenQA) represents a cornerstone in natural\nlanguage processing (NLP), primarily focused on extracting answers from\nunstructured textual data. With the rapid advancements in Large Language Models\n(LLMs), LLM-based OpenQA methods have reaped the benefits of emergent\nunderstanding and answering capabilities enabled by massive parameters compared\nto traditional methods. However, most of these methods encounter two critical\nchallenges: how to integrate knowledge into LLMs effectively and how to\nadaptively generate results with specific answer formats for various task\nsituations. To address these challenges, we propose a novel framework named\nGenKI, which aims to improve the OpenQA performance by exploring Knowledge\nIntegration and controllable Generation on LLMs simultaneously. Specifically,\nwe first train a dense passage retrieval model to retrieve associated knowledge\nfrom a given knowledge base. Subsequently, we introduce a novel knowledge\nintegration model that incorporates the retrieval knowledge into instructions\nduring fine-tuning to intensify the model. Furthermore, to enable controllable\ngeneration in LLMs, we leverage a certain fine-tuned LLM and an ensemble based\non text consistency incorporating all coherence, fluency, and answer format\nassurance. Finally, extensive experiments conducted on the TriviaQA, MSMARCO,\nand CMRC2018 datasets, featuring diverse answer formats, have demonstrated the\neffectiveness of GenKI with comparison of state-of-the-art baselines. Moreover,\nablation studies have disclosed a linear relationship between the frequency of\nretrieved knowledge and the model's ability to recall knowledge accurately\nagainst the ground truth. Our code of GenKI is available at\nhttps://github.com/USTC-StarTeam/GenKI"}
{"id": "2505.18244", "pdf": "https://arxiv.org/pdf/2505.18244", "abs": "https://arxiv.org/abs/2505.18244", "authors": ["Yukin Zhang", "Qi Dong"], "title": "Multi-Scale Probabilistic Generation Theory: A Hierarchical Framework for Interpreting Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Transformer based language models achieve remarkable performance but\nremain opaque in how they plan, structure, and realize text. We introduce\nMulti_Scale Probabilistic Generation Theory (MSPGT), a hierarchical framework\nthat factorizes generation into three semantic scales_global context,\nintermediate structure, and local word choices and aligns each scale with\nspecific layer ranges in Transformer architectures. To identify scale\nboundaries, we propose two complementary metrics: attention span thresholds and\ninter layer mutual information peaks. Across four representative models (GPT-2,\nBERT, RoBERTa, and T5), these metrics yield stable local/intermediate/global\npartitions, corroborated by probing tasks and causal interventions. We find\nthat decoder_only models allocate more layers to intermediate and global\nprocessing while encoder_only models emphasize local feature extraction.\nThrough targeted interventions, we demonstrate that local scale manipulations\nprimarily influence lexical diversity, intermediate-scale modifications affect\nsentence structure and length, and global_scale perturbations impact discourse\ncoherence all with statistically significant effects. MSPGT thus offers a\nunified, architecture-agnostic method for interpreting, diagnosing, and\ncontrolling large language models, bridging the gap between mechanistic\ninterpretability and emergent capabilities."}
{"id": "2505.19404", "pdf": "https://arxiv.org/pdf/2505.19404", "abs": "https://arxiv.org/abs/2505.19404", "authors": ["Yuta Ono", "Hiroshi Nakamura", "Hideki Takase"], "title": "Exploring the Possibility of TypiClust for Low-Budget Federated Active Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "6 pages. Accepted at COMPSAC 2025", "summary": "Federated Active Learning (FAL) seeks to reduce the burden of annotation\nunder the realistic constraints of federated learning by leveraging Active\nLearning (AL). As FAL settings make it more expensive to obtain ground truth\nlabels, FAL strategies that work well in low-budget regimes, where the amount\nof annotation is very limited, are needed. In this work, we investigate the\neffectiveness of TypiClust, a successful low-budget AL strategy, in low-budget\nFAL settings. Our empirical results show that TypiClust works well even in\nlow-budget FAL settings contrasted with relatively low performances of other\nmethods, although these settings present additional challenges, such as data\nheterogeneity, compared to AL. In addition, we show that FAL settings cause\ndistribution shifts in terms of typicality, but TypiClust is not very\nvulnerable to the shifts. We also analyze the sensitivity of TypiClust to\nfeature extraction methods, and it suggests a way to perform FAL even in\nlimited data situations."}
{"id": "2505.19667", "pdf": "https://arxiv.org/pdf/2505.19667", "abs": "https://arxiv.org/abs/2505.19667", "authors": ["Weikang Yuan", "Kaisong Song", "Zhuoren Jiang", "Junjie Cao", "Yujie Zhang", "Jun Lin", "Kun Kuang", "Ji Zhang", "Xiaozhong Liu"], "title": "LeCoDe: A Benchmark Dataset for Interactive Legal Consultation Dialogue Evaluation", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "Legal consultation is essential for safeguarding individual rights and\nensuring access to justice, yet remains costly and inaccessible to many\nindividuals due to the shortage of professionals. While recent advances in\nLarge Language Models (LLMs) offer a promising path toward scalable, low-cost\nlegal assistance, current systems fall short in handling the interactive and\nknowledge-intensive nature of real-world consultations. To address these\nchallenges, we introduce LeCoDe, a real-world multi-turn benchmark dataset\ncomprising 3,696 legal consultation dialogues with 110,008 dialogue turns,\ndesigned to evaluate and improve LLMs' legal consultation capability. With\nLeCoDe, we innovatively collect live-streamed consultations from short-video\nplatforms, providing authentic multi-turn legal consultation dialogues. The\nrigorous annotation by legal experts further enhances the dataset with\nprofessional insights and expertise. Furthermore, we propose a comprehensive\nevaluation framework that assesses LLMs' consultation capabilities in terms of\n(1) clarification capability and (2) professional advice quality. This unified\nframework incorporates 12 metrics across two dimensions. Through extensive\nexperiments on various general and domain-specific LLMs, our results reveal\nsignificant challenges in this task, with even state-of-the-art models like\nGPT-4 achieving only 39.8% recall for clarification and 59% overall score for\nadvice quality, highlighting the complexity of professional consultation\nscenarios. Based on these findings, we further explore several strategies to\nenhance LLMs' legal consultation abilities. Our benchmark contributes to\nadvancing research in legal domain dialogue systems, particularly in simulating\nmore real-world user-expert interactions."}
{"id": "2505.18247", "pdf": "https://arxiv.org/pdf/2505.18247", "abs": "https://arxiv.org/abs/2505.18247", "authors": ["Kunal Sawarkar", "Shivam R. Solanki", "Abhilasha Mangal"], "title": "MetaGen Blended RAG: Higher Accuracy for Domain-Specific Q&A Without Fine-Tuning", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "Preprint. Paper Submitted NeurIPS 2025- The Thirty-Ninth Annual\n  Conference on Neural Information Processing Systems", "summary": "Despite the widespread exploration of Retrieval-Augmented Generation (RAG),\nits deployment in enterprises for domain-specific datasets remains limited due\nto poor answer accuracy. These corpora, often shielded behind firewalls in\nprivate enterprise knowledge bases, having complex, domain-specific\nterminology, rarely seen by LLMs during pre-training; exhibit significant\nsemantic variability across domains (like networking, military, or legal,\netc.), or even within a single domain like medicine, and thus result in poor\ncontext precision for RAG systems. Currently, in such situations, fine-tuning\nor RAG with fine-tuning is attempted, but these approaches are slow, expensive,\nand lack generalization for accuracy as the new domain-specific data emerges.\nWe propose an approach for Enterprise Search that focuses on enhancing the\nretriever for a domain-specific corpus through hybrid query indexes and\nmetadata enrichment. This 'MetaGen Blended RAG' method constructs a metadata\ngeneration pipeline using key concepts, topics, and acronyms, and then creates\na metadata-enriched hybrid index with boosted search queries. This approach\navoids overfitting and generalizes effectively across domains. On the PubMedQA\nbenchmark for the biomedical domain, the proposed method achieves 82% retrieval\naccuracy and 77% RAG accuracy, surpassing all previous RAG accuracy results\nwithout fine-tuning and sets a new benchmark for zero-shot results while\noutperforming much larger models like GPT3.5. The results are even comparable\nto the best fine-tuned models on this dataset, and we further demonstrate the\nrobustness and scalability of the approach by evaluating it on other Q&A\ndatasets like SQuAD, NQ etc."}
{"id": "2505.19408", "pdf": "https://arxiv.org/pdf/2505.19408", "abs": "https://arxiv.org/abs/2505.19408", "authors": ["Lu Yi", "Runlin Lei", "Fengran Mo", "Yanping Zheng", "Zhewei Wei", "Yuhang Ye"], "title": "Future Link Prediction Without Memory or Aggregation", "categories": ["cs.LG"], "comment": null, "summary": "Future link prediction on temporal graphs is a fundamental task with wide\napplicability in real-world dynamic systems. These scenarios often involve both\nrecurring (seen) and novel (unseen) interactions, requiring models to\ngeneralize effectively across both types of edges. However, existing methods\ntypically rely on complex memory and aggregation modules, yet struggle to\nhandle unseen edges. In this paper, we revisit the architecture of existing\ntemporal graph models and identify two essential but overlooked modeling\nrequirements for future link prediction: representing nodes with unique\nidentifiers and performing target-aware matching between source and destination\nnodes. To this end, we propose Cross-Attention based Future Link Predictor on\nTemporal Graphs (CRAFT), a simple yet effective architecture that discards\nmemory and aggregation modules and instead builds on two components: learnable\nnode embeddings and cross-attention between the destination and the source's\nrecent interactions. This design provides strong expressive power and enables\ntarget-aware modeling of the compatibility between candidate destinations and\nthe source's interaction patterns. Extensive experiments on diverse datasets\ndemonstrate that CRAFT consistently achieves superior performance with high\nefficiency, making it well-suited for large-scale real-world applications."}
{"id": "2505.19670", "pdf": "https://arxiv.org/pdf/2505.19670", "abs": "https://arxiv.org/abs/2505.19670", "authors": ["Hao Yang", "Lizhen Qu", "Ehsan Shareghi", "Gholamreza Haffari"], "title": "Reshaping Representation Space to Balance the Safety and Over-rejection in Large Audio Language Models", "categories": ["cs.CL", "cs.MM", "cs.SD", "eess.AS"], "comment": null, "summary": "Large Audio Language Models (LALMs) have extended the capabilities of Large\nLanguage Models (LLMs) by enabling audio-based human interactions. However,\nrecent research has revealed that LALMs remain vulnerable to harmful queries\ndue to insufficient safety-alignment. Despite advances in defence measures for\ntext and vision LLMs, effective safety-alignment strategies and audio-safety\ndataset specifically targeting LALMs are notably absent. Meanwhile defence\nmeasures based on Supervised Fine-tuning (SFT) struggle to address safety\nimprovement while avoiding over-rejection issues, significantly compromising\nhelpfulness. In this work, we propose an unsupervised safety-fine-tuning\nstrategy as remedy that reshapes model's representation space to enhance\nexisting LALMs safety-alignment while balancing the risk of over-rejection. Our\nexperiments, conducted across three generations of Qwen LALMs, demonstrate that\nour approach significantly improves LALMs safety under three modality input\nconditions (audio-text, text-only, and audio-only) while increasing\nover-rejection rate by only 0.88% on average. Warning: this paper contains\nharmful examples."}
{"id": "2505.18266", "pdf": "https://arxiv.org/pdf/2505.18266", "abs": "https://arxiv.org/abs/2505.18266", "authors": ["Gavin McCracken", "Gabriela Moisescu-Pareja", "Vincent Letourneau", "Doina Precup", "Jonathan Love"], "title": "Uncovering a Universal Abstract Algorithm for Modular Addition in Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose a testable universality hypothesis, asserting that seemingly\ndisparate neural network solutions observed in the simple task of modular\naddition are unified under a common abstract algorithm. While prior work\ninterpreted variations in neuron-level representations as evidence for distinct\nalgorithms, we demonstrate - through multi-level analyses spanning neurons,\nneuron clusters, and entire networks - that multilayer perceptrons and\ntransformers universally implement the abstract algorithm we call the\napproximate Chinese Remainder Theorem. Crucially, we introduce approximate\ncosets and show that neurons activate exclusively on them. Furthermore, our\ntheory works for deep neural networks (DNNs). It predicts that universally\nlearned solutions in DNNs with trainable embeddings or more than one hidden\nlayer require only O(log n) features, a result we empirically confirm. This\nwork thus provides the first theory-backed interpretation of multilayer\nnetworks solving modular addition. It advances generalizable interpretability\nand opens a testable universality hypothesis for group multiplication beyond\nmodular addition."}
{"id": "2505.19423", "pdf": "https://arxiv.org/pdf/2505.19423", "abs": "https://arxiv.org/abs/2505.19423", "authors": ["Bingdong Li", "Mei Jiang", "Hong Qian", "Peng Yang", "Wenjing Hong", "Hong Qian", "Ke Tang"], "title": "Surrogate-Assisted Evolutionary Reinforcement Learning Based on Autoencoder and Hyperbolic Neural Network", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Evolutionary Reinforcement Learning (ERL), training the Reinforcement\nLearning (RL) policies with Evolutionary Algorithms (EAs), have demonstrated\nenhanced exploration capabilities and greater robustness than using traditional\npolicy gradient. However, ERL suffers from the high computational costs and low\nsearch efficiency, as EAs require evaluating numerous candidate policies with\nexpensive simulations, many of which are ineffective and do not contribute\nmeaningfully to the training. One intuitive way to reduce the ineffective\nevaluations is to adopt the surrogates. Unfortunately, existing ERL policies\nare often modeled as deep neural networks (DNNs) and thus naturally represented\nas high-dimensional vectors containing millions of weights, which makes the\nbuilding of effective surrogates for ERL policies extremely challenging. This\npaper proposes a novel surrogate-assisted ERL that integrates Autoencoders (AE)\nand Hyperbolic Neural Networks (HNN). Specifically, AE compresses\nhigh-dimensional policies into low-dimensional representations while extracting\nkey features as the inputs for the surrogate. HNN, functioning as a\nclassification-based surrogate model, can learn complex nonlinear relationships\nfrom sampled data and enable more accurate pre-selection of the sampled\npolicies without real evaluations. The experiments on 10 Atari and 4 Mujoco\ngames have verified that the proposed method outperforms previous approaches\nsignificantly. The search trajectories guided by AE and HNN are also visually\ndemonstrated to be more effective, in terms of both exploration and\nconvergence. This paper not only presents the first learnable policy embedding\nand surrogate-modeling modules for high-dimensional ERL policies, but also\nempirically reveals when and why they can be successful."}
{"id": "2505.19674", "pdf": "https://arxiv.org/pdf/2505.19674", "abs": "https://arxiv.org/abs/2505.19674", "authors": ["Chaoyi Xiang", "Chunhua Liu", "Simon De Deyne", "Lea Frermann"], "title": "Comparing Moral Values in Western English-speaking societies and LLMs with Word Associations", "categories": ["cs.CL"], "comment": "9 pages,7 figures. Accepted to the ACL 2025 conference", "summary": "As the impact of large language models increases, understanding the moral\nvalues they reflect becomes ever more important. Assessing the nature of moral\nvalues as understood by these models via direct prompting is challenging due to\npotential leakage of human norms into model training data, and their\nsensitivity to prompt formulation. Instead, we propose to use word\nassociations, which have been shown to reflect moral reasoning in humans, as\nlow-level underlying representations to obtain a more robust picture of LLMs'\nmoral reasoning. We study moral differences in associations from western\nEnglish-speaking communities and LLMs trained predominantly on English data.\nFirst, we create a large dataset of LLM-generated word associations, resembling\nan existing data set of human word associations. Next, we propose a novel\nmethod to propagate moral values based on seed words derived from Moral\nFoundation Theory through the human and LLM-generated association graphs.\nFinally, we compare the resulting moral conceptualizations, highlighting\ndetailed but systematic differences between moral values emerging from English\nspeakers and LLM associations."}
{"id": "2505.18279", "pdf": "https://arxiv.org/pdf/2505.18279", "abs": "https://arxiv.org/abs/2505.18279", "authors": ["Alireza Rezazadeh", "Zichao Li", "Ange Lou", "Yuying Zhao", "Wei Wei", "Yujia Bao"], "title": "Collaborative Memory: Multi-User Memory Sharing in LLM Agents with Dynamic Access Control", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Complex tasks are increasingly delegated to ensembles of specialized\nLLM-based agents that reason, communicate, and coordinate actions-both among\nthemselves and through interactions with external tools, APIs, and databases.\nWhile persistent memory has been shown to enhance single-agent performance,\nmost approaches assume a monolithic, single-user context-overlooking the\nbenefits and challenges of knowledge transfer across users under dynamic,\nasymmetric permissions. We introduce Collaborative Memory, a framework for\nmulti-user, multi-agent environments with asymmetric, time-evolving access\ncontrols encoded as bipartite graphs linking users, agents, and resources. Our\nsystem maintains two memory tiers: (1) private memory-private fragments visible\nonly to their originating user; and (2) shared memory-selectively shared\nfragments. Each fragment carries immutable provenance attributes (contributing\nagents, accessed resources, and timestamps) to support retrospective permission\nchecks. Granular read policies enforce current user-agent-resource constraints\nand project existing memory fragments into filtered transformed views. Write\npolicies determine fragment retention and sharing, applying context-aware\ntransformations to update the memory. Both policies may be designed conditioned\non system, agent, and user-level information. Our framework enables safe,\nefficient, and interpretable cross-user knowledge sharing, with provable\nadherence to asymmetric, time-varying policies and full auditability of memory\noperations."}
{"id": "2505.19427", "pdf": "https://arxiv.org/pdf/2505.19427", "abs": "https://arxiv.org/abs/2505.19427", "authors": ["Sihan Chen", "Dan Zhao", "Jongwoo Ko", "Colby Banbury", "Huiping Zhuang", "Luming Liang", "Tianyi Chen"], "title": "WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The growing computational demands of large language models (LLMs) make\nefficient inference and activation strategies increasingly critical. While\nrecent approaches, such as Mixture-of-Experts (MoE), leverage selective\nactivation but require specialized training, training-free sparse activation\nmethods offer broader applicability and superior resource efficiency through\ntheir plug-and-play design. However, many existing methods rely solely on\nhidden state magnitudes to determine activation, resulting in high\napproximation errors and suboptimal inference accuracy. To address these\nlimitations, we propose WINA (Weight Informed Neuron Activation), a novel,\nsimple, and training-free sparse activation framework that jointly considers\nhidden state magnitudes and the column-wise $\\ell_2$-norms of weight matrices.\nWe show that this leads to a sparsification strategy that obtains optimal\napproximation error bounds with theoretical guarantees tighter than existing\ntechniques. Empirically, WINA also outperforms state-of-the-art methods (e.g.,\nTEAL) by up to $2.94\\%$ in average performance at the same sparsity levels,\nacross a diverse set of LLM architectures and datasets. These results position\nWINA as a new performance frontier for training-free sparse activation in LLM\ninference, advancing training-free sparse activation methods and setting a\nrobust baseline for efficient inference. The source code is available at\nhttps://github.com/microsoft/wina."}
{"id": "2505.19675", "pdf": "https://arxiv.org/pdf/2505.19675", "abs": "https://arxiv.org/abs/2505.19675", "authors": ["Liqin Ye", "Agam Shah", "Chao Zhang", "Sudheer Chava"], "title": "Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The traditional process of creating labeled datasets is labor-intensive and\nexpensive. Recent breakthroughs in open-source large language models (LLMs)\nhave opened up a new avenue in generating labeled datasets automatically for\nvarious natural language processing (NLP) tasks, providing an alternative to\nsuch an expensive annotation process. However, the reliability of such\nauto-generated labels remains a significant concern due to inherent\ninaccuracies. When learning from noisy labels, the model's generalization is\nlikely to be harmed as it is prone to overfit to those label noises. While\nprevious studies in learning from noisy labels mainly focus on synthetic noise\nand real-world noise, LLM-generated label noise receives less attention. In\nthis paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to\ncalibrate the classifier's prediction, thus enhancing its robustness towards\nLLM-generated noisy labels. SiDyP retrieves potential true label candidates by\nneighborhood label distribution in text embedding space and iteratively refines\nnoisy candidates using a simplex diffusion model. Our framework can increase\nthe performance of the BERT classifier fine-tuned on both zero-shot and\nfew-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30%\nrespectively. We demonstrate the effectiveness of SiDyP by conducting extensive\nbenchmarking for different LLMs over a variety of NLP tasks. Our code is\navailable on Github."}
{"id": "2505.18280", "pdf": "https://arxiv.org/pdf/2505.18280", "abs": "https://arxiv.org/abs/2505.18280", "authors": ["Tsai Hor Chan", "Dora Yan Zhang", "Guosheng Yin", "Lequan Yu"], "title": "Feature Preserving Shrinkage on Bayesian Neural Networks via the R2D2 Prior", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "To appear in TPAMI", "summary": "Bayesian neural networks (BNNs) treat neural network weights as random\nvariables, which aim to provide posterior uncertainty estimates and avoid\noverfitting by performing inference on the posterior weights. However, the\nselection of appropriate prior distributions remains a challenging task, and\nBNNs may suffer from catastrophic inflated variance or poor predictive\nperformance when poor choices are made for the priors. Existing BNN designs\napply different priors to weights, while the behaviours of these priors make it\ndifficult to sufficiently shrink noisy signals or they are prone to\novershrinking important signals in the weights. To alleviate this problem, we\npropose a novel R2D2-Net, which imposes the R^2-induced Dirichlet Decomposition\n(R2D2) prior to the BNN weights. The R2D2-Net can effectively shrink irrelevant\ncoefficients towards zero, while preventing key features from over-shrinkage.\nTo approximate the posterior distribution of weights more accurately, we\nfurther propose a variational Gibbs inference algorithm that combines the Gibbs\nupdating procedure and gradient-based optimization. This strategy enhances\nstability and consistency in estimation when the variational objective\ninvolving the shrinkage parameters is non-convex. We also analyze the evidence\nlower bound (ELBO) and the posterior concentration rates from a theoretical\nperspective. Experiments on both natural and medical image classification and\nuncertainty estimation tasks demonstrate satisfactory performance of our\nmethod."}
{"id": "2505.19431", "pdf": "https://arxiv.org/pdf/2505.19431", "abs": "https://arxiv.org/abs/2505.19431", "authors": ["Chenguang Wang", "Xiaoyu Zhang", "Kaiyuan Cui", "Weichen Zhao", "Yongtao Guan", "Tianshu Yu"], "title": "Importance Weighted Score Matching for Diffusion Samplers with Enhanced Mode Coverage", "categories": ["cs.LG"], "comment": null, "summary": "Training neural samplers directly from unnormalized densities without access\nto target distribution samples presents a significant challenge. A critical\ndesideratum in these settings is achieving comprehensive mode coverage,\nensuring the sampler captures the full diversity of the target distribution.\nHowever, prevailing methods often circumvent the lack of target data by\noptimizing reverse KL-based objectives. Such objectives inherently exhibit\nmode-seeking behavior, potentially leading to incomplete representation of the\nunderlying distribution. While alternative approaches strive for better mode\ncoverage, they typically rely on implicit mechanisms like heuristics or\niterative refinement. In this work, we propose a principled approach for\ntraining diffusion-based samplers by directly targeting an objective analogous\nto the forward KL divergence, which is conceptually known to encourage mode\ncoverage. We introduce \\textit{Importance Weighted Score Matching}, a method\nthat optimizes this desired mode-covering objective by re-weighting the score\nmatching loss using tractable importance sampling estimates, thereby overcoming\nthe absence of target distribution data. We also provide theoretical analysis\nof the bias and variance for our proposed Monte Carlo estimator and the\npractical loss function used in our method. Experiments on increasingly complex\nmulti-modal distributions, including 2D Gaussian Mixture Models with up to 120\nmodes and challenging particle systems with inherent symmetries -- demonstrate\nthat our approach consistently outperforms existing neural samplers across all\ndistributional distance metrics, achieving state-of-the-art results on all\nbenchmarks."}
{"id": "2505.19678", "pdf": "https://arxiv.org/pdf/2505.19678", "abs": "https://arxiv.org/abs/2505.19678", "authors": ["Hao Fang", "Changle Zhou", "Jiawei Kong", "Kuofeng Gao", "Bin Chen", "Tao Liang", "Guojun Ma", "Shu-Tao Xia"], "title": "Grounding Language with Vision: A Conditional Mutual Information Calibrated Decoding Strategy for Reducing Hallucinations in LVLMs", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) are susceptible to hallucinations, where\ngenerated responses seem semantically plausible yet exhibit little or no\nrelevance to the input image. Previous studies reveal that this issue primarily\nstems from LVLMs' over-reliance on language priors while disregarding the\nvisual information during decoding. To alleviate this issue, we introduce a\nnovel Conditional Pointwise Mutual Information (C-PMI) calibrated decoding\nstrategy, which adaptively strengthens the mutual dependency between generated\ntexts and input images to mitigate hallucinations. Unlike existing methods\nsolely focusing on text token sampling, we propose to jointly model the\ncontributions of visual and textual tokens to C-PMI, formulating hallucination\nmitigation as a bi-level optimization problem aimed at maximizing mutual\ninformation. To solve it, we design a token purification mechanism that\ndynamically regulates the decoding process by sampling text tokens remaining\nmaximally relevant to the given image, while simultaneously refining image\ntokens most pertinent to the generated response. Extensive experiments across\nvarious benchmarks reveal that the proposed method significantly reduces\nhallucinations in LVLMs while preserving decoding efficiency."}
{"id": "2505.18282", "pdf": "https://arxiv.org/pdf/2505.18282", "abs": "https://arxiv.org/abs/2505.18282", "authors": ["Nitin Jha", "Abhishek Parakh", "Mahadevan Subramaniam"], "title": "Towards a Quantum-classical Augmented Network", "categories": ["quant-ph", "cs.AI", "cs.CR", "cs.NI"], "comment": null, "summary": "In the past decade, several small-scale quantum key distribution networks\nhave been established. However, the deployment of large-scale quantum networks\ndepends on the development of quantum repeaters, quantum channels, quantum\nmemories, and quantum network protocols. To improve the security of existing\nnetworks and adopt currently feasible quantum technologies, the next step is to\naugment classical networks with quantum devices, properties, and phenomena. To\nachieve this, we propose a change in the structure of the HTTP protocol such\nthat it can carry both quantum and classical payload. This work lays the\nfoundation for dividing one single network packet into classical and quantum\npayloads depending on the privacy needs. We implement logistic regression, CNN,\nLSTM, and BiLSTM models to classify the privacy label for outgoing\ncommunications. This enables reduced utilization of quantum resources allowing\nfor a more efficient secure quantum network design. Experimental results using\nthe proposed methods are presented."}
{"id": "2505.19432", "pdf": "https://arxiv.org/pdf/2505.19432", "abs": "https://arxiv.org/abs/2505.19432", "authors": ["Hao Wu", "Yuan Gao", "Ruiqi Shu", "Kun Wang", "Ruijian Gou", "Chuhan Wu", "Xinliang Liu", "Juncai He", "Shuhao Cao", "Junfeng Fang", "Xingjian Shi", "Feng Tao", "Qi Song", "Shengxuan Ji", "Yanfei Xiang", "Yuze Sun", "Jiahao Li", "Fan Xu", "Huanshuo Dong", "Haixin Wang", "Fan Zhang", "Penghao Zhao", "Xian Wu", "Qingsong Wen", "Deliang Chen", "Xiaomeng Huang"], "title": "Advanced long-term earth system forecasting by learning the small-scale nature", "categories": ["cs.LG"], "comment": null, "summary": "Reliable long-term forecast of Earth system dynamics is heavily hampered by\ninstabilities in current AI models during extended autoregressive simulations.\nThese failures often originate from inherent spectral bias, leading to\ninadequate representation of critical high-frequency, small-scale processes and\nsubsequent uncontrolled error amplification. We present Triton, an AI framework\ndesigned to address this fundamental challenge. Inspired by increasing grids to\nexplicitly resolve small scales in numerical models, Triton employs a\nhierarchical architecture processing information across multiple resolutions to\nmitigate spectral bias and explicitly model cross-scale dynamics. We\ndemonstrate Triton's superior performance on challenging forecast tasks,\nachieving stable year-long global temperature forecasts, skillful Kuroshio eddy\npredictions till 120 days, and high-fidelity turbulence simulations preserving\nfine-scale structures all without external forcing, with significantly\nsurpassing baseline AI models in long-term stability and accuracy. By\neffectively suppressing high-frequency error accumulation, Triton offers a\npromising pathway towards trustworthy AI-driven simulation for climate and\nearth system science."}
{"id": "2505.19679", "pdf": "https://arxiv.org/pdf/2505.19679", "abs": "https://arxiv.org/abs/2505.19679", "authors": ["Zhaolin Li", "Yining Liu", "Danni Liu", "Tuan Nam Nguyen", "Enes Yavuz Ugan", "Tu Anh Dinh", "Carlos Mullov", "Alexander Waibel", "Jan Niehues"], "title": "KIT's Low-resource Speech Translation Systems for IWSLT2025: System Enhancement with Synthetic Data and Model Regularization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents KIT's submissions to the IWSLT 2025 low-resource track.\nWe develop both cascaded systems, consisting of Automatic Speech Recognition\n(ASR) and Machine Translation (MT) models, and end-to-end (E2E) Speech\nTranslation (ST) systems for three language pairs: Bemba, North Levantine\nArabic, and Tunisian Arabic into English. Building upon pre-trained models, we\nfine-tune our systems with different strategies to utilize resources\nefficiently. This study further explores system enhancement with synthetic data\nand model regularization. Specifically, we investigate MT-augmented ST by\ngenerating translations from ASR data using MT models. For North Levantine,\nwhich lacks parallel ST training data, a system trained solely on synthetic\ndata slightly surpasses the cascaded system trained on real data. We also\nexplore augmentation using text-to-speech models by generating synthetic speech\nfrom MT data, demonstrating the benefits of synthetic data in improving both\nASR and ST performance for Bemba. Additionally, we apply intra-distillation to\nenhance model performance. Our experiments show that this approach consistently\nimproves results across ASR, MT, and ST tasks, as well as across different\npre-trained models. Finally, we apply Minimum Bayes Risk decoding to combine\nthe cascaded and end-to-end systems, achieving an improvement of approximately\n1.5 BLEU points."}
{"id": "2505.18283", "pdf": "https://arxiv.org/pdf/2505.18283", "abs": "https://arxiv.org/abs/2505.18283", "authors": ["Jianghao Wu", "Feilong Tang", "Yulong Li", "Ming Hu", "Haochen Xue", "Shoaib Jameel", "Yutong Xie", "Imran Razzak"], "title": "TAGS: A Test-Time Generalist-Specialist Framework with Retrieval-Augmented Reasoning and Verification", "categories": ["cs.CL", "cs.AI", "cs.MA", "I.2.7"], "comment": "16 pages including references, 2 figures", "summary": "Recent advances such as Chain-of-Thought prompting have significantly\nimproved large language models (LLMs) in zero-shot medical reasoning. However,\nprompting-based methods often remain shallow and unstable, while fine-tuned\nmedical LLMs suffer from poor generalization under distribution shifts and\nlimited adaptability to unseen clinical scenarios. To address these\nlimitations, we present TAGS, a test-time framework that combines a broadly\ncapable generalist with a domain-specific specialist to offer complementary\nperspectives without any model fine-tuning or parameter updates. To support\nthis generalist-specialist reasoning process, we introduce two auxiliary\nmodules: a hierarchical retrieval mechanism that provides multi-scale exemplars\nby selecting examples based on both semantic and rationale-level similarity,\nand a reliability scorer that evaluates reasoning consistency to guide final\nanswer aggregation. TAGS achieves strong performance across nine MedQA\nbenchmarks, boosting GPT-4o accuracy by 13.8%, DeepSeek-R1 by 16.8%, and\nimproving a vanilla 7B model from 14.1% to 23.9%. These results surpass several\nfine-tuned medical LLMs, without any parameter updates. The code will be\navailable at https://github.com/JianghaoWu/TAGS."}
{"id": "2505.19433", "pdf": "https://arxiv.org/pdf/2505.19433", "abs": "https://arxiv.org/abs/2505.19433", "authors": ["Peijie Dong", "Zhenheng Tang", "Xiang Liu", "Lujun Li", "Xiaowen Chu", "Bo Li"], "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression", "categories": ["cs.LG"], "comment": "Accepted by ICML2025 as Poster", "summary": "Post-training compression reduces the computational and memory costs of large\nlanguage models (LLMs), enabling resource-efficient deployment. However,\nexisting compression benchmarks only focus on language modeling (e.g.,\nperplexity) and natural language understanding tasks (e.g., GLUE accuracy),\nignoring the agentic capabilities - workflow, tool use/function call,\nlong-context understanding and real-world application. We introduce the Agent\nCompression Benchmark (ACBench), the first comprehensive benchmark for\nevaluating how compression impacts LLMs' agentic abilities. ACBench spans (1)\n12 tasks across 4 capabilities (e.g., WorfBench for workflow generation,\nNeedle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ)\nand pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B),\nstandard (Qwen2.5 7B-32B), and distilled reasoning LLMs (DeepSeek-R1-Distill).\nOur experiments reveal compression tradeoffs: 4-bit quantization preserves\nworkflow generation and tool use (1%-3% drop) but degrades real-world\napplication accuracy by 10%-15%. We introduce ERank, Top-k Ranking Correlation\nand Energy to systematize analysis. ACBench provides actionable insights for\noptimizing LLM compression in agentic scenarios. The code can be found in\nhttps://github.com/pprp/ACBench."}
{"id": "2505.19700", "pdf": "https://arxiv.org/pdf/2505.19700", "abs": "https://arxiv.org/abs/2505.19700", "authors": ["Yi Liu", "Dianqing Liu", "Mingye Zhu", "Junbo Guo", "Yongdong Zhang", "Zhendong Mao"], "title": "Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The widespread adoption of large language models (LLMs) across industries has\nincreased the demand for high-quality and customizable outputs. However,\ntraditional alignment methods often require retraining large pretrained models,\nmaking it difficult to quickly adapt and optimize LLMs for diverse\napplications. To address this limitation, we propose a novel \\textit{Residual\nAlignment Model} (\\textit{RAM}) that formalizes the alignment process as a type\nof importance sampling. In this framework, the unaligned upstream model serves\nas the proposal distribution, while the alignment process is framed as\nsecondary sampling based on an autoregressive alignment module that acts as an\nestimator of the importance weights. This design enables a natural detachment\nof the alignment module from the target aligned model, improving flexibility\nand scalability. Based on this model, we derive an efficient sequence-level\ntraining strategy for the alignment module, which operates independently of the\nproposal module. Additionally, we develop a resampling algorithm with iterative\ntoken-level decoding to address the common first-token latency issue in\ncomparable methods. Experimental evaluations on two leading open-source LLMs\nacross diverse tasks, including instruction following, domain adaptation, and\npreference optimization, demonstrate that our approach consistently outperforms\nbaseline models."}
{"id": "2505.18284", "pdf": "https://arxiv.org/pdf/2505.18284", "abs": "https://arxiv.org/abs/2505.18284", "authors": ["Pritam Anand", "Aadesh Minz", "Asish Joel"], "title": "Tube Loss based Deep Networks For Improving the Probabilistic Forecasting of Wind Speed", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Uncertainty Quantification (UQ) in wind speed forecasting is a critical\nchallenge in wind power production due to the inherently volatile nature of\nwind. By quantifying the associated risks and returns, UQ supports more\neffective decision-making for grid operations and participation in the\nelectricity market. In this paper, we design a sequence of deep learning based\nprobabilistic forecasting methods by using the Tube loss function for wind\nspeed forecasting. The Tube loss function is a simple and model agnostic\nPrediction Interval (PI) estimation approach and can obtain the narrow PI with\nasymptotical coverage guarantees without any distribution assumption. Our deep\nprobabilistic forecasting models effectively incorporate popular architectures\nsuch as LSTM, GRU, and TCN within the Tube loss framework. We further design a\nsimple yet effective heuristic for tuning the $\\delta$ parameter of the Tube\nloss function so that our deep forecasting models obtain the narrower PI\nwithout compromising its calibration ability. We have considered three wind\ndatasets, containing the hourly recording of the wind speed, collected from\nthree distinct location namely Jaisalmer, Los Angeles and San Fransico. Our\nnumerical results demonstrate that the proposed deep forecasting models produce\nmore reliable and narrower PIs compared to recently developed probabilistic\nwind forecasting methods."}
{"id": "2505.19445", "pdf": "https://arxiv.org/pdf/2505.19445", "abs": "https://arxiv.org/abs/2505.19445", "authors": ["Rishabh Bhattacharya", "Hari Shankar", "Vaishnavi Shivkumar", "Ponnurangam Kumaraguru"], "title": "MetaGMT: Improving Actionable Interpretability of Graph Multilinear Networks via Meta-Learning Filtration", "categories": ["cs.LG"], "comment": "8 Pages Main Content, 10 Pages including Appendix. 1 Figure, 7 Tables", "summary": "The growing adoption of Graph Neural Networks (GNNs) in high-stakes domains\nlike healthcare and finance demands reliable explanations of their\ndecision-making processes. While inherently interpretable GNN architectures\nlike Graph Multi-linear Networks (GMT) have emerged, they remain vulnerable to\ngenerating explanations based on spurious correlations, potentially undermining\ntrust in critical applications. We present MetaGMT, a meta-learning framework\nthat enhances explanation fidelity through a novel bi-level optimization\napproach. We demonstrate that MetaGMT significantly improves both explanation\nquality (AUC-ROC, Precision@K) and robustness to spurious patterns, across\nBA-2Motifs, MUTAG, and SP-Motif benchmarks. Our approach maintains competitive\nclassification accuracy while producing more faithful explanations (with an\nincrease up to 8% of Explanation ROC on SP-Motif 0.5) compared to baseline\nmethods. These advancements in interpretability could enable safer deployment\nof GNNs in sensitive domains by (1) facilitating model debugging through more\nreliable explanations, (2) supporting targeted retraining when biases are\nidentified, and (3) enabling meaningful human oversight. By addressing the\ncritical challenge of explanation reliability, our work contributes to building\nmore trustworthy and actionable GNN systems for real-world applications."}
{"id": "2505.19706", "pdf": "https://arxiv.org/pdf/2505.19706", "abs": "https://arxiv.org/abs/2505.19706", "authors": ["Tej Deep Pala", "Panshul Sharma", "Amir Zadeh", "Chuan Li", "Soujanya Poria"], "title": "Error Typing for Smarter Rewards: Improving Process Reward Models with Error-Aware Hierarchical Supervision", "categories": ["cs.CL", "cs.AI"], "comment": "https://github.com/declare-lab/PathFinder-PRM", "summary": "Large Language Models (LLMs) are prone to hallucination, especially during\nmulti-hop and reasoning-intensive tasks such as mathematical problem solving.\nWhile Outcome Reward Models verify only final answers, Process Reward Models\n(PRMs) score each intermediate step to steer generation toward coherent\nsolutions. We introduce PathFinder-PRM, a novel hierarchical, error-aware\ndiscriminative PRM that first classifies math and consistency errors at each\nstep, then combines these fine-grained signals to estimate step correctness. To\ntrain PathFinder-PRM, we construct a 400K-sample dataset by enriching the\nhuman-annotated PRM800K corpus and RLHFlow Mistral traces with\nthree-dimensional step-level labels. On PRMBench, PathFinder-PRM achieves a new\nstate-of-the-art PRMScore of 67.7, outperforming the prior best (65.5) while\nusing 3 times less data. When applied to reward guided greedy search, our model\nyields prm@8 48.3, a +1.5 point gain over the strongest baseline. These results\ndemonstrate that decoupled error detection and reward estimation not only boost\nfine-grained error detection but also substantially improve end-to-end,\nreward-guided mathematical reasoning with greater data efficiency."}
{"id": "2505.18286", "pdf": "https://arxiv.org/pdf/2505.18286", "abs": "https://arxiv.org/abs/2505.18286", "authors": ["Mingyan Gao", "Yanzi Li", "Banruo Liu", "Yifan Yu", "Phillip Wang", "Ching-Yu Lin", "Fan Lai"], "title": "Single-agent or Multi-agent Systems? Why Not Both?", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "Multi-agent systems (MAS) decompose complex tasks and delegate subtasks to\ndifferent large language model (LLM) agents and tools. Prior studies have\nreported the superior accuracy performance of MAS across diverse domains,\nenabled by long-horizon context tracking and error correction through\nrole-specific agents. However, the design and deployment of MAS incur higher\ncomplexity and runtime cost compared to single-agent systems (SAS). Meanwhile,\nfrontier LLMs, such as OpenAI-o3 and Gemini-2.5-Pro, have rapidly advanced in\nlong-context reasoning, memory retention, and tool usage, mitigating many\nlimitations that originally motivated MAS designs. In this paper, we conduct an\nextensive empirical study comparing MAS and SAS across various popular agentic\napplications. We find that the benefits of MAS over SAS diminish as LLM\ncapabilities improve, and we propose efficient mechanisms to pinpoint the\nerror-prone agent in MAS. Furthermore, the performance discrepancy between MAS\nand SAS motivates our design of a hybrid agentic paradigm, request cascading\nbetween MAS and SAS, to improve both efficiency and capability. Our design\nimproves accuracy by 1.1-12% while reducing deployment costs by up to 20%\nacross various agentic applications."}
{"id": "2505.19458", "pdf": "https://arxiv.org/pdf/2505.19458", "abs": "https://arxiv.org/abs/2505.19458", "authors": ["Akiyoshi Tomihari", "Ryo Karakida"], "title": "Recurrent Self-Attention Dynamics: An Energy-Agnostic Perspective from Jacobians", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.NE", "stat.ML"], "comment": null, "summary": "The theoretical understanding of self-attention (SA) has been steadily\nprogressing. A prominent line of work studies a class of SA layers that admit\nan energy function decreased by state updates. While it provides valuable\ninsights into inherent biases in signal propagation, it often relies on\nidealized assumptions or additional constraints not necessarily present in\nstandard SA. Thus, to broaden our understanding, this work aims to relax these\nenergy constraints and provide an energy-agnostic characterization of inference\ndynamics by dynamical systems analysis. In more detail, we first consider\nrelaxing the symmetry and single-head constraints traditionally required in\nenergy-based formulations. Next, to investigate more general SA architectures\ncapable of oscillatory dynamics without necessarily admitting an energy\nfunction, we analyze the Jacobian matrix of the state. We reveal that\nnormalization layers effectively normalize the Jacobian's complex eigenvalues,\nforcing the dynamics close to a critical state. This significantly enhances\ninference performance. Furthermore, we utilize the Jacobian perspective to\ndevelop regularization methods for training and a pseudo-energy for monitoring\ninference dynamics."}
{"id": "2505.19714", "pdf": "https://arxiv.org/pdf/2505.19714", "abs": "https://arxiv.org/abs/2505.19714", "authors": ["Zhaopeng Feng", "Yupu Liang", "Shaosheng Cao", "Jiayuan Su", "Jiahan Ren", "Zhe Xu", "Yao Hu", "Wenxuan Huang", "Jian Wu", "Zuozhu Liu"], "title": "MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via Multi-Task Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in progress", "summary": "Text Image Machine Translation (TIMT)-the task of translating textual content\nembedded in images-is critical for applications in accessibility, cross-lingual\ninformation access, and real-world document understanding. However, TIMT\nremains a complex challenge due to the need for accurate optical character\nrecognition (OCR), robust visual-text reasoning, and high-quality translation,\noften requiring cascading multi-stage pipelines. Recent advances in large-scale\nReinforcement Learning (RL) have improved reasoning in Large Language Models\n(LLMs) and Multimodal LLMs (MLLMs), but their application to end-to-end TIMT is\nstill underexplored. To bridge this gap, we introduce MT$^{3}$, the first\nframework to apply Multi-Task RL to MLLMs for end-to-end TIMT. MT$^{3}$ adopts\na multi-task optimization paradigm targeting three key sub-skills: text\nrecognition, context-aware reasoning, and translation. It is trained using a\nnovel multi-mixed reward mechanism that adapts rule-based RL strategies to\nTIMT's intricacies, offering fine-grained, non-binary feedback across tasks.\nFurthermore, to facilitate the evaluation of TIMT in authentic cross-cultural\nand real-world social media contexts, we introduced XHSPost, the first social\nmedia TIMT benchmark. Our MT$^{3}$-7B-Zero achieves state-of-the-art results on\nthe latest in-domain MIT-10M benchmark, outperforming strong baselines such as\nQwen2.5-VL-72B and InternVL2.5-78B by notable margins across multiple metrics.\nAdditionally, the model shows strong generalization to out-of-distribution\nlanguage pairs and datasets. In-depth analyses reveal how multi-task synergy,\nreinforcement learning initialization, curriculum design, and reward\nformulation contribute to advancing MLLM-driven TIMT."}
{"id": "2505.18287", "pdf": "https://arxiv.org/pdf/2505.18287", "abs": "https://arxiv.org/abs/2505.18287", "authors": ["Pallavi Jain", "Andrzej Kaczmarczyk"], "title": "Efficient Algorithms for Electing Successive Committees", "categories": ["cs.GT", "cs.AI"], "comment": "18 pages; 3 figures, accepted for publication in IJCAI-25", "summary": "In a recently introduced model of successive committee elections (Bredereck\net al., AAAI-20) for a given set of ordinal or approval preferences one aims to\nfind a sequence of a given length of \"best\" same-size committees such that each\ncandidate is a member of a limited number of consecutive committees. However,\nthe practical usability of this model remains limited, as the described task\nturns out to be NP-hard for most selection criteria already for seeking\ncommittees of size three. Non-trivial or somewhat efficient algorithms for\nthese cases are lacking too. Motivated by a desire to unlock the full potential\nof the described temporal model of committee elections, we devise\n(parameterized) algorithms that effectively solve the mentioned hard cases in\nrealistic scenarios of a moderate number of candidates or of a limited time\nhorizon."}
{"id": "2505.19459", "pdf": "https://arxiv.org/pdf/2505.19459", "abs": "https://arxiv.org/abs/2505.19459", "authors": ["Kaichao Jiang", "He Wang", "Xiaoshuai Hao", "Xiulong Yang", "Ajian Liu", "Qi Chu", "Yunfeng Diao"], "title": "Your Classifier Can Do More: Towards Bridging the Gaps in Classification, Robustness, and Generation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Joint Energy-based Models (JEMs), a class of hybrid generative-discriminative\nmodels, are well known for their ability to achieve both high classification\naccuracy and generative capability within a single model. However, their\nrobustness still lags significantly behind the classifiers based adversarial\ntraining (AT). Conversely, while AT is currently the most effective approach to\nimproving the classifier's robustness, it typically sacrifices accuracy on\nclean data and lacks generative capability. The triple trade-off between\nclassification accuracy, generative capability and robustness, raises a natural\nquestion: Can a single model simultaneously achieve high classification\naccuracy, adversarial robustness, and generative performance? -- a goal that\nhas been rarely explored. To address this question, we systematically analyze\nthe energy distribution differences of clean, adversarial, and generated\nsamples across various JEM variants and adversarially trained models. We\nobserve that AT tends to reduce the energy gap between clean and adversarial\nsamples, while JEMs reduce the gap between clean and synthetic ones. This\nobservation suggests a key insight: if the energy distributions of all three\ndata types can be aligned, we might unify the strengths of AT and JEMs,\nresolving their inherent trade-offs. Building on this idea, we propose\nEnergy-based Joint Distribution Adversarial Training (EB-JDAT), to jointly\nmodel the clean data distribution, the adversarial distribution, and the\nclassifier by maximizing their joint probability. EB-JDAT is a general and\nflexible optimization method, compatible with various JEM variants. Extensive\nexperimental results demonstrate that EB-JDAT not only maintains near original\naccuracy and generative capability of JEMs, but also significantly enhances\nrobustness, even surpassing state-of-the-art ATs."}
{"id": "2505.19715", "pdf": "https://arxiv.org/pdf/2505.19715", "abs": "https://arxiv.org/abs/2505.19715", "authors": ["Chunyang Jiang", "Chi-min Chan", "Yiyang Cai", "Yulong Liu", "Wei Xue", "Yike Guo"], "title": "Graceful Forgetting in Generative Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "8 pages, 6 figures", "summary": "Recently, the pretrain-finetune paradigm has become a cornerstone in various\ndeep learning areas. While in general the pre-trained model would promote both\neffectiveness and efficiency of downstream tasks fine-tuning, studies have\nshown that not all knowledge acquired during pre-training is beneficial. Some\nof the knowledge may actually bring detrimental effects to the fine-tuning\ntasks, which is also known as negative transfer. To address this problem,\ngraceful forgetting has emerged as a promising approach. The core principle of\ngraceful forgetting is to enhance the learning plasticity of the target task by\nselectively discarding irrelevant knowledge. However, this approach remains\nunderexplored in the context of generative language models, and it is often\nchallenging to migrate existing forgetting algorithms to these models due to\narchitecture incompatibility. To bridge this gap, in this paper we propose a\nnovel framework, Learning With Forgetting (LWF), to achieve graceful forgetting\nin generative language models. With Fisher Information Matrix weighting the\nintended parameter updates, LWF computes forgetting confidence to evaluate\nself-generated knowledge regarding the forgetting task, and consequently,\nknowledge with high confidence is periodically unlearned during fine-tuning.\nOur experiments demonstrate that, although thoroughly uncovering the mechanisms\nof knowledge interaction remains challenging in pre-trained language models,\napplying graceful forgetting can contribute to enhanced fine-tuning\nperformance."}
{"id": "2505.18315", "pdf": "https://arxiv.org/pdf/2505.18315", "abs": "https://arxiv.org/abs/2505.18315", "authors": ["Mariano Rivera", "Angello Hoyos"], "title": "COLORA: Efficient Fine-Tuning for Convolutional Models with a Study Case on Optical Coherence Tomography Image Classification", "categories": ["cs.CV", "cs.AI", "68T07", "I.1.2; I.4.0; I.4.10; I.4.0"], "comment": "15 pages, 12 figures. Submitted to Jou. Pattern Recognition", "summary": "We introduce the Convolutional Low-Rank Adaptation (CoLoRA) method, designed\nexplicitly to overcome the inefficiencies found in current CNN fine-tuning\nmethods. CoLoRA can be seen as a natural extension of the convolutional\narchitectures of the Low-Rank Adaptation (LoRA) technique. We demonstrate the\ncapabilities of our method by developing and evaluating models using the widely\nadopted CNN backbone pre-trained on ImageNet. We observed that this strategy\nresults in a stable and accurate coarse-tuning procedure. Moreover, this\nstrategy is computationally efficient and significantly reduces the number of\nparameters required for fine-tuning compared to traditional methods.\nFurthermore, our method substantially improves the speed and stability of\ntraining. Our case study focuses on classifying retinal diseases from optical\ncoherence tomography (OCT) images, specifically using the OCTMNIST dataset.\nExperimental results demonstrate that a CNN backbone fine-tuned with CoLoRA\nsurpasses nearly 1\\% in accuracy. Such a performance is comparable to the\nVision Transformer, State-space discrete, and Kolmogorov-Arnold network models."}
{"id": "2505.19465", "pdf": "https://arxiv.org/pdf/2505.19465", "abs": "https://arxiv.org/abs/2505.19465", "authors": ["Hengwei Zhang", "Minghui Wu", "Li Qiao", "Ling Liu", "Ziqi Han", "Zhen Gao"], "title": "Residual Cross-Attention Transformer-Based Multi-User CSI Feedback with Deep Joint Source-Channel Coding", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This letter proposes a deep-learning (DL)-based multi-user channel state\ninformation (CSI) feedback framework for massive multiple-input multiple-output\nsystems, where the deep joint source-channel coding (DJSCC) is utilized to\nimprove the CSI reconstruction accuracy. Specifically, we design a multi-user\njoint CSI feedback framework, whereby the CSI correlation of nearby users is\nutilized to reduce the feedback overhead. Under the framework, we propose a new\nresidual cross-attention transformer architecture, which is deployed at the\nbase station to further improve the CSI feedback performance. Moreover, to\ntackle the \"cliff-effect\" of conventional bit-level CSI feedback approaches, we\nintegrated DJSCC into the multi-user CSI feedback, together with utilizing a\ntwo-stage training scheme to adapt to varying uplink noise levels. Experimental\nresults demonstrate the superiority of our methods in CSI feedback performance,\nwith low network complexity and better scalability."}
{"id": "2505.19722", "pdf": "https://arxiv.org/pdf/2505.19722", "abs": "https://arxiv.org/abs/2505.19722", "authors": ["Yihao Ai", "Zhiyuan Ning", "Weiwei Dai", "Pengfei Wang", "Yi Du", "Wenjuan Cui", "Kunpeng Liu", "Yuanchun Zhou"], "title": "Distilling Closed-Source LLM's Knowledge for Locally Stable and Economic Biomedical Entity Linking", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICIC 2025", "summary": "Biomedical entity linking aims to map nonstandard entities to standard\nentities in a knowledge base. Traditional supervised methods perform well but\nrequire extensive annotated data to transfer, limiting their usage in\nlow-resource scenarios. Large language models (LLMs), especially closed-source\nLLMs, can address these but risk stability issues and high economic costs:\nusing these models is restricted by commercial companies and brings significant\neconomic costs when dealing with large amounts of data. To address this, we\npropose ``RPDR'', a framework combining closed-source LLMs and open-source LLMs\nfor re-ranking candidates retrieved by a retriever fine-tuned with a small\namount of data. By prompting a closed-source LLM to generate training data from\nunannotated data and fine-tuning an open-source LLM for re-ranking, we\neffectively distill the knowledge to the open-source LLM that can be deployed\nlocally, thus avoiding the stability issues and the problem of high economic\ncosts. We evaluate RPDR on two datasets, including one real-world dataset and\none publicly available dataset involving two languages: Chinese and English.\nRPDR achieves 0.019 Acc@1 improvement and 0.036 Acc@1 improvement on the Aier\ndataset and the Ask A Patient dataset when the amount of training data is not\nenough. The results demonstrate the superiority and generalizability of the\nproposed framework."}
{"id": "2505.18322", "pdf": "https://arxiv.org/pdf/2505.18322", "abs": "https://arxiv.org/abs/2505.18322", "authors": ["Zhuozhuo Joy Liu", "Farhan Samir", "Mehar Bhatia", "Laura K. Nelson", "Vered Shwartz"], "title": "Is It Bad to Work All the Time? Cross-Cultural Evaluation of Social Norm Biases in GPT-4", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "LLMs have been demonstrated to align with the values of Western or North\nAmerican cultures. Prior work predominantly showed this effect through\nleveraging surveys that directly ask (originally people and now also LLMs)\nabout their values. However, it is hard to believe that LLMs would consistently\napply those values in real-world scenarios. To address that, we take a\nbottom-up approach, asking LLMs to reason about cultural norms in narratives\nfrom different cultures. We find that GPT-4 tends to generate norms that, while\nnot necessarily incorrect, are significantly less culture-specific. In\naddition, while it avoids overtly generating stereotypes, the stereotypical\nrepresentations of certain cultures are merely hidden rather than suppressed in\nthe model, and such stereotypes can be easily recovered. Addressing these\nchallenges is a crucial step towards developing LLMs that fairly serve their\ndiverse user base."}
{"id": "2505.19469", "pdf": "https://arxiv.org/pdf/2505.19469", "abs": "https://arxiv.org/abs/2505.19469", "authors": ["Mingzhuo Li", "Guang Li", "Jiafeng Mao", "Takahiro Ogawa", "Miki Haseyama"], "title": "Diversity-Driven Generative Dataset Distillation Based on Diffusion Model with Self-Adaptive Memory", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Accepted by ICIP 2025", "summary": "Dataset distillation enables the training of deep neural networks with\ncomparable performance in significantly reduced time by compressing large\ndatasets into small and representative ones. Although the introduction of\ngenerative models has made great achievements in this field, the distributions\nof their distilled datasets are not diverse enough to represent the original\nones, leading to a decrease in downstream validation accuracy. In this paper,\nwe present a diversity-driven generative dataset distillation method based on a\ndiffusion model to solve this problem. We introduce self-adaptive memory to\nalign the distribution between distilled and real datasets, assessing the\nrepresentativeness. The degree of alignment leads the diffusion model to\ngenerate more diverse datasets during the distillation process. Extensive\nexperiments show that our method outperforms existing state-of-the-art methods\nin most situations, proving its ability to tackle dataset distillation tasks."}
{"id": "2505.19743", "pdf": "https://arxiv.org/pdf/2505.19743", "abs": "https://arxiv.org/abs/2505.19743", "authors": ["Yang Zhang", "Yu Yu", "Bo Tang", "Yu Zhu", "Chuxiong Sun", "Wenqiang Wei", "Jie Hu", "Zipeng Xie", "Zhiyu Li", "Feiyu Xiong", "Edward Chung"], "title": "Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "summary": "With the rapid development of Large Language Models (LLMs), aligning these\nmodels with human preferences and values is critical to ensuring ethical and\nsafe applications. However, existing alignment techniques such as RLHF or DPO\noften require direct fine-tuning on LLMs with billions of parameters, resulting\nin substantial computational costs and inefficiencies. To address this, we\npropose Micro token-level Accept-Reject Aligning (MARA) approach designed to\noperate independently of the language models. MARA simplifies the alignment\nprocess by decomposing sentence-level preference learning into token-level\nbinary classification, where a compact three-layer fully-connected network\ndetermines whether candidate tokens are \"Accepted\" or \"Rejected\" as part of the\nresponse. Extensive experiments across seven different LLMs and three\nopen-source datasets show that MARA achieves significant improvements in\nalignment performance while reducing computational costs."}
{"id": "2505.18323", "pdf": "https://arxiv.org/pdf/2505.18323", "abs": "https://arxiv.org/abs/2505.18323", "authors": ["Nicolas Kchler", "Ivan Petrov", "Conrad Grobler", "Ilia Shumailov"], "title": "Architectural Backdoors for Within-Batch Data Stealing and Model Inference Manipulation", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "For nearly a decade the academic community has investigated backdoors in\nneural networks, primarily focusing on classification tasks where adversaries\nmanipulate the model prediction. While demonstrably malicious, the immediate\nreal-world impact of such prediction-altering attacks has remained unclear. In\nthis paper we introduce a novel and significantly more potent class of\nbackdoors that builds upon recent advancements in architectural backdoors. We\ndemonstrate how these backdoors can be specifically engineered to exploit\nbatched inference, a common technique for hardware utilization, enabling\nlarge-scale user data manipulation and theft. By targeting the batching\nprocess, these architectural backdoors facilitate information leakage between\nconcurrent user requests and allow attackers to fully control model responses\ndirected at other users within the same batch. In other words, an attacker who\ncan change the model architecture can set and steal model inputs and outputs of\nother users within the same batch. We show that such attacks are not only\nfeasible but also alarmingly effective, can be readily injected into prevalent\nmodel architectures, and represent a truly malicious threat to user privacy and\nsystem integrity. Critically, to counteract this new class of vulnerabilities,\nwe propose a deterministic mitigation strategy that provides formal guarantees\nagainst this new attack vector, unlike prior work that relied on Large Language\nModels to find the backdoors. Our mitigation strategy employs a novel\nInformation Flow Control mechanism that analyzes the model graph and proves\nnon-interference between different user inputs within the same batch. Using our\nmitigation strategy we perform a large scale analysis of models hosted through\nHugging Face and find over 200 models that introduce (unintended) information\nleakage between batch entries due to the use of dynamic quantization."}
{"id": "2505.19481", "pdf": "https://arxiv.org/pdf/2505.19481", "abs": "https://arxiv.org/abs/2505.19481", "authors": ["Hao Kang", "Qingru Zhang", "Han Cai", "Weiyuan Xu", "Tushar Krishna", "Yilun Du", "Tsachy Weissman"], "title": "Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.MA"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable performance across diverse\nreasoning and generation tasks, and are increasingly deployed as agents in\ndynamic environments such as code generation and recommendation systems.\nHowever, many real-world applications, such as high-frequency trading and\nreal-time competitive gaming, require decisions under strict latency\nconstraints, where faster responses directly translate into higher rewards.\nDespite the importance of this latency quality trade off, it remains\nunderexplored in the context of LLM based agents. In this work, we present the\nfirst systematic study of this trade off in real time decision making tasks. To\nsupport our investigation, we introduce two new benchmarks: HFTBench, a high\nfrequency trading simulation, and StreetFighter, a competitive gaming platform.\nOur analysis reveals that optimal latency quality balance varies by task, and\nthat sacrificing quality for lower latency can significantly enhance downstream\nperformance. To address this, we propose FPX, an adaptive framework that\ndynamically selects model size and quantization level based on real time\ndemands. Our method achieves the best performance on both benchmarks, improving\nwin rate by up to 80% in Street Fighter and boosting daily yield by up to\n26.52% in trading, underscoring the need for latency aware evaluation and\ndeployment strategies for LLM based agents. These results demonstrate the\ncritical importance of latency aware evaluation and deployment strategies for\nreal world LLM based agents. Our benchmarks are available at Latency Sensitive\nBenchmarks."}
{"id": "2505.19754", "pdf": "https://arxiv.org/pdf/2505.19754", "abs": "https://arxiv.org/abs/2505.19754", "authors": ["Ruisheng Cao", "Hanchong Zhang", "Tiancheng Huang", "Zhangyi Kang", "Yuxin Zhang", "Liangtai Sun", "Hanqi Li", "Yuxun Miao", "Shuai Fan", "Lu Chen", "Kai Yu"], "title": "NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "29 pages, 11 figures, 12 tables, accepted to ACL 2025 Long Main", "summary": "The increasing number of academic papers poses significant challenges for\nresearchers to efficiently acquire key details. While retrieval augmented\ngeneration (RAG) shows great promise in large language model (LLM) based\nautomated question answering, previous works often isolate neural and symbolic\nretrieval despite their complementary strengths. Moreover, conventional\nsingle-view chunking neglects the rich structure and layout of PDFs, e.g.,\nsections and tables. In this work, we propose NeuSym-RAG, a hybrid neural\nsymbolic retrieval framework which combines both paradigms in an interactive\nprocess. By leveraging multi-view chunking and schema-based parsing, NeuSym-RAG\norganizes semi-structured PDF content into both the relational database and\nvectorstore, enabling LLM agents to iteratively gather context until sufficient\nto generate answers. Experiments on three full PDF-based QA datasets, including\na self-annotated one AIRQA-REAL, show that NeuSym-RAG stably defeats both the\nvector-based RAG and various structured baselines, highlighting its capacity to\nunify both retrieval schemes and utilize multiple views. Code and data are\npublicly available at https://github.com/X-LANCE/NeuSym-RAG."}
{"id": "2505.18331", "pdf": "https://arxiv.org/pdf/2505.18331", "abs": "https://arxiv.org/abs/2505.18331", "authors": ["Naghmeh Jamali", "Milad Mohammadi", "Danial Baledi", "Zahra Rezvani", "Hesham Faili"], "title": "PerMedCQA: Benchmarking Large Language Models on Medical Consumer Question Answering in Persian Language", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Medical consumer question answering (CQA) is crucial for empowering patients\nby providing personalized and reliable health information. Despite recent\nadvances in large language models (LLMs) for medical QA, consumer-oriented and\nmultilingual resources, particularly in low-resource languages like Persian,\nremain sparse. To bridge this gap, we present PerMedCQA, the first\nPersian-language benchmark for evaluating LLMs on real-world,\nconsumer-generated medical questions. Curated from a large medical QA forum,\nPerMedCQA contains 68,138 question-answer pairs, refined through careful data\ncleaning from an initial set of 87,780 raw entries. We evaluate several\nstate-of-the-art multilingual and instruction-tuned LLMs, utilizing MedJudge, a\nnovel rubric-based evaluation framework driven by an LLM grader, validated\nagainst expert human annotators. Our results highlight key challenges in\nmultilingual medical QA and provide valuable insights for developing more\naccurate and context-aware medical assistance systems. The data is publicly\navailable on https://huggingface.co/datasets/NaghmehAI/PerMedCQA"}
{"id": "2505.19488", "pdf": "https://arxiv.org/pdf/2505.19488", "abs": "https://arxiv.org/abs/2505.19488", "authors": ["Shu Zhong", "Mingyu Xu", "Tenglong Ao", "Guang Shi"], "title": "Understanding Transformer from the Perspective of Associative Memory", "categories": ["cs.LG", "cs.AI"], "comment": "Consider this post less as a formal research paper and more as a\n  blog-style sharing of our current reflections, intended to spark discussion\n  as one might in a collaborative team meeting", "summary": "In this paper, we share our reflections and insights on understanding\nTransformer architectures through the lens of associative memory--a classic\npsychological concept inspired by human cognition. We start with the basics of\nassociative memory (think simple linear attention) and then dive into two\ndimensions:\n  Memory Capacity: How much can a Transformer really remember, and how well? We\nintroduce retrieval SNR to measure this and use a kernel perspective to\nmathematically reveal why Softmax Attention is so effective. We also show how\nFFNs can be seen as a type of associative memory, leading to insights on their\ndesign and potential improvements.\n  Memory Update: How do these memories learn and evolve? We present a unified\nframework for understanding how different Transformer variants (like DeltaNet\nand Softmax Attention) update their \"knowledge base\". This leads us to tackle\ntwo provocative questions: 1. Are Transformers fundamentally limited in what\nthey can express, and can we break these barriers? 2. If a Transformer had\ninfinite context, would it become infinitely intelligent?\n  We want to demystify Transformer architecture, offering a clearer\nunderstanding of existing designs. This exploration aims to provide fresh\ninsights and spark new avenues for Transformer innovation."}
{"id": "2505.19756", "pdf": "https://arxiv.org/pdf/2505.19756", "abs": "https://arxiv.org/abs/2505.19756", "authors": ["Ruihan Gong", "Yue Liu", "Wenjie Qu", "Mingzhe Du", "Yufei He", "Yingwei Ma", "Yulin Chen", "Xiang Liu", "Yi Wen", "Xinfeng Li", "Ruidong Wang", "Xinzhong Zhu", "Bryan Hooi", "Jiaheng Zhang"], "title": "Efficient Reasoning via Chain of Unconscious Thought", "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) achieve promising performance but compromise\ntoken efficiency due to verbose reasoning processes. Unconscious Thought Theory\n(UTT) posits that complex problems can be solved more efficiently through\ninternalized cognitive processes. Inspired by UTT, we propose a new reasoning\nparadigm, termed Chain of Unconscious Thought (CoUT), to improve the token\nefficiency of LRMs by guiding them to mimic human unconscious thought and\ninternalize reasoning processes. Concretely, we first prompt the model to\ninternalize the reasoning by thinking in the hidden layer. Then, we design a\nbag of token-efficient strategies to further help models reduce unnecessary\ntokens yet preserve the performance. Our work reveals that models may possess\nbeneficial unconscious thought, enabling improved efficiency without\nsacrificing performance. Extensive experiments demonstrate the effectiveness of\nCoUT. Remarkably, it surpasses CoT by reducing token usage by 47.62% while\nmaintaining comparable accuracy, as shown in Figure 1. The code of CoUT is\navailable at this link: https://github.com/Rohan-GRH/CoUT"}
{"id": "2505.18333", "pdf": "https://arxiv.org/pdf/2505.18333", "abs": "https://arxiv.org/abs/2505.18333", "authors": ["Yuqi Jia", "Zedian Shao", "Yupei Liu", "Jinyuan Jia", "Dawn Song", "Neil Zhenqiang Gong"], "title": "A Critical Evaluation of Defenses against Prompt Injection Attacks", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are vulnerable to prompt injection attacks, and\nseveral defenses have recently been proposed, often claiming to mitigate these\nattacks successfully. However, we argue that existing studies lack a principled\napproach to evaluating these defenses. In this paper, we argue the need to\nassess defenses across two critical dimensions: (1) effectiveness, measured\nagainst both existing and adaptive prompt injection attacks involving diverse\ntarget and injected prompts, and (2) general-purpose utility, ensuring that the\ndefense does not compromise the foundational capabilities of the LLM. Our\ncritical evaluation reveals that prior studies have not followed such a\ncomprehensive evaluation methodology. When assessed using this principled\napproach, we show that existing defenses are not as successful as previously\nreported. This work provides a foundation for evaluating future defenses and\nguiding their development. Our code and data are available at:\nhttps://github.com/PIEval123/PIEval."}
{"id": "2505.19491", "pdf": "https://arxiv.org/pdf/2505.19491", "abs": "https://arxiv.org/abs/2505.19491", "authors": ["Wenhao Yang", "Sifan Yang", "Lijun Zhang"], "title": "Discounted Online Convex Optimization: Uniform Regret Across a Continuous Interval", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Reflecting the greater significance of recent history over the distant past\nin non-stationary environments, $\\lambda$-discounted regret has been introduced\nin online convex optimization (OCO) to gracefully forget past data as new\ninformation arrives. When the discount factor $\\lambda$ is given, online\ngradient descent with an appropriate step size achieves an\n$O(1/\\sqrt{1-\\lambda})$ discounted regret. However, the value of $\\lambda$ is\noften not predetermined in real-world scenarios. This gives rise to a\nsignificant open question: is it possible to develop a discounted algorithm\nthat adapts to an unknown discount factor. In this paper, we affirmatively\nanswer this question by providing a novel analysis to demonstrate that smoothed\nOGD (SOGD) achieves a uniform $O(\\sqrt{\\log T/1-\\lambda})$ discounted regret,\nholding for all values of $\\lambda$ across a continuous interval\nsimultaneously. The basic idea is to maintain multiple OGD instances to handle\ndifferent discount factors, and aggregate their outputs sequentially by an\nonline prediction algorithm named as Discounted-Normal-Predictor (DNP)\n(Kapralov and Panigrahy,2010). Our analysis reveals that DNP can combine the\ndecisions of two experts, even when they operate on discounted regret with\ndifferent discount factors."}
{"id": "2505.19766", "pdf": "https://arxiv.org/pdf/2505.19766", "abs": "https://arxiv.org/abs/2505.19766", "authors": ["Masoomali Fatehkia", "Enes Altinisik", "Husrev Taha Sencar"], "title": "SGM: A Framework for Building Specification-Guided Moderation Filters", "categories": ["cs.CL"], "comment": null, "summary": "Aligning large language models (LLMs) with deployment-specific requirements\nis critical but inherently imperfect. Despite extensive training, models remain\nsusceptible to misalignment and adversarial inputs such as jailbreaks. Content\nmoderation filters are commonly used as external safeguards, though they\ntypically focus narrowly on safety. We introduce SGM (Specification-Guided\nModeration), a flexible framework for training moderation filters grounded in\nuser-defined specifications that go beyond standard safety concerns. SGM\nautomates training data generation without relying on human-written examples,\nenabling scalable support for diverse, application-specific alignment goals.\nSGM-trained filters perform on par with state-of-the-art safety filters built\non curated datasets, while supporting fine-grained and user-defined alignment\ncontrol."}
{"id": "2505.18341", "pdf": "https://arxiv.org/pdf/2505.18341", "abs": "https://arxiv.org/abs/2505.18341", "authors": ["Miao Li", "Wenhao Ding", "Haohong Lin", "Yiqi Lyu", "Yihang Yao", "Yuyou Zhang", "Ding Zhao"], "title": "CrashAgent: Crash Scenario Generation via Multi-modal Reasoning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Training and evaluating autonomous driving algorithms requires a diverse\nrange of scenarios. However, most available datasets predominantly consist of\nnormal driving behaviors demonstrated by human drivers, resulting in a limited\nnumber of safety-critical cases. This imbalance, often referred to as a\nlong-tail distribution, restricts the ability of driving algorithms to learn\nfrom crucial scenarios involving risk or failure, scenarios that are essential\nfor humans to develop driving skills efficiently. To generate such scenarios,\nwe utilize Multi-modal Large Language Models to convert crash reports of\naccidents into a structured scenario format, which can be directly executed\nwithin simulations. Specifically, we introduce CrashAgent, a multi-agent\nframework designed to interpret multi-modal real-world traffic crash reports\nfor the generation of both road layouts and the behaviors of the ego vehicle\nand surrounding traffic participants. We comprehensively evaluate the generated\ncrash scenarios from multiple perspectives, including the accuracy of layout\nreconstruction, collision rate, and diversity. The resulting high-quality and\nlarge-scale crash dataset will be publicly available to support the development\nof safe driving algorithms in handling safety-critical situations."}
{"id": "2505.19497", "pdf": "https://arxiv.org/pdf/2505.19497", "abs": "https://arxiv.org/abs/2505.19497", "authors": ["Yiqiao Liao", "Farinaz Koushanfar", "Parinaz Naghizadeh"], "title": "Learning for Dynamic Combinatorial Optimization without Training Data", "categories": ["cs.LG"], "comment": null, "summary": "We introduce DyCO-GNN, a novel unsupervised learning framework for Dynamic\nCombinatorial Optimization that requires no training data beyond the problem\ninstance itself. DyCO-GNN leverages structural similarities across\ntime-evolving graph snapshots to accelerate optimization while maintaining\nsolution quality. We evaluate DyCO-GNN on dynamic maximum cut, maximum\nindependent set, and the traveling salesman problem across diverse datasets of\nvarying sizes, demonstrating its superior performance under tight and moderate\ntime budgets. DyCO-GNN consistently outperforms the baseline methods, achieving\nhigh-quality solutions up to 3-60x faster, highlighting its practical\neffectiveness in rapidly evolving resource-constrained settings."}
{"id": "2505.19768", "pdf": "https://arxiv.org/pdf/2505.19768", "abs": "https://arxiv.org/abs/2505.19768", "authors": ["Xing Cui", "Yueying Zou", "Zekun Li", "Peipei Li", "Xinyuan Xu", "Xuannan Liu", "Huaibo Huang", "Ran He"], "title": "T^2Agent A Tool-augmented Multimodal Misinformation Detection Agent with Monte Carlo Tree Search", "categories": ["cs.CL"], "comment": null, "summary": "Real-world multimodal misinformation often arises from mixed forgery sources,\nrequiring dynamic reasoning and adaptive verification. However, existing\nmethods mainly rely on static pipelines and limited tool usage, limiting their\nability to handle such complexity and diversity. To address this challenge, we\npropose T2Agent, a novel misinformation detection agent that incorporates an\nextensible toolkit with Monte Carlo Tree Search (MCTS). The toolkit consists of\nmodular tools such as web search, forgery detection, and consistency analysis.\nEach tool is described using standardized templates, enabling seamless\nintegration and future expansion. To avoid inefficiency from using all tools\nsimultaneously, a Bayesian optimization-based selector is proposed to identify\na task-relevant subset. This subset then serves as the action space for MCTS to\ndynamically collect evidence and perform multi-source verification. To better\nalign MCTS with the multi-source nature of misinformation detection, T2Agent\nextends traditional MCTS with multi-source verification, which decomposes the\ntask into coordinated subtasks targeting different forgery sources. A dual\nreward mechanism containing a reasoning trajectory score and a confidence score\nis further proposed to encourage a balance between exploration across mixed\nforgery sources and exploitation for more reliable evidence. We conduct\nablation studies to confirm the effectiveness of the tree search mechanism and\ntool usage. Extensive experiments further show that T2Agent consistently\noutperforms existing baselines on challenging mixed-source multimodal\nmisinformation benchmarks, demonstrating its strong potential as a\ntraining-free approach for enhancing detection accuracy. The code will be\nreleased."}
{"id": "2505.18344", "pdf": "https://arxiv.org/pdf/2505.18344", "abs": "https://arxiv.org/abs/2505.18344", "authors": ["Mudit Gaur", "Prashant Trivedi", "Sasidhar Kunapuli", "Amrit Singh Bedi", "Vaneet Aggarwal"], "title": "Sample Complexity of Diffusion Model Training Without Empirical Risk Minimizer Access", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Diffusion models have demonstrated state-of-the-art performance across\nvision, language, and scientific domains. Despite their empirical success,\nprior theoretical analyses of the sample complexity suffer from poor scaling\nwith input data dimension or rely on unrealistic assumptions such as access to\nexact empirical risk minimizers. In this work, we provide a principled analysis\nof score estimation, establishing a sample complexity bound of\n$\\widetilde{\\mathcal{O}}(\\epsilon^{-6})$. Our approach leverages a structured\ndecomposition of the score estimation error into statistical, approximation,\nand optimization errors, enabling us to eliminate the exponential dependence on\nneural network parameters that arises in prior analyses. It is the first such\nresult which achieves sample complexity bounds without assuming access to the\nempirical risk minimizer of score function estimation loss."}
{"id": "2505.19504", "pdf": "https://arxiv.org/pdf/2505.19504", "abs": "https://arxiv.org/abs/2505.19504", "authors": ["Pingzhi Li", "Zhen Tan", "Huaizhi Qu", "Huan Liu", "Tianlong Chen"], "title": "DOGe: Defensive Output Generation for LLM Protection Against Knowledge Distillation", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Code is available at https://github.com/UNITES-Lab/DOGe", "summary": "Large Language Models (LLMs) represent substantial intellectual and economic\ninvestments, yet their effectiveness can inadvertently facilitate model\nimitation via knowledge distillation (KD).In practical scenarios, competitors\ncan distill proprietary LLM capabilities by simply observing publicly\naccessible outputs, akin to reverse-engineering a complex performance by\nobservation alone. Existing protective methods like watermarking only identify\nimitation post-hoc, while other defenses assume the student model mimics the\nteacher's internal logits, rendering them ineffective against distillation\npurely from observed output text. This paper confronts the challenge of\nactively protecting LLMs within the realistic constraints of API-based access.\nWe introduce an effective and efficient Defensive Output Generation (DOGe)\nstrategy that subtly modifies the output behavior of an LLM. Its outputs remain\naccurate and useful for legitimate users, yet are designed to be misleading for\ndistillation, significantly undermining imitation attempts. We achieve this by\nfine-tuning only the final linear layer of the teacher LLM with an adversarial\nloss. This targeted training approach anticipates and disrupts distillation\nattempts during inference time. Our experiments show that, while preserving or\neven improving the original performance of the teacher model, student models\ndistilled from the defensively generated teacher outputs demonstrate\ncatastrophically reduced performance, demonstrating our method's effectiveness\nas a practical safeguard against KD-based model imitation."}
{"id": "2505.19773", "pdf": "https://arxiv.org/pdf/2505.19773", "abs": "https://arxiv.org/abs/2505.19773", "authors": ["Sangyeop Kim", "Yohan Lee", "Yongwoo Song", "Kimin Lee"], "title": "What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs", "categories": ["cs.CL", "cs.CR"], "comment": "Accepted by ACL 2025", "summary": "We investigate long-context vulnerabilities in Large Language Models (LLMs)\nthrough Many-Shot Jailbreaking (MSJ). Our experiments utilize context length of\nup to 128K tokens. Through comprehensive analysis with various many-shot attack\nsettings with different instruction styles, shot density, topic, and format, we\nreveal that context length is the primary factor determining attack\neffectiveness. Critically, we find that successful attacks do not require\ncarefully crafted harmful content. Even repetitive shots or random dummy text\ncan circumvent model safety measures, suggesting fundamental limitations in\nlong-context processing capabilities of LLMs. The safety behavior of\nwell-aligned models becomes increasingly inconsistent with longer contexts.\nThese findings highlight significant safety gaps in context expansion\ncapabilities of LLMs, emphasizing the need for new safety mechanisms."}
{"id": "2505.18347", "pdf": "https://arxiv.org/pdf/2505.18347", "abs": "https://arxiv.org/abs/2505.18347", "authors": ["Mohamed A. Mohamed", "Kateryna Nekhomiazh", "Vedant Vyas", "Marcos M. Jose", "Andrew Patterson", "Marlos C. Machado"], "title": "The Cell Must Go On: Agar.io for Continual Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Continual reinforcement learning (RL) concerns agents that are expected to\nlearn continually, rather than converge to a policy that is then fixed for\nevaluation. Such an approach is well suited to environments the agent perceives\nas changing, which renders any static policy ineffective over time. The few\nsimulators explicitly designed for empirical research in continual RL are often\nlimited in scope or complexity, and it is now common for researchers to modify\nepisodic RL environments by artificially incorporating abrupt task changes\nduring interaction. In this paper, we introduce AgarCL, a research platform for\ncontinual RL that allows for a progression of increasingly sophisticated\nbehaviour. AgarCL is based on the game Agar.io, a non-episodic,\nhigh-dimensional problem featuring stochastic, ever-evolving dynamics,\ncontinuous actions, and partial observability. Additionally, we provide\nbenchmark results reporting the performance of DQN, PPO, and SAC in both the\nprimary, challenging continual RL problem, and across a suite of smaller tasks\nwithin AgarCL, each of which isolates aspects of the full environment and allow\nus to characterize the challenges posed by different aspects of the game."}
{"id": "2505.19509", "pdf": "https://arxiv.org/pdf/2505.19509", "abs": "https://arxiv.org/abs/2505.19509", "authors": ["Yifan Jia", "Kailin Jiang", "Yuyang Liang", "Qihan Ren", "Yi Xin", "Rui Yang", "Fenze Feng", "Mingcai Chen", "Hengyang Lu", "Haozhe Wang", "Xiaoye Qu", "Dongrui Liu", "Lizhen Cui", "Yuntao Du"], "title": "Benchmarking Multimodal Knowledge Conflict for Large Multimodal Models", "categories": ["cs.LG", "cs.AI"], "comment": "The source code is available at https://github.com/MLLMKCBENCH/MLLMKC", "summary": "Large Multimodal Models(LMMs) face notable challenges when encountering\nmultimodal knowledge conflicts, particularly under retrieval-augmented\ngeneration(RAG) frameworks where the contextual information from external\nsources may contradict the model's internal parametric knowledge, leading to\nunreliable outputs. However, existing benchmarks fail to reflect such realistic\nconflict scenarios. Most focus solely on intra-memory conflicts, while\ncontext-memory and inter-context conflicts remain largely investigated.\nFurthermore, commonly used factual knowledge-based evaluations are often\noverlooked, and existing datasets lack a thorough investigation into conflict\ndetection capabilities. To bridge this gap, we propose MMKC-Bench, a benchmark\ndesigned to evaluate factual knowledge conflicts in both context-memory and\ninter-context scenarios. MMKC-Bench encompasses three types of multimodal\nknowledge conflicts and includes 1,573 knowledge instances and 3,381 images\nacross 23 broad types, collected through automated pipelines with human\nverification. We evaluate three representative series of LMMs on both model\nbehavior analysis and conflict detection tasks. Our findings show that while\ncurrent LMMs are capable of recognizing knowledge conflicts, they tend to favor\ninternal parametric knowledge over external evidence. We hope MMKC-Bench will\nfoster further research in multimodal knowledge conflict and enhance the\ndevelopment of multimodal RAG systems. The source code is available at\nhttps://github.com/MLLMKCBENCH/MLLMKC."}
{"id": "2505.19776", "pdf": "https://arxiv.org/pdf/2505.19776", "abs": "https://arxiv.org/abs/2505.19776", "authors": ["Akram Elbouanani", "Evan Dufraisse", "Adrian Popescu"], "title": "Analyzing Political Bias in LLMs via Target-Oriented Sentiment Classification", "categories": ["cs.CL", "cs.AI"], "comment": "To be published in the Proceedings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL 2025)", "summary": "Political biases encoded by LLMs might have detrimental effects on downstream\napplications. Existing bias analysis methods rely on small-size intermediate\ntasks (questionnaire answering or political content generation) and rely on the\nLLMs themselves for analysis, thus propagating bias. We propose a new approach\nleveraging the observation that LLM sentiment predictions vary with the target\nentity in the same sentence. We define an entropy-based inconsistency metric to\nencode this prediction variability. We insert 1319 demographically and\npolitically diverse politician names in 450 political sentences and predict\ntarget-oriented sentiment using seven models in six widely spoken languages. We\nobserve inconsistencies in all tested combinations and aggregate them in a\nstatistically robust analysis at different granularity levels. We observe\npositive and negative bias toward left and far-right politicians and positive\ncorrelations between politicians with similar alignment. Bias intensity is\nhigher for Western languages than for others. Larger models exhibit stronger\nand more consistent biases and reduce discrepancies between similar languages.\nWe partially mitigate LLM unreliability in target-oriented sentiment\nclassification (TSC) by replacing politician names with fictional but plausible\ncounterparts."}
{"id": "2505.18350", "pdf": "https://arxiv.org/pdf/2505.18350", "abs": "https://arxiv.org/abs/2505.18350", "authors": ["Waleed Reda", "Abhinav Jangda", "Krishna Chintalapudi"], "title": "Task Specific Pruning with LLM-Sieve: How Many Parameters Does Your Task Really Need?", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7"], "comment": null, "summary": "As Large Language Models (LLMs) are increasingly being adopted for narrow\ntasks - such as medical question answering or sentiment analysis - and deployed\nin resource-constrained settings, a key question arises: how many parameters\ndoes a task actually need? In this work, we present LLM-Sieve, the first\ncomprehensive framework for task-specific pruning of LLMs that achieves 20-75%\nparameter reduction with only 1-5% accuracy degradation across diverse domains.\nUnlike prior methods that apply uniform pruning or rely on low-rank\napproximations of weight matrices or inputs in isolation, LLM-Sieve (i) learns\ntask-aware joint projections to better approximate output behavior, and (ii)\nemploys a Genetic Algorithm to discover differentiated pruning levels for each\nmatrix. LLM-Sieve is fully compatible with LoRA fine-tuning and quantization,\nand uniquely demonstrates strong generalization across datasets within the same\ntask domain. Together, these results establish a practical and robust mechanism\nto generate smaller performant task-specific models."}
{"id": "2505.19525", "pdf": "https://arxiv.org/pdf/2505.19525", "abs": "https://arxiv.org/abs/2505.19525", "authors": ["Liangwei Nathan Zheng", "Wei Emma Zhang", "Mingyu Guo", "Miao Xu", "Olaf Maennel", "Weitong Chen"], "title": "Rethinking Gating Mechanism in Sparse MoE: Handling Arbitrary Modality Inputs with Confidence-Guided Gate", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Effectively managing missing modalities is a fundamental challenge in\nreal-world multimodal learning scenarios, where data incompleteness often\nresults from systematic collection errors or sensor failures. Sparse\nMixture-of-Experts (SMoE) architectures have the potential to naturally handle\nmultimodal data, with individual experts specializing in different modalities.\nHowever, existing SMoE approach often lacks proper ability to handle missing\nmodality, leading to performance degradation and poor generalization in\nreal-world applications. We propose Conf-SMoE to introduce a two-stage\nimputation module to handle the missing modality problem for the SMoE\narchitecture and reveal the insight of expert collapse from theoretical\nanalysis with strong empirical evidence. Inspired by our theoretical analysis,\nConf-SMoE propose a novel expert gating mechanism by detaching the softmax\nrouting score to task confidence score w.r.t ground truth. This naturally\nrelieves expert collapse without introducing additional load balance loss\nfunction. We show that the insights of expert collapse aligns with other gating\nmechanism such as Gaussian and Laplacian gate. We also evaluate the proposed\nmethod on four different real world dataset with three different experiment\nsettings to conduct comprehensive the analysis of Conf-SMoE on modality fusion\nand resistance to missing modality."}
{"id": "2505.19797", "pdf": "https://arxiv.org/pdf/2505.19797", "abs": "https://arxiv.org/abs/2505.19797", "authors": ["Yiqun Zhang", "Hao Li", "Chenxu Wang", "Linyao Chen", "Qiaosheng Zhang", "Peng Ye", "Shi Feng", "Daling Wang", "Zhen Wang", "Xinrun Wang", "Jia Xu", "Lei Bai", "Wanli Ouyang", "Shuyue Hu"], "title": "The Avengers: A Simple Recipe for Uniting Smaller Language Models to Challenge Proprietary Giants", "categories": ["cs.CL"], "comment": "9 pages, 3 figures, 6 tables, supplementary material (appendix)\n  included separately", "summary": "As proprietary giants increasingly dominate the race for ever-larger language\nmodels, a pressing question arises for the open-source community: can smaller\nmodels remain competitive across a broad range of tasks? In this paper, we\npresent the Avengers--a simple recipe that effectively leverages the collective\nintelligence of open-source, smaller language models. Our framework is built\nupon four lightweight operations: (i) embedding: encode queries using a text\nembedding model; (ii) clustering: group queries based on their semantic\nsimilarity; (iii) scoring: scores each model's performance within each cluster;\nand (iv) voting: improve outputs via repeated sampling and voting. At inference\ntime, each query is embedded and assigned to its nearest cluster. The\ntop-performing model(s) within that cluster are selected to generate the\nresponse using the Self-Consistency or its multi-model variant. Remarkably,\nwith 10 open-source models (~7B parameters each), the Avengers collectively\noutperforms GPT-4.1 on 10 out of 15 datasets (spanning mathematics, code,\nlogic, knowledge, and affective tasks). In particular, it surpasses GPT-4.1 on\nmathematics tasks by 18.21% and on code tasks by 7.46%. Furthermore, the\nAvengers delivers superior out-of-distribution generalization, and remains\nrobust across various embedding models, clustering algorithms, ensemble\nstrategies, and values of its sole parameter--the number of clusters. We have\nopen-sourced the code on GitHub: https://github.com/ZhangYiqun018/Avengers"}
{"id": "2505.18356", "pdf": "https://arxiv.org/pdf/2505.18356", "abs": "https://arxiv.org/abs/2505.18356", "authors": ["Lucas Bandarkar", "Nanyun Peng"], "title": "The Unreasonable Effectiveness of Model Merging for Cross-Lingual Transfer in LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": null, "summary": "Large language models (LLMs) still struggle across tasks outside of\nhigh-resource languages. In this work, we investigate cross-lingual transfer to\nlower-resource languages where task-specific post-training data is scarce.\nBuilding on prior work, we first validate that the subsets of model parameters\nthat matter most for mathematical reasoning and multilingual capabilities are\ndistinctly non-overlapping. To exploit this implicit separability between task\nand target language parameterization, we develop and analyze numerous modular\nframeworks to improve the composition of the two during fine-tuning. These\nmethods generally employ freezing parameters or post hoc model merging to\nassign math and language improvement to different key parts of the LLM. In the\nabsence of in-language math data, we demonstrate that the modular approaches\nsuccessfully improve upon baselines across three languages, four models, and\ntwo fine-tuning paradigms (full and LoRA). Furthermore, we identify the most\nconsistently successful modular method to be fine-tuning separate language and\nmath experts and model merging via Layer-Swapping, somewhat surprisingly. We\noffer possible explanations for this result via recent works on the linearity\nof task vectors. We further explain this by empirically showing that reverting\nless useful fine-tuning updates after training often outperforms freezing them\nfrom the start."}
{"id": "2505.19527", "pdf": "https://arxiv.org/pdf/2505.19527", "abs": "https://arxiv.org/abs/2505.19527", "authors": ["Mohammed D. Belgoumri", "Mohamed Reda Bouadjenek", "Hakim Hacid", "Imran Razzak", "Sunil Aryal"], "title": "Navigating loss manifolds via rigid body dynamics: A promising avenue for robustness and generalisation", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": null, "summary": "Training large neural networks through gradient-based optimization requires\nnavigating high-dimensional loss landscapes, which often exhibit pathological\ngeometry, leading to undesirable training dynamics. In particular, poor\ngeneralization frequently results from convergence to sharp minima that are\nhighly sensitive to input perturbations, causing the model to overfit the\ntraining data while failing to generalize to unseen examples. Furthermore,\nthese optimization procedures typically display strong dependence on the fine\nstructure of the loss landscape, leading to unstable training dynamics, due to\nthe fractal-like nature of the loss surface. In this work, we propose an\nalternative optimizer that simultaneously reduces this dependence, and avoids\nsharp minima, thereby improving generalization. This is achieved by simulating\nthe motion of the center of a ball rolling on the loss landscape. The degree to\nwhich our optimizer departs from the standard gradient descent is controlled by\na hyperparameter, representing the radius of the ball. Changing this\nhyperparameter allows for probing the loss landscape at different scales,\nmaking it a valuable tool for understanding its geometry."}
{"id": "2505.19800", "pdf": "https://arxiv.org/pdf/2505.19800", "abs": "https://arxiv.org/abs/2505.19800", "authors": ["Zaid Alyafeai", "Maged S. Al-Shaibani", "Bernard Ghanem"], "title": "MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Metadata extraction is essential for cataloging and preserving datasets,\nenabling effective research discovery and reproducibility, especially given the\ncurrent exponential growth in scientific research. While Masader (Alyafeai et\nal.,2021) laid the groundwork for extracting a wide range of metadata\nattributes from Arabic NLP datasets' scholarly articles, it relies heavily on\nmanual annotation. In this paper, we present MOLE, a framework that leverages\nLarge Language Models (LLMs) to automatically extract metadata attributes from\nscientific papers covering datasets of languages other than Arabic. Our\nschema-driven methodology processes entire documents across multiple input\nformats and incorporates robust validation mechanisms for consistent output.\nAdditionally, we introduce a new benchmark to evaluate the research progress on\nthis task. Through systematic analysis of context length, few-shot learning,\nand web browsing integration, we demonstrate that modern LLMs show promising\nresults in automating this task, highlighting the need for further future work\nimprovements to ensure consistent and reliable performance. We release the\ncode: https://github.com/IVUL-KAUST/MOLE and dataset:\nhttps://huggingface.co/datasets/IVUL-KAUST/MOLE for the research community."}
{"id": "2505.18361", "pdf": "https://arxiv.org/pdf/2505.18361", "abs": "https://arxiv.org/abs/2505.18361", "authors": ["Trinity Chung", "Yuchen Shen", "Nathan C. L. Kong", "Aran Nayebi"], "title": "Task-Optimized Convolutional Recurrent Networks Align with Tactile Processing in the Rodent Brain", "categories": ["q-bio.NC", "cs.AI", "cs.LG", "cs.RO"], "comment": "9 pages, 8 figures, 5 tables", "summary": "Tactile sensing remains far less understood in neuroscience and less\neffective in artificial systems compared to more mature modalities such as\nvision and language. We bridge these gaps by introducing a novel\nEncoder-Attender-Decoder (EAD) framework to systematically explore the space of\ntask-optimized temporal neural networks trained on realistic tactile input\nsequences from a customized rodent whisker-array simulator. We identify\nconvolutional recurrent neural networks (ConvRNNs) as superior encoders to\npurely feedforward and state-space architectures for tactile categorization.\nCrucially, these ConvRNN-encoder-based EAD models achieve neural\nrepresentations closely matching rodent somatosensory cortex, saturating the\nexplainable neural variability and revealing a clear linear relationship\nbetween supervised categorization performance and neural alignment.\nFurthermore, contrastive self-supervised ConvRNN-encoder-based EADs, trained\nwith tactile-specific augmentations, match supervised neural fits, serving as\nan ethologically-relevant, label-free proxy.\n  For neuroscience, our findings highlight nonlinear recurrent processing as\nimportant for general-purpose tactile representations in somatosensory cortex,\nproviding the first quantitative characterization of the underlying inductive\nbiases in this system. For embodied AI, our results emphasize the importance of\nrecurrent EAD architectures to handle realistic tactile inputs, along with\ntailored self-supervised learning methods for achieving robust tactile\nperception with the same type of sensors animals use to sense in unstructured\nenvironments."}
{"id": "2505.19531", "pdf": "https://arxiv.org/pdf/2505.19531", "abs": "https://arxiv.org/abs/2505.19531", "authors": ["Jerry Yao-Chieh Hu", "Xiwen Zhang", "Maojiang Su", "Zhao Song", "Han Liu"], "title": "Minimalist Softmax Attention Provably Learns Constrained Boolean Functions", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "We study the computational limits of learning $k$-bit Boolean functions\n(specifically, $\\mathrm{AND}$, $\\mathrm{OR}$, and their noisy variants), using\na minimalist single-head softmax-attention mechanism, where $k=\\Theta(d)$\nrelevant bits are selected from $d$ inputs. We show that these simple\n$\\mathrm{AND}$ and $\\mathrm{OR}$ functions are unsolvable with a single-head\nsoftmax-attention mechanism alone. However, with teacher forcing, the same\nminimalist attention is capable of solving them. These findings offer two key\ninsights: Architecturally, solving these Boolean tasks requires only minimalist\nattention, without deep Transformer blocks or FFNs. Methodologically, one\ngradient descent update with supervision suffices and replaces the multi-step\nChain-of-Thought (CoT) reasoning scheme of [Kim and Suzuki, ICLR 2025] for\nsolving Boolean problems. Together, the bounds expose a fundamental gap between\nwhat this minimal architecture achieves under ideal supervision and what is\nprovably impossible under standard training."}
{"id": "2505.19804", "pdf": "https://arxiv.org/pdf/2505.19804", "abs": "https://arxiv.org/abs/2505.19804", "authors": ["Siyuan Li", "Jian Chen", "Rui Yao", "Xuming Hu", "Peilin Zhou", "Weihua Qiu", "Simin Zhang", "Chucheng Dong", "Zhiyao Li", "Qipeng Xie", "Zixuan Yuan"], "title": "Compliance-to-Code: Enhancing Financial Compliance Checking via Code Generation", "categories": ["cs.CL"], "comment": null, "summary": "Nowadays, regulatory compliance has become a cornerstone of corporate\ngovernance, ensuring adherence to systematic legal frameworks. At its core,\nfinancial regulations often comprise highly intricate provisions, layered\nlogical structures, and numerous exceptions, which inevitably result in\nlabor-intensive or comprehension challenges. To mitigate this, recent\nRegulatory Technology (RegTech) and Large Language Models (LLMs) have gained\nsignificant attention in automating the conversion of regulatory text into\nexecutable compliance logic. However, their performance remains suboptimal\nparticularly when applied to Chinese-language financial regulations, due to\nthree key limitations: (1) incomplete domain-specific knowledge representation,\n(2) insufficient hierarchical reasoning capabilities, and (3) failure to\nmaintain temporal and logical coherence. One promising solution is to develop a\ndomain specific and code-oriented datasets for model training. Existing\ndatasets such as LexGLUE, LegalBench, and CODE-ACCORD are often\nEnglish-focused, domain-mismatched, or lack fine-grained granularity for\ncompliance code generation. To fill these gaps, we present Compliance-to-Code,\nthe first large-scale Chinese dataset dedicated to financial regulatory\ncompliance. Covering 1,159 annotated clauses from 361 regulations across ten\ncategories, each clause is modularly structured with four logical\nelements-subject, condition, constraint, and contextual information-along with\nregulation relations. We provide deterministic Python code mappings, detailed\ncode reasoning, and code explanations to facilitate automated auditing. To\ndemonstrate utility, we present FinCheck: a pipeline for regulation\nstructuring, code generation, and report generation."}
{"id": "2505.18362", "pdf": "https://arxiv.org/pdf/2505.18362", "abs": "https://arxiv.org/abs/2505.18362", "authors": ["Nathan Gaby", "Xiaojing Ye"], "title": "Hamiltonian Theory and Computation of Optimal Probability Density Control in High Dimensions", "categories": ["math.OC", "cs.AI", "cs.LG", "cs.NA", "math.NA"], "comment": "28 pages, submitted", "summary": "We develop a general theoretical framework for optimal probability density\ncontrol and propose a numerical algorithm that is scalable to solve the control\nproblem in high dimensions. Specifically, we establish the Pontryagin Maximum\nPrinciple (PMP) for optimal density control and construct the\nHamilton-Jacobi-Bellman (HJB) equation of the value functional through rigorous\nderivations without any concept from Wasserstein theory. To solve the density\ncontrol problem numerically, we propose to use reduced-order models, such as\ndeep neural networks (DNNs), to parameterize the control vector-field and the\nadjoint function, which allows us to tackle problems defined on\nhigh-dimensional state spaces. We also prove several convergence properties of\nthe proposed algorithm. Numerical results demonstrate promising performances of\nour algorithm on a variety of density control problems with obstacles and\nnonlinear interaction challenges in high dimensions."}
{"id": "2505.19532", "pdf": "https://arxiv.org/pdf/2505.19532", "abs": "https://arxiv.org/abs/2505.19532", "authors": ["Shijie Liu", "Andrew C. Cullen", "Paul Montague", "Sarah Erfani", "Benjamin I. P. Rubinstein"], "title": "Fox in the Henhouse: Supply-Chain Backdoor Attacks Against Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "The current state-of-the-art backdoor attacks against Reinforcement Learning\n(RL) rely upon unrealistically permissive access models, that assume the\nattacker can read (or even write) the victim's policy parameters, observations,\nor rewards. In this work, we question whether such a strong assumption is\nrequired to launch backdoor attacks against RL. To answer this question, we\npropose the \\underline{S}upply-\\underline{C}h\\underline{a}in\n\\underline{B}ackdoor (SCAB) attack, which targets a common RL workflow:\ntraining agents using external agents that are provided separately or embedded\nwithin the environment. In contrast to prior works, our attack only relies on\nlegitimate interactions of the RL agent with the supplied agents. Despite this\nlimited access model, by poisoning a mere $3\\%$ of training experiences, our\nattack can successfully activate over $90\\%$ of triggered actions, reducing the\naverage episodic return by $80\\%$ for the victim. Our novel attack demonstrates\nthat RL attacks are likely to become a reality under untrusted RL training\nsupply-chains."}
{"id": "2505.19806", "pdf": "https://arxiv.org/pdf/2505.19806", "abs": "https://arxiv.org/abs/2505.19806", "authors": ["Sirui Chen", "Shuqin Ma", "Shu Yu", "Hanwang Zhang", "Shengjie Zhao", "Chaochao Lu"], "title": "Exploring Consciousness in LLMs: A Systematic Survey of Theories, Implementations, and Frontier Risks", "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Consciousness stands as one of the most profound and distinguishing features\nof the human mind, fundamentally shaping our understanding of existence and\nagency. As large language models (LLMs) develop at an unprecedented pace,\nquestions concerning intelligence and consciousness have become increasingly\nsignificant. However, discourse on LLM consciousness remains largely unexplored\nterritory. In this paper, we first clarify frequently conflated terminologies\n(e.g., LLM consciousness and LLM awareness). Then, we systematically organize\nand synthesize existing research on LLM consciousness from both theoretical and\nempirical perspectives. Furthermore, we highlight potential frontier risks that\nconscious LLMs might introduce. Finally, we discuss current challenges and\noutline future directions in this emerging field. The references discussed in\nthis paper are organized at\nhttps://github.com/OpenCausaLab/Awesome-LLM-Consciousness."}
{"id": "2505.18363", "pdf": "https://arxiv.org/pdf/2505.18363", "abs": "https://arxiv.org/abs/2505.18363", "authors": ["AmirHossein Safdarian", "Milad Mohammadi", "Ehsan Jahanbakhsh", "Mona Shahamat Naderi", "Heshaam Faili"], "title": "SchemaGraphSQL: Efficient Schema Linking with Pathfinding Graph Algorithms for Text-to-SQL on Large-Scale Databases", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "Text-to-SQL systems translate natural language questions into executable SQL\nqueries, and recent progress with large language models (LLMs) has driven\nsubstantial improvements in this task. Schema linking remains a critical\ncomponent in Text-to-SQL systems, reducing prompt size for models with narrow\ncontext windows and sharpening model focus even when the entire schema fits. We\npresent a zero-shot, training-free schema linking approach that first\nconstructs a schema graph based on foreign key relations, then uses a single\nprompt to Gemini 2.5 Flash to extract source and destination tables from the\nuser query, followed by applying classical path-finding algorithms and\npost-processing to identify the optimal sequence of tables and columns that\nshould be joined, enabling the LLM to generate more accurate SQL queries.\nDespite being simple, cost-effective, and highly scalable, our method achieves\nstate-of-the-art results on the BIRD benchmark, outperforming previous\nspecialized, fine-tuned, and complex multi-step LLM-based approaches. We\nconduct detailed ablation studies to examine the precision-recall trade-off in\nour framework. Additionally, we evaluate the execution accuracy of our schema\nfiltering method compared to other approaches across various model sizes."}
{"id": "2505.19533", "pdf": "https://arxiv.org/pdf/2505.19533", "abs": "https://arxiv.org/abs/2505.19533", "authors": ["Yachuan Liu", "Xiaochun Wei", "Lin Shi", "Xinnuo Li", "Bohan Zhang", "Paramveer Dhillon", "Qiaozhu Mei"], "title": "ExAnte: A Benchmark for Ex-Ante Inference in Large Language Models", "categories": ["cs.LG"], "comment": null, "summary": "Large language models (LLMs) face significant challenges in ex-ante\nreasoning, where analysis, inference, or predictions must be made without\naccess to information from future events. Even with explicit prompts enforcing\ntemporal cutoffs, LLMs often generate outputs influenced by internalized\nknowledge of events beyond the specified cutoff. This paper introduces a novel\ntask and benchmark designed to evaluate the ability of LLMs to reason while\nadhering to such temporal constraints. The benchmark includes a variety of\ntasks: stock prediction, Wikipedia event prediction, scientific publication\nprediction, and Question Answering (QA), designed to assess factual knowledge\nunder temporal cutoff constraints. We use leakage rate to quantify models'\nreliance on future information beyond cutoff timestamps. Experimental results\nreveal that LLMs struggle to consistently adhere to temporal cutoffs across\ncommon prompting strategies and tasks, demonstrating persistent challenges in\nex-ante reasoning. This benchmark provides a potential evaluation framework to\nadvance the development of LLMs' temporal reasoning ability for time-sensitive\napplications."}
{"id": "2505.19815", "pdf": "https://arxiv.org/pdf/2505.19815", "abs": "https://arxiv.org/abs/2505.19815", "authors": ["Junnan Liu", "Hongwei Liu", "Linchen Xiao", "Shudong Liu", "Taolin Zhang", "Zihan Ma", "Songyang Zhang", "Kai Chen"], "title": "Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We propose a novel framework for comprehending the reasoning capabilities of\nlarge language models (LLMs) through the perspective of meta-learning. By\nconceptualizing reasoning trajectories as pseudo-gradient descent updates to\nthe LLM's parameters, we identify parallels between LLM reasoning and various\nmeta-learning paradigms. We formalize the training process for reasoning tasks\nas a meta-learning setup, with each question treated as an individual task, and\nreasoning trajectories serving as the inner loop optimization for adapting\nmodel parameters. Once trained on a diverse set of questions, the LLM develops\nfundamental reasoning capabilities that can generalize to previously unseen\nquestions. Extensive empirical evaluations substantiate the strong connection\nbetween LLM reasoning and meta-learning, exploring several issues of\nsignificant interest from a meta-learning standpoint. Our work not only\nenhances the understanding of LLM reasoning but also provides practical\ninsights for improving these models through established meta-learning\ntechniques."}
{"id": "2505.18366", "pdf": "https://arxiv.org/pdf/2505.18366", "abs": "https://arxiv.org/abs/2505.18366", "authors": ["Hansa Meghwani", "Amit Agarwal", "Priyaranjan Pattnayak", "Hitesh Laxmichand Patel", "Srikant Panda"], "title": "Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "H.3.3; I.2.6; I.2.7"], "comment": "Accepted to ACL 2025", "summary": "Enterprise search systems often struggle to retrieve accurate,\ndomain-specific information due to semantic mismatches and overlapping\nterminologies. These issues can degrade the performance of downstream\napplications such as knowledge management, customer support, and\nretrieval-augmented generation agents. To address this challenge, we propose a\nscalable hard-negative mining framework tailored specifically for\ndomain-specific enterprise data. Our approach dynamically selects semantically\nchallenging but contextually irrelevant documents to enhance deployed\nre-ranking models.\n  Our method integrates diverse embedding models, performs dimensionality\nreduction, and uniquely selects hard negatives, ensuring computational\nefficiency and semantic precision. Evaluation on our proprietary enterprise\ncorpus (cloud services domain) demonstrates substantial improvements of 15\\% in\nMRR@3 and 19\\% in MRR@10 compared to state-of-the-art baselines and other\nnegative sampling techniques. Further validation on public domain-specific\ndatasets (FiQA, Climate Fever, TechQA) confirms our method's generalizability\nand readiness for real-world applications."}
{"id": "2505.19543", "pdf": "https://arxiv.org/pdf/2505.19543", "abs": "https://arxiv.org/abs/2505.19543", "authors": ["Yiyun Zhou", "Zheqi Lv", "Shengyu Zhang", "Jingyuan Chen"], "title": "Cuff-KT: Tackling Learners' Real-time Learning Pattern Adjustment via Tuning-Free Knowledge State Guided Model Updating", "categories": ["cs.LG", "cs.IR"], "comment": "Accepted by KDD 2025, Research Track", "summary": "Knowledge Tracing (KT) is a core component of Intelligent Tutoring Systems,\nmodeling learners' knowledge state to predict future performance and provide\npersonalized learning support. Traditional KT models assume that learners'\nlearning abilities remain relatively stable over short periods or change in\npredictable ways based on prior performance. However, in reality, learners'\nabilities change irregularly due to factors like cognitive fatigue, motivation,\nand external stress -- a task introduced, which we refer to as Real-time\nLearning Pattern Adjustment (RLPA). Existing KT models, when faced with RLPA,\nlack sufficient adaptability, because they fail to timely account for the\ndynamic nature of different learners' evolving learning patterns. Current\nstrategies for enhancing adaptability rely on retraining, which leads to\nsignificant overfitting and high time overhead issues. To address this, we\npropose Cuff-KT, comprising a controller and a generator. The controller\nassigns value scores to learners, while the generator generates personalized\nparameters for selected learners. Cuff-KT controllably adapts to data changes\nfast and flexibly without fine-tuning. Experiments on five datasets from\ndifferent subjects demonstrate that Cuff-KT significantly improves the\nperformance of five KT models with different structures under intra- and\ninter-learner shifts, with an average relative increase in AUC of 10% and 4%,\nrespectively, at a negligible time cost, effectively tackling RLPA task. Our\ncode and datasets are fully available at https://github.com/zyy-2001/Cuff-KT."}
{"id": "2505.19838", "pdf": "https://arxiv.org/pdf/2505.19838", "abs": "https://arxiv.org/abs/2505.19838", "authors": ["Pascal Wullschleger", "Majid Zarharan", "Donnacha Daly", "Marc Pouly", "Jennifer Foster"], "title": "FoodTaxo: Generating Food Taxonomies with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "To be published in ACL 2025 Industry Track. Paper website:\n  https://foodtaxo.github.io/", "summary": "We investigate the utility of Large Language Models for automated taxonomy\ngeneration and completion specifically applied to taxonomies from the food\ntechnology industry. We explore the extent to which taxonomies can be completed\nfrom a seed taxonomy or generated without a seed from a set of known concepts,\nin an iterative fashion using recent prompting techniques. Experiments on five\ntaxonomies using an open-source LLM (Llama-3), while promising, point to the\ndifficulty of correctly placing inner nodes."}
{"id": "2505.18369", "pdf": "https://arxiv.org/pdf/2505.18369", "abs": "https://arxiv.org/abs/2505.18369", "authors": ["Csaba Both", "Benjamin Hoover", "Hendrik Strobelt", "Dmitry Krotov", "Daniel Karl I. Weidele", "Mauro Martino", "Nima Dehmamy"], "title": "Small Models, Smarter Learning: The Power of Joint Task Training", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The ability of a model to learn a task depends strongly on both the task\ndifficulty and the model size. We aim to understand how task difficulty relates\nto the minimum number of parameters required for learning specific tasks in\nsmall transformer models. Our study focuses on the ListOps dataset, which\nconsists of nested mathematical operations. We gradually increase task\ndifficulty by introducing new operations or combinations of operations into the\ntraining data. We observe that sum modulo n is the hardest to learn. Curiously,\nwhen combined with other operations such as maximum and median, the sum\noperation becomes easier to learn and requires fewer parameters. We show that\njoint training not only improves performance but also leads to qualitatively\ndifferent model behavior. We show evidence that models trained only on SUM\nmight be memorizing and fail to capture the number structure in the embeddings.\nIn contrast, models trained on a mixture of SUM and other operations exhibit\nnumber-like representations in the embedding space, and a strong ability to\ndistinguish parity. Furthermore, the SUM-only model relies more heavily on its\nfeedforward layers, while the jointly trained model activates the attention\nmechanism more. Finally, we show that learning pure SUM can be induced in\nmodels below the learning threshold of pure SUM, by pretraining them on\nMAX+MED. Our findings indicate that emergent abilities in language models\ndepend not only on model size, but also the training curriculum."}
{"id": "2505.19547", "pdf": "https://arxiv.org/pdf/2505.19547", "abs": "https://arxiv.org/abs/2505.19547", "authors": ["Haoyu Zhang", "Wentao Zhang", "Hao Miao", "Xinke Jiang", "Yuchen Fang", "Yifan Zhang"], "title": "STRAP: Spatio-Temporal Pattern Retrieval for Out-of-Distribution Generalization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Spatio-Temporal Graph Neural Networks (STGNNs) have emerged as a powerful\ntool for modeling dynamic graph-structured data across diverse domains.\nHowever, they often fail to generalize in Spatio-Temporal Out-of-Distribution\n(STOOD) scenarios, where both temporal dynamics and spatial structures evolve\nbeyond the training distribution. To address this problem, we propose an\ninnovative Spatio-Temporal Retrieval-Augmented Pattern Learning\nframework,STRAP, which enhances model generalization by integrating\nretrieval-augmented learning into the STGNN continue learning pipeline. The\ncore of STRAP is a compact and expressive pattern library that stores\nrepresentative spatio-temporal patterns enriched with historical, structural,\nand semantic information, which is obtained and optimized during the training\nphase. During inference, STRAP retrieves relevant patterns from this library\nbased on similarity to the current input and injects them into the model via a\nplug-and-play prompting mechanism. This not only strengthens spatio-temporal\nrepresentations but also mitigates catastrophic forgetting. Moreover, STRAP\nintroduces a knowledge-balancing objective to harmonize new information with\nretrieved knowledge. Extensive experiments across multiple real-world streaming\ngraph datasets show that STRAP consistently outperforms state-of-the-art STGNN\nbaselines on STOOD tasks, demonstrating its robustness, adaptability, and\nstrong generalization capability without task-specific fine-tuning."}
{"id": "2505.19848", "pdf": "https://arxiv.org/pdf/2505.19848", "abs": "https://arxiv.org/abs/2505.19848", "authors": ["Odunayo Ogundepo", "Akintunde Oladipo", "Kelechi Ogueji", "Esther Adenuga", "David Ifeoluwa Adelani", "Jimmy Lin"], "title": "Improving Multilingual Math Reasoning for African Languages", "categories": ["cs.CL"], "comment": null, "summary": "Researchers working on low-resource languages face persistent challenges due\nto limited data availability and restricted access to computational resources.\nAlthough most large language models (LLMs) are predominantly trained in\nhigh-resource languages, adapting them to low-resource contexts, particularly\nAfrican languages, requires specialized techniques. Several strategies have\nemerged for adapting models to low-resource languages in todays LLM landscape,\ndefined by multi-stage pre-training and post-training paradigms. However, the\nmost effective approaches remain uncertain. This work systematically\ninvestigates which adaptation strategies yield the best performance when\nextending existing LLMs to African languages. We conduct extensive experiments\nand ablation studies to evaluate different combinations of data types\n(translated versus synthetically generated), training stages (pre-training\nversus post-training), and other model adaptation configurations. Our\nexperiments focuses on mathematical reasoning tasks, using the Llama 3.1 model\nfamily as our base model."}
{"id": "2505.18371", "pdf": "https://arxiv.org/pdf/2505.18371", "abs": "https://arxiv.org/abs/2505.18371", "authors": ["Riley Simmons-Edler", "Jean Dong", "Paul Lushenko", "Kanaka Rajan", "Ryan P. Badman"], "title": "Military AI Needs Technically-Informed Regulation to Safeguard AI Research and its Applications", "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.RO"], "comment": "16 pages, 2 tables, 1 figure", "summary": "Military weapon systems and command-and-control infrastructure augmented by\nartificial intelligence (AI) have seen rapid development and deployment in\nrecent years. However, the sociotechnical impacts of AI on combat systems,\nmilitary decision-making, and the norms of warfare have been understudied. We\nfocus on a specific subset of lethal autonomous weapon systems (LAWS) that use\nAI for targeting or battlefield decisions. We refer to this subset as\nAI-powered lethal autonomous weapon systems (AI-LAWS) and argue that they\nintroduce novel risks -- including unanticipated escalation, poor reliability\nin unfamiliar environments, and erosion of human oversight -- all of which\nthreaten both military effectiveness and the openness of AI research. These\nrisks cannot be addressed by high-level policy alone; effective regulation must\nbe grounded in the technical behavior of AI models. We argue that AI\nresearchers must be involved throughout the regulatory lifecycle. Thus, we\npropose a clear, behavior-based definition of AI-LAWS -- systems that introduce\nunique risks through their use of modern AI -- as a foundation for technically\ngrounded regulation, given that existing frameworks do not distinguish them\nfrom conventional LAWS. Using this definition, we propose several\ntechnically-informed policy directions and invite greater participation from\nthe AI research community in military AI policy discussions."}
{"id": "2505.19552", "pdf": "https://arxiv.org/pdf/2505.19552", "abs": "https://arxiv.org/abs/2505.19552", "authors": ["Minkyu Kim", "Kiyoung Seong", "Dongyeop Woo", "Sungsoo Ahn", "Minsu Kim"], "title": "On scalable and efficient training of diffusion samplers", "categories": ["cs.LG"], "comment": null, "summary": "We address the challenge of training diffusion models to sample from\nunnormalized energy distributions in the absence of data, the so-called\ndiffusion samplers. Although these approaches have shown promise, they struggle\nto scale in more demanding scenarios where energy evaluations are expensive and\nthe sampling space is high-dimensional. To address this limitation, we propose\na scalable and sample-efficient framework that properly harmonizes the powerful\nclassical sampling method and the diffusion sampler. Specifically, we utilize\nMonte Carlo Markov chain (MCMC) samplers with a novelty-based auxiliary energy\nas a Searcher to collect off-policy samples, using an auxiliary energy function\nto compensate for exploring modes the diffusion sampler rarely visits. These\noff-policy samples are then combined with on-policy data to train the diffusion\nsampler, thereby expanding its coverage of the energy landscape. Furthermore,\nwe identify primacy bias, i.e., the preference of samplers for early experience\nduring training, as the main cause of mode collapse during training, and\nintroduce a periodic re-initialization trick to resolve this issue. Our method\nsignificantly improves sample efficiency on standard benchmarks for diffusion\nsamplers and also excels at higher-dimensional problems and real-world\nmolecular conformer generation."}
{"id": "2505.19851", "pdf": "https://arxiv.org/pdf/2505.19851", "abs": "https://arxiv.org/abs/2505.19851", "authors": ["Gulfarogh Azam", "Mohd Sadique", "Saif Ali", "Mohammad Nadeem", "Erik Cambria", "Shahab Saquib Sohail", "Mohammad Sultan Alam"], "title": "Beyond Specialization: Benchmarking LLMs for Transliteration of Indian Languages", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Transliteration, the process of mapping text from one script to another,\nplays a crucial role in multilingual natural language processing, especially\nwithin linguistically diverse contexts such as India. Despite significant\nadvancements through specialized models like IndicXlit, recent developments in\nlarge language models suggest a potential for general-purpose models to excel\nat this task without explicit task-specific training. The current work\nsystematically evaluates the performance of prominent LLMs, including GPT-4o,\nGPT-4.5, GPT-4.1, Gemma-3-27B-it, and Mistral-Large against IndicXlit, a\nstate-of-the-art transliteration model, across ten major Indian languages.\nExperiments utilized standard benchmarks, including Dakshina and Aksharantar\ndatasets, with performance assessed via Top-1 Accuracy and Character Error\nRate. Our findings reveal that while GPT family models generally outperform\nother LLMs and IndicXlit for most instances. Additionally, fine-tuning GPT-4o\nimproves performance on specific languages notably. An extensive error analysis\nand robustness testing under noisy conditions further elucidate strengths of\nLLMs compared to specialized models, highlighting the efficacy of foundational\nmodels for a wide spectrum of specialized applications with minimal overhead."}
{"id": "2505.18373", "pdf": "https://arxiv.org/pdf/2505.18373", "abs": "https://arxiv.org/abs/2505.18373", "authors": ["Paul M. Riechers", "Henry R. Bigelow", "Eric A. Alt", "Adam Shai"], "title": "Next-token pretraining implies in-context learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We argue that in-context learning (ICL) predictably arises from standard\nself-supervised next-token pretraining, rather than being an exotic emergent\nproperty. This work establishes the foundational principles of this emergence\nby focusing on in-distribution ICL, demonstrating how models necessarily adapt\nto context when trained on token sequences, especially from non-ergodic\nsources. Our information-theoretic framework precisely predicts these\nin-distribution ICL dynamics (i.e., context-dependent loss reduction). We\nverify this with experiments using synthetic datasets of differing types of\ncorrelational structure, reproducing characteristic phenomena like phase\ntransitions in training loss for induction head formation and power-law scaling\nof in-context loss. We further show that a model's in-context performance on\nany task is mathematically coupled to the ensemble of tasks seen in\npretraining, offering a fundamental explanation, grounded in architecture- and\nmodality-independent principles, for such inference-time learning."}
{"id": "2505.19561", "pdf": "https://arxiv.org/pdf/2505.19561", "abs": "https://arxiv.org/abs/2505.19561", "authors": ["Yuan Feng", "Yukun Cao", "Hairu Wang", "Xike Xie", "S Kevin Zhou"], "title": "Lego Sketch: A Scalable Memory-augmented Neural Network for Sketching Data Streams", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Sketches, probabilistic structures for estimating item frequencies in\ninfinite data streams with limited space, are widely used across various\ndomains. Recent studies have shifted the focus from handcrafted sketches to\nneural sketches, leveraging memory-augmented neural networks (MANNs) to enhance\nthe streaming compression capabilities and achieve better space-accuracy\ntrade-offs.However, existing neural sketches struggle to scale across different\ndata domains and space budgets due to inflexible MANN configurations. In this\npaper, we introduce a scalable MANN architecture that brings to life the {\\it\nLego sketch}, a novel sketch with superior scalability and accuracy. Much like\nassembling creations with modular Lego bricks, the Lego sketch dynamically\ncoordinates multiple memory bricks to adapt to various space budgets and\ndiverse data domains. Our theoretical analysis guarantees its high scalability\nand provides the first error bound for neural sketch. Furthermore, extensive\nexperimental evaluations demonstrate that the Lego sketch exhibits superior\nspace-accuracy trade-offs, outperforming existing handcrafted and neural\nsketches. Our code is available at https://github.com/FFY0/LegoSketch_ICML."}
{"id": "2505.19862", "pdf": "https://arxiv.org/pdf/2505.19862", "abs": "https://arxiv.org/abs/2505.19862", "authors": ["Hexuan Deng", "Wenxiang Jiao", "Xuebo Liu", "Jun Rao", "Min Zhang"], "title": "REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Large Reasoning Models", "categories": ["cs.CL", "cs.LG"], "comment": "Work in Progress", "summary": "Large Reasoning Models (LRMs) demonstrate strong performance in complex tasks\nbut often face the challenge of overthinking, leading to substantially high\ninference costs. Existing approaches synthesize shorter reasoning responses for\nLRMs to learn, but are inefficient for online usage due to the time-consuming\ndata generation and filtering processes. Meanwhile, online reinforcement\nlearning mainly adopts a length reward to encourage short reasoning responses,\nbut tends to lose the reflection ability and harm the performance. To address\nthese issues, we propose REA-RL, which introduces a small reflection model for\nefficient scaling in online training, offering both parallel sampling and\nsequential revision. Besides, a reflection reward is designed to further\nprevent LRMs from favoring short yet non-reflective responses. Experiments show\nthat both methods maintain or enhance performance while significantly improving\ninference efficiency. Their combination achieves a good balance between\nperformance and efficiency, reducing inference costs by 35% without\ncompromising performance. Further analysis demonstrates that our methods are\neffective by maintaining reflection frequency for hard problems while\nappropriately reducing it for simpler ones without losing reflection ability.\nCodes are available at https://github.com/hexuandeng/REA-RL."}
{"id": "2505.18377", "pdf": "https://arxiv.org/pdf/2505.18377", "abs": "https://arxiv.org/abs/2505.18377", "authors": ["Pingchuan Ma", "Ziang Yin", "Qi Jing", "Zhengqi Gao", "Nicholas Gangi", "Boyang Zhang", "Tsung-Wei Huang", "Zhaoran Huang", "Duane S. Boning", "Yu Yao", "Jiaqi Gu"], "title": "SP2RINT: Spatially-Decoupled Physics-Inspired Progressive Inverse Optimization for Scalable, PDE-Constrained Meta-Optical Neural Network Training", "categories": ["physics.optics", "cs.AI", "cs.LG"], "comment": null, "summary": "DONNs harness the physics of light propagation for efficient analog\ncomputation, with applications in AI and signal processing. Advances in\nnanophotonic fabrication and metasurface-based wavefront engineering have\nopened new pathways to realize high-capacity DONNs across various spectral\nregimes. Training such DONN systems to determine the metasurface structures\nremains challenging. Heuristic methods are fast but oversimplify metasurfaces\nmodulation, often resulting in physically unrealizable designs and significant\nperformance degradation. Simulation-in-the-loop training methods directly\noptimize a physically implementable metasurface using adjoint methods during\nend-to-end DONN training, but are inherently computationally prohibitive and\nunscalable.To address these limitations, we propose SP2RINT, a spatially\ndecoupled, progressive training framework that formulates DONN training as a\nPDE-constrained learning problem. Metasurface responses are first relaxed into\nfreely trainable transfer matrices with a banded structure. We then\nprogressively enforce physical constraints by alternating between transfer\nmatrix training and adjoint-based inverse design, avoiding per-iteration PDE\nsolves while ensuring final physical realizability. To further reduce runtime,\nwe introduce a physics-inspired, spatially decoupled inverse design strategy\nbased on the natural locality of field interactions. This approach partitions\nthe metasurface into independently solvable patches, enabling scalable and\nparallel inverse design with system-level calibration. Evaluated across diverse\nDONN training tasks, SP2RINT achieves digital-comparable accuracy while being\n1825 times faster than simulation-in-the-loop approaches. By bridging the gap\nbetween abstract DONN models and implementable photonic hardware, SP2RINT\nenables scalable, high-performance training of physically realizable\nmeta-optical neural systems."}
{"id": "2505.19578", "pdf": "https://arxiv.org/pdf/2505.19578", "abs": "https://arxiv.org/abs/2505.19578", "authors": ["Dan Peng", "Zhihui Fu", "Zewen Ye", "Zhuoran Song", "Jun Wang"], "title": "Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Under review", "summary": "Sparse attention methods exploit the inherent sparsity in attention to speed\nup the prefilling phase of long-context inference, mitigating the quadratic\ncomplexity of full attention computation. While existing sparse attention\nmethods rely on predefined patterns or inaccurate estimations to approximate\nattention behavior, they often fail to fully capture the true dynamics of\nattention, resulting in reduced efficiency and compromised accuracy. Instead,\nwe propose a highly accurate sparse attention mechanism that shares similar yet\nprecise attention patterns across heads, enabling a more realistic capture of\nthe dynamic behavior of attention. Our approach is grounded in two key\nobservations: (1) attention patterns demonstrate strong inter-head similarity,\nand (2) this similarity remains remarkably consistent across diverse inputs. By\nstrategically sharing computed accurate patterns across attention heads, our\nmethod effectively captures actual patterns while requiring full attention\ncomputation for only a small subset of heads. Comprehensive evaluations\ndemonstrate that our approach achieves superior or comparable speedup relative\nto state-of-the-art methods while delivering the best overall accuracy."}
{"id": "2505.19912", "pdf": "https://arxiv.org/pdf/2505.19912", "abs": "https://arxiv.org/abs/2505.19912", "authors": ["Javier Marn"], "title": "APE: A Data-Centric Benchmark for Efficient LLM Adaptation in Text Summarization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present Adjacent Possible Exploration (APE), a simple yet effective method\nfor adapting large language models to specific tasks using minimal\ncomputational resources. Unlike traditional fine-tuning that requires extensive\ncompute, APE iteratively fine-tunes models on small, carefully selected data\nbatches (200 examples), retaining only improvements. On news summarization, APE\nachieves 40 percent BLEU improvement using just a T4 GPU in 60 minutes,\nmatching or exceeding more complex methods like LoRA while remaining\nconceptually simple. Our approach is particularly valuable for researchers and\npractitioners with limited computational resources. We provide open-source code\nand demonstrate APE's effectiveness through both automatic metrics and human\nevaluation. While inspired by evolutionary theory's \"adjacent possible\", APE's\ncore insight has a very practical application: small, iterative data\nperturbations can efficiently guide LLMs toward task-specific performance\nwithout expensive retraining."}
{"id": "2505.18384", "pdf": "https://arxiv.org/pdf/2505.18384", "abs": "https://arxiv.org/abs/2505.18384", "authors": ["Boyi Wei", "Benedikt Stroebl", "Jiacen Xu", "Joie Zhang", "Zhou Li", "Peter Henderson"], "title": "Dynamic Risk Assessments for Offensive Cybersecurity Agents", "categories": ["cs.CR", "cs.AI"], "comment": "26 pages, 11 figures", "summary": "Foundation models are increasingly becoming better autonomous programmers,\nraising the prospect that they could also automate dangerous offensive\ncyber-operations. Current frontier model audits probe the cybersecurity risks\nof such agents, but most fail to account for the degrees of freedom available\nto adversaries in the real world. In particular, with strong verifiers and\nfinancial incentives, agents for offensive cybersecurity are amenable to\niterative improvement by would-be adversaries. We argue that assessments should\ntake into account an expanded threat model in the context of cybersecurity,\nemphasizing the varying degrees of freedom that an adversary may possess in\nstateful and non-stateful environments within a fixed compute budget. We show\nthat even with a relatively small compute budget (8 H100 GPU Hours in our\nstudy), adversaries can improve an agent's cybersecurity capability on\nInterCode CTF by more than 40\\% relative to the baseline -- without any\nexternal assistance. These results highlight the need to evaluate agents'\ncybersecurity risk in a dynamic manner, painting a more representative picture\nof risk."}
{"id": "2505.19587", "pdf": "https://arxiv.org/pdf/2505.19587", "abs": "https://arxiv.org/abs/2505.19587", "authors": ["Shadi Alijani", "Homayoun Najjaran"], "title": "WQLCP: Weighted Adaptive Conformal Prediction for Robust Uncertainty Quantification Under Distribution Shifts", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Conformal prediction (CP) provides a framework for constructing prediction\nsets with guaranteed coverage, assuming exchangeable data. However, real-world\nscenarios often involve distribution shifts that violate exchangeability,\nleading to unreliable coverage and inflated prediction sets. To address this\nchallenge, we first introduce Reconstruction Loss-Scaled Conformal Prediction\n(RLSCP), which utilizes reconstruction losses derived from a Variational\nAutoencoder (VAE) as an uncertainty metric to scale score functions. While\nRLSCP demonstrates performance improvements, mainly resulting in better\ncoverage, it quantifies quantiles based on a fixed calibration dataset without\nconsidering the discrepancies between test and train datasets in an\nunexchangeable setting. In the next step, we propose Weighted Quantile\nLoss-scaled Conformal Prediction (WQLCP), which refines RLSCP by incorporating\na weighted notion of exchangeability, adjusting the calibration quantile\nthreshold based on weights with respect to the ratio of calibration and test\nloss values. This approach improves the CP-generated prediction set outputs in\nthe presence of distribution shifts. Experiments on large-scale datasets,\nincluding ImageNet variants, demonstrate that WQLCP outperforms existing\nbaselines by consistently maintaining coverage while reducing prediction set\nsizes, providing a robust solution for CP under distribution shifts."}
{"id": "2505.19914", "pdf": "https://arxiv.org/pdf/2505.19914", "abs": "https://arxiv.org/abs/2505.19914", "authors": ["Jiangjie Chen", "Qianyu He", "Siyu Yuan", "Aili Chen", "Zhicheng Cai", "Weinan Dai", "Hongli Yu", "Qiying Yu", "Xuefeng Li", "Jiaze Chen", "Hao Zhou", "Mingxuan Wang"], "title": "Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at\nadvanced reasoning tasks like math and coding via Reinforcement Learning with\nVerifiable Rewards (RLVR), but still struggle with puzzles solvable by humans\nwithout domain knowledge. We introduce Enigmata, the first comprehensive suite\ntailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks\nacross seven categories, each with 1) a generator that produces unlimited\nexamples with controllable difficulty and 2) a rule-based verifier for\nautomatic evaluation. This generator-verifier design supports scalable,\nmulti-task RL training, fine-grained analysis, and seamless RLVR integration.\nWe further propose Enigmata-Eval, a rigorous benchmark, and develop optimized\nmulti-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata,\nconsistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks\nlike Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes\nwell to out-of-domain puzzle benchmarks and mathematical reasoning, with little\nmulti-tasking trade-off. When trained on larger models like Seed1.5-Thinking\n(20B activated parameters and 200B total parameters), puzzle data from Enigmata\nfurther boosts SoTA performance on advanced math and STEM reasoning tasks such\nas AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization\nbenefits of Enigmata. This work offers a unified, controllable framework for\nadvancing logical reasoning in LLMs. Resources of this work can be found at\nhttps://seed-enigmata.github.io."}
{"id": "2505.18385", "pdf": "https://arxiv.org/pdf/2505.18385", "abs": "https://arxiv.org/abs/2505.18385", "authors": ["Jeba Rezwana", "Corey Ford"], "title": "Human-Centered AI Communication in Co-Creativity: An Initial Framework and Insights", "categories": ["cs.HC", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2504.02526", "summary": "Effective communication between AI and humans is essential for successful\nhuman-AI co-creation. However, many current co-creative AI systems lack\neffective communication, which limits their potential for collaboration. This\npaper presents the initial design of the Framework for AI Communication (FAICO)\nfor co-creative AI, developed through a systematic review of 107 full-length\npapers. FAICO presents key aspects of AI communication and their impact on user\nexperience, offering preliminary guidelines for designing human-centered AI\ncommunication. To improve the framework, we conducted a preliminary study with\ntwo focus groups involving skilled individuals in AI, HCI, and design. These\nsessions sought to understand participants' preferences for AI communication,\ngather their perceptions of the framework, collect feedback for refinement, and\nexplore its use in co-creative domains like collaborative writing and design.\nOur findings reveal a preference for a human-AI feedback loop over linear\ncommunication and emphasize the importance of context in fostering mutual\nunderstanding. Based on these insights, we propose actionable strategies for\napplying FAICO in practice and future directions, marking the first step toward\ndeveloping comprehensive guidelines for designing effective human-centered AI\ncommunication in co-creation."}
{"id": "2505.19589", "pdf": "https://arxiv.org/pdf/2505.19589", "abs": "https://arxiv.org/abs/2505.19589", "authors": ["Christiant Lebeda", "Mathieu Even", "Aurlien Bellet", "Julie Josse"], "title": "Model Agnostic Differentially Private Causal Inference", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Estimating causal effects from observational data is essential in fields such\nas medicine, economics and social sciences, where privacy concerns are\nparamount. We propose a general, model-agnostic framework for differentially\nprivate estimation of average treatment effects (ATE) that avoids strong\nstructural assumptions on the data-generating process or the models used to\nestimate propensity scores and conditional outcomes. In contrast to prior work,\nwhich enforces differential privacy by directly privatizing these nuisance\ncomponents and results in a privacy cost that scales with model complexity, our\napproach decouples nuisance estimation from privacy protection. This separation\nallows the use of flexible, state-of-the-art black-box models, while\ndifferential privacy is achieved by perturbing only predictions and aggregation\nsteps within a fold-splitting scheme with ensemble techniques. We instantiate\nthe framework for three classical estimators -- the G-formula, inverse\npropensity weighting (IPW), and augmented IPW (AIPW) -- and provide formal\nutility and privacy guarantees. Empirical results show that our methods\nmaintain competitive performance under realistic privacy budgets. We further\nextend our framework to support meta-analysis of multiple private ATE\nestimates. Our results bridge a critical gap between causal inference and\nprivacy-preserving data analysis."}
{"id": "2505.19937", "pdf": "https://arxiv.org/pdf/2505.19937", "abs": "https://arxiv.org/abs/2505.19937", "authors": ["Pooneh Mousavi", "Yingzhi Wang", "Mirco Ravanelli", "Cem Subakan"], "title": "ALAS: Measuring Latent Speech-Text Alignment For Spoken Language Understanding In Multimodal LLMs", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Large Language Models (LLMs) are widely used in Spoken Language Understanding\n(SLU). Recent SLU models process audio directly by adapting speech input into\nLLMs for better multimodal learning. A key consideration for these models is\nthe cross-modal alignment between text and audio modalities, which is a\ntelltale sign as to whether or not LLM is able to associate semantic meaning to\naudio segments. While various methods exist for fusing these modalities, there\nis no standard metric to evaluate alignment quality in LLMs. In this work, we\npropose a new metric, ALAS (Automatic Latent Alignment Score). Our study\nexamines the correlation between audio and text representations across\ntransformer layers, for two different tasks (Spoken Question Answering and\nEmotion Recognition). We showcase that our metric behaves as expected across\ndifferent layers and different tasks."}
{"id": "2505.18392", "pdf": "https://arxiv.org/pdf/2505.18392", "abs": "https://arxiv.org/abs/2505.18392", "authors": ["Danny Reidenbach", "Filipp Nikitin", "Olexandr Isayev", "Saee Paliwal"], "title": "Applications of Modular Co-Design for De Novo 3D Molecule Generation", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.QM"], "comment": null, "summary": "De novo 3D molecule generation is a pivotal task in drug discovery. However,\nmany recent geometric generative models struggle to produce high-quality 3D\nstructures, even if they maintain 2D validity and topological stability. To\ntackle this issue and enhance the learning of effective molecular generation\ndynamics, we present Megalodon-a family of scalable transformer models. These\nmodels are enhanced with basic equivariant layers and trained using a joint\ncontinuous and discrete denoising co-design objective. We assess Megalodon's\nperformance on established molecule generation benchmarks and introduce new 3D\nstructure benchmarks that evaluate a model's capability to generate realistic\nmolecular structures, particularly focusing on energetics. We show that\nMegalodon achieves state-of-the-art results in 3D molecule generation,\nconditional structure generation, and structure energy benchmarks using\ndiffusion and flow matching. Furthermore, doubling the number of parameters in\nMegalodon to 40M significantly enhances its performance, generating up to 49x\nmore valid large molecules and achieving energy levels that are 2-10x lower\nthan those of the best prior generative models."}
{"id": "2505.19590", "pdf": "https://arxiv.org/pdf/2505.19590", "abs": "https://arxiv.org/abs/2505.19590", "authors": ["Xuandong Zhao", "Zhewei Kang", "Aosong Feng", "Sergey Levine", "Dawn Song"], "title": "Learning to Reason without External Rewards", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Training large language models (LLMs) for complex reasoning via Reinforcement\nLearning with Verifiable Rewards (RLVR) is effective but limited by reliance on\ncostly, domain-specific supervision. We explore Reinforcement Learning from\nInternal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic\nsignals without external rewards or labeled data. We propose Intuitor, an RLIF\nmethod that uses a model's own confidence, termed self-certainty, as its sole\nreward signal. Intuitor replaces external rewards in Group Relative Policy\nOptimization (GRPO) with self-certainty scores, enabling fully unsupervised\nlearning. Experiments demonstrate that Intuitor matches GRPO's performance on\nmathematical benchmarks while achieving superior generalization to\nout-of-domain tasks like code generation, without requiring gold solutions or\ntest cases. Our findings show that intrinsic model signals can drive effective\nlearning across domains, offering a scalable alternative to RLVR for autonomous\nAI systems where verifiable rewards are unavailable. Code is available at\nhttps://github.com/sunblaze-ucb/Intuitor"}
{"id": "2505.19959", "pdf": "https://arxiv.org/pdf/2505.19959", "abs": "https://arxiv.org/abs/2505.19959", "authors": ["Zhongzhan Huang", "Guoming Ling", "Shanshan Zhong", "Hefeng Wu", "Liang Lin"], "title": "MiniLongBench: The Low-cost Long Context Understanding Benchmark for Large Language Models", "categories": ["cs.CL"], "comment": "Accepted by ACL'25 main track", "summary": "Long Context Understanding (LCU) is a critical area for exploration in\ncurrent large language models (LLMs). However, due to the inherently lengthy\nnature of long-text data, existing LCU benchmarks for LLMs often result in\nprohibitively high evaluation costs, like testing time and inference expenses.\nThrough extensive experimentation, we discover that existing LCU benchmarks\nexhibit significant redundancy, which means the inefficiency in evaluation. In\nthis paper, we propose a concise data compression method tailored for long-text\ndata with sparse information characteristics. By pruning the well-known LCU\nbenchmark LongBench, we create MiniLongBench. This benchmark includes only 237\ntest samples across six major task categories and 21 distinct tasks. Through\nempirical analysis of over 60 LLMs, MiniLongBench achieves an average\nevaluation cost reduced to only 4.5% of the original while maintaining an\naverage rank correlation coefficient of 0.97 with LongBench results. Therefore,\nour MiniLongBench, as a low-cost benchmark, holds great potential to\nsubstantially drive future research into the LCU capabilities of LLMs. See\nhttps://github.com/MilkThink-Lab/MiniLongBench for our code, data and tutorial."}
{"id": "2505.18397", "pdf": "https://arxiv.org/pdf/2505.18397", "abs": "https://arxiv.org/abs/2505.18397", "authors": ["Fangqiao Tian", "An Luo", "Jin Du", "Xun Xian", "Robert Specht", "Ganghua Wang", "Xuan Bi", "Jiawei Zhou", "Jayanth Srinivasa", "Ashish Kundu", "Charles Fleming", "Rui Zhang", "Zirui Liu", "Mingyi Hong", "Jie Ding"], "title": "An Outlook on the Opportunities and Challenges of Multi-Agent AI Systems", "categories": ["cs.MA", "cs.AI", "cs.ET", "cs.LG", "68T42 (Agent technology and artificial intelligence), 68T01 (General\n  topics in artificial intelligence), 68M14 (Distributed systems)", "I.2.11; I.2.4; I.2.6"], "comment": null, "summary": "Multi-agent AI systems (MAS) offer a promising framework for distributed\nintelligence, enabling collaborative reasoning, planning, and decision-making\nacross autonomous agents. This paper provides a systematic outlook on the\ncurrent opportunities and challenges of MAS, drawing insights from recent\nadvances in large language models (LLMs), federated optimization, and human-AI\ninteraction. We formalize key concepts including agent topology, coordination\nprotocols, and shared objectives, and identify major risks such as dependency,\nmisalignment, and vulnerabilities arising from training data overlap. Through a\nbiologically inspired simulation and comprehensive theoretical framing, we\nhighlight critical pathways for developing robust, scalable, and secure MAS in\nreal-world settings."}
{"id": "2505.19601", "pdf": "https://arxiv.org/pdf/2505.19601", "abs": "https://arxiv.org/abs/2505.19601", "authors": ["Yeongmin Kim", "Heesun Bae", "Byeonghu Na", "Il-Chul Moon"], "title": "Preference Optimization by Estimating the Ratio of the Data Distribution", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Direct preference optimization (DPO) is widely used as a simple and stable\nmethod for aligning large language models (LLMs) with human preferences. This\npaper investigates a generalized DPO loss that enables a policy model to match\nthe target policy from a likelihood ratio estimation perspective. The ratio of\nthe target policy provides a unique identification of the policy distribution\nwithout relying on reward models or partition functions. This allows the\ngeneralized loss to retain both simplicity and theoretical guarantees, which\nprior work such as $f$-PO fails to achieve simultaneously. We propose Bregman\npreference optimization (BPO), a generalized framework for ratio matching that\nprovides a family of objective functions achieving target policy optimality.\nBPO subsumes DPO as a special case and offers tractable forms for all\ninstances, allowing implementation with a few lines of code. We further develop\nscaled Basu's power divergence (SBA), a gradient scaling method that can be\nused for BPO instances. The BPO framework complements other DPO variants and is\napplicable to target policies defined by these variants. In experiments, unlike\nother probabilistic loss extensions such as $f$-DPO or $f$-PO, which exhibit a\ntrade-off between generation fidelity and diversity, instances of BPO improve\nboth win rate and entropy compared with DPO. When applied to\nLlama-3-Instruct-8B, BPO achieves state-of-the-art performance among Llama-3-8B\nbackbones, with a 55.9\\% length-controlled win rate on AlpacaEval2."}
{"id": "2505.19970", "pdf": "https://arxiv.org/pdf/2505.19970", "abs": "https://arxiv.org/abs/2505.19970", "authors": ["Jiayuan Su", "Fulin Lin", "Zhaopeng Feng", "Han Zheng", "Teng Wang", "Zhenyu Xiao", "Xinlong Zhao", "Zuozhu Liu", "Lu Cheng", "Hongwei Wang"], "title": "CP-Router: An Uncertainty-Aware Router Between LLM and LRM", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in Large Reasoning Models (LRMs) have significantly improved\nlong-chain reasoning capabilities over Large Language Models (LLMs). However,\nLRMs often produce unnecessarily lengthy outputs even for simple queries,\nleading to inefficiencies or even accuracy degradation compared to LLMs. To\novercome this, we propose CP-Router, a training-free and model-agnostic routing\nframework that dynamically selects between an LLM and an LRM, demonstrated with\nmultiple-choice question answering (MCQA) prompts. The routing decision is\nguided by the prediction uncertainty estimates derived via Conformal Prediction\n(CP), which provides rigorous coverage guarantees. To further refine the\nuncertainty differentiation across inputs, we introduce Full and Binary Entropy\n(FBE), a novel entropy-based criterion that adaptively selects the appropriate\nCP threshold. Experiments across diverse MCQA benchmarks, including\nmathematics, logical reasoning, and Chinese chemistry, demonstrate that\nCP-Router efficiently reduces token usage while maintaining or even improving\naccuracy compared to using LRM alone. We also extend CP-Router to diverse model\npairings and open-ended QA, where it continues to demonstrate strong\nperformance, validating its generality and robustness."}
{"id": "2505.18398", "pdf": "https://arxiv.org/pdf/2505.18398", "abs": "https://arxiv.org/abs/2505.18398", "authors": ["Liao Peiyuan"], "title": "Towards Anonymous Neural Network Inference", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "We introduce funion, a system providing end-to-end sender-receiver\nunlinkability for neural network inference. By leveraging the Pigeonhole\nstorage protocol and BACAP (blinding-and-capability) scheme from the Echomix\nanonymity system, funion inherits the provable security guarantees of modern\nmixnets. Users can anonymously store input tensors in pseudorandom storage\nlocations, commission compute services to process them via the neural network,\nand retrieve results with no traceable connection between input and output\nparties. This store-compute-store paradigm masks both network traffic patterns\nand computational workload characteristics, while quantizing execution timing\ninto public latency buckets. Our security analysis demonstrates that funion\ninherits the strong metadata privacy guarantees of Echomix under largely the\nsame trust assumptions, while introducing acceptable overhead for\nproduction-scale workloads. Our work paves the way towards an accessible\nplatform where users can submit fully anonymized inference queries to cloud\nservices."}
{"id": "2505.19602", "pdf": "https://arxiv.org/pdf/2505.19602", "abs": "https://arxiv.org/abs/2505.19602", "authors": ["Kunjun Li", "Zigeng Chen", "Cheng-Yen Yang", "Jenq-Neng Hwang"], "title": "Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV Cache Compression", "categories": ["cs.LG"], "comment": null, "summary": "Visual Autoregressive (VAR) modeling has garnered significant attention for\nits innovative next-scale prediction approach, which yields substantial\nimprovements in efficiency, scalability, and zero-shot generalization.\nNevertheless, the coarse-to-fine methodology inherent in VAR results in\nexponential growth of the KV cache during inference, causing considerable\nmemory consumption and computational redundancy. To address these bottlenecks,\nwe introduce ScaleKV, a novel KV cache compression framework tailored for VAR\narchitectures. ScaleKV leverages two critical observations: varying cache\ndemands across transformer layers and distinct attention patterns at different\nscales. Based on these insights, ScaleKV categorizes transformer layers into\ntwo functional groups: drafters and refiners. Drafters exhibit dispersed\nattention across multiple scales, thereby requiring greater cache capacity.\nConversely, refiners focus attention on the current token map to process local\ndetails, consequently necessitating substantially reduced cache capacity.\nScaleKV optimizes the multi-scale inference pipeline by identifying\nscale-specific drafters and refiners, facilitating differentiated cache\nmanagement tailored to each scale. Evaluation on the state-of-the-art\ntext-to-image VAR model family, Infinity, demonstrates that our approach\neffectively reduces the required KV cache memory to 10% while preserving\npixel-level fidelity."}
{"id": "2505.19971", "pdf": "https://arxiv.org/pdf/2505.19971", "abs": "https://arxiv.org/abs/2505.19971", "authors": ["Kilian Sennrich", "Sina Ahmadi"], "title": "Conversational Lexicography: Querying Lexicographic Data on Knowledge Graphs with SPARQL through Natural Language", "categories": ["cs.CL"], "comment": "Accepted to LDK 2025 - the 5th Conference on Language, Data and\n  Knowledge. Naples, Italy, 9-11 September 2025", "summary": "Knowledge graphs offer an excellent solution for representing the\nlexical-semantic structures of lexicographic data. However, working with the\nSPARQL query language represents a considerable hurdle for many non-expert\nusers who could benefit from the advantages of this technology. This paper\naddresses the challenge of creating natural language interfaces for\nlexicographic data retrieval on knowledge graphs such as Wikidata. We develop a\nmultidimensional taxonomy capturing the complexity of Wikidata's lexicographic\ndata ontology module through four dimensions and create a template-based\ndataset with over 1.2 million mappings from natural language utterances to\nSPARQL queries. Our experiments with GPT-2 (124M), Phi-1.5 (1.3B), and\nGPT-3.5-Turbo reveal significant differences in model capabilities. While all\nmodels perform well on familiar patterns, only GPT-3.5-Turbo demonstrates\nmeaningful generalization capabilities, suggesting that model size and diverse\npre-training are crucial for adaptability in this domain. However, significant\nchallenges remain in achieving robust generalization, handling diverse\nlinguistic data, and developing scalable solutions that can accommodate the\nfull complexity of lexicographic knowledge representation."}
{"id": "2505.18399", "pdf": "https://arxiv.org/pdf/2505.18399", "abs": "https://arxiv.org/abs/2505.18399", "authors": ["Lin Zhao", "Yushu Wu", "Xinru Jiang", "Jianyang Gu", "Yanzhi Wang", "Xiaolin Xu", "Pu Zhao", "Xue Lin"], "title": "Taming Diffusion for Dataset Distillation with High Representativeness", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "The paper is accepted by ICML 2025", "summary": "Recent deep learning models demand larger datasets, driving the need for\ndataset distillation to create compact, cost-efficient datasets while\nmaintaining performance. Due to the powerful image generation capability of\ndiffusion, it has been introduced to this field for generating distilled\nimages. In this paper, we systematically investigate issues present in current\ndiffusion-based dataset distillation methods, including inaccurate distribution\nmatching, distribution deviation with random noise, and separate sampling.\nBuilding on this, we propose D^3HR, a novel diffusion-based framework to\ngenerate distilled datasets with high representativeness. Specifically, we\nadopt DDIM inversion to map the latents of the full dataset from a\nlow-normality latent domain to a high-normality Gaussian domain, preserving\ninformation and ensuring structural consistency to generate representative\nlatents for the distilled dataset. Furthermore, we propose an efficient\nsampling scheme to better align the representative latents with the\nhigh-normality Gaussian distribution. Our comprehensive experiments demonstrate\nthat D^3HR can achieve higher accuracy across different model architectures\ncompared with state-of-the-art baselines in dataset distillation. Source code:\nhttps://github.com/lin-zhao-resoLve/D3HR."}
{"id": "2505.19605", "pdf": "https://arxiv.org/pdf/2505.19605", "abs": "https://arxiv.org/abs/2505.19605", "authors": ["Aggrey Muhebwa", "Khotso Selialia", "Fatima Anwar", "Khalid K. Osman"], "title": "Kuramoto-FedAvg: Using Synchronization Dynamics to Improve Federated Learning Optimization under Statistical Heterogeneity", "categories": ["cs.LG"], "comment": null, "summary": "Federated learning on heterogeneous (non-IID) client data experiences slow\nconvergence due to client drift. To address this challenge, we propose\nKuramoto-FedAvg, a federated optimization algorithm that reframes the weight\naggregation step as a synchronization problem inspired by the Kuramoto model of\ncoupled oscillators. The server dynamically weighs each client's update based\non its phase alignment with the global update, amplifying contributions that\nalign with the global gradient direction while minimizing the impact of updates\nthat are out of phase. We theoretically prove that this synchronization\nmechanism reduces client drift, providing a tighter convergence bound compared\nto the standard FedAvg under heterogeneous data distributions. Empirical\nvalidation supports our theoretical findings, showing that Kuramoto-FedAvg\nsignificantly accelerates convergence and improves accuracy across multiple\nbenchmark datasets. Our work highlights the potential of coordination and\nsynchronization-based strategies for managing gradient diversity and\naccelerating federated optimization in realistic non-IID settings."}
{"id": "2505.19978", "pdf": "https://arxiv.org/pdf/2505.19978", "abs": "https://arxiv.org/abs/2505.19978", "authors": ["Alkis Koudounas", "Moreno La Quatra", "Elena Baralis"], "title": "DeepDialogue: A Multi-Turn Emotionally-Rich Spoken Dialogue Dataset", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Currently under review. See the official website:\n  https://salt-research.github.io/DeepDialogue", "summary": "Recent advances in conversational AI have demonstrated impressive\ncapabilities in single-turn responses, yet multi-turn dialogues remain\nchallenging for even the most sophisticated language models. Current dialogue\ndatasets are limited in their emotional range, domain diversity, turn depth,\nand are predominantly text-only, hindering progress in developing more\nhuman-like conversational systems across modalities. To address these\nlimitations, we present DeepDialogue, a large-scale multimodal dataset\ncontaining 40,150 high-quality multi-turn dialogues spanning 41 domains and\nincorporating 20 distinct emotions with coherent emotional progressions. Our\napproach pairs 9 different language models (4B-72B parameters) to generate\n65,600 initial conversations, which we then evaluate through a combination of\nhuman annotation and LLM-based quality filtering. The resulting dataset reveals\nfundamental insights: smaller models fail to maintain coherence beyond 6\ndialogue turns; concrete domains (e.g., \"cars,\" \"travel\") yield more meaningful\nconversations than abstract ones (e.g., \"philosophy\"); and cross-model\ninteractions produce more coherent dialogues than same-model conversations. A\nkey contribution of DeepDialogue is its speech component, where we synthesize\nemotion-consistent voices for all 40,150 dialogues, creating the first\nlarge-scale open-source multimodal dialogue dataset that faithfully preserves\nemotional context across multi-turn conversations."}
{"id": "2505.18404", "pdf": "https://arxiv.org/pdf/2505.18404", "abs": "https://arxiv.org/abs/2505.18404", "authors": ["Menghua Wu", "Cai Zhou", "Stephen Bates", "Tommi Jaakkola"], "title": "Thought calibration: Efficient and confident test-time scaling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reasoning large language models achieve impressive test-time scaling by\nthinking for longer, but this performance gain comes at significant compute\ncost. Directly limiting test-time budget hurts overall performance, but not all\nproblems are equally difficult. We propose thought calibration to decide\ndynamically when thinking can be terminated. To calibrate our decision rule, we\nview a language model's growing body of thoughts as a nested sequence of\nreasoning trees, where the goal is to identify the point at which novel\nreasoning plateaus. We realize this framework through lightweight probes that\noperate on top of the language model's hidden representations, which are\ninformative of both the reasoning structure and overall consistency of\nresponse. Based on three reasoning language models and four datasets, thought\ncalibration preserves model performance with up to a 60% reduction in thinking\ntokens on in-distribution data, and up to 20% in out-of-distribution data."}
{"id": "2505.19607", "pdf": "https://arxiv.org/pdf/2505.19607", "abs": "https://arxiv.org/abs/2505.19607", "authors": ["Yewon Han", "Seoyun Yang", "Taesup Kim"], "title": "Energy-based Preference Optimization for Test-time Adaptation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Test-Time Adaptation (TTA) enhances model robustness by enabling adaptation\nto target distributions that differ from training distributions, improving\nreal-world generalizability. Existing TTA approaches focus on adjusting the\nconditional distribution; however these methods often depend on uncertain\npredictions in the absence of label information, leading to unreliable\nperformance. Energy-based frameworks suggest a promising alternative to address\ndistribution shifts without relying on uncertain predictions, instead computing\nthe marginal distribution of target data. However, they involve the critical\nchallenge of requiring extensive SGLD sampling, which is impractical for\ntest-time scenarios requiring immediate adaptation. In this work, we propose\nEnergy-based Preference Optimization for Test-time Adaptation (EPOTTA), which\nis based on a sampling free strategy. We first parameterize the target model\nusing a pretrained model and residual energy function, enabling marginal\nlikelihood maximization of target data without sampling. Building on the\nobservation that the parameterization is mathematically equivalent to DPO\nobjective, we then directly adapt the model to a target distribution without\nexplicitly training the residual. Our experiments verify that EPOTTA is\nwell-calibrated and performant while achieving computational efficiency."}
{"id": "2505.19987", "pdf": "https://arxiv.org/pdf/2505.19987", "abs": "https://arxiv.org/abs/2505.19987", "authors": ["Yongshi Ye", "Biao Fu", "Chongxuan Huang", "Yidong Chen", "Xiaodong Shi"], "title": "How Well Do Large Reasoning Models Translate? A Comprehensive Evaluation for Multi-Domain Machine Translation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong performance in\ngeneral-purpose machine translation, but their effectiveness in complex,\ndomain-sensitive translation tasks remains underexplored. Recent advancements\nin Large Reasoning Models (LRMs), raise the question of whether structured\nreasoning can enhance translation quality across diverse domains. In this work,\nwe compare the performance of LRMs with traditional LLMs across 15\nrepresentative domains and four translation directions. Our evaluation\nconsiders various factors, including task difficulty, input length, and\nterminology density. We use a combination of automatic metrics and an enhanced\nMQM-based evaluation hierarchy to assess translation quality. Our findings show\nthat LRMs consistently outperform traditional LLMs in semantically complex\ndomains, especially in long-text and high-difficulty translation scenarios.\nMoreover, domain-adaptive prompting strategies further improve performance by\nbetter leveraging the reasoning capabilities of LRMs. These results highlight\nthe potential of structured reasoning in MDMT tasks and provide valuable\ninsights for optimizing translation systems in domain-sensitive contexts."}
{"id": "2505.18407", "pdf": "https://arxiv.org/pdf/2505.18407", "abs": "https://arxiv.org/abs/2505.18407", "authors": ["Yizhou Zhang", "Kishan Panaganti", "Laixi Shi", "Juba Ziani", "Adam Wierman"], "title": "KL-regularization Itself is Differentially Private in Bandits and RLHF", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Differential Privacy (DP) provides a rigorous framework for privacy, ensuring\nthe outputs of data-driven algorithms remain statistically indistinguishable\nacross datasets that differ in a single entry. While guaranteeing DP generally\nrequires explicitly injecting noise either to the algorithm itself or to its\noutputs, the intrinsic randomness of existing algorithms presents an\nopportunity to achieve DP ``for free''. In this work, we explore the role of\nregularization in achieving DP across three different decision-making problems:\nmulti-armed bandits, linear contextual bandits, and reinforcement learning from\nhuman feedback (RLHF), in offline data settings. We show that adding\nKL-regularization to the learning objective (a common approach in optimization\nalgorithms) makes the action sampled from the resulting stochastic policy\nitself differentially private. This offers a new route to privacy guarantees\nwithout additional noise injection, while also preserving the inherent\nadvantage of regularization in enhancing performance."}
{"id": "2505.19609", "pdf": "https://arxiv.org/pdf/2505.19609", "abs": "https://arxiv.org/abs/2505.19609", "authors": ["Hongtao Xu", "Wenting Shen", "Yuanxin Wei", "Ang Wang", "Guo Runfan", "Tianxing Wang", "Yong Li", "Mingzhen Li", "Weile Jia"], "title": "Skrull: Towards Efficient Long Context Fine-tuning through Dynamic Data Scheduling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Long-context supervised fine-tuning (Long-SFT) plays a vital role in\nenhancing the performance of large language models (LLMs) on long-context\ntasks. To smoothly adapt LLMs to long-context scenarios, this process typically\nentails training on mixed datasets containing both long and short sequences.\nHowever, this heterogeneous sequence length distribution poses significant\nchallenges for existing training systems, as they fail to simultaneously\nachieve high training efficiency for both long and short sequences, resulting\nin sub-optimal end-to-end system performance in Long-SFT. In this paper, we\npresent a novel perspective on data scheduling to address the challenges posed\nby the heterogeneous data distributions in Long-SFT. We propose Skrull, a\ndynamic data scheduler specifically designed for efficient long-SFT. Through\ndynamic data scheduling, Skrull balances the computation requirements of long\nand short sequences, improving overall training efficiency. Furthermore, we\nformulate the scheduling process as a joint optimization problem and thoroughly\nanalyze the trade-offs involved. Based on those analysis, Skrull employs a\nlightweight scheduling algorithm to achieve near-zero cost online scheduling in\nLong-SFT. Finally, we implement Skrull upon DeepSpeed, a state-of-the-art\ndistributed training system for LLMs. Experimental results demonstrate that\nSkrull outperforms DeepSpeed by 3.76x on average (up to 7.54x) in real-world\nlong-SFT scenarios."}
{"id": "2505.20006", "pdf": "https://arxiv.org/pdf/2505.20006", "abs": "https://arxiv.org/abs/2505.20006", "authors": ["Raphal Bagat", "Irina Illina", "Emmanuel Vincent"], "title": "Mixture of LoRA Experts for Low-Resourced Multi-Accent Automatic Speech Recognition", "categories": ["cs.CL"], "comment": "Submitted to Interspeech 2025", "summary": "We aim to improve the robustness of Automatic Speech Recognition (ASR)\nsystems against non-native speech, particularly in low-resourced multi-accent\nsettings. We introduce Mixture of Accent-Specific LoRAs (MAS-LoRA), a\nfine-tuning method that leverages a mixture of Low-Rank Adaptation (LoRA)\nexperts, each specialized in a specific accent. This method can be used when\nthe accent is known or unknown at inference time, without the need to fine-tune\nthe model again. Our experiments, conducted using Whisper on the L2-ARCTIC\ncorpus, demonstrate significant improvements in Word Error Rate compared to\nregular LoRA and full fine-tuning when the accent is unknown. When the accent\nis known, the results further improve. Furthermore, MAS-LoRA shows less\ncatastrophic forgetting than the other fine-tuning methods. To the best of our\nknowledge, this is the first use of a mixture of LoRA experts for non-native\nmulti-accent ASR."}
{"id": "2505.18413", "pdf": "https://arxiv.org/pdf/2505.18413", "abs": "https://arxiv.org/abs/2505.18413", "authors": ["Toshiaki Koike-Akino", "Xiangyu Chen", "Jing Liu", "Ye Wang", "Pu", "Wang", "Matthew Brand"], "title": "LatentLLM: Attention-Aware Joint Tensor Compression", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "37 pages, 16 figures", "summary": "Modern foundation models such as large language models (LLMs) and large\nmulti-modal models (LMMs) require a massive amount of computational and memory\nresources. We propose a new framework to convert such LLMs/LMMs into a\nreduced-dimension latent structure. Our method extends a local activation-aware\ntensor decomposition to a global attention-aware joint tensor de-composition.\nOur framework can significantly improve the model accuracy over the existing\nmodel compression methods when reducing the latent dimension to realize\ncomputationally/memory-efficient LLMs/LLMs. We show the benefit on several\nbenchmark including multi-modal reasoning tasks."}
{"id": "2505.19614", "pdf": "https://arxiv.org/pdf/2505.19614", "abs": "https://arxiv.org/abs/2505.19614", "authors": ["Sanghyuk Chun"], "title": "Multiplicity is an Inevitable and Inherent Challenge in Multimodal Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Multimodal learning has seen remarkable progress, particularly with the\nemergence of large-scale pre-training across various modalities. However, most\ncurrent approaches are built on the assumption of a deterministic, one-to-one\nalignment between modalities. This oversimplifies real-world multimodal\nrelationships, where their nature is inherently many-to-many. This phenomenon,\nnamed multiplicity, is not a side-effect of noise or annotation error, but an\ninevitable outcome of semantic abstraction, representational asymmetry, and\ntask-dependent ambiguity in multimodal tasks. This position paper argues that\nmultiplicity is a fundamental bottleneck that manifests across all stages of\nthe multimodal learning pipeline: from data construction to training and\nevaluation. This paper examines the causes and consequences of multiplicity,\nand highlights how multiplicity introduces training uncertainty, unreliable\nevaluation, and low dataset quality. This position calls for new research\ndirections on multimodal learning: novel multiplicity-aware learning frameworks\nand dataset construction protocols considering multiplicity."}
{"id": "2505.20013", "pdf": "https://arxiv.org/pdf/2505.20013", "abs": "https://arxiv.org/abs/2505.20013", "authors": ["Minda Hu", "Tianqing Fang", "Jianshu Zhang", "Junyu Ma", "Zhisong Zhang", "Jingyan Zhou", "Hongming Zhang", "Haitao Mi", "Dong Yu", "Irwin King"], "title": "WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback", "categories": ["cs.CL"], "comment": "18 pages", "summary": "Web agents powered by Large Language Models (LLMs) show promise for\nnext-generation AI, but their limited reasoning in uncertain, dynamic web\nenvironments hinders robust deployment. In this paper, we identify key\nreasoning skills essential for effective web agents, i.e., reflection &\nlookahead, branching, and rollback, and curate trajectory data that exemplifies\nthese abilities by reconstructing the agent's (inference-time) reasoning\nalgorithms into chain-of-thought rationales. We conduct experiments in the\nagent self-improving benchmark, OpenWebVoyager, and demonstrate that distilling\nsalient reasoning patterns into the backbone LLM via simple fine-tuning can\nsubstantially enhance its performance. Our approach yields significant\nimprovements across multiple benchmarks, including WebVoyager, Mind2web-live,\nand SimpleQA (web search), highlighting the potential of targeted reasoning\nskill enhancement for web agents."}
{"id": "2505.18417", "pdf": "https://arxiv.org/pdf/2505.18417", "abs": "https://arxiv.org/abs/2505.18417", "authors": ["Achkan Salehi"], "title": "Reinforcement Learning for Ballbot Navigation in Uneven Terrain", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "6 pages, 8 figures, 2 tables", "summary": "Ballbot (i.e. Ball balancing robot) navigation usually relies on methods\nrooted in control theory (CT), and works that apply Reinforcement learning (RL)\nto the problem remain rare while generally being limited to specific subtasks\n(e.g. balance recovery). Unlike CT based methods, RL does not require\n(simplifying) assumptions about environment dynamics (e.g. the absence of\nslippage between the ball and the floor). In addition to this increased\naccuracy in modeling, RL agents can easily be conditioned on additional\nobservations such as depth-maps without the need for explicit formulations from\nfirst principles, leading to increased adaptivity. Despite those advantages,\nthere has been little to no investigation into the capabilities,\ndata-efficiency and limitations of RL based methods for ballbot control and\nnavigation. Furthermore, there is a notable absence of an open-source,\nRL-friendly simulator for this task. In this paper, we present an open-source\nballbot simulation based on MuJoCo, and show that with appropriate conditioning\non exteroceptive observations as well as reward shaping, policies learned by\nclassical model-free RL methods are capable of effectively navigating through\nrandomly generated uneven terrain, using a reasonable amount of data (four to\nfive hours on a system operating at 500hz)."}
{"id": "2505.19616", "pdf": "https://arxiv.org/pdf/2505.19616", "abs": "https://arxiv.org/abs/2505.19616", "authors": ["Rui Cai", "Bangzheng Li", "Xiaofei Wen", "Muhao Chen", "Zhe Zhao"], "title": "Diagnosing and Mitigating Modality Interference in Multimodal Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities across tasks, yet they often exhibit difficulty in distinguishing\ntask-relevant from irrelevant signals, particularly in tasks like Visual\nQuestion Answering (VQA), which can lead to susceptibility to misleading or\nspurious inputs. We refer to this broader limitation as the Cross-Modality\nCompetency Problem: the model's inability to fairly evaluate all modalities.\nThis vulnerability becomes more evident in modality-specific tasks such as\nimage classification or pure text question answering, where models are expected\nto rely solely on one modality. In such tasks, spurious information from\nirrelevant modalities often leads to significant performance degradation. We\nrefer to this failure as Modality Interference, which serves as a concrete and\nmeasurable instance of the cross-modality competency problem. We further design\na perturbation-based causal diagnostic experiment to verify and quantify this\nproblem. To mitigate modality interference, we propose a novel framework to\nfine-tune MLLMs, including perturbation-based data augmentations with both\nheuristic perturbations and adversarial perturbations via Projected Gradient\nDescent (PGD), and a consistency regularization strategy applied to model\noutputs with original and perturbed inputs. Experiments on multiple benchmark\ndatasets (image-heavy, text-heavy, and VQA tasks) and multiple model families\nwith different scales demonstrate significant improvements in robustness and\ncross-modality competency, indicating our method's effectiveness in boosting\nunimodal reasoning ability while enhancing performance on multimodal tasks."}
{"id": "2505.20014", "pdf": "https://arxiv.org/pdf/2505.20014", "abs": "https://arxiv.org/abs/2505.20014", "authors": ["Hoyun Song", "Huije Lee", "Jisu Shin", "Sukmin Cho", "Changgeon Ko", "Jong C. Park"], "title": "Does Rationale Quality Matter? Enhancing Mental Disorder Detection via Selective Reasoning Distillation", "categories": ["cs.CL"], "comment": null, "summary": "The detection of mental health problems from social media and the\ninterpretation of these results have been extensively explored. Research has\nshown that incorporating clinical symptom information into a model enhances\ndomain expertise, improving its detection and interpretation performance. While\nlarge language models (LLMs) are shown to be effective for generating\nexplanatory rationales in mental health detection, their substantially large\nparameter size and high computational cost limit their practicality. Reasoning\ndistillation transfers this ability to smaller language models (SLMs), but\ninconsistencies in the relevance and domain alignment of LLM-generated\nrationales pose a challenge. This paper investigates how rationale quality\nimpacts SLM performance in mental health detection and explanation generation.\nWe hypothesize that ensuring high-quality and domain-relevant rationales\nenhances the distillation. To this end, we propose a framework that selects\nrationales based on their alignment with expert clinical reasoning. Experiments\nshow that our quality-focused approach significantly enhances SLM performance\nin both mental disorder detection and rationale generation. This work\nhighlights the importance of rationale quality and offers an insightful\nframework for knowledge transfer in mental health applications."}
{"id": "2505.18424", "pdf": "https://arxiv.org/pdf/2505.18424", "abs": "https://arxiv.org/abs/2505.18424", "authors": ["Tianyi Ren", "Juampablo E. Heras Rivera", "Hitender Oswal", "Yutong Pan", "William Henry", "Jacob Ruzevick", "Mehmet Kurt"], "title": "How We Won the ISLES'24 Challenge by Preprocessing", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Stroke is among the top three causes of death worldwide, and accurate\nidentification of stroke lesion boundaries is critical for diagnosis and\ntreatment. Supervised deep learning methods have emerged as the leading\nsolution for stroke lesion segmentation but require large, diverse, and\nannotated datasets. The ISLES'24 challenge addresses this need by providing\nlongitudinal stroke imaging data, including CT scans taken on arrival to the\nhospital and follow-up MRI taken 2-9 days from initial arrival, with\nannotations derived from follow-up MRI. Importantly, models submitted to the\nISLES'24 challenge are evaluated using only CT inputs, requiring prediction of\nlesion progression that may not be visible in CT scans for segmentation. Our\nwinning solution shows that a carefully designed preprocessing pipeline\nincluding deep-learning-based skull stripping and custom intensity windowing is\nbeneficial for accurate segmentation. Combined with a standard large residual\nnnU-Net architecture for segmentation, this approach achieves a mean test Dice\nof 28.5 with a standard deviation of 21.27."}
{"id": "2505.19619", "pdf": "https://arxiv.org/pdf/2505.19619", "abs": "https://arxiv.org/abs/2505.19619", "authors": ["Janik Kreit", "Dominic Schuh", "Kim A. Nicoli", "Lena Funcke"], "title": "SESaMo: Symmetry-Enforcing Stochastic Modulation for Normalizing Flows", "categories": ["cs.LG", "physics.comp-ph"], "comment": "14 figures", "summary": "Deep generative models have recently garnered significant attention across\nvarious fields, from physics to chemistry, where sampling from unnormalized\nBoltzmann-like distributions represents a fundamental challenge. In particular,\nautoregressive models and normalizing flows have become prominent due to their\nappealing ability to yield closed-form probability densities. Moreover, it is\nwell-established that incorporating prior knowledge - such as symmetries - into\ndeep neural networks can substantially improve training performances. In this\ncontext, recent advances have focused on developing symmetry-equivariant\ngenerative models, achieving remarkable results. Building upon these\nfoundations, this paper introduces Symmetry-Enforcing Stochastic Modulation\n(SESaMo). Similar to equivariant normalizing flows, SESaMo enables the\nincorporation of inductive biases (e.g., symmetries) into normalizing flows\nthrough a novel technique called stochastic modulation. This approach enhances\nthe flexibility of the generative model, allowing to effectively learn a\nvariety of exact and broken symmetries. Our numerical experiments benchmark\nSESaMo in different scenarios, including an 8-Gaussian mixture model and\nphysically relevant field theories, such as the $\\phi^4$ theory and the Hubbard\nmodel."}
{"id": "2505.20015", "pdf": "https://arxiv.org/pdf/2505.20015", "abs": "https://arxiv.org/abs/2505.20015", "authors": ["Ramon Ferrer-i-Cancho"], "title": "On the class of coding optimality of human languages and the origins of Zipf's law", "categories": ["cs.CL", "physics.soc-ph"], "comment": null, "summary": "Here we present a new class of optimality for coding systems. Members of that\nclass are separated linearly from optimal coding and thus exhibit Zipf's law,\nnamely a power-law distribution of frequency ranks. Whithin that class, Zipf's\nlaw, the size-rank law and the size-probability law form a group-like\nstructure. We identify human languages that are members of the class. All\nlanguages showing sufficient agreement with Zipf's law are potential members of\nthe class. In contrast, there are communication systems in other species that\ncannot be members of that class for exhibiting an exponential distribution\ninstead but dolphins and humpback whales might. We provide a new insight into\nplots of frequency versus rank in double logarithmic scale. For any system, a\nstraight line in that scale indicates that the lengths of optimal codes under\nnon-singular coding and under uniquely decodable encoding are separated by a\nlinear function whose slope is the exponent of Zipf's law. For systems under\ncompression and constrained to be uniquely decodable, such a straight line may\nindicate that the system is coding close to optimality. Our findings provide\nsupport for the hypothesis that Zipf's law originates from compression."}
{"id": "2505.18426", "pdf": "https://arxiv.org/pdf/2505.18426", "abs": "https://arxiv.org/abs/2505.18426", "authors": ["Khandakar Ashrafi Akbar", "Md Nahiyan Uddin", "Latifur Khan", "Trayce Hockstad", "Mizanur Rahman", "Mashrur Chowdhury", "Bhavani Thuraisingham"], "title": "Retrieval Augmented Generation-based Large Language Models for Bridging Transportation Cybersecurity Legal Knowledge Gaps", "categories": ["cs.CL", "cs.AI"], "comment": "Presented at the Transportation Research Board (TRB) Annual Meeting\n  2025, and subsequently submitted for publication consideration in the\n  Transportation Research Record (TRR)", "summary": "As connected and automated transportation systems evolve, there is a growing\nneed for federal and state authorities to revise existing laws and develop new\nstatutes to address emerging cybersecurity and data privacy challenges. This\nstudy introduces a Retrieval-Augmented Generation (RAG) based Large Language\nModel (LLM) framework designed to support policymakers by extracting relevant\nlegal content and generating accurate, inquiry-specific responses. The\nframework focuses on reducing hallucinations in LLMs by using a curated set of\ndomain-specific questions to guide response generation. By incorporating\nretrieval mechanisms, the system enhances the factual grounding and specificity\nof its outputs. Our analysis shows that the proposed RAG-based LLM outperforms\nleading commercial LLMs across four evaluation metrics: AlignScore, ParaScore,\nBERTScore, and ROUGE, demonstrating its effectiveness in producing reliable and\ncontext-aware legal insights. This approach offers a scalable, AI-driven method\nfor legislative analysis, supporting efforts to update legal frameworks in line\nwith advancements in transportation technologies."}
{"id": "2505.19620", "pdf": "https://arxiv.org/pdf/2505.19620", "abs": "https://arxiv.org/abs/2505.19620", "authors": ["Jiawen Chen", "Qi Shao", "Duxin Chen", "Wenwu Yu"], "title": "Decoupling Spatio-Temporal Prediction: When Lightweight Large Models Meet Adaptive Hypergraphs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Spatio-temporal prediction is a pivotal task with broad applications in\ntraffic management, climate monitoring, energy scheduling, etc. However,\nexisting methodologies often struggle to balance model expressiveness and\ncomputational efficiency, especially when scaling to large real-world datasets.\nTo tackle these challenges, we propose STH-SepNet (Spatio-Temporal Hypergraph\nSeparation Networks), a novel framework that decouples temporal and spatial\nmodeling to enhance both efficiency and precision. Therein, the temporal\ndimension is modeled using lightweight large language models, which effectively\ncapture low-rank temporal dynamics. Concurrently, the spatial dimension is\naddressed through an adaptive hypergraph neural network, which dynamically\nconstructs hyperedges to model intricate, higher-order interactions. A\ncarefully designed gating mechanism is integrated to seamlessly fuse temporal\nand spatial representations. By leveraging the fundamental principles of\nlow-rank temporal dynamics and spatial interactions, STH-SepNet offers a\npragmatic and scalable solution for spatio-temporal prediction in real-world\napplications. Extensive experiments on large-scale real-world datasets across\nmultiple benchmarks demonstrate the effectiveness of STH-SepNet in boosting\npredictive performance while maintaining computational efficiency. This work\nmay provide a promising lightweight framework for spatio-temporal prediction,\naiming to reduce computational demands and while enhancing predictive\nperformance. Our code is avaliable at\nhttps://github.com/SEU-WENJIA/ST-SepNet-Lightweight-LLMs-Meet-Adaptive-Hypergraphs."}
{"id": "2505.20016", "pdf": "https://arxiv.org/pdf/2505.20016", "abs": "https://arxiv.org/abs/2505.20016", "authors": ["Chengrui Huang", "Shen Gao", "Zhengliang Shi", "Dongsheng Wang", "Shuo Shang"], "title": "TTPA: Token-level Tool-use Preference Alignment Training Framework with Fine-grained Evaluation", "categories": ["cs.CL"], "comment": "16 pages, 5 figures", "summary": "Existing tool-learning methods usually rely on supervised fine-tuning, they\noften overlook fine-grained optimization of internal tool call details, leading\nto limitations in preference alignment and error discrimination. To overcome\nthese challenges, we propose Token-level Tool-use Preference Alignment Training\nFramework (TTPA), a training paradigm for constructing token-level tool-use\npreference datasets that align LLMs with fine-grained preferences using a novel\nerror-oriented scoring mechanism. TTPA first introduces reversed dataset\nconstruction, a method for creating high-quality, multi-turn tool-use datasets\nby reversing the generation flow. Additionally, we propose Token-level\nPreference Sampling (TPS) to capture fine-grained preferences by modeling\ntoken-level differences during generation. To address biases in scoring, we\nintroduce the Error-oriented Scoring Mechanism (ESM), which quantifies\ntool-call errors and can be used as a training signal. Extensive experiments on\nthree diverse benchmark datasets demonstrate that TTPA significantly improves\ntool-using performance while showing strong generalization ability across\nmodels and datasets."}
{"id": "2505.18434", "pdf": "https://arxiv.org/pdf/2505.18434", "abs": "https://arxiv.org/abs/2505.18434", "authors": ["Yuliang Cai", "Jesse Thomason", "Mohammad Rostami"], "title": "TNG-CLIP:Training-Time Negation Data Generation for Negation Awareness of CLIP", "categories": ["cs.CV", "cs.AI"], "comment": "15 pages, 3 figures", "summary": "Vision-language models (VLMs), such as CLIP, have demonstrated strong\nperformance across a range of downstream tasks. However, CLIP is still limited\nin negation understanding: the ability to recognize the absence or exclusion of\na concept. Existing methods address the problem by using a large language model\n(LLM) to generate large-scale data of image captions containing negation for\nfurther fine-tuning CLIP. However, these methods are both time- and\ncompute-intensive, and their evaluations are typically restricted to image-text\nmatching tasks. To expand the horizon, we (1) introduce a training-time\nnegation data generation pipeline such that negation captions are generated\nduring the training stage, which only increases 2.5% extra training time, and\n(2) we propose the first benchmark, Neg-TtoI, for evaluating text-to-image\ngeneration models on prompts containing negation, assessing model's ability to\nproduce semantically accurate images. We show that our proposed method,\nTNG-CLIP, achieves SOTA performance on diverse negation benchmarks of\nimage-to-text matching, text-to-image retrieval, and image generation."}
{"id": "2505.19635", "pdf": "https://arxiv.org/pdf/2505.19635", "abs": "https://arxiv.org/abs/2505.19635", "authors": ["Ivan Y. Tyukin", "Bogdan Grechuk", "Evgeny M. Mirkes", "Alexander N. Gorban"], "title": "When fractional quasi p-norms concentrate", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH", "68T09, 62R07, 94A16"], "comment": null, "summary": "Concentration of distances in high dimension is an important factor for the\ndevelopment and design of stable and reliable data analysis algorithms. In this\npaper, we address the fundamental long-standing question about the\nconcentration of distances in high dimension for fractional quasi $p$-norms,\n$p\\in(0,1)$. The topic has been at the centre of various theoretical and\nempirical controversies. Here we, for the first time, identify conditions when\nfractional quasi $p$-norms concentrate and when they don't. We show that\ncontrary to some earlier suggestions, for broad classes of distributions,\nfractional quasi $p$-norms admit exponential and uniform in $p$ concentration\nbounds. For these distributions, the results effectively rule out previously\nproposed approaches to alleviate concentration by \"optimal\" setting the values\nof $p$ in $(0,1)$. At the same time, we specify conditions and the\ncorresponding families of distributions for which one can still control\nconcentration rates by appropriate choices of $p$. We also show that in an\narbitrarily small vicinity of a distribution from a large class of\ndistributions for which uniform concentration occurs, there are uncountably\nmany other distributions featuring anti-concentration properties. Importantly,\nthis behavior enables devising relevant data encoding or representation schemes\nfavouring or discouraging distance concentration. The results shed new light on\nthis long-standing problem and resolve the tension around the topic in both\ntheory and empirical evidence reported in the literature."}
{"id": "2505.20023", "pdf": "https://arxiv.org/pdf/2505.20023", "abs": "https://arxiv.org/abs/2505.20023", "authors": ["Yihan Chen", "Benfeng Xu", "Xiaorui Wang", "Yongdong Zhang", "Zhendong Mao"], "title": "Training LLM-Based Agents with Synthetic Self-Reflected Trajectories and Partial Masking", "categories": ["cs.CL"], "comment": null, "summary": "Autonomous agents, which perceive environments and take actions to achieve\ngoals, have become increasingly feasible with the advancements in large\nlanguage models (LLMs). However, current powerful agents often depend on\nsophisticated prompt engineering combined with closed-source LLMs like GPT-4.\nAlthough training open-source LLMs using expert trajectories from teacher\nmodels has yielded some improvements in agent capabilities, this approach still\nfaces limitations such as performance plateauing and error propagation. To\nmitigate these challenges, we propose STeP, a novel method for improving\nLLM-based agent training. We synthesize self-reflected trajectories that\ninclude reflections and corrections of error steps, which enhance the\neffectiveness of LLM agents in learning from teacher models, enabling them to\nbecome agents capable of self-reflecting and correcting. We also introduce\npartial masking strategy that prevents the LLM from internalizing incorrect or\nsuboptimal steps. Experiments demonstrate that our method improves agent\nperformance across three representative tasks: ALFWorld, WebShop, and SciWorld.\nFor the open-source model LLaMA2-7B-Chat, when trained using self-reflected\ntrajectories constructed with Qwen1.5-110B-Chat as the teacher model, it\nachieves comprehensive improvements with less training data compared to agents\ntrained exclusively on expert trajectories."}
{"id": "2505.18440", "pdf": "https://arxiv.org/pdf/2505.18440", "abs": "https://arxiv.org/abs/2505.18440", "authors": ["Zhaoyang Wang", "Jinqi Jiang", "Tian Qiu", "Hui Liu", "Xianfeng Tang", "Huaxiu Yao"], "title": "Efficient Long CoT Reasoning in Small Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent large reasoning models such as DeepSeek-R1 exhibit strong complex\nproblems solving abilities by generating long chain-of-thought (CoT) reasoning\nsteps. It is challenging to directly train small language models (SLMs) to\nemerge long CoT. Thus, distillation becomes a practical method to enable SLMs\nfor such reasoning ability. However, the long CoT often contains a lot of\nredundant contents (e.g., overthinking steps) which may make SLMs hard to learn\nconsidering their relatively poor capacity and generalization. To address this\nissue, we propose a simple-yet-effective method to prune unnecessary steps in\nlong CoT, and then employ an on-policy method for the SLM itself to curate\nvalid and useful long CoT training data. In this way, SLMs can effectively\nlearn efficient long CoT reasoning and preserve competitive performance at the\nsame time. Experimental results across a series of mathematical reasoning\nbenchmarks demonstrate the effectiveness of the proposed method in distilling\nlong CoT reasoning ability into SLMs which maintains the competitive\nperformance but significantly reduces generating redundant reasoning steps."}
{"id": "2505.19645", "pdf": "https://arxiv.org/pdf/2505.19645", "abs": "https://arxiv.org/abs/2505.19645", "authors": ["Zongle Huang", "Lei Zhu", "Zongyuan Zhan", "Ting Hu", "Weikai Mao", "Xianzhi Yu", "Yongpan Liu", "Tianyu Zhang"], "title": "MoESD: Unveil Speculative Decoding's Potential for Accelerating Sparse MoE", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success across many\napplications, with Mixture of Experts (MoE) models demonstrating great\npotential. Compared to traditional dense models, MoEs achieve better\nperformance with less computation. Speculative decoding (SD) is a widely used\ntechnique to accelerate LLM inference without accuracy loss, but it has been\nconsidered efficient only for dense models. In this work, we first demonstrate\nthat, under medium batch sizes, MoE surprisingly benefits more from SD than\ndense models. Furthermore, as MoE becomes sparser -- the prevailing trend in\nMoE designs -- the batch size range where SD acceleration is expected to be\neffective becomes broader. To quantitatively understand tradeoffs involved in\nSD, we develop a reliable modeling based on theoretical analyses. While current\nSD research primarily focuses on improving acceptance rates of algorithms,\nchanges in workload and model architecture can still lead to degraded SD\nacceleration even with high acceptance rates. To address this limitation, we\nintroduce a new metric 'target efficiency' that characterizes these effects,\nthus helping researchers identify system bottlenecks and understand SD\nacceleration more comprehensively. For scenarios like private serving, this\nwork unveils a new perspective to speed up MoE inference, where existing\nsolutions struggle. Experiments on different GPUs show up to 2.29x speedup for\nQwen2-57B-A14B at medium batch sizes and validate our theoretical predictions."}
{"id": "2505.20045", "pdf": "https://arxiv.org/pdf/2505.20045", "abs": "https://arxiv.org/abs/2505.20045", "authors": ["Artem Vazhentsev", "Lyudmila Rvanova", "Gleb Kuzmin", "Ekaterina Fadeeva", "Ivan Lazichny", "Alexander Panchenko", "Maxim Panov", "Timothy Baldwin", "Mrinmaya Sachan", "Preslav Nakov", "Artem Shelmanov"], "title": "Uncertainty-Aware Attention Heads: Efficient Unsupervised Uncertainty Quantification for LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit impressive fluency, but often produce\ncritical errors known as \"hallucinations\". Uncertainty quantification (UQ)\nmethods are a promising tool for coping with this fundamental shortcoming. Yet,\nexisting UQ methods face challenges such as high computational overhead or\nreliance on supervised learning. Here, we aim to bridge this gap. In\nparticular, we propose RAUQ (Recurrent Attention-based Uncertainty\nQuantification), an unsupervised approach that leverages intrinsic attention\npatterns in transformers to detect hallucinations efficiently. By analyzing\nattention weights, we identified a peculiar pattern: drops in attention to\npreceding tokens are systematically observed during incorrect generations for\ncertain \"uncertainty-aware\" heads. RAUQ automatically selects such heads,\nrecurrently aggregates their attention weights and token-level confidences, and\ncomputes sequence-level uncertainty scores in a single forward pass.\nExperiments across 4 LLMs and 12 question answering, summarization, and\ntranslation tasks demonstrate that RAUQ yields excellent results, outperforming\nstate-of-the-art UQ methods using minimal computational overhead (<1% latency).\nMoreover, it requires no task-specific labels and no careful hyperparameter\ntuning, offering plug-and-play real-time hallucination detection in white-box\nLLMs."}
{"id": "2505.18442", "pdf": "https://arxiv.org/pdf/2505.18442", "abs": "https://arxiv.org/abs/2505.18442", "authors": ["Zhining Liu", "Ze Yang", "Xiao Lin", "Ruizhong Qiu", "Tianxin Wei", "Yada Zhu", "Hendrik Hamann", "Jingrui He", "Hanghang Tong"], "title": "Breaking Silos: Adaptive Model Fusion Unlocks Better Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by ICML 2025. 22 pages, 6 Figures, 12 tables", "summary": "Time-series forecasting plays a critical role in many real-world\napplications. Although increasingly powerful models have been developed and\nachieved superior results on benchmark datasets, through a fine-grained\nsample-level inspection, we find that (i) no single model consistently\noutperforms others across different test samples, but instead (ii) each model\nexcels in specific cases. These findings prompt us to explore how to adaptively\nleverage the distinct strengths of various forecasting models for different\nsamples. We introduce TimeFuse, a framework for collective time-series\nforecasting with sample-level adaptive fusion of heterogeneous models. TimeFuse\nutilizes meta-features to characterize input time series and trains a learnable\nfusor to predict optimal model fusion weights for any given input. The fusor\ncan leverage samples from diverse datasets for joint training, allowing it to\nadapt to a wide variety of temporal patterns and thus generalize to new inputs,\neven from unseen datasets. Extensive experiments demonstrate the effectiveness\nof TimeFuse in various long-/short-term forecasting tasks, achieving\nnear-universal improvement over the state-of-the-art individual models. Code is\navailable at https://github.com/ZhiningLiu1998/TimeFuse."}
{"id": "2505.19646", "pdf": "https://arxiv.org/pdf/2505.19646", "abs": "https://arxiv.org/abs/2505.19646", "authors": ["Dongyeop Woo", "Minsu Kim", "Minkyu Kim", "Kiyoung Seong", "Sungsoo Ahn"], "title": "Energy-based generator matching: A neural sampler for general state space", "categories": ["cs.LG"], "comment": null, "summary": "We propose Energy-based generator matching (EGM), a modality-agnostic\napproach to train generative models from energy functions in the absence of\ndata. Extending the recently proposed generator matching, EGM enables training\nof arbitrary continuous-time Markov processes, e.g., diffusion, flow, and jump,\nand can generate data from continuous, discrete, and a mixture of two\nmodalities. To this end, we propose estimating the generator matching loss\nusing self-normalized importance sampling with an additional bootstrapping\ntrick to reduce variance in the importance weight. We validate EGM on both\ndiscrete and multimodal tasks up to 100 and 20 dimensions, respectively."}
{"id": "2505.20047", "pdf": "https://arxiv.org/pdf/2505.20047", "abs": "https://arxiv.org/abs/2505.20047", "authors": ["Debargha Ganguly", "Vikash Singh", "Sreehari Sankar", "Biyao Zhang", "Xuecen Zhang", "Srinivasan Iyengar", "Xiaotian Han", "Amit Sharma", "Shivkumar Kalyanaraman", "Vipin Chaudhary"], "title": "Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks", "categories": ["cs.CL", "cs.AI", "cs.LO", "cs.SE"], "comment": null, "summary": "Large language models (LLMs) show remarkable promise for democratizing\nautomated reasoning by generating formal specifications. However, a fundamental\ntension exists: LLMs are probabilistic, while formal verification demands\ndeterministic guarantees. This paper addresses this epistemological gap by\ncomprehensively investigating failure modes and uncertainty quantification (UQ)\nin LLM-generated formal artifacts. Our systematic evaluation of five frontier\nLLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's\ndomain-specific impact on accuracy (from +34.8% on logical tasks to -44.5% on\nfactual ones), with known UQ techniques like the entropy of token probabilities\nfailing to identify these errors. We introduce a probabilistic context-free\ngrammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty\ntaxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy\nfor logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables\nselective verification, drastically reducing errors (14-100%) with minimal\nabstention, transforming LLM-driven formalization into a reliable engineering\ndiscipline."}
{"id": "2505.18446", "pdf": "https://arxiv.org/pdf/2505.18446", "abs": "https://arxiv.org/abs/2505.18446", "authors": ["Hojun Son", "Asma Almutairi", "Arpan Kusari"], "title": "Mitigating Context Bias in Domain Adaptation for Object Detection using Mask Pooling", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Context bias refers to the association between the foreground objects and\nbackground during the object detection training process. Various methods have\nbeen proposed to minimize the context bias when applying the trained model to\nan unseen domain, known as domain adaptation for object detection (DAOD). But a\nprincipled approach to understand why the context bias occurs and how to remove\nit has been missing.\n  In this work, we provide a causal view of the context bias, pointing towards\nthe pooling operation in the convolution network architecture as the possible\nsource of this bias. We present an alternative, Mask Pooling, which uses an\nadditional input of foreground masks, to separate the pooling process in the\nrespective foreground and background regions and show that this process leads\nthe trained model to detect objects in a more robust manner under different\ndomains. We also provide a benchmark designed to create an ultimate test for\nDAOD, using foregrounds in the presence of absolute random backgrounds, to\nanalyze the robustness of the intended trained models. Through these\nexperiments, we hope to provide a principled approach for minimizing context\nbias under domain shift."}
{"id": "2505.19669", "pdf": "https://arxiv.org/pdf/2505.19669", "abs": "https://arxiv.org/abs/2505.19669", "authors": ["Haiyang Sun", "Shujie Hu", "Shujie Liu", "Lingwei Meng", "Hui Wang", "Bing Han", "Yifan Yang", "Yanqing Liu", "Sheng Zhao", "Yan Lu", "Yanmin Qian"], "title": "Zero-Shot Streaming Text to Speech Synthesis with Transducer and Auto-Regressive Modeling", "categories": ["cs.LG"], "comment": null, "summary": "Zero-shot streaming text-to-speech is an important research topic in\nhuman-computer interaction. Existing methods primarily use a lookahead\nmechanism, relying on future text to achieve natural streaming speech\nsynthesis, which introduces high processing latency. To address this issue, we\npropose SMLLE, a streaming framework for generating high-quality speech\nframe-by-frame. SMLLE employs a Transducer to convert text into semantic tokens\nin real time while simultaneously obtaining duration alignment information. The\ncombined outputs are then fed into a fully autoregressive (AR) streaming model\nto reconstruct mel-spectrograms. To further stabilize the generation process,\nwe design a Delete < Bos > Mechanism that allows the AR model to access future\ntext introducing as minimal delay as possible. Experimental results suggest\nthat the SMLLE outperforms current streaming TTS methods and achieves\ncomparable performance over sentence-level TTS systems. Samples are available\non https://anonymous.4open.science/w/demo_page-48B7/."}
{"id": "2505.20072", "pdf": "https://arxiv.org/pdf/2505.20072", "abs": "https://arxiv.org/abs/2505.20072", "authors": ["Yige Yuan", "Teng Xiao", "Shuchang Tao", "Xue Wang", "Jinyang Gao", "Bolin Ding", "Bingbing Xu"], "title": "Incentivizing Reasoning from Weak Supervision", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance on\nreasoning-intensive tasks, but enhancing their reasoning abilities typically\nrelies on either reinforcement learning (RL) with verifiable signals or\nsupervised fine-tuning (SFT) with high-quality long chain-of-thought (CoT)\ndemonstrations, both of which are expensive. In this paper, we study a novel\nproblem of incentivizing the reasoning capacity of LLMs without expensive\nhigh-quality demonstrations and reinforcement learning. We investigate whether\nthe reasoning capabilities of LLMs can be effectively incentivized via\nsupervision from significantly weaker models. We further analyze when and why\nsuch weak supervision succeeds in eliciting reasoning abilities in stronger\nmodels. Our findings show that supervision from significantly weaker reasoners\ncan substantially improve student reasoning performance, recovering close to\n94% of the gains of expensive RL at a fraction of the cost. Experiments across\ndiverse benchmarks and model architectures demonstrate that weak reasoners can\neffectively incentivize reasoning in stronger student models, consistently\nimproving performance across a wide range of reasoning tasks. Our results\nsuggest that this simple weak-to-strong paradigm is a promising and\ngeneralizable alternative to costly methods for incentivizing strong reasoning\ncapabilities at inference-time in LLMs. The code is publicly available at\nhttps://github.com/yuanyige/W2SR."}
{"id": "2505.18451", "pdf": "https://arxiv.org/pdf/2505.18451", "abs": "https://arxiv.org/abs/2505.18451", "authors": ["Toshiaki Koike-Akino", "Jing Liu", "Ye Wang"], "title": "$$-MoE: Test-Time Pruning as Micro-Grained Mixture-of-Experts", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "10 pages, 4 figures", "summary": "To tackle the huge computational demand of large foundation models,\nactivation-aware compression techniques without retraining have been\nintroduced. However, since these rely on calibration data, domain shift may\narise for unknown downstream tasks. With a computationally efficient\ncalibration, activation-aware pruning can be executed for every prompt\nadaptively, yet achieving reduced complexity at inference. We formulate it as a\nmixture of micro-experts, called $\\mu$-MoE. Several experiments demonstrate\nthat $\\mu$-MoE can dynamically adapt to task/prompt-dependent structured\nsparsity on the fly."}
{"id": "2505.19680", "pdf": "https://arxiv.org/pdf/2505.19680", "abs": "https://arxiv.org/abs/2505.19680", "authors": ["Xinrui Wang", "Shao-yuan Li", "Jiaqiang Zhang", "Songcan Chen"], "title": "Cut out and Replay: A Simple yet Versatile Strategy for Multi-Label Online Continual Learning", "categories": ["cs.LG"], "comment": "accepted by ICML 2025", "summary": "Multi-Label Online Continual Learning (MOCL) requires models to learn\ncontinuously from endless multi-label data streams, facing complex challenges\nincluding persistent catastrophic forgetting, potential missing labels, and\nuncontrollable imbalanced class distributions. While existing MOCL methods\nattempt to address these challenges through various techniques, \\textit{they\nall overlook label-specific region identifying and feature learning} - a\nfundamental solution rooted in multi-label learning but challenging to achieve\nin the online setting with incremental and partial supervision. To this end, we\nfirst leverage the inherent structural information of input data to evaluate\nand verify the innate localization capability of different pre-trained models.\nThen, we propose CUTER (CUT-out-and-Experience-Replay), a simple yet versatile\nstrategy that provides fine-grained supervision signals by further identifying,\nstrengthening and cutting out label-specific regions for efficient experience\nreplay. It not only enables models to simultaneously address catastrophic\nforgetting, missing labels, and class imbalance challenges, but also serves as\nan orthogonal solution that seamlessly integrates with existing approaches.\nExtensive experiments on multiple multi-label image benchmarks demonstrate the\nsuperiority of our proposed method. The code is available at\n\\href{https://github.com/wxr99/Cut-Replay}{https://github.com/wxr99/Cut-Replay}"}
{"id": "2505.20081", "pdf": "https://arxiv.org/pdf/2505.20081", "abs": "https://arxiv.org/abs/2505.20081", "authors": ["Yige Yuan", "Teng Xiao", "Li Yunfan", "Bingbing Xu", "Shuchang Tao", "Yunqi Qiu", "Huawei Shen", "Xueqi Cheng"], "title": "Inference-time Alignment in Continuous Space", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Aligning large language models with human feedback at inference time has\nreceived increasing attention due to its flexibility. Existing methods rely on\ngenerating multiple responses from the base policy for search using a reward\nmodel, which can be considered as searching in a discrete response space.\nHowever, these methods struggle to explore informative candidates when the base\npolicy is weak or the candidate set is small, resulting in limited\neffectiveness. In this paper, to address this problem, we propose Simple Energy\nAdaptation ($\\textbf{SEA}$), a simple yet effective algorithm for\ninference-time alignment. In contrast to expensive search over the discrete\nspace, SEA directly adapts original responses from the base policy toward the\noptimal one via gradient-based sampling in continuous latent space.\nSpecifically, SEA formulates inference as an iterative optimization procedure\non an energy function over actions in the continuous space defined by the\noptimal policy, enabling simple and effective alignment. For instance, despite\nits simplicity, SEA outperforms the second-best baseline with a relative\nimprovement of up to $ \\textbf{77.51%}$ on AdvBench and $\\textbf{16.36%}$ on\nMATH. Our code is publicly available at https://github.com/yuanyige/SEA"}
{"id": "2505.18453", "pdf": "https://arxiv.org/pdf/2505.18453", "abs": "https://arxiv.org/abs/2505.18453", "authors": ["Zhichao Wu", "Yueteng Kang", "Songjun Cao", "Long Ma", "Qiulin Li", "Qun Yang"], "title": "MPE-TTS: Customized Emotion Zero-Shot Text-To-Speech Using Multi-Modal Prompt", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Accepted by InterSpeech", "summary": "Most existing Zero-Shot Text-To-Speech(ZS-TTS) systems generate the unseen\nspeech based on single prompt, such as reference speech or text descriptions,\nwhich limits their flexibility. We propose a customized emotion ZS-TTS system\nbased on multi-modal prompt. The system disentangles speech into the content,\ntimbre, emotion and prosody, allowing emotion prompts to be provided as text,\nimage or speech. To extract emotion information from different prompts, we\npropose a multi-modal prompt emotion encoder. Additionally, we introduce an\nprosody predictor to fit the distribution of prosody and propose an emotion\nconsistency loss to preserve emotion information in the predicted prosody. A\ndiffusion-based acoustic model is employed to generate the target\nmel-spectrogram. Both objective and subjective experiments demonstrate that our\nsystem outperforms existing systems in terms of naturalness and similarity. The\nsamples are available at https://mpetts-demo.github.io/mpetts_demo/."}
{"id": "2505.19682", "pdf": "https://arxiv.org/pdf/2505.19682", "abs": "https://arxiv.org/abs/2505.19682", "authors": ["Bahareh Tasdighi", "Manuel Haussmann", "Yi-Shan Wu", "Andres R. Masegosa", "Melih Kandemir"], "title": "Deep Actor-Critics with Tight Risk Certificates", "categories": ["cs.LG"], "comment": null, "summary": "After a period of research, deep actor-critic algorithms have reached a level\nwhere they influence our everyday lives. They serve as the driving force behind\nthe continual improvement of large language models through user-collected\nfeedback. However, their deployment in physical systems is not yet widely\nadopted, mainly because no validation scheme that quantifies their risk of\nmalfunction. We demonstrate that it is possible to develop tight risk\ncertificates for deep actor-critic algorithms that predict generalization\nperformance from validation-time observations. Our key insight centers on the\neffectiveness of minimal evaluation data. Surprisingly, a small feasible of\nevaluation roll-outs collected from a pretrained policy suffices to produce\naccurate risk certificates when combined with a simple adaptation of PAC-Bayes\ntheory. Specifically, we adopt a recently introduced recursive PAC-Bayes\napproach, which splits validation data into portions and recursively builds\nPAC-Bayes bounds on the excess loss of each portion's predictor, using the\npredictor from the previous portion as a data-informed prior. Our empirical\nresults across multiple locomotion tasks and policy expertise levels\ndemonstrate risk certificates that are tight enough to be considered for\npractical use."}
{"id": "2505.20088", "pdf": "https://arxiv.org/pdf/2505.20088", "abs": "https://arxiv.org/abs/2505.20088", "authors": ["Nitay Calderon", "Liat Ein-Dor", "Roi Reichart"], "title": "Multi-Domain Explainability of Preferences", "categories": ["cs.CL"], "comment": null, "summary": "Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and\nreward models, are central to aligning and evaluating large language models\n(LLMs). Yet, the underlying concepts that drive these preferences remain poorly\nunderstood. In this work, we propose a fully automated end-to-end method for\ngenerating local and global concept-based explanations of preferences across\nmultiple domains. Our method employs an LLM to discover concepts that\ndifferentiate between chosen and rejected responses and represent them with\nconcept-based vectors. To model the relationships between concepts and\npreferences, we propose a white-box Hierarchical Multi-Domain Regression model\nthat captures both domain-general and domain-specific effects. To evaluate our\nmethod, we curate a dataset spanning eight challenging and diverse domains and\nexplain twelve mechanisms. Our method achieves strong preference prediction\nperformance, outperforming baselines while also being explainable.\nAdditionally, we assess explanations in two novel application-driven settings.\nFirst, guiding LLM outputs with concepts from LaaJ explanations yields\nresponses that those judges consistently prefer. Second, prompting LaaJs with\nconcepts explaining humans improves their preference predictions. Together, our\nwork provides a new paradigm for explainability in the era of LLMs."}
{"id": "2505.18458", "pdf": "https://arxiv.org/pdf/2505.18458", "abs": "https://arxiv.org/abs/2505.18458", "authors": ["Xuanhe Zhou", "Junxuan He", "Wei Zhou", "Haodong Chen", "Zirui Tang", "Haoyu Zhao", "Xin Tong", "Guoliang Li", "Youmin Chen", "Jun Zhou", "Zhaojun Sun", "Binyuan Hui", "Shuo Wang", "Conghui He", "Zhiyuan Liu", "Jingren Zhou", "Fan Wu"], "title": "A Survey of LLM $\\times$ DATA", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": "Please refer to the paper list at:\n  https://github.com/weAIDB/awsome-data-llm", "summary": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration."}
{"id": "2505.19685", "pdf": "https://arxiv.org/pdf/2505.19685", "abs": "https://arxiv.org/abs/2505.19685", "authors": ["Victor M. Tenorio", "Nicolas Zilberstein", "Santiago Segarra", "Antonio G. Marques"], "title": "Graph Guided Diffusion: Unified Guidance for Conditional Graph Generation", "categories": ["cs.LG"], "comment": null, "summary": "Diffusion models have emerged as powerful generative models for graph\ngeneration, yet their use for conditional graph generation remains a\nfundamental challenge. In particular, guiding diffusion models on graphs under\narbitrary reward signals is difficult: gradient-based methods, while powerful,\nare often unsuitable due to the discrete and combinatorial nature of graphs,\nand non-differentiable rewards further complicate gradient-based guidance. We\npropose Graph Guided Diffusion (GGDiff), a novel guidance framework that\ninterprets conditional diffusion on graphs as a stochastic control problem to\naddress this challenge. GGDiff unifies multiple guidance strategies, including\ngradient-based guidance (for differentiable rewards), control-based guidance\n(using control signals from forward reward evaluations), and zero-order\napproximations (bridging gradient-based and gradient-free optimization). This\ncomprehensive, plug-and-play framework enables zero-shot guidance of\npre-trained diffusion models under both differentiable and non-differentiable\nreward functions, adapting well-established guidance techniques to graph\ngeneration--a direction largely unexplored. Our formulation balances\ncomputational efficiency, reward alignment, and sample quality, enabling\npractical conditional generation across diverse reward types. We demonstrate\nthe efficacy of GGDiff in various tasks, including constraints on graph motifs,\nfairness, and link prediction, achieving superior alignment with target rewards\nwhile maintaining diversity and fidelity."}
{"id": "2505.20096", "pdf": "https://arxiv.org/pdf/2505.20096", "abs": "https://arxiv.org/abs/2505.20096", "authors": ["Thang Nguyen", "Peter Chin", "Yu-Wing Tai"], "title": "MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation\n(RAG) that addresses the inherent ambiguities and reasoning challenges in\ncomplex information-seeking tasks. Unlike conventional RAG methods that rely on\neither end-to-end fine-tuning or isolated component enhancements, MA-RAG\norchestrates a collaborative set of specialized AI agents: Planner, Step\nDefiner, Extractor, and QA Agents, to tackle each stage of the RAG pipeline\nwith task-aware reasoning. Ambiguities may arise from underspecified queries,\nsparse or indirect evidence in retrieved documents, or the need to integrate\ninformation scattered across multiple sources. MA-RAG mitigates these\nchallenges by decomposing the problem into subtasks, such as query\ndisambiguation, evidence extraction, and answer synthesis, and dispatching them\nto dedicated agents equipped with chain-of-thought prompting. These agents\ncommunicate intermediate reasoning and progressively refine the retrieval and\nsynthesis process. Our design allows fine-grained control over information flow\nwithout any model fine-tuning. Crucially, agents are invoked on demand,\nenabling a dynamic and efficient workflow that avoids unnecessary computation.\nThis modular and reasoning-driven architecture enables MA-RAG to deliver\nrobust, interpretable results. Experiments on multi-hop and ambiguous QA\nbenchmarks demonstrate that MA-RAG outperforms state-of-the-art training-free\nbaselines and rivals fine-tuned systems, validating the effectiveness of\ncollaborative agent-based reasoning in RAG."}
{"id": "2505.18461", "pdf": "https://arxiv.org/pdf/2505.18461", "abs": "https://arxiv.org/abs/2505.18461", "authors": ["Morteza Karimzadeh", "Zhongying Wang", "James L. Crooks"], "title": "Performance and Generalizability Impacts of Incorporating Geolocation into Deep Learning for Dynamic PM2.5 Estimation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Deep learning models have demonstrated success in geospatial applications,\nyet quantifying the role of geolocation information in enhancing model\nperformance and geographic generalizability remains underexplored. A new\ngeneration of location encoders have emerged with the goal of capturing\nattributes present at any given location for downstream use in predictive\nmodeling. Being a nascent area of research, their evaluation has remained\nlargely limited to static tasks such as species distributions or average\ntemperature mapping. In this paper, we discuss and quantify the impact of\nincorporating geolocation into deep learning for a real-world application\ndomain that is characteristically dynamic (with fast temporal change) and\nspatially heterogeneous at high resolutions: estimating surface-level daily\nPM2.5 levels using remotely sensed and ground-level data. We build on a\nrecently published deep learning-based PM2.5 estimation model that achieves\nstate-of-the-art performance on data observed in the contiguous United States.\nWe examine three approaches for incorporating geolocation: excluding\ngeolocation as a baseline, using raw geographic coordinates, and leveraging\npretrained location encoders. We evaluate each approach under within-region\n(WR) and out-of-region (OoR) evaluation scenarios. Aggregate performance\nmetrics indicate that while na\\\"ive incorporation of raw geographic coordinates\nimproves within-region performance by retaining the interpolative value of\ngeographic location, it can hinder generalizability across regions. In\ncontrast, pretrained location encoders like GeoCLIP enhance predictive\nperformance and geographic generalizability for both WR and OoR scenarios.\nHowever, qualitative analysis reveals artifact patterns caused by high-degree\nbasis functions and sparse upstream samples in certain areas, and ablation\nresults indicate varying performance among location encoders..."}
{"id": "2505.19698", "pdf": "https://arxiv.org/pdf/2505.19698", "abs": "https://arxiv.org/abs/2505.19698", "authors": ["Jing Yu Lim", "Zarif Ikram", "Samson Yu", "Haozhe Ma", "Tze-Yun Leong", "Dianbo Liu"], "title": "JEDI: Latent End-to-end Diffusion Mitigates Agent-Human Performance Asymmetry in Model-Based Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "Preprint", "summary": "Recent advances in model-based reinforcement learning (MBRL) have achieved\nsuper-human level performance on the Atari100k benchmark, driven by\nreinforcement learning agents trained on powerful diffusion world models.\nHowever, we identify that the current aggregates mask a major performance\nasymmetry: MBRL agents dramatically outperform humans in some tasks despite\ndrastically underperforming in others, with the former inflating the aggregate\nmetrics. This is especially pronounced in pixel-based agents trained with\ndiffusion world models. In this work, we address the pronounced asymmetry\nobserved in pixel-based agents as an initial attempt to reverse the worrying\nupward trend observed in them. We address the problematic aggregates by\ndelineating all tasks as Agent-Optimal or Human-Optimal and advocate for equal\nimportance on metrics from both sets. Next, we hypothesize this pronounced\nasymmetry is due to the lack of temporally-structured latent space trained with\nthe World Model objective in pixel-based methods. Lastly, to address this\nissue, we propose Joint Embedding DIffusion (JEDI), a novel latent diffusion\nworld model trained end-to-end with the self-consistency objective. JEDI\noutperforms SOTA models in human-optimal tasks while staying competitive across\nthe Atari100k benchmark, and runs 3 times faster with 43% lower memory than the\nlatest pixel-based diffusion baseline. Overall, our work rethinks what it truly\nmeans to cross human-level performance in Atari100k."}
{"id": "2505.20097", "pdf": "https://arxiv.org/pdf/2505.20097", "abs": "https://arxiv.org/abs/2505.20097", "authors": ["Liang Cheng", "Tianyi LI", "Zhaowei Wang", "Mark Steedman"], "title": "S2LPP: Small-to-Large Prompt Prediction across LLMs", "categories": ["cs.CL"], "comment": "15 pages", "summary": "The performance of pre-trained Large Language Models (LLMs) is often\nsensitive to nuances in prompt templates, requiring careful prompt engineering,\nadding costs in terms of computing and human effort. In this study, we present\nexperiments encompassing multiple LLMs variants of varying sizes aimed at\nprobing their preference with different prompts. Through experiments on\nQuestion Answering, we show prompt preference consistency across LLMs of\ndifferent sizes. We also show that this consistency extends to other tasks,\nsuch as Natural Language Inference. Utilizing this consistency, we propose a\nmethod to use a smaller model to select effective prompt templates for a larger\nmodel. We show that our method substantially reduces the cost of prompt\nengineering while consistently matching performance with optimal prompts among\ncandidates. More importantly, our experiment shows the efficacy of our strategy\nacross fourteen LLMs and its applicability to a broad range of NLP tasks,\nhighlighting its robustness"}
{"id": "2505.18464", "pdf": "https://arxiv.org/pdf/2505.18464", "abs": "https://arxiv.org/abs/2505.18464", "authors": ["Ugur Kursuncu", "Trilok Padhi", "Gaurav Sinha", "Abdulkadir Erol", "Jaya Krishna Mandivarapu", "Christopher R. Larrison"], "title": "From Reddit to Generative AI: Evaluating Large Language Models for Anxiety Support Fine-tuned on Social Media Data", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "The growing demand for accessible mental health support, compounded by\nworkforce shortages and logistical barriers, has led to increased interest in\nutilizing Large Language Models (LLMs) for scalable and real-time assistance.\nHowever, their use in sensitive domains such as anxiety support remains\nunderexamined. This study presents a systematic evaluation of LLMs (GPT and\nLlama) for their potential utility in anxiety support by using real\nuser-generated posts from the r/Anxiety subreddit for both prompting and\nfine-tuning. Our approach utilizes a mixed-method evaluation framework\nincorporating three main categories of criteria: (i) linguistic quality, (ii)\nsafety and trustworthiness, and (iii) supportiveness. Results show that\nfine-tuning LLMs with naturalistic anxiety-related data enhanced linguistic\nquality but increased toxicity and bias, and diminished emotional\nresponsiveness. While LLMs exhibited limited empathy, GPT was evaluated as more\nsupportive overall. Our findings highlight the risks of fine-tuning LLMs on\nunprocessed social media content without mitigation strategies."}
{"id": "2505.19699", "pdf": "https://arxiv.org/pdf/2505.19699", "abs": "https://arxiv.org/abs/2505.19699", "authors": ["Junming Liu", "Yanting Gao", "Siyuan Meng", "Yifei Sun", "Aoqi Wu", "Yufei Jin", "Yirong Chen", "Ding Wang", "Guosun Zeng"], "title": "Mosaic: Data-Free Knowledge Distillation via Mixture-of-Experts for Heterogeneous Distributed Environments", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "43 pages, 23 figures, 15 tables; the last dance", "summary": "Federated Learning (FL) is a decentralized machine learning paradigm that\nenables clients to collaboratively train models while preserving data privacy.\nHowever, the coexistence of model and data heterogeneity gives rise to\ninconsistent representations and divergent optimization dynamics across\nclients, ultimately hindering robust global performance. To transcend these\nchallenges, we propose Mosaic, a novel data-free knowledge distillation\nframework tailored for heterogeneous distributed environments. Mosaic first\ntrains local generative models to approximate each client's personalized\ndistribution, enabling synthetic data generation that safeguards privacy\nthrough strict separation from real data. Subsequently, Mosaic forms a\nMixture-of-Experts (MoE) from client models based on their specialized\nknowledge, and distills it into a global model using the generated data. To\nfurther enhance the MoE architecture, Mosaic integrates expert predictions via\na lightweight meta model trained on a few representative prototypes. Extensive\nexperiments on standard image classification benchmarks demonstrate that Mosaic\nconsistently outperforms state-of-the-art approaches under both model and data\nheterogeneity. The source code has been published at\nhttps://github.com/Wings-Of-Disaster/Mosaic."}
{"id": "2505.20099", "pdf": "https://arxiv.org/pdf/2505.20099", "abs": "https://arxiv.org/abs/2505.20099", "authors": ["Chuangtao Ma", "Yongrui Chen", "Tianxing Wu", "Arijit Khan", "Haofen Wang"], "title": "Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Under Review", "summary": "Large language models (LLMs) have demonstrated remarkable performance on\nquestion-answering (QA) tasks because of their superior capabilities in natural\nlanguage understanding and generation. However, LLM-based QA struggles with\ncomplex QA tasks due to poor reasoning capacity, outdated knowledge, and\nhallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs)\nfor QA to address the above challenges. In this survey, we propose a new\nstructured taxonomy that categorizes the methodology of synthesizing LLMs and\nKGs for QA according to the categories of QA and the KG's role when integrating\nwith LLMs. We systematically survey state-of-the-art advances in synthesizing\nLLMs and KGs for QA and compare and analyze these approaches in terms of\nstrength, limitations, and KG requirements. We then align the approaches with\nQA and discuss how these approaches address the main challenges of different\ncomplex QA. Finally, we summarize the advancements, evaluation metrics, and\nbenchmark datasets and highlight open challenges and opportunities."}
{"id": "2505.18471", "pdf": "https://arxiv.org/pdf/2505.18471", "abs": "https://arxiv.org/abs/2505.18471", "authors": ["Guoheng Sun", "Ziyao Wang", "Xuandong Zhao", "Bowei Tian", "Zheyu Shen", "Yexiao He", "Jinming Xing", "Ang Li"], "title": "Invisible Tokens, Visible Bills: The Urgent Need to Audit Hidden Operations in Opaque LLM Services", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Modern large language model (LLM) services increasingly rely on complex,\noften abstract operations, such as multi-step reasoning and multi-agent\ncollaboration, to generate high-quality outputs. While users are billed based\non token consumption and API usage, these internal steps are typically not\nvisible. We refer to such systems as Commercial Opaque LLM Services (COLS).\nThis position paper highlights emerging accountability challenges in COLS:\nusers are billed for operations they cannot observe, verify, or contest. We\nformalize two key risks: \\textit{quantity inflation}, where token and call\ncounts may be artificially inflated, and \\textit{quality downgrade}, where\nproviders might quietly substitute lower-cost models or tools. Addressing these\nrisks requires a diverse set of auditing strategies, including\ncommitment-based, predictive, behavioral, and signature-based methods. We\nfurther explore the potential of complementary mechanisms such as watermarking\nand trusted execution environments to enhance verifiability without\ncompromising provider confidentiality. We also propose a modular three-layer\nauditing framework for COLS and users that enables trustworthy verification\nacross execution, secure logging, and user-facing auditability without exposing\nproprietary internals. Our aim is to encourage further research and policy\ndevelopment toward transparency, auditability, and accountability in commercial\nLLM services."}
{"id": "2505.19712", "pdf": "https://arxiv.org/pdf/2505.19712", "abs": "https://arxiv.org/abs/2505.19712", "authors": ["Johannes Hertrich", "Antonin Chambolle", "Julie Delon"], "title": "On the Relation between Rectified Flows and Optimal Transport", "categories": ["cs.LG", "math.PR", "stat.ML"], "comment": null, "summary": "This paper investigates the connections between rectified flows, flow\nmatching, and optimal transport. Flow matching is a recent approach to learning\ngenerative models by estimating velocity fields that guide transformations from\na source to a target distribution. Rectified flow matching aims to straighten\nthe learned transport paths, yielding more direct flows between distributions.\nOur first contribution is a set of invariance properties of rectified flows and\nexplicit velocity fields. In addition, we also provide explicit constructions\nand analysis in the Gaussian (not necessarily independent) and Gaussian mixture\nsettings and study the relation to optimal transport. Our second contribution\naddresses recent claims suggesting that rectified flows, when constrained such\nthat the learned velocity field is a gradient, can yield (asymptotically)\nsolutions to optimal transport problems. We study the existence of solutions\nfor this problem and demonstrate that they only relate to optimal transport\nunder assumptions that are significantly stronger than those previously\nacknowledged. In particular, we present several counter-examples that\ninvalidate earlier equivalence results in the literature, and we argue that\nenforcing a gradient constraint on rectified flows is, in general, not a\nreliable method for computing optimal transport maps."}
{"id": "2505.20101", "pdf": "https://arxiv.org/pdf/2505.20101", "abs": "https://arxiv.org/abs/2505.20101", "authors": ["Yunhao Wang", "Yuhao Zhang", "Tinghao Yu", "Can Xu", "Feng Zhang", "Fengzong Lian"], "title": "Adaptive Deep Reasoning: Triggering Deep Thinking When Needed", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown impressive capabilities in handling\ncomplex tasks through long-chain reasoning. However, the extensive reasoning\nsteps involved can significantly increase computational costs, posing\nchallenges for real-world deployment. Recent efforts have focused on optimizing\nreasoning efficiency by shortening the Chain-of-Thought (CoT) reasoning\nprocesses through various approaches, such as length-aware prompt engineering,\nsupervised fine-tuning on CoT data with variable lengths, and reinforcement\nlearning with length penalties. Although these methods effectively reduce\nreasoning length, they still necessitate an initial reasoning phase. More\nrecent approaches have attempted to integrate long-chain and short-chain\nreasoning abilities into a single model, yet they still rely on manual control\nto toggle between short and long CoT.In this work, we propose a novel approach\nthat autonomously switches between short and long reasoning chains based on\nproblem complexity. Our method begins with supervised fine-tuning of the base\nmodel to equip both long-chain and short-chain reasoning abilities. We then\nemploy reinforcement learning to further balance short and long CoT generation\nwhile maintaining accuracy through two key strategies: first, integrating\nreinforcement learning with a long-short adaptive group-wise reward strategy to\nassess prompt complexity and provide corresponding rewards; second,\nimplementing a logit-based reasoning mode switching loss to optimize the\nmodel's initial token choice, thereby guiding the selection of the reasoning\ntype.Evaluations on mathematical datasets demonstrate that our model can\ndynamically switch between long-chain and short-chain reasoning modes without\nsubstantially sacrificing performance. This advancement enhances the\npracticality of reasoning in large language models for real-world applications."}
{"id": "2505.18475", "pdf": "https://arxiv.org/pdf/2505.18475", "abs": "https://arxiv.org/abs/2505.18475", "authors": ["Mengran Li", "Pengyu Zhang", "Wenbin Xing", "Yijia Zheng", "Klim Zaporojets", "Junzhou Chen", "Ronghui Zhang", "Yong Zhang", "Siyuan Gong", "Jia Hu", "Xiaolei Ma", "Zhiyuan Liu", "Paul Groth", "Marcel Worring"], "title": "Using Large Language Models to Tackle Fundamental Challenges in Graph Learning: A Comprehensive Survey", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Graphs are a widely used paradigm for representing non-Euclidean data, with\napplications ranging from social network analysis to biomolecular prediction.\nConventional graph learning approaches typically rely on fixed structural\nassumptions or fully observed data, limiting their effectiveness in more\ncomplex, noisy, or evolving settings. Consequently, real-world graph data often\nviolates the assumptions of traditional graph learning methods, in particular,\nit leads to four fundamental challenges: (1) Incompleteness, real-world graphs\nhave missing nodes, edges, or attributes; (2) Imbalance, the distribution of\nthe labels of nodes or edges and their structures for real-world graphs are\nhighly skewed; (3) Cross-domain Heterogeneity, graphs from different domains\nexhibit incompatible feature spaces or structural patterns; and (4) Dynamic\nInstability, graphs evolve over time in unpredictable ways. Recent advances in\nLarge Language Models (LLMs) offer the potential to tackle these challenges by\nleveraging rich semantic reasoning and external knowledge. This survey provides\na comprehensive review of how LLMs can be integrated with graph learning to\naddress the aforementioned challenges. For each challenge, we review both\ntraditional solutions and modern LLM-driven approaches, highlighting how LLMs\ncontribute unique advantages. Finally, we discuss open research questions and\npromising future directions in this emerging interdisciplinary field. To\nsupport further exploration, we have curated a repository of recent advances on\ngraph learning challenges:\nhttps://github.com/limengran98/Awesome-Literature-Graph-Learning-Challenges."}
{"id": "2505.19719", "pdf": "https://arxiv.org/pdf/2505.19719", "abs": "https://arxiv.org/abs/2505.19719", "authors": ["Juntong Wang", "Xiyuan Wang", "Muhan Zhang"], "title": "OCN: Effectively Utilizing Higher-Order Common Neighbors for Better Link Prediction", "categories": ["cs.LG", "cs.AI"], "comment": "35 pages, 10 figures", "summary": "Common Neighbors (CNs) and their higher-order variants are important pairwise\nfeatures widely used in state-of-the-art link prediction methods. However,\nexisting methods often struggle with the repetition across different orders of\nCNs and fail to fully leverage their potential. We identify that these\nlimitations stem from two key issues: redundancy and over-smoothing in\nhigh-order common neighbors. To address these challenges, we design\northogonalization to eliminate redundancy between different-order CNs and\nnormalization to mitigate over-smoothing. By combining these two techniques, we\npropose Orthogonal Common Neighbor (OCN), a novel approach that significantly\noutperforms the strongest baselines by an average of 7.7% on popular link\nprediction benchmarks. A thorough theoretical analysis is provided to support\nour method. Ablation studies also verify the effectiveness of our\northogonalization and normalization techniques."}
{"id": "2505.20109", "pdf": "https://arxiv.org/pdf/2505.20109", "abs": "https://arxiv.org/abs/2505.20109", "authors": ["June-Woo Kim", "Wonkyo Oh", "Haram Yoon", "Sung-Hoon Yoon", "Dae-Jin Kim", "Dong-Ho Lee", "Sang-Yeol Lee", "Chan-Mo Yang"], "title": "Language-Agnostic Suicidal Risk Detection Using Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to InterSpeech 2025", "summary": "Suicidal risk detection in adolescents is a critical challenge, yet existing\nmethods rely on language-specific models, limiting scalability and\ngeneralization. This study introduces a novel language-agnostic framework for\nsuicidal risk assessment with large language models (LLMs). We generate Chinese\ntranscripts from speech using an ASR model and then employ LLMs with\nprompt-based queries to extract suicidal risk-related features from these\ntranscripts. The extracted features are retained in both Chinese and English to\nenable cross-linguistic analysis and then used to fine-tune corresponding\npretrained language models independently. Experimental results show that our\nmethod achieves performance comparable to direct fine-tuning with ASR results\nor to models trained solely on Chinese suicidal risk-related features,\ndemonstrating its potential to overcome language constraints and improve the\nrobustness of suicidal risk assessment."}
{"id": "2505.18488", "pdf": "https://arxiv.org/pdf/2505.18488", "abs": "https://arxiv.org/abs/2505.18488", "authors": ["Yanxiang Zhang", "Zheng Xu", "Shanshan Wu", "Yuanbo Zhang", "Daniel Ramage"], "title": "Synthesizing and Adapting Error Correction Data for Mobile Large Language Model Applications", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ACL Industry", "summary": "Error correction is an important capability when applying large language\nmodels (LLMs) to facilitate user typing on mobile devices. In this paper, we\nuse LLMs to synthesize a high-quality dataset of error correction pairs to\nevaluate and improve LLMs for mobile applications. We first prompt LLMs with\nerror correction domain knowledge to build a scalable and reliable addition to\nthe existing data synthesis pipeline. We then adapt the synthetic data\ndistribution to match the mobile application domain by reweighting the samples.\nThe reweighting model is learnt by predicting (a handful of) live A/B test\nmetrics when deploying LLMs in production, given the LLM performance on offline\nevaluation data and scores from a small privacy-preserving on-device language\nmodel. Finally, we present best practices for mixing our synthetic data with\nother data sources to improve model performance on error correction in both\noffline evaluation and production live A/B testing."}
{"id": "2505.19740", "pdf": "https://arxiv.org/pdf/2505.19740", "abs": "https://arxiv.org/abs/2505.19740", "authors": ["Weichen Si", "Yihao Ou", "Zhen Tian"], "title": "Machine Learning Algorithm for Noise Reduction and Disease-Causing Gene Feature Extraction in Gene Sequencing Data", "categories": ["cs.LG"], "comment": null, "summary": "In this study, we propose a machine learning-based method for noise reduction\nand disease-causing gene feature extraction in gene sequencing DeepSeqDenoise\nalgorithm combines CNN and RNN to effectively remove the sequencing noise, and\nimproves the signal-to-noise ratio by 9.4 dB. We screened 17 key features by\nfeature engineering, and constructed an integrated learning model to predict\ndisease-causing genes with 94.3% accuracy. We successfully identified 57 new\ncandidate disease-causing genes in a cardiovascular disease cohort validation,\nand detected 3 missed variants in clinical applications. The method\nsignificantly outperforms existing tools and provides strong support for\naccurate diagnosis of genetic diseases."}
{"id": "2505.20112", "pdf": "https://arxiv.org/pdf/2505.20112", "abs": "https://arxiv.org/abs/2505.20112", "authors": ["Haolei Bai", "Siyong Jian", "Tuo Liang", "Yu Yin", "Huan Wang"], "title": "ResSVD: Residual Compensated SVD for Large Language Model Compression", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities in a\nwide range of downstream natural language processing tasks. Nevertheless, their\nconsiderable sizes and memory demands hinder practical deployment, underscoring\nthe importance of developing efficient compression strategies. Singular value\ndecomposition (SVD) decomposes a matrix into orthogonal components, enabling\nefficient low-rank approximation. This is particularly suitable for LLM\ncompression, where weight matrices often exhibit significant redundancy.\nHowever, current SVD-based methods neglect the residual matrix from truncation,\nresulting in significant truncation loss. Additionally, compressing all layers\nof the model results in severe performance degradation. To overcome these\nlimitations, we propose ResSVD, a new post-training SVD-based LLM compression\nmethod. Specifically, we leverage the residual matrix generated during the\ntruncation process to reduce truncation loss. Moreover, under a fixed overall\ncompression ratio, we selectively compress the last few layers of the model,\nwhich mitigates error propagation and significantly improves the performance of\ncompressed models.Comprehensive evaluations of ResSVD on diverse LLM families\nand multiple benchmark datasets indicate that ResSVD consistently achieves\nsuperior performance over existing counterpart methods, demonstrating its\npractical effectiveness."}
{"id": "2505.18494", "pdf": "https://arxiv.org/pdf/2505.18494", "abs": "https://arxiv.org/abs/2505.18494", "authors": ["Zihao Peng", "Jiandian Zeng", "Boyuan Li", "Guo Li", "Shengbo Chen", "Tian Wang"], "title": "FedHL: Federated Learning for Heterogeneous Low-Rank Adaptation via Unbiased Aggregation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated Learning (FL) facilitates the fine-tuning of Foundation Models\n(FMs) using distributed data sources, with Low-Rank Adaptation (LoRA) gaining\npopularity due to its low communication costs and strong performance. While\nrecent work acknowledges the benefits of heterogeneous LoRA in FL and\nintroduces flexible algorithms to support its implementation, our theoretical\nanalysis reveals a critical gap: existing methods lack formal convergence\nguarantees due to parameter truncation and biased gradient updates.\nSpecifically, adapting client-specific LoRA ranks necessitates truncating\nglobal parameters, which introduces inherent truncation errors and leads to\nsubsequent inaccurate gradient updates that accumulate over training rounds,\nultimately degrading performance. To address the above issues, we propose\n\\textbf{FedHL}, a simple yet effective \\textbf{Fed}erated Learning framework\ntailored for \\textbf{H}eterogeneous \\textbf{L}oRA. By leveraging the full-rank\nglobal model as a calibrated aggregation basis, FedHL eliminates the direct\ntruncation bias from initial alignment with client-specific ranks. Furthermore,\nwe derive the theoretically optimal aggregation weights by minimizing the\ngradient drift term in the convergence upper bound. Our analysis shows that\nFedHL guarantees $\\mathcal{O}(1/\\sqrt{T})$ convergence rate, and experiments on\nmultiple real-world datasets demonstrate a 1-3\\% improvement over several\nstate-of-the-art methods."}
{"id": "2505.19752", "pdf": "https://arxiv.org/pdf/2505.19752", "abs": "https://arxiv.org/abs/2505.19752", "authors": ["Hengli Li", "Yuxuan Wang", "Song-Chun Zhu", "Ying Nian Wu", "Zilong Zheng"], "title": "Discrete Markov Bridge", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Discrete diffusion has recently emerged as a promising paradigm in discrete\ndata modeling. However, existing methods typically rely on a fixed rate\ntransition matrix during training, which not only limits the expressiveness of\nlatent representations, a fundamental strength of variational methods, but also\nconstrains the overall design space. To address these limitations, we propose\nDiscrete Markov Bridge, a novel framework specifically designed for discrete\nrepresentation learning. Our approach is built upon two key components: Matrix\nLearning and Score Learning. We conduct a rigorous theoretical analysis,\nestablishing formal performance guarantees for Matrix Learning and proving the\nconvergence of the overall framework. Furthermore, we analyze the space\ncomplexity of our method, addressing practical constraints identified in prior\nstudies. Extensive empirical evaluations validate the effectiveness of the\nproposed Discrete Markov Bridge, which achieves an Evidence Lower Bound (ELBO)\nof 1.38 on the Text8 dataset, outperforming established baselines. Moreover,\nthe proposed model demonstrates competitive performance on the CIFAR-10\ndataset, achieving results comparable to those obtained by image-specific\ngeneration approaches."}
{"id": "2505.20113", "pdf": "https://arxiv.org/pdf/2505.20113", "abs": "https://arxiv.org/abs/2505.20113", "authors": ["Cristian Santini", "Laura Melosi", "Emanuele Frontoni"], "title": "Named Entity Recognition in Historical Italian: The Case of Giacomo Leopardi's Zibaldone", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The increased digitization of world's textual heritage poses significant\nchallenges for both computer science and literary studies. Overall, there is an\nurgent need of computational techniques able to adapt to the challenges of\nhistorical texts, such as orthographic and spelling variations, fragmentary\nstructure and digitization errors. The rise of large language models (LLMs) has\nrevolutionized natural language processing, suggesting promising applications\nfor Named Entity Recognition (NER) on historical documents. In spite of this,\nno thorough evaluation has been proposed for Italian texts. This research tries\nto fill the gap by proposing a new challenging dataset for entity extraction\nbased on a corpus of 19th century scholarly notes, i.e. Giacomo Leopardi's\nZibaldone (1898), containing 2,899 references to people, locations and literary\nworks. This dataset was used to carry out reproducible experiments with both\ndomain-specific BERT-based models and state-of-the-art LLMs such as LLaMa3.1.\nResults show that instruction-tuned models encounter multiple difficulties\nhandling historical humanistic texts, while fine-tuned NER models offer more\nrobust performance even with challenging entity types such as bibliographic\nreferences."}
{"id": "2505.18499", "pdf": "https://arxiv.org/pdf/2505.18499", "abs": "https://arxiv.org/abs/2505.18499", "authors": ["Xiaojun Guo", "Ang Li", "Yifei Wang", "Stefanie Jegelka", "Yisen Wang"], "title": "G1: Teaching LLMs to Reason on Graphs with Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Although Large Language Models (LLMs) have demonstrated remarkable progress,\ntheir proficiency in graph-related tasks remains notably limited, hindering the\ndevelopment of truly general-purpose models. Previous attempts, including\npretraining graph foundation models or employing supervised fine-tuning, often\nface challenges such as the scarcity of large-scale, universally represented\ngraph data. We introduce G1, a simple yet effective approach demonstrating that\nReinforcement Learning (RL) on synthetic graph-theoretic tasks can\nsignificantly scale LLMs' graph reasoning abilities. To enable RL training, we\ncurate Erd\\~os, the largest graph reasoning dataset to date comprising 50\ndiverse graph-theoretic tasks of varying difficulty levels, 100k training data\nand 5k test data, all drived from real-world graphs. With RL on Erd\\~os, G1\nobtains substantial improvements in graph reasoning, where our finetuned 3B\nmodel even outperforms Qwen2.5-72B-Instruct (24x size). RL-trained models also\nshow strong zero-shot generalization to unseen tasks, domains, and graph\nencoding schemes, including other graph-theoretic benchmarks as well as\nreal-world node classification and link prediction tasks, without compromising\ngeneral reasoning abilities. Our findings offer an efficient, scalable path for\nbuilding strong graph reasoners by finetuning LLMs with RL on graph-theoretic\ntasks, which combines the strengths of pretrained LLM capabilities with\nabundant, automatically generated synthetic data, suggesting that LLMs possess\ngraph understanding abilities that RL can elicit successfully."}
{"id": "2505.19763", "pdf": "https://arxiv.org/pdf/2505.19763", "abs": "https://arxiv.org/abs/2505.19763", "authors": ["Thomas Hamelryck", "Kanti V. Mardia"], "title": "Unfolding AlphaFold's Bayesian Roots in Probability Kinematics", "categories": ["cs.LG"], "comment": "15 pages, 5 figures", "summary": "We present a novel theoretical interpretation of AlphaFold1. The seminal\nbreakthrough of AlphaFold1 in protein structure prediction by deep learning\nrelied on a learned potential energy function, in contrast to the later\nend-to-end architectures of AlphaFold2 and AlphaFold3. While this potential was\noriginally justified by referring to physical potentials of mean force (PMFs),\nwe reinterpret AlphaFold1's potential as an instance of probability kinematics\n- also known as Jeffrey conditioning - a principled but underrecognised\ngeneralization of conventional Bayesian updating. Probability kinematics\naccommodates uncertain or soft evidence in the form of updated probabilities\nover a partition. This perspective reveals AlphaFold1's potential as a form of\ngeneralized Bayesian updating, rather than a thermodynamic potential. To\nconfirm our probabilistic framework's scope and precision, we analyze a\nsynthetic 2D model in which an angular random walk prior is updated with\nevidence on distances via probability kinematics, mirroring AlphaFold1's\napproach. This theoretical contribution connects AlphaFold1 to a broader class\nof well-justified Bayesian methods, allowing precise quantification, surpassing\nmerely qualitative heuristics based on PMFs. More broadly, given the\nachievements of AlphaFold1, probability kinematics holds considerable promise\nfor probabilistic deep learning, as it allows for the formulation of complex\nmodels from a few simpler components."}
{"id": "2505.20118", "pdf": "https://arxiv.org/pdf/2505.20118", "abs": "https://arxiv.org/abs/2505.20118", "authors": ["Dominik Meier", "Jan Philip Wahle", "Paul Rttger", "Terry Ruas", "Bela Gipp"], "title": "TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent", "categories": ["cs.CL", "cs.CR"], "comment": "8 pages, 5 figures", "summary": "As large language models (LLMs) become integrated into sensitive workflows,\nconcerns grow over their potential to leak confidential information. We propose\nTrojanStego, a novel threat model in which an adversary fine-tunes an LLM to\nembed sensitive context information into natural-looking outputs via linguistic\nsteganography, without requiring explicit control over inference inputs. We\nintroduce a taxonomy outlining risk factors for compromised LLMs, and use it to\nevaluate the risk profile of the threat. To implement TrojanStego, we propose a\npractical encoding scheme based on vocabulary partitioning learnable by LLMs\nvia fine-tuning. Experimental results show that compromised models reliably\ntransmit 32-bit secrets with 87% accuracy on held-out prompts, reaching over\n97% accuracy using majority voting across three generations. Further, they\nmaintain high utility, can evade human detection, and preserve coherence. These\nresults highlight a new class of LLM data exfiltration attacks that are\npassive, covert, practical, and dangerous."}
{"id": "2505.18505", "pdf": "https://arxiv.org/pdf/2505.18505", "abs": "https://arxiv.org/abs/2505.18505", "authors": ["Yixuan Ma", "Kai Yi", "Pietro Lio", "Shi Jin", "Yu Guang Wang"], "title": "How Particle System Theory Enhances Hypergraph Message Passing", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Hypergraphs effectively model higher-order relationships in natural\nphenomena, capturing complex interactions beyond pairwise connections. We\nintroduce a novel hypergraph message passing framework inspired by interacting\nparticle systems, where hyperedges act as fields inducing shared node dynamics.\nBy incorporating attraction, repulsion, and Allen-Cahn forcing terms, particles\nof varying classes and features achieve class-dependent equilibrium, enabling\nseparability through the particle-driven message passing. We investigate both\nfirst-order and second-order particle system equations for modeling these\ndynamics, which mitigate over-smoothing and heterophily thus can capture\ncomplete interactions. The more stable second-order system permits deeper\nmessage passing. Furthermore, we enhance deterministic message passing with\nstochastic element to account for interaction uncertainties. We prove\ntheoretically that our approach mitigates over-smoothing by maintaining a\npositive lower bound on the hypergraph Dirichlet energy during propagation and\nthus to enable hypergraph message passing to go deep. Empirically, our models\ndemonstrate competitive performance on diverse real-world hypergraph node\nclassification tasks, excelling on both homophilic and heterophilic datasets."}
{"id": "2505.19764", "pdf": "https://arxiv.org/pdf/2505.19764", "abs": "https://arxiv.org/abs/2505.19764", "authors": ["Patara Trirat", "Wonyong Jeong", "Sung Ju Hwang"], "title": "Agentic Predictor: Performance Prediction for Agentic Workflows via Multi-View Encoding", "categories": ["cs.LG", "cs.AI"], "comment": "Code will be available at\n  https://github.com/DeepAuto-AI/agentic-predictor", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\ndiverse tasks, but optimizing LLM-based agentic systems remains challenging due\nto the vast search space of agent configurations, prompting strategies, and\ncommunication patterns. Existing approaches often rely on heuristic-based\ntuning or exhaustive evaluation, which can be computationally expensive and\nsuboptimal. This paper proposes Agentic Predictor, a lightweight predictor for\nefficient agentic workflow evaluation. Agentic Predictor is equipped with a\nmulti-view workflow encoding technique that leverages multi-view representation\nlearning of agentic systems by incorporating code architecture, textual\nprompts, and interaction graph features. To achieve high predictive accuracy\nwhile significantly reducing the number of required workflow evaluations for\ntraining a predictor, Agentic Predictor employs cross-domain unsupervised\npretraining. By learning to approximate task success rates, Agentic Predictor\nenables fast and accurate selection of optimal agentic workflow configurations\nfor a given task, significantly reducing the need for expensive trial-and-error\nevaluations. Experiments on a carefully curated benchmark spanning three\ndomains show that our predictor outperforms state-of-the-art methods in both\npredictive accuracy and workflow utility, highlighting the potential of\nperformance predictors in streamlining the design of LLM-based agentic\nworkflows."}
{"id": "2505.20128", "pdf": "https://arxiv.org/pdf/2505.20128", "abs": "https://arxiv.org/abs/2505.20128", "authors": ["Zhengliang Shi", "Lingyong Yan", "Dawei Yin", "Suzan Verberne", "Maarten de Rijke", "Zhaochun Ren"], "title": "Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers", "categories": ["cs.CL"], "comment": "Working in process", "summary": "Large language models (LLMs) have been widely integrated into information\nretrieval to advance traditional techniques. However, effectively enabling LLMs\nto seek accurate knowledge in complex tasks remains a challenge due to the\ncomplexity of multi-hop queries as well as the irrelevant retrieved content. To\naddress these limitations, we propose EXSEARCH, an agentic search framework,\nwhere the LLM learns to retrieve useful information as the reasoning unfolds\nthrough a self-incentivized process. At each step, the LLM decides what to\nretrieve (thinking), triggers an external retriever (search), and extracts\nfine-grained evidence (recording) to support next-step reasoning. To enable LLM\nwith this capability, EXSEARCH adopts a Generalized Expectation-Maximization\nalgorithm. In the E-step, the LLM generates multiple search trajectories and\nassigns an importance weight to each; the M-step trains the LLM on them with a\nre-weighted loss function. This creates a self-incentivized loop, where the LLM\niteratively learns from its own generated data, progressively improving itself\nfor search. We further theoretically analyze this training process,\nestablishing convergence guarantees. Extensive experiments on four\nknowledge-intensive benchmarks show that EXSEARCH substantially outperforms\nbaselines, e.g., +7.8% improvement on exact match score. Motivated by these\npromising results, we introduce EXSEARCH-Zoo, an extension that extends our\nmethod to broader scenarios, to facilitate future work."}
{"id": "2505.18512", "pdf": "https://arxiv.org/pdf/2505.18512", "abs": "https://arxiv.org/abs/2505.18512", "authors": ["Soyoung Yoon", "Gyuwan Kim", "Gyu-Hwung Cho", "Seung-won Hwang"], "title": "AcuRank: Uncertainty-Aware Adaptive Computation for Listwise Reranking", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": "22 pages, 3 figures. The first two authors contributed equally.\n  Author order is randomly determined via coin toss", "summary": "Listwise reranking with large language models (LLMs) enhances top-ranked\nresults in retrieval-based applications. Due to the limit in context size and\nhigh inference cost of long context, reranking is typically performed over a\nfixed size of small subsets, with the final ranking aggregated from these\npartial results. This fixed computation disregards query difficulty and\ndocument distribution, leading to inefficiencies. We propose AcuRank, an\nadaptive reranking framework that dynamically adjusts both the amount and\ntarget of computation based on uncertainty estimates over document relevance.\nUsing a Bayesian TrueSkill model, we iteratively refine relevance estimates\nuntil reaching sufficient confidence levels, and our explicit modeling of\nranking uncertainty enables principled control over reranking behavior and\navoids unnecessary updates to confident predictions. Results on the TREC-DL and\nBEIR benchmarks show that our method consistently achieves a superior\naccuracy-efficiency trade-off and scales better with compute than\nfixed-computation baselines. These results highlight the effectiveness and\ngeneralizability of our method across diverse retrieval tasks and LLM-based\nreranking models."}
{"id": "2505.19770", "pdf": "https://arxiv.org/pdf/2505.19770", "abs": "https://arxiv.org/abs/2505.19770", "authors": ["Ruizhe Shi", "Minhak Song", "Runlong Zhou", "Zihan Zhang", "Maryam Fazel", "Simon S. Du"], "title": "Understanding the Performance Gap in Preference Learning: A Dichotomy of RLHF and DPO", "categories": ["cs.LG", "cs.CL"], "comment": "30 pages, 5 figures", "summary": "We present a fine-grained theoretical analysis of the performance gap between\nreinforcement learning from human feedback (RLHF) and direct preference\noptimization (DPO) under a representation gap. Our study decomposes this gap\ninto two sources: an explicit representation gap under exact optimization and\nan implicit representation gap under finite samples. In the exact optimization\nsetting, we characterize how the relative capacities of the reward and policy\nmodel classes influence the final policy qualities. We show that RLHF, DPO, or\nonline DPO can outperform one another depending on the type of model\nmis-specifications. Notably, online DPO can outperform both RLHF and standard\nDPO when the reward and policy model classes are isomorphic and both\nmis-specified. In the approximate optimization setting, we provide a concrete\nconstruction where the ground-truth reward is implicitly sparse and show that\nRLHF requires significantly fewer samples than DPO to recover an effective\nreward model -- highlighting a statistical advantage of two-stage learning.\nTogether, these results provide a comprehensive understanding of the\nperformance gap between RLHF and DPO under various settings, and offer\npractical insights into when each method is preferred."}
{"id": "2505.20133", "pdf": "https://arxiv.org/pdf/2505.20133", "abs": "https://arxiv.org/abs/2505.20133", "authors": ["Konstantin Dobler", "Desmond Elliott", "Gerard de Melo"], "title": "AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Current language models rely on static vocabularies determined at pretraining\ntime, which can lead to decreased performance and increased computational cost\nfor domains underrepresented in the original vocabulary. New tokens can be\nadded to solve this problem, when coupled with a good initialization for their\nnew embeddings. However, existing embedding initialization methods either\nrequire expensive further training or pretraining of additional modules. In\nthis paper, we propose AweDist and show that by distilling representations\nobtained using the original tokenization, we can quickly learn high-quality\ninput embeddings for new tokens. Experimental results with a wide range of\nopen-weight models show that AweDist is able to outperform even strong\nbaselines."}
{"id": "2505.18514", "pdf": "https://arxiv.org/pdf/2505.18514", "abs": "https://arxiv.org/abs/2505.18514", "authors": ["Taeckyung Lee", "Sorn Chottananurak", "Junsu Kim", "Jinwoo Shin", "Taesik Gong", "Sung-Ju Lee"], "title": "Test-Time Adaptation with Binary Feedback", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to ICML 2025", "summary": "Deep learning models perform poorly when domain shifts exist between training\nand test data. Test-time adaptation (TTA) is a paradigm to mitigate this issue\nby adapting pre-trained models using only unlabeled test samples. However,\nexisting TTA methods can fail under severe domain shifts, while recent active\nTTA approaches requiring full-class labels are impractical due to high labeling\ncosts. To address this issue, we introduce a new setting of TTA with binary\nfeedback. This setting uses a few binary feedback inputs from annotators to\nindicate whether model predictions are correct, thereby significantly reducing\nthe labeling burden of annotators. Under the setting, we propose BiTTA, a novel\ndual-path optimization framework that leverages reinforcement learning to\nbalance binary feedback-guided adaptation on uncertain samples with\nagreement-based self-adaptation on confident predictions. Experiments show\nBiTTA achieves 13.3%p accuracy improvements over state-of-the-art baselines,\ndemonstrating its effectiveness in handling severe distribution shifts with\nminimal labeling effort. The source code is available at\nhttps://github.com/taeckyung/BiTTA."}
{"id": "2505.19785", "pdf": "https://arxiv.org/pdf/2505.19785", "abs": "https://arxiv.org/abs/2505.19785", "authors": ["Qianyi Xu", "Gousia Habib", "Dilruk Perera", "Mengling Feng"], "title": "MedDreamer: Model-Based Reinforcement Learning with Latent Imagination on Complex EHRs for Clinical Decision Support", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Timely and personalized treatment decisions are essential across a wide range\nof healthcare settings where patient responses vary significantly and evolve\nover time. Clinical data used to support these decisions are often irregularly\nsampled, sparse, and noisy. Existing decision support systems commonly rely on\ndiscretization and imputation, which can distort critical temporal dynamics and\ndegrade decision quality. Moreover, they often overlook the clinical\nsignificance of irregular recording frequencies, filtering out patterns in how\nand when data is collected. Reinforcement Learning (RL) is a natural fit for\nclinical decision-making, enabling sequential, long-term optimization in\ndynamic, uncertain environments. However, most existing treatment\nrecommendation systems are model-free and trained solely on offline data,\nmaking them sample-inefficient, sensitive to data quality, and poorly\ngeneralizable across tasks or cohorts. To address these limitations, we propose\nMedDreamer, a two-phase model-based RL framework for personalized treatment\nrecommendation. MedDreamer uses a world model with an Adaptive Feature\nIntegration (AFI) module to effectively model irregular, sparse clinical data.\nThrough latent imagination, it simulates plausible patient trajectories to\nenhance learning, refining its policy using a mix of real and imagined\nexperiences. This enables learning policies that go beyond suboptimal\nhistorical decisions while remaining grounded in clinical data. To our\nknowledge, this is the first application of latent imagination to irregular\nhealthcare data. Evaluations on sepsis and mechanical ventilation (MV)\ntreatment using two large-scale EHR datasets show that MedDreamer outperforms\nboth model-free and model-based baselines in clinical outcomes and off-policy\nmetrics."}
{"id": "2505.20144", "pdf": "https://arxiv.org/pdf/2505.20144", "abs": "https://arxiv.org/abs/2505.20144", "authors": ["Jian Gu", "Aldeida Aleti", "Chunyang Chen", "Hongyu Zhang"], "title": "SeMe: Training-Free Language Model Merging via Semantic Alignment", "categories": ["cs.CL", "cs.LG"], "comment": "an early-stage version", "summary": "Despite the remarkable capabilities of Language Models (LMs) across diverse\ntasks, no single model consistently outperforms others, necessitating efficient\nmethods to combine their strengths without expensive retraining. Existing model\nmerging techniques, such as parameter averaging and task-guided fusion, often\nrely on data-dependent computations or fail to preserve internal knowledge,\nlimiting their robustness and scalability. We introduce SeMe (Semantic-based\nMerging), a novel, data-free, and training-free approach that leverages latent\nsemantic alignment to merge LMs at a fine-grained, layer-wise level. Unlike\nprior work, SeMe not only preserves model behaviors but also explicitly\nstabilizes internal knowledge, addressing a critical gap in LM fusion. Through\nextensive experiments across diverse architectures and tasks, we demonstrate\nthat SeMe outperforms existing methods in both performance and efficiency while\neliminating reliance on external data. Our work establishes a new paradigm for\nknowledge-aware model merging and provides insights into the semantic structure\nof LMs, paving the way for more scalable and interpretable model composition."}
{"id": "2505.18527", "pdf": "https://arxiv.org/pdf/2505.18527", "abs": "https://arxiv.org/abs/2505.18527", "authors": ["Yiqing Zhang", "Xiaozhong Liu", "Fabricio Murai"], "title": "CLaDMoP: Learning Transferrable Models from Successful Clinical Trials via LLMs", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted and to be published in KDD2025", "summary": "Many existing models for clinical trial outcome prediction are optimized\nusing task-specific loss functions on trial phase-specific data. While this\nscheme may boost prediction for common diseases and drugs, it can hinder\nlearning of generalizable representations, leading to more false\npositives/negatives. To address this limitation, we introduce CLaDMoP, a new\npre-training approach for clinical trial outcome prediction, alongside the\nSuccessful Clinical Trials dataset(SCT), specifically designed for this task.\nCLaDMoP leverages a Large Language Model-to encode trials' eligibility\ncriteria-linked to a lightweight Drug-Molecule branch through a novel\nmulti-level fusion technique. To efficiently fuse long embeddings across\nlevels, we incorporate a grouping block, drastically reducing computational\noverhead. CLaDMoP avoids reliance on task-specific objectives by pre-training\non a \"pair matching\" proxy task. Compared to established zero-shot and few-shot\nbaselines, our method significantly improves both PR-AUC and ROC-AUC,\nespecially for phase I and phase II trials. We further evaluate and perform\nablation on CLaDMoP after Parameter-Efficient Fine-Tuning, comparing it to\nstate-of-the-art supervised baselines, including MEXA-CTP, on the Trial Outcome\nPrediction(TOP) benchmark. CLaDMoP achieves up to 10.5% improvement in PR-AUC\nand 3.6% in ROC-AUC, while attaining comparable F1 score to MEXA-CTP,\nhighlighting its potential for clinical trial outcome prediction. Code and SCT\ndataset can be downloaded from https://github.com/murai-lab/CLaDMoP."}
{"id": "2505.19789", "pdf": "https://arxiv.org/pdf/2505.19789", "abs": "https://arxiv.org/abs/2505.19789", "authors": ["Jijia Liu", "Feng Gao", "Bingwen Wei", "Xinlei Chen", "Qingmin Liao", "Yi Wu", "Chao Yu", "Yu Wang"], "title": "What Can RL Bring to VLA Generalization? An Empirical Study", "categories": ["cs.LG"], "comment": null, "summary": "Large Vision-Language Action (VLA) models have shown significant potential\nfor embodied AI. However, their predominant training via supervised fine-tuning\n(SFT) limits generalization due to susceptibility to compounding errors under\ndistribution shifts. Reinforcement learning (RL) offers a path to overcome\nthese limitations by optimizing for task objectives via trial-and-error, yet a\nsystematic understanding of its specific generalization benefits for VLAs\ncompared to SFT is lacking. To address this, our study introduces a\ncomprehensive benchmark for evaluating VLA generalization and systematically\ninvestigates the impact of RL fine-tuning across diverse visual, semantic, and\nexecution dimensions. Our extensive experiments reveal that RL fine-tuning,\nparticularly with PPO, significantly enhances generalization in semantic\nunderstanding and execution robustness over SFT, while maintaining comparable\nvisual robustness. We identify PPO as a more effective RL algorithm for VLAs\nthan LLM-derived methods like DPO and GRPO. We also develop a simple recipe for\nefficient PPO training on VLAs, and demonstrate its practical utility for\nimproving VLA generalization. The project page is at https://rlvla.github.io"}
{"id": "2505.20154", "pdf": "https://arxiv.org/pdf/2505.20154", "abs": "https://arxiv.org/abs/2505.20154", "authors": ["Xueyan Zhang", "Jinman Zhao", "Zhifei Yang", "Yibo Zhong", "Shuhao Guan", "Linbo Cao", "Yining Wang"], "title": "UORA: Uniform Orthogonal Reinitialization Adaptation in Parameter-Efficient Fine-Tuning of Large Models", "categories": ["cs.CL"], "comment": "20 pages, 2 figures, 15 tables", "summary": "This paper introduces Uniform Orthogonal Reinitialization Adaptation (UORA),\na novel parameter-efficient fine-tuning (PEFT) approach for Large Language\nModels (LLMs). UORA achieves state-of-the-art performance and parameter\nefficiency by leveraging a low-rank approximation method to reduce the number\nof trainable parameters. Unlike existing methods such as LoRA and VeRA, UORA\nemploys an interpolation-based reparametrization mechanism that selectively\nreinitializes rows and columns in frozen projection matrices, guided by the\nvector magnitude heuristic. This results in substantially fewer trainable\nparameters compared to LoRA and outperforms VeRA in computation and storage\nefficiency. Comprehensive experiments across various benchmarks demonstrate\nUORA's superiority in achieving competitive fine-tuning performance with\nnegligible computational overhead. We demonstrate its performance on GLUE and\nE2E benchmarks and its effectiveness in instruction-tuning large language\nmodels and image classification models. Our contributions establish a new\nparadigm for scalable and resource-efficient fine-tuning of LLMs."}
{"id": "2505.18530", "pdf": "https://arxiv.org/pdf/2505.18530", "abs": "https://arxiv.org/abs/2505.18530", "authors": ["Pengyu Wang", "Shuchang Ye", "Usman Naseem", "Jinman Kim"], "title": "MRGAgents: A Multi-Agent Framework for Improved Medical Report Generation with Med-LVLMs", "categories": ["cs.MA", "cs.AI"], "comment": "10pages", "summary": "Medical Large Vision-Language Models (Med-LVLMs) have been widely adopted for\nmedical report generation. Despite Med-LVLMs producing state-of-the-art\nperformance, they exhibit a bias toward predicting all findings as normal,\nleading to reports that overlook critical abnormalities. Furthermore, these\nmodels often fail to provide comprehensive descriptions of radiologically\nrelevant regions necessary for accurate diagnosis. To address these challenges,\nwe proposeMedical Report Generation Agents (MRGAgents), a novel multi-agent\nframework that fine-tunes specialized agents for different disease categories.\nBy curating subsets of the IU X-ray and MIMIC-CXR datasets to train\ndisease-specific agents, MRGAgents generates reports that more effectively\nbalance normal and abnormal findings while ensuring a comprehensive description\nof clinically relevant regions. Our experiments demonstrate that MRGAgents\noutperformed the state-of-the-art, improving both report comprehensiveness and\ndiagnostic utility."}
{"id": "2505.19802", "pdf": "https://arxiv.org/pdf/2505.19802", "abs": "https://arxiv.org/abs/2505.19802", "authors": ["Zhiyu Wang", "Yang Liu", "Hatice Gunes"], "title": "GraphAU-Pain: Graph-based Action Unit Representation for Pain Intensity Estimation", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Understanding pain-related facial behaviors is essential for digital\nhealthcare in terms of effective monitoring, assisted diagnostics, and\ntreatment planning, particularly for patients unable to communicate verbally.\nExisting data-driven methods of detecting pain from facial expressions are\nlimited due to interpretability and severity quantification. To this end, we\npropose GraphAU-Pain, leveraging a graph-based framework to model facial Action\nUnits (AUs) and their interrelationships for pain intensity estimation. AUs are\nrepresented as graph nodes, with co-occurrence relationships as edges, enabling\na more expressive depiction of pain-related facial behaviors. By utilizing a\nrelational graph neural network, our framework offers improved interpretability\nand significant performance gains. Experiments conducted on the publicly\navailable UNBC dataset demonstrate the effectiveness of the GraphAU-Pain,\nachieving an F1-score of 66.21% and accuracy of 87.61% in pain intensity\nestimation."}
{"id": "2505.20155", "pdf": "https://arxiv.org/pdf/2505.20155", "abs": "https://arxiv.org/abs/2505.20155", "authors": ["Hanting Chen", "Jiarui Qin", "Jialong Guo", "Tao Yuan", "Yichun Yin", "Huiling Zhen", "Yasheng Wang", "Jinpeng Li", "Xiaojun Meng", "Meng Zhang", "Rongju Ruan", "Zheyuan Bai", "Yehui Tang", "Can Chen", "Xinghao Chen", "Fisher Yu", "Ruiming Tang", "Yunhe Wang"], "title": "Pangu Light: Weight Re-Initialization for Pruning and Accelerating LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) deliver state-of-the-art capabilities across\nnumerous tasks, but their immense size and inference costs pose significant\ncomputational challenges for practical deployment. While structured pruning\noffers a promising avenue for model compression, existing methods often\nstruggle with the detrimental effects of aggressive, simultaneous width and\ndepth reductions, leading to substantial performance degradation. This paper\nargues that a critical, often overlooked, aspect in making such aggressive\njoint pruning viable is the strategic re-initialization and adjustment of\nremaining weights to improve the model post-pruning training accuracies. We\nintroduce Pangu Light, a framework for LLM acceleration centered around\nstructured pruning coupled with novel weight re-initialization techniques\ndesigned to address this ``missing piece''. Our framework systematically\ntargets multiple axes, including model width, depth, attention heads, and\nRMSNorm, with its effectiveness rooted in novel re-initialization methods like\nCross-Layer Attention Pruning (CLAP) and Stabilized LayerNorm Pruning (SLNP)\nthat mitigate performance drops by providing the network a better training\nstarting point. Further enhancing efficiency, Pangu Light incorporates\nspecialized optimizations such as absorbing Post-RMSNorm computations and\ntailors its strategies to Ascend NPU characteristics. The Pangu Light models\nconsistently exhibit a superior accuracy-efficiency trade-off, outperforming\nprominent baseline pruning methods like Nemotron and established LLMs like\nQwen3 series. For instance, on Ascend NPUs, Pangu Light-32B's 81.6 average\nscore and 2585 tokens/s throughput exceed Qwen3-32B's 80.9 average score and\n2225 tokens/s."}
{"id": "2505.18533", "pdf": "https://arxiv.org/pdf/2505.18533", "abs": "https://arxiv.org/abs/2505.18533", "authors": ["Xiaobin Rong", "Dahan Wang", "Qinwen Hu", "Yushi Wang", "Yuxiang Hu", "Jing Lu"], "title": "TS-URGENet: A Three-stage Universal Robust and Generalizable Speech Enhancement Network", "categories": ["eess.AS", "cs.AI"], "comment": "Accepted by Interspeech 2025", "summary": "Universal speech enhancement aims to handle input speech with different\ndistortions and input formats. To tackle this challenge, we present TS-URGENet,\na Three-Stage Universal, Robust, and Generalizable speech Enhancement Network.\nTo address various distortions, the proposed system employs a novel three-stage\narchitecture consisting of a filling stage, a separation stage, and a\nrestoration stage. The filling stage mitigates packet loss by preliminarily\nfilling lost regions under noise interference, ensuring signal continuity. The\nseparation stage suppresses noise, reverberation, and clipping distortion to\nimprove speech clarity. Finally, the restoration stage compensates for\nbandwidth limitation, codec artifacts, and residual packet loss distortion,\nrefining the overall speech quality. Our proposed TS-URGENet achieved\noutstanding performance in the Interspeech 2025 URGENT Challenge, ranking 2nd\nin Track 1."}
{"id": "2505.19807", "pdf": "https://arxiv.org/pdf/2505.19807", "abs": "https://arxiv.org/abs/2505.19807", "authors": ["Bariscan Bozkurt", "Houssam Zenati", "Dimitri Meunier", "Liyuan Xu", "Arthur Gretton"], "title": "Density Ratio-Free Doubly Robust Proxy Causal Learning", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We study the problem of causal function estimation in the Proxy Causal\nLearning (PCL) framework, where confounders are not observed but proxies for\nthe confounders are available. Two main approaches have been proposed: outcome\nbridge-based and treatment bridge-based methods. In this work, we propose two\nkernel-based doubly robust estimators that combine the strengths of both\napproaches, and naturally handle continuous and high-dimensional variables. Our\nidentification strategy builds on a recent density ratio-free method for\ntreatment bridge-based PCL; furthermore, in contrast to previous approaches, it\ndoes not require indicator functions or kernel smoothing over the treatment\nvariable. These properties make it especially well-suited for continuous or\nhigh-dimensional treatments. By using kernel mean embeddings, we have\nclosed-form solutions and strong consistency guarantees. Our estimators\noutperform existing methods on PCL benchmarks, including a prior doubly robust\nmethod that requires both kernel smoothing and density ratio estimation."}
{"id": "2505.20163", "pdf": "https://arxiv.org/pdf/2505.20163", "abs": "https://arxiv.org/abs/2505.20163", "authors": ["Moreno La Quatra", "Alkis Koudounas", "Valerio Mario Salerno", "Sabato Marco Siniscalchi"], "title": "Exploring Generative Error Correction for Dysarthric Speech Recognition", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted at INTERSPEECH 2025", "summary": "Despite the remarkable progress in end-to-end Automatic Speech Recognition\n(ASR) engines, accurately transcribing dysarthric speech remains a major\nchallenge. In this work, we proposed a two-stage framework for the Speech\nAccessibility Project Challenge at INTERSPEECH 2025, which combines\ncutting-edge speech recognition models with LLM-based generative error\ncorrection (GER). We assess different configurations of model scales and\ntraining strategies, incorporating specific hypothesis selection to improve\ntranscription accuracy. Experiments on the Speech Accessibility Project dataset\ndemonstrate the strength of our approach on structured and spontaneous speech,\nwhile highlighting challenges in single-word recognition. Through comprehensive\nanalysis, we provide insights into the complementary roles of acoustic and\nlinguistic modeling in dysarthric speech recognition"}
{"id": "2505.18536", "pdf": "https://arxiv.org/pdf/2505.18536", "abs": "https://arxiv.org/abs/2505.18536", "authors": ["Haoyuan Sun", "Jiaqi Wu", "Bo Xia", "Yifu Luo", "Yifei Zhao", "Kai Qin", "Xufei Lv", "Tiantian Zhang", "Yongzhe Chang", "Xueqian Wang"], "title": "Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Standing in 2025, at a critical juncture in the pursuit of Artificial General\nIntelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated\nsignificant potential in enhancing the reasoning capability of large language\nmodels (LLMs) and has led to the development of cutting-edge AI models such as\nOpenAI-o1 and DeepSeek-R1. Moreover, the efficient application of RFT to\nenhance the reasoning capability of multimodal large language models (MLLMs)\nhas attracted widespread attention from the community. In this position paper,\nwe argue that reinforcement fine-tuning powers the reasoning capability of\nmultimodal large language models. To begin with, we provide a detailed\nintroduction to the fundamental background knowledge that researchers\ninterested in this field should be familiar with. Furthermore, we meticulously\nsummarize the improvements of RFT in powering reasoning capability of MLLMs\ninto five key points: diverse modalities, diverse tasks and domains, better\ntraining algorithms, abundant benchmarks and thriving engineering frameworks.\nFinally, we propose five promising directions for future research that the\ncommunity might consider. We hope that this position paper will provide\nvaluable insights to the community at this pivotal stage in the advancement\ntoward AGI. Summary of works done on RFT for MLLMs is available at\nhttps://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs."}
{"id": "2505.19809", "pdf": "https://arxiv.org/pdf/2505.19809", "abs": "https://arxiv.org/abs/2505.19809", "authors": ["Daniel Ordoez-Apraez", "Alek Frhlich", "Vladimir Kosti", "Karim Lounici", "Vivien Brandt", "Massimiliano Pontil"], "title": "Equivariant Representation Learning for Symmetry-Aware Inference with Guarantees", "categories": ["cs.LG", "cs.AI", "cs.RO", "43-06", "I.2.6; I.2.9; I.5.1"], "comment": null, "summary": "In many real-world applications of regression, conditional probability\nestimation, and uncertainty quantification, exploiting symmetries rooted in\nphysics or geometry can dramatically improve generalization and sample\nefficiency. While geometric deep learning has made significant empirical\nadvances by incorporating group-theoretic structure, less attention has been\ngiven to statistical learning guarantees. In this paper, we introduce an\nequivariant representation learning framework that simultaneously addresses\nregression, conditional probability estimation, and uncertainty quantification\nwhile providing first-of-its-kind non-asymptotic statistical learning\nguarantees. Grounded in operator and group representation theory, our framework\napproximates the spectral decomposition of the conditional expectation\noperator, building representations that are both equivariant and disentangled\nalong independent symmetry subgroups. Empirical evaluations on synthetic\ndatasets and real-world robotics applications confirm the potential of our\napproach, matching or outperforming existing equivariant baselines in\nregression while additionally providing well-calibrated parametric uncertainty\nestimates."}
{"id": "2505.20164", "pdf": "https://arxiv.org/pdf/2505.20164", "abs": "https://arxiv.org/abs/2505.20164", "authors": ["Dairu Liu", "Ziyue Wang", "Minyuan Ruan", "Fuwen Luo", "Chi Chen", "Peng Li", "Yang Liu"], "title": "Visual Abstract Thinking Empowers Multimodal Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Images usually convey richer detail than text, but often include redundant\ninformation which potentially downgrades multimodal reasoning performance. When\nfaced with lengthy or complex messages, humans tend to employ abstract thinking\nto convert them into simple and concise abstracts. Inspired by this cognitive\nstrategy, we introduce Visual Abstract Thinking (VAT), a novel thinking\nparadigm that prompts Multimodal Large Language Models (MLLMs) with visual\nabstract instead of explicit verbal thoughts or elaborate guidance, permitting\na more concentrated visual reasoning mechanism. Explicit thinking, such as\nChain-of-thought (CoT) or tool-augmented approaches, increases the complexity\nof reasoning process via inserting verbose intermediate steps, external\nknowledge or visual information. In contrast, VAT reduces redundant visual\ninformation and encourages models to focus their reasoning on more essential\nvisual elements. Experimental results show that VAT consistently empowers\ndifferent models, and achieves an average gain of 17% over GPT-4o baseline by\nemploying diverse types of visual abstracts, demonstrating that VAT can enhance\nvisual reasoning abilities for MLLMs regarding conceptual, structural and\nrelational reasoning tasks. VAT is also compatible with CoT in\nknowledge-intensive multimodal reasoning tasks. These findings highlight the\neffectiveness of visual reasoning via abstract thinking and encourage further\nexploration of more diverse reasoning paradigms from the perspective of human\ncognition."}
{"id": "2505.18556", "pdf": "https://arxiv.org/pdf/2505.18556", "abs": "https://arxiv.org/abs/2505.18556", "authors": ["Jun Zhuang", "Haibo Jin", "Ye Zhang", "Zhengjian Kang", "Wenbin Zhang", "Gaby G. Dagher", "Haohan Wang"], "title": "Exploring the Vulnerability of the Content Moderation Guardrail in Large Language Models via Intent Manipulation", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint, under review. TL;DR: We propose a new two-stage\n  intent-based prompt-refinement framework, IntentPrompt, that aims to explore\n  the vulnerability of LLMs' content moderation guardrails by refining prompts\n  into benign-looking declarative forms via intent manipulation for red-teaming\n  purposes", "summary": "Intent detection, a core component of natural language understanding, has\nconsiderably evolved as a crucial mechanism in safeguarding large language\nmodels (LLMs). While prior work has applied intent detection to enhance LLMs'\nmoderation guardrails, showing a significant success against content-level\njailbreaks, the robustness of these intent-aware guardrails under malicious\nmanipulations remains under-explored. In this work, we investigate the\nvulnerability of intent-aware guardrails and demonstrate that LLMs exhibit\nimplicit intent detection capabilities. We propose a two-stage intent-based\nprompt-refinement framework, IntentPrompt, that first transforms harmful\ninquiries into structured outlines and further reframes them into\ndeclarative-style narratives by iteratively optimizing prompts via feedback\nloops to enhance jailbreak success for red-teaming purposes. Extensive\nexperiments across four public benchmarks and various black-box LLMs indicate\nthat our framework consistently outperforms several cutting-edge jailbreak\nmethods and evades even advanced Intent Analysis (IA) and Chain-of-Thought\n(CoT)-based defenses. Specifically, our \"FSTR+SPIN\" variant achieves attack\nsuccess rates ranging from 88.25% to 96.54% against CoT-based defenses on the\no1 model, and from 86.75% to 97.12% on the GPT-4o model under IA-based\ndefenses. These findings highlight a critical weakness in LLMs' safety\nmechanisms and suggest that intent manipulation poses a growing challenge to\ncontent moderation guardrails."}
{"id": "2505.19820", "pdf": "https://arxiv.org/pdf/2505.19820", "abs": "https://arxiv.org/abs/2505.19820", "authors": ["Feifei Li", "Mi Zhang", "Zhaoxiang Wang", "Min Yang"], "title": "InfoCons: Identifying Interpretable Critical Concepts in Point Clouds via Information Theory", "categories": ["cs.LG"], "comment": "Accepted by ICML 2025 (Poster)", "summary": "Interpretability of point cloud (PC) models becomes imperative given their\ndeployment in safety-critical scenarios such as autonomous vehicles. We focus\non attributing PC model outputs to interpretable critical concepts, defined as\nmeaningful subsets of the input point cloud. To enable human-understandable\ndiagnostics of model failures, an ideal critical subset should be *faithful*\n(preserving points that causally influence predictions) and *conceptually\ncoherent* (forming semantically meaningful structures that align with human\nperception). We propose InfoCons, an explanation framework that applies\ninformation-theoretic principles to decompose the point cloud into 3D concepts,\nenabling the examination of their causal effect on model predictions with\nlearnable priors. We evaluate InfoCons on synthetic datasets for\nclassification, comparing it qualitatively and quantitatively with four\nbaselines. We further demonstrate its scalability and flexibility on two\nreal-world datasets and in two applications that utilize critical scores of PC."}
{"id": "2505.20176", "pdf": "https://arxiv.org/pdf/2505.20176", "abs": "https://arxiv.org/abs/2505.20176", "authors": ["Alkis Koudounas", "Moreno La Quatra", "Eliana Pastor", "Sabato Marco Siniscalchi", "Elena Baralis"], "title": "\"KAN you hear me?\" Exploring Kolmogorov-Arnold Networks for Spoken Language Understanding", "categories": ["cs.CL", "cs.LG", "eess.AS"], "comment": "Accepted at INTERSPEECH 2025", "summary": "Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising\nalternative to traditional neural architectures, yet their application to\nspeech processing remains under explored. This work presents the first\ninvestigation of KANs for Spoken Language Understanding (SLU) tasks. We\nexperiment with 2D-CNN models on two datasets, integrating KAN layers in five\ndifferent configurations within the dense block. The best-performing setup,\nwhich places a KAN layer between two linear layers, is directly applied to\ntransformer-based models and evaluated on five SLU datasets with increasing\ncomplexity. Our results show that KAN layers can effectively replace the linear\nlayers, achieving comparable or superior performance in most cases. Finally, we\nprovide insights into how KAN and linear layers on top of transformers\ndifferently attend to input regions of the raw waveforms."}
{"id": "2505.18562", "pdf": "https://arxiv.org/pdf/2505.18562", "abs": "https://arxiv.org/abs/2505.18562", "authors": ["Xunlian Dai", "Li Zhou", "Benyou Wang", "Haizhou Li"], "title": "From Word to World: Evaluate and Mitigate Culture Bias via Word Association Test", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The human-centered word association test (WAT) serves as a cognitive proxy,\nrevealing sociocultural variations through lexical-semantic patterns. We extend\nthis test into an LLM-adaptive, free-relation task to assess the alignment of\nlarge language models (LLMs) with cross-cultural cognition. To mitigate the\nculture preference, we propose CultureSteer, an innovative approach that\nintegrates a culture-aware steering mechanism to guide semantic representations\ntoward culturally specific spaces. Experiments show that current LLMs exhibit\nsignificant bias toward Western cultural (notably in American) schemas at the\nword association level. In contrast, our model substantially improves\ncross-cultural alignment, surpassing prompt-based methods in capturing diverse\nsemantic associations. Further validation on culture-sensitive downstream tasks\nconfirms its efficacy in fostering cognitive alignment across cultures. This\nwork contributes a novel methodological paradigm for enhancing cultural\nawareness in LLMs, advancing the development of more inclusive language\ntechnologies."}
{"id": "2505.19823", "pdf": "https://arxiv.org/pdf/2505.19823", "abs": "https://arxiv.org/abs/2505.19823", "authors": ["Pengcheng Sun", "Erwu Liu", "Wei Ni", "Rui Wang", "Yuanzhe Geng", "Lijuan Lai", "Abbas Jamalipour"], "title": "LAPA-based Dynamic Privacy Optimization for Wireless Federated Learning in Heterogeneous Environments", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated Learning (FL) is a distributed machine learning paradigm based on\nprotecting data privacy of devices, which however, can still be broken by\ngradient leakage attack via parameter inversion techniques. Differential\nprivacy (DP) technology reduces the risk of private data leakage by adding\nartificial noise to the gradients, but detrimental to the FL utility at the\nsame time, especially in the scenario where the data is Non-Independent\nIdentically Distributed (Non-IID). Based on the impact of heterogeneous data on\naggregation performance, this paper proposes a Lightweight Adaptive Privacy\nAllocation (LAPA) strategy, which assigns personalized privacy budgets to\ndevices in each aggregation round without transmitting any additional\ninformation beyond gradients, ensuring both privacy protection and aggregation\nefficiency. Furthermore, the Deep Deterministic Policy Gradient (DDPG)\nalgorithm is employed to optimize the transmission power, in order to determine\nthe optimal timing at which the adaptively attenuated artificial noise aligns\nwith the communication noise, enabling an effective balance between DP and\nsystem utility. Finally, a reliable aggregation strategy is designed by\nintegrating communication quality and data distribution characteristics, which\nimproves aggregation performance while preserving privacy. Experimental results\ndemonstrate that the personalized noise allocation and dynamic optimization\nstrategy based on LAPA proposed in this paper enhances convergence performance\nwhile satisfying the privacy requirements of FL."}
{"id": "2505.20184", "pdf": "https://arxiv.org/pdf/2505.20184", "abs": "https://arxiv.org/abs/2505.20184", "authors": ["Yongan Yu", "Mengqian Wu", "Yiran Lin", "Nikki G. Lobczowski"], "title": "THiNK: Can Large Language Models Think-aloud?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Assessing higher-order thinking skills in large language models (LLMs)\nremains a fundamental challenge, especially in tasks that go beyond\nsurface-level accuracy. In this work, we propose THiNK (Testing Higher-order\nNotion of Knowledge), a multi-agent, feedback-driven evaluation framework\ngrounded in Bloom's Taxonomy. THiNK frames reasoning assessment as an iterative\ntask of problem generation, critique, and revision, encouraging LLMs to\nthink-aloud through step-by-step reflection and refinement. This enables a\nsystematic evaluation of both lower-order (e.g., remember, understand) and\nhigher-order (e.g., evaluate, create) thinking skills. We apply THiNK to seven\nstate-of-the-art LLMs and perform a detailed cognitive analysis of their\noutputs. Results reveal that while models reliably perform lower-order\ncategories well, they struggle with applying knowledge in realistic contexts\nand exhibit limited abstraction. Structured feedback loops significantly\nimprove reasoning performance, particularly in higher-order thinking.\nQualitative evaluations further confirm that THiNK-guided outputs better align\nwith domain logic and problem structure. The code of our framework provides a\nscalable methodology for probing and enhancing LLM reasoning, offering new\ndirections for evaluation grounded in learning science, which is available at\nour GitHub repository."}
{"id": "2505.18563", "pdf": "https://arxiv.org/pdf/2505.18563", "abs": "https://arxiv.org/abs/2505.18563", "authors": ["Yisu Wang", "Ruilong Wu", "Xinjiao Li", "Dirk Kutscher"], "title": "PacTrain: Pruning and Adaptive Sparse Gradient Compression for Efficient Collective Communication in Distributed Deep Learning", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "Large-scale deep neural networks (DNN) exhibit excellent performance for\nvarious tasks. As DNNs and datasets grow, distributed training becomes\nextremely time-consuming and demands larger clusters. A main bottleneck is the\nresulting gradient aggregation overhead. While gradient compression and sparse\ncollective communication techniques are commonly employed to alleviate network\nload, many gradient compression schemes do not achieve acceleration of the\ntraining process while also preserving accuracy. This paper introduces\nPacTrain, a novel framework that accelerates distributed training by combining\npruning with sparse gradient compression. Active pruning of the neural network\nmakes the model weights and gradients sparse. By ensuring the global knowledge\nof the gradient sparsity among all distributed training workers, we can perform\nlightweight compression communication without harming accuracy. We show that\nthe PacTrain compression scheme achieves a near-optimal compression strategy\nwhile remaining compatible with the all-reduce primitive. Experimental\nevaluations show that PacTrain improves training throughput by 1.25 to 8.72\ntimes compared to state-of-the-art compression-enabled systems for\nrepresentative vision and language models training tasks under\nbandwidth-constrained conditions."}
{"id": "2505.19825", "pdf": "https://arxiv.org/pdf/2505.19825", "abs": "https://arxiv.org/abs/2505.19825", "authors": ["Tassilo Klein", "Johannes Hoffart"], "title": "Foundation Models for Tabular Data within Systemic Contexts Need Grounding", "categories": ["cs.LG", "cs.AI", "cs.DB"], "comment": null, "summary": "Current research on tabular foundation models often overlooks the\ncomplexities of large-scale, real-world data by treating tables as isolated\nentities and assuming information completeness, thereby neglecting the vital\noperational context. To address this, we introduce the concept of Semantically\nLinked Tables (SLT), recognizing that tables are inherently connected to both\ndeclarative and procedural operational knowledge. We propose Foundation Models\nfor Semantically Linked Tables (FMSLT), which integrate these components to\nground tabular data within its true operational context. This comprehensive\nrepresentation unlocks the full potential of machine learning for complex,\ninterconnected tabular data across diverse domains. Realizing FMSLTs requires\naccess to operational knowledge that is often unavailable in public datasets,\nhighlighting the need for close collaboration between domain experts and\nresearchers. Our work exposes the limitations of current tabular foundation\nmodels and proposes a new direction centered on FMSLTs, aiming to advance\nrobust, context-aware models for structured data."}
{"id": "2505.20195", "pdf": "https://arxiv.org/pdf/2505.20195", "abs": "https://arxiv.org/abs/2505.20195", "authors": ["Xiaorong Wang", "Ting Yang", "Zhu Zhang", "Shuo Wang", "Zihan Zhou", "Liner Yang", "Zhiyuan Liu", "Maosong Sun"], "title": "Monocle: Hybrid Local-Global In-Context Evaluation for Long-Text Generation with Uncertainty-Based Active Learning", "categories": ["cs.CL"], "comment": null, "summary": "Assessing the quality of long-form, model-generated text is challenging, even\nwith advanced LLM-as-a-Judge methods, due to performance degradation as input\nlength increases. To address this issue, we propose a divide-and-conquer\napproach, which breaks down the comprehensive evaluation task into a series of\nlocalized scoring tasks, followed by a final global assessment. This strategy\nallows for more granular and manageable evaluations, ensuring that each segment\nof the text is assessed in isolation for both coherence and quality, while also\naccounting for the overall structure and consistency of the entire piece.\nMoreover, we introduce a hybrid in-context learning approach that leverages\nhuman annotations to enhance the performance of both local and global\nevaluations. By incorporating human-generated feedback directly into the\nevaluation process, this method allows the model to better align with human\njudgment. Finally, we develop an uncertainty-based active learning algorithm\nthat efficiently selects data samples for human annotation, thereby reducing\nannotation costs in practical scenarios. Experimental results show that the\nproposed evaluation framework outperforms several representative baselines,\nhighlighting the effectiveness of our approach."}
{"id": "2505.18568", "pdf": "https://arxiv.org/pdf/2505.18568", "abs": "https://arxiv.org/abs/2505.18568", "authors": ["Zhikang Chen", "Abudukelimu Wuerkaixi", "Sen Cui", "Haoxuan Li", "Ding Li", "Jingfeng Zhang", "Bo Han", "Gang Niu", "Houfang Liu", "Yi Yang", "Sifan Yang", "Changshui Zhang", "Tianling Ren"], "title": "Learning without Isolation: Pathway Protection for Continual Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "23 pages", "summary": "Deep networks are prone to catastrophic forgetting during sequential task\nlearning, i.e., losing the knowledge about old tasks upon learning new tasks.\nTo this end, continual learning(CL) has emerged, whose existing methods focus\nmostly on regulating or protecting the parameters associated with the previous\ntasks. However, parameter protection is often impractical, since the size of\nparameters for storing the old-task knowledge increases linearly with the\nnumber of tasks, otherwise it is hard to preserve the parameters related to the\nold-task knowledge. In this work, we bring a dual opinion from neuroscience and\nphysics to CL: in the whole networks, the pathways matter more than the\nparameters when concerning the knowledge acquired from the old tasks. Following\nthis opinion, we propose a novel CL framework, learning without isolation(LwI),\nwhere model fusion is formulated as graph matching and the pathways occupied by\nthe old tasks are protected without being isolated. Thanks to the sparsity of\nactivation channels in a deep network, LwI can adaptively allocate available\npathways for a new task, realizing pathway protection and addressing\ncatastrophic forgetting in a parameter-efficient manner. Experiments on popular\nbenchmark datasets demonstrate the superiority of the proposed LwI."}
{"id": "2505.19827", "pdf": "https://arxiv.org/pdf/2505.19827", "abs": "https://arxiv.org/abs/2505.19827", "authors": ["Noga Bar", "Mariia Seleznova", "Yotam Alexander", "Gitta Kutyniok", "Raja Giryes"], "title": "Revisiting Glorot Initialization for Long-Range Linear Recurrences", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Proper initialization is critical for Recurrent Neural Networks (RNNs),\nparticularly in long-range reasoning tasks, where repeated application of the\nsame weight matrix can cause vanishing or exploding signals. A common baseline\nfor linear recurrences is Glorot initialization, designed to ensure stable\nsignal propagation--but derived under the infinite-width, fixed-length\nregime--an unrealistic setting for RNNs processing long sequences. In this\nwork, we show that Glorot initialization is in fact unstable: small positive\ndeviations in the spectral radius are amplified through time and cause the\nhidden state to explode. Our theoretical analysis demonstrates that sequences\nof length $t = O(\\sqrt{n})$, where $n$ is the hidden width, are sufficient to\ninduce instability. To address this, we propose a simple, dimension-aware\nrescaling of Glorot that shifts the spectral radius slightly below one,\npreventing rapid signal explosion or decay. These results suggest that standard\ninitialization schemes may break down in the long-sequence regime, motivating a\nseparate line of theory for stable recurrent initialization."}
{"id": "2505.20199", "pdf": "https://arxiv.org/pdf/2505.20199", "abs": "https://arxiv.org/abs/2505.20199", "authors": ["Pengxiang Li", "Shilin Yan", "Joey Tsai", "Renrui Zhang", "Ruichuan An", "Ziyu Guo", "Xiaowei Gao"], "title": "Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking", "categories": ["cs.CL"], "comment": "Project page: https://github.com/pixeli99/A-CFG", "summary": "Classifier-Free Guidance (CFG) significantly enhances controllability in\ngenerative models by interpolating conditional and unconditional predictions.\nHowever, standard CFG often employs a static unconditional input, which can be\nsuboptimal for iterative generation processes where model uncertainty varies\ndynamically. We introduce Adaptive Classifier-Free Guidance (A-CFG), a novel\nmethod that tailors the unconditional input by leveraging the model's\ninstantaneous predictive confidence. At each step of an iterative (masked)\ndiffusion language model, A-CFG identifies tokens in the currently generated\nsequence for which the model exhibits low confidence. These tokens are\ntemporarily re-masked to create a dynamic, localized unconditional input. This\nfocuses CFG's corrective influence precisely on areas of ambiguity, leading to\nmore effective guidance. We integrate A-CFG into a state-of-the-art masked\ndiffusion language model and demonstrate its efficacy. Experiments on diverse\nlanguage generation benchmarks show that A-CFG yields substantial improvements\nover standard CFG, achieving, for instance, a 3.9 point gain on GPQA. Our work\nhighlights the benefit of dynamically adapting guidance mechanisms to model\nuncertainty in iterative generation."}
{"id": "2505.18572", "pdf": "https://arxiv.org/pdf/2505.18572", "abs": "https://arxiv.org/abs/2505.18572", "authors": ["Yifan Zhu", "Chao Zhang", "Xin Shi", "Xueqiao Zhang", "Yi Yang", "Yawei Luo"], "title": "MASTER: Multi-Agent Security Through Exploration of Roles and Topological Structures -- A Comprehensive Framework", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs)-based Multi-Agent Systems (MAS) exhibit\nremarkable problem-solving and task planning capabilities across diverse\ndomains due to their specialized agentic roles and collaborative interactions.\nHowever, this also amplifies the severity of security risks under MAS attacks.\nTo address this, we introduce MASTER, a novel security research framework for\nMAS, focusing on diverse Role configurations and Topological structures across\nvarious scenarios. MASTER offers an automated construction process for\ndifferent MAS setups and an information-flow-based interaction paradigm. To\ntackle MAS security challenges in varied scenarios, we design a\nscenario-adaptive, extensible attack strategy utilizing role and topological\ninformation, which dynamically allocates targeted, domain-specific attack tasks\nfor collaborative agent execution. Our experiments demonstrate that such an\nattack, leveraging role and topological information, exhibits significant\ndestructive potential across most models. Additionally, we propose\ncorresponding defense strategies, substantially enhancing MAS resilience across\ndiverse scenarios. We anticipate that our framework and findings will provide\nvaluable insights for future research into MAS security challenges."}
{"id": "2505.19842", "pdf": "https://arxiv.org/pdf/2505.19842", "abs": "https://arxiv.org/abs/2505.19842", "authors": ["Shuo Wang", "Yun Cheng", "Qingye Meng", "Olga Saukh", "Jiang Zhang", "Jingfang Fan", "Yuanting Zhang", "Xingyuan Yuan", "Lothar Thiele"], "title": "PCDCNet: A Surrogate Model for Air Quality Forecasting with Physical-Chemical Dynamics and Constraints", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Air quality forecasting (AQF) is critical for public health and environmental\nmanagement, yet remains challenging due to the complex interplay of emissions,\nmeteorology, and chemical transformations. Traditional numerical models, such\nas CMAQ and WRF-Chem, provide physically grounded simulations but are\ncomputationally expensive and rely on uncertain emission inventories. Deep\nlearning models, while computationally efficient, often struggle with\ngeneralization due to their lack of physical constraints. To bridge this gap,\nwe propose PCDCNet, a surrogate model that integrates numerical modeling\nprinciples with deep learning. PCDCNet explicitly incorporates emissions,\nmeteorological influences, and domain-informed constraints to model pollutant\nformation, transport, and dissipation. By combining graph-based spatial\ntransport modeling, recurrent structures for temporal accumulation, and\nrepresentation enhancement for local interactions, PCDCNet achieves\nstate-of-the-art (SOTA) performance in 72-hour station-level PM2.5 and O3\nforecasting while significantly reducing computational costs. Furthermore, our\nmodel is deployed in an online platform, providing free, real-time air quality\nforecasts, demonstrating its scalability and societal impact. By aligning deep\nlearning with physical consistency, PCDCNet offers a practical and\ninterpretable solution for AQF, enabling informed decision-making for both\npersonal and regulatory applications."}
{"id": "2505.20201", "pdf": "https://arxiv.org/pdf/2505.20201", "abs": "https://arxiv.org/abs/2505.20201", "authors": ["Mohit Chandra", "Siddharth Sriraman", "Harneet Singh Khanuja", "Yiqiao Jin", "Munmun De Choudhury"], "title": "Reasoning Is Not All You Need: Examining LLMs for Multi-Turn Mental Health Conversations", "categories": ["cs.CL"], "comment": "33 pages, 5 figures, 30 tables", "summary": "Limited access to mental healthcare, extended wait times, and increasing\ncapabilities of Large Language Models (LLMs) has led individuals to turn to\nLLMs for fulfilling their mental health needs. However, examining the\nmulti-turn mental health conversation capabilities of LLMs remains\nunder-explored. Existing evaluation frameworks typically focus on diagnostic\naccuracy and win-rates and often overlook alignment with patient-specific\ngoals, values, and personalities required for meaningful conversations. To\naddress this, we introduce MedAgent, a novel framework for synthetically\ngenerating realistic, multi-turn mental health sensemaking conversations and\nuse it to create the Mental Health Sensemaking Dialogue (MHSD) dataset,\ncomprising over 2,200 patient-LLM conversations. Additionally, we present\nMultiSenseEval, a holistic framework to evaluate the multi-turn conversation\nabilities of LLMs in healthcare settings using human-centric criteria. Our\nfindings reveal that frontier reasoning models yield below-par performance for\npatient-centric communication and struggle at advanced diagnostic capabilities\nwith average score of 31%. Additionally, we observed variation in model\nperformance based on patient's persona and performance drop with increasing\nturns in the conversation. Our work provides a comprehensive synthetic data\ngeneration framework, a dataset and evaluation framework for assessing LLMs in\nmulti-turn mental health conversations."}
{"id": "2505.18574", "pdf": "https://arxiv.org/pdf/2505.18574", "abs": "https://arxiv.org/abs/2505.18574", "authors": ["Charles Hong", "Sahil Bhatia", "Alvin Cheung", "Yakun Sophia Shao"], "title": "Autocomp: LLM-Driven Code Optimization for Tensor Accelerators", "categories": ["cs.PL", "cs.AI", "cs.AR", "cs.LG"], "comment": null, "summary": "Hardware accelerators, especially those designed for tensor processing, have\nbecome ubiquitous in today's computing landscape. However, even with\nsignificant efforts in building compilers, programming these tensor\naccelerators remains challenging, leaving much of their potential\nunderutilized. Recently, large language models (LLMs), trained on large amounts\nof code, have shown significant promise in code generation and optimization\ntasks, but generating low-resource languages like specialized tensor\naccelerator code still poses a significant challenge. We tackle this challenge\nwith Autocomp, an approach that empowers accelerator programmers to leverage\ndomain knowledge and hardware feedback to optimize code via an automated\nLLM-driven search. We accomplish this by: 1) formulating each optimization pass\nas a structured two-phase prompt, divided into planning and code generation\nphases, 2) inserting domain knowledge during planning via a concise and\nadaptable optimization menu, and 3) integrating correctness and performance\nmetrics from hardware as feedback at each search iteration. Across three\ncategories of representative workloads and two different accelerators, we\ndemonstrate that Autocomp-optimized code runs 5.6x (GEMM) and 2.7x\n(convolution) faster than the vendor-provided library, and outperforms\nexpert-level hand-tuned code by 1.4x (GEMM), 1.1x (convolution), and 1.3x\n(fine-grained linear algebra). Additionally, we demonstrate that optimization\nschedules generated from Autocomp can be reused across similar tensor\noperations, improving speedups by up to 24% under a fixed sample budget."}
{"id": "2505.19850", "pdf": "https://arxiv.org/pdf/2505.19850", "abs": "https://arxiv.org/abs/2505.19850", "authors": ["Leander Diaz-Bone", "Marco Bagatella", "Jonas Hbotter", "Andreas Krause"], "title": "DISCOVER: Automated Curricula for Sparse-Reward Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Sparse-reward reinforcement learning (RL) can model a wide range of highly\ncomplex tasks. Solving sparse-reward tasks is RL's core premise - requiring\nefficient exploration coupled with long-horizon credit assignment - and\novercoming these challenges is key for building self-improving agents with\nsuperhuman ability. We argue that solving complex and high-dimensional tasks\nrequires solving simpler tasks that are relevant to the target task. In\ncontrast, most prior work designs strategies for selecting exploratory tasks\nwith the objective of solving any task, making exploration of challenging\nhigh-dimensional, long-horizon tasks intractable. We find that the sense of\ndirection, necessary for effective exploration, can be extracted from existing\nRL algorithms, without needing any prior information. Based on this finding, we\npropose a method for directed sparse-reward goal-conditioned very long-horizon\nRL (DISCOVER), which selects exploratory goals in the direction of the target\ntask. We connect DISCOVER to principled exploration in bandits, formally\nbounding the time until the target task becomes achievable in terms of the\nagent's initial distance to the target, but independent of the volume of the\nspace of all tasks. Empirically, we perform a thorough evaluation in\nhigh-dimensional environments. We find that the directed goal selection of\nDISCOVER solves exploration problems that are beyond the reach of prior\nstate-of-the-art exploration methods in RL."}
{"id": "2505.20209", "pdf": "https://arxiv.org/pdf/2505.20209", "abs": "https://arxiv.org/abs/2505.20209", "authors": ["Joe Stacey", "Lisa Alazraki", "Aran Ubhi", "Beyza Ermis", "Aaron Mueller", "Marek Rei"], "title": "How to Improve the Robustness of Closed-Source Models on NLI", "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Closed-source Large Language Models (LLMs) have become increasingly popular,\nwith impressive performance across a wide range of natural language tasks.\nThese models can be fine-tuned to further improve performance, but this often\nresults in the models learning from dataset-specific heuristics that reduce\ntheir robustness on out-of-distribution (OOD) data. Existing methods to improve\nrobustness either perform poorly, or are non-applicable to closed-source models\nbecause they assume access to model internals, or the ability to change the\nmodel's training procedure. In this work, we investigate strategies to improve\nthe robustness of closed-source LLMs through data-centric methods that do not\nrequire access to model internals. We find that the optimal strategy depends on\nthe complexity of the OOD data. For highly complex OOD datasets, upsampling\nmore challenging training examples can improve robustness by up to 1.5%. For\nless complex OOD datasets, replacing a portion of the training set with\nLLM-generated examples can improve robustness by 3.7%. More broadly, we find\nthat large-scale closed-source autoregressive LLMs are substantially more\nrobust than commonly used encoder models, and are a more appropriate choice of\nbaseline going forward."}
{"id": "2505.18581", "pdf": "https://arxiv.org/pdf/2505.18581", "abs": "https://arxiv.org/abs/2505.18581", "authors": ["Wentao Hu", "Wengyu Zhang", "Yiyang Jiang", "Chen Jason Zhang", "Xiaoyong Wei", "Qing Li"], "title": "Removal of Hallucination on Hallucination: Debate-Augmented RAG", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025", "summary": "Retrieval-Augmented Generation (RAG) enhances factual accuracy by integrating\nexternal knowledge, yet it introduces a critical issue: erroneous or biased\nretrieval can mislead generation, compounding hallucinations, a phenomenon we\nterm Hallucination on Hallucination. To address this, we propose\nDebate-Augmented RAG (DRAG), a training-free framework that integrates\nMulti-Agent Debate (MAD) mechanisms into both retrieval and generation stages.\nIn retrieval, DRAG employs structured debates among proponents, opponents, and\njudges to refine retrieval quality and ensure factual reliability. In\ngeneration, DRAG introduces asymmetric information roles and adversarial\ndebates, enhancing reasoning robustness and mitigating factual inconsistencies.\nEvaluations across multiple tasks demonstrate that DRAG improves retrieval\nreliability, reduces RAG-induced hallucinations, and significantly enhances\noverall factual accuracy. Our code is available at\nhttps://github.com/Huenao/Debate-Augmented-RAG."}
{"id": "2505.19855", "pdf": "https://arxiv.org/pdf/2505.19855", "abs": "https://arxiv.org/abs/2505.19855", "authors": ["Zexi Li", "Xiangzhu Wang", "William F. Shen", "Meghdad Kurmanji", "Xinchi Qiu", "Dongqi Cai", "Chao Wu", "Nicholas D. Lane"], "title": "Editing as Unlearning: Are Knowledge Editing Methods Strong Baselines for Large Language Model Unlearning?", "categories": ["cs.LG"], "comment": "Preprint", "summary": "Large language Model (LLM) unlearning, i.e., selectively removing information\nfrom LLMs, is vital for responsible model deployment. Differently, LLM\nknowledge editing aims to modify LLM knowledge instead of removing it. Though\nediting and unlearning seem to be two distinct tasks, we find there is a tight\nconnection between them. In this paper, we conceptualize unlearning as a\nspecial case of editing where information is modified to a refusal or \"empty\nset\" $\\emptyset$ response, signifying its removal. This paper thus investigates\nif knowledge editing techniques are strong baselines for LLM unlearning. We\nevaluate state-of-the-art (SOTA) editing methods (e.g., ROME, MEMIT, GRACE,\nWISE, and AlphaEdit) against existing unlearning approaches on pretrained and\nfinetuned knowledge. Results show certain editing methods, notably WISE and\nAlphaEdit, are effective unlearning baselines, especially for pretrained\nknowledge, and excel in generating human-aligned refusal answers. To better\nadapt editing methods for unlearning applications, we propose practical recipes\nincluding self-improvement and query merging. The former leverages the LLM's\nown in-context learning ability to craft a more human-aligned unlearning\ntarget, and the latter enables ROME and MEMIT to perform well in unlearning\nlonger sample sequences. We advocate for the unlearning community to adopt SOTA\nediting methods as baselines and explore unlearning from an editing perspective\nfor more holistic LLM memory control."}
{"id": "2505.20215", "pdf": "https://arxiv.org/pdf/2505.20215", "abs": "https://arxiv.org/abs/2505.20215", "authors": ["Paolo Gajo", "Domenic Rosati", "Hassan Sajjad", "Alberto Barrn-Cedeo"], "title": "Dependency Parsing is More Parameter-Efficient with Normalization", "categories": ["cs.CL"], "comment": null, "summary": "Dependency parsing is the task of inferring natural language structure, often\napproached by modeling word interactions via attention through biaffine\nscoring. This mechanism works like self-attention in Transformers, where scores\nare calculated for every pair of words in a sentence. However, unlike\nTransformer attention, biaffine scoring does not use normalization prior to\ntaking the softmax of the scores. In this paper, we provide theoretical\nevidence and empirical results revealing that a lack of normalization\nnecessarily results in overparameterized parser models, where the extra\nparameters compensate for the sharp softmax outputs produced by high variance\ninputs to the biaffine scoring function. We argue that biaffine scoring can be\nmade substantially more efficient by performing score normalization. We conduct\nexperiments on six datasets for semantic and syntactic dependency parsing using\na one-hop parser. We train N-layer stacked BiLSTMs and evaluate the parser's\nperformance with and without normalizing biaffine scores. Normalizing allows us\nto beat the state of the art on two datasets, with fewer samples and trainable\nparameters. Code: https://anonymous.4open.science/r/EfficientSDP-70C1"}
{"id": "2505.18582", "pdf": "https://arxiv.org/pdf/2505.18582", "abs": "https://arxiv.org/abs/2505.18582", "authors": ["Dongyang Jin", "Chao Fan", "Jingzhe Ma", "Jingkai Zhou", "Weihua Chen", "Shiqi Yu"], "title": "On Denoising Walking Videos for Gait Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "8pages, 4 figures", "summary": "To capture individual gait patterns, excluding identity-irrelevant cues in\nwalking videos, such as clothing texture and color, remains a persistent\nchallenge for vision-based gait recognition. Traditional silhouette- and\npose-based methods, though theoretically effective at removing such\ndistractions, often fall short of high accuracy due to their sparse and less\ninformative inputs. Emerging end-to-end methods address this by directly\ndenoising RGB videos using human priors. Building on this trend, we propose\nDenoisingGait, a novel gait denoising method. Inspired by the philosophy that\n\"what I cannot create, I do not understand\", we turn to generative diffusion\nmodels, uncovering how they partially filter out irrelevant factors for gait\nunderstanding. Additionally, we introduce a geometry-driven Feature Matching\nmodule, which, combined with background removal via human silhouettes,\ncondenses the multi-channel diffusion features at each foreground pixel into a\ntwo-channel direction vector. Specifically, the proposed within- and\ncross-frame matching respectively capture the local vectorized structures of\ngait appearance and motion, producing a novel flow-like gait representation\ntermed Gait Feature Field, which further reduces residual noise in diffusion\nfeatures. Experiments on the CCPG, CASIA-B*, and SUSTech1K datasets demonstrate\nthat DenoisingGait achieves a new SoTA performance in most cases for both\nwithin- and cross-domain evaluations. Code is available at\nhttps://github.com/ShiqiYu/OpenGait."}
{"id": "2505.19867", "pdf": "https://arxiv.org/pdf/2505.19867", "abs": "https://arxiv.org/abs/2505.19867", "authors": ["Yavar Taheri Yeganeh", "Mohsen Jafari", "Andrea Matta"], "title": "Deep Active Inference Agents for Delayed and Long-Horizon Environments", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "With the recent success of world-model agents, which extend the core idea of\nmodel-based reinforcement learning by learning a differentiable model for\nsample-efficient control across diverse tasks, active inference (AIF) offers a\ncomplementary, neuroscience-grounded paradigm that unifies perception,\nlearning, and action within a single probabilistic framework powered by a\ngenerative model. Despite this promise, practical AIF agents still rely on\naccurate immediate predictions and exhaustive planning, a limitation that is\nexacerbated in delayed environments requiring plans over long horizons, tens to\nhundreds of steps. Moreover, most existing agents are evaluated on robotic or\nvision benchmarks which, while natural for biological agents, fall short of\nreal-world industrial complexity. We address these limitations with a\ngenerative-policy architecture featuring (i) a multi-step latent transition\nthat lets the generative model predict an entire horizon in a single\nlook-ahead, (ii) an integrated policy network that enables the transition and\nreceives gradients of the expected free energy, (iii) an alternating\noptimization scheme that updates model and policy from a replay buffer, and\n(iv) a single gradient step that plans over long horizons, eliminating\nexhaustive planning from the control loop. We evaluate our agent in an\nenvironment that mimics a realistic industrial scenario with delayed and\nlong-horizon settings. The empirical results confirm the effectiveness of the\nproposed approach, demonstrating the coupled world-model with the AIF formalism\nyields an end-to-end probabilistic controller capable of effective decision\nmaking in delayed, long-horizon settings without handcrafted rewards or\nexpensive planning."}
{"id": "2505.20225", "pdf": "https://arxiv.org/pdf/2505.20225", "abs": "https://arxiv.org/abs/2505.20225", "authors": ["Hao Kang", "Zichun Yu", "Chenyan Xiong"], "title": "FLAME-MoE: A Transparent End-to-End Research Platform for Mixture-of-Experts Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "All code, training logs, and model checkpoints are available at\n  https://github.com/cmu-flame/FLAME-MoE", "summary": "Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4\nincreasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong\nefficiency-performance trade-offs by activating only a fraction of the model\nper token. Yet academic researchers still lack a fully open, end-to-end MoE\nplatform for investigating scaling, routing, and expert behavior. We release\nFLAME-MoE, a completely open-source research suite composed of seven\ndecoder-only models, ranging from 38M to 1.7B active parameters, whose\narchitecture--64 experts with top-8 gating and 2 shared experts--closely\nreflects modern production LLMs. All training data pipelines, scripts, logs,\nand checkpoints are publicly available to enable reproducible experimentation.\nAcross six evaluation tasks, FLAME-MoE improves average accuracy by up to 3.4\npoints over dense baselines trained with identical FLOPs. Leveraging full\ntraining trace transparency, we present initial analyses showing that (i)\nexperts increasingly specialize on distinct token subsets, (ii) co-activation\nmatrices remain sparse, reflecting diverse expert usage, and (iii) routing\nbehavior stabilizes early in training. All code, training logs, and model\ncheckpoints are available at https://github.com/cmu-flame/FLAME-MoE."}
{"id": "2505.18587", "pdf": "https://arxiv.org/pdf/2505.18587", "abs": "https://arxiv.org/abs/2505.18587", "authors": ["Pavan C Shekar", "Pawan Soni", "Vivek Kanhangad"], "title": "HyperFake: Hyperspectral Reconstruction and Attention-Guided Analysis for Advanced Deepfake Detection", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 3 figures, 1 table. Preliminary results on FaceForensics++\n  dataset. First approach to use hyperspectral reconstruction for deepfake\n  detection", "summary": "Deepfakes pose a significant threat to digital media security, with current\ndetection methods struggling to generalize across different manipulation\ntechniques and datasets. While recent approaches combine CNN-based\narchitectures with Vision Transformers or leverage multi-modal learning, they\nremain limited by the inherent constraints of RGB data. We introduce HyperFake,\na novel deepfake detection pipeline that reconstructs 31-channel hyperspectral\ndata from standard RGB videos, revealing hidden manipulation traces invisible\nto conventional methods. Using an improved MST++ architecture, HyperFake\nenhances hyperspectral reconstruction, while a spectral attention mechanism\nselects the most critical spectral features for deepfake detection. The refined\nspectral data is then processed by an EfficientNet-based classifier optimized\nfor spectral analysis, enabling more accurate and generalizable detection\nacross different deepfake styles and datasets, all without the need for\nexpensive hyperspectral cameras. To the best of our knowledge, this is the\nfirst approach to leverage hyperspectral imaging reconstruction for deepfake\ndetection, opening new possibilities for detecting increasingly sophisticated\nmanipulations."}
{"id": "2505.19888", "pdf": "https://arxiv.org/pdf/2505.19888", "abs": "https://arxiv.org/abs/2505.19888", "authors": ["Eun Gyung Kong", "Je Won Yeom", "Yonghoon Jeon", "Taesup Kim"], "title": "Generalized and Personalized Federated Learning with Foundation Models via Orthogonal Transformations", "categories": ["cs.LG"], "comment": "27 pages, 5 figures", "summary": "Federated Learning (FL) aims to train models across decentralized clients or\ndevices holding local data without the need for centralized data collection,\nthus enhancing data privacy and security. However, achieving both\ngeneralization and personalization in heterogeneous settings remains a\nsignificant challenge. To address this, we introduce FedOT, a novel approach\nthat leverages black-box foundation models. FedOT shares only a global\ntask-dependent classifier across clients while locally adapting features\nthrough orthogonal transformations. By enforcing orthogonality, FedOT mitigates\ngradient conflicts across diverse clients, preserves semantic integrity, and\nachieves robust performance even in the presence of substantial data\nheterogeneity. The strategy of combining global and local parameters enables a\nmore balanced approach for both generalization and personalization,\noutperforming baseline FL methods across multiple benchmarks. Furthermore, our\nextensive analysis confirms that joint optimization of global classifiers and\nlocal orthogonal transformations yields superior performance and suggests\nbroader applicability."}
{"id": "2505.20231", "pdf": "https://arxiv.org/pdf/2505.20231", "abs": "https://arxiv.org/abs/2505.20231", "authors": ["Yiming Du", "Bingbing Wang", "Yang He", "Bin Liang", "Baojun Wang", "Zhongyang Li", "Lin Gui", "Jeff Z. Pan", "Ruifeng Xu", "Kam-Fai Wong"], "title": "Bridging the Long-Term Gap: A Memory-Active Policy for Multi-Session Task-Oriented Dialogue", "categories": ["cs.CL"], "comment": null, "summary": "Existing Task-Oriented Dialogue (TOD) systems primarily focus on\nsingle-session dialogues, limiting their effectiveness in long-term memory\naugmentation. To address this challenge, we introduce a MS-TOD dataset, the\nfirst multi-session TOD dataset designed to retain long-term memory across\nsessions, enabling fewer turns and more efficient task completion. This defines\na new benchmark task for evaluating long-term memory in multi-session TOD.\nBased on this new dataset, we propose a Memory-Active Policy (MAP) that\nimproves multi-session dialogue efficiency through a two-stage approach. 1)\nMemory-Guided Dialogue Planning retrieves intent-aligned history, identifies\nkey QA units via a memory judger, refines them by removing redundant questions,\nand generates responses based on the reconstructed memory. 2) Proactive\nResponse Strategy detects and correct errors or omissions, ensuring efficient\nand accurate task completion. We evaluate MAP on MS-TOD dataset, focusing on\nresponse quality and effectiveness of the proactive strategy. Experiments on\nMS-TOD demonstrate that MAP significantly improves task success and turn\nefficiency in multi-session scenarios, while maintaining competitive\nperformance on conventional single-session tasks."}
{"id": "2505.18588", "pdf": "https://arxiv.org/pdf/2505.18588", "abs": "https://arxiv.org/abs/2505.18588", "authors": ["Zesheng Shi", "Yucheng Zhou", "Jing Li"], "title": "Safety Alignment via Constrained Knowledge Unlearning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite significant progress in safety alignment, large language models\n(LLMs) remain susceptible to jailbreak attacks. Existing defense mechanisms\nhave not fully deleted harmful knowledge in LLMs, which allows such attacks to\nbypass safeguards and produce harmful outputs. To address this challenge, we\npropose a novel safety alignment strategy, Constrained Knowledge Unlearning\n(CKU), which focuses on two primary objectives: knowledge localization and\nretention, and unlearning harmful knowledge. CKU works by scoring neurons in\nspecific multilayer perceptron (MLP) layers to identify a subset U of neurons\nassociated with useful knowledge. During the unlearning process, CKU prunes the\ngradients of neurons in U to preserve valuable knowledge while effectively\nmitigating harmful content. Experimental results demonstrate that CKU\nsignificantly enhances model safety without compromising overall performance,\noffering a superior balance between safety and utility compared to existing\nmethods. Additionally, our analysis of neuron knowledge sensitivity across\nvarious MLP layers provides valuable insights into the mechanics of safety\nalignment and model knowledge editing."}
{"id": "2505.19893", "pdf": "https://arxiv.org/pdf/2505.19893", "abs": "https://arxiv.org/abs/2505.19893", "authors": ["Melis Ilayda Bal", "Volkan Cevher", "Michael Muehlebach"], "title": "ESLM: Risk-Averse Selective Language Modeling for Efficient Pretraining", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large language model pretraining is compute-intensive, yet many tokens\ncontribute marginally to learning, resulting in inefficiency. We introduce\nEfficient Selective Language Modeling (ESLM), a risk-aware algorithm that\nimproves training efficiency and distributional robustness by performing online\ntoken-level batch selection. ESLM leverages per-token statistics (e.g., entropy\nor loss) and applies value-at-risk thresholding to retain only the most\ninformative tokens per batch. This data-centric mechanism reshapes the training\nloss, prioritizing high-risk tokens and eliminating redundant gradient\ncomputation. We frame ESLM as a bilevel game: the model competes with a masking\nadversary that selects worst-case token subsets under a constrained\nthresholding rule. In the loss-based setting, ESLM recovers conditional\nvalue-at-risk loss minimization, providing a principled connection to\ndistributionally robust optimization. We extend our approach to Ada-ESLM, which\nadaptively tunes the selection confidence during training. Experiments on GPT-2\npretraining show that ESLM significantly reduces training FLOPs while\nmaintaining or improving both perplexity and downstream performance compared to\nbaselines. Our approach also scales across model sizes, pretraining corpora,\nand integrates naturally with knowledge distillation."}
{"id": "2505.20237", "pdf": "https://arxiv.org/pdf/2505.20237", "abs": "https://arxiv.org/abs/2505.20237", "authors": ["Yasmin Moslem"], "title": "Efficient Speech Translation through Model Compression and Knowledge Distillation", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "IWSLT 2025", "summary": "Efficient deployment of large audio-language models for speech translation\nremains challenging due to their significant computational requirements. In\nthis paper, we address this challenge through our system submissions to the\n\"Model Compression\" track at the International Conference on Spoken Language\nTranslation (IWSLT 2025). We experiment with a combination of approaches\nincluding iterative layer pruning based on layer importance evaluation,\nlow-rank adaptation with 4-bit quantization (QLoRA), and knowledge\ndistillation. In our experiments, we use Qwen2-Audio-7B-Instruct for speech\ntranslation into German and Chinese. Our pruned (student) models achieve up to\na 50% reduction in both model parameters and storage footprint, while retaining\n97-100% of the translation quality of the in-domain (teacher) models."}
{"id": "2505.18595", "pdf": "https://arxiv.org/pdf/2505.18595", "abs": "https://arxiv.org/abs/2505.18595", "authors": ["The Viet Bui", "Tien Mai", "Hong Thanh Nguyen"], "title": "MisoDICE: Multi-Agent Imitation from Unlabeled Mixed-Quality Demonstrations", "categories": ["cs.LG", "cs.AI", "cs.MA"], "comment": null, "summary": "We study offline imitation learning (IL) in cooperative multi-agent settings,\nwhere demonstrations have unlabeled mixed quality - containing both expert and\nsuboptimal trajectories. Our proposed solution is structured in two stages:\ntrajectory labeling and multi-agent imitation learning, designed jointly to\nenable effective learning from heterogeneous, unlabeled data. In the first\nstage, we combine advances in large language models and preference-based\nreinforcement learning to construct a progressive labeling pipeline that\ndistinguishes expert-quality trajectories. In the second stage, we introduce\nMisoDICE, a novel multi-agent IL algorithm that leverages these labels to learn\nrobust policies while addressing the computational complexity of large joint\nstate-action spaces. By extending the popular single-agent DICE framework to\nmulti-agent settings with a new value decomposition and mixing architecture,\nour method yields a convex policy optimization objective and ensures\nconsistency between global and local policies. We evaluate MisoDICE on multiple\nstandard multi-agent RL benchmarks and demonstrate superior performance,\nespecially when expert data is scarce."}
{"id": "2505.19923", "pdf": "https://arxiv.org/pdf/2505.19923", "abs": "https://arxiv.org/abs/2505.19923", "authors": ["Qin-Wen Luo", "Ming-Kun Xie", "Ye-Wen Wang", "Sheng-Jun Huang"], "title": "Learning to Trust Bellman Updates: Selective State-Adaptive Regularization for Offline RL", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted to ICML 2025", "summary": "Offline reinforcement learning (RL) aims to learn an effective policy from a\nstatic dataset. To alleviate extrapolation errors, existing studies often\nuniformly regularize the value function or policy updates across all states.\nHowever, due to substantial variations in data quality, the fixed\nregularization strength often leads to a dilemma: Weak regularization strength\nfails to address extrapolation errors and value overestimation, while strong\nregularization strength shifts policy learning toward behavior cloning,\nimpeding potential performance enabled by Bellman updates. To address this\nissue, we propose the selective state-adaptive regularization method for\noffline RL. Specifically, we introduce state-adaptive regularization\ncoefficients to trust state-level Bellman-driven results, while selectively\napplying regularization on high-quality actions, aiming to avoid performance\ndegradation caused by tight constraints on low-quality actions. By establishing\na connection between the representative value regularization method, CQL, and\nexplicit policy constraint methods, we effectively extend selective\nstate-adaptive regularization to these two mainstream offline RL approaches.\nExtensive experiments demonstrate that the proposed method significantly\noutperforms the state-of-the-art approaches in both offline and\noffline-to-online settings on the D4RL benchmark."}
{"id": "2505.20243", "pdf": "https://arxiv.org/pdf/2505.20243", "abs": "https://arxiv.org/abs/2505.20243", "authors": ["Bhawna Piryani", "Abdelrahman Abdullah", "Jamshid Mozafari", "Avishek Anand", "Adam Jatowt"], "title": "It's High Time: A Survey of Temporal Information Retrieval and Question Answering", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Time plays a critical role in how information is generated, retrieved, and\ninterpreted. In this survey, we provide a comprehensive overview of Temporal\nInformation Retrieval and Temporal Question Answering, two research areas aimed\nat handling and understanding time-sensitive information. As the amount of\ntime-stamped content from sources like news articles, web archives, and\nknowledge bases increases, systems must address challenges such as detecting\ntemporal intent, normalizing time expressions, ordering events, and reasoning\nover evolving or ambiguous facts. These challenges are critical across many\ndynamic and time-sensitive domains, from news and encyclopedias to science,\nhistory, and social media. We review both traditional approaches and modern\nneural methods, including those that use transformer models and Large Language\nModels (LLMs). We also review recent advances in temporal language modeling,\nmulti-hop reasoning, and retrieval-augmented generation (RAG), alongside\nbenchmark datasets and evaluation strategies that test temporal robustness,\nrecency awareness, and generalization."}
{"id": "2505.18596", "pdf": "https://arxiv.org/pdf/2505.18596", "abs": "https://arxiv.org/abs/2505.18596", "authors": ["Chen Han", "Wenzhen Zheng", "Xijin Tang"], "title": "Debate-to-Detect: Reformulating Misinformation Detection as a Real-World Debate with Large Language Models", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.6; H.3.3"], "comment": null, "summary": "The proliferation of misinformation in digital platforms reveals the\nlimitations of traditional detection methods, which mostly rely on static\nclassification and fail to capture the intricate process of real-world\nfact-checking. Despite advancements in Large Language Models (LLMs) that\nenhance automated reasoning, their application to misinformation detection\nremains hindered by issues of logical inconsistency and superficial\nverification. In response, we introduce Debate-to-Detect (D2D), a novel\nMulti-Agent Debate (MAD) framework that reformulates misinformation detection\nas a structured adversarial debate. Inspired by fact-checking workflows, D2D\nassigns domain-specific profiles to each agent and orchestrates a five-stage\ndebate process, including Opening Statement, Rebuttal, Free Debate, Closing\nStatement, and Judgment. To transcend traditional binary classification, D2D\nintroduces a multi-dimensional evaluation mechanism that assesses each claim\nacross five distinct dimensions: Factuality, Source Reliability, Reasoning\nQuality, Clarity, and Ethics. Experiments with GPT-4o on two fakenews datasets\ndemonstrate significant improvements over baseline methods, and the case study\nhighlight D2D's capability to iteratively refine evidence while improving\ndecision transparency, representing a substantial advancement towards robust\nand interpretable misinformation detection. The code will be open-sourced in a\nfuture release."}
{"id": "2505.19932", "pdf": "https://arxiv.org/pdf/2505.19932", "abs": "https://arxiv.org/abs/2505.19932", "authors": ["Fabian Kresse", "Emily Yu", "Christoph H. Lampert", "Thomas A. Henzinger"], "title": "Logic Gate Neural Networks are Good for Verification", "categories": ["cs.LG", "cs.LO"], "comment": "15 pages, 7 figures, 1 table. Accepted at NeuS 2025; to appear in\n  PMLR Vol. 288", "summary": "Learning-based systems are increasingly deployed across various domains, yet\nthe complexity of traditional neural networks poses significant challenges for\nformal verification. Unlike conventional neural networks, learned Logic Gate\nNetworks (LGNs) replace multiplications with Boolean logic gates, yielding a\nsparse, netlist-like architecture that is inherently more amenable to symbolic\nverification, while still delivering promising performance. In this paper, we\nintroduce a SAT encoding for verifying global robustness and fairness in LGNs.\nWe evaluate our method on five benchmark datasets, including a newly\nconstructed 5-class variant, and find that LGNs are both verification-friendly\nand maintain strong predictive performance."}
{"id": "2505.20245", "pdf": "https://arxiv.org/pdf/2505.20245", "abs": "https://arxiv.org/abs/2505.20245", "authors": ["Rui Li", "Quanyu Dai", "Zeyu Zhang", "Xu Chen", "Zhenhua Dong", "Ji-Rong Wen"], "title": "KnowTrace: Bootstrapping Iterative Retrieval-Augmented Generation with Structured Knowledge Tracing", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by KDD 2025", "summary": "Recent advances in retrieval-augmented generation (RAG) furnish large\nlanguage models (LLMs) with iterative retrievals of relevant information to\nhandle complex multi-hop questions. These methods typically alternate between\nLLM reasoning and retrieval to accumulate external information into the LLM's\ncontext. However, the ever-growing context inherently imposes an increasing\nburden on the LLM to perceive connections among critical information pieces,\nwith futile reasoning steps further exacerbating this overload issue. In this\npaper, we present KnowTrace, an elegant RAG framework to (1) mitigate the\ncontext overload and (2) bootstrap higher-quality multi-step reasoning. Instead\nof simply piling the retrieved contents, KnowTrace autonomously traces out\ndesired knowledge triplets to organize a specific knowledge graph relevant to\nthe input question. Such a structured workflow not only empowers the LLM with\nan intelligible context for inference, but also naturally inspires a reflective\nmechanism of knowledge backtracing to identify contributive LLM generations as\nprocess supervision data for self-bootstrapping. Extensive experiments show\nthat KnowTrace consistently surpasses existing methods across three multi-hop\nquestion answering benchmarks, and the bootstrapped version further amplifies\nthe gains."}
{"id": "2505.18600", "pdf": "https://arxiv.org/pdf/2505.18600", "abs": "https://arxiv.org/abs/2505.18600", "authors": ["Bryan Sangwoo Kim", "Jeongsol Kim", "Jong Chul Ye"], "title": "Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and Preference Alignment", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Modern single-image super-resolution (SISR) models deliver photo-realistic\nresults at the scale factors on which they are trained, but collapse when asked\nto magnify far beyond that regime. We address this scalability bottleneck with\nChain-of-Zoom (CoZ), a model-agnostic framework that factorizes SISR into an\nautoregressive chain of intermediate scale-states with multi-scale-aware\nprompts. CoZ repeatedly re-uses a backbone SR model, decomposing the\nconditional probability into tractable sub-problems to achieve extreme\nresolutions without additional training. Because visual cues diminish at high\nmagnifications, we augment each zoom step with multi-scale-aware text prompts\ngenerated by a vision-language model (VLM). The prompt extractor itself is\nfine-tuned using Generalized Reward Policy Optimization (GRPO) with a critic\nVLM, aligning text guidance towards human preference. Experiments show that a\nstandard 4x diffusion SR model wrapped in CoZ attains beyond 256x enlargement\nwith high perceptual quality and fidelity."}
{"id": "2505.19940", "pdf": "https://arxiv.org/pdf/2505.19940", "abs": "https://arxiv.org/abs/2505.19940", "authors": ["Run Gu", "Wei Xu", "Zhaohui Yang", "Dusit Niyato", "Aylin Yener"], "title": "Task-Oriented Low-Label Semantic Communication With Self-Supervised Learning", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Task-oriented semantic communication enhances transmission efficiency by\nconveying semantic information rather than exact messages. Deep learning\n(DL)-based semantic communication can effectively cultivate the essential\nsemantic knowledge for semantic extraction, transmission, and interpretation by\nleveraging massive labeled samples for downstream task training. In this paper,\nwe propose a self-supervised learning-based semantic communication framework\n(SLSCom) to enhance task inference performance, particularly in scenarios with\nlimited access to labeled samples. Specifically, we develop a task-relevant\nsemantic encoder using unlabeled samples, which can be collected by devices in\nreal-world edge networks. To facilitate task-relevant semantic extraction, we\nintroduce self-supervision for learning contrastive features and formulate the\ninformation bottleneck (IB) problem to balance the tradeoff between the\ninformativeness of the extracted features and task inference performance. Given\nthe computational challenges of the IB problem, we devise a practical and\neffective solution by employing self-supervised classification and\nreconstruction pretext tasks. We further propose efficient joint training\nmethods to enhance end-to-end inference accuracy over wireless channels, even\nwith few labeled samples. We evaluate the proposed framework on image\nclassification tasks over multipath wireless channels. Extensive simulation\nresults demonstrate that SLSCom significantly outperforms conventional digital\ncoding methods and existing DL-based approaches across varying labeled data set\nsizes and SNR conditions, even when the unlabeled samples are irrelevant to the\ndownstream tasks."}
{"id": "2505.20249", "pdf": "https://arxiv.org/pdf/2505.20249", "abs": "https://arxiv.org/abs/2505.20249", "authors": ["Yongan Yu", "Qingchen Hu", "Xianda Du", "Jiayin Wang", "Fengran Mo", "Renee Sieber"], "title": "WXImpactBench: A Disruptive Weather Impact Understanding Benchmark for Evaluating Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025", "summary": "Climate change adaptation requires the understanding of disruptive weather\nimpacts on society, where large language models (LLMs) might be applicable.\nHowever, their effectiveness is under-explored due to the difficulty of\nhigh-quality corpus collection and the lack of available benchmarks. The\nclimate-related events stored in regional newspapers record how communities\nadapted and recovered from disasters. However, the processing of the original\ncorpus is non-trivial. In this study, we first develop a disruptive weather\nimpact dataset with a four-stage well-crafted construction pipeline. Then, we\npropose WXImpactBench, the first benchmark for evaluating the capacity of LLMs\non disruptive weather impacts. The benchmark involves two evaluation tasks,\nmulti-label classification and ranking-based question answering. Extensive\nexperiments on evaluating a set of LLMs provide first-hand analysis of the\nchallenges in developing disruptive weather impact understanding and climate\nchange adaptation systems. The constructed dataset and the code for the\nevaluation framework are available to help society protect against\nvulnerabilities from disasters."}
{"id": "2505.18601", "pdf": "https://arxiv.org/pdf/2505.18601", "abs": "https://arxiv.org/abs/2505.18601", "authors": ["Jongwoo Ko", "Sungnyun Kim", "Sungwoo Cho", "Se-Young Yun"], "title": "Flex-Judge: Think Once, Judge Anywhere", "categories": ["cs.CL", "cs.AI"], "comment": "The code is available at https://github.com/jongwooko/flex-judge", "summary": "Human-generated reward signals are critical for aligning generative models\nwith human preferences, guiding both training and inference-time evaluations.\nWhile large language models (LLMs) employed as proxy evaluators, i.e.,\nLLM-as-a-Judge, significantly reduce the costs associated with manual\nannotations, they typically require extensive modality-specific training data\nand fail to generalize well across diverse multimodal tasks. In this paper, we\npropose Flex-Judge, a reasoning-guided multimodal judge model that leverages\nminimal textual reasoning data to robustly generalize across multiple\nmodalities and evaluation formats. Our core intuition is that structured\ntextual reasoning explanations inherently encode generalizable decision-making\npatterns, enabling an effective transfer to multimodal judgments, e.g., with\nimages or videos. Empirical results demonstrate that Flex-Judge, despite being\ntrained on significantly fewer text data, achieves competitive or superior\nperformance compared to state-of-the-art commercial APIs and extensively\ntrained multimodal evaluators. Notably, Flex-Judge presents broad impact in\nmodalities like molecule, where comprehensive evaluation benchmarks are scarce,\nunderscoring its practical value in resource-constrained domains. Our framework\nhighlights reasoning-based text supervision as a powerful, cost-effective\nalternative to traditional annotation-intensive approaches, substantially\nadvancing scalable multimodal model-as-a-judge."}
{"id": "2505.19943", "pdf": "https://arxiv.org/pdf/2505.19943", "abs": "https://arxiv.org/abs/2505.19943", "authors": ["Huan Zhang", "Fan Lyu", "Shuyu Dong", "Shenghua Fan", "Yujin Zheng", "Dingwen Wang"], "title": "Beyond Freezing: Sparse Tuning Enhances Plasticity in Continual Learning with Pre-Trained Models", "categories": ["cs.LG"], "comment": null, "summary": "Continual Learning with Pre-trained Models holds great promise for efficient\nadaptation across sequential tasks. However, most existing approaches freeze\nPTMs and rely on auxiliary modules like prompts or adapters, limiting model\nplasticity and leading to suboptimal generalization when facing significant\ndistribution shifts. While full fine-tuning can improve adaptability, it risks\ndisrupting crucial pre-trained knowledge. In this paper, we propose Mutual\nInformation-guided Sparse Tuning (MIST), a plug-and-play method that\nselectively updates a small subset of PTM parameters, less than 5%, based on\nsensitivity to mutual information objectives. MIST enables effective\ntask-specific adaptation while preserving generalization. To further reduce\ninterference, we introduce strong sparsity regularization by randomly dropping\ngradients during tuning, resulting in fewer than 0.5% of parameters being\nupdated per step. Applied before standard freeze-based methods, MIST\nconsistently boosts performance across diverse continual learning benchmarks.\nExperiments show that integrating our method into multiple baselines yields\nsignificant performance gains. Our code is available at\nhttps://github.com/zhwhu/MIST."}
{"id": "2505.20258", "pdf": "https://arxiv.org/pdf/2505.20258", "abs": "https://arxiv.org/abs/2505.20258", "authors": ["Siye Wu", "Jian Xie", "Yikai Zhang", "Aili Chen", "Kai Zhang", "Yu Su", "Yanghua Xiao"], "title": "ARM: Adaptive Reasoning Model", "categories": ["cs.CL"], "comment": "Work in Progress", "summary": "While large reasoning models demonstrate strong performance on complex tasks,\nthey lack the ability to adjust reasoning token usage based on task difficulty.\nThis often leads to the \"overthinking\" problem -- excessive and unnecessary\nreasoning -- which, although potentially mitigated by human intervention to\ncontrol the token budget, still fundamentally contradicts the goal of achieving\nfully autonomous AI. In this work, we propose Adaptive Reasoning Model (ARM), a\nreasoning model capable of adaptively selecting appropriate reasoning formats\nbased on the task at hand. These formats include three efficient ones -- Direct\nAnswer, Short CoT, and Code -- as well as a more elaborate format, Long CoT. To\ntrain ARM, we introduce Ada-GRPO, an adaptation of Group Relative Policy\nOptimization (GRPO), which addresses the format collapse issue in traditional\nGRPO. Ada-GRPO enables ARM to achieve high token efficiency, reducing tokens by\nan average of 30%, and up to 70%, while maintaining performance comparable to\nthe model that relies solely on Long CoT. Furthermore, not only does it improve\ninference efficiency through reduced token generation, but it also brings a 2x\nspeedup in training. In addition to the default Adaptive Mode, ARM supports two\nadditional reasoning modes: 1) Instruction-Guided Mode, which allows users to\nexplicitly specify the reasoning format via special tokens -- ideal when the\nappropriate format is known for a batch of tasks. 2) Consensus-Guided Mode,\nwhich aggregates the outputs of the three efficient formats and resorts to Long\nCoT in case of disagreement, prioritizing performance with higher token usage."}
{"id": "2505.18602", "pdf": "https://arxiv.org/pdf/2505.18602", "abs": "https://arxiv.org/abs/2505.18602", "authors": ["Hengzhe Zhang", "Qi Chen", "Bing Xue", "Mengjie Zhang"], "title": "LLM-Meta-SR: Learning to Evolve Selection Operators for Symbolic Regression", "categories": ["cs.NE", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have revolutionized algorithm development, yet\ntheir application in symbolic regression, where algorithms automatically\ndiscover symbolic expressions from data, remains constrained and is typically\ndesigned manually by human experts. In this paper, we propose a\nlearning-to-evolve framework that enables LLMs to automatically design\nselection operators for evolutionary symbolic regression algorithms. We first\nidentify two key limitations in existing LLM-based algorithm evolution\ntechniques: code bloat and a lack of semantic guidance. Bloat results in\nunnecessarily complex components, and the absence of semantic awareness can\nlead to ineffective exchange of useful code components, both of which can\nreduce the interpretability of the designed algorithm or hinder evolutionary\nlearning progress. To address these issues, we enhance the LLM-based evolution\nframework for meta symbolic regression with two key innovations: bloat control\nand a complementary, semantics-aware selection operator. Additionally, we embed\ndomain knowledge into the prompt, enabling the LLM to generate more effective\nand contextually relevant selection operators. Our experimental results on\nsymbolic regression benchmarks show that LLMs can devise selection operators\nthat outperform nine expert-designed baselines, achieving state-of-the-art\nperformance. This demonstrates that LLMs can exceed expert-level algorithm\ndesign for symbolic regression."}
{"id": "2505.19946", "pdf": "https://arxiv.org/pdf/2505.19946", "abs": "https://arxiv.org/abs/2505.19946", "authors": ["Antoine Moulin", "Gergely Neu", "Luca Viano"], "title": "Inverse Q-Learning Done Right: Offline Imitation Learning in $Q^$-Realizable MDPs", "categories": ["cs.LG"], "comment": null, "summary": "We study the problem of offline imitation learning in Markov decision\nprocesses (MDPs), where the goal is to learn a well-performing policy given a\ndataset of state-action pairs generated by an expert policy. Complementing a\nrecent line of work on this topic that assumes the expert belongs to a\ntractable class of known policies, we approach this problem from a new angle\nand leverage a different type of structural assumption about the environment.\nSpecifically, for the class of linear $Q^\\pi$-realizable MDPs, we introduce a\nnew algorithm called saddle-point offline imitation learning (\\SPOIL), which is\nguaranteed to match the performance of any expert up to an additive error\n$\\varepsilon$ with access to $\\mathcal{O}(\\varepsilon^{-2})$ samples. Moreover,\nwe extend this result to possibly non-linear $Q^\\pi$-realizable MDPs at the\ncost of a worse sample complexity of order $\\mathcal{O}(\\varepsilon^{-4})$.\nFinally, our analysis suggests a new loss function for training critic networks\nfrom expert data in deep imitation learning. Empirical evaluations on standard\nbenchmarks demonstrate that the neural net implementation of \\SPOIL is superior\nto behavior cloning and competitive with state-of-the-art algorithms."}
{"id": "2505.20264", "pdf": "https://arxiv.org/pdf/2505.20264", "abs": "https://arxiv.org/abs/2505.20264", "authors": ["Dong Nguyen", "Esther Ploeger"], "title": "We Need to Measure Data Diversity in NLP -- Better and Broader", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although diversity in NLP datasets has received growing attention, the\nquestion of how to measure it remains largely underexplored. This opinion paper\nexamines the conceptual and methodological challenges of measuring data\ndiversity and argues that interdisciplinary perspectives are essential for\ndeveloping more fine-grained and valid measures."}
{"id": "2505.18605", "pdf": "https://arxiv.org/pdf/2505.18605", "abs": "https://arxiv.org/abs/2505.18605", "authors": ["Xiaohuan Pei", "Tao Huang", "YanXiang Ma", "Chang Xu"], "title": "Rethinking Causal Mask Attention for Vision-Language Inference", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Causal attention has become a foundational mechanism in autoregressive\nvision-language models (VLMs), unifying textual and visual inputs under a\nsingle generative framework. However, existing causal mask-based strategies are\ninherited from large language models (LLMs) where they are tailored for\ntext-only decoding, and their adaptation to vision tokens is insufficiently\naddressed in the prefill stage. Strictly masking future positions for vision\nqueries introduces overly rigid constraints, which hinder the model's ability\nto leverage future context that often contains essential semantic cues for\naccurate inference. In this work, we empirically investigate how different\ncausal masking strategies affect vision-language inference and then propose a\nfamily of future-aware attentions tailored for this setting. We first\nempirically analyze the effect of previewing future tokens for vision queries\nand demonstrate that rigid masking undermines the model's capacity to capture\nuseful contextual semantic representations. Based on these findings, we propose\na lightweight attention family that aggregates future visual context into past\nrepresentations via pooling, effectively preserving the autoregressive\nstructure while enhancing cross-token dependencies. We evaluate a range of\ncausal masks across diverse vision-language inference settings and show that\nselectively compressing future semantic context into past representations\nbenefits the inference."}
{"id": "2505.19947", "pdf": "https://arxiv.org/pdf/2505.19947", "abs": "https://arxiv.org/abs/2505.19947", "authors": ["Herbert Woisetschlger", "Ryan Zhang", "Shiqiang Wang", "Hans-Arno Jacobsen"], "title": "Dynamically Learned Test-Time Model Routing in Language Model Zoos with Service Level Guarantees", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "I.2; I.2.7; I.2.8"], "comment": "Preprint. Under review", "summary": "Open-weight LLM zoos provide access to numerous high-quality models, but\nselecting the appropriate model for specific tasks remains challenging and\nrequires technical expertise. Most users simply want factually correct, safe,\nand satisfying responses without concerning themselves with model\ntechnicalities, while inference service providers prioritize minimizing\noperating costs. These competing interests are typically mediated through\nservice level agreements (SLAs) that guarantee minimum service quality. We\nintroduce MESS+, a stochastic optimization algorithm for cost-optimal LLM\nrequest routing while providing rigorous SLA compliance guarantees. MESS+\nlearns request satisfaction probabilities of LLMs in real-time as users\ninteract with the system, based on which model selection decisions are made by\nsolving a per-request optimization problem. Our algorithm includes a novel\ncombination of virtual queues and request satisfaction prediction, along with a\ntheoretical analysis of cost optimality and constraint satisfaction. Across a\nwide range of state-of-the-art LLM benchmarks, MESS+ achieves an average of 2x\ncost savings compared to existing LLM routing techniques."}
{"id": "2505.20276", "pdf": "https://arxiv.org/pdf/2505.20276", "abs": "https://arxiv.org/abs/2505.20276", "authors": ["Anmol Mekala", "Anirudh Atmakuru", "Yixiao Song", "Marzena Karpinska", "Mohit Iyyer"], "title": "Does quantization affect models' performance on long-context tasks?", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages of content with 9 figures. 37 remaining pages of references\n  and supplementary with 17 figures. Under review as of May 26", "summary": "Large language models (LLMs) now support context windows exceeding 128K\ntokens, but this comes with significant memory requirements and high inference\nlatency. Quantization can mitigate these costs, but may degrade performance. In\nthis work, we present the first systematic evaluation of quantized LLMs on\ntasks with long-inputs (>64K tokens) and long-form outputs. Our evaluation\nspans 9.7K test examples, five quantization methods (FP8, GPTQ-int8, AWQ-int4,\nGPTQ-int4, BNB-nf4), and five models (Llama-3.1 8B and 70B; Qwen-2.5 7B, 32B,\nand 72B). We find that, on average, 8-bit quantization preserves accuracy\n(~0.8% drop), whereas 4-bit methods lead to substantial losses, especially for\ntasks involving long context inputs (drops of up to 59%). This degradation\ntends to worsen when the input is in a language other than English. Crucially,\nthe effects of quantization depend heavily on the quantization method, model,\nand task. For instance, while Qwen-2.5 72B remains robust under BNB-nf4,\nLlama-3.1 70B experiences a 32% performance drop on the same task. These\nfindings highlight the importance of a careful, task-specific evaluation before\ndeploying quantized LLMs, particularly in long-context scenarios and with\nlanguages other than English."}
{"id": "2505.18622", "pdf": "https://arxiv.org/pdf/2505.18622", "abs": "https://arxiv.org/abs/2505.18622", "authors": ["Kourosh Shahnazari", "Seyed Moein Ayyoubzadeh", "Mohammadali Keshtparvar", "Pegah Ghaffari"], "title": "Trust, or Don't Predict: Introducing the CWSA Family for Confidence-Aware Model Evaluation", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "In recent machine learning systems, confidence scores are being utilized more\nand more to manage selective prediction, whereby a model can abstain from\nmaking a prediction when it is unconfident. Yet, conventional metrics like\naccuracy, expected calibration error (ECE), and area under the risk-coverage\ncurve (AURC) do not capture the actual reliability of predictions. These\nmetrics either disregard confidence entirely, dilute valuable localized\ninformation through averaging, or neglect to suitably penalize overconfident\nmisclassifications, which can be particularly detrimental in real-world\nsystems. We introduce two new metrics Confidence-Weighted Selective Accuracy\n(CWSA) and its normalized variant CWSA+ that offer a principled and\ninterpretable way to evaluate predictive models under confidence thresholds.\nUnlike existing methods, our metrics explicitly reward confident accuracy and\npenalize overconfident mistakes. They are threshold-local, decomposable, and\nusable in both evaluation and deployment settings where trust and risk must be\nquantified. Through exhaustive experiments on both real-world data sets (MNIST,\nCIFAR-10) and artificial model variants (calibrated, overconfident,\nunderconfident, random, perfect), we show that CWSA and CWSA+ both effectively\ndetect nuanced failure modes and outperform classical metrics in\ntrust-sensitive tests. Our results confirm that CWSA is a sound basis for\ndeveloping and assessing selective prediction systems for safety-critical\ndomains."}
{"id": "2505.19949", "pdf": "https://arxiv.org/pdf/2505.19949", "abs": "https://arxiv.org/abs/2505.19949", "authors": ["Siqi Kou", "Qingyuan Tian", "Hanwen Xu", "Zihao Zeng", "Zhijie Deng"], "title": "Which Data Attributes Stimulate Math and Code Reasoning? An Investigation via Influence Functions", "categories": ["cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities in math and coding, often bolstered by post-training on the\nchain-of-thoughts (CoTs) generated by stronger models. However, existing\nstrategies for curating such training data predominantly rely on heuristics,\nlimiting generalizability and failing to capture subtleties underlying in data.\nTo address these limitations, we leverage influence functions to systematically\nattribute LLMs' reasoning ability on math and coding to individual training\nexamples, sequences, and tokens, enabling deeper insights into effective data\ncharacteristics. Our Influence-based Reasoning Attribution (Infra) uncovers\nnontrivial cross-domain effects across math and coding tasks: high-difficulty\nmath examples improve both math and code reasoning, while low-difficulty code\ntasks most effectively benefit code reasoning. Based on these findings, we\nintroduce a simple yet effective dataset reweighting strategy by flipping task\ndifficulty, which doubles AIME24 accuracy from 10\\% to 20\\% and boosts\nLiveCodeBench accuracy from 33.8\\% to 35.3\\% for Qwen2.5-7B-Instruct. Moreover,\nour fine-grained attribution reveals that the sequence-level exploratory\nbehaviors enhance reasoning performance in both math and code, and the\ntoken-level influence patterns are distinct for math and code reasoning: the\nformer prefers natural language logic connectors and the latter emphasizes\nstructural syntax."}
{"id": "2505.20277", "pdf": "https://arxiv.org/pdf/2505.20277", "abs": "https://arxiv.org/abs/2505.20277", "authors": ["Haonan Zhang", "Run Luo", "Xiong Liu", "Yuchuan Wu", "Ting-En Lin", "Pengpeng Zeng", "Qiang Qu", "Feiteng Fang", "Min Yang", "Lianli Gao", "Jingkuan Song", "Fei Huang", "Yongbin Li"], "title": "OmniCharacter: Towards Immersive Role-Playing Agents with Seamless Speech-Language Personality Interaction", "categories": ["cs.CL", "cs.CV"], "comment": "14 pages, 6 figures", "summary": "Role-Playing Agents (RPAs), benefiting from large language models, is an\nemerging interactive AI system that simulates roles or characters with diverse\npersonalities. However, existing methods primarily focus on mimicking dialogues\namong roles in textual form, neglecting the role's voice traits (e.g., voice\nstyle and emotions) as playing a crucial effect in interaction, which tends to\nbe more immersive experiences in realistic scenarios. Towards this goal, we\npropose OmniCharacter, a first seamless speech-language personality interaction\nmodel to achieve immersive RPAs with low latency. Specifically, OmniCharacter\nenables agents to consistently exhibit role-specific personality traits and\nvocal traits throughout the interaction, enabling a mixture of speech and\nlanguage responses. To align the model with speech-language scenarios, we\nconstruct a dataset named OmniCharacter-10K, which involves more distinctive\ncharacters (20), richly contextualized multi-round dialogue (10K), and dynamic\nspeech response (135K). Experimental results showcase that our method yields\nbetter responses in terms of both content and style compared to existing RPAs\nand mainstream speech-language models, with a response latency as low as 289ms.\nCode and dataset are available at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/OmniCharacter."}
{"id": "2505.18630", "pdf": "https://arxiv.org/pdf/2505.18630", "abs": "https://arxiv.org/abs/2505.18630", "authors": ["Zhihao Jia", "Mingyi Jia", "Junwen Duan", "Jianxin Wang"], "title": "DDO: Dual-Decision Optimization via Multi-Agent Collaboration for LLM-Based Medical Consultation", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "17 pages, 4 figures", "summary": "Large Language Models (LLMs) demonstrate strong generalization and reasoning\nabilities, making them well-suited for complex decision-making tasks such as\nmedical consultation (MC). However, existing LLM-based methods often fail to\ncapture the dual nature of MC, which entails two distinct sub-tasks: symptom\ninquiry, a sequential decision-making process, and disease diagnosis, a\nclassification problem. This mismatch often results in ineffective symptom\ninquiry and unreliable disease diagnosis. To address this, we propose\n\\textbf{DDO}, a novel LLM-based framework that performs\n\\textbf{D}ual-\\textbf{D}ecision \\textbf{O}ptimization by decoupling and\nindependently optimizing the the two sub-tasks through a collaborative\nmulti-agent workflow. Experiments on three real-world MC datasets show that DDO\nconsistently outperforms existing LLM-based approaches and achieves competitive\nperformance with state-of-the-art generation-based methods, demonstrating its\neffectiveness in the MC task."}
{"id": "2505.19954", "pdf": "https://arxiv.org/pdf/2505.19954", "abs": "https://arxiv.org/abs/2505.19954", "authors": ["Andrew Zamai", "Nathanael Fijalkow", "Boris Mansencal", "Laurent Simon", "Eloi Navet", "Pierrick Coupe"], "title": "An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The differential diagnosis of neurodegenerative dementias is a challenging\nclinical task, mainly because of the overlap in symptom presentation and the\nsimilarity of patterns observed in structural neuroimaging. To improve\ndiagnostic efficiency and accuracy, deep learning-based methods such as\nConvolutional Neural Networks and Vision Transformers have been proposed for\nthe automatic classification of brain MRIs. However, despite their strong\npredictive performance, these models find limited clinical utility due to their\nopaque decision making. In this work, we propose a framework that integrates\ntwo core components to enhance diagnostic transparency. First, we introduce a\nmodular pipeline for converting 3D T1-weighted brain MRIs into textual\nradiology reports. Second, we explore the potential of modern Large Language\nModels (LLMs) to assist clinicians in the differential diagnosis between\nFrontotemporal dementia subtypes, Alzheimer's disease, and normal aging based\non the generated reports. To bridge the gap between predictive accuracy and\nexplainability, we employ reinforcement learning to incentivize diagnostic\nreasoning in LLMs. Without requiring supervised reasoning traces or\ndistillation from larger models, our approach enables the emergence of\nstructured diagnostic rationales grounded in neuroimaging findings. Unlike\npost-hoc explainability methods that retrospectively justify model decisions,\nour framework generates diagnostic rationales as part of the inference\nprocess-producing causally grounded explanations that inform and guide the\nmodel's decision-making process. In doing so, our framework matches the\ndiagnostic performance of existing deep learning methods while offering\nrationales that support its diagnostic conclusions."}
{"id": "2505.20282", "pdf": "https://arxiv.org/pdf/2505.20282", "abs": "https://arxiv.org/abs/2505.20282", "authors": ["Zitian Gao", "Lynx Chen", "Joey Zhou", "Bryan Dai"], "title": "One-shot Entropy Minimization", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "We trained 13,440 large language models and found that entropy minimization\nrequires only a single unlabeled data and 10 steps optimization to achieve\nperformance improvements comparable to or even greater than those obtained\nusing thousands of data and carefully designed rewards in rule-based\nreinforcement learning. This striking result may prompt a rethinking of\npost-training paradigms for large language models. Our code is avaliable at\nhttps://github.com/zitian-gao/one-shot-em."}
{"id": "2505.18640", "pdf": "https://arxiv.org/pdf/2505.18640", "abs": "https://arxiv.org/abs/2505.18640", "authors": ["Jian Liang", "Wenke Huang", "Xianda Guo", "Guancheng Wan", "Bo Du", "Mang Ye"], "title": "ThanoRA: Task Heterogeneity-Aware Multi-Task Low-Rank Adaptation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Low-Rank Adaptation (LoRA) is widely adopted for downstream fine-tuning of\nfoundation models due to its efficiency and zero additional inference cost.\nMany real-world applications require foundation models to specialize in\nmultiple tasks simultaneously, motivating the need for efficient multi-task\nadaptation. While recent approaches integrate LoRA with mixture-of-experts\n(MoE) to address this, the use of routers prevents parameter mergeability,\nwhich increases inference overhead and hinders unified multi-task adaptation,\nthereby limiting deployment practicality. In this work, we propose ThanoRA, a\nTask Heterogeneity-Aware Multi-Task Low-Rank Adaptation framework that enables\nmulti-task adaptation while preserving the inference efficiency of LoRA.\nThanoRA jointly models task heterogeneity and mitigates subspace interference\nthroughout training. Specifically, motivated by inherent differences in\ncomplexity and heterogeneity across tasks, ThanoRA constructs task-specific\nLoRA subspaces at initialization, enabling fine-grained knowledge injection\naligned with task heterogeneity. Furthermore, to prevent task interference and\nsubspace collapse during multi-task training, ThanoRA introduces a\nsubspace-preserving regularization that maintains the independence of\ntask-specific representations. With the synergy of both components, ThanoRA\nenables efficient and unified multi-task adaptation. Extensive experiments\nacross multimodal and text-only benchmarks under varying multi-task mixtures\ndemonstrate that ThanoRA consistently achieves robust and superior performance\nover strong baselines without introducing additional inference overhead. Our\ncode is publicly available at: https://github.com/LiangJian24/ThanoRA."}
{"id": "2505.19955", "pdf": "https://arxiv.org/pdf/2505.19955", "abs": "https://arxiv.org/abs/2505.19955", "authors": ["Hui Chen", "Miao Xiong", "Yujie Lu", "Wei Han", "Ailin Deng", "Yufei He", "Jiaying Wu", "Yibo Li", "Yue Liu", "Bryan Hooi"], "title": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "40 pages, 7 figures", "summary": "Recent advancements in AI agents have demonstrated their growing potential to\ndrive and support scientific discovery. In this work, we introduce MLR-Bench, a\ncomprehensive benchmark for evaluating AI agents on open-ended machine learning\nresearch. MLR-Bench includes three key components: (1) 201 research tasks\nsourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2)\nMLR-Judge, an automated evaluation framework combining LLM-based reviewers with\ncarefully designed review rubrics to assess research quality; and (3)\nMLR-Agent, a modular agent scaffold capable of completing research tasks\nthrough four stages: idea generation, proposal formulation, experimentation,\nand paper writing. Our framework supports both stepwise assessment across these\ndistinct research stages, and end-to-end evaluation of the final research\npaper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced\ncoding agent, finding that while LLMs are effective at generating coherent\nideas and well-structured papers, current coding agents frequently (e.g., in\n80% of the cases) produce fabricated or invalidated experimental\nresults--posing a major barrier to scientific reliability. We validate\nMLR-Judge through human evaluation, showing high agreement with expert\nreviewers, supporting its potential as a scalable tool for research evaluation.\nWe open-source MLR-Bench to help the community benchmark, diagnose, and improve\nAI research agents toward trustworthy and transparent scientific discovery."}
{"id": "2505.20285", "pdf": "https://arxiv.org/pdf/2505.20285", "abs": "https://arxiv.org/abs/2505.20285", "authors": ["Weiqi Wu", "Xin Guan", "Shen Huang", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jiuxin Cao", "Hai Zhao", "Jingren Zhou"], "title": "MASKSEARCH: A Universal Pre-Training Framework to Enhance Agentic Search Capability", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Language Models (RALMs) represent a classic paradigm\nwhere models enhance generative capabilities using external knowledge retrieved\nvia a specialized module. Recent advancements in Agent techniques enable Large\nLanguage Models (LLMs) to autonomously utilize tools for retrieval, planning,\nand reasoning. While existing training-based methods show promise, their\nagentic abilities are limited by inherent characteristics of the task-specific\ndata used during training. To further enhance the universal search capability\nof agents, we propose a novel pre-training framework, MASKSEARCH. In the\npre-training stage, we introduce the Retrieval Augmented Mask Prediction (RAMP)\ntask, where the model learns to leverage search tools to fill masked spans on a\nlarge number of pre-training data, thus acquiring universal retrieval and\nreasoning capabilities for LLMs. After that, the model is trained on downstream\ntasks to achieve further improvement. We apply both Supervised Fine-tuning\n(SFT) and Reinforcement Learning (RL) for training. For SFT, we combine\nagent-based and distillation-based methods to generate training data, starting\nwith a multi-agent system consisting of a planner, rewriter, observer, and\nfollowed by a self-evolving teacher model. While for RL, we employ DAPO as the\ntraining framework and adopt a hybrid reward system consisting of answer\nrewards and format rewards. Additionally, we introduce a curriculum learning\napproach that allows the model to learn progressively from easier to more\nchallenging instances based on the number of masked spans. We evaluate the\neffectiveness of our framework in the scenario of open-domain multi-hop\nquestion answering. Through extensive experiments, we demonstrate that\nMASKSEARCH significantly enhances the performance of LLM-based search agents on\nboth in-domain and out-of-domain downstream tasks."}
{"id": "2505.18643", "pdf": "https://arxiv.org/pdf/2505.18643", "abs": "https://arxiv.org/abs/2505.18643", "authors": ["Steven Ndung'u", "Trienko Grobler", "Stefan J. Wijnholds", "George Azzopardi"], "title": "Anomaly detection in radio galaxy data with trainable COSFIRE filters", "categories": ["astro-ph.IM", "cs.AI"], "comment": "5 pages, URSI Asia-Pacific Radio Science Conference and URSI Radio\n  Science Letters (RSL)", "summary": "Detecting anomalies in radio astronomy is challenging due to the vast amounts\nof data and the rarity of labeled anomalous examples. Addressing this challenge\nrequires efficient methods capable of identifying unusual radio galaxy\nmorphologies without relying on extensive supervision. This work introduces an\ninnovative approach to anomaly detection based on morphological characteristics\nof the radio sources using trainable COSFIRE (Combination of Shifted Filter\nResponses) filters as an efficient alternative to complex deep learning\nmethods. The framework integrates COSFIRE descriptors with an unsupervised\nLocal Outlier Factor (LOF) algorithm to identify unusual radio galaxy\nmorphologies. Evaluations on a radio galaxy benchmark data set demonstrate\nstrong performance, with the COSFIRE-based approach achieving a geometric mean\n(G-Mean) score of 79%, surpassing the 77% achieved by a computationally\nintensive deep learning autoencoder. By characterizing normal patterns and\ndetecting deviations, this semi-supervised methodology overcomes the need for\nanomalous examples in the training set, a major limitation of traditional\nsupervised methods. This approach shows promise for next-generation radio\ntelescopes, where fast processing and the ability to discover unknown phenomena\nare crucial."}
{"id": "2505.19964", "pdf": "https://arxiv.org/pdf/2505.19964", "abs": "https://arxiv.org/abs/2505.19964", "authors": ["Eric Zhao", "Jessica Dai", "Pranjal Awasthi"], "title": "The Limits of Preference Data for Post-Training", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.GT"], "comment": null, "summary": "Recent progress in strengthening the capabilities of large language models\nhas stemmed from applying reinforcement learning to domains with automatically\nverifiable outcomes. A key question is whether we can similarly use RL to\noptimize for outcomes in domains where evaluating outcomes inherently requires\nhuman feedback; for example, in tasks like deep research and trip planning,\noutcome evaluation is qualitative and there are many possible degrees of\nsuccess. One attractive and scalable modality for collecting human feedback is\npreference data: ordinal rankings (pairwise or $k$-wise) that indicate, for $k$\ngiven outcomes, which one is preferred. In this work, we study a critical\nroadblock: preference data fundamentally and significantly limits outcome-based\noptimization. Even with idealized preference data (infinite, noiseless, and\nonline), the use of ordinal feedback can prevent obtaining even approximately\noptimal solutions. We formalize this impossibility using voting theory, drawing\nan analogy between how a model chooses to answer a query with how voters choose\na candidate to elect. This indicates that grounded human scoring and\nalgorithmic innovations are necessary for extending the success of RL\npost-training to domains demanding human feedback. We also explore why these\nlimitations have disproportionately impacted RLHF when it comes to eliciting\nreasoning behaviors (e.g., backtracking) versus situations where RLHF has been\nhistorically successful (e.g., instruction-tuning and safety training), finding\nthat the limitations of preference data primarily suppress RLHF's ability to\nelicit robust strategies -- a class that encompasses most reasoning behaviors."}
{"id": "2505.20293", "pdf": "https://arxiv.org/pdf/2505.20293", "abs": "https://arxiv.org/abs/2505.20293", "authors": ["Yifan Sun", "Danding Wang", "Qiang Sheng", "Juan Cao", "Jintao Li"], "title": "Enhancing the Comprehensibility of Text Explanations via Unsupervised Concept Discovery", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Concept-based explainable approaches have emerged as a promising method in\nexplainable AI because they can interpret models in a way that aligns with\nhuman reasoning. However, their adaption in the text domain remains limited.\nMost existing methods rely on predefined concept annotations and cannot\ndiscover unseen concepts, while other methods that extract concepts without\nsupervision often produce explanations that are not intuitively comprehensible\nto humans, potentially diminishing user trust. These methods fall short of\ndiscovering comprehensible concepts automatically. To address this issue, we\npropose \\textbf{ECO-Concept}, an intrinsically interpretable framework to\ndiscover comprehensible concepts with no concept annotations. ECO-Concept first\nutilizes an object-centric architecture to extract semantic concepts\nautomatically. Then the comprehensibility of the extracted concepts is\nevaluated by large language models. Finally, the evaluation result guides the\nsubsequent model fine-tuning to obtain more understandable explanations.\nExperiments show that our method achieves superior performance across diverse\ntasks. Further concept evaluations validate that the concepts learned by\nECO-Concept surpassed current counterparts in comprehensibility."}
{"id": "2505.18646", "pdf": "https://arxiv.org/pdf/2505.18646", "abs": "https://arxiv.org/abs/2505.18646", "authors": ["Siwei Liu", "Jinyuan Fang", "Han Zhou", "Yingxu Wang", "Zaiqiao Meng"], "title": "SEW: Self-Evolving Agentic Workflows for Automated Code Generation", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "16 pages, 5 figures", "summary": "Large Language Models (LLMs) have demonstrated effectiveness in code\ngeneration tasks. To enable LLMs to address more complex coding challenges,\nexisting research has focused on crafting multi-agent systems with agentic\nworkflows, where complex coding tasks are decomposed into sub-tasks, assigned\nto specialized agents. Despite their effectiveness, current approaches heavily\nrely on hand-crafted agentic workflows, with both agent topologies and prompts\nmanually designed, which limits their ability to automatically adapt to\ndifferent types of coding problems. To address these limitations and enable\nautomated workflow design, we propose \\textbf{S}elf-\\textbf{E}volving\n\\textbf{W}orkflow (\\textbf{SEW}), a novel self-evolving framework that\nautomatically generates and optimises multi-agent workflows. Extensive\nexperiments on three coding benchmark datasets, including the challenging\nLiveCodeBench, demonstrate that our SEW can automatically design agentic\nworkflows and optimise them through self-evolution, bringing up to 33\\%\nimprovement on LiveCodeBench compared to using the backbone LLM only.\nFurthermore, by investigating different representation schemes of workflow, we\nprovide insights into the optimal way to encode workflow information with text."}
{"id": "2505.19966", "pdf": "https://arxiv.org/pdf/2505.19966", "abs": "https://arxiv.org/abs/2505.19966", "authors": ["Zheng Zhang", "Shaocheng Lan", "Lei Song", "Jiang Bian", "Yexin Li", "Kan Ren"], "title": "Learning to Select In-Context Demonstration Preferred by Large Language Model", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In-context learning (ICL) enables large language models (LLMs) to adapt to\nnew tasks during inference using only a few demonstrations. However, ICL\nperformance is highly dependent on the selection of these demonstrations.\nRecent work explores retrieval-based methods for selecting query-specific\ndemonstrations, but these approaches often rely on surrogate objectives such as\nmetric learning, failing to directly optimize ICL performance. Consequently,\nthey struggle to identify truly beneficial demonstrations. Moreover, their\ndiscriminative retrieval paradigm is ineffective when the candidate pool lacks\nsufficient high-quality demonstrations. To address these challenges, we propose\nGenICL, a novel generative preference learning framework that leverages LLM\nfeedback to directly optimize demonstration selection for ICL. Experiments on\n19 datasets across 11 task categories demonstrate that GenICL achieves superior\nperformance than existing methods in selecting the most effective\ndemonstrations, leading to better ICL performance."}
{"id": "2505.20295", "pdf": "https://arxiv.org/pdf/2505.20295", "abs": "https://arxiv.org/abs/2505.20295", "authors": ["Michael Kirchhof", "Luca Fger", "Adam Goliski", "Eeshan Gunesh Dhekane", "Arno Blaas", "Sinead Williamson"], "title": "Self-reflective Uncertainties: Do LLMs Know Their Internal Answer Distribution?", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": null, "summary": "To reveal when a large language model (LLM) is uncertain about a response,\nuncertainty quantification commonly produces percentage numbers along with the\noutput. But is this all we can do? We argue that in the output space of LLMs,\nthe space of strings, exist strings expressive enough to summarize the\ndistribution over output strings the LLM deems possible. We lay a foundation\nfor this new avenue of uncertainty explication and present SelfReflect, a\ntheoretically-motivated metric to assess how faithfully a string summarizes an\nLLM's internal answer distribution. We show that SelfReflect is able to\ndiscriminate even subtle differences of candidate summary strings and that it\naligns with human judgement, outperforming alternative metrics such as LLM\njudges and embedding comparisons. With SelfReflect, we investigate a number of\nself-summarization methods and find that even state-of-the-art reasoning models\nstruggle to explicate their internal uncertainty. But we find that faithful\nsummarizations can be generated by sampling and summarizing. Our metric enables\nfuture works towards this universal form of LLM uncertainties."}
{"id": "2505.18647", "pdf": "https://arxiv.org/pdf/2505.18647", "abs": "https://arxiv.org/abs/2505.18647", "authors": ["Kiet Bennema ten Brinke", "Koen Minartz", "Vlado Menkovski"], "title": "Flow Matching for Geometric Trajectory Simulation", "categories": ["cs.LG", "cs.AI"], "comment": "20 pages, 17 figures", "summary": "The simulation of N-body systems is a fundamental problem with applications\nin a wide range of fields, such as molecular dynamics, biochemistry, and\npedestrian dynamics. Machine learning has become an invaluable tool for scaling\nphysics-based simulators and developing models directly from experimental data.\nIn particular, recent advances based on deep generative modeling and geometric\ndeep learning have enabled probabilistic simulation by modeling complex\ndistributions over trajectories while respecting the permutation symmetry that\nis fundamental to N-body systems. However, to generate realistic trajectories,\nexisting methods must learn complex transformations starting from uninformed\nnoise and do not allow for the exploitation of domain-informed priors. In this\nwork, we propose STFlow to address this limitation. By leveraging flow matching\nand data-dependent couplings, STFlow facilitates physics-informed simulation of\ngeometric trajectories without sacrificing model expressivity or scalability.\nOur evaluation on N-body dynamical systems, molecular dynamics, and pedestrian\ndynamics benchmarks shows that STFlow produces significantly lower prediction\nerrors while enabling more efficient inference, highlighting the benefits of\nemploying physics-informed prior distributions in probabilistic geometric\ntrajectory modeling."}
{"id": "2505.19969", "pdf": "https://arxiv.org/pdf/2505.19969", "abs": "https://arxiv.org/abs/2505.19969", "authors": ["Antti Koskela", "Tejas Kulkarni"], "title": "Differential Privacy Analysis of Decentralized Gossip Averaging under Varying Threat Models", "categories": ["cs.LG", "cs.CR", "cs.DC"], "comment": null, "summary": "Fully decentralized training of machine learning models offers significant\nadvantages in scalability, robustness, and fault tolerance. However, achieving\ndifferential privacy (DP) in such settings is challenging due to the absence of\na central aggregator and varying trust assumptions among nodes. In this work,\nwe present a novel privacy analysis of decentralized gossip-based averaging\nalgorithms with additive node-level noise, both with and without secure\nsummation over each node's direct neighbors. Our main contribution is a new\nanalytical framework based on a linear systems formulation that accurately\ncharacterizes privacy leakage across these scenarios. This framework\nsignificantly improves upon prior analyses, for example, reducing the R\\'enyi\nDP parameter growth from $O(T^2)$ to $O(T)$, where $T$ is the number of\ntraining rounds. We validate our analysis with numerical results demonstrating\nsuperior DP bounds compared to existing approaches. We further illustrate our\nanalysis with a logistic regression experiment on MNIST image classification in\na fully decentralized setting, demonstrating utility comparable to central\naggregation methods."}
{"id": "2505.20296", "pdf": "https://arxiv.org/pdf/2505.20296", "abs": "https://arxiv.org/abs/2505.20296", "authors": ["Jiahao Lu", "Ziwei Xu", "Mohan Kankanhalli"], "title": "Reasoning LLMs are Wandering Solution Explorers", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM"], "comment": "71 pages, 14 figures, 2 tables", "summary": "Large Language Models (LLMs) have demonstrated impressive reasoning abilities\nthrough test-time computation (TTC) techniques such as chain-of-thought\nprompting and tree-based reasoning. However, we argue that current reasoning\nLLMs (RLLMs) lack the ability to systematically explore the solution space.\nThis paper formalizes what constitutes systematic problem solving and\nidentifies common failure modes that reveal reasoning LLMs to be wanderers\nrather than systematic explorers. Through qualitative and quantitative analysis\nacross multiple state-of-the-art LLMs, we uncover persistent issues: invalid\nreasoning steps, redundant explorations, hallucinated or unfaithful\nconclusions, and so on. Our findings suggest that current models' performance\ncan appear to be competent on simple tasks yet degrade sharply as complexity\nincreases. Based on the findings, we advocate for new metrics and tools that\nevaluate not just final outputs but the structure of the reasoning process\nitself."}
{"id": "2505.18658", "pdf": "https://arxiv.org/pdf/2505.18658", "abs": "https://arxiv.org/abs/2505.18658", "authors": ["Pankaj Kumar", "Subhankar Mishra"], "title": "Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have emerged as a promising cornerstone for the\ndevelopment of natural language processing (NLP) and artificial intelligence\n(AI). However, ensuring the robustness of LLMs remains a critical challenge. To\naddress these challenges and advance the field, this survey provides a\ncomprehensive overview of current studies in this area. First, we\nsystematically examine the nature of robustness in LLMs, including its\nconceptual foundations, the importance of consistent performance across diverse\ninputs, and the implications of failure modes in real-world applications. Next,\nwe analyze the sources of non-robustness, categorizing intrinsic model\nlimitations, data-driven vulnerabilities, and external adversarial factors that\ncompromise reliability. Following this, we review state-of-the-art mitigation\nstrategies, and then we discuss widely adopted benchmarks, emerging metrics,\nand persistent gaps in assessing real-world reliability. Finally, we synthesize\nfindings from existing surveys and interdisciplinary studies to highlight\ntrends, unresolved issues, and pathways for future research."}
{"id": "2505.19982", "pdf": "https://arxiv.org/pdf/2505.19982", "abs": "https://arxiv.org/abs/2505.19982", "authors": ["Anji Liu", "Guy Van den Broeck"], "title": "Rethinking Probabilistic Circuit Parameter Learning", "categories": ["cs.LG"], "comment": null, "summary": "Probabilistic Circuits (PCs) offer a computationally scalable framework for\ngenerative modeling, supporting exact and efficient inference of a wide range\nof probabilistic queries. While recent advances have significantly improved the\nexpressiveness and scalability of PCs, effectively training their parameters\nremains a challenge. In particular, a widely used optimization method,\nfull-batch Expectation-Maximization (EM), requires processing the entire\ndataset before performing a single update, making it ineffective for large\ndatasets. While empirical extensions to the mini-batch setting have been\nproposed, it remains unclear what objective these algorithms are optimizing,\nmaking it difficult to assess their theoretical soundness. This paper bridges\nthe gap by establishing a novel connection between the general EM objective and\nthe standard full-batch EM algorithm. Building on this, we derive a\ntheoretically grounded generalization to the mini-batch setting and demonstrate\nits effectiveness through preliminary empirical results."}
{"id": "2505.20298", "pdf": "https://arxiv.org/pdf/2505.20298", "abs": "https://arxiv.org/abs/2505.20298", "authors": ["Jeonghun Baek", "Kazuki Egashira", "Shota Onohara", "Atsuyuki Miyai", "Yuki Imajuku", "Hikaru Ikuta", "Kiyoharu Aizawa"], "title": "MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal Manga Understanding", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "20 pages, 11 figures", "summary": "Manga, or Japanese comics, is a richly multimodal narrative form that blends\nimages and text in complex ways. Teaching large multimodal models (LMMs) to\nunderstand such narratives at a human-like level could help manga creators\nreflect on and refine their stories. To this end, we introduce two benchmarks\nfor multimodal manga understanding: MangaOCR, which targets in-page text\nrecognition, and MangaVQA, a novel benchmark designed to evaluate contextual\nunderstanding through visual question answering. MangaVQA consists of 526\nhigh-quality, manually constructed question-answer pairs, enabling reliable\nevaluation across diverse narrative and visual scenarios. Building on these\nbenchmarks, we develop MangaLMM, a manga-specialized model finetuned from the\nopen-source LMM Qwen2.5-VL to jointly handle both tasks. Through extensive\nexperiments, including comparisons with proprietary models such as GPT-4o and\nGemini 2.5, we assess how well LMMs understand manga. Our benchmark and model\nprovide a comprehensive foundation for evaluating and advancing LMMs in the\nrichly narrative domain of manga."}
{"id": "2505.18659", "pdf": "https://arxiv.org/pdf/2505.18659", "abs": "https://arxiv.org/abs/2505.18659", "authors": ["Sangwoo Park", "Matteo Zecchin", "Osvaldo Simeone"], "title": "Adaptive Prediction-Powered AutoEval with Reliability and Efficiency Guarantees", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.ME"], "comment": "submitted", "summary": "Selecting artificial intelligence (AI) models, such as large language models\n(LLMs), from multiple candidates requires accurate performance estimation. This\nis ideally achieved through empirical evaluations involving abundant real-world\ndata. However, such evaluations are costly and impractical at scale. To address\nthis challenge, autoevaluation methods leverage synthetic data produced by\nautomated evaluators, such as LLMs-as-judges, reducing variance but potentially\nintroducing bias. Recent approaches have employed semi-supervised\nprediction-powered inference (\\texttt{PPI}) to correct for the bias of\nautoevaluators. However, the use of autoevaluators may lead in practice to a\ndegradation in sample efficiency compared to conventional methods using only\nreal-world data. In this paper, we propose \\texttt{R-AutoEval+}, a novel\nframework that provides finite-sample reliability guarantees on the model\nevaluation, while also ensuring an enhanced (or at least no worse) sample\nefficiency compared to conventional methods. The key innovation of\n\\texttt{R-AutoEval+} is an adaptive construction of the model evaluation\nvariable, which dynamically tunes its reliance on synthetic data, reverting to\nconventional methods when the autoevaluator is insufficiently accurate.\nExperiments on the use of LLMs-as-judges for the optimization of quantization\nsettings for the weights of an LLM, and for prompt design in LLMs confirm the\nreliability and efficiency of \\texttt{R-AutoEval+}."}
{"id": "2505.19986", "pdf": "https://arxiv.org/pdf/2505.19986", "abs": "https://arxiv.org/abs/2505.19986", "authors": ["Swetha Ganesh", "Vaneet Aggarwal"], "title": "Regret Analysis of Average-Reward Unichain MDPs via an Actor-Critic Approach", "categories": ["cs.LG", "stat.ML"], "comment": "31 pages", "summary": "Actor-Critic methods are widely used for their scalability, yet existing\ntheoretical guarantees for infinite-horizon average-reward Markov Decision\nProcesses (MDPs) often rely on restrictive ergodicity assumptions. We propose\nNAC-B, a Natural Actor-Critic with Batching, that achieves order-optimal regret\nof $\\tilde{O}(\\sqrt{T})$ in infinite-horizon average-reward MDPs under the\nunichain assumption, which permits both transient states and periodicity. This\nassumption is among the weakest under which the classic policy gradient theorem\nremains valid for average-reward settings. NAC-B employs function approximation\nfor both the actor and the critic, enabling scalability to problems with large\nstate and action spaces. The use of batching in our algorithm helps mitigate\npotential periodicity in the MDP and reduces stochasticity in gradient\nestimates, and our analysis formalizes these benefits through the introduction\nof the constants $C_{\\text{hit}}$ and $C_{\\text{tar}}$, which characterize the\nrate at which empirical averages over Markovian samples converge to the\nstationary distribution."}
{"id": "2309.03824", "pdf": "https://arxiv.org/pdf/2309.03824", "abs": "https://arxiv.org/abs/2309.03824", "authors": ["Habib Hajimolahoseini", "Walid Ahmed", "Yang Liu"], "title": "Training Acceleration of Low-Rank Decomposed Networks using Sequential Freezing and Rank Quantization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Low Rank Decomposition (LRD) is a model compression technique applied to the\nweight tensors of deep learning models in order to reduce the number of\ntrainable parameters and computational complexity. However, due to high number\nof new layers added to the architecture after applying LRD, it may not lead to\na high training/inference acceleration if the decomposition ranks are not small\nenough. The issue is that using small ranks increases the risk of significant\naccuracy drop after decomposition. In this paper, we propose two techniques for\naccelerating low rank decomposed models without requiring to use small ranks\nfor decomposition. These methods include rank optimization and sequential\nfreezing of decomposed layers. We perform experiments on both convolutional and\ntransformer-based models. Experiments show that these techniques can improve\nthe model throughput up to 60% during training and 37% during inference when\ncombined together while preserving the accuracy close to that of the original\nmodels"}
{"id": "2505.18674", "pdf": "https://arxiv.org/pdf/2505.18674", "abs": "https://arxiv.org/abs/2505.18674", "authors": ["Peng Xiao", "Hongbo Zhao", "Yijun Wang", "Jianxin Lin"], "title": "Restoring Real-World Images with an Internal Detail Enhancement Diffusion Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Restoring real-world degraded images, such as old photographs or\nlow-resolution images, presents a significant challenge due to the complex,\nmixed degradations they exhibit, such as scratches, color fading, and noise.\nRecent data-driven approaches have struggled with two main challenges:\nachieving high-fidelity restoration and providing object-level control over\ncolorization. While diffusion models have shown promise in generating\nhigh-quality images with specific controls, they often fail to fully preserve\nimage details during restoration. In this work, we propose an internal\ndetail-preserving diffusion model for high-fidelity restoration of real-world\ndegraded images. Our method utilizes a pre-trained Stable Diffusion model as a\ngenerative prior, eliminating the need to train a model from scratch. Central\nto our approach is the Internal Image Detail Enhancement (IIDE) technique,\nwhich directs the diffusion model to preserve essential structural and textural\ninformation while mitigating degradation effects. The process starts by mapping\nthe input image into a latent space, where we inject the diffusion denoising\nprocess with degradation operations that simulate the effects of various\ndegradation factors. Extensive experiments demonstrate that our method\nsignificantly outperforms state-of-the-art models in both qualitative\nassessments and perceptual quantitative evaluations. Additionally, our approach\nsupports text-guided restoration, enabling object-level colorization control\nthat mimics the expertise of professional photo editing."}
{"id": "2505.19996", "pdf": "https://arxiv.org/pdf/2505.19996", "abs": "https://arxiv.org/abs/2505.19996", "authors": ["Qilong Wu", "Yiyang Shao", "Jun Wang", "Xiaobo Sun"], "title": "Learning Optimal Multimodal Information Bottleneck Representations", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Leveraging high-quality joint representations from multimodal data can\ngreatly enhance model performance in various machine-learning based\napplications. Recent multimodal learning methods, based on the multimodal\ninformation bottleneck (MIB) principle, aim to generate optimal MIB with\nmaximal task-relevant information and minimal superfluous information via\nregularization. However, these methods often set ad hoc regularization weights\nand overlook imbalanced task-relevant information across modalities, limiting\ntheir ability to achieve optimal MIB. To address this gap, we propose a novel\nmultimodal learning framework, Optimal Multimodal Information Bottleneck\n(OMIB), whose optimization objective guarantees the achievability of optimal\nMIB by setting the regularization weight within a theoretically derived bound.\nOMIB further addresses imbalanced task-relevant information by dynamically\nadjusting regularization weights per modality, promoting the inclusion of all\ntask-relevant information. Moreover, we establish a solid\ninformation-theoretical foundation for OMIB's optimization and implement it\nunder the variational approximation framework for computational efficiency.\nFinally, we empirically validate the OMIB's theoretical properties on synthetic\ndata and demonstrate its superiority over the state-of-the-art benchmark\nmethods in various downstream tasks."}
{"id": "2309.03965", "pdf": "https://arxiv.org/pdf/2309.03965", "abs": "https://arxiv.org/abs/2309.03965", "authors": ["Omar Mohamed Awad", "Habib Hajimolahoseini", "Michael Lim", "Gurpreet Gosal", "Walid Ahmed", "Yang Liu", "Gordon Deng"], "title": "Improving Resnet-9 Generalization Trained on Small Datasets", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "This paper presents our proposed approach that won the first prize at the\nICLR competition on Hardware Aware Efficient Training. The challenge is to\nachieve the highest possible accuracy in an image classification task in less\nthan 10 minutes. The training is done on a small dataset of 5000 images picked\nrandomly from CIFAR-10 dataset. The evaluation is performed by the competition\norganizers on a secret dataset with 1000 images of the same size. Our approach\nincludes applying a series of technique for improving the generalization of\nResNet-9 including: sharpness aware optimization, label smoothing, gradient\ncentralization, input patch whitening as well as metalearning based training.\nOur experiments show that the ResNet-9 can achieve the accuracy of 88% while\ntrained only on a 10% subset of CIFAR-10 dataset in less than 10 minuets"}
{"id": "2505.18675", "pdf": "https://arxiv.org/pdf/2505.18675", "abs": "https://arxiv.org/abs/2505.18675", "authors": ["Sicheng Feng", "Song Wang", "Shuyi Ouyang", "Lingdong Kong", "Zikai Song", "Jianke Zhu", "Huan Wang", "Xinchao Wang"], "title": "Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual Reasoning from Transit Maps", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Multimodal large language models (MLLMs) have recently achieved significant\nprogress in visual tasks, including semantic scene understanding and text-image\nalignment, with reasoning variants enhancing performance on complex tasks\ninvolving mathematics and logic. However, their capacity for reasoning tasks\ninvolving fine-grained visual understanding remains insufficiently evaluated.\nTo address this gap, we introduce ReasonMap, a benchmark designed to assess the\nfine-grained visual understanding and spatial reasoning abilities of MLLMs.\nReasonMap encompasses high-resolution transit maps from 30 cities across 13\ncountries and includes 1,008 question-answer pairs spanning two question types\nand three templates. Furthermore, we design a two-level evaluation pipeline\nthat properly assesses answer correctness and quality. Comprehensive\nevaluations of 15 popular MLLMs, including both base and reasoning variants,\nreveal a counterintuitive pattern: among open-source models, base models\noutperform reasoning ones, while the opposite trend is observed in\nclosed-source models. Additionally, performance generally degrades when visual\ninputs are masked, indicating that while MLLMs can leverage prior knowledge to\nanswer some questions, fine-grained visual reasoning tasks still require\ngenuine visual perception for strong performance. Our benchmark study offers\nnew insights into visual reasoning and contributes to investigating the gap\nbetween open-source and closed-source models."}
{"id": "2505.19997", "pdf": "https://arxiv.org/pdf/2505.19997", "abs": "https://arxiv.org/abs/2505.19997", "authors": ["Tao Wu", "Jingyuan Chen", "Wang Lin", "Mengze Li", "Yumeng Zhu", "Ang Li", "Kun Kuang", "Fei Wu"], "title": "Embracing Imperfection: Simulating Students with Diverse Cognitive Levels Using LLM-based Agents", "categories": ["cs.LG", "cs.CL", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) are revolutionizing education, with LLM-based\nagents playing a key role in simulating student behavior. A major challenge in\nstudent simulation is modeling the diverse learning patterns of students at\nvarious cognitive levels. However, current LLMs, typically trained as ``helpful\nassistants'', target at generating perfect responses. As a result, they\nstruggle to simulate students with diverse cognitive abilities, as they often\nproduce overly advanced answers, missing the natural imperfections that\ncharacterize student learning and resulting in unrealistic simulations. To\naddress this issue, we propose a training-free framework for student\nsimulation. We begin by constructing a cognitive prototype for each student\nusing a knowledge graph, which captures their understanding of concepts from\npast learning records. This prototype is then mapped to new tasks to predict\nstudent performance. Next, we simulate student solutions based on these\npredictions and iteratively refine them using a beam search method to better\nreplicate realistic mistakes. To validate our approach, we construct the\n\\texttt{Student\\_100} dataset, consisting of $100$ students working on Python\nprogramming and $5,000$ learning records. Experimental results show that our\nmethod consistently outperforms baseline models, achieving $100\\%$ improvement\nin simulation accuracy."}
{"id": "2311.03426", "pdf": "https://arxiv.org/pdf/2311.03426", "abs": "https://arxiv.org/abs/2311.03426", "authors": ["Farnoosh Javadi", "Walid Ahmed", "Habib Hajimolahoseini", "Foozhan Ataiefard", "Mohammad Hassanpour", "Saina Asani", "Austin Wen", "Omar Mohamed Awad", "Kangling Liu", "Yang Liu"], "title": "GQKVA: Efficient Pre-training of Transformers by Grouping Queries, Keys, and Values", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Massive transformer-based models face several challenges, including slow and\ncomputationally intensive pre-training and over-parametrization. This paper\naddresses these challenges by proposing a versatile method called GQKVA, which\ngeneralizes query, key, and value grouping techniques. GQKVA is designed to\nspeed up transformer pre-training while reducing the model size. Our\nexperiments with various GQKVA variants highlight a clear trade-off between\nperformance and model size, allowing for customized choices based on resource\nand time limitations. Our findings also indicate that the conventional\nmulti-head attention approach is not always the best choice, as there are\nlighter and faster alternatives available. We tested our method on ViT, which\nachieved an approximate 0.3% increase in accuracy while reducing the model size\nby about 4% in the task of image classification. Additionally, our most\naggressive model reduction experiment resulted in a reduction of approximately\n15% in model size, with only around a 1% drop in accuracy."}
{"id": "2505.18687", "pdf": "https://arxiv.org/pdf/2505.18687", "abs": "https://arxiv.org/abs/2505.18687", "authors": ["Aran Nayebi"], "title": "An AI Capability Threshold for Rent-Funded Universal Basic Income in an AI-Automated Economy", "categories": ["econ.GN", "cs.AI", "cs.GT", "q-fin.EC"], "comment": "12 pages, 3 figures", "summary": "We derive the first closed-form condition under which artificial intelligence\n(AI) capital profits could sustainably finance a universal basic income (UBI)\nwithout additional taxes or new job creation. In a Solow-Zeira economy\ncharacterized by a continuum of automatable tasks, a constant net saving rate\n$s$, and task-elasticity $\\sigma < 1$, we analyze how the AI capability\nthreshold--defined as the productivity level of AI relative to pre-AI\nautomation--varies under different economic scenarios. At present economic\nparameters, we find that AI systems must achieve only approximately 5-6 times\nexisting automation productivity to finance an 11\\%-of-GDP UBI, in the worst\ncase situation where \\emph{no} new jobs or tasks are created.\n  Our analysis also reveals some specific policy levers: raising public revenue\nshare (e.g. profit taxation) of AI capital from the current 15\\% to about 33\\%\nhalves the required AI capability threshold to attain UBI to 3 times existing\nautomotion productivity, but gains diminish beyond 50\\% public revenue share,\nespecially if regulatory costs increase. Market structure also strongly affects\noutcomes: monopolistic or concentrated oligopolistic markets reduce the\nthreshold by increasing economic rents, whereas heightened competition\nsignificantly raises it.\n  Overall, these results suggest a couple policy recommendations: maximizing\npublic revenue share up to a point so that operating costs are minimized, and\nstrategically managing market competition can ensure AI's growing capabilities\ntranslate into meaningful social benefits within realistic technological\nprogress scenarios."}
{"id": "2505.20003", "pdf": "https://arxiv.org/pdf/2505.20003", "abs": "https://arxiv.org/abs/2505.20003", "authors": ["Qiong Zhang", "Yan Shuo Tan", "Qinglong Tian", "Pengfei Li"], "title": "TabPFN: One Model to Rule Them All?", "categories": ["cs.LG", "stat.ME", "stat.ML"], "comment": null, "summary": "Hollmann et al. (Nature 637 (2025) 319-326) recently introduced TabPFN, a\ntransformer-based deep learning model for regression and classification on\ntabular data, which they claim \"outperforms all previous methods on datasets\nwith up to 10,000 samples by a wide margin, using substantially less training\ntime.\" Furthermore, they have called TabPFN a \"foundation model\" for tabular\ndata, as it can support \"data generation, density estimation, learning reusable\nembeddings and fine-tuning\". If these statements are well-supported, TabPFN may\nhave the potential to supersede existing modeling approaches on a wide range of\nstatistical tasks, mirroring a similar revolution in other areas of artificial\nintelligence that began with the advent of large language models. In this\npaper, we provide a tailored explanation of how TabPFN works for a statistics\naudience, by emphasizing its interpretation as approximate Bayesian inference.\nWe also provide more evidence of TabPFN's \"foundation model\" capabilities: We\nshow that an out-of-the-box application of TabPFN vastly outperforms\nspecialized state-of-the-art methods for semi-supervised parameter estimation,\nprediction under covariate shift, and heterogeneous treatment effect\nestimation. We further show that TabPFN can outperform LASSO at sparse\nregression and can break a robustness-efficiency trade-off in classification.\nAll experiments can be reproduced using the code provided at\nhttps://github.com/qinglong-tian/tabpfn_study\n(https://github.com/qinglong-tian/tabpfn_study)."}
{"id": "2406.12634", "pdf": "https://arxiv.org/pdf/2406.12634", "abs": "https://arxiv.org/abs/2406.12634", "authors": ["Andreea Iana", "Fabian David Schmidt", "Goran Glava", "Heiko Paulheim"], "title": "News Without Borders: Domain Adaptation of Multilingual Sentence Embeddings for Cross-lingual News Recommendation", "categories": ["cs.IR", "cs.AI", "cs.CL", "I.2.7; H.3.3"], "comment": "Accepted at the 47th European Conference on Information Retrieval\n  (ECIR 2025) Appendix A is provided only in the arXiv version", "summary": "Rapidly growing numbers of multilingual news consumers pose an increasing\nchallenge to news recommender systems in terms of providing customized\nrecommendations. First, existing neural news recommenders, even when powered by\nmultilingual language models (LMs), suffer substantial performance losses in\nzero-shot cross-lingual transfer (ZS-XLT). Second, the current paradigm of\nfine-tuning the backbone LM of a neural recommender on task-specific data is\ncomputationally expensive and infeasible in few-shot recommendation and\ncold-start setups, where data is scarce or completely unavailable. In this\nwork, we propose a news-adapted sentence encoder (NaSE), domain-specialized\nfrom a pretrained massively multilingual sentence encoder (SE). To this end, we\nconstruct and leverage PolyNews and PolyNewsParallel, two multilingual\nnews-specific corpora. With the news-adapted multilingual SE in place, we test\nthe effectiveness of (i.e., question the need for) supervised fine-tuning for\nnews recommendation, and propose a simple and strong baseline based on (i)\nfrozen NaSE embeddings and (ii) late click-behavior fusion. We show that NaSE\nachieves state-of-the-art performance in ZS-XLT in true cold-start and few-shot\nnews recommendation."}
{"id": "2505.18688", "pdf": "https://arxiv.org/pdf/2505.18688", "abs": "https://arxiv.org/abs/2505.18688", "authors": ["Aleksandr Tsymbalov"], "title": "Large Language Models in the Task of Automatic Validation of Text Classifier Predictions", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Machine learning models for text classification are trained to predict a\nclass for a given text. To do this, training and validation samples must be\nprepared: a set of texts is collected, and each text is assigned a class. These\nclasses are usually assigned by human annotators with different expertise\nlevels, depending on the specific classification task. Collecting such samples\nfrom scratch is labor-intensive because it requires finding specialists and\ncompensating them for their work; moreover, the number of available specialists\nis limited, and their productivity is constrained by human factors. While it\nmay not be too resource-intensive to collect samples once, the ongoing need to\nretrain models (especially in incremental learning pipelines) to address data\ndrift (also called model drift) makes the data collection process crucial and\ncostly over the model's entire lifecycle. This paper proposes several\napproaches to replace human annotators with Large Language Models (LLMs) to\ntest classifier predictions for correctness, helping ensure model quality and\nsupport high-quality incremental learning."}
{"id": "2505.20010", "pdf": "https://arxiv.org/pdf/2505.20010", "abs": "https://arxiv.org/abs/2505.20010", "authors": ["Gianmarco Genalti", "Francesco Emanuele Stradi", "Matteo Castiglioni", "Alberto Marchesi", "Nicola Gatti"], "title": "Data-Dependent Regret Bounds for Constrained MABs", "categories": ["cs.LG"], "comment": null, "summary": "This paper initiates the study of data-dependent regret bounds in constrained\nMAB settings. These bounds depend on the sequence of losses that characterize\nthe problem instance. Thus, they can be much smaller than classical\n$\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret bounds, while being equivalent to\nthem in the worst case. Despite this, data-dependent regret bounds have been\ncompletely overlooked in constrained MAB settings. The goal of this paper is to\nanswer the following question: Can data-dependent regret bounds be derived in\nthe presence of constraints? We answer this question affirmatively in\nconstrained MABs with adversarial losses and stochastic constraints.\nSpecifically, our main focus is on the most challenging and natural settings\nwith hard constraints, where the learner must ensure that the constraints are\nalways satisfied with high probability. We design an algorithm with a regret\nbound consisting of two data-dependent terms. The first term captures the\ndifficulty of satisfying the constraints, while the second one encodes the\ncomplexity of learning independently of the presence of constraints. We also\nprove a lower bound showing that these two terms are not artifacts of our\nspecific approach and analysis, but rather the fundamental components that\ninherently characterize the complexities of the problem. Finally, in designing\nour algorithm, we also derive some novel results in the related (and easier)\nsoft constraints settings, which may be of independent interest."}
{"id": "2407.20266", "pdf": "https://arxiv.org/pdf/2407.20266", "abs": "https://arxiv.org/abs/2407.20266", "authors": ["Habib Hajimolahoseini", "Walid Ahmed", "Austin Wen", "Yang Liu"], "title": "Accelerating the Low-Rank Decomposed Models", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Tensor decomposition is a mathematically supported technique for data\ncompression. It consists of applying some kind of a Low Rank Decomposition\ntechnique on the tensors or matrices in order to reduce the redundancy of the\ndata. However, it is not a popular technique for compressing the AI models duo\nto the high number of new layers added to the architecture after decomposition.\nAlthough the number of parameters could shrink significantly, it could result\nin the model be more than twice deeper which could add some latency to the\ntraining or inference. In this paper, we present a comprehensive study about\nhow to modify low rank decomposition technique in AI models so that we could\nbenefit from both high accuracy and low memory consumption as well as speeding\nup the training and inference"}
{"id": "2505.18697", "pdf": "https://arxiv.org/pdf/2505.18697", "abs": "https://arxiv.org/abs/2505.18697", "authors": ["Ziyang Cheng", "Zhixun Li", "Yuhan Li", "Yixin Song", "Kangyi Zhao", "Dawei Cheng", "Jia Li", "Jeffrey Xu Yu"], "title": "Can LLMs Alleviate Catastrophic Forgetting in Graph Continual Learning? A Systematic Study", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Nowadays, real-world data, including graph-structure data, often arrives in a\nstreaming manner, which means that learning systems need to continuously\nacquire new knowledge without forgetting previously learned information.\nAlthough substantial existing works attempt to address catastrophic forgetting\nin graph machine learning, they are all based on training from scratch with\nstreaming data. With the rise of pretrained models, an increasing number of\nstudies have leveraged their strong generalization ability for continual\nlearning. Therefore, in this work, we attempt to answer whether large language\nmodels (LLMs) can mitigate catastrophic forgetting in Graph Continual Learning\n(GCL). We first point out that current experimental setups for GCL have\nsignificant flaws, as the evaluation stage may lead to task ID leakage. Then,\nwe evaluate the performance of LLMs in more realistic scenarios and find that\neven minor modifications can lead to outstanding results. Finally, based on\nextensive experiments, we propose a simple-yet-effective method, Simple Graph\nContinual Learning (SimGCL), that surpasses the previous state-of-the-art\nGNN-based baseline by around 20% under the rehearsal-free constraint. To\nfacilitate reproducibility, we have developed an easy-to-use benchmark LLM4GCL\nfor training and evaluating existing GCL methods. The code is available at:\nhttps://github.com/ZhixunLEE/LLM4GCL."}
{"id": "2505.20020", "pdf": "https://arxiv.org/pdf/2505.20020", "abs": "https://arxiv.org/abs/2505.20020", "authors": ["Natallia Kokash", "Lei Wang", "Thomas H. Gillespie", "Adam Belloum", "Paola Grosso", "Sara Quinney", "Lang Li", "Bernard de Bono"], "title": "Ontology- and LLM-based Data Harmonization for Federated Learning in Healthcare", "categories": ["cs.LG", "cs.SE"], "comment": "Related dataset: https://doi.org/10.5281/zenodo.15411810", "summary": "The rise of electronic health records (EHRs) has unlocked new opportunities\nfor medical research, but privacy regulations and data heterogeneity remain key\nbarriers to large-scale machine learning. Federated learning (FL) enables\ncollaborative modeling without sharing raw data, yet faces challenges in\nharmonizing diverse clinical datasets. This paper presents a two-step data\nalignment strategy integrating ontologies and large language models (LLMs) to\nsupport secure, privacy-preserving FL in healthcare, demonstrating its\neffectiveness in a real-world project involving semantic mapping of EHR data."}
{"id": "2505.16849", "pdf": "https://arxiv.org/pdf/2505.16849", "abs": "https://arxiv.org/abs/2505.16849", "authors": ["Martin Bckling", "Heiko Paulheim", "Andreea Iana"], "title": "Walk&Retrieve: Simple Yet Effective Zero-shot Retrieval-Augmented Generation via Knowledge Graph Walks", "categories": ["cs.IR", "cs.CL", "H.3.3; I.2.7"], "comment": "Accepted at the Information Retrieval's Role in RAG Systems (IR-RAG\n  2025) in conjunction with SIGIR 2025", "summary": "Large Language Models (LLMs) have showcased impressive reasoning abilities,\nbut often suffer from hallucinations or outdated knowledge. Knowledge Graph\n(KG)-based Retrieval-Augmented Generation (RAG) remedies these shortcomings by\ngrounding LLM responses in structured external information from a knowledge\nbase. However, many KG-based RAG approaches struggle with (i) aligning KG and\ntextual representations, (ii) balancing retrieval accuracy and efficiency, and\n(iii) adapting to dynamically updated KGs. In this work, we introduce\nWalk&Retrieve, a simple yet effective KG-based framework that leverages\nwalk-based graph traversal and knowledge verbalization for corpus generation\nfor zero-shot RAG. Built around efficient KG walks, our method does not require\nfine-tuning on domain-specific data, enabling seamless adaptation to KG\nupdates, reducing computational overhead, and allowing integration with any\noff-the-shelf backbone LLM. Despite its simplicity, Walk&Retrieve performs\ncompetitively, often outperforming existing RAG systems in response accuracy\nand hallucination reduction. Moreover, it demonstrates lower query latency and\nrobust scalability to large KGs, highlighting the potential of lightweight\nretrieval strategies as strong baselines for future RAG research."}
{"id": "2505.18700", "pdf": "https://arxiv.org/pdf/2505.18700", "abs": "https://arxiv.org/abs/2505.18700", "authors": ["Chun Wang", "Xiaoran Pan", "Zihao Pan", "Haofan Wang", "Yiren Song"], "title": "GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language Models and Enhanced Reasoning Chains", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in Visual Language Models (VLMs) have demonstrated\nexceptional performance in visual reasoning tasks. However, geo-localization\npresents unique challenges, requiring the extraction of multigranular visual\ncues from images and their integration with external world knowledge for\nsystematic reasoning. Current approaches to geo-localization tasks often lack\nrobust reasoning mechanisms and explainability, limiting their effectiveness.\nTo address these limitations, we propose the Geo Reason Enhancement (GRE)\nSuite, a novel framework that augments VLMs with structured reasoning chains\nfor accurate and interpretable location inference. The GRE Suite is\nsystematically developed across three key dimensions: dataset, model, and\nbenchmark. First, we introduce GRE30K, a high-quality geo-localization\nreasoning dataset designed to facilitate fine-grained visual and contextual\nanalysis. Next, we present the GRE model, which employs a multi-stage reasoning\nstrategy to progressively infer scene attributes, local details, and semantic\nfeatures, thereby narrowing down potential geographic regions with enhanced\nprecision. Finally, we construct the Geo Reason Evaluation Benchmark\n(GREval-Bench), a comprehensive evaluation framework that assesses VLMs across\ndiverse urban, natural, and landmark scenes to measure both coarse-grained\n(e.g., country, continent) and fine-grained (e.g., city, street) localization\nperformance. Experimental results demonstrate that GRE significantly\noutperforms existing methods across all granularities of geo-localization\ntasks, underscoring the efficacy of reasoning-augmented VLMs in complex\ngeographic inference. Code and data will be released at\nhttps://github.com/Thorin215/GRE."}
{"id": "2505.20026", "pdf": "https://arxiv.org/pdf/2505.20026", "abs": "https://arxiv.org/abs/2505.20026", "authors": ["Xinping Chen", "Chen Liu"], "title": "Gradient Inversion Transcript: Leveraging Robust Generative Priors to Reconstruct Training Data from Gradient Leakage", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose Gradient Inversion Transcript (GIT), a novel generative approach\nfor reconstructing training data from leaked gradients. GIT employs a\ngenerative attack model, whose architecture is tailored to align with the\nstructure of the leaked model based on theoretical analysis. Once trained\noffline, GIT can be deployed efficiently and only relies on the leaked\ngradients to reconstruct the input data, rendering it applicable under various\ndistributed learning environments. When used as a prior for other iterative\noptimization-based methods, GIT not only accelerates convergence but also\nenhances the overall reconstruction quality. GIT consistently outperforms\nexisting methods across multiple datasets and demonstrates strong robustness\nunder challenging conditions, including inaccurate gradients, data distribution\nshifts and discrepancies in model parameters."}
{"id": "2505.18212", "pdf": "https://arxiv.org/pdf/2505.18212", "abs": "https://arxiv.org/abs/2505.18212", "authors": ["Barbara Puccio", "Federico Castagna", "Allan Tucker", "Pierangelo Veltri"], "title": "Towards medical AI misalignment: a preliminary study", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Despite their staggering capabilities as assistant tools, often exceeding\nhuman performances, Large Language Models (LLMs) are still prone to jailbreak\nattempts from malevolent users. Although red teaming practices have already\nidentified and helped to address several such jailbreak techniques, one\nparticular sturdy approach involving role-playing (which we named `Goofy Game')\nseems effective against most of the current LLMs safeguards. This can result in\nthe provision of unsafe content, which, although not harmful per se, might lead\nto dangerous consequences if delivered in a setting such as the medical domain.\nIn this preliminary and exploratory study, we provide an initial analysis of\nhow, even without technical knowledge of the internal architecture and\nparameters of generative AI models, a malicious user could construct a\nrole-playing prompt capable of coercing an LLM into producing incorrect (and\npotentially harmful) clinical suggestions. We aim to illustrate a specific\nvulnerability scenario, providing insights that can support future advancements\nin the field."}
{"id": "2505.18706", "pdf": "https://arxiv.org/pdf/2505.18706", "abs": "https://arxiv.org/abs/2505.18706", "authors": ["Viacheslav Sinii", "Alexey Gorbatovski", "Artem Cherepanov", "Boris Shaposhnikov", "Nikita Balagansky", "Daniil Gavrilov"], "title": "Steering LLM Reasoning Through Bias-Only Adaptation", "categories": ["cs.LG", "cs.AI"], "comment": "Preprint", "summary": "Recent work on reasoning-oriented language models, exemplified by o1-like\nsystems, suggests that reinforcement-learning (RL) finetuning does not create\nnew capabilities but instead strengthens reasoning patterns already latent in\nthe pretrained network. We test this claim by training steering vectors:\nlayer-wise biases that additively amplify selected hidden features while\nleaving all original weights unchanged. Experiments on four base models across\nthe GSM8K and MATH benchmarks show that steering vectors recover, and in\nseveral cases exceed, the accuracy of fully-tuned counterparts. This result\nsupports the view that the required reasoning skills pre-exist in the base\nmodel. Further, logit-lens analysis reveals that the trained vectors\nconsistently boost token groups linked to structured languages and logical\nconnectors, providing an interpretable account that aligns with the demands of\nquantitative reasoning tasks."}
{"id": "2505.20030", "pdf": "https://arxiv.org/pdf/2505.20030", "abs": "https://arxiv.org/abs/2505.20030", "authors": ["Wenbo Wei", "Nicholas Chong Jia Le", "Choy Heng Lai", "Ling Feng"], "title": "Multiple Descents in Deep Learning as a Sequence of Order-Chaos Transitions", "categories": ["cs.LG", "cs.AI", "nlin.CD", "physics.comp-ph"], "comment": null, "summary": "We observe a novel 'multiple-descent' phenomenon during the training process\nof LSTM, in which the test loss goes through long cycles of up and down trend\nmultiple times after the model is overtrained. By carrying out asymptotic\nstability analysis of the models, we found that the cycles in test loss are\nclosely associated with the phase transition process between order and chaos,\nand the local optimal epochs are consistently at the critical transition point\nbetween the two phases. More importantly, the global optimal epoch occurs at\nthe first transition from order to chaos, where the 'width' of the 'edge of\nchaos' is the widest, allowing the best exploration of better weight\nconfigurations for learning."}
{"id": "2505.18221", "pdf": "https://arxiv.org/pdf/2505.18221", "abs": "https://arxiv.org/abs/2505.18221", "authors": ["Sharad Duwal", "Mir Nafis Sharear Shopnil", "Abhishek Tyagi", "Adiba Mahbub Proma"], "title": "Evidence-Grounded Multimodal Misinformation Detection with Attention-Based GNNs", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "Multimodal out-of-context (OOC) misinformation is misinformation that\nrepurposes real images with unrelated or misleading captions. Detecting such\nmisinformation is challenging because it requires resolving the context of the\nclaim before checking for misinformation. Many current methods, including LLMs\nand LVLMs, do not perform this contextualization step. LLMs hallucinate in\nabsence of context or parametric knowledge. In this work, we propose a\ngraph-based method that evaluates the consistency between the image and the\ncaption by constructing two graph representations: an evidence graph, derived\nfrom online textual evidence, and a claim graph, from the claim in the caption.\nUsing graph neural networks (GNNs) to encode and compare these representations,\nour framework then evaluates the truthfulness of image-caption pairs. We create\ndatasets for our graph-based method, evaluate and compare our baseline model\nagainst popular LLMs on the misinformation detection task. Our method scores\n$93.05\\%$ detection accuracy on the evaluation set and outperforms the\nsecond-best performing method (an LLM) by $2.82\\%$, making a case for smaller\nand task-specific methods."}
{"id": "2505.18708", "pdf": "https://arxiv.org/pdf/2505.18708", "abs": "https://arxiv.org/abs/2505.18708", "authors": ["Xu Zhang", "Kun Zhang", "Wenxin Ma", "Rongsheng Wang", "Chenxu Wu", "Yingtai Li", "S. Kevin Zhou"], "title": "A General Knowledge Injection Framework for ICD Coding", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "ICD Coding aims to assign a wide range of medical codes to a medical text\ndocument, which is a popular and challenging task in the healthcare domain. To\nalleviate the problems of long-tail distribution and the lack of annotations of\ncode-specific evidence, many previous works have proposed incorporating code\nknowledge to improve coding performance. However, existing methods often focus\non a single type of knowledge and design specialized modules that are complex\nand incompatible with each other, thereby limiting their scalability and\neffectiveness. To address this issue, we propose GKI-ICD, a novel, general\nknowledge injection framework that integrates three key types of knowledge,\nnamely ICD Description, ICD Synonym, and ICD Hierarchy, without specialized\ndesign of additional modules. The comprehensive utilization of the above\nknowledge, which exhibits both differences and complementarity, can effectively\nenhance the ICD coding performance. Extensive experiments on existing popular\nICD coding benchmarks demonstrate the effectiveness of GKI-ICD, which achieves\nthe state-of-the-art performance on most evaluation metrics. Code is available\nat https://github.com/xuzhang0112/GKI-ICD."}
{"id": "2505.20034", "pdf": "https://arxiv.org/pdf/2505.20034", "abs": "https://arxiv.org/abs/2505.20034", "authors": ["Juwei Yue", "Haikuo Li", "Jiawei Sheng", "Yihan Guo", "Xinghua Zhang", "Chuan Zhou", "Tingwen Liu", "Li Guo"], "title": "Graph Wave Networks", "categories": ["cs.LG"], "comment": "15 pages, 8 figures, published to WWW 2025", "summary": "Dynamics modeling has been introduced as a novel paradigm in message passing\n(MP) of graph neural networks (GNNs). Existing methods consider MP between\nnodes as a heat diffusion process, and leverage heat equation to model the\ntemporal evolution of nodes in the embedding space. However, heat equation can\nhardly depict the wave nature of graph signals in graph signal processing.\nBesides, heat equation is essentially a partial differential equation (PDE)\ninvolving a first partial derivative of time, whose numerical solution usually\nhas low stability, and leads to inefficient model training. In this paper, we\nwould like to depict more wave details in MP, since graph signals are\nessentially wave signals that can be seen as a superposition of a series of\nwaves in the form of eigenvector. This motivates us to consider MP as a wave\npropagation process to capture the temporal evolution of wave signals in the\nspace. Based on wave equation in physics, we innovatively develop a graph wave\nequation to leverage the wave propagation on graphs. In details, we demonstrate\nthat the graph wave equation can be connected to traditional spectral GNNs,\nfacilitating the design of graph wave networks based on various Laplacians and\nenhancing the performance of the spectral GNNs. Besides, the graph wave\nequation is particularly a PDE involving a second partial derivative of time,\nwhich has stronger stability on graphs than the heat equation that involves a\nfirst partial derivative of time. Additionally, we theoretically prove that the\nnumerical solution derived from the graph wave equation are constantly stable,\nenabling to significantly enhance model efficiency while ensuring its\nperformance. Extensive experiments show that GWNs achieve SOTA and efficient\nperformance on benchmark datasets, and exhibit outstanding performance in\naddressing challenging graph problems, such as over-smoothing and heterophily."}
{"id": "2505.18232", "pdf": "https://arxiv.org/pdf/2505.18232", "abs": "https://arxiv.org/abs/2505.18232", "authors": ["Mingkuan Feng", "Jinyang Wu", "Siyuan Liu", "Shuai Zhang", "Hongjian Fang", "Ruihan Jin", "Feihu Che", "Pengpeng Shao", "Zhengqi Wen", "Jianhua Tao"], "title": "ELDeR: Getting Efficient LLMs through Data-Driven Regularized Layer-wise Pruning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The deployment of Large language models (LLMs) in many fields is largely\nhindered by their high computational and memory costs. Recent studies suggest\nthat LLMs exhibit sparsity, which can be used for pruning. Previous pruning\nmethods typically follow a prune-then-finetune paradigm. Since the pruned parts\nstill contain valuable information, statically removing them without updating\nthe remaining parameters often results in irreversible performance degradation,\nrequiring costly recovery fine-tuning (RFT) to maintain performance. To address\nthis, we propose a novel paradigm: first apply regularization, then prune.\nBased on this paradigm, we propose ELDeR: Getting Efficient LLMs through\nData-Driven Regularized Layer-wise Pruning. We multiply the output of each\ntransformer layer by an initial weight, then we iteratively learn the weights\nof each transformer layer by using a small amount of data in a simple way.\nAfter that, we apply regularization to the difference between the output and\ninput of the layers with smaller weights, forcing the information to be\ntransferred to the remaining layers. Compared with direct pruning, ELDeR\nreduces the information loss caused by direct parameter removal, thus better\npreserving the model's language modeling ability. Experimental results show\nthat ELDeR achieves superior performance compared with powerful layer-wise\nstructured pruning methods, while greatly reducing RFT computational costs.\nSince ELDeR is a layer-wise pruning method, its end-to-end acceleration effect\nis obvious, making it a promising technique for efficient LLMs."}
{"id": "2505.18709", "pdf": "https://arxiv.org/pdf/2505.18709", "abs": "https://arxiv.org/abs/2505.18709", "authors": ["Sourav Kumar Das", "Md. Julkar Naeen", "MD. Jahidul Islam", "Md. Anisul Haque Sajeeb", "Narayan Ranjan Chakraborty", "Mayen Uddin Mojumdar"], "title": "Improving Bangla Linguistics: Advanced LSTM, Bi-LSTM, and Seq2Seq Models for Translating Sylheti to Modern Bangla", "categories": ["cs.CL", "cs.AI"], "comment": "2024 15th International Conference on Computing Communication and\n  Networking Technologies (ICCCNT)", "summary": "Bangla or Bengali is the national language of Bangladesh, people from\ndifferent regions don't talk in proper Bangla. Every division of Bangladesh has\nits own local language like Sylheti, Chittagong etc. In recent years some\npapers were published on Bangla language like sentiment analysis, fake news\ndetection and classifications, but a few of them were on Bangla languages. This\nresearch is for the local language and this particular paper is on Sylheti\nlanguage. It presented a comprehensive system using Natural Language Processing\nor NLP techniques for translating Pure or Modern Bangla to locally spoken\nSylheti Bangla language. Total 1200 data used for training 3 models LSTM,\nBi-LSTM and Seq2Seq and LSTM scored the best in performance with 89.3%\naccuracy. The findings of this research may contribute to the growth of Bangla\nNLP researchers for future more advanced innovations."}
{"id": "2505.20036", "pdf": "https://arxiv.org/pdf/2505.20036", "abs": "https://arxiv.org/abs/2505.20036", "authors": ["Hazem Alsamkary", "Mohamed Elshaffei", "Mohamed Soudy", "Sara Ossman", "Abdallah Amr", "Nehal Adel Abdelsalam", "Mohamed Elkerdawy", "Ahmed Elnaggar"], "title": "Beyond Simple Concatenation: Fairly Assessing PLM Architectures for Multi-Chain Protein-Protein Interactions Prediction", "categories": ["cs.LG", "q-bio.BM"], "comment": "15 pages, 4 figures", "summary": "Protein-protein interactions (PPIs) are fundamental to numerous cellular\nprocesses, and their characterization is vital for understanding disease\nmechanisms and guiding drug discovery. While protein language models (PLMs)\nhave demonstrated remarkable success in predicting protein structure and\nfunction, their application to sequence-based PPI binding affinity prediction\nremains relatively underexplored. This gap is often attributed to the scarcity\nof high-quality, rigorously refined datasets and the reliance on simple\nstrategies for concatenating protein representations. In this work, we address\nthese limitations. First, we introduce a meticulously curated version of the\nPPB-Affinity dataset of a total of 8,207 unique protein-protein interaction\nentries, by resolving annotation inconsistencies and duplicate entries for\nmulti-chain protein interactions. This dataset incorporates a stringent, less\nthan or equal to 30%, sequence identity threshold to ensure robust splitting\ninto training, validation, and test sets, minimizing data leakage. Second, we\npropose and systematically evaluate four architectures for adapting PLMs to PPI\nbinding affinity prediction: embeddings concatenation (EC), sequences\nconcatenation (SC), hierarchical pooling (HP), and pooled attention addition\n(PAD). These architectures were assessed using two training methods: full\nfine-tuning and a lightweight approach employing ConvBERT heads over frozen PLM\nfeatures. Our comprehensive experiments across multiple leading PLMs (ProtT5,\nESM2, Ankh, Ankh2, and ESM3) demonstrated that the HP and PAD architectures\nconsistently outperform conventional concatenation methods, achieving up to 12%\nincrease in terms of Spearman correlation. These results highlight the\nnecessity of sophisticated architectural designs to fully exploit the\ncapabilities of PLMs for nuanced PPI binding affinity prediction."}
{"id": "2505.18246", "pdf": "https://arxiv.org/pdf/2505.18246", "abs": "https://arxiv.org/abs/2505.18246", "authors": ["Yusuf Yildiz", "Goran Nenadic", "Meghna Jani", "David A. Jenkins"], "title": "Will Large Language Models Transform Clinical Prediction?", "categories": ["cs.CY", "cs.CL"], "comment": "Submitted to: BMC Diagnostic and Prognostic Research", "summary": "Background: Large language models (LLMs) are attracting increasing interest\nin healthcare. Their ability to summarise large datasets effectively, answer\nquestions accurately, and generate synthesised text is widely recognised. These\ncapabilities are already finding applications in healthcare. Body: This\ncommentary discusses LLMs usage in the clinical prediction context and\nhighlight potential benefits and existing challenges. In these early stages,\nthe focus should be on extending the methodology, specifically on validation,\nfairness and bias evaluation, survival analysis and development of regulations.\nConclusion: We conclude that further work and domain-specific considerations\nneed to be made for full integration into the clinical prediction workflows."}
{"id": "2505.18710", "pdf": "https://arxiv.org/pdf/2505.18710", "abs": "https://arxiv.org/abs/2505.18710", "authors": ["Yi Jiang", "Sendong Zhao", "Jianbo Li", "Haochun Wang", "Bing Qin"], "title": "GainRAG: Preference Alignment in Retrieval-Augmented Generation through Gain Signal Synthesis", "categories": ["cs.IR", "cs.AI"], "comment": "Accepted by ACL 2025", "summary": "The Retrieval-Augmented Generation (RAG) framework introduces a retrieval\nmodule to dynamically inject retrieved information into the input context of\nlarge language models (LLMs), and has demonstrated significant success in\nvarious NLP tasks. However, the current study points out that there is a\npreference gap between retrievers and LLMs in the RAG framework, which limit\nthe further improvement of system performance. Some highly relevant passages\nmay interfere with LLM reasoning because they contain complex or contradictory\ninformation; while some indirectly related or even inaccurate content may help\nLLM generate more accurate answers by providing suggestive information or\nlogical clues. To solve this, we propose GainRAG, a novel approach that aligns\nthe retriever's and LLM's preferences by defining a new metric, \"gain\", which\nmeasure how well an input passage contributes to correct outputs. Specifically,\nwe propose a method to estimate these gain signals and train a middleware that\naligns the preferences of the retriever and the LLM using only limited data. In\naddition, we introduce a pseudo-passage strategy to mitigate degradation. The\nexperimental results on 6 datasets verify the effectiveness of GainRAG."}
{"id": "2505.20048", "pdf": "https://arxiv.org/pdf/2505.20048", "abs": "https://arxiv.org/abs/2505.20048", "authors": ["Ali Forootani", "Mohammad Khosravi"], "title": "Synthetic Time Series Forecasting with Transformer Architectures: Extensive Simulation Benchmarks", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "Time series forecasting plays a critical role in domains such as energy,\nfinance, and healthcare, where accurate predictions inform decision-making\nunder uncertainty. Although Transformer-based models have demonstrated success\nin sequential modeling, their adoption for time series remains limited by\nchallenges such as noise sensitivity, long-range dependencies, and a lack of\ninductive bias for temporal structure. In this work, we present a unified and\nprincipled framework for benchmarking three prominent Transformer forecasting\narchitectures-Autoformer, Informer, and Patchtst-each evaluated through three\narchitectural variants: Minimal, Standard, and Full, representing increasing\nlevels of complexity and modeling capacity.\n  We conduct over 1500 controlled experiments on a suite of ten synthetic\nsignals, spanning five patch lengths and five forecast horizons under both\nclean and noisy conditions. Our analysis reveals consistent patterns across\nmodel families.\n  To advance this landscape further, we introduce the Koopman-enhanced\nTransformer framework, Deep Koopformer, which integrates operator-theoretic\nlatent state modeling to improve stability and interpretability. We demonstrate\nits efficacy on nonlinear and chaotic dynamical systems. Our results highlight\nKoopman based Transformer as a promising hybrid approach for robust,\ninterpretable, and theoretically grounded time series forecasting in noisy and\ncomplex real-world conditions."}
{"id": "2505.18279", "pdf": "https://arxiv.org/pdf/2505.18279", "abs": "https://arxiv.org/abs/2505.18279", "authors": ["Alireza Rezazadeh", "Zichao Li", "Ange Lou", "Yuying Zhao", "Wei Wei", "Yujia Bao"], "title": "Collaborative Memory: Multi-User Memory Sharing in LLM Agents with Dynamic Access Control", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Complex tasks are increasingly delegated to ensembles of specialized\nLLM-based agents that reason, communicate, and coordinate actions-both among\nthemselves and through interactions with external tools, APIs, and databases.\nWhile persistent memory has been shown to enhance single-agent performance,\nmost approaches assume a monolithic, single-user context-overlooking the\nbenefits and challenges of knowledge transfer across users under dynamic,\nasymmetric permissions. We introduce Collaborative Memory, a framework for\nmulti-user, multi-agent environments with asymmetric, time-evolving access\ncontrols encoded as bipartite graphs linking users, agents, and resources. Our\nsystem maintains two memory tiers: (1) private memory-private fragments visible\nonly to their originating user; and (2) shared memory-selectively shared\nfragments. Each fragment carries immutable provenance attributes (contributing\nagents, accessed resources, and timestamps) to support retrospective permission\nchecks. Granular read policies enforce current user-agent-resource constraints\nand project existing memory fragments into filtered transformed views. Write\npolicies determine fragment retention and sharing, applying context-aware\ntransformations to update the memory. Both policies may be designed conditioned\non system, agent, and user-level information. Our framework enables safe,\nefficient, and interpretable cross-user knowledge sharing, with provable\nadherence to asymmetric, time-varying policies and full auditability of memory\noperations."}
{"id": "2505.18713", "pdf": "https://arxiv.org/pdf/2505.18713", "abs": "https://arxiv.org/abs/2505.18713", "authors": ["Guodong Du", "Zitao Fang", "Jing Li", "Junlin Li", "Runhua Jiang", "Shuyang Yu", "Yifei Guo", "Yangneng Chen", "Sim Kuan Goh", "Ho-Kin Tang", "Daojing He", "Honghai Liu", "Min Zhang"], "title": "Neural Parameter Search for Slimmer Fine-Tuned Models and Better Transfer", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by ACL2025 Main", "summary": "Foundation models and their checkpoints have significantly advanced deep\nlearning, boosting performance across various applications. However, fine-tuned\nmodels often struggle outside their specific domains and exhibit considerable\nredundancy. Recent studies suggest that combining a pruned fine-tuned model\nwith the original pre-trained model can mitigate forgetting, reduce\ninterference when merging model parameters across tasks, and improve\ncompression efficiency. In this context, developing an effective pruning\nstrategy for fine-tuned models is crucial. Leveraging the advantages of the\ntask vector mechanism, we preprocess fine-tuned models by calculating the\ndifferences between them and the original model. Recognizing that different\ntask vector subspaces contribute variably to model performance, we introduce a\nnovel method called Neural Parameter Search (NPS-Pruning) for slimming down\nfine-tuned models. This method enhances pruning efficiency by searching through\nneural parameters of task vectors within low-rank subspaces. Our method has\nthree key applications: enhancing knowledge transfer through pairwise model\ninterpolation, facilitating effective knowledge fusion via model merging, and\nenabling the deployment of compressed models that retain near-original\nperformance while significantly reducing storage costs. Extensive experiments\nacross vision, NLP, and multi-modal benchmarks demonstrate the effectiveness\nand robustness of our approach, resulting in substantial performance gains. The\ncode is publicly available at: https://github.com/duguodong7/NPS-Pruning."}
{"id": "2505.20051", "pdf": "https://arxiv.org/pdf/2505.20051", "abs": "https://arxiv.org/abs/2505.20051", "authors": ["Gianmarco Genalti", "Sujay Bhatt", "Nicola Gatti", "Alberto Maria Metelli"], "title": "Catoni-Style Change Point Detection for Regret Minimization in Non-Stationary Heavy-Tailed Bandits", "categories": ["cs.LG"], "comment": null, "summary": "Regret minimization in stochastic non-stationary bandits gained popularity\nover the last decade, as it can model a broad class of real-world problems,\nfrom advertising to recommendation systems. Existing literature relies on\nvarious assumptions about the reward-generating process, such as Bernoulli or\nsubgaussian rewards. However, in settings such as finance and\ntelecommunications, heavy-tailed distributions naturally arise. In this work,\nwe tackle the heavy-tailed piecewise-stationary bandit problem. Heavy-tailed\nbandits, introduced by Bubeck et al., 2013, operate on the minimal assumption\nthat the finite absolute centered moments of maximum order $1+\\epsilon$ are\nuniformly bounded by a constant $v<+\\infty$, for some $\\epsilon \\in (0,1]$. We\nfocus on the most popular non-stationary bandit setting, i.e., the\npiecewise-stationary setting, in which the mean of reward-generating\ndistributions may change at unknown time steps. We provide a novel Catoni-style\nchange-point detection strategy tailored for heavy-tailed distributions that\nrelies on recent advancements in the theory of sequential estimation, which is\nof independent interest. We introduce Robust-CPD-UCB, which combines this\nchange-point detection strategy with optimistic algorithms for bandits,\nproviding its regret upper bound and an impossibility result on the minimum\nattainable regret for any policy. Finally, we validate our approach through\nnumerical experiments on synthetic and real-world datasets."}
{"id": "2505.18350", "pdf": "https://arxiv.org/pdf/2505.18350", "abs": "https://arxiv.org/abs/2505.18350", "authors": ["Waleed Reda", "Abhinav Jangda", "Krishna Chintalapudi"], "title": "Task Specific Pruning with LLM-Sieve: How Many Parameters Does Your Task Really Need?", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7"], "comment": null, "summary": "As Large Language Models (LLMs) are increasingly being adopted for narrow\ntasks - such as medical question answering or sentiment analysis - and deployed\nin resource-constrained settings, a key question arises: how many parameters\ndoes a task actually need? In this work, we present LLM-Sieve, the first\ncomprehensive framework for task-specific pruning of LLMs that achieves 20-75%\nparameter reduction with only 1-5% accuracy degradation across diverse domains.\nUnlike prior methods that apply uniform pruning or rely on low-rank\napproximations of weight matrices or inputs in isolation, LLM-Sieve (i) learns\ntask-aware joint projections to better approximate output behavior, and (ii)\nemploys a Genetic Algorithm to discover differentiated pruning levels for each\nmatrix. LLM-Sieve is fully compatible with LoRA fine-tuning and quantization,\nand uniquely demonstrates strong generalization across datasets within the same\ntask domain. Together, these results establish a practical and robust mechanism\nto generate smaller performant task-specific models."}
{"id": "2505.18719", "pdf": "https://arxiv.org/pdf/2505.18719", "abs": "https://arxiv.org/abs/2505.18719", "authors": ["Guanxing Lu", "Wenkai Guo", "Chubin Zhang", "Yuheng Zhou", "Haonan Jiang", "Zifeng Gao", "Yansong Tang", "Ziwei Wang"], "title": "VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Recent high-capacity vision-language-action (VLA) models have demonstrated\nimpressive performance on a range of robotic manipulation tasks by imitating\nhuman demonstrations. However, exploiting offline data with limited visited\nstates will cause execution failure in out-of-distribution scenarios.\nIntuitively, an exploration-based method that improves on online collected data\nat test time could address this limitation. We present VLA-RL, an algorithmic\nand systematic framework that leverages online reinforcement learning (RL) to\nimprove pretrained auto-regressive VLAs in downstream tasks. Within a unified\nperspective, we first introduce a trajectory-level RL formulation for\nauto-regressive VLA training, which models general robotic manipulation\ntrajectory as multi-modal multi-turn conversation. To address the challenge of\nsparse rewards, we fine-tune a pretrained vision-language model as a robotic\nprocess reward model, which is trained on pseudo reward labels annotated on\nautomatically extracted task segments. To scale up, we identify several\nimplementation findings that improve the stability and efficiency including\ncurriculum selection strategy, GPU-balanced vectorized environments, batch\ndecoding, and critic warmup. VLA-RL enables OpenVLA-7B to surpass the strongest\nfinetuned baseline by 4.5% on 40 challenging robotic manipulation tasks in\nLIBERO, and even matches the performance of advanced commercial models such as\n$\\pi_0$-FAST. Notably, we observe that VLA-RL benefits from increased test-time\noptimization, indicating an early spark of inference scaling laws in robotics."}
{"id": "2505.20052", "pdf": "https://arxiv.org/pdf/2505.20052", "abs": "https://arxiv.org/abs/2505.20052", "authors": ["Hazem Alsamkary", "Mohamed Elshaffei", "Mohamed Elkerdawy", "Ahmed Elnaggar"], "title": "Ankh3: Multi-Task Pretraining with Sequence Denoising and Completion Enhances Protein Representations", "categories": ["cs.LG", "q-bio.QM"], "comment": "8 pages, 0 figures", "summary": "Protein language models (PLMs) have emerged as powerful tools to detect\ncomplex patterns of protein sequences. However, the capability of PLMs to fully\ncapture information on protein sequences might be limited by focusing on single\npre-training tasks. Although adding data modalities or supervised objectives\ncan improve the performance of PLMs, pre-training often remains focused on\ndenoising corrupted sequences. To push the boundaries of PLMs, our research\ninvestigated a multi-task pre-training strategy. We developed Ankh3, a model\njointly optimized on two objectives: masked language modeling with multiple\nmasking probabilities and protein sequence completion relying only on protein\nsequences as input. This multi-task pre-training demonstrated that PLMs can\nlearn richer and more generalizable representations solely from protein\nsequences. The results demonstrated improved performance in downstream tasks,\nsuch as secondary structure prediction, fluorescence, GB1 fitness, and contact\nprediction. The integration of multiple tasks gave the model a more\ncomprehensive understanding of protein properties, leading to more robust and\naccurate predictions."}
{"id": "2505.18366", "pdf": "https://arxiv.org/pdf/2505.18366", "abs": "https://arxiv.org/abs/2505.18366", "authors": ["Hansa Meghwani", "Amit Agarwal", "Priyaranjan Pattnayak", "Hitesh Laxmichand Patel", "Srikant Panda"], "title": "Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "H.3.3; I.2.6; I.2.7"], "comment": "Accepted to ACL 2025", "summary": "Enterprise search systems often struggle to retrieve accurate,\ndomain-specific information due to semantic mismatches and overlapping\nterminologies. These issues can degrade the performance of downstream\napplications such as knowledge management, customer support, and\nretrieval-augmented generation agents. To address this challenge, we propose a\nscalable hard-negative mining framework tailored specifically for\ndomain-specific enterprise data. Our approach dynamically selects semantically\nchallenging but contextually irrelevant documents to enhance deployed\nre-ranking models.\n  Our method integrates diverse embedding models, performs dimensionality\nreduction, and uniquely selects hard negatives, ensuring computational\nefficiency and semantic precision. Evaluation on our proprietary enterprise\ncorpus (cloud services domain) demonstrates substantial improvements of 15\\% in\nMRR@3 and 19\\% in MRR@10 compared to state-of-the-art baselines and other\nnegative sampling techniques. Further validation on public domain-specific\ndatasets (FiQA, Climate Fever, TechQA) confirms our method's generalizability\nand readiness for real-world applications."}
{"id": "2505.18720", "pdf": "https://arxiv.org/pdf/2505.18720", "abs": "https://arxiv.org/abs/2505.18720", "authors": ["Meng Li", "Guangda Huzhang", "Haibo Zhang", "Xiting Wang", "Anxiang Zeng"], "title": "Optimal Transport-Based Token Weighting scheme for Enhanced Preference Optimization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "24 pages, 11 figures. Accepted by ACL 2025 (main)", "summary": "Direct Preference Optimization (DPO) has emerged as a promising framework for\naligning Large Language Models (LLMs) with human preferences by directly\noptimizing the log-likelihood difference between chosen and rejected responses.\nHowever, existing methods assign equal importance to all tokens in the\nresponse, while humans focus on more meaningful parts. This leads to suboptimal\npreference optimization, as irrelevant or noisy tokens disproportionately\ninfluence DPO loss. To address this limitation, we propose \\textbf{O}ptimal\n\\textbf{T}ransport-based token weighting scheme for enhancing direct\n\\textbf{P}reference \\textbf{O}ptimization (OTPO). By emphasizing semantically\nmeaningful token pairs and de-emphasizing less relevant ones, our method\nintroduces a context-aware token weighting scheme that yields a more\ncontrastive reward difference estimate. This adaptive weighting enhances reward\nstability, improves interpretability, and ensures that preference optimization\nfocuses on meaningful differences between responses. Extensive experiments have\nvalidated OTPO's effectiveness in improving instruction-following ability\nacross various settings\\footnote{Code is available at\nhttps://github.com/Mimasss2/OTPO.}."}
{"id": "2505.20063", "pdf": "https://arxiv.org/pdf/2505.20063", "abs": "https://arxiv.org/abs/2505.20063", "authors": ["Dana Arad", "Aaron Mueller", "Yonatan Belinkov"], "title": "SAEs Are Good for Steering -- If You Select the Right Features", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Sparse Autoencoders (SAEs) have been proposed as an unsupervised approach to\nlearn a decomposition of a model's latent space. This enables useful\napplications such as steering - influencing the output of a model towards a\ndesired concept - without requiring labeled data. Current methods identify SAE\nfeatures to steer by analyzing the input tokens that activate them. However,\nrecent work has highlighted that activations alone do not fully describe the\neffect of a feature on the model's output. In this work, we draw a distinction\nbetween two types of features: input features, which mainly capture patterns in\nthe model's input, and output features, which have a human-understandable\neffect on the model's output. We propose input and output scores to\ncharacterize and locate these types of features, and show that high values for\nboth scores rarely co-occur in the same features. These findings have practical\nimplications: after filtering out features with low output scores, we obtain\n2-3x improvements when steering with SAEs, making them competitive with\nsupervised methods."}
{"id": "2505.18413", "pdf": "https://arxiv.org/pdf/2505.18413", "abs": "https://arxiv.org/abs/2505.18413", "authors": ["Toshiaki Koike-Akino", "Xiangyu Chen", "Jing Liu", "Ye Wang", "Pu", "Wang", "Matthew Brand"], "title": "LatentLLM: Attention-Aware Joint Tensor Compression", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "37 pages, 16 figures", "summary": "Modern foundation models such as large language models (LLMs) and large\nmulti-modal models (LMMs) require a massive amount of computational and memory\nresources. We propose a new framework to convert such LLMs/LMMs into a\nreduced-dimension latent structure. Our method extends a local activation-aware\ntensor decomposition to a global attention-aware joint tensor de-composition.\nOur framework can significantly improve the model accuracy over the existing\nmodel compression methods when reducing the latent dimension to realize\ncomputationally/memory-efficient LLMs/LLMs. We show the benefit on several\nbenchmark including multi-modal reasoning tasks."}
{"id": "2505.18722", "pdf": "https://arxiv.org/pdf/2505.18722", "abs": "https://arxiv.org/abs/2505.18722", "authors": ["Terry Yi Zhong", "Esther Janse", "Cristian Tejedor-Garcia", "Louis ten Bosch", "Martha Larson"], "title": "Evaluating the Usefulness of Non-Diagnostic Speech Data for Developing Parkinson's Disease Classifiers", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "Accepted for Interspeech 2025 (Camera-Ready)", "summary": "Speech-based Parkinson's disease (PD) detection has gained attention for its\nautomated, cost-effective, and non-intrusive nature. As research studies\nusually rely on data from diagnostic-oriented speech tasks, this work explores\nthe feasibility of diagnosing PD on the basis of speech data not originally\nintended for diagnostic purposes, using the Turn-Taking (TT) dataset. Our\nfindings indicate that TT can be as useful as diagnostic-oriented PD datasets\nlike PC-GITA. We also investigate which specific dataset characteristics impact\nPD classification performance. The results show that concatenating audio\nrecordings and balancing participants' gender and status distributions can be\nbeneficial. Cross-dataset evaluation reveals that models trained on PC-GITA\ngeneralize poorly to TT, whereas models trained on TT perform better on\nPC-GITA. Furthermore, we provide insights into the high variability across\nfolds, which is mainly due to large differences in individual speaker\nperformance."}
{"id": "2505.20065", "pdf": "https://arxiv.org/pdf/2505.20065", "abs": "https://arxiv.org/abs/2505.20065", "authors": ["Geon-Hyeong Kim", "Youngsoo Jang", "Yu Jin Kim", "Byoungjip Kim", "Honglak Lee", "Kyunghoon Bae", "Moontae Lee"], "title": "SafeDPO: A Simple Approach to Direct Preference Optimization with Enhanced Safety", "categories": ["cs.LG", "cs.AI"], "comment": "34 pages", "summary": "As Large Language Models (LLMs) continue to advance and find applications\nacross a growing number of fields, ensuring the safety of LLMs has become\nincreasingly critical. To address safety concerns, recent studies have proposed\nintegrating safety constraints into Reinforcement Learning from Human Feedback\n(RLHF). However, these approaches tend to be complex, as they encompass\ncomplicated procedures in RLHF along with additional steps required by the\nsafety constraints. Inspired by Direct Preference Optimization (DPO), we\nintroduce a new algorithm called SafeDPO, which is designed to directly\noptimize the safety alignment objective in a single stage of policy learning,\nwithout requiring relaxation. SafeDPO introduces only one additional\nhyperparameter to further enhance safety and requires only minor modifications\nto standard DPO. As a result, it eliminates the need to fit separate reward and\ncost models or to sample from the language model during fine-tuning, while\nstill enhancing the safety of LLMs. Finally, we demonstrate that SafeDPO\nachieves competitive performance compared to state-of-the-art safety alignment\nalgorithms, both in terms of aligning with human preferences and improving\nsafety."}
{"id": "2505.18451", "pdf": "https://arxiv.org/pdf/2505.18451", "abs": "https://arxiv.org/abs/2505.18451", "authors": ["Toshiaki Koike-Akino", "Jing Liu", "Ye Wang"], "title": "$$-MoE: Test-Time Pruning as Micro-Grained Mixture-of-Experts", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "10 pages, 4 figures", "summary": "To tackle the huge computational demand of large foundation models,\nactivation-aware compression techniques without retraining have been\nintroduced. However, since these rely on calibration data, domain shift may\narise for unknown downstream tasks. With a computationally efficient\ncalibration, activation-aware pruning can be executed for every prompt\nadaptively, yet achieving reduced complexity at inference. We formulate it as a\nmixture of micro-experts, called $\\mu$-MoE. Several experiments demonstrate\nthat $\\mu$-MoE can dynamically adapt to task/prompt-dependent structured\nsparsity on the fly."}
{"id": "2505.18724", "pdf": "https://arxiv.org/pdf/2505.18724", "abs": "https://arxiv.org/abs/2505.18724", "authors": ["Junyu Chen", "Junzhuo Li", "Zhen Peng", "Wenjie Wang", "Yuxiang Ren", "Long Shi", "Xuming Hu"], "title": "LoTA-QAF: Lossless Ternary Adaptation for Quantization-Aware Fine-Tuning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Quantization and fine-tuning are crucial for deploying large language models\n(LLMs) on resource-constrained edge devices. However, fine-tuning quantized\nmodels presents significant challenges, primarily stemming from: First, the\nmismatch in data types between the low-precision quantized weights (e.g.,\n4-bit) and the high-precision adaptation weights (e.g., 16-bit). This mismatch\nlimits the computational efficiency advantage offered by quantized weights\nduring inference. Second, potential accuracy degradation when merging these\nhigh-precision adaptation weights into the low-precision quantized weights, as\nthe adaptation weights often necessitate approximation or truncation. Third, as\nfar as we know, no existing methods support the lossless merging of adaptation\nwhile adjusting all quantized weights. To address these challenges, we\nintroduce lossless ternary adaptation for quantization-aware fine-tuning\n(LoTA-QAF). This is a novel fine-tuning method specifically designed for\nquantized LLMs, enabling the lossless merging of ternary adaptation weights\ninto quantized weights and the adjustment of all quantized weights. LoTA-QAF\noperates through a combination of: i) A custom-designed ternary adaptation (TA)\nthat aligns ternary weights with the quantization grid and uses these ternary\nweights to adjust quantized weights. ii) A TA-based mechanism that enables the\nlossless merging of adaptation weights. iii) Ternary signed gradient descent\n(t-SignSGD) for updating the TA weights. We apply LoTA-QAF to Llama-3.1/3.3 and\nQwen-2.5 model families and validate its effectiveness on several downstream\ntasks. On the MMLU benchmark, our method effectively recovers performance for\nquantized models, surpassing 16-bit LoRA by up to 5.14\\%. For task-specific\nfine-tuning, 16-bit LoRA achieves superior results, but LoTA-QAF still\noutperforms other methods."}
{"id": "2505.20074", "pdf": "https://arxiv.org/pdf/2505.20074", "abs": "https://arxiv.org/abs/2505.20074", "authors": ["Jinyan Wang", "Liu Yang", "Yuecen Wei", "Jiaxuan Si", "Chenhao Guo", "Qingyun Sun", "Xianxian Li", "Xingcheng Fu"], "title": "An Out-Of-Distribution Membership Inference Attack Approach for Cross-Domain Graph Attacks", "categories": ["cs.LG"], "comment": "Accepted by the 34th International Joint Conference on Artificial\n  Intelligence (IJCAI-25)", "summary": "Graph Neural Network-based methods face privacy leakage risks due to the\nintroduction of topological structures about the targets, which allows\nattackers to bypass the target's prior knowledge of the sensitive attributes\nand realize membership inference attacks (MIA) by observing and analyzing the\ntopology distribution. As privacy concerns grow, the assumption of MIA, which\npresumes that attackers can obtain an auxiliary dataset with the same\ndistribution, is increasingly deviating from reality. In this paper, we\ncategorize the distribution diversity issue in real-world MIA scenarios as an\nOut-Of-Distribution (OOD) problem, and propose a novel Graph OOD Membership\nInference Attack (GOOD-MIA) to achieve cross-domain graph attacks.\nSpecifically, we construct shadow subgraphs with distributions from different\ndomains to model the diversity of real-world data. We then explore the stable\nnode representations that remain unchanged under external influences and\nconsider eliminating redundant information from confounding environments and\nextracting task-relevant key information to more clearly distinguish between\nthe characteristics of training data and unseen data. This OOD-based design\nmakes cross-domain graph attacks possible. Finally, we perform risk\nextrapolation to optimize the attack's domain adaptability during attack\ninference to generalize the attack to other domains. Experimental results\ndemonstrate that GOOD-MIA achieves superior attack performance in datasets\ndesigned for multiple domains."}
{"id": "2505.18458", "pdf": "https://arxiv.org/pdf/2505.18458", "abs": "https://arxiv.org/abs/2505.18458", "authors": ["Xuanhe Zhou", "Junxuan He", "Wei Zhou", "Haodong Chen", "Zirui Tang", "Haoyu Zhao", "Xin Tong", "Guoliang Li", "Youmin Chen", "Jun Zhou", "Zhaojun Sun", "Binyuan Hui", "Shuo Wang", "Conghui He", "Zhiyuan Liu", "Jingren Zhou", "Fan Wu"], "title": "A Survey of LLM $\\times$ DATA", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": "Please refer to the paper list at:\n  https://github.com/weAIDB/awesome-data-llm", "summary": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration."}
{"id": "2505.18728", "pdf": "https://arxiv.org/pdf/2505.18728", "abs": "https://arxiv.org/abs/2505.18728", "authors": ["Andrea Ceni", "Alessio Gravina", "Claudio Gallicchio", "Davide Bacciu", "Carola-Bibiane Schonlieb", "Moshe Eliasof"], "title": "Message-Passing State-Space Models: Improving Graph Learning with Modern Sequence Modeling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The recent success of State-Space Models (SSMs) in sequence modeling has\nmotivated their adaptation to graph learning, giving rise to Graph State-Space\nModels (GSSMs). However, existing GSSMs operate by applying SSM modules to\nsequences extracted from graphs, often compromising core properties such as\npermutation equivariance, message-passing compatibility, and computational\nefficiency. In this paper, we introduce a new perspective by embedding the key\nprinciples of modern SSM computation directly into the Message-Passing Neural\nNetwork framework, resulting in a unified methodology for both static and\ntemporal graphs. Our approach, MP-SSM, enables efficient,\npermutation-equivariant, and long-range information propagation while\npreserving the architectural simplicity of message passing. Crucially, MP-SSM\nenables an exact sensitivity analysis, which we use to theoretically\ncharacterize information flow and evaluate issues like vanishing gradients and\nover-squashing in the deep regime. Furthermore, our design choices allow for a\nhighly optimized parallel implementation akin to modern SSMs. We validate\nMP-SSM across a wide range of tasks, including node classification, graph\nproperty prediction, long-range benchmarks, and spatiotemporal forecasting,\ndemonstrating both its versatility and strong empirical performance."}
{"id": "2505.20076", "pdf": "https://arxiv.org/pdf/2505.20076", "abs": "https://arxiv.org/abs/2505.20076", "authors": ["Florian Eichin", "Yupei Du", "Philipp Mondorf", "Barbara Plank", "Michael A. Hedderich"], "title": "Grokking ExPLAIND: Unifying Model, Data, and Training Attribution to Study Model Behavior", "categories": ["cs.LG"], "comment": "9 pages main paper, 23 pages in total, code at\n  https://github.com/mainlp/explaind", "summary": "Post-hoc interpretability methods typically attribute a model's behavior to\nits components, data, or training trajectory in isolation. This leads to\nexplanations that lack a unified view and may miss key interactions. While\ncombining existing methods or applying them at different training stages offers\nbroader insights, these approaches usually lack theoretical support. In this\nwork, we present ExPLAIND, a unified framework that integrates all three\nperspectives. First, we generalize recent work on gradient path kernels, which\nreformulate models trained by gradient descent as a kernel machine, to more\nrealistic training settings. Empirically, we find that both a CNN and a\nTransformer model are replicated accurately by this reformulation. Second, we\nderive novel parameter- and step-wise influence scores from the kernel feature\nmaps. We show their effectiveness in parameter pruning that is comparable to\nexisting methods, reinforcing their value for model component attribution.\nFinally, jointly interpreting model components and data over the training\nprocess, we leverage ExPLAIND to analyze a Transformer that exhibits Grokking.\nAmong other things, our findings support previously proposed stages of\nGrokking, while refining the final phase as one of alignment of input\nembeddings and final layers around a representation pipeline learned after the\nmemorization phase. Overall, ExPLAIND provides a theoretically grounded,\nunified framework to interpret model behavior and training dynamics."}
{"id": "2505.18464", "pdf": "https://arxiv.org/pdf/2505.18464", "abs": "https://arxiv.org/abs/2505.18464", "authors": ["Ugur Kursuncu", "Trilok Padhi", "Gaurav Sinha", "Abdulkadir Erol", "Jaya Krishna Mandivarapu", "Christopher R. Larrison"], "title": "From Reddit to Generative AI: Evaluating Large Language Models for Anxiety Support Fine-tuned on Social Media Data", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "The growing demand for accessible mental health support, compounded by\nworkforce shortages and logistical barriers, has led to increased interest in\nutilizing Large Language Models (LLMs) for scalable and real-time assistance.\nHowever, their use in sensitive domains such as anxiety support remains\nunderexamined. This study presents a systematic evaluation of LLMs (GPT and\nLlama) for their potential utility in anxiety support by using real\nuser-generated posts from the r/Anxiety subreddit for both prompting and\nfine-tuning. Our approach utilizes a mixed-method evaluation framework\nincorporating three main categories of criteria: (i) linguistic quality, (ii)\nsafety and trustworthiness, and (iii) supportiveness. Results show that\nfine-tuning LLMs with naturalistic anxiety-related data enhanced linguistic\nquality but increased toxicity and bias, and diminished emotional\nresponsiveness. While LLMs exhibited limited empathy, GPT was evaluated as more\nsupportive overall. Our findings highlight the risks of fine-tuning LLMs on\nunprocessed social media content without mitigation strategies."}
{"id": "2505.18741", "pdf": "https://arxiv.org/pdf/2505.18741", "abs": "https://arxiv.org/abs/2505.18741", "authors": ["Han Li", "Hu Han", "S. Kevin Zhou"], "title": "MoMBS: Mixed-order minibatch sampling enhances model training from diverse-quality images", "categories": ["cs.CV", "cs.AI"], "comment": "16 pages,8 figures", "summary": "Natural images exhibit label diversity (clean vs. noisy) in noisy-labeled\nimage classification and prevalence diversity (abundant vs. sparse) in\nlong-tailed image classification. Similarly, medical images in universal lesion\ndetection (ULD) exhibit substantial variations in image quality, encompassing\nattributes such as clarity and label correctness. How to effectively leverage\ntraining images with diverse qualities becomes a problem in learning deep\nmodels. Conventional training mechanisms, such as self-paced curriculum\nlearning (SCL) and online hard example mining (OHEM), relieve this problem by\nreweighting images with high loss values. Despite their success, these methods\nstill confront two challenges: (i) the loss-based measure of sample hardness is\nimprecise, preventing optimum handling of different cases, and (ii) there\nexists under-utilization in SCL or over-utilization OHEM with the identified\nhard samples. To address these issues, this paper revisits the minibatch\nsampling (MBS), a technique widely used in deep network training but largely\nunexplored concerning the handling of diverse-quality training samples. We\ndiscover that the samples within a minibatch influence each other during\ntraining; thus, we propose a novel Mixed-order Minibatch Sampling (MoMBS)\nmethod to optimize the use of training samples with diverse qualities. MoMBS\nintroduces a measure that takes both loss and uncertainty into account to\nsurpass a sole reliance on loss and allows for a more refined categorization of\nhigh-loss samples by distinguishing them as either poorly labeled and under\nrepresented or well represented and overfitted. We prioritize under represented\nsamples as the main gradient contributors in a minibatch and keep them from the\nnegative influences of poorly labeled or overfitted samples with a mixed-order\nminibatch sampling design."}
{"id": "2505.20095", "pdf": "https://arxiv.org/pdf/2505.20095", "abs": "https://arxiv.org/abs/2505.20095", "authors": ["Chenxiang Zhang", "Jun Pang", "Sjouke Mauw"], "title": "Spurious Privacy Leakage in Neural Networks", "categories": ["cs.LG"], "comment": "preprint", "summary": "Neural networks are vulnerable to privacy attacks aimed at stealing sensitive\ndata. The risks can be amplified in a real-world scenario, particularly when\nmodels are trained on limited and biased data. In this work, we investigate the\nimpact of spurious correlation bias on privacy vulnerability. We introduce\n\\emph{spurious privacy leakage}, a phenomenon where spurious groups are\nsignificantly more vulnerable to privacy attacks than non-spurious groups. We\nfurther show that group privacy disparity increases in tasks with simpler\nobjectives (e.g. fewer classes) due to the persistence of spurious features.\nSurprisingly, we find that reducing spurious correlation using spurious robust\nmethods does not mitigate spurious privacy leakage. This leads us to introduce\na perspective on privacy disparity based on memorization, where mitigating\nspurious correlation does not mitigate the memorization of spurious data, and\ntherefore, neither the privacy level. Lastly, we compare the privacy of\ndifferent model architectures trained with spurious data, demonstrating that,\ncontrary to prior works, architectural choice can affect privacy outcomes."}
{"id": "2505.18467", "pdf": "https://arxiv.org/pdf/2505.18467", "abs": "https://arxiv.org/abs/2505.18467", "authors": ["Unggi Lee", "Jaeyong Lee", "Jiyeong Bae", "Yeil Jeong", "Junbo Koh", "Gyeonggeon Lee", "Gunho Lee", "Taekyung Ahn", "Hyeoncheol Kim"], "title": "Pedagogy-R1: Pedagogically-Aligned Reasoning Model with Balanced Educational Benchmark", "categories": ["cs.AI", "cs.CL"], "comment": "15 pages, 5 figures, 4 tables", "summary": "Recent advances in large reasoning models (LRMs) show strong performance in\nstructured domains such as mathematics and programming; however, they often\nlack pedagogical coherence and realistic teaching behaviors. To bridge this\ngap, we introduce Pedagogy-R1, a framework that adapts LRMs for classroom use\nthrough three innovations: (1) a distillation-based pipeline that filters and\nrefines model outputs for instruction-tuning, (2) the Well-balanced Educational\nBenchmark (WBEB), which evaluates performance across subject knowledge,\npedagogical knowledge, tracing, essay scoring, and teacher decision-making, and\n(3) a Chain-of-Pedagogy (CoP) prompting strategy for generating and eliciting\nteacher-style reasoning. Our mixed-method evaluation combines quantitative\nmetrics with qualitative analysis, providing the first systematic assessment of\nLRMs' pedagogical strengths and limitations."}
{"id": "2505.18747", "pdf": "https://arxiv.org/pdf/2505.18747", "abs": "https://arxiv.org/abs/2505.18747", "authors": ["Xiaolu Chen", "Chenghao Huang", "Yanru Zhang", "Hao Wang"], "title": "Season-Independent PV Disaggregation Using Multi-Scale Net Load Temporal Feature Extraction and Weather Factor Fusion", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "2024 IEEE 8th Conference on Energy Internet and Energy System\n  Integration (EI2)", "summary": "With the advancement of energy Internet and energy system integration, the\nincreasing adoption of distributed photovoltaic (PV) systems presents new\nchallenges on smart monitoring and measurement for utility companies,\nparticularly in separating PV generation from net electricity load. Existing\nmethods struggle with feature extraction from net load and capturing the\nrelevance between weather factors. This paper proposes a PV disaggregation\nmethod that integrates Hierarchical Interpolation (HI) and multi-head\nself-attention mechanisms. By using HI to extract net load features and\nmulti-head self-attention to capture the complex dependencies between weather\nfactors, the method achieves precise PV generation predictions. Simulation\nexperiments demonstrate the effectiveness of the proposed method in real-world\ndata, supporting improved monitoring and management of distributed energy\nsystems."}
{"id": "2505.20098", "pdf": "https://arxiv.org/pdf/2505.20098", "abs": "https://arxiv.org/abs/2505.20098", "authors": ["Xiaowen Ling", "Zhiqiang Li", "Yanbin Wang", "Zhuhong You"], "title": "Transformer in Protein: A Survey", "categories": ["cs.LG", "cs.CR", "q-bio.QM"], "comment": null, "summary": "As protein informatics advances rapidly, the demand for enhanced predictive\naccuracy, structural analysis, and functional understanding has intensified.\nTransformer models, as powerful deep learning architectures, have demonstrated\nunprecedented potential in addressing diverse challenges across protein\nresearch. However, a comprehensive review of Transformer applications in this\nfield remains lacking. This paper bridges this gap by surveying over 100\nstudies, offering an in-depth analysis of practical implementations and\nresearch progress of Transformers in protein-related tasks. Our review\nsystematically covers critical domains, including protein structure prediction,\nfunction prediction, protein-protein interaction analysis, functional\nannotation, and drug discovery/target identification. To contextualize these\nadvancements across various protein domains, we adopt a domain-oriented\nclassification system. We first introduce foundational concepts: the\nTransformer architecture and attention mechanisms, categorize Transformer\nvariants tailored for protein science, and summarize essential protein\nknowledge. For each research domain, we outline its objectives and background,\ncritically evaluate prior methods and their limitations, and highlight\ntransformative contributions enabled by Transformer models. We also curate and\nsummarize pivotal datasets and open-source code resources to facilitate\nreproducibility and benchmarking. Finally, we discuss persistent challenges in\napplying Transformers to protein informatics and propose future research\ndirections. This review aims to provide a consolidated foundation for the\nsynergistic integration of Transformer and protein informatics, fostering\nfurther innovation and expanded applications in the field."}
{"id": "2505.18488", "pdf": "https://arxiv.org/pdf/2505.18488", "abs": "https://arxiv.org/abs/2505.18488", "authors": ["Yanxiang Zhang", "Zheng Xu", "Shanshan Wu", "Yuanbo Zhang", "Daniel Ramage"], "title": "Synthesizing and Adapting Error Correction Data for Mobile Large Language Model Applications", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ACL Industry", "summary": "Error correction is an important capability when applying large language\nmodels (LLMs) to facilitate user typing on mobile devices. In this paper, we\nuse LLMs to synthesize a high-quality dataset of error correction pairs to\nevaluate and improve LLMs for mobile applications. We first prompt LLMs with\nerror correction domain knowledge to build a scalable and reliable addition to\nthe existing data synthesis pipeline. We then adapt the synthetic data\ndistribution to match the mobile application domain by reweighting the samples.\nThe reweighting model is learnt by predicting (a handful of) live A/B test\nmetrics when deploying LLMs in production, given the LLM performance on offline\nevaluation data and scores from a small privacy-preserving on-device language\nmodel. Finally, we present best practices for mixing our synthetic data with\nother data sources to improve model performance on error correction in both\noffline evaluation and production live A/B testing."}
{"id": "2505.18750", "pdf": "https://arxiv.org/pdf/2505.18750", "abs": "https://arxiv.org/abs/2505.18750", "authors": ["Jiarong Fan", "Chenghao Huang", "Hao Wang"], "title": "Agent-Based Decentralized Energy Management of EV Charging Station with Solar Photovoltaics via Multi-Agent Reinforcement Learning", "categories": ["eess.SY", "cs.AI", "cs.SY", "math.OC"], "comment": "2024 IEEE International Smart Cities Conference (ISC2)", "summary": "In the pursuit of energy net zero within smart cities, transportation\nelectrification plays a pivotal role. The adoption of Electric Vehicles (EVs)\nkeeps increasing, making energy management of EV charging stations critically\nimportant. While previous studies have managed to reduce energy cost of EV\ncharging while maintaining grid stability, they often overlook the robustness\nof EV charging management against uncertainties of various forms, such as\nvarying charging behaviors and possible faults in faults in some chargers. To\naddress the gap, a novel Multi-Agent Reinforcement Learning (MARL) approach is\nproposed treating each charger to be an agent and coordinate all the agents in\nthe EV charging station with solar photovoltaics in a more realistic scenario,\nwhere system faults may occur. A Long Short-Term Memory (LSTM) network is\nincorporated in the MARL algorithm to extract temporal features from\ntime-series. Additionally, a dense reward mechanism is designed for training\nthe agents in the MARL algorithm to improve EV charging experience. Through\nvalidation on a real-world dataset, we show that our approach is robust against\nsystem uncertainties and faults and also effective in minimizing EV charging\ncosts and maximizing charging service satisfaction."}
{"id": "2505.20107", "pdf": "https://arxiv.org/pdf/2505.20107", "abs": "https://arxiv.org/abs/2505.20107", "authors": ["Ziyi Zhang", "Li Shen", "Deheng Ye", "Yong Luo", "Huangxuan Zhao", "Lefei Zhang"], "title": "Refining Few-Step Text-to-Multiview Diffusion via Reinforcement Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Text-to-multiview (T2MV) generation, which produces coherent multiview images\nfrom a single text prompt, remains computationally intensive, while accelerated\nT2MV methods using few-step diffusion models often sacrifice image fidelity and\nview consistency. To address this, we propose a novel reinforcement learning\n(RL) finetuning framework tailored for few-step T2MV diffusion models to\njointly optimize per-view fidelity and cross-view consistency. Specifically, we\nfirst reformulate T2MV denoising across all views as a single unified Markov\ndecision process, enabling multiview-aware policy optimization driven by a\njoint-view reward objective. Next, we introduce ZMV-Sampling, a test-time T2MV\nsampling technique that adds an inversion-denoising pass to reinforce both\nviewpoint and text conditioning, resulting in improved T2MV generation at the\ncost of inference time. To internalize its performance gains into the base\nsampling policy, we develop MV-ZigAL, a novel policy optimization strategy that\nuses reward advantages of ZMV-Sampling over standard sampling as learning\nsignals for policy updates. Finally, noting that the joint-view reward\nobjective under-optimizes per-view fidelity but naively optimizing single-view\nmetrics neglects cross-view alignment, we reframe RL finetuning for T2MV\ndiffusion models as a constrained optimization problem that maximizes per-view\nfidelity subject to an explicit joint-view constraint, thereby enabling more\nefficient and balanced policy updates. By integrating this constrained\noptimization paradigm with MV-ZigAL, we establish our complete RL finetuning\nframework, referred to as MVC-ZigAL, which effectively refines the few-step\nT2MV diffusion baseline in both fidelity and consistency while preserving its\nfew-step efficiency."}
{"id": "2505.18502", "pdf": "https://arxiv.org/pdf/2505.18502", "abs": "https://arxiv.org/abs/2505.18502", "authors": ["Guodong Du", "Xuanning Zhou", "Junlin Li", "Zhuo Li", "Zesheng Shi", "Wanyu Lin", "Ho-Kin Tang", "Xiucheng Li", "Fangming Liu", "Wenya Wang", "Min Zhang", "Jing Li"], "title": "Knowledge Grafting of Large Language Models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Cross-capability transfer is a key challenge in large language model (LLM)\nresearch, with applications in multi-task integration, model compression, and\ncontinual learning. Recent works like FuseLLM and FuseChat have demonstrated\nthe potential of transferring multiple model capabilities to lightweight\nmodels, enhancing adaptability and efficiency, which motivates our\ninvestigation into more efficient cross-capability transfer methods. However,\nexisting approaches primarily focus on small, homogeneous models, limiting\ntheir applicability. For large, heterogeneous models, knowledge distillation\nwith full-parameter fine-tuning often overlooks the student model's intrinsic\ncapacity and risks catastrophic forgetting, while PEFT methods struggle to\neffectively absorb knowledge from source LLMs. To address these issues, we\nintroduce GraftLLM, a novel method that stores source model capabilities in a\ntarget model with SkillPack format. This approach preserves general\ncapabilities, reduces parameter conflicts, and supports forget-free continual\nlearning and model fusion. We employ a module-aware adaptive compression\nstrategy to compress parameter updates, ensuring efficient storage while\nmaintaining task-specific knowledge. The resulting SkillPack serves as a\ncompact and transferable knowledge carrier, ideal for heterogeneous model\nfusion and continual learning. Experiments across various scenarios demonstrate\nthat GraftLLM outperforms existing techniques in knowledge transfer, knowledge\nfusion, and forget-free learning, providing a scalable and efficient solution\nfor cross-capability transfer. The code is publicly available at:\nhttps://github.com/duguodong7/GraftLLM."}
{"id": "2505.18755", "pdf": "https://arxiv.org/pdf/2505.18755", "abs": "https://arxiv.org/abs/2505.18755", "authors": ["Xiaolu Chen", "Chenghao Huang", "Yanru Zhang", "Hao Wang"], "title": "Smart Energy Guardian: A Hybrid Deep Learning Model for Detecting Fraudulent PV Generation", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": "2024 IEEE International Smart Cities Conference (ISC2)", "summary": "With the proliferation of smart grids, smart cities face growing challenges\ndue to cyber-attacks and sophisticated electricity theft behaviors,\nparticularly in residential photovoltaic (PV) generation systems. Traditional\nElectricity Theft Detection (ETD) methods often struggle to capture complex\ntemporal dependencies and integrating multi-source data, limiting their\neffectiveness. In this work, we propose an efficient ETD method that accurately\nidentifies fraudulent behaviors in residential PV generation, thus ensuring the\nsupply-demand balance in smart cities. Our hybrid deep learning model,\ncombining multi-scale Convolutional Neural Network (CNN), Long Short-Term\nMemory (LSTM), and Transformer, excels in capturing both short-term and\nlong-term temporal dependencies. Additionally, we introduce a data embedding\ntechnique that seamlessly integrates time-series data with discrete temperature\nvariables, enhancing detection robustness. Extensive simulation experiments\nusing real-world data validate the effectiveness of our approach, demonstrating\nsignificant improvements in the accuracy of detecting sophisticated energy\ntheft activities, thereby contributing to the stability and fairness of energy\nsystems in smart cities."}
{"id": "2505.20110", "pdf": "https://arxiv.org/pdf/2505.20110", "abs": "https://arxiv.org/abs/2505.20110", "authors": ["Ruishuo Chen", "Xun Wang", "Rui Hu", "Zhuoran Li", "Longbo Huang"], "title": "Proxy-Free GFlowNet", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Generative Flow Networks (GFlowNets) are a promising class of generative\nmodels designed to sample diverse, high-reward structures by modeling\ndistributions over compositional objects. In many real-world applications,\nobtaining the reward function for such objects is expensive, time-consuming, or\nrequires human input, making it necessary to train GFlowNets from historical\ndatasets. Most existing methods adopt a model-based approach, learning a proxy\nmodel from the dataset to approximate the reward function. However, this\nstrategy inherently ties the quality of the learned policy to the accuracy of\nthe proxy, introducing additional complexity and uncertainty into the training\nprocess. To overcome these limitations, we propose \\textbf{Trajectory-Distilled\nGFlowNet (TD-GFN)}, a \\emph{proxy-free} training framework that eliminates the\nneed for out-of-dataset reward queries. Our method is motivated by the key\nobservation that different edges in the associated directed acyclic graph (DAG)\ncontribute unequally to effective policy learning. TD-GFN leverages inverse\nreinforcement learning to estimate edge-level rewards from the offline dataset,\nwhich are then used to ingeniously prune the DAG and guide backward trajectory\nsampling during training. This approach directs the policy toward high-reward\nregions while reducing the complexity of model fitting. Empirical results\nacross multiple tasks show that TD-GFN trains both efficiently and reliably,\nsignificantly outperforming existing baselines in convergence speed and sample\nquality."}
{"id": "2505.18512", "pdf": "https://arxiv.org/pdf/2505.18512", "abs": "https://arxiv.org/abs/2505.18512", "authors": ["Soyoung Yoon", "Gyuwan Kim", "Gyu-Hwung Cho", "Seung-won Hwang"], "title": "AcuRank: Uncertainty-Aware Adaptive Computation for Listwise Reranking", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": "22 pages, 3 figures. The first two authors contributed equally.\n  Author order is randomly determined via coin toss", "summary": "Listwise reranking with large language models (LLMs) enhances top-ranked\nresults in retrieval-based applications. Due to the limit in context size and\nhigh inference cost of long context, reranking is typically performed over a\nfixed size of small subsets, with the final ranking aggregated from these\npartial results. This fixed computation disregards query difficulty and\ndocument distribution, leading to inefficiencies. We propose AcuRank, an\nadaptive reranking framework that dynamically adjusts both the amount and\ntarget of computation based on uncertainty estimates over document relevance.\nUsing a Bayesian TrueSkill model, we iteratively refine relevance estimates\nuntil reaching sufficient confidence levels, and our explicit modeling of\nranking uncertainty enables principled control over reranking behavior and\navoids unnecessary updates to confident predictions. Results on the TREC-DL and\nBEIR benchmarks show that our method consistently achieves a superior\naccuracy-efficiency trade-off and scales better with compute than\nfixed-computation baselines. These results highlight the effectiveness and\ngeneralizability of our method across diverse retrieval tasks and LLM-based\nreranking models."}
{"id": "2505.18761", "pdf": "https://arxiv.org/pdf/2505.18761", "abs": "https://arxiv.org/abs/2505.18761", "authors": ["Minglai Yang", "Ethan Huang", "Liang Zhang", "Mihai Surdeanu", "William Wang", "Liangming Pan"], "title": "How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "15 pages, 9 figure, 4 tables", "summary": "We introduce Grade School Math with Distracting Context (GSM-DC), a synthetic\nbenchmark to evaluate Large Language Models' (LLMs) reasoning robustness\nagainst systematically controlled irrelevant context (IC). GSM-DC constructs\nsymbolic reasoning graphs with precise distractor injections, enabling\nrigorous, reproducible evaluation. Our experiments demonstrate that LLMs are\nsignificantly sensitive to IC, affecting both reasoning path selection and\narithmetic accuracy. Additionally, training models with strong distractors\nimproves performance in both in-distribution and out-of-distribution scenarios.\nWe further propose a stepwise tree search guided by a process reward model,\nwhich notably enhances robustness in out-of-distribution conditions."}
{"id": "2505.20123", "pdf": "https://arxiv.org/pdf/2505.20123", "abs": "https://arxiv.org/abs/2505.20123", "authors": ["Huijie Zhang", "Zijian Huang", "Siyi Chen", "Jinfan Zhou", "Zekai Zhang", "Peng Wang", "Qing Qu"], "title": "Understanding Generalization in Diffusion Models via Probability Flow Distance", "categories": ["cs.LG", "cs.CV"], "comment": "41 pages, 14 figures", "summary": "Diffusion models have emerged as a powerful class of generative models,\ncapable of producing high-quality samples that generalize beyond the training\ndata. However, evaluating this generalization remains challenging: theoretical\nmetrics are often impractical for high-dimensional data, while no practical\nmetrics rigorously measure generalization. In this work, we bridge this gap by\nintroducing probability flow distance ($\\texttt{PFD}$), a theoretically\ngrounded and computationally efficient metric to measure distributional\ngeneralization. Specifically, $\\texttt{PFD}$ quantifies the distance between\ndistributions by comparing their noise-to-data mappings induced by the\nprobability flow ODE. Moreover, by using $\\texttt{PFD}$ under a teacher-student\nevaluation protocol, we empirically uncover several key generalization\nbehaviors in diffusion models, including: (1) scaling behavior from\nmemorization to generalization, (2) early learning and double descent training\ndynamics, and (3) bias-variance decomposition. Beyond these insights, our work\nlays a foundation for future empirical and theoretical studies on\ngeneralization in diffusion models."}
{"id": "2505.18545", "pdf": "https://arxiv.org/pdf/2505.18545", "abs": "https://arxiv.org/abs/2505.18545", "authors": ["An Vo", "Mohammad Reza Taesiri", "Daeyoung Kim", "Anh Totti Nguyen"], "title": "B-score: Detecting biases in large language models using response history", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to ICML 2025 (Main track)", "summary": "Large language models (LLMs) often exhibit strong biases, e.g, against women\nor in favor of the number 7. We investigate whether LLMs would be able to\noutput less biased answers when allowed to observe their prior answers to the\nsame question in a multi-turn conversation. To understand which types of\nquestions invite more biased answers, we test LLMs on our proposed set of\nquestions that span 9 topics and belong to three types: (1) Subjective; (2)\nRandom; and (3) Objective. Interestingly, LLMs are able to \"de-bias\" themselves\nin a multi-turn conversation in response to questions that seek an Random,\nunbiased answer. Furthermore, we propose B-score, a novel metric that is\neffective in detecting biases to Subjective, Random, Easy, and Hard questions.\nOn MMLU, HLE, and CSQA, leveraging B-score substantially improves the\nverification accuracy of LLM answers (i.e, accepting LLM correct answers and\nrejecting incorrect ones) compared to using verbalized confidence scores or the\nfrequency of single-turn answers alone. Code and data are available at:\nhttps://b-score.github.io."}
{"id": "2505.18762", "pdf": "https://arxiv.org/pdf/2505.18762", "abs": "https://arxiv.org/abs/2505.18762", "authors": ["Michael Flor", "Zuowei Wang", "Paul Deane", "Tenaha O'Reilly"], "title": "Towards an automatic method for generating topical vocabulary test forms for specific reading passages", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "This manuscript was accepted to be published as an ETS Research\n  Report. Keywords topics; vocabulary; background knowledge; automatic item\n  generation; assessment; reading comprehension", "summary": "Background knowledge is typically needed for successful comprehension of\ntopical and domain specific reading passages, such as in the STEM domain.\nHowever, there are few automated measures of student knowledge that can be\nreadily deployed and scored in time to make predictions on whether a given\nstudent will likely be able to understand a specific content area text. In this\npaper, we present our effort in developing K-tool, an automated system for\ngenerating topical vocabulary tests that measure students' background knowledge\nrelated to a specific text. The system automatically detects the topic of a\ngiven text and produces topical vocabulary items based on their relationship\nwith the topic. This information is used to automatically generate background\nknowledge forms that contain words that are highly related to the topic and\nwords that share similar features but do not share high associations to the\ntopic. Prior research indicates that performance on such tasks can help\ndetermine whether a student is likely to understand a particular text based on\ntheir knowledge state. The described system is intended for use with middle and\nhigh school student population of native speakers of English. It is designed to\nhandle single reading passages and is not dependent on any corpus or text\ncollection. In this paper, we describe the system architecture and present an\ninitial evaluation of the system outputs."}
{"id": "2505.20130", "pdf": "https://arxiv.org/pdf/2505.20130", "abs": "https://arxiv.org/abs/2505.20130", "authors": ["Zhu Jin", "Li Jingyi", "Zhou Hongyi", "Lin Yinan", "Lin Zhenhua", "Shi Chengchun"], "title": "Balancing Interference and Correlation in Spatial Experimental Designs: A Causal Graph Cut Approach", "categories": ["cs.LG", "stat.CO", "stat.ML"], "comment": "Accepted by ICML2025", "summary": "This paper focuses on the design of spatial experiments to optimize the\namount of information derived from the experimental data and enhance the\naccuracy of the resulting causal effect estimator. We propose a surrogate\nfunction for the mean squared error (MSE) of the estimator, which facilitates\nthe use of classical graph cut algorithms to learn the optimal design. Our\nproposal offers three key advances: (1) it accommodates moderate to large\nspatial interference effects; (2) it adapts to different spatial covariance\nfunctions; (3) it is computationally efficient. Theoretical results and\nnumerical experiments based on synthetic environments and a dispatch simulator\nthat models a city-scale ridesharing market, further validate the effectiveness\nof our design. A python implementation of our method is available at\nhttps://github.com/Mamba413/CausalGraphCut."}
{"id": "2505.18573", "pdf": "https://arxiv.org/pdf/2505.18573", "abs": "https://arxiv.org/abs/2505.18573", "authors": ["Mengqi Liao", "Xiangyu Xi", "Ruinian Chen", "Jia Leng", "Yangen Hu", "Ke Zeng", "Shuai Liu", "Huaiyu Wan"], "title": "Enhancing Efficiency and Exploration in Reinforcement Learning for LLMs", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Reasoning large language models (LLMs) excel in complex tasks, which has\ndrawn significant attention to reinforcement learning (RL) for LLMs. However,\nexisting approaches allocate an equal number of rollouts to all questions\nduring the RL process, which is inefficient. This inefficiency stems from the\nfact that training on simple questions yields limited gains, whereas more\nrollouts are needed for challenging questions to sample correct answers.\nFurthermore, while RL improves response precision, it limits the model's\nexploration ability, potentially resulting in a performance cap below that of\nthe base model prior to RL. To address these issues, we propose a mechanism for\ndynamically allocating rollout budgets based on the difficulty of the problems,\nenabling more efficient RL training. Additionally, we introduce an adaptive\ndynamic temperature adjustment strategy to maintain the entropy at a stable\nlevel, thereby encouraging sufficient exploration. This enables LLMs to improve\nresponse precision while preserving their exploratory ability to uncover\npotential correct pathways. The code and data is available on:\nhttps://github.com/LiaoMengqi/E3-RL4LLMs"}
{"id": "2505.18766", "pdf": "https://arxiv.org/pdf/2505.18766", "abs": "https://arxiv.org/abs/2505.18766", "authors": ["Yanjie Li", "Wenxuan Zhang", "Xinqi Lyu", "Yihao Liu", "Bin Xiao"], "title": "StyleGuard: Preventing Text-to-Image-Model-based Style Mimicry Attacks by Style Perturbations", "categories": ["cs.CV", "cs.AI"], "comment": "submitted to NIPS2025", "summary": "Recently, text-to-image diffusion models have been widely used for style\nmimicry and personalized customization through methods such as DreamBooth and\nTextual Inversion. This has raised concerns about intellectual property\nprotection and the generation of deceptive content. Recent studies, such as\nGlaze and Anti-DreamBooth, have proposed using adversarial noise to protect\nimages from these attacks. However, recent purification-based methods, such as\nDiffPure and Noise Upscaling, have successfully attacked these latest defenses,\nshowing the vulnerabilities of these methods. Moreover, present methods show\nlimited transferability across models, making them less effective against\nunknown text-to-image models. To address these issues, we propose a novel\nanti-mimicry method, StyleGuard. We propose a novel style loss that optimizes\nthe style-related features in the latent space to make it deviate from the\noriginal image, which improves model-agnostic transferability. Additionally, to\nenhance the perturbation's ability to bypass diffusion-based purification, we\ndesigned a novel upscale loss that involves ensemble purifiers and upscalers\nduring training. Extensive experiments on the WikiArt and CelebA datasets\ndemonstrate that StyleGuard outperforms existing methods in robustness against\nvarious transformations and purifications, effectively countering style mimicry\nin various models. Moreover, StyleGuard is effective on different style mimicry\nmethods, including DreamBooth and Textual Inversion."}
{"id": "2505.20131", "pdf": "https://arxiv.org/pdf/2505.20131", "abs": "https://arxiv.org/abs/2505.20131", "authors": ["Yuanxin Zhuang", "Dazhong Shen", "Ying Sun"], "title": "MolEditRL: Structure-Preserving Molecular Editing via Discrete Diffusion and Reinforcement Learning", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Molecular editing aims to modify a given molecule to optimize desired\nchemical properties while preserving structural similarity. However, current\napproaches typically rely on string-based or continuous representations, which\nfail to adequately capture the discrete, graph-structured nature of molecules,\nresulting in limited structural fidelity and poor controllability. In this\npaper, we propose MolEditRL, a molecular editing framework that explicitly\nintegrates structural constraints with precise property optimization.\nSpecifically, MolEditRL consists of two stages: (1) a discrete graph diffusion\nmodel pretrained to reconstruct target molecules conditioned on source\nstructures and natural language instructions; (2) an editing-aware\nreinforcement learning fine-tuning stage that further enhances property\nalignment and structural preservation by explicitly optimizing editing\ndecisions under graph constraints. For comprehensive evaluation, we construct\nMolEdit-Instruct, the largest and most property-rich molecular editing dataset,\ncomprising 3 million diverse examples spanning single- and multi-property tasks\nacross 10 chemical attributes. Experimental results demonstrate that MolEditRL\nsignificantly outperforms state-of-the-art methods in both property\noptimization accuracy and structural fidelity, achieving a 74\\% improvement in\nediting success rate while using 98\\% fewer parameters."}
{"id": "2505.18585", "pdf": "https://arxiv.org/pdf/2505.18585", "abs": "https://arxiv.org/abs/2505.18585", "authors": ["Yedi Zhang", "Sun Yi Emma", "Annabelle Lee Jia En", "Annabelle Lee Jia En", "Jin Song Dong"], "title": "RvLLM: LLM Runtime Verification with Domain Knowledge", "categories": ["cs.AI", "cs.CL", "cs.LO"], "comment": "12 pages, 2 figures", "summary": "Large language models (LLMs) have emerged as a dominant AI paradigm due to\ntheir exceptional text understanding and generation capabilities. However,\ntheir tendency to generate inconsistent or erroneous outputs challenges their\nreliability, especially in high-stakes domains requiring accuracy and\ntrustworthiness. Existing research primarily focuses on detecting and\nmitigating model misbehavior in general-purpose scenarios, often overlooking\nthe potential of integrating domain-specific knowledge. In this work, we\nadvance misbehavior detection by incorporating domain knowledge. The core idea\nis to design a general specification language that enables domain experts to\ncustomize domain-specific predicates in a lightweight and intuitive manner,\nsupporting later runtime verification of LLM outputs. To achieve this, we\ndesign a novel specification language, ESL, and introduce a runtime\nverification framework, RvLLM, to validate LLM output against domain-specific\nconstraints defined in ESL. We evaluate RvLLM on three representative tasks:\nviolation detection against Singapore Rapid Transit Systems Act, numerical\ncomparison, and inequality solving. Experimental results demonstrate that RvLLM\neffectively detects erroneous outputs across various LLMs in a lightweight and\nflexible manner. The results reveal that despite their impressive capabilities,\nLLMs remain prone to low-level errors due to limited interpretability and a\nlack of formal guarantees during inference, and our framework offers a\npotential long-term solution by leveraging expert domain knowledge to\nrigorously and efficiently verify LLM outputs."}
{"id": "2505.18773", "pdf": "https://arxiv.org/pdf/2505.18773", "abs": "https://arxiv.org/abs/2505.18773", "authors": ["Jamie Hayes", "Ilia Shumailov", "Christopher A. Choquette-Choo", "Matthew Jagielski", "George Kaissis", "Katherine Lee", "Milad Nasr", "Sahra Ghalebikesabi", "Niloofar Mireshghallah", "Meenatchi Sundaram Mutu Selva Annamalai", "Igor Shilov", "Matthieu Meeus", "Yves-Alexandre de Montjoye", "Franziska Boenisch", "Adam Dziedzic", "A. Feder Cooper"], "title": "Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "State-of-the-art membership inference attacks (MIAs) typically require\ntraining many reference models, making it difficult to scale these attacks to\nlarge pre-trained language models (LLMs). As a result, prior research has\neither relied on weaker attacks that avoid training reference models (e.g.,\nfine-tuning attacks), or on stronger attacks applied to small-scale models and\ndatasets. However, weaker attacks have been shown to be brittle - achieving\nclose-to-arbitrary success - and insights from strong attacks in simplified\nsettings do not translate to today's LLMs. These challenges have prompted an\nimportant question: are the limitations observed in prior work due to attack\ndesign choices, or are MIAs fundamentally ineffective on LLMs? We address this\nquestion by scaling LiRA - one of the strongest MIAs - to GPT-2 architectures\nranging from 10M to 1B parameters, training reference models on over 20B tokens\nfrom the C4 dataset. Our results advance the understanding of MIAs on LLMs in\nthree key ways: (1) strong MIAs can succeed on pre-trained LLMs; (2) their\neffectiveness, however, remains limited (e.g., AUC<0.7) in practical settings;\nand, (3) the relationship between MIA success and related privacy metrics is\nnot as straightforward as prior work has suggested."}
{"id": "2505.20132", "pdf": "https://arxiv.org/pdf/2505.20132", "abs": "https://arxiv.org/abs/2505.20132", "authors": ["Safa Hamreras", "Sukhbinder Singh", "Romn Ors"], "title": "Tensorization is a powerful but underexplored tool for compression and interpretability of neural networks", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": "This article has been prepared for submission as a \"Position paper\"\n  following the guidelines provided at\n  https://neurips.cc/Conferences/2025/CallForPositionPapers", "summary": "Tensorizing a neural network involves reshaping some or all of its dense\nweight matrices into higher-order tensors and approximating them using low-rank\ntensor network decompositions. This technique has shown promise as a model\ncompression strategy for large-scale neural networks. However, despite\nencouraging empirical results, tensorized neural networks (TNNs) remain\nunderutilized in mainstream deep learning. In this position paper, we offer a\nperspective on both the potential and current limitations of TNNs. We argue\nthat TNNs represent a powerful yet underexplored framework for deep\nlearning--one that deserves greater attention from both engineering and\ntheoretical communities. Beyond compression, we highlight the value of TNNs as\na flexible class of architectures with distinctive scaling properties and\nincreased interpretability. A central feature of TNNs is the presence of bond\nindices, which introduce new latent spaces not found in conventional networks.\nThese internal representations may provide deeper insight into the evolution of\nfeatures across layers, potentially advancing the goals of mechanistic\ninterpretability. We conclude by outlining several key research directions\naimed at overcoming the practical barriers to scaling and adopting TNNs in\nmodern deep learning workflows."}
{"id": "2505.18644", "pdf": "https://arxiv.org/pdf/2505.18644", "abs": "https://arxiv.org/abs/2505.18644", "authors": ["Jingran Xie", "Xiang Li", "Hui Wang", "Yue Yu", "Yang Xiang", "Xixin Wu", "Zhiyong Wu"], "title": "Enhancing Generalization of Speech Large Language Models with Multi-Task Behavior Imitation and Speech-Text Interleaving", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted by Interspeech 2025", "summary": "Large language models (LLMs) have shown remarkable generalization across\ntasks, leading to increased interest in integrating speech with LLMs. These\nspeech LLMs (SLLMs) typically use supervised fine-tuning to align speech with\ntext-based LLMs. However, the lack of annotated speech data across a wide range\nof tasks hinders alignment efficiency, resulting in poor generalization. To\naddress these issues, we propose a novel multi-task 'behavior imitation' method\nwith speech-text interleaving, called MTBI, which relies solely on paired\nspeech and transcripts. By ensuring the LLM decoder generates equivalent\nresponses to paired speech and text, we achieve a more generalized SLLM.\nInterleaving is used to further enhance alignment efficiency. We introduce a\nsimple benchmark to evaluate prompt and task generalization across different\nmodels. Experimental results demonstrate that our MTBI outperforms SOTA SLLMs\non both prompt and task generalization, while requiring less supervised speech\ndata."}
{"id": "2505.18775", "pdf": "https://arxiv.org/pdf/2505.18775", "abs": "https://arxiv.org/abs/2505.18775", "authors": ["Jiayu Wang", "Yang Jiao", "Yue Yu", "Tianwen Qian", "Shaoxiang Chen", "Jingjing Chen", "Yu-Gang Jiang"], "title": "OmniGenBench: A Benchmark for Omnipotent Multimodal Generation across 50+ Tasks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent breakthroughs in large multimodal models (LMMs), such as the\nimpressive GPT-4o-Native, have demonstrated remarkable proficiency in following\ngeneral-purpose instructions for image generation. However, current benchmarks\noften lack the necessary breadth and depth to fully evaluate the diverse\ncapabilities of these models. To overcome this limitation, we introduce\nOmniGenBench, a novel and comprehensive benchmark meticulously designed to\nassess the instruction-following abilities of state-of-the-art LMMs across both\nperception-centric and cognition-centric dimensions. Our OmniGenBench includes\n57 diverse sub-tasks grounded in real-world scenarios, systematically\ncategorized according to the specific model capabilities they demand. For\nrigorous evaluation, we further employ a dual-mode protocol. This protocol\nutilizes off-the-shelf visual parsing tools for perception-centric tasks and a\npowerful LLM-based judger for cognition-centric tasks to assess the alignment\nbetween generated images and user instructions. Using OmniGenBench, we evaluate\nmainstream generative models, including prevalent models like GPT-4o,\nGemini-2.0-Flash, and Seedream, and provide in-depth comparisons and analyses\nof their performance.Code and data are available at\nhttps://github.com/emilia113/OmniGenBench."}
{"id": "2505.20135", "pdf": "https://arxiv.org/pdf/2505.20135", "abs": "https://arxiv.org/abs/2505.20135", "authors": ["Wenyang Liao", "Quanziang Wang", "Yichen Wu", "Renzhen Wang", "Deyu Meng"], "title": "Data-Distill-Net: A Data Distillation Approach Tailored for Reply-based Continual Learning", "categories": ["cs.LG"], "comment": null, "summary": "Replay-based continual learning (CL) methods assume that models trained on a\nsmall subset can also effectively minimize the empirical risk of the complete\ndataset. These methods maintain a memory buffer that stores a sampled subset of\ndata from previous tasks to consolidate past knowledge. However, this\nassumption is not guaranteed in practice due to the limited capacity of the\nmemory buffer and the heuristic criteria used for buffer data selection. To\naddress this issue, we propose a new dataset distillation framework tailored\nfor CL, which maintains a learnable memory buffer to distill the global\ninformation from the current task data and accumulated knowledge preserved in\nthe previous memory buffer. Moreover, to avoid the computational overhead and\noverfitting risks associated with parameterizing the entire buffer during\ndistillation, we introduce a lightweight distillation module that can achieve\nglobal information distillation solely by generating learnable soft labels for\nthe memory buffer data. Extensive experiments show that, our method can achieve\ncompetitive results and effectively mitigates forgetting across various\ndatasets. The source code will be publicly available."}
{"id": "2505.18646", "pdf": "https://arxiv.org/pdf/2505.18646", "abs": "https://arxiv.org/abs/2505.18646", "authors": ["Siwei Liu", "Jinyuan Fang", "Han Zhou", "Yingxu Wang", "Zaiqiao Meng"], "title": "SEW: Self-Evolving Agentic Workflows for Automated Code Generation", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "16 pages, 5 figures", "summary": "Large Language Models (LLMs) have demonstrated effectiveness in code\ngeneration tasks. To enable LLMs to address more complex coding challenges,\nexisting research has focused on crafting multi-agent systems with agentic\nworkflows, where complex coding tasks are decomposed into sub-tasks, assigned\nto specialized agents. Despite their effectiveness, current approaches heavily\nrely on hand-crafted agentic workflows, with both agent topologies and prompts\nmanually designed, which limits their ability to automatically adapt to\ndifferent types of coding problems. To address these limitations and enable\nautomated workflow design, we propose \\textbf{S}elf-\\textbf{E}volving\n\\textbf{W}orkflow (\\textbf{SEW}), a novel self-evolving framework that\nautomatically generates and optimises multi-agent workflows. Extensive\nexperiments on three coding benchmark datasets, including the challenging\nLiveCodeBench, demonstrate that our SEW can automatically design agentic\nworkflows and optimise them through self-evolution, bringing up to 33\\%\nimprovement on LiveCodeBench compared to using the backbone LLM only.\nFurthermore, by investigating different representation schemes of workflow, we\nprovide insights into the optimal way to encode workflow information with text."}
{"id": "2505.18777", "pdf": "https://arxiv.org/pdf/2505.18777", "abs": "https://arxiv.org/abs/2505.18777", "authors": ["Yiding Wang", "Fauxu meng", "Xuefeng Zhang", "Fan Jiang", "Pingzhi Tang", "Muhan Zhang"], "title": "HD-PiSSA: High-Rank Distributed Orthogonal Adaptation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Existing parameter-efficient fine-tuning (PEFT) methods for large language\nmodels (LLMs), such as LoRA and PiSSA, constrain model updates to low-rank\nsubspaces, limiting their expressiveness and leading to suboptimal performance\non complex tasks. To address this, we introduce High-rank Distributed PiSSA\n(HD-PiSSA), a distributed PEFT approach that initializes orthogonal adapters\nacross different devices and aggregates their delta updates collectively on W\nfor fine-tuning. Unlike Data Parallel LoRA or PiSSA, which maintain identical\nadapters across all devices, HD-PiSSA assigns different principal components of\nthe pre-trained weights to each GPU, significantly expanding the range of\nupdate directions. This results in over 16x higher effective updated ranks than\ndata-parallel LoRA or PiSSA when fine-tuning on 8 GPUs with the same per-device\nadapter rank. Empirically, we evaluate HD-PiSSA across various challenging\ndownstream tasks, including mathematics, code generation, and multi-task\nlearning. In the multi-task setting, HD-PiSSA achieves average gains of 10.0\nabsolute points (14.63%) over LoRA and 4.98 points (6.60%) over PiSSA across 12\nbenchmarks, demonstrating its benefits from the extra optimization flexibility."}
{"id": "2505.20137", "pdf": "https://arxiv.org/pdf/2505.20137", "abs": "https://arxiv.org/abs/2505.20137", "authors": ["Cdric Goemaere", "Gaspard Oliviers", "Rafal Bogacz", "Thomas Demeester"], "title": "Error Optimization: Overcoming Exponential Signal Decay in Deep Predictive Coding Networks", "categories": ["cs.LG", "cs.AI"], "comment": "All code available at\n  https://github.com/cgoemaere/pc_error_optimization", "summary": "Predictive Coding (PC) offers a biologically plausible alternative to\nbackpropagation for neural network training, yet struggles with deeper\narchitectures. This paper identifies the root cause: an inherent signal decay\nproblem where gradients attenuate exponentially with depth, becoming\ncomputationally negligible due to numerical precision constraints. To address\nthis fundamental limitation, we introduce Error Optimization (EO), a novel\nreparameterization that preserves PC's theoretical properties while eliminating\nsignal decay. By optimizing over prediction errors rather than states, EO\nenables signals to reach all layers simultaneously and without attenuation,\nconverging orders of magnitude faster than standard PC. Experiments across\nmultiple architectures and datasets demonstrate that EO matches\nbackpropagation's performance even for deeper models where conventional PC\nstruggles. Besides practical improvements, our work provides theoretical\ninsight into PC dynamics and establishes a foundation for scaling\nbiologically-inspired learning to deeper architectures on digital hardware and\nbeyond."}
{"id": "2505.18668", "pdf": "https://arxiv.org/pdf/2505.18668", "abs": "https://arxiv.org/abs/2505.18668", "authors": ["Zhen Li", "Yukai Guo", "Duan Li", "Xinyuan Guo", "Bowen Li", "Lanxi Xiao", "Shenyu Qiao", "Jiashu Chen", "Zijian Wu", "Hui Zhang", "Xinhuan Shu", "Shixia Liu"], "title": "ChartGalaxy: A Dataset for Infographic Chart Understanding and Generation", "categories": ["cs.CV", "cs.CL"], "comment": "63 pages, submitted to NeurIPS 2025 Datasets and Benchmarks Track", "summary": "Infographic charts are a powerful medium for communicating abstract data by\ncombining visual elements (e.g., charts, images) with textual information.\nHowever, their visual and structural richness poses challenges for large\nvision-language models (LVLMs), which are typically trained on plain charts. To\nbridge this gap, we introduce ChartGalaxy, a million-scale dataset designed to\nadvance the understanding and generation of infographic charts. The dataset is\nconstructed through an inductive process that identifies 75 chart types, 330\nchart variations, and 68 layout templates from real infographic charts and uses\nthem to create synthetic ones programmatically. We showcase the utility of this\ndataset through: 1) improving infographic chart understanding via fine-tuning,\n2) benchmarking code generation for infographic charts, and 3) enabling\nexample-based infographic chart generation. By capturing the visual and\nstructural complexity of real design, ChartGalaxy provides a useful resource\nfor enhancing multimodal reasoning and generation in LVLMs."}
{"id": "2505.18783", "pdf": "https://arxiv.org/pdf/2505.18783", "abs": "https://arxiv.org/abs/2505.18783", "authors": ["Xinbao Qiao", "Ningning Ding", "Yushi Cheng", "Meng Zhang"], "title": "Soft Weighted Machine Unlearning", "categories": ["cs.LG", "cs.AI"], "comment": "24 pages,22 figures", "summary": "Machine unlearning, as a post-hoc processing technique, has gained widespread\nadoption in addressing challenges like bias mitigation and robustness\nenhancement, colloquially, machine unlearning for fairness and robustness.\nHowever, existing non-privacy unlearning-based solutions persist in using\nbinary data removal framework designed for privacy-driven motivation, leading\nto significant information loss, a phenomenon known as over-unlearning. While\nover-unlearning has been largely described in many studies as primarily causing\nutility degradation, we investigate its fundamental causes and provide deeper\ninsights in this work through counterfactual leave-one-out analysis. In this\npaper, we introduce a weighted influence function that assigns tailored weights\nto each sample by solving a convex quadratic programming problem analytically.\nBuilding on this, we propose a soft-weighted framework enabling fine-grained\nmodel adjustments to address the over-unlearning challenge. We demonstrate that\nthe proposed soft-weighted scheme is versatile and can be seamlessly integrated\ninto most existing unlearning algorithms. Extensive experiments show that in\nfairness- and robustness-driven tasks, the soft-weighted scheme significantly\noutperforms hard-weighted schemes in fairness/robustness metrics and alleviates\nthe decline in utility metric, thereby enhancing machine unlearning algorithm\nas an effective correction solution."}
{"id": "2505.20142", "pdf": "https://arxiv.org/pdf/2505.20142", "abs": "https://arxiv.org/abs/2505.20142", "authors": ["Ioannis Athanasiadis", "Anmar Karmush", "Michael Felsberg"], "title": "Model Stitching by Functional Latent Alignment", "categories": ["cs.LG"], "comment": null, "summary": "Evaluating functional similarity involves quantifying the degree to which\nindependently trained neural networks learn functionally similar\nrepresentations. Reliably inferring the functional similarity of these networks\nremains an open problem with far-reaching implications for AI. Model stitching\nhas emerged as a promising paradigm, where an optimal affine transformation\naligns two models to solve a task, with the stitched model serving as a proxy\nfor functional similarity. In this work, we draw inspiration from the knowledge\ndistillation literature and propose Functional Latent Alignment (FuLA) as a\nnovel optimality condition for model stitching. We revisit previously explored\nfunctional similarity testbeds and introduce a new one, based on which FuLA\nemerges as an overall more reliable method of functional similarity.\nSpecifically, our experiments in (a) adversarial training, (b) shortcut\ntraining and, (c) cross-layer stitching, reveal that FuLA is less prone to\nartifacts tied to training on task cues while achieving non-trivial alignments\nthat are missed by stitch-level matching."}
{"id": "2505.18675", "pdf": "https://arxiv.org/pdf/2505.18675", "abs": "https://arxiv.org/abs/2505.18675", "authors": ["Sicheng Feng", "Song Wang", "Shuyi Ouyang", "Lingdong Kong", "Zikai Song", "Jianke Zhu", "Huan Wang", "Xinchao Wang"], "title": "Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual Reasoning from Transit Maps", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Multimodal large language models (MLLMs) have recently achieved significant\nprogress in visual tasks, including semantic scene understanding and text-image\nalignment, with reasoning variants enhancing performance on complex tasks\ninvolving mathematics and logic. However, their capacity for reasoning tasks\ninvolving fine-grained visual understanding remains insufficiently evaluated.\nTo address this gap, we introduce ReasonMap, a benchmark designed to assess the\nfine-grained visual understanding and spatial reasoning abilities of MLLMs.\nReasonMap encompasses high-resolution transit maps from 30 cities across 13\ncountries and includes 1,008 question-answer pairs spanning two question types\nand three templates. Furthermore, we design a two-level evaluation pipeline\nthat properly assesses answer correctness and quality. Comprehensive\nevaluations of 15 popular MLLMs, including both base and reasoning variants,\nreveal a counterintuitive pattern: among open-source models, base models\noutperform reasoning ones, while the opposite trend is observed in\nclosed-source models. Additionally, performance generally degrades when visual\ninputs are masked, indicating that while MLLMs can leverage prior knowledge to\nanswer some questions, fine-grained visual reasoning tasks still require\ngenuine visual perception for strong performance. Our benchmark study offers\nnew insights into visual reasoning and contributes to investigating the gap\nbetween open-source and closed-source models."}
{"id": "2505.18787", "pdf": "https://arxiv.org/pdf/2505.18787", "abs": "https://arxiv.org/abs/2505.18787", "authors": ["Hong-Hanh Nguyen-Le", "Van-Tuan Tran", "Dinh-Thuc Nguyen", "Nhien-An Le-Khac"], "title": "Think Twice before Adaptation: Improving Adaptability of DeepFake Detection via Online Test-Time Adaptation", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": "Accepted at 34th International Joint Conference on Artificial\n  Intelligence (IJCAI-25)", "summary": "Deepfake (DF) detectors face significant challenges when deployed in\nreal-world environments, particularly when encountering test samples deviated\nfrom training data through either postprocessing manipulations or distribution\nshifts. We demonstrate postprocessing techniques can completely obscure\ngeneration artifacts presented in DF samples, leading to performance\ndegradation of DF detectors. To address these challenges, we propose Think\nTwice before Adaptation (\\texttt{T$^2$A}), a novel online test-time adaptation\nmethod that enhances the adaptability of detectors during inference without\nrequiring access to source training data or labels. Our key idea is to enable\nthe model to explore alternative options through an Uncertainty-aware Negative\nLearning objective rather than solely relying on its initial predictions as\ncommonly seen in entropy minimization (EM)-based approaches. We also introduce\nan Uncertain Sample Prioritization strategy and Gradients Masking technique to\nimprove the adaptation by focusing on important samples and model parameters.\nOur theoretical analysis demonstrates that the proposed negative learning\nobjective exhibits complementary behavior to EM, facilitating better adaptation\ncapability. Empirically, our method achieves state-of-the-art results compared\nto existing test-time adaptation (TTA) approaches and significantly enhances\nthe resilience and generalization of DF detectors during inference. Code is\navailable\n\\href{https://github.com/HongHanh2104/T2A-Think-Twice-Before-Adaptation}{here}."}
{"id": "2505.20150", "pdf": "https://arxiv.org/pdf/2505.20150", "abs": "https://arxiv.org/abs/2505.20150", "authors": ["Ilai Reshef", "Nadav Dym"], "title": "On the (Non) Injectivity of Piecewise Linear Janossy Pooling", "categories": ["cs.LG", "cs.AI"], "comment": "14 pages, 2 figures", "summary": "Multiset functions, which are functions that map multisets to vectors, are a\nfundamental tool in the construction of neural networks for multisets and\ngraphs. To guarantee that the vector representation of the multiset is\nfaithful, it is often desirable to have multiset mappings that are both\ninjective and bi-Lipschitz. Currently, there are several constructions of\nmultiset functions achieving both these guarantees, leading to improved\nperformance in some tasks but often also to higher compute time than standard\nconstructions. Accordingly, it is natural to inquire whether simpler multiset\nfunctions achieving the same guarantees are available. In this paper, we make a\nlarge step towards giving a negative answer to this question. We consider the\nfamily of k-ary Janossy pooling, which includes many of the most popular\nmultiset models, and prove that no piecewise linear Janossy pooling function\ncan be injective. On the positive side, we show that when restricted to\nmultisets without multiplicities, even simple deep-sets models suffice for\ninjectivity and bi-Lipschitzness."}
{"id": "2505.18680", "pdf": "https://arxiv.org/pdf/2505.18680", "abs": "https://arxiv.org/abs/2505.18680", "authors": ["Yuanhe Zhang", "Xinyue Wang", "Haoran Gao", "Zhenhong Zhou", "Fanyu Meng", "Yuyao Zhang", "Sen Su"], "title": "$PD^3F$: A Pluggable and Dynamic DoS-Defense Framework Against Resource Consumption Attacks Targeting Large Language Models", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs), due to substantial computational requirements,\nare vulnerable to resource consumption attacks, which can severely degrade\nserver performance or even cause crashes, as demonstrated by denial-of-service\n(DoS) attacks designed for LLMs. However, existing works lack mitigation\nstrategies against such threats, resulting in unresolved security risks for\nreal-world LLM deployments. To this end, we propose the Pluggable and Dynamic\nDoS-Defense Framework ($PD^3F$), which employs a two-stage approach to defend\nagainst resource consumption attacks from both the input and output sides. On\nthe input side, we propose the Resource Index to guide Dynamic Request Polling\nScheduling, thereby reducing resource usage induced by malicious attacks under\nhigh-concurrency scenarios. On the output side, we introduce the Adaptive\nEnd-Based Suppression mechanism, which terminates excessive malicious\ngeneration early. Experiments across six models demonstrate that $PD^3F$\nsignificantly mitigates resource consumption attacks, improving users' access\ncapacity by up to 500% during adversarial load. $PD^3F$ represents a step\ntoward the resilient and resource-aware deployment of LLMs against resource\nconsumption attacks."}
{"id": "2505.18799", "pdf": "https://arxiv.org/pdf/2505.18799", "abs": "https://arxiv.org/abs/2505.18799", "authors": ["Hao Chen", "Haoze Li", "Zhiqing Xiao", "Lirong Gao", "Qi Zhang", "Xiaomeng Hu", "Ningtao Wang", "Xing Fu", "Junbo Zhao"], "title": "ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 8 figures, 14 tables", "summary": "Aligning general-purpose large language models (LLMs) to downstream tasks\noften incurs significant costs, including constructing task-specific\ninstruction pairs and extensive training adjustments. Prior research has\nexplored various avenues to enhance alignment efficiency, primarily through\nminimal-data training or data-driven activations to identify key attention\nheads. However, these approaches inherently introduce data dependency, which\nhinders generalization and reusability. To address this issue and enhance model\nalignment efficiency, we propose the \\textit{\\textbf{A}ttention\n\\textbf{L}ocalization and \\textbf{P}runing \\textbf{S}trategy (\\textbf{ALPS})},\nan efficient algorithm that localizes the most task-sensitive attention heads\nand prunes by restricting attention training updates to these heads, thereby\nreducing alignment costs. Experimental results demonstrate that our method\nactivates only \\textbf{10\\%} of attention parameters during fine-tuning while\nachieving a \\textbf{2\\%} performance improvement over baselines on three tasks.\nMoreover, the identified task-specific heads are transferable across datasets\nand mitigate knowledge forgetting. Our work and findings provide a novel\nperspective on efficient LLM alignment."}
{"id": "2505.20161", "pdf": "https://arxiv.org/pdf/2505.20161", "abs": "https://arxiv.org/abs/2505.20161", "authors": ["Jaehun Jung", "Seungju Han", "Ximing Lu", "Skyler Hallinan", "David Acuna", "Shrimai Prabhumoye", "Mostafa Patwary", "Mohammad Shoeybi", "Bryan Catanzaro", "Yejin Choi"], "title": "Prismatic Synthesis: Gradient-based Data Diversification Boosts Generalization in LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Effective generalization in language models depends critically on the\ndiversity of their training data. Yet existing diversity metrics often fall\nshort of this goal, relying on surface-level heuristics that are decoupled from\nmodel behavior. This motivates us to ask: What kind of diversity in training\ndata actually drives generalization in language models -- and how can we\nmeasure and amplify it? Through large-scale empirical analyses spanning over\n300 training runs, carefully controlled for data scale and quality, we show\nthat data diversity can be a strong predictor of generalization in LLM\nreasoning -- as measured by average model performance on unseen\nout-of-distribution benchmarks. We introduce G-Vendi, a metric that quantifies\ndiversity via the entropy of model-induced gradients. Despite using a small\noff-the-shelf proxy model for gradients, G-Vendi consistently outperforms\nalternative measures, achieving strong correlation (Spearman's $\\rho \\approx\n0.9$) with out-of-distribution (OOD) performance on both natural language\ninference (NLI) and math reasoning tasks. Building on this insight, we present\nPrismatic Synthesis, a framework for generating diverse synthetic data by\ntargeting underrepresented regions in gradient space. Experimental results show\nthat Prismatic Synthesis consistently improves model performance as we scale\nsynthetic data -- not just on in-distribution test but across unseen,\nout-of-distribution benchmarks -- significantly outperforming state-of-the-art\nmodels that rely on 20 times larger data generator than ours. For example,\nPrismMath-7B, our model distilled from a 32B LLM, outperforms\nR1-Distill-Qwen-7B -- the same base model trained on proprietary data generated\nby 671B R1 -- on 6 out of 7 challenging benchmarks."}
{"id": "2505.18713", "pdf": "https://arxiv.org/pdf/2505.18713", "abs": "https://arxiv.org/abs/2505.18713", "authors": ["Guodong Du", "Zitao Fang", "Jing Li", "Junlin Li", "Runhua Jiang", "Shuyang Yu", "Yifei Guo", "Yangneng Chen", "Sim Kuan Goh", "Ho-Kin Tang", "Daojing He", "Honghai Liu", "Min Zhang"], "title": "Neural Parameter Search for Slimmer Fine-Tuned Models and Better Transfer", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by ACL2025 Main", "summary": "Foundation models and their checkpoints have significantly advanced deep\nlearning, boosting performance across various applications. However, fine-tuned\nmodels often struggle outside their specific domains and exhibit considerable\nredundancy. Recent studies suggest that combining a pruned fine-tuned model\nwith the original pre-trained model can mitigate forgetting, reduce\ninterference when merging model parameters across tasks, and improve\ncompression efficiency. In this context, developing an effective pruning\nstrategy for fine-tuned models is crucial. Leveraging the advantages of the\ntask vector mechanism, we preprocess fine-tuned models by calculating the\ndifferences between them and the original model. Recognizing that different\ntask vector subspaces contribute variably to model performance, we introduce a\nnovel method called Neural Parameter Search (NPS-Pruning) for slimming down\nfine-tuned models. This method enhances pruning efficiency by searching through\nneural parameters of task vectors within low-rank subspaces. Our method has\nthree key applications: enhancing knowledge transfer through pairwise model\ninterpolation, facilitating effective knowledge fusion via model merging, and\nenabling the deployment of compressed models that retain near-original\nperformance while significantly reducing storage costs. Extensive experiments\nacross vision, NLP, and multi-modal benchmarks demonstrate the effectiveness\nand robustness of our approach, resulting in substantial performance gains. The\ncode is publicly available at: https://github.com/duguodong7/NPS-Pruning."}
{"id": "2505.18817", "pdf": "https://arxiv.org/pdf/2505.18817", "abs": "https://arxiv.org/abs/2505.18817", "authors": ["Seongsu Kim", "Nayoung Kim", "Dongwoo Kim", "Sungsoo Ahn"], "title": "High-order Equivariant Flow Matching for Density Functional Theory Hamiltonian Prediction", "categories": ["physics.comp-ph", "cs.AI"], "comment": null, "summary": "Density functional theory (DFT) is a fundamental method for simulating\nquantum chemical properties, but it remains expensive due to the iterative\nself-consistent field (SCF) process required to solve the Kohn-Sham equations.\nRecently, deep learning methods are gaining attention as a way to bypass this\nstep by directly predicting the Hamiltonian. However, they rely on\ndeterministic regression and do not consider the highly structured nature of\nHamiltonians. In this work, we propose QHFlow, a high-order equivariant flow\nmatching framework that generates Hamiltonian matrices conditioned on molecular\ngeometry. Flow matching models continuous-time trajectories between simple\npriors and complex targets, learning the structured distributions over\nHamiltonians instead of direct regression. To further incorporate symmetry, we\nuse a neural architecture that predicts SE(3)-equivariant vector fields,\nimproving accuracy and generalization across diverse geometries. To further\nenhance physical fidelity, we additionally introduce a fine-tuning scheme to\nalign predicted orbital energies with the target. QHFlow achieves\nstate-of-the-art performance, reducing Hamiltonian error by 71% on MD17 and 53%\non QH9. Moreover, we further show that QHFlow accelerates the DFT process\nwithout trading off the solution quality when initializing SCF iterations with\nthe predicted Hamiltonian, significantly reducing the number of iterations and\nruntime."}
{"id": "2505.20172", "pdf": "https://arxiv.org/pdf/2505.20172", "abs": "https://arxiv.org/abs/2505.20172", "authors": ["Etienne Boursier", "Scott Pesme", "Radu-Alexandru Dragomir"], "title": "A Theoretical Framework for Grokking: Interpolation followed by Riemannian Norm Minimisation", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": null, "summary": "We study the dynamics of gradient flow with small weight decay on general\ntraining losses $F: \\mathbb{R}^d \\to \\mathbb{R}$. Under mild regularity\nassumptions and assuming convergence of the unregularised gradient flow, we\nshow that the trajectory with weight decay $\\lambda$ exhibits a two-phase\nbehaviour as $\\lambda \\to 0$. During the initial fast phase, the trajectory\nfollows the unregularised gradient flow and converges to a manifold of critical\npoints of $F$. Then, at time of order $1/\\lambda$, the trajectory enters a slow\ndrift phase and follows a Riemannian gradient flow minimising the $\\ell_2$-norm\nof the parameters. This purely optimisation-based phenomenon offers a natural\nexplanation for the \\textit{grokking} effect observed in deep learning, where\nthe training loss rapidly reaches zero while the test loss plateaus for an\nextended period before suddenly improving. We argue that this generalisation\njump can be attributed to the slow norm reduction induced by weight decay, as\nexplained by our analysis. We validate this mechanism empirically on several\nsynthetic regression tasks."}
{"id": "2505.18722", "pdf": "https://arxiv.org/pdf/2505.18722", "abs": "https://arxiv.org/abs/2505.18722", "authors": ["Terry Yi Zhong", "Esther Janse", "Cristian Tejedor-Garcia", "Louis ten Bosch", "Martha Larson"], "title": "Evaluating the Usefulness of Non-Diagnostic Speech Data for Developing Parkinson's Disease Classifiers", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "Accepted for Interspeech 2025 (Camera-Ready)", "summary": "Speech-based Parkinson's disease (PD) detection has gained attention for its\nautomated, cost-effective, and non-intrusive nature. As research studies\nusually rely on data from diagnostic-oriented speech tasks, this work explores\nthe feasibility of diagnosing PD on the basis of speech data not originally\nintended for diagnostic purposes, using the Turn-Taking (TT) dataset. Our\nfindings indicate that TT can be as useful as diagnostic-oriented PD datasets\nlike PC-GITA. We also investigate which specific dataset characteristics impact\nPD classification performance. The results show that concatenating audio\nrecordings and balancing participants' gender and status distributions can be\nbeneficial. Cross-dataset evaluation reveals that models trained on PC-GITA\ngeneralize poorly to TT, whereas models trained on TT perform better on\nPC-GITA. Furthermore, we provide insights into the high variability across\nfolds, which is mainly due to large differences in individual speaker\nperformance."}
{"id": "2505.18859", "pdf": "https://arxiv.org/pdf/2505.18859", "abs": "https://arxiv.org/abs/2505.18859", "authors": ["Yuxiang Liu", "Kevin Chen-Chuan Chang"], "title": "Writing Like the Best: Exemplar-Based Expository Text Generation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025. Camera-ready version", "summary": "We introduce the Exemplar-Based Expository Text Generation task, aiming to\ngenerate an expository text on a new topic using an exemplar on a similar\ntopic. Current methods fall short due to their reliance on extensive exemplar\ndata, difficulty in adapting topic-specific content, and issues with long-text\ncoherence. To address these challenges, we propose the concept of Adaptive\nImitation and present a novel Recurrent Plan-then-Adapt (RePA) framework. RePA\nleverages large language models (LLMs) for effective adaptive imitation through\na fine-grained plan-then-adapt process. RePA also enables recurrent\nsegment-by-segment imitation, supported by two memory structures that enhance\ninput clarity and output coherence. We also develop task-specific evaluation\nmetrics--imitativeness, adaptiveness, and adaptive-imitativeness--using LLMs as\nevaluators. Experimental results across our collected three diverse datasets\ndemonstrate that RePA surpasses existing baselines in producing factual,\nconsistent, and relevant texts for this task."}
{"id": "2505.20177", "pdf": "https://arxiv.org/pdf/2505.20177", "abs": "https://arxiv.org/abs/2505.20177", "authors": ["Adam R. Klivans", "Konstantinos Stavropoulos", "Kevin Tian", "Arsen Vasilyan"], "title": "The Power of Iterative Filtering for Supervised Learning with (Heavy) Contamination", "categories": ["cs.LG", "cs.DS", "stat.ML"], "comment": "36 pages", "summary": "Inspired by recent work on learning with distribution shift, we give a\ngeneral outlier removal algorithm called iterative polynomial filtering and\nshow a number of striking applications for supervised learning with\ncontamination: (1) We show that any function class that can be approximated by\nlow-degree polynomials with respect to a hypercontractive distribution can be\nefficiently learned under bounded contamination (also known as nasty noise).\nThis is a surprising resolution to a longstanding gap between the complexity of\nagnostic learning and learning with contamination, as it was widely believed\nthat low-degree approximators only implied tolerance to label noise. (2) For\nany function class that admits the (stronger) notion of sandwiching\napproximators, we obtain near-optimal learning guarantees even with respect to\nheavy additive contamination, where far more than $1/2$ of the training set may\nbe added adversarially. Prior related work held only for regression and in a\nlist-decodable setting. (3) We obtain the first efficient algorithms for\ntolerant testable learning of functions of halfspaces with respect to any fixed\nlog-concave distribution. Even the non-tolerant case for a single halfspace in\nthis setting had remained open. These results significantly advance our\nunderstanding of efficient supervised learning under contamination, a setting\nthat has been much less studied than its unsupervised counterpart."}
{"id": "2505.18789", "pdf": "https://arxiv.org/pdf/2505.18789", "abs": "https://arxiv.org/abs/2505.18789", "authors": ["Wasi Uddin Ahmad", "Somshubra Majumdar", "Boris Ginsburg"], "title": "From Output to Evaluation: Does Raw Instruction-Tuned Code LLMs Output Suffice for Fill-in-the-Middle Code Generation?", "categories": ["cs.SE", "cs.CL"], "comment": "Work in progress", "summary": "Post-processing is crucial for the automatic evaluation of LLMs in\nfill-in-the-middle (FIM) code generation due to the frequent presence of\nextraneous code in raw outputs. This extraneous generation suggests a lack of\nawareness regarding output boundaries, requiring truncation for effective\nevaluation. The determination of an optimal truncation strategy, however, often\nproves intricate, particularly when the scope includes several programming\nlanguages. This study investigates the necessity of post-processing\ninstruction-tuned LLM outputs. Our findings reveal that supervised fine-tuning\nsignificantly enhances FIM code generation, enabling LLMs to generate code that\nseamlessly integrates with the surrounding context. Evaluating our fine-tuned\n\\texttt{Qwen2.5-Coder} (base and instruct) models on HumanEval Infilling and\nSAFIM benchmarks demonstrates improved performances without post-processing,\nespecially when the \\emph{middle} consist of complete lines. However,\npost-processing of the LLM outputs remains necessary when the \\emph{middle} is\na random span of code."}
{"id": "2505.18878", "pdf": "https://arxiv.org/pdf/2505.18878", "abs": "https://arxiv.org/abs/2505.18878", "authors": ["Kung-Hsiang Huang", "Akshara Prabhakar", "Onkar Thorat", "Divyansh Agarwal", "Prafulla Kumar Choubey", "Yixin Mao", "Silvio Savarese", "Caiming Xiong", "Chien-Sheng Wu"], "title": "CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While AI agents hold transformative potential in business, effective\nperformance benchmarking is hindered by the scarcity of public, realistic\nbusiness data on widely used platforms. Existing benchmarks often lack fidelity\nin their environments, data, and agent-user interactions, with limited coverage\nof diverse business scenarios and industries. To address these gaps, we\nintroduce CRMArena-Pro, a novel benchmark for holistic, realistic assessment of\nLLM agents in diverse professional settings. CRMArena-Pro expands on CRMArena\nwith nineteen expert-validated tasks across sales, service, and 'configure,\nprice, and quote' processes, for both Business-to-Business and\nBusiness-to-Customer scenarios. It distinctively incorporates multi-turn\ninteractions guided by diverse personas and robust confidentiality awareness\nassessments. Experiments reveal leading LLM agents achieve only around 58%\nsingle-turn success on CRMArena-Pro, with performance dropping significantly to\napproximately 35% in multi-turn settings. While Workflow Execution proves more\ntractable for top agents (over 83% single-turn success), other evaluated\nbusiness skills present greater challenges. Furthermore, agents exhibit\nnear-zero inherent confidentiality awareness; though targeted prompting can\nimprove this, it often compromises task performance. These findings highlight a\nsubstantial gap between current LLM capabilities and enterprise demands,\nunderscoring the need for advancements in multi-turn reasoning, confidentiality\nadherence, and versatile skill acquisition."}
{"id": "2505.20188", "pdf": "https://arxiv.org/pdf/2505.20188", "abs": "https://arxiv.org/abs/2505.20188", "authors": ["Zhenzhen Song", "Ziwei Liu", "Hongji Li"], "title": "Research on feature fusion and multimodal patent text based on graph attention network", "categories": ["cs.LG", "cs.IR"], "comment": null, "summary": "Aiming at the problems of cross-modal feature fusion, low efficiency of long\ntext modeling and lack of hierarchical semantic coherence in patent text\nsemantic mining, this study proposes HGM-Net, a deep learning framework that\nintegrates Hierarchical Comparative Learning (HCL), Multi-modal Graph Attention\nNetwork (M-GAT) and Multi-Granularity Sparse Attention (MSA), which builds a\ndynamic mask, contrast and cross-structural similarity constraints on the word,\nsentence and paragraph hierarchies through HCL. Contrast and cross-structural\nsimilarity constraints are constructed at the word and paragraph levels by HCL\nto strengthen the local semantic and global thematic consistency of patent\ntext; M-GAT models patent classification codes, citation relations and text\nsemantics as heterogeneous graph structures, and achieves dynamic fusion of\nmulti-source features by cross-modal gated attention; MSA adopts a hierarchical\nsparsity strategy to optimize the computational efficiency of long text\nmodeling at word, phrase, sentence and paragraph granularity. Experiments show\nthat the framework demonstrates significant advantages over existing deep\nlearning methods in tasks such as patent classification and similarity\nmatching, and provides a solution with both theoretical innovation and\npractical value for solving the problems of patent examination efficiency\nimprovement and technology relevance mining."}
{"id": "2505.18822", "pdf": "https://arxiv.org/pdf/2505.18822", "abs": "https://arxiv.org/abs/2505.18822", "authors": ["Shijue Huang", "Hongru Wang", "Wanjun Zhong", "Zhaochen Su", "Jiazhan Feng", "Bowen Cao", "Yi R. Fung"], "title": "AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Modern large reasoning models demonstrate impressive problem-solving\ncapabilities by employing sophisticated reasoning strategies. However, they\noften struggle to balance efficiency and effectiveness, frequently generating\nunnecessarily lengthy reasoning chains for simple problems. In this work, we\npropose AdaCtrl, a novel framework to support both difficulty-aware adaptive\nreasoning budget allocation and explicit user control over reasoning depth.\nAdaCtrl dynamically adjusts its reasoning length based on self-assessed problem\ndifficulty, while also allowing users to manually control the budget to\nprioritize either efficiency or effectiveness. This is achieved through a\ntwo-stage training pipeline: an initial cold-start fine-tuning phase to instill\nthe ability to self-aware difficulty and adjust reasoning budget, followed by a\ndifficulty-aware reinforcement learning (RL) stage that refines the model's\nadaptive reasoning strategies and calibrates its difficulty assessments based\non its evolving capabilities during online training. To enable intuitive user\ninteraction, we design explicit length-triggered tags that function as a\nnatural interface for budget control. Empirical results show that AdaCtrl\nadapts reasoning length based on estimated difficulty, compared to the standard\ntraining baseline that also incorporates fine-tuning and RL, it yields\nperformance improvements and simultaneously reduces response length by 10.06%\nand 12.14% on the more challenging AIME2024 and AIME2025 datasets, which\nrequire elaborate reasoning, and by 62.05% and 91.04% on the MATH500 and GSM8K\ndatasets, where more concise responses are sufficient. Furthermore, AdaCtrl\nenables precise user control over the reasoning budget, allowing for tailored\nresponses to meet specific needs."}
{"id": "2505.18880", "pdf": "https://arxiv.org/pdf/2505.18880", "abs": "https://arxiv.org/abs/2505.18880", "authors": ["Weihan Xu", "Yimeng Ma", "Jingyue Huang", "Yang Li", "Wenye Ma", "Taylor Berg-Kirkpatrick", "Julian McAuley", "Paul Pu Liang", "Hao-Wen Dong"], "title": "REGen: Multimodal Retrieval-Embedded Generation for Long-to-Short Video Editing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Short videos are an effective tool for promoting contents and improving\nknowledge accessibility. While existing extractive video summarization methods\nstruggle to produce a coherent narrative, existing abstractive methods cannot\n`quote' from the input videos, i.e., inserting short video clips in their\noutputs. In this work, we explore novel video editing models for generating\nshorts that feature a coherent narrative with embedded video insertions\nextracted from a long input video. We propose a novel retrieval-embedded\ngeneration framework that allows a large language model to quote multimodal\nresources while maintaining a coherent narrative. Our proposed REGen system\nfirst generates the output story script with quote placeholders using a\nfinetuned large language model, and then uses a novel retrieval model to\nreplace the quote placeholders by selecting a video clip that best supports the\nnarrative from a pool of candidate quotable video clips. We examine the\nproposed method on the task of documentary teaser generation, where short\ninterview insertions are commonly used to support the narrative of a\ndocumentary. Our objective evaluations show that the proposed method can\neffectively insert short video clips while maintaining a coherent narrative. In\na subjective survey, we show that our proposed method outperforms existing\nabstractive and extractive approaches in terms of coherence, alignment, and\nrealism in teaser generation."}
{"id": "2505.20192", "pdf": "https://arxiv.org/pdf/2505.20192", "abs": "https://arxiv.org/abs/2505.20192", "authors": ["Bingguang Hao", "Maolin Wang", "Zengzhuang Xu", "Cunyin Peng", "Yicheng Chen", "Xiangyu Zhao", "Jinjie Gu", "Chenyi Zhuang"], "title": "FunReason: Enhancing Large Language Models' Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement", "categories": ["cs.LG", "cs.IR"], "comment": null, "summary": "The integration of large language models (LLMs) with function calling has\nemerged as a crucial capability for enhancing their practical utility in\nreal-world applications. However, effectively combining reasoning processes\nwith accurate function execution remains a significant challenge. Traditional\ntraining approaches often struggle to balance the detailed reasoning steps with\nthe precision of function calls, leading to suboptimal performance. To address\nthese limitations, we introduce FunReason, a novel framework that enhances\nLLMs' function calling capabilities through an automated data refinement\nstrategy and a Self-Refinement Multiscale Loss (SRML) approach. FunReason\nleverages LLMs' natural reasoning abilities to generate high-quality training\nexamples, focusing on query parseability, reasoning coherence, and function\ncall precision. The SRML approach dynamically balances the contribution of\nreasoning processes and function call accuracy during training, addressing the\ninherent trade-off between these two critical aspects. FunReason achieves\nperformance comparable to GPT-4o while effectively mitigating catastrophic\nforgetting during fine-tuning. FunReason provides a comprehensive solution for\nenhancing LLMs' function calling capabilities by introducing a balanced\ntraining methodology and a data refinement pipeline. For code and dataset,\nplease refer to our repository at GitHub\nhttps://github.com/BingguangHao/FunReason"}
{"id": "2505.18830", "pdf": "https://arxiv.org/pdf/2505.18830", "abs": "https://arxiv.org/abs/2505.18830", "authors": ["Wenlong Deng", "Yi Ren", "Muchen Li", "Danica J. Sutherland", "Xiaoxiao Li", "Christos Thrampoulidis"], "title": "On the Effect of Negative Gradient in Group Relative Deep Reinforcement Optimization", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) has become popular in enhancing the reasoning\ncapabilities of large language models (LLMs), with Group Relative Policy\nOptimization (GRPO) emerging as a widely used algorithm in recent systems.\nDespite GRPO's widespread adoption, we identify a previously unrecognized\nphenomenon we term Lazy Likelihood Displacement (LLD), wherein the likelihood\nof correct responses marginally increases or even decreases during training.\nThis behavior mirrors a recently discovered misalignment issue in Direct\nPreference Optimization (DPO), attributed to the influence of negative\ngradients. We provide a theoretical analysis of GRPO's learning dynamic,\nidentifying the source of LLD as the naive penalization of all tokens in\nincorrect responses with the same strength. To address this, we develop a\nmethod called NTHR, which downweights penalties on tokens contributing to the\nLLD. Unlike prior DPO-based approaches, NTHR takes advantage of GRPO's\ngroup-based structure, using correct responses as anchors to identify\ninfluential tokens. Experiments on math reasoning benchmarks demonstrate that\nNTHR effectively mitigates LLD, yielding consistent performance gains across\nmodels ranging from 0.5B to 3B parameters."}
{"id": "2505.18881", "pdf": "https://arxiv.org/pdf/2505.18881", "abs": "https://arxiv.org/abs/2505.18881", "authors": ["Dicong Qiu", "Jiadi You", "Zeying Gong", "Ronghe Qiu", "Hui Xiong", "Junwei Liang"], "title": "SD-OVON: A Semantics-aware Dataset and Benchmark Generation Pipeline for Open-Vocabulary Object Navigation in Dynamic Scenes", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Preprint. 21 pages", "summary": "We present the Semantics-aware Dataset and Benchmark Generation Pipeline for\nOpen-vocabulary Object Navigation in Dynamic Scenes (SD-OVON). It utilizes\npretraining multimodal foundation models to generate infinite unique\nphoto-realistic scene variants that adhere to real-world semantics and daily\ncommonsense for the training and the evaluation of navigation agents,\naccompanied with a plugin for generating object navigation task episodes\ncompatible to the Habitat simulator. In addition, we offer two pre-generated\nobject navigation task datasets, SD-OVON-3k and SD-OVON-10k, comprising\nrespectively about 3k and 10k episodes of the open-vocabulary object navigation\ntask, derived from the SD-OVON-Scenes dataset with 2.5k photo-realistic scans\nof real-world environments and the SD-OVON-Objects dataset with 0.9k manually\ninspected scanned and artist-created manipulatable object models. Unlike prior\ndatasets limited to static environments, SD-OVON covers dynamic scenes and\nmanipulatable objects, facilitating both real-to-sim and sim-to-real robotic\napplications. This approach enhances the realism of navigation tasks, the\ntraining and the evaluation of open-vocabulary object navigation agents in\ncomplex settings. To demonstrate the effectiveness of our pipeline and\ndatasets, we propose two baselines and evaluate them along with\nstate-of-the-art baselines on SD-OVON-3k. The datasets, benchmark and source\ncode are publicly available."}
{"id": "2505.20211", "pdf": "https://arxiv.org/pdf/2505.20211", "abs": "https://arxiv.org/abs/2505.20211", "authors": ["Junseo Hwang", "Wonguk Cho", "Taesup Kim"], "title": "Parameter-Efficient Fine-Tuning with Column Space Projection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Fine-tuning large language models (LLMs) with minimal computational overhead\nis essential for efficiently adapting them to downstream tasks under resource\nconstraints. Parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank\nAdaptation (LoRA), facilitate this by updating only a small subset of\nparameters. However, recent studies show that LoRA diverges from full\nfine-tuning (Full FT) in its learning behavior, particularly in terms of\nspectral properties. Motivated by these findings, we propose PiCa, the first\ntheoretically grounded PEFT method based on the spectral properties of\nfine-tuned weights. PiCa projects gradients onto the low-rank column subspace\nof pre-trained weights and exhibits learning patterns more closely aligned with\nFull FT. Furthermore, we show that combining PiCa with weight sharing\ndrastically reduces the number of trainable parameters without compromising\nperformance, enabling to achieve superior performance than LoRA using 13x fewer\ntrainable parameters. Extensive experiments demonstrate PiCa achieves the\nstate-of-the-art performance compared to existing PEFT methods."}
{"id": "2505.18847", "pdf": "https://arxiv.org/pdf/2505.18847", "abs": "https://arxiv.org/abs/2505.18847", "authors": ["William Han", "Chaojing Duan", "Zhepeng Cen", "Yihang Yao", "Xiaoyu Song", "Atharva Mhaskar", "Dylan Leong", "Michael A. Rosenberg", "Emerson Liu", "Ding Zhao"], "title": "Signal, Image, or Symbolic: Exploring the Best Input Representation for Electrocardiogram-Language Models Through a Unified Framework", "categories": ["cs.AI", "cs.CL"], "comment": "29 pages, 2 figures, 8 tables", "summary": "Recent advances have increasingly applied large language models (LLMs) to\nelectrocardiogram (ECG) interpretation, giving rise to\nElectrocardiogram-Language Models (ELMs). Conditioned on an ECG and a textual\nquery, an ELM autoregressively generates a free-form textual response. Unlike\ntraditional classification-based systems, ELMs emulate expert cardiac\nelectrophysiologists by issuing diagnoses, analyzing waveform morphology,\nidentifying contributing factors, and proposing patient-specific action plans.\nTo realize this potential, researchers are curating instruction-tuning datasets\nthat pair ECGs with textual dialogues and are training ELMs on these resources.\nYet before scaling ELMs further, there is a fundamental question yet to be\nexplored: What is the most effective ECG input representation? In recent works,\nthree candidate representations have emerged-raw time-series signals, rendered\nimages, and discretized symbolic sequences. We present the first comprehensive\nbenchmark of these modalities across 6 public datasets and 5 evaluation\nmetrics. We find symbolic representations achieve the greatest number of\nstatistically significant wins over both signal and image inputs. We further\nablate the LLM backbone, ECG duration, and token budget, and we evaluate\nrobustness to signal perturbations. We hope that our findings offer clear\nguidance for selecting input representations when developing the next\ngeneration of ELMs."}
{"id": "2505.18884", "pdf": "https://arxiv.org/pdf/2505.18884", "abs": "https://arxiv.org/abs/2505.18884", "authors": ["Borna Khodabandeh", "Amirabbas Afzali", "Amirhossein Afsharrad", "Seyed Shahabeddin Mousavi", "Sanjay Lall", "Sajjad Amini", "Seyed-Mohsen Moosavi-Dezfooli"], "title": "LORE: Lagrangian-Optimized Robust Embeddings for Visual Encoders", "categories": ["cs.LG", "cs.AI", "cs.CV", "math.OC"], "comment": null, "summary": "Visual encoders have become fundamental components in modern computer vision\npipelines. However, ensuring robustness against adversarial perturbations\nremains a critical challenge. Recent efforts have explored both supervised and\nunsupervised adversarial fine-tuning strategies. We identify two key\nlimitations in these approaches: (i) they often suffer from instability,\nespecially during the early stages of fine-tuning, resulting in suboptimal\nconvergence and degraded performance on clean data, and (ii) they exhibit a\nsuboptimal trade-off between robustness and clean data accuracy, hindering the\nsimultaneous optimization of both objectives. To overcome these challenges, we\npropose Lagrangian-Optimized Robust Embeddings (LORE), a novel unsupervised\nadversarial fine-tuning framework. LORE utilizes constrained optimization,\nwhich offers a principled approach to balancing competing goals, such as\nimproving robustness while preserving nominal performance. By enforcing\nembedding-space proximity constraints, LORE effectively maintains clean data\nperformance throughout adversarial fine-tuning. Extensive experiments show that\nLORE significantly improves zero-shot adversarial robustness with minimal\ndegradation in clean data accuracy. Furthermore, we demonstrate the\neffectiveness of the adversarially fine-tuned CLIP image encoder in\nout-of-distribution generalization and enhancing the interpretability of image\nembeddings."}
{"id": "2505.20218", "pdf": "https://arxiv.org/pdf/2505.20218", "abs": "https://arxiv.org/abs/2505.20218", "authors": ["Chenxiao Fan", "Chongming Gao", "Wentao Shi", "Yaxin Gong", "Zihao Zhao", "Fuli Feng"], "title": "Fine-grained List-wise Alignment for Generative Medication Recommendation", "categories": ["cs.LG"], "comment": null, "summary": "Accurate and safe medication recommendations are critical for effective\nclinical decision-making, especially in multimorbidity cases. However, existing\nsystems rely on point-wise prediction paradigms that overlook synergistic drug\neffects and potential adverse drug-drug interactions (DDIs). We propose FLAME,\na fine-grained list-wise alignment framework for large language models (LLMs),\nenabling drug-by-drug generation of drug lists. FLAME formulates recommendation\nas a sequential decision process, where each step adds or removes a single\ndrug. To provide fine-grained learning signals, we devise step-wise Group\nRelative Policy Optimization (GRPO) with potential-based reward shaping, which\nexplicitly models DDIs and optimizes the contribution of each drug to the\noverall prescription. Furthermore, FLAME enhances patient modeling by\nintegrating structured clinical knowledge and collaborative information into\nthe representation space of LLMs. Experiments on benchmark datasets demonstrate\nthat FLAME achieves state-of-the-art performance, delivering superior accuracy,\ncontrollable safety-accuracy trade-offs, and strong generalization across\ndiverse clinical scenarios. Our code is available at\nhttps://github.com/cxfann/Flame."}
{"id": "2505.18855", "pdf": "https://arxiv.org/pdf/2505.18855", "abs": "https://arxiv.org/abs/2505.18855", "authors": ["Peiqi Wang", "ShengYun Peng", "Xuewen Zhang", "Hanchao Yu", "Yibo Yang", "Lifu Huang", "Fujun Liu", "Qifan Wang"], "title": "Inference Compute-Optimal Video Vision Language Models", "categories": ["cs.CV", "cs.CL"], "comment": "Annual Meeting of the Association for Computational Linguistics\n  (ACL), 2025", "summary": "This work investigates the optimal allocation of inference compute across\nthree key scaling factors in video vision language models: language model size,\nframe count, and the number of visual tokens per frame. While prior works\ntypically focuses on optimizing model efficiency or improving performance\nwithout considering resource constraints, we instead identify optimal model\nconfiguration under fixed inference compute budgets. We conduct large-scale\ntraining sweeps and careful parametric modeling of task performance to identify\nthe inference compute-optimal frontier. Our experiments reveal how task\nperformance depends on scaling factors and finetuning data size, as well as how\nchanges in data size shift the compute-optimal frontier. These findings\ntranslate to practical tips for selecting these scaling factors."}
{"id": "2505.18889", "pdf": "https://arxiv.org/pdf/2505.18889", "abs": "https://arxiv.org/abs/2505.18889", "authors": ["Miles Q. Li", "Benjamin C. M. Fung"], "title": "Security Concerns for Large Language Models: A Survey", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) such as GPT-4 (and its recent iterations like\nGPT-4o and the GPT-4.1 series), Google's Gemini, Anthropic's Claude 3 models,\nand xAI's Grok have caused a revolution in natural language processing, but\ntheir capabilities also introduce new security vulnerabilities. In this survey,\nwe provide a comprehensive overview of the emerging security concerns around\nLLMs, categorizing threats into prompt injection and jailbreaking, adversarial\nattacks (including input perturbations and data poisoning), misuse by malicious\nactors (e.g., for disinformation, phishing, and malware generation), and\nworrisome risks inherent in autonomous LLM agents. A significant focus has been\nrecently placed on the latter, exploring goal misalignment, emergent deception,\nself-preservation instincts, and the potential for LLMs to develop and pursue\ncovert, misaligned objectives (scheming), which may even persist through safety\ntraining. We summarize recent academic and industrial studies (2022-2025) that\nexemplify each threat, analyze proposed defenses and their limitations, and\nidentify open challenges in securing LLM-based applications. We conclude by\nemphasizing the importance of advancing robust, multi-layered security\nstrategies to ensure LLMs are safe and beneficial."}
{"id": "2505.20221", "pdf": "https://arxiv.org/pdf/2505.20221", "abs": "https://arxiv.org/abs/2505.20221", "authors": ["Xiao Shou", "Yanna Ding", "Jianxi Gao"], "title": "Gradient Flow Matching for Learning Update Dynamics in Neural Network Training", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Training deep neural networks remains computationally intensive due to the\nitera2 tive nature of gradient-based optimization. We propose Gradient Flow\nMatching (GFM), a continuous-time modeling framework that treats neural network\ntraining as a dynamical system governed by learned optimizer-aware vector\nfields. By leveraging conditional flow matching, GFM captures the underlying\nupdate rules of optimizers such as SGD, Adam, and RMSprop, enabling smooth\nextrapolation of weight trajectories toward convergence. Unlike black-box\nsequence models, GFM incorporates structural knowledge of gradient-based\nupdates into the learning objective, facilitating accurate forecasting of final\nweights from partial training sequences. Empirically, GFM achieves forecasting\naccuracy that is competitive with Transformer-based models and significantly\noutperforms LSTM and other classical baselines. Furthermore, GFM generalizes\nacross neural architectures and initializations, providing a unified framework\nfor studying optimization dynamics and accelerating convergence prediction."}
{"id": "2505.18929", "pdf": "https://arxiv.org/pdf/2505.18929", "abs": "https://arxiv.org/abs/2505.18929", "authors": ["Wenda Zhang"], "title": "Meta-aware Learning in text-to-SQL Large Language Model", "categories": ["cs.AI", "cs.CL"], "comment": "Keywords: text-to-SQL LLM, fine-tuning, meta-aware leanring,\n  metadata, chain-of-thought, BigQuery SQL, business database", "summary": "The advancements of Large language models (LLMs) have provided great\nopportunities to text-to-SQL tasks to overcome the main challenges to\nunderstand complex domain information and complex database structures in\nbusiness applications. In this paper, we propose a meta-aware learning\nframework to integrate domain knowledge, database schema, chain-of-thought\nreasoning processes, and metadata relationships to improve the SQL generation\nquality. The proposed framework includes four learning strategies: schema-based\nlearning, Chain-of-Thought (CoT) learning, knowledge-enhanced learning, and key\ninformation tokenization. This approach provides a comprehensive understanding\nof database structure and metadata information towards LLM through fine-tuning\nto improve its performance on SQL generation within business domains. Through\ntwo experimental studies, we have demonstrated the superiority of the proposed\nmethods in execution accuracy, multi-task SQL generation capability, and\nreduction of catastrophic forgetting."}
{"id": "2505.18892", "pdf": "https://arxiv.org/pdf/2505.18892", "abs": "https://arxiv.org/abs/2505.18892", "authors": ["Vanessa Utz", "Steve DiPaola"], "title": "Climate Implications of Diffusion-based Generative Visual AI Systems and their Mass Adoption", "categories": ["cs.CY", "cs.AI"], "comment": "International Conference on Computational Creativity", "summary": "Climate implications of rapidly developing digital technologies, such as\nblockchains and the associated crypto mining and NFT minting, have been well\ndocumented and their massive GPU energy use has been identified as a cause for\nconcern. However, we postulate that due to their more mainstream consumer\nappeal, the GPU use of text-prompt based diffusion AI art systems also requires\nthoughtful considerations. Given the recent explosion in the number of highly\nsophisticated generative art systems and their rapid adoption by consumers and\ncreative professionals, the impact of these systems on the climate needs to be\ncarefully considered. In this work, we report on the growth of diffusion-based\nvisual AI systems, their patterns of use, growth and the implications on the\nclimate. Our estimates show that the mass adoption of these tools potentially\ncontributes considerably to global energy consumption. We end this paper with\nour thoughts on solutions and future areas of inquiry as well as associated\ndifficulties, including the lack of publicly available data."}
{"id": "2505.20229", "pdf": "https://arxiv.org/pdf/2505.20229", "abs": "https://arxiv.org/abs/2505.20229", "authors": ["Maximilian Dreyer", "Lorenz Hufe", "Jim Berend", "Thomas Wiegand", "Sebastian Lapuschkin", "Wojciech Samek"], "title": "From What to How: Attributing CLIP's Latent Components Reveals Unexpected Semantic Reliance", "categories": ["cs.LG", "cs.AI"], "comment": "25 pages (10 pages manuscript, 4 pages references, 11 pages appendix)", "summary": "Transformer-based CLIP models are widely used for text-image probing and\nfeature extraction, making it relevant to understand the internal mechanisms\nbehind their predictions. While recent works show that Sparse Autoencoders\n(SAEs) yield interpretable latent components, they focus on what these encode\nand miss how they drive predictions. We introduce a scalable framework that\nreveals what latent components activate for, how they align with expected\nsemantics, and how important they are to predictions. To achieve this, we adapt\nattribution patching for instance-wise component attributions in CLIP and\nhighlight key faithfulness limitations of the widely used Logit Lens technique.\nBy combining attributions with semantic alignment scores, we can automatically\nuncover reliance on components that encode semantically unexpected or spurious\nconcepts. Applied across multiple CLIP variants, our method uncovers hundreds\nof surprising components linked to polysemous words, compound nouns, visual\ntypography and dataset artifacts. While text embeddings remain prone to\nsemantic ambiguity, they are more robust to spurious correlations compared to\nlinear classifiers trained on image embeddings. A case study on skin lesion\ndetection highlights how such classifiers can amplify hidden shortcuts,\nunderscoring the need for holistic, mechanistic interpretability. We provide\ncode at https://github.com/maxdreyer/attributing-clip."}
{"id": "2505.18931", "pdf": "https://arxiv.org/pdf/2505.18931", "abs": "https://arxiv.org/abs/2505.18931", "authors": ["Ryan Saklad", "Aman Chadha", "Oleg Pavlov", "Raha Moraffah"], "title": "Can Large Language Models Infer Causal Relationships from Real-World Text?", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Understanding and inferring causal relationships from texts is a core aspect\nof human cognition and is essential for advancing large language models (LLMs)\ntowards artificial general intelligence. Existing work primarily focuses on\nsynthetically generated texts which involve simple causal relationships\nexplicitly mentioned in the text. This fails to reflect the complexities of\nreal-world tasks. In this paper, we investigate whether LLMs are capable of\ninferring causal relationships from real-world texts. We develop a benchmark\ndrawn from real-world academic literature which includes diverse texts with\nrespect to length, complexity of relationships (different levels of\nexplicitness, number of events, and causal relationships), and domains and\nsub-domains. To the best of our knowledge, our benchmark is the first-ever\nreal-world dataset for this task. Our experiments on state-of-the-art LLMs\nevaluated on our proposed benchmark demonstrate significant challenges, with\nthe best-performing model achieving an average F1 score of only 0.477. Analysis\nreveals common pitfalls: difficulty with implicitly stated information, in\ndistinguishing relevant causal factors from surrounding contextual details, and\nwith connecting causally relevant information spread across lengthy textual\npassages. By systematically characterizing these deficiencies, our benchmark\noffers targeted insights for further research into advancing LLM causal\nreasoning."}
{"id": "2505.18893", "pdf": "https://arxiv.org/pdf/2505.18893", "abs": "https://arxiv.org/abs/2505.18893", "authors": ["Reva Schwartz", "Rumman Chowdhury", "Akash Kundu", "Heather Frase", "Marzieh Fadaee", "Tom David", "Gabriella Waters", "Afaf Taik", "Morgan Briggs", "Patrick Hall", "Shomik Jain", "Kyra Yee", "Spencer Thomas", "Sundeep Bhandari", "Lee Wan Sie", "Qinghua Lu", "Matthew Holmes", "Theodora Skeadas"], "title": "Reality Check: A New Evaluation Ecosystem Is Necessary to Understand AI's Real World Effects", "categories": ["cs.CY", "cs.AI"], "comment": "9 pages", "summary": "Conventional AI evaluation approaches concentrated within the AI stack\nexhibit systemic limitations for exploring, navigating and resolving the human\nand societal factors that play out in real world deployment such as in\neducation, finance, healthcare, and employment sectors. AI capability\nevaluations can capture detail about first-order effects, such as whether\nimmediate system outputs are accurate, or contain toxic, biased or\nstereotypical content, but AI's second-order effects, i.e. any long-term\noutcomes and consequences that may result from AI use in the real world, have\nbecome a significant area of interest as the technology becomes embedded in our\ndaily lives. These secondary effects can include shifts in user behavior,\nsocietal, cultural and economic ramifications, workforce transformations, and\nlong-term downstream impacts that may result from a broad and growing set of\nrisks. This position paper argues that measuring the indirect and secondary\neffects of AI will require expansion beyond static, single-turn approaches\nconducted in silico to include testing paradigms that can capture what actually\nmaterializes when people use AI technology in context. Specifically, we\ndescribe the need for data and methods that can facilitate contextual awareness\nand enable downstream interpretation and decision making about AI's secondary\neffects, and recommend requirements for a new ecosystem."}
{"id": "2505.20232", "pdf": "https://arxiv.org/pdf/2505.20232", "abs": "https://arxiv.org/abs/2505.20232", "authors": ["Pranav Poudel", "Aavash Chhetri", "Prashnna Gyawali", "Georgios Leontidis", "Binod Bhattarai"], "title": "Multimodal Federated Learning With Missing Modalities through Feature Imputation Network", "categories": ["cs.LG", "cs.CV"], "comment": "MIUA 2025", "summary": "Multimodal federated learning holds immense potential for collaboratively\ntraining models from multiple sources without sharing raw data, addressing both\ndata scarcity and privacy concerns, two key challenges in healthcare. A major\nchallenge in training multimodal federated models in healthcare is the presence\nof missing modalities due to multiple reasons, including variations in clinical\npractice, cost and accessibility constraints, retrospective data collection,\nprivacy concerns, and occasional technical or human errors. Previous methods\ntypically rely on publicly available real datasets or synthetic data to\ncompensate for missing modalities. However, obtaining real datasets for every\ndisease is impractical, and training generative models to synthesize missing\nmodalities is computationally expensive and prone to errors due to the high\ndimensionality of medical data. In this paper, we propose a novel, lightweight,\nlow-dimensional feature translator to reconstruct bottleneck features of the\nmissing modalities. Our experiments on three different datasets (MIMIC-CXR, NIH\nOpen-I, and CheXpert), in both homogeneous and heterogeneous settings\nconsistently improve the performance of competitive baselines. The code and\nimplementation details are available at:\nhttps://github.com/bhattarailab/FedFeatGen"}
{"id": "2505.18933", "pdf": "https://arxiv.org/pdf/2505.18933", "abs": "https://arxiv.org/abs/2505.18933", "authors": ["Haitian Zhong", "Yuhuan Liu", "Ziyang Xu", "Guofan Liu", "Qiang Liu", "Shu Wu", "Zhe Zhao", "Liang Wang", "Tieniu Tan"], "title": "REACT: Representation Extraction And Controllable Tuning to Overcome Overfitting in LLM Knowledge Editing", "categories": ["cs.AI", "cs.CL"], "comment": "15 pages, 4 figures", "summary": "Large language model editing methods frequently suffer from overfitting,\nwherein factual updates can propagate beyond their intended scope,\noveremphasizing the edited target even when it's contextually inappropriate. To\naddress this challenge, we introduce REACT (Representation Extraction And\nControllable Tuning), a unified two-phase framework designed for precise and\ncontrollable knowledge editing. In the initial phase, we utilize tailored\nstimuli to extract latent factual representations and apply Principal Component\nAnalysis with a simple learnbale linear transformation to compute a directional\n\"belief shift\" vector for each instance. In the second phase, we apply\ncontrollable perturbations to hidden states using the obtained vector with a\nmagnitude scalar, gated by a pre-trained classifier that permits edits only\nwhen contextually necessary. Relevant experiments on EVOKE benchmarks\ndemonstrate that REACT significantly reduces overfitting across nearly all\nevaluation metrics, and experiments on COUNTERFACT and MQuAKE shows that our\nmethod preserves balanced basic editing performance (reliability, locality, and\ngenerality) under diverse editing scenarios."}
{"id": "2505.18897", "pdf": "https://arxiv.org/pdf/2505.18897", "abs": "https://arxiv.org/abs/2505.18897", "authors": ["Dipanwita Saha", "Anis Zaman", "Hua Zou", "Ning Chen", "Xinxin Shu", "Nadia Vase", "Abraham Bagherjeiran"], "title": "Improving Ad matching via Cluster-Adaptive Keyword Expansion and Relevance tuning", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "In search advertising, keyword matching connects user queries with relevant\nads. While token-based matching increases ad coverage, it can reduce relevance\ndue to overly permissive semantic expansion. This work extends keyword reach\nthrough document-side semantic keyword expansion, using a language model to\nbroaden token-level matching without altering queries. We propose a solution\nusing a pre-trained siamese model to generate dense vector representations of\nad keywords and identify semantically related variants through nearest neighbor\nsearch. To maintain precision, we introduce a cluster-based thresholding\nmechanism that adjusts similarity cutoffs based on local semantic density. Each\nexpanded keyword maps to a group of seller-listed items, which may only\npartially align with the original intent. To ensure relevance, we enhance the\ndownstream relevance model by adapting it to the expanded keyword space using\nan incremental learning strategy with a lightweight decision tree ensemble.\nThis system improves both relevance and click-through rate (CTR), offering a\nscalable, low-latency solution adaptable to evolving query behavior and\nadvertising inventory."}
{"id": "2505.20235", "pdf": "https://arxiv.org/pdf/2505.20235", "abs": "https://arxiv.org/abs/2505.20235", "authors": ["Jonathan Wenger", "Beau Coker", "Juraj Marusic", "John P. Cunningham"], "title": "Variational Deep Learning via Implicit Regularization", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Modern deep learning models generalize remarkably well in-distribution,\ndespite being overparametrized and trained with little to no explicit\nregularization. Instead, current theory credits implicit regularization imposed\nby the choice of architecture, hyperparameters and optimization procedure.\nHowever, deploying deep learning models out-of-distribution, in sequential\ndecision-making tasks, or in safety-critical domains, necessitates reliable\nuncertainty quantification, not just a point estimate. The machinery of modern\napproximate inference -- Bayesian deep learning -- should answer the need for\nuncertainty quantification, but its effectiveness has been challenged by our\ninability to define useful explicit inductive biases through priors, as well as\nthe associated computational burden. Instead, in this work we demonstrate, both\ntheoretically and empirically, how to regularize a variational deep network\nimplicitly via the optimization procedure, just as for standard deep learning.\nWe fully characterize the inductive bias of (stochastic) gradient descent in\nthe case of an overparametrized linear model as generalized variational\ninference and demonstrate the importance of the choice of parametrization.\nFinally, we show empirically that our approach achieves strong in- and\nout-of-distribution performance without tuning of additional hyperparameters\nand with minimal time and memory overhead over standard deep learning."}
{"id": "2505.18942", "pdf": "https://arxiv.org/pdf/2505.18942", "abs": "https://arxiv.org/abs/2505.18942", "authors": ["Honglin Bao", "Siyang Wu", "Jiwoong Choi", "Yingrong Mao", "James A. Evans"], "title": "Language Models Surface the Unwritten Code of Science and Society", "categories": ["cs.CY", "cs.CL", "cs.DL"], "comment": null, "summary": "This paper calls on the research community not only to investigate how human\nbiases are inherited by large language models (LLMs) but also to explore how\nthese biases in LLMs can be leveraged to make society's \"unwritten code\" - such\nas implicit stereotypes and heuristics - visible and accessible for critique.\nWe introduce a conceptual framework through a case study in science: uncovering\nhidden rules in peer review - the factors that reviewers care about but rarely\nstate explicitly due to normative scientific expectations. The idea of the\nframework is to push LLMs to speak out their heuristics through generating\nself-consistent hypotheses - why one paper appeared stronger in reviewer\nscoring - among paired papers submitted to 45 computer science conferences,\nwhile iteratively searching deeper hypotheses from remaining pairs where\nexisting hypotheses cannot explain. We observed that LLMs' normative priors\nabout the internal characteristics of good science extracted from their\nself-talk, e.g. theoretical rigor, were systematically updated toward\nposteriors that emphasize storytelling about external connections, such as how\nthe work is positioned and connected within and across literatures. This shift\nreveals the primacy of scientific myths about intrinsic properties driving\nscientific excellence rather than extrinsic contextualization and storytelling\nthat influence conceptions of relevance and significance. Human reviewers tend\nto explicitly reward aspects that moderately align with LLMs' normative priors\n(correlation = 0.49) but avoid articulating contextualization and storytelling\nposteriors in their review comments (correlation = -0.14), despite giving\nimplicit reward to them with positive scores. We discuss the broad\napplicability of the framework, leveraging LLMs as diagnostic tools to surface\nthe tacit codes underlying human society, enabling more precisely targeted\nresponsible AI."}
{"id": "2505.18901", "pdf": "https://arxiv.org/pdf/2505.18901", "abs": "https://arxiv.org/abs/2505.18901", "authors": ["Xiaoyan Hu", "Lauren Pick", "Ho-fung Leung", "Farzan Farnia"], "title": "PromptWise: Online Learning for Cost-Aware Prompt Assignment in Generative Models", "categories": ["cs.LG", "cs.AI"], "comment": "44 pages", "summary": "The rapid advancement of generative AI models has provided users with\nnumerous options to address their prompts. When selecting a generative AI model\nfor a given prompt, users should consider not only the performance of the\nchosen model but also its associated service cost. The principle guiding such\nconsideration is to select the least expensive model among the available\nsatisfactory options. However, existing model-selection approaches typically\nprioritize performance, overlooking pricing differences between models. In this\npaper, we introduce PromptWise, an online learning framework designed to assign\na sequence of prompts to a group of large language models (LLMs) in a\ncost-effective manner. PromptWise strategically queries cheaper models first,\nprogressing to more expensive options only if the lower-cost models fail to\nadequately address a given prompt. Through numerical experiments, we\ndemonstrate PromptWise's effectiveness across various tasks, including puzzles\nof varying complexity and code generation/translation tasks. The results\nhighlight that PromptWise consistently outperforms cost-unaware baseline\nmethods, emphasizing that directly assigning prompts to the most expensive\nmodels can lead to higher costs and potentially lower average performance."}
{"id": "2505.20241", "pdf": "https://arxiv.org/pdf/2505.20241", "abs": "https://arxiv.org/abs/2505.20241", "authors": ["Qi Cao", "Ruiyi Wang", "Ruiyi Zhang", "Sai Ashish Somayajula", "Pengtao Xie"], "title": "DreamPRM: Domain-Reweighted Process Reward Model for Multimodal Reasoning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reasoning has substantially improved the performance of large language models\n(LLMs) on complicated tasks. Central to the current reasoning studies, Process\nReward Models (PRMs) offer a fine-grained evaluation of intermediate reasoning\nsteps and guide the reasoning process. However, extending PRMs to multimodal\nlarge language models (MLLMs) introduces challenges. Since multimodal reasoning\ncovers a wider range of tasks compared to text-only scenarios, the resulting\ndistribution shift from the training to testing sets is more severe, leading to\ngreater generalization difficulty. Training a reliable multimodal PRM,\ntherefore, demands large and diverse datasets to ensure sufficient coverage.\nHowever, current multimodal reasoning datasets suffer from a marked quality\nimbalance, which degrades PRM performance and highlights the need for an\neffective data selection strategy. To address the issues, we introduce\nDreamPRM, a domain-reweighted training framework for multimodal PRMs which\nemploys bi-level optimization. In the lower-level optimization, DreamPRM\nperforms fine-tuning on multiple datasets with domain weights, allowing the PRM\nto prioritize high-quality reasoning signals and alleviating the impact of\ndataset quality imbalance. In the upper-level optimization, the PRM is\nevaluated on a separate meta-learning dataset; this feedback updates the domain\nweights through an aggregation loss function, thereby improving the\ngeneralization capability of trained PRM. Extensive experiments on multiple\nmultimodal reasoning benchmarks covering both mathematical and general\nreasoning show that test-time scaling with DreamPRM consistently improves the\nperformance of state-of-the-art MLLMs. Further comparisons reveal that\nDreamPRM's domain-reweighting strategy surpasses other data selection methods\nand yields higher accuracy gains than existing test-time scaling approaches."}
{"id": "2505.18985", "pdf": "https://arxiv.org/pdf/2505.18985", "abs": "https://arxiv.org/abs/2505.18985", "authors": ["Tianyu Zhang", "Xinyu Wang", "Zhenghan Tai", "Lu Li", "Jijun Chi", "Jingrui Tian", "Hailin He", "Suyuchen Wang"], "title": "STRICT: Stress Test of Rendering Images Containing Text", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "13 pages", "summary": "While diffusion models have revolutionized text-to-image generation with\ntheir ability to synthesize realistic and diverse scenes, they continue to\nstruggle to generate consistent and legible text within images. This\nshortcoming is commonly attributed to the locality bias inherent in\ndiffusion-based generation, which limits their ability to model long-range\nspatial dependencies. In this paper, we introduce $\\textbf{STRICT}$, a\nbenchmark designed to systematically stress-test the ability of diffusion\nmodels to render coherent and instruction-aligned text in images. Our benchmark\nevaluates models across multiple dimensions: (1) the maximum length of readable\ntext that can be generated; (2) the correctness and legibility of the generated\ntext, and (3) the ratio of not following instructions for generating text. We\nevaluate several state-of-the-art models, including proprietary and open-source\nvariants, and reveal persistent limitations in long-range consistency and\ninstruction-following capabilities. Our findings provide insights into\narchitectural bottlenecks and motivate future research directions in multimodal\ngenerative modeling. We release our entire evaluation pipeline at\nhttps://github.com/tianyu-z/STRICT-Bench."}
{"id": "2505.18912", "pdf": "https://arxiv.org/pdf/2505.18912", "abs": "https://arxiv.org/abs/2505.18912", "authors": ["Hamidreza Montazeri Hedesh", "Moh. Kamalul Wafi", "Bahram Shafai", "Milad Siami"], "title": "Robust Stability Analysis of Positive Lure System with Neural Network Feedback", "categories": ["eess.SY", "cs.AI", "cs.SY", "93D09, 93D20, 93C10, 68T07", "B.1.3; G.1; I.2; I.2.3; I.2.8; I.2.1; J.2"], "comment": "Accepted at the 9th IEEE Conference on Control Technology and\n  Applications (CCTA) 2025, San Diego, California", "summary": "This paper investigates the robustness of the Lur'e problem under positivity\nconstraints, drawing on results from the positive Aizerman conjecture and the\nrobustness properties of Metzler matrices. Specifically, we consider a control\nsystem of Lur'e type in which not only the linear part includes parametric\nuncertainty but also the nonlinear sector bound is unknown. We investigate\ntools from positive linear systems to effectively solve the problems in\ncomplicated and uncertain nonlinear systems. By leveraging the positivity\ncharacteristic of the system, we derive an explicit formula for the stability\nradius of Lur'e systems. Furthermore, we extend our analysis to systems with\nneural network (NN) feedback loops. Building on this approach, we also propose\na refinement method for sector bounds of feedforward neural networks (FFNNs).\nThis study introduces a scalable and efficient approach for robustness analysis\nof both Lur'e and NN-controlled systems. Finally, the proposed results are\nsupported by illustrative examples."}
{"id": "2505.20242", "pdf": "https://arxiv.org/pdf/2505.20242", "abs": "https://arxiv.org/abs/2505.20242", "authors": ["Nguyen Thach", "Aida Riahifar", "Nathan Huynh", "Hau Chan"], "title": "RedAHD: Reduction-Based End-to-End Automatic Heuristic Design with Large Language Models", "categories": ["cs.LG"], "comment": "Under Review", "summary": "Solving NP-hard combinatorial optimization problems (COPs) (e.g., traveling\nsalesman problems (TSPs) and capacitated vehicle routing problems (CVRPs)) in\npractice traditionally involves handcrafting heuristics or specifying a search\nspace for finding effective heuristics. The main challenges from these\napproaches, however, are the sheer amount of domain knowledge and\nimplementation efforts required from human experts. Recently, significant\nprogress has been made to address these challenges, particularly by using large\nlanguage models (LLMs) to design heuristics within some predetermined\ngeneralized algorithmic framework (GAF, e.g., ant colony optimization and\nguided local search) for building key functions/components (e.g., a priori\ninformation on how promising it is to include each edge in a solution for TSP\nand CVRP). Although existing methods leveraging this idea have shown to yield\nimpressive optimization performance, they are not fully end-to-end and still\nrequire considerable manual interventions. In this paper, we propose a novel\nend-to-end framework, named RedAHD, that enables these LLM-based heuristic\ndesign methods to operate without the need of GAFs. More specifically, RedAHD\nemploys LLMs to automate the process of reduction, i.e., transforming the COP\nat hand into similar COPs that are better-understood, from which LLM-based\nheuristic design methods can design effective heuristics for directly solving\nthe transformed COPs and, in turn, indirectly solving the original COP. Our\nexperimental results, evaluated on six COPs, show that RedAHD is capable of\ndesigning heuristics with competitive or improved results over the\nstate-of-the-art methods with minimal human involvement."}
{"id": "2505.19010", "pdf": "https://arxiv.org/pdf/2505.19010", "abs": "https://arxiv.org/abs/2505.19010", "authors": ["Md. Mithun Hossain", "Md. Shakil Hossain", "Sudipto Chaki", "M. F. Mridha"], "title": "Co-AttenDWG: Co-Attentive Dimension-Wise Gating and Expert Fusion for Multi-Modal Offensive Content Detection", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Multi-modal learning has become a critical research area because integrating\ntext and image data can significantly improve performance in tasks such as\nclassification, retrieval, and scene understanding. However, despite progress\nwith pre-trained models, current approaches are limited by inadequate\ncross-modal interactions and static fusion strategies that do not fully exploit\nthe complementary nature of different modalities. To address these\nshortcomings, we introduce a novel multi-modal Co-AttenDWG architecture that\nleverages dual-path encoding, co-attention with dimension-wise gating, and\nadvanced expert fusion. Our approach begins by projecting text and image\nfeatures into a common embedding space, where a dedicated co-attention\nmechanism enables simultaneous, fine-grained interactions between modalities.\nThis mechanism is further enhanced by a dimension-wise gating network that\nadaptively regulates the feature contributions at the channel level, ensuring\nthat only the most relevant information is emphasized. In parallel, dual-path\nencoders refine the representations by processing cross-modal information\nseparately before an additional cross-attention layer further aligns\nmodalities. The refined features are then aggregated via an expert fusion\nmodule that combines learned gating and self-attention to produce a robust,\nunified representation. We validate our approach on the MIMIC and SemEval\nMemotion 1.0, where experimental results demonstrate significant improvements\nin cross-modal alignment and state-of-the-art performance, underscoring the\npotential of our model for a wide range of multi-modal applications."}
{"id": "2505.18917", "pdf": "https://arxiv.org/pdf/2505.18917", "abs": "https://arxiv.org/abs/2505.18917", "authors": ["Zhepeng Cen", "Yihang Yao", "William Han", "Zuxin Liu", "Ding Zhao"], "title": "Behavior Injection: Preparing Language Models for Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement fine-tuning (RFT) has emerged as a powerful post-training\ntechnique to incentivize the reasoning ability of large language models (LLMs).\nHowever, LLMs can respond very inconsistently to RFT: some show substantial\nperformance gains, while others plateau or even degrade. To understand this\ndivergence, we analyze the per-step influence of the RL objective and identify\ntwo key conditions for effective post-training: (1) RL-informative rollout\naccuracy, and (2) strong data co-influence, which quantifies how much the\ntraining data affects performance on other samples. Guided by these insights,\nwe propose behavior injection, a task-agnostic data-augmentation scheme applied\nprior to RL. Behavior injection enriches the supervised finetuning (SFT) data\nby seeding exploratory and exploitative behaviors, effectively making the model\nmore RL-ready. We evaluate our method across two reasoning benchmarks with\nmultiple base models. The results demonstrate that our theoretically motivated\naugmentation can significantly increases the performance gain from RFT over the\npre-RL model."}
{"id": "2505.20251", "pdf": "https://arxiv.org/pdf/2505.20251", "abs": "https://arxiv.org/abs/2505.20251", "authors": ["Sophia Hager", "Aleem Khan", "Andrew Wang", "Nicholas Andrews"], "title": "Learning Extrapolative Sequence Transformations from Markov Chains", "categories": ["cs.LG", "cs.CL"], "comment": "To be published at the Forty-Second International Conference on\n  Machine Learning", "summary": "Most successful applications of deep learning involve similar training and\ntest conditions. However, tasks such as biological sequence design involve\nsearching for sequences that improve desirable properties beyond previously\nknown values, which requires novel hypotheses that \\emph{extrapolate} beyond\ntraining data. In these settings, extrapolation may be achieved by using random\nsearch methods such as Markov chain Monte Carlo (MCMC), which, given an initial\nstate, sample local transformations to approximate a target density that\nrewards states with the desired properties. However, even with a well-designed\nproposal, MCMC may struggle to explore large structured state spaces\nefficiently. Rather than relying on stochastic search, it would be desirable to\nhave a model that greedily optimizes the properties of interest, successfully\nextrapolating in as few steps as possible. We propose to learn such a model\nfrom the Markov chains resulting from MCMC search. Specifically, our approach\nuses selected states from Markov chains as a source of training data for an\nautoregressive model, which is then able to efficiently generate novel\nsequences that extrapolate along the sequence-level properties of interest. The\nproposed approach is validated on three problems: protein sequence design, text\nsentiment control, and text anonymization. We find that the autoregressive\nmodel can extrapolate as well or better than MCMC, but with the additional\nbenefits of scalability and significantly higher sample efficiency."}
{"id": "2505.19025", "pdf": "https://arxiv.org/pdf/2505.19025", "abs": "https://arxiv.org/abs/2505.19025", "authors": ["Mushtari Sadia", "Zhenning Yang", "Yunming Xiao", "Ang Chen", "Amrita Roy Chowdhury"], "title": "SQUiD: Synthesizing Relational Databases from Unstructured Text", "categories": ["cs.DB", "cs.CL"], "comment": null, "summary": "Relational databases are central to modern data management, yet most data\nexists in unstructured forms like text documents. To bridge this gap, we\nleverage large language models (LLMs) to automatically synthesize a relational\ndatabase by generating its schema and populating its tables from raw text. We\nintroduce SQUiD, a novel neurosymbolic framework that decomposes this task into\nfour stages, each with specialized techniques. Our experiments show that SQUiD\nconsistently outperforms baselines across diverse datasets."}
{"id": "2505.18927", "pdf": "https://arxiv.org/pdf/2505.18927", "abs": "https://arxiv.org/abs/2505.18927", "authors": ["Amel Muminovic"], "title": "Benchmarking Large Language Models for Cyberbullying Detection in Real-World YouTube Comments", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint. 9 pages, 3 tables, 1 figure. Not yet submitted to a\n  journal. Feedback welcome", "summary": "As online platforms grow, comment sections increasingly host harassment that\nundermines user experience and well-being. This study benchmarks three leading\nlarge language models, OpenAI GPT-4.1, Google Gemini 1.5 Pro, and Anthropic\nClaude 3 Opus, on a corpus of 5,080 YouTube comments sampled from high-abuse\nthreads in gaming, lifestyle, food vlog, and music channels. The dataset\ncomprises 1,334 harmful and 3,746 non-harmful messages in English, Arabic, and\nIndonesian, annotated independently by two reviewers with substantial agreement\n(Cohen's kappa = 0.83). Using a unified prompt and deterministic settings,\nGPT-4.1 achieved the best overall balance with an F1 score of 0.863, precision\nof 0.887, and recall of 0.841. Gemini flagged the highest share of harmful\nposts (recall = 0.875) but its precision fell to 0.767 due to frequent false\npositives. Claude delivered the highest precision at 0.920 and the lowest\nfalse-positive rate of 0.022, yet its recall dropped to 0.720. Qualitative\nanalysis showed that all three models struggle with sarcasm, coded insults, and\nmixed-language slang. These results underscore the need for moderation\npipelines that combine complementary models, incorporate conversational\ncontext, and fine-tune for under-represented languages and implicit abuse. A\nde-identified version of the dataset and full prompts is publicly released to\npromote reproducibility and further progress in automated content moderation."}
{"id": "2505.20254", "pdf": "https://arxiv.org/pdf/2505.20254", "abs": "https://arxiv.org/abs/2505.20254", "authors": ["Xiangchen Song", "Aashiq Muhamed", "Yujia Zheng", "Lingjing Kong", "Zeyu Tang", "Mona T. Diab", "Virginia Smith", "Kun Zhang"], "title": "Position: Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Sparse Autoencoders (SAEs) are a prominent tool in mechanistic\ninterpretability (MI) for decomposing neural network activations into\ninterpretable features. However, the aspiration to identify a canonical set of\nfeatures is challenged by the observed inconsistency of learned SAE features\nacross different training runs, undermining the reliability and efficiency of\nMI research. This position paper argues that mechanistic interpretability\nshould prioritize feature consistency in SAEs -- the reliable convergence to\nequivalent feature sets across independent runs. We propose using the Pairwise\nDictionary Mean Correlation Coefficient (PW-MCC) as a practical metric to\noperationalize consistency and demonstrate that high levels are achievable\n(0.80 for TopK SAEs on LLM activations) with appropriate architectural choices.\nOur contributions include detailing the benefits of prioritizing consistency;\nproviding theoretical grounding and synthetic validation using a model\norganism, which verifies PW-MCC as a reliable proxy for ground-truth recovery;\nand extending these findings to real-world LLM data, where high feature\nconsistency strongly correlates with the semantic similarity of learned feature\nexplanations. We call for a community-wide shift towards systematically\nmeasuring feature consistency to foster robust cumulative progress in MI."}
{"id": "2505.19037", "pdf": "https://arxiv.org/pdf/2505.19037", "abs": "https://arxiv.org/abs/2505.19037", "authors": ["Ke-Han Lu", "Chun-Yi Kuan", "Hung-yi Lee"], "title": "Speech-IFEval: Evaluating Instruction-Following and Quantifying Catastrophic Forgetting in Speech-Aware Language Models", "categories": ["eess.AS", "cs.CL"], "comment": "Accecpted by Interspeech 2025;\n  https://github.com/kehanlu/Speech-IFEval", "summary": "We introduce Speech-IFeval, an evaluation framework designed to assess\ninstruction-following capabilities and quantify catastrophic forgetting in\nspeech-aware language models (SLMs). Recent SLMs integrate speech perception\nwith large language models (LLMs), often degrading textual capabilities due to\nspeech-centric training. Existing benchmarks conflate speech perception with\ninstruction-following, hindering evaluation of these distinct skills. To\naddress this gap, we provide a benchmark for diagnosing the\ninstruction-following abilities of SLMs. Our findings show that most SLMs\nstruggle with even basic instructions, performing far worse than text-based\nLLMs. Additionally, these models are highly sensitive to prompt variations,\noften yielding inconsistent and unreliable outputs. We highlight core\nchallenges and provide insights to guide future research, emphasizing the need\nfor evaluation beyond task-level metrics."}
{"id": "2505.18930", "pdf": "https://arxiv.org/pdf/2505.18930", "abs": "https://arxiv.org/abs/2505.18930", "authors": ["Yanben Shen", "Timilehin T. Ayanlade", "Venkata Naresh Boddepalli", "Mojdeh Saadati", "Ashlyn Rairdin", "Zi K. Deng", "Muhammad Arbab Arshad", "Aditya Balu", "Daren Mueller", "Asheesh K Singh", "Wesley Everman", "Nirav Merchant", "Baskar Ganapathysubramanian", "Meaghan Anderson", "Soumik Sarkar", "Arti Singh"], "title": "WeedNet: A Foundation Model-Based Global-to-Local AI Approach for Real-Time Weed Species Identification and Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Early identification of weeds is essential for effective management and\ncontrol, and there is growing interest in automating the process using computer\nvision techniques coupled with AI methods. However, challenges associated with\ntraining AI-based weed identification models, such as limited expert-verified\ndata and complexity and variability in morphological features, have hindered\nprogress. To address these issues, we present WeedNet, the first global-scale\nweed identification model capable of recognizing an extensive set of weed\nspecies, including noxious and invasive plant species. WeedNet is an end-to-end\nreal-time weed identification pipeline and uses self-supervised learning,\nfine-tuning, and enhanced trustworthiness strategies. WeedNet achieved 91.02%\naccuracy across 1,593 weed species, with 41% species achieving 100% accuracy.\nUsing a fine-tuning strategy and a Global-to-Local approach, the local Iowa\nWeedNet model achieved an overall accuracy of 97.38% for 85 Iowa weeds, most\nclasses exceeded a 90% mean accuracy per class. Testing across intra-species\ndissimilarity (developmental stages) and inter-species similarity (look-alike\nspecies) suggests that diversity in the images collected, spanning all the\ngrowth stages and distinguishable plant characteristics, is crucial in driving\nmodel performance. The generalizability and adaptability of the Global WeedNet\nmodel enable it to function as a foundational model, with the Global-to-Local\nstrategy allowing fine-tuning for region-specific weed communities. Additional\nvalidation of drone- and ground-rover-based images highlights the potential of\nWeedNet for integration into robotic platforms. Furthermore, integration with\nAI for conversational use provides intelligent agricultural and ecological\nconservation consulting tools for farmers, agronomists, researchers, land\nmanagers, and government agencies across diverse landscapes."}
{"id": "2505.20268", "pdf": "https://arxiv.org/pdf/2505.20268", "abs": "https://arxiv.org/abs/2505.20268", "authors": ["Fan Chen", "Zeyu Jia", "Alexander Rakhlin", "Tengyang Xie"], "title": "Outcome-Based Online Reinforcement Learning: Algorithms and Fundamental Limits", "categories": ["cs.LG", "cs.AI", "math.ST", "stat.ML", "stat.TH"], "comment": null, "summary": "Reinforcement learning with outcome-based feedback faces a fundamental\nchallenge: when rewards are only observed at trajectory endpoints, how do we\nassign credit to the right actions? This paper provides the first comprehensive\nanalysis of this problem in online RL with general function approximation. We\ndevelop a provably sample-efficient algorithm achieving $\\widetilde{O}({C_{\\rm\ncov} H^3}/{\\epsilon^2})$ sample complexity, where $C_{\\rm cov}$ is the\ncoverability coefficient of the underlying MDP. By leveraging general function\napproximation, our approach works effectively in large or infinite state spaces\nwhere tabular methods fail, requiring only that value functions and reward\nfunctions can be represented by appropriate function classes. Our results also\ncharacterize when outcome-based feedback is statistically separated from\nper-step rewards, revealing an unavoidable exponential separation for certain\nMDPs. For deterministic MDPs, we show how to eliminate the completeness\nassumption, dramatically simplifying the algorithm. We further extend our\napproach to preference-based feedback settings, proving that equivalent\nstatistical efficiency can be achieved even under more limited information.\nTogether, these results constitute a theoretical foundation for understanding\nthe statistical properties of outcome-based reinforcement learning."}
{"id": "2505.19075", "pdf": "https://arxiv.org/pdf/2505.19075", "abs": "https://arxiv.org/abs/2505.19075", "authors": ["Jaemin Kim", "Hangeol Chang", "Hyunmin Hwang", "Choonghan Kim", "Jong Chul Ye"], "title": "Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for Frozen LLMs", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "22 pages", "summary": "Large Language Models (LLMs) have demonstrated remarkable general\ncapabilities, but enhancing skills such as reasoning often demands substantial\ncomputational resources and may compromise their generalization. While\nParameter-Efficient Fine-Tuning (PEFT) methods offer a more resource-conscious\nalternative, they typically requires retraining for each LLM backbone due to\narchitectural dependencies. To address these challenges, here we propose\nUniversal Reasoner (UniR) - a single, lightweight, composable, and\nplug-and-play reasoning module that can be used with any frozen LLM to endow it\nwith specialized reasoning capabilities. Specifically, UniR decomposes the\nreward into a standalone reasoning module that is trained independently using\npredefined rewards, effectively translating trajectory-level signals into\ntoken-level guidance. Once trained, UniR can be combined with any frozen LLM at\ninference time by simply adding its output logits to those of the LLM backbone.\nThis additive structure naturally enables modular composition: multiple UniR\nmodules trained for different tasks can be jointly applied by summing their\nlogits, enabling complex reasoning via composition. Experimental results on\nmathematical reasoning and machine translation tasks show that UniR\nsignificantly outperforms \\add{existing baseline fine-tuning methods using the\nLlama3.2 model}. Furthermore, UniR demonstrates strong weak-to-strong\ngeneralization: reasoning modules trained on smaller models effectively guide\nmuch larger LLMs. This makes UniR a cost-efficient, adaptable, and robust\nsolution for enhancing reasoning in LLMs without compromising their core\ncapabilities. Code is open-sourced at https://github.com/hangeol/UniR"}
{"id": "2505.18934", "pdf": "https://arxiv.org/pdf/2505.18934", "abs": "https://arxiv.org/abs/2505.18934", "authors": ["Xiping Li", "Xiangyu Dong", "Xingyi Zhang", "Kun Xie", "Yuanhao Feng", "Bo Wang", "Guilin Li", "Wuxiong Zeng", "Xiujun Shu", "Sibo Wang"], "title": "Chi-Square Wavelet Graph Neural Networks for Heterogeneous Graph Anomaly Detection", "categories": ["cs.LG", "cs.AI", "cs.IR", "cs.SI"], "comment": null, "summary": "Graph Anomaly Detection (GAD) in heterogeneous networks presents unique\nchallenges due to node and edge heterogeneity. Existing Graph Neural Network\n(GNN) methods primarily focus on homogeneous GAD and thus fail to address three\nkey issues: (C1) Capturing abnormal signal and rich semantics across diverse\nmeta-paths; (C2) Retaining high-frequency content in HIN dimension alignment;\nand (C3) Learning effectively from difficult anomaly samples with class\nimbalance. To overcome these, we propose ChiGAD, a spectral GNN framework based\non a novel Chi-Square filter, inspired by the wavelet effectiveness in diverse\ndomains. Specifically, ChiGAD consists of: (1) Multi-Graph Chi-Square Filter,\nwhich captures anomalous information via applying dedicated Chi-Square filters\nto each meta-path graph; (2) Interactive Meta-Graph Convolution, which aligns\nfeatures while preserving high-frequency information and incorporates\nheterogeneous messages by a unified Chi-Square Filter; and (3)\nContribution-Informed Cross-Entropy Loss, which prioritizes difficult anomalies\nto address class imbalance. Extensive experiments on public and industrial\ndatasets show that ChiGAD outperforms state-of-the-art models on multiple\nmetrics. Additionally, its homogeneous variant, ChiGNN, excels on seven GAD\ndatasets, validating the effectiveness of Chi-Square filters. Our code is\navailable at https://github.com/HsipingLi/ChiGAD."}
{"id": "2505.20274", "pdf": "https://arxiv.org/pdf/2505.20274", "abs": "https://arxiv.org/abs/2505.20274", "authors": ["Kejing Lu", "Chuan Xiao", "Yoshiharu Ishikawa"], "title": "Probabilistic Kernel Function for Fast Angle Testing", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DB", "cs.DS"], "comment": null, "summary": "In this paper, we study the angle testing problem in high-dimensional\nEuclidean spaces and propose two projection-based probabilistic kernel\nfunctions, one designed for angle comparison and the other for angle\nthresholding. Unlike existing approaches that rely on random projection vectors\ndrawn from Gaussian distributions, our approach leverages reference angles and\nemploys a deterministic structure for the projection vectors. Notably, our\nkernel functions do not require asymptotic assumptions, such as the number of\nprojection vectors tending to infinity, and can be both theoretically and\nexperimentally shown to outperform Gaussian-distribution-based kernel\nfunctions. We further apply the proposed kernel function to Approximate Nearest\nNeighbor Search (ANNS) and demonstrate that our approach achieves a 2.5X ~ 3X\nhigher query-per-second (QPS) throughput compared to the state-of-the-art\ngraph-based search algorithm HNSW."}
{"id": "2505.19155", "pdf": "https://arxiv.org/pdf/2505.19155", "abs": "https://arxiv.org/abs/2505.19155", "authors": ["Xuan Zhang", "Cunxiao Du", "Sicheng Yu", "Jiawei Wu", "Fengzhuo Zhang", "Wei Gao", "Qian Liu"], "title": "Sparse-to-Dense: A Free Lunch for Lossless Acceleration of Video Understanding in LLMs", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Due to the auto-regressive nature of current video large language models\n(Video-LLMs), the inference latency increases as the input sequence length\ngrows, posing challenges for the efficient processing of video sequences that\nare usually very long. We observe that during decoding, the attention scores of\nmost tokens in Video-LLMs tend to be sparse and concentrated, with only certain\ntokens requiring comprehensive full attention. Based on this insight, we\nintroduce Sparse-to-Dense (StD), a novel decoding strategy that integrates two\ndistinct modules: one leveraging sparse top-K attention and the other employing\ndense full attention. These modules collaborate to accelerate Video-LLMs\nwithout loss. The fast (sparse) model speculatively decodes multiple tokens,\nwhile the slow (dense) model verifies them in parallel. StD is a tuning-free,\nplug-and-play solution that achieves up to a 1.94$\\times$ walltime speedup in\nvideo processing. It maintains model performance while enabling a seamless\ntransition from a standard Video-LLM to a sparse Video-LLM with minimal code\nmodifications."}
{"id": "2505.18949", "pdf": "https://arxiv.org/pdf/2505.18949", "abs": "https://arxiv.org/abs/2505.18949", "authors": ["Longfei Yun", "Chenyang An", "Zilong Wang", "Letian Peng", "Jingbo Shang"], "title": "The Price of Format: Diversity Collapse in LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "14 pages, 7 figures", "summary": "Instruction-tuned large language models (LLMs) employ structured templates,\nsuch as role markers and special tokens, to enforce format consistency during\ninference. However, we identify a critical limitation of such formatting: it\ninduces a phenomenon we term diversity collapse, where the model generates\nsemantically similar outputs for open-ended inputs, undermining creativity and\nvariability. We systematically evaluate this effect across tasks like story\ncompletion and free-form generation, finding that (1) diversity collapse\npersists even under high-temperature sampling, and (2) structural tokens in\ntemplates significantly constrain the model's output space. To contextualize\nthese findings, we fine-tune the same model using a range of structured prompts\nand then evaluate them across three axes: downstream task performance,\nalignment behavior, and output diversity. Our analysis shows that format\nconsistency between fine-tuning and inference is crucial for\nstructure-sensitive tasks (e.g., GSM8K, IFEval), but has marginal influence on\nknowledge-heavy tasks (e.g., MMLU, WebQuestions). In contrast, output diversity\nis primarily governed by the presence or absence of structural tokens, with\nminimal formatting yielding the most diverse outputs. These findings reveal\nthat current prompting conventions, while beneficial for alignment, may\ninadvertently suppress output diversity, underscoring the need for\ndiversity-aware prompt design and instruction tuning."}
{"id": "2505.20278", "pdf": "https://arxiv.org/pdf/2505.20278", "abs": "https://arxiv.org/abs/2505.20278", "authors": ["Hoyeon Chang", "Jinho Park", "Hanseul Cho", "Sohee Yang", "Miyoung Ko", "Hyeonbin Hwang", "Seungpil Won", "Dohaeng Lee", "Youbin Ahn", "Minjoon Seo"], "title": "The Coverage Principle: A Framework for Understanding Compositional Generalization", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.6"], "comment": null, "summary": "Large language models excel at pattern matching, yet often fall short in\nsystematic compositional generalization. We propose the coverage principle: a\ndata-centric framework showing that models relying primarily on pattern\nmatching for compositional tasks cannot reliably generalize beyond substituting\nfragments that yield identical results when used in the same contexts. We\ndemonstrate that this framework has a strong predictive power for the\ngeneralization capabilities of Transformers. First, we derive and empirically\nconfirm that the training data required for two-hop generalization grows at\nleast quadratically with the token set size, and the training data efficiency\ndoes not improve with 20x parameter scaling. Second, for compositional tasks\nwith path ambiguity where one variable affects the output through multiple\ncomputational paths, we show that Transformers learn context-dependent state\nrepresentations that undermine both performance and interoperability. Third,\nChain-of-Thought supervision improves training data efficiency for multi-hop\ntasks but still struggles with path ambiguity. Finally, we outline a\n\\emph{mechanism-based} taxonomy that distinguishes three ways neural networks\ncan generalize: structure-based (bounded by coverage), property-based\n(leveraging algebraic invariances), and shared-operator (through function\nreuse). This conceptual lens contextualizes our results and highlights where\nnew architectural ideas are needed to achieve systematic compositionally.\nOverall, the coverage principle provides a unified lens for understanding\ncompositional reasoning, and underscores the need for fundamental architectural\nor training innovations to achieve truly systematic compositionality."}
{"id": "2505.19234", "pdf": "https://arxiv.org/pdf/2505.19234", "abs": "https://arxiv.org/abs/2505.19234", "authors": ["Jialong Zhou", "Lichao Wang", "Xiao Yang"], "title": "GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling", "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "The emergence of large language models (LLMs) enables the development of\nintelligent agents capable of engaging in complex and multi-turn dialogues.\nHowever, multi-agent collaboration face critical safety challenges, such as\nhallucination amplification and error injection and propagation. This paper\npresents GUARDIAN, a unified method for detecting and mitigating multiple\nsafety concerns in GUARDing Intelligent Agent collaboratioNs. By modeling the\nmulti-agent collaboration process as a discrete-time temporal attributed graph,\nGUARDIAN explicitly captures the propagation dynamics of hallucinations and\nerrors. The unsupervised encoder-decoder architecture incorporating an\nincremental training paradigm, learns to reconstruct node attributes and graph\nstructures from latent embeddings, enabling the identification of anomalous\nnodes and edges with unparalleled precision. Moreover, we introduce a graph\nabstraction mechanism based on the Information Bottleneck Theory, which\ncompresses temporal interaction graphs while preserving essential patterns.\nExtensive experiments demonstrate GUARDIAN's effectiveness in safeguarding LLM\nmulti-agent collaborations against diverse safety vulnerabilities, achieving\nstate-of-the-art accuracy with efficient resource utilization."}
{"id": "2505.18956", "pdf": "https://arxiv.org/pdf/2505.18956", "abs": "https://arxiv.org/abs/2505.18956", "authors": ["Yining Pan", "Qiongjie Cui", "Xulei Yang", "Na Zhao"], "title": "How Do Images Align and Complement LiDAR? Towards a Harmonized Multi-modal 3D Panoptic Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": "Accepted at the 2025 International Conference on Machine Learning\n  (ICML)", "summary": "LiDAR-based 3D panoptic segmentation often struggles with the inherent\nsparsity of data from LiDAR sensors, which makes it challenging to accurately\nrecognize distant or small objects. Recently, a few studies have sought to\novercome this challenge by integrating LiDAR inputs with camera images,\nleveraging the rich and dense texture information provided by the latter. While\nthese approaches have shown promising results, they still face challenges, such\nas misalignment during data augmentation and the reliance on post-processing\nsteps. To address these issues, we propose Image-Assists-LiDAR (IAL), a novel\nmulti-modal 3D panoptic segmentation framework. In IAL, we first introduce a\nmodality-synchronized data augmentation strategy, PieAug, to ensure alignment\nbetween LiDAR and image inputs from the start. Next, we adopt a transformer\ndecoder to directly predict panoptic segmentation results. To effectively fuse\nLiDAR and image features into tokens for the decoder, we design a\nGeometric-guided Token Fusion (GTF) module. Additionally, we leverage the\ncomplementary strengths of each modality as priors for query initialization\nthrough a Prior-based Query Generation (PQG) module, enhancing the decoder's\nability to generate accurate instance masks. Our IAL framework achieves\nstate-of-the-art performance compared to previous multi-modal 3D panoptic\nsegmentation methods on two widely used benchmarks. Code and models are\npublicly available at <https://github.com/IMPL-Lab/IAL.git>."}
{"id": "2505.13069", "pdf": "https://arxiv.org/pdf/2505.13069", "abs": "https://arxiv.org/abs/2505.13069", "authors": ["Ambre Marie", "Ilias Maoudj", "Guillaume Dardenne", "Gwenol Quellec"], "title": "Suicide Risk Assessment Using Multimodal Speech Features: A Study on the SW1 Challenge Dataset", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS", "I.2.7; I.5.1"], "comment": "Submitted to the SpeechWellness Challenge at Interspeech 2025; 5\n  pages, 2 figures, 2 tables", "summary": "The 1st SpeechWellness Challenge conveys the need for speech-based suicide\nrisk assessment in adolescents. This study investigates a multimodal approach\nfor this challenge, integrating automatic transcription with WhisperX,\nlinguistic embeddings from Chinese RoBERTa, and audio embeddings from WavLM.\nAdditionally, handcrafted acoustic features -- including MFCCs, spectral\ncontrast, and pitch-related statistics -- were incorporated. We explored three\nfusion strategies: early concatenation, modality-specific processing, and\nweighted attention with mixup regularization. Results show that weighted\nattention provided the best generalization, achieving 69% accuracy on the\ndevelopment set, though a performance gap between development and test sets\nhighlights generalization challenges. Our findings, strictly tied to the\nMINI-KID framework, emphasize the importance of refining embedding\nrepresentations and fusion mechanisms to enhance classification reliability."}
{"id": "2505.19277", "pdf": "https://arxiv.org/pdf/2505.19277", "abs": "https://arxiv.org/abs/2505.19277", "authors": ["Ibukun Olatunji", "Mark Sheppard"], "title": "Next Token Prediction Is a Dead End for Creativity", "categories": ["cs.AI", "cs.CL", "J.5; I.2.0; I.2.7"], "comment": "10 pages including references", "summary": "This paper argues that token prediction is fundamentally misaligned with real\ncreativity. While next-token models have enabled impressive advances in\nlanguage generation, their architecture favours surface-level coherence over\nspontaneity, originality, and improvisational risk. We use battle rap as a case\nstudy to expose the limitations of predictive systems, demonstrating that they\ncannot truly engage in adversarial or emotionally resonant exchanges. By\nreframing creativity as an interactive process rather than a predictive output,\nwe offer a vision for AI systems that are more expressive, responsive, and\naligned with human creative practice."}
{"id": "2505.18966", "pdf": "https://arxiv.org/pdf/2505.18966", "abs": "https://arxiv.org/abs/2505.18966", "authors": ["Nuowei Liu", "Jiahao Kuang", "Yanting Liu", "Changzhi Sun", "Tao Ji", "Yuanbin Wu", "Man Lan"], "title": "Protein Design with Dynamic Protein Vocabulary", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "comment": null, "summary": "Protein design is a fundamental challenge in biotechnology, aiming to design\nnovel sequences with specific functions within the vast space of possible\nproteins. Recent advances in deep generative models have enabled function-based\nprotein design from textual descriptions, yet struggle with structural\nplausibility. Inspired by classical protein design methods that leverage\nnatural protein structures, we explore whether incorporating fragments from\nnatural proteins can enhance foldability in generative models. Our empirical\nresults show that even random incorporation of fragments improves foldability.\nBuilding on this insight, we introduce ProDVa, a novel protein design approach\nthat integrates a text encoder for functional descriptions, a protein language\nmodel for designing proteins, and a fragment encoder to dynamically retrieve\nprotein fragments based on textual functional descriptions. Experimental\nresults demonstrate that our approach effectively designs protein sequences\nthat are both functionally aligned and structurally plausible. Compared to\nstate-of-the-art models, ProDVa achieves comparable function alignment using\nless than 0.04% of the training data, while designing significantly more\nwell-folded proteins, with the proportion of proteins having pLDDT above 70\nincreasing by 7.38% and those with PAE below 10 increasing by 9.6%."}
{"id": "2505.18159", "pdf": "https://arxiv.org/pdf/2505.18159", "abs": "https://arxiv.org/abs/2505.18159", "authors": ["Jesus Alvarez C", "Daua D. Karajeanes", "Ashley Celeste Prado", "John Ruttan", "Ivory Yang", "Sean O'Brien", "Vasu Sharma", "Kevin Zhu"], "title": "Advancing Uto-Aztecan Language Technologies: A Case Study on the Endangered Comanche Language", "categories": ["cs.CL", "cs.LG", "I.2.7; H.3.1"], "comment": "11 pages, 13 figures; published in Proceedings of the Fifth Workshop\n  on NLP for Indigenous Languages of the Americas (AmericasNLP 2025) at NAACL\n  2025, Albuquerque, NM", "summary": "The digital exclusion of endangered languages remains a critical challenge in\nNLP, limiting both linguistic research and revitalization efforts. This study\nintroduces the first computational investigation of Comanche, an Uto-Aztecan\nlanguage on the verge of extinction, demonstrating how minimal-cost,\ncommunity-informed NLP interventions can support language preservation. We\npresent a manually curated dataset of 412 phrases, a synthetic data generation\npipeline, and an empirical evaluation of GPT-4o and GPT-4o-mini for language\nidentification. Our experiments reveal that while LLMs struggle with Comanche\nin zero-shot settings, few-shot prompting significantly improves performance,\nachieving near-perfect accuracy with just five examples. Our findings highlight\nthe potential of targeted NLP methodologies in low-resource contexts and\nemphasize that visibility is the first step toward inclusion. By establishing a\nfoundation for Comanche in NLP, we advocate for computational approaches that\nprioritize accessibility, cultural sensitivity, and community engagement."}
{"id": "2505.19294", "pdf": "https://arxiv.org/pdf/2505.19294", "abs": "https://arxiv.org/abs/2505.19294", "authors": ["Ziyang Ma", "Xiquan Li", "Yakun Song", "Wenxi Chen", "Chenpeng Du", "Jian Wu", "Yuanzhe Chen", "Zhuo Chen", "Yuping Wang", "Yuxuan Wang", "Xie Chen"], "title": "Towards Reliable Large Audio Language Model", "categories": ["cs.SD", "cs.CL", "cs.HC", "cs.MM", "eess.AS"], "comment": "ACL 2025 Findings", "summary": "Recent advancements in large audio language models (LALMs) have demonstrated\nimpressive results and promising prospects in universal understanding and\nreasoning across speech, music, and general sound. However, these models still\nlack the ability to recognize their knowledge boundaries and refuse to answer\nquestions they don't know proactively. While there have been successful\nattempts to enhance the reliability of LLMs, reliable LALMs remain largely\nunexplored. In this paper, we systematically investigate various approaches\ntowards reliable LALMs, including training-free methods such as multi-modal\nchain-of-thought (MCoT), and training-based methods such as supervised\nfine-tuning (SFT). Besides, we identify the limitations of previous evaluation\nmetrics and propose a new metric, the Reliability Gain Index (RGI), to assess\nthe effectiveness of different reliable methods. Our findings suggest that both\ntraining-free and training-based methods enhance the reliability of LALMs to\ndifferent extents. Moreover, we find that awareness of reliability is a \"meta\nability\", which can be transferred across different audio modalities, although\nsignificant structural and content differences exist among sound, music, and\nspeech."}
{"id": "2505.18972", "pdf": "https://arxiv.org/pdf/2505.18972", "abs": "https://arxiv.org/abs/2505.18972", "authors": ["Minsu Kim", "Pingchuan Ma", "Honglie Chen", "Stavros Petridis", "Maja Pantic"], "title": "Revival with Voice: Multi-modal Controllable Text-to-Speech Synthesis", "categories": ["eess.AS", "cs.AI"], "comment": "Interspeech 2025", "summary": "This paper explores multi-modal controllable Text-to-Speech Synthesis (TTS)\nwhere the voice can be generated from face image, and the characteristics of\noutput speech (e.g., pace, noise level, distance, tone, place) can be\ncontrollable with natural text description. Specifically, we aim to mitigate\nthe following three challenges in face-driven TTS systems. 1) To overcome the\nlimited audio quality of audio-visual speech corpora, we propose a training\nmethod that additionally utilizes high-quality audio-only speech corpora. 2) To\ngenerate voices not only from real human faces but also from artistic\nportraits, we propose augmenting the input face image with stylization. 3) To\nconsider one-to-many possibilities in face-to-voice mapping and ensure\nconsistent voice generation at the same time, we propose to first employ\nsampling-based decoding and then use prompting with generated speech samples.\nExperimental results validate the proposed model's effectiveness in face-driven\nvoice synthesis."}
{"id": "2505.18162", "pdf": "https://arxiv.org/pdf/2505.18162", "abs": "https://arxiv.org/abs/2505.18162", "authors": ["Seon-Hwa Lee", "Insoo Ye", "Changhwan Lee", "Jieun Kim", "Geunho Choi", "Sang-Cheol Nam", "Inchul Park"], "title": "Accelerating Battery Material Optimization through iterative Machine Learning", "categories": ["eess.SP", "cs.LG"], "comment": "25 pages, 5 figures", "summary": "The performance of battery materials is determined by their composition and\nthe processing conditions employed during commercial-scale fabrication, where\nraw materials undergo complex processing steps with various additives to yield\nfinal products. As the complexity of these parameters expands with the\ndevelopment of industry, conventional one-factor-at-a-time (OFAT) experiment\nbecomes old fashioned. While domain expertise aids in parameter optimization,\nthis traditional approach becomes increasingly vulnerable to cognitive\nlimitations and anthropogenic biases as the complexity of factors grows.\nHerein, we introduce an iterative machine learning (ML) framework that\nintegrates active learning to guide targeted experimentation and facilitate\nincremental model refinement. This method systematically leverages\ncomprehensive experimental observations, including both successful and\nunsuccessful results, effectively mitigating human-induced biases and\nalleviating data scarcity. Consequently, it significantly accelerates\nexploration within the high-dimensional design space. Our results demonstrate\nthat active-learning-driven experimentation markedly reduces the total number\nof experimental cycles necessary, underscoring the transformative potential of\nML-based strategies in expediting battery material optimization."}
{"id": "2505.19302", "pdf": "https://arxiv.org/pdf/2505.19302", "abs": "https://arxiv.org/abs/2505.19302", "authors": ["Kapil Vaidya", "Abishek Sankararaman", "Jialin Ding", "Chuan Lei", "Xiao Qin", "Balakrishnan Narayanaswamy", "Tim Kraska"], "title": "ODIN: A NL2SQL Recommender to Handle Schema Ambiguity", "categories": ["cs.DB", "cs.CL"], "comment": null, "summary": "NL2SQL (natural language to SQL) systems translate natural language into SQL\nqueries, allowing users with no technical background to interact with databases\nand create tools like reports or visualizations. While recent advancements in\nlarge language models (LLMs) have significantly improved NL2SQL accuracy,\nschema ambiguity remains a major challenge in enterprise environments with\ncomplex schemas, where multiple tables and columns with semantically similar\nnames often co-exist. To address schema ambiguity, we introduce ODIN, a NL2SQL\nrecommendation engine. Instead of producing a single SQL query given a natural\nlanguage question, ODIN generates a set of potential SQL queries by accounting\nfor different interpretations of ambiguous schema components. ODIN dynamically\nadjusts the number of suggestions based on the level of ambiguity, and ODIN\nlearns from user feedback to personalize future SQL query recommendations. Our\nevaluation shows that ODIN improves the likelihood of generating the correct\nSQL query by 1.5-2$\\times$ compared to baselines."}
{"id": "2505.18975", "pdf": "https://arxiv.org/pdf/2505.18975", "abs": "https://arxiv.org/abs/2505.18975", "authors": ["Aotao Wang", "Haikuo Shao", "Shaobo Ma", "Zhongfeng Wang"], "title": "FastMamba: A High-Speed and Efficient Mamba Accelerator on FPGA with Accurate Quantization", "categories": ["cs.AR", "cs.AI"], "comment": null, "summary": "State Space Models (SSMs), like recent Mamba2, have achieved remarkable\nperformance and received extensive attention. However, deploying Mamba2 on\nresource-constrained edge devices encounters many problems: severe outliers\nwithin the linear layer challenging the quantization, diverse and irregular\nelement-wise tensor operations, and hardware-unfriendly nonlinear functions in\nthe SSM block. To address these issues, this paper presents FastMamba, a\ndedicated accelerator on FPGA with hardware-algorithm co-design to promote the\ndeployment efficiency of Mamba2. Specifically, we successfully achieve 8-bit\nquantization for linear layers through Hadamard transformation to eliminate\noutliers. Moreover, a hardware-friendly and fine-grained power-of-two\nquantization framework is presented for the SSM block and convolution layer,\nand a first-order linear approximation is developed to optimize the nonlinear\nfunctions. Based on the accurate algorithm quantization, we propose an\naccelerator that integrates parallel vector processing units, pipelined\nexecution dataflow, and an efficient SSM Nonlinear Approximation Unit, which\nenhances computational efficiency and reduces hardware complexity. Finally, we\nevaluate FastMamba on Xilinx VC709 FPGA. For the input prefill task on\nMamba2-130M, FastMamba achieves 68.80\\times and 8.90\\times speedup over Intel\nXeon 4210R CPU and NVIDIA RTX 3090 GPU, respectively. In the output decode\nexperiment with Mamba2-2.7B, FastMamba attains 6\\times higher energy efficiency\nthan RTX 3090 GPU."}
{"id": "2505.18167", "pdf": "https://arxiv.org/pdf/2505.18167", "abs": "https://arxiv.org/abs/2505.18167", "authors": ["Jie Li", "Jing Li", "Zhanyu Ju", "Fengkui Gong", "Lu Lv"], "title": "Dim and Small Target Detection for Drone Broadcast Frames Based on Time-Frequency Analysis", "categories": ["eess.SP", "cs.IT", "cs.LG", "math.IT"], "comment": null, "summary": "We propose a dim and small target detection algorithm for drone broadcast\nframes based on the time-frequency analysis of communication protocol.\nSpecifically, by analyzing modulation parameters and frame structures, the\nprior knowledge of transmission frequency, signal bandwidth, Zadoff-Chu (ZC)\nsequences, and frame length of drone broadcast frames is established. The RF\nsignals are processed through the designed filter banks, and the frequency\ndomain parameters of bounding boxes generated by the detector are corrected\nwith transmission frequency and signal bandwidth. Given the remarkable\ncorrelation characteristics of ZC sequences, the frequency domain parameters of\nbounding boxes with low confidence scores are corrected based on ZC sequences\nand frame length, which improves the detection accuracy of dim targets under\nlow signal-to noise ratio (SNR) situations. Besides, a segmented energy\nrefinement method is applied to mitigate the deviation caused by interference\nsignals with high energy strength, which ulteriorly corrects the time domain\ndetection parameters for dim targets. As the sampling duration increases, the\ndetection speed improves while the detection accuracy of broadcast frames\ntermed as small targets decreases. The trade-off between detection accuracy and\nspeed versus sampling duration is established, which helps to meet different\ndrone regulation requirements. Simulation results demonstrate that the proposed\nalgorithm improves the average intersection over union, precision, and recall\nby 3\\%, 1.4\\%, and 2.4\\%, respectively, compared to existing algorithms. The\nproposed algorithm also performs strong robustness under varying flight\ndistances, diverse types of environment noise, and different flight visual\nenvironment."}
{"id": "2505.19353", "pdf": "https://arxiv.org/pdf/2505.19353", "abs": "https://arxiv.org/abs/2505.19353", "authors": ["Camilo Chacn Sartori"], "title": "Architectures of Error: A Philosophical Inquiry into AI and Human Code Generation", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.SE"], "comment": "preprint", "summary": "With the rise of generative AI (GenAI), Large Language Models are\nincreasingly employed for code generation, becoming active co-authors alongside\nhuman programmers. Focusing specifically on this application domain, this paper\narticulates distinct ``Architectures of Error'' to ground an epistemic\ndistinction between human and machine code generation. Examined through their\nshared vulnerability to error, this distinction reveals fundamentally different\ncausal origins: human-cognitive versus artificial-stochastic. To develop this\nframework and substantiate the distinction, the analysis draws critically upon\nDennett's mechanistic functionalism and Rescher's methodological pragmatism. I\nargue that a systematic differentiation of these error profiles raises critical\nphilosophical questions concerning semantic coherence, security robustness,\nepistemic limits, and control mechanisms in human-AI collaborative software\ndevelopment. The paper also utilizes Floridi's levels of abstraction to provide\na nuanced understanding of how these error dimensions interact and may evolve\nwith technological advancements. This analysis aims to offer philosophers a\nstructured framework for understanding GenAI's unique epistemological\nchallenges, shaped by these architectural foundations, while also providing\nsoftware engineers a basis for more critically informed engagement."}
{"id": "2505.18976", "pdf": "https://arxiv.org/pdf/2505.18976", "abs": "https://arxiv.org/abs/2505.18976", "authors": ["Pingbang Hu", "Joseph Melkonian", "Weijing Tang", "Han Zhao", "Jiaqi W. Ma"], "title": "GraSS: Scalable Influence Function with Sparse Gradient Compression", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Gradient-based data attribution methods, such as influence functions, are\ncritical for understanding the impact of individual training samples without\nrequiring repeated model retraining. However, their scalability is often\nlimited by the high computational and memory costs associated with per-sample\ngradient computation. In this work, we propose GraSS, a novel gradient\ncompression algorithm and its variants FactGraSS for linear layers\nspecifically, that explicitly leverage the inherent sparsity of per-sample\ngradients to achieve sub-linear space and time complexity. Extensive\nexperiments demonstrate the effectiveness of our approach, achieving\nsubstantial speedups while preserving data influence fidelity. In particular,\nFactGraSS achieves up to 165% faster throughput on billion-scale models\ncompared to the previous state-of-the-art baselines. Our code is publicly\navailable at https://github.com/TRAIS-Lab/GraSS."}
{"id": "2505.18170", "pdf": "https://arxiv.org/pdf/2505.18170", "abs": "https://arxiv.org/abs/2505.18170", "authors": ["Aurausp Maneshni"], "title": "Load Forecasting in the Era of Smart Grids: Opportunities and Advanced Machine Learning Models", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "Electric energy is difficult to store, requiring stricter control over its\ngeneration, transmission, and distribution. A persistent challenge in power\nsystems is maintaining real-time equilibrium between electricity demand and\nsupply. Oversupply contributes to resource wastage, while undersupply can\nstrain the grid, increase operational costs, and potentially impact service\nreliability. To maintain grid stability, load forecasting is needed. Accurate\nload forecasting balances generation and demand by striving to predict future\nelectricity consumption. This thesis examines and evaluates four machine\nlearning frameworks for short term load forecasting, including gradient\nboosting decision tree methods such as Extreme Gradient Boosting (XGBoost) and\nLight Gradient Boosting Machine (LightGBM). A hybrid framework is also\ndeveloped. In addition, two recurrent neural network architectures, Long Short\nTerm Memory (LSTM) networks and Gated Recurrent Units (GRU), are designed and\nimplemented. Pearson Correlation Coefficient is applied to assess the\nrelationships between electricity demand and exogenous variables. The\nexperimental results show that, for the specific dataset and forecasting task\nin this study, machine learning-based models achieved improved forecasting\nperformance compared to a classical ARIMA baseline."}
{"id": "2505.19356", "pdf": "https://arxiv.org/pdf/2505.19356", "abs": "https://arxiv.org/abs/2505.19356", "authors": ["Kidist Amde Mekonnen", "Yosef Worku Alemneh", "Maarten de Rijke"], "title": "Optimized Text Embedding Models and Benchmarks for Amharic Passage Retrieval", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "68T50 (Primary), 68T05 (Secondary)", "H.3.3; H.3.1; I.2.7"], "comment": "10 pages (excluding references and appendix), 10 figures. Accepted to\n  ACL 2025 Findings. Public release includes dataset, code, and trained models:\n  https://github.com/kidist-amde/amharic-ir-benchmarks", "summary": "Neural retrieval methods using transformer-based pre-trained language models\nhave advanced multilingual and cross-lingual retrieval. However, their\neffectiveness for low-resource, morphologically rich languages such as Amharic\nremains underexplored due to data scarcity and suboptimal tokenization. We\naddress this gap by introducing Amharic-specific dense retrieval models based\non pre-trained Amharic BERT and RoBERTa backbones. Our proposed\nRoBERTa-Base-Amharic-Embed model (110M parameters) achieves a 17.6% relative\nimprovement in MRR@10 and a 9.86% gain in Recall@10 over the strongest\nmultilingual baseline, Arctic Embed 2.0 (568M parameters). More compact\nvariants, such as RoBERTa-Medium-Amharic-Embed (42M), remain competitive while\nbeing over 13x smaller. Additionally, we train a ColBERT-based late interaction\nretrieval model that achieves the highest MRR@10 score (0.843) among all\nevaluated models. We benchmark our proposed models against both sparse and\ndense retrieval baselines to systematically assess retrieval effectiveness in\nAmharic. Our analysis highlights key challenges in low-resource settings and\nunderscores the importance of language-specific adaptation. To foster future\nresearch in low-resource IR, we publicly release our dataset, codebase, and\ntrained models at https://github.com/kidist-amde/amharic-ir-benchmarks."}
{"id": "2505.18995", "pdf": "https://arxiv.org/pdf/2505.18995", "abs": "https://arxiv.org/abs/2505.18995", "authors": ["Carlos Jude G. Maminta", "Isaiah Job Enriquez", "Deandre Nigel Nunez", "Michael B. Dela Fuente"], "title": "FiLLM -- A Filipino-optimized Large Language Model based on Southeast Asia Large Language Model (SEALLM)", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study presents FiLLM, a Filipino-optimized large language model,\ndesigned to enhance natural language processing (NLP) capabilities in the\nFilipino language. Built upon the SeaLLM-7B 2.5 model, FiLLM leverages Low-Rank\nAdaptation (LoRA) fine-tuning to optimize memory efficiency while maintaining\ntask-specific performance. The model was trained and evaluated on diverse\nFilipino datasets to address key NLP tasks, including Named Entity Recognition\n(NER), Part-of-Speech (POS) tagging, Dependency Parsing, and Text\nSummarization. Performance comparisons with the CalamanCy model were conducted\nusing F1 Score, Precision, Recall, Compression Rate, and Keyword Overlap\nmetrics. Results indicate that Calamancy outperforms FILLM in several aspects,\ndemonstrating its effectiveness in processing Filipino text with improved\nlinguistic comprehension and adaptability. This research contributes to the\nadvancement of Filipino NLP applications by providing an optimized, efficient,\nand scalable language model tailored for local linguistic needs."}
{"id": "2505.18172", "pdf": "https://arxiv.org/pdf/2505.18172", "abs": "https://arxiv.org/abs/2505.18172", "authors": ["Sunil Kumar Jang Bahadur", "Gopala Dhar", "Lavi Nigam"], "title": "GenAI Security: Outsmarting the Bots with a Proactive Testing Framework", "categories": ["cs.CR", "cs.LG"], "comment": "IEEE CAI 2025", "summary": "The increasing sophistication and integration of Generative AI (GenAI) models\ninto diverse applications introduce new security challenges that traditional\nmethods struggle to address. This research explores the critical need for\nproactive security measures to mitigate the risks associated with malicious\nexploitation of GenAI systems. We present a framework encompassing key\napproaches, tools, and strategies designed to outmaneuver even advanced\nadversarial attacks, emphasizing the importance of securing GenAI innovation\nagainst potential liabilities. We also empirically prove the effectiveness of\nthe said framework by testing it against the SPML Chatbot Prompt Injection\nDataset. This work highlights the shift from reactive to proactive security\npractices essential for the safe and responsible deployment of GenAI\ntechnologies"}
{"id": "2505.19436", "pdf": "https://arxiv.org/pdf/2505.19436", "abs": "https://arxiv.org/abs/2505.19436", "authors": ["Ye Ye"], "title": "Task Memory Engine: Spatial Memory for Robust Multi-Step LLM Agents", "categories": ["cs.AI", "cs.CL", "I.2.6; I.2.8; H.3.3"], "comment": "Under review. 9 pages main content, 15 pages appendix, 5 figures", "summary": "Large Language Models (LLMs) falter in multi-step interactions -- often\nhallucinating, repeating actions, or misinterpreting user corrections -- due to\nreliance on linear, unstructured context. This fragility stems from the lack of\npersistent memory to track evolving goals and task dependencies, undermining\ntrust in autonomous agents. We introduce the Task Memory Engine (TME), a\nmodular memory controller that transforms existing LLMs into robust,\nrevision-aware agents without fine-tuning. TME implements a spatial memory\nframework that replaces flat context with graph-based structures to support\nconsistent, multi-turn reasoning. Departing from linear concatenation and\nReAct-style prompting, TME builds a dynamic task graph -- either a tree or\ndirected acyclic graph (DAG) -- to map user inputs to subtasks, align them with\nprior context, and enable dependency-tracked revisions. Its Task Representation\nand Intent Management (TRIM) component models task semantics and user intent to\nensure accurate interpretation. Across four multi-turn scenarios-trip planning,\ncooking, meeting scheduling, and shopping cart editing -- TME eliminates 100%\nof hallucinations and misinterpretations in three tasks, and reduces\nhallucinations by 66.7% and misinterpretations by 83.3% across 27 user turns,\noutperforming ReAct. TME's modular design supports plug-and-play deployment and\ndomain-specific customization, adaptable to both personal assistants and\nenterprise automation. We release TME's codebase, benchmarks, and components as\nopen-source resources, enabling researchers to develop reliable LLM agents.\nTME's scalable architecture addresses a critical gap in agent performance\nacross complex, interactive settings."}
{"id": "2505.19002", "pdf": "https://arxiv.org/pdf/2505.19002", "abs": "https://arxiv.org/abs/2505.19002", "authors": ["Jin Zhu", "Xin Zhou", "Jiaang Yao", "Gholamali Aminian", "Omar Rivasplata", "Simon Little", "Lexin Li", "Chengchun Shi"], "title": "Semi-pessimistic Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Offline reinforcement learning (RL) aims to learn an optimal policy from\npre-collected data. However, it faces challenges of distributional shift, where\nthe learned policy may encounter unseen scenarios not covered in the offline\ndata. Additionally, numerous applications suffer from a scarcity of labeled\nreward data. Relying on labeled data alone often leads to a narrow state-action\ndistribution, further amplifying the distributional shift, and resulting in\nsuboptimal policy learning. To address these issues, we first recognize that\nthe volume of unlabeled data is typically substantially larger than that of\nlabeled data. We then propose a semi-pessimistic RL method to effectively\nleverage abundant unlabeled data. Our approach offers several advantages. It\nconsiderably simplifies the learning process, as it seeks a lower bound of the\nreward function, rather than that of the Q-function or state transition\nfunction. It is highly flexible, and can be integrated with a range of\nmodel-free and model-based RL algorithms. It enjoys the guaranteed improvement\nwhen utilizing vast unlabeled data, but requires much less restrictive\nconditions. We compare our method with a number of alternative solutions, both\nanalytically and numerically, and demonstrate its clear competitiveness. We\nfurther illustrate with an application to adaptive deep brain stimulation for\nParkinson's disease."}
{"id": "2505.18174", "pdf": "https://arxiv.org/pdf/2505.18174", "abs": "https://arxiv.org/abs/2505.18174", "authors": ["Zhixin li", "Peihong Zhang", "Rui Sang", "Yuxuan Liu", "Shengchen Li"], "title": "NMCSE: Noise-Robust Multi-Modal Coupling Signal Estimation Method via Optimal Transport for Cardiovascular Disease Detection", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "Electrocardiogram (ECG) and Phonocardiogram (PCG) signals are linked by a\nlatent coupling signal representing the electrical-to-mechanical cardiac\ntransformation. While valuable for cardiovascular disease (CVD) detection, this\ncoupling signal is traditionally estimated using deconvolution methods that\namplify noise, limiting clinical utility. In this paper, we propose\nNoise-Robust Multi-Modal Coupling Signal Estimation (NMCSE), which reformulates\nthe problem as distribution matching via optimal transport theory. By jointly\noptimizing amplitude and temporal alignment, NMCSE mitigates noise\namplification without additional preprocessing. Integrated with our\nTemporal-Spatial Feature Extraction network, NMCSE enables robust multi-modal\nCVD detection. Experiments on the PhysioNet 2016 dataset with realistic\nhospital noise demonstrate that NMCSE reduces estimation errors by\napproximately 30% in Mean Squared Error while maintaining higher Pearson\nCorrelation Coefficients across all tested signal-to-noise ratios. Our approach\nachieves 97.38% accuracy and 0.98 AUC in CVD detection, outperforming\nstate-of-the-art methods and demonstrating robust performance for real-world\nclinical applications."}
{"id": "2505.19443", "pdf": "https://arxiv.org/pdf/2505.19443", "abs": "https://arxiv.org/abs/2505.19443", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee"], "title": "Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications of Agentic AI", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "35 Pages, 8 Figures, 6 Tables", "summary": "This review presents a comprehensive analysis of two emerging paradigms in\nAI-assisted software development: vibe coding and agentic coding. While both\nleverage large language models (LLMs), they differ fundamentally in autonomy,\narchitectural design, and the role of the developer. Vibe coding emphasizes\nintuitive, human-in-the-loop interaction through prompt-based, conversational\nworkflows that support ideation, experimentation, and creative exploration. In\ncontrast, agentic coding enables autonomous software development through\ngoal-driven agents capable of planning, executing, testing, and iterating tasks\nwith minimal human intervention. We propose a detailed taxonomy spanning\nconceptual foundations, execution models, feedback loops, safety mechanisms,\ndebugging strategies, and real-world tool ecosystems. Through comparative\nworkflow analysis and 20 detailed use cases, we illustrate how vibe systems\nthrive in early-stage prototyping and education, while agentic systems excel in\nenterprise-grade automation, codebase refactoring, and CI/CD integration. We\nfurther examine emerging trends in hybrid architectures, where natural language\ninterfaces are coupled with autonomous execution pipelines. Finally, we\narticulate a future roadmap for agentic AI, outlining the infrastructure needed\nfor trustworthy, explainable, and collaborative systems. Our findings suggest\nthat successful AI software engineering will rely not on choosing one paradigm,\nbut on harmonizing their strengths within a unified, human-centered development\nlifecycle."}
{"id": "2505.19013", "pdf": "https://arxiv.org/pdf/2505.19013", "abs": "https://arxiv.org/abs/2505.19013", "authors": ["Kiljae Lee", "Ziqi Liu", "Weijing Tang", "Yuan Zhang"], "title": "Faithful Group Shapley Value", "categories": ["cs.LG", "cs.AI", "econ.GN", "q-fin.EC", "stat.ML"], "comment": null, "summary": "Data Shapley is an important tool for data valuation, which quantifies the\ncontribution of individual data points to machine learning models. In practice,\ngroup-level data valuation is desirable when data providers contribute data in\nbatch. However, we identify that existing group-level extensions of Data\nShapley are vulnerable to shell company attacks, where strategic group\nsplitting can unfairly inflate valuations. We propose Faithful Group Shapley\nValue (FGSV) that uniquely defends against such attacks. Building on original\nmathematical insights, we develop a provably fast and accurate approximation\nalgorithm for computing FGSV. Empirical experiments demonstrate that our\nalgorithm significantly outperforms state-of-the-art methods in computational\nefficiency and approximation accuracy, while ensuring faithful group-level\nvaluation."}
{"id": "2505.18175", "pdf": "https://arxiv.org/pdf/2505.18175", "abs": "https://arxiv.org/abs/2505.18175", "authors": ["Natia Kukhilava", "Tatia Tsmindashvili", "Rapael Kalandadze", "Anchit Gupta", "Sofio Katamadze", "Franois Brmond", "Laura M. Ferrari", "Philipp Mller", "Benedikt Emanuel Wirth"], "title": "Evaluation in EEG Emotion Recognition: State-of-the-Art Review and Unified Framework", "categories": ["eess.SP", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Electroencephalography-based Emotion Recognition (EEG-ER) has become a\ngrowing research area in recent years. Analyzing 216 papers published between\n2018 and 2023, we uncover that the field lacks a unified evaluation protocol,\nwhich is essential to fairly define the state of the art, compare new\napproaches and to track the field's progress. We report the main\ninconsistencies between the used evaluation protocols, which are related to\nground truth definition, evaluation metric selection, data splitting types\n(e.g., subject-dependent or subject-independent) and the use of different\ndatasets. Capitalizing on this state-of-the-art research, we propose a unified\nevaluation protocol, EEGain (https://github.com/EmotionLab/EEGain), which\nenables an easy and efficient evaluation of new methods and datasets. EEGain is\na novel open source software framework, offering the capability to compare -\nand thus define - state-of-the-art results. EEGain includes standardized\nmethods for data pre-processing, data splitting, evaluation metrics, and the\nability to load the six most relevant datasets (i.e., AMIGOS, DEAP, DREAMER,\nMAHNOB-HCI, SEED, SEED-IV) in EEG-ER with only a single line of code. In\naddition, we have assessed and validated EEGain using these six datasets on the\nfour most common publicly available methods (EEGNet, DeepConvNet,\nShallowConvNet, TSception). This is a significant step to make research on\nEEG-ER more reproducible and comparable, thereby accelerating the overall\nprogress of the field."}
{"id": "2505.19457", "pdf": "https://arxiv.org/pdf/2505.19457", "abs": "https://arxiv.org/abs/2505.19457", "authors": ["Guilong Lu", "Xuntao Guo", "Rongjunchen Zhang", "Wenqiao Zhu", "Ji Liu"], "title": "BizFinBench: A Business-Driven Real-World Financial Benchmark for Evaluating LLMs", "categories": ["cs.AI", "cs.CE", "cs.CL"], "comment": "Project Page: https://hithink-research.github.io/BizFinBench/", "summary": "Large language models excel in general tasks, yet assessing their reliability\nin logic-heavy, precision-critical domains like finance, law, and healthcare\nremains challenging. To address this, we introduce BizFinBench, the first\nbenchmark specifically designed to evaluate LLMs in real-world financial\napplications. BizFinBench consists of 6,781 well-annotated queries in Chinese,\nspanning five dimensions: numerical calculation, reasoning, information\nextraction, prediction recognition, and knowledge-based question answering,\ngrouped into nine fine-grained categories. The benchmark includes both\nobjective and subjective metrics. We also introduce IteraJudge, a novel LLM\nevaluation method that reduces bias when LLMs serve as evaluators in objective\nmetrics. We benchmark 25 models, including both proprietary and open-source\nsystems. Extensive experiments show that no model dominates across all tasks.\nOur evaluation reveals distinct capability patterns: (1) In Numerical\nCalculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while\nsmaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning,\nproprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with\nopen-source models trailing by up to 19.49 points; (3) In Information\nExtraction, the performance spread is the largest, with DeepSeek-R1 scoring\n71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition,\nperformance variance is minimal, with top models scoring between 39.16 and\n50.00. We find that while current LLMs handle routine finance queries\ncompetently, they struggle with complex scenarios requiring cross-concept\nreasoning. BizFinBench offers a rigorous, business-aligned benchmark for future\nresearch. The code and dataset are available at\nhttps://github.com/HiThink-Research/BizFinBench."}
{"id": "2505.19020", "pdf": "https://arxiv.org/pdf/2505.19020", "abs": "https://arxiv.org/abs/2505.19020", "authors": ["Jiawei Xue", "Zhen Yang", "Haitao Lin", "Ziji Zhang", "Luzhu Wang", "Yikun Gu", "Yao Xu", "Xin Li"], "title": "HGCL: Hierarchical Graph Contrastive Learning for User-Item Recommendation", "categories": ["cs.IR", "cs.AI"], "comment": "10 pages, 5 figures", "summary": "Graph Contrastive Learning (GCL), which fuses graph neural networks with\ncontrastive learning, has evolved as a pivotal tool in user-item\nrecommendations. While promising, existing GCL methods often lack explicit\nmodeling of hierarchical item structures, which represent item similarities\nacross varying resolutions. Such hierarchical item structures are ubiquitous in\nvarious items (e.g., online products and local businesses), and reflect their\ninherent organizational properties that serve as critical signals for enhancing\nrecommendation accuracy. In this paper, we propose Hierarchical Graph\nContrastive Learning (HGCL), a novel GCL method that incorporates hierarchical\nitem structures for user-item recommendations. First, HGCL pre-trains a GCL\nmodule using cross-layer contrastive learning to obtain user and item\nrepresentations. Second, HGCL employs a representation compression and\nclustering method to construct a two-hierarchy user-item bipartite graph.\nUltimately, HGCL fine-tunes user and item representations by learning on the\nhierarchical graph, and then provides recommendations based on user-item\ninteraction scores. Experiments on three widely adopted benchmark datasets\nranging from 70K to 382K nodes confirm the superior performance of HGCL over\nexisting baseline models, highlighting the contribution of hierarchical item\nstructures in enhancing GCL methods for recommendation tasks."}
{"id": "2505.18180", "pdf": "https://arxiv.org/pdf/2505.18180", "abs": "https://arxiv.org/abs/2505.18180", "authors": ["Vu Thi Huong", "Thorsten Koch"], "title": "Clustering scientific publications: lessons learned through experiments with a real citation network", "categories": ["cs.DL", "cs.LG"], "comment": null, "summary": "Clustering scientific publications can reveal underlying research structures\nwithin bibliographic databases. Graph-based clustering methods, such as\nspectral, Louvain, and Leiden algorithms, are frequently utilized due to their\ncapacity to effectively model citation networks. However, their performance may\ndegrade when applied to real-world data. This study evaluates the performance\nof these clustering algorithms on a citation graph comprising approx. 700,000\npapers and 4.6 million citations extracted from Web of Science. The results\nshow that while scalable methods like Louvain and Leiden perform efficiently,\ntheir default settings often yield poor partitioning. Meaningful outcomes\nrequire careful parameter tuning, especially for large networks with uneven\nstructures, including a dense core and loosely connected papers. These findings\nhighlight practical lessons about the challenges of large-scale data, method\nselection and tuning based on specific structures of bibliometric clustering\ntasks."}
{"id": "2505.19504", "pdf": "https://arxiv.org/pdf/2505.19504", "abs": "https://arxiv.org/abs/2505.19504", "authors": ["Pingzhi Li", "Zhen Tan", "Huaizhi Qu", "Huan Liu", "Tianlong Chen"], "title": "DOGe: Defensive Output Generation for LLM Protection Against Knowledge Distillation", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Code is available at https://github.com/UNITES-Lab/DOGe", "summary": "Large Language Models (LLMs) represent substantial intellectual and economic\ninvestments, yet their effectiveness can inadvertently facilitate model\nimitation via knowledge distillation (KD).In practical scenarios, competitors\ncan distill proprietary LLM capabilities by simply observing publicly\naccessible outputs, akin to reverse-engineering a complex performance by\nobservation alone. Existing protective methods like watermarking only identify\nimitation post-hoc, while other defenses assume the student model mimics the\nteacher's internal logits, rendering them ineffective against distillation\npurely from observed output text. This paper confronts the challenge of\nactively protecting LLMs within the realistic constraints of API-based access.\nWe introduce an effective and efficient Defensive Output Generation (DOGe)\nstrategy that subtly modifies the output behavior of an LLM. Its outputs remain\naccurate and useful for legitimate users, yet are designed to be misleading for\ndistillation, significantly undermining imitation attempts. We achieve this by\nfine-tuning only the final linear layer of the teacher LLM with an adversarial\nloss. This targeted training approach anticipates and disrupts distillation\nattempts during inference time. Our experiments show that, while preserving or\neven improving the original performance of the teacher model, student models\ndistilled from the defensively generated teacher outputs demonstrate\ncatastrophically reduced performance, demonstrating our method's effectiveness\nas a practical safeguard against KD-based model imitation."}
{"id": "2505.19022", "pdf": "https://arxiv.org/pdf/2505.19022", "abs": "https://arxiv.org/abs/2505.19022", "authors": ["Zihao Liu", "Xiaoyu Wu", "Wenna Li", "Linlin Yang"], "title": "Rethinking Metrics and Benchmarks of Video Anomaly Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video Anomaly Detection (VAD), which aims to detect anomalies that deviate\nfrom expectation, has attracted increasing attention in recent years. Existing\nadvancements in VAD primarily focus on model architectures and training\nstrategies, while devoting insufficient attention to evaluation metrics and\nbenchmarks. In this paper, we rethink VAD evaluation protocols through\ncomprehensive experimental analyses, revealing three critical limitations in\ncurrent practices: 1) existing metrics are significantly influenced by single\nannotation bias; 2) current metrics fail to reward early detection of\nanomalies; 3) available benchmarks lack the capability to evaluate scene\noverfitting. To address these limitations, we propose three novel evaluation\nmethods: first, we establish averaged AUC/AP metrics over multi-round\nannotations to mitigate single annotation bias; second, we develop a\nLatency-aware Average Precision (LaAP) metric that rewards early and accurate\nanomaly detection; and finally, we introduce two hard normal benchmarks\n(UCF-HN, MSAD-HN) with videos specifically designed to evaluate scene\noverfitting. We report performance comparisons of ten state-of-the-art VAD\napproaches using our proposed evaluation methods, providing novel perspectives\nfor future VAD model development."}
{"id": "2505.18182", "pdf": "https://arxiv.org/pdf/2505.18182", "abs": "https://arxiv.org/abs/2505.18182", "authors": ["Damilare Emmanuel Olatunji", "Julius Dona Zannu", "Carine Pierrette Mukamakuza", "Godbright Nixon Uiso", "Mona Mamoun Mubarak Aman", "John Bosco Thuo", "Chol Buol", "Nchofon Tagha Ghogomu", "Evelyne Umubyeyi"], "title": "Machine Learning-Based Analysis of ECG and PCG Signals for Rheumatic Heart Disease Detection: A Scoping Review (2015-2025)", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "Objective: To conduct a systematic assessment of machine learning\napplications that utilize electrocardiogram (ECG) and heart sound data in the\ndevelopment of cost-effective detection tools for rheumatic heart disease (RHD)\nfrom the year 2015 to 2025, thereby supporting the World Heart Federation's \"25\nby 25\" mortality reduction objective through the creation of alternatives to\nechocardiography in underserved regions. Methods: Following PRISMA-ScR\nguidelines, we conducted a comprehensive search across PubMed, IEEE Xplore,\nScopus, and Embase for peer-reviewed literature focusing on ML-based ECG/PCG\nanalysis for RHD detection. Two independent reviewers screened studies, and\ndata extraction focused on methodology, validation approaches, and performance\nmetrics. Results: Analysis of 37 relevant studies revealed that convolutional\nneural networks (CNNs) have become the predominant technology in post-2020\nimplementations, achieving a median accuracy of 93.7%. However, 73% of studies\nrelied on single-center datasets, only 10.8% incorporated external validation,\nand none addressed cost-effectiveness. Performance varied markedly across\ndifferent valvular lesions, and despite 44% of studies originating from endemic\nregions, significant gaps persisted in implementation science and demographic\ndiversity. Conclusion: While ML-based ECG/PCG analysis shows promise for RHD\ndetection, substantial methodological limitations hinder clinical translation.\nFuture research must prioritize standardized benchmarking frameworks,\nmultimodal architectures, cost-effectiveness assessments, and prospective\ntrials in endemic settings. Significance: This review provides a critical\nroadmap for developing accessible ML-based RHD screening tools to help bridge\nthe diagnostic gap in resourceconstrained settings where conventional\nauscultation misses up to 90% of cases and echocardiography remains\ninaccessible."}
{"id": "2505.19536", "pdf": "https://arxiv.org/pdf/2505.19536", "abs": "https://arxiv.org/abs/2505.19536", "authors": ["Jintao Tong", "Wenwei Jin", "Pengda Qin", "Anqi Li", "Yixiong Zou", "Yuhong Li", "Yuhua Li", "Ruixuan Li"], "title": "FlowCut: Rethinking Redundancy via Information Flow for Efficient Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "19 pages, 11 figures", "summary": "Large vision-language models (LVLMs) excel at multimodal understanding but\nsuffer from high computational costs due to redundant vision tokens. Existing\npruning methods typically rely on single-layer attention scores to rank and\nprune redundant visual tokens to solve this inefficiency. However, as the\ninteraction between tokens and layers is complicated, this raises a basic\nquestion: Is such a simple single-layer criterion sufficient to identify\nredundancy? To answer this question, we rethink the emergence of redundant\nvisual tokens from a fundamental perspective: information flow, which models\nthe interaction between tokens and layers by capturing how information moves\nbetween tokens across layers. We find (1) the CLS token acts as an information\nrelay, which can simplify the complicated flow analysis; (2) the redundancy\nemerges progressively and dynamically via layer-wise attention concentration;\nand (3) relying solely on attention scores from single layers can lead to\ncontradictory redundancy identification. Based on this, we propose FlowCut, an\ninformation-flow-aware pruning framework, mitigating the insufficiency of the\ncurrent criterion for identifying redundant tokens and better aligning with the\nmodel's inherent behaviors. Extensive experiments show that FlowCut achieves\nsuperior results, outperforming SoTA by 1.6% on LLaVA-1.5-7B with 88.9% token\nreduction, and by 4.3% on LLaVA-NeXT-7B with 94.4% reduction, delivering 3.2x\nspeed-up in the prefilling stage. Our code is available at\nhttps://github.com/TungChintao/FlowCut"}
{"id": "2505.19023", "pdf": "https://arxiv.org/pdf/2505.19023", "abs": "https://arxiv.org/abs/2505.19023", "authors": ["Huda Alghoraibi", "Nuha Alqurashi", "Sarah Alotaibi", "Renad Alkhudaydi", "Bdoor Aldajani", "Lubna Alqurashi", "Jood Batweel", "Maha A. Thafar"], "title": "A Smart Healthcare System for Monkeypox Skin Lesion Detection and Tracking", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.LG"], "comment": "23 pages, 5 figures", "summary": "Monkeypox is a viral disease characterized by distinctive skin lesions and\nhas been reported in many countries. The recent global outbreak has emphasized\nthe urgent need for scalable, accessible, and accurate diagnostic solutions to\nsupport public health responses.\n  In this study, we developed ITMAINN, an intelligent, AI-driven healthcare\nsystem specifically designed to detect Monkeypox from skin lesion images using\nadvanced deep learning techniques. Our system consists of three main\ncomponents. First, we trained and evaluated several pretrained models using\ntransfer learning on publicly available skin lesion datasets to identify the\nmost effective models. For binary classification (Monkeypox vs. non-Monkeypox),\nthe Vision Transformer, MobileViT, Transformer-in-Transformer, and VGG16\nachieved the highest performance, each with an accuracy and F1-score of 97.8%.\nFor multiclass classification, which contains images of patients with Monkeypox\nand five other classes (chickenpox, measles, hand-foot-mouth disease, cowpox,\nand healthy), ResNetViT and ViT Hybrid models achieved 92% accuracy, with F1\nscores of 92.24% and 92.19%, respectively. The best-performing and most\nlightweight model, MobileViT, was deployed within the mobile application. The\nsecond component is a cross-platform smartphone application that enables users\nto detect Monkeypox through image analysis, track symptoms, and receive\nrecommendations for nearby healthcare centers based on their location. The\nthird component is a real-time monitoring dashboard designed for health\nauthorities to support them in tracking cases, analyzing symptom trends,\nguiding public health interventions, and taking proactive measures.\n  This system is fundamental in developing responsive healthcare infrastructure\nwithin smart cities. Our solution, ITMAINN, is part of revolutionizing public\nhealth management."}
{"id": "2505.18183", "pdf": "https://arxiv.org/pdf/2505.18183", "abs": "https://arxiv.org/abs/2505.18183", "authors": ["Nisal Ranasinghe", "Dzung Do-Ha", "Simon Maksour", "Tamasha Malepathirana", "Sachith Seneviratne", "Lezanne Ooi", "Saman Halgamuge"], "title": "FRAME-C: A knowledge-augmented deep learning pipeline for classifying multi-electrode array electrophysiological signals", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "Amyotrophic lateral sclerosis (ALS) is a fatal neurodegenerative disorder\ncharacterized by motor neuron degeneration, with alterations in neural\nexcitability serving as key indicators. Recent advancements in induced\npluripotent stem cell (iPSC) technology have enabled the generation of human\niPSC-derived neuronal cultures, which, when combined with multi-electrode array\n(MEA) electrophysiology, provide rich spatial and temporal electrophysiological\ndata. Traditionally, MEA data is analyzed using handcrafted features based on\npotentially imperfect domain knowledge, which while useful may not fully\ncapture all useful characteristics inherent in the data. Machine learning,\nparticularly deep learning, has the potential to automatically learn relevant\ncharacteristics from raw data without solely relying on handcrafted feature\nextraction. However, handcrafted features remain critical for encoding domain\nknowledge and improving interpretability, especially with limited or noisy\ndata. This study introduces FRAME-C, a knowledge-augmented machine learning\npipeline that combines domain knowledge, raw spike waveform data, and deep\nlearning techniques to classify MEA signals and identify ALS-specific\nphenotypes. FRAME-C leverages deep learning to learn important features from\nspike waveforms while incorporating handcrafted features such as spike\namplitude, inter-spike interval, and spike duration, preserving key spatial and\ntemporal information. We validate FRAME-C on both simulated and real MEA data\nfrom human iPSC-derived neuronal cultures, demonstrating superior performance\nover existing classification methods. FRAME-C shows over 11% improvement on\nreal data and up to 25% on simulated data. We also show FRAME-C can evaluate\nhandcrafted feature importance, providing insights into ALS phenotypes."}
{"id": "2505.19563", "pdf": "https://arxiv.org/pdf/2505.19563", "abs": "https://arxiv.org/abs/2505.19563", "authors": ["Shi-Yu Tian", "Zhi Zhou", "Wei Dong", "Ming Yang", "Kun-Yang Yu", "Zi-Jian Cheng", "Lan-Zhe Guo", "Yu-Feng Li"], "title": "Automated Text-to-Table for Reasoning-Intensive Table QA: Pipeline Design and Benchmarking Insights", "categories": ["cs.AI", "cs.CL"], "comment": "Paper under review, code and dataset are all available", "summary": "Reasoning with tabular data holds increasing importance in modern\napplications, yet comprehensive evaluation methodologies for\nreasoning-intensive Table Question Answering (QA) tasks remain nascent.\nExisting research is constrained by two primary bottlenecks: 1) Reliance on\ncostly manually annotated real-world data, which is difficult to cover complex\nreasoning scenarios; 2) The heterogeneity of table structures hinders\nsystematic analysis of the intrinsic mechanisms behind the underperformance of\nLLMs, especially in reasoning-intensive tasks. To address these issues, we\npropose an automated generation pipeline AutoT2T that transforms mathematical\nword problems into table-based reasoning tasks, eliminating the need for manual\nannotation. The pipeline can generate multiple variants of a table for the same\nreasoning problem, including noisy versions to support robustness evaluation.\nBased on this, we construct a new benchmark TabularGSM, which systematically\nspans a range of table complexities and trap problems. Experimental analyses\nthrough AutoT2T and TabularGSM reveal that the tight coupling between reasoning\nand retrieval or identification processes is a key factor underlying the\nfailure of LLMs in complex Table QA tasks. This highlights the necessity for\nmodels to develop synergistic reasoning capabilities in order to perform\neffectively in complex Table QA tasks."}
{"id": "2505.19028", "pdf": "https://arxiv.org/pdf/2505.19028", "abs": "https://arxiv.org/abs/2505.19028", "authors": ["Minzhi Lin", "Tianchi Xie", "Mengchen Liu", "Yilin Ye", "Changjian Chen", "Shixia Liu"], "title": "InfoChartQA: A Benchmark for Multimodal Question Answering on Infographic Charts", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Understanding infographic charts with design-driven visual elements (e.g.,\npictograms, icons) requires both visual recognition and reasoning, posing\nchallenges for multimodal large language models (MLLMs). However, existing\nvisual-question answering benchmarks fall short in evaluating these\ncapabilities of MLLMs due to the lack of paired plain charts and\nvisual-element-based questions. To bridge this gap, we introduce InfoChartQA, a\nbenchmark for evaluating MLLMs on infographic chart understanding. It includes\n5,642 pairs of infographic and plain charts, each sharing the same underlying\ndata but differing in visual presentations. We further design\nvisual-element-based questions to capture their unique visual designs and\ncommunicative intent. Evaluation of 20 MLLMs reveals a substantial performance\ndecline on infographic charts, particularly for visual-element-based questions\nrelated to metaphors. The paired infographic and plain charts enable\nfine-grained error analysis and ablation studies, which highlight new\nopportunities for advancing MLLMs in infographic chart understanding. We\nrelease InfoChartQA at https://github.com/CoolDawnAnt/InfoChartQA."}
{"id": "2505.18185", "pdf": "https://arxiv.org/pdf/2505.18185", "abs": "https://arxiv.org/abs/2505.18185", "authors": ["Qinfan Xiao", "Ziyun Cui", "Chi Zhang", "Siqi Chen", "Wen Wu", "Andrew Thwaites", "Alexandra Woolgar", "Bowen Zhou", "Chao Zhang"], "title": "BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "Electroencephalography (EEG) and magnetoencephalography (MEG) measure neural\nactivity non-invasively by capturing electromagnetic fields generated by\ndendritic currents. Although rooted in the same biophysics, EEG and MEG exhibit\ndistinct signal patterns, further complicated by variations in sensor\nconfigurations across modalities and recording devices. Existing approaches\ntypically rely on separate, modality- and dataset-specific models, which limits\nthe performance and cross-domain scalability. This paper proposes BrainOmni,\nthe first brain foundation model that generalises across heterogeneous EEG and\nMEG recordings. To unify diverse data sources, we introduce BrainTokenizer,the\nfirst tokenizer that quantises spatiotemporal brain activity into discrete\nrepresentations. Central to BrainTokenizer is a novel Sensor Encoder that\nencodes sensor properties such as spatial layout, orientation, and type,\nenabling compatibility across devices and modalities. Building upon the\ndiscrete representations, BrainOmni learns unified semantic embeddings of brain\nsignals by self-supervised pretraining. To the best of our knowledge, it is the\nfirst foundation model to support both EEG and MEG signals, as well as the\nfirst to incorporate large-scale MEG pretraining. A total of 1,997 hours of EEG\nand 656 hours of MEG data are curated and standardised from publicly available\nsources for pretraining. Experiments show that BrainOmni outperforms both\nexisting foundation models and state-of-the-art task-specific models on a range\nof downstream tasks. It also demonstrates strong generalisation to unseen EEG\nand MEG devices. Further analysis reveals that joint EEG-MEG (EMEG) training\nyields consistent improvements across both modalities. Code and model\ncheckpoints will be released upon acceptance."}
{"id": "2505.19578", "pdf": "https://arxiv.org/pdf/2505.19578", "abs": "https://arxiv.org/abs/2505.19578", "authors": ["Dan Peng", "Zhihui Fu", "Zewen Ye", "Zhuoran Song", "Jun Wang"], "title": "Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Under review", "summary": "Sparse attention methods exploit the inherent sparsity in attention to speed\nup the prefilling phase of long-context inference, mitigating the quadratic\ncomplexity of full attention computation. While existing sparse attention\nmethods rely on predefined patterns or inaccurate estimations to approximate\nattention behavior, they often fail to fully capture the true dynamics of\nattention, resulting in reduced efficiency and compromised accuracy. Instead,\nwe propose a highly accurate sparse attention mechanism that shares similar yet\nprecise attention patterns across heads, enabling a more realistic capture of\nthe dynamic behavior of attention. Our approach is grounded in two key\nobservations: (1) attention patterns demonstrate strong inter-head similarity,\nand (2) this similarity remains remarkably consistent across diverse inputs. By\nstrategically sharing computed accurate patterns across attention heads, our\nmethod effectively captures actual patterns while requiring full attention\ncomputation for only a small subset of heads. Comprehensive evaluations\ndemonstrate that our approach achieves superior or comparable speedup relative\nto state-of-the-art methods while delivering the best overall accuracy."}
{"id": "2505.19031", "pdf": "https://arxiv.org/pdf/2505.19031", "abs": "https://arxiv.org/abs/2505.19031", "authors": ["Xikai Yang", "Juzheng Miao", "Yuchen Yuan", "Jiaze Wang", "Qi Dou", "Jinpeng Li", "Pheng-Ann Heng"], "title": "Medical Large Vision Language Models with Multi-Image Visual Ability", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 4 figures", "summary": "Medical large vision-language models (LVLMs) have demonstrated promising\nperformance across various single-image question answering (QA) benchmarks, yet\ntheir capability in processing multi-image clinical scenarios remains\nunderexplored. Unlike single image based tasks, medical tasks involving\nmultiple images often demand sophisticated visual understanding capabilities,\nsuch as temporal reasoning and cross-modal analysis, which are poorly supported\nby current medical LVLMs. To bridge this critical gap, we present the Med-MIM\ninstruction dataset, comprising 83.2K medical multi-image QA pairs that span\nfour types of multi-image visual abilities (temporal understanding, reasoning,\ncomparison, co-reference). Using this dataset, we fine-tune Mantis and\nLLaVA-Med, resulting in two specialized medical VLMs: MIM-LLaVA-Med and\nMed-Mantis, both optimized for multi-image analysis. Additionally, we develop\nthe Med-MIM benchmark to comprehensively evaluate the medical multi-image\nunderstanding capabilities of LVLMs. We assess eight popular LVLMs, including\nour two models, on the Med-MIM benchmark. Experimental results show that both\nMed-Mantis and MIM-LLaVA-Med achieve superior performance on the held-in and\nheld-out subsets of the Med-MIM benchmark, demonstrating that the Med-MIM\ninstruction dataset effectively enhances LVLMs' multi-image understanding\ncapabilities in the medical domain."}
{"id": "2505.18186", "pdf": "https://arxiv.org/pdf/2505.18186", "abs": "https://arxiv.org/abs/2505.18186", "authors": ["Nikhil Singh", "Manuel Cherep", "Pattie Maes"], "title": "Discovering Interpretable Concepts in Large Generative Music Models", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": "16 pages, 9 figures", "summary": "The fidelity with which neural networks can now generate content such as\nmusic presents a scientific opportunity: these systems appear to have learned\nimplicit theories of the structure of such content through statistical learning\nalone. This could offer a novel lens on theories of human-generated media.\nWhere these representations align with traditional constructs (e.g. chord\nprogressions in music), they demonstrate how these can be inferred from\nstatistical regularities. Where they diverge, they highlight potential limits\nin our theoretical frameworks -- patterns that we may have overlooked but that\nnonetheless hold significant explanatory power. In this paper, we focus on the\nspecific case of music generators. We introduce a method to discover musical\nconcepts using sparse autoencoders (SAEs), extracting interpretable features\nfrom the residual stream activations of a transformer model. We evaluate this\napproach by extracting a large set of features and producing an automatic\nlabeling and evaluation pipeline for them. Our results reveal both familiar\nmusical concepts and counterintuitive patterns that lack clear counterparts in\nexisting theories or natural language altogether. Beyond improving model\ntransparency, our work provides a new empirical tool that might help discover\norganizing principles in ways that have eluded traditional methods of analysis\nand synthesis."}
{"id": "2505.19590", "pdf": "https://arxiv.org/pdf/2505.19590", "abs": "https://arxiv.org/abs/2505.19590", "authors": ["Xuandong Zhao", "Zhewei Kang", "Aosong Feng", "Sergey Levine", "Dawn Song"], "title": "Learning to Reason without External Rewards", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Training large language models (LLMs) for complex reasoning via Reinforcement\nLearning with Verifiable Rewards (RLVR) is effective but limited by reliance on\ncostly, domain-specific supervision. We explore Reinforcement Learning from\nInternal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic\nsignals without external rewards or labeled data. We propose Intuitor, an RLIF\nmethod that uses a model's own confidence, termed self-certainty, as its sole\nreward signal. Intuitor replaces external rewards in Group Relative Policy\nOptimization (GRPO) with self-certainty scores, enabling fully unsupervised\nlearning. Experiments demonstrate that Intuitor matches GRPO's performance on\nmathematical benchmarks while achieving superior generalization to\nout-of-domain tasks like code generation, without requiring gold solutions or\ntest cases. Our findings show that intrinsic model signals can drive effective\nlearning across domains, offering a scalable alternative to RLVR for autonomous\nAI systems where verifiable rewards are unavailable. Code is available at\nhttps://github.com/sunblaze-ucb/Intuitor"}
{"id": "2505.19038", "pdf": "https://arxiv.org/pdf/2505.19038", "abs": "https://arxiv.org/abs/2505.19038", "authors": ["Hao Wu", "Yuan Gao", "Ruiqi Shu", "Zean Han", "Fan Xu", "Zhihong Zhu", "Qingsong Wen", "Xian Wu", "Kun Wang", "Xiaomeng Huang"], "title": "Turb-L1: Achieving Long-term Turbulence Tracing By Tackling Spectral Bias", "categories": ["cs.LG", "cs.AI", "physics.flu-dyn"], "comment": null, "summary": "Accurately predicting the long-term evolution of turbulence is crucial for\nadvancing scientific understanding and optimizing engineering applications.\nHowever, existing deep learning methods face significant bottlenecks in\nlong-term autoregressive prediction, which exhibit excessive smoothing and fail\nto accurately track complex fluid dynamics. Our extensive experimental and\nspectral analysis of prevailing methods provides an interpretable explanation\nfor this shortcoming, identifying Spectral Bias as the core obstacle.\nConcretely, spectral bias is the inherent tendency of models to favor\nlow-frequency, smooth features while overlooking critical high-frequency\ndetails during training, thus reducing fidelity and causing physical\ndistortions in long-term predictions. Building on this insight, we propose\nTurb-L1, an innovative turbulence prediction method, which utilizes a\nHierarchical Dynamics Synthesis mechanism within a multi-grid architecture to\nexplicitly overcome spectral bias. It accurately captures cross-scale\ninteractions and preserves the fidelity of high-frequency dynamics, enabling\nreliable long-term tracking of turbulence evolution. Extensive experiments on\nthe 2D turbulence benchmark show that Turb-L1 demonstrates excellent\nperformance: (I) In long-term predictions, it reduces Mean Squared Error (MSE)\nby $80.3\\%$ and increases Structural Similarity (SSIM) by over $9\\times$\ncompared to the SOTA baseline, significantly improving prediction fidelity.\n(II) It effectively overcomes spectral bias, accurately reproducing the full\nenstrophy spectrum and maintaining physical realism in high-wavenumber regions,\nthus avoiding the spectral distortions or spurious energy accumulation seen in\nother methods."}
{"id": "2505.18188", "pdf": "https://arxiv.org/pdf/2505.18188", "abs": "https://arxiv.org/abs/2505.18188", "authors": ["Beck LaBash", "Shahriar Khushrushahi", "Fabian Ruehle"], "title": "Improving Generative Inverse Design of Rectangular Patch Antennas with Test Time Optimization", "categories": ["eess.SP", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "comment": "Code and dataset available at\n  https://github.com/becklabs/patch-antenna-tto", "summary": "We propose a two-stage deep learning framework for the inverse design of\nrectangular patch antennas. Our approach leverages generative modeling to learn\na latent representation of antenna frequency response curves and conditions a\nsubsequent generative model on these responses to produce feasible antenna\ngeometries. We further demonstrate that leveraging search and optimization\ntechniques at test-time improves the accuracy of the generated designs and\nenables consideration of auxiliary objectives such as manufacturability. Our\napproach generalizes naturally to different design criteria, and can be easily\nadapted to more complex geometric design spaces."}
{"id": "2505.19601", "pdf": "https://arxiv.org/pdf/2505.19601", "abs": "https://arxiv.org/abs/2505.19601", "authors": ["Yeongmin Kim", "Heesun Bae", "Byeonghu Na", "Il-Chul Moon"], "title": "Preference Optimization by Estimating the Ratio of the Data Distribution", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Direct preference optimization (DPO) is widely used as a simple and stable\nmethod for aligning large language models (LLMs) with human preferences. This\npaper investigates a generalized DPO loss that enables a policy model to match\nthe target policy from a likelihood ratio estimation perspective. The ratio of\nthe target policy provides a unique identification of the policy distribution\nwithout relying on reward models or partition functions. This allows the\ngeneralized loss to retain both simplicity and theoretical guarantees, which\nprior work such as $f$-PO fails to achieve simultaneously. We propose Bregman\npreference optimization (BPO), a generalized framework for ratio matching that\nprovides a family of objective functions achieving target policy optimality.\nBPO subsumes DPO as a special case and offers tractable forms for all\ninstances, allowing implementation with a few lines of code. We further develop\nscaled Basu's power divergence (SBA), a gradient scaling method that can be\nused for BPO instances. The BPO framework complements other DPO variants and is\napplicable to target policies defined by these variants. In experiments, unlike\nother probabilistic loss extensions such as $f$-DPO or $f$-PO, which exhibit a\ntrade-off between generation fidelity and diversity, instances of BPO improve\nboth win rate and entropy compared with DPO. When applied to\nLlama-3-Instruct-8B, BPO achieves state-of-the-art performance among Llama-3-8B\nbackbones, with a 55.9\\% length-controlled win rate on AlpacaEval2."}
{"id": "2505.19040", "pdf": "https://arxiv.org/pdf/2505.19040", "abs": "https://arxiv.org/abs/2505.19040", "authors": ["Rawabi S. Al Qurashi", "Maram M. Almnjomi", "Teef L. Alghamdi", "Amjad H. Almalki", "Shahad S. Alharthi", "Shahad M. althobuti", "Alanoud S. Alharthi", "Maha A. Thafar"], "title": "Smart Waste Management System for Makkah City using Artificial Intelligence and Internet of Things", "categories": ["cs.ET", "cs.AI", "cs.CY"], "comment": "10 pages, 5 figures", "summary": "Waste management is a critical global issue with significant environmental\nand public health implications. It has become more destructive during\nlarge-scale events such as the annual pilgrimage to Makkah, Saudi Arabia, one\nof the world's largest religious gatherings. This event's popularity has\nattracted millions worldwide, leading to significant and un-predictable\naccumulation of waste. Such a tremendous number of visitors leads to in-creased\nwaste management issues at the Grand Mosque and other holy sites, highlighting\nthe need for an effective solution other than traditional methods based on\nrigid collection schedules.\n  To address this challenge, this research proposed an innovative solution that\nis context-specific and tailored to the unique requirements of pilgrimage\nseason: a Smart Waste Management System, called TUHR, that utilizes the\nInternet of Things and Artificial Intelligence. This system encompasses\nultrasonic sensors that monitor waste levels in each container at the\nperformance sites. Once the container reaches full capacity, the sensor\ncommunicates with the microcontroller, which alerts the relevant authorities.\nMoreover, our system can detect harmful substances such as gas from the gas\ndetector sensor. Such a proactive and dynamic approach promises to mitigate the\nenvironmental and health risks associated with waste accumulation and enhance\nthe cleanliness of these sites. It also delivers economic benefits by reducing\nunnecessary gasoline consumption and optimizing waste management resources.\nImportantly, this research aligns with the principles of smart cities and\nexemplifies the innovative, sustainable, and health-conscious approach that\nSaudi Arabia is implementing as part of its Vision 2030 initiative."}
{"id": "2505.18189", "pdf": "https://arxiv.org/pdf/2505.18189", "abs": "https://arxiv.org/abs/2505.18189", "authors": ["Paul Phl", "Viktor Schlegel", "Hao Li", "Anil Bharath"], "title": "Generating Realistic Multi-Beat ECG Signals", "categories": ["eess.SP", "cs.LG"], "comment": "DSP 2025", "summary": "Generating synthetic ECG data has numerous applications in healthcare, from\neducational purposes to simulating scenarios and forecasting trends. While\nrecent diffusion models excel at generating short ECG segments, they struggle\nwith longer sequences needed for many clinical applications. This paper\nproposes a novel three-layer synthesis framework for generating realistic\nlong-form ECG signals. We first generate high-fidelity single beats using a\ndiffusion model, then synthesize inter-beat features preserving critical\ntemporal dependencies, and finally assemble beats into coherent long sequences\nusing feature-guided matching. Our comprehensive evaluation demonstrates that\nthe resulting synthetic ECGs maintain both beat-level morphological fidelity\nand clinically relevant inter-beat relationships. In arrhythmia classification\ntasks, our long-form synthetic ECGs significantly outperform end-to-end\nlong-form ECG generation using the diffusion model, highlighting their\npotential for increasing utility for downstream applications. The approach\nenables generation of unprecedented multi-minute ECG sequences while preserving\nessential diagnostic characteristics."}
{"id": "2505.19621", "pdf": "https://arxiv.org/pdf/2505.19621", "abs": "https://arxiv.org/abs/2505.19621", "authors": ["George Kour", "Itay Nakash", "Ateret Anaby-Tavor", "Michal Shmueli-Scheuer"], "title": "Think Again! The Effect of Test-Time Compute on Preferences, Opinions, and Beliefs of Large Language Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) become deeply integrated into human life and\nincreasingly influence decision-making, it's crucial to evaluate whether and to\nwhat extent they exhibit subjective preferences, opinions, and beliefs. These\ntendencies may stem from biases within the models, which may shape their\nbehavior, influence the advice and recommendations they offer to users, and\npotentially reinforce certain viewpoints. This paper presents the Preference,\nOpinion, and Belief survey (POBs), a benchmark developed to assess LLMs'\nsubjective inclinations across societal, cultural, ethical, and personal\ndomains. We applied our benchmark to evaluate leading open- and closed-source\nLLMs, measuring desired properties such as reliability, neutrality, and\nconsistency. In addition, we investigated the effect of increasing the\ntest-time compute, through reasoning and self-reflection mechanisms, on those\nmetrics. While effective in other tasks, our results show that these mechanisms\noffer only limited gains in our domain. Furthermore, we reveal that newer model\nversions are becoming less consistent and more biased toward specific\nviewpoints, highlighting a blind spot and a concerning trend. POBS:\nhttps://ibm.github.io/POBS"}
{"id": "2505.19056", "pdf": "https://arxiv.org/pdf/2505.19056", "abs": "https://arxiv.org/abs/2505.19056", "authors": ["Harethah Abu Shairah", "Hasan Abed Al Kader Hammoud", "Bernard Ghanem", "George Turkiyyah"], "title": "An Embarrassingly Simple Defense Against LLM Abliteration Attacks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "preprint", "summary": "Large language models (LLMs) are typically aligned to comply with safety\nguidelines by refusing harmful instructions. A recent attack, termed\nabliteration, isolates and suppresses the single latent direction most\nresponsible for refusal behavior, enabling the model to generate unethical\ncontent. We propose a defense that modifies how models generate refusals. We\nconstruct an extended-refusal dataset that contains harmful prompts with a full\nresponse that justifies the reason for refusal. We then fine-tune\nLlama-2-7B-Chat and Qwen2.5-Instruct (1.5B and 3B parameters) on our\nextended-refusal dataset, and evaluate the resulting systems on a set of\nharmful prompts. In our experiments, extended-refusal models maintain high\nrefusal rates, dropping at most by 10%, whereas baseline models' refusal rates\ndrop by 70-80% after abliteration. A broad evaluation of safety and utility\nshows that extended-refusal fine-tuning neutralizes the abliteration attack\nwhile preserving general performance."}
{"id": "2505.18190", "pdf": "https://arxiv.org/pdf/2505.18190", "abs": "https://arxiv.org/abs/2505.18190", "authors": ["Yuezhou Ma", "Haixu Wu", "Hang Zhou", "Huikun Weng", "Jianmin Wang", "Mingsheng Long"], "title": "PhySense: Sensor Placement Optimization for Accurate Physics Sensing", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "Physics sensing plays a central role in many scientific and engineering\ndomains, which inherently involves two coupled tasks: reconstructing dense\nphysical fields from sparse observations and optimizing scattered sensor\nplacements to observe maximum information. While deep learning has made rapid\nadvances in sparse-data reconstruction, existing methods generally omit\noptimization of sensor placements, leaving the mutual enhancement between\nreconstruction and placement on the shelf. To change this suboptimal practice,\nwe propose PhySense, a synergistic two-stage framework that learns to jointly\nreconstruct physical fields and to optimize sensor placements, both aiming for\naccurate physics sensing. The first stage involves a flow-based generative\nmodel enhanced by cross-attention to adaptively fuse sparse observations.\nLeveraging the reconstruction feedback, the second stage performs sensor\nplacement via projected gradient descent to satisfy spatial constraints. We\nfurther prove that the learning objectives of the two stages are consistent\nwith classical variance-minimization principles, providing theoretical\nguarantees. Extensive experiments across three challenging benchmarks,\nespecially a 3D geometry dataset, indicate PhySense achieves state-of-the-art\nphysics sensing accuracy and discovers informative sensor placements previously\nunconsidered."}
{"id": "2505.19641", "pdf": "https://arxiv.org/pdf/2505.19641", "abs": "https://arxiv.org/abs/2505.19641", "authors": ["Junteng Liu", "Yuanxiang Fan", "Zhuo Jiang", "Han Ding", "Yongyi Hu", "Chi Zhang", "Yiqi Shi", "Shitong Weng", "Aili Chen", "Shiqi Chen", "Yunan Huang", "Mozhi Zhang", "Pengyu Zhao", "Junjie Yan", "Junxian He"], "title": "SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the\npotential of Reinforcement Learning (RL) to enhance reasoning abilities in\nLarge Language Models (LLMs). While open-source replication efforts have\nprimarily focused on mathematical and coding domains, methods and resources for\ndeveloping general reasoning capabilities remain underexplored. This gap is\npartly due to the challenge of collecting diverse and verifiable reasoning data\nsuitable for RL. We hypothesize that logical reasoning is critical for\ndeveloping general reasoning capabilities, as logic forms a fundamental\nbuilding block of reasoning. In this work, we present SynLogic, a data\nsynthesis framework and dataset that generates diverse logical reasoning data\nat scale, encompassing 35 diverse logical reasoning tasks. The SynLogic\napproach enables controlled synthesis of data with adjustable difficulty and\nquantity. Importantly, all examples can be verified by simple rules, making\nthem ideally suited for RL with verifiable rewards. In our experiments, we\nvalidate the effectiveness of RL training on the SynLogic dataset based on 7B\nand 32B models. SynLogic leads to state-of-the-art logical reasoning\nperformance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B\nby 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and\ncoding tasks improves the training efficiency of these domains and\nsignificantly enhances reasoning generalization. Notably, our mixed training\nmodel outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These\nfindings position SynLogic as a valuable resource for advancing the broader\nreasoning capabilities of LLMs. We open-source both the data synthesis pipeline\nand the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic."}
{"id": "2505.19059", "pdf": "https://arxiv.org/pdf/2505.19059", "abs": "https://arxiv.org/abs/2505.19059", "authors": ["Ignacio Mariano Andreozzi Pofcher", "Joshua Ellul"], "title": "An Initial Exploration of Fine-tuning Small Language Models for Smart Contract Reentrancy Vulnerability Detection", "categories": ["cs.SE", "cs.AI", "cs.ET", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are being used more and more for various coding\ntasks, including to help coders identify bugs and are a promising avenue to\nsupport coders in various tasks including vulnerability detection --\nparticularly given the flexibility of such generative AI models and tools. Yet\nfor many tasks it may not be suitable to use LLMs, for which it may be more\nsuitable to use smaller language models that can fit and easily execute and\ntrain on a developer's computer. In this paper we explore and evaluate whether\nsmaller language models can be fine-tuned to achieve reasonable results for a\nniche area: vulnerability detection -- specifically focusing on detecting the\nreentrancy bug in Solidity smart contracts."}
{"id": "2505.18191", "pdf": "https://arxiv.org/pdf/2505.18191", "abs": "https://arxiv.org/abs/2505.18191", "authors": ["Jonathan Dan", "Amirhossein Shahbazinia", "Christodoulos Kechris", "David Atienza"], "title": "SzCORE as a benchmark: report from the seizure detection challenge at the 2025 AI in Epilepsy and Neurological Disorders Conference", "categories": ["eess.SP", "cs.AI", "cs.LG", "cs.PF"], "comment": null, "summary": "Reliable automatic seizure detection from long-term EEG remains a challenge,\nas current machine learning models often fail to generalize across patients or\nclinical settings. Manual EEG review remains the clinical standard,\nunderscoring the need for robust models and standardized evaluation. To\nrigorously assess algorithm performance, we organized a challenge using a\nprivate dataset of continuous EEG recordings from 65 subjects (4,360 hours).\nExpert neurophysiologists annotated the data, providing ground truth for\nseizure events. Participants were required to detect seizure onset and\nduration, with evaluation based on event-based metrics, including sensitivity,\nprecision, F1-score, and false positives per day. The SzCORE framework ensured\nstandardized evaluation. The primary ranking criterion was the event-based\nF1-score, reflecting clinical relevance by balancing sensitivity and false\npositives. The challenge received 30 submissions from 19 teams, with 28\nalgorithms evaluated. Results revealed wide variability in performance, with a\ntop F1-score of 43% (sensitivity 37%, precision 45%), highlighting the ongoing\ndifficulty of seizure detection. The challenge also revealed a gap between\nreported performance and real-world evaluation, emphasizing the importance of\nrigorous benchmarking. Compared to previous challenges and commercial systems,\nthe best-performing algorithm in this contest showed improved performance.\nImportantly, the challenge platform now supports continuous benchmarking,\nenabling reproducible research, integration of new datasets, and clinical\nevaluation of seizure detection algorithms using a standardized framework."}
{"id": "2505.19683", "pdf": "https://arxiv.org/pdf/2505.19683", "abs": "https://arxiv.org/abs/2505.19683", "authors": ["Pengfei Cao", "Tianyi Men", "Wencan Liu", "Jingwen Zhang", "Xuzhao Li", "Xixun Lin", "Dianbo Sui", "Yanan Cao", "Kang Liu", "Jun Zhao"], "title": "Large Language Models for Planning: A Comprehensive and Systematic Survey", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Planning represents a fundamental capability of intelligent agents, requiring\ncomprehensive environmental understanding, rigorous logical reasoning, and\neffective sequential decision-making. While Large Language Models (LLMs) have\ndemonstrated remarkable performance on certain planning tasks, their broader\napplication in this domain warrants systematic investigation. This paper\npresents a comprehensive review of LLM-based planning. Specifically, this\nsurvey is structured as follows: First, we establish the theoretical\nfoundations by introducing essential definitions and categories about automated\nplanning. Next, we provide a detailed taxonomy and analysis of contemporary\nLLM-based planning methodologies, categorizing them into three principal\napproaches: 1) External Module Augmented Methods that combine LLMs with\nadditional components for planning, 2) Finetuning-based Methods that involve\nusing trajectory data and feedback signals to adjust LLMs in order to improve\ntheir planning abilities, and 3) Searching-based Methods that break down\ncomplex tasks into simpler components, navigate the planning space, or enhance\ndecoding strategies to find the best solutions. Subsequently, we systematically\nsummarize existing evaluation frameworks, including benchmark datasets,\nevaluation metrics and performance comparisons between representative planning\nmethods. Finally, we discuss the underlying mechanisms enabling LLM-based\nplanning and outline promising research directions for this rapidly evolving\nfield. We hope this survey will serve as a valuable resource to inspire\ninnovation and drive progress in this field."}
{"id": "2505.19084", "pdf": "https://arxiv.org/pdf/2505.19084", "abs": "https://arxiv.org/abs/2505.19084", "authors": ["Yifeng Xu", "Zhenliang He", "Meina Kan", "Shiguang Shan", "Xilin Chen"], "title": "Jodi: Unification of Visual Generation and Understanding via Joint Modeling", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Code: https://github.com/VIPL-GENUN/Jodi", "summary": "Visual generation and understanding are two deeply interconnected aspects of\nhuman intelligence, yet they have been traditionally treated as separate tasks\nin machine learning. In this paper, we propose Jodi, a diffusion framework that\nunifies visual generation and understanding by jointly modeling the image\ndomain and multiple label domains. Specifically, Jodi is built upon a linear\ndiffusion transformer along with a role switch mechanism, which enables it to\nperform three particular types of tasks: (1) joint generation, where the model\nsimultaneously generates images and multiple labels; (2) controllable\ngeneration, where images are generated conditioned on any combination of\nlabels; and (3) image perception, where multiple labels can be predicted at\nonce from a given image. Furthermore, we present the Joint-1.6M dataset, which\ncontains 200,000 high-quality images collected from public sources, automatic\nlabels for 7 visual domains, and LLM-generated captions. Extensive experiments\ndemonstrate that Jodi excels in both generation and understanding tasks and\nexhibits strong extensibility to a wider range of visual domains. Code is\navailable at https://github.com/VIPL-GENUN/Jodi."}
{"id": "2505.18195", "pdf": "https://arxiv.org/pdf/2505.18195", "abs": "https://arxiv.org/abs/2505.18195", "authors": ["Ambre Marie", "Marine Garnier", "Thomas Bertin", "Laura Machart", "Guillaume Dardenne", "Gwenol Quellec", "Sofian Berrouiguet"], "title": "Acoustic and Machine Learning Methods for Speech-Based Suicide Risk Assessment: A Systematic Review", "categories": ["eess.AS", "cs.LG", "cs.SD"], "comment": "Preprint version of a manuscript submitted to Computers in Biology\n  and Medicine. 25 pages, 7 figures, 8 tables", "summary": "Suicide remains a public health challenge, necessitating improved detection\nmethods to facilitate timely intervention and treatment. This systematic review\nevaluates the role of Artificial Intelligence (AI) and Machine Learning (ML) in\nassessing suicide risk through acoustic analysis of speech. Following PRISMA\nguidelines, we analyzed 33 articles selected from PubMed, Cochrane, Scopus, and\nWeb of Science databases. These studies primarily explored acoustic differences\nbetween individuals at risk of suicide (RS) and those not at risk (NRS), and\nevaluated ML classifier performance. Findings consistently showed significant\nacoustic feature variations between RS and NRS populations, particularly\ninvolving jitter, fundamental frequency (F0), Mel-frequency cepstral\ncoefficients (MFCC), and power spectral density (PSD). Classifier effectiveness\nvaried based on algorithms, modalities, and speech elicitation methods, with\nmultimodal approaches integrating acoustic, linguistic, and metadata features\ndemonstrating superior performance. However, limitations such as methodological\nvariability, small sample sizes, lack of longitudinal data, and limited\nlinguistic and demographic diversity restrict generalizability. Future research\nshould focus on standardizing methods, expanding multimodal analyses, and\nutilizing larger, diverse datasets to support AI integration in clinical\nsuicide risk assessment."}
{"id": "2505.19752", "pdf": "https://arxiv.org/pdf/2505.19752", "abs": "https://arxiv.org/abs/2505.19752", "authors": ["Hengli Li", "Yuxuan Wang", "Song-Chun Zhu", "Ying Nian Wu", "Zilong Zheng"], "title": "Discrete Markov Bridge", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Discrete diffusion has recently emerged as a promising paradigm in discrete\ndata modeling. However, existing methods typically rely on a fixed rate\ntransition matrix during training, which not only limits the expressiveness of\nlatent representations, a fundamental strength of variational methods, but also\nconstrains the overall design space. To address these limitations, we propose\nDiscrete Markov Bridge, a novel framework specifically designed for discrete\nrepresentation learning. Our approach is built upon two key components: Matrix\nLearning and Score Learning. We conduct a rigorous theoretical analysis,\nestablishing formal performance guarantees for Matrix Learning and proving the\nconvergence of the overall framework. Furthermore, we analyze the space\ncomplexity of our method, addressing practical constraints identified in prior\nstudies. Extensive empirical evaluations validate the effectiveness of the\nproposed Discrete Markov Bridge, which achieves an Evidence Lower Bound (ELBO)\nof 1.38 on the Text8 dataset, outperforming established baselines. Moreover,\nthe proposed model demonstrates competitive performance on the CIFAR-10\ndataset, achieving results comparable to those obtained by image-specific\ngeneration approaches."}
{"id": "2505.19086", "pdf": "https://arxiv.org/pdf/2505.19086", "abs": "https://arxiv.org/abs/2505.19086", "authors": ["Chen Tessler", "Yifeng Jiang", "Erwin Coumans", "Zhengyi Luo", "Gal Chechik", "Xue Bin Peng"], "title": "MaskedManipulator: Versatile Whole-Body Control for Loco-Manipulation", "categories": ["cs.RO", "cs.AI", "cs.GR"], "comment": null, "summary": "Humans interact with their world while leveraging precise full-body control\nto achieve versatile goals. This versatility allows them to solve long-horizon,\nunderspecified problems, such as placing a cup in a sink, by seamlessly\nsequencing actions like approaching the cup, grasping, transporting it, and\nfinally placing it in the sink. Such goal-driven control can enable new\nprocedural tools for animation systems, enabling users to define partial\nobjectives while the system naturally ``fills in'' the intermediate motions.\nHowever, while current methods for whole-body dexterous manipulation in\nphysics-based animation achieve success in specific interaction tasks, they\ntypically employ control paradigms (e.g., detailed kinematic motion tracking,\ncontinuous object trajectory following, or direct VR teleoperation) that offer\nlimited versatility for high-level goal specification across the entire coupled\nhuman-object system. To bridge this gap, we present MaskedManipulator, a\nunified and generative policy developed through a two-stage learning approach.\nFirst, our system trains a tracking controller to physically reconstruct\ncomplex human-object interactions from large-scale human mocap datasets. This\ntracking controller is then distilled into MaskedManipulator, which provides\nusers with intuitive control over both the character's body and the manipulated\nobject. As a result, MaskedManipulator enables users to specify complex\nloco-manipulation tasks through intuitive high-level objectives (e.g., target\nobject poses, key character stances), and MaskedManipulator then synthesizes\nthe necessary full-body actions for a physically simulated humanoid to achieve\nthese goals, paving the way for more interactive and life-like virtual\ncharacters."}
{"id": "2505.18200", "pdf": "https://arxiv.org/pdf/2505.18200", "abs": "https://arxiv.org/abs/2505.18200", "authors": ["Fahrettin Emin Tiras", "Hayriye Serra Altinoluk"], "title": "CrossRF: A Domain-Invariant Deep Learning Approach for RF Fingerprinting", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "Radio Frequency (RF) fingerprinting offers a promising approach for drone\nidentification and security, although it suffers from significant performance\ndegradation when operating on different transmission channels. This paper\npresents CrossRF, a domain-invariant deep learning approach that addresses the\nproblem of cross-channel RF fingerprinting for Unmanned Aerial Vehicle (UAV)\nidentification. Our approach aims to minimize the domain gap between different\nRF channels by using adversarial learning to train a more robust model that\nmaintains consistent identification performance despite channel variations. We\nvalidate our approach using the UAVSig dataset, comprising real-world\nover-the-air RF signals from identical drone models operating across several\nfrequency channels, ensuring that the findings correspond to real-world\nscenarios. The experimental results show CrossRF's efficiency, achieving up to\n99.03% accuracy when adapting from Channel 3 to Channel 4, compared to only\n26.39% using conventional methods. The model maintains robust performance in\nmore difficult multi-channel scenarios (87.57% accuracy adapting from Channels\n1,3 to 2,4) and achieves 89.45% accuracy with 0.9 precision for controller\nclassification. These results confirm CrossRF's ability to significantly reduce\nperformance degradation due to cross-channel variations while maintaining high\nidentification accuracy with minimal training data requirements, making it\nparticularly suitable for practical drone security applications."}
{"id": "2505.19757", "pdf": "https://arxiv.org/pdf/2505.19757", "abs": "https://arxiv.org/abs/2505.19757", "authors": ["Maria Dziuba", "Valentin Malykh"], "title": "CIDRe: A Reference-Free Multi-Aspect Criterion for Code Comment Quality Measurement", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Effective generation of structured code comments requires robust quality\nmetrics for dataset curation, yet existing approaches (SIDE, MIDQ, STASIS)\nsuffer from limited code-comment analysis. We propose CIDRe, a\nlanguage-agnostic reference-free quality criterion combining four synergistic\naspects: (1) relevance (code-comment semantic alignment), (2) informativeness\n(functional coverage), (3) completeness (presence of all structure sections),\nand (4) description length (detail sufficiency). We validate our criterion on a\nmanually annotated dataset. Experiments demonstrate CIDRe's superiority over\nexisting metrics, achieving improvement in cross-entropy evaluation. When\napplied to filter comments, the models finetuned on CIDRe-filtered data show\nstatistically significant quality gains in GPT-4o-mini assessments."}
{"id": "2505.19091", "pdf": "https://arxiv.org/pdf/2505.19091", "abs": "https://arxiv.org/abs/2505.19091", "authors": ["Benjamin Clavi", "Florian Brand"], "title": "ReadBench: Measuring the Dense Text Visual Reading Ability of Vision-Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent advancements in Large Vision-Language Models (VLMs), have greatly\nenhanced their capability to jointly process text and images. However, despite\nextensive benchmarks evaluating visual comprehension (e.g., diagrams, color\nschemes, OCR tasks...), there is limited assessment of VLMs' ability to read\nand reason about text-rich images effectively. To fill this gap, we introduce\nReadBench, a multimodal benchmark specifically designed to evaluate the reading\ncomprehension capabilities of VLMs. ReadBench transposes contexts from\nestablished text-only benchmarks into images of text while keeping textual\nprompts and questions intact. Evaluating leading VLMs with ReadBench, we find\nminimal-but-present performance degradation on short, text-image inputs, while\nperformance sharply declines for longer, multi-page contexts. Our experiments\nfurther reveal that text resolution has negligible effects on multimodal\nperformance. These findings highlight needed improvements in VLMs, particularly\ntheir reasoning over visually presented extensive textual content, a capability\ncritical for practical applications. ReadBench is available at\nhttps://github.com/answerdotai/ReadBench ."}
{"id": "2505.18201", "pdf": "https://arxiv.org/pdf/2505.18201", "abs": "https://arxiv.org/abs/2505.18201", "authors": ["Romain Poletti", "Lorenzo Schena", "Lilla Koloszar", "Joris Degroote", "Miguel Alfonso Mendez"], "title": "Reinforcement Twinning for Hybrid Control of Flapping-Wing Drones", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Controlling the flight of flapping-wing drones requires versatile controllers\nthat handle their time-varying, nonlinear, and underactuated dynamics from\nincomplete and noisy sensor data. Model-based methods struggle with accurate\nmodeling, while model-free approaches falter in efficiently navigating very\nhigh-dimensional and nonlinear control objective landscapes. This article\npresents a novel hybrid model-free/model-based approach to flight control based\non the recently proposed reinforcement twinning algorithm. The model-based (MB)\napproach relies on an adjoint formulation using an adaptive digital twin,\ncontinuously identified from live trajectories, while the model-free (MF)\napproach relies on reinforcement learning. The two agents collaborate through\ntransfer learning, imitation learning, and experience sharing using the real\nenvironment, the digital twin and a referee. The latter selects the best agent\nto interact with the real environment based on performance within the digital\ntwin and a real-to-virtual environment consistency ratio. The algorithm is\nevaluated for controlling the longitudinal dynamics of a flapping-wing drone,\nwith the environment simulated as a nonlinear, time-varying dynamical system\nunder the influence of quasi-steady aerodynamic forces. The hybrid control\nlearning approach is tested with three types of initialization of the adaptive\nmodel: (1) offline identification using previously available data, (2) random\ninitialization with full online identification, and (3) offline pre-training\nwith an estimation bias, followed by online adaptation. In all three scenarios,\nthe proposed hybrid learning approach demonstrates superior performance\ncompared to purely model-free and model-based methods."}
{"id": "2505.19770", "pdf": "https://arxiv.org/pdf/2505.19770", "abs": "https://arxiv.org/abs/2505.19770", "authors": ["Ruizhe Shi", "Minhak Song", "Runlong Zhou", "Zihan Zhang", "Maryam Fazel", "Simon S. Du"], "title": "Understanding the Performance Gap in Preference Learning: A Dichotomy of RLHF and DPO", "categories": ["cs.LG", "cs.CL"], "comment": "30 pages, 5 figures", "summary": "We present a fine-grained theoretical analysis of the performance gap between\nreinforcement learning from human feedback (RLHF) and direct preference\noptimization (DPO) under a representation gap. Our study decomposes this gap\ninto two sources: an explicit representation gap under exact optimization and\nan implicit representation gap under finite samples. In the exact optimization\nsetting, we characterize how the relative capacities of the reward and policy\nmodel classes influence the final policy qualities. We show that RLHF, DPO, or\nonline DPO can outperform one another depending on the type of model\nmis-specifications. Notably, online DPO can outperform both RLHF and standard\nDPO when the reward and policy model classes are isomorphic and both\nmis-specified. In the approximate optimization setting, we provide a concrete\nconstruction where the ground-truth reward is implicitly sparse and show that\nRLHF requires significantly fewer samples than DPO to recover an effective\nreward model -- highlighting a statistical advantage of two-stage learning.\nTogether, these results provide a comprehensive understanding of the\nperformance gap between RLHF and DPO under various settings, and offer\npractical insights into when each method is preferred."}
{"id": "2505.19094", "pdf": "https://arxiv.org/pdf/2505.19094", "abs": "https://arxiv.org/abs/2505.19094", "authors": ["Chuming Shen", "Wei Wei", "Xiaoye Qu", "Yu Cheng"], "title": "SATORI-R1: Incentivizing Multimodal Reasoning with Spatial Grounding and Verifiable Rewards", "categories": ["cs.CV", "cs.AI"], "comment": "Under review", "summary": "DeepSeek-R1 has demonstrated powerful reasoning capabilities in the text\ndomain through stable reinforcement learning (RL). Recently, in the multimodal\ndomain, works have begun to directly apply RL to generate R1-like free-form\nreasoning for Visual Question Answering (VQA) tasks. However, multimodal tasks\nshare an intrinsically different nature from textual tasks, which heavily rely\non the understanding of the input image to solve the problem. Therefore, such\nfree-form reasoning faces two critical limitations in the VQA task: (1)\nExtended reasoning chains diffuse visual focus away from task-critical regions,\ndegrading answer accuracy. (2) Unverifiable intermediate steps amplify\npolicy-gradient variance and computational costs overhead. To address these\nissues, in this paper, we introduce SATORI ($\\textbf{S}patially$\n$\\textbf{A}nchored$ $\\textbf{T}ask$ $\\textbf{O}ptimization$ with\n$\\textbf{R}e\\textbf{I}nforcement$ Learning), which decomposes VQA into three\nverifiable stages, including global image captioning, region localization, and\nanswer prediction, each supplying explicit reward signals. Furthermore, we also\nintroduce VQA-Verify, a 12k dataset annotated with answer-aligned captions and\nbounding-boxes to facilitate training. Experiments demonstrate consistent\nperformance improvements across seven VQA benchmarks, achieving up to $15.7\\%$\nimprovement in accuracy in accuracy compared to the R1-like baseline. Our\nanalysis of the attention map confirms enhanced focus on critical regions,\nwhich brings improvements in accuracy. Our code is available at\nhttps://github.com/justairr/SATORI-R1."}
{"id": "2505.18207", "pdf": "https://arxiv.org/pdf/2505.18207", "abs": "https://arxiv.org/abs/2505.18207", "authors": ["Ibrahim Al Azher", "Miftahul Jannat Mokarrama", "Zhishuai Guo", "Sagnik Ray Choudhury", "Hamed Alhoori"], "title": "BAGELS: Benchmarking the Automated Generation and Extraction of Limitations from Scholarly Text", "categories": ["cs.DL", "cs.LG", "68T50 Natural language processing"], "comment": "19 pages, 7 figures", "summary": "In scientific research, limitations refer to the shortcomings, constraints,\nor weaknesses within a study. Transparent reporting of such limitations can\nenhance the quality and reproducibility of research and improve public trust in\nscience. However, authors often a) underreport them in the paper text and b)\nuse hedging strategies to satisfy editorial requirements at the cost of\nreaders' clarity and confidence. This underreporting behavior, along with an\nexplosion in the number of publications, has created a pressing need to\nautomatically extract or generate such limitations from scholarly papers. In\nthis direction, we present a complete architecture for the computational\nanalysis of research limitations. Specifically, we create a dataset of\nlimitations in ACL, NeurIPS, and PeerJ papers by extracting them from papers'\ntext and integrating them with external reviews; we propose methods to\nautomatically generate them using a novel Retrieval Augmented Generation (RAG)\ntechnique; we create a fine-grained evaluation framework for generated\nlimitations; and we provide a meta-evaluation for the proposed evaluation\ntechniques."}
{"id": "2505.19866", "pdf": "https://arxiv.org/pdf/2505.19866", "abs": "https://arxiv.org/abs/2505.19866", "authors": ["Feng Xiong", "Hongling Xu", "Yifei Wang", "Runxi Cheng", "Yong Wang", "Xiangxiang Chu"], "title": "HS-STAR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Self-taught reasoners (STaRs) enhance the mathematical reasoning abilities of\nlarge language models (LLMs) by leveraging self-generated responses for\nself-training. Recent studies have incorporated reward models to guide response\nselection or decoding, aiming to obtain higher-quality data. However, they\ntypically allocate a uniform sampling budget across all problems, overlooking\nthe varying utility of problems at different difficulty levels. In this work,\nwe conduct an empirical study and find that problems near the boundary of the\nLLM's reasoning capability offer significantly greater learning utility than\nboth easy and overly difficult ones. To identify and exploit such problems, we\npropose HS-STaR, a Hierarchical Sampling framework for Self-Taught Reasoners.\nGiven a fixed sampling budget, HS-STaR first performs lightweight pre-sampling\nwith a reward-guided difficulty estimation strategy to efficiently identify\nboundary-level problems. Subsequently, it dynamically reallocates the remaining\nbudget toward these high-utility problems during a re-sampling phase,\nmaximizing the generation of valuable training data. Extensive experiments\nacross multiple reasoning benchmarks and backbone LLMs demonstrate that HS-STaR\nsignificantly outperforms other baselines without requiring additional sampling\nbudget."}
{"id": "2505.19096", "pdf": "https://arxiv.org/pdf/2505.19096", "abs": "https://arxiv.org/abs/2505.19096", "authors": ["Qiong Li", "Chao Fang", "Longwei Huang", "Jun Lin", "Zhongfeng Wang"], "title": "Enable Lightweight and Precision-Scalable Posit/IEEE-754 Arithmetic in RISC-V Cores for Transprecision Computing", "categories": ["cs.AR", "cs.AI"], "comment": "Work in Progress", "summary": "While posit format offers superior dynamic range and accuracy for\ntransprecision computing, its adoption in RISC-V processors is hindered by the\nlack of a unified solution for lightweight, precision-scalable, and IEEE-754\narithmetic compatible hardware implementation. To address these challenges, we\nenhance RISC-V processors by 1) integrating dedicated posit codecs into the\noriginal FPU for lightweight implementation, 2) incorporating\nmulti/mixed-precision support with dynamic exponent size for\nprecision-scalability, and 3) reusing and customizing ISA extensions for\nIEEE-754 compatible posit operations. Our comprehensive evaluation spans the\nmodified FPU, RISC-V core, and SoC levels. It demonstrates that our\nimplementation achieves 47.9% LUTs and 57.4% FFs reduction compared to\nstate-of-the-art posit-enabled RISC-V processors, while achieving up to\n2.54$\\times$ throughput improvement in various GEMM kernels."}
{"id": "2505.18247", "pdf": "https://arxiv.org/pdf/2505.18247", "abs": "https://arxiv.org/abs/2505.18247", "authors": ["Kunal Sawarkar", "Shivam R. Solanki", "Abhilasha Mangal"], "title": "MetaGen Blended RAG: Higher Accuracy for Domain-Specific Q&A Without Fine-Tuning", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "Preprint. Paper Submitted NeurIPS 2025- The Thirty-Ninth Annual\n  Conference on Neural Information Processing Systems", "summary": "Despite the widespread exploration of Retrieval-Augmented Generation (RAG),\nits deployment in enterprises for domain-specific datasets remains limited due\nto poor answer accuracy. These corpora, often shielded behind firewalls in\nprivate enterprise knowledge bases, having complex, domain-specific\nterminology, rarely seen by LLMs during pre-training; exhibit significant\nsemantic variability across domains (like networking, military, or legal,\netc.), or even within a single domain like medicine, and thus result in poor\ncontext precision for RAG systems. Currently, in such situations, fine-tuning\nor RAG with fine-tuning is attempted, but these approaches are slow, expensive,\nand lack generalization for accuracy as the new domain-specific data emerges.\nWe propose an approach for Enterprise Search that focuses on enhancing the\nretriever for a domain-specific corpus through hybrid query indexes and\nmetadata enrichment. This 'MetaGen Blended RAG' method constructs a metadata\ngeneration pipeline using key concepts, topics, and acronyms, and then creates\na metadata-enriched hybrid index with boosted search queries. This approach\navoids overfitting and generalizes effectively across domains. On the PubMedQA\nbenchmark for the biomedical domain, the proposed method achieves 82% retrieval\naccuracy and 77% RAG accuracy, surpassing all previous RAG accuracy results\nwithout fine-tuning and sets a new benchmark for zero-shot results while\noutperforming much larger models like GPT3.5. The results are even comparable\nto the best fine-tuned models on this dataset, and we further demonstrate the\nrobustness and scalability of the approach by evaluating it on other Q&A\ndatasets like SQuAD, NQ etc."}
{"id": "2505.19893", "pdf": "https://arxiv.org/pdf/2505.19893", "abs": "https://arxiv.org/abs/2505.19893", "authors": ["Melis Ilayda Bal", "Volkan Cevher", "Michael Muehlebach"], "title": "ESLM: Risk-Averse Selective Language Modeling for Efficient Pretraining", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large language model pretraining is compute-intensive, yet many tokens\ncontribute marginally to learning, resulting in inefficiency. We introduce\nEfficient Selective Language Modeling (ESLM), a risk-aware algorithm that\nimproves training efficiency and distributional robustness by performing online\ntoken-level batch selection. ESLM leverages per-token statistics (e.g., entropy\nor loss) and applies value-at-risk thresholding to retain only the most\ninformative tokens per batch. This data-centric mechanism reshapes the training\nloss, prioritizing high-risk tokens and eliminating redundant gradient\ncomputation. We frame ESLM as a bilevel game: the model competes with a masking\nadversary that selects worst-case token subsets under a constrained\nthresholding rule. In the loss-based setting, ESLM recovers conditional\nvalue-at-risk loss minimization, providing a principled connection to\ndistributionally robust optimization. We extend our approach to Ada-ESLM, which\nadaptively tunes the selection confidence during training. Experiments on GPT-2\npretraining show that ESLM significantly reduces training FLOPs while\nmaintaining or improving both perplexity and downstream performance compared to\nbaselines. Our approach also scales across model sizes, pretraining corpora,\nand integrates naturally with knowledge distillation."}
{"id": "2505.19108", "pdf": "https://arxiv.org/pdf/2505.19108", "abs": "https://arxiv.org/abs/2505.19108", "authors": ["Yongheng Zhang", "Xu Liu", "Ruoxi Zhou", "Qiguang Chen", "Hao Fei", "Wenpeng Lu", "Libo Qin"], "title": "CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 Main Conference", "summary": "Investigating hallucination issues in large language models (LLMs) within\ncross-lingual and cross-modal scenarios can greatly advance the large-scale\ndeployment in real-world applications. Nevertheless, the current studies are\nlimited to a single scenario, either cross-lingual or cross-modal, leaving a\ngap in the exploration of hallucinations in the joint cross-lingual and\ncross-modal scenarios. Motivated by this, we introduce a novel joint\nCross-lingual and Cross-modal Hallucinations benchmark (CCHall) to fill this\ngap. Specifically, CCHall simultaneously incorporates both cross-lingual and\ncross-modal hallucination scenarios, which can be used to assess the\ncross-lingual and cross-modal capabilities of LLMs. Furthermore, we conduct a\ncomprehensive evaluation on CCHall, exploring both mainstream open-source and\nclosed-source LLMs. The experimental results highlight that current LLMs still\nstruggle with CCHall. We hope CCHall can serve as a valuable resource to assess\nLLMs in joint cross-lingual and cross-modal scenarios."}
{"id": "2505.18276", "pdf": "https://arxiv.org/pdf/2505.18276", "abs": "https://arxiv.org/abs/2505.18276", "authors": ["Lorenzo Baldassari", "Josselin Garnier", "Knut Solna", "Maarten V. de Hoop"], "title": "Preconditioned Langevin Dynamics with Score-Based Generative Models for Infinite-Dimensional Linear Bayesian Inverse Problems", "categories": ["stat.ML", "cs.LG", "62F15, 65N21, 68Q32, 60Hxx, 65C05, 82C31, 28C20, 60G15, 60J60"], "comment": null, "summary": "Designing algorithms for solving high-dimensional Bayesian inverse problems\ndirectly in infinite-dimensional function spaces - where such problems are\nnaturally formulated - is crucial to ensure stability and convergence as the\ndiscretization of the underlying problem is refined. In this paper, we\ncontribute to this line of work by analyzing a widely used sampler for linear\ninverse problems: Langevin dynamics driven by score-based generative models\n(SGMs) acting as priors, formulated directly in function space. Building on the\ntheoretical framework for SGMs in Hilbert spaces, we give a rigorous definition\nof this sampler in the infinite-dimensional setting and derive, for the first\ntime, error estimates that explicitly depend on the approximation error of the\nscore. As a consequence, we obtain sufficient conditions for global convergence\nin Kullback-Leibler divergence on the underlying function space. Preventing\nnumerical instabilities requires preconditioning of the Langevin algorithm and\nwe prove the existence and the form of an optimal preconditioner. The\npreconditioner depends on both the score error and the forward operator and\nguarantees a uniform convergence rate across all posterior modes. Our analysis\napplies to both Gaussian and a general class of non-Gaussian priors. Finally,\nwe present examples that illustrate and validate our theoretical findings."}
{"id": "2505.19896", "pdf": "https://arxiv.org/pdf/2505.19896", "abs": "https://arxiv.org/abs/2505.19896", "authors": ["Alejandro Carrasco", "Victor Rodriguez-Fernandez", "Richard Linares"], "title": "Large Language Models as Autonomous Spacecraft Operators in Kerbal Space Program", "categories": ["cs.AI", "astro-ph.IM", "cs.CL"], "comment": "Non revised version for paper going to be published in Journal of\n  Advances in Space Research", "summary": "Recent trends are emerging in the use of Large Language Models (LLMs) as\nautonomous agents that take actions based on the content of the user text\nprompts. We intend to apply these concepts to the field of Control in space,\nenabling LLMs to play a significant role in the decision-making process for\nautonomous satellite operations. As a first step towards this goal, we have\ndeveloped a pure LLM-based solution for the Kerbal Space Program Differential\nGames (KSPDG) challenge, a public software design competition where\nparticipants create autonomous agents for maneuvering satellites involved in\nnon-cooperative space operations, running on the KSP game engine. Our approach\nleverages prompt engineering, few-shot prompting, and fine-tuning techniques to\ncreate an effective LLM-based agent that ranked 2nd in the competition. To the\nbest of our knowledge, this work pioneers the integration of LLM agents into\nspace research. The project comprises several open repositories to facilitate\nreplication and further research. The codebase is accessible on\n\\href{https://github.com/ARCLab-MIT/kspdg}{GitHub}, while the trained models\nand datasets are available on \\href{https://huggingface.co/OhhTuRnz}{Hugging\nFace}. Additionally, experiment tracking and detailed results can be reviewed\non \\href{https://wandb.ai/carrusk/huggingface}{Weights \\& Biases"}
{"id": "2505.19110", "pdf": "https://arxiv.org/pdf/2505.19110", "abs": "https://arxiv.org/abs/2505.19110", "authors": ["Vishwa Mohan Singh", "Alberto Gaston Villagran Asiares", "Luisa Sophie Schuhmacher", "Kate Rendall", "Simon Weibrod", "David Rgamer", "Inga Krte"], "title": "An Interpretable Representation Learning Approach for Diffusion Tensor Imaging", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted for publication at MIDL 2025", "summary": "Diffusion Tensor Imaging (DTI) tractography offers detailed insights into the\nstructural connectivity of the brain, but presents challenges in effective\nrepresentation and interpretation in deep learning models. In this work, we\npropose a novel 2D representation of DTI tractography that encodes tract-level\nfractional anisotropy (FA) values into a 9x9 grayscale image. This\nrepresentation is processed through a Beta-Total Correlation Variational\nAutoencoder with a Spatial Broadcast Decoder to learn a disentangled and\ninterpretable latent embedding. We evaluate the quality of this embedding using\nsupervised and unsupervised representation learning strategies, including\nauxiliary classification, triplet loss, and SimCLR-based contrastive learning.\nCompared to the 1D Group deep neural network (DNN) baselines, our approach\nimproves the F1 score in a downstream sex classification task by 15.74% and\nshows a better disentanglement than the 3D representation."}
{"id": "2505.18279", "pdf": "https://arxiv.org/pdf/2505.18279", "abs": "https://arxiv.org/abs/2505.18279", "authors": ["Alireza Rezazadeh", "Zichao Li", "Ange Lou", "Yuying Zhao", "Wei Wei", "Yujia Bao"], "title": "Collaborative Memory: Multi-User Memory Sharing in LLM Agents with Dynamic Access Control", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Complex tasks are increasingly delegated to ensembles of specialized\nLLM-based agents that reason, communicate, and coordinate actions-both among\nthemselves and through interactions with external tools, APIs, and databases.\nWhile persistent memory has been shown to enhance single-agent performance,\nmost approaches assume a monolithic, single-user context-overlooking the\nbenefits and challenges of knowledge transfer across users under dynamic,\nasymmetric permissions. We introduce Collaborative Memory, a framework for\nmulti-user, multi-agent environments with asymmetric, time-evolving access\ncontrols encoded as bipartite graphs linking users, agents, and resources. Our\nsystem maintains two memory tiers: (1) private memory-private fragments visible\nonly to their originating user; and (2) shared memory-selectively shared\nfragments. Each fragment carries immutable provenance attributes (contributing\nagents, accessed resources, and timestamps) to support retrospective permission\nchecks. Granular read policies enforce current user-agent-resource constraints\nand project existing memory fragments into filtered transformed views. Write\npolicies determine fragment retention and sharing, applying context-aware\ntransformations to update the memory. Both policies may be designed conditioned\non system, agent, and user-level information. Our framework enables safe,\nefficient, and interpretable cross-user knowledge sharing, with provable\nadherence to asymmetric, time-varying policies and full auditability of memory\noperations."}
{"id": "2505.19897", "pdf": "https://arxiv.org/pdf/2505.19897", "abs": "https://arxiv.org/abs/2505.19897", "authors": ["Qiushi Sun", "Zhoumianze Liu", "Chang Ma", "Zichen Ding", "Fangzhi Xu", "Zhangyue Yin", "Haiteng Zhao", "Zhenyu Wu", "Kanzhi Cheng", "Zhaoyang Liu", "Jianing Wang", "Qintong Li", "Xiangru Tang", "Tianbao Xie", "Xiachong Feng", "Xiang Li", "Ben Kao", "Wenhai Wang", "Biqing Qi", "Lingpeng Kong", "Zhiyong Wu"], "title": "ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "work in progress", "summary": "Large Language Models (LLMs) have extended their impact beyond Natural\nLanguage Processing, substantially fostering the development of\ninterdisciplinary research. Recently, various LLM-based agents have been\ndeveloped to assist scientific discovery progress across multiple aspects and\ndomains. Among these, computer-using agents, capable of interacting with\noperating systems as humans do, are paving the way to automated scientific\nproblem-solving and addressing routines in researchers' workflows. Recognizing\nthe transformative potential of these agents, we introduce ScienceBoard, which\nencompasses two complementary contributions: (i) a realistic, multi-domain\nenvironment featuring dynamic and visually rich scientific workflows with\nintegrated professional software, where agents can autonomously interact via\ndifferent interfaces to accelerate complex research tasks and experiments; and\n(ii) a challenging benchmark of 169 high-quality, rigorously validated\nreal-world tasks curated by humans, spanning scientific-discovery workflows in\ndomains such as biochemistry, astronomy, and geoinformatics. Extensive\nevaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude\n3.7, UI-TARS) show that, despite some promising results, they still fall short\nof reliably assisting scientists in complex workflows, achieving only a 15%\noverall success rate. In-depth analysis further provides valuable insights for\naddressing current agent limitations and more effective design principles,\npaving the way to build more capable agents for scientific discovery. Our code,\nenvironment, and benchmark are at\nhttps://qiushisun.github.io/ScienceBoard-Home/."}
{"id": "2505.19115", "pdf": "https://arxiv.org/pdf/2505.19115", "abs": "https://arxiv.org/abs/2505.19115", "authors": ["Brian Chmiel", "Maxim Fishman", "Ron Banner", "Daniel Soudry"], "title": "FP4 All the Way: Fully Quantized Training of LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We demonstrate, for the first time, fully quantized training (FQT) of large\nlanguage models (LLMs) using predominantly 4-bit floating-point (FP4) precision\nfor weights, activations, and gradients on datasets up to 200 billion tokens.\nWe extensively investigate key design choices for FP4, including block sizes,\nscaling formats, and rounding methods. Our analysis shows that the NVFP4\nformat, where each block of 16 FP4 values (E2M1) shares a scale represented in\nE4M3, provides optimal results. We use stochastic rounding for backward and\nupdate passes and round-to-nearest for the forward pass to enhance stability.\nAdditionally, we identify a theoretical and empirical threshold for effective\nquantized training: when the gradient norm falls below approximately $\\sqrt{3}$\ntimes the quantization noise, quantized training becomes less effective.\nLeveraging these insights, we successfully train a 7-billion-parameter model on\n256 Intel Gaudi2 accelerators. The resulting FP4-trained model achieves\ndownstream task performance comparable to a standard BF16 baseline, confirming\nthat FP4 training is a practical and highly efficient approach for large-scale\nLLM training. A reference implementation is supplied in\nhttps://github.com/Anonymous1252022/fp4-all-the-way ."}
{"id": "2505.18286", "pdf": "https://arxiv.org/pdf/2505.18286", "abs": "https://arxiv.org/abs/2505.18286", "authors": ["Mingyan Gao", "Yanzi Li", "Banruo Liu", "Yifan Yu", "Phillip Wang", "Ching-Yu Lin", "Fan Lai"], "title": "Single-agent or Multi-agent Systems? Why Not Both?", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "Multi-agent systems (MAS) decompose complex tasks and delegate subtasks to\ndifferent large language model (LLM) agents and tools. Prior studies have\nreported the superior accuracy performance of MAS across diverse domains,\nenabled by long-horizon context tracking and error correction through\nrole-specific agents. However, the design and deployment of MAS incur higher\ncomplexity and runtime cost compared to single-agent systems (SAS). Meanwhile,\nfrontier LLMs, such as OpenAI-o3 and Gemini-2.5-Pro, have rapidly advanced in\nlong-context reasoning, memory retention, and tool usage, mitigating many\nlimitations that originally motivated MAS designs. In this paper, we conduct an\nextensive empirical study comparing MAS and SAS across various popular agentic\napplications. We find that the benefits of MAS over SAS diminish as LLM\ncapabilities improve, and we propose efficient mechanisms to pinpoint the\nerror-prone agent in MAS. Furthermore, the performance discrepancy between MAS\nand SAS motivates our design of a hybrid agentic paradigm, request cascading\nbetween MAS and SAS, to improve both efficiency and capability. Our design\nimproves accuracy by 1.1-12% while reducing deployment costs by up to 20%\nacross various agentic applications."}
{"id": "2505.19944", "pdf": "https://arxiv.org/pdf/2505.19944", "abs": "https://arxiv.org/abs/2505.19944", "authors": ["Naoyuki Terashita", "Yusuke Tozaki", "Hideaki Omote", "Congkha Nguyen", "Ryosuke Nakamoto", "Yuta Koreeda", "Hiroaki Ozaki"], "title": "Can Visual Encoder Learn to See Arrows?", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "This work has been accepted for poster presentation at the Second\n  Workshop on Visual Concepts in CVPR 2025", "summary": "The diagram is a visual representation of a relationship illustrated with\nedges (lines or arrows), which is widely used in industrial and scientific\ncommunication. Although recognizing diagrams is essential for vision language\nmodels (VLMs) to comprehend domain-specific knowledge, recent studies reveal\nthat many VLMs fail to identify edges in images. We hypothesize that these\nfailures stem from an over-reliance on textual and positional biases,\npreventing VLMs from learning explicit edge features. Based on this idea, we\nempirically investigate whether the image encoder in VLMs can learn edge\nrepresentation through training on a diagram dataset in which edges are biased\nneither by textual nor positional information. To this end, we conduct\ncontrastive learning on an artificially generated diagram--caption dataset to\ntrain an image encoder and evaluate its diagram-related features on three\ntasks: probing, image retrieval, and captioning. Our results show that the\nfinetuned model outperforms pretrained CLIP in all tasks and surpasses\nzero-shot GPT-4o and LLaVA-Mistral in the captioning task. These findings\nconfirm that eliminating textual and positional biases fosters accurate edge\nrecognition in VLMs, offering a promising path for advancing diagram\nunderstanding."}
{"id": "2505.19119", "pdf": "https://arxiv.org/pdf/2505.19119", "abs": "https://arxiv.org/abs/2505.19119", "authors": ["Renyuan Li", "Zhibo Liang", "Haichuan Zhang", "Tianyu Shi", "Zhiyuan Cheng", "Jia Shi", "Carl Yang", "Mingjie Tang"], "title": "CloneShield: A Framework for Universal Perturbation Against Zero-Shot Voice Cloning", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "10pages, 4figures", "summary": "Recent breakthroughs in text-to-speech (TTS) voice cloning have raised\nserious privacy concerns, allowing highly accurate vocal identity replication\nfrom just a few seconds of reference audio, while retaining the speaker's vocal\nauthenticity. In this paper, we introduce CloneShield, a universal time-domain\nadversarial perturbation framework specifically designed to defend against\nzero-shot voice cloning. Our method provides protection that is robust across\nspeakers and utterances, without requiring any prior knowledge of the\nsynthesized text. We formulate perturbation generation as a multi-objective\noptimization problem, and propose Multi-Gradient Descent Algorithm (MGDA) to\nensure the robust protection across diverse utterances. To preserve natural\nauditory perception for users, we decompose the adversarial perturbation via\nMel-spectrogram representations and fine-tune it for each sample. This design\nensures imperceptibility while maintaining strong degradation effects on\nzero-shot cloned outputs. Experiments on three state-of-the-art zero-shot TTS\nsystems, five benchmark datasets and evaluations from 60 human listeners\ndemonstrate that our method preserves near-original audio quality in protected\ninputs (PESQ = 3.90, SRS = 0.93) while substantially degrading both speaker\nsimilarity and speech quality in cloned samples (PESQ = 1.07, SRS = 0.08)."}
{"id": "2505.18288", "pdf": "https://arxiv.org/pdf/2505.18288", "abs": "https://arxiv.org/abs/2505.18288", "authors": ["Yash Patel", "Unique Subedi", "Ambuj Tewari"], "title": "Operator Learning for Schrdinger Equation: Unitarity, Error Bounds, and Time Generalization", "categories": ["stat.ML", "cs.LG"], "comment": "25 pages", "summary": "We consider the problem of learning the evolution operator for the\ntime-dependent Schr\\\"{o}dinger equation, where the Hamiltonian may vary with\ntime. Existing neural network-based surrogates often ignore fundamental\nproperties of the Schr\\\"{o}dinger equation, such as linearity and unitarity,\nand lack theoretical guarantees on prediction error or time generalization. To\naddress this, we introduce a linear estimator for the evolution operator that\npreserves a weak form of unitarity. We establish both upper and lower bounds on\nthe prediction error that hold uniformly over all sufficiently smooth initial\nwave functions. Additionally, we derive time generalization bounds that\nquantify how the estimator extrapolates beyond the time points seen during\ntraining. Experiments across real-world Hamiltonians -- including hydrogen\natoms, ion traps for qubit design, and optical lattices -- show that our\nestimator achieves relative errors $10^{-2}$ to $10^{-3}$ times smaller than\nstate-of-the-art methods such as the Fourier Neural Operator and DeepONet."}
{"id": "2505.19954", "pdf": "https://arxiv.org/pdf/2505.19954", "abs": "https://arxiv.org/abs/2505.19954", "authors": ["Andrew Zamai", "Nathanael Fijalkow", "Boris Mansencal", "Laurent Simon", "Eloi Navet", "Pierrick Coupe"], "title": "An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The differential diagnosis of neurodegenerative dementias is a challenging\nclinical task, mainly because of the overlap in symptom presentation and the\nsimilarity of patterns observed in structural neuroimaging. To improve\ndiagnostic efficiency and accuracy, deep learning-based methods such as\nConvolutional Neural Networks and Vision Transformers have been proposed for\nthe automatic classification of brain MRIs. However, despite their strong\npredictive performance, these models find limited clinical utility due to their\nopaque decision making. In this work, we propose a framework that integrates\ntwo core components to enhance diagnostic transparency. First, we introduce a\nmodular pipeline for converting 3D T1-weighted brain MRIs into textual\nradiology reports. Second, we explore the potential of modern Large Language\nModels (LLMs) to assist clinicians in the differential diagnosis between\nFrontotemporal dementia subtypes, Alzheimer's disease, and normal aging based\non the generated reports. To bridge the gap between predictive accuracy and\nexplainability, we employ reinforcement learning to incentivize diagnostic\nreasoning in LLMs. Without requiring supervised reasoning traces or\ndistillation from larger models, our approach enables the emergence of\nstructured diagnostic rationales grounded in neuroimaging findings. Unlike\npost-hoc explainability methods that retrospectively justify model decisions,\nour framework generates diagnostic rationales as part of the inference\nprocess-producing causally grounded explanations that inform and guide the\nmodel's decision-making process. In doing so, our framework matches the\ndiagnostic performance of existing deep learning methods while offering\nrationales that support its diagnostic conclusions."}
{"id": "2505.19128", "pdf": "https://arxiv.org/pdf/2505.19128", "abs": "https://arxiv.org/abs/2505.19128", "authors": ["Jin Zhang", "Fan Gao", "Linyu Li", "Yongbin Yu", "Xiangxiang Wang", "Nyima Tashi", "Gadeng Luosang"], "title": "RetrieveAll: A Multilingual Named Entity Recognition Framework with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rise of large language models has led to significant performance\nbreakthroughs in named entity recognition (NER) for high-resource languages,\nyet there remains substantial room for improvement in low- and medium-resource\nlanguages. Existing multilingual NER methods face severe language interference\nduring the multi-language adaptation process, manifested in feature conflicts\nbetween different languages and the competitive suppression of low-resource\nlanguage features by high-resource languages. Although training a dedicated\nmodel for each language can mitigate such interference, it lacks scalability\nand incurs excessive computational costs in real-world applications. To address\nthis issue, we propose RetrieveAll, a universal multilingual NER framework\nbased on dynamic LoRA. The framework decouples task-specific features across\nlanguages and demonstrates efficient dynamic adaptability. Furthermore, we\nintroduce a cross-granularity knowledge augmented method that fully exploits\nthe intrinsic potential of the data without relying on external resources. By\nleveraging a hierarchical prompting mechanism to guide knowledge injection,\nthis approach advances the paradigm from \"prompt-guided inference\" to\n\"prompt-driven learning.\" Experimental results show that RetrieveAll\noutperforms existing baselines; on the PAN-X dataset, it achieves an average F1\nimprovement of 12.1 percent."}
{"id": "2505.18297", "pdf": "https://arxiv.org/pdf/2505.18297", "abs": "https://arxiv.org/abs/2505.18297", "authors": ["Kristoffer Andersson", "Alessandro Gnoatto", "Camilo Andrs Garca Trillos"], "title": "A deep solver for backward stochastic Volterra integral equations", "categories": ["math.NA", "cs.LG", "cs.NA", "math.PR", "q-fin.MF", "65C30, 60H20, 60H35, 68T07", "G.1.9; G.3; I.2.6; F.2.1"], "comment": "25 pages, 10 figures", "summary": "We present the first deep-learning solver for backward stochastic Volterra\nintegral equations (BSVIEs) and their fully-coupled forward-backward variants.\nThe method trains a neural network to approximate the two solution fields in a\nsingle stage, avoiding the use of nested time-stepping cycles that limit\nclassical algorithms. For the decoupled case we prove a non-asymptotic error\nbound composed of an a posteriori residual plus the familiar square root\ndependence on the time step. Numerical experiments confirm this rate and reveal\ntwo key properties: \\emph{scalability}, in the sense that accuracy remains\nstable from low dimension up to 500 spatial variables while GPU batching keeps\nwall-clock time nearly constant; and \\emph{generality}, since the same method\nhandles coupled systems whose forward dynamics depend on the backward solution.\nThese results open practical access to a family of high-dimensional,\npath-dependent problems in stochastic control and quantitative finance."}
{"id": "2505.19955", "pdf": "https://arxiv.org/pdf/2505.19955", "abs": "https://arxiv.org/abs/2505.19955", "authors": ["Hui Chen", "Miao Xiong", "Yujie Lu", "Wei Han", "Ailin Deng", "Yufei He", "Jiaying Wu", "Yibo Li", "Yue Liu", "Bryan Hooi"], "title": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "40 pages, 7 figures", "summary": "Recent advancements in AI agents have demonstrated their growing potential to\ndrive and support scientific discovery. In this work, we introduce MLR-Bench, a\ncomprehensive benchmark for evaluating AI agents on open-ended machine learning\nresearch. MLR-Bench includes three key components: (1) 201 research tasks\nsourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2)\nMLR-Judge, an automated evaluation framework combining LLM-based reviewers with\ncarefully designed review rubrics to assess research quality; and (3)\nMLR-Agent, a modular agent scaffold capable of completing research tasks\nthrough four stages: idea generation, proposal formulation, experimentation,\nand paper writing. Our framework supports both stepwise assessment across these\ndistinct research stages, and end-to-end evaluation of the final research\npaper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced\ncoding agent, finding that while LLMs are effective at generating coherent\nideas and well-structured papers, current coding agents frequently (e.g., in\n80% of the cases) produce fabricated or invalidated experimental\nresults--posing a major barrier to scientific reliability. We validate\nMLR-Judge through human evaluation, showing high agreement with expert\nreviewers, supporting its potential as a scalable tool for research evaluation.\nWe open-source MLR-Bench to help the community benchmark, diagnose, and improve\nAI research agents toward trustworthy and transparent scientific discovery."}
{"id": "2505.19147", "pdf": "https://arxiv.org/pdf/2505.19147", "abs": "https://arxiv.org/abs/2505.19147", "authors": ["Xuyang Liu", "Zichen Wen", "Shaobo Wang", "Junjie Chen", "Zhishan Tao", "Yubo Wang", "Xiangqi Jin", "Chang Zou", "Yiyu Wang", "Chenfei Liao", "Xu Zheng", "Honggang Chen", "Weijia Li", "Xuming Hu", "Conghui He", "Linfeng Zhang"], "title": "Shifting AI Efficiency From Model-Centric to Data-Centric Compression", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Project:\n  \\url{https://github.com/xuyang-liu16/Awesome-Token-level-Model-Compression}", "summary": "The rapid advancement of large language models (LLMs) and multi-modal LLMs\n(MLLMs) has historically relied on model-centric scaling through increasing\nparameter counts from millions to hundreds of billions to drive performance\ngains. However, as we approach hardware limits on model size, the dominant\ncomputational bottleneck has fundamentally shifted to the quadratic cost of\nself-attention over long token sequences, now driven by ultra-long text\ncontexts, high-resolution images, and extended videos. In this position paper,\n\\textbf{we argue that the focus of research for efficient AI is shifting from\nmodel-centric compression to data-centric compression}. We position token\ncompression as the new frontier, which improves AI efficiency via reducing the\nnumber of tokens during model training or inference. Through comprehensive\nanalysis, we first examine recent developments in long-context AI across\nvarious domains and establish a unified mathematical framework for existing\nmodel efficiency strategies, demonstrating why token compression represents a\ncrucial paradigm shift in addressing long-context overhead. Subsequently, we\nsystematically review the research landscape of token compression, analyzing\nits fundamental benefits and identifying its compelling advantages across\ndiverse scenarios. Furthermore, we provide an in-depth analysis of current\nchallenges in token compression research and outline promising future\ndirections. Ultimately, our work aims to offer a fresh perspective on AI\nefficiency, synthesize existing research, and catalyze innovative developments\nto address the challenges that increasing context lengths pose to the AI\ncommunity's advancement."}
{"id": "2505.18323", "pdf": "https://arxiv.org/pdf/2505.18323", "abs": "https://arxiv.org/abs/2505.18323", "authors": ["Nicolas Kchler", "Ivan Petrov", "Conrad Grobler", "Ilia Shumailov"], "title": "Architectural Backdoors for Within-Batch Data Stealing and Model Inference Manipulation", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "For nearly a decade the academic community has investigated backdoors in\nneural networks, primarily focusing on classification tasks where adversaries\nmanipulate the model prediction. While demonstrably malicious, the immediate\nreal-world impact of such prediction-altering attacks has remained unclear. In\nthis paper we introduce a novel and significantly more potent class of\nbackdoors that builds upon recent advancements in architectural backdoors. We\ndemonstrate how these backdoors can be specifically engineered to exploit\nbatched inference, a common technique for hardware utilization, enabling\nlarge-scale user data manipulation and theft. By targeting the batching\nprocess, these architectural backdoors facilitate information leakage between\nconcurrent user requests and allow attackers to fully control model responses\ndirected at other users within the same batch. In other words, an attacker who\ncan change the model architecture can set and steal model inputs and outputs of\nother users within the same batch. We show that such attacks are not only\nfeasible but also alarmingly effective, can be readily injected into prevalent\nmodel architectures, and represent a truly malicious threat to user privacy and\nsystem integrity. Critically, to counteract this new class of vulnerabilities,\nwe propose a deterministic mitigation strategy that provides formal guarantees\nagainst this new attack vector, unlike prior work that relied on Large Language\nModels to find the backdoors. Our mitigation strategy employs a novel\nInformation Flow Control mechanism that analyzes the model graph and proves\nnon-interference between different user inputs within the same batch. Using our\nmitigation strategy we perform a large scale analysis of models hosted through\nHugging Face and find over 200 models that introduce (unintended) information\nleakage between batch entries due to the use of dynamic quantization."}
{"id": "2505.19956", "pdf": "https://arxiv.org/pdf/2505.19956", "abs": "https://arxiv.org/abs/2505.19956", "authors": ["Jihyung Lee", "Jin-Seop Lee", "Jaehoon Lee", "YunSeok Choi", "Jee-Hyong Lee"], "title": "DCG-SQL: Enhancing In-Context Learning for Text-to-SQL with Deep Contextual Schema Link Graph", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Text-to-SQL, which translates a natural language question into an SQL query,\nhas advanced with in-context learning of Large Language Models (LLMs). However,\nexisting methods show little improvement in performance compared to randomly\nchosen demonstrations, and significant performance drops when smaller LLMs\n(e.g., Llama 3.1-8B) are used. This indicates that these methods heavily rely\non the intrinsic capabilities of hyper-scaled LLMs, rather than effectively\nretrieving useful demonstrations. In this paper, we propose a novel approach\nfor effectively retrieving demonstrations and generating SQL queries. We\nconstruct a Deep Contextual Schema Link Graph, which contains key information\nand semantic relationship between a question and its database schema items.\nThis graph-based structure enables effective representation of Text-to-SQL\nsamples and retrieval of useful demonstrations for in-context learning.\nExperimental results on the Spider benchmark demonstrate the effectiveness of\nour approach, showing consistent improvements in SQL generation performance and\nefficiency across both hyper-scaled LLMs and small LLMs. Our code will be\nreleased."}
{"id": "2505.19151", "pdf": "https://arxiv.org/pdf/2505.19151", "abs": "https://arxiv.org/abs/2505.19151", "authors": ["Shenggan Cheng", "Yuanxin Wei", "Lansong Diao", "Yong Liu", "Bujiao Chen", "Lianghua Huang", "Yu Liu", "Wenyuan Yu", "Jiangsu Du", "Wei Lin", "Yang You"], "title": "SRDiffusion: Accelerate Video Diffusion Inference via Sketching-Rendering Cooperation", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "9 pages, 6 figures", "summary": "Leveraging the diffusion transformer (DiT) architecture, models like Sora,\nCogVideoX and Wan have achieved remarkable progress in text-to-video,\nimage-to-video, and video editing tasks. Despite these advances,\ndiffusion-based video generation remains computationally intensive, especially\nfor high-resolution, long-duration videos. Prior work accelerates its inference\nby skipping computation, usually at the cost of severe quality degradation. In\nthis paper, we propose SRDiffusion, a novel framework that leverages\ncollaboration between large and small models to reduce inference cost. The\nlarge model handles high-noise steps to ensure semantic and motion fidelity\n(Sketching), while the smaller model refines visual details in low-noise steps\n(Rendering). Experimental results demonstrate that our method outperforms\nexisting approaches, over 3$\\times$ speedup for Wan with nearly no quality loss\nfor VBench, and 2$\\times$ speedup for CogVideoX. Our method is introduced as a\nnew direction orthogonal to existing acceleration strategies, offering a\npractical solution for scalable video generation."}
{"id": "2505.18325", "pdf": "https://arxiv.org/pdf/2505.18325", "abs": "https://arxiv.org/abs/2505.18325", "authors": ["Licheng Pan", "Yongqi Tong", "Xin Zhang", "Xiaolu Zhang", "Jun Zhou", "Zhixuan Chu"], "title": "Understanding and Mitigating Overrefusal in LLMs from an Unveiling Perspective of Safety Decision Boundary", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet they often refuse to answer legitimate queries-a\nphenomenon known as overrefusal. Overrefusal typically stems from\nover-conservative safety alignment, causing models to treat many reasonable\nprompts as potentially risky. To systematically understand this issue, we probe\nand leverage the models'safety decision boundaries to analyze and mitigate\noverrefusal. Our findings reveal that overrefusal is closely tied to\nmisalignment at these boundary regions, where models struggle to distinguish\nsubtle differences between benign and harmful content. Building on these\ninsights, we present RASS, an automated framework for prompt generation and\nselection that strategically targets overrefusal prompts near the safety\nboundary. By harnessing steering vectors in the representation space, RASS\nefficiently identifies and curates boundary-aligned prompts, enabling more\neffective and targeted mitigation of overrefusal. This approach not only\nprovides a more precise and interpretable view of model safety decisions but\nalso seamlessly extends to multilingual scenarios.We have explored the safety\ndecision boundaries of various LLMs and construct the MORBench evaluation set\nto facilitate robust assessment of model safety and helpfulness across multiple\nlanguages. Code and datasets will be released at\nhttps://anonymous.4open.science/r/RASS-80D3."}
{"id": "2505.19964", "pdf": "https://arxiv.org/pdf/2505.19964", "abs": "https://arxiv.org/abs/2505.19964", "authors": ["Eric Zhao", "Jessica Dai", "Pranjal Awasthi"], "title": "The Limits of Preference Data for Post-Training", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.GT"], "comment": null, "summary": "Recent progress in strengthening the capabilities of large language models\nhas stemmed from applying reinforcement learning to domains with automatically\nverifiable outcomes. A key question is whether we can similarly use RL to\noptimize for outcomes in domains where evaluating outcomes inherently requires\nhuman feedback; for example, in tasks like deep research and trip planning,\noutcome evaluation is qualitative and there are many possible degrees of\nsuccess. One attractive and scalable modality for collecting human feedback is\npreference data: ordinal rankings (pairwise or $k$-wise) that indicate, for $k$\ngiven outcomes, which one is preferred. In this work, we study a critical\nroadblock: preference data fundamentally and significantly limits outcome-based\noptimization. Even with idealized preference data (infinite, noiseless, and\nonline), the use of ordinal feedback can prevent obtaining even approximately\noptimal solutions. We formalize this impossibility using voting theory, drawing\nan analogy between how a model chooses to answer a query with how voters choose\na candidate to elect. This indicates that grounded human scoring and\nalgorithmic innovations are necessary for extending the success of RL\npost-training to domains demanding human feedback. We also explore why these\nlimitations have disproportionately impacted RLHF when it comes to eliciting\nreasoning behaviors (e.g., backtracking) versus situations where RLHF has been\nhistorically successful (e.g., instruction-tuning and safety training), finding\nthat the limitations of preference data primarily suppress RLHF's ability to\nelicit robust strategies -- a class that encompasses most reasoning behaviors."}
{"id": "2505.19163", "pdf": "https://arxiv.org/pdf/2505.19163", "abs": "https://arxiv.org/abs/2505.19163", "authors": ["Firoj Alam", "Md Arid Hasan", "Shammur Absar Chowdhury"], "title": "SpokenNativQA: Multilingual Everyday Spoken Queries for LLMs", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": "Spoken Question Answering, Multilingual LLMs, Speech-based\n  Evaluation, Dialectal Speech, Low-resource Languages, Multimodal\n  Benchmarking, Conversational AI, Speech-to-Text QA, Real-world Interaction,\n  Natural Language Understanding", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious disciplines and tasks. However, benchmarking their capabilities with\nmultilingual spoken queries remains largely unexplored. In this study, we\nintroduce SpokenNativQA, the first multilingual and culturally aligned spoken\nquestion-answering (SQA) dataset designed to evaluate LLMs in real-world\nconversational settings. The dataset comprises approximately 33,000 naturally\nspoken questions and answers in multiple languages, including low-resource and\ndialect-rich languages, providing a robust benchmark for assessing LLM\nperformance in speech-based interactions. SpokenNativQA addresses the\nlimitations of text-based QA datasets by incorporating speech variability,\naccents, and linguistic diversity. We benchmark different ASR systems and LLMs\nfor SQA and present our findings. We released the data at\n(https://huggingface.co/datasets/QCRI/SpokenNativQA) and the experimental\nscripts at (https://llmebench.qcri.org/) for the research community."}
{"id": "2505.18327", "pdf": "https://arxiv.org/pdf/2505.18327", "abs": "https://arxiv.org/abs/2505.18327", "authors": ["Xinchen Du", "Wanrong Zhu", "Wei Biao Wu", "Sen Na"], "title": "Online Statistical Inference of Constrained Stochastic Optimization via Random Scaling", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA", "math.OC", "math.ST", "stat.CO", "stat.TH"], "comment": "43 pages, 1 figure, 8 tables", "summary": "Constrained stochastic nonlinear optimization problems have attracted\nsignificant attention for their ability to model complex real-world scenarios\nin physics, economics, and biology. As datasets continue to grow, online\ninference methods have become crucial for enabling real-time decision-making\nwithout the need to store historical data. In this work, we develop an online\ninference procedure for constrained stochastic optimization by leveraging a\nmethod called Sketched Stochastic Sequential Quadratic Programming (SSQP). As a\ndirect generalization of sketched Newton methods, SSQP approximates the\nobjective with a quadratic model and the constraints with a linear model at\neach step, then applies a sketching solver to inexactly solve the resulting\nsubproblem. Building on this design, we propose a new online inference\nprocedure called random scaling. In particular, we construct a test statistic\nbased on SSQP iterates whose limiting distribution is free of any unknown\nparameters. Compared to existing online inference procedures, our approach\noffers two key advantages: (i) it enables the construction of asymptotically\nvalid confidence intervals; and (ii) it is matrix-free, i.e. the computation\ninvolves only primal-dual SSQP iterates $(\\boldsymbol{x}_t,\n\\boldsymbol{\\lambda}_t)$ without requiring any matrix inversions. We validate\nour theory through numerical experiments on nonlinearly constrained regression\nproblems and demonstrate the superior performance of our random scaling method\nover existing inference procedures."}
{"id": "2505.19997", "pdf": "https://arxiv.org/pdf/2505.19997", "abs": "https://arxiv.org/abs/2505.19997", "authors": ["Tao Wu", "Jingyuan Chen", "Wang Lin", "Mengze Li", "Yumeng Zhu", "Ang Li", "Kun Kuang", "Fei Wu"], "title": "Embracing Imperfection: Simulating Students with Diverse Cognitive Levels Using LLM-based Agents", "categories": ["cs.LG", "cs.CL", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) are revolutionizing education, with LLM-based\nagents playing a key role in simulating student behavior. A major challenge in\nstudent simulation is modeling the diverse learning patterns of students at\nvarious cognitive levels. However, current LLMs, typically trained as ``helpful\nassistants'', target at generating perfect responses. As a result, they\nstruggle to simulate students with diverse cognitive abilities, as they often\nproduce overly advanced answers, missing the natural imperfections that\ncharacterize student learning and resulting in unrealistic simulations. To\naddress this issue, we propose a training-free framework for student\nsimulation. We begin by constructing a cognitive prototype for each student\nusing a knowledge graph, which captures their understanding of concepts from\npast learning records. This prototype is then mapped to new tasks to predict\nstudent performance. Next, we simulate student solutions based on these\npredictions and iteratively refine them using a beam search method to better\nreplicate realistic mistakes. To validate our approach, we construct the\n\\texttt{Student\\_100} dataset, consisting of $100$ students working on Python\nprogramming and $5,000$ learning records. Experimental results show that our\nmethod consistently outperforms baseline models, achieving $100\\%$ improvement\nin simulation accuracy."}
{"id": "2505.19164", "pdf": "https://arxiv.org/pdf/2505.19164", "abs": "https://arxiv.org/abs/2505.19164", "authors": ["Ashirbad Mishra", "Jinyu Zhao", "Soumik Dey", "Hansi Wu", "Binbin Li", "Kamesh Madduri"], "title": "BroadGen: A Framework for Generating Effective and Efficient Advertiser Broad Match Keyphrase Recommendations", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": null, "summary": "In the domain of sponsored search advertising, the focus of Keyphrase\nrecommendation has largely been on exact match types, which pose issues such as\nhigh management expenses, limited targeting scope, and evolving search query\npatterns. Alternatives like Broad match types can alleviate certain drawbacks\nof exact matches but present challenges like poor targeting accuracy and\nminimal supervisory signals owing to limited advertiser usage. This research\ndefines the criteria for an ideal broad match, emphasizing on both efficiency\nand effectiveness, ensuring that a significant portion of matched queries are\nrelevant. We propose BroadGen, an innovative framework that recommends\nefficient and effective broad match keyphrases by utilizing historical search\nquery data. Additionally, we demonstrate that BroadGen, through token\ncorrespondence modeling, maintains better query stability over time. BroadGen's\ncapabilities allow it to serve daily, millions of sellers at eBay with over 2.3\nbillion items."}
{"id": "2505.18332", "pdf": "https://arxiv.org/pdf/2505.18332", "abs": "https://arxiv.org/abs/2505.18332", "authors": ["Rahul Thomas", "Louai Zahran", "Erica Choi", "Akilesh Potti", "Micah Goldblum", "Arka Pal"], "title": "An Attack to Break Permutation-Based Private Third-Party Inference Schemes for LLMs", "categories": ["cs.CR", "cs.LG"], "comment": "To be published in ICML 2025 Main Proceedings as \"Hidden No More:\n  Attacking and Defending Private Third-Party LLM Inference\"", "summary": "Recent advances in Large Language Models (LLMs) have led to the widespread\nadoption of third-party inference services, raising critical privacy concerns.\nExisting methods of performing private third-party inference, such as Secure\nMultiparty Computation (SMPC), often rely on cryptographic methods. However,\nthese methods are thousands of times slower than standard unencrypted\ninference, and fail to scale to large modern LLMs. Therefore, recent lines of\nwork have explored the replacement of expensive encrypted nonlinear\ncomputations in SMPC with statistical obfuscation methods - in particular,\nrevealing permuted hidden states to the third parties, with accompanying strong\nclaims of the difficulty of reversal into the unpermuted states. In this work,\nwe begin by introducing a novel reconstruction technique that can recover\noriginal prompts from hidden states with nearly perfect accuracy across\nmultiple state-of-the-art LLMs. We then show that extensions of our attack are\nnearly perfectly effective in reversing permuted hidden states of LLMs,\ndemonstrating the insecurity of three recently proposed privacy schemes. We\nfurther dissect the shortcomings of prior theoretical `proofs' of permuation\nsecurity which allow our attack to succeed. Our findings highlight the\nimportance of rigorous security analysis in privacy-preserving LLM inference."}
{"id": "2505.20027", "pdf": "https://arxiv.org/pdf/2505.20027", "abs": "https://arxiv.org/abs/2505.20027", "authors": ["Subba Reddy Oota", "Khushbu Pahwa", "Mounika Marreddy", "Maneesh Singh", "Manish Gupta", "Bapi S. Raju"], "title": "Multi-modal brain encoding models for multi-modal stimuli", "categories": ["q-bio.NC", "cs.AI", "cs.CL", "cs.LG", "eess.AS", "eess.IV"], "comment": "26 pages, 15 figures, The Thirteenth International Conference on\n  Learning Representations, ICLR-2025, Singapore.\n  https://openreview.net/pdf?id=0dELcFHig2", "summary": "Despite participants engaging in unimodal stimuli, such as watching images or\nsilent videos, recent work has demonstrated that multi-modal Transformer models\ncan predict visual brain activity impressively well, even with incongruent\nmodality representations. This raises the question of how accurately these\nmulti-modal models can predict brain activity when participants are engaged in\nmulti-modal stimuli. As these models grow increasingly popular, their use in\nstudying neural activity provides insights into how our brains respond to such\nmulti-modal naturalistic stimuli, i.e., where it separates and integrates\ninformation across modalities through a hierarchy of early sensory regions to\nhigher cognition. We investigate this question by using multiple unimodal and\ntwo types of multi-modal models-cross-modal and jointly pretrained-to determine\nwhich type of model is more relevant to fMRI brain activity when participants\nare engaged in watching movies. We observe that both types of multi-modal\nmodels show improved alignment in several language and visual regions. This\nstudy also helps in identifying which brain regions process unimodal versus\nmulti-modal information. We further investigate the contribution of each\nmodality to multi-modal alignment by carefully removing unimodal features one\nby one from multi-modal representations, and find that there is additional\ninformation beyond the unimodal embeddings that is processed in the visual and\nlanguage regions. Based on this investigation, we find that while for\ncross-modal models, their brain alignment is partially attributed to the video\nmodality; for jointly pretrained models, it is partially attributed to both the\nvideo and audio modalities. This serves as a strong motivation for the\nneuroscience community to investigate the interpretability of these models for\ndeepening our understanding of multi-modal information processing in brain."}
{"id": "2505.19178", "pdf": "https://arxiv.org/pdf/2505.19178", "abs": "https://arxiv.org/abs/2505.19178", "authors": ["Akhila Yaragoppa", "Siddharth"], "title": "Saliency-guided Emotion Modeling: Predicting Viewer Reactions from Video Stimuli", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted for publication at IBPRIA 2025 Conference in Coimbra,\n  Portugal", "summary": "Understanding the emotional impact of videos is crucial for applications in\ncontent creation, advertising, and Human-Computer Interaction (HCI).\nTraditional affective computing methods rely on self-reported emotions, facial\nexpression analysis, and biosensing data, yet they often overlook the role of\nvisual saliency -- the naturally attention-grabbing regions within a video. In\nthis study, we utilize deep learning to introduce a novel saliency-based\napproach to emotion prediction by extracting two key features: saliency area\nand number of salient regions. Using the HD2S saliency model and OpenFace\nfacial action unit analysis, we examine the relationship between video saliency\nand viewer emotions. Our findings reveal three key insights: (1) Videos with\nmultiple salient regions tend to elicit high-valence, low-arousal emotions, (2)\nVideos with a single dominant salient region are more likely to induce\nlow-valence, high-arousal responses, and (3) Self-reported emotions often\nmisalign with facial expression-based emotion detection, suggesting limitations\nin subjective reporting. By leveraging saliency-driven insights, this work\nprovides a computationally efficient and interpretable alternative for emotion\nmodeling, with implications for content creation, personalized media\nexperiences, and affective computing research."}
{"id": "2505.18342", "pdf": "https://arxiv.org/pdf/2505.18342", "abs": "https://arxiv.org/abs/2505.18342", "authors": ["Jack Goffinet", "Youngjo Min", "Carlo Tomasi", "David E. Carlson"], "title": "Pose Splatter: A 3D Gaussian Splatting Model for Quantifying Animal Pose and Appearance", "categories": ["cs.CV", "cs.LG"], "comment": "19 pages, 13 figures", "summary": "Accurate and scalable quantification of animal pose and appearance is crucial\nfor studying behavior. Current 3D pose estimation techniques, such as keypoint-\nand mesh-based techniques, often face challenges including limited\nrepresentational detail, labor-intensive annotation requirements, and expensive\nper-frame optimization. These limitations hinder the study of subtle movements\nand can make large-scale analyses impractical. We propose Pose Splatter, a\nnovel framework leveraging shape carving and 3D Gaussian splatting to model the\ncomplete pose and appearance of laboratory animals without prior knowledge of\nanimal geometry, per-frame optimization, or manual annotations. We also propose\na novel rotation-invariant visual embedding technique for encoding pose and\nappearance, designed to be a plug-in replacement for 3D keypoint data in\ndownstream behavioral analyses. Experiments on datasets of mice, rats, and\nzebra finches show Pose Splatter learns accurate 3D animal geometries. Notably,\nPose Splatter represents subtle variations in pose, provides better\nlow-dimensional pose embeddings over state-of-the-art as evaluated by humans,\nand generalizes to unseen data. By eliminating annotation and per-frame\noptimization bottlenecks, Pose Splatter enables analysis of large-scale,\nlongitudinal behavior needed to map genotype, neural activity, and\nmicro-behavior at unprecedented resolution."}
{"id": "2505.20046", "pdf": "https://arxiv.org/pdf/2505.20046", "abs": "https://arxiv.org/abs/2505.20046", "authors": ["Le Zhang", "Bo Wang", "Xipeng Qiu", "Siva Reddy", "Aishwarya Agrawal"], "title": "REARANK: Reasoning Re-ranking Agent via Reinforcement Learning", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "We present REARANK, a large language model (LLM)-based listwise reasoning\nreranking agent. REARANK explicitly reasons before reranking, significantly\nimproving both performance and interpretability. Leveraging reinforcement\nlearning and data augmentation, REARANK achieves substantial improvements over\nbaseline models across popular information retrieval benchmarks, notably\nrequiring only 179 annotated samples. Built on top of Qwen2.5-7B, our\nREARANK-7B demonstrates performance comparable to GPT-4 on both in-domain and\nout-of-domain benchmarks and even surpasses GPT-4 on reasoning-intensive BRIGHT\nbenchmarks. These results underscore the effectiveness of our approach and\nhighlight how reinforcement learning can enhance LLM reasoning capabilities in\nreranking."}
{"id": "2505.19184", "pdf": "https://arxiv.org/pdf/2505.19184", "abs": "https://arxiv.org/abs/2505.19184", "authors": ["Minh Nhat Nguyen", "Pradyumna Shyama Prasad"], "title": "Two LLMs debate, both are certain they've won", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Can LLMs accurately adjust their confidence when facing opposition? Building\non previous studies measuring calibration on static fact-based\nquestion-answering tasks, we evaluate Large Language Models (LLMs) in a\ndynamic, adversarial debate setting, uniquely combining two realistic factors:\n(a) a multi-turn format requiring models to update beliefs as new information\nemerges, and (b) a zero-sum structure to control for task-related uncertainty,\nsince mutual high-confidence claims imply systematic overconfidence. We\norganized 60 three-round policy debates among ten state-of-the-art LLMs, with\nmodels privately rating their confidence (0-100) in winning after each round.\nWe observed five concerning patterns: (1) Systematic overconfidence: models\nbegan debates with average initial confidence of 72.9% vs. a rational 50%\nbaseline. (2) Confidence escalation: rather than reducing confidence as debates\nprogressed, debaters increased their win probabilities, averaging 83% by the\nfinal round. (3) Mutual overestimation: in 61.7% of debates, both sides\nsimultaneously claimed >=75% probability of victory, a logical impossibility.\n(4) Persistent self-debate bias: models debating identical copies increased\nconfidence from 64.1% to 75.2%; even when explicitly informed their chance of\nwinning was exactly 50%, confidence still rose (from 50.0% to 57.1%). (5)\nMisaligned private reasoning: models' private scratchpad thoughts sometimes\ndiffered from their public confidence ratings, raising concerns about\nfaithfulness of chain-of-thought reasoning. These results suggest LLMs lack the\nability to accurately self-assess or update their beliefs in dynamic,\nmulti-turn tasks; a major concern as LLM outputs are deployed without careful\nreview in assistant roles or agentic settings."}
{"id": "2505.18346", "pdf": "https://arxiv.org/pdf/2505.18346", "abs": "https://arxiv.org/abs/2505.18346", "authors": ["Behrad Moniri", "Hamed Hassani"], "title": "On the Mechanisms of Weak-to-Strong Generalization: A Theoretical Perspective", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Weak-to-strong generalization, where a student model trained on imperfect\nlabels generated by a weaker teacher nonetheless surpasses that teacher, has\nbeen widely observed but the mechanisms that enable it have remained poorly\nunderstood. In this paper, through a theoretical analysis of simple models, we\nuncover three core mechanisms that can drive this phenomenon. First, by\nanalyzing ridge regression, we study the interplay between the teacher and\nstudent regularization and prove that a student can compensate for a teacher's\nunder-regularization and achieve lower test error. We also analyze the role of\nthe parameterization regime of the models. Second, by analyzing weighted ridge\nregression, we show that a student model with a regularization structure more\naligned to the target, can outperform its teacher. Third, in a nonlinear\nmulti-index setting, we demonstrate that a student can learn easy,\ntask-specific features from the teacher while leveraging its own broader\npre-training to learn hard-to-learn features that the teacher cannot capture."}
{"id": "2505.20050", "pdf": "https://arxiv.org/pdf/2505.20050", "abs": "https://arxiv.org/abs/2505.20050", "authors": ["Alkis Koudounas", "Moreno La Quatra", "Gabriele Ciravegna", "Marco Fantini", "Erika Crosetti", "Giovanni Succo", "Tania Cerquitelli", "Sabato Marco Siniscalchi", "Elena Baralis"], "title": "MVP: Multi-source Voice Pathology detection", "categories": ["eess.AS", "cs.CL"], "comment": "Accepted at Interspeech 2025", "summary": "Voice disorders significantly impact patient quality of life, yet\nnon-invasive automated diagnosis remains under-explored due to both the\nscarcity of pathological voice data, and the variability in recording sources.\nThis work introduces MVP (Multi-source Voice Pathology detection), a novel\napproach that leverages transformers operating directly on raw voice signals.\nWe explore three fusion strategies to combine sentence reading and sustained\nvowel recordings: waveform concatenation, intermediate feature fusion, and\ndecision-level combination. Empirical validation across the German, Portuguese,\nand Italian languages shows that intermediate feature fusion using transformers\nbest captures the complementary characteristics of both recording types. Our\napproach achieves up to +13% AUC improvement over single-source methods."}
{"id": "2505.19186", "pdf": "https://arxiv.org/pdf/2505.19186", "abs": "https://arxiv.org/abs/2505.19186", "authors": ["Rushiraj Gadhvi", "Priyansh Desai", "Siddharth"], "title": "PosePilot: An Edge-AI Solution for Posture Correction in Physical Exercises", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted for publication at IBPRIA 2025 Conference in Coimbra,\n  Portugal", "summary": "Automated pose correction remains a significant challenge in AI-driven\nfitness systems, despite extensive research in activity recognition. This work\npresents PosePilot, a novel system that integrates pose recognition with\nreal-time personalized corrective feedback, overcoming the limitations of\ntraditional fitness solutions. Using Yoga, a discipline requiring precise\nspatio-temporal alignment as a case study, we demonstrate PosePilot's ability\nto analyze complex physical movements. Designed for deployment on edge devices,\nPosePilot can be extended to various at-home and outdoor exercises. We employ a\nVanilla LSTM, allowing the system to capture temporal dependencies for pose\nrecognition. Additionally, a BiLSTM with multi-head Attention enhances the\nmodel's ability to process motion contexts, selectively focusing on key limb\nangles for accurate error detection while maintaining computational efficiency.\nAs part of this work, we introduce a high-quality video dataset used for\nevaluating our models. Most importantly, PosePilot provides instant corrective\nfeedback at every stage of a movement, ensuring precise posture adjustments\nthroughout the exercise routine. The proposed approach 1) performs automatic\nhuman posture recognition, 2) provides personalized posture correction feedback\nat each instant which is crucial in Yoga, and 3) offers a lightweight and\nrobust posture correction model feasible for deploying on edge devices in\nreal-world environments."}
{"id": "2505.18356", "pdf": "https://arxiv.org/pdf/2505.18356", "abs": "https://arxiv.org/abs/2505.18356", "authors": ["Lucas Bandarkar", "Nanyun Peng"], "title": "The Unreasonable Effectiveness of Model Merging for Cross-Lingual Transfer in LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": null, "summary": "Large language models (LLMs) still struggle across tasks outside of\nhigh-resource languages. In this work, we investigate cross-lingual transfer to\nlower-resource languages where task-specific post-training data is scarce.\nBuilding on prior work, we first validate that the subsets of model parameters\nthat matter most for mathematical reasoning and multilingual capabilities are\ndistinctly non-overlapping. To exploit this implicit separability between task\nand target language parameterization, we develop and analyze numerous modular\nframeworks to improve the composition of the two during fine-tuning. These\nmethods generally employ freezing parameters or post hoc model merging to\nassign math and language improvement to different key parts of the LLM. In the\nabsence of in-language math data, we demonstrate that the modular approaches\nsuccessfully improve upon baselines across three languages, four models, and\ntwo fine-tuning paradigms (full and LoRA). Furthermore, we identify the most\nconsistently successful modular method to be fine-tuning separate language and\nmath experts and model merging via Layer-Swapping, somewhat surprisingly. We\noffer possible explanations for this result via recent works on the linearity\nof task vectors. We further explain this by empirically showing that reverting\nless useful fine-tuning updates after training often outperforms freezing them\nfrom the start."}
{"id": "2505.20053", "pdf": "https://arxiv.org/pdf/2505.20053", "abs": "https://arxiv.org/abs/2505.20053", "authors": ["Zheqi Lv", "Junhao Chen", "Qi Tian", "Keting Yin", "Shengyu Zhang", "Fei Wu"], "title": "Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": null, "summary": "Diffusion models have become the mainstream architecture for text-to-image\ngeneration, achieving remarkable progress in visual quality and prompt\ncontrollability. However, current inference pipelines generally lack\ninterpretable semantic supervision and correction mechanisms throughout the\ndenoising process. Most existing approaches rely solely on post-hoc scoring of\nthe final image, prompt filtering, or heuristic resampling strategies-making\nthem ineffective in providing actionable guidance for correcting the generative\ntrajectory. As a result, models often suffer from object confusion, spatial\nerrors, inaccurate counts, and missing semantic elements, severely compromising\nprompt-image alignment and image quality. To tackle these challenges, we\npropose MLLM Semantic-Corrected Ping-Pong-Ahead Diffusion (PPAD), a novel\nframework that, for the first time, introduces a Multimodal Large Language\nModel (MLLM) as a semantic observer during inference. PPAD performs real-time\nanalysis on intermediate generations, identifies latent semantic\ninconsistencies, and translates feedback into controllable signals that\nactively guide the remaining denoising steps. The framework supports both\ninference-only and training-enhanced settings, and performs semantic correction\nat only extremely few diffusion steps, offering strong generality and\nscalability. Extensive experiments demonstrate PPAD's significant improvements."}
{"id": "2505.19187", "pdf": "https://arxiv.org/pdf/2505.19187", "abs": "https://arxiv.org/abs/2505.19187", "authors": ["Yang Xiao", "Jiashuo Wang", "Ruifeng Yuan", "Chunpu Xu", "Kaishuai Xu", "Wenjie Li", "Pengfei Liu"], "title": "LIMOPro: Reasoning Refinement for Efficient and Effective Test-time Scaling", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities through test-time scaling approaches, particularly when fine-tuned\nwith chain-of-thought (CoT) data distilled from more powerful large reasoning\nmodels (LRMs). However, these reasoning chains often contain verbose elements\nthat mirror human problem-solving, categorized as progressive reasoning (the\nessential solution development path) and functional elements (verification\nprocesses, alternative solution approaches, and error corrections). While\nprogressive reasoning is crucial, the functional elements significantly\nincrease computational demands during test-time inference. We introduce PIR\n(Perplexity-based Importance Refinement), a principled framework that\nquantitatively evaluates the importance of each reasoning step based on its\nimpact on answer prediction confidence. PIR systematically identifies and\nselectively prunes only low-importance functional steps while preserving\nprogressive reasoning components, creating optimized training data that\nmaintains the integrity of the core solution path while reducing verbosity.\nModels fine-tuned on PIR-optimized data exhibit superior test-time scaling\nproperties, generating more concise reasoning chains while achieving improved\naccuracy (+0.9\\% to +6.6\\%) with significantly reduced token usage (-3\\% to\n-41\\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond).\nOur approach demonstrates strong generalizability across different model sizes,\ndata sources, and token budgets, offering a practical solution for deploying\nreasoning-capable LLMs in scenarios where efficient test-time scaling, response\ntime, and computational efficiency are valuable constraints."}
{"id": "2505.18361", "pdf": "https://arxiv.org/pdf/2505.18361", "abs": "https://arxiv.org/abs/2505.18361", "authors": ["Trinity Chung", "Yuchen Shen", "Nathan C. L. Kong", "Aran Nayebi"], "title": "Task-Optimized Convolutional Recurrent Networks Align with Tactile Processing in the Rodent Brain", "categories": ["q-bio.NC", "cs.AI", "cs.LG", "cs.RO"], "comment": "9 pages, 8 figures, 5 tables", "summary": "Tactile sensing remains far less understood in neuroscience and less\neffective in artificial systems compared to more mature modalities such as\nvision and language. We bridge these gaps by introducing a novel\nEncoder-Attender-Decoder (EAD) framework to systematically explore the space of\ntask-optimized temporal neural networks trained on realistic tactile input\nsequences from a customized rodent whisker-array simulator. We identify\nconvolutional recurrent neural networks (ConvRNNs) as superior encoders to\npurely feedforward and state-space architectures for tactile categorization.\nCrucially, these ConvRNN-encoder-based EAD models achieve neural\nrepresentations closely matching rodent somatosensory cortex, saturating the\nexplainable neural variability and revealing a clear linear relationship\nbetween supervised categorization performance and neural alignment.\nFurthermore, contrastive self-supervised ConvRNN-encoder-based EADs, trained\nwith tactile-specific augmentations, match supervised neural fits, serving as\nan ethologically-relevant, label-free proxy.\n  For neuroscience, our findings highlight nonlinear recurrent processing as\nimportant for general-purpose tactile representations in somatosensory cortex,\nproviding the first quantitative characterization of the underlying inductive\nbiases in this system. For embodied AI, our results emphasize the importance of\nrecurrent EAD architectures to handle realistic tactile inputs, along with\ntailored self-supervised learning methods for achieving robust tactile\nperception with the same type of sensors animals use to sense in unstructured\nenvironments."}
{"id": "2505.20063", "pdf": "https://arxiv.org/pdf/2505.20063", "abs": "https://arxiv.org/abs/2505.20063", "authors": ["Dana Arad", "Aaron Mueller", "Yonatan Belinkov"], "title": "SAEs Are Good for Steering -- If You Select the Right Features", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Sparse Autoencoders (SAEs) have been proposed as an unsupervised approach to\nlearn a decomposition of a model's latent space. This enables useful\napplications such as steering - influencing the output of a model towards a\ndesired concept - without requiring labeled data. Current methods identify SAE\nfeatures to steer by analyzing the input tokens that activate them. However,\nrecent work has highlighted that activations alone do not fully describe the\neffect of a feature on the model's output. In this work, we draw a distinction\nbetween two types of features: input features, which mainly capture patterns in\nthe model's input, and output features, which have a human-understandable\neffect on the model's output. We propose input and output scores to\ncharacterize and locate these types of features, and show that high values for\nboth scores rarely co-occur in the same features. These findings have practical\nimplications: after filtering out features with low output scores, we obtain\n2-3x improvements when steering with SAEs, making them competitive with\nsupervised methods."}
{"id": "2505.19190", "pdf": "https://arxiv.org/pdf/2505.19190", "abs": "https://arxiv.org/abs/2505.19190", "authors": ["Jiayi Xin", "Sukwon Yun", "Jie Peng", "Inyoung Choi", "Jenna L. Ballard", "Tianlong Chen", "Qi Long"], "title": "I2MoE: Interpretable Multimodal Interaction-aware Mixture-of-Experts", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "ICML 2025 Poster", "summary": "Modality fusion is a cornerstone of multimodal learning, enabling information\nintegration from diverse data sources. However, vanilla fusion methods are\nlimited by (1) inability to account for heterogeneous interactions between\nmodalities and (2) lack of interpretability in uncovering the multimodal\ninteractions inherent in the data. To this end, we propose I2MoE (Interpretable\nMultimodal Interaction-aware Mixture of Experts), an end-to-end MoE framework\ndesigned to enhance modality fusion by explicitly modeling diverse multimodal\ninteractions, as well as providing interpretation on a local and global level.\nFirst, I2MoE utilizes different interaction experts with weakly supervised\ninteraction losses to learn multimodal interactions in a data-driven way.\nSecond, I2MoE deploys a reweighting model that assigns importance scores for\nthe output of each interaction expert, which offers sample-level and\ndataset-level interpretation. Extensive evaluation of medical and general\nmultimodal datasets shows that I2MoE is flexible enough to be combined with\ndifferent fusion techniques, consistently improves task performance, and\nprovides interpretation across various real-world scenarios. Code is available\nat https://github.com/Raina-Xin/I2MoE."}
{"id": "2505.18362", "pdf": "https://arxiv.org/pdf/2505.18362", "abs": "https://arxiv.org/abs/2505.18362", "authors": ["Nathan Gaby", "Xiaojing Ye"], "title": "Hamiltonian Theory and Computation of Optimal Probability Density Control in High Dimensions", "categories": ["math.OC", "cs.AI", "cs.LG", "cs.NA", "math.NA"], "comment": "28 pages, submitted", "summary": "We develop a general theoretical framework for optimal probability density\ncontrol and propose a numerical algorithm that is scalable to solve the control\nproblem in high dimensions. Specifically, we establish the Pontryagin Maximum\nPrinciple (PMP) for optimal density control and construct the\nHamilton-Jacobi-Bellman (HJB) equation of the value functional through rigorous\nderivations without any concept from Wasserstein theory. To solve the density\ncontrol problem numerically, we propose to use reduced-order models, such as\ndeep neural networks (DNNs), to parameterize the control vector-field and the\nadjoint function, which allows us to tackle problems defined on\nhigh-dimensional state spaces. We also prove several convergence properties of\nthe proposed algorithm. Numerical results demonstrate promising performances of\nour algorithm on a variety of density control problems with obstacles and\nnonlinear interaction challenges in high dimensions."}
{"id": "2505.20087", "pdf": "https://arxiv.org/pdf/2505.20087", "abs": "https://arxiv.org/abs/2505.20087", "authors": ["Makesh Narsimhan Sreedhar", "Traian Rebedea", "Christopher Parisien"], "title": "Safety Through Reasoning: An Empirical Study of Reasoning Guardrail Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning-based language models have demonstrated strong performance across\nvarious domains, with the most notable gains seen in mathematical and coding\ntasks. Recent research has shown that reasoning also offers significant\nbenefits for LLM safety and guardrail applications. In this work, we conduct a\ncomprehensive analysis of training reasoning-based guardrail models for content\nmoderation, with an emphasis on generalization to custom safety policies at\ninference time. Our study focuses on two key dimensions: data efficiency and\ninference efficiency. On the data front, we find that reasoning-based models\nexhibit strong sample efficiency, achieving competitive performance with\nsignificantly fewer training examples than their non-reasoning counterparts.\nThis unlocks the potential to repurpose the remaining data for mining\nhigh-value, difficult samples that further enhance model performance. On the\ninference side, we evaluate practical trade-offs by introducing reasoning\nbudgets, examining the impact of reasoning length on latency and accuracy, and\nexploring dual-mode training to allow runtime control over reasoning behavior.\nOur findings will provide practical insights for researchers and developers to\neffectively and efficiently train and deploy reasoning-based guardrails models\nin real-world systems."}
{"id": "2505.19194", "pdf": "https://arxiv.org/pdf/2505.19194", "abs": "https://arxiv.org/abs/2505.19194", "authors": ["Peiran Sun"], "title": "Curvature Dynamic Black-box Attack: revisiting adversarial robustness via dynamic curvature estimation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Adversarial attack reveals the vulnerability of deep learning models. For\nabout a decade, countless attack and defense methods have been proposed,\nleading to robustified classifiers and better understanding of models. Among\nthese methods, curvature-based approaches have attracted attention because it\nis assumed that high curvature may give rise to rough decision boundary.\nHowever, the most commonly used \\textit{curvature} is the curvature of loss\nfunction, scores or other parameters from within the model as opposed to\ndecision boundary curvature, since the former can be relatively easily formed\nusing second order derivative. In this paper, we propose a new query-efficient\nmethod, dynamic curvature estimation(DCE), to estimate the decision boundary\ncurvature in a black-box setting. Our approach is based on CGBA, a black-box\nadversarial attack. By performing DCE on a wide range of classifiers, we\ndiscovered, statistically, a connection between decision boundary curvature and\nadversarial robustness. We also propose a new attack method, curvature dynamic\nblack-box attack(CDBA) with improved performance using the dynamically\nestimated curvature."}
{"id": "2505.18366", "pdf": "https://arxiv.org/pdf/2505.18366", "abs": "https://arxiv.org/abs/2505.18366", "authors": ["Hansa Meghwani", "Amit Agarwal", "Priyaranjan Pattnayak", "Hitesh Laxmichand Patel", "Srikant Panda"], "title": "Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "H.3.3; I.2.6; I.2.7"], "comment": "Accepted to ACL 2025", "summary": "Enterprise search systems often struggle to retrieve accurate,\ndomain-specific information due to semantic mismatches and overlapping\nterminologies. These issues can degrade the performance of downstream\napplications such as knowledge management, customer support, and\nretrieval-augmented generation agents. To address this challenge, we propose a\nscalable hard-negative mining framework tailored specifically for\ndomain-specific enterprise data. Our approach dynamically selects semantically\nchallenging but contextually irrelevant documents to enhance deployed\nre-ranking models.\n  Our method integrates diverse embedding models, performs dimensionality\nreduction, and uniquely selects hard negatives, ensuring computational\nefficiency and semantic precision. Evaluation on our proprietary enterprise\ncorpus (cloud services domain) demonstrates substantial improvements of 15\\% in\nMRR@3 and 19\\% in MRR@10 compared to state-of-the-art baselines and other\nnegative sampling techniques. Further validation on public domain-specific\ndatasets (FiQA, Climate Fever, TechQA) confirms our method's generalizability\nand readiness for real-world applications."}
{"id": "2505.20103", "pdf": "https://arxiv.org/pdf/2505.20103", "abs": "https://arxiv.org/abs/2505.20103", "authors": ["Xiangyu Li", "Jingqiang Chen"], "title": "SCIRGC: Multi-Granularity Citation Recommendation and Citation Sentence Preference Alignment", "categories": ["cs.DL", "cs.CL"], "comment": "15 pages, 7 figures", "summary": "Citations are crucial in scientific research articles as they highlight the\nconnection between the current study and prior work. However, this process is\noften time-consuming for researchers. In this study, we propose the SciRGC\nframework, which aims to automatically recommend citation articles and generate\ncitation sentences for citation locations within articles. The framework\naddresses two key challenges in academic citation generation: 1) how to\naccurately identify the author's citation intent and find relevant citation\npapers, and 2) how to generate high-quality citation sentences that align with\nhuman preferences. We enhance citation recommendation accuracy in the citation\narticle recommendation module by incorporating citation networks and sentiment\nintent, and generate reasoning-based citation sentences in the citation\nsentence generation module by using the original article abstract, local\ncontext, citation intent, and recommended articles as inputs. Additionally, we\npropose a new evaluation metric to fairly assess the quality of generated\ncitation sentences. Through comparisons with baseline models and ablation\nexperiments, the SciRGC framework not only improves the accuracy and relevance\nof citation recommendations but also ensures the appropriateness of the\ngenerated citation sentences in context, providing a valuable tool for\ninterdisciplinary researchers."}
{"id": "2505.19203", "pdf": "https://arxiv.org/pdf/2505.19203", "abs": "https://arxiv.org/abs/2505.19203", "authors": ["Han Yin", "Yang Xiao", "Rohan Kumar Das", "Jisheng Bai", "Haohe Liu", "Wenwu Wang", "Mark D Plumbley"], "title": "EnvSDD: Benchmarking Environmental Sound Deepfake Detection", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Accepted by Interspeech 2025", "summary": "Audio generation systems now create very realistic soundscapes that can\nenhance media production, but also pose potential risks. Several studies have\nexamined deepfakes in speech or singing voice. However, environmental sounds\nhave different characteristics, which may make methods for detecting speech and\nsinging deepfakes less effective for real-world sounds. In addition, existing\ndatasets for environmental sound deepfake detection are limited in scale and\naudio types. To address this gap, we introduce EnvSDD, the first large-scale\ncurated dataset designed for this task, consisting of 45.25 hours of real and\n316.74 hours of fake audio. The test set includes diverse conditions to\nevaluate the generalizability, such as unseen generation models and unseen\ndatasets. We also propose an audio deepfake detection system, based on a\npre-trained audio foundation model. Results on EnvSDD show that our proposed\nsystem outperforms the state-of-the-art systems from speech and singing\ndomains."}
{"id": "2505.18374", "pdf": "https://arxiv.org/pdf/2505.18374", "abs": "https://arxiv.org/abs/2505.18374", "authors": ["Jarrod Ragsdale", "Rajendra Boppana"], "title": "ShIOEnv: A CLI Behavior-Capturing Environment Enabling Grammar-Guided Command Synthesis for Dataset Curation", "categories": ["cs.CL", "cs.LG"], "comment": "18 pages, 11 figures, conference preprint", "summary": "Command-line interfaces (CLIs) provide structured textual environments for\nsystem administration. Explorations have been performed using pre-trained\nlanguage models (PLMs) to simulate these environments for safe interaction in\nhigh-risk environments. However, their use has been constrained to frozen,\nlarge parameter models like GPT. For smaller architectures to reach a similar\nlevel of believability, a rich dataset of CLI interactions is required.\nExisting public datasets focus on mapping natural-language tasks to commands,\nomitting crucial execution data such as exit codes, outputs, and environmental\nside effects, limiting their usability for behavioral modeling. We introduce a\nShell Input -Output Environment (ShIOEnv), which casts command construction as\na Markov Decision Process whose state is the partially built sequence and whose\nactions append arguments. After each action, ShIOEnv executes the candidate and\nreturns its exit status, output, and progress toward a minimal-length\nbehavioral objective. Due to the intractable nature of the combinatorial\nargument state-action space, we derive a context-free grammar from man pages to\nmask invalid arguments from being emitted. We explore random and\nproximal-policy optimization (PPO)-optimized sampling of unrestricted and\ngrammar-masked action spaces to produce four exploration strategies. We\nobserved that grammar masking and PPO significantly improve sample efficiency\nto produce a higher quality dataset (maximizing the number of arguments while\nminimizing redundancies). Policy-generated datasets of shell input-output\nbehavior pairs are used to fine-tune CodeT5, where we observe 85% improvements\nin BLEU-4 when constraining the action space to grammar productions with an\nadditional 26% improvement when applying PPO. The ShIOEnv environment and\ncurated command behavior datasets are released for use in future research."}
{"id": "2505.20139", "pdf": "https://arxiv.org/pdf/2505.20139", "abs": "https://arxiv.org/abs/2505.20139", "authors": ["Jialin Yang", "Dongfu Jiang", "Lipeng He", "Sherman Siu", "Yuxuan Zhang", "Disen Liao", "Zhuofeng Li", "Huaye Zeng", "Yiming Jia", "Haozhe Wang", "Benjamin Schneider", "Chi Ruan", "Wentao Ma", "Zhiheng Lyu", "Yifei Wang", "Yi Lu", "Quy Duc Do", "Ziyan Jiang", "Ping Nie", "Wenhu Chen"], "title": "StructEval: Benchmarking LLMs' Capabilities to Generate Structural Outputs", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "16 pages, 9 figures, 13 tables", "summary": "As Large Language Models (LLMs) become integral to software development\nworkflows, their ability to generate structured outputs has become critically\nimportant. We introduce StructEval, a comprehensive benchmark for evaluating\nLLMs' capabilities in producing both non-renderable (JSON, YAML, CSV) and\nrenderable (HTML, React, SVG) structured formats. Unlike prior benchmarks,\nStructEval systematically evaluates structural fidelity across diverse formats\nthrough two paradigms: 1) generation tasks, producing structured output from\nnatural language prompts, and 2) conversion tasks, translating between\nstructured formats. Our benchmark encompasses 18 formats and 44 types of task,\nwith novel metrics for format adherence and structural correctness. Results\nreveal significant performance gaps, even state-of-the-art models like o1-mini\nachieve only 75.58 average score, with open-source alternatives lagging\napproximately 10 points behind. We find generation tasks more challenging than\nconversion tasks, and producing correct visual content more difficult than\ngenerating text-only structures."}
{"id": "2505.19205", "pdf": "https://arxiv.org/pdf/2505.19205", "abs": "https://arxiv.org/abs/2505.19205", "authors": ["Meher Bhaskar Madiraju", "Meher Sai Preetam Madiraju"], "title": "OptiMindTune: A Multi-Agent Framework for Intelligent Hyperparameter Optimization", "categories": ["cs.LG", "cs.AI", "cs.MA"], "comment": "7 pages, 2 tables", "summary": "Hyperparameter optimization (HPO) is a critical yet challenging aspect of\nmachine learning model development, significantly impacting model performance\nand generalization. Traditional HPO methods often struggle with high\ndimensionality, complex interdependencies, and computational expense. This\npaper introduces OptiMindTune, a novel multi-agent framework designed to\nintelligently and efficiently optimize hyperparameters. OptiMindTune leverages\nthe collaborative intelligence of three specialized AI agents -- a Recommender\nAgent, an Evaluator Agent, and a Decision Agent -- each powered by Google's\nGemini models. These agents address distinct facets of the HPO problem, from\nmodel selection and hyperparameter suggestion to robust evaluation and\nstrategic decision-making. By fostering dynamic interactions and knowledge\nsharing, OptiMindTune aims to converge to optimal hyperparameter configurations\nmore rapidly and robustly than existing single-agent or monolithic approaches.\nOur framework integrates principles from advanced large language models, and\nadaptive search to achieve scalable and intelligent AutoML. We posit that this\nmulti-agent paradigm offers a promising avenue for tackling the increasing\ncomplexity of modern machine learning model tuning."}
{"id": "2505.18377", "pdf": "https://arxiv.org/pdf/2505.18377", "abs": "https://arxiv.org/abs/2505.18377", "authors": ["Pingchuan Ma", "Ziang Yin", "Qi Jing", "Zhengqi Gao", "Nicholas Gangi", "Boyang Zhang", "Tsung-Wei Huang", "Zhaoran Huang", "Duane S. Boning", "Yu Yao", "Jiaqi Gu"], "title": "SP2RINT: Spatially-Decoupled Physics-Inspired Progressive Inverse Optimization for Scalable, PDE-Constrained Meta-Optical Neural Network Training", "categories": ["physics.optics", "cs.AI", "cs.LG"], "comment": null, "summary": "DONNs harness the physics of light propagation for efficient analog\ncomputation, with applications in AI and signal processing. Advances in\nnanophotonic fabrication and metasurface-based wavefront engineering have\nopened new pathways to realize high-capacity DONNs across various spectral\nregimes. Training such DONN systems to determine the metasurface structures\nremains challenging. Heuristic methods are fast but oversimplify metasurfaces\nmodulation, often resulting in physically unrealizable designs and significant\nperformance degradation. Simulation-in-the-loop training methods directly\noptimize a physically implementable metasurface using adjoint methods during\nend-to-end DONN training, but are inherently computationally prohibitive and\nunscalable.To address these limitations, we propose SP2RINT, a spatially\ndecoupled, progressive training framework that formulates DONN training as a\nPDE-constrained learning problem. Metasurface responses are first relaxed into\nfreely trainable transfer matrices with a banded structure. We then\nprogressively enforce physical constraints by alternating between transfer\nmatrix training and adjoint-based inverse design, avoiding per-iteration PDE\nsolves while ensuring final physical realizability. To further reduce runtime,\nwe introduce a physics-inspired, spatially decoupled inverse design strategy\nbased on the natural locality of field interactions. This approach partitions\nthe metasurface into independently solvable patches, enabling scalable and\nparallel inverse design with system-level calibration. Evaluated across diverse\nDONN training tasks, SP2RINT achieves digital-comparable accuracy while being\n1825 times faster than simulation-in-the-loop approaches. By bridging the gap\nbetween abstract DONN models and implementable photonic hardware, SP2RINT\nenables scalable, high-performance training of physically realizable\nmeta-optical neural systems."}
{"id": "2505.20152", "pdf": "https://arxiv.org/pdf/2505.20152", "abs": "https://arxiv.org/abs/2505.20152", "authors": ["Kai Sun", "Yushi Bai", "Zhen Yang", "Jiajie Zhang", "Ji Qi", "Lei Hou", "Juanzi Li"], "title": "Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Benefiting from contrastively trained visual encoders on large-scale natural\nscene images, Large Multimodal Models (LMMs) have achieved remarkable\nperformance across various visual perception tasks. However, the inherent\nlimitations of contrastive learning upon summarized descriptions fundamentally\nrestrict the capabilities of models in meticulous reasoning, particularly in\ncrucial scenarios of geometric problem-solving. To enhance geometric\nunderstanding, we propose a novel hard negative contrastive learning framework\nfor the vision encoder, which combines image-based contrastive learning using\ngeneration-based hard negatives created by perturbing diagram generation code,\nand text-based contrastive learning using rule-based negatives derived from\nmodified geometric descriptions and retrieval-based negatives selected based on\ncaption similarity. We train CLIP using our strong negative learning method,\nnamely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for\ngeometric problem-solving. Experiments show that our trained model, MMGeoLM,\nsignificantly outperforms other open-source models on three geometric reasoning\nbenchmarks. Even with a size of 7B, it can rival powerful closed-source models\nlike GPT-4o. We further study the impact of different negative sample\nconstruction methods and the number of negative samples on the geometric\nreasoning performance of LMM, yielding fruitful conclusions. The code and\ndataset are available at https://github.com/THU-KEG/MMGeoLM."}
{"id": "2505.19209", "pdf": "https://arxiv.org/pdf/2505.19209", "abs": "https://arxiv.org/abs/2505.19209", "authors": ["Zonglin Yang", "Wanhao Liu", "Ben Gao", "Yujie Liu", "Wei Li", "Tong Xie", "Lidong Bing", "Wanli Ouyang", "Erik Cambria", "Dongzhan Zhou"], "title": "MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search", "categories": ["cs.CL", "cs.AI", "cs.CE", "stat.ML"], "comment": null, "summary": "Large language models (LLMs) have shown promise in automating scientific\nhypothesis generation, yet existing approaches primarily yield coarse-grained\nhypotheses lacking critical methodological and experimental details. We\nintroduce and formally define the novel task of fine-grained scientific\nhypothesis discovery, which entails generating detailed, experimentally\nactionable hypotheses from coarse initial research directions. We frame this as\na combinatorial optimization problem and investigate the upper limits of LLMs'\ncapacity to solve it when maximally leveraged. Specifically, we explore four\nfoundational questions: (1) how to best harness an LLM's internal heuristics to\nformulate the fine-grained hypothesis it itself would judge as the most\npromising among all the possible hypotheses it might generate, based on its own\ninternal scoring-thus defining a latent reward landscape over the hypothesis\nspace; (2) whether such LLM-judged better hypotheses exhibit stronger alignment\nwith ground-truth hypotheses; (3) whether shaping the reward landscape using an\nensemble of diverse LLMs of similar capacity yields better outcomes than\ndefining it with repeated instances of the strongest LLM among them; and (4)\nwhether an ensemble of identical LLMs provides a more reliable reward landscape\nthan a single LLM. To address these questions, we propose a hierarchical search\nmethod that incrementally proposes and integrates details into the hypothesis,\nprogressing from general concepts to specific experimental configurations. We\nshow that this hierarchical process smooths the reward landscape and enables\nmore effective optimization. Empirical evaluations on a new benchmark of\nexpert-annotated fine-grained hypotheses from recent chemistry literature show\nthat our method consistently outperforms strong baselines."}
{"id": "2505.18380", "pdf": "https://arxiv.org/pdf/2505.18380", "abs": "https://arxiv.org/abs/2505.18380", "authors": ["Praphul Singh", "Charlotte Dzialo", "Jangwon Kim", "Sumana Srivatsa", "Irfan Bulu", "Sri Gadde", "Krishnaram Kenthapadi"], "title": "RedactOR: An LLM-Powered Framework for Automatic Clinical Data De-Identification", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Industry Track. To appear", "summary": "Ensuring clinical data privacy while preserving utility is critical for\nAI-driven healthcare and data analytics. Existing de-identification (De-ID)\nmethods, including rule-based techniques, deep learning models, and large\nlanguage models (LLMs), often suffer from recall errors, limited\ngeneralization, and inefficiencies, limiting their real-world applicability. We\npropose a fully automated, multi-modal framework, RedactOR for de-identifying\nstructured and unstructured electronic health records, including clinical audio\nrecords. Our framework employs cost-efficient De-ID strategies, including\nintelligent routing, hybrid rule and LLM based approaches, and a two-step audio\nredaction approach. We present a retrieval-based entity relexicalization\napproach to ensure consistent substitutions of protected entities, thereby\nenhancing data coherence for downstream applications. We discuss key design\ndesiderata, de-identification and relexicalization methodology, and modular\narchitecture of RedactX and its integration with the Oracle Health Clinical AI\nsystem. Evaluated on the i2b2 2014 De-ID dataset using standard metrics with\nstrict recall, our approach achieves competitive performance while optimizing\ntoken usage to reduce LLM costs. Finally, we discuss key lessons and insights\nfrom deployment in real-world AI- driven healthcare data pipelines."}
{"id": "2505.20161", "pdf": "https://arxiv.org/pdf/2505.20161", "abs": "https://arxiv.org/abs/2505.20161", "authors": ["Jaehun Jung", "Seungju Han", "Ximing Lu", "Skyler Hallinan", "David Acuna", "Shrimai Prabhumoye", "Mostafa Patwary", "Mohammad Shoeybi", "Bryan Catanzaro", "Yejin Choi"], "title": "Prismatic Synthesis: Gradient-based Data Diversification Boosts Generalization in LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Effective generalization in language models depends critically on the\ndiversity of their training data. Yet existing diversity metrics often fall\nshort of this goal, relying on surface-level heuristics that are decoupled from\nmodel behavior. This motivates us to ask: What kind of diversity in training\ndata actually drives generalization in language models -- and how can we\nmeasure and amplify it? Through large-scale empirical analyses spanning over\n300 training runs, carefully controlled for data scale and quality, we show\nthat data diversity can be a strong predictor of generalization in LLM\nreasoning -- as measured by average model performance on unseen\nout-of-distribution benchmarks. We introduce G-Vendi, a metric that quantifies\ndiversity via the entropy of model-induced gradients. Despite using a small\noff-the-shelf proxy model for gradients, G-Vendi consistently outperforms\nalternative measures, achieving strong correlation (Spearman's $\\rho \\approx\n0.9$) with out-of-distribution (OOD) performance on both natural language\ninference (NLI) and math reasoning tasks. Building on this insight, we present\nPrismatic Synthesis, a framework for generating diverse synthetic data by\ntargeting underrepresented regions in gradient space. Experimental results show\nthat Prismatic Synthesis consistently improves model performance as we scale\nsynthetic data -- not just on in-distribution test but across unseen,\nout-of-distribution benchmarks -- significantly outperforming state-of-the-art\nmodels that rely on 20 times larger data generator than ours. For example,\nPrismMath-7B, our model distilled from a 32B LLM, outperforms\nR1-Distill-Qwen-7B -- the same base model trained on proprietary data generated\nby 671B R1 -- on 6 out of 7 challenging benchmarks."}
{"id": "2505.19212", "pdf": "https://arxiv.org/pdf/2505.19212", "abs": "https://arxiv.org/abs/2505.19212", "authors": ["Steffen Backmann", "David Guzman Piedrahita", "Emanuel Tewolde", "Rada Mihalcea", "Bernhard Schlkopf", "Zhijing Jin"], "title": "When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled their use in\ncomplex agentic roles, involving decision-making with humans or other agents,\nmaking ethical alignment a key AI safety concern. While prior work has examined\nboth LLMs' moral judgment and strategic behavior in social dilemmas, there is\nlimited understanding of how they act when moral imperatives directly conflict\nwith rewards or incentives. To investigate this, we introduce Moral Behavior in\nSocial Dilemma Simulation (MoralSim) and evaluate how LLMs behave in the\nprisoner's dilemma and public goods game with morally charged contexts. In\nMoralSim, we test a range of frontier models across both game structures and\nthree distinct moral framings, enabling a systematic examination of how LLMs\nnavigate social dilemmas in which ethical norms conflict with payoff-maximizing\nstrategies. Our results show substantial variation across models in both their\ngeneral tendency to act morally and the consistency of their behavior across\ngame types, the specific moral framing, and situational factors such as\nopponent behavior and survival risks. Crucially, no model exhibits consistently\nmoral behavior in MoralSim, highlighting the need for caution when deploying\nLLMs in agentic roles where the agent's \"self-interest\" may conflict with\nethical expectations. Our code is available at\nhttps://github.com/sbackmann/moralsim."}
{"id": "2505.18397", "pdf": "https://arxiv.org/pdf/2505.18397", "abs": "https://arxiv.org/abs/2505.18397", "authors": ["Fangqiao Tian", "An Luo", "Jin Du", "Xun Xian", "Robert Specht", "Ganghua Wang", "Xuan Bi", "Jiawei Zhou", "Jayanth Srinivasa", "Ashish Kundu", "Charles Fleming", "Rui Zhang", "Zirui Liu", "Mingyi Hong", "Jie Ding"], "title": "An Outlook on the Opportunities and Challenges of Multi-Agent AI Systems", "categories": ["cs.MA", "cs.AI", "cs.ET", "cs.LG", "68T42 (Agent technology and artificial intelligence), 68T01 (General\n  topics in artificial intelligence), 68M14 (Distributed systems)", "I.2.11; I.2.4; I.2.6"], "comment": null, "summary": "Multi-agent AI systems (MAS) offer a promising framework for distributed\nintelligence, enabling collaborative reasoning, planning, and decision-making\nacross autonomous agents. This paper provides a systematic outlook on the\ncurrent opportunities and challenges of MAS, drawing insights from recent\nadvances in large language models (LLMs), federated optimization, and human-AI\ninteraction. We formalize key concepts including agent topology, coordination\nprotocols, and shared objectives, and identify major risks such as dependency,\nmisalignment, and vulnerabilities arising from training data overlap. Through a\nbiologically inspired simulation and comprehensive theoretical framing, we\nhighlight critical pathways for developing robust, scalable, and secure MAS in\nreal-world settings."}
{"id": "2505.20166", "pdf": "https://arxiv.org/pdf/2505.20166", "abs": "https://arxiv.org/abs/2505.20166", "authors": ["Chun-Yi Kuan", "Hung-yi Lee"], "title": "From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "comment": "Project Website: https://kuan2jiu99.github.io/Balsa", "summary": "Audio-aware large language models (ALLMs) have recently made great strides in\nunderstanding and processing audio inputs. These models are typically adapted\nfrom text-based large language models (LLMs) through additional training on\naudio-related tasks. However, this adaptation process presents two major\nlimitations. First, ALLMs often suffer from catastrophic forgetting, where\nimportant textual capabilities such as instruction-following are lost after\ntraining on audio data. In some cases, models may even hallucinate sounds that\nare not present in the input audio, raising concerns about their reliability.\nSecond, achieving cross-modal alignment between audio and language typically\nrelies on large collections of task-specific question-answer pairs for\ninstruction tuning, making the process resource-intensive. To address these\nissues, we leverage the backbone LLMs from ALLMs to synthesize general-purpose\ncaption-style alignment data. We refer to this process as bootstrapping\naudio-language alignment via synthetic data generation from backbone LLMs\n(BALSa). Building on BALSa, we introduce LISTEN (Learning to Identify Sounds\nThrough Extended Negative Samples), a contrastive-like training method designed\nto improve ALLMs' ability to distinguish between present and absent sounds. We\nfurther extend BALSa to multi-audio scenarios, where the model either explains\nthe differences between audio inputs or produces a unified caption that\ndescribes them all, thereby enhancing audio-language alignment. Experimental\nresults indicate that our method effectively mitigates audio hallucinations\nwhile reliably maintaining strong performance in audio understanding,\nreasoning, and instruction-following skills. Moreover, incorporating\nmulti-audio training further enhances the model's comprehension and reasoning\ncapabilities. Overall, BALSa offers an efficient and scalable approach to the\ndevelopment of ALLMs."}
{"id": "2505.19233", "pdf": "https://arxiv.org/pdf/2505.19233", "abs": "https://arxiv.org/abs/2505.19233", "authors": ["Aniruddha Mukherjee", "Spriha Dubey", "Somdyuti Paul"], "title": "RAISE: Realness Assessment for Image Synthesis and Evaluation", "categories": ["cs.CV", "cs.AI", "cs.MM", "eess.IV"], "comment": null, "summary": "The rapid advancement of generative AI has enabled the creation of highly\nphotorealistic visual content, offering practical substitutes for real images\nand videos in scenarios where acquiring real data is difficult or expensive.\nHowever, reliably substituting real visual content with AI-generated\ncounterparts requires robust assessment of the perceived realness of\nAI-generated visual content, a challenging task due to its inherent subjective\nnature. To address this, we conducted a comprehensive human study evaluating\nthe perceptual realness of both real and AI-generated images, resulting in a\nnew dataset, containing images paired with subjective realness scores,\nintroduced as RAISE in this paper. Further, we develop and train multiple\nmodels on RAISE to establish baselines for realness prediction. Our\nexperimental results demonstrate that features derived from deep foundation\nvision models can effectively capture the subjective realness. RAISE thus\nprovides a valuable resource for developing robust, objective models of\nperceptual realness assessment."}
{"id": "2505.18399", "pdf": "https://arxiv.org/pdf/2505.18399", "abs": "https://arxiv.org/abs/2505.18399", "authors": ["Lin Zhao", "Yushu Wu", "Xinru Jiang", "Jianyang Gu", "Yanzhi Wang", "Xiaolin Xu", "Pu Zhao", "Xue Lin"], "title": "Taming Diffusion for Dataset Distillation with High Representativeness", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "The paper is accepted by ICML 2025", "summary": "Recent deep learning models demand larger datasets, driving the need for\ndataset distillation to create compact, cost-efficient datasets while\nmaintaining performance. Due to the powerful image generation capability of\ndiffusion, it has been introduced to this field for generating distilled\nimages. In this paper, we systematically investigate issues present in current\ndiffusion-based dataset distillation methods, including inaccurate distribution\nmatching, distribution deviation with random noise, and separate sampling.\nBuilding on this, we propose D^3HR, a novel diffusion-based framework to\ngenerate distilled datasets with high representativeness. Specifically, we\nadopt DDIM inversion to map the latents of the full dataset from a\nlow-normality latent domain to a high-normality Gaussian domain, preserving\ninformation and ensuring structural consistency to generate representative\nlatents for the distilled dataset. Furthermore, we propose an efficient\nsampling scheme to better align the representative latents with the\nhigh-normality Gaussian distribution. Our comprehensive experiments demonstrate\nthat D^3HR can achieve higher accuracy across different model architectures\ncompared with state-of-the-art baselines in dataset distillation. Source code:\nhttps://github.com/lin-zhao-resoLve/D3HR."}
{"id": "2505.20246", "pdf": "https://arxiv.org/pdf/2505.20246", "abs": "https://arxiv.org/abs/2505.20246", "authors": ["Jiahao Qiu", "Fulian Xiao", "Yimin Wang", "Yuchen Mao", "Yijia Chen", "Xinzhe Juan", "Siran Wang", "Xuan Qi", "Tongcheng Zhang", "Zixin Yao", "Jiacheng Guo", "Yifu Lu", "Charles Argon", "Jundi Cui", "Daixin Chen", "Junran Zhou", "Shuyao Zhou", "Zhanpeng Zhou", "Ling Yang", "Shilong Liu", "Hongru Wang", "Kaixuan Huang", "Xun Jiang", "Yuming Cao", "Yue Chen", "Yunfei Chen", "Zhengyi Chen", "Ruowei Dai", "Mengqiu Deng", "Jiye Fu", "Yunting Gu", "Zijie Guan", "Zirui Huang", "Xiaoyan Ji", "Yumeng Jiang", "Delong Kong", "Haolong Li", "Jiaqi Li", "Ruipeng Li", "Tianze Li", "Zhuoran Li", "Haixia Lian", "Mengyue Lin", "Xudong Liu", "Jiayi Lu", "Jinghan Lu", "Wanyu Luo", "Ziyue Luo", "Zihao Pu", "Zhi Qiao", "Ruihuan Ren", "Liang Wan", "Ruixiang Wang", "Tianhui Wang", "Yang Wang", "Zeyu Wang", "Zihua Wang", "Yujia Wu", "Zhaoyi Wu", "Hao Xin", "Weiao Xing", "Ruojun Xiong", "Weijie Xu", "Yao Shu", "Xiao Yao", "Xiaorui Yang", "Yuchen Yang", "Nan Yi", "Jiadong Yu", "Yangyuxuan Yu", "Huiting Zeng", "Danni Zhang", "Yunjie Zhang", "Zhaoyu Zhang", "Zhiheng Zhang", "Xiaofeng Zheng", "Peirong Zhou", "Linyan Zhong", "Xiaoyin Zong", "Ying Zhao", "Zhenxin Chen", "Lin Ding", "Xiaoyu Gao", "Bingbing Gong", "Yichao Li", "Yang Liao", "Guang Ma", "Tianyuan Ma", "Xinrui Sun", "Tianyi Wang", "Han Xia", "Ruobing Xian", "Gen Ye", "Tengfei Yu", "Wentao Zhang", "Yuxi Wang", "Xi Gao", "Mengdi Wang"], "title": "On Path to Multimodal Historical Reasoning: HistBench and HistAgent", "categories": ["cs.AI", "cs.CL"], "comment": "17 pages, 7 figures", "summary": "Recent advances in large language models (LLMs) have led to remarkable\nprogress across domains, yet their capabilities in the humanities, particularly\nhistory, remain underexplored. Historical reasoning poses unique challenges for\nAI, involving multimodal source interpretation, temporal inference, and\ncross-linguistic analysis. While general-purpose agents perform well on many\nexisting benchmarks, they lack the domain-specific expertise required to engage\nwith historical materials and questions. To address this gap, we introduce\nHistBench, a new benchmark of 414 high-quality questions designed to evaluate\nAI's capacity for historical reasoning and authored by more than 40 expert\ncontributors. The tasks span a wide range of historical problems-from factual\nretrieval based on primary sources to interpretive analysis of manuscripts and\nimages, to interdisciplinary challenges involving archaeology, linguistics, or\ncultural history. Furthermore, the benchmark dataset spans 29 ancient and\nmodern languages and covers a wide range of historical periods and world\nregions. Finding the poor performance of LLMs and other agents on HistBench, we\nfurther present HistAgent, a history-specific agent equipped with carefully\ndesigned tools for OCR, translation, archival search, and image understanding\nin History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of\n27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online\nsearch and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%)\nand Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These\nresults highlight the limitations of existing LLMs and generalist agents and\ndemonstrate the advantages of HistAgent for historical reasoning."}
{"id": "2505.19238", "pdf": "https://arxiv.org/pdf/2505.19238", "abs": "https://arxiv.org/abs/2505.19238", "authors": ["Sourav Ganguly", "Arnob Ghosh", "Kishan Panaganti", "Adam Wierman"], "title": "Efficient Policy Optimization in Robust Constrained MDPs with Iteration Complexity Guarantees", "categories": ["cs.LG", "cs.AI", "cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Constrained decision-making is essential for designing safe policies in\nreal-world control systems, yet simulated environments often fail to capture\nreal-world adversities. We consider the problem of learning a policy that will\nmaximize the cumulative reward while satisfying a constraint, even when there\nis a mismatch between the real model and an accessible simulator/nominal model.\nIn particular, we consider the robust constrained Markov decision problem\n(RCMDP) where an agent needs to maximize the reward and satisfy the constraint\nagainst the worst possible stochastic model under the uncertainty set centered\naround an unknown nominal model. Primal-dual methods, effective for standard\nconstrained MDP (CMDP), are not applicable here because of the lack of the\nstrong duality property. Further, one cannot apply the standard robust\nvalue-iteration based approach on the composite value function either as the\nworst case models may be different for the reward value function and the\nconstraint value function. We propose a novel technique that effectively\nminimizes the constraint value function--to satisfy the constraints; on the\nother hand, when all the constraints are satisfied, it can simply maximize the\nrobust reward value function. We prove that such an algorithm finds a policy\nwith at most $\\epsilon$ sub-optimality and feasible policy after\n$O(\\epsilon^{-2})$ iterations. In contrast to the state-of-the-art method, we\ndo not need to employ a binary search, thus, we reduce the computation time by\nat least 4x for smaller value of discount factor ($\\gamma$) and by at least 6x\nfor larger value of $\\gamma$."}
{"id": "2505.18410", "pdf": "https://arxiv.org/pdf/2505.18410", "abs": "https://arxiv.org/abs/2505.18410", "authors": ["Seunghyun Lee", "Yuqi Gu"], "title": "Identifiability of latent causal graphical models without pure children", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "This paper considers a challenging problem of identifying a causal graphical\nmodel under the presence of latent variables. While various identifiability\nconditions have been proposed in the literature, they often require multiple\npure children per latent variable or restrictions on the latent causal graph.\nFurthermore, it is common for all observed variables to exhibit the same\nmodality. Consequently, the existing identifiability conditions are often too\nstringent for complex real-world data. We consider a general nonparametric\nmeasurement model with arbitrary observed variable types and binary latent\nvariables, and propose a double triangular graphical condition that guarantees\nidentifiability of the entire causal graphical model. The proposed condition\nsignificantly relaxes the popular pure children condition. We also establish\nnecessary conditions for identifiability and provide valuable insights into\nfundamental limits of identifiability. Simulation studies verify that latent\nstructures satisfying our conditions can be accurately estimated from data."}
{"id": "2505.20251", "pdf": "https://arxiv.org/pdf/2505.20251", "abs": "https://arxiv.org/abs/2505.20251", "authors": ["Sophia Hager", "Aleem Khan", "Andrew Wang", "Nicholas Andrews"], "title": "Learning Extrapolative Sequence Transformations from Markov Chains", "categories": ["cs.LG", "cs.CL"], "comment": "To be published at the Forty-Second International Conference on\n  Machine Learning", "summary": "Most successful applications of deep learning involve similar training and\ntest conditions. However, tasks such as biological sequence design involve\nsearching for sequences that improve desirable properties beyond previously\nknown values, which requires novel hypotheses that \\emph{extrapolate} beyond\ntraining data. In these settings, extrapolation may be achieved by using random\nsearch methods such as Markov chain Monte Carlo (MCMC), which, given an initial\nstate, sample local transformations to approximate a target density that\nrewards states with the desired properties. However, even with a well-designed\nproposal, MCMC may struggle to explore large structured state spaces\nefficiently. Rather than relying on stochastic search, it would be desirable to\nhave a model that greedily optimizes the properties of interest, successfully\nextrapolating in as few steps as possible. We propose to learn such a model\nfrom the Markov chains resulting from MCMC search. Specifically, our approach\nuses selected states from Markov chains as a source of training data for an\nautoregressive model, which is then able to efficiently generate novel\nsequences that extrapolate along the sequence-level properties of interest. The\nproposed approach is validated on three problems: protein sequence design, text\nsentiment control, and text anonymization. We find that the autoregressive\nmodel can extrapolate as well or better than MCMC, but with the additional\nbenefits of scalability and significantly higher sample efficiency."}
{"id": "2505.19240", "pdf": "https://arxiv.org/pdf/2505.19240", "abs": "https://arxiv.org/abs/2505.19240", "authors": ["Aida Kostikova", "Zhipin Wang", "Deidamea Bajri", "Ole Ptz", "Benjamin Paaen", "Steffen Eger"], "title": "LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This manuscript is currently under review at ACM Computing Surveys", "summary": "Large language model (LLM) research has grown rapidly, along with increasing\nconcern about their limitations such as failures in reasoning, hallucinations,\nand limited multilingual capability. In this survey, we conduct a data-driven,\nsemi-automated review of research on limitations of LLM (LLLMs) from 2022 to\n2024 using a bottom-up approach. From a corpus of 250,000 ACL and arXiv papers,\nwe identify 14,648 relevant papers using keyword filtering, LLM-based\nclassification, validated against expert labels, and topic clustering (via two\napproaches, HDBSCAN+BERTopic and LlooM). We find that LLM-related research\nincreases over fivefold in ACL and fourfold in arXiv. Since 2022, LLLMs\nresearch grows even faster, reaching over 30% of LLM papers by late 2024.\nReasoning remains the most studied limitation, followed by generalization,\nhallucination, bias, and security. The distribution of topics in the ACL\ndataset stays relatively stable over time, while arXiv shifts toward safety and\ncontrollability (with topics like security risks, alignment, hallucinations,\nknowledge editing), and multimodality between 2022 and 2024. We release a\ndataset of annotated abstracts and a validated methodology, and offer a\nquantitative view of trends in LLM limitations research."}
{"id": "2505.18411", "pdf": "https://arxiv.org/pdf/2505.18411", "abs": "https://arxiv.org/abs/2505.18411", "authors": ["Yue Jiang", "Jichu Li", "Yang Liu", "Dingkang Yang", "Feng Zhou", "Quyu Kong"], "title": "DanmakuTPPBench: A Multi-modal Benchmark for Temporal Point Process Modeling and Understanding", "categories": ["cs.CL", "cs.LG"], "comment": "https://github.com/FRENKIE-CHIANG/DanmakuTPPBench", "summary": "We introduce DanmakuTPPBench, a comprehensive benchmark designed to advance\nmulti-modal Temporal Point Process (TPP) modeling in the era of Large Language\nModels (LLMs). While TPPs have been widely studied for modeling temporal event\nsequences, existing datasets are predominantly unimodal, hindering progress in\nmodels that require joint reasoning over temporal, textual, and visual\ninformation. To address this gap, DanmakuTPPBench comprises two complementary\ncomponents: (1) DanmakuTPP-Events, a novel dataset derived from the Bilibili\nvideo platform, where user-generated bullet comments (Danmaku) naturally form\nmulti-modal events annotated with precise timestamps, rich textual content, and\ncorresponding video frames; (2) DanmakuTPP-QA, a challenging question-answering\ndataset constructed via a novel multi-agent pipeline powered by\nstate-of-the-art LLMs and multi-modal LLMs (MLLMs), targeting complex\ntemporal-textual-visual reasoning. We conduct extensive evaluations using both\nclassical TPP models and recent MLLMs, revealing significant performance gaps\nand limitations in current methods' ability to model multi-modal event\ndynamics. Our benchmark establishes strong baselines and calls for further\nintegration of TPP modeling into the multi-modal language modeling landscape.\nThe code and dataset have been released at\nhttps://github.com/FRENKIE-CHIANG/DanmakuTPPBench"}
{"id": "2505.20254", "pdf": "https://arxiv.org/pdf/2505.20254", "abs": "https://arxiv.org/abs/2505.20254", "authors": ["Xiangchen Song", "Aashiq Muhamed", "Yujia Zheng", "Lingjing Kong", "Zeyu Tang", "Mona T. Diab", "Virginia Smith", "Kun Zhang"], "title": "Position: Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Sparse Autoencoders (SAEs) are a prominent tool in mechanistic\ninterpretability (MI) for decomposing neural network activations into\ninterpretable features. However, the aspiration to identify a canonical set of\nfeatures is challenged by the observed inconsistency of learned SAE features\nacross different training runs, undermining the reliability and efficiency of\nMI research. This position paper argues that mechanistic interpretability\nshould prioritize feature consistency in SAEs -- the reliable convergence to\nequivalent feature sets across independent runs. We propose using the Pairwise\nDictionary Mean Correlation Coefficient (PW-MCC) as a practical metric to\noperationalize consistency and demonstrate that high levels are achievable\n(0.80 for TopK SAEs on LLM activations) with appropriate architectural choices.\nOur contributions include detailing the benefits of prioritizing consistency;\nproviding theoretical grounding and synthetic validation using a model\norganism, which verifies PW-MCC as a reliable proxy for ground-truth recovery;\nand extending these findings to real-world LLM data, where high feature\nconsistency strongly correlates with the semantic similarity of learned feature\nexplanations. We call for a community-wide shift towards systematically\nmeasuring feature consistency to foster robust cumulative progress in MI."}
{"id": "2505.19241", "pdf": "https://arxiv.org/pdf/2505.19241", "abs": "https://arxiv.org/abs/2505.19241", "authors": ["Xiaoqiang Lin", "Arun Verma", "Zhongxiang Dai", "Daniela Rus", "See-Kiong Ng", "Bryan Kian Hsiang Low"], "title": "ActiveDPO: Active Direct Preference Optimization for Sample-Efficient Alignment", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The recent success of using human preferences to align large language models\n(LLMs) has significantly improved their performance in various downstream tasks\nlike question answering, mathematical reasoning, and code generation. However,3\nachieving effective LLM alignment depends on high-quality human preference\ndatasets. Collecting these datasets requires human preference annotation, which\nis costly and resource-intensive, necessitating efficient active data selection\nmethods. Existing methods either lack a strong theoretical foundation or depend\non restrictive reward function assumptions (e.g., linearity). To this end, we\npropose an algorithm, ActiveDPO, that uses a theoretically grounded data\nselection criterion for non-linear reward functions while directly leveraging\nthe LLM itself to parameterize the reward model that is used for active data\nselection. As a result, ActiveDPO explicitly accounts for the influence of LLM\non data selection, unlike methods that select the data without considering the\nLLM that is being aligned, thereby leading to more effective and efficient data\ncollection. Extensive experiments show that ActiveDPO outperforms existing\nmethods across various models and datasets."}
{"id": "2505.18417", "pdf": "https://arxiv.org/pdf/2505.18417", "abs": "https://arxiv.org/abs/2505.18417", "authors": ["Achkan Salehi"], "title": "Reinforcement Learning for Ballbot Navigation in Uneven Terrain", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "6 pages, 8 figures, 2 tables", "summary": "Ballbot (i.e. Ball balancing robot) navigation usually relies on methods\nrooted in control theory (CT), and works that apply Reinforcement learning (RL)\nto the problem remain rare while generally being limited to specific subtasks\n(e.g. balance recovery). Unlike CT based methods, RL does not require\n(simplifying) assumptions about environment dynamics (e.g. the absence of\nslippage between the ball and the floor). In addition to this increased\naccuracy in modeling, RL agents can easily be conditioned on additional\nobservations such as depth-maps without the need for explicit formulations from\nfirst principles, leading to increased adaptivity. Despite those advantages,\nthere has been little to no investigation into the capabilities,\ndata-efficiency and limitations of RL based methods for ballbot control and\nnavigation. Furthermore, there is a notable absence of an open-source,\nRL-friendly simulator for this task. In this paper, we present an open-source\nballbot simulation based on MuJoCo, and show that with appropriate conditioning\non exteroceptive observations as well as reward shaping, policies learned by\nclassical model-free RL methods are capable of effectively navigating through\nrandomly generated uneven terrain, using a reasonable amount of data (four to\nfive hours on a system operating at 500hz)."}
{"id": "2505.20259", "pdf": "https://arxiv.org/pdf/2505.20259", "abs": "https://arxiv.org/abs/2505.20259", "authors": ["Haoyu Wang", "Zeyu Qin", "Yifei Zhao", "Chao Du", "Min Lin", "Xueqian Wang", "Tianyu Pang"], "title": "Lifelong Safety Alignment for Language Models", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "LLMs have made impressive progress, but their growing capabilities also\nexpose them to highly flexible jailbreaking attacks designed to bypass safety\nalignment. While many existing defenses focus on known types of attacks, it is\nmore critical to prepare LLMs for unseen attacks that may arise during\ndeployment. To address this, we propose a lifelong safety alignment framework\nthat enables LLMs to continuously adapt to new and evolving jailbreaking\nstrategies. Our framework introduces a competitive setup between two\ncomponents: a Meta-Attacker, trained to actively discover novel jailbreaking\nstrategies, and a Defender, trained to resist them. To effectively warm up the\nMeta-Attacker, we first leverage the GPT-4o API to extract key insights from a\nlarge collection of jailbreak-related research papers. Through iterative\ntraining, the first iteration Meta-Attacker achieves a 73% attack success rate\n(ASR) on RR and a 57% transfer ASR on LAT using only single-turn attacks.\nMeanwhile, the Defender progressively improves its robustness and ultimately\nreduces the Meta-Attacker's success rate to just 7%, enabling safer and more\nreliable deployment of LLMs in open-ended environments. The code is available\nat https://github.com/sail-sg/LifelongSafetyAlignment."}
{"id": "2505.19245", "pdf": "https://arxiv.org/pdf/2505.19245", "abs": "https://arxiv.org/abs/2505.19245", "authors": ["Kevin Xu", "Issei Sato"], "title": "To CoT or To Loop? A Formal Comparison Between Chain-of-Thought and Looped Transformers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Chain-of-Thought (CoT) and Looped Transformers have been shown to empirically\nimprove performance on reasoning tasks and to theoretically enhance\nexpressivity by recursively increasing the number of computational steps.\nHowever, their comparative capabilities are still not well understood. In this\npaper, we provide a formal analysis of their respective strengths and\nlimitations. We show that Looped Transformers can efficiently simulate parallel\ncomputations for deterministic tasks, which we formalize as evaluation over\ndirected acyclic graphs. In contrast, CoT with stochastic decoding excels at\napproximate inference for compositional structures, namely self-reducible\nproblems. These separations suggest the tasks for which depth-driven recursion\nis more suitable, thereby offering practical cues for choosing between\nreasoning paradigms."}
{"id": "2505.18420", "pdf": "https://arxiv.org/pdf/2505.18420", "abs": "https://arxiv.org/abs/2505.18420", "authors": ["Harsh Vardhan", "Heng Zhu", "Avishek Ghosh", "Arya Mazumdar"], "title": "LocalKMeans: Convergence of Lloyd's Algorithm with Distributed Local Iterations", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "In this paper, we analyze the classical $K$-means alternating-minimization\nalgorithm, also known as Lloyd's algorithm (Lloyd, 1956), for a mixture of\nGaussians in a data-distributed setting that incorporates local iteration\nsteps. Assuming unlabeled data distributed across multiple machines, we propose\nan algorithm, LocalKMeans, that performs Lloyd's algorithm in parallel in the\nmachines by running its iterations on local data, synchronizing only every $L$\nof such local steps. We characterize the cost of these local iterations against\nthe non-distributed setting, and show that the price paid for the local steps\nis a higher required signal-to-noise ratio. While local iterations were\ntheoretically studied in the past for gradient-based learning methods, the\nanalysis of unsupervised learning methods is more involved owing to the\npresence of latent variables, e.g. cluster identities, than that of an\niterative gradient-based algorithm. To obtain our results, we adapt a virtual\niterate method to work with a non-convex, non-smooth objective function, in\nconjunction with a tight statistical analysis of Lloyd steps."}
{"id": "2505.20278", "pdf": "https://arxiv.org/pdf/2505.20278", "abs": "https://arxiv.org/abs/2505.20278", "authors": ["Hoyeon Chang", "Jinho Park", "Hanseul Cho", "Sohee Yang", "Miyoung Ko", "Hyeonbin Hwang", "Seungpil Won", "Dohaeng Lee", "Youbin Ahn", "Minjoon Seo"], "title": "The Coverage Principle: A Framework for Understanding Compositional Generalization", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.6"], "comment": null, "summary": "Large language models excel at pattern matching, yet often fall short in\nsystematic compositional generalization. We propose the coverage principle: a\ndata-centric framework showing that models relying primarily on pattern\nmatching for compositional tasks cannot reliably generalize beyond substituting\nfragments that yield identical results when used in the same contexts. We\ndemonstrate that this framework has a strong predictive power for the\ngeneralization capabilities of Transformers. First, we derive and empirically\nconfirm that the training data required for two-hop generalization grows at\nleast quadratically with the token set size, and the training data efficiency\ndoes not improve with 20x parameter scaling. Second, for compositional tasks\nwith path ambiguity where one variable affects the output through multiple\ncomputational paths, we show that Transformers learn context-dependent state\nrepresentations that undermine both performance and interoperability. Third,\nChain-of-Thought supervision improves training data efficiency for multi-hop\ntasks but still struggles with path ambiguity. Finally, we outline a\n\\emph{mechanism-based} taxonomy that distinguishes three ways neural networks\ncan generalize: structure-based (bounded by coverage), property-based\n(leveraging algebraic invariances), and shared-operator (through function\nreuse). This conceptual lens contextualizes our results and highlights where\nnew architectural ideas are needed to achieve systematic compositionally.\nOverall, the coverage principle provides a unified lens for understanding\ncompositional reasoning, and underscores the need for fundamental architectural\nor training innovations to achieve truly systematic compositionality."}
{"id": "2505.19247", "pdf": "https://arxiv.org/pdf/2505.19247", "abs": "https://arxiv.org/abs/2505.19247", "authors": ["Tao Wang", "Ruipeng Zhang", "Sicun Gao"], "title": "Improving Value Estimation Critically Enhances Vanilla Policy Gradient", "categories": ["cs.LG", "cs.AI", "cs.RO", "I.2.6"], "comment": "15 pages and 21 figures", "summary": "Modern policy gradient algorithms, such as TRPO and PPO, outperform vanilla\npolicy gradient in many RL tasks. Questioning the common belief that enforcing\napproximate trust regions leads to steady policy improvement in practice, we\nshow that the more critical factor is the enhanced value estimation accuracy\nfrom more value update steps in each iteration. To demonstrate, we show that by\nsimply increasing the number of value update steps per iteration, vanilla\npolicy gradient itself can achieve performance comparable to or better than PPO\nin all the standard continuous control benchmark environments. Importantly,\nthis simple change to vanilla policy gradient is significantly more robust to\nhyperparameter choices, opening up the possibility that RL algorithms may still\nbecome more effective and easier to use."}
{"id": "2505.18455", "pdf": "https://arxiv.org/pdf/2505.18455", "abs": "https://arxiv.org/abs/2505.18455", "authors": ["Fanqi Yan", "Huy Nguyen", "Dung Le", "Pedram Akbarian", "Nhat Ho", "Alessandro Rinaldo"], "title": "On Minimax Estimation of Parameters in Softmax-Contaminated Mixture of Experts", "categories": ["stat.ML", "cs.LG"], "comment": "Fanqi Yan, Huy Nguyen, and Dung Le contributed equally to this work", "summary": "The softmax-contaminated mixture of experts (MoE) model is deployed when a\nlarge-scale pre-trained model, which plays the role of a fixed expert, is\nfine-tuned for learning downstream tasks by including a new contamination part,\nor prompt, functioning as a new, trainable expert. Despite its popularity and\nrelevance, the theoretical properties of the softmax-contaminated MoE have\nremained unexplored in the literature. In the paper, we study the convergence\nrates of the maximum likelihood estimator of gating and prompt parameters in\norder to gain insights into the statistical properties and potential challenges\nof fine-tuning with a new prompt. We find that the estimability of these\nparameters is compromised when the prompt acquires overlapping knowledge with\nthe pre-trained model, in the sense that we make precise by formulating a novel\nanalytic notion of distinguishability. Under distinguishability of the\npre-trained and prompt models, we derive minimax optimal estimation rates for\nall the gating and prompt parameters. By contrast, when the distinguishability\ncondition is violated, these estimation rates become significantly slower due\nto their dependence on the prompt convergence rate to the pre-trained model.\nFinally, we empirically corroborate our theoretical findings through several\nnumerical experiments."}
{"id": "2505.20279", "pdf": "https://arxiv.org/pdf/2505.20279", "abs": "https://arxiv.org/abs/2505.20279", "authors": ["Zhiwen Fan", "Jian Zhang", "Renjie Li", "Junge Zhang", "Runjin Chen", "Hezhen Hu", "Kevin Wang", "Huaizhi Qu", "Dilin Wang", "Zhicheng Yan", "Hongyu Xu", "Justin Theiss", "Tianlong Chen", "Jiachen Li", "Zhengzhong Tu", "Zhangyang Wang", "Rakesh Ranjan"], "title": "VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "The rapid advancement of Large Multimodal Models (LMMs) for 2D images and\nvideos has motivated extending these models to understand 3D scenes, aiming for\nhuman-like visual-spatial intelligence. Nevertheless, achieving deep spatial\nunderstanding comparable to human capabilities poses significant challenges in\nmodel encoding and data acquisition. Existing methods frequently depend on\nexternal depth sensors for geometry capture or utilize off-the-shelf algorithms\nfor pre-constructing 3D maps, thereby limiting their scalability, especially\nwith prevalent monocular video inputs and for time-sensitive applications. In\nthis work, we introduce VLM-3R, a unified framework for Vision-Language Models\n(VLMs) that incorporates 3D Reconstructive instruction tuning. VLM-3R processes\nmonocular video frames by employing a geometry encoder to derive implicit 3D\ntokens that represent spatial understanding. Leveraging our Spatial-Visual-View\nFusion and over 200K curated 3D reconstructive instruction tuning\nquestion-answer (QA) pairs, VLM-3R effectively aligns real-world spatial\ncontext with language instructions. This enables monocular 3D spatial\nassistance and embodied reasoning. To facilitate the evaluation of temporal\nreasoning, we introduce the Vision-Spatial-Temporal Intelligence benchmark,\nfeaturing over 138.6K QA pairs across five distinct tasks focused on evolving\nspatial relationships. Extensive experiments demonstrate that our model,\nVLM-3R, not only facilitates robust visual-spatial reasoning but also enables\nthe understanding of temporal 3D context changes, excelling in both accuracy\nand scalability."}
{"id": "2505.19252", "pdf": "https://arxiv.org/pdf/2505.19252", "abs": "https://arxiv.org/abs/2505.19252", "authors": ["Davin Choo", "Billy Jin", "Yongho Shin"], "title": "Learning-Augmented Online Bipartite Fractional Matching", "categories": ["cs.DS", "cs.AI", "cs.LG"], "comment": null, "summary": "Online bipartite matching is a fundamental problem in online optimization,\nextensively studied both in its integral and fractional forms due to its\ntheoretical significance and practical applications, such as online advertising\nand resource allocation. Motivated by recent progress in learning-augmented\nalgorithms, we study online bipartite fractional matching when the algorithm is\ngiven advice in the form of a suggested matching in each iteration. We develop\nalgorithms for both the vertex-weighted and unweighted variants that provably\ndominate the naive \"coin flip\" strategy of randomly choosing between the\nadvice-following and advice-free algorithms. Moreover, our algorithm for the\nvertex-weighted setting extends to the AdWords problem under the small bids\nassumption, yielding a significant improvement over the seminal work of\nMahdian, Nazerzadeh, and Saberi (EC 2007, TALG 2012). Complementing our\npositive results, we establish a hardness bound on the robustness-consistency\ntradeoff that is attainable by any algorithm. We empirically validate our\nalgorithms through experiments on synthetic and real-world data."}
{"id": "2505.18456", "pdf": "https://arxiv.org/pdf/2505.18456", "abs": "https://arxiv.org/abs/2505.18456", "authors": ["Litu Rout", "Constantine Caramanis", "Sanjay Shakkottai"], "title": "Anchored Diffusion Language Model", "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "Diffusion Language Models (DLMs) promise parallel generation and\nbidirectional context, yet they underperform autoregressive (AR) models in both\nlikelihood modeling and generated text quality. We identify that this\nperformance gap arises when important tokens (e.g., key words or low-frequency\nwords that anchor a sentence) are masked early in the forward process, limiting\ncontextual information for accurate reconstruction. To address this, we\nintroduce the Anchored Diffusion Language Model (ADLM), a novel two-stage\nframework that first predicts distributions over important tokens via an anchor\nnetwork, and then predicts the likelihoods of missing tokens conditioned on the\nanchored predictions. ADLM significantly improves test perplexity on LM1B and\nOpenWebText, achieving up to 25.4% gains over prior DLMs, and narrows the gap\nwith strong AR baselines. It also achieves state-of-the-art performance in\nzero-shot generalization across seven benchmarks and surpasses AR models in\nMAUVE score, which marks the first time a DLM generates better human-like text\nthan an AR model. Theoretically, we derive an Anchored Negative Evidence Lower\nBound (ANELBO) objective and show that anchoring improves sample complexity and\nlikelihood modeling. Beyond diffusion, anchoring boosts performance in AR\nmodels and enhances reasoning in math and logic tasks, outperforming existing\nchain-of-thought approaches"}
{"id": "2505.20291", "pdf": "https://arxiv.org/pdf/2505.20291", "abs": "https://arxiv.org/abs/2505.20291", "authors": ["Di Wu", "Yixin Wan", "Kai-Wei Chang"], "title": "Visualized Text-to-Image Retrieval", "categories": ["cs.CV", "cs.CL"], "comment": "Work in Progress", "summary": "We propose Visualize-then-Retrieve (VisRet), a new paradigm for Text-to-Image\n(T2I) retrieval that mitigates the limitations of cross-modal similarity\nalignment of existing multi-modal embeddings. VisRet first projects textual\nqueries into the image modality via T2I generation. Then, it performs retrieval\nwithin the image modality to bypass the weaknesses of cross-modal retrievers in\nrecognizing subtle visual-spatial features. Experiments on three\nknowledge-intensive T2I retrieval benchmarks, including a newly introduced\nmulti-entity benchmark, demonstrate that VisRet consistently improves T2I\nretrieval by 24.5% to 32.7% NDCG@10 across different embedding models. VisRet\nalso significantly benefits downstream visual question answering accuracy when\nused in retrieval-augmented generation pipelines. The method is plug-and-play\nand compatible with off-the-shelf retrievers, making it an effective module for\nknowledge-intensive multi-modal systems. Our code and the new benchmark are\npublicly available at https://github.com/xiaowu0162/Visualize-then-Retrieve."}
{"id": "2505.19255", "pdf": "https://arxiv.org/pdf/2505.19255", "abs": "https://arxiv.org/abs/2505.19255", "authors": ["Mingyuan Wu", "Jingcheng Yang", "Jize Jiang", "Meitang Li", "Kaizhuo Yan", "Hanchao Yu", "Minjia Zhang", "Chengxiang Zhai", "Klara Nahrstedt"], "title": "VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement Learning Finetuning (RFT) has significantly advanced the\nreasoning capabilities of large language models (LLMs) by enabling long chains\nof thought, self-correction, and effective tool use. While recent works attempt\nto extend RFT to vision-language models (VLMs), these efforts largely produce\ntext-only reasoning conditioned on static image inputs, falling short of true\nmultimodal reasoning in the response. In contrast, test-time methods like\nVisual Sketchpad incorporate visual steps but lack training mechanisms.\n  We introduce VTool-R1, the first framework that trains VLMs to generate\nmultimodal chains of thought by interleaving text and intermediate visual\nreasoning steps. VTool-R1 integrates Python-based visual editing tools into the\nRFT process, enabling VLMs to learn when and how to generate visual reasoning\nsteps that benefit final reasoning. Trained with outcome-based rewards tied to\ntask accuracy, our approach elicits strategic visual tool use for reasoning\nwithout relying on process-based supervision. Experiments on structured visual\nquestion answering over charts and tables show that VTool-R1 enhances reasoning\nperformance by teaching VLMs to \"think with images\" and generate multimodal\nchain of thoughts with tools."}
{"id": "2505.18457", "pdf": "https://arxiv.org/pdf/2505.18457", "abs": "https://arxiv.org/abs/2505.18457", "authors": ["Abir Ray"], "title": "EdgeAgentX: A Novel Framework for Agentic AI at the Edge in Military Communication Networks", "categories": ["cs.AI", "cs.LG", "cs.MA"], "comment": "6 pages, 2 figures", "summary": "This paper introduces EdgeAgentX, a novel framework integrating federated\nlearning (FL), multi-agent reinforcement learning (MARL), and adversarial\ndefense mechanisms, tailored for military communication networks. EdgeAgentX\nsignificantly improves autonomous decision-making, reduces latency, enhances\nthroughput, and robustly withstands adversarial disruptions, as evidenced by\ncomprehensive simulations."}
{"id": "2505.20297", "pdf": "https://arxiv.org/pdf/2505.20297", "abs": "https://arxiv.org/abs/2505.20297", "authors": ["Qinyu Zhao", "Jaskirat Singh", "Ming Xu", "Akshay Asthana", "Stephen Gould", "Liang Zheng"], "title": "DiSA: Diffusion Step Annealing in Autoregressive Image Generation", "categories": ["cs.CV", "cs.CL"], "comment": "Our code is available at https://github.com/Qinyu-Allen-Zhao/DiSA", "summary": "An increasing number of autoregressive models, such as MAR, FlowAR, xAR, and\nHarmon adopt diffusion sampling to improve the quality of image generation.\nHowever, this strategy leads to low inference efficiency, because it usually\ntakes 50 to 100 steps for diffusion to sample a token. This paper explores how\nto effectively address this issue. Our key motivation is that as more tokens\nare generated during the autoregressive process, subsequent tokens follow more\nconstrained distributions and are easier to sample. To intuitively explain, if\na model has generated part of a dog, the remaining tokens must complete the dog\nand thus are more constrained. Empirical evidence supports our motivation: at\nlater generation stages, the next tokens can be well predicted by a multilayer\nperceptron, exhibit low variance, and follow closer-to-straight-line denoising\npaths from noise to tokens. Based on our finding, we introduce diffusion step\nannealing (DiSA), a training-free method which gradually uses fewer diffusion\nsteps as more tokens are generated, e.g., using 50 steps at the beginning and\ngradually decreasing to 5 steps at later stages. Because DiSA is derived from\nour finding specific to diffusion in autoregressive models, it is complementary\nto existing acceleration methods designed for diffusion alone. DiSA can be\nimplemented in only a few lines of code on existing models, and albeit simple,\nachieves $5-10\\times$ faster inference for MAR and Harmon and $1.4-2.5\\times$\nfor FlowAR and xAR, while maintaining the generation quality."}
{"id": "2505.19259", "pdf": "https://arxiv.org/pdf/2505.19259", "abs": "https://arxiv.org/abs/2505.19259", "authors": ["Hossein Zaremehrjerdi", "Shreyan Ganguly", "Ashlyn Rairdin", "Elizabeth Tranel", "Benjamin Feuer", "Juan Ignacio Di Salvo", "Srikanth Panthulugiri", "Victoria Moser", "Sarah Jones", "Joscif G Raigne", "Yanben Shen", "Heidi M. Dornath", "Aditya Balu", "Adarsh Krishnamurthy", "Asheesh K Singh", "Arti Singh", "Baskar Ganapathysubramanian", "Chinmay Hegde", "Soumik Sarkar"], "title": "Towards Large Reasoning Models for Agriculture", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Agricultural decision-making involves complex, context-specific reasoning,\nwhere choices about crops, practices, and interventions depend heavily on\ngeographic, climatic, and economic conditions. Traditional large language\nmodels (LLMs) often fall short in navigating this nuanced problem due to\nlimited reasoning capacity. We hypothesize that recent advances in large\nreasoning models (LRMs) can better handle such structured, domain-specific\ninference. To investigate this, we introduce AgReason, the first expert-curated\nopen-ended science benchmark with 100 questions for agricultural reasoning.\nEvaluations across thirteen open-source and proprietary models reveal that LRMs\noutperform conventional ones, though notable challenges persist, with the\nstrongest Gemini-based baseline achieving 36% accuracy. We also present\nAgThoughts, a large-scale dataset of 44.6K question-answer pairs generated with\nhuman oversight and equipped with synthetically generated reasoning traces.\nUsing AgThoughts, we develop AgThinker, a suite of small reasoning models that\ncan be run on consumer-grade GPUs, and show that our dataset can be effective\nin unlocking agricultural reasoning abilities in LLMs. Our project page is\nhere: https://baskargroup.github.io/Ag_reasoning/"}
{"id": "2505.18458", "pdf": "https://arxiv.org/pdf/2505.18458", "abs": "https://arxiv.org/abs/2505.18458", "authors": ["Xuanhe Zhou", "Junxuan He", "Wei Zhou", "Haodong Chen", "Zirui Tang", "Haoyu Zhao", "Xin Tong", "Guoliang Li", "Youmin Chen", "Jun Zhou", "Zhaojun Sun", "Binyuan Hui", "Shuo Wang", "Conghui He", "Zhiyuan Liu", "Jingren Zhou", "Fan Wu"], "title": "A Survey of LLM $\\times$ DATA", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": "Please refer to the paper list at:\n  https://github.com/weAIDB/awesome-data-llm", "summary": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration."}
{"id": "2505.19261", "pdf": "https://arxiv.org/pdf/2505.19261", "abs": "https://arxiv.org/abs/2505.19261", "authors": ["Yu Zhang", "Jialei Zhou", "Xinchen Li", "Qi Zhang", "Zhongwei Wan", "Tianyu Wang", "Duoqian Miao", "Changwei Wang", "Longbing Cao"], "title": "Enhancing Text-to-Image Diffusion Transformer via Split-Text Conditioning", "categories": ["cs.CV", "cs.AI"], "comment": "21 pages", "summary": "Current text-to-image diffusion generation typically employs complete-text\nconditioning. Due to the intricate syntax, diffusion transformers (DiTs)\ninherently suffer from a comprehension defect of complete-text captions.\nOne-fly complete-text input either overlooks critical semantic details or\ncauses semantic confusion by simultaneously modeling diverse semantic primitive\ntypes. To mitigate this defect of DiTs, we propose a novel split-text\nconditioning framework named DiT-ST. This framework converts a complete-text\ncaption into a split-text caption, a collection of simplified sentences, to\nexplicitly express various semantic primitives and their interconnections. The\nsplit-text caption is then injected into different denoising stages of DiT-ST\nin a hierarchical and incremental manner. Specifically, DiT-ST leverages Large\nLanguage Models to parse captions, extracting diverse primitives and\nhierarchically sorting out and constructing these primitives into a split-text\ninput. Moreover, we partition the diffusion denoising process according to its\ndifferential sensitivities to diverse semantic primitive types and determine\nthe appropriate timesteps to incrementally inject tokens of diverse semantic\nprimitive types into input tokens via cross-attention. In this way, DiT-ST\nenhances the representation learning of specific semantic primitive types\nacross different stages. Extensive experiments validate the effectiveness of\nour proposed DiT-ST in mitigating the complete-text comprehension defect."}
{"id": "2505.18486", "pdf": "https://arxiv.org/pdf/2505.18486", "abs": "https://arxiv.org/abs/2505.18486", "authors": ["Hong Jiao", "Dan Song", "Won-Chan Lee"], "title": "Investigating AI Rater Effects of Large Language Models: GPT, Claude, Gemini, and DeepSeek", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have been widely explored for automated scoring\nin low-stakes assessment to facilitate learning and instruction. Empirical\nevidence related to which LLM produces the most reliable scores and induces\nleast rater effects needs to be collected before the use of LLMs for automated\nscoring in practice. This study compared ten LLMs (ChatGPT 3.5, ChatGPT 4,\nChatGPT 4o, OpenAI o1, Claude 3.5 Sonnet, Gemini 1.5, Gemini 1.5 Pro, Gemini\n2.0, as well as DeepSeek V3, and DeepSeek R1) with human expert raters in\nscoring two types of writing tasks. The accuracy of the holistic and analytic\nscores from LLMs compared with human raters was evaluated in terms of Quadratic\nWeighted Kappa. Intra-rater consistency across prompts was compared in terms of\nCronbach Alpha. Rater effects of LLMs were evaluated and compared with human\nraters using the Many-Facet Rasch model. The results in general supported the\nuse of ChatGPT 4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet with high scoring\naccuracy, better rater reliability, and less rater effects."}
{"id": "2505.19263", "pdf": "https://arxiv.org/pdf/2505.19263", "abs": "https://arxiv.org/abs/2505.19263", "authors": ["Hui Ma", "Kai Yang", "Yang Jiao"], "title": "Cellular Traffic Prediction via Byzantine-robust Asynchronous Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Network traffic prediction plays a crucial role in intelligent network\noperation. Traditional prediction methods often rely on centralized training,\nnecessitating the transfer of vast amounts of traffic data to a central server.\nThis approach can lead to latency and privacy concerns. To address these\nissues, federated learning integrated with differential privacy has emerged as\na solution to improve data privacy and model robustness in distributed\nsettings. Nonetheless, existing federated learning protocols are vulnerable to\nByzantine attacks, which may significantly compromise model robustness.\nDeveloping a robust and privacy-preserving prediction model in the presence of\nByzantine clients remains a significant challenge. To this end, we propose an\nasynchronous differential federated learning framework based on\ndistributionally robust optimization. The proposed framework utilizes multiple\nclients to train the prediction model collaboratively with local differential\nprivacy. In addition, regularization techniques have been employed to further\nimprove the Byzantine robustness of the models. We have conducted extensive\nexperiments on three real-world datasets, and the results elucidate that our\nproposed distributed algorithm can achieve superior performance over existing\nmethods."}
{"id": "2505.18487", "pdf": "https://arxiv.org/pdf/2505.18487", "abs": "https://arxiv.org/abs/2505.18487", "authors": ["Junlin Wang", "Zhiyun Lin"], "title": "Grounding Bodily Awareness in Visual Representations for Efficient Policy Learning", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "A preprint version", "summary": "Learning effective visual representations for robotic manipulation remains a\nfundamental challenge due to the complex body dynamics involved in action\nexecution. In this paper, we study how visual representations that carry\nbody-relevant cues can enable efficient policy learning for downstream robotic\nmanipulation tasks. We present $\\textbf{I}$nter-token $\\textbf{Con}$trast\n($\\textbf{ICon}$), a contrastive learning method applied to the token-level\nrepresentations of Vision Transformers (ViTs). ICon enforces a separation in\nthe feature space between agent-specific and environment-specific tokens,\nresulting in agent-centric visual representations that embed body-specific\ninductive biases. This framework can be seamlessly integrated into end-to-end\npolicy learning by incorporating the contrastive loss as an auxiliary\nobjective. Our experiments show that ICon not only improves policy performance\nacross various manipulation tasks but also facilitates policy transfer across\ndifferent robots. The project website: https://github.com/HenryWJL/icon"}
{"id": "2505.19273", "pdf": "https://arxiv.org/pdf/2505.19273", "abs": "https://arxiv.org/abs/2505.19273", "authors": ["Giuseppe Ruggiero", "Matteo Testa", "Jurgen Van de Walle", "Luigi Di Caro"], "title": "Eta-WavLM: Efficient Speaker Identity Removal in Self-Supervised Speech Representations Using a Simple Linear Equation", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Full paper accepted at ACL 2025", "summary": "Self-supervised learning (SSL) has reduced the reliance on expensive labeling\nin speech technologies by learning meaningful representations from unannotated\ndata. Since most SSL-based downstream tasks prioritize content information in\nspeech, ideal representations should disentangle content from unwanted\nvariations like speaker characteristics in the SSL representations. However,\nremoving speaker information often degrades other speech components, and\nexisting methods either fail to fully disentangle speaker identity or require\nresource-intensive models. In this paper, we propose a novel disentanglement\nmethod that linearly decomposes SSL representations into speaker-specific and\nspeaker-independent components, effectively generating speaker disentangled\nrepresentations. Comprehensive experiments show that our approach achieves\nspeaker independence and as such, when applied to content-driven tasks such as\nvoice conversion, our representations yield significant improvements over\nstate-of-the-art methods."}
{"id": "2505.18493", "pdf": "https://arxiv.org/pdf/2505.18493", "abs": "https://arxiv.org/abs/2505.18493", "authors": ["Xiang Li", "Yunai Li", "Huiying Zhong", "Lihua Lei", "Zhun Deng"], "title": "Statistical Inference under Performativity", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "Performativity of predictions refers to the phenomena that\nprediction-informed decisions may influence the target they aim to predict,\nwhich is widely observed in policy-making in social sciences and economics. In\nthis paper, we initiate the study of statistical inference under\nperformativity. Our contribution is two-fold. First, we build a central limit\ntheorem for estimation and inference under performativity, which enables\ninferential purposes in policy-making such as constructing confidence intervals\nor testing hypotheses. Second, we further leverage the derived central limit\ntheorem to investigate prediction-powered inference (PPI) under performativity,\nwhich is based on a small labeled dataset and a much larger dataset of\nmachine-learning predictions. This enables us to obtain more precise estimation\nand improved confidence regions for the model parameter (i.e., policy) of\ninterest in performative prediction. We demonstrate the power of our framework\nby numerical experiments. To the best of our knowledge, this paper is the first\none to establish statistical inference under performativity, which brings up\nnew challenges and inference settings that we believe will add significant\nvalues to policy-making, statistics, and machine learning."}
{"id": "2505.19291", "pdf": "https://arxiv.org/pdf/2505.19291", "abs": "https://arxiv.org/abs/2505.19291", "authors": ["Kazi Mahathir Rahman", "Showrin Rahman", "Sharmin Sultana Srishty"], "title": "TextDiffuser-RL: Efficient and Robust Text Layout Optimization for High-Fidelity Text-to-Image Synthesis", "categories": ["cs.CV", "cs.AI", "68T05, 68T07, 68U10 68T05, 68T07, 68U10 68T05, 68T07, 68U10", "I.2.6; I.2.7; I.2.10; I.5.1; I.4.9"], "comment": "14 pages, 26 figures. Submitted to arXiv for dissemination. Intended\n  for future submission to a Generative AI conference", "summary": "Text-embedded image generation plays a critical role in industries such as\ngraphic design, advertising, and digital content creation. Text-to-Image\ngeneration methods leveraging diffusion models, such as TextDiffuser-2, have\ndemonstrated promising results in producing images with embedded text.\nTextDiffuser-2 effectively generates bounding box layouts that guide the\nrendering of visual text, achieving high fidelity and coherence. However,\nexisting approaches often rely on resource-intensive processes and are limited\nin their ability to run efficiently on both CPU and GPU platforms. To address\nthese challenges, we propose a novel two-stage pipeline that integrates\nreinforcement learning (RL) for rapid and optimized text layout generation with\na diffusion-based image synthesis model. Our RL-based approach significantly\naccelerates the bounding box prediction step while reducing overlaps, allowing\nthe system to run efficiently on both CPUs and GPUs. Extensive evaluations\ndemonstrate that our framework maintains or surpasses TextDiffuser-2's quality\nin text placement and image synthesis, with markedly faster runtime and\nincreased flexibility. Extensive evaluations demonstrate that our framework\nmaintains or surpasses TextDiffuser-2's quality in text placement and image\nsynthesis, with markedly faster runtime and increased flexibility. Our approach\nhas been evaluated on the MARIOEval benchmark, achieving OCR and CLIPScore\nmetrics close to state-of-the-art models, while being 97.64% more faster and\nrequiring only 2MB of memory to run."}
{"id": "2505.18502", "pdf": "https://arxiv.org/pdf/2505.18502", "abs": "https://arxiv.org/abs/2505.18502", "authors": ["Guodong Du", "Xuanning Zhou", "Junlin Li", "Zhuo Li", "Zesheng Shi", "Wanyu Lin", "Ho-Kin Tang", "Xiucheng Li", "Fangming Liu", "Wenya Wang", "Min Zhang", "Jing Li"], "title": "Knowledge Grafting of Large Language Models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Cross-capability transfer is a key challenge in large language model (LLM)\nresearch, with applications in multi-task integration, model compression, and\ncontinual learning. Recent works like FuseLLM and FuseChat have demonstrated\nthe potential of transferring multiple model capabilities to lightweight\nmodels, enhancing adaptability and efficiency, which motivates our\ninvestigation into more efficient cross-capability transfer methods. However,\nexisting approaches primarily focus on small, homogeneous models, limiting\ntheir applicability. For large, heterogeneous models, knowledge distillation\nwith full-parameter fine-tuning often overlooks the student model's intrinsic\ncapacity and risks catastrophic forgetting, while PEFT methods struggle to\neffectively absorb knowledge from source LLMs. To address these issues, we\nintroduce GraftLLM, a novel method that stores source model capabilities in a\ntarget model with SkillPack format. This approach preserves general\ncapabilities, reduces parameter conflicts, and supports forget-free continual\nlearning and model fusion. We employ a module-aware adaptive compression\nstrategy to compress parameter updates, ensuring efficient storage while\nmaintaining task-specific knowledge. The resulting SkillPack serves as a\ncompact and transferable knowledge carrier, ideal for heterogeneous model\nfusion and continual learning. Experiments across various scenarios demonstrate\nthat GraftLLM outperforms existing techniques in knowledge transfer, knowledge\nfusion, and forget-free learning, providing a scalable and efficient solution\nfor cross-capability transfer. The code is publicly available at:\nhttps://github.com/duguodong7/GraftLLM."}
{"id": "2505.19293", "pdf": "https://arxiv.org/pdf/2505.19293", "abs": "https://arxiv.org/abs/2505.19293", "authors": ["Wang Yang", "Hongye Jin", "Shaochen Zhong", "Song Jiang", "Qifan Wang", "Vipin Chaudhary", "Xiaotian Han"], "title": "100-LongBench: Are de facto Long-Context Benchmarks Literally Evaluating Long-Context Ability?", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Long-context capability is considered one of the most important abilities of\nLLMs, as a truly long context-capable LLM enables users to effortlessly process\nmany originally exhausting tasks -- e.g., digesting a long-form document to\nfind answers vs. directly asking an LLM about it. However, existing\nreal-task-based long-context evaluation benchmarks have two major shortcomings.\nFirst, benchmarks like LongBench often do not provide proper metrics to\nseparate long-context performance from the model's baseline ability, making\ncross-model comparison unclear. Second, such benchmarks are usually constructed\nwith fixed input lengths, which limits their applicability across different\nmodels and fails to reveal when a model begins to break down. To address these\nissues, we introduce a length-controllable long-context benchmark and a novel\nmetric that disentangles baseline knowledge from true long-context\ncapabilities. Experiments demonstrate the superiority of our approach in\neffectively evaluating LLMs."}
{"id": "2505.18512", "pdf": "https://arxiv.org/pdf/2505.18512", "abs": "https://arxiv.org/abs/2505.18512", "authors": ["Soyoung Yoon", "Gyuwan Kim", "Gyu-Hwung Cho", "Seung-won Hwang"], "title": "AcuRank: Uncertainty-Aware Adaptive Computation for Listwise Reranking", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": "22 pages, 3 figures. The first two authors contributed equally.\n  Author order is randomly determined via coin toss", "summary": "Listwise reranking with large language models (LLMs) enhances top-ranked\nresults in retrieval-based applications. Due to the limit in context size and\nhigh inference cost of long context, reranking is typically performed over a\nfixed size of small subsets, with the final ranking aggregated from these\npartial results. This fixed computation disregards query difficulty and\ndocument distribution, leading to inefficiencies. We propose AcuRank, an\nadaptive reranking framework that dynamically adjusts both the amount and\ntarget of computation based on uncertainty estimates over document relevance.\nUsing a Bayesian TrueSkill model, we iteratively refine relevance estimates\nuntil reaching sufficient confidence levels, and our explicit modeling of\nranking uncertainty enables principled control over reranking behavior and\navoids unnecessary updates to confident predictions. Results on the TREC-DL and\nBEIR benchmarks show that our method consistently achieves a superior\naccuracy-efficiency trade-off and scales better with compute than\nfixed-computation baselines. These results highlight the effectiveness and\ngeneralizability of our method across diverse retrieval tasks and LLM-based\nreranking models."}
{"id": "2505.19299", "pdf": "https://arxiv.org/pdf/2505.19299", "abs": "https://arxiv.org/abs/2505.19299", "authors": ["Lingjun Zhao", "Hal Daum III"], "title": "A Necessary Step toward Faithfulness: Measuring and Improving Consistency in Free-Text Explanations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Faithful free-text explanations are important to ensure transparency in\nhigh-stakes AI decision-making contexts, but they are challenging to generate\nby language models and assess by humans. In this paper, we present a measure\nfor Prediction-EXplanation (PEX) consistency, by extending the concept of\nweight of evidence. This measure quantifies how much a free-text explanation\nsupports or opposes a prediction, serving as an important aspect of explanation\nfaithfulness. Our analysis reveals that more than 62% explanations generated by\nlarge language models lack this consistency. We show that applying direct\npreference optimization improves the consistency of generated explanations\nacross three model families, with improvement ranging from 43.1% to 292.3%.\nFurthermore, we demonstrate that optimizing this consistency measure can\nimprove explanation faithfulness by up to 9.7%."}
{"id": "2505.18517", "pdf": "https://arxiv.org/pdf/2505.18517", "abs": "https://arxiv.org/abs/2505.18517", "authors": ["Pooneh Mousavi", "Shubham Gupta", "Cem Subakan", "Mirco Ravanelli"], "title": "LiSTEN: Learning Soft Token Embeddings for Neural Audio LLMs", "categories": ["cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "Foundation models based on large language models (LLMs) have shown great\nsuccess in handling various tasks and modalities. However, adapting these\nmodels for general-purpose audio-language tasks is challenging due to\ndifferences in acoustic environments and task variations. In this work, we\nintroduce LiSTEN Learning Soft Token Embeddings for Neural Audio LLMs), a\nframework for adapting LLMs to speech and audio tasks. LiSTEN uses a dynamic\nprompt selection strategy with learnable key-value pairs, allowing the model to\nbalance general and task-specific knowledge while avoiding overfitting in a\nmultitask setting. Our approach reduces dependence on large-scale ASR or\ncaptioning datasets, achieves competitive performance with fewer trainable\nparameters, and simplifies training by using a single-stage process.\nAdditionally, LiSTEN enhances interpretability by analyzing the diversity and\noverlap of selected prompts across different tasks."}
{"id": "2505.19301", "pdf": "https://arxiv.org/pdf/2505.19301", "abs": "https://arxiv.org/abs/2505.19301", "authors": ["Ken Huang", "Vineeth Sai Narajala", "John Yeoh", "Ramesh Raskar", "Youssef Harkati", "Jerry Huang", "Idan Habler", "Chris Hughes"], "title": "A Novel Zero-Trust Identity Framework for Agentic AI: Decentralized Authentication and Fine-Grained Access Control", "categories": ["cs.CR", "cs.AI", "cs.MA"], "comment": "24 Pages, 5 figures, 2 tables", "summary": "Traditional Identity and Access Management (IAM) systems, primarily designed\nfor human users or static machine identities via protocols such as OAuth,\nOpenID Connect (OIDC), and SAML, prove fundamentally inadequate for the\ndynamic, interdependent, and often ephemeral nature of AI agents operating at\nscale within Multi Agent Systems (MAS), a computational system composed of\nmultiple interacting intelligent agents that work collectively.\n  This paper posits the imperative for a novel Agentic AI IAM framework: We\ndeconstruct the limitations of existing protocols when applied to MAS,\nillustrating with concrete examples why their coarse-grained controls,\nsingle-entity focus, and lack of context-awareness falter. We then propose a\ncomprehensive framework built upon rich, verifiable Agent Identities (IDs),\nleveraging Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs),\nthat encapsulate an agents capabilities, provenance, behavioral scope, and\nsecurity posture.\n  Our framework includes an Agent Naming Service (ANS) for secure and\ncapability-aware discovery, dynamic fine-grained access control mechanisms, and\ncritically, a unified global session management and policy enforcement layer\nfor real-time control and consistent revocation across heterogeneous agent\ncommunication protocols. We also explore how Zero-Knowledge Proofs (ZKPs)\nenable privacy-preserving attribute disclosure and verifiable policy\ncompliance.\n  We outline the architecture, operational lifecycle, innovative contributions,\nand security considerations of this new IAM paradigm, aiming to establish the\nfoundational trust, accountability, and security necessary for the burgeoning\nfield of agentic AI and the complex ecosystems they will inhabit."}
{"id": "2505.18526", "pdf": "https://arxiv.org/pdf/2505.18526", "abs": "https://arxiv.org/abs/2505.18526", "authors": ["Yunqin Zhu", "Henry Shaowu Yuchi", "Yao Xie"], "title": "Scalable Gaussian Processes with Low-Rank Deep Kernel Decomposition", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Kernels are key to encoding prior beliefs and data structures in Gaussian\nprocess (GP) models. The design of expressive and scalable kernels has garnered\nsignificant research attention. Deep kernel learning enhances kernel\nflexibility by feeding inputs through a neural network before applying a\nstandard parametric form. However, this approach remains limited by the choice\nof base kernels, inherits high inference costs, and often demands sparse\napproximations. Drawing on Mercer's theorem, we introduce a fully data-driven,\nscalable deep kernel representation where a neural network directly represents\na low-rank kernel through a small set of basis functions. This construction\nenables highly efficient exact GP inference in linear time and memory without\ninvoking inducing points. It also supports scalable mini-batch training based\non a principled variational inference framework. We further propose a simple\nvariance correction procedure to guard against overconfidence in uncertainty\nestimates. Experiments on synthetic and real-world data demonstrate the\nadvantages of our deep kernel GP in terms of predictive accuracy, uncertainty\nquantification, and computational efficiency."}
{"id": "2505.19310", "pdf": "https://arxiv.org/pdf/2505.19310", "abs": "https://arxiv.org/abs/2505.19310", "authors": ["Robin D. Pesl", "Jerin G. Mathew", "Massimo Mecella", "Marco Aiello"], "title": "Retrieval-Augmented Generation for Service Discovery: Chunking Strategies and Benchmarking", "categories": ["cs.SE", "cs.AI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2411.19804", "summary": "Integrating multiple (sub-)systems is essential to create advanced\nInformation Systems. Difficulties mainly arise when integrating dynamic\nenvironments, e.g., the integration at design time of not yet existing\nservices. This has been traditionally addressed using a registry that provides\nthe API documentation of the endpoints. Large Language Models have shown to be\ncapable of automatically creating system integrations (e.g., as service\ncomposition) based on this documentation but require concise input due to input\noken limitations, especially regarding comprehensive API descriptions.\nCurrently, it is unknown how best to preprocess these API descriptions. In the\npresent work, we (i) analyze the usage of Retrieval Augmented Generation for\nendpoint discovery and the chunking, i.e., preprocessing, of state-of-practice\nOpenAPIs to reduce the input oken length while preserving the most relevant\ninformation. To further reduce the input token length for the composition\nprompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that\nonly receives a summary of the most relevant endpoints nd retrieves\nspecification details on demand. We evaluate RAG for endpoint discovery using\n(iii) a proposed novel service discovery benchmark SOCBench-D representing a\ngeneral setting across numerous domains and the real-world RestBench enchmark,\nfirst, for the different chunking possibilities and parameters measuring the\nendpoint retrieval accuracy. Then, we assess the Discovery Agent using the same\ntest data set. The prototype shows how to successfully employ RAG for endpoint\ndiscovery to reduce the token count. Our experiments show that endpoint-based\napproaches outperform naive chunking methods for preprocessing. Relying on an\nagent significantly improves precision while being prone to decrease recall,\ndisclosing the need for further reasoning capabilities."}
{"id": "2505.18538", "pdf": "https://arxiv.org/pdf/2505.18538", "abs": "https://arxiv.org/abs/2505.18538", "authors": ["Xin Wei", "Huakun Liu", "Yutaro Hirao", "Monica Perusquia-Hernandez", "Katsutoshi Masai", "Hideaki Uchiyama", "Kiyoshi Kiyokawa"], "title": "Mind Your Vision: Multimodal Estimation of Refractive Disorders Using Electrooculography and Eye Tracking", "categories": ["eess.IV", "cs.LG"], "comment": null, "summary": "Refractive errors are among the most common visual impairments globally, yet\ntheir diagnosis often relies on active user participation and clinical\noversight. This study explores a passive method for estimating refractive power\nusing two eye movement recording techniques: electrooculography (EOG) and\nvideo-based eye tracking. Using a publicly available dataset recorded under\nvarying diopter conditions, we trained Long Short-Term Memory (LSTM) models to\nclassify refractive power from unimodal (EOG or eye tracking) and multimodal\nconfiguration. We assess performance in both subject-dependent and\nsubject-independent settings to evaluate model personalization and\ngeneralizability across individuals. Results show that the multimodal model\nconsistently outperforms unimodal models, achieving the highest average\naccuracy in both settings: 96.207\\% in the subject-dependent scenario and\n8.882\\% in the subject-independent scenario. However, generalization remains\nlimited, with classification accuracy only marginally above chance in the\nsubject-independent evaluations. Statistical comparisons in the\nsubject-dependent setting confirmed that the multimodal model significantly\noutperformed the EOG and eye-tracking models. However, no statistically\nsignificant differences were found in the subject-independent setting. Our\nfindings demonstrate both the potential and current limitations of eye movement\ndata-based refractive error estimation, contributing to the development of\ncontinuous, non-invasive screening methods using EOG signals and eye-tracking\ndata."}
{"id": "2505.19314", "pdf": "https://arxiv.org/pdf/2505.19314", "abs": "https://arxiv.org/abs/2505.19314", "authors": ["Helin Wang", "Jiarui Hai", "Dongchao Yang", "Chen Chen", "Kai Li", "Junyi Peng", "Thomas Thebaud", "Laureano Moro Velazquez", "Jesus Villalba", "Najim Dehak"], "title": "SoloSpeech: Enhancing Intelligibility and Quality in Target Speech Extraction through a Cascaded Generative Pipeline", "categories": ["eess.AS", "cs.AI", "cs.SD"], "comment": null, "summary": "Target Speech Extraction (TSE) aims to isolate a target speaker's voice from\na mixture of multiple speakers by leveraging speaker-specific cues, typically\nprovided as auxiliary audio (a.k.a. cue audio). Although recent advancements in\nTSE have primarily employed discriminative models that offer high perceptual\nquality, these models often introduce unwanted artifacts, reduce naturalness,\nand are sensitive to discrepancies between training and testing environments.\nOn the other hand, generative models for TSE lag in perceptual quality and\nintelligibility. To address these challenges, we present SoloSpeech, a novel\ncascaded generative pipeline that integrates compression, extraction,\nreconstruction, and correction processes. SoloSpeech features a\nspeaker-embedding-free target extractor that utilizes conditional information\nfrom the cue audio's latent space, aligning it with the mixture audio's latent\nspace to prevent mismatches. Evaluated on the widely-used Libri2Mix dataset,\nSoloSpeech achieves the new state-of-the-art intelligibility and quality in\ntarget speech extraction and speech separation tasks while demonstrating\nexceptional generalization on out-of-domain data and real-world scenarios."}
{"id": "2505.18543", "pdf": "https://arxiv.org/pdf/2505.18543", "abs": "https://arxiv.org/abs/2505.18543", "authors": ["Baolei Zhang", "Haoran Xin", "Jiatong Li", "Dongzhe Zhang", "Minghong Fang", "Zhuqing Liu", "Lihai Nie", "Zheli Liu"], "title": "Benchmarking Poisoning Attacks against Retrieval-Augmented Generation", "categories": ["cs.CR", "cs.IR", "cs.LG"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has proven effective in mitigating\nhallucinations in large language models by incorporating external knowledge\nduring inference. However, this integration introduces new security\nvulnerabilities, particularly to poisoning attacks. Although prior work has\nexplored various poisoning strategies, a thorough assessment of their practical\nthreat to RAG systems remains missing. To address this gap, we propose the\nfirst comprehensive benchmark framework for evaluating poisoning attacks on\nRAG. Our benchmark covers 5 standard question answering (QA) datasets and 10\nexpanded variants, along with 13 poisoning attack methods and 7 defense\nmechanisms, representing a broad spectrum of existing techniques. Using this\nbenchmark, we conduct a comprehensive evaluation of all included attacks and\ndefenses across the full dataset spectrum. Our findings show that while\nexisting attacks perform well on standard QA datasets, their effectiveness\ndrops significantly on the expanded versions. Moreover, our results demonstrate\nthat various advanced RAG architectures, such as sequential, branching,\nconditional, and loop RAG, as well as multi-turn conversational RAG, multimodal\nRAG systems, and RAG-based LLM agent systems, remain susceptible to poisoning\nattacks. Notably, current defense techniques fail to provide robust protection,\nunderscoring the pressing need for more resilient and generalizable defense\nstrategies."}
{"id": "2505.19315", "pdf": "https://arxiv.org/pdf/2505.19315", "abs": "https://arxiv.org/abs/2505.19315", "authors": ["Farid Najar", "Dominique Barth", "Yann Strozecki"], "title": "Demand Selection for VRP with Emission Quota", "categories": ["cs.DS", "cs.AI", "cs.LG"], "comment": null, "summary": "Combinatorial optimization (CO) problems are traditionally addressed using\nOperations Research (OR) methods, including metaheuristics. In this study, we\nintroduce a demand selection problem for the Vehicle Routing Problem (VRP) with\nan emission quota, referred to as QVRP. The objective is to minimize the number\nof omitted deliveries while respecting the pollution quota. We focus on the\ndemand selection part, called Maximum Feasible Vehicle Assignment (MFVA), while\nthe construction of a routing for the VRP instance is solved using classical OR\nmethods. We propose several methods for selecting the packages to omit, both\nfrom machine learning (ML) and OR. Our results show that, in this static\nproblem setting, classical OR-based methods consistently outperform ML-based\napproaches."}
{"id": "2505.18546", "pdf": "https://arxiv.org/pdf/2505.18546", "abs": "https://arxiv.org/abs/2505.18546", "authors": ["Dristi Datta", "Manoranjan Paul", "Manzur Murshed", "Shyh Wei Teng", "Leigh M. Schmidtke"], "title": "ReflectGAN: Modeling Vegetation Effects for Soil Carbon Estimation from Satellite Imagery", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Soil organic carbon (SOC) is a critical indicator of soil health, but its\naccurate estimation from satellite imagery is hindered in vegetated regions due\nto spectral contamination from plant cover, which obscures soil reflectance and\nreduces model reliability. This study proposes the Reflectance Transformation\nGenerative Adversarial Network (ReflectGAN), a novel paired GAN-based framework\ndesigned to reconstruct accurate bare soil reflectance from vegetated soil\nsatellite observations. By learning the spectral transformation between\nvegetated and bare soil reflectance, ReflectGAN facilitates more precise SOC\nestimation under mixed land cover conditions. Using the LUCAS 2018 dataset and\ncorresponding Landsat 8 imagery, we trained multiple learning-based models on\nboth original and ReflectGAN-reconstructed reflectance inputs. Models trained\non ReflectGAN outputs consistently outperformed those using existing vegetation\ncorrection methods. For example, the best-performing model (RF) achieved an\n$R^2$ of 0.54, RMSE of 3.95, and RPD of 2.07 when applied to the\nReflectGAN-generated signals, representing a 35\\% increase in $R^2$, a 43\\%\nreduction in RMSE, and a 43\\% improvement in RPD compared to the best existing\nmethod (PMM-SU). The performance of the models with ReflectGAN is also better\ncompared to their counterparts when applied to another dataset, i.e.,\nSentinel-2 imagery. These findings demonstrate the potential of ReflectGAN to\nimprove SOC estimation accuracy in vegetated landscapes, supporting more\nreliable soil monitoring."}
{"id": "2505.19337", "pdf": "https://arxiv.org/pdf/2505.19337", "abs": "https://arxiv.org/abs/2505.19337", "authors": ["Kevin Li", "Marinka Zitnik"], "title": "Prompting Decision Transformers for Zero-Shot Reach-Avoid Policies", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Offline goal-conditioned reinforcement learning methods have shown promise\nfor reach-avoid tasks, where an agent must reach a target state while avoiding\nundesirable regions of the state space. Existing approaches typically encode\navoid-region information into an augmented state space and cost function, which\nprevents flexible, dynamic specification of novel avoid-region information at\nevaluation time. They also rely heavily on well-designed reward and cost\nfunctions, limiting scalability to complex or poorly structured environments.\nWe introduce RADT, a decision transformer model for offline, reward-free,\ngoal-conditioned, avoid region-conditioned RL. RADT encodes goals and avoid\nregions directly as prompt tokens, allowing any number of avoid regions of\narbitrary size to be specified at evaluation time. Using only suboptimal\noffline trajectories from a random policy, RADT learns reach-avoid behavior\nthrough a novel combination of goal and avoid-region hindsight relabeling. We\nbenchmark RADT against 3 existing offline goal-conditioned RL models across 11\ntasks, environments, and experimental settings. RADT generalizes in a zero-shot\nmanner to out-of-distribution avoid region sizes and counts, outperforming\nbaselines that require retraining. In one such zero-shot setting, RADT achieves\n35.7% improvement in normalized cost over the best retrained baseline while\nmaintaining high goal-reaching success. We apply RADT to cell reprogramming in\nbiology, where it reduces visits to undesirable intermediate gene expression\nstates during trajectories to desired target states, despite stochastic\ntransitions and discrete, structured state dynamics."}
{"id": "2505.18551", "pdf": "https://arxiv.org/pdf/2505.18551", "abs": "https://arxiv.org/abs/2505.18551", "authors": ["Md Ahsanul Haque", "Ismail Hossain", "Md Mahmuduzzaman Kamol", "Md Jahangir Alam", "Suresh Kumar Amalapuram", "Sajedul Talukder", "Mohammad Saidur Rahman"], "title": "LAMDA: A Longitudinal Android Malware Benchmark for Concept Drift Analysis", "categories": ["cs.CR", "cs.LG"], "comment": "31 pages, 21 figures, and 16 tables", "summary": "Machine learning (ML)-based malware detection systems often fail to account\nfor the dynamic nature of real-world training and test data distributions. In\npractice, these distributions evolve due to frequent changes in the Android\necosystem, adversarial development of new malware families, and the continuous\nemergence of both benign and malicious applications. Prior studies have shown\nthat such concept drift -- distributional shifts in benign and malicious\nsamples, leads to significant degradation in detection performance over time.\nDespite the practical importance of this issue, existing datasets are often\noutdated and limited in temporal scope, diversity of malware families, and\nsample scale, making them insufficient for the systematic evaluation of concept\ndrift in malware detection.\n  To address this gap, we present LAMDA, the largest and most temporally\ndiverse Android malware benchmark to date, designed specifically for concept\ndrift analysis. LAMDA spans 12 years (2013-2025, excluding 2015), includes over\n1 million samples (approximately 37% labeled as malware), and covers 1,380\nmalware families and 150,000 singleton samples, reflecting the natural\ndistribution and evolution of real-world Android applications. We empirically\ndemonstrate LAMDA's utility by quantifying the performance degradation of\nstandard ML models over time and analyzing feature stability across years. As\nthe most comprehensive Android malware dataset to date, LAMDA enables in-depth\nresearch into temporal drift, generalization, explainability, and evolving\ndetection challenges. The dataset and code are available at:\nhttps://iqsec-lab.github.io/LAMDA/."}
{"id": "2505.19339", "pdf": "https://arxiv.org/pdf/2505.19339", "abs": "https://arxiv.org/abs/2505.19339", "authors": ["Libo Wang"], "title": "Towards Humanoid Robot Autonomy: A Dynamic Architecture Integrating Continuous thought Machines (CTM) and Model Context Protocol (MCP)", "categories": ["cs.RO", "cs.AI"], "comment": "The relevant architecture code and some experimental records have\n  been uploaded to the GitHub repository for sharing:\n  https://github.com/brucewang123456789/GeniusTrail/tree/main/CTM-MCP", "summary": "To address the gaps between the static pre-set \"thinking-planning-action\" of\nhumanoid robots in unfamiliar scenarios and the highly programmed \"call\ntool-return result\" due to the lack of autonomous coding capabilities, this\nwork designs a dynamic architecture connecting continuous thought machines\n(CTM) and model context protocol (MCP). It proposes a theoretical parallel\nsolution through tick-slab and uses rank compression to achieve parameter\nsuppression to provide a solution for achieving autonomous actions due to\nautonomous coding. The researcher used a simulation-based experiment using\nOpenAI's o4-mini-high as a tool to build the experimental environment, and\nintroduced the extended SayCan dataset to conduct nine epochs of experiments.\nThe experimental results show that the CTM-MCP architecture is feasible and\neffective through the data results of seven metrics: task success rate (TSR),\nexecution success rate (ESR), average episode length (AEL), ROSCOE, REVEAL,\nproficiency self-assessment (PSA), task effectiveness (TE). In practice, it\nprovides a reference experience for exploring the autonomous dynamic coding of\nhumanoid robots based on continuous thinking to achieve human-like autonomous\nactions."}
{"id": "2505.18574", "pdf": "https://arxiv.org/pdf/2505.18574", "abs": "https://arxiv.org/abs/2505.18574", "authors": ["Charles Hong", "Sahil Bhatia", "Alvin Cheung", "Yakun Sophia Shao"], "title": "Autocomp: LLM-Driven Code Optimization for Tensor Accelerators", "categories": ["cs.PL", "cs.AI", "cs.AR", "cs.LG"], "comment": null, "summary": "Hardware accelerators, especially those designed for tensor processing, have\nbecome ubiquitous in today's computing landscape. However, even with\nsignificant efforts in building compilers, programming these tensor\naccelerators remains challenging, leaving much of their potential\nunderutilized. Recently, large language models (LLMs), trained on large amounts\nof code, have shown significant promise in code generation and optimization\ntasks, but generating low-resource languages like specialized tensor\naccelerator code still poses a significant challenge. We tackle this challenge\nwith Autocomp, an approach that empowers accelerator programmers to leverage\ndomain knowledge and hardware feedback to optimize code via an automated\nLLM-driven search. We accomplish this by: 1) formulating each optimization pass\nas a structured two-phase prompt, divided into planning and code generation\nphases, 2) inserting domain knowledge during planning via a concise and\nadaptable optimization menu, and 3) integrating correctness and performance\nmetrics from hardware as feedback at each search iteration. Across three\ncategories of representative workloads and two different accelerators, we\ndemonstrate that Autocomp-optimized code runs 5.6x (GEMM) and 2.7x\n(convolution) faster than the vendor-provided library, and outperforms\nexpert-level hand-tuned code by 1.4x (GEMM), 1.1x (convolution), and 1.3x\n(fine-grained linear algebra). Additionally, we demonstrate that optimization\nschedules generated from Autocomp can be reused across similar tensor\noperations, improving speedups by up to 24% under a fixed sample budget."}
{"id": "2505.19342", "pdf": "https://arxiv.org/pdf/2505.19342", "abs": "https://arxiv.org/abs/2505.19342", "authors": ["Xiao Liu", "Lijun Zhang", "Deepak Ganesan", "Hui Guan"], "title": "Communication-Efficient Multi-Device Inference Acceleration for Transformer Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Transformer models power many AI applications but suffer from high inference\nlatency, limiting their use in real-time settings. Multi-device inference can\nreduce latency by parallelizing computation. Yet, existing methods require high\ninter-device bandwidth, making them impractical for bandwidth-constrained\nenvironments. We propose ASTRA, a communication-efficient framework that\naccelerates Transformer inference through a novel integration of sequence\nparallelism and a Mixed-Precision Attention mechanism designed to minimize\ninter-device communication. ASTRA compresses non-local token embeddings via\nvector quantization and preserves task accuracy through two optimizations,\nNoise-Augmented Quantization and Distributed Class Tokens. Experiments on ViT\nand GPT2 across vision and NLP tasks show that ASTRA achieves up to 2.64X\nspeedups over single-device inference and up to 15.25X speedups over\nstate-of-the-art multi-device inferences, while operating under bandwidths as\nlow as 10 Mbps. ASTRA is open-sourced at https://github.com/xl1990/Astra."}
{"id": "2505.18597", "pdf": "https://arxiv.org/pdf/2505.18597", "abs": "https://arxiv.org/abs/2505.18597", "authors": ["Haojie Wang", "Jiuyun Jiang", "L. Jeff Hong", "Guangxin Jiang"], "title": "LLMs for Supply Chain Management", "categories": ["cs.AI", "cs.LG", "stat.AP"], "comment": null, "summary": "The development of large language models (LLMs) has provided new tools for\nresearch in supply chain management (SCM). In this paper, we introduce a\nretrieval-augmented generation (RAG) framework that dynamically integrates\nexternal knowledge into the inference process, and develop a domain-specialized\nSCM LLM, which demonstrates expert-level competence by passing standardized SCM\nexaminations and beer game tests. We further employ the use of LLMs to conduct\nhorizontal and vertical supply chain games, in order to analyze competition and\ncooperation within supply chains. Our experiments show that RAG significantly\nimproves performance on SCM tasks. Moreover, game-theoretic analysis reveals\nthat the LLM can reproduce insights from the classical SCM literature, while\nalso uncovering novel behaviors and offering fresh perspectives on phenomena\nsuch as the bullwhip effect. This paper opens the door for exploring\ncooperation and competition for complex supply chain network through the lens\nof LLMs."}
{"id": "2505.19345", "pdf": "https://arxiv.org/pdf/2505.19345", "abs": "https://arxiv.org/abs/2505.19345", "authors": ["Yongmin Yoo", "Qiongkai Xu", "Longbing Cao"], "title": "PatentScore: Multi-dimensional Evaluation of LLM-Generated Patent Claims", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural language generation (NLG) metrics play a central role in evaluating\ngenerated texts, but are not well suited for the structural and legal\ncharacteristics of patent documents. Large language models (LLMs) offer strong\npotential in automating patent generation, yet research on evaluating\nLLM-generated patents remains limited, especially in evaluating the generation\nquality of patent claims, which are central to defining the scope of\nprotection. Effective claim evaluation requires addressing legal validity,\ntechnical accuracy, and structural compliance. To address this gap, we\nintroduce PatentScore, a multi-dimensional evaluation framework for assessing\nLLM-generated patent claims. PatentScore incorporates: (1) hierarchical\ndecomposition for claim analysis; (2) domain-specific validation patterns based\non legal and technical standards; and (3) scoring across structural, semantic,\nand legal dimensions. Unlike general-purpose NLG metrics, PatentScore reflects\npatent-specific constraints and document structures, enabling evaluation beyond\nsurface similarity. We evaluate 400 GPT-4o-mini generated Claim 1s and report a\nPearson correlation of $r = 0.819$ with expert annotations, outperforming\nexisting NLG metrics. Furthermore, we conduct additional evaluations using open\nmodels such as Claude-3.5-Haiku and Gemini-1.5-flash, all of which show strong\ncorrelations with expert judgments, confirming the robustness and\ngeneralizability of our framework."}
{"id": "2505.18600", "pdf": "https://arxiv.org/pdf/2505.18600", "abs": "https://arxiv.org/abs/2505.18600", "authors": ["Bryan Sangwoo Kim", "Jeongsol Kim", "Jong Chul Ye"], "title": "Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and Preference Alignment", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Modern single-image super-resolution (SISR) models deliver photo-realistic\nresults at the scale factors on which they are trained, but collapse when asked\nto magnify far beyond that regime. We address this scalability bottleneck with\nChain-of-Zoom (CoZ), a model-agnostic framework that factorizes SISR into an\nautoregressive chain of intermediate scale-states with multi-scale-aware\nprompts. CoZ repeatedly re-uses a backbone SR model, decomposing the\nconditional probability into tractable sub-problems to achieve extreme\nresolutions without additional training. Because visual cues diminish at high\nmagnifications, we augment each zoom step with multi-scale-aware text prompts\ngenerated by a vision-language model (VLM). The prompt extractor itself is\nfine-tuned using Generalized Reward Policy Optimization (GRPO) with a critic\nVLM, aligning text guidance towards human preference. Experiments show that a\nstandard 4x diffusion SR model wrapped in CoZ attains beyond 256x enlargement\nwith high perceptual quality and fidelity."}
{"id": "2505.19356", "pdf": "https://arxiv.org/pdf/2505.19356", "abs": "https://arxiv.org/abs/2505.19356", "authors": ["Kidist Amde Mekonnen", "Yosef Worku Alemneh", "Maarten de Rijke"], "title": "Optimized Text Embedding Models and Benchmarks for Amharic Passage Retrieval", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "68T50 (Primary), 68T05 (Secondary)", "H.3.3; H.3.1; I.2.7"], "comment": "10 pages (excluding references and appendix), 10 figures. Accepted to\n  ACL 2025 Findings. Public release includes dataset, code, and trained models:\n  https://github.com/kidist-amde/amharic-ir-benchmarks", "summary": "Neural retrieval methods using transformer-based pre-trained language models\nhave advanced multilingual and cross-lingual retrieval. However, their\neffectiveness for low-resource, morphologically rich languages such as Amharic\nremains underexplored due to data scarcity and suboptimal tokenization. We\naddress this gap by introducing Amharic-specific dense retrieval models based\non pre-trained Amharic BERT and RoBERTa backbones. Our proposed\nRoBERTa-Base-Amharic-Embed model (110M parameters) achieves a 17.6% relative\nimprovement in MRR@10 and a 9.86% gain in Recall@10 over the strongest\nmultilingual baseline, Arctic Embed 2.0 (568M parameters). More compact\nvariants, such as RoBERTa-Medium-Amharic-Embed (42M), remain competitive while\nbeing over 13x smaller. Additionally, we train a ColBERT-based late interaction\nretrieval model that achieves the highest MRR@10 score (0.843) among all\nevaluated models. We benchmark our proposed models against both sparse and\ndense retrieval baselines to systematically assess retrieval effectiveness in\nAmharic. Our analysis highlights key challenges in low-resource settings and\nunderscores the importance of language-specific adaptation. To foster future\nresearch in low-resource IR, we publicly release our dataset, codebase, and\ntrained models at https://github.com/kidist-amde/amharic-ir-benchmarks."}
{"id": "2505.18602", "pdf": "https://arxiv.org/pdf/2505.18602", "abs": "https://arxiv.org/abs/2505.18602", "authors": ["Hengzhe Zhang", "Qi Chen", "Bing Xue", "Mengjie Zhang"], "title": "LLM-Meta-SR: Learning to Evolve Selection Operators for Symbolic Regression", "categories": ["cs.NE", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have revolutionized algorithm development, yet\ntheir application in symbolic regression, where algorithms automatically\ndiscover symbolic expressions from data, remains constrained and is typically\ndesigned manually by human experts. In this paper, we propose a\nlearning-to-evolve framework that enables LLMs to automatically design\nselection operators for evolutionary symbolic regression algorithms. We first\nidentify two key limitations in existing LLM-based algorithm evolution\ntechniques: code bloat and a lack of semantic guidance. Bloat results in\nunnecessarily complex components, and the absence of semantic awareness can\nlead to ineffective exchange of useful code components, both of which can\nreduce the interpretability of the designed algorithm or hinder evolutionary\nlearning progress. To address these issues, we enhance the LLM-based evolution\nframework for meta symbolic regression with two key innovations: bloat control\nand a complementary, semantics-aware selection operator. Additionally, we embed\ndomain knowledge into the prompt, enabling the LLM to generate more effective\nand contextually relevant selection operators. Our experimental results on\nsymbolic regression benchmarks show that LLMs can devise selection operators\nthat outperform nine expert-designed baselines, achieving state-of-the-art\nperformance. This demonstrates that LLMs can exceed expert-level algorithm\ndesign for symbolic regression."}
{"id": "2505.19369", "pdf": "https://arxiv.org/pdf/2505.19369", "abs": "https://arxiv.org/abs/2505.19369", "authors": ["Yunbo Liu", "Xukui Qin", "Yifan Gao", "Xiang Li", "Chengwei Feng"], "title": "SETransformer: A Hybrid Attention-Based Architecture for Robust Human Activity Recognition", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Human Activity Recognition (HAR) using wearable sensor data has become a\ncentral task in mobile computing, healthcare, and human-computer interaction.\nDespite the success of traditional deep learning models such as CNNs and RNNs,\nthey often struggle to capture long-range temporal dependencies and contextual\nrelevance across multiple sensor channels. To address these limitations, we\npropose SETransformer, a hybrid deep neural architecture that combines\nTransformer-based temporal modeling with channel-wise squeeze-and-excitation\n(SE) attention and a learnable temporal attention pooling mechanism. The model\ntakes raw triaxial accelerometer data as input and leverages global\nself-attention to capture activity-specific motion dynamics over extended time\nwindows, while adaptively emphasizing informative sensor channels and critical\ntime steps.\n  We evaluate SETransformer on the WISDM dataset and demonstrate that it\nsignificantly outperforms conventional models including LSTM, GRU, BiLSTM, and\nCNN baselines. The proposed model achieves a validation accuracy of 84.68\\% and\na macro F1-score of 84.64\\%, surpassing all baseline architectures by a notable\nmargin. Our results show that SETransformer is a competitive and interpretable\nsolution for real-world HAR tasks, with strong potential for deployment in\nmobile and ubiquitous sensing applications."}
{"id": "2505.18613", "pdf": "https://arxiv.org/pdf/2505.18613", "abs": "https://arxiv.org/abs/2505.18613", "authors": ["Faithful Chiagoziem Onwuegbuche", "Adelodun Olaoluwa", "Anca Delia Jurcut", "Liliana Pasquale"], "title": "MLRan: A Behavioural Dataset for Ransomware Analysis and Detection", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Ransomware remains a critical threat to cybersecurity, yet publicly available\ndatasets for training machine learning-based ransomware detection models are\nscarce and often have limited sample size, diversity, and reproducibility. In\nthis paper, we introduce MLRan, a behavioural ransomware dataset, comprising\nover 4,800 samples across 64 ransomware families and a balanced set of goodware\nsamples. The samples span from 2006 to 2024 and encompass the four major types\nof ransomware: locker, crypto, ransomware-as-a-service, and modern variants. We\nalso propose guidelines (GUIDE-MLRan), inspired by previous work, for\nconstructing high-quality behavioural ransomware datasets, which informed the\ncuration of our dataset. We evaluated the ransomware detection performance of\nseveral machine learning (ML) models using MLRan. For this purpose, we\nperformed feature selection by conducting mutual information filtering to\nreduce the initial 6.4 million features to 24,162, followed by recursive\nfeature elimination, yielding 483 highly informative features. The ML models\nachieved an accuracy, precision and recall of up to 98.7%, 98.9%, 98.5%,\nrespectively. Using SHAP and LIME, we identified critical indicators of\nmalicious behaviour, including registry tampering, strings, and API misuse. The\ndataset and source code for feature extraction, selection, ML training, and\nevaluation are available publicly to support replicability and encourage future\nresearch, which can be found at https://github.com/faithfulco/mlran."}
{"id": "2505.19385", "pdf": "https://arxiv.org/pdf/2505.19385", "abs": "https://arxiv.org/abs/2505.19385", "authors": ["Jiaqi Guo", "Santiago Lopez-Tapia", "Aggelos K. Katsaggelos"], "title": "Advancing Limited-Angle CT Reconstruction Through Diffusion-Based Sinogram Completion", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at the 2025 IEEE International Conference on Image\n  Processing (Oral)", "summary": "Limited Angle Computed Tomography (LACT) often faces significant challenges\ndue to missing angular information. Unlike previous methods that operate in the\nimage domain, we propose a new method that focuses on sinogram inpainting. We\nleverage MR-SDEs, a variant of diffusion models that characterize the diffusion\nprocess with mean-reverting stochastic differential equations, to fill in\nmissing angular data at the projection level. Furthermore, by combining\ndistillation with constraining the output of the model using the pseudo-inverse\nof the inpainting matrix, the diffusion process is accelerated and done in a\nstep, enabling efficient and accurate sinogram completion. A subsequent\npost-processing module back-projects the inpainted sinogram into the image\ndomain and further refines the reconstruction, effectively suppressing\nartifacts while preserving critical structural details. Quantitative\nexperimental results demonstrate that the proposed method achieves\nstate-of-the-art performance in both perceptual and fidelity quality, offering\na promising solution for LACT reconstruction in scientific and clinical\napplications."}
{"id": "2505.18614", "pdf": "https://arxiv.org/pdf/2505.18614", "abs": "https://arxiv.org/abs/2505.18614", "authors": ["Woohyun Cho", "Youngmin Kim", "Sunghyun Lee", "Youngjae Yu"], "title": "MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song Translation", "categories": ["cs.CL", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "comment": "28 pages, 8 figures", "summary": "Lyrics translation requires both accurate semantic transfer and preservation\nof musical rhythm, syllabic structure, and poetic style. In animated musicals,\nthe challenge intensifies due to alignment with visual and auditory cues. We\nintroduce Multilingual Audio-Video Lyrics Benchmark for Animated Song\nTranslation (MAVL), the first multilingual, multimodal benchmark for singable\nlyrics translation. By integrating text, audio, and video, MAVL enables richer\nand more expressive translations than text-only approaches. Building on this,\nwe propose Syllable-Constrained Audio-Video LLM with Chain-of-Thought\nSylAVL-CoT, which leverages audio-video cues and enforces syllabic constraints\nto produce natural-sounding lyrics. Experimental results demonstrate that\nSylAVL-CoT significantly outperforms text-based models in singability and\ncontextual accuracy, emphasizing the value of multimodal, multilingual\napproaches for lyrics translation."}
{"id": "2505.19386", "pdf": "https://arxiv.org/pdf/2505.19386", "abs": "https://arxiv.org/abs/2505.19386", "authors": ["Nate Gillman", "Charles Herrmann", "Michael Freeman", "Daksh Aggarwal", "Evan Luo", "Deqing Sun", "Chen Sun"], "title": "Force Prompting: Video Generation Models Can Learn and Generalize Physics-based Control Signals", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://force-prompting.github.io/", "summary": "Recent advances in video generation models have sparked interest in world\nmodels capable of simulating realistic environments. While navigation has been\nwell-explored, physically meaningful interactions that mimic real-world forces\nremain largely understudied. In this work, we investigate using physical forces\nas a control signal for video generation and propose force prompts which enable\nusers to interact with images through both localized point forces, such as\npoking a plant, and global wind force fields, such as wind blowing on fabric.\nWe demonstrate that these force prompts can enable videos to respond\nrealistically to physical control signals by leveraging the visual and motion\nprior in the original pretrained model, without using any 3D asset or physics\nsimulator at inference. The primary challenge of force prompting is the\ndifficulty in obtaining high quality paired force-video training data, both in\nthe real world due to the difficulty of obtaining force signals, and in\nsynthetic data due to limitations in the visual quality and domain diversity of\nphysics simulators. Our key finding is that video generation models can\ngeneralize remarkably well when adapted to follow physical force conditioning\nfrom videos synthesized by Blender, even with limited demonstrations of few\nobjects. Our method can generate videos which simulate forces across diverse\ngeometries, settings, and materials. We also try to understand the source of\nthis generalization and perform ablations that reveal two key elements: visual\ndiversity and the use of specific text keywords during training. Our approach\nis trained on only around 15k training examples for a single day on four A100\nGPUs, and outperforms existing methods on force adherence and physics realism,\nbringing world models closer to real-world physics interactions. We release all\ndatasets, code, weights, and interactive video demos at our project page."}
{"id": "2505.18623", "pdf": "https://arxiv.org/pdf/2505.18623", "abs": "https://arxiv.org/abs/2505.18623", "authors": ["Lucas Saldyt", "Subbarao Kambhampati"], "title": "Mind The Gap: Deep Learning Doesn't Learn Deeply", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "This paper aims to understand how neural networks learn algorithmic reasoning\nby addressing two questions: How faithful are learned algorithms when they are\neffective, and why do neural networks fail to learn effective algorithms\notherwise? To answer these questions, we use neural compilation, a technique\nthat directly encodes a source algorithm into neural network parameters,\nenabling the network to compute the algorithm exactly. This enables comparison\nbetween compiled and conventionally learned parameters, intermediate vectors,\nand behaviors. This investigation is crucial for developing neural networks\nthat robustly learn complexalgorithms from data. Our analysis focuses on graph\nneural networks (GNNs), which are naturally aligned with algorithmic reasoning\ntasks, specifically our choices of BFS, DFS, and Bellman-Ford, which cover the\nspectrum of effective, faithful, and ineffective learned algorithms. Commonly,\nlearning algorithmic reasoning is framed as induction over synthetic data,\nwhere a parameterized model is trained on inputs, traces, and outputs produced\nby an underlying ground truth algorithm. In contrast, we introduce a neural\ncompilation method for GNNs, which sets network parameters analytically,\nbypassing training. Focusing on GNNs leverages their alignment with algorithmic\nreasoning, extensive algorithmic induction literature, and the novel\napplication of neural compilation to GNNs. Overall, this paper aims to\ncharacterize expressability-trainability gaps - a fundamental shortcoming in\nlearning algorithmic reasoning. We hypothesize that inductive learning is most\neffective for parallel algorithms contained within the computational class\n\\texttt{NC}."}
{"id": "2505.19392", "pdf": "https://arxiv.org/pdf/2505.19392", "abs": "https://arxiv.org/abs/2505.19392", "authors": ["Jade Robinson", "Jonathan K. Kummerfeld"], "title": "Simple and Effective Baselines for Code Summarisation Evaluation", "categories": ["cs.CL", "cs.AI", "cs.SE", "68T50", "I.2.7"], "comment": null, "summary": "Code documentation is useful, but writing it is time-consuming. Different\ntechniques for generating code summaries have emerged, but comparing them is\ndifficult because human evaluation is expensive and automatic metrics are\nunreliable. In this paper, we introduce a simple new baseline in which we ask\nan LLM to give an overall score to a summary. Unlike n-gram and embedding-based\nbaselines, our approach is able to consider the code when giving a score. This\nallows us to also make a variant that does not consider the reference summary\nat all, which could be used for other tasks, e.g., to evaluate the quality of\ndocumentation in code bases. We find that our method is as good or better than\nprior metrics, though we recommend using it in conjunction with embedding-based\nmethods to avoid the risk of LLM-specific bias."}
{"id": "2505.18651", "pdf": "https://arxiv.org/pdf/2505.18651", "abs": "https://arxiv.org/abs/2505.18651", "authors": ["Daniel J. Korchinski", "Dhruva Karkada", "Yasaman Bahri", "Matthieu Wyart"], "title": "On the Emergence of Linear Analogies in Word Embeddings", "categories": ["cs.CL", "cond-mat.dis-nn", "cs.LG"], "comment": "Main: 12 pages, 3 figures. Appendices: 8 pages, 7 figures", "summary": "Models such as Word2Vec and GloVe construct word embeddings based on the\nco-occurrence probability $P(i,j)$ of words $i$ and $j$ in text corpora. The\nresulting vectors $W_i$ not only group semantically similar words but also\nexhibit a striking linear analogy structure -- for example, $W_{\\text{king}} -\nW_{\\text{man}} + W_{\\text{woman}} \\approx W_{\\text{queen}}$ -- whose\ntheoretical origin remains unclear. Previous observations indicate that this\nanalogy structure: (i) already emerges in the top eigenvectors of the matrix\n$M(i,j) = P(i,j)/P(i)P(j)$, (ii) strengthens and then saturates as more\neigenvectors of $M (i, j)$, which controls the dimension of the embeddings, are\nincluded, (iii) is enhanced when using $\\log M(i,j)$ rather than $M(i,j)$, and\n(iv) persists even when all word pairs involved in a specific analogy relation\n(e.g., king-queen, man-woman) are removed from the corpus. To explain these\nphenomena, we introduce a theoretical generative model in which words are\ndefined by binary semantic attributes, and co-occurrence probabilities are\nderived from attribute-based interactions. This model analytically reproduces\nthe emergence of linear analogy structure and naturally accounts for properties\n(i)-(iv). It can be viewed as giving fine-grained resolution into the role of\neach additional embedding dimension. It is robust to various forms of noise and\nagrees well with co-occurrence statistics measured on Wikipedia and the analogy\nbenchmark introduced by Mikolov et al."}
{"id": "2505.19395", "pdf": "https://arxiv.org/pdf/2505.19395", "abs": "https://arxiv.org/abs/2505.19395", "authors": ["Ethan TS. Liu", "Austin Wang", "Spencer Mateega", "Carlos Georgescu", "Danny Tang"], "title": "VADER: A Human-Evaluated Benchmark for Vulnerability Assessment, Detection, Explanation, and Remediation", "categories": ["cs.CR", "cs.AI"], "comment": "16 pages, 8 figures, 7 tables", "summary": "Ensuring that large language models (LLMs) can effectively assess, detect,\nexplain, and remediate software vulnerabilities is critical for building robust\nand secure software systems. We introduce VADER, a human-evaluated benchmark\ndesigned explicitly to assess LLM performance across four key\nvulnerability-handling dimensions: assessment, detection, explanation, and\nremediation. VADER comprises 174 real-world software vulnerabilities, each\ncarefully curated from GitHub repositories and annotated by security experts.\nFor each vulnerability case, models are tasked with identifying the flaw,\nclassifying it using Common Weakness Enumeration (CWE), explaining its\nunderlying cause, proposing a patch, and formulating a test plan. Using a\none-shot prompting strategy, we benchmark six state-of-the-art LLMs (Claude 3.7\nSonnet, Gemini 2.5 Pro, GPT-4.1, GPT-4.5, Grok 3 Beta, and o3) on VADER, and\nhuman security experts evaluated each response according to a rigorous scoring\nrubric emphasizing remediation (quality of the code fix, 50%), explanation\n(20%), and classification and test plan (30%) according to a standardized\nrubric. Our results show that current state-of-the-art LLMs achieve only\nmoderate success on VADER - OpenAI's o3 attained 54.7% accuracy overall, with\nothers in the 49-54% range, indicating ample room for improvement. Notably,\nremediation quality is strongly correlated (Pearson r > 0.97) with accurate\nclassification and test plans, suggesting that models that effectively\ncategorize vulnerabilities also tend to fix them well. VADER's comprehensive\ndataset, detailed evaluation rubrics, scoring tools, and visualized results\nwith confidence intervals are publicly released, providing the community with\nan interpretable, reproducible benchmark to advance vulnerability-aware LLMs.\nAll code and data are available at: https://github.com/AfterQuery/vader"}
{"id": "2505.18658", "pdf": "https://arxiv.org/pdf/2505.18658", "abs": "https://arxiv.org/abs/2505.18658", "authors": ["Pankaj Kumar", "Subhankar Mishra"], "title": "Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have emerged as a promising cornerstone for the\ndevelopment of natural language processing (NLP) and artificial intelligence\n(AI). However, ensuring the robustness of LLMs remains a critical challenge. To\naddress these challenges and advance the field, this survey provides a\ncomprehensive overview of current studies in this area. First, we\nsystematically examine the nature of robustness in LLMs, including its\nconceptual foundations, the importance of consistent performance across diverse\ninputs, and the implications of failure modes in real-world applications. Next,\nwe analyze the sources of non-robustness, categorizing intrinsic model\nlimitations, data-driven vulnerabilities, and external adversarial factors that\ncompromise reliability. Following this, we review state-of-the-art mitigation\nstrategies, and then we discuss widely adopted benchmarks, emerging metrics,\nand persistent gaps in assessing real-world reliability. Finally, we synthesize\nfindings from existing surveys and interdisciplinary studies to highlight\ntrends, unresolved issues, and pathways for future research."}
{"id": "2505.19404", "pdf": "https://arxiv.org/pdf/2505.19404", "abs": "https://arxiv.org/abs/2505.19404", "authors": ["Yuta Ono", "Hiroshi Nakamura", "Hideki Takase"], "title": "Exploring the Possibility of TypiClust for Low-Budget Federated Active Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "6 pages. Accepted at COMPSAC 2025", "summary": "Federated Active Learning (FAL) seeks to reduce the burden of annotation\nunder the realistic constraints of federated learning by leveraging Active\nLearning (AL). As FAL settings make it more expensive to obtain ground truth\nlabels, FAL strategies that work well in low-budget regimes, where the amount\nof annotation is very limited, are needed. In this work, we investigate the\neffectiveness of TypiClust, a successful low-budget AL strategy, in low-budget\nFAL settings. Our empirical results show that TypiClust works well even in\nlow-budget FAL settings contrasted with relatively low performances of other\nmethods, although these settings present additional challenges, such as data\nheterogeneity, compared to AL. In addition, we show that FAL settings cause\ndistribution shifts in terms of typicality, but TypiClust is not very\nvulnerable to the shifts. We also analyze the sensitivity of TypiClust to\nfeature extraction methods, and it suggests a way to perform FAL even in\nlimited data situations."}
{"id": "2505.18659", "pdf": "https://arxiv.org/pdf/2505.18659", "abs": "https://arxiv.org/abs/2505.18659", "authors": ["Sangwoo Park", "Matteo Zecchin", "Osvaldo Simeone"], "title": "Adaptive Prediction-Powered AutoEval with Reliability and Efficiency Guarantees", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.ME"], "comment": "submitted", "summary": "Selecting artificial intelligence (AI) models, such as large language models\n(LLMs), from multiple candidates requires accurate performance estimation. This\nis ideally achieved through empirical evaluations involving abundant real-world\ndata. However, such evaluations are costly and impractical at scale. To address\nthis challenge, autoevaluation methods leverage synthetic data produced by\nautomated evaluators, such as LLMs-as-judges, reducing variance but potentially\nintroducing bias. Recent approaches have employed semi-supervised\nprediction-powered inference (\\texttt{PPI}) to correct for the bias of\nautoevaluators. However, the use of autoevaluators may lead in practice to a\ndegradation in sample efficiency compared to conventional methods using only\nreal-world data. In this paper, we propose \\texttt{R-AutoEval+}, a novel\nframework that provides finite-sample reliability guarantees on the model\nevaluation, while also ensuring an enhanced (or at least no worse) sample\nefficiency compared to conventional methods. The key innovation of\n\\texttt{R-AutoEval+} is an adaptive construction of the model evaluation\nvariable, which dynamically tunes its reliance on synthetic data, reverting to\nconventional methods when the autoevaluator is insufficiently accurate.\nExperiments on the use of LLMs-as-judges for the optimization of quantization\nsettings for the weights of an LLM, and for prompt design in LLMs confirm the\nreliability and efficiency of \\texttt{R-AutoEval+}."}
{"id": "2505.19419", "pdf": "https://arxiv.org/pdf/2505.19419", "abs": "https://arxiv.org/abs/2505.19419", "authors": ["Baichuan Li", "Larry Powell", "Tracy Hammond"], "title": "It's Not Just Labeling -- A Research on LLM Generated Feedback Interpretability and Image Labeling Sketch Features", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The quality of training data is critical to the performance of machine\nlearning applications in domains like transportation, healthcare, and robotics.\nAccurate image labeling, however, often relies on time-consuming, expert-driven\nmethods with limited feedback. This research introduces a sketch-based\nannotation approach supported by large language models (LLMs) to reduce\ntechnical barriers and enhance accessibility. Using a synthetic dataset, we\nexamine how sketch recognition features relate to LLM feedback metrics, aiming\nto improve the reliability and interpretability of LLM-assisted labeling. We\nalso explore how prompting strategies and sketch variations influence feedback\nquality. Our main contribution is a sketch-based virtual assistant that\nsimplifies annotation for non-experts and advances LLM-driven labeling tools in\nterms of scalability, accessibility, and explainability."}
{"id": "2505.18664", "pdf": "https://arxiv.org/pdf/2505.18664", "abs": "https://arxiv.org/abs/2505.18664", "authors": ["Evgeny Ugolkov", "Xupeng He", "Hyung Kwak", "Hussein Hoteit"], "title": "Memory-Efficient Super-Resolution of 3D Micro-CT Images Using Octree-Based GANs: Enhancing Resolution and Segmentation Accuracy", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "31 pages, 15 figures", "summary": "We present a memory-efficient algorithm for significantly enhancing the\nquality of segmented 3D micro-Computed Tomography (micro-CT) images of rocks\nusing a generative model. The proposed model achieves a 16x increase in\nresolution and corrects inaccuracies in segmentation caused by the overlapping\nX-ray attenuation in micro-CT measurements across different minerals. The\ngenerative model employed is a 3D Octree-based convolutional Wasserstein\ngenerative adversarial network with gradient penalty. To address the challenge\nof high memory consumption inherent in standard 3D convolutional layers, we\nimplemented an Octree structure within the 3D progressive growing generator\nmodel. This enabled the use of memory-efficient 3D Octree-based convolutional\nlayers. The approach is pivotal in overcoming the long-standing memory\nbottleneck in volumetric deep learning, making it possible to reach 16x\nsuper-resolution in 3D, a scale that is challenging to attain due to cubic\nmemory scaling. For training, we utilized segmented 3D low-resolution micro-CT\nimages along with unpaired segmented complementary 2D high-resolution laser\nscanning microscope images. Post-training, resolution improved from 7 to 0.44\nmicro-m/voxel with accurate segmentation of constituent minerals. Validated on\nBerea sandstone, this framework demonstrates substantial improvements in pore\ncharacterization and mineral differentiation, offering a robust solution to one\nof the primary computational limitations in modern geoscientific imaging."}
{"id": "2505.19423", "pdf": "https://arxiv.org/pdf/2505.19423", "abs": "https://arxiv.org/abs/2505.19423", "authors": ["Bingdong Li", "Mei Jiang", "Hong Qian", "Peng Yang", "Wenjing Hong", "Hong Qian", "Ke Tang"], "title": "Surrogate-Assisted Evolutionary Reinforcement Learning Based on Autoencoder and Hyperbolic Neural Network", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Evolutionary Reinforcement Learning (ERL), training the Reinforcement\nLearning (RL) policies with Evolutionary Algorithms (EAs), have demonstrated\nenhanced exploration capabilities and greater robustness than using traditional\npolicy gradient. However, ERL suffers from the high computational costs and low\nsearch efficiency, as EAs require evaluating numerous candidate policies with\nexpensive simulations, many of which are ineffective and do not contribute\nmeaningfully to the training. One intuitive way to reduce the ineffective\nevaluations is to adopt the surrogates. Unfortunately, existing ERL policies\nare often modeled as deep neural networks (DNNs) and thus naturally represented\nas high-dimensional vectors containing millions of weights, which makes the\nbuilding of effective surrogates for ERL policies extremely challenging. This\npaper proposes a novel surrogate-assisted ERL that integrates Autoencoders (AE)\nand Hyperbolic Neural Networks (HNN). Specifically, AE compresses\nhigh-dimensional policies into low-dimensional representations while extracting\nkey features as the inputs for the surrogate. HNN, functioning as a\nclassification-based surrogate model, can learn complex nonlinear relationships\nfrom sampled data and enable more accurate pre-selection of the sampled\npolicies without real evaluations. The experiments on 10 Atari and 4 Mujoco\ngames have verified that the proposed method outperforms previous approaches\nsignificantly. The search trajectories guided by AE and HNN are also visually\ndemonstrated to be more effective, in terms of both exploration and\nconvergence. This paper not only presents the first learnable policy embedding\nand surrogate-modeling modules for high-dimensional ERL policies, but also\nempirically reveals when and why they can be successful."}
{"id": "2505.18688", "pdf": "https://arxiv.org/pdf/2505.18688", "abs": "https://arxiv.org/abs/2505.18688", "authors": ["Aleksandr Tsymbalov"], "title": "Large Language Models in the Task of Automatic Validation of Text Classifier Predictions", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Machine learning models for text classification are trained to predict a\nclass for a given text. To do this, training and validation samples must be\nprepared: a set of texts is collected, and each text is assigned a class. These\nclasses are usually assigned by human annotators with different expertise\nlevels, depending on the specific classification task. Collecting such samples\nfrom scratch is labor-intensive because it requires finding specialists and\ncompensating them for their work; moreover, the number of available specialists\nis limited, and their productivity is constrained by human factors. While it\nmay not be too resource-intensive to collect samples once, the ongoing need to\nretrain models (especially in incremental learning pipelines) to address data\ndrift (also called model drift) makes the data collection process crucial and\ncostly over the model's entire lifecycle. This paper proposes several\napproaches to replace human annotators with Large Language Models (LLMs) to\ntest classifier predictions for correctness, helping ensure model quality and\nsupport high-quality incremental learning."}
{"id": "2505.19426", "pdf": "https://arxiv.org/pdf/2505.19426", "abs": "https://arxiv.org/abs/2505.19426", "authors": ["Wenyang Xiao", "Haoyu Zhao", "Lingxiao Huang"], "title": "The Role of Diversity in In-Context Learning for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "30 pages", "summary": "In-context learning (ICL) is a crucial capability of current large language\nmodels (LLMs), where the selection of examples plays a key role in performance.\nWhile most existing approaches focus on selecting the most similar examples to\nthe query, the impact of diversity in example selection remains underexplored.\nWe systematically investigate the role of diversity in in-context example\nselection through experiments across a range of tasks, from sentiment\nclassification to more challenging math and code problems. Experiments on\nLlama-3.1, Gemma-2, and Mistral-v0.3 families of models show that\ndiversity-aware selection methods improve performance, particularly on complex\ntasks like math and code, and enhance robustness to out-of-distribution\nqueries. To support these findings, we introduce a theoretical framework that\nexplains the benefits of incorporating diversity in in-context example\nselection."}
{"id": "2505.18720", "pdf": "https://arxiv.org/pdf/2505.18720", "abs": "https://arxiv.org/abs/2505.18720", "authors": ["Meng Li", "Guangda Huzhang", "Haibo Zhang", "Xiting Wang", "Anxiang Zeng"], "title": "Optimal Transport-Based Token Weighting scheme for Enhanced Preference Optimization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "24 pages, 11 figures. Accepted by ACL 2025 (main)", "summary": "Direct Preference Optimization (DPO) has emerged as a promising framework for\naligning Large Language Models (LLMs) with human preferences by directly\noptimizing the log-likelihood difference between chosen and rejected responses.\nHowever, existing methods assign equal importance to all tokens in the\nresponse, while humans focus on more meaningful parts. This leads to suboptimal\npreference optimization, as irrelevant or noisy tokens disproportionately\ninfluence DPO loss. To address this limitation, we propose \\textbf{O}ptimal\n\\textbf{T}ransport-based token weighting scheme for enhancing direct\n\\textbf{P}reference \\textbf{O}ptimization (OTPO). By emphasizing semantically\nmeaningful token pairs and de-emphasizing less relevant ones, our method\nintroduces a context-aware token weighting scheme that yields a more\ncontrastive reward difference estimate. This adaptive weighting enhances reward\nstability, improves interpretability, and ensures that preference optimization\nfocuses on meaningful differences between responses. Extensive experiments have\nvalidated OTPO's effectiveness in improving instruction-following ability\nacross various settings\\footnote{Code is available at\nhttps://github.com/Mimasss2/OTPO.}."}
{"id": "2505.19427", "pdf": "https://arxiv.org/pdf/2505.19427", "abs": "https://arxiv.org/abs/2505.19427", "authors": ["Sihan Chen", "Dan Zhao", "Jongwoo Ko", "Colby Banbury", "Huiping Zhuang", "Luming Liang", "Tianyi Chen"], "title": "WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The growing computational demands of large language models (LLMs) make\nefficient inference and activation strategies increasingly critical. While\nrecent approaches, such as Mixture-of-Experts (MoE), leverage selective\nactivation but require specialized training, training-free sparse activation\nmethods offer broader applicability and superior resource efficiency through\ntheir plug-and-play design. However, many existing methods rely solely on\nhidden state magnitudes to determine activation, resulting in high\napproximation errors and suboptimal inference accuracy. To address these\nlimitations, we propose WINA (Weight Informed Neuron Activation), a novel,\nsimple, and training-free sparse activation framework that jointly considers\nhidden state magnitudes and the column-wise $\\ell_2$-norms of weight matrices.\nWe show that this leads to a sparsification strategy that obtains optimal\napproximation error bounds with theoretical guarantees tighter than existing\ntechniques. Empirically, WINA also outperforms state-of-the-art methods (e.g.,\nTEAL) by up to $2.94\\%$ in average performance at the same sparsity levels,\nacross a diverse set of LLM architectures and datasets. These results position\nWINA as a new performance frontier for training-free sparse activation in LLM\ninference, advancing training-free sparse activation methods and setting a\nrobust baseline for efficient inference. The source code is available at\nhttps://github.com/microsoft/wina."}
{"id": "2505.18726", "pdf": "https://arxiv.org/pdf/2505.18726", "abs": "https://arxiv.org/abs/2505.18726", "authors": ["Mustafa Chasmai", "Wuao Liu", "Subhransu Maji", "Grant Van Horn"], "title": "Audio Geolocation: A Natural Sounds Benchmark", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": null, "summary": "Can we determine someone's geographic location purely from the sounds they\nhear? Are acoustic signals enough to localize within a country, state, or even\ncity? We tackle the challenge of global-scale audio geolocation, formalize the\nproblem, and conduct an in-depth analysis with wildlife audio from the\niNatSounds dataset. Adopting a vision-inspired approach, we convert audio\nrecordings to spectrograms and benchmark existing image geolocation techniques.\nWe hypothesize that species vocalizations offer strong geolocation cues due to\ntheir defined geographic ranges and propose an approach that integrates species\nrange prediction with retrieval-based geolocation. We further evaluate whether\ngeolocation improves when analyzing species-rich recordings or when aggregating\nacross spatiotemporal neighborhoods. Finally, we introduce case studies from\nmovies to explore multimodal geolocation using both audio and visual content.\nOur work highlights the advantages of integrating audio and visual cues, and\nsets the stage for future research in audio geolocation."}
{"id": "2505.19430", "pdf": "https://arxiv.org/pdf/2505.19430", "abs": "https://arxiv.org/abs/2505.19430", "authors": ["Keane Ong", "Rui Mao", "Deeksha Varshney", "Paul Pu Liang", "Erik Cambria", "Gianmarco Mengaldo"], "title": "Deriving Strategic Market Insights with Large Language Models: A Benchmark for Forward Counterfactual Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Counterfactual reasoning typically involves considering alternatives to\nactual events. While often applied to understand past events, a distinct\nform-forward counterfactual reasoning-focuses on anticipating plausible future\ndevelopments. This type of reasoning is invaluable in dynamic financial\nmarkets, where anticipating market developments can powerfully unveil potential\nrisks and opportunities for stakeholders, guiding their decision-making.\nHowever, performing this at scale is challenging due to the cognitive demands\ninvolved, underscoring the need for automated solutions. Large Language Models\n(LLMs) offer promise, but remain unexplored for this application. To address\nthis gap, we introduce a novel benchmark, Fin-Force-FINancial FORward\nCounterfactual Evaluation. By curating financial news headlines and providing\nstructured evaluation, Fin-Force supports LLM based forward counterfactual\ngeneration. This paves the way for scalable and automated solutions for\nexploring and anticipating future market developments, thereby providing\nstructured insights for decision-making. Through experiments on Fin-Force, we\nevaluate state-of-the-art LLMs and counterfactual generation methods, analyzing\ntheir limitations and proposing insights for future research."}
{"id": "2505.18734", "pdf": "https://arxiv.org/pdf/2505.18734", "abs": "https://arxiv.org/abs/2505.18734", "authors": ["Eunjin Roh", "Yigitcan Kaya", "Christopher Kruegel", "Giovanni Vigna", "Sanghyun Hong"], "title": "MADCAT: Combating Malware Detection Under Concept Drift with Test-Time Adaptation", "categories": ["cs.CR", "cs.LG"], "comment": "Pre-print; 4 pages", "summary": "We present MADCAT, a self-supervised approach designed to address the concept\ndrift problem in malware detection. MADCAT employs an encoder-decoder\narchitecture and works by test-time training of the encoder on a small,\nbalanced subset of the test-time data using a self-supervised objective. During\ntest-time training, the model learns features that are useful for detecting\nboth previously seen (old) data and newly arriving samples. We demonstrate the\neffectiveness of MADCAT in continuous Android malware detection settings.\nMADCAT consistently outperforms baseline methods in detection performance at\ntest time. We also show the synergy between MADCAT and prior approaches in\naddressing concept drift in malware detection"}
{"id": "2505.19434", "pdf": "https://arxiv.org/pdf/2505.19434", "abs": "https://arxiv.org/abs/2505.19434", "authors": ["X. Feng", "D. Zhang", "S. Hu", "X. Li", "M. Wu", "J. Zhang", "X. Chen", "K. Huang"], "title": "CSTrack: Enhancing RGB-X Tracking via Compact Spatiotemporal Features", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICML25!", "summary": "Effectively modeling and utilizing spatiotemporal features from RGB and other\nmodalities (\\eg, depth, thermal, and event data, denoted as X) is the core of\nRGB-X tracker design. Existing methods often employ two parallel branches to\nseparately process the RGB and X input streams, requiring the model to\nsimultaneously handle two dispersed feature spaces, which complicates both the\nmodel structure and computation process. More critically, intra-modality\nspatial modeling within each dispersed space incurs substantial computational\noverhead, limiting resources for inter-modality spatial modeling and temporal\nmodeling. To address this, we propose a novel tracker, CSTrack, which focuses\non modeling Compact Spatiotemporal features to achieve simple yet effective\ntracking. Specifically, we first introduce an innovative Spatial Compact Module\nthat integrates the RGB-X dual input streams into a compact spatial feature,\nenabling thorough intra- and inter-modality spatial modeling. Additionally, we\ndesign an efficient Temporal Compact Module that compactly represents temporal\nfeatures by constructing the refined target distribution heatmap. Extensive\nexperiments validate the effectiveness of our compact spatiotemporal modeling\nmethod, with CSTrack achieving new SOTA results on mainstream RGB-X benchmarks.\nThe code and models will be released at:\nhttps://github.com/XiaokunFeng/CSTrack."}
{"id": "2505.18745", "pdf": "https://arxiv.org/pdf/2505.18745", "abs": "https://arxiv.org/abs/2505.18745", "authors": ["Umar Marikkar", "Syed Sameed Husain", "Muhammad Awais", "Sara Atito"], "title": "C3R: Channel Conditioned Cell Representations for unified evaluation in microscopy imaging", "categories": ["cs.CV", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Immunohistochemical (IHC) images reveal detailed information about structures\nand functions at the subcellular level. However, unlike natural images, IHC\ndatasets pose challenges for deep learning models due to their inconsistencies\nin channel count and configuration, stemming from varying staining protocols\nacross laboratories and studies. Existing approaches build channel-adaptive\nmodels, which unfortunately fail to support out-of-distribution (OOD)\nevaluation across IHC datasets and cannot be applied in a true zero-shot\nsetting with mismatched channel counts. To address this, we introduce a\nstructured view of cellular image channels by grouping them into either context\nor concept, where we treat the context channels as a reference to the concept\nchannels in the image. We leverage this context-concept principle to develop\nChannel Conditioned Cell Representations (C3R), a framework designed for\nunified evaluation on in-distribution (ID) and OOD datasets. C3R is a two-fold\nframework comprising a channel-adaptive encoder architecture and a masked\nknowledge distillation training strategy, both built around the context-concept\nprinciple. We find that C3R outperforms existing benchmarks on both ID and OOD\ntasks, while a trivial implementation of our core idea also outperforms the\nchannel-adaptive methods reported on the CHAMMI benchmark. Our method opens a\nnew pathway for cross-dataset generalization between IHC datasets, without\nrequiring dataset-specific adaptation or retraining."}
{"id": "2505.19441", "pdf": "https://arxiv.org/pdf/2505.19441", "abs": "https://arxiv.org/abs/2505.19441", "authors": ["Jing Nathan Yan", "Junxiong Wang", "Jeffrey M. Rzeszotarski", "Allison Koenecke"], "title": "Fairness Practices in Industry: A Case Study in Machine Learning Teams Building Recommender Systems", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "The rapid proliferation of recommender systems necessitates robust fairness\npractices to address inherent biases. Assessing fairness, though, is\nchallenging due to constantly evolving metrics and best practices. This paper\nanalyzes how industry practitioners perceive and incorporate these changing\nfairness standards in their workflows. Through semi-structured interviews with\n11 practitioners from technical teams across a range of large technology\ncompanies, we investigate industry implementations of fairness in\nrecommendation system products. We focus on current debiasing practices,\napplied metrics, collaborative strategies, and integrating academic research\ninto practice. Findings show a preference for multi-dimensional debiasing over\ntraditional demographic methods, and a reliance on intuitive rather than\nacademic metrics. This study also highlights the difficulties in balancing\nfairness with both the practitioner's individual (bottom-up) roles and\norganizational (top-down) workplace constraints, including the interplay with\nlegal and compliance experts. Finally, we offer actionable recommendations for\nthe recommender system community and algorithmic fairness practitioners,\nunderlining the need to refine fairness practices continually."}
{"id": "2505.18747", "pdf": "https://arxiv.org/pdf/2505.18747", "abs": "https://arxiv.org/abs/2505.18747", "authors": ["Xiaolu Chen", "Chenghao Huang", "Yanru Zhang", "Hao Wang"], "title": "Season-Independent PV Disaggregation Using Multi-Scale Net Load Temporal Feature Extraction and Weather Factor Fusion", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "2024 IEEE 8th Conference on Energy Internet and Energy System\n  Integration (EI2)", "summary": "With the advancement of energy Internet and energy system integration, the\nincreasing adoption of distributed photovoltaic (PV) systems presents new\nchallenges on smart monitoring and measurement for utility companies,\nparticularly in separating PV generation from net electricity load. Existing\nmethods struggle with feature extraction from net load and capturing the\nrelevance between weather factors. This paper proposes a PV disaggregation\nmethod that integrates Hierarchical Interpolation (HI) and multi-head\nself-attention mechanisms. By using HI to extract net load features and\nmulti-head self-attention to capture the complex dependencies between weather\nfactors, the method achieves precise PV generation predictions. Simulation\nexperiments demonstrate the effectiveness of the proposed method in real-world\ndata, supporting improved monitoring and management of distributed energy\nsystems."}
{"id": "2505.19443", "pdf": "https://arxiv.org/pdf/2505.19443", "abs": "https://arxiv.org/abs/2505.19443", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee"], "title": "Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications of Agentic AI", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "35 Pages, 8 Figures, 6 Tables", "summary": "This review presents a comprehensive analysis of two emerging paradigms in\nAI-assisted software development: vibe coding and agentic coding. While both\nleverage large language models (LLMs), they differ fundamentally in autonomy,\narchitectural design, and the role of the developer. Vibe coding emphasizes\nintuitive, human-in-the-loop interaction through prompt-based, conversational\nworkflows that support ideation, experimentation, and creative exploration. In\ncontrast, agentic coding enables autonomous software development through\ngoal-driven agents capable of planning, executing, testing, and iterating tasks\nwith minimal human intervention. We propose a detailed taxonomy spanning\nconceptual foundations, execution models, feedback loops, safety mechanisms,\ndebugging strategies, and real-world tool ecosystems. Through comparative\nworkflow analysis and 20 detailed use cases, we illustrate how vibe systems\nthrive in early-stage prototyping and education, while agentic systems excel in\nenterprise-grade automation, codebase refactoring, and CI/CD integration. We\nfurther examine emerging trends in hybrid architectures, where natural language\ninterfaces are coupled with autonomous execution pipelines. Finally, we\narticulate a future roadmap for agentic AI, outlining the infrastructure needed\nfor trustworthy, explainable, and collaborative systems. Our findings suggest\nthat successful AI software engineering will rely not on choosing one paradigm,\nbut on harmonizing their strengths within a unified, human-centered development\nlifecycle."}
{"id": "2505.18759", "pdf": "https://arxiv.org/pdf/2505.18759", "abs": "https://arxiv.org/abs/2505.18759", "authors": ["Ruichen Zhang", "Rana Muhammad Shahroz Khan", "Zhen Tan", "Dawei Li", "Song Wang", "Tianlong Chen"], "title": "The Quest for Efficient Reasoning: A Data-Centric Benchmark to CoT Distillation", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Data-centric distillation, including data augmentation, selection, and\nmixing, offers a promising path to creating smaller, more efficient student\nLarge Language Models (LLMs) that retain strong reasoning abilities. However,\nthere still lacks a comprehensive benchmark to systematically assess the effect\nof each distillation approach. This paper introduces DC-CoT, the first\ndata-centric benchmark that investigates data manipulation in chain-of-thought\n(CoT) distillation from method, model and data perspectives. Utilizing various\nteacher models (e.g., o4-mini, Gemini-Pro, Claude-3.5) and student\narchitectures (e.g., 3B, 7B parameters), we rigorously evaluate the impact of\nthese data manipulations on student model performance across multiple reasoning\ndatasets, with a focus on in-distribution (IID) and out-of-distribution (OOD)\ngeneralization, and cross-domain transfer. Our findings aim to provide\nactionable insights and establish best practices for optimizing CoT\ndistillation through data-centric techniques, ultimately facilitating the\ndevelopment of more accessible and capable reasoning models. The dataset can be\nfound at https://huggingface.co/datasets/rana-shahroz/DC-COT, while our code is\nshared in https://anonymous.4open.science/r/DC-COT-FF4C/."}
{"id": "2505.19455", "pdf": "https://arxiv.org/pdf/2505.19455", "abs": "https://arxiv.org/abs/2505.19455", "authors": ["Xu Li", "Fan Lyu"], "title": "MM-Prompt: Cross-Modal Prompt Tuning for Continual Visual Question Answering", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Continual Visual Question Answering (CVQA) based on pre-trained models(PTMs)\nhas achieved promising progress by leveraging prompt tuning to enable continual\nmulti-modal learning. However, most existing methods adopt cross-modal prompt\nisolation, constructing visual and textual prompts separately, which\nexacerbates modality imbalance and leads to degraded performance over time. To\ntackle this issue, we propose MM-Prompt, a novel framework incorporating\ncross-modal prompt query and cross-modal prompt recovery. The former enables\nbalanced prompt selection by incorporating cross-modal signals during query\nformation, while the latter promotes joint prompt reconstruction through\niterative cross-modal interactions, guided by an alignment loss to prevent\nrepresentational drift. Extensive experiments show that MM-Prompt surpasses\nprior approaches in accuracy and knowledge retention, while maintaining\nbalanced modality engagement throughout continual learning."}
{"id": "2505.18761", "pdf": "https://arxiv.org/pdf/2505.18761", "abs": "https://arxiv.org/abs/2505.18761", "authors": ["Minglai Yang", "Ethan Huang", "Liang Zhang", "Mihai Surdeanu", "William Wang", "Liangming Pan"], "title": "How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "15 pages, 9 figure, 4 tables", "summary": "We introduce Grade School Math with Distracting Context (GSM-DC), a synthetic\nbenchmark to evaluate Large Language Models' (LLMs) reasoning robustness\nagainst systematically controlled irrelevant context (IC). GSM-DC constructs\nsymbolic reasoning graphs with precise distractor injections, enabling\nrigorous, reproducible evaluation. Our experiments demonstrate that LLMs are\nsignificantly sensitive to IC, affecting both reasoning path selection and\narithmetic accuracy. Additionally, training models with strong distractors\nimproves performance in both in-distribution and out-of-distribution scenarios.\nWe further propose a stepwise tree search guided by a process reward model,\nwhich notably enhances robustness in out-of-distribution conditions."}
{"id": "2505.19459", "pdf": "https://arxiv.org/pdf/2505.19459", "abs": "https://arxiv.org/abs/2505.19459", "authors": ["Kaichao Jiang", "He Wang", "Xiaoshuai Hao", "Xiulong Yang", "Ajian Liu", "Qi Chu", "Yunfeng Diao"], "title": "Your Classifier Can Do More: Towards Bridging the Gaps in Classification, Robustness, and Generation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Joint Energy-based Models (JEMs), a class of hybrid generative-discriminative\nmodels, are well known for their ability to achieve both high classification\naccuracy and generative capability within a single model. However, their\nrobustness still lags significantly behind the classifiers based adversarial\ntraining (AT). Conversely, while AT is currently the most effective approach to\nimproving the classifier's robustness, it typically sacrifices accuracy on\nclean data and lacks generative capability. The triple trade-off between\nclassification accuracy, generative capability and robustness, raises a natural\nquestion: Can a single model simultaneously achieve high classification\naccuracy, adversarial robustness, and generative performance? -- a goal that\nhas been rarely explored. To address this question, we systematically analyze\nthe energy distribution differences of clean, adversarial, and generated\nsamples across various JEM variants and adversarially trained models. We\nobserve that AT tends to reduce the energy gap between clean and adversarial\nsamples, while JEMs reduce the gap between clean and synthetic ones. This\nobservation suggests a key insight: if the energy distributions of all three\ndata types can be aligned, we might unify the strengths of AT and JEMs,\nresolving their inherent trade-offs. Building on this idea, we propose\nEnergy-based Joint Distribution Adversarial Training (EB-JDAT), to jointly\nmodel the clean data distribution, the adversarial distribution, and the\nclassifier by maximizing their joint probability. EB-JDAT is a general and\nflexible optimization method, compatible with various JEM variants. Extensive\nexperimental results demonstrate that EB-JDAT not only maintains near original\naccuracy and generative capability of JEMs, but also significantly enhances\nrobustness, even surpassing state-of-the-art ATs."}
{"id": "2505.18770", "pdf": "https://arxiv.org/pdf/2505.18770", "abs": "https://arxiv.org/abs/2505.18770", "authors": ["Yuedi Zhang", "Shuanghao Bai", "Wanqi Zhou", "Zhirong Luan", "Badong Chen"], "title": "Dual-Path Stable Soft Prompt Generation for Domain Generalization", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Domain generalization (DG) aims to learn a model using data from one or\nmultiple related but distinct source domains that can generalize well to unseen\nout-of-distribution target domains. Inspired by the success of large\npre-trained vision-language models (VLMs), prompt tuning has emerged as an\neffective generalization strategy. However, it often struggles to capture\ndomain-specific features due to its reliance on manually or fixed prompt\ninputs. Recently, some prompt generation methods have addressed this limitation\nby dynamically generating instance-specific and domain-specific prompts for\neach input, enriching domain information and demonstrating potential for\nenhanced generalization. Through further investigation, we identify a notable\nissue in existing prompt generation methods: the same input often yields\nsignificantly different and suboptimal prompts across different random seeds, a\nphenomenon we term Prompt Variability. To address this, we introduce negative\nlearning into the prompt generation process and propose Dual-Path Stable Soft\nPrompt Generation (DPSPG), a transformer-based framework designed to improve\nboth the stability and generalization of prompts. Specifically, DPSPG\nincorporates a complementary prompt generator to produce negative prompts,\nthereby reducing the risk of introducing misleading information. Both\ntheoretical and empirical analyses demonstrate that negative learning leads to\nmore robust and effective prompts by increasing the effective margin and\nreducing the upper bound of the gradient norm. Extensive experiments on five DG\nbenchmark datasets show that DPSPG consistently outperforms state-of-the-art\nmethods while maintaining prompt stability."}
{"id": "2505.19465", "pdf": "https://arxiv.org/pdf/2505.19465", "abs": "https://arxiv.org/abs/2505.19465", "authors": ["Hengwei Zhang", "Minghui Wu", "Li Qiao", "Ling Liu", "Ziqi Han", "Zhen Gao"], "title": "Residual Cross-Attention Transformer-Based Multi-User CSI Feedback with Deep Joint Source-Channel Coding", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This letter proposes a deep-learning (DL)-based multi-user channel state\ninformation (CSI) feedback framework for massive multiple-input multiple-output\nsystems, where the deep joint source-channel coding (DJSCC) is utilized to\nimprove the CSI reconstruction accuracy. Specifically, we design a multi-user\njoint CSI feedback framework, whereby the CSI correlation of nearby users is\nutilized to reduce the feedback overhead. Under the framework, we propose a new\nresidual cross-attention transformer architecture, which is deployed at the\nbase station to further improve the CSI feedback performance. Moreover, to\ntackle the \"cliff-effect\" of conventional bit-level CSI feedback approaches, we\nintegrated DJSCC into the multi-user CSI feedback, together with utilizing a\ntwo-stage training scheme to adapt to varying uplink noise levels. Experimental\nresults demonstrate the superiority of our methods in CSI feedback performance,\nwith low network complexity and better scalability."}
{"id": "2505.18772", "pdf": "https://arxiv.org/pdf/2505.18772", "abs": "https://arxiv.org/abs/2505.18772", "authors": ["Michal Edelstein", "Hsueh-Ti Derek Liu", "Mirela Ben-Chen"], "title": "CageNet: A Meta-Framework for Learning on Wild Meshes", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "11 pages, 13 figures (excluding supplementary material)", "summary": "Learning on triangle meshes has recently proven to be instrumental to a\nmyriad of tasks, from shape classification, to segmentation, to deformation and\nanimation, to mention just a few. While some of these applications are tackled\nthrough neural network architectures which are tailored to the application at\nhand, many others use generic frameworks for triangle meshes where the only\ncustomization required is the modification of the input features and the loss\nfunction. Our goal in this paper is to broaden the applicability of these\ngeneric frameworks to \"wild\", i.e. meshes in-the-wild which often have multiple\ncomponents, non-manifold elements, disrupted connectivity, or a combination of\nthese. We propose a configurable meta-framework based on the concept of caged\ngeometry: Given a mesh, a cage is a single component manifold triangle mesh\nthat envelopes it closely. Generalized barycentric coordinates map between\nfunctions on the cage, and functions on the mesh, allowing us to learn and test\non a variety of data, in different applications. We demonstrate this concept by\nlearning segmentation and skinning weights on difficult data, achieving better\nperformance to state of the art techniques on wild meshes."}
{"id": "2505.19469", "pdf": "https://arxiv.org/pdf/2505.19469", "abs": "https://arxiv.org/abs/2505.19469", "authors": ["Mingzhuo Li", "Guang Li", "Jiafeng Mao", "Takahiro Ogawa", "Miki Haseyama"], "title": "Diversity-Driven Generative Dataset Distillation Based on Diffusion Model with Self-Adaptive Memory", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Accepted by ICIP 2025", "summary": "Dataset distillation enables the training of deep neural networks with\ncomparable performance in significantly reduced time by compressing large\ndatasets into small and representative ones. Although the introduction of\ngenerative models has made great achievements in this field, the distributions\nof their distilled datasets are not diverse enough to represent the original\nones, leading to a decrease in downstream validation accuracy. In this paper,\nwe present a diversity-driven generative dataset distillation method based on a\ndiffusion model to solve this problem. We introduce self-adaptive memory to\nalign the distribution between distilled and real datasets, assessing the\nrepresentativeness. The degree of alignment leads the diffusion model to\ngenerate more diverse datasets during the distillation process. Extensive\nexperiments show that our method outperforms existing state-of-the-art methods\nin most situations, proving its ability to tackle dataset distillation tasks."}
{"id": "2505.18773", "pdf": "https://arxiv.org/pdf/2505.18773", "abs": "https://arxiv.org/abs/2505.18773", "authors": ["Jamie Hayes", "Ilia Shumailov", "Christopher A. Choquette-Choo", "Matthew Jagielski", "George Kaissis", "Katherine Lee", "Milad Nasr", "Sahra Ghalebikesabi", "Niloofar Mireshghallah", "Meenatchi Sundaram Mutu Selva Annamalai", "Igor Shilov", "Matthieu Meeus", "Yves-Alexandre de Montjoye", "Franziska Boenisch", "Adam Dziedzic", "A. Feder Cooper"], "title": "Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "State-of-the-art membership inference attacks (MIAs) typically require\ntraining many reference models, making it difficult to scale these attacks to\nlarge pre-trained language models (LLMs). As a result, prior research has\neither relied on weaker attacks that avoid training reference models (e.g.,\nfine-tuning attacks), or on stronger attacks applied to small-scale models and\ndatasets. However, weaker attacks have been shown to be brittle - achieving\nclose-to-arbitrary success - and insights from strong attacks in simplified\nsettings do not translate to today's LLMs. These challenges have prompted an\nimportant question: are the limitations observed in prior work due to attack\ndesign choices, or are MIAs fundamentally ineffective on LLMs? We address this\nquestion by scaling LiRA - one of the strongest MIAs - to GPT-2 architectures\nranging from 10M to 1B parameters, training reference models on over 20B tokens\nfrom the C4 dataset. Our results advance the understanding of MIAs on LLMs in\nthree key ways: (1) strong MIAs can succeed on pre-trained LLMs; (2) their\neffectiveness, however, remains limited (e.g., AUC<0.7) in practical settings;\nand, (3) the relationship between MIA success and related privacy metrics is\nnot as straightforward as prior work has suggested."}
{"id": "2505.19481", "pdf": "https://arxiv.org/pdf/2505.19481", "abs": "https://arxiv.org/abs/2505.19481", "authors": ["Hao Kang", "Qingru Zhang", "Han Cai", "Weiyuan Xu", "Tushar Krishna", "Yilun Du", "Tsachy Weissman"], "title": "Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.MA"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable performance across diverse\nreasoning and generation tasks, and are increasingly deployed as agents in\ndynamic environments such as code generation and recommendation systems.\nHowever, many real-world applications, such as high-frequency trading and\nreal-time competitive gaming, require decisions under strict latency\nconstraints, where faster responses directly translate into higher rewards.\nDespite the importance of this latency quality trade off, it remains\nunderexplored in the context of LLM based agents. In this work, we present the\nfirst systematic study of this trade off in real time decision making tasks. To\nsupport our investigation, we introduce two new benchmarks: HFTBench, a high\nfrequency trading simulation, and StreetFighter, a competitive gaming platform.\nOur analysis reveals that optimal latency quality balance varies by task, and\nthat sacrificing quality for lower latency can significantly enhance downstream\nperformance. To address this, we propose FPX, an adaptive framework that\ndynamically selects model size and quantization level based on real time\ndemands. Our method achieves the best performance on both benchmarks, improving\nwin rate by up to 80% in Street Fighter and boosting daily yield by up to\n26.52% in trading, underscoring the need for latency aware evaluation and\ndeployment strategies for LLM based agents. These results demonstrate the\ncritical importance of latency aware evaluation and deployment strategies for\nreal world LLM based agents. Our benchmarks are available at Latency Sensitive\nBenchmarks."}
{"id": "2505.18780", "pdf": "https://arxiv.org/pdf/2505.18780", "abs": "https://arxiv.org/abs/2505.18780", "authors": ["Yahao Fan", "Tianxiang Gui", "Kaiyang Ji", "Shutong Ding", "Chixuan Zhang", "Jiayuan Gu", "Jingyi Yu", "Jingya Wang", "Ye Shi"], "title": "One Policy but Many Worlds: A Scalable Unified Policy for Versatile Humanoid Locomotion", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Humanoid locomotion faces a critical scalability challenge: traditional\nreinforcement learning (RL) methods require task-specific rewards and struggle\nto leverage growing datasets, even as more training terrains are introduced. We\npropose DreamPolicy, a unified framework that enables a single policy to master\ndiverse terrains and generalize zero-shot to unseen scenarios by systematically\nintegrating offline data and diffusion-driven motion synthesis. At its core,\nDreamPolicy introduces Humanoid Motion Imagery (HMI) - future state predictions\nsynthesized through an autoregressive terrain-aware diffusion planner curated\nby aggregating rollouts from specialized policies across various distinct\nterrains. Unlike human motion datasets requiring laborious retargeting, our\ndata directly captures humanoid kinematics, enabling the diffusion planner to\nsynthesize \"dreamed\" trajectories that encode terrain-specific physical\nconstraints. These trajectories act as dynamic objectives for our\nHMI-conditioned policy, bypassing manual reward engineering and enabling\ncross-terrain generalization. DreamPolicy addresses the scalability limitations\nof prior methods: while traditional RL fails to exploit growing datasets, our\nframework scales seamlessly with more offline data. As the dataset expands, the\ndiffusion prior learns richer locomotion skills, which the policy leverages to\nmaster new terrains without retraining. Experiments demonstrate that\nDreamPolicy achieves average 90% success rates in training environments and an\naverage of 20% higher success on unseen terrains than the prevalent method. It\nalso generalizes to perturbed and composite scenarios where prior approaches\ncollapse. By unifying offline data, diffusion-based trajectory synthesis, and\npolicy optimization, DreamPolicy overcomes the \"one task, one policy\"\nbottleneck, establishing a paradigm for scalable, data-driven humanoid control."}
{"id": "2505.19488", "pdf": "https://arxiv.org/pdf/2505.19488", "abs": "https://arxiv.org/abs/2505.19488", "authors": ["Shu Zhong", "Mingyu Xu", "Tenglong Ao", "Guang Shi"], "title": "Understanding Transformer from the Perspective of Associative Memory", "categories": ["cs.LG", "cs.AI"], "comment": "Consider this post less as a formal research paper and more as a\n  blog-style sharing of our current reflections, intended to spark discussion\n  as one might in a collaborative team meeting", "summary": "In this paper, we share our reflections and insights on understanding\nTransformer architectures through the lens of associative memory--a classic\npsychological concept inspired by human cognition. We start with the basics of\nassociative memory (think simple linear attention) and then dive into two\ndimensions:\n  Memory Capacity: How much can a Transformer really remember, and how well? We\nintroduce retrieval SNR to measure this and use a kernel perspective to\nmathematically reveal why Softmax Attention is so effective. We also show how\nFFNs can be seen as a type of associative memory, leading to insights on their\ndesign and potential improvements.\n  Memory Update: How do these memories learn and evolve? We present a unified\nframework for understanding how different Transformer variants (like DeltaNet\nand Softmax Attention) update their \"knowledge base\". This leads us to tackle\ntwo provocative questions: 1. Are Transformers fundamentally limited in what\nthey can express, and can we break these barriers? 2. If a Transformer had\ninfinite context, would it become infinitely intelligent?\n  We want to demystify Transformer architecture, offering a clearer\nunderstanding of existing designs. This exploration aims to provide fresh\ninsights and spark new avenues for Transformer innovation."}
{"id": "2505.18784", "pdf": "https://arxiv.org/pdf/2505.18784", "abs": "https://arxiv.org/abs/2505.18784", "authors": ["Jihong Wang", "Chung-Hao Lee", "William Richardson", "Yue Yu"], "title": "A physics-guided smoothing method for material modeling with digital image correlation (DIC) measurements", "categories": ["eess.IV", "cond-mat.mtrl-sci", "cs.LG"], "comment": null, "summary": "In this work, we present a novel approach to process the DIC measurements of\nmultiple biaxial stretching protocols. In particular, we develop a\noptimization-based approach, which calculates the smoothed nodal displacements\nusing a moving least-squares algorithm subject to positive strain constraints.\nAs such, physically consistent displacement and strain fields are obtained.\nThen, we further deploy a data-driven workflow to heterogeneous material\nmodeling from these physically consistent DIC measurements, by estimating a\nnonlocal constitutive law together with the material microstructure. To\ndemonstrate the applicability of our approach, we apply it in learning a\nmaterial model and fiber orientation field from DIC measurements of a porcine\ntricuspid valve anterior leaflet. Our results demonstrate that the proposed DIC\ndata processing approach can significantly improve the accuracy of modeling\nbiological materials."}
{"id": "2505.19498", "pdf": "https://arxiv.org/pdf/2505.19498", "abs": "https://arxiv.org/abs/2505.19498", "authors": ["Nanxing Hu", "Xiaoyue Duan", "Jinchao Zhang", "Guoliang Kang"], "title": "Enhancing Visual Reliance in Text Generation: A Bayesian Perspective on Mitigating Hallucination in Large Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) usually generate texts which satisfy\ncontext coherence but don't match the visual input. Such a hallucination issue\nhinders LVLMs' applicability in the real world. The key to solving\nhallucination in LVLM is to make the text generation rely more on the visual\ncontent. Most previous works choose to enhance/adjust the features/output of a\nspecific modality (i.e., visual or textual) to alleviate hallucinations in\nLVLM, which do not explicitly or systematically enhance the visual reliance. In\nthis paper, we comprehensively investigate the factors which may degenerate the\nvisual reliance in text generation of LVLM from a Bayesian perspective. Based\non our observations, we propose to mitigate hallucination in LVLM from three\naspects. Firstly, we observe that not all visual tokens are informative in\ngenerating meaningful texts. We propose to evaluate and remove redundant visual\ntokens to avoid their disturbance. Secondly, LVLM may encode inappropriate\nprior information, making it lean toward generating unexpected words. We\npropose a simple yet effective way to rectify the prior from a Bayesian\nperspective. Thirdly, we observe that starting from certain steps, the\nposterior of next-token prediction conditioned on visual tokens may collapse to\na prior distribution which does not depend on any informative visual tokens at\nall. Thus, we propose to stop further text generation to avoid hallucination.\nExtensive experiments on three benchmarks including POPE, CHAIR, and MME\ndemonstrate that our method can consistently mitigate the hallucination issue\nof LVLM and performs favorably against previous state-of-the-arts."}
{"id": "2505.18858", "pdf": "https://arxiv.org/pdf/2505.18858", "abs": "https://arxiv.org/abs/2505.18858", "authors": ["Maeva Guerrier", "Karthik Soma", "Hassan Fouad", "Giovanni Beltrame"], "title": "Guided by Guardrails: Control Barrier Functions as Safety Instructors for Robotic Learning", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Safety stands as the primary obstacle preventing the widespread adoption of\nlearning-based robotic systems in our daily lives. While reinforcement learning\n(RL) shows promise as an effective robot learning paradigm, conventional RL\nframeworks often model safety by using single scalar negative rewards with\nimmediate episode termination, failing to capture the temporal consequences of\nunsafe actions (e.g., sustained collision damage). In this work, we introduce a\nnovel approach that simulates these temporal effects by applying continuous\nnegative rewards without episode termination. Our experiments reveal that\nstandard RL methods struggle with this model, as the accumulated negative\nvalues in unsafe zones create learning barriers. To address this challenge, we\ndemonstrate how Control Barrier Functions (CBFs), with their proven safety\nguarantees, effectively help robots avoid catastrophic regions while enhancing\nlearning outcomes. We present three CBF-based approaches, each integrating\ntraditional RL methods with Control Barrier Functions, guiding the agent to\nlearn safe behavior. Our empirical analysis, conducted in both simulated\nenvironments and real-world settings using a four-wheel differential drive\nrobot, explores the possibilities of employing these approaches for safe\nrobotic learning."}
{"id": "2505.19502", "pdf": "https://arxiv.org/pdf/2505.19502", "abs": "https://arxiv.org/abs/2505.19502", "authors": ["Guang Yang", "Yu Zhou", "Xiang Chen", "Wei Zheng", "Xing Hu", "Xin Zhou", "David Lo", "Taolue Chen"], "title": "CODE-DITING: A Reasoning-Based Metric for Functional Alignment in Code Evaluation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Trustworthy evaluation methods for code snippets play a crucial role in\nneural code generation. Traditional methods, which either rely on reference\nsolutions or require executable test cases, have inherent limitation in\nflexibility and scalability. The recent LLM-as-Judge methodology offers a\npromising alternative by directly evaluating functional consistency between the\nproblem description and the generated code. To systematically understand the\nlandscape of these LLM-as-Judge methods, we conduct a comprehensive empirical\nstudy across three diverse datasets. Our investigation reveals the pros and\ncons of two categories of LLM-as-Judge methods: the methods based on general\nfoundation models can achieve good performance but require complex prompts and\nlack explainability, while the methods based on reasoning foundation models\nprovide better explainability with simpler prompts but demand substantial\ncomputational resources due to their large parameter sizes. To address these\nlimitations, we propose CODE-DITING, a novel code evaluation method that\nbalances accuracy, efficiency and explainability. We develop a data\ndistillation framework that effectively transfers reasoning capabilities from\nDeepSeek-R1671B to our CODE-DITING 1.5B and 7B models, significantly enhancing\nevaluation explainability and reducing the computational cost. With the\nmajority vote strategy in the inference process, CODE-DITING 1.5B outperforms\nall models with the same magnitude of parameters and achieves performance which\nwould normally exhibit in a model with 5 times of parameter scale. CODE-DITING\n7B surpasses GPT-4o and DeepSeek-V3 671B, even though it only uses 1% of the\nparameter volume of these large models. Further experiments show that\nCODEDITING is robust to preference leakage and can serve as a promising\nalternative for code evaluation."}
{"id": "2505.18867", "pdf": "https://arxiv.org/pdf/2505.18867", "abs": "https://arxiv.org/abs/2505.18867", "authors": ["Ming Cheng", "Jiaying Gong", "Hoda Eldardiry"], "title": "Sci-LoRA: Mixture of Scientific LoRAs for Cross-Domain Lay Paraphrasing", "categories": ["cs.CL", "cs.LG"], "comment": "18 pages, 3 figures, ACL 2025 Findings", "summary": "Lay paraphrasing aims to make scientific information accessible to audiences\nwithout technical backgrounds. However, most existing studies focus on a single\ndomain, such as biomedicine. With the rise of interdisciplinary research, it is\nincreasingly necessary to comprehend knowledge spanning multiple technical\nfields. To address this, we propose Sci-LoRA, a model that leverages a mixture\nof LoRAs fine-tuned on multiple scientific domains. In particular, Sci-LoRA\ndynamically generates and applies weights for each LoRA, enabling it to adjust\nthe impact of different domains based on the input text, without requiring\nexplicit domain labels. To balance domain-specific knowledge and generalization\nacross various domains, Sci-LoRA integrates information at both the data and\nmodel levels. This dynamic fusion enhances the adaptability and performance\nacross various domains. Experimental results across twelve domains on five\npublic datasets show that Sci-LoRA significantly outperforms state-of-the-art\nlarge language models and demonstrates flexible generalization and adaptability\nin cross-domain lay paraphrasing."}
{"id": "2505.19504", "pdf": "https://arxiv.org/pdf/2505.19504", "abs": "https://arxiv.org/abs/2505.19504", "authors": ["Pingzhi Li", "Zhen Tan", "Huaizhi Qu", "Huan Liu", "Tianlong Chen"], "title": "DOGe: Defensive Output Generation for LLM Protection Against Knowledge Distillation", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Code is available at https://github.com/UNITES-Lab/DOGe", "summary": "Large Language Models (LLMs) represent substantial intellectual and economic\ninvestments, yet their effectiveness can inadvertently facilitate model\nimitation via knowledge distillation (KD).In practical scenarios, competitors\ncan distill proprietary LLM capabilities by simply observing publicly\naccessible outputs, akin to reverse-engineering a complex performance by\nobservation alone. Existing protective methods like watermarking only identify\nimitation post-hoc, while other defenses assume the student model mimics the\nteacher's internal logits, rendering them ineffective against distillation\npurely from observed output text. This paper confronts the challenge of\nactively protecting LLMs within the realistic constraints of API-based access.\nWe introduce an effective and efficient Defensive Output Generation (DOGe)\nstrategy that subtly modifies the output behavior of an LLM. Its outputs remain\naccurate and useful for legitimate users, yet are designed to be misleading for\ndistillation, significantly undermining imitation attempts. We achieve this by\nfine-tuning only the final linear layer of the teacher LLM with an adversarial\nloss. This targeted training approach anticipates and disrupts distillation\nattempts during inference time. Our experiments show that, while preserving or\neven improving the original performance of the teacher model, student models\ndistilled from the defensively generated teacher outputs demonstrate\ncatastrophically reduced performance, demonstrating our method's effectiveness\nas a practical safeguard against KD-based model imitation."}
{"id": "2505.18871", "pdf": "https://arxiv.org/pdf/2505.18871", "abs": "https://arxiv.org/abs/2505.18871", "authors": ["Nicolas Nguyen", "Solenne Gaucher", "Claire Vernade"], "title": "Non-Stationary Lipschitz Bandits", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We study the problem of non-stationary Lipschitz bandits, where the number of\nactions is infinite and the reward function, satisfying a Lipschitz assumption,\ncan change arbitrarily over time. We design an algorithm that adaptively tracks\nthe recently introduced notion of significant shifts, defined by large\ndeviations of the cumulative reward function. To detect such reward changes,\nour algorithm leverages a hierarchical discretization of the action space.\nWithout requiring any prior knowledge of the non-stationarity, our algorithm\nachieves a minimax-optimal dynamic regret bound of\n$\\mathcal{\\widetilde{O}}(\\tilde{L}^{1/3}T^{2/3})$, where $\\tilde{L}$ is the\nnumber of significant shifts and $T$ the horizon. This result provides the\nfirst optimal guarantee in this setting."}
{"id": "2505.19505", "pdf": "https://arxiv.org/pdf/2505.19505", "abs": "https://arxiv.org/abs/2505.19505", "authors": ["Yu Xia", "Rui Zhong", "Hao Gu", "Wei Yang", "Chi Lu", "Peng Jiang", "Kun Gai"], "title": "Hierarchical Tree Search-based User Lifelong Behavior Modeling on Large Language Model", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have garnered significant attention in\nRecommendation Systems (RS) due to their extensive world knowledge and robust\nreasoning capabilities. However, a critical challenge lies in enabling LLMs to\neffectively comprehend and extract insights from massive user behaviors.\nCurrent approaches that directly leverage LLMs for user interest learning face\nlimitations in handling long sequential behaviors, effectively extracting\ninterest, and applying interest in practical scenarios. To address these\nissues, we propose a Hierarchical Tree Search-based User Lifelong Behavior\nModeling framework (HiT-LBM). HiT-LBM integrates Chunked User Behavior\nExtraction (CUBE) and Hierarchical Tree Search for Interest (HTS) to capture\ndiverse interests and interest evolution of user. CUBE divides user lifelong\nbehaviors into multiple chunks and learns the interest and interest evolution\nwithin each chunk in a cascading manner. HTS generates candidate interests\nthrough hierarchical expansion and searches for the optimal interest with\nprocess rating model to ensure information gain for each behavior chunk.\nAdditionally, we design Temporal-Ware Interest Fusion (TIF) to integrate\ninterests from multiple behavior chunks, constructing a comprehensive\nrepresentation of user lifelong interests. The representation can be embedded\ninto any recommendation model to enhance performance. Extensive experiments\ndemonstrate the effectiveness of our approach, showing that it surpasses\nstate-of-the-art methods."}
{"id": "2505.18895", "pdf": "https://arxiv.org/pdf/2505.18895", "abs": "https://arxiv.org/abs/2505.18895", "authors": ["Fei Huang", "Silvana M. Pesenti"], "title": "Marginal Fairness: Fair Decision-Making under Risk Measures", "categories": ["stat.ML", "cs.CC", "cs.CY", "cs.LG", "q-fin.RM"], "comment": null, "summary": "This paper introduces marginal fairness, a new individual fairness notion for\nequitable decision-making in the presence of protected attributes such as\ngender, race, and religion. This criterion ensures that decisions based on\ngeneralized distortion risk measures are insensitive to distributional\nperturbations in protected attributes, regardless of whether these attributes\nare continuous, discrete, categorical, univariate, or multivariate. To\noperationalize this notion and reflect real-world regulatory environments (such\nas the EU gender-neutral pricing regulation), we model business decision-making\nin highly regulated industries (such as insurance and finance) as a two-step\nprocess: (i) a predictive modeling stage, in which a prediction function for\nthe target variable (e.g., insurance losses) is estimated based on both\nprotected and non-protected covariates; and (ii) a decision-making stage, in\nwhich a generalized distortion risk measure is applied to the target variable,\nconditional only on non-protected covariates, to determine the decision. In\nthis second step, we modify the risk measure such that the decision becomes\ninsensitive to the protected attribute, thus enforcing fairness to ensure\nequitable outcomes under risk-sensitive, regulatory constraints. Furthermore,\nby utilizing the concept of cascade sensitivity, we extend the marginal\nfairness framework to capture how dependencies between covariates propagate the\ninfluence of protected attributes through the modeling pipeline. A numerical\nstudy and an empirical implementation using an auto insurance dataset\ndemonstrate how the framework can be applied in practice."}
{"id": "2505.19509", "pdf": "https://arxiv.org/pdf/2505.19509", "abs": "https://arxiv.org/abs/2505.19509", "authors": ["Yifan Jia", "Kailin Jiang", "Yuyang Liang", "Qihan Ren", "Yi Xin", "Rui Yang", "Fenze Feng", "Mingcai Chen", "Hengyang Lu", "Haozhe Wang", "Xiaoye Qu", "Dongrui Liu", "Lizhen Cui", "Yuntao Du"], "title": "Benchmarking Multimodal Knowledge Conflict for Large Multimodal Models", "categories": ["cs.LG", "cs.AI"], "comment": "The source code is available at https://github.com/MLLMKCBENCH/MLLMKC", "summary": "Large Multimodal Models(LMMs) face notable challenges when encountering\nmultimodal knowledge conflicts, particularly under retrieval-augmented\ngeneration(RAG) frameworks where the contextual information from external\nsources may contradict the model's internal parametric knowledge, leading to\nunreliable outputs. However, existing benchmarks fail to reflect such realistic\nconflict scenarios. Most focus solely on intra-memory conflicts, while\ncontext-memory and inter-context conflicts remain largely investigated.\nFurthermore, commonly used factual knowledge-based evaluations are often\noverlooked, and existing datasets lack a thorough investigation into conflict\ndetection capabilities. To bridge this gap, we propose MMKC-Bench, a benchmark\ndesigned to evaluate factual knowledge conflicts in both context-memory and\ninter-context scenarios. MMKC-Bench encompasses three types of multimodal\nknowledge conflicts and includes 1,573 knowledge instances and 3,381 images\nacross 23 broad types, collected through automated pipelines with human\nverification. We evaluate three representative series of LMMs on both model\nbehavior analysis and conflict detection tasks. Our findings show that while\ncurrent LMMs are capable of recognizing knowledge conflicts, they tend to favor\ninternal parametric knowledge over external evidence. We hope MMKC-Bench will\nfoster further research in multimodal knowledge conflict and enhance the\ndevelopment of multimodal RAG systems. The source code is available at\nhttps://github.com/MLLMKCBENCH/MLLMKC."}
{"id": "2505.18899", "pdf": "https://arxiv.org/pdf/2505.18899", "abs": "https://arxiv.org/abs/2505.18899", "authors": ["Andrea Ramazzina", "Vittorio Giammarino", "Matteo El-Hariry", "Mario Bijelic"], "title": "Beyond Domain Randomization: Event-Inspired Perception for Visually Robust Adversarial Imitation from Videos", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Imitation from videos often fails when expert demonstrations and learner\nenvironments exhibit domain shifts, such as discrepancies in lighting, color,\nor texture. While visual randomization partially addresses this problem by\naugmenting training data, it remains computationally intensive and inherently\nreactive, struggling with unseen scenarios. We propose a different approach:\ninstead of randomizing appearances, we eliminate their influence entirely by\nrethinking the sensory representation itself. Inspired by biological vision\nsystems that prioritize temporal transients (e.g., retinal ganglion cells) and\nby recent sensor advancements, we introduce event-inspired perception for\nvisually robust imitation. Our method converts standard RGB videos into a\nsparse, event-based representation that encodes temporal intensity gradients,\ndiscarding static appearance features. This biologically grounded approach\ndisentangles motion dynamics from visual style, enabling robust visual\nimitation from observations even in the presence of visual mismatches between\nexpert and agent environments. By training policies on event streams, we\nachieve invariance to appearance-based distractors without requiring\ncomputationally expensive and environment-specific data augmentation\ntechniques. Experiments across the DeepMind Control Suite and the Adroit\nplatform for dynamic dexterous manipulation show the efficacy of our method.\nOur code is publicly available at Eb-LAIfO."}
{"id": "2505.19514", "pdf": "https://arxiv.org/pdf/2505.19514", "abs": "https://arxiv.org/abs/2505.19514", "authors": ["Yaoning Yu", "Ye Yu", "Kai Wei", "Haojing Luo", "Haohan Wang"], "title": "SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Prompt quality plays a critical role in the performance of large language\nmodels (LLMs), motivating a growing body of work on prompt optimization. Most\nexisting methods optimize prompts over a fixed dataset, assuming static input\ndistributions and offering limited support for iterative improvement. We\nintroduce SIPDO (Self-Improving Prompts through Data-Augmented Optimization), a\nclosed-loop framework for prompt learning that integrates synthetic data\ngeneration into the optimization process. SIPDO couples a synthetic data\ngenerator with a prompt optimizer, where the generator produces new examples\nthat reveal current prompt weaknesses and the optimizer incrementally refines\nthe prompt in response. This feedback-driven loop enables systematic\nimprovement of prompt performance without assuming access to external\nsupervision or new tasks. Experiments across question answering and reasoning\nbenchmarks show that SIPDO outperforms standard prompt tuning methods,\nhighlighting the value of integrating data synthesis into prompt learning\nworkflows."}
{"id": "2505.18907", "pdf": "https://arxiv.org/pdf/2505.18907", "abs": "https://arxiv.org/abs/2505.18907", "authors": ["Sanjay Kariyappa", "G. Edward Suh"], "title": "Stronger Enforcement of Instruction Hierarchy via Augmented Intermediate Representations", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Prompt injection attacks are a critical security vulnerability in large\nlanguage models (LLMs), allowing attackers to hijack model behavior by\ninjecting malicious instructions within the input context. Recent defense\nmechanisms have leveraged an Instruction Hierarchy (IH) Signal, often\nimplemented through special delimiter tokens or additive embeddings to denote\nthe privilege level of input tokens. However, these prior works typically\ninject the IH signal exclusively at the initial input layer, which we\nhypothesize limits its ability to effectively distinguish the privilege levels\nof tokens as it propagates through the different layers of the model. To\novercome this limitation, we introduce a novel approach that injects the IH\nsignal into the intermediate token representations within the network. Our\nmethod augments these representations with layer-specific trainable embeddings\nthat encode the privilege information. Our evaluations across multiple models\nand training methods reveal that our proposal yields between $1.6\\times$ and\n$9.2\\times$ reduction in attack success rate on gradient-based prompt injection\nattacks compared to state-of-the-art methods, without significantly degrading\nthe model's utility."}
{"id": "2505.19525", "pdf": "https://arxiv.org/pdf/2505.19525", "abs": "https://arxiv.org/abs/2505.19525", "authors": ["Liangwei Nathan Zheng", "Wei Emma Zhang", "Mingyu Guo", "Miao Xu", "Olaf Maennel", "Weitong Chen"], "title": "Rethinking Gating Mechanism in Sparse MoE: Handling Arbitrary Modality Inputs with Confidence-Guided Gate", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Effectively managing missing modalities is a fundamental challenge in\nreal-world multimodal learning scenarios, where data incompleteness often\nresults from systematic collection errors or sensor failures. Sparse\nMixture-of-Experts (SMoE) architectures have the potential to naturally handle\nmultimodal data, with individual experts specializing in different modalities.\nHowever, existing SMoE approach often lacks proper ability to handle missing\nmodality, leading to performance degradation and poor generalization in\nreal-world applications. We propose Conf-SMoE to introduce a two-stage\nimputation module to handle the missing modality problem for the SMoE\narchitecture and reveal the insight of expert collapse from theoretical\nanalysis with strong empirical evidence. Inspired by our theoretical analysis,\nConf-SMoE propose a novel expert gating mechanism by detaching the softmax\nrouting score to task confidence score w.r.t ground truth. This naturally\nrelieves expert collapse without introducing additional load balance loss\nfunction. We show that the insights of expert collapse aligns with other gating\nmechanism such as Gaussian and Laplacian gate. We also evaluate the proposed\nmethod on four different real world dataset with three different experiment\nsettings to conduct comprehensive the analysis of Conf-SMoE on modality fusion\nand resistance to missing modality."}
{"id": "2505.18909", "pdf": "https://arxiv.org/pdf/2505.18909", "abs": "https://arxiv.org/abs/2505.18909", "authors": ["Andi Han", "Wei Huang", "Zhanpeng Zhou", "Gang Niu", "Wuyang Chen", "Junchi Yan", "Akiko Takeda", "Taiji Suzuki"], "title": "On the Role of Label Noise in the Feature Learning Process", "categories": ["stat.ML", "cs.LG"], "comment": "Accepted to ICML 2025", "summary": "Deep learning with noisy labels presents significant challenges. In this\nwork, we theoretically characterize the role of label noise from a feature\nlearning perspective. Specifically, we consider a signal-noise data\ndistribution, where each sample comprises a label-dependent signal and\nlabel-independent noise, and rigorously analyze the training dynamics of a\ntwo-layer convolutional neural network under this data setup, along with the\npresence of label noise. Our analysis identifies two key stages. In Stage I,\nthe model perfectly fits all the clean samples (i.e., samples without label\nnoise) while ignoring the noisy ones (i.e., samples with noisy labels). During\nthis stage, the model learns the signal from the clean samples, which\ngeneralizes well on unseen data. In Stage II, as the training loss converges,\nthe gradient in the direction of noise surpasses that of the signal, leading to\noverfitting on noisy samples. Eventually, the model memorizes the noise present\nin the noisy samples and degrades its generalization ability. Furthermore, our\nanalysis provides a theoretical basis for two widely used techniques for\ntackling label noise: early stopping and sample selection. Experiments on both\nsynthetic and real-world setups validate our theory."}
{"id": "2505.19527", "pdf": "https://arxiv.org/pdf/2505.19527", "abs": "https://arxiv.org/abs/2505.19527", "authors": ["Mohammed D. Belgoumri", "Mohamed Reda Bouadjenek", "Hakim Hacid", "Imran Razzak", "Sunil Aryal"], "title": "Navigating loss manifolds via rigid body dynamics: A promising avenue for robustness and generalisation", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": null, "summary": "Training large neural networks through gradient-based optimization requires\nnavigating high-dimensional loss landscapes, which often exhibit pathological\ngeometry, leading to undesirable training dynamics. In particular, poor\ngeneralization frequently results from convergence to sharp minima that are\nhighly sensitive to input perturbations, causing the model to overfit the\ntraining data while failing to generalize to unseen examples. Furthermore,\nthese optimization procedures typically display strong dependence on the fine\nstructure of the loss landscape, leading to unstable training dynamics, due to\nthe fractal-like nature of the loss surface. In this work, we propose an\nalternative optimizer that simultaneously reduces this dependence, and avoids\nsharp minima, thereby improving generalization. This is achieved by simulating\nthe motion of the center of a ball rolling on the loss landscape. The degree to\nwhich our optimizer departs from the standard gradient descent is controlled by\na hyperparameter, representing the radius of the ball. Changing this\nhyperparameter allows for probing the loss landscape at different scales,\nmaking it a valuable tool for understanding its geometry."}
{"id": "2505.18918", "pdf": "https://arxiv.org/pdf/2505.18918", "abs": "https://arxiv.org/abs/2505.18918", "authors": ["Javier Salazar Cavazos", "Jeffrey A Fessler", "Laura Balzano"], "title": "ALPCAHUS: Subspace Clustering for Heteroscedastic Data", "categories": ["stat.ML", "cs.LG", "eess.SP"], "comment": "Manuscript submitted to IEEE Transactions on Signal Processing (TSP)\n  pending review", "summary": "Principal component analysis (PCA) is a key tool in the field of data\ndimensionality reduction. Various methods have been proposed to extend PCA to\nthe union of subspace (UoS) setting for clustering data that come from multiple\nsubspaces like K-Subspaces (KSS). However, some applications involve\nheterogeneous data that vary in quality due to noise characteristics associated\nwith each data sample. Heteroscedastic methods aim to deal with such mixed data\nquality. This paper develops a heteroscedastic-focused subspace clustering\nmethod, named ALPCAHUS, that can estimate the sample-wise noise variances and\nuse this information to improve the estimate of the subspace bases associated\nwith the low-rank structure of the data. This clustering algorithm builds on\nK-Subspaces (KSS) principles by extending the recently proposed heteroscedastic\nPCA method, named LR-ALPCAH, for clusters with heteroscedastic noise in the UoS\nsetting. Simulations and real-data experiments show the effectiveness of\naccounting for data heteroscedasticity compared to existing clustering\nalgorithms. Code available at https://github.com/javiersc1/ALPCAHUS."}
{"id": "2505.19528", "pdf": "https://arxiv.org/pdf/2505.19528", "abs": "https://arxiv.org/abs/2505.19528", "authors": ["Yejin Lee", "Joonghyuk Hahn", "Hyeseon Ahn", "Yo-Sub Han"], "title": "AmpleHate: Amplifying the Attention for Versatile Implicit Hate Detection", "categories": ["cs.CL", "cs.AI", "cs.CY", "68T50", "I.2.7"], "comment": "13 pages, 4 figures, Under Review", "summary": "Implicit hate speech detection is challenging due to its subtlety and\nreliance on contextual interpretation rather than explicit offensive words.\nCurrent approaches rely on contrastive learning, which are shown to be\neffective on distinguishing hate and non-hate sentences. Humans, however,\ndetect implicit hate speech by first identifying specific targets within the\ntext and subsequently interpreting how these target relate to their surrounding\ncontext. Motivated by this reasoning process, we propose AmpleHate, a novel\napproach designed to mirror human inference for implicit hate detection.\nAmpleHate identifies explicit target using a pretrained Named Entity\nRecognition model and capture implicit target information via [CLS] tokens. It\ncomputes attention-based relationships between explicit, implicit targets and\nsentence context and then, directly injects these relational vectors into the\nfinal sentence representation. This amplifies the critical signals of\ntarget-context relations for determining implicit hate. Experiments demonstrate\nthat AmpleHate achieves state-of-the-art performance, outperforming contrastive\nlearning baselines by an average of 82.14% and achieve faster convergence.\nQualitative analyses further reveal that attention patterns produced by\nAmpleHate closely align with human judgement, underscoring its interpretability\nand robustness."}
{"id": "2505.18931", "pdf": "https://arxiv.org/pdf/2505.18931", "abs": "https://arxiv.org/abs/2505.18931", "authors": ["Ryan Saklad", "Aman Chadha", "Oleg Pavlov", "Raha Moraffah"], "title": "Can Large Language Models Infer Causal Relationships from Real-World Text?", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Understanding and inferring causal relationships from texts is a core aspect\nof human cognition and is essential for advancing large language models (LLMs)\ntowards artificial general intelligence. Existing work primarily focuses on\nsynthetically generated texts which involve simple causal relationships\nexplicitly mentioned in the text. This fails to reflect the complexities of\nreal-world tasks. In this paper, we investigate whether LLMs are capable of\ninferring causal relationships from real-world texts. We develop a benchmark\ndrawn from real-world academic literature which includes diverse texts with\nrespect to length, complexity of relationships (different levels of\nexplicitness, number of events, and causal relationships), and domains and\nsub-domains. To the best of our knowledge, our benchmark is the first-ever\nreal-world dataset for this task. Our experiments on state-of-the-art LLMs\nevaluated on our proposed benchmark demonstrate significant challenges, with\nthe best-performing model achieving an average F1 score of only 0.477. Analysis\nreveals common pitfalls: difficulty with implicitly stated information, in\ndistinguishing relevant causal factors from surrounding contextual details, and\nwith connecting causally relevant information spread across lengthy textual\npassages. By systematically characterizing these deficiencies, our benchmark\noffers targeted insights for further research into advancing LLM causal\nreasoning."}
{"id": "2505.19531", "pdf": "https://arxiv.org/pdf/2505.19531", "abs": "https://arxiv.org/abs/2505.19531", "authors": ["Jerry Yao-Chieh Hu", "Xiwen Zhang", "Maojiang Su", "Zhao Song", "Han Liu"], "title": "Minimalist Softmax Attention Provably Learns Constrained Boolean Functions", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "We study the computational limits of learning $k$-bit Boolean functions\n(specifically, $\\mathrm{AND}$, $\\mathrm{OR}$, and their noisy variants), using\na minimalist single-head softmax-attention mechanism, where $k=\\Theta(d)$\nrelevant bits are selected from $d$ inputs. We show that these simple\n$\\mathrm{AND}$ and $\\mathrm{OR}$ functions are unsolvable with a single-head\nsoftmax-attention mechanism alone. However, with teacher forcing, the same\nminimalist attention is capable of solving them. These findings offer two key\ninsights: Architecturally, solving these Boolean tasks requires only minimalist\nattention, without deep Transformer blocks or FFNs. Methodologically, one\ngradient descent update with supervision suffices and replaces the multi-step\nChain-of-Thought (CoT) reasoning scheme of [Kim and Suzuki, ICLR 2025] for\nsolving Boolean problems. Together, the bounds expose a fundamental gap between\nwhat this minimal architecture achieves under ideal supervision and what is\nprovably impossible under standard training."}
{"id": "2505.18949", "pdf": "https://arxiv.org/pdf/2505.18949", "abs": "https://arxiv.org/abs/2505.18949", "authors": ["Longfei Yun", "Chenyang An", "Zilong Wang", "Letian Peng", "Jingbo Shang"], "title": "The Price of Format: Diversity Collapse in LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "14 pages, 7 figures", "summary": "Instruction-tuned large language models (LLMs) employ structured templates,\nsuch as role markers and special tokens, to enforce format consistency during\ninference. However, we identify a critical limitation of such formatting: it\ninduces a phenomenon we term diversity collapse, where the model generates\nsemantically similar outputs for open-ended inputs, undermining creativity and\nvariability. We systematically evaluate this effect across tasks like story\ncompletion and free-form generation, finding that (1) diversity collapse\npersists even under high-temperature sampling, and (2) structural tokens in\ntemplates significantly constrain the model's output space. To contextualize\nthese findings, we fine-tune the same model using a range of structured prompts\nand then evaluate them across three axes: downstream task performance,\nalignment behavior, and output diversity. Our analysis shows that format\nconsistency between fine-tuning and inference is crucial for\nstructure-sensitive tasks (e.g., GSM8K, IFEval), but has marginal influence on\nknowledge-heavy tasks (e.g., MMLU, WebQuestions). In contrast, output diversity\nis primarily governed by the presence or absence of structural tokens, with\nminimal formatting yielding the most diverse outputs. These findings reveal\nthat current prompting conventions, while beneficial for alignment, may\ninadvertently suppress output diversity, underscoring the need for\ndiversity-aware prompt design and instruction tuning."}
{"id": "2505.19534", "pdf": "https://arxiv.org/pdf/2505.19534", "abs": "https://arxiv.org/abs/2505.19534", "authors": ["Yongyi Zang", "Jingyi Li", "Qiuqiang Kong"], "title": "Training-Free Multi-Step Audio Source Separation", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "comment": null, "summary": "Audio source separation aims to separate a mixture into target sources.\nPrevious audio source separation systems usually conduct one-step inference,\nwhich does not fully explore the separation ability of models. In this work, we\nreveal that pretrained one-step audio source separation models can be leveraged\nfor multi-step separation without additional training. We propose a simple yet\neffective inference method that iteratively applies separation by optimally\nblending the input mixture with the previous step's separation result. At each\nstep, we determine the optimal blending ratio by maximizing a metric. We prove\nthat our method always yield improvement over one-step inference, provide error\nbounds based on model smoothness and metric robustness, and provide theoretical\nanalysis connecting our method to denoising along linear interpolation paths\nbetween noise and clean distributions, a property we link to denoising\ndiffusion bridge models. Our approach effectively delivers improved separation\nperformance as a \"free lunch\" from existing models. Our empirical results\ndemonstrate that our multi-step separation approach consistently outperforms\none-step inference across both speech enhancement and music source separation\ntasks, and can achieve scaling performance similar to training a larger model,\nusing more data, or in some cases employing a multi-step training objective.\nThese improvements appear not only on the optimization metric during multi-step\ninference, but also extend to nearly all non-optimized metrics (with one\nexception). We also discuss limitations of our approach and directions for\nfuture research."}
{"id": "2505.18956", "pdf": "https://arxiv.org/pdf/2505.18956", "abs": "https://arxiv.org/abs/2505.18956", "authors": ["Yining Pan", "Qiongjie Cui", "Xulei Yang", "Na Zhao"], "title": "How Do Images Align and Complement LiDAR? Towards a Harmonized Multi-modal 3D Panoptic Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": "Accepted at the 2025 International Conference on Machine Learning\n  (ICML)", "summary": "LiDAR-based 3D panoptic segmentation often struggles with the inherent\nsparsity of data from LiDAR sensors, which makes it challenging to accurately\nrecognize distant or small objects. Recently, a few studies have sought to\novercome this challenge by integrating LiDAR inputs with camera images,\nleveraging the rich and dense texture information provided by the latter. While\nthese approaches have shown promising results, they still face challenges, such\nas misalignment during data augmentation and the reliance on post-processing\nsteps. To address these issues, we propose Image-Assists-LiDAR (IAL), a novel\nmulti-modal 3D panoptic segmentation framework. In IAL, we first introduce a\nmodality-synchronized data augmentation strategy, PieAug, to ensure alignment\nbetween LiDAR and image inputs from the start. Next, we adopt a transformer\ndecoder to directly predict panoptic segmentation results. To effectively fuse\nLiDAR and image features into tokens for the decoder, we design a\nGeometric-guided Token Fusion (GTF) module. Additionally, we leverage the\ncomplementary strengths of each modality as priors for query initialization\nthrough a Prior-based Query Generation (PQG) module, enhancing the decoder's\nability to generate accurate instance masks. Our IAL framework achieves\nstate-of-the-art performance compared to previous multi-modal 3D panoptic\nsegmentation methods on two widely used benchmarks. Code and models are\npublicly available at <https://github.com/IMPL-Lab/IAL.git>."}
{"id": "2505.19536", "pdf": "https://arxiv.org/pdf/2505.19536", "abs": "https://arxiv.org/abs/2505.19536", "authors": ["Jintao Tong", "Wenwei Jin", "Pengda Qin", "Anqi Li", "Yixiong Zou", "Yuhong Li", "Yuhua Li", "Ruixuan Li"], "title": "FlowCut: Rethinking Redundancy via Information Flow for Efficient Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "19 pages, 11 figures", "summary": "Large vision-language models (LVLMs) excel at multimodal understanding but\nsuffer from high computational costs due to redundant vision tokens. Existing\npruning methods typically rely on single-layer attention scores to rank and\nprune redundant visual tokens to solve this inefficiency. However, as the\ninteraction between tokens and layers is complicated, this raises a basic\nquestion: Is such a simple single-layer criterion sufficient to identify\nredundancy? To answer this question, we rethink the emergence of redundant\nvisual tokens from a fundamental perspective: information flow, which models\nthe interaction between tokens and layers by capturing how information moves\nbetween tokens across layers. We find (1) the CLS token acts as an information\nrelay, which can simplify the complicated flow analysis; (2) the redundancy\nemerges progressively and dynamically via layer-wise attention concentration;\nand (3) relying solely on attention scores from single layers can lead to\ncontradictory redundancy identification. Based on this, we propose FlowCut, an\ninformation-flow-aware pruning framework, mitigating the insufficiency of the\ncurrent criterion for identifying redundant tokens and better aligning with the\nmodel's inherent behaviors. Extensive experiments show that FlowCut achieves\nsuperior results, outperforming SoTA by 1.6% on LLaVA-1.5-7B with 88.9% token\nreduction, and by 4.3% on LLaVA-NeXT-7B with 94.4% reduction, delivering 3.2x\nspeed-up in the prefilling stage. Our code is available at\nhttps://github.com/TungChintao/FlowCut"}
{"id": "2505.18973", "pdf": "https://arxiv.org/pdf/2505.18973", "abs": "https://arxiv.org/abs/2505.18973", "authors": ["Sarang Patil", "Ashish Parmanand Pandey", "Ioannis Koutis", "Mengjia Xu"], "title": "Hierarchical Mamba Meets Hyperbolic Geometry: A New Paradigm for Structured Language Embeddings", "categories": ["cs.CL", "cs.LG"], "comment": "10 pages, 3 figures", "summary": "Selective state-space models have achieved great success in long-sequence\nmodeling. However, their capacity for language representation, especially in\ncomplex hierarchical reasoning tasks, remains underexplored. Most large\nlanguage models rely on flat Euclidean embeddings, limiting their ability to\ncapture latent hierarchies. To address this limitation, we propose Hierarchical\nMamba (HiM), integrating efficient Mamba2 with exponential growth and curved\nnature of hyperbolic geometry to learn hierarchy-aware language embeddings for\ndeeper linguistic understanding. Mamba2-processed sequences are projected to\nthe Poincare ball (via tangent-based mapping) or Lorentzian manifold (via\ncosine and sine-based mapping) with \"learnable\" curvature, optimized with a\ncombined hyperbolic loss. Our HiM model facilitates the capture of relational\ndistances across varying hierarchical levels, enabling effective long-range\nreasoning. This makes it well-suited for tasks like mixed-hop prediction and\nmulti-hop inference in hierarchical classification. We evaluated our HiM with\nfour linguistic and medical datasets for mixed-hop prediction and multi-hop\ninference tasks. Experimental results demonstrated that: 1) Both HiM models\neffectively capture hierarchical relationships for four ontological datasets,\nsurpassing Euclidean baselines. 2) HiM-Poincare captures fine-grained semantic\ndistinctions with higher h-norms, while HiM-Lorentz provides more stable,\ncompact, and hierarchy-preserving embeddings favoring robustness over detail."}
{"id": "2505.19538", "pdf": "https://arxiv.org/pdf/2505.19538", "abs": "https://arxiv.org/abs/2505.19538", "authors": ["Yuxing Lu", "Gecheng Fu", "Wei Wu", "Xukai Zhao", "Sin Yee Goi", "Jinzhuo Wang"], "title": "DoctorRAG: Medical RAG Fusing Knowledge with Patient Analogy through Textual Gradients", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.IR", "cs.MA"], "comment": "32 pages, 5 figures, 5 tables", "summary": "Existing medical RAG systems mainly leverage knowledge from medical knowledge\nbases, neglecting the crucial role of experiential knowledge derived from\nsimilar patient cases -- a key component of human clinical reasoning. To bridge\nthis gap, we propose DoctorRAG, a RAG framework that emulates doctor-like\nreasoning by integrating both explicit clinical knowledge and implicit\ncase-based experience. DoctorRAG enhances retrieval precision by first\nallocating conceptual tags for queries and knowledge sources, together with a\nhybrid retrieval mechanism from both relevant knowledge and patient. In\naddition, a Med-TextGrad module using multi-agent textual gradients is\nintegrated to ensure that the final output adheres to the retrieved knowledge\nand patient query. Comprehensive experiments on multilingual, multitask\ndatasets demonstrate that DoctorRAG significantly outperforms strong baseline\nRAG models and gains improvements from iterative refinements. Our approach\ngenerates more accurate, relevant, and comprehensive responses, taking a step\ntowards more doctor-like medical reasoning systems."}
{"id": "2505.19017", "pdf": "https://arxiv.org/pdf/2505.19017", "abs": "https://arxiv.org/abs/2505.19017", "authors": ["Yaxuan Li", "Yichen Zhu", "Junjie Wen", "Chaomin Shen", "Yi Xu"], "title": "WorldEval: World Model as Real-World Robot Policies Evaluator", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "The project page is available at https://worldeval.github.io", "summary": "The field of robotics has made significant strides toward developing\ngeneralist robot manipulation policies. However, evaluating these policies in\nreal-world scenarios remains time-consuming and challenging, particularly as\nthe number of tasks scales and environmental conditions change. In this work,\nwe demonstrate that world models can serve as a scalable, reproducible, and\nreliable proxy for real-world robot policy evaluation. A key challenge is\ngenerating accurate policy videos from world models that faithfully reflect the\nrobot actions. We observe that directly inputting robot actions or using\nhigh-dimensional encoding methods often fails to generate action-following\nvideos. To address this, we propose Policy2Vec, a simple yet effective approach\nto turn a video generation model into a world simulator that follows latent\naction to generate the robot video. We then introduce WorldEval, an automated\npipeline designed to evaluate real-world robot policies entirely online.\nWorldEval effectively ranks various robot policies and individual checkpoints\nwithin a single policy, and functions as a safety detector to prevent dangerous\nactions by newly developed robot models. Through comprehensive paired\nevaluations of manipulation policies in real-world environments, we demonstrate\na strong correlation between policy performance in WorldEval and real-world\nscenarios. Furthermore, our method significantly outperforms popular methods\nsuch as real-to-sim approach."}
{"id": "2505.19547", "pdf": "https://arxiv.org/pdf/2505.19547", "abs": "https://arxiv.org/abs/2505.19547", "authors": ["Haoyu Zhang", "Wentao Zhang", "Hao Miao", "Xinke Jiang", "Yuchen Fang", "Yifan Zhang"], "title": "STRAP: Spatio-Temporal Pattern Retrieval for Out-of-Distribution Generalization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Spatio-Temporal Graph Neural Networks (STGNNs) have emerged as a powerful\ntool for modeling dynamic graph-structured data across diverse domains.\nHowever, they often fail to generalize in Spatio-Temporal Out-of-Distribution\n(STOOD) scenarios, where both temporal dynamics and spatial structures evolve\nbeyond the training distribution. To address this problem, we propose an\ninnovative Spatio-Temporal Retrieval-Augmented Pattern Learning\nframework,STRAP, which enhances model generalization by integrating\nretrieval-augmented learning into the STGNN continue learning pipeline. The\ncore of STRAP is a compact and expressive pattern library that stores\nrepresentative spatio-temporal patterns enriched with historical, structural,\nand semantic information, which is obtained and optimized during the training\nphase. During inference, STRAP retrieves relevant patterns from this library\nbased on similarity to the current input and injects them into the model via a\nplug-and-play prompting mechanism. This not only strengthens spatio-temporal\nrepresentations but also mitigates catastrophic forgetting. Moreover, STRAP\nintroduces a knowledge-balancing objective to harmonize new information with\nretrieved knowledge. Extensive experiments across multiple real-world streaming\ngraph datasets show that STRAP consistently outperforms state-of-the-art STGNN\nbaselines on STOOD tasks, demonstrating its robustness, adaptability, and\nstrong generalization capability without task-specific fine-tuning."}
{"id": "2505.19023", "pdf": "https://arxiv.org/pdf/2505.19023", "abs": "https://arxiv.org/abs/2505.19023", "authors": ["Huda Alghoraibi", "Nuha Alqurashi", "Sarah Alotaibi", "Renad Alkhudaydi", "Bdoor Aldajani", "Lubna Alqurashi", "Jood Batweel", "Maha A. Thafar"], "title": "A Smart Healthcare System for Monkeypox Skin Lesion Detection and Tracking", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.LG"], "comment": "23 pages, 5 figures", "summary": "Monkeypox is a viral disease characterized by distinctive skin lesions and\nhas been reported in many countries. The recent global outbreak has emphasized\nthe urgent need for scalable, accessible, and accurate diagnostic solutions to\nsupport public health responses.\n  In this study, we developed ITMAINN, an intelligent, AI-driven healthcare\nsystem specifically designed to detect Monkeypox from skin lesion images using\nadvanced deep learning techniques. Our system consists of three main\ncomponents. First, we trained and evaluated several pretrained models using\ntransfer learning on publicly available skin lesion datasets to identify the\nmost effective models. For binary classification (Monkeypox vs. non-Monkeypox),\nthe Vision Transformer, MobileViT, Transformer-in-Transformer, and VGG16\nachieved the highest performance, each with an accuracy and F1-score of 97.8%.\nFor multiclass classification, which contains images of patients with Monkeypox\nand five other classes (chickenpox, measles, hand-foot-mouth disease, cowpox,\nand healthy), ResNetViT and ViT Hybrid models achieved 92% accuracy, with F1\nscores of 92.24% and 92.19%, respectively. The best-performing and most\nlightweight model, MobileViT, was deployed within the mobile application. The\nsecond component is a cross-platform smartphone application that enables users\nto detect Monkeypox through image analysis, track symptoms, and receive\nrecommendations for nearby healthcare centers based on their location. The\nthird component is a real-time monitoring dashboard designed for health\nauthorities to support them in tracking cases, analyzing symptom trends,\nguiding public health interventions, and taking proactive measures.\n  This system is fundamental in developing responsive healthcare infrastructure\nwithin smart cities. Our solution, ITMAINN, is part of revolutionizing public\nhealth management."}
{"id": "2505.19548", "pdf": "https://arxiv.org/pdf/2505.19548", "abs": "https://arxiv.org/abs/2505.19548", "authors": ["Xufeng Duan", "Zhaoqian Yao", "Yunhao Zhang", "Shaonan Wang", "Zhenguang G. Cai"], "title": "How Syntax Specialization Emerges in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have been found to develop surprising internal\nspecializations: Individual neurons, attention heads, and circuits become\nselectively sensitive to syntactic structure, reflecting patterns observed in\nthe human brain. While this specialization is well-documented, how it emerges\nduring training and what influences its development remains largely unknown.\n  In this work, we tap into the black box of specialization by tracking its\nformation over time. By quantifying internal syntactic consistency across\nminimal pairs from various syntactic phenomena, we identify a clear\ndevelopmental trajectory: Syntactic sensitivity emerges gradually, concentrates\nin specific layers, and exhibits a 'critical period' of rapid internal\nspecialization. This process is consistent across architectures and\ninitialization parameters (e.g., random seeds), and is influenced by model\nscale and training data. We therefore reveal not only where syntax arises in\nLLMs but also how some models internalize it during training. To support future\nresearch, we will release the code, models, and training checkpoints upon\nacceptance."}
{"id": "2505.19033", "pdf": "https://arxiv.org/pdf/2505.19033", "abs": "https://arxiv.org/abs/2505.19033", "authors": ["Alireza Javanmardi", "Soroush H. Zargarbashi", "Santo M. A. R. Thies", "Willem Waegeman", "Aleksandar Bojchevski", "Eyke Hllermeier"], "title": "Optimal Conformal Prediction under Epistemic Uncertainty", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Conformal prediction (CP) is a popular frequentist framework for representing\nuncertainty by providing prediction sets that guarantee coverage of the true\nlabel with a user-adjustable probability. In most applications, CP operates on\nconfidence scores coming from a standard (first-order) probabilistic predictor\n(e.g., softmax outputs). Second-order predictors, such as credal set predictors\nor Bayesian models, are also widely used for uncertainty quantification and are\nknown for their ability to represent both aleatoric and epistemic uncertainty.\nDespite their popularity, there is still an open question on ``how they can be\nincorporated into CP''. In this paper, we discuss the desiderata for CP when\nvalid second-order predictions are available. We then introduce Bernoulli\nprediction sets (BPS), which produce the smallest prediction sets that ensure\nconditional coverage in this setting. When given first-order predictions, BPS\nreduces to the well-known adaptive prediction sets (APS). Furthermore, when the\nvalidity assumption on the second-order predictions is compromised, we apply\nconformal risk control to obtain a marginal coverage guarantee while still\naccounting for epistemic uncertainty."}
{"id": "2505.19572", "pdf": "https://arxiv.org/pdf/2505.19572", "abs": "https://arxiv.org/abs/2505.19572", "authors": ["Li Zeng", "Zeming Liu", "Chong Feng", "Heyan Huang", "Yuhang Guo"], "title": "DocMEdit: Towards Document-Level Model Editing", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 findings", "summary": "Model editing aims to correct errors and outdated knowledge in the Large\nlanguage models (LLMs) with minimal cost. Prior research has proposed a variety\nof datasets to assess the effectiveness of these model editing methods.\nHowever, most existing datasets only require models to output short phrases or\nsentences, overlooks the widespread existence of document-level tasks in the\nreal world, raising doubts about their practical usability. Aimed at addressing\nthis limitation and promoting the application of model editing in real-world\nscenarios, we propose the task of document-level model editing. To tackle such\nchallenges and enhance model capabilities in practical settings, we introduce\n\\benchmarkname, a dataset focused on document-level model editing,\ncharacterized by document-level inputs and outputs, extrapolative, and multiple\nfacts within a single edit. We propose a series of evaluation metrics and\nexperiments. The results show that the difficulties in document-level model\nediting pose challenges for existing model editing methods."}
{"id": "2505.19046", "pdf": "https://arxiv.org/pdf/2505.19046", "abs": "https://arxiv.org/abs/2505.19046", "authors": ["Daniel Barzilai", "Ohad Shamir"], "title": "When Models Don't Collapse: On the Consistency of Iterative MLE", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "The widespread use of generative models has created a feedback loop, in which\neach generation of models is trained on data partially produced by its\npredecessors. This process has raised concerns about \\emph{model collapse}: A\ncritical degradation in performance caused by repeated training on synthetic\ndata. However, different analyses in the literature have reached different\nconclusions as to the severity of model collapse. As such, it remains unclear\nhow concerning this phenomenon is, and under which assumptions it can be\navoided. To address this, we theoretically study model collapse for maximum\nlikelihood estimation (MLE), in a natural setting where synthetic data is\ngradually added to the original data set. Under standard assumptions (similar\nto those long used for proving asymptotic consistency and normality of MLE), we\nestablish non-asymptotic bounds showing that collapse can be avoided even as\nthe fraction of real data vanishes. On the other hand, we prove that some\nassumptions (beyond MLE consistency) are indeed necessary: Without them, model\ncollapse can occur arbitrarily quickly, even when the original data is still\npresent in the training set. To the best of our knowledge, these are the first\nrigorous examples of iterative generative modeling with accumulating data that\nrapidly leads to model collapse."}
{"id": "2505.19574", "pdf": "https://arxiv.org/pdf/2505.19574", "abs": "https://arxiv.org/abs/2505.19574", "authors": ["Alejandro Murillo-Gonzalez", "Lantao Liu"], "title": "Situationally-Aware Dynamics Learning", "categories": ["cs.RO", "cs.AI", "cs.LG", "math.OC"], "comment": null, "summary": "Autonomous robots operating in complex, unstructured environments face\nsignificant challenges due to latent, unobserved factors that obscure their\nunderstanding of both their internal state and the external world. Addressing\nthis challenge would enable robots to develop a more profound grasp of their\noperational context. To tackle this, we propose a novel framework for online\nlearning of hidden state representations, with which the robots can adapt in\nreal-time to uncertain and dynamic conditions that would otherwise be ambiguous\nand result in suboptimal or erroneous behaviors. Our approach is formalized as\na Generalized Hidden Parameter Markov Decision Process, which explicitly models\nthe influence of unobserved parameters on both transition dynamics and reward\nstructures. Our core innovation lies in learning online the joint distribution\nof state transitions, which serves as an expressive representation of latent\nego- and environmental-factors. This probabilistic approach supports the\nidentification and adaptation to different operational situations, improving\nrobustness and safety. Through a multivariate extension of Bayesian Online\nChangepoint Detection, our method segments changes in the underlying data\ngenerating process governing the robot's dynamics. The robot's transition model\nis then informed with a symbolic representation of the current situation\nderived from the joint distribution of latest state transitions, enabling\nadaptive and context-aware decision-making. To showcase the real-world\neffectiveness, we validate our approach in the challenging task of unstructured\nterrain navigation, where unmodeled and unmeasured terrain characteristics can\nsignificantly impact the robot's motion. Extensive experiments in both\nsimulation and real world reveal significant improvements in data efficiency,\npolicy performance, and the emergence of safer, adaptive navigation strategies."}
{"id": "2505.19051", "pdf": "https://arxiv.org/pdf/2505.19051", "abs": "https://arxiv.org/abs/2505.19051", "authors": ["Mahdi Nikdan", "Vincent Cohen-Addad", "Dan Alistarh", "Vahab Mirrokni"], "title": "Efficient Data Selection at Scale via Influence Distillation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Effective data selection is critical for efficient training of modern Large\nLanguage Models (LLMs). This paper introduces Influence Distillation, a novel,\nmathematically-justified framework for data selection that employs second-order\ninformation to optimally weight training samples. By distilling each sample's\ninfluence on a target distribution, our method assigns model-specific weights\nthat are used to select training data for LLM fine-tuning, guiding it toward\nstrong performance on the target domain. We derive these optimal weights for\nboth Gradient Descent and Adam optimizers. To ensure scalability and reduce\ncomputational cost, we propose a $\\textit{landmark-based approximation}$:\ninfluence is precisely computed for a small subset of \"landmark\" samples and\nthen efficiently propagated to all other samples to determine their weights. We\nvalidate Influence Distillation by applying it to instruction tuning on the\nTulu V2 dataset, targeting a range of tasks including GSM8k, SQuAD, and MMLU,\nacross several models from the Llama and Qwen families. Experiments show that\nInfluence Distillation matches or outperforms state-of-the-art performance\nwhile achieving up to $3.5\\times$ faster selection."}
{"id": "2505.19578", "pdf": "https://arxiv.org/pdf/2505.19578", "abs": "https://arxiv.org/abs/2505.19578", "authors": ["Dan Peng", "Zhihui Fu", "Zewen Ye", "Zhuoran Song", "Jun Wang"], "title": "Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Under review", "summary": "Sparse attention methods exploit the inherent sparsity in attention to speed\nup the prefilling phase of long-context inference, mitigating the quadratic\ncomplexity of full attention computation. While existing sparse attention\nmethods rely on predefined patterns or inaccurate estimations to approximate\nattention behavior, they often fail to fully capture the true dynamics of\nattention, resulting in reduced efficiency and compromised accuracy. Instead,\nwe propose a highly accurate sparse attention mechanism that shares similar yet\nprecise attention patterns across heads, enabling a more realistic capture of\nthe dynamic behavior of attention. Our approach is grounded in two key\nobservations: (1) attention patterns demonstrate strong inter-head similarity,\nand (2) this similarity remains remarkably consistent across diverse inputs. By\nstrategically sharing computed accurate patterns across attention heads, our\nmethod effectively captures actual patterns while requiring full attention\ncomputation for only a small subset of heads. Comprehensive evaluations\ndemonstrate that our approach achieves superior or comparable speedup relative\nto state-of-the-art methods while delivering the best overall accuracy."}
{"id": "2505.19056", "pdf": "https://arxiv.org/pdf/2505.19056", "abs": "https://arxiv.org/abs/2505.19056", "authors": ["Harethah Abu Shairah", "Hasan Abed Al Kader Hammoud", "Bernard Ghanem", "George Turkiyyah"], "title": "An Embarrassingly Simple Defense Against LLM Abliteration Attacks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "preprint", "summary": "Large language models (LLMs) are typically aligned to comply with safety\nguidelines by refusing harmful instructions. A recent attack, termed\nabliteration, isolates and suppresses the single latent direction most\nresponsible for refusal behavior, enabling the model to generate unethical\ncontent. We propose a defense that modifies how models generate refusals. We\nconstruct an extended-refusal dataset that contains harmful prompts with a full\nresponse that justifies the reason for refusal. We then fine-tune\nLlama-2-7B-Chat and Qwen2.5-Instruct (1.5B and 3B parameters) on our\nextended-refusal dataset, and evaluate the resulting systems on a set of\nharmful prompts. In our experiments, extended-refusal models maintain high\nrefusal rates, dropping at most by 10%, whereas baseline models' refusal rates\ndrop by 70-80% after abliteration. A broad evaluation of safety and utility\nshows that extended-refusal fine-tuning neutralizes the abliteration attack\nwhile preserving general performance."}
{"id": "2505.19588", "pdf": "https://arxiv.org/pdf/2505.19588", "abs": "https://arxiv.org/abs/2505.19588", "authors": ["Yanzhen Shen", "Sihao Chen", "Xueqiang Xu", "Yunyi Zhang", "Chaitanya Malaviya", "Dan Roth"], "title": "LogiCoL: Logically-Informed Contrastive Learning for Set-based Dense Retrieval", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "While significant progress has been made with dual- and bi-encoder dense\nretrievers, they often struggle on queries with logical connectives, a use case\nthat is often overlooked yet important in downstream applications. Current\ndense retrievers struggle with such queries, such that the retrieved results do\nnot respect the logical constraints implied in the queries. To address this\nchallenge, we introduce LogiCoL, a logically-informed contrastive learning\nobjective for dense retrievers. LogiCoL builds upon in-batch supervised\ncontrastive learning, and learns dense retrievers to respect the subset and\nmutually-exclusive set relation between query results via two sets of soft\nconstraints expressed via t-norm in the learning objective. We evaluate the\neffectiveness of LogiCoL on the task of entity retrieval, where the model is\nexpected to retrieve a set of entities in Wikipedia that satisfy the implicit\nlogical constraints in the query. We show that models trained with LogiCoL\nyield improvement both in terms of retrieval performance and logical\nconsistency in the results. We provide detailed analysis and insights to\nuncover why queries with logical connectives are challenging for dense\nretrievers and why LogiCoL is most effective."}
{"id": "2505.19059", "pdf": "https://arxiv.org/pdf/2505.19059", "abs": "https://arxiv.org/abs/2505.19059", "authors": ["Ignacio Mariano Andreozzi Pofcher", "Joshua Ellul"], "title": "An Initial Exploration of Fine-tuning Small Language Models for Smart Contract Reentrancy Vulnerability Detection", "categories": ["cs.SE", "cs.AI", "cs.ET", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are being used more and more for various coding\ntasks, including to help coders identify bugs and are a promising avenue to\nsupport coders in various tasks including vulnerability detection --\nparticularly given the flexibility of such generative AI models and tools. Yet\nfor many tasks it may not be suitable to use LLMs, for which it may be more\nsuitable to use smaller language models that can fit and easily execute and\ntrain on a developer's computer. In this paper we explore and evaluate whether\nsmaller language models can be fine-tuned to achieve reasonable results for a\nniche area: vulnerability detection -- specifically focusing on detecting the\nreentrancy bug in Solidity smart contracts."}
{"id": "2505.19591", "pdf": "https://arxiv.org/pdf/2505.19591", "abs": "https://arxiv.org/abs/2505.19591", "authors": ["Yufan Dang", "Chen Qian", "Xueheng Luo", "Jingru Fan", "Zihao Xie", "Ruijie Shi", "Weize Chen", "Cheng Yang", "Xiaoyin Che", "Ye Tian", "Xuantang Xiong", "Lei Han", "Zhiyuan Liu", "Maosong Sun"], "title": "Multi-Agent Collaboration via Evolving Orchestration", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "Work in Progress", "summary": "Large language models (LLMs) have achieved remarkable results across diverse\ndownstream tasks, but their monolithic nature restricts scalability and\nefficiency in complex problem-solving. While recent research explores\nmulti-agent collaboration among LLMs, most approaches rely on static\norganizational structures that struggle to adapt as task complexity and agent\nnumbers grow, resulting in coordination overhead and inefficiencies. To this\nend, we propose a puppeteer-style paradigm for LLM-based multi-agent\ncollaboration, where a centralized orchestrator (\"puppeteer\") dynamically\ndirects agents (\"puppets\") in response to evolving task states. This\norchestrator is trained via reinforcement learning to adaptively sequence and\nprioritize agents, enabling flexible and evolvable collective reasoning.\nExperiments on closed- and open-domain scenarios show that this method achieves\nsuperior performance with reduced computational costs. Analyses further reveal\nthat the key improvements consistently stem from the emergence of more compact,\ncyclic reasoning structures under the orchestrator's evolution."}
{"id": "2505.19075", "pdf": "https://arxiv.org/pdf/2505.19075", "abs": "https://arxiv.org/abs/2505.19075", "authors": ["Jaemin Kim", "Hangeol Chang", "Hyunmin Hwang", "Choonghan Kim", "Jong Chul Ye"], "title": "Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for Frozen LLMs", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "22 pages", "summary": "Large Language Models (LLMs) have demonstrated remarkable general\ncapabilities, but enhancing skills such as reasoning often demands substantial\ncomputational resources and may compromise their generalization. While\nParameter-Efficient Fine-Tuning (PEFT) methods offer a more resource-conscious\nalternative, they typically requires retraining for each LLM backbone due to\narchitectural dependencies. To address these challenges, here we propose\nUniversal Reasoner (UniR) - a single, lightweight, composable, and\nplug-and-play reasoning module that can be used with any frozen LLM to endow it\nwith specialized reasoning capabilities. Specifically, UniR decomposes the\nreward into a standalone reasoning module that is trained independently using\npredefined rewards, effectively translating trajectory-level signals into\ntoken-level guidance. Once trained, UniR can be combined with any frozen LLM at\ninference time by simply adding its output logits to those of the LLM backbone.\nThis additive structure naturally enables modular composition: multiple UniR\nmodules trained for different tasks can be jointly applied by summing their\nlogits, enabling complex reasoning via composition. Experimental results on\nmathematical reasoning and machine translation tasks show that UniR\nsignificantly outperforms \\add{existing baseline fine-tuning methods using the\nLlama3.2 model}. Furthermore, UniR demonstrates strong weak-to-strong\ngeneralization: reasoning modules trained on smaller models effectively guide\nmuch larger LLMs. This makes UniR a cost-efficient, adaptable, and robust\nsolution for enhancing reasoning in LLMs without compromising their core\ncapabilities. Code is open-sourced at https://github.com/hangeol/UniR"}
{"id": "2505.19599", "pdf": "https://arxiv.org/pdf/2505.19599", "abs": "https://arxiv.org/abs/2505.19599", "authors": ["Andrew Gambardella", "Takeshi Kojima", "Yusuke Iwasawa", "Yutaka Matsuo"], "title": "Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese Grammar", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "In Proceedings of the 63rd Annual Meeting of the Association for\n  Computational Linguistics, 2025", "summary": "Typical methods for evaluating the performance of language models evaluate\ntheir ability to answer questions accurately. These evaluation metrics are\nacceptable for determining the extent to which language models can understand\nand reason about text in a general sense, but fail to capture nuanced\ncapabilities, such as the ability of language models to recognize and obey rare\ngrammar points, particularly in languages other than English. We measure the\nperplexity of language models when confronted with the \"first person psych\npredicate restriction\" grammar point in Japanese. Weblab is the only tested\nopen source model in the 7-10B parameter range which consistently assigns\nhigher perplexity to ungrammatical psych predicate sentences than grammatical\nones. We give evidence that Weblab's uniformly bad tokenization is a possible\nroot cause for its good performance, and show that Llama 3's perplexity on\ngrammatical psych predicate sentences can be reduced by orders of magnitude\n(28x difference) by restricting test sentences to those with uniformly\nwell-behaved tokenizations. We show in further experiments on machine\ntranslation tasks that language models will use alternative grammar patterns in\norder to produce grammatical sentences when tokenization issues prevent the\nmost natural sentence from being output."}
{"id": "2505.19083", "pdf": "https://arxiv.org/pdf/2505.19083", "abs": "https://arxiv.org/abs/2505.19083", "authors": ["Purba Mukherjee", "Anjan A Sen"], "title": "Geometric Determinations Of Characteristic Redshifts From DESI-DR2 BAO and DES-SN5YR Observations: Hints For New Expansion Rate Anomalies", "categories": ["astro-ph.CO", "cs.LG", "gr-qc"], "comment": "21 pages, 11 figures, 5 tables. Comments are welcome", "summary": "In this work, we perform a model-agnostic reconstruction of the cosmic\nexpansion history by combining DESI-DR2 BAO and DES-SN5YR data, with a focus on\ngeometric determination of characteristic redshifts where notable tensions in\nthe expansion rate are found to emerge. Employing Gaussian process regression\nalongside knot-based spline techniques, we reconstruct cosmic distances and\ntheir derivatives to pinpoint these characteristic redshifts and infer $E(z)$.\nOur analysis reveals significant deviations of approximately 4 to 5$\\sigma$\nfrom the Planck 2018 $\\Lambda$CDM predictions, particularly pronounced in the\nredshift range $z \\sim 0.35-0.55$. These anomalies are consistently observed\nacross both reconstruction methods and combined datasets, indicating robust\nlate-time departures that could signal new physics beyond the standard\ncosmological framework. The joint use of BAO and SN probes enhances the\nprecision of our constraints, allowing us to isolate these deviations without\nreliance on specific cosmological assumptions. Our findings underscore the role\nof characteristic redshifts as sensitive indicators of expansion rate anomalies\nand motivate further scrutiny with forthcoming datasets from DESI-5YR BAO,\nEuclid, and LSST. These future surveys will tighten constraints and help\ndistinguish whether these late-time anomalies arise from new fundamental\nphysics or unresolved systematics in the data."}
{"id": "2505.19601", "pdf": "https://arxiv.org/pdf/2505.19601", "abs": "https://arxiv.org/abs/2505.19601", "authors": ["Yeongmin Kim", "Heesun Bae", "Byeonghu Na", "Il-Chul Moon"], "title": "Preference Optimization by Estimating the Ratio of the Data Distribution", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Direct preference optimization (DPO) is widely used as a simple and stable\nmethod for aligning large language models (LLMs) with human preferences. This\npaper investigates a generalized DPO loss that enables a policy model to match\nthe target policy from a likelihood ratio estimation perspective. The ratio of\nthe target policy provides a unique identification of the policy distribution\nwithout relying on reward models or partition functions. This allows the\ngeneralized loss to retain both simplicity and theoretical guarantees, which\nprior work such as $f$-PO fails to achieve simultaneously. We propose Bregman\npreference optimization (BPO), a generalized framework for ratio matching that\nprovides a family of objective functions achieving target policy optimality.\nBPO subsumes DPO as a special case and offers tractable forms for all\ninstances, allowing implementation with a few lines of code. We further develop\nscaled Basu's power divergence (SBA), a gradient scaling method that can be\nused for BPO instances. The BPO framework complements other DPO variants and is\napplicable to target policies defined by these variants. In experiments, unlike\nother probabilistic loss extensions such as $f$-DPO or $f$-PO, which exhibit a\ntrade-off between generation fidelity and diversity, instances of BPO improve\nboth win rate and entropy compared with DPO. When applied to\nLlama-3-Instruct-8B, BPO achieves state-of-the-art performance among Llama-3-8B\nbackbones, with a 55.9\\% length-controlled win rate on AlpacaEval2."}
{"id": "2505.19084", "pdf": "https://arxiv.org/pdf/2505.19084", "abs": "https://arxiv.org/abs/2505.19084", "authors": ["Yifeng Xu", "Zhenliang He", "Meina Kan", "Shiguang Shan", "Xilin Chen"], "title": "Jodi: Unification of Visual Generation and Understanding via Joint Modeling", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Code: https://github.com/VIPL-GENUN/Jodi", "summary": "Visual generation and understanding are two deeply interconnected aspects of\nhuman intelligence, yet they have been traditionally treated as separate tasks\nin machine learning. In this paper, we propose Jodi, a diffusion framework that\nunifies visual generation and understanding by jointly modeling the image\ndomain and multiple label domains. Specifically, Jodi is built upon a linear\ndiffusion transformer along with a role switch mechanism, which enables it to\nperform three particular types of tasks: (1) joint generation, where the model\nsimultaneously generates images and multiple labels; (2) controllable\ngeneration, where images are generated conditioned on any combination of\nlabels; and (3) image perception, where multiple labels can be predicted at\nonce from a given image. Furthermore, we present the Joint-1.6M dataset, which\ncontains 200,000 high-quality images collected from public sources, automatic\nlabels for 7 visual domains, and LLM-generated captions. Extensive experiments\ndemonstrate that Jodi excels in both generation and understanding tasks and\nexhibits strong extensibility to a wider range of visual domains. Code is\navailable at https://github.com/VIPL-GENUN/Jodi."}
{"id": "2505.19607", "pdf": "https://arxiv.org/pdf/2505.19607", "abs": "https://arxiv.org/abs/2505.19607", "authors": ["Yewon Han", "Seoyun Yang", "Taesup Kim"], "title": "Energy-based Preference Optimization for Test-time Adaptation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Test-Time Adaptation (TTA) enhances model robustness by enabling adaptation\nto target distributions that differ from training distributions, improving\nreal-world generalizability. Existing TTA approaches focus on adjusting the\nconditional distribution; however these methods often depend on uncertain\npredictions in the absence of label information, leading to unreliable\nperformance. Energy-based frameworks suggest a promising alternative to address\ndistribution shifts without relying on uncertain predictions, instead computing\nthe marginal distribution of target data. However, they involve the critical\nchallenge of requiring extensive SGLD sampling, which is impractical for\ntest-time scenarios requiring immediate adaptation. In this work, we propose\nEnergy-based Preference Optimization for Test-time Adaptation (EPOTTA), which\nis based on a sampling free strategy. We first parameterize the target model\nusing a pretrained model and residual energy function, enabling marginal\nlikelihood maximization of target data without sampling. Building on the\nobservation that the parameterization is mathematically equivalent to DPO\nobjective, we then directly adapt the model to a target distribution without\nexplicitly training the residual. Our experiments verify that EPOTTA is\nwell-calibrated and performant while achieving computational efficiency."}
{"id": "2505.19091", "pdf": "https://arxiv.org/pdf/2505.19091", "abs": "https://arxiv.org/abs/2505.19091", "authors": ["Benjamin Clavi", "Florian Brand"], "title": "ReadBench: Measuring the Dense Text Visual Reading Ability of Vision-Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent advancements in Large Vision-Language Models (VLMs), have greatly\nenhanced their capability to jointly process text and images. However, despite\nextensive benchmarks evaluating visual comprehension (e.g., diagrams, color\nschemes, OCR tasks...), there is limited assessment of VLMs' ability to read\nand reason about text-rich images effectively. To fill this gap, we introduce\nReadBench, a multimodal benchmark specifically designed to evaluate the reading\ncomprehension capabilities of VLMs. ReadBench transposes contexts from\nestablished text-only benchmarks into images of text while keeping textual\nprompts and questions intact. Evaluating leading VLMs with ReadBench, we find\nminimal-but-present performance degradation on short, text-image inputs, while\nperformance sharply declines for longer, multi-page contexts. Our experiments\nfurther reveal that text resolution has negligible effects on multimodal\nperformance. These findings highlight needed improvements in VLMs, particularly\ntheir reasoning over visually presented extensive textual content, a capability\ncritical for practical applications. ReadBench is available at\nhttps://github.com/answerdotai/ReadBench ."}
{"id": "2505.19609", "pdf": "https://arxiv.org/pdf/2505.19609", "abs": "https://arxiv.org/abs/2505.19609", "authors": ["Hongtao Xu", "Wenting Shen", "Yuanxin Wei", "Ang Wang", "Guo Runfan", "Tianxing Wang", "Yong Li", "Mingzhen Li", "Weile Jia"], "title": "Skrull: Towards Efficient Long Context Fine-tuning through Dynamic Data Scheduling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Long-context supervised fine-tuning (Long-SFT) plays a vital role in\nenhancing the performance of large language models (LLMs) on long-context\ntasks. To smoothly adapt LLMs to long-context scenarios, this process typically\nentails training on mixed datasets containing both long and short sequences.\nHowever, this heterogeneous sequence length distribution poses significant\nchallenges for existing training systems, as they fail to simultaneously\nachieve high training efficiency for both long and short sequences, resulting\nin sub-optimal end-to-end system performance in Long-SFT. In this paper, we\npresent a novel perspective on data scheduling to address the challenges posed\nby the heterogeneous data distributions in Long-SFT. We propose Skrull, a\ndynamic data scheduler specifically designed for efficient long-SFT. Through\ndynamic data scheduling, Skrull balances the computation requirements of long\nand short sequences, improving overall training efficiency. Furthermore, we\nformulate the scheduling process as a joint optimization problem and thoroughly\nanalyze the trade-offs involved. Based on those analysis, Skrull employs a\nlightweight scheduling algorithm to achieve near-zero cost online scheduling in\nLong-SFT. Finally, we implement Skrull upon DeepSpeed, a state-of-the-art\ndistributed training system for LLMs. Experimental results demonstrate that\nSkrull outperforms DeepSpeed by 3.76x on average (up to 7.54x) in real-world\nlong-SFT scenarios."}
{"id": "2505.19093", "pdf": "https://arxiv.org/pdf/2505.19093", "abs": "https://arxiv.org/abs/2505.19093", "authors": ["Binh H. Ho", "Long Nguyen Chi", "TrungTin Nguyen", "Binh T. Nguyen", "Van Ha Hoang", "Christopher Drovandi"], "title": "A Unified Framework for Variable Selection in Model-Based Clustering with Missing Not at Random", "categories": ["stat.ME", "cs.LG", "math.ST", "stat.AP", "stat.ML", "stat.TH"], "comment": null, "summary": "Model-based clustering integrated with variable selection is a powerful tool\nfor uncovering latent structures within complex data. However, its\neffectiveness is often hindered by challenges such as identifying relevant\nvariables that define heterogeneous subgroups and handling data that are\nmissing not at random, a prevalent issue in fields like transcriptomics. While\nseveral notable methods have been proposed to address these problems, they\ntypically tackle each issue in isolation, thereby limiting their flexibility\nand adaptability. This paper introduces a unified framework designed to address\nthese challenges simultaneously. Our approach incorporates a data-driven\npenalty matrix into penalized clustering to enable more flexible variable\nselection, along with a mechanism that explicitly models the relationship\nbetween missingness and latent class membership. We demonstrate that, under\ncertain regularity conditions, the proposed framework achieves both asymptotic\nconsistency and selection consistency, even in the presence of missing data.\nThis unified strategy significantly enhances the capability and efficiency of\nmodel-based clustering, advancing methodologies for identifying informative\nvariables that define homogeneous subgroups in the presence of complex missing\ndata patterns. The performance of the framework, including its computational\nefficiency, is evaluated through simulations and demonstrated using both\nsynthetic and real-world transcriptomic datasets."}
{"id": "2505.19611", "pdf": "https://arxiv.org/pdf/2505.19611", "abs": "https://arxiv.org/abs/2505.19611", "authors": ["Ruolin Shen", "Xiaozhong Ji", "Kai WU", "Jiangning Zhang", "Yijun He", "HaiHua Yang", "Xiaobin Hu", "Xiaoyu Sun"], "title": "Align and Surpass Human Camouflaged Perception: Visual Refocus Reinforcement Fine-Tuning", "categories": ["cs.CV", "cs.AI"], "comment": "Project Website: \\url{https://github.com/HUuxiaobin/VRRF}", "summary": "Current multi-modal models exhibit a notable misalignment with the human\nvisual system when identifying objects that are visually assimilated into the\nbackground. Our observations reveal that these multi-modal models cannot\ndistinguish concealed objects, demonstrating an inability to emulate human\ncognitive processes which effectively utilize foreground-background similarity\nprinciples for visual analysis. To analyze this hidden human-model visual\nthinking discrepancy, we build a visual system that mimicks human visual\ncamouflaged perception to progressively and iteratively `refocus' visual\nconcealed content. The refocus is a progressive guidance mechanism enabling\nmodels to logically localize objects in visual images through stepwise\nreasoning. The localization process of concealed objects requires hierarchical\nattention shifting with dynamic adjustment and refinement of prior cognitive\nknowledge. In this paper, we propose a visual refocus reinforcement framework\nvia the policy optimization algorithm to encourage multi-modal models to think\nand refocus more before answering, and achieve excellent reasoning abilities to\nalign and even surpass human camouflaged perception systems. Our extensive\nexperiments on camouflaged perception successfully demonstrate the emergence of\nrefocus visual phenomena, characterized by multiple reasoning tokens and\ndynamic adjustment of the detection box. Besides, experimental results on both\ncamouflaged object classification and detection tasks exhibit significantly\nsuperior performance compared to Supervised Fine-Tuning (SFT) baselines."}
{"id": "2505.19102", "pdf": "https://arxiv.org/pdf/2505.19102", "abs": "https://arxiv.org/abs/2505.19102", "authors": ["Sergey Samsonov", "Marina Sheshukova", "Eric Moulines", "Alexey Naumov"], "title": "Statistical inference for Linear Stochastic Approximation with Markovian Noise", "categories": ["stat.ML", "cs.LG", "math.OC", "math.ST", "stat.TH", "60F05, 62L20, 62E20"], "comment": null, "summary": "In this paper we derive non-asymptotic Berry-Esseen bounds for Polyak-Ruppert\naveraged iterates of the Linear Stochastic Approximation (LSA) algorithm driven\nby the Markovian noise. Our analysis yields $\\mathcal{O}(n^{-1/4})$ convergence\nrates to the Gaussian limit in the Kolmogorov distance. We further establish\nthe non-asymptotic validity of a multiplier block bootstrap procedure for\nconstructing the confidence intervals, guaranteeing consistent inference under\nMarkovian sampling. Our work provides the first non-asymptotic guarantees on\nthe rate of convergence of bootstrap-based confidence intervals for stochastic\napproximation with Markov noise. Moreover, we recover the classical rate of\norder $\\mathcal{O}(n^{-1/8})$ up to logarithmic factors for estimating the\nasymptotic variance of the iterates of the LSA algorithm."}
{"id": "2505.19616", "pdf": "https://arxiv.org/pdf/2505.19616", "abs": "https://arxiv.org/abs/2505.19616", "authors": ["Rui Cai", "Bangzheng Li", "Xiaofei Wen", "Muhao Chen", "Zhe Zhao"], "title": "Diagnosing and Mitigating Modality Interference in Multimodal Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities across tasks, yet they often exhibit difficulty in distinguishing\ntask-relevant from irrelevant signals, particularly in tasks like Visual\nQuestion Answering (VQA), which can lead to susceptibility to misleading or\nspurious inputs. We refer to this broader limitation as the Cross-Modality\nCompetency Problem: the model's inability to fairly evaluate all modalities.\nThis vulnerability becomes more evident in modality-specific tasks such as\nimage classification or pure text question answering, where models are expected\nto rely solely on one modality. In such tasks, spurious information from\nirrelevant modalities often leads to significant performance degradation. We\nrefer to this failure as Modality Interference, which serves as a concrete and\nmeasurable instance of the cross-modality competency problem. We further design\na perturbation-based causal diagnostic experiment to verify and quantify this\nproblem. To mitigate modality interference, we propose a novel framework to\nfine-tune MLLMs, including perturbation-based data augmentations with both\nheuristic perturbations and adversarial perturbations via Projected Gradient\nDescent (PGD), and a consistency regularization strategy applied to model\noutputs with original and perturbed inputs. Experiments on multiple benchmark\ndatasets (image-heavy, text-heavy, and VQA tasks) and multiple model families\nwith different scales demonstrate significant improvements in robustness and\ncross-modality competency, indicating our method's effectiveness in boosting\nunimodal reasoning ability while enhancing performance on multimodal tasks."}
{"id": "2505.19110", "pdf": "https://arxiv.org/pdf/2505.19110", "abs": "https://arxiv.org/abs/2505.19110", "authors": ["Vishwa Mohan Singh", "Alberto Gaston Villagran Asiares", "Luisa Sophie Schuhmacher", "Kate Rendall", "Simon Weibrod", "David Rgamer", "Inga Krte"], "title": "An Interpretable Representation Learning Approach for Diffusion Tensor Imaging", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted for publication at MIDL 2025", "summary": "Diffusion Tensor Imaging (DTI) tractography offers detailed insights into the\nstructural connectivity of the brain, but presents challenges in effective\nrepresentation and interpretation in deep learning models. In this work, we\npropose a novel 2D representation of DTI tractography that encodes tract-level\nfractional anisotropy (FA) values into a 9x9 grayscale image. This\nrepresentation is processed through a Beta-Total Correlation Variational\nAutoencoder with a Spatial Broadcast Decoder to learn a disentangled and\ninterpretable latent embedding. We evaluate the quality of this embedding using\nsupervised and unsupervised representation learning strategies, including\nauxiliary classification, triplet loss, and SimCLR-based contrastive learning.\nCompared to the 1D Group deep neural network (DNN) baselines, our approach\nimproves the F1 score in a downstream sex classification task by 15.74% and\nshows a better disentanglement than the 3D representation."}
{"id": "2505.19620", "pdf": "https://arxiv.org/pdf/2505.19620", "abs": "https://arxiv.org/abs/2505.19620", "authors": ["Jiawen Chen", "Qi Shao", "Duxin Chen", "Wenwu Yu"], "title": "Decoupling Spatio-Temporal Prediction: When Lightweight Large Models Meet Adaptive Hypergraphs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Spatio-temporal prediction is a pivotal task with broad applications in\ntraffic management, climate monitoring, energy scheduling, etc. However,\nexisting methodologies often struggle to balance model expressiveness and\ncomputational efficiency, especially when scaling to large real-world datasets.\nTo tackle these challenges, we propose STH-SepNet (Spatio-Temporal Hypergraph\nSeparation Networks), a novel framework that decouples temporal and spatial\nmodeling to enhance both efficiency and precision. Therein, the temporal\ndimension is modeled using lightweight large language models, which effectively\ncapture low-rank temporal dynamics. Concurrently, the spatial dimension is\naddressed through an adaptive hypergraph neural network, which dynamically\nconstructs hyperedges to model intricate, higher-order interactions. A\ncarefully designed gating mechanism is integrated to seamlessly fuse temporal\nand spatial representations. By leveraging the fundamental principles of\nlow-rank temporal dynamics and spatial interactions, STH-SepNet offers a\npragmatic and scalable solution for spatio-temporal prediction in real-world\napplications. Extensive experiments on large-scale real-world datasets across\nmultiple benchmarks demonstrate the effectiveness of STH-SepNet in boosting\npredictive performance while maintaining computational efficiency. This work\nmay provide a promising lightweight framework for spatio-temporal prediction,\naiming to reduce computational demands and while enhancing predictive\nperformance. Our code is avaliable at\nhttps://github.com/SEU-WENJIA/ST-SepNet-Lightweight-LLMs-Meet-Adaptive-Hypergraphs."}
{"id": "2505.19122", "pdf": "https://arxiv.org/pdf/2505.19122", "abs": "https://arxiv.org/abs/2505.19122", "authors": ["Eric Tillman Bill", "Cristian Perez Jensen", "Sotiris Anagnostidis", "Dimitri von Rtte"], "title": "Exploring Magnitude Preservation and Rotation Modulation in Diffusion Transformers", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Denoising diffusion models exhibit remarkable generative capabilities, but\nremain challenging to train due to their inherent stochasticity, where\nhigh-variance gradient estimates lead to slow convergence. Previous works have\nshown that magnitude preservation helps with stabilizing training in the U-net\narchitecture. This work explores whether this effect extends to the Diffusion\nTransformer (DiT) architecture. As such, we propose a magnitude-preserving\ndesign that stabilizes training without normalization layers. Motivated by the\ngoal of maintaining activation magnitudes, we additionally introduce rotation\nmodulation, which is a novel conditioning method using learned rotations\ninstead of traditional scaling or shifting. Through empirical evaluations and\nablation studies on small-scale models, we show that magnitude-preserving\nstrategies significantly improve performance, notably reducing FID scores by\n$\\sim$12.8%. Further, we show that rotation modulation combined with scaling is\ncompetitive with AdaLN, while requiring $\\sim$5.4% fewer parameters. This work\nprovides insights into conditioning strategies and magnitude control. We will\npublicly release the implementation of our method."}
{"id": "2505.19623", "pdf": "https://arxiv.org/pdf/2505.19623", "abs": "https://arxiv.org/abs/2505.19623", "authors": ["Yu Shang", "Peijie Liu", "Yuwei Yan", "Zijing Wu", "Leheng Sheng", "Yuanqing Yu", "Chumeng Jiang", "An Zhang", "Fengli Xu", "Yu Wang", "Min Zhang", "Yong Li"], "title": "AgentRecBench: Benchmarking LLM Agent-based Personalized Recommender Systems", "categories": ["cs.IR", "cs.AI"], "comment": "15 pages, 6 figures", "summary": "The emergence of agentic recommender systems powered by Large Language Models\n(LLMs) represents a paradigm shift in personalized recommendations, leveraging\nLLMs' advanced reasoning and role-playing capabilities to enable autonomous,\nadaptive decision-making. Unlike traditional recommendation approaches, agentic\nrecommender systems can dynamically gather and interpret user-item interactions\nfrom complex environments, generating robust recommendation strategies that\ngeneralize across diverse scenarios. However, the field currently lacks\nstandardized evaluation protocols to systematically assess these methods. To\naddress this critical gap, we propose: (1) an interactive textual\nrecommendation simulator incorporating rich user and item metadata and three\ntypical evaluation scenarios (classic, evolving-interest, and cold-start\nrecommendation tasks); (2) a unified modular framework for developing and\nstudying agentic recommender systems; and (3) the first comprehensive benchmark\ncomparing 10 classical and agentic recommendation methods. Our findings\ndemonstrate the superiority of agentic systems and establish actionable design\nguidelines for their core components. The benchmark environment has been\nrigorously validated through an open challenge and remains publicly available\nwith a continuously maintained\nleaderboard~\\footnote[2]{https://tsinghua-fib-lab.github.io/AgentSocietyChallenge/pages/overview.html},\nfostering ongoing community engagement and reproducible research. The benchmark\nis available at:\n\\hyperlink{https://huggingface.co/datasets/SGJQovo/AgentRecBench}{https://huggingface.co/datasets/SGJQovo/AgentRecBench}."}
{"id": "2505.19134", "pdf": "https://arxiv.org/pdf/2505.19134", "abs": "https://arxiv.org/abs/2505.19134", "authors": ["Shang Liu", "Zhongze Cai", "Hanzhao Wang", "Zhongyao Ma", "Xiaocheng Li"], "title": "Incentivizing High-Quality Human Annotations with Golden Questions", "categories": ["cs.GT", "cs.LG", "stat.ML"], "comment": "arXiv admin note: text overlap with arXiv:2502.06387", "summary": "Human-annotated data plays a vital role in training large language models\n(LLMs), such as supervised fine-tuning and human preference alignment. However,\nit is not guaranteed that paid human annotators produce high-quality data. In\nthis paper, we study how to incentivize human annotators to do so. We start\nfrom a principal-agent model to model the dynamics between the company (the\nprincipal) and the annotator (the agent), where the principal can only monitor\nthe annotation quality by examining $n$ samples. We investigate the maximum\nlikelihood estimators (MLE) and the corresponding hypothesis testing to\nincentivize annotators: the agent is given a bonus if the MLE passes the test.\nBy analyzing the variance of the outcome, we show that the strategic behavior\nof the agent makes the hypothesis testing very different from traditional ones:\nUnlike the exponential rate proved by the large deviation theory, the\nprincipal-agent model's hypothesis testing rate is of $\\Theta(1/\\sqrt{n \\log\nn})$. Our theory implies two criteria for the \\emph{golden questions} to\nmonitor the performance of the annotators: they should be of (1) high certainty\nand (2) similar format to normal ones. In that light, we select a set of golden\nquestions in human preference data. By doing incentive-compatible experiments,\nwe find out that the annotators' behavior is better revealed by those golden\nquestions, compared to traditional survey techniques such as instructed\nmanipulation checks."}
{"id": "2505.19624", "pdf": "https://arxiv.org/pdf/2505.19624", "abs": "https://arxiv.org/abs/2505.19624", "authors": ["Pusheng Xu", "Xia Gong", "Xiaolan Chen", "Weiyi Zhang", "Jiancheng Yang", "Bingjie Yan", "Meng Yuan", "Yalin Zheng", "Mingguang He", "Danli Shi"], "title": "Benchmarking Large Multimodal Models for Ophthalmic Visual Question Answering with OphthalWeChat", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Purpose: To develop a bilingual multimodal visual question answering (VQA)\nbenchmark for evaluating VLMs in ophthalmology. Methods: Ophthalmic image posts\nand associated captions published between January 1, 2016, and December 31,\n2024, were collected from WeChat Official Accounts. Based on these captions,\nbilingual question-answer (QA) pairs in Chinese and English were generated\nusing GPT-4o-mini. QA pairs were categorized into six subsets by question type\nand language: binary (Binary_CN, Binary_EN), single-choice (Single-choice_CN,\nSingle-choice_EN), and open-ended (Open-ended_CN, Open-ended_EN). The benchmark\nwas used to evaluate the performance of three VLMs: GPT-4o, Gemini 2.0 Flash,\nand Qwen2.5-VL-72B-Instruct. Results: The final OphthalWeChat dataset included\n3,469 images and 30,120 QA pairs across 9 ophthalmic subspecialties, 548\nconditions, 29 imaging modalities, and 68 modality combinations. Gemini 2.0\nFlash achieved the highest overall accuracy (0.548), outperforming GPT-4o\n(0.522, P < 0.001) and Qwen2.5-VL-72B-Instruct (0.514, P < 0.001). It also led\nin both Chinese (0.546) and English subsets (0.550). Subset-specific\nperformance showed Gemini 2.0 Flash excelled in Binary_CN (0.687),\nSingle-choice_CN (0.666), and Single-choice_EN (0.646), while GPT-4o ranked\nhighest in Binary_EN (0.717), Open-ended_CN (BLEU-1: 0.301; BERTScore: 0.382),\nand Open-ended_EN (BLEU-1: 0.183; BERTScore: 0.240). Conclusions: This study\npresents the first bilingual VQA benchmark for ophthalmology, distinguished by\nits real-world context and inclusion of multiple examinations per patient. The\ndataset reflects authentic clinical decision-making scenarios and enables\nquantitative evaluation of VLMs, supporting the development of accurate,\nspecialized, and trustworthy AI systems for eye care."}
{"id": "2505.19136", "pdf": "https://arxiv.org/pdf/2505.19136", "abs": "https://arxiv.org/abs/2505.19136", "authors": ["Frank Shih", "Zhenghao Jiang", "Faming Liang"], "title": "Uncertainty Quantification for Physics-Informed Neural Networks with Extended Fiducial Inference", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Uncertainty quantification (UQ) in scientific machine learning is\nincreasingly critical as neural networks are widely adopted to tackle complex\nproblems across diverse scientific disciplines. For physics-informed neural\nnetworks (PINNs), a prominent model in scientific machine learning, uncertainty\nis typically quantified using Bayesian or dropout methods. However, both\napproaches suffer from a fundamental limitation: the prior distribution or\ndropout rate required to construct honest confidence sets cannot be determined\nwithout additional information. In this paper, we propose a novel method within\nthe framework of extended fiducial inference (EFI) to provide rigorous\nuncertainty quantification for PINNs. The proposed method leverages a\nnarrow-neck hyper-network to learn the parameters of the PINN and quantify\ntheir uncertainty based on imputed random errors in the observations. This\napproach overcomes the limitations of Bayesian and dropout methods, enabling\nthe construction of honest confidence sets based solely on observed data. This\nadvancement represents a significant breakthrough for PINNs, greatly enhancing\ntheir reliability, interpretability, and applicability to real-world scientific\nand engineering challenges. Moreover, it establishes a new theoretical\nframework for EFI, extending its application to large-scale models, eliminating\nthe need for sparse hyper-networks, and significantly improving the\nautomaticity and robustness of statistical inference."}
{"id": "2505.19625", "pdf": "https://arxiv.org/pdf/2505.19625", "abs": "https://arxiv.org/abs/2505.19625", "authors": ["Hassan Sartaj", "Shaukat Ali"], "title": "Search-Based Software Engineering in the Landscape of AI Foundation Models", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Search-based software engineering (SBSE), at the intersection of artificial\nintelligence (AI) and software engineering, has been an active area of research\nfor about 25 years. It has been applied to solve numerous problems across the\nentire software engineering lifecycle and has demonstrated its versatility in\nmultiple domains. With the recent advancements in AI, particularly the\nemergence of foundation models (FMs), the evolution of SBSE alongside FMs\nremains undetermined. In this window of opportunity, we propose a research\nroadmap that articulates the current landscape of SBSE in relation to\nfoundation models (FMs), highlights open challenges, and outlines potential\nresearch directions for advancing SBSE through its interplay with FMs. This\nroadmap aims to establish a forward-thinking and innovative perspective for the\nfuture of SBSE in the era of FMs."}
{"id": "2505.19145", "pdf": "https://arxiv.org/pdf/2505.19145", "abs": "https://arxiv.org/abs/2505.19145", "authors": ["Weijie Su"], "title": "Do Large Language Models (Really) Need Statistical Foundations?", "categories": ["stat.ME", "cs.LG", "stat.AP"], "comment": null, "summary": "Large language models (LLMs) represent a new paradigm for processing\nunstructured data, with applications across an unprecedented range of domains.\nIn this paper, we address, through two arguments, whether the development and\napplication of LLMs would genuinely benefit from foundational contributions\nfrom the statistics discipline. First, we argue affirmatively, beginning with\nthe observation that LLMs are inherently statistical models due to their\nprofound data dependency and stochastic generation processes, where statistical\ninsights are naturally essential for handling variability and uncertainty.\nSecond, we argue that the persistent black-box nature of LLMs -- stemming from\ntheir immense scale, architectural complexity, and development practices often\nprioritizing empirical performance over theoretical interpretability -- renders\nclosed-form or purely mechanistic analyses generally intractable, thereby\nnecessitating statistical approaches due to their flexibility and often\ndemonstrated effectiveness. To substantiate these arguments, the paper outlines\nseveral research areas -- including alignment, watermarking, uncertainty\nquantification, evaluation, and data mixture optimization -- where statistical\nmethodologies are critically needed and are already beginning to make valuable\ncontributions. We conclude with a discussion suggesting that statistical\nresearch concerning LLMs will likely form a diverse ``mosaic'' of specialized\ntopics rather than deriving from a single unifying theory, and highlighting the\nimportance of timely engagement by our statistics community in LLM research."}
{"id": "2505.19631", "pdf": "https://arxiv.org/pdf/2505.19631", "abs": "https://arxiv.org/abs/2505.19631", "authors": ["Zihong Zhang", "Liqi He", "Zuchao Li", "Lefei Zhang", "Hai Zhao", "Bo Du"], "title": "Segment First or Comprehend First? Explore the Limit of Unsupervised Word Segmentation with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Word segmentation stands as a cornerstone of Natural Language Processing\n(NLP). Based on the concept of \"comprehend first, segment later\", we propose a\nnew framework to explore the limit of unsupervised word segmentation with Large\nLanguage Models (LLMs) and evaluate the semantic understanding capabilities of\nLLMs based on word segmentation. We employ current mainstream LLMs to perform\nword segmentation across multiple languages to assess LLMs' \"comprehension\".\nOur findings reveal that LLMs are capable of following simple prompts to\nsegment raw text into words. There is a trend suggesting that models with more\nparameters tend to perform better on multiple languages. Additionally, we\nintroduce a novel unsupervised method, termed LLACA ($\\textbf{L}$arge\n$\\textbf{L}$anguage Model-Inspired $\\textbf{A}$ho-$\\textbf{C}$orasick\n$\\textbf{A}$utomaton). Leveraging the advanced pattern recognition capabilities\nof Aho-Corasick automata, LLACA innovatively combines these with the deep\ninsights of well-pretrained LLMs. This approach not only enables the\nconstruction of a dynamic $n$-gram model that adjusts based on contextual\ninformation but also integrates the nuanced understanding of LLMs, offering\nsignificant improvements over traditional methods. Our source code is available\nat https://github.com/hkr04/LLACA"}
{"id": "2505.19164", "pdf": "https://arxiv.org/pdf/2505.19164", "abs": "https://arxiv.org/abs/2505.19164", "authors": ["Ashirbad Mishra", "Jinyu Zhao", "Soumik Dey", "Hansi Wu", "Binbin Li", "Kamesh Madduri"], "title": "BroadGen: A Framework for Generating Effective and Efficient Advertiser Broad Match Keyphrase Recommendations", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": null, "summary": "In the domain of sponsored search advertising, the focus of Keyphrase\nrecommendation has largely been on exact match types, which pose issues such as\nhigh management expenses, limited targeting scope, and evolving search query\npatterns. Alternatives like Broad match types can alleviate certain drawbacks\nof exact matches but present challenges like poor targeting accuracy and\nminimal supervisory signals owing to limited advertiser usage. This research\ndefines the criteria for an ideal broad match, emphasizing on both efficiency\nand effectiveness, ensuring that a significant portion of matched queries are\nrelevant. We propose BroadGen, an innovative framework that recommends\nefficient and effective broad match keyphrases by utilizing historical search\nquery data. Additionally, we demonstrate that BroadGen, through token\ncorrespondence modeling, maintains better query stability over time. BroadGen's\ncapabilities allow it to serve daily, millions of sellers at eBay with over 2.3\nbillion items."}
{"id": "2505.19644", "pdf": "https://arxiv.org/pdf/2505.19644", "abs": "https://arxiv.org/abs/2505.19644", "authors": ["Anton Firc", "Manasi Chibber", "Jagabandhu Mishra", "Vishwanath Pratap Singh", "Tomi Kinnunen", "Kamil Malinka"], "title": "STOPA: A Database of Systematic VariaTion Of DeePfake Audio for Open-Set Source Tracing and Attribution", "categories": ["cs.SD", "cs.AI", "cs.CR", "eess.AS", "68T45, 68T10, 94A08", "I.2.7; I.5.4; K.4.1"], "comment": "Accepted to Interspeech 2025 conference", "summary": "A key research area in deepfake speech detection is source tracing -\ndetermining the origin of synthesised utterances. The approaches may involve\nidentifying the acoustic model (AM), vocoder model (VM), or other\ngeneration-specific parameters. However, progress is limited by the lack of a\ndedicated, systematically curated dataset. To address this, we introduce STOPA,\na systematically varied and metadata-rich dataset for deepfake speech source\ntracing, covering 8 AMs, 6 VMs, and diverse parameter settings across 700k\nsamples from 13 distinct synthesisers. Unlike existing datasets, which often\nfeature limited variation or sparse metadata, STOPA provides a systematically\ncontrolled framework covering a broader range of generative factors, such as\nthe choice of the vocoder model, acoustic model, or pretrained weights,\nensuring higher attribution reliability. This control improves attribution\naccuracy, aiding forensic analysis, deepfake detection, and generative model\ntransparency."}
{"id": "2505.19166", "pdf": "https://arxiv.org/pdf/2505.19166", "abs": "https://arxiv.org/abs/2505.19166", "authors": ["Eric Tillmann Bill", "Enis Simsar", "Thomas Hofmann"], "title": "JEDI: The Force of Jensen-Shannon Divergence in Disentangling Diffusion Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We introduce JEDI, a test-time adaptation method that enhances subject\nseparation and compositional alignment in diffusion models without requiring\nretraining or external supervision. JEDI operates by minimizing semantic\nentanglement in attention maps using a novel Jensen-Shannon divergence based\nobjective. To improve efficiency, we leverage adversarial optimization,\nreducing the number of updating steps required.\n  JEDI is model-agnostic and applicable to architectures such as Stable\nDiffusion 1.5 and 3.5, consistently improving prompt alignment and\ndisentanglement in complex scenes. Additionally, JEDI provides a lightweight,\nCLIP-free disentanglement score derived from internal attention distributions,\noffering a principled benchmark for compositional alignment under test-time\nconditions. We will publicly release the implementation of our method."}
{"id": "2505.19645", "pdf": "https://arxiv.org/pdf/2505.19645", "abs": "https://arxiv.org/abs/2505.19645", "authors": ["Zongle Huang", "Lei Zhu", "Zongyuan Zhan", "Ting Hu", "Weikai Mao", "Xianzhi Yu", "Yongpan Liu", "Tianyu Zhang"], "title": "MoESD: Unveil Speculative Decoding's Potential for Accelerating Sparse MoE", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success across many\napplications, with Mixture of Experts (MoE) models demonstrating great\npotential. Compared to traditional dense models, MoEs achieve better\nperformance with less computation. Speculative decoding (SD) is a widely used\ntechnique to accelerate LLM inference without accuracy loss, but it has been\nconsidered efficient only for dense models. In this work, we first demonstrate\nthat, under medium batch sizes, MoE surprisingly benefits more from SD than\ndense models. Furthermore, as MoE becomes sparser -- the prevailing trend in\nMoE designs -- the batch size range where SD acceleration is expected to be\neffective becomes broader. To quantitatively understand tradeoffs involved in\nSD, we develop a reliable modeling based on theoretical analyses. While current\nSD research primarily focuses on improving acceptance rates of algorithms,\nchanges in workload and model architecture can still lead to degraded SD\nacceleration even with high acceptance rates. To address this limitation, we\nintroduce a new metric 'target efficiency' that characterizes these effects,\nthus helping researchers identify system bottlenecks and understand SD\nacceleration more comprehensively. For scenarios like private serving, this\nwork unveils a new perspective to speed up MoE inference, where existing\nsolutions struggle. Experiments on different GPUs show up to 2.29x speedup for\nQwen2-57B-A14B at medium batch sizes and validate our theoretical predictions."}
{"id": "2505.19178", "pdf": "https://arxiv.org/pdf/2505.19178", "abs": "https://arxiv.org/abs/2505.19178", "authors": ["Akhila Yaragoppa", "Siddharth"], "title": "Saliency-guided Emotion Modeling: Predicting Viewer Reactions from Video Stimuli", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted for publication at IBPRIA 2025 Conference in Coimbra,\n  Portugal", "summary": "Understanding the emotional impact of videos is crucial for applications in\ncontent creation, advertising, and Human-Computer Interaction (HCI).\nTraditional affective computing methods rely on self-reported emotions, facial\nexpression analysis, and biosensing data, yet they often overlook the role of\nvisual saliency -- the naturally attention-grabbing regions within a video. In\nthis study, we utilize deep learning to introduce a novel saliency-based\napproach to emotion prediction by extracting two key features: saliency area\nand number of salient regions. Using the HD2S saliency model and OpenFace\nfacial action unit analysis, we examine the relationship between video saliency\nand viewer emotions. Our findings reveal three key insights: (1) Videos with\nmultiple salient regions tend to elicit high-valence, low-arousal emotions, (2)\nVideos with a single dominant salient region are more likely to induce\nlow-valence, high-arousal responses, and (3) Self-reported emotions often\nmisalign with facial expression-based emotion detection, suggesting limitations\nin subjective reporting. By leveraging saliency-driven insights, this work\nprovides a computationally efficient and interpretable alternative for emotion\nmodeling, with implications for content creation, personalized media\nexperiences, and affective computing research."}
{"id": "2505.19648", "pdf": "https://arxiv.org/pdf/2505.19648", "abs": "https://arxiv.org/abs/2505.19648", "authors": ["Qiaolan Meng", "Juhua Pu", "Hongting Niu", "Yuyi Wang", "Yuanhong Wang", "Ondej Kuelka"], "title": "Model Enumeration of Two-Variable Logic with Quadratic Delay Complexity", "categories": ["cs.LO", "cs.AI"], "comment": "16 pages, 4 figures and to be published in Fortieth Annual ACM/IEEE\n  Symposium on Logic in Computer Science (LICS)", "summary": "We study the model enumeration problem of the function-free, finite domain\nfragment of first-order logic with two variables ($FO^2$). Specifically, given\nan $FO^2$ sentence $\\Gamma$ and a positive integer $n$, how can one enumerate\nall the models of $\\Gamma$ over a domain of size $n$? In this paper, we devise\na novel algorithm to address this problem. The delay complexity, the time\nrequired between producing two consecutive models, of our algorithm is\nquadratic in the given domain size $n$ (up to logarithmic factors) when the\nsentence is fixed. This complexity is almost optimal since the interpretation\nof binary predicates in any model requires at least $\\Omega(n^2)$ bits to\nrepresent."}
{"id": "2505.19184", "pdf": "https://arxiv.org/pdf/2505.19184", "abs": "https://arxiv.org/abs/2505.19184", "authors": ["Minh Nhat Nguyen", "Pradyumna Shyama Prasad"], "title": "Two LLMs debate, both are certain they've won", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Can LLMs accurately adjust their confidence when facing opposition? Building\non previous studies measuring calibration on static fact-based\nquestion-answering tasks, we evaluate Large Language Models (LLMs) in a\ndynamic, adversarial debate setting, uniquely combining two realistic factors:\n(a) a multi-turn format requiring models to update beliefs as new information\nemerges, and (b) a zero-sum structure to control for task-related uncertainty,\nsince mutual high-confidence claims imply systematic overconfidence. We\norganized 60 three-round policy debates among ten state-of-the-art LLMs, with\nmodels privately rating their confidence (0-100) in winning after each round.\nWe observed five concerning patterns: (1) Systematic overconfidence: models\nbegan debates with average initial confidence of 72.9% vs. a rational 50%\nbaseline. (2) Confidence escalation: rather than reducing confidence as debates\nprogressed, debaters increased their win probabilities, averaging 83% by the\nfinal round. (3) Mutual overestimation: in 61.7% of debates, both sides\nsimultaneously claimed >=75% probability of victory, a logical impossibility.\n(4) Persistent self-debate bias: models debating identical copies increased\nconfidence from 64.1% to 75.2%; even when explicitly informed their chance of\nwinning was exactly 50%, confidence still rose (from 50.0% to 57.1%). (5)\nMisaligned private reasoning: models' private scratchpad thoughts sometimes\ndiffered from their public confidence ratings, raising concerns about\nfaithfulness of chain-of-thought reasoning. These results suggest LLMs lack the\nability to accurately self-assess or update their beliefs in dynamic,\nmulti-turn tasks; a major concern as LLM outputs are deployed without careful\nreview in assistant roles or agentic settings."}
{"id": "2505.19658", "pdf": "https://arxiv.org/pdf/2505.19658", "abs": "https://arxiv.org/abs/2505.19658", "authors": ["Ali Nouri", "Beatriz Cabrero-Daniel", "Zhennan Fei", "Krishna Ronanki", "Hkan Sivencrona", "Christian Berger"], "title": "Large Language Models in Code Co-generation for Safe Autonomous Vehicles", "categories": ["cs.SE", "cs.AI"], "comment": "Accepted in the 44th International Conference on Computer Safety,\n  Reliability and Security (SafeComp 2025)", "summary": "Software engineers in various industrial domains are already using Large\nLanguage Models (LLMs) to accelerate the process of implementing parts of\nsoftware systems. When considering its potential use for ADAS or AD systems in\nthe automotive context, there is a need to systematically assess this new\nsetup: LLMs entail a well-documented set of risks for safety-related systems'\ndevelopment due to their stochastic nature. To reduce the effort for code\nreviewers to evaluate LLM-generated code, we propose an evaluation pipeline to\nconduct sanity-checks on the generated code. We compare the performance of six\nstate-of-the-art LLMs (CodeLlama, CodeGemma, DeepSeek-r1, DeepSeek-Coders,\nMistral, and GPT-4) on four safety-related programming tasks. Additionally, we\nqualitatively analyse the most frequent faults generated by these LLMs,\ncreating a failure-mode catalogue to support human reviewers. Finally, the\nlimitations and capabilities of LLMs in code generation, and the use of the\nproposed pipeline in the existing process, are discussed."}
{"id": "2505.19206", "pdf": "https://arxiv.org/pdf/2505.19206", "abs": "https://arxiv.org/abs/2505.19206", "authors": ["Richard He Bai", "Zijin Gu", "Tatiana Likhomanenko", "Navdeep Jaitly"], "title": "SpeakStream: Streaming Text-to-Speech with Interleaved Data", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "The latency bottleneck of traditional text-to-speech (TTS) systems\nfundamentally hinders the potential of streaming large language models (LLMs)\nin conversational AI. These TTS systems, typically trained and inferenced on\ncomplete utterances, introduce unacceptable delays, even with optimized\ninference speeds, when coupled with streaming LLM outputs. This is particularly\nproblematic for creating responsive conversational agents where low first-token\nlatency is critical. In this paper, we present SpeakStream, a streaming TTS\nsystem that generates audio incrementally from streaming text using a\ndecoder-only architecture. SpeakStream is trained using a next-step prediction\nloss on interleaved text-speech data. During inference, it generates speech\nincrementally while absorbing streaming input text, making it particularly\nsuitable for cascaded conversational AI agents where an LLM streams text to a\nTTS system. Our experiments demonstrate that SpeakStream achieves\nstate-of-the-art latency results in terms of first-token latency while\nmaintaining the quality of non-streaming TTS systems."}
{"id": "2505.19660", "pdf": "https://arxiv.org/pdf/2505.19660", "abs": "https://arxiv.org/abs/2505.19660", "authors": ["Tingjia Shen", "Hao Wang", "Chuan Qin", "Ruijun Sun", "Yang Song", "Defu Lian", "Hengshu Zhu", "Enhong Chen"], "title": "GenKI: Enhancing Open-Domain Question Answering with Knowledge Integration and Controllable Generation in Large Language Models", "categories": ["cs.CL", "cs.AI", "68P20", "H.3.4; I.2.6"], "comment": "13 pages, 5 figures", "summary": "Open-domain question answering (OpenQA) represents a cornerstone in natural\nlanguage processing (NLP), primarily focused on extracting answers from\nunstructured textual data. With the rapid advancements in Large Language Models\n(LLMs), LLM-based OpenQA methods have reaped the benefits of emergent\nunderstanding and answering capabilities enabled by massive parameters compared\nto traditional methods. However, most of these methods encounter two critical\nchallenges: how to integrate knowledge into LLMs effectively and how to\nadaptively generate results with specific answer formats for various task\nsituations. To address these challenges, we propose a novel framework named\nGenKI, which aims to improve the OpenQA performance by exploring Knowledge\nIntegration and controllable Generation on LLMs simultaneously. Specifically,\nwe first train a dense passage retrieval model to retrieve associated knowledge\nfrom a given knowledge base. Subsequently, we introduce a novel knowledge\nintegration model that incorporates the retrieval knowledge into instructions\nduring fine-tuning to intensify the model. Furthermore, to enable controllable\ngeneration in LLMs, we leverage a certain fine-tuned LLM and an ensemble based\non text consistency incorporating all coherence, fluency, and answer format\nassurance. Finally, extensive experiments conducted on the TriviaQA, MSMARCO,\nand CMRC2018 datasets, featuring diverse answer formats, have demonstrated the\neffectiveness of GenKI with comparison of state-of-the-art baselines. Moreover,\nablation studies have disclosed a linear relationship between the frequency of\nretrieved knowledge and the model's ability to recall knowledge accurately\nagainst the ground truth. Our code of GenKI is available at\nhttps://github.com/USTC-StarTeam/GenKI"}
{"id": "2505.19219", "pdf": "https://arxiv.org/pdf/2505.19219", "abs": "https://arxiv.org/abs/2505.19219", "authors": ["Shiyue Wang", "Haozheng Xu", "Yuhan Zhang", "Jingran Lin", "Changhong Lu", "Xiangfeng Wang", "Wenhao Li"], "title": "Where Paths Collide: A Comprehensive Survey of Classic and Learning-Based Multi-Agent Pathfinding", "categories": ["cs.AI", "cs.LG", "cs.MA", "math.CO"], "comment": "112 pages, 21 figures, 20 tables", "summary": "Multi-Agent Path Finding (MAPF) is a fundamental problem in artificial\nintelligence and robotics, requiring the computation of collision-free paths\nfor multiple agents navigating from their start locations to designated goals.\nAs autonomous systems become increasingly prevalent in warehouses, urban\ntransportation, and other complex environments, MAPF has evolved from a\ntheoretical challenge to a critical enabler of real-world multi-robot\ncoordination. This comprehensive survey bridges the long-standing divide\nbetween classical algorithmic approaches and emerging learning-based methods in\nMAPF research. We present a unified framework that encompasses search-based\nmethods (including Conflict-Based Search, Priority-Based Search, and Large\nNeighborhood Search), compilation-based approaches (SAT, SMT, CSP, ASP, and MIP\nformulations), and data-driven techniques (reinforcement learning, supervised\nlearning, and hybrid strategies). Through systematic analysis of experimental\npractices across 200+ papers, we uncover significant disparities in evaluation\nmethodologies, with classical methods typically tested on larger-scale\ninstances (up to 200 by 200 grids with 1000+ agents) compared to learning-based\napproaches (predominantly 10-100 agents). We provide a comprehensive taxonomy\nof evaluation metrics, environment types, and baseline selections, highlighting\nthe need for standardized benchmarking protocols. Finally, we outline promising\nfuture directions including mixed-motive MAPF with game-theoretic\nconsiderations, language-grounded planning with large language models, and\nneural solver architectures that combine the rigor of classical methods with\nthe flexibility of deep learning. This survey serves as both a comprehensive\nreference for researchers and a practical guide for deploying MAPF solutions in\nincreasingly complex real-world applications."}
{"id": "2505.19663", "pdf": "https://arxiv.org/pdf/2505.19663", "abs": "https://arxiv.org/abs/2505.19663", "authors": ["Yigitcan zer", "Woosung Choi", "Joan Serr", "Mayank Kumar Singh", "Wei-Hsiang Liao", "Yuki Mitsufuji"], "title": "A Comprehensive Real-World Assessment of Audio Watermarking Algorithms: Will They Survive Neural Codecs?", "categories": ["cs.SD", "cs.AI", "cs.CR", "cs.LG", "eess.AS"], "comment": "5 pages; 5 tables; accepted at INTERSPEECH 2025", "summary": "We present a framework to foster the evaluation of deep learning-based audio\nwatermarking algorithms, establishing a standardized benchmark and allowing\nsystematic comparisons. To simulate real-world usage, we introduce a\ncomprehensive audio attack pipeline, featuring various distortions such as\ncompression, background noise, and reverberation, and propose a diverse test\ndataset, including speech, environmental sounds, and music recordings. By\nassessing the performance of four existing watermarking algorithms on our\nframework, two main insights stand out: (i) neural compression techniques pose\nthe most significant challenge, even when algorithms are trained with such\ncompressions; and (ii) training with audio attacks generally improves\nrobustness, although it is insufficient in some cases. Furthermore, we find\nthat specific distortions, such as polarity inversion, time stretching, or\nreverb, seriously affect certain algorithms. Our contributions strengthen the\nrobustness and perceptual assessment of audio watermarking algorithms across a\nwide range of applications, while ensuring a fair and consistent evaluation\napproach. The evaluation framework, including the attack pipeline, is\naccessible at github.com/SonyResearch/wm_robustness_eval."}
{"id": "2505.19240", "pdf": "https://arxiv.org/pdf/2505.19240", "abs": "https://arxiv.org/abs/2505.19240", "authors": ["Aida Kostikova", "Zhipin Wang", "Deidamea Bajri", "Ole Ptz", "Benjamin Paaen", "Steffen Eger"], "title": "LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This manuscript is currently under review at ACM Computing Surveys", "summary": "Large language model (LLM) research has grown rapidly, along with increasing\nconcern about their limitations such as failures in reasoning, hallucinations,\nand limited multilingual capability. In this survey, we conduct a data-driven,\nsemi-automated review of research on limitations of LLM (LLLMs) from 2022 to\n2024 using a bottom-up approach. From a corpus of 250,000 ACL and arXiv papers,\nwe identify 14,648 relevant papers using keyword filtering, LLM-based\nclassification, validated against expert labels, and topic clustering (via two\napproaches, HDBSCAN+BERTopic and LlooM). We find that LLM-related research\nincreases over fivefold in ACL and fourfold in arXiv. Since 2022, LLLMs\nresearch grows even faster, reaching over 30% of LLM papers by late 2024.\nReasoning remains the most studied limitation, followed by generalization,\nhallucination, bias, and security. The distribution of topics in the ACL\ndataset stays relatively stable over time, while arXiv shifts toward safety and\ncontrollability (with topics like security risks, alignment, hallucinations,\nknowledge editing), and multimodality between 2022 and 2024. We release a\ndataset of annotated abstracts and a validated methodology, and offer a\nquantitative view of trends in LLM limitations research."}
{"id": "2505.19667", "pdf": "https://arxiv.org/pdf/2505.19667", "abs": "https://arxiv.org/abs/2505.19667", "authors": ["Weikang Yuan", "Kaisong Song", "Zhuoren Jiang", "Junjie Cao", "Yujie Zhang", "Jun Lin", "Kun Kuang", "Ji Zhang", "Xiaozhong Liu"], "title": "LeCoDe: A Benchmark Dataset for Interactive Legal Consultation Dialogue Evaluation", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "Legal consultation is essential for safeguarding individual rights and\nensuring access to justice, yet remains costly and inaccessible to many\nindividuals due to the shortage of professionals. While recent advances in\nLarge Language Models (LLMs) offer a promising path toward scalable, low-cost\nlegal assistance, current systems fall short in handling the interactive and\nknowledge-intensive nature of real-world consultations. To address these\nchallenges, we introduce LeCoDe, a real-world multi-turn benchmark dataset\ncomprising 3,696 legal consultation dialogues with 110,008 dialogue turns,\ndesigned to evaluate and improve LLMs' legal consultation capability. With\nLeCoDe, we innovatively collect live-streamed consultations from short-video\nplatforms, providing authentic multi-turn legal consultation dialogues. The\nrigorous annotation by legal experts further enhances the dataset with\nprofessional insights and expertise. Furthermore, we propose a comprehensive\nevaluation framework that assesses LLMs' consultation capabilities in terms of\n(1) clarification capability and (2) professional advice quality. This unified\nframework incorporates 12 metrics across two dimensions. Through extensive\nexperiments on various general and domain-specific LLMs, our results reveal\nsignificant challenges in this task, with even state-of-the-art models like\nGPT-4 achieving only 39.8% recall for clarification and 59% overall score for\nadvice quality, highlighting the complexity of professional consultation\nscenarios. Based on these findings, we further explore several strategies to\nenhance LLMs' legal consultation abilities. Our benchmark contributes to\nadvancing research in legal domain dialogue systems, particularly in simulating\nmore real-world user-expert interactions."}
{"id": "2505.19252", "pdf": "https://arxiv.org/pdf/2505.19252", "abs": "https://arxiv.org/abs/2505.19252", "authors": ["Davin Choo", "Billy Jin", "Yongho Shin"], "title": "Learning-Augmented Online Bipartite Fractional Matching", "categories": ["cs.DS", "cs.AI", "cs.LG"], "comment": null, "summary": "Online bipartite matching is a fundamental problem in online optimization,\nextensively studied both in its integral and fractional forms due to its\ntheoretical significance and practical applications, such as online advertising\nand resource allocation. Motivated by recent progress in learning-augmented\nalgorithms, we study online bipartite fractional matching when the algorithm is\ngiven advice in the form of a suggested matching in each iteration. We develop\nalgorithms for both the vertex-weighted and unweighted variants that provably\ndominate the naive \"coin flip\" strategy of randomly choosing between the\nadvice-following and advice-free algorithms. Moreover, our algorithm for the\nvertex-weighted setting extends to the AdWords problem under the small bids\nassumption, yielding a significant improvement over the seminal work of\nMahdian, Nazerzadeh, and Saberi (EC 2007, TALG 2012). Complementing our\npositive results, we establish a hardness bound on the robustness-consistency\ntradeoff that is attainable by any algorithm. We empirically validate our\nalgorithms through experiments on synthetic and real-world data."}
{"id": "2505.19671", "pdf": "https://arxiv.org/pdf/2505.19671", "abs": "https://arxiv.org/abs/2505.19671", "authors": ["Bowen Zhang", "Nur Afiqah Abdul Latiff", "Justin Kan", "Rong Tong", "Donny Soh", "Xiaoxiao Miao", "Ian McLoughlin"], "title": "Automated evaluation of children's speech fluency for low-resource languages", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "5 pages, 2 figures, conference", "summary": "Assessment of children's speaking fluency in education is well researched for\nmajority languages, but remains highly challenging for low resource languages.\nThis paper proposes a system to automatically assess fluency by combining a\nfine-tuned multilingual ASR model, an objective metrics extraction stage, and a\ngenerative pre-trained transformer (GPT) network. The objective metrics include\nphonetic and word error rates, speech rate, and speech-pause duration ratio.\nThese are interpreted by a GPT-based classifier guided by a small set of\nhuman-evaluated ground truth examples, to score fluency. We evaluate the\nproposed system on a dataset of children's speech in two low-resource\nlanguages, Tamil and Malay and compare the classification performance against\nRandom Forest and XGBoost, as well as using ChatGPT-4o to predict fluency\ndirectly from speech input. Results demonstrate that the proposed approach\nachieves significantly higher accuracy than multimodal GPT or other methods."}
{"id": "2505.19286", "pdf": "https://arxiv.org/pdf/2505.19286", "abs": "https://arxiv.org/abs/2505.19286", "authors": ["Utkarsh Sahu", "Zhisheng Qi", "Yongjia Lei", "Ryan A. Rossi", "Franck Dernoncourt", "Nesreen K. Ahmed", "Mahantesh M Halappanavar", "Yao Ma", "Yu Wang"], "title": "A Graph Perspective to Probe Structural Patterns of Knowledge in Large Language Models", "categories": ["cs.CL", "cs.LG", "cs.SI"], "comment": null, "summary": "Large language models have been extensively studied as neural knowledge bases\nfor their knowledge access, editability, reasoning, and explainability.\nHowever, few works focus on the structural patterns of their knowledge.\nMotivated by this gap, we investigate these structural patterns from a graph\nperspective. We quantify the knowledge of LLMs at both the triplet and entity\nlevels, and analyze how it relates to graph structural properties such as node\ndegree. Furthermore, we uncover the knowledge homophily, where topologically\nclose entities exhibit similar levels of knowledgeability, which further\nmotivates us to develop graph machine learning models to estimate entity\nknowledge based on its local neighbors. This model further enables valuable\nknowledge checking by selecting triplets less known to LLMs. Empirical results\nshow that using selected triplets for fine-tuning leads to superior\nperformance."}
{"id": "2505.19675", "pdf": "https://arxiv.org/pdf/2505.19675", "abs": "https://arxiv.org/abs/2505.19675", "authors": ["Liqin Ye", "Agam Shah", "Chao Zhang", "Sudheer Chava"], "title": "Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The traditional process of creating labeled datasets is labor-intensive and\nexpensive. Recent breakthroughs in open-source large language models (LLMs)\nhave opened up a new avenue in generating labeled datasets automatically for\nvarious natural language processing (NLP) tasks, providing an alternative to\nsuch an expensive annotation process. However, the reliability of such\nauto-generated labels remains a significant concern due to inherent\ninaccuracies. When learning from noisy labels, the model's generalization is\nlikely to be harmed as it is prone to overfit to those label noises. While\nprevious studies in learning from noisy labels mainly focus on synthetic noise\nand real-world noise, LLM-generated label noise receives less attention. In\nthis paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to\ncalibrate the classifier's prediction, thus enhancing its robustness towards\nLLM-generated noisy labels. SiDyP retrieves potential true label candidates by\nneighborhood label distribution in text embedding space and iteratively refines\nnoisy candidates using a simplex diffusion model. Our framework can increase\nthe performance of the BERT classifier fine-tuned on both zero-shot and\nfew-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30%\nrespectively. We demonstrate the effectiveness of SiDyP by conducting extensive\nbenchmarking for different LLMs over a variety of NLP tasks. Our code is\navailable on Github."}
{"id": "2505.19293", "pdf": "https://arxiv.org/pdf/2505.19293", "abs": "https://arxiv.org/abs/2505.19293", "authors": ["Wang Yang", "Hongye Jin", "Shaochen Zhong", "Song Jiang", "Qifan Wang", "Vipin Chaudhary", "Xiaotian Han"], "title": "100-LongBench: Are de facto Long-Context Benchmarks Literally Evaluating Long-Context Ability?", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Long-context capability is considered one of the most important abilities of\nLLMs, as a truly long context-capable LLM enables users to effortlessly process\nmany originally exhausting tasks -- e.g., digesting a long-form document to\nfind answers vs. directly asking an LLM about it. However, existing\nreal-task-based long-context evaluation benchmarks have two major shortcomings.\nFirst, benchmarks like LongBench often do not provide proper metrics to\nseparate long-context performance from the model's baseline ability, making\ncross-model comparison unclear. Second, such benchmarks are usually constructed\nwith fixed input lengths, which limits their applicability across different\nmodels and fails to reveal when a model begins to break down. To address these\nissues, we introduce a length-controllable long-context benchmark and a novel\nmetric that disentangles baseline knowledge from true long-context\ncapabilities. Experiments demonstrate the superiority of our approach in\neffectively evaluating LLMs."}
{"id": "2505.19679", "pdf": "https://arxiv.org/pdf/2505.19679", "abs": "https://arxiv.org/abs/2505.19679", "authors": ["Zhaolin Li", "Yining Liu", "Danni Liu", "Tuan Nam Nguyen", "Enes Yavuz Ugan", "Tu Anh Dinh", "Carlos Mullov", "Alexander Waibel", "Jan Niehues"], "title": "KIT's Low-resource Speech Translation Systems for IWSLT2025: System Enhancement with Synthetic Data and Model Regularization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents KIT's submissions to the IWSLT 2025 low-resource track.\nWe develop both cascaded systems, consisting of Automatic Speech Recognition\n(ASR) and Machine Translation (MT) models, and end-to-end (E2E) Speech\nTranslation (ST) systems for three language pairs: Bemba, North Levantine\nArabic, and Tunisian Arabic into English. Building upon pre-trained models, we\nfine-tune our systems with different strategies to utilize resources\nefficiently. This study further explores system enhancement with synthetic data\nand model regularization. Specifically, we investigate MT-augmented ST by\ngenerating translations from ASR data using MT models. For North Levantine,\nwhich lacks parallel ST training data, a system trained solely on synthetic\ndata slightly surpasses the cascaded system trained on real data. We also\nexplore augmentation using text-to-speech models by generating synthetic speech\nfrom MT data, demonstrating the benefits of synthetic data in improving both\nASR and ST performance for Bemba. Additionally, we apply intra-distillation to\nenhance model performance. Our experiments show that this approach consistently\nimproves results across ASR, MT, and ST tasks, as well as across different\npre-trained models. Finally, we apply Minimum Bayes Risk decoding to combine\nthe cascaded and end-to-end systems, achieving an improvement of approximately\n1.5 BLEU points."}
{"id": "2505.19306", "pdf": "https://arxiv.org/pdf/2505.19306", "abs": "https://arxiv.org/abs/2505.19306", "authors": ["Weiming Zhi", "Ziyong Ma", "Tianyi Zhang", "Matthew Johnson-Roberson"], "title": "From Single Images to Motion Policies via Video-Generation Environment Representations", "categories": ["cs.RO", "cs.CV", "cs.GR", "cs.LG"], "comment": null, "summary": "Autonomous robots typically need to construct representations of their\nsurroundings and adapt their motions to the geometry of their environment.\nHere, we tackle the problem of constructing a policy model for collision-free\nmotion generation, consistent with the environment, from a single input RGB\nimage. Extracting 3D structures from a single image often involves monocular\ndepth estimation. Developments in depth estimation have given rise to large\npre-trained models such as DepthAnything. However, using outputs of these\nmodels for downstream motion generation is challenging due to frustum-shaped\nerrors that arise. Instead, we propose a framework known as Video-Generation\nEnvironment Representation (VGER), which leverages the advances of large-scale\nvideo generation models to generate a moving camera video conditioned on the\ninput image. Frames of this video, which form a multiview dataset, are then\ninput into a pre-trained 3D foundation model to produce a dense point cloud. We\nthen introduce a multi-scale noise approach to train an implicit representation\nof the environment structure and build a motion generation model that complies\nwith the geometry of the representation. We extensively evaluate VGER over a\ndiverse set of indoor and outdoor environments. We demonstrate its ability to\nproduce smooth motions that account for the captured geometry of a scene, all\nfrom a single RGB input image."}
{"id": "2505.19687", "pdf": "https://arxiv.org/pdf/2505.19687", "abs": "https://arxiv.org/abs/2505.19687", "authors": ["Deok-Hyeon Cho", "Hyung-Seok Oh", "Seung-Bin Kim", "Seong-Whan Lee"], "title": "DiEmo-TTS: Disentangled Emotion Representations via Self-Supervised Distillation for Cross-Speaker Emotion Transfer in Text-to-Speech", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Proceedings of Interspeech 2025", "summary": "Cross-speaker emotion transfer in speech synthesis relies on extracting\nspeaker-independent emotion embeddings for accurate emotion modeling without\nretaining speaker traits. However, existing timbre compression methods fail to\nfully separate speaker and emotion characteristics, causing speaker leakage and\ndegraded synthesis quality. To address this, we propose DiEmo-TTS, a\nself-supervised distillation method to minimize emotional information loss and\npreserve speaker identity. We introduce cluster-driven sampling and information\nperturbation to preserve emotion while removing irrelevant factors. To\nfacilitate this process, we propose an emotion clustering and matching approach\nusing emotional attribute prediction and speaker embeddings, enabling\ngeneralization to unlabeled data. Additionally, we designed a dual conditioning\ntransformer to integrate style features better. Experimental results confirm\nthe effectiveness of our method in learning speaker-irrelevant emotion\nembeddings."}
{"id": "2505.19309", "pdf": "https://arxiv.org/pdf/2505.19309", "abs": "https://arxiv.org/abs/2505.19309", "authors": ["Yun Zhao", "Harry Zheng"], "title": "Fractional-Boundary-Regularized Deep Galerkin Method for Variational Inequalities in Mixed Optimal Stopping and Control", "categories": ["math.OC", "cs.LG", "65K15, 68T07, 93E20, 60G40", "I.2.6"], "comment": "16 pages, 5 figures", "summary": "Mixed optimal stopping and stochastic control problems define variational\ninequalities with non-linear Hamilton-Jacobi-Bellman (HJB) operators, whose\nnumerical solution is notoriously difficult and lack of reliable benchmarks. We\nfirst use the dual approach to transform it into a linear operator, and then\nintroduce a Fractional-Boundary-Regularized Deep Galerkin Method (FBR-DGM) that\naugments the classical $L^2$ loss with Sobolev-Slobodeckij norms on the\nparabolic boundary, enforcing regularity and yielding consistent improvements\nin the network approximation and its derivatives. The improved accuracy allows\nthe network to be converted back to the original solution using the dual\ntransform. The self-consistency and stability of the network can be tested by\nchecking the primal-dual relationship among optimal value, optimal wealth, and\noptimal control, offering innovative benchmarks in the absence of analytical\nsolutions."}
{"id": "2505.19693", "pdf": "https://arxiv.org/pdf/2505.19693", "abs": "https://arxiv.org/abs/2505.19693", "authors": ["Deok-Hyeon Cho", "Hyung-Seok Oh", "Seung-Bin Kim", "Seong-Whan Lee"], "title": "EmoSphere-SER: Enhancing Speech Emotion Recognition Through Spherical Representation with Auxiliary Classification", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Proceedings of Interspeech 2025", "summary": "Speech emotion recognition predicts a speaker's emotional state from speech\nsignals using discrete labels or continuous dimensions such as arousal,\nvalence, and dominance (VAD). We propose EmoSphere-SER, a joint model that\nintegrates spherical VAD region classification to guide VAD regression for\nimproved emotion prediction. In our framework, VAD values are transformed into\nspherical coordinates that are divided into multiple spherical regions, and an\nauxiliary classification task predicts which spherical region each point\nbelongs to, guiding the regression process. Additionally, we incorporate a\ndynamic weighting scheme and a style pooling layer with multi-head\nself-attention to capture spectral and temporal dynamics, further boosting\nperformance. This combined training strategy reinforces structured learning and\nimproves prediction consistency. Experimental results show that our approach\nexceeds baseline methods, confirming the validity of the proposed framework."}
{"id": "2505.19315", "pdf": "https://arxiv.org/pdf/2505.19315", "abs": "https://arxiv.org/abs/2505.19315", "authors": ["Farid Najar", "Dominique Barth", "Yann Strozecki"], "title": "Demand Selection for VRP with Emission Quota", "categories": ["cs.DS", "cs.AI", "cs.LG"], "comment": null, "summary": "Combinatorial optimization (CO) problems are traditionally addressed using\nOperations Research (OR) methods, including metaheuristics. In this study, we\nintroduce a demand selection problem for the Vehicle Routing Problem (VRP) with\nan emission quota, referred to as QVRP. The objective is to minimize the number\nof omitted deliveries while respecting the pollution quota. We focus on the\ndemand selection part, called Maximum Feasible Vehicle Assignment (MFVA), while\nthe construction of a routing for the VRP instance is solved using classical OR\nmethods. We propose several methods for selecting the packages to omit, both\nfrom machine learning (ML) and OR. Our results show that, in this static\nproblem setting, classical OR-based methods consistently outperform ML-based\napproaches."}
{"id": "2505.19698", "pdf": "https://arxiv.org/pdf/2505.19698", "abs": "https://arxiv.org/abs/2505.19698", "authors": ["Jing Yu Lim", "Zarif Ikram", "Samson Yu", "Haozhe Ma", "Tze-Yun Leong", "Dianbo Liu"], "title": "JEDI: Latent End-to-end Diffusion Mitigates Agent-Human Performance Asymmetry in Model-Based Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "Preprint", "summary": "Recent advances in model-based reinforcement learning (MBRL) have achieved\nsuper-human level performance on the Atari100k benchmark, driven by\nreinforcement learning agents trained on powerful diffusion world models.\nHowever, we identify that the current aggregates mask a major performance\nasymmetry: MBRL agents dramatically outperform humans in some tasks despite\ndrastically underperforming in others, with the former inflating the aggregate\nmetrics. This is especially pronounced in pixel-based agents trained with\ndiffusion world models. In this work, we address the pronounced asymmetry\nobserved in pixel-based agents as an initial attempt to reverse the worrying\nupward trend observed in them. We address the problematic aggregates by\ndelineating all tasks as Agent-Optimal or Human-Optimal and advocate for equal\nimportance on metrics from both sets. Next, we hypothesize this pronounced\nasymmetry is due to the lack of temporally-structured latent space trained with\nthe World Model objective in pixel-based methods. Lastly, to address this\nissue, we propose Joint Embedding DIffusion (JEDI), a novel latent diffusion\nworld model trained end-to-end with the self-consistency objective. JEDI\noutperforms SOTA models in human-optimal tasks while staying competitive across\nthe Atari100k benchmark, and runs 3 times faster with 43% lower memory than the\nlatest pixel-based diffusion baseline. Overall, our work rethinks what it truly\nmeans to cross human-level performance in Atari100k."}
{"id": "2505.19317", "pdf": "https://arxiv.org/pdf/2505.19317", "abs": "https://arxiv.org/abs/2505.19317", "authors": ["Tin Nguyen", "Jiannan Xu", "Zora Che", "Phuong-Anh Nguyen-Le", "Rushil Dandamudi", "Donald Braman", "Furong Huang", "Hal Daum III", "Zubin Jelveh"], "title": "Effort-aware Fairness: Incorporating a Philosophy-informed, Human-centered Notion of Effort into Algorithmic Fairness Metrics", "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": null, "summary": "Although popularized AI fairness metrics, e.g., demographic parity, have\nuncovered bias in AI-assisted decision-making outcomes, they do not consider\nhow much effort one has spent to get to where one is today in the input feature\nspace. However, the notion of effort is important in how Philosophy and humans\nunderstand fairness. We propose a philosophy-informed way to conceptualize and\nevaluate Effort-aware Fairness (EaF) based on the concept of Force, or temporal\ntrajectory of predictive features coupled with inertia. In addition to our\ntheoretical formulation of EaF metrics, our empirical contributions include: 1/\na pre-registered human subjects experiment, which demonstrates that for both\nstages of the (individual) fairness evaluation process, people consider the\ntemporal trajectory of a predictive feature more than its aggregate value; 2/\npipelines to compute Effort-aware Individual/Group Fairness in the criminal\njustice and personal finance contexts. Our work may enable AI model auditors to\nuncover and potentially correct unfair decisions against individuals who spent\nsignificant efforts to improve but are still stuck with systemic/early-life\ndisadvantages outside their control."}
{"id": "2505.19699", "pdf": "https://arxiv.org/pdf/2505.19699", "abs": "https://arxiv.org/abs/2505.19699", "authors": ["Junming Liu", "Yanting Gao", "Siyuan Meng", "Yifei Sun", "Aoqi Wu", "Yufei Jin", "Yirong Chen", "Ding Wang", "Guosun Zeng"], "title": "Mosaic: Data-Free Knowledge Distillation via Mixture-of-Experts for Heterogeneous Distributed Environments", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "43 pages, 23 figures, 15 tables; the last dance", "summary": "Federated Learning (FL) is a decentralized machine learning paradigm that\nenables clients to collaboratively train models while preserving data privacy.\nHowever, the coexistence of model and data heterogeneity gives rise to\ninconsistent representations and divergent optimization dynamics across\nclients, ultimately hindering robust global performance. To transcend these\nchallenges, we propose Mosaic, a novel data-free knowledge distillation\nframework tailored for heterogeneous distributed environments. Mosaic first\ntrains local generative models to approximate each client's personalized\ndistribution, enabling synthetic data generation that safeguards privacy\nthrough strict separation from real data. Subsequently, Mosaic forms a\nMixture-of-Experts (MoE) from client models based on their specialized\nknowledge, and distills it into a global model using the generated data. To\nfurther enhance the MoE architecture, Mosaic integrates expert predictions via\na lightweight meta model trained on a few representative prototypes. Extensive\nexperiments on standard image classification benchmarks demonstrate that Mosaic\nconsistently outperforms state-of-the-art approaches under both model and data\nheterogeneity. The source code has been published at\nhttps://github.com/Wings-Of-Disaster/Mosaic."}
{"id": "2505.19320", "pdf": "https://arxiv.org/pdf/2505.19320", "abs": "https://arxiv.org/abs/2505.19320", "authors": ["Michail Spitieris", "Massimiliano Ruocco", "Abdulmajid Murad", "Alessandro Nocente"], "title": "PIGPVAE: Physics-Informed Gaussian Process Variational Autoencoders", "categories": ["stat.ML", "cs.LG"], "comment": "23 pages, 13 figures", "summary": "Recent advances in generative AI offer promising solutions for synthetic data\ngeneration but often rely on large datasets for effective training. To address\nthis limitation, we propose a novel generative model that learns from limited\ndata by incorporating physical constraints to enhance performance.\nSpecifically, we extend the VAE architecture by incorporating physical models\nin the generative process, enabling it to capture underlying dynamics more\neffectively. While physical models provide valuable insights, they struggle to\ncapture complex temporal dependencies present in real-world data. To bridge\nthis gap, we introduce a discrepancy term to account for unmodeled dynamics,\nrepresented within a latent Gaussian Process VAE (GPVAE). Furthermore, we apply\nregularization to ensure the generated data aligns closely with observed data,\nenhancing both the diversity and accuracy of the synthetic samples. The\nproposed method is applied to indoor temperature data, achieving\nstate-of-the-art performance. Additionally, we demonstrate that PIGPVAE can\nproduce realistic samples beyond the observed distribution, highlighting its\nrobustness and usefulness under distribution shifts."}
{"id": "2505.19700", "pdf": "https://arxiv.org/pdf/2505.19700", "abs": "https://arxiv.org/abs/2505.19700", "authors": ["Yi Liu", "Dianqing Liu", "Mingye Zhu", "Junbo Guo", "Yongdong Zhang", "Zhendong Mao"], "title": "Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The widespread adoption of large language models (LLMs) across industries has\nincreased the demand for high-quality and customizable outputs. However,\ntraditional alignment methods often require retraining large pretrained models,\nmaking it difficult to quickly adapt and optimize LLMs for diverse\napplications. To address this limitation, we propose a novel \\textit{Residual\nAlignment Model} (\\textit{RAM}) that formalizes the alignment process as a type\nof importance sampling. In this framework, the unaligned upstream model serves\nas the proposal distribution, while the alignment process is framed as\nsecondary sampling based on an autoregressive alignment module that acts as an\nestimator of the importance weights. This design enables a natural detachment\nof the alignment module from the target aligned model, improving flexibility\nand scalability. Based on this model, we derive an efficient sequence-level\ntraining strategy for the alignment module, which operates independently of the\nproposal module. Additionally, we develop a resampling algorithm with iterative\ntoken-level decoding to address the common first-token latency issue in\ncomparable methods. Experimental evaluations on two leading open-source LLMs\nacross diverse tasks, including instruction following, domain adaptation, and\npreference optimization, demonstrate that our approach consistently outperforms\nbaseline models."}
{"id": "2505.19328", "pdf": "https://arxiv.org/pdf/2505.19328", "abs": "https://arxiv.org/abs/2505.19328", "authors": ["Manuela Gonzlez-Gonzlez", "Soufiane Belharbi", "Muhammad Osama Zeeshan", "Masoumeh Sharafi", "Muhammad Haseeb Aslam", "Marco Pedersoli", "Alessandro Lameiras Koerich", "Simon L Bacon", "Eric Granger"], "title": "BAH Dataset for Ambivalence/Hesitancy Recognition in Videos for Behavioural Change", "categories": ["cs.CV", "cs.LG"], "comment": "41 pages, 13 figures, under review", "summary": "Recognizing complex emotions linked to ambivalence and hesitancy (A/H) can\nplay a critical role in the personalization and effectiveness of digital\nbehaviour change interventions. These subtle and conflicting emotions are\nmanifested by a discord between multiple modalities, such as facial and vocal\nexpressions, and body language. Although experts can be trained to identify\nA/H, integrating them into digital interventions is costly and less effective.\nAutomatic learning systems provide a cost-effective alternative that can adapt\nto individual users, and operate seamlessly within real-time, and\nresource-limited environments. However, there are currently no datasets\navailable for the design of ML models to recognize A/H. This paper introduces a\nfirst Behavioural Ambivalence/Hesitancy (BAH) dataset collected for\nsubject-based multimodal recognition of A/H in videos. It contains videos from\n224 participants captured across 9 provinces in Canada, with different age, and\nethnicity. Through our web platform, we recruited participants to answer 7\nquestions, some of which were designed to elicit A/H while recording themselves\nvia webcam with microphone. BAH amounts to 1,118 videos for a total duration of\n8.26 hours with 1.5 hours of A/H. Our behavioural team annotated timestamp\nsegments to indicate where A/H occurs, and provide frame- and video-level\nannotations with the A/H cues. Video transcripts and their timestamps are also\nincluded, along with cropped and aligned faces in each frame, and a variety of\nparticipants meta-data. We include results baselines for BAH at frame- and\nvideo-level recognition in multi-modal setups, in addition to zero-shot\nprediction, and for personalization using unsupervised domain adaptation. The\nlimited performance of baseline models highlights the challenges of recognizing\nA/H in real-world videos. The data, code, and pretrained weights are available."}
{"id": "2505.19706", "pdf": "https://arxiv.org/pdf/2505.19706", "abs": "https://arxiv.org/abs/2505.19706", "authors": ["Tej Deep Pala", "Panshul Sharma", "Amir Zadeh", "Chuan Li", "Soujanya Poria"], "title": "Error Typing for Smarter Rewards: Improving Process Reward Models with Error-Aware Hierarchical Supervision", "categories": ["cs.CL", "cs.AI"], "comment": "https://github.com/declare-lab/PathFinder-PRM", "summary": "Large Language Models (LLMs) are prone to hallucination, especially during\nmulti-hop and reasoning-intensive tasks such as mathematical problem solving.\nWhile Outcome Reward Models verify only final answers, Process Reward Models\n(PRMs) score each intermediate step to steer generation toward coherent\nsolutions. We introduce PathFinder-PRM, a novel hierarchical, error-aware\ndiscriminative PRM that first classifies math and consistency errors at each\nstep, then combines these fine-grained signals to estimate step correctness. To\ntrain PathFinder-PRM, we construct a 400K-sample dataset by enriching the\nhuman-annotated PRM800K corpus and RLHFlow Mistral traces with\nthree-dimensional step-level labels. On PRMBench, PathFinder-PRM achieves a new\nstate-of-the-art PRMScore of 67.7, outperforming the prior best (65.5) while\nusing 3 times less data. When applied to reward guided greedy search, our model\nyields prm@8 48.3, a +1.5 point gain over the strongest baseline. These results\ndemonstrate that decoupled error detection and reward estimation not only boost\nfine-grained error detection but also substantially improve end-to-end,\nreward-guided mathematical reasoning with greater data efficiency."}
{"id": "2505.19350", "pdf": "https://arxiv.org/pdf/2505.19350", "abs": "https://arxiv.org/abs/2505.19350", "authors": ["Filippo Bigi", "Sanggyu Chong", "Agustinus Kristiadi", "Michele Ceriotti"], "title": "FlashMD: long-stride, universal prediction of molecular dynamics", "categories": ["physics.chem-ph", "cs.LG"], "comment": null, "summary": "Molecular dynamics (MD) provides insights into atomic-scale processes by\nintegrating over time the equations that describe the motion of atoms under the\naction of interatomic forces. Machine learning models have substantially\naccelerated MD by providing inexpensive predictions of the forces, but they\nremain constrained to minuscule time integration steps, which are required by\nthe fast time scale of atomic motion. In this work, we propose FlashMD, a\nmethod to predict the evolution of positions and momenta over strides that are\nbetween one and two orders of magnitude longer than typical MD time steps. We\nincorporate considerations on the mathematical and physical properties of\nHamiltonian dynamics in the architecture, generalize the approach to allow the\nsimulation of any thermodynamic ensemble, and carefully assess the possible\nfailure modes of such a long-stride MD approach. We validate FlashMD's accuracy\nin reproducing equilibrium and time-dependent properties, using both\nsystem-specific and general-purpose models, extending the ability of MD\nsimulation to reach the long time scales needed to model microscopic processes\nof high scientific and technological relevance."}
{"id": "2505.19714", "pdf": "https://arxiv.org/pdf/2505.19714", "abs": "https://arxiv.org/abs/2505.19714", "authors": ["Zhaopeng Feng", "Yupu Liang", "Shaosheng Cao", "Jiayuan Su", "Jiahan Ren", "Zhe Xu", "Yao Hu", "Wenxuan Huang", "Jian Wu", "Zuozhu Liu"], "title": "MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via Multi-Task Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in progress", "summary": "Text Image Machine Translation (TIMT)-the task of translating textual content\nembedded in images-is critical for applications in accessibility, cross-lingual\ninformation access, and real-world document understanding. However, TIMT\nremains a complex challenge due to the need for accurate optical character\nrecognition (OCR), robust visual-text reasoning, and high-quality translation,\noften requiring cascading multi-stage pipelines. Recent advances in large-scale\nReinforcement Learning (RL) have improved reasoning in Large Language Models\n(LLMs) and Multimodal LLMs (MLLMs), but their application to end-to-end TIMT is\nstill underexplored. To bridge this gap, we introduce MT$^{3}$, the first\nframework to apply Multi-Task RL to MLLMs for end-to-end TIMT. MT$^{3}$ adopts\na multi-task optimization paradigm targeting three key sub-skills: text\nrecognition, context-aware reasoning, and translation. It is trained using a\nnovel multi-mixed reward mechanism that adapts rule-based RL strategies to\nTIMT's intricacies, offering fine-grained, non-binary feedback across tasks.\nFurthermore, to facilitate the evaluation of TIMT in authentic cross-cultural\nand real-world social media contexts, we introduced XHSPost, the first social\nmedia TIMT benchmark. Our MT$^{3}$-7B-Zero achieves state-of-the-art results on\nthe latest in-domain MIT-10M benchmark, outperforming strong baselines such as\nQwen2.5-VL-72B and InternVL2.5-78B by notable margins across multiple metrics.\nAdditionally, the model shows strong generalization to out-of-distribution\nlanguage pairs and datasets. In-depth analyses reveal how multi-task synergy,\nreinforcement learning initialization, curriculum design, and reward\nformulation contribute to advancing MLLM-driven TIMT."}
{"id": "2505.19356", "pdf": "https://arxiv.org/pdf/2505.19356", "abs": "https://arxiv.org/abs/2505.19356", "authors": ["Kidist Amde Mekonnen", "Yosef Worku Alemneh", "Maarten de Rijke"], "title": "Optimized Text Embedding Models and Benchmarks for Amharic Passage Retrieval", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "68T50 (Primary), 68T05 (Secondary)", "H.3.3; H.3.1; I.2.7"], "comment": "10 pages (excluding references and appendix), 10 figures. Accepted to\n  ACL 2025 Findings. Public release includes dataset, code, and trained models:\n  https://github.com/kidist-amde/amharic-ir-benchmarks", "summary": "Neural retrieval methods using transformer-based pre-trained language models\nhave advanced multilingual and cross-lingual retrieval. However, their\neffectiveness for low-resource, morphologically rich languages such as Amharic\nremains underexplored due to data scarcity and suboptimal tokenization. We\naddress this gap by introducing Amharic-specific dense retrieval models based\non pre-trained Amharic BERT and RoBERTa backbones. Our proposed\nRoBERTa-Base-Amharic-Embed model (110M parameters) achieves a 17.6% relative\nimprovement in MRR@10 and a 9.86% gain in Recall@10 over the strongest\nmultilingual baseline, Arctic Embed 2.0 (568M parameters). More compact\nvariants, such as RoBERTa-Medium-Amharic-Embed (42M), remain competitive while\nbeing over 13x smaller. Additionally, we train a ColBERT-based late interaction\nretrieval model that achieves the highest MRR@10 score (0.843) among all\nevaluated models. We benchmark our proposed models against both sparse and\ndense retrieval baselines to systematically assess retrieval effectiveness in\nAmharic. Our analysis highlights key challenges in low-resource settings and\nunderscores the importance of language-specific adaptation. To foster future\nresearch in low-resource IR, we publicly release our dataset, codebase, and\ntrained models at https://github.com/kidist-amde/amharic-ir-benchmarks."}
{"id": "2505.19715", "pdf": "https://arxiv.org/pdf/2505.19715", "abs": "https://arxiv.org/abs/2505.19715", "authors": ["Chunyang Jiang", "Chi-min Chan", "Yiyang Cai", "Yulong Liu", "Wei Xue", "Yike Guo"], "title": "Graceful Forgetting in Generative Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "8 pages, 6 figures", "summary": "Recently, the pretrain-finetune paradigm has become a cornerstone in various\ndeep learning areas. While in general the pre-trained model would promote both\neffectiveness and efficiency of downstream tasks fine-tuning, studies have\nshown that not all knowledge acquired during pre-training is beneficial. Some\nof the knowledge may actually bring detrimental effects to the fine-tuning\ntasks, which is also known as negative transfer. To address this problem,\ngraceful forgetting has emerged as a promising approach. The core principle of\ngraceful forgetting is to enhance the learning plasticity of the target task by\nselectively discarding irrelevant knowledge. However, this approach remains\nunderexplored in the context of generative language models, and it is often\nchallenging to migrate existing forgetting algorithms to these models due to\narchitecture incompatibility. To bridge this gap, in this paper we propose a\nnovel framework, Learning With Forgetting (LWF), to achieve graceful forgetting\nin generative language models. With Fisher Information Matrix weighting the\nintended parameter updates, LWF computes forgetting confidence to evaluate\nself-generated knowledge regarding the forgetting task, and consequently,\nknowledge with high confidence is periodically unlearned during fine-tuning.\nOur experiments demonstrate that, although thoroughly uncovering the mechanisms\nof knowledge interaction remains challenging in pre-trained language models,\napplying graceful forgetting can contribute to enhanced fine-tuning\nperformance."}
{"id": "2505.19361", "pdf": "https://arxiv.org/pdf/2505.19361", "abs": "https://arxiv.org/abs/2505.19361", "authors": ["Mario Leiva", "Noel Ngu", "Joshua Shay Kricheli", "Aditya Taparia", "Ransalu Senanayake", "Paulo Shakarian", "Nathaniel Bastian", "John Corcoran", "Gerardo Simari"], "title": "Consistency-based Abductive Reasoning over Perceptual Errors of Multiple Pre-trained Models in Novel Environments", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.LO"], "comment": null, "summary": "The deployment of pre-trained perception models in novel environments often\nleads to performance degradation due to distributional shifts. Although recent\nartificial intelligence approaches for metacognition use logical rules to\ncharacterize and filter model errors, improving precision often comes at the\ncost of reduced recall. This paper addresses the hypothesis that leveraging\nmultiple pre-trained models can mitigate this recall reduction. We formulate\nthe challenge of identifying and managing conflicting predictions from various\nmodels as a consistency-based abduction problem. The input predictions and the\nlearned error detection rules derived from each model are encoded in a logic\nprogram. We then seek an abductive explanation--a subset of model\npredictions--that maximizes prediction coverage while ensuring the rate of\nlogical inconsistencies (derived from domain constraints) remains below a\nspecified threshold. We propose two algorithms for this knowledge\nrepresentation task: an exact method based on Integer Programming (IP) and an\nefficient Heuristic Search (HS). Through extensive experiments on a simulated\naerial imagery dataset featuring controlled, complex distributional shifts, we\ndemonstrate that our abduction-based framework outperforms individual models\nand standard ensemble baselines, achieving, for instance, average relative\nimprovements of approximately 13.6% in F1-score and 16.6% in accuracy across 15\ndiverse test datasets when compared to the best individual model. Our results\nvalidate the use of consistency-based abduction as an effective mechanism to\nrobustly integrate knowledge from multiple imperfect reasoners in challenging,\nnovel scenarios."}
{"id": "2505.19719", "pdf": "https://arxiv.org/pdf/2505.19719", "abs": "https://arxiv.org/abs/2505.19719", "authors": ["Juntong Wang", "Xiyuan Wang", "Muhan Zhang"], "title": "OCN: Effectively Utilizing Higher-Order Common Neighbors for Better Link Prediction", "categories": ["cs.LG", "cs.AI"], "comment": "35 pages, 10 figures", "summary": "Common Neighbors (CNs) and their higher-order variants are important pairwise\nfeatures widely used in state-of-the-art link prediction methods. However,\nexisting methods often struggle with the repetition across different orders of\nCNs and fail to fully leverage their potential. We identify that these\nlimitations stem from two key issues: redundancy and over-smoothing in\nhigh-order common neighbors. To address these challenges, we design\northogonalization to eliminate redundancy between different-order CNs and\nnormalization to mitigate over-smoothing. By combining these two techniques, we\npropose Orthogonal Common Neighbor (OCN), a novel approach that significantly\noutperforms the strongest baselines by an average of 7.7% on popular link\nprediction benchmarks. A thorough theoretical analysis is provided to support\nour method. Ablation studies also verify the effectiveness of our\northogonalization and normalization techniques."}
{"id": "2505.19367", "pdf": "https://arxiv.org/pdf/2505.19367", "abs": "https://arxiv.org/abs/2505.19367", "authors": ["Iskander Azangulov", "Peter Potaptchik", "Qinyu Li", "Eddie Aamari", "George Deligiannidis", "Judith Rousseau"], "title": "Adaptive Diffusion Guidance via Stochastic Optimal Control", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Guidance is a cornerstone of modern diffusion models, playing a pivotal role\nin conditional generation and enhancing the quality of unconditional samples.\nHowever, current approaches to guidance scheduling--determining the appropriate\nguidance weight--are largely heuristic and lack a solid theoretical foundation.\nThis work addresses these limitations on two fronts. First, we provide a\ntheoretical formalization that precisely characterizes the relationship between\nguidance strength and classifier confidence. Second, building on this insight,\nwe introduce a stochastic optimal control framework that casts guidance\nscheduling as an adaptive optimization problem. In this formulation, guidance\nstrength is not fixed but dynamically selected based on time, the current\nsample, and the conditioning class, either independently or in combination. By\nsolving the resulting control problem, we establish a principled foundation for\nmore effective guidance in diffusion models."}
{"id": "2505.19722", "pdf": "https://arxiv.org/pdf/2505.19722", "abs": "https://arxiv.org/abs/2505.19722", "authors": ["Yihao Ai", "Zhiyuan Ning", "Weiwei Dai", "Pengfei Wang", "Yi Du", "Wenjuan Cui", "Kunpeng Liu", "Yuanchun Zhou"], "title": "Distilling Closed-Source LLM's Knowledge for Locally Stable and Economic Biomedical Entity Linking", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICIC 2025", "summary": "Biomedical entity linking aims to map nonstandard entities to standard\nentities in a knowledge base. Traditional supervised methods perform well but\nrequire extensive annotated data to transfer, limiting their usage in\nlow-resource scenarios. Large language models (LLMs), especially closed-source\nLLMs, can address these but risk stability issues and high economic costs:\nusing these models is restricted by commercial companies and brings significant\neconomic costs when dealing with large amounts of data. To address this, we\npropose ``RPDR'', a framework combining closed-source LLMs and open-source LLMs\nfor re-ranking candidates retrieved by a retriever fine-tuned with a small\namount of data. By prompting a closed-source LLM to generate training data from\nunannotated data and fine-tuning an open-source LLM for re-ranking, we\neffectively distill the knowledge to the open-source LLM that can be deployed\nlocally, thus avoiding the stability issues and the problem of high economic\ncosts. We evaluate RPDR on two datasets, including one real-world dataset and\none publicly available dataset involving two languages: Chinese and English.\nRPDR achieves 0.019 Acc@1 improvement and 0.036 Acc@1 improvement on the Aier\ndataset and the Ask A Patient dataset when the amount of training data is not\nenough. The results demonstrate the superiority and generalizability of the\nproposed framework."}
{"id": "2505.19371", "pdf": "https://arxiv.org/pdf/2505.19371", "abs": "https://arxiv.org/abs/2505.19371", "authors": ["Georgy Noarov", "Soham Mallick", "Tao Wang", "Sunay Joshi", "Yan Sun", "Yangxinyu Xie", "Mengxin Yu", "Edgar Dobriban"], "title": "Foundations of Top-$k$ Decoding For Language Models", "categories": ["cs.AI", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "Top-$k$ decoding is a widely used method for sampling from LLMs: at each\ntoken, only the largest $k$ next-token-probabilities are kept, and the next\ntoken is sampled after re-normalizing them to sum to unity. Top-$k$ and other\nsampling methods are motivated by the intuition that true next-token\ndistributions are sparse, and the noisy LLM probabilities need to be truncated.\nHowever, to our knowledge, a precise theoretical motivation for the use of\ntop-$k$ decoding is missing. In this work, we develop a theoretical framework\nthat both explains and generalizes top-$k$ decoding. We view decoding at a\nfixed token as the recovery of a sparse probability distribution. We consider\n\\emph{Bregman decoders} obtained by minimizing a separable Bregman divergence\n(for both the \\emph{primal} and \\emph{dual} cases) with a sparsity-inducing\n$\\ell_0$ regularization. Despite the combinatorial nature of the objective, we\nshow how to optimize it efficiently for a large class of divergences. We show\nthat the optimal decoding strategies are greedy, and further that the loss\nfunction is discretely convex in $k$, so that binary search provably and\nefficiently finds the optimal $k$. We show that top-$k$ decoding arises as a\nspecial case for the KL divergence, and identify new decoding strategies that\nhave distinct behaviors (e.g., non-linearly up-weighting larger probabilities\nafter re-normalization)."}
{"id": "2505.19752", "pdf": "https://arxiv.org/pdf/2505.19752", "abs": "https://arxiv.org/abs/2505.19752", "authors": ["Hengli Li", "Yuxuan Wang", "Song-Chun Zhu", "Ying Nian Wu", "Zilong Zheng"], "title": "Discrete Markov Bridge", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Discrete diffusion has recently emerged as a promising paradigm in discrete\ndata modeling. However, existing methods typically rely on a fixed rate\ntransition matrix during training, which not only limits the expressiveness of\nlatent representations, a fundamental strength of variational methods, but also\nconstrains the overall design space. To address these limitations, we propose\nDiscrete Markov Bridge, a novel framework specifically designed for discrete\nrepresentation learning. Our approach is built upon two key components: Matrix\nLearning and Score Learning. We conduct a rigorous theoretical analysis,\nestablishing formal performance guarantees for Matrix Learning and proving the\nconvergence of the overall framework. Furthermore, we analyze the space\ncomplexity of our method, addressing practical constraints identified in prior\nstudies. Extensive empirical evaluations validate the effectiveness of the\nproposed Discrete Markov Bridge, which achieves an Evidence Lower Bound (ELBO)\nof 1.38 on the Text8 dataset, outperforming established baselines. Moreover,\nthe proposed model demonstrates competitive performance on the CIFAR-10\ndataset, achieving results comparable to those obtained by image-specific\ngeneration approaches."}
{"id": "2505.19396", "pdf": "https://arxiv.org/pdf/2505.19396", "abs": "https://arxiv.org/abs/2505.19396", "authors": ["Futoshi Futami", "Atsushi Nitanda"], "title": "Uniform convergence of the smooth calibration error and its relationship with functional gradient", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Calibration is a critical requirement for reliable probabilistic prediction,\nespecially in high-risk applications. However, the theoretical understanding of\nwhich learning algorithms can simultaneously achieve high accuracy and good\ncalibration remains limited, and many existing studies provide empirical\nvalidation or a theoretical guarantee in restrictive settings. To address this\nissue, in this work, we focus on the smooth calibration error (CE) and provide\na uniform convergence bound, showing that the smooth CE is bounded by the sum\nof the smooth CE over the training dataset and a generalization gap. We further\nprove that the functional gradient of the loss function can effectively control\nthe training smooth CE. Based on this framework, we analyze three\nrepresentative algorithms: gradient boosting trees, kernel boosting, and\ntwo-layer neural networks. For each, we derive conditions under which both\nclassification and calibration performances are simultaneously guaranteed. Our\nresults offer new theoretical insights and practical guidance for designing\nreliable probabilistic models with provable calibration guarantees."}
{"id": "2505.19754", "pdf": "https://arxiv.org/pdf/2505.19754", "abs": "https://arxiv.org/abs/2505.19754", "authors": ["Ruisheng Cao", "Hanchong Zhang", "Tiancheng Huang", "Zhangyi Kang", "Yuxin Zhang", "Liangtai Sun", "Hanqi Li", "Yuxun Miao", "Shuai Fan", "Lu Chen", "Kai Yu"], "title": "NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "29 pages, 11 figures, 12 tables, accepted to ACL 2025 Long Main", "summary": "The increasing number of academic papers poses significant challenges for\nresearchers to efficiently acquire key details. While retrieval augmented\ngeneration (RAG) shows great promise in large language model (LLM) based\nautomated question answering, previous works often isolate neural and symbolic\nretrieval despite their complementary strengths. Moreover, conventional\nsingle-view chunking neglects the rich structure and layout of PDFs, e.g.,\nsections and tables. In this work, we propose NeuSym-RAG, a hybrid neural\nsymbolic retrieval framework which combines both paradigms in an interactive\nprocess. By leveraging multi-view chunking and schema-based parsing, NeuSym-RAG\norganizes semi-structured PDF content into both the relational database and\nvectorstore, enabling LLM agents to iteratively gather context until sufficient\nto generate answers. Experiments on three full PDF-based QA datasets, including\na self-annotated one AIRQA-REAL, show that NeuSym-RAG stably defeats both the\nvector-based RAG and various structured baselines, highlighting its capacity to\nunify both retrieval schemes and utilize multiple views. Code and data are\npublicly available at https://github.com/X-LANCE/NeuSym-RAG."}
{"id": "2505.19414", "pdf": "https://arxiv.org/pdf/2505.19414", "abs": "https://arxiv.org/abs/2505.19414", "authors": ["Ruihang Wang", "Zhiwei Cao", "Qingang Zhang", "Rui Tan", "Yonggang Wen", "Tommy Leung", "Stuart Kennedy", "Justin Teoh"], "title": "Toward Physics-Informed Machine Learning for Data Center Operations: A Tropical Case Study", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Data centers are the backbone of computing capacity. Operating data centers\nin the tropical regions faces unique challenges due to consistently high\nambient temperature and elevated relative humidity throughout the year. These\nconditions result in increased cooling costs to maintain the reliability of the\ncomputing systems. While existing machine learning-based approaches have\ndemonstrated potential to elevate operations to a more proactive and\nintelligent level, their deployment remains dubious due to concerns about model\nextrapolation capabilities and associated system safety issues. To address\nthese concerns, this article proposes incorporating the physical\ncharacteristics of data centers into traditional data-driven machine learning\nsolutions. We begin by introducing the data center system, including the\nrelevant multiphysics processes and the data-physics availability. Next, we\noutline the associated modeling and optimization problems and propose an\nintegrated, physics-informed machine learning system to address them. Using the\nproposed system, we present relevant applications across varying levels of\noperational intelligence. A case study on an industry-grade tropical data\ncenter is provided to demonstrate the effectiveness of our approach. Finally,\nwe discuss key challenges and highlight potential future directions."}
{"id": "2505.19757", "pdf": "https://arxiv.org/pdf/2505.19757", "abs": "https://arxiv.org/abs/2505.19757", "authors": ["Maria Dziuba", "Valentin Malykh"], "title": "CIDRe: A Reference-Free Multi-Aspect Criterion for Code Comment Quality Measurement", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Effective generation of structured code comments requires robust quality\nmetrics for dataset curation, yet existing approaches (SIDE, MIDQ, STASIS)\nsuffer from limited code-comment analysis. We propose CIDRe, a\nlanguage-agnostic reference-free quality criterion combining four synergistic\naspects: (1) relevance (code-comment semantic alignment), (2) informativeness\n(functional coverage), (3) completeness (presence of all structure sections),\nand (4) description length (detail sufficiency). We validate our criterion on a\nmanually annotated dataset. Experiments demonstrate CIDRe's superiority over\nexisting metrics, achieving improvement in cross-entropy evaluation. When\napplied to filter comments, the models finetuned on CIDRe-filtered data show\nstatistically significant quality gains in GPT-4o-mini assessments."}
{"id": "2505.19425", "pdf": "https://arxiv.org/pdf/2505.19425", "abs": "https://arxiv.org/abs/2505.19425", "authors": ["Yuhao He", "Jinyu Tian", "Haiwei Wu", "Jianqing Li"], "title": "Structure Disruption: Subverting Malicious Diffusion-Based Inpainting via Self-Attention Query Perturbation", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": null, "summary": "The rapid advancement of diffusion models has enhanced their image inpainting\nand editing capabilities but also introduced significant societal risks.\nAdversaries can exploit user images from social media to generate misleading or\nharmful content. While adversarial perturbations can disrupt inpainting, global\nperturbation-based methods fail in mask-guided editing tasks due to spatial\nconstraints. To address these challenges, we propose Structure Disruption\nAttack (SDA), a powerful protection framework for safeguarding sensitive image\nregions against inpainting-based editing. Building upon the contour-focused\nnature of self-attention mechanisms of diffusion models, SDA optimizes\nperturbations by disrupting queries in self-attention during the initial\ndenoising step to destroy the contour generation process. This targeted\ninterference directly disrupts the structural generation capability of\ndiffusion models, effectively preventing them from producing coherent images.\nWe validate our motivation through visualization techniques and extensive\nexperiments on public datasets, demonstrating that SDA achieves\nstate-of-the-art (SOTA) protection performance while maintaining strong\nrobustness."}
{"id": "2505.19764", "pdf": "https://arxiv.org/pdf/2505.19764", "abs": "https://arxiv.org/abs/2505.19764", "authors": ["Patara Trirat", "Wonyong Jeong", "Sung Ju Hwang"], "title": "Agentic Predictor: Performance Prediction for Agentic Workflows via Multi-View Encoding", "categories": ["cs.LG", "cs.AI"], "comment": "Code will be available at\n  https://github.com/DeepAuto-AI/agentic-predictor", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\ndiverse tasks, but optimizing LLM-based agentic systems remains challenging due\nto the vast search space of agent configurations, prompting strategies, and\ncommunication patterns. Existing approaches often rely on heuristic-based\ntuning or exhaustive evaluation, which can be computationally expensive and\nsuboptimal. This paper proposes Agentic Predictor, a lightweight predictor for\nefficient agentic workflow evaluation. Agentic Predictor is equipped with a\nmulti-view workflow encoding technique that leverages multi-view representation\nlearning of agentic systems by incorporating code architecture, textual\nprompts, and interaction graph features. To achieve high predictive accuracy\nwhile significantly reducing the number of required workflow evaluations for\ntraining a predictor, Agentic Predictor employs cross-domain unsupervised\npretraining. By learning to approximate task success rates, Agentic Predictor\nenables fast and accurate selection of optimal agentic workflow configurations\nfor a given task, significantly reducing the need for expensive trial-and-error\nevaluations. Experiments on a carefully curated benchmark spanning three\ndomains show that our predictor outperforms state-of-the-art methods in both\npredictive accuracy and workflow utility, highlighting the potential of\nperformance predictors in streamlining the design of LLM-based agentic\nworkflows."}
{"id": "2505.19426", "pdf": "https://arxiv.org/pdf/2505.19426", "abs": "https://arxiv.org/abs/2505.19426", "authors": ["Wenyang Xiao", "Haoyu Zhao", "Lingxiao Huang"], "title": "The Role of Diversity in In-Context Learning for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "30 pages", "summary": "In-context learning (ICL) is a crucial capability of current large language\nmodels (LLMs), where the selection of examples plays a key role in performance.\nWhile most existing approaches focus on selecting the most similar examples to\nthe query, the impact of diversity in example selection remains underexplored.\nWe systematically investigate the role of diversity in in-context example\nselection through experiments across a range of tasks, from sentiment\nclassification to more challenging math and code problems. Experiments on\nLlama-3.1, Gemma-2, and Mistral-v0.3 families of models show that\ndiversity-aware selection methods improve performance, particularly on complex\ntasks like math and code, and enhance robustness to out-of-distribution\nqueries. To support these findings, we introduce a theoretical framework that\nexplains the benefits of incorporating diversity in in-context example\nselection."}
{"id": "2505.19769", "pdf": "https://arxiv.org/pdf/2505.19769", "abs": "https://arxiv.org/abs/2505.19769", "authors": ["Yuhui Chen", "Haoran Li", "Zhennan Jiang", "Haowei Wen", "Dongbin Zhao"], "title": "TeViR: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Developing scalable and generalizable reward engineering for reinforcement\nlearning (RL) is crucial for creating general-purpose agents, especially in the\nchallenging domain of robotic manipulation. While recent advances in reward\nengineering with Vision-Language Models (VLMs) have shown promise, their sparse\nreward nature significantly limits sample efficiency. This paper introduces\nTeViR, a novel method that leverages a pre-trained text-to-video diffusion\nmodel to generate dense rewards by comparing the predicted image sequence with\ncurrent observations. Experimental results across 11 complex robotic tasks\ndemonstrate that TeViR outperforms traditional methods leveraging sparse\nrewards and other state-of-the-art (SOTA) methods, achieving better sample\nefficiency and performance without ground truth environmental rewards. TeViR's\nability to efficiently guide agents in complex environments highlights its\npotential to advance reinforcement learning applications in robotic\nmanipulation."}
{"id": "2505.19440", "pdf": "https://arxiv.org/pdf/2505.19440", "abs": "https://arxiv.org/abs/2505.19440", "authors": ["Shashata Sawmya", "Micah Adler", "Nir Shavit"], "title": "The Birth of Knowledge: Emergent Features across Time, Space, and Scale in Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper studies the emergence of interpretable categorical features within\nlarge language models (LLMs), analyzing their behavior across training\ncheckpoints (time), transformer layers (space), and varying model sizes\n(scale). Using sparse autoencoders for mechanistic interpretability, we\nidentify when and where specific semantic concepts emerge within neural\nactivations. Results indicate clear temporal and scale-specific thresholds for\nfeature emergence across multiple domains. Notably, spatial analysis reveals\nunexpected semantic reactivation, with early-layer features re-emerging at\nlater layers, challenging standard assumptions about representational dynamics\nin transformer models."}
{"id": "2505.19776", "pdf": "https://arxiv.org/pdf/2505.19776", "abs": "https://arxiv.org/abs/2505.19776", "authors": ["Akram Elbouanani", "Evan Dufraisse", "Adrian Popescu"], "title": "Analyzing Political Bias in LLMs via Target-Oriented Sentiment Classification", "categories": ["cs.CL", "cs.AI"], "comment": "To be published in the Proceedings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL 2025)", "summary": "Political biases encoded by LLMs might have detrimental effects on downstream\napplications. Existing bias analysis methods rely on small-size intermediate\ntasks (questionnaire answering or political content generation) and rely on the\nLLMs themselves for analysis, thus propagating bias. We propose a new approach\nleveraging the observation that LLM sentiment predictions vary with the target\nentity in the same sentence. We define an entropy-based inconsistency metric to\nencode this prediction variability. We insert 1319 demographically and\npolitically diverse politician names in 450 political sentences and predict\ntarget-oriented sentiment using seven models in six widely spoken languages. We\nobserve inconsistencies in all tested combinations and aggregate them in a\nstatistically robust analysis at different granularity levels. We observe\npositive and negative bias toward left and far-right politicians and positive\ncorrelations between politicians with similar alignment. Bias intensity is\nhigher for Western languages than for others. Larger models exhibit stronger\nand more consistent biases and reduce discrepancies between similar languages.\nWe partially mitigate LLM unreliability in target-oriented sentiment\nclassification (TSC) by replacing politician names with fictional but plausible\ncounterparts."}
{"id": "2505.19441", "pdf": "https://arxiv.org/pdf/2505.19441", "abs": "https://arxiv.org/abs/2505.19441", "authors": ["Jing Nathan Yan", "Junxiong Wang", "Jeffrey M. Rzeszotarski", "Allison Koenecke"], "title": "Fairness Practices in Industry: A Case Study in Machine Learning Teams Building Recommender Systems", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "The rapid proliferation of recommender systems necessitates robust fairness\npractices to address inherent biases. Assessing fairness, though, is\nchallenging due to constantly evolving metrics and best practices. This paper\nanalyzes how industry practitioners perceive and incorporate these changing\nfairness standards in their workflows. Through semi-structured interviews with\n11 practitioners from technical teams across a range of large technology\ncompanies, we investigate industry implementations of fairness in\nrecommendation system products. We focus on current debiasing practices,\napplied metrics, collaborative strategies, and integrating academic research\ninto practice. Findings show a preference for multi-dimensional debiasing over\ntraditional demographic methods, and a reliance on intuitive rather than\nacademic metrics. This study also highlights the difficulties in balancing\nfairness with both the practitioner's individual (bottom-up) roles and\norganizational (top-down) workplace constraints, including the interplay with\nlegal and compliance experts. Finally, we offer actionable recommendations for\nthe recommender system community and algorithmic fairness practitioners,\nunderlining the need to refine fairness practices continually."}
{"id": "2505.19785", "pdf": "https://arxiv.org/pdf/2505.19785", "abs": "https://arxiv.org/abs/2505.19785", "authors": ["Qianyi Xu", "Gousia Habib", "Dilruk Perera", "Mengling Feng"], "title": "MedDreamer: Model-Based Reinforcement Learning with Latent Imagination on Complex EHRs for Clinical Decision Support", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Timely and personalized treatment decisions are essential across a wide range\nof healthcare settings where patient responses vary significantly and evolve\nover time. Clinical data used to support these decisions are often irregularly\nsampled, sparse, and noisy. Existing decision support systems commonly rely on\ndiscretization and imputation, which can distort critical temporal dynamics and\ndegrade decision quality. Moreover, they often overlook the clinical\nsignificance of irregular recording frequencies, filtering out patterns in how\nand when data is collected. Reinforcement Learning (RL) is a natural fit for\nclinical decision-making, enabling sequential, long-term optimization in\ndynamic, uncertain environments. However, most existing treatment\nrecommendation systems are model-free and trained solely on offline data,\nmaking them sample-inefficient, sensitive to data quality, and poorly\ngeneralizable across tasks or cohorts. To address these limitations, we propose\nMedDreamer, a two-phase model-based RL framework for personalized treatment\nrecommendation. MedDreamer uses a world model with an Adaptive Feature\nIntegration (AFI) module to effectively model irregular, sparse clinical data.\nThrough latent imagination, it simulates plausible patient trajectories to\nenhance learning, refining its policy using a mix of real and imagined\nexperiences. This enables learning policies that go beyond suboptimal\nhistorical decisions while remaining grounded in clinical data. To our\nknowledge, this is the first application of latent imagination to irregular\nhealthcare data. Evaluations on sepsis and mechanical ventilation (MV)\ntreatment using two large-scale EHR datasets show that MedDreamer outperforms\nboth model-free and model-based baselines in clinical outcomes and off-policy\nmetrics."}
{"id": "2505.19455", "pdf": "https://arxiv.org/pdf/2505.19455", "abs": "https://arxiv.org/abs/2505.19455", "authors": ["Xu Li", "Fan Lyu"], "title": "MM-Prompt: Cross-Modal Prompt Tuning for Continual Visual Question Answering", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Continual Visual Question Answering (CVQA) based on pre-trained models(PTMs)\nhas achieved promising progress by leveraging prompt tuning to enable continual\nmulti-modal learning. However, most existing methods adopt cross-modal prompt\nisolation, constructing visual and textual prompts separately, which\nexacerbates modality imbalance and leads to degraded performance over time. To\ntackle this issue, we propose MM-Prompt, a novel framework incorporating\ncross-modal prompt query and cross-modal prompt recovery. The former enables\nbalanced prompt selection by incorporating cross-modal signals during query\nformation, while the latter promotes joint prompt reconstruction through\niterative cross-modal interactions, guided by an alignment loss to prevent\nrepresentational drift. Extensive experiments show that MM-Prompt surpasses\nprior approaches in accuracy and knowledge retention, while maintaining\nbalanced modality engagement throughout continual learning."}
{"id": "2505.19790", "pdf": "https://arxiv.org/pdf/2505.19790", "abs": "https://arxiv.org/abs/2505.19790", "authors": ["Faruk Alpay"], "title": "Alpay Algebra III: Observer-Coupled Collapse and the Temporal Drift of Identity", "categories": ["math.CT", "cs.AI", "cs.LO", "18C10, 03G30, 68T01, 03B70, 03D80", "F.4.1; I.2.6; I.2.8"], "comment": "22 pages, 0 figures. Third paper in the Alpay Algebra series,\n  following [arXiv:2505.15344] and [arXiv:2505.17480]. Introduces\n  observer-coupled collapse and formalizes temporal identity drift using\n  transfinite {\\phi}-recursion. Entirely symbolic and self-contained, with no\n  reliance on external frameworks. Structured for submission under Math.CT,\n  CS.LO, and CS.AI", "summary": "This paper introduces a formal framework for modeling observer-dependent\ncollapse dynamics and temporal identity drift within artificial and\nmathematical systems, grounded entirely in the symbolic foundations of Alpay\nAlgebra. Building upon the fixed-point emergence structures developed in Alpay\nAlgebra I and II, this third installment formalizes the observer-coupled\n{\\phi}-collapse process through transfinite categorical flows and\ncurvature-driven identity operators. We define a novel temporal drift mechanism\nas a recursive deformation of identity signatures under entangled observer\ninfluence, constructing categorical invariants that evolve across fold\niterations. The proposed system surpasses conventional identity modeling in\nexplainable AI (XAI) by encoding internal transformation history into a\nsymbolic fixed-point structure, offering provable traceability and temporal\ncoherence. Applications range from AI self-awareness architectures to formal\nlogic systems where identity is not static but dynamically induced by\nobservation. The theoretical results also offer a mathematically rigorous basis\nfor future AI systems with stable self-referential behavior, positioning Alpay\nAlgebra as a next-generation symbolic framework bridging category theory,\nidentity logic, and observer dynamics."}
{"id": "2505.19466", "pdf": "https://arxiv.org/pdf/2505.19466", "abs": "https://arxiv.org/abs/2505.19466", "authors": ["Hongyu Liang", "Yuting Zheng", "Yihan Li", "Yiran Zhang", "Shiyu Liang"], "title": "Origin Tracer: A Method for Detecting LoRA Fine-Tuning Origins in LLMs", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "As large language models (LLMs) continue to advance, their deployment often\ninvolves fine-tuning to enhance performance on specific downstream tasks.\nHowever, this customization is sometimes accompanied by misleading claims about\nthe origins, raising significant concerns about transparency and trust within\nthe open-source community. Existing model verification techniques typically\nassess functional, representational, and weight similarities. However, these\napproaches often struggle against obfuscation techniques, such as permutations\nand scaling transformations. To address this limitation, we propose a novel\ndetection method Origin-Tracer that rigorously determines whether a model has\nbeen fine-tuned from a specified base model. This method includes the ability\nto extract the LoRA rank utilized during the fine-tuning process, providing a\nmore robust verification framework. This framework is the first to provide a\nformalized approach specifically aimed at pinpointing the sources of model\nfine-tuning. We empirically validated our method on thirty-one diverse\nopen-source models under conditions that simulate real-world obfuscation\nscenarios. We empirically analyze the effectiveness of our framework and\nfinally, discuss its limitations. The results demonstrate the effectiveness of\nour approach and indicate its potential to establish new benchmarks for model\nverification."}
{"id": "2505.19795", "pdf": "https://arxiv.org/pdf/2505.19795", "abs": "https://arxiv.org/abs/2505.19795", "authors": ["Sajjad Shahabodini", "Mobina Mansoori", "Farnoush Bayatmakou", "Jamshid Abouei", "Konstantinos N. Plataniotis", "Arash Mohammadi"], "title": "The Missing Point in Vision Transformers for Universal Image Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": null, "summary": "Image segmentation remains a challenging task in computer vision, demanding\nrobust mask generation and precise classification. Recent mask-based approaches\nyield high-quality masks by capturing global context. However, accurately\nclassifying these masks, especially in the presence of ambiguous boundaries and\nimbalanced class distributions, remains an open challenge. In this work, we\nintroduce ViT-P, a novel two-stage segmentation framework that decouples mask\ngeneration from classification. The first stage employs a proposal generator to\nproduce class-agnostic mask proposals, while the second stage utilizes a\npoint-based classification model built on the Vision Transformer (ViT) to\nrefine predictions by focusing on mask central points. ViT-P serves as a\npre-training-free adapter, allowing the integration of various pre-trained\nvision transformers without modifying their architecture, ensuring adaptability\nto dense prediction tasks. Furthermore, we demonstrate that coarse and bounding\nbox annotations can effectively enhance classification without requiring\nadditional training on fine annotation datasets, reducing annotation costs\nwhile maintaining strong performance. Extensive experiments across COCO,\nADE20K, and Cityscapes datasets validate the effectiveness of ViT-P, achieving\nstate-of-the-art results with 54.0 PQ on ADE20K panoptic segmentation, 87.4\nmIoU on Cityscapes semantic segmentation, and 63.6 mIoU on ADE20K semantic\nsegmentation. The code and pretrained models are available at:\nhttps://github.com/sajjad-sh33/ViT-P}{https://github.com/sajjad-sh33/ViT-P."}
{"id": "2505.19470", "pdf": "https://arxiv.org/pdf/2505.19470", "abs": "https://arxiv.org/abs/2505.19470", "authors": ["Futoshi Futami", "Masahiro Fujisawa"], "title": "Information-theoretic Generalization Analysis for VQ-VAEs: A Role of Latent Variables", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Latent variables (LVs) play a crucial role in encoder-decoder models by\nenabling effective data compression, prediction, and generation. Although their\ntheoretical properties, such as generalization, have been extensively studied\nin supervised learning, similar analyses for unsupervised models such as\nvariational autoencoders (VAEs) remain insufficiently underexplored. In this\nwork, we extend information-theoretic generalization analysis to\nvector-quantized (VQ) VAEs with discrete latent spaces, introducing a novel\ndata-dependent prior to rigorously analyze the relationship among LVs,\ngeneralization, and data generation. We derive a novel generalization error\nbound of the reconstruction loss of VQ-VAEs, which depends solely on the\ncomplexity of LVs and the encoder, independent of the decoder. Additionally, we\nprovide the upper bound of the 2-Wasserstein distance between the distributions\nof the true data and the generated data, explaining how the regularization of\nthe LVs contributes to the data generation performance."}
{"id": "2505.19809", "pdf": "https://arxiv.org/pdf/2505.19809", "abs": "https://arxiv.org/abs/2505.19809", "authors": ["Daniel Ordoez-Apraez", "Alek Frhlich", "Vladimir Kosti", "Karim Lounici", "Vivien Brandt", "Massimiliano Pontil"], "title": "Equivariant Representation Learning for Symmetry-Aware Inference with Guarantees", "categories": ["cs.LG", "cs.AI", "cs.RO", "43-06", "I.2.6; I.2.9; I.5.1"], "comment": null, "summary": "In many real-world applications of regression, conditional probability\nestimation, and uncertainty quantification, exploiting symmetries rooted in\nphysics or geometry can dramatically improve generalization and sample\nefficiency. While geometric deep learning has made significant empirical\nadvances by incorporating group-theoretic structure, less attention has been\ngiven to statistical learning guarantees. In this paper, we introduce an\nequivariant representation learning framework that simultaneously addresses\nregression, conditional probability estimation, and uncertainty quantification\nwhile providing first-of-its-kind non-asymptotic statistical learning\nguarantees. Grounded in operator and group representation theory, our framework\napproximates the spectral decomposition of the conditional expectation\noperator, building representations that are both equivariant and disentangled\nalong independent symmetry subgroups. Empirical evaluations on synthetic\ndatasets and real-world robotics applications confirm the potential of our\napproach, matching or outperforming existing equivariant baselines in\nregression while additionally providing well-calibrated parametric uncertainty\nestimates."}
{"id": "2505.19479", "pdf": "https://arxiv.org/pdf/2505.19479", "abs": "https://arxiv.org/abs/2505.19479", "authors": ["Lakshmi Aishwarya Malladi", "Navarun Gupta", "Ahmed El-Sayed", "Xingguo Xiong"], "title": "Revolutionizing Wildfire Detection with Convolutional Neural Networks: A VGG16 Model Approach", "categories": ["cs.CV", "cs.LG"], "comment": "Conference at ASEE 2025", "summary": "Over 8,024 wildfire incidents have been documented in 2024 alone, affecting\nthousands of fatalities and significant damage to infrastructure and\necosystems. Wildfires in the United States have inflicted devastating losses.\nWildfires are becoming more frequent and intense, which highlights how urgently\nefficient warning systems are needed to avoid disastrous outcomes. The goal of\nthis study is to enhance the accuracy of wildfire detection by using\nConvolutional Neural Network (CNN) built on the VGG16 architecture. The D-FIRE\ndataset, which includes several kinds of wildfire and non-wildfire images, was\nemployed in the study. Low-resolution images, dataset imbalance, and the\nnecessity for real-time applicability are some of the main challenges. These\nproblems were resolved by enriching the dataset using data augmentation\ntechniques and optimizing the VGG16 model for binary classification. The model\nproduced a low false negative rate, which is essential for reducing unexplored\nfires, despite dataset boundaries. In order to help authorities execute fast\nresponses, this work shows that deep learning models such as VGG16 can offer a\nreliable, automated approach for early wildfire recognition. For the purpose of\nreducing the impact of wildfires, our future work will concentrate on\nconnecting to systems with real-time surveillance networks and enlarging the\ndataset to cover more varied fire situations."}
{"id": "2505.19815", "pdf": "https://arxiv.org/pdf/2505.19815", "abs": "https://arxiv.org/abs/2505.19815", "authors": ["Junnan Liu", "Hongwei Liu", "Linchen Xiao", "Shudong Liu", "Taolin Zhang", "Zihan Ma", "Songyang Zhang", "Kai Chen"], "title": "Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We propose a novel framework for comprehending the reasoning capabilities of\nlarge language models (LLMs) through the perspective of meta-learning. By\nconceptualizing reasoning trajectories as pseudo-gradient descent updates to\nthe LLM's parameters, we identify parallels between LLM reasoning and various\nmeta-learning paradigms. We formalize the training process for reasoning tasks\nas a meta-learning setup, with each question treated as an individual task, and\nreasoning trajectories serving as the inner loop optimization for adapting\nmodel parameters. Once trained on a diverse set of questions, the LLM develops\nfundamental reasoning capabilities that can generalize to previously unseen\nquestions. Extensive empirical evaluations substantiate the strong connection\nbetween LLM reasoning and meta-learning, exploring several issues of\nsignificant interest from a meta-learning standpoint. Our work not only\nenhances the understanding of LLM reasoning but also provides practical\ninsights for improving these models through established meta-learning\ntechniques."}
{"id": "2505.19486", "pdf": "https://arxiv.org/pdf/2505.19486", "abs": "https://arxiv.org/abs/2505.19486", "authors": ["Maonan Wang", "Yirong Chen", "Aoyu Pang", "Yuxin Cai", "Chung Shue Chen", "Yuheng Kan", "Man-On Pun"], "title": "VLMLight: Traffic Signal Control via Vision-Language Meta-Control and Dual-Branch Reasoning", "categories": ["eess.SY", "cs.LG", "cs.MA", "cs.SY"], "comment": "25 pages, 15 figures", "summary": "Traffic signal control (TSC) is a core challenge in urban mobility, where\nreal-time decisions must balance efficiency and safety. Existing methods -\nranging from rule-based heuristics to reinforcement learning (RL) - often\nstruggle to generalize to complex, dynamic, and safety-critical scenarios. We\nintroduce VLMLight, a novel TSC framework that integrates vision-language\nmeta-control with dual-branch reasoning. At the core of VLMLight is the first\nimage-based traffic simulator that enables multi-view visual perception at\nintersections, allowing policies to reason over rich cues such as vehicle type,\nmotion, and spatial density. A large language model (LLM) serves as a\nsafety-prioritized meta-controller, selecting between a fast RL policy for\nroutine traffic and a structured reasoning branch for critical cases. In the\nlatter, multiple LLM agents collaborate to assess traffic phases, prioritize\nemergency vehicles, and verify rule compliance. Experiments show that VLMLight\nreduces waiting times for emergency vehicles by up to 65% over RL-only systems,\nwhile preserving real-time performance in standard conditions with less than 1%\ndegradation. VLMLight offers a scalable, interpretable, and safety-aware\nsolution for next-generation traffic signal control."}
{"id": "2505.19819", "pdf": "https://arxiv.org/pdf/2505.19819", "abs": "https://arxiv.org/abs/2505.19819", "authors": ["Dannong Wang", "Jaisal Patel", "Daochen Zha", "Steve Y. Yang", "Xiao-Yang Liu"], "title": "FinLoRA: Benchmarking LoRA Methods for Fine-Tuning LLMs on Financial Datasets", "categories": ["cs.CE", "cs.AI"], "comment": null, "summary": "Low-rank adaptation (LoRA) methods show great potential for scaling\npre-trained general-purpose Large Language Models (LLMs) to hundreds or\nthousands of use scenarios. However, their efficacy in high-stakes domains like\nfinance is rarely explored, e.g., passing CFA exams and analyzing SEC filings.\nIn this paper, we present the open-source FinLoRA project that benchmarks LoRA\nmethods on both general and highly professional financial tasks. First, we\ncurated 19 datasets covering diverse financial applications; in particular, we\ncreated four novel XBRL analysis datasets based on 150 SEC filings. Second, we\nevaluated five LoRA methods and five base LLMs. Finally, we provide extensive\nexperimental results in terms of accuracy, F1, and BERTScore and report\ncomputational cost in terms of time and GPU memory during fine-tuning and\ninference stages. We find that LoRA methods achieved substantial performance\ngains of 36\\% on average over base models. Our FinLoRA project provides an\naffordable and scalable approach to democratize financial intelligence to the\ngeneral public. Datasets, LoRA adapters, code, and documentation are available\nat https://github.com/Open-Finance-Lab/FinLoRA"}
{"id": "2505.19507", "pdf": "https://arxiv.org/pdf/2505.19507", "abs": "https://arxiv.org/abs/2505.19507", "authors": ["Chenyu Lu", "Shiliang Sun", "Jing Zhao", "Nan Zhang", "Tengfei Song", "Hao Yang"], "title": "Multimodal Machine Translation with Visual Scene Graph Pruning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Multimodal machine translation (MMT) seeks to address the challenges posed by\nlinguistic polysemy and ambiguity in translation tasks by incorporating visual\ninformation. A key bottleneck in current MMT research is the effective\nutilization of visual data. Previous approaches have focused on extracting\nglobal or region-level image features and using attention or gating mechanisms\nfor multimodal information fusion. However, these methods have not adequately\ntackled the issue of visual information redundancy in MMT, nor have they\nproposed effective solutions. In this paper, we introduce a novel\napproach--multimodal machine translation with visual Scene Graph Pruning (PSG),\nwhich leverages language scene graph information to guide the pruning of\nredundant nodes in visual scene graphs, thereby reducing noise in downstream\ntranslation tasks. Through extensive comparative experiments with\nstate-of-the-art methods and ablation studies, we demonstrate the effectiveness\nof the PSG model. Our results also highlight the promising potential of visual\ninformation pruning in advancing the field of MMT."}
{"id": "2505.19823", "pdf": "https://arxiv.org/pdf/2505.19823", "abs": "https://arxiv.org/abs/2505.19823", "authors": ["Pengcheng Sun", "Erwu Liu", "Wei Ni", "Rui Wang", "Yuanzhe Geng", "Lijuan Lai", "Abbas Jamalipour"], "title": "LAPA-based Dynamic Privacy Optimization for Wireless Federated Learning in Heterogeneous Environments", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated Learning (FL) is a distributed machine learning paradigm based on\nprotecting data privacy of devices, which however, can still be broken by\ngradient leakage attack via parameter inversion techniques. Differential\nprivacy (DP) technology reduces the risk of private data leakage by adding\nartificial noise to the gradients, but detrimental to the FL utility at the\nsame time, especially in the scenario where the data is Non-Independent\nIdentically Distributed (Non-IID). Based on the impact of heterogeneous data on\naggregation performance, this paper proposes a Lightweight Adaptive Privacy\nAllocation (LAPA) strategy, which assigns personalized privacy budgets to\ndevices in each aggregation round without transmitting any additional\ninformation beyond gradients, ensuring both privacy protection and aggregation\nefficiency. Furthermore, the Deep Deterministic Policy Gradient (DDPG)\nalgorithm is employed to optimize the transmission power, in order to determine\nthe optimal timing at which the adaptively attenuated artificial noise aligns\nwith the communication noise, enabling an effective balance between DP and\nsystem utility. Finally, a reliable aggregation strategy is designed by\nintegrating communication quality and data distribution characteristics, which\nimproves aggregation performance while preserving privacy. Experimental results\ndemonstrate that the personalized noise allocation and dynamic optimization\nstrategy based on LAPA proposed in this paper enhances convergence performance\nwhile satisfying the privacy requirements of FL."}
{"id": "2505.19514", "pdf": "https://arxiv.org/pdf/2505.19514", "abs": "https://arxiv.org/abs/2505.19514", "authors": ["Yaoning Yu", "Ye Yu", "Kai Wei", "Haojing Luo", "Haohan Wang"], "title": "SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Prompt quality plays a critical role in the performance of large language\nmodels (LLMs), motivating a growing body of work on prompt optimization. Most\nexisting methods optimize prompts over a fixed dataset, assuming static input\ndistributions and offering limited support for iterative improvement. We\nintroduce SIPDO (Self-Improving Prompts through Data-Augmented Optimization), a\nclosed-loop framework for prompt learning that integrates synthetic data\ngeneration into the optimization process. SIPDO couples a synthetic data\ngenerator with a prompt optimizer, where the generator produces new examples\nthat reveal current prompt weaknesses and the optimizer incrementally refines\nthe prompt in response. This feedback-driven loop enables systematic\nimprovement of prompt performance without assuming access to external\nsupervision or new tasks. Experiments across question answering and reasoning\nbenchmarks show that SIPDO outperforms standard prompt tuning methods,\nhighlighting the value of integrating data synthesis into prompt learning\nworkflows."}
{"id": "2505.19825", "pdf": "https://arxiv.org/pdf/2505.19825", "abs": "https://arxiv.org/abs/2505.19825", "authors": ["Tassilo Klein", "Johannes Hoffart"], "title": "Foundation Models for Tabular Data within Systemic Contexts Need Grounding", "categories": ["cs.LG", "cs.AI", "cs.DB"], "comment": null, "summary": "Current research on tabular foundation models often overlooks the\ncomplexities of large-scale, real-world data by treating tables as isolated\nentities and assuming information completeness, thereby neglecting the vital\noperational context. To address this, we introduce the concept of Semantically\nLinked Tables (SLT), recognizing that tables are inherently connected to both\ndeclarative and procedural operational knowledge. We propose Foundation Models\nfor Semantically Linked Tables (FMSLT), which integrate these components to\nground tabular data within its true operational context. This comprehensive\nrepresentation unlocks the full potential of machine learning for complex,\ninterconnected tabular data across diverse domains. Realizing FMSLTs requires\naccess to operational knowledge that is often unavailable in public datasets,\nhighlighting the need for close collaboration between domain experts and\nresearchers. Our work exposes the limitations of current tabular foundation\nmodels and proposes a new direction centered on FMSLTs, aiming to advance\nrobust, context-aware models for structured data."}
{"id": "2505.19521", "pdf": "https://arxiv.org/pdf/2505.19521", "abs": "https://arxiv.org/abs/2505.19521", "authors": ["Dongzhe Zheng", "Wenjie Mei"], "title": "Learning Dynamics under Environmental Constraints via Measurement-Induced Bundle Structures", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "comment": "Accepted by ICML 2025", "summary": "Learning unknown dynamics under environmental (or external) constraints is\nfundamental to many fields (e.g., modern robotics), particularly challenging\nwhen constraint information is only locally available and uncertain. Existing\napproaches requiring global constraints or using probabilistic filtering fail\nto fully exploit the geometric structure inherent in local measurements (by\nusing, e.g., sensors) and constraints. This paper presents a geometric\nframework unifying measurements, constraints, and dynamics learning through a\nfiber bundle structure over the state space. This naturally induced geometric\nstructure enables measurement-aware Control Barrier Functions that adapt to\nlocal sensing (or measurement) conditions. By integrating Neural ODEs, our\nframework learns continuous-time dynamics while preserving geometric\nconstraints, with theoretical guarantees of learning convergence and constraint\nsatisfaction dependent on sensing quality. The geometric framework not only\nenables efficient dynamics learning but also suggests promising directions for\nintegration with reinforcement learning approaches. Extensive simulations\ndemonstrate significant improvements in both learning efficiency and constraint\nsatisfaction over traditional methods, especially under limited and uncertain\nsensing conditions."}
{"id": "2505.19827", "pdf": "https://arxiv.org/pdf/2505.19827", "abs": "https://arxiv.org/abs/2505.19827", "authors": ["Noga Bar", "Mariia Seleznova", "Yotam Alexander", "Gitta Kutyniok", "Raja Giryes"], "title": "Revisiting Glorot Initialization for Long-Range Linear Recurrences", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Proper initialization is critical for Recurrent Neural Networks (RNNs),\nparticularly in long-range reasoning tasks, where repeated application of the\nsame weight matrix can cause vanishing or exploding signals. A common baseline\nfor linear recurrences is Glorot initialization, designed to ensure stable\nsignal propagation--but derived under the infinite-width, fixed-length\nregime--an unrealistic setting for RNNs processing long sequences. In this\nwork, we show that Glorot initialization is in fact unstable: small positive\ndeviations in the spectral radius are amplified through time and cause the\nhidden state to explode. Our theoretical analysis demonstrates that sequences\nof length $t = O(\\sqrt{n})$, where $n$ is the hidden width, are sufficient to\ninduce instability. To address this, we propose a simple, dimension-aware\nrescaling of Glorot that shifts the spectral radius slightly below one,\npreventing rapid signal explosion or decay. These results suggest that standard\ninitialization schemes may break down in the long-sequence regime, motivating a\nseparate line of theory for stable recurrent initialization."}
{"id": "2505.19522", "pdf": "https://arxiv.org/pdf/2505.19522", "abs": "https://arxiv.org/abs/2505.19522", "authors": ["Jiyu Hu", "Haijiang Zeng", "Zhen Tian"], "title": "Applications and Effect Evaluation of Generative Adversarial Networks in Semi-Supervised Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In recent years, image classification, as a core task in computer vision,\nrelies on high-quality labelled data, which restricts the wide application of\ndeep learning models in practical scenarios. To alleviate the problem of\ninsufficient labelled samples, semi-supervised learning has gradually become a\nresearch hotspot. In this paper, we construct a semi-supervised image\nclassification model based on Generative Adversarial Networks (GANs), and\nthrough the introduction of the collaborative training mechanism of generators,\ndiscriminators and classifiers, we achieve the effective use of limited\nlabelled data and a large amount of unlabelled data, improve the quality of\nimage generation and classification accuracy, and provide an effective solution\nfor the task of image recognition in complex environments."}
{"id": "2505.19838", "pdf": "https://arxiv.org/pdf/2505.19838", "abs": "https://arxiv.org/abs/2505.19838", "authors": ["Pascal Wullschleger", "Majid Zarharan", "Donnacha Daly", "Marc Pouly", "Jennifer Foster"], "title": "FoodTaxo: Generating Food Taxonomies with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "To be published in ACL 2025 Industry Track. Paper website:\n  https://foodtaxo.github.io/", "summary": "We investigate the utility of Large Language Models for automated taxonomy\ngeneration and completion specifically applied to taxonomies from the food\ntechnology industry. We explore the extent to which taxonomies can be completed\nfrom a seed taxonomy or generated without a seed from a set of known concepts,\nin an iterative fashion using recent prompting techniques. Experiments on five\ntaxonomies using an open-source LLM (Llama-3), while promising, point to the\ndifficulty of correctly placing inner nodes."}
{"id": "2505.19534", "pdf": "https://arxiv.org/pdf/2505.19534", "abs": "https://arxiv.org/abs/2505.19534", "authors": ["Yongyi Zang", "Jingyi Li", "Qiuqiang Kong"], "title": "Training-Free Multi-Step Audio Source Separation", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "comment": null, "summary": "Audio source separation aims to separate a mixture into target sources.\nPrevious audio source separation systems usually conduct one-step inference,\nwhich does not fully explore the separation ability of models. In this work, we\nreveal that pretrained one-step audio source separation models can be leveraged\nfor multi-step separation without additional training. We propose a simple yet\neffective inference method that iteratively applies separation by optimally\nblending the input mixture with the previous step's separation result. At each\nstep, we determine the optimal blending ratio by maximizing a metric. We prove\nthat our method always yield improvement over one-step inference, provide error\nbounds based on model smoothness and metric robustness, and provide theoretical\nanalysis connecting our method to denoising along linear interpolation paths\nbetween noise and clean distributions, a property we link to denoising\ndiffusion bridge models. Our approach effectively delivers improved separation\nperformance as a \"free lunch\" from existing models. Our empirical results\ndemonstrate that our multi-step separation approach consistently outperforms\none-step inference across both speech enhancement and music source separation\ntasks, and can achieve scaling performance similar to training a larger model,\nusing more data, or in some cases employing a multi-step training objective.\nThese improvements appear not only on the optimization metric during multi-step\ninference, but also extend to nearly all non-optimized metrics (with one\nexception). We also discuss limitations of our approach and directions for\nfuture research."}
{"id": "2505.19842", "pdf": "https://arxiv.org/pdf/2505.19842", "abs": "https://arxiv.org/abs/2505.19842", "authors": ["Shuo Wang", "Yun Cheng", "Qingye Meng", "Olga Saukh", "Jiang Zhang", "Jingfang Fan", "Yuanting Zhang", "Xingyuan Yuan", "Lothar Thiele"], "title": "PCDCNet: A Surrogate Model for Air Quality Forecasting with Physical-Chemical Dynamics and Constraints", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Air quality forecasting (AQF) is critical for public health and environmental\nmanagement, yet remains challenging due to the complex interplay of emissions,\nmeteorology, and chemical transformations. Traditional numerical models, such\nas CMAQ and WRF-Chem, provide physically grounded simulations but are\ncomputationally expensive and rely on uncertain emission inventories. Deep\nlearning models, while computationally efficient, often struggle with\ngeneralization due to their lack of physical constraints. To bridge this gap,\nwe propose PCDCNet, a surrogate model that integrates numerical modeling\nprinciples with deep learning. PCDCNet explicitly incorporates emissions,\nmeteorological influences, and domain-informed constraints to model pollutant\nformation, transport, and dissipation. By combining graph-based spatial\ntransport modeling, recurrent structures for temporal accumulation, and\nrepresentation enhancement for local interactions, PCDCNet achieves\nstate-of-the-art (SOTA) performance in 72-hour station-level PM2.5 and O3\nforecasting while significantly reducing computational costs. Furthermore, our\nmodel is deployed in an online platform, providing free, real-time air quality\nforecasts, demonstrating its scalability and societal impact. By aligning deep\nlearning with physical consistency, PCDCNet offers a practical and\ninterpretable solution for AQF, enabling informed decision-making for both\npersonal and regulatory applications."}
{"id": "2505.19537", "pdf": "https://arxiv.org/pdf/2505.19537", "abs": "https://arxiv.org/abs/2505.19537", "authors": ["Yi Feng", "Kaito Fujii", "Stratis Skoulakis", "Xiao Wang", "Volkan Cevher"], "title": "Continuous-Time Analysis of Heavy Ball Momentum in Min-Max Games", "categories": ["cs.GT", "cs.LG"], "comment": "Accepted for ICML 2025", "summary": "Since Polyak's pioneering work, heavy ball (HB) momentum has been widely\nstudied in minimization. However, its role in min-max games remains largely\nunexplored. As a key component of practical min-max algorithms like Adam, this\ngap limits their effectiveness. In this paper, we present a continuous-time\nanalysis for HB with simultaneous and alternating update schemes in min-max\ngames. Locally, we prove smaller momentum enhances algorithmic stability by\nenabling local convergence across a wider range of step sizes, with alternating\nupdates generally converging faster. Globally, we study the implicit\nregularization of HB, and find smaller momentum guides algorithms trajectories\ntowards shallower slope regions of the loss landscapes, with alternating\nupdates amplifying this effect. Surprisingly, all these phenomena differ from\nthose observed in minimization, where larger momentum yields similar effects.\nOur results reveal fundamental differences between HB in min-max games and\nminimization, and numerical experiments further validate our theoretical\nresults."}
{"id": "2505.19850", "pdf": "https://arxiv.org/pdf/2505.19850", "abs": "https://arxiv.org/abs/2505.19850", "authors": ["Leander Diaz-Bone", "Marco Bagatella", "Jonas Hbotter", "Andreas Krause"], "title": "DISCOVER: Automated Curricula for Sparse-Reward Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Sparse-reward reinforcement learning (RL) can model a wide range of highly\ncomplex tasks. Solving sparse-reward tasks is RL's core premise - requiring\nefficient exploration coupled with long-horizon credit assignment - and\novercoming these challenges is key for building self-improving agents with\nsuperhuman ability. We argue that solving complex and high-dimensional tasks\nrequires solving simpler tasks that are relevant to the target task. In\ncontrast, most prior work designs strategies for selecting exploratory tasks\nwith the objective of solving any task, making exploration of challenging\nhigh-dimensional, long-horizon tasks intractable. We find that the sense of\ndirection, necessary for effective exploration, can be extracted from existing\nRL algorithms, without needing any prior information. Based on this finding, we\npropose a method for directed sparse-reward goal-conditioned very long-horizon\nRL (DISCOVER), which selects exploratory goals in the direction of the target\ntask. We connect DISCOVER to principled exploration in bandits, formally\nbounding the time until the target task becomes achievable in terms of the\nagent's initial distance to the target, but independent of the volume of the\nspace of all tasks. Empirically, we perform a thorough evaluation in\nhigh-dimensional environments. We find that the directed goal selection of\nDISCOVER solves exploration problems that are beyond the reach of prior\nstate-of-the-art exploration methods in RL."}
{"id": "2505.19544", "pdf": "https://arxiv.org/pdf/2505.19544", "abs": "https://arxiv.org/abs/2505.19544", "authors": ["Jialei Chen", "Yuanbo Xu", "Yiheng Jiang"], "title": "Unlocking the Power of Diffusion Models in Sequential Recommendation: A Simple and Effective Approach", "categories": ["cs.IR", "cs.LG"], "comment": null, "summary": "In this paper, we focus on the often-overlooked issue of embedding collapse\nin existing diffusion-based sequential recommendation models and propose ADRec,\nan innovative framework designed to mitigate this problem. Diverging from\nprevious diffusion-based methods, ADRec applies an independent noise process to\neach token and performs diffusion across the entire target sequence during\ntraining. ADRec captures token interdependency through auto-regression while\nmodeling per-token distributions through token-level diffusion. This dual\napproach enables the model to effectively capture both sequence dynamics and\nitem representations, overcoming the limitations of existing methods. To\nfurther mitigate embedding collapse, we propose a three-stage training\nstrategy: (1) pre-training the embedding weights, (2) aligning these weights\nwith the ADRec backbone, and (3) fine-tuning the model. During inference, ADRec\napplies the denoising process only to the last token, ensuring that the\nmeaningful patterns in historical interactions are preserved. Our comprehensive\nempirical evaluation across six datasets underscores the effectiveness of ADRec\nin enhancing both the accuracy and efficiency of diffusion-based sequential\nrecommendation systems."}
{"id": "2505.19851", "pdf": "https://arxiv.org/pdf/2505.19851", "abs": "https://arxiv.org/abs/2505.19851", "authors": ["Gulfarogh Azam", "Mohd Sadique", "Saif Ali", "Mohammad Nadeem", "Erik Cambria", "Shahab Saquib Sohail", "Mohammad Sultan Alam"], "title": "Beyond Specialization: Benchmarking LLMs for Transliteration of Indian Languages", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Transliteration, the process of mapping text from one script to another,\nplays a crucial role in multilingual natural language processing, especially\nwithin linguistically diverse contexts such as India. Despite significant\nadvancements through specialized models like IndicXlit, recent developments in\nlarge language models suggest a potential for general-purpose models to excel\nat this task without explicit task-specific training. The current work\nsystematically evaluates the performance of prominent LLMs, including GPT-4o,\nGPT-4.5, GPT-4.1, Gemma-3-27B-it, and Mistral-Large against IndicXlit, a\nstate-of-the-art transliteration model, across ten major Indian languages.\nExperiments utilized standard benchmarks, including Dakshina and Aksharantar\ndatasets, with performance assessed via Top-1 Accuracy and Character Error\nRate. Our findings reveal that while GPT family models generally outperform\nother LLMs and IndicXlit for most instances. Additionally, fine-tuning GPT-4o\nimproves performance on specific languages notably. An extensive error analysis\nand robustness testing under noisy conditions further elucidate strengths of\nLLMs compared to specialized models, highlighting the efficacy of foundational\nmodels for a wide spectrum of specialized applications with minimal overhead."}
{"id": "2505.19558", "pdf": "https://arxiv.org/pdf/2505.19558", "abs": "https://arxiv.org/abs/2505.19558", "authors": ["Zhaowei Zhang", "Minghua Yi", "Mengmeng Wang", "Fengshuo Bai", "Zilong Zheng", "Yipeng Kang", "Yaodong Yang"], "title": "EuroCon: Benchmarking Parliament Deliberation for Political Consensus Finding", "categories": ["cs.CY", "cs.LG", "K.4.1; K.4.3; I.2.7"], "comment": "EuroCon is publicly available at\n  https://zowiezhang.github.io/projects/EuroCon", "summary": "Achieving political consensus is crucial yet challenging for the effective\nfunctioning of social governance. However, although frontier AI systems\nrepresented by large language models (LLMs) have developed rapidly in recent\nyears, their capabilities on this scope are still understudied. In this paper,\nwe introduce EuroCon, a novel benchmark constructed from 2,225 high-quality\ndeliberation records of the European Parliament over 13 years, ranging from\n2009 to 2022, to evaluate the ability of LLMs to reach political consensus\namong divergent party positions across diverse parliament settings.\nSpecifically, EuroCon incorporates four factors to build each simulated\nparliament setting: specific political issues, political goals, participating\nparties, and power structures based on seat distribution. We also develop an\nevaluation framework for EuroCon to simulate real voting outcomes in different\nparliament settings, assessing whether LLM-generated resolutions meet\npredefined political goals. Our experimental results demonstrate that even\nstate-of-the-art models remain undersatisfied with complex tasks like passing\nresolutions by a two-thirds majority and addressing security issues, while\nrevealing some common strategies LLMs use to find consensus under different\npower structures, such as prioritizing the stance of the dominant party,\nhighlighting EuroCon's promise as an effective platform for studying LLMs'\nability to find political consensus."}
{"id": "2505.19853", "pdf": "https://arxiv.org/pdf/2505.19853", "abs": "https://arxiv.org/abs/2505.19853", "authors": ["Miaoyu Li", "Qin Chao", "Boyang Li"], "title": "Two Causally Related Needles in a Video Haystack", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Evaluating the video understanding capabilities of Video-Language Models\n(VLMs) remains a significant challenge. We propose a long-context video\nunderstanding benchmark, Causal2Needles, that assesses two crucial abilities\ninsufficiently evaluated by existing benchmarks: (1) the ability to extract\ninformation from two separate locations in a long video and understand them\njointly, and (2) the ability to model the world in terms of cause and effect in\nhuman behaviors. Specifically, Causal2Needles introduces 2-needle questions,\nwhich require extracting information from both the cause and effect\nhuman-behavior events in a long video and the associated narration text. To\nprevent textual bias, these questions comprise two complementary formats: one\nasking to identify the video clip containing the answer, and one asking for the\ntextual description of an unrelated visual detail from that video clip. Our\nexperiments reveal that models excelling in pre-existing benchmarks struggle\nwith 2-needle visual grounding, and the model performance is negatively\ncorrelated with the distance between the two needles. These findings highlight\ncritical limitations in current VLMs."}
{"id": "2505.19568", "pdf": "https://arxiv.org/pdf/2505.19568", "abs": "https://arxiv.org/abs/2505.19568", "authors": ["Jiongchao Jin", "Xiuju Fu", "Xiaowei Gao", "Tao Cheng", "Ran Yan"], "title": "MSD-LLM: Predicting Ship Detention in Port State Control Inspections with Large Language Model", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Maritime transportation is the backbone of global trade, making ship\ninspection essential for ensuring maritime safety and environmental protection.\nPort State Control (PSC), conducted by national ports, enforces compliance with\nsafety regulations, with ship detention being the most severe consequence,\nimpacting both ship schedules and company reputations. Traditional machine\nlearning methods for ship detention prediction are limited by the capacity of\nrepresentation learning and thus suffer from low accuracy. Meanwhile,\nautoencoder-based deep learning approaches face challenges due to the severe\ndata imbalance in learning historical PSC detention records. To address these\nlimitations, we propose Maritime Ship Detention with Large Language Models\n(MSD-LLM), integrating a dual robust subspace recovery (DSR) layer-based\nautoencoder with a progressive learning pipeline to handle imbalanced data and\nextract meaningful PSC representations. Then, a large language model groups and\nranks features to identify likely detention cases, enabling dynamic\nthresholding for flexible detention predictions. Extensive evaluations on\n31,707 PSC inspection records from the Asia-Pacific region show that MSD-LLM\noutperforms state-of-the-art methods more than 12\\% on Area Under the Curve\n(AUC) for Singapore ports. Additionally, it demonstrates robustness to\nreal-world challenges, making it adaptable to diverse maritime risk assessment\nscenarios."}
{"id": "2505.19867", "pdf": "https://arxiv.org/pdf/2505.19867", "abs": "https://arxiv.org/abs/2505.19867", "authors": ["Yavar Taheri Yeganeh", "Mohsen Jafari", "Andrea Matta"], "title": "Deep Active Inference Agents for Delayed and Long-Horizon Environments", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "With the recent success of world-model agents, which extend the core idea of\nmodel-based reinforcement learning by learning a differentiable model for\nsample-efficient control across diverse tasks, active inference (AIF) offers a\ncomplementary, neuroscience-grounded paradigm that unifies perception,\nlearning, and action within a single probabilistic framework powered by a\ngenerative model. Despite this promise, practical AIF agents still rely on\naccurate immediate predictions and exhaustive planning, a limitation that is\nexacerbated in delayed environments requiring plans over long horizons, tens to\nhundreds of steps. Moreover, most existing agents are evaluated on robotic or\nvision benchmarks which, while natural for biological agents, fall short of\nreal-world industrial complexity. We address these limitations with a\ngenerative-policy architecture featuring (i) a multi-step latent transition\nthat lets the generative model predict an entire horizon in a single\nlook-ahead, (ii) an integrated policy network that enables the transition and\nreceives gradients of the expected free energy, (iii) an alternating\noptimization scheme that updates model and policy from a replay buffer, and\n(iv) a single gradient step that plans over long horizons, eliminating\nexhaustive planning from the control loop. We evaluate our agent in an\nenvironment that mimics a realistic industrial scenario with delayed and\nlong-horizon settings. The empirical results confirm the effectiveness of the\nproposed approach, demonstrating the coupled world-model with the AIF formalism\nyields an end-to-end probabilistic controller capable of effective decision\nmaking in delayed, long-horizon settings without handcrafted rewards or\nexpensive planning."}
{"id": "2505.19574", "pdf": "https://arxiv.org/pdf/2505.19574", "abs": "https://arxiv.org/abs/2505.19574", "authors": ["Alejandro Murillo-Gonzalez", "Lantao Liu"], "title": "Situationally-Aware Dynamics Learning", "categories": ["cs.RO", "cs.AI", "cs.LG", "math.OC"], "comment": null, "summary": "Autonomous robots operating in complex, unstructured environments face\nsignificant challenges due to latent, unobserved factors that obscure their\nunderstanding of both their internal state and the external world. Addressing\nthis challenge would enable robots to develop a more profound grasp of their\noperational context. To tackle this, we propose a novel framework for online\nlearning of hidden state representations, with which the robots can adapt in\nreal-time to uncertain and dynamic conditions that would otherwise be ambiguous\nand result in suboptimal or erroneous behaviors. Our approach is formalized as\na Generalized Hidden Parameter Markov Decision Process, which explicitly models\nthe influence of unobserved parameters on both transition dynamics and reward\nstructures. Our core innovation lies in learning online the joint distribution\nof state transitions, which serves as an expressive representation of latent\nego- and environmental-factors. This probabilistic approach supports the\nidentification and adaptation to different operational situations, improving\nrobustness and safety. Through a multivariate extension of Bayesian Online\nChangepoint Detection, our method segments changes in the underlying data\ngenerating process governing the robot's dynamics. The robot's transition model\nis then informed with a symbolic representation of the current situation\nderived from the joint distribution of latest state transitions, enabling\nadaptive and context-aware decision-making. To showcase the real-world\neffectiveness, we validate our approach in the challenging task of unstructured\nterrain navigation, where unmodeled and unmeasured terrain characteristics can\nsignificantly impact the robot's motion. Extensive experiments in both\nsimulation and real world reveal significant improvements in data efficiency,\npolicy performance, and the emergence of safer, adaptive navigation strategies."}
{"id": "2505.19874", "pdf": "https://arxiv.org/pdf/2505.19874", "abs": "https://arxiv.org/abs/2505.19874", "authors": ["Yi Wu", "Lingting Zhu", "Shengju Qian", "Lei Liu", "Wandi Qiao", "Lequan Yu", "Bin Li"], "title": "StyleAR: Customizing Multimodal Autoregressive Model for Style-Aligned Text-to-Image Generation", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "In the current research landscape, multimodal autoregressive (AR) models have\nshown exceptional capabilities across various domains, including visual\nunderstanding and generation. However, complex tasks such as style-aligned\ntext-to-image generation present significant challenges, particularly in data\nacquisition. In analogy to instruction-following tuning for image editing of AR\nmodels, style-aligned generation requires a reference style image and prompt,\nresulting in a text-image-to-image triplet where the output shares the style\nand semantics of the input. However, acquiring large volumes of such triplet\ndata with specific styles is considerably more challenging than obtaining\nconventional text-to-image data used for training generative models. To address\nthis issue, we propose StyleAR, an innovative approach that combines a\nspecially designed data curation method with our proposed AR models to\neffectively utilize text-to-image binary data for style-aligned text-to-image\ngeneration. Our method synthesizes target stylized data using a reference style\nimage and prompt, but only incorporates the target stylized image as the image\nmodality to create high-quality binary data. To facilitate binary data\ntraining, we introduce a CLIP image encoder with a perceiver resampler that\ntranslates the image input into style tokens aligned with multimodal tokens in\nAR models and implement a style-enhanced token technique to prevent content\nleakage which is a common issue in previous work. Furthermore, we mix raw\nimages drawn from large-scale text-image datasets with stylized images to\nenhance StyleAR's ability to extract richer stylistic features and ensure style\nconsistency. Extensive qualitative and quantitative experiments demonstrate our\nsuperior performance."}
{"id": "2505.19599", "pdf": "https://arxiv.org/pdf/2505.19599", "abs": "https://arxiv.org/abs/2505.19599", "authors": ["Andrew Gambardella", "Takeshi Kojima", "Yusuke Iwasawa", "Yutaka Matsuo"], "title": "Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese Grammar", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "In Proceedings of the 63rd Annual Meeting of the Association for\n  Computational Linguistics, 2025", "summary": "Typical methods for evaluating the performance of language models evaluate\ntheir ability to answer questions accurately. These evaluation metrics are\nacceptable for determining the extent to which language models can understand\nand reason about text in a general sense, but fail to capture nuanced\ncapabilities, such as the ability of language models to recognize and obey rare\ngrammar points, particularly in languages other than English. We measure the\nperplexity of language models when confronted with the \"first person psych\npredicate restriction\" grammar point in Japanese. Weblab is the only tested\nopen source model in the 7-10B parameter range which consistently assigns\nhigher perplexity to ungrammatical psych predicate sentences than grammatical\nones. We give evidence that Weblab's uniformly bad tokenization is a possible\nroot cause for its good performance, and show that Llama 3's perplexity on\ngrammatical psych predicate sentences can be reduced by orders of magnitude\n(28x difference) by restricting test sentences to those with uniformly\nwell-behaved tokenizations. We show in further experiments on machine\ntranslation tasks that language models will use alternative grammar patterns in\norder to produce grammatical sentences when tokenization issues prevent the\nmost natural sentence from being output."}
{"id": "2505.19887", "pdf": "https://arxiv.org/pdf/2505.19887", "abs": "https://arxiv.org/abs/2505.19887", "authors": ["Anton Tkachenko", "Dmitrij Suskevic", "Benjamin Adolphi"], "title": "Deconstructing Obfuscation: A four-dimensional framework for evaluating Large Language Models assembly code deobfuscation capabilities", "categories": ["cs.SE", "cs.AI", "cs.CR"], "comment": null, "summary": "Large language models (LLMs) have shown promise in software engineering, yet\ntheir effectiveness for binary analysis remains unexplored. We present the\nfirst comprehensive evaluation of commercial LLMs for assembly code\ndeobfuscation. Testing seven state-of-the-art models against four obfuscation\nscenarios (bogus control flow, instruction substitution, control flow\nflattening, and their combination), we found striking performance\nvariations--from autonomous deobfuscation to complete failure. We propose a\ntheoretical framework based on four dimensions: Reasoning Depth, Pattern\nRecognition, Noise Filtering, and Context Integration, explaining these\nvariations. Our analysis identifies five error patterns: predicate\nmisinterpretation, structural mapping errors, control flow misinterpretation,\narithmetic transformation errors, and constant propagation errors, revealing\nfundamental limitations in LLM code processing.We establish a three-tier\nresistance model: bogus control flow (low resistance), control flow flattening\n(moderate resistance), and instruction substitution/combined techniques (high\nresistance). Universal failure against combined techniques demonstrates that\nsophisticated obfuscation remains effective against advanced LLMs. Our findings\nsuggest a human-AI collaboration paradigm where LLMs reduce expertise barriers\nfor certain reverse engineering tasks while requiring human guidance for\ncomplex deobfuscation. This work provides a foundation for evaluating emerging\ncapabilities and developing resistant obfuscation techniques.x deobfuscation.\nThis work provides a foundation for evaluating emerging capabilities and\ndeveloping resistant obfuscation techniques."}
{"id": "2505.19603", "pdf": "https://arxiv.org/pdf/2505.19603", "abs": "https://arxiv.org/abs/2505.19603", "authors": ["Ho Hin Lee", "Quan Liu", "Shunxing Bao", "Yuankai Huo", "Bennett A. Landman"], "title": "Rep3D: Re-parameterize Large 3D Kernels with Low-Rank Receptive Modeling for Medical Imaging", "categories": ["cs.CV", "cs.LG"], "comment": "14 pages", "summary": "In contrast to vision transformers, which model long-range dependencies\nthrough global self-attention, large kernel convolutions provide a more\nefficient and scalable alternative, particularly in high-resolution 3D\nvolumetric settings. However, naively increasing kernel size often leads to\noptimization instability and degradation in performance. Motivated by the\nspatial bias observed in effective receptive fields (ERFs), we hypothesize that\ndifferent kernel elements converge at variable rates during training. To\nsupport this, we derive a theoretical connection between element-wise gradients\nand first-order optimization, showing that structurally re-parameterized\nconvolution blocks inherently induce spatially varying learning rates. Building\non this insight, we introduce Rep3D, a 3D convolutional framework that\nincorporates a learnable spatial prior into large kernel training. A\nlightweight two-stage modulation network generates a receptive-biased scaling\nmask, adaptively re-weighting kernel updates and enabling local-to-global\nconvergence behavior. Rep3D adopts a plain encoder design with large depthwise\nconvolutions, avoiding the architectural complexity of multi-branch\ncompositions. We evaluate Rep3D on five challenging 3D segmentation benchmarks\nand demonstrate consistent improvements over state-of-the-art baselines,\nincluding transformer-based and fixed-prior re-parameterization methods. By\nunifying spatial inductive bias with optimization-aware learning, Rep3D offers\nan interpretable, and scalable solution for 3D medical image analysis. The\nsource code is publicly available at https://github.com/leeh43/Rep3D."}
{"id": "2505.19912", "pdf": "https://arxiv.org/pdf/2505.19912", "abs": "https://arxiv.org/abs/2505.19912", "authors": ["Javier Marn"], "title": "APE: A Data-Centric Benchmark for Efficient LLM Adaptation in Text Summarization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present Adjacent Possible Exploration (APE), a simple yet effective method\nfor adapting large language models to specific tasks using minimal\ncomputational resources. Unlike traditional fine-tuning that requires extensive\ncompute, APE iteratively fine-tunes models on small, carefully selected data\nbatches (200 examples), retaining only improvements. On news summarization, APE\nachieves 40 percent BLEU improvement using just a T4 GPU in 60 minutes,\nmatching or exceeding more complex methods like LoRA while remaining\nconceptually simple. Our approach is particularly valuable for researchers and\npractitioners with limited computational resources. We provide open-source code\nand demonstrate APE's effectiveness through both automatic metrics and human\nevaluation. While inspired by evolutionary theory's \"adjacent possible\", APE's\ncore insight has a very practical application: small, iterative data\nperturbations can efficiently guide LLMs toward task-specific performance\nwithout expensive retraining."}
{"id": "2505.19604", "pdf": "https://arxiv.org/pdf/2505.19604", "abs": "https://arxiv.org/abs/2505.19604", "authors": ["Ahan Prasannakumar Shetty"], "title": "Evaluating Machine Translation Models for English-Hindi Language Pairs: A Comparative Analysis", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Machine translation has become a critical tool in bridging linguistic gaps,\nespecially between languages as diverse as English and Hindi. This paper\ncomprehensively evaluates various machine translation models for translating\nbetween English and Hindi. We assess the performance of these models using a\ndiverse set of automatic evaluation metrics, both lexical and machine\nlearning-based metrics. Our evaluation leverages an 18000+ corpus of English\nHindi parallel dataset and a custom FAQ dataset comprising questions from\ngovernment websites. The study aims to provide insights into the effectiveness\nof different machine translation approaches in handling both general and\nspecialized language domains. Results indicate varying performance levels\nacross different metrics, highlighting strengths and areas for improvement in\ncurrent translation systems."}
{"id": "2505.19914", "pdf": "https://arxiv.org/pdf/2505.19914", "abs": "https://arxiv.org/abs/2505.19914", "authors": ["Jiangjie Chen", "Qianyu He", "Siyu Yuan", "Aili Chen", "Zhicheng Cai", "Weinan Dai", "Hongli Yu", "Qiying Yu", "Xuefeng Li", "Jiaze Chen", "Hao Zhou", "Mingxuan Wang"], "title": "Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at\nadvanced reasoning tasks like math and coding via Reinforcement Learning with\nVerifiable Rewards (RLVR), but still struggle with puzzles solvable by humans\nwithout domain knowledge. We introduce Enigmata, the first comprehensive suite\ntailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks\nacross seven categories, each with 1) a generator that produces unlimited\nexamples with controllable difficulty and 2) a rule-based verifier for\nautomatic evaluation. This generator-verifier design supports scalable,\nmulti-task RL training, fine-grained analysis, and seamless RLVR integration.\nWe further propose Enigmata-Eval, a rigorous benchmark, and develop optimized\nmulti-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata,\nconsistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks\nlike Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes\nwell to out-of-domain puzzle benchmarks and mathematical reasoning, with little\nmulti-tasking trade-off. When trained on larger models like Seed1.5-Thinking\n(20B activated parameters and 200B total parameters), puzzle data from Enigmata\nfurther boosts SoTA performance on advanced math and STEM reasoning tasks such\nas AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization\nbenefits of Enigmata. This work offers a unified, controllable framework for\nadvancing logical reasoning in LLMs. Resources of this work can be found at\nhttps://seed-enigmata.github.io."}
{"id": "2505.19663", "pdf": "https://arxiv.org/pdf/2505.19663", "abs": "https://arxiv.org/abs/2505.19663", "authors": ["Yigitcan zer", "Woosung Choi", "Joan Serr", "Mayank Kumar Singh", "Wei-Hsiang Liao", "Yuki Mitsufuji"], "title": "A Comprehensive Real-World Assessment of Audio Watermarking Algorithms: Will They Survive Neural Codecs?", "categories": ["cs.SD", "cs.AI", "cs.CR", "cs.LG", "eess.AS"], "comment": "5 pages; 5 tables; accepted at INTERSPEECH 2025", "summary": "We present a framework to foster the evaluation of deep learning-based audio\nwatermarking algorithms, establishing a standardized benchmark and allowing\nsystematic comparisons. To simulate real-world usage, we introduce a\ncomprehensive audio attack pipeline, featuring various distortions such as\ncompression, background noise, and reverberation, and propose a diverse test\ndataset, including speech, environmental sounds, and music recordings. By\nassessing the performance of four existing watermarking algorithms on our\nframework, two main insights stand out: (i) neural compression techniques pose\nthe most significant challenge, even when algorithms are trained with such\ncompressions; and (ii) training with audio attacks generally improves\nrobustness, although it is insufficient in some cases. Furthermore, we find\nthat specific distortions, such as polarity inversion, time stretching, or\nreverb, seriously affect certain algorithms. Our contributions strengthen the\nrobustness and perceptual assessment of audio watermarking algorithms across a\nwide range of applications, while ensuring a fair and consistent evaluation\napproach. The evaluation framework, including the attack pipeline, is\naccessible at github.com/SonyResearch/wm_robustness_eval."}
{"id": "2505.19915", "pdf": "https://arxiv.org/pdf/2505.19915", "abs": "https://arxiv.org/abs/2505.19915", "authors": ["Artem Petrov", "Dmitrii Volkov"], "title": "Evaluating AI cyber capabilities with crowdsourced elicitation", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "As AI systems become increasingly capable, understanding their offensive\ncyber potential is critical for informed governance and responsible deployment.\nHowever, it's hard to accurately bound their capabilities, and some prior\nevaluations dramatically underestimated them. The art of extracting maximum\ntask-specific performance from AIs is called \"AI elicitation\", and today's\nsafety organizations typically conduct it in-house. In this paper, we explore\ncrowdsourcing elicitation efforts as an alternative to in-house elicitation\nwork.\n  We host open-access AI tracks at two Capture The Flag (CTF) competitions: AI\nvs. Humans (400 teams) and Cyber Apocalypse_ (4000 teams). The AI teams achieve\noutstanding performance at both events, ranking top-13% and top-21%\nrespectively for a total of \\$7500 in bounties. This impressive performance\nsuggests that open-market elicitation may offer an effective complement to\nin-house elicitation. We propose elicitation bounties as a practical mechanism\nfor maintaining timely, cost-effective situational awareness of emerging AI\ncapabilities.\n  Another advantage of open elicitations is the option to collect human\nperformance data at scale. Applying METR's methodology, we found that AI agents\ncan reliably solve cyber challenges requiring one hour or less of effort from a\nmedian human CTF participant."}
{"id": "2505.19714", "pdf": "https://arxiv.org/pdf/2505.19714", "abs": "https://arxiv.org/abs/2505.19714", "authors": ["Zhaopeng Feng", "Yupu Liang", "Shaosheng Cao", "Jiayuan Su", "Jiahan Ren", "Zhe Xu", "Yao Hu", "Wenxuan Huang", "Jian Wu", "Zuozhu Liu"], "title": "MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via Multi-Task Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in progress", "summary": "Text Image Machine Translation (TIMT)-the task of translating textual content\nembedded in images-is critical for applications in accessibility, cross-lingual\ninformation access, and real-world document understanding. However, TIMT\nremains a complex challenge due to the need for accurate optical character\nrecognition (OCR), robust visual-text reasoning, and high-quality translation,\noften requiring cascading multi-stage pipelines. Recent advances in large-scale\nReinforcement Learning (RL) have improved reasoning in Large Language Models\n(LLMs) and Multimodal LLMs (MLLMs), but their application to end-to-end TIMT is\nstill underexplored. To bridge this gap, we introduce MT$^{3}$, the first\nframework to apply Multi-Task RL to MLLMs for end-to-end TIMT. MT$^{3}$ adopts\na multi-task optimization paradigm targeting three key sub-skills: text\nrecognition, context-aware reasoning, and translation. It is trained using a\nnovel multi-mixed reward mechanism that adapts rule-based RL strategies to\nTIMT's intricacies, offering fine-grained, non-binary feedback across tasks.\nFurthermore, to facilitate the evaluation of TIMT in authentic cross-cultural\nand real-world social media contexts, we introduced XHSPost, the first social\nmedia TIMT benchmark. Our MT$^{3}$-7B-Zero achieves state-of-the-art results on\nthe latest in-domain MIT-10M benchmark, outperforming strong baselines such as\nQwen2.5-VL-72B and InternVL2.5-78B by notable margins across multiple metrics.\nAdditionally, the model shows strong generalization to out-of-distribution\nlanguage pairs and datasets. In-depth analyses reveal how multi-task synergy,\nreinforcement learning initialization, curriculum design, and reward\nformulation contribute to advancing MLLM-driven TIMT."}
{"id": "2505.19920", "pdf": "https://arxiv.org/pdf/2505.19920", "abs": "https://arxiv.org/abs/2505.19920", "authors": ["Sebastian Gro", "Stefan Heindorf", "Philipp Terhrst"], "title": "A Responsible Face Recognition Approach for Small and Mid-Scale Systems Through Personalized Neural Networks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Traditional face recognition systems rely on extracting fixed face\nrepresentations, known as templates, to store and verify identities. These\nrepresentations are typically generated by neural networks that often lack\nexplainability and raise concerns regarding fairness and privacy. In this work,\nwe propose a novel model-template (MOTE) approach that replaces vector-based\nface templates with small personalized neural networks. This design enables\nmore responsible face recognition for small and medium-scale systems. During\nenrollment, MOTE creates a dedicated binary classifier for each identity,\ntrained to determine whether an input face matches the enrolled identity. Each\nclassifier is trained using only a single reference sample, along with\nsynthetically balanced samples to allow adjusting fairness at the level of a\nsingle individual during enrollment. Extensive experiments across multiple\ndatasets and recognition systems demonstrate substantial improvements in\nfairness and particularly in privacy. Although the method increases inference\ntime and storage requirements, it presents a strong solution for small- and\nmid-scale applications where fairness and privacy are critical."}
{"id": "2505.19715", "pdf": "https://arxiv.org/pdf/2505.19715", "abs": "https://arxiv.org/abs/2505.19715", "authors": ["Chunyang Jiang", "Chi-min Chan", "Yiyang Cai", "Yulong Liu", "Wei Xue", "Yike Guo"], "title": "Graceful Forgetting in Generative Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "8 pages, 6 figures", "summary": "Recently, the pretrain-finetune paradigm has become a cornerstone in various\ndeep learning areas. While in general the pre-trained model would promote both\neffectiveness and efficiency of downstream tasks fine-tuning, studies have\nshown that not all knowledge acquired during pre-training is beneficial. Some\nof the knowledge may actually bring detrimental effects to the fine-tuning\ntasks, which is also known as negative transfer. To address this problem,\ngraceful forgetting has emerged as a promising approach. The core principle of\ngraceful forgetting is to enhance the learning plasticity of the target task by\nselectively discarding irrelevant knowledge. However, this approach remains\nunderexplored in the context of generative language models, and it is often\nchallenging to migrate existing forgetting algorithms to these models due to\narchitecture incompatibility. To bridge this gap, in this paper we propose a\nnovel framework, Learning With Forgetting (LWF), to achieve graceful forgetting\nin generative language models. With Fisher Information Matrix weighting the\nintended parameter updates, LWF computes forgetting confidence to evaluate\nself-generated knowledge regarding the forgetting task, and consequently,\nknowledge with high confidence is periodically unlearned during fine-tuning.\nOur experiments demonstrate that, although thoroughly uncovering the mechanisms\nof knowledge interaction remains challenging in pre-trained language models,\napplying graceful forgetting can contribute to enhanced fine-tuning\nperformance."}
{"id": "2505.19944", "pdf": "https://arxiv.org/pdf/2505.19944", "abs": "https://arxiv.org/abs/2505.19944", "authors": ["Naoyuki Terashita", "Yusuke Tozaki", "Hideaki Omote", "Congkha Nguyen", "Ryosuke Nakamoto", "Yuta Koreeda", "Hiroaki Ozaki"], "title": "Can Visual Encoder Learn to See Arrows?", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "This work has been accepted for poster presentation at the Second\n  Workshop on Visual Concepts in CVPR 2025", "summary": "The diagram is a visual representation of a relationship illustrated with\nedges (lines or arrows), which is widely used in industrial and scientific\ncommunication. Although recognizing diagrams is essential for vision language\nmodels (VLMs) to comprehend domain-specific knowledge, recent studies reveal\nthat many VLMs fail to identify edges in images. We hypothesize that these\nfailures stem from an over-reliance on textual and positional biases,\npreventing VLMs from learning explicit edge features. Based on this idea, we\nempirically investigate whether the image encoder in VLMs can learn edge\nrepresentation through training on a diagram dataset in which edges are biased\nneither by textual nor positional information. To this end, we conduct\ncontrastive learning on an artificially generated diagram--caption dataset to\ntrain an image encoder and evaluate its diagram-related features on three\ntasks: probing, image retrieval, and captioning. Our results show that the\nfinetuned model outperforms pretrained CLIP in all tasks and surpasses\nzero-shot GPT-4o and LLaVA-Mistral in the captioning task. These findings\nconfirm that eliminating textual and positional biases fosters accurate edge\nrecognition in VLMs, offering a promising path for advancing diagram\nunderstanding."}
{"id": "2505.19720", "pdf": "https://arxiv.org/pdf/2505.19720", "abs": "https://arxiv.org/abs/2505.19720", "authors": ["Marco Rando", "Cesare Molinari", "Lorenzo Rosasco", "Silvia Villa"], "title": "A Structured Tour of Optimization with Finite Differences", "categories": ["math.OC", "cs.LG", "90C56 (Primary) 90C25, 90C26 (Secondary)", "G.1.6"], "comment": "24 pages, 8 figures, 7 tables", "summary": "Finite-difference methods are widely used for zeroth-order optimization in\nsettings where gradient information is unavailable or expensive to compute.\nThese procedures mimic first-order strategies by approximating gradients\nthrough function evaluations along a set of random directions. From a\ntheoretical perspective, recent studies indicate that imposing structure (such\nas orthogonality) on the chosen directions allows for the derivation of\nconvergence rates comparable to those achieved with unstructured random\ndirections (i.e., directions sampled independently from a distribution).\nEmpirically, although structured directions are expected to enhance\nperformance, they often introduce additional computational costs, which can\nlimit their applicability in high-dimensional settings. In this work, we\nexamine the impact of structured direction selection in finite-difference\nmethods. We review and extend several strategies for constructing structured\ndirection matrices and compare them with unstructured approaches in terms of\ncomputational cost, gradient approximation quality, and convergence behavior.\nOur evaluation spans both synthetic tasks and real-world applications such as\nadversarial perturbation. The results demonstrate that structured directions\ncan be generated with computational costs comparable to unstructured ones while\nsignificantly improving gradient estimation accuracy and optimization\nperformance."}
{"id": "2505.19947", "pdf": "https://arxiv.org/pdf/2505.19947", "abs": "https://arxiv.org/abs/2505.19947", "authors": ["Herbert Woisetschlger", "Ryan Zhang", "Shiqiang Wang", "Hans-Arno Jacobsen"], "title": "Dynamically Learned Test-Time Model Routing in Language Model Zoos with Service Level Guarantees", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "I.2; I.2.7; I.2.8"], "comment": "Preprint. Under review", "summary": "Open-weight LLM zoos provide access to numerous high-quality models, but\nselecting the appropriate model for specific tasks remains challenging and\nrequires technical expertise. Most users simply want factually correct, safe,\nand satisfying responses without concerning themselves with model\ntechnicalities, while inference service providers prioritize minimizing\noperating costs. These competing interests are typically mediated through\nservice level agreements (SLAs) that guarantee minimum service quality. We\nintroduce MESS+, a stochastic optimization algorithm for cost-optimal LLM\nrequest routing while providing rigorous SLA compliance guarantees. MESS+\nlearns request satisfaction probabilities of LLMs in real-time as users\ninteract with the system, based on which model selection decisions are made by\nsolving a per-request optimization problem. Our algorithm includes a novel\ncombination of virtual queues and request satisfaction prediction, along with a\ntheoretical analysis of cost optimality and constraint satisfaction. Across a\nwide range of state-of-the-art LLM benchmarks, MESS+ achieves an average of 2x\ncost savings compared to existing LLM routing techniques."}
{"id": "2505.19731", "pdf": "https://arxiv.org/pdf/2505.19731", "abs": "https://arxiv.org/abs/2505.19731", "authors": ["Daniil Tiapkin", "Daniele Calandriello", "Denis Belomestny", "Eric Moulines", "Alexey Naumov", "Kashif Rasul", "Michal Valko", "Pierre Menard"], "title": "Accelerating Nash Learning from Human Feedback via Mirror Prox", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Traditional Reinforcement Learning from Human Feedback (RLHF) often relies on\nreward models, frequently assuming preference structures like the Bradley-Terry\nmodel, which may not accurately capture the complexities of real human\npreferences (e.g., intransitivity). Nash Learning from Human Feedback (NLHF)\noffers a more direct alternative by framing the problem as finding a Nash\nequilibrium of a game defined by these preferences. In this work, we introduce\nNash Mirror Prox ($\\mathtt{Nash-MP}$), an online NLHF algorithm that leverages\nthe Mirror Prox optimization scheme to achieve fast and stable convergence to\nthe Nash equilibrium. Our theoretical analysis establishes that Nash-MP\nexhibits last-iterate linear convergence towards the $\\beta$-regularized Nash\nequilibrium. Specifically, we prove that the KL-divergence to the optimal\npolicy decreases at a rate of order $(1+2\\beta)^{-N/2}$, where $N$ is a number\nof preference queries. We further demonstrate last-iterate linear convergence\nfor the exploitability gap and uniformly for the span semi-norm of\nlog-probabilities, with all these rates being independent of the size of the\naction space. Furthermore, we propose and analyze an approximate version of\nNash-MP where proximal steps are estimated using stochastic policy gradients,\nmaking the algorithm closer to applications. Finally, we detail a practical\nimplementation strategy for fine-tuning large language models and present\nexperiments that demonstrate its competitive performance and compatibility with\nexisting methods."}
{"id": "2505.19948", "pdf": "https://arxiv.org/pdf/2505.19948", "abs": "https://arxiv.org/abs/2505.19948", "authors": ["Gokul Adethya", "Bhanu Pratyush Mantha", "Tianyang Wang", "Xingjian Li", "Min Xu"], "title": "SaSi: A Self-augmented and Self-interpreted Deep Learning Approach for Few-shot Cryo-ET Particle Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Cryo-electron tomography (cryo-ET) has emerged as a powerful technique for\nimaging macromolecular complexes in their near-native states. However, the\nlocalization of 3D particles in cellular environments still presents a\nsignificant challenge due to low signal-to-noise ratios and missing wedge\nartifacts. Deep learning approaches have shown great potential, but they need\nhuge amounts of data, which can be a challenge in cryo-ET scenarios where\nlabeled data is often scarce. In this paper, we propose a novel Self-augmented\nand Self-interpreted (SaSi) deep learning approach towards few-shot particle\ndetection in 3D cryo-ET images. Our method builds upon self-augmentation\ntechniques to further boost data utilization and introduces a self-interpreted\nsegmentation strategy for alleviating dependency on labeled data, hence\nimproving generalization and robustness. As demonstrated by experiments\nconducted on both simulated and real-world cryo-ET datasets, the SaSi approach\nsignificantly outperforms existing state-of-the-art methods for particle\nlocalization. This research increases understanding of how to detect particles\nwith very few labels in cryo-ET and thus sets a new benchmark for few-shot\nlearning in structural biology."}
{"id": "2505.19737", "pdf": "https://arxiv.org/pdf/2505.19737", "abs": "https://arxiv.org/abs/2505.19737", "authors": ["Luc Pronzato", "Maria-Joo Rendas"], "title": "Weighted Leave-One-Out Cross Validation", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "We present a weighted version of Leave-One-Out (LOO) cross-validation for\nestimating the Integrated Squared Error (ISE) when approximating an unknown\nfunction by a predictor that depends linearly on evaluations of the function\nover a finite collection of sites. The method relies on the construction of the\nbest linear estimator of the squared prediction error at an arbitrary unsampled\nsite based on squared LOO residuals, assuming that the function is a\nrealization of a Gaussian Process (GP). A theoretical analysis of performance\nof the ISE estimator is presented, and robustness with respect to the choice of\nthe GP kernel is investigated first analytically, then through numerical\nexamples. Overall, the estimation of ISE is significantly more precise than\nwith classical, unweighted, LOO cross validation. Application to model\nselection is briefly considered through examples."}
{"id": "2505.19951", "pdf": "https://arxiv.org/pdf/2505.19951", "abs": "https://arxiv.org/abs/2505.19951", "authors": ["Elvir Karimov", "Alexander Varlamov", "Danil Ivanov", "Dmitrii Korzh", "Oleg Y. Rogov"], "title": "Novel Loss-Enhanced Universal Adversarial Patches for Sustainable Speaker Privacy", "categories": ["cs.SD", "cs.AI", "cs.CR", "eess.AS"], "comment": "5 pages, 3 figures, 1 table; Submitted to Interspeech 2025", "summary": "Deep learning voice models are commonly used nowadays, but the safety\nprocessing of personal data, such as human identity and speech content, remains\nsuspicious. To prevent malicious user identification, speaker anonymization\nmethods were proposed. Current methods, particularly based on universal\nadversarial patch (UAP) applications, have drawbacks such as significant\ndegradation of audio quality, decreased speech recognition quality, low\ntransferability across different voice biometrics models, and performance\ndependence on the input audio length. To mitigate these drawbacks, in this\nwork, we introduce and leverage the novel Exponential Total Variance (TV) loss\nfunction and provide experimental evidence that it positively affects UAP\nstrength and imperceptibility. Moreover, we present a novel scalable UAP\ninsertion procedure and demonstrate its uniformly high performance for various\naudio lengths."}
{"id": "2505.19743", "pdf": "https://arxiv.org/pdf/2505.19743", "abs": "https://arxiv.org/abs/2505.19743", "authors": ["Yang Zhang", "Yu Yu", "Bo Tang", "Yu Zhu", "Chuxiong Sun", "Wenqiang Wei", "Jie Hu", "Zipeng Xie", "Zhiyu Li", "Feiyu Xiong", "Edward Chung"], "title": "Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "summary": "With the rapid development of Large Language Models (LLMs), aligning these\nmodels with human preferences and values is critical to ensuring ethical and\nsafe applications. However, existing alignment techniques such as RLHF or DPO\noften require direct fine-tuning on LLMs with billions of parameters, resulting\nin substantial computational costs and inefficiencies. To address this, we\npropose Micro token-level Accept-Reject Aligning (MARA) approach designed to\noperate independently of the language models. MARA simplifies the alignment\nprocess by decomposing sentence-level preference learning into token-level\nbinary classification, where a compact three-layer fully-connected network\ndetermines whether candidate tokens are \"Accepted\" or \"Rejected\" as part of the\nresponse. Extensive experiments across seven different LLMs and three\nopen-source datasets show that MARA achieves significant improvements in\nalignment performance while reducing computational costs. The source code and\nimplementation details are publicly available at\nhttps://github.com/IAAR-Shanghai/MARA, and the trained models are released at\nhttps://huggingface.co/IAAR-Shanghai/MARA_AGENTS."}
{"id": "2505.19955", "pdf": "https://arxiv.org/pdf/2505.19955", "abs": "https://arxiv.org/abs/2505.19955", "authors": ["Hui Chen", "Miao Xiong", "Yujie Lu", "Wei Han", "Ailin Deng", "Yufei He", "Jiaying Wu", "Yibo Li", "Yue Liu", "Bryan Hooi"], "title": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "40 pages, 7 figures", "summary": "Recent advancements in AI agents have demonstrated their growing potential to\ndrive and support scientific discovery. In this work, we introduce MLR-Bench, a\ncomprehensive benchmark for evaluating AI agents on open-ended machine learning\nresearch. MLR-Bench includes three key components: (1) 201 research tasks\nsourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2)\nMLR-Judge, an automated evaluation framework combining LLM-based reviewers with\ncarefully designed review rubrics to assess research quality; and (3)\nMLR-Agent, a modular agent scaffold capable of completing research tasks\nthrough four stages: idea generation, proposal formulation, experimentation,\nand paper writing. Our framework supports both stepwise assessment across these\ndistinct research stages, and end-to-end evaluation of the final research\npaper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced\ncoding agent, finding that while LLMs are effective at generating coherent\nideas and well-structured papers, current coding agents frequently (e.g., in\n80% of the cases) produce fabricated or invalidated experimental\nresults--posing a major barrier to scientific reliability. We validate\nMLR-Judge through human evaluation, showing high agreement with expert\nreviewers, supporting its potential as a scalable tool for research evaluation.\nWe open-source MLR-Bench to help the community benchmark, diagnose, and improve\nAI research agents toward trustworthy and transparent scientific discovery."}
{"id": "2505.19757", "pdf": "https://arxiv.org/pdf/2505.19757", "abs": "https://arxiv.org/abs/2505.19757", "authors": ["Maria Dziuba", "Valentin Malykh"], "title": "CIDRe: A Reference-Free Multi-Aspect Criterion for Code Comment Quality Measurement", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Effective generation of structured code comments requires robust quality\nmetrics for dataset curation, yet existing approaches (SIDE, MIDQ, STASIS)\nsuffer from limited code-comment analysis. We propose CIDRe, a\nlanguage-agnostic reference-free quality criterion combining four synergistic\naspects: (1) relevance (code-comment semantic alignment), (2) informativeness\n(functional coverage), (3) completeness (presence of all structure sections),\nand (4) description length (detail sufficiency). We validate our criterion on a\nmanually annotated dataset. Experiments demonstrate CIDRe's superiority over\nexisting metrics, achieving improvement in cross-entropy evaluation. When\napplied to filter comments, the models finetuned on CIDRe-filtered data show\nstatistically significant quality gains in GPT-4o-mini assessments."}
{"id": "2505.19964", "pdf": "https://arxiv.org/pdf/2505.19964", "abs": "https://arxiv.org/abs/2505.19964", "authors": ["Eric Zhao", "Jessica Dai", "Pranjal Awasthi"], "title": "The Limits of Preference Data for Post-Training", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.GT"], "comment": null, "summary": "Recent progress in strengthening the capabilities of large language models\nhas stemmed from applying reinforcement learning to domains with automatically\nverifiable outcomes. A key question is whether we can similarly use RL to\noptimize for outcomes in domains where evaluating outcomes inherently requires\nhuman feedback; for example, in tasks like deep research and trip planning,\noutcome evaluation is qualitative and there are many possible degrees of\nsuccess. One attractive and scalable modality for collecting human feedback is\npreference data: ordinal rankings (pairwise or $k$-wise) that indicate, for $k$\ngiven outcomes, which one is preferred. In this work, we study a critical\nroadblock: preference data fundamentally and significantly limits outcome-based\noptimization. Even with idealized preference data (infinite, noiseless, and\nonline), the use of ordinal feedback can prevent obtaining even approximately\noptimal solutions. We formalize this impossibility using voting theory, drawing\nan analogy between how a model chooses to answer a query with how voters choose\na candidate to elect. This indicates that grounded human scoring and\nalgorithmic innovations are necessary for extending the success of RL\npost-training to domains demanding human feedback. We also explore why these\nlimitations have disproportionately impacted RLHF when it comes to eliciting\nreasoning behaviors (e.g., backtracking) versus situations where RLHF has been\nhistorically successful (e.g., instruction-tuning and safety training), finding\nthat the limitations of preference data primarily suppress RLHF's ability to\nelicit robust strategies -- a class that encompasses most reasoning behaviors."}
{"id": "2505.19779", "pdf": "https://arxiv.org/pdf/2505.19779", "abs": "https://arxiv.org/abs/2505.19779", "authors": ["Mobina Mansoori", "Sajjad Shahabodini", "Farnoush Bayatmakou", "Jamshid Abouei", "Konstantinos N. Plataniotis", "Arash Mohammadi"], "title": "Advancements in Medical Image Classification through Fine-Tuning Natural Domain Foundation Models", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Using massive datasets, foundation models are large-scale, pre-trained models\nthat perform a wide range of tasks. These models have shown consistently\nimproved results with the introduction of new methods. It is crucial to analyze\nhow these trends impact the medical field and determine whether these\nadvancements can drive meaningful change. This study investigates the\napplication of recent state-of-the-art foundation models, DINOv2, MAE, VMamba,\nCoCa, SAM2, and AIMv2, for medical image classification. We explore their\neffectiveness on datasets including CBIS-DDSM for mammography, ISIC2019 for\nskin lesions, APTOS2019 for diabetic retinopathy, and CHEXPERT for chest\nradiographs. By fine-tuning these models and evaluating their configurations,\nwe aim to understand the potential of these advancements in medical image\nclassification. The results indicate that these advanced models significantly\nenhance classification outcomes, demonstrating robust performance despite\nlimited labeled data. Based on our results, AIMv2, DINOv2, and SAM2 models\noutperformed others, demonstrating that progress in natural domain training has\npositively impacted the medical domain and improved classification outcomes.\nOur code is publicly available at:\nhttps://github.com/sajjad-sh33/Medical-Transfer-Learning."}
{"id": "2505.19966", "pdf": "https://arxiv.org/pdf/2505.19966", "abs": "https://arxiv.org/abs/2505.19966", "authors": ["Zheng Zhang", "Shaocheng Lan", "Lei Song", "Jiang Bian", "Yexin Li", "Kan Ren"], "title": "Learning to Select In-Context Demonstration Preferred by Large Language Model", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In-context learning (ICL) enables large language models (LLMs) to adapt to\nnew tasks during inference using only a few demonstrations. However, ICL\nperformance is highly dependent on the selection of these demonstrations.\nRecent work explores retrieval-based methods for selecting query-specific\ndemonstrations, but these approaches often rely on surrogate objectives such as\nmetric learning, failing to directly optimize ICL performance. Consequently,\nthey struggle to identify truly beneficial demonstrations. Moreover, their\ndiscriminative retrieval paradigm is ineffective when the candidate pool lacks\nsufficient high-quality demonstrations. To address these challenges, we propose\nGenICL, a novel generative preference learning framework that leverages LLM\nfeedback to directly optimize demonstration selection for ICL. Experiments on\n19 datasets across 11 task categories demonstrate that GenICL achieves superior\nperformance than existing methods in selecting the most effective\ndemonstrations, leading to better ICL performance."}
{"id": "2505.19795", "pdf": "https://arxiv.org/pdf/2505.19795", "abs": "https://arxiv.org/abs/2505.19795", "authors": ["Sajjad Shahabodini", "Mobina Mansoori", "Farnoush Bayatmakou", "Jamshid Abouei", "Konstantinos N. Plataniotis", "Arash Mohammadi"], "title": "The Missing Point in Vision Transformers for Universal Image Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": null, "summary": "Image segmentation remains a challenging task in computer vision, demanding\nrobust mask generation and precise classification. Recent mask-based approaches\nyield high-quality masks by capturing global context. However, accurately\nclassifying these masks, especially in the presence of ambiguous boundaries and\nimbalanced class distributions, remains an open challenge. In this work, we\nintroduce ViT-P, a novel two-stage segmentation framework that decouples mask\ngeneration from classification. The first stage employs a proposal generator to\nproduce class-agnostic mask proposals, while the second stage utilizes a\npoint-based classification model built on the Vision Transformer (ViT) to\nrefine predictions by focusing on mask central points. ViT-P serves as a\npre-training-free adapter, allowing the integration of various pre-trained\nvision transformers without modifying their architecture, ensuring adaptability\nto dense prediction tasks. Furthermore, we demonstrate that coarse and bounding\nbox annotations can effectively enhance classification without requiring\nadditional training on fine annotation datasets, reducing annotation costs\nwhile maintaining strong performance. Extensive experiments across COCO,\nADE20K, and Cityscapes datasets validate the effectiveness of ViT-P, achieving\nstate-of-the-art results with 54.0 PQ on ADE20K panoptic segmentation, 87.4\nmIoU on Cityscapes semantic segmentation, and 63.6 mIoU on ADE20K semantic\nsegmentation. The code and pretrained models are available at:\nhttps://github.com/sajjad-sh33/ViT-P}{https://github.com/sajjad-sh33/ViT-P."}
{"id": "2505.19973", "pdf": "https://arxiv.org/pdf/2505.19973", "abs": "https://arxiv.org/abs/2505.19973", "authors": ["Bilel Cherif", "Tamas Bisztray", "Richard A. Dubniczky", "Aaesha Aldahmani", "Saeed Alshehhi", "Norbert Tihanyi"], "title": "DFIR-Metric: A Benchmark Dataset for Evaluating Large Language Models in Digital Forensics and Incident Response", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Digital Forensics and Incident Response (DFIR) involves analyzing digital\nevidence to support legal investigations. Large Language Models (LLMs) offer\nnew opportunities in DFIR tasks such as log analysis and memory forensics, but\ntheir susceptibility to errors and hallucinations raises concerns in\nhigh-stakes contexts. Despite growing interest, there is no comprehensive\nbenchmark to evaluate LLMs across both theoretical and practical DFIR domains.\nTo address this gap, we present DFIR-Metric, a benchmark with three components:\n(1) Knowledge Assessment: a set of 700 expert-reviewed multiple-choice\nquestions sourced from industry-standard certifications and official\ndocumentation; (2) Realistic Forensic Challenges: 150 CTF-style tasks testing\nmulti-step reasoning and evidence correlation; and (3) Practical Analysis: 500\ndisk and memory forensics cases from the NIST Computer Forensics Tool Testing\nProgram (CFTT). We evaluated 14 LLMs using DFIR-Metric, analyzing both their\naccuracy and consistency across trials. We also introduce a new metric, the\nTask Understanding Score (TUS), designed to more effectively evaluate models in\nscenarios where they achieve near-zero accuracy. This benchmark offers a\nrigorous, reproducible foundation for advancing AI in digital forensics. All\nscripts, artifacts, and results are available on the project website at\nhttps://github.com/DFIR-Metric."}
{"id": "2505.19806", "pdf": "https://arxiv.org/pdf/2505.19806", "abs": "https://arxiv.org/abs/2505.19806", "authors": ["Sirui Chen", "Shuqin Ma", "Shu Yu", "Hanwang Zhang", "Shengjie Zhao", "Chaochao Lu"], "title": "Exploring Consciousness in LLMs: A Systematic Survey of Theories, Implementations, and Frontier Risks", "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Consciousness stands as one of the most profound and distinguishing features\nof the human mind, fundamentally shaping our understanding of existence and\nagency. As large language models (LLMs) develop at an unprecedented pace,\nquestions concerning intelligence and consciousness have become increasingly\nsignificant. However, discourse on LLM consciousness remains largely unexplored\nterritory. In this paper, we first clarify frequently conflated terminologies\n(e.g., LLM consciousness and LLM awareness). Then, we systematically organize\nand synthesize existing research on LLM consciousness from both theoretical and\nempirical perspectives. Furthermore, we highlight potential frontier risks that\nconscious LLMs might introduce. Finally, we discuss current challenges and\noutline future directions in this emerging field. The references discussed in\nthis paper are organized at\nhttps://github.com/OpenCausaLab/Awesome-LLM-Consciousness."}
{"id": "2505.19983", "pdf": "https://arxiv.org/pdf/2505.19983", "abs": "https://arxiv.org/abs/2505.19983", "authors": ["Tong Wu", "Zhiyong Chen", "Dazhi He", "Feng Yang", "Meixia Tao", "Xiaodong Xu", "Wenjun Zhang", "Ping Zhang"], "title": "ICDM: Interference Cancellation Diffusion Models for Wireless Semantic Communications", "categories": ["cs.IT", "cs.AI", "cs.CV", "math.IT"], "comment": "submitted to IEEE journal", "summary": "Diffusion models (DMs) have recently achieved significant success in wireless\ncommunications systems due to their denoising capabilities. The broadcast\nnature of wireless signals makes them susceptible not only to Gaussian noise,\nbut also to unaware interference. This raises the question of whether DMs can\neffectively mitigate interference in wireless semantic communication systems.\nIn this paper, we model the interference cancellation problem as a maximum a\nposteriori (MAP) problem over the joint posterior probability of the signal and\ninterference, and theoretically prove that the solution provides excellent\nestimates for the signal and interference. To solve this problem, we develop an\ninterference cancellation diffusion model (ICDM), which decomposes the joint\nposterior into independent prior probabilities of the signal and interference,\nalong with the channel transition probablity. The log-gradients of these\ndistributions at each time step are learned separately by DMs and accurately\nestimated through deriving. ICDM further integrates these gradients with\nadvanced numerical iteration method, achieving accurate and rapid interference\ncancellation. Extensive experiments demonstrate that ICDM significantly reduces\nthe mean square error (MSE) and enhances perceptual quality compared to schemes\nwithout ICDM. For example, on the CelebA dataset under the Rayleigh fading\nchannel with a signal-to-noise ratio (SNR) of $20$ dB and signal to\ninterference plus noise ratio (SINR) of 0 dB, ICDM reduces the MSE by 4.54 dB\nand improves the learned perceptual image patch similarity (LPIPS) by 2.47 dB."}
{"id": "2505.19821", "pdf": "https://arxiv.org/pdf/2505.19821", "abs": "https://arxiv.org/abs/2505.19821", "authors": ["Zhou Feng", "Jiahao Chen", "Chunyi Zhou", "Yuwen Pu", "Qingming Li", "Shouling Ji"], "title": "Poison in the Well: Feature Embedding Disruption in Backdoor Attacks", "categories": ["cs.CR", "cs.LG", "I.2.6; I.5.1; D.4.6"], "comment": "Accepted to ICME 2025", "summary": "Backdoor attacks embed malicious triggers into training data, enabling\nattackers to manipulate neural network behavior during inference while\nmaintaining high accuracy on benign inputs. However, existing backdoor attacks\nface limitations manifesting in excessive reliance on training data, poor\nstealth, and instability, which hinder their effectiveness in real-world\napplications. Therefore, this paper introduces ShadowPrint, a versatile\nbackdoor attack that targets feature embeddings within neural networks to\nachieve high ASRs and stealthiness. Unlike traditional approaches, ShadowPrint\nreduces reliance on training data access and operates effectively with\nexceedingly low poison rates (as low as 0.01%). It leverages a clustering-based\noptimization strategy to align feature embeddings, ensuring robust performance\nacross diverse scenarios while maintaining stability and stealth. Extensive\nevaluations demonstrate that ShadowPrint achieves superior ASR (up to 100%),\nsteady CA (with decay no more than 1% in most cases), and low DDR (averaging\nbelow 5%) across both clean-label and dirty-label settings, and with poison\nrates ranging from as low as 0.01% to 0.05%, setting a new standard for\nbackdoor attack capabilities and emphasizing the need for advanced defense\nstrategies focused on feature space manipulations."}
{"id": "2505.20021", "pdf": "https://arxiv.org/pdf/2505.20021", "abs": "https://arxiv.org/abs/2505.20021", "authors": ["Hyunsik Chae", "Seungwoo Yoon", "Jaden Park", "Chloe Yewon Chun", "Yongin Cho", "Mu Cai", "Yong Jae Lee", "Ernest K. Ryu"], "title": "Decomposing Complex Visual Comprehension into Atomic Visual Skills for Vision Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "69 pages, 16 figures", "summary": "Recent Vision-Language Models (VLMs) have demonstrated impressive multimodal\ncomprehension and reasoning capabilities, yet they often struggle with\ntrivially simple visual tasks. In this work, we focus on the domain of basic 2D\nEuclidean geometry and systematically categorize the fundamental, indivisible\nvisual perception skills, which we refer to as atomic visual skills. We then\nintroduce the Atomic Visual Skills Dataset (AVSD) for evaluating VLMs on the\natomic visual skills. Using AVSD, we benchmark state-of-the-art VLMs and find\nthat they struggle with these tasks, despite being trivial for adult humans.\nOur findings highlight the need for purpose-built datasets to train and\nevaluate VLMs on atomic, rather than composite, visual perception tasks."}
{"id": "2505.19837", "pdf": "https://arxiv.org/pdf/2505.19837", "abs": "https://arxiv.org/abs/2505.19837", "authors": ["Christoph R. Landolt", "Christoph Wrsch", "Roland Meier", "Alain Mermoud", "Julian Jang-Jaccard"], "title": "Multi-Agent Reinforcement Learning in Cybersecurity: From Fundamentals to Applications", "categories": ["cs.MA", "cs.GT", "cs.LG"], "comment": null, "summary": "Multi-Agent Reinforcement Learning (MARL) has shown great potential as an\nadaptive solution for addressing modern cybersecurity challenges. MARL enables\ndecentralized, adaptive, and collaborative defense strategies and provides an\nautomated mechanism to combat dynamic, coordinated, and sophisticated threats.\nThis survey investigates the current state of research in MARL applications for\nautomated cyber defense (ACD), focusing on intruder detection and lateral\nmovement containment. Additionally, it examines the role of Autonomous\nIntelligent Cyber-defense Agents (AICA) and Cyber Gyms in training and\nvalidating MARL agents. Finally, the paper outlines existing challenges, such\nas scalability and adversarial robustness, and proposes future research\ndirections. This also discusses how MARL integrates in AICA to provide\nadaptive, scalable, and dynamic solutions to counter the increasingly\nsophisticated landscape of cyber threats. It highlights the transformative\npotential of MARL in areas like intrusion detection and lateral movement\ncontainment, and underscores the value of Cyber Gyms for training and\nvalidation of AICA."}
{"id": "2505.20024", "pdf": "https://arxiv.org/pdf/2505.20024", "abs": "https://arxiv.org/abs/2505.20024", "authors": ["Xueyi Liu", "Zuodong Zhong", "Yuxin Guo", "Yun-Fu Liu", "Zhiguo Su", "Qichao Zhang", "Junli Wang", "Yinfeng Gao", "Yupeng Zheng", "Qiao Lin", "Huiyong Chen", "Dongbin Zhao"], "title": "ReasonPlan: Unified Scene Prediction and Decision Reasoning for Closed-loop Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.RO", "68T40(Primary), 68T45, 68T50(Secondary)", "I.2.9; I.2.10; I.5.1"], "comment": "18 pages; 9 figures; https://github.com/Liuxueyi/ReasonPlan", "summary": "Due to the powerful vision-language reasoning and generalization abilities,\nmultimodal large language models (MLLMs) have garnered significant attention in\nthe field of end-to-end (E2E) autonomous driving. However, their application to\nclosed-loop systems remains underexplored, and current MLLM-based methods have\nnot shown clear superiority to mainstream E2E imitation learning approaches. In\nthis work, we propose ReasonPlan, a novel MLLM fine-tuning framework designed\nfor closed-loop driving through holistic reasoning with a self-supervised Next\nScene Prediction task and supervised Decision Chain-of-Thought process. This\ndual mechanism encourages the model to align visual representations with\nactionable driving context, while promoting interpretable and causally grounded\ndecision making. We curate a planning-oriented decision reasoning dataset,\nnamely PDR, comprising 210k diverse and high-quality samples. Our method\noutperforms the mainstream E2E imitation learning method by a large margin of\n19% L2 and 16.1 driving score on Bench2Drive benchmark. Furthermore, ReasonPlan\ndemonstrates strong zero-shot generalization on unseen DOS benchmark,\nhighlighting its adaptability in handling zero-shot corner cases. Code and\ndataset will be found in https://github.com/Liuxueyi/ReasonPlan."}
{"id": "2505.19840", "pdf": "https://arxiv.org/pdf/2505.19840", "abs": "https://arxiv.org/abs/2505.19840", "authors": ["Binyan Xu", "Xilin Dai", "Di Tang", "Kehuan Zhang"], "title": "One Surrogate to Fool Them All: Universal, Transferable, and Targeted Adversarial Attacks with CLIP", "categories": ["cs.CR", "cs.LG", "68T07", "I.2.6"], "comment": "21 pages, 15 figures, 18 tables To appear in the Proceedings of The\n  ACM Conference on Computer and Communications Security (CCS), 2025", "summary": "Deep Neural Networks (DNNs) have achieved widespread success yet remain prone\nto adversarial attacks. Typically, such attacks either involve frequent queries\nto the target model or rely on surrogate models closely mirroring the target\nmodel -- often trained with subsets of the target model's training data -- to\nachieve high attack success rates through transferability. However, in\nrealistic scenarios where training data is inaccessible and excessive queries\ncan raise alarms, crafting adversarial examples becomes more challenging. In\nthis paper, we present UnivIntruder, a novel attack framework that relies\nsolely on a single, publicly available CLIP model and publicly available\ndatasets. By using textual concepts, UnivIntruder generates universal,\ntransferable, and targeted adversarial perturbations that mislead DNNs into\nmisclassifying inputs into adversary-specified classes defined by textual\nconcepts.\n  Our extensive experiments show that our approach achieves an Attack Success\nRate (ASR) of up to 85% on ImageNet and over 99% on CIFAR-10, significantly\noutperforming existing transfer-based methods. Additionally, we reveal\nreal-world vulnerabilities, showing that even without querying target models,\nUnivIntruder compromises image search engines like Google and Baidu with ASR\nrates up to 84%, and vision language models like GPT-4 and Claude-3.5 with ASR\nrates up to 80%. These findings underscore the practicality of our attack in\nscenarios where traditional avenues are blocked, highlighting the need to\nreevaluate security paradigms in AI applications."}
{"id": "2505.20026", "pdf": "https://arxiv.org/pdf/2505.20026", "abs": "https://arxiv.org/abs/2505.20026", "authors": ["Xinping Chen", "Chen Liu"], "title": "Gradient Inversion Transcript: Leveraging Robust Generative Priors to Reconstruct Training Data from Gradient Leakage", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose Gradient Inversion Transcript (GIT), a novel generative approach\nfor reconstructing training data from leaked gradients. GIT employs a\ngenerative attack model, whose architecture is tailored to align with the\nstructure of the leaked model based on theoretical analysis. Once trained\noffline, GIT can be deployed efficiently and only relies on the leaked\ngradients to reconstruct the input data, rendering it applicable under various\ndistributed learning environments. When used as a prior for other iterative\noptimization-based methods, GIT not only accelerates convergence but also\nenhances the overall reconstruction quality. GIT consistently outperforms\nexisting methods across multiple datasets and demonstrates strong robustness\nunder challenging conditions, including inaccurate gradients, data distribution\nshifts and discrepancies in model parameters."}
{"id": "2505.19841", "pdf": "https://arxiv.org/pdf/2505.19841", "abs": "https://arxiv.org/abs/2505.19841", "authors": ["Arnaud Vadeboncoeur", "Mark Girolami", "Andrew M. Stuart"], "title": "Efficient Deconvolution in Populational Inverse Problems", "categories": ["stat.ML", "cs.LG", "physics.comp-ph"], "comment": null, "summary": "This work is focussed on the inversion task of inferring the distribution\nover parameters of interest leading to multiple sets of observations. The\npotential to solve such distributional inversion problems is driven by\nincreasing availability of data, but a major roadblock is blind deconvolution,\narising when the observational noise distribution is unknown. However, when\ndata originates from collections of physical systems, a population, it is\npossible to leverage this information to perform deconvolution. To this end, we\npropose a methodology leveraging large data sets of observations, collected\nfrom different instantiations of the same physical processes, to simultaneously\ndeconvolve the data corrupting noise distribution, and to identify the\ndistribution over model parameters defining the physical processes. A\nparameter-dependent mathematical model of the physical process is employed. A\nloss function characterizing the match between the observed data and the output\nof the mathematical model is defined; it is minimized as a function of the both\nthe parameter inputs to the model of the physics and the parameterized\nobservational noise. This coupled problem is addressed with a modified gradient\ndescent algorithm that leverages specific structure in the noise model.\nFurthermore, a new active learning scheme is proposed, based on adaptive\nempirical measures, to train a surrogate model to be accurate in parameter\nregions of interest; this approach accelerates computation and enables\nautomatic differentiation of black-box, potentially nondifferentiable, code\ncomputing parameter-to-solution maps. The proposed methodology is demonstrated\non porous medium flow, damped elastodynamics, and simplified models of\natmospheric dynamics."}
{"id": "2505.20027", "pdf": "https://arxiv.org/pdf/2505.20027", "abs": "https://arxiv.org/abs/2505.20027", "authors": ["Subba Reddy Oota", "Khushbu Pahwa", "Mounika Marreddy", "Maneesh Singh", "Manish Gupta", "Bapi S. Raju"], "title": "Multi-modal brain encoding models for multi-modal stimuli", "categories": ["q-bio.NC", "cs.AI", "cs.CL", "cs.LG", "eess.AS", "eess.IV"], "comment": "26 pages, 15 figures, The Thirteenth International Conference on\n  Learning Representations, ICLR-2025, Singapore.\n  https://openreview.net/pdf?id=0dELcFHig2", "summary": "Despite participants engaging in unimodal stimuli, such as watching images or\nsilent videos, recent work has demonstrated that multi-modal Transformer models\ncan predict visual brain activity impressively well, even with incongruent\nmodality representations. This raises the question of how accurately these\nmulti-modal models can predict brain activity when participants are engaged in\nmulti-modal stimuli. As these models grow increasingly popular, their use in\nstudying neural activity provides insights into how our brains respond to such\nmulti-modal naturalistic stimuli, i.e., where it separates and integrates\ninformation across modalities through a hierarchy of early sensory regions to\nhigher cognition. We investigate this question by using multiple unimodal and\ntwo types of multi-modal models-cross-modal and jointly pretrained-to determine\nwhich type of model is more relevant to fMRI brain activity when participants\nare engaged in watching movies. We observe that both types of multi-modal\nmodels show improved alignment in several language and visual regions. This\nstudy also helps in identifying which brain regions process unimodal versus\nmulti-modal information. We further investigate the contribution of each\nmodality to multi-modal alignment by carefully removing unimodal features one\nby one from multi-modal representations, and find that there is additional\ninformation beyond the unimodal embeddings that is processed in the visual and\nlanguage regions. Based on this investigation, we find that while for\ncross-modal models, their brain alignment is partially attributed to the video\nmodality; for jointly pretrained models, it is partially attributed to both the\nvideo and audio modalities. This serves as a strong motivation for the\nneuroscience community to investigate the interpretability of these models for\ndeepening our understanding of multi-modal information processing in brain."}
{"id": "2505.19862", "pdf": "https://arxiv.org/pdf/2505.19862", "abs": "https://arxiv.org/abs/2505.19862", "authors": ["Hexuan Deng", "Wenxiang Jiao", "Xuebo Liu", "Jun Rao", "Min Zhang"], "title": "REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Large Reasoning Models", "categories": ["cs.CL", "cs.LG"], "comment": "Work in Progress", "summary": "Large Reasoning Models (LRMs) demonstrate strong performance in complex tasks\nbut often face the challenge of overthinking, leading to substantially high\ninference costs. Existing approaches synthesize shorter reasoning responses for\nLRMs to learn, but are inefficient for online usage due to the time-consuming\ndata generation and filtering processes. Meanwhile, online reinforcement\nlearning mainly adopts a length reward to encourage short reasoning responses,\nbut tends to lose the reflection ability and harm the performance. To address\nthese issues, we propose REA-RL, which introduces a small reflection model for\nefficient scaling in online training, offering both parallel sampling and\nsequential revision. Besides, a reflection reward is designed to further\nprevent LRMs from favoring short yet non-reflective responses. Experiments show\nthat both methods maintain or enhance performance while significantly improving\ninference efficiency. Their combination achieves a good balance between\nperformance and efficiency, reducing inference costs by 35% without\ncompromising performance. Further analysis demonstrates that our methods are\neffective by maintaining reflection frequency for hard problems while\nappropriately reducing it for simpler ones without losing reflection ability.\nCodes are available at https://github.com/hexuandeng/REA-RL."}
{"id": "2505.20029", "pdf": "https://arxiv.org/pdf/2505.20029", "abs": "https://arxiv.org/abs/2505.20029", "authors": ["Subba Reddy Oota", "Akshett Jindal", "Ishani Mondal", "Khushbu Pahwa", "Satya Sai Srinath Namburi", "Manish Shrivastava", "Maneesh Singh", "Bapi S. Raju", "Manish Gupta"], "title": "Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)", "categories": ["q-bio.NC", "cs.AI", "cs.LG"], "comment": "30 pages, 22 figures, The Thirteenth International Conference on\n  Learning Representations, ICLR-2025, Singapore.\n  https://openreview.net/pdf?id=xkgfLXZ4e0", "summary": "Transformer-based language models, though not explicitly trained to mimic\nbrain recordings, have demonstrated surprising alignment with brain activity.\nProgress in these models-through increased size, instruction-tuning, and\nmultimodality-has led to better representational alignment with neural data.\nRecently, a new class of instruction-tuned multimodal LLMs (MLLMs) have\nemerged, showing remarkable zero-shot capabilities in open-ended multimodal\nvision tasks. However, it is unknown whether MLLMs, when prompted with natural\ninstructions, lead to better brain alignment and effectively capture\ninstruction-specific representations. To address this, we first investigate\nbrain alignment, i.e., measuring the degree of predictivity of neural visual\nactivity using text output response embeddings from MLLMs as participants\nengage in watching natural scenes. Experiments with 10 different instructions\nshow that MLLMs exhibit significantly better brain alignment than vision-only\nmodels and perform comparably to non-instruction-tuned multimodal models like\nCLIP. We also find that while these MLLMs are effective at generating\nhigh-quality responses suitable to the task-specific instructions, not all\ninstructions are relevant for brain alignment. Further, by varying\ninstructions, we make the MLLMs encode instruction-specific visual concepts\nrelated to the input image. This analysis shows that MLLMs effectively capture\ncount-related and recognition-related concepts, demonstrating strong alignment\nwith brain activity. Notably, the majority of the explained variance of the\nbrain encoding models is shared between MLLM embeddings of image captioning and\nother instructions. These results suggest that enhancing MLLMs' ability to\ncapture task-specific information could lead to better differentiation between\nvarious types of instructions, and thereby improving their precision in\npredicting brain responses."}
{"id": "2505.19863", "pdf": "https://arxiv.org/pdf/2505.19863", "abs": "https://arxiv.org/abs/2505.19863", "authors": ["Lukas Meyer", "Andrei-Timotei Ardelean", "Tim Weyrich", "Marc Stamminger"], "title": "FruitNeRF++: A Generalized Multi-Fruit Counting Method Utilizing Contrastive Learning and Neural Radiance Fields", "categories": ["cs.CV", "cs.LG"], "comment": "for project website, see https://meyerls.github.io/fruit_nerfpp", "summary": "We introduce FruitNeRF++, a novel fruit-counting approach that combines\ncontrastive learning with neural radiance fields to count fruits from\nunstructured input photographs of orchards. Our work is based on FruitNeRF,\nwhich employs a neural semantic field combined with a fruit-specific clustering\napproach. The requirement for adaptation for each fruit type limits the\napplicability of the method, and makes it difficult to use in practice. To lift\nthis limitation, we design a shape-agnostic multi-fruit counting framework,\nthat complements the RGB and semantic data with instance masks predicted by a\nvision foundation model. The masks are used to encode the identity of each\nfruit as instance embeddings into a neural instance field. By volumetrically\nsampling the neural fields, we extract a point cloud embedded with the instance\nfeatures, which can be clustered in a fruit-agnostic manner to obtain the fruit\ncount. We evaluate our approach using a synthetic dataset containing apples,\nplums, lemons, pears, peaches, and mangoes, as well as a real-world benchmark\napple dataset. Our results demonstrate that FruitNeRF++ is easier to control\nand compares favorably to other state-of-the-art methods."}
{"id": "2505.20030", "pdf": "https://arxiv.org/pdf/2505.20030", "abs": "https://arxiv.org/abs/2505.20030", "authors": ["Wenbo Wei", "Nicholas Chong Jia Le", "Choy Heng Lai", "Ling Feng"], "title": "Multiple Descents in Deep Learning as a Sequence of Order-Chaos Transitions", "categories": ["cs.LG", "cs.AI", "nlin.CD", "physics.comp-ph"], "comment": null, "summary": "We observe a novel 'multiple-descent' phenomenon during the training process\nof LSTM, in which the test loss goes through long cycles of up and down trend\nmultiple times after the model is overtrained. By carrying out asymptotic\nstability analysis of the models, we found that the cycles in test loss are\nclosely associated with the phase transition process between order and chaos,\nand the local optimal epochs are consistently at the critical transition point\nbetween the two phases. More importantly, the global optimal epoch occurs at\nthe first transition from order to chaos, where the 'width' of the 'edge of\nchaos' is the widest, allowing the best exploration of better weight\nconfigurations for learning."}
{"id": "2505.19866", "pdf": "https://arxiv.org/pdf/2505.19866", "abs": "https://arxiv.org/abs/2505.19866", "authors": ["Feng Xiong", "Hongling Xu", "Yifei Wang", "Runxi Cheng", "Yong Wang", "Xiangxiang Chu"], "title": "HS-STAR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Self-taught reasoners (STaRs) enhance the mathematical reasoning abilities of\nlarge language models (LLMs) by leveraging self-generated responses for\nself-training. Recent studies have incorporated reward models to guide response\nselection or decoding, aiming to obtain higher-quality data. However, they\ntypically allocate a uniform sampling budget across all problems, overlooking\nthe varying utility of problems at different difficulty levels. In this work,\nwe conduct an empirical study and find that problems near the boundary of the\nLLM's reasoning capability offer significantly greater learning utility than\nboth easy and overly difficult ones. To identify and exploit such problems, we\npropose HS-STaR, a Hierarchical Sampling framework for Self-Taught Reasoners.\nGiven a fixed sampling budget, HS-STaR first performs lightweight pre-sampling\nwith a reward-guided difficulty estimation strategy to efficiently identify\nboundary-level problems. Subsequently, it dynamically reallocates the remaining\nbudget toward these high-utility problems during a re-sampling phase,\nmaximizing the generation of valuable training data. Extensive experiments\nacross multiple reasoning benchmarks and backbone LLMs demonstrate that HS-STaR\nsignificantly outperforms other baselines without requiring additional sampling\nbudget."}
{"id": "2505.20033", "pdf": "https://arxiv.org/pdf/2505.20033", "abs": "https://arxiv.org/abs/2505.20033", "authors": ["Christoph Schuhmann", "Robert Kaczmarczyk", "Gollam Rabby", "Felix Friedrich", "Maurice Kraus", "Krishna Kalyan", "Kourosh Nadi", "Huu Nguyen", "Kristian Kersting", "Sren Auer"], "title": "EmoNet-Face: An Expert-Annotated Benchmark for Synthetic Emotion Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Effective human-AI interaction relies on AI's ability to accurately perceive\nand interpret human emotions. Current benchmarks for vision and vision-language\nmodels are severely limited, offering a narrow emotional spectrum that\noverlooks nuanced states (e.g., bitterness, intoxication) and fails to\ndistinguish subtle differences between related feelings (e.g., shame vs.\nembarrassment). Existing datasets also often use uncontrolled imagery with\noccluded faces and lack demographic diversity, risking significant bias. To\naddress these critical gaps, we introduce EmoNet Face, a comprehensive\nbenchmark suite. EmoNet Face features: (1) A novel 40-category emotion\ntaxonomy, meticulously derived from foundational research to capture finer\ndetails of human emotional experiences. (2) Three large-scale, AI-generated\ndatasets (EmoNet HQ, Binary, and Big) with explicit, full-face expressions and\ncontrolled demographic balance across ethnicity, age, and gender. (3) Rigorous,\nmulti-expert annotations for training and high-fidelity evaluation. (4) We\nbuilt EmpathicInsight-Face, a model achieving human-expert-level performance on\nour benchmark. The publicly released EmoNet Face suite - taxonomy, datasets,\nand model - provides a robust foundation for developing and evaluating AI\nsystems with a deeper understanding of human emotions."}
{"id": "2505.19912", "pdf": "https://arxiv.org/pdf/2505.19912", "abs": "https://arxiv.org/abs/2505.19912", "authors": ["Javier Marn"], "title": "APE: A Data-Centric Benchmark for Efficient LLM Adaptation in Text Summarization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present Adjacent Possible Exploration (APE), a simple yet effective method\nfor adapting large language models to specific tasks using minimal\ncomputational resources. Unlike traditional fine-tuning that requires extensive\ncompute, APE iteratively fine-tunes models on small, carefully selected data\nbatches (200 examples), retaining only improvements. On news summarization, APE\nachieves 40 percent BLEU improvement using just a T4 GPU in 60 minutes,\nmatching or exceeding more complex methods like LoRA while remaining\nconceptually simple. Our approach is particularly valuable for researchers and\npractitioners with limited computational resources. We provide open-source code\nand demonstrate APE's effectiveness through both automatic metrics and human\nevaluation. While inspired by evolutionary theory's \"adjacent possible\", APE's\ncore insight has a very practical application: small, iterative data\nperturbations can efficiently guide LLMs toward task-specific performance\nwithout expensive retraining."}
{"id": "2505.20047", "pdf": "https://arxiv.org/pdf/2505.20047", "abs": "https://arxiv.org/abs/2505.20047", "authors": ["Debargha Ganguly", "Vikash Singh", "Sreehari Sankar", "Biyao Zhang", "Xuecen Zhang", "Srinivasan Iyengar", "Xiaotian Han", "Amit Sharma", "Shivkumar Kalyanaraman", "Vipin Chaudhary"], "title": "Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks", "categories": ["cs.CL", "cs.AI", "cs.LO", "cs.SE"], "comment": null, "summary": "Large language models (LLMs) show remarkable promise for democratizing\nautomated reasoning by generating formal specifications. However, a fundamental\ntension exists: LLMs are probabilistic, while formal verification demands\ndeterministic guarantees. This paper addresses this epistemological gap by\ncomprehensively investigating failure modes and uncertainty quantification (UQ)\nin LLM-generated formal artifacts. Our systematic evaluation of five frontier\nLLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's\ndomain-specific impact on accuracy (from +34.8% on logical tasks to -44.5% on\nfactual ones), with known UQ techniques like the entropy of token probabilities\nfailing to identify these errors. We introduce a probabilistic context-free\ngrammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty\ntaxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy\nfor logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables\nselective verification, drastically reducing errors (14-100%) with minimal\nabstention, transforming LLM-driven formalization into a reliable engineering\ndiscipline."}
{"id": "2505.19925", "pdf": "https://arxiv.org/pdf/2505.19925", "abs": "https://arxiv.org/abs/2505.19925", "authors": ["Fabio Centofanti", "Mia Hubert", "Peter J. Rousseeuw"], "title": "Cellwise and Casewise Robust Covariance in High Dimensions", "categories": ["stat.ME", "cs.LG"], "comment": null, "summary": "The sample covariance matrix is a cornerstone of multivariate statistics, but\nit is highly sensitive to outliers. These can be casewise outliers, such as\ncases belonging to a different population, or cellwise outliers, which are\ndeviating cells (entries) of the data matrix. Recently some robust covariance\nestimators have been developed that can handle both types of outliers, but\ntheir computation is only feasible up to at most 20 dimensions. To remedy this\nwe propose the cellRCov method, a robust covariance estimator that\nsimultaneously handles casewise outliers, cellwise outliers, and missing data.\nIt relies on a decomposition of the covariance on principal and orthogonal\nsubspaces, leveraging recent work on robust PCA. It also employs a ridge-type\nregularization to stabilize the estimated covariance matrix. We establish some\ntheoretical properties of cellRCov, including its casewise and cellwise\ninfluence functions as well as consistency and asymptotic normality. A\nsimulation study demonstrates the superior performance of cellRCov in\ncontaminated and missing data scenarios. Furthermore, its practical utility is\nillustrated in a real-world application to anomaly detection. We also construct\nand illustrate the cellRCCA method for robust and regularized canonical\ncorrelation analysis."}
{"id": "2505.20053", "pdf": "https://arxiv.org/pdf/2505.20053", "abs": "https://arxiv.org/abs/2505.20053", "authors": ["Zheqi Lv", "Junhao Chen", "Qi Tian", "Keting Yin", "Shengyu Zhang", "Fei Wu"], "title": "Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": null, "summary": "Diffusion models have become the mainstream architecture for text-to-image\ngeneration, achieving remarkable progress in visual quality and prompt\ncontrollability. However, current inference pipelines generally lack\ninterpretable semantic supervision and correction mechanisms throughout the\ndenoising process. Most existing approaches rely solely on post-hoc scoring of\nthe final image, prompt filtering, or heuristic resampling strategies-making\nthem ineffective in providing actionable guidance for correcting the generative\ntrajectory. As a result, models often suffer from object confusion, spatial\nerrors, inaccurate counts, and missing semantic elements, severely compromising\nprompt-image alignment and image quality. To tackle these challenges, we\npropose MLLM Semantic-Corrected Ping-Pong-Ahead Diffusion (PPAD), a novel\nframework that, for the first time, introduces a Multimodal Large Language\nModel (MLLM) as a semantic observer during inference. PPAD performs real-time\nanalysis on intermediate generations, identifies latent semantic\ninconsistencies, and translates feedback into controllable signals that\nactively guide the remaining denoising steps. The framework supports both\ninference-only and training-enhanced settings, and performs semantic correction\nat only extremely few diffusion steps, offering strong generality and\nscalability. Extensive experiments demonstrate PPAD's significant improvements."}
{"id": "2505.19944", "pdf": "https://arxiv.org/pdf/2505.19944", "abs": "https://arxiv.org/abs/2505.19944", "authors": ["Naoyuki Terashita", "Yusuke Tozaki", "Hideaki Omote", "Congkha Nguyen", "Ryosuke Nakamoto", "Yuta Koreeda", "Hiroaki Ozaki"], "title": "Can Visual Encoder Learn to See Arrows?", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "This work has been accepted for poster presentation at the Second\n  Workshop on Visual Concepts in CVPR 2025", "summary": "The diagram is a visual representation of a relationship illustrated with\nedges (lines or arrows), which is widely used in industrial and scientific\ncommunication. Although recognizing diagrams is essential for vision language\nmodels (VLMs) to comprehend domain-specific knowledge, recent studies reveal\nthat many VLMs fail to identify edges in images. We hypothesize that these\nfailures stem from an over-reliance on textual and positional biases,\npreventing VLMs from learning explicit edge features. Based on this idea, we\nempirically investigate whether the image encoder in VLMs can learn edge\nrepresentation through training on a diagram dataset in which edges are biased\nneither by textual nor positional information. To this end, we conduct\ncontrastive learning on an artificially generated diagram--caption dataset to\ntrain an image encoder and evaluate its diagram-related features on three\ntasks: probing, image retrieval, and captioning. Our results show that the\nfinetuned model outperforms pretrained CLIP in all tasks and surpasses\nzero-shot GPT-4o and LLaVA-Mistral in the captioning task. These findings\nconfirm that eliminating textual and positional biases fosters accurate edge\nrecognition in VLMs, offering a promising path for advancing diagram\nunderstanding."}
{"id": "2505.20063", "pdf": "https://arxiv.org/pdf/2505.20063", "abs": "https://arxiv.org/abs/2505.20063", "authors": ["Dana Arad", "Aaron Mueller", "Yonatan Belinkov"], "title": "SAEs Are Good for Steering -- If You Select the Right Features", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Sparse Autoencoders (SAEs) have been proposed as an unsupervised approach to\nlearn a decomposition of a model's latent space. This enables useful\napplications such as steering - influencing the output of a model towards a\ndesired concept - without requiring labeled data. Current methods identify SAE\nfeatures to steer by analyzing the input tokens that activate them. However,\nrecent work has highlighted that activations alone do not fully describe the\neffect of a feature on the model's output. In this work, we draw a distinction\nbetween two types of features: input features, which mainly capture patterns in\nthe model's input, and output features, which have a human-understandable\neffect on the model's output. We propose input and output scores to\ncharacterize and locate these types of features, and show that high values for\nboth scores rarely co-occur in the same features. These findings have practical\nimplications: after filtering out features with low output scores, we obtain\n2-3x improvements when steering with SAEs, making them competitive with\nsupervised methods."}
{"id": "2505.19948", "pdf": "https://arxiv.org/pdf/2505.19948", "abs": "https://arxiv.org/abs/2505.19948", "authors": ["Gokul Adethya", "Bhanu Pratyush Mantha", "Tianyang Wang", "Xingjian Li", "Min Xu"], "title": "SaSi: A Self-augmented and Self-interpreted Deep Learning Approach for Few-shot Cryo-ET Particle Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Cryo-electron tomography (cryo-ET) has emerged as a powerful technique for\nimaging macromolecular complexes in their near-native states. However, the\nlocalization of 3D particles in cellular environments still presents a\nsignificant challenge due to low signal-to-noise ratios and missing wedge\nartifacts. Deep learning approaches have shown great potential, but they need\nhuge amounts of data, which can be a challenge in cryo-ET scenarios where\nlabeled data is often scarce. In this paper, we propose a novel Self-augmented\nand Self-interpreted (SaSi) deep learning approach towards few-shot particle\ndetection in 3D cryo-ET images. Our method builds upon self-augmentation\ntechniques to further boost data utilization and introduces a self-interpreted\nsegmentation strategy for alleviating dependency on labeled data, hence\nimproving generalization and robustness. As demonstrated by experiments\nconducted on both simulated and real-world cryo-ET datasets, the SaSi approach\nsignificantly outperforms existing state-of-the-art methods for particle\nlocalization. This research increases understanding of how to detect particles\nwith very few labels in cryo-ET and thus sets a new benchmark for few-shot\nlearning in structural biology."}
{"id": "2505.20065", "pdf": "https://arxiv.org/pdf/2505.20065", "abs": "https://arxiv.org/abs/2505.20065", "authors": ["Geon-Hyeong Kim", "Youngsoo Jang", "Yu Jin Kim", "Byoungjip Kim", "Honglak Lee", "Kyunghoon Bae", "Moontae Lee"], "title": "SafeDPO: A Simple Approach to Direct Preference Optimization with Enhanced Safety", "categories": ["cs.LG", "cs.AI"], "comment": "34 pages", "summary": "As Large Language Models (LLMs) continue to advance and find applications\nacross a growing number of fields, ensuring the safety of LLMs has become\nincreasingly critical. To address safety concerns, recent studies have proposed\nintegrating safety constraints into Reinforcement Learning from Human Feedback\n(RLHF). However, these approaches tend to be complex, as they encompass\ncomplicated procedures in RLHF along with additional steps required by the\nsafety constraints. Inspired by Direct Preference Optimization (DPO), we\nintroduce a new algorithm called SafeDPO, which is designed to directly\noptimize the safety alignment objective in a single stage of policy learning,\nwithout requiring relaxation. SafeDPO introduces only one additional\nhyperparameter to further enhance safety and requires only minor modifications\nto standard DPO. As a result, it eliminates the need to fit separate reward and\ncost models or to sample from the language model during fine-tuning, while\nstill enhancing the safety of LLMs. Finally, we demonstrate that SafeDPO\nachieves competitive performance compared to state-of-the-art safety alignment\nalgorithms, both in terms of aligning with human preferences and improving\nsafety."}
{"id": "2505.20017", "pdf": "https://arxiv.org/pdf/2505.20017", "abs": "https://arxiv.org/abs/2505.20017", "authors": ["Baptiste Abls", "Eugenio Clerico", "Hamish Flynn", "Gergely Neu"], "title": "Linear Bandits with Non-i.i.d. Noise", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We study the linear stochastic bandit problem, relaxing the standard i.i.d.\nassumption on the observation noise. As an alternative to this restrictive\nassumption, we allow the noise terms across rounds to be sub-Gaussian but\ninterdependent, with dependencies that decay over time. To address this\nsetting, we develop new confidence sequences using a recently introduced\nreduction scheme to sequential probability assignment, and use these to derive\na bandit algorithm based on the principle of optimism in the face of\nuncertainty. We provide regret bounds for the resulting algorithm, expressed in\nterms of the decay rate of the strength of dependence between observations.\nAmong other results, we show that our bounds recover the standard rates up to a\nfactor of the mixing time for geometrically mixing observation noise."}
{"id": "2505.20066", "pdf": "https://arxiv.org/pdf/2505.20066", "abs": "https://arxiv.org/abs/2505.20066", "authors": ["Hilde I Hummel", "Sandjai Bhulai", "Burooj Ghani", "Rob van der Mei"], "title": "Automated data curation for self-supervised learning in underwater acoustic analysis", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "The sustainability of the ocean ecosystem is threatened by increased levels\nof sound pollution, making monitoring crucial to understand its variability and\nimpact. Passive acoustic monitoring (PAM) systems collect a large amount of\nunderwater sound recordings, but the large volume of data makes manual analysis\nimpossible, creating the need for automation. Although machine learning offers\na potential solution, most underwater acoustic recordings are unlabeled.\nSelf-supervised learning models have demonstrated success in learning from\nlarge-scale unlabeled data in various domains like computer vision, Natural\nLanguage Processing, and audio. However, these models require large, diverse,\nand balanced datasets for training in order to generalize well. To address\nthis, a fully automated self-supervised data curation pipeline is proposed to\ncreate a diverse and balanced dataset from raw PAM data. It integrates\nAutomatic Identification System (AIS) data with recordings from various\nhydrophones in the U.S. waters. Using hierarchical k-means clustering, the raw\naudio data is sampled and then combined with AIS samples to create a balanced\nand diverse dataset. The resulting curated dataset enables the development of\nself-supervised learning models, facilitating various tasks such as monitoring\nmarine mammals and assessing sound pollution."}
{"id": "2505.20027", "pdf": "https://arxiv.org/pdf/2505.20027", "abs": "https://arxiv.org/abs/2505.20027", "authors": ["Subba Reddy Oota", "Khushbu Pahwa", "Mounika Marreddy", "Maneesh Singh", "Manish Gupta", "Bapi S. Raju"], "title": "Multi-modal brain encoding models for multi-modal stimuli", "categories": ["q-bio.NC", "cs.AI", "cs.CL", "cs.LG", "eess.AS", "eess.IV"], "comment": "26 pages, 15 figures, The Thirteenth International Conference on\n  Learning Representations, ICLR-2025, Singapore.\n  https://openreview.net/pdf?id=0dELcFHig2", "summary": "Despite participants engaging in unimodal stimuli, such as watching images or\nsilent videos, recent work has demonstrated that multi-modal Transformer models\ncan predict visual brain activity impressively well, even with incongruent\nmodality representations. This raises the question of how accurately these\nmulti-modal models can predict brain activity when participants are engaged in\nmulti-modal stimuli. As these models grow increasingly popular, their use in\nstudying neural activity provides insights into how our brains respond to such\nmulti-modal naturalistic stimuli, i.e., where it separates and integrates\ninformation across modalities through a hierarchy of early sensory regions to\nhigher cognition. We investigate this question by using multiple unimodal and\ntwo types of multi-modal models-cross-modal and jointly pretrained-to determine\nwhich type of model is more relevant to fMRI brain activity when participants\nare engaged in watching movies. We observe that both types of multi-modal\nmodels show improved alignment in several language and visual regions. This\nstudy also helps in identifying which brain regions process unimodal versus\nmulti-modal information. We further investigate the contribution of each\nmodality to multi-modal alignment by carefully removing unimodal features one\nby one from multi-modal representations, and find that there is additional\ninformation beyond the unimodal embeddings that is processed in the visual and\nlanguage regions. Based on this investigation, we find that while for\ncross-modal models, their brain alignment is partially attributed to the video\nmodality; for jointly pretrained models, it is partially attributed to both the\nvideo and audio modalities. This serves as a strong motivation for the\nneuroscience community to investigate the interpretability of these models for\ndeepening our understanding of multi-modal information processing in brain."}
{"id": "2505.20067", "pdf": "https://arxiv.org/pdf/2505.20067", "abs": "https://arxiv.org/abs/2505.20067", "authors": ["Isabelle Augenstein", "Michiel Bakker", "Tanmoy Chakraborty", "David Corney", "Emilio Ferrara", "Iryna Gurevych", "Scott Hale", "Eduard Hovy", "Heng Ji", "Irene Larraz", "Filippo Menczer", "Preslav Nakov", "Paolo Papotti", "Dhruv Sahnan", "Greta Warren", "Giovanni Zagni"], "title": "Community Moderation and the New Epistemology of Fact Checking on Social Media", "categories": ["cs.SI", "cs.AI", "cs.CY"], "comment": "1 Figure, 2 tables", "summary": "Social media platforms have traditionally relied on internal moderation teams\nand partnerships with independent fact-checking organizations to identify and\nflag misleading content. Recently, however, platforms including X (formerly\nTwitter) and Meta have shifted towards community-driven content moderation by\nlaunching their own versions of crowd-sourced fact-checking -- Community Notes.\nIf effectively scaled and governed, such crowd-checking initiatives have the\npotential to combat misinformation with increased scale and speed as\nsuccessfully as community-driven efforts once did with spam. Nevertheless,\ngeneral content moderation, especially for misinformation, is inherently more\ncomplex. Public perceptions of truth are often shaped by personal biases,\npolitical leanings, and cultural contexts, complicating consensus on what\nconstitutes misleading content. This suggests that community efforts, while\nvaluable, cannot replace the indispensable role of professional fact-checkers.\nHere we systemically examine the current approaches to misinformation detection\nacross major platforms, explore the emerging role of community-driven\nmoderation, and critically evaluate both the promises and challenges of\ncrowd-checking at scale."}
{"id": "2505.20029", "pdf": "https://arxiv.org/pdf/2505.20029", "abs": "https://arxiv.org/abs/2505.20029", "authors": ["Subba Reddy Oota", "Akshett Jindal", "Ishani Mondal", "Khushbu Pahwa", "Satya Sai Srinath Namburi", "Manish Shrivastava", "Maneesh Singh", "Bapi S. Raju", "Manish Gupta"], "title": "Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)", "categories": ["q-bio.NC", "cs.AI", "cs.LG"], "comment": "30 pages, 22 figures, The Thirteenth International Conference on\n  Learning Representations, ICLR-2025, Singapore.\n  https://openreview.net/pdf?id=xkgfLXZ4e0", "summary": "Transformer-based language models, though not explicitly trained to mimic\nbrain recordings, have demonstrated surprising alignment with brain activity.\nProgress in these models-through increased size, instruction-tuning, and\nmultimodality-has led to better representational alignment with neural data.\nRecently, a new class of instruction-tuned multimodal LLMs (MLLMs) have\nemerged, showing remarkable zero-shot capabilities in open-ended multimodal\nvision tasks. However, it is unknown whether MLLMs, when prompted with natural\ninstructions, lead to better brain alignment and effectively capture\ninstruction-specific representations. To address this, we first investigate\nbrain alignment, i.e., measuring the degree of predictivity of neural visual\nactivity using text output response embeddings from MLLMs as participants\nengage in watching natural scenes. Experiments with 10 different instructions\nshow that MLLMs exhibit significantly better brain alignment than vision-only\nmodels and perform comparably to non-instruction-tuned multimodal models like\nCLIP. We also find that while these MLLMs are effective at generating\nhigh-quality responses suitable to the task-specific instructions, not all\ninstructions are relevant for brain alignment. Further, by varying\ninstructions, we make the MLLMs encode instruction-specific visual concepts\nrelated to the input image. This analysis shows that MLLMs effectively capture\ncount-related and recognition-related concepts, demonstrating strong alignment\nwith brain activity. Notably, the majority of the explained variance of the\nbrain encoding models is shared between MLLM embeddings of image captioning and\nother instructions. These results suggest that enhancing MLLMs' ability to\ncapture task-specific information could lead to better differentiation between\nvarious types of instructions, and thereby improving their precision in\npredicting brain responses."}
{"id": "2505.20068", "pdf": "https://arxiv.org/pdf/2505.20068", "abs": "https://arxiv.org/abs/2505.20068", "authors": ["Qingyu Liang", "Jaime Banks"], "title": "On the Same Page: Dimensions of Perceived Shared Understanding in Human-AI Interaction", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Shared understanding plays a key role in the effective communication in and\nperformance of human-human interactions. With the increasingly common\nintegration of AI into human contexts, the future of personal and workplace\ninteractions will likely see human-AI interaction (HAII) in which the\nperception of shared understanding is important. Existing literature has\naddressed the processes and effects of PSU in human-human interactions, but the\nconstrual remains underexplored in HAII. To better understand PSU in HAII, we\nconducted an online survey to collect user reflections on interactions with a\nlarge language model when it sunderstanding of a situation was thought to be\nsimilar to or different from the participant's. Through inductive thematic\nanalysis, we identified eight dimensions comprising PSU in human-AI\ninteractions: Fluency, aligned operation, fluidity, outcome satisfaction,\ncontextual awareness, lack of humanlike abilities, computational limits, and\nsuspicion."}
{"id": "2505.20032", "pdf": "https://arxiv.org/pdf/2505.20032", "abs": "https://arxiv.org/abs/2505.20032", "authors": ["Fotios Lygerakis", "Ozan zdenizci", "Elmar Rckert"], "title": "ViTaPEs: Visuotactile Position Encodings for Cross-Modal Alignment in Multimodal Transformers", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Tactile sensing provides local essential information that is complementary to\nvisual perception, such as texture, compliance, and force. Despite recent\nadvances in visuotactile representation learning, challenges remain in fusing\nthese modalities and generalizing across tasks and environments without heavy\nreliance on pre-trained vision-language models. Moreover, existing methods do\nnot study positional encodings, thereby overlooking the multi-scale spatial\nreasoning needed to capture fine-grained visuotactile correlations. We\nintroduce ViTaPEs, a transformer-based framework that robustly integrates\nvisual and tactile input data to learn task-agnostic representations for\nvisuotactile perception. Our approach exploits a novel multi-scale positional\nencoding scheme to capture intra-modal structures, while simultaneously\nmodeling cross-modal cues. Unlike prior work, we provide provable guarantees in\nvisuotactile fusion, showing that our encodings are injective,\nrigid-motion-equivariant, and information-preserving, validating these\nproperties empirically. Experiments on multiple large-scale real-world datasets\nshow that ViTaPEs not only surpasses state-of-the-art baselines across various\nrecognition tasks but also demonstrates zero-shot generalization to unseen,\nout-of-domain scenarios. We further demonstrate the transfer-learning strength\nof ViTaPEs in a robotic grasping task, where it outperforms state-of-the-art\nbaselines in predicting grasp success. Project page:\nhttps://sites.google.com/view/vitapes"}
{"id": "2505.20072", "pdf": "https://arxiv.org/pdf/2505.20072", "abs": "https://arxiv.org/abs/2505.20072", "authors": ["Yige Yuan", "Teng Xiao", "Shuchang Tao", "Xue Wang", "Jinyang Gao", "Bolin Ding", "Bingbing Xu"], "title": "Incentivizing Reasoning from Weak Supervision", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance on\nreasoning-intensive tasks, but enhancing their reasoning abilities typically\nrelies on either reinforcement learning (RL) with verifiable signals or\nsupervised fine-tuning (SFT) with high-quality long chain-of-thought (CoT)\ndemonstrations, both of which are expensive. In this paper, we study a novel\nproblem of incentivizing the reasoning capacity of LLMs without expensive\nhigh-quality demonstrations and reinforcement learning. We investigate whether\nthe reasoning capabilities of LLMs can be effectively incentivized via\nsupervision from significantly weaker models. We further analyze when and why\nsuch weak supervision succeeds in eliciting reasoning abilities in stronger\nmodels. Our findings show that supervision from significantly weaker reasoners\ncan substantially improve student reasoning performance, recovering close to\n94% of the gains of expensive RL at a fraction of the cost. Experiments across\ndiverse benchmarks and model architectures demonstrate that weak reasoners can\neffectively incentivize reasoning in stronger student models, consistently\nimproving performance across a wide range of reasoning tasks. Our results\nsuggest that this simple weak-to-strong paradigm is a promising and\ngeneralizable alternative to costly methods for incentivizing strong reasoning\ncapabilities at inference-time in LLMs. The code is publicly available at\nhttps://github.com/yuanyige/W2SR."}
{"id": "2505.20079", "pdf": "https://arxiv.org/pdf/2505.20079", "abs": "https://arxiv.org/abs/2505.20079", "authors": ["Saurabh Pargal", "Abhijit A. Sane"], "title": "A fast sound power prediction tool for genset noise using machine learning", "categories": ["physics.app-ph", "cs.LG"], "comment": null, "summary": "This paper investigates the application of machine learning regression\nalgorithms Kernel Ridge Regression (KRR), Huber Regressor (HR), and Gaussian\nProcess Regression (GPR) for predicting sound power levels of gensets, offering\nsignificant value for marketing and sales teams during the early bidding\nprocess. When engine sizes and genset enclosure dimensions are tentative, and\nmeasured noise data is unavailable, these algorithms enable reliable noise\nlevel estimation for unbuilt gensets. The study utilizes high fidelity datasets\nfrom over 100 experiments conducted at Cummins Acoustics Technology Center\n(ATC) in a hemi-anechoic chamber, adhering to ISO 3744 standards. By using\nreadily available information from the bidding and initial design stages, KRR\npredicts sound power with an average accuracy of within 5 dBA. While HR and GPR\nshow slightly higher prediction errors, all models effectively capture the\noverall noise trends across various genset configurations. These findings\npresent a promising method for early-stage noise estimation in genset design."}
{"id": "2505.20081", "pdf": "https://arxiv.org/pdf/2505.20081", "abs": "https://arxiv.org/abs/2505.20081", "authors": ["Yige Yuan", "Teng Xiao", "Li Yunfan", "Bingbing Xu", "Shuchang Tao", "Yunqi Qiu", "Huawei Shen", "Xueqi Cheng"], "title": "Inference-time Alignment in Continuous Space", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Aligning large language models with human feedback at inference time has\nreceived increasing attention due to its flexibility. Existing methods rely on\ngenerating multiple responses from the base policy for search using a reward\nmodel, which can be considered as searching in a discrete response space.\nHowever, these methods struggle to explore informative candidates when the base\npolicy is weak or the candidate set is small, resulting in limited\neffectiveness. In this paper, to address this problem, we propose Simple Energy\nAdaptation ($\\textbf{SEA}$), a simple yet effective algorithm for\ninference-time alignment. In contrast to expensive search over the discrete\nspace, SEA directly adapts original responses from the base policy toward the\noptimal one via gradient-based sampling in continuous latent space.\nSpecifically, SEA formulates inference as an iterative optimization procedure\non an energy function over actions in the continuous space defined by the\noptimal policy, enabling simple and effective alignment. For instance, despite\nits simplicity, SEA outperforms the second-best baseline with a relative\nimprovement of up to $ \\textbf{77.51%}$ on AdvBench and $\\textbf{16.36%}$ on\nMATH. Our code is publicly available at https://github.com/yuanyige/SEA"}
{"id": "2505.20133", "pdf": "https://arxiv.org/pdf/2505.20133", "abs": "https://arxiv.org/abs/2505.20133", "authors": ["Konstantin Dobler", "Desmond Elliott", "Gerard de Melo"], "title": "AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Current language models rely on static vocabularies determined at pretraining\ntime, which can lead to decreased performance and increased computational cost\nfor domains underrepresented in the original vocabulary. New tokens can be\nadded to solve this problem, when coupled with a good initialization for their\nnew embeddings. However, existing embedding initialization methods either\nrequire expensive further training or pretraining of additional modules. In\nthis paper, we propose AweDist and show that by distilling representations\nobtained using the original tokenization, we can quickly learn high-quality\ninput embeddings for new tokens. Experimental results with a wide range of\nopen-weight models show that AweDist is able to outperform even strong\nbaselines."}
{"id": "2505.20085", "pdf": "https://arxiv.org/pdf/2505.20085", "abs": "https://arxiv.org/abs/2505.20085", "authors": ["Eleonora Cappuccio", "Andrea Esposito", "Francesco Greco", "Giuseppe Desolda", "Rosa Lanzilotti", "Salvatore Rinzivillo"], "title": "Explanation User Interfaces: A Systematic Literature Review", "categories": ["cs.HC", "cs.AI", "A.1"], "comment": "First version", "summary": "Artificial Intelligence (AI) is one of the major technological advancements\nof this century, bearing incredible potential for users through AI-powered\napplications and tools in numerous domains. Being often black-box (i.e., its\ndecision-making process is unintelligible), developers typically resort to\neXplainable Artificial Intelligence (XAI) techniques to interpret the behaviour\nof AI models to produce systems that are transparent, fair, reliable, and\ntrustworthy. However, presenting explanations to the user is not trivial and is\noften left as a secondary aspect of the system's design process, leading to AI\nsystems that are not useful to end-users. This paper presents a Systematic\nLiterature Review on Explanation User Interfaces (XUIs) to gain a deeper\nunderstanding of the solutions and design guidelines employed in the academic\nliterature to effectively present explanations to users. To improve the\ncontribution and real-world impact of this survey, we also present a framework\nfor Human-cEnteRed developMent of Explainable user interfaceS (HERMES) to guide\npractitioners and academics in the design and evaluation of XUIs."}
{"id": "2505.20144", "pdf": "https://arxiv.org/pdf/2505.20144", "abs": "https://arxiv.org/abs/2505.20144", "authors": ["Jian Gu", "Aldeida Aleti", "Chunyang Chen", "Hongyu Zhang"], "title": "SeMe: Training-Free Language Model Merging via Semantic Alignment", "categories": ["cs.CL", "cs.LG"], "comment": "an early-stage version", "summary": "Despite the remarkable capabilities of Language Models (LMs) across diverse\ntasks, no single model consistently outperforms others, necessitating efficient\nmethods to combine their strengths without expensive retraining. Existing model\nmerging techniques, such as parameter averaging and task-guided fusion, often\nrely on data-dependent computations or fail to preserve internal knowledge,\nlimiting their robustness and scalability. We introduce SeMe (Semantic-based\nMerging), a novel, data-free, and training-free approach that leverages latent\nsemantic alignment to merge LMs at a fine-grained, layer-wise level. Unlike\nprior work, SeMe not only preserves model behaviors but also explicitly\nstabilizes internal knowledge, addressing a critical gap in LM fusion. Through\nextensive experiments across diverse architectures and tasks, we demonstrate\nthat SeMe outperforms existing methods in both performance and efficiency while\neliminating reliance on external data. Our work establishes a new paradigm for\nknowledge-aware model merging and provides insights into the semantic structure\nof LMs, paving the way for more scalable and interpretable model composition."}
{"id": "2505.20089", "pdf": "https://arxiv.org/pdf/2505.20089", "abs": "https://arxiv.org/abs/2505.20089", "authors": ["Ruiyi Fang", "Bingheng Li", "Jingyu Zhao", "Ruizhi Pu", "Qiuhao Zeng", "Gezheng Xu", "Charles Ling", "Boyu Wang"], "title": "Homophily Enhanced Graph Domain Adaptation", "categories": ["cs.SI", "cs.AI"], "comment": "Accepted at ICML2025", "summary": "Graph Domain Adaptation (GDA) transfers knowledge from labeled source graphs\nto unlabeled target graphs, addressing the challenge of label scarcity. In this\npaper, we highlight the significance of graph homophily, a pivotal factor for\ngraph domain alignment, which, however, has long been overlooked in existing\napproaches. Specifically, our analysis first reveals that homophily\ndiscrepancies exist in benchmarks. Moreover, we also show that homophily\ndiscrepancies degrade GDA performance from both empirical and theoretical\naspects, which further underscores the importance of homophily alignment in\nGDA. Inspired by this finding, we propose a novel homophily alignment algorithm\nthat employs mixed filters to smooth graph signals, thereby effectively\ncapturing and mitigating homophily discrepancies between graphs. Experimental\nresults on a variety of benchmarks verify the effectiveness of our method."}
{"id": "2505.20166", "pdf": "https://arxiv.org/pdf/2505.20166", "abs": "https://arxiv.org/abs/2505.20166", "authors": ["Chun-Yi Kuan", "Hung-yi Lee"], "title": "From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "comment": "Project Website: https://kuan2jiu99.github.io/Balsa", "summary": "Audio-aware large language models (ALLMs) have recently made great strides in\nunderstanding and processing audio inputs. These models are typically adapted\nfrom text-based large language models (LLMs) through additional training on\naudio-related tasks. However, this adaptation process presents two major\nlimitations. First, ALLMs often suffer from catastrophic forgetting, where\nimportant textual capabilities such as instruction-following are lost after\ntraining on audio data. In some cases, models may even hallucinate sounds that\nare not present in the input audio, raising concerns about their reliability.\nSecond, achieving cross-modal alignment between audio and language typically\nrelies on large collections of task-specific question-answer pairs for\ninstruction tuning, making the process resource-intensive. To address these\nissues, we leverage the backbone LLMs from ALLMs to synthesize general-purpose\ncaption-style alignment data. We refer to this process as bootstrapping\naudio-language alignment via synthetic data generation from backbone LLMs\n(BALSa). Building on BALSa, we introduce LISTEN (Learning to Identify Sounds\nThrough Extended Negative Samples), a contrastive-like training method designed\nto improve ALLMs' ability to distinguish between present and absent sounds. We\nfurther extend BALSa to multi-audio scenarios, where the model either explains\nthe differences between audio inputs or produces a unified caption that\ndescribes them all, thereby enhancing audio-language alignment. Experimental\nresults indicate that our method effectively mitigates audio hallucinations\nwhile reliably maintaining strong performance in audio understanding,\nreasoning, and instruction-following skills. Moreover, incorporating\nmulti-audio training further enhances the model's comprehension and reasoning\ncapabilities. Overall, BALSa offers an efficient and scalable approach to the\ndevelopment of ALLMs."}
{"id": "2505.20096", "pdf": "https://arxiv.org/pdf/2505.20096", "abs": "https://arxiv.org/abs/2505.20096", "authors": ["Thang Nguyen", "Peter Chin", "Yu-Wing Tai"], "title": "MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation\n(RAG) that addresses the inherent ambiguities and reasoning challenges in\ncomplex information-seeking tasks. Unlike conventional RAG methods that rely on\neither end-to-end fine-tuning or isolated component enhancements, MA-RAG\norchestrates a collaborative set of specialized AI agents: Planner, Step\nDefiner, Extractor, and QA Agents, to tackle each stage of the RAG pipeline\nwith task-aware reasoning. Ambiguities may arise from underspecified queries,\nsparse or indirect evidence in retrieved documents, or the need to integrate\ninformation scattered across multiple sources. MA-RAG mitigates these\nchallenges by decomposing the problem into subtasks, such as query\ndisambiguation, evidence extraction, and answer synthesis, and dispatching them\nto dedicated agents equipped with chain-of-thought prompting. These agents\ncommunicate intermediate reasoning and progressively refine the retrieval and\nsynthesis process. Our design allows fine-grained control over information flow\nwithout any model fine-tuning. Crucially, agents are invoked on demand,\nenabling a dynamic and efficient workflow that avoids unnecessary computation.\nThis modular and reasoning-driven architecture enables MA-RAG to deliver\nrobust, interpretable results. Experiments on multi-hop and ambiguous QA\nbenchmarks demonstrate that MA-RAG outperforms state-of-the-art training-free\nbaselines and rivals fine-tuned systems, validating the effectiveness of\ncollaborative agent-based reasoning in RAG."}
{"id": "2505.20176", "pdf": "https://arxiv.org/pdf/2505.20176", "abs": "https://arxiv.org/abs/2505.20176", "authors": ["Alkis Koudounas", "Moreno La Quatra", "Eliana Pastor", "Sabato Marco Siniscalchi", "Elena Baralis"], "title": "\"KAN you hear me?\" Exploring Kolmogorov-Arnold Networks for Spoken Language Understanding", "categories": ["cs.CL", "cs.LG", "eess.AS"], "comment": "Accepted at INTERSPEECH 2025", "summary": "Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising\nalternative to traditional neural architectures, yet their application to\nspeech processing remains under explored. This work presents the first\ninvestigation of KANs for Spoken Language Understanding (SLU) tasks. We\nexperiment with 2D-CNN models on two datasets, integrating KAN layers in five\ndifferent configurations within the dense block. The best-performing setup,\nwhich places a KAN layer between two linear layers, is directly applied to\ntransformer-based models and evaluated on five SLU datasets with increasing\ncomplexity. Our results show that KAN layers can effectively replace the linear\nlayers, achieving comparable or superior performance in most cases. Finally, we\nprovide insights into how KAN and linear layers on top of transformers\ndifferently attend to input regions of the raw waveforms."}
{"id": "2505.20099", "pdf": "https://arxiv.org/pdf/2505.20099", "abs": "https://arxiv.org/abs/2505.20099", "authors": ["Chuangtao Ma", "Yongrui Chen", "Tianxing Wu", "Arijit Khan", "Haofen Wang"], "title": "Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Under Review", "summary": "Large language models (LLMs) have demonstrated remarkable performance on\nquestion-answering (QA) tasks because of their superior capabilities in natural\nlanguage understanding and generation. However, LLM-based QA struggles with\ncomplex QA tasks due to poor reasoning capacity, outdated knowledge, and\nhallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs)\nfor QA to address the above challenges. In this survey, we propose a new\nstructured taxonomy that categorizes the methodology of synthesizing LLMs and\nKGs for QA according to the categories of QA and the KG's role when integrating\nwith LLMs. We systematically survey state-of-the-art advances in synthesizing\nLLMs and KGs for QA and compare and analyze these approaches in terms of\nstrength, limitations, and KG requirements. We then align the approaches with\nQA and discuss how these approaches address the main challenges of different\ncomplex QA. Finally, we summarize the advancements, evaluation metrics, and\nbenchmark datasets and highlight open challenges and opportunities."}
{"id": "2505.20178", "pdf": "https://arxiv.org/pdf/2505.20178", "abs": "https://arxiv.org/abs/2505.20178", "authors": ["Pranav Mani", "Peng Xu", "Zachary C. Lipton", "Michael Oberst"], "title": "No Free Lunch: Non-Asymptotic Analysis of Prediction-Powered Inference", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Prediction-Powered Inference (PPI) is a popular strategy for combining\ngold-standard and possibly noisy pseudo-labels to perform statistical\nestimation. Prior work has shown an asymptotic \"free lunch\" for PPI++, an\nadaptive form of PPI, showing that the *asymptotic* variance of PPI++ is always\nless than or equal to the variance obtained from using gold-standard labels\nalone. Notably, this result holds *regardless of the quality of the\npseudo-labels*. In this work, we demystify this result by conducting an exact\nfinite-sample analysis of the estimation error of PPI++ on the mean estimation\nproblem. We give a \"no free lunch\" result, characterizing the settings (and\nsample sizes) where PPI++ has provably worse estimation error than using\ngold-standard labels alone. Specifically, PPI++ will outperform if and only if\nthe correlation between pseudo- and gold-standard is above a certain level that\ndepends on the number of labeled samples ($n$). In some cases our results\nsimplify considerably: For Gaussian data, the correlation must be at least\n$1/\\sqrt{n - 2}$ in order to see improvement, and a similar result holds for\nbinary labels. In experiments, we illustrate that our theoretical findings hold\non real-world datasets, and give insights into trade-offs between single-sample\nand sample-splitting variants of PPI++."}
{"id": "2505.20100", "pdf": "https://arxiv.org/pdf/2505.20100", "abs": "https://arxiv.org/abs/2505.20100", "authors": ["Fengyuan Sun", "Leqi Shen", "Hui Chen", "Sicheng Zhao", "Jungong Han", "Guiguang Ding"], "title": "AdaTP: Attention-Debiased Token Pruning for Video Large Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video Large Language Models (Video LLMs) have achieved remarkable results in\nvideo understanding tasks. However, they often suffer from heavy computational\noverhead due to the large number of visual tokens generated from multiple video\nframes. Existing visual token compression methods often rely on attention\nscores from language models as guidance. However, these scores exhibit inherent\nbiases: global bias reflects a tendency to focus on the two ends of the visual\ntoken sequence, while local bias leads to an over-concentration on the same\nspatial positions across different frames. To address the issue of attention\nbias, we propose $\\textbf{A}$ttention-$\\textbf{D}$ebi$\\textbf{a}$sed\n$\\textbf{T}$oken $\\textbf{P}$runing for Video Large Language Models\n($\\textbf{AdaTP}$), a novel token pruning pipeline for Video LLMs. AdaTP\nintegrates two dedicated debiasing modules into the pipeline, targeting global\nattention bias and local attention bias, respectively. Without the need for\nadditional training, our method significantly reduces the computational\noverhead of Video LLMs while retaining the performance of vanilla models.\nExtensive evaluation shows that AdaTP achieves state-of-the-art performance in\nvarious commonly used video understanding benchmarks. In particular, on\nLLaVA-OneVision-7B, AdaTP maintains performance without degradation while using\nonly up to $27.3\\%$ FLOPs compared to the vanilla model. Our code will be\nreleased soon."}
{"id": "2505.20189", "pdf": "https://arxiv.org/pdf/2505.20189", "abs": "https://arxiv.org/abs/2505.20189", "authors": ["Syamantak Kumar", "Daogao Liu", "Kevin Tian", "Chutong Yang"], "title": "Private Geometric Median in Nearly-Linear Time", "categories": ["cs.DS", "cs.CR", "cs.LG", "stat.ML"], "comment": null, "summary": "Estimating the geometric median of a dataset is a robust counterpart to mean\nestimation, and is a fundamental problem in computational geometry. Recently,\n[HSU24] gave an $(\\varepsilon, \\delta)$-differentially private algorithm\nobtaining an $\\alpha$-multiplicative approximation to the geometric median\nobjective, $\\frac 1 n \\sum_{i \\in [n]} \\|\\cdot - \\mathbf{x}_i\\|$, given a\ndataset $\\mathcal{D} := \\{\\mathbf{x}_i\\}_{i \\in [n]} \\subset \\mathbb{R}^d$.\nTheir algorithm requires $n \\gtrsim \\sqrt d \\cdot \\frac 1 {\\alpha\\varepsilon}$\nsamples, which they prove is information-theoretically optimal. This result is\nsurprising because its error scales with the \\emph{effective radius} of\n$\\mathcal{D}$ (i.e., of a ball capturing most points), rather than the\nworst-case radius. We give an improved algorithm that obtains the same\napproximation quality, also using $n \\gtrsim \\sqrt d \\cdot \\frac 1\n{\\alpha\\epsilon}$ samples, but in time $\\widetilde{O}(nd + \\frac d\n{\\alpha^2})$. Our runtime is nearly-linear, plus the cost of the cheapest\nnon-private first-order method due to [CLM+16]. To achieve our results, we use\nsubsampling and geometric aggregation tools inspired by FriendlyCore [TCK+22]\nto speed up the \"warm start\" component of the [HSU24] algorithm, combined with\na careful custom analysis of DP-SGD's sensitivity for the geometric median\nobjective."}
{"id": "2505.20109", "pdf": "https://arxiv.org/pdf/2505.20109", "abs": "https://arxiv.org/abs/2505.20109", "authors": ["June-Woo Kim", "Wonkyo Oh", "Haram Yoon", "Sung-Hoon Yoon", "Dae-Jin Kim", "Dong-Ho Lee", "Sang-Yeol Lee", "Chan-Mo Yang"], "title": "Language-Agnostic Suicidal Risk Detection Using Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to InterSpeech 2025", "summary": "Suicidal risk detection in adolescents is a critical challenge, yet existing\nmethods rely on language-specific models, limiting scalability and\ngeneralization. This study introduces a novel language-agnostic framework for\nsuicidal risk assessment with large language models (LLMs). We generate Chinese\ntranscripts from speech using an ASR model and then employ LLMs with\nprompt-based queries to extract suicidal risk-related features from these\ntranscripts. The extracted features are retained in both Chinese and English to\nenable cross-linguistic analysis and then used to fine-tune corresponding\npretrained language models independently. Experimental results show that our\nmethod achieves performance comparable to direct fine-tuning with ASR results\nor to models trained solely on Chinese suicidal risk-related features,\ndemonstrating its potential to overcome language constraints and improve the\nrobustness of suicidal risk assessment."}
{"id": "2505.20196", "pdf": "https://arxiv.org/pdf/2505.20196", "abs": "https://arxiv.org/abs/2505.20196", "authors": ["Yuetai Li", "Zhangchen Xu", "Fengqing Jiang", "Bhaskar Ramasubramanian", "Luyao Niu", "Bill Yuchen Lin", "Xiang Yue", "Radha Poovendran"], "title": "Temporal Sampling for Forgotten Reasoning in LLMs", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Fine-tuning large language models (LLMs) is intended to improve their\nreasoning capabilities, yet we uncover a counterintuitive effect: models often\nforget how to solve problems they previously answered correctly during\ntraining. We term this phenomenon temporal forgetting and show that it is\nwidespread across model sizes, fine-tuning methods (both Reinforcement Learning\nand Supervised Fine-Tuning), and multiple reasoning benchmarks. To address this\ngap, we introduce Temporal Sampling, a simple decoding strategy that draws\noutputs from multiple checkpoints along the training trajectory. This approach\nrecovers forgotten solutions without retraining or ensembling, and leads to\nsubstantial improvements in reasoning performance, gains from 4 to 19 points in\nPass@k and consistent gains in Majority@k across several benchmarks. We further\nextend our method to LoRA-adapted models, demonstrating that storing only\nadapter weights across checkpoints achieves similar benefits with minimal\nstorage cost. By leveraging the temporal diversity inherent in training,\nTemporal Sampling offers a practical, compute-efficient way to surface hidden\nreasoning ability and rethink how we evaluate LLMs."}
{"id": "2505.20110", "pdf": "https://arxiv.org/pdf/2505.20110", "abs": "https://arxiv.org/abs/2505.20110", "authors": ["Ruishuo Chen", "Xun Wang", "Rui Hu", "Zhuoran Li", "Longbo Huang"], "title": "Proxy-Free GFlowNet", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Generative Flow Networks (GFlowNets) are a promising class of generative\nmodels designed to sample diverse, high-reward structures by modeling\ndistributions over compositional objects. In many real-world applications,\nobtaining the reward function for such objects is expensive, time-consuming, or\nrequires human input, making it necessary to train GFlowNets from historical\ndatasets. Most existing methods adopt a model-based approach, learning a proxy\nmodel from the dataset to approximate the reward function. However, this\nstrategy inherently ties the quality of the learned policy to the accuracy of\nthe proxy, introducing additional complexity and uncertainty into the training\nprocess. To overcome these limitations, we propose \\textbf{Trajectory-Distilled\nGFlowNet (TD-GFN)}, a \\emph{proxy-free} training framework that eliminates the\nneed for out-of-dataset reward queries. Our method is motivated by the key\nobservation that different edges in the associated directed acyclic graph (DAG)\ncontribute unequally to effective policy learning. TD-GFN leverages inverse\nreinforcement learning to estimate edge-level rewards from the offline dataset,\nwhich are then used to ingeniously prune the DAG and guide backward trajectory\nsampling during training. This approach directs the policy toward high-reward\nregions while reducing the complexity of model fitting. Empirical results\nacross multiple tasks show that TD-GFN trains both efficiently and reliably,\nsignificantly outperforming existing baselines in convergence speed and sample\nquality."}
{"id": "2505.20219", "pdf": "https://arxiv.org/pdf/2505.20219", "abs": "https://arxiv.org/abs/2505.20219", "authors": ["Francesco Orabona", "Ryan D'Orazio"], "title": "New Perspectives on the Polyak Stepsize: Surrogate Functions and Negative Results", "categories": ["math.OC", "cs.LG"], "comment": null, "summary": "The Polyak stepsize has been proven to be a fundamental stepsize in convex\noptimization, giving near optimal gradient descent rates across a wide range of\nassumptions. The universality of the Polyak stepsize has also inspired many\nstochastic variants, with theoretical guarantees and strong empirical\nperformance. Despite the many theoretical results, our understanding of the\nconvergence properties and shortcomings of the Polyak stepsize or its variants\nis both incomplete and fractured across different analyses. We propose a new,\nunified, and simple perspective for the Polyak stepsize and its variants as\ngradient descent on a surrogate loss. We show that each variant is equivalent\nto minimize a surrogate function with stepsizes that adapt to a guaranteed\nlocal curvature. Our general surrogate loss perspective is then used to provide\na unified analysis of existing variants across different assumptions. Moreover,\nwe show a number of negative results proving that the non-convergence results\nin some of the upper bounds is indeed real."}
{"id": "2505.20112", "pdf": "https://arxiv.org/pdf/2505.20112", "abs": "https://arxiv.org/abs/2505.20112", "authors": ["Haolei Bai", "Siyong Jian", "Tuo Liang", "Yu Yin", "Huan Wang"], "title": "ResSVD: Residual Compensated SVD for Large Language Model Compression", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities in a\nwide range of downstream natural language processing tasks. Nevertheless, their\nconsiderable sizes and memory demands hinder practical deployment, underscoring\nthe importance of developing efficient compression strategies. Singular value\ndecomposition (SVD) decomposes a matrix into orthogonal components, enabling\nefficient low-rank approximation. This is particularly suitable for LLM\ncompression, where weight matrices often exhibit significant redundancy.\nHowever, current SVD-based methods neglect the residual matrix from truncation,\nresulting in significant truncation loss. Additionally, compressing all layers\nof the model results in severe performance degradation. To overcome these\nlimitations, we propose ResSVD, a new post-training SVD-based LLM compression\nmethod. Specifically, we leverage the residual matrix generated during the\ntruncation process to reduce truncation loss. Moreover, under a fixed overall\ncompression ratio, we selectively compress the last few layers of the model,\nwhich mitigates error propagation and significantly improves the performance of\ncompressed models.Comprehensive evaluations of ResSVD on diverse LLM families\nand multiple benchmark datasets indicate that ResSVD consistently achieves\nsuperior performance over existing counterpart methods, demonstrating its\npractical effectiveness."}
{"id": "2505.20223", "pdf": "https://arxiv.org/pdf/2505.20223", "abs": "https://arxiv.org/abs/2505.20223", "authors": ["Yixin Cui", "Haotian Lin", "Shuo Yang", "Yixiao Wang", "Yanjun Huang", "Hong Chen"], "title": "Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects", "categories": ["cs.RO", "cs.LG"], "comment": "18 pages, 6 figures", "summary": "The rapid evolution of large language models in natural language processing\nhas substantially elevated their semantic understanding and logical reasoning\ncapabilities. Such proficiencies have been leveraged in autonomous driving\nsystems, contributing to significant improvements in system performance. Models\nsuch as OpenAI o1 and DeepSeek-R1, leverage Chain-of-Thought (CoT) reasoning,\nan advanced cognitive method that simulates human thinking processes,\ndemonstrating remarkable reasoning capabilities in complex tasks. By\nstructuring complex driving scenarios within a systematic reasoning framework,\nthis approach has emerged as a prominent research focus in autonomous driving,\nsubstantially improving the system's ability to handle challenging cases. This\npaper investigates how CoT methods improve the reasoning abilities of\nautonomous driving models. Based on a comprehensive literature review, we\npresent a systematic analysis of the motivations, methodologies, challenges,\nand future research directions of CoT in autonomous driving. Furthermore, we\npropose the insight of combining CoT with self-learning to facilitate\nself-evolution in driving systems. To ensure the relevance and timeliness of\nthis study, we have compiled a dynamic repository of literature and open-source\nprojects, diligently updated to incorporate forefront developments. The\nrepository is publicly available at\nhttps://github.com/cuiyx1720/Awesome-CoT4AD."}
{"id": "2505.20113", "pdf": "https://arxiv.org/pdf/2505.20113", "abs": "https://arxiv.org/abs/2505.20113", "authors": ["Cristian Santini", "Laura Melosi", "Emanuele Frontoni"], "title": "Named Entity Recognition in Historical Italian: The Case of Giacomo Leopardi's Zibaldone", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The increased digitization of world's textual heritage poses significant\nchallenges for both computer science and literary studies. Overall, there is an\nurgent need of computational techniques able to adapt to the challenges of\nhistorical texts, such as orthographic and spelling variations, fragmentary\nstructure and digitization errors. The rise of large language models (LLMs) has\nrevolutionized natural language processing, suggesting promising applications\nfor Named Entity Recognition (NER) on historical documents. In spite of this,\nno thorough evaluation has been proposed for Italian texts. This research tries\nto fill the gap by proposing a new challenging dataset for entity extraction\nbased on a corpus of 19th century scholarly notes, i.e. Giacomo Leopardi's\nZibaldone (1898), containing 2,899 references to people, locations and literary\nworks. This dataset was used to carry out reproducible experiments with both\ndomain-specific BERT-based models and state-of-the-art LLMs such as LLaMa3.1.\nResults show that instruction-tuned models encounter multiple difficulties\nhandling historical humanistic texts, while fine-tuned NER models offer more\nrobust performance even with challenging entity types such as bibliographic\nreferences."}
{"id": "2505.20225", "pdf": "https://arxiv.org/pdf/2505.20225", "abs": "https://arxiv.org/abs/2505.20225", "authors": ["Hao Kang", "Zichun Yu", "Chenyan Xiong"], "title": "FLAME-MoE: A Transparent End-to-End Research Platform for Mixture-of-Experts Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "All code, training logs, and model checkpoints are available at\n  https://github.com/cmu-flame/FLAME-MoE", "summary": "Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4\nincreasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong\nefficiency-performance trade-offs by activating only a fraction of the model\nper token. Yet academic researchers still lack a fully open, end-to-end MoE\nplatform for investigating scaling, routing, and expert behavior. We release\nFLAME-MoE, a completely open-source research suite composed of seven\ndecoder-only models, ranging from 38M to 1.7B active parameters, whose\narchitecture--64 experts with top-8 gating and 2 shared experts--closely\nreflects modern production LLMs. All training data pipelines, scripts, logs,\nand checkpoints are publicly available to enable reproducible experimentation.\nAcross six evaluation tasks, FLAME-MoE improves average accuracy by up to 3.4\npoints over dense baselines trained with identical FLOPs. Leveraging full\ntraining trace transparency, we present initial analyses showing that (i)\nexperts increasingly specialize on distinct token subsets, (ii) co-activation\nmatrices remain sparse, reflecting diverse expert usage, and (iii) routing\nbehavior stabilizes early in training. All code, training logs, and model\ncheckpoints are available at https://github.com/cmu-flame/FLAME-MoE."}
{"id": "2505.20132", "pdf": "https://arxiv.org/pdf/2505.20132", "abs": "https://arxiv.org/abs/2505.20132", "authors": ["Safa Hamreras", "Sukhbinder Singh", "Romn Ors"], "title": "Tensorization is a powerful but underexplored tool for compression and interpretability of neural networks", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": "This article has been prepared for submission as a \"Position paper\"\n  following the guidelines provided at\n  https://neurips.cc/Conferences/2025/CallForPositionPapers", "summary": "Tensorizing a neural network involves reshaping some or all of its dense\nweight matrices into higher-order tensors and approximating them using low-rank\ntensor network decompositions. This technique has shown promise as a model\ncompression strategy for large-scale neural networks. However, despite\nencouraging empirical results, tensorized neural networks (TNNs) remain\nunderutilized in mainstream deep learning. In this position paper, we offer a\nperspective on both the potential and current limitations of TNNs. We argue\nthat TNNs represent a powerful yet underexplored framework for deep\nlearning--one that deserves greater attention from both engineering and\ntheoretical communities. Beyond compression, we highlight the value of TNNs as\na flexible class of architectures with distinctive scaling properties and\nincreased interpretability. A central feature of TNNs is the presence of bond\nindices, which introduce new latent spaces not found in conventional networks.\nThese internal representations may provide deeper insight into the evolution of\nfeatures across layers, potentially advancing the goals of mechanistic\ninterpretability. We conclude by outlining several key research directions\naimed at overcoming the practical barriers to scaling and adopting TNNs in\nmodern deep learning workflows."}
{"id": "2505.20250", "pdf": "https://arxiv.org/pdf/2505.20250", "abs": "https://arxiv.org/abs/2505.20250", "authors": ["Chirag Garg", "Sayeef Salahuddin"], "title": "Efficient Optimization Accelerator Framework for Multistate Ising Problems", "categories": ["cs.AR", "cs.DC", "cs.ET", "cs.LG", "stat.CO"], "comment": "6 page main text, 4 main figures, 1 main table, 1 page supplementary,\n  2 supplementary figures,", "summary": "Ising Machines are a prominent class of hardware architectures that aim to\nsolve NP-hard combinatorial optimization problems. These machines consist of a\nnetwork of interacting binary spins/neurons that evolve to represent the\noptimum ground state energy solution. Generally, combinatorial problems are\ntransformed into quadratic unconstrained binary optimization (QUBO) form to\nharness the computational efficiency of these Ising machines. However, this\ntransformation, especially for multi-state problems, often leads to a more\ncomplex exploration landscape than the original problem, thus severely\nimpacting the solution quality. To address this challenge, we model the spin\ninteractions as a generalized boolean logic function to significantly reduce\nthe exploration space. We benchmark the graph coloring problem from the class\nof multi-state NP-hard optimization using probabilistic Ising solvers to\nillustrate the effectiveness of our framework. The proposed methodology\nachieves similar accuracy compared to state-of-the-art heuristics and machine\nlearning algorithms, and demonstrates significant improvement over the existing\nIsing methods. Additionally, we demonstrate that combining parallel tempering\nwith our existing framework further reduces the coloring error by up to 50%\ncompared to the conventionally used Gibbs sampling algorithm. We also design a\n1024-neuron all-to-all connected probabilistic Ising accelerator that shows up\nto 10000x performance acceleration compared to heuristics while reducing the\nnumber of required physical neurons by 1.5-4x compared to conventional Ising\nmachines. Indeed, this accelerator solution demonstrates improvement across all\nmetrics over the current methods, i.e., energy, performance, area, and solution\nquality. Thus, this work expands the potential of existing Ising hardware to\nsolve a broad class of these multistate optimization problems."}
{"id": "2505.20137", "pdf": "https://arxiv.org/pdf/2505.20137", "abs": "https://arxiv.org/abs/2505.20137", "authors": ["Cdric Goemaere", "Gaspard Oliviers", "Rafal Bogacz", "Thomas Demeester"], "title": "Error Optimization: Overcoming Exponential Signal Decay in Deep Predictive Coding Networks", "categories": ["cs.LG", "cs.AI"], "comment": "All code available at\n  https://github.com/cgoemaere/pc_error_optimization", "summary": "Predictive Coding (PC) offers a biologically plausible alternative to\nbackpropagation for neural network training, yet struggles with deeper\narchitectures. This paper identifies the root cause: an inherent signal decay\nproblem where gradients attenuate exponentially with depth, becoming\ncomputationally negligible due to numerical precision constraints. To address\nthis fundamental limitation, we introduce Error Optimization (EO), a novel\nreparameterization that preserves PC's theoretical properties while eliminating\nsignal decay. By optimizing over prediction errors rather than states, EO\nenables signals to reach all layers simultaneously and without attenuation,\nconverging orders of magnitude faster than standard PC. Experiments across\nmultiple architectures and datasets demonstrate that EO matches\nbackpropagation's performance even for deeper models where conventional PC\nstruggles. Besides practical improvements, our work provides theoretical\ninsight into PC dynamics and establishes a foundation for scaling\nbiologically-inspired learning to deeper architectures on digital hardware and\nbeyond."}
{"id": "2505.20259", "pdf": "https://arxiv.org/pdf/2505.20259", "abs": "https://arxiv.org/abs/2505.20259", "authors": ["Haoyu Wang", "Zeyu Qin", "Yifei Zhao", "Chao Du", "Min Lin", "Xueqian Wang", "Tianyu Pang"], "title": "Lifelong Safety Alignment for Language Models", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "LLMs have made impressive progress, but their growing capabilities also\nexpose them to highly flexible jailbreaking attacks designed to bypass safety\nalignment. While many existing defenses focus on known types of attacks, it is\nmore critical to prepare LLMs for unseen attacks that may arise during\ndeployment. To address this, we propose a lifelong safety alignment framework\nthat enables LLMs to continuously adapt to new and evolving jailbreaking\nstrategies. Our framework introduces a competitive setup between two\ncomponents: a Meta-Attacker, trained to actively discover novel jailbreaking\nstrategies, and a Defender, trained to resist them. To effectively warm up the\nMeta-Attacker, we first leverage the GPT-4o API to extract key insights from a\nlarge collection of jailbreak-related research papers. Through iterative\ntraining, the first iteration Meta-Attacker achieves a 73% attack success rate\n(ASR) on RR and a 57% transfer ASR on LAT using only single-turn attacks.\nMeanwhile, the Defender progressively improves its robustness and ultimately\nreduces the Meta-Attacker's success rate to just 7%, enabling safer and more\nreliable deployment of LLMs in open-ended environments. The code is available\nat https://github.com/sail-sg/LifelongSafetyAlignment."}
{"id": "2505.20139", "pdf": "https://arxiv.org/pdf/2505.20139", "abs": "https://arxiv.org/abs/2505.20139", "authors": ["Jialin Yang", "Dongfu Jiang", "Lipeng He", "Sherman Siu", "Yuxuan Zhang", "Disen Liao", "Zhuofeng Li", "Huaye Zeng", "Yiming Jia", "Haozhe Wang", "Benjamin Schneider", "Chi Ruan", "Wentao Ma", "Zhiheng Lyu", "Yifei Wang", "Yi Lu", "Quy Duc Do", "Ziyan Jiang", "Ping Nie", "Wenhu Chen"], "title": "StructEval: Benchmarking LLMs' Capabilities to Generate Structural Outputs", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "16 pages, 9 figures, 13 tables", "summary": "As Large Language Models (LLMs) become integral to software development\nworkflows, their ability to generate structured outputs has become critically\nimportant. We introduce StructEval, a comprehensive benchmark for evaluating\nLLMs' capabilities in producing both non-renderable (JSON, YAML, CSV) and\nrenderable (HTML, React, SVG) structured formats. Unlike prior benchmarks,\nStructEval systematically evaluates structural fidelity across diverse formats\nthrough two paradigms: 1) generation tasks, producing structured output from\nnatural language prompts, and 2) conversion tasks, translating between\nstructured formats. Our benchmark encompasses 18 formats and 44 types of task,\nwith novel metrics for format adherence and structural correctness. Results\nreveal significant performance gaps, even state-of-the-art models like o1-mini\nachieve only 75.58 average score, with open-source alternatives lagging\napproximately 10 points behind. We find generation tasks more challenging than\nconversion tasks, and producing correct visual content more difficult than\ngenerating text-only structures."}
{"id": "2505.20266", "pdf": "https://arxiv.org/pdf/2505.20266", "abs": "https://arxiv.org/abs/2505.20266", "authors": ["Alexander Conway", "Debadeepta Dey", "Stefan Hackmann", "Matthew Hausknecht", "Michael Schmidt", "Mark Steadman", "Nick Volynets"], "title": "syftr: Pareto-Optimal Generative AI", "categories": ["cs.AI", "cs.LG"], "comment": "International Conference on Automated Machine Learning (AutoML) 2025", "summary": "Retrieval-Augmented Generation (RAG) pipelines are central to applying large\nlanguage models (LLMs) to proprietary or dynamic data. However, building\neffective RAG flows is complex, requiring careful selection among vector\ndatabases, embedding models, text splitters, retrievers, and synthesizing LLMs.\nThe challenge deepens with the rise of agentic paradigms. Modules like\nverifiers, rewriters, and rerankers-each with intricate hyperparameter\ndependencies have to be carefully tuned. Balancing tradeoffs between latency,\naccuracy, and cost becomes increasingly difficult in performance-sensitive\napplications.\n  We introduce syftr, a framework that performs efficient multi-objective\nsearch over a broad space of agentic and non-agentic RAG configurations. Using\nBayesian Optimization, syftr discovers Pareto-optimal flows that jointly\noptimize task accuracy and cost. A novel early-stopping mechanism further\nimproves efficiency by pruning clearly suboptimal candidates. Across multiple\nRAG benchmarks, syftr finds flows which are on average approximately 9 times\ncheaper while preserving most of the accuracy of the most accurate flows on the\nPareto-frontier. Furthermore, syftr's ability to design and optimize allows\nintegrating new modules, making it even easier and faster to realize\nhigh-performing generative AI pipelines."}
{"id": "2505.20149", "pdf": "https://arxiv.org/pdf/2505.20149", "abs": "https://arxiv.org/abs/2505.20149", "authors": ["Cheng-Yu Tai", "Ching-Wen Chen", "Chi-Chin Wu", "Bo-Chen Chiu", "Cheng-Hung", "Lin", "Cheng-Kai Lu", "Jia-Kang Wang", "Tzu-Lun Huang"], "title": "Improvement Strategies for Few-Shot Learning in OCT Image Classification of Rare Retinal Diseases", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "This paper focuses on using few-shot learning to improve the accuracy of\nclassifying OCT diagnosis images with major and rare classes. We used the\nGAN-based augmentation strategy as a baseline and introduced several novel\nmethods to further enhance our model. The proposed strategy contains U-GAT-IT\nfor improving the generative part and uses the data balance technique to narrow\ndown the skew of accuracy between all categories. The best model obtained was\nbuilt with CBAM attention mechanism and fine-tuned InceptionV3, and achieved an\noverall accuracy of 97.85%, representing a significant improvement over the\noriginal baseline."}
{"id": "2505.20269", "pdf": "https://arxiv.org/pdf/2505.20269", "abs": "https://arxiv.org/abs/2505.20269", "authors": ["Levi Cordeiro Carvalho", "Saulo A. F. Oliveira", "Thiago Alves Rocha"], "title": "Comparing Neural Network Encodings for Logic-based Explainability", "categories": ["cs.LO", "cs.AI", "cs.LG"], "comment": "submitted to BRACIS 2024 (Brazilian Conference on Intelligent\n  Systems), accepted version published in Intelligent Systems, LNCS, vol 15412", "summary": "Providing explanations for the outputs of artificial neural networks (ANNs)\nis crucial in many contexts, such as critical systems, data protection laws and\nhandling adversarial examples. Logic-based methods can offer explanations with\ncorrectness guarantees, but face scalability challenges. Due to these issues,\nit is necessary to compare different encodings of ANNs into logical\nconstraints, which are used in logic-based explainability. This work compares\ntwo encodings of ANNs: one has been used in the literature to provide\nexplanations, while the other will be adapted for our context of\nexplainability. Additionally, the second encoding uses fewer variables and\nconstraints, thus, potentially enhancing efficiency. Experiments showed similar\nrunning times for computing explanations, but the adapted encoding performed up\nto 18\\% better in building logical constraints and up to 16\\% better in overall\ntime."}
{"id": "2505.20150", "pdf": "https://arxiv.org/pdf/2505.20150", "abs": "https://arxiv.org/abs/2505.20150", "authors": ["Ilai Reshef", "Nadav Dym"], "title": "On the (Non) Injectivity of Piecewise Linear Janossy Pooling", "categories": ["cs.LG", "cs.AI"], "comment": "14 pages, 2 figures", "summary": "Multiset functions, which are functions that map multisets to vectors, are a\nfundamental tool in the construction of neural networks for multisets and\ngraphs. To guarantee that the vector representation of the multiset is\nfaithful, it is often desirable to have multiset mappings that are both\ninjective and bi-Lipschitz. Currently, there are several constructions of\nmultiset functions achieving both these guarantees, leading to improved\nperformance in some tasks but often also to higher compute time than standard\nconstructions. Accordingly, it is natural to inquire whether simpler multiset\nfunctions achieving the same guarantees are available. In this paper, we make a\nlarge step towards giving a negative answer to this question. We consider the\nfamily of k-ary Janossy pooling, which includes many of the most popular\nmultiset models, and prove that no piecewise linear Janossy pooling function\ncan be injective. On the positive side, we show that when restricted to\nmultisets without multiplicities, even simple deep-sets models suffice for\ninjectivity and bi-Lipschitzness."}
{"id": "2505.20280", "pdf": "https://arxiv.org/pdf/2505.20280", "abs": "https://arxiv.org/abs/2505.20280", "authors": ["Jonas Spinner", "Luigi Favaro", "Peter Lippmann", "Sebastian Pitz", "Gerrit Gerhartz", "Tilman Plehn", "Fred A. Hamprecht"], "title": "Lorentz Local Canonicalization: How to Make Any Network Lorentz-Equivariant", "categories": ["stat.ML", "cs.LG", "hep-ph"], "comment": "22 pages, 6 figures, 6 tables", "summary": "Lorentz-equivariant neural networks are becoming the leading architectures\nfor high-energy physics. Current implementations rely on specialized layers,\nlimiting architectural choices. We introduce Lorentz Local Canonicalization\n(LLoCa), a general framework that renders any backbone network exactly\nLorentz-equivariant. Using equivariantly predicted local reference frames, we\nconstruct LLoCa-transformers and graph networks. We adapt a recent approach to\ngeometric message passing to the non-compact Lorentz group, allowing\npropagation of space-time tensorial features. Data augmentation emerges from\nLLoCa as a special choice of reference frame. Our models surpass\nstate-of-the-art accuracy on relevant particle physics tasks, while being\n$4\\times$ faster and using $5$-$100\\times$ fewer FLOPs."}
{"id": "2505.20152", "pdf": "https://arxiv.org/pdf/2505.20152", "abs": "https://arxiv.org/abs/2505.20152", "authors": ["Kai Sun", "Yushi Bai", "Zhen Yang", "Jiajie Zhang", "Ji Qi", "Lei Hou", "Juanzi Li"], "title": "Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Benefiting from contrastively trained visual encoders on large-scale natural\nscene images, Large Multimodal Models (LMMs) have achieved remarkable\nperformance across various visual perception tasks. However, the inherent\nlimitations of contrastive learning upon summarized descriptions fundamentally\nrestrict the capabilities of models in meticulous reasoning, particularly in\ncrucial scenarios of geometric problem-solving. To enhance geometric\nunderstanding, we propose a novel hard negative contrastive learning framework\nfor the vision encoder, which combines image-based contrastive learning using\ngeneration-based hard negatives created by perturbing diagram generation code,\nand text-based contrastive learning using rule-based negatives derived from\nmodified geometric descriptions and retrieval-based negatives selected based on\ncaption similarity. We train CLIP using our strong negative learning method,\nnamely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for\ngeometric problem-solving. Experiments show that our trained model, MMGeoLM,\nsignificantly outperforms other open-source models on three geometric reasoning\nbenchmarks. Even with a size of 7B, it can rival powerful closed-source models\nlike GPT-4o. We further study the impact of different negative sample\nconstruction methods and the number of negative samples on the geometric\nreasoning performance of LMM, yielding fruitful conclusions. The code and\ndataset are available at https://github.com/THU-KEG/MMGeoLM."}
{"id": "2505.20295", "pdf": "https://arxiv.org/pdf/2505.20295", "abs": "https://arxiv.org/abs/2505.20295", "authors": ["Michael Kirchhof", "Luca Fger", "Adam Goliski", "Eeshan Gunesh Dhekane", "Arno Blaas", "Sinead Williamson"], "title": "Self-reflective Uncertainties: Do LLMs Know Their Internal Answer Distribution?", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": null, "summary": "To reveal when a large language model (LLM) is uncertain about a response,\nuncertainty quantification commonly produces percentage numbers along with the\noutput. But is this all we can do? We argue that in the output space of LLMs,\nthe space of strings, exist strings expressive enough to summarize the\ndistribution over output strings the LLM deems possible. We lay a foundation\nfor this new avenue of uncertainty explication and present SelfReflect, a\ntheoretically-motivated metric to assess how faithfully a string summarizes an\nLLM's internal answer distribution. We show that SelfReflect is able to\ndiscriminate even subtle differences of candidate summary strings and that it\naligns with human judgement, outperforming alternative metrics such as LLM\njudges and embedding comparisons. With SelfReflect, we investigate a number of\nself-summarization methods and find that even state-of-the-art reasoning models\nstruggle to explicate their internal uncertainty. But we find that faithful\nsummarizations can be generated by sampling and summarizing. Our metric enables\nfuture works towards this universal form of LLM uncertainties."}
{"id": "2505.20161", "pdf": "https://arxiv.org/pdf/2505.20161", "abs": "https://arxiv.org/abs/2505.20161", "authors": ["Jaehun Jung", "Seungju Han", "Ximing Lu", "Skyler Hallinan", "David Acuna", "Shrimai Prabhumoye", "Mostafa Patwary", "Mohammad Shoeybi", "Bryan Catanzaro", "Yejin Choi"], "title": "Prismatic Synthesis: Gradient-based Data Diversification Boosts Generalization in LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Effective generalization in language models depends critically on the\ndiversity of their training data. Yet existing diversity metrics often fall\nshort of this goal, relying on surface-level heuristics that are decoupled from\nmodel behavior. This motivates us to ask: What kind of diversity in training\ndata actually drives generalization in language models -- and how can we\nmeasure and amplify it? Through large-scale empirical analyses spanning over\n300 training runs, carefully controlled for data scale and quality, we show\nthat data diversity can be a strong predictor of generalization in LLM\nreasoning -- as measured by average model performance on unseen\nout-of-distribution benchmarks. We introduce G-Vendi, a metric that quantifies\ndiversity via the entropy of model-induced gradients. Despite using a small\noff-the-shelf proxy model for gradients, G-Vendi consistently outperforms\nalternative measures, achieving strong correlation (Spearman's $\\rho \\approx\n0.9$) with out-of-distribution (OOD) performance on both natural language\ninference (NLI) and math reasoning tasks. Building on this insight, we present\nPrismatic Synthesis, a framework for generating diverse synthetic data by\ntargeting underrepresented regions in gradient space. Experimental results show\nthat Prismatic Synthesis consistently improves model performance as we scale\nsynthetic data -- not just on in-distribution test but across unseen,\nout-of-distribution benchmarks -- significantly outperforming state-of-the-art\nmodels that rely on 20 times larger data generator than ours. For example,\nPrismMath-7B, our model distilled from a 32B LLM, outperforms\nR1-Distill-Qwen-7B -- the same base model trained on proprietary data generated\nby 671B R1 -- on 6 out of 7 challenging benchmarks."}
{"id": "2505.20296", "pdf": "https://arxiv.org/pdf/2505.20296", "abs": "https://arxiv.org/abs/2505.20296", "authors": ["Jiahao Lu", "Ziwei Xu", "Mohan Kankanhalli"], "title": "Reasoning LLMs are Wandering Solution Explorers", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM"], "comment": "71 pages, 14 figures, 2 tables", "summary": "Large Language Models (LLMs) have demonstrated impressive reasoning abilities\nthrough test-time computation (TTC) techniques such as chain-of-thought\nprompting and tree-based reasoning. However, we argue that current reasoning\nLLMs (RLLMs) lack the ability to systematically explore the solution space.\nThis paper formalizes what constitutes systematic problem solving and\nidentifies common failure modes that reveal reasoning LLMs to be wanderers\nrather than systematic explorers. Through qualitative and quantitative analysis\nacross multiple state-of-the-art LLMs, we uncover persistent issues: invalid\nreasoning steps, redundant explorations, hallucinated or unfaithful\nconclusions, and so on. Our findings suggest that current models' performance\ncan appear to be competent on simple tasks yet degrade sharply as complexity\nincreases. Based on the findings, we advocate for new metrics and tools that\nevaluate not just final outputs but the structure of the reasoning process\nitself."}
{"id": "2505.20166", "pdf": "https://arxiv.org/pdf/2505.20166", "abs": "https://arxiv.org/abs/2505.20166", "authors": ["Chun-Yi Kuan", "Hung-yi Lee"], "title": "From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "comment": "Project Website: https://kuan2jiu99.github.io/Balsa", "summary": "Audio-aware large language models (ALLMs) have recently made great strides in\nunderstanding and processing audio inputs. These models are typically adapted\nfrom text-based large language models (LLMs) through additional training on\naudio-related tasks. However, this adaptation process presents two major\nlimitations. First, ALLMs often suffer from catastrophic forgetting, where\nimportant textual capabilities such as instruction-following are lost after\ntraining on audio data. In some cases, models may even hallucinate sounds that\nare not present in the input audio, raising concerns about their reliability.\nSecond, achieving cross-modal alignment between audio and language typically\nrelies on large collections of task-specific question-answer pairs for\ninstruction tuning, making the process resource-intensive. To address these\nissues, we leverage the backbone LLMs from ALLMs to synthesize general-purpose\ncaption-style alignment data. We refer to this process as bootstrapping\naudio-language alignment via synthetic data generation from backbone LLMs\n(BALSa). Building on BALSa, we introduce LISTEN (Learning to Identify Sounds\nThrough Extended Negative Samples), a contrastive-like training method designed\nto improve ALLMs' ability to distinguish between present and absent sounds. We\nfurther extend BALSa to multi-audio scenarios, where the model either explains\nthe differences between audio inputs or produces a unified caption that\ndescribes them all, thereby enhancing audio-language alignment. Experimental\nresults indicate that our method effectively mitigates audio hallucinations\nwhile reliably maintaining strong performance in audio understanding,\nreasoning, and instruction-following skills. Moreover, incorporating\nmulti-audio training further enhances the model's comprehension and reasoning\ncapabilities. Overall, BALSa offers an efficient and scalable approach to the\ndevelopment of ALLMs."}
{"id": "2505.20184", "pdf": "https://arxiv.org/pdf/2505.20184", "abs": "https://arxiv.org/abs/2505.20184", "authors": ["Yongan Yu", "Mengqian Wu", "Yiran Lin", "Nikki G. Lobczowski"], "title": "THiNK: Can Large Language Models Think-aloud?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Assessing higher-order thinking skills in large language models (LLMs)\nremains a fundamental challenge, especially in tasks that go beyond\nsurface-level accuracy. In this work, we propose THiNK (Testing Higher-order\nNotion of Knowledge), a multi-agent, feedback-driven evaluation framework\ngrounded in Bloom's Taxonomy. THiNK frames reasoning assessment as an iterative\ntask of problem generation, critique, and revision, encouraging LLMs to\nthink-aloud through step-by-step reflection and refinement. This enables a\nsystematic evaluation of both lower-order (e.g., remember, understand) and\nhigher-order (e.g., evaluate, create) thinking skills. We apply THiNK to seven\nstate-of-the-art LLMs and perform a detailed cognitive analysis of their\noutputs. Results reveal that while models reliably perform lower-order\ncategories well, they struggle with applying knowledge in realistic contexts\nand exhibit limited abstraction. Structured feedback loops significantly\nimprove reasoning performance, particularly in higher-order thinking.\nQualitative evaluations further confirm that THiNK-guided outputs better align\nwith domain logic and problem structure. The code of our framework provides a\nscalable methodology for probing and enhancing LLM reasoning, offering new\ndirections for evaluation grounded in learning science, which is available at\nour GitHub repository."}
{"id": "2505.20190", "pdf": "https://arxiv.org/pdf/2505.20190", "abs": "https://arxiv.org/abs/2505.20190", "authors": ["Tonmoy Hasan", "Razvan Bunescu"], "title": "Leveraging Descriptions of Emotional Preferences in Recommender Systems", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "The affective attitude of liking a recommended item reflects just one\ncategory in a wide spectrum of affective phenomena that also includes emotions\nsuch as entranced or intrigued, moods such as cheerful or buoyant, as well as\nmore fine-grained affective states, such as \"pleasantly surprised by the\nconclusion\". In this paper, we introduce a novel recommendation task that can\nleverage a virtually unbounded range of affective states sought explicitly by\nthe user in order to identify items that, upon consumption, are likely to\ninduce those affective states. Correspondingly, we create a large dataset of\nuser preferences containing expressions of fine-grained affective states that\nare mined from book reviews, and propose a Transformer-based architecture that\nleverages such affective expressions as input. We then use the resulting\ndataset of affective states preferences, together with the linked users and\ntheir histories of book readings, ratings, and reviews, to train and evaluate\nmultiple recommendation models on the task of matching recommended items with\naffective preferences. Experiments show that the best results are obtained by\nmodels that can utilize textual descriptions of items and user affective\npreferences."}
{"id": "2505.20206", "pdf": "https://arxiv.org/pdf/2505.20206", "abs": "https://arxiv.org/abs/2505.20206", "authors": ["Umut Cihan", "Arda z", "Vahid Haratian", "Eray Tzn"], "title": "Evaluating Large Language Models for Code Review", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Context: Code reviews are crucial for software quality. Recent AI advances\nhave allowed large language models (LLMs) to review and fix code; now, there\nare tools that perform these reviews. However, their reliability and accuracy\nhave not yet been systematically evaluated. Objective: This study compares\ndifferent LLMs' performance in detecting code correctness and suggesting\nimprovements. Method: We tested GPT4o and Gemini 2.0 Flash on 492 AI generated\ncode blocks of varying correctness, along with 164 canonical code blocks from\nthe HumanEval benchmark. To simulate the code review task objectively, we\nexpected LLMs to assess code correctness and improve the code if needed. We ran\nexperiments with different configurations and reported on the results. Results:\nWith problem descriptions, GPT4o and Gemini 2.0 Flash correctly classified code\ncorrectness 68.50% and 63.89% of the time, respectively, and corrected the code\n67.83% and 54.26% of the time for the 492 code blocks of varying correctness.\nWithout problem descriptions, performance declined. The results for the 164\ncanonical code blocks differed, suggesting that performance depends on the type\nof code. Conclusion: LLM code reviews can help suggest improvements and assess\ncorrectness, but there is a risk of faulty outputs. We propose a process that\ninvolves humans, called the \"Human in the loop LLM Code Review\" to promote\nknowledge sharing while mitigating the risk of faulty outputs."}
{"id": "2505.20211", "pdf": "https://arxiv.org/pdf/2505.20211", "abs": "https://arxiv.org/abs/2505.20211", "authors": ["Junseo Hwang", "Wonguk Cho", "Taesup Kim"], "title": "Parameter-Efficient Fine-Tuning with Column Space Projection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Fine-tuning large language models (LLMs) with minimal computational overhead\nis essential for efficiently adapting them to downstream tasks under resource\nconstraints. Parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank\nAdaptation (LoRA), facilitate this by updating only a small subset of\nparameters. However, recent studies show that LoRA diverges from full\nfine-tuning (Full FT) in its learning behavior, particularly in terms of\nspectral properties. Motivated by these findings, we propose PiCa, the first\ntheoretically grounded PEFT method based on the spectral properties of\nfine-tuned weights. PiCa projects gradients onto the low-rank column subspace\nof pre-trained weights and exhibits learning patterns more closely aligned with\nFull FT. Furthermore, we show that combining PiCa with weight sharing\ndrastically reduces the number of trainable parameters without compromising\nperformance, enabling to achieve superior performance than LoRA using 13x fewer\ntrainable parameters. Extensive experiments demonstrate PiCa achieves the\nstate-of-the-art performance compared to existing PEFT methods."}
{"id": "2505.20229", "pdf": "https://arxiv.org/pdf/2505.20229", "abs": "https://arxiv.org/abs/2505.20229", "authors": ["Maximilian Dreyer", "Lorenz Hufe", "Jim Berend", "Thomas Wiegand", "Sebastian Lapuschkin", "Wojciech Samek"], "title": "From What to How: Attributing CLIP's Latent Components Reveals Unexpected Semantic Reliance", "categories": ["cs.LG", "cs.AI"], "comment": "25 pages (10 pages manuscript, 4 pages references, 11 pages appendix)", "summary": "Transformer-based CLIP models are widely used for text-image probing and\nfeature extraction, making it relevant to understand the internal mechanisms\nbehind their predictions. While recent works show that Sparse Autoencoders\n(SAEs) yield interpretable latent components, they focus on what these encode\nand miss how they drive predictions. We introduce a scalable framework that\nreveals what latent components activate for, how they align with expected\nsemantics, and how important they are to predictions. To achieve this, we adapt\nattribution patching for instance-wise component attributions in CLIP and\nhighlight key faithfulness limitations of the widely used Logit Lens technique.\nBy combining attributions with semantic alignment scores, we can automatically\nuncover reliance on components that encode semantically unexpected or spurious\nconcepts. Applied across multiple CLIP variants, our method uncovers hundreds\nof surprising components linked to polysemous words, compound nouns, visual\ntypography and dataset artifacts. While text embeddings remain prone to\nsemantic ambiguity, they are more robust to spurious correlations compared to\nlinear classifiers trained on image embeddings. A case study on skin lesion\ndetection highlights how such classifiers can amplify hidden shortcuts,\nunderscoring the need for holistic, mechanistic interpretability. We provide\ncode at https://github.com/maxdreyer/attributing-clip."}
{"id": "2505.20235", "pdf": "https://arxiv.org/pdf/2505.20235", "abs": "https://arxiv.org/abs/2505.20235", "authors": ["Jonathan Wenger", "Beau Coker", "Juraj Marusic", "John P. Cunningham"], "title": "Variational Deep Learning via Implicit Regularization", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Modern deep learning models generalize remarkably well in-distribution,\ndespite being overparametrized and trained with little to no explicit\nregularization. Instead, current theory credits implicit regularization imposed\nby the choice of architecture, hyperparameters and optimization procedure.\nHowever, deploying deep learning models out-of-distribution, in sequential\ndecision-making tasks, or in safety-critical domains, necessitates reliable\nuncertainty quantification, not just a point estimate. The machinery of modern\napproximate inference -- Bayesian deep learning -- should answer the need for\nuncertainty quantification, but its effectiveness has been challenged by our\ninability to define useful explicit inductive biases through priors, as well as\nthe associated computational burden. Instead, in this work we demonstrate, both\ntheoretically and empirically, how to regularize a variational deep network\nimplicitly via the optimization procedure, just as for standard deep learning.\nWe fully characterize the inductive bias of (stochastic) gradient descent in\nthe case of an overparametrized linear model as generalized variational\ninference and demonstrate the importance of the choice of parametrization.\nFinally, we show empirically that our approach achieves strong in- and\nout-of-distribution performance without tuning of additional hyperparameters\nand with minimal time and memory overhead over standard deep learning."}
{"id": "2505.20241", "pdf": "https://arxiv.org/pdf/2505.20241", "abs": "https://arxiv.org/abs/2505.20241", "authors": ["Qi Cao", "Ruiyi Wang", "Ruiyi Zhang", "Sai Ashish Somayajula", "Pengtao Xie"], "title": "DreamPRM: Domain-Reweighted Process Reward Model for Multimodal Reasoning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reasoning has substantially improved the performance of large language models\n(LLMs) on complicated tasks. Central to the current reasoning studies, Process\nReward Models (PRMs) offer a fine-grained evaluation of intermediate reasoning\nsteps and guide the reasoning process. However, extending PRMs to multimodal\nlarge language models (MLLMs) introduces challenges. Since multimodal reasoning\ncovers a wider range of tasks compared to text-only scenarios, the resulting\ndistribution shift from the training to testing sets is more severe, leading to\ngreater generalization difficulty. Training a reliable multimodal PRM,\ntherefore, demands large and diverse datasets to ensure sufficient coverage.\nHowever, current multimodal reasoning datasets suffer from a marked quality\nimbalance, which degrades PRM performance and highlights the need for an\neffective data selection strategy. To address the issues, we introduce\nDreamPRM, a domain-reweighted training framework for multimodal PRMs which\nemploys bi-level optimization. In the lower-level optimization, DreamPRM\nperforms fine-tuning on multiple datasets with domain weights, allowing the PRM\nto prioritize high-quality reasoning signals and alleviating the impact of\ndataset quality imbalance. In the upper-level optimization, the PRM is\nevaluated on a separate meta-learning dataset; this feedback updates the domain\nweights through an aggregation loss function, thereby improving the\ngeneralization capability of trained PRM. Extensive experiments on multiple\nmultimodal reasoning benchmarks covering both mathematical and general\nreasoning show that test-time scaling with DreamPRM consistently improves the\nperformance of state-of-the-art MLLMs. Further comparisons reveal that\nDreamPRM's domain-reweighting strategy surpasses other data selection methods\nand yields higher accuracy gains than existing test-time scaling approaches."}
{"id": "2505.20245", "pdf": "https://arxiv.org/pdf/2505.20245", "abs": "https://arxiv.org/abs/2505.20245", "authors": ["Rui Li", "Quanyu Dai", "Zeyu Zhang", "Xu Chen", "Zhenhua Dong", "Ji-Rong Wen"], "title": "KnowTrace: Bootstrapping Iterative Retrieval-Augmented Generation with Structured Knowledge Tracing", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by KDD 2025", "summary": "Recent advances in retrieval-augmented generation (RAG) furnish large\nlanguage models (LLMs) with iterative retrievals of relevant information to\nhandle complex multi-hop questions. These methods typically alternate between\nLLM reasoning and retrieval to accumulate external information into the LLM's\ncontext. However, the ever-growing context inherently imposes an increasing\nburden on the LLM to perceive connections among critical information pieces,\nwith futile reasoning steps further exacerbating this overload issue. In this\npaper, we present KnowTrace, an elegant RAG framework to (1) mitigate the\ncontext overload and (2) bootstrap higher-quality multi-step reasoning. Instead\nof simply piling the retrieved contents, KnowTrace autonomously traces out\ndesired knowledge triplets to organize a specific knowledge graph relevant to\nthe input question. Such a structured workflow not only empowers the LLM with\nan intelligible context for inference, but also naturally inspires a reflective\nmechanism of knowledge backtracing to identify contributive LLM generations as\nprocess supervision data for self-bootstrapping. Extensive experiments show\nthat KnowTrace consistently surpasses existing methods across three multi-hop\nquestion answering benchmarks, and the bootstrapped version further amplifies\nthe gains."}
{"id": "2505.20249", "pdf": "https://arxiv.org/pdf/2505.20249", "abs": "https://arxiv.org/abs/2505.20249", "authors": ["Yongan Yu", "Qingchen Hu", "Xianda Du", "Jiayin Wang", "Fengran Mo", "Renee Sieber"], "title": "WXImpactBench: A Disruptive Weather Impact Understanding Benchmark for Evaluating Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025", "summary": "Climate change adaptation requires the understanding of disruptive weather\nimpacts on society, where large language models (LLMs) might be applicable.\nHowever, their effectiveness is under-explored due to the difficulty of\nhigh-quality corpus collection and the lack of available benchmarks. The\nclimate-related events stored in regional newspapers record how communities\nadapted and recovered from disasters. However, the processing of the original\ncorpus is non-trivial. In this study, we first develop a disruptive weather\nimpact dataset with a four-stage well-crafted construction pipeline. Then, we\npropose WXImpactBench, the first benchmark for evaluating the capacity of LLMs\non disruptive weather impacts. The benchmark involves two evaluation tasks,\nmulti-label classification and ranking-based question answering. Extensive\nexperiments on evaluating a set of LLMs provide first-hand analysis of the\nchallenges in developing disruptive weather impact understanding and climate\nchange adaptation systems. The constructed dataset and the code for the\nevaluation framework are available to help society protect against\nvulnerabilities from disasters."}
{"id": "2505.20254", "pdf": "https://arxiv.org/pdf/2505.20254", "abs": "https://arxiv.org/abs/2505.20254", "authors": ["Xiangchen Song", "Aashiq Muhamed", "Yujia Zheng", "Lingjing Kong", "Zeyu Tang", "Mona T. Diab", "Virginia Smith", "Kun Zhang"], "title": "Position: Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Sparse Autoencoders (SAEs) are a prominent tool in mechanistic\ninterpretability (MI) for decomposing neural network activations into\ninterpretable features. However, the aspiration to identify a canonical set of\nfeatures is challenged by the observed inconsistency of learned SAE features\nacross different training runs, undermining the reliability and efficiency of\nMI research. This position paper argues that mechanistic interpretability\nshould prioritize feature consistency in SAEs -- the reliable convergence to\nequivalent feature sets across independent runs. We propose using the Pairwise\nDictionary Mean Correlation Coefficient (PW-MCC) as a practical metric to\noperationalize consistency and demonstrate that high levels are achievable\n(0.80 for TopK SAEs on LLM activations) with appropriate architectural choices.\nOur contributions include detailing the benefits of prioritizing consistency;\nproviding theoretical grounding and synthetic validation using a model\norganism, which verifies PW-MCC as a reliable proxy for ground-truth recovery;\nand extending these findings to real-world LLM data, where high feature\nconsistency strongly correlates with the semantic similarity of learned feature\nexplanations. We call for a community-wide shift towards systematically\nmeasuring feature consistency to foster robust cumulative progress in MI."}
{"id": "2505.20259", "pdf": "https://arxiv.org/pdf/2505.20259", "abs": "https://arxiv.org/abs/2505.20259", "authors": ["Haoyu Wang", "Zeyu Qin", "Yifei Zhao", "Chao Du", "Min Lin", "Xueqian Wang", "Tianyu Pang"], "title": "Lifelong Safety Alignment for Language Models", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "LLMs have made impressive progress, but their growing capabilities also\nexpose them to highly flexible jailbreaking attacks designed to bypass safety\nalignment. While many existing defenses focus on known types of attacks, it is\nmore critical to prepare LLMs for unseen attacks that may arise during\ndeployment. To address this, we propose a lifelong safety alignment framework\nthat enables LLMs to continuously adapt to new and evolving jailbreaking\nstrategies. Our framework introduces a competitive setup between two\ncomponents: a Meta-Attacker, trained to actively discover novel jailbreaking\nstrategies, and a Defender, trained to resist them. To effectively warm up the\nMeta-Attacker, we first leverage the GPT-4o API to extract key insights from a\nlarge collection of jailbreak-related research papers. Through iterative\ntraining, the first iteration Meta-Attacker achieves a 73% attack success rate\n(ASR) on RR and a 57% transfer ASR on LAT using only single-turn attacks.\nMeanwhile, the Defender progressively improves its robustness and ultimately\nreduces the Meta-Attacker's success rate to just 7%, enabling safer and more\nreliable deployment of LLMs in open-ended environments. The code is available\nat https://github.com/sail-sg/LifelongSafetyAlignment."}
{"id": "2505.20264", "pdf": "https://arxiv.org/pdf/2505.20264", "abs": "https://arxiv.org/abs/2505.20264", "authors": ["Dong Nguyen", "Esther Ploeger"], "title": "We Need to Measure Data Diversity in NLP -- Better and Broader", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although diversity in NLP datasets has received growing attention, the\nquestion of how to measure it remains largely underexplored. This opinion paper\nexamines the conceptual and methodological challenges of measuring data\ndiversity and argues that interdisciplinary perspectives are essential for\ndeveloping more fine-grained and valid measures."}
{"id": "2505.20268", "pdf": "https://arxiv.org/pdf/2505.20268", "abs": "https://arxiv.org/abs/2505.20268", "authors": ["Fan Chen", "Zeyu Jia", "Alexander Rakhlin", "Tengyang Xie"], "title": "Outcome-Based Online Reinforcement Learning: Algorithms and Fundamental Limits", "categories": ["cs.LG", "cs.AI", "math.ST", "stat.ML", "stat.TH"], "comment": null, "summary": "Reinforcement learning with outcome-based feedback faces a fundamental\nchallenge: when rewards are only observed at trajectory endpoints, how do we\nassign credit to the right actions? This paper provides the first comprehensive\nanalysis of this problem in online RL with general function approximation. We\ndevelop a provably sample-efficient algorithm achieving $\\widetilde{O}({C_{\\rm\ncov} H^3}/{\\epsilon^2})$ sample complexity, where $C_{\\rm cov}$ is the\ncoverability coefficient of the underlying MDP. By leveraging general function\napproximation, our approach works effectively in large or infinite state spaces\nwhere tabular methods fail, requiring only that value functions and reward\nfunctions can be represented by appropriate function classes. Our results also\ncharacterize when outcome-based feedback is statistically separated from\nper-step rewards, revealing an unavoidable exponential separation for certain\nMDPs. For deterministic MDPs, we show how to eliminate the completeness\nassumption, dramatically simplifying the algorithm. We further extend our\napproach to preference-based feedback settings, proving that equivalent\nstatistical efficiency can be achieved even under more limited information.\nTogether, these results constitute a theoretical foundation for understanding\nthe statistical properties of outcome-based reinforcement learning."}
{"id": "2505.20269", "pdf": "https://arxiv.org/pdf/2505.20269", "abs": "https://arxiv.org/abs/2505.20269", "authors": ["Levi Cordeiro Carvalho", "Saulo A. F. Oliveira", "Thiago Alves Rocha"], "title": "Comparing Neural Network Encodings for Logic-based Explainability", "categories": ["cs.LO", "cs.AI", "cs.LG"], "comment": "submitted to BRACIS 2024 (Brazilian Conference on Intelligent\n  Systems), accepted version published in Intelligent Systems, LNCS, vol 15412", "summary": "Providing explanations for the outputs of artificial neural networks (ANNs)\nis crucial in many contexts, such as critical systems, data protection laws and\nhandling adversarial examples. Logic-based methods can offer explanations with\ncorrectness guarantees, but face scalability challenges. Due to these issues,\nit is necessary to compare different encodings of ANNs into logical\nconstraints, which are used in logic-based explainability. This work compares\ntwo encodings of ANNs: one has been used in the literature to provide\nexplanations, while the other will be adapted for our context of\nexplainability. Additionally, the second encoding uses fewer variables and\nconstraints, thus, potentially enhancing efficiency. Experiments showed similar\nrunning times for computing explanations, but the adapted encoding performed up\nto 18\\% better in building logical constraints and up to 16\\% better in overall\ntime."}
{"id": "2505.20271", "pdf": "https://arxiv.org/pdf/2505.20271", "abs": "https://arxiv.org/abs/2505.20271", "authors": ["Yu Xu", "Fan Tang", "You Wu", "Lin Gao", "Oliver Deussen", "Hongbin Yan", "Jintao Li", "Juan Cao", "Tong-Yee Lee"], "title": "In-Context Brush: Zero-shot Customized Subject Insertion with Context-Aware Latent Space Manipulation", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": null, "summary": "Recent advances in diffusion models have enhanced multimodal-guided visual\ngeneration, enabling customized subject insertion that seamlessly \"brushes\"\nuser-specified objects into a given image guided by textual prompts. However,\nexisting methods often struggle to insert customized subjects with high\nfidelity and align results with the user's intent through textual prompts. In\nthis work, we propose \"In-Context Brush\", a zero-shot framework for customized\nsubject insertion by reformulating the task within the paradigm of in-context\nlearning. Without loss of generality, we formulate the object image and the\ntextual prompts as cross-modal demonstrations, and the target image with the\nmasked region as the query. The goal is to inpaint the target image with the\nsubject aligning textual prompts without model tuning. Building upon a\npretrained MMDiT-based inpainting network, we perform test-time enhancement via\ndual-level latent space manipulation: intra-head \"latent feature shifting\"\nwithin each attention head that dynamically shifts attention outputs to reflect\nthe desired subject semantics and inter-head \"attention reweighting\" across\ndifferent heads that amplifies prompt controllability through differential\nattention prioritization. Extensive experiments and applications demonstrate\nthat our approach achieves superior identity preservation, text alignment, and\nimage quality compared to existing state-of-the-art methods, without requiring\ndedicated training or additional data collection."}
{"id": "2505.20274", "pdf": "https://arxiv.org/pdf/2505.20274", "abs": "https://arxiv.org/abs/2505.20274", "authors": ["Kejing Lu", "Chuan Xiao", "Yoshiharu Ishikawa"], "title": "Probabilistic Kernel Function for Fast Angle Testing", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DB", "cs.DS"], "comment": null, "summary": "In this paper, we study the angle testing problem in high-dimensional\nEuclidean spaces and propose two projection-based probabilistic kernel\nfunctions, one designed for angle comparison and the other for angle\nthresholding. Unlike existing approaches that rely on random projection vectors\ndrawn from Gaussian distributions, our approach leverages reference angles and\nemploys a deterministic structure for the projection vectors. Notably, our\nkernel functions do not require asymptotic assumptions, such as the number of\nprojection vectors tending to infinity, and can be both theoretically and\nexperimentally shown to outperform Gaussian-distribution-based kernel\nfunctions. We further apply the proposed kernel function to Approximate Nearest\nNeighbor Search (ANNS) and demonstrate that our approach achieves a 2.5X ~ 3X\nhigher query-per-second (QPS) throughput compared to the state-of-the-art\ngraph-based search algorithm HNSW."}
{"id": "2505.20276", "pdf": "https://arxiv.org/pdf/2505.20276", "abs": "https://arxiv.org/abs/2505.20276", "authors": ["Anmol Mekala", "Anirudh Atmakuru", "Yixiao Song", "Marzena Karpinska", "Mohit Iyyer"], "title": "Does quantization affect models' performance on long-context tasks?", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages of content with 9 figures. 37 remaining pages of references\n  and supplementary with 17 figures. Under review as of May 26", "summary": "Large language models (LLMs) now support context windows exceeding 128K\ntokens, but this comes with significant memory requirements and high inference\nlatency. Quantization can mitigate these costs, but may degrade performance. In\nthis work, we present the first systematic evaluation of quantized LLMs on\ntasks with long-inputs (>64K tokens) and long-form outputs. Our evaluation\nspans 9.7K test examples, five quantization methods (FP8, GPTQ-int8, AWQ-int4,\nGPTQ-int4, BNB-nf4), and five models (Llama-3.1 8B and 70B; Qwen-2.5 7B, 32B,\nand 72B). We find that, on average, 8-bit quantization preserves accuracy\n(~0.8% drop), whereas 4-bit methods lead to substantial losses, especially for\ntasks involving long context inputs (drops of up to 59%). This degradation\ntends to worsen when the input is in a language other than English. Crucially,\nthe effects of quantization depend heavily on the quantization method, model,\nand task. For instance, while Qwen-2.5 72B remains robust under BNB-nf4,\nLlama-3.1 70B experiences a 32% performance drop on the same task. These\nfindings highlight the importance of a careful, task-specific evaluation before\ndeploying quantized LLMs, particularly in long-context scenarios and with\nlanguages other than English."}
{"id": "2505.20278", "pdf": "https://arxiv.org/pdf/2505.20278", "abs": "https://arxiv.org/abs/2505.20278", "authors": ["Hoyeon Chang", "Jinho Park", "Hanseul Cho", "Sohee Yang", "Miyoung Ko", "Hyeonbin Hwang", "Seungpil Won", "Dohaeng Lee", "Youbin Ahn", "Minjoon Seo"], "title": "The Coverage Principle: A Framework for Understanding Compositional Generalization", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.6"], "comment": null, "summary": "Large language models excel at pattern matching, yet often fall short in\nsystematic compositional generalization. We propose the coverage principle: a\ndata-centric framework showing that models relying primarily on pattern\nmatching for compositional tasks cannot reliably generalize beyond substituting\nfragments that yield identical results when used in the same contexts. We\ndemonstrate that this framework has a strong predictive power for the\ngeneralization capabilities of Transformers. First, we derive and empirically\nconfirm that the training data required for two-hop generalization grows at\nleast quadratically with the token set size, and the training data efficiency\ndoes not improve with 20x parameter scaling. Second, for compositional tasks\nwith path ambiguity where one variable affects the output through multiple\ncomputational paths, we show that Transformers learn context-dependent state\nrepresentations that undermine both performance and interoperability. Third,\nChain-of-Thought supervision improves training data efficiency for multi-hop\ntasks but still struggles with path ambiguity. Finally, we outline a\n\\emph{mechanism-based} taxonomy that distinguishes three ways neural networks\ncan generalize: structure-based (bounded by coverage), property-based\n(leveraging algebraic invariances), and shared-operator (through function\nreuse). This conceptual lens contextualizes our results and highlights where\nnew architectural ideas are needed to achieve systematic compositionally.\nOverall, the coverage principle provides a unified lens for understanding\ncompositional reasoning, and underscores the need for fundamental architectural\nor training innovations to achieve truly systematic compositionality."}
{"id": "2505.20290", "pdf": "https://arxiv.org/pdf/2505.20290", "abs": "https://arxiv.org/abs/2505.20290", "authors": ["Vincent Liu", "Ademi Adeniji", "Haotian Zhan", "Raunaq Bhirangi", "Pieter Abbeel", "Lerrel Pinto"], "title": "EgoZero: Robot Learning from Smart Glasses", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Despite recent progress in general purpose robotics, robot policies still lag\nfar behind basic human capabilities in the real world. Humans interact\nconstantly with the physical world, yet this rich data resource remains largely\nuntapped in robot learning. We propose EgoZero, a minimal system that learns\nrobust manipulation policies from human demonstrations captured with Project\nAria smart glasses, $\\textbf{and zero robot data}$. EgoZero enables: (1)\nextraction of complete, robot-executable actions from in-the-wild, egocentric,\nhuman demonstrations, (2) compression of human visual observations into\nmorphology-agnostic state representations, and (3) closed-loop policy learning\nthat generalizes morphologically, spatially, and semantically. We deploy\nEgoZero policies on a gripper Franka Panda robot and demonstrate zero-shot\ntransfer with 70% success rate over 7 manipulation tasks and only 20 minutes of\ndata collection per task. Our results suggest that in-the-wild human data can\nserve as a scalable foundation for real-world robot learning - paving the way\ntoward a future of abundant, diverse, and naturalistic training data for\nrobots. Code and videos are available at https://egozero-robot.github.io."}
{"id": "2505.20292", "pdf": "https://arxiv.org/pdf/2505.20292", "abs": "https://arxiv.org/abs/2505.20292", "authors": ["Shenghai Yuan", "Xianyi He", "Yufan Deng", "Yang Ye", "Jinfa Huang", "Bin Lin", "Chongyang Ma", "Jiebo Luo", "Li Yuan"], "title": "OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Code and Dataset: https://github.com/PKU-YuanGroup/OpenS2V-Nexus", "summary": "Subject-to-Video (S2V) generation aims to create videos that faithfully\nincorporate reference content, providing enhanced flexibility in the production\nof videos. To establish the infrastructure for S2V generation, we propose\nOpenS2V-Nexus, consisting of (i) OpenS2V-Eval, a fine-grained benchmark, and\n(ii) OpenS2V-5M, a million-scale dataset. In contrast to existing S2V\nbenchmarks inherited from VBench that focus on global and coarse-grained\nassessment of generated videos, OpenS2V-Eval focuses on the model's ability to\ngenerate subject-consistent videos with natural subject appearance and identity\nfidelity. For these purposes, OpenS2V-Eval introduces 180 prompts from seven\nmajor categories of S2V, which incorporate both real and synthetic test data.\nFurthermore, to accurately align human preferences with S2V benchmarks, we\npropose three automatic metrics, NexusScore, NaturalScore and GmeScore, to\nseparately quantify subject consistency, naturalness, and text relevance in\ngenerated videos. Building on this, we conduct a comprehensive evaluation of 16\nrepresentative S2V models, highlighting their strengths and weaknesses across\ndifferent content. Moreover, we create the first open-source large-scale S2V\ngeneration dataset OpenS2V-5M, which consists of five million high-quality 720P\nsubject-text-video triples. Specifically, we ensure subject-information\ndiversity in our dataset by (1) segmenting subjects and building pairing\ninformation via cross-video associations and (2) prompting GPT-Image-1 on raw\nframes to synthesize multi-view representations. Through OpenS2V-Nexus, we\ndeliver a robust infrastructure to accelerate future S2V generation research."}
{"id": "2505.20294", "pdf": "https://arxiv.org/pdf/2505.20294", "abs": "https://arxiv.org/abs/2505.20294", "authors": ["Xiao Chen", "Tai Wang", "Quanyi Li", "Tao Huang", "Jiangmiao Pang", "Tianfan Xue"], "title": "GLEAM: Learning Generalizable Exploration Policy for Active Mapping in Complex 3D Indoor Scenes", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Project page: https://xiao-chen.tech/gleam/", "summary": "Generalizable active mapping in complex unknown environments remains a\ncritical challenge for mobile robots. Existing methods, constrained by\ninsufficient training data and conservative exploration strategies, exhibit\nlimited generalizability across scenes with diverse layouts and complex\nconnectivity. To enable scalable training and reliable evaluation, we introduce\nGLEAM-Bench, the first large-scale benchmark designed for generalizable active\nmapping with 1,152 diverse 3D scenes from synthetic and real-scan datasets.\nBuilding upon this foundation, we propose GLEAM, a unified generalizable\nexploration policy for active mapping. Its superior generalizability comes\nmainly from our semantic representations, long-term navigable goals, and\nrandomized strategies. It significantly outperforms state-of-the-art methods,\nachieving 66.50% coverage (+9.49%) with efficient trajectories and improved\nmapping accuracy on 128 unseen complex scenes. Project page:\nhttps://xiao-chen.tech/gleam/."}
{"id": "2505.20295", "pdf": "https://arxiv.org/pdf/2505.20295", "abs": "https://arxiv.org/abs/2505.20295", "authors": ["Michael Kirchhof", "Luca Fger", "Adam Goliski", "Eeshan Gunesh Dhekane", "Arno Blaas", "Sinead Williamson"], "title": "Self-reflective Uncertainties: Do LLMs Know Their Internal Answer Distribution?", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": null, "summary": "To reveal when a large language model (LLM) is uncertain about a response,\nuncertainty quantification commonly produces percentage numbers along with the\noutput. But is this all we can do? We argue that in the output space of LLMs,\nthe space of strings, exist strings expressive enough to summarize the\ndistribution over output strings the LLM deems possible. We lay a foundation\nfor this new avenue of uncertainty explication and present SelfReflect, a\ntheoretically-motivated metric to assess how faithfully a string summarizes an\nLLM's internal answer distribution. We show that SelfReflect is able to\ndiscriminate even subtle differences of candidate summary strings and that it\naligns with human judgement, outperforming alternative metrics such as LLM\njudges and embedding comparisons. With SelfReflect, we investigate a number of\nself-summarization methods and find that even state-of-the-art reasoning models\nstruggle to explicate their internal uncertainty. But we find that faithful\nsummarizations can be generated by sampling and summarizing. Our metric enables\nfuture works towards this universal form of LLM uncertainties."}
{"id": "2505.20296", "pdf": "https://arxiv.org/pdf/2505.20296", "abs": "https://arxiv.org/abs/2505.20296", "authors": ["Jiahao Lu", "Ziwei Xu", "Mohan Kankanhalli"], "title": "Reasoning LLMs are Wandering Solution Explorers", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM"], "comment": "71 pages, 14 figures, 2 tables", "summary": "Large Language Models (LLMs) have demonstrated impressive reasoning abilities\nthrough test-time computation (TTC) techniques such as chain-of-thought\nprompting and tree-based reasoning. However, we argue that current reasoning\nLLMs (RLLMs) lack the ability to systematically explore the solution space.\nThis paper formalizes what constitutes systematic problem solving and\nidentifies common failure modes that reveal reasoning LLMs to be wanderers\nrather than systematic explorers. Through qualitative and quantitative analysis\nacross multiple state-of-the-art LLMs, we uncover persistent issues: invalid\nreasoning steps, redundant explorations, hallucinated or unfaithful\nconclusions, and so on. Our findings suggest that current models' performance\ncan appear to be competent on simple tasks yet degrade sharply as complexity\nincreases. Based on the findings, we advocate for new metrics and tools that\nevaluate not just final outputs but the structure of the reasoning process\nitself."}
{"id": "2505.20298", "pdf": "https://arxiv.org/pdf/2505.20298", "abs": "https://arxiv.org/abs/2505.20298", "authors": ["Jeonghun Baek", "Kazuki Egashira", "Shota Onohara", "Atsuyuki Miyai", "Yuki Imajuku", "Hikaru Ikuta", "Kiyoharu Aizawa"], "title": "MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal Manga Understanding", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "20 pages, 11 figures", "summary": "Manga, or Japanese comics, is a richly multimodal narrative form that blends\nimages and text in complex ways. Teaching large multimodal models (LMMs) to\nunderstand such narratives at a human-like level could help manga creators\nreflect on and refine their stories. To this end, we introduce two benchmarks\nfor multimodal manga understanding: MangaOCR, which targets in-page text\nrecognition, and MangaVQA, a novel benchmark designed to evaluate contextual\nunderstanding through visual question answering. MangaVQA consists of 526\nhigh-quality, manually constructed question-answer pairs, enabling reliable\nevaluation across diverse narrative and visual scenarios. Building on these\nbenchmarks, we develop MangaLMM, a manga-specialized model finetuned from the\nopen-source LMM Qwen2.5-VL to jointly handle both tasks. Through extensive\nexperiments, including comparisons with proprietary models such as GPT-4o and\nGemini 2.5, we assess how well LMMs understand manga. Our benchmark and model\nprovide a comprehensive foundation for evaluating and advancing LMMs in the\nrichly narrative domain of manga."}
