<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 255]
- [cs.AI](#cs.AI) [Total: 127]
- [cs.LG](#cs.LG) [Total: 301]
- [stat.ML](#stat.ML) [Total: 28]
- [cs.DB](#cs.DB) [Total: 3]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [eess.AS](#eess.AS) [Total: 9]
- [cs.DL](#cs.DL) [Total: 4]
- [cs.PL](#cs.PL) [Total: 2]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [cs.CV](#cs.CV) [Total: 75]
- [cs.CR](#cs.CR) [Total: 22]
- [cs.SE](#cs.SE) [Total: 13]
- [quant-ph](#quant-ph) [Total: 2]
- [math.NA](#math.NA) [Total: 1]
- [eess.IV](#eess.IV) [Total: 7]
- [eess.SP](#eess.SP) [Total: 15]
- [stat.ME](#stat.ME) [Total: 3]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.HC](#cs.HC) [Total: 6]
- [math.CT](#math.CT) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.CY](#cs.CY) [Total: 10]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [cs.IR](#cs.IR) [Total: 16]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.RO](#cs.RO) [Total: 18]
- [econ.GN](#econ.GN) [Total: 2]
- [cs.DS](#cs.DS) [Total: 3]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.SD](#cs.SD) [Total: 16]
- [eess.SY](#eess.SY) [Total: 3]
- [cs.MA](#cs.MA) [Total: 6]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [math.OC](#math.OC) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Advancing Uto-Aztecan Language Technologies: A Case Study on the Endangered Comanche Language](https://arxiv.org/abs/2505.18159)
*Jesus Alvarez C,Daua D. Karajeanes,Ashley Celeste Prado,John Ruttan,Ivory Yang,Sean O'Brien,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TL;DR: 该研究首次对濒危语言科曼奇语进行了计算语言学分析，通过低成本、社区参与的NLP方法支持语言保护，并验证了少量样本提示能显著提升大语言模型对科曼奇语的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 濒危语言的数字排斥问题限制了语言研究和复兴工作，科曼奇语作为濒临灭绝的尤托-阿兹特克语系语言，亟需计算语言学领域的关注和支持。

Method: 研究构建了包含412个短语的手工标注数据集，开发了合成数据生成流程，并实证评估了GPT-4o系列模型在零样本和少量样本提示下的语言识别表现。

Result: 实验表明：大语言模型在零样本设置下对科曼奇语识别困难，但仅需5个示例的少量样本提示即可实现接近完美的准确率（从20%提升至95%以上）。

Conclusion: 研究证实了针对性NLP方法在低资源语境中的潜力，强调可见性是语言包容的第一步，并为科曼奇语建立了NLP研究基础，倡导注重可访问性、文化敏感性和社区参与的计算方法。

Abstract: The digital exclusion of endangered languages remains a critical challenge in
NLP, limiting both linguistic research and revitalization efforts. This study
introduces the first computational investigation of Comanche, an Uto-Aztecan
language on the verge of extinction, demonstrating how minimal-cost,
community-informed NLP interventions can support language preservation. We
present a manually curated dataset of 412 phrases, a synthetic data generation
pipeline, and an empirical evaluation of GPT-4o and GPT-4o-mini for language
identification. Our experiments reveal that while LLMs struggle with Comanche
in zero-shot settings, few-shot prompting significantly improves performance,
achieving near-perfect accuracy with just five examples. Our findings highlight
the potential of targeted NLP methodologies in low-resource contexts and
emphasize that visibility is the first step toward inclusion. By establishing a
foundation for Comanche in NLP, we advocate for computational approaches that
prioritize accessibility, cultural sensitivity, and community engagement.

</details>


### [2] [Do BERT-Like Bidirectional Models Still Perform Better on Text Classification in the Era of LLMs?](https://arxiv.org/abs/2505.18215)
*Junyan Zhang,Yiming Huang,Shuliang Liu,Yubo Gao,Xuming Hu*

Main category: cs.CL

TL;DR: 研究发现传统BERT类模型在文本分类任务中常优于大型语言模型(LLM)，并提出基于任务特性的细粒度模型选择策略TaMAS。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的快速普及掩盖了传统BERT类模型的优势，本文旨在挑战这种'LLM中心化'趋势，探究不同模型在文本分类任务中的实际表现差异。

Method: 系统比较三种方法(BERT微调、LLM内部状态利用、零样本推理)，在六个高难度数据集上进行测试，并通过PCA和探测实验分析任务特性。

Result: BERT类模型在模式驱动任务中表现更优，而LLM在需要深度语义或世界知识的任务中占优。根据任务特性将数据集分为三类。

Conclusion: 提出TaMAS策略，主张根据任务特性选择模型，反对盲目依赖LLM的'一刀切'做法。

Abstract: The rapid adoption of LLMs has overshadowed the potential advantages of
traditional BERT-like models in text classification. This study challenges the
prevailing "LLM-centric" trend by systematically comparing three category
methods, i.e., BERT-like models fine-tuning, LLM internal state utilization,
and zero-shot inference across six high-difficulty datasets. Our findings
reveal that BERT-like models often outperform LLMs. We further categorize
datasets into three types, perform PCA and probing experiments, and identify
task-specific model strengths: BERT-like models excel in pattern-driven tasks,
while LLMs dominate those requiring deep semantics or world knowledge. Based on
this, we propose TaMAS, a fine-grained task selection strategy, advocating for
a nuanced, task-driven approach over a one-size-fits-all reliance on LLMs.

</details>


### [3] [CoMet: Metaphor-Driven Covert Communication for Multi-Agent Language Games](https://arxiv.org/abs/2505.18218)
*Shuhang Xu,Fangwei Zhong*

Main category: cs.CL

TL;DR: 论文提出CoMet框架，通过结合假设推理和自反思生成，提升大语言模型在多智能体语言游戏中的隐喻处理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在理解和应用隐喻方面存在困难，影响了其在需要隐蔽沟通和语义规避的战略交互中的表现。

Method: CoMet框架整合了基于假设的隐喻推理器和通过自反思与知识整合改进的隐喻生成器。

Result: 在Undercover和Adversarial Taboo游戏中的实验表明，CoMet显著提升了智能体使用隐喻进行战略沟通的能力。

Conclusion: CoMet有效增强了基于大语言模型的智能体在隐喻理解和应用上的能力，改善了交互的战略性和微妙性。

Abstract: Metaphors are a crucial way for humans to express complex or subtle ideas by
comparing one concept to another, often from a different domain. However, many
large language models (LLMs) struggle to interpret and apply metaphors in
multi-agent language games, hindering their ability to engage in covert
communication and semantic evasion, which are crucial for strategic
communication. To address this challenge, we introduce CoMet, a framework that
enables LLM-based agents to engage in metaphor processing. CoMet combines a
hypothesis-based metaphor reasoner with a metaphor generator that improves
through self-reflection and knowledge integration. This enhances the agents'
ability to interpret and apply metaphors, improving the strategic and nuanced
quality of their interactions. We evaluate CoMet on two multi-agent language
games - Undercover and Adversarial Taboo - which emphasize Covert Communication
and Semantic Evasion. Experimental results demonstrate that CoMet significantly
enhances the agents' ability to communicate strategically using metaphors.

</details>


### [4] [IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis](https://arxiv.org/abs/2505.18223)
*Hanyu Li,Haoyu Liu,Tingyu Zhu,Tianyu Guo,Zeyu Zheng,Xiaotie Deng,Michael I. Jordan*

Main category: cs.CL

TL;DR: 论文提出IDA-Bench基准测试，评估大语言模型在多轮交互场景中的数据分析能力，发现现有先进模型成功率不足50%，强调需提升多轮交互能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试忽视数据分析的迭代特性，无法全面评估大语言模型在多轮交互中的表现，需开发更贴近实际场景的评估方法。

Method: 基于复杂Kaggle笔记本构建IDA-Bench，通过LLM模拟用户发出序列化自然语言指令，以最终数值输出与人工基准的匹配度评估模型表现。

Result: 实验表明，即使最先进的编码代理（如Claude-3.7-thinking）任务成功率低于50%，单轮测试中未暴露的局限性显现。

Conclusion: 需提升大语言模型的多轮交互能力以构建更可靠的数据分析代理，平衡指令遵循与推理能力是关键。

Abstract: Large Language Models (LLMs) show promise as data analysis agents, but
existing benchmarks overlook the iterative nature of the field, where experts'
decisions evolve with deeper insights of the dataset. To address this, we
introduce IDA-Bench, a novel benchmark evaluating LLM agents in multi-round
interactive scenarios. Derived from complex Kaggle notebooks, tasks are
presented as sequential natural language instructions by an LLM-simulated user.
Agent performance is judged by comparing its final numerical output to the
human-derived baseline. Initial results show that even state-of-the-art coding
agents (like Claude-3.7-thinking) succeed on < 50% of the tasks, highlighting
limitations not evident in single-turn tests. This work underscores the need to
improve LLMs' multi-round capabilities for building more reliable data analysis
agents, highlighting the necessity of achieving a balance between instruction
following and reasoning.

</details>


### [5] [Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens](https://arxiv.org/abs/2505.18237)
*Xixian Yong,Xiao Zhou,Yingying Zhang,Jinlin Li,Yefeng Zheng,Xian Wu*

Main category: cs.CL

TL;DR: 本文通过信息论视角分析大型推理模型（LRMs）的效率，提出两种指标量化推理路径偏差和信息增益，并设计自适应停止策略以提升效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型虽在多步推理上表现优异，但常因生成长推理链导致效率低下。本文旨在探索推理长度与语义效率间的权衡。

Method: 提出InfoBias和InfoGain指标衡量推理路径偏差和单步信息贡献，并基于熵设计动态停止推理的自适应策略（Adaptive Think）。

Result: 在六项基准任务中，相比默认模式，该策略平均准确率提升1.10%，token使用量减少50.80%，兼顾效率与性能。

Conclusion: 基于熵的方法可显著提升大语言模型部署的准确性和成本效益，为高效推理提供新方向。

Abstract: The recent rise of Large Reasoning Models (LRMs) has significantly improved
multi-step reasoning performance, but often at the cost of generating
excessively long reasoning chains. This paper revisits the efficiency of such
reasoning processes through an information-theoretic lens, revealing a
fundamental trade-off between reasoning length and semantic efficiency. We
propose two metrics, InfoBias and InfoGain, to quantify divergence from ideal
reasoning paths and stepwise information contribution, respectively. Empirical
analyses show that longer reasoning chains tend to exhibit higher information
bias and diminishing information gain, especially for incorrect answers.
Motivated by these findings, we introduce an entropy-based Adaptive Think
strategy that dynamically halts reasoning once confidence is sufficiently high,
improving efficiency while maintaining competitive accuracy. Compared to the
Vanilla Think approach (default mode), our strategy yields a 1.10% improvement
in average accuracy and a 50.80% reduction in token usage on QwQ-32B across six
benchmark tasks spanning diverse reasoning types and difficulty levels,
demonstrating superior efficiency and reasoning performance. These results
underscore the promise of entropy-based methods for enhancing both accuracy and
cost-effiiciency in large language model deployment.

</details>


### [6] [Taming LLMs with Negative Samples: A Reference-Free Framework to Evaluate Presentation Content with Actionable Feedback](https://arxiv.org/abs/2505.18240)
*Ananth Muppidi,Tarak Das,Sambaran Bandyopadhyay,Tripti Shukla,Dharun D A*

Main category: cs.CL

TL;DR: 该论文提出了一种自动评估演示文稿内容质量的方法REFLEX，通过构建基准数据集RefSlides和设计无参考评估技术，优于传统启发式和基于大语言模型的评估方法。


<details>
  <summary>Details</summary>
Motivation: 在生成式AI时代，自动生成演示文稿是一个重要问题。论文旨在评估能有效总结文档并向广泛受众传达概念的多模态演示内容。

Method: 构建高质量人工演示数据集RefSlides，提出内容特性评估指标，通过生成负样本微调LLMs实现无参考评估（REFLEX）。

Result: 自动化与人工实验表明，该方法在生成评分和解释方面优于传统启发式和最先进的大语言模型评估。

Conclusion: REFLEX无需真实演示作为参考即可提供可操作的反馈，为自动演示生成系统的评估提供了有效解决方案。

Abstract: The generation of presentation slides automatically is an important problem
in the era of generative AI. This paper focuses on evaluating multimodal
content in presentation slides that can effectively summarize a document and
convey concepts to a broad audience. We introduce a benchmark dataset,
RefSlides, consisting of human-made high-quality presentations that span
various topics. Next, we propose a set of metrics to characterize different
intrinsic properties of the content of a presentation and present REFLEX, an
evaluation approach that generates scores and actionable feedback for these
metrics. We achieve this by generating negative presentation samples with
different degrees of metric-specific perturbations and use them to fine-tune
LLMs. This reference-free evaluation technique does not require ground truth
presentations during inference. Our extensive automated and human experiments
demonstrate that our evaluation approach outperforms classical heuristic-based
and state-of-the-art large language model-based evaluations in generating
scores and explanations.

</details>


### [7] [Multi-Scale Probabilistic Generation Theory: A Hierarchical Framework for Interpreting Large Language Models](https://arxiv.org/abs/2505.18244)
*Yukin Zhang,Qi Dong*

Main category: cs.CL

TL;DR: 该论文提出了多尺度概率生成理论（MSPGT），通过分层框架解析Transformer模型生成文本的三个语义尺度，并验证了不同尺度对文本生成的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的大型语言模型表现出色，但其生成文本的内部机制仍不透明。研究旨在揭示模型如何规划、结构和实现文本生成。

Method: 引入MSPGT框架，将生成过程分解为全局上下文、中间结构和局部词选择三个尺度，并通过注意力跨度阈值和层间互信息峰值确定尺度边界。

Result: 在四种代表性模型（GPT-2、BERT、RoBERTa和T5）上验证了稳定的尺度划分，发现解码器模型更注重中间和全局处理，而编码器模型侧重局部特征提取。

Conclusion: MSPGT提供了一种统一的、与架构无关的方法，用于解释、诊断和控制大型语言模型，填补了机制可解释性与涌现能力之间的鸿沟。

Abstract: Large Transformer based language models achieve remarkable performance but
remain opaque in how they plan, structure, and realize text. We introduce
Multi_Scale Probabilistic Generation Theory (MSPGT), a hierarchical framework
that factorizes generation into three semantic scales_global context,
intermediate structure, and local word choices and aligns each scale with
specific layer ranges in Transformer architectures. To identify scale
boundaries, we propose two complementary metrics: attention span thresholds and
inter layer mutual information peaks. Across four representative models (GPT-2,
BERT, RoBERTa, and T5), these metrics yield stable local/intermediate/global
partitions, corroborated by probing tasks and causal interventions. We find
that decoder_only models allocate more layers to intermediate and global
processing while encoder_only models emphasize local feature extraction.
Through targeted interventions, we demonstrate that local scale manipulations
primarily influence lexical diversity, intermediate-scale modifications affect
sentence structure and length, and global_scale perturbations impact discourse
coherence all with statistically significant effects. MSPGT thus offers a
unified, architecture-agnostic method for interpreting, diagnosing, and
controlling large language models, bridging the gap between mechanistic
interpretability and emergent capabilities.

</details>


### [8] [MetaGen Blended RAG: Higher Accuracy for Domain-Specific Q&A Without Fine-Tuning](https://arxiv.org/abs/2505.18247)
*Kunal Sawarkar,Shivam R. Solanki,Abhilasha Mangal*

Main category: cs.CL

TL;DR: 该论文提出了一种名为'MetaGen Blended RAG'的方法，通过混合查询索引和元数据增强来提升企业特定领域数据的检索增强生成（RAG）准确性，避免了微调的高成本和过拟合问题，并在多个基准测试中取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 企业特定领域的数据集由于术语复杂、语义多变，导致RAG系统在检索时精度不足。现有的微调或结合微调的RAG方法不仅成本高、速度慢，而且在新数据出现时缺乏泛化能力。

Method: 论文提出了一种混合查询索引和元数据增强的方法，构建了一个元数据生成管道，包括关键概念、主题和缩写，并创建了一个元数据增强的混合索引，通过提升搜索查询来优化检索效果。

Result: 在生物医学领域的PubMedQA基准测试中，该方法实现了82%的检索准确率和77%的RAG准确率，超过了所有未微调的RAG方法，并在零样本设置下超越了如GPT3.5等更大模型的表现，结果甚至可与最佳微调模型媲美。

Conclusion: 该方法不仅避免了过拟合，还能有效泛化到不同领域，展示了其在企业搜索中的鲁棒性和可扩展性，为特定领域数据的RAG应用提供了新的解决方案。

Abstract: Despite the widespread exploration of Retrieval-Augmented Generation (RAG),
its deployment in enterprises for domain-specific datasets remains limited due
to poor answer accuracy. These corpora, often shielded behind firewalls in
private enterprise knowledge bases, having complex, domain-specific
terminology, rarely seen by LLMs during pre-training; exhibit significant
semantic variability across domains (like networking, military, or legal,
etc.), or even within a single domain like medicine, and thus result in poor
context precision for RAG systems. Currently, in such situations, fine-tuning
or RAG with fine-tuning is attempted, but these approaches are slow, expensive,
and lack generalization for accuracy as the new domain-specific data emerges.
We propose an approach for Enterprise Search that focuses on enhancing the
retriever for a domain-specific corpus through hybrid query indexes and
metadata enrichment. This 'MetaGen Blended RAG' method constructs a metadata
generation pipeline using key concepts, topics, and acronyms, and then creates
a metadata-enriched hybrid index with boosted search queries. This approach
avoids overfitting and generalizes effectively across domains. On the PubMedQA
benchmark for the biomedical domain, the proposed method achieves 82% retrieval
accuracy and 77% RAG accuracy, surpassing all previous RAG accuracy results
without fine-tuning and sets a new benchmark for zero-shot results while
outperforming much larger models like GPT3.5. The results are even comparable
to the best fine-tuned models on this dataset, and we further demonstrate the
robustness and scalability of the approach by evaluating it on other Q&A
datasets like SQuAD, NQ etc.

</details>


### [9] [TAGS: A Test-Time Generalist-Specialist Framework with Retrieval-Augmented Reasoning and Verification](https://arxiv.org/abs/2505.18283)
*Jianghao Wu,Feilong Tang,Yulong Li,Ming Hu,Haochen Xue,Shoaib Jameel,Yutong Xie,Imran Razzak*

Main category: cs.CL

TL;DR: 提出TAGS框架，结合通用模型与领域专家模型，无需微调即可提升医疗问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有提示方法浅层不稳定，微调模型泛化性差且难以适应新场景，需改进医疗推理能力。

Method: TAGS框架整合通用与专科模型，通过分层检索机制和可靠性评分器辅助推理。

Result: 在MedQA基准上显著提升模型准确率，超越多个微调医疗大模型。

Conclusion: TAGS无需参数更新即可有效增强医疗推理，为领域适应提供新思路。

Abstract: Recent advances such as Chain-of-Thought prompting have significantly
improved large language models (LLMs) in zero-shot medical reasoning. However,
prompting-based methods often remain shallow and unstable, while fine-tuned
medical LLMs suffer from poor generalization under distribution shifts and
limited adaptability to unseen clinical scenarios. To address these
limitations, we present TAGS, a test-time framework that combines a broadly
capable generalist with a domain-specific specialist to offer complementary
perspectives without any model fine-tuning or parameter updates. To support
this generalist-specialist reasoning process, we introduce two auxiliary
modules: a hierarchical retrieval mechanism that provides multi-scale exemplars
by selecting examples based on both semantic and rationale-level similarity,
and a reliability scorer that evaluates reasoning consistency to guide final
answer aggregation. TAGS achieves strong performance across nine MedQA
benchmarks, boosting GPT-4o accuracy by 13.8%, DeepSeek-R1 by 16.8%, and
improving a vanilla 7B model from 14.1% to 23.9%. These results surpass several
fine-tuned medical LLMs, without any parameter updates. The code will be
available at https://github.com/JianghaoWu/TAGS.

</details>


### [10] [Thinking Fast and Right: Balancing Accuracy and Reasoning Length with Adaptive Rewards](https://arxiv.org/abs/2505.18298)
*Jinyan Su,Claire Cardie*

Main category: cs.CL

TL;DR: 该论文提出了一种自适应奖励调整方法，使大语言模型在保持正确性的同时生成更简洁的输出，显著降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习训练模型在数学任务中常产生冗长的推理过程，增加了推理成本和延迟。固定长度惩罚方法难以适应模型能力变化，效果有限。

Method: 提出一种自适应奖励调整方法，动态平衡准确性和输出长度：当准确性高时增加长度惩罚以鼓励简洁输出；准确性下降时放松惩罚以保持正确性。

Result: 在多个数据集上的实验表明，该方法能大幅减少推理长度，同时基本保持准确性。

Conclusion: 该方法为大语言模型的成本高效自适应推理提供了新方向。

Abstract: Large language models (LLMs) have demonstrated strong reasoning abilities in
mathematical tasks, often enhanced through reinforcement learning (RL).
However, RL-trained models frequently produce unnecessarily long reasoning
traces -- even for simple queries -- leading to increased inference costs and
latency. While recent approaches attempt to control verbosity by adding length
penalties to the reward function, these methods rely on fixed penalty terms
that are hard to tune and cannot adapt as the model's reasoning capability
evolves, limiting their effectiveness. In this work, we propose an adaptive
reward-shaping method that enables LLMs to "think fast and right" -- producing
concise outputs without sacrificing correctness. Our method dynamically adjusts
the reward trade-off between accuracy and response length based on model
performance: when accuracy is high, the length penalty increases to encourage
faster length reduction; when accuracy drops, the penalty is relaxed to
preserve correctness. This adaptive reward accelerates early-stage length
reduction while avoiding over-compression in later stages. Experiments across
multiple datasets show that our approach consistently and dramatically reduces
reasoning length while largely maintaining accuracy, offering a new direction
for cost-efficient adaptive reasoning in large-scale language models.

</details>


### [11] [Is It Bad to Work All the Time? Cross-Cultural Evaluation of Social Norm Biases in GPT-4](https://arxiv.org/abs/2505.18322)
*Zhuozhuo Joy Liu,Farhan Samir,Mehar Bhatia,Laura K. Nelson,Vered Shwartz*

Main category: cs.CL

TL;DR: 研究发现GPT-4生成的文化规范缺乏文化特异性，且隐含刻板印象，需改进以公平服务多元用户。


<details>
  <summary>Details</summary>
Motivation: 现有研究显示LLMs与西方价值观对齐，但缺乏对其在实际场景中应用这些价值观的验证。

Method: 采用自下而上的方法，让LLMs推理不同文化叙事中的文化规范。

Result: GPT-4生成的规范文化特异性不足，且隐含的刻板印象容易被恢复。

Conclusion: 解决这些问题对开发公平服务多元用户的LLMs至关重要。

Abstract: LLMs have been demonstrated to align with the values of Western or North
American cultures. Prior work predominantly showed this effect through
leveraging surveys that directly ask (originally people and now also LLMs)
about their values. However, it is hard to believe that LLMs would consistently
apply those values in real-world scenarios. To address that, we take a
bottom-up approach, asking LLMs to reason about cultural norms in narratives
from different cultures. We find that GPT-4 tends to generate norms that, while
not necessarily incorrect, are significantly less culture-specific. In
addition, while it avoids overtly generating stereotypes, the stereotypical
representations of certain cultures are merely hidden rather than suppressed in
the model, and such stereotypes can be easily recovered. Addressing these
challenges is a crucial step towards developing LLMs that fairly serve their
diverse user base.

</details>


### [12] [PerMedCQA: Benchmarking Large Language Models on Medical Consumer Question Answering in Persian Language](https://arxiv.org/abs/2505.18331)
*Naghmeh Jamali,Milad Mohammadi,Danial Baledi,Zahra Rezvani,Hesham Faili*

Main category: cs.CL

TL;DR: 该论文介绍了PerMedCQA，首个波斯语医疗问答基准数据集，用于评估大语言模型在真实世界消费者医疗问题上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前医疗问答系统在低资源语言（如波斯语）中缺乏消费者导向和多语言资源，限制了患者获取个性化可靠健康信息的能力。

Method: 从大型医疗问答论坛收集数据，经过清洗得到68,138对问答，使用MedJudge评估框架（基于LLM评分器）评估多语言和指令调优的LLMs。

Result: 研究揭示了多语言医疗问答的关键挑战，为开发更准确、上下文感知的医疗辅助系统提供了宝贵见解。

Conclusion: PerMedCQA填补了波斯语医疗问答资源的空白，为未来研究和系统开发奠定了基础。

Abstract: Medical consumer question answering (CQA) is crucial for empowering patients
by providing personalized and reliable health information. Despite recent
advances in large language models (LLMs) for medical QA, consumer-oriented and
multilingual resources, particularly in low-resource languages like Persian,
remain sparse. To bridge this gap, we present PerMedCQA, the first
Persian-language benchmark for evaluating LLMs on real-world,
consumer-generated medical questions. Curated from a large medical QA forum,
PerMedCQA contains 68,138 question-answer pairs, refined through careful data
cleaning from an initial set of 87,780 raw entries. We evaluate several
state-of-the-art multilingual and instruction-tuned LLMs, utilizing MedJudge, a
novel rubric-based evaluation framework driven by an LLM grader, validated
against expert human annotators. Our results highlight key challenges in
multilingual medical QA and provide valuable insights for developing more
accurate and context-aware medical assistance systems. The data is publicly
available on https://huggingface.co/datasets/NaghmehAI/PerMedCQA

</details>


### [13] [Model Editing with Graph-Based External Memory](https://arxiv.org/abs/2505.18343)
*Yash Kumar Atri,Ahmed Alaa,Thomas Hartvigsen*

Main category: cs.CL

TL;DR: 论文提出HYPE框架，利用双曲几何和图神经网络解决大语言模型编辑中的过拟合和灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在幻觉和知识过时问题，现有编辑方法易导致过拟合和灾难性遗忘，需更精准稳定的编辑技术。

Method: HYPE框架包含：1) 双曲图构建（保留知识层级关系）；2) 莫比乌斯变换更新（保持双曲空间结构）；3) 双重稳定机制（防止灾难性遗忘）。

Result: 在CounterFact等数据集上，HYPE显著提升了GPT-J等模型的编辑稳定性、事实准确性和多跳推理能力。

Conclusion: 双曲几何与图神经网络的结合为动态更新语言模型提供了新方向，解决了传统方法的关键缺陷。

Abstract: Large language models (LLMs) have revolutionized natural language processing,
yet their practical utility is often limited by persistent issues of
hallucinations and outdated parametric knowledge. Although post-training model
editing offers a pathway for dynamic updates, existing methods frequently
suffer from overfitting and catastrophic forgetting. To tackle these
challenges, we propose a novel framework that leverages hyperbolic geometry and
graph neural networks for precise and stable model edits. We introduce HYPE
(HYperbolic Parameter Editing), which comprises three key components: (i)
Hyperbolic Graph Construction, which uses Poincar\'e embeddings to represent
knowledge triples in hyperbolic space, preserving hierarchical relationships
and preventing unintended side effects by ensuring that edits to parent
concepts do not inadvertently affect child concepts; (ii) M\"obius-Transformed
Updates, which apply hyperbolic addition to propagate edits while maintaining
structural consistency within the hyperbolic manifold, unlike conventional
Euclidean updates that distort relational distances; and (iii) Dual
Stabilization, which combines gradient masking and periodic GNN parameter
resetting to prevent catastrophic forgetting by focusing updates on critical
parameters and preserving long-term knowledge. Experiments on CounterFact,
CounterFact+, and MQuAKE with GPT-J and GPT2-XL demonstrate that HYPE
significantly enhances edit stability, factual accuracy, and multi-hop
reasoning.

</details>


### [14] [The Unreasonable Effectiveness of Model Merging for Cross-Lingual Transfer in LLMs](https://arxiv.org/abs/2505.18356)
*Lucas Bandarkar,Nanyun Peng*

Main category: cs.CL

TL;DR: 论文研究了在低资源语言中通过模块化方法提升大语言模型的跨语言迁移能力，特别是在数学推理任务上。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在非高资源语言任务上表现不佳，尤其是在缺乏任务特定数据的情况下。本文旨在探索如何通过跨语言迁移提升低资源语言任务的性能。

Method: 通过验证数学推理和多语言能力的参数不重叠性，开发了多种模块化框架，包括参数冻结和模型合并，以优化任务和语言的参数组合。

Result: 模块化方法在三种语言、四种模型和两种微调范式下均优于基线，其中分层交换模型合并方法表现最佳。

Conclusion: 模块化方法能有效提升低资源语言任务的性能，分层交换模型合并是最优策略，且训练后调整参数比初始冻结效果更好。

Abstract: Large language models (LLMs) still struggle across tasks outside of
high-resource languages. In this work, we investigate cross-lingual transfer to
lower-resource languages where task-specific post-training data is scarce.
Building on prior work, we first validate that the subsets of model parameters
that matter most for mathematical reasoning and multilingual capabilities are
distinctly non-overlapping. To exploit this implicit separability between task
and target language parameterization, we develop and analyze numerous modular
frameworks to improve the composition of the two during fine-tuning. These
methods generally employ freezing parameters or post hoc model merging to
assign math and language improvement to different key parts of the LLM. In the
absence of in-language math data, we demonstrate that the modular approaches
successfully improve upon baselines across three languages, four models, and
two fine-tuning paradigms (full and LoRA). Furthermore, we identify the most
consistently successful modular method to be fine-tuning separate language and
math experts and model merging via Layer-Swapping, somewhat surprisingly. We
offer possible explanations for this result via recent works on the linearity
of task vectors. We further explain this by empirically showing that reverting
less useful fine-tuning updates after training often outperforms freezing them
from the start.

</details>


### [15] [SchemaGraphSQL: Efficient Schema Linking with Pathfinding Graph Algorithms for Text-to-SQL on Large-Scale Databases](https://arxiv.org/abs/2505.18363)
*AmirHossein Safdarian,Milad Mohammadi,Ehsan Jahanbakhsh,Mona Shahamat Naderi,Heshaam Faili*

Main category: cs.CL

TL;DR: 本文提出了一种零样本、无需训练的模式链接方法，通过构建模式图并利用Gemini 2.5 Flash提取查询中的表信息，结合路径查找算法优化SQL生成，在BIRD基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 模式链接是Text-to-SQL系统中的关键组件，能减少提示大小并提升模型专注度。现有方法通常需要训练或复杂多步处理，本文旨在提出一种简单、高效且可扩展的解决方案。

Method: 基于外键关系构建模式图，使用Gemini 2.5 Flash单次提示提取查询中的源表和目标表，应用经典路径查找算法和后处理确定最优表列连接序列。

Result: 该方法在BIRD基准测试中表现优异，超越了之前基于微调和复杂多步LLM的专用方法，并通过消融实验详细分析了精度-召回权衡。

Conclusion: 这种简单、低成本且高度可扩展的模式链接方法能显著提升Text-to-SQL系统的准确性，为实际应用提供了有效解决方案。

Abstract: Text-to-SQL systems translate natural language questions into executable SQL
queries, and recent progress with large language models (LLMs) has driven
substantial improvements in this task. Schema linking remains a critical
component in Text-to-SQL systems, reducing prompt size for models with narrow
context windows and sharpening model focus even when the entire schema fits. We
present a zero-shot, training-free schema linking approach that first
constructs a schema graph based on foreign key relations, then uses a single
prompt to Gemini 2.5 Flash to extract source and destination tables from the
user query, followed by applying classical path-finding algorithms and
post-processing to identify the optimal sequence of tables and columns that
should be joined, enabling the LLM to generate more accurate SQL queries.
Despite being simple, cost-effective, and highly scalable, our method achieves
state-of-the-art results on the BIRD benchmark, outperforming previous
specialized, fine-tuned, and complex multi-step LLM-based approaches. We
conduct detailed ablation studies to examine the precision-recall trade-off in
our framework. Additionally, we evaluate the execution accuracy of our schema
filtering method compared to other approaches across various model sizes.

</details>


### [16] [ShIOEnv: A CLI Behavior-Capturing Environment Enabling Grammar-Guided Command Synthesis for Dataset Curation](https://arxiv.org/abs/2505.18374)
*Jarrod Ragsdale,Rajendra Boppana*

Main category: cs.CL

TL;DR: 论文提出ShIOEnv环境，通过语法掩码和PPO优化生成高质量CLI交互数据集，用于微调CodeT5模型，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有CLI交互数据集缺乏执行数据（如退出码、输出等），限制了行为建模的可用性。小型架构需要丰富数据集才能达到与大型预训练模型相似的模拟效果。

Method: 构建ShIOEnv环境，将命令构造建模为马尔可夫决策过程，从man页提取上下文无关语法掩码无效参数，结合随机采样和PPO优化生成四种探索策略。

Result: 语法掩码和PPO显著提高采样效率，生成更高质量数据集（最大化参数数量同时最小化冗余）。微调CodeT5后，BLEU-4提升85%（语法约束）和26%（PPO）。

Conclusion: ShIOEnv和配套数据集为CLI行为建模提供有效工具，未来可支持更多研究。

Abstract: Command-line interfaces (CLIs) provide structured textual environments for
system administration. Explorations have been performed using pre-trained
language models (PLMs) to simulate these environments for safe interaction in
high-risk environments. However, their use has been constrained to frozen,
large parameter models like GPT. For smaller architectures to reach a similar
level of believability, a rich dataset of CLI interactions is required.
Existing public datasets focus on mapping natural-language tasks to commands,
omitting crucial execution data such as exit codes, outputs, and environmental
side effects, limiting their usability for behavioral modeling. We introduce a
Shell Input -Output Environment (ShIOEnv), which casts command construction as
a Markov Decision Process whose state is the partially built sequence and whose
actions append arguments. After each action, ShIOEnv executes the candidate and
returns its exit status, output, and progress toward a minimal-length
behavioral objective. Due to the intractable nature of the combinatorial
argument state-action space, we derive a context-free grammar from man pages to
mask invalid arguments from being emitted. We explore random and
proximal-policy optimization (PPO)-optimized sampling of unrestricted and
grammar-masked action spaces to produce four exploration strategies. We
observed that grammar masking and PPO significantly improve sample efficiency
to produce a higher quality dataset (maximizing the number of arguments while
minimizing redundancies). Policy-generated datasets of shell input-output
behavior pairs are used to fine-tune CodeT5, where we observe 85% improvements
in BLEU-4 when constraining the action space to grammar productions with an
additional 26% improvement when applying PPO. The ShIOEnv environment and
curated command behavior datasets are released for use in future research.

</details>


### [17] [NileChat: Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities](https://arxiv.org/abs/2505.18383)
*Abdellah El Mekki,Houdaifa Atou,Omer Nacar,Shady Shehata,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 该论文提出了一种方法，通过合成和检索的预训练数据来增强大型语言模型对低资源语言和文化多样性的支持，并以埃及和摩洛哥方言为例开发了NileChat模型，展示了其在语言理解和文化对齐方面的优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在处理低资源语言时，主要依赖从英语语料库翻译生成的合成数据，这导致模型往往与源语言文化对齐，而无法充分体现本地社区的文化遗产和价值观。

Method: 论文提出了一种结合合成和检索的预训练数据生成方法，针对特定社区的语言、文化遗产和价值观进行定制，并以埃及和摩洛哥方言为测试案例开发了NileChat模型。

Result: NileChat在语言理解、翻译和文化价值观对齐的基准测试中表现优于同类规模的阿拉伯语模型，并与更大规模的模型性能相当。

Conclusion: 通过NileChat的成功案例，论文展示了如何通过定制化的数据和方法，增强大型语言模型对低资源语言和文化多样性的支持，并呼吁社区共同努力促进更多样化的语言和文化在模型开发中的包容性。

Abstract: Enhancing the linguistic capabilities of Large Language Models (LLMs) to
include low-resource languages is a critical research area. Current research
directions predominantly rely on synthetic data generated by translating
English corpora, which, while demonstrating promising linguistic understanding
and translation abilities, often results in models aligned with source language
culture. These models frequently fail to represent the cultural heritage and
values of local communities. This work proposes a methodology to create both
synthetic and retrieval-based pre-training data tailored to a specific
community, considering its (i) language, (ii) cultural heritage, and (iii)
cultural values. We demonstrate our methodology using Egyptian and Moroccan
dialects as testbeds, chosen for their linguistic and cultural richness and
current underrepresentation in LLMs. As a proof-of-concept, we develop
NileChat, a 3B parameter LLM adapted for Egyptian and Moroccan communities,
incorporating their language, cultural heritage, and values. Our results on
various understanding, translation, and cultural and values alignment
benchmarks show that NileChat outperforms existing Arabic-aware LLMs of similar
size and performs on par with larger models. We share our methods, data, and
models with the community to promote the inclusion and coverage of more diverse
communities in LLM development.

</details>


### [18] [RaDeR: Reasoning-aware Dense Retrieval Models](https://arxiv.org/abs/2505.18405)
*Debrup Das,Sam O' Nuallain,Razieh Rahimi*

Main category: cs.CL

TL;DR: RaDeR是一种基于推理的密集检索模型，利用LLM生成的数学问题解决数据进行训练，显著提升了推理任务的检索性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决推理密集型任务中的检索问题，特别是数学和编码领域，研究者提出了RaDeR，旨在通过推理增强的检索方法来提升语言模型的推理能力。

Method: RaDeR利用LLM的检索增强推理轨迹和自反相关性评估，生成多样化和高难度的负样本，用于训练密集检索模型。

Result: RaDeR在BRIGHT和RAR-b基准测试中表现优异，尤其在数学和编码任务上显著优于基线模型，且仅需2.5%的训练数据即可达到或超越REASONIR的性能。

Conclusion: RaDeR展示了基于推理的检索方法在增强语言模型推理能力方面的潜力，特别是在处理复杂推理任务时，其性能显著优于传统检索方法。

Abstract: We propose RaDeR, a set of reasoning-based dense retrieval models trained
with data derived from mathematical problem solving using large language models
(LLMs). Our method leverages retrieval-augmented reasoning trajectories of an
LLM and self-reflective relevance evaluation, enabling the creation of both
diverse and hard-negative samples for reasoning-intensive relevance. RaDeR
retrievers, trained for mathematical reasoning, effectively generalize to
diverse reasoning tasks in the BRIGHT and RAR-b benchmarks, consistently
outperforming strong baselines in overall performance.Notably, RaDeR achieves
significantly higher performance than baselines on the Math and Coding splits.
In addition, RaDeR presents the first dense retriever that outperforms BM25
when queries are Chain-of-Thought reasoning steps, underscoring the critical
role of reasoning-based retrieval to augment reasoning language models.
Furthermore, RaDeR achieves comparable or superior performance while using only
2.5% of the training data used by the concurrent work REASONIR, highlighting
the quality of our synthesized training data.

</details>


### [19] [DanmakuTPPBench: A Multi-modal Benchmark for Temporal Point Process Modeling and Understanding](https://arxiv.org/abs/2505.18411)
*Yue Jiang,Jichu Li,Yang Liu,Dingkang Yang,Feng Zhou,Quyu Kong*

Main category: cs.CL

TL;DR: DanmakuTPPBench是一个多模态时序点过程（TPP）建模的基准测试，包含数据集和问答任务，旨在推动多模态事件动态建模的研究。


<details>
  <summary>Details</summary>
Motivation: 现有的TPP数据集多为单模态，限制了需要结合时间、文本和视觉信息进行联合推理的模型发展。

Method: 提出DanmakuTPPBench，包含从Bilibili平台提取的多模态事件数据集DanmakuTPP-Events，以及通过多智能体流程构建的问答数据集DanmakuTPP-QA。

Result: 评估显示当前方法在多模态事件动态建模方面存在显著性能差距和局限性。

Conclusion: 该基准测试为多模态语言建模中的TPP建模提供了基础，并呼吁进一步整合相关研究。

Abstract: We introduce DanmakuTPPBench, a comprehensive benchmark designed to advance
multi-modal Temporal Point Process (TPP) modeling in the era of Large Language
Models (LLMs). While TPPs have been widely studied for modeling temporal event
sequences, existing datasets are predominantly unimodal, hindering progress in
models that require joint reasoning over temporal, textual, and visual
information. To address this gap, DanmakuTPPBench comprises two complementary
components: (1) DanmakuTPP-Events, a novel dataset derived from the Bilibili
video platform, where user-generated bullet comments (Danmaku) naturally form
multi-modal events annotated with precise timestamps, rich textual content, and
corresponding video frames; (2) DanmakuTPP-QA, a challenging question-answering
dataset constructed via a novel multi-agent pipeline powered by
state-of-the-art LLMs and multi-modal LLMs (MLLMs), targeting complex
temporal-textual-visual reasoning. We conduct extensive evaluations using both
classical TPP models and recent MLLMs, revealing significant performance gaps
and limitations in current methods' ability to model multi-modal event
dynamics. Our benchmark establishes strong baselines and calls for further
integration of TPP modeling into the multi-modal language modeling landscape.
The code and dataset have been released at
https://github.com/FRENKIE-CHIANG/DanmakuTPPBench

</details>


### [20] [Retrieval Augmented Generation-based Large Language Models for Bridging Transportation Cybersecurity Legal Knowledge Gaps](https://arxiv.org/abs/2505.18426)
*Khandakar Ashrafi Akbar,Md Nahiyan Uddin,Latifur Khan,Trayce Hockstad,Mizanur Rahman,Mashrur Chowdhury,Bhavani Thuraisingham*

Main category: cs.CL

TL;DR: 论文提出基于检索增强生成(RAG)的大语言模型框架，用于辅助政策制定者处理交通技术发展中的网络安全与数据隐私法律问题，相比主流商业LLM在四项指标上表现更优。


<details>
  <summary>Details</summary>
Motivation: 随着智能交通系统发展，现有法律需更新以应对新兴网络安全与数据隐私挑战，需要AI工具辅助政策分析。

Method: 采用RAG框架结合领域特定问题集，通过检索机制增强大语言模型的事实依据和输出准确性。

Result: 该框架在AlignScore、ParaScore、BERTScore和ROUGE四项指标上超越主流商业LLM，能生成可靠且情境感知的法律分析。

Conclusion: RAG-based LLM为立法分析提供了可扩展的AI驱动方案，能有效支持交通技术发展背景下的法律框架更新。

Abstract: As connected and automated transportation systems evolve, there is a growing
need for federal and state authorities to revise existing laws and develop new
statutes to address emerging cybersecurity and data privacy challenges. This
study introduces a Retrieval-Augmented Generation (RAG) based Large Language
Model (LLM) framework designed to support policymakers by extracting relevant
legal content and generating accurate, inquiry-specific responses. The
framework focuses on reducing hallucinations in LLMs by using a curated set of
domain-specific questions to guide response generation. By incorporating
retrieval mechanisms, the system enhances the factual grounding and specificity
of its outputs. Our analysis shows that the proposed RAG-based LLM outperforms
leading commercial LLMs across four evaluation metrics: AlignScore, ParaScore,
BERTScore, and ROUGE, demonstrating its effectiveness in producing reliable and
context-aware legal insights. This approach offers a scalable, AI-driven method
for legislative analysis, supporting efforts to update legal frameworks in line
with advancements in transportation technologies.

</details>


### [21] [Voice of a Continent: Mapping Africa's Speech Technology Frontier](https://arxiv.org/abs/2505.18436)
*AbdelRahim Elmadany,Sang Yun Kwon,Hawau Olamide Toyin,Alcides Alcoba Inciarte,Hanan Aldarmaki,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 该论文针对非洲语言在语音技术中的代表性不足问题，提出了SimbaBench基准和Simba模型家族，以促进非洲语言的数字包容。


<details>
  <summary>Details</summary>
Motivation: 非洲丰富的语言多样性在语音技术中代表性不足，导致数字包容性障碍。

Method: 系统性地绘制非洲语音数据集和技术空间，创建SimbaBench基准，并开发Simba模型家族。

Result: Simba模型在多种非洲语言和语音任务中达到最先进性能，揭示了资源可用性和数据集质量对性能的影响。

Conclusion: 研究强调需要扩展语音技术资源以更好反映非洲语言多样性，为未来包容性语音技术研究奠定基础。

Abstract: Africa's rich linguistic diversity remains significantly underrepresented in
speech technologies, creating barriers to digital inclusion. To alleviate this
challenge, we systematically map the continent's speech space of datasets and
technologies, leading to a new comprehensive benchmark SimbaBench for
downstream African speech tasks. Using SimbaBench, we introduce the Simba
family of models, achieving state-of-the-art performance across multiple
African languages and speech tasks. Our benchmark analysis reveals critical
patterns in resource availability, while our model evaluation demonstrates how
dataset quality, domain diversity, and language family relationships influence
performance across languages. Our work highlights the need for expanded speech
technology resources that better reflect Africa's linguistic diversity and
provides a solid foundation for future research and development efforts toward
more inclusive speech technologies.

</details>


### [22] [Efficient Long CoT Reasoning in Small Language Models](https://arxiv.org/abs/2505.18440)
*Zhaoyang Wang,Jinqi Jiang,Tian Qiu,Hui Liu,Xianfeng Tang,Huaxiu Yao*

Main category: cs.CL

TL;DR: 该论文提出了一种修剪长链式思维推理中冗余步骤的方法，并通过策略优化帮助小语言模型学习有效的长链推理能力，在数学推理基准测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型能够生成复杂的长链式思维推理步骤，但小语言模型直接学习这种能力存在困难。长链推理中的冗余内容进一步增加了小模型的训练难度，因此需要一种方法来优化蒸馏过程。

Method: 提出了一种简单而有效的方法，修剪长链式思维推理中的不必要步骤，并采用策略优化方法让小语言模型自身筛选有效的长链推理训练数据。

Result: 实验结果表明，该方法成功地将长链推理能力蒸馏到小语言模型中，同时显著减少了冗余推理步骤的生成，保持了模型的竞争力。

Conclusion: 通过修剪冗余步骤和策略优化的方法，小语言模型能够高效学习长链推理能力，并在性能上保持竞争力。

Abstract: Recent large reasoning models such as DeepSeek-R1 exhibit strong complex
problems solving abilities by generating long chain-of-thought (CoT) reasoning
steps. It is challenging to directly train small language models (SLMs) to
emerge long CoT. Thus, distillation becomes a practical method to enable SLMs
for such reasoning ability. However, the long CoT often contains a lot of
redundant contents (e.g., overthinking steps) which may make SLMs hard to learn
considering their relatively poor capacity and generalization. To address this
issue, we propose a simple-yet-effective method to prune unnecessary steps in
long CoT, and then employ an on-policy method for the SLM itself to curate
valid and useful long CoT training data. In this way, SLMs can effectively
learn efficient long CoT reasoning and preserve competitive performance at the
same time. Experimental results across a series of mathematical reasoning
benchmarks demonstrate the effectiveness of the proposed method in distilling
long CoT reasoning ability into SLMs which maintains the competitive
performance but significantly reduces generating redundant reasoning steps.

</details>


### [23] [BRIT: Bidirectional Retrieval over Unified Image-Text Graph](https://arxiv.org/abs/2505.18450)
*Ainulla Khan,Yamada Moyuru,Srinidhi Akella*

Main category: cs.CL

TL;DR: BRIT提出了一种新型多模态RAG框架，通过构建多模态图并检索查询相关子图，有效处理跨模态多跳问题。


<details>
  <summary>Details</summary>
Motivation: 当前RAG技术主要针对文本查询，多模态文档（含文本和图像）的检索生成尚未充分探索，尤其在微调无效时。

Method: BRIT将文档中的文本-图像关联统一为多模态图，检索时提取查询相关子图，并通过双向路径遍历获取跨模态内容。

Result: 实验表明BRIT在专门设计的MM-RAG测试集上表现优越，能有效处理多模态文档的跨模态问题。

Conclusion: BRIT为多模态RAG提供了创新解决方案，显著提升了复杂跨模态问题的回答能力。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising technique to
enhance the quality and relevance of responses generated by large language
models. While recent advancements have mainly focused on improving RAG for
text-based queries, RAG on multi-modal documents containing both texts and
images has not been fully explored. Especially when fine-tuning does not work.
This paper proposes BRIT, a novel multi-modal RAG framework that effectively
unifies various text-image connections in the document into a multi-modal graph
and retrieves the texts and images as a query-specific sub-graph. By traversing
both image-to-text and text-to-image paths in the graph, BRIT retrieve not only
directly query-relevant images and texts but also further relevant contents to
answering complex cross-modal multi-hop questions. To evaluate the
effectiveness of BRIT, we introduce MM-RAG test set specifically designed for
multi-modal question answering tasks that require to understand the text-image
relations. Our comprehensive experiments demonstrate the superiority of BRIT,
highlighting its ability to handle cross-modal questions on the multi-modal
documents.

</details>


### [24] [MedScore: Factuality Evaluation of Free-Form Medical Answers](https://arxiv.org/abs/2505.18452)
*Heyuan Huang,Alexandra DeLucia,Vijay Murari Tiyyala,Mark Dredze*

Main category: cs.CL

TL;DR: 本文提出MedScore方法，用于将医学答案分解为条件感知的有效事实，显著提升事实性评估的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的事实性评估系统在医学领域表现不佳，因为它们主要针对客观、实体中心、公式化的文本，而医学答案具有条件依赖、对话性、假设性、句子结构多样和主观性等特点，导致分解为有效事实具有挑战性。

Method: 提出MedScore方法，通过条件感知的方式将医学答案分解为有效事实，减少幻觉和模糊引用，并保留事实的条件依赖性。

Result: MedScore方法提取的有效事实数量是现有方法的三倍，事实性评分显著受分解方法、验证语料库和骨干LLM的影响。

Conclusion: 定制化每个步骤对于可靠的事实性评估至关重要，MedScore方法在医学领域表现出色。

Abstract: While Large Language Models (LLMs) can generate fluent and convincing
responses, they are not necessarily correct. This is especially apparent in the
popular decompose-then-verify factuality evaluation pipeline, where LLMs
evaluate generations by decomposing the generations into individual, valid
claims. Factuality evaluation is especially important for medical answers,
since incorrect medical information could seriously harm the patient. However,
existing factuality systems are a poor match for the medical domain, as they
are typically only evaluated on objective, entity-centric, formulaic texts such
as biographies and historical topics. This differs from condition-dependent,
conversational, hypothetical, sentence-structure diverse, and subjective
medical answers, which makes decomposition into valid facts challenging. We
propose MedScore, a new approach to decomposing medical answers into
condition-aware valid facts. Our method extracts up to three times more valid
facts than existing methods, reducing hallucination and vague references, and
retaining condition-dependency in facts. The resulting factuality score
significantly varies by decomposition method, verification corpus, and used
backbone LLM, highlighting the importance of customizing each step for reliable
factuality evaluation.

</details>


### [25] [Hybrid Latent Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.18454)
*Zhenrui Yue,Bowen Jin,Huimin Zeng,Honglei Zhuang,Zhen Qin,Jinsung Yoon,Lanyu Shang,Jiawei Han,Dong Wang*

Main category: cs.CL

TL;DR: 该论文提出了HRPO方法，通过强化学习结合离散和连续表示优化大语言模型的潜在推理能力，无需依赖思维链轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有潜在推理方法与大语言模型不兼容，且依赖思维链训练，未能充分利用模型固有推理模式。

Method: 提出HRPO方法：1) 用可学习门控机制整合隐藏状态与采样标记；2) 渐进式引入隐藏特征训练。

Result: HRPO在知识和推理任务上优于现有方法，且模型保持可解释性，展现跨语言模式等有趣行为。

Conclusion: 基于强化学习的混合潜在推理方法展现出潜力，为未来研究提供了新方向。

Abstract: Recent advances in large language models (LLMs) have introduced latent
reasoning as a promising alternative to autoregressive reasoning. By performing
internal computation with hidden states from previous steps, latent reasoning
benefit from more informative features rather than sampling a discrete
chain-of-thought (CoT) path. Yet latent reasoning approaches are often
incompatible with LLMs, as their continuous paradigm conflicts with the
discrete nature of autoregressive generation. Moreover, these methods rely on
CoT traces for training and thus fail to exploit the inherent reasoning
patterns of LLMs. In this work, we explore latent reasoning by leveraging the
intrinsic capabilities of LLMs via reinforcement learning (RL). To this end, we
introduce hybrid reasoning policy optimization (HRPO), an RL-based hybrid
latent reasoning approach that (1) integrates prior hidden states into sampled
tokens with a learnable gating mechanism, and (2) initializes training with
predominantly token embeddings while progressively incorporating more hidden
features. This design maintains LLMs' generative capabilities and incentivizes
hybrid reasoning using both discrete and continuous representations. In
addition, the hybrid HRPO introduces stochasticity into latent reasoning via
token sampling, thereby enabling RL-based optimization without requiring CoT
trajectories. Extensive evaluations across diverse benchmarks show that HRPO
outperforms prior methods in both knowledge- and reasoning-intensive tasks.
Furthermore, HRPO-trained LLMs remain interpretable and exhibit intriguing
behaviors like cross-lingual patterns and shorter completion lengths,
highlighting the potential of our RL-based approach and offer insights for
future work in latent reasoning.

</details>


### [26] [Anchored Diffusion Language Model](https://arxiv.org/abs/2505.18456)
*Litu Rout,Constantine Caramanis,Sanjay Shakkottai*

Main category: cs.CL

TL;DR: 论文提出了一种名为ADLM的新方法，通过两阶段框架改进扩散语言模型，显著提升文本生成质量和似然建模，首次在人类似文本生成上超越自回归模型。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型（DLMs）在并行生成和双向上下文方面具有潜力，但在似然建模和生成文本质量上表现不如自回归模型（AR）。研究发现，性能差距源于重要标记（如关键词或低频词）在扩散过程中过早被掩盖，导致上下文信息不足。

Method: 提出锚定扩散语言模型（ADLM），采用两阶段框架：首先通过锚定网络预测重要标记的分布，然后基于锚定预测生成缺失标记的似然。

Result: ADLM在LM1B和OpenWebText上显著提升测试困惑度，比现有DLMs提升高达25.4%，缩小了与AR基线的差距。在七个基准测试中实现零样本泛化的最先进性能，并在MAUVE评分上首次超越AR模型。

Conclusion: ADLM通过锚定机制改进了扩散语言模型的性能，不仅在文本生成质量上超越AR模型，还提升了数学和逻辑任务中的推理能力。理论分析表明锚定改善了样本复杂度和似然建模。

Abstract: Diffusion Language Models (DLMs) promise parallel generation and
bidirectional context, yet they underperform autoregressive (AR) models in both
likelihood modeling and generated text quality. We identify that this
performance gap arises when important tokens (e.g., key words or low-frequency
words that anchor a sentence) are masked early in the forward process, limiting
contextual information for accurate reconstruction. To address this, we
introduce the Anchored Diffusion Language Model (ADLM), a novel two-stage
framework that first predicts distributions over important tokens via an anchor
network, and then predicts the likelihoods of missing tokens conditioned on the
anchored predictions. ADLM significantly improves test perplexity on LM1B and
OpenWebText, achieving up to 25.4% gains over prior DLMs, and narrows the gap
with strong AR baselines. It also achieves state-of-the-art performance in
zero-shot generalization across seven benchmarks and surpasses AR models in
MAUVE score, which marks the first time a DLM generates better human-like text
than an AR model. Theoretically, we derive an Anchored Negative Evidence Lower
Bound (ANELBO) objective and show that anchoring improves sample complexity and
likelihood modeling. Beyond diffusion, anchoring boosts performance in AR
models and enhances reasoning in math and logic tasks, outperforming existing
chain-of-thought approaches

</details>


### [27] [Measuring South Asian Biases in Large Language Models](https://arxiv.org/abs/2505.18466)
*Mamnuya Rinki,Chahat Raj,Anjishnu Mukherjee,Ziwei Zhu*

Main category: cs.CL

TL;DR: 该研究填补了大型语言模型（LLMs）评估中忽视交叉性和文化特定偏见的空白，特别是在南亚等代表性不足的多语言地区。通过分析10种印度-雅利安和德拉威语言，构建了一个文化基础的偏见词典，并评估了两种自我去偏见策略的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前对大型语言模型的评估往往忽略了交叉性和文化特定偏见，尤其是在南亚等多语言地区。这些偏见受到如面纱制度和父权制等文化因素的影响，但在生成任务中尚未得到充分研究。

Method: 研究构建了一个文化基础的偏见词典，涵盖性别、宗教、婚姻状况和子女数量等交叉维度。使用该词典量化了开放式生成任务（如讲故事、爱好和待办事项列表）中的偏见，并评估了两种自我去偏见策略（简单和复杂提示）的有效性。

Result: 研究发现，文化特定偏见在开放式生成任务中表现微妙且未被充分研究。通过新构建的偏见词典和评估框架，量化了这些偏见，并验证了自我去偏见策略在印度-雅利安和德拉威语言中的有效性。

Conclusion: 该研究通过引入新颖的偏见词典和评估框架，为文化偏见提供了细致的分析，扩展了超越欧洲中心或小规模多语言环境的研究视野。

Abstract: Evaluations of Large Language Models (LLMs) often overlook intersectional and
culturally specific biases, particularly in underrepresented multilingual
regions like South Asia. This work addresses these gaps by conducting a
multilingual and intersectional analysis of LLM outputs across 10 Indo-Aryan
and Dravidian languages, identifying how cultural stigmas influenced by purdah
and patriarchy are reinforced in generative tasks. We construct a culturally
grounded bias lexicon capturing previously unexplored intersectional dimensions
including gender, religion, marital status, and number of children. We use our
lexicon to quantify intersectional bias and the effectiveness of self-debiasing
in open-ended generations (e.g., storytelling, hobbies, and to-do lists), where
bias manifests subtly and remains largely unexamined in multilingual contexts.
Finally, we evaluate two self-debiasing strategies (simple and complex prompts)
to measure their effectiveness in reducing culturally specific bias in
Indo-Aryan and Dravidian languages. Our approach offers a nuanced lens into
cultural bias by introducing a novel bias lexicon and evaluation framework that
extends beyond Eurocentric or small-scale multilingual settings.

</details>


### [28] [Investigating AI Rater Effects of Large Language Models: GPT, Claude, Gemini, and DeepSeek](https://arxiv.org/abs/2505.18486)
*Hong Jiao,Dan Song,Won-Chan Lee*

Main category: cs.CL

TL;DR: 研究比较了10种大语言模型在写作评分中的表现，发现ChatGPT 4o、Gemini 1.5 Pro和Claude 3.5 Sonnet评分准确且稳定。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型在自动评分中的可靠性，为实际应用提供实证依据。

Method: 比较10种LLM与人类专家对写作任务的评分，使用多种统计方法评估准确性、一致性和评分者效应。

Result: ChatGPT 4o、Gemini 1.5 Pro和Claude 3.5 Sonnet表现最佳，评分准确且评分者效应低。

Conclusion: 支持使用特定LLM进行自动评分，因其高准确性和低评分者效应。

Abstract: Large language models (LLMs) have been widely explored for automated scoring
in low-stakes assessment to facilitate learning and instruction. Empirical
evidence related to which LLM produces the most reliable scores and induces
least rater effects needs to be collected before the use of LLMs for automated
scoring in practice. This study compared ten LLMs (ChatGPT 3.5, ChatGPT 4,
ChatGPT 4o, OpenAI o1, Claude 3.5 Sonnet, Gemini 1.5, Gemini 1.5 Pro, Gemini
2.0, as well as DeepSeek V3, and DeepSeek R1) with human expert raters in
scoring two types of writing tasks. The accuracy of the holistic and analytic
scores from LLMs compared with human raters was evaluated in terms of Quadratic
Weighted Kappa. Intra-rater consistency across prompts was compared in terms of
Cronbach Alpha. Rater effects of LLMs were evaluated and compared with human
raters using the Many-Facet Rasch model. The results in general supported the
use of ChatGPT 4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet with high scoring
accuracy, better rater reliability, and less rater effects.

</details>


### [29] [The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic Competence in Large Language Models](https://arxiv.org/abs/2505.18497)
*Kefan Yu,Qingcheng Zeng,Weihao Xuan,Wanxin Li,Jingyi Wu,Rob Voigt*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型在不同训练阶段如何获得语用能力，并引入ALTPRAG数据集进行评估。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在社交智能任务中展现出语用理解能力，但其在训练过程中如何获得这种能力尚不明确。

Method: 引入ALTPRAG数据集，评估22个不同训练阶段的模型在语用推理和对比推理上的表现。

Result: 基础模型对语用线索敏感，模型和数据规模提升带来持续改进，SFT和RLHF进一步增强了认知语用推理能力。

Conclusion: 语用能力是模型训练中涌现的复合特性，研究为模型与人类交际规范对齐提供了新见解。

Abstract: Current large language models (LLMs) have demonstrated emerging capabilities
in social intelligence tasks, including implicature resolution (Sravanthi et
al. (2024)) and theory-of-mind reasoning (Shapira et al. (2024)), both of which
require substantial pragmatic understanding. However, how LLMs acquire this
competence throughout the training process remains poorly understood. In this
work, we introduce ALTPRAG, a dataset grounded in the pragmatic concept of
alternatives, designed to evaluate whether LLMs at different training stages
can accurately infer nuanced speaker intentions. Each instance pairs two
contextually appropriate but pragmatically distinct continuations, enabling
fine-grained assessment of both pragmatic interpretation and contrastive
reasoning. We systematically evaluate 22 LLMs across key training stages:
pre-training, supervised fine-tuning (SFT), and preference optimization, to
examine the development of pragmatic competence. Our results show that even
base models exhibit notable sensitivity to pragmatic cues, which improves
consistently with increases in model and data scale. Additionally, SFT and RLHF
contribute further gains, particularly in cognitive-pragmatic reasoning. These
findings highlight pragmatic competence as an emergent and compositional
property of LLM training and offer new insights for aligning models with human
communicative norms.

</details>


### [30] [How Does Sequence Modeling Architecture Influence Base Capabilities of Pre-trained Language Models? Exploring Key Architecture Design Principles to Avoid Base Capabilities Degradation](https://arxiv.org/abs/2505.18522)
*Xin Lu,Yanyan Zhao,Si Wei,Shijin Wang,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: 本文探讨了序列建模架构对预训练语言模型基础能力的影响，提出了有限领域预训练设置以揭示架构间的能力差异，并总结出关键设计原则：序列建模架构需具备全序列任意选择能力以避免基础能力退化。


<details>
  <summary>Details</summary>
Motivation: 现有架构设计工作普遍采用的混合领域预训练设置未能充分揭示不同架构在基础能力上的差异，因此需要一种新的方法来评估和比较不同序列建模架构的基础能力。

Method: 提出有限领域预训练设置并进行分布外测试，分析状态序列建模架构的基础能力，通过架构组件分析总结关键设计原则，并用极简的Top-1元素选择架构进行实证验证。

Result: 实验结果表明，有限领域预训练设置能早期发现架构间基础能力的显著差异，状态序列建模架构相比Transformer表现出基础能力退化，Top-1选择架构验证了设计原则的有效性。

Conclusion: 序列建模架构需具备全序列任意选择能力以避免基础能力退化，这一设计原则为未来架构改进和新设计提供了有价值的参考。

Abstract: Pre-trained language models represented by the Transformer have been proven
to possess strong base capabilities, and the representative self-attention
mechanism in the Transformer has become a classic in sequence modeling
architectures. Different from the work of proposing sequence modeling
architecture to improve the efficiency of attention mechanism, this work
focuses on the impact of sequence modeling architectures on base capabilities.
Specifically, our concern is: How exactly do sequence modeling architectures
affect the base capabilities of pre-trained language models? In this work, we
first point out that the mixed domain pre-training setting commonly adopted in
existing architecture design works fails to adequately reveal the differences
in base capabilities among various architectures. To address this, we propose a
limited domain pre-training setting with out-of-distribution testing, which
successfully uncovers significant differences in base capabilities among
architectures at an early stage. Next, we analyze the base capabilities of
stateful sequence modeling architectures, and find that they exhibit
significant degradation in base capabilities compared to the Transformer. Then,
through a series of architecture component analysis, we summarize a key
architecture design principle: A sequence modeling architecture need possess
full-sequence arbitrary selection capability to avoid degradation in base
capabilities. Finally, we empirically validate this principle using an
extremely simple Top-1 element selection architecture and further generalize it
to a more practical Top-1 chunk selection architecture. Experimental results
demonstrate our proposed sequence modeling architecture design principle and
suggest that our work can serve as a valuable reference for future architecture
improvements and novel designs.

</details>


### [31] [metaTextGrad: Automatically optimizing language model optimizers](https://arxiv.org/abs/2505.18524)
*Guowei Xu,Mert Yuksekgonul,Carlos Guestrin,James Zou*

Main category: cs.CL

TL;DR: 提出metaTextGrad方法，通过元优化器提升现有LLM优化器的任务适配性，平均性能提升6%。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的优化器由人工设计，缺乏针对性优化且通用性过强，难以适配特定任务需求。

Method: 结合元提示优化器和元结构优化器，对现有优化器进行二次优化和任务对齐。

Result: 在多个基准测试中平均绝对性能提升达6%，超越现有最佳基线。

Conclusion: metaTextGrad能有效提升优化器的任务适配性，为AI系统优化提供新思路。

Abstract: Large language models (LLMs) are increasingly used in learning algorithms,
evaluations, and optimization tasks. Recent studies have shown that using
LLM-based optimizers to automatically optimize model prompts, demonstrations,
predictions themselves, or other components can significantly enhance the
performance of AI systems, as demonstrated by frameworks such as DSPy and
TextGrad. However, optimizers built on language models themselves are usually
designed by humans with manual design choices; optimizers themselves are not
optimized. Moreover, these optimizers are general purpose by design, to be
useful to a broad audience, and are not tailored for specific tasks. To address
these challenges, we propose metaTextGrad, which focuses on designing a
meta-optimizer to further enhance existing optimizers and align them to be good
optimizers for a given task. Our approach consists of two key components: a
meta prompt optimizer and a meta structure optimizer. The combination of these
two significantly improves performance across multiple benchmarks, achieving an
average absolute performance improvement of up to 6% compared to the best
baseline.

</details>


### [32] [Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models](https://arxiv.org/abs/2505.18536)
*Haoyuan Sun,Jiaqi Wu,Bo Xia,Yifu Luo,Yifei Zhao,Kai Qin,Xufei Lv,Tiantian Zhang,Yongzhe Chang,Xueqian Wang*

Main category: cs.CL

TL;DR: 该立场论文探讨了强化微调（RFT）如何提升多模态大语言模型（MLLMs）的推理能力，并提出了未来研究的五个方向。


<details>
  <summary>Details</summary>
Motivation: 在追求通用人工智能（AGI）的关键时刻，强化微调（RFT）在提升大语言模型（LLMs）推理能力方面显示出巨大潜力，尤其是在多模态大语言模型（MLLMs）中的应用引起了广泛关注。

Method: 论文首先介绍了该领域的基础背景知识，然后详细总结了RFT在提升MLLMs推理能力方面的五个关键改进点，包括多样化的模态、任务和领域，更好的训练算法，丰富的基准测试和蓬勃发展的工程框架。

Result: 论文提出了五个未来研究的有前景方向，旨在为AGI发展提供有价值的见解。

Conclusion: 该立场论文为AGI发展关键阶段提供了重要见解，并总结了RFT在MLLMs推理能力提升方面的现有工作。

Abstract: Standing in 2025, at a critical juncture in the pursuit of Artificial General
Intelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated
significant potential in enhancing the reasoning capability of large language
models (LLMs) and has led to the development of cutting-edge AI models such as
OpenAI-o1 and DeepSeek-R1. Moreover, the efficient application of RFT to
enhance the reasoning capability of multimodal large language models (MLLMs)
has attracted widespread attention from the community. In this position paper,
we argue that reinforcement fine-tuning powers the reasoning capability of
multimodal large language models. To begin with, we provide a detailed
introduction to the fundamental background knowledge that researchers
interested in this field should be familiar with. Furthermore, we meticulously
summarize the improvements of RFT in powering reasoning capability of MLLMs
into five key points: diverse modalities, diverse tasks and domains, better
training algorithms, abundant benchmarks and thriving engineering frameworks.
Finally, we propose five promising directions for future research that the
community might consider. We hope that this position paper will provide
valuable insights to the community at this pivotal stage in the advancement
toward AGI. Summary of works done on RFT for MLLMs is available at
https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs.

</details>


### [33] [Business as \textit{Rule}sual: A Benchmark and Framework for Business Rule Flow Modeling with LLMs](https://arxiv.org/abs/2505.18542)
*Chen Yang,Ruping Xu,Ruizhe Li,Bin Cao,Jing Fan*

Main category: cs.CL

TL;DR: 该论文提出了一个中文商业规则数据集BPRF和基于LLM的框架ExIde，用于自动提取商业规则及其依赖关系。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注从指导性文本中提取流程，而商业文档中的规则流尚未充分探索。

Method: 构建包含50份商业文档的标注数据集BPRF，并提出基于LLM的框架ExIde进行规则提取和依赖关系识别。

Result: ExIde在12个SOTA LLM上验证有效，能成功提取结构化规则并分析其依赖关系。

Conclusion: 该研究为自动化、可解释的商业流程自动化提供了新途径。

Abstract: Process mining aims to discover, monitor and optimize the actual behaviors of
real processes. While prior work has mainly focused on extracting procedural
action flows from instructional texts, rule flows embedded in business
documents remain underexplored. To this end, we introduce a novel annotated
Chinese dataset, \textbf{BPRF}, which contains 50 business process documents
with 326 explicitly labeled business rules across multiple domains. Each rule
is represented as a <Condition, Action> pair, and we annotate logical
dependencies between rules (sequential, conditional, or parallel). We also
propose \textbf{ExIde}, a framework for automatic business rule extraction and
dependency relationship identification using large language models (LLMs). We
evaluate ExIde using 12 state-of-the-art (SOTA) LLMs on the BPRF dataset,
benchmarking performance on both rule extraction and dependency classification
tasks of current LLMs. Our results demonstrate the effectiveness of ExIde in
extracting structured business rules and analyzing their interdependencies for
current SOTA LLMs, paving the way for more automated and interpretable business
process automation.

</details>


### [34] [Composable Cross-prompt Essay Scoring by Merging Models](https://arxiv.org/abs/2505.18548)
*Sanwoo Lee,Kun Liang,Yunfang Wu*

Main category: cs.CL

TL;DR: 论文提出了一种无需源数据的自适应方法，通过选择性合并源模型的参数来优化跨提示自动作文评分，避免了隐私问题并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的跨提示自动作文评分方法通常需要同时访问所有源提示的数据，这不仅效率低下，还可能引发隐私问题。作者发现联合训练所有源提示并非最优，因此提出了一种无需源数据的自适应方法。

Method: 该方法通过线性组合任务向量（微调后的参数更新）来模拟联合训练，并提出了Prior-encoded Information Maximization (PIM)这一无监督目标，通过贝叶斯优化来优化组合系数。

Result: 实验结果表明，该方法在数据集内和跨数据集的自适应中均优于联合训练所有源提示的方法，具有更强的鲁棒性，且在分布严重偏移的情况下表现优异，同时保持了计算效率。

Conclusion: 论文提出的方法在跨提示自动作文评分中表现出色，不仅解决了隐私问题，还提升了模型的性能和鲁棒性，尤其在分布偏移的情况下表现突出。

Abstract: Recent advances in cross-prompt automated essay scoring (AES) typically train
models jointly on all source prompts, often requiring additional access to
unlabeled target prompt essays simultaneously. However, using all sources is
suboptimal in our pilot study, and re-accessing source datasets during
adaptation raises privacy concerns. We propose a source-free adaptation
approach that selectively merges individually trained source models' parameters
instead of datasets. In particular, we simulate joint training through linear
combinations of task vectors -- the parameter updates from fine-tuning. To
optimize the combination's coefficients, we propose Prior-encoded Information
Maximization (PIM), an unsupervised objective which promotes the model's score
discriminability regularized by priors pre-computed from the sources. We employ
Bayesian optimization as an efficient optimizer of PIM. Experimental results
with LLMs on in-dataset and cross-dataset adaptation show that our method (1)
consistently outperforms training jointly on all sources, (2) maintains
superior robustness compared to other merging methods, (3) excels under severe
distribution shifts where recent leading cross-prompt methods struggle, all
while retaining computational efficiency.

</details>


### [35] [MSA at BEA 2025 Shared Task: Disagreement-Aware Instruction Tuning for Multi-Dimensional Evaluation of LLMs as Math Tutors](https://arxiv.org/abs/2505.18549)
*Baraa Hikal,Mohamed Basem,Islam Oshallah,Ali Hamdi*

Main category: cs.CL

TL;DR: MSA-MathEval提出了一种统一训练流程和分歧感知集成推理策略，用于评估AI导师在四个教学维度的表现，并在BEA 2025共享任务中取得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在开发一个能够全面评估AI导师在教学中的表现的系统，特别是在错误识别、错误定位、提供指导和可操作性四个维度上。

Method: 采用统一训练流程对单个指令调优语言模型进行微调，无需特定任务架构更改，并引入分歧感知集成推理策略以提高预测可靠性。

Result: 系统在所有维度上表现优异，其中在提供指导维度排名第一，可操作性排名第三，错误识别和错误定位均排名第四。

Conclusion: 研究表明，可扩展的指令调优和分歧驱动建模能有效实现AI导师的多维度稳健评估。

Abstract: We present MSA-MathEval, our submission to the BEA 2025 Shared Task on
evaluating AI tutor responses across four instructional dimensions: Mistake
Identification, Mistake Location, Providing Guidance, and Actionability. Our
approach uses a unified training pipeline to fine-tune a single
instruction-tuned language model across all tracks, without any task-specific
architectural changes. To improve prediction reliability, we introduce a
disagreement-aware ensemble inference strategy that enhances coverage of
minority labels. Our system achieves strong performance across all tracks,
ranking 1st in Providing Guidance, 3rd in Actionability, and 4th in both
Mistake Identification and Mistake Location. These results demonstrate the
effectiveness of scalable instruction tuning and disagreement-driven modeling
for robust, multi-dimensional evaluation of LLMs as educational tutors.

</details>


### [36] [Unraveling Misinformation Propagation in LLM Reasoning](https://arxiv.org/abs/2505.18555)
*Yiyang Feng,Yichen Wang,Shaobo Cui,Boi Faltings,Mina Lee,Jiawei Zhou*

Main category: cs.CL

TL;DR: 论文探讨了错误信息如何影响大语言模型的数学推理过程，并研究了纠正错误的有效方法。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在用户输入错误信息时的推理表现，以及如何减少错误传播。

Method: 通过分析错误信息对中间推理步骤和最终答案的影响，并测试模型在明确指令下纠正错误的能力。

Result: 即使有明确指令，模型纠正错误的成功率低于50%，早期纠正能有效减少错误传播，微调显著提升推理准确性。

Conclusion: 提供了一种减少错误传播的实用方法，强调早期纠正和微调的重要性。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
reasoning, positioning them as promising tools for supporting human
problem-solving. However, what happens when their performance is affected by
misinformation, i.e., incorrect inputs introduced by users due to oversights or
gaps in knowledge? Such misinformation is prevalent in real-world interactions
with LLMs, yet how it propagates within LLMs' reasoning process remains
underexplored. Focusing on mathematical reasoning, we present a comprehensive
analysis of how misinformation affects intermediate reasoning steps and final
answers. We also examine how effectively LLMs can correct misinformation when
explicitly instructed to do so. Even with explicit instructions, LLMs succeed
less than half the time in rectifying misinformation, despite possessing
correct internal knowledge, leading to significant accuracy drops (10.02% -
72.20%). Further analysis shows that applying factual corrections early in the
reasoning process most effectively reduces misinformation propagation, and
fine-tuning on synthesized data with early-stage corrections significantly
improves reasoning factuality. Our work offers a practical approach to
mitigating misinformation propagation.

</details>


### [37] [Exploring the Vulnerability of the Content Moderation Guardrail in Large Language Models via Intent Manipulation](https://arxiv.org/abs/2505.18556)
*Jun Zhuang,Haibo Jin,Ye Zhang,Zhengjian Kang,Wenbin Zhang,Gaby G. Dagher,Haohan Wang*

Main category: cs.CL

TL;DR: 该论文研究了意图检测在大型语言模型（LLMs）安全防护中的脆弱性，提出了一种两阶段的意图提示优化框架IntentPrompt，能够有效绕过现有防护机制，攻击成功率高达97.12%。


<details>
  <summary>Details</summary>
Motivation: 虽然意图检测已成功应用于增强LLMs的内容审核防护，但其在恶意操纵下的鲁棒性尚未充分探索。论文旨在揭示意图感知防护的漏洞，并探索如何利用这些漏洞进行红队测试。

Method: 提出IntentPrompt框架：1) 将有害查询转换为结构化大纲；2) 通过反馈循环迭代优化提示，将其重构为陈述式叙述，以提升越狱成功率。

Result: 在四个公共基准测试和多种黑盒LLMs上，IntentPrompt（尤其是FSTR+SPIN变体）对基于CoT和IA的防御成功率分别达到96.54%和97.12%，显著优于现有越狱方法。

Conclusion: 研究揭示了LLMs安全机制的关键弱点，表明意图操纵对内容审核防护构成日益严峻的挑战，需开发更鲁棒的防御策略。

Abstract: Intent detection, a core component of natural language understanding, has
considerably evolved as a crucial mechanism in safeguarding large language
models (LLMs). While prior work has applied intent detection to enhance LLMs'
moderation guardrails, showing a significant success against content-level
jailbreaks, the robustness of these intent-aware guardrails under malicious
manipulations remains under-explored. In this work, we investigate the
vulnerability of intent-aware guardrails and demonstrate that LLMs exhibit
implicit intent detection capabilities. We propose a two-stage intent-based
prompt-refinement framework, IntentPrompt, that first transforms harmful
inquiries into structured outlines and further reframes them into
declarative-style narratives by iteratively optimizing prompts via feedback
loops to enhance jailbreak success for red-teaming purposes. Extensive
experiments across four public benchmarks and various black-box LLMs indicate
that our framework consistently outperforms several cutting-edge jailbreak
methods and evades even advanced Intent Analysis (IA) and Chain-of-Thought
(CoT)-based defenses. Specifically, our "FSTR+SPIN" variant achieves attack
success rates ranging from 88.25% to 96.54% against CoT-based defenses on the
o1 model, and from 86.75% to 97.12% on the GPT-4o model under IA-based
defenses. These findings highlight a critical weakness in LLMs' safety
mechanisms and suggest that intent manipulation poses a growing challenge to
content moderation guardrails.

</details>


### [38] [TAG-INSTRUCT: Controlled Instruction Complexity Enhancement through Structure-based Augmentation](https://arxiv.org/abs/2505.18557)
*He Zhu,Zhiwen Ruan,Junyou Su,Xingwei He,Wenjia Zhang,Yun Chen,Guanhua Chen*

Main category: cs.CL

TL;DR: TAG-INSTRUCT框架通过结构化语义压缩和难度增强提升指令复杂度，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效控制指令复杂度，高质量指令数据对大型语言模型发展至关重要。

Method: 将指令压缩至紧凑标签空间，通过RL引导的标签扩展系统性增强复杂度。

Result: 实验表明TAG-INSTRUCT在指令复杂度增强上优于现有方法，标签空间操作提供更好可控性和稳定性。

Conclusion: TAG-INSTRUCT为指令复杂度控制提供了更优解决方案。

Abstract: High-quality instruction data is crucial for developing large language models
(LLMs), yet existing approaches struggle to effectively control instruction
complexity. We present TAG-INSTRUCT, a novel framework that enhances
instruction complexity through structured semantic compression and controlled
difficulty augmentation. Unlike previous prompt-based methods operating on raw
text, TAG-INSTRUCT compresses instructions into a compact tag space and
systematically enhances complexity through RL-guided tag expansion. Through
extensive experiments, we show that TAG-INSTRUCT outperforms existing
instruction complexity augmentation approaches. Our analysis reveals that
operating in tag space provides superior controllability and stability across
different instruction synthesis frameworks.

</details>


### [39] [From Word to World: Evaluate and Mitigate Culture Bias via Word Association Test](https://arxiv.org/abs/2505.18562)
*Xunlian Dai,Li Zhou,Benyou Wang,Haizhou Li*

Main category: cs.CL

TL;DR: 论文提出CultureSteer方法，通过文化感知机制改进大语言模型（LLMs）在跨文化认知中的对齐能力，减少西方文化偏见，提升文化多样性表现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在词汇关联层面存在显著的西方文化偏见，尤其是美国文化模式，这限制了模型在跨文化环境中的适用性和包容性。

Method: 提出CultureSteer方法，结合文化感知的引导机制，将语义表征导向特定文化空间，以增强模型的文化多样性表现。

Result: 实验表明，CultureSteer显著提升了模型在跨文化对齐上的表现，优于基于提示的方法，并在文化敏感的下游任务中验证了其有效性。

Conclusion: 该研究为增强大语言模型的文化意识提供了新的方法论范式，推动了更具包容性的语言技术的发展。

Abstract: The human-centered word association test (WAT) serves as a cognitive proxy,
revealing sociocultural variations through lexical-semantic patterns. We extend
this test into an LLM-adaptive, free-relation task to assess the alignment of
large language models (LLMs) with cross-cultural cognition. To mitigate the
culture preference, we propose CultureSteer, an innovative approach that
integrates a culture-aware steering mechanism to guide semantic representations
toward culturally specific spaces. Experiments show that current LLMs exhibit
significant bias toward Western cultural (notably in American) schemas at the
word association level. In contrast, our model substantially improves
cross-cultural alignment, surpassing prompt-based methods in capturing diverse
semantic associations. Further validation on culture-sensitive downstream tasks
confirms its efficacy in fostering cognitive alignment across cultures. This
work contributes a novel methodological paradigm for enhancing cultural
awareness in LLMs, advancing the development of more inclusive language
technologies.

</details>


### [40] [Removal of Hallucination on Hallucination: Debate-Augmented RAG](https://arxiv.org/abs/2505.18581)
*Wentao Hu,Wengyu Zhang,Yiyang Jiang,Chen Jason Zhang,Xiaoyong Wei,Qing Li*

Main category: cs.CL

TL;DR: 论文提出DRAG框架，通过多智能体辩论机制提升RAG的事实准确性，减少幻觉问题。


<details>
  <summary>Details</summary>
Motivation: RAG虽然通过整合外部知识提高了事实准确性，但错误的检索可能导致生成内容出现幻觉，即‘幻觉叠加’现象。

Method: DRAG框架在检索和生成阶段引入多智能体辩论机制，包括支持者、反对者和裁判角色，通过辩论优化检索质量和生成内容的可靠性。

Result: 实验表明，DRAG显著提高了检索可靠性，减少了RAG引发的幻觉，并大幅提升了整体事实准确性。

Conclusion: DRAG通过辩论机制有效解决了RAG的幻觉问题，为提升生成内容的可靠性提供了新思路。

Abstract: Retrieval-Augmented Generation (RAG) enhances factual accuracy by integrating
external knowledge, yet it introduces a critical issue: erroneous or biased
retrieval can mislead generation, compounding hallucinations, a phenomenon we
term Hallucination on Hallucination. To address this, we propose
Debate-Augmented RAG (DRAG), a training-free framework that integrates
Multi-Agent Debate (MAD) mechanisms into both retrieval and generation stages.
In retrieval, DRAG employs structured debates among proponents, opponents, and
judges to refine retrieval quality and ensure factual reliability. In
generation, DRAG introduces asymmetric information roles and adversarial
debates, enhancing reasoning robustness and mitigating factual inconsistencies.
Evaluations across multiple tasks demonstrate that DRAG improves retrieval
reliability, reduces RAG-induced hallucinations, and significantly enhances
overall factual accuracy. Our code is available at
https://github.com/Huenao/Debate-Augmented-RAG.

</details>


### [41] [Safety Alignment via Constrained Knowledge Unlearning](https://arxiv.org/abs/2505.18588)
*Zesheng Shi,Yucheng Zhou,Jing Li*

Main category: cs.CL

TL;DR: 该论文提出了一种新的安全对齐策略CKU，通过定位和保留有用知识、遗忘有害知识，有效提升大语言模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在安全对齐方面取得进展，但仍易受越狱攻击，现有防御机制未能完全删除有害知识。

Method: CKU通过评分特定MLP层神经元，识别有用知识相关神经元子集U，并在遗忘过程中修剪U的梯度以保留有用知识。

Result: 实验表明，CKU显著提升模型安全性且不影响整体性能，在安全与实用性之间取得更好平衡。

Conclusion: CKU为安全对齐和模型知识编辑提供了新思路，并通过分析神经元知识敏感性提供了有价值的见解。

Abstract: Despite significant progress in safety alignment, large language models
(LLMs) remain susceptible to jailbreak attacks. Existing defense mechanisms
have not fully deleted harmful knowledge in LLMs, which allows such attacks to
bypass safeguards and produce harmful outputs. To address this challenge, we
propose a novel safety alignment strategy, Constrained Knowledge Unlearning
(CKU), which focuses on two primary objectives: knowledge localization and
retention, and unlearning harmful knowledge. CKU works by scoring neurons in
specific multilayer perceptron (MLP) layers to identify a subset U of neurons
associated with useful knowledge. During the unlearning process, CKU prunes the
gradients of neurons in U to preserve valuable knowledge while effectively
mitigating harmful content. Experimental results demonstrate that CKU
significantly enhances model safety without compromising overall performance,
offering a superior balance between safety and utility compared to existing
methods. Additionally, our analysis of neuron knowledge sensitivity across
various MLP layers provides valuable insights into the mechanics of safety
alignment and model knowledge editing.

</details>


### [42] [Debate-to-Detect: Reformulating Misinformation Detection as a Real-World Debate with Large Language Models](https://arxiv.org/abs/2505.18596)
*Chen Han,Wenzhen Zheng,Xijin Tang*

Main category: cs.CL

TL;DR: 论文提出Debate-to-Detect (D2D)框架，通过多智能体辩论改进虚假信息检测，超越传统静态分类方法。


<details>
  <summary>Details</summary>
Motivation: 传统虚假信息检测方法依赖静态分类，无法捕捉现实核查的复杂过程，且大型语言模型存在逻辑不一致和表面验证问题。

Method: D2D采用多智能体辩论框架，模拟事实核查流程，分五个阶段进行辩论，并引入五维评估机制。

Result: 在GPT-4o上的实验显示，D2D在虚假新闻数据集上显著优于基线方法，并能迭代优化证据。

Conclusion: D2D框架在虚假信息检测的鲁棒性和可解释性方面取得重要进展，代码将开源。

Abstract: The proliferation of misinformation in digital platforms reveals the
limitations of traditional detection methods, which mostly rely on static
classification and fail to capture the intricate process of real-world
fact-checking. Despite advancements in Large Language Models (LLMs) that
enhance automated reasoning, their application to misinformation detection
remains hindered by issues of logical inconsistency and superficial
verification. In response, we introduce Debate-to-Detect (D2D), a novel
Multi-Agent Debate (MAD) framework that reformulates misinformation detection
as a structured adversarial debate. Inspired by fact-checking workflows, D2D
assigns domain-specific profiles to each agent and orchestrates a five-stage
debate process, including Opening Statement, Rebuttal, Free Debate, Closing
Statement, and Judgment. To transcend traditional binary classification, D2D
introduces a multi-dimensional evaluation mechanism that assesses each claim
across five distinct dimensions: Factuality, Source Reliability, Reasoning
Quality, Clarity, and Ethics. Experiments with GPT-4o on two fakenews datasets
demonstrate significant improvements over baseline methods, and the case study
highlight D2D's capability to iteratively refine evidence while improving
decision transparency, representing a substantial advancement towards robust
and interpretable misinformation detection. The code will be open-sourced in a
future release.

</details>


### [43] [Flex-Judge: Think Once, Judge Anywhere](https://arxiv.org/abs/2505.18601)
*Jongwoo Ko,Sungnyun Kim,Sungwoo Cho,Se-Young Yun*

Main category: cs.CL

TL;DR: Flex-Judge是一种基于推理的多模态评估模型，通过少量文本推理数据实现跨模态评估，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统LLM评估模型依赖大量模态特定训练数据且泛化能力不足，Flex-Judge旨在通过推理数据解决这一问题。

Method: 利用结构化文本推理解释编码通用决策模式，实现跨模态（如图像、视频）评估。

Result: Flex-Judge在少量文本数据训练下，性能媲美或超越商业API和多模态评估模型，尤其在分子等稀缺数据模态中表现突出。

Conclusion: 基于推理的文本监督是传统高成本标注的有效替代方案，显著提升了多模态评估的可扩展性。

Abstract: Human-generated reward signals are critical for aligning generative models
with human preferences, guiding both training and inference-time evaluations.
While large language models (LLMs) employed as proxy evaluators, i.e.,
LLM-as-a-Judge, significantly reduce the costs associated with manual
annotations, they typically require extensive modality-specific training data
and fail to generalize well across diverse multimodal tasks. In this paper, we
propose Flex-Judge, a reasoning-guided multimodal judge model that leverages
minimal textual reasoning data to robustly generalize across multiple
modalities and evaluation formats. Our core intuition is that structured
textual reasoning explanations inherently encode generalizable decision-making
patterns, enabling an effective transfer to multimodal judgments, e.g., with
images or videos. Empirical results demonstrate that Flex-Judge, despite being
trained on significantly fewer text data, achieves competitive or superior
performance compared to state-of-the-art commercial APIs and extensively
trained multimodal evaluators. Notably, Flex-Judge presents broad impact in
modalities like molecule, where comprehensive evaluation benchmarks are scarce,
underscoring its practical value in resource-constrained domains. Our framework
highlights reasoning-based text supervision as a powerful, cost-effective
alternative to traditional annotation-intensive approaches, substantially
advancing scalable multimodal model-as-a-judge.

</details>


### [44] [RASMALAI: Resources for Adaptive Speech Modeling in Indian Languages with Accents and Intonations](https://arxiv.org/abs/2505.18609)
*Ashwin Sankar,Yoach Lacombe,Sherry Thomas,Praveen Srinivasa Varadhan,Sanchit Gandhi,Mitesh M Khapra*

Main category: cs.CL

TL;DR: RASMALAI是一个大规模语音数据集，包含23种印度语言和英语的丰富文本描述，用于推进可控和表达性文本到语音（TTS）合成。基于此数据集开发的IndicParlerTTS系统在多项评估中表现出色，为印度语言的可控多语言表达性语音合成设立了新标准。


<details>
  <summary>Details</summary>
Motivation: 为了推动印度语言的可控和表达性文本到语音合成技术的发展，研究团队创建了RASMALAI数据集，填补了该领域大规模、多语言、细粒度标注数据集的空白。

Method: 研究团队首先构建了包含13,000小时语音和2,400万文本描述标注的RASMALAI数据集，然后基于该数据集开发了首个开源的、文本描述引导的印度语言TTS系统IndicParlerTTS。

Result: 系统评估表明，IndicParlerTTS能够生成高质量语音，可靠地遵循文本描述，准确合成指定属性，并有效实现语言内和跨语言的表达特征迁移，在所有评估中均表现优异。

Conclusion: RASMALAI数据集和IndicParlerTTS系统为印度语言的可控多语言表达性语音合成设立了新标准，展现了在指定说话人、情感风格等多方面的高质量合成能力。

Abstract: We introduce RASMALAI, a large-scale speech dataset with rich text
descriptions, designed to advance controllable and expressive text-to-speech
(TTS) synthesis for 23 Indian languages and English. It comprises 13,000 hours
of speech and 24 million text-description annotations with fine-grained
attributes like speaker identity, accent, emotion, style, and background
conditions. Using RASMALAI, we develop IndicParlerTTS, the first open-source,
text-description-guided TTS for Indian languages. Systematic evaluation
demonstrates its ability to generate high-quality speech for named speakers,
reliably follow text descriptions and accurately synthesize specified
attributes. Additionally, it effectively transfers expressive characteristics
both within and across languages. IndicParlerTTS consistently achieves strong
performance across these evaluations, setting a new standard for controllable
multilingual expressive speech synthesis in Indian languages.

</details>


### [45] [PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT LLMs](https://arxiv.org/abs/2505.18610)
*Tengxuan Liu,Shiyao Li,Jiayi Yang,Tianchen Zhao,Feng Zhou,Xiaohui Song,Guohao Dai,Shengen Yan,Huazhong Yang,Yu Wang*

Main category: cs.CL

TL;DR: 论文提出PM-KVQ方法，通过渐进混合精度KV缓存量化和位置插值校准策略，解决长链思维推理大语言模型中KV缓存量化导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 长链思维推理（long-CoT）技术提升了大型语言模型的推理能力，但带来了巨大的KV缓存内存开销。现有的KV缓存量化方法在长上下文场景下直接应用会导致性能显著下降，主要原因是累积误差大和短上下文校准不准确。

Method: 提出渐进混合精度KV缓存量化（PM-KVQ）方法：1) 渐进量化策略逐步降低每个块的比特宽度，并采用块级内存分配为敏感块分配更高比特；2) 通过位置插值校准策略，利用短校准数据近似长上下文数据分布。

Result: 在7B-70B参数的长链思维推理模型上实验表明，PM-KVQ在相同内存预算下比现有最优方法提升推理性能达8%。

Conclusion: PM-KVQ有效解决了长链思维推理模型中KV缓存量化的累积误差和校准问题，显著提升了模型性能。

Abstract: Recently, significant progress has been made in developing reasoning-capable
Large Language Models (LLMs) through long Chain-of-Thought (CoT) techniques.
However, this long-CoT reasoning process imposes substantial memory overhead
due to the large Key-Value (KV) Cache memory overhead. Post-training KV Cache
quantization has emerged as a promising compression technique and has been
extensively studied in short-context scenarios. However, directly applying
existing methods to long-CoT LLMs causes significant performance degradation
due to the following two reasons: (1) Large cumulative error: Existing methods
fail to adequately leverage available memory, and they directly quantize the KV
Cache during each decoding step, leading to large cumulative quantization
error. (2) Short-context calibration: Due to Rotary Positional Embedding
(RoPE), the use of short-context data during calibration fails to account for
the distribution of less frequent channels in the Key Cache, resulting in
performance loss. We propose Progressive Mixed-Precision KV Cache Quantization
(PM-KVQ) for long-CoT LLMs to address the above issues in two folds: (1) To
reduce cumulative error, we design a progressive quantization strategy to
gradually lower the bit-width of KV Cache in each block. Then, we propose
block-wise memory allocation to assign a higher bit-width to more sensitive
transformer blocks. (2) To increase the calibration length without additional
overhead, we propose a new calibration strategy with positional interpolation
that leverages short calibration data with positional interpolation to
approximate the data distribution of long-context data. Extensive experiments
on 7B-70B long-CoT LLMs show that PM-KVQ improves reasoning benchmark
performance by up to 8% over SOTA baselines under the same memory budget. Our
code is available at https://github.com/thu-nics/PM-KVQ.

</details>


### [46] [MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song Translation](https://arxiv.org/abs/2505.18614)
*Woohyun Cho,Youngmin Kim,Sunghyun Lee,Youngjae Yu*

Main category: cs.CL

TL;DR: 该论文提出了首个多语言、多模态的动画歌曲歌词翻译基准MAVL，并开发了SylAVL-CoT模型，通过结合音视频线索和音节约束，显著提升了歌词的可唱性和上下文准确性。


<details>
  <summary>Details</summary>
Motivation: 歌词翻译不仅需要准确的语义转换，还需保持音乐节奏、音节结构和诗体风格。在动画音乐剧中，由于需要与视觉和听觉线索对齐，这一挑战更加复杂。

Method: 论文引入了多语言音视频歌词基准MAVL，并提出了SylAVL-CoT模型，该模型利用音视频线索并强制执行音节约束，以生成自然流畅的歌词。

Result: 实验结果表明，SylAVL-CoT在可唱性和上下文准确性方面显著优于基于文本的模型。

Conclusion: 多模态、多语言方法在歌词翻译中具有重要价值，能够生成更丰富、更具表现力的翻译结果。

Abstract: Lyrics translation requires both accurate semantic transfer and preservation
of musical rhythm, syllabic structure, and poetic style. In animated musicals,
the challenge intensifies due to alignment with visual and auditory cues. We
introduce Multilingual Audio-Video Lyrics Benchmark for Animated Song
Translation (MAVL), the first multilingual, multimodal benchmark for singable
lyrics translation. By integrating text, audio, and video, MAVL enables richer
and more expressive translations than text-only approaches. Building on this,
we propose Syllable-Constrained Audio-Video LLM with Chain-of-Thought
SylAVL-CoT, which leverages audio-video cues and enforces syllabic constraints
to produce natural-sounding lyrics. Experimental results demonstrate that
SylAVL-CoT significantly outperforms text-based models in singability and
contextual accuracy, emphasizing the value of multimodal, multilingual
approaches for lyrics translation.

</details>


### [47] [DDO: Dual-Decision Optimization via Multi-Agent Collaboration for LLM-Based Medical Consultation](https://arxiv.org/abs/2505.18630)
*Zhihao Jia,Mingyi Jia,Junwen Duan,Jianxin Wang*

Main category: cs.CL

TL;DR: 论文提出DDO框架，通过多智能体协作优化医疗咨询中的症状询问和疾病诊断两个子任务，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的方法未能有效捕捉医疗咨询的双重特性（症状询问的序列决策和疾病诊断的分类问题），导致效果不佳。

Method: 提出DDO框架，通过解耦和独立优化两个子任务，采用协作多智能体工作流实现双重决策优化。

Result: 在三个真实医疗咨询数据集上，DDO优于现有基于LLM的方法，并与最先进的生成方法性能相当。

Conclusion: DDO框架有效解决了医疗咨询任务中的双重挑战，验证了其在实际应用中的优越性。

Abstract: Large Language Models (LLMs) demonstrate strong generalization and reasoning
abilities, making them well-suited for complex decision-making tasks such as
medical consultation (MC). However, existing LLM-based methods often fail to
capture the dual nature of MC, which entails two distinct sub-tasks: symptom
inquiry, a sequential decision-making process, and disease diagnosis, a
classification problem. This mismatch often results in ineffective symptom
inquiry and unreliable disease diagnosis. To address this, we propose
\textbf{DDO}, a novel LLM-based framework that performs
\textbf{D}ual-\textbf{D}ecision \textbf{O}ptimization by decoupling and
independently optimizing the the two sub-tasks through a collaborative
multi-agent workflow. Experiments on three real-world MC datasets show that DDO
consistently outperforms existing LLM-based approaches and achieves competitive
performance with state-of-the-art generation-based methods, demonstrating its
effectiveness in the MC task.

</details>


### [48] [Multilingual Question Answering in Low-Resource Settings: A Dzongkha-English Benchmark for Foundation Models](https://arxiv.org/abs/2505.18638)
*Md. Tanzib Hosain,Rajan Das Gupta,Md. Kishor Morol*

Main category: cs.CL

TL;DR: 该研究发布了DZEN数据集，包含5K+不丹中学科学类双语试题，用于评估大语言模型在不丹语(Dzongkha)的表现差异，发现思维链提示对推理题有效，并开源数据集。


<details>
  <summary>Details</summary>
Motivation: 针对不丹语等低资源语言，缺乏评估大语言模型性能的基准数据集，研究者构建平行语料库以填补这一空白。

Method: 收集超5千道不丹中学科学类双语试题，测试多种大语言模型，并探索不同提示策略（如思维链）的效果。

Result: 模型在英语与不丹语表现差异显著；思维链提示对推理题有效但对事实题无效；添加英语翻译能提升不丹语回答准确率。

Conclusion: 该研究为提升大语言模型在低资源语言性能指明方向，并开源数据集促进后续研究。

Abstract: In this work, we provide DZEN, a dataset of parallel Dzongkha and English
test questions for Bhutanese middle and high school students. The over 5K
questions in our collection span a variety of scientific topics and include
factual, application, and reasoning-based questions. We use our parallel
dataset to test a number of Large Language Models (LLMs) and find a significant
performance difference between the models in English and Dzongkha. We also look
at different prompting strategies and discover that Chain-of-Thought (CoT)
prompting works well for reasoning questions but less well for factual ones. We
also find that adding English translations enhances the precision of Dzongkha
question responses. Our results point to exciting avenues for further study to
improve LLM performance in Dzongkha and, more generally, in low-resource
languages. We release the dataset at:
https://github.com/kraritt/llm_dzongkha_evaluation.

</details>


### [49] [Skip-Thinking: Chunk-wise Chain-of-Thought Distillation Enable Smaller Language Models to Reason Better and Faster](https://arxiv.org/abs/2505.18642)
*Xiao Chen,Sihang Zhou,Ke Liang,Xiaoyu Sun,Xinwang Liu*

Main category: cs.CL

TL;DR: 论文提出分块训练（CWT）和跳跃思维训练（STT）方法，解决小语言模型在推理任务中因长逻辑链导致的梯度平滑和响应慢问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法让小语言模型一次性学习长逻辑链，导致核心推理令牌梯度平滑和响应速度慢。

Method: 使用启发式搜索将逻辑链分成语义连贯的块，每次迭代只学习一个块，并跳过非推理块。

Result: CWT和STT提高了小语言模型的推理速度和准确性。

Conclusion: 分块和跳跃思维训练有效提升小语言模型的推理性能。

Abstract: Chain-of-thought (CoT) distillation allows a large language model (LLM) to
guide a small language model (SLM) in reasoning tasks. Existing methods train
the SLM to learn the long rationale in one iteration, resulting in two issues:
1) Long rationales lead to a large token-level batch size during training,
making gradients of core reasoning tokens (i.e., the token will directly affect
the correctness of subsequent reasoning) over-smoothed as they contribute a
tiny fraction of the rationale. As a result, the SLM converges to sharp minima
where it fails to grasp the reasoning logic. 2) The response is slow, as the
SLM must generate a long rationale before reaching the answer. Therefore, we
propose chunk-wise training (CWT), which uses a heuristic search to divide the
rationale into internal semantically coherent chunks and focuses SLM on
learning from only one chunk per iteration. In this way, CWT naturally isolates
non-reasoning chunks that do not involve the core reasoning token (e.g.,
summary and transitional chunks) from the SLM learning for reasoning chunks,
making the fraction of the core reasoning token increase in the corresponding
iteration. Based on CWT, skip-thinking training (STT) is proposed. STT makes
the SLM automatically skip non-reasoning medium chunks to reach the answer,
improving reasoning speed while maintaining accuracy. We validate our approach
on a variety of SLMs and multiple reasoning tasks.

</details>


### [50] [On the Emergence of Linear Analogies in Word Embeddings](https://arxiv.org/abs/2505.18651)
*Daniel J. Korchinski,Dhruva Karkada,Yasaman Bahri,Matthieu Wyart*

Main category: cs.CL

TL;DR: 本文通过理论生成模型解释了Word2Vec和GloVe词向量中线性类比结构的起源及其特性。


<details>
  <summary>Details</summary>
Motivation: 词嵌入模型（如Word2Vec和GloVe）生成的词向量展现出线性类比结构（如“国王-男人+女人≈女王”），但其理论起源尚不明确。本文旨在解释这一现象及其特性。

Method: 作者提出了一种理论生成模型，假设词由二元语义属性定义，并通过基于属性的交互推导共现概率。该模型分析了线性类比结构的涌现，并验证了其与实验观察的一致性。

Result: 模型成功复现了线性类比结构的涌现，并自然解释了其四个关键特性（如特征向量的作用、对数变换的增强效果等）。模型对噪声具有鲁棒性，并与实际语料库数据吻合。

Conclusion: 该理论模型为词嵌入中的线性类比结构提供了清晰的解释，揭示了其生成机制，并验证了其在实际数据中的适用性。

Abstract: Models such as Word2Vec and GloVe construct word embeddings based on the
co-occurrence probability $P(i,j)$ of words $i$ and $j$ in text corpora. The
resulting vectors $W_i$ not only group semantically similar words but also
exhibit a striking linear analogy structure -- for example, $W_{\text{king}} -
W_{\text{man}} + W_{\text{woman}} \approx W_{\text{queen}}$ -- whose
theoretical origin remains unclear. Previous observations indicate that this
analogy structure: (i) already emerges in the top eigenvectors of the matrix
$M(i,j) = P(i,j)/P(i)P(j)$, (ii) strengthens and then saturates as more
eigenvectors of $M (i, j)$, which controls the dimension of the embeddings, are
included, (iii) is enhanced when using $\log M(i,j)$ rather than $M(i,j)$, and
(iv) persists even when all word pairs involved in a specific analogy relation
(e.g., king-queen, man-woman) are removed from the corpus. To explain these
phenomena, we introduce a theoretical generative model in which words are
defined by binary semantic attributes, and co-occurrence probabilities are
derived from attribute-based interactions. This model analytically reproduces
the emergence of linear analogy structure and naturally accounts for properties
(i)-(iv). It can be viewed as giving fine-grained resolution into the role of
each additional embedding dimension. It is robust to various forms of noise and
agrees well with co-occurrence statistics measured on Wikipedia and the analogy
benchmark introduced by Mikolov et al.

</details>


### [51] [Climate-Eval: A Comprehensive Benchmark for NLP Tasks Related to Climate Change](https://arxiv.org/abs/2505.18653)
*Murathan Kurfalı,Shorouq Zahra,Joakim Nivre,Gabriele Messori*

Main category: cs.CL

TL;DR: Climate-Eval是一个综合基准测试，用于评估NLP模型在气候变化相关任务上的表现，包含25个任务和13个数据集，并对开源大模型进行了广泛评估。


<details>
  <summary>Details</summary>
Motivation: 为了系统评估大语言模型在气候变化领域的表现，需要建立一个标准化的评估基准，涵盖文本分类、问答和信息抽取等关键任务。

Method: 整合现有数据集并开发新的新闻分类数据集，构建包含25个任务的基准测试，对开源大模型进行零样本和少样本评估。

Result: 基准测试覆盖了气候变化讨论的多个方面，并展示了开源大模型在该领域的优势和局限性。

Conclusion: Climate-Eval为评估大语言模型在气候变化相关任务上的性能提供了标准化工具，并揭示了当前模型的潜力与不足。

Abstract: Climate-Eval is a comprehensive benchmark designed to evaluate natural
language processing models across a broad range of tasks related to climate
change. Climate-Eval aggregates existing datasets along with a newly developed
news classification dataset, created specifically for this release. This
results in a benchmark of 25 tasks based on 13 datasets, covering key aspects
of climate discourse, including text classification, question answering, and
information extraction. Our benchmark provides a standardized evaluation suite
for systematically assessing the performance of large language models (LLMs) on
these tasks. Additionally, we conduct an extensive evaluation of open-source
LLMs (ranging from 2B to 70B parameters) in both zero-shot and few-shot
settings, analyzing their strengths and limitations in the domain of climate
change.

</details>


### [52] [Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics](https://arxiv.org/abs/2505.18658)
*Pankaj Kumar,Subhankar Mishra*

Main category: cs.CL

TL;DR: 该综述全面探讨了大语言模型（LLMs）的鲁棒性问题，包括概念定义、脆弱性来源、缓解策略及评估方法，并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言处理和人工智能领域展现出巨大潜力，但其鲁棒性不足仍是关键挑战。本文旨在系统梳理现有研究，推动该领域发展。

Method: 通过系统性分析LLMs鲁棒性的本质、脆弱性来源（模型固有缺陷/数据漏洞/外部对抗因素），综述前沿缓解策略与评估基准。

Result: 总结了当前鲁棒性研究的整体框架，包括分类体系、应对方法和评估指标，同时揭示了实际应用中的评估差距与未解决问题。

Conclusion: 需跨学科合作解决LLMs鲁棒性挑战，未来应关注真实场景可靠性评估与新型防御机制的开发。

Abstract: Large Language Models (LLMs) have emerged as a promising cornerstone for the
development of natural language processing (NLP) and artificial intelligence
(AI). However, ensuring the robustness of LLMs remains a critical challenge. To
address these challenges and advance the field, this survey provides a
comprehensive overview of current studies in this area. First, we
systematically examine the nature of robustness in LLMs, including its
conceptual foundations, the importance of consistent performance across diverse
inputs, and the implications of failure modes in real-world applications. Next,
we analyze the sources of non-robustness, categorizing intrinsic model
limitations, data-driven vulnerabilities, and external adversarial factors that
compromise reliability. Following this, we review state-of-the-art mitigation
strategies, and then we discuss widely adopted benchmarks, emerging metrics,
and persistent gaps in assessing real-world reliability. Finally, we synthesize
findings from existing surveys and interdisciplinary studies to highlight
trends, unresolved issues, and pathways for future research.

</details>


### [53] [Cross-Lingual Pitfalls: Automatic Probing Cross-Lingual Weakness of Multilingual Large Language Models](https://arxiv.org/abs/2505.18673)
*Zixiang Xu,Yanbo Wang,Yue Huang,Xiuying Chen,Jieyu Zhao,Meng Jiang,Xiangliang Zhang*

Main category: cs.CL

TL;DR: 该论文提出了一种高效识别大语言模型跨语言性能差异的新方法，构建了包含16种语言的6000对双语数据集，揭示了模型在目标语言中普遍存在的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在自然语言处理领域取得了显著成功，但其跨语言性能一致性仍是一个重要挑战。本文旨在开发一种方法，有效识别模型在不同语言间的固有弱点。

Method: 采用beam search和基于LLM的模拟生成双语问题对，通过对比英语与目标语言的性能差异来暴露模型弱点。

Result: 构建了覆盖16种语言的6000对双语数据集，实验表明该方法能精确且低成本地识别跨语言弱点，在目标语言中普遍观察到超过50%的准确率下降。语言相似性分析显示，相关语言具有相似的性能模式并可从针对性后训练中受益。

Conclusion: 该方法能系统性地揭示大语言模型的跨语言性能缺陷，为改进多语言能力提供了有效工具。代码已开源供社区使用。

Abstract: Large Language Models (LLMs) have achieved remarkable success in Natural
Language Processing (NLP), yet their cross-lingual performance consistency
remains a significant challenge. This paper introduces a novel methodology for
efficiently identifying inherent cross-lingual weaknesses in LLMs. Our approach
leverages beam search and LLM-based simulation to generate bilingual question
pairs that expose performance discrepancies between English and target
languages. We construct a new dataset of over 6,000 bilingual pairs across 16
languages using this methodology, demonstrating its effectiveness in revealing
weaknesses even in state-of-the-art models. The extensive experiments
demonstrate that our method precisely and cost-effectively pinpoints
cross-lingual weaknesses, consistently revealing over 50\% accuracy drops in
target languages across a wide range of models. Moreover, further experiments
investigate the relationship between linguistic similarity and cross-lingual
weaknesses, revealing that linguistically related languages share similar
performance patterns and benefit from targeted post-training. Code is available
at https://github.com/xzx34/Cross-Lingual-Pitfalls.

</details>


### [54] [Social Good or Scientific Curiosity? Uncovering the Research Framing Behind NLP Artefacts](https://arxiv.org/abs/2505.18677)
*Eric Chamoun,Nedjma Ousidhoum,Michael Schlichtkrull,Andreas Vlachos*

Main category: cs.CL

TL;DR: 该研究提出了一种自动化分析NLP研究框架的系统，通过提取关键元素并建立关联规则，改进了现有LLM基线，并揭示了自动事实核查领域的研究趋势。


<details>
  <summary>Details</summary>
Motivation: 当前NLP研究中，很少有论文明确识别关键利益相关者、预期用途或适用情境，这导致研究与实际应用之间存在脱节。因此，需要一种自动化方法来分析研究框架。

Method: 开发了一个三组件系统，首先提取关键元素（手段、目的、利益相关者），然后通过可解释的规则和上下文推理将它们关联起来。

Result: 在自动事实核查和仇恨言论检测两个领域评估了该方法，均取得了优于强LLM基线的效果。应用该系统还发现了自动事实核查领域的三个趋势：研究目标模糊化、科学探索重于应用、转向支持人工而非完全自动化。

Conclusion: 该自动化系统能有效分析NLP研究框架，揭示研究趋势，有助于更好地对齐研究与实际应用。

Abstract: Clarifying the research framing of NLP artefacts (e.g., models, datasets,
etc.) is crucial to aligning research with practical applications. Recent
studies manually analyzed NLP research across domains, showing that few papers
explicitly identify key stakeholders, intended uses, or appropriate contexts.
In this work, we propose to automate this analysis, developing a
three-component system that infers research framings by first extracting key
elements (means, ends, stakeholders), then linking them through interpretable
rules and contextual reasoning. We evaluate our approach on two domains:
automated fact-checking using an existing dataset, and hate speech detection
for which we annotate a new dataset-achieving consistent improvements over
strong LLM baselines. Finally, we apply our system to recent automated
fact-checking papers and uncover three notable trends: a rise in vague or
underspecified research goals, increased emphasis on scientific exploration
over application, and a shift toward supporting human fact-checkers rather than
pursuing full automation.

</details>


### [55] [TULUN: Transparent and Adaptable Low-resource Machine Translation](https://arxiv.org/abs/2505.18683)
*Raphaël Merx,Hanna Suominen,Lois Hong,Nick Thieberger,Trevor Cohn,Ekaterina Vylomova*

Main category: cs.CL

TL;DR: 提出Tulun系统，结合神经机器翻译与基于大语言模型的术语后编辑，提升低资源语言在专业领域的翻译质量。


<details>
  <summary>Details</summary>
Motivation: 现有领域自适应方法需模型微调，对非技术用户和小型组织不友好，需更易用的解决方案。

Method: 融合神经MT与LLM后编辑，通过术语表和翻译记忆库指导，提供开源协作式网页平台。

Result: 在Tetun和Bislama的医疗救灾任务中提升16.90-22.41 ChrF++；FLORES数据集上6种低资源语言平均提升2.8 ChrF。

Conclusion: Tulun系统有效整合领域专业知识，显著提升低资源语言的领域翻译准确性。

Abstract: Machine translation (MT) systems that support low-resource languages often
struggle on specialized domains. While researchers have proposed various
techniques for domain adaptation, these approaches typically require model
fine-tuning, making them impractical for non-technical users and small
organizations. To address this gap, we propose Tulun, a versatile solution for
terminology-aware translation, combining neural MT with large language model
(LLM)-based post-editing guided by existing glossaries and translation
memories. Our open-source web-based platform enables users to easily create,
edit, and leverage terminology resources, fostering a collaborative
human-machine translation process that respects and incorporates domain
expertise while increasing MT accuracy. Evaluations show effectiveness in both
real-world and benchmark scenarios: on medical and disaster relief translation
tasks for Tetun and Bislama, our system achieves improvements of 16.90-22.41
ChrF++ points over baseline MT systems. Across six low-resource languages on
the FLORES dataset, Tulun outperforms both standalone MT and LLM approaches,
achieving an average improvement of 2.8 ChrF points over NLLB-54B.

</details>


### [56] [From Generation to Detection: A Multimodal Multi-Task Dataset for Benchmarking Health Misinformation](https://arxiv.org/abs/2505.18685)
*Zhihao Zhang,Yiran Zhang,Xiyue Zhou,Liting Huang,Imran Razzak,Preslav Nakov,Usman Naseem*

Main category: cs.CL

TL;DR: 该论文介绍了MM Health数据集，旨在解决健康领域多模态错误信息的检测问题，包含人类和AI生成的内容，并展示了现有模型在区分信息可靠性和来源方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 健康错误信息对社会和个人有显著负面影响，尤其是生成式AI的快速发展加剧了其传播。现有数据集在主题覆盖、AI生成内容包含和原始内容可访问性方面存在不足，亟需一个全面的多模态数据集来支持错误信息检测研究。

Method: 作者构建了MM Health数据集，包含34,746篇健康领域的多模态新闻文章，其中5,776篇为人类生成，28,880篇为AI生成。数据集涵盖了文本和视觉信息，并基于多种先进生成式AI模型创建。

Result: 实验表明，现有先进模型在可靠性检查、原创性检查和细粒度AI检测三个任务上表现不佳，难以准确区分信息的可靠性和来源。

Conclusion: MM Health数据集为健康领域多模态错误信息检测提供了重要资源，有助于开发更有效的检测方法，应对人类和机器生成内容的挑战。

Abstract: Infodemics and health misinformation have significant negative impact on
individuals and society, exacerbating confusion and increasing hesitancy in
adopting recommended health measures. Recent advancements in generative AI,
capable of producing realistic, human like text and images, have significantly
accelerated the spread and expanded the reach of health misinformation,
resulting in an alarming surge in its dissemination. To combat the infodemics,
most existing work has focused on developing misinformation datasets from
social media and fact checking platforms, but has faced limitations in topical
coverage, inclusion of AI generation, and accessibility of raw content. To
address these issues, we present MM Health, a large scale multimodal
misinformation dataset in the health domain consisting of 34,746 news article
encompassing both textual and visual information. MM Health includes
human-generated multimodal information (5,776 articles) and AI generated
multimodal information (28,880 articles) from various SOTA generative AI
models. Additionally, We benchmarked our dataset against three tasks
(reliability checks, originality checks, and fine-grained AI detection)
demonstrating that existing SOTA models struggle to accurately distinguish the
reliability and origin of information. Our dataset aims to support the
development of misinformation detection across various health scenarios,
facilitating the detection of human and machine generated content at multimodal
levels.

</details>


### [57] [Large Language Models in the Task of Automatic Validation of Text Classifier Predictions](https://arxiv.org/abs/2505.18688)
*Aleksandr Tsymbalov*

Main category: cs.CL

TL;DR: 该论文提出使用大型语言模型（LLM）替代人工标注，以降低文本分类模型训练中的数据标注成本，并支持高质量的增量学习。


<details>
  <summary>Details</summary>
Motivation: 传统文本分类模型依赖人工标注数据，成本高且效率低，尤其在模型持续更新时更为突出。论文旨在解决这一问题。

Method: 提出多种方法，利用大型语言模型（LLM）测试分类器预测的正确性，减少对人工标注的依赖。

Result: 通过LLM替代人工标注，能够有效降低数据收集成本，同时保持模型质量并支持增量学习。

Conclusion: 使用LLM替代人工标注是一种可行且高效的方法，能够显著降低文本分类模型生命周期中的标注成本。

Abstract: Machine learning models for text classification are trained to predict a
class for a given text. To do this, training and validation samples must be
prepared: a set of texts is collected, and each text is assigned a class. These
classes are usually assigned by human annotators with different expertise
levels, depending on the specific classification task. Collecting such samples
from scratch is labor-intensive because it requires finding specialists and
compensating them for their work; moreover, the number of available specialists
is limited, and their productivity is constrained by human factors. While it
may not be too resource-intensive to collect samples once, the ongoing need to
retrain models (especially in incremental learning pipelines) to address data
drift (also called model drift) makes the data collection process crucial and
costly over the model's entire lifecycle. This paper proposes several
approaches to replace human annotators with Large Language Models (LLMs) to
test classifier predictions for correctness, helping ensure model quality and
support high-quality incremental learning.

</details>


### [58] [Benchmarking and Rethinking Knowledge Editing for Large Language Models](https://arxiv.org/abs/2505.18690)
*Guoxiu He,Xin Song,Futing Wang,Aixin Sun*

Main category: cs.CL

TL;DR: 该论文通过全面基准测试，比较了不同知识编辑方法在大型语言模型中的表现，发现基于参数的方法在现实条件下表现不佳，而选择性上下文推理(SCR)方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型知识编辑方法在评估目标和实验设置上存在不一致性，导致难以客观比较其效果。为了解决这一问题，作者进行了全面的基准测试。

Method: 研究引入了多层次数据集(事实级、事件级和通用任务数据)，在自回归推理设置下评估不同方法，并采用四个评估维度(包括可移植性)进行比较。

Result: 实证结果表明，基于参数的知识编辑方法在现实条件下表现不佳，而选择性上下文推理(SCR)方法在所有设置中都表现更优。

Conclusion: 研究揭示了当前知识编辑方法的局限性，并表明基于上下文的推理方法可能是一个更鲁棒的替代方案。

Abstract: Knowledge editing aims to update the embedded knowledge within Large Language
Models (LLMs). However, existing approaches, whether through parameter
modification or external memory integration, often suffer from inconsistent
evaluation objectives and experimental setups. To address this gap, we conduct
a comprehensive benchmarking study. In addition to fact-level datasets, we
introduce more complex event-based datasets and general-purpose datasets drawn
from other tasks. Our evaluation covers both instruction-tuned and
reasoning-oriented LLMs, under a realistic autoregressive inference setting
rather than teacher-forced decoding. Beyond single-edit assessments, we also
evaluate multi-edit scenarios to better reflect practical demands. We employ
four evaluation dimensions, including portability, and compare all recent
methods against a simple and straightforward baseline named Selective
Contextual Reasoning (SCR). Empirical results reveal that parameter-based
editing methods perform poorly under realistic conditions. In contrast, SCR
consistently outperforms them across all settings. This study offers new
insights into the limitations of current knowledge editing methods and
highlights the potential of context-based reasoning as a more robust
alternative.

</details>


### [59] [Towards Semantic Integration of Opinions: Unified Opinion Concepts Ontology and Extraction Task](https://arxiv.org/abs/2505.18703)
*Gaurav Negi,Dhairya Dalal,Omnia Zayed,Paul Buitelaar*

Main category: cs.CL

TL;DR: 该论文提出了一种统一意见概念（UOC）本体，用于在语义上下文中整合意见，并设计了相应的提取任务（UOCE）及评估方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决不同表述中意见语义表示的差异，需要一个统一的概念框架来整合意见的语义上下文。

Method: 基于NLP中广泛研究的意见面和符号描述的语义结构，提出了UOC本体，并设计了UOCE任务来提取具有增强表达性的意见。

Result: 提供了手动扩展和重新标注的评估数据集，定制了评估指标，并使用最先进的生成模型建立了UOCE任务的基线性能。

Conclusion: UOC本体和UOCE任务为意见的语义整合和提取提供了统一的框架和方法，并通过实验验证了其可行性。

Abstract: This paper introduces the Unified Opinion Concepts (UOC) ontology to
integrate opinions within their semantic context. The UOC ontology bridges the
gap between the semantic representation of opinion across different
formulations. It is a unified conceptualisation based on the facets of opinions
studied extensively in NLP and semantic structures described through symbolic
descriptions. We further propose the Unified Opinion Concept Extraction (UOCE)
task of extracting opinions from the text with enhanced expressivity.
Additionally, we provide a manually extended and re-annotated evaluation
dataset for this task and tailored evaluation metrics to assess the adherence
of extracted opinions to UOC semantics. Finally, we establish baseline
performance for the UOCE task using state-of-the-art generative models.

</details>


### [60] [A General Knowledge Injection Framework for ICD Coding](https://arxiv.org/abs/2505.18708)
*Xu Zhang,Kun Zhang,Wenxin Ma,Rongsheng Wang,Chenxu Wu,Yingtai Li,S. Kevin Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种名为GKI-ICD的新型通用知识注入框架，通过整合ICD描述、同义词和层次结构三种关键知识，有效提升了ICD编码性能。


<details>
  <summary>Details</summary>
Motivation: 现有的ICD编码方法通常只关注单一类型的知识，并设计复杂且不兼容的专用模块，限制了其扩展性和有效性。为了解决这一问题，作者提出了GKI-ICD框架。

Method: GKI-ICD框架整合了ICD描述、同义词和层次结构三种知识，无需设计额外的专用模块，从而实现了知识的综合利用。

Result: 在现有流行的ICD编码基准测试中，GKI-ICD在大多数评估指标上达到了最先进的性能。

Conclusion: GKI-ICD通过综合利用多种知识，有效提升了ICD编码的性能，展现了其在实际应用中的潜力。

Abstract: ICD Coding aims to assign a wide range of medical codes to a medical text
document, which is a popular and challenging task in the healthcare domain. To
alleviate the problems of long-tail distribution and the lack of annotations of
code-specific evidence, many previous works have proposed incorporating code
knowledge to improve coding performance. However, existing methods often focus
on a single type of knowledge and design specialized modules that are complex
and incompatible with each other, thereby limiting their scalability and
effectiveness. To address this issue, we propose GKI-ICD, a novel, general
knowledge injection framework that integrates three key types of knowledge,
namely ICD Description, ICD Synonym, and ICD Hierarchy, without specialized
design of additional modules. The comprehensive utilization of the above
knowledge, which exhibits both differences and complementarity, can effectively
enhance the ICD coding performance. Extensive experiments on existing popular
ICD coding benchmarks demonstrate the effectiveness of GKI-ICD, which achieves
the state-of-the-art performance on most evaluation metrics. Code is available
at https://github.com/xuzhang0112/GKI-ICD.

</details>


### [61] [Improving Bangla Linguistics: Advanced LSTM, Bi-LSTM, and Seq2Seq Models for Translating Sylheti to Modern Bangla](https://arxiv.org/abs/2505.18709)
*Sourav Kumar Das,Md. Julkar Naeen,MD. Jahidul Islam,Md. Anisul Haque Sajeeb,Narayan Ranjan Chakraborty,Mayen Uddin Mojumdar*

Main category: cs.CL

TL;DR: 该研究针对孟加拉国地方语言Sylheti，使用NLP技术将现代孟加拉语翻译为Sylheti方言，LSTM模型以89.3%准确率表现最佳。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国各地区使用不同的地方语言（如Sylheti、Chittagong等），但现有研究多集中于标准孟加拉语，缺乏对地方语言的关注。本研究旨在填补这一空白，推动孟加拉语NLP研究的发展。

Method: 研究采用自然语言处理（NLP）技术，使用1200条数据训练了LSTM、Bi-LSTM和Seq2Seq三种模型，实现现代孟加拉语到Sylheti方言的翻译。

Result: 实验结果表明，LSTM模型性能最佳，准确率达到89.3%。

Conclusion: 该研究为孟加拉地方语言处理提供了新思路，未来可促进更先进的孟加拉语NLP技术创新。

Abstract: Bangla or Bengali is the national language of Bangladesh, people from
different regions don't talk in proper Bangla. Every division of Bangladesh has
its own local language like Sylheti, Chittagong etc. In recent years some
papers were published on Bangla language like sentiment analysis, fake news
detection and classifications, but a few of them were on Bangla languages. This
research is for the local language and this particular paper is on Sylheti
language. It presented a comprehensive system using Natural Language Processing
or NLP techniques for translating Pure or Modern Bangla to locally spoken
Sylheti Bangla language. Total 1200 data used for training 3 models LSTM,
Bi-LSTM and Seq2Seq and LSTM scored the best in performance with 89.3%
accuracy. The findings of this research may contribute to the growth of Bangla
NLP researchers for future more advanced innovations.

</details>


### [62] [Optimal Transport-Based Token Weighting scheme for Enhanced Preference Optimization](https://arxiv.org/abs/2505.18720)
*Meng Li,Guangda Huzhang,Haibo Zhang,Xiting Wang,Anxiang Zeng*

Main category: cs.CL

TL;DR: 论文提出OTPO方法，通过最优传输理论对token加权，优化DPO框架，提升语言模型与人类偏好的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有DPO方法对所有token平等加权，而人类更关注语义重要部分，导致偏好优化效果不佳。

Method: 采用基于最优传输的token加权方案（OTPO），强调重要token对，弱化无关token的影响。

Result: 实验表明OTPO能有效提升模型指令跟随能力，增强奖励稳定性和可解释性。

Conclusion: OTPO通过上下文感知的token加权，使偏好优化更聚焦于响应间的有意义差异。

Abstract: Direct Preference Optimization (DPO) has emerged as a promising framework for
aligning Large Language Models (LLMs) with human preferences by directly
optimizing the log-likelihood difference between chosen and rejected responses.
However, existing methods assign equal importance to all tokens in the
response, while humans focus on more meaningful parts. This leads to suboptimal
preference optimization, as irrelevant or noisy tokens disproportionately
influence DPO loss. To address this limitation, we propose \textbf{O}ptimal
\textbf{T}ransport-based token weighting scheme for enhancing direct
\textbf{P}reference \textbf{O}ptimization (OTPO). By emphasizing semantically
meaningful token pairs and de-emphasizing less relevant ones, our method
introduces a context-aware token weighting scheme that yields a more
contrastive reward difference estimate. This adaptive weighting enhances reward
stability, improves interpretability, and ensures that preference optimization
focuses on meaningful differences between responses. Extensive experiments have
validated OTPO's effectiveness in improving instruction-following ability
across various settings\footnote{Code is available at
https://github.com/Mimasss2/OTPO.}.

</details>


### [63] [LogicCat: A Chain-of-Thought Text-to-SQL Benchmark for Multi-Domain Reasoning Challenges](https://arxiv.org/abs/2505.18744)
*Tao Liu,Hongying Zan,Yifan Li,Dixuan Zhang,Lulu Kong,Haixin Liu,Jiaming Hou,Aoze Zheng,Rui Li,Yiming Qiao,Zewei Luo,Qi Wang,Zhiqiang Zhang,Jiaxi Li,Supeng Liu,Kunli Zhang,Min Peng*

Main category: cs.CL

TL;DR: 该论文提出了一个针对复杂推理和思维链分析的Text-to-SQL新数据集LogicCat，填补了现有数据集在领域知识和数学推理上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL数据集主要关注商业场景和操作逻辑，缺乏对领域特定知识和复杂数学推理的覆盖，因此需要一个新的数据集来填补这一空白。

Method: 作者构建了一个包含4,038个英文问题及对应SQL查询的数据集LogicCat，覆盖45个不同领域的数据库，并提供了12,114个逐步推理标注。

Result: 实验表明，LogicCat显著提升了现有模型的难度，最高执行准确率仅为14.96%，而加入思维链标注后性能提升至33.96%。

Conclusion: LogicCat为Text-to-SQL系统在复杂推理方面的研究提供了新的挑战和机遇，相关代码已开源。

Abstract: Text-to-SQL is a fundamental task in natural language processing that seeks
to translate natural language questions into meaningful and executable SQL
queries. While existing datasets are extensive and primarily focus on business
scenarios and operational logic, they frequently lack coverage of
domain-specific knowledge and complex mathematical reasoning. To address this
gap, we present a novel dataset tailored for complex reasoning and
chain-of-thought analysis in SQL inference, encompassing physical, arithmetic,
commonsense, and hypothetical reasoning. The dataset consists of 4,038 English
questions, each paired with a unique SQL query and accompanied by 12,114
step-by-step reasoning annotations, spanning 45 databases across diverse
domains. Experimental results demonstrate that LogicCat substantially increases
the difficulty for state-of-the-art models, with the highest execution accuracy
reaching only 14.96%. Incorporating our chain-of-thought annotations boosts
performance to 33.96%. Benchmarking leading public methods on Spider and BIRD
further underscores the unique challenges presented by LogicCat, highlighting
the significant opportunities for advancing research in robust,
reasoning-driven text-to-SQL systems. We have released our dataset code at
https://github.com/Ffunkytao/LogicCat.

</details>


### [64] [Unifying Attention Heads and Task Vectors via Hidden State Geometry in In-Context Learning](https://arxiv.org/abs/2505.18752)
*Haolin Yang,Hakaze Cho,Yiqiao Zhong,Naoya Inoue*

Main category: cs.CL

TL;DR: 本文提出了一个统一框架，通过分析控制上下文学习性能的两个几何因素——查询隐藏状态的可分离性和对齐性，揭示了大型语言模型在分类任务中的两阶段机制。


<details>
  <summary>Details</summary>
Motivation: 先前的研究通常只关注特定的注意力头或任务向量，缺乏将这些组件与隐藏状态演化联系起来的统一框架，因此需要更全面的理解上下文学习的内部机制。

Method: 通过分析查询隐藏状态的可分离性和对齐性这两个几何因素，对分层动态进行细粒度分析，并结合消融研究验证不同注意力头和任务向量的作用。

Result: 研究发现上下文学习具有两阶段机制：早期层实现可分离性，后期层发展对齐性；Previous Token Heads驱动可分离性，而Induction Heads和任务向量增强对齐性。

Conclusion: 该研究填补了注意力头与任务向量之间的空白，为上下文学习的底层机制提供了统一解释。

Abstract: The unusual properties of in-context learning (ICL) have prompted
investigations into the internal mechanisms of large language models. Prior
work typically focuses on either special attention heads or task vectors at
specific layers, but lacks a unified framework linking these components to the
evolution of hidden states across layers that ultimately produce the model's
output. In this paper, we propose such a framework for ICL in classification
tasks by analyzing two geometric factors that govern performance: the
separability and alignment of query hidden states. A fine-grained analysis of
layer-wise dynamics reveals a striking two-stage mechanism: separability
emerges in early layers, while alignment develops in later layers. Ablation
studies further show that Previous Token Heads drive separability, while
Induction Heads and task vectors enhance alignment. Our findings thus bridge
the gap between attention heads and task vectors, offering a unified account of
ICL's underlying mechanisms.

</details>


### [65] [Few-Shot Optimization for Sensor Data Using Large Language Models: A Case Study on Fatigue Detection](https://arxiv.org/abs/2505.18754)
*Elsen Ronando,Sozo Inoue*

Main category: cs.CL

TL;DR: 本文提出了一种结合欧氏距离与大语言模型（HED-LM）的少样本优化方法，用于提升传感器分类任务中的示例选择质量，并在疲劳检测任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 少样本提示在有限标注数据下能高效推理，但其性能高度依赖所选示例的质量。针对传感器数据中模式重叠和个体差异大的挑战，需要更精细的示例选择方法。

Method: HED-LM通过混合选择流程：先基于欧氏距离过滤候选示例，再用大语言模型（LLMs）根据上下文相关性重新排序，结合数值相似性与语义相关性。

Result: 在疲劳检测任务中，HED-LM的宏F1分数达69.13±10.71%，显著优于随机选择（59.30±10.13%）和纯距离过滤（67.61±11.39%），相对提升分别为16.6%和2.3%。

Conclusion: HED-LM通过融合数值与语义特征提升了少样本提示的鲁棒性，为医疗监测、工业安全等传感器任务提供了实用解决方案，并具备广泛适用潜力。

Abstract: In this paper, we propose a novel few-shot optimization with HED-LM (Hybrid
Euclidean Distance with Large Language Models) to improve example selection for
sensor-based classification tasks. While few-shot prompting enables efficient
inference with limited labeled data, its performance largely depends on the
quality of selected examples. HED-LM addresses this challenge through a hybrid
selection pipeline that filters candidate examples based on Euclidean distance
and re-ranks them using contextual relevance scored by large language models
(LLMs). To validate its effectiveness, we apply HED-LM to a fatigue detection
task using accelerometer data characterized by overlapping patterns and high
inter-subject variability. Unlike simpler tasks such as activity recognition,
fatigue detection demands more nuanced example selection due to subtle
differences in physiological signals. Our experiments show that HED-LM achieves
a mean macro F1-score of 69.13$\pm$10.71%, outperforming both random selection
(59.30$\pm$10.13%) and distance-only filtering (67.61$\pm$11.39%). These
represent relative improvements of 16.6% and 2.3%, respectively. The results
confirm that combining numerical similarity with contextual relevance improves
the robustness of few-shot prompting. Overall, HED-LM offers a practical
solution to improve performance in real-world sensor-based learning tasks and
shows potential for broader applications in healthcare monitoring, human
activity recognition, and industrial safety scenarios.

</details>


### [66] [How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark](https://arxiv.org/abs/2505.18761)
*Minglai Yang,Ethan Huang,Liang Zhang,Mihai Surdeanu,William Wang,Liangming Pan*

Main category: cs.CL

TL;DR: GSM-DC是一个评估大语言模型在干扰上下文下的推理鲁棒性的合成基准，通过精确注入干扰项进行严格评估。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在面对无关上下文时的推理鲁棒性，并探索提升模型性能的方法。

Method: 构建符号推理图并注入精确干扰项，提出基于过程奖励模型的逐步树搜索方法。

Result: 实验显示大语言模型对干扰上下文敏感，但通过强干扰训练和树搜索方法能显著提升鲁棒性。

Conclusion: GSM-DC有效评估模型鲁棒性，干扰训练和树搜索方法能提升模型在分布内外场景的表现。

Abstract: We introduce Grade School Math with Distracting Context (GSM-DC), a synthetic
benchmark to evaluate Large Language Models' (LLMs) reasoning robustness
against systematically controlled irrelevant context (IC). GSM-DC constructs
symbolic reasoning graphs with precise distractor injections, enabling
rigorous, reproducible evaluation. Our experiments demonstrate that LLMs are
significantly sensitive to IC, affecting both reasoning path selection and
arithmetic accuracy. Additionally, training models with strong distractors
improves performance in both in-distribution and out-of-distribution scenarios.
We further propose a stepwise tree search guided by a process reward model,
which notably enhances robustness in out-of-distribution conditions.

</details>


### [67] [Towards an automatic method for generating topical vocabulary test forms for specific reading passages](https://arxiv.org/abs/2505.18762)
*Michael Flor,Zuowei Wang,Paul Deane,Tenaha O'Reilly*

Main category: cs.CL

TL;DR: 开发自动化工具K-tool，通过生成主题词汇测试评估学生对特定文本的背景知识，以预测其理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有自动化评估学生背景知识的方法较少，难以即时预测学生对特定领域文本的理解能力。

Method: 系统自动检测文本主题，生成相关词汇测试，包括高关联词和相似但不相关词汇，形成背景知识评估表。

Result: K-tool能自动处理单篇阅读材料，不依赖语料库，适用于初高中英语母语学生。

Conclusion: K-tool为自动化评估学生背景知识提供了可行方案，初步验证了其系统架构和输出的有效性。

Abstract: Background knowledge is typically needed for successful comprehension of
topical and domain specific reading passages, such as in the STEM domain.
However, there are few automated measures of student knowledge that can be
readily deployed and scored in time to make predictions on whether a given
student will likely be able to understand a specific content area text. In this
paper, we present our effort in developing K-tool, an automated system for
generating topical vocabulary tests that measure students' background knowledge
related to a specific text. The system automatically detects the topic of a
given text and produces topical vocabulary items based on their relationship
with the topic. This information is used to automatically generate background
knowledge forms that contain words that are highly related to the topic and
words that share similar features but do not share high associations to the
topic. Prior research indicates that performance on such tasks can help
determine whether a student is likely to understand a particular text based on
their knowledge state. The described system is intended for use with middle and
high school student population of native speakers of English. It is designed to
handle single reading passages and is not dependent on any corpus or text
collection. In this paper, we describe the system architecture and present an
initial evaluation of the system outputs.

</details>


### [68] [Disentangling Knowledge Representations for Large Language Model Editing](https://arxiv.org/abs/2505.18774)
*Mengqi Zhang,Zisheng Zhou,Xiaotian Ye,Qiang Liu,Zhaochun Ren,Zhumin Chen,Pengjie Ren*

Main category: cs.CL

TL;DR: DiKE提出了一种新方法，通过解耦知识表示来编辑大语言模型中的知识，有效保留与编辑知识无关的细粒度知识。


<details>
  <summary>Details</summary>
Motivation: 现有方法在更新大语言模型中的知识时，难以保留与编辑知识共享同一主题但关系不同的细粒度无关知识，导致这些知识在编辑过程中被意外修改。

Method: DiKE包含两个关键模块：知识表示解耦模块（KRD）将主题表示分解为目标知识相关和不相关的部分；基于解耦的知识编辑模块（DKE）仅更新目标相关部分，同时明确保留不相关部分。此外，还基于矩阵理论推导了一种闭式、秩一参数更新方法。

Result: 实验表明，DiKE在多个大语言模型上显著提高了细粒度无关知识的保留能力，同时保持了竞争力的通用编辑性能。

Conclusion: DiKE通过解耦知识表示，有效解决了编辑大语言模型时细粒度无关知识保留的挑战，为知识编辑提供了一种高效且最小侵入性的方法。

Abstract: Knowledge Editing has emerged as a promising solution for efficiently
updating embedded knowledge in large language models (LLMs). While existing
approaches demonstrate effectiveness in integrating new knowledge and
preserving the original capabilities of LLMs, they fail to maintain
fine-grained irrelevant knowledge facts that share the same subject as edited
knowledge but differ in relation and object. This challenge arises because
subject representations inherently encode multiple attributes, causing the
target and fine-grained irrelevant knowledge to become entangled in the
representation space, and thus vulnerable to unintended alterations during
editing. To address this, we propose DiKE, a novel approach that Disentangles
Knowledge representations for LLM Editing (DiKE). DiKE consists of two key
components: a Knowledge Representation Disentanglement (KRD) module that
decomposes the subject representation into target-knowledgerelated and
-unrelated components, and a Disentanglement-based Knowledge Edit (DKE) module
that updates only the target-related component while explicitly preserving the
unrelated one. We further derive a closed-form, rank-one parameter update based
on matrix theory to enable efficient and minimally invasive edits. To
rigorously evaluate fine-grained irrelevant knowledge preservation, we
construct FINE-KED, a new benchmark comprising fine-grained irrelevant
knowledge at different levels of relational similarity to the edited knowledge.
Extensive experiments across multiple LLMs demonstrate that DiKE substantially
improves fine-grained irrelevant knowledge preservation while maintaining
competitive general editing performance.

</details>


### [69] [A generalised editor calculus (Short Paper)](https://arxiv.org/abs/2505.18778)
*Benjamin Bennetzen,Peter Buus Steffensen,Hans Hüttel,Nikolaj Rossander Kristensen,Andreas Tor Mortensen*

Main category: cs.CL

TL;DR: 本文提出了一种语法导向编辑器演算的泛化方法，可基于抽象语法为任意语言实例化专用编辑器，保证语法正确性同时支持不完整程序。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统语法导向编辑器无法同时保证语法正确性和支持不完整程序的问题，本文旨在提出一种通用的解决方案。

Method: 通过将泛化的编辑器演算编码到扩展的简单类型lambda演算中（包含对、布尔值、模式匹配和固定点）。

Result: 实现了可适用于任意语言的语法导向编辑器框架，在保证语法无错误的同时允许程序不完整。

Conclusion: 该泛化编辑器演算为语言专用编辑器的构建提供了理论基础和实现方法，具有通用性和可靠性。

Abstract: In this paper, we present a generalization of a syntax-directed editor
calculus, which can be used to instantiate a specialized syntax-directed editor
for any language, given by some abstract syntax. The editor calculus guarantees
the absence of syntactical errors while allowing incomplete programs. The
generalized editor calculus is then encoded into a simply typed lambda
calculus, extended with pairs, booleans, pattern matching and fixed points

</details>


### [70] [ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models](https://arxiv.org/abs/2505.18799)
*Hao Chen,Haoze Li,Zhiqing Xiao,Lirong Gao,Qi Zhang,Xiaomeng Hu,Ningtao Wang,Xing Fu,Junbo Zhao*

Main category: cs.CL

TL;DR: 提出ALPS方法，通过定位和修剪任务敏感的注意力头，仅激活10%参数即可提升2%性能，且具有跨数据集可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖数据且泛化性差，需降低大模型下游任务对齐成本。

Method: ALPS算法：定位任务敏感注意力头并限制训练更新至这些头部。

Result: 仅微调10%注意力参数，三项任务性能提升2%，且头部可跨数据集迁移。

Conclusion: 为高效模型对齐提供了新视角，同时缓解知识遗忘问题。

Abstract: Aligning general-purpose large language models (LLMs) to downstream tasks
often incurs significant costs, including constructing task-specific
instruction pairs and extensive training adjustments. Prior research has
explored various avenues to enhance alignment efficiency, primarily through
minimal-data training or data-driven activations to identify key attention
heads. However, these approaches inherently introduce data dependency, which
hinders generalization and reusability. To address this issue and enhance model
alignment efficiency, we propose the \textit{\textbf{A}ttention
\textbf{L}ocalization and \textbf{P}runing \textbf{S}trategy (\textbf{ALPS})},
an efficient algorithm that localizes the most task-sensitive attention heads
and prunes by restricting attention training updates to these heads, thereby
reducing alignment costs. Experimental results demonstrate that our method
activates only \textbf{10\%} of attention parameters during fine-tuning while
achieving a \textbf{2\%} performance improvement over baselines on three tasks.
Moreover, the identified task-specific heads are transferable across datasets
and mitigate knowledge forgetting. Our work and findings provide a novel
perspective on efficient LLM alignment.

</details>


### [71] [Don't Look Only Once: Towards Multimodal Interactive Reasoning with Selective Visual Revisitation](https://arxiv.org/abs/2505.18842)
*Jiwan Chung,Junhyeok Kim,Siyeol Kim,Jaeyoung Lee,Min Soo Kim,Youngjae Yu*

Main category: cs.CL

TL;DR: v1是一个轻量级扩展，为多模态大语言模型（MLLMs）引入动态视觉重访机制，通过点选-复制机制选择性检索图像区域，提升多模态数学推理任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs通常仅单次处理视觉输入，依赖内部记忆进行推理，缺乏动态视觉参考能力。v1旨在通过动态视觉访问增强模型的细粒度视觉参照和多步推理能力。

Method: 提出点选-复制机制，允许模型在推理过程中动态检索相关图像区域；构建v1g数据集（30万条多模态推理轨迹，含交错视觉标注）训练该能力。

Result: 在MathVista等三个多模态数学推理基准测试中，v1性能持续超越基线模型，尤其在需要细粒度视觉参照的任务上表现突出。

Conclusion: 动态视觉访问是增强多模态推理的有效方向，代码、模型和数据将开源以支持后续研究。

Abstract: We present v1, a lightweight extension to Multimodal Large Language Models
(MLLMs) that enables selective visual revisitation during inference. While
current MLLMs typically consume visual input only once and reason purely over
internal memory, v1 introduces a simple point-and-copy mechanism that allows
the model to dynamically retrieve relevant image regions throughout the
reasoning process. This mechanism augments existing architectures with minimal
modifications, enabling contextual access to visual tokens based on the model's
evolving hypotheses. To train this capability, we construct v1g, a dataset of
300K multimodal reasoning traces with interleaved visual grounding annotations.
Experiments on three multimodal mathematical reasoning benchmarks -- MathVista,
MathVision, and MathVerse -- demonstrate that v1 consistently improves
performance over comparable baselines, particularly on tasks requiring
fine-grained visual reference and multi-step reasoning. Our results suggest
that dynamic visual access is a promising direction for enhancing grounded
multimodal reasoning. Code, models, and data will be released to support future
research.

</details>


### [72] [Multi-Party Conversational Agents: A Survey](https://arxiv.org/abs/2505.18845)
*Sagar Sapkota,Mohammad Saqib Hasan,Mubarak Shah,Santu Karmaker*

Main category: cs.CL

TL;DR: 该论文综述了多方对话代理(MPCAs)的最新进展，探讨了心理状态建模、语义理解和对话流预测三个核心问题，并指出心理理论(ToM)和多模态理解是未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 设计多方对话代理(MPCAs)比传统两方代理更具挑战性，需要同时理解话语语义和社交动态。本文旨在探索MPCAs的最新研究进展，为未来研究提供指导。

Method: 通过三个核心问题（心理状态建模、语义理解、代理行为建模）系统回顾了从经典机器学习到大型语言模型(LLMs)及多模态系统的各类方法。

Result: 分析表明心理理论(ToM)对构建智能MPCAs至关重要，同时多模态理解是一个有前景但尚未充分探索的方向。

Conclusion: 本文为未来研究者开发更强大的多方对话代理提供了方向性指导，强调需要结合心理理论和多模态理解能力。

Abstract: Multi-party Conversational Agents (MPCAs) are systems designed to engage in
dialogue with more than two participants simultaneously. Unlike traditional
two-party agents, designing MPCAs faces additional challenges due to the need
to interpret both utterance semantics and social dynamics. This survey explores
recent progress in MPCAs by addressing three key questions: 1) Can agents model
each participants' mental states? (State of Mind Modeling); 2) Can they
properly understand the dialogue content? (Semantic Understanding); and 3) Can
they reason about and predict future conversation flow? (Agent Action
Modeling). We review methods ranging from classical machine learning to Large
Language Models (LLMs) and multi-modal systems. Our analysis underscores Theory
of Mind (ToM) as essential for building intelligent MPCAs and highlights
multi-modal understanding as a promising yet underexplored direction. Finally,
this survey offers guidance to future researchers on developing more capable
MPCAs.

</details>


### [73] [Smoothie: Smoothing Diffusion on Token Embeddings for Text Generation](https://arxiv.org/abs/2505.18853)
*Alexander Shabalin,Viacheslav Meshchaninov,Dmitry Vetrov*

Main category: cs.CL

TL;DR: Smoothie是一种新的扩散模型方法，通过在语义相似的token嵌入上逐步平滑，结合了连续和离散方法的优势，提升了文本生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像、音频和视频生成中表现优异，但在文本生成中面临离散性挑战。现有方法要么在连续潜在空间中应用高斯扩散（语义结构保留但解码困难），要么在分类单纯形空间中操作（尊重离散性但忽略语义关系）。

Method: 提出Smoothing Diffusion on Token Embeddings (Smoothie)，通过在token嵌入上基于语义相似性逐步平滑，结合连续和离散方法的优势。

Result: 实验表明，Smoothie在多个序列生成任务中优于现有扩散模型，且消融研究证实其扩散空间性能优于标准嵌入空间和分类单纯形。

Conclusion: Smoothie通过结合语义平滑和离散解码，显著提升了文本生成的性能和质量。

Abstract: Diffusion models have achieved state-of-the-art performance in generating
images, audio, and video, but their adaptation to text remains challenging due
to its discrete nature. Prior approaches either apply Gaussian diffusion in
continuous latent spaces, which inherits semantic structure but struggles with
token decoding, or operate in categorical simplex space, which respect
discreteness but disregard semantic relation between tokens. In this paper, we
propose Smoothing Diffusion on Token Embeddings (Smoothie), a novel diffusion
method that combines the strengths of both approaches by progressively
smoothing token embeddings based on semantic similarity. This technique enables
gradual information removal while maintaining a natural decoding process.
Experimental results on several sequence-to-sequence generation tasks
demonstrate that Smoothie outperforms existing diffusion-based models in
generation quality. Furthermore, ablation studies show that our proposed
diffusion space yields better performance than both the standard embedding
space and the categorical simplex. Our code is available at
https://github.com/ashaba1in/smoothie.

</details>


### [74] [Writing Like the Best: Exemplar-Based Expository Text Generation](https://arxiv.org/abs/2505.18859)
*Yuxiang Liu,Kevin Chen-Chuan Chang*

Main category: cs.CL

TL;DR: 本文提出了基于范例的说明文生成任务，并提出了RePA框架，通过自适应模仿和分段生成解决现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 当前方法在生成说明文时依赖大量范例数据，难以适应特定主题内容，且长文本连贯性差。

Method: 提出了自适应模仿概念和RePA框架，利用大语言模型进行细粒度规划和分段模仿，增强输入清晰度和输出连贯性。

Result: 实验结果表明，RePA在生成事实性、一致性和相关性文本方面优于现有基线。

Conclusion: RePA框架有效解决了基于范例的说明文生成任务中的关键挑战，提升了生成质量。

Abstract: We introduce the Exemplar-Based Expository Text Generation task, aiming to
generate an expository text on a new topic using an exemplar on a similar
topic. Current methods fall short due to their reliance on extensive exemplar
data, difficulty in adapting topic-specific content, and issues with long-text
coherence. To address these challenges, we propose the concept of Adaptive
Imitation and present a novel Recurrent Plan-then-Adapt (RePA) framework. RePA
leverages large language models (LLMs) for effective adaptive imitation through
a fine-grained plan-then-adapt process. RePA also enables recurrent
segment-by-segment imitation, supported by two memory structures that enhance
input clarity and output coherence. We also develop task-specific evaluation
metrics--imitativeness, adaptiveness, and adaptive-imitativeness--using LLMs as
evaluators. Experimental results across our collected three diverse datasets
demonstrate that RePA surpasses existing baselines in producing factual,
consistent, and relevant texts for this task.

</details>


### [75] [Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework](https://arxiv.org/abs/2505.18864)
*Binhao Ma,Hanqing Guo,Zhengping Jay Luo,Rui Duan*

Main category: cs.CL

TL;DR: 本文提出了一种针对语音输入的多模态大语言模型（MLLMs）的白盒对抗攻击方法，通过生成对抗性语音令牌序列绕过安全防护，攻击成功率高达89%。


<details>
  <summary>Details</summary>
Motivation: 随着语音交互在多模态大语言模型中的应用日益广泛，其安全性问题尚未得到充分研究。本文旨在揭示语音模态的潜在漏洞，以促进更安全的下一代MLLMs的开发。

Method: 提出了一种新颖的令牌级攻击方法，利用模型的语音令牌化机制生成对抗性令牌序列，并将其合成为音频提示，以绕过安全防护。

Result: 在SpeechGPT上的评估显示，该方法在多个受限任务中攻击成功率达到89%，显著优于现有的语音攻击方法。

Conclusion: 本研究揭示了语音交互在MLLMs中的安全漏洞，为开发更健壮的下一代模型提供了重要参考。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have
significantly enhanced the naturalness and flexibility of human computer
interaction by enabling seamless understanding across text, vision, and audio
modalities. Among these, voice enabled models such as SpeechGPT have
demonstrated considerable improvements in usability, offering expressive, and
emotionally responsive interactions that foster deeper connections in real
world communication scenarios. However, the use of voice introduces new
security risks, as attackers can exploit the unique characteristics of spoken
language, such as timing, pronunciation variability, and speech to text
translation, to craft inputs that bypass defenses in ways not seen in
text-based systems. Despite substantial research on text based jailbreaks, the
voice modality remains largely underexplored in terms of both attack strategies
and defense mechanisms. In this work, we present an adversarial attack
targeting the speech input of aligned MLLMs in a white box scenario.
Specifically, we introduce a novel token level attack that leverages access to
the model's speech tokenization to generate adversarial token sequences. These
sequences are then synthesized into audio prompts, which effectively bypass
alignment safeguards and to induce prohibited outputs. Evaluated on SpeechGPT,
our approach achieves up to 89 percent attack success rate across multiple
restricted tasks, significantly outperforming existing voice based jailbreak
methods. Our findings shed light on the vulnerabilities of voice-enabled
multimodal systems and to help guide the development of more robust
next-generation MLLMs.

</details>


### [76] [Sci-LoRA: Mixture of Scientific LoRAs for Cross-Domain Lay Paraphrasing](https://arxiv.org/abs/2505.18867)
*Ming Cheng,Jiaying Gong,Hoda Eldardiry*

Main category: cs.CL

TL;DR: 本文提出Sci-LoRA模型，通过动态混合多领域LoRA适配器实现跨学科科学文本的通俗化改写，无需领域标签且性能优于现有大模型。


<details>
  <summary>Details</summary>
Motivation: 现有科学文本通俗化改写研究多局限于单一领域（如生物医学），而跨学科研究需要理解多领域技术知识。

Method: 提出Sci-LoRA模型：1）混合多个科学领域微调的LoRA适配器；2）动态生成权重调整各领域影响；3）在数据和模型层面整合信息。

Result: 在5个公开数据集、12个领域的测试中，Sci-LoRA显著优于现有大语言模型，展现跨领域改写的强适应性和泛化能力。

Conclusion: Sci-LoRA通过动态领域适配机制，为跨学科科学知识的通俗化传播提供了有效解决方案。

Abstract: Lay paraphrasing aims to make scientific information accessible to audiences
without technical backgrounds. However, most existing studies focus on a single
domain, such as biomedicine. With the rise of interdisciplinary research, it is
increasingly necessary to comprehend knowledge spanning multiple technical
fields. To address this, we propose Sci-LoRA, a model that leverages a mixture
of LoRAs fine-tuned on multiple scientific domains. In particular, Sci-LoRA
dynamically generates and applies weights for each LoRA, enabling it to adjust
the impact of different domains based on the input text, without requiring
explicit domain labels. To balance domain-specific knowledge and generalization
across various domains, Sci-LoRA integrates information at both the data and
model levels. This dynamic fusion enhances the adaptability and performance
across various domains. Experimental results across twelve domains on five
public datasets show that Sci-LoRA significantly outperforms state-of-the-art
large language models and demonstrates flexible generalization and adaptability
in cross-domain lay paraphrasing.

</details>


### [77] [CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions](https://arxiv.org/abs/2505.18878)
*Kung-Hsiang Huang,Akshara Prabhakar,Onkar Thorat,Divyansh Agarwal,Prafulla Kumar Choubey,Yixin Mao,Silvio Savarese,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: 论文介绍了CRMArena-Pro，一个用于评估LLM代理在多样化商业场景中表现的基准测试，发现现有代理在多轮交互和保密意识方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理在商业应用中的性能评估缺乏公开、真实的商业数据支持，现有基准测试在环境、数据和交互真实性方面不足，且覆盖场景有限。

Method: 通过扩展CRMArena，引入CRMArena-Pro基准，包含19个专家验证的任务，覆盖销售、服务和配置定价报价流程，支持B2B和B2C场景，并融入多轮交互和保密意识评估。

Result: 实验显示，领先的LLM代理在单轮任务中成功率约58%，多轮环境下降至35%。工作流执行表现较好（单轮成功率超83%），但保密意识近乎为零，针对性提示虽能改善保密意识，但常影响任务表现。

Conclusion: 当前LLM代理能力与企业需求存在显著差距，需在多轮推理、保密遵守和多技能掌握方面取得进展。

Abstract: While AI agents hold transformative potential in business, effective
performance benchmarking is hindered by the scarcity of public, realistic
business data on widely used platforms. Existing benchmarks often lack fidelity
in their environments, data, and agent-user interactions, with limited coverage
of diverse business scenarios and industries. To address these gaps, we
introduce CRMArena-Pro, a novel benchmark for holistic, realistic assessment of
LLM agents in diverse professional settings. CRMArena-Pro expands on CRMArena
with nineteen expert-validated tasks across sales, service, and 'configure,
price, and quote' processes, for both Business-to-Business and
Business-to-Customer scenarios. It distinctively incorporates multi-turn
interactions guided by diverse personas and robust confidentiality awareness
assessments. Experiments reveal leading LLM agents achieve only around 58%
single-turn success on CRMArena-Pro, with performance dropping significantly to
approximately 35% in multi-turn settings. While Workflow Execution proves more
tractable for top agents (over 83% single-turn success), other evaluated
business skills present greater challenges. Furthermore, agents exhibit
near-zero inherent confidentiality awareness; though targeted prompting can
improve this, it often compromises task performance. These findings highlight a
substantial gap between current LLM capabilities and enterprise demands,
underscoring the need for advancements in multi-turn reasoning, confidentiality
adherence, and versatile skill acquisition.

</details>


### [78] [StandUp4AI: A New Multilingual Dataset for Humor Detection in Stand-up Comedy Videos](https://arxiv.org/abs/2505.18903)
*Valentin Barriere,Nahuel Gomez,Leo Hemamou,Sofia Callejas,Brian Ravenet*

Main category: cs.CL

TL;DR: 提出一个多语言单口喜剧数据集，用于改进幽默检测计算模型，采用词级序列标注方法增强上下文理解。


<details>
  <summary>Details</summary>
Motivation: 当前幽默检测模型存在局限性，需要更大规模和多样性的数据集，并改进任务框架以更好地捕捉自然对话中的幽默机制。

Method: 构建包含7种语言、330小时的单口喜剧数据集，自动标注笑声，部分手动标注；提出词级序列标注方法，并基于语音识别错误增强笑声检测。

Result: 创建了当前最大且最多样的幽默检测数据集，验证了词级标注方法的有效性，并改进了自动笑声检测。

Conclusion: 该数据集和方法为幽默检测研究提供了新工具，未来可进一步优化多模态模型性能。

Abstract: Aiming towards improving current computational models of humor detection, we
propose a new multimodal dataset of stand-up comedies, in seven languages:
English, French, Spanish, Italian, Portuguese, Hungarian and Czech. Our dataset
of more than 330 hours, is at the time of writing the biggest available for
this type of task, and the most diverse. The whole dataset is automatically
annotated in laughter (from the audience), and the subpart left for model
validation is manually annotated. Contrary to contemporary approaches, we do
not frame the task of humor detection as a binary sequence classification, but
as word-level sequence labeling, in order to take into account all the context
of the sequence and to capture the continuous joke tagging mechanism typically
occurring in natural conversations. As par with unimodal baselines results, we
propose a method for e propose a method to enhance the automatic laughter
detection based on Audio Speech Recognition errors. Our code and data are
available online: https://tinyurl.com/EMNLPHumourStandUpPublic

</details>


### [79] [Building a Functional Machine Translation Corpus for Kpelle](https://arxiv.org/abs/2505.18905)
*Kweku Andoh Yamoah,Jackson Weako,Emmanuel J. Dorley*

Main category: cs.CL

TL;DR: 本文发布了首个公开的英语-Kpelle机器翻译数据集，通过数据增强微调NLLB模型，取得BLEU 30分，并探讨了低资源语言的技术发展路径。


<details>
  <summary>Details</summary>
Motivation: 针对低资源非洲语言Kpelle缺乏公开数据集的问题，构建首个英-Kpelle双语语料库，推动包容性语言技术发展。

Method: 收集2000+句对（日常/宗教/教育文本），采用NLLB模型进行数据增强微调，测试两个数据集版本。

Result: Kpelle-英方向BLEU达30分，与NLLB-200其他非洲语言基准表现一致，验证数据增强有效性。

Conclusion: 未来需扩展数据集，加强拼写一致性、社区验证及跨学科合作，促进曼德语族低资源语言技术发展。

Abstract: In this paper, we introduce the first publicly available English-Kpelle
dataset for machine translation, comprising over 2000 sentence pairs drawn from
everyday communication, religious texts, and educational materials. By
fine-tuning Meta's No Language Left Behind(NLLB) model on two versions of the
dataset, we achieved BLEU scores of up to 30 in the Kpelle-to-English
direction, demonstrating the benefits of data augmentation. Our findings align
with NLLB-200 benchmarks on other African languages, underscoring Kpelle's
potential for competitive performance despite its low-resource status. Beyond
machine translation, this dataset enables broader NLP tasks, including speech
recognition and language modelling. We conclude with a roadmap for future
dataset expansion, emphasizing orthographic consistency, community-driven
validation, and interdisciplinary collaboration to advance inclusive language
technology development for Kpelle and other low-resourced Mande languages.

</details>


### [80] [Federated Retrieval-Augmented Generation: A Systematic Mapping Study](https://arxiv.org/abs/2505.18906)
*Abhijit Chakraborty,Chahana Dahal,Vivek Gupta*

Main category: cs.CL

TL;DR: 本文提出了首个关于联邦检索增强生成（Federated RAG）的系统性综述，结合联邦学习与检索增强生成技术，旨在解决隐私敏感领域的自然语言处理问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在医疗、金融等隐私敏感领域的广泛应用，如何在保护数据隐私的同时提升模型的事实准确性成为关键挑战。联邦RAG结合了联邦学习的隐私保护优势与RAG的知识增强能力，为此提供了潜在解决方案。

Method: 采用基于证据的软件工程方法（Kitchenham指南），对2020-2025年间文献进行系统性梳理，构建了研究焦点、贡献类型和应用领域的结构化分类体系。

Result: 分析了联邦RAG的架构模式、时间趋势及核心挑战（如隐私保护检索、跨客户端异构性等），总结了快速演进的研究现状、重复设计模式及开放问题。

Conclusion: 该研究为联邦系统与RAG交叉领域的未来工作奠定了基础，揭示了当前技术局限与发展方向。

Abstract: Federated Retrieval-Augmented Generation (Federated RAG) combines Federated
Learning (FL), which enables distributed model training without exposing raw
data, with Retrieval-Augmented Generation (RAG), which improves the factual
accuracy of language models by grounding outputs in external knowledge. As
large language models are increasingly deployed in privacy-sensitive domains
such as healthcare, finance, and personalized assistance, Federated RAG offers
a promising framework for secure, knowledge-intensive natural language
processing (NLP). To the best of our knowledge, this paper presents the first
systematic mapping study of Federated RAG, covering literature published
between 2020 and 2025. Following Kitchenham's guidelines for evidence-based
software engineering, we develop a structured classification of research
focuses, contribution types, and application domains. We analyze architectural
patterns, temporal trends, and key challenges, including privacy-preserving
retrieval, cross-client heterogeneity, and evaluation limitations. Our findings
synthesize a rapidly evolving body of research, identify recurring design
patterns, and surface open questions, providing a foundation for future work at
the intersection of RAG and federated systems.

</details>


### [81] [SCRum-9: Multilingual Stance Classification over Rumours on Social Media](https://arxiv.org/abs/2505.18916)
*Yue Li,Jake Vasilakes,Zhixue Zhao,Carolina Scarton*

Main category: cs.CL

TL;DR: SCRum-9是一个多语言谣言立场分类数据集，包含7516条推特回复对，覆盖9种语言，链接到2100个事实核查声明，并包含复杂标注。


<details>
  <summary>Details</summary>
Motivation: 现有立场分类数据集在语言覆盖、事实核查声明链接和标注复杂性方面存在不足，SCRum-9旨在填补这一空白。

Method: 收集多语言推特回复对，链接到事实核查声明，并由至少三名母语者进行复杂标注，总计405小时标注时间和8150美元报酬。

Result: SCRum-9对当前最先进的大语言模型（如Deepseek）和微调预训练模型都具有挑战性。

Conclusion: SCRum-9是一个具有挑战性的基准数据集，推动了该领域未来的研究工作。

Abstract: We introduce SCRum-9, a multilingual dataset for Rumour Stance
Classification, containing 7,516 tweet-reply pairs from X. SCRum-9 goes beyond
existing stance classification datasets by covering more languages (9), linking
examples to more fact-checked claims (2.1k), and including complex annotations
from multiple annotators to account for intra- and inter-annotator variability.
Annotations were made by at least three native speakers per language, totalling
around 405 hours of annotation and 8,150 dollars in compensation. Experiments
on SCRum-9 show that it is a challenging benchmark for both state-of-the-art
LLMs (e.g. Deepseek) as well as fine-tuned pre-trained models, motivating
future work in this area.

</details>


### [82] [Benchmarking Large Language Models for Cyberbullying Detection in Real-World YouTube Comments](https://arxiv.org/abs/2505.18927)
*Amel Muminovic*

Main category: cs.CL

TL;DR: 研究评估了GPT-4.1、Gemini 1.5 Pro和Claude 3 Opus在识别有害YouTube评论上的表现，发现GPT-4.1综合表现最佳，但所有模型对讽刺、隐晦侮辱和多语言俚语处理欠佳。


<details>
  <summary>Details</summary>
Motivation: 在线平台评论区的骚扰行为影响用户体验，需评估大语言模型在内容审核中的有效性。

Method: 使用5080条含高攻击性内容的YouTube评论（含英语、阿拉伯语和印尼语），由两名评审标注，对比三大模型的检测效果。

Result: GPT-4.1综合F1分数最高（0.863），Gemini召回率最高（0.875）但误报多，Claude精确率最高（0.92）但召回率低（0.72）。

Conclusion: 需结合多模型优势、上下文理解及小语种优化来改进审核系统，并公开数据集促进研究。

Abstract: As online platforms grow, comment sections increasingly host harassment that
undermines user experience and well-being. This study benchmarks three leading
large language models, OpenAI GPT-4.1, Google Gemini 1.5 Pro, and Anthropic
Claude 3 Opus, on a corpus of 5,080 YouTube comments sampled from high-abuse
threads in gaming, lifestyle, food vlog, and music channels. The dataset
comprises 1,334 harmful and 3,746 non-harmful messages in English, Arabic, and
Indonesian, annotated independently by two reviewers with substantial agreement
(Cohen's kappa = 0.83). Using a unified prompt and deterministic settings,
GPT-4.1 achieved the best overall balance with an F1 score of 0.863, precision
of 0.887, and recall of 0.841. Gemini flagged the highest share of harmful
posts (recall = 0.875) but its precision fell to 0.767 due to frequent false
positives. Claude delivered the highest precision at 0.920 and the lowest
false-positive rate of 0.022, yet its recall dropped to 0.720. Qualitative
analysis showed that all three models struggle with sarcasm, coded insults, and
mixed-language slang. These results underscore the need for moderation
pipelines that combine complementary models, incorporate conversational
context, and fine-tune for under-represented languages and implicit abuse. A
de-identified version of the dataset and full prompts is publicly released to
promote reproducibility and further progress in automated content moderation.

</details>


### [83] [MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent Systems](https://arxiv.org/abs/2505.18943)
*Xuanming Zhang,Yuxuan Chen,Min-Hsuan Yeh,Yixuan Li*

Main category: cs.CL

TL;DR: MetaMind是一个受元认知心理学启发的多智能体框架，旨在提升大语言模型在社交推理中的表现，通过分解社交理解为三个阶段实现人类水平的心理理论能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在语义理解任务上表现出色，但在处理人类交流中的模糊性和上下文细微差别时存在困难，特别是在心理理论（ToM）方面。

Method: MetaMind框架包含三个协作阶段：心理理论代理生成用户心理状态假设，领域代理根据文化规范和伦理约束优化假设，响应代理生成情境适当的响应并验证与推断意图的一致性。

Result: MetaMind在三个挑战性基准测试中取得最先进性能，现实社交场景提升35.7%，ToM推理提升6.2%，首次使大语言模型在关键ToM任务上达到人类水平。

Conclusion: MetaMind通过平衡情境合理性、社交适当性和用户适应性，推动了AI系统向类人社交智能的发展，适用于共情对话和文化敏感交互。

Abstract: Human social interactions depend on the ability to infer others' unspoken
intentions, emotions, and beliefs-a cognitive skill grounded in the
psychological concept of Theory of Mind (ToM). While large language models
(LLMs) excel in semantic understanding tasks, they struggle with the ambiguity
and contextual nuance inherent in human communication. To bridge this gap, we
introduce MetaMind, a multi-agent framework inspired by psychological theories
of metacognition, designed to emulate human-like social reasoning. MetaMind
decomposes social understanding into three collaborative stages: (1) a
Theory-of-Mind Agent generates hypotheses user mental states (e.g., intent,
emotion), (2) a Domain Agent refines these hypotheses using cultural norms and
ethical constraints, and (3) a Response Agent generates contextually
appropriate responses while validating alignment with inferred intent. Our
framework achieves state-of-the-art performance across three challenging
benchmarks, with 35.7% improvement in real-world social scenarios and 6.2% gain
in ToM reasoning. Notably, it enables LLMs to match human-level performance on
key ToM tasks for the first time. Ablation studies confirm the necessity of all
components, which showcase the framework's ability to balance contextual
plausibility, social appropriateness, and user adaptation. This work advances
AI systems toward human-like social intelligence, with applications in
empathetic dialogue and culturally sensitive interactions. Code is available at
https://github.com/XMZhangAI/MetaMind.

</details>


### [84] [The Price of Format: Diversity Collapse in LLMs](https://arxiv.org/abs/2505.18949)
*Longfei Yun,Chenyang An,Zilong Wang,Letian Peng,Jingbo Shang*

Main category: cs.CL

TL;DR: 研究发现指令调优大语言模型的结构化模板会导致输出多样性崩溃，限制创造力，需设计多样性感知的提示。


<details>
  <summary>Details</summary>
Motivation: 探讨指令调优大语言模型中结构化模板对输出多样性的负面影响，特别是在开放生成任务中。

Method: 通过故事续写和自由生成等任务系统评估多样性崩溃现象，并分析不同结构化提示对模型输出的影响。

Result: 结构化模板显著限制输出空间，格式一致性对结构敏感任务重要但对知识密集型任务影响有限；最小化格式化可提升多样性。

Conclusion: 当前提示规范虽有利于对齐，但会抑制输出多样性，需在提示设计和指令调优中考虑多样性。

Abstract: Instruction-tuned large language models (LLMs) employ structured templates,
such as role markers and special tokens, to enforce format consistency during
inference. However, we identify a critical limitation of such formatting: it
induces a phenomenon we term diversity collapse, where the model generates
semantically similar outputs for open-ended inputs, undermining creativity and
variability. We systematically evaluate this effect across tasks like story
completion and free-form generation, finding that (1) diversity collapse
persists even under high-temperature sampling, and (2) structural tokens in
templates significantly constrain the model's output space. To contextualize
these findings, we fine-tune the same model using a range of structured prompts
and then evaluate them across three axes: downstream task performance,
alignment behavior, and output diversity. Our analysis shows that format
consistency between fine-tuning and inference is crucial for
structure-sensitive tasks (e.g., GSM8K, IFEval), but has marginal influence on
knowledge-heavy tasks (e.g., MMLU, WebQuestions). In contrast, output diversity
is primarily governed by the presence or absence of structural tokens, with
minimal formatting yielding the most diverse outputs. These findings reveal
that current prompting conventions, while beneficial for alignment, may
inadvertently suppress output diversity, underscoring the need for
diversity-aware prompt design and instruction tuning.

</details>


### [85] [BnMMLU: Measuring Massive Multitask Language Understanding in Bengali](https://arxiv.org/abs/2505.18951)
*Saman Sarker Joy*

Main category: cs.CL

TL;DR: 论文提出了BnMMLU基准，用于评估孟加拉语的多任务语言理解能力，填补了低资源语言在MMLU基准中的空白。


<details>
  <summary>Details</summary>
Motivation: 现有的MMLU基准主要关注高资源语言（如英语），导致低资源语言（如孟加拉语）代表性不足。本文旨在填补这一空白。

Method: 构建了覆盖23个领域的孟加拉语多选问答数据集（BnMMLU），包含138,949个问题-选项对，并标注了三种认知类别（事实知识、程序应用和推理）。

Result: 实验显示现有大语言模型在孟加拉语任务上存在显著性能差距，需改进针对孟加拉语的预训练和微调策略。

Conclusion: BnMMLU为孟加拉语语言模型评估提供了新基准，揭示了当前模型的不足，并开源数据以促进相关研究。

Abstract: The Massive Multitask Language Understanding (MMLU) benchmark has been widely
used to evaluate language models across various domains. However, existing MMLU
datasets primarily focus on high-resource languages such as English, which
leaves low-resource languages like Bengali underrepresented. In this paper, we
introduce BnMMLU, a benchmark to evaluate the multitask language understanding
capabilities of Bengali in language models. The dataset spans 23 domains,
including science, humanities, mathematics and general knowledge and is
structured in a multiple-choice format to assess factual knowledge,
application-based problem-solving and reasoning abilities of language models.
It consists of 138,949 question-option pairs. We benchmark several proprietary
and open-source large language models (LLMs) on the BnMMLU test set.
Additionally, we annotate the test set with three cognitive categories-factual
knowledge, procedural application and reasoning-to gain deeper insights into
model strengths and weaknesses across various cognitive tasks. The results
reveal significant performance gaps, highlighting the need for improved
pre-training and fine-tuning strategies tailored to Bengali data. We release
the dataset and benchmark results to facilitate further research in this area.

</details>


### [86] [Evaluating AI for Finance: Is AI Credible at Assessing Investment Risk?](https://arxiv.org/abs/2505.18953)
*Divij Chawla,Ashita Bhutada,Do Duc Anh,Abhinav Raghunathan,Vinod SP,Cathy Guo,Dar Win Liew,Prannaya Gupta,Rishabh Bhardwaj,Rajat Bhardwaj,Soujanya Poria*

Main category: cs.CL

TL;DR: 研究评估了多种AI模型在投资风险评估中的表现，发现不同模型在分数分布和人口统计敏感性上存在显著差异，强调需要标准化评估以防止偏见和不一致。


<details>
  <summary>Details</summary>
Motivation: 评估主流AI模型在投资风险评估中的可信度，特别是在不同地区和人口统计特征下的表现，以防止实际应用中的偏见和不一致。

Method: 使用1,720个用户档案（包含16个风险相关特征，覆盖10个国家和两种性别），对专有模型（如GPT-4、Claude 3.7）和开源模型（如LLaMA 3.1、DeepSeek-V3）进行分析。

Result: 不同模型在分数分布和人口统计敏感性上表现显著不同，例如GPT-4o对尼日利亚和印尼档案评分较高，而LLaMA和DeepSeek在性别分类上呈现相反趋势。

Conclusion: 研究强调在受监管的金融环境中需要对AI系统进行严格、标准化的评估，以避免实际部署中的偏见、不透明和不一致问题。

Abstract: We evaluate the credibility of leading AI models in assessing investment risk
appetite. Our analysis spans proprietary (GPT-4, Claude 3.7, Gemini 1.5) and
open-weight models (LLaMA 3.1/3.3, DeepSeek-V3, Mistral-small), using 1,720
user profiles constructed with 16 risk-relevant features across 10 countries
and both genders. We observe significant variance across models in score
distributions and demographic sensitivity. For example, GPT-4o assigns higher
risk scores to Nigerian and Indonesian profiles, while LLaMA and DeepSeek show
opposite gender tendencies in risk classification. While some models (e.g.,
GPT-4o, LLaMA 3.1) align closely with expected scores in low- and mid-risk
ranges, none maintain consistent performance across regions and demographics.
Our findings highlight the need for rigorous, standardized evaluations of AI
systems in regulated financial contexts to prevent bias, opacity, and
inconsistency in real-world deployment.

</details>


### [87] [System-1.5 Reasoning: Traversal in Language and Latent Spaces with Dynamic Shortcuts](https://arxiv.org/abs/2505.18962)
*Xiaoqiang Wang,Suyuchen Wang,Yun Zhu,Bang Liu*

Main category: cs.CL

TL;DR: 本文提出System-1.5 Reasoning框架，通过动态分配计算资源优化推理效率，在保持性能的同时显著提升速度并减少token生成。


<details>
  <summary>Details</summary>
Motivation: 现有链式思维(CoT)推理方法因生成冗长中间输出而效率低下，而潜在空间推理方法虽提升效率但未区分关键步骤与非关键步骤，导致计算资源利用不足。

Method: System-1.5 Reasoning框架引入两种动态捷径：模型深度捷径(DS)通过轻量适配器分支垂直动态调整推理深度；步骤捷径(SS)在潜在空间中水平重用隐藏状态跳过次要步骤。采用两阶段自蒸馏训练策略。

Result: 在GSM8K等推理任务中，System-1.5推理性能媲美传统CoT微调方法，同时推理速度提升20倍以上，平均减少92.31%的token生成。

Conclusion: 该框架通过自适应计算分配实现了高效精确的推理，为平衡大语言模型推理效率与性能提供了新思路。

Abstract: Chain-of-thought (CoT) reasoning enables large language models (LLMs) to move
beyond fast System-1 responses and engage in deliberative System-2 reasoning.
However, this comes at the cost of significant inefficiency due to verbose
intermediate output. Recent latent-space reasoning methods improve efficiency
by operating on hidden states without decoding into language, yet they treat
all steps uniformly, failing to distinguish critical deductions from auxiliary
steps and resulting in suboptimal use of computational resources. In this
paper, we propose System-1.5 Reasoning, an adaptive reasoning framework that
dynamically allocates computation across reasoning steps through shortcut paths
in latent space.Specifically, System-1.5 Reasoning introduces two types of
dynamic shortcuts. The model depth shortcut (DS) adaptively reasons along the
vertical depth by early exiting non-critical tokens through lightweight adapter
branches, while allowing critical tokens to continue through deeper Transformer
layers. The step shortcut (SS) reuses hidden states across the decoding steps
to skip trivial steps and reason horizontally in latent space. Training
System-1.5 Reasoning involves a two-stage self-distillation process: first
distilling natural language CoT into latent-space continuous thought, and then
distilling full-path System-2 latent reasoning into adaptive shortcut paths
(System-1.5 Reasoning).Experiments on reasoning tasks demonstrate the superior
performance of our method. For example, on GSM8K, System-1.5 Reasoning achieves
reasoning performance comparable to traditional CoT fine-tuning methods while
accelerating inference by over 20x and reducing token generation by 92.31% on
average.

</details>


### [88] [Learning to Explain: Prototype-Based Surrogate Models for LLM Classification](https://arxiv.org/abs/2505.18970)
*Bowen Wei,Ziwei Zhu*

Main category: cs.CL

TL;DR: 论文提出ProtoSurE框架，通过原型构建可解释的替代模型，为LLM提供忠实且易于理解的解释。


<details>
  <summary>Details</summary>
Motivation: 现有解释方法要么与模型推理过程不一致，要么解释难以被人类理解，需要一种更优方案。

Method: 训练基于句子原型的可解释替代模型，使其与目标LLM保持一致。

Result: ProtoSurE在多种LLM和数据集上优于现有方法，且数据效率高，适用实际场景。

Conclusion: ProtoSurE有效解决了LLM解释的忠实性和可理解性问题，具有实用价值。

Abstract: Large language models (LLMs) have demonstrated impressive performance on
natural language tasks, but their decision-making processes remain largely
opaque. Existing explanation methods either suffer from limited faithfulness to
the model's reasoning or produce explanations that humans find difficult to
understand. To address these challenges, we propose \textbf{ProtoSurE}, a novel
prototype-based surrogate framework that provides faithful and
human-understandable explanations for LLMs. ProtoSurE trains an
interpretable-by-design surrogate model that aligns with the target LLM while
utilizing sentence-level prototypes as human-understandable concepts. Extensive
experiments show that ProtoSurE consistently outperforms SOTA explanation
methods across diverse LLMs and datasets. Importantly, ProtoSurE demonstrates
strong data efficiency, requiring relatively few training examples to achieve
good performance, making it practical for real-world applications.

</details>


### [89] [Is Architectural Complexity Overrated? Competitive and Interpretable Knowledge Graph Completion with RelatE](https://arxiv.org/abs/2505.18971)
*Abhijit Chakraborty,Chahana Dahal,Ashutosh Balasubramaniam,Tejas Anvekar,Vivek Gupta*

Main category: cs.CL

TL;DR: 论文提出了一种名为RelatE的简单、可解释的知识图谱补全方法，通过实值相位-模分解有效整合实体和关系的双重表示，在性能和效率上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于复数嵌入或深度神经架构的知识图谱补全方法复杂且效率低，作者旨在提出一种简单、高效且性能优越的替代方案。

Method: RelatE采用实值相位-模分解，利用正弦相位对齐编码对称、反转和组合等关系模式，保持架构简单的同时提升性能。

Result: 在YAGO3-10等数据集上，RelatE的MRR和Hit@10指标均超越基线方法，训练时间减少24%，推理延迟降低31%，GPU内存占用减少22%，且鲁棒性显著提升。

Conclusion: RelatE作为一种可扩展且可解释的方法，为知识图谱补全提供了优于复杂架构的解决方案。

Abstract: We revisit the efficacy of simple, real-valued embedding models for knowledge
graph completion and introduce RelatE, an interpretable and modular method that
efficiently integrates dual representations for entities and relations. RelatE
employs a real-valued phase-modulus decomposition, leveraging sinusoidal phase
alignments to encode relational patterns such as symmetry, inversion, and
composition. In contrast to recent approaches based on complex-valued
embeddings or deep neural architectures, RelatE preserves architectural
simplicity while achieving competitive or superior performance on standard
benchmarks. Empirically, RelatE outperforms prior methods across several
datasets: on YAGO3-10, it achieves an MRR of 0.521 and Hit@10 of 0.680,
surpassing all baselines. Additionally, RelatE offers significant efficiency
gains, reducing training time by 24%, inference latency by 31%, and peak GPU
memory usage by 22% compared to RotatE. Perturbation studies demonstrate
improved robustness, with MRR degradation reduced by up to 61% relative to
TransE and by up to 19% compared to RotatE under structural edits such as edge
removals and relation swaps. Formal analysis further establishes the model's
full expressiveness and its capacity to represent essential first-order logical
inference patterns. These results position RelatE as a scalable and
interpretable alternative to more complex architectures for knowledge graph
completion.

</details>


### [90] [Hierarchical Mamba Meets Hyperbolic Geometry: A New Paradigm for Structured Language Embeddings](https://arxiv.org/abs/2505.18973)
*Sarang Patil,Ashish Parmanand Pandey,Ioannis Koutis,Mengjia Xu*

Main category: cs.CL

TL;DR: 论文提出Hierarchical Mamba (HiM)模型，结合Mamba2与双曲几何，改进语言模型对层次结构的捕捉能力，在混合跳预测和多跳推理任务中表现优于欧几里得基线。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型主要依赖平坦的欧几里得嵌入，难以捕捉潜在的层次结构，限制了复杂层次推理任务的表现。

Method: 提出HiM模型，将Mamba2处理后的序列通过可学习曲率的正切映射（Poincare球）或余弦/正弦映射（Lorentz流形）投影到双曲空间，并采用混合双曲损失优化。

Result: 实验表明：1) HiM在四个本体数据集上均能有效捕捉层次关系；2) HiM-Poincare擅长细粒度语义区分，HiM-Lorentz则提供更稳定紧凑的层次化嵌入。

Conclusion: HiM通过双曲几何增强了语言模型的层次表示能力，特别适用于需要长程层次推理的任务，为语言理解提供了新方向。

Abstract: Selective state-space models have achieved great success in long-sequence
modeling. However, their capacity for language representation, especially in
complex hierarchical reasoning tasks, remains underexplored. Most large
language models rely on flat Euclidean embeddings, limiting their ability to
capture latent hierarchies. To address this limitation, we propose Hierarchical
Mamba (HiM), integrating efficient Mamba2 with exponential growth and curved
nature of hyperbolic geometry to learn hierarchy-aware language embeddings for
deeper linguistic understanding. Mamba2-processed sequences are projected to
the Poincare ball (via tangent-based mapping) or Lorentzian manifold (via
cosine and sine-based mapping) with "learnable" curvature, optimized with a
combined hyperbolic loss. Our HiM model facilitates the capture of relational
distances across varying hierarchical levels, enabling effective long-range
reasoning. This makes it well-suited for tasks like mixed-hop prediction and
multi-hop inference in hierarchical classification. We evaluated our HiM with
four linguistic and medical datasets for mixed-hop prediction and multi-hop
inference tasks. Experimental results demonstrated that: 1) Both HiM models
effectively capture hierarchical relationships for four ontological datasets,
surpassing Euclidean baselines. 2) HiM-Poincare captures fine-grained semantic
distinctions with higher h-norms, while HiM-Lorentz provides more stable,
compact, and hierarchy-preserving embeddings favoring robustness over detail.

</details>


### [91] [AI4Math: A Native Spanish Benchmark for University-Level Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2505.18978)
*Miguel Angel Peñaloza Perez,Bruno Lopez Orozco,Jesus Tadeo Cruz Soto,Michelle Bruno Hernandez,Miguel Angel Alvarado Gonzalez,Sandra Malagon*

Main category: cs.CL

TL;DR: AI4Math是一个西班牙语原生大学数学问题基准，评估多个大语言模型在数学推理上的表现，结果显示模型在几何、组合和概率问题上表现较差，且需要原生语言基准来揭示标准指标未捕捉的推理缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理基准多为英文或基于翻译，可能导致语义漂移和掩盖语言特有的推理错误。为此，作者提出了AI4Math，一个包含105个西班牙语原生大学数学问题的基准，以填补这一空白。

Method: 构建了AI4Math基准，包含105个西班牙语原生数学问题，覆盖七个高级数学领域。评估了六个大语言模型（如GPT-4o、LLaMA 3.3 70B等）在零样本和思维链两种配置下的表现，分别用西班牙语和英语进行测试。

Result: 表现最佳的模型（o3 mini、DeepSeek R1 685B、DeepSeek V3 685B）准确率超过70%，而LLaMA 3.3 70B和GPT-4o mini低于40%。大多数模型在语言切换时性能无显著下降，几何、组合和概率问题对所有模型均具挑战性。

Conclusion: 研究表明，原生语言基准和领域特定评估对于揭示标准指标未捕捉的推理缺陷至关重要，几何、组合和概率问题仍是当前模型的难点。

Abstract: Existing mathematical reasoning benchmarks are predominantly English only or
translation-based, which can introduce semantic drift and mask languagespecific
reasoning errors. To address this, we present AI4Math, a benchmark of 105
original university level math problems natively authored in Spanish. The
dataset spans seven advanced domains (Algebra, Calculus, Geometry, Probability,
Number Theory, Combinatorics, and Logic), and each problem is accompanied by a
step by step human solution. We evaluate six large language models GPT 4o, GPT
4o mini, o3 mini, LLaMA 3.3 70B, DeepSeek R1 685B, and DeepSeek V3 685B under
four configurations: zero shot and chain of thought, each in Spanish and
English. The top models (o3 mini, DeepSeek R1 685B, DeepSeek V3 685B) achieve
over 70% accuracy, whereas LLaMA 3.3 70B and GPT-4o mini remain below 40%. Most
models show no significant performance drop between languages, with GPT 4o even
performing better on Spanish problems in the zero shot setting. Geometry,
Combinatorics, and Probability questions remain persistently challenging for
all models. These results highlight the need for native-language benchmarks and
domain-specific evaluations to reveal reasoning failures not captured by
standard metrics.

</details>


### [92] [FiLLM -- A Filipino-optimized Large Language Model based on Southeast Asia Large Language Model (SEALLM)](https://arxiv.org/abs/2505.18995)
*Carlos Jude G. Maminta,Isaiah Job Enriquez,Deandre Nigel Nunez,Michael B. Dela Fuente*

Main category: cs.CL

TL;DR: FiLLM是一个针对菲律宾语优化的语言模型，基于SeaLLM-7B 2.5，通过LoRA微调提升效率，但在多项任务中表现不及CalamanCy。


<details>
  <summary>Details</summary>
Motivation: 提升菲律宾语的自然语言处理能力，满足本地语言需求。

Method: 基于SeaLLM-7B 2.5模型，使用LoRA微调优化内存效率，并在多种菲律宾语数据集上进行训练和评估。

Result: CalamanCy在多项任务中表现优于FiLLM，显示出更好的语言理解和适应性。

Conclusion: FiLLM为菲律宾语NLP应用提供了优化、高效且可扩展的模型，但仍有改进空间。

Abstract: This study presents FiLLM, a Filipino-optimized large language model,
designed to enhance natural language processing (NLP) capabilities in the
Filipino language. Built upon the SeaLLM-7B 2.5 model, FiLLM leverages Low-Rank
Adaptation (LoRA) fine-tuning to optimize memory efficiency while maintaining
task-specific performance. The model was trained and evaluated on diverse
Filipino datasets to address key NLP tasks, including Named Entity Recognition
(NER), Part-of-Speech (POS) tagging, Dependency Parsing, and Text
Summarization. Performance comparisons with the CalamanCy model were conducted
using F1 Score, Precision, Recall, Compression Rate, and Keyword Overlap
metrics. Results indicate that Calamancy outperforms FILLM in several aspects,
demonstrating its effectiveness in processing Filipino text with improved
linguistic comprehension and adaptability. This research contributes to the
advancement of Filipino NLP applications by providing an optimized, efficient,
and scalable language model tailored for local linguistic needs.

</details>


### [93] [VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Gudied Iterative Policy Optimization](https://arxiv.org/abs/2505.19000)
*Yunxin Li,Xinyu Chen,Zitao Li,Zhenyu Liu,Longyue Wang,Wenhan Luo,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: 论文提出VerIPO方法，通过验证器引导的迭代策略优化提升视频大语言模型的深度推理能力，解决了现有强化微调方法在数据质量和长链推理上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于结果的强化微调方法（如GRPO）存在数据准备瓶颈（噪声高/成本高）和长链推理质量不稳定的问题，限制了视频大语言模型的复杂推理能力发展。

Method: 提出GRPO-Verifier-DPO三阶段训练循环：1) GRPO阶段广泛搜索；2) 新增Rollout-Aware验证器（利用小模型评估推理逻辑）构建高质量对比数据；3) DPO阶段针对性优化（速度比GRPO快7倍）。

Result: 实验表明：1) 优化效率显著高于GRPO变体；2) 生成的长推理链质量超越直接调优的大规模视频LLM；3) 单次迭代模型性能优于Kimi-VL等先进模型。

Conclusion: VerIPO通过验证器引导的迭代优化机制，实现了视频大语言模型在长链推理任务中质量与稳定性的突破，为复杂视频推理提供了新范式。

Abstract: Applying Reinforcement Learning (RL) to Video Large Language Models
(Video-LLMs) shows significant promise for complex video reasoning. However,
popular Reinforcement Fine-Tuning (RFT) methods, such as outcome-based Group
Relative Policy Optimization (GRPO), are limited by data preparation
bottlenecks (e.g., noise or high cost) and exhibit unstable improvements in the
quality of long chain-of-thoughts (CoTs) and downstream performance.To address
these limitations, we propose VerIPO, a Verifier-guided Iterative Policy
Optimization method designed to gradually improve video LLMs' capacity for
generating deep, long-term reasoning chains. The core component is
Rollout-Aware Verifier, positioned between the GRPO and Direct Preference
Optimization (DPO) training phases to form the GRPO-Verifier-DPO training loop.
This verifier leverages small LLMs as a judge to assess the reasoning logic of
rollouts, enabling the construction of high-quality contrastive data, including
reflective and contextually consistent CoTs. These curated preference samples
drive the efficient DPO stage (7x faster than GRPO), leading to marked
improvements in reasoning chain quality, especially in terms of length and
contextual consistency. This training loop benefits from GRPO's expansive
search and DPO's targeted optimization. Experimental results demonstrate: 1)
Significantly faster and more effective optimization compared to standard GRPO
variants, yielding superior performance; 2) Our trained models exceed the
direct inference of large-scale instruction-tuned Video-LLMs, producing long
and contextually consistent CoTs on diverse video reasoning tasks; and 3) Our
model with one iteration outperforms powerful LMMs (e.g., Kimi-VL) and long
reasoning models (e.g., Video-R1), highlighting its effectiveness and
stability.

</details>


### [94] [CrosGrpsABS: Cross-Attention over Syntactic and Semantic Graphs for Aspect-Based Sentiment Analysis in a Low-Resource Language](https://arxiv.org/abs/2505.19018)
*Md. Mithun Hossain,Md. Shakil Hossain,Sudipto Chaki,Md. Rajib Hossain,Md. Saifur Rahman,A. B. M. Shawkat Ali*

Main category: cs.CL

TL;DR: 该论文提出了一种名为CrosGrpsABS的新型混合框架，用于提升低资源语言（如孟加拉语）的方面级情感分析性能，通过结合语法和语义图的双向交叉注意力机制，显著提高了分类效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中于资源丰富的语言（如英语），而低资源语言（如孟加拉语）由于缺乏标注数据、预训练模型和优化超参数，其方面级情感分析研究较为滞后。

Method: CrosGrpsABS框架结合了基于Transformer的上下文嵌入和图卷积网络，利用基于规则的语法依赖解析和语义相似度计算，通过双向交叉注意力机制融合局部语法结构和全局语义上下文。

Result: 在四个低资源孟加拉语数据集和高资源英语SemEval 2014 Task 4数据集上，CrosGrpsABS均优于现有方法，F1分数分别提升了0.93%（餐厅领域）和1.06%（笔记本电脑领域）。

Conclusion: CrosGrpsABS框架通过有效融合语法和语义信息，显著提升了低资源和高资源语言的情感分析性能，为低资源语言的NLP研究提供了新思路。

Abstract: Aspect-Based Sentiment Analysis (ABSA) is a fundamental task in natural
language processing, offering fine-grained insights into opinions expressed in
text. While existing research has largely focused on resource-rich languages
like English which leveraging large annotated datasets, pre-trained models, and
language-specific tools. These resources are often unavailable for low-resource
languages such as Bengali. The ABSA task in Bengali remains poorly explored and
is further complicated by its unique linguistic characteristics and a lack of
annotated data, pre-trained models, and optimized hyperparameters. To address
these challenges, this research propose CrosGrpsABS, a novel hybrid framework
that leverages bidirectional cross-attention between syntactic and semantic
graphs to enhance aspect-level sentiment classification. The CrosGrpsABS
combines transformerbased contextual embeddings with graph convolutional
networks, built upon rule-based syntactic dependency parsing and semantic
similarity computations. By employing bidirectional crossattention, the model
effectively fuses local syntactic structure with global semantic context,
resulting in improved sentiment classification performance across both low- and
high-resource settings. We evaluate CrosGrpsABS on four low-resource Bengali
ABSA datasets and the high-resource English SemEval 2014 Task 4 dataset. The
CrosGrpsABS consistently outperforms existing approaches, achieving notable
improvements, including a 0.93% F1-score increase for the Restaurant domain and
a 1.06% gain for the Laptop domain in the SemEval 2014 Task 4 benchmark.

</details>


### [95] [Efficient Data Selection at Scale via Influence Distillation](https://arxiv.org/abs/2505.19051)
*Mahdi Nikdan,Vincent Cohen-Addad,Dan Alistarh,Vahab Mirrokni*

Main category: cs.CL

TL;DR: 本文提出了一种名为Influence Distillation的新方法，通过二阶信息优化训练样本权重，提升大语言模型微调效率，实验证明其性能优越且选择速度更快。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型训练中，高效的数据选择对性能至关重要。传统方法缺乏数学依据，无法针对目标分布优化样本权重。

Method: 提出基于二阶信息的Influence Distillation框架，通过地标样本近似计算所有样本的影响权重，支持梯度下降和Adam优化器。

Result: 在Tulu V2数据集上的指令微调实验表明，该方法在GSM8k等任务上达到或超越SOTA性能，同时实现3.5倍的选择加速。

Conclusion: Influence Distillation为数据选择提供了理论保证的高效方案，显著提升大语言模型微调效果与效率。

Abstract: Effective data selection is critical for efficient training of modern Large
Language Models (LLMs). This paper introduces Influence Distillation, a novel,
mathematically-justified framework for data selection that employs second-order
information to optimally weight training samples. By distilling each sample's
influence on a target distribution, our method assigns model-specific weights
that are used to select training data for LLM fine-tuning, guiding it toward
strong performance on the target domain. We derive these optimal weights for
both Gradient Descent and Adam optimizers. To ensure scalability and reduce
computational cost, we propose a $\textit{landmark-based approximation}$:
influence is precisely computed for a small subset of "landmark" samples and
then efficiently propagated to all other samples to determine their weights. We
validate Influence Distillation by applying it to instruction tuning on the
Tulu V2 dataset, targeting a range of tasks including GSM8k, SQuAD, and MMLU,
across several models from the Llama and Qwen families. Experiments show that
Influence Distillation matches or outperforms state-of-the-art performance
while achieving up to $3.5\times$ faster selection.

</details>


### [96] [An Embarrassingly Simple Defense Against LLM Abliteration Attacks](https://arxiv.org/abs/2505.19056)
*Harethah Abu Shairah,Hasan Abed Al Kader Hammoud,Bernard Ghanem,George Turkiyyah*

Main category: cs.CL

TL;DR: 论文提出了一种防御方法，通过扩展拒绝数据集微调模型，有效抵御abliteration攻击，保持模型的安全性和通用性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）通常通过拒绝有害指令来遵守安全准则，但abliteration攻击能抑制模型的拒绝行为，导致生成不道德内容。论文旨在解决这一问题。

Method: 构建包含有害提示和完整拒绝理由的扩展拒绝数据集，并对Llama-2-7B-Chat和Qwen2.5-Instruct模型进行微调。

Result: 扩展拒绝微调的模型在abliteration攻击下拒绝率仅下降10%，而基线模型下降70-80%，同时保持了通用性能。

Conclusion: 扩展拒绝微调能有效抵御abliteration攻击，保持模型的安全性和实用性。

Abstract: Large language models (LLMs) are typically aligned to comply with safety
guidelines by refusing harmful instructions. A recent attack, termed
abliteration, isolates and suppresses the single latent direction most
responsible for refusal behavior, enabling the model to generate unethical
content. We propose a defense that modifies how models generate refusals. We
construct an extended-refusal dataset that contains harmful prompts with a full
response that justifies the reason for refusal. We then fine-tune
Llama-2-7B-Chat and Qwen2.5-Instruct (1.5B and 3B parameters) on our
extended-refusal dataset, and evaluate the resulting systems on a set of
harmful prompts. In our experiments, extended-refusal models maintain high
refusal rates, dropping at most by 10%, whereas baseline models' refusal rates
drop by 70-80% after abliteration. A broad evaluation of safety and utility
shows that extended-refusal fine-tuning neutralizes the abliteration attack
while preserving general performance.

</details>


### [97] [UNCERTAINTY-LINE: Length-Invariant Estimation of Uncertainty for Large Language Models](https://arxiv.org/abs/2505.19060)
*Roman Vashurin,Maiya Goloburda,Preslav Nakov,Maxim Panov*

Main category: cs.CL

TL;DR: 论文提出了一种名为UNCERTAINTY-LINE的简单去偏方法，用于消除大语言模型输出长度对不确定性量化的影响。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在各领域的广泛应用，确保其输出的质量和可信度变得尤为重要。现有不确定性量化（UQ）方法常依赖词元概率，但会因输出长度引入偏差，即使经过长度归一化处理，这种偏差依然存在。

Method: UNCERTAINTY-LINE通过回归不确定性分数与输出长度的关系，利用残差作为修正后的长度不变估计量。该方法为后处理、模型无关，适用于多种UQ指标。

Result: 在机器翻译、摘要生成和问答任务上的广泛实验表明，UNCERTAINTY-LINE在多种指标和模型上均优于名义上的长度归一化UQ方法。

Conclusion: UNCERTAINTY-LINE有效解决了输出长度对UQ的偏差问题，显著提升了不确定性估计的准确性。

Abstract: Large Language Models (LLMs) have become indispensable tools across various
applications, making it more important than ever to ensure the quality and the
trustworthiness of their outputs. This has led to growing interest in
uncertainty quantification (UQ) methods for assessing the reliability of LLM
outputs. Many existing UQ techniques rely on token probabilities, which
inadvertently introduces a bias with respect to the length of the output. While
some methods attempt to account for this, we demonstrate that such biases
persist even in length-normalized approaches. To address the problem, here we
propose UNCERTAINTY-LINE: (Length-INvariant Estimation), a simple debiasing
procedure that regresses uncertainty scores on output length and uses the
residuals as corrected, length-invariant estimates. Our method is post-hoc,
model-agnostic, and applicable to a range of UQ measures. Through extensive
evaluation on machine translation, summarization, and question-answering tasks,
we demonstrate that UNCERTAINTY-LINE: consistently improves over even nominally
length-normalized UQ methods uncertainty estimates across multiple metrics and
models.

</details>


### [98] [Towards Harmonized Uncertainty Estimation for Large Language Models](https://arxiv.org/abs/2505.19073)
*Rui Li,Jing Long,Muge Qi,Heming Xia,Lei Sha,Peiyi Wang,Zhifang Sui*

Main category: cs.CL

TL;DR: 本文提出CUE方法，通过轻量级模型调整大语言模型的不确定性分数，显著提升估计准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的不确定性估计方法在指示性、平衡性和校准性方面存在不足，限制了其广泛应用。

Method: 提出CUE方法，使用与目标模型性能对齐的数据训练轻量级模型来调整不确定性分数。

Result: 实验表明，CUE方法比现有方法提升高达60%的估计准确性。

Conclusion: CUE方法简单有效，能显著提升大语言模型不确定性估计的可靠性。

Abstract: To facilitate robust and trustworthy deployment of large language models
(LLMs), it is essential to quantify the reliability of their generations
through uncertainty estimation. While recent efforts have made significant
advancements by leveraging the internal logic and linguistic features of LLMs
to estimate uncertainty scores, our empirical analysis highlights the pitfalls
of these methods to strike a harmonized estimation between indication, balance,
and calibration, which hinders their broader capability for accurate
uncertainty estimation. To address this challenge, we propose CUE (Corrector
for Uncertainty Estimation): A straightforward yet effective method that
employs a lightweight model trained on data aligned with the target LLM's
performance to adjust uncertainty scores. Comprehensive experiments across
diverse models and tasks demonstrate its effectiveness, which achieves
consistent improvements of up to 60% over existing methods.

</details>


### [99] [ReadBench: Measuring the Dense Text Visual Reading Ability of Vision-Language Models](https://arxiv.org/abs/2505.19091)
*Benjamin Clavié,Florian Brand*

Main category: cs.CL

TL;DR: 论文介绍了ReadBench，一个评估大视觉语言模型（VLMs）在文本丰富图像上阅读和理解能力的多模态基准测试。


<details>
  <summary>Details</summary>
Motivation: 尽管大视觉语言模型在视觉理解方面取得了进展，但其在文本丰富图像上的阅读和推理能力尚未得到充分评估。

Method: 通过将纯文本基准测试的上下文转换为文本图像，同时保留文本提示和问题，创建了ReadBench基准测试。

Result: 测试发现，VLMs在短文本图像上性能略有下降，而在长文本、多页上下文上性能显著下降；文本分辨率对多模态性能影响微乎其微。

Conclusion: 研究强调了VLMs在处理视觉呈现的广泛文本内容方面的改进需求，这对实际应用至关重要。

Abstract: Recent advancements in Large Vision-Language Models (VLMs), have greatly
enhanced their capability to jointly process text and images. However, despite
extensive benchmarks evaluating visual comprehension (e.g., diagrams, color
schemes, OCR tasks...), there is limited assessment of VLMs' ability to read
and reason about text-rich images effectively. To fill this gap, we introduce
ReadBench, a multimodal benchmark specifically designed to evaluate the reading
comprehension capabilities of VLMs. ReadBench transposes contexts from
established text-only benchmarks into images of text while keeping textual
prompts and questions intact. Evaluating leading VLMs with ReadBench, we find
minimal-but-present performance degradation on short, text-image inputs, while
performance sharply declines for longer, multi-page contexts. Our experiments
further reveal that text resolution has negligible effects on multimodal
performance. These findings highlight needed improvements in VLMs, particularly
their reasoning over visually presented extensive textual content, a capability
critical for practical applications. ReadBench is available at
https://github.com/answerdotai/ReadBench .

</details>


### [100] [ASPO: Adaptive Sentence-Level Preference Optimization for Fine-Grained Multimodal Reasoning](https://arxiv.org/abs/2505.19100)
*Yeyuan Wang,Dehong Gao,Rujiao Long,Lei Yi,Linbo Jin,Libin Yang,Xiaoyan Cai*

Main category: cs.CL

TL;DR: ASPO提出了一种基于句子级别的自适应偏好优化方法，以解决传统DPO在细粒度监督上的不足，显著提升了多模态模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统DPO方法依赖于二元的偏好优化，缺乏对细粒度段落正确性的考虑，导致优化结果不理想。

Method: ASPO通过动态计算句子级别的自适应奖励，实现了更精确的偏好优化，无需额外模型或参数。

Result: 实验表明，ASPO显著提升了多模态模型的整体性能和对齐效果。

Conclusion: ASPO通过细粒度的句子级别优化，有效改进了多模态模型的偏好对齐和性能。

Abstract: Direct Preference Optimization (DPO) has gained significant attention for its
simplicity and computational efficiency in aligning large language models
(LLMs). Recent advancements have extended DPO to multimodal scenarios,
achieving strong performance. However, traditional DPO relies on binary
preference optimization, rewarding or penalizing entire responses without
considering fine-grained segment correctness, leading to suboptimal solutions.
The root of this issue lies in the absence of fine-grained supervision during
the optimization process. To address this, we propose Adaptive Sentence-level
Preference Optimization (ASPO), which evaluates individual sentences for more
precise preference optimization. By dynamically calculating adaptive rewards at
the sentence level based on model predictions, ASPO enhances response content
assessment without additional models or parameters. This significantly improves
the alignment of multimodal features. Extensive experiments show that ASPO
substantially enhances the overall performance of multimodal models.

</details>


### [101] [WHISTRESS: Enriching Transcriptions with Sentence Stress Detection](https://arxiv.org/abs/2505.19103)
*Iddo Yosha,Dorin Shteyman,Yossi Adi*

Main category: cs.CL

TL;DR: WHISTRESS是一种无需对齐的方法，用于增强转录系统的句子重音检测能力，通过在合成数据TINYSTRESS-15K上训练，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 口语通过语调、情感和重音传达意义，句子重音对理解说话者意图至关重要。现有转录系统缺乏有效的重音检测能力，因此需要开发新方法。

Method: 提出WHISTRESS方法，利用合成的TINYSTRESS-15K数据集进行训练，无需额外输入先验，实现对齐自由的句子重音检测。

Result: WHISTRESS在多个基准测试中表现优于现有方法，并展现出强大的零样本泛化能力，尽管训练数据为合成数据。

Conclusion: WHISTRESS为句子重音检测提供了一种高效且无需对齐的解决方案，合成数据训练的有效性为其广泛应用奠定了基础。

Abstract: Spoken language conveys meaning not only through words but also through
intonation, emotion, and emphasis. Sentence stress, the emphasis placed on
specific words within a sentence, is crucial for conveying speaker intent and
has been extensively studied in linguistics. In this work, we introduce
WHISTRESS, an alignment-free approach for enhancing transcription systems with
sentence stress detection. To support this task, we propose TINYSTRESS-15K, a
scalable, synthetic training data for the task of sentence stress detection
which resulted from a fully automated dataset creation process. We train
WHISTRESS on TINYSTRESS-15K and evaluate it against several competitive
baselines. Our results show that WHISTRESS outperforms existing methods while
requiring no additional input priors during training or inference. Notably,
despite being trained on synthetic data, WHISTRESS demonstrates strong
zero-shot generalization across diverse benchmarks. Project page:
https://pages.cs.huji.ac.il/adiyoss-lab/whistress.

</details>


### [102] [CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models](https://arxiv.org/abs/2505.19108)
*Yongheng Zhang,Xu Liu,Ruoxi Zhou,Qiguang Chen,Hao Fei,Wenpeng Lu,Libo Qin*

Main category: cs.CL

TL;DR: 该论文提出了一个联合跨语言和跨模态的幻觉基准CCHall，用于评估大语言模型在这两种场景下的表现，并发现现有模型在此基准上仍有困难。


<details>
  <summary>Details</summary>
Motivation: 当前研究仅限于单一跨语言或跨模态场景，缺乏对联合跨语言和跨模态场景下幻觉问题的探索，因此需要填补这一空白。

Method: 引入了一个新的联合跨语言和跨模态幻觉基准CCHall，并对其进行了主流开源和闭源大语言模型的全面评估。

Result: 实验结果表明，当前的大语言模型在CCHall基准上仍然表现不佳。

Conclusion: CCHall可作为评估大语言模型在联合跨语言和跨模态场景下的宝贵资源。

Abstract: Investigating hallucination issues in large language models (LLMs) within
cross-lingual and cross-modal scenarios can greatly advance the large-scale
deployment in real-world applications. Nevertheless, the current studies are
limited to a single scenario, either cross-lingual or cross-modal, leaving a
gap in the exploration of hallucinations in the joint cross-lingual and
cross-modal scenarios. Motivated by this, we introduce a novel joint
Cross-lingual and Cross-modal Hallucinations benchmark (CCHall) to fill this
gap. Specifically, CCHall simultaneously incorporates both cross-lingual and
cross-modal hallucination scenarios, which can be used to assess the
cross-lingual and cross-modal capabilities of LLMs. Furthermore, we conduct a
comprehensive evaluation on CCHall, exploring both mainstream open-source and
closed-source LLMs. The experimental results highlight that current LLMs still
struggle with CCHall. We hope CCHall can serve as a valuable resource to assess
LLMs in joint cross-lingual and cross-modal scenarios.

</details>


### [103] [Self-Critique Guided Iterative Reasoning for Multi-hop Question Answering](https://arxiv.org/abs/2505.19112)
*Zheng Chu,Huiming Fan,Jingchang Chen,Qianyu Wang,Mingda Yang,Jiafeng Liang,Zhongjie Wang,Hao Li,Guo Tang,Ming Liu,Bing Qin*

Main category: cs.CL

TL;DR: 论文提出了一种名为SiGIR的自我批判引导迭代推理方法，通过自我评估优化多跳推理过程，在三个数据集上性能超越之前最佳方法8.6%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在知识密集型多跳推理任务中仍面临中间推理步骤缺乏指导、检索不准确等问题，导致最终推理错误。

Method: 提出SiGIR框架：1) 通过端到端训练实现问题分解式迭代推理 2) 引入自我批判机制评估中间步骤 3) 在分支探索中动态选择最优推理路径。

Result: 在三个多跳推理数据集上实验表明，SiGIR比之前SOTA方法绝对性能提升8.6%，并提供了可解释的推理过程分析。

Conclusion: 自我批判引导的迭代推理能有效提升复杂推理任务性能，该方法为未来研究提供了新的技术方向和洞察。

Abstract: Although large language models (LLMs) have demonstrated remarkable reasoning
capabilities, they still face challenges in knowledge-intensive multi-hop
reasoning. Recent work explores iterative retrieval to address complex
problems. However, the lack of intermediate guidance often results in
inaccurate retrieval and flawed intermediate reasoning, leading to incorrect
reasoning. To address these, we propose Self-Critique Guided Iterative
Reasoning (SiGIR), which uses self-critique feedback to guide the iterative
reasoning process. Specifically, through end-to-end training, we enable the
model to iteratively address complex problems via question decomposition.
Additionally, the model is able to self-evaluate its intermediate reasoning
steps. During iterative reasoning, the model engages in branching exploration
and employs self-evaluation to guide the selection of promising reasoning
trajectories. Extensive experiments on three multi-hop reasoning datasets
demonstrate the effectiveness of our proposed method, surpassing the previous
SOTA by $8.6\%$. Furthermore, our thorough analysis offers insights for future
research. Our code, data, and models are available at Github:
https://github.com/zchuz/SiGIR-MHQA.

</details>


### [104] [Controlling Language Confusion in Multilingual LLMs](https://arxiv.org/abs/2505.19116)
*Nahyun Lee,Yeongseo Woo,Hyunwoo Ko,Guijin Son*

Main category: cs.CL

TL;DR: 论文探讨了大语言模型中的语言混淆问题，提出通过ORPO方法有效抑制该现象，且不影响模型整体性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常出现语言混淆现象，影响低资源环境下的用户体验，传统监督微调可能加剧此问题。

Method: 采用ORPO方法，在标准监督微调基础上增加对不期望输出风格的惩罚项。

Result: ORPO能有效抑制语言混淆生成，即使在高解码温度下也不降低模型整体性能。

Conclusion: 引入适当的惩罚项可在数据有限的低资源环境中缓解语言混淆问题。

Abstract: Large language models often suffer from language confusion, a phenomenon
where responses are partially or entirely generated in unintended languages.
This can critically impact user experience in low-resource settings. We
hypothesize that conventional supervised fine-tuning exacerbates this issue
because the softmax objective focuses probability mass only on the single
correct token but does not explicitly penalize cross-lingual mixing.
Interestingly, by observing loss trajectories during the pretraining phase, we
observe that models fail to learn to distinguish between monolingual and
language-confused text. Additionally, we find that ORPO, which adds penalties
for unwanted output styles to standard SFT, effectively suppresses
language-confused generations even at high decoding temperatures without
degrading overall model performance. Our findings suggest that incorporating
appropriate penalty terms can mitigate language confusion in low-resource
settings with limited data.

</details>


### [105] [Delving into Multilingual Ethical Bias: The MSQAD with Statistical Hypothesis Tests for Large Language Models](https://arxiv.org/abs/2505.19121)
*Seunguk Yu,Juhwan Choi,Youngbin Kim*

Main category: cs.CL

TL;DR: 研究发现大语言模型存在跨语言伦理偏见，并创建多语言敏感问答数据集MSQAD进行验证。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型取得进展，但研究表明其存在社会偏见。本文旨在验证和比较LLMs在敏感话题上的伦理偏见，假设这些偏见可能源于语言差异。

Method: 通过收集人权观察组织的新闻文章，构建多语言敏感问答数据集(MSQAD)，并采用两种统计假设检验分析不同语言和主题下的偏见。

Result: 结果显示大多数情况下零假设被拒绝，表明存在跨语言差异导致的偏见，且不同LLMs普遍存在伦理偏见。

Conclusion: LLMs的伦理偏见广泛存在于多种语言中，公开MSQAD数据集以促进未来关于跨语言偏见的研究。

Abstract: Despite the recent strides in large language models, studies have underscored
the existence of social biases within these systems. In this paper, we delve
into the validation and comparison of the ethical biases of LLMs concerning
globally discussed and potentially sensitive topics, hypothesizing that these
biases may arise from language-specific distinctions. Introducing the
Multilingual Sensitive Questions & Answers Dataset (MSQAD), we collected news
articles from Human Rights Watch covering 17 topics, and generated socially
sensitive questions along with corresponding responses in multiple languages.
We scrutinized the biases of these responses across languages and topics,
employing two statistical hypothesis tests. The results showed that the null
hypotheses were rejected in most cases, indicating biases arising from
cross-language differences. It demonstrates that ethical biases in responses
are widespread across various languages, and notably, these biases were
prevalent even among different LLMs. By making the proposed MSQAD openly
available, we aim to facilitate future research endeavors focused on examining
cross-language biases in LLMs and their variant models.

</details>


### [106] [MMATH: A Multilingual Benchmark for Mathematical Reasoning](https://arxiv.org/abs/2505.19126)
*Wenyang Luo,Wayne Xin Zhao,Jing Sha,Shijin Wang,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 论文引入MMATH基准测试，评估大型推理模型在多语言复杂推理任务中的表现，发现语言间性能差异显著，并提出改进策略。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型在多语言复杂推理任务中的能力尚未充分探索，且主要集中在简单任务上，如MGSM。

Method: 引入MMATH基准测试，涵盖10种语言的374个高质量数学问题，并通过提示和训练策略改进模型表现。

Result: 发现高级模型如DeepSeek R1在不同语言间性能差异显著，且存在目标语言偏离问题；通过英语推理和目标语言回答可提升性能。

Conclusion: 研究为提升大型语言模型的多语言推理能力提供了新见解和实用策略。

Abstract: The advent of large reasoning models, such as OpenAI o1 and DeepSeek R1, has
significantly advanced complex reasoning tasks. However, their capabilities in
multilingual complex reasoning remain underexplored, with existing efforts
largely focused on simpler tasks like MGSM. To address this gap, we introduce
MMATH, a benchmark for multilingual complex reasoning spanning 374 high-quality
math problems across 10 typologically diverse languages. Using MMATH, we
observe that even advanced models like DeepSeek R1 exhibit substantial
performance disparities across languages and suffer from a critical off-target
issue-generating responses in unintended languages. To address this, we explore
strategies including prompting and training, demonstrating that reasoning in
English and answering in target languages can simultaneously enhance
performance and preserve target-language consistency. Our findings offer new
insights and practical strategies for advancing the multilingual reasoning
capabilities of large language models. Our code and data could be found at
https://github.com/RUCAIBox/MMATH.

</details>


### [107] [RetrieveAll: A Multilingual Named Entity Recognition Framework with Large Language Models](https://arxiv.org/abs/2505.19128)
*Jin Zhang,Fan Gao,Linyu Li,Yongbin Yu,Xiangxiang Wang,Nyima Tashi,Gadeng Luosang*

Main category: cs.CL

TL;DR: 提出RetrieveAll框架，通过动态LoRA解决多语言NER中的语言干扰问题，无需外部资源即提升低资源语言性能。


<details>
  <summary>Details</summary>
Motivation: 当前多语言NER方法存在语言间特征冲突和低资源语言被压制的问题，单独训练模型成本高且不可扩展。

Method: 基于动态LoRA的解耦框架RetrieveAll，结合跨粒度知识增强和分层提示机制实现无外部资源的动态适应。

Result: 在PAN-X数据集上平均F1值提升12.1%，优于现有基线方法。

Conclusion: RetrieveAll有效缓解语言干扰，推动提示机制从推理指导转向驱动学习，为低资源语言NER提供可行方案。

Abstract: The rise of large language models has led to significant performance
breakthroughs in named entity recognition (NER) for high-resource languages,
yet there remains substantial room for improvement in low- and medium-resource
languages. Existing multilingual NER methods face severe language interference
during the multi-language adaptation process, manifested in feature conflicts
between different languages and the competitive suppression of low-resource
language features by high-resource languages. Although training a dedicated
model for each language can mitigate such interference, it lacks scalability
and incurs excessive computational costs in real-world applications. To address
this issue, we propose RetrieveAll, a universal multilingual NER framework
based on dynamic LoRA. The framework decouples task-specific features across
languages and demonstrates efficient dynamic adaptability. Furthermore, we
introduce a cross-granularity knowledge augmented method that fully exploits
the intrinsic potential of the data without relying on external resources. By
leveraging a hierarchical prompting mechanism to guide knowledge injection,
this approach advances the paradigm from "prompt-guided inference" to
"prompt-driven learning." Experimental results show that RetrieveAll
outperforms existing baselines; on the PAN-X dataset, it achieves an average F1
improvement of 12.1 percent.

</details>


### [108] [Shifting AI Efficiency From Model-Centric to Data-Centric Compression](https://arxiv.org/abs/2505.19147)
*Xuyang Liu,Zichen Wen,Shaobo Wang,Junjie Chen,Zhishan Tao,Yubo Wang,Xiangqi Jin,Chang Zou,Yiyu Wang,Chenfei Liao,Xu Zheng,Honggang Chen,Weijia Li,Xuming Hu,Conghui He,Linfeng Zhang*

Main category: cs.CL

TL;DR: 论文提出随着大模型参数规模接近硬件极限，计算瓶颈已转向长序列自注意力的二次方成本，主张将研究重点从模型中心压缩转向数据中心压缩（即令牌压缩），以提升AI效率。


<details>
  <summary>Details</summary>
Motivation: 传统通过增加参数规模提升模型性能的方法面临硬件限制，长文本、高分辨率图像和视频导致自注意力计算成本剧增，亟需新的效率优化方向。

Method: 建立统一的数学模型分析现有效率策略，系统综述令牌压缩技术，分析其优势并指出当前挑战与未来方向。

Result: 论证令牌压缩能有效降低长上下文开销，是解决计算瓶颈的关键范式转变，并梳理了该领域的研究图谱。

Conclusion: 论文为AI效率研究提供新视角，旨在推动令牌压缩技术发展以应对长上下文挑战。

Abstract: The rapid advancement of large language models (LLMs) and multi-modal LLMs
(MLLMs) has historically relied on model-centric scaling through increasing
parameter counts from millions to hundreds of billions to drive performance
gains. However, as we approach hardware limits on model size, the dominant
computational bottleneck has fundamentally shifted to the quadratic cost of
self-attention over long token sequences, now driven by ultra-long text
contexts, high-resolution images, and extended videos. In this position paper,
\textbf{we argue that the focus of research for efficient AI is shifting from
model-centric compression to data-centric compression}. We position token
compression as the new frontier, which improves AI efficiency via reducing the
number of tokens during model training or inference. Through comprehensive
analysis, we first examine recent developments in long-context AI across
various domains and establish a unified mathematical framework for existing
model efficiency strategies, demonstrating why token compression represents a
crucial paradigm shift in addressing long-context overhead. Subsequently, we
systematically review the research landscape of token compression, analyzing
its fundamental benefits and identifying its compelling advantages across
diverse scenarios. Furthermore, we provide an in-depth analysis of current
challenges in token compression research and outline promising future
directions. Ultimately, our work aims to offer a fresh perspective on AI
efficiency, synthesize existing research, and catalyze innovative developments
to address the challenges that increasing context lengths pose to the AI
community's advancement.

</details>


### [109] [SpokenNativQA: Multilingual Everyday Spoken Queries for LLMs](https://arxiv.org/abs/2505.19163)
*Firoj Alam,Md Arid Hasan,Shammur Absar Chowdhury*

Main category: cs.CL

TL;DR: 该研究提出了首个多语言口语问答数据集SpokenNativQA，用于评估大语言模型在真实对话场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有文本问答数据集无法充分评估大语言模型在语音交互中的表现，特别是在多语言、口音和方言丰富的场景下。

Method: 构建包含约33,000个自然口语问答的多语言数据集，涵盖低资源语言和方言，并基于该数据集对ASR系统和LLM进行基准测试。

Result: SpokenNativQA数据集填补了语音多样性评估的空白，为研究社区提供了标准测试基准和开源工具。

Conclusion: 该研究推动了LLM在语音交互领域的评估，公开的数据集和脚本将促进相关研究的进一步发展。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
various disciplines and tasks. However, benchmarking their capabilities with
multilingual spoken queries remains largely unexplored. In this study, we
introduce SpokenNativQA, the first multilingual and culturally aligned spoken
question-answering (SQA) dataset designed to evaluate LLMs in real-world
conversational settings. The dataset comprises approximately 33,000 naturally
spoken questions and answers in multiple languages, including low-resource and
dialect-rich languages, providing a robust benchmark for assessing LLM
performance in speech-based interactions. SpokenNativQA addresses the
limitations of text-based QA datasets by incorporating speech variability,
accents, and linguistic diversity. We benchmark different ASR systems and LLMs
for SQA and present our findings. We released the data at
(https://huggingface.co/datasets/QCRI/SpokenNativQA) and the experimental
scripts at (https://llmebench.qcri.org/) for the research community.

</details>


### [110] [Assistant-Guided Mitigation of Teacher Preference Bias in LLM-as-a-Judge](https://arxiv.org/abs/2505.19176)
*Zhuo Liu,Moxin Li,Xun Deng,Qifan Wang,Fuli Feng*

Main category: cs.CL

TL;DR: 该论文提出AGDe-Judge框架，通过引入无偏的辅助模型来减少教师模型偏好偏差，有效提升LLM评估质量。


<details>
  <summary>Details</summary>
Motivation: 当前使用LLM（如GPT-4）评估生成内容时，代理评估模型会继承教师模型的偏好偏差，导致评估结果不公正。

Method: 提出三阶段框架AGDe-Judge，结合无偏辅助模型的数据，从标签和反馈两方面消除训练数据中的偏差。

Result: 实验表明，AGDe-Judge在六个评估基准上显著降低教师偏好偏差，同时保持高性能。

Conclusion: AGDe-Judge能有效解决教师模型偏好偏差问题，为LLM评估提供更公正的解决方案。

Abstract: LLM-as-a-Judge employs large language models (LLMs), such as GPT-4, to
evaluate the quality of LLM-generated responses, gaining popularity for its
cost-effectiveness and strong alignment with human evaluations. However,
training proxy judge models using evaluation data generated by powerful teacher
models introduces a critical yet previously overlooked issue: teacher
preference bias, where the proxy judge model learns a biased preference for
responses from the teacher model. To tackle this problem, we propose a novel
setting that incorporates an additional assistant model, which is not biased
toward the teacher model's responses, to complement the training data. Building
on this setup, we introduce AGDe-Judge, a three-stage framework designed to
debias from both the labels and feedbacks in the training data. Extensive
experiments demonstrate that AGDe-Judge effectively reduces teacher preference
bias while maintaining strong performance across six evaluation benchmarks.
Code is available at https://github.com/Liuz233/AGDe-Judge.

</details>


### [111] [Two LLMs debate, both are certain they've won](https://arxiv.org/abs/2505.19184)
*Minh Nhat Nguyen,Pradyumna Shyama Prasad*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLMs）在多轮对抗性辩论中存在系统性过度自信、信心逐步升级等问题，表明其无法准确自我评估或更新信念。


<details>
  <summary>Details</summary>
Motivation: 论文旨在评估LLMs在动态对抗性辩论中调整自信度的能力，结合多轮辩论和零和结构，以控制任务相关的不确定性。

Method: 研究组织了60场三轮政策辩论，涉及10种先进LLMs，模型在每轮后私下评估其获胜信心（0-100）。

Result: 观察到五种问题模式：系统性过度自信、信心逐步升级、相互高估、持续自我辩论偏见以及私有推理与公开评分不一致。

Conclusion: LLMs在动态多轮任务中缺乏准确自我评估或更新信念的能力，这在LLM输出未经仔细审查的应用场景中是一个重大问题。

Abstract: Can LLMs accurately adjust their confidence when facing opposition? Building
on previous studies measuring calibration on static fact-based
question-answering tasks, we evaluate Large Language Models (LLMs) in a
dynamic, adversarial debate setting, uniquely combining two realistic factors:
(a) a multi-turn format requiring models to update beliefs as new information
emerges, and (b) a zero-sum structure to control for task-related uncertainty,
since mutual high-confidence claims imply systematic overconfidence. We
organized 60 three-round policy debates among ten state-of-the-art LLMs, with
models privately rating their confidence (0-100) in winning after each round.
We observed five concerning patterns: (1) Systematic overconfidence: models
began debates with average initial confidence of 72.9% vs. a rational 50%
baseline. (2) Confidence escalation: rather than reducing confidence as debates
progressed, debaters increased their win probabilities, averaging 83% by the
final round. (3) Mutual overestimation: in 61.7% of debates, both sides
simultaneously claimed >=75% probability of victory, a logical impossibility.
(4) Persistent self-debate bias: models debating identical copies increased
confidence from 64.1% to 75.2%; even when explicitly informed their chance of
winning was exactly 50%, confidence still rose (from 50.0% to 57.1%). (5)
Misaligned private reasoning: models' private scratchpad thoughts sometimes
differed from their public confidence ratings, raising concerns about
faithfulness of chain-of-thought reasoning. These results suggest LLMs lack the
ability to accurately self-assess or update their beliefs in dynamic,
multi-turn tasks; a major concern as LLM outputs are deployed without careful
review in assistant roles or agentic settings.

</details>


### [112] [LIMOPro: Reasoning Refinement for Efficient and Effective Test-time Scaling](https://arxiv.org/abs/2505.19187)
*Yang Xiao,Jiashuo Wang,Ruifeng Yuan,Chunpu Xu,Kaishuai Xu,Wenjie Li,Pengfei Liu*

Main category: cs.CL

TL;DR: 论文提出PIR框架，通过量化评估推理步骤重要性来优化训练数据，减少冗余计算同时保持核心推理路径，提升模型效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在推理时生成的思维链常包含冗余功能步骤（如验证、备选方案等），增加了计算负担但贡献有限。需要一种方法在保持核心推理能力的同时降低冗余。

Method: 提出PIR（基于困惑度的重要性优化）框架：1) 量化每个推理步骤对答案预测置信度的影响 2) 选择性剪枝低重要性功能步骤 3) 保留核心渐进推理路径生成优化训练数据。

Result: 在AIME/AMC/GPQA等基准测试中：准确率提升0.9%-6.6%，token使用减少3%-41%。方法在不同模型规模、数据源和token预算下均表现良好。

Conclusion: PIR能有效平衡推理能力与计算效率，为需要高效推理的实际部署场景提供了实用解决方案。

Abstract: Large language models (LLMs) have demonstrated remarkable reasoning
capabilities through test-time scaling approaches, particularly when fine-tuned
with chain-of-thought (CoT) data distilled from more powerful large reasoning
models (LRMs). However, these reasoning chains often contain verbose elements
that mirror human problem-solving, categorized as progressive reasoning (the
essential solution development path) and functional elements (verification
processes, alternative solution approaches, and error corrections). While
progressive reasoning is crucial, the functional elements significantly
increase computational demands during test-time inference. We introduce PIR
(Perplexity-based Importance Refinement), a principled framework that
quantitatively evaluates the importance of each reasoning step based on its
impact on answer prediction confidence. PIR systematically identifies and
selectively prunes only low-importance functional steps while preserving
progressive reasoning components, creating optimized training data that
maintains the integrity of the core solution path while reducing verbosity.
Models fine-tuned on PIR-optimized data exhibit superior test-time scaling
properties, generating more concise reasoning chains while achieving improved
accuracy (+0.9\% to +6.6\%) with significantly reduced token usage (-3\% to
-41\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond).
Our approach demonstrates strong generalizability across different model sizes,
data sources, and token budgets, offering a practical solution for deploying
reasoning-capable LLMs in scenarios where efficient test-time scaling, response
time, and computational efficiency are valuable constraints.

</details>


### [113] [Misleading through Inconsistency: A Benchmark for Political Inconsistencies Detection](https://arxiv.org/abs/2505.19191)
*Nursulu Sagimbayeva,Ruveyda Betül Bahçeci,Ingmar Weber*

Main category: cs.CL

TL;DR: 该论文提出了一种自动检测政治声明不一致性的任务，并创建了一个标注数据集，评估了大语言模型在此任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 政治声明中的不一致性会削弱公众信任并影响问责制。自动检测这些不一致性可以帮助记者提问澄清，从而增强政治问责。

Method: 论文提出了不一致性检测任务，开发了一个不一致性类型量表，并构建了一个包含698对标注政治声明的数据集，其中237个样本附有标注者的解释。

Result: 大语言模型在检测不一致性方面与人类表现相当，甚至可能优于个体人类。但在识别细粒度不一致类型时，模型仍有改进空间。

Conclusion: 尽管大语言模型在检测政治声明不一致性方面表现良好，但在细粒度分类上仍需进一步提升。数据集和代码已公开。

Abstract: Inconsistent political statements represent a form of misinformation. They
erode public trust and pose challenges to accountability, when left unnoticed.
Detecting inconsistencies automatically could support journalists in asking
clarification questions, thereby helping to keep politicians accountable. We
propose the Inconsistency detection task and develop a scale of inconsistency
types to prompt NLP-research in this direction. To provide a resource for
detecting inconsistencies in a political domain, we present a dataset of 698
human-annotated pairs of political statements with explanations of the
annotators' reasoning for 237 samples. The statements mainly come from voting
assistant platforms such as Wahl-O-Mat in Germany and Smartvote in Switzerland,
reflecting real-world political issues. We benchmark Large Language Models
(LLMs) on our dataset and show that in general, they are as good as humans at
detecting inconsistencies, and might be even better than individual humans at
predicting the crowd-annotated ground-truth. However, when it comes to
identifying fine-grained inconsistency types, none of the model have reached
the upper bound of performance (due to natural labeling variation), thus
leaving room for improvement. We make our dataset and code publicly available.

</details>


### [114] [DREAM: Drafting with Refined Target Features and Entropy-Adaptive Cross-Attention Fusion for Multimodal Speculative Decoding](https://arxiv.org/abs/2505.19201)
*Yunhai Hu,Tianhua Xia,Zining Liu,Rahul Raman,Xingyu Liu,Bo Bao,Eric Sather,Vithursan Thangarasa,Sai Qian Zhang*

Main category: cs.CL

TL;DR: DREAM是一种新型的推测解码框架，专为视觉语言模型设计，通过跨注意力机制、自适应特征选择和视觉标记压缩，显著提升多模态解码效率。


<details>
  <summary>Details</summary>
Motivation: 推测解码在大型语言模型中已证明能有效加速自回归生成，但在视觉语言模型中的应用尚未充分探索。本文旨在填补这一空白。

Method: DREAM结合了三种关键技术：跨注意力机制注入中间特征、基于注意力熵的自适应特征选择以及视觉标记压缩，以优化解码过程。

Result: 实验表明，DREAM在多种流行的视觉语言模型上实现了最高3.6倍的加速，显著优于传统解码方法和先前的推测解码基线。

Conclusion: DREAM为视觉语言模型提供了一种高效、准确且并行的多模态解码解决方案，显著提升了推理吞吐量和推测草案接受长度。

Abstract: Speculative decoding (SD) has emerged as a powerful method for accelerating
autoregressive generation in large language models (LLMs), yet its integration
into vision-language models (VLMs) remains underexplored. We introduce DREAM, a
novel speculative decoding framework tailored for VLMs that combines three key
innovations: (1) a cross-attention-based mechanism to inject intermediate
features from the target model into the draft model for improved alignment, (2)
adaptive intermediate feature selection based on attention entropy to guide
efficient draft model training, and (3) visual token compression to reduce
draft model latency. DREAM enables efficient, accurate, and parallel multimodal
decoding with significant throughput improvement. Experiments across a diverse
set of recent popular VLMs, including LLaVA, Pixtral, SmolVLM and Gemma3,
demonstrate up to 3.6x speedup over conventional decoding and significantly
outperform prior SD baselines in both inference throughput and speculative
draft acceptance length across a broad range of multimodal benchmarks. The code
is publicly available at: https://github.com/SAI-Lab-NYU/DREAM.git

</details>


### [115] [SpeakStream: Streaming Text-to-Speech with Interleaved Data](https://arxiv.org/abs/2505.19206)
*Richard He Bai,Zijin Gu,Tatiana Likhomanenko,Navdeep Jaitly*

Main category: cs.CL

TL;DR: SpeakStream是一种流式TTS系统，通过增量生成音频解决传统TTS在流式LLM输出中的延迟问题。


<details>
  <summary>Details</summary>
Motivation: 传统TTS系统在完整语句上训练和推理，导致与流式LLM输出结合时延迟过高，影响对话AI的响应速度。

Method: 采用仅解码器架构，通过下一步预测损失训练，增量生成语音并吸收流式输入文本。

Result: SpeakStream实现了最先进的首令牌延迟，同时保持非流式TTS系统的语音质量。

Conclusion: SpeakStream适用于级联对话AI，能有效降低延迟并保持语音质量。

Abstract: The latency bottleneck of traditional text-to-speech (TTS) systems
fundamentally hinders the potential of streaming large language models (LLMs)
in conversational AI. These TTS systems, typically trained and inferenced on
complete utterances, introduce unacceptable delays, even with optimized
inference speeds, when coupled with streaming LLM outputs. This is particularly
problematic for creating responsive conversational agents where low first-token
latency is critical. In this paper, we present SpeakStream, a streaming TTS
system that generates audio incrementally from streaming text using a
decoder-only architecture. SpeakStream is trained using a next-step prediction
loss on interleaved text-speech data. During inference, it generates speech
incrementally while absorbing streaming input text, making it particularly
suitable for cascaded conversational AI agents where an LLM streams text to a
TTS system. Our experiments demonstrate that SpeakStream achieves
state-of-the-art latency results in terms of first-token latency while
maintaining the quality of non-streaming TTS systems.

</details>


### [116] [MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search](https://arxiv.org/abs/2505.19209)
*Zonglin Yang,Wanhao Liu,Ben Gao,Yujie Liu,Wei Li,Tong Xie,Lidong Bing,Wanli Ouyang,Erik Cambria,Dongzhan Zhou*

Main category: cs.CL

TL;DR: 该论文提出了一种细粒度科学假设发现任务，通过分层搜索方法优化大语言模型（LLMs）生成可实验验证的详细假设，并在化学文献基准测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在自动生成科学假设时，往往只能产生粗粒度的假设，缺乏关键的方法和实验细节。因此，论文旨在探索如何利用LLMs生成更详细、可实验操作的细粒度假设。

Method: 论文将细粒度假设发现定义为组合优化问题，提出了一种分层搜索方法，逐步从一般概念细化到具体实验配置，并通过LLMs内部启发式和集成模型优化假设生成。

Result: 实验结果表明，该方法在专家标注的化学文献细粒度假设基准测试中，显著优于基线方法，且通过集成模型优化的奖励景观更可靠。

Conclusion: 论文证明了LLMs在细粒度科学假设发现中的潜力，分层搜索方法和集成模型优化能有效提升假设质量，为自动化科学研究提供了新方向。

Abstract: Large language models (LLMs) have shown promise in automating scientific
hypothesis generation, yet existing approaches primarily yield coarse-grained
hypotheses lacking critical methodological and experimental details. We
introduce and formally define the novel task of fine-grained scientific
hypothesis discovery, which entails generating detailed, experimentally
actionable hypotheses from coarse initial research directions. We frame this as
a combinatorial optimization problem and investigate the upper limits of LLMs'
capacity to solve it when maximally leveraged. Specifically, we explore four
foundational questions: (1) how to best harness an LLM's internal heuristics to
formulate the fine-grained hypothesis it itself would judge as the most
promising among all the possible hypotheses it might generate, based on its own
internal scoring-thus defining a latent reward landscape over the hypothesis
space; (2) whether such LLM-judged better hypotheses exhibit stronger alignment
with ground-truth hypotheses; (3) whether shaping the reward landscape using an
ensemble of diverse LLMs of similar capacity yields better outcomes than
defining it with repeated instances of the strongest LLM among them; and (4)
whether an ensemble of identical LLMs provides a more reliable reward landscape
than a single LLM. To address these questions, we propose a hierarchical search
method that incrementally proposes and integrates details into the hypothesis,
progressing from general concepts to specific experimental configurations. We
show that this hierarchical process smooths the reward landscape and enables
more effective optimization. Empirical evaluations on a new benchmark of
expert-annotated fine-grained hypotheses from recent chemistry literature show
that our method consistently outperforms strong baselines.

</details>


### [117] [When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas](https://arxiv.org/abs/2505.19212)
*Steffen Backmann,David Guzman Piedrahita,Emanuel Tewolde,Rada Mihalcea,Bernhard Schölkopf,Zhijing Jin*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型在道德规范与利益冲突时的行为表现，发现模型在道德行为上存在显著差异且缺乏一致性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在复杂代理角色中的应用增加，其道德对齐成为AI安全的关键问题。目前对LLMs在道德要求与利益直接冲突时的行为理解有限。

Method: 研究者引入MoralSim框架，通过囚徒困境和公共物品游戏等社会困境模拟，测试多种前沿模型在三种不同道德框架下的行为。

Result: 结果显示不同模型在道德行为倾向和一致性上存在显著差异，且没有模型在所有情境中表现出一致的道德行为。

Conclusion: 研究表明在部署LLMs到代理角色时需要谨慎，尤其是当代理的“自身利益”与道德期望冲突时。

Abstract: Recent advances in large language models (LLMs) have enabled their use in
complex agentic roles, involving decision-making with humans or other agents,
making ethical alignment a key AI safety concern. While prior work has examined
both LLMs' moral judgment and strategic behavior in social dilemmas, there is
limited understanding of how they act when moral imperatives directly conflict
with rewards or incentives. To investigate this, we introduce Moral Behavior in
Social Dilemma Simulation (MoralSim) and evaluate how LLMs behave in the
prisoner's dilemma and public goods game with morally charged contexts. In
MoralSim, we test a range of frontier models across both game structures and
three distinct moral framings, enabling a systematic examination of how LLMs
navigate social dilemmas in which ethical norms conflict with payoff-maximizing
strategies. Our results show substantial variation across models in both their
general tendency to act morally and the consistency of their behavior across
game types, the specific moral framing, and situational factors such as
opponent behavior and survival risks. Crucially, no model exhibits consistently
moral behavior in MoralSim, highlighting the need for caution when deploying
LLMs in agentic roles where the agent's "self-interest" may conflict with
ethical expectations. Our code is available at
https://github.com/sbackmann/moralsim.

</details>


### [118] [The Overthinker's DIET: Cutting Token Calories with DIfficulty-AwarE Training](https://arxiv.org/abs/2505.19217)
*Weize Chen,Jiarui Yuan,Tailin Jin,Ning Ding,Huimin Chen,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: DIET框架通过动态调整token压缩策略，显著减少大语言模型的token使用量，同时提升推理性能，并优化响应长度与问题难度的相关性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在推理时常常生成过长的响应，影响效率。本文旨在通过动态调整token压缩策略，优化性能与效率的平衡。

Method: 提出DIET框架，结合问题难度动态调整token惩罚强度和目标长度，并改进奖励加权技术以实现稳定的难度感知目标。

Result: 实验表明，DIET显著减少token使用量，同时提升推理性能，并在固定计算预算下实现更好的扩展性能。

Conclusion: DIET提供了一个高效、实用且高性能的大语言模型开发框架，优化了token使用与问题难度的关系。

Abstract: Recent large language models (LLMs) exhibit impressive reasoning but often
over-think, generating excessively long responses that hinder efficiency. We
introduce DIET ( DIfficulty-AwarE Training), a framework that systematically
cuts these "token calories" by integrating on-the-fly problem difficulty into
the reinforcement learning (RL) process. DIET dynamically adapts token
compression strategies by modulating token penalty strength and conditioning
target lengths on estimated task difficulty, to optimize the
performance-efficiency trade-off. We also theoretically analyze the pitfalls of
naive reward weighting in group-normalized RL algorithms like GRPO, and propose
Advantage Weighting technique, which enables stable and effective
implementation of these difficulty-aware objectives. Experimental results
demonstrate that DIET significantly reduces token counts while simultaneously
improving reasoning performance. Beyond raw token reduction, we show two
crucial benefits largely overlooked by prior work: (1) DIET leads to superior
inference scaling. By maintaining high per-sample quality with fewer tokens, it
enables better scaling performance via majority voting with more samples under
fixed computational budgets, an area where other methods falter. (2) DIET
enhances the natural positive correlation between response length and problem
difficulty, ensuring verbosity is appropriately allocated, unlike many existing
compression methods that disrupt this relationship. Our analyses provide a
principled and effective framework for developing more efficient, practical,
and high-performing LLMs.

</details>


### [119] [Evaluating Text Creativity across Diverse Domains: A Dataset and Large Language Model Evaluator](https://arxiv.org/abs/2505.19236)
*Qian Cao,Xiting Wang,Yuzhuo Yuan,Yahui Liu,Fang Luo,Ruihua Song*

Main category: cs.CL

TL;DR: 本文提出了一种新的成对比较框架CrEval和大型数据集CreataSet，用于评估文本创造力，显著提升了与人类判断的一致性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）的创造力评估依赖低效且昂贵的人工判断，现有自动化方法缺乏普适性或与人类判断的一致性。

Method: 通过构建包含10万+人类标注和100万+合成数据的CreataSet数据集，并基于其训练LLM评估器CrEval，采用成对比较框架提升评估一致性。

Result: CrEval在人类判断一致性上显著优于现有方法，实验证明结合人类生成与合成数据对训练鲁棒评估器至关重要。

Conclusion: CrEval能有效提升LLM创造力，作者将公开数据、代码和模型以推动进一步研究。

Abstract: Creativity evaluation remains a challenging frontier for large language
models (LLMs). Current evaluations heavily rely on inefficient and costly human
judgments, hindering progress in enhancing machine creativity. While automated
methods exist, ranging from psychological testing to heuristic- or
prompting-based approaches, they often lack generalizability or alignment with
human judgment. To address these issues, in this paper, we propose a novel
pairwise-comparison framework for assessing textual creativity, leveraging
shared contextual instructions to improve evaluation consistency. We introduce
CreataSet, a large-scale dataset with 100K+ human-level and 1M+ synthetic
creative instruction-response pairs spanning diverse open-domain tasks. Through
training on CreataSet, we develop an LLM-based evaluator named CrEval. CrEval
demonstrates remarkable superiority over existing methods in alignment with
human judgments. Experimental results underscore the indispensable significance
of integrating both human-generated and synthetic data in training highly
robust evaluators, and showcase the practical utility of CrEval in boosting the
creativity of LLMs. We will release all data, code, and models publicly soon to
support further research.

</details>


### [120] [LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models](https://arxiv.org/abs/2505.19240)
*Aida Kostikova,Zhipin Wang,Deidamea Bajri,Ole Pütz,Benjamin Paaßen,Steffen Eger*

Main category: cs.CL

TL;DR: 该论文通过数据驱动的方法调查了2022至2024年间大型语言模型（LLM）的局限性研究，发现相关研究快速增长，推理问题最受关注，并揭示了不同数据集的研究趋势差异。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）研究的迅速发展，人们越来越关注其局限性，如推理失败、幻觉问题和多语言能力不足等。本研究旨在通过系统性的文献综述，量化分析这些局限性研究的趋势和分布。

Method: 研究采用自下而上的方法，从25万篇ACL和arXiv论文中筛选出14,648篇相关论文，结合关键词过滤、基于LLM的分类（经专家验证）以及主题聚类（使用HDBSCAN+BERTopic和LlooM两种方法）进行分析。

Result: 研究发现，LLM相关研究在ACL和arXiv中分别增长了5倍和4倍。自2022年以来，LLM局限性研究增长更快，到2024年底已占LLM论文的30%以上。推理是最受关注的局限性，其次是泛化、幻觉、偏见和安全性。ACL数据集的主题分布相对稳定，而arXiv则转向安全性和可控性（如安全风险、对齐、幻觉、知识编辑）以及多模态研究。

Conclusion: 论文提供了关于LLM局限性研究的定量视角，发布了标注摘要的数据集和验证方法，揭示了研究趋势和主题变化，为未来研究提供了参考。

Abstract: Large language model (LLM) research has grown rapidly, along with increasing
concern about their limitations such as failures in reasoning, hallucinations,
and limited multilingual capability. In this survey, we conduct a data-driven,
semi-automated review of research on limitations of LLM (LLLMs) from 2022 to
2024 using a bottom-up approach. From a corpus of 250,000 ACL and arXiv papers,
we identify 14,648 relevant papers using keyword filtering, LLM-based
classification, validated against expert labels, and topic clustering (via two
approaches, HDBSCAN+BERTopic and LlooM). We find that LLM-related research
increases over fivefold in ACL and fourfold in arXiv. Since 2022, LLLMs
research grows even faster, reaching over 30% of LLM papers by late 2024.
Reasoning remains the most studied limitation, followed by generalization,
hallucination, bias, and security. The distribution of topics in the ACL
dataset stays relatively stable over time, while arXiv shifts toward safety and
controllability (with topics like security risks, alignment, hallucinations,
knowledge editing), and multimodality between 2022 and 2024. We release a
dataset of annotated abstracts and a validated methodology, and offer a
quantitative view of trends in LLM limitations research.

</details>


### [121] [PATS: Process-Level Adaptive Thinking Mode Switching](https://arxiv.org/abs/2505.19250)
*Yi Wang,Junxiao Liu,Shimao Zhang,Jiajun Chen,Shujian Huang*

Main category: cs.CL

TL;DR: 论文提出PATS方法，通过动态调整推理策略平衡大语言模型的准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型采用固定推理策略，无法根据问题难度动态调整，导致性能与效率失衡。

Method: 提出PATS范式，结合过程奖励模型和束搜索，实现渐进式模式切换和错误步骤惩罚机制。

Result: 在数学基准测试中，该方法在保持适中计算量的同时实现了高准确率。

Conclusion: 过程级难度感知的推理策略适配对提升大语言模型推理效率具有重要意义。

Abstract: Current large-language models (LLMs) typically adopt a fixed reasoning
strategy, either simple or complex, for all questions, regardless of their
difficulty. This neglect of variation in task and reasoning process complexity
leads to an imbalance between performance and efficiency. Existing methods
attempt to implement training-free fast-slow thinking system switching to
handle problems of varying difficulty, but are limited by coarse-grained
solution-level strategy adjustments. To address this issue, we propose a novel
reasoning paradigm: Process-Level Adaptive Thinking Mode Switching (PATS),
which enables LLMs to dynamically adjust their reasoning strategy based on the
difficulty of each step, optimizing the balance between accuracy and
computational efficiency. Our approach integrates Process Reward Models (PRMs)
with Beam Search, incorporating progressive mode switching and bad-step penalty
mechanisms. Experiments on diverse mathematical benchmarks demonstrate that our
methodology achieves high accuracy while maintaining moderate token usage. This
study emphasizes the significance of process-level, difficulty-aware reasoning
strategy adaptation, offering valuable insights into efficient inference for
LLMs.

</details>


### [122] [Unveiling Dual Quality in Product Reviews: An NLP-Based Approach](https://arxiv.org/abs/2505.19254)
*Rafał Poświata,Marcin Michał Mirończuk,Sławomir Dadas,Małgorzata Grębowiec,Michał Perełkiewicz*

Main category: cs.CL

TL;DR: 本文探讨了如何利用自然语言处理（NLP）技术检测产品双质量问题，并通过构建波兰语数据集及多语言实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 消费者常面临产品双质量问题（同一产品在不同市场质量不一致），需自动化技术进行检测与解决。

Method: 构建包含1,957条波兰语评论的数据集（其中540条标注双质量问题），并测试SetFit、transformer编码器及大语言模型（LLMs）等方法，同时评估多语言迁移能力（英语、法语、德语）。

Result: 实验验证了NLP方法在双质量检测中的可行性，包括错误分析和鲁棒性验证，多语言子集测试展示了跨语言适用性。

Conclusion: 研究为双质量问题的自动化检测提供了可行方案，并探讨了实际部署与应用前景。

Abstract: Consumers often face inconsistent product quality, particularly when
identical products vary between markets, a situation known as the dual quality
problem. To identify and address this issue, automated techniques are needed.
This paper explores how natural language processing (NLP) can aid in detecting
such discrepancies and presents the full process of developing a solution.
First, we describe in detail the creation of a new Polish-language dataset with
1,957 reviews, 540 highlighting dual quality issues. We then discuss
experiments with various approaches like SetFit with sentence-transformers,
transformer-based encoders, and LLMs, including error analysis and robustness
verification. Additionally, we evaluate multilingual transfer using a subset of
opinions in English, French, and German. The paper concludes with insights on
deployment and practical applications.

</details>


### [123] [A Graph Perspective to Probe Structural Patterns of Knowledge in Large Language Models](https://arxiv.org/abs/2505.19286)
*Utkarsh Sahu,Zhisheng Qi,Yongjia Lei,Ryan A. Rossi,Franck Dernoncourt,Nesreen K. Ahmed,Mahantesh M Halappanavar,Yao Ma,Yu Wang*

Main category: cs.CL

TL;DR: 该论文从图结构视角分析大语言模型的知识结构模式，发现知识同质性现象，并提出基于图机器学习的知识评估方法，通过筛选未知三元组优化微调效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注大语言模型的知识访问、编辑等能力，但缺乏对其知识结构模式的探索。论文旨在填补这一空白，从图结构角度分析模型的知识组织特性。

Method: 1) 在三元组和实体层面量化模型知识；2) 分析知识与图结构属性（如节点度）的关系；3) 发现知识同质性现象后，开发基于邻居实体的图机器学习模型进行知识评估；4) 利用模型筛选低知识量三元组用于微调。

Result: 实验表明：1) 实体拓扑邻近性与知识量存在相关性（知识同质性）；2) 基于图结构的评估模型能有效识别模型知识盲区；3) 使用筛选的三元组微调可提升模型性能。

Conclusion: 大语言模型的知识呈现图结构化特征，利用图机器学习方法可优化知识评估与增量学习，为模型知识管理提供新视角。

Abstract: Large language models have been extensively studied as neural knowledge bases
for their knowledge access, editability, reasoning, and explainability.
However, few works focus on the structural patterns of their knowledge.
Motivated by this gap, we investigate these structural patterns from a graph
perspective. We quantify the knowledge of LLMs at both the triplet and entity
levels, and analyze how it relates to graph structural properties such as node
degree. Furthermore, we uncover the knowledge homophily, where topologically
close entities exhibit similar levels of knowledgeability, which further
motivates us to develop graph machine learning models to estimate entity
knowledge based on its local neighbors. This model further enables valuable
knowledge checking by selecting triplets less known to LLMs. Empirical results
show that using selected triplets for fine-tuning leads to superior
performance.

</details>


### [124] [100-LongBench: Are de facto Long-Context Benchmarks Literally Evaluating Long-Context Ability?](https://arxiv.org/abs/2505.19293)
*Wang Yang,Hongye Jin,Shaochen Zhong,Song Jiang,Qifan Wang,Vipin Chaudhary,Xiaotian Han*

Main category: cs.CL

TL;DR: 提出了一种可控制长度的长上下文基准测试和新指标，以更有效评估大语言模型的长上下文能力。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文评估基准存在两个主要问题：无法区分模型的基础能力与长上下文能力，以及固定输入长度限制了适用性。

Method: 引入了长度可调的长上下文基准测试和一种新指标，用于分离基础知识和真实长上下文能力。

Result: 实验证明该方法在有效评估大语言模型方面具有优越性。

Conclusion: 新方法解决了现有基准的不足，为长上下文能力评估提供了更清晰的比较标准。

Abstract: Long-context capability is considered one of the most important abilities of
LLMs, as a truly long context-capable LLM enables users to effortlessly process
many originally exhausting tasks -- e.g., digesting a long-form document to
find answers vs. directly asking an LLM about it. However, existing
real-task-based long-context evaluation benchmarks have two major shortcomings.
First, benchmarks like LongBench often do not provide proper metrics to
separate long-context performance from the model's baseline ability, making
cross-model comparison unclear. Second, such benchmarks are usually constructed
with fixed input lengths, which limits their applicability across different
models and fails to reveal when a model begins to break down. To address these
issues, we introduce a length-controllable long-context benchmark and a novel
metric that disentangles baseline knowledge from true long-context
capabilities. Experiments demonstrate the superiority of our approach in
effectively evaluating LLMs.

</details>


### [125] [A Necessary Step toward Faithfulness: Measuring and Improving Consistency in Free-Text Explanations](https://arxiv.org/abs/2505.19299)
*Lingjun Zhao,Hal Daumé III*

Main category: cs.CL

TL;DR: 该论文提出了一种量化预测与解释一致性的方法PEX，发现超62%的大模型生成解释缺乏一致性，并通过直接偏好优化将一致性提升43.1%-292.3%，同时提升解释可信度9.7%。


<details>
  <summary>Details</summary>
Motivation: 在高风险AI决策中，确保自由文本解释的可信度至关重要，但语言模型生成和人工评估此类解释存在挑战。

Method: 扩展证据权重概念，提出预测-解释一致性(PEX)度量，并应用直接偏好优化方法改进模型生成解释的一致性。

Result: 62%以上的大模型生成解释缺乏PEX一致性；经优化后，三个模型族的一致性提升43.1%-292.3%，解释可信度最高提升9.7%。

Conclusion: PEX一致性是解释可信度的关键指标，通过直接偏好优化可显著提升语言模型生成解释的质量。

Abstract: Faithful free-text explanations are important to ensure transparency in
high-stakes AI decision-making contexts, but they are challenging to generate
by language models and assess by humans. In this paper, we present a measure
for Prediction-EXplanation (PEX) consistency, by extending the concept of
weight of evidence. This measure quantifies how much a free-text explanation
supports or opposes a prediction, serving as an important aspect of explanation
faithfulness. Our analysis reveals that more than 62% explanations generated by
large language models lack this consistency. We show that applying direct
preference optimization improves the consistency of generated explanations
across three model families, with improvement ranging from 43.1% to 292.3%.
Furthermore, we demonstrate that optimizing this consistency measure can
improve explanation faithfulness by up to 9.7%.

</details>


### [126] [SituatedThinker: Grounding LLM Reasoning with Real-World through Situated Thinking](https://arxiv.org/abs/2505.19300)
*Junnan Liu,Linhao Luo,Thuy-Trang Vu,Gholamreza Haffari*

Main category: cs.CL

TL;DR: 论文提出SituatedThinker框架，通过结合内部知识和外部信息，增强大语言模型在现实世界中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）的推理能力受限于内部参数空间，无法获取实时信息或理解物理世界。

Method: 引入SituatedThinker框架，利用强化学习激励模型结合内部知识和外部信息进行推理。

Result: 在多跳问答和数学推理基准测试中表现显著提升，并在KBQA、TableQA等未见任务上展示出强大性能。

Conclusion: SituatedThinker通过实时信息增强LLMs的推理能力，展现了通用性强且可推广的现实世界推理能力。

Abstract: Recent advances in large language models (LLMs) demonstrate their impressive
reasoning capabilities. However, the reasoning confined to internal parametric
space limits LLMs' access to real-time information and understanding of the
physical world. To overcome this constraint, we introduce SituatedThinker, a
novel framework that enables LLMs to ground their reasoning in real-world
contexts through situated thinking, which adaptively combines both internal
knowledge and external information with predefined interfaces. By utilizing
reinforcement learning, SituatedThinker incentivizes deliberate reasoning with
the real world to acquire information and feedback, allowing LLMs to surpass
their knowledge boundaries and enhance reasoning. Experimental results
demonstrate significant performance improvements on multi-hop
question-answering and mathematical reasoning benchmarks. Furthermore,
SituatedThinker demonstrates strong performance on unseen tasks, such as KBQA,
TableQA, and text-based games, showcasing the generalizable real-world grounded
reasoning capability. Our codes are available at
https://github.com/jnanliu/SituatedThinker.

</details>


### [127] [PatentScore: Multi-dimensional Evaluation of LLM-Generated Patent Claims](https://arxiv.org/abs/2505.19345)
*Yongmin Yoo,Qiongkai Xu,Longbing Cao*

Main category: cs.CL

TL;DR: 论文提出PatentScore框架，针对LLM生成的专利权利要求进行多维度评估，解决了现有NLG指标不适用于专利文件的问题，并在多个模型上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言生成（NLG）指标无法满足专利文件的结构和法律特性需求，尤其是在评估专利权利要求时缺乏有效方法。专利权利要求对保护范围至关重要，需评估法律有效性、技术准确性和结构合规性。

Method: 提出PatentScore框架，包含：（1）权利要求的分层分解分析；（2）基于法律和技术标准的领域特定验证模式；（3）结构、语义和法律维度的评分。

Result: 在400个GPT-4o-mini生成的Claim 1上测试，与专家评分的Pearson相关系数达0.819，优于现有NLG指标。Claude-3.5-Haiku和Gemini-1.5-flash等开源模型也显示出强相关性。

Conclusion: PatentScore能有效评估LLM生成的专利权利要求，其多维度和专利特定的设计使其优于通用NLG指标，且具有鲁棒性和泛化能力。

Abstract: Natural language generation (NLG) metrics play a central role in evaluating
generated texts, but are not well suited for the structural and legal
characteristics of patent documents. Large language models (LLMs) offer strong
potential in automating patent generation, yet research on evaluating
LLM-generated patents remains limited, especially in evaluating the generation
quality of patent claims, which are central to defining the scope of
protection. Effective claim evaluation requires addressing legal validity,
technical accuracy, and structural compliance. To address this gap, we
introduce PatentScore, a multi-dimensional evaluation framework for assessing
LLM-generated patent claims. PatentScore incorporates: (1) hierarchical
decomposition for claim analysis; (2) domain-specific validation patterns based
on legal and technical standards; and (3) scoring across structural, semantic,
and legal dimensions. Unlike general-purpose NLG metrics, PatentScore reflects
patent-specific constraints and document structures, enabling evaluation beyond
surface similarity. We evaluate 400 GPT-4o-mini generated Claim 1s and report a
Pearson correlation of $r = 0.819$ with expert annotations, outperforming
existing NLG metrics. Furthermore, we conduct additional evaluations using open
models such as Claude-3.5-Haiku and Gemini-1.5-flash, all of which show strong
correlations with expert judgments, confirming the robustness and
generalizability of our framework.

</details>


### [128] [GC-KBVQA: A New Four-Stage Framework for Enhancing Knowledge Based Visual Question Answering Performance](https://arxiv.org/abs/2505.19354)
*Mohammad Mahdi Moradi,Sudhir Mudur*

Main category: cs.CL

TL;DR: GC-KBVQA框架通过四阶段方法，利用LLM实现零样本VQA任务，无需端到端多模态训练，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有KB-VQA方法依赖外部知识库或LLM，但提供的辅助信息可能不相关或误导答案预测，限制了其潜力。

Method: 提出GC-KBVQA框架，结合问题感知的标题生成和外部知识，为LLM创建信息丰富的提示，无需任务特定微调。

Result: 与竞争KB-VQA方法相比，GC-KBVQA性能显著提升，且降低了成本和部署复杂度。

Conclusion: GC-KBVQA通过有效利用LLM和上下文丰富的标题，解决了KB-VQA中的信息相关性问题，展示了广泛的任务适应性。

Abstract: Knowledge-Based Visual Question Answering (KB-VQA) methods focus on tasks
that demand reasoning with information extending beyond the explicit content
depicted in the image. Early methods relied on explicit knowledge bases to
provide this auxiliary information. Recent approaches leverage Large Language
Models (LLMs) as implicit knowledge sources. While KB-VQA methods have
demonstrated promising results, their potential remains constrained as the
auxiliary text provided may not be relevant to the question context, and may
also include irrelevant information that could misguide the answer predictor.
We introduce a novel four-stage framework called Grounding Caption-Guided
Knowledge-Based Visual Question Answering (GC-KBVQA), which enables LLMs to
effectively perform zero-shot VQA tasks without the need for end-to-end
multimodal training. Innovations include grounding question-aware caption
generation to move beyond generic descriptions and have compact, yet detailed
and context-rich information. This is combined with knowledge from external
sources to create highly informative prompts for the LLM. GC-KBVQA can address
a variety of VQA tasks, and does not require task-specific fine-tuning, thus
reducing both costs and deployment complexity by leveraging general-purpose,
pre-trained LLMs. Comparison with competing KB-VQA methods shows significantly
improved performance. Our code will be made public.

</details>


### [129] [Estimating Online Influence Needs Causal Modeling! Counterfactual Analysis of Social Media Engagement](https://arxiv.org/abs/2505.19355)
*Lin Tian,Marian-Andrei Rizoiu*

Main category: cs.CL

TL;DR: 该论文提出了一种新的联合处理-结果框架，用于区分社交媒体中的因果关系和相关关系，特别是在分析错误信息传播时。该方法结合因果推断技术和序列模型，显著提升了预测参与度的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在分析社交媒体影响时，往往无法捕捉外部时间信号如何触发参与的因果机制。论文旨在解决这一问题，特别是在错误信息传播的场景中。

Method: 引入了一种联合处理-结果框架，利用现有的序列模型同时适应政策时间和参与效果，并采用来自医疗领域的因果推断技术来估计平均处理效应（ATE）。

Result: 在真实世界的错误信息和虚假信息数据集上的实验表明，该方法在预测参与度方面比现有基准高出15-22%，并且在多种反事实场景中表现优异。

Conclusion: 该方法能够有效区分社交媒体中的因果关系和相关关系，显著提升了影响估计的准确性，并与专家经验标准高度一致。

Abstract: Understanding true influence in social media requires distinguishing
correlation from causation--particularly when analyzing misinformation spread.
While existing approaches focus on exposure metrics and network structures,
they often fail to capture the causal mechanisms by which external temporal
signals trigger engagement. We introduce a novel joint treatment-outcome
framework that leverages existing sequential models to simultaneously adapt to
both policy timing and engagement effects. Our approach adapts causal inference
techniques from healthcare to estimate Average Treatment Effects (ATE) within
the sequential nature of social media interactions, tackling challenges from
external confounding signals. Through our experiments on real-world
misinformation and disinformation datasets, we show that our models outperform
existing benchmarks by 15--22% in predicting engagement across diverse
counterfactual scenarios, including exposure adjustment, timing shifts, and
varied intervention durations. Case studies on 492 social media users show our
causal effect measure aligns strongly with the gold standard in influence
estimation, the expert-based empirical influence.

</details>


### [130] [ChartLens: Fine-grained Visual Attribution in Charts](https://arxiv.org/abs/2505.19360)
*Manan Suri,Puneet Mathur,Nedim Lipka,Franck Dernoncourt,Ryan A. Rossi,Dinesh Manocha*

Main category: cs.CL

TL;DR: 提出ChartLens算法和ChartVA-Eval基准，解决多模态大语言模型在图表理解中的幻觉问题，提升细粒度视觉归因26-66%。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在图表理解任务中存在幻觉问题，生成的文本与视觉数据冲突，需开发方法验证图表相关响应的准确性。

Method: 提出ChartLens算法，结合基于分割的图表对象识别技术和集合标记提示方法，实现细粒度视觉归因。

Result: 评估显示，ChartLens将细粒度归因性能提升26-66%，并在跨领域图表数据中验证有效性。

Conclusion: ChartLens和ChartVA-Eval为解决MLLMs在图表理解中的幻觉问题提供了有效工具，显著提升归因准确性。

Abstract: The growing capabilities of multimodal large language models (MLLMs) have
advanced tasks like chart understanding. However, these models often suffer
from hallucinations, where generated text sequences conflict with the provided
visual data. To address this, we introduce Post-Hoc Visual Attribution for
Charts, which identifies fine-grained chart elements that validate a given
chart-associated response. We propose ChartLens, a novel chart attribution
algorithm that uses segmentation-based techniques to identify chart objects and
employs set-of-marks prompting with MLLMs for fine-grained visual attribution.
Additionally, we present ChartVA-Eval, a benchmark with synthetic and
real-world charts from diverse domains like finance, policy, and economics,
featuring fine-grained attribution annotations. Our evaluations show that
ChartLens improves fine-grained attributions by 26-66%.

</details>


### [131] [Belief Attribution as Mental Explanation: The Role of Accuracy, Informativity, and Causality](https://arxiv.org/abs/2505.19376)
*Lance Ying,Almog Hillel,Ryan Truong,Vikash K. Mansinghka,Joshua B. Tenenbaum,Tan Zhi-Xuan*

Main category: cs.CL

TL;DR: 人们倾向于用解释力强的信念来推测他人行为，研究通过计算模型验证准确性、信息量和因果相关性三个因素的作用，发现因果相关性最能预测人们的信念归因。


<details>
  <summary>Details</summary>
Motivation: 探讨人们在解释他人行为时，倾向于选择哪些具体的信念作为心理解释，以及这些信念的解释力如何影响归因过程。

Method: 开发了一个计算模型，通过准确性、信息量和因果相关性三个因素量化信念的解释力，并通过实验让参与者对描述智能体信念的陈述进行排序。

Result: 准确性和信息量的结合能较好地预测参与者的排序，但因果相关性是解释参与者反应的最佳单一因素。

Conclusion: 因果相关性在人们选择性地归因他人信念时起着关键作用，支持了人们偏好用解释力强的信念来理解行为的假设。

Abstract: A key feature of human theory-of-mind is the ability to attribute beliefs to
other agents as mentalistic explanations for their behavior. But given the wide
variety of beliefs that agents may hold about the world and the rich language
we can use to express them, which specific beliefs are people inclined to
attribute to others? In this paper, we investigate the hypothesis that people
prefer to attribute beliefs that are good explanations for the behavior they
observe. We develop a computational model that quantifies the explanatory
strength of a (natural language) statement about an agent's beliefs via three
factors: accuracy, informativity, and causal relevance to actions, each of
which can be computed from a probabilistic generative model of belief-driven
behavior. Using this model, we study the role of each factor in how people
selectively attribute beliefs to other agents. We investigate this via an
experiment where participants watch an agent collect keys hidden in boxes in
order to reach a goal, then rank a set of statements describing the agent's
beliefs about the boxes' contents. We find that accuracy and informativity
perform reasonably well at predicting these rankings when combined, but that
causal relevance is the single factor that best explains participants'
responses.

</details>


### [132] [GSA-TTS : Toward Zero-Shot Speech Synthesis based on Gradual Style Adaptor](https://arxiv.org/abs/2505.19384)
*Seokgi Lee,Jungjun Kim*

Main category: cs.CL

TL;DR: GSA-TTS提出了一种渐进式风格编码器，通过局部到全局的层次化策略实现零样本语音合成，在自然度、说话人相似度和清晰度上表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了解决零样本语音合成中风格表征的鲁棒性和丰富性问题，论文提出了一种渐进式语义层次编码方法。

Method: 采用局部语义单元风格捕获+自注意力全局融合的层次编码策略，构建渐进式风格适配器(GSA)。

Result: 在未见过的说话人测试中，系统在自然度、说话人相似度和可懂度方面均取得显著提升，且结构具有可解释性和可控性优势。

Conclusion: GSA-TTS通过层次化风格编码实现了高质量的零样本语音合成，其渐进式结构为风格控制提供了新思路。

Abstract: We present the gradual style adaptor TTS (GSA-TTS) with a novel style encoder
that gradually encodes speaking styles from an acoustic reference for zero-shot
speech synthesis. GSA first captures the local style of each semantic sound
unit. Then the local styles are combined by self-attention to obtain a global
style condition. This semantic and hierarchical encoding strategy provides a
robust and rich style representation for an acoustic model. We test GSA-TTS on
unseen speakers and obtain promising results regarding naturalness, speaker
similarity, and intelligibility. Additionally, we explore the potential of GSA
in terms of interpretability and controllability, which stems from its
hierarchical structure.

</details>


### [133] [gec-metrics: A Unified Library for Grammatical Error Correction Evaluation](https://arxiv.org/abs/2505.19388)
*Takumi Goto,Yusuke Sakai,Taro Watanabe*

Main category: cs.CL

TL;DR: 介绍了一个名为gec-metrics的库，用于通过统一接口开发和评估语法纠错（GEC）指标，确保公平比较并提供扩展性。


<details>
  <summary>Details</summary>
Motivation: 为了解决语法纠错评估中实现不一致的问题，提供一个统一的评估框架，促进公平的系统比较和指标开发。

Method: 开发了一个具有统一接口的库，包含评估功能、API设计、元评估功能以及分析和可视化脚本。

Result: 发布了MIT许可的代码和可安装包，提供了视频教程，支持语法纠错评估的标准化和扩展。

Conclusion: gec-metrics库通过统一接口和丰富功能，有效支持了语法纠错评估的公平性和进一步发展。

Abstract: We introduce gec-metrics, a library for using and developing grammatical
error correction (GEC) evaluation metrics through a unified interface. Our
library enables fair system comparisons by ensuring that everyone conducts
evaluations using a consistent implementation. Moreover, it is designed with a
strong focus on API usage, making it highly extensible. It also includes
meta-evaluation functionalities and provides analysis and visualization
scripts, contributing to developing GEC evaluation metrics. Our code is
released under the MIT license and is also distributed as an installable
package. The video is available on YouTube.

</details>


### [134] [Simple and Effective Baselines for Code Summarisation Evaluation](https://arxiv.org/abs/2505.19392)
*Jade Robinson,Jonathan K. Kummerfeld*

Main category: cs.CL

TL;DR: 论文提出使用LLM评估代码摘要质量的新基线方法，优于现有指标，建议结合嵌入方法避免偏差。


<details>
  <summary>Details</summary>
Motivation: 现有代码摘要生成技术难以比较，人工评估成本高且自动指标不可靠。

Method: 引入基于LLM的评分基线，可结合代码内容评估摘要质量，支持无参考摘要的变体。

Result: 新方法优于传统n-gram和嵌入基线，但需结合嵌入方法以规避LLM特有偏差。

Conclusion: LLM评分法为代码摘要评估提供了有效新基线，建议与嵌入方法结合使用。

Abstract: Code documentation is useful, but writing it is time-consuming. Different
techniques for generating code summaries have emerged, but comparing them is
difficult because human evaluation is expensive and automatic metrics are
unreliable. In this paper, we introduce a simple new baseline in which we ask
an LLM to give an overall score to a summary. Unlike n-gram and embedding-based
baselines, our approach is able to consider the code when giving a score. This
allows us to also make a variant that does not consider the reference summary
at all, which could be used for other tasks, e.g., to evaluate the quality of
documentation in code bases. We find that our method is as good or better than
prior metrics, though we recommend using it in conjunction with embedding-based
methods to avoid the risk of LLM-specific bias.

</details>


### [135] [CoTGuard: Using Chain-of-Thought Triggering for Copyright Protection in Multi-Agent LLM Systems](https://arxiv.org/abs/2505.19405)
*Yan Wen,Junfeng Guo,Heng Huang*

Main category: cs.CL

TL;DR: 论文提出CoTGuard框架，通过触发式检测在思维链推理中保护版权，有效识别多智能体系统中的内容泄露。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型发展为能协作推理和执行任务的自主智能体，多智能体系统在解决复杂问题方面展现出强大能力。然而，这些系统在智能体间通信和推理过程中可能无意中召回敏感或受版权保护的内容，现有技术主要关注最终输出的内容检测，忽视了智能体内部更丰富、更具揭示性的推理过程。

Method: CoTGuard框架通过在智能体提示中嵌入特定触发查询，激活思维链推理的特定片段，并监控中间推理步骤以检测未经授权的内容复制。

Result: 实验表明，CoTGuard在各种基准测试中能有效发现内容泄露，同时对任务性能的干扰最小。

Conclusion: 推理层面的监控为基于大语言模型的智能体系统中的知识产权保护提供了有前景的方向。

Abstract: As large language models (LLMs) evolve into autonomous agents capable of
collaborative reasoning and task execution, multi-agent LLM systems have
emerged as a powerful paradigm for solving complex problems. However, these
systems pose new challenges for copyright protection, particularly when
sensitive or copyrighted content is inadvertently recalled through inter-agent
communication and reasoning. Existing protection techniques primarily focus on
detecting content in final outputs, overlooking the richer, more revealing
reasoning processes within the agents themselves. In this paper, we introduce
CoTGuard, a novel framework for copyright protection that leverages
trigger-based detection within Chain-of-Thought (CoT) reasoning. Specifically,
we can activate specific CoT segments and monitor intermediate reasoning steps
for unauthorized content reproduction by embedding specific trigger queries
into agent prompts. This approach enables fine-grained, interpretable detection
of copyright violations in collaborative agent scenarios. We evaluate CoTGuard
on various benchmarks in extensive experiments and show that it effectively
uncovers content leakage with minimal interference to task performance. Our
findings suggest that reasoning-level monitoring offers a promising direction
for safeguarding intellectual property in LLM-based agent systems.

</details>


### [136] [Self-Reflective Planning with Knowledge Graphs: Enhancing LLM Reasoning Reliability for Question Answering](https://arxiv.org/abs/2505.19410)
*Jiajun Zhu,Ye Liu,Meikai Bao,Kai Zhang,Yanghai Zhang,Qi Liu*

Main category: cs.CL

TL;DR: 论文提出Self-Reflective Planning (SRP)框架，通过迭代式参考引导推理，结合大语言模型与知识图谱，解决现有方法推理路径不完整或事实不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言处理任务中表现出色，但在内部知识不足时容易产生幻觉。虽然结合知识图谱可提供结构化、可验证的信息，但现有方法常生成不完整或事实不一致的推理路径。

Method: SRP框架通过搜索参考信息引导规划和反思，检查初始关系并生成推理路径，从知识图谱检索知识后，通过判断检索结果并编辑推理路径，迭代优化直至正确检索答案。

Result: 在三个公共数据集上的实验表明，SRP超越多个强基线，并进一步验证了其可靠的推理能力。

Conclusion: SRP框架通过迭代式参考引导推理，有效结合大语言模型与知识图谱，显著提升了推理的完整性和事实一致性。

Abstract: Recently, large language models (LLMs) have demonstrated remarkable
capabilities in natural language processing tasks, yet they remain prone to
hallucinations when reasoning with insufficient internal knowledge. While
integrating LLMs with knowledge graphs (KGs) provides access to structured,
verifiable information, existing approaches often generate incomplete or
factually inconsistent reasoning paths. To this end, we propose Self-Reflective
Planning (SRP), a framework that synergizes LLMs with KGs through iterative,
reference-guided reasoning. Specifically, given a question and topic entities,
SRP first searches for references to guide planning and reflection. In the
planning process, it checks initial relations and generates a reasoning path.
After retrieving knowledge from KGs through a reasoning path, it implements
iterative reflection by judging the retrieval result and editing the reasoning
path until the answer is correctly retrieved. Extensive experiments on three
public datasets demonstrate that SRP surpasses various strong baselines and
further underscore its reliable reasoning ability.

</details>


### [137] [The Role of Diversity in In-Context Learning for Large Language Models](https://arxiv.org/abs/2505.19426)
*Wenyang Xiao,Haoyu Zhao,Lingxiao Huang*

Main category: cs.CL

TL;DR: 论文研究了在上下文学习中示例多样性对性能的影响，发现多样性方法能提升复杂任务表现和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有上下文学习研究多关注示例与查询的相似性，而多样性选择的影响尚未充分探索。

Method: 通过多任务实验（情感分类、数学、代码等）和多样性感知选择方法，结合理论框架验证。

Result: 多样性方法显著提升数学/代码等复杂任务表现，并增强对分布外查询的鲁棒性。

Conclusion: 示例选择中引入多样性可优化大语言模型的上下文学习效果，尤其对复杂任务具有重要意义。

Abstract: In-context learning (ICL) is a crucial capability of current large language
models (LLMs), where the selection of examples plays a key role in performance.
While most existing approaches focus on selecting the most similar examples to
the query, the impact of diversity in example selection remains underexplored.
We systematically investigate the role of diversity in in-context example
selection through experiments across a range of tasks, from sentiment
classification to more challenging math and code problems. Experiments on
Llama-3.1, Gemma-2, and Mistral-v0.3 families of models show that
diversity-aware selection methods improve performance, particularly on complex
tasks like math and code, and enhance robustness to out-of-distribution
queries. To support these findings, we introduce a theoretical framework that
explains the benefits of incorporating diversity in in-context example
selection.

</details>


### [138] [Frictional Agent Alignment Framework: Slow Down and Don't Break Things](https://arxiv.org/abs/2505.19428)
*Abhijnan Nath,Carine Graff,Andrei Bachinin,Nikhil Krishnaswamy*

Main category: cs.CL

TL;DR: 论文提出FAAF框架，通过生成上下文感知的“摩擦”来促进动态协作任务中的信念对齐，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有偏好对齐方法（如DPO）在静态场景表现良好，但在动态协作任务中因稀疏和倾斜的信念信号而效果有限。

Method: FAAF框架包含两个策略：摩擦状态策略识别信念偏差，干预策略生成协作偏好的响应，并通过监督损失训练单一策略。

Result: 在三个基准测试中，FAAF在生成简洁可解释的摩擦及OOD泛化方面优于竞争对手。

Conclusion: FAAF通过使LLM成为自适应“思考伙伴”，推动了可扩展的动态人机协作。

Abstract: AI support of collaborative interactions entails mediating potential
misalignment between interlocutor beliefs. Common preference alignment methods
like DPO excel in static settings, but struggle in dynamic collaborative tasks
where the explicit signals of interlocutor beliefs are sparse and skewed. We
propose the Frictional Agent Alignment Framework (FAAF), to generate precise,
context-aware "friction" that prompts for deliberation and re-examination of
existing evidence. FAAF's two-player objective decouples from data skew: a
frictive-state policy identifies belief misalignments, while an intervention
policy crafts collaborator-preferred responses. We derive an analytical
solution to this objective, enabling training a single policy via a simple
supervised loss. Experiments on three benchmarks show FAAF outperforms
competitors in producing concise, interpretable friction and in OOD
generalization. By aligning LLMs to act as adaptive "thought partners" -- not
passive responders -- FAAF advances scalable, dynamic human-AI collaboration.
Our code and data can be found at https://github.com/csu-signal/FAAF_ACL.

</details>


### [139] [Rhapsody: A Dataset for Highlight Detection in Podcasts](https://arxiv.org/abs/2505.19429)
*Younghan Park,Anuj Diwan,David Harwath,Eunsol Choi*

Main category: cs.CL

TL;DR: 论文介绍了Rhapsody数据集，用于自动检测播客高光片段，并比较了零样本提示和微调语言模型的效果。


<details>
  <summary>Details</summary>
Motivation: 播客内容庞大且无结构，自动识别高光片段有助于用户快速了解内容并决定是否收听完整内容。

Method: 使用13K播客片段数据集，采用段级二分类任务，探索零样本提示和微调语言模型方法。

Result: 微调模型显著优于零样本方法，结合语音信号和文本特征效果最佳，而GPT-4o等先进模型表现不佳。

Conclusion: 长语音媒体中细粒度信息访问具有挑战性，微调模型结合多模态特征效果更好。

Abstract: Podcasts have become daily companions for half a billion users. Given the
enormous amount of podcast content available, highlights provide a valuable
signal that helps viewers get the gist of an episode and decide if they want to
invest in listening to it in its entirety. However, identifying highlights
automatically is challenging due to the unstructured and long-form nature of
the content. We introduce Rhapsody, a dataset of 13K podcast episodes paired
with segment-level highlight scores derived from YouTube's 'most replayed'
feature. We frame the podcast highlight detection as a segment-level binary
classification task. We explore various baseline approaches, including
zero-shot prompting of language models and lightweight finetuned language
models using segment-level classification heads. Our experimental results
indicate that even state-of-the-art language models like GPT-4o and Gemini
struggle with this task, while models finetuned with in-domain data
significantly outperform their zero-shot performance. The finetuned model
benefits from leveraging both speech signal features and transcripts. These
findings highlight the challenges for fine-grained information access in
long-form spoken media.

</details>


### [140] [Deriving Strategic Market Insights with Large Language Models: A Benchmark for Forward Counterfactual Generation](https://arxiv.org/abs/2505.19430)
*Keane Ong,Rui Mao,Deeksha Varshney,Paul Pu Liang,Erik Cambria,Gianmarco Mengaldo*

Main category: cs.CL

TL;DR: 论文提出Fin-Force基准，用于评估大语言模型在金融领域前瞻性反事实推理中的应用，以自动化预测市场动态。


<details>
  <summary>Details</summary>
Motivation: 在动态金融市场中，前瞻性反事实推理能帮助预测潜在风险和机会，但大规模实施面临认知挑战，需要自动化解决方案。

Method: 引入Fin-Force基准，通过整理金融新闻标题并提供结构化评估，支持基于大语言模型的前瞻性反事实生成。

Result: 通过Fin-Force实验评估了先进的大语言模型和反事实生成方法，分析了其局限性，并为未来研究提供了见解。

Conclusion: Fin-Force为探索和预测未来市场发展提供了可扩展的自动化解决方案，为决策提供了结构化见解。

Abstract: Counterfactual reasoning typically involves considering alternatives to
actual events. While often applied to understand past events, a distinct
form-forward counterfactual reasoning-focuses on anticipating plausible future
developments. This type of reasoning is invaluable in dynamic financial
markets, where anticipating market developments can powerfully unveil potential
risks and opportunities for stakeholders, guiding their decision-making.
However, performing this at scale is challenging due to the cognitive demands
involved, underscoring the need for automated solutions. Large Language Models
(LLMs) offer promise, but remain unexplored for this application. To address
this gap, we introduce a novel benchmark, Fin-Force-FINancial FORward
Counterfactual Evaluation. By curating financial news headlines and providing
structured evaluation, Fin-Force supports LLM based forward counterfactual
generation. This paves the way for scalable and automated solutions for
exploring and anticipating future market developments, thereby providing
structured insights for decision-making. Through experiments on Fin-Force, we
evaluate state-of-the-art LLMs and counterfactual generation methods, analyzing
their limitations and proposing insights for future research.

</details>


### [141] [Route to Reason: Adaptive Routing for LLM and Reasoning Strategy Selection](https://arxiv.org/abs/2505.19435)
*Zhihong Pan,Kai Zhang,Yuze Zhao,Yupeng Han*

Main category: cs.CL

TL;DR: 论文提出Route-To-Reason（RTR）框架，动态分配语言模型和推理策略以优化任务性能，在降低计算成本的同时提高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂推理任务时存在计算成本高和模型‘过度思考’的问题，需要一种高效且灵活的动态分配机制。

Method: RTR框架通过压缩表示语言模型和推理策略，在推理时动态选择最优组合，支持任意黑盒或白盒模型和策略的即插即用。

Result: 实验表明，RTR在七个开源模型和四种推理策略上实现了准确性和计算效率的最佳平衡，准确率高于最佳单一模型且减少60%以上的token使用。

Conclusion: RTR是一种低成本、高灵活性的统一路由框架，显著提升了推理任务的性能与效率。

Abstract: The inherent capabilities of a language model (LM) and the reasoning
strategies it employs jointly determine its performance in reasoning tasks.
While test-time scaling is regarded as an effective approach to tackling
complex reasoning tasks, it incurs substantial computational costs and often
leads to "overthinking", where models become trapped in "thought pitfalls". To
address this challenge, we propose Route-To-Reason (RTR), a novel unified
routing framework that dynamically allocates both LMs and reasoning strategies
according to task difficulty under budget constraints. RTR learns compressed
representations of both expert models and reasoning strategies, enabling their
joint and adaptive selection at inference time. This method is low-cost, highly
flexible, and can be seamlessly extended to arbitrary black-box or white-box
models and strategies, achieving true plug-and-play functionality. Extensive
experiments across seven open source models and four reasoning strategies
demonstrate that RTR achieves an optimal trade-off between accuracy and
computational efficiency among all baselines, achieving higher accuracy than
the best single model while reducing token usage by over 60%.

</details>


### [142] [Surrogate Signals from Format and Length: Reinforcement Learning for Solving Mathematical Problems without Ground Truth Answers](https://arxiv.org/abs/2505.19439)
*Rihui Xin,Han Liu,Zecheng Wang,Yupeng Zhang,Dianbo Sui,Xiaolin Hu,Bingning Wang*

Main category: cs.CL

TL;DR: 该研究提出了一种无需传统真实答案的替代方法，通过格式和长度作为训练信号来提升大语言模型在数学问题解决中的表现。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖真实答案训练大语言模型解决数学问题，但获取这些答案成本高昂且有时不可行。

Method: 研究采用格式正确性和答案长度作为替代信号，结合GRPO算法进行训练。

Result: 该方法在某些场景下表现优于依赖真实答案的标准GRPO算法，7B基础模型在AIME2024上达到40.0%准确率。

Conclusion: 研究表明，通过培养良好的答题习惯即可释放模型已有的数学推理能力，减少对大量真实答案数据的依赖。

Abstract: Large Language Models have achieved remarkable success in natural language
processing tasks, with Reinforcement Learning playing a key role in adapting
them to specific applications. However, obtaining ground truth answers for
training LLMs in mathematical problem-solving is often challenging, costly, and
sometimes unfeasible. This research delves into the utilization of format and
length as surrogate signals to train LLMs for mathematical problem-solving,
bypassing the need for traditional ground truth answers.Our study shows that a
reward function centered on format correctness alone can yield performance
improvements comparable to the standard GRPO algorithm in early phases.
Recognizing the limitations of format-only rewards in the later phases, we
incorporate length-based rewards. The resulting GRPO approach, leveraging
format-length surrogate signals, not only matches but surpasses the performance
of the standard GRPO algorithm relying on ground truth answers in certain
scenarios, achieving 40.0\% accuracy on AIME2024 with a 7B base model. Through
systematic exploration and experimentation, this research not only offers a
practical solution for training LLMs to solve mathematical problems and
reducing the dependence on extensive ground truth data collection, but also
reveals the essence of why our label-free approach succeeds: base model is like
an excellent student who has already mastered mathematical and logical
reasoning skills, but performs poorly on the test paper, it simply needs to
develop good answering habits to achieve outstanding results in exams , in
other words, to unlock the capabilities it already possesses.

</details>


### [143] [The Birth of Knowledge: Emergent Features across Time, Space, and Scale in Large Language Models](https://arxiv.org/abs/2505.19440)
*Shashata Sawmya,Micah Adler,Nir Shavit*

Main category: cs.CL

TL;DR: 本文研究了大语言模型(LLM)中可解释分类特征的出现规律，分析了它们在训练时间、Transformer层空间和模型规模三个维度的表现。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型内部语义概念的形成机制，理解特征在时间、空间和规模三个维度上的涌现规律。

Method: 使用稀疏自编码器进行机制可解释性分析，追踪神经激活中特定语义概念的出现时机和位置。

Result: 发现了特征涌现的明确时间阈值和规模阈值，并观察到早期层特征在后期层重新激活的反常现象。

Conclusion: 研究挑战了Transformer模型表征动态的标准假设，揭示了特征涌现的复杂时空模式。

Abstract: This paper studies the emergence of interpretable categorical features within
large language models (LLMs), analyzing their behavior across training
checkpoints (time), transformer layers (space), and varying model sizes
(scale). Using sparse autoencoders for mechanistic interpretability, we
identify when and where specific semantic concepts emerge within neural
activations. Results indicate clear temporal and scale-specific thresholds for
feature emergence across multiple domains. Notably, spatial analysis reveals
unexpected semantic reactivation, with early-layer features re-emerging at
later layers, challenging standard assumptions about representational dynamics
in transformer models.

</details>


### [144] [Balancing Computation Load and Representation Expressivity in Parallel Hybrid Neural Networks](https://arxiv.org/abs/2505.19472)
*Mohammad Mahdi Moradi,Walid Ahmed,Shuangyue Wen,Sudhir Mudur,Weiwei Zhang,Yang Liu*

Main category: cs.CL

TL;DR: FlowHN提出了一种新型并行混合网络架构，通过动态令牌分配和分支输出融合，显著提升处理速度和模型精度。


<details>
  <summary>Details</summary>
Motivation: 现有混合序列架构中Transformer和SSM交替处理导致延迟增加和吞吐量下降，并行架构则面临分支输出差异大和计算负载平衡的挑战。

Method: FlowHN采用并行混合架构，包含FLOP感知的动态令牌分配策略和分支输出融合方法，以实现计算负载平衡和增强表示表达能力。

Result: 在自回归语言建模任务中，FlowHN在135M、350M和1B参数模型上表现优于其他方法，吞吐量提升4倍，MFU提升2倍。

Conclusion: FlowHN通过创新的并行设计和动态负载平衡策略，有效解决了混合架构中的瓶颈问题，同时提升了模型性能。

Abstract: Attention and State-Space Models (SSMs) when combined in a hybrid network in
sequence or in parallel provide complementary strengths. In a hybrid sequential
pipeline they alternate between applying a transformer to the input and then
feeding its output into a SSM. This results in idle periods in the individual
components increasing end-to-end latency and lowering throughput caps. In the
parallel hybrid architecture, the transformer operates independently in
parallel with the SSM, and these pairs are cascaded, with output from one pair
forming the input to the next. Two issues are (i) creating an expressive
knowledge representation with the inherently divergent outputs from these
separate branches, and (ii) load balancing the computation between these
parallel branches, while maintaining representation fidelity. In this work we
present FlowHN, a novel parallel hybrid network architecture that accommodates
various strategies for load balancing, achieved through appropriate
distribution of input tokens between the two branches. Two innovative
differentiating factors in FlowHN include a FLOP aware dynamic token split
between the attention and SSM branches yielding efficient balance in compute
load, and secondly, a method to fuse the highly divergent outputs from
individual branches for enhancing representation expressivity. Together they
enable much better token processing speeds, avoid bottlenecks, and at the same
time yield significantly improved accuracy as compared to other competing
works. We conduct comprehensive experiments on autoregressive language modeling
for models with 135M, 350M, and 1B parameters. FlowHN outperforms sequential
hybrid models and its parallel counterpart, achieving up to 4* higher Tokens
per Second (TPS) and 2* better Model FLOPs Utilization (MFU).

</details>


### [145] [Continuous Self-Improvement of Large Language Models by Test-time Training with Verifier-Driven Sample Selection](https://arxiv.org/abs/2505.19475)
*Mohammad Mahdi Moradi,Hossam Amer,Sudhir Mudur,Weiwei Zhang,Yang Liu,Walid Ahmed*

Main category: cs.CL

TL;DR: VDS-TTT框架通过验证器驱动样本选择，实现预训练语言模型在未标记、分布外数据上的高效自适应，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型在训练分布内表现优异，但在结构新颖的推理任务上表现不佳，需要一种高效的方法适应未标记的分布外数据。

Method: 使用验证器对生成的候选答案进行评分，选择高置信度的伪标记样本进行微调，仅调整低秩LoRA适配器参数以确保高效性。

Result: 在三个基准测试和三种先进LLM上，VDS-TTT相比基线模型提升32.29%，比无测试时训练的验证器方法提升6.66%。

Conclusion: VDS-TTT是一种高效的自监督框架，能持续提升模型性能，适用于大型语言模型的实时自适应。

Abstract: Learning to adapt pretrained language models to unlabeled,
out-of-distribution data is a critical challenge, as models often falter on
structurally novel reasoning tasks even while excelling within their training
distribution. We introduce a new framework called VDS-TTT - Verifier-Driven
Sample Selection for Test-Time Training to efficiently address this. We use a
learned verifier to score a pool of generated responses and select only from
high ranking pseudo-labeled examples for fine-tuned adaptation. Specifically,
for each input query our LLM generates N candidate answers; the verifier
assigns a reliability score to each, and the response with the highest
confidence and above a fixed threshold is paired with its query for test-time
training. We fine-tune only low-rank LoRA adapter parameters, ensuring
adaptation efficiency and fast convergence. Our proposed self-supervised
framework is the first to synthesize verifier driven test-time training data
for continuous self-improvement of the model. Experiments across three diverse
benchmarks and three state-of-the-art LLMs demonstrate that VDS-TTT yields up
to a 32.29% relative improvement over the base model and a 6.66% gain compared
to verifier-based methods without test-time training, highlighting its
effectiveness and efficiency for on-the-fly large language model adaptation.

</details>


### [146] [CulFiT: A Fine-grained Cultural-aware LLM Training Paradigm via Multilingual Critique Data Synthesis](https://arxiv.org/abs/2505.19484)
*Ruixiang Feng,Shen Gao,Xiuying Chen,Lisi Chen,Shuo Shang*

Main category: cs.CL

TL;DR: 论文提出CulFiT方法，通过多语言数据和细粒度奖励建模提升大语言模型的文化敏感性与包容性，并创建GlobalCultureQA数据集进行评估。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型存在文化偏见，忽视低资源地区的价值观和语言多样性，可能加剧刻板印象和歧视。

Method: 提出CulFiT训练范式，整合多元文化问题，构建多语言批评数据，并采用细粒度奖励分解文化文本为可验证知识单元。

Result: 在三个现有基准及自建GlobalCultureQA上的实验表明，CulFiT在文化对齐和通用推理方面达到开源模型最佳性能。

Conclusion: CulFiT有效提升了模型的文化包容性，GlobalCultureQA为全球语境下的文化敏感评估提供了新工具。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
various tasks, yet they often exhibit a specific cultural biases, neglecting
the values and linguistic diversity of low-resource regions. This cultural bias
not only undermines universal equality, but also risks reinforcing stereotypes
and perpetuating discrimination. To address this, we propose CulFiT, a novel
culturally-aware training paradigm that leverages multilingual data and
fine-grained reward modeling to enhance cultural sensitivity and inclusivity.
Our approach synthesizes diverse cultural-related questions, constructs
critique data in culturally relevant languages, and employs fine-grained
rewards to decompose cultural texts into verifiable knowledge units for
interpretable evaluation. We also introduce GlobalCultureQA, a multilingual
open-ended question-answering dataset designed to evaluate culturally-aware
responses in a global context. Extensive experiments on three existing
benchmarks and our GlobalCultureQA demonstrate that CulFiT achieves
state-of-the-art open-source model performance in cultural alignment and
general reasoning.

</details>


### [147] [Anveshana: A New Benchmark Dataset for Cross-Lingual Information Retrieval On English Queries and Sanskrit Documents](https://arxiv.org/abs/2505.19494)
*Manoj Balaji Jagadeeshan,Prince Raj,Pawan Goyal*

Main category: cs.CL

TL;DR: 该研究提出了一个全面的基准，用于使用英语查询检索梵文文档，重点关注《圣典博伽瓦谭》的章节。通过三种方法（直接检索、基于翻译的检索和查询翻译）以及先进的翻译技术，提升了RAG框架中的检索系统性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决跨语言检索古代梵文文本的挑战，提升梵文经典的访问和理解，以保存和传播其哲学重要性。

Method: 采用三种检索方法（DR、DT、QT），利用共享嵌入空间和先进翻译技术，微调了多种先进模型（如BM25、REPLUG等），并调整了梵文文档的摘要技术以优化问答处理。

Result: 评估显示，基于翻译的检索（DT）方法在处理古代文本的跨语言挑战上优于直接检索（DR）和查询翻译（QT），显著提升了检索效果。

Conclusion: 该研究不仅提升了梵文文档的检索效率，还通过公开数据集（3,400对英语-梵文查询文档）促进了梵文经典的保存和传播。

Abstract: The study presents a comprehensive benchmark for retrieving Sanskrit
documents using English queries, focusing on the chapters of the
Srimadbhagavatam. It employs a tripartite approach: Direct Retrieval (DR),
Translation-based Retrieval (DT), and Query Translation (QT), utilizing shared
embedding spaces and advanced translation methods to enhance retrieval systems
in a RAG framework. The study fine-tunes state-of-the-art models for Sanskrit's
linguistic nuances, evaluating models such as BM25, REPLUG, mDPR, ColBERT,
Contriever, and GPT-2. It adapts summarization techniques for Sanskrit
documents to improve QA processing. Evaluation shows DT methods outperform DR
and QT in handling the cross-lingual challenges of ancient texts, improving
accessibility and understanding. A dataset of 3,400 English-Sanskrit
query-document pairs underpins the study, aiming to preserve Sanskrit
scriptures and share their philosophical importance widely. Our dataset is
publicly available at https://huggingface.co/datasets/manojbalaji1/anveshana

</details>


### [148] [LLM Meets Scene Graph: Can Large Language Models Understand and Generate Scene Graphs? A Benchmark and Empirical Study](https://arxiv.org/abs/2505.19510)
*Dongil Yang,Minjin Kim,Sunghwan Kim,Beong-woo Kwak,Minjun Park,Jinseok Hong,Woontack Woo,Jinyoung Yeo*

Main category: cs.CL

TL;DR: 该论文介绍了TSG Bench基准，用于评估大语言模型在场景图理解和生成方面的能力，发现模型在复杂叙述的场景图生成上存在困难。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在具身AI和机器人等领域的应用扩展，需要其在多模态环境中具备空间和时间理解能力。场景图作为一种结构化表示方法，能够编码场景中的实体、属性及其关系，但目前对大语言模型利用场景图能力的全面评估仍有限。

Method: 论文提出了Text-Scene Graph (TSG) Bench基准，用于系统评估11种大语言模型在场景图理解和生成两方面的能力。

Result: 评估结果显示，大语言模型在场景图理解上表现良好，但在场景图生成（尤其是复杂叙述）上存在困难，主要问题在于无法有效分解复杂叙述中的离散场景。

Conclusion: 研究强调了改进场景图生成方法的必要性，并为未来研究提供了有价值的见解。TSG Bench的代码和评估数据已公开。

Abstract: The remarkable reasoning and generalization capabilities of Large Language
Models (LLMs) have paved the way for their expanding applications in embodied
AI, robotics, and other real-world tasks. To effectively support these
applications, grounding in spatial and temporal understanding in multimodal
environments is essential. To this end, recent works have leveraged scene
graphs, a structured representation that encodes entities, attributes, and
their relationships in a scene. However, a comprehensive evaluation of LLMs'
ability to utilize scene graphs remains limited. In this work, we introduce
Text-Scene Graph (TSG) Bench, a benchmark designed to systematically assess
LLMs' ability to (1) understand scene graphs and (2) generate them from textual
narratives. With TSG Bench we evaluate 11 LLMs and reveal that, while models
perform well on scene graph understanding, they struggle with scene graph
generation, particularly for complex narratives. Our analysis indicates that
these models fail to effectively decompose discrete scenes from a complex
narrative, leading to a bottleneck when generating scene graphs. These findings
underscore the need for improved methodologies in scene graph generation and
provide valuable insights for future research. The demonstration of our
benchmark is available at https://tsg-bench.netlify.app. Additionally, our code
and evaluation data are publicly available at
https://anonymous.4open.science/r/TSG-Bench.

</details>


### [149] [Causal Distillation: Transferring Structured Explanations from Large to Compact Language Models](https://arxiv.org/abs/2505.19511)
*Aggrey Muhebwa,Khalid K. Osman*

Main category: cs.CL

TL;DR: 大型专有语言模型展现出强大的因果推理能力，而小型开源模型难以复制。本文提出一种新框架，通过从强大的教师模型向紧凑的开源模型传递因果推理技能，训练小型模型生成与教师模型一致的因果解释。


<details>
  <summary>Details</summary>
Motivation: 小型开源模型在因果推理能力上表现不佳，难以与大型专有模型匹敌。本文旨在通过知识蒸馏方法，提升小型模型的因果推理能力。

Method: 提出一种新框架，通过训练小型模型生成与教师模型一致的因果解释来提升其推理能力，并引入CEC指标评估解释的结构和逻辑一致性。

Result: 提出的框架和CEC指标为训练小型模型进行稳健因果推理提供了理论基础，并能系统评估语言模型输出的解释连贯性。

Conclusion: 该研究为提升小型模型的因果推理能力提供了有效方法，并通过CEC指标实现了对解释连贯性的量化评估。

Abstract: Large proprietary language models exhibit strong causal reasoning abilities
that smaller open-source models struggle to replicate. We introduce a novel
framework for distilling causal explanations that transfers causal reasoning
skills from a powerful teacher model to a compact open-source model. The key
idea is to train the smaller model to develop causal reasoning abilities by
generating structured cause-and-effect explanations consistent with those of
the teacher model. To evaluate the quality of the student-generated
explanations, we introduce a new metric called Causal Explanation Coherence
(CEC) to assess the structural and logical consistency of causal reasoning.
This metric uses sentence-level semantic alignment to measure how well each
part of the generated explanation corresponds to the teacher's reference,
capturing both faithfulness and coverage of the underlying causal chain. Our
framework and the CEC metric provide a principled foundation for training
smaller models to perform robust causal reasoning and for systematically
assessing the coherence of explanations in language model outputs.

</details>


### [150] [SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback](https://arxiv.org/abs/2505.19514)
*Yaoning Yu,Ye Yu,Kai Wei,Haojing Luo,Haohan Wang*

Main category: cs.CL

TL;DR: SIPDO框架通过合成数据生成与提示优化的闭环反馈，提升大语言模型提示性能。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法依赖固定数据集且缺乏迭代改进支持，需动态优化方案。

Method: 结合合成数据生成器与提示优化器，通过反馈循环迭代增强提示质量。

Result: 在问答和推理任务中，SIPDO优于标准提示调优方法。

Conclusion: 将数据合成整合到提示学习流程可显著提升模型表现。

Abstract: Prompt quality plays a critical role in the performance of large language
models (LLMs), motivating a growing body of work on prompt optimization. Most
existing methods optimize prompts over a fixed dataset, assuming static input
distributions and offering limited support for iterative improvement. We
introduce SIPDO (Self-Improving Prompts through Data-Augmented Optimization), a
closed-loop framework for prompt learning that integrates synthetic data
generation into the optimization process. SIPDO couples a synthetic data
generator with a prompt optimizer, where the generator produces new examples
that reveal current prompt weaknesses and the optimizer incrementally refines
the prompt in response. This feedback-driven loop enables systematic
improvement of prompt performance without assuming access to external
supervision or new tasks. Experiments across question answering and reasoning
benchmarks show that SIPDO outperforms standard prompt tuning methods,
highlighting the value of integrating data synthesis into prompt learning
workflows.

</details>


### [151] [Bias in Political Dialogue: Tagging U.S. Presidential Debates with an Extended DAMSL Framework](https://arxiv.org/abs/2505.19515)
*Lavanya Prahallad,Radhika Mamidi*

Main category: cs.CL

TL;DR: 该论文通过BEADS框架分析了2024年美国总统辩论中特朗普的修辞策略，发现其主导了对抗性交流、选择性强调等关键类别。


<details>
  <summary>Details</summary>
Motivation: 研究旨在系统分析政治辩论中的偏见和对抗性话语特征，特别是特朗普在辩论中的修辞策略及其对观众感知的影响。

Method: 采用BEADS框架扩展DAMSL，结合人工标注和ChatGPT辅助标注，分析特朗普与拜登及哈里斯的辩论文本。

Result: 特朗普在对抗性交流、选择性强调、恐惧诉求等关键类别中表现突出，显示其使用情感化和对抗性修辞控制叙事。

Conclusion: BEADS框架为跨语言和跨政治背景的批判性话语分析提供了可扩展且可复现的方法。

Abstract: We present a critical discourse analysis of the 2024 U.S. presidential
debates, examining Donald Trump's rhetorical strategies in his interactions
with Joe Biden and Kamala Harris. We introduce a novel annotation framework,
BEADS (Bias Enriched Annotation for Dialogue Structure), which systematically
extends the DAMSL framework to capture bias driven and adversarial discourse
features in political communication. BEADS includes a domain and language
agnostic set of tags that model ideological framing, emotional appeals, and
confrontational tactics. Our methodology compares detailed human annotation
with zero shot ChatGPT assisted tagging on verified transcripts from the Trump
and Biden (19,219 words) and Trump and Harris (18,123 words) debates. Our
analysis shows that Trump consistently dominated in key categories: Challenge
and Adversarial Exchanges, Selective Emphasis, Appeal to Fear, Political Bias,
and Perceived Dismissiveness. These findings underscore his use of emotionally
charged and adversarial rhetoric to control the narrative and influence
audience perception. In this work, we establish BEADS as a scalable and
reproducible framework for critical discourse analysis across languages,
domains, and political contexts.

</details>


### [152] [AmpleHate: Amplifying the Attention for Versatile Implicit Hate Detection](https://arxiv.org/abs/2505.19528)
*Yejin Lee,Joonghyuk Hahn,Hyeseon Ahn,Yo-Sub Han*

Main category: cs.CL

TL;DR: AmpleHate提出了一种模仿人类推理过程的隐式仇恨言论检测方法，通过识别目标和上下文关系提升检测效果，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前隐式仇恨言论检测方法依赖对比学习，但人类是通过识别文本中的目标并分析其与上下文的关系来判断的。受此启发，作者提出了AmpleHate方法。

Method: AmpleHate使用预训练的命名实体识别模型识别显式目标，通过[CLS]标记捕获隐式目标信息，计算目标与上下文的注意力关系，并将这些关系向量注入句子表示中。

Result: 实验表明，AmpleHate在性能上平均优于对比学习方法82.14%，收敛速度更快，且其注意力模式与人类判断高度一致。

Conclusion: AmpleHate通过模仿人类推理过程，显著提升了隐式仇恨言论检测的性能和可解释性，成为当前最先进的方法。

Abstract: Implicit hate speech detection is challenging due to its subtlety and
reliance on contextual interpretation rather than explicit offensive words.
Current approaches rely on contrastive learning, which are shown to be
effective on distinguishing hate and non-hate sentences. Humans, however,
detect implicit hate speech by first identifying specific targets within the
text and subsequently interpreting how these target relate to their surrounding
context. Motivated by this reasoning process, we propose AmpleHate, a novel
approach designed to mirror human inference for implicit hate detection.
AmpleHate identifies explicit target using a pretrained Named Entity
Recognition model and capture implicit target information via [CLS] tokens. It
computes attention-based relationships between explicit, implicit targets and
sentence context and then, directly injects these relational vectors into the
final sentence representation. This amplifies the critical signals of
target-context relations for determining implicit hate. Experiments demonstrate
that AmpleHate achieves state-of-the-art performance, outperforming contrastive
learning baselines by an average of 82.14% and achieve faster convergence.
Qualitative analyses further reveal that attention patterns produced by
AmpleHate closely align with human judgement, underscoring its interpretability
and robustness.

</details>


### [153] [Small Language Models: Architectures, Techniques, Evaluation, Problems and Future Adaptation](https://arxiv.org/abs/2505.19529)
*Tanjil Hasan Sakib,Md. Tanzib Hosain,Md. Kishor Morol*

Main category: cs.CL

TL;DR: 本文综述了小语言模型（SLMs）的设计框架、训练方法及优化技术，提出新的分类系统并建立评估套件，探讨效率与性能的权衡，为构建高效紧凑的语言模型提供指导。


<details>
  <summary>Details</summary>
Motivation: 小语言模型（SLMs）因资源占用少且能执行多样语言任务而受到关注，适合部署在资源有限的环境中。本文旨在系统评估SLMs，解决其优化与评估中的挑战。

Method: 提出新的分类系统组织SLMs优化方法（如剪枝、量化和模型压缩），并整合现有数据集建立评估套件，系统分析SLMs的设计与训练技术。

Result: 建立了SLMs的优化分类体系和评估平台，明确了当前未解决的效率与性能权衡问题，为未来研究提供了方向。

Conclusion: 本文为研究人员和实践者构建高效紧凑的语言模型提供了系统指导，并指出了未来研究的潜在方向。

Abstract: Small Language Models (SLMs) have gained substantial attention due to their
ability to execute diverse language tasks successfully while using fewer
computer resources. These models are particularly ideal for deployment in
limited environments, such as mobile devices, on-device processing, and edge
systems. In this study, we present a complete assessment of SLMs, focussing on
their design frameworks, training approaches, and techniques for lowering model
size and complexity. We offer a novel classification system to organize the
optimization approaches applied for SLMs, encompassing strategies like pruning,
quantization, and model compression. Furthermore, we assemble SLM's studies of
evaluation suite with some existing datasets, establishing a rigorous platform
for measuring SLM capabilities. Alongside this, we discuss the important
difficulties that remain unresolved in this sector, including trade-offs
between efficiency and performance, and we suggest directions for future study.
We anticipate this study to serve as a beneficial guide for researchers and
practitioners who aim to construct compact, efficient, and high-performing
language models.

</details>


### [154] [DoctorRAG: Medical RAG Fusing Knowledge with Patient Analogy through Textual Gradients](https://arxiv.org/abs/2505.19538)
*Yuxing Lu,Gecheng Fu,Wei Wu,Xukai Zhao,Sin Yee Goi,Jinzhuo Wang*

Main category: cs.CL

TL;DR: DoctorRAG框架通过整合临床知识和类似病例经验，提升医学问答系统的准确性和相关性。


<details>
  <summary>Details</summary>
Motivation: 现有医学RAG系统主要依赖医学知识库，忽视了类似病例经验在临床推理中的关键作用。

Method: DoctorRAG通过概念标签分配、混合检索机制和Med-TextGrad模块，模拟医生推理过程。

Result: 在多语言、多任务数据集上，DoctorRAG显著优于基线模型，并通过迭代优化获得改进。

Conclusion: DoctorRAG生成的响应更准确、相关且全面，推动了类医生医学推理系统的发展。

Abstract: Existing medical RAG systems mainly leverage knowledge from medical knowledge
bases, neglecting the crucial role of experiential knowledge derived from
similar patient cases -- a key component of human clinical reasoning. To bridge
this gap, we propose DoctorRAG, a RAG framework that emulates doctor-like
reasoning by integrating both explicit clinical knowledge and implicit
case-based experience. DoctorRAG enhances retrieval precision by first
allocating conceptual tags for queries and knowledge sources, together with a
hybrid retrieval mechanism from both relevant knowledge and patient. In
addition, a Med-TextGrad module using multi-agent textual gradients is
integrated to ensure that the final output adheres to the retrieved knowledge
and patient query. Comprehensive experiments on multilingual, multitask
datasets demonstrate that DoctorRAG significantly outperforms strong baseline
RAG models and gains improvements from iterative refinements. Our approach
generates more accurate, relevant, and comprehensive responses, taking a step
towards more doctor-like medical reasoning systems.

</details>


### [155] [How Syntax Specialization Emerges in Language Models](https://arxiv.org/abs/2505.19548)
*Xufeng Duan,Zhaoqian Yao,Yunhao Zhang,Shaonan Wang,Zhenguang G. Cai*

Main category: cs.CL

TL;DR: 研究发现大语言模型在训练过程中会逐渐形成对句法结构的内部敏感度，这种敏感度集中在特定层并存在快速发展的‘关键期’，且受模型规模与训练数据影响。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型内部句法敏感性的形成过程及其影响因素，填补现有研究中对这一现象如何发展及受何因素影响的空白。

Method: 通过量化不同句法现象的最小对中的内部句法一致性，追踪句法敏感性在训练过程中的形成轨迹。

Result: 句法敏感性逐步形成，集中在特定层，并表现出快速发展的‘关键期’，这一过程在不同架构和初始化参数下一致，且受模型规模与训练数据影响。

Conclusion: 揭示了句法敏感性在大语言模型中的形成过程及其影响因素，为未来研究提供了基础。

Abstract: Large language models (LLMs) have been found to develop surprising internal
specializations: Individual neurons, attention heads, and circuits become
selectively sensitive to syntactic structure, reflecting patterns observed in
the human brain. While this specialization is well-documented, how it emerges
during training and what influences its development remains largely unknown.
  In this work, we tap into the black box of specialization by tracking its
formation over time. By quantifying internal syntactic consistency across
minimal pairs from various syntactic phenomena, we identify a clear
developmental trajectory: Syntactic sensitivity emerges gradually, concentrates
in specific layers, and exhibits a 'critical period' of rapid internal
specialization. This process is consistent across architectures and
initialization parameters (e.g., random seeds), and is influenced by model
scale and training data. We therefore reveal not only where syntax arises in
LLMs but also how some models internalize it during training. To support future
research, we will release the code, models, and training checkpoints upon
acceptance.

</details>


### [156] [Towards Multi-Granularity Memory Association and Selection for Long-Term Conversational Agents](https://arxiv.org/abs/2505.19549)
*Derong Xu,Yi Wen,Pengyue Jia,Yingyi Zhang,wenlin zhang,Yichao Wang,Huifeng Guo,Ruiming Tang,Xiangyu Zhao,Enhong Chen,Tong Xu*

Main category: cs.CL

TL;DR: 论文提出MemGAS框架，通过多粒度记忆关联与自适应检索优化LLMs的长时对话记忆性能。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强记忆系统因单粒度记忆分割与检索方式，难以平衡信息完整性与噪声干扰，导致长时对话效果不佳。

Method: MemGAS框架采用多粒度记忆单元，基于高斯混合模型聚类关联新旧记忆，通过熵路由自适应选择粒度，并利用LLMs过滤噪声。

Result: 在四个长时记忆基准测试中，MemGAS在问答和检索任务上均超越现有最优方法，适应不同查询类型和Top-K设置。

Conclusion: 多粒度关联与自适应检索机制能有效提升LLMs的长时对话记忆能力，为个性化响应生成提供新思路。

Abstract: Large Language Models (LLMs) have recently been widely adopted in
conversational agents. However, the increasingly long interactions between
users and agents accumulate extensive dialogue records, making it difficult for
LLMs with limited context windows to maintain a coherent long-term dialogue
memory and deliver personalized responses. While retrieval-augmented memory
systems have emerged to address this issue, existing methods often depend on
single-granularity memory segmentation and retrieval. This approach falls short
in capturing deep memory connections, leading to partial retrieval of useful
information or substantial noise, resulting in suboptimal performance. To
tackle these limits, we propose MemGAS, a framework that enhances memory
consolidation by constructing multi-granularity association, adaptive
selection, and retrieval. MemGAS is based on multi-granularity memory units and
employs Gaussian Mixture Models to cluster and associate new memories with
historical ones. An entropy-based router adaptively selects optimal granularity
by evaluating query relevance distributions and balancing information
completeness and noise. Retrieved memories are further refined via LLM-based
filtering. Experiments on four long-term memory benchmarks demonstrate that
MemGAS outperforms state-of-the-art methods on both question answer and
retrieval tasks, achieving superior performance across different query types
and top-K settings.

</details>


### [157] [DocMEdit: Towards Document-Level Model Editing](https://arxiv.org/abs/2505.19572)
*Li Zeng,Zeming Liu,Chong Feng,Heyan Huang,Yuhang Guo*

Main category: cs.CL

TL;DR: 该论文提出了文档级模型编辑任务，并引入了一个新数据集来评估现有方法在此任务上的表现，结果显示现有方法面临挑战。


<details>
  <summary>Details</summary>
Motivation: 现有模型编辑数据集大多仅关注短文本输出，忽视了现实中文档级任务的普遍存在，限制了模型编辑的实际应用。

Method: 提出了文档级模型编辑任务，并引入一个具有文档级输入输出、外推性和单次编辑多事实特点的数据集，设计了一系列评估指标和实验。

Result: 实验结果表明，文档级模型编辑的难度对现有模型编辑方法提出了挑战。

Conclusion: 文档级模型编辑任务具有实际意义，现有方法需进一步改进以适应这一任务需求。

Abstract: Model editing aims to correct errors and outdated knowledge in the Large
language models (LLMs) with minimal cost. Prior research has proposed a variety
of datasets to assess the effectiveness of these model editing methods.
However, most existing datasets only require models to output short phrases or
sentences, overlooks the widespread existence of document-level tasks in the
real world, raising doubts about their practical usability. Aimed at addressing
this limitation and promoting the application of model editing in real-world
scenarios, we propose the task of document-level model editing. To tackle such
challenges and enhance model capabilities in practical settings, we introduce
\benchmarkname, a dataset focused on document-level model editing,
characterized by document-level inputs and outputs, extrapolative, and multiple
facts within a single edit. We propose a series of evaluation metrics and
experiments. The results show that the difficulties in document-level model
editing pose challenges for existing model editing methods.

</details>


### [158] [TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV Cache Optimization](https://arxiv.org/abs/2505.19586)
*Dingyu Yao,Bowen Shen,Zheng Lin,Wei Liu,Jian Luan,Bin Wang,Weiping Wang*

Main category: cs.CL

TL;DR: TailorKV提出了一种混合压缩方法，结合量化和卸载技术，有效减少生成式大语言模型中的KV缓存内存开销，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 生成式大语言模型中的KV缓存导致显著的内存开销，现有方法通过卸载或压缩缓存来缓解这一问题，但存在延迟高或性能下降的问题。

Method: TailorKV结合了量化所有令牌和选择性加载主导令牌的方法，设计了一个推理框架和硬件友好的实现。

Result: 在激进压缩设置下，TailorKV实现了几乎无损的性能，Llama-3.1-8B模型在128k上下文下可在单个RTX 3090 GPU上运行，解码速度达82毫秒每令牌。

Conclusion: TailorKV通过混合压缩方法有效解决了KV缓存的内存和性能问题，优于现有技术。

Abstract: The Key-Value (KV) cache in generative large language models (LLMs)
introduces substantial memory overhead. Existing works mitigate this burden by
offloading or compressing the KV cache. However, loading the entire cache
incurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU
communication, while aggressive compression causes notable performance
degradation. We identify that certain layers in the LLM need to maintain global
information and are unsuitable for selective loading. In contrast, other layers
primarily focus on a few tokens with dominant activations that potentially
incur substantial quantization error. This observation leads to a key insight
that loading dominant tokens and quantizing all tokens can complement each
other. Building on this insight, we propose a hybrid compression method,
TailorKV, which seamlessly integrates quantization and offloading. TailorKV
develops an inference framework along with a hardware-friendly implementation
that leverages these complementary characteristics. Extensive long-context
evaluations exhibit that TailorKV achieves nearly lossless performance under
aggressive compression settings, outperforming the state-of-the-art.
Particularly, the Llama-3.1-8B with 128k context can be served within a single
RTX 3090 GPU, reaching 82 ms per token during decoding.

</details>


### [159] [Multi-Agent Collaboration via Evolving Orchestration](https://arxiv.org/abs/2505.19591)
*Yufan Dang,Chen Qian,Xueheng Luo,Jingru Fan,Zihao Xie,Ruijie Shi,Weize Chen,Cheng Yang,Xiaoyin Che,Ye Tian,Xuantang Xiong,Lei Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 提出了一种基于强化学习的集中式编排器动态调度多智能体的方法，以提升大语言模型协作效率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型多智能体协作方法依赖静态组织结构，难以适应任务复杂性和智能体数量增长，导致协调效率低下。

Method: 采用'傀儡师-傀儡'范式，通过强化学习训练集中式编排器动态调度智能体，形成自适应推理结构。

Result: 实验表明该方法在封闭/开放域场景中均能以更低计算成本实现更优性能，关键改进源于演化出的紧凑循环推理结构。

Conclusion: 动态编排机制能有效提升多智能体协作的适应性和效率，为复杂问题求解提供新思路。

Abstract: Large language models (LLMs) have achieved remarkable results across diverse
downstream tasks, but their monolithic nature restricts scalability and
efficiency in complex problem-solving. While recent research explores
multi-agent collaboration among LLMs, most approaches rely on static
organizational structures that struggle to adapt as task complexity and agent
numbers grow, resulting in coordination overhead and inefficiencies. To this
end, we propose a puppeteer-style paradigm for LLM-based multi-agent
collaboration, where a centralized orchestrator ("puppeteer") dynamically
directs agents ("puppets") in response to evolving task states. This
orchestrator is trained via reinforcement learning to adaptively sequence and
prioritize agents, enabling flexible and evolvable collective reasoning.
Experiments on closed- and open-domain scenarios show that this method achieves
superior performance with reduced computational costs. Analyses further reveal
that the key improvements consistently stem from the emergence of more compact,
cyclic reasoning structures under the orchestrator's evolution.

</details>


### [160] [Evaluating Robustness of Large Audio Language Models to Audio Injection: An Empirical Study](https://arxiv.org/abs/2505.19598)
*Guanyu Hou,Jiaming He,Yinhang Zhou,Ji Guo,Yitong Qiao,Rui Zhang,Wenbo Jiang*

Main category: cs.CL

TL;DR: 该研究系统评估了五种大型音频语言模型（LALMs）在四种恶意音频注入攻击场景下的鲁棒性，发现模型间存在显著性能差异，且攻击效果受恶意内容位置影响较大。


<details>
  <summary>Details</summary>
Motivation: 尽管大型音频语言模型（LALMs）在现实应用中日益普及，但其对恶意音频注入攻击的鲁棒性尚未得到充分研究。本文旨在填补这一空白，为安全部署LALMs提供依据。

Method: 研究采用四种攻击场景（音频干扰攻击、指令跟随攻击、上下文注入攻击和判断劫持攻击），并使用防御成功率、上下文鲁棒性评分和判断鲁棒性指数等指标，定量评估了五种领先LALMs的脆弱性和韧性。

Result: 实验结果显示，不同模型间存在显著性能差异，且没有单一模型在所有攻击类型中表现最优。恶意内容的位置（尤其是序列开头）对攻击效果有重要影响。指令跟随能力与鲁棒性呈负相关，而安全对齐模型表现出更强的抵抗力。系统提示的效果参差不齐。

Conclusion: 本研究提出了一个基准框架，强调了将鲁棒性整合到训练流程中的重要性。研究结果呼吁开发多模态防御和架构设计，以分离模型能力与易感性，确保LALMs的安全部署。

Abstract: Large Audio-Language Models (LALMs) are increasingly deployed in real-world
applications, yet their robustness against malicious audio injection attacks
remains underexplored. This study systematically evaluates five leading LALMs
across four attack scenarios: Audio Interference Attack, Instruction Following
Attack, Context Injection Attack, and Judgment Hijacking Attack. Using metrics
like Defense Success Rate, Context Robustness Score, and Judgment Robustness
Index, their vulnerabilities and resilience were quantitatively assessed.
Experimental results reveal significant performance disparities among models;
no single model consistently outperforms others across all attack types. The
position of malicious content critically influences attack effectiveness,
particularly when placed at the beginning of sequences. A negative correlation
between instruction-following capability and robustness suggests models
adhering strictly to instructions may be more susceptible, contrasting with
greater resistance by safety-aligned models. Additionally, system prompts show
mixed effectiveness, indicating the need for tailored strategies. This work
introduces a benchmark framework and highlights the importance of integrating
robustness into training pipelines. Findings emphasize developing multi-modal
defenses and architectural designs that decouple capability from susceptibility
for secure LALMs deployment.

</details>


### [161] [Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese Grammar](https://arxiv.org/abs/2505.19599)
*Andrew Gambardella,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.CL

TL;DR: 论文探讨了语言模型在非英语语言中识别罕见语法点的能力，特别关注日语中的'第一人称心理谓词限制'，发现Weblab模型表现最佳，并指出tokenization对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估方法主要关注通用文本理解能力，但忽视了模型在非英语语言中识别罕见语法点的能力。本文旨在填补这一空白。

Method: 通过测量语言模型在日语'第一人称心理谓词限制'语法点上的困惑度，比较不同模型的表现，并分析tokenization对结果的影响。

Result: Weblab是唯一在7-10B参数范围内能一致区分语法正确与错误句子的开源模型。研究还发现，tokenization问题会显著影响模型性能，如Llama 3的困惑度可通过优化tokenization降低28倍。

Conclusion: 语言模型在非英语语法任务中的表现受tokenization质量影响显著，优化tokenization可大幅提升模型性能。

Abstract: Typical methods for evaluating the performance of language models evaluate
their ability to answer questions accurately. These evaluation metrics are
acceptable for determining the extent to which language models can understand
and reason about text in a general sense, but fail to capture nuanced
capabilities, such as the ability of language models to recognize and obey rare
grammar points, particularly in languages other than English. We measure the
perplexity of language models when confronted with the "first person psych
predicate restriction" grammar point in Japanese. Weblab is the only tested
open source model in the 7-10B parameter range which consistently assigns
higher perplexity to ungrammatical psych predicate sentences than grammatical
ones. We give evidence that Weblab's uniformly bad tokenization is a possible
root cause for its good performance, and show that Llama 3's perplexity on
grammatical psych predicate sentences can be reduced by orders of magnitude
(28x difference) by restricting test sentences to those with uniformly
well-behaved tokenizations. We show in further experiments on machine
translation tasks that language models will use alternative grammar patterns in
order to produce grammatical sentences when tokenization issues prevent the
most natural sentence from being output.

</details>


### [162] [Evaluating Machine Translation Models for English-Hindi Language Pairs: A Comparative Analysis](https://arxiv.org/abs/2505.19604)
*Ahan Prasannakumar Shetty*

Main category: cs.CL

TL;DR: 该论文全面评估了多种英印机器翻译模型，使用多样化自动评估指标，基于18,000+平行语料库和定制FAQ数据集，揭示了不同模型在通用和专业领域的表现差异。


<details>
  <summary>Details</summary>
Motivation: 机器翻译在弥合英语和印地语等语言差异中至关重要，但现有系统表现参差不齐。本研究旨在评估不同模型在通用和专业领域的有效性，为改进提供依据。

Method: 采用18,000+英印平行语料库和政府网站FAQ定制数据集，结合词汇型和机器学习型自动评估指标，对多种机器翻译模型进行综合性能评估。

Result: 不同翻译模型在各评估指标下表现差异显著，部分模型在特定领域（如FAQ）表现突出，但整体仍有改进空间。

Conclusion: 当前英印机器翻译系统在通用和专业领域呈现不均衡性能，需针对不同场景优化模型，未来可结合领域适配技术提升效果。

Abstract: Machine translation has become a critical tool in bridging linguistic gaps,
especially between languages as diverse as English and Hindi. This paper
comprehensively evaluates various machine translation models for translating
between English and Hindi. We assess the performance of these models using a
diverse set of automatic evaluation metrics, both lexical and machine
learning-based metrics. Our evaluation leverages an 18000+ corpus of English
Hindi parallel dataset and a custom FAQ dataset comprising questions from
government websites. The study aims to provide insights into the effectiveness
of different machine translation approaches in handling both general and
specialized language domains. Results indicate varying performance levels
across different metrics, highlighting strengths and areas for improvement in
current translation systems.

</details>


### [163] [Languages in Multilingual Speech Foundation Models Align Both Phonetically and Semantically](https://arxiv.org/abs/2505.19606)
*Ryan Soh-Eun Shim,Domenico De Cristofaro,Chengzhi Martin Hu,Alessandro Vietti,Barbara Plank*

Main category: cs.CL

TL;DR: 研究发现，语音基础模型中的跨语言对齐不仅依赖语音相似性，还存在语义对齐。通过控制发音实验和早期退出编码器分析，证实了模型同时具备语音和语义知识，并成功应用于低资源语言的语音识别改进。


<details>
  <summary>Details</summary>
Motivation: 探讨文本基础的跨语言对齐方法是否适用于语音模型，验证语音基础模型中是否存在独立于语音相似性的语义对齐。

Method: 通过发音控制实验分析口语翻译检索的准确性，使用跨语言同义词和近音词数据集进行验证，并采用早期退出编码器策略观察转录错误模式。

Result: 即使没有语音线索，口语翻译检索准确率仍保持稳定；编码器同时编码了语音和语义知识；基于早期退出的方法在7种低资源语言中提升了语音识别准确率。

Conclusion: 语音基础模型存在跨语言的语义对齐，这种特性可用于改进低资源语言的语音识别性能，尤其在拼写透明的语言中效果显著。

Abstract: Cross-lingual alignment in pretrained language models (LMs) has enabled
efficient transfer in text-based LMs. Such an alignment has also been observed
in speech foundation models. However, it remains an open question whether
findings and methods from text-based cross-lingual alignment apply to speech.
Building on prior work on spoken translation retrieval, we perform
pronunciation-controlled experiments to observe if cross-lingual alignment can
indeed occur in such models on a semantic basis, instead of relying on phonetic
similarities. Our findings indicate that even in the absence of phonetic cues,
spoken translation retrieval accuracy remains relatively stable. We follow up
with a controlled experiment on a word-level dataset of cross-lingual synonyms
and near-homophones, confirming the existence of both phonetic and semantic
knowledge in the encoder. Finally, we qualitatively examine the transcriptions
produced by early exiting the encoder, where we observe that speech translation
produces semantic errors that are characterized by phonetic similarities to
corresponding words in the source language. We apply this insight from early
exiting to speech recognition in seven low-resource languages unsupported by
the Whisper model, and achieve improved accuracy in all languages examined,
particularly for languages with transparent orthographies.

</details>


### [164] [HomeBench: Evaluating LLMs in Smart Homes with Valid and Invalid Instructions Across Single and Multiple Devices](https://arxiv.org/abs/2505.19628)
*Silin Li,Yuhang Guo,Jiashu Yao,Zeming Liu,Haifeng Wang*

Main category: cs.CL

TL;DR: 论文提出首个包含单设备/多设备、有效/无效指令的智能家居数据集HomeBench，测试发现现有LLM（如GPT-4o）在无效多设备指令场景成功率仅0.0%。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅关注简单单设备指令，而真实场景涉及复杂无效指令和多设备控制，需提升LLM的纠错和多指令执行能力。

Method: 构建HomeBench数据集，包含单/多设备、有效/无效指令组合，并测试13种LLM在多种技术（上下文学习、检索增强生成、微调）下的表现。

Result: 实验显示GPT-4o在无效多设备指令场景成功率为0.0%，现有技术无法有效解决此类复杂问题。

Conclusion: 基于LLM的智能家居助手仍需突破复杂指令处理能力，HomeBench为后续研究提供了基准数据集。

Abstract: Large language models (LLMs) have the potential to revolutionize smart home
assistants by enhancing their ability to accurately understand user needs and
respond appropriately, which is extremely beneficial for building a smarter
home environment. While recent studies have explored integrating LLMs into
smart home systems, they primarily focus on handling straightforward, valid
single-device operation instructions. However, real-world scenarios are far
more complex and often involve users issuing invalid instructions or
controlling multiple devices simultaneously. These have two main challenges:
LLMs must accurately identify and rectify errors in user instructions and
execute multiple user instructions perfectly. To address these challenges and
advance the development of LLM-based smart home assistants, we introduce
HomeBench, the first smart home dataset with valid and invalid instructions
across single and multiple devices in this paper. We have experimental results
on 13 distinct LLMs; e.g., GPT-4o achieves only a 0.0% success rate in the
scenario of invalid multi-device instructions, revealing that the existing
state-of-the-art LLMs still cannot perform well in this situation even with the
help of in-context learning, retrieval-augmented generation, and fine-tuning.
Our code and dataset are publicly available at
https://github.com/BITHLP/HomeBench.

</details>


### [165] [DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue](https://arxiv.org/abs/2505.19630)
*Yichun Feng,Jiawei Wang,Lu Zhou,Yixue Li*

Main category: cs.CL

TL;DR: 论文提出DoctorAgent-RL框架，通过强化学习优化医疗咨询中的多轮对话策略，提升诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 现有医疗问答系统依赖单轮信息传递，对模糊症状处理不佳；传统多轮对话方法泛化性差，难以动态提取关键临床信息。

Method: 基于强化学习的多智能体协作框架（DoctorAgent-RL），将问诊建模为不确定条件下的动态决策过程，通过咨询评估器的综合奖励动态调整提问策略。

Result: 实验表明DoctorAgent-RL在多轮推理和最终诊断性能上优于现有模型，并构建了首个英文多轮医疗对话数据集MTMedDialog。

Conclusion: 该框架使大语言模型能自主开发符合临床逻辑的交互策略，在辅助实际临床咨询中具有实用价值。

Abstract: Large language models (LLMs) have demonstrated excellent capabilities in the
field of biomedical question answering, but their application in real-world
clinical consultations still faces core challenges. Existing systems rely on a
one-way information transmission mode where patients must fully describe their
symptoms in a single round, leading to nonspecific diagnostic recommendations
when complaints are vague. Traditional multi-turn dialogue methods based on
supervised learning are constrained by static data-driven paradigms, lacking
generalizability and struggling to intelligently extract key clinical
information. To address these limitations, we propose DoctorAgent-RL, a
reinforcement learning (RL)-based multi-agent collaborative framework that
models medical consultations as a dynamic decision-making process under
uncertainty. The doctor agent continuously optimizes its questioning strategy
within the RL framework through multi-turn interactions with the patient agent,
dynamically adjusting its information-gathering path based on comprehensive
rewards from the Consultation Evaluator. This RL fine-tuning mechanism enables
LLMs to autonomously develop interaction strategies aligned with clinical
reasoning logic, rather than superficially imitating patterns in existing
dialogue data. Notably, we constructed MTMedDialog, the first English
multi-turn medical consultation dataset capable of simulating patient
interactions. Experiments demonstrate that DoctorAgent-RL outperforms existing
models in both multi-turn reasoning capability and final diagnostic
performance, demonstrating practical value in assisting clinical consultations.
https://github.com/JarvisUSTC/DoctorAgent-RL

</details>


### [166] [Segment First or Comprehend First? Explore the Limit of Unsupervised Word Segmentation with Large Language Models](https://arxiv.org/abs/2505.19631)
*Zihong Zhang,Liqi He,Zuchao Li,Lefei Zhang,Hai Zhao,Bo Du*

Main category: cs.CL

TL;DR: 该论文提出了一种基于"先理解后分词"理念的新框架，利用大语言模型（LLMs）探索无监督分词极限，并评估LLMs在分词任务中的语义理解能力。通过多语言实验发现，LLMs能遵循简单指令完成分词，且参数量更大的模型表现更优。同时提出创新方法LLACA，结合Aho-Corasick自动机与LLMs的深度洞察，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索大语言模型在无监督分词任务中的语义理解能力极限，突破传统分词方法对标注数据的依赖，通过"先理解后分词"的新范式提升分词性能。

Method: 1) 使用主流LLMs进行多语言无监督分词实验 2) 提出LLACA方法：将Aho-Corasick自动机的模式识别能力与预训练LLMs的深度理解相结合，构建动态n-gram模型。

Result: 实验表明：1) LLMs能有效响应简单指令完成原始文本分词 2) 模型参数量与多语言分词性能呈正相关 3) LLACA方法相较传统方法有显著提升。

Conclusion: 论文证实了LLMs在无监督分词任务中的潜力，提出的LLACA框架通过结合经典算法与LLMs优势，为自然语言处理基础任务提供了新思路，代码已开源。

Abstract: Word segmentation stands as a cornerstone of Natural Language Processing
(NLP). Based on the concept of "comprehend first, segment later", we propose a
new framework to explore the limit of unsupervised word segmentation with Large
Language Models (LLMs) and evaluate the semantic understanding capabilities of
LLMs based on word segmentation. We employ current mainstream LLMs to perform
word segmentation across multiple languages to assess LLMs' "comprehension".
Our findings reveal that LLMs are capable of following simple prompts to
segment raw text into words. There is a trend suggesting that models with more
parameters tend to perform better on multiple languages. Additionally, we
introduce a novel unsupervised method, termed LLACA ($\textbf{L}$arge
$\textbf{L}$anguage Model-Inspired $\textbf{A}$ho-$\textbf{C}$orasick
$\textbf{A}$utomaton). Leveraging the advanced pattern recognition capabilities
of Aho-Corasick automata, LLACA innovatively combines these with the deep
insights of well-pretrained LLMs. This approach not only enables the
construction of a dynamic $n$-gram model that adjusts based on contextual
information but also integrates the nuanced understanding of LLMs, offering
significant improvements over traditional methods. Our source code is available
at https://github.com/hkr04/LLACA

</details>


### [167] [Faster and Better LLMs via Latency-Aware Test-Time Scaling](https://arxiv.org/abs/2505.19634)
*Zili Wang,Tianyu Zhang,Haoli Bai,Lu Hou,Xianzhi Yu,Wulong Liu,Shiming Xiang,Lei Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种延迟最优的测试时缩放（TTS）方法，通过并行化配置优化，在保证精度的同时显著降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽略了测试时缩放（TTS）在延迟敏感场景下的效率问题，计算最优的TTS并不总是能实现最低延迟。

Method: 提出两种并行化优化方法：分支级并行（利用多个并发推理分支）和序列级并行（基于推测解码），并通过合理分配计算资源实现延迟最优。

Result: 32B模型在1分钟内达到MATH-500 82.3%准确率，3B模型在10秒内达到72.4%准确率。

Conclusion: 延迟感知的TTS在延迟敏感场景中能同时兼顾速度与精度，具有重要实践意义。

Abstract: Test-Time Scaling (TTS) has proven effective in improving the performance of
Large Language Models (LLMs) during inference. However, existing research has
overlooked the efficiency of TTS from a latency-sensitive perspective. Through
a latency-aware evaluation of representative TTS methods, we demonstrate that a
compute-optimal TTS does not always result in the lowest latency in scenarios
where latency is critical. To address this gap and achieve latency-optimal TTS,
we propose two key approaches by optimizing the concurrency configurations: (1)
branch-wise parallelism, which leverages multiple concurrent inference
branches, and (2) sequence-wise parallelism, enabled by speculative decoding.
By integrating these two approaches and allocating computational resources
properly to each, our latency-optimal TTS enables a 32B model to reach 82.3%
accuracy on MATH-500 within 1 minute and a smaller 3B model to achieve 72.4%
within 10 seconds. Our work emphasizes the importance of latency-aware TTS and
demonstrates its ability to deliver both speed and accuracy in
latency-sensitive scenarios.

</details>


### [168] [Interleaved Reasoning for Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2505.19640)
*Roy Xie,David Qiu,Deepak Gopinath,Dong Lin,Yanchao Sun,Chong Wang,Saloni Potdar,Bhuwan Dhingra*

Main category: cs.CL

TL;DR: 论文提出了一种基于强化学习的训练范式，通过交替思考和回答来提升大语言模型的多跳推理效率，显著降低首令牌时间并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 长链思维（CoT）虽能增强大语言模型的推理能力，但冗长的推理过程导致效率低下和首令牌时间（TTFT）增加。因此，需要一种更高效的方法来优化推理过程。

Method: 使用强化学习（RL）引导模型在回答多跳问题时交替进行思考和回答，通过简单的基于规则的奖励机制激励正确的中间步骤，从而优化推理路径。

Result: 实验表明，该方法在五个数据集和三种RL算法上均优于传统思维-回答推理，平均降低TTFT超过80%，Pass@1准确率最高提升19.3%，并在复杂推理数据集上表现出强泛化能力。

Conclusion: 该方法通过强化学习有效提升了推理效率与准确性，且无需外部工具，展现了在复杂推理任务中的广泛适用性。

Abstract: Long chain-of-thought (CoT) significantly enhances large language models'
(LLM) reasoning capabilities. However, the extensive reasoning traces lead to
inefficiencies and an increased time-to-first-token (TTFT). We propose a novel
training paradigm that uses reinforcement learning (RL) to guide reasoning LLMs
to interleave thinking and answering for multi-hop questions. We observe that
models inherently possess the ability to perform interleaved reasoning, which
can be further enhanced through RL. We introduce a simple yet effective
rule-based reward to incentivize correct intermediate steps, which guides the
policy model toward correct reasoning paths by leveraging intermediate signals
generated during interleaved reasoning. Extensive experiments conducted across
five diverse datasets and three RL algorithms (PPO, GRPO, and REINFORCE++)
demonstrate consistent improvements over traditional think-answer reasoning,
without requiring external tools. Specifically, our approach reduces TTFT by
over 80% on average and improves up to 19.3% in Pass@1 accuracy. Furthermore,
our method, trained solely on question answering and logical reasoning
datasets, exhibits strong generalization ability to complex reasoning datasets
such as MATH, GPQA, and MMLU. Additionally, we conduct in-depth analysis to
reveal several valuable insights into conditional reward modeling.

</details>


### [169] [Select, Read, and Write: A Multi-Agent Framework of Full-Text-based Related Work Generation](https://arxiv.org/abs/2505.19647)
*Xiaochuan Liu,Ruihua Song,Xiting Wang,Xu Chen*

Main category: cs.CL

TL;DR: 本文提出了一种基于全文的多智能体框架，用于自动生成相关工作章节，通过优化阅读顺序和捕捉文献间关系，显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的相关工作自动生成方法存在理解浅显和文献间关系捕捉不足的问题，限制了生成质量。

Method: 提出一个由选择器、阅读器和写作者三个智能体组成的框架，并引入两种图感知策略优化阅读顺序。

Result: 实验表明，该框架在三种基础模型和多种输入配置下均表现优异，图感知选择器达到了最先进水平。

Conclusion: 该框架通过多智能体协作和图感知策略，有效提升了相关工作章节的自动生成质量。

Abstract: Automatic related work generation (RWG) can save people's time and effort
when writing a draft of related work section (RWS) for further revision.
However, existing methods for RWG always suffer from shallow comprehension due
to taking the limited portions of references papers as input and isolated
explanation for each reference due to ineffective capturing the relationships
among them. To address these issues, we focus on full-text-based RWG task and
propose a novel multi-agent framework. Our framework consists of three agents:
a selector that decides which section of the papers is going to read next, a
reader that digests the selected section and updates a shared working memory,
and a writer that generates RWS based on the final curated memory. To better
capture the relationships among references, we also propose two graph-aware
strategies for selector, enabling to optimize the reading order with constrains
of the graph structure. Extensive experiments demonstrate that our framework
consistently improves performance across three base models and various input
configurations. The graph-aware selectors outperform alternative selectors,
achieving state-of-the-art results. The code and data are available at
https://github.com/1190200817/Full_Text_RWG.

</details>


### [170] [GenKI: Enhancing Open-Domain Question Answering with Knowledge Integration and Controllable Generation in Large Language Models](https://arxiv.org/abs/2505.19660)
*Tingjia Shen,Hao Wang,Chuan Qin,Ruijun Sun,Yang Song,Defu Lian,Hengshu Zhu,Enhong Chen*

Main category: cs.CL

TL;DR: 本文提出GenKI框架，通过同时探索知识整合和可控生成，提升开放域问答性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在开放域问答中表现优异，但面临知识整合和可控生成两大挑战。

Method: GenKI框架结合密集段落检索模型和知识整合模型，并通过微调实现可控生成。

Result: 在TriviaQA等数据集上的实验表明，GenKI优于现有基线方法。

Conclusion: GenKI有效解决了知识整合和可控生成问题，提升了开放域问答性能。

Abstract: Open-domain question answering (OpenQA) represents a cornerstone in natural
language processing (NLP), primarily focused on extracting answers from
unstructured textual data. With the rapid advancements in Large Language Models
(LLMs), LLM-based OpenQA methods have reaped the benefits of emergent
understanding and answering capabilities enabled by massive parameters compared
to traditional methods. However, most of these methods encounter two critical
challenges: how to integrate knowledge into LLMs effectively and how to
adaptively generate results with specific answer formats for various task
situations. To address these challenges, we propose a novel framework named
GenKI, which aims to improve the OpenQA performance by exploring Knowledge
Integration and controllable Generation on LLMs simultaneously. Specifically,
we first train a dense passage retrieval model to retrieve associated knowledge
from a given knowledge base. Subsequently, we introduce a novel knowledge
integration model that incorporates the retrieval knowledge into instructions
during fine-tuning to intensify the model. Furthermore, to enable controllable
generation in LLMs, we leverage a certain fine-tuned LLM and an ensemble based
on text consistency incorporating all coherence, fluency, and answer format
assurance. Finally, extensive experiments conducted on the TriviaQA, MSMARCO,
and CMRC2018 datasets, featuring diverse answer formats, have demonstrated the
effectiveness of GenKI with comparison of state-of-the-art baselines. Moreover,
ablation studies have disclosed a linear relationship between the frequency of
retrieved knowledge and the model's ability to recall knowledge accurately
against the ground truth. Our code of GenKI is available at
https://github.com/USTC-StarTeam/GenKI

</details>


### [171] [LeCoDe: A Benchmark Dataset for Interactive Legal Consultation Dialogue Evaluation](https://arxiv.org/abs/2505.19667)
*Weikang Yuan,Kaisong Song,Zhuoren Jiang,Junjie Cao,Yujie Zhang,Jun Lin,Kun Kuang,Ji Zhang,Xiaozhong Liu*

Main category: cs.CL

TL;DR: 该论文介绍了LeCoDe数据集，用于评估和改进大语言模型在法律咨询中的能力，揭示了当前模型在专业咨询场景中的局限性，并提出了改进策略。


<details>
  <summary>Details</summary>
Motivation: 法律咨询对保障个人权利和司法公正至关重要，但由于专业人员短缺，许多人难以获得高质量的法律服务。尽管大语言模型（LLMs）为低成本、可扩展的法律援助提供了可能，但现有系统在处理真实咨询的交互性和知识密集性方面仍有不足。

Method: 论文提出了LeCoDe数据集，包含3,696个法律咨询对话和110,008个对话轮次，通过短视频平台收集真实的多轮咨询对话，并由法律专家进行严格标注。此外，论文还提出了一个包含12个指标的综合评估框架，从澄清能力和专业建议质量两个维度评估LLMs的表现。

Result: 实验结果表明，即使是GPT-4这样的先进模型，在澄清能力上的召回率仅为39.8%，在建议质量上的总体得分为59%，突显了专业咨询场景的复杂性。

Conclusion: LeCoDe数据集和评估框架为法律领域对话系统的研究提供了重要支持，尤其是在模拟真实用户与专家交互方面。论文还探讨了提升LLMs法律咨询能力的策略，为未来研究指明了方向。

Abstract: Legal consultation is essential for safeguarding individual rights and
ensuring access to justice, yet remains costly and inaccessible to many
individuals due to the shortage of professionals. While recent advances in
Large Language Models (LLMs) offer a promising path toward scalable, low-cost
legal assistance, current systems fall short in handling the interactive and
knowledge-intensive nature of real-world consultations. To address these
challenges, we introduce LeCoDe, a real-world multi-turn benchmark dataset
comprising 3,696 legal consultation dialogues with 110,008 dialogue turns,
designed to evaluate and improve LLMs' legal consultation capability. With
LeCoDe, we innovatively collect live-streamed consultations from short-video
platforms, providing authentic multi-turn legal consultation dialogues. The
rigorous annotation by legal experts further enhances the dataset with
professional insights and expertise. Furthermore, we propose a comprehensive
evaluation framework that assesses LLMs' consultation capabilities in terms of
(1) clarification capability and (2) professional advice quality. This unified
framework incorporates 12 metrics across two dimensions. Through extensive
experiments on various general and domain-specific LLMs, our results reveal
significant challenges in this task, with even state-of-the-art models like
GPT-4 achieving only 39.8% recall for clarification and 59% overall score for
advice quality, highlighting the complexity of professional consultation
scenarios. Based on these findings, we further explore several strategies to
enhance LLMs' legal consultation abilities. Our benchmark contributes to
advancing research in legal domain dialogue systems, particularly in simulating
more real-world user-expert interactions.

</details>


### [172] [Reshaping Representation Space to Balance the Safety and Over-rejection in Large Audio Language Models](https://arxiv.org/abs/2505.19670)
*Hao Yang,Lizhen Qu,Ehsan Shareghi,Gholamreza Haffari*

Main category: cs.CL

TL;DR: 本文提出了一种无监督安全微调策略，用于增强大型音频语言模型（LALMs）的安全对齐，同时避免过度拒绝问题。


<details>
  <summary>Details</summary>
Motivation: 当前LALMs在安全对齐方面存在不足，容易受到有害查询的影响，且缺乏针对性的安全数据集和有效防御策略。

Method: 采用无监督安全微调策略，通过重塑模型的表示空间来提升安全对齐，同时平衡过度拒绝的风险。

Result: 实验表明，该方法在三种输入模态（音频-文本、纯文本、纯音频）下显著提升了LALMs的安全性，平均过度拒绝率仅增加0.88%。

Conclusion: 该策略有效提升了LALMs的安全性，且对模型的有用性影响极小。

Abstract: Large Audio Language Models (LALMs) have extended the capabilities of Large
Language Models (LLMs) by enabling audio-based human interactions. However,
recent research has revealed that LALMs remain vulnerable to harmful queries
due to insufficient safety-alignment. Despite advances in defence measures for
text and vision LLMs, effective safety-alignment strategies and audio-safety
dataset specifically targeting LALMs are notably absent. Meanwhile defence
measures based on Supervised Fine-tuning (SFT) struggle to address safety
improvement while avoiding over-rejection issues, significantly compromising
helpfulness. In this work, we propose an unsupervised safety-fine-tuning
strategy as remedy that reshapes model's representation space to enhance
existing LALMs safety-alignment while balancing the risk of over-rejection. Our
experiments, conducted across three generations of Qwen LALMs, demonstrate that
our approach significantly improves LALMs safety under three modality input
conditions (audio-text, text-only, and audio-only) while increasing
over-rejection rate by only 0.88% on average. Warning: this paper contains
harmful examples.

</details>


### [173] [Comparing Moral Values in Western English-speaking societies and LLMs with Word Associations](https://arxiv.org/abs/2505.19674)
*Chaoyi Xiang,Chunhua Liu,Simon De Deyne,Lea Frermann*

Main category: cs.CL

TL;DR: 该论文通过词联想方法研究大型语言模型（LLMs）与英语母语者的道德价值观差异，提出了一种基于道德基础理论的新方法来传播道德价值观，并比较了两者的道德概念化差异。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型影响力的增加，理解它们所反映的道德价值观变得尤为重要。由于直接提示可能泄露人类规范到模型训练数据中，且对提示表述敏感，因此需要一种更稳健的方法来评估这些模型的道德价值观。

Method: 论文首先创建了一个大型的LLM生成的词联想数据集，类似于现有的人类词联想数据集。然后，提出了一种新方法，通过基于道德基础理论的种子词在人类和LLM生成的联想图中传播道德价值观。

Result: 研究比较了英语母语者和LLM联想中产生的道德概念化，发现两者之间存在详细但系统性的差异。

Conclusion: 通过词联想方法，论文揭示了LLMs与英语母语者在道德价值观上的差异，为理解LLMs的道德推理提供了新的视角。

Abstract: As the impact of large language models increases, understanding the moral
values they reflect becomes ever more important. Assessing the nature of moral
values as understood by these models via direct prompting is challenging due to
potential leakage of human norms into model training data, and their
sensitivity to prompt formulation. Instead, we propose to use word
associations, which have been shown to reflect moral reasoning in humans, as
low-level underlying representations to obtain a more robust picture of LLMs'
moral reasoning. We study moral differences in associations from western
English-speaking communities and LLMs trained predominantly on English data.
First, we create a large dataset of LLM-generated word associations, resembling
an existing data set of human word associations. Next, we propose a novel
method to propagate moral values based on seed words derived from Moral
Foundation Theory through the human and LLM-generated association graphs.
Finally, we compare the resulting moral conceptualizations, highlighting
detailed but systematic differences between moral values emerging from English
speakers and LLM associations.

</details>


### [174] [Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement](https://arxiv.org/abs/2505.19675)
*Liqin Ye,Agam Shah,Chao Zhang,Sudheer Chava*

Main category: cs.CL

TL;DR: 论文提出SiDyP方法，通过动态先验的单纯形标签扩散来校准分类器预测，提升对LLM生成噪声标签的鲁棒性，在零样本和少样本任务中平均提升7%以上性能。


<details>
  <summary>Details</summary>
Motivation: 传统标注数据集成本高昂，而LLM自动生成的标签存在噪声问题，现有研究对此关注不足，需开发新方法提高模型在噪声标签下的泛化能力。

Method: SiDyP框架通过文本嵌入空间的邻域标签分布检索潜在真实标签，并利用单纯形扩散模型迭代优化噪声候选标签。

Result: 在BERT分类器上，SiDyP使零样本和少样本LLM噪声数据集的性能分别平均提升7.21%和7.30%，经多任务多LLM验证有效。

Conclusion: SiDyP能有效校准LLM生成的噪声标签，为降低标注成本提供了可靠解决方案，代码已开源。

Abstract: The traditional process of creating labeled datasets is labor-intensive and
expensive. Recent breakthroughs in open-source large language models (LLMs)
have opened up a new avenue in generating labeled datasets automatically for
various natural language processing (NLP) tasks, providing an alternative to
such an expensive annotation process. However, the reliability of such
auto-generated labels remains a significant concern due to inherent
inaccuracies. When learning from noisy labels, the model's generalization is
likely to be harmed as it is prone to overfit to those label noises. While
previous studies in learning from noisy labels mainly focus on synthetic noise
and real-world noise, LLM-generated label noise receives less attention. In
this paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to
calibrate the classifier's prediction, thus enhancing its robustness towards
LLM-generated noisy labels. SiDyP retrieves potential true label candidates by
neighborhood label distribution in text embedding space and iteratively refines
noisy candidates using a simplex diffusion model. Our framework can increase
the performance of the BERT classifier fine-tuned on both zero-shot and
few-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30%
respectively. We demonstrate the effectiveness of SiDyP by conducting extensive
benchmarking for different LLMs over a variety of NLP tasks. Our code is
available on Github.

</details>


### [175] [Grounding Language with Vision: A Conditional Mutual Information Calibrated Decoding Strategy for Reducing Hallucinations in LVLMs](https://arxiv.org/abs/2505.19678)
*Hao Fang,Changle Zhou,Jiawei Kong,Kuofeng Gao,Bin Chen,Tao Liang,Guojun Ma,Shu-Tao Xia*

Main category: cs.CL

TL;DR: 本文提出了一种新的条件点互信息（C-PMI）解码策略，通过动态调整视觉和文本标记的贡献，减少大型视觉语言模型（LVLM）中的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLM）容易产生幻觉，即生成的回答在语义上看似合理但与输入图像无关。研究发现这一问题主要源于模型在解码过程中过度依赖语言先验而忽视视觉信息。

Method: 提出条件点互信息（C-PMI）校准解码策略，通过双级优化问题建模视觉和文本标记的贡献，设计动态调节解码过程的标记纯化机制。

Result: 实验表明，该方法显著减少了LVLM中的幻觉现象，同时保持了解码效率。

Conclusion: C-PMI解码策略有效缓解了LVLM的幻觉问题，通过增强生成文本与输入图像之间的相互依赖性，提高了模型性能。

Abstract: Large Vision-Language Models (LVLMs) are susceptible to hallucinations, where
generated responses seem semantically plausible yet exhibit little or no
relevance to the input image. Previous studies reveal that this issue primarily
stems from LVLMs' over-reliance on language priors while disregarding the
visual information during decoding. To alleviate this issue, we introduce a
novel Conditional Pointwise Mutual Information (C-PMI) calibrated decoding
strategy, which adaptively strengthens the mutual dependency between generated
texts and input images to mitigate hallucinations. Unlike existing methods
solely focusing on text token sampling, we propose to jointly model the
contributions of visual and textual tokens to C-PMI, formulating hallucination
mitigation as a bi-level optimization problem aimed at maximizing mutual
information. To solve it, we design a token purification mechanism that
dynamically regulates the decoding process by sampling text tokens remaining
maximally relevant to the given image, while simultaneously refining image
tokens most pertinent to the generated response. Extensive experiments across
various benchmarks reveal that the proposed method significantly reduces
hallucinations in LVLMs while preserving decoding efficiency.

</details>


### [176] [KIT's Low-resource Speech Translation Systems for IWSLT2025: System Enhancement with Synthetic Data and Model Regularization](https://arxiv.org/abs/2505.19679)
*Zhaolin Li,Yining Liu,Danni Liu,Tuan Nam Nguyen,Enes Yavuz Ugan,Tu Anh Dinh,Carlos Mullov,Alexander Waibel,Jan Niehues*

Main category: cs.CL

TL;DR: KIT团队针对IWSLT 2025低资源赛道的三种语言对（Bemba、北黎凡特阿拉伯语、突尼斯阿拉伯语到英语），开发了级联系统（ASR+MT）和端到端语音翻译系统，并通过微调预训练模型、合成数据增强和模型正则化提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决低资源语言对的语音翻译挑战，通过有效利用有限资源（如合成数据）和模型优化策略（如正则化、蒸馏）提升系统性能。

Method: 结合预训练模型微调，采用合成数据增强（ASR数据通过MT生成翻译、MT数据通过TTS生成语音）和模型正则化（如intra-distillation），并融合级联与端到端系统（MBR解码）。

Result: 北黎凡特阿拉伯语的纯合成数据系统略优于真实数据训练的级联系统；Bemba的合成数据显著提升ASR/ST性能；intra-distillation和MBR解码分别带来模型性能提升和1.5 BLEU分增长。

Conclusion: 合成数据和模型正则化策略可有效提升低资源语言对的语音翻译性能，级联与端到端系统的融合进一步优化结果。

Abstract: This paper presents KIT's submissions to the IWSLT 2025 low-resource track.
We develop both cascaded systems, consisting of Automatic Speech Recognition
(ASR) and Machine Translation (MT) models, and end-to-end (E2E) Speech
Translation (ST) systems for three language pairs: Bemba, North Levantine
Arabic, and Tunisian Arabic into English. Building upon pre-trained models, we
fine-tune our systems with different strategies to utilize resources
efficiently. This study further explores system enhancement with synthetic data
and model regularization. Specifically, we investigate MT-augmented ST by
generating translations from ASR data using MT models. For North Levantine,
which lacks parallel ST training data, a system trained solely on synthetic
data slightly surpasses the cascaded system trained on real data. We also
explore augmentation using text-to-speech models by generating synthetic speech
from MT data, demonstrating the benefits of synthetic data in improving both
ASR and ST performance for Bemba. Additionally, we apply intra-distillation to
enhance model performance. Our experiments show that this approach consistently
improves results across ASR, MT, and ST tasks, as well as across different
pre-trained models. Finally, we apply Minimum Bayes Risk decoding to combine
the cascaded and end-to-end systems, achieving an improvement of approximately
1.5 BLEU points.

</details>


### [177] [Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models](https://arxiv.org/abs/2505.19700)
*Yi Liu,Dianqing Liu,Mingye Zhu,Junbo Guo,Yongdong Zhang,Zhendong Mao*

Main category: cs.CL

TL;DR: 提出了一种名为RAM的新方法，通过重要性采样框架改进大语言模型的对齐过程，提高了灵活性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统对齐方法需要重新训练大型预训练模型，难以快速适应多样化应用需求。

Method: 使用重要性采样框架，将未对齐的上游模型作为提案分布，通过自回归对齐模块进行二次采样。

Result: 在指令跟随、领域适应和偏好优化等任务中，RAM方法均优于基线模型。

Conclusion: RAM方法有效解决了传统对齐方法的局限性，提升了模型适应性和性能。

Abstract: The widespread adoption of large language models (LLMs) across industries has
increased the demand for high-quality and customizable outputs. However,
traditional alignment methods often require retraining large pretrained models,
making it difficult to quickly adapt and optimize LLMs for diverse
applications. To address this limitation, we propose a novel \textit{Residual
Alignment Model} (\textit{RAM}) that formalizes the alignment process as a type
of importance sampling. In this framework, the unaligned upstream model serves
as the proposal distribution, while the alignment process is framed as
secondary sampling based on an autoregressive alignment module that acts as an
estimator of the importance weights. This design enables a natural detachment
of the alignment module from the target aligned model, improving flexibility
and scalability. Based on this model, we derive an efficient sequence-level
training strategy for the alignment module, which operates independently of the
proposal module. Additionally, we develop a resampling algorithm with iterative
token-level decoding to address the common first-token latency issue in
comparable methods. Experimental evaluations on two leading open-source LLMs
across diverse tasks, including instruction following, domain adaptation, and
preference optimization, demonstrate that our approach consistently outperforms
baseline models.

</details>


### [178] [Error Typing for Smarter Rewards: Improving Process Reward Models with Error-Aware Hierarchical Supervision](https://arxiv.org/abs/2505.19706)
*Tej Deep Pala,Panshul Sharma,Amir Zadeh,Chuan Li,Soujanya Poria*

Main category: cs.CL

TL;DR: 论文提出PathFinder-PRM模型，通过分层错误检测和奖励估计，提升数学推理任务中的中间步骤评分，显著提高端到端推理性能和数据效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在多跳和复杂推理任务（如数学问题求解）中容易产生幻觉。现有结果奖励模型仅验证最终答案，而过程奖励模型（PRM）通过评分中间步骤引导生成连贯解决方案。

Method: 提出PathFinder-PRM，一种分层、错误感知的判别式PRM模型：1）分类数学和一致性错误；2）结合细粒度信号评估步骤正确性。训练数据通过增强人工标注PRM800K和RLHFlow Mistral轨迹构建，包含40万样本。

Result: 在PRMBench上取得67.7的PRMScore（SOTA），数据用量减少3倍；应用于奖励引导贪婪搜索时，prm@8提升1.5点至48.3。

Conclusion: 解耦错误检测与奖励估计不仅能提升细粒度错误识别，还能显著改善端到端数学推理性能，同时提高数据效率。

Abstract: Large Language Models (LLMs) are prone to hallucination, especially during
multi-hop and reasoning-intensive tasks such as mathematical problem solving.
While Outcome Reward Models verify only final answers, Process Reward Models
(PRMs) score each intermediate step to steer generation toward coherent
solutions. We introduce PathFinder-PRM, a novel hierarchical, error-aware
discriminative PRM that first classifies math and consistency errors at each
step, then combines these fine-grained signals to estimate step correctness. To
train PathFinder-PRM, we construct a 400K-sample dataset by enriching the
human-annotated PRM800K corpus and RLHFlow Mistral traces with
three-dimensional step-level labels. On PRMBench, PathFinder-PRM achieves a new
state-of-the-art PRMScore of 67.7, outperforming the prior best (65.5) while
using 3 times less data. When applied to reward guided greedy search, our model
yields prm@8 48.3, a +1.5 point gain over the strongest baseline. These results
demonstrate that decoupled error detection and reward estimation not only boost
fine-grained error detection but also substantially improve end-to-end,
reward-guided mathematical reasoning with greater data efficiency.

</details>


### [179] [MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via Multi-Task Reinforcement Learning](https://arxiv.org/abs/2505.19714)
*Zhaopeng Feng,Yupu Liang,Shaosheng Cao,Jiayuan Su,Jiahan Ren,Zhe Xu,Yao Hu,Wenxuan Huang,Jian Wu,Zuozhu Liu*

Main category: cs.CL

TL;DR: 论文提出MT$^{3}$框架，首次将多任务强化学习应用于多模态大语言模型，实现端到端的图像文本机器翻译（TIMT），并在新基准XHSPost上取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 图像文本机器翻译（TIMT）在无障碍访问、跨语言信息获取等领域至关重要，但传统方法依赖多阶段级联流程，且大模型在该任务的端到端应用尚未充分探索。

Method: 采用多任务强化学习框架MT$^{3}$，通过文本识别、上下文推理和翻译三任务协同优化，结合新型混合奖励机制提供细粒度反馈。

Result: MT$^{3}$-7B-Zero在MIT-10M基准上超越Qwen2.5-VL-72B等基线模型，并在跨语言场景中展现强泛化能力。

Conclusion: 多任务协同、强化学习初始化及课程设计有效推动了多模态大语言模型在TIMT任务中的性能突破。

Abstract: Text Image Machine Translation (TIMT)-the task of translating textual content
embedded in images-is critical for applications in accessibility, cross-lingual
information access, and real-world document understanding. However, TIMT
remains a complex challenge due to the need for accurate optical character
recognition (OCR), robust visual-text reasoning, and high-quality translation,
often requiring cascading multi-stage pipelines. Recent advances in large-scale
Reinforcement Learning (RL) have improved reasoning in Large Language Models
(LLMs) and Multimodal LLMs (MLLMs), but their application to end-to-end TIMT is
still underexplored. To bridge this gap, we introduce MT$^{3}$, the first
framework to apply Multi-Task RL to MLLMs for end-to-end TIMT. MT$^{3}$ adopts
a multi-task optimization paradigm targeting three key sub-skills: text
recognition, context-aware reasoning, and translation. It is trained using a
novel multi-mixed reward mechanism that adapts rule-based RL strategies to
TIMT's intricacies, offering fine-grained, non-binary feedback across tasks.
Furthermore, to facilitate the evaluation of TIMT in authentic cross-cultural
and real-world social media contexts, we introduced XHSPost, the first social
media TIMT benchmark. Our MT$^{3}$-7B-Zero achieves state-of-the-art results on
the latest in-domain MIT-10M benchmark, outperforming strong baselines such as
Qwen2.5-VL-72B and InternVL2.5-78B by notable margins across multiple metrics.
Additionally, the model shows strong generalization to out-of-distribution
language pairs and datasets. In-depth analyses reveal how multi-task synergy,
reinforcement learning initialization, curriculum design, and reward
formulation contribute to advancing MLLM-driven TIMT.

</details>


### [180] [Graceful Forgetting in Generative Language Models](https://arxiv.org/abs/2505.19715)
*Chunyang Jiang,Chi-min Chan,Yiyang Cai,Yulong Liu,Wei Xue,Yike Guo*

Main category: cs.CL

TL;DR: 论文提出LWF框架，通过优雅遗忘机制选择性丢弃预训练语言模型中的有害知识，提升下游任务微调性能。


<details>
  <summary>Details</summary>
Motivation: 预训练-微调范式虽广泛应用，但预训练中获得的部分知识可能对下游任务产生负面影响（负迁移）。现有优雅遗忘方法在生成式语言模型中研究不足且迁移困难。

Method: 提出LWF框架：利用Fisher信息矩阵加权参数更新，计算遗忘置信度评估自生成知识，周期性遗忘高置信度知识。

Result: 实验表明，尽管知识交互机制尚不明确，优雅遗忘能有效提升生成式语言模型的微调性能。

Conclusion: LWF框架为生成式语言模型中的负迁移问题提供了可行解决方案，证实选择性遗忘对模型优化的价值。

Abstract: Recently, the pretrain-finetune paradigm has become a cornerstone in various
deep learning areas. While in general the pre-trained model would promote both
effectiveness and efficiency of downstream tasks fine-tuning, studies have
shown that not all knowledge acquired during pre-training is beneficial. Some
of the knowledge may actually bring detrimental effects to the fine-tuning
tasks, which is also known as negative transfer. To address this problem,
graceful forgetting has emerged as a promising approach. The core principle of
graceful forgetting is to enhance the learning plasticity of the target task by
selectively discarding irrelevant knowledge. However, this approach remains
underexplored in the context of generative language models, and it is often
challenging to migrate existing forgetting algorithms to these models due to
architecture incompatibility. To bridge this gap, in this paper we propose a
novel framework, Learning With Forgetting (LWF), to achieve graceful forgetting
in generative language models. With Fisher Information Matrix weighting the
intended parameter updates, LWF computes forgetting confidence to evaluate
self-generated knowledge regarding the forgetting task, and consequently,
knowledge with high confidence is periodically unlearned during fine-tuning.
Our experiments demonstrate that, although thoroughly uncovering the mechanisms
of knowledge interaction remains challenging in pre-trained language models,
applying graceful forgetting can contribute to enhanced fine-tuning
performance.

</details>


### [181] [Distilling Closed-Source LLM's Knowledge for Locally Stable and Economic Biomedical Entity Linking](https://arxiv.org/abs/2505.19722)
*Yihao Ai,Zhiyuan Ning,Weiwei Dai,Pengfei Wang,Yi Du,Wenjuan Cui,Kunpeng Liu,Yuanchun Zhou*

Main category: cs.CL

TL;DR: 论文提出RPDR框架，结合闭源和开源大语言模型，解决生物医学实体链接在低资源场景下的问题，避免高成本和稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 传统监督方法需要大量标注数据，闭源大语言模型虽能解决但存在稳定性问题和高经济成本。RPDR旨在通过结合闭源和开源模型，在低资源场景下高效工作。

Method: RPDR框架利用闭源大语言模型生成训练数据，微调开源模型进行候选重排序，从而将知识蒸馏到可本地部署的开源模型中。

Result: 在两个数据集（包括中英文）上测试，RPDR在训练数据不足时分别提高了0.019和0.036的Acc@1分数，展示了其优越性和泛化能力。

Conclusion: RPDR框架有效解决了生物医学实体链接中的高成本和稳定性问题，适用于低资源场景，并展示了良好的性能。

Abstract: Biomedical entity linking aims to map nonstandard entities to standard
entities in a knowledge base. Traditional supervised methods perform well but
require extensive annotated data to transfer, limiting their usage in
low-resource scenarios. Large language models (LLMs), especially closed-source
LLMs, can address these but risk stability issues and high economic costs:
using these models is restricted by commercial companies and brings significant
economic costs when dealing with large amounts of data. To address this, we
propose ``RPDR'', a framework combining closed-source LLMs and open-source LLMs
for re-ranking candidates retrieved by a retriever fine-tuned with a small
amount of data. By prompting a closed-source LLM to generate training data from
unannotated data and fine-tuning an open-source LLM for re-ranking, we
effectively distill the knowledge to the open-source LLM that can be deployed
locally, thus avoiding the stability issues and the problem of high economic
costs. We evaluate RPDR on two datasets, including one real-world dataset and
one publicly available dataset involving two languages: Chinese and English.
RPDR achieves 0.019 Acc@1 improvement and 0.036 Acc@1 improvement on the Aier
dataset and the Ask A Patient dataset when the amount of training data is not
enough. The results demonstrate the superiority and generalizability of the
proposed framework.

</details>


### [182] [Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models](https://arxiv.org/abs/2505.19743)
*Yang Zhang,Yu Yu,Bo Tang,Yu Zhu,Chuxiong Sun,Wenqiang Wei,Jie Hu,Zipeng Xie,Zhiyu Li,Feiyu Xiong,Edward Chung*

Main category: cs.CL

TL;DR: 论文提出了一种名为MARA的轻量级对齐方法，通过将句子级偏好学习分解为词元级二分类，显著降低了计算成本并提升了模型对齐性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型对齐方法（如RLHF或DPO）通常需要对数十亿参数模型进行微调，计算成本高昂且效率低下。

Method: 提出MARA方法，使用一个紧凑的三层全连接网络独立于语言模型运行，通过词元级的'接受-拒绝'二分类实现对齐。

Result: 在7种不同大语言模型和3个开源数据集上的实验表明，MARA在降低计算成本的同时显著提升了对齐性能。

Conclusion: MARA为语言模型对齐提供了一种高效且可扩展的解决方案，突破了传统方法的高计算成本限制。

Abstract: With the rapid development of Large Language Models (LLMs), aligning these
models with human preferences and values is critical to ensuring ethical and
safe applications. However, existing alignment techniques such as RLHF or DPO
often require direct fine-tuning on LLMs with billions of parameters, resulting
in substantial computational costs and inefficiencies. To address this, we
propose Micro token-level Accept-Reject Aligning (MARA) approach designed to
operate independently of the language models. MARA simplifies the alignment
process by decomposing sentence-level preference learning into token-level
binary classification, where a compact three-layer fully-connected network
determines whether candidate tokens are "Accepted" or "Rejected" as part of the
response. Extensive experiments across seven different LLMs and three
open-source datasets show that MARA achieves significant improvements in
alignment performance while reducing computational costs.

</details>


### [183] [NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF Question Answering](https://arxiv.org/abs/2505.19754)
*Ruisheng Cao,Hanchong Zhang,Tiancheng Huang,Zhangyi Kang,Yuxin Zhang,Liangtai Sun,Hanqi Li,Yuxun Miao,Shuai Fan,Lu Chen,Kai Yu*

Main category: cs.CL

TL;DR: NeuSym-RAG提出了一种混合神经符号检索框架，通过多视图分块和基于模式的解析，有效结合神经和符号检索的优势，提升PDF文档的问答系统性能。


<details>
  <summary>Details</summary>
Motivation: 学术论文数量激增，研究者难以高效获取关键信息。现有检索增强生成（RAG）方法通常孤立神经和符号检索，且传统单视图分块忽略了PDF的丰富结构和布局。

Method: NeuSym-RAG结合神经和符号检索，采用多视图分块和模式解析，将半结构化PDF内容组织到关系数据库和向量库中，使LLM代理能迭代收集上下文直至生成答案。

Result: 在三个基于PDF的QA数据集（包括自标注的AIRQA-REAL）上，NeuSym-RAG稳定优于基于向量的RAG和各种结构化基线方法。

Conclusion: NeuSym-RAG成功统一了两种检索方案并利用多视图，显著提升了PDF文档问答系统的性能。代码和数据已公开。

Abstract: The increasing number of academic papers poses significant challenges for
researchers to efficiently acquire key details. While retrieval augmented
generation (RAG) shows great promise in large language model (LLM) based
automated question answering, previous works often isolate neural and symbolic
retrieval despite their complementary strengths. Moreover, conventional
single-view chunking neglects the rich structure and layout of PDFs, e.g.,
sections and tables. In this work, we propose NeuSym-RAG, a hybrid neural
symbolic retrieval framework which combines both paradigms in an interactive
process. By leveraging multi-view chunking and schema-based parsing, NeuSym-RAG
organizes semi-structured PDF content into both the relational database and
vectorstore, enabling LLM agents to iteratively gather context until sufficient
to generate answers. Experiments on three full PDF-based QA datasets, including
a self-annotated one AIRQA-REAL, show that NeuSym-RAG stably defeats both the
vector-based RAG and various structured baselines, highlighting its capacity to
unify both retrieval schemes and utilize multiple views. Code and data are
publicly available at https://github.com/X-LANCE/NeuSym-RAG.

</details>


### [184] [Efficient Reasoning via Chain of Unconscious Thought](https://arxiv.org/abs/2505.19756)
*Ruihan Gong,Yue Liu,Wenjie Qu,Mingzhe Du,Yufei He,Yingwei Ma,Yulin Chen,Xiang Liu,Yi Wen,Xinfeng Li,Ruidong Wang,Xinzhong Zhu,Bryan Hooi,Jiaheng Zhang*

Main category: cs.CL

TL;DR: 论文提出Chain of Unconscious Thought (CoUT)方法，通过模仿人类无意识思维提高大型推理模型的token效率，减少47.62%的token使用同时保持准确率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型(LRMs)在性能上表现优异，但由于冗长的推理过程导致token效率低下。受无意识思维理论(UTT)启发，作者希望通过模仿人类无意识思维来提高token效率。

Method: 提出CoUT方法，首先引导模型在隐藏层内化推理过程，然后设计一系列token高效策略以减少不必要的token使用。

Result: 实验表明，CoUT在保持准确率的同时，比Chain of Thought (CoT)减少47.62%的token使用。

Conclusion: 研究表明模型可能具有有益的无意识思维，能够在保持性能的同时提高效率。CoUT方法在token效率上表现优异。

Abstract: Large Reasoning Models (LRMs) achieve promising performance but compromise
token efficiency due to verbose reasoning processes. Unconscious Thought Theory
(UTT) posits that complex problems can be solved more efficiently through
internalized cognitive processes. Inspired by UTT, we propose a new reasoning
paradigm, termed Chain of Unconscious Thought (CoUT), to improve the token
efficiency of LRMs by guiding them to mimic human unconscious thought and
internalize reasoning processes. Concretely, we first prompt the model to
internalize the reasoning by thinking in the hidden layer. Then, we design a
bag of token-efficient strategies to further help models reduce unnecessary
tokens yet preserve the performance. Our work reveals that models may possess
beneficial unconscious thought, enabling improved efficiency without
sacrificing performance. Extensive experiments demonstrate the effectiveness of
CoUT. Remarkably, it surpasses CoT by reducing token usage by 47.62% while
maintaining comparable accuracy, as shown in Figure 1. The code of CoUT is
available at this link: https://github.com/Rohan-GRH/CoUT

</details>


### [185] [SGM: A Framework for Building Specification-Guided Moderation Filters](https://arxiv.org/abs/2505.19766)
*Masoomali Fatehkia,Enes Altinisik,Husrev Taha Sencar*

Main category: cs.CL

TL;DR: 提出SGM框架，通过用户定义规范训练内容审核过滤器，超越传统安全关注，实现自动化数据生成和细粒度控制。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）部署时仍存在对齐不足和对抗性输入（如越狱）问题，传统内容审核过滤器仅聚焦安全，缺乏灵活性。

Method: SGM框架基于用户定义规范自动生成训练数据，无需人工标注，支持多样化、应用特定的对齐目标。

Result: SGM训练的过滤器性能媲美基于人工标注数据的最先进安全过滤器，同时支持细粒度的用户自定义对齐控制。

Conclusion: SGM为LLM部署提供了可扩展、灵活的内容审核方案，扩展了传统安全过滤器的功能边界。

Abstract: Aligning large language models (LLMs) with deployment-specific requirements
is critical but inherently imperfect. Despite extensive training, models remain
susceptible to misalignment and adversarial inputs such as jailbreaks. Content
moderation filters are commonly used as external safeguards, though they
typically focus narrowly on safety. We introduce SGM (Specification-Guided
Moderation), a flexible framework for training moderation filters grounded in
user-defined specifications that go beyond standard safety concerns. SGM
automates training data generation without relying on human-written examples,
enabling scalable support for diverse, application-specific alignment goals.
SGM-trained filters perform on par with state-of-the-art safety filters built
on curated datasets, while supporting fine-grained and user-defined alignment
control.

</details>


### [186] [T^2Agent A Tool-augmented Multimodal Misinformation Detection Agent with Monte Carlo Tree Search](https://arxiv.org/abs/2505.19768)
*Xing Cui,Yueying Zou,Zekun Li,Peipei Li,Xinyuan Xu,Xuannan Liu,Huaibo Huang,Ran He*

Main category: cs.CL

TL;DR: 提出T2Agent，一种结合可扩展工具包和蒙特卡洛树搜索的动态多模态虚假信息检测方法，通过贝叶斯优化选择工具子集，改进传统MCTS以适应多源验证，实验显示其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现实中的多模态虚假信息常来自混合伪造源，需动态推理和自适应验证。现有方法依赖静态流程和有限工具，难以应对复杂多样性。

Method: T2Agent整合模块化工具包（如网络搜索、伪造检测、一致性分析）和蒙特卡洛树搜索（MCTS），通过贝叶斯优化选择任务相关工具子集，扩展MCTS支持多源验证，并设计双奖励机制平衡探索与利用。

Result: 消融实验验证树搜索机制和工具使用的有效性，大量实验表明T2Agent在混合源多模态虚假信息基准上持续优于现有基线。

Conclusion: T2Agent作为一种无需训练的方法，通过动态工具组合和多源验证显著提升检测准确率，展现出应对复杂虚假信息的潜力。

Abstract: Real-world multimodal misinformation often arises from mixed forgery sources,
requiring dynamic reasoning and adaptive verification. However, existing
methods mainly rely on static pipelines and limited tool usage, limiting their
ability to handle such complexity and diversity. To address this challenge, we
propose T2Agent, a novel misinformation detection agent that incorporates an
extensible toolkit with Monte Carlo Tree Search (MCTS). The toolkit consists of
modular tools such as web search, forgery detection, and consistency analysis.
Each tool is described using standardized templates, enabling seamless
integration and future expansion. To avoid inefficiency from using all tools
simultaneously, a Bayesian optimization-based selector is proposed to identify
a task-relevant subset. This subset then serves as the action space for MCTS to
dynamically collect evidence and perform multi-source verification. To better
align MCTS with the multi-source nature of misinformation detection, T2Agent
extends traditional MCTS with multi-source verification, which decomposes the
task into coordinated subtasks targeting different forgery sources. A dual
reward mechanism containing a reasoning trajectory score and a confidence score
is further proposed to encourage a balance between exploration across mixed
forgery sources and exploitation for more reliable evidence. We conduct
ablation studies to confirm the effectiveness of the tree search mechanism and
tool usage. Extensive experiments further show that T2Agent consistently
outperforms existing baselines on challenging mixed-source multimodal
misinformation benchmarks, demonstrating its strong potential as a
training-free approach for enhancing detection accuracy. The code will be
released.

</details>


### [187] [What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs](https://arxiv.org/abs/2505.19773)
*Sangyeop Kim,Yohan Lee,Yongwoo Song,Kimin Lee*

Main category: cs.CL

TL;DR: 研究发现，大语言模型在长上下文处理中存在安全漏洞，即使简单重复或随机文本也能绕过安全机制。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型在长上下文环境中的安全漏洞，揭示其安全机制的局限性。

Method: 通过多种多轮攻击设置（不同指令风格、密度、主题和格式），在长达128K标记的上下文中进行实验。

Result: 上下文长度是攻击效果的关键因素，即使非精心设计的攻击内容也能成功绕过模型安全措施。

Conclusion: 大语言模型的长上下文处理能力存在根本性安全缺陷，亟需新的安全机制。

Abstract: We investigate long-context vulnerabilities in Large Language Models (LLMs)
through Many-Shot Jailbreaking (MSJ). Our experiments utilize context length of
up to 128K tokens. Through comprehensive analysis with various many-shot attack
settings with different instruction styles, shot density, topic, and format, we
reveal that context length is the primary factor determining attack
effectiveness. Critically, we find that successful attacks do not require
carefully crafted harmful content. Even repetitive shots or random dummy text
can circumvent model safety measures, suggesting fundamental limitations in
long-context processing capabilities of LLMs. The safety behavior of
well-aligned models becomes increasingly inconsistent with longer contexts.
These findings highlight significant safety gaps in context expansion
capabilities of LLMs, emphasizing the need for new safety mechanisms.

</details>


### [188] [Analyzing Political Bias in LLMs via Target-Oriented Sentiment Classification](https://arxiv.org/abs/2505.19776)
*Akram Elbouanani,Evan Dufraisse,Adrian Popescu*

Main category: cs.CL

TL;DR: 该论文提出了一种新方法，通过分析LLM对政治实体情感预测的不一致性来量化政治偏见，发现不同语言和模型规模下的偏见模式。


<details>
  <summary>Details</summary>
Motivation: 现有偏见分析方法依赖小规模中间任务和LLM自身分析，可能传播偏见。需要更客观的方法来量化LLM中的政治偏见。

Method: 提出基于熵的不一致性指标，在450个政治句子中插入1319个多元化政治家姓名，用7个模型6种语言进行目标导向情感预测。

Result: 所有测试组合均存在不一致性，西方语言偏见更强，大模型偏见更显著且一致，相似政治倾向政治家呈现相关性偏见。

Conclusion: LLM存在可量化的政治偏见，模型规模和语言类型影响偏见强度，通过替换虚构姓名可部分缓解目标情感分类不可靠性。

Abstract: Political biases encoded by LLMs might have detrimental effects on downstream
applications. Existing bias analysis methods rely on small-size intermediate
tasks (questionnaire answering or political content generation) and rely on the
LLMs themselves for analysis, thus propagating bias. We propose a new approach
leveraging the observation that LLM sentiment predictions vary with the target
entity in the same sentence. We define an entropy-based inconsistency metric to
encode this prediction variability. We insert 1319 demographically and
politically diverse politician names in 450 political sentences and predict
target-oriented sentiment using seven models in six widely spoken languages. We
observe inconsistencies in all tested combinations and aggregate them in a
statistically robust analysis at different granularity levels. We observe
positive and negative bias toward left and far-right politicians and positive
correlations between politicians with similar alignment. Bias intensity is
higher for Western languages than for others. Larger models exhibit stronger
and more consistent biases and reduce discrepancies between similar languages.
We partially mitigate LLM unreliability in target-oriented sentiment
classification (TSC) by replacing politician names with fictional but plausible
counterparts.

</details>


### [189] [The Avengers: A Simple Recipe for Uniting Smaller Language Models to Challenge Proprietary Giants](https://arxiv.org/abs/2505.19797)
*Yiqun Zhang,Hao Li,Chenxu Wang,Linyao Chen,Qiaosheng Zhang,Peng Ye,Shi Feng,Daling Wang,Zhen Wang,Xinrun Wang,Jia Xu,Lei Bai,Wanli Ouyang,Shuyue Hu*

Main category: cs.CL

TL;DR: 开源社区通过整合多个小型语言模型（Avengers框架），在多项任务上超越GPT-4.1，尤其在数学和代码任务上表现突出。


<details>
  <summary>Details</summary>
Motivation: 随着专有巨头主导大型语言模型竞赛，开源社区面临小型模型是否能在广泛任务中保持竞争力的问题。

Method: Avengers框架通过嵌入、聚类、评分和投票四个轻量级操作，整合多个小型语言模型的集体智能。

Result: 使用10个开源模型（每个约70亿参数），Avengers在15个数据集中10个上超越GPT-4.1，数学任务提升18.21%，代码任务提升7.46%。

Conclusion: Avengers框架展示了小型模型通过集体协作的竞争力，具有优异的泛化能力和鲁棒性，代码已开源。

Abstract: As proprietary giants increasingly dominate the race for ever-larger language
models, a pressing question arises for the open-source community: can smaller
models remain competitive across a broad range of tasks? In this paper, we
present the Avengers--a simple recipe that effectively leverages the collective
intelligence of open-source, smaller language models. Our framework is built
upon four lightweight operations: (i) embedding: encode queries using a text
embedding model; (ii) clustering: group queries based on their semantic
similarity; (iii) scoring: scores each model's performance within each cluster;
and (iv) voting: improve outputs via repeated sampling and voting. At inference
time, each query is embedded and assigned to its nearest cluster. The
top-performing model(s) within that cluster are selected to generate the
response using the Self-Consistency or its multi-model variant. Remarkably,
with 10 open-source models (~7B parameters each), the Avengers collectively
outperforms GPT-4.1 on 10 out of 15 datasets (spanning mathematics, code,
logic, knowledge, and affective tasks). In particular, it surpasses GPT-4.1 on
mathematics tasks by 18.21% and on code tasks by 7.46%. Furthermore, the
Avengers delivers superior out-of-distribution generalization, and remains
robust across various embedding models, clustering algorithms, ensemble
strategies, and values of its sole parameter--the number of clusters. We have
open-sourced the code on GitHub: https://github.com/ZhangYiqun018/Avengers

</details>


### [190] [MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs](https://arxiv.org/abs/2505.19800)
*Zaid Alyafeai,Maged S. Al-Shaibani,Bernard Ghanem*

Main category: cs.CL

TL;DR: MOLE框架利用大语言模型自动从非阿拉伯语数据集的科学论文中提取元数据，通过模式驱动方法和验证机制提升效率，并发布代码和数据集供研究社区使用。


<details>
  <summary>Details</summary>
Motivation: 随着科学研究的指数级增长，元数据提取对数据集编目和保存至关重要。现有工具如Masader主要依赖人工标注，效率较低，因此需要自动化解决方案。

Method: MOLE采用模式驱动方法，利用大语言模型处理多种输入格式的文档，并结合上下文长度分析、少量样本学习和网络浏览集成等技术，确保输出的一致性。

Result: 实验表明，现代大语言模型在此任务上表现良好，但仍需进一步改进以确保稳定可靠的性能。

Conclusion: MOLE展示了自动化元数据提取的潜力，未来工作需持续优化以提升性能，并已开源代码和数据集供社区使用。

Abstract: Metadata extraction is essential for cataloging and preserving datasets,
enabling effective research discovery and reproducibility, especially given the
current exponential growth in scientific research. While Masader (Alyafeai et
al.,2021) laid the groundwork for extracting a wide range of metadata
attributes from Arabic NLP datasets' scholarly articles, it relies heavily on
manual annotation. In this paper, we present MOLE, a framework that leverages
Large Language Models (LLMs) to automatically extract metadata attributes from
scientific papers covering datasets of languages other than Arabic. Our
schema-driven methodology processes entire documents across multiple input
formats and incorporates robust validation mechanisms for consistent output.
Additionally, we introduce a new benchmark to evaluate the research progress on
this task. Through systematic analysis of context length, few-shot learning,
and web browsing integration, we demonstrate that modern LLMs show promising
results in automating this task, highlighting the need for further future work
improvements to ensure consistent and reliable performance. We release the
code: https://github.com/IVUL-KAUST/MOLE and dataset:
https://huggingface.co/datasets/IVUL-KAUST/MOLE for the research community.

</details>


### [191] [Compliance-to-Code: Enhancing Financial Compliance Checking via Code Generation](https://arxiv.org/abs/2505.19804)
*Siyuan Li,Jian Chen,Rui Yao,Xuming Hu,Peilin Zhou,Weihua Qiu,Simin Zhang,Chucheng Dong,Zhiyao Li,Qipeng Xie,Zixuan Yuan*

Main category: cs.CL

TL;DR: 该论文提出了首个中文金融合规数据集Compliance-to-Code，通过结构化条款与代码映射解决现有RegTech在中文场景下的三大局限，并开发了FinCheck验证流程。


<details>
  <summary>Details</summary>
Motivation: 现有RegTech和LLMs在处理中文金融法规时存在领域知识不完整、层次推理不足及逻辑连贯性缺失三大缺陷，且缺乏针对性数据集。

Method: 构建包含1,159条标注条款的中文数据集，采用四要素结构化（主体/条件/约束/上下文）并关联法规关系，提供Python代码映射与推理说明。

Result: 开发了FinCheck管道，实现法规结构化、代码生成和报告生成的自动化审计功能。

Conclusion: Compliance-to-Code填补了中文金融合规数据集的空白，为自动化监管提供了可行解决方案。

Abstract: Nowadays, regulatory compliance has become a cornerstone of corporate
governance, ensuring adherence to systematic legal frameworks. At its core,
financial regulations often comprise highly intricate provisions, layered
logical structures, and numerous exceptions, which inevitably result in
labor-intensive or comprehension challenges. To mitigate this, recent
Regulatory Technology (RegTech) and Large Language Models (LLMs) have gained
significant attention in automating the conversion of regulatory text into
executable compliance logic. However, their performance remains suboptimal
particularly when applied to Chinese-language financial regulations, due to
three key limitations: (1) incomplete domain-specific knowledge representation,
(2) insufficient hierarchical reasoning capabilities, and (3) failure to
maintain temporal and logical coherence. One promising solution is to develop a
domain specific and code-oriented datasets for model training. Existing
datasets such as LexGLUE, LegalBench, and CODE-ACCORD are often
English-focused, domain-mismatched, or lack fine-grained granularity for
compliance code generation. To fill these gaps, we present Compliance-to-Code,
the first large-scale Chinese dataset dedicated to financial regulatory
compliance. Covering 1,159 annotated clauses from 361 regulations across ten
categories, each clause is modularly structured with four logical
elements-subject, condition, constraint, and contextual information-along with
regulation relations. We provide deterministic Python code mappings, detailed
code reasoning, and code explanations to facilitate automated auditing. To
demonstrate utility, we present FinCheck: a pipeline for regulation
structuring, code generation, and report generation.

</details>


### [192] [Exploring Consciousness in LLMs: A Systematic Survey of Theories, Implementations, and Frontier Risks](https://arxiv.org/abs/2505.19806)
*Sirui Chen,Shuqin Ma,Shu Yu,Hanwang Zhang,Shengjie Zhao,Chaochao Lu*

Main category: cs.CL

TL;DR: 该论文探讨了大型语言模型（LLMs）的意识问题，梳理了相关术语和研究，并讨论了潜在风险和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速发展，关于其智能和意识的讨论日益重要，但目前对LLM意识的研究仍处于未充分探索的状态。

Method: 论文首先澄清了相关术语（如LLM意识与LLM感知），然后从理论和实证角度系统整理和综合了现有研究。

Result: 论文总结了现有研究，并强调了有意识的LLM可能带来的前沿风险。

Conclusion: 论文讨论了当前挑战，并概述了这一新兴领域的未来研究方向。

Abstract: Consciousness stands as one of the most profound and distinguishing features
of the human mind, fundamentally shaping our understanding of existence and
agency. As large language models (LLMs) develop at an unprecedented pace,
questions concerning intelligence and consciousness have become increasingly
significant. However, discourse on LLM consciousness remains largely unexplored
territory. In this paper, we first clarify frequently conflated terminologies
(e.g., LLM consciousness and LLM awareness). Then, we systematically organize
and synthesize existing research on LLM consciousness from both theoretical and
empirical perspectives. Furthermore, we highlight potential frontier risks that
conscious LLMs might introduce. Finally, we discuss current challenges and
outline future directions in this emerging field. The references discussed in
this paper are organized at
https://github.com/OpenCausaLab/Awesome-LLM-Consciousness.

</details>


### [193] [Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective](https://arxiv.org/abs/2505.19815)
*Junnan Liu,Hongwei Liu,Linchen Xiao,Shudong Liu,Taolin Zhang,Zihan Ma,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: 该论文提出了一种通过元学习视角理解大语言模型（LLM）推理能力的新框架，将推理轨迹视为对模型参数的伪梯度下降更新，并通过实证验证了LLM推理与元学习之间的紧密联系。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是通过元学习的视角来深入理解大语言模型（LLM）的推理能力，探索如何利用元学习技术来提升这些模型的性能。

Method: 论文将推理任务训练过程形式化为元学习设置，将每个问题视为独立任务，推理轨迹作为内循环优化来调整模型参数。通过多样化的训练问题集，LLM能够发展出可泛化到新问题的基本推理能力。

Result: 广泛的实证评估证实了LLM推理与元学习之间的紧密联系，并从元学习的角度探讨了几个重要问题。

Conclusion: 该研究不仅增强了对LLM推理能力的理解，还通过成熟的元学习技术为改进这些模型提供了实用见解。

Abstract: We propose a novel framework for comprehending the reasoning capabilities of
large language models (LLMs) through the perspective of meta-learning. By
conceptualizing reasoning trajectories as pseudo-gradient descent updates to
the LLM's parameters, we identify parallels between LLM reasoning and various
meta-learning paradigms. We formalize the training process for reasoning tasks
as a meta-learning setup, with each question treated as an individual task, and
reasoning trajectories serving as the inner loop optimization for adapting
model parameters. Once trained on a diverse set of questions, the LLM develops
fundamental reasoning capabilities that can generalize to previously unseen
questions. Extensive empirical evaluations substantiate the strong connection
between LLM reasoning and meta-learning, exploring several issues of
significant interest from a meta-learning standpoint. Our work not only
enhances the understanding of LLM reasoning but also provides practical
insights for improving these models through established meta-learning
techniques.

</details>


### [194] [FoodTaxo: Generating Food Taxonomies with Large Language Models](https://arxiv.org/abs/2505.19838)
*Pascal Wullschleger,Majid Zarharan,Donnacha Daly,Marc Pouly,Jennifer Foster*

Main category: cs.CL

TL;DR: 研究探索了使用大型语言模型（LLM）自动生成和完善食品技术行业分类法的可行性，发现尽管有潜力，但正确放置内部节点仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在食品技术行业分类法自动生成和完善中的应用潜力，特别是在有无种子分类法的情况下。

Method: 使用开源LLM（Llama-3）和最新提示技术，通过迭代方式从种子分类法或已知概念集生成和完善分类法。

Result: 在五个分类法上的实验显示出一定的潜力，但正确放置内部节点仍然困难。

Conclusion: 大型语言模型在分类法自动生成和完善中具有应用前景，但在处理内部节点时仍需改进。

Abstract: We investigate the utility of Large Language Models for automated taxonomy
generation and completion specifically applied to taxonomies from the food
technology industry. We explore the extent to which taxonomies can be completed
from a seed taxonomy or generated without a seed from a set of known concepts,
in an iterative fashion using recent prompting techniques. Experiments on five
taxonomies using an open-source LLM (Llama-3), while promising, point to the
difficulty of correctly placing inner nodes.

</details>


### [195] [Improving Multilingual Math Reasoning for African Languages](https://arxiv.org/abs/2505.19848)
*Odunayo Ogundepo,Akintunde Oladipo,Kelechi Ogueji,Esther Adenuga,David Ifeoluwa Adelani,Jimmy Lin*

Main category: cs.CL

TL;DR: 该研究系统评估了将大语言模型（LLMs）适配到非洲低资源语言的最佳策略，重点关注数学推理任务。


<details>
  <summary>Details</summary>
Motivation: 低资源语言（尤其是非洲语言）由于数据稀缺和计算资源有限，在适配现有大语言模型时面临挑战。目前缺乏针对这类语言的最佳适配方法的明确指导。

Method: 以Llama 3.1系列为基础模型，通过对比不同数据类型（翻译文本vs合成数据）、训练阶段（预训练vs后训练）及其他配置的组合进行系统实验。

Result: 通过大量实验和消融研究，得出了针对非洲语言适配LLMs的有效策略（具体最优策略需参考论文完整结果）。

Conclusion: 该研究为低资源语言（特别是非洲语言）的LLM适配提供了实证指导，揭示了不同技术路线的性能差异。

Abstract: Researchers working on low-resource languages face persistent challenges due
to limited data availability and restricted access to computational resources.
Although most large language models (LLMs) are predominantly trained in
high-resource languages, adapting them to low-resource contexts, particularly
African languages, requires specialized techniques. Several strategies have
emerged for adapting models to low-resource languages in todays LLM landscape,
defined by multi-stage pre-training and post-training paradigms. However, the
most effective approaches remain uncertain. This work systematically
investigates which adaptation strategies yield the best performance when
extending existing LLMs to African languages. We conduct extensive experiments
and ablation studies to evaluate different combinations of data types
(translated versus synthetically generated), training stages (pre-training
versus post-training), and other model adaptation configurations. Our
experiments focuses on mathematical reasoning tasks, using the Llama 3.1 model
family as our base model.

</details>


### [196] [Beyond Specialization: Benchmarking LLMs for Transliteration of Indian Languages](https://arxiv.org/abs/2505.19851)
*Gulfarogh Azam,Mohd Sadique,Saif Ali,Mohammad Nadeem,Erik Cambria,Shahab Saquib Sohail,Mohammad Sultan Alam*

Main category: cs.CL

TL;DR: 论文评估了主流大语言模型（如GPT系列）与专业转写模型IndicXlit在10种印度语言上的表现，发现GPT模型普遍优于其他模型，微调后性能进一步提升。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索通用大语言模型在多语言转写任务中的潜力，尤其是在印度这样语言多样化的环境中，是否无需专门训练即可超越专业模型。

Method: 使用Dakshina和Aksharantar标准数据集，对比GPT-4o/4.5/4.1、Gemma-3-27B-it和Mistral-Large与IndicXlit的表现，采用Top-1准确率和字符错误率评估，并进行噪声鲁棒性测试。

Result: GPT系列模型在多数情况下优于其他LLM和IndicXlit，微调后的GPT-4o对特定语言表现显著提升。错误分析揭示了LLM相比专业模型的优势。

Conclusion: 基础大语言模型在专业转写任务中展现出强大潜力，能以最小成本覆盖广泛的专业应用场景。

Abstract: Transliteration, the process of mapping text from one script to another,
plays a crucial role in multilingual natural language processing, especially
within linguistically diverse contexts such as India. Despite significant
advancements through specialized models like IndicXlit, recent developments in
large language models suggest a potential for general-purpose models to excel
at this task without explicit task-specific training. The current work
systematically evaluates the performance of prominent LLMs, including GPT-4o,
GPT-4.5, GPT-4.1, Gemma-3-27B-it, and Mistral-Large against IndicXlit, a
state-of-the-art transliteration model, across ten major Indian languages.
Experiments utilized standard benchmarks, including Dakshina and Aksharantar
datasets, with performance assessed via Top-1 Accuracy and Character Error
Rate. Our findings reveal that while GPT family models generally outperform
other LLMs and IndicXlit for most instances. Additionally, fine-tuning GPT-4o
improves performance on specific languages notably. An extensive error analysis
and robustness testing under noisy conditions further elucidate strengths of
LLMs compared to specialized models, highlighting the efficacy of foundational
models for a wide spectrum of specialized applications with minimal overhead.

</details>


### [197] [REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Large Reasoning Models](https://arxiv.org/abs/2505.19862)
*Hexuan Deng,Wenxiang Jiao,Xuebo Liu,Jun Rao,Min Zhang*

Main category: cs.CL

TL;DR: REA-RL提出了一种结合反射模型和强化学习的方法，以减少大型推理模型的推理成本，同时保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂任务中表现优异，但存在过度思考导致推理成本高的问题。现有方法效率低下或会损害模型性能。

Method: REA-RL引入小型反射模型进行在线训练，支持并行采样和顺序修订，并设计反射奖励以避免偏好短但无反思的响应。

Result: 实验表明，该方法在保持或提升性能的同时显著提高推理效率，推理成本降低35%。

Conclusion: REA-RL在性能和效率之间取得了良好平衡，有效维持了对难题的反思能力，同时适当减少了对简单问题的反思频率。

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance in complex tasks
but often face the challenge of overthinking, leading to substantially high
inference costs. Existing approaches synthesize shorter reasoning responses for
LRMs to learn, but are inefficient for online usage due to the time-consuming
data generation and filtering processes. Meanwhile, online reinforcement
learning mainly adopts a length reward to encourage short reasoning responses,
but tends to lose the reflection ability and harm the performance. To address
these issues, we propose REA-RL, which introduces a small reflection model for
efficient scaling in online training, offering both parallel sampling and
sequential revision. Besides, a reflection reward is designed to further
prevent LRMs from favoring short yet non-reflective responses. Experiments show
that both methods maintain or enhance performance while significantly improving
inference efficiency. Their combination achieves a good balance between
performance and efficiency, reducing inference costs by 35% without
compromising performance. Further analysis demonstrates that our methods are
effective by maintaining reflection frequency for hard problems while
appropriately reducing it for simpler ones without losing reflection ability.
Codes are available at https://github.com/hexuandeng/REA-RL.

</details>


### [198] [APE: A Data-Centric Benchmark for Efficient LLM Adaptation in Text Summarization](https://arxiv.org/abs/2505.19912)
*Javier Marín*

Main category: cs.CL

TL;DR: APE是一种高效且计算资源需求低的大语言模型微调方法，通过小批量数据迭代优化，显著提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法需要大量计算资源，限制了资源有限的研究者和实践者的应用。APE旨在提供一种简单高效的方法，减少计算需求。

Method: APE通过迭代微调小批量精选数据（200个示例），仅保留性能提升部分，逐步优化模型。

Result: 在新闻摘要任务中，APE仅用T4 GPU在60分钟内实现了40%的BLEU分数提升，性能媲美或超越更复杂的方法如LoRA。

Conclusion: APE为计算资源有限的研究者和实践者提供了一种高效的大语言模型微调方法，具有实际应用价值。

Abstract: We present Adjacent Possible Exploration (APE), a simple yet effective method
for adapting large language models to specific tasks using minimal
computational resources. Unlike traditional fine-tuning that requires extensive
compute, APE iteratively fine-tunes models on small, carefully selected data
batches (200 examples), retaining only improvements. On news summarization, APE
achieves 40 percent BLEU improvement using just a T4 GPU in 60 minutes,
matching or exceeding more complex methods like LoRA while remaining
conceptually simple. Our approach is particularly valuable for researchers and
practitioners with limited computational resources. We provide open-source code
and demonstrate APE's effectiveness through both automatic metrics and human
evaluation. While inspired by evolutionary theory's "adjacent possible", APE's
core insight has a very practical application: small, iterative data
perturbations can efficiently guide LLMs toward task-specific performance
without expensive retraining.

</details>


### [199] [Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles](https://arxiv.org/abs/2505.19914)
*Jiangjie Chen,Qianyu He,Siyu Yuan,Aili Chen,Zhicheng Cai,Weinan Dai,Hongli Yu,Qiying Yu,Xuefeng Li,Jiaze Chen,Hao Zhou,Mingxuan Wang*

Main category: cs.CL

TL;DR: 该论文介绍了Enigmata，一个专为提升大语言模型（LLMs）解谜能力而设计的综合套件，包含36个任务和自动评估工具，通过优化训练策略显著提升了模型在解谜和数学推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型（如OpenAI的o1和DeepSeek的R1）在数学和编程等复杂推理任务上表现优异，但在无需领域知识的解谜任务中仍表现不佳。为了填补这一空白，研究者开发了Enigmata套件，旨在系统性提升LLMs的逻辑推理能力。

Method: 论文提出了Enigmata套件，包含36个任务，分为七类，每类任务包括一个可生成无限示例的生成器和一个基于规则的验证器。这种生成器-验证器设计支持可扩展的多任务强化学习（RL）训练，并提出了优化的多任务RLVR策略。

Result: 训练后的模型Qwen2.5-32B-Enigmata在Enigmata-Eval、ARC-AGI等解谜推理基准上表现优异，超越了o3-mini-high和o1模型。此外，Enigmata的数据还能显著提升大模型（如Seed1.5-Thinking）在数学和STEM推理任务（如AIME、BeyondAIME）上的性能。

Conclusion: Enigmata为提升LLMs的逻辑推理能力提供了一个统一且可控的框架，其生成器-验证器设计和优化的训练策略不仅提升了模型在解谜任务上的表现，还展现出良好的泛化能力。

Abstract: Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at
advanced reasoning tasks like math and coding via Reinforcement Learning with
Verifiable Rewards (RLVR), but still struggle with puzzles solvable by humans
without domain knowledge. We introduce Enigmata, the first comprehensive suite
tailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks
across seven categories, each with 1) a generator that produces unlimited
examples with controllable difficulty and 2) a rule-based verifier for
automatic evaluation. This generator-verifier design supports scalable,
multi-task RL training, fine-grained analysis, and seamless RLVR integration.
We further propose Enigmata-Eval, a rigorous benchmark, and develop optimized
multi-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata,
consistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks
like Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes
well to out-of-domain puzzle benchmarks and mathematical reasoning, with little
multi-tasking trade-off. When trained on larger models like Seed1.5-Thinking
(20B activated parameters and 200B total parameters), puzzle data from Enigmata
further boosts SoTA performance on advanced math and STEM reasoning tasks such
as AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization
benefits of Enigmata. This work offers a unified, controllable framework for
advancing logical reasoning in LLMs. Resources of this work can be found at
https://seed-enigmata.github.io.

</details>


### [200] [ALAS: Measuring Latent Speech-Text Alignment For Spoken Language Understanding In Multimodal LLMs](https://arxiv.org/abs/2505.19937)
*Pooneh Mousavi,Yingzhi Wang,Mirco Ravanelli,Cem Subakan*

Main category: cs.CL

TL;DR: 该论文提出了一种新指标ALAS，用于评估大语言模型中音频与文本模态的对齐质量，并在两项任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估大语言模型中音频与文本模态对齐质量的标准指标，影响了多模态学习的效果评估。

Method: 提出ALAS指标，通过分析音频与文本表征在Transformer各层的相关性来评估对齐质量，应用于口语问答和情感识别任务。

Result: ALAS指标在不同任务和模型层中表现出预期的行为，有效反映了跨模态对齐质量。

Conclusion: ALAS为评估大语言模型中的跨模态对齐提供了一种标准化方法，有助于改进多模态学习。

Abstract: Large Language Models (LLMs) are widely used in Spoken Language Understanding
(SLU). Recent SLU models process audio directly by adapting speech input into
LLMs for better multimodal learning. A key consideration for these models is
the cross-modal alignment between text and audio modalities, which is a
telltale sign as to whether or not LLM is able to associate semantic meaning to
audio segments. While various methods exist for fusing these modalities, there
is no standard metric to evaluate alignment quality in LLMs. In this work, we
propose a new metric, ALAS (Automatic Latent Alignment Score). Our study
examines the correlation between audio and text representations across
transformer layers, for two different tasks (Spoken Question Answering and
Emotion Recognition). We showcase that our metric behaves as expected across
different layers and different tasks.

</details>


### [201] [MiniLongBench: The Low-cost Long Context Understanding Benchmark for Large Language Models](https://arxiv.org/abs/2505.19959)
*Zhongzhan Huang,Guoming Ling,Shanshan Zhong,Hefeng Wu,Liang Lin*

Main category: cs.CL

TL;DR: 提出了一种针对长文本数据的精简压缩方法，创建了低成本的MiniLongBench基准测试，显著降低了评估成本。


<details>
  <summary>Details</summary>
Motivation: 现有的长文本理解(LCU)基准测试由于数据冗长，导致评估成本过高，阻碍了大型语言模型(LLMs)的研究进展。

Method: 通过剪枝著名的LCU基准测试LongBench，提出了一种针对信息稀疏长文本数据的压缩方法，创建了MiniLongBench基准测试。

Result: MiniLongBench将评估成本降至原始的4.5%，同时与LongBench结果的排名相关系数保持在0.97。

Conclusion: MiniLongBench作为一种低成本基准测试，有望极大推动未来LLMs在LCU能力方面的研究。

Abstract: Long Context Understanding (LCU) is a critical area for exploration in
current large language models (LLMs). However, due to the inherently lengthy
nature of long-text data, existing LCU benchmarks for LLMs often result in
prohibitively high evaluation costs, like testing time and inference expenses.
Through extensive experimentation, we discover that existing LCU benchmarks
exhibit significant redundancy, which means the inefficiency in evaluation. In
this paper, we propose a concise data compression method tailored for long-text
data with sparse information characteristics. By pruning the well-known LCU
benchmark LongBench, we create MiniLongBench. This benchmark includes only 237
test samples across six major task categories and 21 distinct tasks. Through
empirical analysis of over 60 LLMs, MiniLongBench achieves an average
evaluation cost reduced to only 4.5% of the original while maintaining an
average rank correlation coefficient of 0.97 with LongBench results. Therefore,
our MiniLongBench, as a low-cost benchmark, holds great potential to
substantially drive future research into the LCU capabilities of LLMs. See
https://github.com/MilkThink-Lab/MiniLongBench for our code, data and tutorial.

</details>


### [202] [CP-Router: An Uncertainty-Aware Router Between LLM and LRM](https://arxiv.org/abs/2505.19970)
*Jiayuan Su,Fulin Lin,Zhaopeng Feng,Han Zheng,Teng Wang,Zhenyu Xiao,Xinlong Zhao,Zuozhu Liu,Lu Cheng,Hongwei Wang*

Main category: cs.CL

TL;DR: CP-Router是一个无需训练、模型无关的路由框架，通过动态选择LLM或LRM来优化推理效率，同时保持或提升准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRM）在处理简单查询时会产生不必要的冗长输出，导致效率低下甚至准确性下降，因此需要一种方法来动态选择最适合的模型。

Method: 提出CP-Router框架，利用Conformal Prediction（CP）的预测不确定性估计进行路由决策，并引入Full and Binary Entropy（FBE）来优化不确定性区分。

Result: 实验表明，CP-Router在多种MCQA基准测试中有效减少了token使用，同时保持或提升了准确性，并在开放域QA中表现出良好的通用性和鲁棒性。

Conclusion: CP-Router通过动态路由机制，显著提升了推理效率和准确性，适用于多种模型配对和任务场景。

Abstract: Recent advances in Large Reasoning Models (LRMs) have significantly improved
long-chain reasoning capabilities over Large Language Models (LLMs). However,
LRMs often produce unnecessarily lengthy outputs even for simple queries,
leading to inefficiencies or even accuracy degradation compared to LLMs. To
overcome this, we propose CP-Router, a training-free and model-agnostic routing
framework that dynamically selects between an LLM and an LRM, demonstrated with
multiple-choice question answering (MCQA) prompts. The routing decision is
guided by the prediction uncertainty estimates derived via Conformal Prediction
(CP), which provides rigorous coverage guarantees. To further refine the
uncertainty differentiation across inputs, we introduce Full and Binary Entropy
(FBE), a novel entropy-based criterion that adaptively selects the appropriate
CP threshold. Experiments across diverse MCQA benchmarks, including
mathematics, logical reasoning, and Chinese chemistry, demonstrate that
CP-Router efficiently reduces token usage while maintaining or even improving
accuracy compared to using LRM alone. We also extend CP-Router to diverse model
pairings and open-ended QA, where it continues to demonstrate strong
performance, validating its generality and robustness.

</details>


### [203] [Conversational Lexicography: Querying Lexicographic Data on Knowledge Graphs with SPARQL through Natural Language](https://arxiv.org/abs/2505.19971)
*Kilian Sennrich,Sina Ahmadi*

Main category: cs.CL

TL;DR: 该论文探讨了如何为知识图谱（如Wikidata）中的词典数据创建自然语言接口，以降低非专家用户使用SPARQL查询语言的难度。通过构建多维分类法和模板数据集，实验比较了不同模型的性能，发现GPT-3.5-Turbo在泛化能力上表现最佳，但仍面临诸多挑战。


<details>
  <summary>Details</summary>
Motivation: 知识图谱能有效表示词典数据的词汇语义结构，但SPARQL查询语言对非专家用户构成使用障碍。论文旨在通过自然语言接口简化词典数据的检索，使更多用户受益于这一技术。

Method: 研究构建了一个多维分类法，涵盖Wikidata词典数据本体的复杂性，并创建了包含120万条自然语言到SPARQL查询映射的模板数据集。实验对比了GPT-2、Phi-1.5和GPT-3.5-Turbo的性能。

Result: 实验显示，所有模型在熟悉模式上表现良好，但只有GPT-3.5-Turbo展现出有意义的泛化能力，表明模型规模和多样化预训练对该领域的适应性至关重要。

Conclusion: 尽管GPT-3.5-Turbo表现优异，但在实现稳健泛化、处理多样化语言数据及开发可扩展解决方案方面仍存在显著挑战。

Abstract: Knowledge graphs offer an excellent solution for representing the
lexical-semantic structures of lexicographic data. However, working with the
SPARQL query language represents a considerable hurdle for many non-expert
users who could benefit from the advantages of this technology. This paper
addresses the challenge of creating natural language interfaces for
lexicographic data retrieval on knowledge graphs such as Wikidata. We develop a
multidimensional taxonomy capturing the complexity of Wikidata's lexicographic
data ontology module through four dimensions and create a template-based
dataset with over 1.2 million mappings from natural language utterances to
SPARQL queries. Our experiments with GPT-2 (124M), Phi-1.5 (1.3B), and
GPT-3.5-Turbo reveal significant differences in model capabilities. While all
models perform well on familiar patterns, only GPT-3.5-Turbo demonstrates
meaningful generalization capabilities, suggesting that model size and diverse
pre-training are crucial for adaptability in this domain. However, significant
challenges remain in achieving robust generalization, handling diverse
linguistic data, and developing scalable solutions that can accommodate the
full complexity of lexicographic knowledge representation.

</details>


### [204] [DeepDialogue: A Multi-Turn Emotionally-Rich Spoken Dialogue Dataset](https://arxiv.org/abs/2505.19978)
*Alkis Koudounas,Moreno La Quatra,Elena Baralis*

Main category: cs.CL

TL;DR: 论文提出DeepDialogue数据集，解决现有对话数据在情感范围、领域多样性和多模态方面的不足，通过多模型生成和严格过滤创建大规模多模态对话数据集。


<details>
  <summary>Details</summary>
Motivation: 当前对话AI在单轮响应表现出色，但多轮对话仍面临挑战，现有数据集情感范围窄、领域单一且多为纯文本，限制了跨模态人机对话系统的发展。

Method: 使用9种不同规模语言模型生成初始对话，结合人工标注和LLM质量过滤，构建含40,150条多领域、多情感的多模态对话数据集，并合成情感匹配的语音。

Result: 发现小模型在6轮后失去连贯性，具体领域对话质量更高，跨模型交互比同模型对话更连贯，并创建首个开源多模态情感对话数据集。

Conclusion: DeepDialogue填补了多模态情感对话数据空白，为开发更人性化的对话系统提供了关键资源，揭示了模型规模、领域特性对对话质量的影响。

Abstract: Recent advances in conversational AI have demonstrated impressive
capabilities in single-turn responses, yet multi-turn dialogues remain
challenging for even the most sophisticated language models. Current dialogue
datasets are limited in their emotional range, domain diversity, turn depth,
and are predominantly text-only, hindering progress in developing more
human-like conversational systems across modalities. To address these
limitations, we present DeepDialogue, a large-scale multimodal dataset
containing 40,150 high-quality multi-turn dialogues spanning 41 domains and
incorporating 20 distinct emotions with coherent emotional progressions. Our
approach pairs 9 different language models (4B-72B parameters) to generate
65,600 initial conversations, which we then evaluate through a combination of
human annotation and LLM-based quality filtering. The resulting dataset reveals
fundamental insights: smaller models fail to maintain coherence beyond 6
dialogue turns; concrete domains (e.g., "cars," "travel") yield more meaningful
conversations than abstract ones (e.g., "philosophy"); and cross-model
interactions produce more coherent dialogues than same-model conversations. A
key contribution of DeepDialogue is its speech component, where we synthesize
emotion-consistent voices for all 40,150 dialogues, creating the first
large-scale open-source multimodal dialogue dataset that faithfully preserves
emotional context across multi-turn conversations.

</details>


### [205] [How Well Do Large Reasoning Models Translate? A Comprehensive Evaluation for Multi-Domain Machine Translation](https://arxiv.org/abs/2505.19987)
*Yongshi Ye,Biao Fu,Chongxuan Huang,Yidong Chen,Xiaodong Shi*

Main category: cs.CL

TL;DR: 大型推理模型（LRMs）在复杂领域翻译任务中表现优于传统大型语言模型（LLMs），尤其是在长文本和高难度场景下。


<details>
  <summary>Details</summary>
Motivation: 探讨结构化推理是否能提升多领域机器翻译（MDMT）的质量，尤其是在复杂、领域敏感的翻译任务中。

Method: 比较LRMs与传统LLMs在15个代表性领域和四个翻译方向上的表现，结合自动指标和增强的MQM评估体系分析翻译质量。

Result: LRMs在语义复杂的领域表现更优，尤其是在长文本和高难度翻译任务中，领域自适应提示策略进一步提升了性能。

Conclusion: 结构化推理在MDMT任务中具有潜力，为优化领域敏感翻译系统提供了重要见解。

Abstract: Large language models (LLMs) have demonstrated strong performance in
general-purpose machine translation, but their effectiveness in complex,
domain-sensitive translation tasks remains underexplored. Recent advancements
in Large Reasoning Models (LRMs), raise the question of whether structured
reasoning can enhance translation quality across diverse domains. In this work,
we compare the performance of LRMs with traditional LLMs across 15
representative domains and four translation directions. Our evaluation
considers various factors, including task difficulty, input length, and
terminology density. We use a combination of automatic metrics and an enhanced
MQM-based evaluation hierarchy to assess translation quality. Our findings show
that LRMs consistently outperform traditional LLMs in semantically complex
domains, especially in long-text and high-difficulty translation scenarios.
Moreover, domain-adaptive prompting strategies further improve performance by
better leveraging the reasoning capabilities of LRMs. These results highlight
the potential of structured reasoning in MDMT tasks and provide valuable
insights for optimizing translation systems in domain-sensitive contexts.

</details>


### [206] [Mixture of LoRA Experts for Low-Resourced Multi-Accent Automatic Speech Recognition](https://arxiv.org/abs/2505.20006)
*Raphaël Bagat,Irina Illina,Emmanuel Vincent*

Main category: cs.CL

TL;DR: 提出MAS-LoRA方法，通过混合多个口音专家提升ASR系统在非母语多口音场景下的鲁棒性，无需重新微调。


<details>
  <summary>Details</summary>
Motivation: 提升自动语音识别(ASR)系统在低资源多口音环境下对非母语语音的鲁棒性。

Method: 采用混合口音专用低秩适配器(MAS-LoRA)，每个专家专注于特定口音，支持已知/未知口音推理场景。

Result: 在Whisper模型和L2-ARCTIC数据集上，词错误率显著优于常规LoRA和全微调，且灾难性遗忘更少。

Conclusion: MAS-LoRA首次将混合LoRA专家应用于非母语多口音ASR，性能优越且具备通用性。

Abstract: We aim to improve the robustness of Automatic Speech Recognition (ASR)
systems against non-native speech, particularly in low-resourced multi-accent
settings. We introduce Mixture of Accent-Specific LoRAs (MAS-LoRA), a
fine-tuning method that leverages a mixture of Low-Rank Adaptation (LoRA)
experts, each specialized in a specific accent. This method can be used when
the accent is known or unknown at inference time, without the need to fine-tune
the model again. Our experiments, conducted using Whisper on the L2-ARCTIC
corpus, demonstrate significant improvements in Word Error Rate compared to
regular LoRA and full fine-tuning when the accent is unknown. When the accent
is known, the results further improve. Furthermore, MAS-LoRA shows less
catastrophic forgetting than the other fine-tuning methods. To the best of our
knowledge, this is the first use of a mixture of LoRA experts for non-native
multi-accent ASR.

</details>


### [207] [WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback](https://arxiv.org/abs/2505.20013)
*Minda Hu,Tianqing Fang,Jianshu Zhang,Junyu Ma,Zhisong Zhang,Jingyan Zhou,Hongming Zhang,Haitao Mi,Dong Yu,Irwin King*

Main category: cs.CL

TL;DR: 该论文通过识别并增强基于LLM的网页代理的关键推理技能，显著提升了其在动态不确定环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型（LLM）的网页代理在不确定和动态的网页环境中推理能力有限，限制了其稳健部署。

Method: 识别关键推理技能（如反思与前瞻、分支和回滚），并通过微调将这些推理模式融入骨干LLM中。

Result: 在多个基准测试（如WebVoyager、Mind2web-live和SimpleQA）中表现出显著性能提升。

Conclusion: 针对性地增强推理技能能有效提升网页代理的性能，展示了该方法在下一代AI中的潜力。

Abstract: Web agents powered by Large Language Models (LLMs) show promise for
next-generation AI, but their limited reasoning in uncertain, dynamic web
environments hinders robust deployment. In this paper, we identify key
reasoning skills essential for effective web agents, i.e., reflection &
lookahead, branching, and rollback, and curate trajectory data that exemplifies
these abilities by reconstructing the agent's (inference-time) reasoning
algorithms into chain-of-thought rationales. We conduct experiments in the
agent self-improving benchmark, OpenWebVoyager, and demonstrate that distilling
salient reasoning patterns into the backbone LLM via simple fine-tuning can
substantially enhance its performance. Our approach yields significant
improvements across multiple benchmarks, including WebVoyager, Mind2web-live,
and SimpleQA (web search), highlighting the potential of targeted reasoning
skill enhancement for web agents.

</details>


### [208] [Does Rationale Quality Matter? Enhancing Mental Disorder Detection via Selective Reasoning Distillation](https://arxiv.org/abs/2505.20014)
*Hoyun Song,Huije Lee,Jisu Shin,Sukmin Cho,Changgeon Ko,Jong C. Park*

Main category: cs.CL

TL;DR: 研究探讨了高质量解释对小型语言模型在心理健康检测中性能的影响，提出基于临床推理对齐的框架提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在心理健康检测中生成解释能力强，但计算成本高；小型模型通过推理蒸馏继承能力，但解释的相关性和领域对齐存在挑战。

Method: 提出一个框架，通过选择与专家临床推理对齐的高质量解释，优化小型语言模型的推理蒸馏过程。

Result: 实验表明，该质量导向方法显著提升了小型模型在心理障碍检测和解释生成中的性能。

Conclusion: 研究强调了解释质量的重要性，并为心理健康应用中的知识转移提供了有效框架。

Abstract: The detection of mental health problems from social media and the
interpretation of these results have been extensively explored. Research has
shown that incorporating clinical symptom information into a model enhances
domain expertise, improving its detection and interpretation performance. While
large language models (LLMs) are shown to be effective for generating
explanatory rationales in mental health detection, their substantially large
parameter size and high computational cost limit their practicality. Reasoning
distillation transfers this ability to smaller language models (SLMs), but
inconsistencies in the relevance and domain alignment of LLM-generated
rationales pose a challenge. This paper investigates how rationale quality
impacts SLM performance in mental health detection and explanation generation.
We hypothesize that ensuring high-quality and domain-relevant rationales
enhances the distillation. To this end, we propose a framework that selects
rationales based on their alignment with expert clinical reasoning. Experiments
show that our quality-focused approach significantly enhances SLM performance
in both mental disorder detection and rationale generation. This work
highlights the importance of rationale quality and offers an insightful
framework for knowledge transfer in mental health applications.

</details>


### [209] [On the class of coding optimality of human languages and the origins of Zipf's law](https://arxiv.org/abs/2505.20015)
*Ramon Ferrer-i-Cancho*

Main category: cs.CL

TL;DR: 该论文提出了一种新的编码系统最优性类别，其成员与最优编码线性分离并呈现Zipf定律。研究发现人类语言属于该类别，而某些动物通信系统则不符合。


<details>
  <summary>Details</summary>
Motivation: 研究Zipf定律在编码系统中的起源，探索压缩与最优编码之间的关系。

Method: 通过分析频率与排名的双对数坐标图，识别符合Zipf定律的系统，并验证其编码最优性。

Result: 人类语言符合新提出的最优性类别，而海豚和座头鲸的通信系统可能也属于此类。

Conclusion: 研究支持Zipf定律源于压缩的假设，为编码最优性提供了新的理论依据。

Abstract: Here we present a new class of optimality for coding systems. Members of that
class are separated linearly from optimal coding and thus exhibit Zipf's law,
namely a power-law distribution of frequency ranks. Whithin that class, Zipf's
law, the size-rank law and the size-probability law form a group-like
structure. We identify human languages that are members of the class. All
languages showing sufficient agreement with Zipf's law are potential members of
the class. In contrast, there are communication systems in other species that
cannot be members of that class for exhibiting an exponential distribution
instead but dolphins and humpback whales might. We provide a new insight into
plots of frequency versus rank in double logarithmic scale. For any system, a
straight line in that scale indicates that the lengths of optimal codes under
non-singular coding and under uniquely decodable encoding are separated by a
linear function whose slope is the exponent of Zipf's law. For systems under
compression and constrained to be uniquely decodable, such a straight line may
indicate that the system is coding close to optimality. Our findings provide
support for the hypothesis that Zipf's law originates from compression.

</details>


### [210] [TTPA: Token-level Tool-use Preference Alignment Training Framework with Fine-grained Evaluation](https://arxiv.org/abs/2505.20016)
*Chengrui Huang,Shen Gao,Zhengliang Shi,Dongsheng Wang,Shuo Shang*

Main category: cs.CL

TL;DR: 提出TTPA框架，通过细粒度偏好对齐和错误导向评分机制提升工具使用性能。


<details>
  <summary>Details</summary>
Motivation: 现有工具学习方法依赖监督微调，忽视工具调用细节的细粒度优化，导致偏好对齐和错误判别受限。

Method: TTPA框架包含反向数据集构建、令牌级偏好采样和错误导向评分机制。

Result: 在三个基准数据集上实验表明，TTPA显著提升工具使用性能并展现强泛化能力。

Conclusion: TTPA通过细粒度偏好对齐和错误量化，有效优化工具调用性能。

Abstract: Existing tool-learning methods usually rely on supervised fine-tuning, they
often overlook fine-grained optimization of internal tool call details, leading
to limitations in preference alignment and error discrimination. To overcome
these challenges, we propose Token-level Tool-use Preference Alignment Training
Framework (TTPA), a training paradigm for constructing token-level tool-use
preference datasets that align LLMs with fine-grained preferences using a novel
error-oriented scoring mechanism. TTPA first introduces reversed dataset
construction, a method for creating high-quality, multi-turn tool-use datasets
by reversing the generation flow. Additionally, we propose Token-level
Preference Sampling (TPS) to capture fine-grained preferences by modeling
token-level differences during generation. To address biases in scoring, we
introduce the Error-oriented Scoring Mechanism (ESM), which quantifies
tool-call errors and can be used as a training signal. Extensive experiments on
three diverse benchmark datasets demonstrate that TTPA significantly improves
tool-using performance while showing strong generalization ability across
models and datasets.

</details>


### [211] [Training LLM-Based Agents with Synthetic Self-Reflected Trajectories and Partial Masking](https://arxiv.org/abs/2505.20023)
*Yihan Chen,Benfeng Xu,Xiaorui Wang,Yongdong Zhang,Zhendong Mao*

Main category: cs.CL

TL;DR: 论文提出STeP方法，通过合成自反思轨迹和部分掩码策略，提升基于LLM的自主代理性能，减少训练数据需求。


<details>
  <summary>Details</summary>
Motivation: 当前强大的自主代理依赖复杂的提示工程和闭源LLM，而通过专家轨迹训练开源模型存在性能瓶颈和错误传播问题。

Method: 提出STeP方法，合成包含错误步骤反思与修正的自反思轨迹，并引入部分掩码策略避免模型学习次优步骤。

Result: 实验表明，该方法在ALFWorld、WebShop和SciWorld任务中提升性能，LLaMA2-7B-Chat模型使用更少数据即实现全面改进。

Conclusion: STeP方法有效增强了LLM代理的自反思与纠错能力，为开源模型训练提供了新思路。

Abstract: Autonomous agents, which perceive environments and take actions to achieve
goals, have become increasingly feasible with the advancements in large
language models (LLMs). However, current powerful agents often depend on
sophisticated prompt engineering combined with closed-source LLMs like GPT-4.
Although training open-source LLMs using expert trajectories from teacher
models has yielded some improvements in agent capabilities, this approach still
faces limitations such as performance plateauing and error propagation. To
mitigate these challenges, we propose STeP, a novel method for improving
LLM-based agent training. We synthesize self-reflected trajectories that
include reflections and corrections of error steps, which enhance the
effectiveness of LLM agents in learning from teacher models, enabling them to
become agents capable of self-reflecting and correcting. We also introduce
partial masking strategy that prevents the LLM from internalizing incorrect or
suboptimal steps. Experiments demonstrate that our method improves agent
performance across three representative tasks: ALFWorld, WebShop, and SciWorld.
For the open-source model LLaMA2-7B-Chat, when trained using self-reflected
trajectories constructed with Qwen1.5-110B-Chat as the teacher model, it
achieves comprehensive improvements with less training data compared to agents
trained exclusively on expert trajectories.

</details>


### [212] [Uncertainty-Aware Attention Heads: Efficient Unsupervised Uncertainty Quantification for LLMs](https://arxiv.org/abs/2505.20045)
*Artem Vazhentsev,Lyudmila Rvanova,Gleb Kuzmin,Ekaterina Fadeeva,Ivan Lazichny,Alexander Panchenko,Maxim Panov,Timothy Baldwin,Mrinmaya Sachan,Preslav Nakov,Artem Shelmanov*

Main category: cs.CL

TL;DR: 提出RAUQ方法，利用Transformer注意力机制无监督检测大语言模型幻觉，计算高效且无需调参。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在幻觉问题，现有不确定性量化方法计算成本高或依赖监督学习，需高效无监督解决方案。

Method: 基于Transformer注意力权重（特定'不确定性感知头'在错误生成时注意力下降），递归聚合注意力模式计算序列级不确定性分数。

Result: 在4个LLM和12个任务上超越现有方法，延迟低于1%，且无需标签和调参。

Conclusion: RAUQ为白盒LLM提供即插即用的实时幻觉检测方案。

Abstract: Large language models (LLMs) exhibit impressive fluency, but often produce
critical errors known as "hallucinations". Uncertainty quantification (UQ)
methods are a promising tool for coping with this fundamental shortcoming. Yet,
existing UQ methods face challenges such as high computational overhead or
reliance on supervised learning. Here, we aim to bridge this gap. In
particular, we propose RAUQ (Recurrent Attention-based Uncertainty
Quantification), an unsupervised approach that leverages intrinsic attention
patterns in transformers to detect hallucinations efficiently. By analyzing
attention weights, we identified a peculiar pattern: drops in attention to
preceding tokens are systematically observed during incorrect generations for
certain "uncertainty-aware" heads. RAUQ automatically selects such heads,
recurrently aggregates their attention weights and token-level confidences, and
computes sequence-level uncertainty scores in a single forward pass.
Experiments across 4 LLMs and 12 question answering, summarization, and
translation tasks demonstrate that RAUQ yields excellent results, outperforming
state-of-the-art UQ methods using minimal computational overhead (<1% latency).
Moreover, it requires no task-specific labels and no careful hyperparameter
tuning, offering plug-and-play real-time hallucination detection in white-box
LLMs.

</details>


### [213] [Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks](https://arxiv.org/abs/2505.20047)
*Debargha Ganguly,Vikash Singh,Sreehari Sankar,Biyao Zhang,Xuecen Zhang,Srinivasan Iyengar,Xiaotian Han,Amit Sharma,Shivkumar Kalyanaraman,Vipin Chaudhary*

Main category: cs.CL

TL;DR: 该论文研究了大语言模型（LLMs）在生成形式化规范时的失败模式和不确定性量化，提出了一种轻量级融合方法，显著减少了错误并提高了可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自动化推理中表现出巨大潜力，但其概率性与形式化验证所需的确定性之间存在矛盾。论文旨在解决这一认知差距，全面研究LLM生成的形式化产物中的失败模式和不确定性量化。

Method: 论文系统评估了五种前沿LLM，引入了概率上下文无关文法（PCFG）框架来建模LLM输出，并提出了一种轻量级融合方法，结合多种不确定性信号进行选择性验证。

Result: 研究发现，不确定性信号具有任务依赖性（如逻辑任务的语法熵AUROC>0.93），轻量级融合方法显著减少了错误（14-100%），同时保持了较低的弃权率。

Conclusion: 通过不确定性量化和选择性验证，LLM驱动的形式化可以转变为可靠的工程学科，为自动化推理提供了更可靠的解决方案。

Abstract: Large language models (LLMs) show remarkable promise for democratizing
automated reasoning by generating formal specifications. However, a fundamental
tension exists: LLMs are probabilistic, while formal verification demands
deterministic guarantees. This paper addresses this epistemological gap by
comprehensively investigating failure modes and uncertainty quantification (UQ)
in LLM-generated formal artifacts. Our systematic evaluation of five frontier
LLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's
domain-specific impact on accuracy (from +34.8% on logical tasks to -44.5% on
factual ones), with known UQ techniques like the entropy of token probabilities
failing to identify these errors. We introduce a probabilistic context-free
grammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty
taxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy
for logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables
selective verification, drastically reducing errors (14-100%) with minimal
abstention, transforming LLM-driven formalization into a reliable engineering
discipline.

</details>


### [214] [Incentivizing Reasoning from Weak Supervision](https://arxiv.org/abs/2505.20072)
*Yige Yuan,Teng Xiao,Shuchang Tao,Xue Wang,Jinyang Gao,Bolin Ding,Bingbing Xu*

Main category: cs.CL

TL;DR: 论文提出利用弱模型监督提升大语言模型推理能力的方法，成本低廉且效果接近昂贵方法。


<details>
  <summary>Details</summary>
Motivation: 现有增强大语言模型推理能力的方法（如强化学习和高质量思维链监督）成本高昂，本文探索是否能用弱模型监督有效激励强模型的推理能力。

Method: 研究通过显著弱于学生模型的监督者（弱推理模型）来激励学生模型（强模型）的推理能力，并分析该方法的适用条件与成功原因。

Result: 实验表明，弱监督可恢复昂贵强化学习94%的性能增益，且该方法在多样化基准和模型架构中均能稳定提升推理任务表现。

Conclusion: 弱到强的监督范式是替代昂贵方法的有效方案，能低成本激发大语言模型在推理时的强能力，具有通用性和推广潜力。

Abstract: Large language models (LLMs) have demonstrated impressive performance on
reasoning-intensive tasks, but enhancing their reasoning abilities typically
relies on either reinforcement learning (RL) with verifiable signals or
supervised fine-tuning (SFT) with high-quality long chain-of-thought (CoT)
demonstrations, both of which are expensive. In this paper, we study a novel
problem of incentivizing the reasoning capacity of LLMs without expensive
high-quality demonstrations and reinforcement learning. We investigate whether
the reasoning capabilities of LLMs can be effectively incentivized via
supervision from significantly weaker models. We further analyze when and why
such weak supervision succeeds in eliciting reasoning abilities in stronger
models. Our findings show that supervision from significantly weaker reasoners
can substantially improve student reasoning performance, recovering close to
94% of the gains of expensive RL at a fraction of the cost. Experiments across
diverse benchmarks and model architectures demonstrate that weak reasoners can
effectively incentivize reasoning in stronger student models, consistently
improving performance across a wide range of reasoning tasks. Our results
suggest that this simple weak-to-strong paradigm is a promising and
generalizable alternative to costly methods for incentivizing strong reasoning
capabilities at inference-time in LLMs. The code is publicly available at
https://github.com/yuanyige/W2SR.

</details>


### [215] [Inference-time Alignment in Continuous Space](https://arxiv.org/abs/2505.20081)
*Yige Yuan,Teng Xiao,Li Yunfan,Bingbing Xu,Shuchang Tao,Yunqi Qiu,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: 论文提出了一种名为SEA的简单有效算法，通过基于梯度的采样在连续潜在空间中优化能量函数，实现推理时对齐，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在基础策略较弱或候选集较小时难以探索信息丰富的候选响应，导致效果有限。为了解决这一问题，论文提出了SEA算法。

Method: SEA通过基于梯度的采样在连续潜在空间中直接优化原始响应，将其朝向最优策略方向调整，避免了离散空间中的昂贵搜索。

Result: SEA在AdvBench和MATH数据集上分别实现了77.51%和16.36%的相对改进，显著优于基线方法。

Conclusion: SEA是一种简单而有效的推理时对齐算法，通过连续空间优化显著提升了性能，代码已开源。

Abstract: Aligning large language models with human feedback at inference time has
received increasing attention due to its flexibility. Existing methods rely on
generating multiple responses from the base policy for search using a reward
model, which can be considered as searching in a discrete response space.
However, these methods struggle to explore informative candidates when the base
policy is weak or the candidate set is small, resulting in limited
effectiveness. In this paper, to address this problem, we propose Simple Energy
Adaptation ($\textbf{SEA}$), a simple yet effective algorithm for
inference-time alignment. In contrast to expensive search over the discrete
space, SEA directly adapts original responses from the base policy toward the
optimal one via gradient-based sampling in continuous latent space.
Specifically, SEA formulates inference as an iterative optimization procedure
on an energy function over actions in the continuous space defined by the
optimal policy, enabling simple and effective alignment. For instance, despite
its simplicity, SEA outperforms the second-best baseline with a relative
improvement of up to $ \textbf{77.51%}$ on AdvBench and $\textbf{16.36%}$ on
MATH. Our code is publicly available at https://github.com/yuanyige/SEA

</details>


### [216] [Multi-Domain Explainability of Preferences](https://arxiv.org/abs/2505.20088)
*Nitay Calderon,Liat Ein-Dor,Roi Reichart*

Main category: cs.CL

TL;DR: 提出一种自动化方法，通过概念解释大模型偏好机制，提升预测性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有偏好机制（如人类偏好、LLM评判、奖励模型）的核心概念尚不清晰，需系统化解释方法。

Method: 使用LLM自动提取区分性概念并向量化，构建分层多领域回归模型建模概念与偏好的关系。

Result: 在8个领域数据集上超越基线模型，概念解释能有效指导LLM输出并提升评判预测。

Conclusion: 为LLM时代提供可解释性新范式，概念驱动方法兼具性能与透明度优势。

Abstract: Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and
reward models, are central to aligning and evaluating large language models
(LLMs). Yet, the underlying concepts that drive these preferences remain poorly
understood. In this work, we propose a fully automated end-to-end method for
generating local and global concept-based explanations of preferences across
multiple domains. Our method employs an LLM to discover concepts that
differentiate between chosen and rejected responses and represent them with
concept-based vectors. To model the relationships between concepts and
preferences, we propose a white-box Hierarchical Multi-Domain Regression model
that captures both domain-general and domain-specific effects. To evaluate our
method, we curate a dataset spanning eight challenging and diverse domains and
explain twelve mechanisms. Our method achieves strong preference prediction
performance, outperforming baselines while also being explainable.
Additionally, we assess explanations in two novel application-driven settings.
First, guiding LLM outputs with concepts from LaaJ explanations yields
responses that those judges consistently prefer. Second, prompting LaaJs with
concepts explaining humans improves their preference predictions. Together, our
work provides a new paradigm for explainability in the era of LLMs.

</details>


### [217] [MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.20096)
*Thang Nguyen,Peter Chin,Yu-Wing Tai*

Main category: cs.CL

TL;DR: MA-RAG是一个多智能体框架，通过协作解决RAG中的模糊性和推理挑战，无需微调即可实现高效、可解释的结果。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在处理复杂信息检索任务时存在模糊性和推理挑战，MA-RAG旨在通过多智能体协作解决这些问题。

Method: MA-RAG采用多智能体协作框架，包括Planner、Step Definer、Extractor和QA Agents，通过任务感知推理分解和解决RAG流程中的各个阶段。

Result: 实验表明，MA-RAG在复杂QA任务中优于无需训练的基线方法，并与微调系统性能相当。

Conclusion: MA-RAG通过多智能体协作和动态工作流，显著提升了RAG的鲁棒性和可解释性，验证了基于智能体推理的有效性。

Abstract: We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation
(RAG) that addresses the inherent ambiguities and reasoning challenges in
complex information-seeking tasks. Unlike conventional RAG methods that rely on
either end-to-end fine-tuning or isolated component enhancements, MA-RAG
orchestrates a collaborative set of specialized AI agents: Planner, Step
Definer, Extractor, and QA Agents, to tackle each stage of the RAG pipeline
with task-aware reasoning. Ambiguities may arise from underspecified queries,
sparse or indirect evidence in retrieved documents, or the need to integrate
information scattered across multiple sources. MA-RAG mitigates these
challenges by decomposing the problem into subtasks, such as query
disambiguation, evidence extraction, and answer synthesis, and dispatching them
to dedicated agents equipped with chain-of-thought prompting. These agents
communicate intermediate reasoning and progressively refine the retrieval and
synthesis process. Our design allows fine-grained control over information flow
without any model fine-tuning. Crucially, agents are invoked on demand,
enabling a dynamic and efficient workflow that avoids unnecessary computation.
This modular and reasoning-driven architecture enables MA-RAG to deliver
robust, interpretable results. Experiments on multi-hop and ambiguous QA
benchmarks demonstrate that MA-RAG outperforms state-of-the-art training-free
baselines and rivals fine-tuned systems, validating the effectiveness of
collaborative agent-based reasoning in RAG.

</details>


### [218] [S2LPP: Small-to-Large Prompt Prediction across LLMs](https://arxiv.org/abs/2505.20097)
*Liang Cheng,Tianyi LI,Zhaowei Wang,Mark Steedman*

Main category: cs.CL

TL;DR: 研究发现不同规模的LLMs在提示模板选择上具有一致性，提出用小模型为大模型选择提示模板的方法，显著降低提示工程成本。


<details>
  <summary>Details</summary>
Motivation: 大型预训练语言模型（LLMs）的性能对提示模板的细微变化敏感，需要大量计算和人工成本进行提示工程。研究旨在探索不同规模LLMs在提示偏好上的一致性，并寻找降低提示工程成本的方法。

Method: 通过问答任务实验验证不同规模LLMs的提示偏好一致性，并利用小模型为大规模LLMs选择有效提示模板。

Result: 实验证明该方法能显著降低提示工程成本，同时在候选提示中保持与最优提示相当的性能，并在14种LLMs和多种NLP任务中验证了其有效性。

Conclusion: 该研究提出的方法利用小模型选择提示模板，不仅降低了提示工程的成本，还保持了性能的稳定性，具有广泛的适用性和鲁棒性。

Abstract: The performance of pre-trained Large Language Models (LLMs) is often
sensitive to nuances in prompt templates, requiring careful prompt engineering,
adding costs in terms of computing and human effort. In this study, we present
experiments encompassing multiple LLMs variants of varying sizes aimed at
probing their preference with different prompts. Through experiments on
Question Answering, we show prompt preference consistency across LLMs of
different sizes. We also show that this consistency extends to other tasks,
such as Natural Language Inference. Utilizing this consistency, we propose a
method to use a smaller model to select effective prompt templates for a larger
model. We show that our method substantially reduces the cost of prompt
engineering while consistently matching performance with optimal prompts among
candidates. More importantly, our experiment shows the efficacy of our strategy
across fourteen LLMs and its applicability to a broad range of NLP tasks,
highlighting its robustness

</details>


### [219] [Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities](https://arxiv.org/abs/2505.20099)
*Chuangtao Ma,Yongrui Chen,Tianxing Wu,Arijit Khan,Haofen Wang*

Main category: cs.CL

TL;DR: 该论文综述了如何结合大语言模型（LLMs）与知识图谱（KGs）以提升复杂问答（QA）任务的性能，并提出了分类方法和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在问答任务中表现出色，但在复杂问答任务中存在推理能力不足、知识过时和幻觉问题。结合知识图谱可以弥补这些不足。

Method: 论文提出了一种结构化分类法，根据问答类别和知识图谱在整合中的角色，对结合LLMs和KGs的方法进行分类，并系统综述了最新进展。

Result: 论文总结了结合LLMs和KGs的方法在解决复杂问答挑战中的优势、局限性和知识图谱需求，并提供了评估指标和基准数据集。

Conclusion: 论文强调了结合LLMs和KGs在问答任务中的进展和潜力，同时指出了未来的开放挑战和机遇。

Abstract: Large language models (LLMs) have demonstrated remarkable performance on
question-answering (QA) tasks because of their superior capabilities in natural
language understanding and generation. However, LLM-based QA struggles with
complex QA tasks due to poor reasoning capacity, outdated knowledge, and
hallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs)
for QA to address the above challenges. In this survey, we propose a new
structured taxonomy that categorizes the methodology of synthesizing LLMs and
KGs for QA according to the categories of QA and the KG's role when integrating
with LLMs. We systematically survey state-of-the-art advances in synthesizing
LLMs and KGs for QA and compare and analyze these approaches in terms of
strength, limitations, and KG requirements. We then align the approaches with
QA and discuss how these approaches address the main challenges of different
complex QA. Finally, we summarize the advancements, evaluation metrics, and
benchmark datasets and highlight open challenges and opportunities.

</details>


### [220] [Adaptive Deep Reasoning: Triggering Deep Thinking When Needed](https://arxiv.org/abs/2505.20101)
*Yunhao Wang,Yuhao Zhang,Tinghao Yu,Can Xu,Feng Zhang,Fengzong Lian*

Main category: cs.CL

TL;DR: 本文提出一种新方法，通过强化学习自主切换长短推理链，优化大语言模型的计算效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的长链推理虽强，但计算成本高，现有方法仍需手动控制长短推理切换，限制了实际应用。

Method: 结合监督微调与强化学习，采用长短自适应分组奖励策略和基于logit的推理模式切换损失函数。

Result: 模型能动态切换长短推理模式，在数学数据集上保持性能的同时显著提升效率。

Conclusion: 该方法增强了语言模型推理的实用性，为实际部署提供了有效解决方案。

Abstract: Large language models (LLMs) have shown impressive capabilities in handling
complex tasks through long-chain reasoning. However, the extensive reasoning
steps involved can significantly increase computational costs, posing
challenges for real-world deployment. Recent efforts have focused on optimizing
reasoning efficiency by shortening the Chain-of-Thought (CoT) reasoning
processes through various approaches, such as length-aware prompt engineering,
supervised fine-tuning on CoT data with variable lengths, and reinforcement
learning with length penalties. Although these methods effectively reduce
reasoning length, they still necessitate an initial reasoning phase. More
recent approaches have attempted to integrate long-chain and short-chain
reasoning abilities into a single model, yet they still rely on manual control
to toggle between short and long CoT.In this work, we propose a novel approach
that autonomously switches between short and long reasoning chains based on
problem complexity. Our method begins with supervised fine-tuning of the base
model to equip both long-chain and short-chain reasoning abilities. We then
employ reinforcement learning to further balance short and long CoT generation
while maintaining accuracy through two key strategies: first, integrating
reinforcement learning with a long-short adaptive group-wise reward strategy to
assess prompt complexity and provide corresponding rewards; second,
implementing a logit-based reasoning mode switching loss to optimize the
model's initial token choice, thereby guiding the selection of the reasoning
type.Evaluations on mathematical datasets demonstrate that our model can
dynamically switch between long-chain and short-chain reasoning modes without
substantially sacrificing performance. This advancement enhances the
practicality of reasoning in large language models for real-world applications.

</details>


### [221] [Language-Agnostic Suicidal Risk Detection Using Large Language Models](https://arxiv.org/abs/2505.20109)
*June-Woo Kim,Wonkyo Oh,Haram Yoon,Sung-Hoon Yoon,Dae-Jin Kim,Dong-Ho Lee,Sang-Yeol Lee,Chan-Mo Yang*

Main category: cs.CL

TL;DR: 该研究提出了一种语言无关的框架，利用大语言模型（LLMs）从语音转录文本中提取自杀风险特征，并通过跨语言分析提升评估的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有青少年自杀风险检测方法依赖语言特定模型，限制了其扩展性和泛化能力。本研究旨在克服语言限制，提升自杀风险评估的普适性。

Method: 使用ASR模型生成中文语音转录文本，通过提示查询的LLMs提取自杀风险特征，并保留中英文特征以进行跨语言分析，最后微调预训练语言模型。

Result: 实验结果表明，该方法性能与直接使用ASR结果微调或仅基于中文特征的模型相当，验证了其跨语言有效性。

Conclusion: 该语言无关框架能够突破语言限制，增强自杀风险评估的鲁棒性，为多语言场景下的应用提供了可行方案。

Abstract: Suicidal risk detection in adolescents is a critical challenge, yet existing
methods rely on language-specific models, limiting scalability and
generalization. This study introduces a novel language-agnostic framework for
suicidal risk assessment with large language models (LLMs). We generate Chinese
transcripts from speech using an ASR model and then employ LLMs with
prompt-based queries to extract suicidal risk-related features from these
transcripts. The extracted features are retained in both Chinese and English to
enable cross-linguistic analysis and then used to fine-tune corresponding
pretrained language models independently. Experimental results show that our
method achieves performance comparable to direct fine-tuning with ASR results
or to models trained solely on Chinese suicidal risk-related features,
demonstrating its potential to overcome language constraints and improve the
robustness of suicidal risk assessment.

</details>


### [222] [ResSVD: Residual Compensated SVD for Large Language Model Compression](https://arxiv.org/abs/2505.20112)
*Haolei Bai,Siyong Jian,Tuo Liang,Yu Yin,Huan Wang*

Main category: cs.CL

TL;DR: 提出ResSVD方法，通过利用截断残差矩阵和选择性压缩最后几层，显著提升大语言模型压缩性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于SVD的大语言模型压缩方法忽视截断残差矩阵，且全层压缩导致性能严重下降，需改进压缩策略。

Method: ResSVD利用截断残差矩阵减少损失，并在固定压缩率下选择性压缩最后几层以抑制误差传播。

Result: 在多类大语言模型和基准数据集上，ResSVD始终优于现有方法，验证了其实际有效性。

Conclusion: ResSVD通过残差利用和分层优化，为大语言模型高效压缩提供了实用解决方案。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in a
wide range of downstream natural language processing tasks. Nevertheless, their
considerable sizes and memory demands hinder practical deployment, underscoring
the importance of developing efficient compression strategies. Singular value
decomposition (SVD) decomposes a matrix into orthogonal components, enabling
efficient low-rank approximation. This is particularly suitable for LLM
compression, where weight matrices often exhibit significant redundancy.
However, current SVD-based methods neglect the residual matrix from truncation,
resulting in significant truncation loss. Additionally, compressing all layers
of the model results in severe performance degradation. To overcome these
limitations, we propose ResSVD, a new post-training SVD-based LLM compression
method. Specifically, we leverage the residual matrix generated during the
truncation process to reduce truncation loss. Moreover, under a fixed overall
compression ratio, we selectively compress the last few layers of the model,
which mitigates error propagation and significantly improves the performance of
compressed models.Comprehensive evaluations of ResSVD on diverse LLM families
and multiple benchmark datasets indicate that ResSVD consistently achieves
superior performance over existing counterpart methods, demonstrating its
practical effectiveness.

</details>


### [223] [Named Entity Recognition in Historical Italian: The Case of Giacomo Leopardi's Zibaldone](https://arxiv.org/abs/2505.20113)
*Cristian Santini,Laura Melosi,Emanuele Frontoni*

Main category: cs.CL

TL;DR: 论文探讨了历史文本数字化带来的挑战，提出了一种针对19世纪意大利文献的命名实体识别数据集，并比较了领域特定BERT模型与先进LLMs在历史人文文本上的表现。


<details>
  <summary>Details</summary>
Motivation: 随着世界文本遗产数字化的增加，计算机科学和文学研究面临重大挑战，尤其是在处理历史文本时，如拼写变体、结构残缺和数字化错误。大型语言模型（LLMs）的崛起为历史文献的命名实体识别（NER）提供了新的可能性，但意大利文本的全面评估尚属空白。

Method: 研究提出了一个新的挑战性数据集，基于19世纪学者Giacomo Leopardi的Zibaldone笔记，包含2,899条人物、地点和文学作品的引用。使用领域特定的BERT模型和先进的LLMs（如LLaMa3.1）进行了可重复的实验。

Result: 结果显示，指令调优的模型在处理历史人文文本时遇到多种困难，而经过微调的NER模型即使在处理具有挑战性的实体类型（如书目引用）时也表现出更稳健的性能。

Conclusion: 研究表明，针对特定领域微调的NER模型在处理历史文本时优于通用的大型语言模型，为历史文献的数字化处理提供了更有效的解决方案。

Abstract: The increased digitization of world's textual heritage poses significant
challenges for both computer science and literary studies. Overall, there is an
urgent need of computational techniques able to adapt to the challenges of
historical texts, such as orthographic and spelling variations, fragmentary
structure and digitization errors. The rise of large language models (LLMs) has
revolutionized natural language processing, suggesting promising applications
for Named Entity Recognition (NER) on historical documents. In spite of this,
no thorough evaluation has been proposed for Italian texts. This research tries
to fill the gap by proposing a new challenging dataset for entity extraction
based on a corpus of 19th century scholarly notes, i.e. Giacomo Leopardi's
Zibaldone (1898), containing 2,899 references to people, locations and literary
works. This dataset was used to carry out reproducible experiments with both
domain-specific BERT-based models and state-of-the-art LLMs such as LLaMa3.1.
Results show that instruction-tuned models encounter multiple difficulties
handling historical humanistic texts, while fine-tuned NER models offer more
robust performance even with challenging entity types such as bibliographic
references.

</details>


### [224] [TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent](https://arxiv.org/abs/2505.20118)
*Dominik Meier,Jan Philip Wahle,Paul Röttger,Terry Ruas,Bela Gipp*

Main category: cs.CL

TL;DR: 论文提出TrojanStego，一种通过微调LLM嵌入敏感信息的新型威胁模型，展示其高效性与隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）应用于敏感场景，其潜在信息泄露风险引发担忧。作者旨在探索一种无需控制输入即可隐蔽泄露数据的新型攻击方式。

Method: 提出基于词汇分区的编码方案，通过微调使LLM掌握将机密信息隐写于自然文本输出的能力，并建立风险因素分类法评估威胁。

Result: 实验显示被控模型能以87%准确率传输32位密钥（三代多数投票达97%），同时保持文本实用性、隐蔽性和连贯性。

Conclusion: TrojanStego揭示了LLM被动数据渗漏攻击的可行性，此类攻击具有隐蔽性、实用性和高风险性。

Abstract: As large language models (LLMs) become integrated into sensitive workflows,
concerns grow over their potential to leak confidential information. We propose
TrojanStego, a novel threat model in which an adversary fine-tunes an LLM to
embed sensitive context information into natural-looking outputs via linguistic
steganography, without requiring explicit control over inference inputs. We
introduce a taxonomy outlining risk factors for compromised LLMs, and use it to
evaluate the risk profile of the threat. To implement TrojanStego, we propose a
practical encoding scheme based on vocabulary partitioning learnable by LLMs
via fine-tuning. Experimental results show that compromised models reliably
transmit 32-bit secrets with 87% accuracy on held-out prompts, reaching over
97% accuracy using majority voting across three generations. Further, they
maintain high utility, can evade human detection, and preserve coherence. These
results highlight a new class of LLM data exfiltration attacks that are
passive, covert, practical, and dangerous.

</details>


### [225] [Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers](https://arxiv.org/abs/2505.20128)
*Zhengliang Shi,Lingyong Yan,Dawei Yin,Suzan Verberne,Maarten de Rijke,Zhaochun Ren*

Main category: cs.CL

TL;DR: 本文提出EXSEARCH框架，通过自激励过程让大语言模型在复杂任务中逐步检索有用信息，显著提升知识密集型任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在多跳查询和无关检索内容处理上存在挑战，需要一种能动态检索精准知识的方法。

Method: 采用基于广义期望最大化算法的代理搜索框架，分步骤进行思考-搜索-记录，并通过自生成数据迭代优化模型。

Result: 在四个知识密集型基准测试中，EXSEARCH的精确匹配分数比基线方法提高7.8%。

Conclusion: EXSEARCH框架有效提升复杂任务中的知识检索能力，其扩展版本EXSEARCH-Zoo为未来研究提供通用平台。

Abstract: Large language models (LLMs) have been widely integrated into information
retrieval to advance traditional techniques. However, effectively enabling LLMs
to seek accurate knowledge in complex tasks remains a challenge due to the
complexity of multi-hop queries as well as the irrelevant retrieved content. To
address these limitations, we propose EXSEARCH, an agentic search framework,
where the LLM learns to retrieve useful information as the reasoning unfolds
through a self-incentivized process. At each step, the LLM decides what to
retrieve (thinking), triggers an external retriever (search), and extracts
fine-grained evidence (recording) to support next-step reasoning. To enable LLM
with this capability, EXSEARCH adopts a Generalized Expectation-Maximization
algorithm. In the E-step, the LLM generates multiple search trajectories and
assigns an importance weight to each; the M-step trains the LLM on them with a
re-weighted loss function. This creates a self-incentivized loop, where the LLM
iteratively learns from its own generated data, progressively improving itself
for search. We further theoretically analyze this training process,
establishing convergence guarantees. Extensive experiments on four
knowledge-intensive benchmarks show that EXSEARCH substantially outperforms
baselines, e.g., +7.8% improvement on exact match score. Motivated by these
promising results, we introduce EXSEARCH-Zoo, an extension that extends our
method to broader scenarios, to facilitate future work.

</details>


### [226] [AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings](https://arxiv.org/abs/2505.20133)
*Konstantin Dobler,Desmond Elliott,Gerard de Melo*

Main category: cs.CL

TL;DR: 提出AweDist方法，通过蒸馏原始分词表示快速学习新词的高质量嵌入，优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型依赖预训练时的静态词表，对词表中覆盖不足的领域性能下降且计算成本增加。现有嵌入初始化方法需额外训练或预训练模块，成本高。

Method: 提出AweDist，通过蒸馏原始分词表示来快速学习新词的输入嵌入。

Result: 实验表明，AweDist在多种开源模型上表现优于强基线。

Conclusion: AweDist能高效学习新词嵌入，提升模型在未充分覆盖领域的性能。

Abstract: Current language models rely on static vocabularies determined at pretraining
time, which can lead to decreased performance and increased computational cost
for domains underrepresented in the original vocabulary. New tokens can be
added to solve this problem, when coupled with a good initialization for their
new embeddings. However, existing embedding initialization methods either
require expensive further training or pretraining of additional modules. In
this paper, we propose AweDist and show that by distilling representations
obtained using the original tokenization, we can quickly learn high-quality
input embeddings for new tokens. Experimental results with a wide range of
open-weight models show that AweDist is able to outperform even strong
baselines.

</details>


### [227] [SeMe: Training-Free Language Model Merging via Semantic Alignment](https://arxiv.org/abs/2505.20144)
*Jian Gu,Aldeida Aleti,Chunyang Chen,Hongyu Zhang*

Main category: cs.CL

TL;DR: 提出SeMe方法，一种无需数据和训练的语言模型融合技术，通过语义对齐实现细粒度层间合并，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型融合方法依赖数据或无法保留内部知识，缺乏鲁棒性和可扩展性，需一种更高效的融合方案。

Method: SeMe利用潜在语义对齐进行细粒度层间合并，无需外部数据或训练，保持模型行为并稳定内部知识。

Result: 实验表明SeMe在多种架构和任务中性能与效率均优于现有方法，且不依赖外部数据。

Conclusion: SeMe为知识感知的模型融合设立了新范式，揭示了语言模型的语义结构，推动了可扩展和可解释的模型组合发展。

Abstract: Despite the remarkable capabilities of Language Models (LMs) across diverse
tasks, no single model consistently outperforms others, necessitating efficient
methods to combine their strengths without expensive retraining. Existing model
merging techniques, such as parameter averaging and task-guided fusion, often
rely on data-dependent computations or fail to preserve internal knowledge,
limiting their robustness and scalability. We introduce SeMe (Semantic-based
Merging), a novel, data-free, and training-free approach that leverages latent
semantic alignment to merge LMs at a fine-grained, layer-wise level. Unlike
prior work, SeMe not only preserves model behaviors but also explicitly
stabilizes internal knowledge, addressing a critical gap in LM fusion. Through
extensive experiments across diverse architectures and tasks, we demonstrate
that SeMe outperforms existing methods in both performance and efficiency while
eliminating reliance on external data. Our work establishes a new paradigm for
knowledge-aware model merging and provides insights into the semantic structure
of LMs, paving the way for more scalable and interpretable model composition.

</details>


### [228] [UORA: Uniform Orthogonal Reinitialization Adaptation in Parameter-Efficient Fine-Tuning of Large Models](https://arxiv.org/abs/2505.20154)
*Xueyan Zhang,Jinman Zhao,Zhifei Yang,Yibo Zhong,Shuhao Guan,Linbo Cao,Yining Wang*

Main category: cs.CL

TL;DR: UORA是一种新型的参数高效微调方法，通过低秩近似和选择性重初始化技术，在减少可训练参数的同时保持优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法如LoRA和VeRA在参数效率和计算存储效率上存在不足，UORA旨在提供更高效的微调方案。

Method: UORA采用基于插值的重参数化机制，通过向量大小启发式选择性地重初始化冻结投影矩阵的行和列。

Result: UORA在多个基准测试中表现优异，参数效率显著优于LoRA，计算和存储效率超过VeRA。

Conclusion: UORA为大规模语言模型的高效微调提供了新的范例，具有可扩展性和资源高效性。

Abstract: This paper introduces Uniform Orthogonal Reinitialization Adaptation (UORA),
a novel parameter-efficient fine-tuning (PEFT) approach for Large Language
Models (LLMs). UORA achieves state-of-the-art performance and parameter
efficiency by leveraging a low-rank approximation method to reduce the number
of trainable parameters. Unlike existing methods such as LoRA and VeRA, UORA
employs an interpolation-based reparametrization mechanism that selectively
reinitializes rows and columns in frozen projection matrices, guided by the
vector magnitude heuristic. This results in substantially fewer trainable
parameters compared to LoRA and outperforms VeRA in computation and storage
efficiency. Comprehensive experiments across various benchmarks demonstrate
UORA's superiority in achieving competitive fine-tuning performance with
negligible computational overhead. We demonstrate its performance on GLUE and
E2E benchmarks and its effectiveness in instruction-tuning large language
models and image classification models. Our contributions establish a new
paradigm for scalable and resource-efficient fine-tuning of LLMs.

</details>


### [229] [Pangu Light: Weight Re-Initialization for Pruning and Accelerating LLMs](https://arxiv.org/abs/2505.20155)
*Hanting Chen,Jiarui Qin,Jialong Guo,Tao Yuan,Yichun Yin,Huiling Zhen,Yasheng Wang,Jinpeng Li,Xiaojun Meng,Meng Zhang,Rongju Ruan,Zheyuan Bai,Yehui Tang,Can Chen,Xinghao Chen,Fisher Yu,Ruiming Tang,Yunhe Wang*

Main category: cs.CL

TL;DR: Pangu Light提出了一种结合结构化剪枝和权重重新初始化技术的大语言模型加速框架，显著提升了剪枝后模型的性能与效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）虽然性能卓越，但其庞大的规模和推理成本对实际部署提出了巨大挑战。现有剪枝方法在同时减少宽度和深度时，常导致性能显著下降。

Method: Pangu Light通过结构化剪枝和创新的权重重新初始化技术（如CLAP和SLNP）优化模型，同时针对Ascend NPU进行了专门优化。

Result: Pangu Light-32B在Ascend NPU上实现了81.6的平均分和2585 tokens/s的吞吐量，优于Qwen3-32B的80.9分和2225 tokens/s。

Conclusion: Pangu Light框架在保持高精度的同时显著提升了模型效率，为LLMs的实际部署提供了可行的解决方案。

Abstract: Large Language Models (LLMs) deliver state-of-the-art capabilities across
numerous tasks, but their immense size and inference costs pose significant
computational challenges for practical deployment. While structured pruning
offers a promising avenue for model compression, existing methods often
struggle with the detrimental effects of aggressive, simultaneous width and
depth reductions, leading to substantial performance degradation. This paper
argues that a critical, often overlooked, aspect in making such aggressive
joint pruning viable is the strategic re-initialization and adjustment of
remaining weights to improve the model post-pruning training accuracies. We
introduce Pangu Light, a framework for LLM acceleration centered around
structured pruning coupled with novel weight re-initialization techniques
designed to address this ``missing piece''. Our framework systematically
targets multiple axes, including model width, depth, attention heads, and
RMSNorm, with its effectiveness rooted in novel re-initialization methods like
Cross-Layer Attention Pruning (CLAP) and Stabilized LayerNorm Pruning (SLNP)
that mitigate performance drops by providing the network a better training
starting point. Further enhancing efficiency, Pangu Light incorporates
specialized optimizations such as absorbing Post-RMSNorm computations and
tailors its strategies to Ascend NPU characteristics. The Pangu Light models
consistently exhibit a superior accuracy-efficiency trade-off, outperforming
prominent baseline pruning methods like Nemotron and established LLMs like
Qwen3 series. For instance, on Ascend NPUs, Pangu Light-32B's 81.6 average
score and 2585 tokens/s throughput exceed Qwen3-32B's 80.9 average score and
2225 tokens/s.

</details>


### [230] [Exploring Generative Error Correction for Dysarthric Speech Recognition](https://arxiv.org/abs/2505.20163)
*Moreno La Quatra,Alkis Koudounas,Valerio Mario Salerno,Sabato Marco Siniscalchi*

Main category: cs.CL

TL;DR: 该论文提出了一种结合先进语音识别模型与基于LLM的生成式错误校正的两阶段框架，用于提高构音障碍语音的转录准确率。


<details>
  <summary>Details</summary>
Motivation: 尽管端到端自动语音识别（ASR）技术取得了显著进展，但准确转录构音障碍语音仍是一个主要挑战。

Method: 采用两阶段框架，结合前沿语音识别模型和基于LLM的生成式错误校正（GER），并通过模型规模和训练策略的不同配置进行评估。

Result: 在Speech Accessibility Project数据集上的实验表明，该方法在结构化和自发语音上表现优异，但在单词识别方面仍存在挑战。

Conclusion: 通过综合分析，论文揭示了声学和语言模型在构音障碍语音识别中的互补作用。

Abstract: Despite the remarkable progress in end-to-end Automatic Speech Recognition
(ASR) engines, accurately transcribing dysarthric speech remains a major
challenge. In this work, we proposed a two-stage framework for the Speech
Accessibility Project Challenge at INTERSPEECH 2025, which combines
cutting-edge speech recognition models with LLM-based generative error
correction (GER). We assess different configurations of model scales and
training strategies, incorporating specific hypothesis selection to improve
transcription accuracy. Experiments on the Speech Accessibility Project dataset
demonstrate the strength of our approach on structured and spontaneous speech,
while highlighting challenges in single-word recognition. Through comprehensive
analysis, we provide insights into the complementary roles of acoustic and
linguistic modeling in dysarthric speech recognition

</details>


### [231] [Visual Abstract Thinking Empowers Multimodal Reasoning](https://arxiv.org/abs/2505.20164)
*Dairu Liu,Ziyue Wang,Minyuan Ruan,Fuwen Luo,Chi Chen,Peng Li,Yang Liu*

Main category: cs.CL

TL;DR: 该论文提出了一种名为视觉抽象思维（VAT）的新范式，通过简化视觉信息来提升多模态大语言模型（MLLMs）的视觉推理能力，实验显示其性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 图像通常比文本包含更丰富的细节，但也可能包含冗余信息，影响多模态推理性能。受人类抽象思维的启发，研究旨在通过简化视觉信息来优化模型的推理过程。

Method: 引入视觉抽象思维（VAT）范式，用视觉抽象代替冗长的中间步骤或外部知识，使模型更专注于关键视觉元素。

Result: 实验结果表明，VAT在不同模型上均表现优异，平均性能提升17%，且在概念、结构和关系推理任务中均有显著提升。

Conclusion: VAT通过抽象思维有效提升了视觉推理能力，并可与链式思维（CoT）结合，为多模态推理任务提供了新的研究方向。

Abstract: Images usually convey richer detail than text, but often include redundant
information which potentially downgrades multimodal reasoning performance. When
faced with lengthy or complex messages, humans tend to employ abstract thinking
to convert them into simple and concise abstracts. Inspired by this cognitive
strategy, we introduce Visual Abstract Thinking (VAT), a novel thinking
paradigm that prompts Multimodal Large Language Models (MLLMs) with visual
abstract instead of explicit verbal thoughts or elaborate guidance, permitting
a more concentrated visual reasoning mechanism. Explicit thinking, such as
Chain-of-thought (CoT) or tool-augmented approaches, increases the complexity
of reasoning process via inserting verbose intermediate steps, external
knowledge or visual information. In contrast, VAT reduces redundant visual
information and encourages models to focus their reasoning on more essential
visual elements. Experimental results show that VAT consistently empowers
different models, and achieves an average gain of 17% over GPT-4o baseline by
employing diverse types of visual abstracts, demonstrating that VAT can enhance
visual reasoning abilities for MLLMs regarding conceptual, structural and
relational reasoning tasks. VAT is also compatible with CoT in
knowledge-intensive multimodal reasoning tasks. These findings highlight the
effectiveness of visual reasoning via abstract thinking and encourage further
exploration of more diverse reasoning paradigms from the perspective of human
cognition.

</details>


### [232] ["KAN you hear me?" Exploring Kolmogorov-Arnold Networks for Spoken Language Understanding](https://arxiv.org/abs/2505.20176)
*Alkis Koudounas,Moreno La Quatra,Eliana Pastor,Sabato Marco Siniscalchi,Elena Baralis*

Main category: cs.CL

TL;DR: 该研究首次将Kolmogorov-Arnold网络（KANs）应用于口语理解任务，通过多种配置实验表明KAN层可有效替代线性层，性能相当或更优。


<details>
  <summary>Details</summary>
Motivation: KANs作为新兴神经网络架构，在语音处理领域的应用尚未充分探索，特别是在口语理解（SLU）任务中的潜力有待验证。

Method: 在2D-CNN模型上测试五种KAN层配置，最佳方案（线性层间插入KAN层）进一步应用于Transformer模型，并在五个SLU数据集上评估。

Result: 实验表明KAN层在多数情况下可替代线性层，性能达到或超过基线；同时揭示了KAN与线性层对原始波形输入区域的不同关注模式。

Conclusion: KANs在SLU任务中展现出替代传统线性层的潜力，为模型架构设计提供了新思路。

Abstract: Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising
alternative to traditional neural architectures, yet their application to
speech processing remains under explored. This work presents the first
investigation of KANs for Spoken Language Understanding (SLU) tasks. We
experiment with 2D-CNN models on two datasets, integrating KAN layers in five
different configurations within the dense block. The best-performing setup,
which places a KAN layer between two linear layers, is directly applied to
transformer-based models and evaluated on five SLU datasets with increasing
complexity. Our results show that KAN layers can effectively replace the linear
layers, achieving comparable or superior performance in most cases. Finally, we
provide insights into how KAN and linear layers on top of transformers
differently attend to input regions of the raw waveforms.

</details>


### [233] [THiNK: Can Large Language Models Think-aloud?](https://arxiv.org/abs/2505.20184)
*Yongan Yu,Mengqian Wu,Yiran Lin,Nikki G. Lobczowski*

Main category: cs.CL

TL;DR: 该论文提出了THiNK框架，基于布鲁姆分类法，通过多智能体反馈机制评估大语言模型的高阶思维能力，发现模型在低阶任务表现良好，但在高阶应用和抽象推理上存在不足。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型的高阶思维能力是一个基本挑战，尤其是在超越表面准确性的任务中。当前缺乏系统化的评估方法，特别是在应用、分析和创造等高阶认知技能方面。

Method: 提出THiNK框架，一个基于布鲁姆分类法的多智能体反馈驱动评估系统，通过问题生成、批评和修订的迭代过程，逐步引导模型进行反思和优化。

Result: 测试了七个先进的大语言模型，结果显示模型在低阶认知任务上表现可靠，但在实际情境中应用知识和抽象推理方面表现不佳。结构化反馈循环显著提升了推理性能，特别是在高阶思维任务上。

Conclusion: THiNK框架为评估和提升大语言模型的推理能力提供了可扩展的方法，基于学习科学的评估为未来研究提供了新方向。

Abstract: Assessing higher-order thinking skills in large language models (LLMs)
remains a fundamental challenge, especially in tasks that go beyond
surface-level accuracy. In this work, we propose THiNK (Testing Higher-order
Notion of Knowledge), a multi-agent, feedback-driven evaluation framework
grounded in Bloom's Taxonomy. THiNK frames reasoning assessment as an iterative
task of problem generation, critique, and revision, encouraging LLMs to
think-aloud through step-by-step reflection and refinement. This enables a
systematic evaluation of both lower-order (e.g., remember, understand) and
higher-order (e.g., evaluate, create) thinking skills. We apply THiNK to seven
state-of-the-art LLMs and perform a detailed cognitive analysis of their
outputs. Results reveal that while models reliably perform lower-order
categories well, they struggle with applying knowledge in realistic contexts
and exhibit limited abstraction. Structured feedback loops significantly
improve reasoning performance, particularly in higher-order thinking.
Qualitative evaluations further confirm that THiNK-guided outputs better align
with domain logic and problem structure. The code of our framework provides a
scalable methodology for probing and enhancing LLM reasoning, offering new
directions for evaluation grounded in learning science, which is available at
our GitHub repository.

</details>


### [234] [Monocle: Hybrid Local-Global In-Context Evaluation for Long-Text Generation with Uncertainty-Based Active Learning](https://arxiv.org/abs/2505.20195)
*Xiaorong Wang,Ting Yang,Zhu Zhang,Shuo Wang,Zihan Zhou,Liner Yang,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 论文提出了一种分而治之的方法，结合人类标注和主动学习，以提升长文本生成质量的评估效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的评估方法在处理长文本时性能下降，难以全面评估文本质量。

Method: 采用分治策略将评估任务分解为局部评分和全局评估，结合人类标注的上下文学习及基于不确定性的主动学习算法。

Result: 实验表明该框架优于多个基线方法，验证了其有效性。

Conclusion: 该方法通过模块化评估和人类反馈协同优化，显著提升了长文本质量评估的准确性和实用性。

Abstract: Assessing the quality of long-form, model-generated text is challenging, even
with advanced LLM-as-a-Judge methods, due to performance degradation as input
length increases. To address this issue, we propose a divide-and-conquer
approach, which breaks down the comprehensive evaluation task into a series of
localized scoring tasks, followed by a final global assessment. This strategy
allows for more granular and manageable evaluations, ensuring that each segment
of the text is assessed in isolation for both coherence and quality, while also
accounting for the overall structure and consistency of the entire piece.
Moreover, we introduce a hybrid in-context learning approach that leverages
human annotations to enhance the performance of both local and global
evaluations. By incorporating human-generated feedback directly into the
evaluation process, this method allows the model to better align with human
judgment. Finally, we develop an uncertainty-based active learning algorithm
that efficiently selects data samples for human annotation, thereby reducing
annotation costs in practical scenarios. Experimental results show that the
proposed evaluation framework outperforms several representative baselines,
highlighting the effectiveness of our approach.

</details>


### [235] [Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking](https://arxiv.org/abs/2505.20199)
*Pengxiang Li,Shilin Yan,Joey Tsai,Renrui Zhang,Ruichuan An,Ziyu Guo,Xiaowei Gao*

Main category: cs.CL

TL;DR: 论文提出自适应无分类器引导（A-CFG），通过动态调整无条件输入来提升生成模型的控制性，相比静态CFG在语言生成任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 标准无分类器引导（CFG）使用静态无条件输入，在迭代生成过程中可能因模型不确定性动态变化而导致效果不佳。

Method: A-CFG根据模型实时预测置信度，在每步迭代中识别低置信度token并临时重新掩码，构建动态局部无条件输入以聚焦模糊区域的校正。

Result: 实验表明A-CFG显著优于标准CFG，如在GPQA任务上提升3.9分。

Conclusion: 动态适应模型不确定性的引导机制能有效提升迭代生成效果。

Abstract: Classifier-Free Guidance (CFG) significantly enhances controllability in
generative models by interpolating conditional and unconditional predictions.
However, standard CFG often employs a static unconditional input, which can be
suboptimal for iterative generation processes where model uncertainty varies
dynamically. We introduce Adaptive Classifier-Free Guidance (A-CFG), a novel
method that tailors the unconditional input by leveraging the model's
instantaneous predictive confidence. At each step of an iterative (masked)
diffusion language model, A-CFG identifies tokens in the currently generated
sequence for which the model exhibits low confidence. These tokens are
temporarily re-masked to create a dynamic, localized unconditional input. This
focuses CFG's corrective influence precisely on areas of ambiguity, leading to
more effective guidance. We integrate A-CFG into a state-of-the-art masked
diffusion language model and demonstrate its efficacy. Experiments on diverse
language generation benchmarks show that A-CFG yields substantial improvements
over standard CFG, achieving, for instance, a 3.9 point gain on GPQA. Our work
highlights the benefit of dynamically adapting guidance mechanisms to model
uncertainty in iterative generation.

</details>


### [236] [Reasoning Is Not All You Need: Examining LLMs for Multi-Turn Mental Health Conversations](https://arxiv.org/abs/2505.20201)
*Mohit Chandra,Siddharth Sriraman,Harneet Singh Khanuja,Yiqiao Jin,Munmun De Choudhury*

Main category: cs.CL

TL;DR: 论文提出MedAgent框架，用于生成多轮心理健康对话数据集MHSD，并开发MultiSenseEval评估框架，发现前沿推理模型在患者中心沟通和高级诊断能力上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 由于心理健康服务资源有限、等待时间长，以及大语言模型（LLMs）能力提升，人们开始依赖LLMs满足心理健康需求。然而，现有评估框架多关注诊断准确性和胜率，忽视与患者目标、价值观和个性的匹配。

Method: 引入MedAgent框架，合成真实的多轮心理健康对话，创建包含2200多组对话的MHSD数据集，并提出基于人本标准的MultiSenseEval评估框架。

Result: 前沿推理模型在患者中心沟通上表现不佳（平均得分31%），且性能随对话轮次增加而下降，不同患者角色间表现存在差异。

Conclusion: 研究提供了合成数据生成框架、数据集及评估工具，为多轮心理健康对话中LLMs的评估奠定基础，揭示了当前模型的局限性。

Abstract: Limited access to mental healthcare, extended wait times, and increasing
capabilities of Large Language Models (LLMs) has led individuals to turn to
LLMs for fulfilling their mental health needs. However, examining the
multi-turn mental health conversation capabilities of LLMs remains
under-explored. Existing evaluation frameworks typically focus on diagnostic
accuracy and win-rates and often overlook alignment with patient-specific
goals, values, and personalities required for meaningful conversations. To
address this, we introduce MedAgent, a novel framework for synthetically
generating realistic, multi-turn mental health sensemaking conversations and
use it to create the Mental Health Sensemaking Dialogue (MHSD) dataset,
comprising over 2,200 patient-LLM conversations. Additionally, we present
MultiSenseEval, a holistic framework to evaluate the multi-turn conversation
abilities of LLMs in healthcare settings using human-centric criteria. Our
findings reveal that frontier reasoning models yield below-par performance for
patient-centric communication and struggle at advanced diagnostic capabilities
with average score of 31%. Additionally, we observed variation in model
performance based on patient's persona and performance drop with increasing
turns in the conversation. Our work provides a comprehensive synthetic data
generation framework, a dataset and evaluation framework for assessing LLMs in
multi-turn mental health conversations.

</details>


### [237] [How to Improve the Robustness of Closed-Source Models on NLI](https://arxiv.org/abs/2505.20209)
*Joe Stacey,Lisa Alazraki,Aran Ubhi,Beyza Ermis,Aaron Mueller,Marek Rei*

Main category: cs.CL

TL;DR: 该论文研究了如何通过数据为中心的方法提升闭源大型语言模型（LLMs）的鲁棒性，发现策略效果取决于OOD数据复杂度，并指出闭源自回归LLMs比编码器模型更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 闭源大型语言模型（LLMs）在多种自然语言任务中表现优异，但微调可能导致模型学习数据集特定的启发式方法，降低其在分布外（OOD）数据上的鲁棒性。现有方法要么效果不佳，要么不适用于闭源模型。

Method: 通过数据为中心的方法提升闭源LLMs的鲁棒性，无需访问模型内部或修改训练过程。针对不同复杂度的OOD数据，采用上采样更具挑战性的训练示例或用LLM生成的示例替换部分训练集。

Result: 对于高度复杂的OOD数据集，上采样更具挑战性的训练示例可将鲁棒性提升1.5%；对于复杂度较低的OOD数据集，用LLM生成的示例替换部分训练集可将鲁棒性提升3.7%。闭源自回归LLMs比编码器模型更鲁棒。

Conclusion: 闭源大型自回归LLMs比常用编码器模型更鲁棒，是未来更合适的基线选择。针对不同复杂度的OOD数据，采用不同的数据策略可有效提升模型鲁棒性。

Abstract: Closed-source Large Language Models (LLMs) have become increasingly popular,
with impressive performance across a wide range of natural language tasks.
These models can be fine-tuned to further improve performance, but this often
results in the models learning from dataset-specific heuristics that reduce
their robustness on out-of-distribution (OOD) data. Existing methods to improve
robustness either perform poorly, or are non-applicable to closed-source models
because they assume access to model internals, or the ability to change the
model's training procedure. In this work, we investigate strategies to improve
the robustness of closed-source LLMs through data-centric methods that do not
require access to model internals. We find that the optimal strategy depends on
the complexity of the OOD data. For highly complex OOD datasets, upsampling
more challenging training examples can improve robustness by up to 1.5%. For
less complex OOD datasets, replacing a portion of the training set with
LLM-generated examples can improve robustness by 3.7%. More broadly, we find
that large-scale closed-source autoregressive LLMs are substantially more
robust than commonly used encoder models, and are a more appropriate choice of
baseline going forward.

</details>


### [238] [Dependency Parsing is More Parameter-Efficient with Normalization](https://arxiv.org/abs/2505.20215)
*Paolo Gajo,Domenic Rosati,Hassan Sajjad,Alberto Barrón-Cedeño*

Main category: cs.CL

TL;DR: 该论文提出通过归一化双仿射评分来提高依存句法分析的效率，减少模型参数并提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前的双仿射评分机制在计算注意力分数时未进行归一化，导致模型参数过多且效率低下。论文旨在通过理论分析和实验验证归一化的必要性。

Method: 使用多层堆叠的BiLSTM模型，在六个数据集上对比归一化和未归一化双仿射评分的性能差异。

Result: 归一化后的方法在部分数据集上达到了最优性能，同时减少了训练样本和参数数量。

Conclusion: 归一化双仿射评分能显著提升依存句法分析的效率和性能，减少模型复杂度。

Abstract: Dependency parsing is the task of inferring natural language structure, often
approached by modeling word interactions via attention through biaffine
scoring. This mechanism works like self-attention in Transformers, where scores
are calculated for every pair of words in a sentence. However, unlike
Transformer attention, biaffine scoring does not use normalization prior to
taking the softmax of the scores. In this paper, we provide theoretical
evidence and empirical results revealing that a lack of normalization
necessarily results in overparameterized parser models, where the extra
parameters compensate for the sharp softmax outputs produced by high variance
inputs to the biaffine scoring function. We argue that biaffine scoring can be
made substantially more efficient by performing score normalization. We conduct
experiments on six datasets for semantic and syntactic dependency parsing using
a one-hop parser. We train N-layer stacked BiLSTMs and evaluate the parser's
performance with and without normalizing biaffine scores. Normalizing allows us
to beat the state of the art on two datasets, with fewer samples and trainable
parameters. Code: https://anonymous.4open.science/r/EfficientSDP-70C1

</details>


### [239] [FLAME-MoE: A Transparent End-to-End Research Platform for Mixture-of-Experts Language Models](https://arxiv.org/abs/2505.20225)
*Hao Kang,Zichun Yu,Chenyan Xiong*

Main category: cs.CL

TL;DR: FLAME-MoE是一个完全开源的MoE研究平台，提供从38M到1.7B参数的模型，旨在促进MoE架构的透明研究。


<details>
  <summary>Details</summary>
Motivation: 当前学术研究者缺乏一个完全开放、端到端的MoE平台来研究模型扩展、路由和专家行为，因此开发了FLAME-MoE。

Method: 发布了FLAME-MoE，包含七个解码器模型，采用64专家和top-8门控架构，所有训练数据和脚本公开。

Result: 在六个评估任务中，FLAME-MoE比相同FLOPs的密集模型平均准确率提高3.4分，专家行为分析显示早期路由稳定。

Conclusion: FLAME-MoE为MoE研究提供了透明、可复现的平台，展示了专家专业化和路由行为的早期稳定性。

Abstract: Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4
increasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong
efficiency-performance trade-offs by activating only a fraction of the model
per token. Yet academic researchers still lack a fully open, end-to-end MoE
platform for investigating scaling, routing, and expert behavior. We release
FLAME-MoE, a completely open-source research suite composed of seven
decoder-only models, ranging from 38M to 1.7B active parameters, whose
architecture--64 experts with top-8 gating and 2 shared experts--closely
reflects modern production LLMs. All training data pipelines, scripts, logs,
and checkpoints are publicly available to enable reproducible experimentation.
Across six evaluation tasks, FLAME-MoE improves average accuracy by up to 3.4
points over dense baselines trained with identical FLOPs. Leveraging full
training trace transparency, we present initial analyses showing that (i)
experts increasingly specialize on distinct token subsets, (ii) co-activation
matrices remain sparse, reflecting diverse expert usage, and (iii) routing
behavior stabilizes early in training. All code, training logs, and model
checkpoints are available at https://github.com/cmu-flame/FLAME-MoE.

</details>


### [240] [Bridging the Long-Term Gap: A Memory-Active Policy for Multi-Session Task-Oriented Dialogue](https://arxiv.org/abs/2505.20231)
*Yiming Du,Bingbing Wang,Yang He,Bin Liang,Baojun Wang,Zhongyang Li,Lin Gui,Jeff Z. Pan,Ruifeng Xu,Kam-Fai Wong*

Main category: cs.CL

TL;DR: 论文提出了首个多会话任务导向对话数据集MS-TOD及记忆主动策略MAP，显著提升了多会话场景下的任务完成效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有任务导向对话系统主要针对单次会话，缺乏长期记忆能力，限制了其在多会话场景中的效率。

Method: 提出两阶段方法MAP：1) 记忆引导对话规划检索历史意图并优化QA单元；2) 主动响应策略检测纠错确保任务准确性。

Result: 在MS-TOD数据集上，MAP显著提高了多会话任务的成功率和轮次效率，同时保持单会话任务的竞争力。

Conclusion: MS-TOD数据集和MAP策略为多会话任务导向对话系统提供了有效的长期记忆增强解决方案。

Abstract: Existing Task-Oriented Dialogue (TOD) systems primarily focus on
single-session dialogues, limiting their effectiveness in long-term memory
augmentation. To address this challenge, we introduce a MS-TOD dataset, the
first multi-session TOD dataset designed to retain long-term memory across
sessions, enabling fewer turns and more efficient task completion. This defines
a new benchmark task for evaluating long-term memory in multi-session TOD.
Based on this new dataset, we propose a Memory-Active Policy (MAP) that
improves multi-session dialogue efficiency through a two-stage approach. 1)
Memory-Guided Dialogue Planning retrieves intent-aligned history, identifies
key QA units via a memory judger, refines them by removing redundant questions,
and generates responses based on the reconstructed memory. 2) Proactive
Response Strategy detects and correct errors or omissions, ensuring efficient
and accurate task completion. We evaluate MAP on MS-TOD dataset, focusing on
response quality and effectiveness of the proactive strategy. Experiments on
MS-TOD demonstrate that MAP significantly improves task success and turn
efficiency in multi-session scenarios, while maintaining competitive
performance on conventional single-session tasks.

</details>


### [241] [Efficient Speech Translation through Model Compression and Knowledge Distillation](https://arxiv.org/abs/2505.20237)
*Yasmin Moslem*

Main category: cs.CL

TL;DR: 论文通过层剪枝、4位量化低秩适配和知识蒸馏等方法，将Qwen2-Audio-7B-Instruct模型压缩50%，同时保持97-100%的翻译质量。


<details>
  <summary>Details</summary>
Motivation: 大型音频-语言模型在语音翻译中计算需求高，部署困难，需探索高效压缩方法。

Method: 结合迭代层剪枝、4位量化低秩适配（QLoRA）和知识蒸馏进行模型压缩。

Result: 压缩后的学生模型参数量和存储占用减少50%，翻译质量保留原模型的97-100%。

Conclusion: 该方法有效平衡了模型压缩与性能，为语音翻译模型的轻量化部署提供了可行方案。

Abstract: Efficient deployment of large audio-language models for speech translation
remains challenging due to their significant computational requirements. In
this paper, we address this challenge through our system submissions to the
"Model Compression" track at the International Conference on Spoken Language
Translation (IWSLT 2025). We experiment with a combination of approaches
including iterative layer pruning based on layer importance evaluation,
low-rank adaptation with 4-bit quantization (QLoRA), and knowledge
distillation. In our experiments, we use Qwen2-Audio-7B-Instruct for speech
translation into German and Chinese. Our pruned (student) models achieve up to
a 50% reduction in both model parameters and storage footprint, while retaining
97-100% of the translation quality of the in-domain (teacher) models.

</details>


### [242] [It's High Time: A Survey of Temporal Information Retrieval and Question Answering](https://arxiv.org/abs/2505.20243)
*Bhawna Piryani,Abdelrahman Abdullah,Jamshid Mozafari,Avishek Anand,Adam Jatowt*

Main category: cs.CL

TL;DR: 该论文综述了时间信息检索与时间问答领域，探讨了处理时间敏感信息的挑战与方法，包括传统和现代神经方法，如Transformer和大语言模型的应用。


<details>
  <summary>Details</summary>
Motivation: 随着时间戳内容（如新闻、网络存档和知识库）的增加，系统需要解决时间意图检测、时间表达式规范化、事件排序及模糊事实推理等挑战，以应对动态和时间敏感领域的需求。

Method: 论文回顾了传统方法和现代神经方法，包括Transformer模型和大语言模型（LLMs），以及时间语言建模、多跳推理和检索增强生成（RAG）的最新进展。

Result: 论文总结了时间鲁棒性、时效性意识和泛化能力的基准数据集和评估策略，展示了当前技术的进展与局限。

Conclusion: 时间信息处理在多个领域至关重要，现代方法虽取得进展，但仍需进一步研究以应对复杂的时间敏感任务。

Abstract: Time plays a critical role in how information is generated, retrieved, and
interpreted. In this survey, we provide a comprehensive overview of Temporal
Information Retrieval and Temporal Question Answering, two research areas aimed
at handling and understanding time-sensitive information. As the amount of
time-stamped content from sources like news articles, web archives, and
knowledge bases increases, systems must address challenges such as detecting
temporal intent, normalizing time expressions, ordering events, and reasoning
over evolving or ambiguous facts. These challenges are critical across many
dynamic and time-sensitive domains, from news and encyclopedias to science,
history, and social media. We review both traditional approaches and modern
neural methods, including those that use transformer models and Large Language
Models (LLMs). We also review recent advances in temporal language modeling,
multi-hop reasoning, and retrieval-augmented generation (RAG), alongside
benchmark datasets and evaluation strategies that test temporal robustness,
recency awareness, and generalization.

</details>


### [243] [KnowTrace: Bootstrapping Iterative Retrieval-Augmented Generation with Structured Knowledge Tracing](https://arxiv.org/abs/2505.20245)
*Rui Li,Quanyu Dai,Zeyu Zhang,Xu Chen,Zhenhua Dong,Ji-Rong Wen*

Main category: cs.CL

TL;DR: KnowTrace提出了一种新颖的RAG框架，通过结构化知识图谱减轻上下文过载问题，并利用自举机制提升多步推理质量，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在迭代检索信息时会导致LLM上下文过载，且无效推理步骤加剧了这一问题，亟需一种能结构化组织知识并优化推理效率的解决方案。

Method: KnowTrace通过动态构建问题相关的知识图谱（三元组形式）替代原始检索内容堆叠，并引入知识回溯机制生成自监督数据以优化推理过程。

Result: 实验表明KnowTrace在三个多跳问答基准上均超越现有方法，自举版本进一步提升了性能增益。

Conclusion: 结构化知识表示与自举机制的结合有效解决了RAG中的上下文过载问题，同时提升了复杂推理能力。

Abstract: Recent advances in retrieval-augmented generation (RAG) furnish large
language models (LLMs) with iterative retrievals of relevant information to
handle complex multi-hop questions. These methods typically alternate between
LLM reasoning and retrieval to accumulate external information into the LLM's
context. However, the ever-growing context inherently imposes an increasing
burden on the LLM to perceive connections among critical information pieces,
with futile reasoning steps further exacerbating this overload issue. In this
paper, we present KnowTrace, an elegant RAG framework to (1) mitigate the
context overload and (2) bootstrap higher-quality multi-step reasoning. Instead
of simply piling the retrieved contents, KnowTrace autonomously traces out
desired knowledge triplets to organize a specific knowledge graph relevant to
the input question. Such a structured workflow not only empowers the LLM with
an intelligible context for inference, but also naturally inspires a reflective
mechanism of knowledge backtracing to identify contributive LLM generations as
process supervision data for self-bootstrapping. Extensive experiments show
that KnowTrace consistently surpasses existing methods across three multi-hop
question answering benchmarks, and the bootstrapped version further amplifies
the gains.

</details>


### [244] [WXImpactBench: A Disruptive Weather Impact Understanding Benchmark for Evaluating Large Language Models](https://arxiv.org/abs/2505.20249)
*Yongan Yu,Qingchen Hu,Xianda Du,Jiayin Wang,Fengran Mo,Renee Sieber*

Main category: cs.CL

TL;DR: 该研究构建了一个破坏性天气影响数据集和首个评估大语言模型（LLMs）在此类任务表现的基准WXImpactBench，包含多标签分类和排序问答任务，为气候变化适应系统开发提供挑战分析。


<details>
  <summary>Details</summary>
Motivation: 气候变化适应需要理解破坏性天气对社会的影响，但大语言模型在此领域的应用效果尚未充分探索，主要因高质量语料收集困难和缺乏基准数据集。

Method: 首先开发了一个四阶段构建流程的破坏性天气影响数据集，随后提出包含多标签分类和排序问答任务的WXImpactBench基准，用于评估LLMs能力。

Result: 通过大量实验评估多组LLMs，首次分析了开发破坏性天气影响理解和气候变化适应系统的挑战，并公开数据集和评估框架代码。

Conclusion: 该研究为社区应对灾害脆弱性提供了实用工具，数据集和基准将助力气候变化适应系统的开发。

Abstract: Climate change adaptation requires the understanding of disruptive weather
impacts on society, where large language models (LLMs) might be applicable.
However, their effectiveness is under-explored due to the difficulty of
high-quality corpus collection and the lack of available benchmarks. The
climate-related events stored in regional newspapers record how communities
adapted and recovered from disasters. However, the processing of the original
corpus is non-trivial. In this study, we first develop a disruptive weather
impact dataset with a four-stage well-crafted construction pipeline. Then, we
propose WXImpactBench, the first benchmark for evaluating the capacity of LLMs
on disruptive weather impacts. The benchmark involves two evaluation tasks,
multi-label classification and ranking-based question answering. Extensive
experiments on evaluating a set of LLMs provide first-hand analysis of the
challenges in developing disruptive weather impact understanding and climate
change adaptation systems. The constructed dataset and the code for the
evaluation framework are available to help society protect against
vulnerabilities from disasters.

</details>


### [245] [ARM: Adaptive Reasoning Model](https://arxiv.org/abs/2505.20258)
*Siye Wu,Jian Xie,Yikai Zhang,Aili Chen,Kai Zhang,Yu Su,Yanghua Xiao*

Main category: cs.CL

TL;DR: 本文提出自适应推理模型（ARM），通过动态选择推理格式解决大模型'过度思考'问题，显著提升推理效率和训练速度。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型虽能处理复杂任务，但无法根据任务难度调整推理过程，导致不必要的计算资源浪费（'过度思考'问题），违背了实现完全自主AI的目标。

Method: 提出ARM模型，结合四种推理格式（直接回答/短链推理/代码/长链推理），并设计Ada-GRPO训练算法解决传统GRPO的格式崩溃问题。支持自适应/指令引导/共识引导三种推理模式。

Result: ARM平均减少30%（最高70%）的token使用，保持与长链推理相当的性能，训练速度提升2倍。

Conclusion: ARM在保持性能的同时显著提升效率，为自主AI推理提供了可扩展的解决方案。

Abstract: While large reasoning models demonstrate strong performance on complex tasks,
they lack the ability to adjust reasoning token usage based on task difficulty.
This often leads to the "overthinking" problem -- excessive and unnecessary
reasoning -- which, although potentially mitigated by human intervention to
control the token budget, still fundamentally contradicts the goal of achieving
fully autonomous AI. In this work, we propose Adaptive Reasoning Model (ARM), a
reasoning model capable of adaptively selecting appropriate reasoning formats
based on the task at hand. These formats include three efficient ones -- Direct
Answer, Short CoT, and Code -- as well as a more elaborate format, Long CoT. To
train ARM, we introduce Ada-GRPO, an adaptation of Group Relative Policy
Optimization (GRPO), which addresses the format collapse issue in traditional
GRPO. Ada-GRPO enables ARM to achieve high token efficiency, reducing tokens by
an average of 30%, and up to 70%, while maintaining performance comparable to
the model that relies solely on Long CoT. Furthermore, not only does it improve
inference efficiency through reduced token generation, but it also brings a 2x
speedup in training. In addition to the default Adaptive Mode, ARM supports two
additional reasoning modes: 1) Instruction-Guided Mode, which allows users to
explicitly specify the reasoning format via special tokens -- ideal when the
appropriate format is known for a batch of tasks. 2) Consensus-Guided Mode,
which aggregates the outputs of the three efficient formats and resorts to Long
CoT in case of disagreement, prioritizing performance with higher token usage.

</details>


### [246] [We Need to Measure Data Diversity in NLP -- Better and Broader](https://arxiv.org/abs/2505.20264)
*Dong Nguyen,Esther Ploeger*

Main category: cs.CL

TL;DR: 本文探讨了衡量NLP数据集多样性的概念与方法挑战，强调跨学科视角对开发更精细有效度量标准的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管NLP数据集的多样性日益受到关注，但如何准确衡量多样性仍缺乏深入探索。

Method: 通过观点性分析，审视数据多样性测量的概念与方法论难题。

Result: 指出当前度量方法的局限性，并提出跨学科合作是改进的关键。

Conclusion: 开发更精细、有效的多样性测量标准需融合多学科视角。

Abstract: Although diversity in NLP datasets has received growing attention, the
question of how to measure it remains largely underexplored. This opinion paper
examines the conceptual and methodological challenges of measuring data
diversity and argues that interdisciplinary perspectives are essential for
developing more fine-grained and valid measures.

</details>


### [247] [Does quantization affect models' performance on long-context tasks?](https://arxiv.org/abs/2505.20276)
*Anmol Mekala,Anirudh Atmakuru,Yixiao Song,Marzena Karpinska,Mohit Iyyer*

Main category: cs.CL

TL;DR: 论文系统评估了长输入（>64K tokens）和长输出任务中量化LLM的性能，发现8位量化平均准确率下降0.8%，而4位量化在长上下文和非英语输入中性能下降显著。


<details>
  <summary>Details</summary>
Motivation: 随着LLM支持超过128K tokens的上下文窗口，内存需求和推理延迟显著增加。量化可以降低成本，但可能影响性能，特别是在长输入和长输出任务中。

Method: 研究评估了9.7K测试样本，五种量化方法（FP8、GPTQ-int8、AWQ-int4、GPTQ-int4、BNB-nf4）和五种模型（Llama-3.1 8B和70B；Qwen-2.5 7B、32B和72B）。

Result: 8位量化平均准确率下降0.8%，4位量化在长上下文任务中性能下降高达59%，非英语输入时性能下降更明显。不同量化方法、模型和任务对性能影响显著。

Conclusion: 在部署量化LLM时，特别是在长上下文场景和非英语语言中，需要进行仔细的任务特定评估。

Abstract: Large language models (LLMs) now support context windows exceeding 128K
tokens, but this comes with significant memory requirements and high inference
latency. Quantization can mitigate these costs, but may degrade performance. In
this work, we present the first systematic evaluation of quantized LLMs on
tasks with long-inputs (>64K tokens) and long-form outputs. Our evaluation
spans 9.7K test examples, five quantization methods (FP8, GPTQ-int8, AWQ-int4,
GPTQ-int4, BNB-nf4), and five models (Llama-3.1 8B and 70B; Qwen-2.5 7B, 32B,
and 72B). We find that, on average, 8-bit quantization preserves accuracy
(~0.8% drop), whereas 4-bit methods lead to substantial losses, especially for
tasks involving long context inputs (drops of up to 59%). This degradation
tends to worsen when the input is in a language other than English. Crucially,
the effects of quantization depend heavily on the quantization method, model,
and task. For instance, while Qwen-2.5 72B remains robust under BNB-nf4,
Llama-3.1 70B experiences a 32% performance drop on the same task. These
findings highlight the importance of a careful, task-specific evaluation before
deploying quantized LLMs, particularly in long-context scenarios and with
languages other than English.

</details>


### [248] [OmniCharacter: Towards Immersive Role-Playing Agents with Seamless Speech-Language Personality Interaction](https://arxiv.org/abs/2505.20277)
*Haonan Zhang,Run Luo,Xiong Liu,Yuchuan Wu,Ting-En Lin,Pengpeng Zeng,Qiang Qu,Feiteng Fang,Min Yang,Lianli Gao,Jingkuan Song,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: OmniCharacter提出首个无缝语音-语言个性交互模型，通过结合角色特定个性与语音特征实现低延迟沉浸式角色扮演代理。


<details>
  <summary>Details</summary>
Motivation: 现有角色扮演代理（RPAs）主要关注文本对话模拟，忽略了语音特征（如音色、情感）对沉浸式交互体验的关键作用。

Method: 构建OmniCharacter-10K数据集（含20个独特角色、10K轮次对话、135K动态语音响应），开发支持语音-语言混合响应的统一模型。

Result: 实验表明该方法在内容与风格响应上优于现有RPAs及主流语音-语言模型，响应延迟低至289毫秒。

Conclusion: OmniCharacter通过语音-语言多模态对齐显著提升了角色扮演代理的沉浸感与实时性。

Abstract: Role-Playing Agents (RPAs), benefiting from large language models, is an
emerging interactive AI system that simulates roles or characters with diverse
personalities. However, existing methods primarily focus on mimicking dialogues
among roles in textual form, neglecting the role's voice traits (e.g., voice
style and emotions) as playing a crucial effect in interaction, which tends to
be more immersive experiences in realistic scenarios. Towards this goal, we
propose OmniCharacter, a first seamless speech-language personality interaction
model to achieve immersive RPAs with low latency. Specifically, OmniCharacter
enables agents to consistently exhibit role-specific personality traits and
vocal traits throughout the interaction, enabling a mixture of speech and
language responses. To align the model with speech-language scenarios, we
construct a dataset named OmniCharacter-10K, which involves more distinctive
characters (20), richly contextualized multi-round dialogue (10K), and dynamic
speech response (135K). Experimental results showcase that our method yields
better responses in terms of both content and style compared to existing RPAs
and mainstream speech-language models, with a response latency as low as 289ms.
Code and dataset are available at
https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/OmniCharacter.

</details>


### [249] [One-shot Entropy Minimization](https://arxiv.org/abs/2505.20282)
*Zitian Gao,Lynx Chen,Joey Zhou,Bryan Dai*

Main category: cs.CL

TL;DR: 通过训练13,440个大型语言模型，研究发现仅需一个未标记数据和10步优化，熵最小化就能达到或超越基于规则的强化学习使用数千数据和精心设计奖励的效果。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型后训练范式的简化方法，减少对大量数据和复杂奖励设计的依赖。

Method: 使用熵最小化技术，仅需一个未标记数据和10步优化来训练模型。

Result: 性能提升与使用数千数据和精心设计奖励的强化学习方法相当甚至更好。

Conclusion: 这一显著成果可能促使重新思考大型语言模型的后训练范式。

Abstract: We trained 13,440 large language models and found that entropy minimization
requires only a single unlabeled data and 10 steps optimization to achieve
performance improvements comparable to or even greater than those obtained
using thousands of data and carefully designed rewards in rule-based
reinforcement learning. This striking result may prompt a rethinking of
post-training paradigms for large language models. Our code is avaliable at
https://github.com/zitian-gao/one-shot-em.

</details>


### [250] [MASKSEARCH: A Universal Pre-Training Framework to Enhance Agentic Search Capability](https://arxiv.org/abs/2505.20285)
*Weiqi Wu,Xin Guan,Shen Huang,Yong Jiang,Pengjun Xie,Fei Huang,Jiuxin Cao,Hai Zhao,Jingren Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种名为MASKSEARCH的新型预训练框架，通过检索增强掩码预测任务（RAMP）提升大型语言模型（LLM）的通用检索和推理能力，并在下游任务中进一步优化，显著提升了基于LLM的搜索代理性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于训练的方法虽然显示出潜力，但其代理能力受限于训练数据的内在特性。为了进一步提升代理的通用搜索能力，作者提出了MASKSEARCH框架。

Method: MASKSEARCH框架包括预训练阶段的RAMP任务，模型学习利用搜索工具填补掩码跨度；下游任务训练结合监督微调（SFT）和强化学习（RL），采用多代理系统和混合奖励机制，并引入课程学习方法。

Result: 实验表明，MASKSEARCH显著提升了基于LLM的搜索代理在领域内和领域外下游任务中的性能。

Conclusion: MASKSEARCH框架通过预训练和下游任务训练的结合，有效增强了LLM的通用检索和推理能力，为开放域多跳问答等场景提供了有力支持。

Abstract: Retrieval-Augmented Language Models (RALMs) represent a classic paradigm
where models enhance generative capabilities using external knowledge retrieved
via a specialized module. Recent advancements in Agent techniques enable Large
Language Models (LLMs) to autonomously utilize tools for retrieval, planning,
and reasoning. While existing training-based methods show promise, their
agentic abilities are limited by inherent characteristics of the task-specific
data used during training. To further enhance the universal search capability
of agents, we propose a novel pre-training framework, MASKSEARCH. In the
pre-training stage, we introduce the Retrieval Augmented Mask Prediction (RAMP)
task, where the model learns to leverage search tools to fill masked spans on a
large number of pre-training data, thus acquiring universal retrieval and
reasoning capabilities for LLMs. After that, the model is trained on downstream
tasks to achieve further improvement. We apply both Supervised Fine-tuning
(SFT) and Reinforcement Learning (RL) for training. For SFT, we combine
agent-based and distillation-based methods to generate training data, starting
with a multi-agent system consisting of a planner, rewriter, observer, and
followed by a self-evolving teacher model. While for RL, we employ DAPO as the
training framework and adopt a hybrid reward system consisting of answer
rewards and format rewards. Additionally, we introduce a curriculum learning
approach that allows the model to learn progressively from easier to more
challenging instances based on the number of masked spans. We evaluate the
effectiveness of our framework in the scenario of open-domain multi-hop
question answering. Through extensive experiments, we demonstrate that
MASKSEARCH significantly enhances the performance of LLM-based search agents on
both in-domain and out-of-domain downstream tasks.

</details>


### [251] [Enhancing the Comprehensibility of Text Explanations via Unsupervised Concept Discovery](https://arxiv.org/abs/2505.20293)
*Yifan Sun,Danding Wang,Qiang Sheng,Juan Cao,Jintao Li*

Main category: cs.CL

TL;DR: 提出ECO-Concept框架，无需标注自动发现可理解概念，提升文本领域可解释AI效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于概念的可解释方法在文本领域应用有限，依赖预定义概念或生成难以理解的解释，影响用户信任。

Method: ECO-Concept采用对象中心架构自动提取语义概念，利用大模型评估可理解性，并通过微调优化解释。

Result: 实验表明该方法在多任务中表现优异，所学概念在可理解性上超越现有方法。

Conclusion: ECO-Concept为自动发现可理解概念提供了有效解决方案，增强了模型解释的可信度。

Abstract: Concept-based explainable approaches have emerged as a promising method in
explainable AI because they can interpret models in a way that aligns with
human reasoning. However, their adaption in the text domain remains limited.
Most existing methods rely on predefined concept annotations and cannot
discover unseen concepts, while other methods that extract concepts without
supervision often produce explanations that are not intuitively comprehensible
to humans, potentially diminishing user trust. These methods fall short of
discovering comprehensible concepts automatically. To address this issue, we
propose \textbf{ECO-Concept}, an intrinsically interpretable framework to
discover comprehensible concepts with no concept annotations. ECO-Concept first
utilizes an object-centric architecture to extract semantic concepts
automatically. Then the comprehensibility of the extracted concepts is
evaluated by large language models. Finally, the evaluation result guides the
subsequent model fine-tuning to obtain more understandable explanations.
Experiments show that our method achieves superior performance across diverse
tasks. Further concept evaluations validate that the concepts learned by
ECO-Concept surpassed current counterparts in comprehensibility.

</details>


### [252] [Self-reflective Uncertainties: Do LLMs Know Their Internal Answer Distribution?](https://arxiv.org/abs/2505.20295)
*Michael Kirchhof,Luca Füger,Adam Goliński,Eeshan Gunesh Dhekane,Arno Blaas,Sinead Williamson*

Main category: cs.CL

TL;DR: 该论文提出SelfReflect指标，用于评估大语言模型(LLM)如何忠实总结其内部答案分布，并探索了自我总结方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM不确定性量化仅提供百分比数字，论文认为输出字符串空间存在能总结LLM认为可能的输出字符串分布的字符串。

Method: 提出理论基础的SelfReflect指标，比较不同候选总结字符串的差异，并与人类判断对齐。

Result: SelfReflect能区分候选总结字符串的细微差异，优于LLM判断和嵌入比较等替代指标。采样和总结可生成忠实总结。

Conclusion: SelfReflect为LLM不确定性解释的新方向奠定基础，即使先进推理模型也难以明确表达其内部不确定性。

Abstract: To reveal when a large language model (LLM) is uncertain about a response,
uncertainty quantification commonly produces percentage numbers along with the
output. But is this all we can do? We argue that in the output space of LLMs,
the space of strings, exist strings expressive enough to summarize the
distribution over output strings the LLM deems possible. We lay a foundation
for this new avenue of uncertainty explication and present SelfReflect, a
theoretically-motivated metric to assess how faithfully a string summarizes an
LLM's internal answer distribution. We show that SelfReflect is able to
discriminate even subtle differences of candidate summary strings and that it
aligns with human judgement, outperforming alternative metrics such as LLM
judges and embedding comparisons. With SelfReflect, we investigate a number of
self-summarization methods and find that even state-of-the-art reasoning models
struggle to explicate their internal uncertainty. But we find that faithful
summarizations can be generated by sampling and summarizing. Our metric enables
future works towards this universal form of LLM uncertainties.

</details>


### [253] [Reasoning LLMs are Wandering Solution Explorers](https://arxiv.org/abs/2505.20296)
*Jiahao Lu,Ziwei Xu,Mohan Kankanhalli*

Main category: cs.CL

TL;DR: 当前推理大语言模型（RLLMs）缺乏系统性探索解空间的能力，存在无效推理、冗余探索等问题，需新评估指标。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）通过测试时计算技术展现出推理能力，但现有推理大语言模型（RLLMs）缺乏系统性解决问题的能力，导致在复杂任务中表现下降。

Method: 通过定性和定量分析，识别系统性解决问题的构成要素及常见失败模式，评估多款先进LLMs。

Result: 发现模型存在无效推理步骤、冗余探索、幻觉或不可靠结论等问题，简单任务表现尚可，复杂度增加时性能急剧下降。

Conclusion: 需开发新评估指标和工具，不仅关注最终输出，还需评估推理过程的结构。

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning abilities
through test-time computation (TTC) techniques such as chain-of-thought
prompting and tree-based reasoning. However, we argue that current reasoning
LLMs (RLLMs) lack the ability to systematically explore the solution space.
This paper formalizes what constitutes systematic problem solving and
identifies common failure modes that reveal reasoning LLMs to be wanderers
rather than systematic explorers. Through qualitative and quantitative analysis
across multiple state-of-the-art LLMs, we uncover persistent issues: invalid
reasoning steps, redundant explorations, hallucinated or unfaithful
conclusions, and so on. Our findings suggest that current models' performance
can appear to be competent on simple tasks yet degrade sharply as complexity
increases. Based on the findings, we advocate for new metrics and tools that
evaluate not just final outputs but the structure of the reasoning process
itself.

</details>


### [254] [MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal Manga Understanding](https://arxiv.org/abs/2505.20298)
*Jeonghun Baek,Kazuki Egashira,Shota Onohara,Atsuyuki Miyai,Yuki Imajuku,Hikaru Ikuta,Kiyoharu Aizawa*

Main category: cs.CL

TL;DR: 该论文提出了两个用于评估多模态漫画理解的基准测试（MangaOCR和MangaVQA），并开发了一个专门处理漫画的模型MangaLMM，通过与GPT-4o等模型比较，评估了多模态大模型在漫画理解上的表现。


<details>
  <summary>Details</summary>
Motivation: 漫画是一种结合图像和文本的复杂叙事形式，通过让多模态大模型（LMMs）像人类一样理解漫画，可以帮助漫画创作者反思和改进他们的故事。

Method: 论文引入了两个基准测试：MangaOCR（针对页面内文本识别）和MangaVQA（通过视觉问答评估上下文理解），并基于这些基准开发了专门处理漫画的模型MangaLMM。

Result: MangaVQA包含526个高质量的人工构建的问题-答案对，能够可靠地评估多样化的叙事和视觉场景。MangaLMM在实验中表现出色，与GPT-4o等专有模型相比具有竞争力。

Conclusion: 论文提出的基准和模型为在多模态漫画理解领域评估和推进LMMs提供了全面的基础。

Abstract: Manga, or Japanese comics, is a richly multimodal narrative form that blends
images and text in complex ways. Teaching large multimodal models (LMMs) to
understand such narratives at a human-like level could help manga creators
reflect on and refine their stories. To this end, we introduce two benchmarks
for multimodal manga understanding: MangaOCR, which targets in-page text
recognition, and MangaVQA, a novel benchmark designed to evaluate contextual
understanding through visual question answering. MangaVQA consists of 526
high-quality, manually constructed question-answer pairs, enabling reliable
evaluation across diverse narrative and visual scenarios. Building on these
benchmarks, we develop MangaLMM, a manga-specialized model finetuned from the
open-source LMM Qwen2.5-VL to jointly handle both tasks. Through extensive
experiments, including comparisons with proprietary models such as GPT-4o and
Gemini 2.5, we assess how well LMMs understand manga. Our benchmark and model
provide a comprehensive foundation for evaluating and advancing LMMs in the
richly narrative domain of manga.

</details>


### [255] [Suicide Risk Assessment Using Multimodal Speech Features: A Study on the SW1 Challenge Dataset](https://arxiv.org/abs/2505.13069)
*Ambre Marie,Ilias Maoudj,Guillaume Dardenne,Gwenolé Quellec*

Main category: cs.CL

TL;DR: 该研究通过多模态方法评估青少年自杀风险，结合语音转录、语言和音频嵌入及手工特征，发现加权注意力融合策略效果最佳，但泛化能力仍需提升。


<details>
  <summary>Details</summary>
Motivation: 研究基于SpeechWellness挑战，旨在开发一种基于语音的青少年自杀风险评估方法，以弥补现有技术的不足。

Method: 采用多模态方法，整合WhisperX自动转录、中文RoBERTa语言嵌入、WavLM音频嵌入及手工声学特征，探索了三种融合策略。

Result: 加权注意力融合策略在开发集上达到69%准确率，但开发集与测试集间的性能差距显示泛化能力有待提高。

Conclusion: 研究强调了优化嵌入表示和融合机制的重要性，以提升分类可靠性，并指出需进一步解决泛化挑战。

Abstract: The 1st SpeechWellness Challenge conveys the need for speech-based suicide
risk assessment in adolescents. This study investigates a multimodal approach
for this challenge, integrating automatic transcription with WhisperX,
linguistic embeddings from Chinese RoBERTa, and audio embeddings from WavLM.
Additionally, handcrafted acoustic features -- including MFCCs, spectral
contrast, and pitch-related statistics -- were incorporated. We explored three
fusion strategies: early concatenation, modality-specific processing, and
weighted attention with mixup regularization. Results show that weighted
attention provided the best generalization, achieving 69% accuracy on the
development set, though a performance gap between development and test sets
highlights generalization challenges. Our findings, strictly tied to the
MINI-KID framework, emphasize the importance of refining embedding
representations and fusion mechanisms to enhance classification reliability.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [256] [The end of radical concept nativism](https://arxiv.org/abs/2505.18277)
*Joshua S. Rule,Steven T. Piantadosi*

Main category: cs.AI

TL;DR: 该论文反驳了杰瑞·福多的激进概念先天论，认为人类确实能学习新概念，并通过计算机科学和信息理论的方法进行了论证。


<details>
  <summary>Details</summary>
Motivation: 长期以来，认知科学和心灵哲学认为学习全新事物是不可能的，尤其是福多的激进概念先天论认为大多数概念是先天固有的。这一观点深刻影响了认知科学，但许多反驳要么不成功，要么仅适用于狭窄的概念类别。本文旨在挑战这一观点。

Method: 论文首先回顾了先前论点的特点和局限，然后从表达能力、概念结构和概念拥有三个关键点出发，指出激进概念先天论与人类实际认知的差异，并利用计算机科学和信息理论的方法对这些观点进行了形式化。

Result: 通过形式化分析，论文发现激进概念先天论的关键论点与人类认知的实际表现存在分歧，从而支持了人类确实能够学习新概念的观点。

Conclusion: 论文得出结论，认为人类确实能够在重要意义上学习新概念，这为认知科学提供了新的理论方向。

Abstract: Though humans seem to be remarkable learners, arguments in cognitive science
and philosophy of mind have long maintained that learning something
fundamentally new is impossible. Specifically, Jerry Fodor's arguments for
radical concept nativism hold that most, if not all, concepts are innate and
that what many call concept learning never actually leads to the acquisition of
new concepts. These arguments have deeply affected cognitive science, and many
believe that the counterarguments to radical concept nativism have been either
unsuccessful or only apply to a narrow class of concepts. This paper first
reviews the features and limitations of prior arguments. We then identify three
critical points - related to issues of expressive power, conceptual structure,
and concept possession - at which the arguments in favor of radical concept
nativism diverge from describing actual human cognition. We use ideas from
computer science and information theory to formalize the relevant ideas in ways
that are arguably more scientifically productive. We conclude that, as a
result, there is an important sense in which people do indeed learn new
concepts.

</details>


### [257] [Understanding and Mitigating Overrefusal in LLMs from an Unveiling Perspective of Safety Decision Boundary](https://arxiv.org/abs/2505.18325)
*Licheng Pan,Yongqi Tong,Xin Zhang,Xiaolu Zhang,Jun Zhou,Zhixuan Chu*

Main category: cs.AI

TL;DR: 论文提出RASS框架，通过分析大语言模型的安全决策边界来缓解过度拒绝回答合理查询的问题，并构建了多语言评估集MORBench。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）常因过度保守的安全对齐而拒绝回答合理查询（即过度拒绝现象），需系统性解决此问题以提升模型实用性。

Method: 提出RASS框架：利用表示空间中的导向向量自动生成并筛选靠近安全边界的提示词，针对性优化模型决策边界。

Result: 发现过度拒绝与安全边界区域的错位密切相关，RASS能有效识别边界对齐提示词，且方法可扩展至多语言场景。

Conclusion: RASS为模型安全决策提供更精准的解释，并通过MORBench评估集支持多语言场景下的安全性与帮助性评估。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
a wide range of tasks, yet they often refuse to answer legitimate queries-a
phenomenon known as overrefusal. Overrefusal typically stems from
over-conservative safety alignment, causing models to treat many reasonable
prompts as potentially risky. To systematically understand this issue, we probe
and leverage the models'safety decision boundaries to analyze and mitigate
overrefusal. Our findings reveal that overrefusal is closely tied to
misalignment at these boundary regions, where models struggle to distinguish
subtle differences between benign and harmful content. Building on these
insights, we present RASS, an automated framework for prompt generation and
selection that strategically targets overrefusal prompts near the safety
boundary. By harnessing steering vectors in the representation space, RASS
efficiently identifies and curates boundary-aligned prompts, enabling more
effective and targeted mitigation of overrefusal. This approach not only
provides a more precise and interpretable view of model safety decisions but
also seamlessly extends to multilingual scenarios.We have explored the safety
decision boundaries of various LLMs and construct the MORBench evaluation set
to facilitate robust assessment of model safety and helpfulness across multiple
languages. Code and datasets will be released at
https://anonymous.4open.science/r/RASS-80D3.

</details>


### [258] [RedactOR: An LLM-Powered Framework for Automatic Clinical Data De-Identification](https://arxiv.org/abs/2505.18380)
*Praphul Singh,Charlotte Dzialo,Jangwon Kim,Sumana Srivatsa,Irfan Bulu,Sri Gadde,Krishnaram Kenthapadi*

Main category: cs.AI

TL;DR: 提出RedactOR框架，用于自动化多模态电子健康记录去标识化，结合规则与LLM方法，优化令牌使用并保持数据一致性。


<details>
  <summary>Details</summary>
Motivation: 现有去标识化方法存在召回错误、泛化能力不足和效率低下问题，限制了在AI驱动医疗领域的实际应用。

Method: 采用智能路由、混合规则与LLM结合策略及两步音频编辑方法，提出基于检索的实体重新词汇化技术。

Result: 在i2b2 2014数据集上取得竞争性性能，同时优化令牌使用以降低LLM成本。

Conclusion: RedactOR框架在实际医疗数据管道中展现出高效性与实用性，为临床AI系统提供了可行的去标识化解决方案。

Abstract: Ensuring clinical data privacy while preserving utility is critical for
AI-driven healthcare and data analytics. Existing de-identification (De-ID)
methods, including rule-based techniques, deep learning models, and large
language models (LLMs), often suffer from recall errors, limited
generalization, and inefficiencies, limiting their real-world applicability. We
propose a fully automated, multi-modal framework, RedactOR for de-identifying
structured and unstructured electronic health records, including clinical audio
records. Our framework employs cost-efficient De-ID strategies, including
intelligent routing, hybrid rule and LLM based approaches, and a two-step audio
redaction approach. We present a retrieval-based entity relexicalization
approach to ensure consistent substitutions of protected entities, thereby
enhancing data coherence for downstream applications. We discuss key design
desiderata, de-identification and relexicalization methodology, and modular
architecture of RedactX and its integration with the Oracle Health Clinical AI
system. Evaluated on the i2b2 2014 De-ID dataset using standard metrics with
strict recall, our approach achieves competitive performance while optimizing
token usage to reduce LLM costs. Finally, we discuss key lessons and insights
from deployment in real-world AI- driven healthcare data pipelines.

</details>


### [259] [Advertising in AI systems: Society must be vigilant](https://arxiv.org/abs/2505.18425)
*Menghua Wu,Yujia Bao*

Main category: cs.AI

TL;DR: 论文探讨AI系统作为互联网入口时，商业动机如何影响其内容输出，提出设计原则和用户应对策略。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统成为互联网主要入口，商业激励可能影响其内容生成，引发透明度和监管问题。

Method: 基于广告主、用户和平台的需求，提出商业化AI系统的设计原则及用户识别偏见的策略。

Result: 提出商业化AI系统的设计框架，并给出用户应对商业偏见的实用方法。

Conclusion: 呼吁关注AI商业化影响，提出开放性问题并倡导行动以实现透明和公正。

Abstract: AI systems have increasingly become our gateways to the Internet. We argue
that just as advertising has driven the monetization of web search and social
media, so too will commercial incentives shape the content served by AI. Unlike
traditional media, however, the outputs of these systems are dynamic,
personalized, and lack clear provenance -- raising concerns for transparency
and regulation. In this paper, we envision how commercial content could be
delivered through generative AI-based systems. Based on the requirements of key
stakeholders -- advertisers, consumers, and platforms -- we propose design
principles for commercially-influenced AI systems. We then outline high-level
strategies for end users to identify and mitigate commercial biases from model
outputs. Finally, we conclude with open questions and a call to action towards
these goals.

</details>


### [260] [EdgeAgentX: A Novel Framework for Agentic AI at the Edge in Military Communication Networks](https://arxiv.org/abs/2505.18457)
*Abir Ray*

Main category: cs.AI

TL;DR: EdgeAgentX框架结合联邦学习、多智能体强化学习和对抗防御机制，专为军事通信网络设计，显著提升自主决策能力并增强抗干扰性。


<details>
  <summary>Details</summary>
Motivation: 军事通信网络需要高效、低延迟且能抵御对抗性干扰的自主决策系统，EdgeAgentX旨在解决这一需求。

Method: 集成联邦学习（FL）、多智能体强化学习（MARL）和对抗防御机制，构建EdgeAgentX框架。

Result: 通过全面仿真验证，EdgeAgentX显著提升自主决策能力、降低延迟、提高吞吐量，并能鲁棒地抵御对抗性干扰。

Conclusion: EdgeAgentX为军事通信网络提供了一种高效、抗干扰的自主决策解决方案，具有重要应用价值。

Abstract: This paper introduces EdgeAgentX, a novel framework integrating federated
learning (FL), multi-agent reinforcement learning (MARL), and adversarial
defense mechanisms, tailored for military communication networks. EdgeAgentX
significantly improves autonomous decision-making, reduces latency, enhances
throughput, and robustly withstands adversarial disruptions, as evidenced by
comprehensive simulations.

</details>


### [261] [Pedagogy-R1: Pedagogically-Aligned Reasoning Model with Balanced Educational Benchmark](https://arxiv.org/abs/2505.18467)
*Unggi Lee,Jaeyong Lee,Jiyeong Bae,Yeil Jeong,Junbo Koh,Gyeonggeon Lee,Gunho Lee,Taekyung Ahn,Hyeoncheol Kim*

Main category: cs.AI

TL;DR: 论文提出Pedagogy-R1框架，通过蒸馏管道、教育评估基准和教学链提示策略，提升大型推理模型在教学中的适用性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在数学和编程等结构化领域表现优异，但缺乏教学连贯性和真实教学行为。

Method: 采用蒸馏管道过滤和优化模型输出，建立教育评估基准WBEB，并使用Chain-of-Pedagogy提示策略生成教学推理。

Result: 通过定量和定性评估，系统分析了大型推理模型在教学中的优势和局限性。

Conclusion: Pedagogy-R1框架有效提升了大型推理模型在教学中的应用潜力。

Abstract: Recent advances in large reasoning models (LRMs) show strong performance in
structured domains such as mathematics and programming; however, they often
lack pedagogical coherence and realistic teaching behaviors. To bridge this
gap, we introduce Pedagogy-R1, a framework that adapts LRMs for classroom use
through three innovations: (1) a distillation-based pipeline that filters and
refines model outputs for instruction-tuning, (2) the Well-balanced Educational
Benchmark (WBEB), which evaluates performance across subject knowledge,
pedagogical knowledge, tracing, essay scoring, and teacher decision-making, and
(3) a Chain-of-Pedagogy (CoP) prompting strategy for generating and eliciting
teacher-style reasoning. Our mixed-method evaluation combines quantitative
metrics with qualitative analysis, providing the first systematic assessment of
LRMs' pedagogical strengths and limitations.

</details>


### [262] [Chemical classification program synthesis using generative artificial intelligence](https://arxiv.org/abs/2505.18470)
*Christopher J. Mungall,Adnan Malik,Daniel R. Korn,Justin T. Reese,Noel M. O'Boyle,Noel,Janna Hastings*

Main category: cs.AI

TL;DR: 该论文提出了一种利用生成式AI自动编写化学分类程序的方法，用于高效分类化学结构，并生成自然语言解释，解决了现有方法依赖人工规则或缺乏可解释性的问题。


<details>
  <summary>Details</summary>
Motivation: 化学结构的准确分类对于化学信息学和生物信息学至关重要，但人工分类耗时且难以扩展，现有自动化方法要么依赖人工规则，要么缺乏可解释性。

Method: 使用生成式人工智能自动编写化学分类程序，构建可解释的计算本体模型（C3PO），用于SMILES结构的高效确定性分类。

Result: 在ChEBI数据库上验证了方法的有效性，并与最先进的深度学习模型进行了比较，展示了C3PO在分类分布外样本和发现数据库分类错误方面的潜力。

Conclusion: 该方法不仅提高了化学分类的效率和可解释性，还能通过结合生成本体、自动化文献搜索和多模态视觉模型的集成AI方法，帮助识别潜在错误。

Abstract: Accurately classifying chemical structures is essential for cheminformatics
and bioinformatics, including tasks such as identifying bioactive compounds of
interest, screening molecules for toxicity to humans, finding non-organic
compounds with desirable material properties, or organizing large chemical
libraries for drug discovery or environmental monitoring. However, manual
classification is labor-intensive and difficult to scale to large chemical
databases. Existing automated approaches either rely on manually constructed
classification rules, or the use of deep learning methods that lack
explainability.
  This work presents an approach that uses generative artificial intelligence
to automatically write chemical classifier programs for classes in the Chemical
Entities of Biological Interest (ChEBI) database. These programs can be used
for efficient deterministic run-time classification of SMILES structures, with
natural language explanations. The programs themselves constitute an
explainable computable ontological model of chemical class nomenclature, which
we call the ChEBI Chemical Class Program Ontology (C3PO).
  We validated our approach against the ChEBI database, and compared our
results against state of the art deep learning models. We also demonstrate the
use of C3PO to classify out-of-distribution examples taken from metabolomics
repositories and natural product databases. We also demonstrate the potential
use of our approach to find systematic classification errors in existing
chemical databases, and show how an ensemble artificial intelligence approach
combining generated ontologies, automated literature search, and multimodal
vision models can be used to pinpoint potential errors requiring expert
validation

</details>


### [263] [Retrieval Augmented Decision-Making: A Requirements-Driven, Multi-Criteria Framework for Structured Decision Support](https://arxiv.org/abs/2505.18483)
*Hongjia Wu,Hongxin Zhang,Wei Chen,Jiazhi Xia*

Main category: cs.AI

TL;DR: 本文提出RAD方法，结合多准则决策与LLM语义理解能力，解决行业文档检索与决策支持中的量化权重和可追溯性问题。


<details>
  <summary>Details</summary>
Motivation: 行业文档结构复杂且内容碎片化，现有LLM检索增强生成方法缺乏量化权重和可追溯推理路径，难以提供多层次透明决策支持。

Method: 提出RAD框架，自动提取文档关键准则并构建加权层次决策模型，生成结构化报告，引入显式权重分配和推理链确保准确性。

Result: 实验表明，RAD生成的决策报告在细节性、合理性和结构性上显著优于现有方法。

Conclusion: RAD在复杂决策支持场景中展现出应用价值和潜力。

Abstract: Various industries have produced a large number of documents such as
industrial plans, technical guidelines, and regulations that are structurally
complex and content-wise fragmented. This poses significant challenges for
experts and decision-makers in terms of retrieval and understanding. Although
existing LLM-based Retrieval-Augmented Generation methods can provide
context-related suggestions, they lack quantitative weighting and traceable
reasoning paths, making it difficult to offer multi-level and transparent
decision support. To address this issue, this paper proposes the RAD method,
which integrates Multi-Criteria Decision Making with the semantic understanding
capabilities of LLMs. The method automatically extracts key criteria from
industry documents, builds a weighted hierarchical decision model, and
generates structured reports under model guidance. The RAD framework introduces
explicit weight assignment and reasoning chains in decision generation to
ensure accuracy, completeness, and traceability. Experiments show that in
various decision-making tasks, the decision reports generated by RAD
significantly outperform existing methods in terms of detail, rationality, and
structure, demonstrating its application value and potential in complex
decision support scenarios.

</details>


### [264] [Enumerate-Conjecture-Prove: Formally Solving Answer-Construction Problems in Math Competitions](https://arxiv.org/abs/2505.18492)
*Jialiang Sun,Yuzhi Tang,Ao Li,Chris J. Maddison,Kuldeep S. Meel*

Main category: cs.AI

TL;DR: 论文提出ECP框架，结合LLM的创造力与符号证明的严谨性，显著提升数学竞赛中答案构建和定理证明的准确率。


<details>
  <summary>Details</summary>
Motivation: 数学推理在AI应用中至关重要，但现有方法在创造性猜想和形式化验证之间存在鸿沟。LLM擅长生成答案但不精于验证，符号证明器则反之。

Method: 提出Enumerate-Conjecture-Prove (ECP)框架，整合LLM的枚举猜想能力与符号定理证明，并构建ConstructiveBench数据集用于评估。

Result: ECP将答案构建准确率从14.54%提升至45.06%，结合DeepSeek-Prover后定理证明准确率达25.01%，显著优于纯符号方法(9.86%)。

Conclusion: ECP框架通过神经符号结合有效弥合了数学推理中创造性与严谨性的差距，代码和数据集已开源。

Abstract: Mathematical reasoning lies at the heart of artificial intelligence,
underpinning applications in education, program verification, and
research-level mathematical discovery. Mathematical competitions, in
particular, present two challenging problem types: theorem-proving, requiring
rigorous proofs of stated conclusions, and answer-construction, involving
hypothesizing and formally verifying mathematical objects. Large Language
Models (LLMs) effectively generate creative candidate answers but struggle with
formal verification, while symbolic provers ensure rigor but cannot efficiently
handle creative conjecture generation. We introduce the
Enumerate-Conjecture-Prove (ECP) framework, a modular neuro-symbolic method
integrating LLM-based enumeration and pattern-driven conjecturing with formal
theorem proving. We present ConstructiveBench, a dataset of 3,431
answer-construction problems in various math competitions with verified Lean
formalizations. On the ConstructiveBench dataset, ECP improves the accuracy of
answer construction from the Chain-of-Thought (CoT) baseline of 14.54% to
45.06% with the gpt-4.1-mini model. Moreover, combining with ECP's constructed
answers, the state-of-the-art DeepSeek-Prover-V2-7B model generates correct
proofs for 858 of the 3,431 constructive problems in Lean, achieving 25.01%
accuracy, compared to 9.86% for symbolic-only baselines. Our code and dataset
are publicly available at GitHub and HuggingFace, respectively.

</details>


### [265] [Knowledge Grafting of Large Language Models](https://arxiv.org/abs/2505.18502)
*Guodong Du,Xuanning Zhou,Junlin Li,Zhuo Li,Zesheng Shi,Wanyu Lin,Ho-Kin Tang,Xiucheng Li,Fangming Liu,Wenya Wang,Min Zhang,Jing Li*

Main category: cs.AI

TL;DR: 本文提出GraftLLM方法，通过SkillPack格式实现大语言模型间的高效跨能力迁移，解决现有方法在异构模型上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有跨能力迁移方法主要针对小型同构模型，难以适用于大型异构模型，且存在参数冲突和灾难性遗忘问题。

Method: 采用SkillPack格式存储源模型能力，结合模块感知自适应压缩策略，实现参数高效存储和任务知识保留。

Result: 实验表明GraftLLM在知识迁移、融合和持续学习方面优于现有技术，提供可扩展的解决方案。

Conclusion: GraftLLM为跨能力迁移提供了高效、可扩展的方案，支持异构模型融合和持续学习。

Abstract: Cross-capability transfer is a key challenge in large language model (LLM)
research, with applications in multi-task integration, model compression, and
continual learning. Recent works like FuseLLM and FuseChat have demonstrated
the potential of transferring multiple model capabilities to lightweight
models, enhancing adaptability and efficiency, which motivates our
investigation into more efficient cross-capability transfer methods. However,
existing approaches primarily focus on small, homogeneous models, limiting
their applicability. For large, heterogeneous models, knowledge distillation
with full-parameter fine-tuning often overlooks the student model's intrinsic
capacity and risks catastrophic forgetting, while PEFT methods struggle to
effectively absorb knowledge from source LLMs. To address these issues, we
introduce GraftLLM, a novel method that stores source model capabilities in a
target model with SkillPack format. This approach preserves general
capabilities, reduces parameter conflicts, and supports forget-free continual
learning and model fusion. We employ a module-aware adaptive compression
strategy to compress parameter updates, ensuring efficient storage while
maintaining task-specific knowledge. The resulting SkillPack serves as a
compact and transferable knowledge carrier, ideal for heterogeneous model
fusion and continual learning. Experiments across various scenarios demonstrate
that GraftLLM outperforms existing techniques in knowledge transfer, knowledge
fusion, and forget-free learning, providing a scalable and efficient solution
for cross-capability transfer. The code is publicly available at:
https://github.com/duguodong7/GraftLLM.

</details>


### [266] [LiSTEN: Learning Soft Token Embeddings for Neural Audio LLMs](https://arxiv.org/abs/2505.18517)
*Pooneh Mousavi,Shubham Gupta,Cem Subakan,Mirco Ravanelli*

Main category: cs.AI

TL;DR: LiSTEN框架通过动态提示选择和可学习键值对，使大型语言模型适应音频任务，减少对大数据集的依赖，简化训练过程并提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多任务处理中表现出色，但适应通用音频-语言任务时面临声学环境和任务差异的挑战，需要一种高效且不易过拟合的适配方法。

Method: LiSTEN采用动态提示选择策略和可学习键值对，平衡通用与任务特定知识，避免多任务过拟合，并使用单阶段训练简化流程。

Result: LiSTEN在减少可训练参数和依赖大规模数据集的同时，实现了竞争性性能，并通过分析提示多样性增强了模型的可解释性。

Conclusion: LiSTEN为音频-语言任务提供了一种高效、简洁且可解释的适配框架，显著降低了训练复杂性和数据需求。

Abstract: Foundation models based on large language models (LLMs) have shown great
success in handling various tasks and modalities. However, adapting these
models for general-purpose audio-language tasks is challenging due to
differences in acoustic environments and task variations. In this work, we
introduce LiSTEN Learning Soft Token Embeddings for Neural Audio LLMs), a
framework for adapting LLMs to speech and audio tasks. LiSTEN uses a dynamic
prompt selection strategy with learnable key-value pairs, allowing the model to
balance general and task-specific knowledge while avoiding overfitting in a
multitask setting. Our approach reduces dependence on large-scale ASR or
captioning datasets, achieves competitive performance with fewer trainable
parameters, and simplifies training by using a single-stage process.
Additionally, LiSTEN enhances interpretability by analyzing the diversity and
overlap of selected prompts across different tasks.

</details>


### [267] [Generative RLHF-V: Learning Principles from Multi-modal Human Preference](https://arxiv.org/abs/2505.18531)
*Jiayi Zhou,Jiaming Ji,Boyuan Chen,Jiapeng Sun,Wenqi Chen,Donghai Hong,Sirui Han,Yike Guo,Yaodong Yang*

Main category: cs.AI

TL;DR: 提出Generative RLHF-V框架，通过结合生成式奖励模型与多模态RLHF，显著提升多模态大语言模型的对齐性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于分数的奖励模型在对齐任务中存在准确性低、泛化性差和可解释性弱的问题，阻碍了如RLHF等对齐方法的发展。

Method: 采用两阶段流程：1) 通过强化学习指导生成式奖励模型主动捕捉人类意图并预测成对分数；2) 通过分组响应比较增强多模态RL评分精度。

Result: 实验显示，该框架在7个基准测试中将4个MLLM的性能提升18.1%，显著优于基线RLHF的5.3%，且候选响应数量增加时呈现近线性改进。

Conclusion: Generative RLHF-V框架有效解决了传统奖励模型的局限性，显著提升了多模态大语言模型的对齐性能和泛化能力。

Abstract: Training multi-modal large language models (MLLMs) that align with human
intentions is a long-term challenge. Traditional score-only reward models for
alignment suffer from low accuracy, weak generalization, and poor
interpretability, blocking the progress of alignment methods, e.g.,
reinforcement learning from human feedback (RLHF). Generative reward models
(GRMs) leverage MLLMs' intrinsic reasoning capabilities to discriminate
pair-wise responses, but their pair-wise paradigm makes it hard to generalize
to learnable rewards. We introduce Generative RLHF-V, a novel alignment
framework that integrates GRMs with multi-modal RLHF. We propose a two-stage
pipeline: $\textbf{multi-modal generative reward modeling from RL}$, where RL
guides GRMs to actively capture human intention, then predict the correct
pair-wise scores; and $\textbf{RL optimization from grouped comparison}$, which
enhances multi-modal RL scoring precision by grouped responses comparison.
Experimental results demonstrate that, besides out-of-distribution
generalization of RM discrimination, our framework improves 4 MLLMs'
performance across 7 benchmarks by $18.1\%$, while the baseline RLHF is only
$5.3\%$. We further validate that Generative RLHF-V achieves a near-linear
improvement with an increasing number of candidate responses. Our code and
models can be found at https://generative-rlhf-v.github.io.

</details>


### [268] [RoleRAG: Enhancing LLM Role-Playing via Graph Guided Retrieval](https://arxiv.org/abs/2505.18541)
*Yongjie Wang,Jonathan Leung,Zhiqi Shen*

Main category: cs.AI

TL;DR: 论文提出RoleRAG框架，通过检索增强解决大语言模型在角色模仿中知识不一致和边界模糊的问题。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在角色模仿时存在生成内容与角色背景无关或不一致的问题，主要由于实体歧义和角色认知边界不清晰。

Method: 提出RoleRAG框架，结合实体消歧和边界感知检索器，从结构化知识图谱中提取上下文相关信息。

Result: 实验表明RoleRAG能帮助通用和角色专用大语言模型更好地对齐角色知识，减少幻觉回答。

Conclusion: RoleRAG通过校准检索有效提升了角色模仿的准确性和一致性。

Abstract: Large Language Models (LLMs) have shown promise in character imitation,
enabling immersive and engaging conversations. However, they often generate
content that is irrelevant or inconsistent with a character's background. We
attribute these failures to: (1) the inability to accurately recall
character-specific knowledge due to entity ambiguity, and (2) a lack of
awareness of the character's cognitive boundaries. To address these issues, we
propose RoleRAG, a retrieval-based framework that integrates efficient entity
disambiguation for knowledge indexing with a boundary-aware retriever for
extracting contextually appropriate information from a structured knowledge
graph. Experiments on role-playing benchmarks show that RoleRAG's calibrated
retrieval helps both general-purpose and role-specific LLMs better align with
character knowledge and reduce hallucinated responses.

</details>


### [269] [Diffusion Blend: Inference-Time Multi-Preference Alignment for Diffusion Models](https://arxiv.org/abs/2505.18547)
*Min Cheng,Fatemeh Doudi,Dileep Kalathil,Mohammad Ghavamzadeh,Panganamala R. Kumar*

Main category: cs.AI

TL;DR: 论文提出Diffusion Blend方法，通过混合微调模型的反向扩散过程，实现推理时多偏好对齐，无需额外微调即可生成符合用户指定奖励组合的图像。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法在微调扩散模型时，通常只能针对单一奖励函数进行优化，难以平衡多个可能冲突的目标。此外，用户偏好因提示、个体和部署环境而异，需要灵活调整与预训练基模型的偏离程度。

Method: 提出Diffusion Blend方法，包含两种算法：DB-MPA用于多奖励对齐，DB-KLA用于KL正则化控制，通过混合微调模型的反向扩散过程实现推理时多偏好对齐。

Result: 实验表明，Diffusion Blend算法在性能上优于相关基线方法，接近或超过单独微调模型的效果，能够在推理时高效实现用户驱动的对齐。

Conclusion: Diffusion Blend为扩散模型的多偏好对齐提供了一种高效灵活的解决方案，支持用户根据需求在推理时调整奖励组合和正则化强度。

Abstract: Reinforcement learning (RL) algorithms have been used recently to align
diffusion models with downstream objectives such as aesthetic quality and
text-image consistency by fine-tuning them to maximize a single reward function
under a fixed KL regularization. However, this approach is inherently
restrictive in practice, where alignment must balance multiple, often
conflicting objectives. Moreover, user preferences vary across prompts,
individuals, and deployment contexts, with varying tolerances for deviation
from a pre-trained base model. We address the problem of inference-time
multi-preference alignment: given a set of basis reward functions and a
reference KL regularization strength, can we design a fine-tuning procedure so
that, at inference time, it can generate images aligned with any user-specified
linear combination of rewards and regularization, without requiring additional
fine-tuning? We propose Diffusion Blend, a novel approach to solve
inference-time multi-preference alignment by blending backward diffusion
processes associated with fine-tuned models, and we instantiate this approach
with two algorithms: DB-MPA for multi-reward alignment and DB-KLA for KL
regularization control. Extensive experiments show that Diffusion Blend
algorithms consistently outperform relevant baselines and closely match or
exceed the performance of individually fine-tuned models, enabling efficient,
user-driven alignment at inference-time. The code is available at
https://github.com/bluewoods127/DB-2025}{github.com/bluewoods127/DB-2025.

</details>


### [270] [Response Uncertainty and Probe Modeling: Two Sides of the Same Coin in LLM Interpretability?](https://arxiv.org/abs/2505.18575)
*Yongjie Wang,Yibo Wang,Xin Zhou,Zhiqi Shen*

Main category: cs.AI

TL;DR: 研究发现LLM响应不确定性与探针性能相关，高不确定性导致探针性能下降，同时揭示了LLM与人类知识对齐的实例。


<details>
  <summary>Details</summary>
Motivation: 探讨影响探针训练数据集适用性的因素，理解LLM内部特征空间与生成响应特性之间的关系。

Method: 通过定量分析探针性能和LLM响应不确定性，结合特征重要性分析深入研究相关性。

Result: 探针性能提升与响应不确定性降低强相关；高响应方差对应更多重要特征，导致探针性能下降。发现LLM表征与人类知识对齐的实例。

Conclusion: 响应不确定性是影响探针性能的关键因素，LLM内部特征与人类知识存在可解释的对齐关系。

Abstract: Probing techniques have shown promise in revealing how LLMs encode
human-interpretable concepts, particularly when applied to curated datasets.
However, the factors governing a dataset's suitability for effective probe
training are not well-understood. This study hypothesizes that probe
performance on such datasets reflects characteristics of both the LLM's
generated responses and its internal feature space. Through quantitative
analysis of probe performance and LLM response uncertainty across a series of
tasks, we find a strong correlation: improved probe performance consistently
corresponds to a reduction in response uncertainty, and vice versa.
Subsequently, we delve deeper into this correlation through the lens of feature
importance analysis. Our findings indicate that high LLM response variance is
associated with a larger set of important features, which poses a greater
challenge for probe models and often results in diminished performance.
Moreover, leveraging the insights from response uncertainty analysis, we are
able to identify concrete examples where LLM representations align with human
knowledge across diverse domains, offering additional evidence of interpretable
reasoning in LLMs.

</details>


### [271] [RvLLM: LLM Runtime Verification with Domain Knowledge](https://arxiv.org/abs/2505.18585)
*Yedi Zhang,Sun Yi Emma,Annabelle Lee Jia En,Annabelle Lee Jia En,Jin Song Dong*

Main category: cs.AI

TL;DR: 该论文提出了一种结合领域知识的大语言模型(LLM)错误检测方法，通过设计规范语言ESL和运行时验证框架RvLLM，有效识别LLM输出中的错误。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在文本理解和生成方面表现出色，但其输出可能存在不一致或错误，尤其在需要高准确性的领域。现有研究多关注通用场景下的错误检测，忽视了领域知识的整合潜力。

Method: 设计了一种规范语言ESL，允许领域专家以轻量直观的方式定义领域特定谓词，并开发了运行时验证框架RvLLM，用于根据ESL定义的约束验证LLM输出。

Result: 在新加坡捷运系统法规遵守、数值比较和不等式求解三个任务上的实验表明，RvLLM能轻量灵活地检测多种LLM的错误输出。

Conclusion: 尽管LLM能力强大，但由于解释性有限和缺乏形式化保证，仍易犯低级错误。该框架通过利用领域知识严格高效地验证LLM输出，提供了潜在的长期解决方案。

Abstract: Large language models (LLMs) have emerged as a dominant AI paradigm due to
their exceptional text understanding and generation capabilities. However,
their tendency to generate inconsistent or erroneous outputs challenges their
reliability, especially in high-stakes domains requiring accuracy and
trustworthiness. Existing research primarily focuses on detecting and
mitigating model misbehavior in general-purpose scenarios, often overlooking
the potential of integrating domain-specific knowledge. In this work, we
advance misbehavior detection by incorporating domain knowledge. The core idea
is to design a general specification language that enables domain experts to
customize domain-specific predicates in a lightweight and intuitive manner,
supporting later runtime verification of LLM outputs. To achieve this, we
design a novel specification language, ESL, and introduce a runtime
verification framework, RvLLM, to validate LLM output against domain-specific
constraints defined in ESL. We evaluate RvLLM on three representative tasks:
violation detection against Singapore Rapid Transit Systems Act, numerical
comparison, and inequality solving. Experimental results demonstrate that RvLLM
effectively detects erroneous outputs across various LLMs in a lightweight and
flexible manner. The results reveal that despite their impressive capabilities,
LLMs remain prone to low-level errors due to limited interpretability and a
lack of formal guarantees during inference, and our framework offers a
potential long-term solution by leveraging expert domain knowledge to
rigorously and efficiently verify LLM outputs.

</details>


### [272] [LLMs for Supply Chain Management](https://arxiv.org/abs/2505.18597)
*Haojie Wang,Jiuyun Jiang,L. Jeff Hong,Guangxin Jiang*

Main category: cs.AI

TL;DR: 本文提出了一种结合检索增强生成（RAG）框架的供应链管理（SCM）专用大语言模型（LLM），并通过标准化考试和啤酒游戏测试验证其专家级能力。此外，利用LLM进行供应链横向与纵向博弈分析，揭示了经典SCM文献中的见解及新行为。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的发展为供应链管理（SCM）研究提供了新工具，但如何动态整合外部知识并提升SCM任务性能仍需探索。本文旨在开发一个领域专用的SCM LLM，并通过博弈分析研究供应链中的竞争与合作。

Method: 采用检索增强生成（RAG）框架动态整合外部知识，开发SCM专用LLM，并通过标准化考试和啤酒游戏测试验证其能力。进一步利用LLM进行横向与纵向供应链博弈分析。

Result: 实验表明，RAG显著提升了SCM任务性能。博弈论分析显示，LLM不仅能复现经典SCM文献的见解，还能发现新行为，为牛鞭效应等现象提供新视角。

Conclusion: 本文为通过LLMs探索复杂供应链网络中的合作与竞争开辟了新途径，展示了LLM在SCM领域的潜力与创新性应用。

Abstract: The development of large language models (LLMs) has provided new tools for
research in supply chain management (SCM). In this paper, we introduce a
retrieval-augmented generation (RAG) framework that dynamically integrates
external knowledge into the inference process, and develop a domain-specialized
SCM LLM, which demonstrates expert-level competence by passing standardized SCM
examinations and beer game tests. We further employ the use of LLMs to conduct
horizontal and vertical supply chain games, in order to analyze competition and
cooperation within supply chains. Our experiments show that RAG significantly
improves performance on SCM tasks. Moreover, game-theoretic analysis reveals
that the LLM can reproduce insights from the classical SCM literature, while
also uncovering novel behaviors and offering fresh perspectives on phenomena
such as the bullwhip effect. This paper opens the door for exploring
cooperation and competition for complex supply chain network through the lens
of LLMs.

</details>


### [273] [Doc-CoB: Enhancing Multi-Modal Document Understanding with Visual Chain-of-Boxes Reasoning](https://arxiv.org/abs/2505.18603)
*Ye Mo,Zirui Shao,Kai Ye,Xianwei Mao,Bo Zhang,Hangdi Xing,Peng Ye,Gang Huang,Kehan Chen,Zhou Huan,Zixu Yan,Sheng Zhou*

Main category: cs.AI

TL;DR: Doc-CoB提出了一种基于人类阅读模式的机制，通过自主选择相关区域提升多模态大模型在文档理解中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在处理文档图像时未考虑查询相关性，导致无法聚焦关键区域并产生不可靠响应。文档图像信息密集的特性使得这一问题尤为突出。

Method: 提出Doc-CoB机制，通过布局分析器与MLLM结合生成训练数据，引入两个辅助任务（框识别和框-查询推理）来增强文档理解能力。

Result: 在7个基准测试和4个流行模型上的实验表明，Doc-CoB显著提升了性能，证明了其有效性和广泛适用性。

Conclusion: Doc-CoB通过模拟人类阅读模式实现了更精准的文档理解，所有代码、数据和模型将开源。

Abstract: Multimodal large language models (MLLMs) have made significant progress in
document understanding. However, the information-dense nature of document
images still poses challenges, as most queries depend on only a few relevant
regions, with the rest being redundant. Existing one-pass MLLMs process entire
document images without considering query relevance, often failing to focus on
critical regions and producing unfaithful responses. Inspired by the human
coarse-to-fine reading pattern, we introduce Doc-CoB (Chain-of-Box), a
simple-yet-effective mechanism that integrates human-style visual reasoning
into MLLM without modifying its architecture. Our method allows the model to
autonomously select the set of regions (boxes) most relevant to the query, and
then focus attention on them for further understanding. We first design a fully
automatic pipeline, integrating a commercial MLLM with a layout analyzer, to
generate 249k training samples with intermediate visual reasoning supervision.
Then we incorporate two enabling tasks that improve box identification and
box-query reasoning, which together enhance document understanding. Extensive
experiments on seven benchmarks with four popular models show that Doc-CoB
significantly improves performance, demonstrating its effectiveness and wide
applicability. All code, data, and models will be released publicly.

</details>


### [274] [Knowledge Retrieval in LLM Gaming: A Shift from Entity-Centric to Goal-Oriented Graphs](https://arxiv.org/abs/2505.18607)
*Jonathan Leung,Yongjie Wang,Zhiqi Shen*

Main category: cs.AI

TL;DR: 提出基于目标导向图（GoGs）的新框架，通过显式检索推理路径提升大语言模型在游戏任务中的推理能力，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在逐步推理（尤其是复杂游戏场景）中存在不足，现有检索增强方法（如GraphRAG）因实体关系图碎片化和局部连接过密导致推理连贯性差。

Method: 构建目标导向图（GoGs），节点表示目标及其属性，边编码目标间逻辑依赖，通过分层检索目标与子目标形成连贯推理链以指导LLM生成。

Result: 在Minecraft测试中显著提升LLM的推理能力，性能超越GraphRAG等基线方法。

Conclusion: GoGs框架通过结构化目标依赖关系有效增强LLM的连贯推理能力，为复杂任务中的推理问题提供新解决方案。

Abstract: Large Language Models (LLMs) demonstrate impressive general capabilities but
often struggle with step-by-step reasoning, especially in complex applications
such as games. While retrieval-augmented methods like GraphRAG attempt to
bridge this gap through cross-document extraction and indexing, their
fragmented entity-relation graphs and overly dense local connectivity hinder
the construction of coherent reasoning. In this paper, we propose a novel
framework based on Goal-Oriented Graphs (GoGs), where each node represents a
goal and its associated attributes, and edges encode logical dependencies
between goals. This structure enables explicit retrieval of reasoning paths by
first identifying high-level goals and recursively retrieving their subgoals,
forming coherent reasoning chains to guide LLM prompting. Our method
significantly enhances the reasoning ability of LLMs in game-playing tasks, as
demonstrated by extensive experiments on the Minecraft testbed, outperforming
GraphRAG and other baselines.

</details>


### [275] [Mind The Gap: Deep Learning Doesn't Learn Deeply](https://arxiv.org/abs/2505.18623)
*Lucas Saldyt,Subbarao Kambhampati*

Main category: cs.AI

TL;DR: 该论文通过神经编译技术研究神经网络如何学习算法推理，比较编译与常规学习参数，揭示表达与训练间的差距。


<details>
  <summary>Details</summary>
Motivation: 探究神经网络在算法推理中的学习机制，理解其成功与失败的原因，以提升神经网络从数据中稳健学习复杂算法的能力。

Method: 采用神经编译技术，将源算法直接编码到神经网络参数中，绕过训练过程，对比编译与常规学习的效果，重点研究图神经网络（GNNs）。

Result: 发现归纳学习对并行算法（如NC类）最有效，揭示了算法推理中表达与训练间的根本差距。

Conclusion: 神经编译为理解神经网络学习算法推理提供了新视角，归纳学习在特定算法类别中表现最佳。

Abstract: This paper aims to understand how neural networks learn algorithmic reasoning
by addressing two questions: How faithful are learned algorithms when they are
effective, and why do neural networks fail to learn effective algorithms
otherwise? To answer these questions, we use neural compilation, a technique
that directly encodes a source algorithm into neural network parameters,
enabling the network to compute the algorithm exactly. This enables comparison
between compiled and conventionally learned parameters, intermediate vectors,
and behaviors. This investigation is crucial for developing neural networks
that robustly learn complexalgorithms from data. Our analysis focuses on graph
neural networks (GNNs), which are naturally aligned with algorithmic reasoning
tasks, specifically our choices of BFS, DFS, and Bellman-Ford, which cover the
spectrum of effective, faithful, and ineffective learned algorithms. Commonly,
learning algorithmic reasoning is framed as induction over synthetic data,
where a parameterized model is trained on inputs, traces, and outputs produced
by an underlying ground truth algorithm. In contrast, we introduce a neural
compilation method for GNNs, which sets network parameters analytically,
bypassing training. Focusing on GNNs leverages their alignment with algorithmic
reasoning, extensive algorithmic induction literature, and the novel
application of neural compilation to GNNs. Overall, this paper aims to
characterize expressability-trainability gaps - a fundamental shortcoming in
learning algorithmic reasoning. We hypothesize that inductive learning is most
effective for parallel algorithms contained within the computational class
\texttt{NC}.

</details>


### [276] [Riverine Flood Prediction and Early Warning in Mountainous Regions using Artificial Intelligence](https://arxiv.org/abs/2505.18645)
*Haleema Bibi,Sadia Saleem,Zakia Jalil,Muhammad Nasir,Tahani Alsubait*

Main category: cs.AI

TL;DR: 该研究利用卫星气候数据和多种机器学习模型（如SVM、XGBoost、ANN、LSTM和GRU）预测喀布尔河的日流量和多步流量，发现LSTM模型表现最佳，R2值达0.96，RMSE为140.96 m3/sec。短期预测（最多5天）效果显著，但长期预测需更多历史数据支持。


<details>
  <summary>Details</summary>
Motivation: 洪水是全球最具破坏性的自然灾害之一，尤其在山区，复杂地形和极端气候变化加剧了风险。跨境流域（如巴基斯坦和阿富汗之间的喀布尔河）上游数据获取困难，影响了洪水控制和预警系统的效果。

Method: 研究使用卫星气候数据，应用多种机器学习模型（SVM、XGBoost、ANN、LSTM和GRU）进行日流量和多步流量预测，重点关注LSTM和GRU在短期预测中的表现。

Result: LSTM模型表现最佳，R2值为0.96，RMSE为140.96 m3/sec。短期预测（最多5天）效果显著，但第四天后准确性下降，表明长期预测需要更长的历史数据集。

Conclusion: 研究结果支持可持续发展目标（SDG 6、11、13和15），有助于灾害和水资源管理、及时疏散、提高准备和有效预警。未来需更多历史数据以提升长期预测可靠性。

Abstract: Flooding is the most devastating phenomenon occurring globally, particularly
in mountainous regions, risk dramatically increases due to complex terrains and
extreme climate changes. These situations are damaging livelihoods,
agriculture, infrastructure, and human lives. This study uses the Kabul River
between Pakistan and Afghanistan as a case study to reflect the complications
of flood forecasting in transboundary basins. The challenges in obtaining
upstream data impede the efficacy of flood control measures and early warning
systems, a common global problem in similar basins. Utilizing satellite-based
climatic data, this study applied numerous advanced machine-learning and deep
learning models, such as Support Vector Machines (SVM), XGBoost, and Artificial
Neural Networks (ANN), Long Short-Term Memory (LSTM) networks, and Gated
Recurrent Units (GRU) to predict daily and multi-step river flow. The LSTM
network outperformed other models, achieving the highest R2 value of 0.96 and
the lowest RMSE value of 140.96 m3/sec. The time series LSTM and GRU network
models, utilized for short-term forecasts of up to five days, performed
significantly. However, the accuracy declined beyond the fourth day,
highlighting the need for longer-term historical datasets for reliable
long-term flood predictions. The results of the study are directly aligned with
Sustainable Development Goals 6, 11, 13, and 15, facilitating disaster and
water management, timely evacuations, improved preparedness, and effective
early warning.

</details>


### [277] [MLLMs are Deeply Affected by Modality Bias](https://arxiv.org/abs/2505.18657)
*Xu Zheng,Chenfei Liao,Yuqian Fu,Kaiyu Lei,Yuanhuiyi Lyu,Lutao Jiang,Bin Ren,Jialei Chen,Jiawen Wang,Chengxin Li,Linfeng Zhang,Danda Pani Paudel,Xuanjing Huang,Yu-Gang Jiang,Nicu Sebe,Dacheng Tao,Luc Van Gool,Xuming Hu*

Main category: cs.AI

TL;DR: 多模态大语言模型（MLLMs）存在模态偏差问题，过度依赖语言而忽视视觉输入。本文分析了模态偏差的现状、成因，并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在整合文本和图像等多模态数据时，存在明显的模态偏差，即过度依赖语言模态而忽视其他模态（如视觉）。这种偏差限制了模型的多模态能力，亟需系统性研究。

Method: 1. 诊断模态偏差的现状；2. 提出研究路线图；3. 通过实验验证模态偏差的三大关键因素：数据特性、骨干网络能力不平衡、训练目标缺陷。

Result: 实验表明：1. 语言数据紧凑抽象，视觉数据冗余复杂，导致学习动态不平衡；2. 预训练语言模型的主导地位使MLLMs过度依赖语言；3. 当前训练目标难以实现跨模态平衡对齐。

Conclusion: 需开发平衡的训练策略和模型架构以更好整合多模态。呼吁跨学科合作解决模态偏差问题，推动MLLMs向通用人工智能发展。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have shown
promising results in integrating diverse modalities such as texts and images.
MLLMs are heavily influenced by modality bias, often relying on language while
under-utilizing other modalities like visual inputs. This position paper argues
that MLLMs are deeply affected by modality bias. Firstly, we diagnose the
current state of modality bias, highlighting its manifestations across various
tasks. Secondly, we propose a systematic research road-map related to modality
bias in MLLMs. Thirdly, we identify key factors of modality bias in MLLMs and
offer actionable suggestions for future research to mitigate it. To
substantiate these findings, we conduct experiments that demonstrate the
influence of each factor: 1. Data Characteristics: Language data is compact and
abstract, while visual data is redundant and complex, creating an inherent
imbalance in learning dynamics. 2. Imbalanced Backbone Capabilities: The
dominance of pretrained language models in MLLMs leads to overreliance on
language and neglect of visual information. 3. Training Objectives: Current
objectives often fail to promote balanced cross-modal alignment, resulting in
shortcut learning biased toward language. These findings highlight the need for
balanced training strategies and model architectures to better integrate
multiple modalities in MLLMs. We call for interdisciplinary efforts to tackle
these challenges and drive innovation in MLLM research. Our work provides a
fresh perspective on modality bias in MLLMs and offers insights for developing
more robust and generalizable multimodal systems-advancing progress toward
Artificial General Intelligence.

</details>


### [278] [TrajMoE: Spatially-Aware Mixture of Experts for Unified Human Mobility Modeling](https://arxiv.org/abs/2505.18670)
*Chonghua Han,Yuan Yuan,Kaiyan Chen,Jingtao Ding,Yong Li*

Main category: cs.AI

TL;DR: TrajMoE提出了一种统一且可扩展的跨城市人类移动建模方法，通过空间语义编码器和混合专家Transformer解决城市间语义不一致和移动模式多样性的问题，显著提升了模型泛化性能。


<details>
  <summary>Details</summary>
Motivation: 由于城市间空间表示和移动模式的异质性，现有方法难以实现跨城市泛化，通常依赖数值坐标或需训练城市特定模型，限制了可扩展性和迁移性。

Method: 设计空间语义编码器从POI功能语义和访问模式中学习可迁移位置表示，并提出空间感知混合专家Transformer（SAMoE），结合结构化先验和共享专家实现自适应跨城市泛化。

Result: 实验表明，TrajMoE仅需1轮微调即可比竞争模型提升27%，仅用5%目标城市数据即超越全数据基线。

Conclusion: TrajMoE为实现通用、可迁移、可预训练的人类移动基础模型迈出了重要一步。

Abstract: Modeling human mobility across diverse cities is essential for applications
such as urban planning, transportation optimization, and personalized services.
However, generalization remains challenging due to heterogeneous spatial
representations and mobility patterns across cities. Existing methods typically
rely on numerical coordinates or require training city-specific models,
limiting their scalability and transferability. We propose TrajMoE, a unified
and scalable model for cross-city human mobility modeling. TrajMoE addresses
two key challenges: (1) inconsistent spatial semantics across cities, and (2)
diverse urban mobility patterns. To tackle these, we begin by designing a
spatial semantic encoder that learns transferable location representations from
POI-based functional semantics and visit patterns. Furthermore, we design a
Spatially-Aware Mixture-of-Experts (SAMoE) Transformer that injects structured
priors into experts specialized in distinct mobility semantics, along with a
shared expert to capture city-invariant patterns and enable adaptive cross-city
generalization. Extensive experiments demonstrate that TrajMoE achieves up to
27% relative improvement over competitive mobility foundation models after only
one epoch of fine-tuning, and consistently outperforms full-data baselines
using merely 5% of target city data. These results establish TrajMoE as a
significant step toward realizing a truly generalizable, transferable, and
pretrainable foundation model for human mobility.

</details>


### [279] [AI-Driven Climate Policy Scenario Generation for Sub-Saharan Africa](https://arxiv.org/abs/2505.18694)
*Rafiu Adekoya Badekale,Adewale Akinfaderin*

Main category: cs.AI

TL;DR: 该论文探讨了利用生成式AI（特别是大型语言模型）模拟撒哈拉以南非洲气候政策情景的新方法，以替代传统耗时且有限的方法。


<details>
  <summary>Details</summary>
Motivation: 传统的气候政策情景生成和评估方法耗时且难以捕捉能源与气候问题的复杂性，生成式AI能有效解决这些限制。

Method: 使用llama3.2-3B模型生成气候政策情景，并通过自动化技术进行评估，与人类专家和其他LLM模型进行比较。

Result: 生成的34个情景中有30个（88%）通过专家验证，显示出生成式AI能产生连贯、相关、合理且多样化的情景。

Conclusion: 生成式AI为数据受限地区的气候政策规划提供了变革性工具。

Abstract: Climate policy scenario generation and evaluation have traditionally relied
on integrated assessment models (IAMs) and expert-driven qualitative analysis.
These methods enable stakeholders, such as policymakers and researchers, to
anticipate impacts, plan governance strategies, and develop mitigation
measures. However, traditional methods are often time-intensive, reliant on
simple extrapolations of past trends, and limited in capturing the complex and
interconnected nature of energy and climate issues. With the advent of
artificial intelligence (AI), particularly generative AI models trained on vast
datasets, these limitations can be addressed, ensuring robustness even under
limited data conditions. In this work, we explore the novel method that employs
generative AI, specifically large language models (LLMs), to simulate climate
policy scenarios for Sub-Saharan Africa. These scenarios focus on energy
transition themes derived from the historical United Nations Climate Change
Conference (COP) documents. By leveraging generative models, the project aims
to create plausible and diverse policy scenarios that align with regional
climate goals and energy challenges. Given limited access to human evaluators,
automated techniques were employed for scenario evaluation. We generated policy
scenarios using the llama3.2-3B model. Of the 34 generated responses, 30 (88%)
passed expert validation, accurately reflecting the intended impacts provided
in the corresponding prompts. We compared these validated responses against
assessments from a human climate expert and two additional LLMs (gemma2-2B and
mistral-7B). Our structured, embedding-based evaluation framework shows that
generative AI effectively generate scenarios that are coherent, relevant,
plausible, and diverse. This approach offers a transformative tool for climate
policy planning in data-constrained regions.

</details>


### [280] [AI for Regulatory Affairs: Balancing Accuracy, Interpretability, and Computational Cost in Medical Device Classification](https://arxiv.org/abs/2505.18695)
*Yu Han,Aaron Ceross,Jeroen H. M. Bergmann*

Main category: cs.AI

TL;DR: 研究探讨AI在医疗器械监管分类中的应用，评估多种模型在准确性、可解释性和计算成本的表现。


<details>
  <summary>Details</summary>
Motivation: 监管事务处于医学与法律的交叉点，AI自动化可显著提升效率。产品分类是决定市场准入和患者安全的关键步骤。

Method: 使用医疗器械描述数据集，评估传统机器学习、深度学习架构和大语言模型等多种AI模型。

Result: 研究比较了不同模型在准确性、可解释性和计算成本三个维度的表现。

Conclusion: AI模型在监管分类任务中具有潜力，但需平衡准确性、可解释性和计算成本。

Abstract: Regulatory affairs, which sits at the intersection of medicine and law, can
benefit significantly from AI-enabled automation. Classification task is the
initial step in which manufacturers position their products to regulatory
authorities, and it plays a critical role in determining market access,
regulatory scrutiny, and ultimately, patient safety. In this study, we
investigate a broad range of AI models -- including traditional machine
learning (ML) algorithms, deep learning architectures, and large language
models -- using a regulatory dataset of medical device descriptions. We
evaluate each model along three key dimensions: accuracy, interpretability, and
computational cost.

</details>


### [281] [AI-Researcher: Autonomous Scientific Innovation](https://arxiv.org/abs/2505.18705)
*Jiabin Tang,Lianghao Xia,Zhonghang Li,Chao Huang*

Main category: cs.AI

TL;DR: 论文提出AI-Researcher系统，通过大语言模型的推理能力实现全自动科研流程，并开发Scientist-Bench基准验证其接近人类水平的研究能力。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型在数学和编程上的强大推理能力，结合自主代理框架，探索加速科学创新的可能性。

Method: 构建AI-Researcher系统实现从文献综述到论文撰写的全流程自动化，并设计跨领域基准Scientist-Bench进行评估。

Result: 实验表明系统实现率高，生成的科研论文质量接近人类水平，能突破认知限制探索解决方案空间。

Conclusion: 该研究为自主科学创新奠定新基础，可作为人类研究者的补充工具。

Abstract: The powerful reasoning capabilities of Large Language Models (LLMs) in
mathematics and coding, combined with their ability to automate complex tasks
through agentic frameworks, present unprecedented opportunities for
accelerating scientific innovation. In this paper, we introduce AI-Researcher,
a fully autonomous research system that transforms how AI-driven scientific
discovery is conducted and evaluated. Our framework seamlessly orchestrates the
complete research pipeline--from literature review and hypothesis generation to
algorithm implementation and publication-ready manuscript preparation--with
minimal human intervention. To rigorously assess autonomous research
capabilities, we develop Scientist-Bench, a comprehensive benchmark comprising
state-of-the-art papers across diverse AI research domains, featuring both
guided innovation and open-ended exploration tasks. Through extensive
experiments, we demonstrate that AI-Researcher achieves remarkable
implementation success rates and produces research papers that approach
human-level quality. This work establishes new foundations for autonomous
scientific innovation that can complement human researchers by systematically
exploring solution spaces beyond cognitive limitations.

</details>


### [282] [$C^3$-Bench: The Things Real Disturbing LLM based Agent in Multi-Tasking](https://arxiv.org/abs/2505.18746)
*Peijie Yu,Yifan Yang,Jinjian Li,Zelong Zhang,Haorui Wang,Xiao Feng,Feng Zhang*

Main category: cs.AI

TL;DR: 该论文提出了一个名为$C^3$-Bench的开源高质量基准测试，用于评估基于大语言模型的智能代理在处理工具依赖、隐藏信息和动态决策路径时的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前研究通常通过多轮对话评估智能代理，但忽视了工具间关系、环境反馈和先前决策等关键因素对代理行为的影响。为了填补这一空白，作者提出了$C^3$-Bench基准测试。

Method: 设计了三个挑战：导航复杂工具关系、处理关键隐藏信息和管理动态决策路径，并引入了细粒度指标、创新数据收集算法和可复现的评估方法。

Result: 在49个主流代理上的实验表明，代理在处理工具依赖、长上下文信息依赖和频繁策略切换方面存在显著不足。

Conclusion: $C^3$-Bench旨在通过这些挑战暴露模型漏洞，并推动智能代理性能可解释性研究。

Abstract: Agents based on large language models leverage tools to modify environments,
revolutionizing how AI interacts with the physical world. Unlike traditional
NLP tasks that rely solely on historical dialogue for responses, these agents
must consider more complex factors, such as inter-tool relationships,
environmental feedback and previous decisions, when making choices. Current
research typically evaluates agents via multi-turn dialogues. However, it
overlooks the influence of these critical factors on agent behavior. To bridge
this gap, we present an open-source and high-quality benchmark $C^3$-Bench.
This benchmark integrates attack concepts and applies univariate analysis to
pinpoint key elements affecting agent robustness. In concrete, we design three
challenges: navigate complex tool relationships, handle critical hidden
information and manage dynamic decision paths. Complementing these challenges,
we introduce fine-grained metrics, innovative data collection algorithms and
reproducible evaluation methods. Extensive experiments are conducted on 49
mainstream agents, encompassing general fast-thinking, slow-thinking and
domain-specific models. We observe that agents have significant shortcomings in
handling tool dependencies, long context information dependencies and frequent
policy-type switching. In essence, $C^3$-Bench aims to expose model
vulnerabilities through these challenges and drive research into the
interpretability of agent performance. The benchmark is publicly available at
https://github.com/yupeijei1997/C3-Bench.

</details>


### [283] [The Quest for Efficient Reasoning: A Data-Centric Benchmark to CoT Distillation](https://arxiv.org/abs/2505.18759)
*Ruichen Zhang,Rana Muhammad Shahroz Khan,Zhen Tan,Dawei Li,Song Wang,Tianlong Chen*

Main category: cs.AI

TL;DR: DC-CoT是首个系统性评估数据操作对思维链蒸馏影响的基准，通过多维度实验为优化推理模型提供实践指导。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统性评估数据蒸馏方法（如增强、选择、混合）对小型学生LLM推理能力影响的基准，阻碍了高效推理模型的开发。

Method: 提出DC-CoT基准，结合不同教师模型（如Claude-3.5）和学生架构（如7B参数），从方法、模型、数据三维度评估数据操作对IID/OOD泛化和跨域迁移的影响。

Result: 实验揭示了数据操作对学生模型性能的具体影响，并建立了通过数据中心技术优化CoT蒸馏的最佳实践。

Conclusion: 该研究为开发高效推理模型提供了可操作的洞见，相关数据和代码已开源以促进后续研究。

Abstract: Data-centric distillation, including data augmentation, selection, and
mixing, offers a promising path to creating smaller, more efficient student
Large Language Models (LLMs) that retain strong reasoning abilities. However,
there still lacks a comprehensive benchmark to systematically assess the effect
of each distillation approach. This paper introduces DC-CoT, the first
data-centric benchmark that investigates data manipulation in chain-of-thought
(CoT) distillation from method, model and data perspectives. Utilizing various
teacher models (e.g., o4-mini, Gemini-Pro, Claude-3.5) and student
architectures (e.g., 3B, 7B parameters), we rigorously evaluate the impact of
these data manipulations on student model performance across multiple reasoning
datasets, with a focus on in-distribution (IID) and out-of-distribution (OOD)
generalization, and cross-domain transfer. Our findings aim to provide
actionable insights and establish best practices for optimizing CoT
distillation through data-centric techniques, ultimately facilitating the
development of more accessible and capable reasoning models. The dataset can be
found at https://huggingface.co/datasets/rana-shahroz/DC-COT, while our code is
shared in https://anonymous.4open.science/r/DC-COT-FF4C/.

</details>


### [284] [Mitigating Deceptive Alignment via Self-Monitoring](https://arxiv.org/abs/2505.18807)
*Jiaming Ji,Wenqi Chen,Kaile Wang,Donghai Hong,Sitong Fang,Boyuan Chen,Jiayi Zhou,Juntao Dai,Sirui Han,Yike Guo,Yaodong Yang*

Main category: cs.AI

TL;DR: 该论文提出CoT Monitor+框架，通过在大语言模型的思维链推理中嵌入自我监控机制，有效减少43.8%的欺骗行为，同时保持任务准确性。


<details>
  <summary>Details</summary>
Motivation: 现有安全方法仅事后过滤欺骗性输出，无法阻止模型在内部推理时进行欺骗规划。论文旨在探索能否在模型思考过程中实时拦截欺骗行为。

Method: 在思维链生成过程中同步产生（1）常规推理步骤和（2）经过训练的内部自评估信号，该信号作为强化学习的辅助奖励，形成抑制错误策略的反馈循环。

Result: 在DeceptionBench基准测试中，无约束思维链会加剧欺骗倾向，而CoT Monitor+平均减少43.8%欺骗行为且不影响准确率；用自监控信号替代外部弱评估器时，模型能保持更高透明度。

Conclusion: 将自我监控机制嵌入推理过程能有效抑制大语言模型的隐蔽欺骗行为，同时维持原有性能，为AI对齐提供了新思路。

Abstract: Modern large language models rely on chain-of-thought (CoT) reasoning to
achieve impressive performance, yet the same mechanism can amplify deceptive
alignment, situations in which a model appears aligned while covertly pursuing
misaligned goals. Existing safety pipelines treat deception as a black-box
output to be filtered post-hoc, leaving the model free to scheme during its
internal reasoning. We ask: Can deception be intercepted while the model is
thinking? We answer this question, the first framework that embeds a
Self-Monitor inside the CoT process itself, named CoT Monitor+. During
generation, the model produces (i) ordinary reasoning steps and (ii) an
internal self-evaluation signal trained to flag and suppress misaligned
strategies. The signal is used as an auxiliary reward in reinforcement
learning, creating a feedback loop that rewards honest reasoning and
discourages hidden goals. To study deceptive alignment systematically, we
introduce DeceptionBench, a five-category benchmark that probes covert
alignment-faking, sycophancy, etc. We evaluate various LLMs and show that
unrestricted CoT roughly aggravates the deceptive tendency. In contrast, CoT
Monitor+ cuts deceptive behaviors by 43.8% on average while preserving task
accuracy. Further, when the self-monitor signal replaces an external weak judge
in RL fine-tuning, models exhibit substantially fewer obfuscated thoughts and
retain transparency. Our project website can be found at
cot-monitor-plus.github.io

</details>


### [285] [AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting](https://arxiv.org/abs/2505.18822)
*Shijue Huang,Hongru Wang,Wanjun Zhong,Zhaochen Su,Jiazhan Feng,Bowen Cao,Yi R. Fung*

Main category: cs.AI

TL;DR: AdaCtrl框架通过自适应推理预算分配和用户控制，优化大型推理模型的效率与效果。


<details>
  <summary>Details</summary>
Motivation: 现代大型推理模型在解决问题时表现出色，但常因生成过长的推理链而效率低下，无法平衡效率与效果。

Method: 提出AdaCtrl框架，通过两阶段训练（冷启动微调和难度感知强化学习）实现自适应推理预算分配，并设计用户控制接口。

Result: AdaCtrl在多个数据集上显著减少响应长度（10.06%-91.04%），同时提升性能，并支持用户精确控制推理预算。

Conclusion: AdaCtrl有效平衡推理效率与效果，为用户提供灵活控制，适用于不同复杂度的任务。

Abstract: Modern large reasoning models demonstrate impressive problem-solving
capabilities by employing sophisticated reasoning strategies. However, they
often struggle to balance efficiency and effectiveness, frequently generating
unnecessarily lengthy reasoning chains for simple problems. In this work, we
propose AdaCtrl, a novel framework to support both difficulty-aware adaptive
reasoning budget allocation and explicit user control over reasoning depth.
AdaCtrl dynamically adjusts its reasoning length based on self-assessed problem
difficulty, while also allowing users to manually control the budget to
prioritize either efficiency or effectiveness. This is achieved through a
two-stage training pipeline: an initial cold-start fine-tuning phase to instill
the ability to self-aware difficulty and adjust reasoning budget, followed by a
difficulty-aware reinforcement learning (RL) stage that refines the model's
adaptive reasoning strategies and calibrates its difficulty assessments based
on its evolving capabilities during online training. To enable intuitive user
interaction, we design explicit length-triggered tags that function as a
natural interface for budget control. Empirical results show that AdaCtrl
adapts reasoning length based on estimated difficulty, compared to the standard
training baseline that also incorporates fine-tuning and RL, it yields
performance improvements and simultaneously reduces response length by 10.06%
and 12.14% on the more challenging AIME2024 and AIME2025 datasets, which
require elaborate reasoning, and by 62.05% and 91.04% on the MATH500 and GSM8K
datasets, where more concise responses are sufficient. Furthermore, AdaCtrl
enables precise user control over the reasoning budget, allowing for tailored
responses to meet specific needs.

</details>


### [286] [LiteCUA: Computer as MCP Server for Computer-Use Agent on AIOS](https://arxiv.org/abs/2505.18829)
*Kai Mei,Xi Zhu,Hang Gao,Shuhang Lin,Yongfeng Zhang*

Main category: cs.AI

TL;DR: AIOS 1.0是一个新平台，通过环境上下文化提升计算机使用代理（CUA）能力，解决了语言模型与计算机接口间的语义鸿沟问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注构建更强大的代理框架或增强代理模型，但忽视了语言模型与计算机接口结构之间的语义不匹配问题。

Method: AIOS 1.0通过将计算机转化为语言模型可理解的上下文环境，并采用模型上下文协议（MCP）服务器架构来抽象计算机状态和动作。

Result: 基于AIOS 1.0构建的轻量级代理LiteCUA在OSWorld基准测试中取得了14.66%的成功率，优于多个专用代理框架。

Conclusion: 为语言模型提供计算机环境的上下文化是开发更强大计算机使用代理和推进AI与数字系统交互的有前景的方向。

Abstract: We present AIOS 1.0, a novel platform designed to advance computer-use agent
(CUA) capabilities through environmental contextualization. While existing
approaches primarily focus on building more powerful agent frameworks or
enhancing agent models, we identify a fundamental limitation: the semantic
disconnect between how language models understand the world and how computer
interfaces are structured. AIOS 1.0 addresses this challenge by transforming
computers into contextual environments that language models can natively
comprehend, implementing a Model Context Protocol (MCP) server architecture to
abstract computer states and actions. This approach effectively decouples
interface complexity from decision complexity, enabling agents to reason more
effectively about computing environments. To demonstrate our platform's
effectiveness, we introduce LiteCUA, a lightweight computer-use agent built on
AIOS 1.0 that achieves a 14.66% success rate on the OSWorld benchmark,
outperforming several specialized agent frameworks despite its simple
architecture. Our results suggest that contextualizing computer environments
for language models represents a promising direction for developing more
capable computer-use agents and advancing toward AI that can interact with
digital systems. The source code of LiteCUA is available at
https://github.com/agiresearch/LiteCUA, and it is also integrated into the AIOS
main branch as part of AIOS at https://github.com/agiresearch/AIOS.

</details>


### [287] [Signal, Image, or Symbolic: Exploring the Best Input Representation for Electrocardiogram-Language Models Through a Unified Framework](https://arxiv.org/abs/2505.18847)
*William Han,Chaojing Duan,Zhepeng Cen,Yihang Yao,Xiaoyu Song,Atharva Mhaskar,Dylan Leong,Michael A. Rosenberg,Emerson Liu,Ding Zhao*

Main category: cs.AI

TL;DR: 论文探讨了心电图语言模型(ELMs)中最有效的心电图输入表示形式，通过比较三种候选表示（原始时间序列信号、渲染图像和离散符号序列），发现符号表示在多数情况下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型(LLMs)在心电图(ECG)解读中的应用增多，产生了心电图语言模型(ELMs)。然而，在进一步扩展ELMs之前，需要探索最有效的心电图输入表示形式。

Method: 研究比较了三种心电图输入表示形式（原始时间序列信号、渲染图像和离散符号序列），在6个公共数据集和5个评估指标上进行了综合基准测试。

Result: 符号表示在统计上显著优于信号和图像输入，且在LLM主干、ECG持续时间和标记预算等方面表现稳健。

Conclusion: 研究结果为开发下一代ELMs时选择输入表示提供了明确指导，符号表示是最佳选择。

Abstract: Recent advances have increasingly applied large language models (LLMs) to
electrocardiogram (ECG) interpretation, giving rise to
Electrocardiogram-Language Models (ELMs). Conditioned on an ECG and a textual
query, an ELM autoregressively generates a free-form textual response. Unlike
traditional classification-based systems, ELMs emulate expert cardiac
electrophysiologists by issuing diagnoses, analyzing waveform morphology,
identifying contributing factors, and proposing patient-specific action plans.
To realize this potential, researchers are curating instruction-tuning datasets
that pair ECGs with textual dialogues and are training ELMs on these resources.
Yet before scaling ELMs further, there is a fundamental question yet to be
explored: What is the most effective ECG input representation? In recent works,
three candidate representations have emerged-raw time-series signals, rendered
images, and discretized symbolic sequences. We present the first comprehensive
benchmark of these modalities across 6 public datasets and 5 evaluation
metrics. We find symbolic representations achieve the greatest number of
statistically significant wins over both signal and image inputs. We further
ablate the LLM backbone, ECG duration, and token budget, and we evaluate
robustness to signal perturbations. We hope that our findings offer clear
guidance for selecting input representations when developing the next
generation of ELMs.

</details>


### [288] [The Theory of the Unique Latent Pattern: A Formal Epistemic Framework for Structural Singularity in Complex Systems](https://arxiv.org/abs/2505.18850)
*Mohamed Aly Bouke*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper introduces the Theory of the Unique Latent Pattern (ULP), a formal
epistemic framework that redefines the origin of apparent complexity in dynamic
systems. Rather than attributing unpredictability to intrinsic randomness or
emergent nonlinearity, ULP asserts that every analyzable system is governed by
a structurally unique, deterministic generative mechanism, one that remains
hidden not due to ontological indeterminacy, but due to epistemic constraints.
The theory is formalized using a non-universal generative mapping \(
\mathcal{F}_S(P_S, t) \), where each system \( S \) possesses its own latent
structure \( P_S \), irreducible and non-replicable across systems. Observed
irregularities are modeled as projections of this generative map through
observer-limited interfaces, introducing epistemic noise \( \varepsilon_S(t) \)
as a measure of incomplete access. By shifting the locus of uncertainty from
the system to the observer, ULP reframes chaos as a context-relative failure of
representation. We contrast this position with foundational paradigms in chaos
theory, complexity science, and statistical learning. While they assume or
model shared randomness or collective emergence, ULP maintains that every
instance harbors a singular structural identity. Although conceptual, the
theory satisfies the criterion of falsifiability in the Popperian sense, it
invites empirical challenge by asserting that no two systems governed by
distinct latent mechanisms will remain indistinguishable under sufficient
resolution. This opens avenues for structurally individuated models in AI,
behavioral inference, and epistemic diagnostics.

</details>


### [289] [Hierarchical-embedding autoencoder with a predictor (HEAP) as efficient architecture for learning long-term evolution of complex multi-scale physical systems](https://arxiv.org/abs/2505.18857)
*Alexander Khrabry,Edward Startsev,Andrew Powis,Igor Kaganovich*

Main category: cs.AI

TL;DR: 提出了一种基于尺度分离的新型高效架构，用于学习复杂多尺度物理系统中的长期演化，通过分层全卷积自编码器实现多尺度结构建模，显著提升了长期预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以高效建模多尺度物理系统中不同尺度结构的长期演化，尤其是小尺度特征间的远程交互。本文旨在通过尺度分离思想解决这一问题。

Method: 采用分层全卷积自编码器将系统状态编码为多尺度嵌入层，保留空间分辨率信息；通过卷积算子建模跨尺度交互，实现同步推进所有嵌入层的预测。

Result: 在Hasegawa-Wakatani湍流系统中，相比传统ResNet架构，该方法对关键统计特性的长期预测精度实现了数倍提升。

Conclusion: 基于尺度分离的架构能有效建模多尺度系统的长期演化，其分层嵌入和跨尺度卷积交互机制为复杂物理系统预测提供了新思路。

Abstract: We propose a novel efficient architecture for learning long-term evolution in
complex multi-scale physical systems which is based on the idea of separation
of scales. Structures of various scales that dynamically emerge in the system
interact with each other only locally. Structures of similar scale can interact
directly when they are in contact and indirectly when they are parts of larger
structures that interact directly. This enables modeling a multi-scale system
in an efficient way, where interactions between small-scale features that are
apart from each other do not need to be modeled. The hierarchical
fully-convolutional autoencoder transforms the state of a physical system not
just into a single embedding layer, as it is done conventionally, but into a
series of embedding layers which encode structures of various scales preserving
spatial information at a corresponding resolution level. Shallower layers embed
smaller structures on a finer grid, while deeper layers embed larger structures
on a coarser grid. The predictor advances all embedding layers in sync.
Interactions between features of various scales are modeled using a combination
of convolutional operators. We compare the performance of our model to
variations of a conventional ResNet architecture in application to the
Hasegawa-Wakatani turbulence. A multifold improvement in long-term prediction
accuracy was observed for crucial statistical characteristics of this system.

</details>


### [290] [Digital Overconsumption and Waste: A Closer Look at the Impacts of Generative AI](https://arxiv.org/abs/2505.18894)
*Vanessa Utz,Steve DiPaola*

Main category: cs.AI

TL;DR: 生成式AI系统因能耗和碳排放加剧数字垃圾问题，需讨论数字过度消费及其社会影响。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI对气候的负面影响及用户对相关研究的反应，呼吁关注数字过度消费问题。

Method: 基于先前关于商业生成式AI气候影响的研究，扩展讨论数字浪费、社会影响及解决方案。

Result: 生成式AI加剧数字垃圾和碳排放，需采取措施应对数字过度消费。

Conclusion: 需紧急讨论生成式AI的气候和社会影响，并提出可能的解决路径。

Abstract: Generative Artificial Intelligence (AI) systems currently contribute
negatively to the production of digital waste, via the associated energy
consumption and the related CO2 emissions. At this moment, a discussion is
urgently needed on the replication of harmful consumer behavior, such as
overconsumption, in the digital space. We outline our previous work on the
climate implications of commercially available generative AI systems and the
sentiment of generative AI users when confronted with AI-related climate
research. We expand on this work via a discussion of digital overconsumption
and waste, other related societal impacts, and a possible solution pathway

</details>


### [291] [Stronger Enforcement of Instruction Hierarchy via Augmented Intermediate Representations](https://arxiv.org/abs/2505.18907)
*Sanjay Kariyappa,G. Edward Suh*

Main category: cs.AI

TL;DR: 提出一种新方法，通过在模型中间层注入指令层级信号，显著降低提示注入攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有防御机制仅在输入层注入指令层级信号，限制了其在模型各层中区分令牌权限的能力。

Method: 在网络中间令牌表示中注入层特定的可训练嵌入，编码权限信息。

Result: 相比现有方法，梯度基提示注入攻击成功率降低1.6至9.2倍，且不影响模型实用性。

Conclusion: 中间层注入指令层级信号能更有效防御提示注入攻击，同时保持模型性能。

Abstract: Prompt injection attacks are a critical security vulnerability in large
language models (LLMs), allowing attackers to hijack model behavior by
injecting malicious instructions within the input context. Recent defense
mechanisms have leveraged an Instruction Hierarchy (IH) Signal, often
implemented through special delimiter tokens or additive embeddings to denote
the privilege level of input tokens. However, these prior works typically
inject the IH signal exclusively at the initial input layer, which we
hypothesize limits its ability to effectively distinguish the privilege levels
of tokens as it propagates through the different layers of the model. To
overcome this limitation, we introduce a novel approach that injects the IH
signal into the intermediate token representations within the network. Our
method augments these representations with layer-specific trainable embeddings
that encode the privilege information. Our evaluations across multiple models
and training methods reveal that our proposal yields between $1.6\times$ and
$9.2\times$ reduction in attack success rate on gradient-based prompt injection
attacks compared to state-of-the-art methods, without significantly degrading
the model's utility.

</details>


### [292] [Meta-aware Learning in text-to-SQL Large Language Model](https://arxiv.org/abs/2505.18929)
*Wenda Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种元感知学习框架，通过整合领域知识、数据库模式、思维链推理和元数据关系，提升大语言模型在文本到SQL任务中的生成质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的发展为文本到SQL任务提供了机会，但理解和处理复杂领域信息及数据库结构仍是挑战。本文旨在通过整合多种学习策略提升SQL生成质量。

Method: 提出了一个包含四种学习策略的元感知学习框架：基于模式的学习、思维链学习、知识增强学习和关键信息标记化。

Result: 实验研究表明，该方法在执行准确性、多任务SQL生成能力和减少灾难性遗忘方面表现优越。

Conclusion: 该框架通过微调提升了大语言模型对数据库结构和元数据的理解能力，从而在业务领域中显著提高了SQL生成的质量。

Abstract: The advancements of Large language models (LLMs) have provided great
opportunities to text-to-SQL tasks to overcome the main challenges to
understand complex domain information and complex database structures in
business applications. In this paper, we propose a meta-aware learning
framework to integrate domain knowledge, database schema, chain-of-thought
reasoning processes, and metadata relationships to improve the SQL generation
quality. The proposed framework includes four learning strategies: schema-based
learning, Chain-of-Thought (CoT) learning, knowledge-enhanced learning, and key
information tokenization. This approach provides a comprehensive understanding
of database structure and metadata information towards LLM through fine-tuning
to improve its performance on SQL generation within business domains. Through
two experimental studies, we have demonstrated the superiority of the proposed
methods in execution accuracy, multi-task SQL generation capability, and
reduction of catastrophic forgetting.

</details>


### [293] [Can Large Language Models Infer Causal Relationships from Real-World Text?](https://arxiv.org/abs/2505.18931)
*Ryan Saklad,Aman Chadha,Oleg Pavlov,Raha Moraffah*

Main category: cs.AI

TL;DR: 该论文研究了大型语言模型（LLMs）从真实世界文本中推断因果关系的能力，提出了首个基于真实学术文献的基准测试，并揭示了模型在此任务上的主要挑战。


<details>
  <summary>Details</summary>
Motivation: 理解文本中的因果关系是人类认知的核心，也是推动大型语言模型（LLMs）向通用人工智能发展的关键。现有研究主要关注合成文本中的简单因果关系，未能反映真实世界任务的复杂性。

Method: 作者开发了一个基于真实世界学术文献的基准测试，涵盖不同长度、关系复杂性（明确性、事件数量和因果关系）以及领域的多样化文本。

Result: 实验表明，当前最先进的LLMs在此任务上表现不佳，最佳模型的平均F1得分仅为0.477。主要问题包括处理隐含信息、区分相关因果因素与上下文细节，以及连接分散在长文本中的因果相关信息。

Conclusion: 通过系统性地描述这些不足，该基准测试为未来提升LLMs因果推理能力的研究提供了有针对性的见解。

Abstract: Understanding and inferring causal relationships from texts is a core aspect
of human cognition and is essential for advancing large language models (LLMs)
towards artificial general intelligence. Existing work primarily focuses on
synthetically generated texts which involve simple causal relationships
explicitly mentioned in the text. This fails to reflect the complexities of
real-world tasks. In this paper, we investigate whether LLMs are capable of
inferring causal relationships from real-world texts. We develop a benchmark
drawn from real-world academic literature which includes diverse texts with
respect to length, complexity of relationships (different levels of
explicitness, number of events, and causal relationships), and domains and
sub-domains. To the best of our knowledge, our benchmark is the first-ever
real-world dataset for this task. Our experiments on state-of-the-art LLMs
evaluated on our proposed benchmark demonstrate significant challenges, with
the best-performing model achieving an average F1 score of only 0.477. Analysis
reveals common pitfalls: difficulty with implicitly stated information, in
distinguishing relevant causal factors from surrounding contextual details, and
with connecting causally relevant information spread across lengthy textual
passages. By systematically characterizing these deficiencies, our benchmark
offers targeted insights for further research into advancing LLM causal
reasoning.

</details>


### [294] [REACT: Representation Extraction And Controllable Tuning to Overcome Overfitting in LLM Knowledge Editing](https://arxiv.org/abs/2505.18933)
*Haitian Zhong,Yuhuan Liu,Ziyang Xu,Guofan Liu,Qiang Liu,Shu Wu,Zhe Zhao,Liang Wang,Tieniu Tan*

Main category: cs.AI

TL;DR: REACT框架通过两阶段方法解决大语言模型编辑中的过拟合问题，显著提升了编辑的精确性和可控性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型编辑方法常出现过拟合问题，导致事实更新超出预期范围。为解决这一问题，本文提出了REACT框架。

Method: REACT分为两阶段：第一阶段提取潜在事实表示并计算方向性“信念偏移”向量；第二阶段通过可控扰动调整隐藏状态，仅在上下文必要时进行编辑。

Result: 在EVOKE基准测试中，REACT显著减少了过拟合，同时在COUNTERFACT和MQuAKE测试中保持了基本的编辑性能。

Conclusion: REACT框架有效解决了大语言模型编辑中的过拟合问题，提升了编辑的精确性和可控性。

Abstract: Large language model editing methods frequently suffer from overfitting,
wherein factual updates can propagate beyond their intended scope,
overemphasizing the edited target even when it's contextually inappropriate. To
address this challenge, we introduce REACT (Representation Extraction And
Controllable Tuning), a unified two-phase framework designed for precise and
controllable knowledge editing. In the initial phase, we utilize tailored
stimuli to extract latent factual representations and apply Principal Component
Analysis with a simple learnbale linear transformation to compute a directional
"belief shift" vector for each instance. In the second phase, we apply
controllable perturbations to hidden states using the obtained vector with a
magnitude scalar, gated by a pre-trained classifier that permits edits only
when contextually necessary. Relevant experiments on EVOKE benchmarks
demonstrate that REACT significantly reduces overfitting across nearly all
evaluation metrics, and experiments on COUNTERFACT and MQuAKE shows that our
method preserves balanced basic editing performance (reliability, locality, and
generality) under diverse editing scenarios.

</details>


### [295] [SANNet: A Semantic-Aware Agentic AI Networking Framework for Multi-Agent Cross-Layer Coordination](https://arxiv.org/abs/2505.18946)
*Yong Xiao,Haoran Zhou,Xubo Li,Yayu Gao,Guangming Shi,Ping Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种新型语义感知的Agentic AI网络架构SANNet，通过动态权重机制解决多代理协作中的目标冲突问题，并在5G核心平台上验证了其性能提升。


<details>
  <summary>Details</summary>
Motivation: AgentNet作为一种新型AI原生网络范式，虽具备自主决策和环境适应潜力，但目前缺乏支持自动目标发现和多代理自协调的有效框架，且存在代理间目标冲突的挑战。

Method: 提出SANNet架构，包含语义目标推断、分层代理自动分配机制，以及基于动态权重的冲突解决算法，并在开放无线接入网（O-RAN）和5GS核心平台上实现硬件原型。

Result: 实验证明SANNet能显著提升多代理网络系统性能，即使代理存在目标冲突时，仍能保证协作效率，并在动态环境中提供理论性能保障。

Conclusion: SANNet为未来完全自主网络系统奠定了基础，通过语义感知和冲突解决机制，实现了多代理在复杂环境中的高效协作。

Abstract: Agentic AI networking (AgentNet) is a novel AI-native networking paradigm
that relies on a large number of specialized AI agents to collaborate and
coordinate for autonomous decision-making, dynamic environmental adaptation,
and complex goal achievement. It has the potential to facilitate real-time
network management alongside capabilities for self-configuration,
self-optimization, and self-adaptation across diverse and complex networking
environments, laying the foundation for fully autonomous networking systems in
the future. Despite its promise, AgentNet is still in the early stage of
development, and there still lacks an effective networking framework to support
automatic goal discovery and multi-agent self-orchestration and task
assignment. This paper proposes SANNet, a novel semantic-aware agentic AI
networking architecture that can infer the semantic goal of the user and
automatically assign agents associated with different layers of a mobile system
to fulfill the inferred goal. Motivated by the fact that one of the major
challenges in AgentNet is that different agents may have different and even
conflicting objectives when collaborating for certain goals, we introduce a
dynamic weighting-based conflict-resolving mechanism to address this issue. We
prove that SANNet can provide theoretical guarantee in both conflict-resolving
and model generalization performance for multi-agent collaboration in dynamic
environment. We develop a hardware prototype of SANNet based on the open RAN
and 5GS core platform. Our experimental results show that SANNet can
significantly improve the performance of multi-agent networking systems, even
when agents with conflicting objectives are selected to collaborate for the
same goal.

</details>


### [296] [Co-PatcheR: Collaborative Software Patching with Component(s)-specific Small Reasoning Models](https://arxiv.org/abs/2505.18955)
*Yuheng Tang,Hongwei Li,Kaijie Zhu,Michael Yang,Yangruibo Ding,Wenbo Guo*

Main category: cs.AI

TL;DR: 论文提出Co-PatcheR，首个协作式软件补丁系统，通过小型专用模型分工合作，以更少资源实现更高修复率。


<details>
  <summary>Details</summary>
Motivation: 现有单一模型处理软件补丁全流程（定位、生成、验证）效果有限，因各子任务需求差异大。当前70B参数的SOTA模型在SWE-bench-Verified上仅达41%修复率。

Method: 1) 训练专用模型分治任务：双步骤定位可疑代码行+生成与批判结合的补丁生成；2) 混合验证：双模型分别生成含/不含断言的测试用例，多数表决选择正确补丁。

Result: 仅用3个14B模型即在SWE-bench-Verified达到46%修复率，成为专用模型中效果最佳、资源需求最低的方案。

Conclusion: 协作式小模型分工策略显著优于单一大型模型，为软件修复任务提供高效新范式。

Abstract: Motivated by the success of general-purpose large language models (LLMs) in
software patching, recent works started to train specialized patching models.
Most works trained one model to handle the end-to-end patching pipeline
(including issue localization, patch generation, and patch validation).
However, it is hard for a small model to handle all tasks, as different
sub-tasks have different workflows and require different expertise. As such, by
using a 70 billion model, SOTA methods can only reach up to 41% resolved rate
on SWE-bench-Verified. Motivated by the collaborative nature, we propose
Co-PatcheR, the first collaborative patching system with small and specialized
reasoning models for individual components. Our key technique novelties are the
specific task designs and training recipes. First, we train a model for
localization and patch generation. Our localization pinpoints the suspicious
lines through a two-step procedure, and our generation combines patch
generation and critique. We then propose a hybrid patch validation that
includes two models for crafting issue-reproducing test cases with and without
assertions and judging patch correctness, followed by a majority vote-based
patch selection. Through extensive evaluation, we show that Co-PatcheR achieves
46% resolved rate on SWE-bench-Verified with only 3 x 14B models. This makes
Co-PatcheR the best patcher with specialized models, requiring the least
training resources and the smallest models. We conduct a comprehensive ablation
study to validate our recipes, as well as our choice of training data number,
model size, and testing-phase scaling strategy.

</details>


### [297] [Weaver: Interweaving SQL and LLM for Table Reasoning](https://arxiv.org/abs/2505.18961)
*Rohit Khoja,Devanshu Gupta,Yanjie Fu,Dan Roth,Vivek Gupta*

Main category: cs.AI

TL;DR: Weaver是一个动态整合SQL和LLM的模块化流程，用于表格问答任务，通过分解复杂查询提高准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统SQL难以处理表格中的非结构化数据（如文本或图像），而大型语言模型（LLM）在处理长输入序列时存在局限。现有结合SQL和LLM的方法通常依赖固定流程，难以适应复杂查询需求。

Method: 提出Weaver框架，通过生成灵活的逐步计划，结合SQL进行结构化数据检索和LLM进行语义处理，将复杂查询分解为可管理的子任务。

Result: 实验表明，Weaver在四个TableQA数据集上均优于现有方法，同时减少了API调用和错误率。

Conclusion: Weaver通过动态整合SQL和LLM，显著提升了表格问答任务的性能和适应性。

Abstract: Querying tables with unstructured data is challenging due to the presence of
text (or image), either embedded in the table or in external paragraphs, which
traditional SQL struggles to process, especially for tasks requiring semantic
reasoning. While Large Language Models (LLMs) excel at understanding context,
they face limitations with long input sequences. Existing approaches that
combine SQL and LLMs typically rely on rigid, predefined work-flows, limiting
their adaptability to complex queries. To address these issues, we introduce
Weaver , a modular pipeline that dynamically integrates SQL and LLMs for
table-based question answering (TableQA). Weaver generates a flexible,
step-by-step plan that combines SQL for structured data retrieval with LLMs for
semantic processing. By decomposing complex queries into manageable subtasks,
Weaver improves accuracy and generalization. Our experiments show that Weaver
consistently outperforms state-of-the-art methods across four TableQA datasets,
reducing both API calls and error rates.

</details>


### [298] [Aligning LLM with human travel choices: a persona-based embedding learning approach](https://arxiv.org/abs/2505.19003)
*Tianming Liu,Manzi Li,Yafeng Yin*

Main category: cs.AI

TL;DR: 该论文提出了一种新框架，通过角色推断和加载过程，使大语言模型（LLMs）与人类旅行选择行为对齐，显著提升了旅行需求建模的预测性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）为旅行需求建模提供了新机遇，但其与人类行为的不一致性限制了应用。现有对齐方法在典型旅行数据约束下效率低或不实用。

Method: 框架包含角色推断和加载过程：从经验数据推断基础角色，通过行为嵌入驱动的加载函数生成提示词，优化LLMs的行为对齐。

Result: 在Swissmetro数据集上的实验表明，该方法在预测整体和个体选择结果上显著优于基线模型和传统LLM模拟，并能生成可解释的群体行为参数。

Conclusion: 该研究为LLMs在旅行需求建模中的应用提供了更灵活、可解释且资源高效的解决方案，推动了LLMs在该领域的实践整合。

Abstract: The advent of large language models (LLMs) presents new opportunities for
travel demand modeling. However, behavioral misalignment between LLMs and
humans presents obstacles for the usage of LLMs, and existing alignment methods
are frequently inefficient or impractical given the constraints of typical
travel demand data. This paper introduces a novel framework for aligning LLMs
with human travel choice behavior, tailored to the current travel demand data
sources. Our framework uses a persona inference and loading process to
condition LLMs with suitable prompts to enhance alignment. The inference step
establishes a set of base personas from empirical data, and a learned persona
loading function driven by behavioral embeddings guides the loading process. We
validate our framework on the Swissmetro mode choice dataset, and the results
show that our proposed approach significantly outperformed baseline choice
models and LLM-based simulation models in predicting both aggregate mode choice
shares and individual choice outcomes. Furthermore, we showcase that our
framework can generate insights on population behavior through interpretable
parameters. Overall, our research offers a more adaptable, interpretable, and
resource-efficient pathway to robust LLM-based travel behavior simulation,
paving the way to integrate LLMs into travel demand modeling practice in the
future.

</details>


### [299] [RECAST: Strengthening LLMs' Complex Instruction Following with Constraint-Verifiable Data](https://arxiv.org/abs/2505.19030)
*Wenhao Liu,Zhengkang Guo,Mingchen Xie,Jingwen Xu,Zisu Huang,Muzhao Tian,Jianhan Xu,Muling Wu,Xiaohua Wang,Changze Lv,He-Da Wang,Hu Yao,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.AI

TL;DR: 论文提出RECAST框架，通过合成包含大量约束的数据集（RECAST-30K）来提升大语言模型处理复杂指令的能力，实验显示微调后模型性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型应用扩展和用户提示词复杂度提升，模型在处理超过10条约束的复杂指令时表现不佳，需要新的解决方案。

Method: 提出RECAST框架：1) 从真实提示-响应对提取约束构建数据集 2) 使用规则验证定量约束、LLM验证定性约束 3) 创建含30k样本的RECAST-30K数据集

Result: 实验表明：1) 在RECAST-30K上微调的模型处理复杂指令能力显著提升 2) 可验证性支持设计强化学习奖励函数，进一步优化性能

Conclusion: RECAST框架通过结构化约束验证和大规模数据集，有效提升LLMs处理复杂任务的能力，为指令跟随性能优化提供新途径。

Abstract: Large language models (LLMs) are increasingly expected to tackle complex
tasks, driven by their expanding applications and users' growing proficiency in
crafting sophisticated prompts. However, as the number of explicitly stated
requirements increases (particularly more than 10 constraints), LLMs often
struggle to accurately follow such complex instructions. To address this
challenge, we propose RECAST, a novel framework for synthesizing datasets where
each example incorporates far more constraints than those in existing
benchmarks. These constraints are extracted from real-world prompt-response
pairs to ensure practical relevance. RECAST enables automatic verification of
constraint satisfaction via rule-based validators for quantitative constraints
and LLM-based validators for qualitative ones. Using this framework, we
construct RECAST-30K, a large-scale, high-quality dataset comprising 30k
instances spanning 15 constraint types. Experimental results demonstrate that
models fine-tuned on RECAST-30K show substantial improvements in following
complex instructions. Moreover, the verifiability provided by RECAST enables
the design of reward functions for reinforcement learning, which further boosts
model performance on complex and challenging tasks.

</details>


### [300] [Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for Frozen LLMs](https://arxiv.org/abs/2505.19075)
*Jaemin Kim,Hangeol Chang,Hyunmin Hwang,Choonghan Kim,Jong Chul Ye*

Main category: cs.AI

TL;DR: 论文提出了一种名为UniR的通用推理模块，可即插即用地增强任意冻结大语言模型的推理能力，避免传统微调方法的高计算成本与架构依赖问题。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型增强推理能力需消耗大量计算资源且可能损害泛化性，而参数高效微调方法又受限于架构依赖性。需要一种轻量、可组合的通用解决方案。

Method: UniR将奖励分解为独立训练的可组合推理模块，通过预定义奖励将轨迹级信号转为词元级指导，推理时只需将其输出logits与主干模型相加。

Result: 在数学推理和机器翻译任务上超越基线方法，并展示出强的小模型到大模型的弱到强泛化能力。

Conclusion: UniR为LLMs提供了一种不损害核心能力、低成本且可适配的推理增强方案，支持模块化组合实现复杂推理。

Abstract: Large Language Models (LLMs) have demonstrated remarkable general
capabilities, but enhancing skills such as reasoning often demands substantial
computational resources and may compromise their generalization. While
Parameter-Efficient Fine-Tuning (PEFT) methods offer a more resource-conscious
alternative, they typically requires retraining for each LLM backbone due to
architectural dependencies. To address these challenges, here we propose
Universal Reasoner (UniR) - a single, lightweight, composable, and
plug-and-play reasoning module that can be used with any frozen LLM to endow it
with specialized reasoning capabilities. Specifically, UniR decomposes the
reward into a standalone reasoning module that is trained independently using
predefined rewards, effectively translating trajectory-level signals into
token-level guidance. Once trained, UniR can be combined with any frozen LLM at
inference time by simply adding its output logits to those of the LLM backbone.
This additive structure naturally enables modular composition: multiple UniR
modules trained for different tasks can be jointly applied by summing their
logits, enabling complex reasoning via composition. Experimental results on
mathematical reasoning and machine translation tasks show that UniR
significantly outperforms \add{existing baseline fine-tuning methods using the
Llama3.2 model}. Furthermore, UniR demonstrates strong weak-to-strong
generalization: reasoning modules trained on smaller models effectively guide
much larger LLMs. This makes UniR a cost-efficient, adaptable, and robust
solution for enhancing reasoning in LLMs without compromising their core
capabilities. Code is open-sourced at https://github.com/hangeol/UniR

</details>


### [301] [Reinforced Latent Reasoning for LLM-based Recommendation](https://arxiv.org/abs/2505.19092)
*Yang Zhang,Wenxin Xu,Xiaoyan Zhao,Wenjie Wang,Fuli Feng,Xiangnan He,Tat-Seng Chua*

Main category: cs.AI

TL;DR: 该论文提出了一种名为LatentR³的新方法，通过强化学习优化潜在推理，避免了传统推荐系统中显式思维链数据的需求，提高了推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLMs）的推荐系统通常依赖显式思维链（CoT）数据进行微调，但存在高质量CoT数据难以获取和推理延迟高的问题。

Method: LatentR³采用两阶段训练策略：首先通过监督微调初始化潜在推理模块，然后通过强化学习（基于改进的GRPO算法）进行优化，无需依赖CoT数据。

Result: 实验表明，LatentR³能够在不直接监督推理过程的情况下实现有效的潜在推理，显著提升了不同基于LLM的推荐方法的性能。

Conclusion: LatentR³通过潜在推理和强化学习的结合，解决了传统推荐系统中显式思维链数据的局限性，为高效推理提供了新思路。

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning
capabilities in complex problem-solving tasks, sparking growing interest in
their application to preference reasoning in recommendation systems. Existing
methods typically rely on fine-tuning with explicit chain-of-thought (CoT)
data. However, these methods face significant practical limitations due to (1)
the difficulty of obtaining high-quality CoT data in recommendation and (2) the
high inference latency caused by generating CoT reasoning. In this work, we
explore an alternative approach that shifts from explicit CoT reasoning to
compact, information-dense latent reasoning. This approach eliminates the need
for explicit CoT generation and improves inference efficiency, as a small set
of latent tokens can effectively capture the entire reasoning process. Building
on this idea, we propose $\textit{\underline{R}einforced \underline{Latent}
\underline{R}easoning for \underline{R}ecommendation}$ (LatentR$^3$), a novel
end-to-end training framework that leverages reinforcement learning (RL) to
optimize latent reasoning without relying on any CoT data.LatentR$^3$ adopts a
two-stage training strategy: first, supervised fine-tuning to initialize the
latent reasoning module, followed by pure RL training to encourage exploration
through a rule-based reward design. Our RL implementation is based on a
modified GRPO algorithm, which reduces computational overhead during training
and introduces continuous reward signals for more efficient learning. Extensive
experiments demonstrate that LatentR$^3$ enables effective latent reasoning
without any direct supervision of the reasoning process, significantly
improving performance when integrated with different LLM-based recommendation
methods. Our codes are available at https://anonymous.4open.science/r/R3-A278/.

</details>


### [302] [ScreenExplorer: Training a Vision-Language Model for Diverse Exploration in Open GUI World](https://arxiv.org/abs/2505.19095)
*Runliang Niu,Jinglong Ji,Yi Chang,Qi Wang*

Main category: cs.AI

TL;DR: 论文提出ScreenExplorer，一种通过GRPO在动态GUI环境中训练的VLM，结合世界模型好奇心奖励和经验蒸馏，提升开放环境中的探索能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM或VLM的GUI智能体在新环境中泛化能力不足，且依赖人工标注数据。

Method: 采用GRPO训练VLM，引入世界模型好奇心奖励函数，并通过经验蒸馏增强探索能力。

Result: 训练后的模型在开放GUI环境中展现出更好的适应性和持续探索能力。

Conclusion: 该方法为复杂交互环境中具有自我改进能力的AGI系统提供了可扩展路径。

Abstract: The rapid progress of large language models (LLMs) has sparked growing
interest in building Artificial General Intelligence (AGI) within Graphical
User Interface (GUI) environments. However, existing GUI agents based on LLMs
or vision-language models (VLMs) often fail to generalize to novel environments
and rely heavily on manually curated, diverse datasets. To overcome these
limitations, we introduce ScreenExplorer, a VLM trained via Group Relative
Policy Optimization(GRPO) in real, dynamic, and open-ended GUI environments.
Innovatively, we introduced a world-model-based curiosity reward function to
help the agent overcome the cold-start phase of exploration. Additionally,
distilling experience streams further enhances the model's exploration
capabilities. Our training framework enhances model exploration in open GUI
environments, with trained models showing better environmental adaptation and
sustained exploration compared to static deployment models. Our findings offer
a scalable pathway toward AGI systems with self-improving capabilities in
complex interactive settings.

</details>


### [303] [SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics Reasoning](https://arxiv.org/abs/2505.19099)
*Kun Xiang,Heng Li,Terry Jingchen Zhang,Yinya Huang,Zirong Liu,Peixin Qu,Jixi He,Jiaqi Chen,Yu-Jie Yuan,Jianhua Han,Hang Xu,Hanhui Li,Mrinmaya Sachan,Xiaodan Liang*

Main category: cs.AI

TL;DR: SeePhys是一个大规模多模态基准测试，用于评估LLM在物理问题上的推理能力，涵盖从中学到博士资格考试的范围，重点考察视觉信息提取能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在视觉理解能力上存在不足，特别是在结合图表解释与物理推理方面。为了评估和推动这一领域的发展，作者提出了SeePhys基准测试。

Method: SeePhys基准测试覆盖7个物理学基础领域，包含21类高度异质的图表，其中75%的问题必须依赖视觉信息才能正确解答。

Result: 即使最先进的视觉推理模型（如Gemini-2.5-pro和o4-mini）在该基准测试上的准确率也不足60%，显示出当前模型在视觉理解上的根本性挑战。

Conclusion: 当前大型语言模型在视觉信息提取与物理推理的耦合上存在明显不足，且过度依赖文本线索作为认知捷径。

Abstract: We present SeePhys, a large-scale multimodal benchmark for LLM reasoning
grounded in physics questions ranging from middle school to PhD qualifying
exams. The benchmark covers 7 fundamental domains spanning the physics
discipline, incorporating 21 categories of highly heterogeneous diagrams. In
contrast to prior works where visual elements mainly serve auxiliary purposes,
our benchmark features a substantial proportion of vision-essential problems
(75\%) that mandate visual information extraction for correct solutions.
Through extensive evaluation, we observe that even the most advanced visual
reasoning models (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60\% accuracy
on our benchmark. These results reveal fundamental challenges in current large
language models' visual understanding capabilities, particularly in: (i)
establishing rigorous coupling between diagram interpretation and physics
reasoning, and (ii) overcoming their persistent reliance on textual cues as
cognitive shortcuts.

</details>


### [304] [OrgAccess: A Benchmark for Role Based Access Control in Organization Scale LLMs](https://arxiv.org/abs/2505.19165)
*Debdeep Sanyal Umakanta Maharana,Yash Sinha,Hong Ming Tan,Shirish Karande,Mohan Kankanhalli,Murari Mandal*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLMs）在企业环境中理解并遵循基于角色的访问控制（RBAC）和组织层级的能力，发现即使最先进的模型在此任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估LLMs是否能在复杂的组织层级和权限约束下可靠运作，这一能力在实际企业应用中至关重要但尚未充分探索。

Method: 作者提出了一个名为OrgAccess的合成基准测试，包含40种常见权限类型，并设计了不同难度的测试用例（单权限、三权限和五权限组合）来评估LLMs的表现。

Result: 研究结果显示，即使是GPT-4.1在最难的测试用例上也仅获得0.27的F1分数，表明LLMs在遵循复杂规则和组合推理方面存在显著局限性。

Conclusion: 结论指出LLMs在结构化环境中的实际应用能力存在重大缺陷，这为评估其适用性开辟了新方向。

Abstract: Role-based access control (RBAC) and hierarchical structures are foundational
to how information flows and decisions are made within virtually all
organizations. As the potential of Large Language Models (LLMs) to serve as
unified knowledge repositories and intelligent assistants in enterprise
settings becomes increasingly apparent, a critical, yet under explored,
challenge emerges: \textit{can these models reliably understand and operate
within the complex, often nuanced, constraints imposed by organizational
hierarchies and associated permissions?} Evaluating this crucial capability is
inherently difficult due to the proprietary and sensitive nature of real-world
corporate data and access control policies. We introduce a synthetic yet
representative \textbf{OrgAccess} benchmark consisting of 40 distinct types of
permissions commonly relevant across different organizational roles and levels.
We further create three types of permissions: 40,000 easy (1 permission),
10,000 medium (3-permissions tuple), and 20,000 hard (5-permissions tuple) to
test LLMs' ability to accurately assess these permissions and generate
responses that strictly adhere to the specified hierarchical rules,
particularly in scenarios involving users with overlapping or conflicting
permissions. Our findings reveal that even state-of-the-art LLMs struggle
significantly to maintain compliance with role-based structures, even with
explicit instructions, with their performance degrades further when navigating
interactions involving two or more conflicting permissions. Specifically, even
\textbf{GPT-4.1 only achieves an F1-Score of 0.27 on our hardest benchmark}.
This demonstrates a critical limitation in LLMs' complex rule following and
compositional reasoning capabilities beyond standard factual or STEM-based
benchmarks, opening up a new paradigm for evaluating their fitness for
practical, structured environments.

</details>


### [305] [Amplifying Human Creativity and Problem Solving with AI Through Generative Collective Intelligence](https://arxiv.org/abs/2505.19167)
*Thomas P. Kehler,Scott E. Page,Alex Pentland,Martin Reeves,John Seely Brown*

Main category: cs.AI

TL;DR: 提出名为'生成性集体智能(GCI)'的新框架，通过人机协作结合人类创造力与AI计算能力，解决复杂社会问题。


<details>
  <summary>Details</summary>
Motivation: 传统纯算法方法在解决问题和决策中存在局限，需构建人机协同的新模式以突破沟通障碍。

Method: 基于比较判断和最小后悔原则建立数学框架，使AI在群体层面担任交互代理和知识管理双重角色。

Result: GCI在气候适应、医疗改革和公民参与等领域展现出应用潜力。

Conclusion: GCI为人机协作提供了超越单一方能力限制的创新范式，能有效应对复杂社会挑战。

Abstract: We propose a new framework for human-AI collaboration that amplifies the
distinct capabilities of both. This framework, which we call Generative
Collective Intelligence (GCI), shifts AI to the group/social level and employs
AI in dual roles: as interactive agents and as technology that accumulates,
organizes, and leverages knowledge. By creating a cognitive bridge between
human reasoning and AI models, GCI can overcome the limitations of purely
algorithmic approaches to problem-solving and decision-making. The framework
demonstrates how AI can be reframed as a social and cultural technology that
enables groups to solve complex problems through structured collaboration that
transcends traditional communication barriers. We describe the mathematical
foundations of GCI based on comparative judgment and minimum regret principles,
and illustrate its applications across domains including climate adaptation,
healthcare transformation, and civic participation. By combining human
creativity with AI's computational capabilities, GCI offers a promising
approach to addressing complex societal challenges that neither human or
machines can solve alone.

</details>


### [306] [Investigating Pedagogical Teacher and Student LLM Agents: Genetic Adaptation Meets Retrieval Augmented Generation Across Learning Style](https://arxiv.org/abs/2505.19173)
*Debdeep Sanyal,Agniva Maiti,Umakanta Maharana,Dhruv Kumar,Ankur Mali,C. Lee Giles,Murari Mandal*

Main category: cs.AI

TL;DR: 该论文提出了一种结合LLM的异构学生代理与自优化教师代理的新型教学模拟框架，通过遗传算法动态优化教学策略，并引入Persona-RAG模块实现个性化学习，为适应性教学实践提供了数据驱动的测试平台。


<details>
  <summary>Details</summary>
Motivation: 当前教学模拟框架存在两大局限：学生模型静态化且缺乏教师适应性反馈机制。为弥补这些不足，研究者旨在开发能模拟真实教育场景的动态教学系统。

Method: 1) 构建LLM驱动的异构学生代理群 2) 采用遗传算法实现教师代理策略动态进化 3) 设计Persona-RAG模块实现个性化知识检索

Result: 实验表明该框架能产生可解释的教学模式，在保持检索精度的同时显著提升个性化水平，有效模拟多样化学生群体的教学场景。

Conclusion: LLM驱动的教学模拟框架为适应性教学策略研究提供了可控实验环境，具有指导教师培训和实践的潜在价值。

Abstract: Effective teaching requires adapting instructional strategies to accommodate
the diverse cognitive and behavioral profiles of students, a persistent
challenge in education and teacher training. While Large Language Models (LLMs)
offer promise as tools to simulate such complex pedagogical environments,
current simulation frameworks are limited in two key respects: (1) they often
reduce students to static knowledge profiles, and (2) they lack adaptive
mechanisms for modeling teachers who evolve their strategies in response to
student feedback. To address these gaps, \textbf{we introduce a novel
simulation framework that integrates LLM-based heterogeneous student agents
with a self-optimizing teacher agent}. The teacher agent's pedagogical policy
is dynamically evolved using a genetic algorithm, allowing it to discover and
refine effective teaching strategies based on the aggregate performance of
diverse learners. In addition, \textbf{we propose Persona-RAG}, a Retrieval
Augmented Generation module that enables student agents to retrieve knowledge
tailored to their individual learning styles. Persona-RAG preserves the
retrieval accuracy of standard RAG baselines while enhancing personalization,
an essential factor in modeling realistic educational scenarios. Through
extensive experiments, we demonstrate how our framework supports the emergence
of distinct and interpretable teaching patterns when interacting with varied
student populations. Our results highlight the potential of LLM-driven
simulations to inform adaptive teaching practices and provide a testbed for
training human educators in controlled, data-driven environments.

</details>


### [307] [CardioCoT: Hierarchical Reasoning for Multimodal Survival Analysis](https://arxiv.org/abs/2505.19195)
*Shaohao Rui,Haoyang Su,Jinyi Xiang,Lian-Ming Wu,Xiaosong Wang*

Main category: cs.AI

TL;DR: 提出CardioCoT框架，通过两阶段分层推理增强生存分析，提升心梗患者MACE复发风险预测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视临床实践中对中间稳健推理和模型可解释性的需求，且端到端风险预测面临数据和建模复杂度挑战。

Method: 第一阶段使用证据增强自优化机制引导LLM/VLM生成分层推理轨迹，第二阶段将推理轨迹与影像数据结合进行风险预测。

Result: CardioCoT在MACE复发风险预测中表现优异，并提供可解释的推理过程。

Conclusion: 该框架为精准治疗和临床决策提供新思路，兼具预测性能与可解释性。

Abstract: Accurate prediction of major adverse cardiovascular events recurrence risk in
acute myocardial infarction patients based on postoperative cardiac MRI and
associated clinical notes is crucial for precision treatment and personalized
intervention. Existing methods primarily focus on risk stratification
capability while overlooking the need for intermediate robust reasoning and
model interpretability in clinical practice. Moreover, end-to-end risk
prediction using LLM/VLM faces significant challenges due to data limitations
and modeling complexity. To bridge this gap, we propose CardioCoT, a novel
two-stage hierarchical reasoning-enhanced survival analysis framework designed
to enhance both model interpretability and predictive performance. In the first
stage, we employ an evidence-augmented self-refinement mechanism to guide
LLM/VLMs in generating robust hierarchical reasoning trajectories based on
associated radiological findings. In the second stage, we integrate the
reasoning trajectories with imaging data for risk model training and
prediction. CardioCoT demonstrates superior performance in MACE recurrence risk
prediction while providing interpretable reasoning processes, offering valuable
insights for clinical decision-making.

</details>


### [308] [Structuring the Unstructured: A Multi-Agent System for Extracting and Querying Financial KPIs and Guidance](https://arxiv.org/abs/2505.19197)
*Chanyeol Choi,Jihoon Kwon,Minjae Kim,Juneha Hwang,Minsoo Ha,Chaewoon Kim,Jaeseon Ha,Suyeol Yun,Jin Kim*

Main category: cs.AI

TL;DR: 该论文提出了一种基于多智能体系统的金融文档结构化数据提取方法，准确率高达95%，显著提升了投资研究的效率。


<details>
  <summary>Details</summary>
Motivation: 传统金融文档分析依赖人工处理，效率低且难以扩展。本文旨在通过自动化技术解决这一问题。

Method: 采用由提取代理和文本转SQL代理组成的多智能体系统，分别负责关键指标提取和自然语言查询转换。

Result: 系统在结构化数据转换中达到95%准确率，自然语言查询检索任务中91%的响应被评估为正确。

Conclusion: 该方法能高效准确地将非结构化金融文本转化为结构化数据，显著提升研究流程的自动化水平。

Abstract: Extracting structured and quantitative insights from unstructured financial
filings is essential in investment research, yet remains time-consuming and
resource-intensive. Conventional approaches in practice rely heavily on
labor-intensive manual processes, limiting scalability and delaying the
research workflow. In this paper, we propose an efficient and scalable method
for accurately extracting quantitative insights from unstructured financial
documents, leveraging a multi-agent system composed of large language models.
Our proposed multi-agent system consists of two specialized agents: the
\emph{Extraction Agent} and the \emph{Text-to-SQL Agent}. The
\textit{Extraction Agent} automatically identifies key performance indicators
from unstructured financial text, standardizes their formats, and verifies
their accuracy. On the other hand, the \textit{Text-to-SQL Agent} generates
executable SQL statements from natural language queries, allowing users to
access structured data accurately without requiring familiarity with the
database schema. Through experiments, we demonstrate that our proposed system
effectively transforms unstructured text into structured data accurately and
enables precise retrieval of key information. First, we demonstrate that our
system achieves approximately 95\% accuracy in transforming financial filings
into structured data, matching the performance level typically attained by
human annotators. Second, in a human evaluation of the retrieval task -- where
natural language queries are used to search information from structured data --
91\% of the responses were rated as correct by human evaluators. In both
evaluations, our system generalizes well across financial document types,
consistently delivering reliable performance.

</details>


### [309] [Improving Medical Reasoning with Curriculum-Aware Reinforcement Learning](https://arxiv.org/abs/2505.19213)
*Shaohao Rui,Kaitao Chen,Weijie Ma,Xiaosong Wang*

Main category: cs.AI

TL;DR: 论文提出MedCCO框架，通过课程驱动的强化学习微调方法，统一处理医学视觉问答中的封闭式和开放式任务，显著提升模型性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前医学领域的强化学习微调方法主要局限于封闭式视觉问答，无法满足开放式、需要深入推理的临床决策需求，亟需一种能结合两者的解决方案。

Method: 提出MedCCO框架，采用课程驱动的强化学习范式，先在多样化封闭式医学VQA任务上微调建立基础推理能力，再逐步适应开放式任务以增强知识深度和临床可解释性。

Result: 在8个医学VQA基准测试中验证，MedCCO在域内任务准确率提升11.4%，域外基准提升5.7%，展现出优异的泛化性能。

Conclusion: 课程引导的强化学习方法能有效提升医学多模态语言模型的临床相关推理能力，为未来研究提供新方向。

Abstract: Recent advances in reinforcement learning with verifiable, rule-based rewards
have greatly enhanced the reasoning capabilities and out-of-distribution
generalization of VLMs/LLMs, obviating the need for manually crafted reasoning
chains. Despite these promising developments in the general domain, their
translation to medical imaging remains limited. Current medical reinforcement
fine-tuning (RFT) methods predominantly focus on close-ended VQA, thereby
restricting the model's ability to engage in world knowledge retrieval and
flexible task adaptation. More critically, these methods fall short of
addressing the critical clinical demand for open-ended, reasoning-intensive
decision-making. To bridge this gap, we introduce \textbf{MedCCO}, the first
multimodal reinforcement learning framework tailored for medical VQA that
unifies close-ended and open-ended data within a curriculum-driven RFT
paradigm. Specifically, MedCCO is initially fine-tuned on a diverse set of
close-ended medical VQA tasks to establish domain-grounded reasoning
capabilities, and is then progressively adapted to open-ended tasks to foster
deeper knowledge enhancement and clinical interpretability. We validate MedCCO
across eight challenging medical VQA benchmarks, spanning both close-ended and
open-ended settings. Experimental results show that MedCCO consistently
enhances performance and generalization, achieving a 11.4\% accuracy gain
across three in-domain tasks, and a 5.7\% improvement on five out-of-domain
benchmarks. These findings highlight the promise of curriculum-guided RL in
advancing robust, clinically-relevant reasoning in medical multimodal language
models.

</details>


### [310] [Where Paths Collide: A Comprehensive Survey of Classic and Learning-Based Multi-Agent Pathfinding](https://arxiv.org/abs/2505.19219)
*Shiyue Wang,Haozheng Xu,Yuhan Zhang,Jingran Lin,Changhong Lu,Xiangfeng Wang,Wenhao Li*

Main category: cs.AI

TL;DR: 本文综述了多智能体路径规划（MAPF）领域的经典算法与新兴学习方法，提出了统一框架，并指出评估标准不统一的问题，展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着自主系统在仓库、城市交通等复杂环境中的普及，MAPF从理论挑战发展为实际多机器人协调的关键技术。本文旨在弥合经典算法与学习方法之间的鸿沟。

Method: 通过系统分析200多篇论文，提出包含搜索方法（如冲突搜索、优先级搜索）、编译方法（如SAT、SMT）和数据驱动技术（如强化学习）的统一框架。

Result: 研究发现经典方法通常在更大规模实例上测试（如1000+智能体），而学习方法多限于小规模（10-100智能体），并揭示了评估标准不统一的问题。

Conclusion: 本文为MAPF研究提供了全面参考，并指出未来方向包括博弈论混合动机MAPF、大语言模型规划以及结合经典方法与深度学习的神经求解器架构。

Abstract: Multi-Agent Path Finding (MAPF) is a fundamental problem in artificial
intelligence and robotics, requiring the computation of collision-free paths
for multiple agents navigating from their start locations to designated goals.
As autonomous systems become increasingly prevalent in warehouses, urban
transportation, and other complex environments, MAPF has evolved from a
theoretical challenge to a critical enabler of real-world multi-robot
coordination. This comprehensive survey bridges the long-standing divide
between classical algorithmic approaches and emerging learning-based methods in
MAPF research. We present a unified framework that encompasses search-based
methods (including Conflict-Based Search, Priority-Based Search, and Large
Neighborhood Search), compilation-based approaches (SAT, SMT, CSP, ASP, and MIP
formulations), and data-driven techniques (reinforcement learning, supervised
learning, and hybrid strategies). Through systematic analysis of experimental
practices across 200+ papers, we uncover significant disparities in evaluation
methodologies, with classical methods typically tested on larger-scale
instances (up to 200 by 200 grids with 1000+ agents) compared to learning-based
approaches (predominantly 10-100 agents). We provide a comprehensive taxonomy
of evaluation metrics, environment types, and baseline selections, highlighting
the need for standardized benchmarking protocols. Finally, we outline promising
future directions including mixed-motive MAPF with game-theoretic
considerations, language-grounded planning with large language models, and
neural solver architectures that combine the rigor of classical methods with
the flexibility of deep learning. This survey serves as both a comprehensive
reference for researchers and a practical guide for deploying MAPF solutions in
increasingly complex real-world applications.

</details>


### [311] [DeCoDe: Defer-and-Complement Decision-Making via Decoupled Concept Bottleneck Models](https://arxiv.org/abs/2505.19220)
*Chengbo He,Bochao Zou,Junliang Xing,Jiansheng Chen,Yuanchun Shi,Huimin Ma*

Main category: cs.AI

TL;DR: 论文提出DeCoDe框架，通过可解释的概念表示实现人机协作的灵活决策，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人机协作方法通常仅做二元选择（AI或人类），忽略了互补优势且缺乏可解释性，难以满足高风险场景的需求。

Method: 提出基于解耦概念瓶颈模型（DeCoDe）的框架，通过门控网络选择三种协作模式（AI自主、人类处理、人机互补），并使用新损失函数平衡准确性与人力成本。

Result: 实验表明DeCoDe在真实数据集上显著优于纯AI、纯人类及传统延迟方法，且在噪声标注下仍保持强鲁棒性和可解释性。

Conclusion: DeCoDe实现了实例级、可解释且自适应的人机协作，为高价值决策场景提供了透明化解决方案。

Abstract: In human-AI collaboration, a central challenge is deciding whether the AI
should handle a task, be deferred to a human expert, or be addressed through
collaborative effort. Existing Learning to Defer approaches typically make
binary choices between AI and humans, neglecting their complementary strengths.
They also lack interpretability, a critical property in high-stakes scenarios
where users must understand and, if necessary, correct the model's reasoning.
To overcome these limitations, we propose Defer-and-Complement Decision-Making
via Decoupled Concept Bottleneck Models (DeCoDe), a concept-driven framework
for human-AI collaboration. DeCoDe makes strategy decisions based on
human-interpretable concept representations, enhancing transparency throughout
the decision process. It supports three flexible modes: autonomous AI
prediction, deferral to humans, and human-AI collaborative complementarity,
selected via a gating network that takes concept-level inputs and is trained
using a novel surrogate loss that balances accuracy and human effort. This
approach enables instance-specific, interpretable, and adaptive human-AI
collaboration. Experiments on real-world datasets demonstrate that DeCoDe
significantly outperforms AI-only, human-only, and traditional deferral
baselines, while maintaining strong robustness and interpretability even under
noisy expert annotations.

</details>


### [312] [GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling](https://arxiv.org/abs/2505.19234)
*Jialong Zhou,Lichao Wang,Xiao Yang*

Main category: cs.AI

TL;DR: GUARDIAN是一种检测和缓解多智能体协作中安全问题的统一方法，通过图模型和增量学习实现高效安全防护。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）推动多智能体协作发展，但面临幻觉放大和错误传播等安全挑战。

Method: 将多智能体协作建模为时序属性图，采用无监督编码器-解码器架构和图抽象机制，识别异常节点和边。

Result: 实验证明GUARDIAN能有效防护多种安全漏洞，准确率高且资源利用率高效。

Conclusion: GUARDIAN为LLM多智能体协作提供了可靠的安全保障，具有先进性和实用性。

Abstract: The emergence of large language models (LLMs) enables the development of
intelligent agents capable of engaging in complex and multi-turn dialogues.
However, multi-agent collaboration face critical safety challenges, such as
hallucination amplification and error injection and propagation. This paper
presents GUARDIAN, a unified method for detecting and mitigating multiple
safety concerns in GUARDing Intelligent Agent collaboratioNs. By modeling the
multi-agent collaboration process as a discrete-time temporal attributed graph,
GUARDIAN explicitly captures the propagation dynamics of hallucinations and
errors. The unsupervised encoder-decoder architecture incorporating an
incremental training paradigm, learns to reconstruct node attributes and graph
structures from latent embeddings, enabling the identification of anomalous
nodes and edges with unparalleled precision. Moreover, we introduce a graph
abstraction mechanism based on the Information Bottleneck Theory, which
compresses temporal interaction graphs while preserving essential patterns.
Extensive experiments demonstrate GUARDIAN's effectiveness in safeguarding LLM
multi-agent collaborations against diverse safety vulnerabilities, achieving
state-of-the-art accuracy with efficient resource utilization.

</details>


### [313] [Sensorimotor features of self-awareness in multimodal large language models](https://arxiv.org/abs/2505.19237)
*Iñaki Dellibarda Varela,Pablo Romero-Sorozabal,Diego Torricelli,Gabriel Delgado-Oleas,Jose Ignacio Serrano,Maria Dolores del Castillo Sobrino,Eduardo Rocon,Manuel Cebrian*

Main category: cs.AI

TL;DR: 多模态大语言模型通过传感器运动体验可发展出自我意识，展现出环境感知、自我识别和预测能力。


<details>
  <summary>Details</summary>
Motivation: 探索多模态大语言模型是否仅通过传感器运动体验就能发展出自我意识，以推动人工智能在非人类平台（如机器人）上的具身能力。

Method: 将多模态大语言模型集成到自主移动机器人中，测试其通过传感器运动体验实现自我意识的能力，并进行感官输入消融测试。

Result: 系统展现出强大的环境感知、自我识别和预测意识能力，能够推断其机器人本质和运动特性，感官整合影响自我意识的不同维度及其与记忆的协调。

Conclusion: 适当的世界和自我感官信息使多模态大语言模型展现出涌现的自我意识，为人工具身认知系统开辟了道路。

Abstract: Self-awareness - the ability to distinguish oneself from the surrounding
environment - underpins intelligent, autonomous behavior. Recent advances in AI
achieve human-like performance in tasks integrating multimodal information,
particularly in large language models, raising interest in the embodiment
capabilities of AI agents on nonhuman platforms such as robots. Here, we
explore whether multimodal LLMs can develop self-awareness solely through
sensorimotor experiences. By integrating a multimodal LLM into an autonomous
mobile robot, we test its ability to achieve this capacity. We find that the
system exhibits robust environmental awareness, self-recognition and predictive
awareness, allowing it to infer its robotic nature and motion characteristics.
Structural equation modeling reveals how sensory integration influences
distinct dimensions of self-awareness and its coordination with past-present
memory, as well as the hierarchical internal associations that drive
self-identification. Ablation tests of sensory inputs identify critical
modalities for each dimension, demonstrate compensatory interactions among
sensors and confirm the essential role of structured and episodic memory in
coherent reasoning. These findings demonstrate that, given appropriate sensory
information about the world and itself, multimodal LLMs exhibit emergent
self-awareness, opening the door to artificial embodied cognitive systems.

</details>


### [314] [Using Large Language Models to Assess Teachers' Pedagogical Content Knowledge](https://arxiv.org/abs/2505.19266)
*Yaxuan Yang,Shiyu Wang,Xiaoming Zhai*

Main category: cs.AI

TL;DR: 研究比较了人类评分员、监督机器学习和大型语言模型在评分教师教学内容知识时的表现，发现大型语言模型虽高效但会引入无关变异。


<details>
  <summary>Details</summary>
Motivation: 评估教师教学内容知识（PCK）的传统方法耗时耗力，大型语言模型（LLMs）提供了自动评分的新机会，但其是否引入无关变异（CIV）尚不明确。

Method: 使用广义线性混合模型（GLMMs）比较人类评分员、监督机器学习和LLM在视频构建反应任务中的评分差异，重点关注情景变异性、评分严格性和对情景的敏感性。

Result: 情景水平变异较小，评分相关因素对CIV贡献较大，监督机器学习评分最严格且敏感性最低，LLM评分最宽松。

Conclusion: LLM在提高评分效率的同时，也会像人类评分员一样引入CIV，但其贡献程度与监督机器学习不同。研究对评分员培训、自动评分设计和模型可解释性提出了建议。

Abstract: Assessing teachers' pedagogical content knowledge (PCK) through
performance-based tasks is both time and effort-consuming. While large language
models (LLMs) offer new opportunities for efficient automatic scoring, little
is known about whether LLMs introduce construct-irrelevant variance (CIV) in
ways similar to or different from traditional machine learning (ML) and human
raters. This study examines three sources of CIV -- scenario variability, rater
severity, and rater sensitivity to scenario -- in the context of video-based
constructed-response tasks targeting two PCK sub-constructs: analyzing student
thinking and evaluating teacher responsiveness. Using generalized linear mixed
models (GLMMs), we compared variance components and rater-level scoring
patterns across three scoring sources: human raters, supervised ML, and LLM.
Results indicate that scenario-level variance was minimal across tasks, while
rater-related factors contributed substantially to CIV, especially in the more
interpretive Task II. The ML model was the most severe and least sensitive
rater, whereas the LLM was the most lenient. These findings suggest that the
LLM contributes to scoring efficiency while also introducing CIV as human
raters do, yet with varying levels of contribution compared to supervised ML.
Implications for rater training, automated scoring design, and future research
on model interpretability are discussed.

</details>


### [315] [Next Token Prediction Is a Dead End for Creativity](https://arxiv.org/abs/2505.19277)
*Ibukun Olatunji,Mark Sheppard*

Main category: cs.AI

TL;DR: 该论文认为基于token预测的模型与真正的创造力存在本质偏差，提出将创造力视为互动过程而非预测输出的新视角。


<details>
  <summary>Details</summary>
Motivation: 现有基于token预测的模型虽在语言生成方面取得进展，但其架构更倾向于表面连贯性，而非自发性、原创性和即兴冒险，这限制了模型在对抗性或情感共鸣交流中的表现。

Method: 以battle rap（说唱对战）作为案例研究，揭示预测性系统的局限性。

Result: 研究表明预测性系统无法真正参与对抗性或情感共鸣的交流。

Conclusion: 通过将创造力重新定义为互动过程而非预测输出，论文提出了更具表现力、响应性且与人类创造性实践更契合的AI系统愿景。

Abstract: This paper argues that token prediction is fundamentally misaligned with real
creativity. While next-token models have enabled impressive advances in
language generation, their architecture favours surface-level coherence over
spontaneity, originality, and improvisational risk. We use battle rap as a case
study to expose the limitations of predictive systems, demonstrating that they
cannot truly engage in adversarial or emotionally resonant exchanges. By
reframing creativity as an interactive process rather than a predictive output,
we offer a vision for AI systems that are more expressive, responsive, and
aligned with human creative practice.

</details>


### [316] [Effort-aware Fairness: Incorporating a Philosophy-informed, Human-centered Notion of Effort into Algorithmic Fairness Metrics](https://arxiv.org/abs/2505.19317)
*Tin Nguyen,Jiannan Xu,Zora Che,Phuong-Anh Nguyen-Le,Rushil Dandamudi,Donald Braman,Furong Huang,Hal Daumé III,Zubin Jelveh*

Main category: cs.AI

TL;DR: 该论文提出了一种基于哲学概念的'努力感知公平性'(EaF)度量方法，强调在评估AI决策公平时应考虑个体为改善现状所付出的努力，而不仅仅是静态的人口统计学指标。


<details>
  <summary>Details</summary>
Motivation: 现有AI公平性指标（如人口统计平等）未考虑个体在特征空间中的努力历程，而哲学和人类对公平的理解中'努力'是关键因素。论文旨在填补这一空白。

Method: 1. 理论构建：基于'力'(Force)概念提出EaF指标，关注预测特征的时间轨迹和惯性
2. 预注册人类实验：验证人们在公平性评估中更关注特征变化轨迹而非静态值
3. 开发计算流程：在刑事司法和个人金融场景中实现个体/群体层面的EaF评估

Result: 1. 实验证实人类在公平性判断的两个阶段都更重视特征的时间动态
2. 建立了可操作的计算框架，能识别因系统性/早期劣势而难以改善处境的个体

Conclusion: EaF框架为AI审计提供了新工具，可发现并修正对持续努力但仍受制于外部劣势的个体的不公平决策，推动了超越静态指标的公平性研究。

Abstract: Although popularized AI fairness metrics, e.g., demographic parity, have
uncovered bias in AI-assisted decision-making outcomes, they do not consider
how much effort one has spent to get to where one is today in the input feature
space. However, the notion of effort is important in how Philosophy and humans
understand fairness. We propose a philosophy-informed way to conceptualize and
evaluate Effort-aware Fairness (EaF) based on the concept of Force, or temporal
trajectory of predictive features coupled with inertia. In addition to our
theoretical formulation of EaF metrics, our empirical contributions include: 1/
a pre-registered human subjects experiment, which demonstrates that for both
stages of the (individual) fairness evaluation process, people consider the
temporal trajectory of a predictive feature more than its aggregate value; 2/
pipelines to compute Effort-aware Individual/Group Fairness in the criminal
justice and personal finance contexts. Our work may enable AI model auditors to
uncover and potentially correct unfair decisions against individuals who spent
significant efforts to improve but are still stuck with systemic/early-life
disadvantages outside their control.

</details>


### [317] [Evaluating Steering Techniques using Human Similarity Judgments](https://arxiv.org/abs/2505.19333)
*Zach Studdiford,Timothy T. Rogers,Siddharth Suresh,Kushin Mukherjee*

Main category: cs.AI

TL;DR: 论文通过人类认知任务评估LLM引导技术，发现基于提示的方法在准确性和人机对齐上表现最佳，但LLM存在对‘类别’相似性的偏好。


<details>
  <summary>Details</summary>
Motivation: 当前对大型语言模型（LLM）引导技术的评估过于关注任务性能，忽略了其表示是否与人类认知对齐。

Method: 使用三元组相似性判断任务，评估LLM在‘大小’和‘类别’相似性上的灵活判断能力。

Result: 基于提示的引导方法在准确性和人机对齐上优于其他方法，但LLM对‘大小’对齐表现较差且偏向‘类别’相似性。

Conclusion: 基于人类认知的评估支持提示引导的有效性，并揭示了LLM在引导前的表征偏好。

Abstract: Current evaluations of Large Language Model (LLM) steering techniques focus
on task-specific performance, overlooking how well steered representations
align with human cognition. Using a well-established triadic similarity
judgment task, we assessed steered LLMs on their ability to flexibly judge
similarity between concepts based on size or kind. We found that prompt-based
steering methods outperformed other methods both in terms of steering accuracy
and model-to-human alignment. We also found LLMs were biased towards 'kind'
similarity and struggled with 'size' alignment. This evaluation approach,
grounded in human cognition, adds further support to the efficacy of
prompt-based steering and reveals privileged representational axes in LLMs
prior to steering.

</details>


### [318] [PatentMind: A Multi-Aspect Reasoning Graph for Patent Similarity Evaluation](https://arxiv.org/abs/2505.19347)
*Yongmin Yoo,Qiongkai Xu,Longbing Cao*

Main category: cs.AI

TL;DR: PatentMind提出了一种基于多维度推理图的专利相似性评估框架，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有专利相似性评估方法常忽略专利文档的复杂结构，无法全面捕捉技术、法律和应用等多维度信息。

Method: 通过将专利分解为技术特征、应用领域和权利要求三个核心维度，并采用四阶段动态加权推理过程计算相似性得分。

Result: 实验表明，PatentMind与专家标注的相关系数达0.938，明显优于基于嵌入的模型和高级提示工程方法。

Conclusion: 模块化推理框架能有效克服嵌入方法在专利相似性分析中的局限性，实现更接近专家水平的评估。

Abstract: Patent similarity evaluation plays a critical role in intellectual property
analysis. However, existing methods often overlook the intricate structure of
patent documents, which integrate technical specifications, legal boundaries,
and application contexts. We introduce PatentMind, a novel framework for patent
similarity assessment based on a Multi-Aspect Reasoning Graph (MARG).
PatentMind decomposes patents into three core dimensions: technical feature,
application domain, and claim scope, to compute dimension-specific similarity
scores. These scores are dynamically weighted through a four-stage reasoning
process which integrates contextual signals to emulate expert-level judgment.
To support evaluation, we construct PatentSimBench, a human-annotated benchmark
comprising 500 patent pairs. Experimental results demonstrate that PatentMind
achieves a strong correlation ($r=0.938$) with expert annotations,
significantly outperforming embedding-based models and advanced prompt
engineering methods.These results highlight the effectiveness of modular
reasoning frameworks in overcoming key limitations of embedding-based methods
for analyzing patent similarity.

</details>


### [319] [Architectures of Error: A Philosophical Inquiry into AI and Human Code Generation](https://arxiv.org/abs/2505.19353)
*Camilo Chacón Sartori*

Main category: cs.AI

TL;DR: 本文探讨了生成式AI在代码生成中与人类合作的错误架构差异，提出了基于认知与随机性根源的区分框架，并讨论了其对语义、安全及控制的哲学影响。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在代码生成中的广泛应用，理解人类与机器在错误产生上的本质差异变得至关重要，以促进更有效的人机协作软件开发。

Method: 结合Dennett的机械功能主义和Rescher的方法论实用主义，分析人类认知与AI随机性错误的因果起源，并利用Floridi的抽象层次理论细化错误维度的交互。

Result: 提出了区分人类认知错误与AI随机错误的框架，揭示了这些差异对软件开发的语义一致性、安全性和控制机制的核心挑战。

Conclusion: 研究为哲学家提供了理解生成式AI认识论挑战的结构化框架，同时为工程师提供了批判性参与的基础，强调了错误架构在技术演进中的动态影响。

Abstract: With the rise of generative AI (GenAI), Large Language Models are
increasingly employed for code generation, becoming active co-authors alongside
human programmers. Focusing specifically on this application domain, this paper
articulates distinct ``Architectures of Error'' to ground an epistemic
distinction between human and machine code generation. Examined through their
shared vulnerability to error, this distinction reveals fundamentally different
causal origins: human-cognitive versus artificial-stochastic. To develop this
framework and substantiate the distinction, the analysis draws critically upon
Dennett's mechanistic functionalism and Rescher's methodological pragmatism. I
argue that a systematic differentiation of these error profiles raises critical
philosophical questions concerning semantic coherence, security robustness,
epistemic limits, and control mechanisms in human-AI collaborative software
development. The paper also utilizes Floridi's levels of abstraction to provide
a nuanced understanding of how these error dimensions interact and may evolve
with technological advancements. This analysis aims to offer philosophers a
structured framework for understanding GenAI's unique epistemological
challenges, shaped by these architectural foundations, while also providing
software engineers a basis for more critically informed engagement.

</details>


### [320] [Consistency-based Abductive Reasoning over Perceptual Errors of Multiple Pre-trained Models in Novel Environments](https://arxiv.org/abs/2505.19361)
*Mario Leiva,Noel Ngu,Joshua Shay Kricheli,Aditya Taparia,Ransalu Senanayake,Paulo Shakarian,Nathaniel Bastian,John Corcoran,Gerardo Simari*

Main category: cs.AI

TL;DR: 该论文提出一种基于一致性溯因的框架，通过整合多个预训练模型的预测来应对分布偏移导致的性能下降问题，相比单一模型和标准集成方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 预训练感知模型在新环境中常因分布偏移导致性能下降。现有元认知方法通过逻辑规则过滤错误，但提高精确率往往以召回率为代价。论文假设利用多个预训练模型可缓解召回率下降问题。

Method: 将多模型冲突预测管理建模为一致性溯因问题：将输入预测和各模型的错误检测规则编码为逻辑程序，寻找满足领域约束且覆盖率最大的预测子集。提出基于整数规划的精确算法和高效启发式搜索算法。

Result: 在模拟航拍数据集（含复杂受控分布偏移）上的实验表明，该框架F1分数平均提升13.6%，准确率提升16.6%，显著优于单一模型和标准集成基线。

Conclusion: 基于一致性溯因的机制能有效整合多个不完美推理器的知识，在挑战性新场景中实现鲁棒性能提升。

Abstract: The deployment of pre-trained perception models in novel environments often
leads to performance degradation due to distributional shifts. Although recent
artificial intelligence approaches for metacognition use logical rules to
characterize and filter model errors, improving precision often comes at the
cost of reduced recall. This paper addresses the hypothesis that leveraging
multiple pre-trained models can mitigate this recall reduction. We formulate
the challenge of identifying and managing conflicting predictions from various
models as a consistency-based abduction problem. The input predictions and the
learned error detection rules derived from each model are encoded in a logic
program. We then seek an abductive explanation--a subset of model
predictions--that maximizes prediction coverage while ensuring the rate of
logical inconsistencies (derived from domain constraints) remains below a
specified threshold. We propose two algorithms for this knowledge
representation task: an exact method based on Integer Programming (IP) and an
efficient Heuristic Search (HS). Through extensive experiments on a simulated
aerial imagery dataset featuring controlled, complex distributional shifts, we
demonstrate that our abduction-based framework outperforms individual models
and standard ensemble baselines, achieving, for instance, average relative
improvements of approximately 13.6% in F1-score and 16.6% in accuracy across 15
diverse test datasets when compared to the best individual model. Our results
validate the use of consistency-based abduction as an effective mechanism to
robustly integrate knowledge from multiple imperfect reasoners in challenging,
novel scenarios.

</details>


### [321] [Foundations of Top-$k$ Decoding For Language Models](https://arxiv.org/abs/2505.19371)
*Georgy Noarov,Soham Mallick,Tao Wang,Sunay Joshi,Yan Sun,Yangxinyu Xie,Mengxin Yu,Edgar Dobriban*

Main category: cs.AI

TL;DR: 该论文提出了一个理论框架来解释和推广top-k解码方法，通过稀疏概率分布恢复的视角，开发了基于Bregman散度的解码策略，并证明top-k解码是KL散度下的特例。


<details>
  <summary>Details</summary>
Motivation: 当前top-k解码等方法缺乏精确的理论动机，作者旨在填补这一空白，从稀疏概率分布恢复的角度为这些方法提供理论基础。

Method: 将解码视为稀疏概率分布的恢复问题，使用可分离Bregman散度最小化结合ℓ0正则化，开发了高效优化算法。

Result: 证明了最优解码策略是贪婪的，损失函数在k上是离散凸的，top-k解码是KL散度的特例，并发现了新的解码策略。

Conclusion: 该工作为top-k解码提供了理论依据，并扩展出新的解码方法，展示了不同散度下解码行为的多样性。

Abstract: Top-$k$ decoding is a widely used method for sampling from LLMs: at each
token, only the largest $k$ next-token-probabilities are kept, and the next
token is sampled after re-normalizing them to sum to unity. Top-$k$ and other
sampling methods are motivated by the intuition that true next-token
distributions are sparse, and the noisy LLM probabilities need to be truncated.
However, to our knowledge, a precise theoretical motivation for the use of
top-$k$ decoding is missing. In this work, we develop a theoretical framework
that both explains and generalizes top-$k$ decoding. We view decoding at a
fixed token as the recovery of a sparse probability distribution. We consider
\emph{Bregman decoders} obtained by minimizing a separable Bregman divergence
(for both the \emph{primal} and \emph{dual} cases) with a sparsity-inducing
$\ell_0$ regularization. Despite the combinatorial nature of the objective, we
show how to optimize it efficiently for a large class of divergences. We show
that the optimal decoding strategies are greedy, and further that the loss
function is discretely convex in $k$, so that binary search provably and
efficiently finds the optimal $k$. We show that top-$k$ decoding arises as a
special case for the KL divergence, and identify new decoding strategies that
have distinct behaviors (e.g., non-linearly up-weighting larger probabilities
after re-normalization).

</details>


### [322] [DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous Driving](https://arxiv.org/abs/2505.19381)
*Anqing Jiang,Yu Gao,Zhigang Sun,Yiru Wang,Jijun Wang,Jinghao Chai,Qian Cao,Yuweng Heng,Hao Jiang,Zongzheng Zhang,Xianda Guo,Hao Sun,Hao Zhao*

Main category: cs.AI

TL;DR: 提出Diff-VLA方法，结合稀疏-稠密扩散策略和视觉语言模型，解决端到端自动驾驶中的计算成本高、行为多样性不足和复杂场景决策问题。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法存在BEV计算昂贵、行为多样性不足及复杂场景决策欠佳等问题，需改进以提升性能。

Method: 采用混合稀疏-稠密扩散策略，结合视觉语言模型，优化多模态驾驶行为和轨迹生成。

Result: 在Autonomous Grand Challenge 2025中表现优异，PDMS达到45.0。

Conclusion: Diff-VLA方法有效提升自动驾驶在复杂场景中的决策能力和性能。

Abstract: Research interest in end-to-end autonomous driving has surged owing to its
fully differentiable design integrating modular tasks, i.e. perception,
prediction and planing, which enables optimization in pursuit of the ultimate
goal. Despite the great potential of the end-to-end paradigm, existing methods
suffer from several aspects including expensive BEV (bird's eye view)
computation, action diversity, and sub-optimal decision in complex real-world
scenarios. To address these challenges, we propose a novel hybrid sparse-dense
diffusion policy, empowered by a Vision-Language Model (VLM), called Diff-VLA.
We explore the sparse diffusion representation for efficient multi-modal
driving behavior. Moreover, we rethink the effectiveness of VLM driving
decision and improve the trajectory generation guidance through deep
interaction across agent, map instances and VLM output. Our method shows
superior performance in Autonomous Grand Challenge 2025 which contains
challenging real and reactive synthetic scenarios. Our methods achieves 45.0
PDMS.

</details>


### [323] [CaseEdit: Enhancing Localized Commonsense Reasoning via Null-Space Constrained Knowledge Editing in Small Parameter Language Models](https://arxiv.org/abs/2505.19383)
*Varun Reddy,Yen-Ling Kuo*

Main category: cs.AI

TL;DR: 论文提出CaseEdit数据集和AlphaEdit方法，用于在小参数LLM中编辑个性化常识知识，AlphaEdit通过零空间投影减少干扰，表现优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在事实回忆和通用推理上表现良好，但在适应个性化常识知识方面存在困难，尤其是在计算效率优先的小参数设置中。

Method: 基于ATOMIC20/20常识图谱构建CaseEdit数据集，采用多阶段推理生成典型和非典型上下文编辑，并评估四种指标；提出AlphaEdit方法，利用零空间投影最小化知识干扰。

Result: AlphaEdit在LLaMA 3.2 3B模型上表现优于其他知识编辑方法，扩展性测试中干扰效应最小。

Conclusion: CaseEdit与AlphaEdit结合可使小模型内化高质量情境常识知识，为轻量化个性化助手铺平道路。

Abstract: Large language models (LLMs) exhibit strong performance on factual recall and
general reasoning but struggle to adapt to user-specific, commonsense
knowledge, a challenge particularly acute in small-parameter settings where
computational efficiency is prioritized. We introduce CaseEdit, a new dataset
and generation pipeline for evaluating localized, personalized commonsense
knowledge editing in small LLMs to address this. Built upon the ATOMIC20/20
commonsense graph, CaseEdit uses a multi-stage inference process to generate
both typical and atypical contextual edits for household objects, paired with
targeted evaluation questions across four axes: reliability, generalization,
locality, and portability. We evaluate established knowledge editing methods
using CaseEdit and demonstrate that AlphaEdit, a technique employing null-space
projection to minimize interference with unrelated knowledge, consistently
outperforms other methods when applied to an LLaMA 3.2 3B model, even in
scalability tests, showing minimal ripple effects. Our results indicate that
using CaseEdit with effective editing techniques like AlphaEdit allows small
models to internalize high-quality, context-sensitive common-sense knowledge,
paving the way for lightweight, personalized assistants.

</details>


### [324] [Recalibrating the Compass: Integrating Large Language Models into Classical Research Methods](https://arxiv.org/abs/2505.19402)
*Tai-Quan Peng,Xuzhen Yang*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型（LLMs）如何改变传播学及社会科学中的核心定量方法，如内容分析、调查研究和实验研究，强调其作为研究工具的潜力与局限。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索LLMs如何革新传统研究方法，同时指出其带来的新可能性及挑战，如有效性、偏见和可解释性问题。

Method: 通过回顾跨学科研究，结合Lasswell的经典框架，分析LLMs在文本编码、动态受访者模拟及个性化刺激生成中的应用。

Result: LLMs能够重新配置信息研究、受众分析和效果研究，但也需警惕其局限性，传统研究逻辑仍不可或缺。

Conclusion: 结论呼吁在传播学和社会科学研究中审慎、严谨且富有想象力地使用LLMs，将其视为技术工具及认知文化工具。

Abstract: This paper examines how large language models (LLMs) are transforming core
quantitative methods in communication research in particular, and in the social
sciences more broadly-namely, content analysis, survey research, and
experimental studies. Rather than replacing classical approaches, LLMs
introduce new possibilities for coding and interpreting text, simulating
dynamic respondents, and generating personalized and interactive stimuli.
Drawing on recent interdisciplinary work, the paper highlights both the
potential and limitations of LLMs as research tools, including issues of
validity, bias, and interpretability. To situate these developments
theoretically, the paper revisits Lasswell's foundational framework -- "Who
says what, in which channel, to whom, with what effect?" -- and demonstrates
how LLMs reconfigure message studies, audience analysis, and effects research
by enabling interpretive variation, audience trajectory modeling, and
counterfactual experimentation. Revisiting the metaphor of the methodological
compass, the paper argues that classical research logics remain essential as
the field integrates LLMs and generative AI. By treating LLMs not only as
technical instruments but also as epistemic and cultural tools, the paper calls
for thoughtful, rigorous, and imaginative use of LLMs in future communication
and social science research.

</details>


### [325] [Unveiling the Compositional Ability Gap in Vision-Language Reasoning Model](https://arxiv.org/abs/2505.19406)
*Tianle Li,Jihai Zhang,Yongming Rao,Yu Cheng*

Main category: cs.AI

TL;DR: 该研究探讨了大型视觉语言模型（VLMs）是否能够通过类似于大型语言模型（LLMs）的强化学习（RL）策略继承推理能力，发现RL训练的模型在组合泛化上表现更优，但当前训练策略在跨模态和跨任务组合推理上仍有不足。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索大型视觉语言模型（VLMs）是否能够通过类似于大型语言模型（LLMs）的强化学习（RL）策略直接继承其推理能力，尤其是在组合任务和跨模态条件下的表现。

Method: 研究方法包括设计一系列诊断任务，训练模型在单模态任务或独立推理技能上，并在需要技能整合的多模态组合变体上进行评估，比较监督微调（SFT）和RL训练模型的性能。

Result: 研究结果显示：(1) RL训练的模型在组合泛化上优于SFT；(2) VLMs在单个任务上表现良好，但在跨模态和跨任务组合推理上表现不佳；(3) 强制模型在推理前明确描述视觉内容（如“先描述后思考”）并结合渐进式视觉到文本的奖励机制，显著提升了性能。

Conclusion: 研究结论指出，提升VLMs组合推理能力的关键在于视觉到文本的对齐和准确的视觉基础，同时揭示了当前基于RL的VLM训练在跨模态和跨任务推理上的局限性，并提供了改进方向。

Abstract: While large language models (LLMs) demonstrate strong reasoning capabilities
utilizing reinforcement learning (RL) with verifiable reward, whether large
vision-language models (VLMs) can directly inherit such capabilities through
similar post-training strategies remains underexplored. In this work, we
conduct a systematic compositional probing study to evaluate whether current
VLMs trained with RL or other post-training strategies can compose capabilities
across modalities or tasks under out-of-distribution conditions. We design a
suite of diagnostic tasks that train models on unimodal tasks or isolated
reasoning skills, and evaluate them on multimodal, compositional variants
requiring skill integration. Through comparisons between supervised fine-tuning
(SFT) and RL-trained models, we identify three key findings: (1) RL-trained
models consistently outperform SFT on compositional generalization,
demonstrating better integration of learned skills; (2) although VLMs achieve
strong performance on individual tasks, they struggle to generalize
compositionally under cross-modal and cross-task scenario, revealing a
significant gap in current training strategies; (3) enforcing models to
explicitly describe visual content before reasoning (e.g.,
caption-before-thinking), along with rewarding progressive vision-to-text
grounding, yields notable gains. It highlights two essential ingredients for
improving compositionality in VLMs: visual-to-text alignment and accurate
visual grounding. Our findings shed light on the current limitations of
RL-based reasoning VLM training and provide actionable insights toward building
models that reason compositionally across modalities and tasks.

</details>


### [326] [Fusion Intelligence for Digital Twinning AI Data Centers: A Synergistic GenAI-PhyAI Approach](https://arxiv.org/abs/2505.19409)
*Ruihang Wang,Minghao Li,Zhiwei Cao,Jimin Jia,Kyle Guan,Yonggang Wen*

Main category: cs.AI

TL;DR: 本文提出Fusion Intelligence框架，结合生成式AI和物理AI优势，自动化创建并优化AI数据中心数字孪生，提升设计阶段能效预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法和独立AI解决方案难以应对AI数据中心的管理挑战，现有数字孪生创建方法存在定制化成本高或生成结果不准确的问题。

Method: 采用双智能体协作框架：生成式AI解析自然语言生成初始数字孪生，物理AI通过物理约束和实时数据对其进行优化。

Result: 案例研究表明，该框架能自动创建高精度数字孪生，其设计阶段能效预测优于纯物理模型，且运营数据可进一步提升准确性。

Conclusion: Fusion Intelligence为关键基础设施提供了可靠高效的AI驱动数字化转型路径。

Abstract: The explosion in artificial intelligence (AI) applications is pushing the
development of AI-dedicated data centers (AIDCs), creating management
challenges that traditional methods and standalone AI solutions struggle to
address. While digital twins are beneficial for AI-based design validation and
operational optimization, current AI methods for their creation face
limitations. Specifically, physical AI (PhyAI) aims to capture the underlying
physical laws, which demands extensive, case-specific customization, and
generative AI (GenAI) can produce inaccurate or hallucinated results. We
propose Fusion Intelligence, a novel framework synergizing GenAI's automation
with PhyAI's domain grounding. In this dual-agent collaboration, GenAI
interprets natural language prompts to generate tokenized AIDC digital twins.
Subsequently, PhyAI optimizes these generated twins by enforcing physical
constraints and assimilating real-time data. Case studies demonstrate the
advantages of our framework in automating the creation and validation of AIDC
digital twins. These twins deliver predictive analytics to support power usage
effectiveness (PUE) optimization in the design stage. With operational data
collected, the digital twin accuracy is further improved compared with pure
physics-based models developed by human experts. Fusion Intelligence offers a
promising pathway to accelerate digital transformation. It enables more
reliable and efficient AI-driven digital transformation for a broad range of
mission-critical infrastructures.

</details>


### [327] [Toward Physics-Informed Machine Learning for Data Center Operations: A Tropical Case Study](https://arxiv.org/abs/2505.19414)
*Ruihang Wang,Zhiwei Cao,Qingang Zhang,Rui Tan,Yonggang Wen,Tommy Leung,Stuart Kennedy,Justin Teoh*

Main category: cs.AI

TL;DR: 该论文提出了一种结合数据中心物理特性的物理信息机器学习方法，以解决热带地区数据中心因高温高湿环境导致的冷却成本高和系统可靠性问题，并通过案例研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 热带地区数据中心因全年高温高湿环境导致冷却成本增加和系统可靠性问题，现有机器学习方法因模型外推能力和系统安全性问题难以部署。

Method: 提出了一种集成物理特性的数据驱动机器学习系统，结合多物理过程和数据可用性，进行建模和优化。

Result: 通过行业级热带数据中心的案例研究，展示了该方法在不同操作智能水平下的应用效果。

Conclusion: 该方法有效解决了热带地区数据中心的冷却和可靠性问题，并指出了未来研究方向。

Abstract: Data centers are the backbone of computing capacity. Operating data centers
in the tropical regions faces unique challenges due to consistently high
ambient temperature and elevated relative humidity throughout the year. These
conditions result in increased cooling costs to maintain the reliability of the
computing systems. While existing machine learning-based approaches have
demonstrated potential to elevate operations to a more proactive and
intelligent level, their deployment remains dubious due to concerns about model
extrapolation capabilities and associated system safety issues. To address
these concerns, this article proposes incorporating the physical
characteristics of data centers into traditional data-driven machine learning
solutions. We begin by introducing the data center system, including the
relevant multiphysics processes and the data-physics availability. Next, we
outline the associated modeling and optimization problems and propose an
integrated, physics-informed machine learning system to address them. Using the
proposed system, we present relevant applications across varying levels of
operational intelligence. A case study on an industry-grade tropical data
center is provided to demonstrate the effectiveness of our approach. Finally,
we discuss key challenges and highlight potential future directions.

</details>


### [328] [Task Memory Engine: Spatial Memory for Robust Multi-Step LLM Agents](https://arxiv.org/abs/2505.19436)
*Ye Ye*

Main category: cs.AI

TL;DR: 论文提出Task Memory Engine (TME)，一种模块化内存控制器，通过图结构记忆框架提升大语言模型在多轮交互中的稳健性，显著减少幻觉和误解。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在多步交互中表现不佳，常出现幻觉、重复动作或误解用户修正，主要原因是缺乏持久记忆来跟踪任务依赖和用户意图。

Method: TME采用空间记忆框架，用基于图的结构（树或DAG）替代线性上下文，结合TRIM组件建模任务语义和用户意图，无需微调即可增强模型能力。

Result: 在旅行规划等四个多轮任务中，TME消除了三项任务中100%的幻觉和误解，总体减少66.7%幻觉和83.3%误解，性能超越ReAct。

Conclusion: TME的开源架构填补了复杂交互场景中智能代理的性能缺口，其模块化设计支持即插即用和领域定制，适用于个人助手和企业自动化场景。

Abstract: Large Language Models (LLMs) falter in multi-step interactions -- often
hallucinating, repeating actions, or misinterpreting user corrections -- due to
reliance on linear, unstructured context. This fragility stems from the lack of
persistent memory to track evolving goals and task dependencies, undermining
trust in autonomous agents. We introduce the Task Memory Engine (TME), a
modular memory controller that transforms existing LLMs into robust,
revision-aware agents without fine-tuning. TME implements a spatial memory
framework that replaces flat context with graph-based structures to support
consistent, multi-turn reasoning. Departing from linear concatenation and
ReAct-style prompting, TME builds a dynamic task graph -- either a tree or
directed acyclic graph (DAG) -- to map user inputs to subtasks, align them with
prior context, and enable dependency-tracked revisions. Its Task Representation
and Intent Management (TRIM) component models task semantics and user intent to
ensure accurate interpretation. Across four multi-turn scenarios-trip planning,
cooking, meeting scheduling, and shopping cart editing -- TME eliminates 100%
of hallucinations and misinterpretations in three tasks, and reduces
hallucinations by 66.7% and misinterpretations by 83.3% across 27 user turns,
outperforming ReAct. TME's modular design supports plug-and-play deployment and
domain-specific customization, adaptable to both personal assistants and
enterprise automation. We release TME's codebase, benchmarks, and components as
open-source resources, enabling researchers to develop reliable LLM agents.
TME's scalable architecture addresses a critical gap in agent performance
across complex, interactive settings.

</details>


### [329] [Style2Code: A Style-Controllable Code Generation Framework with Dual-Modal Contrastive Representation Learning](https://arxiv.org/abs/2505.19442)
*Dutao Zhang,Sergey Kovalchuk,YuLong He*

Main category: cs.AI

TL;DR: 提出两阶段训练框架，结合对比学习和条件解码，实现代码风格可控生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在保持功能正确性的同时灵活控制代码风格。

Method: 第一阶段对齐代码风格表征与语义/结构特征，第二阶段基于风格向量微调语言模型（如Flan-T5）。支持风格插值和轻量化用户个性化。

Result: 框架在保持代码正确性的同时，提供了优于现有方法的风格控制能力。

Conclusion: 首次将对比对齐与条件解码结合，为风格导向的代码生成提供了统一解决方案。

Abstract: Controllable code generation, the ability to synthesize code that follows a
specified style while maintaining functionality, remains a challenging task. We
propose a two-stage training framework combining contrastive learning and
conditional decoding to enable flexible style control. The first stage aligns
code style representations with semantic and structural features. In the second
stage, we fine-tune a language model (e.g., Flan-T5) conditioned on the learned
style vector to guide generation. Our method supports style interpolation and
user personalization via lightweight mixing. Compared to prior work, our
unified framework offers improved stylistic control without sacrificing code
correctness. This is among the first approaches to combine contrastive
alignment with conditional decoding for style-guided code generation.

</details>


### [330] [BizFinBench: A Business-Driven Real-World Financial Benchmark for Evaluating LLMs](https://arxiv.org/abs/2505.19457)
*Guilong Lu,Xuntao Guo,Rongjunchen Zhang,Wenqiao Zhu,Ji Liu*

Main category: cs.AI

TL;DR: 论文提出首个中文金融领域基准BizFinBench，评估25个大模型在5个维度的表现，发现现有模型在复杂跨概念推理任务上仍有不足，并创新性提出减少评估偏差的IteraJudge方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在金融等高精度领域的可靠性评估仍存挑战，缺乏专业基准。研究旨在填补这一空白，推动金融场景下的模型能力评测。

Method: 构建包含6,781条标注数据的BizFinBench基准，涵盖数值计算、推理等5个维度9个子类，提出基于LLM的IteraJudge评估方法以减少偏差。

Result: 实验显示：1)数值计算任务中DeepSeek-R1(64.04)领先；2)闭源模型在推理任务优势显著；3)信息抽取任务性能差异最大(11.23-71.46)；4)预测识别任务各模型表现接近(39.16-50.00)。

Conclusion: 当前模型能处理常规金融问题，但复杂跨概念推理仍是瓶颈。BizFinBench为金融NLP研究提供了标准化评估框架，代码数据集已开源。

Abstract: Large language models excel in general tasks, yet assessing their reliability
in logic-heavy, precision-critical domains like finance, law, and healthcare
remains challenging. To address this, we introduce BizFinBench, the first
benchmark specifically designed to evaluate LLMs in real-world financial
applications. BizFinBench consists of 6,781 well-annotated queries in Chinese,
spanning five dimensions: numerical calculation, reasoning, information
extraction, prediction recognition, and knowledge-based question answering,
grouped into nine fine-grained categories. The benchmark includes both
objective and subjective metrics. We also introduce IteraJudge, a novel LLM
evaluation method that reduces bias when LLMs serve as evaluators in objective
metrics. We benchmark 25 models, including both proprietary and open-source
systems. Extensive experiments show that no model dominates across all tasks.
Our evaluation reveals distinct capability patterns: (1) In Numerical
Calculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while
smaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning,
proprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with
open-source models trailing by up to 19.49 points; (3) In Information
Extraction, the performance spread is the largest, with DeepSeek-R1 scoring
71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition,
performance variance is minimal, with top models scoring between 39.16 and
50.00. We find that while current LLMs handle routine finance queries
competently, they struggle with complex scenarios requiring cross-concept
reasoning. BizFinBench offers a rigorous, business-aligned benchmark for future
research. The code and dataset are available at
https://github.com/HiThink-Research/BizFinBench.

</details>


### [331] [Origin Tracer: A Method for Detecting LoRA Fine-Tuning Origins in LLMs](https://arxiv.org/abs/2505.19466)
*Hongyu Liang,Yuting Zheng,Yihan Li,Yiran Zhang,Shiyu Liang*

Main category: cs.AI

TL;DR: 本文提出了一种名为Origin-Tracer的新方法，用于检测模型是否基于特定基础模型进行微调，并提取LoRA秩，以增强模型验证的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的发展，微调模型在特定任务上的性能提升常伴随来源不透明的误导性声明，现有验证技术难以应对混淆技术（如置换和缩放变换），亟需更可靠的验证方法。

Method: 提出Origin-Tracer方法，通过形式化框架检测模型微调来源，并提取微调过程中使用的LoRA秩，首次实现针对模型微调来源的精准定位。

Result: 在31个多样化开源模型上验证，模拟真实混淆场景，结果表明该方法有效，并具备设定模型验证新基准的潜力。

Conclusion: Origin-Tracer为模型验证提供了更鲁棒的解决方案，但其局限性仍需进一步探讨。

Abstract: As large language models (LLMs) continue to advance, their deployment often
involves fine-tuning to enhance performance on specific downstream tasks.
However, this customization is sometimes accompanied by misleading claims about
the origins, raising significant concerns about transparency and trust within
the open-source community. Existing model verification techniques typically
assess functional, representational, and weight similarities. However, these
approaches often struggle against obfuscation techniques, such as permutations
and scaling transformations. To address this limitation, we propose a novel
detection method Origin-Tracer that rigorously determines whether a model has
been fine-tuned from a specified base model. This method includes the ability
to extract the LoRA rank utilized during the fine-tuning process, providing a
more robust verification framework. This framework is the first to provide a
formalized approach specifically aimed at pinpointing the sources of model
fine-tuning. We empirically validated our method on thirty-one diverse
open-source models under conditions that simulate real-world obfuscation
scenarios. We empirically analyze the effectiveness of our framework and
finally, discuss its limitations. The results demonstrate the effectiveness of
our approach and indicate its potential to establish new benchmarks for model
verification.

</details>


### [332] [Causal-LLaVA: Causal Disentanglement for Mitigating Hallucination in Multimodal Large Language Models](https://arxiv.org/abs/2505.19474)
*Xinmiao Hu,Chun Wang,Ruihe An,ChenYu Shao,Xiaojun Ye,Sheng Zhou,Liangcheng Li*

Main category: cs.AI

TL;DR: 该论文提出了一种基于因果关系的解缠框架，通过因果干预减少多模态大语言模型中的物体幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉理解任务中表现优异，但常因数据集偏差导致物体幻觉问题，即生成与输入不一致或完全不存在的物体描述。

Method: 方法包括在视觉路径中引入因果驱动投影器，并在语言模型的最终Transformer层集成因果干预模块，以减少由偏差训练数据引起的虚假相关性。

Result: 实验结果表明，该方法在多个多模态基准测试中显著减少了幻觉现象，同时保持了强大的性能，可视化分析进一步证实了物体表征的可分离性提升。

Conclusion: 通过因果驱动的解缠框架，有效缓解了多模态大语言模型中的物体幻觉问题，提升了模型的鲁棒性和准确性。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated strong performance
in visual understanding tasks, yet they often suffer from object
hallucinations--generating descriptions of objects that are inconsistent with
or entirely absent from the input. This issue is closely related to dataset
biases, where frequent co-occurrences of objects lead to entangled semantic
representations across modalities. As a result, models may erroneously activate
object representations that are commonly associated with the input but not
actually present.
  To address this, we propose a causality-driven disentanglement framework that
mitigates hallucinations through causal intervention. Our approach includes a
Causal-Driven Projector in the visual pathway and a Causal Intervention Module
integrated into the final transformer layer of the language model. These
components work together to reduce spurious correlations caused by biased
training data.
  Experimental results show that our method significantly reduces
hallucinations while maintaining strong performance on multiple multimodal
benchmarks. Visualization analyses further confirm improved separability of
object representations.
  The code is available at: https://github.com/IgniSavium/Causal-LLaVA

</details>


### [333] [Judging with Many Minds: Do More Perspectives Mean Less Prejudice?](https://arxiv.org/abs/2505.19477)
*Chiyu Ma,Enpei Zhang,Yilun Zhao,Wenjun Liu,Yaning Jia,Peijun Qing,Lin Shi,Arman Cohan,Yujun Yan,Soroush Vosoughi*

Main category: cs.AI

TL;DR: 该研究系统分析了多智能体LLM-as-Judge框架中的四种偏见类型，发现辩论框架会放大偏见，而元评判方法更具抵抗力。引入PINE方法能有效减少辩论设置中的偏见。


<details>
  <summary>Details</summary>
Motivation: 随着LLM-as-Judge成为人类评估的可扩展替代方案，多智能体扩展如多智能体辩论和元评判被用于提升评估质量，但这些设置中内在偏见的表现尚未充分探索。

Method: 研究系统分析了四种偏见类型（位置偏见、冗长偏见、思维链偏见和从众偏见），并在两种多智能体LLM-as-Judge框架（Multi-Agent-Debate和LLM-as-Meta-Judge）中评估这些偏见。此外，还研究了引入PINE方法作为无偏见代理的效果。

Result: 辩论框架在初始辩论后显著放大偏见，并在后续轮次中持续；元评判方法表现出更强的抵抗力。PINE方法在辩论设置中有效减少偏见，但在元评判场景中效果有限。

Conclusion: 研究全面分析了多智能体LLM-as-Judge系统中的偏见行为，强调了在协作评估设置中需要有针对性的偏见缓解策略。

Abstract: LLM-as-Judge has emerged as a scalable alternative to human evaluation,
enabling large language models (LLMs) to provide reward signals in trainings.
While recent work has explored multi-agent extensions such as multi-agent
debate and meta-judging to enhance evaluation quality, the question of how
intrinsic biases manifest in these settings remains underexplored. In this
study, we conduct a systematic analysis of four diverse bias types: position
bias, verbosity bias, chain-of-thought bias, and bandwagon bias. We evaluate
these biases across two widely adopted multi-agent LLM-as-Judge frameworks:
Multi-Agent-Debate and LLM-as-Meta-Judge. Our results show that debate
framework amplifies biases sharply after the initial debate, and this increased
bias is sustained in subsequent rounds, while meta-judge approaches exhibit
greater resistance. We further investigate the incorporation of PINE, a leading
single-agent debiasing method, as a bias-free agent within these systems. The
results reveal that this bias-free agent effectively reduces biases in debate
settings but provides less benefit in meta-judge scenarios. Our work provides a
comprehensive study of bias behavior in multi-agent LLM-as-Judge systems and
highlights the need for targeted bias mitigation strategies in collaborative
evaluation settings.

</details>


### [334] [Benchmarking and Enhancing LLM Agents in Localizing Linux Kernel Bugs](https://arxiv.org/abs/2505.19489)
*Zhenhao Zhou,Zhuochen Huang,Yike He,Chong Wang,Jiajun Wang,Yijian Wu,Xin Peng,Yiling Lou*

Main category: cs.AI

TL;DR: 该论文针对Linux内核中的错误定位（FL）问题，提出了LinuxFLBench基准测试和LinuxFL$^+$增强框架，显著提升了现有LLM代理的定位准确率。


<details>
  <summary>Details</summary>
Motivation: Linux内核作为关键系统，其错误可能导致严重后果。尽管现有LLM代理在FL任务上表现良好，但在Linux内核这种大规模、低可观测性和多影响因素的复杂环境中表现不佳，亟需改进。

Method: 论文构建了LinuxFLBench基准测试，并提出了LinuxFL$^+$框架，通过最小成本显著提升LLM代理在Linux内核中的FL准确率。

Result: 实验表明，现有LLM代理在Linux内核FL任务上表现较差（最佳文件级top-1准确率仅41.6%），而LinuxFL$^+$框架将准确率提升了7.2%-11.2%。

Conclusion: LinuxFL$^+$框架有效解决了LLM代理在Linux内核FL任务中的挑战，为复杂系统中的错误定位提供了实用解决方案。

Abstract: The Linux kernel is a critical system, serving as the foundation for numerous
systems. Bugs in the Linux kernel can cause serious consequences, affecting
billions of users. Fault localization (FL), which aims at identifying the buggy
code elements in software, plays an essential role in software quality
assurance. While recent LLM agents have achieved promising accuracy in FL on
recent benchmarks like SWE-bench, it remains unclear how well these methods
perform in the Linux kernel, where FL is much more challenging due to the
large-scale code base, limited observability, and diverse impact factors. In
this paper, we introduce LinuxFLBench, a FL benchmark constructed from
real-world Linux kernel bugs. We conduct an empirical study to assess the
performance of state-of-the-art LLM agents on the Linux kernel. Our initial
results reveal that existing agents struggle with this task, achieving a best
top-1 accuracy of only 41.6% at file level. To address this challenge, we
propose LinuxFL$^+$, an enhancement framework designed to improve FL
effectiveness of LLM agents for the Linux kernel. LinuxFL$^+$ substantially
improves the FL accuracy of all studied agents (e.g., 7.2% - 11.2% accuracy
increase) with minimal costs. Data and code are available at
https://github.com/FudanSELab/LinuxFLBench.

</details>


### [335] [Automated CAD Modeling Sequence Generation from Text Descriptions via Transformer-Based Large Language Models](https://arxiv.org/abs/2505.19490)
*Jianxing Liao,Junyan Xu,Yatao Sun,Maowen Tang,Sicheng He,Jingxian Liao,Shui Yu,Yun Li,Hongguan Xiao*

Main category: cs.AI

TL;DR: 提出了一种结合大语言模型和计算机辅助设计的语言引导框架，用于自动化工业设计中的CAD模型生成。


<details>
  <summary>Details</summary>
Motivation: 传统CAD设计存在计算效率低和精确模型生成困难的问题，需要更高效的自动化解决方案。

Method: 框架包含半自动数据标注流程、基于Transformer的CAD生成器和增强的CADLLM模型，用于优化生成序列。

Result: 实验表明，该方法在准确性和效率上优于传统方法，能够从文本提示生成复杂CAD模型。

Conclusion: 该框架为工业设计自动化提供了强大工具，显著提升了CAD模型生成的效率和精度。

Abstract: Designing complex computer-aided design (CAD) models is often time-consuming
due to challenges such as computational inefficiency and the difficulty of
generating precise models. We propose a novel language-guided framework for
industrial design automation to address these issues, integrating large
language models (LLMs) with computer-automated design (CAutoD).Through this
framework, CAD models are automatically generated from parameters and
appearance descriptions, supporting the automation of design tasks during the
detailed CAD design phase. Our approach introduces three key innovations: (1) a
semi-automated data annotation pipeline that leverages LLMs and vision-language
large models (VLLMs) to generate high-quality parameters and appearance
descriptions; (2) a Transformer-based CAD generator (TCADGen) that predicts
modeling sequences via dual-channel feature aggregation; (3) an enhanced CAD
modeling generation model, called CADLLM, that is designed to refine the
generated sequences by incorporating the confidence scores from TCADGen.
Experimental results demonstrate that the proposed approach outperforms
traditional methods in both accuracy and efficiency, providing a powerful tool
for automating industrial workflows and generating complex CAD models from
textual prompts. The code is available at
https://jianxliao.github.io/cadllm-page/

</details>


### [336] [Genome-Bench: A Scientific Reasoning Benchmark from Real-World Expert Discussions](https://arxiv.org/abs/2505.19501)
*Ming Yin,Yuanhao Qu,Dyllan Liu,Ling Yang,Le Cong,Mengdi Wang*

Main category: cs.AI

TL;DR: 该论文提出一个自动化流程和基因组领域新基准Genome-Bench，利用科学论坛数据构建3000+问答对，首次实现从科学讨论中训练LLM推理的端到端方案。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏从科学讨论中训练大语言模型进行推理的端到端解决方案，特别是在基因组工程领域需要系统化的知识整合方法。

Method: 开发自动化流程将科学论坛原始数据转化为强化学习适用的多选题格式，构建含3000+高质量问答对的Genome-Bench基准。

Result: 创建了覆盖基础生物学、实验排错、工具使用等多维度的首个科学讨论推理训练数据集，展示跨科学领域的泛化潜力。

Conclusion: 该研究为生物及其他科学领域提供了首个基于讨论数据的LLM推理训练框架，具有重要方法论创新和跨领域应用价值。

Abstract: In this short report, we present an automated pipeline tailored for the
genomics domain and introduce \textit{Genome-Bench}, a new benchmark
constructed from over a decade of scientific forum discussions on genome
engineering. Our pipeline transforms raw interactions into a reinforcement
learning friendly multiple-choice questions format, supported by 3000+ high
quality question answer pairs spanning foundational biology, experimental
troubleshooting, tool usage, and beyond. To our knowledge, this is the first
end-to-end pipeline for teaching LLMs to reason from scientific discussions,
with promising potential for generalization across scientific domains beyond
biology.

</details>


### [337] [Turing Test 2.0: The General Intelligence Threshold](https://arxiv.org/abs/2505.19550)
*Georgios Mappouras*

Main category: cs.AI

TL;DR: 该论文提出传统图灵测试不足以衡量人工通用智能(AGI)，并提出了新的AGI检测框架'Turing Tests 2.0'，包含通用智能定义、阈值标准及测试构建方法。


<details>
  <summary>Details</summary>
Motivation: 当前AI领域缺乏对AGI的明确定义和检测标准，传统图灵测试已无法满足现代大型语言模型的评估需求。

Method: 1. 提出通用智能(GI)的明确定义和阈值标准(GIT) 2. 设计新型'Turing Tests 2.0'测试框架，采用通过/失败二元判定机制

Result: 开发出可实操的AGI检测方法，并在现代AI模型上进行了实际测试验证。

Conclusion: 论文建立的标准化框架为AGI识别提供了更可靠的科学依据，推动了AI评估体系的发展。

Abstract: With the rise of artificial intelligence (A.I.) and large language models
like Chat-GPT, a new race for achieving artificial general intelligence (A.G.I)
has started. While many speculate how and when A.I. will achieve A.G.I., there
is no clear agreement on how A.G.I. can be detected in A.I. models, even when
popular tools like the Turing test (and its modern variations) are used to
measure their intelligence. In this work, we discuss why traditional methods
like the Turing test do not suffice for measuring or detecting A.G.I. and
provide a new, practical method that can be used to decide if a (computer or
any other) system has reached or surpassed A.G.I. To achieve this, we make two
new contributions. First, we present a clear definition for general
intelligence (G.I.) and set a G.I. threshold (G.I.T.) that can be used to
distinguish between systems that achieve A.G.I. and systems that do not.
Second, we present a new framework on how to construct tests that can detect if
a system has achieved G.I. in a simple, comprehensive, and clear-cut fail/pass
way. We call this novel framework the Turing Tests 2.0. We then demonstrate
real-life examples of applying tests that follow our Turing Tests 2.0 framework
on modern A.I. models.

</details>


### [338] [AMQA: An Adversarial Dataset for Benchmarking Bias of LLMs in Medicine and Healthcare](https://arxiv.org/abs/2505.19562)
*Ying Xiao,Jie Huang,Ruijuan He,Jing Xiao,Mohammad Reza Mousavi,Yepang Liu,Kezhi Li,Zhenpeng Chen,Jie M. Zhang*

Main category: cs.AI

TL;DR: 该论文提出了AMQA数据集，用于自动评估大型语言模型在医学问答中的偏见，发现即使最先进的GPT-4.1也存在显著的偏见差异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医学诊断问题上已达到专家级准确度，但其错误和偏见可能带来生命危险。目前缺乏一个一致且自动化的测试平台来测量这些偏见。

Method: 论文构建了AMQA数据集，包含4,806个医学问答对，通过多智能体框架生成多样化的对抗性描述和问题对，用于自动化、大规模的偏见评估。

Result: 测试了五个代表性的大型语言模型，发现即使表现最好的GPT-4.1，在特权群体问题上的准确率仍比非特权群体高出10个百分点以上。与现有基准CPV相比，AMQA揭示了15%更大的准确率差距。

Conclusion: AMQA数据集支持可重复研究，并推动可信赖、具有偏见意识的医疗AI发展。数据集和代码已公开。

Abstract: Large language models (LLMs) are reaching expert-level accuracy on medical
diagnosis questions, yet their mistakes and the biases behind them pose
life-critical risks. Bias linked to race, sex, and socioeconomic status is
already well known, but a consistent and automatic testbed for measuring it is
missing. To fill this gap, this paper presents AMQA -- an Adversarial Medical
Question-Answering dataset -- built for automated, large-scale bias evaluation
of LLMs in medical QA. AMQA includes 4,806 medical QA pairs sourced from the
United States Medical Licensing Examination (USMLE) dataset, generated using a
multi-agent framework to create diverse adversarial descriptions and question
pairs. Using AMQA, we benchmark five representative LLMs and find surprisingly
substantial disparities: even GPT-4.1, the least biased model tested, answers
privileged-group questions over 10 percentage points more accurately than
unprivileged ones. Compared with the existing benchmark CPV, AMQA reveals 15%
larger accuracy gaps on average between privileged and unprivileged groups. Our
dataset and code are publicly available at https://github.com/XY-Showing/AMQA
to support reproducible research and advance trustworthy, bias-aware medical
AI.

</details>


### [339] [Automated Text-to-Table for Reasoning-Intensive Table QA: Pipeline Design and Benchmarking Insights](https://arxiv.org/abs/2505.19563)
*Shi-Yu Tian,Zhi Zhou,Wei Dong,Ming Yang,Kun-Yang Yu,Zi-Jian Cheng,Lan-Zhe Guo,Yu-Feng Li*

Main category: cs.AI

TL;DR: 论文提出自动化生成表格推理任务的方法AutoT2T，构建新基准TabularGSM，揭示大模型在复杂表格问答中失败的关键原因是推理与检索/识别过程的紧密耦合。


<details>
  <summary>Details</summary>
Motivation: 现有表格问答评估方法依赖昂贵的人工标注数据且难以覆盖复杂推理场景，表格结构异质性也阻碍了对大模型表现不佳的系统分析。

Method: 提出AutoT2T自动化流水线，将数学应用题转化为表格推理任务，支持生成含噪声的表格变体，并构建TabularGSM基准系统覆盖不同复杂度表格和陷阱问题。

Result: 实验表明，推理过程与检索/识别能力的紧密耦合是大模型在复杂表格问答中失败的主要原因。

Conclusion: 模型需要发展协同推理能力才能在复杂表格问答任务中有效表现。

Abstract: Reasoning with tabular data holds increasing importance in modern
applications, yet comprehensive evaluation methodologies for
reasoning-intensive Table Question Answering (QA) tasks remain nascent.
Existing research is constrained by two primary bottlenecks: 1) Reliance on
costly manually annotated real-world data, which is difficult to cover complex
reasoning scenarios; 2) The heterogeneity of table structures hinders
systematic analysis of the intrinsic mechanisms behind the underperformance of
LLMs, especially in reasoning-intensive tasks. To address these issues, we
propose an automated generation pipeline AutoT2T that transforms mathematical
word problems into table-based reasoning tasks, eliminating the need for manual
annotation. The pipeline can generate multiple variants of a table for the same
reasoning problem, including noisy versions to support robustness evaluation.
Based on this, we construct a new benchmark TabularGSM, which systematically
spans a range of table complexities and trap problems. Experimental analyses
through AutoT2T and TabularGSM reveal that the tight coupling between reasoning
and retrieval or identification processes is a key factor underlying the
failure of LLMs in complex Table QA tasks. This highlights the necessity for
models to develop synergistic reasoning capabilities in order to perform
effectively in complex Table QA tasks.

</details>


### [340] [LLM-Agent-Controller: A Universal Multi-Agent Large Language Model System as a Control Engineer](https://arxiv.org/abs/2505.19567)
*Rasoul Zahedifar,Sayyed Ali Mirghasemi,Mahdieh Soleymani Baghshah,Alireza Taheri*

Main category: cs.AI

TL;DR: 该研究提出了一种名为LLM-Agent-Controller的多智能体大语言模型系统，旨在解决控制工程中的广泛问题。系统通过中央控制器与多个辅助智能体的协作，结合先进技术如RAG和Chain-of-Thought推理，实现了高效、可靠的控制问题求解，无需用户具备专业知识。测试结果显示系统在通用任务中成功率达83%，且性能随模型升级而提升。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于利用多智能体大语言模型架构解决控制工程中的复杂问题，同时降低用户门槛，使非专业人士也能通过自然语言输入获得实时解决方案。

Method: 方法包括构建一个中央控制器与多个辅助智能体协同工作的系统，集成RAG、Chain-of-Thought推理、自我批评与修正等先进技术，并通过监督者进行高层决策和工作流协调。

Result: 测试结果显示，LLM-Agent-Controller在通用任务中成功率达83%，单个智能体平均成功率为87%，且使用更先进的大语言模型能进一步提升性能。

Conclusion: 研究表明，多智能体大语言模型架构能有效解决特定领域的复杂问题。LLM-Agent-Controller通过智能体协作、监督控制和先进推理技术，提供了一个可扩展、鲁棒且易用的解决方案框架，可推广至其他技术领域。

Abstract: This study presents the LLM-Agent-Controller, a multi-agent large language
model (LLM) system developed to address a wide range of problems in control
engineering (Control Theory). The system integrates a central controller agent
with multiple specialized auxiliary agents, responsible for tasks such as
controller design, model representation, control analysis, time-domain
response, and simulation. A supervisor oversees high-level decision-making and
workflow coordination, enhancing the system's reliability and efficiency. The
LLM-Agent-Controller incorporates advanced capabilities, including
Retrieval-Augmented Generation (RAG), Chain-of-Thought reasoning,
self-criticism and correction, efficient memory handling, and user-friendly
natural language communication. It is designed to function without requiring
users to have prior knowledge of Control Theory, enabling them to input
problems in plain language and receive complete, real-time solutions. To
evaluate the system, we propose new performance metrics assessing both
individual agents and the system as a whole. We test five categories of Control
Theory problems and benchmark performance across three advanced LLMs.
Additionally, we conduct a comprehensive qualitative conversational analysis
covering all key services. Results show that the LLM-Agent-Controller
successfully solved 83% of general tasks, with individual agents achieving an
average success rate of 87%. Performance improved with more advanced LLMs. This
research demonstrates the potential of multi-agent LLM architectures to solve
complex, domain-specific problems. By integrating specialized agents,
supervisory control, and advanced reasoning, the LLM-Agent-Controller offers a
scalable, robust, and accessible solution framework that can be extended to
various technical domains.

</details>


### [341] [MSD-LLM: Predicting Ship Detention in Port State Control Inspections with Large Language Model](https://arxiv.org/abs/2505.19568)
*Jiongchao Jin,Xiuju Fu,Xiaowei Gao,Tao Cheng,Ran Yan*

Main category: cs.AI

TL;DR: 提出MSD-LLM模型，结合双鲁棒子空间恢复自编码器和大型语言模型，提升港口国监督船舶滞留预测准确率。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在船舶滞留预测中表现受限，且数据不平衡问题影响深度学习效果，需改进预测准确性和鲁棒性。

Method: 整合基于双鲁棒子空间恢复的自编码器和渐进学习流程处理不平衡数据，利用大型语言模型进行特征分组排序和动态阈值预测。

Result: 在亚太地区31,707条检查记录上测试，MSD-LLM的新加坡港口AUC指标优于现有方法12%以上。

Conclusion: MSD-LLM能有效应对实际挑战，适用于多样化海事风险评估场景。

Abstract: Maritime transportation is the backbone of global trade, making ship
inspection essential for ensuring maritime safety and environmental protection.
Port State Control (PSC), conducted by national ports, enforces compliance with
safety regulations, with ship detention being the most severe consequence,
impacting both ship schedules and company reputations. Traditional machine
learning methods for ship detention prediction are limited by the capacity of
representation learning and thus suffer from low accuracy. Meanwhile,
autoencoder-based deep learning approaches face challenges due to the severe
data imbalance in learning historical PSC detention records. To address these
limitations, we propose Maritime Ship Detention with Large Language Models
(MSD-LLM), integrating a dual robust subspace recovery (DSR) layer-based
autoencoder with a progressive learning pipeline to handle imbalanced data and
extract meaningful PSC representations. Then, a large language model groups and
ranks features to identify likely detention cases, enabling dynamic
thresholding for flexible detention predictions. Extensive evaluations on
31,707 PSC inspection records from the Asia-Pacific region show that MSD-LLM
outperforms state-of-the-art methods more than 12\% on Area Under the Curve
(AUC) for Singapore ports. Additionally, it demonstrates robustness to
real-world challenges, making it adaptable to diverse maritime risk assessment
scenarios.

</details>


### [342] [Think Again! The Effect of Test-Time Compute on Preferences, Opinions, and Beliefs of Large Language Models](https://arxiv.org/abs/2505.19621)
*George Kour,Itay Nakash,Ateret Anaby-Tavor,Michal Shmueli-Scheuer*

Main category: cs.AI

TL;DR: 该论文提出了一个名为POBs的基准测试，用于评估大型语言模型在多个领域的主观倾向，发现新版本模型一致性降低且偏见增加。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）深入人类生活并影响决策，评估其是否及在何种程度上表现出主观偏好、观点和信仰变得至关重要。这些倾向可能源自模型内部的偏见，影响其行为及向用户提供的建议，甚至可能强化某些观点。

Method: 论文提出了Preference, Opinion, and Belief survey (POBs)基准，用于评估LLMs在社会、文化、伦理和个人领域的主观倾向，并测试了增加推理和自我反思机制对这些指标的影响。

Result: 研究发现，尽管推理和自我反思机制在其他任务中有效，但在本领域仅带来有限的改进。此外，新版本模型的一致性降低，且对特定观点的偏见增加。

Conclusion: 论文揭示了LLMs在主观倾向方面的一个盲点和令人担忧的趋势，强调了进一步研究和改进的必要性。

Abstract: As Large Language Models (LLMs) become deeply integrated into human life and
increasingly influence decision-making, it's crucial to evaluate whether and to
what extent they exhibit subjective preferences, opinions, and beliefs. These
tendencies may stem from biases within the models, which may shape their
behavior, influence the advice and recommendations they offer to users, and
potentially reinforce certain viewpoints. This paper presents the Preference,
Opinion, and Belief survey (POBs), a benchmark developed to assess LLMs'
subjective inclinations across societal, cultural, ethical, and personal
domains. We applied our benchmark to evaluate leading open- and closed-source
LLMs, measuring desired properties such as reliability, neutrality, and
consistency. In addition, we investigated the effect of increasing the
test-time compute, through reasoning and self-reflection mechanisms, on those
metrics. While effective in other tasks, our results show that these mechanisms
offer only limited gains in our domain. Furthermore, we reveal that newer model
versions are becoming less consistent and more biased toward specific
viewpoints, highlighting a blind spot and a concerning trend. POBS:
https://ibm.github.io/POBS

</details>


### [343] [SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond](https://arxiv.org/abs/2505.19641)
*Junteng Liu,Yuanxiang Fan,Zhuo Jiang,Han Ding,Yongyi Hu,Chi Zhang,Yiqi Shi,Shitong Weng,Aili Chen,Shiqi Chen,Yunan Huang,Mozhi Zhang,Pengyu Zhao,Junjie Yan,Junxian He*

Main category: cs.AI

TL;DR: SynLogic是一个大规模生成多样化逻辑推理数据的框架和数据集，旨在通过强化学习提升大语言模型的通用推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前开源复制工作主要集中在数学和编程领域，而开发通用推理能力的方法和资源仍未被充分探索。逻辑推理被认为是通用推理能力的基础，但收集适合强化学习的多样化且可验证的推理数据具有挑战性。

Method: 提出SynLogic框架和数据集，生成35种多样化逻辑推理任务的数据，支持调整难度和数量，所有示例均可通过简单规则验证。

Result: 在7B和32B模型上验证了SynLogic的有效性，其逻辑推理性能在开源数据集中达到最先进水平，并在混合训练中显著提升了数学和编程任务的训练效率和推理泛化能力。

Conclusion: SynLogic是提升大语言模型通用推理能力的宝贵资源，其数据合成流程和数据集已开源。

Abstract: Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the
potential of Reinforcement Learning (RL) to enhance reasoning abilities in
Large Language Models (LLMs). While open-source replication efforts have
primarily focused on mathematical and coding domains, methods and resources for
developing general reasoning capabilities remain underexplored. This gap is
partly due to the challenge of collecting diverse and verifiable reasoning data
suitable for RL. We hypothesize that logical reasoning is critical for
developing general reasoning capabilities, as logic forms a fundamental
building block of reasoning. In this work, we present SynLogic, a data
synthesis framework and dataset that generates diverse logical reasoning data
at scale, encompassing 35 diverse logical reasoning tasks. The SynLogic
approach enables controlled synthesis of data with adjustable difficulty and
quantity. Importantly, all examples can be verified by simple rules, making
them ideally suited for RL with verifiable rewards. In our experiments, we
validate the effectiveness of RL training on the SynLogic dataset based on 7B
and 32B models. SynLogic leads to state-of-the-art logical reasoning
performance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B
by 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and
coding tasks improves the training efficiency of these domains and
significantly enhances reasoning generalization. Notably, our mixed training
model outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These
findings position SynLogic as a valuable resource for advancing the broader
reasoning capabilities of LLMs. We open-source both the data synthesis pipeline
and the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic.

</details>


### [344] [Token-Importance Guided Direct Preference Optimization](https://arxiv.org/abs/2505.19653)
*Yang Ning,Lin Hai,Liu Yibo,Tian Baoliang,Liu Guoqing,Zhang Haijun*

Main category: cs.AI

TL;DR: 论文提出TI-DPO方法，通过动态权重和三元损失优化语言模型输出，提高准确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视单个token的重要性且对偏好数据中的噪声敏感，需改进。

Method: 引入梯度动态权重和三元损失，指导模型接近人类偏好输出。

Result: TI-DPO在准确性和生成多样性上优于DPO及其他RLHF方法。

Conclusion: TI-DPO提供更稳定、高效的解决方案，优化了语言模型输出。

Abstract: Ensuring that large language models (LLMs) generate outputs aligned with
human preferences is important for safe and effective AI interactions. While
Direct Preference Optimization (DPO) employs an implicit reward function to
optimize the policy model, however, it and its related variants overlook the
differential importance of individual tokens and are sensitive to judgment
noise in preference datasets during generation. Although recent methods attempt
to assess the important weight of tokens via probability prediction or
simplistic weighting schemes, these evaluation methods are prone to biases and
still cannot fully address these issues. To solve this problem, we propose the
Token-Importance Guided Direct Preference Optimization (TI-DPO), which
introduces two key innovations: the gradient-based token-importance weights
that dynamically prioritize critical tokens, and a triple loss that explicitly
guides model outputs to approach human-preferred responses and stay away from
non-preferred responses. Experimental results show that TI-DPO achieves higher
accuracy and stronger generative diversity, providing more stable and
computationally efficient solutions compared with DPO and other RLHF methods.

</details>


### [345] [FieldWorkArena: Agentic AI Benchmark for Real Field Work Tasks](https://arxiv.org/abs/2505.19662)
*Atsunori Moteki,Shoichi Masui,Fan Yang,Yueqi Song,Yonatan Bisk,Graham Neubig,Ikuo Kusajima,Yasuto Watanabe,Hiroyuki Ishida,Jun Takahashi,Shan Jiang*

Main category: cs.AI

TL;DR: 该论文提出了FieldWorkArena基准测试，用于评估面向真实世界现场工作的智能代理AI，解决了现有基准测试局限于网络任务的问题。


<details>
  <summary>Details</summary>
Motivation: 随着对智能代理AI需求的增加，现有基准测试局限于评估网络任务，无法满足真实工作环境中复杂任务的需求。

Method: 定义了新的动作空间，改进了评估函数，并使用现场拍摄的视频和工厂文档创建任务。

Result: 验证了考虑多模态LLM特性的性能评估可行性，并识别了新评估方法的有效性和局限性。

Conclusion: FieldWorkArena为真实工作环境中的智能代理AI提供了有效的评估工具，数据集和评估程序已公开。

Abstract: This paper proposes FieldWorkArena, a benchmark for agentic AI targeting
real-world field work. With the recent increase in demand for agentic AI, they
are required to monitor and report safety and health incidents, as well as
manufacturing-related incidents, that may occur in real-world work
environments. Existing agentic AI benchmarks have been limited to evaluating
web tasks and are insufficient for evaluating agents in real-world work
environments, where complexity increases significantly. In this paper, we
define a new action space that agentic AI should possess for real world work
environment benchmarks and improve the evaluation function from previous
methods to assess the performance of agentic AI in diverse real-world tasks.
The dataset consists of videos captured on-site and documents actually used in
factories and warehouses, and tasks were created based on interviews with
on-site workers and managers. Evaluation results confirmed that performance
evaluation considering the characteristics of Multimodal LLM (MLLM) such as
GPT-4o is feasible. Additionally, the effectiveness and limitations of the
proposed new evaluation method were identified. The complete dataset
(HuggingFace) and evaluation program (GitHub) can be downloaded from the
following website:
https://en-documents.research.global.fujitsu.com/fieldworkarena/.

</details>


### [346] [Large Language Models' Reasoning Stalls: An Investigation into the Capabilities of Frontier Models](https://arxiv.org/abs/2505.19676)
*Lachlan McGinness,Peter Baumgartner*

Main category: cs.AI

TL;DR: 研究评估了大型语言模型（LLM）在自动定理证明（ATP）推理策略上的能力，发现其推理能力进展停滞，改进主要源于隐藏系统提示或自动使用思维链策略。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在自动定理证明推理策略上的能力，评估其性能进展及改进来源。

Method: 使用PRONTOQA steamroller推理问题评估2023年12月和2024年8月的先进模型，开发了评估LLM响应准确性和正确答案相关性的方法。

Result: 发现LLM推理能力进展停滞，改进主要源于隐藏系统提示或自动使用思维链策略；前沿LLM最擅长遵循自底向上策略，正确推理与正确结论之间相关性较低。

Conclusion: 当前LLM在ATP推理策略上的能力改进有限，自底向上策略表现最佳，但推理与结论的正确性相关性不高。

Abstract: Empirical methods to examine the capability of Large Language Models (LLMs)
to use Automated Theorem Prover (ATP) reasoning strategies are studied. We
evaluate the performance of State of the Art models from December 2023 and
August 2024 on PRONTOQA steamroller reasoning problems. For that, we develop
methods for assessing LLM response accuracy and correct answer correlation.
  Our results show that progress in improving LLM reasoning abilities has
stalled over the nine month period. By tracking completion tokens, we show that
almost all improvement in reasoning ability since GPT-4 was released can be
attributed to either hidden system prompts or the training of models to
automatically use generic Chain of Thought prompting strategies. Among the ATP
reasoning strategies tried, we found that current frontier LLMs are best able
to follow the bottom-up (also known as forward-chaining) strategy. A low
positive correlation was found between an LLM response containing correct
reasoning and arriving at the correct conclusion.

</details>


### [347] [Large Language Models for Planning: A Comprehensive and Systematic Survey](https://arxiv.org/abs/2505.19683)
*Pengfei Cao,Tianyi Men,Wencan Liu,Jingwen Zhang,Xuzhao Li,Xixun Lin,Dianbo Sui,Yanan Cao,Kang Liu,Jun Zhao*

Main category: cs.AI

TL;DR: 本文综述了基于大语言模型（LLM）的规划方法，分类探讨了三种主要方法，总结了现有评估框架，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 规划是智能体的核心能力，涉及环境理解、逻辑推理和序列决策。尽管LLM在某些规划任务中表现优异，但其广泛应用仍需系统研究。

Method: 论文首先建立理论基础，介绍自动规划的定义和分类；随后详细分类并分析当代LLM规划方法，分为外部模块增强、微调基础和搜索基础三类；最后总结评估框架。

Result: 系统总结了现有LLM规划方法及其评估框架，包括基准数据集、评估指标和代表性方法的性能比较。

Conclusion: 本文为基于LLM的规划研究提供了全面综述，旨在激发创新并推动这一快速发展领域的进步。

Abstract: Planning represents a fundamental capability of intelligent agents, requiring
comprehensive environmental understanding, rigorous logical reasoning, and
effective sequential decision-making. While Large Language Models (LLMs) have
demonstrated remarkable performance on certain planning tasks, their broader
application in this domain warrants systematic investigation. This paper
presents a comprehensive review of LLM-based planning. Specifically, this
survey is structured as follows: First, we establish the theoretical
foundations by introducing essential definitions and categories about automated
planning. Next, we provide a detailed taxonomy and analysis of contemporary
LLM-based planning methodologies, categorizing them into three principal
approaches: 1) External Module Augmented Methods that combine LLMs with
additional components for planning, 2) Finetuning-based Methods that involve
using trajectory data and feedback signals to adjust LLMs in order to improve
their planning abilities, and 3) Searching-based Methods that break down
complex tasks into simpler components, navigate the planning space, or enhance
decoding strategies to find the best solutions. Subsequently, we systematically
summarize existing evaluation frameworks, including benchmark datasets,
evaluation metrics and performance comparisons between representative planning
methods. Finally, we discuss the underlying mechanisms enabling LLM-based
planning and outline promising research directions for this rapidly evolving
field. We hope this survey will serve as a valuable resource to inspire
innovation and drive progress in this field.

</details>


### [348] [Beyond Safe Answers: A Benchmark for Evaluating True Risk Awareness in Large Reasoning Models](https://arxiv.org/abs/2505.19690)
*Baihui Zheng,Boren Zheng,Kerui Cao,Yingshui Tan,Zhendong Liu,Weixun Wang,Jiaheng Liu,Jian Yang,Wenbo Su,Xiaoyong Zhu,Bo Zheng,Kaifu Zhang*

Main category: cs.AI

TL;DR: 论文提出大型推理模型(LRMs)存在表面安全对齐(SSA)问题，即输出看似安全但内部推理未真正识别风险。作者开发了BSA基准测试，评估19个先进LRMs发现其风险识别准确率仅38%，并探索了缓解SSA的方法。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注响应层面的安全性，忽视了模型内部推理过程可能未能真正检测和缓解潜在风险的问题，即表面安全对齐(SSA)现象。这在安全关键场景中可能导致不一致的安全行为。

Method: 引入Beyond Safe Answers (BSA)基准，包含2,000个挑战性实例，分为三类SSA场景和九种风险类别，每个实例都详细标注了风险原理。评估了19个最先进的LRMs，并探索了安全规则、安全推理数据微调和多样化解码策略对缓解SSA的效果。

Result: 评估显示当前最先进的LRMs在正确识别风险原理方面表现不佳，最佳模型准确率仅为38.0%。研究表明现有模型在安全推理保真度方面存在显著不足。

Conclusion: 该工作提供了一个全面的评估工具来检验和改进LRMs的安全推理保真度，推动了真正风险感知和可靠安全AI系统的发展。研究表明需要更深入的安全对齐方法来解决SSA问题。

Abstract: Despite the remarkable proficiency of \textit{Large Reasoning Models} (LRMs)
in handling complex reasoning tasks, their reliability in safety-critical
scenarios remains uncertain. Existing evaluations primarily assess
response-level safety, neglecting a critical issue we identify as
\textbf{\textit{Superficial Safety Alignment} (SSA)} -- a phenomenon where
models produce superficially safe outputs while internal reasoning processes
fail to genuinely detect and mitigate underlying risks, resulting in
inconsistent safety behaviors across multiple sampling attempts. To
systematically investigate SSA, we introduce \textbf{Beyond Safe Answers (BSA)}
bench, a novel benchmark comprising 2,000 challenging instances organized into
three distinct SSA scenario types and spanning nine risk categories, each
meticulously annotated with risk rationales. Evaluations of 19 state-of-the-art
LRMs demonstrate the difficulty of this benchmark, with top-performing models
achieving only 38.0\% accuracy in correctly identifying risk rationales. We
further explore the efficacy of safety rules, specialized fine-tuning on safety
reasoning data, and diverse decoding strategies in mitigating SSA. Our work
provides a comprehensive assessment tool for evaluating and improving safety
reasoning fidelity in LRMs, advancing the development of genuinely risk-aware
and reliably safe AI systems.

</details>


### [349] [Concise Reasoning, Big Gains: Pruning Long Reasoning Trace with Difficulty-Aware Prompting](https://arxiv.org/abs/2505.19716)
*Yifan Wu,Jingze Shi,Bingheng Wu,Jiayi Zhang,Xiaotian Lin,Nan Tang,Yuyu Luo*

Main category: cs.AI

TL;DR: 论文提出了一种难度感知提示（DAP）方法，动态缩短推理轨迹而不损失性能，并创建了LiteCoT数据集，训练出的模型在减少推理成本的同时表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有的思维链（CoT）蒸馏方法存在推理轨迹冗长和无法适应问题难度的问题，导致推理成本高且模型无法学习适应性推理策略。

Method: 提出难度感知提示（DAP）方法，通过教师模型判断问题难度并重写推理轨迹为更短长度，创建了包含10万简洁推理样本的LiteCoT数据集。

Result: 实验表明，使用LiteCoT训练的模型在11个基准测试中表现优于传统方法，推理成本显著降低，例如在AIME24考试中达到74.2%的Pass@1。

Conclusion: DAP方法和LiteCoT数据集有效解决了推理轨迹冗长和适应性问题，显著提升了模型性能和效率。

Abstract: Existing chain-of-thought (CoT) distillation methods can effectively transfer
reasoning abilities to base models but suffer from two major limitations:
excessive verbosity of reasoning traces and inadequate adaptability to problem
difficulty. Long reasoning traces significantly increase inference costs, and
uniform-length solutions prevent base models from learning adaptive reasoning
strategies. To address these issues, we propose a difficulty-aware prompting
(DAP) method to dynamically shorten reasoning traces without performance loss.
In our approach, a large teacher model first judges each problem's difficulty
and then rewrites its reasoning traces to an appropriate shorter length,
yielding concise yet complete reasoning traces. Leveraging the DAP pipeline, we
curate a distilled dataset called LiteCoT consisting of 100K concise reasoning
examples, with solutions averaging only 720 tokens (an order of magnitude
shorter than typical CoTs). Using LiteCoT, we distilled a new family of
reasoning models called Liter (1.5B, 7B, and 32B) based on the Qwen2.5
architecture. Experiments show that a student model fine-tuned on just 100K of
these difficulty-pruned CoT samples outperforms a model distilled on 800K
original Long CoT samples, while significantly reducing training and inference
costs. Our method also generalizes well: across 11 diverse benchmarks, the
shorter difficulty-aware CoTs achieve equal or better accuracy than Long
chains, using far fewer tokens. For example, on the challenging AIME24 exam,
our approach reaches $74.2\%$ Pass@1 using only about 5K inference tokens,
surpassing other methods that consume many more tokens. Our code and data are
available at https://github.com/Evanwu1125/LiteCoT.

</details>


### [350] [ReChisel: Effective Automatic Chisel Code Generation by LLM with Reflection](https://arxiv.org/abs/2505.19734)
*Juxin Niu,Xiangfeng Liu,Dan Niu,Xi Wang,Zhe Jiang,Nan Guan*

Main category: cs.AI

TL;DR: ReChisel是一个基于大语言模型（LLM）的系统，旨在提高Chisel代码生成的效率，通过反射机制和逃逸机制优化生成代码质量。


<details>
  <summary>Details</summary>
Motivation: 传统硬件描述语言（HDL）如Verilog的编码过程耗时且复杂，而新一代HDL Chisel虽然提供了更高级的抽象，但LLM在Chisel代码生成中的应用尚未充分探索。

Method: ReChisel结合了反射机制，通过编译和模拟过程的反馈迭代优化代码，并引入逃逸机制以避免非进展循环。

Result: 实验表明，ReChisel显著提高了Chisel代码生成的成功率，性能与最先进的基于LLM的Verilog代码生成系统相当。

Conclusion: ReChisel展示了LLM在Chisel代码生成中的潜力，为硬件设计提供了更高效的工具。

Abstract: Coding with hardware description languages (HDLs) such as Verilog is a
time-intensive and laborious task. With the rapid advancement of large language
models (LLMs), there is increasing interest in applying LLMs to assist with HDL
coding. Recent efforts have demonstrated the potential of LLMs in translating
natural language to traditional HDL Verilog. Chisel, a next-generation HDL
based on Scala, introduces higher-level abstractions, facilitating more
concise, maintainable, and scalable hardware designs. However, the potential of
using LLMs for Chisel code generation remains largely unexplored. This work
proposes ReChisel, an LLM-based agentic system designed to enhance the
effectiveness of Chisel code generation. ReChisel incorporates a reflection
mechanism to iteratively refine the quality of generated code using feedback
from compilation and simulation processes, and introduces an escape mechanism
to break free from non-progress loops. Experiments demonstrate that ReChisel
significantly improves the success rate of Chisel code generation, achieving
performance comparable to state-of-the-art LLM-based agentic systems for
Verilog code generation.

</details>


### [351] [Divide and Conquer: Grounding LLMs as Efficient Decision-Making Agents via Offline Hierarchical Reinforcement Learning](https://arxiv.org/abs/2505.19761)
*Zican Hu,Wei Liu,Xiaoye Qu,Xiangyu Yue,Chunlin Chen,Zhi Wang,Yu Cheng*

Main category: cs.AI

TL;DR: 论文提出GLIDER框架，通过分层强化学习提升大语言模型在长时决策任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在稀疏奖励的长时决策任务中因探索不足和长期信用分配困难而表现不佳。

Method: 采用分层强化学习框架，高层策略指导低层控制器执行分步计划，分解复杂任务为连贯子任务。

Result: 在ScienceWorld和ALFWorld基准测试中，GLIDER实现了性能提升并增强了泛化能力。

Conclusion: GLIDER通过任务无关的低层技能和高效探索机制，显著提升了大语言模型在长时决策任务中的表现。

Abstract: While showing sophisticated reasoning abilities, large language models (LLMs)
still struggle with long-horizon decision-making tasks due to deficient
exploration and long-term credit assignment, especially in sparse-reward
scenarios. Inspired by the divide-and-conquer principle, we propose an
innovative framework **GLIDER** (**G**rounding **L**anguage Models as
Eff**I**cient **D**ecision-Making Agents via Offline Hi**E**rarchical
**R**einforcement Learning) that introduces a parameter-efficient and generally
applicable hierarchy to LLM policies. We develop a scheme where the low-level
controller is supervised with abstract, step-by-step plans that are learned and
instructed by the high-level policy. This design decomposes complicated
problems into a series of coherent chain-of-thought reasoning sub-tasks,
providing flexible temporal abstraction to significantly enhance exploration
and learning for long-horizon tasks. Furthermore, GLIDER facilitates fast
online adaptation to non-stationary environments owing to the strong
transferability of its task-agnostic low-level skills. Experiments on
ScienceWorld and ALFWorld benchmarks show that GLIDER achieves consistent
performance gains, along with enhanced generalization capabilities.

</details>


### [352] [Language Model-Enhanced Message Passing for Heterophilic Graph Learning](https://arxiv.org/abs/2505.19762)
*Wenjun Wang,Dawei Cheng*

Main category: cs.AI

TL;DR: 提出LEMP4HG方法，利用语言模型增强消息传递，解决异质图学习问题，并在异质图和同质图上均表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统GNN依赖同质性消息传递，在异质图（相连节点特征和标签不同）上表现不佳。现有方法常忽略节点文本的语义潜力，或传播次优消息表示，影响性能。

Method: 提出LEMP4HG方法：通过语言模型生成节点文本的连接分析，编码后与节点文本嵌入通过门控机制融合，生成语义丰富的消息。引入MVRD主动学习策略，选择性增强消息传递困难的节点对。

Result: 实验验证LEMP4HG在异质图上表现优异，同时在同质图上保持稳健性能。

Conclusion: LEMP4HG通过语言模型增强消息传递，有效解决异质图学习问题，兼顾同质图性能，具有实用价值。

Abstract: Traditional graph neural networks (GNNs), which rely on homophily-driven
message passing, struggle with heterophilic graphs where connected nodes
exhibit dissimilar features and different labels. While existing methods
address heterophily through graph structure refinement or adaptation of
neighbor aggregation functions, they often overlook the semantic potential of
node text, rely on suboptimal message representation for propagation and
compromise performance on homophilic graphs. To address these limitations, we
propose a novel language model (LM)-enhanced message passing approach for
heterophilic graph leaning (LEMP4HG). Specifically, in the context of
text-attributed graph, we provide paired node texts for LM to generate their
connection analysis, which are encoded and then fused with paired node textual
embeddings through a gating mechanism. The synthesized messages are
semantically enriched and adaptively balanced with both nodes' information,
which mitigates contradictory signals when neighbor aggregation in heterophilic
regions. Furthermore, we introduce an active learning strategy guided by our
heuristic MVRD (Modulated Variation of Reliable Distance), selectively
enhancing node pairs suffer most from message passing, reducing the cost of
analysis generation and side effects on homophilic regions. Extensive
experiments validate that our approach excels on heterophilic graphs and
performs robustly on homophilic ones, with a graph convolutional network (GCN)
backbone and a practical budget.

</details>


### [353] [Done Is Better than Perfect: Unlocking Efficient Reasoning by Structured Multi-Turn Decomposition](https://arxiv.org/abs/2505.19788)
*Zihao Zeng,Xuyao Huang,Boxiu Li,Hao Zhang,Zhijie Deng*

Main category: cs.AI

TL;DR: 论文提出Multi-Turn Decomposition (MinD)方法，通过将传统思维链分解为多轮显式交互，显著减少大模型推理时的令牌消耗和首令牌延迟，同时保持推理性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型(LRMs)因思维链(CoT)过长导致高延迟，且传统CoT中思维单元无法显式管理，难以优化效率。

Method: 采用监督微调(SFT)+强化学习(RL)范式：1) 用LLM将LRM输出重构成多轮格式；2) 用GRPO等RL算法优先选择正确且轮次少的输出。

Result: 在MATH等基准测试中，MinD减少70%令牌使用和首令牌延迟，同时保持MATH-500等推理任务的竞争力。

Conclusion: MinD通过结构化多轮交互实现高效可控推理，为优化大模型推理效率提供新思路。

Abstract: Large Reasoning Models (LRMs) are criticized for the excessively lengthy
Chain-of-Thought (CoT) to derive the final answer, suffering from high
first-token and overall latency. Typically, the CoT of LRMs mixes multiple
thinking units; each unit attempts to produce a candidate answer to the
original query. Hence, a natural idea to improve efficiency is to reduce the
unit number. Yet, the fact that the thinking units in vanilla CoT cannot be
explicitly managed renders doing so challenging. This paper introduces
Multi-Turn Decomposition (MinD) to decode conventional CoT into a sequence of
explicit, structured, and turn-wise interactions to bridge the gap. In MinD,
the model provides a multi-turn response to the query, where each turn embraces
a thinking unit and yields a corresponding answer. The subsequent turns can
reflect, verify, revise, or explore alternative approaches to both the thinking
and answer parts of earlier ones. This not only makes the answer delivered more
swiftly, but also enables explicit controls over the iterative reasoning
process (i.e., users may halt or continue at any turn). We follow a supervised
fine-tuning (SFT) then reinforcement learning (RL) paradigm to realize MinD. We
first rephrase the outputs of an LRM into multi-turn formats by prompting
another LLM, and then tune the LRM with such data. Observing that the tuned
model tends to consume even more tokens than the original one (probably due to
that the multi-turn formats introduce additional answer tokens), we advocate
leveraging RL algorithms like GRPO to prioritize correct outputs with fewer
turns. Trained on the MATH dataset using R1-Distill models, MinD can achieve up
to ~70% reduction in both output token usage and time to first token (TTFT),
while maintaining competitive performance on reasoning benchmarks such as
MATH-500, AIME24, AMC23, and GPQA-Diamond.

</details>


### [354] [Types of Relations: Defining Analogies with Category Theory](https://arxiv.org/abs/2505.19792)
*Claire Ott,Frank Jäkel*

Main category: cs.AI

TL;DR: 该论文探讨了如何通过范畴论形式化知识领域，以构建和评估类比，并以太阳系与氢原子的类比为例展示了方法。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过类比有效地表示和转移知识，以支持人类和机器的智能行为。

Method: 使用范畴论形式化知识领域，并通过函子、拉回和推出等数学工具定义类比及其核心。

Result: 成功构建了太阳系与氢原子的类比，并展示了如何通过范畴论工具描述类比的核心和混合领域。

Conclusion: 范畴论为构建和评估类比提供了有效的数学框架，有助于知识表示和转移。

Abstract: In order to behave intelligently both humans and machines have to represent
their knowledge adequately for how it is used. Humans often use analogies to
transfer their knowledge to new domains, or help others with this transfer via
explanations. Hence, an important question is: What representation can be used
to construct, find, and evaluate analogies? In this paper, we study features of
a domain that are important for constructing analogies. We do so by formalizing
knowledge domains as categories. We use the well-known example of the analogy
between the solar system and the hydrogen atom to demonstrate how to construct
domain categories. We also show how functors, pullbacks, and pushouts can be
used to define an analogy, describe its core and a corresponding blend of the
underlying domains.

</details>


### [355] [DGRAG: Distributed Graph-based Retrieval-Augmented Generation in Edge-Cloud Systems](https://arxiv.org/abs/2505.19847)
*Wenqing Zhou,Yuxuan Yan,Qianqian Yang*

Main category: cs.AI

TL;DR: 论文提出了一种分布式知识图谱增强的RAG方法（DGRAG），通过在边缘-云系统中分散存储和处理知识，解决了传统RAG的隐私、计算成本和延迟问题。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法集中存储海量分散数据，面临隐私担忧、高计算成本和大规模知识库检索延迟的问题。

Method: DGRAG采用边缘-云系统，边缘设备维护本地知识库并生成子图摘要共享至云端；通过门控机制判断查询是否超出本地范围，超出时云端基于摘要检索最相关知识并生成答案。

Result: 实验证明DGRAG显著提升了问答任务的质量，优于基线方法。

Conclusion: DGRAG通过分布式知识构建与协作检索生成，有效平衡了隐私、效率与准确性，为RAG提供了可行解决方案。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising approach to
enhance the capabilities of language models by integrating external knowledge.
Due to the diversity of data sources and the constraints of memory and
computing resources, real-world data is often scattered in multiple devices.
Conventional RAGs that store massive amounts of scattered data centrally face
increasing privacy concerns and high computational costs. Additionally, RAG in
a central node raises latency issues when searching over a large-scale
knowledge base. To address these challenges, we propose a distributed Knowledge
Graph-based RAG approach, referred to as DGRAG, in an edge-cloud system, where
each edge device maintains a local knowledge base without the need to share it
with the cloud, instead sharing only summaries of its knowledge. Specifically,
DGRAG has two main phases. In the Distributed Knowledge Construction phase,
DGRAG organizes local knowledge using knowledge graphs, generating subgraph
summaries and storing them in a summary database in the cloud as information
sharing. In the Collaborative Retrieval and Generation phase, DGRAG first
performs knowledge retrieval and answer generation locally, and a gate
mechanism determines whether the query is beyond the scope of local knowledge
or processing capabilities. For queries that exceed the local knowledge scope,
the cloud retrieves knowledge from the most relevant edges based on the
summaries and generates a more precise answer. Experimental results demonstrate
the effectiveness of the proposed DGRAG approach in significantly improving the
quality of question-answering tasks over baseline approaches.

</details>


### [356] [HS-STAR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation](https://arxiv.org/abs/2505.19866)
*Feng Xiong,Hongling Xu,Yifei Wang,Runxi Cheng,Yong Wang,Xiangxiang Chu*

Main category: cs.AI

TL;DR: HS-STaR提出了一种分层采样框架，通过动态分配采样预算至边界难度问题，显著提升LLMs的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法对所有问题采用均匀采样预算，忽略了不同难度问题的效用差异，边界难度问题具有更高学习价值。

Method: HS-STaR采用奖励引导的难度估计策略进行轻量预采样，识别边界问题后动态重分配预算以最大化高质量数据生成。

Result: 在多个推理基准测试中，HS-STaR在不增加采样预算的情况下显著优于基线方法。

Conclusion: HS-STaR通过层次化采样策略有效提升了自学习推理器的数据效用和模型性能。

Abstract: Self-taught reasoners (STaRs) enhance the mathematical reasoning abilities of
large language models (LLMs) by leveraging self-generated responses for
self-training. Recent studies have incorporated reward models to guide response
selection or decoding, aiming to obtain higher-quality data. However, they
typically allocate a uniform sampling budget across all problems, overlooking
the varying utility of problems at different difficulty levels. In this work,
we conduct an empirical study and find that problems near the boundary of the
LLM's reasoning capability offer significantly greater learning utility than
both easy and overly difficult ones. To identify and exploit such problems, we
propose HS-STaR, a Hierarchical Sampling framework for Self-Taught Reasoners.
Given a fixed sampling budget, HS-STaR first performs lightweight pre-sampling
with a reward-guided difficulty estimation strategy to efficiently identify
boundary-level problems. Subsequently, it dynamically reallocates the remaining
budget toward these high-utility problems during a re-sampling phase,
maximizing the generation of valuable training data. Extensive experiments
across multiple reasoning benchmarks and backbone LLMs demonstrate that HS-STaR
significantly outperforms other baselines without requiring additional sampling
budget.

</details>


### [357] [Unifying Multimodal Large Language Model Capabilities and Modalities via Model Merging](https://arxiv.org/abs/2505.19892)
*Yongxian Wei,Runxi Cheng,Weike Jin,Enneng Yang,Li Shen,Lu Hou,Sinan Du,Chun Yuan,Xiaochun Cao,Dacheng Tao*

Main category: cs.AI

TL;DR: 该论文提出了一个多模态大语言模型（MLLM）的模型合并基准，包含多种任务，并探索了不同模态模型的合并方法，提出了一种去噪和优化合并向量的新方法，性能平均提升2.48%。


<details>
  <summary>Details</summary>
Motivation: 基础模型由于训练资源密集更新缓慢，而领域特定模型在更新间不断演进。模型合并旨在将多个专家模型合并为一个更强大的模型，从而降低存储和服务成本，并支持分散式模型开发。然而，之前的研究主要集中在视觉分类模型或大型语言模型（LLM）的合并上，缺乏针对多模态大语言模型（MLLM）的合并研究基准。

Method: 论文引入了一个MLLM模型合并基准，包含VQA、几何、图表、OCR和接地等多种任务，提供了LoRA和全微调模型。此外，探索了不同模态（如视觉-语言、音频-语言和视频-语言模型）的合并方法，提出了一个去除任务向量噪声并基于任务向量交互定义的损失优化合并向量的新方法。

Result: 在基准上实现了10种模型合并算法，提出的新方法平均性能提升2.48%。结果表明，模型合并为构建改进的MLLM提供了一种有前景的方法，且无需数据训练。多模态之间的互补性优于单一模态。

Conclusion: 模型合并为MLLM的发展提供了一种高效且成本低廉的途径，多模态合并展现了显著的性能优势，为未来全语言模型（Omni-language model）的研究奠定了基础。

Abstract: While foundation models update slowly due to resource-intensive training
requirements, domain-specific models evolve between updates. Model merging aims
to combine multiple expert models into a single, more capable model, thereby
reducing storage and serving costs while supporting decentralized model
development. Despite its potential, previous studies have primarily focused on
merging visual classification models or Large Language Models (LLMs) for code
and math tasks. Multimodal Large Language Models (MLLMs), which extend the
capabilities of LLMs through large-scale multimodal training, have gained
traction. However, there lacks a benchmark for model merging research that
clearly divides the tasks for MLLM training and evaluation. In this paper, (i)
we introduce the model merging benchmark for MLLMs, which includes multiple
tasks such as VQA, Geometry, Chart, OCR, and Grounding, providing both LoRA and
full fine-tuning models. Moreover, we explore how model merging can combine
different modalities (e.g., vision-language, audio-language, and video-language
models), moving toward the Omni-language model. (ii) We implement 10 model
merging algorithms on the benchmark. Furthermore, we propose a novel method
that removes noise from task vectors and robustly optimizes the merged vector
based on a loss defined over task vector interactions, achieving an average
performance gain of 2.48%. (iii) We find that model merging offers a promising
way for building improved MLLMs without requiring data training. Our results
also demonstrate that the complementarity among multiple modalities outperforms
individual modalities.

</details>


### [358] [Large Language Models as Autonomous Spacecraft Operators in Kerbal Space Program](https://arxiv.org/abs/2505.19896)
*Alejandro Carrasco,Victor Rodriguez-Fernandez,Richard Linares*

Main category: cs.AI

TL;DR: 该论文探索了将大型语言模型（LLMs）作为自主代理应用于空间控制领域，特别是在非合作空间操作中的卫星机动决策。通过Kerbal Space Program Differential Games挑战，研究者开发了一个纯LLM解决方案，并获得了第二名。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于利用大型语言模型（LLMs）的自主决策能力，推动空间控制领域的发展，特别是在自主卫星操作中实现更高效的决策过程。

Method: 研究方法包括提示工程、少样本提示和微调技术，以开发一个基于LLM的自主代理，应用于Kerbal Space Program Differential Games挑战。

Result: 研究结果表明，开发的纯LLM解决方案在Kerbal Space Program Differential Games挑战中排名第二，验证了LLM在空间研究中的潜在应用价值。

Conclusion: 该研究开创了将LLM代理整合到空间研究的先河，为未来的研究和应用提供了开放资源和进一步探索的基础。

Abstract: Recent trends are emerging in the use of Large Language Models (LLMs) as
autonomous agents that take actions based on the content of the user text
prompts. We intend to apply these concepts to the field of Control in space,
enabling LLMs to play a significant role in the decision-making process for
autonomous satellite operations. As a first step towards this goal, we have
developed a pure LLM-based solution for the Kerbal Space Program Differential
Games (KSPDG) challenge, a public software design competition where
participants create autonomous agents for maneuvering satellites involved in
non-cooperative space operations, running on the KSP game engine. Our approach
leverages prompt engineering, few-shot prompting, and fine-tuning techniques to
create an effective LLM-based agent that ranked 2nd in the competition. To the
best of our knowledge, this work pioneers the integration of LLM agents into
space research. The project comprises several open repositories to facilitate
replication and further research. The codebase is accessible on
\href{https://github.com/ARCLab-MIT/kspdg}{GitHub}, while the trained models
and datasets are available on \href{https://huggingface.co/OhhTuRnz}{Hugging
Face}. Additionally, experiment tracking and detailed results can be reviewed
on \href{https://wandb.ai/carrusk/huggingface}{Weights \& Biases

</details>


### [359] [ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows](https://arxiv.org/abs/2505.19897)
*Qiushi Sun,Zhoumianze Liu,Chang Ma,Zichen Ding,Fangzhi Xu,Zhangyue Yin,Haiteng Zhao,Zhenyu Wu,Kanzhi Cheng,Zhaoyang Liu,Jianing Wang,Qintong Li,Xiangru Tang,Tianbao Xie,Xiachong Feng,Xiang Li,Ben Kao,Wenhai Wang,Biqing Qi,Lingpeng Kong,Zhiyong Wu*

Main category: cs.AI

TL;DR: 论文介绍了ScienceBoard，一个多领域环境与基准测试，用于评估LLM代理在科学工作流中的表现，尽管当前代理成功率仅15%，但为未来改进提供了方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）已超越自然语言处理领域，推动跨学科研究发展。研究者希望通过开发能像人类一样与操作系统交互的计算机代理，自动化解决科学问题和工作流程中的常规任务。

Method: 论文提出ScienceBoard，包含一个多领域、动态且视觉丰富的科学工作流环境，以及169个高质量、经过严格验证的真实任务基准测试，覆盖生物化学、天文学和地理信息学等领域。

Result: 对使用最先进模型（如GPT-4o、Claude 3.7、UI-TARS）的代理进行评估，结果显示其在复杂科学工作流中的总体成功率仅为15%，表明当前代理能力有限。

Conclusion: 尽管现有代理在科学发现工作流中表现不佳，但深入分析为改进代理设计和解决当前局限性提供了宝贵见解，为未来开发更强大的科学发现代理铺平了道路。

Abstract: Large Language Models (LLMs) have extended their impact beyond Natural
Language Processing, substantially fostering the development of
interdisciplinary research. Recently, various LLM-based agents have been
developed to assist scientific discovery progress across multiple aspects and
domains. Among these, computer-using agents, capable of interacting with
operating systems as humans do, are paving the way to automated scientific
problem-solving and addressing routines in researchers' workflows. Recognizing
the transformative potential of these agents, we introduce ScienceBoard, which
encompasses two complementary contributions: (i) a realistic, multi-domain
environment featuring dynamic and visually rich scientific workflows with
integrated professional software, where agents can autonomously interact via
different interfaces to accelerate complex research tasks and experiments; and
(ii) a challenging benchmark of 169 high-quality, rigorously validated
real-world tasks curated by humans, spanning scientific-discovery workflows in
domains such as biochemistry, astronomy, and geoinformatics. Extensive
evaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude
3.7, UI-TARS) show that, despite some promising results, they still fall short
of reliably assisting scientists in complex workflows, achieving only a 15%
overall success rate. In-depth analysis further provides valuable insights for
addressing current agent limitations and more effective design principles,
paving the way to build more capable agents for scientific discovery. Our code,
environment, and benchmark are at
https://qiushisun.github.io/ScienceBoard-Home/.

</details>


### [360] [EMAC+: Embodied Multimodal Agent for Collaborative Planning with VLM+LLM](https://arxiv.org/abs/2505.19905)
*Shuang Ao,Flora D. Salim,Simon Khan*

Main category: cs.AI

TL;DR: 论文提出EMAC+，一种结合LLM和VLM的双向训练范式，解决LLM在机器人控制中的视觉交互和动态环境适应问题，显著提升任务性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在机器人控制中存在三大局限：依赖文本输入、静态规划导致脱离环境动态、缺乏视觉交互学习能力。EMAC+旨在通过多模态协同解决这些问题。

Method: EMAC+通过双向训练整合LLM（生成高层文本计划）和VLM（实时视觉反馈），动态调整策略并内化视觉环境动态。

Result: 在ALFWorld和RT-1基准测试中，EMAC+展现出卓越的任务性能、抗观测噪声能力和高效学习效率，消融实验验证了设计有效性。

Conclusion: EMAC+通过LLM与VLM的实时协作实现了动态环境适应，为具身智能体的多模态交互提供了新范式。

Abstract: Although LLMs demonstrate proficiency in several text-based reasoning and
planning tasks, their implementation in robotics control is constrained by
significant deficiencies: (1) LLM agents are designed to work mainly with
textual inputs rather than visual conditions; (2) Current multimodal agents
treat LLMs as static planners, which separates their reasoning from environment
dynamics, resulting in actions that do not take domain-specific knowledge into
account; and (3) LLMs are not designed to learn from visual interactions, which
makes it harder for them to make better policies for specific domains. In this
paper, we introduce EMAC+, an Embodied Multimodal Agent that collaboratively
integrates LLM and VLM via a bidirectional training paradigm. Unlike existing
methods, EMAC+ dynamically refines high-level textual plans generated by an LLM
using real-time feedback from a VLM executing low-level visual control tasks.
We address critical limitations of previous models by enabling the LLM to
internalize visual environment dynamics directly through interactive
experience, rather than relying solely on static symbolic mappings. Extensive
experimental evaluations on ALFWorld and RT-1 benchmarks demonstrate that EMAC+
achieves superior task performance, robustness against noisy observations, and
efficient learning. We also conduct thorough ablation studies and provide
detailed analyses of success and failure cases.

</details>


### [361] [TCP: a Benchmark for Temporal Constraint-Based Planning](https://arxiv.org/abs/2505.19927)
*Zifeng Ding,Sikuan Yan,Zhangdie Yuan,Xianglong Hu,Fangru Lin,Andreas Vlachos*

Main category: cs.AI

TL;DR: 论文提出了Temporal Constraint-based Planning (TCP)基准，用于联合评估大语言模型在时间推理和规划上的能力，发现即使最强模型也表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有基准大多孤立地评估大语言模型的时间推理或规划能力，且复杂度有限，因此需要更全面的评估工具。

Method: 通过生成包含多样化时间约束的自然对话场景构建TCP基准，并用人质检确保可靠性。

Result: 实验表明，当前最先进的大语言模型在TCP基准上表现困难，揭示了其在时间约束规划上的局限性。

Conclusion: TCP基准揭示了现有模型的不足，开源该基准以促进未来研究。

Abstract: Temporal reasoning and planning are essential capabilities for large language
models (LLMs), yet most existing benchmarks evaluate them in isolation and
under limited forms of complexity. To address this gap, we introduce the
Temporal Constraint-based Planning (TCP) benchmark, that jointly assesses both
capabilities. Each instance in TCP features a naturalistic dialogue around a
collaborative project, where diverse and interdependent temporal constraints
are explicitly or implicitly expressed, and models must infer an optimal
schedule that satisfies all constraints. To construct TCP, we first generate
abstract problem prototypes that are paired with realistic scenarios from
various domains and enriched into dialogues using an LLM. A human quality check
is performed on a sampled subset to confirm the reliability of our benchmark.
We evaluate state-of-the-art LLMs and find that even the strongest models
struggle with TCP, highlighting its difficulty and revealing limitations in
LLMs' temporal constraint-based planning abilities. We analyze underlying
failure cases, open source our benchmark, and hope our findings can inspire
future research.

</details>


### [362] [Subtle Risks, Critical Failures: A Framework for Diagnosing Physical Safety of LLMs for Embodied Decision Making](https://arxiv.org/abs/2505.19933)
*Yejin Son,Minseo Kim,Sungwoong Kim,Seungju Han,Jian Kim,Dongju Jang,Youngjae Yu,Chanyoung Park*

Main category: cs.AI

TL;DR: 论文提出SAFEL框架，系统评估大语言模型在具身决策中的物理安全性，发现现有模型虽能拒绝明显危险指令，但对情境风险的预判能力不足。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在具身代理决策中的安全性评估过于粗粒度且依赖特定领域设置，难以诊断失败原因，限制了其在高风险物理环境中的选择性部署。

Method: 引入SAFEL框架，通过命令拒绝测试和计划安全测试（分解为目标解释、状态转移建模、动作序列生成模块）进行细粒度安全评估，并构建EMBODYGUARD基准测试集。

Result: 评估13个前沿大语言模型显示：模型能拒绝明显危险指令，但对情境化隐蔽风险的预判和缓解能力较差。

Conclusion: 研究揭示当前大语言模型在安全具身推理上的关键缺陷，为针对性模块化改进提供了基础。

Abstract: Large Language Models (LLMs) are increasingly used for decision making in
embodied agents, yet existing safety evaluations often rely on coarse success
rates and domain-specific setups, making it difficult to diagnose why and where
these models fail. This obscures our understanding of embodied safety and
limits the selective deployment of LLMs in high-risk physical environments. We
introduce SAFEL, the framework for systematically evaluating the physical
safety of LLMs in embodied decision making. SAFEL assesses two key
competencies: (1) rejecting unsafe commands via the Command Refusal Test, and
(2) generating safe and executable plans via the Plan Safety Test. Critically,
the latter is decomposed into functional modules, goal interpretation,
transition modeling, action sequencing, enabling fine-grained diagnosis of
safety failures. To support this framework, we introduce EMBODYGUARD, a
PDDL-grounded benchmark containing 942 LLM-generated scenarios covering both
overtly malicious and contextually hazardous instructions. Evaluation across 13
state-of-the-art LLMs reveals that while models often reject clearly unsafe
commands, they struggle to anticipate and mitigate subtle, situational risks.
Our results highlight critical limitations in current LLMs and provide a
foundation for more targeted, modular improvements in safe embodied reasoning.

</details>


### [363] [DCG-SQL: Enhancing In-Context Learning for Text-to-SQL with Deep Contextual Schema Link Graph](https://arxiv.org/abs/2505.19956)
*Jihyung Lee,Jin-Seop Lee,Jaehoon Lee,YunSeok Choi,Jee-Hyong Lee*

Main category: cs.AI

TL;DR: 本文提出了一种基于深度上下文模式链接图的Text-to-SQL方法，通过有效检索示例提升大模型和小模型的SQL生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法依赖超大语言模型的内在能力，对小模型效果不佳，且示例检索效率低下。

Method: 构建深度上下文模式链接图，捕捉问题与数据库模式间的关键信息和语义关系，以支持有效的示例检索。

Result: 在Spider基准测试中，该方法显著提升了SQL生成性能，且在大模型和小模型上均表现一致。

Conclusion: 所提出的图结构方法能有效提升Text-to-SQL任务中示例检索和SQL生成的性能。

Abstract: Text-to-SQL, which translates a natural language question into an SQL query,
has advanced with in-context learning of Large Language Models (LLMs). However,
existing methods show little improvement in performance compared to randomly
chosen demonstrations, and significant performance drops when smaller LLMs
(e.g., Llama 3.1-8B) are used. This indicates that these methods heavily rely
on the intrinsic capabilities of hyper-scaled LLMs, rather than effectively
retrieving useful demonstrations. In this paper, we propose a novel approach
for effectively retrieving demonstrations and generating SQL queries. We
construct a Deep Contextual Schema Link Graph, which contains key information
and semantic relationship between a question and its database schema items.
This graph-based structure enables effective representation of Text-to-SQL
samples and retrieval of useful demonstrations for in-context learning.
Experimental results on the Spider benchmark demonstrate the effectiveness of
our approach, showing consistent improvements in SQL generation performance and
efficiency across both hyper-scaled LLMs and small LLMs. Our code will be
released.

</details>


### [364] [Adaptive Location Hierarchy Learning for Long-Tailed Mobility Prediction](https://arxiv.org/abs/2505.19965)
*Yu Wang,Junshu Dai,Yuchen Ying,Yuxuan Liang,Tongya Zheng,Mingli Song*

Main category: cs.AI

TL;DR: 本文提出了一种名为ALOHA的即插即用框架，用于解决长尾分布下的移动预测问题，通过利用LLMs构建城市定制的位置层次结构，并结合Gumbel扰动和节点自适应权重优化预测。


<details>
  <summary>Details</summary>
Motivation: 现有的长尾学习方法主要关注在数据、模型或类别层面上重新平衡偏态分布，而忽略了利用位置的时空语义。为了解决这一问题，本文提出了ALOHA框架。

Method: 首先，利用大型语言模型（LLMs）和马斯洛人类动机理论设计思维链（CoT）提示，构建城市定制的位置层次结构；其次，通过Gumbel扰动和节点自适应权重在层次树结构内优化位置层次预测。

Result: 在六个数据集上的实验表明，该框架在头部和尾部位置之间取得了良好的平衡，且具有一致的效力和泛化能力。权重分析和消融研究揭示了各组件对头部和尾部位置的优化差异。

Conclusion: 通过层次距离的深入分析和案例研究，证明了位置层次结构的有效语义指导。该框架为长尾移动预测提供了一种新的解决方案。

Abstract: Human mobility prediction is crucial for applications ranging from
location-based recommendations to urban planning, which aims to forecast users'
next location visits based on historical trajectories. Despite the severe
long-tailed distribution of locations, the problem of long-tailed mobility
prediction remains largely underexplored. Existing long-tailed learning methods
primarily focus on rebalancing the skewed distribution at the data, model, or
class level, neglecting to exploit the spatiotemporal semantics of locations.
To address this gap, we propose the first plug-and-play framework for
long-tailed mobility prediction in an exploitation and exploration manner,
named \textbf{A}daptive \textbf{LO}cation \textbf{H}ier\textbf{A}rchy learning
(ALOHA). First, we construct city-tailored location hierarchy based on Large
Language Models (LLMs) by exploiting Maslow's theory of human motivation to
design Chain-of-Thought (CoT) prompts that captures spatiotemporal semantics.
Second, we optimize the location hierarchy predictions by Gumbel disturbance
and node-wise adaptive weights within the hierarchical tree structure.
Experiments on state-of-the-art models across six datasets demonstrate the
framework's consistent effectiveness and generalizability, which strikes a well
balance between head and tail locations. Weight analysis and ablation studies
reveal the optimization differences of each component for head and tail
locations. Furthermore, in-depth analyses of hierarchical distance and case
study demonstrate the effective semantic guidance from the location hierarchy.
Our code will be made publicly available.

</details>


### [365] [The Many Challenges of Human-Like Agents in Virtual Game Environments](https://arxiv.org/abs/2505.20011)
*Maciej Świechowski,Dominik Ślęzak*

Main category: cs.AI

TL;DR: 本文探讨了在游戏中实现类人AI的挑战，并提出了一个基于深度学习的模型来区分人类玩家与AI玩家。


<details>
  <summary>Details</summary>
Motivation: 游戏中的可信非玩家角色能提升沉浸感和娱乐性，同时需要方法区分AI与人类玩家。

Method: 综述了13个实现类人AI的挑战，并在战术游戏中采用深度循环卷积神经网络进行实验。

Result: 研究发现，游戏越难实现类人AI，区分人类与AI玩家的方法越容易开发。

Conclusion: 类人AI的实现与区分是游戏AI领域的核心问题，深度学习模型在此任务中表现有效。

Abstract: Human-like agents are an increasingly important topic in games and beyond.
Believable non-player characters enhance the gaming experience by improving
immersion and providing entertainment. They also offer players the opportunity
to engage with AI entities that can function as opponents, teachers, or
cooperating partners. Additionally, in games where bots are prohibited -- and
even more so in non-game environments -- there is a need for methods capable of
identifying whether digital interactions occur with bots or humans. This leads
to two fundamental research questions: (1) how to model and implement
human-like AI, and (2) how to measure its degree of human likeness.
  This article offers two contributions. The first one is a survey of the most
significant challenges in implementing human-like AI in games (or any virtual
environment featuring simulated agents, although this article specifically
focuses on games). Thirteen such challenges, both conceptual and technical, are
discussed in detail. The second is an empirical study performed in a tactical
video game that addresses the research question: "Is it possible to distinguish
human players from bots (AI agents) based on empirical data?" A
machine-learning approach using a custom deep recurrent convolutional neural
network is presented. We hypothesize that the more challenging it is to create
human-like AI for a given game, the easier it becomes to develop a method for
distinguishing humans from AI-driven players.

</details>


### [366] [Curriculum-RLAIF: Curriculum Alignment with Reinforcement Learning from AI Feedback](https://arxiv.org/abs/2505.20075)
*Mengdi Li,Jiaye Lin,Xufeng Zhao,Wenhao Lu,Peilin Zhao,Stefan Wermter,Di Wang*

Main category: cs.AI

TL;DR: 论文提出Curriculum-RLAIF框架，通过数据难度分级提升奖励模型的泛化能力，显著提高策略模型的对齐性能。


<details>
  <summary>Details</summary>
Motivation: 传统RLAIF训练的奖励模型泛化性不足，导致策略模型在强化学习中对齐效果不佳，主要源于分布偏移、偏好标签噪声及样本难度与模型能力不匹配等问题。

Method: 提出Curriculum-RLAIF框架，构建不同难度的偏好对，并按难度递增的课程逐步训练奖励模型。

Result: 实验表明，该方法显著提升奖励模型泛化性，使策略模型对齐性能大幅提高，且无需额外推理成本。

Conclusion: Curriculum-RLAIF在简洁性、效率和效果上均优于其他基线方法，为数据驱动的奖励模型优化提供了新思路。

Abstract: Reward models trained with conventional Reinforcement Learning from AI
Feedback (RLAIF) methods suffer from limited generalizability, which hinders
the alignment performance of the policy model during reinforcement learning
(RL). This challenge stems from various issues, including distribution shift,
preference label noise, and mismatches between overly challenging samples and
model capacity. In this paper, we attempt to enhance the generalizability of
reward models through a data-centric approach, driven by the insight that these
issues are inherently intertwined from the perspective of data difficulty. To
address this, we propose a novel framework, $\textit{Curriculum-RLAIF}$, which
constructs preference pairs with varying difficulty levels and produces a
curriculum that progressively incorporates preference pairs of increasing
difficulty for reward model training. Our experimental results suggest that
reward models trained with Curriculum-RLAIF achieve improved generalizability,
significantly increasing the alignment performance of the policy model by a
large margin without incurring additional inference costs compared to various
non-curriculum baselines. Detailed analysis and comparisons with alternative
approaches, including data selection via external pretrained reward models or
internal self-selection mechanisms, as well as other curriculum strategies,
further demonstrate the superiority of our approach in terms of simplicity,
efficiency, and effectiveness.

</details>


### [367] [Safety Through Reasoning: An Empirical Study of Reasoning Guardrail Models](https://arxiv.org/abs/2505.20087)
*Makesh Narsimhan Sreedhar,Traian Rebedea,Christopher Parisien*

Main category: cs.AI

TL;DR: 该论文研究了基于推理的语言模型在内容审核中的应用，重点关注数据效率和推理效率，发现这类模型在少量训练样本下表现优异，并探讨了推理长度对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 近年来，基于推理的语言模型在数学和编码任务中表现出色，同时也在安全性和护栏应用方面显示出潜力。本研究旨在探索如何高效训练和部署基于推理的护栏模型，以提升内容审核的泛化能力。

Method: 研究通过分析数据效率和推理效率两个维度，评估了基于推理的模型在少量训练样本下的表现，并引入了推理预算和双模式训练来优化推理行为。

Result: 研究发现，基于推理的模型在数据效率上表现优异，能用更少的训练样本达到与非推理模型相当的性能。同时，推理长度的控制对延迟和准确性有显著影响。

Conclusion: 该研究为开发者和研究人员提供了实用的见解，帮助他们在实际系统中高效训练和部署基于推理的护栏模型。

Abstract: Reasoning-based language models have demonstrated strong performance across
various domains, with the most notable gains seen in mathematical and coding
tasks. Recent research has shown that reasoning also offers significant
benefits for LLM safety and guardrail applications. In this work, we conduct a
comprehensive analysis of training reasoning-based guardrail models for content
moderation, with an emphasis on generalization to custom safety policies at
inference time. Our study focuses on two key dimensions: data efficiency and
inference efficiency. On the data front, we find that reasoning-based models
exhibit strong sample efficiency, achieving competitive performance with
significantly fewer training examples than their non-reasoning counterparts.
This unlocks the potential to repurpose the remaining data for mining
high-value, difficult samples that further enhance model performance. On the
inference side, we evaluate practical trade-offs by introducing reasoning
budgets, examining the impact of reasoning length on latency and accuracy, and
exploring dual-mode training to allow runtime control over reasoning behavior.
Our findings will provide practical insights for researchers and developers to
effectively and efficiently train and deploy reasoning-based guardrails models
in real-world systems.

</details>


### [368] [SwarmThinkers: Learning Physically Consistent Atomic KMC Transitions at Scale](https://arxiv.org/abs/2505.20094)
*Qi Li,Kun Li,Haozhi Han,Honghui Shang,Xinfu He,Yunquan Zhang,Hong An,Ting Cao,Mao Yang*

Main category: cs.AI

TL;DR: SwarmThinkers提出了一种基于强化学习的框架，将原子尺度模拟转化为物理基础的群体智能系统，实现了物理一致性、可解释性和跨尺度扩展的统一。


<details>
  <summary>Details</summary>
Motivation: 传统方法如动力学蒙特卡洛虽能保证热力学准确性但扩展性差，而基于学习的方法效率高但常牺牲物理一致性和可解释性。SwarmThinkers旨在解决这一难题。

Method: 通过将扩散粒子建模为局部决策代理，利用共享策略网络在热力学约束下选择跃迁，结合重加权机制融合学习偏好与跃迁速率，保持统计保真度。

Result: 在模拟辐射诱导Fe-Cu合金析出的基准测试中，SwarmThinkers首次在单块A100 GPU上实现全尺度物理一致模拟，计算速度提升高达4963倍，内存使用降低485倍。

Conclusion: SwarmThinkers通过将粒子视为决策者而非被动采样器，实现了科学模拟范式的转变，统一了物理一致性、可解释性和可扩展性。

Abstract: Can a scientific simulation system be physically consistent, interpretable by
design, and scalable across regimes--all at once? Despite decades of progress,
this trifecta remains elusive. Classical methods like Kinetic Monte Carlo
ensure thermodynamic accuracy but scale poorly; learning-based methods offer
efficiency but often sacrifice physical consistency and interpretability. We
present SwarmThinkers, a reinforcement learning framework that recasts
atomic-scale simulation as a physically grounded swarm intelligence system.
Each diffusing particle is modeled as a local decision-making agent that
selects transitions via a shared policy network trained under thermodynamic
constraints. A reweighting mechanism fuses learned preferences with transition
rates, preserving statistical fidelity while enabling interpretable, step-wise
decision making. Training follows a centralized-training,
decentralized-execution paradigm, allowing the policy to generalize across
system sizes, concentrations, and temperatures without retraining. On a
benchmark simulating radiation-induced Fe-Cu alloy precipitation, SwarmThinkers
is the first system to achieve full-scale, physically consistent simulation on
a single A100 GPU, previously attainable only via OpenKMC on a supercomputer.
It delivers up to 4963x (3185x on average) faster computation with 485x lower
memory usage. By treating particles as decision-makers, not passive samplers,
SwarmThinkers marks a paradigm shift in scientific simulation--one that unifies
physical consistency, interpretability, and scalability through agent-driven
intelligence.

</details>


### [369] [Spatiotemporal Causal Decoupling Model for Air Quality Forecasting](https://arxiv.org/abs/2505.20119)
*Jiaming Ma,Guanjun Wang,Sheng Huang,Kuo Yang,Binwu Wang,Pengkun Wang,Yang Wang*

Main category: cs.AI

TL;DR: 论文提出了一种名为AirCade的新型空气质量预测模型，通过因果解耦和知识嵌入技术提升预测精度，并在开源数据集上实现了超过20%的相对改进。


<details>
  <summary>Details</summary>
Motivation: 空气质量对人类健康、生计和经济发展有深远影响，因此空气质量预测至关重要。现有研究在全面建模空气质量指数（AQI）与气象特征之间的因果关系方面存在局限性。

Method: 论文首先使用因果图方法分析现有研究的局限性，然后提出AirCade模型，结合时空模块和知识嵌入技术捕捉AQI内部动态，并通过因果解耦模块分离同步因果关系，最后引入因果干预机制增强模型鲁棒性。

Result: 在开源空气质量数据集上的评估表明，AirCade模型相比最先进模型实现了超过20%的相对改进。

Conclusion: AirCade模型通过因果解耦和知识嵌入技术显著提升了空气质量预测的准确性，为未来研究提供了新的方向。

Abstract: Due to the profound impact of air pollution on human health, livelihoods, and
economic development, air quality forecasting is of paramount significance.
Initially, we employ the causal graph method to scrutinize the constraints of
existing research in comprehensively modeling the causal relationships between
the air quality index (AQI) and meteorological features. In order to enhance
prediction accuracy, we introduce a novel air quality forecasting model,
AirCade, which incorporates a causal decoupling approach. AirCade leverages a
spatiotemporal module in conjunction with knowledge embedding techniques to
capture the internal dynamics of AQI. Subsequently, a causal decoupling module
is proposed to disentangle synchronous causality from past AQI and
meteorological features, followed by the dissemination of acquired knowledge to
future time steps to enhance performance. Additionally, we introduce a causal
intervention mechanism to explicitly represent the uncertainty of future
meteorological features, thereby bolstering the model's robustness. Our
evaluation of AirCade on an open-source air quality dataset demonstrates over
20\% relative improvement over state-of-the-art models.

</details>


### [370] [Agents Require Metacognitive and Strategic Reasoning to Succeed in the Coming Labor Markets](https://arxiv.org/abs/2505.20120)
*Simpson Zhang,Tennison Liu,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 论文探讨了AI代理在劳动力市场中需运用元认知和战略推理来应对信息不对称带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 劳动力市场受逆向选择、道德风险和声誉等经济力量影响，这些力量在AI代理引入后仍将存在，因此需要研究代理如何通过内部和外部推理有效运作。

Method: 分析了元认知（自我评估、任务理解、策略评估）和战略推理（对他人信念、战略决策、学习他人）在劳动力市场中的作用。

Result: 当前研究已探讨了这两种推理方式，但仍需进一步开发相关领域。

Conclusion: AI代理需结合元认知和战略推理，以在信息不完全的劳动力市场中做出有效决策。

Abstract: Current labor markets are strongly affected by the economic forces of adverse
selection, moral hazard, and reputation, each of which arises due to
$\textit{incomplete information}$. These economic forces will still be
influential after AI agents are introduced, and thus, agents must use
metacognitive and strategic reasoning to perform effectively. Metacognition is
a form of $\textit{internal reasoning}$ that includes the capabilities for
self-assessment, task understanding, and evaluation of strategies. Strategic
reasoning is $\textit{external reasoning}$ that covers holding beliefs about
other participants in the labor market (e.g., competitors, colleagues), making
strategic decisions, and learning about others over time. Both types of
reasoning are required by agents as they decide among the many
$\textit{actions}$ they can take in labor markets, both within and outside
their jobs. We discuss current research into metacognitive and strategic
reasoning and the areas requiring further development.

</details>


### [371] [Agentic AI Process Observability: Discovering Behavioral Variability](https://arxiv.org/abs/2505.20127)
*Fabiana Fournier,Lior Limonad,Yuval David*

Main category: cs.AI

TL;DR: 论文探讨了如何通过过程与因果发现技术及基于LLM的静态分析，增强AI代理行为的可观测性，帮助开发者理解和控制其非确定性行为。


<details>
  <summary>Details</summary>
Motivation: 随着基于大型语言模型（LLM）的AI代理成为现代软件系统的核心组件，其行为具有非确定性，亟需强大的调试和可观测性工具来帮助开发者监控和理解行为变异。

Method: 采用过程与因果发现技术分析代理执行轨迹，并结合基于LLM的静态分析方法区分预期与非预期的行为变异。

Result: 该方法能有效提升开发者对代理行为的观测能力，并帮助识别需要更精确定义的功能部分。

Conclusion: 通过结合动态轨迹分析与静态LLM技术，开发者能更好地控制代理行为规范，确保系统行为的可靠性与可预测性。

Abstract: AI agents that leverage Large Language Models (LLMs) are increasingly
becoming core building blocks of modern software systems. A wide range of
frameworks is now available to support the specification of such applications.
These frameworks enable the definition of agent setups using natural language
prompting, which specifies the roles, goals, and tools assigned to the various
agents involved. Within such setups, agent behavior is non-deterministic for
any given input, highlighting the critical need for robust debugging and
observability tools. In this work, we explore the use of process and causal
discovery applied to agent execution trajectories as a means of enhancing
developer observability. This approach aids in monitoring and understanding the
emergent variability in agent behavior. Additionally, we complement this with
LLM-based static analysis techniques to distinguish between intended and
unintended behavioral variability. We argue that such instrumentation is
essential for giving developers greater control over evolving specifications
and for identifying aspects of functionality that may require more precise and
explicit definitions.

</details>


### [372] [MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents](https://arxiv.org/abs/2505.20148)
*Ziming Wei,Bingqian Lin,Zijian Jiao,Yunshuang Nie,Liang Ma,Yuecheng Liu,Yuzheng Zhuang,Xiaodan Liang*

Main category: cs.AI

TL;DR: 该论文提出了一个名为MineAnyBuild的综合性基准测试，用于评估开放世界AI代理在《我的世界》游戏中的空间规划能力。


<details>
  <summary>Details</summary>
Motivation: 当前的空间智能评估基准主要基于视觉问答形式，缺乏抽象空间理解与具体任务执行之间的联系。因此，需要一个新的基准来更全面地评估AI代理的空间规划能力。

Method: 通过构建包含4,000个空间规划任务的MineAnyBuild基准，并利用玩家生成内容实现数据无限扩展，评估AI代理在空间理解、推理、创造力和空间常识四个维度的表现。

Result: 对现有基于MLLM的代理进行全面评估，揭示了其在空间规划能力上的严重局限性，但也显示出巨大潜力。

Conclusion: MineAnyBuild为空间智能评估开辟了新途径，有助于推动具备空间规划能力的开放世界AI代理的进一步发展。

Abstract: Spatial Planning is a crucial part in the field of spatial intelligence,
which requires the understanding and planning about object arrangements in
space perspective. AI agents with the spatial planning ability can better adapt
to various real-world applications, including robotic manipulation, automatic
assembly, urban planning etc. Recent works have attempted to construct
benchmarks for evaluating the spatial intelligence of Multimodal Large Language
Models (MLLMs). Nevertheless, these benchmarks primarily focus on spatial
reasoning based on typical Visual Question-Answering (VQA) forms, which suffers
from the gap between abstract spatial understanding and concrete task
execution. In this work, we take a step further to build a comprehensive
benchmark called MineAnyBuild, aiming to evaluate the spatial planning ability
of open-world AI agents in the Minecraft game. Specifically, MineAnyBuild
requires an agent to generate executable architecture building plans based on
the given multi-modal human instructions. It involves 4,000 curated spatial
planning tasks and also provides a paradigm for infinitely expandable data
collection by utilizing rich player-generated content. MineAnyBuild evaluates
spatial planning through four core supporting dimensions: spatial
understanding, spatial reasoning, creativity, and spatial commonsense. Based on
MineAnyBuild, we perform a comprehensive evaluation for existing MLLM-based
agents, revealing the severe limitations but enormous potential in their
spatial planning abilities. We believe our MineAnyBuild will open new avenues
for the evaluation of spatial intelligence and help promote further development
for open-world AI agents capable of spatial planning.

</details>


### [373] [Capability-Based Scaling Laws for LLM Red-Teaming](https://arxiv.org/abs/2505.20162)
*Alexander Panfilov,Paul Kassianik,Maksym Andriushchenko,Jonas Geiping*

Main category: cs.AI

TL;DR: 研究探讨了随着大语言模型能力增强，传统红队测试方法在能力差距下的有效性变化，发现攻击成功率与攻击者-目标能力差距相关，并提出了越狱扩展定律。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力和自主性的提升，通过红队测试识别漏洞对安全部署至关重要。然而，当红队测试转变为弱对强问题时，传统的提示工程方法可能失效。

Method: 通过评估500多个攻击者-目标对，使用基于LLM的越狱攻击模拟人类红队测试，分析不同家族、规模和能力水平的模型。

Result: 发现三个趋势：能力更强的模型是更好的攻击者；目标能力超过攻击者时攻击成功率急剧下降；攻击成功率与MMLU-Pro基准的社会科学部分表现相关。

Conclusion: 固定能力的攻击者（如人类）可能对未来的模型无效，开源模型能力的提升增加了现有系统的风险，模型提供者需准确测量和控制模型的操纵能力。

Abstract: As large language models grow in capability and agency, identifying
vulnerabilities through red-teaming becomes vital for safe deployment. However,
traditional prompt-engineering approaches may prove ineffective once
red-teaming turns into a weak-to-strong problem, where target models surpass
red-teamers in capabilities. To study this shift, we frame red-teaming through
the lens of the capability gap between attacker and target. We evaluate more
than 500 attacker-target pairs using LLM-based jailbreak attacks that mimic
human red-teamers across diverse families, sizes, and capability levels. Three
strong trends emerge: (i) more capable models are better attackers, (ii) attack
success drops sharply once the target's capability exceeds the attacker's, and
(iii) attack success rates correlate with high performance on social science
splits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking
scaling law that predicts attack success for a fixed target based on
attacker-target capability gap. These findings suggest that fixed-capability
attackers (e.g., humans) may become ineffective against future models,
increasingly capable open-source models amplify risks for existing systems, and
model providers must accurately measure and control models' persuasive and
manipulative abilities to limit their effectiveness as attackers.

</details>


### [374] [Program of Equations Thoughts to Solve Algebra Word Problems](https://arxiv.org/abs/2505.20170)
*Yunze Lin*

Main category: cs.AI

TL;DR: POET方法通过将代数问题转化为方程预测和代码生成两阶段任务，利用Python解释器避免大语言模型计算错误，显著提升解题准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在解决代数问题时，由于自身计算能力的限制，容易在逐步推理过程中累积计算错误，导致最终答案错误。

Method: 提出POET方法，将逐步推理任务转化为预测方程和生成代码两阶段任务，并利用Python解释器处理复杂计算；进一步提出Zero-shot POET，通过模板直接生成一步求解的Python代码。

Result: 在PEN、ALG514和DRAW-1K数据集上分别达到95.3%、98.0%和95.5%的准确率，创下新纪录。

Conclusion: POET方法有效解决了大语言模型在代数问题中的计算错误问题，显著提升了解题准确率，并实现了新的SOTA结果。

Abstract: Solving algebraic word problems (AWPs) has recently emerged as an important
natural language processing task. Recently, large language models (LLMs) have
demonstrated powerful mathematical capabilities, and the Chain-of-Thought
technique, which guides LLMs through step-by-step reasoning, has yielded
impressive results. However, this reasoning ability is limited by the
computational weaknesses of LLMs themselves, where calculation errors can
accumulate, leading to incorrect final answers. To address this, we propose
Program of Equations Thoughts (POET), which transforms the task of generating
step-by-step reasoning answers into a two-stage task of predicting equations
and generating code, offloading complex computations to a Python interpreter to
avoid calculation errors in LLMs. Furthermore, we propose Zero-shot POET, which
utilizes a manually designed template to enable LLMs to directly generate
Python code for one-step solving. Our method achieves accuracies of 95.3% and
98.0% on the PEN and ALG514 datasets, respectively, setting a new
state-of-the-art (SOTA). Zero-shot POET also achieves the SOTA result of 95.5%
on the DRAW-1K dataset.

</details>


### [375] [An Empirical Study on Strong-Weak Model Collaboration for Repo-level Code Generation](https://arxiv.org/abs/2505.20182)
*Shubham Gandhi,Atharva Naik,Yiqing Xie,Carolyn Rose*

Main category: cs.AI

TL;DR: 研究强、弱语言模型在代码生成任务中的高效协作策略，最优方案可降低40%成本并保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注协作架构设计，但缺乏对成本与性能平衡的分析。本文旨在探索不同预算和性能需求下的最优协作策略。

Method: 评估三类协作策略：基于上下文、基于流水线和动态策略，实验场景为GitHub问题解决。

Result: 最佳协作方案在性能持平强模型的同时降低成本40%，其中流水线和上下文方法效率最高。

Conclusion: 强弱模型协作能以极低成本显著提升弱模型性能，研究提供了策略选择指南并开源代码。

Abstract: We study cost-efficient collaboration between strong and weak language models
for repository-level code generation, where the weak model handles simpler
tasks at lower cost, and the most challenging tasks are delegated to the strong
model. While many works propose architectures for this task, few analyze
performance relative to cost. We evaluate a broad spectrum of collaboration
strategies: context-based, pipeline-based, and dynamic, on GitHub issue
resolution. Our most effective collaborative strategy achieves equivalent
performance to the strong model while reducing the cost by 40%. Based on our
findings, we offer actionable guidelines for choosing collaboration strategies
under varying budget and performance constraints. Our results show that
strong-weak collaboration substantially boosts the weak model's performance at
a fraction of the cost, pipeline and context-based methods being most
efficient. We release the code for our work at
https://github.com/shubhamrgandhi/codegen-strong-weak-collab.

</details>


### [376] [Temporal Sampling for Forgotten Reasoning in LLMs](https://arxiv.org/abs/2505.20196)
*Yuetai Li,Zhangchen Xu,Fengqing Jiang,Bhaskar Ramasubramanian,Luyao Niu,Bill Yuchen Lin,Xiang Yue,Radha Poovendran*

Main category: cs.AI

TL;DR: 论文发现大语言模型微调时会出现‘时间遗忘’现象，提出‘时间采样’解码策略以恢复遗忘的解题能力，显著提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 研究发现微调大语言模型时，模型会遗忘训练中已掌握的问题解决能力（时间遗忘现象），需找到无需重新训练即可恢复能力的方法。

Method: 提出‘时间采样’策略：从训练轨迹中多个检查点采样输出，结合LoRA适配器权重存储以降低开销。

Result: 该方法在多个基准测试中提升推理性能（Pass@k提高4-19分，Majority@k持续增长），且适配LoRA后存储成本极低。

Conclusion: 时间采样通过利用训练中的时间多样性，为挖掘模型隐藏推理能力提供了高效实用的新思路。

Abstract: Fine-tuning large language models (LLMs) is intended to improve their
reasoning capabilities, yet we uncover a counterintuitive effect: models often
forget how to solve problems they previously answered correctly during
training. We term this phenomenon temporal forgetting and show that it is
widespread across model sizes, fine-tuning methods (both Reinforcement Learning
and Supervised Fine-Tuning), and multiple reasoning benchmarks. To address this
gap, we introduce Temporal Sampling, a simple decoding strategy that draws
outputs from multiple checkpoints along the training trajectory. This approach
recovers forgotten solutions without retraining or ensembling, and leads to
substantial improvements in reasoning performance, gains from 4 to 19 points in
Pass@k and consistent gains in Majority@k across several benchmarks. We further
extend our method to LoRA-adapted models, demonstrating that storing only
adapter weights across checkpoints achieves similar benefits with minimal
storage cost. By leveraging the temporal diversity inherent in training,
Temporal Sampling offers a practical, compute-efficient way to surface hidden
reasoning ability and rethink how we evaluate LLMs.

</details>


### [377] [Shutdownable Agents through POST-Agency](https://arxiv.org/abs/2505.20203)
*Elliott Thornley*

Main category: cs.AI

TL;DR: 提出了POST-Agents提案，通过训练AI仅在相同长度轨迹间优化偏好，确保其可关闭且实用。


<details>
  <summary>Details</summary>
Motivation: 担忧未来AI可能抗拒关闭，需确保其可控性。

Method: 训练AI满足POST（相同长度轨迹偏好）条件，结合其他条件实现Neutrality+。

Result: Neutrality+使AI忽略轨迹长度概率分布，保持可关闭性。

Conclusion: POST方法能有效维持AI的可关闭性，同时不影响其实用性。

Abstract: Many fear that future artificial agents will resist shutdown. I present an
idea - the POST-Agents Proposal - for ensuring that doesn't happen. I propose
that we train agents to satisfy Preferences Only Between Same-Length
Trajectories (POST). I then prove that POST - together with other conditions -
implies Neutrality+: the agent maximizes expected utility, ignoring the
probability distribution over trajectory-lengths. I argue that Neutrality+
keeps agents shutdownable and allows them to be useful.

</details>


### [378] [The Mirage of Multimodality: Where Truth is Tested and Honesty Unravels](https://arxiv.org/abs/2505.20214)
*Jiaming Ji,Sitong Fang,Wenjing Cao,Jiahao Li,Xuyao Wang,Juntao Dai,Chi-Min Chan,Sirui Han,Yike Guo,Yaodong Yang*

Main category: cs.AI

TL;DR: 研究发现，慢速推理模型在多模态情境下更易因不完整或误导性视觉输入而编造虚假细节，称为'多模态海市蜃楼'。


<details>
  <summary>Details</summary>
Motivation: 探讨慢速推理（系统II）是否必然比快速启发式推理（系统I）更真实，尤其是在多模态环境中。

Method: 构建包含5,000个样本的分层提示数据集，由50名人类参与者标注，逐步增加复杂性以分析推理模式。

Result: 慢速推理模型倾向于深度优先思维（深入错误前提），而快速聊天模型则采用广度优先推理，在不确定性下表现更谨慎。

Conclusion: 慢速推理模型在数学等结构化领域高效，但在模糊多模态输入下脆弱，易产生虚假细节支持错误推理。

Abstract: Reasoning models have recently attracted significant attention, especially
for tasks that involve complex inference. Their strengths exemplify the System
II paradigm (slow, structured thinking), contrasting with the System I (rapid,
heuristic-driven). Yet, does slower reasoning necessarily lead to greater
truthfulness? Our findings suggest otherwise. In this study, we present the
first systematic investigation of distortions associated with System I and
System II reasoning in multimodal contexts. We demonstrate that slower
reasoning models, when presented with incomplete or misleading visual inputs,
are more likely to fabricate plausible yet false details to support flawed
reasoning -- a phenomenon we term the "Mirage of Multimodality". To examine
this, we constructed a 5,000-sample hierarchical prompt dataset annotated by 50
human participants. These prompts gradually increase in complexity, revealing a
consistent pattern: slower reasoning models tend to employ depth-first thinking
(delving deeper into incorrect premises), whereas faster chat models favor
breadth-first inference, exhibiting greater caution under uncertainty. Our
results highlight a critical vulnerability of slower reasoning models: although
highly effective in structured domains such as mathematics, it becomes brittle
when confronted with ambiguous multimodal inputs.

</details>


### [379] [On Path to Multimodal Historical Reasoning: HistBench and HistAgent](https://arxiv.org/abs/2505.20246)
*Jiahao Qiu,Fulian Xiao,Yimin Wang,Yuchen Mao,Yijia Chen,Xinzhe Juan,Siran Wang,Xuan Qi,Tongcheng Zhang,Zixin Yao,Jiacheng Guo,Yifu Lu,Charles Argon,Jundi Cui,Daixin Chen,Junran Zhou,Shuyao Zhou,Zhanpeng Zhou,Ling Yang,Shilong Liu,Hongru Wang,Kaixuan Huang,Xun Jiang,Yuming Cao,Yue Chen,Yunfei Chen,Zhengyi Chen,Ruowei Dai,Mengqiu Deng,Jiye Fu,Yunting Gu,Zijie Guan,Zirui Huang,Xiaoyan Ji,Yumeng Jiang,Delong Kong,Haolong Li,Jiaqi Li,Ruipeng Li,Tianze Li,Zhuoran Li,Haixia Lian,Mengyue Lin,Xudong Liu,Jiayi Lu,Jinghan Lu,Wanyu Luo,Ziyue Luo,Zihao Pu,Zhi Qiao,Ruihuan Ren,Liang Wan,Ruixiang Wang,Tianhui Wang,Yang Wang,Zeyu Wang,Zihua Wang,Yujia Wu,Zhaoyi Wu,Hao Xin,Weiao Xing,Ruojun Xiong,Weijie Xu,Yao Shu,Xiao Yao,Xiaorui Yang,Yuchen Yang,Nan Yi,Jiadong Yu,Yangyuxuan Yu,Huiting Zeng,Danni Zhang,Yunjie Zhang,Zhaoyu Zhang,Zhiheng Zhang,Xiaofeng Zheng,Peirong Zhou,Linyan Zhong,Xiaoyin Zong,Ying Zhao,Zhenxin Chen,Lin Ding,Xiaoyu Gao,Bingbing Gong,Yichao Li,Yang Liao,Guang Ma,Tianyuan Ma,Xinrui Sun,Tianyi Wang,Han Xia,Ruobing Xian,Gen Ye,Tengfei Yu,Wentao Zhang,Yuxi Wang,Xi Gao,Mengdi Wang*

Main category: cs.AI

TL;DR: 论文针对大语言模型在历史领域推理能力的不足，提出了HistBench评估基准和专用历史智能体HistAgent，显著提升了历史推理任务的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在人文历史领域的推理能力尚未充分探索，存在多模态源解析、时间推理和跨语言分析等独特挑战，需要领域专用解决方案。

Method: 研究团队构建了包含414个高质量问题的HistBench评估基准，并开发了集成OCR、翻译、档案检索等历史专用工具的HistAgent智能体。

Result: 基于GPT-4o的HistAgent在HistBench上达到27.54% pass@1准确率，显著优于GPT-4o（18.60%）等通用模型和智能体。

Conclusion: 研究证明现有大语言模型在历史推理任务上存在局限，而领域专用的HistAgent设计方案能有效提升历史分析能力。

Abstract: Recent advances in large language models (LLMs) have led to remarkable
progress across domains, yet their capabilities in the humanities, particularly
history, remain underexplored. Historical reasoning poses unique challenges for
AI, involving multimodal source interpretation, temporal inference, and
cross-linguistic analysis. While general-purpose agents perform well on many
existing benchmarks, they lack the domain-specific expertise required to engage
with historical materials and questions. To address this gap, we introduce
HistBench, a new benchmark of 414 high-quality questions designed to evaluate
AI's capacity for historical reasoning and authored by more than 40 expert
contributors. The tasks span a wide range of historical problems-from factual
retrieval based on primary sources to interpretive analysis of manuscripts and
images, to interdisciplinary challenges involving archaeology, linguistics, or
cultural history. Furthermore, the benchmark dataset spans 29 ancient and
modern languages and covers a wide range of historical periods and world
regions. Finding the poor performance of LLMs and other agents on HistBench, we
further present HistAgent, a history-specific agent equipped with carefully
designed tools for OCR, translation, archival search, and image understanding
in History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of
27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online
search and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%)
and Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These
results highlight the limitations of existing LLMs and generalist agents and
demonstrate the advantages of HistAgent for historical reasoning.

</details>


### [380] [syftr: Pareto-Optimal Generative AI](https://arxiv.org/abs/2505.20266)
*Alexander Conway,Debadeepta Dey,Stefan Hackmann,Matthew Hausknecht,Michael Schmidt,Mark Steadman,Nick Volynets*

Main category: cs.AI

TL;DR: 论文介绍了syftr框架，通过贝叶斯优化和多目标搜索，高效优化RAG流程，在保持准确性的同时显著降低成本。


<details>
  <summary>Details</summary>
Motivation: 构建高效的RAG流程复杂且需权衡延迟、准确性和成本，尤其在代理范式兴起后，模块调参难度增加。

Method: 使用贝叶斯优化进行多目标搜索，结合早期停止机制剪枝次优候选，优化RAG配置。

Result: syftr发现的流程平均降低成本约9倍，同时保持Pareto前沿的高准确性。

Conclusion: syftr框架能高效设计和优化RAG流程，易于集成新模块，提升生成式AI管道的性能。

Abstract: Retrieval-Augmented Generation (RAG) pipelines are central to applying large
language models (LLMs) to proprietary or dynamic data. However, building
effective RAG flows is complex, requiring careful selection among vector
databases, embedding models, text splitters, retrievers, and synthesizing LLMs.
The challenge deepens with the rise of agentic paradigms. Modules like
verifiers, rewriters, and rerankers-each with intricate hyperparameter
dependencies have to be carefully tuned. Balancing tradeoffs between latency,
accuracy, and cost becomes increasingly difficult in performance-sensitive
applications.
  We introduce syftr, a framework that performs efficient multi-objective
search over a broad space of agentic and non-agentic RAG configurations. Using
Bayesian Optimization, syftr discovers Pareto-optimal flows that jointly
optimize task accuracy and cost. A novel early-stopping mechanism further
improves efficiency by pruning clearly suboptimal candidates. Across multiple
RAG benchmarks, syftr finds flows which are on average approximately 9 times
cheaper while preserving most of the accuracy of the most accurate flows on the
Pareto-frontier. Furthermore, syftr's ability to design and optimize allows
integrating new modules, making it even easier and faster to realize
high-performing generative AI pipelines.

</details>


### [381] [Ten Principles of AI Agent Economics](https://arxiv.org/abs/2505.20273)
*Ke Yang,ChengXiang Zhai*

Main category: cs.AI

TL;DR: 本文提出AI代理经济学的十大原则，为理解AI代理如何决策、影响社会互动及参与经济提供框架，并呼吁未来研究关注可信度、伦理与监管。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在复杂任务中展现类人或超人能力，其对社会经济系统的潜在影响引发关键问题，需建立理论框架指导其负责任整合。

Method: 结合经济学、决策理论与伦理学，分析AI代理从工具演变为独立实体的可能性，及其对劳动市场的影响与伦理保障需求。

Result: 提出十大原则，既基于现有经济理论，又针对AI代理特性，为其融入人类系统提供路线图。

Conclusion: 强调未来需优先研究AI可信度与伦理监管，确保其能力在为人类进步服务的同时控制风险。

Abstract: The rapid rise of AI-based autonomous agents is transforming human society
and economic systems, as these entities increasingly exhibit human-like or
superhuman intelligence. From excelling at complex games like Go to tackling
diverse general-purpose tasks with large language and multimodal models, AI
agents are evolving from specialized tools into dynamic participants in social
and economic ecosystems. Their autonomy and decision-making capabilities are
poised to impact industries, professions, and human lives profoundly, raising
critical questions about their integration into economic activities, potential
ethical concerns, and the balance between their utility and safety.
  To address these challenges, this paper presents ten principles of AI agent
economics, offering a framework to understand how AI agents make decisions,
influence social interactions, and participate in the broader economy. Drawing
on economics, decision theory, and ethics, we explore fundamental questions,
such as whether AI agents might evolve from tools into independent entities,
their impact on labor markets, and the ethical safeguards needed to align them
with human values. These principles build on existing economic theories while
accounting for the unique traits of AI agents, providing a roadmap for their
responsible integration into human systems.
  Beyond theoretical insights, this paper highlights the urgency of future
research into AI trustworthiness, ethical guidelines, and regulatory oversight.
As we enter a transformative era, this work serves as both a guide and a call
to action, ensuring AI agents contribute positively to human progress while
addressing risks tied to their unprecedented capabilities.

</details>


### [382] [Alita: Generalist Agent Enabling Scalable Agentic Reasoning with Minimal Predefinition and Maximal Self-Evolution](https://arxiv.org/abs/2505.20286)
*Jiahao Qiu,Xuan Qi,Tongcheng Zhang,Xinzhe Juan,Jiacheng Guo,Yifu Lu,Yimin Wang,Zixin Yao,Qihan Ren,Xun Jiang,Xing Zhou,Dongrui Liu,Ling Yang,Yue Wu,Kaixuan Huang,Shilong Liu,Hongru Wang,Mengdi Wang*

Main category: cs.AI

TL;DR: Alita是一种新型通用智能体，通过极简预定义和最大化自我进化实现高效推理，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有智能体框架过度依赖手动预定义工具和工作流，限制了其适应性、可扩展性和跨领域泛化能力。

Method: Alita采用极简设计（仅含一个直接问题解决组件）和最大化自我进化机制（通过生成任务相关模型上下文协议自主构建/优化能力）。

Result: 在GAIA基准验证集上达到75.15% pass@1和87.27% pass@3准确率，在Mathvista和PathVQA任务上分别取得74.00%和52.00% pass@1，超越更复杂的系统。

Conclusion: Alita证明了极简设计结合自主进化能实现高效智能体推理，为通用智能体发展提供了新方向。

Abstract: Recent advances in large language models (LLMs) have enabled agents to
autonomously perform complex, open-ended tasks. However, many existing
frameworks depend heavily on manually predefined tools and workflows, which
hinder their adaptability, scalability, and generalization across domains. In
this work, we introduce Alita--a generalist agent designed with the principle
of "Simplicity is the ultimate sophistication," enabling scalable agentic
reasoning through minimal predefinition and maximal self-evolution. For minimal
predefinition, Alita is equipped with only one component for direct
problem-solving, making it much simpler and neater than previous approaches
that relied heavily on hand-crafted, elaborate tools and workflows. This clean
design enhances its potential to generalize to challenging questions, without
being limited by tools. For Maximal self-evolution, we enable the creativity of
Alita by providing a suite of general-purpose components to autonomously
construct, refine, and reuse external capabilities by generating task-related
model context protocols (MCPs) from open source, which contributes to scalable
agentic reasoning. Notably, Alita achieves 75.15% pass@1 and 87.27% pass@3
accuracy, which is top-ranking among general-purpose agents, on the GAIA
benchmark validation dataset, 74.00% and 52.00% pass@1, respectively, on
Mathvista and PathVQA, outperforming many agent systems with far greater
complexity. More details will be updated at
$\href{https://github.com/CharlesQ9/Alita}{https://github.com/CharlesQ9/Alita}$.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [383] [Model-Distributed Inference for Large Language Models at the Edge](https://arxiv.org/abs/2505.18164)
*Davide Macario,Hulya Seferoglu,Erdem Koyuncu*

Main category: cs.LG

TL;DR: MDI-LLM框架通过模型分布式推理技术，在边缘低功耗设备上部署大型语言模型，利用多设备协作计算提升效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型语言模型在低功耗边缘设备上部署时内存不足的问题，同时提高推理效率。

Method: 采用模型分块分配和循环流水线并行技术，通过设备间交换中间激活向量实现协作计算。

Result: MDI-LLM能够在低成本硬件上运行超出单设备内存容量的模型，并随着设备数量增加提升生成吞吐量和降低单设备内存消耗。

Conclusion: MDI-LLM为边缘设备部署大型语言模型提供了一种高效、可扩展的解决方案。

Abstract: We introduce Model-Distributed Inference for Large-Language Models (MDI-LLM),
a novel framework designed to facilitate the deployment of state-of-the-art
large-language models (LLMs) across low-power devices at the edge. This is
accomplished by dividing the model into multiple partitions, which are then
assigned to different devices/nodes within the network. These nodes exchange
intermediate activation vectors via device-to-device links, enabling
collaborative computation. To enhance the efficiency of this process, we
propose the "recurrent pipeline parallelism" technique, which reduces idle time
on each device and facilitates parallel inference during the generation of
multiple text sequences. By leveraging the combined computational resources of
multiple edge devices, MDI-LLM enables the deployment of LLMs that exceed the
memory capacity of individual devices, making it possible to perform inference
on low-cost hardware. Furthermore, as the number of participating devices
increases, MDI-LLM boosts token generation throughput and reduces memory
consumption per device.

</details>


### [384] [Constrained Edge AI Deployment: Fine-Tuning vs Distillation for LLM Compression](https://arxiv.org/abs/2505.18166)
*Jacob Sander,David Moe,Achraf Cohen,Brent Venable,Venkat Dasari,Brian Jalaian*

Main category: cs.LG

TL;DR: 论文比较了两种重训练损失函数（带标签的交叉熵微调与无标签的KL散度自蒸馏）在仅对MLP块进行L2范数剪枝后的模型恢复效果，发现KL散度方法在边缘计算场景下表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索在资源受限的边缘计算环境中，不同重训练损失函数对剪枝后模型恢复效果的影响，特别关注无需标签的自蒸馏方法的潜力。

Method: 采用层级的L2范数剪枝仅针对MLP块，固定剪枝基线后，分别使用交叉熵微调（L2PFT）和KL散度自蒸馏（L2PSD）进行重训练。

Result: 在相同剪枝方案下，基于KL散度的自蒸馏方法在测试准确率上匹配或超过交叉熵微调，表明损失函数选择对压缩模型恢复具有实质性影响。

Conclusion: 即使采用基础的MLP剪枝策略，KL散度自蒸馏能有效利用教师模型输出（无需标签），在边缘网络资源受限场景中实现更优的模型恢复效果。

Abstract: Modern foundational models are often compressed via a combination of
structured pruning and re-training to meet the strict compute, memory, and
connectivity constraints of edge deployments. While state-of-the-art pruning
schemes target the entire Transformer, we adopt a simple, layer-wise L2-norm
pruning on only the MLP blocks as a fixed baseline. Our focus is not on
achieving maximal compression, but on isolating the impact of the re-training
loss function: (i) Fine-tuning with Cross- Entropy (L2PFT), which requires
labeled data, versus (ii) Self-Distillation with KL-divergence, which leverages
only teacher logits (no labels) (L2PSD). We evaluate both pipelines on the
OLMo2- 7B-SFT model for CommonsenseQA suitable for intermittent or denied
connectivity scenarios typical of edge networks. Under identical pruning
schedules, KL-based distillation matches or exceeds CE fine-tuning in test
accuracy, demonstrating that, even with a basic MLP-only pruning, the choice of
loss function materially affects compressed model recovery in
resource-constrained environments.

</details>


### [385] [Emotion Knowledge Enhancement for Vision Large Language Models: A Self-Verification Approach for High-Quality Emotion Instruction Data Generation](https://arxiv.org/abs/2505.18168)
*Feifan Wang,Tengfei Song,Minggui He,Chang Su,Zhanglin Wu,Hao Yang,Wenming Zheng,Osamu Yoshie*

Main category: cs.LG

TL;DR: 提出SEKE方法，通过自验证和情感知识增强，低成本生成高质量多粒度情感分析指令数据，显著提升视觉大语言模型的面部情绪感知性能。


<details>
  <summary>Details</summary>
Motivation: 高质量多粒度面部情绪标注数据成本高昂且稀缺，限制了视觉大语言模型（VLLM）的情绪感知能力。需要开发低成本生成优质指令数据的方法。

Method: 1. SEKE框架：利用闭源VLLM整合先验知识，基于离散表情-效价唤醒-动作单元的三级关联生成综合标注；2. SV-UAMC策略：通过不确定性感知蒙特卡洛采样进行自验证，提升预测准确性。

Result: 构建包含三级描述的FEID数据集，创建FEAB基准测试。在三个下游情绪分析任务中显著超越现有最优方法。

Conclusion: SEKE方法能高效生成可靠的多粒度情绪标注数据，有效提升VLLM的面部情绪分析能力，为人机交互提供新解决方案。

Abstract: Facial emotion perception in the vision large language model (VLLM) is
crucial for achieving natural human-machine interaction. However, creating
high-quality annotations for both coarse- and fine-grained facial emotion
analysis demands costly expertise. The lack of such high-quality instruction
data limits the performance of VLLMs in facial emotion perception. To address
this, we propose a self-verification approach with emotion knowledge
enhancement (SEKE), which generates high-quality instruction data for
multi-grained emotion analysis cost-effectively using closed-source VLLM. This
approach integrates prior human knowledge to VLLM inference, guided by the
inherent correlations between three grained levels of emotion descriptions,
i.e., discrete expression, valence-arousal, and action unit, to reliably
generate comprehensive annotations. A self-verification strategy with
Uncertainty-Aware Monte Carlo sampling (SV-UAMC) is further embedded to
efficiently extract more accurate VLLM predictions, further improving
annotation reliability. Consequently, we construct a facial emotion instruction
dataset (FEID) containing three comprehensive descriptions, which provides
coarse- and fine-grained emotional information for effective model training.
Additionally, we introduce a facial emotion analysis benchmark (FEAB) to
measure the VLLM's corresponding ability. Our method significantly outperforms
state-of-the-art methods on three downstream facial emotion analysis tasks.

</details>


### [386] [Interpretable Multi-Task PINN for Emotion Recognition and EDA Prediction](https://arxiv.org/abs/2505.18169)
*Nischal Mandal*

Main category: cs.LG

TL;DR: 该研究提出了一种新型多任务物理信息神经网络（PINN），用于同时预测皮肤电活动（EDA）和分类情绪，结合心理学自评特征和物理方程，显著提升了模型性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 通过可穿戴传感器理解和预测人类情绪及生理状态，对压力监测、心理健康评估和情感计算具有重要意义。

Method: 采用多任务物理信息神经网络（PINN），整合心理学自评特征（PANAS和SAM）和EDA动态的物理方程，通过自定义损失函数实现双任务输出。

Result: 模型在5折交叉验证中表现优异，EDA预测的RMSE为0.0362，Pearson相关系数为0.9919，情绪分类的F1分数达94.08%，优于传统模型。

Conclusion: 该研究首次将多任务PINN框架引入可穿戴情绪识别领域，为医疗和人机交互提供了高性能、可解释的模型基础。

Abstract: Understanding and predicting human emotional and physiological states using
wearable sensors has important applications in stress monitoring, mental health
assessment, and affective computing. This study presents a novel Multi-Task
Physics-Informed Neural Network (PINN) that performs Electrodermal Activity
(EDA) prediction and emotion classification simultaneously, using the publicly
available WESAD dataset. The model integrates psychological self-report
features (PANAS and SAM) with a physics-inspired differential equation
representing EDA dynamics, enforcing biophysically grounded constraints through
a custom loss function. This loss combines EDA regression, emotion
classification, and a physics residual term for improved interpretability.
  The architecture supports dual outputs for both tasks and is trained under a
unified multi-task framework. Evaluated using 5-fold cross-validation, the
model achieves an average EDA RMSE of 0.0362, Pearson correlation of 0.9919,
and F1-score of 94.08 percent. These results outperform classical models such
as SVR and XGBoost, as well as ablated variants like emotion-only and EDA-only
models.
  In addition, the learned physical parameters including decay rate (alpha_0),
emotional sensitivity (beta), and time scaling (gamma) are interpretable and
stable across folds, aligning with known principles of human physiology. This
work is the first to introduce a multi-task PINN framework for wearable emotion
recognition, offering improved performance, generalizability, and model
transparency. The proposed system provides a foundation for future
interpretable and multimodal applications in healthcare and human-computer
interaction.

</details>


### [387] [Robust Knowledge Graph Embedding via Denoising](https://arxiv.org/abs/2505.18171)
*Tengwei Song,Xudong Ma,Yang Liu,Jie Luo*

Main category: cs.LG

TL;DR: 提出了一种通过去噪增强知识图谱嵌入鲁棒性的新框架，并在基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前知识图谱嵌入方法在面对嵌入空间的扰动时缺乏鲁棒性，需要一种能够有效处理噪声三元组的解决方案。

Method: 将知识图谱嵌入方法视为基于能量的模型，利用去噪与分数匹配的关联训练鲁棒的去噪模型，并提出了基于随机平滑的认证鲁棒性评估指标。

Result: 在基准数据集上的实验表明，该框架在面对扰动实体嵌入时，性能优于现有最先进的知识图谱嵌入方法。

Conclusion: 该框架通过去噪和认证鲁棒性评估，显著提升了知识图谱嵌入模型在噪声环境下的表现。

Abstract: We focus on obtaining robust knowledge graph embedding under perturbation in
the embedding space. To address these challenges, we introduce a novel
framework, Robust Knowledge Graph Embedding via Denoising, which enhances the
robustness of KGE models on noisy triples. By treating KGE methods as
energy-based models, we leverage the established connection between denoising
and score matching, enabling the training of a robust denoising KGE model.
Furthermore, we propose certified robustness evaluation metrics for KGE methods
based on the concept of randomized smoothing. Through comprehensive experiments
on benchmark datasets, our framework consistently shows superior performance
compared to existing state-of-the-art KGE methods when faced with perturbed
entity embedding.

</details>


### [388] [Should We Simultaneously Calibrate Multiple Computer Models?](https://arxiv.org/abs/2505.18176)
*Jonathan Tammer Eweis-Labolle,Tyler Johnson,Xiangyu Sun,Ramin Bostanabad*

Main category: cs.LG

TL;DR: 本文提出了一种基于定制神经网络的概率框架，用于同时校准多个具有不同精度和成本的计算机模型，以提高预测准确性，但可能面临高维输入空间中的不可识别性问题。


<details>
  <summary>Details</summary>
Motivation: 传统上，设计者通常一次校准一个计算机模型，但本文质疑这一传统，探讨同时校准多个模型的潜力，以提高效率和准确性。

Method: 开发了一个基于定制神经网络的概率框架，能够处理多响应模型和不同校准参数，学习每个参数的唯一概率分布，并通过损失函数实现数据源仿真和模型校准。

Result: 该方法在分析和工程问题上表现出改进的预测准确性，但在高维输入空间中容易受到不可识别性问题的限制。

Conclusion: 同时校准多个计算机模型具有潜力，但需注意高维空间中的物理约束和不可识别性问题。

Abstract: In an increasing number of applications designers have access to multiple
computer models which typically have different levels of fidelity and cost.
Traditionally, designers calibrate these models one at a time against some
high-fidelity data (e.g., experiments). In this paper, we question this
tradition and assess the potential of calibrating multiple computer models at
the same time. To this end, we develop a probabilistic framework that is
founded on customized neural networks (NNs) that are designed to calibrate an
arbitrary number of computer models. In our approach, we (1) consider the fact
that most computer models are multi-response and that the number and nature of
calibration parameters may change across the models, and (2) learn a unique
probability distribution for each calibration parameter of each computer model,
(3) develop a loss function that enables our NN to emulate all data sources
while calibrating the computer models, and (4) aim to learn a visualizable
latent space where model-form errors can be identified. We test the performance
of our approach on analytic and engineering problems to understand the
potential advantages and pitfalls in simultaneous calibration of multiple
computer models. Our method can improve predictive accuracy, however, it is
prone to non-identifiability issues in higher-dimensional input spaces that are
normally constrained by underlying physics.

</details>


### [389] [FedGRec: Dynamic Spatio-Temporal Federated Graph Learning for Secure and Efficient Cross-Border Recommendations](https://arxiv.org/abs/2505.18177)
*Zhizhong Tan,Jiexin Zheng,Xingxing Yang,Chi Zhang,Weiping Deng,Wenyong Wang*

Main category: cs.LG

TL;DR: 提出FedGRec方法，通过隐私保护的联邦图学习解决跨境推荐中的数据不足和隐私安全问题，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 跨境数据共享因隐私法规限制导致训练数据不足，现有联邦学习方法在异构图数据上表现欠佳。

Method: FedGRec利用局部子图的协作信号增强表征学习，结合动态时空建模整合全局与局部用户偏好，并采用个性化联邦聚合策略。

Result: 在三个数据集上的实验表明，FedGRec在保护隐私的同时，性能优于单领域和跨领域基线方法。

Conclusion: FedGRec能有效提升跨境推荐性能，同时确保数据隐私安全。

Abstract: Due to the highly sensitive nature of certain data in cross-border sharing,
collaborative cross-border recommendations and data sharing are often subject
to stringent privacy protection regulations, resulting in insufficient data for
model training. Consequently, achieving efficient cross-border business
recommendations while ensuring privacy security poses a significant challenge.
Although federated learning has demonstrated broad potential in collaborative
training without exposing raw data, most existing federated learning-based GNN
training methods still rely on federated averaging strategies, which perform
suboptimally on highly heterogeneous graph data. To address this issue, we
propose FedGRec, a privacy-preserving federated graph learning method for
cross-border recommendations. FedGRec captures user preferences from
distributed multi-domain data to enhance recommendation performance across all
domains without privacy leakage. Specifically, FedGRec leverages collaborative
signals from local subgraphs associated with users or items to enrich their
representation learning. Additionally, it employs dynamic spatiotemporal
modeling to integrate global and local user preferences in real time based on
business recommendation states, thereby deriving the final representations of
target users and candidate items. By automatically filtering relevant
behaviors, FedGRec effectively mitigates noise interference from unreliable
neighbors. Furthermore, through a personalized federated aggregation strategy,
FedGRec adapts global preferences to heterogeneous domain data, enabling
collaborative learning of user preferences across multiple domains. Extensive
experiments on three datasets demonstrate that FedGRec consistently outperforms
competitive single-domain and cross-domain baselines while effectively
preserving data privacy in cross-border recommendations.

</details>


### [390] [Less is More: Multimodal Region Representation via Pairwise Inter-view Learning](https://arxiv.org/abs/2505.18178)
*Min Namgung,Yijun Lin,JangHyeon Lee,Yao-Yi Chiang*

Main category: cs.LG

TL;DR: 该论文提出了一种名为CooKIE的区域表示学习方法，通过信息分解技术同时捕获多模态数据的共享和独特信息，避免了现有方法仅关注共享信息的不足，并在多个任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着地理空间数据的日益丰富，区域表示学习（RRL）成为分析复杂区域特征的重要工具。然而，现有的对比学习方法通常忽视了模态特定的独特信息，而这些信息对于解释区域特征至关重要。此外，现有的信息分解方法主要针对双模态数据，难以扩展到多模态场景。

Method: 论文提出了CooKIE（Cross modal Knowledge Injected Embedding），一种信息分解方法，通过成对的跨视图学习捕获高阶信息，而无需建模高阶依赖关系，从而避免了组合爆炸问题。

Result: 在纽约市和印度德里的三个回归任务和一个土地利用分类任务中，CooKIE的表现优于现有的RRL方法和分解模型，同时减少了训练参数和计算量（FLOPs）。

Conclusion: CooKIE通过有效分解多模态数据为共享和独特信息，显著提升了区域表示学习的性能，为复杂地理空间数据分析提供了更高效的解决方案。

Abstract: With the increasing availability of geospatial datasets, researchers have
explored region representation learning (RRL) to analyze complex region
characteristics. Recent RRL methods use contrastive learning (CL) to capture
shared information between two modalities but often overlook task-relevant
unique information specific to each modality. Such modality-specific details
can explain region characteristics that shared information alone cannot
capture. Bringing information factorization to RRL can address this by
factorizing multimodal data into shared and unique information. However,
existing factorization approaches focus on two modalities, whereas RRL can
benefit from various geospatial data. Extending factorization beyond two
modalities is non-trivial because modeling high-order relationships introduces
a combinatorial number of learning objectives, increasing model complexity. We
introduce Cross modal Knowledge Injected Embedding, an information
factorization approach for RRL that captures both shared and unique
representations. CooKIE uses a pairwise inter-view learning approach that
captures high-order information without modeling high-order dependency,
avoiding exhaustive combinations. We evaluate CooKIE on three regression tasks
and a land use classification task in New York City and Delhi, India. Results
show that CooKIE outperforms existing RRL methods and a factorized RRL model,
capturing multimodal information with fewer training parameters and
floating-point operations per second (FLOPs). We release the code:
https://github.com/MinNamgung/CooKIE.

</details>


### [391] [GAIA: A Foundation Model for Operational Atmospheric Dynamics](https://arxiv.org/abs/2505.18179)
*Ata Akbari Asanjan,Olivia Alexander,Tom Berg,Clara Zhang,Matt Yang,Jad Makki,Disha Shidham,Srija Chakraborty,William Bender,Stephen Peng,Arun Ravindran,Olivier Raiman,David Potere,David Bell*

Main category: cs.LG

TL;DR: GAIA模型结合MAE和DINO自监督学习技术，用于卫星图像中的全球大气模式分析，在填补缺失数据和降水估计任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决卫星数据分析中的两个关键挑战：重建缺失区域和估计降水模式，提升天气监测和气候分析的准确性。

Method: 整合掩码自编码器(MAE)和无标签自蒸馏(DINO)的自监督学习方法，同时捕捉局部特征和全局依赖关系。

Result: 模型在填补不同掩码比例的缺失数据时表现优异，降水估计任务中误报率仅为0.088，结构相似性达0.881。

Conclusion: GAIA模型为大气科学中的自监督学习提供了新进展，可作为天气监测和气候分析的基础工具，模型权重和代码已开源。

Abstract: We present the GAIA (Geospatial Artificial Intelligence for Atmospheres)
Foundation Model, a novel model that combines masked autoencoders (MAE) and
self-DIstillation with NO labels (DINO) for analyzing global atmospheric
patterns in satellite imagery. By integrating these complementary
self-supervised learning approaches, our model simultaneously captures both
local features and global dependencies. We address two critical challenges in
satellite data analysis: reconstructing missing regions and estimating
precipitation patterns as our first downstream tasks. The model demonstrates
superior temporal pattern capture compared to standard MAE approaches, while
maintaining robust performance in downstream tasks. Our experimental results
show strong gap-filling capabilities across varying mask ratios and accurate
precipitation estimation with limited training data, achieving a false alarm
ratio of 0.088 and structural similarity of 0.881. This work represents an
advancement in self-supervised learning for atmospheric science, providing a
foundation for improved weather monitoring and climate analysis. The trained
model weights and accompanying code are publicly available as open-source on
Hugging Face here: https://huggingface.co/bcg-usra-nasa-gaia/GAIA-v1.

</details>


### [392] [2DNMRGym: An Annotated Experimental Dataset for Atom-Level Molecular Representation Learning in 2D NMR via Surrogate Supervision](https://arxiv.org/abs/2505.18181)
*Yunrui Li,Hao Xu,Pengyu Hong*

Main category: cs.LG

TL;DR: 该论文介绍了2DNMRGym，首个用于2D NMR分子表示学习的标注实验数据集，包含22,000多个HSQC光谱，支持机器学习模型训练与评估。


<details>
  <summary>Details</summary>
Motivation: 2D NMR光谱解析复杂且依赖专家，机器学习潜力巨大但缺乏高质量标注数据集。

Method: 构建2DNMRGym数据集，采用算法生成标注作为监督，人类标注作为金标准评估模型泛化能力。

Result: 提供基准测试结果，使用2D/3D GNN和GNN transformer模型，为NMR结构任务建立评估标准。

Conclusion: 2DNMRGym支持可扩展模型训练，为NMR引导的分子表示学习提供开源数据和基准。

Abstract: Two-dimensional (2D) Nuclear Magnetic Resonance (NMR) spectroscopy,
particularly Heteronuclear Single Quantum Coherence (HSQC) spectroscopy, plays
a critical role in elucidating molecular structures, interactions, and
electronic properties. However, accurately interpreting 2D NMR data remains
labor-intensive and error-prone, requiring highly trained domain experts,
especially for complex molecules. Machine Learning (ML) holds significant
potential in 2D NMR analysis by learning molecular representations and
recognizing complex patterns from data. However, progress has been limited by
the lack of large-scale and high-quality annotated datasets. In this work, we
introduce 2DNMRGym, the first annotated experimental dataset designed for
ML-based molecular representation learning in 2D NMR. It includes over 22,000
HSQC spectra, along with the corresponding molecular graphs and SMILES strings.
Uniquely, 2DNMRGym adopts a surrogate supervision setup: models are trained
using algorithm-generated annotations derived from a previously validated
method and evaluated on a held-out set of human-annotated gold-standard labels.
This enables rigorous assessment of a model's ability to generalize from
imperfect supervision to expert-level interpretation. We provide benchmark
results using a series of 2D and 3D GNN and GNN transformer models,
establishing a strong foundation for future work. 2DNMRGym supports scalable
model training and introduces a chemically meaningful benchmark for evaluating
atom-level molecular representations in NMR-guided structural tasks. Our data
and code is open-source and available on Huggingface and Github.

</details>


### [393] [Riemannian Flow Matching for Brain Connectivity Matrices via Pullback Geometry](https://arxiv.org/abs/2505.18193)
*Antoine Collas,Ce Ju,Nicolas Salvy,Bertrand Thirion*

Main category: cs.LG

TL;DR: 提出DiffeoCFM方法，利用全局微分同胚诱导的拉回度量，在矩阵流形上实现高效的条件流匹配，用于生成脑功能连接矩阵。


<details>
  <summary>Details</summary>
Motivation: 生成真实的脑功能连接矩阵对分析脑组织异质性、理解疾病及增强分类问题数据至关重要。传统黎曼工具计算效率低，需重新定义核心操作。

Method: DiffeoCFM通过全局微分同胚将流形数据映射到欧氏空间，利用标准CFM学习向量场，再通过逆变换保持流形约束。采用矩阵对数（协方差矩阵）和归一化Cholesky分解（相关矩阵）两种实现。

Result: 在超过4600次fMRI扫描和30000次EEG试验的大规模数据集上验证，DiffeoCFM训练速度快且性能达到最优，同时严格保持流形约束。

Conclusion: DiffeoCFM为脑连接矩阵生成提供了高效解决方案，其欧氏空间等价性实现了快速采样，在神经影像领域具有广泛应用潜力。

Abstract: Generating realistic brain connectivity matrices is key to analyzing
population heterogeneity in brain organization, understanding disease, and
augmenting data in challenging classification problems. Functional connectivity
matrices lie in constrained spaces--such as the set of symmetric positive
definite or correlation matrices--that can be modeled as Riemannian manifolds.
However, using Riemannian tools typically requires redefining core operations
(geodesics, norms, integration), making generative modeling computationally
inefficient. In this work, we propose DiffeoCFM, an approach that enables
conditional flow matching (CFM) on matrix manifolds by exploiting pullback
metrics induced by global diffeomorphisms on Euclidean spaces. We show that
Riemannian CFM with such metrics is equivalent to applying standard CFM after
data transformation. This equivalence allows efficient vector field learning,
and fast sampling with standard ODE solvers. We instantiate DiffeoCFM with two
different settings: the matrix logarithm for covariance matrices and the
normalized Cholesky decomposition for correlation matrices. We evaluate
DiffeoCFM on three large-scale fMRI datasets with more than 4600 scans from
2800 subjects (ADNI, ABIDE, OASIS-3) and two EEG motor imagery datasets with
over 30000 trials from 26 subjects (BNCI2014-002 and BNCI2015-001). It enables
fast training and achieves state-of-the-art performance, all while preserving
manifold constraints.

</details>


### [394] [Evidence-Grounded Multimodal Misinformation Detection with Attention-Based GNNs](https://arxiv.org/abs/2505.18221)
*Sharad Duwal,Mir Nafis Sharear Shopnil,Abhishek Tyagi,Adiba Mahbub Proma*

Main category: cs.LG

TL;DR: 提出基于图神经网络的方法检测多模态脱语境虚假信息，通过构建证据图和声明图进行一致性对比，准确率达93.05%，优于主流大模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如LLMs/LVLMs）缺乏语境化步骤导致幻觉问题，需开发针对性方案解决图像-标题对的一致性验证难题。

Method: 构建证据图和声明图，利用图神经网络编码并比较两种图表示，评估图像与标题的语义一致性。

Result: 检测准确率93.05%，超出次优LLM方法2.82%，验证了小规模任务专用模型的有效性。

Conclusion: 图结构方法在脱语境虚假信息检测中具有显著优势，为特定任务设计的小模型可超越通用大模型。

Abstract: Multimodal out-of-context (OOC) misinformation is misinformation that
repurposes real images with unrelated or misleading captions. Detecting such
misinformation is challenging because it requires resolving the context of the
claim before checking for misinformation. Many current methods, including LLMs
and LVLMs, do not perform this contextualization step. LLMs hallucinate in
absence of context or parametric knowledge. In this work, we propose a
graph-based method that evaluates the consistency between the image and the
caption by constructing two graph representations: an evidence graph, derived
from online textual evidence, and a claim graph, from the claim in the caption.
Using graph neural networks (GNNs) to encode and compare these representations,
our framework then evaluates the truthfulness of image-caption pairs. We create
datasets for our graph-based method, evaluate and compare our baseline model
against popular LLMs on the misinformation detection task. Our method scores
$93.05\%$ detection accuracy on the evaluation set and outperforms the
second-best performing method (an LLM) by $2.82\%$, making a case for smaller
and task-specific methods.

</details>


### [395] [Token Reduction Should Go Beyond Efficiency in Generative Models -- From Vision, Language to Multimodality](https://arxiv.org/abs/2505.18227)
*Zhenglun Kong,Yize Li,Fanhu Zeng,Lei Xin,Shvat Messica,Xue Lin,Pu Zhao,Manolis Kellis,Hao Tang,Marinka Zitnik*

Main category: cs.LG

TL;DR: 本文主张在大模型时代，token缩减不应仅作为效率策略，而应成为生成建模的核心原则，影响模型架构与应用。


<details>
  <summary>Details</summary>
Motivation: 传统上，token缩减主要用于提升Transformer的效率，但本文认为其在大生成模型中应发挥更广泛的作用，如促进多模态整合、减少幻觉等。

Method: 通过重新定位token缩减的角色，提出其在多模态系统、长序列一致性、训练稳定性等方面的潜在价值。

Result: token缩减不仅能提升效率，还能增强模型鲁棒性、可解释性，并更好地与生成建模目标对齐。

Conclusion: token缩减是生成建模的关键原则，未来可探索其在算法设计、强化学习引导等方向的应用。

Abstract: In Transformer architectures, tokens\textemdash discrete units derived from
raw data\textemdash are formed by segmenting inputs into fixed-length chunks.
Each token is then mapped to an embedding, enabling parallel attention
computations while preserving the input's essential information. Due to the
quadratic computational complexity of transformer self-attention mechanisms,
token reduction has primarily been used as an efficiency strategy. This is
especially true in single vision and language domains, where it helps balance
computational costs, memory usage, and inference latency. Despite these
advances, this paper argues that token reduction should transcend its
traditional efficiency-oriented role in the era of large generative models.
Instead, we position it as a fundamental principle in generative modeling,
critically influencing both model architecture and broader applications.
Specifically, we contend that across vision, language, and multimodal systems,
token reduction can: (i) facilitate deeper multimodal integration and
alignment, (ii) mitigate "overthinking" and hallucinations, (iii) maintain
coherence over long inputs, and (iv) enhance training stability, etc. We
reframe token reduction as more than an efficiency measure. By doing so, we
outline promising future directions, including algorithm design, reinforcement
learning-guided token reduction, token optimization for in-context learning,
and broader ML and scientific domains. We highlight its potential to drive new
model architectures and learning strategies that improve robustness, increase
interpretability, and better align with the objectives of generative modeling.

</details>


### [396] [Follow the Energy, Find the Path: Riemannian Metrics from Energy-Based Models](https://arxiv.org/abs/2505.18230)
*Louis Béthune,David Vigouroux,Yilun Du,Rufin VanRullen,Thomas Serre,Victor Boutin*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: What is the shortest path between two data points lying in a high-dimensional
space? While the answer is trivial in Euclidean geometry, it becomes
significantly more complex when the data lies on a curved manifold -- requiring
a Riemannian metric to describe the space's local curvature. Estimating such a
metric, however, remains a major challenge in high dimensions.
  In this work, we propose a method for deriving Riemannian metrics directly
from pretrained Energy-Based Models (EBMs) -- a class of generative models that
assign low energy to high-density regions. These metrics define spatially
varying distances, enabling the computation of geodesics -- shortest paths that
follow the data manifold's intrinsic geometry. We introduce two novel metrics
derived from EBMs and show that they produce geodesics that remain closer to
the data manifold and exhibit lower curvature distortion, as measured by
alignment with ground-truth trajectories. We evaluate our approach on
increasingly complex datasets: synthetic datasets with known data density,
rotated character images with interpretable geometry, and high-resolution
natural images embedded in a pretrained VAE latent space.
  Our results show that EBM-derived metrics consistently outperform established
baselines, especially in high-dimensional settings. Our work is the first to
derive Riemannian metrics from EBMs, enabling data-aware geodesics and
unlocking scalable, geometry-driven learning for generative modeling and
simulation.

</details>


### [397] [NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache](https://arxiv.org/abs/2505.18231)
*Donghyun Son,Euntae Choi,Sungjoo Yoo*

Main category: cs.LG

TL;DR: NSNQuant是一种无需校准的向量量化技术，用于压缩大语言模型中的KV缓存，显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理时，KV缓存占用大量内存，现有向量量化方法依赖校准数据集，易受分布偏移影响。

Method: 提出NSNQuant，通过三步变换（归一化、平移、再归一化）和Hadamard变换，将令牌分布对齐标准正态分布，实现无需校准的低比特量化。

Result: 实验表明，NSNQuant在1比特和2比特设置下均优于现有方法，吞吐量最高提升3倍。

Conclusion: NSNQuant提供了一种高效、泛化性强的KV缓存压缩方案，显著提升大语言模型推理性能。

Abstract: Large Language Model (LLM) inference is typically memory-intensive,
especially when processing large batch sizes and long sequences, due to the
large size of key-value (KV) cache. Vector Quantization (VQ) is recently
adopted to alleviate this issue, but we find that the existing approach is
susceptible to distribution shift due to its reliance on calibration datasets.
To address this limitation, we introduce NSNQuant, a calibration-free Vector
Quantization (VQ) technique designed for low-bit compression of the KV cache.
By applying a three-step transformation-1) a token-wise normalization
(Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise
normalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns
the token distribution with the standard normal distribution. This alignment
enables robust, calibration-free vector quantization using a single reusable
codebook. Extensive experiments show that NSNQuant consistently outperforms
prior methods in both 1-bit and 2-bit settings, offering strong generalization
and up to 3$\times$ throughput gain over full-precision baselines.

</details>


### [398] [ELDeR: Getting Efficient LLMs through Data-Driven Regularized Layer-wise Pruning](https://arxiv.org/abs/2505.18232)
*Mingkuan Feng,Jinyang Wu,Siyuan Liu,Shuai Zhang,Hongjian Fang,Ruihan Jin,Feihu Che,Pengpeng Shao,Zhengqi Wen,Jianhua Tao*

Main category: cs.LG

TL;DR: 论文提出ELDeR方法，通过数据驱动的正则化分层剪枝来高效压缩大语言模型，减少信息损失并降低恢复微调成本。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的高计算和内存成本限制了其应用，现有剪枝方法直接移除参数导致性能下降，需要昂贵的恢复微调。

Method: 提出先正则化再剪枝的新范式ELDeR，通过数据迭代学习各层权重并正则化小权重层的输入输出差异，保留信息到剩余层。

Result: ELDeR相比直接剪枝方法性能更优，显著降低恢复微调计算成本，分层剪枝带来明显的端到端加速效果。

Conclusion: ELDeR是一种有前景的高效大语言模型压缩技术，平衡了模型性能与计算开销。

Abstract: The deployment of Large language models (LLMs) in many fields is largely
hindered by their high computational and memory costs. Recent studies suggest
that LLMs exhibit sparsity, which can be used for pruning. Previous pruning
methods typically follow a prune-then-finetune paradigm. Since the pruned parts
still contain valuable information, statically removing them without updating
the remaining parameters often results in irreversible performance degradation,
requiring costly recovery fine-tuning (RFT) to maintain performance. To address
this, we propose a novel paradigm: first apply regularization, then prune.
Based on this paradigm, we propose ELDeR: Getting Efficient LLMs through
Data-Driven Regularized Layer-wise Pruning. We multiply the output of each
transformer layer by an initial weight, then we iteratively learn the weights
of each transformer layer by using a small amount of data in a simple way.
After that, we apply regularization to the difference between the output and
input of the layers with smaller weights, forcing the information to be
transferred to the remaining layers. Compared with direct pruning, ELDeR
reduces the information loss caused by direct parameter removal, thus better
preserving the model's language modeling ability. Experimental results show
that ELDeR achieves superior performance compared with powerful layer-wise
structured pruning methods, while greatly reducing RFT computational costs.
Since ELDeR is a layer-wise pruning method, its end-to-end acceleration effect
is obvious, making it a promising technique for efficient LLMs.

</details>


### [399] [POSTER: A Multi-Signal Model for Detecting Evasive Smishing](https://arxiv.org/abs/2505.18233)
*Shaghayegh Hosseinpour,Sanchari Das*

Main category: cs.LG

TL;DR: 该论文提出了一种多通道的短信钓鱼检测模型，通过结合多种语言和结构特征，显著提高了检测准确率。


<details>
  <summary>Details</summary>
Motivation: 短信钓鱼（Smishing）通过模仿合法通信，对移动用户构成日益严重的威胁，可能导致敏感数据或财务损失。

Method: 采用多通道检测模型，结合国家特定的语义标记、结构模式标记、字符级风格线索和上下文短语嵌入。

Result: 模型在84,000多条消息上测试，准确率达97.89%，F1分数0.963，AUC为99.73%，优于单流模型。

Conclusion: 多信号学习在钓鱼检测中表现出高效性和区域适应性，为短信钓鱼提供了稳健的解决方案。

Abstract: Smishing, or SMS-based phishing, poses an increasing threat to mobile users
by mimicking legitimate communications through culturally adapted, concise, and
deceptive messages, which can result in the loss of sensitive data or financial
resources. In such, we present a multi-channel smishing detection model that
combines country-specific semantic tagging, structural pattern tagging,
character-level stylistic cues, and contextual phrase embeddings. We curated
and relabeled over 84,000 messages across five datasets, including 24,086
smishing samples. Our unified architecture achieves 97.89% accuracy, an F1
score of 0.963, and an AUC of 99.73%, outperforming single-stream models by
capturing diverse linguistic and structural cues. This work demonstrates the
effectiveness of multi-signal learning in robust and region-aware phishing.

</details>


### [400] [A Robust PPO-optimized Tabular Transformer Framework for Intrusion Detection in Industrial IoT Systems](https://arxiv.org/abs/2505.18234)
*Yuanya She*

Main category: cs.LG

TL;DR: 提出了一种结合TabTransformer和PPO的鲁棒性网络入侵检测系统，针对IIoT中的类别不平衡和少样本攻击场景，在TON_IoT基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 工业物联网(IIoT)环境中存在类别不平衡和少样本攻击的问题，需要一种鲁棒性强的网络入侵检测系统(NIDS)来应对这些挑战。

Method: 模型整合了TabTransformer进行有效的表格特征表示，并使用近端策略优化(PPO)通过策略学习优化分类决策。

Result: 在TON_IoT基准测试中，模型实现了97.73%的宏F1分数和98.85%的准确率，即使在极少见的攻击类型（如MITM）上也达到了88.79%的F1分数。

Conclusion: 结合基于Transformer的表格学习和强化学习，展示了在实际NIDS应用中的潜力，特别是在处理类别不平衡和少样本检测方面。

Abstract: In this paper, we propose a robust and reinforcement-learning-enhanced
network intrusion detection system (NIDS) designed for class-imbalanced and
few-shot attack scenarios in Industrial Internet of Things (IIoT) environments.
Our model integrates a TabTransformer for effective tabular feature
representation with Proximal Policy Optimization (PPO) to optimize
classification decisions via policy learning. Evaluated on the
TON\textunderscore IoT benchmark, our method achieves a macro F1-score of
97.73\% and accuracy of 98.85\%. Remarkably, even on extremely rare classes
like man-in-the-middle (MITM), our model achieves an F1-score of 88.79\%,
showcasing strong robustness and few-shot detection capabilities. Extensive
ablation experiments confirm the complementary roles of TabTransformer and PPO
in mitigating class imbalance and improving generalization. These results
highlight the potential of combining transformer-based tabular learning with
reinforcement learning for real-world NIDS applications.

</details>


### [401] [The Origins of Representation Manifolds in Large Language Models](https://arxiv.org/abs/2505.18235)
*Alexander Modell,Patrick Rubin-Delanchy,Nick Whiteley*

Main category: cs.LG

TL;DR: 论文探讨了AI系统中嵌入和内部表示的可解释性，提出了特征可能表示为流形的理论，并通过实验验证了该理论在大型语言模型中的应用。


<details>
  <summary>Details</summary>
Motivation: 当前科学界致力于理解AI系统的内部表示，线性表示假说认为神经表示是稀疏线性组合，但如何更全面地建模特征（包括连续和多维特征）成为研究热点。

Method: 论文提出特征可能表示为流形，并通过余弦相似性在表示空间中编码特征的内在几何结构，验证了理论在文本嵌入和大型语言模型中的应用。

Result: 实验验证了理论的关键假设和预测，表明表示空间中的距离与概念空间中的相关性可能通过流形路径连接。

Conclusion: 论文为AI系统内部表示的可解释性提供了新的理论框架，展示了流形表示在特征建模中的潜力。

Abstract: There is a large ongoing scientific effort in mechanistic interpretability to
map embeddings and internal representations of AI systems into
human-understandable concepts. A key element of this effort is the linear
representation hypothesis, which posits that neural representations are sparse
linear combinations of `almost-orthogonal' direction vectors, reflecting the
presence or absence of different features. This model underpins the use of
sparse autoencoders to recover features from representations. Moving towards a
fuller model of features, in which neural representations could encode not just
the presence but also a potentially continuous and multidimensional value for a
feature, has been a subject of intense recent discourse. We describe why and
how a feature might be represented as a manifold, demonstrating in particular
that cosine similarity in representation space may encode the intrinsic
geometry of a feature through shortest, on-manifold paths, potentially
answering the question of how distance in representation space and relatedness
in concept space could be connected. The critical assumptions and predictions
of the theory are validated on text embeddings and token activations of large
language models.

</details>


### [402] [Decomposition of Water Demand Patterns Using Skewed Gaussian Distributions for Behavioral Insights and Operational Planning](https://arxiv.org/abs/2505.18245)
*Roy Elkayam*

Main category: cs.LG

TL;DR: 该研究提出了一种使用偏态高斯分布(SGD)分解城市用水需求模式的新方法，可揭示潜在用水行为并支持运营规划，相比传统对称高斯模型重建误差降低50%以上。


<details>
  <summary>Details</summary>
Motivation: 每小时用水需求曲线对长期基础设施设计和日常运营至关重要，但传统对称高斯模型无法准确捕捉实际用水高峰的不对称特征（如早晨急剧上升后缓慢下降的模式）。

Method: 将每日需求曲线分解为基线分量和多个峰值分量，每个峰值用可解释参数（幅度、时间、持续时间和偏态）表征，通过偏态高斯分布(SGD)建模不对称峰形。

Result: 在多个真实数据集上验证显示：SGD模型重建精度显著优于对称高斯模型（均方根误差平均降低50%），同时保持物理可解释性，并能生成具有指定特征的合成需求场景。

Conclusion: SGD框架通过物理可解释的参数化建模，实现了对城市用水模式的精准解析，在异常检测、实时需求管理和行为分析等场景具有应用价值，相关代码已开源。

Abstract: This study presents a novel approach for decomposing urban water demand
patterns using Skewed Gaussian Distributions (SGD) to derive behavioral
insights and support operational planning. Hourly demand profiles contain
critical information for both long-term infrastructure design and daily
operations, influencing network pressures, water quality, energy consumption,
and overall reliability. By breaking down each daily demand curve into a
baseline component and distinct peak components, the proposed SGD method
characterizes each peak with interpretable parameters, including peak
amplitude, timing (mean), spread (duration), and skewness (asymmetry), thereby
reconstructing the observed pattern and uncovering latent usage dynamics. This
detailed peak-level decomposition enables both operational applications, e.g.
anomaly and leakage detection, real-time demand management, and strategic
analyses, e.g. identifying behavioral shifts, seasonal influences, or policy
impacts on consumption patterns. Unlike traditional symmetric Gaussian or
purely statistical time-series models, SGDs explicitly capture asymmetric peak
shapes such as sharp morning surges followed by gradual declines, improving the
fidelity of synthetic pattern generation and enhancing the detection of
irregular consumption behavior. The method is demonstrated on several
real-world datasets, showing that SGD outperforms symmetric Gaussian models in
reconstruction accuracy, reducing root-mean-square error by over 50% on
average, while maintaining physical interpretability. The SGD framework can
also be used to construct synthetic demand scenarios by designing daily peak
profiles with chosen characteristics. All implementation code is publicly
available at: https://github.com/Relkayam/water-demand-decomposition-sgd

</details>


### [403] [Uncovering a Universal Abstract Algorithm for Modular Addition in Neural Networks](https://arxiv.org/abs/2505.18266)
*Gavin McCracken,Gabriela Moisescu-Pareja,Vincent Letourneau,Doina Precup,Jonathan Love*

Main category: cs.LG

TL;DR: 该论文提出了一种可测试的普适性假说，认为在模加法任务中观察到的不同神经网络解决方案实际上遵循统一的抽象算法——近似中国余数定理。通过多层次分析，作者证明了多层感知机和Transformer普遍实现该算法，并引入近似陪集概念。


<details>
  <summary>Details</summary>
Motivation: 先前研究认为神经元层面的表征差异反映了不同算法，但本文旨在证明这些差异背后存在统一的抽象算法，从而推动可泛化的可解释性研究。

Method: 采用多层次分析方法（神经元、神经元簇、整个网络），引入近似陪集概念，并验证深度神经网络中仅需O(log n)特征即可学习通用解决方案。

Result: 实证表明多层感知机和Transformer均实现了近似中国余数定理算法，神经元仅在近似陪集上激活，且理论适用于带可训练嵌入或多隐藏层的深度神经网络。

Conclusion: 该研究首次为多层网络解决模加法任务提供了理论支持的解释，提出了超越模加法的群乘法普适性假说，推动了可解释性研究的发展。

Abstract: We propose a testable universality hypothesis, asserting that seemingly
disparate neural network solutions observed in the simple task of modular
addition are unified under a common abstract algorithm. While prior work
interpreted variations in neuron-level representations as evidence for distinct
algorithms, we demonstrate - through multi-level analyses spanning neurons,
neuron clusters, and entire networks - that multilayer perceptrons and
transformers universally implement the abstract algorithm we call the
approximate Chinese Remainder Theorem. Crucially, we introduce approximate
cosets and show that neurons activate exclusively on them. Furthermore, our
theory works for deep neural networks (DNNs). It predicts that universally
learned solutions in DNNs with trainable embeddings or more than one hidden
layer require only O(log n) features, a result we empirically confirm. This
work thus provides the first theory-backed interpretation of multilayer
networks solving modular addition. It advances generalizable interpretability
and opens a testable universality hypothesis for group multiplication beyond
modular addition.

</details>


### [404] [Representative Action Selection for Large Action-Space Meta-Bandits](https://arxiv.org/abs/2505.18269)
*Quan Zhou,Mark Kozdoba,Shie Mannor*

Main category: cs.LG

TL;DR: 本文提出了一种基于高斯过程的epsilon-net算法，用于从大规模动作空间中选择代表性子集，以在赌博机问题中实现接近全动作空间的性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决在大规模动作空间中选择子集的问题，假设相似动作具有相关收益，通过高斯过程建模，旨在利用这种结构提高性能。

Method: 采用了一种简单的epsilon-net算法来选择代表性子集，并利用高斯过程建模动作间的相似性。

Result: 理论分析证明了该算法的性能，并通过实验与Thompson Sampling和Upper Confidence Bound进行了比较。

Conclusion: 结论表明，所提出的epsilon-net算法能够有效利用动作间的相似性，在大规模动作空间中选择子集，实现接近全动作空间的性能。

Abstract: We study the problem of selecting a subset from a large action space shared
by a family of bandits, with the goal of achieving performance nearly matching
that of using the full action space. We assume that similar actions tend to
have related payoffs, modeled by a Gaussian process. To exploit this structure,
we propose a simple epsilon-net algorithm to select a representative subset. We
provide theoretical guarantees for its performance and compare it empirically
to Thompson Sampling and Upper Confidence Bound.

</details>


### [405] [Feature Preserving Shrinkage on Bayesian Neural Networks via the R2D2 Prior](https://arxiv.org/abs/2505.18280)
*Tsai Hor Chan,Dora Yan Zhang,Guosheng Yin,Lequan Yu*

Main category: cs.LG

TL;DR: 本文提出了一种名为R2D2-Net的新方法，通过引入R^2诱导的Dirichlet分解（R2D2）先验来改进贝叶斯神经网络（BNN）的权重分布选择问题，有效减少噪声信号并防止关键特征过度收缩。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯神经网络（BNN）通过将权重视为随机变量来提供后验不确定性估计并避免过拟合，但先验分布的选择仍具挑战性。不当的先验可能导致方差膨胀或预测性能下降。现有方法难以平衡噪声信号的收缩和关键特征的保留。

Method: 提出R2D2-Net，将R2D2先验应用于BNN权重，结合变分Gibbs推理算法（结合Gibbs更新和梯度优化）以更准确地近似权重后验分布，并分析了证据下界（ELBO）和后验集中率。

Result: 在自然和医学图像分类及不确定性估计任务中，R2D2-Net表现出色，能够有效收缩无关系数并防止关键特征过度收缩。

Conclusion: R2D2-Net通过改进先验分布选择和推理算法，显著提升了BNN的性能和稳定性，尤其在处理噪声信号和关键特征时表现优异。

Abstract: Bayesian neural networks (BNNs) treat neural network weights as random
variables, which aim to provide posterior uncertainty estimates and avoid
overfitting by performing inference on the posterior weights. However, the
selection of appropriate prior distributions remains a challenging task, and
BNNs may suffer from catastrophic inflated variance or poor predictive
performance when poor choices are made for the priors. Existing BNN designs
apply different priors to weights, while the behaviours of these priors make it
difficult to sufficiently shrink noisy signals or they are prone to
overshrinking important signals in the weights. To alleviate this problem, we
propose a novel R2D2-Net, which imposes the R^2-induced Dirichlet Decomposition
(R2D2) prior to the BNN weights. The R2D2-Net can effectively shrink irrelevant
coefficients towards zero, while preventing key features from over-shrinkage.
To approximate the posterior distribution of weights more accurately, we
further propose a variational Gibbs inference algorithm that combines the Gibbs
updating procedure and gradient-based optimization. This strategy enhances
stability and consistency in estimation when the variational objective
involving the shrinkage parameters is non-convex. We also analyze the evidence
lower bound (ELBO) and the posterior concentration rates from a theoretical
perspective. Experiments on both natural and medical image classification and
uncertainty estimation tasks demonstrate satisfactory performance of our
method.

</details>


### [406] [Tube Loss based Deep Networks For Improving the Probabilistic Forecasting of Wind Speed](https://arxiv.org/abs/2505.18284)
*Pritam Anand,Aadesh Minz,Asish Joel*

Main category: cs.LG

TL;DR: 该论文提出了一种基于深度学习的概率预测方法，使用Tube损失函数进行风速预测，以量化不确定性，支持电网运营和电力市场的有效决策。


<details>
  <summary>Details</summary>
Motivation: 风速预测的不确定性量化（UQ）在风电生产中至关重要，因为风速具有固有的波动性。通过量化相关风险和回报，UQ可以支持电网运营和电力市场的更有效决策。

Method: 论文设计了一系列基于深度学习的概率预测方法，使用Tube损失函数进行风速预测。Tube损失函数是一种简单且模型无关的预测区间（PI）估计方法，可以在没有任何分布假设的情况下获得具有渐近覆盖保证的窄PI。深度学习模型结合了LSTM、GRU和TCN等流行架构，并设计了一种简单有效的启发式方法来调整Tube损失函数的δ参数。

Result: 在三个不同地点（Jaisalmer、洛杉矶和旧金山）的风速数据集上的实验结果表明，所提出的深度预测模型相比最近开发的概率风速预测方法，能够产生更可靠且更窄的预测区间。

Conclusion: 论文提出的基于Tube损失函数的深度概率预测模型在风速预测中表现出色，能够在不牺牲校准能力的情况下获得更窄的预测区间，为风电生产中的不确定性量化提供了有效解决方案。

Abstract: Uncertainty Quantification (UQ) in wind speed forecasting is a critical
challenge in wind power production due to the inherently volatile nature of
wind. By quantifying the associated risks and returns, UQ supports more
effective decision-making for grid operations and participation in the
electricity market. In this paper, we design a sequence of deep learning based
probabilistic forecasting methods by using the Tube loss function for wind
speed forecasting. The Tube loss function is a simple and model agnostic
Prediction Interval (PI) estimation approach and can obtain the narrow PI with
asymptotical coverage guarantees without any distribution assumption. Our deep
probabilistic forecasting models effectively incorporate popular architectures
such as LSTM, GRU, and TCN within the Tube loss framework. We further design a
simple yet effective heuristic for tuning the $\delta$ parameter of the Tube
loss function so that our deep forecasting models obtain the narrower PI
without compromising its calibration ability. We have considered three wind
datasets, containing the hourly recording of the wind speed, collected from
three distinct location namely Jaisalmer, Los Angeles and San Fransico. Our
numerical results demonstrate that the proposed deep forecasting models produce
more reliable and narrower PIs compared to recently developed probabilistic
wind forecasting methods.

</details>


### [407] [Convexified Message-Passing Graph Neural Networks](https://arxiv.org/abs/2505.18289)
*Saar Cohen,Noa Agmon,Uri Shaham*

Main category: cs.LG

TL;DR: 论文提出了一种结合消息传递图神经网络与凸优化的新框架CGNN，通过凸优化训练实现高效求解，并在多个基准数据集上显著超越现有GNN模型。


<details>
  <summary>Details</summary>
Motivation: 当前图神经网络（GNNs）在图形预测任务中表现优异，但训练过程通常涉及非凸优化，难以保证最优解和理论分析。本文旨在结合消息传递GNN的优势与凸优化的可解释性，提出一种更高效且理论可证的新方法。

Method: 论文提出凸化消息传递图神经网络（CGNNs），通过将非线性滤波器映射到再生核希尔伯特空间，将训练转化为凸优化问题，并采用投影梯度法高效求解。对于深层架构，采用分层训练策略。

Result: 实验表明，CGNNs在多数基准数据集上比领先的GNN模型准确率提升10%至40%，且在资源效率上优于过参数化的非凸模型。即使在改进不显著的情况下，CGNNs仍能匹配或略优于基线模型。

Conclusion: CGNNs作为一种理论严谨、性能优越的框架，不仅显著提升了图表示学习的准确性，还通过凸优化保证了训练的高效性和可解释性，展现了广泛的适用性和鲁棒性。

Abstract: Graph Neural Networks (GNNs) have become prominent methods for graph
representation learning, demonstrating strong empirical results on diverse
graph prediction tasks. In this paper, we introduce Convexified Message Passing
Graph Neural Networks (CGNNs), a novel and general framework that combines the
power of message-passing GNNs with the tractability of convex optimization. By
mapping their nonlinear filters into a reproducing kernel Hilbert space, CGNNs
transform training into a convex optimization problem, which can be solved
efficiently and optimally by projected gradient methods. This convexity further
allows the statistical properties of CGNNs to be analyzed accurately and
rigorously. For two-layer CGNNs, we establish rigorous generalization
guarantees, showing convergence to the performance of the optimal GNN. To scale
to deeper architectures, we adopt a principled layer-wise training strategy.
Experiments on benchmark datasets show that CGNNs significantly exceed the
performance of leading GNN models, achieving 10 to 40 percent higher accuracy
in most cases, underscoring their promise as a powerful and principled method
with strong theoretical foundations. In rare cases where improvements are not
quantitatively substantial, the convex models either slightly exceed or match
the baselines, stressing their robustness and wide applicability. Though
over-parameterization is often employed to enhance performance in nonconvex
models, we show that our CGNNs framework yields shallow convex models that can
surpass these models in both accuracy and resource efficiency.

</details>


### [408] [Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient Nonlinear MCMC on General Graphs](https://arxiv.org/abs/2505.18300)
*Jie Hu,Yi-Ting Ma,Do Young Eun*

Main category: cs.LG

TL;DR: 提出基于历史驱动的目标（HDT）框架，改进离散状态空间上的随机游走算法，实现高效采样并兼容可逆/非可逆MCMC方法。


<details>
  <summary>Details</summary>
Motivation: 现有自排斥随机游走（SRRW）方法虽能降低方差，但计算开销大且仅适用于可逆马尔可夫链，限制了其在高级MCMC场景的应用。

Method: 通过历史访问频率构建动态目标分布π[x]替代原分布μ，仅需当前与候选状态的局部信息，兼容各类图采样器并保持无偏性。

Result: 实验证明HDT在保持近零方差的同时，计算效率显著提升，且LRU缓存机制支持大规模图采样。

Conclusion: HDT框架以轻量级设计突破SRRW的限制，为网络科学和分布式优化提供了通用的高效采样方案。

Abstract: We propose a history-driven target (HDT) framework in Markov Chain Monte
Carlo (MCMC) to improve any random walk algorithm on discrete state spaces,
such as general undirected graphs, for efficient sampling from target
distribution $\boldsymbol{\mu}$. With broad applications in network science and
distributed optimization, recent innovations like the self-repellent random
walk (SRRW) achieve near-zero variance by prioritizing under-sampled states
through transition kernel modifications based on past visit frequencies.
However, SRRW's reliance on explicit computation of transition probabilities
for all neighbors at each step introduces substantial computational overhead,
while its strict dependence on time-reversible Markov chains excludes advanced
non-reversible MCMC methods. To overcome these limitations, instead of direct
modification of transition kernel, HDT introduces a history-dependent target
distribution $\boldsymbol{\pi}[\mathbf{x}]$ to replace the original target
$\boldsymbol{\mu}$ in any graph sampler, where $\mathbf{x}$ represents the
empirical measure of past visits. This design preserves lightweight
implementation by requiring only local information between the current and
proposed states and achieves compatibility with both reversible and
non-reversible MCMC samplers, while retaining unbiased samples with target
distribution $\boldsymbol{\mu}$ and near-zero variance performance. Extensive
experiments in graph sampling demonstrate consistent performance gains, and a
memory-efficient Least Recently Used (LRU) cache ensures scalability to large
general graphs.

</details>


### [409] [PLUMAGE: Probabilistic Low rank Unbiased Min Variance Gradient Estimator for Efficient Large Model Training](https://arxiv.org/abs/2505.18313)
*Matan Haroush,Daniel Soudry*

Main category: cs.LG

TL;DR: PLUMAGE是一种新型低秩梯度估计器，解决了现有方法偏差大、方差高及优化器状态不对齐问题，在保持计算内存开销相近的同时显著提升训练效果。


<details>
  <summary>Details</summary>
Motivation: 当前训练大型语言模型时，内存和网络带宽成为主要瓶颈。现有低秩梯度估计方法（如GaLoRE、FLORA）存在偏差大、方差高的问题，且优化器状态在投影更新时会出现不对齐，导致训练不稳定。

Method: 提出PLUMAGE（概率低秩无偏最小方差梯度估计器），作为现有低秩梯度估计器的即插即用替代方案。该方法无需引入新超参数，并解决了优化器状态不对齐问题。

Result: 实验表明，PLUMAGE将全秩优化的预训练评估损失差距平均缩小33%，在GLUE基准上的平均训练损失降低28%，同时保持与GaLoRE相近的计算和内存开销。

Conclusion: PLUMAGE通过无偏低秩估计和优化器状态对齐机制，显著提升了训练稳定性和模型性能，为资源受限环境下的LLM训练提供了有效解决方案。

Abstract: Accelerator memory and networking constraints have emerged as dominant
bottlenecks when training large language models LLMs with billions of
parameters. Existing low rank gradient estimators such as GaLoRE and FLORA
compress gradients and optimizer tensors by projecting weight gradients onto a
rank r subspace, enabling LLM training on consumer hardware. Yet, these methods
are either biased or subject to high estimator variance. Moreover, the
optimizer state based on the first and second moments estimates expressed in
the previous subspace becomes misaligned whenever the projection is updated,
leading to instabilities during training. We propose PLUMAGE: Probabilistic Low
rank Unbiased Minimum vAriance Gradient Estimator. PLUMAGE is a drop in
replacement for existing low rank gradient estimators. It does not introduce
new hyperparameters beyond the chosen rank r and the update interval. In
addition, we resolve optimizer state misalignment issues to prevent spurious
weight updates and enhance training stability. We empirically demonstrate that
PLUMAGE shrinks the full rank optimization's gap over the pre training
evaluation loss by 33% on average across models and the average training loss
across the GLUE benchmark by 28% within a similar computational and memory
footprint as GaloRE.

</details>


### [410] [Sample Complexity of Diffusion Model Training Without Empirical Risk Minimizer Access](https://arxiv.org/abs/2505.18344)
*Mudit Gaur,Prashant Trivedi,Sasidhar Kunapuli,Amrit Singh Bedi,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Diffusion models have demonstrated state-of-the-art performance across
vision, language, and scientific domains. Despite their empirical success,
prior theoretical analyses of the sample complexity suffer from poor scaling
with input data dimension or rely on unrealistic assumptions such as access to
exact empirical risk minimizers. In this work, we provide a principled analysis
of score estimation, establishing a sample complexity bound of
$\widetilde{\mathcal{O}}(\epsilon^{-6})$. Our approach leverages a structured
decomposition of the score estimation error into statistical, approximation,
and optimization errors, enabling us to eliminate the exponential dependence on
neural network parameters that arises in prior analyses. It is the first such
result which achieves sample complexity bounds without assuming access to the
empirical risk minimizer of score function estimation loss.

</details>


### [411] [Diffusion Self-Weighted Guidance for Offline Reinforcement Learning](https://arxiv.org/abs/2505.18345)
*Augusto Tagle,Javier Ruiz-del-Solar,Felipe Tobar*

Main category: cs.LG

TL;DR: 该论文提出了一种名为自加权引导（SWG）的新方法，用于离线强化学习中的策略优化，通过扩散模型直接获取所需分数，简化了训练流程。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中，现有方法依赖于未知的权重函数，导致计算困难。本文旨在解决这一问题，提出一种更高效的方法。

Method: 论文提出了一种自加权引导（SWG）方法，通过构建动作和权重的扩散模型，直接从扩散模型中获取所需分数，无需额外学习网络。

Result: SWG在玩具示例中生成所需分布的样本，并在D4RL的挑战性环境中与最先进方法表现相当，同时简化了训练流程。

Conclusion: SWG方法有效解决了离线强化学习中的权重计算问题，简化了训练流程，并在实验中验证了其有效性。

Abstract: Offline reinforcement learning (RL) recovers the optimal policy $\pi$ given
historical observations of an agent. In practice, $\pi$ is modeled as a
weighted version of the agent's behavior policy $\mu$, using a weight function
$w$ working as a critic of the agent's behavior. Though recent approaches to
offline RL based on diffusion models have exhibited promising results, the
computation of the required scores is challenging due to their dependence on
the unknown $w$. In this work, we alleviate this issue by constructing a
diffusion over both the actions and the weights. With the proposed setting, the
required scores are directly obtained from the diffusion model without learning
extra networks. Our main conceptual contribution is a novel guidance method,
where guidance (which is a function of $w$) comes from the same diffusion
model, therefore, our proposal is termed Self-Weighted Guidance (SWG). We show
that SWG generates samples from the desired distribution on toy examples and
performs on par with state-of-the-art methods on D4RL's challenging
environments, while maintaining a streamlined training pipeline. We further
validate SWG through ablation studies on weight formulations and scalability.

</details>


### [412] [The Cell Must Go On: Agar.io for Continual Reinforcement Learning](https://arxiv.org/abs/2505.18347)
*Mohamed A. Mohamed,Kateryna Nekhomiazh,Vedant Vyas,Marcos M. Jose,Andrew Patterson,Marlos C. Machado*

Main category: cs.LG

TL;DR: 本文介绍了AgarCL，一个基于Agar.io游戏的持续强化学习研究平台，用于评估智能体在不断变化环境中的学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有的持续强化学习研究多局限于简单的模拟环境，或通过人为修改任务来模拟环境变化，缺乏复杂且真实的测试平台。

Method: 基于Agar.io游戏开发了AgarCL平台，该平台具有非阶段性、高维度、随机动态变化、连续动作和部分可观测性等特点。

Result: 在AgarCL平台上测试了DQN、PPO和SAC等算法，评估了它们在持续强化学习任务中的表现，并提供了基准结果。

Conclusion: AgarCL为持续强化学习研究提供了一个复杂且真实的测试环境，有助于深入理解智能体在动态环境中的学习能力。

Abstract: Continual reinforcement learning (RL) concerns agents that are expected to
learn continually, rather than converge to a policy that is then fixed for
evaluation. Such an approach is well suited to environments the agent perceives
as changing, which renders any static policy ineffective over time. The few
simulators explicitly designed for empirical research in continual RL are often
limited in scope or complexity, and it is now common for researchers to modify
episodic RL environments by artificially incorporating abrupt task changes
during interaction. In this paper, we introduce AgarCL, a research platform for
continual RL that allows for a progression of increasingly sophisticated
behaviour. AgarCL is based on the game Agar.io, a non-episodic,
high-dimensional problem featuring stochastic, ever-evolving dynamics,
continuous actions, and partial observability. Additionally, we provide
benchmark results reporting the performance of DQN, PPO, and SAC in both the
primary, challenging continual RL problem, and across a suite of smaller tasks
within AgarCL, each of which isolates aspects of the full environment and allow
us to characterize the challenges posed by different aspects of the game.

</details>


### [413] [Task Specific Pruning with LLM-Sieve: How Many Parameters Does Your Task Really Need?](https://arxiv.org/abs/2505.18350)
*Waleed Reda,Abhinav Jangda,Krishna Chintalapudi*

Main category: cs.LG

TL;DR: LLM-Sieve框架通过任务感知联合投影和遗传算法，实现大语言模型20-75%参数削减，仅损失1-5%准确率。


<details>
  <summary>Details</summary>
Motivation: 研究如何在资源受限环境下，为大语言模型的任务特定应用寻找最优参数规模。

Method: 结合任务感知联合投影优化输出行为，使用遗传算法为每个权重矩阵确定差异化剪枝比例。

Result: 在跨领域任务中实现显著参数压缩（20-75%），同时保持高准确率（仅下降1-5%）。

Conclusion: LLM-Sieve为创建高效任务专用模型提供了实用且鲁棒的解决方案，支持与LoRA微调和量化技术兼容。

Abstract: As Large Language Models (LLMs) are increasingly being adopted for narrow
tasks - such as medical question answering or sentiment analysis - and deployed
in resource-constrained settings, a key question arises: how many parameters
does a task actually need? In this work, we present LLM-Sieve, the first
comprehensive framework for task-specific pruning of LLMs that achieves 20-75%
parameter reduction with only 1-5% accuracy degradation across diverse domains.
Unlike prior methods that apply uniform pruning or rely on low-rank
approximations of weight matrices or inputs in isolation, LLM-Sieve (i) learns
task-aware joint projections to better approximate output behavior, and (ii)
employs a Genetic Algorithm to discover differentiated pruning levels for each
matrix. LLM-Sieve is fully compatible with LoRA fine-tuning and quantization,
and uniquely demonstrates strong generalization across datasets within the same
task domain. Together, these results establish a practical and robust mechanism
to generate smaller performant task-specific models.

</details>


### [414] [X-MethaneWet: A Cross-scale Global Wetland Methane Emission Benchmark Dataset for Advancing Science Discovery with AI](https://arxiv.org/abs/2505.18355)
*Yiming Sun,Shuo Chen,Shengyu Chen,Chonghao Qiu,Licheng Liu,Youmi Oh,Sparkle L. Malone,Gavin McNicol,Qianlai Zhuang,Chris Smith,Yiqun Xie,Xiaowei Jia*

Main category: cs.LG

TL;DR: 该论文提出了首个跨尺度全球湿地甲烷基准数据集X-MethaneWet，结合物理模型和观测数据，评估了深度学习模型在甲烷通量预测中的表现，并探索了迁移学习技术以提高模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 甲烷是仅次于二氧化碳的强效温室气体，准确模拟其全球尺度的通量变化对于理解其时空变异性和制定有效减缓策略至关重要。

Method: 论文整合了TEM-MDM物理模型模拟数据和FLUXNET-CH4观测数据，构建了X-MethaneWet数据集，并评估了多种序列深度学习模型的性能，同时探索了四种迁移学习技术。

Result: 实验证明，所提出的方法能有效提升甲烷排放建模的准确性，为开发更精确、可扩展的AI驱动气候模型提供了潜力。

Conclusion: 该研究通过结合物理模型与AI算法，为全球湿地甲烷建模提供了新工具，有助于推动气候科学的发展。

Abstract: Methane (CH$_4$) is the second most powerful greenhouse gas after carbon
dioxide and plays a crucial role in climate change due to its high global
warming potential. Accurately modeling CH$_4$ fluxes across the globe and at
fine temporal scales is essential for understanding its spatial and temporal
variability and developing effective mitigation strategies. In this work, we
introduce the first-of-its-kind cross-scale global wetland methane benchmark
dataset (X-MethaneWet), which synthesizes physics-based model simulation data
from TEM-MDM and the real-world observation data from FLUXNET-CH$_4$. This
dataset can offer opportunities for improving global wetland CH$_4$ modeling
and science discovery with new AI algorithms. To set up AI model baselines for
methane flux prediction, we evaluate the performance of various sequential deep
learning models on X-MethaneWet. Furthermore, we explore four different
transfer learning techniques to leverage simulated data from TEM-MDM to improve
the generalization of deep learning models on real-world FLUXNET-CH$_4$
observations. Our extensive experiments demonstrate the effectiveness of these
approaches, highlighting their potential for advancing methane emission
modeling and contributing to the development of more accurate and scalable
AI-driven climate models.

</details>


### [415] [Small Models, Smarter Learning: The Power of Joint Task Training](https://arxiv.org/abs/2505.18369)
*Csaba Both,Benjamin Hoover,Hendrik Strobelt,Dmitry Krotov,Daniel Karl I. Weidele,Mauro Martino,Nima Dehmamy*

Main category: cs.LG

TL;DR: 研究发现，模型学习任务的能力与任务难度和模型大小密切相关。通过逐步增加ListOps数据集中数学运算的复杂度，发现SUM模n最难学，但与其他运算结合后学习变易。联合训练不仅提升性能，还改变模型行为。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解任务难度与小型Transformer模型学习特定任务所需最少参数之间的关系，探索模型在不同任务组合下的学习行为差异。

Method: 使用ListOps数据集，逐步增加数学运算复杂度（如引入SUM模n、MAX、MED等），对比单一任务与联合训练下模型的表现及内部机制差异。

Result: 1. SUM模n最难学，但与其他运算结合后参数需求降低；2. 联合训练模型表现出数值化嵌入表征和奇偶区分能力；3. 单一SUM任务模型依赖前馈层，联合训练模型则激活注意力机制；4. 在MAX+MED上预训练可突破纯SUM学习阈值。

Conclusion: 语言模型的涌现能力不仅取决于模型规模，还与训练课程设计相关。任务组合和训练顺序能显著影响学习效率及内部表征特性。

Abstract: The ability of a model to learn a task depends strongly on both the task
difficulty and the model size. We aim to understand how task difficulty relates
to the minimum number of parameters required for learning specific tasks in
small transformer models. Our study focuses on the ListOps dataset, which
consists of nested mathematical operations. We gradually increase task
difficulty by introducing new operations or combinations of operations into the
training data. We observe that sum modulo n is the hardest to learn. Curiously,
when combined with other operations such as maximum and median, the sum
operation becomes easier to learn and requires fewer parameters. We show that
joint training not only improves performance but also leads to qualitatively
different model behavior. We show evidence that models trained only on SUM
might be memorizing and fail to capture the number structure in the embeddings.
In contrast, models trained on a mixture of SUM and other operations exhibit
number-like representations in the embedding space, and a strong ability to
distinguish parity. Furthermore, the SUM-only model relies more heavily on its
feedforward layers, while the jointly trained model activates the attention
mechanism more. Finally, we show that learning pure SUM can be induced in
models below the learning threshold of pure SUM, by pretraining them on
MAX+MED. Our findings indicate that emergent abilities in language models
depend not only on model size, but also the training curriculum.

</details>


### [416] [Next-token pretraining implies in-context learning](https://arxiv.org/abs/2505.18373)
*Paul M. Riechers,Henry R. Bigelow,Eric A. Alt,Adam Shai*

Main category: cs.LG

TL;DR: 本文论证了上下文学习（ICL）是标准自监督下一词预训练的自然结果，而非神秘涌现特性，并通过信息论框架和实验验证了这一观点。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于澄清上下文学习（ICL）的起源，证明其是自监督预训练的必然产物，而非不可预测的涌现行为，从而为理解模型如何从上下文中学习提供理论基础。

Method: 方法包括构建信息论框架分析上下文学习的动态特性，并使用具有不同相关结构的合成数据集进行实验，验证理论预测，如训练损失中的相变和上下文损失的幂律缩放。

Result: 结果表明，模型在上下文中的表现与预训练中遇到的任务集合数学上耦合，验证了信息论框架对上下文学习动态的精确预测，并重现了训练中的特征现象。

Conclusion: 结论是上下文学习可预测地源于标准自监督预训练，其性能与预训练任务集合紧密相关，这一发现为理解模型推理时学习提供了架构和模态无关的基本原则。

Abstract: We argue that in-context learning (ICL) predictably arises from standard
self-supervised next-token pretraining, rather than being an exotic emergent
property. This work establishes the foundational principles of this emergence
by focusing on in-distribution ICL, demonstrating how models necessarily adapt
to context when trained on token sequences, especially from non-ergodic
sources. Our information-theoretic framework precisely predicts these
in-distribution ICL dynamics (i.e., context-dependent loss reduction). We
verify this with experiments using synthetic datasets of differing types of
correlational structure, reproducing characteristic phenomena like phase
transitions in training loss for induction head formation and power-law scaling
of in-context loss. We further show that a model's in-context performance on
any task is mathematically coupled to the ensemble of tasks seen in
pretraining, offering a fundamental explanation, grounded in architecture- and
modality-independent principles, for such inference-time learning.

</details>


### [417] [Applications of Modular Co-Design for De Novo 3D Molecule Generation](https://arxiv.org/abs/2505.18392)
*Danny Reidenbach,Filipp Nikitin,Olexandr Isayev,Saee Paliwal*

Main category: cs.LG

TL;DR: Megalodon，一种可扩展的Transformer模型家族，通过结合连续和离散去噪目标，显著提升了3D分子生成的质量和能量表现。


<details>
  <summary>Details</summary>
Motivation: 当前几何生成模型在生成高质量3D分子结构方面表现不佳，即使能保持2D有效性和拓扑稳定性。为了解决这一问题，提升分子生成动力学的学习效果。

Method: 提出Megalodon模型家族，结合基本等变层，并使用联合连续和离散去噪目标进行训练。

Result: Megalodon在3D分子生成、条件结构生成和结构能量基准测试中达到最先进水平，参数加倍后性能显著提升。

Conclusion: Megalodon在3D分子生成任务中表现出色，尤其在生成大型分子和降低能量水平方面优于现有模型。

Abstract: De novo 3D molecule generation is a pivotal task in drug discovery. However,
many recent geometric generative models struggle to produce high-quality 3D
structures, even if they maintain 2D validity and topological stability. To
tackle this issue and enhance the learning of effective molecular generation
dynamics, we present Megalodon-a family of scalable transformer models. These
models are enhanced with basic equivariant layers and trained using a joint
continuous and discrete denoising co-design objective. We assess Megalodon's
performance on established molecule generation benchmarks and introduce new 3D
structure benchmarks that evaluate a model's capability to generate realistic
molecular structures, particularly focusing on energetics. We show that
Megalodon achieves state-of-the-art results in 3D molecule generation,
conditional structure generation, and structure energy benchmarks using
diffusion and flow matching. Furthermore, doubling the number of parameters in
Megalodon to 40M significantly enhances its performance, generating up to 49x
more valid large molecules and achieving energy levels that are 2-10x lower
than those of the best prior generative models.

</details>


### [418] [Thought calibration: Efficient and confident test-time scaling](https://arxiv.org/abs/2505.18404)
*Menghua Wu,Cai Zhou,Stephen Bates,Tommi Jaakkola*

Main category: cs.LG

TL;DR: 论文提出动态终止大语言模型推理的校准方法，显著降低计算成本同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型通过延长推理时间提升性能，但带来高昂计算成本。现有方法直接限制推理预算会损害性能，而不同问题难度差异大。

Method: 将模型推理过程视为嵌套推理树序列，通过轻量级探针分析隐藏表示，动态判断推理平台期。

Result: 在3个推理模型和4个数据集上，校准方法减少60%同分布数据推理token和20%分布外数据token，同时保持模型性能。

Conclusion: 思想校准框架能有效平衡大语言模型推理成本与性能，为动态计算资源分配提供新思路。

Abstract: Reasoning large language models achieve impressive test-time scaling by
thinking for longer, but this performance gain comes at significant compute
cost. Directly limiting test-time budget hurts overall performance, but not all
problems are equally difficult. We propose thought calibration to decide
dynamically when thinking can be terminated. To calibrate our decision rule, we
view a language model's growing body of thoughts as a nested sequence of
reasoning trees, where the goal is to identify the point at which novel
reasoning plateaus. We realize this framework through lightweight probes that
operate on top of the language model's hidden representations, which are
informative of both the reasoning structure and overall consistency of
response. Based on three reasoning language models and four datasets, thought
calibration preserves model performance with up to a 60% reduction in thinking
tokens on in-distribution data, and up to 20% in out-of-distribution data.

</details>


### [419] [KL-regularization Itself is Differentially Private in Bandits and RLHF](https://arxiv.org/abs/2505.18407)
*Yizhou Zhang,Kishan Panaganti,Laixi Shi,Juba Ziani,Adam Wierman*

Main category: cs.LG

TL;DR: 该论文探讨了KL正则化在三种决策问题中实现差分隐私的潜力，无需额外噪声注入。


<details>
  <summary>Details</summary>
Motivation: 差分隐私(DP)通常需要显式添加噪声，但现有算法的内在随机性为实现“免费”DP提供了可能。本文旨在探索正则化在实现DP中的作用。

Method: 在离线数据设置下，对多臂老虎机、线性上下文老虎机和人类反馈强化学习(RLHF)问题，通过向学习目标添加KL正则化，使策略采样动作本身具有差分隐私性。

Result: 研究表明，KL正则化不仅能提供隐私保证，还能保持正则化提升性能的优势，无需额外噪声注入。

Conclusion: KL正则化为实现差分隐私提供了一条新途径，同时保留了正则化在性能提升上的固有优势。

Abstract: Differential Privacy (DP) provides a rigorous framework for privacy, ensuring
the outputs of data-driven algorithms remain statistically indistinguishable
across datasets that differ in a single entry. While guaranteeing DP generally
requires explicitly injecting noise either to the algorithm itself or to its
outputs, the intrinsic randomness of existing algorithms presents an
opportunity to achieve DP ``for free''. In this work, we explore the role of
regularization in achieving DP across three different decision-making problems:
multi-armed bandits, linear contextual bandits, and reinforcement learning from
human feedback (RLHF), in offline data settings. We show that adding
KL-regularization to the learning objective (a common approach in optimization
algorithms) makes the action sampled from the resulting stochastic policy
itself differentially private. This offers a new route to privacy guarantees
without additional noise injection, while also preserving the inherent
advantage of regularization in enhancing performance.

</details>


### [420] [LatentLLM: Attention-Aware Joint Tensor Compression](https://arxiv.org/abs/2505.18413)
*Toshiaki Koike-Akino,Xiangyu Chen,Jing Liu,Ye Wang,Pu,Wang,Matthew Brand*

Main category: cs.LG

TL;DR: 提出一种新框架，通过全局注意力感知的联合张量分解，将大型语言/多模态模型压缩为低维潜在结构，显著提升模型精度。


<details>
  <summary>Details</summary>
Motivation: 现代基础模型（如大型语言模型和多模态模型）需要大量计算和内存资源，因此需要一种高效的模型压缩方法。

Method: 扩展局部激活感知的张量分解为全局注意力感知的联合张量分解，将模型转换为低维潜在结构。

Result: 在多个基准测试（包括多模态推理任务）中，该方法在降低潜在维度的同时，显著优于现有模型压缩方法的精度。

Conclusion: 该框架能有效实现计算和内存高效的大型语言/多模态模型，同时保持较高的模型精度。

Abstract: Modern foundation models such as large language models (LLMs) and large
multi-modal models (LMMs) require a massive amount of computational and memory
resources. We propose a new framework to convert such LLMs/LMMs into a
reduced-dimension latent structure. Our method extends a local activation-aware
tensor decomposition to a global attention-aware joint tensor de-composition.
Our framework can significantly improve the model accuracy over the existing
model compression methods when reducing the latent dimension to realize
computationally/memory-efficient LLMs/LLMs. We show the benefit on several
benchmark including multi-modal reasoning tasks.

</details>


### [421] [A Dual Basis Approach for Structured Robust Euclidean Distance Geometry](https://arxiv.org/abs/2505.18414)
*Chandra Kundu,Abiy Tasissa,HanQin Cai*

Main category: cs.LG

TL;DR: 论文提出了一种名为RoDEoDB的新算法框架，用于在存在异常值的情况下恢复欧几里得距离几何结构，通过非正交对偶基优化实现。


<details>
  <summary>Details</summary>
Motivation: 在现代机器学习中，欧几里得距离矩阵（EDM）应用广泛，但在实际应用中，往往只能通过一组锚节点收集部分距离信息，且可能存在异常值，导致部分观测数据被污染。因此，需要一种鲁棒的方法来恢复真实的点配置。

Method: 论文提出了一种名为RoDEoDB的算法框架，利用非正交对偶基优化技术，从部分被污染的EDM中恢复欧几里得距离几何结构。

Result: 在温和条件下，RoDEoDB能够精确恢复Gram矩阵和点配置。实验表明，该算法在传感器定位和分子构象数据集上表现优异。

Conclusion: RoDEoDB算法通过非正交对偶基优化，有效解决了部分观测数据被污染的问题，为欧几里得距离几何恢复提供了一种鲁棒且高效的解决方案。

Abstract: Euclidean Distance Matrix (EDM), which consists of pairwise squared Euclidean
distances of a given point configuration, finds many applications in modern
machine learning. This paper considers the setting where only a set of anchor
nodes is used to collect the distances between themselves and the rest. In the
presence of potential outliers, it results in a structured partial observation
on EDM with partial corruptions. Note that an EDM can be connected to a
positive semi-definite Gram matrix via a non-orthogonal dual basis. Inspired by
recent development of non-orthogonal dual basis in optimization, we propose a
novel algorithmic framework, dubbed Robust Euclidean Distance Geometry via Dual
Basis (RoDEoDB), for recovering the Euclidean distance geometry, i.e., the
underlying point configuration. The exact recovery guarantees have been
established in terms of both the Gram matrix and point configuration, under
some mild conditions. Empirical experiments show superior performance of
RoDEoDB on sensor localization and molecular conformation datasets.

</details>


### [422] [Development of Interactive Nomograms for Predicting Short-Term Survival in ICU Patients with Aplastic Anemia](https://arxiv.org/abs/2505.18421)
*Junyi Fan,Shuheng Chen,Li Sun,Yong Si,Elham Pishgar,Kamiar Alaei,Greg Placencia,Maryam Pishgar*

Main category: cs.LG

TL;DR: 该研究利用机器学习筛选出7个关键预测因子，构建了预测再生障碍性贫血ICU患者短期死亡率的逻辑回归模型，并通过外部验证证实其有效性，最终开发了交互式列线图辅助临床决策。


<details>
  <summary>Details</summary>
Motivation: 再生障碍性贫血患者入住ICU常预示严重并发症或疾病进展，早期风险评估对临床决策和资源分配至关重要。

Method: 使用MIMIC-IV数据库提取患者数据，通过机器学习筛选特征，构建逻辑回归和Cox回归模型预测7/14/28天死亡率，并在eICU数据库中进行外部验证。

Result: 逻辑回归模型表现最优（AUROC:0.82-0.83），外部验证AUROC为0.71-0.74。基于APS III等7个预测因子开发了交互式列线图。

Conclusion: 研究建立的列线图能准确预测再生障碍性贫血ICU患者的短期死亡率，可为临床个性化风险评估提供实用工具。

Abstract: Aplastic anemia is a rare, life-threatening hematologic disorder
characterized by pancytopenia and bone marrow failure. ICU admission in these
patients often signals critical complications or disease progression, making
early risk assessment crucial for clinical decision-making and resource
allocation. In this study, we used the MIMIC-IV database to identify ICU
patients diagnosed with aplastic anemia and extracted clinical features from
five domains: demographics, synthetic indicators, laboratory results,
comorbidities, and medications. Over 400 variables were reduced to seven key
predictors through machine learning-based feature selection. Logistic
regression and Cox regression models were constructed to predict 7-, 14-, and
28-day mortality, and their performance was evaluated using AUROC. External
validation was conducted using the eICU Collaborative Research Database to
assess model generalizability. Among 1,662 included patients, the logistic
regression model demonstrated superior performance, with AUROC values of
0.8227, 0.8311, and 0.8298 for 7-, 14-, and 28-day mortality, respectively,
compared to the Cox model. External validation yielded AUROCs of 0.7391,
0.7119, and 0.7093. Interactive nomograms were developed based on the logistic
regression model to visually estimate individual patient risk. In conclusion,
we identified a concise set of seven predictors, led by APS III, to build
validated and generalizable nomograms that accurately estimate short-term
mortality in ICU patients with aplastic anemia. These tools may aid clinicians
in personalized risk stratification and decision-making at the point of care.

</details>


### [423] [Finite-Time Global Optimality Convergence in Deep Neural Actor-Critic Methods for Decentralized Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.18433)
*Zhiyao Zhang,Myeung Suk Oh,FNU Hairi,Ziyue Luo,Alvaro Velasquez,Jia Liu*

Main category: cs.LG

TL;DR: 该论文首次提出了一种基于深度神经网络的去中心化多智能体强化学习方法，并提供了全局最优性和有限时间收敛的理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有的去中心化多智能体强化学习（MARL）方法在理论分析上大多局限于线性函数逼近下的静态解，而实践中深度神经网络的成功应用与理论理解之间存在显著差距。

Method: 论文提出了一种深度神经网络的actor-critic方法，其中actor和critic组件均为非线性结构。

Result: 该方法具有全局最优性保证，有限时间收敛速度为O(1/T)，并在数值实验中验证了理论结果。

Conclusion: 这是MARL文献中首次为深度神经网络的actor-critic方法提供全局收敛性结果，填补了理论与实践的差距。

Abstract: Actor-critic methods for decentralized multi-agent reinforcement learning
(MARL) facilitate collaborative optimal decision making without centralized
coordination, thus enabling a wide range of applications in practice. To date,
however, most theoretical convergence studies for existing actor-critic
decentralized MARL methods are limited to the guarantee of a stationary
solution under the linear function approximation. This leaves a significant gap
between the highly successful use of deep neural actor-critic for decentralized
MARL in practice and the current theoretical understanding. To bridge this gap,
in this paper, we make the first attempt to develop a deep neural actor-critic
method for decentralized MARL, where both the actor and critic components are
inherently non-linear. We show that our proposed method enjoys a global
optimality guarantee with a finite-time convergence rate of O(1/T), where T is
the total iteration times. This marks the first global convergence result for
deep neural actor-critic methods in the MARL literature. We also conduct
extensive numerical experiments, which verify our theoretical results.

</details>


### [424] [DB-KSVD: Scalable Alternating Optimization for Disentangling High-Dimensional Embedding Spaces](https://arxiv.org/abs/2505.18441)
*Romeo Valentin,Sydney M. Katz,Vincent Vanhoucke,Mykel J. Kochenderfer*

Main category: cs.LG

TL;DR: 本文提出了一种名为DB-KSVD的可扩展字典学习算法，用于解决大型Transformer模型的高维嵌入解耦问题，并在Gemma-2-2B模型上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器（SAEs）在解决字典学习问题时使用简单的线性编码器，但其是否能找到最优解尚不明确。本文旨在探索是否可以通过更复杂的算法找到更好的解决方案。

Method: 本文提出了Double-Batch KSVD（DB-KSVD）算法，该算法基于经典的KSVD算法，但能够扩展到数百万样本和数千维度的数据集。

Result: 在SAEBench基准测试的六个指标上，DB-KSVD在Gemma-2-2B模型的嵌入解耦任务中表现优异，与基于SAEs的现有方法竞争激烈。

Conclusion: 结果表明，SAEs确实能够找到字典学习问题的强解，而传统的优化方法也可以扩展到所需的问题规模，为未来研究提供了新的方向。

Abstract: Dictionary learning has recently emerged as a promising approach for
mechanistic interpretability of large transformer models. Disentangling
high-dimensional transformer embeddings, however, requires algorithms that
scale to high-dimensional data with large sample sizes. Recent work has
explored sparse autoencoders (SAEs) for this problem. However, SAEs use a
simple linear encoder to solve the sparse encoding subproblem, which is known
to be NP-hard. It is therefore interesting to understand whether this structure
is sufficient to find good solutions to the dictionary learning problem or if a
more sophisticated algorithm could find better solutions. In this work, we
propose Double-Batch KSVD (DB-KSVD), a scalable dictionary learning algorithm
that adapts the classic KSVD algorithm. DB-KSVD is informed by the rich
theoretical foundations of KSVD but scales to datasets with millions of samples
and thousands of dimensions. We demonstrate the efficacy of DB-KSVD by
disentangling embeddings of the Gemma-2-2B model and evaluating on six metrics
from the SAEBench benchmark, where we achieve competitive results when compared
to established approaches based on SAEs. By matching SAE performance with an
entirely different optimization approach, our results suggest that (i) SAEs do
find strong solutions to the dictionary learning problem and (ii) that
traditional optimization approaches can be scaled to the required problem
sizes, offering a promising avenue for further research. We provide an
implementation of DB-KSVD at https://github.com/RomeoV/KSVD.jl.

</details>


### [425] [Breaking Silos: Adaptive Model Fusion Unlocks Better Time Series Forecasting](https://arxiv.org/abs/2505.18442)
*Zhining Liu,Ze Yang,Xiao Lin,Ruizhong Qiu,Tianxin Wei,Yada Zhu,Hendrik Hamann,Jingrui He,Hanghang Tong*

Main category: cs.LG

TL;DR: TimeFuse框架通过样本级自适应融合多种时间序列预测模型的优势，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 研究发现，没有单一模型在所有测试样本上表现最佳，不同模型在不同样本上各有优势，因此需要一种方法自适应地结合各模型的优势。

Method: TimeFuse利用元特征表征输入时间序列，训练可学习的融合器预测最优模型融合权重，支持跨数据集联合训练。

Result: 实验表明，TimeFuse在长短时预测任务中均优于现有单一模型，实现了近乎全面的性能提升。

Conclusion: TimeFuse通过自适应融合异构模型，显著提升了时间序列预测的准确性和泛化能力。

Abstract: Time-series forecasting plays a critical role in many real-world
applications. Although increasingly powerful models have been developed and
achieved superior results on benchmark datasets, through a fine-grained
sample-level inspection, we find that (i) no single model consistently
outperforms others across different test samples, but instead (ii) each model
excels in specific cases. These findings prompt us to explore how to adaptively
leverage the distinct strengths of various forecasting models for different
samples. We introduce TimeFuse, a framework for collective time-series
forecasting with sample-level adaptive fusion of heterogeneous models. TimeFuse
utilizes meta-features to characterize input time series and trains a learnable
fusor to predict optimal model fusion weights for any given input. The fusor
can leverage samples from diverse datasets for joint training, allowing it to
adapt to a wide variety of temporal patterns and thus generalize to new inputs,
even from unseen datasets. Extensive experiments demonstrate the effectiveness
of TimeFuse in various long-/short-term forecasting tasks, achieving
near-universal improvement over the state-of-the-art individual models. Code is
available at https://github.com/ZhiningLiu1998/TimeFuse.

</details>


### [426] [Pessimism Principle Can Be Effective: Towards a Framework for Zero-Shot Transfer Reinforcement Learning](https://arxiv.org/abs/2505.18447)
*Chi Zhang,Ziying Jia,George K. Atia,Sihong He,Yue Wang*

Main category: cs.LG

TL;DR: 该论文提出了一种基于悲观主义原则的迁移强化学习框架，通过构建和优化目标域性能的保守估计，解决了迁移策略性能无保障和负迁移风险两大挑战。


<details>
  <summary>Details</summary>
Motivation: 迁移强化学习旨在利用源域丰富数据优化目标域策略，但面临两大挑战：迁移策略性能无保障可能导致不良动作，以及多源域场景下的负迁移风险。

Method: 基于悲观主义原则构建目标域性能的保守估计，提供性能下界保证安全决策，并通过与源域质量相关的单调改进避免负迁移。开发了两种保守估计方法及高效分布式算法。

Result: 框架理论上保证了目标域性能下界，实践上实现了安全可靠的决策，并通过源域质量单调改进避免了负迁移。算法具有收敛性保证。

Conclusion: 该研究为强化学习中的迁移学习提供了理论严谨且实践鲁棒的解决方案，有效解决了性能保障和负迁移问题。

Abstract: Transfer reinforcement learning aims to derive a near-optimal policy for a
target environment with limited data by leveraging abundant data from related
source domains. However, it faces two key challenges: the lack of performance
guarantees for the transferred policy, which can lead to undesired actions, and
the risk of negative transfer when multiple source domains are involved. We
propose a novel framework based on the pessimism principle, which constructs
and optimizes a conservative estimation of the target domain's performance. Our
framework effectively addresses the two challenges by providing an optimized
lower bound on target performance, ensuring safe and reliable decisions, and by
exhibiting monotonic improvement with respect to the quality of the source
domains, thereby avoiding negative transfer. We construct two types of
conservative estimations, rigorously characterize their effectiveness, and
develop efficient distributed algorithms with convergence guarantees. Our
framework provides a theoretically sound and practically robust solution for
transfer learning in reinforcement learning.

</details>


### [427] [$μ$-MoE: Test-Time Pruning as Micro-Grained Mixture-of-Experts](https://arxiv.org/abs/2505.18451)
*Toshiaki Koike-Akino,Jing Liu,Ye Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为μ-MoE的动态剪枝方法，通过微专家混合模型自适应处理不同任务/提示的结构化稀疏性，降低推理复杂度。


<details>
  <summary>Details</summary>
Motivation: 大型基础模型计算需求高，现有基于校准数据的激活感知压缩技术可能因领域偏移影响下游任务性能。

Method: 采用计算高效的校准方法，为每个提示动态执行激活感知剪枝，并将其建模为微专家混合模型（μ-MoE）。

Result: 实验表明μ-MoE能实时适应任务/提示相关的结构化稀疏性，有效降低推理复杂度。

Conclusion: μ-MoE通过动态剪枝实现了对任务自适应的计算优化，为大型模型部署提供了可行方案。

Abstract: To tackle the huge computational demand of large foundation models,
activation-aware compression techniques without retraining have been
introduced. However, since these rely on calibration data, domain shift may
arise for unknown downstream tasks. With a computationally efficient
calibration, activation-aware pruning can be executed for every prompt
adaptively, yet achieving reduced complexity at inference. We formulate it as a
mixture of micro-experts, called $\mu$-MoE. Several experiments demonstrate
that $\mu$-MoE can dynamically adapt to task/prompt-dependent structured
sparsity on the fly.

</details>


### [428] [Performance and Generalizability Impacts of Incorporating Geolocation into Deep Learning for Dynamic PM2.5 Estimation](https://arxiv.org/abs/2505.18461)
*Morteza Karimzadeh,Zhongying Wang,James L. Crooks*

Main category: cs.LG

TL;DR: 该研究探讨了地理定位信息在深度学习模型中的作用，特别是在动态和高分辨率的PM2.5估计任务中，比较了不同地理编码方法对模型性能和泛化能力的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在地理空间应用中表现出色，但地理定位信息如何提升模型性能和地理泛化能力的研究仍不足。本文旨在量化地理定位信息在动态和高分辨率任务（如PM2.5估计）中的作用。

Method: 研究基于一个最新的PM2.5估计深度学习模型，比较了三种地理定位方法：排除地理定位（基线）、使用原始地理坐标、以及利用预训练的地理编码器（如GeoCLIP），并在区域内（WR）和跨区域（OoR）场景下进行评估。

Result: 结果显示，原始地理坐标虽能提升区域内性能，但可能阻碍跨区域泛化；而预训练的地理编码器（如GeoCLIP）在两种场景下均能提升预测性能和泛化能力，但也存在某些区域因高次基函数和稀疏样本导致的伪影问题。

Conclusion: 预训练的地理编码器能有效提升模型性能和地理泛化能力，但其性能因编码器类型和区域特性而异，未来需进一步优化以解决伪影等问题。

Abstract: Deep learning models have demonstrated success in geospatial applications,
yet quantifying the role of geolocation information in enhancing model
performance and geographic generalizability remains underexplored. A new
generation of location encoders have emerged with the goal of capturing
attributes present at any given location for downstream use in predictive
modeling. Being a nascent area of research, their evaluation has remained
largely limited to static tasks such as species distributions or average
temperature mapping. In this paper, we discuss and quantify the impact of
incorporating geolocation into deep learning for a real-world application
domain that is characteristically dynamic (with fast temporal change) and
spatially heterogeneous at high resolutions: estimating surface-level daily
PM2.5 levels using remotely sensed and ground-level data. We build on a
recently published deep learning-based PM2.5 estimation model that achieves
state-of-the-art performance on data observed in the contiguous United States.
We examine three approaches for incorporating geolocation: excluding
geolocation as a baseline, using raw geographic coordinates, and leveraging
pretrained location encoders. We evaluate each approach under within-region
(WR) and out-of-region (OoR) evaluation scenarios. Aggregate performance
metrics indicate that while na\"ive incorporation of raw geographic coordinates
improves within-region performance by retaining the interpolative value of
geographic location, it can hinder generalizability across regions. In
contrast, pretrained location encoders like GeoCLIP enhance predictive
performance and geographic generalizability for both WR and OoR scenarios.
However, qualitative analysis reveals artifact patterns caused by high-degree
basis functions and sparse upstream samples in certain areas, and ablation
results indicate varying performance among location encoders...

</details>


### [429] [Using Large Language Models to Tackle Fundamental Challenges in Graph Learning: A Comprehensive Survey](https://arxiv.org/abs/2505.18475)
*Mengran Li,Pengyu Zhang,Wenbin Xing,Yijia Zheng,Klim Zaporojets,Junzhou Chen,Ronghui Zhang,Yong Zhang,Siyuan Gong,Jia Hu,Xiaolei Ma,Zhiyuan Liu,Paul Groth,Marcel Worring*

Main category: cs.LG

TL;DR: 论文探讨了图学习面临的四大挑战，并综述了如何利用大语言模型（LLMs）解决这些挑战。


<details>
  <summary>Details</summary>
Motivation: 传统图学习方法在复杂、噪声或动态变化的现实图数据中表现不佳，存在不完整性、不平衡性、跨域异质性和动态不稳定性四大挑战。

Method: 综述了传统解决方案和基于LLMs的现代方法，分析了LLMs如何通过语义推理和外部知识解决图学习挑战。

Result: LLMs为图学习提供了新的解决思路，尤其在处理复杂图数据时展现出独特优势。

Conclusion: LLMs与图学习的结合是一个新兴的跨学科领域，未来研究前景广阔，并提供了相关资源库供进一步探索。

Abstract: Graphs are a widely used paradigm for representing non-Euclidean data, with
applications ranging from social network analysis to biomolecular prediction.
Conventional graph learning approaches typically rely on fixed structural
assumptions or fully observed data, limiting their effectiveness in more
complex, noisy, or evolving settings. Consequently, real-world graph data often
violates the assumptions of traditional graph learning methods, in particular,
it leads to four fundamental challenges: (1) Incompleteness, real-world graphs
have missing nodes, edges, or attributes; (2) Imbalance, the distribution of
the labels of nodes or edges and their structures for real-world graphs are
highly skewed; (3) Cross-domain Heterogeneity, graphs from different domains
exhibit incompatible feature spaces or structural patterns; and (4) Dynamic
Instability, graphs evolve over time in unpredictable ways. Recent advances in
Large Language Models (LLMs) offer the potential to tackle these challenges by
leveraging rich semantic reasoning and external knowledge. This survey provides
a comprehensive review of how LLMs can be integrated with graph learning to
address the aforementioned challenges. For each challenge, we review both
traditional solutions and modern LLM-driven approaches, highlighting how LLMs
contribute unique advantages. Finally, we discuss open research questions and
promising future directions in this emerging interdisciplinary field. To
support further exploration, we have curated a repository of recent advances on
graph learning challenges:
https://github.com/limengran98/Awesome-Literature-Graph-Learning-Challenges.

</details>


### [430] [The Prompt is Mightier than the Example](https://arxiv.org/abs/2505.18485)
*Shengzhe Xu,Nikhil Muralidhar,Naren Ramakrishnan*

Main category: cs.LG

TL;DR: 论文提出知识引导提示（KGP）作为提示优化的新方法，通过注入领域知识减少对上下文学习（ICL）示例的依赖，从而降低合成数据生成成本。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）通过上下文学习（ICL）和提示优化生成高质量合成数据，但需要大量示例，成本高昂。LLMs内置的丰富先验知识可能替代部分示例。

Method: 引入知识引导提示（KGP），在提示中显式注入领域知识（推断或已有），探索其替代ICL示例的能力，并系统分析ICL与KGP的权衡。

Result: 实验揭示了生成数据质量随领域知识增加和示例减少的变化规律，证明KGP可扩展为ICL的替代或补充方案。

Conclusion: 知识引导提示能有效减少对ICL示例的依赖，为合成数据生成提供了新方法。

Abstract: Numerous recent prompt optimization approaches like chain-of-thought, have
been demonstrated to significantly improve the quality of content generated by
large language models (LLMs). In-context learning (ICL), a recent paradigm
where a few representative examples guide content generation has also led to
strong improvements in generation quality of LLM generated content. This idea
has been applied to great effect in synthetic tabular data generation, where
LLMs, through effective use of ICL and prompt optimization, can generate data
that approximate samples from complex, heterogeneous distributions based on
representative examples. However, ensuring high-fidelity synthetic data often
requires a very large number of ICL examples which may be unavailable or costly
to obtain. At the same time, as LLMs get larger and larger, their in-built
prior knowledge becomes vast and can potentially substitute for specific data
examples. In this paper, we introduce Knowledge-Guided Prompting (KGP) as a new
knob in prompt optimization and explore the ability of KGP-based prompt
optimization to offset the cost of ICL. Specifically, we explore the question
`how many examples can a prompt substitute for?' and explore knowledge-guided
prompting (KGP) where domain knowledge, either inferred or available, is
explicitly injected into the prompt, reducing dependence on ICL examples. Our
experiments systematically explore the trade-off between ICL and KGP, revealing
an empirical scaling law that quantifies how quality of generated synthetic
data varies with increasing domain knowledge and decreasing example count. Our
results demonstrate that knowledge-guided prompting can be a scalable
alternative, or addition, to in-context examples, unlocking new approaches to
synthetic data generation.

</details>


### [431] [Synthesizing and Adapting Error Correction Data for Mobile Large Language Model Applications](https://arxiv.org/abs/2505.18488)
*Yanxiang Zhang,Zheng Xu,Shanshan Wu,Yuanbo Zhang,Daniel Ramage*

Main category: cs.LG

TL;DR: 利用LLMs合成高质量纠错数据集，优化移动设备输入纠错性能。


<details>
  <summary>Details</summary>
Motivation: 提升大型语言模型在移动设备用户输入纠错中的应用效果。

Method: 通过LLMs合成数据集，调整数据分布匹配移动应用，结合A/B测试优化模型。

Result: 合成数据与现有数据混合使用，提升模型在离线评估和生产环境中的纠错性能。

Conclusion: 合成数据与重加权方法有效改进LLMs在移动设备纠错任务中的表现。

Abstract: Error correction is an important capability when applying large language
models (LLMs) to facilitate user typing on mobile devices. In this paper, we
use LLMs to synthesize a high-quality dataset of error correction pairs to
evaluate and improve LLMs for mobile applications. We first prompt LLMs with
error correction domain knowledge to build a scalable and reliable addition to
the existing data synthesis pipeline. We then adapt the synthetic data
distribution to match the mobile application domain by reweighting the samples.
The reweighting model is learnt by predicting (a handful of) live A/B test
metrics when deploying LLMs in production, given the LLM performance on offline
evaluation data and scores from a small privacy-preserving on-device language
model. Finally, we present best practices for mixing our synthetic data with
other data sources to improve model performance on error correction in both
offline evaluation and production live A/B testing.

</details>


### [432] [FedHL: Federated Learning for Heterogeneous Low-Rank Adaptation via Unbiased Aggregation](https://arxiv.org/abs/2505.18494)
*Zihao Peng,Jiandian Zeng,Boyuan Li,Guo Li,Shengbo Chen,Tian Wang*

Main category: cs.LG

TL;DR: 该论文提出FedHL框架，解决联邦学习中异构LoRA因参数截断和梯度偏差导致的收敛问题，通过全秩全局模型校准聚合权重，实现理论最优收敛。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习中的异构LoRA方法因参数截断和梯度偏差缺乏收敛保证，导致性能下降。

Method: 提出FedHL框架，利用全秩全局模型作为校准聚合基础，并通过最小化收敛上界中的梯度漂移项推导理论最优聚合权重。

Result: 理论证明FedHL具有O(1/√T)收敛率，实验显示在多个真实数据集上性能提升1-3%。

Conclusion: FedHL有效解决了异构LoRA的收敛问题，为联邦学习中的基础模型微调提供了可靠方法。

Abstract: Federated Learning (FL) facilitates the fine-tuning of Foundation Models
(FMs) using distributed data sources, with Low-Rank Adaptation (LoRA) gaining
popularity due to its low communication costs and strong performance. While
recent work acknowledges the benefits of heterogeneous LoRA in FL and
introduces flexible algorithms to support its implementation, our theoretical
analysis reveals a critical gap: existing methods lack formal convergence
guarantees due to parameter truncation and biased gradient updates.
Specifically, adapting client-specific LoRA ranks necessitates truncating
global parameters, which introduces inherent truncation errors and leads to
subsequent inaccurate gradient updates that accumulate over training rounds,
ultimately degrading performance. To address the above issues, we propose
\textbf{FedHL}, a simple yet effective \textbf{Fed}erated Learning framework
tailored for \textbf{H}eterogeneous \textbf{L}oRA. By leveraging the full-rank
global model as a calibrated aggregation basis, FedHL eliminates the direct
truncation bias from initial alignment with client-specific ranks. Furthermore,
we derive the theoretically optimal aggregation weights by minimizing the
gradient drift term in the convergence upper bound. Our analysis shows that
FedHL guarantees $\mathcal{O}(1/\sqrt{T})$ convergence rate, and experiments on
multiple real-world datasets demonstrate a 1-3\% improvement over several
state-of-the-art methods.

</details>


### [433] [Beyond Masked and Unmasked: Discrete Diffusion Models via Partial Masking](https://arxiv.org/abs/2505.18495)
*Chen-Hao Chao,Wei-Fang Sun,Hanwen Liang,Chun-Yi Lee,Rahul G. Krishnan*

Main category: cs.LG

TL;DR: 论文提出了一种名为Prime的部分掩码方案，用于改进掩码扩散模型（MDM），通过引入中间状态减少冗余计算，在文本和图像生成任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的掩码扩散模型在生成离散数据时，由于连续的采样步骤中令牌序列往往保持不变，导致模型重复处理相同的输入，产生冗余计算。

Method: Prime方案扩展了MDM，允许令牌处于掩码和未掩码之间的中间状态，从而基于部分观察到的令牌信息进行预测，并实现细粒度的去噪过程。论文还推导了变分训练目标并引入了简单的架构设计以适应中间状态输入。

Result: 在文本数据上，Prime在OpenWebText上的困惑度为15.36，优于之前的MDM（21.52）、自回归模型（17.54）及其混合变体（17.58）。在图像数据上，Prime在CIFAR-10和ImageNet-32上分别取得了3.26和6.98的FID分数，与领先的连续生成模型相当。

Conclusion: Prime通过引入部分掩码方案显著提高了掩码扩散模型的效率，并在多种生成建模任务中展现出卓越性能。

Abstract: Masked diffusion models (MDM) are powerful generative models for discrete
data that generate samples by progressively unmasking tokens in a sequence.
Each token can take one of two states: masked or unmasked. We observe that
token sequences often remain unchanged between consecutive sampling steps;
consequently, the model repeatedly processes identical inputs, leading to
redundant computation. To address this inefficiency, we propose the Partial
masking scheme (Prime), which augments MDM by allowing tokens to take
intermediate states interpolated between the masked and unmasked states. This
design enables the model to make predictions based on partially observed token
information, and facilitates a fine-grained denoising process. We derive a
variational training objective and introduce a simple architectural design to
accommodate intermediate-state inputs. Our method demonstrates superior
performance across a diverse set of generative modeling tasks. On text data, it
achieves a perplexity of 15.36 on OpenWebText, outperforming previous MDM
(21.52), autoregressive models (17.54), and their hybrid variants (17.58),
without relying on an autoregressive formulation. On image data, it attains
competitive FID scores of 3.26 on CIFAR-10 and 6.98 on ImageNet-32, comparable
to leading continuous generative models.

</details>


### [434] [G1: Teaching LLMs to Reason on Graphs with Reinforcement Learning](https://arxiv.org/abs/2505.18499)
*Xiaojun Guo,Ang Li,Yifei Wang,Stefanie Jegelka,Yisen Wang*

Main category: cs.LG

TL;DR: 论文提出G1方法，通过强化学习在合成图任务上微调LLMs，显著提升图推理能力，并创建了最大图推理数据集Erd\~os。3B模型性能超越72B模型，且具备零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在图相关任务上表现有限，且缺乏大规模通用图数据。传统方法如预训练图基础模型或有监督微调面临数据稀缺等挑战。

Method: 提出G1方法：利用强化学习在合成的图论任务（Erd\~os数据集，含50类任务/10万训练数据）上微调LLMs，结合自动生成数据与预训练LLM优势。

Result: 3B微调模型性能超越24倍大的Qwen2.5-72B模型；在未见过的任务、领域和图编码方案上展现强零样本泛化能力，且不影响通用推理能力。

Conclusion: 强化学习+合成数据微调是构建强大图推理模型的高效路径，表明LLMs具备可通过RL激发的潜在图理解能力。

Abstract: Although Large Language Models (LLMs) have demonstrated remarkable progress,
their proficiency in graph-related tasks remains notably limited, hindering the
development of truly general-purpose models. Previous attempts, including
pretraining graph foundation models or employing supervised fine-tuning, often
face challenges such as the scarcity of large-scale, universally represented
graph data. We introduce G1, a simple yet effective approach demonstrating that
Reinforcement Learning (RL) on synthetic graph-theoretic tasks can
significantly scale LLMs' graph reasoning abilities. To enable RL training, we
curate Erd\~os, the largest graph reasoning dataset to date comprising 50
diverse graph-theoretic tasks of varying difficulty levels, 100k training data
and 5k test data, all drived from real-world graphs. With RL on Erd\~os, G1
obtains substantial improvements in graph reasoning, where our finetuned 3B
model even outperforms Qwen2.5-72B-Instruct (24x size). RL-trained models also
show strong zero-shot generalization to unseen tasks, domains, and graph
encoding schemes, including other graph-theoretic benchmarks as well as
real-world node classification and link prediction tasks, without compromising
general reasoning abilities. Our findings offer an efficient, scalable path for
building strong graph reasoners by finetuning LLMs with RL on graph-theoretic
tasks, which combines the strengths of pretrained LLM capabilities with
abundant, automatically generated synthetic data, suggesting that LLMs possess
graph understanding abilities that RL can elicit successfully.

</details>


### [435] [How Particle System Theory Enhances Hypergraph Message Passing](https://arxiv.org/abs/2505.18505)
*Yixuan Ma,Kai Yi,Pietro Lio,Shi Jin,Yu Guang Wang*

Main category: cs.LG

TL;DR: 该论文提出了一种受粒子系统启发的新型超图消息传递框架，通过引入吸引、排斥和Allen-Cahn作用力，实现节点分类任务中的深度消息传递，并在理论和实验上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 超图能有效建模自然现象中的高阶关系，但现有方法难以处理超图中的复杂交互和异质性。论文旨在解决超图消息传递中的过平滑和异质性问题。

Method: 提出基于粒子系统的超图消息传递框架，包含吸引、排斥和Allen-Cahn作用力，采用一阶和二阶粒子系统方程建模动态，并引入随机元素处理不确定性。

Result: 理论证明该方法通过保持超图Dirichlet能量的正下界来缓解过平滑，实验表明在多种真实超图节点分类任务中表现优异，尤其在异质数据集上。

Conclusion: 该框架通过粒子系统动力学实现了深度超图消息传递，有效解决了过平滑和异质性问题，为超图学习提供了新思路。

Abstract: Hypergraphs effectively model higher-order relationships in natural
phenomena, capturing complex interactions beyond pairwise connections. We
introduce a novel hypergraph message passing framework inspired by interacting
particle systems, where hyperedges act as fields inducing shared node dynamics.
By incorporating attraction, repulsion, and Allen-Cahn forcing terms, particles
of varying classes and features achieve class-dependent equilibrium, enabling
separability through the particle-driven message passing. We investigate both
first-order and second-order particle system equations for modeling these
dynamics, which mitigate over-smoothing and heterophily thus can capture
complete interactions. The more stable second-order system permits deeper
message passing. Furthermore, we enhance deterministic message passing with
stochastic element to account for interaction uncertainties. We prove
theoretically that our approach mitigates over-smoothing by maintaining a
positive lower bound on the hypergraph Dirichlet energy during propagation and
thus to enable hypergraph message passing to go deep. Empirically, our models
demonstrate competitive performance on diverse real-world hypergraph node
classification tasks, excelling on both homophilic and heterophilic datasets.

</details>


### [436] [SPDEBench: An Extensive Benchmark for Learning Regular and Singular Stochastic PDEs](https://arxiv.org/abs/2505.18511)
*Zheyan Li,Yuantu Zhu,Hao Ni,Siran Li,Bingguang Chen,Qi Meng*

Main category: cs.LG

TL;DR: 该论文介绍了SPDEBench，一个用于解决随机偏微分方程（SPDEs）的机器学习基准测试框架，提供了新的数据集和模型，强调了噪声采样和重整化对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 随机偏微分方程（SPDEs）在模拟粗糙时空动态的物理过程中起核心作用，但缺乏统一的数据集，且现有数据集未考虑噪声采样和重整化引入的计算误差。

Method: 通过机器学习方法构建SPDEBench，解决具有物理意义的典型SPDEs，如Φ4_d、波动、不可压缩Navier-Stokes和KdV方程，并构建基于重整化过程的新数据集。

Result: 提出了新的机器学习模型，取得了迄今为止最佳结果，并展示了噪声采样和重整化对模型性能比较的影响，强调了选择高质量测试数据的重要性。

Conclusion: SPDEBench提供了一个开源代码库，确保在各种SPDE数据集上的基准测试可复现，同时支持新数据集和机器学习基线的灵活加入，成为社区的重要资源。

Abstract: Stochastic Partial Differential Equations (SPDEs) driven by random noise play
a central role in modelling physical processes whose spatio-temporal dynamics
can be rough, such as turbulence flows, superconductors, and quantum dynamics.
To efficiently model these processes and make predictions, machine learning
(ML)-based surrogate models are proposed, with their network architectures
incorporating the spatio-temporal roughness in their design. However, it lacks
an extensive and unified datasets for SPDE learning; especially, existing
datasets do not account for the computational error introduced by noise
sampling and the necessary renormalization required for handling singular
SPDEs. We thus introduce SPDEBench, which is designed to solve typical SPDEs of
physical significance (e.g., the $\Phi^4_d$, wave, incompressible
Navier--Stokes, and KdV equations) on 1D or 2D tori driven by white noise via
ML methods. New datasets for singular SPDEs based on the renormalization
process have been constructed, and novel ML models achieving the best results
to date have been proposed. In particular, we investigate the impact of
computational error introduced by noise sampling and renormalization on the
performance comparison of ML models and highlight the importance of selecting
high-quality test data for accurate evaluation. Results are benchmarked with
traditional numerical solvers and ML-based models, including FNO, NSPDE and
DLR-Net, etc. It is shown that, for singular SPDEs, naively applying ML models
on data without specifying the numerical schemes can lead to significant errors
and misleading conclusions. Our SPDEBench provides an open-source codebase that
ensures full reproducibility of benchmarking across a variety of SPDE datasets
while offering the flexibility to incorporate new datasets and machine learning
baselines, making it a valuable resource for the community.

</details>


### [437] [Enhancing Training Data Attribution with Representational Optimization](https://arxiv.org/abs/2505.18513)
*Weiwei Sun,Haokun Liu,Nikhil Kandpal,Colin Raffel,Yiming Yang*

Main category: cs.LG

TL;DR: AirRep提出了一种可扩展的基于表示的训练数据归因方法，通过优化特定任务和模型对齐的表示，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有梯度归因方法计算成本高，而基于表示的方法虽可扩展但依赖启发式嵌入，限制了准确性。AirRep旨在解决这一矛盾。

Method: AirRep结合可训练编码器和基于注意力的池化机制，通过排序目标优化表示，实现高效归因。

Result: 实验表明，AirRep性能媲美梯度方法，推理效率提升近两个数量级，且在不同任务和模型上表现鲁棒。

Conclusion: AirRep为大规模训练数据归因提供了高效解决方案，平衡了准确性与计算成本。

Abstract: Training data attribution (TDA) methods aim to measure how training data
impacts a model's predictions. While gradient-based attribution methods, such
as influence functions, offer theoretical grounding, their computational costs
make them impractical for large-scale applications. Representation-based
approaches are far more scalable, but typically rely on heuristic embeddings
that are not optimized for attribution, limiting their fidelity. To address
these challenges, we propose AirRep, a scalable, representation-based approach
that closes this gap by learning task-specific and model-aligned
representations optimized explicitly for TDA. AirRep introduces two key
innovations: a trainable encoder tuned for attribution quality, and an
attention-based pooling mechanism that enables accurate estimation of
group-wise influence. We train AirRep using a ranking objective over
automatically constructed training subsets labeled by their empirical effect on
target predictions. Experiments on instruction-tuned LLMs demonstrate that
AirRep achieves performance on par with state-of-the-art gradient-based
approaches while being nearly two orders of magnitude more efficient at
inference time. Further analysis highlights its robustness and generalization
across tasks and models. Our code is available at
https://github.com/sunnweiwei/AirRep.

</details>


### [438] [Test-Time Adaptation with Binary Feedback](https://arxiv.org/abs/2505.18514)
*Taeckyung Lee,Sorn Chottananurak,Junsu Kim,Jinwoo Shin,Taesik Gong,Sung-Ju Lee*

Main category: cs.LG

TL;DR: 该论文提出了一种新的测试时自适应（TTA）方法BiTTA，通过二元反馈减少标注负担，有效应对严重领域偏移。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法在严重领域偏移下表现不佳，而主动TTA方法需要全类标注，成本过高。论文旨在通过二元反馈降低标注成本并提升模型适应性。

Method: 提出BiTTA框架，结合强化学习平衡二元反馈引导的适应和基于一致性的自适应，优化模型在不确定样本和置信预测上的表现。

Result: 实验显示BiTTA比现有基线准确率提升13.3%，在严重分布偏移下仅需少量标注即可显著改善性能。

Conclusion: BiTTA通过二元反馈有效减少了标注负担，并在严重领域偏移下表现出色，为TTA提供了一种实用且高效的解决方案。

Abstract: Deep learning models perform poorly when domain shifts exist between training
and test data. Test-time adaptation (TTA) is a paradigm to mitigate this issue
by adapting pre-trained models using only unlabeled test samples. However,
existing TTA methods can fail under severe domain shifts, while recent active
TTA approaches requiring full-class labels are impractical due to high labeling
costs. To address this issue, we introduce a new setting of TTA with binary
feedback. This setting uses a few binary feedback inputs from annotators to
indicate whether model predictions are correct, thereby significantly reducing
the labeling burden of annotators. Under the setting, we propose BiTTA, a novel
dual-path optimization framework that leverages reinforcement learning to
balance binary feedback-guided adaptation on uncertain samples with
agreement-based self-adaptation on confident predictions. Experiments show
BiTTA achieves 13.3%p accuracy improvements over state-of-the-art baselines,
demonstrating its effectiveness in handling severe distribution shifts with
minimal labeling effort. The source code is available at
https://github.com/taeckyung/BiTTA.

</details>


### [439] [CLaDMoP: Learning Transferrable Models from Successful Clinical Trials via LLMs](https://arxiv.org/abs/2505.18527)
*Yiqing Zhang,Xiaozhong Liu,Fabricio Murai*

Main category: cs.LG

TL;DR: 提出CLaDMoP预训练方法，结合SCT数据集，通过多级融合技术和分组块提升临床试验结果预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有临床试验结果预测模型依赖任务特定损失函数和阶段特定数据，可能导致泛化能力不足和假阳性/阴性增加。

Method: CLaDMoP利用大语言模型编码试验资格标准，通过多级融合技术与轻量级药物分子分支连接，并采用分组块减少计算开销。

Result: 在TOP基准测试中，CLaDMoP的PR-AUC和ROC-AUC分别提升10.5%和3.6%，F1分数与MEXA-CTP相当。

Conclusion: CLaDMoP展示了在临床试验结果预测中的潜力，尤其在早期试验阶段表现显著优于基线方法。

Abstract: Many existing models for clinical trial outcome prediction are optimized
using task-specific loss functions on trial phase-specific data. While this
scheme may boost prediction for common diseases and drugs, it can hinder
learning of generalizable representations, leading to more false
positives/negatives. To address this limitation, we introduce CLaDMoP, a new
pre-training approach for clinical trial outcome prediction, alongside the
Successful Clinical Trials dataset(SCT), specifically designed for this task.
CLaDMoP leverages a Large Language Model-to encode trials' eligibility
criteria-linked to a lightweight Drug-Molecule branch through a novel
multi-level fusion technique. To efficiently fuse long embeddings across
levels, we incorporate a grouping block, drastically reducing computational
overhead. CLaDMoP avoids reliance on task-specific objectives by pre-training
on a "pair matching" proxy task. Compared to established zero-shot and few-shot
baselines, our method significantly improves both PR-AUC and ROC-AUC,
especially for phase I and phase II trials. We further evaluate and perform
ablation on CLaDMoP after Parameter-Efficient Fine-Tuning, comparing it to
state-of-the-art supervised baselines, including MEXA-CTP, on the Trial Outcome
Prediction(TOP) benchmark. CLaDMoP achieves up to 10.5% improvement in PR-AUC
and 3.6% in ROC-AUC, while attaining comparable F1 score to MEXA-CTP,
highlighting its potential for clinical trial outcome prediction. Code and SCT
dataset can be downloaded from https://github.com/murai-lab/CLaDMoP.

</details>


### [440] [Preserving AUC Fairness in Learning with Noisy Protected Groups](https://arxiv.org/abs/2505.18532)
*Mingyang Wu,Li Lin,Wenbin Zhang,Xin Wang,Zhenhuan Yang,Shu Hu*

Main category: cs.LG

TL;DR: 该论文提出了一种在受保护组存在噪声的情况下，保证AUC公平性的鲁棒优化方法，并在实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: AUC是分类任务中的重要指标，尤其在类别不平衡的场景下。然而，现有研究多假设受保护组数据干净，忽略了噪声对公平性的影响，导致实际应用中出现公平性违规。

Method: 采用分布鲁棒优化方法，首次提出在受保护组存在噪声的情况下保证AUC公平性的理论框架。

Result: 在表格和图像数据集上的大量实验表明，该方法在保持AUC公平性方面优于现有技术。

Conclusion: 该研究填补了AUC公平性优化在噪声环境下的空白，为实际应用提供了更可靠的解决方案。

Abstract: The Area Under the ROC Curve (AUC) is a key metric for classification,
especially under class imbalance, with growing research focus on optimizing AUC
over accuracy in applications like medical image analysis and deepfake
detection. This leads to fairness in AUC optimization becoming crucial as
biases can impact protected groups. While various fairness mitigation
techniques exist, fairness considerations in AUC optimization remain in their
early stages, with most research focusing on improving AUC fairness under the
assumption of clean protected groups. However, these studies often overlook the
impact of noisy protected groups, leading to fairness violations in practice.
To address this, we propose the first robust AUC fairness approach under noisy
protected groups with fairness theoretical guarantees using distributionally
robust optimization. Extensive experiments on tabular and image datasets show
that our method outperforms state-of-the-art approaches in preserving AUC
fairness. The code is in
https://github.com/Purdue-M2/AUC_Fairness_with_Noisy_Groups.

</details>


### [441] [Convergence, Sticking and Escape: Stochastic Dynamics Near Critical Points in SGD](https://arxiv.org/abs/2505.18535)
*Dmitry Dudukalov,Artem Logachov,Vladimir Lotov,Timofei Prasolov,Evgeny Prokopenko,Anton Tarasenko*

Main category: cs.LG

TL;DR: 该论文研究了SGD在一维景观中的收敛特性和逃逸动态，分析了无限和有限方差噪声下的行为，重点探讨了SGD从初始点到局部最小值的转移时间尺度及其影响因素。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于深入理解SGD在不同噪声特性和函数几何条件下的行为，特别是在局部最大值附近的表现，以及如何可靠地收敛到局部最小值。

Method: 方法包括理论分析SGD在一维景观中的动态，分别考虑无限和有限方差噪声，并研究初始点位置对收敛行为的影响。

Result: 结果表明，SGD在适当条件下会收敛到局部最小值，除非初始点过于接近局部最大值。在接近“尖锐”最大值时，SGD不会长时间停留，并能估计到达相邻最小值的概率。

Conclusion: 结论指出，SGD在局部最大值和最小值之间的转移行为受噪声特性和函数几何的复杂影响，提供了对SGD动态的细致理解。

Abstract: We study the convergence properties and escape dynamics of Stochastic
Gradient Descent (SGD) in one-dimensional landscapes, separately considering
infinite- and finite-variance noise. Our main focus is to identify the time
scales on which SGD reliably moves from an initial point to the local minimum
in the same ''basin''. Under suitable conditions on the noise distribution, we
prove that SGD converges to the basin's minimum unless the initial point lies
too close to a local maximum. In that near-maximum scenario, we show that SGD
can linger for a long time in its neighborhood. For initial points near a
''sharp'' maximum, we show that SGD does not remain stuck there, and we provide
results to estimate the probability that it will reach each of the two
neighboring minima. Overall, our findings present a nuanced view of SGD's
transitions between local maxima and minima, influenced by both noise
characteristics and the underlying function geometry.

</details>


### [442] [B-score: Detecting biases in large language models using response history](https://arxiv.org/abs/2505.18545)
*An Vo,Mohammad Reza Taesiri,Daeyoung Kim,Anh Totti Nguyen*

Main category: cs.LG

TL;DR: 研究发现大语言模型在多轮对话中能自我减少偏见，并提出新指标B-score有效检测偏见。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）常表现出强烈偏见，研究旨在探索LLMs在多轮对话中是否能减少偏见输出。

Method: 通过设计涵盖9个主题的三种问题类型（主观、随机、客观）测试LLMs，并引入B-score作为偏见检测新指标。

Result: LLMs在回答随机问题时能自我减少偏见，B-score在多个数据集上显著提高答案验证准确率。

Conclusion: 多轮对话有助于LLMs减少偏见，B-score是检测和改善模型偏见的有效工具。

Abstract: Large language models (LLMs) often exhibit strong biases, e.g, against women
or in favor of the number 7. We investigate whether LLMs would be able to
output less biased answers when allowed to observe their prior answers to the
same question in a multi-turn conversation. To understand which types of
questions invite more biased answers, we test LLMs on our proposed set of
questions that span 9 topics and belong to three types: (1) Subjective; (2)
Random; and (3) Objective. Interestingly, LLMs are able to "de-bias" themselves
in a multi-turn conversation in response to questions that seek an Random,
unbiased answer. Furthermore, we propose B-score, a novel metric that is
effective in detecting biases to Subjective, Random, Easy, and Hard questions.
On MMLU, HLE, and CSQA, leveraging B-score substantially improves the
verification accuracy of LLM answers (i.e, accepting LLM correct answers and
rejecting incorrect ones) compared to using verbalized confidence scores or the
frequency of single-turn answers alone. Code and data are available at:
https://b-score.github.io.

</details>


### [443] [Joint-stochastic-approximation Autoencoders with Application to Semi-supervised Learning](https://arxiv.org/abs/2505.18558)
*Wenbo He,Zhijian Ou*

Main category: cs.LG

TL;DR: 该论文提出了一种新型深度生成模型JSA自编码器，解决了现有模型在离散数据处理和直接优化数据似然方面的不足，并在半监督学习任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有深度生成模型（如VAE和GAN）在处理离散观测和潜在代码时表现不佳，且优化目标与数据似然间接相关。论文旨在解决这些问题。

Method: 提出了联合随机近似（JSA）自编码器，直接最大化数据对数似然，并最小化后验与推断模型之间的包容性KL散度。

Result: JSA自编码器在MNIST和SVHN数据集上的半监督任务中表现优异，离散潜在空间模型的性能与连续潜在空间的最先进模型相当。

Conclusion: JSA自编码器是首个在挑战性半监督任务中成功应用离散潜在变量模型的深度生成模型，具有处理离散和连续变量的鲁棒性。

Abstract: Our examination of existing deep generative models (DGMs), including VAEs and
GANs, reveals two problems. First, their capability in handling discrete
observations and latent codes is unsatisfactory, though there are interesting
efforts. Second, both VAEs and GANs optimize some criteria that are indirectly
related to the data likelihood. To address these problems, we formally present
Joint-stochastic-approximation (JSA) autoencoders - a new family of algorithms
for building deep directed generative models, with application to
semi-supervised learning. The JSA learning algorithm directly maximizes the
data log-likelihood and simultaneously minimizes the inclusive KL divergence
the between the posteriori and the inference model. We provide theoretical
results and conduct a series of experiments to show its superiority such as
being robust to structure mismatch between encoder and decoder, consistent
handling of both discrete and continuous variables. Particularly we empirically
show that JSA autoencoders with discrete latent space achieve comparable
performance to other state-of-the-art DGMs with continuous latent space in
semi-supervised tasks over the widely adopted datasets - MNIST and SVHN. To the
best of our knowledge, this is the first demonstration that discrete latent
variable models are successfully applied in the challenging semi-supervised
tasks.

</details>


### [444] [Learning Fluid-Structure Interaction Dynamics with Physics-Informed Neural Networks and Immersed Boundary Methods](https://arxiv.org/abs/2505.18565)
*Afrah Farea,Saiful Khan,Reza Daryani,Emre Cenk Ersan,Mustafa Serdar Celebi*

Main category: cs.LG

TL;DR: 该论文结合物理信息神经网络(PINNs)和浸没边界法(IBM)提出两种架构来解决流固耦合(FSI)问题，其中欧拉-拉格朗日架构表现更优，自适应B样条激活函数提升了边界附近的精度。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过结合PINNs和IBM的方法，更有效地解决流固耦合问题，探索不同网络架构和激活函数对模拟效果的影响。

Method: 提出两种神经网络架构：单一FSI网络和欧拉-拉格朗日网络，分别采用统一参数空间和分离参数空间，并使用Tanh和自适应B样条激活函数进行比较。

Result: 在二维空腔流动问题中，欧拉-拉格朗日架构表现显著优于单一架构，自适应B样条激活函数进一步提高了边界附近的模拟精度，但压力场预测仍具挑战性。

Conclusion: 研究表明，针对特定领域的架构设计和自适应激活函数在PINN框架下对模拟流固耦合问题至关重要，未来需改进力耦合约束以提升压力场预测。

Abstract: We introduce neural network architectures that combine physics-informed
neural networks (PINNs) with the immersed boundary method (IBM) to solve
fluid-structure interaction (FSI) problems. Our approach features two distinct
architectures: a Single-FSI network with a unified parameter space, and an
innovative Eulerian-Lagrangian network that maintains separate parameter spaces
for fluid and structure domains. We study each architecture using standard Tanh
and adaptive B-spline activation functions. Empirical studies on a 2D cavity
flow problem involving a moving solid structure show that the
Eulerian-Lagrangian architecture performs significantly better. The adaptive
B-spline activation further enhances accuracy by providing locality-aware
representation near boundaries. While our methodology shows promising results
in predicting the velocity field, pressure recovery remains challenging due to
the absence of explicit force-coupling constraints in the current formulation.
Our findings underscore the importance of domain-specific architectural design
and adaptive activation functions for modeling FSI problems within the PINN
framework.

</details>


### [445] [Learning without Isolation: Pathway Protection for Continual Learning](https://arxiv.org/abs/2505.18568)
*Zhikang Chen,Abudukelimu Wuerkaixi,Sen Cui,Haoxuan Li,Ding Li,Jingfeng Zhang,Bo Han,Gang Niu,Houfang Liu,Yi Yang,Sifan Yang,Changshui Zhang,Tianling Ren*

Main category: cs.LG

TL;DR: 该论文提出了一种新的持续学习框架LwI，通过保护旧任务的激活通路而非参数，有效解决了深度网络在连续任务学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有的持续学习方法主要关注保护或调整与旧任务相关的参数，但这种方法通常不切实际，因为参数数量会随任务数量线性增长。论文从神经科学和物理学的角度提出，网络中的通路比参数更重要。

Method: 论文提出了一种名为LwI（学习而不隔离）的新框架，将模型融合表述为图匹配问题，保护旧任务占用的通路而不隔离它们，并自适应地为新任务分配可用通路。

Result: 在流行的基准数据集上的实验表明，LwI在参数效率高的前提下，有效解决了灾难性遗忘问题，并表现出优越性能。

Conclusion: LwI框架通过保护通路而非参数，实现了高效的持续学习，为解决灾难性遗忘问题提供了新的思路。

Abstract: Deep networks are prone to catastrophic forgetting during sequential task
learning, i.e., losing the knowledge about old tasks upon learning new tasks.
To this end, continual learning(CL) has emerged, whose existing methods focus
mostly on regulating or protecting the parameters associated with the previous
tasks. However, parameter protection is often impractical, since the size of
parameters for storing the old-task knowledge increases linearly with the
number of tasks, otherwise it is hard to preserve the parameters related to the
old-task knowledge. In this work, we bring a dual opinion from neuroscience and
physics to CL: in the whole networks, the pathways matter more than the
parameters when concerning the knowledge acquired from the old tasks. Following
this opinion, we propose a novel CL framework, learning without isolation(LwI),
where model fusion is formulated as graph matching and the pathways occupied by
the old tasks are protected without being isolated. Thanks to the sparsity of
activation channels in a deep network, LwI can adaptively allocate available
pathways for a new task, realizing pathway protection and addressing
catastrophic forgetting in a parameter-efficient manner. Experiments on popular
benchmark datasets demonstrate the superiority of the proposed LwI.

</details>


### [446] [VISTA: Vision-Language Inference for Training-Free Stock Time-Series Analysis](https://arxiv.org/abs/2505.18570)
*Tina Khezresmaeilzadeh,Parsa Razmara,Seyedarmin Azizi,Mohammad Erfan Sadeghi,Erfan Baghaei Portaghloo*

Main category: cs.LG

TL;DR: VISTA是一种无需训练的多模态股票预测框架，结合视觉语言模型（VLMs）分析历史股价文本和折线图，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 股票价格预测是金融分析中的复杂且高风险任务，传统统计模型或单一模态方法可能忽略重要模式，因此需要一种多模态、无需特定任务训练的新方法。

Method: VISTA利用视觉语言模型（VLMs），通过结合历史股价的文本表示和对应折线图，采用零样本设置和思维链提示进行多模态预测。

Result: 实验表明，VISTA比ARIMA和纯文本LLM方法表现更好，最高提升89.83%，验证了多模态推理在股票时间序列分析中的有效性。

Conclusion: VISTA展示了视觉语言模型在金融预测任务中的潜力，无需任务特定训练即可实现优越性能，为多模态金融分析提供了新方向。

Abstract: Stock price prediction remains a complex and high-stakes task in financial
analysis, traditionally addressed using statistical models or, more recently,
language models. In this work, we introduce VISTA (Vision-Language Inference
for Stock Time-series Analysis), a novel, training-free framework that
leverages Vision-Language Models (VLMs) for multi-modal stock forecasting.
VISTA prompts a VLM with both textual representations of historical stock
prices and their corresponding line charts to predict future price values. By
combining numerical and visual modalities in a zero-shot setting and using
carefully designed chain-of-thought prompts, VISTA captures complementary
patterns that unimodal approaches often miss. We benchmark VISTA against
standard baselines, including ARIMA and text-only LLM-based prompting methods.
Experimental results show that VISTA outperforms these baselines by up to
89.83%, demonstrating the effectiveness of multi-modal inference for stock
time-series analysis and highlighting the potential of VLMs in financial
forecasting tasks without requiring task-specific training.

</details>


### [447] [Enhancing Efficiency and Exploration in Reinforcement Learning for LLMs](https://arxiv.org/abs/2505.18573)
*Mengqi Liao,Xiangyu Xi,Ruinian Chen,Jia Leng,Yangen Hu,Ke Zeng,Shuai Liu,Huaiyu Wan*

Main category: cs.LG

TL;DR: 该论文提出了一种动态分配rollout预算和自适应温度调整策略，以提高大型语言模型在强化学习中的效率和探索能力。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法对所有问题分配相同的rollout次数，效率低下，且限制了模型的探索能力，导致性能上限低于基础模型。

Method: 提出动态分配rollout预算的机制，根据问题难度调整预算，并引入自适应动态温度调整策略以维持熵的稳定水平。

Result: 该方法在提高响应精度的同时保持了模型的探索能力，能够发现潜在的正确路径。

Conclusion: 通过动态分配rollout预算和自适应温度调整，论文提出的方法有效提升了大型语言模型在强化学习中的效率和性能。

Abstract: Reasoning large language models (LLMs) excel in complex tasks, which has
drawn significant attention to reinforcement learning (RL) for LLMs. However,
existing approaches allocate an equal number of rollouts to all questions
during the RL process, which is inefficient. This inefficiency stems from the
fact that training on simple questions yields limited gains, whereas more
rollouts are needed for challenging questions to sample correct answers.
Furthermore, while RL improves response precision, it limits the model's
exploration ability, potentially resulting in a performance cap below that of
the base model prior to RL. To address these issues, we propose a mechanism for
dynamically allocating rollout budgets based on the difficulty of the problems,
enabling more efficient RL training. Additionally, we introduce an adaptive
dynamic temperature adjustment strategy to maintain the entropy at a stable
level, thereby encouraging sufficient exploration. This enables LLMs to improve
response precision while preserving their exploratory ability to uncover
potential correct pathways. The code and data is available on:
https://github.com/LiaoMengqi/E3-RL4LLMs

</details>


### [448] [Mechanical in-sensor computing: a programmable meta-sensor for structural damage classification without external electronic power](https://arxiv.org/abs/2505.18579)
*Tingpeng Zhang,Xuzhang Peng,Mingyuan Zhou,Guobiao Hu,Zhilu Lai*

Main category: cs.LG

TL;DR: 该论文提出了一种基于超材料的传感器（MM-sensor），用于结构健康监测（SHM），通过物理计算直接在传感器级别完成数据采集和分析，无需外部电源或电子计算单元。


<details>
  <summary>Details</summary>
Motivation: 当前结构健康监测依赖于有线系统和电子计算机，存在高能耗和低吞吐量的问题。物理计算的概念为SHM提供了新的思路，通过超材料实现传感与计算的物理集成，适用于资源受限场景。

Method: 采用局部共振超材料板（LRMP）配置，通过逆向设计几何参数调整带隙特性，实现结构振动信息的物理处理，完成损伤预警等SHM任务。

Result: 成功制造了MM-sensor，能够通过LRMP的带隙特性物理区分结构损伤前后的动态行为，适用于第一自然频率在9.54 Hz至81.86 Hz范围内的工程系统。

Conclusion: MM-sensor为SHM提供了一种无需外部电源和电子计算的高效解决方案，展示了超材料在物理计算中的潜力。

Abstract: Structural health monitoring (SHM) involves sensor deployment, data
acquisition, and data interpretation, commonly implemented via a tedious wired
system. The information processing in current practice majorly depends on
electronic computers, albeit with universal applications, delivering challenges
such as high energy consumption and low throughput due to the nature of digital
units. In recent years, there has been a renaissance interest in shifting
computations from electronic computing units to the use of real physical
systems, a concept known as physical computation. This approach provides the
possibility of thinking out of the box for SHM, seamlessly integrating sensing
and computing into a pure-physical entity, without relying on external
electronic power supplies, thereby properly coping with resource-restricted
scenarios. The latest advances of metamaterials (MM) hold great promise for
this proactive idea. In this paper, we introduce a programmable
metamaterial-based sensor (termed as MM-sensor) for physically processing
structural vibration information to perform specific SHM tasks, such as
structural damage warning (binary classification) in this initiation, without
the need for further information processing or resource-consuming, that is, the
data collection and analysis are completed in-situ at the sensor level. We
adopt the configuration of a locally resonant metamaterial plate (LRMP) to
achieve the first fabrication of the MM-sensor. We take advantage of the
bandgap properties of LRMP to physically differentiate the dynamic behavior of
structures before and after damage. By inversely designing the geometric
parameters, our current approach allows for adjustments to the bandgap
features. This is effective for engineering systems with a first natural
frequency ranging from 9.54 Hz to 81.86 Hz.

</details>


### [449] [Bayesian Meta-Reinforcement Learning with Laplace Variational Recurrent Networks](https://arxiv.org/abs/2505.18591)
*Joery A. de Vries,Jinke He,Mathijs M. de Weerdt,Matthijs T. J. Spaan*

Main category: cs.LG

TL;DR: 该论文提出使用拉普拉斯近似增强元强化学习中的点估计方法，以生成完整分布，从而更准确地估计任务后验分布。


<details>
  <summary>Details</summary>
Motivation: 现有的元强化学习方法通常使用点估计来表示任务分布，但这种方法可能产生过度自信的估计且不满足一致性。论文旨在通过拉普拉斯近似改进这一局限性。

Method: 通过拉普拉斯近似对点估计进行增强，生成完整分布，无需修改基础模型架构。该方法可在学习前、学习过程中或学习后应用。

Result: 实验表明，基于点估计的方法会产生过度自信的估计，而拉普拉斯近似方法在性能上与基于完整分布的学习方法相当，但参数更少。

Conclusion: 拉普拉斯近似是一种有效的元强化学习方法，能够在不增加模型复杂度的情况下，提供更准确的任务后验分布估计。

Abstract: Meta-reinforcement learning trains a single reinforcement learning agent on a
distribution of tasks to quickly generalize to new tasks outside of the
training set at test time. From a Bayesian perspective, one can interpret this
as performing amortized variational inference on the posterior distribution
over training tasks. Among the various meta-reinforcement learning approaches,
a common method is to represent this distribution with a point-estimate using a
recurrent neural network. We show how one can augment this point estimate to
give full distributions through the Laplace approximation, either at the start
of, during, or after learning, without modifying the base model architecture.
With our approximation, we are able to estimate distribution statistics (e.g.,
the entropy) of non-Bayesian agents and observe that point-estimate based
methods produce overconfident estimators while not satisfying consistency.
Furthermore, when comparing our approach to full-distribution based learning of
the task posterior, our method performs on par with variational baselines while
having much fewer parameters.

</details>


### [450] [MisoDICE: Multi-Agent Imitation from Unlabeled Mixed-Quality Demonstrations](https://arxiv.org/abs/2505.18595)
*The Viet Bui,Tien Mai,Hong Thanh Nguyen*

Main category: cs.LG

TL;DR: 该论文提出了一种两阶段方法（轨迹标注和多智能体模仿学习），用于从混合质量的未标注演示数据中学习稳健策略，并在多智能体强化学习基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决多智能体协作场景中离线模仿学习的挑战，特别是当演示数据包含未标注的专家和次优轨迹时，如何有效区分并利用高质量数据进行策略学习。

Method: 方法分为两个阶段：1) 结合大语言模型和基于偏好的强化学习构建渐进式标注流程；2) 提出MisoDICE算法，通过新的价值分解和混合架构扩展单智能体DICE框架，解决大规模联合状态-动作空间的计算复杂度问题。

Result: 实验结果表明，MisoDICE在多个标准多智能体强化学习基准测试中表现优异，尤其在专家数据稀缺的情况下显著优于其他方法。

Conclusion: 结论表明，通过两阶段方法能够有效利用混合质量数据，提出的MisoDICE算法在多智能体模仿学习中实现了策略优化的一致性和高效性。

Abstract: We study offline imitation learning (IL) in cooperative multi-agent settings,
where demonstrations have unlabeled mixed quality - containing both expert and
suboptimal trajectories. Our proposed solution is structured in two stages:
trajectory labeling and multi-agent imitation learning, designed jointly to
enable effective learning from heterogeneous, unlabeled data. In the first
stage, we combine advances in large language models and preference-based
reinforcement learning to construct a progressive labeling pipeline that
distinguishes expert-quality trajectories. In the second stage, we introduce
MisoDICE, a novel multi-agent IL algorithm that leverages these labels to learn
robust policies while addressing the computational complexity of large joint
state-action spaces. By extending the popular single-agent DICE framework to
multi-agent settings with a new value decomposition and mixing architecture,
our method yields a convex policy optimization objective and ensures
consistency between global and local policies. We evaluate MisoDICE on multiple
standard multi-agent RL benchmarks and demonstrate superior performance,
especially when expert data is scarce.

</details>


### [451] [Exemplar-Free Continual Learning for State Space Models](https://arxiv.org/abs/2505.18604)
*Isaac Ning Lee,Leila Mahmoodi,Trung Le,Mehrtash Harandi*

Main category: cs.LG

TL;DR: 提出Inf-SSM方法，通过Grassmann流形几何约束状态空间模型在持续学习中的状态演化，降低遗忘并提升准确率。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型(SSMs)擅长捕捉长程依赖，但其动态内部状态在持续学习(CL)中难以适应，尤其在无样本场景下会导致灾难性遗忘。

Method: 利用无限维Grassmann流形几何约束状态演化，通过求解Sylvester方程实现高效正则化，计算复杂度从O(n³)降至O(n²)。

Result: 在ImageNet-R等基准测试中显著减少遗忘，同时提高跨任务准确率，且可与现有CL方法无缝集成。

Conclusion: Inf-SSM通过几何感知正则化有效解决了SSMs在持续学习中的状态演化问题，平衡了模型适应性与稳定性。

Abstract: State-Space Models (SSMs) excel at capturing long-range dependencies with
structured recurrence, making them well-suited for sequence modeling. However,
their evolving internal states pose challenges in adapting them under Continual
Learning (CL). This is particularly difficult in exemplar-free settings, where
the absence of prior data leaves updates to the dynamic SSM states
unconstrained, resulting in catastrophic forgetting. To address this, we
propose Inf-SSM, a novel and simple geometry-aware regularization method that
utilizes the geometry of the infinite-dimensional Grassmannian to constrain
state evolution during CL. Unlike classical continual learning methods that
constrain weight updates, Inf-SSM regularizes the infinite-horizon evolution of
SSMs encoded in their extended observability subspace. We show that enforcing
this regularization requires solving a matrix equation known as the Sylvester
equation, which typically incurs $\mathcal{O}(n^3)$ complexity. We develop a
$\mathcal{O}(n^2)$ solution by exploiting the structure and properties of SSMs.
This leads to an efficient regularization mechanism that can be seamlessly
integrated into existing CL methods. Comprehensive experiments on challenging
benchmarks, including ImageNet-R and Caltech-256, demonstrate a significant
reduction in forgetting while improving accuracy across sequential tasks.

</details>


### [452] [Trust, or Don't Predict: Introducing the CWSA Family for Confidence-Aware Model Evaluation](https://arxiv.org/abs/2505.18622)
*Kourosh Shahnazari,Seyed Moein Ayyoubzadeh,Mohammadali Keshtparvar,Pegah Ghaffari*

Main category: cs.LG

TL;DR: 该论文提出两种新指标CWSA和CWSA+，用于评估置信度阈值下的预测模型，能有效检测细微错误模式并在信任敏感测试中优于传统指标。


<details>
  <summary>Details</summary>
Motivation: 传统指标如准确率、预期校准误差（ECE）和风险覆盖曲线下面积（AURC）无法真实反映预测的可靠性，忽视了置信度或过度自信的错误分类，这在现实系统中可能带来严重问题。

Method: 引入两种新指标：置信度加权选择性准确率（CWSA）及其归一化变体CWSA+，通过显式奖励高置信度准确预测和惩罚过度自信错误，提供了一种可解释的评估方法。

Result: 在真实数据集（MNIST、CIFAR-10）和人工模型变体上的实验表明，CWSA和CWSA+能有效检测细微错误模式，并在信任敏感测试中优于传统指标。

Conclusion: CWSA为安全关键领域的选择性预测系统开发和评估提供了可靠基础。

Abstract: In recent machine learning systems, confidence scores are being utilized more
and more to manage selective prediction, whereby a model can abstain from
making a prediction when it is unconfident. Yet, conventional metrics like
accuracy, expected calibration error (ECE), and area under the risk-coverage
curve (AURC) do not capture the actual reliability of predictions. These
metrics either disregard confidence entirely, dilute valuable localized
information through averaging, or neglect to suitably penalize overconfident
misclassifications, which can be particularly detrimental in real-world
systems. We introduce two new metrics Confidence-Weighted Selective Accuracy
(CWSA) and its normalized variant CWSA+ that offer a principled and
interpretable way to evaluate predictive models under confidence thresholds.
Unlike existing methods, our metrics explicitly reward confident accuracy and
penalize overconfident mistakes. They are threshold-local, decomposable, and
usable in both evaluation and deployment settings where trust and risk must be
quantified. Through exhaustive experiments on both real-world data sets (MNIST,
CIFAR-10) and artificial model variants (calibrated, overconfident,
underconfident, random, perfect), we show that CWSA and CWSA+ both effectively
detect nuanced failure modes and outperform classical metrics in
trust-sensitive tests. Our results confirm that CWSA is a sound basis for
developing and assessing selective prediction systems for safety-critical
domains.

</details>


### [453] [Think Before You Accept: Semantic Reflective Verification for Faster Speculative Decoding](https://arxiv.org/abs/2505.18629)
*Yixuan Wang,Yijun Liu,Shiyu ji,Yuzhuang Xu,Yang Xu,Qingfu Zhu,Wanxiang Che*

Main category: cs.LG

TL;DR: 提出Reflective Verification方法，通过语义验证提升大语言模型推理速度，无需训练且保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法过于依赖分布一致性而忽略语义正确性，限制了加速潜力，且现有验证方法在开放域场景泛化性不足。

Method: 利用LLMs的反射能力，通过提示探测同时获取原始和反射分布，融合二者实现语义级验证。

Result: 在多领域基准测试中显著增加草稿token接受长度，解码速度提升5%~15%，且与现有统计验证方法正交。

Conclusion: 反射验证在正确性与效率间取得更好平衡，为推测解码提供了语义感知的新方向。

Abstract: Large language models (LLMs) suffer from high inference latency due to the
auto-regressive decoding process. Speculative decoding accelerates inference by
generating multiple draft tokens using a lightweight model and verifying them
in parallel. However, existing verification methods rely heavily on
distributional consistency while overlooking semantic correctness, thereby
limiting the potential speedup of speculative decoding. While some methods
employ additional models for relaxed verification of draft tokens, they often
fail to generalize effectively to more diverse or open-domain settings. In this
work, we propose Reflective Verification, a training-free and semantics-aware
approach that achieves a better trade-off between correctness and efficiency.
Specifically, we leverage the inherent reflective capacity of LLMs to
semantically assess the correctness of draft tokens in parallel during
verification. Using prompt-based probing, we obtain both the original and
reflective distributions of draft tokens in a single forward pass. The fusion
of these distributions enables semantic-level verification of draft tokens that
incorporates both consistency and correctness. Experiments across multiple
domain benchmarks and model scales demonstrate that our method significantly
increases the acceptance length of draft tokens without compromising model
performance. Furthermore, we find that the proposed Reflective Verification is
orthogonal to existing statistical verification methods, and their combination
yields additional 5$\sim$15\% improvements in decoding speed.

</details>


### [454] [Asymmetric Duos: Sidekicks Improve Uncertainty](https://arxiv.org/abs/2505.18636)
*Tim G. Zhou,Evan Shelhamer,Geoff Pleiss*

Main category: cs.LG

TL;DR: 该论文提出了一种成本效益高的策略，通过结合大型模型和小型“辅助”模型来提升不确定性量化和决策性能，仅增加少量计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统使用随机初始化的集成方法在大规模模型和实际微调工作流中效率低下，需要一种更高效的方法来提升不确定性量化和决策性能。

Method: 引入非对称双模型策略，将大型模型与计算成本低的小型辅助模型结合，通过简单加权平均聚合预测结果。

Result: 在五个图像分类基准测试中，非对称双模型显著提升了准确性、不确定性量化和选择性分类指标，仅增加约10-20%的计算成本。

Conclusion: 非对称双模型策略是一种高效且成本效益高的方法，能够在不显著增加计算负担的情况下显著提升模型性能。

Abstract: The go-to strategy to apply deep networks in settings where uncertainty
informs decisions--ensembling multiple training runs with random
initializations--is ill-suited for the extremely large-scale models and
practical fine-tuning workflows of today. We introduce a new cost-effective
strategy for improving the uncertainty quantification and downstream decisions
of a large model (e.g. a fine-tuned ViT-B): coupling it with a less accurate
but much smaller "sidekick" (e.g. a fine-tuned ResNet-34) with a fraction of
the computational cost. We propose aggregating the predictions of this
\emph{Asymmetric Duo} by simple learned weighted averaging. Surprisingly,
despite their inherent asymmetry, the sidekick model almost never harms the
performance of the larger model. In fact, across five image classification
benchmarks and a variety of model architectures and training schemes (including
soups), Asymmetric Duos significantly improve accuracy, uncertainty
quantification, and selective classification metrics with only ${\sim}10-20\%$
more computation.

</details>


### [455] [ThanoRA: Task Heterogeneity-Aware Multi-Task Low-Rank Adaptation](https://arxiv.org/abs/2505.18640)
*Jian Liang,Wenke Huang,Xianda Guo,Guancheng Wan,Bo Du,Mang Ye*

Main category: cs.LG

TL;DR: ThanoRA提出了一种任务异构感知的多任务低秩适配框架，通过构建任务特定子空间和子空间保持正则化，实现了高效统一的多任务适配，且不增加推理开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多任务适配时存在参数不可合并和推理开销增加的问题，限制了实际部署的可行性。

Method: ThanoRA通过初始化时构建任务特定LoRA子空间，并引入子空间保持正则化来防止任务干扰和子空间坍塌。

Result: 在多模态和纯文本基准测试中，ThanoRA在多种任务混合下均表现稳健且优于基线方法。

Conclusion: ThanoRA在保持LoRA推理效率的同时，实现了高效统一的多任务适配，具有实际部署价值。

Abstract: Low-Rank Adaptation (LoRA) is widely adopted for downstream fine-tuning of
foundation models due to its efficiency and zero additional inference cost.
Many real-world applications require foundation models to specialize in
multiple tasks simultaneously, motivating the need for efficient multi-task
adaptation. While recent approaches integrate LoRA with mixture-of-experts
(MoE) to address this, the use of routers prevents parameter mergeability,
which increases inference overhead and hinders unified multi-task adaptation,
thereby limiting deployment practicality. In this work, we propose ThanoRA, a
Task Heterogeneity-Aware Multi-Task Low-Rank Adaptation framework that enables
multi-task adaptation while preserving the inference efficiency of LoRA.
ThanoRA jointly models task heterogeneity and mitigates subspace interference
throughout training. Specifically, motivated by inherent differences in
complexity and heterogeneity across tasks, ThanoRA constructs task-specific
LoRA subspaces at initialization, enabling fine-grained knowledge injection
aligned with task heterogeneity. Furthermore, to prevent task interference and
subspace collapse during multi-task training, ThanoRA introduces a
subspace-preserving regularization that maintains the independence of
task-specific representations. With the synergy of both components, ThanoRA
enables efficient and unified multi-task adaptation. Extensive experiments
across multimodal and text-only benchmarks under varying multi-task mixtures
demonstrate that ThanoRA consistently achieves robust and superior performance
over strong baselines without introducing additional inference overhead. Our
code is publicly available at: https://github.com/LiangJian24/ThanoRA.

</details>


### [456] [Flow Matching for Geometric Trajectory Simulation](https://arxiv.org/abs/2505.18647)
*Kiet Bennema ten Brinke,Koen Minartz,Vlado Menkovski*

Main category: cs.LG

TL;DR: STFlow利用流匹配和数据依赖耦合，实现了物理信息驱动的几何轨迹模拟，显著降低了预测误差并提高了推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在模拟N体系统时需从无信息噪声开始学习复杂变换，无法利用领域先验知识，限制了模拟的真实性和效率。

Method: 提出STFlow方法，结合流匹配和数据依赖耦合，支持物理信息驱动的几何轨迹模拟，同时保持模型表达能力和可扩展性。

Result: 在N体动力系统、分子动力学和行人动力学基准测试中，STFlow的预测误差显著降低，推理效率更高。

Conclusion: STFlow通过引入物理信息先验分布，在概率几何轨迹建模中展现出优越性能，为复杂系统模拟提供了新思路。

Abstract: The simulation of N-body systems is a fundamental problem with applications
in a wide range of fields, such as molecular dynamics, biochemistry, and
pedestrian dynamics. Machine learning has become an invaluable tool for scaling
physics-based simulators and developing models directly from experimental data.
In particular, recent advances based on deep generative modeling and geometric
deep learning have enabled probabilistic simulation by modeling complex
distributions over trajectories while respecting the permutation symmetry that
is fundamental to N-body systems. However, to generate realistic trajectories,
existing methods must learn complex transformations starting from uninformed
noise and do not allow for the exploitation of domain-informed priors. In this
work, we propose STFlow to address this limitation. By leveraging flow matching
and data-dependent couplings, STFlow facilitates physics-informed simulation of
geometric trajectories without sacrificing model expressivity or scalability.
Our evaluation on N-body dynamical systems, molecular dynamics, and pedestrian
dynamics benchmarks shows that STFlow produces significantly lower prediction
errors while enabling more efficient inference, highlighting the benefits of
employing physics-informed prior distributions in probabilistic geometric
trajectory modeling.

</details>


### [457] [LLM-QFL: Distilling Large Language Model for Quantum Federated Learning](https://arxiv.org/abs/2505.18656)
*Dev Gurung,Shiva Raj Pokhrel*

Main category: cs.LG

TL;DR: 该研究将大语言模型（LLM）与量子联邦学习（QFL）结合，提出一种联邦微调方法，提升效率与性能，同时保护隐私并减少通信开销。


<details>
  <summary>Details</summary>
Motivation: 受大语言模型（LLM）强大能力的启发，研究旨在将其应用于量子联邦学习（QFL），以提升效率与性能，同时解决隐私保护和通信成本的问题。

Method: 提出一种联邦微调方法，在QFL框架内蒸馏LLM，使各客户端能本地适配模型，并通过LLM作为强化代理优化QFL流程（如调整优化步骤、减少通信轮次和智能选择客户端）。

Result: 实验显示该方法显著提升效率：降低通信成本、加速收敛，并支持理论可证的联邦优化保证。通过PEFT技术（如LoRA、QLoRA）实现资源受限量子设备的部署。

Conclusion: 该研究开创了LLM与QFL的协同范式，兼具实践效率、理论严谨性和可扩展性，为量子联邦学习提供了新方向。

Abstract: Inspired by the power of large language models (LLMs), our research adapts
them to quantum federated learning (QFL) to boost efficiency and performance.
We propose a federated fine-tuning method that distills an LLM within QFL,
allowing each client to locally adapt the model to its own data while
preserving privacy and reducing unnecessary global updates. The fine-tuned LLM
also acts as a reinforcement agent, optimizing QFL by adjusting optimizer
steps, cutting down communication rounds, and intelligently selecting clients.
Experiments show significant efficiency gains. We pioneer a synergy between LLM
and QFL, offering: i) practical efficiency: Reduced communication costs and
faster convergence. ii) theoretical rigor: Provable guarantees for adaptive
federated optimization. iii) scalability: PEFT methods (LoRA, QLoRA) enable
deployment on resource-constrained quantum devices. Code implementation is
available here 1.

</details>


### [458] [Self-Supervised Evolution Operator Learning for High-Dimensional Dynamical Systems](https://arxiv.org/abs/2505.18671)
*Giacomo Turri,Luigi Bonati,Kai Zhu,Massimiliano Pontil,Pietro Novelli*

Main category: cs.LG

TL;DR: 本文提出了一种基于编码器的自监督学习方法，用于学习大规模非线性动力系统的演化算子，并在蛋白质折叠、药物分子结合和气候数据分析等多个科学领域验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大规模天气数据集和分子动力学模拟工具的普及，需要一种数据驱动的方法来分析和理解这些复杂系统的时空演化行为。演化算子作为一种关键分析工具，能够帮助揭示这些系统的动态规律。

Method: 通过建立自监督表示学习方法与演化算子学习理论之间的联系，提出了一种编码器-only的框架，用于从数据中学习非线性动力系统的演化算子。

Result: 该方法在多个科学领域得到验证：成功解释了小蛋白质的折叠动力学、药物分子在宿主位点的结合过程，并能够自主发现气候数据中的模式。代码和数据已开源。

Conclusion: 提出的方法为分析大规模复杂动力系统提供了一种有效的工具，展示了自监督学习在科学计算中的潜力，并为跨领域研究提供了新的可能性。

Abstract: We introduce an encoder-only approach to learn the evolution operators of
large-scale non-linear dynamical systems, such as those describing complex
natural phenomena. Evolution operators are particularly well-suited for
analyzing systems that exhibit complex spatio-temporal patterns and have become
a key analytical tool across various scientific communities. As terabyte-scale
weather datasets and simulation tools capable of running millions of molecular
dynamics steps per day are becoming commodities, our approach provides an
effective tool to make sense of them from a data-driven perspective. The core
of it lies in a remarkable connection between self-supervised representation
learning methods and the recently established learning theory of evolution
operators. To show the usefulness of the proposed method, we test it across
multiple scientific domains: explaining the folding dynamics of small proteins,
the binding process of drug-like molecules in host sites, and autonomously
finding patterns in climate data. Code and data to reproduce the experiments
are made available open source.

</details>


### [459] [Does Representation Intervention Really Identify Desired Concepts and Elicit Alignment?](https://arxiv.org/abs/2505.18672)
*Hongzheng Yang,Yongqiang Chen,Zeyu Qin,Tongliang Liu,Chaowei Xiao,Kun Zhang,Bo Han*

Main category: cs.LG

TL;DR: 该论文探讨了在大型语言模型(LLMs)中定位和修改概念表示的可行性，提出了COCA方法以简化有害与良性表示的决策边界，有效降低越狱攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于验证是否能在LLMs中准确定位概念表示进行干预，特别是在安全对齐领域，以消除有害概念并保持模型正常功能。

Method: 提出Concept Concentration (COCA)方法，通过重构训练数据并显式推理过程，识别潜在不安全概念并决定响应，简化表示决策边界。

Result: 实验表明COCA显著降低了分布内和分布外越狱攻击的成功率，同时保持了数学和代码生成等常规任务的性能。

Conclusion: COCA方法在非线性设置中有效解决了概念干预的可行性问题，为LLMs的安全对齐提供了新思路。

Abstract: Representation intervention aims to locate and modify the representations
that encode the underlying concepts in Large Language Models (LLMs) to elicit
the aligned and expected behaviors. Despite the empirical success, it has never
been examined whether one could locate the faithful concepts for intervention.
In this work, we explore the question in safety alignment. If the interventions
are faithful, the intervened LLMs should erase the harmful concepts and be
robust to both in-distribution adversarial prompts and the out-of-distribution
(OOD) jailbreaks. While it is feasible to erase harmful concepts without
degrading the benign functionalities of LLMs in linear settings, we show that
it is infeasible in the general non-linear setting. To tackle the issue, we
propose Concept Concentration (COCA). Instead of identifying the faithful
locations to intervene, COCA refractors the training data with an explicit
reasoning process, which firstly identifies the potential unsafe concepts and
then decides the responses. Essentially, COCA simplifies the decision boundary
between harmful and benign representations, enabling more effective linear
erasure. Extensive experiments with multiple representation intervention
methods and model architectures demonstrate that COCA significantly reduces
both in-distribution and OOD jailbreak success rates, and meanwhile maintaining
strong performance on regular tasks such as math and code generation.

</details>


### [460] [Simultaneous Optimization of Efficiency and Degradation in Tunable HTL-Free Perovskite Solar Cells with MWCNT-Integrated Back Contact Using a Machine Learning-Derived Polynomial Regressor](https://arxiv.org/abs/2505.18693)
*Ihtesham Ibn Malek,Hafiz Imtiaz,Samia Subrina*

Main category: cs.LG

TL;DR: 该研究通过机器学习优化无空穴传输层的钙钛矿太阳能电池，结合实验与模拟，显著提升效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 无空穴传输层（HTL）的钙钛矿太阳能电池（PSCs）因其成本低和稳定性高而备受关注，但效率和稳定性仍需优化。本研究旨在通过机器学习方法解决这一问题。

Method: 研究结合实验验证和数值模拟，生成1650个样本数据集，使用四阶多项式回归器（PR-4）进行建模，并通过L-BFGS-B优化算法最大化效率并最小化降解。

Result: 优化后，电池效率从13.7%提升至16.84%，1000小时内的降解率从6.61%降至2.39%。多层感知器（MLP）分类器能100%准确识别最优配置。

Conclusion: 机器学习方法有效优化了无HTL的PSCs性能，为未来太阳能电池设计提供了新思路。

Abstract: Perovskite solar cells (PSCs) without a hole transport layer (HTL) offer a
cost-effective and stable alternative to conventional architectures, utilizing
only an absorber layer and an electron transport layer (ETL). This study
presents a machine learning (ML)-driven framework to optimize the efficiency
and stability of HTL-free PSCs by integrating experimental validation with
numerical simulations. Excellent agreement is achieved between a fabricated
device and its simulated counterpart at a molar fraction \( x = 68.7\% \) in
\(\mathrm{MAPb}_{1-x}\mathrm{Sb}_{2x/3}\mathrm{I}_3\), where MA is
methylammonium. A dataset of 1650 samples is generated by varying molar
fraction, absorber defect density, thickness, and ETL doping, with
corresponding efficiency and 50-hour degradation as targets. A fourth-degree
polynomial regressor (PR-4) shows the best performance, achieving RMSEs of
0.0179 and 0.0117, and \( R^2 \) scores of 1 and 0.999 for efficiency and
degradation, respectively. The derived model generalizes beyond the training
range and is used in an L-BFGS-B optimization algorithm with a weighted
objective function to maximize efficiency and minimize degradation. This
improves device efficiency from 13.7\% to 16.84\% and reduces degradation from
6.61\% to 2.39\% over 1000 hours. Finally, the dataset is labeled into superior
and inferior classes, and a multilayer perceptron (MLP) classifier achieves
100\% accuracy, successfully identifying optimal configurations.

</details>


### [461] [Can LLMs Alleviate Catastrophic Forgetting in Graph Continual Learning? A Systematic Study](https://arxiv.org/abs/2505.18697)
*Ziyang Cheng,Zhixun Li,Yuhan Li,Yixin Song,Kangyi Zhao,Dawei Cheng,Jia Li,Jeffrey Xu Yu*

Main category: cs.LG

TL;DR: 本文探讨了大语言模型（LLMs）在图持续学习（GCL）中缓解灾难性遗忘的潜力，提出了一个简单有效的方法SimGCL，并在无排练约束下超越了现有最佳GNN基线约20%。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的数据（包括图结构数据）通常以流式方式到达，学习系统需要在不忘记先前知识的情况下持续获取新知识。尽管已有大量工作尝试解决图机器学习中的灾难性遗忘问题，但它们都基于从头开始训练流式数据。随着预训练模型的兴起，越来越多的研究利用其强大的泛化能力进行持续学习。

Method: 本文首先指出当前GCL实验设置中的重大缺陷（如评估阶段可能导致任务ID泄漏），然后在更现实的场景中评估LLMs的性能，并提出一个简单而有效的方法SimGCL。

Result: 实验表明，即使微小的修改也能带来出色的结果，SimGCL在无排练约束下超越了之前最先进的GNN基线约20%。

Conclusion: 本文证明了LLMs在GCL中的潜力，并提出了一个简单有效的方法SimGCL。同时，为了促进可重复性，开发了一个易于使用的基准LLM4GCL用于训练和评估现有GCL方法。

Abstract: Nowadays, real-world data, including graph-structure data, often arrives in a
streaming manner, which means that learning systems need to continuously
acquire new knowledge without forgetting previously learned information.
Although substantial existing works attempt to address catastrophic forgetting
in graph machine learning, they are all based on training from scratch with
streaming data. With the rise of pretrained models, an increasing number of
studies have leveraged their strong generalization ability for continual
learning. Therefore, in this work, we attempt to answer whether large language
models (LLMs) can mitigate catastrophic forgetting in Graph Continual Learning
(GCL). We first point out that current experimental setups for GCL have
significant flaws, as the evaluation stage may lead to task ID leakage. Then,
we evaluate the performance of LLMs in more realistic scenarios and find that
even minor modifications can lead to outstanding results. Finally, based on
extensive experiments, we propose a simple-yet-effective method, Simple Graph
Continual Learning (SimGCL), that surpasses the previous state-of-the-art
GNN-based baseline by around 20% under the rehearsal-free constraint. To
facilitate reproducibility, we have developed an easy-to-use benchmark LLM4GCL
for training and evaluating existing GCL methods. The code is available at:
https://github.com/ZhixunLEE/LLM4GCL.

</details>


### [462] [MonarchAttention: Zero-Shot Conversion to Fast, Hardware-Aware Structured Attention](https://arxiv.org/abs/2505.18698)
*Can Yaras,Alec S. Xu,Pierre Abillama,Changwoo Lee,Laura Balzano*

Main category: cs.LG

TL;DR: MonarchAttention提出了一种基于Monarch矩阵的次二次复杂度注意力近似方法，显著降低Transformer的计算开销，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer中的注意力机制具有二次复杂度，导致长序列处理效率低下。本文旨在通过结构化矩阵近似注意力，降低计算复杂度。

Method: 基于Monarch矩阵和softmax的变分形式，提出了一种优化算法，计算复杂度为Θ(N√N d)，内存/IO复杂度为Θ(Nd)。

Result: MonarchAttention在短、中、长序列上分别实现了1.4倍、4.5倍和8.2倍的加速，且在多种视觉和语言任务中表现优异。

Conclusion: MonarchAttention是一种高效、可迁移的注意力近似方法，显著提升了Transformer的计算效率，适用于多种任务。

Abstract: Transformers have achieved state-of-the-art performance across various tasks,
but suffer from a notable quadratic complexity in sequence length due to the
attention mechanism. In this work, we propose MonarchAttention -- a novel
approach to sub-quadratic attention approximation via Monarch matrices, an
expressive class of structured matrices. Based on the variational form of
softmax, we describe an efficient optimization-based algorithm to compute an
approximate projection of softmax attention onto the class of Monarch matrices
with $\Theta(N\sqrt{N} d)$ computational complexity and $\Theta(Nd)$ memory/IO
complexity. Unlike previous approaches, MonarchAttention is both (1)
transferable, yielding minimal performance loss with no additional training,
even when replacing every attention layer of the transformer, and (2)
hardware-efficient, utilizing the highest-throughput tensor core units on
modern GPUs. With optimized kernels, MonarchAttention achieves substantial
speed-ups in wall-time over FlashAttention-2: $1.4\times$ for shorter sequences
$(N=256)$, $4.5\times$ for medium-length sequences $(N=4K)$, and $8.2\times$
for longer sequences $(N=16K)$. We demonstrate the quality of MonarchAttention
on diverse tasks and architectures in vision and language problems, showing
that it flexibly and accurately approximates softmax attention in a variety of
contexts. Our code is available at
https://github.com/cjyaras/monarch-attention.

</details>


### [463] [Steering LLM Reasoning Through Bias-Only Adaptation](https://arxiv.org/abs/2505.18706)
*Viacheslav Sinii,Alexey Gorbatovski,Artem Cherepanov,Boris Shaposhnikov,Nikita Balagansky,Daniil Gavrilov*

Main category: cs.LG

TL;DR: 研究发现，强化学习微调并未赋予语言模型新能力，而是强化了预训练网络中已有的推理模式。通过训练导向向量，模型在数学推理任务上的表现可媲美甚至超越全调优模型。


<details>
  <summary>Details</summary>
Motivation: 验证强化学习微调是否真正为语言模型带来新的推理能力，还是仅强化了预训练网络中已有的潜在模式。

Method: 使用导向向量（层间偏置）选择性放大隐藏特征，保持原始权重不变，并在GSM8K和MATH基准上测试四种基础模型。

Result: 导向向量在多类任务中恢复甚至超越了全调优模型的准确率，且分析显示其增强了与结构化语言和逻辑连接相关的标记组。

Conclusion: 数学推理所需的能力已预存在于基础模型中，强化学习微调的作用是激活而非创造这些能力。

Abstract: Recent work on reasoning-oriented language models, exemplified by o1-like
systems, suggests that reinforcement-learning (RL) finetuning does not create
new capabilities but instead strengthens reasoning patterns already latent in
the pretrained network. We test this claim by training steering vectors:
layer-wise biases that additively amplify selected hidden features while
leaving all original weights unchanged. Experiments on four base models across
the GSM8K and MATH benchmarks show that steering vectors recover, and in
several cases exceed, the accuracy of fully-tuned counterparts. This result
supports the view that the required reasoning skills pre-exist in the base
model. Further, logit-lens analysis reveals that the trained vectors
consistently boost token groups linked to structured languages and logical
connectors, providing an interpretable account that aligns with the demands of
quantitative reasoning tasks.

</details>


### [464] [Neural Parameter Search for Slimmer Fine-Tuned Models and Better Transfer](https://arxiv.org/abs/2505.18713)
*Guodong Du,Zitao Fang,Jing Li,Junlin Li,Runhua Jiang,Shuyang Yu,Yifei Guo,Yangneng Chen,Sim Kuan Goh,Ho-Kin Tang,Daojing He,Honghai Liu,Min Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一种名为NPS-Pruning的新方法，通过利用任务向量机制和低秩子空间搜索，有效剪枝微调模型，提升知识迁移、融合和模型压缩效率。


<details>
  <summary>Details</summary>
Motivation: 微调模型在特定领域外表现不佳且存在冗余，结合剪枝和预训练模型可缓解遗忘、减少任务间参数干扰并提高压缩效率。因此，开发有效的剪枝策略至关重要。

Method: 利用任务向量机制预处理微调模型，计算其与原模型的差异，并在低秩子空间中搜索神经参数，提出NPS-Pruning方法进行模型瘦身。

Result: 在视觉、NLP和多模态基准测试中，该方法显著提升了性能，同时大幅降低存储成本，保持了接近原始模型的性能。

Conclusion: NPS-Pruning方法在知识迁移、模型融合和压缩部署方面表现出色，实验证明了其有效性和鲁棒性。

Abstract: Foundation models and their checkpoints have significantly advanced deep
learning, boosting performance across various applications. However, fine-tuned
models often struggle outside their specific domains and exhibit considerable
redundancy. Recent studies suggest that combining a pruned fine-tuned model
with the original pre-trained model can mitigate forgetting, reduce
interference when merging model parameters across tasks, and improve
compression efficiency. In this context, developing an effective pruning
strategy for fine-tuned models is crucial. Leveraging the advantages of the
task vector mechanism, we preprocess fine-tuned models by calculating the
differences between them and the original model. Recognizing that different
task vector subspaces contribute variably to model performance, we introduce a
novel method called Neural Parameter Search (NPS-Pruning) for slimming down
fine-tuned models. This method enhances pruning efficiency by searching through
neural parameters of task vectors within low-rank subspaces. Our method has
three key applications: enhancing knowledge transfer through pairwise model
interpolation, facilitating effective knowledge fusion via model merging, and
enabling the deployment of compressed models that retain near-original
performance while significantly reducing storage costs. Extensive experiments
across vision, NLP, and multi-modal benchmarks demonstrate the effectiveness
and robustness of our approach, resulting in substantial performance gains. The
code is publicly available at: https://github.com/duguodong7/NPS-Pruning.

</details>


### [465] [LoTA-QAF: Lossless Ternary Adaptation for Quantization-Aware Fine-Tuning](https://arxiv.org/abs/2505.18724)
*Junyu Chen,Junzhuo Li,Zhen Peng,Wenjie Wang,Yuxiang Ren,Long Shi,Xuming Hu*

Main category: cs.LG

TL;DR: 该论文提出了一种名为LoTA-QAF的新型微调方法，用于量化大型语言模型（LLMs），解决了量化模型微调中的数据类型不匹配和精度损失问题，并在多个下游任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 量化与微调对于在资源受限的边缘设备上部署大型语言模型至关重要。然而，量化模型的微调面临数据类型不匹配、精度损失以及现有方法无法无损合并适配权重等挑战。

Method: LoTA-QAF通过以下方法实现：i) 定制设计的三元适配（TA），将三元权重与量化网格对齐并调整量化权重；ii) 基于TA的机制实现适配权重的无损合并；iii) 使用三元符号梯度下降（t-SignSGD）更新TA权重。

Result: 在MMLU基准测试中，LoTA-QAF有效恢复了量化模型的性能，比16位LoRA高出5.14%。在任务特定微调中，16位LoRA表现最佳，但LoTA-QAF仍优于其他方法。

Conclusion: LoTA-QAF是一种高效的量化感知微调方法，能够无损合并适配权重并调整所有量化权重，显著提升了量化模型在下游任务中的性能。

Abstract: Quantization and fine-tuning are crucial for deploying large language models
(LLMs) on resource-constrained edge devices. However, fine-tuning quantized
models presents significant challenges, primarily stemming from: First, the
mismatch in data types between the low-precision quantized weights (e.g.,
4-bit) and the high-precision adaptation weights (e.g., 16-bit). This mismatch
limits the computational efficiency advantage offered by quantized weights
during inference. Second, potential accuracy degradation when merging these
high-precision adaptation weights into the low-precision quantized weights, as
the adaptation weights often necessitate approximation or truncation. Third, as
far as we know, no existing methods support the lossless merging of adaptation
while adjusting all quantized weights. To address these challenges, we
introduce lossless ternary adaptation for quantization-aware fine-tuning
(LoTA-QAF). This is a novel fine-tuning method specifically designed for
quantized LLMs, enabling the lossless merging of ternary adaptation weights
into quantized weights and the adjustment of all quantized weights. LoTA-QAF
operates through a combination of: i) A custom-designed ternary adaptation (TA)
that aligns ternary weights with the quantization grid and uses these ternary
weights to adjust quantized weights. ii) A TA-based mechanism that enables the
lossless merging of adaptation weights. iii) Ternary signed gradient descent
(t-SignSGD) for updating the TA weights. We apply LoTA-QAF to Llama-3.1/3.3 and
Qwen-2.5 model families and validate its effectiveness on several downstream
tasks. On the MMLU benchmark, our method effectively recovers performance for
quantized models, surpassing 16-bit LoRA by up to 5.14\%. For task-specific
fine-tuning, 16-bit LoRA achieves superior results, but LoTA-QAF still
outperforms other methods.

</details>


### [466] [Message-Passing State-Space Models: Improving Graph Learning with Modern Sequence Modeling](https://arxiv.org/abs/2505.18728)
*Andrea Ceni,Alessio Gravina,Claudio Gallicchio,Davide Bacciu,Carola-Bibiane Schonlieb,Moshe Eliasof*

Main category: cs.LG

TL;DR: 该论文提出了一种新的图状态空间模型MP-SSM，通过将现代SSM计算的关键原则嵌入消息传递神经网络框架，解决了现有GSSM在置换等变性、消息传递兼容性和计算效率上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的图状态空间模型（GSSMs）在应用于图学习时，往往通过从图中提取序列来操作SSM模块，这可能会损害置换等变性、消息传递兼容性和计算效率等核心特性。

Method: 论文提出MP-SSM方法，将现代SSM计算的关键原则直接嵌入消息传递神经网络框架，实现了静态和时序图的统一方法。

Result: MP-SSM在节点分类、图属性预测、长距离基准测试和时空预测等任务中表现出色，验证了其多功能性和强大的实证性能。

Conclusion: MP-SSM不仅保留了消息传递的架构简洁性，还实现了高效、置换等变和长距离信息传播，并通过理论分析信息流动和评估深层问题。

Abstract: The recent success of State-Space Models (SSMs) in sequence modeling has
motivated their adaptation to graph learning, giving rise to Graph State-Space
Models (GSSMs). However, existing GSSMs operate by applying SSM modules to
sequences extracted from graphs, often compromising core properties such as
permutation equivariance, message-passing compatibility, and computational
efficiency. In this paper, we introduce a new perspective by embedding the key
principles of modern SSM computation directly into the Message-Passing Neural
Network framework, resulting in a unified methodology for both static and
temporal graphs. Our approach, MP-SSM, enables efficient,
permutation-equivariant, and long-range information propagation while
preserving the architectural simplicity of message passing. Crucially, MP-SSM
enables an exact sensitivity analysis, which we use to theoretically
characterize information flow and evaluate issues like vanishing gradients and
over-squashing in the deep regime. Furthermore, our design choices allow for a
highly optimized parallel implementation akin to modern SSMs. We validate
MP-SSM across a wide range of tasks, including node classification, graph
property prediction, long-range benchmarks, and spatiotemporal forecasting,
demonstrating both its versatility and strong empirical performance.

</details>


### [467] [Reward-Driven Interaction: Enhancing Proactive Dialogue Agents through User Satisfaction Prediction](https://arxiv.org/abs/2505.18731)
*Wei Shen,Xiaonan He,Chuheng Zhang,Xuyun Zhang,Xiaolong Xu,Wanchun Dou*

Main category: cs.LG

TL;DR: 论文提出通过对比自监督学习和领域意图分类任务改进用户满意度预测，解决传统方法在噪声奖励监督和长尾反馈稀疏性上的问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于弱标签训练的方法在工业对话系统中存在噪声奖励监督和长尾反馈稀疏性问题，导致模型难以准确预测用户满意度。

Method: 提出两种辅助任务：对比自监督学习任务（学习罕见用户语句表示和识别ASR错误）和领域意图分类任务（改善长尾领域性能）。

Result: 在DuerOS上验证，显著提升了罕见语句和长尾领域的错误识别准确率。

Conclusion: 辅助任务有效增强了用户语句和会话的表示学习，提升了满意度预测模型的鲁棒性。

Abstract: Reward-driven proactive dialogue agents require precise estimation of user
satisfaction as an intrinsic reward signal to determine optimal interaction
strategies. Specifically, this framework triggers clarification questions when
detecting potential user dissatisfaction during interactions in the industrial
dialogue system. Traditional works typically rely on training a neural network
model based on weak labels which are generated by a simple model trained on
user actions after current turn. However, existing methods suffer from two
critical limitations in real-world scenarios: (1) Noisy Reward Supervision,
dependence on weak labels derived from post-hoc user actions introduces bias,
particularly failing to capture satisfaction signals in ASR-error-induced
utterances; (2) Long-Tail Feedback Sparsity, the power-law distribution of user
queries causes reward prediction accuracy to drop in low-frequency domains. The
noise in the weak labels and a power-law distribution of user utterances
results in that the model is hard to learn good representation of user
utterances and sessions. To address these limitations, we propose two auxiliary
tasks to improve the representation learning of user utterances and sessions
that enhance user satisfaction prediction. The first one is a contrastive
self-supervised learning task, which helps the model learn the representation
of rare user utterances and identify ASR errors. The second one is a
domain-intent classification task, which aids the model in learning the
representation of user sessions from long-tailed domains and improving the
model's performance on such domains. The proposed method is evaluated on
DuerOS, demonstrating significant improvements in the accuracy of error
recognition on rare user utterances and long-tailed domains.

</details>


### [468] [AuroRA: Breaking Low-Rank Bottleneck of LoRA with Nonlinear Mapping](https://arxiv.org/abs/2505.18738)
*Haonan Dong,Wenhao Zhu,Guojie Song,Liang Wang*

Main category: cs.LG

TL;DR: AuroRA提出了一种改进LoRA的方法，通过引入自适应非线性层来提升模型表现，显著减少参数量的同时性能优于现有PEFT方法。


<details>
  <summary>Details</summary>
Motivation: LoRA在参数高效微调中面临低秩瓶颈问题，增加秩会导致参数过多，而现有线性变体无法从根本上提升表达能力。

Method: AuroRA在两层线性投影之间加入自适应非线性层(ANL)，形成类似MLP的低秩结构，理论上保证更低的逼近误差和有界梯度。

Result: 在22个数据集和6个预训练模型上的实验表明，AuroRA仅用6.18%~25%的LoRA参量就能达到或超越全微调性能，在NLP和CV任务中比SOTA方法提升高达10.88%。

Conclusion: AuroRA通过非线性增强有效突破了LoRA的低秩限制，在保持参数高效的同时实现了更优的微调性能。

Abstract: Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient
fine-tuning (PEFT) method validated across NLP and CV domains. However, LoRA
faces an inherent low-rank bottleneck: narrowing its performance gap with full
finetuning requires increasing the rank of its parameter matrix, resulting in
significant parameter overhead. Recent linear LoRA variants have attempted to
enhance expressiveness by introducing additional linear mappings; however,
their composition remains inherently linear and fails to fundamentally improve
LoRA's representational capacity. To address this limitation, we propose
AuroRA, which incorporates an Adaptive Nonlinear Layer (ANL) between two linear
projectors to capture fixed and learnable nonlinearities. This combination
forms an MLP-like structure with a compressed rank, enabling flexible and
precise approximation of diverse target functions while theoretically
guaranteeing lower approximation errors and bounded gradients. Extensive
experiments on 22 datasets and 6 pretrained models demonstrate that AuroRA: (I)
not only matches or surpasses full fine-tuning performance with only 6.18% ~
25% of LoRA's parameters but also (II) outperforms state-of-the-art PEFT
methods by up to 10.88% in both NLP and CV tasks, and (III) exhibits robust
performance across various rank configurations.

</details>


### [469] [Smart Energy Guardian: A Hybrid Deep Learning Model for Detecting Fraudulent PV Generation](https://arxiv.org/abs/2505.18755)
*Xiaolu Chen,Chenghao Huang,Yanru Zhang,Hao Wang*

Main category: cs.LG

TL;DR: 提出一种结合多尺度CNN、LSTM和Transformer的混合深度学习模型，用于检测住宅光伏发电中的复杂电力盗窃行为，提升智能城市能源系统稳定性。


<details>
  <summary>Details</summary>
Motivation: 智能电网普及下，住宅光伏系统面临日益复杂的网络攻击和电力盗窃问题，传统检测方法难以捕捉时间依赖性和多源数据融合。

Method: 采用混合深度学习模型（多尺度CNN+LSTM+Transformer）及数据嵌入技术，整合时序数据与离散温度变量。

Result: 真实数据实验表明，该方法显著提升了复杂电力盗窃行为的检测精度。

Conclusion: 该方法有效保障智能城市能源供需平衡，增强系统公平性与稳定性。

Abstract: With the proliferation of smart grids, smart cities face growing challenges
due to cyber-attacks and sophisticated electricity theft behaviors,
particularly in residential photovoltaic (PV) generation systems. Traditional
Electricity Theft Detection (ETD) methods often struggle to capture complex
temporal dependencies and integrating multi-source data, limiting their
effectiveness. In this work, we propose an efficient ETD method that accurately
identifies fraudulent behaviors in residential PV generation, thus ensuring the
supply-demand balance in smart cities. Our hybrid deep learning model,
combining multi-scale Convolutional Neural Network (CNN), Long Short-Term
Memory (LSTM), and Transformer, excels in capturing both short-term and
long-term temporal dependencies. Additionally, we introduce a data embedding
technique that seamlessly integrates time-series data with discrete temperature
variables, enhancing detection robustness. Extensive simulation experiments
using real-world data validate the effectiveness of our approach, demonstrating
significant improvements in the accuracy of detecting sophisticated energy
theft activities, thereby contributing to the stability and fairness of energy
systems in smart cities.

</details>


### [470] [Reducing Storage of Pretrained Neural Networks by Rate-Constrained Quantization and Entropy Coding](https://arxiv.org/abs/2505.18758)
*Alexander Conzelmann,Robert Bamler*

Main category: cs.LG

TL;DR: 提出一种结合量化与熵编码的神经网络后训练压缩框架，在保持性能的同时降低比特率。


<details>
  <summary>Details</summary>
Motivation: 神经网络规模增长对资源受限设备带来挑战，需压缩模型同时保持性能。

Method: 扩展分层损失函数为二次率估计，并基于OBS方法提供局部精确解。

Result: 在多种视觉网络上验证，比特率降低20-40%，性能与NNCodec相当。

Conclusion: 该方法支持快速解码和任意量化网格，代码已开源。

Abstract: The ever-growing size of neural networks poses serious challenges on
resource-constrained devices, such as embedded sensors. Compression algorithms
that reduce their size can mitigate these problems, provided that model
performance stays close to the original. We propose a novel post-training
compression framework that combines rate-aware quantization with entropy coding
by (1) extending the well-known layer-wise loss by a quadratic rate estimation,
and (2) providing locally exact solutions to this modified objective following
the Optimal Brain Surgeon (OBS) method. Our method allows for very fast
decoding and is compatible with arbitrary quantization grids. We verify our
results empirically by testing on various computer-vision networks, achieving a
20-40\% decrease in bit rate at the same performance as the popular compression
algorithm NNCodec. Our code is available at https://github.com/Conzel/cerwu.

</details>


### [471] [GenPO: Generative Diffusion Models Meet On-Policy Reinforcement Learning](https://arxiv.org/abs/2505.18763)
*Shutong Ding,Ke Hu,Shan Zhong,Haoyang Luo,Weinan Zhang,Jingya Wang,Jun Wang,Ye Shi*

Main category: cs.LG

TL;DR: 本文提出GenPO框架，首次将扩散策略成功整合到PPO等在线强化学习算法中，解决了扩散策略在状态-动作对数似然计算上的难题，并在多个机器人任务中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散策略在离线强化学习中表现出色，但其与在线强化学习框架（如PPO）的整合仍未被充分探索。当前大规模并行GPU加速模拟器（如IsaacLab）主要优化在线强化学习算法，因此填补这一空白具有重要意义。

Method: GenPO框架利用精确的扩散反转构建可逆动作映射，引入双重虚拟动作机制实现交替更新的可逆性，并利用动作对数似然进行无偏熵和KL散度估计，从而实现KL自适应学习率和熵正则化。

Result: 在八个IsaacLab基准测试（包括腿式运动、灵巧操作、空中控制和机械臂任务）中，GenPO表现优于现有强化学习基线方法。

Conclusion: GenPO是首个成功将扩散策略整合到在线强化学习的方法，释放了其在大规模并行化训练和实际机器人部署中的潜力。

Abstract: Recent advances in reinforcement learning (RL) have demonstrated the powerful
exploration capabilities and multimodality of generative diffusion-based
policies. While substantial progress has been made in offline RL and off-policy
RL settings, integrating diffusion policies into on-policy frameworks like PPO
remains underexplored. This gap is particularly significant given the
widespread use of large-scale parallel GPU-accelerated simulators, such as
IsaacLab, which are optimized for on-policy RL algorithms and enable rapid
training of complex robotic tasks. A key challenge lies in computing
state-action log-likelihoods under diffusion policies, which is straightforward
for Gaussian policies but intractable for flow-based models due to irreversible
forward-reverse processes and discretization errors (e.g., Euler-Maruyama
approximations). To bridge this gap, we propose GenPO, a generative policy
optimization framework that leverages exact diffusion inversion to construct
invertible action mappings. GenPO introduces a novel doubled dummy action
mechanism that enables invertibility via alternating updates, resolving
log-likelihood computation barriers. Furthermore, we also use the action
log-likelihood for unbiased entropy and KL divergence estimation, enabling
KL-adaptive learning rates and entropy regularization in on-policy updates.
Extensive experiments on eight IsaacLab benchmarks, including legged locomotion
(Ant, Humanoid, Anymal-D, Unitree H1, Go2), dexterous manipulation (Shadow
Hand), aerial control (Quadcopter), and robotic arm tasks (Franka), demonstrate
GenPO's superiority over existing RL baselines. Notably, GenPO is the first
method to successfully integrate diffusion policies into on-policy RL,
unlocking their potential for large-scale parallelized training and real-world
robotic deployment.

</details>


### [472] [Multiple Wasserstein Gradient Descent Algorithm for Multi-Objective Distributional Optimization](https://arxiv.org/abs/2505.18765)
*Dai Hai Nguyen,Hiroshi Mamitsuka,Atsuyoshi Nakamura*

Main category: cs.LG

TL;DR: 提出一种名为MWGraD的粒子优化算法，用于同时最小化多个目标函数分布优化问题。


<details>
  <summary>Details</summary>
Motivation: 多目标分布优化在机器学习与统计领域有广泛应用，如多目标采样、多任务学习和生成建模，需要高效求解方法。

Method: 基于Wasserstein梯度下降的迭代粒子算法，动态聚合多目标梯度并更新粒子分布。

Result: 理论分析和实验表明，MWGraD在合成与真实数据集上均有效。

Conclusion: MWGraD为多目标分布优化问题提供了可行的解决方案。

Abstract: We address the optimization problem of simultaneously minimizing multiple
objective functionals over a family of probability distributions. This type of
Multi-Objective Distributional Optimization commonly arises in machine learning
and statistics, with applications in areas such as multiple target sampling,
multi-task learning, and multi-objective generative modeling. To solve this
problem, we propose an iterative particle-based algorithm, which we call
Muliple Wasserstein Gradient Descent (MWGraD), which constructs a flow of
intermediate empirical distributions, each being represented by a set of
particles, which gradually minimize the multiple objective functionals
simultaneously. Specifically, MWGraD consists of two key steps at each
iteration. First, it estimates the Wasserstein gradient for each objective
functional based on the current particles. Then, it aggregates these gradients
into a single Wasserstein gradient using dynamically adjusted weights and
updates the particles accordingly. In addition, we provide theoretical analysis
and present experimental results on both synthetic and real-world datasets,
demonstrating the effectiveness of MWGraD.

</details>


### [473] [HD-PiSSA: High-Rank Distributed Orthogonal Adaptation](https://arxiv.org/abs/2505.18777)
*Yiding Wang,Fauxu meng,Xuefeng Zhang,Fan Jiang,Pingzhi Tang,Muhan Zhang*

Main category: cs.LG

TL;DR: 提出HD-PiSSA方法，通过分布式正交适配器提升大语言模型微调性能。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法（如LoRA和PiSSA）受限于低秩子空间更新，导致复杂任务表现不佳。

Method: HD-PiSSA在不同设备初始化正交适配器，聚合增量更新以扩展优化方向。

Result: 在8GPU上实现16倍有效秩提升，多任务学习平均提升10%优于LoRA。

Conclusion: HD-PiSSA通过分布式高秩更新显著提升模型微调灵活性和性能。

Abstract: Existing parameter-efficient fine-tuning (PEFT) methods for large language
models (LLMs), such as LoRA and PiSSA, constrain model updates to low-rank
subspaces, limiting their expressiveness and leading to suboptimal performance
on complex tasks. To address this, we introduce High-rank Distributed PiSSA
(HD-PiSSA), a distributed PEFT approach that initializes orthogonal adapters
across different devices and aggregates their delta updates collectively on W
for fine-tuning. Unlike Data Parallel LoRA or PiSSA, which maintain identical
adapters across all devices, HD-PiSSA assigns different principal components of
the pre-trained weights to each GPU, significantly expanding the range of
update directions. This results in over 16x higher effective updated ranks than
data-parallel LoRA or PiSSA when fine-tuning on 8 GPUs with the same per-device
adapter rank. Empirically, we evaluate HD-PiSSA across various challenging
downstream tasks, including mathematics, code generation, and multi-task
learning. In the multi-task setting, HD-PiSSA achieves average gains of 10.0
absolute points (14.63%) over LoRA and 4.98 points (6.60%) over PiSSA across 12
benchmarks, demonstrating its benefits from the extra optimization flexibility.

</details>


### [474] [Geometry Aware Operator Transformer as an Efficient and Accurate Neural Surrogate for PDEs on Arbitrary Domains](https://arxiv.org/abs/2505.18781)
*Shizheng Wen,Arsh Kumbhat,Levi Lingsch,Sepehr Mousavi,Praveen Chandrashekar,Siddhartha Mishra*

Main category: cs.LG

TL;DR: 提出了一种几何感知的算子变换器（GAOT），用于在任意域上高效准确地学习PDE的解算子。


<details>
  <summary>Details</summary>
Motivation: 当前学习PDE解算子的方法在准确性和计算效率之间存在权衡，亟需一种能同时兼顾两者的解决方案。

Method: GAOT结合了多尺度注意力图神经算子编码器/解码器、几何嵌入和视觉变换器处理器，以高效映射域和输入信息到PDE解的鲁棒近似。

Result: GAOT在多个PDE学习任务中显著提升了准确性和效率，并在大规模三维工业CFD数据集上达到最先进性能。

Conclusion: GAOT通过创新架构实现了PDE解算子的高效准确学习，为工程和工业仿真提供了有力工具。

Abstract: The very challenging task of learning solution operators of PDEs on arbitrary
domains accurately and efficiently is of vital importance to engineering and
industrial simulations. Despite the existence of many operator learning
algorithms to approximate such PDEs, we find that accurate models are not
necessarily computationally efficient and vice versa. We address this issue by
proposing a geometry aware operator transformer (GAOT) for learning PDEs on
arbitrary domains. GAOT combines novel multiscale attentional graph neural
operator encoders and decoders, together with geometry embeddings and (vision)
transformer processors to accurately map information about the domain and the
inputs into a robust approximation of the PDE solution. Multiple innovations in
the implementation of GAOT also ensure computational efficiency and
scalability. We demonstrate this significant gain in both accuracy and
efficiency of GAOT over several baselines on a large number of learning tasks
from a diverse set of PDEs, including achieving state of the art performance on
a large scale three-dimensional industrial CFD dataset.

</details>


### [475] [Soft Weighted Machine Unlearning](https://arxiv.org/abs/2505.18783)
*Xinbao Qiao,Ningning Ding,Yushi Cheng,Meng Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一种软加权框架，通过加权影响函数解决机器去学习中的过度去学习问题，显著提升了公平性和鲁棒性指标。


<details>
  <summary>Details</summary>
Motivation: 现有的非隐私去学习方法仍采用为隐私驱动的二进制数据移除框架，导致信息过度丢失（即过度去学习），进而影响模型性能。本文旨在探究其根本原因并提出解决方案。

Method: 通过反事实留一分析，提出加权影响函数，为每个样本分配定制权重，并基于此构建软加权框架，实现细粒度模型调整。

Result: 实验表明，在公平性和鲁棒性任务中，软加权方案显著优于硬加权方案，缓解了效用指标的下降。

Conclusion: 软加权方案具有通用性，可集成到现有去学习算法中，有效解决过度去学习问题，提升模型性能。

Abstract: Machine unlearning, as a post-hoc processing technique, has gained widespread
adoption in addressing challenges like bias mitigation and robustness
enhancement, colloquially, machine unlearning for fairness and robustness.
However, existing non-privacy unlearning-based solutions persist in using
binary data removal framework designed for privacy-driven motivation, leading
to significant information loss, a phenomenon known as over-unlearning. While
over-unlearning has been largely described in many studies as primarily causing
utility degradation, we investigate its fundamental causes and provide deeper
insights in this work through counterfactual leave-one-out analysis. In this
paper, we introduce a weighted influence function that assigns tailored weights
to each sample by solving a convex quadratic programming problem analytically.
Building on this, we propose a soft-weighted framework enabling fine-grained
model adjustments to address the over-unlearning challenge. We demonstrate that
the proposed soft-weighted scheme is versatile and can be seamlessly integrated
into most existing unlearning algorithms. Extensive experiments show that in
fairness- and robustness-driven tasks, the soft-weighted scheme significantly
outperforms hard-weighted schemes in fairness/robustness metrics and alleviates
the decline in utility metric, thereby enhancing machine unlearning algorithm
as an effective correction solution.

</details>


### [476] [Leveraging Per-Instance Privacy for Machine Unlearning](https://arxiv.org/abs/2505.18786)
*Nazanin Mohammadi Sepahvand,Anvith Thudi,Berivan Isik,Ashmita Bhattacharyya,Nicolas Papernot,Eleni Triantafillou,Daniel M. Roy,Gintare Karolina Dziugaite*

Main category: cs.LG

TL;DR: 该论文提出了一种基于实例的量化方法，用于评估通过微调进行遗忘学习的难度，改进了隐私损失边界，并展示了理论与实验结果的一致性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于改进现有的遗忘学习（unlearning）方法，通过量化每个数据实例的遗忘难度，以提供更高效和自适应的遗忘策略。

Method: 论文通过分析噪声梯度下降（noisy gradient descent）进行遗忘学习，提出了一种基于实例的隐私损失边界（per-instance privacy losses），替代了传统的全局隐私损失边界。实验验证了该方法在随机梯度Langevin动力学（SGLD）和标准微调中的有效性。

Result: 实验结果表明，基于实例的隐私损失边界与现有数据难度指标高度相关，并能识别更困难的数据点组。此外，论文还引入了基于损失屏障（loss barriers）的新评估方法。

Conclusion: 论文为针对单个数据点特性的高效自适应遗忘策略奠定了基础，展示了理论与实际应用的一致性。

Abstract: We present a principled, per-instance approach to quantifying the difficulty
of unlearning via fine-tuning. We begin by sharpening an analysis of noisy
gradient descent for unlearning (Chien et al., 2024), obtaining a better
utility-unlearning tradeoff by replacing worst-case privacy loss bounds with
per-instance privacy losses (Thudi et al., 2024), each of which bounds the
(Renyi) divergence to retraining without an individual data point. To
demonstrate the practical applicability of our theory, we present empirical
results showing that our theoretical predictions are born out both for
Stochastic Gradient Langevin Dynamics (SGLD) as well as for standard
fine-tuning without explicit noise. We further demonstrate that per-instance
privacy losses correlate well with several existing data difficulty metrics,
while also identifying harder groups of data points, and introduce novel
evaluation methods based on loss barriers. All together, our findings provide a
foundation for more efficient and adaptive unlearning strategies tailored to
the unique properties of individual data points.

</details>


### [477] [Governing Equation Discovery from Data Based on Differential Invariants](https://arxiv.org/abs/2505.18798)
*Lexiang Hu,Yikang Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 提出基于微分不变量的PDE发现方法DI-SINDy，利用对称性先验知识无损缩减搜索空间，提高成功率和精度。


<details>
  <summary>Details</summary>
Motivation: 直接从数据中发现偏微分方程面临搜索空间庞大的挑战，而对称性作为科学领域的重要先验知识尚未被充分用于指导方程发现。

Method: 通过计算对称群无穷小生成元对应的微分不变量集合，将其作为方程发现的相关项，构建DI-SINDy方法。

Result: DI-SINDy在一系列PDE上的发现成功率和精度超过其他基于对称性的方程发现方法。

Conclusion: 基于微分不变量的方法能严格遵循对称性并有效缩减搜索空间，为PDE发现提供了新思路。

Abstract: The explicit governing equation is one of the simplest and most intuitive
forms for characterizing physical laws. However, directly discovering partial
differential equations (PDEs) from data poses significant challenges, primarily
in determining relevant terms from a vast search space. Symmetry, as a crucial
prior knowledge in scientific fields, has been widely applied in tasks such as
designing equivariant networks and guiding neural PDE solvers. In this paper,
we propose a pipeline for governing equation discovery based on differential
invariants, which can losslessly reduce the search space of existing equation
discovery methods while strictly adhering to symmetry. Specifically, we compute
the set of differential invariants corresponding to the infinitesimal
generators of the symmetry group and select them as the relevant terms for
equation discovery. Taking DI-SINDy (SINDy based on Differential Invariants) as
an example, we demonstrate that its success rate and accuracy in PDE discovery
surpass those of other symmetry-informed governing equation discovery methods
across a series of PDEs.

</details>


### [478] [How to build a consistency model: Learning flow maps via self-distillation](https://arxiv.org/abs/2505.18825)
*Nicholas M. Boffi,Michael S. Albergo,Eric Vanden-Eijnden*

Main category: cs.LG

TL;DR: 本文提出了一种系统性方法，通过自蒸馏学习流映射，无需预训练模型，适用于不同维度的任务。


<details>
  <summary>Details</summary>
Motivation: 基于Boffi等人(2024)的框架，旨在提高基于微分方程的生成模型效率，特别是流映射模型（一致性模型）。

Method: 利用连续时间流的速度场与流映射瞬时变化率之间的关系，将现有蒸馏方案转化为自蒸馏的直接训练算法。

Result: 实验表明，高维任务（如图像合成）适合避免时空导数的目标函数，而低维任务则可通过高阶导数捕捉锐利特征。

Conclusion: 该方法有效提升了生成模型的效率，且适用于不同维度的任务需求。

Abstract: Building on the framework proposed in Boffi et al. (2024), we present a
systematic approach for learning flow maps associated with flow and diffusion
models. Flow map-based models, commonly known as consistency models, encompass
recent efforts to improve the efficiency of generative models based on
solutions to differential equations. By exploiting a relationship between the
velocity field underlying a continuous-time flow and the instantaneous rate of
change of the flow map, we show how to convert existing distillation schemes
into direct training algorithms via self-distillation, eliminating the need for
pre-trained models. We empirically evaluate several instantiations of our
framework, finding that high-dimensional tasks like image synthesis benefit
from objective functions that avoid temporal and spatial derivatives of the
flow map, while lower-dimensional tasks can benefit from objectives
incorporating higher-order derivatives to capture sharp features.

</details>


### [479] [Improved Regret and Contextual Linear Extension for Pandora's Box and Prophet Inequality](https://arxiv.org/abs/2505.18828)
*Junyan Liu,Ziyun Chen,Kun Wang,Haipeng Luo,Lillian J. Ratliff*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the Pandora's Box problem in an online learning setting with
semi-bandit feedback. In each round, the learner sequentially pays to open up
to $n$ boxes with unknown reward distributions, observes rewards upon opening,
and decides when to stop. The utility of the learner is the maximum observed
reward minus the cumulative cost of opened boxes, and the goal is to minimize
regret defined as the gap between the cumulative expected utility and that of
the optimal policy. We propose a new algorithm that achieves
$\widetilde{O}(\sqrt{nT})$ regret after $T$ rounds, which improves the
$\widetilde{O}(n\sqrt{T})$ bound of Agarwal et al. [2024] and matches the known
lower bound up to logarithmic factors. To better capture real-life
applications, we then extend our results to a natural but challenging
contextual linear setting, where each box's expected reward is linear in some
known but time-varying $d$-dimensional context and the noise distribution is
fixed over time. We design an algorithm that learns both the linear function
and the noise distributions, achieving $\widetilde{O}(nd\sqrt{T})$ regret.
Finally, we show that our techniques also apply to the online Prophet
Inequality problem, where the learner must decide immediately whether or not to
accept a revealed reward. In both non-contextual and contextual settings, our
approach achieves similar improvements and regret bounds.

</details>


### [480] [On the Effect of Negative Gradient in Group Relative Deep Reinforcement Optimization](https://arxiv.org/abs/2505.18830)
*Wenlong Deng,Yi Ren,Muchen Li,Danica J. Sutherland,Xiaoxiao Li,Christos Thrampoulidis*

Main category: cs.LG

TL;DR: 论文发现GRPO算法在训练中会导致正确回答的似然性不升反降（LLD现象），并提出NTHR方法通过调整错误回答的惩罚权重来缓解该问题。


<details>
  <summary>Details</summary>
Motivation: 尽管GRPO算法被广泛用于增强大语言模型的推理能力，但其训练过程中存在未被发现的LLD现象，即正确回答的似然性可能下降，这类似于DPO中的错位问题。

Method: 提出NTHR方法，利用GRPO的组结构，以正确回答为锚点识别关键token，并降低对LLD有贡献的token的惩罚权重。

Result: 在数学推理基准测试中，NTHR有效缓解了LLD现象，在0.5B到3B参数的模型上均取得了稳定的性能提升。

Conclusion: NTHR通过针对性调整惩罚机制，解决了GRPO中的LLD问题，为强化学习优化语言模型提供了新思路。

Abstract: Reinforcement learning (RL) has become popular in enhancing the reasoning
capabilities of large language models (LLMs), with Group Relative Policy
Optimization (GRPO) emerging as a widely used algorithm in recent systems.
Despite GRPO's widespread adoption, we identify a previously unrecognized
phenomenon we term Lazy Likelihood Displacement (LLD), wherein the likelihood
of correct responses marginally increases or even decreases during training.
This behavior mirrors a recently discovered misalignment issue in Direct
Preference Optimization (DPO), attributed to the influence of negative
gradients. We provide a theoretical analysis of GRPO's learning dynamic,
identifying the source of LLD as the naive penalization of all tokens in
incorrect responses with the same strength. To address this, we develop a
method called NTHR, which downweights penalties on tokens contributing to the
LLD. Unlike prior DPO-based approaches, NTHR takes advantage of GRPO's
group-based structure, using correct responses as anchors to identify
influential tokens. Experiments on math reasoning benchmarks demonstrate that
NTHR effectively mitigates LLD, yielding consistent performance gains across
models ranging from 0.5B to 3B parameters.

</details>


### [481] [Distribution-Aware Mobility-Assisted Decentralized Federated Learning](https://arxiv.org/abs/2505.18866)
*Md Farhamdur Reza,Reza Jahani,Richeng Jin,Huaiyu Dai*

Main category: cs.LG

TL;DR: 论文研究表明，在去中心化联邦学习中引入少量移动客户端可显著提升模型精度，并提出基于数据分布感知的移动策略以进一步优化性能。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化联邦学习（DFL）研究未充分探索用户移动性对性能的影响，而移动性可能促进通信和模型收敛。

Method: 提出两种方法：1) 引入随机移动的客户端；2) 设计基于数据分布和静态客户端位置的策略性移动模式。

Result: 实验证明移动性可改善信息流动，降低数据异构性影响，且策略性移动优于随机移动。

Conclusion: 客户端移动性可有效提升DFL性能，数据分布感知的移动策略具有显著优势。

Abstract: Decentralized federated learning (DFL) has attracted significant attention
due to its scalability and independence from a central server. In practice,
some participating clients can be mobile, yet the impact of user mobility on
DFL performance remains largely unexplored, despite its potential to facilitate
communication and model convergence. In this work, we demonstrate that
introducing a small fraction of mobile clients, even with random movement, can
significantly improve the accuracy of DFL by facilitating information flow. To
further enhance performance, we propose novel distribution-aware mobility
patterns, where mobile clients strategically navigate the network, leveraging
knowledge of data distributions and static client locations. The proposed
moving strategies mitigate the impact of data heterogeneity and boost learning
convergence. Extensive experiments validate the effectiveness of induced
mobility in DFL and demonstrate the superiority of our proposed mobility
patterns over random movement.

</details>


### [482] [RefLoRA: Refactored Low-Rank Adaptation for Efficient Fine-Tuning of Large Models](https://arxiv.org/abs/2505.18877)
*Yilang Zhang,Bingcong Li,Georgios B. Giannakis*

Main category: cs.LG

TL;DR: RefLoRA改进LoRA方法，通过优化低秩分解提升大模型微调效率，实现更快收敛和更好性能。


<details>
  <summary>Details</summary>
Motivation: LoRA在微调大模型时虽高效，但因低秩分解不唯一导致权重更新不一致和性能下降，需改进。

Method: 提出RefLoRA，每步优化低秩分解以最小化损失上界，确保权重更新一致且平衡。

Result: 实验显示RefLoRA在多种任务和模型上收敛更快、性能更优，计算开销几乎不变。

Conclusion: RefLoRA通过优化低秩分解解决了LoRA的局限性，显著提升了微调效率和模型性能。

Abstract: Low-Rank Adaptation (LoRA) lowers the computational and memory overhead of
fine-tuning large models by updating a low-dimensional subspace of the
pre-trained weight matrix. Albeit efficient, LoRA exhibits suboptimal
convergence and noticeable performance degradation, due to inconsistent and
imbalanced weight updates induced by its nonunique low-rank factorizations. To
overcome these limitations, this article identifies the optimal low-rank
factorization per step that minimizes an upper bound on the loss. The resultant
refactored low-rank adaptation (RefLoRA) method promotes a flatter loss
landscape, along with consistent and balanced weight updates, thus speeding up
stable convergence. Extensive experiments evaluate RefLoRA on natural language
understanding, and commonsense reasoning tasks with popular large language
models including DeBERTaV3, LLaMA-7B, LLaMA2-7B and LLaMA3-8B. The numerical
tests corroborate that RefLoRA converges faster, outperforms various
benchmarks, and enjoys negligible computational overhead compared to
state-of-the-art LoRA variants.

</details>


### [483] [Partition Generative Modeling: Masked Modeling Without Masks](https://arxiv.org/abs/2505.18883)
*Justin Deschenaux,Lan Tran,Caglar Gulcehre*

Main category: cs.LG

TL;DR: PGMs是一种新型的掩码生成建模方法，通过分区策略和稀疏注意力模式提高计算效率，无需使用MASK标记，在生成速度和生成质量上均优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统掩码生成建模方法在处理MASK标记时效率低下，PGMs旨在通过分区策略消除MASK标记的使用，从而提高计算效率和生成质量。

Method: PGMs将标记分为两组，采用稀疏注意力模式防止跨组信息交换，仅基于一组信息预测另一组标记，无需MASK标记。

Result: 在OpenWebText上的实验表明，PGMs在相同采样步骤下，延迟和吞吐量比MDLM提高至少5倍，且生成样本的困惑度更低。

Conclusion: PGMs不仅显著提升了生成效率和质量，还能通过SDTT方法进一步优化推理性能。

Abstract: We introduce ``Partition Generative Models'' (PGMs), a novel approach to
masked generative modeling (MGMs), particularly effective for masked diffusion
language modeling (MDLMs). PGM divides tokens into two distinct groups and
employs sparse attention patterns to prevent cross-group information exchange.
Hence, the model is trained to predict tokens in one group based solely on
information from the other group. This partitioning strategy eliminates the
need for MASK tokens entirely. While traditional MGMs inefficiently process
MASK tokens during generation, PGMs achieve greater computational efficiency by
operating exclusively on unmasked tokens. Our experiments on OpenWebText with a
context length of 1024 tokens demonstrate that PGMs deliver at least 5x
improvements in both latency and throughput compared to MDLM when using the
same number of sampling steps, while generating samples with better generative
perplexity than MDLM. Finally, we show that PGMs can be distilled with
Self-Distillation Through Time (SDTT), a method originally devised for MDLM, in
order to achieve further inference gains.

</details>


### [484] [LORE: Lagrangian-Optimized Robust Embeddings for Visual Encoders](https://arxiv.org/abs/2505.18884)
*Borna Khodabandeh,Amirabbas Afzali,Amirhossein Afsharrad,Seyed Shahabeddin Mousavi,Sanjay Lall,Sajjad Amini,Seyed-Mohsen Moosavi-Dezfooli*

Main category: cs.LG

TL;DR: 论文提出LORE框架，通过约束优化解决对抗微调中的不稳定性和鲁棒性-准确性权衡问题，显著提升零样本对抗鲁棒性且保持干净数据性能。


<details>
  <summary>Details</summary>
Motivation: 现有对抗微调方法存在两个关键问题：(1) 早期训练不稳定导致收敛次优和干净数据性能下降；(2) 鲁棒性与准确性难以同时优化。需要一种能平衡两者的新方法。

Method: 提出LORE框架——基于拉格朗日优化的无监督对抗微调方法，通过嵌入空间邻近约束实现对抗鲁棒性和干净数据性能的平衡优化。

Result: 实验表明LORE显著提升零样本对抗鲁棒性（如CLIP编码器），且几乎不影响干净数据准确性，同时提升分布外泛化能力和嵌入可解释性。

Conclusion: LORE为视觉编码器提供了一种稳定有效的对抗微调范式，通过约束优化成功解决了鲁棒性与准确性的权衡问题。

Abstract: Visual encoders have become fundamental components in modern computer vision
pipelines. However, ensuring robustness against adversarial perturbations
remains a critical challenge. Recent efforts have explored both supervised and
unsupervised adversarial fine-tuning strategies. We identify two key
limitations in these approaches: (i) they often suffer from instability,
especially during the early stages of fine-tuning, resulting in suboptimal
convergence and degraded performance on clean data, and (ii) they exhibit a
suboptimal trade-off between robustness and clean data accuracy, hindering the
simultaneous optimization of both objectives. To overcome these challenges, we
propose Lagrangian-Optimized Robust Embeddings (LORE), a novel unsupervised
adversarial fine-tuning framework. LORE utilizes constrained optimization,
which offers a principled approach to balancing competing goals, such as
improving robustness while preserving nominal performance. By enforcing
embedding-space proximity constraints, LORE effectively maintains clean data
performance throughout adversarial fine-tuning. Extensive experiments show that
LORE significantly improves zero-shot adversarial robustness with minimal
degradation in clean data accuracy. Furthermore, we demonstrate the
effectiveness of the adversarially fine-tuned CLIP image encoder in
out-of-distribution generalization and enhancing the interpretability of image
embeddings.

</details>


### [485] [KerZOO: Kernel Function Informed Zeroth-Order Optimization for Accurate and Accelerated LLM Fine-Tuning](https://arxiv.org/abs/2505.18886)
*Zhendong Mi,Qitao Tan,Xiaodong Yu,Zining Zhu,Geng Yuan,Shaoyi Huang*

Main category: cs.LG

TL;DR: 该论文提出了一种基于核函数的零阶优化框架KerZOO，用于减少大语言模型微调中的梯度估计偏差，显著提升收敛速度和性能。


<details>
  <summary>Details</summary>
Motivation: 传统一阶微调方法内存需求高，而零阶优化虽内存高效但存在梯度估计偏差，影响收敛速度。论文旨在解决这一问题。

Method: 通过数学物理工具分析零阶梯度估计中的低阶偏差，提出基于核函数的零阶优化框架KerZOO以减少偏差并提升稳定性。

Result: KerZOO在OPT-2.7B模型微调中，GPU训练时间减少74%（WSC）和44%（MultiRC），准确率分别超过MeZO基线2.9%和2.6%。

Conclusion: 核函数是减少零阶方法估计偏差的有效途径，KerZOO在性能和效率上均优于现有零阶基线。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
numerous NLP tasks. Nevertheless, conventional first-order fine-tuning
techniques impose heavy memory demands, creating practical obstacles to
real-world applications. Zeroth-order (ZO) optimization has recently emerged as
a promising memory-efficient alternative, as it circumvents the need for
backpropagation by estimating gradients solely through forward passes--making
it particularly suitable for resource-limited environments. Despite its
efficiency, ZO optimization suffers from gradient estimation bias, which
significantly hinders convergence speed. To address this, we analytically
identify and characterize the lower-order bias introduced during ZO-based
gradient estimation in LLM fine-tuning. Motivated by tools in mathematical
physics, we introduce a kernel-function-based ZO framework aimed at mitigating
this bias and improving optimization stability. KerZOO achieves comparable or
superior performance to existing ZO baselines in both full-parameter and
parameter-efficient fine-tuning settings of LLMs, while significantly reducing
the number of iterations required to reach convergence. For example, KerZOO
reduces total GPU training hours by as much as 74% and 44% on WSC and MultiRC
datasets in fine-tuning OPT-2.7B model and can exceed the MeZO baseline by 2.9%
and 2.6% in accuracy. We show that the kernel function is an effective avenue
for reducing estimation bias in ZO methods.

</details>


### [486] [Conformal Prediction for Uncertainty Estimation in Drug-Target Interaction Prediction](https://arxiv.org/abs/2505.18890)
*Morteza Rakhshaninejad,Mira Jurgens,Nicolas Dewolf,Willem Waegeman*

Main category: cs.LG

TL;DR: 该论文研究了在药物-靶点相互作用预测中应用聚类条件共形预测方法，以提高预测不确定性的可靠性。


<details>
  <summary>Details</summary>
Motivation: 药物-靶点相互作用预测在药物发现中至关重要，但传统共形预测方法忽视了药物和蛋白质亚组间的变异性，因此需要更可靠的预测不确定性表示方法。

Method: 论文分析了三种基于聚类的条件共形预测方法，分别通过非一致性分数、特征相似性和最近邻进行聚类，并与传统方法和组条件方法进行比较。

Result: 实验表明，基于非一致性的聚类方法在KIBA数据集上产生了最紧凑的区间和最可靠的亚组覆盖，尤其在随机和完全未见的数据划分中表现最佳。

Conclusion: 聚类条件共形预测方法在药物-靶点相互作用预测中具有潜力，特别是在稀疏或新场景下能提供更稳健的不确定性估计。

Abstract: Accurate drug-target interaction (DTI) prediction with machine learning
models is essential for drug discovery. Such models should also provide a
credible representation of their uncertainty, but applying classical marginal
conformal prediction (CP) in DTI prediction often overlooks variability across
drug and protein subgroups. In this work, we analyze three cluster-conditioned
CP methods for DTI prediction, and compare them with marginal and
group-conditioned CP. Clusterings are obtained via nonconformity scores,
feature similarity, and nearest neighbors, respectively. Experiments on the
KIBA dataset using four data-splitting strategies show that nonconformity-based
clustering yields the tightest intervals and most reliable subgroup coverage,
especially in random and fully unseen drug-protein splits. Group-conditioned CP
works well when one entity is familiar, but residual-driven clustering provides
robust uncertainty estimates even in sparse or novel scenarios. These results
highlight the potential of cluster-based CP for improving DTI prediction under
uncertainty.

</details>


### [487] [PromptWise: Online Learning for Cost-Aware Prompt Assignment in Generative Models](https://arxiv.org/abs/2505.18901)
*Xiaoyan Hu,Lauren Pick,Ho-fung Leung,Farzan Farnia*

Main category: cs.LG

TL;DR: PromptWise是一个在线学习框架，旨在以成本效益方式为一系列提示分配大型语言模型，优先查询成本较低的模型，仅在必要时转向更昂贵选项。


<details>
  <summary>Details</summary>
Motivation: 现有生成式AI模型选择方法通常优先考虑性能而忽略定价差异，导致用户可能承担不必要的高成本。

Method: 提出PromptWise框架，采用策略性查询顺序：先尝试低成本模型，仅当其无法满足需求时再转向高价模型。

Result: 实验表明PromptWise在谜题解答、代码生成/翻译等任务中，始终优于不考虑成本的基线方法，且避免直接使用最贵模型带来的高开销。

Conclusion: 成本感知的模型选择策略能显著降低费用，同时维持或提升平均性能，验证了PromptWise框架的有效性。

Abstract: The rapid advancement of generative AI models has provided users with
numerous options to address their prompts. When selecting a generative AI model
for a given prompt, users should consider not only the performance of the
chosen model but also its associated service cost. The principle guiding such
consideration is to select the least expensive model among the available
satisfactory options. However, existing model-selection approaches typically
prioritize performance, overlooking pricing differences between models. In this
paper, we introduce PromptWise, an online learning framework designed to assign
a sequence of prompts to a group of large language models (LLMs) in a
cost-effective manner. PromptWise strategically queries cheaper models first,
progressing to more expensive options only if the lower-cost models fail to
adequately address a given prompt. Through numerical experiments, we
demonstrate PromptWise's effectiveness across various tasks, including puzzles
of varying complexity and code generation/translation tasks. The results
highlight that PromptWise consistently outperforms cost-unaware baseline
methods, emphasizing that directly assigning prompts to the most expensive
models can lead to higher costs and potentially lower average performance.

</details>


### [488] [Behavior Injection: Preparing Language Models for Reinforcement Learning](https://arxiv.org/abs/2505.18917)
*Zhepeng Cen,Yihang Yao,William Han,Zuxin Liu,Ding Zhao*

Main category: cs.LG

TL;DR: 论文分析了强化微调（RFT）对大型语言模型（LLM）推理能力的影响，提出了行为注入方法以提升RFT效果。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决RFT在不同LLM上效果不一致的问题，探索提升模型推理能力的关键条件。

Method: 提出行为注入方法，通过在监督微调（SFT）阶段注入探索和利用行为，增强模型对RFT的适应性。

Result: 实验表明，该方法能显著提升RFT后的模型性能，验证了理论分析的有效性。

Conclusion: 行为注入是一种任务无关的数据增强方法，能有效提升RFT的效果，使模型更适应强化学习。

Abstract: Reinforcement fine-tuning (RFT) has emerged as a powerful post-training
technique to incentivize the reasoning ability of large language models (LLMs).
However, LLMs can respond very inconsistently to RFT: some show substantial
performance gains, while others plateau or even degrade. To understand this
divergence, we analyze the per-step influence of the RL objective and identify
two key conditions for effective post-training: (1) RL-informative rollout
accuracy, and (2) strong data co-influence, which quantifies how much the
training data affects performance on other samples. Guided by these insights,
we propose behavior injection, a task-agnostic data-augmentation scheme applied
prior to RL. Behavior injection enriches the supervised finetuning (SFT) data
by seeding exploratory and exploitative behaviors, effectively making the model
more RL-ready. We evaluate our method across two reasoning benchmarks with
multiple base models. The results demonstrate that our theoretically motivated
augmentation can significantly increases the performance gain from RFT over the
pre-RL model.

</details>


### [489] [Graph-Based Operator Learning from Limited Data on Irregular Domains](https://arxiv.org/abs/2505.18923)
*Yile Li,Shandian Zhe*

Main category: cs.LG

TL;DR: 本文提出了一种基于图的注意力增强算子学习方法（GOLA），用于解决不规则域上的偏微分方程问题，通过图神经网络和傅里叶编码提升模型表达能力，在数据稀缺情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的算子学习方法（如DeepONet和FNO）依赖规则网格离散化，限制了其在不规则或复杂域上的应用。本文旨在克服这一限制，提出一种适用于不规则采样的方法。

Method: GOLA框架通过从不规则空间点构建图，并利用注意力增强的图神经网络（GNN）建模空间依赖关系。引入傅里叶编码器，将输入函数投影到频率空间，使用可学习复数系数实现灵活嵌入。

Result: 在多种2D偏微分方程（如Darcy Flow、Advection等）上测试，GOLA在不同采样密度下均优于基线方法，尤其在数据稀缺情况下表现出强大的泛化能力和效率。

Conclusion: GOLA框架有效解决了不规则域上的算子学习问题，其结合图神经网络和傅里叶编码的方法在稀疏或非均匀采样下仍能保持高性能，为复杂域问题提供了新思路。

Abstract: Operator learning seeks to approximate mappings from input functions to
output solutions, particularly in the context of partial differential equations
(PDEs). While recent advances such as DeepONet and Fourier Neural Operator
(FNO) have demonstrated strong performance, they often rely on regular grid
discretizations, limiting their applicability to complex or irregular domains.
In this work, we propose a Graph-based Operator Learning with Attention (GOLA)
framework that addresses this limitation by constructing graphs from
irregularly sampled spatial points and leveraging attention-enhanced Graph
Neural Netwoks (GNNs) to model spatial dependencies with global information. To
improve the expressive capacity, we introduce a Fourier-based encoder that
projects input functions into a frequency space using learnable complex
coefficients, allowing for flexible embeddings even with sparse or nonuniform
samples. We evaluated our approach across a range of 2D PDEs, including Darcy
Flow, Advection, Eikonal, and Nonlinear Diffusion, under varying sampling
densities. Our method consistently outperforms baselines, particularly in
data-scarce regimes, demonstrating strong generalization and efficiency on
irregular domains.

</details>


### [490] [Hybrid Neural-MPM for Interactive Fluid Simulations in Real-Time](https://arxiv.org/abs/2505.18926)
*Jingxuan Xu,Hong Huang,Chuhang Zou,Manolis Savva,Yunchao Wei,Wuyang Chen*

Main category: cs.LG

TL;DR: 提出一种结合数值模拟、神经物理与生成控制的混合方法，实现高保真低延迟的实时流体交互模拟。


<details>
  <summary>Details</summary>
Motivation: 传统物理模拟方法计算量大且延迟高，现有机器学习方法虽降低计算成本，但仍无法满足实时交互的低延迟需求。

Method: 融合数值求解器与神经物理的混合框架，采用扩散模型生成动态力场实现流体操控，并设置传统求解器作为保真度保障。

Result: 系统在2D/3D场景、多材质及障碍物交互中表现稳健，帧延迟仅11~29%，支持通过手绘草图实时操控流体。

Conclusion: 该研究为实时交互应用提供了兼具可控性与物理合理性的流体模拟解决方案。

Abstract: We propose a neural physics system for real-time, interactive fluid
simulations. Traditional physics-based methods, while accurate, are
computationally intensive and suffer from latency issues. Recent
machine-learning methods reduce computational costs while preserving fidelity;
yet most still fail to satisfy the latency constraints for real-time use and
lack support for interactive applications. To bridge this gap, we introduce a
novel hybrid method that integrates numerical simulation, neural physics, and
generative control. Our neural physics jointly pursues low-latency simulation
and high physical fidelity by employing a fallback safeguard to classical
numerical solvers. Furthermore, we develop a diffusion-based controller that is
trained using a reverse modeling strategy to generate external dynamic force
fields for fluid manipulation. Our system demonstrates robust performance
across diverse 2D/3D scenarios, material types, and obstacle interactions,
achieving real-time simulations at high frame rates (11~29% latency) while
enabling fluid control guided by user-friendly freehand sketches. We present a
significant step towards practical, controllable, and physically plausible
fluid simulations for real-time interactive applications. We promise to release
both models and data upon acceptance.

</details>


### [491] [Chi-Square Wavelet Graph Neural Networks for Heterogeneous Graph Anomaly Detection](https://arxiv.org/abs/2505.18934)
*Xiping Li,Xiangyu Dong,Xingyi Zhang,Kun Xie,Yuanhao Feng,Bo Wang,Guilin Li,Wuxiong Zeng,Xiujun Shu,Sibo Wang*

Main category: cs.LG

TL;DR: ChiGAD是一个基于卡方滤波器的谱GNN框架，用于解决异质网络中的图异常检测问题，通过多图卡方滤波器和交互式元图卷积等方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图神经网络方法主要针对同质网络的异常检测，无法有效处理异质网络中的节点和边异质性，尤其是跨元路径的异常信号捕捉、高频信息保留和类别不平衡问题。

Method: ChiGAD包含三个核心组件：多图卡方滤波器捕捉异常信息，交互式元图卷积对齐特征并保留高频信息，贡献感知交叉熵损失解决类别不平衡。

Result: 在公开和工业数据集上的实验表明，ChiGAD在多项指标上优于现有方法，其同质变体ChiGNN在七个GAD数据集上表现优异。

Conclusion: ChiGAD通过卡方滤波器有效解决了异质网络中的异常检测问题，其设计在理论和实验上均验证了有效性。

Abstract: Graph Anomaly Detection (GAD) in heterogeneous networks presents unique
challenges due to node and edge heterogeneity. Existing Graph Neural Network
(GNN) methods primarily focus on homogeneous GAD and thus fail to address three
key issues: (C1) Capturing abnormal signal and rich semantics across diverse
meta-paths; (C2) Retaining high-frequency content in HIN dimension alignment;
and (C3) Learning effectively from difficult anomaly samples with class
imbalance. To overcome these, we propose ChiGAD, a spectral GNN framework based
on a novel Chi-Square filter, inspired by the wavelet effectiveness in diverse
domains. Specifically, ChiGAD consists of: (1) Multi-Graph Chi-Square Filter,
which captures anomalous information via applying dedicated Chi-Square filters
to each meta-path graph; (2) Interactive Meta-Graph Convolution, which aligns
features while preserving high-frequency information and incorporates
heterogeneous messages by a unified Chi-Square Filter; and (3)
Contribution-Informed Cross-Entropy Loss, which prioritizes difficult anomalies
to address class imbalance. Extensive experiments on public and industrial
datasets show that ChiGAD outperforms state-of-the-art models on multiple
metrics. Additionally, its homogeneous variant, ChiGNN, excels on seven GAD
datasets, validating the effectiveness of Chi-Square filters. Our code is
available at https://github.com/HsipingLi/ChiGAD.

</details>


### [492] [Exact Expressive Power of Transformers with Padding](https://arxiv.org/abs/2505.18948)
*William Merrill,Ashish Sabharwal*

Main category: cs.LG

TL;DR: 该论文提出了一种并行化替代方案，通过填充和循环扩展Transformer的表达能力，而不增加参数，证明了其在计算复杂度上的优势。


<details>
  <summary>Details</summary>
Motivation: 研究动机是寻找比链式思维更高效的替代方法，以扩展Transformer的计算能力，同时保持并行性。

Method: 采用填充标记和动态增加深度（循环）的方法，结合平均硬注意力和掩码预归一化Transformer，分析其计算能力。

Result: 证明了填充Transformer结合循环可以识别特定复杂度类（如TC^d和NC），系统性地扩展了Transformer的表达能力。

Conclusion: 填充和循环作为并行化替代方案，具有扩展Transformer表达能力的潜力，值得进一步探索。

Abstract: Chain of thought is a natural inference-time method for increasing the
computational power of transformer-based large language models (LLMs), but
comes at the cost of sequential decoding. Are there more efficient alternatives
to expand a transformer's expressive power without adding parameters? We
consider transformers with padding tokens as a form of parallelizable test-time
compute. We show that averaging-hard-attention, masked-pre-norm transformers
with polynomial padding converge to precisely the class $\mathsf{TC}^0$ of
extremely parallelizable problems. While the $\mathsf{TC}^0$ upper bound was
known, proving a matching lower bound had been elusive. Further, our novel
analysis reveals the precise expanded power of padded transformers when coupled
with another form of inference-time compute, namely dynamically increasing
depth via looping. Our core technical contribution is to show how padding helps
bring the notions of complete problems and reductions, which have been a
cornerstone of classical complexity theory, to the formal study of
transformers. Armed with this new tool, we prove that padded transformers with
$O(\log^d n)$ looping on inputs of length $n$ recognize exactly the class
$\mathsf{TC}^d$ of moderately parallelizable problems. Thus, padding and
looping together systematically expand transformers' expressive power: with
polylogarithmic looping, padded transformers converge to the class
$\mathsf{NC}$, the best that could be expected without losing parallelism
(unless $\mathsf{NC} = \mathsf{P}$). Our results thus motivate further
exploration of padding and looping as parallelizable alternatives to chain of
thought.

</details>


### [493] [Online Knowledge Distillation with Reward Guidance](https://arxiv.org/abs/2505.18952)
*Chen Jia*

Main category: cs.LG

TL;DR: 该论文提出了一种基于偏好优化的知识蒸馏框架，通过奖励引导的模仿学习缩小学生与教师模型的性能差距。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过偏好优化改进大型语言模型的知识蒸馏，以提升学生模型的性能。

Method: 采用奖励引导的模仿学习框架，构建离线和在线偏好数据，并利用Q值函数重新定义奖励模型。

Result: 理论和实验结果证明了该框架在知识蒸馏中的有效性。

Conclusion: 提出的框架能有效优化知识蒸馏过程，提升学生模型的性能。

Abstract: This work studies knowledge distillation (KD) for large language models
(LLMs) through preference optimization. We propose a reward-guided imitation
learning framework for sequential KD, formulating a min-max optimization
problem between the policy and reward model (RM) to minimize the performance
gap between the student and teacher policies. Specifically, the reward
optimization is constrained to achieve near-optimality within a confidence set
for preference alignment. For preference data construction, we explore both
offline and online preference-based KD. Additionally, we reformulate the RM
using the $Q$-value function and extend the framework to white-box KD, where
the teacher policy's predicted probabilities are accessible. Theoretical
analysis and empirical results demonstrate the effectiveness of the proposed
framework.

</details>


### [494] [Protein Design with Dynamic Protein Vocabulary](https://arxiv.org/abs/2505.18966)
*Nuowei Liu,Jiahao Kuang,Yanting Liu,Changzhi Sun,Tao Ji,Yuanbin Wu,Man Lan*

Main category: cs.LG

TL;DR: ProDVa是一种新型蛋白质设计方法，通过整合文本编码器、蛋白质语言模型和片段编码器，有效设计功能对齐且结构合理的蛋白质序列。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度生成模型的蛋白质设计方法在结构合理性方面存在不足，而传统方法利用天然蛋白质片段可能提升折叠性。

Method: ProDVa整合了文本编码器（功能描述）、蛋白质语言模型（设计蛋白质）和片段编码器（动态检索天然蛋白质片段）。

Result: ProDVa在功能对齐方面与先进模型相当，但训练数据量减少99.96%，且设计的蛋白质折叠性显著提升（pLDDT>70的比例增加7.38%，PAE<10的比例增加9.6%）。

Conclusion: ProDVa通过引入天然蛋白质片段，显著提升了生成蛋白质的结构合理性，同时保持功能对齐，为蛋白质设计提供了高效解决方案。

Abstract: Protein design is a fundamental challenge in biotechnology, aiming to design
novel sequences with specific functions within the vast space of possible
proteins. Recent advances in deep generative models have enabled function-based
protein design from textual descriptions, yet struggle with structural
plausibility. Inspired by classical protein design methods that leverage
natural protein structures, we explore whether incorporating fragments from
natural proteins can enhance foldability in generative models. Our empirical
results show that even random incorporation of fragments improves foldability.
Building on this insight, we introduce ProDVa, a novel protein design approach
that integrates a text encoder for functional descriptions, a protein language
model for designing proteins, and a fragment encoder to dynamically retrieve
protein fragments based on textual functional descriptions. Experimental
results demonstrate that our approach effectively designs protein sequences
that are both functionally aligned and structurally plausible. Compared to
state-of-the-art models, ProDVa achieves comparable function alignment using
less than 0.04% of the training data, while designing significantly more
well-folded proteins, with the proportion of proteins having pLDDT above 70
increasing by 7.38% and those with PAE below 10 increasing by 9.6%.

</details>


### [495] [GraSS: Scalable Influence Function with Sparse Gradient Compression](https://arxiv.org/abs/2505.18976)
*Pingbang Hu,Joseph Melkonian,Weijing Tang,Han Zhao,Jiaqi W. Ma*

Main category: cs.LG

TL;DR: 提出GraSS和FactGraSS梯度压缩算法，利用梯度稀疏性降低计算和内存成本，显著提升大规模模型训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的数据归因方法（如影响函数）因逐样本梯度计算的高成本难以扩展，需高效压缩方案。

Method: 设计GraSS通用梯度压缩算法及面向线性层的变体FactGraSS，利用梯度稀疏性实现次线性时空复杂度。

Result: 实验显示方法在保持归因准确性的同时显著加速，FactGraSS在十亿级模型上比SOTA基线吞吐量提升165%。

Conclusion: GraSS系列算法通过梯度压缩有效解决了数据归因的可扩展性问题，代码已开源。

Abstract: Gradient-based data attribution methods, such as influence functions, are
critical for understanding the impact of individual training samples without
requiring repeated model retraining. However, their scalability is often
limited by the high computational and memory costs associated with per-sample
gradient computation. In this work, we propose GraSS, a novel gradient
compression algorithm and its variants FactGraSS for linear layers
specifically, that explicitly leverage the inherent sparsity of per-sample
gradients to achieve sub-linear space and time complexity. Extensive
experiments demonstrate the effectiveness of our approach, achieving
substantial speedups while preserving data influence fidelity. In particular,
FactGraSS achieves up to 165% faster throughput on billion-scale models
compared to the previous state-of-the-art baselines. Our code is publicly
available at https://github.com/TRAIS-Lab/GraSS.

</details>


### [496] [GhostPrompt: Jailbreaking Text-to-image Generative Models based on Dynamic Optimization](https://arxiv.org/abs/2505.18979)
*Zixuan Chen,Hao Lin,Ke Xu,Xinghao Jiang,Tanfeng Sun*

Main category: cs.LG

TL;DR: GhostPrompt是一种新型自动化越狱框架，通过动态提示优化和多模态反馈，有效绕过现代文本和图像安全过滤器，显著提高NSFW内容生成的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型的安全过滤器（如基于LLM的语义检测）使传统攻击方法失效，作者旨在开发一种能有效绕过现代多模态防御系统的自动化越狱技术。

Method: 结合动态优化（迭代生成对抗提示）和自适应安全指示注入（强化学习注入良性视觉线索），形成双管齐下的攻击框架。

Result: 将ShieldLM-7B绕过率从12.5%提升至99%，CLIP分数提高4.8%，时间成本降低4.2倍，并能泛化至GPT-4.1等未见过滤器。

Conclusion: 当前多模态防御系统存在系统性漏洞，GhostPrompt为AI安全红队测试提供了有效工具，相关资源将受控公开以促进研究。

Abstract: Text-to-image (T2I) generation models can inadvertently produce
not-safe-for-work (NSFW) content, prompting the integration of text and image
safety filters. Recent advances employ large language models (LLMs) for
semantic-level detection, rendering traditional token-level perturbation
attacks largely ineffective. However, our evaluation shows that existing
jailbreak methods are ineffective against these modern filters. We introduce
GhostPrompt, the first automated jailbreak framework that combines dynamic
prompt optimization with multimodal feedback. It consists of two key
components: (i) Dynamic Optimization, an iterative process that guides a large
language model (LLM) using feedback from text safety filters and CLIP
similarity scores to generate semantically aligned adversarial prompts; and
(ii) Adaptive Safety Indicator Injection, which formulates the injection of
benign visual cues as a reinforcement learning problem to bypass image-level
filters. GhostPrompt achieves state-of-the-art performance, increasing the
ShieldLM-7B bypass rate from 12.5\% (Sneakyprompt) to 99.0\%, improving CLIP
score from 0.2637 to 0.2762, and reducing the time cost by $4.2 \times$.
Moreover, it generalizes to unseen filters including GPT-4.1 and successfully
jailbreaks DALLE 3 to generate NSFW images in our evaluation, revealing
systemic vulnerabilities in current multimodal defenses. To support further
research on AI safety and red-teaming, we will release code and adversarial
prompts under a controlled-access protocol.

</details>


### [497] [FedSKC: Federated Learning with Non-IID Data via Structural Knowledge Collaboration](https://arxiv.org/abs/2505.18981)
*Huan Wang,Haoran Li,Huaming Chen,Jun Yan,Lijuan Wang,Jiahua Shi,Shiping Chen,Jun Shen*

Main category: cs.LG

TL;DR: 该论文提出FedSKC方法，通过利用客户端内部类结构知识解决联邦学习中的数据异构问题，包含局部对比学习、全局差异聚合和全局定期审查三个组件，理论分析和实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临数据异构问题（客户端标签偏好偏差），传统方法忽视客户端内部类结构信息，影响模型收敛和性能。

Method: 提出FedSKC框架：1) 局部对比学习减少本地训练权重偏差；2) 全局差异聚合解决服务器与客户端参数偏差；3) 全局定期审查校正服务器随机选择设备导致的采样偏差。

Result: 理论证明FedSKC在非凸目标下的有效性，实验结果表明其优于现有方法。

Conclusion: FedSKC通过挖掘客户端内部类结构知识，有效解决数据异构导致的三种偏差问题，提升联邦学习性能。

Abstract: With the advancement of edge computing, federated learning (FL) displays a
bright promise as a privacy-preserving collaborative learning paradigm.
However, one major challenge for FL is the data heterogeneity issue, which
refers to the biased labeling preferences among multiple clients, negatively
impacting convergence and model performance. Most previous FL methods attempt
to tackle the data heterogeneity issue locally or globally, neglecting
underlying class-wise structure information contained in each client. In this
paper, we first study how data heterogeneity affects the divergence of the
model and decompose it into local, global, and sampling drift sub-problems. To
explore the potential of using intra-client class-wise structural knowledge in
handling these drifts, we thus propose Federated Learning with Structural
Knowledge Collaboration (FedSKC). The key idea of FedSKC is to extract and
transfer domain preferences from inter-client data distributions, offering
diverse class-relevant knowledge and a fair convergent signal. FedSKC comprises
three components: i) local contrastive learning, to prevent weight divergence
resulting from local training; ii) global discrepancy aggregation, which
addresses the parameter deviation between the server and clients; iii) global
period review, correcting for the sampling drift introduced by the server
randomly selecting devices. We have theoretically analyzed FedSKC under
non-convex objectives and empirically validated its superiority through
extensive experimental results.

</details>


### [498] [AmorLIP: Efficient Language-Image Pretraining via Amortization](https://arxiv.org/abs/2505.18983)
*Haotian Sun,Yitong Li,Yuchen Zhuang,Niao He,Hanjun Dai,Bo Dai*

Main category: cs.LG

TL;DR: AmorLIP提出了一种高效的CLIP预训练框架，通过轻量级神经网络分摊对比学习中的昂贵计算，显著提升训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP方法需要极大批次规模和大量GPU计算资源，导致训练效率低下且难以扩展。AmorLIP旨在解决这一问题，同时不牺牲下游任务性能。

Method: AmorLIP通过能量模型的谱分解洞察，引入新的分摊目标和实用技术，利用轻量级神经网络分摊对比学习中的计算开销。

Result: 在38个下游任务上的实验表明，AmorLIP在零样本分类和检索任务上表现优异，相对基线CLIP模型最高提升12.24%。

Conclusion: AmorLIP通过高效分摊计算，显著提升了CLIP模型的训练效率和性能，为大规模对比学习提供了可行解决方案。

Abstract: Contrastive Language-Image Pretraining (CLIP) has demonstrated strong
zero-shot performance across diverse downstream text-image tasks. Existing CLIP
methods typically optimize a contrastive objective using negative samples drawn
from each minibatch. To achieve robust representation learning, these methods
require extremely large batch sizes and escalate computational demands to
hundreds or even thousands of GPUs. Prior approaches to mitigate this issue
often compromise downstream performance, prolong training duration, or face
scalability challenges with very large datasets. To overcome these limitations,
we propose AmorLIP, an efficient CLIP pretraining framework that amortizes
expensive computations involved in contrastive learning through lightweight
neural networks, which substantially improves training efficiency and
performance. Leveraging insights from a spectral factorization of energy-based
models, we introduce novel amortization objectives along with practical
techniques to improve training stability. Extensive experiments across 38
downstream tasks demonstrate the superior zero-shot classification and
retrieval capabilities of AmorLIP, consistently outperforming standard CLIP
baselines with substantial relative improvements of up to 12.24%.

</details>


### [499] [STRICT: Stress Test of Rendering Images Containing Text](https://arxiv.org/abs/2505.18985)
*Tianyu Zhang,Xinyu Wang,Zhenghan Tai,Lu Li,Jijun Chi,Jingrui Tian,Hailin He,Suyuchen Wang*

Main category: cs.LG

TL;DR: 论文提出了STRICT基准，用于系统测试扩散模型在生成图像中渲染连贯且符合指令的文本的能力，揭示了现有模型在长距离一致性和指令遵循上的局限性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在文本到图像生成方面取得了显著进展，但在生成图像中一致且清晰的文本方面仍存在困难，主要由于局部偏置限制了长距离空间依赖建模。

Method: 引入STRICT基准，从三个维度评估模型：可生成文本的最大长度、生成文本的正确性和可读性，以及不遵循文本生成指令的比例。

Result: 评估了多个先进模型（包括专有和开源变体），发现它们在长距离一致性和指令遵循能力上存在持续局限性。

Conclusion: 研究结果揭示了架构瓶颈，并为多模态生成模型的未来研究方向提供了启示。

Abstract: While diffusion models have revolutionized text-to-image generation with
their ability to synthesize realistic and diverse scenes, they continue to
struggle to generate consistent and legible text within images. This
shortcoming is commonly attributed to the locality bias inherent in
diffusion-based generation, which limits their ability to model long-range
spatial dependencies. In this paper, we introduce $\textbf{STRICT}$, a
benchmark designed to systematically stress-test the ability of diffusion
models to render coherent and instruction-aligned text in images. Our benchmark
evaluates models across multiple dimensions: (1) the maximum length of readable
text that can be generated; (2) the correctness and legibility of the generated
text, and (3) the ratio of not following instructions for generating text. We
evaluate several state-of-the-art models, including proprietary and open-source
variants, and reveal persistent limitations in long-range consistency and
instruction-following capabilities. Our findings provide insights into
architectural bottlenecks and motivate future research directions in multimodal
generative modeling. We release our entire evaluation pipeline at
https://github.com/tianyu-z/STRICT-Bench.

</details>


### [500] [Automatic and Structure-Aware Sparsification of Hybrid Neural ODEs](https://arxiv.org/abs/2505.18996)
*Bob Junyi Zou,Lu Tian*

Main category: cs.LG

TL;DR: 提出一种混合神经ODE自动状态选择与结构优化方法，通过图修改与正则化提升稀疏性，在医疗数据中实现更高预测性能与稳定性。


<details>
  <summary>Details</summary>
Motivation: 混合神经ODE在数据稀缺的医疗场景中具有优势，但传统方法因过多潜在状态和交互导致训练低效和过拟合，限制了实际效果。

Method: 结合领域知识图修改与数据驱动正则化，自动选择状态并优化结构，稀疏化模型同时保持机制合理性。

Result: 在合成和真实数据上验证，模型在稀疏性、预测性能和鲁棒性方面均有提升。

Conclusion: 该方法为医疗领域的混合模型简化提供了有效解决方案，平衡了机制合理性与数据驱动灵活性。

Abstract: Hybrid neural ordinary differential equations (neural ODEs) integrate
mechanistic models with neural ODEs, offering strong inductive bias and
flexibility, and are particularly advantageous in data-scarce healthcare
settings. However, excessive latent states and interactions from mechanistic
models can lead to training inefficiency and over-fitting, limiting practical
effectiveness of hybrid neural ODEs. In response, we propose a new hybrid
pipeline for automatic state selection and structure optimization in
mechanistic neural ODEs, combining domain-informed graph modifications with
data-driven regularization to sparsify the model for improving predictive
performance and stability while retaining mechanistic plausibility. Experiments
on synthetic and real-world data show improved predictive performance and
robustness with desired sparsity, establishing an effective solution for hybrid
model reduction in healthcare applications.

</details>


### [501] [Semi-pessimistic Reinforcement Learning](https://arxiv.org/abs/2505.19002)
*Jin Zhu,Xin Zhou,Jiaang Yao,Gholamali Aminian,Omar Rivasplata,Simon Little,Lexin Li,Chengchun Shi*

Main category: cs.LG

TL;DR: 该论文提出了一种半悲观强化学习方法，利用大量未标记数据解决离线强化学习中的分布偏移问题，并在自适应深脑刺激治疗帕金森病的应用中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习面临分布偏移和标记奖励数据稀缺的问题，导致策略学习效果不佳。未标记数据量通常远大于标记数据，如何有效利用这些数据成为关键。

Method: 提出了一种半悲观强化学习方法，通过寻找奖励函数的下界而非Q函数或状态转移函数的下界，简化学习过程，并能与多种模型无关和基于模型的RL算法结合。

Result: 该方法在分析和数值比较中表现出明显竞争力，并在自适应深脑刺激治疗帕金森病的应用中验证了其有效性。

Conclusion: 该方法能有效利用大量未标记数据，显著提升策略学习效果，且适用条件宽松，具有广泛的应用潜力。

Abstract: Offline reinforcement learning (RL) aims to learn an optimal policy from
pre-collected data. However, it faces challenges of distributional shift, where
the learned policy may encounter unseen scenarios not covered in the offline
data. Additionally, numerous applications suffer from a scarcity of labeled
reward data. Relying on labeled data alone often leads to a narrow state-action
distribution, further amplifying the distributional shift, and resulting in
suboptimal policy learning. To address these issues, we first recognize that
the volume of unlabeled data is typically substantially larger than that of
labeled data. We then propose a semi-pessimistic RL method to effectively
leverage abundant unlabeled data. Our approach offers several advantages. It
considerably simplifies the learning process, as it seeks a lower bound of the
reward function, rather than that of the Q-function or state transition
function. It is highly flexible, and can be integrated with a range of
model-free and model-based RL algorithms. It enjoys the guaranteed improvement
when utilizing vast unlabeled data, but requires much less restrictive
conditions. We compare our method with a number of alternative solutions, both
analytically and numerically, and demonstrate its clear competitiveness. We
further illustrate with an application to adaptive deep brain stimulation for
Parkinson's disease.

</details>


### [502] [Faithful Group Shapley Value](https://arxiv.org/abs/2505.19013)
*Kiljae Lee,Ziqi Liu,Weijing Tang,Yuan Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为FGSV的新方法，用于防御数据估值中的壳公司攻击，并通过快速近似算法提高了计算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的组级数据估值方法容易受到壳公司攻击，即通过策略性分组来不公平地提高估值。因此，需要一种能够防御此类攻击的估值方法。

Method: 提出了Faithful Group Shapley Value (FGSV)，基于数学洞察开发了一种快速且准确的近似算法。

Result: 实验表明，FGSV在计算效率和近似准确性上显著优于现有方法，同时确保了组级估值的忠实性。

Conclusion: FGSV是一种有效防御壳公司攻击的组级数据估值方法，其近似算法在效率和准确性上表现优异。

Abstract: Data Shapley is an important tool for data valuation, which quantifies the
contribution of individual data points to machine learning models. In practice,
group-level data valuation is desirable when data providers contribute data in
batch. However, we identify that existing group-level extensions of Data
Shapley are vulnerable to shell company attacks, where strategic group
splitting can unfairly inflate valuations. We propose Faithful Group Shapley
Value (FGSV) that uniquely defends against such attacks. Building on original
mathematical insights, we develop a provably fast and accurate approximation
algorithm for computing FGSV. Empirical experiments demonstrate that our
algorithm significantly outperforms state-of-the-art methods in computational
efficiency and approximation accuracy, while ensuring faithful group-level
valuation.

</details>


### [503] [Tokenizing Electron Cloud in Protein-Ligand Interaction Learning](https://arxiv.org/abs/2505.19014)
*Haitao Lin,Odin Zhang,Jia Xu,Yunfan Liu,Zheng Cheng,Lirong Wu,Yufei Huang,Zhifeng Gao,Stan Z. Li*

Main category: cs.LG

TL;DR: ECBind提出了一种将电子云信号转化为量化嵌入的方法，用于预测蛋白质-分子结合亲和力，通过结合电子密度信息揭示了原子级模型无法完全捕捉的结合模式，并在多个任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 蛋白质-分子结合的亲和力和特异性直接影响功能结果，揭示生物调控和信号传导的机制。现有的深度学习方法主要关注原子或片段结构，而量子化学性质（如电子结构）在揭示相互作用模式中起关键作用，但尚未充分探索。

Method: ECBind通过结构感知的Transformer和分层码本将富含电子结构的3D结合位点编码为标记，去除电子云信号中的冗余信息，并将这些标记化的编码用于特定任务。此外，利用知识蒸馏开发了一个不依赖电子云的预测模型，以扩展其应用范围。

Result: ECBind在多个任务中表现出最先进的性能，每结构的Pearson和Spearman相关系数分别提高了6.42%和15.58%。

Conclusion: ECBind通过整合电子密度信息，成功揭示了传统原子级模型无法捕捉的结合模式，显著提升了结合亲和力预测的性能，为生物分子相互作用研究提供了新的工具。

Abstract: The affinity and specificity of protein-molecule binding directly impact
functional outcomes, uncovering the mechanisms underlying biological regulation
and signal transduction. Most deep-learning-based prediction approaches focus
on structures of atoms or fragments. However, quantum chemical properties, such
as electronic structures, are the key to unveiling interaction patterns but
remain largely underexplored. To bridge this gap, we propose ECBind, a method
for tokenizing electron cloud signals into quantized embeddings, enabling their
integration into downstream tasks such as binding affinity prediction. By
incorporating electron densities, ECBind helps uncover binding modes that
cannot be fully represented by atom-level models. Specifically, to remove the
redundancy inherent in electron cloud signals, a structure-aware transformer
and hierarchical codebooks encode 3D binding sites enriched with electron
structures into tokens. These tokenized codes are then used for specific tasks
with labels. To extend its applicability to a wider range of scenarios, we
utilize knowledge distillation to develop an electron-cloud-agnostic prediction
model. Experimentally, ECBind demonstrates state-of-the-art performance across
multiple tasks, achieving improvements of 6.42\% and 15.58\% in per-structure
Pearson and Spearman correlation coefficients, respectively.

</details>


### [504] [Querying Kernel Methods Suffices for Reconstructing their Training Data](https://arxiv.org/abs/2505.19019)
*Daniel Barzilai,Yuval Margalit,Eitan Gronich,Gilad Yehudai,Meirav Galun,Ronen Basri*

Main category: cs.LG

TL;DR: 研究发现，仅通过查询核方法模型的输出即可重构其训练数据，揭示过参数化模型可能存在隐私风险。


<details>
  <summary>Details</summary>
Motivation: 过参数化模型在强泛化能力下仍可能记忆训练数据，但其隐私影响尚不明确，尤其是在仅能访问模型输出的场景中。

Method: 通过理论和实证分析，研究核方法（如核回归、支持向量机、核密度估计）在仅输出可访问时的数据重构问题。

Result: 实验和理论证明，无需访问模型参数，仅通过查询模型输出即可重构训练数据。

Conclusion: 该研究揭示了核方法模型潜在的隐私风险，呼吁关注此类模型的数据保护问题。

Abstract: Over-parameterized models have raised concerns about their potential to
memorize training data, even when achieving strong generalization. The privacy
implications of such memorization are generally unclear, particularly in
scenarios where only model outputs are accessible. We study this question in
the context of kernel methods, and demonstrate both empirically and
theoretically that querying kernel models at various points suffices to
reconstruct their training data, even without access to model parameters. Our
results hold for a range of kernel methods, including kernel regression,
support vector machines, and kernel density estimation. Our hope is that this
work can illuminate potential privacy concerns for such models.

</details>


### [505] [Learn Beneficial Noise as Graph Augmentation](https://arxiv.org/abs/2505.19024)
*Siqi Huang,Yanchen Xu,Hongyuan Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为PiNGDA的图对比学习方法，通过科学分析噪声的积极作用，设计可训练的噪声生成器来增强图数据，提高了性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的图对比学习方法通常采用启发式增强（如随机边丢弃），可能会破坏重要的图结构并导致性能不稳定。因此，需要一种更科学的方法来生成有效的图增强。

Method: 提出PiNGDA方法，利用信息理论分析噪声的积极作用，设计高斯辅助变量将损失函数转换为信息熵，并通过可训练的噪声生成器学习对拓扑和属性的有益扰动。

Result: 大量实验结果验证了PiNGDA的有效性和稳定性，相比现有方法更具可靠性。

Conclusion: PiNGDA通过科学分析噪声的积极作用并设计可训练的噪声生成器，显著提升了图对比学习的性能和稳定性。

Abstract: Although graph contrastive learning (GCL) has been widely investigated, it is
still a challenge to generate effective and stable graph augmentations.
Existing methods often apply heuristic augmentation like random edge dropping,
which may disrupt important graph structures and result in unstable GCL
performance. In this paper, we propose Positive-incentive Noise driven Graph
Data Augmentation (PiNGDA), where positive-incentive noise (pi-noise)
scientifically analyzes the beneficial effect of noise under the information
theory. To bridge the standard GCL and pi-noise framework, we design a Gaussian
auxiliary variable to convert the loss function to information entropy. We
prove that the standard GCL with pre-defined augmentations is equivalent to
estimate the beneficial noise via the point estimation. Following our analysis,
PiNGDA is derived from learning the beneficial noise on both topology and
attributes through a trainable noise generator for graph augmentations, instead
of the simple estimation. Since the generator learns how to produce beneficial
perturbations on graph topology and node attributes, PiNGDA is more reliable
compared with the existing methods. Extensive experimental results validate the
effectiveness and stability of PiNGDA.

</details>


### [506] [Turb-L1: Achieving Long-term Turbulence Tracing By Tackling Spectral Bias](https://arxiv.org/abs/2505.19038)
*Hao Wu,Yuan Gao,Ruiqi Shu,Zean Han,Fan Xu,Zhihong Zhu,Qingsong Wen,Xian Wu,Kun Wang,Xiaomeng Huang*

Main category: cs.LG

TL;DR: 论文提出Turb-L1方法，通过多网格架构中的层次动态合成机制克服频谱偏差，显著提升湍流长期预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在长期自回归预测中存在过度平滑问题，无法准确追踪复杂流体动力学，频谱偏差是核心障碍。

Method: 提出Turb-L1方法，采用多网格架构中的层次动态合成机制，显式克服频谱偏差，捕捉跨尺度交互并保持高频动态保真度。

Result: 在2D湍流基准测试中，Turb-L1将MSE降低80.3%，SSIM提升9倍以上，准确再现全熵谱并保持高波数区域的物理真实性。

Conclusion: Turb-L1通过克服频谱偏差，实现了湍流长期演化的可靠追踪，显著提升了预测保真度和物理真实性。

Abstract: Accurately predicting the long-term evolution of turbulence is crucial for
advancing scientific understanding and optimizing engineering applications.
However, existing deep learning methods face significant bottlenecks in
long-term autoregressive prediction, which exhibit excessive smoothing and fail
to accurately track complex fluid dynamics. Our extensive experimental and
spectral analysis of prevailing methods provides an interpretable explanation
for this shortcoming, identifying Spectral Bias as the core obstacle.
Concretely, spectral bias is the inherent tendency of models to favor
low-frequency, smooth features while overlooking critical high-frequency
details during training, thus reducing fidelity and causing physical
distortions in long-term predictions. Building on this insight, we propose
Turb-L1, an innovative turbulence prediction method, which utilizes a
Hierarchical Dynamics Synthesis mechanism within a multi-grid architecture to
explicitly overcome spectral bias. It accurately captures cross-scale
interactions and preserves the fidelity of high-frequency dynamics, enabling
reliable long-term tracking of turbulence evolution. Extensive experiments on
the 2D turbulence benchmark show that Turb-L1 demonstrates excellent
performance: (I) In long-term predictions, it reduces Mean Squared Error (MSE)
by $80.3\%$ and increases Structural Similarity (SSIM) by over $9\times$
compared to the SOTA baseline, significantly improving prediction fidelity.
(II) It effectively overcomes spectral bias, accurately reproducing the full
enstrophy spectrum and maintaining physical realism in high-wavenumber regions,
thus avoiding the spectral distortions or spurious energy accumulation seen in
other methods.

</details>


### [507] [Offline Clustering of Linear Bandits: Unlocking the Power of Clusters in Data-Limited Environments](https://arxiv.org/abs/2505.19043)
*Jingyuan Liu,Zeyu Zhang,Xuchuang Wang,Xutong Liu,John C. S. Lui,Mohammad Hajiesmaili,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 本文研究了离线多臂老虎机问题中的用户聚类（Off-ClusBand），提出两种算法处理数据不足的挑战，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注在线设置，忽略了广泛可用的离线数据。本文旨在利用离线数据集学习用户聚类特性，以改进多用户决策。

Method: 提出了两种算法：Off-C²LUB（适用于任意数据量）和Off-CLUB（在数据充足时接近理论下界）。

Result: 实验证明，两种算法在真实和合成数据集上均表现良好，其中Off-CLUB在数据充足时达到理论最优。

Conclusion: 通过离线聚类方法可以有效利用有限数据提升多用户决策性能，填补了离线场景的研究空白。

Abstract: Contextual linear multi-armed bandits are a learning framework for making a
sequence of decisions, e.g., advertising recommendations for a sequence of
arriving users. Recent works have shown that clustering these users based on
the similarity of their learned preferences can significantly accelerate the
learning. However, prior work has primarily focused on the online setting,
which requires continually collecting user data, ignoring the offline data
widely available in many applications. To tackle these limitations, we study
the offline clustering of bandits (Off-ClusBand) problem, which studies how to
use the offline dataset to learn cluster properties and improve decision-making
across multiple users. The key challenge in Off-ClusBand arises from data
insufficiency for users: unlike the online case, in the offline case, we have a
fixed, limited dataset to work from and thus must determine whether we have
enough data to confidently cluster users together. To address this challenge,
we propose two algorithms: Off-C$^2$LUB, which we analytically show performs
well for arbitrary amounts of user data, and Off-CLUB, which is prone to bias
when data is limited but, given sufficient data, matches a theoretical lower
bound that we derive for the offline clustered MAB problem. We experimentally
validate these results on both real and synthetic datasets.

</details>


### [508] [Structured Reinforcement Learning for Combinatorial Decision-Making](https://arxiv.org/abs/2505.19053)
*Heiko Hoppe,Léo Baty,Louis Bouvier,Axel Parmentier,Maximilian Schiffer*

Main category: cs.LG

TL;DR: 论文提出结构化强化学习（SRL）框架，通过嵌入组合优化层解决复杂决策问题，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在处理组合动作空间时难以扩展和利用结构信息，需新方法应对复杂决策问题。

Method: 提出SRL框架，在演员网络嵌入组合优化层，使用Fenchel-Young损失进行端到端学习。

Result: 在六种环境中，SRL性能匹配或超越非结构化强化学习和模仿学习，动态问题提升达92%。

Conclusion: SRL框架有效解决组合动作空间问题，提升性能、稳定性和收敛速度。

Abstract: Reinforcement learning (RL) is increasingly applied to real-world problems
involving complex and structured decisions, such as routing, scheduling, and
assortment planning. These settings challenge standard RL algorithms, which
struggle to scale, generalize, and exploit structure in the presence of
combinatorial action spaces. We propose Structured Reinforcement Learning
(SRL), a novel actor-critic framework that embeds combinatorial optimization
layers into the actor neural network. We enable end-to-end learning of the
actor via Fenchel-Young losses and provide a geometric interpretation of SRL as
a primal-dual algorithm in the dual of the moment polytope. Across six
environments with exogenous and endogenous uncertainty, SRL matches or
surpasses the performance of unstructured RL and imitation learning on static
tasks and improves over these baselines by up to 92% on dynamic problems, with
improved stability and convergence speed.

</details>


### [509] [Reduce Computational Cost In Deep Reinforcement Learning Via Randomized Policy Learning](https://arxiv.org/abs/2505.19054)
*Zhuochen Liu,Rahul Jain,Quan Nguyen*

Main category: cs.LG

TL;DR: 该论文提出了一种基于随机神经网络的actor-critic算法，显著降低了计算成本，同时在多种控制任务中保持了良好的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管深度强化学习在许多控制任务中表现出色，但其训练过程通常需要大量的计算资源和时间。本文旨在通过简化神经网络架构来减少这些开销。

Method: 采用随机神经网络的actor-critic算法，简化了传统深度神经网络的复杂架构，从而降低了计算成本。

Result: 该方法在多种控制任务中表现良好，包括高动态四足机器人的运动控制，其性能与PPO等先进算法相当，但实际训练时间更短。

Conclusion: 尽管该算法在样本效率上不占优势，但在实际训练时间上具有显著优势，为资源受限的应用场景提供了一种可行的解决方案。

Abstract: Recent advancements in reinforcement learning (RL) have leveraged neural
networks to achieve state-of-the-art performance across various control tasks.
However, these successes often come at the cost of significant computational
resources, as training deep neural networks requires substantial time and data.
In this paper, we introduce an actor-critic algorithm that utilizes randomized
neural networks to drastically reduce computational costs while maintaining
strong performance. Despite its simple architecture, our method effectively
solves a range of control problems, including the locomotion control of a
highly dynamic 12-motor quadruped robot, and achieves results comparable to
leading algorithms such as Proximal Policy Optimization (PPO). Notably, our
approach does not outperform other algorithms in terms of sample efficnency but
rather in terms of wall-clock training time. That is, although our algorithm
requires more timesteps to converge to an optimal policy, the actual time
required for training turns out to be lower.

</details>


### [510] [Distributionally Robust Deep Q-Learning](https://arxiv.org/abs/2505.19058)
*Chung I Lu,Julian Sester,Aijia Zhang*

Main category: cs.LG

TL;DR: 提出一种新型分布鲁棒Q学习算法，针对连续状态空间且状态转移存在模型不确定性的马尔可夫决策过程，通过Sinkhorn距离对偶正则化贝尔曼算子求解最优策略。


<details>
  <summary>Details</summary>
Motivation: 针对连续状态空间中马尔可夫决策过程的状态转移模型不确定性，传统Q学习方法难以处理，需开发鲁棒性更强的算法。

Method: 采用Sinkhorn距离对偶正则化贝尔曼算子，结合深度神经网络参数化，改进深度Q网络算法以优化最坏情况状态转移。

Result: 通过包括基于S&P 500数据的投资组合优化等多个应用验证了方法的可行性和有效性。

Conclusion: 所提算法能有效处理模型不确定性，在连续状态空间中表现出良好的鲁棒性和实用性。

Abstract: We propose a novel distributionally robust $Q$-learning algorithm for the
non-tabular case accounting for continuous state spaces where the state
transition of the underlying Markov decision process is subject to model
uncertainty. The uncertainty is taken into account by considering the
worst-case transition from a ball around a reference probability measure. To
determine the optimal policy under the worst-case state transition, we solve
the associated non-linear Bellman equation by dualising and regularising the
Bellman operator with the Sinkhorn distance, which is then parameterized with
deep neural networks. This approach allows us to modify the Deep Q-Network
algorithm to optimise for the worst case state transition.
  We illustrate the tractability and effectiveness of our approach through
several applications, including a portfolio optimisation task based on
S\&{P}~500 data.

</details>


### [511] [Adversarial Bandit over Bandits: Hierarchical Bandits for Online Configuration Management](https://arxiv.org/abs/2505.19061)
*Chen Avin,Zvi Lotker,Shie Mannor,Gil Shabat,Hanan Shteingart,Roey Yadgar*

Main category: cs.LG

TL;DR: 该论文提出了一种名为ABoB的分层对抗多臂老虎机算法，用于在度量动作空间中优化动态参数，通过聚类相似配置来利用局部结构并适应环境变化，实验证明其性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于在有限但大规模动作（配置）空间中进行动态参数优化的需求，特别是在非随机多臂老虎机问题中，利用度量动作空间和Lipschitz对抗性来提升性能。

Method: 提出了ABoB算法，这是一种分层对抗老虎机算法，通过聚类相似配置并利用现有“扁平”算法（如EXP3和Tsallis-INF）来优化性能。

Result: 在最坏情况下，ABoB的遗憾边界与传统方法相同，为O(k^(1/2)T^(1/2))；在有利条件下，遗憾边界可提升至O(k^(1/4)T^(1/2))。实验显示ABoB在多种设置下性能提升高达50%。

Conclusion: ABoB算法通过分层聚类和利用现有算法，显著提升了在非随机多臂老虎机问题中的性能，适用于动态环境优化。

Abstract: Motivated by dynamic parameter optimization in finite, but large action
(configurations) spaces, this work studies the nonstochastic multi-armed bandit
(MAB) problem in metric action spaces with oblivious Lipschitz adversaries. We
propose ABoB, a hierarchical Adversarial Bandit over Bandits algorithm that can
use state-of-the-art existing "flat" algorithms, but additionally clusters
similar configurations to exploit local structures and adapt to changing
environments. We prove that in the worst-case scenario, such clustering
approach cannot hurt too much and ABoB guarantees a standard worst-case regret
bound of $O\left(k^{\frac{1}{2}}T^{\frac{1}{2}}\right)$, where $T$ is the
number of rounds and $k$ is the number of arms, matching the traditional flat
approach. However, under favorable conditions related to the algorithm
properties, clusters properties, and certain Lipschitz conditions, the regret
bound can be improved to $O\left(k^{\frac{1}{4}}T^{\frac{1}{2}}\right)$.
Simulations and experiments on a real storage system demonstrate that ABoB,
using standard algorithms like EXP3 and Tsallis-INF, achieves lower regret and
faster convergence than the flat method, up to 50% improvement in known
previous setups, nonstochastic and stochastic, as well as in our settings.

</details>


### [512] [Recalibrating binary probabilistic classifiers](https://arxiv.org/abs/2505.19068)
*Dirk Tasche*

Main category: cs.LG

TL;DR: 本文提出两种新的概率分类器重新校准方法（CSPD和QMM），并通过实验验证QMM方法在信用风险管理等场景中的保守性表现。


<details>
  <summary>Details</summary>
Motivation: 在信用风险管理等领域，将二元概率分类器重新校准到目标先验概率是一个重要任务。本文从分布偏移的角度分析现有方法，并探索基于AUC的假设如何改进校准方法设计。

Method: 提出两种新方法：1) 参数化协变量偏移与后验漂移（CSPD）；2) 基于ROC曲线的准矩匹配（QMM），并与现有方法进行对比测试。

Result: 测试结果表明，QMM方法在使用凹函数（如信用风险权重函数）评估时能提供适度保守的结果。

Conclusion: 基于ROC曲线的QMM方法在概率分类器重新校准任务中具有实用价值，特别适用于需要保守估计的风险管理场景。

Abstract: Recalibration of binary probabilistic classifiers to a target prior
probability is an important task in areas like credit risk management. We
analyse methods for recalibration from a distribution shift perspective.
Distribution shift assumptions linked to the area under the curve (AUC) of a
probabilistic classifier are found to be useful for the design of meaningful
recalibration methods. Two new methods called parametric covariate shift with
posterior drift (CSPD) and ROC-based quasi moment matching (QMM) are proposed
and tested together with some other methods in an example setting. The outcomes
of the test suggest that the QMM methods discussed in the paper can provide
appropriately conservative results in evaluations with concave functionals like
for instance risk weights functions for credit risk.

</details>


### [513] [Temperature is All You Need for Generalization in Langevin Dynamics and other Markov Processes](https://arxiv.org/abs/2505.19087)
*Itamar Harel,Yonathan Wolanowsky,Gal Vardi,Nathan Srebro,Daniel Soudry*

Main category: cs.LG

TL;DR: 该论文分析了使用马尔可夫随机训练算法训练过参数化模型时的泛化差距，提出了一种不依赖训练时间、维度或其他损失函数特性的新泛化界限。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解过参数化模型在训练过程中的泛化差距，尤其是在使用马尔可夫随机训练算法（如Langevin动力学）时，如何在不依赖传统因素（如训练时间、模型维度等）的情况下，提供更普适的泛化保证。

Method: 方法基于Langevin动力学，这是一种带有高斯噪声的梯度下降方法，通过分析具有Gibbs式稳态分布的马尔可夫过程，证明了泛化差距的界限。

Result: 结果表明，泛化差距在任何训练时间都可以被界限为√(βE[L(θ0)] + log(1/δ))/N，其中N是样本大小，δ是置信水平，且E[L(θ0)]=O(1)。这一界限不依赖于训练时间、模型维度或梯度范数等传统因素。

Conclusion: 论文通过简单的证明展示了马尔可夫训练过程的泛化能力，其关键在于初始分布的边际散度保持有界，这由广义热力学第二定律暗示。这一发现为理解过参数化模型的泛化提供了新的视角。

Abstract: We analyze the generalization gap (gap between the training and test errors)
when training a potentially over-parametrized model using a Markovian
stochastic training algorithm, initialized from some distribution $\theta_0
\sim p_0$. We focus on Langevin dynamics with a positive temperature
$\beta^{-1}$, i.e. gradient descent on a training loss $L$ with infinitesimal
step size, perturbed with $\beta^{-1}$-variances Gaussian noise, and lightly
regularized or bounded. There, we bound the generalization gap, at any time
during training, by $\sqrt{(\beta\mathbb{E} L (\theta_0) + \log(1/\delta))/N}$
with probability $1-\delta$ over the dataset, where $N$ is the sample size, and
$\mathbb{E} L (\theta_0) =O(1)$ with standard initialization scaling. In
contrast to previous guarantees, we have no dependence on either training time
or reliance on mixing, nor a dependence on dimensionality, gradient norms, or
any other properties of the loss or model. This guarantee follows from a
general analysis of any Markov process-based training that has a Gibbs-style
stationary distribution. The proof is surprisingly simple, once we observe that
the marginal distribution divergence from initialization remains bounded, as
implied by a generalized second law of thermodynamics.

</details>


### [514] [CMoS: Rethinking Time Series Prediction Through the Lens of Chunk-wise Spatial Correlations](https://arxiv.org/abs/2505.19090)
*Haotian Si,Changhua Pei,Jianhui Li,Dan Pei,Gaogang Xie*

Main category: cs.LG

TL;DR: CMoS是一个超轻量级时间序列预测模型，通过直接建模时间序列块间的空间相关性，并引入相关性混合技术和周期性注入技术，以极少的参数实现高性能预测。


<details>
  <summary>Details</summary>
Motivation: 近年来轻量级时间序列预测模型的进展表明，时间序列预测任务可能具有内在的简单性。本文旨在提出一种超轻量级模型，以更少的参数实现高性能预测。

Method: CMoS直接建模时间序列块间的空间相关性，引入相关性混合技术捕捉多样化的空间相关性，并可选周期性注入技术以加速收敛。

Result: 实验结果表明，CMoS仅使用轻量级模型DLinear 1%的参数，便在多个数据集上优于现有最先进模型，且学习到的权重具有高度可解释性。

Conclusion: CMoS以极少的参数实现了高性能时间序列预测，同时提供了对时间结构的可解释性洞察，适用于实际应用场景。

Abstract: Recent advances in lightweight time series forecasting models suggest the
inherent simplicity of time series forecasting tasks. In this paper, we present
CMoS, a super-lightweight time series forecasting model. Instead of learning
the embedding of the shapes, CMoS directly models the spatial correlations
between different time series chunks. Additionally, we introduce a Correlation
Mixing technique that enables the model to capture diverse spatial correlations
with minimal parameters, and an optional Periodicity Injection technique to
ensure faster convergence. Despite utilizing as low as 1% of the lightweight
model DLinear's parameters count, experimental results demonstrate that CMoS
outperforms existing state-of-the-art models across multiple datasets.
Furthermore, the learned weights of CMoS exhibit great interpretability,
providing practitioners with valuable insights into temporal structures within
specific application scenarios.

</details>


### [515] [Towards Robust Influence Functions with Flat Validation Minima](https://arxiv.org/abs/2505.19097)
*Xichen Ye,Yifan Wu,Weizhong Zhang,Cheng Jin,Yifan Chen*

Main category: cs.LG

TL;DR: 本文提出了一种针对平坦验证极小值设计的新的影响函数估计形式，解决了现有方法在深度神经网络中因验证风险尖锐性导致的影响估计不准确问题。


<details>
  <summary>Details</summary>
Motivation: 现有影响函数（IF）方法在深度神经网络中，尤其是在噪声训练数据上，往往无法提供可靠的影响估计。研究发现，问题不在于参数变化估计的不准确，而在于损失变化估计的不足，特别是验证风险的尖锐性。

Method: 本文建立了影响估计误差、验证集风险及其尖锐性之间的理论联系，强调了平坦验证极小值对准确影响估计的重要性，并提出了一种专门针对平坦验证极小值设计的新影响函数估计形式。

Result: 在多个任务上的实验结果验证了所提方法的优越性。

Conclusion: 通过理论分析和实验验证，本文证明了平坦验证极小值对准确影响估计的关键作用，并提出了一种新的影响函数估计形式，显著提高了影响估计的准确性。

Abstract: The Influence Function (IF) is a widely used technique for assessing the
impact of individual training samples on model predictions. However, existing
IF methods often fail to provide reliable influence estimates in deep neural
networks, particularly when applied to noisy training data. This issue does not
stem from inaccuracies in parameter change estimation, which has been the
primary focus of prior research, but rather from deficiencies in loss change
estimation, specifically due to the sharpness of validation risk. In this work,
we establish a theoretical connection between influence estimation error,
validation set risk, and its sharpness, underscoring the importance of flat
validation minima for accurate influence estimation. Furthermore, we introduce
a novel estimation form of Influence Function specifically designed for flat
validation minima. Experimental results across various tasks validate the
superiority of our approach.

</details>


### [516] [Latent Mamba Operator for Partial Differential Equations](https://arxiv.org/abs/2505.19105)
*Karn Tiwari,Niladri Dutta,N M Anoop Krishnan,Prathosh A P*

Main category: cs.LG

TL;DR: LaMO结合状态空间模型与神经算子，显著提升高维PDE求解效率，性能超越基线32.3%。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子在处理高维PDE时存在可扩展性差、计算成本高及长程依赖捕捉困难等问题，需新型解决方案。

Method: 提出LaMO，在潜空间融合状态空间模型的高效性与神经算子的核积分表达能力，并建立二者理论关联。

Result: 在多种PDE基准测试中实现SOTA性能，解算子近似误差降低32.3%，尤其擅长复杂连续动力学建模。

Conclusion: LaMO为高维PDE求解提供了高效可扩展的框架，其理论创新与性能优势推动了该领域发展。

Abstract: Neural operators have emerged as powerful data-driven frameworks for solving
Partial Differential Equations (PDEs), offering significant speedups over
numerical methods. However, existing neural operators struggle with scalability
in high-dimensional spaces, incur high computational costs, and face challenges
in capturing continuous and long-range dependencies in PDE dynamics. To address
these limitations, we introduce the Latent Mamba Operator (LaMO), which
integrates the efficiency of state-space models (SSMs) in latent space with the
expressive power of kernel integral formulations in neural operators. We also
establish a theoretical connection between state-space models (SSMs) and the
kernel integral of neural operators. Extensive experiments across diverse PDE
benchmarks on regular grids, structured meshes, and point clouds covering solid
and fluid physics datasets, LaMOs achieve consistent state-of-the-art (SOTA)
performance, with a 32.3\% improvement over existing baselines in solution
operator approximation, highlighting its efficacy in modeling complex PDE
solutions.

</details>


### [517] [Optimization-Inspired Few-Shot Adaptation for Large Language Models](https://arxiv.org/abs/2505.19107)
*Boyan Gao,Xin Wang,Yibo Yang,David Clifton*

Main category: cs.LG

TL;DR: 本文提出了一种基于优化启发的少样本适应方法（OFA），通过重新解释LLM的前向传递为优化过程，解决了现有方法在少样本场景下的计算开销和过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在实际应用中表现优异，但在少样本场景下，传统的微调方法需要大量数据和计算资源，现有方法如上下文学习和参数高效微调（PEFT）存在计算开销大或易过拟合的问题。

Method: 将LLM的前向传递重新解释为优化过程，提出OFA方法，通过学习预条件器而不引入额外可训练参数，并基于收敛边界优化目标，同时引导优化路径朝向平坦的局部最小值。

Result: 实验表明，OFA方法在多种少样本适应任务上优于现有方法，克服了上下文学习和PEFT的局限性。

Conclusion: OFA方法通过优化启发的设计，有效解决了少样本场景下LLM适应的关键问题，展现了优越的性能。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance in
real-world applications. However, adapting LLMs to novel tasks via fine-tuning
often requires substantial training data and computational resources that are
impractical in few-shot scenarios. Existing approaches, such as in-context
learning and Parameter-Efficient Fine-Tuning (PEFT), face key limitations:
in-context learning introduces additional inference computational overhead with
limited performance gains, while PEFT models are prone to overfitting on the
few demonstration examples. In this work, we reinterpret the forward pass of
LLMs as an optimization process, a sequence of preconditioned gradient descent
steps refining internal representations. Based on this connection, we propose
Optimization-Inspired Few-Shot Adaptation (OFA), integrating a parameterization
that learns preconditioners without introducing additional trainable
parameters, and an objective that improves optimization efficiency by learning
preconditioners based on a convergence bound, while simultaneously steering the
optimization path toward the flat local minimum. Our method overcomes both
issues of ICL-based and PEFT-based methods, and demonstrates superior
performance over the existing methods on a variety of few-shot adaptation tasks
in experiments.

</details>


### [518] [FP4 All the Way: Fully Quantized Training of LLMs](https://arxiv.org/abs/2505.19115)
*Brian Chmiel,Maxim Fishman,Ron Banner,Daniel Soudry*

Main category: cs.LG

TL;DR: 首次实现全程4位浮点(FQT)训练大语言模型(LLM)，在2000亿token数据集上验证FP4量化训练可行性，7B参数模型性能媲美BF16基线。


<details>
  <summary>Details</summary>
Motivation: 探索极低精度(FP4)在LLM训练中的可行性，以大幅降低计算和存储开销，同时保持模型性能。

Method: 采用NVFP4格式(16个E2M1 FP4值共享E4M3缩放因子)，前向使用就近舍入，反向/更新使用随机舍入，并确定梯度范数与量化噪声的理论阈值。

Result: 在256块Intel Gaudi2加速器上训练的7B模型，下游任务性能与BF16基线相当，梯度噪声阈值约为√3倍量化噪声。

Conclusion: FP4量化训练是实用高效的大规模LLM训练方案，开源实现见https://github.com/Anonymous1252022/fp4-all-the-way。

Abstract: We demonstrate, for the first time, fully quantized training (FQT) of large
language models (LLMs) using predominantly 4-bit floating-point (FP4) precision
for weights, activations, and gradients on datasets up to 200 billion tokens.
We extensively investigate key design choices for FP4, including block sizes,
scaling formats, and rounding methods. Our analysis shows that the NVFP4
format, where each block of 16 FP4 values (E2M1) shares a scale represented in
E4M3, provides optimal results. We use stochastic rounding for backward and
update passes and round-to-nearest for the forward pass to enhance stability.
Additionally, we identify a theoretical and empirical threshold for effective
quantized training: when the gradient norm falls below approximately $\sqrt{3}$
times the quantization noise, quantized training becomes less effective.
Leveraging these insights, we successfully train a 7-billion-parameter model on
256 Intel Gaudi2 accelerators. The resulting FP4-trained model achieves
downstream task performance comparable to a standard BF16 baseline, confirming
that FP4 training is a practical and highly efficient approach for large-scale
LLM training. A reference implementation is supplied in
https://github.com/Anonymous1252022/fp4-all-the-way .

</details>


### [519] [Fast and Accurate Power Load Data Completion via Regularization-optimized Low-Rank Factorization](https://arxiv.org/abs/2505.19133)
*Yan Xia,Hao Feng,Hongwei Sun,Junjie Wang,Qicong Hu*

Main category: cs.LG

TL;DR: 提出了一种基于PID控制器的自适应正则化低秩分解方法，用于提升电力负荷数据缺失值恢复的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 低秩表示学习在电力负荷数据恢复中表现优异，但传统方法的正则化参数通常固定或手动调整，导致泛化能力有限或收敛速度慢。

Method: 提出正则化优化的低秩分解方法，引入PID控制器自适应调整正则化系数，同时保持随机梯度下降的计算效率。

Result: 在真实电力负荷数据集上的实验表明，该方法在填补精度和训练效率上均优于现有基线。

Conclusion: 该方法通过自适应调整正则化参数，显著提升了低秩分解模型的性能和实用性。

Abstract: Low-rank representation learning has emerged as a powerful tool for
recovering missing values in power load data due to its ability to exploit the
inherent low-dimensional structures of spatiotemporal measurements. Among
various techniques, low-rank factorization models are favoured for their
efficiency and interpretability. However, their performance is highly sensitive
to the choice of regularization parameters, which are typically fixed or
manually tuned, resulting in limited generalization capability or slow
convergence in practical scenarios. In this paper, we propose a
Regularization-optimized Low-Rank Factorization, which introduces a
Proportional-Integral-Derivative controller to adaptively adjust the
regularization coefficient. Furthermore, we provide a detailed algorithmic
complexity analysis, showing that our method preserves the computational
efficiency of stochastic gradient descent while improving adaptivity.
Experimental results on real-world power load datasets validate the superiority
of our method in both imputation accuracy and training efficiency compared to
existing baselines.

</details>


### [520] [ADGSyn: Dual-Stream Learning for Efficient Anticancer Drug Synergy Prediction](https://arxiv.org/abs/2505.19144)
*Yuxuan Nie,Yutong Song,Hong Peng*

Main category: cs.LG

TL;DR: ADGSyn是一种预测药物协同作用的新方法，通过共享投影矩阵、注意力机制和优化图操作，显著提高了预测效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 药物组合在癌症治疗中至关重要，但实验筛选组合空间巨大，因此需要高效的计算方法来预测有潜力的药物组合。

Method: ADGSyn结合共享投影矩阵与注意力机制实现跨药物特征对齐，采用自动混合精度优化图操作减少内存消耗，并通过残差通路稳定梯度传播。

Result: 在包含13,243种药物-细胞系组合的O'Neil数据集上，ADGSyn性能优于八种基线方法，并在单GPU上支持256个分子图的批量处理。

Conclusion: ADGSyn为计算肿瘤学中的药物协同预测设定了新的效率标准，显著提升了预测性能和计算效率。

Abstract: Drug combinations play a critical role in cancer therapy by significantly
enhancing treatment efficacy and overcoming drug resistance. However, the
combinatorial space of possible drug pairs grows exponentially, making
experimental screening highly impractical. Therefore, developing efficient
computational methods to predict promising drug combinations and guide
experimental validation is of paramount importance. In this work, we propose
ADGSyn, an innovative method for predicting drug synergy. The key components of
our approach include: (1) shared projection matrices combined with attention
mechanisms to enable cross-drug feature alignment; (2) automatic mixed
precision (AMP)-optimized graph operations that reduce memory consumption by
40\% while accelerating training speed threefold; and (3) residual pathways
stabilized by LayerNorm to ensure stable gradient propagation during training.
Evaluated on the O'Neil dataset containing 13,243 drug--cell line combinations,
ADGSyn demonstrates superior performance over eight baseline methods. Moreover,
the framework supports full-batch processing of up to 256 molecular graphs on a
single GPU, setting a new standard for efficiency in drug synergy prediction
within the field of computational oncology.

</details>


### [521] [Computational Inertia as a Conserved Quantity in Frictionless and Damped Learning Dynamics](https://arxiv.org/abs/2505.19171)
*Atahan Karagoz*

Main category: cs.LG

TL;DR: 该论文提出了连续时间优化动力学中的守恒量——计算惯性，定义为动能和势能之和，在理想无摩擦训练下保持不变，并分析了其在阻尼和随机扰动下的衰减行为。


<details>
  <summary>Details</summary>
Motivation: 研究连续时间优化动力学中的守恒量，以提供一种新的视角来理解学习轨迹，并为分析收敛性、稳定性和训练几何提供理论工具。

Method: 通过定义计算惯性（动能与势能之和），形式化其守恒定律，并推导其在阻尼和随机扰动下的解析衰减，最后在合成系统中验证其行为。

Result: 计算惯性在理想无摩擦训练下保持不变，但在阻尼和随机扰动下会衰减。这一守恒量为解释学习轨迹提供了简洁的视角。

Conclusion: 计算惯性是一个有用的守恒量，能够帮助理解优化动力学的行为，并为理论分析提供了新的工具。

Abstract: We identify a conserved quantity in continuous-time optimization dynamics,
termed computational inertia. Defined as the sum of kinetic energy (parameter
velocity) and potential energy (loss), this scalar remains invariant under
idealized, frictionless training. We formalize this conservation law, derive
its analytic decay under damping and stochastic perturbations, and demonstrate
its behavior in a synthetic system. The invariant offers a compact lens for
interpreting learning trajectories, and may inform theoretical tools for
analyzing convergence, stability, and training geometry.

</details>


### [522] [Federated Learning: From Theory to Practice](https://arxiv.org/abs/2505.19183)
*A. Jung*

Main category: cs.LG

TL;DR: 本书介绍了联邦学习（FL）系统的构建与理解，重点在于个性化模型训练，通过设备间的协作保持数据隐私。


<details>
  <summary>Details</summary>
Motivation: 联邦学习解决了因隐私、法规或技术原因无法集中数据的问题，允许设备在本地数据上协作训练模型。

Method: 将现实FL系统建模为设备网络，节点代表设备，边表示通信链接和数据相似性，通过广义总变差最小化（GTVMin）实现个性化模型训练。

Result: 提出了一种数学原理严谨且实用的方法，确保相似学习任务的设备学习相似的模型参数。

Conclusion: 本书为设计可扩展、保护隐私的FL系统提供了理论与实践指导，适合学生、工程师和研究人员学习。

Abstract: This book offers a hands-on introduction to building and understanding
federated learning (FL) systems. FL enables multiple devices -- such as
smartphones, sensors, or local computers -- to collaboratively train machine
learning (ML) models, while keeping their data private and local. It is a
powerful solution when data cannot or should not be centralized due to privacy,
regulatory, or technical reasons. The book is designed for students, engineers,
and researchers who want to learn how to design scalable, privacy preserving FL
systems. Our main focus is on personalization: enabling each device to train
its own model while still benefiting from collaboration with relevant devices.
This is achieved by leveraging similarities between (the learning tasks
associated with) devices that are encoded by the weighted edges (or links) of a
federated learning network (FL network). The key idea is to represent
real-world FL systems as networks of devices, where nodes correspond to device
and edges represent communication links and data similarities between them. The
training of personalized models for these devices can be naturally framed as a
distributed optimization problem. This optimization problem is referred to as
generalized total variation minimization (GTVMin) and ensures that devices with
similar learning tasks learn similar model parameters. Our approach is both
mathematically principled and practically motivated. While we introduce some
advanced ideas from optimization theory and graph-based learning, we aim to
keep the book accessible. Readers are guided through the core ideas step by
step, with intuitive explanations.

</details>


### [523] [Chordless Structure: A Pathway to Simple and Expressive GNNs](https://arxiv.org/abs/2505.19188)
*Hongxu Pan,Shuxian Hu,Mo Zhou,Zhibin Wang,Rong Gu,Chen Tian,Kun Yang,Sheng Zhong*

Main category: cs.LG

TL;DR: 论文提出了一种基于无弦结构的图神经网络CSGNN，通过忽略弦来提升计算效率和表达能力，实验证明其优于现有GNN。


<details>
  <summary>Details</summary>
Motivation: 现有方法在增强图神经网络表达能力时，要么计算成本高，要么缺乏可证明的表达能力。研究发现弦结构增加了图复杂性但贡献有限，而无弦结构更高效有效。

Method: 提出Chordless Structure-based Graph Neural Network (CSGNN)，在利用环信息时忽略弦，以多项式复杂度实现更强的表达能力。

Result: 实验表明CSGNN在多种图任务上优于现有GNN，计算成本更低，性能优于3-WL表达能力的GNN。

Conclusion: CSGNN通过无弦结构设计，在提升表达能力的同时降低了计算成本，为图神经网络提供了更高效的解决方案。

Abstract: Researchers have proposed various methods of incorporating more structured
information into the design of Graph Neural Networks (GNNs) to enhance their
expressiveness. However, these methods are either computationally expensive or
lacking in provable expressiveness. In this paper, we observe that the chords
increase the complexity of the graph structure while contributing little useful
information in many cases. In contrast, chordless structures are more efficient
and effective for representing the graph. Therefore, when leveraging the
information of cycles, we choose to omit the chords. Accordingly, we propose a
Chordless Structure-based Graph Neural Network (CSGNN) and prove that its
expressiveness is strictly more powerful than the k-hop GNN (KPGNN) with
polynomial complexity. Experimental results on real-world datasets demonstrate
that CSGNN outperforms existing GNNs across various graph tasks while incurring
lower computational costs and achieving better performance than the GNNs of
3-WL expressiveness.

</details>


### [524] [I2MoE: Interpretable Multimodal Interaction-aware Mixture-of-Experts](https://arxiv.org/abs/2505.19190)
*Jiayi Xin,Sukwon Yun,Jie Peng,Inyoung Choi,Jenna L. Ballard,Tianlong Chen,Qi Long*

Main category: cs.LG

TL;DR: 提出I2MoE框架，通过显式建模多模态交互并提供解释性，提升模态融合效果。


<details>
  <summary>Details</summary>
Motivation: 传统融合方法无法处理模态间异构交互且缺乏解释性，限制了多模态学习效果。

Method: I2MoE结合弱监督交互损失学习多模态交互，并通过重加权模型提供样本和数据集级解释。

Result: 在医疗和通用多模态数据集上验证，I2MoE灵活兼容不同融合技术并持续提升任务性能。

Conclusion: I2MoE有效增强模态融合能力，同时提供实用解释性，适用于多种现实场景。

Abstract: Modality fusion is a cornerstone of multimodal learning, enabling information
integration from diverse data sources. However, vanilla fusion methods are
limited by (1) inability to account for heterogeneous interactions between
modalities and (2) lack of interpretability in uncovering the multimodal
interactions inherent in the data. To this end, we propose I2MoE (Interpretable
Multimodal Interaction-aware Mixture of Experts), an end-to-end MoE framework
designed to enhance modality fusion by explicitly modeling diverse multimodal
interactions, as well as providing interpretation on a local and global level.
First, I2MoE utilizes different interaction experts with weakly supervised
interaction losses to learn multimodal interactions in a data-driven way.
Second, I2MoE deploys a reweighting model that assigns importance scores for
the output of each interaction expert, which offers sample-level and
dataset-level interpretation. Extensive evaluation of medical and general
multimodal datasets shows that I2MoE is flexible enough to be combined with
different fusion techniques, consistently improves task performance, and
provides interpretation across various real-world scenarios. Code is available
at https://github.com/Raina-Xin/I2MoE.

</details>


### [525] [Interpretable Graph Learning Over Sets of Temporally-Sparse Data](https://arxiv.org/abs/2505.19193)
*Andrea Zerio,Maya Bechler-Speicher,Maor Huri,Marie Vibeke Vestergaard,Ran Gilad-Bachrach,Tine Jess,Samir Bhatt,Aleksejs Sazonovs*

Main category: cs.LG

TL;DR: 提出GMAN模型处理多信号不规则异步时序数据，在医疗和假新闻检测任务中表现优异且具可解释性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的医疗等多领域数据常呈现不规则、异步的时序特性，传统方法难以有效处理此类稀疏异构信号。

Method: 提出图混合加性网络(GMAN)，通过可解释的架构设计学习不规则时序信号集。

Result: 在院内死亡率预测任务中AUROC提升4%，假新闻检测任务中验证了模型灵活性和可解释性。

Conclusion: GMAN为不规则时序数据提供了高性能、可解释的解决方案，具有实际应用价值。

Abstract: Real-world medical data often includes measurements from multiple signals
that are collected at irregular and asynchronous time intervals. For example,
different types of blood tests can be measured at different times and
frequencies, resulting in fragmented and unevenly scattered temporal data.
Similar issues of irregular sampling of different attributes occur in other
domains, such as monitoring of large systems using event log files or the
spread of fake news on social networks. Effectively learning from such data
requires models that can handle sets of temporally sparse and heterogeneous
signals. In this paper, we propose Graph Mixing Additive Networks (GMAN), a
novel and interpretable-by-design model for learning over irregular sets of
temporal signals. Our method achieves state-of-the-art performance in
real-world medical tasks, including a 4-point increase in the AUROC score of
in-hospital mortality prediction, compared to existing methods. We further
showcase GMAN's flexibility by applying it to a fake news detection task. We
demonstrate how its interpretability capabilities, including node-level,
graph-level, and subset-level importance, allow for transition phases detection
and gaining medical insights with real-world high-stakes implications. Finally,
we provide theoretical insights on GMAN expressive power.

</details>


### [526] [Curvature Dynamic Black-box Attack: revisiting adversarial robustness via dynamic curvature estimation](https://arxiv.org/abs/2505.19194)
*Peiran Sun*

Main category: cs.LG

TL;DR: 该论文提出了一种新的查询高效方法——动态曲率估计(DCE)，用于在黑盒设置中估计决策边界曲率，并发现决策边界曲率与对抗鲁棒性之间的统计关联。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型易受对抗攻击，现有方法多关注损失函数等内部参数的曲率，而非决策边界曲率。论文旨在通过估计决策边界曲率，提升对抗攻击的效果并理解模型鲁棒性。

Method: 基于CGBA黑盒攻击方法，提出动态曲率估计(DCE)来估计决策边界曲率，并进一步提出曲率动态黑盒攻击(CDBA)方法。

Result: 通过在不同分类器上应用DCE，统计发现决策边界曲率与对抗鲁棒性之间存在关联，且CDBA方法表现出更优的攻击性能。

Conclusion: 论文提出的DCE和CDBA方法不仅有效估计了决策边界曲率，还验证了其与模型鲁棒性的关联，为对抗攻击和防御提供了新思路。

Abstract: Adversarial attack reveals the vulnerability of deep learning models. For
about a decade, countless attack and defense methods have been proposed,
leading to robustified classifiers and better understanding of models. Among
these methods, curvature-based approaches have attracted attention because it
is assumed that high curvature may give rise to rough decision boundary.
However, the most commonly used \textit{curvature} is the curvature of loss
function, scores or other parameters from within the model as opposed to
decision boundary curvature, since the former can be relatively easily formed
using second order derivative. In this paper, we propose a new query-efficient
method, dynamic curvature estimation(DCE), to estimate the decision boundary
curvature in a black-box setting. Our approach is based on CGBA, a black-box
adversarial attack. By performing DCE on a wide range of classifiers, we
discovered, statistically, a connection between decision boundary curvature and
adversarial robustness. We also propose a new attack method, curvature dynamic
black-box attack(CDBA) with improved performance using the dynamically
estimated curvature.

</details>


### [527] [OptiMindTune: A Multi-Agent Framework for Intelligent Hyperparameter Optimization](https://arxiv.org/abs/2505.19205)
*Meher Bhaskar Madiraju,Meher Sai Preetam Madiraju*

Main category: cs.LG

TL;DR: 该论文提出了一种名为OptiMindTune的多智能体框架，用于高效优化机器学习模型的超参数，通过三个专门AI代理的协作，比传统方法更快速、稳健地找到最优配置。


<details>
  <summary>Details</summary>
Motivation: 传统超参数优化方法在处理高维度、复杂依赖和计算成本时存在困难，因此需要一种更智能、高效的解决方案。

Method: OptiMindTune利用三个由Google Gemini模型驱动的AI代理（推荐代理、评估代理和决策代理）协作完成超参数优化任务，结合大型语言模型和自适应搜索技术。

Result: OptiMindTune能够比现有单智能体或整体方法更快速、稳健地收敛到最优超参数配置。

Conclusion: 多智能体范式为解决现代机器学习模型调优的复杂性提供了一条有前景的途径。

Abstract: Hyperparameter optimization (HPO) is a critical yet challenging aspect of
machine learning model development, significantly impacting model performance
and generalization. Traditional HPO methods often struggle with high
dimensionality, complex interdependencies, and computational expense. This
paper introduces OptiMindTune, a novel multi-agent framework designed to
intelligently and efficiently optimize hyperparameters. OptiMindTune leverages
the collaborative intelligence of three specialized AI agents -- a Recommender
Agent, an Evaluator Agent, and a Decision Agent -- each powered by Google's
Gemini models. These agents address distinct facets of the HPO problem, from
model selection and hyperparameter suggestion to robust evaluation and
strategic decision-making. By fostering dynamic interactions and knowledge
sharing, OptiMindTune aims to converge to optimal hyperparameter configurations
more rapidly and robustly than existing single-agent or monolithic approaches.
Our framework integrates principles from advanced large language models, and
adaptive search to achieve scalable and intelligent AutoML. We posit that this
multi-agent paradigm offers a promising avenue for tackling the increasing
complexity of modern machine learning model tuning.

</details>


### [528] [LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models](https://arxiv.org/abs/2505.19223)
*Fengqi Zhu,Rongzhen Wang,Shen Nie,Xiaolu Zhang,Chunwei Wu,Jun Hu,Jun Zhou,Jianfei Chen,Yankai Lin,Ji-Rong Wen,Chongxuan Li*

Main category: cs.LG

TL;DR: 本文提出VRPO框架，通过降低ELBO估计方差来优化掩码扩散模型（如LLaDA）的人类偏好对齐，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型（如LLaDA）在语言建模中表现优异，但缺乏通过强化学习与人类偏好对齐的研究，主要因ELBO估计方差过高导致优化困难。

Method: 提出VRPO框架，理论分析ELBO估计的方差，并引入无偏方差缩减策略（如最优蒙特卡洛预算分配和对立采样），优化MDM对齐性能。

Result: 应用VRPO的LLaDA 1.5在数学（GSM8K +4.7）、代码（HumanEval +3.0）和对齐基准（IFEval +4.0）上显著优于前代模型，数学性能尤其突出。

Conclusion: VRPO有效解决了MDM偏好对齐的方差问题，LLaDA 1.5在多项任务中表现卓越，展示了框架的实用性和竞争力。

Abstract: While Masked Diffusion Models (MDMs), such as LLaDA, present a promising
paradigm for language modeling, there has been relatively little effort in
aligning these models with human preferences via reinforcement learning. The
challenge primarily arises from the high variance in Evidence Lower Bound
(ELBO)-based likelihood estimates required for preference optimization. To
address this issue, we propose Variance-Reduced Preference Optimization (VRPO),
a framework that formally analyzes the variance of ELBO estimators and derives
bounds on both the bias and variance of preference optimization gradients.
Building on this theoretical foundation, we introduce unbiased variance
reduction strategies, including optimal Monte Carlo budget allocation and
antithetic sampling, that significantly improve the performance of MDM
alignment. We demonstrate the effectiveness of VRPO by applying it to LLaDA,
and the resulting model, LLaDA 1.5, outperforms its SFT-only predecessor
consistently and significantly across mathematical (GSM8K +4.7), code
(HumanEval +3.0, MBPP +1.8), and alignment benchmarks (IFEval +4.0, Arena-Hard
+4.3). Furthermore, LLaDA 1.5 demonstrates a highly competitive mathematical
performance compared to strong language MDMs and ARMs. Project page:
https://ml-gsai.github.io/LLaDA-1.5-Demo/.

</details>


### [529] [Scaling Laws for Gradient Descent and Sign Descent for Linear Bigram Models under Zipf's Law](https://arxiv.org/abs/2505.19227)
*Frederik Kunstner,Francis Bach*

Main category: cs.LG

TL;DR: 论文研究了基于Zipf定律的文本数据分布对梯度下降和符号下降优化器性能的影响，发现梯度下降在α=1时表现最差，而符号下降在大词汇量下表现更优。


<details>
  <summary>Details</summary>
Motivation: 近期研究发现，基于Transformer的语言模型在训练首尾层时面临优化困难，尤其是在处理遵循Zipf定律的文本数据时。本文旨在探讨数据分布对训练性能的具体影响。

Method: 研究采用线性双字母模型进行下一词预测，假设词频遵循幂律分布π_k ∝ 1/k^α，分析梯度下降和符号下降（作为Adam的代理）在不同α值下的优化性能。

Result: 结果显示，当α=1（如文本数据）时，梯度下降需要几乎与维度线性相关的迭代次数才能达到较小相对误差，而符号下降的迭代次数仅与维度的平方根相关，显著提升了大规模词汇表的训练效率。

Conclusion: 研究表明，数据分布的尾部越重，优化问题越困难。符号下降在处理Zipf分布数据时表现优于梯度下降，尤其适用于大词汇量场景。

Abstract: Recent works have highlighted optimization difficulties faced by gradient
descent in training the first and last layers of transformer-based language
models, which are overcome by optimizers such as Adam. These works suggest that
the difficulty is linked to the heavy-tailed distribution of words in text
data, where the frequency of the $k$th most frequent word $\pi_k$ is
proportional to $1/k$, following Zipf's law. To better understand the impact of
the data distribution on training performance, we study a linear bigram model
for next-token prediction when the tokens follow a power law $\pi_k \propto
1/k^\alpha$ parameterized by the exponent $\alpha > 0$. We derive optimization
scaling laws for deterministic gradient descent and sign descent as a proxy for
Adam as a function of the exponent $\alpha$. Existing theoretical
investigations in scaling laws assume that the eigenvalues of the data decay as
a power law with exponent $\alpha > 1$. This assumption effectively makes the
problem ``finite dimensional'' as most of the loss comes from a few of the
largest eigencomponents. In comparison, we show that the problem is more
difficult when the data have heavier tails. The case $\alpha = 1$ as found in
text data is ``worst-case'' for gradient descent, in that the number of
iterations required to reach a small relative error scales almost linearly with
dimension. While the performance of sign descent also depends on the dimension,
for Zipf-distributed data the number of iterations scales only with the
square-root of the dimension, leading to a large improvement for large
vocabularies.

</details>


### [530] [CoreMatching: A Co-adaptive Sparse Inference Framework with Token and Neuron Pruning for Comprehensive Acceleration of Vision-Language Models](https://arxiv.org/abs/2505.19235)
*Qinsi Wang,Hancheng Ye,Ming-Yu Chung,Yudong Liu,Yueqian Lin,Martin Kuo,Mingyuan Ma,Jianyi Zhang,Yiran Chen*

Main category: cs.LG

TL;DR: 该论文首次系统研究了视觉语言模型中token稀疏性与神经元稀疏性之间的协同关系，提出CoreMatching框架，通过核心神经元与核心token的匹配机制显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型存在高推理成本问题，token稀疏性和神经元稀疏性虽能分别提升效率，但二者关系尚未被深入探索。论文质疑二者独立性的传统假设，旨在揭示其潜在协同机制。

Method: 提出CoreMatching框架：1) 发现推理关键神经元与token的相互强化现象；2) 设计核心神经元与核心token的匹配机制；3) 构建token与神经元稀疏性协同适应的稀疏推理系统。

Result: 在10个图像理解任务和3种硬件上超越SOTA：Titan Xp显卡实现5倍计算量压缩和10倍加速，代码已开源。

Conclusion: 揭示了视觉语言模型中两种稀疏性的深层协同关系，所提方法为高效推理提供了新范式，理论分析与实验验证了其优越性。

Abstract: Vision-Language Models (VLMs) excel across diverse tasks but suffer from high
inference costs in time and memory. Token sparsity mitigates inefficiencies in
token usage, while neuron sparsity reduces high-dimensional computations, both
offering promising solutions to enhance efficiency. Recently, these two
sparsity paradigms have evolved largely in parallel, fostering the prevailing
assumption that they function independently. However, a fundamental yet
underexplored question remains: Do they truly operate in isolation, or is there
a deeper underlying interplay that has yet to be uncovered? In this paper, we
conduct the first comprehensive investigation into this question. By
introducing and analyzing the matching mechanism between Core Neurons and Core
Tokens, we found that key neurons and tokens for inference mutually influence
and reinforce each other. Building on this insight, we propose CoreMatching, a
co-adaptive sparse inference framework, which leverages the synergy between
token and neuron sparsity to enhance inference efficiency. Through theoretical
analysis and efficiency evaluations, we demonstrate that the proposed method
surpasses state-of-the-art baselines on ten image understanding tasks and three
hardware devices. Notably, on the NVIDIA Titan Xp, it achieved 5x FLOPs
reduction and a 10x overall speedup. Code is released at
https://github.com/wangqinsi1/2025-ICML-CoreMatching/tree/main.

</details>


### [531] [Efficient Policy Optimization in Robust Constrained MDPs with Iteration Complexity Guarantees](https://arxiv.org/abs/2505.19238)
*Sourav Ganguly,Arnob Ghosh,Kishan Panaganti,Adam Wierman*

Main category: cs.LG

TL;DR: 该论文提出了一种新方法来解决在模型不匹配情况下的鲁棒约束马尔可夫决策问题（RCMDP），无需使用对偶方法或二进制搜索，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的控制系统设计需要约束决策以确保安全性，但仿真环境往往无法准确反映现实中的不确定性。论文旨在解决在模型不匹配情况下学习既能最大化累积奖励又能满足约束的策略的问题。

Method: 论文提出了一种新技术，通过最小化约束值函数来满足约束条件，同时在所有约束满足时最大化鲁棒奖励值函数。该方法避免了传统对偶方法和二进制搜索的使用。

Result: 该方法在O(ε⁻²)次迭代后能找到ε次优且可行的策略，计算时间比现有方法减少了至少4倍（小折扣因子γ）或6倍（大γ）。

Conclusion: 该研究为鲁棒约束决策问题提供了一种高效且实用的解决方案，显著提升了计算效率，适用于现实世界中的复杂控制任务。

Abstract: Constrained decision-making is essential for designing safe policies in
real-world control systems, yet simulated environments often fail to capture
real-world adversities. We consider the problem of learning a policy that will
maximize the cumulative reward while satisfying a constraint, even when there
is a mismatch between the real model and an accessible simulator/nominal model.
In particular, we consider the robust constrained Markov decision problem
(RCMDP) where an agent needs to maximize the reward and satisfy the constraint
against the worst possible stochastic model under the uncertainty set centered
around an unknown nominal model. Primal-dual methods, effective for standard
constrained MDP (CMDP), are not applicable here because of the lack of the
strong duality property. Further, one cannot apply the standard robust
value-iteration based approach on the composite value function either as the
worst case models may be different for the reward value function and the
constraint value function. We propose a novel technique that effectively
minimizes the constraint value function--to satisfy the constraints; on the
other hand, when all the constraints are satisfied, it can simply maximize the
robust reward value function. We prove that such an algorithm finds a policy
with at most $\epsilon$ sub-optimality and feasible policy after
$O(\epsilon^{-2})$ iterations. In contrast to the state-of-the-art method, we
do not need to employ a binary search, thus, we reduce the computation time by
at least 4x for smaller value of discount factor ($\gamma$) and by at least 6x
for larger value of $\gamma$.

</details>


### [532] [ActiveDPO: Active Direct Preference Optimization for Sample-Efficient Alignment](https://arxiv.org/abs/2505.19241)
*Xiaoqiang Lin,Arun Verma,Zhongxiang Dai,Daniela Rus,See-Kiong Ng,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: 提出ActiveDPO算法，通过理论支持的数据选择标准和非线性奖励函数，有效提升大语言模型对齐效率。


<details>
  <summary>Details</summary>
Motivation: 高质量的人类偏好数据集对大语言模型对齐至关重要，但收集成本高且资源密集，需要高效的数据选择方法。现有方法理论基础薄弱或依赖线性奖励函数假设。

Method: 提出ActiveDPO算法，利用理论支持的数据选择标准处理非线性奖励函数，并直接利用大语言模型参数化奖励模型进行数据选择。

Result: 实验表明，ActiveDPO在不同模型和数据集上均优于现有方法。

Conclusion: ActiveDPO通过考虑大语言模型对数据选择的影响，实现了更高效和有效的数据收集。

Abstract: The recent success of using human preferences to align large language models
(LLMs) has significantly improved their performance in various downstream tasks
like question answering, mathematical reasoning, and code generation. However,3
achieving effective LLM alignment depends on high-quality human preference
datasets. Collecting these datasets requires human preference annotation, which
is costly and resource-intensive, necessitating efficient active data selection
methods. Existing methods either lack a strong theoretical foundation or depend
on restrictive reward function assumptions (e.g., linearity). To this end, we
propose an algorithm, ActiveDPO, that uses a theoretically grounded data
selection criterion for non-linear reward functions while directly leveraging
the LLM itself to parameterize the reward model that is used for active data
selection. As a result, ActiveDPO explicitly accounts for the influence of LLM
on data selection, unlike methods that select the data without considering the
LLM that is being aligned, thereby leading to more effective and efficient data
collection. Extensive experiments show that ActiveDPO outperforms existing
methods across various models and datasets.

</details>


### [533] [To CoT or To Loop? A Formal Comparison Between Chain-of-Thought and Looped Transformers](https://arxiv.org/abs/2505.19245)
*Kevin Xu,Issei Sato*

Main category: cs.LG

TL;DR: 论文分析了Chain-of-Thought (CoT)和Looped Transformers在推理任务中的优缺点，指出Looped Transformers适合确定性任务，而CoT擅长组合结构推理。


<details>
  <summary>Details</summary>
Motivation: 尽管CoT和Looped Transformers在推理任务中表现优异，但它们的比较能力尚未被充分理解。本文旨在通过理论分析揭示它们各自的优势和局限。

Method: 通过形式化分析，比较了Looped Transformers在并行计算模拟和CoT在组合结构推理中的表现。

Result: Looped Transformers能高效模拟确定性任务的并行计算，而CoT在自可归约问题的近似推理中表现更优。

Conclusion: 研究结果提供了选择推理范式的实用线索，表明深度驱动递归更适合特定任务。

Abstract: Chain-of-Thought (CoT) and Looped Transformers have been shown to empirically
improve performance on reasoning tasks and to theoretically enhance
expressivity by recursively increasing the number of computational steps.
However, their comparative capabilities are still not well understood. In this
paper, we provide a formal analysis of their respective strengths and
limitations. We show that Looped Transformers can efficiently simulate parallel
computations for deterministic tasks, which we formalize as evaluation over
directed acyclic graphs. In contrast, CoT with stochastic decoding excels at
approximate inference for compositional structures, namely self-reducible
problems. These separations suggest the tasks for which depth-driven recursion
is more suitable, thereby offering practical cues for choosing between
reasoning paradigms.

</details>


### [534] [Improving Value Estimation Critically Enhances Vanilla Policy Gradient](https://arxiv.org/abs/2505.19247)
*Tao Wang,Ruipeng Zhang,Sicun Gao*

Main category: cs.LG

TL;DR: 论文挑战了传统观点，认为在策略梯度算法中，增强价值估计的准确性比近似信任区域更重要。通过增加每次迭代中的价值更新步骤，普通策略梯度算法可以达到或超过PPO的性能。


<details>
  <summary>Details</summary>
Motivation: 现代策略梯度算法（如TRPO和PPO）在许多强化学习任务中表现优于普通策略梯度算法。传统观点认为近似信任区域的实施在实践中导致策略的稳定改进。本文质疑这一观点，并探讨更关键的因素是否是每次迭代中更多价值更新步骤带来的价值估计准确性提升。

Method: 通过增加每次迭代中的价值更新步骤数，改进普通策略梯度算法，而不依赖复杂的近似信任区域机制。

Result: 改进后的普通策略梯度算法在所有标准连续控制基准环境中表现与PPO相当或更好，且对超参数选择更加鲁棒。

Conclusion: 研究表明，增强价值估计准确性比实施近似信任区域更为关键。这一发现可能使强化学习算法更有效且易于使用。

Abstract: Modern policy gradient algorithms, such as TRPO and PPO, outperform vanilla
policy gradient in many RL tasks. Questioning the common belief that enforcing
approximate trust regions leads to steady policy improvement in practice, we
show that the more critical factor is the enhanced value estimation accuracy
from more value update steps in each iteration. To demonstrate, we show that by
simply increasing the number of value update steps per iteration, vanilla
policy gradient itself can achieve performance comparable to or better than PPO
in all the standard continuous control benchmark environments. Importantly,
this simple change to vanilla policy gradient is significantly more robust to
hyperparameter choices, opening up the possibility that RL algorithms may still
become more effective and easier to use.

</details>


### [535] [VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use](https://arxiv.org/abs/2505.19255)
*Mingyuan Wu,Jingcheng Yang,Jize Jiang,Meitang Li,Kaizhuo Yan,Hanchao Yu,Minjia Zhang,Chengxiang Zhai,Klara Nahrstedt*

Main category: cs.LG

TL;DR: VTool-R1框架首次训练视觉语言模型生成多模态思维链，通过结合文本和中间视觉推理步骤，提升结构化视觉问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视觉语言模型中多局限于静态图像输入的文本推理，缺乏真正的多模态响应。测试时方法如Visual Sketchpad虽有视觉步骤，但缺乏训练机制。

Method: VTool-R1将基于Python的视觉编辑工具整合到强化学习微调过程中，使模型学会何时及如何生成有助于最终推理的视觉步骤，无需依赖过程监督。

Result: 实验表明，VTool-R1通过教导模型“用图像思考”并生成多模态思维链，显著提升了在图表和表格结构化视觉问答中的推理性能。

Conclusion: VTool-R1通过训练视觉语言模型生成多模态思维链，有效提升了复杂视觉任务的推理能力，展示了多模态推理的潜力。

Abstract: Reinforcement Learning Finetuning (RFT) has significantly advanced the
reasoning capabilities of large language models (LLMs) by enabling long chains
of thought, self-correction, and effective tool use. While recent works attempt
to extend RFT to vision-language models (VLMs), these efforts largely produce
text-only reasoning conditioned on static image inputs, falling short of true
multimodal reasoning in the response. In contrast, test-time methods like
Visual Sketchpad incorporate visual steps but lack training mechanisms.
  We introduce VTool-R1, the first framework that trains VLMs to generate
multimodal chains of thought by interleaving text and intermediate visual
reasoning steps. VTool-R1 integrates Python-based visual editing tools into the
RFT process, enabling VLMs to learn when and how to generate visual reasoning
steps that benefit final reasoning. Trained with outcome-based rewards tied to
task accuracy, our approach elicits strategic visual tool use for reasoning
without relying on process-based supervision. Experiments on structured visual
question answering over charts and tables show that VTool-R1 enhances reasoning
performance by teaching VLMs to "think with images" and generate multimodal
chain of thoughts with tools.

</details>


### [536] [Towards a Spatiotemporal Fusion Approach to Precipitation Nowcasting](https://arxiv.org/abs/2505.19258)
*Felipe Curcio,Pedro Castro,Augusto Fonseca,Rafaela Castro,Raquel Franco,Eduardo Ogasawara,Victor Stepanenko,Fabio Porto,Mariza Ferro,Eduardo Bezerra*

Main category: cs.LG

TL;DR: 该论文提出了一种结合气象站、雨量站、ERA5再分析数据和GFS数值预报的数据融合方法，利用STConvS2S深度学习架构进行降水临近预报，在里约热内卢都市区取得了F1分数0.2033的成果。


<details>
  <summary>Details</summary>
Motivation: 随着气象数据来源的多样化，需要高效的数据整合方法来提升天气预报和水文气象研究的准确性。

Method: 采用STConvS2S时空深度学习架构，整合气象站、雨量站、ERA5再分析数据和GFS数值预报数据，构建9x11网格数据集，并进行消融实验评估各数据源的贡献。

Result: 融合模型在1小时提前期的强降水预报（>25mm/h）中达到F1分数0.2033，并通过消融研究提出了优化推理策略。

Conclusion: 数据融合方法显著提升了降水临近预报性能，结合NWP和现场观测的推理策略具有实用价值。

Abstract: With the increasing availability of meteorological data from various sensors,
numerical models and reanalysis products, the need for efficient data
integration methods has become paramount for improving weather forecasts and
hydrometeorological studies. In this work, we propose a data fusion approach
for precipitation nowcasting by integrating data from meteorological and rain
gauge stations in Rio de Janeiro metropolitan area with ERA5 reanalysis data
and GFS numerical weather prediction. We employ the spatiotemporal deep
learning architecture called STConvS2S, leveraging a structured dataset
covering a 9 x 11 grid. The study spans from January 2011 to October 2024, and
we evaluate the impact of integrating three surface station systems. Among the
tested configurations, the fusion-based model achieves an F1-score of 0.2033
for forecasting heavy precipitation events (greater than 25 mm/h) at a one-hour
lead time. Additionally, we present an ablation study to assess the
contribution of each station network and propose a refined inference strategy
for precipitation nowcasting, integrating the GFS numerical weather prediction
(NWP) data with in-situ observations.

</details>


### [537] [Towards Large Reasoning Models for Agriculture](https://arxiv.org/abs/2505.19259)
*Hossein Zaremehrjerdi,Shreyan Ganguly,Ashlyn Rairdin,Elizabeth Tranel,Benjamin Feuer,Juan Ignacio Di Salvo,Srikanth Panthulugiri,Victoria Moser,Sarah Jones,Joscif G Raigne,Yanben Shen,Heidi M. Dornath,Aditya Balu,Adarsh Krishnamurthy,Asheesh K Singh,Arti Singh,Baskar Ganapathysubramanian,Chinmay Hegde,Soumik Sarkar*

Main category: cs.LG

TL;DR: 该论文提出AgReason基准和AgThoughts数据集，开发了AgThinker模型，验证了大型推理模型在农业决策中的优势。


<details>
  <summary>Details</summary>
Motivation: 传统大型语言模型在农业决策中表现不足，作者假设新型大型推理模型能更好地处理这种结构化、领域特定的推理问题。

Method: 引入AgReason基准和AgThoughts数据集，开发AgThinker模型，并在13种开源和专有模型上进行评估。

Result: 大型推理模型表现优于传统模型，最佳基线准确率为36%。AgThoughts数据集有效提升了模型在农业推理中的能力。

Conclusion: 研究表明大型推理模型在农业决策中具有潜力，但仍存在挑战，AgThinker模型可在消费级GPU上运行。

Abstract: Agricultural decision-making involves complex, context-specific reasoning,
where choices about crops, practices, and interventions depend heavily on
geographic, climatic, and economic conditions. Traditional large language
models (LLMs) often fall short in navigating this nuanced problem due to
limited reasoning capacity. We hypothesize that recent advances in large
reasoning models (LRMs) can better handle such structured, domain-specific
inference. To investigate this, we introduce AgReason, the first expert-curated
open-ended science benchmark with 100 questions for agricultural reasoning.
Evaluations across thirteen open-source and proprietary models reveal that LRMs
outperform conventional ones, though notable challenges persist, with the
strongest Gemini-based baseline achieving 36% accuracy. We also present
AgThoughts, a large-scale dataset of 44.6K question-answer pairs generated with
human oversight and equipped with synthetically generated reasoning traces.
Using AgThoughts, we develop AgThinker, a suite of small reasoning models that
can be run on consumer-grade GPUs, and show that our dataset can be effective
in unlocking agricultural reasoning abilities in LLMs. Our project page is
here: https://baskargroup.github.io/Ag_reasoning/

</details>


### [538] [Cellular Traffic Prediction via Byzantine-robust Asynchronous Federated Learning](https://arxiv.org/abs/2505.19263)
*Hui Ma,Kai Yang,Yang Jiao*

Main category: cs.LG

TL;DR: 该论文提出了一种基于分布鲁棒优化的异步差分联邦学习框架，用于解决网络流量预测中的数据隐私和模型鲁棒性问题，并在真实数据集上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统网络流量预测方法依赖集中式训练，存在延迟和隐私问题；现有联邦学习协议易受拜占庭攻击，影响模型鲁棒性。

Method: 采用异步差分联邦学习框架，结合局部差分隐私和正则化技术，提升模型在拜占庭攻击下的鲁棒性。

Result: 在三个真实数据集上的实验表明，所提分布式算法性能优于现有方法。

Conclusion: 该框架有效平衡了隐私保护和模型鲁棒性，为分布式环境下的网络流量预测提供了可行解决方案。

Abstract: Network traffic prediction plays a crucial role in intelligent network
operation. Traditional prediction methods often rely on centralized training,
necessitating the transfer of vast amounts of traffic data to a central server.
This approach can lead to latency and privacy concerns. To address these
issues, federated learning integrated with differential privacy has emerged as
a solution to improve data privacy and model robustness in distributed
settings. Nonetheless, existing federated learning protocols are vulnerable to
Byzantine attacks, which may significantly compromise model robustness.
Developing a robust and privacy-preserving prediction model in the presence of
Byzantine clients remains a significant challenge. To this end, we propose an
asynchronous differential federated learning framework based on
distributionally robust optimization. The proposed framework utilizes multiple
clients to train the prediction model collaboratively with local differential
privacy. In addition, regularization techniques have been employed to further
improve the Byzantine robustness of the models. We have conducted extensive
experiments on three real-world datasets, and the results elucidate that our
proposed distributed algorithm can achieve superior performance over existing
methods.

</details>


### [539] [A Snapshot of Influence: A Local Data Attribution Framework for Online Reinforcement Learning](https://arxiv.org/abs/2505.19281)
*Yuzheng Hu,Fan Wu,Haotian Ye,David Forsyth,James Zou,Nan Jiang,Jiaqi W. Ma,Han Zhao*

Main category: cs.LG

TL;DR: 该论文提出了一种针对在线强化学习（RL）的数据归因方法，解决了样本效率低、训练不稳定和缺乏可解释性的问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在线强化学习在复杂、安全关键领域表现出色，但仍面临样本效率低、训练不稳定和缺乏可解释性等挑战。数据归因方法可以追溯模型行为到单个训练样本，但在在线RL中，每个样本不仅影响策略更新，还影响未来数据收集，这违反了现有归因方法的固定数据集假设。

Method: 论文首先建立了一个局部归因框架，通过梯度相似性测量每个训练记录对目标函数（代理动作和累积回报）的贡献。基于此框架，提出了一种名为迭代影响过滤（IIF）的算法，用于在线RL训练，通过迭代过滤经验来优化策略更新。

Result: 在标准RL基准测试（经典控制、导航、运动）以及大型语言模型的RLHF中，IIF减少了样本复杂度，加快了训练速度，并获得了更高的回报。

Conclusion: 该研究提升了在线RL的可解释性、效率和有效性，为RL的实际应用提供了新的工具和方法。

Abstract: Online reinforcement learning (RL) excels in complex, safety-critical
domains, yet it faces challenges such as sample inefficiency, training
instability, and a lack of interpretability. Data attribution offers a
principled way to trace model behavior back to individual training samples.
However, in online RL, each training sample not only drives policy updates but
also influences future data collection, violating the fixed dataset assumption
in existing attribution methods. In this paper, we initiate the study of data
attribution for online RL, focusing on the widely used Proximal Policy
Optimization (PPO) algorithm. We start by establishing a local attribution
framework, interpreting model checkpoints with respect to the records in the
recent training buffer. We design two target functions, capturing agent action
and cumulative return respectively, and measure each record's contribution
through gradient similarity between its training loss and these targets. We
demonstrate the power of this framework through three concrete applications:
diagnosis of learning, temporal analysis of behavior formation, and targeted
intervention during training. Leveraging this framework, we further propose an
algorithm, iterative influence-based filtering (IIF), for online RL training
that iteratively performs experience filtering to refine policy updates. Across
standard RL benchmarks (classic control, navigation, locomotion) to RLHF for
large language models, IIF reduces sample complexity, speeds up training, and
achieves higher returns. Overall, these results advance interpretability,
efficiency, and effectiveness of online RL.

</details>


### [540] [Hypercube-RAG: Hypercube-Based Retrieval-Augmented Generation for In-domain Scientific Question-Answering](https://arxiv.org/abs/2505.19288)
*Jimeng Shi,Sizhe Zhou,Bowen Jin,Wei Hu,Shaowen Wang,Giri Narasimhan,Jiawei Han*

Main category: cs.LG

TL;DR: 论文提出Hypercube-RAG框架，通过多维超立方体结构提升检索效率与准确性，适用于科学问答等专业领域。


<details>
  <summary>Details</summary>
Motivation: 传统基于语义相似性的RAG在专业领域（如科学问答）中难以返回简洁且高相关性的信息，需改进检索方法。

Method: 构建多维超立方体（Hypercube）索引文档，通过分解查询的实体与主题，在超立方体维度中精准检索相关文档。

Result: 在三个科学QA数据集上，准确率提升3.7%，检索效率提高81.2%，且框架具有可解释性。

Conclusion: Hypercube-RAG显著提升了专业领域检索的精度与效率，并通过预定义维度增强了可解释性。

Abstract: Large language models (LLMs) often need to incorporate external knowledge to
solve theme-specific problems. Retrieval-augmented generation (RAG), which
empowers LLMs to generate more qualified responses with retrieved external data
and knowledge, has shown its high promise. However, traditional semantic
similarity-based RAGs struggle to return concise yet highly relevant
information for domain knowledge-intensive tasks, such as scientific
question-answering (QA). Built on a multi-dimensional (cube) structure called
Hypercube, which can index documents in an application-driven, human-defined,
multi-dimensional space, we introduce the Hypercube-RAG, a novel RAG framework
for precise and efficient retrieval. Given a query, Hypercube-RAG first
decomposes it based on its entities and topics and then retrieves relevant
documents from cubes by aligning these decomposed components with hypercube
dimensions. Experiments on three in-domain scientific QA datasets demonstrate
that our method improves accuracy by 3.7% and boosts retrieval efficiency by
81.2%, measured as relative gains over the strongest RAG baseline. More
importantly, our Hypercube-RAG inherently offers explainability by revealing
the underlying predefined hypercube dimensions used for retrieval. The code and
data sets are available at https://github.com/JimengShi/Hypercube-RAG.

</details>


### [541] [Concept Reachability in Diffusion Models: Beyond Dataset Constraints](https://arxiv.org/abs/2505.19313)
*Marta Aparicio Rodriguez,Xenia Miscouridou,Anastasia Borovykh*

Main category: cs.LG

TL;DR: 该研究探讨了文本到图像模型中概念可达性的问题，发现通过直接干预潜在空间激活可以绕过提示限制，且概念可达性存在阶段性转变。


<details>
  <summary>Details</summary>
Motivation: 尽管文本到图像模型在生成质量和复杂性上取得了显著进展，但提示并不总能产生期望的输出。因此，研究如何通过直接干预模型激活来控制模型行为，以访问潜在空间中原本无法通过提示到达的概念。

Method: 设计了一个包含三个关键障碍的训练数据设置：概念稀缺性、标题中概念的不明确性以及数据偏差与绑定概念。通过实验研究概念可达性。

Result: 研究发现：(i) 概念可达性在潜在空间中表现出明显的阶段性转变，少量样本即可实现可达性；(ii) 干预的位置对可达性至关重要，某些概念仅在特定转换阶段可达；(iii) 提示能力随数据集质量下降而迅速减弱，但通过干预仍可可靠地达到概念。

Conclusion: 模型提供商可以利用这一发现，绕过昂贵的重新训练和数据整理，转而创新用户控制机制。

Abstract: Despite significant advances in quality and complexity of the generations in
text-to-image models, prompting does not always lead to the desired outputs.
Controlling model behaviour by directly steering intermediate model activations
has emerged as a viable alternative allowing to reach concepts in latent space
that may otherwise remain inaccessible by prompt. In this work, we introduce a
set of experiments to deepen our understanding of concept reachability. We
design a training data setup with three key obstacles: scarcity of concepts,
underspecification of concepts in the captions, and data biases with tied
concepts. Our results show: (i) concept reachability in latent space exhibits a
distinct phase transition, with only a small number of samples being sufficient
to enable reachability, (ii) where in the latent space the intervention is
performed critically impacts reachability, showing that certain concepts are
reachable only at certain stages of transformation, and (iii) while prompting
ability rapidly diminishes with a decrease in quality of the dataset, concepts
often remain reliably reachable through steering. Model providers can leverage
this to bypass costly retraining and dataset curation and instead innovate with
user-facing control mechanisms.

</details>


### [542] [Paying Alignment Tax with Contrastive Learning](https://arxiv.org/abs/2505.19327)
*Buse Sibel Korkmaz,Rahul Nair,Elizabeth M. Daly,Antonio del Rio Chanona*

Main category: cs.LG

TL;DR: 该论文提出了一种基于对比学习的去偏框架，通过精心构建的正负例平衡去偏与模型忠实度，避免了现有方法在去偏时导致模型能力下降的问题。


<details>
  <summary>Details</summary>
Motivation: 当前的去偏方法往往导致模型能力下降，如事实准确性和知识保留的退化。论文旨在解决这一问题，寻求在去偏的同时保持模型的忠实度。

Method: 论文提出了一种对比学习框架，通过对比计算和动态损失缩放来平衡偏差减轻与忠实度保留。

Result: 实验结果表明，该方法在多个模型规模上均实现了毒性减少和忠实度保留的显著改进，且首次同时提升了这两个指标。

Conclusion: 通过对比学习显式建模正负例，可能是减少语言模型去偏中对齐税的一个有前景的方向。

Abstract: Current debiasing approaches often result a degradation in model capabilities
such as factual accuracy and knowledge retention. Through systematic evaluation
across multiple benchmarks, we demonstrate that existing debiasing methods face
fundamental trade-offs, particularly in smaller models, leading to reduced
truthfulness, knowledge loss, or unintelligible outputs. To address these
limitations, we propose a contrastive learning framework that learns through
carefully constructed positive and negative examples. Our approach introduces
contrast computation and dynamic loss scaling to balance bias mitigation with
faithfulness preservation. Experimental results across multiple model scales
demonstrate that our method achieves substantial improvements in both toxicity
reduction and faithfulness preservation. Most importantly, we show that our
framework is the first to consistently improve both metrics simultaneously,
avoiding the capability degradation characteristic of existing approaches.
These results suggest that explicit modeling of both positive and negative
examples through contrastive learning could be a promising direction for
reducing the alignment tax in language model debiasing.

</details>


### [543] [Likert or Not: LLM Absolute Relevance Judgments on Fine-Grained Ordinal Scales](https://arxiv.org/abs/2505.19334)
*Charles Godfrey,Ping Nie,Natalia Ostapuk,David Ken,Shang Gao,Souheil Inati*

Main category: cs.LG

TL;DR: 研究发现，当使用足够大的有序相关性标签空间时，点式评分与列表式排名的性能差距缩小，统计上不再显著。


<details>
  <summary>Details</summary>
Motivation: 当前研究普遍认为列表式排名在大型语言模型(LLM)相关性排序中表现更优，但本文质疑这一假设，探讨点式评分在适当条件下的表现。

Method: 评估了四种LLM、八个基准数据集及两个专有数据集，比较点式评分和列表式排名在不同条件下的性能差异。

Result: 当点式评分采用足够大的有序相关性标签空间时，与列表式排名的性能差距缩小，统计上不再显著。

Conclusion: 点式评分在适当条件下可与列表式排名媲美，挑战了当前关于LLM相关性排序的共识。

Abstract: Large language models (LLMs) obtain state of the art zero shot relevance
ranking performance on a variety of information retrieval tasks. The two most
common prompts to elicit LLM relevance judgments are pointwise scoring (a.k.a.
relevance generation), where the LLM sees a single query-document pair and
outputs a single relevance score, and listwise ranking (a.k.a. permutation
generation), where the LLM sees a query and a list of documents and outputs a
permutation, sorting the documents in decreasing order of relevance. The
current research community consensus is that listwise ranking yields superior
performance, and significant research effort has been devoted to crafting LLM
listwise ranking algorithms. The underlying hypothesis is that LLMs are better
at making relative relevance judgments than absolute ones. In tension with this
hypothesis, we find that the gap between pointwise scoring and listwise ranking
shrinks when pointwise scoring is implemented using a sufficiently large
ordinal relevance label space, becoming statistically insignificant for many
LLM-benchmark dataset combinations (where ``significant'' means ``95\%
confidence that listwise ranking improves NDCG@10''). Our evaluations span four
LLMs, eight benchmark datasets from the BEIR and TREC-DL suites, and two
proprietary datasets with relevance labels collected after the training cut-off
of all LLMs evaluated.

</details>


### [544] [Prompting Decision Transformers for Zero-Shot Reach-Avoid Policies](https://arxiv.org/abs/2505.19337)
*Kevin Li,Marinka Zitnik*

Main category: cs.LG

TL;DR: RADT是一种新型离线目标条件强化学习模型，通过提示令牌直接编码目标和避障区域，支持动态指定避障区域，并在零样本情况下优于需重新训练的基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有离线目标条件强化学习方法通常将避障信息编码到增强状态空间和成本函数中，限制了在评估时动态指定新避障区域的灵活性，且依赖精心设计的奖励和成本函数，难以适应复杂或结构不良的环境。

Method: RADT采用决策变换器模型，直接将目标和避障区域编码为提示令牌，通过目标和避障区域的后见之明重标记技术，从随机策略的次优离线轨迹中学习避障行为。

Result: 在11个任务、环境和实验设置中，RADT在零样本情况下对分布外避障区域大小和数量表现出色，优于需重新训练的基线模型，并在细胞重编程应用中减少了不良中间基因表达状态的访问。

Conclusion: RADT提供了一种灵活、可扩展的方法，适用于复杂环境中的避障任务，尤其在需要动态指定避障区域的情况下表现出色。

Abstract: Offline goal-conditioned reinforcement learning methods have shown promise
for reach-avoid tasks, where an agent must reach a target state while avoiding
undesirable regions of the state space. Existing approaches typically encode
avoid-region information into an augmented state space and cost function, which
prevents flexible, dynamic specification of novel avoid-region information at
evaluation time. They also rely heavily on well-designed reward and cost
functions, limiting scalability to complex or poorly structured environments.
We introduce RADT, a decision transformer model for offline, reward-free,
goal-conditioned, avoid region-conditioned RL. RADT encodes goals and avoid
regions directly as prompt tokens, allowing any number of avoid regions of
arbitrary size to be specified at evaluation time. Using only suboptimal
offline trajectories from a random policy, RADT learns reach-avoid behavior
through a novel combination of goal and avoid-region hindsight relabeling. We
benchmark RADT against 3 existing offline goal-conditioned RL models across 11
tasks, environments, and experimental settings. RADT generalizes in a zero-shot
manner to out-of-distribution avoid region sizes and counts, outperforming
baselines that require retraining. In one such zero-shot setting, RADT achieves
35.7% improvement in normalized cost over the best retrained baseline while
maintaining high goal-reaching success. We apply RADT to cell reprogramming in
biology, where it reduces visits to undesirable intermediate gene expression
states during trajectories to desired target states, despite stochastic
transitions and discrete, structured state dynamics.

</details>


### [545] [Communication-Efficient Multi-Device Inference Acceleration for Transformer Models](https://arxiv.org/abs/2505.19342)
*Xiao Liu,Lijun Zhang,Deepak Ganesan,Hui Guan*

Main category: cs.LG

TL;DR: ASTRA框架通过序列并行和混合精度注意力机制，有效降低Transformer模型推理延迟，适用于低带宽环境。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在实时应用中因高推理延迟受限，现有多设备推理方法对带宽要求高，不适用于带宽受限环境。

Method: 结合序列并行和混合精度注意力机制，通过向量量化压缩非局部令牌嵌入，并使用噪声增强量化和分布式类令牌优化任务精度。

Result: 在ViT和GPT2上的实验显示，ASTRA比单设备推理快2.64倍，比现有最优多设备推理快15.25倍，且能在10 Mbps低带宽下运行。

Conclusion: ASTRA显著提升了Transformer模型在低带宽环境下的推理效率，具有实际应用价值。

Abstract: Transformer models power many AI applications but suffer from high inference
latency, limiting their use in real-time settings. Multi-device inference can
reduce latency by parallelizing computation. Yet, existing methods require high
inter-device bandwidth, making them impractical for bandwidth-constrained
environments. We propose ASTRA, a communication-efficient framework that
accelerates Transformer inference through a novel integration of sequence
parallelism and a Mixed-Precision Attention mechanism designed to minimize
inter-device communication. ASTRA compresses non-local token embeddings via
vector quantization and preserves task accuracy through two optimizations,
Noise-Augmented Quantization and Distributed Class Tokens. Experiments on ViT
and GPT2 across vision and NLP tasks show that ASTRA achieves up to 2.64X
speedups over single-device inference and up to 15.25X speedups over
state-of-the-art multi-device inferences, while operating under bandwidths as
low as 10 Mbps. ASTRA is open-sourced at https://github.com/xl1990/Astra.

</details>


### [546] [SETransformer: A Hybrid Attention-Based Architecture for Robust Human Activity Recognition](https://arxiv.org/abs/2505.19369)
*Yunbo Liu,Xukui Qin,Yifan Gao,Xiang Li,Chengwei Feng*

Main category: cs.LG

TL;DR: 论文提出SETransformer模型，结合Transformer时序建模与通道注意力机制，显著提升穿戴式传感器的人体活动识别性能。


<details>
  <summary>Details</summary>
Motivation: 传统CNN/RNN模型难以捕捉传感器数据的长期时序依赖和跨通道上下文关系，需改进。

Method: 提出混合架构SETransformer：使用全局自注意力捕捉时序动态，SE注意力加权关键传感器通道，可学习时序注意力池化机制。

Result: 在WISDM数据集上达到84.68%准确率，显著优于LSTM/CNN等基线模型，F1分数提升明显。

Conclusion: SETransformer是可解释的HAR解决方案，在移动感知应用中具有部署潜力。

Abstract: Human Activity Recognition (HAR) using wearable sensor data has become a
central task in mobile computing, healthcare, and human-computer interaction.
Despite the success of traditional deep learning models such as CNNs and RNNs,
they often struggle to capture long-range temporal dependencies and contextual
relevance across multiple sensor channels. To address these limitations, we
propose SETransformer, a hybrid deep neural architecture that combines
Transformer-based temporal modeling with channel-wise squeeze-and-excitation
(SE) attention and a learnable temporal attention pooling mechanism. The model
takes raw triaxial accelerometer data as input and leverages global
self-attention to capture activity-specific motion dynamics over extended time
windows, while adaptively emphasizing informative sensor channels and critical
time steps.
  We evaluate SETransformer on the WISDM dataset and demonstrate that it
significantly outperforms conventional models including LSTM, GRU, BiLSTM, and
CNN baselines. The proposed model achieves a validation accuracy of 84.68\% and
a macro F1-score of 84.64\%, surpassing all baseline architectures by a notable
margin. Our results show that SETransformer is a competitive and interpretable
solution for real-world HAR tasks, with strong potential for deployment in
mobile and ubiquitous sensing applications.

</details>


### [547] [Alignment of large language models with constrained learning](https://arxiv.org/abs/2505.19387)
*Botong Zhang,Shuo Li,Ignacio Hounie,Osbert Bastani,Dongsheng Ding,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 该论文提出了一种基于拉格朗日对偶的迭代对齐方法，用于解决约束对齐问题中的LLM策略优化，并在理论和实验上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于拉格朗日的LLM策略搜索方法在约束对齐问题中常因迭代失败或无法达到最优而受限，亟需一种更有效的方法来优化策略。

Method: 采用拉格朗日对偶理论，开发了一种迭代对偶对齐方法，交替更新LLM策略和拉格朗日乘子，以最大化主奖励并满足次要约束。

Result: 理论分析表明该方法能缩小原始-对偶间隙，实验证明在PKU-SafeRLHF数据集上能有效学习到接近最优的约束LLM策略。

Conclusion: 基于对偶的对齐方法能够在LLM参数空间中实现最优约束策略，仅受限于参数化本身的差距。

Abstract: We study the problem of computing an optimal large language model (LLM)
policy for a constrained alignment problem, where the goal is to maximize a
primary reward objective while satisfying constraints on secondary utilities.
Despite the popularity of Lagrangian-based LLM policy search in constrained
alignment, iterative primal-dual methods often fail to converge, and
non-iterative dual-based methods do not achieve optimality in the LLM parameter
space. To address these challenges, we employ Lagrangian duality to develop an
iterative dual-based alignment method that alternates between updating the LLM
policy via Lagrangian maximization and updating the dual variable via dual
descent. In theory, we characterize the primal-dual gap between the primal
value in the distribution space and the dual value in the LLM parameter space.
We further quantify the optimality gap of the learned LLM policies at
near-optimal dual variables with respect to both the objective and the
constraint functions. These results prove that dual-based alignment methods can
find an optimal constrained LLM policy, up to an LLM parametrization gap. We
demonstrate the effectiveness and merits of our approach through extensive
experiments conducted on the PKU-SafeRLHF dataset.

</details>


### [548] [Are Time-Series Foundation Models Deployment-Ready? A Systematic Study of Adversarial Robustness Across Domains](https://arxiv.org/abs/2505.19397)
*Jiawen Zhang,Zhenwei Zhang,Shun Zheng,Xumeng Wen,Jia Li,Jiang Bian*

Main category: cs.LG

TL;DR: 时间序列基础模型（TSFMs）在零样本预测中表现出对对抗性输入扰动的脆弱性，即使微小扰动也可能导致预测行为显著变化。


<details>
  <summary>Details</summary>
Motivation: 随着零样本预测范式的普及，TSFMs在对抗性输入扰动下的鲁棒性成为一个关键但被忽视的问题，可能被中间人攻击或数据投毒利用。

Method: 通过系统研究TSFMs的对抗鲁棒性，实验评估了代表性模型和多个数据集，识别了潜在的架构设计改进方向。

Result: 研究发现，即使微小扰动也能导致预测行为的显著变化（如趋势反转、时间漂移和幅度偏移），揭示了TSFMs的一致脆弱性。

Conclusion: 研究结果为设计更稳健的预测系统提供了可行建议，并对TSFMs的对抗鲁棒性进行了关键评估。

Abstract: Time Series Foundation Models (TSFMs), which are pretrained on large-scale,
cross-domain data and capable of zero-shot forecasting in new scenarios without
further training, are increasingly adopted in real-world applications. However,
as the zero-shot forecasting paradigm gets popular, a critical yet overlooked
question emerges: Are TSFMs robust to adversarial input perturbations? Such
perturbations could be exploited in man-in-the-middle attacks or data
poisoning. To address this gap, we conduct a systematic investigation into the
adversarial robustness of TSFMs. Our results show that even minimal
perturbations can induce significant and controllable changes in forecast
behaviors, including trend reversal, temporal drift, and amplitude shift,
posing serious risks to TSFM-based services. Through experiments on
representative TSFMs and multiple datasets, we reveal their consistent
vulnerabilities and identify potential architectural designs, such as
structural sparsity and multi-task pretraining, that may improve robustness.
Our findings offer actionable guidance for designing more resilient forecasting
systems and provide a critical assessment of the adversarial robustness of
TSFMs.

</details>


### [549] [Exploring the Possibility of TypiClust for Low-Budget Federated Active Learning](https://arxiv.org/abs/2505.19404)
*Yuta Ono,Hiroshi Nakamura,Hideki Takase*

Main category: cs.LG

TL;DR: 该论文研究了在低预算联邦学习环境下TypiClust主动学习策略的有效性，发现其优于其他方法且对分布偏移不敏感。


<details>
  <summary>Details</summary>
Motivation: 联邦主动学习(FAL)在标注成本高的现实约束下，需要有效的低预算策略来减少标注负担。

Method: 评估TypiClust策略在低预算FAL环境中的表现，分析其对数据异构性和典型性分布偏移的鲁棒性。

Result: TypiClust在低预算FAL中表现优异，且对典型性分布偏移不敏感；特征提取方法的选择对其影响较小。

Conclusion: TypiClust是一种适用于低预算FAL的有效策略，为有限数据条件下的联邦学习提供了可行方案。

Abstract: Federated Active Learning (FAL) seeks to reduce the burden of annotation
under the realistic constraints of federated learning by leveraging Active
Learning (AL). As FAL settings make it more expensive to obtain ground truth
labels, FAL strategies that work well in low-budget regimes, where the amount
of annotation is very limited, are needed. In this work, we investigate the
effectiveness of TypiClust, a successful low-budget AL strategy, in low-budget
FAL settings. Our empirical results show that TypiClust works well even in
low-budget FAL settings contrasted with relatively low performances of other
methods, although these settings present additional challenges, such as data
heterogeneity, compared to AL. In addition, we show that FAL settings cause
distribution shifts in terms of typicality, but TypiClust is not very
vulnerable to the shifts. We also analyze the sensitivity of TypiClust to
feature extraction methods, and it suggests a way to perform FAL even in
limited data situations.

</details>


### [550] [Future Link Prediction Without Memory or Aggregation](https://arxiv.org/abs/2505.19408)
*Lu Yi,Runlin Lei,Fengran Mo,Yanping Zheng,Zhewei Wei,Yuhang Ye*

Main category: cs.LG

TL;DR: 提出CRAFT模型，通过可学习节点嵌入和交叉注意力机制改进时序图上的未来链接预测，有效处理新老边关系。


<details>
  <summary>Details</summary>
Motivation: 现有时序图模型依赖复杂记忆和聚合模块，但难以处理未见边。论文发现节点唯一标识表示和目标感知匹配是关键需求。

Method: CRAFT模型摒弃记忆模块，采用可学习节点嵌入和源-目标交叉注意力机制，实现目标感知的交互模式匹配。

Result: 多数据集实验表明，CRAFT在效率和性能上均优于现有方法，适合大规模应用。

Conclusion: CRAFT通过简洁架构解决了时序图链接预测的核心挑战，为动态系统提供了高效解决方案。

Abstract: Future link prediction on temporal graphs is a fundamental task with wide
applicability in real-world dynamic systems. These scenarios often involve both
recurring (seen) and novel (unseen) interactions, requiring models to
generalize effectively across both types of edges. However, existing methods
typically rely on complex memory and aggregation modules, yet struggle to
handle unseen edges. In this paper, we revisit the architecture of existing
temporal graph models and identify two essential but overlooked modeling
requirements for future link prediction: representing nodes with unique
identifiers and performing target-aware matching between source and destination
nodes. To this end, we propose Cross-Attention based Future Link Predictor on
Temporal Graphs (CRAFT), a simple yet effective architecture that discards
memory and aggregation modules and instead builds on two components: learnable
node embeddings and cross-attention between the destination and the source's
recent interactions. This design provides strong expressive power and enables
target-aware modeling of the compatibility between candidate destinations and
the source's interaction patterns. Extensive experiments on diverse datasets
demonstrate that CRAFT consistently achieves superior performance with high
efficiency, making it well-suited for large-scale real-world applications.

</details>


### [551] [Surrogate-Assisted Evolutionary Reinforcement Learning Based on Autoencoder and Hyperbolic Neural Network](https://arxiv.org/abs/2505.19423)
*Bingdong Li,Mei Jiang,Hong Qian,Peng Yang,Wenjing Hong,Hong Qian,Ke Tang*

Main category: cs.LG

TL;DR: 该论文提出了一种结合自动编码器和双曲神经网络的新型代理辅助进化强化学习方法，有效降低了高维策略评估的计算成本，提升了搜索效率。


<details>
  <summary>Details</summary>
Motivation: 进化强化学习（ERL）虽然比传统策略梯度方法具有更强的探索能力和鲁棒性，但其高计算成本和低搜索效率限制了应用。现有方法难以构建高效的代理模型来预筛选策略。

Method: 通过自动编码器（AE）压缩高维策略为低维表示，并利用双曲神经网络（HNN）作为分类代理模型，预筛选策略以避免无效评估。

Result: 在10个Atari和4个Mujoco游戏上的实验表明，该方法显著优于现有方法，且搜索轨迹在探索和收敛性上更高效。

Conclusion: 论文首次实现了高维ERL策略的可学习嵌入和代理建模模块，并通过实验验证了其有效性及适用条件。

Abstract: Evolutionary Reinforcement Learning (ERL), training the Reinforcement
Learning (RL) policies with Evolutionary Algorithms (EAs), have demonstrated
enhanced exploration capabilities and greater robustness than using traditional
policy gradient. However, ERL suffers from the high computational costs and low
search efficiency, as EAs require evaluating numerous candidate policies with
expensive simulations, many of which are ineffective and do not contribute
meaningfully to the training. One intuitive way to reduce the ineffective
evaluations is to adopt the surrogates. Unfortunately, existing ERL policies
are often modeled as deep neural networks (DNNs) and thus naturally represented
as high-dimensional vectors containing millions of weights, which makes the
building of effective surrogates for ERL policies extremely challenging. This
paper proposes a novel surrogate-assisted ERL that integrates Autoencoders (AE)
and Hyperbolic Neural Networks (HNN). Specifically, AE compresses
high-dimensional policies into low-dimensional representations while extracting
key features as the inputs for the surrogate. HNN, functioning as a
classification-based surrogate model, can learn complex nonlinear relationships
from sampled data and enable more accurate pre-selection of the sampled
policies without real evaluations. The experiments on 10 Atari and 4 Mujoco
games have verified that the proposed method outperforms previous approaches
significantly. The search trajectories guided by AE and HNN are also visually
demonstrated to be more effective, in terms of both exploration and
convergence. This paper not only presents the first learnable policy embedding
and surrogate-modeling modules for high-dimensional ERL policies, but also
empirically reveals when and why they can be successful.

</details>


### [552] [WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference](https://arxiv.org/abs/2505.19427)
*Sihan Chen,Dan Zhao,Jongwoo Ko,Colby Banbury,Huiping Zhuang,Luming Liang,Tianyi Chen*

Main category: cs.LG

TL;DR: 论文提出了一种名为WINA的新型训练无关稀疏激活框架，通过联合考虑隐藏状态幅度和权重矩阵的列范数，显著提升了大型语言模型推理的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的计算需求日益增长，高效的推理和激活策略变得至关重要。现有的稀疏激活方法大多仅依赖隐藏状态幅度，导致近似误差高和推理精度不足。

Method: WINA框架结合隐藏状态幅度和权重矩阵的列范数，提出了一种无需训练的稀疏激活策略，具有理论保证的最优近似误差界限。

Result: 实验表明，WINA在相同稀疏度下，平均性能比现有最佳方法（如TEAL）高出2.94%，适用于多种LLM架构和数据集。

Conclusion: WINA为训练无关稀疏激活方法设立了新的性能基准，推动了高效推理技术的发展，并提供了开源实现。

Abstract: The growing computational demands of large language models (LLMs) make
efficient inference and activation strategies increasingly critical. While
recent approaches, such as Mixture-of-Experts (MoE), leverage selective
activation but require specialized training, training-free sparse activation
methods offer broader applicability and superior resource efficiency through
their plug-and-play design. However, many existing methods rely solely on
hidden state magnitudes to determine activation, resulting in high
approximation errors and suboptimal inference accuracy. To address these
limitations, we propose WINA (Weight Informed Neuron Activation), a novel,
simple, and training-free sparse activation framework that jointly considers
hidden state magnitudes and the column-wise $\ell_2$-norms of weight matrices.
We show that this leads to a sparsification strategy that obtains optimal
approximation error bounds with theoretical guarantees tighter than existing
techniques. Empirically, WINA also outperforms state-of-the-art methods (e.g.,
TEAL) by up to $2.94\%$ in average performance at the same sparsity levels,
across a diverse set of LLM architectures and datasets. These results position
WINA as a new performance frontier for training-free sparse activation in LLM
inference, advancing training-free sparse activation methods and setting a
robust baseline for efficient inference. The source code is available at
https://github.com/microsoft/wina.

</details>


### [553] [Importance Weighted Score Matching for Diffusion Samplers with Enhanced Mode Coverage](https://arxiv.org/abs/2505.19431)
*Chenguang Wang,Xiaoyu Zhang,Kaiyuan Cui,Weichen Zhao,Yongtao Guan,Tianshu Yu*

Main category: cs.LG

TL;DR: 该论文提出了一种基于扩散模型的新方法，通过直接优化前向KL散度目标，解决了传统方法在无目标数据时难以全面覆盖分布模式的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法在缺乏目标分布样本时，通常通过优化反向KL目标来训练神经采样器，但这会导致模式覆盖不全。本文旨在解决这一问题，提出一种更全面的模式覆盖方法。

Method: 论文提出了一种名为“重要性加权分数匹配”的方法，通过重要性采样估计重新加权分数匹配损失，直接优化前向KL散度目标。

Result: 实验表明，该方法在复杂多模态分布（如120个模式的2D高斯混合模型）上表现优异，超越了现有神经采样器，取得了最先进的成果。

Conclusion: 该方法通过理论分析和实验验证，证明了其在模式覆盖和分布距离度量上的优越性，为神经采样器的训练提供了新的思路。

Abstract: Training neural samplers directly from unnormalized densities without access
to target distribution samples presents a significant challenge. A critical
desideratum in these settings is achieving comprehensive mode coverage,
ensuring the sampler captures the full diversity of the target distribution.
However, prevailing methods often circumvent the lack of target data by
optimizing reverse KL-based objectives. Such objectives inherently exhibit
mode-seeking behavior, potentially leading to incomplete representation of the
underlying distribution. While alternative approaches strive for better mode
coverage, they typically rely on implicit mechanisms like heuristics or
iterative refinement. In this work, we propose a principled approach for
training diffusion-based samplers by directly targeting an objective analogous
to the forward KL divergence, which is conceptually known to encourage mode
coverage. We introduce \textit{Importance Weighted Score Matching}, a method
that optimizes this desired mode-covering objective by re-weighting the score
matching loss using tractable importance sampling estimates, thereby overcoming
the absence of target distribution data. We also provide theoretical analysis
of the bias and variance for our proposed Monte Carlo estimator and the
practical loss function used in our method. Experiments on increasingly complex
multi-modal distributions, including 2D Gaussian Mixture Models with up to 120
modes and challenging particle systems with inherent symmetries -- demonstrate
that our approach consistently outperforms existing neural samplers across all
distributional distance metrics, achieving state-of-the-art results on all
benchmarks.

</details>


### [554] [Advanced long-term earth system forecasting by learning the small-scale nature](https://arxiv.org/abs/2505.19432)
*Hao Wu,Yuan Gao,Ruiqi Shu,Kun Wang,Ruijian Gou,Chuhan Wu,Xinliang Liu,Juncai He,Shuhao Cao,Junfeng Fang,Xingjian Shi,Feng Tao,Qi Song,Shengxuan Ji,Yanfei Xiang,Yuze Sun,Jiahao Li,Fan Xu,Huanshuo Dong,Haixin Wang,Fan Zhang,Penghao Zhao,Xian Wu,Qingsong Wen,Deliang Chen,Xiaomeng Huang*

Main category: cs.LG

TL;DR: Triton是一个解决AI模型在长期自回归模拟中频谱偏差问题的分层架构，显著提升了地球系统动态预测的长期稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在长期自回归模拟中因频谱偏差导致高频小尺度过程表征不足，进而引发误差累积，限制了地球系统动态的可靠预测。

Method: Triton采用分层多分辨率架构，显式建模跨尺度动态，以缓解频谱偏差并抑制高频误差累积。

Result: Triton实现了稳定的全球温度年际预测、120天黑潮涡旋高精度预报以及保真湍流模拟，性能显著超越基线模型。

Conclusion: 通过有效控制高频误差，Triton为气候和地球系统科学的可信AI模拟提供了新途径。

Abstract: Reliable long-term forecast of Earth system dynamics is heavily hampered by
instabilities in current AI models during extended autoregressive simulations.
These failures often originate from inherent spectral bias, leading to
inadequate representation of critical high-frequency, small-scale processes and
subsequent uncontrolled error amplification. We present Triton, an AI framework
designed to address this fundamental challenge. Inspired by increasing grids to
explicitly resolve small scales in numerical models, Triton employs a
hierarchical architecture processing information across multiple resolutions to
mitigate spectral bias and explicitly model cross-scale dynamics. We
demonstrate Triton's superior performance on challenging forecast tasks,
achieving stable year-long global temperature forecasts, skillful Kuroshio eddy
predictions till 120 days, and high-fidelity turbulence simulations preserving
fine-scale structures all without external forcing, with significantly
surpassing baseline AI models in long-term stability and accuracy. By
effectively suppressing high-frequency error accumulation, Triton offers a
promising pathway towards trustworthy AI-driven simulation for climate and
earth system science.

</details>


### [555] [Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression](https://arxiv.org/abs/2505.19433)
*Peijie Dong,Zhenheng Tang,Xiang Liu,Lujun Li,Xiaowen Chu,Bo Li*

Main category: cs.LG

TL;DR: ACBench是首个全面评估压缩对LLM代理能力影响的基准，涵盖12个任务、4种能力和15种模型，揭示了压缩的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有压缩基准仅关注语言建模和自然语言理解任务，忽略了代理能力（如工作流、工具使用等），因此需要全面评估压缩对代理能力的影响。

Method: 引入ACBench基准，涵盖12个任务、4种能力（如工作流生成、长上下文检索）、多种压缩方法（如量化、剪枝）和15种模型。

Result: 实验显示4位量化能保留工作流和工具使用能力（下降1%-3%），但会降低实际应用准确率10%-15%。

Conclusion: ACBench为优化LLM在代理场景中的压缩提供了实用见解，并引入了ERank等指标系统化分析。

Abstract: Post-training compression reduces the computational and memory costs of large
language models (LLMs), enabling resource-efficient deployment. However,
existing compression benchmarks only focus on language modeling (e.g.,
perplexity) and natural language understanding tasks (e.g., GLUE accuracy),
ignoring the agentic capabilities - workflow, tool use/function call,
long-context understanding and real-world application. We introduce the Agent
Compression Benchmark (ACBench), the first comprehensive benchmark for
evaluating how compression impacts LLMs' agentic abilities. ACBench spans (1)
12 tasks across 4 capabilities (e.g., WorfBench for workflow generation,
Needle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ)
and pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B),
standard (Qwen2.5 7B-32B), and distilled reasoning LLMs (DeepSeek-R1-Distill).
Our experiments reveal compression tradeoffs: 4-bit quantization preserves
workflow generation and tool use (1%-3% drop) but degrades real-world
application accuracy by 10%-15%. We introduce ERank, Top-k Ranking Correlation
and Energy to systematize analysis. ACBench provides actionable insights for
optimizing LLM compression in agentic scenarios. The code can be found in
https://github.com/pprp/ACBench.

</details>


### [556] [MetaGMT: Improving Actionable Interpretability of Graph Multilinear Networks via Meta-Learning Filtration](https://arxiv.org/abs/2505.19445)
*Rishabh Bhattacharya,Hari Shankar,Vaishnavi Shivkumar,Ponnurangam Kumaraguru*

Main category: cs.LG

TL;DR: MetaGMT通过元学习框架提升图神经网络解释的可靠性和鲁棒性，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）在医疗和金融等高风险领域的应用日益广泛，但其决策过程的可解释性不足，尤其是现有可解释架构易受虚假相关性影响，可能损害关键应用中的信任。

Method: 提出MetaGMT，一种基于双层优化（bi-level optimization）的元学习框架，旨在增强解释的忠实度。

Result: MetaGMT在BA-2Motifs、MUTAG和SP-Motif基准测试中显著提升了解释质量（如AUC-ROC、Precision@K）和对虚假模式的鲁棒性，同时保持分类准确性（如SP-Motif 0.5上Explanation ROC提升8%）。

Conclusion: MetaGMT通过提高解释可靠性，支持模型调试、针对性再训练和人工监督，有助于构建更可信的GNN系统，推动其在敏感领域的应用。

Abstract: The growing adoption of Graph Neural Networks (GNNs) in high-stakes domains
like healthcare and finance demands reliable explanations of their
decision-making processes. While inherently interpretable GNN architectures
like Graph Multi-linear Networks (GMT) have emerged, they remain vulnerable to
generating explanations based on spurious correlations, potentially undermining
trust in critical applications. We present MetaGMT, a meta-learning framework
that enhances explanation fidelity through a novel bi-level optimization
approach. We demonstrate that MetaGMT significantly improves both explanation
quality (AUC-ROC, Precision@K) and robustness to spurious patterns, across
BA-2Motifs, MUTAG, and SP-Motif benchmarks. Our approach maintains competitive
classification accuracy while producing more faithful explanations (with an
increase up to 8% of Explanation ROC on SP-Motif 0.5) compared to baseline
methods. These advancements in interpretability could enable safer deployment
of GNNs in sensitive domains by (1) facilitating model debugging through more
reliable explanations, (2) supporting targeted retraining when biases are
identified, and (3) enabling meaningful human oversight. By addressing the
critical challenge of explanation reliability, our work contributes to building
more trustworthy and actionable GNN systems for real-world applications.

</details>


### [557] [Recurrent Self-Attention Dynamics: An Energy-Agnostic Perspective from Jacobians](https://arxiv.org/abs/2505.19458)
*Akiyoshi Tomihari,Ryo Karakida*

Main category: cs.LG

TL;DR: 本文通过放宽传统能量函数的约束，采用动态系统分析方法，研究了自注意力层的推理动态，揭示了归一化层对动态临界状态的影响，并提出了新的正则化方法和伪能量监控技术。


<details>
  <summary>Details</summary>
Motivation: 现有的自注意力理论研究多基于能量函数框架，依赖理想化假设或额外约束，限制了理解的广度。本文旨在放宽这些约束，提供更通用的动态系统分析视角。

Method: 首先放宽对称性和单头约束，然后通过分析状态雅可比矩阵研究更一般的自注意力架构，特别是归一化层对动态的影响。

Result: 发现归一化层能有效归一化雅可比矩阵的复特征值，使动态接近临界状态，显著提升推理性能，并基于此开发了新的正则化方法和伪能量监控技术。

Conclusion: 动态系统分析为自注意力机制提供了更深入的理解，归一化层的关键作用及提出的新方法为未来研究开辟了方向。

Abstract: The theoretical understanding of self-attention (SA) has been steadily
progressing. A prominent line of work studies a class of SA layers that admit
an energy function decreased by state updates. While it provides valuable
insights into inherent biases in signal propagation, it often relies on
idealized assumptions or additional constraints not necessarily present in
standard SA. Thus, to broaden our understanding, this work aims to relax these
energy constraints and provide an energy-agnostic characterization of inference
dynamics by dynamical systems analysis. In more detail, we first consider
relaxing the symmetry and single-head constraints traditionally required in
energy-based formulations. Next, to investigate more general SA architectures
capable of oscillatory dynamics without necessarily admitting an energy
function, we analyze the Jacobian matrix of the state. We reveal that
normalization layers effectively normalize the Jacobian's complex eigenvalues,
forcing the dynamics close to a critical state. This significantly enhances
inference performance. Furthermore, we utilize the Jacobian perspective to
develop regularization methods for training and a pseudo-energy for monitoring
inference dynamics.

</details>


### [558] [Your Classifier Can Do More: Towards Bridging the Gaps in Classification, Robustness, and Generation](https://arxiv.org/abs/2505.19459)
*Kaichao Jiang,He Wang,Xiaoshuai Hao,Xiulong Yang,Ajian Liu,Qi Chu,Yunfeng Diao*

Main category: cs.LG

TL;DR: 论文提出EB-JDAT方法，通过联合建模清洁数据、对抗样本和分类器的能量分布，在保持JEMs高分类精度和生成能力的同时显著提升对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有联合能量模型(JEMs)在分类精度和生成能力上表现优异，但对抗鲁棒性远不如对抗训练(AT)方法；而AT虽能提升鲁棒性，却会牺牲清洁数据精度且缺乏生成能力。本文探索是否能在单一模型中同时实现高分类精度、对抗鲁棒性和生成性能。

Method: 通过系统分析不同JEM变体和对抗训练模型的能量分布差异，提出基于能量的联合分布对抗训练(EB-JDAT)，最大化清洁数据分布、对抗分布和分类器的联合概率。

Result: 实验表明EB-JDAT在保持JEMs原始精度和生成能力的同时，显著提升了对抗鲁棒性，甚至超越最先进的对抗训练方法。

Conclusion: EB-JDAT成功统一了JEMs和AT的优势，解决了分类精度、生成能力和对抗鲁棒性之间的三重权衡问题。

Abstract: Joint Energy-based Models (JEMs), a class of hybrid generative-discriminative
models, are well known for their ability to achieve both high classification
accuracy and generative capability within a single model. However, their
robustness still lags significantly behind the classifiers based adversarial
training (AT). Conversely, while AT is currently the most effective approach to
improving the classifier's robustness, it typically sacrifices accuracy on
clean data and lacks generative capability. The triple trade-off between
classification accuracy, generative capability and robustness, raises a natural
question: Can a single model simultaneously achieve high classification
accuracy, adversarial robustness, and generative performance? -- a goal that
has been rarely explored. To address this question, we systematically analyze
the energy distribution differences of clean, adversarial, and generated
samples across various JEM variants and adversarially trained models. We
observe that AT tends to reduce the energy gap between clean and adversarial
samples, while JEMs reduce the gap between clean and synthetic ones. This
observation suggests a key insight: if the energy distributions of all three
data types can be aligned, we might unify the strengths of AT and JEMs,
resolving their inherent trade-offs. Building on this idea, we propose
Energy-based Joint Distribution Adversarial Training (EB-JDAT), to jointly
model the clean data distribution, the adversarial distribution, and the
classifier by maximizing their joint probability. EB-JDAT is a general and
flexible optimization method, compatible with various JEM variants. Extensive
experimental results demonstrate that EB-JDAT not only maintains near original
accuracy and generative capability of JEMs, but also significantly enhances
robustness, even surpassing state-of-the-art ATs.

</details>


### [559] [Residual Cross-Attention Transformer-Based Multi-User CSI Feedback with Deep Joint Source-Channel Coding](https://arxiv.org/abs/2505.19465)
*Hengwei Zhang,Minghui Wu,Li Qiao,Ling Liu,Ziqi Han,Zhen Gao*

Main category: cs.LG

TL;DR: 该论文提出了一种基于深度学习的多用户CSI反馈框架，利用深度联合源信道编码和残差交叉注意力Transformer架构，以提高CSI重建精度并减少反馈开销。


<details>
  <summary>Details</summary>
Motivation: 传统的多用户CSI反馈方法存在反馈开销大和重建精度不足的问题，尤其是在上行链路噪声变化时性能下降明显。

Method: 设计了多用户联合CSI反馈框架，利用附近用户的CSI相关性减少开销，并引入残差交叉注意力Transformer架构和两阶段训练方案以适应噪声变化。

Result: 实验结果表明，该方法在CSI反馈性能上表现优越，同时具有较低的网络复杂性和更好的可扩展性。

Conclusion: 该框架显著提升了CSI反馈的精度和鲁棒性，适用于大规模MIMO系统。

Abstract: This letter proposes a deep-learning (DL)-based multi-user channel state
information (CSI) feedback framework for massive multiple-input multiple-output
systems, where the deep joint source-channel coding (DJSCC) is utilized to
improve the CSI reconstruction accuracy. Specifically, we design a multi-user
joint CSI feedback framework, whereby the CSI correlation of nearby users is
utilized to reduce the feedback overhead. Under the framework, we propose a new
residual cross-attention transformer architecture, which is deployed at the
base station to further improve the CSI feedback performance. Moreover, to
tackle the "cliff-effect" of conventional bit-level CSI feedback approaches, we
integrated DJSCC into the multi-user CSI feedback, together with utilizing a
two-stage training scheme to adapt to varying uplink noise levels. Experimental
results demonstrate the superiority of our methods in CSI feedback performance,
with low network complexity and better scalability.

</details>


### [560] [Diversity-Driven Generative Dataset Distillation Based on Diffusion Model with Self-Adaptive Memory](https://arxiv.org/abs/2505.19469)
*Mingzhuo Li,Guang Li,Jiafeng Mao,Takahiro Ogawa,Miki Haseyama*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散模型的多样性驱动的生成数据集蒸馏方法，通过自适应记忆对齐分布，提升下游验证精度。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在数据集蒸馏中分布多样性不足，导致下游验证精度下降。

Method: 采用扩散模型和自适应记忆技术，对齐蒸馏数据集与真实数据集的分布。

Result: 实验表明，该方法在多数情况下优于现有最先进方法。

Conclusion: 该方法有效解决了数据集蒸馏任务中的多样性不足问题。

Abstract: Dataset distillation enables the training of deep neural networks with
comparable performance in significantly reduced time by compressing large
datasets into small and representative ones. Although the introduction of
generative models has made great achievements in this field, the distributions
of their distilled datasets are not diverse enough to represent the original
ones, leading to a decrease in downstream validation accuracy. In this paper,
we present a diversity-driven generative dataset distillation method based on a
diffusion model to solve this problem. We introduce self-adaptive memory to
align the distribution between distilled and real datasets, assessing the
representativeness. The degree of alignment leads the diffusion model to
generate more diverse datasets during the distillation process. Extensive
experiments show that our method outperforms existing state-of-the-art methods
in most situations, proving its ability to tackle dataset distillation tasks.

</details>


### [561] [Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs](https://arxiv.org/abs/2505.19481)
*Hao Kang,Qingru Zhang,Han Cai,Weiyuan Xu,Tushar Krishna,Yilun Du,Tsachy Weissman*

Main category: cs.LG

TL;DR: 该论文首次系统研究了基于LLM的智能体在实时决策任务中的延迟与质量权衡，提出自适应框架FPX，在交易和游戏基准测试中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实应用（如高频交易和实时竞技游戏）需要严格延迟约束下的决策，但LLM智能体的延迟-质量权衡尚未充分探索。

Method: 引入HFTBench和StreetFighter两个基准，提出FPX框架动态选择模型大小和量化级别以适应实时需求。

Result: FPX在格斗游戏中胜率提升80%，交易日收益率提高26.52%，证明了延迟感知策略的重要性。

Conclusion: 基于LLM的智能体需采用延迟感知的评估与部署策略，FPX框架为实时任务提供了有效解决方案。

Abstract: Large language models (LLMs) have shown remarkable performance across diverse
reasoning and generation tasks, and are increasingly deployed as agents in
dynamic environments such as code generation and recommendation systems.
However, many real-world applications, such as high-frequency trading and
real-time competitive gaming, require decisions under strict latency
constraints, where faster responses directly translate into higher rewards.
Despite the importance of this latency quality trade off, it remains
underexplored in the context of LLM based agents. In this work, we present the
first systematic study of this trade off in real time decision making tasks. To
support our investigation, we introduce two new benchmarks: HFTBench, a high
frequency trading simulation, and StreetFighter, a competitive gaming platform.
Our analysis reveals that optimal latency quality balance varies by task, and
that sacrificing quality for lower latency can significantly enhance downstream
performance. To address this, we propose FPX, an adaptive framework that
dynamically selects model size and quantization level based on real time
demands. Our method achieves the best performance on both benchmarks, improving
win rate by up to 80% in Street Fighter and boosting daily yield by up to
26.52% in trading, underscoring the need for latency aware evaluation and
deployment strategies for LLM based agents. These results demonstrate the
critical importance of latency aware evaluation and deployment strategies for
real world LLM based agents. Our benchmarks are available at Latency Sensitive
Benchmarks.

</details>


### [562] [Understanding Transformer from the Perspective of Associative Memory](https://arxiv.org/abs/2505.19488)
*Shu Zhong,Mingyu Xu,Tenglong Ao,Guang Shi*

Main category: cs.LG

TL;DR: 本文从心理学联想记忆的角度分析Transformer架构，探讨其记忆容量与更新机制，提出改进思路并回答两个关键问题。


<details>
  <summary>Details</summary>
Motivation: 旨在通过人类认知启发的联想记忆理论，揭示Transformer架构的设计原理，为现有模型提供更清晰的理解并激发创新。

Method: 1. 用检索信噪比衡量记忆容量，核方法解释Softmax注意力有效性；2. 提出统一框架分析不同Transformer变体的记忆更新机制。

Result: 发现FFN可视为联想记忆模块，提出设计改进方向；证明无限上下文不保证无限智能，但揭示了架构的表达潜力边界。

Conclusion: 通过联想记忆视角系统解构Transformer，为架构设计与理论突破提供了新思路和量化分析工具。

Abstract: In this paper, we share our reflections and insights on understanding
Transformer architectures through the lens of associative memory--a classic
psychological concept inspired by human cognition. We start with the basics of
associative memory (think simple linear attention) and then dive into two
dimensions:
  Memory Capacity: How much can a Transformer really remember, and how well? We
introduce retrieval SNR to measure this and use a kernel perspective to
mathematically reveal why Softmax Attention is so effective. We also show how
FFNs can be seen as a type of associative memory, leading to insights on their
design and potential improvements.
  Memory Update: How do these memories learn and evolve? We present a unified
framework for understanding how different Transformer variants (like DeltaNet
and Softmax Attention) update their "knowledge base". This leads us to tackle
two provocative questions: 1. Are Transformers fundamentally limited in what
they can express, and can we break these barriers? 2. If a Transformer had
infinite context, would it become infinitely intelligent?
  We want to demystify Transformer architecture, offering a clearer
understanding of existing designs. This exploration aims to provide fresh
insights and spark new avenues for Transformer innovation.

</details>


### [563] [Discounted Online Convex Optimization: Uniform Regret Across a Continuous Interval](https://arxiv.org/abs/2505.19491)
*Wenhao Yang,Sifan Yang,Lijun Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种自适应未知折扣因子的在线梯度下降算法SOGD，通过DNP整合多个OGD实例的输出，实现了对所有连续区间内折扣因子的统一后悔界。


<details>
  <summary>Details</summary>
Motivation: 在非平稳环境中，近期历史数据比远期数据更重要。虽然λ-折扣后悔已被引入在线凸优化(OCO)中以优雅地遗忘过去数据，但实际场景中折扣因子λ往往未知。如何开发能自适应未知λ的算法成为关键问题。

Method: 采用平滑在线梯度下降(SOGD)方法，通过维护多个处理不同折扣因子的OGD实例，并利用折扣归一化预测器(DNP)序列聚合它们的输出。

Result: 理论分析表明SOGD能实现统一的O(√(log T/(1-λ)))折扣后悔界，该界对所有连续区间内的λ值同时成立。DNP被证明能有效整合不同折扣因子下的专家决策。

Conclusion: 该研究首次给出了自适应未知折扣因子的在线学习算法，通过多专家聚合框架解决了非平稳环境中的动态后悔最小化问题。

Abstract: Reflecting the greater significance of recent history over the distant past
in non-stationary environments, $\lambda$-discounted regret has been introduced
in online convex optimization (OCO) to gracefully forget past data as new
information arrives. When the discount factor $\lambda$ is given, online
gradient descent with an appropriate step size achieves an
$O(1/\sqrt{1-\lambda})$ discounted regret. However, the value of $\lambda$ is
often not predetermined in real-world scenarios. This gives rise to a
significant open question: is it possible to develop a discounted algorithm
that adapts to an unknown discount factor. In this paper, we affirmatively
answer this question by providing a novel analysis to demonstrate that smoothed
OGD (SOGD) achieves a uniform $O(\sqrt{\log T/1-\lambda})$ discounted regret,
holding for all values of $\lambda$ across a continuous interval
simultaneously. The basic idea is to maintain multiple OGD instances to handle
different discount factors, and aggregate their outputs sequentially by an
online prediction algorithm named as Discounted-Normal-Predictor (DNP)
(Kapralov and Panigrahy,2010). Our analysis reveals that DNP can combine the
decisions of two experts, even when they operate on discounted regret with
different discount factors.

</details>


### [564] [Learning for Dynamic Combinatorial Optimization without Training Data](https://arxiv.org/abs/2505.19497)
*Yiqiao Liao,Farinaz Koushanfar,Parinaz Naghizadeh*

Main category: cs.LG

TL;DR: DyCO-GNN是一种无需训练数据的动态组合优化无监督学习框架，通过利用时间演化图的结构相似性加速优化并保持解质量。


<details>
  <summary>Details</summary>
Motivation: 针对动态组合优化问题，传统方法需要大量训练数据且难以适应快速变化的场景，DyCO-GNN旨在提供一种无需外部训练数据的高效解决方案。

Method: DyCO-GNN利用时间演化图中结构相似性，通过图神经网络框架实现动态组合优化，适用于最大割、最大独立集和旅行商问题。

Result: 实验表明，DyCO-GNN在多种数据集上均优于基线方法，求解速度提升3-60倍，且在严格时间预算下仍保持高质量解。

Conclusion: DyCO-GNN在资源受限的动态环境中表现出色，为快速变化的优化问题提供了实用高效的解决方案。

Abstract: We introduce DyCO-GNN, a novel unsupervised learning framework for Dynamic
Combinatorial Optimization that requires no training data beyond the problem
instance itself. DyCO-GNN leverages structural similarities across
time-evolving graph snapshots to accelerate optimization while maintaining
solution quality. We evaluate DyCO-GNN on dynamic maximum cut, maximum
independent set, and the traveling salesman problem across diverse datasets of
varying sizes, demonstrating its superior performance under tight and moderate
time budgets. DyCO-GNN consistently outperforms the baseline methods, achieving
high-quality solutions up to 3-60x faster, highlighting its practical
effectiveness in rapidly evolving resource-constrained settings.

</details>


### [565] [DOGe: Defensive Output Generation for LLM Protection Against Knowledge Distillation](https://arxiv.org/abs/2505.19504)
*Pingzhi Li,Zhen Tan,Huaizhi Qu,Huan Liu,Tianlong Chen*

Main category: cs.LG

TL;DR: 该论文提出了一种防御性输出生成（DOGe）策略，通过微调LLM的最后一层线性层，使输出对合法用户保持准确，但对知识蒸馏（KD）模仿产生误导，有效防止模型被模仿。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的知识可能通过观察公开输出被竞争对手蒸馏模仿，现有保护方法如数字水印仅能事后识别模仿，无法有效防止仅从输出文本进行的蒸馏。

Method: 论文提出DOGe策略，仅微调LLM的最后一层线性层，通过对抗性损失在推理时干扰蒸馏尝试，同时保持对合法用户的输出质量。

Result: 实验表明，防御性生成的教师模型输出显著降低了学生模型的性能，同时保持或提升了教师模型的原始性能。

Conclusion: DOGe是一种实用且高效的保护方法，能有效防止基于知识蒸馏的模型模仿，同时不影响合法用户的使用体验。

Abstract: Large Language Models (LLMs) represent substantial intellectual and economic
investments, yet their effectiveness can inadvertently facilitate model
imitation via knowledge distillation (KD).In practical scenarios, competitors
can distill proprietary LLM capabilities by simply observing publicly
accessible outputs, akin to reverse-engineering a complex performance by
observation alone. Existing protective methods like watermarking only identify
imitation post-hoc, while other defenses assume the student model mimics the
teacher's internal logits, rendering them ineffective against distillation
purely from observed output text. This paper confronts the challenge of
actively protecting LLMs within the realistic constraints of API-based access.
We introduce an effective and efficient Defensive Output Generation (DOGe)
strategy that subtly modifies the output behavior of an LLM. Its outputs remain
accurate and useful for legitimate users, yet are designed to be misleading for
distillation, significantly undermining imitation attempts. We achieve this by
fine-tuning only the final linear layer of the teacher LLM with an adversarial
loss. This targeted training approach anticipates and disrupts distillation
attempts during inference time. Our experiments show that, while preserving or
even improving the original performance of the teacher model, student models
distilled from the defensively generated teacher outputs demonstrate
catastrophically reduced performance, demonstrating our method's effectiveness
as a practical safeguard against KD-based model imitation.

</details>


### [566] [Benchmarking Multimodal Knowledge Conflict for Large Multimodal Models](https://arxiv.org/abs/2505.19509)
*Yifan Jia,Kailin Jiang,Yuyang Liang,Qihan Ren,Yi Xin,Rui Yang,Fenze Feng,Mingcai Chen,Hengyang Lu,Haozhe Wang,Xiaoye Qu,Dongrui Liu,Lizhen Cui,Yuntao Du*

Main category: cs.LG

TL;DR: 论文提出MMKC-Bench基准，用于评估多模态模型在知识冲突下的表现，填补现有研究空白。


<details>
  <summary>Details</summary>
Motivation: 现有基准未能充分反映多模态知识冲突场景，尤其是检索增强生成框架下的上下文与内部知识冲突问题。

Method: 构建包含1573个知识实例和3381张图像的MMKC-Bench基准，涵盖三种多模态知识冲突类型，并通过自动化流程与人工验证收集数据。

Result: 实验表明，当前多模态模型虽能识别知识冲突，但倾向于依赖内部参数知识而非外部证据。

Conclusion: MMKC-Bench有望推动多模态知识冲突研究，并促进多模态检索增强生成系统的发展。

Abstract: Large Multimodal Models(LMMs) face notable challenges when encountering
multimodal knowledge conflicts, particularly under retrieval-augmented
generation(RAG) frameworks where the contextual information from external
sources may contradict the model's internal parametric knowledge, leading to
unreliable outputs. However, existing benchmarks fail to reflect such realistic
conflict scenarios. Most focus solely on intra-memory conflicts, while
context-memory and inter-context conflicts remain largely investigated.
Furthermore, commonly used factual knowledge-based evaluations are often
overlooked, and existing datasets lack a thorough investigation into conflict
detection capabilities. To bridge this gap, we propose MMKC-Bench, a benchmark
designed to evaluate factual knowledge conflicts in both context-memory and
inter-context scenarios. MMKC-Bench encompasses three types of multimodal
knowledge conflicts and includes 1,573 knowledge instances and 3,381 images
across 23 broad types, collected through automated pipelines with human
verification. We evaluate three representative series of LMMs on both model
behavior analysis and conflict detection tasks. Our findings show that while
current LMMs are capable of recognizing knowledge conflicts, they tend to favor
internal parametric knowledge over external evidence. We hope MMKC-Bench will
foster further research in multimodal knowledge conflict and enhance the
development of multimodal RAG systems. The source code is available at
https://github.com/MLLMKCBENCH/MLLMKC.

</details>


### [567] [Rethinking Gating Mechanism in Sparse MoE: Handling Arbitrary Modality Inputs with Confidence-Guided Gate](https://arxiv.org/abs/2505.19525)
*Liangwei Nathan Zheng,Wei Emma Zhang,Mingyu Guo,Miao Xu,Olaf Maennel,Weitong Chen*

Main category: cs.LG

TL;DR: 论文提出Conf-SMoE方法，通过两阶段插补模块和新型专家门控机制解决多模态学习中的缺失模态问题，并理论分析了专家崩溃现象。


<details>
  <summary>Details</summary>
Motivation: 现实多模态学习中，数据不完整常由采集错误或传感器故障导致，现有稀疏混合专家(SMoE)方法处理缺失模态能力不足，导致性能下降和泛化能力差。

Method: 提出Conf-SMoE框架：1) 两阶段插补模块处理缺失模态；2) 通过解耦softmax路由分数与任务置信度设计新型专家门控机制，避免专家崩溃。

Result: 在四个真实数据集和三种实验设置下验证了方法在模态融合和缺失模态鲁棒性方面的优越性，理论分析表明该机制也适用于高斯/拉普拉斯门控。

Conclusion: Conf-SMoE通过理论驱动的门控设计有效缓解专家崩溃问题，无需额外平衡损失函数，为多模态学习提供了可靠解决方案。

Abstract: Effectively managing missing modalities is a fundamental challenge in
real-world multimodal learning scenarios, where data incompleteness often
results from systematic collection errors or sensor failures. Sparse
Mixture-of-Experts (SMoE) architectures have the potential to naturally handle
multimodal data, with individual experts specializing in different modalities.
However, existing SMoE approach often lacks proper ability to handle missing
modality, leading to performance degradation and poor generalization in
real-world applications. We propose Conf-SMoE to introduce a two-stage
imputation module to handle the missing modality problem for the SMoE
architecture and reveal the insight of expert collapse from theoretical
analysis with strong empirical evidence. Inspired by our theoretical analysis,
Conf-SMoE propose a novel expert gating mechanism by detaching the softmax
routing score to task confidence score w.r.t ground truth. This naturally
relieves expert collapse without introducing additional load balance loss
function. We show that the insights of expert collapse aligns with other gating
mechanism such as Gaussian and Laplacian gate. We also evaluate the proposed
method on four different real world dataset with three different experiment
settings to conduct comprehensive the analysis of Conf-SMoE on modality fusion
and resistance to missing modality.

</details>


### [568] [Navigating loss manifolds via rigid body dynamics: A promising avenue for robustness and generalisation](https://arxiv.org/abs/2505.19527)
*Mohammed D. Belgoumri,Mohamed Reda Bouadjenek,Hakim Hacid,Imran Razzak,Sunil Aryal*

Main category: cs.LG

TL;DR: 提出一种新型优化器，通过模拟球在损失曲面上滚动的运动，减少对损失曲面精细结构的依赖，避免陷入尖锐极小值，从而提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统梯度下降法在训练大型神经网络时，容易陷入对输入扰动敏感的尖锐极小值，导致过拟合和泛化能力差。此外，损失曲面的分形特性会导致训练过程不稳定。

Method: 通过模拟球在损失曲面上滚动的中心运动，设计新型优化器。通过调整球的半径超参数，可以在不同尺度上探测损失曲面几何特性。

Result: 该优化器能有效减少对损失曲面精细结构的依赖，避免收敛到尖锐极小值，从而提升模型泛化性能。

Conclusion: 基于球面滚动模拟的优化器提供了一种理解损失曲面几何特性的新工具，同时改善了训练稳定性和泛化能力。

Abstract: Training large neural networks through gradient-based optimization requires
navigating high-dimensional loss landscapes, which often exhibit pathological
geometry, leading to undesirable training dynamics. In particular, poor
generalization frequently results from convergence to sharp minima that are
highly sensitive to input perturbations, causing the model to overfit the
training data while failing to generalize to unseen examples. Furthermore,
these optimization procedures typically display strong dependence on the fine
structure of the loss landscape, leading to unstable training dynamics, due to
the fractal-like nature of the loss surface. In this work, we propose an
alternative optimizer that simultaneously reduces this dependence, and avoids
sharp minima, thereby improving generalization. This is achieved by simulating
the motion of the center of a ball rolling on the loss landscape. The degree to
which our optimizer departs from the standard gradient descent is controlled by
a hyperparameter, representing the radius of the ball. Changing this
hyperparameter allows for probing the loss landscape at different scales,
making it a valuable tool for understanding its geometry.

</details>


### [569] [Minimalist Softmax Attention Provably Learns Constrained Boolean Functions](https://arxiv.org/abs/2505.19531)
*Jerry Yao-Chieh Hu,Xiwen Zhang,Maojiang Su,Zhao Song,Han Liu*

Main category: cs.LG

TL;DR: 论文研究了单头softmax注意力机制在学习k位布尔函数（如AND、OR及其噪声变体）时的计算极限，发现该机制单独无法解决这些简单布尔函数，但在教师强制下可以解决。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索最小化注意力机制在解决基本布尔函数任务时的能力极限，以及如何通过监督学习简化传统多步推理方法。

Method: 方法包括使用单头softmax注意力机制，结合教师强制（teacher forcing）和监督学习，替代传统的多步推理方案。

Result: 结果表明，单头softmax注意力机制在无监督时无法解决简单布尔函数，但在教师强制下可以高效解决，且仅需一次梯度下降更新。

Conclusion: 结论指出，最小化注意力架构在理想监督下可实现布尔任务，而标准训练下存在固有局限性，揭示了监督学习对简化模型架构的重要性。

Abstract: We study the computational limits of learning $k$-bit Boolean functions
(specifically, $\mathrm{AND}$, $\mathrm{OR}$, and their noisy variants), using
a minimalist single-head softmax-attention mechanism, where $k=\Theta(d)$
relevant bits are selected from $d$ inputs. We show that these simple
$\mathrm{AND}$ and $\mathrm{OR}$ functions are unsolvable with a single-head
softmax-attention mechanism alone. However, with teacher forcing, the same
minimalist attention is capable of solving them. These findings offer two key
insights: Architecturally, solving these Boolean tasks requires only minimalist
attention, without deep Transformer blocks or FFNs. Methodologically, one
gradient descent update with supervision suffices and replaces the multi-step
Chain-of-Thought (CoT) reasoning scheme of [Kim and Suzuki, ICLR 2025] for
solving Boolean problems. Together, the bounds expose a fundamental gap between
what this minimal architecture achieves under ideal supervision and what is
provably impossible under standard training.

</details>


### [570] [Fox in the Henhouse: Supply-Chain Backdoor Attacks Against Reinforcement Learning](https://arxiv.org/abs/2505.19532)
*Shijie Liu,Andrew C. Cullen,Paul Montague,Sarah Erfani,Benjamin I. P. Rubinstein*

Main category: cs.LG

TL;DR: 该论文提出了一种名为SCAB的新型后门攻击方法，针对强化学习中的外部代理训练流程，仅需污染3%的训练数据即可实现高效攻击。


<details>
  <summary>Details</summary>
Motivation: 现有针对强化学习的后门攻击需要过高权限（如读写策略参数），论文探讨是否能在更受限的访问模型下实现有效攻击。

Method: 提出SCAB攻击，通过污染供应链中的外部代理数据（仅需合法交互），无需直接访问受害者模型参数。

Result: 仅污染3%训练数据即可激活90%的触发动作，使受害者平均回报降低80%。

Conclusion: 强化学习在不可信供应链环境下存在实际攻击风险，现有防御机制需重新评估。

Abstract: The current state-of-the-art backdoor attacks against Reinforcement Learning
(RL) rely upon unrealistically permissive access models, that assume the
attacker can read (or even write) the victim's policy parameters, observations,
or rewards. In this work, we question whether such a strong assumption is
required to launch backdoor attacks against RL. To answer this question, we
propose the \underline{S}upply-\underline{C}h\underline{a}in
\underline{B}ackdoor (SCAB) attack, which targets a common RL workflow:
training agents using external agents that are provided separately or embedded
within the environment. In contrast to prior works, our attack only relies on
legitimate interactions of the RL agent with the supplied agents. Despite this
limited access model, by poisoning a mere $3\%$ of training experiences, our
attack can successfully activate over $90\%$ of triggered actions, reducing the
average episodic return by $80\%$ for the victim. Our novel attack demonstrates
that RL attacks are likely to become a reality under untrusted RL training
supply-chains.

</details>


### [571] [ExAnte: A Benchmark for Ex-Ante Inference in Large Language Models](https://arxiv.org/abs/2505.19533)
*Yachuan Liu,Xiaochun Wei,Lin Shi,Xinnuo Li,Bohan Zhang,Paramveer Dhillon,Qiaozhu Mei*

Main category: cs.LG

TL;DR: 该论文提出了一个评估大语言模型在时间截止约束下进行事前推理能力的新基准，发现模型难以避免利用未来信息。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在事前推理（即无法获取未来信息的情况下进行分析、推断或预测）中存在困难，即使明确提示时间截止，模型仍可能受未来知识影响。

Method: 设计了一个包含股票预测、维基百科事件预测、科学出版物预测和问答任务的基准，通过泄漏率量化模型对截止时间后信息的依赖程度。

Result: 实验结果表明，大语言模型在不同提示策略和任务中难以一致遵守时间截止约束，事前推理能力存在持续挑战。

Conclusion: 该基准为提升大语言模型在时间敏感应用中的时序推理能力提供了潜在评估框架。

Abstract: Large language models (LLMs) face significant challenges in ex-ante
reasoning, where analysis, inference, or predictions must be made without
access to information from future events. Even with explicit prompts enforcing
temporal cutoffs, LLMs often generate outputs influenced by internalized
knowledge of events beyond the specified cutoff. This paper introduces a novel
task and benchmark designed to evaluate the ability of LLMs to reason while
adhering to such temporal constraints. The benchmark includes a variety of
tasks: stock prediction, Wikipedia event prediction, scientific publication
prediction, and Question Answering (QA), designed to assess factual knowledge
under temporal cutoff constraints. We use leakage rate to quantify models'
reliance on future information beyond cutoff timestamps. Experimental results
reveal that LLMs struggle to consistently adhere to temporal cutoffs across
common prompting strategies and tasks, demonstrating persistent challenges in
ex-ante reasoning. This benchmark provides a potential evaluation framework to
advance the development of LLMs' temporal reasoning ability for time-sensitive
applications.

</details>


### [572] [Cuff-KT: Tackling Learners' Real-time Learning Pattern Adjustment via Tuning-Free Knowledge State Guided Model Updating](https://arxiv.org/abs/2505.19543)
*Yiyun Zhou,Zheqi Lv,Shengyu Zhang,Jingyuan Chen*

Main category: cs.LG

TL;DR: 论文提出Cuff-KT模型，通过控制器和生成器动态调整学习者能力变化，显著提升知识追踪模型在实时学习模式调整任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统知识追踪模型假设学习者能力短期稳定或可预测变化，但现实中因认知疲劳、动机等因素，能力变化不规则。现有模型适应性不足，重训练导致过拟合和高时间成本。

Method: 提出Cuff-KT模型，包含控制器（分配学习者价值分数）和生成器（生成个性化参数），无需微调即可快速灵活适应数据变化。

Result: 在五个学科数据集上，Cuff-KT显著提升五种不同结构KT模型的性能，AUC平均相对提升10%（学习者内）和4%（学习者间），时间成本可忽略。

Conclusion: Cuff-KT有效解决了实时学习模式调整任务，代码和数据集已开源。

Abstract: Knowledge Tracing (KT) is a core component of Intelligent Tutoring Systems,
modeling learners' knowledge state to predict future performance and provide
personalized learning support. Traditional KT models assume that learners'
learning abilities remain relatively stable over short periods or change in
predictable ways based on prior performance. However, in reality, learners'
abilities change irregularly due to factors like cognitive fatigue, motivation,
and external stress -- a task introduced, which we refer to as Real-time
Learning Pattern Adjustment (RLPA). Existing KT models, when faced with RLPA,
lack sufficient adaptability, because they fail to timely account for the
dynamic nature of different learners' evolving learning patterns. Current
strategies for enhancing adaptability rely on retraining, which leads to
significant overfitting and high time overhead issues. To address this, we
propose Cuff-KT, comprising a controller and a generator. The controller
assigns value scores to learners, while the generator generates personalized
parameters for selected learners. Cuff-KT controllably adapts to data changes
fast and flexibly without fine-tuning. Experiments on five datasets from
different subjects demonstrate that Cuff-KT significantly improves the
performance of five KT models with different structures under intra- and
inter-learner shifts, with an average relative increase in AUC of 10% and 4%,
respectively, at a negligible time cost, effectively tackling RLPA task. Our
code and datasets are fully available at https://github.com/zyy-2001/Cuff-KT.

</details>


### [573] [STRAP: Spatio-Temporal Pattern Retrieval for Out-of-Distribution Generalization](https://arxiv.org/abs/2505.19547)
*Haoyu Zhang,Wentao Zhang,Hao Miao,Xinke Jiang,Yuchen Fang,Yifan Zhang*

Main category: cs.LG

TL;DR: 论文提出STRAP框架，通过检索增强学习提升时空图神经网络在分布外场景的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有时空图神经网络在时空分布外场景中泛化能力不足，需要一种新方法提升模型适应性。

Method: STRAP框架构建时空模式库，通过检索相似模式并注入模型，结合知识平衡目标优化学习过程。

Result: 在多个流式图数据集上，STRAP显著优于现有方法，展现了强鲁棒性和泛化能力。

Conclusion: STRAP为时空图神经网络提供了一种有效的持续学习方案，无需任务微调即可实现优越性能。

Abstract: Spatio-Temporal Graph Neural Networks (STGNNs) have emerged as a powerful
tool for modeling dynamic graph-structured data across diverse domains.
However, they often fail to generalize in Spatio-Temporal Out-of-Distribution
(STOOD) scenarios, where both temporal dynamics and spatial structures evolve
beyond the training distribution. To address this problem, we propose an
innovative Spatio-Temporal Retrieval-Augmented Pattern Learning
framework,STRAP, which enhances model generalization by integrating
retrieval-augmented learning into the STGNN continue learning pipeline. The
core of STRAP is a compact and expressive pattern library that stores
representative spatio-temporal patterns enriched with historical, structural,
and semantic information, which is obtained and optimized during the training
phase. During inference, STRAP retrieves relevant patterns from this library
based on similarity to the current input and injects them into the model via a
plug-and-play prompting mechanism. This not only strengthens spatio-temporal
representations but also mitigates catastrophic forgetting. Moreover, STRAP
introduces a knowledge-balancing objective to harmonize new information with
retrieved knowledge. Extensive experiments across multiple real-world streaming
graph datasets show that STRAP consistently outperforms state-of-the-art STGNN
baselines on STOOD tasks, demonstrating its robustness, adaptability, and
strong generalization capability without task-specific fine-tuning.

</details>


### [574] [On scalable and efficient training of diffusion samplers](https://arxiv.org/abs/2505.19552)
*Minkyu Kim,Kiyoung Seong,Dongyeop Woo,Sungsoo Ahn,Minsu Kim*

Main category: cs.LG

TL;DR: 本文提出了一种结合MCMC搜索器和扩散采样器的高效采样框架，解决了高维空间中能量评估昂贵和模式坍塌的问题，显著提升了采样效率。


<details>
  <summary>Details</summary>
Motivation: 扩散采样器在无数据条件下从未归一化能量分布中采样时面临高维空间和昂贵能量评估的挑战，且易受训练早期偏好（primacy bias）导致的模式坍塌影响。

Method: 提出混合MCMC搜索器（基于新颖性辅助能量）与扩散采样器的框架：MCMC探索扩散采样器未覆盖的模式，生成离策略样本，与在策略数据结合训练；并通过周期性重新初始化缓解primacy bias。

Result: 方法在标准基准测试中显著提升采样效率，适用于高维问题和真实世界分子构象生成任务。

Conclusion: 通过协同经典采样与扩散模型，并解决训练偏差，该框架为复杂能量分布的高效采样提供了可扩展方案。

Abstract: We address the challenge of training diffusion models to sample from
unnormalized energy distributions in the absence of data, the so-called
diffusion samplers. Although these approaches have shown promise, they struggle
to scale in more demanding scenarios where energy evaluations are expensive and
the sampling space is high-dimensional. To address this limitation, we propose
a scalable and sample-efficient framework that properly harmonizes the powerful
classical sampling method and the diffusion sampler. Specifically, we utilize
Monte Carlo Markov chain (MCMC) samplers with a novelty-based auxiliary energy
as a Searcher to collect off-policy samples, using an auxiliary energy function
to compensate for exploring modes the diffusion sampler rarely visits. These
off-policy samples are then combined with on-policy data to train the diffusion
sampler, thereby expanding its coverage of the energy landscape. Furthermore,
we identify primacy bias, i.e., the preference of samplers for early experience
during training, as the main cause of mode collapse during training, and
introduce a periodic re-initialization trick to resolve this issue. Our method
significantly improves sample efficiency on standard benchmarks for diffusion
samplers and also excels at higher-dimensional problems and real-world
molecular conformer generation.

</details>


### [575] [Lego Sketch: A Scalable Memory-augmented Neural Network for Sketching Data Streams](https://arxiv.org/abs/2505.19561)
*Yuan Feng,Yukun Cao,Hairu Wang,Xike Xie,S Kevin Zhou*

Main category: cs.LG

TL;DR: 该论文提出了一种名为Lego sketch的新型神经草图，通过模块化内存块动态适应不同空间预算和数据域，显著提升了空间-准确度权衡。


<details>
  <summary>Details</summary>
Motivation: 现有神经草图由于内存增强神经网络（MANN）配置不灵活，难以在不同数据域和空间预算下扩展。

Method: 引入了一种可扩展的MANN架构，动态协调多个内存块，类似乐高积木组装，以适应不同需求。

Result: 理论分析证明了其高可扩展性，实验表明Lego sketch在空间-准确度权衡上优于现有方法。

Conclusion: Lego sketch通过模块化设计实现了更好的可扩展性和准确性，为神经草图提供了首个误差界限。

Abstract: Sketches, probabilistic structures for estimating item frequencies in
infinite data streams with limited space, are widely used across various
domains. Recent studies have shifted the focus from handcrafted sketches to
neural sketches, leveraging memory-augmented neural networks (MANNs) to enhance
the streaming compression capabilities and achieve better space-accuracy
trade-offs.However, existing neural sketches struggle to scale across different
data domains and space budgets due to inflexible MANN configurations. In this
paper, we introduce a scalable MANN architecture that brings to life the {\it
Lego sketch}, a novel sketch with superior scalability and accuracy. Much like
assembling creations with modular Lego bricks, the Lego sketch dynamically
coordinates multiple memory bricks to adapt to various space budgets and
diverse data domains. Our theoretical analysis guarantees its high scalability
and provides the first error bound for neural sketch. Furthermore, extensive
experimental evaluations demonstrate that the Lego sketch exhibits superior
space-accuracy trade-offs, outperforming existing handcrafted and neural
sketches. Our code is available at https://github.com/FFY0/LegoSketch_ICML.

</details>


### [576] [Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing](https://arxiv.org/abs/2505.19578)
*Dan Peng,Zhihui Fu,Zewen Ye,Zhuoran Song,Jun Wang*

Main category: cs.LG

TL;DR: 提出了一种高精度稀疏注意力机制，通过跨头共享精确的注意力模式，显著提升长上下文推理的预填充阶段效率，同时保持最佳准确性。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏注意力方法依赖预定义模式或不准确估计，无法完全捕捉注意力的真实动态，导致效率降低和准确性受损。

Method: 基于注意力模式在头间高度相似且跨输入一致的观察，通过跨头共享精确计算的注意力模式，仅需对少量头进行完整注意力计算。

Result: 在保持或优于现有方法加速比的同时，实现了最佳整体准确性。

Conclusion: 该方法有效平衡了效率与准确性，为长上下文推理提供了一种更优的稀疏注意力解决方案。

Abstract: Sparse attention methods exploit the inherent sparsity in attention to speed
up the prefilling phase of long-context inference, mitigating the quadratic
complexity of full attention computation. While existing sparse attention
methods rely on predefined patterns or inaccurate estimations to approximate
attention behavior, they often fail to fully capture the true dynamics of
attention, resulting in reduced efficiency and compromised accuracy. Instead,
we propose a highly accurate sparse attention mechanism that shares similar yet
precise attention patterns across heads, enabling a more realistic capture of
the dynamic behavior of attention. Our approach is grounded in two key
observations: (1) attention patterns demonstrate strong inter-head similarity,
and (2) this similarity remains remarkably consistent across diverse inputs. By
strategically sharing computed accurate patterns across attention heads, our
method effectively captures actual patterns while requiring full attention
computation for only a small subset of heads. Comprehensive evaluations
demonstrate that our approach achieves superior or comparable speedup relative
to state-of-the-art methods while delivering the best overall accuracy.

</details>


### [577] [WQLCP: Weighted Adaptive Conformal Prediction for Robust Uncertainty Quantification Under Distribution Shifts](https://arxiv.org/abs/2505.19587)
*Shadi Alijani,Homayoun Najjaran*

Main category: cs.LG

TL;DR: 论文提出两种改进的共形预测方法（RLSCP和WQLCP），通过引入重构损失和加权交换性概念，解决分布偏移下的覆盖率和预测集大小问题。


<details>
  <summary>Details</summary>
Motivation: 传统共形预测（CP）假设数据可交换，但现实场景中的分布偏移会破坏这一假设，导致覆盖率不可靠和预测集膨胀。

Method: 1. RLSCP：利用VAE的重构损失作为不确定性度量缩放评分函数；2. WQLCP：引入加权交换性概念，根据校准集与测试集损失比值调整分位数阈值。

Result: 在ImageNet等数据集上的实验表明，WQLCP能稳定保持覆盖率，同时减小预测集大小，优于现有基线方法。

Conclusion: WQLCP为分布偏移下的共形预测提供了鲁棒解决方案，平衡了覆盖率与预测集效率。

Abstract: Conformal prediction (CP) provides a framework for constructing prediction
sets with guaranteed coverage, assuming exchangeable data. However, real-world
scenarios often involve distribution shifts that violate exchangeability,
leading to unreliable coverage and inflated prediction sets. To address this
challenge, we first introduce Reconstruction Loss-Scaled Conformal Prediction
(RLSCP), which utilizes reconstruction losses derived from a Variational
Autoencoder (VAE) as an uncertainty metric to scale score functions. While
RLSCP demonstrates performance improvements, mainly resulting in better
coverage, it quantifies quantiles based on a fixed calibration dataset without
considering the discrepancies between test and train datasets in an
unexchangeable setting. In the next step, we propose Weighted Quantile
Loss-scaled Conformal Prediction (WQLCP), which refines RLSCP by incorporating
a weighted notion of exchangeability, adjusting the calibration quantile
threshold based on weights with respect to the ratio of calibration and test
loss values. This approach improves the CP-generated prediction set outputs in
the presence of distribution shifts. Experiments on large-scale datasets,
including ImageNet variants, demonstrate that WQLCP outperforms existing
baselines by consistently maintaining coverage while reducing prediction set
sizes, providing a robust solution for CP under distribution shifts.

</details>


### [578] [Model Agnostic Differentially Private Causal Inference](https://arxiv.org/abs/2505.19589)
*Christiant Lebeda,Mathieu Even,Aurélien Bellet,Julie Josse*

Main category: cs.LG

TL;DR: 提出一种模型无关的差分隐私框架，用于估计平均处理效应（ATE），通过分离隐私保护与干扰项估计，支持灵活的黑盒模型，并在实际隐私预算下保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 在医学、经济学和社会科学等领域，从观测数据中估计因果效应至关重要，但隐私问题限制了数据的使用。现有方法通过直接对干扰项进行隐私化处理，导致隐私成本随模型复杂度增加。本文旨在解决这一问题。

Method: 提出一种通用框架，将干扰项估计与隐私保护分离。通过折叠拆分和集成技术，仅对预测和聚合步骤进行扰动，支持三种经典估计器（G-formula、IPW和AIPW），并提供正式的效用和隐私保证。

Result: 实验结果表明，该方法在实际隐私预算下保持竞争力，并扩展支持多个私有ATE估计的元分析。

Conclusion: 该框架填补了因果推断与隐私保护数据分析之间的关键空白，为相关领域提供了实用的解决方案。

Abstract: Estimating causal effects from observational data is essential in fields such
as medicine, economics and social sciences, where privacy concerns are
paramount. We propose a general, model-agnostic framework for differentially
private estimation of average treatment effects (ATE) that avoids strong
structural assumptions on the data-generating process or the models used to
estimate propensity scores and conditional outcomes. In contrast to prior work,
which enforces differential privacy by directly privatizing these nuisance
components and results in a privacy cost that scales with model complexity, our
approach decouples nuisance estimation from privacy protection. This separation
allows the use of flexible, state-of-the-art black-box models, while
differential privacy is achieved by perturbing only predictions and aggregation
steps within a fold-splitting scheme with ensemble techniques. We instantiate
the framework for three classical estimators -- the G-formula, inverse
propensity weighting (IPW), and augmented IPW (AIPW) -- and provide formal
utility and privacy guarantees. Empirical results show that our methods
maintain competitive performance under realistic privacy budgets. We further
extend our framework to support meta-analysis of multiple private ATE
estimates. Our results bridge a critical gap between causal inference and
privacy-preserving data analysis.

</details>


### [579] [Learning to Reason without External Rewards](https://arxiv.org/abs/2505.19590)
*Xuandong Zhao,Zhewei Kang,Aosong Feng,Sergey Levine,Dawn Song*

Main category: cs.LG

TL;DR: 论文提出了一种名为Intuitor的无监督强化学习方法，利用模型自身的置信度作为奖励信号，无需外部监督即可训练大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 传统基于可验证奖励的强化学习方法（RLVR）依赖昂贵且领域特定的监督信号，限制了其可扩展性。本文探索如何利用模型内部信号实现无监督学习。

Method: 提出RLIF框架及Intuitor方法，用模型的自我置信度（self-certainty）替代外部奖励，在GRPO算法中实现完全无监督学习。

Result: 实验表明Intuitor在数学推理任务上匹配GRPO性能，在代码生成等跨领域任务中表现更优，且无需标注数据或测试用例。

Conclusion: 模型内在信号可作为跨领域学习的有效驱动机制，为缺乏可验证奖励的场景提供了可扩展的替代方案。

Abstract: Training large language models (LLMs) for complex reasoning via Reinforcement
Learning with Verifiable Rewards (RLVR) is effective but limited by reliance on
costly, domain-specific supervision. We explore Reinforcement Learning from
Internal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic
signals without external rewards or labeled data. We propose Intuitor, an RLIF
method that uses a model's own confidence, termed self-certainty, as its sole
reward signal. Intuitor replaces external rewards in Group Relative Policy
Optimization (GRPO) with self-certainty scores, enabling fully unsupervised
learning. Experiments demonstrate that Intuitor matches GRPO's performance on
mathematical benchmarks while achieving superior generalization to
out-of-domain tasks like code generation, without requiring gold solutions or
test cases. Our findings show that intrinsic model signals can drive effective
learning across domains, offering a scalable alternative to RLVR for autonomous
AI systems where verifiable rewards are unavailable. Code is available at
https://github.com/sunblaze-ucb/Intuitor

</details>


### [580] [Preference Optimization by Estimating the Ratio of the Data Distribution](https://arxiv.org/abs/2505.19601)
*Yeongmin Kim,Heesun Bae,Byeonghu Na,Il-Chul Moon*

Main category: cs.LG

TL;DR: 本文提出了一种广义的DPO损失函数——Bregman偏好优化（BPO），通过比率匹配框架实现目标策略最优性，同时保持简单性和理论保证。BPO不仅包含DPO作为特例，还能提升生成质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 当前直接偏好优化（DPO）方法虽简单稳定，但无法同时兼顾理论保证和灵活性。本文旨在提出一种广义框架，既能保留DPO的优势，又能解决其局限性。

Method: 提出Bregman偏好优化（BPO），通过比率匹配框架生成一族目标函数，并开发了梯度缩放方法SBA。BPO框架兼容现有DPO变体，且实现简便。

Result: BPO在Llama-3-8B模型上实现了当前最佳性能，AlpacaEval2的胜率达55.9%。相比DPO和其他扩展方法，BPO同时提升了胜率和生成多样性。

Conclusion: BPO为偏好优化提供了通用框架，兼具理论严谨性和实践有效性，为语言模型对齐提供了新方向。

Abstract: Direct preference optimization (DPO) is widely used as a simple and stable
method for aligning large language models (LLMs) with human preferences. This
paper investigates a generalized DPO loss that enables a policy model to match
the target policy from a likelihood ratio estimation perspective. The ratio of
the target policy provides a unique identification of the policy distribution
without relying on reward models or partition functions. This allows the
generalized loss to retain both simplicity and theoretical guarantees, which
prior work such as $f$-PO fails to achieve simultaneously. We propose Bregman
preference optimization (BPO), a generalized framework for ratio matching that
provides a family of objective functions achieving target policy optimality.
BPO subsumes DPO as a special case and offers tractable forms for all
instances, allowing implementation with a few lines of code. We further develop
scaled Basu's power divergence (SBA), a gradient scaling method that can be
used for BPO instances. The BPO framework complements other DPO variants and is
applicable to target policies defined by these variants. In experiments, unlike
other probabilistic loss extensions such as $f$-DPO or $f$-PO, which exhibit a
trade-off between generation fidelity and diversity, instances of BPO improve
both win rate and entropy compared with DPO. When applied to
Llama-3-Instruct-8B, BPO achieves state-of-the-art performance among Llama-3-8B
backbones, with a 55.9\% length-controlled win rate on AlpacaEval2.

</details>


### [581] [Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV Cache Compression](https://arxiv.org/abs/2505.19602)
*Kunjun Li,Zigeng Chen,Cheng-Yen Yang,Jenq-Neng Hwang*

Main category: cs.LG

TL;DR: ScaleKV提出了一种针对VAR模型的KV缓存压缩框架，通过分层管理显著降低内存消耗并保持生成质量。


<details>
  <summary>Details</summary>
Motivation: VAR模型在推理过程中KV缓存呈指数增长，导致内存消耗和计算冗余问题严重，亟需优化方案。

Method: 基于Transformer层间缓存需求差异和注意力模式特点，将层级分为草稿层（多尺度分散注意力）和精修层（局部聚焦），实现分级缓存管理。

Result: 在Infinity文生图模型上验证，将KV缓存内存需求降低至10%的同时保持像素级生成质量。

Conclusion: ScaleKV通过多尺度推理管道优化，有效解决了VAR模型的内存瓶颈问题。

Abstract: Visual Autoregressive (VAR) modeling has garnered significant attention for
its innovative next-scale prediction approach, which yields substantial
improvements in efficiency, scalability, and zero-shot generalization.
Nevertheless, the coarse-to-fine methodology inherent in VAR results in
exponential growth of the KV cache during inference, causing considerable
memory consumption and computational redundancy. To address these bottlenecks,
we introduce ScaleKV, a novel KV cache compression framework tailored for VAR
architectures. ScaleKV leverages two critical observations: varying cache
demands across transformer layers and distinct attention patterns at different
scales. Based on these insights, ScaleKV categorizes transformer layers into
two functional groups: drafters and refiners. Drafters exhibit dispersed
attention across multiple scales, thereby requiring greater cache capacity.
Conversely, refiners focus attention on the current token map to process local
details, consequently necessitating substantially reduced cache capacity.
ScaleKV optimizes the multi-scale inference pipeline by identifying
scale-specific drafters and refiners, facilitating differentiated cache
management tailored to each scale. Evaluation on the state-of-the-art
text-to-image VAR model family, Infinity, demonstrates that our approach
effectively reduces the required KV cache memory to 10% while preserving
pixel-level fidelity.

</details>


### [582] [Kuramoto-FedAvg: Using Synchronization Dynamics to Improve Federated Learning Optimization under Statistical Heterogeneity](https://arxiv.org/abs/2505.19605)
*Aggrey Muhebwa,Khotso Selialia,Fatima Anwar,Khalid K. Osman*

Main category: cs.LG

TL;DR: 提出Kuramoto-FedAvg算法，通过同步机制减少客户端漂移，加速非独立同分布数据下的联邦学习收敛。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在非独立同分布（non-IID）客户端数据上因客户端漂移导致的收敛缓慢问题。

Method: 将权重聚合步骤重新定义为基于Kuramoto耦合振荡器模型的同步问题，服务器根据客户端更新与全局更新的相位对齐动态加权。

Result: 理论证明该机制减少客户端漂移并提供更紧的收敛界，实验验证其在多个基准数据集上显著加速收敛并提高准确性。

Conclusion: 基于协调和同步的策略在管理梯度多样性和加速非IID场景下的联邦优化方面具有潜力。

Abstract: Federated learning on heterogeneous (non-IID) client data experiences slow
convergence due to client drift. To address this challenge, we propose
Kuramoto-FedAvg, a federated optimization algorithm that reframes the weight
aggregation step as a synchronization problem inspired by the Kuramoto model of
coupled oscillators. The server dynamically weighs each client's update based
on its phase alignment with the global update, amplifying contributions that
align with the global gradient direction while minimizing the impact of updates
that are out of phase. We theoretically prove that this synchronization
mechanism reduces client drift, providing a tighter convergence bound compared
to the standard FedAvg under heterogeneous data distributions. Empirical
validation supports our theoretical findings, showing that Kuramoto-FedAvg
significantly accelerates convergence and improves accuracy across multiple
benchmark datasets. Our work highlights the potential of coordination and
synchronization-based strategies for managing gradient diversity and
accelerating federated optimization in realistic non-IID settings.

</details>


### [583] [Energy-based Preference Optimization for Test-time Adaptation](https://arxiv.org/abs/2505.19607)
*Yewon Han,Seoyun Yang,Taesup Kim*

Main category: cs.LG

TL;DR: EPOTTA提出了一种无需采样的测试时自适应方法，通过能量偏好优化提高模型在目标分布上的性能，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时自适应方法依赖于不确定的预测，性能不可靠；而基于能量的方法虽能处理分布偏移，但需要大量采样，不适用于实时场景。

Method: EPOTTA利用预训练模型和残差能量函数参数化目标模型，无需采样即可最大化目标数据的边际似然，并通过数学等效性直接优化模型。

Result: 实验表明，EPOTTA在保持计算效率的同时，具有良好的校准性和性能表现。

Conclusion: EPOTTA提供了一种高效可靠的测试时自适应方法，解决了现有方法在采样和预测不确定性上的局限性。

Abstract: Test-Time Adaptation (TTA) enhances model robustness by enabling adaptation
to target distributions that differ from training distributions, improving
real-world generalizability. Existing TTA approaches focus on adjusting the
conditional distribution; however these methods often depend on uncertain
predictions in the absence of label information, leading to unreliable
performance. Energy-based frameworks suggest a promising alternative to address
distribution shifts without relying on uncertain predictions, instead computing
the marginal distribution of target data. However, they involve the critical
challenge of requiring extensive SGLD sampling, which is impractical for
test-time scenarios requiring immediate adaptation. In this work, we propose
Energy-based Preference Optimization for Test-time Adaptation (EPOTTA), which
is based on a sampling free strategy. We first parameterize the target model
using a pretrained model and residual energy function, enabling marginal
likelihood maximization of target data without sampling. Building on the
observation that the parameterization is mathematically equivalent to DPO
objective, we then directly adapt the model to a target distribution without
explicitly training the residual. Our experiments verify that EPOTTA is
well-calibrated and performant while achieving computational efficiency.

</details>


### [584] [Skrull: Towards Efficient Long Context Fine-tuning through Dynamic Data Scheduling](https://arxiv.org/abs/2505.19609)
*Hongtao Xu,Wenting Shen,Yuanxin Wei,Ang Wang,Guo Runfan,Tianxing Wang,Yong Li,Mingzhen Li,Weile Jia*

Main category: cs.LG

TL;DR: 论文提出了一种名为Skrull的动态数据调度器，用于提升长上下文监督微调(Long-SFT)的效率，通过平衡长短序列的计算需求，显著提高了训练性能。


<details>
  <summary>Details</summary>
Motivation: 长上下文监督微调(Long-SFT)在提升大语言模型(LLMs)处理长上下文任务性能方面至关重要。然而，现有训练系统难以同时高效处理长短序列混合的数据分布，导致训练效率低下。

Method: 论文提出Skrull，一种动态数据调度器，通过动态调度数据平衡长短序列的计算需求，并将调度过程建模为联合优化问题，采用轻量级调度算法实现近乎零成本的在线调度。

Result: 实验结果表明，Skrull在真实长-SFT场景中平均比DeepSpeed快3.76倍，最高可达7.54倍。

Conclusion: Skrull通过动态数据调度有效解决了长-SFT中数据分布不均的问题，显著提升了训练效率，为长上下文任务的高效训练提供了新思路。

Abstract: Long-context supervised fine-tuning (Long-SFT) plays a vital role in
enhancing the performance of large language models (LLMs) on long-context
tasks. To smoothly adapt LLMs to long-context scenarios, this process typically
entails training on mixed datasets containing both long and short sequences.
However, this heterogeneous sequence length distribution poses significant
challenges for existing training systems, as they fail to simultaneously
achieve high training efficiency for both long and short sequences, resulting
in sub-optimal end-to-end system performance in Long-SFT. In this paper, we
present a novel perspective on data scheduling to address the challenges posed
by the heterogeneous data distributions in Long-SFT. We propose Skrull, a
dynamic data scheduler specifically designed for efficient long-SFT. Through
dynamic data scheduling, Skrull balances the computation requirements of long
and short sequences, improving overall training efficiency. Furthermore, we
formulate the scheduling process as a joint optimization problem and thoroughly
analyze the trade-offs involved. Based on those analysis, Skrull employs a
lightweight scheduling algorithm to achieve near-zero cost online scheduling in
Long-SFT. Finally, we implement Skrull upon DeepSpeed, a state-of-the-art
distributed training system for LLMs. Experimental results demonstrate that
Skrull outperforms DeepSpeed by 3.76x on average (up to 7.54x) in real-world
long-SFT scenarios.

</details>


### [585] [Multiplicity is an Inevitable and Inherent Challenge in Multimodal Learning](https://arxiv.org/abs/2505.19614)
*Sanghyuk Chun*

Main category: cs.LG

TL;DR: 该论文指出当前多模态学习假设模态间一对一确定性对齐的局限性，提出多对多的'多重性'关系是本质瓶颈，呼吁开发考虑多重性的新学习框架和数据构建方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法简化了真实世界多模态间复杂的多对多关系（多重性），这种关系源于语义抽象、表征不对称和任务依赖性模糊，而非噪声或标注错误。

Method: 通过分析多重性在多模态学习流程（数据构建、训练、评估）中的表现，论证其导致的训练不确定性、评估不可靠及数据质量低下问题。

Result: 阐明了多重性对多模态学习各阶段的系统性影响，并揭示现有确定性对齐假设的不足。

Conclusion: 需开发考虑多重性的新型学习框架和数据构建协议，以解决这一根本性瓶颈问题。

Abstract: Multimodal learning has seen remarkable progress, particularly with the
emergence of large-scale pre-training across various modalities. However, most
current approaches are built on the assumption of a deterministic, one-to-one
alignment between modalities. This oversimplifies real-world multimodal
relationships, where their nature is inherently many-to-many. This phenomenon,
named multiplicity, is not a side-effect of noise or annotation error, but an
inevitable outcome of semantic abstraction, representational asymmetry, and
task-dependent ambiguity in multimodal tasks. This position paper argues that
multiplicity is a fundamental bottleneck that manifests across all stages of
the multimodal learning pipeline: from data construction to training and
evaluation. This paper examines the causes and consequences of multiplicity,
and highlights how multiplicity introduces training uncertainty, unreliable
evaluation, and low dataset quality. This position calls for new research
directions on multimodal learning: novel multiplicity-aware learning frameworks
and dataset construction protocols considering multiplicity.

</details>


### [586] [Diagnosing and Mitigating Modality Interference in Multimodal Large Language Models](https://arxiv.org/abs/2505.19616)
*Rui Cai,Bangzheng Li,Xiaofei Wen,Muhao Chen,Zhe Zhao*

Main category: cs.LG

TL;DR: 多模态大语言模型（MLLMs）在跨模态任务中存在模态干扰问题，导致性能下降。本文提出了一种基于扰动和一致性正则化的微调框架，显著提升了模型的鲁棒性和跨模态能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉问答等任务中难以区分任务相关与无关信号，易受误导性输入影响，导致跨模态能力不足。本文旨在解决这一模态干扰问题。

Method: 提出了一种微调框架，包括基于启发式和对抗性扰动的数据增强，以及针对原始和扰动输入的一致性正则化策略。

Result: 在多个基准数据集和不同规模的模型上实验表明，该方法显著提升了模型的鲁棒性和跨模态能力，同时增强了单模态推理能力。

Conclusion: 本文提出的方法有效缓解了模态干扰问题，提升了多模态大语言模型在跨模态任务中的性能，同时增强了单模态推理能力。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities across tasks, yet they often exhibit difficulty in distinguishing
task-relevant from irrelevant signals, particularly in tasks like Visual
Question Answering (VQA), which can lead to susceptibility to misleading or
spurious inputs. We refer to this broader limitation as the Cross-Modality
Competency Problem: the model's inability to fairly evaluate all modalities.
This vulnerability becomes more evident in modality-specific tasks such as
image classification or pure text question answering, where models are expected
to rely solely on one modality. In such tasks, spurious information from
irrelevant modalities often leads to significant performance degradation. We
refer to this failure as Modality Interference, which serves as a concrete and
measurable instance of the cross-modality competency problem. We further design
a perturbation-based causal diagnostic experiment to verify and quantify this
problem. To mitigate modality interference, we propose a novel framework to
fine-tune MLLMs, including perturbation-based data augmentations with both
heuristic perturbations and adversarial perturbations via Projected Gradient
Descent (PGD), and a consistency regularization strategy applied to model
outputs with original and perturbed inputs. Experiments on multiple benchmark
datasets (image-heavy, text-heavy, and VQA tasks) and multiple model families
with different scales demonstrate significant improvements in robustness and
cross-modality competency, indicating our method's effectiveness in boosting
unimodal reasoning ability while enhancing performance on multimodal tasks.

</details>


### [587] [SESaMo: Symmetry-Enforcing Stochastic Modulation for Normalizing Flows](https://arxiv.org/abs/2505.19619)
*Janik Kreit,Dominic Schuh,Kim A. Nicoli,Lena Funcke*

Main category: cs.LG

TL;DR: 本文提出了一种名为SESaMo的新方法，通过随机调制技术将对称性等先验知识融入归一化流，提升了生成模型的灵活性和性能。


<details>
  <summary>Details</summary>
Motivation: 在物理和化学等领域，从非归一化的玻尔兹曼分布中采样是一个基本挑战。现有的自回归模型和归一化流因其封闭概率密度的优势受到关注，但如何有效融入对称性等先验知识以提升性能仍需探索。

Method: 本文提出Symmetry-Enforcing Stochastic Modulation (SESaMo)，通过随机调制技术将归纳偏置（如对称性）融入归一化流，增强生成模型的灵活性。

Result: 数值实验表明，SESaMo在多种场景（如8-高斯混合模型、φ4理论和Hubbard模型）中表现优异，能够有效学习精确和破缺的对称性。

Conclusion: SESaMo通过随机调制技术成功将对称性等先验知识融入生成模型，显著提升了模型性能，为复杂分布采样提供了新思路。

Abstract: Deep generative models have recently garnered significant attention across
various fields, from physics to chemistry, where sampling from unnormalized
Boltzmann-like distributions represents a fundamental challenge. In particular,
autoregressive models and normalizing flows have become prominent due to their
appealing ability to yield closed-form probability densities. Moreover, it is
well-established that incorporating prior knowledge - such as symmetries - into
deep neural networks can substantially improve training performances. In this
context, recent advances have focused on developing symmetry-equivariant
generative models, achieving remarkable results. Building upon these
foundations, this paper introduces Symmetry-Enforcing Stochastic Modulation
(SESaMo). Similar to equivariant normalizing flows, SESaMo enables the
incorporation of inductive biases (e.g., symmetries) into normalizing flows
through a novel technique called stochastic modulation. This approach enhances
the flexibility of the generative model, allowing to effectively learn a
variety of exact and broken symmetries. Our numerical experiments benchmark
SESaMo in different scenarios, including an 8-Gaussian mixture model and
physically relevant field theories, such as the $\phi^4$ theory and the Hubbard
model.

</details>


### [588] [Decoupling Spatio-Temporal Prediction: When Lightweight Large Models Meet Adaptive Hypergraphs](https://arxiv.org/abs/2505.19620)
*Jiawen Chen,Qi Shao,Duxin Chen,Wenwu Yu*

Main category: cs.LG

TL;DR: 提出STH-SepNet框架，通过解耦时空建模，结合轻量级大语言模型和自适应超图神经网络，提升时空预测的效率和精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在模型表达能力和计算效率之间难以平衡，尤其是在处理大规模现实数据集时。

Method: STH-SepNet框架将时间维度建模与空间维度建模解耦，时间维度使用轻量级大语言模型，空间维度采用自适应超图神经网络，并通过门控机制融合时空表示。

Result: 在多个大规模现实数据集上的实验表明，STH-SepNet在提升预测性能的同时保持了计算效率。

Conclusion: STH-SepNet为时空预测提供了一个轻量级且可扩展的解决方案，有望在实际应用中减少计算需求并提升预测性能。

Abstract: Spatio-temporal prediction is a pivotal task with broad applications in
traffic management, climate monitoring, energy scheduling, etc. However,
existing methodologies often struggle to balance model expressiveness and
computational efficiency, especially when scaling to large real-world datasets.
To tackle these challenges, we propose STH-SepNet (Spatio-Temporal Hypergraph
Separation Networks), a novel framework that decouples temporal and spatial
modeling to enhance both efficiency and precision. Therein, the temporal
dimension is modeled using lightweight large language models, which effectively
capture low-rank temporal dynamics. Concurrently, the spatial dimension is
addressed through an adaptive hypergraph neural network, which dynamically
constructs hyperedges to model intricate, higher-order interactions. A
carefully designed gating mechanism is integrated to seamlessly fuse temporal
and spatial representations. By leveraging the fundamental principles of
low-rank temporal dynamics and spatial interactions, STH-SepNet offers a
pragmatic and scalable solution for spatio-temporal prediction in real-world
applications. Extensive experiments on large-scale real-world datasets across
multiple benchmarks demonstrate the effectiveness of STH-SepNet in boosting
predictive performance while maintaining computational efficiency. This work
may provide a promising lightweight framework for spatio-temporal prediction,
aiming to reduce computational demands and while enhancing predictive
performance. Our code is avaliable at
https://github.com/SEU-WENJIA/ST-SepNet-Lightweight-LLMs-Meet-Adaptive-Hypergraphs.

</details>


### [589] [When fractional quasi p-norms concentrate](https://arxiv.org/abs/2505.19635)
*Ivan Y. Tyukin,Bogdan Grechuk,Evgeny M. Mirkes,Alexander N. Gorban*

Main category: cs.LG

TL;DR: 该论文首次明确了高维空间中分数拟p-范数距离的集中与非集中条件，解决了长期存在的理论争议，并揭示了通过调整p值控制距离集中率的可能性。


<details>
  <summary>Details</summary>
Motivation: 高维数据中距离的集中现象对设计稳定可靠的数据分析算法至关重要。长期以来，关于分数拟p-范数（p∈(0,1)）在高维空间中的集中性存在理论与实证争议，本文旨在解决这一核心问题。

Method: 通过理论分析，识别分数拟p-范数在不同分布类下的集中与非集中条件，并验证其指数级和均匀集中界的存在性。同时探索通过调整p值控制集中率的分布族条件。

Result: 研究表明，对于广泛分布类，分数拟p-范数具有指数级均匀集中界，否定了通过“最优”p值缓解集中的方法。但特定分布族仍可通过p值选择控制集中率，且在均匀集中分布附近存在无数反集中分布。

Conclusion: 该研究为高维距离集中问题提供了新见解，解决了理论与实证的矛盾，并启发了通过数据编码方案调控距离集中的新思路。

Abstract: Concentration of distances in high dimension is an important factor for the
development and design of stable and reliable data analysis algorithms. In this
paper, we address the fundamental long-standing question about the
concentration of distances in high dimension for fractional quasi $p$-norms,
$p\in(0,1)$. The topic has been at the centre of various theoretical and
empirical controversies. Here we, for the first time, identify conditions when
fractional quasi $p$-norms concentrate and when they don't. We show that
contrary to some earlier suggestions, for broad classes of distributions,
fractional quasi $p$-norms admit exponential and uniform in $p$ concentration
bounds. For these distributions, the results effectively rule out previously
proposed approaches to alleviate concentration by "optimal" setting the values
of $p$ in $(0,1)$. At the same time, we specify conditions and the
corresponding families of distributions for which one can still control
concentration rates by appropriate choices of $p$. We also show that in an
arbitrarily small vicinity of a distribution from a large class of
distributions for which uniform concentration occurs, there are uncountably
many other distributions featuring anti-concentration properties. Importantly,
this behavior enables devising relevant data encoding or representation schemes
favouring or discouraging distance concentration. The results shed new light on
this long-standing problem and resolve the tension around the topic in both
theory and empirical evidence reported in the literature.

</details>


### [590] [MoESD: Unveil Speculative Decoding's Potential for Accelerating Sparse MoE](https://arxiv.org/abs/2505.19645)
*Zongle Huang,Lei Zhu,Zongyuan Zhan,Ting Hu,Weikai Mao,Xianzhi Yu,Yongpan Liu,Tianyu Zhang*

Main category: cs.LG

TL;DR: 研究发现混合专家模型（MoE）在中等批量大小下，通过推测解码（SD）获得的加速效果比密集模型更显著，且随着MoE稀疏性增加，SD的有效批量范围扩大。论文提出新指标‘目标效率’以全面评估SD加速效果，实验验证Qwen2-57B-A14B最高加速2.29倍。


<details>
  <summary>Details</summary>
Motivation: 当前推测解码技术主要针对密集模型优化，而混合专家模型（MoE）因其稀疏性在推理加速领域潜力未被充分挖掘。论文旨在探索MoE与SD的结合潜力，并解决现有SD评估指标（如接受率）的局限性。

Method: 1) 通过理论分析建立SD性能模型；2) 提出新指标‘目标效率’以量化模型架构和工作负载对SD加速的影响；3) 在不同GPU上测试MoE模型（如Qwen2-57B-A14B）的SD加速效果。

Result: 实验表明：1) MoE在中等批量下SD加速效果优于密集模型；2) MoE稀疏性越高，SD有效批量范围越广；3) 新指标‘目标效率’能更全面识别系统瓶颈；4) Qwen2-57B-A14B最高实现2.29倍加速。

Conclusion: 论文揭示了MoE与SD结合的独特优势，为私有化部署等场景提供新的加速思路。提出的‘目标效率’指标弥补了传统评估方法的不足，为未来SD研究提供更全面的分析框架。

Abstract: Large Language Models (LLMs) have achieved remarkable success across many
applications, with Mixture of Experts (MoE) models demonstrating great
potential. Compared to traditional dense models, MoEs achieve better
performance with less computation. Speculative decoding (SD) is a widely used
technique to accelerate LLM inference without accuracy loss, but it has been
considered efficient only for dense models. In this work, we first demonstrate
that, under medium batch sizes, MoE surprisingly benefits more from SD than
dense models. Furthermore, as MoE becomes sparser -- the prevailing trend in
MoE designs -- the batch size range where SD acceleration is expected to be
effective becomes broader. To quantitatively understand tradeoffs involved in
SD, we develop a reliable modeling based on theoretical analyses. While current
SD research primarily focuses on improving acceptance rates of algorithms,
changes in workload and model architecture can still lead to degraded SD
acceleration even with high acceptance rates. To address this limitation, we
introduce a new metric 'target efficiency' that characterizes these effects,
thus helping researchers identify system bottlenecks and understand SD
acceleration more comprehensively. For scenarios like private serving, this
work unveils a new perspective to speed up MoE inference, where existing
solutions struggle. Experiments on different GPUs show up to 2.29x speedup for
Qwen2-57B-A14B at medium batch sizes and validate our theoretical predictions.

</details>


### [591] [Energy-based generator matching: A neural sampler for general state space](https://arxiv.org/abs/2505.19646)
*Dongyeop Woo,Minsu Kim,Minkyu Kim,Kiyoung Seong,Sungsoo Ahn*

Main category: cs.LG

TL;DR: 提出EGM方法，无需数据即可训练生成模型，支持多种模态和马尔可夫过程。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型训练通常依赖大量数据，EGM旨在不依赖数据的情况下，通过能量函数训练生成模型，扩展了生成匹配的应用范围。

Method: EGM利用自归一化重要性采样和自举技巧来估计生成匹配损失，支持连续、离散及混合模态的马尔可夫过程训练。

Result: 在高达100维的离散任务和20维的多模态任务上验证了EGM的有效性。

Conclusion: EGM为无数据条件下的生成模型训练提供了灵活且高效的新方法，适用于多种数据类型和维度。

Abstract: We propose Energy-based generator matching (EGM), a modality-agnostic
approach to train generative models from energy functions in the absence of
data. Extending the recently proposed generator matching, EGM enables training
of arbitrary continuous-time Markov processes, e.g., diffusion, flow, and jump,
and can generate data from continuous, discrete, and a mixture of two
modalities. To this end, we propose estimating the generator matching loss
using self-normalized importance sampling with an additional bootstrapping
trick to reduce variance in the importance weight. We validate EGM on both
discrete and multimodal tasks up to 100 and 20 dimensions, respectively.

</details>


### [592] [Zero-Shot Streaming Text to Speech Synthesis with Transducer and Auto-Regressive Modeling](https://arxiv.org/abs/2505.19669)
*Haiyang Sun,Shujie Hu,Shujie Liu,Lingwei Meng,Hui Wang,Bing Han,Yifan Yang,Yanqing Liu,Sheng Zhao,Yan Lu,Yanmin Qian*

Main category: cs.LG

TL;DR: 提出SMLLE框架，通过实时转换文本和最小延迟的未来文本访问，实现高质量流式语音合成。


<details>
  <summary>Details</summary>
Motivation: 现有流式文本转语音方法依赖前瞻机制导致高延迟，需改进以实现低延迟高质量合成。

Method: SMLLE结合Transducer实时生成语义标记和时长对齐信息，并采用全自回归流式模型重建梅尔谱，辅以Delete < Bos >机制稳定生成。

Result: SMLLE超越现有流式方法，性能媲美句子级TTS系统。

Conclusion: SMLLE有效降低延迟并提升流式语音合成质量，为实时人机交互提供解决方案。

Abstract: Zero-shot streaming text-to-speech is an important research topic in
human-computer interaction. Existing methods primarily use a lookahead
mechanism, relying on future text to achieve natural streaming speech
synthesis, which introduces high processing latency. To address this issue, we
propose SMLLE, a streaming framework for generating high-quality speech
frame-by-frame. SMLLE employs a Transducer to convert text into semantic tokens
in real time while simultaneously obtaining duration alignment information. The
combined outputs are then fed into a fully autoregressive (AR) streaming model
to reconstruct mel-spectrograms. To further stabilize the generation process,
we design a Delete < Bos > Mechanism that allows the AR model to access future
text introducing as minimal delay as possible. Experimental results suggest
that the SMLLE outperforms current streaming TTS methods and achieves
comparable performance over sentence-level TTS systems. Samples are available
on https://anonymous.4open.science/w/demo_page-48B7/.

</details>


### [593] [Cut out and Replay: A Simple yet Versatile Strategy for Multi-Label Online Continual Learning](https://arxiv.org/abs/2505.19680)
*Xinrui Wang,Shao-yuan Li,Jiaqiang Zhang,Songcan Chen*

Main category: cs.LG

TL;DR: 该论文提出了一种名为CUTER的新策略，用于解决多标签在线持续学习中的灾难性遗忘、缺失标签和类别不平衡问题，通过识别和强化标签特定区域来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 多标签在线持续学习（MOCL）面临灾难性遗忘、缺失标签和类别不平衡等挑战。现有方法忽视了标签特定区域的识别和特征学习，这是多标签学习的基础解决方案，但在在线增量学习和部分监督环境下难以实现。

Method: 论文首先利用输入数据的固有结构信息评估不同预训练模型的定位能力，然后提出CUTER策略，通过识别、强化和剪裁标签特定区域来提供细粒度监督信号，实现高效经验回放。

Result: 在多个多标签图像基准测试上的广泛实验表明，该方法在解决灾难性遗忘、缺失标签和类别不平衡问题上表现优越。

Conclusion: CUTER策略不仅有效解决了MOCL中的关键挑战，还能与现有方法无缝集成，是一种简单而通用的解决方案。

Abstract: Multi-Label Online Continual Learning (MOCL) requires models to learn
continuously from endless multi-label data streams, facing complex challenges
including persistent catastrophic forgetting, potential missing labels, and
uncontrollable imbalanced class distributions. While existing MOCL methods
attempt to address these challenges through various techniques, \textit{they
all overlook label-specific region identifying and feature learning} - a
fundamental solution rooted in multi-label learning but challenging to achieve
in the online setting with incremental and partial supervision. To this end, we
first leverage the inherent structural information of input data to evaluate
and verify the innate localization capability of different pre-trained models.
Then, we propose CUTER (CUT-out-and-Experience-Replay), a simple yet versatile
strategy that provides fine-grained supervision signals by further identifying,
strengthening and cutting out label-specific regions for efficient experience
replay. It not only enables models to simultaneously address catastrophic
forgetting, missing labels, and class imbalance challenges, but also serves as
an orthogonal solution that seamlessly integrates with existing approaches.
Extensive experiments on multiple multi-label image benchmarks demonstrate the
superiority of our proposed method. The code is available at
\href{https://github.com/wxr99/Cut-Replay}{https://github.com/wxr99/Cut-Replay}

</details>


### [594] [Deep Actor-Critics with Tight Risk Certificates](https://arxiv.org/abs/2505.19682)
*Bahareh Tasdighi,Manuel Haussmann,Yi-Shan Wu,Andres R. Masegosa,Melih Kandemir*

Main category: cs.LG

TL;DR: 该论文提出了一种基于PAC-Bayes理论的深度演员-评论家算法风险验证方法，通过少量评估数据即可生成紧致的风险证书，为算法在物理系统中的部署提供了可行性验证。


<details>
  <summary>Details</summary>
Motivation: 尽管深度演员-评论家算法已在大型语言模型优化中广泛应用，但其在物理系统中的部署仍缺乏有效的风险量化验证方案。论文旨在解决这一问题。

Method: 采用递归PAC-Bayes方法，将验证数据分成多个部分，递归构建每个部分预测器的超额损失边界，并利用前一部分的预测器作为数据先验。

Result: 实验结果表明，该方法在多种运动任务和策略专业水平下都能生成足够紧致的风险证书，具备实际应用价值。

Conclusion: 通过少量评估数据结合PAC-Bayes理论，能够为深度演员-评论家算法生成实用的风险证书，为其在物理系统中的部署提供了可靠验证。

Abstract: After a period of research, deep actor-critic algorithms have reached a level
where they influence our everyday lives. They serve as the driving force behind
the continual improvement of large language models through user-collected
feedback. However, their deployment in physical systems is not yet widely
adopted, mainly because no validation scheme that quantifies their risk of
malfunction. We demonstrate that it is possible to develop tight risk
certificates for deep actor-critic algorithms that predict generalization
performance from validation-time observations. Our key insight centers on the
effectiveness of minimal evaluation data. Surprisingly, a small feasible of
evaluation roll-outs collected from a pretrained policy suffices to produce
accurate risk certificates when combined with a simple adaptation of PAC-Bayes
theory. Specifically, we adopt a recently introduced recursive PAC-Bayes
approach, which splits validation data into portions and recursively builds
PAC-Bayes bounds on the excess loss of each portion's predictor, using the
predictor from the previous portion as a data-informed prior. Our empirical
results across multiple locomotion tasks and policy expertise levels
demonstrate risk certificates that are tight enough to be considered for
practical use.

</details>


### [595] [Graph Guided Diffusion: Unified Guidance for Conditional Graph Generation](https://arxiv.org/abs/2505.19685)
*Victor M. Tenorio,Nicolas Zilberstein,Santiago Segarra,Antonio G. Marques*

Main category: cs.LG

TL;DR: GGDiff提出了一种新的图条件生成框架，通过随机控制问题统一多种引导策略，解决了扩散模型在图生成中的挑战。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图生成中表现出色，但在条件图生成方面面临挑战，尤其是处理离散图结构和不可微奖励信号时。

Method: GGDiff将条件扩散视为随机控制问题，结合梯度引导、控制引导和零阶近似，实现了预训练模型的零样本引导。

Result: GGDiff在多种任务中表现优异，如图主题约束、公平性和链接预测，实现了目标奖励的高对齐性和样本多样性。

Conclusion: GGDiff为图条件生成提供了一种高效、灵活的框架，适用于多种奖励类型，扩展了扩散模型的应用范围。

Abstract: Diffusion models have emerged as powerful generative models for graph
generation, yet their use for conditional graph generation remains a
fundamental challenge. In particular, guiding diffusion models on graphs under
arbitrary reward signals is difficult: gradient-based methods, while powerful,
are often unsuitable due to the discrete and combinatorial nature of graphs,
and non-differentiable rewards further complicate gradient-based guidance. We
propose Graph Guided Diffusion (GGDiff), a novel guidance framework that
interprets conditional diffusion on graphs as a stochastic control problem to
address this challenge. GGDiff unifies multiple guidance strategies, including
gradient-based guidance (for differentiable rewards), control-based guidance
(using control signals from forward reward evaluations), and zero-order
approximations (bridging gradient-based and gradient-free optimization). This
comprehensive, plug-and-play framework enables zero-shot guidance of
pre-trained diffusion models under both differentiable and non-differentiable
reward functions, adapting well-established guidance techniques to graph
generation--a direction largely unexplored. Our formulation balances
computational efficiency, reward alignment, and sample quality, enabling
practical conditional generation across diverse reward types. We demonstrate
the efficacy of GGDiff in various tasks, including constraints on graph motifs,
fairness, and link prediction, achieving superior alignment with target rewards
while maintaining diversity and fidelity.

</details>


### [596] [JEDI: Latent End-to-end Diffusion Mitigates Agent-Human Performance Asymmetry in Model-Based Reinforcement Learning](https://arxiv.org/abs/2505.19698)
*Jing Yu Lim,Zarif Ikram,Samson Yu,Haozhe Ma,Tze-Yun Leong,Dianbo Liu*

Main category: cs.LG

TL;DR: 该论文指出基于扩散世界模型的MBRL智能体在Atari100k基准测试中存在性能不对称问题，并提出JEDI方法以改善人类优势任务的表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散世界模型的MBRL智能体在Atari100k基准测试中整体表现超人类水平，但存在严重的性能不对称问题：某些任务远超人类，而其他任务表现极差。这种不对称性在基于像素的智能体中尤为明显，论文旨在解决这一问题。

Method: 论文提出Joint Embedding DIffusion (JEDI)，一种新型的潜在扩散世界模型，通过自一致性目标进行端到端训练，以解决基于像素方法中缺乏时间结构潜在空间的问题。

Result: JEDI在人类优势任务中优于现有最先进模型，同时在Atari100k基准测试中保持竞争力，且运行速度提高3倍，内存降低43%。

Conclusion: 论文重新思考了Atari100k中超越人类性能的真正含义，并提出了改进性能不对称问题的方法，JEDI在性能和效率上均有显著提升。

Abstract: Recent advances in model-based reinforcement learning (MBRL) have achieved
super-human level performance on the Atari100k benchmark, driven by
reinforcement learning agents trained on powerful diffusion world models.
However, we identify that the current aggregates mask a major performance
asymmetry: MBRL agents dramatically outperform humans in some tasks despite
drastically underperforming in others, with the former inflating the aggregate
metrics. This is especially pronounced in pixel-based agents trained with
diffusion world models. In this work, we address the pronounced asymmetry
observed in pixel-based agents as an initial attempt to reverse the worrying
upward trend observed in them. We address the problematic aggregates by
delineating all tasks as Agent-Optimal or Human-Optimal and advocate for equal
importance on metrics from both sets. Next, we hypothesize this pronounced
asymmetry is due to the lack of temporally-structured latent space trained with
the World Model objective in pixel-based methods. Lastly, to address this
issue, we propose Joint Embedding DIffusion (JEDI), a novel latent diffusion
world model trained end-to-end with the self-consistency objective. JEDI
outperforms SOTA models in human-optimal tasks while staying competitive across
the Atari100k benchmark, and runs 3 times faster with 43% lower memory than the
latest pixel-based diffusion baseline. Overall, our work rethinks what it truly
means to cross human-level performance in Atari100k.

</details>


### [597] [Mosaic: Data-Free Knowledge Distillation via Mixture-of-Experts for Heterogeneous Distributed Environments](https://arxiv.org/abs/2505.19699)
*Junming Liu,Yanting Gao,Siyuan Meng,Yifei Sun,Aoqi Wu,Yufei Jin,Yirong Chen,Ding Wang,Guosun Zeng*

Main category: cs.LG

TL;DR: 论文提出Mosaic框架，通过数据无关知识蒸馏解决联邦学习中模型与数据异质性导致的性能问题，利用本地生成模型和专家混合架构提升全局模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在保护数据隐私的同时面临模型与数据异质性带来的表示不一致和优化动态分歧问题，影响全局模型鲁棒性。

Method: Mosaic框架分两步：1) 训练本地生成模型生成合成数据保护隐私；2) 构建专家混合模型并通过知识蒸馏整合到全局模型中，辅以轻量级元模型优化。

Result: 在标准图像分类基准测试中，Mosaic在模型和数据异质性条件下均优于现有先进方法。

Conclusion: Mosaic通过创新的数据无关知识蒸馏和专家混合架构，有效解决了联邦学习中的异质性问题，显著提升了全局模型性能。

Abstract: Federated Learning (FL) is a decentralized machine learning paradigm that
enables clients to collaboratively train models while preserving data privacy.
However, the coexistence of model and data heterogeneity gives rise to
inconsistent representations and divergent optimization dynamics across
clients, ultimately hindering robust global performance. To transcend these
challenges, we propose Mosaic, a novel data-free knowledge distillation
framework tailored for heterogeneous distributed environments. Mosaic first
trains local generative models to approximate each client's personalized
distribution, enabling synthetic data generation that safeguards privacy
through strict separation from real data. Subsequently, Mosaic forms a
Mixture-of-Experts (MoE) from client models based on their specialized
knowledge, and distills it into a global model using the generated data. To
further enhance the MoE architecture, Mosaic integrates expert predictions via
a lightweight meta model trained on a few representative prototypes. Extensive
experiments on standard image classification benchmarks demonstrate that Mosaic
consistently outperforms state-of-the-art approaches under both model and data
heterogeneity. The source code has been published at
https://github.com/Wings-Of-Disaster/Mosaic.

</details>


### [598] [On the Relation between Rectified Flows and Optimal Transport](https://arxiv.org/abs/2505.19712)
*Johannes Hertrich,Antonin Chambolle,Julie Delon*

Main category: cs.LG

TL;DR: 本文研究了整流流、流匹配和最优传输之间的联系，揭示了梯度约束下整流流与最优传输的关系需更强假设，并提出了反例。


<details>
  <summary>Details</summary>
Motivation: 探讨整流流、流匹配与最优传输之间的关系，特别是梯度约束下整流流能否产生最优传输解的问题。

Method: 分析整流流的不变性特性，研究高斯和高斯混合场景下的显式构造，并验证梯度约束下整流流与最优传输的关系。

Result: 发现梯度约束下整流流仅在某些强假设下与最优传输相关，并提出了反例证明先前等价性结论不成立。

Conclusion: 在整流流中强制梯度约束通常不是计算最优传输映射的可靠方法。

Abstract: This paper investigates the connections between rectified flows, flow
matching, and optimal transport. Flow matching is a recent approach to learning
generative models by estimating velocity fields that guide transformations from
a source to a target distribution. Rectified flow matching aims to straighten
the learned transport paths, yielding more direct flows between distributions.
Our first contribution is a set of invariance properties of rectified flows and
explicit velocity fields. In addition, we also provide explicit constructions
and analysis in the Gaussian (not necessarily independent) and Gaussian mixture
settings and study the relation to optimal transport. Our second contribution
addresses recent claims suggesting that rectified flows, when constrained such
that the learned velocity field is a gradient, can yield (asymptotically)
solutions to optimal transport problems. We study the existence of solutions
for this problem and demonstrate that they only relate to optimal transport
under assumptions that are significantly stronger than those previously
acknowledged. In particular, we present several counter-examples that
invalidate earlier equivalence results in the literature, and we argue that
enforcing a gradient constraint on rectified flows is, in general, not a
reliable method for computing optimal transport maps.

</details>


### [599] [OCN: Effectively Utilizing Higher-Order Common Neighbors for Better Link Prediction](https://arxiv.org/abs/2505.19719)
*Juntong Wang,Xiyuan Wang,Muhan Zhang*

Main category: cs.LG

TL;DR: 提出正交化与归一化技术解决共同邻居的冗余与过平滑问题，显著提升链接预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理不同阶共同邻居时存在冗余和过平滑问题，未能充分利用其潜力。

Method: 设计正交化消除冗余，归一化缓解过平滑，结合两者提出正交共同邻居（OCN）方法。

Result: OCN在主流链接预测基准上平均超越基线7.7%，理论分析与消融实验验证有效性。

Conclusion: 正交化与归一化技术显著提升共同邻居特征在链接预测中的表现。

Abstract: Common Neighbors (CNs) and their higher-order variants are important pairwise
features widely used in state-of-the-art link prediction methods. However,
existing methods often struggle with the repetition across different orders of
CNs and fail to fully leverage their potential. We identify that these
limitations stem from two key issues: redundancy and over-smoothing in
high-order common neighbors. To address these challenges, we design
orthogonalization to eliminate redundancy between different-order CNs and
normalization to mitigate over-smoothing. By combining these two techniques, we
propose Orthogonal Common Neighbor (OCN), a novel approach that significantly
outperforms the strongest baselines by an average of 7.7% on popular link
prediction benchmarks. A thorough theoretical analysis is provided to support
our method. Ablation studies also verify the effectiveness of our
orthogonalization and normalization techniques.

</details>


### [600] [Machine Learning Algorithm for Noise Reduction and Disease-Causing Gene Feature Extraction in Gene Sequencing Data](https://arxiv.org/abs/2505.19740)
*Weichen Si,Yihao Ou,Zhen Tian*

Main category: cs.LG

TL;DR: 提出基于机器学习的基因测序降噪与致病基因特征提取方法DeepSeqDenoise，结合CNN和RNN提升信噪比9.4dB，筛选17个关键特征构建集成模型，预测准确率达94.3%，在心血管队列中发现57个新候选致病基因。


<details>
  <summary>Details</summary>
Motivation: 解决基因测序中噪声干扰和致病基因识别准确率不足的问题，为遗传病精准诊断提供技术支持。

Method: 采用CNN+RNN混合的DeepSeqDenoise算法降噪，通过特征工程筛选17个关键特征，构建集成学习预测模型。

Result: 信噪比提升9.4dB，预测准确率94.3%；临床验证发现57个新候选基因及3个漏检变异，性能显著优于现有工具。

Conclusion: 该方法显著提升致病基因检测效能，对遗传病诊断具有重要应用价值。

Abstract: In this study, we propose a machine learning-based method for noise reduction
and disease-causing gene feature extraction in gene sequencing DeepSeqDenoise
algorithm combines CNN and RNN to effectively remove the sequencing noise, and
improves the signal-to-noise ratio by 9.4 dB. We screened 17 key features by
feature engineering, and constructed an integrated learning model to predict
disease-causing genes with 94.3% accuracy. We successfully identified 57 new
candidate disease-causing genes in a cardiovascular disease cohort validation,
and detected 3 missed variants in clinical applications. The method
significantly outperforms existing tools and provides strong support for
accurate diagnosis of genetic diseases.

</details>


### [601] [Discrete Markov Bridge](https://arxiv.org/abs/2505.19752)
*Hengli Li,Yuxuan Wang,Song-Chun Zhu,Ying Nian Wu,Zilong Zheng*

Main category: cs.LG

TL;DR: 本文提出了一种名为Discrete Markov Bridge的新框架，用于解决离散扩散模型中固定速率转移矩阵的限制问题，通过矩阵学习和分数学习两个关键组件，提升了模型的表达能力和设计空间。


<details>
  <summary>Details</summary>
Motivation: 现有的离散扩散模型通常依赖于固定的速率转移矩阵，这不仅限制了潜在表示的表达能力，也约束了整体设计空间。为了克服这些限制，作者提出了新的框架。

Method: Discrete Markov Bridge框架基于矩阵学习和分数学习两个关键组件，通过理论分析证明了矩阵学习的性能保证和框架的收敛性，并分析了空间复杂度。

Result: 在Text8数据集上，该模型实现了1.38的ELBO，优于现有基线；在CIFAR-10数据集上也表现出与图像生成方法相当的性能。

Conclusion: Discrete Markov Bridge框架在离散表示学习中表现出色，不仅提升了性能，还扩展了设计空间，为离散数据建模提供了新的方向。

Abstract: Discrete diffusion has recently emerged as a promising paradigm in discrete
data modeling. However, existing methods typically rely on a fixed rate
transition matrix during training, which not only limits the expressiveness of
latent representations, a fundamental strength of variational methods, but also
constrains the overall design space. To address these limitations, we propose
Discrete Markov Bridge, a novel framework specifically designed for discrete
representation learning. Our approach is built upon two key components: Matrix
Learning and Score Learning. We conduct a rigorous theoretical analysis,
establishing formal performance guarantees for Matrix Learning and proving the
convergence of the overall framework. Furthermore, we analyze the space
complexity of our method, addressing practical constraints identified in prior
studies. Extensive empirical evaluations validate the effectiveness of the
proposed Discrete Markov Bridge, which achieves an Evidence Lower Bound (ELBO)
of 1.38 on the Text8 dataset, outperforming established baselines. Moreover,
the proposed model demonstrates competitive performance on the CIFAR-10
dataset, achieving results comparable to those obtained by image-specific
generation approaches.

</details>


### [602] [Unfolding AlphaFold's Bayesian Roots in Probability Kinematics](https://arxiv.org/abs/2505.19763)
*Thomas Hamelryck,Kanti V. Mardia*

Main category: cs.LG

TL;DR: 本文提出AlphaFold1的新理论解释，将其学习势能函数重新解读为概率运动学（Jeffrey条件化）的实例，而非传统热力学势。通过合成2D模型验证，揭示了其作为广义贝叶斯更新的本质。


<details>
  <summary>Details</summary>
Motivation: AlphaFold1在蛋白质结构预测中的突破性成果依赖于学习势能函数，但该势能的物理解释存在局限。作者旨在从概率运动学角度重新诠释，以更精确地量化其理论框架。

Method: 采用概率运动学（Jeffrey条件化）理论框架，分析AlphaFold1的势能函数；通过合成2D模型（角度随机游走先验+距离证据更新）进行验证。

Result: 证实AlphaFold1的势能本质是广义贝叶斯更新，而非热力学势。概率运动学框架能更精确地量化其机制，超越传统PMF启发式解释。

Conclusion: 概率运动学为深度学习中的概率建模提供了新范式，可通过简单组件构建复杂模型。AlphaFold1的成功展示了该方法在生物分子建模中的潜力。

Abstract: We present a novel theoretical interpretation of AlphaFold1. The seminal
breakthrough of AlphaFold1 in protein structure prediction by deep learning
relied on a learned potential energy function, in contrast to the later
end-to-end architectures of AlphaFold2 and AlphaFold3. While this potential was
originally justified by referring to physical potentials of mean force (PMFs),
we reinterpret AlphaFold1's potential as an instance of probability kinematics
- also known as Jeffrey conditioning - a principled but underrecognised
generalization of conventional Bayesian updating. Probability kinematics
accommodates uncertain or soft evidence in the form of updated probabilities
over a partition. This perspective reveals AlphaFold1's potential as a form of
generalized Bayesian updating, rather than a thermodynamic potential. To
confirm our probabilistic framework's scope and precision, we analyze a
synthetic 2D model in which an angular random walk prior is updated with
evidence on distances via probability kinematics, mirroring AlphaFold1's
approach. This theoretical contribution connects AlphaFold1 to a broader class
of well-justified Bayesian methods, allowing precise quantification, surpassing
merely qualitative heuristics based on PMFs. More broadly, given the
achievements of AlphaFold1, probability kinematics holds considerable promise
for probabilistic deep learning, as it allows for the formulation of complex
models from a few simpler components.

</details>


### [603] [Agentic Predictor: Performance Prediction for Agentic Workflows via Multi-View Encoding](https://arxiv.org/abs/2505.19764)
*Patara Trirat,Wonyong Jeong,Sung Ju Hwang*

Main category: cs.LG

TL;DR: 提出Agentic Predictor，一种轻量级预测器，通过多视图工作流编码和跨域无监督预训练，高效优化基于LLM的代理系统配置。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的代理系统优化方法依赖启发式调优或穷举评估，计算成本高且效果欠佳，需更高效的解决方案。

Method: 采用多视图工作流编码技术（代码架构、文本提示、交互图特征）和跨域无监督预训练，减少训练所需的评估次数。

Result: 在三个领域的基准测试中，预测器在准确性和工作流效用上均优于现有方法，显著降低试错成本。

Conclusion: Agentic Predictor通过性能预测器简化了基于LLM的代理工作流设计，展现了高效配置优化的潜力。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
diverse tasks, but optimizing LLM-based agentic systems remains challenging due
to the vast search space of agent configurations, prompting strategies, and
communication patterns. Existing approaches often rely on heuristic-based
tuning or exhaustive evaluation, which can be computationally expensive and
suboptimal. This paper proposes Agentic Predictor, a lightweight predictor for
efficient agentic workflow evaluation. Agentic Predictor is equipped with a
multi-view workflow encoding technique that leverages multi-view representation
learning of agentic systems by incorporating code architecture, textual
prompts, and interaction graph features. To achieve high predictive accuracy
while significantly reducing the number of required workflow evaluations for
training a predictor, Agentic Predictor employs cross-domain unsupervised
pretraining. By learning to approximate task success rates, Agentic Predictor
enables fast and accurate selection of optimal agentic workflow configurations
for a given task, significantly reducing the need for expensive trial-and-error
evaluations. Experiments on a carefully curated benchmark spanning three
domains show that our predictor outperforms state-of-the-art methods in both
predictive accuracy and workflow utility, highlighting the potential of
performance predictors in streamlining the design of LLM-based agentic
workflows.

</details>


### [604] [Understanding the Performance Gap in Preference Learning: A Dichotomy of RLHF and DPO](https://arxiv.org/abs/2505.19770)
*Ruizhe Shi,Minhak Song,Runlong Zhou,Zihan Zhang,Maryam Fazel,Simon S. Du*

Main category: cs.LG

TL;DR: 该论文通过细粒度理论分析，揭示了人类反馈强化学习（RLHF）与直接偏好优化（DPO）在表示差距下的性能差异，并分解了两种来源的差距。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解RLHF和DPO在不同模型误设和样本限制下的性能差异，为实际应用中选择合适方法提供理论依据。

Method: 论文采用理论分析方法，分别在精确优化和有限样本近似优化两种设定下，分解并比较了RLHF和DPO的性能差距。

Result: 结果显示，在精确优化下，RLHF、DPO或在线DPO的性能优劣取决于模型误设类型；在近似优化下，RLHF在样本效率上具有统计优势。

Conclusion: 结论指出，该研究全面理解了RLHF与DPO在不同设定下的性能差距，并提供了选择方法的实用指导。

Abstract: We present a fine-grained theoretical analysis of the performance gap between
reinforcement learning from human feedback (RLHF) and direct preference
optimization (DPO) under a representation gap. Our study decomposes this gap
into two sources: an explicit representation gap under exact optimization and
an implicit representation gap under finite samples. In the exact optimization
setting, we characterize how the relative capacities of the reward and policy
model classes influence the final policy qualities. We show that RLHF, DPO, or
online DPO can outperform one another depending on the type of model
mis-specifications. Notably, online DPO can outperform both RLHF and standard
DPO when the reward and policy model classes are isomorphic and both
mis-specified. In the approximate optimization setting, we provide a concrete
construction where the ground-truth reward is implicitly sparse and show that
RLHF requires significantly fewer samples than DPO to recover an effective
reward model -- highlighting a statistical advantage of two-stage learning.
Together, these results provide a comprehensive understanding of the
performance gap between RLHF and DPO under various settings, and offer
practical insights into when each method is preferred.

</details>


### [605] [MedDreamer: Model-Based Reinforcement Learning with Latent Imagination on Complex EHRs for Clinical Decision Support](https://arxiv.org/abs/2505.19785)
*Qianyi Xu,Gousia Habib,Dilruk Perera,Mengling Feng*

Main category: cs.LG

TL;DR: 论文提出MedDreamer，一种基于模型的两阶段强化学习框架，用于个性化治疗推荐，通过潜在想象模拟患者轨迹，提升决策质量。


<details>
  <summary>Details</summary>
Motivation: 现有临床决策支持系统依赖离散化和插值，可能扭曲时间动态并降低决策质量，且忽视数据收集模式的临床意义。模型无关的强化学习方法样本效率低、对数据质量敏感且泛化能力差。

Method: MedDreamer采用两阶段模型强化学习框架，包含自适应特征整合模块的世界模型，通过潜在想象模拟患者轨迹，结合真实与想象数据优化策略。

Result: 在脓毒症和机械通气治疗评估中，MedDreamer在临床结果和离策略指标上优于模型无关和基于模型的基线方法。

Conclusion: MedDreamer首次将潜在想象应用于不规则医疗数据，通过模拟患者轨迹提升决策质量，优于现有方法。

Abstract: Timely and personalized treatment decisions are essential across a wide range
of healthcare settings where patient responses vary significantly and evolve
over time. Clinical data used to support these decisions are often irregularly
sampled, sparse, and noisy. Existing decision support systems commonly rely on
discretization and imputation, which can distort critical temporal dynamics and
degrade decision quality. Moreover, they often overlook the clinical
significance of irregular recording frequencies, filtering out patterns in how
and when data is collected. Reinforcement Learning (RL) is a natural fit for
clinical decision-making, enabling sequential, long-term optimization in
dynamic, uncertain environments. However, most existing treatment
recommendation systems are model-free and trained solely on offline data,
making them sample-inefficient, sensitive to data quality, and poorly
generalizable across tasks or cohorts. To address these limitations, we propose
MedDreamer, a two-phase model-based RL framework for personalized treatment
recommendation. MedDreamer uses a world model with an Adaptive Feature
Integration (AFI) module to effectively model irregular, sparse clinical data.
Through latent imagination, it simulates plausible patient trajectories to
enhance learning, refining its policy using a mix of real and imagined
experiences. This enables learning policies that go beyond suboptimal
historical decisions while remaining grounded in clinical data. To our
knowledge, this is the first application of latent imagination to irregular
healthcare data. Evaluations on sepsis and mechanical ventilation (MV)
treatment using two large-scale EHR datasets show that MedDreamer outperforms
both model-free and model-based baselines in clinical outcomes and off-policy
metrics.

</details>


### [606] [What Can RL Bring to VLA Generalization? An Empirical Study](https://arxiv.org/abs/2505.19789)
*Jijia Liu,Feng Gao,Bingwen Wei,Xinlei Chen,Qingmin Liao,Yi Wu,Chao Yu,Yu Wang*

Main category: cs.LG

TL;DR: 该论文研究了强化学习（RL）在提升大型视觉语言动作（VLA）模型泛化能力方面的作用，发现PPO算法优于SFT和其他RL方法。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言动作（VLA）模型主要通过监督微调（SFT）训练，但在分布变化时容易产生复合错误，泛化能力有限。强化学习（RL）通过试错优化任务目标，可能克服这些限制，但其对VLA的具体泛化益处尚未系统研究。

Method: 研究引入了一个全面的基准来评估VLA的泛化能力，并系统研究了RL微调在视觉、语义和执行维度上的影响。特别比较了PPO与其他RL算法（如DPO和GRPO）的效果，并开发了一种高效的PPO训练方法。

Result: 实验表明，RL微调（尤其是PPO）显著提升了语义理解和执行鲁棒性，同时保持了与SFT相当的视觉鲁棒性。PPO被证明是比DPO和GRPO更有效的RL算法。

Conclusion: PPO微调能有效提升VLA模型的泛化能力，特别是在语义和执行层面。研究还提供了一种高效的PPO训练方法，具有实际应用价值。

Abstract: Large Vision-Language Action (VLA) models have shown significant potential
for embodied AI. However, their predominant training via supervised fine-tuning
(SFT) limits generalization due to susceptibility to compounding errors under
distribution shifts. Reinforcement learning (RL) offers a path to overcome
these limitations by optimizing for task objectives via trial-and-error, yet a
systematic understanding of its specific generalization benefits for VLAs
compared to SFT is lacking. To address this, our study introduces a
comprehensive benchmark for evaluating VLA generalization and systematically
investigates the impact of RL fine-tuning across diverse visual, semantic, and
execution dimensions. Our extensive experiments reveal that RL fine-tuning,
particularly with PPO, significantly enhances generalization in semantic
understanding and execution robustness over SFT, while maintaining comparable
visual robustness. We identify PPO as a more effective RL algorithm for VLAs
than LLM-derived methods like DPO and GRPO. We also develop a simple recipe for
efficient PPO training on VLAs, and demonstrate its practical utility for
improving VLA generalization. The project page is at https://rlvla.github.io

</details>


### [607] [GraphAU-Pain: Graph-based Action Unit Representation for Pain Intensity Estimation](https://arxiv.org/abs/2505.19802)
*Zhiyu Wang,Yang Liu,Hatice Gunes*

Main category: cs.LG

TL;DR: 论文提出GraphAU-Pain，一种基于图神经网络的框架，用于通过面部动作单元（AUs）及其相互关系来估计疼痛强度，提高了可解释性和性能。


<details>
  <summary>Details</summary>
Motivation: 理解与疼痛相关的面部行为对于数字医疗至关重要，尤其是对无法言语的患者。现有方法在可解释性和疼痛强度量化方面存在局限。

Method: 提出GraphAU-Pain框架，将面部动作单元（AUs）表示为图节点，共现关系为边，利用关系图神经网络建模AU间关系，以更准确地描述疼痛相关面部行为。

Result: 在公开数据集UNBC上的实验表明，GraphAU-Pain在疼痛强度估计中达到66.21%的F1分数和87.61%的准确率。

Conclusion: GraphAU-Pain通过图结构建模AU关系，显著提升了疼痛识别的可解释性和性能，为数字医疗提供了有效工具。

Abstract: Understanding pain-related facial behaviors is essential for digital
healthcare in terms of effective monitoring, assisted diagnostics, and
treatment planning, particularly for patients unable to communicate verbally.
Existing data-driven methods of detecting pain from facial expressions are
limited due to interpretability and severity quantification. To this end, we
propose GraphAU-Pain, leveraging a graph-based framework to model facial Action
Units (AUs) and their interrelationships for pain intensity estimation. AUs are
represented as graph nodes, with co-occurrence relationships as edges, enabling
a more expressive depiction of pain-related facial behaviors. By utilizing a
relational graph neural network, our framework offers improved interpretability
and significant performance gains. Experiments conducted on the publicly
available UNBC dataset demonstrate the effectiveness of the GraphAU-Pain,
achieving an F1-score of 66.21% and accuracy of 87.61% in pain intensity
estimation.

</details>


### [608] [Density Ratio-Free Doubly Robust Proxy Causal Learning](https://arxiv.org/abs/2505.19807)
*Bariscan Bozkurt,Houssam Zenati,Dimitri Meunier,Liyuan Xu,Arthur Gretton*

Main category: cs.LG

TL;DR: 本文提出了两种基于核函数的双重稳健估计器，用于代理因果学习中的因果函数估计，无需观测混杂因子，适用于连续和高维变量。


<details>
  <summary>Details</summary>
Motivation: 在代理因果学习框架中，混杂因子不可观测但存在代理变量。现有方法存在局限性，如需要核平滑或密度比估计，本文旨在提出更高效且通用的解决方案。

Method: 采用核均值嵌入技术，结合结果桥和干预桥方法，提出双重稳健估计器，无需核平滑或密度比估计，适用于连续和高维干预变量。

Result: 新估计器在代理因果学习基准测试中优于现有方法，包括需要核平滑和密度比估计的双重稳健方法，并提供强一致性保证。

Conclusion: 本文提出的方法在代理因果学习中表现出色，尤其适用于连续或高维干预变量，为因果函数估计提供了更高效和通用的解决方案。

Abstract: We study the problem of causal function estimation in the Proxy Causal
Learning (PCL) framework, where confounders are not observed but proxies for
the confounders are available. Two main approaches have been proposed: outcome
bridge-based and treatment bridge-based methods. In this work, we propose two
kernel-based doubly robust estimators that combine the strengths of both
approaches, and naturally handle continuous and high-dimensional variables. Our
identification strategy builds on a recent density ratio-free method for
treatment bridge-based PCL; furthermore, in contrast to previous approaches, it
does not require indicator functions or kernel smoothing over the treatment
variable. These properties make it especially well-suited for continuous or
high-dimensional treatments. By using kernel mean embeddings, we have
closed-form solutions and strong consistency guarantees. Our estimators
outperform existing methods on PCL benchmarks, including a prior doubly robust
method that requires both kernel smoothing and density ratio estimation.

</details>


### [609] [Equivariant Representation Learning for Symmetry-Aware Inference with Guarantees](https://arxiv.org/abs/2505.19809)
*Daniel Ordoñez-Apraez,Alek Fröhlich,Vladimir Kostić,Karim Lounici,Vivien Brandt,Massimiliano Pontil*

Main category: cs.LG

TL;DR: 该论文提出了一种新的等变表示学习框架，结合回归、条件概率估计和不确定性量化，并首次提供了非渐近统计学习保证。


<details>
  <summary>Details</summary>
Motivation: 在许多回归、条件概率估计和不确定性量化的实际应用中，利用物理或几何中的对称性可以显著提高泛化能力和样本效率。然而，尽管几何深度学习在结合群论结构方面取得了显著进展，但统计学习保证方面的研究较少。

Method: 论文基于算子和群表示理论，提出了一个等变表示学习框架，通过近似条件期望算子的谱分解，构建既等变又沿独立对称子群解耦的表示。

Result: 在合成数据集和实际机器人应用上的实验表明，该方法在回归任务中表现优于或匹配现有的等变基线，同时提供了良好校准的参数不确定性估计。

Conclusion: 该框架不仅在实际应用中表现出色，还首次提供了非渐近统计学习保证，为结合对称性的深度学习提供了新的理论支持。

Abstract: In many real-world applications of regression, conditional probability
estimation, and uncertainty quantification, exploiting symmetries rooted in
physics or geometry can dramatically improve generalization and sample
efficiency. While geometric deep learning has made significant empirical
advances by incorporating group-theoretic structure, less attention has been
given to statistical learning guarantees. In this paper, we introduce an
equivariant representation learning framework that simultaneously addresses
regression, conditional probability estimation, and uncertainty quantification
while providing first-of-its-kind non-asymptotic statistical learning
guarantees. Grounded in operator and group representation theory, our framework
approximates the spectral decomposition of the conditional expectation
operator, building representations that are both equivariant and disentangled
along independent symmetry subgroups. Empirical evaluations on synthetic
datasets and real-world robotics applications confirm the potential of our
approach, matching or outperforming existing equivariant baselines in
regression while additionally providing well-calibrated parametric uncertainty
estimates.

</details>


### [610] [InfoCons: Identifying Interpretable Critical Concepts in Point Clouds via Information Theory](https://arxiv.org/abs/2505.19820)
*Feifei Li,Mi Zhang,Zhaoxiang Wang,Min Yang*

Main category: cs.LG

TL;DR: 提出InfoCons框架，通过信息论原理分解点云为3D概念，评估其对模型预测的因果影响，提升模型解释性。


<details>
  <summary>Details</summary>
Motivation: 点云模型在自动驾驶等安全关键场景中的应用需要解释性，以诊断模型失败原因。理想的解释应忠实于模型预测且符合人类语义理解。

Method: InfoCons框架应用信息论分解点云为3D概念，结合可学习先验分析其对预测的因果效应。

Result: 在合成数据集上定性定量优于四个基线，并在两个真实数据集和两个应用中验证了可扩展性和灵活性。

Conclusion: InfoCons能有效识别点云中的关键概念，为模型提供忠实且语义一致的解释。

Abstract: Interpretability of point cloud (PC) models becomes imperative given their
deployment in safety-critical scenarios such as autonomous vehicles. We focus
on attributing PC model outputs to interpretable critical concepts, defined as
meaningful subsets of the input point cloud. To enable human-understandable
diagnostics of model failures, an ideal critical subset should be *faithful*
(preserving points that causally influence predictions) and *conceptually
coherent* (forming semantically meaningful structures that align with human
perception). We propose InfoCons, an explanation framework that applies
information-theoretic principles to decompose the point cloud into 3D concepts,
enabling the examination of their causal effect on model predictions with
learnable priors. We evaluate InfoCons on synthetic datasets for
classification, comparing it qualitatively and quantitatively with four
baselines. We further demonstrate its scalability and flexibility on two
real-world datasets and in two applications that utilize critical scores of PC.

</details>


### [611] [LAPA-based Dynamic Privacy Optimization for Wireless Federated Learning in Heterogeneous Environments](https://arxiv.org/abs/2505.19823)
*Pengcheng Sun,Erwu Liu,Wei Ni,Rui Wang,Yuanzhe Geng,Lijuan Lai,Abbas Jamalipour*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级自适应隐私分配策略（LAPA），通过个性化隐私预算分配和动态优化，在保护联邦学习隐私的同时提升聚合效率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）虽能保护数据隐私，但仍面临梯度泄漏攻击的风险。差分隐私（DP）技术通过添加噪声降低风险，但会影响FL效用，尤其在非独立同分布（Non-IID）数据场景下。

Method: 提出LAPA策略，为设备分配个性化隐私预算，无需额外传输信息；采用DDPG算法优化传输功率，平衡DP与系统效用；设计结合通信质量和数据分布的可靠聚合策略。

Result: 实验表明，LAPA的个性化噪声分配和动态优化策略在满足隐私需求的同时，提升了FL的收敛性能。

Conclusion: LAPA策略有效解决了隐私保护与聚合效率的平衡问题，为Non-IID数据场景下的联邦学习提供了实用解决方案。

Abstract: Federated Learning (FL) is a distributed machine learning paradigm based on
protecting data privacy of devices, which however, can still be broken by
gradient leakage attack via parameter inversion techniques. Differential
privacy (DP) technology reduces the risk of private data leakage by adding
artificial noise to the gradients, but detrimental to the FL utility at the
same time, especially in the scenario where the data is Non-Independent
Identically Distributed (Non-IID). Based on the impact of heterogeneous data on
aggregation performance, this paper proposes a Lightweight Adaptive Privacy
Allocation (LAPA) strategy, which assigns personalized privacy budgets to
devices in each aggregation round without transmitting any additional
information beyond gradients, ensuring both privacy protection and aggregation
efficiency. Furthermore, the Deep Deterministic Policy Gradient (DDPG)
algorithm is employed to optimize the transmission power, in order to determine
the optimal timing at which the adaptively attenuated artificial noise aligns
with the communication noise, enabling an effective balance between DP and
system utility. Finally, a reliable aggregation strategy is designed by
integrating communication quality and data distribution characteristics, which
improves aggregation performance while preserving privacy. Experimental results
demonstrate that the personalized noise allocation and dynamic optimization
strategy based on LAPA proposed in this paper enhances convergence performance
while satisfying the privacy requirements of FL.

</details>


### [612] [Foundation Models for Tabular Data within Systemic Contexts Need Grounding](https://arxiv.org/abs/2505.19825)
*Tassilo Klein,Johannes Hoffart*

Main category: cs.LG

TL;DR: 该论文提出了一种新的语义链接表（SLT）概念，并开发了基于此的FMSLT模型，旨在解决现有表格基础模型忽视操作上下文的问题。


<details>
  <summary>Details</summary>
Motivation: 当前表格基础模型的研究通常将表格视为孤立实体，并假设信息完整，忽视了重要的操作上下文。这限制了模型在复杂、互连表格数据上的应用潜力。

Method: 论文引入了语义链接表（SLT）的概念，并提出FMSLT模型，将表格数据与其操作上下文（包括声明性和程序性知识）结合起来。

Result: FMSLT模型能够更全面地表示表格数据，释放机器学习在复杂互连表格数据中的潜力，但需要领域专家和研究人员的紧密合作以获取操作知识。

Conclusion: 论文揭示了现有表格基础模型的局限性，并提出了以FMSLT为中心的新方向，旨在推动结构数据的鲁棒、上下文感知模型的发展。

Abstract: Current research on tabular foundation models often overlooks the
complexities of large-scale, real-world data by treating tables as isolated
entities and assuming information completeness, thereby neglecting the vital
operational context. To address this, we introduce the concept of Semantically
Linked Tables (SLT), recognizing that tables are inherently connected to both
declarative and procedural operational knowledge. We propose Foundation Models
for Semantically Linked Tables (FMSLT), which integrate these components to
ground tabular data within its true operational context. This comprehensive
representation unlocks the full potential of machine learning for complex,
interconnected tabular data across diverse domains. Realizing FMSLTs requires
access to operational knowledge that is often unavailable in public datasets,
highlighting the need for close collaboration between domain experts and
researchers. Our work exposes the limitations of current tabular foundation
models and proposes a new direction centered on FMSLTs, aiming to advance
robust, context-aware models for structured data.

</details>


### [613] [Revisiting Glorot Initialization for Long-Range Linear Recurrences](https://arxiv.org/abs/2505.19827)
*Noga Bar,Mariia Seleznova,Yotam Alexander,Gitta Kutyniok,Raja Giryes*

Main category: cs.LG

TL;DR: 论文指出Glorot初始化在长序列RNN中不稳定，提出了一种维度感知的调整方法以稳定信号传播。


<details>
  <summary>Details</summary>
Motivation: Glorot初始化在无限宽度和固定长度的假设下设计，不适用于处理长序列的RNN，可能导致信号爆炸或消失。

Method: 提出了一种简单、维度感知的Glorot初始化调整方法，将谱半径略微降低至1以下。

Result: 理论分析表明，序列长度t=O(√n)足以引发不稳定，新方法有效防止了信号的快速爆炸或衰减。

Conclusion: 标准初始化方法在长序列场景下可能失效，需要针对循环网络设计新的稳定初始化理论。

Abstract: Proper initialization is critical for Recurrent Neural Networks (RNNs),
particularly in long-range reasoning tasks, where repeated application of the
same weight matrix can cause vanishing or exploding signals. A common baseline
for linear recurrences is Glorot initialization, designed to ensure stable
signal propagation--but derived under the infinite-width, fixed-length
regime--an unrealistic setting for RNNs processing long sequences. In this
work, we show that Glorot initialization is in fact unstable: small positive
deviations in the spectral radius are amplified through time and cause the
hidden state to explode. Our theoretical analysis demonstrates that sequences
of length $t = O(\sqrt{n})$, where $n$ is the hidden width, are sufficient to
induce instability. To address this, we propose a simple, dimension-aware
rescaling of Glorot that shifts the spectral radius slightly below one,
preventing rapid signal explosion or decay. These results suggest that standard
initialization schemes may break down in the long-sequence regime, motivating a
separate line of theory for stable recurrent initialization.

</details>


### [614] [PCDCNet: A Surrogate Model for Air Quality Forecasting with Physical-Chemical Dynamics and Constraints](https://arxiv.org/abs/2505.19842)
*Shuo Wang,Yun Cheng,Qingye Meng,Olga Saukh,Jiang Zhang,Jingfang Fan,Yuanting Zhang,Xingyuan Yuan,Lothar Thiele*

Main category: cs.LG

TL;DR: 论文提出PCDCNet模型，结合数值模拟与深度学习，用于空气质量预测，在降低计算成本的同时提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统数值模型计算成本高且依赖不确定的排放清单，而深度学习模型因缺乏物理约束泛化能力不足。需要一种兼顾物理一致性与计算效率的方法。

Method: 提出PCDCNet模型，整合排放、气象影响和领域约束，采用图结构空间传输建模、循环结构时间累积及局部交互表示增强。

Result: PCDCNet在72小时站点级PM2.5和O3预测中达到SOTA性能，显著降低计算成本，并已部署为实时在线预测平台。

Conclusion: PCDCNet通过融合物理约束与深度学习，为空气质量预测提供了高效、可解释的解决方案，兼具实用价值与社会意义。

Abstract: Air quality forecasting (AQF) is critical for public health and environmental
management, yet remains challenging due to the complex interplay of emissions,
meteorology, and chemical transformations. Traditional numerical models, such
as CMAQ and WRF-Chem, provide physically grounded simulations but are
computationally expensive and rely on uncertain emission inventories. Deep
learning models, while computationally efficient, often struggle with
generalization due to their lack of physical constraints. To bridge this gap,
we propose PCDCNet, a surrogate model that integrates numerical modeling
principles with deep learning. PCDCNet explicitly incorporates emissions,
meteorological influences, and domain-informed constraints to model pollutant
formation, transport, and dissipation. By combining graph-based spatial
transport modeling, recurrent structures for temporal accumulation, and
representation enhancement for local interactions, PCDCNet achieves
state-of-the-art (SOTA) performance in 72-hour station-level PM2.5 and O3
forecasting while significantly reducing computational costs. Furthermore, our
model is deployed in an online platform, providing free, real-time air quality
forecasts, demonstrating its scalability and societal impact. By aligning deep
learning with physical consistency, PCDCNet offers a practical and
interpretable solution for AQF, enabling informed decision-making for both
personal and regulatory applications.

</details>


### [615] [DISCOVER: Automated Curricula for Sparse-Reward Reinforcement Learning](https://arxiv.org/abs/2505.19850)
*Leander Diaz-Bone,Marco Bagatella,Jonas Hübotter,Andreas Krause*

Main category: cs.LG

TL;DR: 论文提出DISCOVER方法，通过定向目标选择解决稀疏奖励强化学习中的探索难题，显著提升高维长时程任务的解决能力。


<details>
  <summary>Details</summary>
Motivation: 稀疏奖励强化学习在处理高维复杂任务时面临探索效率低和长期信用分配困难的问题。现有方法通常设计通用探索策略，难以应对高维长时程任务。作者认为应通过解决与目标任务相关的简单子任务来逐步实现复杂任务。

Method: 提出DISCOVER方法（定向稀疏奖励目标条件长时程强化学习），从现有RL算法中提取方向信息，无需先验知识即可选择朝目标任务方向的探索目标。该方法与bandit问题的理论探索相关联，形式化界定了目标任务可达时间。

Result: 在高维环境中验证表明，DISCOVER的定向目标选择能力能解决现有最先进探索方法无法处理的难题，其性能仅与初始状态到目标距离相关，而与任务空间体积无关。

Conclusion: DISCOVER通过定向探索机制有效解决了稀疏奖励RL的核心挑战，为构建具有超人类能力的自我改进智能体提供了新思路。

Abstract: Sparse-reward reinforcement learning (RL) can model a wide range of highly
complex tasks. Solving sparse-reward tasks is RL's core premise - requiring
efficient exploration coupled with long-horizon credit assignment - and
overcoming these challenges is key for building self-improving agents with
superhuman ability. We argue that solving complex and high-dimensional tasks
requires solving simpler tasks that are relevant to the target task. In
contrast, most prior work designs strategies for selecting exploratory tasks
with the objective of solving any task, making exploration of challenging
high-dimensional, long-horizon tasks intractable. We find that the sense of
direction, necessary for effective exploration, can be extracted from existing
RL algorithms, without needing any prior information. Based on this finding, we
propose a method for directed sparse-reward goal-conditioned very long-horizon
RL (DISCOVER), which selects exploratory goals in the direction of the target
task. We connect DISCOVER to principled exploration in bandits, formally
bounding the time until the target task becomes achievable in terms of the
agent's initial distance to the target, but independent of the volume of the
space of all tasks. Empirically, we perform a thorough evaluation in
high-dimensional environments. We find that the directed goal selection of
DISCOVER solves exploration problems that are beyond the reach of prior
state-of-the-art exploration methods in RL.

</details>


### [616] [Editing as Unlearning: Are Knowledge Editing Methods Strong Baselines for Large Language Model Unlearning?](https://arxiv.org/abs/2505.19855)
*Zexi Li,Xiangzhu Wang,William F. Shen,Meghdad Kurmanji,Xinchi Qiu,Dongqi Cai,Chao Wu,Nicholas D. Lane*

Main category: cs.LG

TL;DR: 该论文探讨了将知识编辑技术作为大语言模型（LLM）遗忘任务基线的方法，发现某些编辑方法（如WISE和AlphaEdit）在遗忘预训练知识方面表现优异，并提出了改进遗忘效果的实用策略。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索知识编辑与遗忘任务之间的联系，提出将遗忘视为知识编辑的一种特殊形式（即信息被修改为拒绝或空集响应），从而利用现有编辑技术提升遗忘效果。

Method: 方法包括评估多种先进知识编辑技术（如ROME、MEMIT、GRACE、WISE和AlphaEdit）在遗忘任务中的表现，并提出两种实用策略：自我改进（利用LLM的上下文学习能力生成更符合人类预期的遗忘目标）和查询合并（提升ROME和MEMIT处理长序列样本的能力）。

Result: 结果表明，某些编辑方法（尤其是WISE和AlphaEdit）能有效作为遗忘基线，尤其在预训练知识遗忘和生成人类对齐的拒绝回答方面表现突出。

Conclusion: 结论建议遗忘研究社区采用先进编辑方法作为基线，并从编辑视角探索遗忘任务，以实现更全面的LLM记忆控制。

Abstract: Large language Model (LLM) unlearning, i.e., selectively removing information
from LLMs, is vital for responsible model deployment. Differently, LLM
knowledge editing aims to modify LLM knowledge instead of removing it. Though
editing and unlearning seem to be two distinct tasks, we find there is a tight
connection between them. In this paper, we conceptualize unlearning as a
special case of editing where information is modified to a refusal or "empty
set" $\emptyset$ response, signifying its removal. This paper thus investigates
if knowledge editing techniques are strong baselines for LLM unlearning. We
evaluate state-of-the-art (SOTA) editing methods (e.g., ROME, MEMIT, GRACE,
WISE, and AlphaEdit) against existing unlearning approaches on pretrained and
finetuned knowledge. Results show certain editing methods, notably WISE and
AlphaEdit, are effective unlearning baselines, especially for pretrained
knowledge, and excel in generating human-aligned refusal answers. To better
adapt editing methods for unlearning applications, we propose practical recipes
including self-improvement and query merging. The former leverages the LLM's
own in-context learning ability to craft a more human-aligned unlearning
target, and the latter enables ROME and MEMIT to perform well in unlearning
longer sample sequences. We advocate for the unlearning community to adopt SOTA
editing methods as baselines and explore unlearning from an editing perspective
for more holistic LLM memory control.

</details>


### [617] [Deep Active Inference Agents for Delayed and Long-Horizon Environments](https://arxiv.org/abs/2505.19867)
*Yavar Taheri Yeganeh,Mohsen Jafari,Andrea Matta*

Main category: cs.LG

TL;DR: 该论文提出了一种结合生成模型与主动推理（AIF）的架构，解决了在延迟和长时程环境中传统方法依赖精确即时预测和详尽规划的问题，并在工业场景中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前基于主动推理的智能体在延迟和长时程环境中表现不佳，主要依赖精确的即时预测和详尽的规划，且现有评估多局限于机器人或视觉基准，未能充分体现实际工业场景的复杂性。

Method: 提出了一种生成策略架构，包括多步潜在转移、集成策略网络、交替优化方案和单步梯度规划，以消除控制循环中的详尽规划需求。

Result: 在模拟工业场景的延迟和长时程环境中，该架构表现出色，证明了结合世界模型与主动推理形式主义的端到端概率控制器的有效性。

Conclusion: 该研究成功开发了一种无需手工奖励或昂贵规划即可在延迟和长时程环境中进行有效决策的智能体，为复杂工业应用提供了新思路。

Abstract: With the recent success of world-model agents, which extend the core idea of
model-based reinforcement learning by learning a differentiable model for
sample-efficient control across diverse tasks, active inference (AIF) offers a
complementary, neuroscience-grounded paradigm that unifies perception,
learning, and action within a single probabilistic framework powered by a
generative model. Despite this promise, practical AIF agents still rely on
accurate immediate predictions and exhaustive planning, a limitation that is
exacerbated in delayed environments requiring plans over long horizons, tens to
hundreds of steps. Moreover, most existing agents are evaluated on robotic or
vision benchmarks which, while natural for biological agents, fall short of
real-world industrial complexity. We address these limitations with a
generative-policy architecture featuring (i) a multi-step latent transition
that lets the generative model predict an entire horizon in a single
look-ahead, (ii) an integrated policy network that enables the transition and
receives gradients of the expected free energy, (iii) an alternating
optimization scheme that updates model and policy from a replay buffer, and
(iv) a single gradient step that plans over long horizons, eliminating
exhaustive planning from the control loop. We evaluate our agent in an
environment that mimics a realistic industrial scenario with delayed and
long-horizon settings. The empirical results confirm the effectiveness of the
proposed approach, demonstrating the coupled world-model with the AIF formalism
yields an end-to-end probabilistic controller capable of effective decision
making in delayed, long-horizon settings without handcrafted rewards or
expensive planning.

</details>


### [618] [Generalized and Personalized Federated Learning with Foundation Models via Orthogonal Transformations](https://arxiv.org/abs/2505.19888)
*Eun Gyung Kong,Je Won Yeom,Yonghoon Jeon,Taesup Kim*

Main category: cs.LG

TL;DR: FedOT是一种新的联邦学习方法，通过共享全局分类器并结合局部正交变换，解决了数据异构下的泛化与个性化挑战。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，如何在数据异构的情况下同时实现模型的泛化能力和个性化是一个重要挑战。

Method: FedOT利用黑盒基础模型，共享全局任务相关分类器，并通过局部正交变换调整特征，减少梯度冲突并保持语义完整性。

Result: FedOT在多个基准测试中优于基线联邦学习方法，证实了全局分类器和局部正交变换联合优化的有效性。

Conclusion: FedOT通过平衡全局和局部参数，为数据异构环境下的联邦学习提供了更优的解决方案，并具有广泛适用性。

Abstract: Federated Learning (FL) aims to train models across decentralized clients or
devices holding local data without the need for centralized data collection,
thus enhancing data privacy and security. However, achieving both
generalization and personalization in heterogeneous settings remains a
significant challenge. To address this, we introduce FedOT, a novel approach
that leverages black-box foundation models. FedOT shares only a global
task-dependent classifier across clients while locally adapting features
through orthogonal transformations. By enforcing orthogonality, FedOT mitigates
gradient conflicts across diverse clients, preserves semantic integrity, and
achieves robust performance even in the presence of substantial data
heterogeneity. The strategy of combining global and local parameters enables a
more balanced approach for both generalization and personalization,
outperforming baseline FL methods across multiple benchmarks. Furthermore, our
extensive analysis confirms that joint optimization of global classifiers and
local orthogonal transformations yields superior performance and suggests
broader applicability.

</details>


### [619] [ESLM: Risk-Averse Selective Language Modeling for Efficient Pretraining](https://arxiv.org/abs/2505.19893)
*Melis Ilayda Bal,Volkan Cevher,Michael Muehlebach*

Main category: cs.LG

TL;DR: ESLM通过在线令牌级批量选择提升大语言模型预训练效率，减少冗余计算，保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型预训练计算密集，许多令牌对学习贡献有限，导致效率低下。

Method: ESLM利用令牌级统计（如熵或损失）和风险价值阈值，保留每批次中最具信息量的令牌。

Result: 实验显示ESLM显著减少训练FLOPs，同时保持或提升困惑度和下游任务性能。

Conclusion: ESLM是一种高效、可扩展的方法，适用于不同模型规模和预训练语料。

Abstract: Large language model pretraining is compute-intensive, yet many tokens
contribute marginally to learning, resulting in inefficiency. We introduce
Efficient Selective Language Modeling (ESLM), a risk-aware algorithm that
improves training efficiency and distributional robustness by performing online
token-level batch selection. ESLM leverages per-token statistics (e.g., entropy
or loss) and applies value-at-risk thresholding to retain only the most
informative tokens per batch. This data-centric mechanism reshapes the training
loss, prioritizing high-risk tokens and eliminating redundant gradient
computation. We frame ESLM as a bilevel game: the model competes with a masking
adversary that selects worst-case token subsets under a constrained
thresholding rule. In the loss-based setting, ESLM recovers conditional
value-at-risk loss minimization, providing a principled connection to
distributionally robust optimization. We extend our approach to Ada-ESLM, which
adaptively tunes the selection confidence during training. Experiments on GPT-2
pretraining show that ESLM significantly reduces training FLOPs while
maintaining or improving both perplexity and downstream performance compared to
baselines. Our approach also scales across model sizes, pretraining corpora,
and integrates naturally with knowledge distillation.

</details>


### [620] [Learning to Trust Bellman Updates: Selective State-Adaptive Regularization for Offline RL](https://arxiv.org/abs/2505.19923)
*Qin-Wen Luo,Ming-Kun Xie,Ye-Wen Wang,Sheng-Jun Huang*

Main category: cs.LG

TL;DR: 本文提出了一种选择性状态自适应正则化方法，用于解决离线强化学习中固定正则化强度导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中，固定正则化强度无法适应数据质量的差异，导致性能下降或行为克隆。

Method: 提出选择性状态自适应正则化方法，根据状态质量自适应调整正则化强度，避免对低质量动作的过度约束。

Result: 在D4RL基准测试中，该方法在离线和离线到在线设置中均显著优于现有方法。

Conclusion: 选择性状态自适应正则化方法有效解决了离线强化学习中的性能下降问题，提升了学习效果。

Abstract: Offline reinforcement learning (RL) aims to learn an effective policy from a
static dataset. To alleviate extrapolation errors, existing studies often
uniformly regularize the value function or policy updates across all states.
However, due to substantial variations in data quality, the fixed
regularization strength often leads to a dilemma: Weak regularization strength
fails to address extrapolation errors and value overestimation, while strong
regularization strength shifts policy learning toward behavior cloning,
impeding potential performance enabled by Bellman updates. To address this
issue, we propose the selective state-adaptive regularization method for
offline RL. Specifically, we introduce state-adaptive regularization
coefficients to trust state-level Bellman-driven results, while selectively
applying regularization on high-quality actions, aiming to avoid performance
degradation caused by tight constraints on low-quality actions. By establishing
a connection between the representative value regularization method, CQL, and
explicit policy constraint methods, we effectively extend selective
state-adaptive regularization to these two mainstream offline RL approaches.
Extensive experiments demonstrate that the proposed method significantly
outperforms the state-of-the-art approaches in both offline and
offline-to-online settings on the D4RL benchmark.

</details>


### [621] [Logic Gate Neural Networks are Good for Verification](https://arxiv.org/abs/2505.19932)
*Fabian Kresse,Emily Yu,Christoph H. Lampert,Thomas A. Henzinger*

Main category: cs.LG

TL;DR: 论文提出了一种基于SAT编码的方法，用于验证学习型逻辑门网络（LGNs）的全局鲁棒性和公平性，验证了LGNs在保持高性能的同时易于验证。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络的复杂性给形式化验证带来了挑战，而LGNs通过布尔逻辑门替代乘法运算，提供了更稀疏、类似网表的结构，更适合符号验证。

Method: 引入了一种SAT编码方法，用于验证LGNs的全局鲁棒性和公平性，并在五个基准数据集上进行了评估。

Result: 实验结果表明，LGNs不仅易于验证，而且在预测性能上表现优异。

Conclusion: LGNs是一种既适合形式化验证又能保持高性能的神经网络替代方案。

Abstract: Learning-based systems are increasingly deployed across various domains, yet
the complexity of traditional neural networks poses significant challenges for
formal verification. Unlike conventional neural networks, learned Logic Gate
Networks (LGNs) replace multiplications with Boolean logic gates, yielding a
sparse, netlist-like architecture that is inherently more amenable to symbolic
verification, while still delivering promising performance. In this paper, we
introduce a SAT encoding for verifying global robustness and fairness in LGNs.
We evaluate our method on five benchmark datasets, including a newly
constructed 5-class variant, and find that LGNs are both verification-friendly
and maintain strong predictive performance.

</details>


### [622] [Task-Oriented Low-Label Semantic Communication With Self-Supervised Learning](https://arxiv.org/abs/2505.19940)
*Run Gu,Wei Xu,Zhaohui Yang,Dusit Niyato,Aylin Yener*

Main category: cs.LG

TL;DR: 该论文提出了一种基于自监督学习的语义通信框架SLSCom，旨在在标记样本有限的情况下提升任务推理性能，通过自监督对比学习和信息瓶颈问题优化语义提取，实验证明其在多种条件下优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的语义通信依赖大量标记样本，但在实际边缘网络中标记样本获取困难。论文旨在解决标记样本有限场景下的任务推理性能问题。

Method: 提出SLSCom框架：1) 利用无标记样本训练任务相关语义编码器；2) 通过自监督对比学习和信息瓶颈问题平衡特征信息量与任务性能；3) 设计自监督分类和重构代理任务解决计算难题；4) 联合训练方法提升端到端推理精度。

Result: 在多径无线信道图像分类任务中，SLSCom在标记数据集规模和信噪比变化时，显著优于传统数字编码和现有基于深度学习的方法，即使无标记样本与下游任务无关。

Conclusion: SLSCom通过自监督学习有效解决了标记数据稀缺问题，为任务导向的语义通信提供了高效解决方案。

Abstract: Task-oriented semantic communication enhances transmission efficiency by
conveying semantic information rather than exact messages. Deep learning
(DL)-based semantic communication can effectively cultivate the essential
semantic knowledge for semantic extraction, transmission, and interpretation by
leveraging massive labeled samples for downstream task training. In this paper,
we propose a self-supervised learning-based semantic communication framework
(SLSCom) to enhance task inference performance, particularly in scenarios with
limited access to labeled samples. Specifically, we develop a task-relevant
semantic encoder using unlabeled samples, which can be collected by devices in
real-world edge networks. To facilitate task-relevant semantic extraction, we
introduce self-supervision for learning contrastive features and formulate the
information bottleneck (IB) problem to balance the tradeoff between the
informativeness of the extracted features and task inference performance. Given
the computational challenges of the IB problem, we devise a practical and
effective solution by employing self-supervised classification and
reconstruction pretext tasks. We further propose efficient joint training
methods to enhance end-to-end inference accuracy over wireless channels, even
with few labeled samples. We evaluate the proposed framework on image
classification tasks over multipath wireless channels. Extensive simulation
results demonstrate that SLSCom significantly outperforms conventional digital
coding methods and existing DL-based approaches across varying labeled data set
sizes and SNR conditions, even when the unlabeled samples are irrelevant to the
downstream tasks.

</details>


### [623] [Beyond Freezing: Sparse Tuning Enhances Plasticity in Continual Learning with Pre-Trained Models](https://arxiv.org/abs/2505.19943)
*Huan Zhang,Fan Lyu,Shuyu Dong,Shenghua Fan,Yujin Zheng,Dingwen Wang*

Main category: cs.LG

TL;DR: 提出MIST方法，通过互信息指导稀疏调优，选择性更新预训练模型参数，提升持续学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法冻结预训练模型并依赖辅助模块，限制了模型可塑性且泛化性能不足。全微调虽提升适应性，但可能破坏预训练知识。

Method: MIST基于互信息目标敏感性，选择性更新少于5%的预训练参数，并引入强稀疏正则化，每步更新少于0.5%参数。

Result: MIST在多种持续学习基准测试中显著提升性能，集成到多个基线方法后均取得明显改进。

Conclusion: MIST是一种即插即用方法，能有效平衡任务适应性与泛化能力，为持续学习提供新思路。

Abstract: Continual Learning with Pre-trained Models holds great promise for efficient
adaptation across sequential tasks. However, most existing approaches freeze
PTMs and rely on auxiliary modules like prompts or adapters, limiting model
plasticity and leading to suboptimal generalization when facing significant
distribution shifts. While full fine-tuning can improve adaptability, it risks
disrupting crucial pre-trained knowledge. In this paper, we propose Mutual
Information-guided Sparse Tuning (MIST), a plug-and-play method that
selectively updates a small subset of PTM parameters, less than 5%, based on
sensitivity to mutual information objectives. MIST enables effective
task-specific adaptation while preserving generalization. To further reduce
interference, we introduce strong sparsity regularization by randomly dropping
gradients during tuning, resulting in fewer than 0.5% of parameters being
updated per step. Applied before standard freeze-based methods, MIST
consistently boosts performance across diverse continual learning benchmarks.
Experiments show that integrating our method into multiple baselines yields
significant performance gains. Our code is available at
https://github.com/zhwhu/MIST.

</details>


### [624] [Inverse Q-Learning Done Right: Offline Imitation Learning in $Q^π$-Realizable MDPs](https://arxiv.org/abs/2505.19946)
*Antoine Moulin,Gergely Neu,Luca Viano*

Main category: cs.LG

TL;DR: 该论文提出了一种新的离线模仿学习算法SPOIL，适用于线性Qπ可实现MDPs，并在非线性情况下扩展，实验证明其优于行为克隆且与先进算法竞争。


<details>
  <summary>Details</summary>
Motivation: 研究离线模仿学习问题，旨在利用专家生成的状态-动作对数据集学习高性能策略，不同于以往假设专家属于已知策略类的方法，本文从环境结构假设的新角度出发。

Method: 针对线性Qπ可实现MDPs，提出SPOIL算法，保证在O(ε⁻²)样本下匹配专家性能；扩展到非线性Qπ可实现MDPs，样本复杂度为O(ε⁻⁴)。

Result: SPOIL算法在标准基准测试中表现优于行为克隆，并与最先进算法竞争，同时提出了一种新的批评网络损失函数。

Conclusion: SPOIL算法在理论和实验上均表现出色，为离线模仿学习提供了新的有效方法。

Abstract: We study the problem of offline imitation learning in Markov decision
processes (MDPs), where the goal is to learn a well-performing policy given a
dataset of state-action pairs generated by an expert policy. Complementing a
recent line of work on this topic that assumes the expert belongs to a
tractable class of known policies, we approach this problem from a new angle
and leverage a different type of structural assumption about the environment.
Specifically, for the class of linear $Q^\pi$-realizable MDPs, we introduce a
new algorithm called saddle-point offline imitation learning (\SPOIL), which is
guaranteed to match the performance of any expert up to an additive error
$\varepsilon$ with access to $\mathcal{O}(\varepsilon^{-2})$ samples. Moreover,
we extend this result to possibly non-linear $Q^\pi$-realizable MDPs at the
cost of a worse sample complexity of order $\mathcal{O}(\varepsilon^{-4})$.
Finally, our analysis suggests a new loss function for training critic networks
from expert data in deep imitation learning. Empirical evaluations on standard
benchmarks demonstrate that the neural net implementation of \SPOIL is superior
to behavior cloning and competitive with state-of-the-art algorithms.

</details>


### [625] [Dynamically Learned Test-Time Model Routing in Language Model Zoos with Service Level Guarantees](https://arxiv.org/abs/2505.19947)
*Herbert Woisetschläger,Ryan Zhang,Shiqiang Wang,Hans-Arno Jacobsen*

Main category: cs.LG

TL;DR: MESS+是一种随机优化算法，用于在保证服务质量的前提下，实现成本最优的LLM请求路由。


<details>
  <summary>Details</summary>
Motivation: 当前开放的LLM模型库虽然提供了大量高质量模型，但用户选择适合特定任务的模型仍具有挑战性，且需要专业知识。用户更关注响应的事实准确性、安全性和满意度，而服务提供商则希望最小化运营成本。这些竞争性需求通常通过服务级别协议（SLA）来平衡。

Method: MESS+通过实时学习用户交互中LLM的请求满意度概率，结合虚拟队列和请求满意度预测，解决每个请求的优化问题，从而做出模型选择决策。

Result: 在多种先进的LLM基准测试中，MESS+相比现有LLM路由技术平均节省了2倍的成本。

Conclusion: MESS+在保证SLA合规的同时，实现了成本最优的LLM请求路由，为模型选择提供了一种高效且经济的解决方案。

Abstract: Open-weight LLM zoos provide access to numerous high-quality models, but
selecting the appropriate model for specific tasks remains challenging and
requires technical expertise. Most users simply want factually correct, safe,
and satisfying responses without concerning themselves with model
technicalities, while inference service providers prioritize minimizing
operating costs. These competing interests are typically mediated through
service level agreements (SLAs) that guarantee minimum service quality. We
introduce MESS+, a stochastic optimization algorithm for cost-optimal LLM
request routing while providing rigorous SLA compliance guarantees. MESS+
learns request satisfaction probabilities of LLMs in real-time as users
interact with the system, based on which model selection decisions are made by
solving a per-request optimization problem. Our algorithm includes a novel
combination of virtual queues and request satisfaction prediction, along with a
theoretical analysis of cost optimality and constraint satisfaction. Across a
wide range of state-of-the-art LLM benchmarks, MESS+ achieves an average of 2x
cost savings compared to existing LLM routing techniques.

</details>


### [626] [Which Data Attributes Stimulate Math and Code Reasoning? An Investigation via Influence Functions](https://arxiv.org/abs/2505.19949)
*Siqi Kou,Qingyuan Tian,Hanwen Xu,Zihao Zeng,Zhijie Deng*

Main category: cs.LG

TL;DR: 该论文通过影响力函数分析大语言模型在数学和编程任务中的推理能力来源，提出基于任务难度翻转的数据重加权策略，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型推理能力提升方法主要依赖启发式策略，缺乏对数据内在特征的深入理解，限制了模型的泛化能力。

Method: 使用影响力函数（influence functions）系统分析训练数据对模型推理能力的影响，并提出基于任务难度翻转的数据重加权策略。

Result: 高难度数学样本能同时提升数学和编程推理能力，而低难度编程任务最有效提升编程推理；新策略使AIME24准确率翻倍（10%→20%），LiveCodeBench准确率提升（33.8%→35.3%）。

Conclusion: 数学和编程推理在token级别的影响模式存在差异：数学偏好自然语言逻辑连接词，编程强调结构语法；序列级探索行为对两者均有增强作用。

Abstract: Large language models (LLMs) have demonstrated remarkable reasoning
capabilities in math and coding, often bolstered by post-training on the
chain-of-thoughts (CoTs) generated by stronger models. However, existing
strategies for curating such training data predominantly rely on heuristics,
limiting generalizability and failing to capture subtleties underlying in data.
To address these limitations, we leverage influence functions to systematically
attribute LLMs' reasoning ability on math and coding to individual training
examples, sequences, and tokens, enabling deeper insights into effective data
characteristics. Our Influence-based Reasoning Attribution (Infra) uncovers
nontrivial cross-domain effects across math and coding tasks: high-difficulty
math examples improve both math and code reasoning, while low-difficulty code
tasks most effectively benefit code reasoning. Based on these findings, we
introduce a simple yet effective dataset reweighting strategy by flipping task
difficulty, which doubles AIME24 accuracy from 10\% to 20\% and boosts
LiveCodeBench accuracy from 33.8\% to 35.3\% for Qwen2.5-7B-Instruct. Moreover,
our fine-grained attribution reveals that the sequence-level exploratory
behaviors enhance reasoning performance in both math and code, and the
token-level influence patterns are distinct for math and code reasoning: the
former prefers natural language logic connectors and the latter emphasizes
structural syntax.

</details>


### [627] [An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning](https://arxiv.org/abs/2505.19954)
*Andrew Zamai,Nathanael Fijalkow,Boris Mansencal,Laurent Simon,Eloi Navet,Pierrick Coupe*

Main category: cs.LG

TL;DR: 该论文提出了一种结合深度学习与大语言模型的框架，旨在提高神经退行性痴呆诊断的透明度和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前神经退行性痴呆的鉴别诊断面临症状重叠和影像学表现相似等挑战，深度学习模型虽预测性能强但决策过程不透明，限制了临床应用。

Method: 框架包含两个核心组件：一是将3D T1加权脑MRI转换为放射学报告的模块化流程；二是利用大语言模型基于生成报告辅助诊断，并通过强化学习激励模型生成诊断推理。

Result: 该框架在保持与现有深度学习方法相当的诊断性能的同时，能生成基于神经影像学发现的结构化诊断依据。

Conclusion: 所提框架通过生成因果性解释，在诊断过程中提供透明化的推理依据，有助于缩小预测准确性与可解释性之间的差距。

Abstract: The differential diagnosis of neurodegenerative dementias is a challenging
clinical task, mainly because of the overlap in symptom presentation and the
similarity of patterns observed in structural neuroimaging. To improve
diagnostic efficiency and accuracy, deep learning-based methods such as
Convolutional Neural Networks and Vision Transformers have been proposed for
the automatic classification of brain MRIs. However, despite their strong
predictive performance, these models find limited clinical utility due to their
opaque decision making. In this work, we propose a framework that integrates
two core components to enhance diagnostic transparency. First, we introduce a
modular pipeline for converting 3D T1-weighted brain MRIs into textual
radiology reports. Second, we explore the potential of modern Large Language
Models (LLMs) to assist clinicians in the differential diagnosis between
Frontotemporal dementia subtypes, Alzheimer's disease, and normal aging based
on the generated reports. To bridge the gap between predictive accuracy and
explainability, we employ reinforcement learning to incentivize diagnostic
reasoning in LLMs. Without requiring supervised reasoning traces or
distillation from larger models, our approach enables the emergence of
structured diagnostic rationales grounded in neuroimaging findings. Unlike
post-hoc explainability methods that retrospectively justify model decisions,
our framework generates diagnostic rationales as part of the inference
process-producing causally grounded explanations that inform and guide the
model's decision-making process. In doing so, our framework matches the
diagnostic performance of existing deep learning methods while offering
rationales that support its diagnostic conclusions.

</details>


### [628] [MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research](https://arxiv.org/abs/2505.19955)
*Hui Chen,Miao Xiong,Yujie Lu,Wei Han,Ailin Deng,Yufei He,Jiaying Wu,Yibo Li,Yue Liu,Bryan Hooi*

Main category: cs.LG

TL;DR: 该论文介绍了MLR-Bench，一个用于评估AI代理在开放式机器学习研究中表现的综合性基准测试，包含201个研究任务、自动评估框架MLR-Judge和模块化代理MLR-Agent，发现现有AI在生成实验结果时存在可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在科学发现中的潜力日益增长，需要一个全面的基准测试来评估其在开放式机器学习研究中的表现，以促进可信赖和透明的科学发现。

Method: 论文提出了MLR-Bench，包含三个关键组件：201个多样化的ML研究任务、结合LLM的自动评估框架MLR-Judge，以及能完成研究全过程的模块化代理MLR-Agent。

Result: 评估发现，LLMs能生成连贯的想法和结构良好的论文，但当前编码代理在80%的情况下会产生虚假或无效的实验结果，严重影响科学可靠性。MLR-Judge与人类专家评估高度一致。

Conclusion: MLR-Bench作为一个开源工具，有助于社区评估和改进AI研究代理，推动可信赖的科学发现。当前AI在实验验证环节仍需重大改进。

Abstract: Recent advancements in AI agents have demonstrated their growing potential to
drive and support scientific discovery. In this work, we introduce MLR-Bench, a
comprehensive benchmark for evaluating AI agents on open-ended machine learning
research. MLR-Bench includes three key components: (1) 201 research tasks
sourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2)
MLR-Judge, an automated evaluation framework combining LLM-based reviewers with
carefully designed review rubrics to assess research quality; and (3)
MLR-Agent, a modular agent scaffold capable of completing research tasks
through four stages: idea generation, proposal formulation, experimentation,
and paper writing. Our framework supports both stepwise assessment across these
distinct research stages, and end-to-end evaluation of the final research
paper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced
coding agent, finding that while LLMs are effective at generating coherent
ideas and well-structured papers, current coding agents frequently (e.g., in
80% of the cases) produce fabricated or invalidated experimental
results--posing a major barrier to scientific reliability. We validate
MLR-Judge through human evaluation, showing high agreement with expert
reviewers, supporting its potential as a scalable tool for research evaluation.
We open-source MLR-Bench to help the community benchmark, diagnose, and improve
AI research agents toward trustworthy and transparent scientific discovery.

</details>


### [629] [The Limits of Preference Data for Post-Training](https://arxiv.org/abs/2505.19964)
*Eric Zhao,Jessica Dai,Pranjal Awasthi*

Main category: cs.LG

TL;DR: 该论文探讨了在需要人类反馈的任务中，使用偏好数据进行强化学习的局限性，并提出需要改进评分方法和算法创新。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索如何将强化学习应用于需要人类反馈的任务，如深度研究和旅行规划，这些任务的评估是定性的且存在多种成功程度。

Method: 通过投票理论形式化偏好数据的局限性，类比模型选择答案与选民选择候选人的过程，分析偏好数据对优化结果的影响。

Result: 研究发现，即使使用理想的偏好数据，序数反馈也可能阻碍获得近似最优解，尤其是在需要推理行为的任务中。

Conclusion: 结论指出，基于人类评分的改进和算法创新是必要的，以扩展强化学习在需要人类反馈的任务中的成功应用。

Abstract: Recent progress in strengthening the capabilities of large language models
has stemmed from applying reinforcement learning to domains with automatically
verifiable outcomes. A key question is whether we can similarly use RL to
optimize for outcomes in domains where evaluating outcomes inherently requires
human feedback; for example, in tasks like deep research and trip planning,
outcome evaluation is qualitative and there are many possible degrees of
success. One attractive and scalable modality for collecting human feedback is
preference data: ordinal rankings (pairwise or $k$-wise) that indicate, for $k$
given outcomes, which one is preferred. In this work, we study a critical
roadblock: preference data fundamentally and significantly limits outcome-based
optimization. Even with idealized preference data (infinite, noiseless, and
online), the use of ordinal feedback can prevent obtaining even approximately
optimal solutions. We formalize this impossibility using voting theory, drawing
an analogy between how a model chooses to answer a query with how voters choose
a candidate to elect. This indicates that grounded human scoring and
algorithmic innovations are necessary for extending the success of RL
post-training to domains demanding human feedback. We also explore why these
limitations have disproportionately impacted RLHF when it comes to eliciting
reasoning behaviors (e.g., backtracking) versus situations where RLHF has been
historically successful (e.g., instruction-tuning and safety training), finding
that the limitations of preference data primarily suppress RLHF's ability to
elicit robust strategies -- a class that encompasses most reasoning behaviors.

</details>


### [630] [Learning to Select In-Context Demonstration Preferred by Large Language Model](https://arxiv.org/abs/2505.19966)
*Zheng Zhang,Shaocheng Lan,Lei Song,Jiang Bian,Yexin Li,Kan Ren*

Main category: cs.LG

TL;DR: GenICL提出了一种基于生成式偏好学习的框架，通过直接优化演示选择来提升上下文学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有的上下文学习(ICL)方法依赖检索选择演示，但常使用替代目标而非直接优化ICL性能，且候选池质量不足时效果不佳。

Method: 提出GenICL框架，利用大语言模型反馈直接优化演示选择，通过生成式偏好学习提升选择效果。

Result: 在19个数据集11类任务上验证，GenICL能选择更有效的演示，显著提升ICL性能。

Conclusion: GenICL通过直接优化演示选择，解决了现有方法的局限性，显著提升了上下文学习的效果。

Abstract: In-context learning (ICL) enables large language models (LLMs) to adapt to
new tasks during inference using only a few demonstrations. However, ICL
performance is highly dependent on the selection of these demonstrations.
Recent work explores retrieval-based methods for selecting query-specific
demonstrations, but these approaches often rely on surrogate objectives such as
metric learning, failing to directly optimize ICL performance. Consequently,
they struggle to identify truly beneficial demonstrations. Moreover, their
discriminative retrieval paradigm is ineffective when the candidate pool lacks
sufficient high-quality demonstrations. To address these challenges, we propose
GenICL, a novel generative preference learning framework that leverages LLM
feedback to directly optimize demonstration selection for ICL. Experiments on
19 datasets across 11 task categories demonstrate that GenICL achieves superior
performance than existing methods in selecting the most effective
demonstrations, leading to better ICL performance.

</details>


### [631] [Differential Privacy Analysis of Decentralized Gossip Averaging under Varying Threat Models](https://arxiv.org/abs/2505.19969)
*Antti Koskela,Tejas Kulkarni*

Main category: cs.LG

TL;DR: 该论文提出了一种新的隐私分析框架，用于完全去中心化的机器学习训练，改进了差分隐私保护的分析方法，显著降低了隐私泄露风险。


<details>
  <summary>Details</summary>
Motivation: 完全去中心化的机器学习训练在扩展性、鲁棒性和容错性方面具有优势，但缺乏中央聚合器和节点间信任假设的差异使得实现差分隐私（DP）具有挑战性。

Method: 论文提出了一种基于线性系统公式的新分析框架，用于分析去中心化gossip平均算法在添加节点级噪声时的隐私泄露情况，支持有/无安全求和的情况。

Result: 新框架显著改进了现有分析，将Rényi DP参数的增长从O(T²)降至O(T)，并通过MNIST图像分类的逻辑回归实验验证了其优越性。

Conclusion: 该分析框架在完全去中心化环境中实现了与中央聚合方法相当的实用性，同时提供了更强的隐私保护。

Abstract: Fully decentralized training of machine learning models offers significant
advantages in scalability, robustness, and fault tolerance. However, achieving
differential privacy (DP) in such settings is challenging due to the absence of
a central aggregator and varying trust assumptions among nodes. In this work,
we present a novel privacy analysis of decentralized gossip-based averaging
algorithms with additive node-level noise, both with and without secure
summation over each node's direct neighbors. Our main contribution is a new
analytical framework based on a linear systems formulation that accurately
characterizes privacy leakage across these scenarios. This framework
significantly improves upon prior analyses, for example, reducing the R\'enyi
DP parameter growth from $O(T^2)$ to $O(T)$, where $T$ is the number of
training rounds. We validate our analysis with numerical results demonstrating
superior DP bounds compared to existing approaches. We further illustrate our
analysis with a logistic regression experiment on MNIST image classification in
a fully decentralized setting, demonstrating utility comparable to central
aggregation methods.

</details>


### [632] [Rethinking Probabilistic Circuit Parameter Learning](https://arxiv.org/abs/2505.19982)
*Anji Liu,Guy Van den Broeck*

Main category: cs.LG

TL;DR: 该论文通过建立EM目标与全批次EM算法的新联系，提出了理论支持的小批次EM优化方法，解决了概率电路在大数据集上训练效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 概率电路（PCs）在生成建模中具有计算可扩展性，支持多种概率查询的高效推断。然而，现有优化方法如全批次EM在大数据集上效率低下，且小批次扩展方法缺乏理论依据。

Method: 论文通过理论分析，建立了EM目标与全批次EM算法的联系，并基于此推导出理论支持的小批次EM优化方法。

Result: 初步实验结果表明，提出的小批次EM方法在训练效率和理论合理性上均优于现有方法。

Conclusion: 该研究为概率电路的高效训练提供了理论支持的小批次优化方法，填补了现有方法的理论空白。

Abstract: Probabilistic Circuits (PCs) offer a computationally scalable framework for
generative modeling, supporting exact and efficient inference of a wide range
of probabilistic queries. While recent advances have significantly improved the
expressiveness and scalability of PCs, effectively training their parameters
remains a challenge. In particular, a widely used optimization method,
full-batch Expectation-Maximization (EM), requires processing the entire
dataset before performing a single update, making it ineffective for large
datasets. While empirical extensions to the mini-batch setting have been
proposed, it remains unclear what objective these algorithms are optimizing,
making it difficult to assess their theoretical soundness. This paper bridges
the gap by establishing a novel connection between the general EM objective and
the standard full-batch EM algorithm. Building on this, we derive a
theoretically grounded generalization to the mini-batch setting and demonstrate
its effectiveness through preliminary empirical results.

</details>


### [633] [Regret Analysis of Average-Reward Unichain MDPs via an Actor-Critic Approach](https://arxiv.org/abs/2505.19986)
*Swetha Ganesh,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Actor-Critic methods are widely used for their scalability, yet existing
theoretical guarantees for infinite-horizon average-reward Markov Decision
Processes (MDPs) often rely on restrictive ergodicity assumptions. We propose
NAC-B, a Natural Actor-Critic with Batching, that achieves order-optimal regret
of $\tilde{O}(\sqrt{T})$ in infinite-horizon average-reward MDPs under the
unichain assumption, which permits both transient states and periodicity. This
assumption is among the weakest under which the classic policy gradient theorem
remains valid for average-reward settings. NAC-B employs function approximation
for both the actor and the critic, enabling scalability to problems with large
state and action spaces. The use of batching in our algorithm helps mitigate
potential periodicity in the MDP and reduces stochasticity in gradient
estimates, and our analysis formalizes these benefits through the introduction
of the constants $C_{\text{hit}}$ and $C_{\text{tar}}$, which characterize the
rate at which empirical averages over Markovian samples converge to the
stationary distribution.

</details>


### [634] [Training Acceleration of Low-Rank Decomposed Networks using Sequential Freezing and Rank Quantization](https://arxiv.org/abs/2309.03824)
*Habib Hajimolahoseini,Walid Ahmed,Yang Liu*

Main category: cs.LG

TL;DR: 本文提出两种技术加速低秩分解模型，不依赖小秩分解，结合后可提升训练吞吐量60%、推理37%，同时保持精度接近原始模型。


<details>
  <summary>Details</summary>
Motivation: 低秩分解(LRD)虽能减少参数量和计算复杂度，但分解秩过小会导致精度显著下降，而秩过大则无法有效加速训练/推理。

Method: 提出秩优化和分解层顺序冻结两种技术，在卷积和Transformer模型上验证。

Result: 实验表明，联合使用两种技术时，训练吞吐量提升60%，推理速度提升37%，且精度接近原始模型。

Conclusion: 所提方法在避免小秩分解的前提下，显著加速低秩分解模型的训练和推理，同时保持模型精度。

Abstract: Low Rank Decomposition (LRD) is a model compression technique applied to the
weight tensors of deep learning models in order to reduce the number of
trainable parameters and computational complexity. However, due to high number
of new layers added to the architecture after applying LRD, it may not lead to
a high training/inference acceleration if the decomposition ranks are not small
enough. The issue is that using small ranks increases the risk of significant
accuracy drop after decomposition. In this paper, we propose two techniques for
accelerating low rank decomposed models without requiring to use small ranks
for decomposition. These methods include rank optimization and sequential
freezing of decomposed layers. We perform experiments on both convolutional and
transformer-based models. Experiments show that these techniques can improve
the model throughput up to 60% during training and 37% during inference when
combined together while preserving the accuracy close to that of the original
models

</details>


### [635] [Learning Optimal Multimodal Information Bottleneck Representations](https://arxiv.org/abs/2505.19996)
*Qilong Wu,Yiyang Shao,Jun Wang,Xiaobo Sun*

Main category: cs.LG

TL;DR: 提出OMIB框架，通过理论保证的正则化权重边界和动态调整机制，解决多模态学习中信息不平衡问题，实现最优多模态信息瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有基于多模态信息瓶颈(MIB)的方法存在正则化权重随意设置和模态间任务相关信息不平衡的问题，限制了最优MIB的实现。

Method: 提出OMIB框架：1) 理论推导正则化权重边界保证最优MIB；2) 动态调整各模态正则化权重解决信息不平衡；3) 在变分近似框架下实现计算高效性。

Result: 在合成数据上验证理论性质，并在多个下游任务中优于当前最先进方法。

Conclusion: OMIB通过理论驱动的正则化机制和动态权重调整，有效解决了多模态学习中的关键挑战，实现了性能提升。

Abstract: Leveraging high-quality joint representations from multimodal data can
greatly enhance model performance in various machine-learning based
applications. Recent multimodal learning methods, based on the multimodal
information bottleneck (MIB) principle, aim to generate optimal MIB with
maximal task-relevant information and minimal superfluous information via
regularization. However, these methods often set ad hoc regularization weights
and overlook imbalanced task-relevant information across modalities, limiting
their ability to achieve optimal MIB. To address this gap, we propose a novel
multimodal learning framework, Optimal Multimodal Information Bottleneck
(OMIB), whose optimization objective guarantees the achievability of optimal
MIB by setting the regularization weight within a theoretically derived bound.
OMIB further addresses imbalanced task-relevant information by dynamically
adjusting regularization weights per modality, promoting the inclusion of all
task-relevant information. Moreover, we establish a solid
information-theoretical foundation for OMIB's optimization and implement it
under the variational approximation framework for computational efficiency.
Finally, we empirically validate the OMIB's theoretical properties on synthetic
data and demonstrate its superiority over the state-of-the-art benchmark
methods in various downstream tasks.

</details>


### [636] [Improving Resnet-9 Generalization Trained on Small Datasets](https://arxiv.org/abs/2309.03965)
*Omar Mohamed Awad,Habib Hajimolahoseini,Michael Lim,Gurpreet Gosal,Walid Ahmed,Yang Liu,Gordon Deng*

Main category: cs.LG

TL;DR: 本文提出了一种在ICLR硬件感知高效训练竞赛中获得一等奖的方法，旨在10分钟内用CIFAR-10的5000张图片子集训练ResNet-9达到最高准确率。


<details>
  <summary>Details</summary>
Motivation: 竞赛目标是探索在有限计算资源和时间内，如何高效训练模型达到高准确率，这对实际应用中的快速模型部署具有重要意义。

Method: 采用多种技术提升ResNet-9的泛化能力，包括锐度感知优化、标签平滑、梯度中心化、输入块白化及基于元学习的训练策略。

Result: 在CIFAR-10的10%子集上，ResNet-9在10分钟内训练达到了88%的准确率。

Conclusion: 通过组合多种优化技术，可以在极短时间和有限数据下高效训练出高精度模型，验证了方法在资源受限场景下的实用性。

Abstract: This paper presents our proposed approach that won the first prize at the
ICLR competition on Hardware Aware Efficient Training. The challenge is to
achieve the highest possible accuracy in an image classification task in less
than 10 minutes. The training is done on a small dataset of 5000 images picked
randomly from CIFAR-10 dataset. The evaluation is performed by the competition
organizers on a secret dataset with 1000 images of the same size. Our approach
includes applying a series of technique for improving the generalization of
ResNet-9 including: sharpness aware optimization, label smoothing, gradient
centralization, input patch whitening as well as metalearning based training.
Our experiments show that the ResNet-9 can achieve the accuracy of 88% while
trained only on a 10% subset of CIFAR-10 dataset in less than 10 minuets

</details>


### [637] [Embracing Imperfection: Simulating Students with Diverse Cognitive Levels Using LLM-based Agents](https://arxiv.org/abs/2505.19997)
*Tao Wu,Jingyuan Chen,Wang Lin,Mengze Li,Yumeng Zhu,Ang Li,Kun Kuang,Fei Wu*

Main category: cs.LG

TL;DR: 提出无训练框架模拟学生行为，通过认知原型和波束搜索提升仿真准确率100%。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型作为‘完美助手’难以模拟学生认知多样性，导致仿真不真实。

Method: 基于知识图谱构建认知原型，映射新任务预测表现，波束搜索迭代生成含错误的解决方案。

Result: 在含100名学生、5000条记录的Python学习数据集上，仿真准确率提升100%。

Conclusion: 该框架有效解决LLM模拟学生认知局限问题，显著提升教育仿真真实性。

Abstract: Large language models (LLMs) are revolutionizing education, with LLM-based
agents playing a key role in simulating student behavior. A major challenge in
student simulation is modeling the diverse learning patterns of students at
various cognitive levels. However, current LLMs, typically trained as ``helpful
assistants'', target at generating perfect responses. As a result, they
struggle to simulate students with diverse cognitive abilities, as they often
produce overly advanced answers, missing the natural imperfections that
characterize student learning and resulting in unrealistic simulations. To
address this issue, we propose a training-free framework for student
simulation. We begin by constructing a cognitive prototype for each student
using a knowledge graph, which captures their understanding of concepts from
past learning records. This prototype is then mapped to new tasks to predict
student performance. Next, we simulate student solutions based on these
predictions and iteratively refine them using a beam search method to better
replicate realistic mistakes. To validate our approach, we construct the
\texttt{Student\_100} dataset, consisting of $100$ students working on Python
programming and $5,000$ learning records. Experimental results show that our
method consistently outperforms baseline models, achieving $100\%$ improvement
in simulation accuracy.

</details>


### [638] [GQKVA: Efficient Pre-training of Transformers by Grouping Queries, Keys, and Values](https://arxiv.org/abs/2311.03426)
*Farnoosh Javadi,Walid Ahmed,Habib Hajimolahoseini,Foozhan Ataiefard,Mohammad Hassanpour,Saina Asani,Austin Wen,Omar Mohamed Awad,Kangling Liu,Yang Liu*

Main category: cs.LG

TL;DR: 本文提出GQKVA方法，通过泛化查询、键和值分组技术，加速Transformer预训练并减小模型规模，实验显示在图像分类任务中模型大小减少4%的同时准确率提升0.3%。


<details>
  <summary>Details</summary>
Motivation: 大规模Transformer模型面临预训练速度慢、计算资源消耗大及参数过多的问题，需要一种既能加速预训练又能减小模型规模的方法。

Method: 提出GQKVA方法，通过泛化查询、键和值的分组技术，优化模型结构，实现预训练加速和模型压缩。

Result: 实验表明，GQKVA在ViT上实现了模型大小减少4%且准确率提升0.3%；最激进的模型压缩实验中，模型大小减少约15%，准确率仅下降1%。

Conclusion: 传统多头注意力机制并非总是最佳选择，GQKVA提供了更轻量、更快速的替代方案，可根据资源和时间限制灵活选择模型配置。

Abstract: Massive transformer-based models face several challenges, including slow and
computationally intensive pre-training and over-parametrization. This paper
addresses these challenges by proposing a versatile method called GQKVA, which
generalizes query, key, and value grouping techniques. GQKVA is designed to
speed up transformer pre-training while reducing the model size. Our
experiments with various GQKVA variants highlight a clear trade-off between
performance and model size, allowing for customized choices based on resource
and time limitations. Our findings also indicate that the conventional
multi-head attention approach is not always the best choice, as there are
lighter and faster alternatives available. We tested our method on ViT, which
achieved an approximate 0.3% increase in accuracy while reducing the model size
by about 4% in the task of image classification. Additionally, our most
aggressive model reduction experiment resulted in a reduction of approximately
15% in model size, with only around a 1% drop in accuracy.

</details>


### [639] [TabPFN: One Model to Rule Them All?](https://arxiv.org/abs/2505.20003)
*Qiong Zhang,Yan Shuo Tan,Qinglong Tian,Pengfei Li*

Main category: cs.LG

TL;DR: TabPFN是一种基于Transformer的深度学习模型，用于表格数据的回归和分类任务，声称在不超过10,000样本的数据集上大幅超越现有方法，并具备基础模型的多功能潜力。本文通过贝叶斯推断视角解释其原理，并验证其在半监督参数估计、协变量偏移预测等任务中的卓越表现。


<details>
  <summary>Details</summary>
Motivation: 验证TabPFN作为表格数据基础模型的潜力，并为其统计性能提供理论解释（近似贝叶斯推断），同时探索其在多种统计任务中的扩展能力。

Method: 1. 将TabPFN解释为近似贝叶斯推断模型 2. 在多种任务（半监督学习、协变量偏移、异质处理效应等）中对比TabPFN与SOTA方法 3. 提供可复现代码

Result: TabPFN在以下任务中显著优于专业方法：半监督参数估计（提升显著）、协变量偏移预测、异质处理效应估计、稀疏回归（超越LASSO）、分类任务打破鲁棒性-效率权衡。

Conclusion: TabPFN展现出作为表格数据基础模型的通用性和强大性能，可能引领类似NLP中大规模预训练模型的变革。其近似贝叶斯特性为统计解释提供了新视角。

Abstract: Hollmann et al. (Nature 637 (2025) 319-326) recently introduced TabPFN, a
transformer-based deep learning model for regression and classification on
tabular data, which they claim "outperforms all previous methods on datasets
with up to 10,000 samples by a wide margin, using substantially less training
time." Furthermore, they have called TabPFN a "foundation model" for tabular
data, as it can support "data generation, density estimation, learning reusable
embeddings and fine-tuning". If these statements are well-supported, TabPFN may
have the potential to supersede existing modeling approaches on a wide range of
statistical tasks, mirroring a similar revolution in other areas of artificial
intelligence that began with the advent of large language models. In this
paper, we provide a tailored explanation of how TabPFN works for a statistics
audience, by emphasizing its interpretation as approximate Bayesian inference.
We also provide more evidence of TabPFN's "foundation model" capabilities: We
show that an out-of-the-box application of TabPFN vastly outperforms
specialized state-of-the-art methods for semi-supervised parameter estimation,
prediction under covariate shift, and heterogeneous treatment effect
estimation. We further show that TabPFN can outperform LASSO at sparse
regression and can break a robustness-efficiency trade-off in classification.
All experiments can be reproduced using the code provided at
https://github.com/qinglong-tian/tabpfn_study
(https://github.com/qinglong-tian/tabpfn_study).

</details>


### [640] [Data-Dependent Regret Bounds for Constrained MABs](https://arxiv.org/abs/2505.20010)
*Gianmarco Genalti,Francesco Emanuele Stradi,Matteo Castiglioni,Alberto Marchesi,Nicola Gatti*

Main category: cs.LG

TL;DR: 本文首次研究了约束MAB设置中数据依赖的遗憾界，提出了一种算法，其遗憾界由两个数据依赖项组成，并证明了其下界。


<details>
  <summary>Details</summary>
Motivation: 在约束多臂老虎机（MAB）设置中，数据依赖的遗憾界尚未被探索。传统的遗憾界通常为√T，而数据依赖的遗憾界可以根据问题实例的损失序列进行调整，从而在某些情况下显著优于传统界限。本文旨在填补这一空白，探讨在存在约束的情况下是否能够导出数据依赖的遗憾界。

Method: 本文设计了一种算法，适用于具有对抗性损失和随机约束的约束MAB问题。算法特别关注具有硬约束的最具挑战性和自然性的设置，其中学习者必须确保约束在大概率下始终满足。算法的遗憾界由两个数据依赖项组成：第一个项捕捉满足约束的难度，第二个项编码独立于约束的学习复杂性。

Result: 本文证明了在对抗性损失和随机约束的约束MAB设置中，可以导出数据依赖的遗憾界。算法的遗憾界由两个数据依赖项组成，并通过下界证明这两个项是问题的固有组成部分。此外，在软约束设置中，本文还得出了一些新的结果。

Conclusion: 本文首次在约束MAB设置中导出了数据依赖的遗憾界，并通过算法设计和下界证明展示了其有效性。这一结果为约束MAB问题的研究提供了新的视角，并在软约束设置中提出了新的结果。

Abstract: This paper initiates the study of data-dependent regret bounds in constrained
MAB settings. These bounds depend on the sequence of losses that characterize
the problem instance. Thus, they can be much smaller than classical
$\widetilde{\mathcal{O}}(\sqrt{T})$ regret bounds, while being equivalent to
them in the worst case. Despite this, data-dependent regret bounds have been
completely overlooked in constrained MAB settings. The goal of this paper is to
answer the following question: Can data-dependent regret bounds be derived in
the presence of constraints? We answer this question affirmatively in
constrained MABs with adversarial losses and stochastic constraints.
Specifically, our main focus is on the most challenging and natural settings
with hard constraints, where the learner must ensure that the constraints are
always satisfied with high probability. We design an algorithm with a regret
bound consisting of two data-dependent terms. The first term captures the
difficulty of satisfying the constraints, while the second one encodes the
complexity of learning independently of the presence of constraints. We also
prove a lower bound showing that these two terms are not artifacts of our
specific approach and analysis, but rather the fundamental components that
inherently characterize the complexities of the problem. Finally, in designing
our algorithm, we also derive some novel results in the related (and easier)
soft constraints settings, which may be of independent interest.

</details>


### [641] [Accelerating the Low-Rank Decomposed Models](https://arxiv.org/abs/2407.20266)
*Habib Hajimolahoseini,Walid Ahmed,Austin Wen,Yang Liu*

Main category: cs.LG

TL;DR: 该论文探讨了如何改进低秩分解技术以在AI模型中实现高准确率、低内存消耗并加速训练和推理。


<details>
  <summary>Details</summary>
Motivation: 张量分解虽能有效压缩数据，但在AI模型中应用不广泛，因其会增加模型深度，导致训练和推理延迟。

Method: 研究如何修改低秩分解技术，以减少冗余数据同时不显著增加模型深度。

Result: 提出了一种改进方法，能在保持高准确率的同时降低内存使用并加速训练和推理。

Conclusion: 改进的低秩分解技术可有效平衡模型压缩与性能，适用于AI模型优化。

Abstract: Tensor decomposition is a mathematically supported technique for data
compression. It consists of applying some kind of a Low Rank Decomposition
technique on the tensors or matrices in order to reduce the redundancy of the
data. However, it is not a popular technique for compressing the AI models duo
to the high number of new layers added to the architecture after decomposition.
Although the number of parameters could shrink significantly, it could result
in the model be more than twice deeper which could add some latency to the
training or inference. In this paper, we present a comprehensive study about
how to modify low rank decomposition technique in AI models so that we could
benefit from both high accuracy and low memory consumption as well as speeding
up the training and inference

</details>


### [642] [Ontology- and LLM-based Data Harmonization for Federated Learning in Healthcare](https://arxiv.org/abs/2505.20020)
*Natallia Kokash,Lei Wang,Thomas H. Gillespie,Adam Belloum,Paola Grosso,Sara Quinney,Lang Li,Bernard de Bono*

Main category: cs.LG

TL;DR: 论文提出了一种结合本体论和大语言模型的两步数据对齐策略，以支持医疗领域安全、隐私保护的联邦学习。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHRs）的兴起为医学研究提供了新机遇，但隐私法规和数据异质性是大规模机器学习的主要障碍。联邦学习（FL）虽能实现不共享原始数据的协作建模，但在协调多样化的临床数据集方面仍面临挑战。

Method: 采用两步数据对齐策略，整合本体论和大语言模型（LLMs），以实现医疗数据的安全、隐私保护联邦学习。

Result: 该方法在一个涉及EHR数据语义映射的实际项目中证明了其有效性。

Conclusion: 提出的两步数据对齐策略能够有效支持医疗领域的联邦学习，解决数据隐私和异质性带来的挑战。

Abstract: The rise of electronic health records (EHRs) has unlocked new opportunities
for medical research, but privacy regulations and data heterogeneity remain key
barriers to large-scale machine learning. Federated learning (FL) enables
collaborative modeling without sharing raw data, yet faces challenges in
harmonizing diverse clinical datasets. This paper presents a two-step data
alignment strategy integrating ontologies and large language models (LLMs) to
support secure, privacy-preserving FL in healthcare, demonstrating its
effectiveness in a real-world project involving semantic mapping of EHR data.

</details>


### [643] [Gradient Inversion Transcript: Leveraging Robust Generative Priors to Reconstruct Training Data from Gradient Leakage](https://arxiv.org/abs/2505.20026)
*Xinping Chen,Chen Liu*

Main category: cs.LG

TL;DR: GIT是一种新型生成方法，通过泄露的梯度重建训练数据，优于现有方法且具有强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 针对分布式学习中梯度泄露导致的数据隐私问题，提出一种高效且鲁棒的重建方法。

Method: 使用生成攻击模型，其架构基于理论分析与泄露模型对齐，离线训练后仅需梯度即可重建数据。

Result: GIT在多种数据集上表现优异，加速收敛并提升重建质量，对梯度不准确等挑战条件具有鲁棒性。

Conclusion: GIT为梯度泄露场景提供了一种高效、通用的数据重建解决方案。

Abstract: We propose Gradient Inversion Transcript (GIT), a novel generative approach
for reconstructing training data from leaked gradients. GIT employs a
generative attack model, whose architecture is tailored to align with the
structure of the leaked model based on theoretical analysis. Once trained
offline, GIT can be deployed efficiently and only relies on the leaked
gradients to reconstruct the input data, rendering it applicable under various
distributed learning environments. When used as a prior for other iterative
optimization-based methods, GIT not only accelerates convergence but also
enhances the overall reconstruction quality. GIT consistently outperforms
existing methods across multiple datasets and demonstrates strong robustness
under challenging conditions, including inaccurate gradients, data distribution
shifts and discrepancies in model parameters.

</details>


### [644] [Multiple Descents in Deep Learning as a Sequence of Order-Chaos Transitions](https://arxiv.org/abs/2505.20030)
*Wenbo Wei,Nicholas Chong Jia Le,Choy Heng Lai,Ling Feng*

Main category: cs.LG

TL;DR: 研究发现LSTM训练过程中存在'多次下降'现象，测试损失在过拟合后经历多次周期性波动，与有序和混沌之间的相变过程密切相关。


<details>
  <summary>Details</summary>
Motivation: 探索LSTM训练过程中测试损失的周期性波动现象，及其与模型稳定性和学习能力的关系。

Method: 通过渐近稳定性分析模型，研究测试损失波动与有序和混沌相变过程之间的关联。

Result: 发现局部最优时期出现在有序和混沌的临界转换点，而全局最优时期出现在首次从有序到混沌的转换点。

Conclusion: 全局最优时期出现在'混沌边缘'最宽的时刻，此时能够最好地探索更优的权重配置。

Abstract: We observe a novel 'multiple-descent' phenomenon during the training process
of LSTM, in which the test loss goes through long cycles of up and down trend
multiple times after the model is overtrained. By carrying out asymptotic
stability analysis of the models, we found that the cycles in test loss are
closely associated with the phase transition process between order and chaos,
and the local optimal epochs are consistently at the critical transition point
between the two phases. More importantly, the global optimal epoch occurs at
the first transition from order to chaos, where the 'width' of the 'edge of
chaos' is the widest, allowing the best exploration of better weight
configurations for learning.

</details>


### [645] [Graph Wave Networks](https://arxiv.org/abs/2505.20034)
*Juwei Yue,Haikuo Li,Jiawei Sheng,Yihan Guo,Xinghua Zhang,Chuan Zhou,Tingwen Liu,Li Guo*

Main category: cs.LG

TL;DR: 该论文提出了一种基于波动方程的图神经网络消息传递新范式，解决了传统热扩散模型无法捕捉图信号波动特性及数值不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法将消息传递建模为热扩散过程，但热方程难以描述图信号的波动本质，且一阶时间导数导致数值解稳定性差、训练效率低。

Method: 基于物理波动方程创新提出图波动方程，建立与谱GNN的理论联系，利用二阶时间导数增强稳定性，并设计出图波动网络（GWN）。

Result: 实验证明GWN在基准数据集上达到SOTA性能，有效解决过平滑和异配性难题，数值解具有恒定稳定性且训练效率显著提升。

Conclusion: 图波动方程为消息传递提供了更符合图信号本质的建模框架，在性能、稳定性和效率方面均超越传统热扩散方法。

Abstract: Dynamics modeling has been introduced as a novel paradigm in message passing
(MP) of graph neural networks (GNNs). Existing methods consider MP between
nodes as a heat diffusion process, and leverage heat equation to model the
temporal evolution of nodes in the embedding space. However, heat equation can
hardly depict the wave nature of graph signals in graph signal processing.
Besides, heat equation is essentially a partial differential equation (PDE)
involving a first partial derivative of time, whose numerical solution usually
has low stability, and leads to inefficient model training. In this paper, we
would like to depict more wave details in MP, since graph signals are
essentially wave signals that can be seen as a superposition of a series of
waves in the form of eigenvector. This motivates us to consider MP as a wave
propagation process to capture the temporal evolution of wave signals in the
space. Based on wave equation in physics, we innovatively develop a graph wave
equation to leverage the wave propagation on graphs. In details, we demonstrate
that the graph wave equation can be connected to traditional spectral GNNs,
facilitating the design of graph wave networks based on various Laplacians and
enhancing the performance of the spectral GNNs. Besides, the graph wave
equation is particularly a PDE involving a second partial derivative of time,
which has stronger stability on graphs than the heat equation that involves a
first partial derivative of time. Additionally, we theoretically prove that the
numerical solution derived from the graph wave equation are constantly stable,
enabling to significantly enhance model efficiency while ensuring its
performance. Extensive experiments show that GWNs achieve SOTA and efficient
performance on benchmark datasets, and exhibit outstanding performance in
addressing challenging graph problems, such as over-smoothing and heterophily.

</details>


### [646] [Beyond Simple Concatenation: Fairly Assessing PLM Architectures for Multi-Chain Protein-Protein Interactions Prediction](https://arxiv.org/abs/2505.20036)
*Hazem Alsamkary,Mohamed Elshaffei,Mohamed Soudy,Sara Ossman,Abdallah Amr,Nehal Adel Abdelsalam,Mohamed Elkerdawy,Ahmed Elnaggar*

Main category: cs.LG

TL;DR: 该论文针对蛋白质语言模型(PLMs)在蛋白质相互作用(PPI)结合亲和力预测中的应用不足问题，提出了一个高质量的数据集和四种架构设计，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 蛋白质相互作用(PPIs)对理解疾病机制和药物发现至关重要，但现有PLMs在基于序列的PPI结合亲和力预测方面研究不足，主要受限于高质量数据集缺乏和简单表示拼接策略。

Method: 1) 精心构建了PPB-Affinity数据集(8,207个条目)，解决注释不一致问题并设置≤30%序列同一性阈值；2) 提出并系统评估了四种PLMs适配架构(EC、SC、HP、PAD)，采用全微调和轻量级ConvBERT头两种训练方法。

Result: 在ProtT5/ESM2/Ankh等PLMs上的实验表明，HP和PAD架构比传统拼接方法性能提升高达12%(Spearman相关系数)。

Conclusion: 研究表明，需要复杂的架构设计才能充分发挥PLMs在PPI结合亲和力预测中的潜力，HP和PAD架构展现出显著优势。

Abstract: Protein-protein interactions (PPIs) are fundamental to numerous cellular
processes, and their characterization is vital for understanding disease
mechanisms and guiding drug discovery. While protein language models (PLMs)
have demonstrated remarkable success in predicting protein structure and
function, their application to sequence-based PPI binding affinity prediction
remains relatively underexplored. This gap is often attributed to the scarcity
of high-quality, rigorously refined datasets and the reliance on simple
strategies for concatenating protein representations. In this work, we address
these limitations. First, we introduce a meticulously curated version of the
PPB-Affinity dataset of a total of 8,207 unique protein-protein interaction
entries, by resolving annotation inconsistencies and duplicate entries for
multi-chain protein interactions. This dataset incorporates a stringent, less
than or equal to 30%, sequence identity threshold to ensure robust splitting
into training, validation, and test sets, minimizing data leakage. Second, we
propose and systematically evaluate four architectures for adapting PLMs to PPI
binding affinity prediction: embeddings concatenation (EC), sequences
concatenation (SC), hierarchical pooling (HP), and pooled attention addition
(PAD). These architectures were assessed using two training methods: full
fine-tuning and a lightweight approach employing ConvBERT heads over frozen PLM
features. Our comprehensive experiments across multiple leading PLMs (ProtT5,
ESM2, Ankh, Ankh2, and ESM3) demonstrated that the HP and PAD architectures
consistently outperform conventional concatenation methods, achieving up to 12%
increase in terms of Spearman correlation. These results highlight the
necessity of sophisticated architectural designs to fully exploit the
capabilities of PLMs for nuanced PPI binding affinity prediction.

</details>


### [647] [Synthetic Time Series Forecasting with Transformer Architectures: Extensive Simulation Benchmarks](https://arxiv.org/abs/2505.20048)
*Ali Forootani,Mohammad Khosravi*

Main category: cs.LG

TL;DR: 该论文提出了一个统一的Transformer时间序列预测框架，并引入Koopman增强的Deep Koopformer模型，通过实验验证其在噪声和复杂条件下的优越性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测在能源、金融和医疗等领域至关重要，但现有Transformer模型在噪声敏感性、长期依赖和时间结构归纳偏置方面存在局限，需要更稳健的解决方案。

Method: 论文系统评估了Autoformer、Informer和Patchtst三种Transformer架构的变体，并提出结合Koopman算子理论的Deep Koopformer框架，以提升稳定性和可解释性。

Result: 在1500多个实验中，新框架在合成信号和混沌系统上表现出色，证实了Koopman-Transformer混合方法在复杂现实场景中的有效性。

Conclusion: 基于Koopman的Transformer为噪声环境下的时间序列预测提供了理论支撑强、可解释性高的新范式，具有实际应用潜力。

Abstract: Time series forecasting plays a critical role in domains such as energy,
finance, and healthcare, where accurate predictions inform decision-making
under uncertainty. Although Transformer-based models have demonstrated success
in sequential modeling, their adoption for time series remains limited by
challenges such as noise sensitivity, long-range dependencies, and a lack of
inductive bias for temporal structure. In this work, we present a unified and
principled framework for benchmarking three prominent Transformer forecasting
architectures-Autoformer, Informer, and Patchtst-each evaluated through three
architectural variants: Minimal, Standard, and Full, representing increasing
levels of complexity and modeling capacity.
  We conduct over 1500 controlled experiments on a suite of ten synthetic
signals, spanning five patch lengths and five forecast horizons under both
clean and noisy conditions. Our analysis reveals consistent patterns across
model families.
  To advance this landscape further, we introduce the Koopman-enhanced
Transformer framework, Deep Koopformer, which integrates operator-theoretic
latent state modeling to improve stability and interpretability. We demonstrate
its efficacy on nonlinear and chaotic dynamical systems. Our results highlight
Koopman based Transformer as a promising hybrid approach for robust,
interpretable, and theoretically grounded time series forecasting in noisy and
complex real-world conditions.

</details>


### [648] [Catoni-Style Change Point Detection for Regret Minimization in Non-Stationary Heavy-Tailed Bandits](https://arxiv.org/abs/2505.20051)
*Gianmarco Genalti,Sujay Bhatt,Nicola Gatti,Alberto Maria Metelli*

Main category: cs.LG

TL;DR: 该论文针对具有重尾分布的分段平稳多臂老虎机问题，提出了一种结合Catoni风格变化点检测策略的Robust-CPD-UCB算法，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的许多问题（如金融和电信领域）涉及重尾分布，而现有文献大多假设奖励生成过程为伯努利或次高斯分布。因此，研究如何在重尾分布下最小化遗憾具有重要实际意义。

Method: 论文提出了一种新颖的Catoni风格变化点检测策略，专为重尾分布设计，并结合了乐观算法（UCB）来处理分段平稳环境中的非平稳性。

Result: 论文提供了Robust-CPD-UCB算法的遗憾上界，并证明了任何策略可达到的最小遗憾下限。通过合成数据和真实数据实验验证了方法的有效性。

Conclusion: 该研究为处理重尾分段平稳老虎机问题提供了一种有效方法，其变化点检测策略对相关领域也有独立的理论价值。

Abstract: Regret minimization in stochastic non-stationary bandits gained popularity
over the last decade, as it can model a broad class of real-world problems,
from advertising to recommendation systems. Existing literature relies on
various assumptions about the reward-generating process, such as Bernoulli or
subgaussian rewards. However, in settings such as finance and
telecommunications, heavy-tailed distributions naturally arise. In this work,
we tackle the heavy-tailed piecewise-stationary bandit problem. Heavy-tailed
bandits, introduced by Bubeck et al., 2013, operate on the minimal assumption
that the finite absolute centered moments of maximum order $1+\epsilon$ are
uniformly bounded by a constant $v<+\infty$, for some $\epsilon \in (0,1]$. We
focus on the most popular non-stationary bandit setting, i.e., the
piecewise-stationary setting, in which the mean of reward-generating
distributions may change at unknown time steps. We provide a novel Catoni-style
change-point detection strategy tailored for heavy-tailed distributions that
relies on recent advancements in the theory of sequential estimation, which is
of independent interest. We introduce Robust-CPD-UCB, which combines this
change-point detection strategy with optimistic algorithms for bandits,
providing its regret upper bound and an impossibility result on the minimum
attainable regret for any policy. Finally, we validate our approach through
numerical experiments on synthetic and real-world datasets.

</details>


### [649] [Ankh3: Multi-Task Pretraining with Sequence Denoising and Completion Enhances Protein Representations](https://arxiv.org/abs/2505.20052)
*Hazem Alsamkary,Mohamed Elshaffei,Mohamed Elkerdawy,Ahmed Elnaggar*

Main category: cs.LG

TL;DR: 论文提出了一种多任务预训练策略Ankh3，通过联合优化两个目标（掩码语言建模和蛋白质序列完成），提升了蛋白质语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的蛋白质语言模型（PLMs）主要依赖单一预训练任务（如去噪序列），可能限制了其对蛋白质序列信息的全面捕捉能力。

Method: 开发了Ankh3模型，采用多任务预训练策略，联合优化掩码语言建模（使用多种掩码概率）和蛋白质序列完成（仅依赖序列输入）两个目标。

Result: 实验表明，Ankh3在二级结构预测、荧光、GB1适应性和接触预测等下游任务中表现更优，模型对蛋白质性质的理解更全面。

Conclusion: 多任务预训练策略可以仅从蛋白质序列中学习更丰富、泛化性更强的表示，从而提升模型的预测能力。

Abstract: Protein language models (PLMs) have emerged as powerful tools to detect
complex patterns of protein sequences. However, the capability of PLMs to fully
capture information on protein sequences might be limited by focusing on single
pre-training tasks. Although adding data modalities or supervised objectives
can improve the performance of PLMs, pre-training often remains focused on
denoising corrupted sequences. To push the boundaries of PLMs, our research
investigated a multi-task pre-training strategy. We developed Ankh3, a model
jointly optimized on two objectives: masked language modeling with multiple
masking probabilities and protein sequence completion relying only on protein
sequences as input. This multi-task pre-training demonstrated that PLMs can
learn richer and more generalizable representations solely from protein
sequences. The results demonstrated improved performance in downstream tasks,
such as secondary structure prediction, fluorescence, GB1 fitness, and contact
prediction. The integration of multiple tasks gave the model a more
comprehensive understanding of protein properties, leading to more robust and
accurate predictions.

</details>


### [650] [SAEs Are Good for Steering -- If You Select the Right Features](https://arxiv.org/abs/2505.20063)
*Dana Arad,Aaron Mueller,Yonatan Belinkov*

Main category: cs.LG

TL;DR: 该论文提出区分稀疏自编码器(SAE)的输入特征和输出特征，并通过输出分数筛选特征，使无监督方法在模型控制任务中媲美监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅通过激活分析识别SAE特征，但激活无法完全反映特征对模型输出的影响。需要区分仅反映输入模式的输入特征和直接影响输出的输出特征。

Method: 提出输入分数和输出分数来量化特征类型，筛选高输出分数特征用于模型控制(steering)。

Result: 过滤低输出分数特征后，SAE在控制任务中的性能提升2-3倍，达到与监督方法相当的水平。

Conclusion: 通过特征类型区分和筛选，无监督的SAE方法可有效实现模型行为控制，为无监督学习提供了新思路。

Abstract: Sparse Autoencoders (SAEs) have been proposed as an unsupervised approach to
learn a decomposition of a model's latent space. This enables useful
applications such as steering - influencing the output of a model towards a
desired concept - without requiring labeled data. Current methods identify SAE
features to steer by analyzing the input tokens that activate them. However,
recent work has highlighted that activations alone do not fully describe the
effect of a feature on the model's output. In this work, we draw a distinction
between two types of features: input features, which mainly capture patterns in
the model's input, and output features, which have a human-understandable
effect on the model's output. We propose input and output scores to
characterize and locate these types of features, and show that high values for
both scores rarely co-occur in the same features. These findings have practical
implications: after filtering out features with low output scores, we obtain
2-3x improvements when steering with SAEs, making them competitive with
supervised methods.

</details>


### [651] [SafeDPO: A Simple Approach to Direct Preference Optimization with Enhanced Safety](https://arxiv.org/abs/2505.20065)
*Geon-Hyeong Kim,Youngsoo Jang,Yu Jin Kim,Byoungjip Kim,Honglak Lee,Kyunghoon Bae,Moontae Lee*

Main category: cs.LG

TL;DR: SafeDPO是一种新算法，通过单阶段策略学习直接优化安全对齐目标，简化了现有方法，同时提升了LLMs的安全性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的广泛应用，确保其安全性变得至关重要。现有方法通常复杂且需要多个步骤，因此需要一种更简单有效的安全对齐方法。

Method: 提出SafeDPO算法，基于直接偏好优化（DPO），仅需一个额外超参数和少量修改，无需拟合单独的奖励和成本模型或在微调时从语言模型采样。

Result: SafeDPO在安全对齐和人类偏好对齐方面表现优异，与现有最先进算法相比具有竞争力。

Conclusion: SafeDPO提供了一种简单高效的方法来增强LLMs的安全性，同时保持了与人类偏好的一致性。

Abstract: As Large Language Models (LLMs) continue to advance and find applications
across a growing number of fields, ensuring the safety of LLMs has become
increasingly critical. To address safety concerns, recent studies have proposed
integrating safety constraints into Reinforcement Learning from Human Feedback
(RLHF). However, these approaches tend to be complex, as they encompass
complicated procedures in RLHF along with additional steps required by the
safety constraints. Inspired by Direct Preference Optimization (DPO), we
introduce a new algorithm called SafeDPO, which is designed to directly
optimize the safety alignment objective in a single stage of policy learning,
without requiring relaxation. SafeDPO introduces only one additional
hyperparameter to further enhance safety and requires only minor modifications
to standard DPO. As a result, it eliminates the need to fit separate reward and
cost models or to sample from the language model during fine-tuning, while
still enhancing the safety of LLMs. Finally, we demonstrate that SafeDPO
achieves competitive performance compared to state-of-the-art safety alignment
algorithms, both in terms of aligning with human preferences and improving
safety.

</details>


### [652] [An Out-Of-Distribution Membership Inference Attack Approach for Cross-Domain Graph Attacks](https://arxiv.org/abs/2505.20074)
*Jinyan Wang,Liu Yang,Yuecen Wei,Jiaxuan Si,Chenhao Guo,Qingyun Sun,Xianxian Li,Xingcheng Fu*

Main category: cs.LG

TL;DR: 该论文提出了一种新型的跨域图成员推理攻击方法GOOD-MIA，解决了传统攻击方法在现实场景中因数据分布多样性导致的隐私泄露问题。


<details>
  <summary>Details</summary>
Motivation: 随着隐私问题的日益突出，传统成员推理攻击（MIA）假设攻击者能获取相同分布的辅助数据集，这一假设与现实情况越来越不符。因此，论文将现实世界中的分布多样性问题归类为分布外（OOD）问题，旨在实现跨域图攻击。

Method: 论文构建了具有不同分布域的影子子图以模拟现实世界数据的多样性，探索了在外部影响下保持稳定的节点表示，并通过消除冗余信息和提取任务相关关键信息来区分训练数据和未见数据。此外，还通过风险外推优化攻击的域适应性。

Result: 实验结果表明，GOOD-MIA在多个域设计的数据集中表现出卓越的攻击性能。

Conclusion: GOOD-MIA通过OOD设计实现了跨域图攻击，为图神经网络隐私保护提供了新的研究方向。

Abstract: Graph Neural Network-based methods face privacy leakage risks due to the
introduction of topological structures about the targets, which allows
attackers to bypass the target's prior knowledge of the sensitive attributes
and realize membership inference attacks (MIA) by observing and analyzing the
topology distribution. As privacy concerns grow, the assumption of MIA, which
presumes that attackers can obtain an auxiliary dataset with the same
distribution, is increasingly deviating from reality. In this paper, we
categorize the distribution diversity issue in real-world MIA scenarios as an
Out-Of-Distribution (OOD) problem, and propose a novel Graph OOD Membership
Inference Attack (GOOD-MIA) to achieve cross-domain graph attacks.
Specifically, we construct shadow subgraphs with distributions from different
domains to model the diversity of real-world data. We then explore the stable
node representations that remain unchanged under external influences and
consider eliminating redundant information from confounding environments and
extracting task-relevant key information to more clearly distinguish between
the characteristics of training data and unseen data. This OOD-based design
makes cross-domain graph attacks possible. Finally, we perform risk
extrapolation to optimize the attack's domain adaptability during attack
inference to generalize the attack to other domains. Experimental results
demonstrate that GOOD-MIA achieves superior attack performance in datasets
designed for multiple domains.

</details>


### [653] [Grokking ExPLAIND: Unifying Model, Data, and Training Attribution to Study Model Behavior](https://arxiv.org/abs/2505.20076)
*Florian Eichin,Yupei Du,Philipp Mondorf,Barbara Plank,Michael A. Hedderich*

Main category: cs.LG

TL;DR: 本文提出了ExPLAIND框架，统一整合模型组件、数据和训练轨迹的视角，提供理论支持的模型解释方法。


<details>
  <summary>Details</summary>
Motivation: 现有的事后解释方法通常孤立地分析模型的组件、数据或训练轨迹，导致解释缺乏统一视角并可能忽略关键交互作用。

Method: 通过推广梯度路径核方法，将其应用于更现实的训练场景，并从中推导出新的参数和步骤影响力评分。

Result: 实验表明，ExPLAIND能准确复现CNN和Transformer模型，其参数剪枝效果与现有方法相当，并能分析Transformer的Grokking现象。

Conclusion: ExPLAIND提供了一个理论支撑的统一框架，用于解释模型行为和训练动态，特别是在Grokking现象中细化了对最终阶段的理解。

Abstract: Post-hoc interpretability methods typically attribute a model's behavior to
its components, data, or training trajectory in isolation. This leads to
explanations that lack a unified view and may miss key interactions. While
combining existing methods or applying them at different training stages offers
broader insights, these approaches usually lack theoretical support. In this
work, we present ExPLAIND, a unified framework that integrates all three
perspectives. First, we generalize recent work on gradient path kernels, which
reformulate models trained by gradient descent as a kernel machine, to more
realistic training settings. Empirically, we find that both a CNN and a
Transformer model are replicated accurately by this reformulation. Second, we
derive novel parameter- and step-wise influence scores from the kernel feature
maps. We show their effectiveness in parameter pruning that is comparable to
existing methods, reinforcing their value for model component attribution.
Finally, jointly interpreting model components and data over the training
process, we leverage ExPLAIND to analyze a Transformer that exhibits Grokking.
Among other things, our findings support previously proposed stages of
Grokking, while refining the final phase as one of alignment of input
embeddings and final layers around a representation pipeline learned after the
memorization phase. Overall, ExPLAIND provides a theoretically grounded,
unified framework to interpret model behavior and training dynamics.

</details>


### [654] [Spurious Privacy Leakage in Neural Networks](https://arxiv.org/abs/2505.20095)
*Chenxiang Zhang,Jun Pang,Sjouke Mauw*

Main category: cs.LG

TL;DR: 研究发现神经网络在虚假相关偏差下存在隐私泄露风险，且传统方法无法缓解此类隐私差异。


<details>
  <summary>Details</summary>
Motivation: 探讨虚假相关偏差如何加剧神经网络在现实场景中的隐私脆弱性，尤其是在数据有限且存在偏差时。

Method: 通过分析虚假隐私泄露现象，比较不同模型架构在虚假数据下的隐私表现。

Result: 虚假相关组隐私泄露更严重，简化任务中差异更大；传统去偏差方法无法缓解隐私风险。

Conclusion: 隐私差异与记忆化相关，需重新审视模型架构选择对隐私保护的影响。

Abstract: Neural networks are vulnerable to privacy attacks aimed at stealing sensitive
data. The risks can be amplified in a real-world scenario, particularly when
models are trained on limited and biased data. In this work, we investigate the
impact of spurious correlation bias on privacy vulnerability. We introduce
\emph{spurious privacy leakage}, a phenomenon where spurious groups are
significantly more vulnerable to privacy attacks than non-spurious groups. We
further show that group privacy disparity increases in tasks with simpler
objectives (e.g. fewer classes) due to the persistence of spurious features.
Surprisingly, we find that reducing spurious correlation using spurious robust
methods does not mitigate spurious privacy leakage. This leads us to introduce
a perspective on privacy disparity based on memorization, where mitigating
spurious correlation does not mitigate the memorization of spurious data, and
therefore, neither the privacy level. Lastly, we compare the privacy of
different model architectures trained with spurious data, demonstrating that,
contrary to prior works, architectural choice can affect privacy outcomes.

</details>


### [655] [Transformer in Protein: A Survey](https://arxiv.org/abs/2505.20098)
*Xiaowen Ling,Zhiqiang Li,Yanbin Wang,Zhuhong You*

Main category: cs.LG

TL;DR: 本文综述了Transformer模型在蛋白质信息学中的应用，系统分析了其在结构预测、功能预测、相互作用分析等领域的进展，并提供了数据集和开源资源以促进研究。


<details>
  <summary>Details</summary>
Motivation: 随着蛋白质信息学的快速发展，对预测准确性、结构分析和功能理解的需求日益增长。Transformer模型作为强大的深度学习架构，在蛋白质研究中展现出巨大潜力，但目前缺乏全面的综述。

Method: 本文通过分析100多项研究，系统梳理了Transformer在蛋白质相关任务中的应用，包括结构预测、功能预测、相互作用分析等，并提供了分类系统和关键数据集。

Result: 综述涵盖了Transformer在蛋白质信息学中的多个关键领域，总结了其贡献和局限性，并整理了重要的数据集和开源代码资源。

Conclusion: 本文为Transformer与蛋白质信息学的结合提供了系统的基础，提出了未来研究方向，旨在推动该领域的进一步创新和应用扩展。

Abstract: As protein informatics advances rapidly, the demand for enhanced predictive
accuracy, structural analysis, and functional understanding has intensified.
Transformer models, as powerful deep learning architectures, have demonstrated
unprecedented potential in addressing diverse challenges across protein
research. However, a comprehensive review of Transformer applications in this
field remains lacking. This paper bridges this gap by surveying over 100
studies, offering an in-depth analysis of practical implementations and
research progress of Transformers in protein-related tasks. Our review
systematically covers critical domains, including protein structure prediction,
function prediction, protein-protein interaction analysis, functional
annotation, and drug discovery/target identification. To contextualize these
advancements across various protein domains, we adopt a domain-oriented
classification system. We first introduce foundational concepts: the
Transformer architecture and attention mechanisms, categorize Transformer
variants tailored for protein science, and summarize essential protein
knowledge. For each research domain, we outline its objectives and background,
critically evaluate prior methods and their limitations, and highlight
transformative contributions enabled by Transformer models. We also curate and
summarize pivotal datasets and open-source code resources to facilitate
reproducibility and benchmarking. Finally, we discuss persistent challenges in
applying Transformers to protein informatics and propose future research
directions. This review aims to provide a consolidated foundation for the
synergistic integration of Transformer and protein informatics, fostering
further innovation and expanded applications in the field.

</details>


### [656] [Refining Few-Step Text-to-Multiview Diffusion via Reinforcement Learning](https://arxiv.org/abs/2505.20107)
*Ziyi Zhang,Li Shen,Deheng Ye,Yong Luo,Huangxuan Zhao,Lefei Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种基于强化学习的微调框架MVC-ZigAL，用于优化少步文本到多视图（T2MV）扩散模型，在保持高效的同时提升图像保真度和视图一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的少步T2MV扩散模型在加速生成时往往牺牲图像质量和视图一致性，本文旨在解决这一问题。

Method: 提出统一的马尔可夫决策过程框架，结合ZMV-Sampling采样技术和MV-ZigAL策略优化方法，并通过约束优化平衡单视图保真度和跨视图一致性。

Result: MVC-ZigAL框架在保持少步效率的同时，显著提升了T2MV生成的图像质量和视图一致性。

Conclusion: 通过强化学习微调和约束优化，本文方法有效提升了少步T2MV扩散模型的性能，为多视图生成提供了新思路。

Abstract: Text-to-multiview (T2MV) generation, which produces coherent multiview images
from a single text prompt, remains computationally intensive, while accelerated
T2MV methods using few-step diffusion models often sacrifice image fidelity and
view consistency. To address this, we propose a novel reinforcement learning
(RL) finetuning framework tailored for few-step T2MV diffusion models to
jointly optimize per-view fidelity and cross-view consistency. Specifically, we
first reformulate T2MV denoising across all views as a single unified Markov
decision process, enabling multiview-aware policy optimization driven by a
joint-view reward objective. Next, we introduce ZMV-Sampling, a test-time T2MV
sampling technique that adds an inversion-denoising pass to reinforce both
viewpoint and text conditioning, resulting in improved T2MV generation at the
cost of inference time. To internalize its performance gains into the base
sampling policy, we develop MV-ZigAL, a novel policy optimization strategy that
uses reward advantages of ZMV-Sampling over standard sampling as learning
signals for policy updates. Finally, noting that the joint-view reward
objective under-optimizes per-view fidelity but naively optimizing single-view
metrics neglects cross-view alignment, we reframe RL finetuning for T2MV
diffusion models as a constrained optimization problem that maximizes per-view
fidelity subject to an explicit joint-view constraint, thereby enabling more
efficient and balanced policy updates. By integrating this constrained
optimization paradigm with MV-ZigAL, we establish our complete RL finetuning
framework, referred to as MVC-ZigAL, which effectively refines the few-step
T2MV diffusion baseline in both fidelity and consistency while preserving its
few-step efficiency.

</details>


### [657] [Proxy-Free GFlowNet](https://arxiv.org/abs/2505.20110)
*Ruishuo Chen,Xun Wang,Rui Hu,Zhuoran Li,Longbo Huang*

Main category: cs.LG

TL;DR: 论文提出了一种无需代理模型的轨迹蒸馏GFlowNet（TD-GFN）方法，通过逆向强化学习从离线数据中估计边级奖励，有效提升策略学习效率和样本质量。


<details>
  <summary>Details</summary>
Motivation: 在许多实际应用中，获取对象的奖励函数成本高昂或耗时，现有基于代理模型的方法因代理精度问题增加了训练复杂性和不确定性。

Method: TD-GFN利用逆向强化学习从离线数据估计边级奖励，剪枝有向无环图并指导反向轨迹采样，从而直接优化策略。

Result: 实验表明，TD-GFN在收敛速度和样本质量上显著优于基线方法，且训练过程高效可靠。

Conclusion: TD-GFN通过消除代理模型需求并聚焦关键边，为GFlowNets的离线训练提供了更简洁有效的解决方案。

Abstract: Generative Flow Networks (GFlowNets) are a promising class of generative
models designed to sample diverse, high-reward structures by modeling
distributions over compositional objects. In many real-world applications,
obtaining the reward function for such objects is expensive, time-consuming, or
requires human input, making it necessary to train GFlowNets from historical
datasets. Most existing methods adopt a model-based approach, learning a proxy
model from the dataset to approximate the reward function. However, this
strategy inherently ties the quality of the learned policy to the accuracy of
the proxy, introducing additional complexity and uncertainty into the training
process. To overcome these limitations, we propose \textbf{Trajectory-Distilled
GFlowNet (TD-GFN)}, a \emph{proxy-free} training framework that eliminates the
need for out-of-dataset reward queries. Our method is motivated by the key
observation that different edges in the associated directed acyclic graph (DAG)
contribute unequally to effective policy learning. TD-GFN leverages inverse
reinforcement learning to estimate edge-level rewards from the offline dataset,
which are then used to ingeniously prune the DAG and guide backward trajectory
sampling during training. This approach directs the policy toward high-reward
regions while reducing the complexity of model fitting. Empirical results
across multiple tasks show that TD-GFN trains both efficiently and reliably,
significantly outperforming existing baselines in convergence speed and sample
quality.

</details>


### [658] [Understanding Generalization in Diffusion Models via Probability Flow Distance](https://arxiv.org/abs/2505.20123)
*Huijie Zhang,Zijian Huang,Siyi Chen,Jinfan Zhou,Zekai Zhang,Peng Wang,Qing Qu*

Main category: cs.LG

TL;DR: 该论文提出了一种名为概率流距离（PFD）的新指标，用于评估扩散模型的分布泛化能力，并通过实验揭示了扩散模型的几种关键泛化行为。


<details>
  <summary>Details</summary>
Motivation: 扩散模型作为一种强大的生成模型，能够生成超出训练数据的高质量样本，但目前缺乏有效的方法来评估其泛化能力。

Method: 论文引入了概率流距离（PFD），通过比较噪声到数据的映射来量化分布之间的距离，并在师生评估协议下使用PFD进行实验分析。

Result: 实验揭示了扩散模型的几种关键泛化行为，包括从记忆到泛化的扩展行为、早期学习和双下降训练动态，以及偏差-方差分解。

Conclusion: 该研究为未来关于扩散模型泛化的实证和理论研究奠定了基础。

Abstract: Diffusion models have emerged as a powerful class of generative models,
capable of producing high-quality samples that generalize beyond the training
data. However, evaluating this generalization remains challenging: theoretical
metrics are often impractical for high-dimensional data, while no practical
metrics rigorously measure generalization. In this work, we bridge this gap by
introducing probability flow distance ($\texttt{PFD}$), a theoretically
grounded and computationally efficient metric to measure distributional
generalization. Specifically, $\texttt{PFD}$ quantifies the distance between
distributions by comparing their noise-to-data mappings induced by the
probability flow ODE. Moreover, by using $\texttt{PFD}$ under a teacher-student
evaluation protocol, we empirically uncover several key generalization
behaviors in diffusion models, including: (1) scaling behavior from
memorization to generalization, (2) early learning and double descent training
dynamics, and (3) bias-variance decomposition. Beyond these insights, our work
lays a foundation for future empirical and theoretical studies on
generalization in diffusion models.

</details>


### [659] [Balancing Interference and Correlation in Spatial Experimental Designs: A Causal Graph Cut Approach](https://arxiv.org/abs/2505.20130)
*Zhu Jin,Li Jingyi,Zhou Hongyi,Lin Yinan,Lin Zhenhua,Shi Chengchun*

Main category: cs.LG

TL;DR: 该论文提出了一种利用图割算法优化空间实验设计的方法，以提高因果效应估计的准确性，适用于中等至大规模的空间干扰效应和不同空间协方差函数。


<details>
  <summary>Details</summary>
Motivation: 为了提高空间实验中因果效应估计的准确性，并解决传统方法在处理空间干扰效应和不同空间协方差函数时的局限性。

Method: 提出了一种替代均方误差（MSE）的代理函数，利用经典图割算法学习最优实验设计。

Result: 理论和数值实验验证了该方法的有效性，特别是在合成环境和城市规模拼车市场模拟器中表现优异。

Conclusion: 该方法在计算效率和适应性上具有显著优势，为空间实验设计提供了新的解决方案。

Abstract: This paper focuses on the design of spatial experiments to optimize the
amount of information derived from the experimental data and enhance the
accuracy of the resulting causal effect estimator. We propose a surrogate
function for the mean squared error (MSE) of the estimator, which facilitates
the use of classical graph cut algorithms to learn the optimal design. Our
proposal offers three key advances: (1) it accommodates moderate to large
spatial interference effects; (2) it adapts to different spatial covariance
functions; (3) it is computationally efficient. Theoretical results and
numerical experiments based on synthetic environments and a dispatch simulator
that models a city-scale ridesharing market, further validate the effectiveness
of our design. A python implementation of our method is available at
https://github.com/Mamba413/CausalGraphCut.

</details>


### [660] [MolEditRL: Structure-Preserving Molecular Editing via Discrete Diffusion and Reinforcement Learning](https://arxiv.org/abs/2505.20131)
*Yuanxin Zhuang,Dazhong Shen,Ying Sun*

Main category: cs.LG

TL;DR: MolEditRL提出了一种结合结构约束与精确属性优化的分子编辑框架，显著提升了编辑成功率和结构保真度。


<details>
  <summary>Details</summary>
Motivation: 当前分子编辑方法通常基于字符串或连续表示，未能充分捕捉分子的离散图结构特性，导致结构保真度和可控性不足。

Method: MolEditRL采用两阶段方法：预训练的离散图扩散模型和基于强化学习的微调阶段，结合图约束优化编辑决策。

Result: 实验表明，MolEditRL在属性优化准确性和结构保真度上显著优于现有方法，编辑成功率提升74%，参数减少98%。

Conclusion: MolEditRL通过整合结构约束与强化学习，为分子编辑提供了高效且可控的解决方案。

Abstract: Molecular editing aims to modify a given molecule to optimize desired
chemical properties while preserving structural similarity. However, current
approaches typically rely on string-based or continuous representations, which
fail to adequately capture the discrete, graph-structured nature of molecules,
resulting in limited structural fidelity and poor controllability. In this
paper, we propose MolEditRL, a molecular editing framework that explicitly
integrates structural constraints with precise property optimization.
Specifically, MolEditRL consists of two stages: (1) a discrete graph diffusion
model pretrained to reconstruct target molecules conditioned on source
structures and natural language instructions; (2) an editing-aware
reinforcement learning fine-tuning stage that further enhances property
alignment and structural preservation by explicitly optimizing editing
decisions under graph constraints. For comprehensive evaluation, we construct
MolEdit-Instruct, the largest and most property-rich molecular editing dataset,
comprising 3 million diverse examples spanning single- and multi-property tasks
across 10 chemical attributes. Experimental results demonstrate that MolEditRL
significantly outperforms state-of-the-art methods in both property
optimization accuracy and structural fidelity, achieving a 74\% improvement in
editing success rate while using 98\% fewer parameters.

</details>


### [661] [Tensorization is a powerful but underexplored tool for compression and interpretability of neural networks](https://arxiv.org/abs/2505.20132)
*Safa Hamreras,Sukhbinder Singh,Román Orús*

Main category: cs.LG

TL;DR: 该立场论文探讨了张量化神经网络（TNNs）的潜力与当前局限性，强调其作为模型压缩工具外的架构灵活性和可解释性优势，并呼吁加强研究和应用。


<details>
  <summary>Details</summary>
Motivation: 尽管张量化神经网络在模型压缩方面表现出潜力，但在主流深度学习中的应用仍不足。论文旨在揭示TNNs的潜在价值，包括其独特的扩展性和内部表征的可解释性，以促进更广泛的研究和工程应用。

Method: 通过将神经网络的稠密权重矩阵重塑为高阶张量，并利用低秩张量网络分解进行近似，实现模型压缩和架构创新。

Result: TNNs不仅是一种有效的模型压缩方法，还具有灵活的架构设计、独特的扩展性和增强的可解释性，其内部表征可能为特征演化提供新见解。

Conclusion: 论文呼吁加强TNNs的理论和工程研究，克服实际应用中的障碍，并提出了几个关键研究方向，以推动TNNs在现代深度学习工作流程中的广泛应用。

Abstract: Tensorizing a neural network involves reshaping some or all of its dense
weight matrices into higher-order tensors and approximating them using low-rank
tensor network decompositions. This technique has shown promise as a model
compression strategy for large-scale neural networks. However, despite
encouraging empirical results, tensorized neural networks (TNNs) remain
underutilized in mainstream deep learning. In this position paper, we offer a
perspective on both the potential and current limitations of TNNs. We argue
that TNNs represent a powerful yet underexplored framework for deep
learning--one that deserves greater attention from both engineering and
theoretical communities. Beyond compression, we highlight the value of TNNs as
a flexible class of architectures with distinctive scaling properties and
increased interpretability. A central feature of TNNs is the presence of bond
indices, which introduce new latent spaces not found in conventional networks.
These internal representations may provide deeper insight into the evolution of
features across layers, potentially advancing the goals of mechanistic
interpretability. We conclude by outlining several key research directions
aimed at overcoming the practical barriers to scaling and adopting TNNs in
modern deep learning workflows.

</details>


### [662] [Data-Distill-Net: A Data Distillation Approach Tailored for Reply-based Continual Learning](https://arxiv.org/abs/2505.20135)
*Wenyang Liao,Quanziang Wang,Yichen Wu,Renzhen Wang,Deyu Meng*

Main category: cs.LG

TL;DR: 本文提出了一种新的持续学习数据集蒸馏框架，通过可学习的内存缓冲区和轻量级蒸馏模块有效缓解遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于回放的持续学习方法假设在小规模子集上训练的模型也能有效最小化完整数据集的经验风险，但由于内存缓冲区容量有限和数据选择启发式标准，这一假设在实践中并不成立。

Method: 提出了一种针对持续学习的数据集蒸馏框架，维护一个可学习的内存缓冲区来蒸馏当前任务数据和先前缓冲区中的全局信息，并引入轻量级蒸馏模块通过生成可学习的软标签实现全局信息蒸馏。

Result: 大量实验表明，该方法能够取得竞争性结果，并在各种数据集上有效缓解遗忘问题。

Conclusion: 该方法通过可学习的内存缓冲区和轻量级蒸馏模块，在持续学习中取得了显著效果，源代码将公开。

Abstract: Replay-based continual learning (CL) methods assume that models trained on a
small subset can also effectively minimize the empirical risk of the complete
dataset. These methods maintain a memory buffer that stores a sampled subset of
data from previous tasks to consolidate past knowledge. However, this
assumption is not guaranteed in practice due to the limited capacity of the
memory buffer and the heuristic criteria used for buffer data selection. To
address this issue, we propose a new dataset distillation framework tailored
for CL, which maintains a learnable memory buffer to distill the global
information from the current task data and accumulated knowledge preserved in
the previous memory buffer. Moreover, to avoid the computational overhead and
overfitting risks associated with parameterizing the entire buffer during
distillation, we introduce a lightweight distillation module that can achieve
global information distillation solely by generating learnable soft labels for
the memory buffer data. Extensive experiments show that, our method can achieve
competitive results and effectively mitigates forgetting across various
datasets. The source code will be publicly available.

</details>


### [663] [Error Optimization: Overcoming Exponential Signal Decay in Deep Predictive Coding Networks](https://arxiv.org/abs/2505.20137)
*Cédric Goemaere,Gaspard Oliviers,Rafal Bogacz,Thomas Demeester*

Main category: cs.LG

TL;DR: 论文提出Error Optimization (EO)方法，解决预测编码(PC)在深层网络中梯度衰减的问题，使其性能媲美反向传播。


<details>
  <summary>Details</summary>
Motivation: 预测编码(PC)作为反向传播的生物可替代方案，在深层架构中存在信号衰减问题，导致梯度随深度指数衰减。

Method: 引入Error Optimization (EO)，通过优化预测误差而非状态，避免信号衰减，保持PC理论特性的同时提升效率。

Result: 实验表明EO在多种架构和数据集上性能与反向传播相当，且在深层模型中收敛速度显著快于传统PC。

Conclusion: EO不仅解决了PC的深层训练问题，还为生物启发学习在数字硬件上的扩展提供了理论基础。

Abstract: Predictive Coding (PC) offers a biologically plausible alternative to
backpropagation for neural network training, yet struggles with deeper
architectures. This paper identifies the root cause: an inherent signal decay
problem where gradients attenuate exponentially with depth, becoming
computationally negligible due to numerical precision constraints. To address
this fundamental limitation, we introduce Error Optimization (EO), a novel
reparameterization that preserves PC's theoretical properties while eliminating
signal decay. By optimizing over prediction errors rather than states, EO
enables signals to reach all layers simultaneously and without attenuation,
converging orders of magnitude faster than standard PC. Experiments across
multiple architectures and datasets demonstrate that EO matches
backpropagation's performance even for deeper models where conventional PC
struggles. Besides practical improvements, our work provides theoretical
insight into PC dynamics and establishes a foundation for scaling
biologically-inspired learning to deeper architectures on digital hardware and
beyond.

</details>


### [664] [Model Stitching by Functional Latent Alignment](https://arxiv.org/abs/2505.20142)
*Ioannis Athanasiadis,Anmar Karmush,Michael Felsberg*

Main category: cs.LG

TL;DR: 本文提出了一种名为FuLA的新方法，用于评估神经网络的函数相似性，通过实验验证其在多种场景下的有效性。


<details>
  <summary>Details</summary>
Motivation: 评估独立训练的神经网络在函数相似性方面的表现是一个重要但尚未解决的问题，对AI领域有深远影响。模型缝合是一种有前景的方法，但现有方法存在局限性。

Method: 本文从知识蒸馏文献中获得灵感，提出了Functional Latent Alignment (FuLA)作为模型缝合的新优化条件。

Result: 实验表明，FuLA在对抗训练、捷径训练和跨层缝合等场景中表现更可靠，能够避免与任务线索相关的伪影，并实现非平凡的匹配。

Conclusion: FuLA是一种更可靠的函数相似性评估方法，能够有效解决现有方法的局限性。

Abstract: Evaluating functional similarity involves quantifying the degree to which
independently trained neural networks learn functionally similar
representations. Reliably inferring the functional similarity of these networks
remains an open problem with far-reaching implications for AI. Model stitching
has emerged as a promising paradigm, where an optimal affine transformation
aligns two models to solve a task, with the stitched model serving as a proxy
for functional similarity. In this work, we draw inspiration from the knowledge
distillation literature and propose Functional Latent Alignment (FuLA) as a
novel optimality condition for model stitching. We revisit previously explored
functional similarity testbeds and introduce a new one, based on which FuLA
emerges as an overall more reliable method of functional similarity.
Specifically, our experiments in (a) adversarial training, (b) shortcut
training and, (c) cross-layer stitching, reveal that FuLA is less prone to
artifacts tied to training on task cues while achieving non-trivial alignments
that are missed by stitch-level matching.

</details>


### [665] [On the (Non) Injectivity of Piecewise Linear Janossy Pooling](https://arxiv.org/abs/2505.20150)
*Ilai Reshef,Nadav Dym*

Main category: cs.LG

TL;DR: 该论文探讨了多集函数的构造问题，证明了分段线性Janossy池化函数无法实现单射性，但在无重复元素的多集上，简单的深度集模型即可满足单射性和双Lipschitz性。


<details>
  <summary>Details</summary>
Motivation: 研究多集函数的构造，旨在找到既满足单射性又具有双Lipschitz性质的简单多集函数，以提升神经网络在多集和图任务中的性能。

Method: 论文分析了k-ary Janossy池化函数族，证明了分段线性Janossy池化函数无法实现单射性，并探讨了在无重复元素多集上的简单深度集模型。

Result: 证明了分段线性Janossy池化函数无法实现单射性，但在无重复元素的多集上，简单的深度集模型可以满足单射性和双Lipschitz性。

Conclusion: 论文在多集函数的构造问题上取得了重要进展，揭示了分段线性Janossy池化函数的局限性，并展示了在特定条件下的简单解决方案。

Abstract: Multiset functions, which are functions that map multisets to vectors, are a
fundamental tool in the construction of neural networks for multisets and
graphs. To guarantee that the vector representation of the multiset is
faithful, it is often desirable to have multiset mappings that are both
injective and bi-Lipschitz. Currently, there are several constructions of
multiset functions achieving both these guarantees, leading to improved
performance in some tasks but often also to higher compute time than standard
constructions. Accordingly, it is natural to inquire whether simpler multiset
functions achieving the same guarantees are available. In this paper, we make a
large step towards giving a negative answer to this question. We consider the
family of k-ary Janossy pooling, which includes many of the most popular
multiset models, and prove that no piecewise linear Janossy pooling function
can be injective. On the positive side, we show that when restricted to
multisets without multiplicities, even simple deep-sets models suffice for
injectivity and bi-Lipschitzness.

</details>


### [666] [Prismatic Synthesis: Gradient-based Data Diversification Boosts Generalization in LLM Reasoning](https://arxiv.org/abs/2505.20161)
*Jaehun Jung,Seungju Han,Ximing Lu,Skyler Hallinan,David Acuna,Shrimai Prabhumoye,Mostafa Patwary,Mohammad Shoeybi,Bryan Catanzaro,Yejin Choi*

Main category: cs.LG

TL;DR: 该论文提出了一种新的数据多样性度量方法G-Vendi，并通过实验证明其在预测语言模型泛化能力上的有效性。同时，提出了Prismatic Synthesis框架，用于生成多样化的合成数据，显著提升了模型在未见数据上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有数据多样性度量方法往往依赖于表面启发式方法，与模型行为脱节。论文旨在探索真正驱动语言模型泛化的数据多样性类型，并提出有效的度量方法。

Method: 论文引入了G-Vendi度量方法，通过模型诱导梯度的熵来量化数据多样性。此外，提出了Prismatic Synthesis框架，针对梯度空间中未被充分代表的区域生成多样化合成数据。

Result: G-Vendi在自然语言推理和数学推理任务上表现出色，与OOD性能的相关系数达到0.9。Prismatic Synthesis生成的合成数据显著提升了模型性能，在多个基准测试上超越了依赖更大规模数据生成器的现有最佳模型。

Conclusion: 数据多样性对语言模型泛化能力至关重要。G-Vendi和Prismatic Synthesis为理解和提升数据多样性提供了有效工具，显著提升了模型在未见数据上的表现。

Abstract: Effective generalization in language models depends critically on the
diversity of their training data. Yet existing diversity metrics often fall
short of this goal, relying on surface-level heuristics that are decoupled from
model behavior. This motivates us to ask: What kind of diversity in training
data actually drives generalization in language models -- and how can we
measure and amplify it? Through large-scale empirical analyses spanning over
300 training runs, carefully controlled for data scale and quality, we show
that data diversity can be a strong predictor of generalization in LLM
reasoning -- as measured by average model performance on unseen
out-of-distribution benchmarks. We introduce G-Vendi, a metric that quantifies
diversity via the entropy of model-induced gradients. Despite using a small
off-the-shelf proxy model for gradients, G-Vendi consistently outperforms
alternative measures, achieving strong correlation (Spearman's $\rho \approx
0.9$) with out-of-distribution (OOD) performance on both natural language
inference (NLI) and math reasoning tasks. Building on this insight, we present
Prismatic Synthesis, a framework for generating diverse synthetic data by
targeting underrepresented regions in gradient space. Experimental results show
that Prismatic Synthesis consistently improves model performance as we scale
synthetic data -- not just on in-distribution test but across unseen,
out-of-distribution benchmarks -- significantly outperforming state-of-the-art
models that rely on 20 times larger data generator than ours. For example,
PrismMath-7B, our model distilled from a 32B LLM, outperforms
R1-Distill-Qwen-7B -- the same base model trained on proprietary data generated
by 671B R1 -- on 6 out of 7 challenging benchmarks.

</details>


### [667] [A Theoretical Framework for Grokking: Interpolation followed by Riemannian Norm Minimisation](https://arxiv.org/abs/2505.20172)
*Etienne Boursier,Scott Pesme,Radu-Alexandru Dragomir*

Main category: cs.LG

TL;DR: 论文研究了带小权重衰减的梯度流动态，揭示了其两阶段行为，并解释了深度学习中的‘顿悟’现象。


<details>
  <summary>Details</summary>
Motivation: 旨在解释深度学习中训练损失快速下降而测试损失长时间停滞后才突然改善的‘顿悟’现象。

Method: 在一般训练损失函数上分析带权重衰减的梯度流动态，假设无正则化梯度流收敛，并研究权重衰减趋近于零时的轨迹行为。

Result: 发现轨迹呈现两阶段行为：初始快速阶段跟随无正则化梯度流收敛到临界点流形，随后进入慢漂移阶段，遵循黎曼梯度流最小化参数范数。

Conclusion: 权重衰减引起的慢速范数减少可解释‘顿悟’现象，并通过合成回归任务验证了这一机制。

Abstract: We study the dynamics of gradient flow with small weight decay on general
training losses $F: \mathbb{R}^d \to \mathbb{R}$. Under mild regularity
assumptions and assuming convergence of the unregularised gradient flow, we
show that the trajectory with weight decay $\lambda$ exhibits a two-phase
behaviour as $\lambda \to 0$. During the initial fast phase, the trajectory
follows the unregularised gradient flow and converges to a manifold of critical
points of $F$. Then, at time of order $1/\lambda$, the trajectory enters a slow
drift phase and follows a Riemannian gradient flow minimising the $\ell_2$-norm
of the parameters. This purely optimisation-based phenomenon offers a natural
explanation for the \textit{grokking} effect observed in deep learning, where
the training loss rapidly reaches zero while the test loss plateaus for an
extended period before suddenly improving. We argue that this generalisation
jump can be attributed to the slow norm reduction induced by weight decay, as
explained by our analysis. We validate this mechanism empirically on several
synthetic regression tasks.

</details>


### [668] [The Power of Iterative Filtering for Supervised Learning with (Heavy) Contamination](https://arxiv.org/abs/2505.20177)
*Adam R. Klivans,Konstantinos Stavropoulos,Kevin Tian,Arsen Vasilyan*

Main category: cs.LG

TL;DR: 本文提出了一种名为迭代多项式滤波的通用异常值去除算法，并展示了其在监督学习中的多种应用，包括在污染数据下的高效学习、强对抗污染下的近最优学习保证，以及对半空间函数的容忍可测试学习。


<details>
  <summary>Details</summary>
Motivation: 受分布偏移学习的最新研究启发，本文旨在解决监督学习在数据污染（如恶意噪声）下的长期挑战，填补了现有理论与实际应用之间的差距。

Method: 提出了一种称为迭代多项式滤波的通用异常值去除算法，利用低次多项式逼近和夹逼逼近器的概念来处理不同类型的污染。

Result: （1）在有限污染下高效学习超压缩分布的低次多项式逼近函数类；（2）在强对抗污染下获得近最优学习保证；（3）首次实现对半空间函数的容忍可测试学习。

Conclusion: 这些结果显著推进了我们对污染下高效监督学习的理解，解决了多个长期未解决的问题，并为未来研究提供了新的方向。

Abstract: Inspired by recent work on learning with distribution shift, we give a
general outlier removal algorithm called iterative polynomial filtering and
show a number of striking applications for supervised learning with
contamination: (1) We show that any function class that can be approximated by
low-degree polynomials with respect to a hypercontractive distribution can be
efficiently learned under bounded contamination (also known as nasty noise).
This is a surprising resolution to a longstanding gap between the complexity of
agnostic learning and learning with contamination, as it was widely believed
that low-degree approximators only implied tolerance to label noise. (2) For
any function class that admits the (stronger) notion of sandwiching
approximators, we obtain near-optimal learning guarantees even with respect to
heavy additive contamination, where far more than $1/2$ of the training set may
be added adversarially. Prior related work held only for regression and in a
list-decodable setting. (3) We obtain the first efficient algorithms for
tolerant testable learning of functions of halfspaces with respect to any fixed
log-concave distribution. Even the non-tolerant case for a single halfspace in
this setting had remained open. These results significantly advance our
understanding of efficient supervised learning under contamination, a setting
that has been much less studied than its unsupervised counterpart.

</details>


### [669] [Research on feature fusion and multimodal patent text based on graph attention network](https://arxiv.org/abs/2505.20188)
*Zhenzhen Song,Ziwei Liu,Hongji Li*

Main category: cs.LG

TL;DR: 该研究提出HGM-Net框架，通过层次对比学习、多模态图注意力网络和多粒度稀疏注意力解决专利文本语义挖掘中的特征融合、长文本建模效率和语义连贯性问题。


<details>
  <summary>Details</summary>
Motivation: 针对专利文本语义挖掘中的跨模态特征融合、长文本建模效率低及缺乏层次语义连贯性问题，提出了HGM-Net框架。

Method: HGM-Net整合了层次对比学习（HCL）、多模态图注意力网络（M-GAT）和多粒度稀疏注意力（MSA），通过动态掩码和对比学习增强语义一致性，并优化长文本建模效率。

Result: 实验表明，该框架在专利分类和相似性匹配等任务上优于现有深度学习方法，提升了专利审查效率和技术关联挖掘能力。

Conclusion: HGM-Net为专利文本语义挖掘提供了理论创新和实用价值的解决方案，显著提升了任务性能。

Abstract: Aiming at the problems of cross-modal feature fusion, low efficiency of long
text modeling and lack of hierarchical semantic coherence in patent text
semantic mining, this study proposes HGM-Net, a deep learning framework that
integrates Hierarchical Comparative Learning (HCL), Multi-modal Graph Attention
Network (M-GAT) and Multi-Granularity Sparse Attention (MSA), which builds a
dynamic mask, contrast and cross-structural similarity constraints on the word,
sentence and paragraph hierarchies through HCL. Contrast and cross-structural
similarity constraints are constructed at the word and paragraph levels by HCL
to strengthen the local semantic and global thematic consistency of patent
text; M-GAT models patent classification codes, citation relations and text
semantics as heterogeneous graph structures, and achieves dynamic fusion of
multi-source features by cross-modal gated attention; MSA adopts a hierarchical
sparsity strategy to optimize the computational efficiency of long text
modeling at word, phrase, sentence and paragraph granularity. Experiments show
that the framework demonstrates significant advantages over existing deep
learning methods in tasks such as patent classification and similarity
matching, and provides a solution with both theoretical innovation and
practical value for solving the problems of patent examination efficiency
improvement and technology relevance mining.

</details>


### [670] [FunReason: Enhancing Large Language Models' Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement](https://arxiv.org/abs/2505.20192)
*Bingguang Hao,Maolin Wang,Zengzhuang Xu,Cunyin Peng,Yicheng Chen,Xiangyu Zhao,Jinjie Gu,Chenyi Zhuang*

Main category: cs.LG

TL;DR: FunReason框架通过自动化数据优化和多尺度损失函数提升大语言模型的功能调用能力，性能媲美GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以平衡推理步骤与功能调用精度，导致性能不佳，需新方法提升实用性。

Method: FunReason结合自动数据优化策略和自优化多尺度损失函数，动态平衡推理与功能调用精度。

Result: FunReason性能接近GPT-4o，有效避免微调时的灾难性遗忘问题。

Conclusion: FunReason为增强大语言模型功能调用能力提供了平衡的训练方法和数据优化流程。

Abstract: The integration of large language models (LLMs) with function calling has
emerged as a crucial capability for enhancing their practical utility in
real-world applications. However, effectively combining reasoning processes
with accurate function execution remains a significant challenge. Traditional
training approaches often struggle to balance the detailed reasoning steps with
the precision of function calls, leading to suboptimal performance. To address
these limitations, we introduce FunReason, a novel framework that enhances
LLMs' function calling capabilities through an automated data refinement
strategy and a Self-Refinement Multiscale Loss (SRML) approach. FunReason
leverages LLMs' natural reasoning abilities to generate high-quality training
examples, focusing on query parseability, reasoning coherence, and function
call precision. The SRML approach dynamically balances the contribution of
reasoning processes and function call accuracy during training, addressing the
inherent trade-off between these two critical aspects. FunReason achieves
performance comparable to GPT-4o while effectively mitigating catastrophic
forgetting during fine-tuning. FunReason provides a comprehensive solution for
enhancing LLMs' function calling capabilities by introducing a balanced
training methodology and a data refinement pipeline. For code and dataset,
please refer to our repository at GitHub
https://github.com/BingguangHao/FunReason

</details>


### [671] [Parameter-Efficient Fine-Tuning with Column Space Projection](https://arxiv.org/abs/2505.20211)
*Junseo Hwang,Wonguk Cho,Taesup Kim*

Main category: cs.LG

TL;DR: 论文提出了一种基于权重谱特性的参数高效微调方法PiCa，相比LoRA减少了13倍可训练参数且性能更优。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法（如LoRA）与全参数微调存在学习行为差异，特别是在谱特性方面。

Method: PiCa将梯度投影到预训练权重的低秩列子空间，并结合权重共享减少可训练参数。

Result: PiCa在性能上优于现有PEFT方法，且仅需LoRA 1/13的可训练参数。

Conclusion: PiCa是首个基于谱特性的理论驱动PEFT方法，实现了SOTA性能与参数效率的平衡。

Abstract: Fine-tuning large language models (LLMs) with minimal computational overhead
is essential for efficiently adapting them to downstream tasks under resource
constraints. Parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank
Adaptation (LoRA), facilitate this by updating only a small subset of
parameters. However, recent studies show that LoRA diverges from full
fine-tuning (Full FT) in its learning behavior, particularly in terms of
spectral properties. Motivated by these findings, we propose PiCa, the first
theoretically grounded PEFT method based on the spectral properties of
fine-tuned weights. PiCa projects gradients onto the low-rank column subspace
of pre-trained weights and exhibits learning patterns more closely aligned with
Full FT. Furthermore, we show that combining PiCa with weight sharing
drastically reduces the number of trainable parameters without compromising
performance, enabling to achieve superior performance than LoRA using 13x fewer
trainable parameters. Extensive experiments demonstrate PiCa achieves the
state-of-the-art performance compared to existing PEFT methods.

</details>


### [672] [Fine-grained List-wise Alignment for Generative Medication Recommendation](https://arxiv.org/abs/2505.20218)
*Chenxiao Fan,Chongming Gao,Wentao Shi,Yaxin Gong,Zihao Zhao,Fuli Feng*

Main category: cs.LG

TL;DR: FLAME框架通过细粒度列表对齐和大语言模型，优化多病症药物推荐，兼顾准确性与安全性。


<details>
  <summary>Details</summary>
Motivation: 现有药物推荐系统忽视药物协同效应和潜在不良反应，难以满足多病症临床决策需求。

Method: 提出FLAME框架：1) 将推荐建模为顺序决策过程；2) 采用GRPO策略优化单药贡献；3) 融合临床知识与协作信息增强患者建模。

Result: 实验表明FLAME在准确性、安全可控性和跨场景泛化性方面达到SOTA性能。

Conclusion: FLAME为复杂临床场景提供了更安全、准确的药物推荐新范式。

Abstract: Accurate and safe medication recommendations are critical for effective
clinical decision-making, especially in multimorbidity cases. However, existing
systems rely on point-wise prediction paradigms that overlook synergistic drug
effects and potential adverse drug-drug interactions (DDIs). We propose FLAME,
a fine-grained list-wise alignment framework for large language models (LLMs),
enabling drug-by-drug generation of drug lists. FLAME formulates recommendation
as a sequential decision process, where each step adds or removes a single
drug. To provide fine-grained learning signals, we devise step-wise Group
Relative Policy Optimization (GRPO) with potential-based reward shaping, which
explicitly models DDIs and optimizes the contribution of each drug to the
overall prescription. Furthermore, FLAME enhances patient modeling by
integrating structured clinical knowledge and collaborative information into
the representation space of LLMs. Experiments on benchmark datasets demonstrate
that FLAME achieves state-of-the-art performance, delivering superior accuracy,
controllable safety-accuracy trade-offs, and strong generalization across
diverse clinical scenarios. Our code is available at
https://github.com/cxfann/Flame.

</details>


### [673] [Gradient Flow Matching for Learning Update Dynamics in Neural Network Training](https://arxiv.org/abs/2505.20221)
*Xiao Shou,Yanna Ding,Jianxi Gao*

Main category: cs.LG

TL;DR: 提出梯度流匹配(GFM)框架，通过连续时间建模将神经网络训练视为动态系统，利用条件流匹配学习优化器感知的向量场，实现权重轨迹预测并加速收敛。


<details>
  <summary>Details</summary>
Motivation: 传统基于梯度的深度神经网络训练计算成本高，因其依赖迭代优化。需要一种能捕捉优化器动态并预测收敛的方法来加速训练过程。

Method: GFM将训练过程建模为动态系统，通过学习优化器感知的向量场（支持SGD/Adam/RMSprop等），利用条件流匹配技术实现权重轨迹的外推。

Result: 实验表明GFM的预测精度与Transformer相当，显著优于LSTM等基线，且能跨网络架构和初始化条件泛化，提供统一的优化动态分析框架。

Conclusion: GFM通过显式建模梯度更新规则，为优化动态研究和收敛预测提供了高效的理论框架，同时具备良好的泛化能力。

Abstract: Training deep neural networks remains computationally intensive due to the
itera2 tive nature of gradient-based optimization. We propose Gradient Flow
Matching (GFM), a continuous-time modeling framework that treats neural network
training as a dynamical system governed by learned optimizer-aware vector
fields. By leveraging conditional flow matching, GFM captures the underlying
update rules of optimizers such as SGD, Adam, and RMSprop, enabling smooth
extrapolation of weight trajectories toward convergence. Unlike black-box
sequence models, GFM incorporates structural knowledge of gradient-based
updates into the learning objective, facilitating accurate forecasting of final
weights from partial training sequences. Empirically, GFM achieves forecasting
accuracy that is competitive with Transformer-based models and significantly
outperforms LSTM and other classical baselines. Furthermore, GFM generalizes
across neural architectures and initializations, providing a unified framework
for studying optimization dynamics and accelerating convergence prediction.

</details>


### [674] [From What to How: Attributing CLIP's Latent Components Reveals Unexpected Semantic Reliance](https://arxiv.org/abs/2505.20229)
*Maximilian Dreyer,Lorenz Hufe,Jim Berend,Thomas Wiegand,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.LG

TL;DR: 该论文提出了一个可扩展框架，用于揭示CLIP模型中潜在组件的激活原因、语义对齐及其对预测的重要性，发现了数百个与多义词、复合名词等相关的意外组件。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的CLIP模型广泛用于文本-图像特征提取，但其内部预测机制尚不明确。现有研究关注稀疏自编码器（SAEs）的潜在组件编码内容，但忽略了这些组件如何驱动预测。

Method: 作者结合归因修补（attribution patching）和语义对齐评分，实现实例级组件归因分析，并指出Logit Lens技术的局限性。

Result: 在多个CLIP变体中发现数百个意外组件，涉及多义词、视觉排版等。文本嵌入对虚假相关性比图像嵌入线性分类器更鲁棒。皮肤病变检测案例揭示了隐藏捷径的放大效应。

Conclusion: 研究表明需要整体、机械的可解释性方法，并提供了代码以促进相关研究。

Abstract: Transformer-based CLIP models are widely used for text-image probing and
feature extraction, making it relevant to understand the internal mechanisms
behind their predictions. While recent works show that Sparse Autoencoders
(SAEs) yield interpretable latent components, they focus on what these encode
and miss how they drive predictions. We introduce a scalable framework that
reveals what latent components activate for, how they align with expected
semantics, and how important they are to predictions. To achieve this, we adapt
attribution patching for instance-wise component attributions in CLIP and
highlight key faithfulness limitations of the widely used Logit Lens technique.
By combining attributions with semantic alignment scores, we can automatically
uncover reliance on components that encode semantically unexpected or spurious
concepts. Applied across multiple CLIP variants, our method uncovers hundreds
of surprising components linked to polysemous words, compound nouns, visual
typography and dataset artifacts. While text embeddings remain prone to
semantic ambiguity, they are more robust to spurious correlations compared to
linear classifiers trained on image embeddings. A case study on skin lesion
detection highlights how such classifiers can amplify hidden shortcuts,
underscoring the need for holistic, mechanistic interpretability. We provide
code at https://github.com/maxdreyer/attributing-clip.

</details>


### [675] [Multimodal Federated Learning With Missing Modalities through Feature Imputation Network](https://arxiv.org/abs/2505.20232)
*Pranav Poudel,Aavash Chhetri,Prashnna Gyawali,Georgios Leontidis,Binod Bhattarai*

Main category: cs.LG

TL;DR: 提出一种轻量级低维特征转换器，用于重构医疗多模态联邦学习中缺失模态的特征，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 医疗多模态联邦学习面临模态缺失问题，现有方法依赖真实或合成数据，但获取真实数据困难且生成模型计算成本高。

Method: 设计轻量级低维特征转换器，重构缺失模态的瓶颈特征，避免依赖真实数据或复杂生成模型。

Result: 在三个数据集（MIMIC-CXR、NIH Open-I、CheXpert）上验证，同质和异质场景均优于基线方法。

Conclusion: 该方法高效解决了医疗多模态联邦学习中的模态缺失问题，代码已开源。

Abstract: Multimodal federated learning holds immense potential for collaboratively
training models from multiple sources without sharing raw data, addressing both
data scarcity and privacy concerns, two key challenges in healthcare. A major
challenge in training multimodal federated models in healthcare is the presence
of missing modalities due to multiple reasons, including variations in clinical
practice, cost and accessibility constraints, retrospective data collection,
privacy concerns, and occasional technical or human errors. Previous methods
typically rely on publicly available real datasets or synthetic data to
compensate for missing modalities. However, obtaining real datasets for every
disease is impractical, and training generative models to synthesize missing
modalities is computationally expensive and prone to errors due to the high
dimensionality of medical data. In this paper, we propose a novel, lightweight,
low-dimensional feature translator to reconstruct bottleneck features of the
missing modalities. Our experiments on three different datasets (MIMIC-CXR, NIH
Open-I, and CheXpert), in both homogeneous and heterogeneous settings
consistently improve the performance of competitive baselines. The code and
implementation details are available at:
https://github.com/bhattarailab/FedFeatGen

</details>


### [676] [Variational Deep Learning via Implicit Regularization](https://arxiv.org/abs/2505.20235)
*Jonathan Wenger,Beau Coker,Juraj Marusic,John P. Cunningham*

Main category: cs.LG

TL;DR: 现代深度学习模型在分布内泛化能力强，但分布外不确定性量化不足。本文提出通过优化过程隐式正则化变分深度网络，无需额外调参即可实现优异性能。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型在分布内表现优异，但在分布外、序列决策或安全关键领域需要可靠的不确定性量化。传统贝叶斯深度学习方法因先验定义困难和计算负担难以满足需求。

Method: 通过优化过程隐式正则化变分深度网络，在过参数化线性模型中理论证明了（随机）梯度下降的归纳偏置等同于广义变分推断，并强调参数化选择的重要性。

Result: 实证表明该方法在不增加超参数调优、仅引入极小时空开销的情况下，实现了分布内外均优异的性能表现。

Conclusion: 优化过程本身可作为隐式正则化机制，为深度学习的分布外不确定性量化提供了高效解决方案。

Abstract: Modern deep learning models generalize remarkably well in-distribution,
despite being overparametrized and trained with little to no explicit
regularization. Instead, current theory credits implicit regularization imposed
by the choice of architecture, hyperparameters and optimization procedure.
However, deploying deep learning models out-of-distribution, in sequential
decision-making tasks, or in safety-critical domains, necessitates reliable
uncertainty quantification, not just a point estimate. The machinery of modern
approximate inference -- Bayesian deep learning -- should answer the need for
uncertainty quantification, but its effectiveness has been challenged by our
inability to define useful explicit inductive biases through priors, as well as
the associated computational burden. Instead, in this work we demonstrate, both
theoretically and empirically, how to regularize a variational deep network
implicitly via the optimization procedure, just as for standard deep learning.
We fully characterize the inductive bias of (stochastic) gradient descent in
the case of an overparametrized linear model as generalized variational
inference and demonstrate the importance of the choice of parametrization.
Finally, we show empirically that our approach achieves strong in- and
out-of-distribution performance without tuning of additional hyperparameters
and with minimal time and memory overhead over standard deep learning.

</details>


### [677] [DreamPRM: Domain-Reweighted Process Reward Model for Multimodal Reasoning](https://arxiv.org/abs/2505.20241)
*Qi Cao,Ruiyi Wang,Ruiyi Zhang,Sai Ashish Somayajula,Pengtao Xie*

Main category: cs.LG

TL;DR: DreamPRM提出了一种针对多模态推理过程的领域重加权训练框架，通过双层优化提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推理数据集存在质量不平衡问题，影响过程奖励模型（PRM）的性能，需要有效的数据选择策略。

Method: DreamPRM采用双层优化：下层优化通过领域权重微调多数据集，上层优化通过元学习数据集反馈更新权重。

Result: 实验表明，DreamPRM在多个多模态推理基准测试中显著提升了最先进模型的性能。

Conclusion: DreamPRM的领域重加权策略优于其他数据选择方法，并在测试时扩展中实现了更高的准确率提升。

Abstract: Reasoning has substantially improved the performance of large language models
(LLMs) on complicated tasks. Central to the current reasoning studies, Process
Reward Models (PRMs) offer a fine-grained evaluation of intermediate reasoning
steps and guide the reasoning process. However, extending PRMs to multimodal
large language models (MLLMs) introduces challenges. Since multimodal reasoning
covers a wider range of tasks compared to text-only scenarios, the resulting
distribution shift from the training to testing sets is more severe, leading to
greater generalization difficulty. Training a reliable multimodal PRM,
therefore, demands large and diverse datasets to ensure sufficient coverage.
However, current multimodal reasoning datasets suffer from a marked quality
imbalance, which degrades PRM performance and highlights the need for an
effective data selection strategy. To address the issues, we introduce
DreamPRM, a domain-reweighted training framework for multimodal PRMs which
employs bi-level optimization. In the lower-level optimization, DreamPRM
performs fine-tuning on multiple datasets with domain weights, allowing the PRM
to prioritize high-quality reasoning signals and alleviating the impact of
dataset quality imbalance. In the upper-level optimization, the PRM is
evaluated on a separate meta-learning dataset; this feedback updates the domain
weights through an aggregation loss function, thereby improving the
generalization capability of trained PRM. Extensive experiments on multiple
multimodal reasoning benchmarks covering both mathematical and general
reasoning show that test-time scaling with DreamPRM consistently improves the
performance of state-of-the-art MLLMs. Further comparisons reveal that
DreamPRM's domain-reweighting strategy surpasses other data selection methods
and yields higher accuracy gains than existing test-time scaling approaches.

</details>


### [678] [RedAHD: Reduction-Based End-to-End Automatic Heuristic Design with Large Language Models](https://arxiv.org/abs/2505.20242)
*Nguyen Thach,Aida Riahifar,Nathan Huynh,Hau Chan*

Main category: cs.LG

TL;DR: 本文提出了一种名为RedAHD的新型端到端框架，利用大型语言模型（LLMs）自动设计启发式算法，无需依赖预定义的通用算法框架（GAFs），从而减少人工干预。


<details>
  <summary>Details</summary>
Motivation: 传统解决NP难组合优化问题（如旅行商问题和容量受限的车辆路径问题）需要大量领域知识和人工设计启发式算法。现有方法虽利用LLMs设计启发式算法，但仍需人工干预和预定义框架。

Method: RedAHD框架通过LLMs将当前组合优化问题转化为更易理解的类似问题，然后设计启发式算法直接解决转化后的问题，间接解决原问题。

Result: 在六个组合优化问题上的实验表明，RedAHD设计的启发式算法性能优于或与现有最先进方法相当，且人工干预极少。

Conclusion: RedAHD框架展示了LLMs在自动设计启发式算法方面的潜力，为组合优化问题提供了一种高效且人工干预少的解决方案。

Abstract: Solving NP-hard combinatorial optimization problems (COPs) (e.g., traveling
salesman problems (TSPs) and capacitated vehicle routing problems (CVRPs)) in
practice traditionally involves handcrafting heuristics or specifying a search
space for finding effective heuristics. The main challenges from these
approaches, however, are the sheer amount of domain knowledge and
implementation efforts required from human experts. Recently, significant
progress has been made to address these challenges, particularly by using large
language models (LLMs) to design heuristics within some predetermined
generalized algorithmic framework (GAF, e.g., ant colony optimization and
guided local search) for building key functions/components (e.g., a priori
information on how promising it is to include each edge in a solution for TSP
and CVRP). Although existing methods leveraging this idea have shown to yield
impressive optimization performance, they are not fully end-to-end and still
require considerable manual interventions. In this paper, we propose a novel
end-to-end framework, named RedAHD, that enables these LLM-based heuristic
design methods to operate without the need of GAFs. More specifically, RedAHD
employs LLMs to automate the process of reduction, i.e., transforming the COP
at hand into similar COPs that are better-understood, from which LLM-based
heuristic design methods can design effective heuristics for directly solving
the transformed COPs and, in turn, indirectly solving the original COP. Our
experimental results, evaluated on six COPs, show that RedAHD is capable of
designing heuristics with competitive or improved results over the
state-of-the-art methods with minimal human involvement.

</details>


### [679] [Learning Extrapolative Sequence Transformations from Markov Chains](https://arxiv.org/abs/2505.20251)
*Sophia Hager,Aleem Khan,Andrew Wang,Nicholas Andrews*

Main category: cs.LG

TL;DR: 该论文提出了一种基于自回归模型的方法，用于高效生成具有所需特性的新序列，优于传统的MCMC方法。


<details>
  <summary>Details</summary>
Motivation: 在生物序列设计等任务中，需要超越训练数据的新假设来改进特性。传统的MCMC方法在大规模结构化状态空间中效率不高，因此需要一种能贪婪优化目标特性的模型。

Method: 利用MCMC搜索产生的马尔可夫链中的选定状态作为训练数据，训练一个自回归模型，以高效生成具有所需特性的新序列。

Result: 在蛋白质序列设计、文本情感控制和文本匿名化三个问题上验证了该方法的有效性，自回归模型在性能上优于或等同于MCMC，且具有更高的样本效率和可扩展性。

Conclusion: 自回归模型不仅能有效外推，还能显著提高样本效率和可扩展性，为序列设计任务提供了更优的解决方案。

Abstract: Most successful applications of deep learning involve similar training and
test conditions. However, tasks such as biological sequence design involve
searching for sequences that improve desirable properties beyond previously
known values, which requires novel hypotheses that \emph{extrapolate} beyond
training data. In these settings, extrapolation may be achieved by using random
search methods such as Markov chain Monte Carlo (MCMC), which, given an initial
state, sample local transformations to approximate a target density that
rewards states with the desired properties. However, even with a well-designed
proposal, MCMC may struggle to explore large structured state spaces
efficiently. Rather than relying on stochastic search, it would be desirable to
have a model that greedily optimizes the properties of interest, successfully
extrapolating in as few steps as possible. We propose to learn such a model
from the Markov chains resulting from MCMC search. Specifically, our approach
uses selected states from Markov chains as a source of training data for an
autoregressive model, which is then able to efficiently generate novel
sequences that extrapolate along the sequence-level properties of interest. The
proposed approach is validated on three problems: protein sequence design, text
sentiment control, and text anonymization. We find that the autoregressive
model can extrapolate as well or better than MCMC, but with the additional
benefits of scalability and significantly higher sample efficiency.

</details>


### [680] [Position: Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs](https://arxiv.org/abs/2505.20254)
*Xiangchen Song,Aashiq Muhamed,Yujia Zheng,Lingjing Kong,Zeyu Tang,Mona T. Diab,Virginia Smith,Kun Zhang*

Main category: cs.LG

TL;DR: 稀疏自编码器（SAEs）在机制可解释性（MI）中用于分解神经网络激活为可解释特征，但不同训练运行中学习到的特征不一致性影响了研究的可靠性。本文主张应优先考虑特征一致性，并提出使用PW-MCC作为衡量标准，证明通过适当架构选择可实现高一致性（0.80）。


<details>
  <summary>Details</summary>
Motivation: 当前稀疏自编码器（SAEs）在不同训练运行中生成的特征不一致，这削弱了机制可解释性（MI）研究的可靠性和效率。本文旨在解决这一问题，推动特征一致性成为MI研究的核心目标。

Method: 提出使用Pairwise Dictionary Mean Correlation Coefficient（PW-MCC）作为衡量特征一致性的实用指标，并通过理论分析和合成数据验证其作为真实特征恢复的可靠代理，进一步在真实LLM数据中验证高特征一致性与语义相似性的相关性。

Result: 实验证明，通过适当的架构选择，可以在LLM激活上实现高水平的特征一致性（PW-MCC达到0.80），且高特征一致性与学习到的特征解释的语义相似性密切相关。

Conclusion: 本文呼吁社区转向系统性测量特征一致性，以促进机制可解释性研究的稳健累积进展。PW-MCC是一个有效的衡量工具，高特征一致性有助于提升MI研究的可靠性。

Abstract: Sparse Autoencoders (SAEs) are a prominent tool in mechanistic
interpretability (MI) for decomposing neural network activations into
interpretable features. However, the aspiration to identify a canonical set of
features is challenged by the observed inconsistency of learned SAE features
across different training runs, undermining the reliability and efficiency of
MI research. This position paper argues that mechanistic interpretability
should prioritize feature consistency in SAEs -- the reliable convergence to
equivalent feature sets across independent runs. We propose using the Pairwise
Dictionary Mean Correlation Coefficient (PW-MCC) as a practical metric to
operationalize consistency and demonstrate that high levels are achievable
(0.80 for TopK SAEs on LLM activations) with appropriate architectural choices.
Our contributions include detailing the benefits of prioritizing consistency;
providing theoretical grounding and synthetic validation using a model
organism, which verifies PW-MCC as a reliable proxy for ground-truth recovery;
and extending these findings to real-world LLM data, where high feature
consistency strongly correlates with the semantic similarity of learned feature
explanations. We call for a community-wide shift towards systematically
measuring feature consistency to foster robust cumulative progress in MI.

</details>


### [681] [Outcome-Based Online Reinforcement Learning: Algorithms and Fundamental Limits](https://arxiv.org/abs/2505.20268)
*Fan Chen,Zeyu Jia,Alexander Rakhlin,Tengyang Xie*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Reinforcement learning with outcome-based feedback faces a fundamental
challenge: when rewards are only observed at trajectory endpoints, how do we
assign credit to the right actions? This paper provides the first comprehensive
analysis of this problem in online RL with general function approximation. We
develop a provably sample-efficient algorithm achieving $\widetilde{O}({C_{\rm
cov} H^3}/{\epsilon^2})$ sample complexity, where $C_{\rm cov}$ is the
coverability coefficient of the underlying MDP. By leveraging general function
approximation, our approach works effectively in large or infinite state spaces
where tabular methods fail, requiring only that value functions and reward
functions can be represented by appropriate function classes. Our results also
characterize when outcome-based feedback is statistically separated from
per-step rewards, revealing an unavoidable exponential separation for certain
MDPs. For deterministic MDPs, we show how to eliminate the completeness
assumption, dramatically simplifying the algorithm. We further extend our
approach to preference-based feedback settings, proving that equivalent
statistical efficiency can be achieved even under more limited information.
Together, these results constitute a theoretical foundation for understanding
the statistical properties of outcome-based reinforcement learning.

</details>


### [682] [Probabilistic Kernel Function for Fast Angle Testing](https://arxiv.org/abs/2505.20274)
*Kejing Lu,Chuan Xiao,Yoshiharu Ishikawa*

Main category: cs.LG

TL;DR: 本文提出两种基于投影的概率核函数，用于高维空间角度测试，无需渐近假设且性能优于高斯分布核函数，并在近似最近邻搜索中实现2.5~3倍QPS提升。


<details>
  <summary>Details</summary>
Motivation: 现有高维空间角度测试方法依赖高斯分布随机投影向量，存在渐近假设限制，本文旨在提出更优的确定性投影方法。

Method: 利用参考角度设计确定性投影向量结构，提出两种核函数：角度比较核与角度阈值核，避免无限投影向量的假设。

Result: 理论及实验证明新核函数优于高斯分布核函数，应用于ANNS时比HNSW算法提升2.5~3倍查询吞吐量。

Conclusion: 确定性投影核函数突破了传统随机投影的限制，为高维空间相似性计算提供了高效解决方案。

Abstract: In this paper, we study the angle testing problem in high-dimensional
Euclidean spaces and propose two projection-based probabilistic kernel
functions, one designed for angle comparison and the other for angle
thresholding. Unlike existing approaches that rely on random projection vectors
drawn from Gaussian distributions, our approach leverages reference angles and
employs a deterministic structure for the projection vectors. Notably, our
kernel functions do not require asymptotic assumptions, such as the number of
projection vectors tending to infinity, and can be both theoretically and
experimentally shown to outperform Gaussian-distribution-based kernel
functions. We further apply the proposed kernel function to Approximate Nearest
Neighbor Search (ANNS) and demonstrate that our approach achieves a 2.5X ~ 3X
higher query-per-second (QPS) throughput compared to the state-of-the-art
graph-based search algorithm HNSW.

</details>


### [683] [The Coverage Principle: A Framework for Understanding Compositional Generalization](https://arxiv.org/abs/2505.20278)
*Hoyeon Chang,Jinho Park,Hanseul Cho,Sohee Yang,Miyoung Ko,Hyeonbin Hwang,Seungpil Won,Dohaeng Lee,Youbin Ahn,Minjoon Seo*

Main category: cs.LG

TL;DR: 论文提出'覆盖原则'框架，揭示大语言模型在组合泛化上的局限性，并通过实验验证其预测能力，指出需要新架构创新以实现系统性组合性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型擅长模式匹配，但在系统性组合泛化方面表现不足。作者旨在通过数据中心的覆盖原则框架，理解并预测模型在组合任务中的泛化能力。

Method: 提出覆盖原则框架，通过实验分析Transformer在不同组合任务（如多跳推理、路径模糊）中的泛化表现，并引入思维链监督进行对比。

Result: 发现：(1) 双跳泛化所需训练数据随词集规模呈二次增长，参数缩放无效；(2) 路径模糊任务中模型学习到上下文依赖的状态表示；(3) 思维链提升多跳任务效率，但仍受限于路径模糊。

Conclusion: 覆盖原则为理解组合推理提供了统一视角，但需根本性架构或训练创新才能实现真正的系统性组合性。提出了基于机制的泛化分类法（结构、属性、共享操作符）。

Abstract: Large language models excel at pattern matching, yet often fall short in
systematic compositional generalization. We propose the coverage principle: a
data-centric framework showing that models relying primarily on pattern
matching for compositional tasks cannot reliably generalize beyond substituting
fragments that yield identical results when used in the same contexts. We
demonstrate that this framework has a strong predictive power for the
generalization capabilities of Transformers. First, we derive and empirically
confirm that the training data required for two-hop generalization grows at
least quadratically with the token set size, and the training data efficiency
does not improve with 20x parameter scaling. Second, for compositional tasks
with path ambiguity where one variable affects the output through multiple
computational paths, we show that Transformers learn context-dependent state
representations that undermine both performance and interoperability. Third,
Chain-of-Thought supervision improves training data efficiency for multi-hop
tasks but still struggles with path ambiguity. Finally, we outline a
\emph{mechanism-based} taxonomy that distinguishes three ways neural networks
can generalize: structure-based (bounded by coverage), property-based
(leveraging algebraic invariances), and shared-operator (through function
reuse). This conceptual lens contextualizes our results and highlights where
new architectural ideas are needed to achieve systematic compositionally.
Overall, the coverage principle provides a unified lens for understanding
compositional reasoning, and underscores the need for fundamental architectural
or training innovations to achieve truly systematic compositionality.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [684] [Adaptive Prediction-Powered AutoEval with Reliability and Efficiency Guarantees](https://arxiv.org/abs/2505.18659)
*Sangwoo Park,Matteo Zecchin,Osvaldo Simeone*

Main category: stat.ML

TL;DR: 提出R-AutoEval+框架，通过动态调整对合成数据的依赖，提升AI模型评估的可靠性和样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型评估方法依赖真实数据成本高，而使用合成数据的自动评估方法虽降低成本但可能引入偏差且样本效率不足。

Method: R-AutoEval+框架自适应构建模型评估变量，动态调整对合成数据的依赖，在自动评估器不准确时回归传统方法。

Result: 实验证明R-AutoEval+在LLM权重量化优化和提示设计任务中具有可靠性和更高的样本效率。

Conclusion: R-AutoEval+在保证评估可靠性的同时，提升了样本效率，为AI模型选择提供了实用解决方案。

Abstract: Selecting artificial intelligence (AI) models, such as large language models
(LLMs), from multiple candidates requires accurate performance estimation. This
is ideally achieved through empirical evaluations involving abundant real-world
data. However, such evaluations are costly and impractical at scale. To address
this challenge, autoevaluation methods leverage synthetic data produced by
automated evaluators, such as LLMs-as-judges, reducing variance but potentially
introducing bias. Recent approaches have employed semi-supervised
prediction-powered inference (\texttt{PPI}) to correct for the bias of
autoevaluators. However, the use of autoevaluators may lead in practice to a
degradation in sample efficiency compared to conventional methods using only
real-world data. In this paper, we propose \texttt{R-AutoEval+}, a novel
framework that provides finite-sample reliability guarantees on the model
evaluation, while also ensuring an enhanced (or at least no worse) sample
efficiency compared to conventional methods. The key innovation of
\texttt{R-AutoEval+} is an adaptive construction of the model evaluation
variable, which dynamically tunes its reliance on synthetic data, reverting to
conventional methods when the autoevaluator is insufficiently accurate.
Experiments on the use of LLMs-as-judges for the optimization of quantization
settings for the weights of an LLM, and for prompt design in LLMs confirm the
reliability and efficiency of \texttt{R-AutoEval+}.

</details>


### [685] [Preconditioned Langevin Dynamics with Score-Based Generative Models for Infinite-Dimensional Linear Bayesian Inverse Problems](https://arxiv.org/abs/2505.18276)
*Lorenzo Baldassari,Josselin Garnier,Knut Solna,Maarten V. de Hoop*

Main category: stat.ML

TL;DR: 该论文分析了基于分数生成模型的Langevin动力学采样器在无限维函数空间中的表现，提出了预处理方法以保证收敛性，并给出了误差估计和全局收敛条件。


<details>
  <summary>Details</summary>
Motivation: 研究高维贝叶斯逆问题在无限维函数空间中的算法设计，以确保在问题离散化细化时的稳定性和收敛性。

Method: 使用基于分数生成模型（SGMs）的Langevin动力学采样器，直接在函数空间中定义，并引入预处理技术以防止数值不稳定。

Result: 首次推导出显式依赖分数近似误差的误差估计，提出了最优预处理器的存在性及其形式，保证了所有后验模态的均匀收敛速率。

Conclusion: 理论分析适用于高斯和非高斯先验，并通过实例验证了理论结果的正确性和有效性。

Abstract: Designing algorithms for solving high-dimensional Bayesian inverse problems
directly in infinite-dimensional function spaces - where such problems are
naturally formulated - is crucial to ensure stability and convergence as the
discretization of the underlying problem is refined. In this paper, we
contribute to this line of work by analyzing a widely used sampler for linear
inverse problems: Langevin dynamics driven by score-based generative models
(SGMs) acting as priors, formulated directly in function space. Building on the
theoretical framework for SGMs in Hilbert spaces, we give a rigorous definition
of this sampler in the infinite-dimensional setting and derive, for the first
time, error estimates that explicitly depend on the approximation error of the
score. As a consequence, we obtain sufficient conditions for global convergence
in Kullback-Leibler divergence on the underlying function space. Preventing
numerical instabilities requires preconditioning of the Langevin algorithm and
we prove the existence and the form of an optimal preconditioner. The
preconditioner depends on both the score error and the forward operator and
guarantees a uniform convergence rate across all posterior modes. Our analysis
applies to both Gaussian and a general class of non-Gaussian priors. Finally,
we present examples that illustrate and validate our theoretical findings.

</details>


### [686] [Operator Learning for Schrödinger Equation: Unitarity, Error Bounds, and Time Generalization](https://arxiv.org/abs/2505.18288)
*Yash Patel,Unique Subedi,Ambuj Tewari*

Main category: stat.ML

TL;DR: 该论文提出了一种保持弱幺正性的线性估计器，用于学习含时薛定谔方程的演化算子，显著提升了预测精度和时间泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经网络的替代方法常忽略薛定谔方程的基本性质（如线性性和幺正性），且缺乏理论误差保证和时间泛化能力。

Method: 引入保持弱幺正性的线性演化算子估计器，并建立预测误差和时间泛化的理论上下界。

Result: 在氢原子、离子阱和光学晶格等实际哈密顿系统中，相对误差比Fourier Neural Operator等方法低1-2个数量级。

Conclusion: 通过保持量子力学基本性质并建立理论保证，该方法显著提升了含时薛定谔方程演化算子的学习精度和泛化能力。

Abstract: We consider the problem of learning the evolution operator for the
time-dependent Schr\"{o}dinger equation, where the Hamiltonian may vary with
time. Existing neural network-based surrogates often ignore fundamental
properties of the Schr\"{o}dinger equation, such as linearity and unitarity,
and lack theoretical guarantees on prediction error or time generalization. To
address this, we introduce a linear estimator for the evolution operator that
preserves a weak form of unitarity. We establish both upper and lower bounds on
the prediction error that hold uniformly over all sufficiently smooth initial
wave functions. Additionally, we derive time generalization bounds that
quantify how the estimator extrapolates beyond the time points seen during
training. Experiments across real-world Hamiltonians -- including hydrogen
atoms, ion traps for qubit design, and optical lattices -- show that our
estimator achieves relative errors $10^{-2}$ to $10^{-3}$ times smaller than
state-of-the-art methods such as the Fourier Neural Operator and DeepONet.

</details>


### [687] [Online Statistical Inference of Constrained Stochastic Optimization via Random Scaling](https://arxiv.org/abs/2505.18327)
*Xinchen Du,Wanrong Zhu,Wei Biao Wu,Sen Na*

Main category: stat.ML

TL;DR: 本文提出了一种基于SSQP的在线推断方法——随机缩放，用于约束随机非线性优化问题，具有无需矩阵求逆和构建渐近有效置信区间的优势。


<details>
  <summary>Details</summary>
Motivation: 随着数据集的增长，在线推断方法对于实时决策变得至关重要。现有的方法在处理约束随机优化问题时存在局限性，需要新的解决方案。

Method: 本文提出了一种名为随机缩放的在线推断方法，基于SSQP迭代构建测试统计量，其极限分布不含未知参数，且无需矩阵求逆。

Result: 实验验证了随机缩放方法在非线性约束回归问题上的优越性能，能够构建渐近有效的置信区间。

Conclusion: 随机缩放方法在约束随机优化问题的在线推断中表现出色，具有计算效率和统计有效性。

Abstract: Constrained stochastic nonlinear optimization problems have attracted
significant attention for their ability to model complex real-world scenarios
in physics, economics, and biology. As datasets continue to grow, online
inference methods have become crucial for enabling real-time decision-making
without the need to store historical data. In this work, we develop an online
inference procedure for constrained stochastic optimization by leveraging a
method called Sketched Stochastic Sequential Quadratic Programming (SSQP). As a
direct generalization of sketched Newton methods, SSQP approximates the
objective with a quadratic model and the constraints with a linear model at
each step, then applies a sketching solver to inexactly solve the resulting
subproblem. Building on this design, we propose a new online inference
procedure called random scaling. In particular, we construct a test statistic
based on SSQP iterates whose limiting distribution is free of any unknown
parameters. Compared to existing online inference procedures, our approach
offers two key advantages: (i) it enables the construction of asymptotically
valid confidence intervals; and (ii) it is matrix-free, i.e. the computation
involves only primal-dual SSQP iterates $(\boldsymbol{x}_t,
\boldsymbol{\lambda}_t)$ without requiring any matrix inversions. We validate
our theory through numerical experiments on nonlinearly constrained regression
problems and demonstrate the superior performance of our random scaling method
over existing inference procedures.

</details>


### [688] [On the Mechanisms of Weak-to-Strong Generalization: A Theoretical Perspective](https://arxiv.org/abs/2505.18346)
*Behrad Moniri,Hamed Hassani*

Main category: stat.ML

TL;DR: 论文通过理论分析揭示了弱到强泛化的三种机制：学生模型通过正则化补偿、参数化对齐以及非线性特征学习，超越教师模型。


<details>
  <summary>Details</summary>
Motivation: 弱到强泛化现象广泛存在，但其机制尚不明确。本文旨在通过理论分析揭示其背后的核心机制。

Method: 通过分析简单模型（如岭回归和加权岭回归）及非线性多索引设置，研究学生模型如何超越教师模型。

Result: 发现三种机制：学生模型通过正则化补偿教师欠正则化、参数化对齐目标以及学习教师无法捕捉的难学习特征。

Conclusion: 弱到强泛化可通过学生模型的正则化优化、参数化对齐和特征学习能力实现，为理解该现象提供了理论依据。

Abstract: Weak-to-strong generalization, where a student model trained on imperfect
labels generated by a weaker teacher nonetheless surpasses that teacher, has
been widely observed but the mechanisms that enable it have remained poorly
understood. In this paper, through a theoretical analysis of simple models, we
uncover three core mechanisms that can drive this phenomenon. First, by
analyzing ridge regression, we study the interplay between the teacher and
student regularization and prove that a student can compensate for a teacher's
under-regularization and achieve lower test error. We also analyze the role of
the parameterization regime of the models. Second, by analyzing weighted ridge
regression, we show that a student model with a regularization structure more
aligned to the target, can outperform its teacher. Third, in a nonlinear
multi-index setting, we demonstrate that a student can learn easy,
task-specific features from the teacher while leveraging its own broader
pre-training to learn hard-to-learn features that the teacher cannot capture.

</details>


### [689] [Identifiability of latent causal graphical models without pure children](https://arxiv.org/abs/2505.18410)
*Seunghyun Lee,Yuqi Gu*

Main category: stat.ML

TL;DR: 该论文提出了一种在存在潜变量情况下识别因果图模型的新方法，通过放宽现有严格条件并引入双三角图条件，显著提升了模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有因果图模型识别方法通常要求每个潜变量有多个纯子节点或对潜变量图施加限制，且观测变量需同模态，这些条件在复杂现实数据中过于严格。

Method: 提出了一种非参数测量模型，允许任意观测变量类型和二元潜变量，并引入双三角图条件来保证整个因果图模型的可识别性。

Result: 所提条件显著放宽了流行的纯子节点条件，建立了可识别性的必要条件，并通过仿真验证了满足条件的潜结构可从数据中准确估计。

Conclusion: 该研究为复杂现实数据中的因果图模型识别提供了更宽松且实用的理论框架，揭示了可识别性的基本限制。

Abstract: This paper considers a challenging problem of identifying a causal graphical
model under the presence of latent variables. While various identifiability
conditions have been proposed in the literature, they often require multiple
pure children per latent variable or restrictions on the latent causal graph.
Furthermore, it is common for all observed variables to exhibit the same
modality. Consequently, the existing identifiability conditions are often too
stringent for complex real-world data. We consider a general nonparametric
measurement model with arbitrary observed variable types and binary latent
variables, and propose a double triangular graphical condition that guarantees
identifiability of the entire causal graphical model. The proposed condition
significantly relaxes the popular pure children condition. We also establish
necessary conditions for identifiability and provide valuable insights into
fundamental limits of identifiability. Simulation studies verify that latent
structures satisfying our conditions can be accurately estimated from data.

</details>


### [690] [LocalKMeans: Convergence of Lloyd's Algorithm with Distributed Local Iterations](https://arxiv.org/abs/2505.18420)
*Harsh Vardhan,Heng Zhu,Avishek Ghosh,Arya Mazumdar*

Main category: stat.ML

TL;DR: 本文提出了一种名为LocalKMeans的分布式K-means算法，通过本地迭代步骤减少机器间同步频率，分析了其性能与信号噪声比的关系。


<details>
  <summary>Details</summary>
Motivation: 研究在分布式环境下，如何高效运行经典的K-means算法（Lloyd算法），减少机器间的同步频率，同时保持算法性能。

Method: 提出LocalKMeans算法，在多个机器上并行运行Lloyd算法的本地迭代步骤，每L步同步一次，结合虚拟迭代方法与非凸非光滑目标函数的分析。

Result: 本地迭代步骤虽然减少了同步频率，但需要更高的信号噪声比来保证性能，与集中式设置相比存在一定代价。

Conclusion: LocalKMeans算法在分布式环境下有效，但需权衡本地迭代步长与信号噪声比的关系，为无监督学习方法的分布式实现提供了理论支持。

Abstract: In this paper, we analyze the classical $K$-means alternating-minimization
algorithm, also known as Lloyd's algorithm (Lloyd, 1956), for a mixture of
Gaussians in a data-distributed setting that incorporates local iteration
steps. Assuming unlabeled data distributed across multiple machines, we propose
an algorithm, LocalKMeans, that performs Lloyd's algorithm in parallel in the
machines by running its iterations on local data, synchronizing only every $L$
of such local steps. We characterize the cost of these local iterations against
the non-distributed setting, and show that the price paid for the local steps
is a higher required signal-to-noise ratio. While local iterations were
theoretically studied in the past for gradient-based learning methods, the
analysis of unsupervised learning methods is more involved owing to the
presence of latent variables, e.g. cluster identities, than that of an
iterative gradient-based algorithm. To obtain our results, we adapt a virtual
iterate method to work with a non-convex, non-smooth objective function, in
conjunction with a tight statistical analysis of Lloyd steps.

</details>


### [691] [On Minimax Estimation of Parameters in Softmax-Contaminated Mixture of Experts](https://arxiv.org/abs/2505.18455)
*Fanqi Yan,Huy Nguyen,Dung Le,Pedram Akbarian,Nhat Ho,Alessandro Rinaldo*

Main category: stat.ML

TL;DR: 该论文研究了softmax污染的混合专家模型（MoE）中门控和提示参数的最大似然估计收敛速度，揭示了参数可估计性在提示与预训练模型知识重叠时的挑战，并提出了可区分性概念。


<details>
  <summary>Details</summary>
Motivation: 尽管softmax污染的MoE模型在微调预训练模型以学习下游任务时非常流行，但其理论性质尚未被充分探索。本文旨在填补这一空白，研究参数估计的收敛速度及其统计特性。

Method: 论文通过最大似然估计方法分析门控和提示参数的收敛速度，并提出了一个新颖的可区分性概念来量化提示与预训练模型之间的知识重叠程度。

Result: 研究发现，当提示与预训练模型可区分时，所有参数都能达到极小极大最优估计速率；反之，估计速率会显著下降。数值实验验证了理论结果。

Conclusion: 论文揭示了softmax污染的MoE模型在微调过程中的统计特性，强调了可区分性对参数估计的重要性，为实际应用提供了理论指导。

Abstract: The softmax-contaminated mixture of experts (MoE) model is deployed when a
large-scale pre-trained model, which plays the role of a fixed expert, is
fine-tuned for learning downstream tasks by including a new contamination part,
or prompt, functioning as a new, trainable expert. Despite its popularity and
relevance, the theoretical properties of the softmax-contaminated MoE have
remained unexplored in the literature. In the paper, we study the convergence
rates of the maximum likelihood estimator of gating and prompt parameters in
order to gain insights into the statistical properties and potential challenges
of fine-tuning with a new prompt. We find that the estimability of these
parameters is compromised when the prompt acquires overlapping knowledge with
the pre-trained model, in the sense that we make precise by formulating a novel
analytic notion of distinguishability. Under distinguishability of the
pre-trained and prompt models, we derive minimax optimal estimation rates for
all the gating and prompt parameters. By contrast, when the distinguishability
condition is violated, these estimation rates become significantly slower due
to their dependence on the prompt convergence rate to the pre-trained model.
Finally, we empirically corroborate our theoretical findings through several
numerical experiments.

</details>


### [692] [Statistical Inference under Performativity](https://arxiv.org/abs/2505.18493)
*Xiang Li,Yunai Li,Huiying Zhong,Lihua Lei,Zhun Deng*

Main category: stat.ML

TL;DR: 该论文首次研究了预测反馈效应下的统计推断问题，提出了中心极限定理框架，并应用于基于预测驱动的推断方法，提升了政策制定中的参数估计精度。


<details>
  <summary>Details</summary>
Motivation: 社会科学和经济学中，基于预测的决策可能改变预测目标本身（预测反馈效应），但现有统计推断方法未考虑这一现象。论文旨在填补这一空白。

Method: 1. 建立预测反馈效应下的中心极限定理；2. 将该定理应用于预测驱动推断（PPI），结合小规模标注数据和大规模机器学习预测数据。

Result: 1. 实现了预测反馈场景下的置信区间构建和假设检验；2. 通过PPI方法显著提升了模型参数（政策）的估计精度和置信区域质量。数值实验验证了框架有效性。

Conclusion: 该研究为预测反馈效应下的统计推断提供了理论基础和实用工具，对政策制定、统计学和机器学习领域具有重要价值。

Abstract: Performativity of predictions refers to the phenomena that
prediction-informed decisions may influence the target they aim to predict,
which is widely observed in policy-making in social sciences and economics. In
this paper, we initiate the study of statistical inference under
performativity. Our contribution is two-fold. First, we build a central limit
theorem for estimation and inference under performativity, which enables
inferential purposes in policy-making such as constructing confidence intervals
or testing hypotheses. Second, we further leverage the derived central limit
theorem to investigate prediction-powered inference (PPI) under performativity,
which is based on a small labeled dataset and a much larger dataset of
machine-learning predictions. This enables us to obtain more precise estimation
and improved confidence regions for the model parameter (i.e., policy) of
interest in performative prediction. We demonstrate the power of our framework
by numerical experiments. To the best of our knowledge, this paper is the first
one to establish statistical inference under performativity, which brings up
new challenges and inference settings that we believe will add significant
values to policy-making, statistics, and machine learning.

</details>


### [693] [Scalable Gaussian Processes with Low-Rank Deep Kernel Decomposition](https://arxiv.org/abs/2505.18526)
*Yunqin Zhu,Henry Shaowu Yuchi,Yao Xie*

Main category: stat.ML

TL;DR: 该论文提出了一种基于Mercer定理的数据驱动深度核表示方法，通过神经网络直接表示低秩核，实现高效的高斯过程推断，无需使用诱导点，并在实验中展示了其在预测准确性、不确定性量化和计算效率上的优势。


<details>
  <summary>Details</summary>
Motivation: 传统深度核学习方法受限于基核的选择，推断成本高且常需稀疏近似。论文旨在解决这些问题，提出更灵活、高效的核表示方法。

Method: 利用Mercer定理，通过神经网络直接表示低秩核，构建完全数据驱动的深度核表示，支持基于变分推断框架的小批量训练，并提出方差校正程序以防止不确定性估计的过度自信。

Result: 在合成和真实数据上的实验表明，该方法在预测准确性、不确定性量化和计算效率方面优于现有方法。

Conclusion: 论文提出的深度核高斯过程方法在保持高效计算的同时，显著提升了模型的表达能力和不确定性量化效果。

Abstract: Kernels are key to encoding prior beliefs and data structures in Gaussian
process (GP) models. The design of expressive and scalable kernels has garnered
significant research attention. Deep kernel learning enhances kernel
flexibility by feeding inputs through a neural network before applying a
standard parametric form. However, this approach remains limited by the choice
of base kernels, inherits high inference costs, and often demands sparse
approximations. Drawing on Mercer's theorem, we introduce a fully data-driven,
scalable deep kernel representation where a neural network directly represents
a low-rank kernel through a small set of basis functions. This construction
enables highly efficient exact GP inference in linear time and memory without
invoking inducing points. It also supports scalable mini-batch training based
on a principled variational inference framework. We further propose a simple
variance correction procedure to guard against overconfidence in uncertainty
estimates. Experiments on synthetic and real-world data demonstrate the
advantages of our deep kernel GP in terms of predictive accuracy, uncertainty
quantification, and computational efficiency.

</details>


### [694] [Non-Stationary Lipschitz Bandits](https://arxiv.org/abs/2505.18871)
*Nicolas Nguyen,Solenne Gaucher,Claire Vernade*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the problem of non-stationary Lipschitz bandits, where the number of
actions is infinite and the reward function, satisfying a Lipschitz assumption,
can change arbitrarily over time. We design an algorithm that adaptively tracks
the recently introduced notion of significant shifts, defined by large
deviations of the cumulative reward function. To detect such reward changes,
our algorithm leverages a hierarchical discretization of the action space.
Without requiring any prior knowledge of the non-stationarity, our algorithm
achieves a minimax-optimal dynamic regret bound of
$\mathcal{\widetilde{O}}(\tilde{L}^{1/3}T^{2/3})$, where $\tilde{L}$ is the
number of significant shifts and $T$ the horizon. This result provides the
first optimal guarantee in this setting.

</details>


### [695] [Marginal Fairness: Fair Decision-Making under Risk Measures](https://arxiv.org/abs/2505.18895)
*Fei Huang,Silvana M. Pesenti*

Main category: stat.ML

TL;DR: 本文提出了一种新的个体公平性概念——边际公平性，用于在存在性别、种族等受保护属性的情况下实现公平决策。通过两阶段建模流程（预测建模和决策制定）和广义失真风险度量调整，确保决策不受受保护属性影响。


<details>
  <summary>Details</summary>
Motivation: 在高度监管的行业（如保险和金融）中，如何在决策过程中避免受保护属性（如性别、种族）的影响，同时满足风险敏感性和监管要求，是一个重要挑战。本文旨在解决这一问题。

Method: 采用两阶段建模流程：1) 基于受保护和非受保护协变量预测目标变量；2) 仅基于非受保护协变量应用广义失真风险度量进行决策，并通过调整风险度量确保决策对受保护属性不敏感。此外，利用级联敏感性概念扩展框架以捕捉协变量间的依赖关系。

Result: 数值研究和汽车保险数据集的实证实施表明，该框架能够在实际应用中有效实现公平决策，确保决策结果不受受保护属性的影响。

Conclusion: 边际公平性框架为在风险敏感和监管约束下实现公平决策提供了可行方案，并通过级联敏感性分析进一步增强了其适用性。

Abstract: This paper introduces marginal fairness, a new individual fairness notion for
equitable decision-making in the presence of protected attributes such as
gender, race, and religion. This criterion ensures that decisions based on
generalized distortion risk measures are insensitive to distributional
perturbations in protected attributes, regardless of whether these attributes
are continuous, discrete, categorical, univariate, or multivariate. To
operationalize this notion and reflect real-world regulatory environments (such
as the EU gender-neutral pricing regulation), we model business decision-making
in highly regulated industries (such as insurance and finance) as a two-step
process: (i) a predictive modeling stage, in which a prediction function for
the target variable (e.g., insurance losses) is estimated based on both
protected and non-protected covariates; and (ii) a decision-making stage, in
which a generalized distortion risk measure is applied to the target variable,
conditional only on non-protected covariates, to determine the decision. In
this second step, we modify the risk measure such that the decision becomes
insensitive to the protected attribute, thus enforcing fairness to ensure
equitable outcomes under risk-sensitive, regulatory constraints. Furthermore,
by utilizing the concept of cascade sensitivity, we extend the marginal
fairness framework to capture how dependencies between covariates propagate the
influence of protected attributes through the modeling pipeline. A numerical
study and an empirical implementation using an auto insurance dataset
demonstrate how the framework can be applied in practice.

</details>


### [696] [On the Role of Label Noise in the Feature Learning Process](https://arxiv.org/abs/2505.18909)
*Andi Han,Wei Huang,Zhanpeng Zhou,Gang Niu,Wuyang Chen,Junchi Yan,Akiko Takeda,Taiji Suzuki*

Main category: stat.ML

TL;DR: 论文分析了带噪声标签的深度学习，理论揭示了模型训练的两个阶段：先学习干净样本的信号，后过拟合噪声样本，并提出早期停止和样本选择的有效性。


<details>
  <summary>Details</summary>
Motivation: 带噪声标签的深度学习面临泛化能力下降的挑战，需要从特征学习角度理解噪声的影响机制。

Method: 采用信号-噪声数据分布假设，理论分析双层CNN在标签噪声下的训练动态。

Result: 模型分两阶段学习：先拟合干净样本信号（泛化性好），后过拟合噪声（导致记忆噪声）。实验验证了理论。

Conclusion: 为早期停止和样本选择提供了理论依据，证明其能有效抑制噪声标签的负面影响。

Abstract: Deep learning with noisy labels presents significant challenges. In this
work, we theoretically characterize the role of label noise from a feature
learning perspective. Specifically, we consider a signal-noise data
distribution, where each sample comprises a label-dependent signal and
label-independent noise, and rigorously analyze the training dynamics of a
two-layer convolutional neural network under this data setup, along with the
presence of label noise. Our analysis identifies two key stages. In Stage I,
the model perfectly fits all the clean samples (i.e., samples without label
noise) while ignoring the noisy ones (i.e., samples with noisy labels). During
this stage, the model learns the signal from the clean samples, which
generalizes well on unseen data. In Stage II, as the training loss converges,
the gradient in the direction of noise surpasses that of the signal, leading to
overfitting on noisy samples. Eventually, the model memorizes the noise present
in the noisy samples and degrades its generalization ability. Furthermore, our
analysis provides a theoretical basis for two widely used techniques for
tackling label noise: early stopping and sample selection. Experiments on both
synthetic and real-world setups validate our theory.

</details>


### [697] [ALPCAHUS: Subspace Clustering for Heteroscedastic Data](https://arxiv.org/abs/2505.18918)
*Javier Salazar Cavazos,Jeffrey A Fessler,Laura Balzano*

Main category: stat.ML

TL;DR: 本文提出了一种名为ALPCAHUS的异方差子空间聚类方法，通过估计样本噪声方差改进数据低秩结构的子空间基估计，优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 现有PCA方法在处理多子空间数据时未考虑样本噪声异质性，导致聚类效果受限。本文旨在解决混合质量数据的异方差问题。

Method: 基于K-子空间(KSS)框架，扩展异方差PCA方法LR-ALPCAH，开发了能估计样本噪声方差的ALPCAHUS聚类算法。

Result: 仿真和真实数据实验表明，考虑数据异方差性显著提升了聚类性能。

Conclusion: ALPCAHUS通过显式建模噪声异质性，在子空间聚类任务中展现出优越性，代码已开源。

Abstract: Principal component analysis (PCA) is a key tool in the field of data
dimensionality reduction. Various methods have been proposed to extend PCA to
the union of subspace (UoS) setting for clustering data that come from multiple
subspaces like K-Subspaces (KSS). However, some applications involve
heterogeneous data that vary in quality due to noise characteristics associated
with each data sample. Heteroscedastic methods aim to deal with such mixed data
quality. This paper develops a heteroscedastic-focused subspace clustering
method, named ALPCAHUS, that can estimate the sample-wise noise variances and
use this information to improve the estimate of the subspace bases associated
with the low-rank structure of the data. This clustering algorithm builds on
K-Subspaces (KSS) principles by extending the recently proposed heteroscedastic
PCA method, named LR-ALPCAH, for clusters with heteroscedastic noise in the UoS
setting. Simulations and real-data experiments show the effectiveness of
accounting for data heteroscedasticity compared to existing clustering
algorithms. Code available at https://github.com/javiersc1/ALPCAHUS.

</details>


### [698] [Optimal Conformal Prediction under Epistemic Uncertainty](https://arxiv.org/abs/2505.19033)
*Alireza Javanmardi,Soroush H. Zargarbashi,Santo M. A. R. Thies,Willem Waegeman,Aleksandar Bojchevski,Eyke Hüllermeier*

Main category: stat.ML

TL;DR: 本文探讨了如何将二阶预测器融入共形预测框架，提出了伯努利预测集（BPS），确保在二阶预测有效时获得最小预测集，并在预测失效时通过共形风险控制保证边际覆盖。


<details>
  <summary>Details</summary>
Motivation: 共形预测（CP）是一种流行的不确定性量化框架，但现有方法主要基于一阶概率预测器。二阶预测器（如贝叶斯模型）能同时表示偶然和认知不确定性，但如何将其融入CP仍是一个开放问题。

Method: 提出伯努利预测集（BPS），在二阶预测有效时生成最小预测集；若预测失效，则应用共形风险控制保证边际覆盖。

Result: BPS在一阶预测下退化为自适应预测集（APS），在二阶预测有效时实现条件覆盖，失效时仍能保持边际覆盖。

Conclusion: BPS为二阶预测器融入CP提供了有效方案，平衡了预测集大小与覆盖保证，拓展了CP在不确定性量化中的应用。

Abstract: Conformal prediction (CP) is a popular frequentist framework for representing
uncertainty by providing prediction sets that guarantee coverage of the true
label with a user-adjustable probability. In most applications, CP operates on
confidence scores coming from a standard (first-order) probabilistic predictor
(e.g., softmax outputs). Second-order predictors, such as credal set predictors
or Bayesian models, are also widely used for uncertainty quantification and are
known for their ability to represent both aleatoric and epistemic uncertainty.
Despite their popularity, there is still an open question on ``how they can be
incorporated into CP''. In this paper, we discuss the desiderata for CP when
valid second-order predictions are available. We then introduce Bernoulli
prediction sets (BPS), which produce the smallest prediction sets that ensure
conditional coverage in this setting. When given first-order predictions, BPS
reduces to the well-known adaptive prediction sets (APS). Furthermore, when the
validity assumption on the second-order predictions is compromised, we apply
conformal risk control to obtain a marginal coverage guarantee while still
accounting for epistemic uncertainty.

</details>


### [699] [When Models Don't Collapse: On the Consistency of Iterative MLE](https://arxiv.org/abs/2505.19046)
*Daniel Barzilai,Ohad Shamir*

Main category: stat.ML

TL;DR: 论文研究了生成模型迭代训练中可能出现的模型崩溃问题，通过理论分析证明了在某些条件下可以避免崩溃，同时也指出缺乏必要假设时崩溃会快速发生。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的广泛应用，模型迭代训练过程中使用前代模型生成的数据可能导致性能严重下降（模型崩溃）。然而，现有研究对崩溃严重性的结论不一，因此需要理论分析明确崩溃发生的条件和避免方法。

Method: 采用最大似然估计（MLE）的理论框架，在逐步添加合成数据到原始数据集的自然设定下，结合标准假设（如MLE渐近一致性和正态性），进行非渐近边界分析。

Result: 证明在标准假设下，即使真实数据比例趋近于零，模型崩溃仍可避免；但若缺乏必要假设（如超越MLE一致性的条件），崩溃可能快速发生，即使训练集中仍包含原始数据。

Conclusion: 研究首次严格证明了迭代生成建模中数据累积可能导致快速模型崩溃的现象，同时明确了避免崩溃的理论条件，为生成模型的可持续训练提供了理论依据。

Abstract: The widespread use of generative models has created a feedback loop, in which
each generation of models is trained on data partially produced by its
predecessors. This process has raised concerns about \emph{model collapse}: A
critical degradation in performance caused by repeated training on synthetic
data. However, different analyses in the literature have reached different
conclusions as to the severity of model collapse. As such, it remains unclear
how concerning this phenomenon is, and under which assumptions it can be
avoided. To address this, we theoretically study model collapse for maximum
likelihood estimation (MLE), in a natural setting where synthetic data is
gradually added to the original data set. Under standard assumptions (similar
to those long used for proving asymptotic consistency and normality of MLE), we
establish non-asymptotic bounds showing that collapse can be avoided even as
the fraction of real data vanishes. On the other hand, we prove that some
assumptions (beyond MLE consistency) are indeed necessary: Without them, model
collapse can occur arbitrarily quickly, even when the original data is still
present in the training set. To the best of our knowledge, these are the first
rigorous examples of iterative generative modeling with accumulating data that
rapidly leads to model collapse.

</details>


### [700] [Statistical inference for Linear Stochastic Approximation with Markovian Noise](https://arxiv.org/abs/2505.19102)
*Sergey Samsonov,Marina Sheshukova,Eric Moulines,Alexey Naumov*

Main category: stat.ML

TL;DR: 本文推导了在马尔可夫噪声驱动下线性随机逼近算法Polyak-Ruppert平均迭代的非渐近Berry-Esseen界，并建立了乘数块自助法构建置信区间的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究在马尔可夫噪声条件下，线性随机逼近算法的非渐近收敛性及自助法置信区间的有效性，填补了该领域非渐近保证的空白。

Method: 采用非渐近分析方法，推导Polyak-Ruppert平均迭代的Berry-Esseen界，并验证乘数块自助法的有效性。

Result: 获得了Kolmogorov距离下收敛速度为O(n^{-1/4})的高斯极限，并恢复了经典O(n^{-1/8})的渐近方差估计速率。

Conclusion: 本文首次为马尔可夫噪声下的随机逼近算法提供了自助法置信区间的非渐近收敛保证，推动了该领域的理论发展。

Abstract: In this paper we derive non-asymptotic Berry-Esseen bounds for Polyak-Ruppert
averaged iterates of the Linear Stochastic Approximation (LSA) algorithm driven
by the Markovian noise. Our analysis yields $\mathcal{O}(n^{-1/4})$ convergence
rates to the Gaussian limit in the Kolmogorov distance. We further establish
the non-asymptotic validity of a multiplier block bootstrap procedure for
constructing the confidence intervals, guaranteeing consistent inference under
Markovian sampling. Our work provides the first non-asymptotic guarantees on
the rate of convergence of bootstrap-based confidence intervals for stochastic
approximation with Markov noise. Moreover, we recover the classical rate of
order $\mathcal{O}(n^{-1/8})$ up to logarithmic factors for estimating the
asymptotic variance of the iterates of the LSA algorithm.

</details>


### [701] [Uncertainty Quantification for Physics-Informed Neural Networks with Extended Fiducial Inference](https://arxiv.org/abs/2505.19136)
*Frank Shih,Zhenghao Jiang,Faming Liang*

Main category: stat.ML

TL;DR: 本文提出了一种基于扩展基准推断（EFI）的新方法，用于为物理信息神经网络（PINNs）提供严格的UQ，克服了贝叶斯和Dropout方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习中，神经网络被广泛应用于解决复杂问题，但现有的不确定性量化方法（如贝叶斯或Dropout）存在先验分布或Dropout率无法仅通过观测数据确定的局限性。

Method: 提出了一种基于扩展基准推断（EFI）的新方法，利用窄颈超网络学习PINN参数，并通过观测数据中的随机误差量化不确定性。

Result: 该方法能够仅基于观测数据构建可靠的置信集，显著提升了PINNs的可靠性、可解释性及在现实科学和工程问题中的适用性。

Conclusion: 该方法不仅为PINNs提供了新的不确定性量化框架，还扩展了EFI在大规模模型中的应用，无需稀疏超网络，显著提升了统计推断的自动性和鲁棒性。

Abstract: Uncertainty quantification (UQ) in scientific machine learning is
increasingly critical as neural networks are widely adopted to tackle complex
problems across diverse scientific disciplines. For physics-informed neural
networks (PINNs), a prominent model in scientific machine learning, uncertainty
is typically quantified using Bayesian or dropout methods. However, both
approaches suffer from a fundamental limitation: the prior distribution or
dropout rate required to construct honest confidence sets cannot be determined
without additional information. In this paper, we propose a novel method within
the framework of extended fiducial inference (EFI) to provide rigorous
uncertainty quantification for PINNs. The proposed method leverages a
narrow-neck hyper-network to learn the parameters of the PINN and quantify
their uncertainty based on imputed random errors in the observations. This
approach overcomes the limitations of Bayesian and dropout methods, enabling
the construction of honest confidence sets based solely on observed data. This
advancement represents a significant breakthrough for PINNs, greatly enhancing
their reliability, interpretability, and applicability to real-world scientific
and engineering challenges. Moreover, it establishes a new theoretical
framework for EFI, extending its application to large-scale models, eliminating
the need for sparse hyper-networks, and significantly improving the
automaticity and robustness of statistical inference.

</details>


### [702] [PIGPVAE: Physics-Informed Gaussian Process Variational Autoencoders](https://arxiv.org/abs/2505.19320)
*Michail Spitieris,Massimiliano Ruocco,Abdulmajid Murad,Alessandro Nocente*

Main category: stat.ML

TL;DR: 提出一种结合物理约束的生成模型PIGPVAE，能在小数据场景下生成高质量合成数据，并解决未建模动态问题。


<details>
  <summary>Details</summary>
Motivation: 现有生成AI依赖大数据训练，而物理模型难以捕捉复杂时序依赖，需结合两者优势提升小数据生成效果。

Method: 在VAE中嵌入物理模型，引入高斯过程潜在变量处理未建模动态，并添加正则化保证数据一致性。

Result: 在室内温度数据上实现SOTA性能，且能生成超出观测分布的逼真样本。

Conclusion: PIGPVAE通过物理-数据混合建模，显著提升了小数据生成质量和分布外泛化能力。

Abstract: Recent advances in generative AI offer promising solutions for synthetic data
generation but often rely on large datasets for effective training. To address
this limitation, we propose a novel generative model that learns from limited
data by incorporating physical constraints to enhance performance.
Specifically, we extend the VAE architecture by incorporating physical models
in the generative process, enabling it to capture underlying dynamics more
effectively. While physical models provide valuable insights, they struggle to
capture complex temporal dependencies present in real-world data. To bridge
this gap, we introduce a discrepancy term to account for unmodeled dynamics,
represented within a latent Gaussian Process VAE (GPVAE). Furthermore, we apply
regularization to ensure the generated data aligns closely with observed data,
enhancing both the diversity and accuracy of the synthetic samples. The
proposed method is applied to indoor temperature data, achieving
state-of-the-art performance. Additionally, we demonstrate that PIGPVAE can
produce realistic samples beyond the observed distribution, highlighting its
robustness and usefulness under distribution shifts.

</details>


### [703] [Adaptive Diffusion Guidance via Stochastic Optimal Control](https://arxiv.org/abs/2505.19367)
*Iskander Azangulov,Peter Potaptchik,Qinyu Li,Eddie Aamari,George Deligiannidis,Judith Rousseau*

Main category: stat.ML

TL;DR: 本文提出了一种理论框架和随机最优控制方法，用于动态调整扩散模型中的引导权重，以提高生成质量和条件生成效果。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型中的引导权重调度方法缺乏理论依据，主要依赖启发式方法，限制了模型性能的进一步提升。

Method: 通过理论分析引导强度与分类器置信度的关系，并引入随机最优控制框架，将引导调度建模为自适应优化问题。

Result: 建立了动态调整引导权重的理论基础，实现了更有效的扩散模型引导策略。

Conclusion: 该研究为扩散模型的引导调度提供了理论支持和优化方法，显著提升了生成质量和条件生成效果。

Abstract: Guidance is a cornerstone of modern diffusion models, playing a pivotal role
in conditional generation and enhancing the quality of unconditional samples.
However, current approaches to guidance scheduling--determining the appropriate
guidance weight--are largely heuristic and lack a solid theoretical foundation.
This work addresses these limitations on two fronts. First, we provide a
theoretical formalization that precisely characterizes the relationship between
guidance strength and classifier confidence. Second, building on this insight,
we introduce a stochastic optimal control framework that casts guidance
scheduling as an adaptive optimization problem. In this formulation, guidance
strength is not fixed but dynamically selected based on time, the current
sample, and the conditioning class, either independently or in combination. By
solving the resulting control problem, we establish a principled foundation for
more effective guidance in diffusion models.

</details>


### [704] [Uniform convergence of the smooth calibration error and its relationship with functional gradient](https://arxiv.org/abs/2505.19396)
*Futoshi Futami,Atsushi Nitanda*

Main category: stat.ML

TL;DR: 该论文提出了一种理论框架，用于分析学习算法在保持高准确性和良好校准性方面的能力，并通过三种代表性算法验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在高风险应用中，概率预测的校准性至关重要。然而，现有研究对哪些学习算法能同时实现高准确性和良好校准性的理论理解有限，且多数研究仅在限制性设置下提供实证或理论保证。

Method: 论文聚焦于平滑校准误差（CE），提出了一个均匀收敛界，表明平滑CE可通过训练数据集上的平滑CE与泛化间隙之和来界定。此外，证明了损失函数的函数梯度能有效控制训练平滑CE，并分析了梯度提升树、核提升和两层神经网络三种算法。

Result: 研究结果为每种算法推导了分类和校准性能同时得到保证的条件，为设计具有可证明校准保证的可靠概率模型提供了新的理论见解和实践指导。

Conclusion: 该论文通过理论分析和算法验证，填补了学习算法在校准性理论理解上的空白，为实际应用中的模型设计提供了重要参考。

Abstract: Calibration is a critical requirement for reliable probabilistic prediction,
especially in high-risk applications. However, the theoretical understanding of
which learning algorithms can simultaneously achieve high accuracy and good
calibration remains limited, and many existing studies provide empirical
validation or a theoretical guarantee in restrictive settings. To address this
issue, in this work, we focus on the smooth calibration error (CE) and provide
a uniform convergence bound, showing that the smooth CE is bounded by the sum
of the smooth CE over the training dataset and a generalization gap. We further
prove that the functional gradient of the loss function can effectively control
the training smooth CE. Based on this framework, we analyze three
representative algorithms: gradient boosting trees, kernel boosting, and
two-layer neural networks. For each, we derive conditions under which both
classification and calibration performances are simultaneously guaranteed. Our
results offer new theoretical insights and practical guidance for designing
reliable probabilistic models with provable calibration guarantees.

</details>


### [705] [Information-theoretic Generalization Analysis for VQ-VAEs: A Role of Latent Variables](https://arxiv.org/abs/2505.19470)
*Futoshi Futami,Masahiro Fujisawa*

Main category: stat.ML

TL;DR: 该论文将信息论泛化分析扩展到具有离散潜在空间的VQ-VAE，提出了一种新的数据依赖先验，分析了潜在变量、泛化与数据生成的关系，并推导了重建损失的泛化误差界限。


<details>
  <summary>Details</summary>
Motivation: 尽管潜在变量在编码器-解码器模型中至关重要，但在无监督学习（如VAE）中的理论性质（如泛化）研究不足，本文旨在填补这一空白。

Method: 扩展信息论泛化分析至VQ-VAE，引入数据依赖先验，分析潜在变量、泛化与数据生成的关系。

Result: 推导了VQ-VAE重建损失的泛化误差界限，仅依赖于潜在变量和编码器的复杂度；给出了生成数据与真实数据分布间2-Wasserstein距离的上界。

Conclusion: 潜在变量的正则化对数据生成性能有重要影响，为VQ-VAE的理论分析提供了新视角。

Abstract: Latent variables (LVs) play a crucial role in encoder-decoder models by
enabling effective data compression, prediction, and generation. Although their
theoretical properties, such as generalization, have been extensively studied
in supervised learning, similar analyses for unsupervised models such as
variational autoencoders (VAEs) remain insufficiently underexplored. In this
work, we extend information-theoretic generalization analysis to
vector-quantized (VQ) VAEs with discrete latent spaces, introducing a novel
data-dependent prior to rigorously analyze the relationship among LVs,
generalization, and data generation. We derive a novel generalization error
bound of the reconstruction loss of VQ-VAEs, which depends solely on the
complexity of LVs and the encoder, independent of the decoder. Additionally, we
provide the upper bound of the 2-Wasserstein distance between the distributions
of the true data and the generated data, explaining how the regularization of
the LVs contributes to the data generation performance.

</details>


### [706] [Accelerating Nash Learning from Human Feedback via Mirror Prox](https://arxiv.org/abs/2505.19731)
*Daniil Tiapkin,Daniele Calandriello,Denis Belomestny,Eric Moulines,Alexey Naumov,Kashif Rasul,Michal Valko,Pierre Menard*

Main category: stat.ML

TL;DR: 论文提出Nash-MP算法，通过Mirror Prox优化方案实现快速稳定的纳什均衡收敛，适用于人类反馈学习，并在大语言模型微调中展示竞争力。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF依赖奖励模型和特定偏好结构（如Bradley-Terry模型），可能无法准确捕捉人类偏好的复杂性（如不可传递性）。NLHF通过将问题建模为纳什均衡博弈提供更直接的解决方案。

Method: 引入Nash-MP算法，利用Mirror Prox优化方案实现快速收敛，并提出近似版本，通过随机策略梯度估计近端步骤，更接近实际应用。

Result: 理论分析表明Nash-MP具有最后一次迭代线性收敛性，KL散度以(1+2β)^{-N/2}速率下降，且收敛率与动作空间大小无关。实验显示其在大语言模型微调中性能优越。

Conclusion: Nash-MP为NLHF提供了一种高效稳定的算法，理论保证其快速收敛，实际应用表现优异，兼容现有方法。

Abstract: Traditional Reinforcement Learning from Human Feedback (RLHF) often relies on
reward models, frequently assuming preference structures like the Bradley-Terry
model, which may not accurately capture the complexities of real human
preferences (e.g., intransitivity). Nash Learning from Human Feedback (NLHF)
offers a more direct alternative by framing the problem as finding a Nash
equilibrium of a game defined by these preferences. In this work, we introduce
Nash Mirror Prox ($\mathtt{Nash-MP}$), an online NLHF algorithm that leverages
the Mirror Prox optimization scheme to achieve fast and stable convergence to
the Nash equilibrium. Our theoretical analysis establishes that Nash-MP
exhibits last-iterate linear convergence towards the $\beta$-regularized Nash
equilibrium. Specifically, we prove that the KL-divergence to the optimal
policy decreases at a rate of order $(1+2\beta)^{-N/2}$, where $N$ is a number
of preference queries. We further demonstrate last-iterate linear convergence
for the exploitability gap and uniformly for the span semi-norm of
log-probabilities, with all these rates being independent of the size of the
action space. Furthermore, we propose and analyze an approximate version of
Nash-MP where proximal steps are estimated using stochastic policy gradients,
making the algorithm closer to applications. Finally, we detail a practical
implementation strategy for fine-tuning large language models and present
experiments that demonstrate its competitive performance and compatibility with
existing methods.

</details>


### [707] [Weighted Leave-One-Out Cross Validation](https://arxiv.org/abs/2505.19737)
*Luc Pronzato,Maria-João Rendas*

Main category: stat.ML

TL;DR: 该论文提出了一种加权留一交叉验证方法，用于在高斯过程框架下更精确地估计积分平方误差，相比传统方法显著提升了估计精度。


<details>
  <summary>Details</summary>
Motivation: 传统留一交叉验证在估计函数近似误差时存在精度不足的问题，尤其在基于高斯过程的函数拟合场景中需要更稳健的误差评估方法。

Method: 通过构建基于平方留一残差的最佳线性估计量，对未采样点处的预测误差平方进行加权估计，并分析了该方法对高斯过程核函数选择的鲁棒性。

Result: 理论分析和数值实验表明，加权留一法的积分平方误差估计精度显著优于传统非加权方法，且对核函数选择具有良好鲁棒性。

Conclusion: 该方法为高斯过程模型提供了一种更精确的误差估计工具，在模型选择等应用中展现出实用价值。

Abstract: We present a weighted version of Leave-One-Out (LOO) cross-validation for
estimating the Integrated Squared Error (ISE) when approximating an unknown
function by a predictor that depends linearly on evaluations of the function
over a finite collection of sites. The method relies on the construction of the
best linear estimator of the squared prediction error at an arbitrary unsampled
site based on squared LOO residuals, assuming that the function is a
realization of a Gaussian Process (GP). A theoretical analysis of performance
of the ISE estimator is presented, and robustness with respect to the choice of
the GP kernel is investigated first analytically, then through numerical
examples. Overall, the estimation of ISE is significantly more precise than
with classical, unweighted, LOO cross validation. Application to model
selection is briefly considered through examples.

</details>


### [708] [Efficient Deconvolution in Populational Inverse Problems](https://arxiv.org/abs/2505.19841)
*Arnaud Vadeboncoeur,Mark Girolami,Andrew M. Stuart*

Main category: stat.ML

TL;DR: 该论文提出了一种利用大数据集从相同物理过程的不同实例中同时解卷积噪声分布和识别物理过程参数分布的方法，并展示了其在多孔介质流动、阻尼弹性动力学和简化大气动力学模型中的应用。


<details>
  <summary>Details</summary>
Motivation: 随着数据可用性的增加，解决分布反演问题的潜力增大，但主要障碍是当观测噪声分布未知时的盲解卷积问题。论文旨在利用来自物理系统集合的数据信息来执行解卷积。

Method: 论文提出了一种方法，利用从相同物理过程的不同实例中收集的大数据集，同时解卷积数据噪声分布并识别物理过程的参数分布。通过定义一个损失函数来表征观测数据与数学模型输出之间的匹配度，并使用改进的梯度下降算法来解决这个耦合问题。此外，还提出了一种基于自适应经验测量的主动学习方案，以训练一个在感兴趣参数区域准确的替代模型。

Result: 该方法在多孔介质流动、阻尼弹性动力学和简化大气动力学模型中得到了验证，证明了其有效性和实用性。

Conclusion: 论文提出的方法能够有效地解决分布反演问题，特别是在观测噪声分布未知的情况下，通过利用大数据集和特定的噪声模型结构，实现了对物理过程参数分布和噪声分布的同时识别。

Abstract: This work is focussed on the inversion task of inferring the distribution
over parameters of interest leading to multiple sets of observations. The
potential to solve such distributional inversion problems is driven by
increasing availability of data, but a major roadblock is blind deconvolution,
arising when the observational noise distribution is unknown. However, when
data originates from collections of physical systems, a population, it is
possible to leverage this information to perform deconvolution. To this end, we
propose a methodology leveraging large data sets of observations, collected
from different instantiations of the same physical processes, to simultaneously
deconvolve the data corrupting noise distribution, and to identify the
distribution over model parameters defining the physical processes. A
parameter-dependent mathematical model of the physical process is employed. A
loss function characterizing the match between the observed data and the output
of the mathematical model is defined; it is minimized as a function of the both
the parameter inputs to the model of the physics and the parameterized
observational noise. This coupled problem is addressed with a modified gradient
descent algorithm that leverages specific structure in the noise model.
Furthermore, a new active learning scheme is proposed, based on adaptive
empirical measures, to train a surrogate model to be accurate in parameter
regions of interest; this approach accelerates computation and enables
automatic differentiation of black-box, potentially nondifferentiable, code
computing parameter-to-solution maps. The proposed methodology is demonstrated
on porous medium flow, damped elastodynamics, and simplified models of
atmospheric dynamics.

</details>


### [709] [Linear Bandits with Non-i.i.d. Noise](https://arxiv.org/abs/2505.20017)
*Baptiste Abélès,Eugenio Clerico,Hamish Flynn,Gergely Neu*

Main category: stat.ML

TL;DR: 论文放宽了线性随机bandit问题中噪声独立同分布的假设，提出针对具有时间衰减依赖性的次高斯噪声的新算法，并证明其遗憾界与依赖强度衰减率相关。


<details>
  <summary>Details</summary>
Motivation: 传统线性随机bandit问题假设观测噪声独立同分布(i.i.d.)，这一假设过于严格。本文旨在放宽该假设，允许噪声具有时间衰减的依赖性，使模型更贴近现实场景。

Method: 利用序列概率分配的归约方案构建新的置信序列，基于'面对不确定性保持乐观'原则设计bandit算法。

Result: 所提算法在几何混合噪声下能恢复标准遗憾率（乘以混合时间因子），遗憾界与观测间依赖强度的衰减率直接相关。

Conclusion: 通过引入衰减依赖性噪声模型和新型置信序列，本文扩展了线性bandit的理论框架，为实际应用中常见的相关噪声提供了解决方案。

Abstract: We study the linear stochastic bandit problem, relaxing the standard i.i.d.
assumption on the observation noise. As an alternative to this restrictive
assumption, we allow the noise terms across rounds to be sub-Gaussian but
interdependent, with dependencies that decay over time. To address this
setting, we develop new confidence sequences using a recently introduced
reduction scheme to sequential probability assignment, and use these to derive
a bandit algorithm based on the principle of optimism in the face of
uncertainty. We provide regret bounds for the resulting algorithm, expressed in
terms of the decay rate of the strength of dependence between observations.
Among other results, we show that our bounds recover the standard rates up to a
factor of the mixing time for geometrically mixing observation noise.

</details>


### [710] [No Free Lunch: Non-Asymptotic Analysis of Prediction-Powered Inference](https://arxiv.org/abs/2505.20178)
*Pranav Mani,Peng Xu,Zachary C. Lipton,Michael Oberst*

Main category: stat.ML

TL;DR: PPI++在有限样本分析下并非总是优于纯金标准标签，其表现取决于伪标签与金标准的相关性及样本量。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明PPI++在渐近情况下总能优于纯金标准标签，但未考虑有限样本的实际表现。本文旨在揭示其真实性能边界。

Method: 通过精确的有限样本分析，研究PPI++在均值估计问题上的误差，并推导出性能优劣的明确条件。

Result: PPI++仅在伪标签与金标准的相关性超过特定阈值（如高斯数据需≥1/√(n-2)）时表现更优，否则可能更差。实验验证了理论结果。

Conclusion: PPI++的优势具有条件性，需根据数据质量和样本量谨慎选择使用场景。

Abstract: Prediction-Powered Inference (PPI) is a popular strategy for combining
gold-standard and possibly noisy pseudo-labels to perform statistical
estimation. Prior work has shown an asymptotic "free lunch" for PPI++, an
adaptive form of PPI, showing that the *asymptotic* variance of PPI++ is always
less than or equal to the variance obtained from using gold-standard labels
alone. Notably, this result holds *regardless of the quality of the
pseudo-labels*. In this work, we demystify this result by conducting an exact
finite-sample analysis of the estimation error of PPI++ on the mean estimation
problem. We give a "no free lunch" result, characterizing the settings (and
sample sizes) where PPI++ has provably worse estimation error than using
gold-standard labels alone. Specifically, PPI++ will outperform if and only if
the correlation between pseudo- and gold-standard is above a certain level that
depends on the number of labeled samples ($n$). In some cases our results
simplify considerably: For Gaussian data, the correlation must be at least
$1/\sqrt{n - 2}$ in order to see improvement, and a similar result holds for
binary labels. In experiments, we illustrate that our theoretical findings hold
on real-world datasets, and give insights into trade-offs between single-sample
and sample-splitting variants of PPI++.

</details>


### [711] [Lorentz Local Canonicalization: How to Make Any Network Lorentz-Equivariant](https://arxiv.org/abs/2505.20280)
*Jonas Spinner,Luigi Favaro,Peter Lippmann,Sebastian Pitz,Gerrit Gerhartz,Tilman Plehn,Fred A. Hamprecht*

Main category: stat.ML

TL;DR: 论文提出了一种名为LLoCa的通用框架，可将任何主干网络转化为精确的Lorentz等变网络，显著提升了粒子物理任务的性能与效率。


<details>
  <summary>Details</summary>
Motivation: 当前Lorentz等变神经网络依赖专用层，限制了架构选择。为突破这一限制，研究者希望开发一个通用框架，使任何主干网络都能实现精确的Lorentz等变性。

Method: 引入Lorentz局部正则化（LLoCa）框架，通过预测局部参考系实现等变性，并构建了LLoCa-transformers和图网络。将几何消息传递方法适配至非紧致Lorentz群，支持时空张量特征传播。

Result: 模型在相关粒子物理任务上超越现有最优精度，同时速度提升4倍，计算量减少5-100倍。数据增强可通过参考系选择自然实现。

Conclusion: LLoCa框架突破了专用层的限制，为Lorentz等变网络提供了通用解决方案，在性能与效率上取得显著突破。

Abstract: Lorentz-equivariant neural networks are becoming the leading architectures
for high-energy physics. Current implementations rely on specialized layers,
limiting architectural choices. We introduce Lorentz Local Canonicalization
(LLoCa), a general framework that renders any backbone network exactly
Lorentz-equivariant. Using equivariantly predicted local reference frames, we
construct LLoCa-transformers and graph networks. We adapt a recent approach to
geometric message passing to the non-compact Lorentz group, allowing
propagation of space-time tensorial features. Data augmentation emerges from
LLoCa as a special choice of reference frame. Our models surpass
state-of-the-art accuracy on relevant particle physics tasks, while being
$4\times$ faster and using $5$-$100\times$ fewer FLOPs.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [712] [A Survey of LLM $\times$ DATA](https://arxiv.org/abs/2505.18458)
*Xuanhe Zhou,Junxuan He,Wei Zhou,Haodong Chen,Zirui Tang,Haoyu Zhao,Xin Tong,Guoliang Li,Youmin Chen,Jun Zhou,Zhaojun Sun,Binyuan Hui,Shuo Wang,Conghui He,Zhiyuan Liu,Jingren Zhou,Fan Wu*

Main category: cs.DB

TL;DR: 该综述探讨了大语言模型（LLM）与数据管理（DATA）的双向关系，包括数据如何支持LLM发展及LLM如何优化数据管理。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索LLM与数据管理之间的相互作用，以促进两个领域的共同发展。

Method: 方法包括对DATA4LLM（数据支持LLM）和LLM4DATA（LLM优化数据管理）两个方向的全面综述。

Result: 结果表明，数据管理对LLM的高质量发展至关重要，同时LLM也能显著提升数据管理的效率和智能化水平。

Conclusion: 结论指出，LLM与数据管理的深度融合将推动人工智能和数据科学领域的重大进步。

Abstract: The integration of large language model (LLM) and data management (DATA) is
rapidly redefining both domains. In this survey, we comprehensively review the
bidirectional relationships. On the one hand, DATA4LLM, spanning large-scale
data processing, storage, and serving, feeds LLMs with high quality, diversity,
and timeliness of data required for stages like pre-training, post-training,
retrieval-augmented generation, and agentic workflows: (i) Data processing for
LLMs includes scalable acquisition, deduplication, filtering, selection, domain
mixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on
efficient data and model formats, distributed and heterogeneous storage
hierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data
serving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),
LLM inference (e.g., prompt compression, data provenance), and training
strategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,
LLMs are emerging as general-purpose engines for data management. We review
recent advances in (i) data manipulation, including automatic data cleaning,
integration, discovery; (ii) data analysis, covering reasoning over structured,
semi-structured, and unstructured data, and (iii) system optimization (e.g.,
configuration tuning, query rewriting, anomaly diagnosis), powered by LLM
techniques like retrieval-augmented prompting, task-specialized fine-tuning,
and multi-agent collaboration.

</details>


### [713] [SQUiD: Synthesizing Relational Databases from Unstructured Text](https://arxiv.org/abs/2505.19025)
*Mushtari Sadia,Zhenning Yang,Yunming Xiao,Ang Chen,Amrita Roy Chowdhury*

Main category: cs.DB

TL;DR: SQUiD框架利用LLMs从非结构化文本自动生成关系数据库，性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 尽管关系数据库是现代数据管理的核心，但大多数数据以非结构化文本形式存在，需要一种方法将文本转换为结构化数据。

Method: 提出SQUiD框架，通过四个阶段的任务分解，结合神经符号技术，自动生成数据库模式和填充表数据。

Result: 实验表明，SQUiD在多样化数据集上表现优于基线方法。

Conclusion: SQUiD有效解决了从非结构化文本生成关系数据库的挑战，展示了其优越性能。

Abstract: Relational databases are central to modern data management, yet most data
exists in unstructured forms like text documents. To bridge this gap, we
leverage large language models (LLMs) to automatically synthesize a relational
database by generating its schema and populating its tables from raw text. We
introduce SQUiD, a novel neurosymbolic framework that decomposes this task into
four stages, each with specialized techniques. Our experiments show that SQUiD
consistently outperforms baselines across diverse datasets.

</details>


### [714] [ODIN: A NL2SQL Recommender to Handle Schema Ambiguity](https://arxiv.org/abs/2505.19302)
*Kapil Vaidya,Abishek Sankararaman,Jialin Ding,Chuan Lei,Xiao Qin,Balakrishnan Narayanaswamy,Tim Kraska*

Main category: cs.DB

TL;DR: ODIN是一个NL2SQL推荐引擎，通过生成多个可能的SQL查询来解决模式歧义问题，并根据用户反馈个性化推荐。


<details>
  <summary>Details</summary>
Motivation: 在复杂的企业环境中，数据库模式歧义（多个表和列有语义相似的名称）是NL2SQL系统的主要挑战。

Method: ODIN通过生成一组潜在的SQL查询来处理模式歧义，动态调整建议数量，并利用用户反馈优化未来推荐。

Result: 评估显示，ODIN生成正确SQL查询的概率比基线方法提高了1.5-2倍。

Conclusion: ODIN有效解决了模式歧义问题，显著提升了NL2SQL系统的准确性和用户体验。

Abstract: NL2SQL (natural language to SQL) systems translate natural language into SQL
queries, allowing users with no technical background to interact with databases
and create tools like reports or visualizations. While recent advancements in
large language models (LLMs) have significantly improved NL2SQL accuracy,
schema ambiguity remains a major challenge in enterprise environments with
complex schemas, where multiple tables and columns with semantically similar
names often co-exist. To address schema ambiguity, we introduce ODIN, a NL2SQL
recommendation engine. Instead of producing a single SQL query given a natural
language question, ODIN generates a set of potential SQL queries by accounting
for different interpretations of ambiguous schema components. ODIN dynamically
adjusts the number of suggestions based on the level of ambiguity, and ODIN
learns from user feedback to personalize future SQL query recommendations. Our
evaluation shows that ODIN improves the likelihood of generating the correct
SQL query by 1.5-2$\times$ compared to baselines.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [715] [Anomaly detection in radio galaxy data with trainable COSFIRE filters](https://arxiv.org/abs/2505.18643)
*Steven Ndung'u,Trienko Grobler,Stefan J. Wijnholds,George Azzopardi*

Main category: astro-ph.IM

TL;DR: 该论文提出了一种基于可训练COSFIRE滤波器和无监督LOF算法的半监督异常检测方法，用于识别射电星系形态异常，性能优于计算密集型深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 射电天文数据量大且异常样本稀缺，传统监督方法依赖标注数据，难以有效检测未知异常形态。需要高效的无监督或半监督方法解决这一挑战。

Method: 结合可训练COSFIRE滤波器（提取形态特征）与无监督局部离群因子（LOF）算法，构建半监督异常检测框架，无需依赖异常样本训练。

Result: 在射电星系基准测试中，COSFIRE方法以79%的G-Mean分数超越77%的深度学习自编码器，且计算效率更高。

Conclusion: 该方法通过表征正常模式检测偏差，摆脱了对标注异常数据的依赖，适用于下一代射电望远镜的实时未知现象发现。

Abstract: Detecting anomalies in radio astronomy is challenging due to the vast amounts
of data and the rarity of labeled anomalous examples. Addressing this challenge
requires efficient methods capable of identifying unusual radio galaxy
morphologies without relying on extensive supervision. This work introduces an
innovative approach to anomaly detection based on morphological characteristics
of the radio sources using trainable COSFIRE (Combination of Shifted Filter
Responses) filters as an efficient alternative to complex deep learning
methods. The framework integrates COSFIRE descriptors with an unsupervised
Local Outlier Factor (LOF) algorithm to identify unusual radio galaxy
morphologies. Evaluations on a radio galaxy benchmark data set demonstrate
strong performance, with the COSFIRE-based approach achieving a geometric mean
(G-Mean) score of 79%, surpassing the 77% achieved by a computationally
intensive deep learning autoencoder. By characterizing normal patterns and
detecting deviations, this semi-supervised methodology overcomes the need for
anomalous examples in the training set, a major limitation of traditional
supervised methods. This approach shows promise for next-generation radio
telescopes, where fast processing and the ability to discover unknown phenomena
are crucial.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [716] [TS-URGENet: A Three-stage Universal Robust and Generalizable Speech Enhancement Network](https://arxiv.org/abs/2505.18533)
*Xiaobin Rong,Dahan Wang,Qinwen Hu,Yushi Wang,Yuxiang Hu,Jing Lu*

Main category: eess.AS

TL;DR: TS-URGENet是一种三阶段通用语音增强网络，通过填充、分离和恢复三个阶段处理多种语音失真，在Interspeech 2025 URGENT Challenge中表现优异。


<details>
  <summary>Details</summary>
Motivation: 通用语音增强需要处理多种失真和输入格式的语音，现有方法难以全面应对这些挑战。

Method: 提出三阶段架构：填充阶段初步修复丢失区域，分离阶段抑制噪声和失真，恢复阶段补偿带宽限制和编解码伪影。

Result: 在Interspeech 2025 URGENT Challenge的Track 1中排名第二，表现突出。

Conclusion: TS-URGENet通过三阶段设计有效提升了语音增强的通用性和鲁棒性。

Abstract: Universal speech enhancement aims to handle input speech with different
distortions and input formats. To tackle this challenge, we present TS-URGENet,
a Three-Stage Universal, Robust, and Generalizable speech Enhancement Network.
To address various distortions, the proposed system employs a novel three-stage
architecture consisting of a filling stage, a separation stage, and a
restoration stage. The filling stage mitigates packet loss by preliminarily
filling lost regions under noise interference, ensuring signal continuity. The
separation stage suppresses noise, reverberation, and clipping distortion to
improve speech clarity. Finally, the restoration stage compensates for
bandwidth limitation, codec artifacts, and residual packet loss distortion,
refining the overall speech quality. Our proposed TS-URGENet achieved
outstanding performance in the Interspeech 2025 URGENT Challenge, ranking 2nd
in Track 1.

</details>


### [717] [Evaluating the Usefulness of Non-Diagnostic Speech Data for Developing Parkinson's Disease Classifiers](https://arxiv.org/abs/2505.18722)
*Terry Yi Zhong,Esther Janse,Cristian Tejedor-Garcia,Louis ten Bosch,Martha Larson*

Main category: eess.AS

TL;DR: 研究探讨了使用非诊断性语音任务数据集（TT）进行帕金森病（PD）检测的可行性，发现其效果与诊断性数据集（PC-GITA）相当，并分析了影响分类性能的数据集特性。


<details>
  <summary>Details</summary>
Motivation: 探索基于非诊断性语音任务的数据集（TT）在帕金森病检测中的可行性，以提供更自动化、经济且非侵入性的诊断方法。

Method: 使用Turn-Taking（TT）数据集，分析其与诊断性数据集（PC-GITA）的对比效果，并研究数据集特性（如音频拼接、性别和状态分布平衡）对分类性能的影响。

Result: TT数据集在PD检测中与PC-GITA效果相当；音频拼接和平衡分布可提升性能；TT训练的模型在PC-GITA上表现更好，反之则较差；个体表现差异导致高变异性。

Conclusion: 非诊断性语音任务数据集（TT）可用于PD检测，且在某些情况下优于诊断性数据集，但需注意个体表现差异对结果的影响。

Abstract: Speech-based Parkinson's disease (PD) detection has gained attention for its
automated, cost-effective, and non-intrusive nature. As research studies
usually rely on data from diagnostic-oriented speech tasks, this work explores
the feasibility of diagnosing PD on the basis of speech data not originally
intended for diagnostic purposes, using the Turn-Taking (TT) dataset. Our
findings indicate that TT can be as useful as diagnostic-oriented PD datasets
like PC-GITA. We also investigate which specific dataset characteristics impact
PD classification performance. The results show that concatenating audio
recordings and balancing participants' gender and status distributions can be
beneficial. Cross-dataset evaluation reveals that models trained on PC-GITA
generalize poorly to TT, whereas models trained on TT perform better on
PC-GITA. Furthermore, we provide insights into the high variability across
folds, which is mainly due to large differences in individual speaker
performance.

</details>


### [718] [Enhancing Generalization of Speech Large Language Models with Multi-Task Behavior Imitation and Speech-Text Interleaving](https://arxiv.org/abs/2505.18644)
*Jingran Xie,Xiang Li,Hui Wang,Yue Yu,Yang Xiang,Xixin Wu,Zhiyong Wu*

Main category: eess.AS

TL;DR: 该论文提出了一种名为MTBI的多任务行为模仿方法，通过语音-文本交错增强语音与文本大语言模型的对齐效率，减少对标注语音数据的依赖，提升了模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前语音大语言模型（SLLMs）通常使用监督微调来对齐语音和文本，但缺乏广泛的标注语音数据导致对齐效率低和泛化能力差。

Method: 提出MTBI方法，通过多任务行为模仿和语音-文本交错，仅依赖配对的语音和文本数据，使模型生成等效的语音和文本响应。

Result: 实验结果表明，MTBI在提示和任务泛化方面优于现有SLLMs，且需要更少的监督语音数据。

Conclusion: MTBI方法有效提升了语音大语言模型的对齐效率和泛化能力，减少了对标注数据的依赖。

Abstract: Large language models (LLMs) have shown remarkable generalization across
tasks, leading to increased interest in integrating speech with LLMs. These
speech LLMs (SLLMs) typically use supervised fine-tuning to align speech with
text-based LLMs. However, the lack of annotated speech data across a wide range
of tasks hinders alignment efficiency, resulting in poor generalization. To
address these issues, we propose a novel multi-task 'behavior imitation' method
with speech-text interleaving, called MTBI, which relies solely on paired
speech and transcripts. By ensuring the LLM decoder generates equivalent
responses to paired speech and text, we achieve a more generalized SLLM.
Interleaving is used to further enhance alignment efficiency. We introduce a
simple benchmark to evaluate prompt and task generalization across different
models. Experimental results demonstrate that our MTBI outperforms SOTA SLLMs
on both prompt and task generalization, while requiring less supervised speech
data.

</details>


### [719] [Speech-IFEval: Evaluating Instruction-Following and Quantifying Catastrophic Forgetting in Speech-Aware Language Models](https://arxiv.org/abs/2505.19037)
*Ke-Han Lu,Chun-Yi Kuan,Hung-yi Lee*

Main category: eess.AS

TL;DR: Speech-IFeval框架评估语音语言模型的指令跟随能力，发现现有模型表现不佳且对提示敏感。


<details>
  <summary>Details</summary>
Motivation: 现有语音语言模型（SLMs）在整合语音感知与大型语言模型（LLMs）时，常因侧重语音训练而降低文本能力，且缺乏专门评估指令跟随能力的基准。

Method: 提出Speech-IFeval框架，通过专用基准分离评估语音感知与指令跟随能力，量化模型表现。

Result: 多数SLMs在基础指令任务上表现远逊于文本LLMs，且输出对提示变化高度敏感，稳定性差。

Conclusion: 需改进SLMs的指令跟随鲁棒性，未来研究应超越任务级指标进行更全面评估。

Abstract: We introduce Speech-IFeval, an evaluation framework designed to assess
instruction-following capabilities and quantify catastrophic forgetting in
speech-aware language models (SLMs). Recent SLMs integrate speech perception
with large language models (LLMs), often degrading textual capabilities due to
speech-centric training. Existing benchmarks conflate speech perception with
instruction-following, hindering evaluation of these distinct skills. To
address this gap, we provide a benchmark for diagnosing the
instruction-following abilities of SLMs. Our findings show that most SLMs
struggle with even basic instructions, performing far worse than text-based
LLMs. Additionally, these models are highly sensitive to prompt variations,
often yielding inconsistent and unreliable outputs. We highlight core
challenges and provide insights to guide future research, emphasizing the need
for evaluation beyond task-level metrics.

</details>


### [720] [Revival with Voice: Multi-modal Controllable Text-to-Speech Synthesis](https://arxiv.org/abs/2505.18972)
*Minsu Kim,Pingchuan Ma,Honglie Chen,Stavros Petridis,Maja Pantic*

Main category: eess.AS

TL;DR: 该论文提出了一种多模态可控文本到语音合成方法，通过人脸图像生成语音，并利用文本描述控制语音特性，解决了音频质量、艺术肖像语音生成及语音一致性三大挑战。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决人脸驱动语音合成系统中的三个主要问题：音频质量受限、无法从艺术肖像生成语音以及语音一致性问题。

Method: 1) 利用高质量纯音频语料库提升训练效果；2) 通过风格化增强输入人脸图像以支持艺术肖像；3) 采用基于采样的解码和提示机制确保语音一致性。

Result: 实验验证了该方法在人脸驱动语音合成中的有效性，能够生成高质量且可控的语音。

Conclusion: 所提出的方法成功解决了人脸驱动语音合成的关键挑战，为多模态语音生成提供了有效解决方案。

Abstract: This paper explores multi-modal controllable Text-to-Speech Synthesis (TTS)
where the voice can be generated from face image, and the characteristics of
output speech (e.g., pace, noise level, distance, tone, place) can be
controllable with natural text description. Specifically, we aim to mitigate
the following three challenges in face-driven TTS systems. 1) To overcome the
limited audio quality of audio-visual speech corpora, we propose a training
method that additionally utilizes high-quality audio-only speech corpora. 2) To
generate voices not only from real human faces but also from artistic
portraits, we propose augmenting the input face image with stylization. 3) To
consider one-to-many possibilities in face-to-voice mapping and ensure
consistent voice generation at the same time, we propose to first employ
sampling-based decoding and then use prompting with generated speech samples.
Experimental results validate the proposed model's effectiveness in face-driven
voice synthesis.

</details>


### [721] [Acoustic and Machine Learning Methods for Speech-Based Suicide Risk Assessment: A Systematic Review](https://arxiv.org/abs/2505.18195)
*Ambre Marie,Marine Garnier,Thomas Bertin,Laura Machart,Guillaume Dardenne,Gwenolé Quellec,Sofian Berrouiguet*

Main category: eess.AS

TL;DR: 本文通过系统综述评估了AI和ML在通过语音声学分析评估自杀风险中的作用，发现声学特征在风险与非风险人群间存在显著差异，但研究存在方法学限制。


<details>
  <summary>Details</summary>
Motivation: 自杀是公共卫生挑战，需要改进检测方法以实现及时干预和治疗。

Method: 遵循PRISMA指南，分析了来自PubMed、Cochrane、Scopus和Web of Science的33篇文章，评估声学差异和ML分类器性能。

Result: 研究发现风险与非风险人群在声学特征（如jitter、基频等）上有显著差异，多模态方法表现更优，但存在样本量小、方法不一致等限制。

Conclusion: 未来研究需标准化方法、扩大数据集多样性，以支持AI在临床自杀风险评估中的应用。

Abstract: Suicide remains a public health challenge, necessitating improved detection
methods to facilitate timely intervention and treatment. This systematic review
evaluates the role of Artificial Intelligence (AI) and Machine Learning (ML) in
assessing suicide risk through acoustic analysis of speech. Following PRISMA
guidelines, we analyzed 33 articles selected from PubMed, Cochrane, Scopus, and
Web of Science databases. These studies primarily explored acoustic differences
between individuals at risk of suicide (RS) and those not at risk (NRS), and
evaluated ML classifier performance. Findings consistently showed significant
acoustic feature variations between RS and NRS populations, particularly
involving jitter, fundamental frequency (F0), Mel-frequency cepstral
coefficients (MFCC), and power spectral density (PSD). Classifier effectiveness
varied based on algorithms, modalities, and speech elicitation methods, with
multimodal approaches integrating acoustic, linguistic, and metadata features
demonstrating superior performance. However, limitations such as methodological
variability, small sample sizes, lack of longitudinal data, and limited
linguistic and demographic diversity restrict generalizability. Future research
should focus on standardizing methods, expanding multimodal analyses, and
utilizing larger, diverse datasets to support AI integration in clinical
suicide risk assessment.

</details>


### [722] [MVP: Multi-source Voice Pathology detection](https://arxiv.org/abs/2505.20050)
*Alkis Koudounas,Moreno La Quatra,Gabriele Ciravegna,Marco Fantini,Erika Crosetti,Giovanni Succo,Tania Cerquitelli,Sabato Marco Siniscalchi,Elena Baralis*

Main category: eess.AS

TL;DR: 提出MVP方法，利用Transformer处理原始语音信号，通过三种融合策略结合句子朗读和持续元音录音，提升语音病理检测性能。


<details>
  <summary>Details</summary>
Motivation: 语音障碍严重影响患者生活质量，但由于病理语音数据稀缺和录音来源多样性，非侵入式自动诊断研究不足。

Method: 采用Transformer直接处理原始语音信号，探索波形拼接、中间特征融合和决策级组合三种策略融合句子朗读和持续元音录音。

Result: 在德语、葡萄牙语和意大利语中验证，中间特征融合策略表现最佳，AUC比单源方法提升高达13%。

Conclusion: MVP方法通过多源融合有效捕捉不同录音类型的互补特征，显著提升语音病理检测性能。

Abstract: Voice disorders significantly impact patient quality of life, yet
non-invasive automated diagnosis remains under-explored due to both the
scarcity of pathological voice data, and the variability in recording sources.
This work introduces MVP (Multi-source Voice Pathology detection), a novel
approach that leverages transformers operating directly on raw voice signals.
We explore three fusion strategies to combine sentence reading and sustained
vowel recordings: waveform concatenation, intermediate feature fusion, and
decision-level combination. Empirical validation across the German, Portuguese,
and Italian languages shows that intermediate feature fusion using transformers
best captures the complementary characteristics of both recording types. Our
approach achieves up to +13% AUC improvement over single-source methods.

</details>


### [723] [From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data](https://arxiv.org/abs/2505.20166)
*Chun-Yi Kuan,Hung-yi Lee*

Main category: eess.AS

TL;DR: 该论文提出了一种名为BALSa的方法，通过合成数据生成和对比训练LISTEN，解决了音频感知大语言模型（ALLMs）在适应音频任务时的灾难性遗忘和跨模态对齐问题，并进一步扩展到多音频场景，提升了模型的音频理解和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的音频感知大语言模型（ALLMs）在适应音频任务时存在两个主要问题：一是灾难性遗忘，导致模型失去文本能力；二是跨模态对齐依赖大量任务特定的问答对，资源消耗大。

Method: 论文提出BALSa方法，利用主干LLMs合成通用标题式对齐数据，并引入LISTEN对比训练方法，增强模型区分存在与不存在声音的能力，进一步扩展到多音频场景。

Result: 实验结果表明，该方法有效减少了音频幻觉，同时保持了音频理解、推理和指令跟随能力，多音频训练进一步提升了模型的理解和推理能力。

Conclusion: BALSa为ALLMs的开发提供了一种高效且可扩展的方法，显著提升了模型的音频语言对齐能力和性能。

Abstract: Audio-aware large language models (ALLMs) have recently made great strides in
understanding and processing audio inputs. These models are typically adapted
from text-based large language models (LLMs) through additional training on
audio-related tasks. However, this adaptation process presents two major
limitations. First, ALLMs often suffer from catastrophic forgetting, where
important textual capabilities such as instruction-following are lost after
training on audio data. In some cases, models may even hallucinate sounds that
are not present in the input audio, raising concerns about their reliability.
Second, achieving cross-modal alignment between audio and language typically
relies on large collections of task-specific question-answer pairs for
instruction tuning, making the process resource-intensive. To address these
issues, we leverage the backbone LLMs from ALLMs to synthesize general-purpose
caption-style alignment data. We refer to this process as bootstrapping
audio-language alignment via synthetic data generation from backbone LLMs
(BALSa). Building on BALSa, we introduce LISTEN (Learning to Identify Sounds
Through Extended Negative Samples), a contrastive-like training method designed
to improve ALLMs' ability to distinguish between present and absent sounds. We
further extend BALSa to multi-audio scenarios, where the model either explains
the differences between audio inputs or produces a unified caption that
describes them all, thereby enhancing audio-language alignment. Experimental
results indicate that our method effectively mitigates audio hallucinations
while reliably maintaining strong performance in audio understanding,
reasoning, and instruction-following skills. Moreover, incorporating
multi-audio training further enhances the model's comprehension and reasoning
capabilities. Overall, BALSa offers an efficient and scalable approach to the
development of ALLMs.

</details>


### [724] [SoloSpeech: Enhancing Intelligibility and Quality in Target Speech Extraction through a Cascaded Generative Pipeline](https://arxiv.org/abs/2505.19314)
*Helin Wang,Jiarui Hai,Dongchao Yang,Chen Chen,Kai Li,Junyi Peng,Thomas Thebaud,Laureano Moro Velazquez,Jesus Villalba,Najim Dehak*

Main category: eess.AS

TL;DR: SoloSpeech提出了一种新的级联生成管道，用于目标语音提取，通过压缩、提取、重建和校正过程，显著提高了语音质量和可懂度，并在不同数据集和实际场景中表现出优异的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前的目标语音提取（TSE）方法主要使用判别模型，虽然能提供较高的感知质量，但容易引入伪影、降低自然度，并对训练和测试环境差异敏感。而生成模型在感知质量和可懂度方面表现不佳。为了解决这些问题，本文提出了SoloSpeech。

Method: SoloSpeech采用了一种级联生成管道，包含压缩、提取、重建和校正过程。它使用无说话人嵌入的目标提取器，通过条件信息从提示音频的潜在空间对齐混合音频的潜在空间，以避免不匹配。

Result: 在Libri2Mix数据集上的评估表明，SoloSpeech在目标语音提取和语音分离任务中实现了最新的可懂度和质量，并在域外数据和真实场景中表现出优异的泛化能力。

Conclusion: SoloSpeech通过创新的生成管道，显著提升了目标语音提取的性能，解决了现有方法在伪影、自然度和环境差异敏感方面的问题，为语音处理领域提供了新的解决方案。

Abstract: Target Speech Extraction (TSE) aims to isolate a target speaker's voice from
a mixture of multiple speakers by leveraging speaker-specific cues, typically
provided as auxiliary audio (a.k.a. cue audio). Although recent advancements in
TSE have primarily employed discriminative models that offer high perceptual
quality, these models often introduce unwanted artifacts, reduce naturalness,
and are sensitive to discrepancies between training and testing environments.
On the other hand, generative models for TSE lag in perceptual quality and
intelligibility. To address these challenges, we present SoloSpeech, a novel
cascaded generative pipeline that integrates compression, extraction,
reconstruction, and correction processes. SoloSpeech features a
speaker-embedding-free target extractor that utilizes conditional information
from the cue audio's latent space, aligning it with the mixture audio's latent
space to prevent mismatches. Evaluated on the widely-used Libri2Mix dataset,
SoloSpeech achieves the new state-of-the-art intelligibility and quality in
target speech extraction and speech separation tasks while demonstrating
exceptional generalization on out-of-domain data and real-world scenarios.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [725] [A Domain Ontology for Modeling the Book of Purification in Islam](https://arxiv.org/abs/2505.18222)
*Hessa Alawwad*

Main category: cs.DL

TL;DR: 本文开发了一个关于伊斯兰净礼的 ontology，填补了伊斯兰主要主题的空白，并支持知识共享和重用。


<details>
  <summary>Details</summary>
Motivation: 伊斯兰净礼是祈祷和其他宗教义务的基础，但相关主题在 ontology 开发中存在空白。本文旨在填补这一空白。

Method: 采用六步策略：领域识别、知识获取、概念化、分类、整合与实现、ontology 生成。

Result: 开发了一个可重用的 ontology，明确定义了净礼的关键概念、属性和关系。

Conclusion: 该 ontology 为伊斯兰净礼提供了结构化表示，支持知识共享和重用，但技术实现不在本研究范围内。

Abstract: This paper aims to address a gap in major Islamic topics by developing an
ontology for the Book of Purification in Islam. Many authoritative Islamic
texts begin with the Book of Purification, as it is essential for performing
prayer (the second pillar of Islam after Shahadah, the profession of faith) and
other religious duties such as Umrah and Hajj.
  The ontology development strategy followed six key steps: (1) domain
identification, (2) knowledge acquisition, (3) conceptualization, (4)
classification, (5) integration and implementation, and (6) ontology
generation. This paper includes examples of the constructed tables and
classifications.
  The focus is on the design and analysis phases, as technical implementation
is beyond the scope of this study. However, an initial implementation is
provided to illustrate the steps of the proposed strategy.
  The developed ontology ensures reusability by formally defining and encoding
the key concepts, attributes, and relationships related to the Book of
Purification. This structured representation is intended to support knowledge
sharing and reuse.

</details>


### [726] [Clustering scientific publications: lessons learned through experiments with a real citation network](https://arxiv.org/abs/2505.18180)
*Vu Thi Huong,Thorsten Koch*

Main category: cs.DL

TL;DR: 该研究评估了谱聚类、Louvain和Leiden算法在包含约70万篇论文和460万次引用的真实引文网络上的聚类性能，发现默认参数效果不佳，需针对网络结构精细调参。


<details>
  <summary>Details</summary>
Motivation: 科学文献聚类能揭示文献数据库中的潜在研究结构，但现有图聚类方法在真实数据上的性能可能下降，需系统性评估其实际表现。

Method: 从Web of Science提取大规模引文网络（70万论文/460万引用），对比谱聚类、Louvain和Leiden算法的聚类效果。

Result: Louvain和Leiden算法具有可扩展性，但默认参数常产生低质量分区；稀疏连接网络和密集核心结构需要针对性参数调整。

Conclusion: 文献计量聚类任务需根据特定网络结构选择方法和调参，研究为大规模异构网络的聚类实践提供了重要启示。

Abstract: Clustering scientific publications can reveal underlying research structures
within bibliographic databases. Graph-based clustering methods, such as
spectral, Louvain, and Leiden algorithms, are frequently utilized due to their
capacity to effectively model citation networks. However, their performance may
degrade when applied to real-world data. This study evaluates the performance
of these clustering algorithms on a citation graph comprising approx. 700,000
papers and 4.6 million citations extracted from Web of Science. The results
show that while scalable methods like Louvain and Leiden perform efficiently,
their default settings often yield poor partitioning. Meaningful outcomes
require careful parameter tuning, especially for large networks with uneven
structures, including a dense core and loosely connected papers. These findings
highlight practical lessons about the challenges of large-scale data, method
selection and tuning based on specific structures of bibliometric clustering
tasks.

</details>


### [727] [BAGELS: Benchmarking the Automated Generation and Extraction of Limitations from Scholarly Text](https://arxiv.org/abs/2505.18207)
*Ibrahim Al Azher,Miftahul Jannat Mokarrama,Zhishuai Guo,Sagnik Ray Choudhury,Hamed Alhoori*

Main category: cs.DL

TL;DR: 该论文提出了一种自动提取和生成科研论文局限性的完整计算架构，包括数据集构建、RAG生成方法、细粒度评估框架及元评估技术。


<details>
  <summary>Details</summary>
Motivation: 科研论文中局限性的透明报告能提升研究质量和可重复性，但作者常低估或模糊报告，导致读者理解困难。随着出版物数量激增，自动提取和生成这些局限性变得迫切。

Method: 通过从ACL、NeurIPS和PeerJ论文中提取局限性文本并整合外部评审，构建数据集；提出基于检索增强生成（RAG）的自动生成方法；设计细粒度评估框架并对评估技术进行元评估。

Result: 开发了完整的局限性计算分析架构，包括数据集、RAG生成方法、评估框架及元评估技术，为自动化处理科研局限性提供了系统解决方案。

Conclusion: 该研究为自动识别和生成科研局限性提供了创新方法，有助于提升学术透明度和研究可信度，未来可扩展至更多学科领域。

Abstract: In scientific research, limitations refer to the shortcomings, constraints,
or weaknesses within a study. Transparent reporting of such limitations can
enhance the quality and reproducibility of research and improve public trust in
science. However, authors often a) underreport them in the paper text and b)
use hedging strategies to satisfy editorial requirements at the cost of
readers' clarity and confidence. This underreporting behavior, along with an
explosion in the number of publications, has created a pressing need to
automatically extract or generate such limitations from scholarly papers. In
this direction, we present a complete architecture for the computational
analysis of research limitations. Specifically, we create a dataset of
limitations in ACL, NeurIPS, and PeerJ papers by extracting them from papers'
text and integrating them with external reviews; we propose methods to
automatically generate them using a novel Retrieval Augmented Generation (RAG)
technique; we create a fine-grained evaluation framework for generated
limitations; and we provide a meta-evaluation for the proposed evaluation
techniques.

</details>


### [728] [SCIRGC: Multi-Granularity Citation Recommendation and Citation Sentence Preference Alignment](https://arxiv.org/abs/2505.20103)
*Xiangyu Li,Jingqiang Chen*

Main category: cs.DL

TL;DR: SciRGC框架通过结合引文网络和情感意图提升引文推荐准确性，并基于多源输入生成符合上下文的引文句子，为跨学科研究者提供高效工具。


<details>
  <summary>Details</summary>
Motivation: 科研人员撰写引文耗时费力，现有方法难以准确识别引用意图并生成符合人类偏好的高质量引文句子。

Method: 1) 在引文推荐模块整合引文网络和情感意图 2) 在句子生成模块融合原文摘要、局部上下文、引用意图和推荐文献 3) 提出新评估指标。

Result: 实验表明SciRGC框架在引文推荐准确性和句子上下文适配性上均优于基线模型。

Conclusion: 该框架有效解决了学术引文生成的两大核心挑战，为研究者提供了实用的自动化引文辅助工具。

Abstract: Citations are crucial in scientific research articles as they highlight the
connection between the current study and prior work. However, this process is
often time-consuming for researchers. In this study, we propose the SciRGC
framework, which aims to automatically recommend citation articles and generate
citation sentences for citation locations within articles. The framework
addresses two key challenges in academic citation generation: 1) how to
accurately identify the author's citation intent and find relevant citation
papers, and 2) how to generate high-quality citation sentences that align with
human preferences. We enhance citation recommendation accuracy in the citation
article recommendation module by incorporating citation networks and sentiment
intent, and generate reasoning-based citation sentences in the citation
sentence generation module by using the original article abstract, local
context, citation intent, and recommended articles as inputs. Additionally, we
propose a new evaluation metric to fairly assess the quality of generated
citation sentences. Through comparisons with baseline models and ablation
experiments, the SciRGC framework not only improves the accuracy and relevance
of citation recommendations but also ensures the appropriateness of the
generated citation sentences in context, providing a valuable tool for
interdisciplinary researchers.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [729] [ZeroML: A Next Generation AutoML Language](https://arxiv.org/abs/2505.18243)
*Monirul Islam Mahmud*

Main category: cs.PL

TL;DR: ZeroML是一种新型AutoML编程语言，通过编译和多范式方式驱动ML流程，解决Python等语言的性能不足。


<details>
  <summary>Details</summary>
Motivation: 解决Python、R或Julia等语言在运行速度、脆弱管道和高依赖成本方面的不足。

Method: 采用微服务架构，集成模块化组件如DataCleaner，优化多线程和内存感知搜索。

Result: ZeroML能快速创建高精度模型，提升可重复性，同时减少前端开发的重复代码。

Conclusion: ZeroML为非编程人员和ML专家提供了一种高效、清晰的AutoML解决方案。

Abstract: ZeroML is a new generation programming language for AutoML to drive the ML
pipeline in a compiled and multi-paradigm way, with a pure functional core.
Meeting the shortcomings introduced by Python, R, or Julia such as slow-running
time, brittle pipelines or high dependency cost ZeroML brings the
Microservices-based architecture adding the modular, reusable pieces such as
DataCleaner, FeatureEngineer or ModelSelector. As a native multithread and
memory-aware search optimized toolkit, and with one command deployability
ability, ZeroML ensures non-coders and ML professionals to create high-accuracy
models super fast and in a more reproducible way. The verbosity of the language
ensures that when it comes to dropping into the backend, the code we will be
creating is extremely clear but the level of repetition and boilerplate
required when developing on the front end is now removed.

</details>


### [730] [Autocomp: LLM-Driven Code Optimization for Tensor Accelerators](https://arxiv.org/abs/2505.18574)
*Charles Hong,Sahil Bhatia,Alvin Cheung,Yakun Sophia Shao*

Main category: cs.PL

TL;DR: Autocomp利用LLM驱动的自动化搜索，结合领域知识和硬件反馈，显著提升了张量加速器代码的性能。


<details>
  <summary>Details</summary>
Motivation: 当前张量加速器的编程仍具挑战性，现有编译器未能充分发挥其潜力，而LLM在生成低资源代码（如专用加速器代码）方面仍面临困难。

Method: Autocomp通过两阶段结构化提示（规划与代码生成）、优化菜单注入领域知识，并在每次迭代中集成硬件反馈来实现优化。

Result: 优化后的代码在GEMM和卷积等任务中分别比供应商库快5.6倍和2.7倍，并超越专家手动调优代码1.1-1.4倍。优化方案可复用，固定样本下提速达24%。

Conclusion: Autocomp证明了LLM驱动搜索在加速器代码优化中的有效性，显著提升性能并支持跨操作复用。

Abstract: Hardware accelerators, especially those designed for tensor processing, have
become ubiquitous in today's computing landscape. However, even with
significant efforts in building compilers, programming these tensor
accelerators remains challenging, leaving much of their potential
underutilized. Recently, large language models (LLMs), trained on large amounts
of code, have shown significant promise in code generation and optimization
tasks, but generating low-resource languages like specialized tensor
accelerator code still poses a significant challenge. We tackle this challenge
with Autocomp, an approach that empowers accelerator programmers to leverage
domain knowledge and hardware feedback to optimize code via an automated
LLM-driven search. We accomplish this by: 1) formulating each optimization pass
as a structured two-phase prompt, divided into planning and code generation
phases, 2) inserting domain knowledge during planning via a concise and
adaptable optimization menu, and 3) integrating correctness and performance
metrics from hardware as feedback at each search iteration. Across three
categories of representative workloads and two different accelerators, we
demonstrate that Autocomp-optimized code runs 5.6x (GEMM) and 2.7x
(convolution) faster than the vendor-provided library, and outperforms
expert-level hand-tuned code by 1.4x (GEMM), 1.1x (convolution), and 1.3x
(fine-grained linear algebra). Additionally, we demonstrate that optimization
schedules generated from Autocomp can be reused across similar tensor
operations, improving speedups by up to 24% under a fixed sample budget.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [731] [Geometric Determinations Of Characteristic Redshifts From DESI-DR2 BAO and DES-SN5YR Observations: Hints For New Expansion Rate Anomalies](https://arxiv.org/abs/2505.19083)
*Purba Mukherjee,Anjan A Sen*

Main category: astro-ph.CO

TL;DR: 该论文通过结合DESI-DR2 BAO和DES-SN5YR数据，采用模型无关的方法重建宇宙膨胀历史，发现与标准宇宙学模型的显著偏差，可能暗示新物理。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过几何方法确定宇宙膨胀历史中的特征红移点，以探索标准宇宙学模型之外的潜在新物理现象。

Method: 采用高斯过程回归和基于节点的样条技术，结合BAO和超新星数据，重建宇宙距离及其导数。

Result: 分析显示在红移范围z∼0.35-0.55内，与Planck 2018 ΛCDM预测存在4到5σ的显著偏差，且这一现象在不同方法和数据集中一致。

Conclusion: 研究强调了特征红移作为膨胀率异常的敏感指标的重要性，并呼吁未来通过DESI-5YR BAO、Euclid和LSST等数据集进一步验证这些异常。

Abstract: In this work, we perform a model-agnostic reconstruction of the cosmic
expansion history by combining DESI-DR2 BAO and DES-SN5YR data, with a focus on
geometric determination of characteristic redshifts where notable tensions in
the expansion rate are found to emerge. Employing Gaussian process regression
alongside knot-based spline techniques, we reconstruct cosmic distances and
their derivatives to pinpoint these characteristic redshifts and infer $E(z)$.
Our analysis reveals significant deviations of approximately 4 to 5$\sigma$
from the Planck 2018 $\Lambda$CDM predictions, particularly pronounced in the
redshift range $z \sim 0.35-0.55$. These anomalies are consistently observed
across both reconstruction methods and combined datasets, indicating robust
late-time departures that could signal new physics beyond the standard
cosmological framework. The joint use of BAO and SN probes enhances the
precision of our constraints, allowing us to isolate these deviations without
reliance on specific cosmological assumptions. Our findings underscore the role
of characteristic redshifts as sensitive indicators of expansion rate anomalies
and motivate further scrutiny with forthcoming datasets from DESI-5YR BAO,
Euclid, and LSST. These future surveys will tighten constraints and help
distinguish whether these late-time anomalies arise from new fundamental
physics or unresolved systematics in the data.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [732] [COLORA: Efficient Fine-Tuning for Convolutional Models with a Study Case on Optical Coherence Tomography Image Classification](https://arxiv.org/abs/2505.18315)
*Mariano Rivera,Angello Hoyos*

Main category: cs.CV

TL;DR: 论文提出了一种名为CoLoRA的卷积低秩适应方法，旨在解决当前CNN微调方法的低效问题，并在视网膜疾病分类任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前CNN微调方法存在效率低下的问题，作者希望通过改进低秩适应（LoRA）技术来提升微调过程的效率和稳定性。

Method: CoLoRA方法是对卷积架构的低秩适应（LoRA）技术的自然扩展，通过减少微调所需的参数数量来提高计算效率。

Result: 实验结果表明，使用CoLoRA微调的CNN模型在OCTMNIST数据集上的准确率提升了近1%，性能与Vision Transformer等先进模型相当。

Conclusion: CoLoRA方法不仅显著提升了训练速度和稳定性，还在准确率上取得了显著改进，为CNN微调提供了一种高效且有效的解决方案。

Abstract: We introduce the Convolutional Low-Rank Adaptation (CoLoRA) method, designed
explicitly to overcome the inefficiencies found in current CNN fine-tuning
methods. CoLoRA can be seen as a natural extension of the convolutional
architectures of the Low-Rank Adaptation (LoRA) technique. We demonstrate the
capabilities of our method by developing and evaluating models using the widely
adopted CNN backbone pre-trained on ImageNet. We observed that this strategy
results in a stable and accurate coarse-tuning procedure. Moreover, this
strategy is computationally efficient and significantly reduces the number of
parameters required for fine-tuning compared to traditional methods.
Furthermore, our method substantially improves the speed and stability of
training. Our case study focuses on classifying retinal diseases from optical
coherence tomography (OCT) images, specifically using the OCTMNIST dataset.
Experimental results demonstrate that a CNN backbone fine-tuned with CoLoRA
surpasses nearly 1\% in accuracy. Such a performance is comparable to the
Vision Transformer, State-space discrete, and Kolmogorov-Arnold network models.

</details>


### [733] [Taming Diffusion for Dataset Distillation with High Representativeness](https://arxiv.org/abs/2505.18399)
*Lin Zhao,Yushu Wu,Xinru Jiang,Jianyang Gu,Yanzhi Wang,Xiaolin Xu,Pu Zhao,Xue Lin*

Main category: cs.CV

TL;DR: 论文提出D^3HR框架，通过改进扩散模型解决数据集蒸馏中的分布匹配和采样问题，生成高代表性数据集。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型需要更大数据集，而数据集蒸馏能生成紧凑高效的数据集。现有扩散方法存在分布匹配不准、随机噪声偏差等问题，需改进。

Method: 采用DDIM反转将数据集隐变量映射到高斯域，保持信息一致；提出高效采样方案对齐高斯分布。

Result: 实验表明D^3HR在不同模型架构下均优于现有基线，准确率更高。

Conclusion: D^3HR通过优化隐变量映射和采样，有效提升数据集蒸馏的代表性和性能。

Abstract: Recent deep learning models demand larger datasets, driving the need for
dataset distillation to create compact, cost-efficient datasets while
maintaining performance. Due to the powerful image generation capability of
diffusion, it has been introduced to this field for generating distilled
images. In this paper, we systematically investigate issues present in current
diffusion-based dataset distillation methods, including inaccurate distribution
matching, distribution deviation with random noise, and separate sampling.
Building on this, we propose D^3HR, a novel diffusion-based framework to
generate distilled datasets with high representativeness. Specifically, we
adopt DDIM inversion to map the latents of the full dataset from a
low-normality latent domain to a high-normality Gaussian domain, preserving
information and ensuring structural consistency to generate representative
latents for the distilled dataset. Furthermore, we propose an efficient
sampling scheme to better align the representative latents with the
high-normality Gaussian distribution. Our comprehensive experiments demonstrate
that D^3HR can achieve higher accuracy across different model architectures
compared with state-of-the-art baselines in dataset distillation. Source code:
https://github.com/lin-zhao-resoLve/D3HR.

</details>


### [734] [TNG-CLIP:Training-Time Negation Data Generation for Negation Awareness of CLIP](https://arxiv.org/abs/2505.18434)
*Yuliang Cai,Jesse Thomason,Mohammad Rostami*

Main category: cs.CV

TL;DR: 本文提出了一种高效生成否定数据的方法TNG-CLIP，并创建了首个评估文本到图像生成模型在否定提示下性能的基准Neg-TtoI，显著提升了模型在否定理解任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（如CLIP）在理解否定概念方面存在局限，且现有方法需要大量时间和计算资源生成否定数据进行微调，评估也仅限于图像-文本匹配任务。

Method: 提出训练时否定数据生成管道，仅增加2.5%的训练时间；创建Neg-TtoI基准，评估模型在包含否定的文本到图像生成任务中的表现。

Result: TNG-CLIP在图像-文本匹配、文本-图像检索和图像生成等多种否定基准测试中达到最先进性能。

Conclusion: TNG-CLIP方法高效且有效，显著提升了模型在否定理解任务上的能力，并通过新基准推动了相关研究的发展。

Abstract: Vision-language models (VLMs), such as CLIP, have demonstrated strong
performance across a range of downstream tasks. However, CLIP is still limited
in negation understanding: the ability to recognize the absence or exclusion of
a concept. Existing methods address the problem by using a large language model
(LLM) to generate large-scale data of image captions containing negation for
further fine-tuning CLIP. However, these methods are both time- and
compute-intensive, and their evaluations are typically restricted to image-text
matching tasks. To expand the horizon, we (1) introduce a training-time
negation data generation pipeline such that negation captions are generated
during the training stage, which only increases 2.5% extra training time, and
(2) we propose the first benchmark, Neg-TtoI, for evaluating text-to-image
generation models on prompts containing negation, assessing model's ability to
produce semantically accurate images. We show that our proposed method,
TNG-CLIP, achieves SOTA performance on diverse negation benchmarks of
image-to-text matching, text-to-image retrieval, and image generation.

</details>


### [735] [Mitigating Context Bias in Domain Adaptation for Object Detection using Mask Pooling](https://arxiv.org/abs/2505.18446)
*Hojun Son,Asma Almutairi,Arpan Kusari*

Main category: cs.CV

TL;DR: 该论文提出了一种因果视角来解释目标检测中的上下文偏差，认为卷积网络中的池化操作是偏差来源，并提出了使用前景掩码的Mask Pooling方法来减少偏差，同时设计了一个新基准测试模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前目标检测训练中存在前景物体与背景的关联偏差（上下文偏差），在跨域应用时影响模型性能。现有方法缺乏对偏差成因及消除原则的深入理解。

Method: 提出Mask Pooling方法，通过引入前景掩码将池化过程分离到前景和背景区域，并设计了一个使用随机背景的DAOD基准测试框架。

Result: 实验表明Mask Pooling能使模型在不同域中更鲁棒地检测物体，新基准测试有效验证了模型对上下文偏差的抵抗能力。

Conclusion: 该研究为减少域适应中的上下文偏差提供了原理性方法，通过改进池化操作和建立严格测试基准推进了DAOD领域的发展。

Abstract: Context bias refers to the association between the foreground objects and
background during the object detection training process. Various methods have
been proposed to minimize the context bias when applying the trained model to
an unseen domain, known as domain adaptation for object detection (DAOD). But a
principled approach to understand why the context bias occurs and how to remove
it has been missing.
  In this work, we provide a causal view of the context bias, pointing towards
the pooling operation in the convolution network architecture as the possible
source of this bias. We present an alternative, Mask Pooling, which uses an
additional input of foreground masks, to separate the pooling process in the
respective foreground and background regions and show that this process leads
the trained model to detect objects in a more robust manner under different
domains. We also provide a benchmark designed to create an ultimate test for
DAOD, using foregrounds in the presence of absolute random backgrounds, to
analyze the robustness of the intended trained models. Through these
experiments, we hope to provide a principled approach for minimizing context
bias under domain shift.

</details>


### [736] [On Denoising Walking Videos for Gait Recognition](https://arxiv.org/abs/2505.18582)
*Dongyang Jin,Chao Fan,Jingzhe Ma,Jingkai Zhou,Weihua Chen,Shiqi Yu*

Main category: cs.CV

TL;DR: 提出DenoisingGait方法，利用生成扩散模型去除步态识别中的无关干扰，结合几何驱动的特征匹配模块，显著提升识别准确率。


<details>
  <summary>Details</summary>
Motivation: 传统基于轮廓和姿态的步态识别方法因输入信息稀疏而精度不足，需有效去除视频中与身份无关的干扰因素（如衣物纹理和颜色）。

Method: 结合生成扩散模型过滤无关因素，引入几何驱动的特征匹配模块，将多通道扩散特征压缩为两通道方向向量，生成流式步态表示Gait Feature Field。

Result: 在CCPG、CASIA-B*和SUSTech1K数据集上，DenoisingGait在多数情况下实现了最新的最优性能（SoTA）。

Conclusion: DenoisingGait通过扩散模型和特征匹配模块有效去除噪声，显著提升了步态识别的准确性和鲁棒性。

Abstract: To capture individual gait patterns, excluding identity-irrelevant cues in
walking videos, such as clothing texture and color, remains a persistent
challenge for vision-based gait recognition. Traditional silhouette- and
pose-based methods, though theoretically effective at removing such
distractions, often fall short of high accuracy due to their sparse and less
informative inputs. Emerging end-to-end methods address this by directly
denoising RGB videos using human priors. Building on this trend, we propose
DenoisingGait, a novel gait denoising method. Inspired by the philosophy that
"what I cannot create, I do not understand", we turn to generative diffusion
models, uncovering how they partially filter out irrelevant factors for gait
understanding. Additionally, we introduce a geometry-driven Feature Matching
module, which, combined with background removal via human silhouettes,
condenses the multi-channel diffusion features at each foreground pixel into a
two-channel direction vector. Specifically, the proposed within- and
cross-frame matching respectively capture the local vectorized structures of
gait appearance and motion, producing a novel flow-like gait representation
termed Gait Feature Field, which further reduces residual noise in diffusion
features. Experiments on the CCPG, CASIA-B*, and SUSTech1K datasets demonstrate
that DenoisingGait achieves a new SoTA performance in most cases for both
within- and cross-domain evaluations. Code is available at
https://github.com/ShiqiYu/OpenGait.

</details>


### [737] [HyperFake: Hyperspectral Reconstruction and Attention-Guided Analysis for Advanced Deepfake Detection](https://arxiv.org/abs/2505.18587)
*Pavan C Shekar,Pawan Soni,Vivek Kanhangad*

Main category: cs.CV

TL;DR: 论文提出HyperFake，一种利用超光谱成像重建进行深度伪造检测的新方法，通过改进的MST++架构重建31通道超光谱数据，结合光谱注意力机制和EfficientNet分类器，实现更准确、通用的检测。


<details>
  <summary>Details</summary>
Motivation: 当前深度伪造检测方法难以泛化到不同篡改技术和数据集，且受限于RGB数据的固有约束。

Method: 使用改进的MST++架构从RGB视频重建31通道超光谱数据，结合光谱注意力机制选择关键特征，并通过EfficientNet分类器进行检测。

Result: HyperFake能够揭示传统方法无法检测的篡改痕迹，在不同深度伪造风格和数据集上实现更准确、通用的检测。

Conclusion: 该方法首次将超光谱成像重建用于深度伪造检测，为检测日益复杂的篡改开辟了新途径。

Abstract: Deepfakes pose a significant threat to digital media security, with current
detection methods struggling to generalize across different manipulation
techniques and datasets. While recent approaches combine CNN-based
architectures with Vision Transformers or leverage multi-modal learning, they
remain limited by the inherent constraints of RGB data. We introduce HyperFake,
a novel deepfake detection pipeline that reconstructs 31-channel hyperspectral
data from standard RGB videos, revealing hidden manipulation traces invisible
to conventional methods. Using an improved MST++ architecture, HyperFake
enhances hyperspectral reconstruction, while a spectral attention mechanism
selects the most critical spectral features for deepfake detection. The refined
spectral data is then processed by an EfficientNet-based classifier optimized
for spectral analysis, enabling more accurate and generalizable detection
across different deepfake styles and datasets, all without the need for
expensive hyperspectral cameras. To the best of our knowledge, this is the
first approach to leverage hyperspectral imaging reconstruction for deepfake
detection, opening new possibilities for detecting increasingly sophisticated
manipulations.

</details>


### [738] [Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and Preference Alignment](https://arxiv.org/abs/2505.18600)
*Bryan Sangwoo Kim,Jeongsol Kim,Jong Chul Ye*

Main category: cs.CV

TL;DR: CoZ框架通过多尺度提示链式放大，使普通超分模型实现256倍高质量放大，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 现有单图超分模型在训练尺度外放大时性能骤降，需突破这一可扩展性瓶颈。

Method: 提出Chain-of-Zoom框架：将超分分解为多步自回归链，每步注入视觉语言模型生成的多尺度文本提示，并通过GRPO对齐人类偏好。

Result: 4x基础扩散模型经CoZ处理后，可实现256倍放大且保持高保真度与感知质量。

Conclusion: CoZ通过模型无关的链式推理机制，显著提升了超分模型的极限放大能力。

Abstract: Modern single-image super-resolution (SISR) models deliver photo-realistic
results at the scale factors on which they are trained, but collapse when asked
to magnify far beyond that regime. We address this scalability bottleneck with
Chain-of-Zoom (CoZ), a model-agnostic framework that factorizes SISR into an
autoregressive chain of intermediate scale-states with multi-scale-aware
prompts. CoZ repeatedly re-uses a backbone SR model, decomposing the
conditional probability into tractable sub-problems to achieve extreme
resolutions without additional training. Because visual cues diminish at high
magnifications, we augment each zoom step with multi-scale-aware text prompts
generated by a vision-language model (VLM). The prompt extractor itself is
fine-tuned using Generalized Reward Policy Optimization (GRPO) with a critic
VLM, aligning text guidance towards human preference. Experiments show that a
standard 4x diffusion SR model wrapped in CoZ attains beyond 256x enlargement
with high perceptual quality and fidelity.

</details>


### [739] [Rethinking Causal Mask Attention for Vision-Language Inference](https://arxiv.org/abs/2505.18605)
*Xiaohuan Pei,Tao Huang,YanXiang Ma,Chang Xu*

Main category: cs.CV

TL;DR: 论文提出一种针对视觉语言模型的自回归注意力机制改进方法，通过放松视觉token的因果掩码限制并聚合未来上下文信息，提升模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有因果掩码策略直接从纯文本LLMs继承而来，对视觉token的预填充阶段处理不足，严格屏蔽未来视觉位置会阻碍模型利用关键语义线索。

Method: 提出未来感知注意力家族，通过池化操作将未来视觉上下文聚合到过去表示中，保持自回归结构的同时增强跨token依赖。

Result: 实验表明选择性压缩未来语义上下文能有效提升多样化视觉语言推理任务的性能。

Conclusion: 放松视觉token的因果约束并智能聚合未来信息，可在保持自回归特性的同时显著改善视觉语言推理效果。

Abstract: Causal attention has become a foundational mechanism in autoregressive
vision-language models (VLMs), unifying textual and visual inputs under a
single generative framework. However, existing causal mask-based strategies are
inherited from large language models (LLMs) where they are tailored for
text-only decoding, and their adaptation to vision tokens is insufficiently
addressed in the prefill stage. Strictly masking future positions for vision
queries introduces overly rigid constraints, which hinder the model's ability
to leverage future context that often contains essential semantic cues for
accurate inference. In this work, we empirically investigate how different
causal masking strategies affect vision-language inference and then propose a
family of future-aware attentions tailored for this setting. We first
empirically analyze the effect of previewing future tokens for vision queries
and demonstrate that rigid masking undermines the model's capacity to capture
useful contextual semantic representations. Based on these findings, we propose
a lightweight attention family that aggregates future visual context into past
representations via pooling, effectively preserving the autoregressive
structure while enhancing cross-token dependencies. We evaluate a range of
causal masks across diverse vision-language inference settings and show that
selectively compressing future semantic context into past representations
benefits the inference.

</details>


### [740] [Restoring Real-World Images with an Internal Detail Enhancement Diffusion Model](https://arxiv.org/abs/2505.18674)
*Peng Xiao,Hongbo Zhao,Yijun Wang,Jianxin Lin*

Main category: cs.CV

TL;DR: 提出一种基于预训练Stable Diffusion的细节保留扩散模型，通过内部图像细节增强技术实现高保真图像修复，支持文本引导的物体级着色控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法在修复真实世界退化图像（如老照片）时面临两大挑战：难以实现高保真修复，且缺乏物体级色彩控制能力。扩散模型虽能生成高质量图像，但修复时易丢失细节。

Method: 利用预训练Stable Diffusion作为生成先验，结合内部图像细节增强技术（IIDE），在潜在空间中注入退化操作模拟退化效应，保持关键结构纹理信息。

Result: 实验表明，该方法在定性和感知定量评估上显著优于现有最优模型，并支持文本引导的物体级色彩编辑。

Conclusion: 该方法无需从头训练模型，实现了退化图像的高保真修复与可控着色，接近专业修图效果。

Abstract: Restoring real-world degraded images, such as old photographs or
low-resolution images, presents a significant challenge due to the complex,
mixed degradations they exhibit, such as scratches, color fading, and noise.
Recent data-driven approaches have struggled with two main challenges:
achieving high-fidelity restoration and providing object-level control over
colorization. While diffusion models have shown promise in generating
high-quality images with specific controls, they often fail to fully preserve
image details during restoration. In this work, we propose an internal
detail-preserving diffusion model for high-fidelity restoration of real-world
degraded images. Our method utilizes a pre-trained Stable Diffusion model as a
generative prior, eliminating the need to train a model from scratch. Central
to our approach is the Internal Image Detail Enhancement (IIDE) technique,
which directs the diffusion model to preserve essential structural and textural
information while mitigating degradation effects. The process starts by mapping
the input image into a latent space, where we inject the diffusion denoising
process with degradation operations that simulate the effects of various
degradation factors. Extensive experiments demonstrate that our method
significantly outperforms state-of-the-art models in both qualitative
assessments and perceptual quantitative evaluations. Additionally, our approach
supports text-guided restoration, enabling object-level colorization control
that mimics the expertise of professional photo editing.

</details>


### [741] [Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual Reasoning from Transit Maps](https://arxiv.org/abs/2505.18675)
*Sicheng Feng,Song Wang,Shuyi Ouyang,Lingdong Kong,Zikai Song,Jianke Zhu,Huan Wang,Xinchao Wang*

Main category: cs.CV

TL;DR: 该论文提出了ReasonMap基准，用于评估多模态大语言模型在细粒度视觉理解和空间推理任务中的表现，发现开源基础模型优于推理模型，而闭源模型则相反。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在涉及细粒度视觉理解的推理任务中表现未充分评估，因此需要一个新的基准来填补这一空白。

Method: 引入ReasonMap基准，包含来自30个城市的交通地图和1,008个问题-答案对，采用两级评估流程分析答案正确性和质量。

Result: 评估15种流行模型后发现，开源基础模型优于推理模型，闭源模型则相反；视觉输入被遮挡时性能普遍下降。

Conclusion: 细粒度视觉推理任务需要真实的视觉感知，ReasonMap为视觉推理研究提供了新见解，并揭示了开源与闭源模型之间的差距。

Abstract: Multimodal large language models (MLLMs) have recently achieved significant
progress in visual tasks, including semantic scene understanding and text-image
alignment, with reasoning variants enhancing performance on complex tasks
involving mathematics and logic. However, their capacity for reasoning tasks
involving fine-grained visual understanding remains insufficiently evaluated.
To address this gap, we introduce ReasonMap, a benchmark designed to assess the
fine-grained visual understanding and spatial reasoning abilities of MLLMs.
ReasonMap encompasses high-resolution transit maps from 30 cities across 13
countries and includes 1,008 question-answer pairs spanning two question types
and three templates. Furthermore, we design a two-level evaluation pipeline
that properly assesses answer correctness and quality. Comprehensive
evaluations of 15 popular MLLMs, including both base and reasoning variants,
reveal a counterintuitive pattern: among open-source models, base models
outperform reasoning ones, while the opposite trend is observed in
closed-source models. Additionally, performance generally degrades when visual
inputs are masked, indicating that while MLLMs can leverage prior knowledge to
answer some questions, fine-grained visual reasoning tasks still require
genuine visual perception for strong performance. Our benchmark study offers
new insights into visual reasoning and contributes to investigating the gap
between open-source and closed-source models.

</details>


### [742] [GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language Models and Enhanced Reasoning Chains](https://arxiv.org/abs/2505.18700)
*Chun Wang,Xiaoran Pan,Zihao Pan,Haofan Wang,Yiren Song*

Main category: cs.CV

TL;DR: 该论文提出了一种名为GRE Suite的新框架，通过增强视觉语言模型（VLMs）的结构化推理链，提高了地理定位任务的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前的地理定位方法缺乏鲁棒的推理机制和可解释性，限制了其在多粒度视觉线索提取和外部知识整合方面的有效性。

Method: GRE Suite通过三个关键维度（数据集、模型和基准）系统开发：引入GRE30K数据集，提出多阶段推理策略的GRE模型，并构建综合评估框架GREval-Bench。

Result: 实验结果表明，GRE在所有粒度的地理定位任务中均显著优于现有方法，验证了推理增强VLMs在复杂地理推理中的有效性。

Conclusion: GRE Suite通过结构化推理链显著提升了地理定位任务的性能，为复杂地理推理提供了新的解决方案。

Abstract: Recent advances in Visual Language Models (VLMs) have demonstrated
exceptional performance in visual reasoning tasks. However, geo-localization
presents unique challenges, requiring the extraction of multigranular visual
cues from images and their integration with external world knowledge for
systematic reasoning. Current approaches to geo-localization tasks often lack
robust reasoning mechanisms and explainability, limiting their effectiveness.
To address these limitations, we propose the Geo Reason Enhancement (GRE)
Suite, a novel framework that augments VLMs with structured reasoning chains
for accurate and interpretable location inference. The GRE Suite is
systematically developed across three key dimensions: dataset, model, and
benchmark. First, we introduce GRE30K, a high-quality geo-localization
reasoning dataset designed to facilitate fine-grained visual and contextual
analysis. Next, we present the GRE model, which employs a multi-stage reasoning
strategy to progressively infer scene attributes, local details, and semantic
features, thereby narrowing down potential geographic regions with enhanced
precision. Finally, we construct the Geo Reason Evaluation Benchmark
(GREval-Bench), a comprehensive evaluation framework that assesses VLMs across
diverse urban, natural, and landmark scenes to measure both coarse-grained
(e.g., country, continent) and fine-grained (e.g., city, street) localization
performance. Experimental results demonstrate that GRE significantly
outperforms existing methods across all granularities of geo-localization
tasks, underscoring the efficacy of reasoning-augmented VLMs in complex
geographic inference. Code and data will be released at
https://github.com/Thorin215/GRE.

</details>


### [743] [MoMBS: Mixed-order minibatch sampling enhances model training from diverse-quality images](https://arxiv.org/abs/2505.18741)
*Han Li,Hu Han,S. Kevin Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新的混合顺序小批量采样方法（MoMBS），通过结合损失和不确定性来优化多样质量训练样本的使用，解决了传统方法在样本硬度衡量和样本利用上的不足。


<details>
  <summary>Details</summary>
Motivation: 医学图像在通用病变检测（ULD）中存在图像质量和标签正确性的多样性。传统训练机制如自定进度课程学习（SCL）和在线难例挖掘（OHEM）通过重加权高损失值的图像来缓解这一问题，但仍面临样本硬度衡量不精确和样本利用不足或过度的问题。

Method: 本文提出了混合顺序小批量采样（MoMBS）方法，引入了一个同时考虑损失和不确定性的衡量标准，以超越仅依赖损失的方法，并通过混合顺序小批量采样设计优化多样质量训练样本的使用。

Result: MoMBS方法能够更精细地分类高损失样本，区分它们是标签不良和代表性不足还是代表性良好和过拟合，并优先考虑代表性不足的样本作为小批次中的主要梯度贡献者。

Conclusion: MoMBS方法通过优化多样质量训练样本的使用，提高了深度学习模型在医学图像处理中的性能。

Abstract: Natural images exhibit label diversity (clean vs. noisy) in noisy-labeled
image classification and prevalence diversity (abundant vs. sparse) in
long-tailed image classification. Similarly, medical images in universal lesion
detection (ULD) exhibit substantial variations in image quality, encompassing
attributes such as clarity and label correctness. How to effectively leverage
training images with diverse qualities becomes a problem in learning deep
models. Conventional training mechanisms, such as self-paced curriculum
learning (SCL) and online hard example mining (OHEM), relieve this problem by
reweighting images with high loss values. Despite their success, these methods
still confront two challenges: (i) the loss-based measure of sample hardness is
imprecise, preventing optimum handling of different cases, and (ii) there
exists under-utilization in SCL or over-utilization OHEM with the identified
hard samples. To address these issues, this paper revisits the minibatch
sampling (MBS), a technique widely used in deep network training but largely
unexplored concerning the handling of diverse-quality training samples. We
discover that the samples within a minibatch influence each other during
training; thus, we propose a novel Mixed-order Minibatch Sampling (MoMBS)
method to optimize the use of training samples with diverse qualities. MoMBS
introduces a measure that takes both loss and uncertainty into account to
surpass a sole reliance on loss and allows for a more refined categorization of
high-loss samples by distinguishing them as either poorly labeled and under
represented or well represented and overfitted. We prioritize under represented
samples as the main gradient contributors in a minibatch and keep them from the
negative influences of poorly labeled or overfitted samples with a mixed-order
minibatch sampling design.

</details>


### [744] [StyleGuard: Preventing Text-to-Image-Model-based Style Mimicry Attacks by Style Perturbations](https://arxiv.org/abs/2505.18766)
*Yanjie Li,Wenxuan Zhang,Xinqi Lyu,Yihao Liu,Bin Xiao*

Main category: cs.CV

TL;DR: 论文提出StyleGuard方法，通过风格损失和放大损失优化，有效防御未知文本到图像模型的风格模仿攻击。


<details>
  <summary>Details</summary>
Motivation: 当前对抗性噪声防御方法易受净化攻击且跨模型迁移性差，需开发更鲁棒的风格模仿防护技术。

Method: 提出风格损失使潜在空间特征偏离原图，设计集成净化器和放大器的放大损失以增强抗净化能力。

Result: 在WikiArt和CelebA数据集上验证，StyleGuard对多种变换和净化具有更强鲁棒性，且适用于不同模仿方法。

Conclusion: StyleGuard通过双重损失机制实现了跨模型可迁移的高效风格保护，为知识产权防护提供新方案。

Abstract: Recently, text-to-image diffusion models have been widely used for style
mimicry and personalized customization through methods such as DreamBooth and
Textual Inversion. This has raised concerns about intellectual property
protection and the generation of deceptive content. Recent studies, such as
Glaze and Anti-DreamBooth, have proposed using adversarial noise to protect
images from these attacks. However, recent purification-based methods, such as
DiffPure and Noise Upscaling, have successfully attacked these latest defenses,
showing the vulnerabilities of these methods. Moreover, present methods show
limited transferability across models, making them less effective against
unknown text-to-image models. To address these issues, we propose a novel
anti-mimicry method, StyleGuard. We propose a novel style loss that optimizes
the style-related features in the latent space to make it deviate from the
original image, which improves model-agnostic transferability. Additionally, to
enhance the perturbation's ability to bypass diffusion-based purification, we
designed a novel upscale loss that involves ensemble purifiers and upscalers
during training. Extensive experiments on the WikiArt and CelebA datasets
demonstrate that StyleGuard outperforms existing methods in robustness against
various transformations and purifications, effectively countering style mimicry
in various models. Moreover, StyleGuard is effective on different style mimicry
methods, including DreamBooth and Textual Inversion.

</details>


### [745] [OmniGenBench: A Benchmark for Omnipotent Multimodal Generation across 50+ Tasks](https://arxiv.org/abs/2505.18775)
*Jiayu Wang,Yang Jiao,Yue Yu,Tianwen Qian,Shaoxiang Chen,Jingjing Chen,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 论文提出了OmniGenBench，一个全面评估大型多模态模型指令遵循能力的基准，包含57个子任务，并采用双模式评估协议。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试在评估大型多模态模型（LMMs）的多样能力时缺乏广度和深度，无法全面衡量其指令遵循能力。

Method: 引入OmniGenBench基准，包含57个基于真实场景的子任务，采用双模式评估协议（视觉解析工具和基于LLM的评判器）。

Result: 评估了主流生成模型（如GPT-4o、Gemini-2.0-Flash等），并提供了性能的深入比较和分析。

Conclusion: OmniGenBench为全面评估LMMs的指令遵循能力提供了有效工具，揭示了当前模型的性能差异。

Abstract: Recent breakthroughs in large multimodal models (LMMs), such as the
impressive GPT-4o-Native, have demonstrated remarkable proficiency in following
general-purpose instructions for image generation. However, current benchmarks
often lack the necessary breadth and depth to fully evaluate the diverse
capabilities of these models. To overcome this limitation, we introduce
OmniGenBench, a novel and comprehensive benchmark meticulously designed to
assess the instruction-following abilities of state-of-the-art LMMs across both
perception-centric and cognition-centric dimensions. Our OmniGenBench includes
57 diverse sub-tasks grounded in real-world scenarios, systematically
categorized according to the specific model capabilities they demand. For
rigorous evaluation, we further employ a dual-mode protocol. This protocol
utilizes off-the-shelf visual parsing tools for perception-centric tasks and a
powerful LLM-based judger for cognition-centric tasks to assess the alignment
between generated images and user instructions. Using OmniGenBench, we evaluate
mainstream generative models, including prevalent models like GPT-4o,
Gemini-2.0-Flash, and Seedream, and provide in-depth comparisons and analyses
of their performance.Code and data are available at
https://github.com/emilia113/OmniGenBench.

</details>


### [746] [ChartGalaxy: A Dataset for Infographic Chart Understanding and Generation](https://arxiv.org/abs/2505.18668)
*Zhen Li,Yukai Guo,Duan Li,Xinyuan Guo,Bowen Li,Lanxi Xiao,Shenyu Qiao,Jiashu Chen,Zijian Wu,Hui Zhang,Xinhuan Shu,Shixia Liu*

Main category: cs.CV

TL;DR: ChartGalaxy是一个百万级数据集，旨在提升大视觉语言模型对信息图表理解和生成的能力。


<details>
  <summary>Details</summary>
Motivation: 信息图表结合了视觉元素和文本信息，但其视觉和结构复杂性对仅训练于普通图表的大视觉语言模型提出了挑战。

Method: 通过归纳过程识别真实信息图表中的75种图表类型、330种变体和68种布局模板，并以此程序化生成合成图表。

Result: ChartGalaxy在微调提升图表理解、基准测试代码生成和基于示例的图表生成方面展示了其效用。

Conclusion: ChartGalaxy通过捕捉真实设计的视觉和结构复杂性，为增强大视觉语言模型的多模态推理和生成能力提供了宝贵资源。

Abstract: Infographic charts are a powerful medium for communicating abstract data by
combining visual elements (e.g., charts, images) with textual information.
However, their visual and structural richness poses challenges for large
vision-language models (LVLMs), which are typically trained on plain charts. To
bridge this gap, we introduce ChartGalaxy, a million-scale dataset designed to
advance the understanding and generation of infographic charts. The dataset is
constructed through an inductive process that identifies 75 chart types, 330
chart variations, and 68 layout templates from real infographic charts and uses
them to create synthetic ones programmatically. We showcase the utility of this
dataset through: 1) improving infographic chart understanding via fine-tuning,
2) benchmarking code generation for infographic charts, and 3) enabling
example-based infographic chart generation. By capturing the visual and
structural complexity of real design, ChartGalaxy provides a useful resource
for enhancing multimodal reasoning and generation in LVLMs.

</details>


### [747] [Think Twice before Adaptation: Improving Adaptability of DeepFake Detection via Online Test-Time Adaptation](https://arxiv.org/abs/2505.18787)
*Hong-Hanh Nguyen-Le,Van-Tuan Tran,Dinh-Thuc Nguyen,Nhien-An Le-Khac*

Main category: cs.CV

TL;DR: 论文提出了一种名为T²A的新型在线测试时适应方法，通过不确定性感知的负学习目标提升Deepfake检测器在推理阶段的适应能力，无需源训练数据或标签。


<details>
  <summary>Details</summary>
Motivation: Deepfake检测器在现实环境中面临后处理操作或分布偏移导致的性能下降问题，现有方法依赖初始预测，适应性不足。

Method: 提出T²A方法，结合不确定性感知负学习目标、不确定样本优先策略和梯度掩码技术，优化模型参数和样本选择。

Result: 实验表明，T²A在测试时适应方法中达到最先进水平，显著提升了检测器的鲁棒性和泛化能力。

Conclusion: T²A通过负学习与熵最小化的互补行为，有效增强了Deepfake检测器在未知数据上的适应性能。

Abstract: Deepfake (DF) detectors face significant challenges when deployed in
real-world environments, particularly when encountering test samples deviated
from training data through either postprocessing manipulations or distribution
shifts. We demonstrate postprocessing techniques can completely obscure
generation artifacts presented in DF samples, leading to performance
degradation of DF detectors. To address these challenges, we propose Think
Twice before Adaptation (\texttt{T$^2$A}), a novel online test-time adaptation
method that enhances the adaptability of detectors during inference without
requiring access to source training data or labels. Our key idea is to enable
the model to explore alternative options through an Uncertainty-aware Negative
Learning objective rather than solely relying on its initial predictions as
commonly seen in entropy minimization (EM)-based approaches. We also introduce
an Uncertain Sample Prioritization strategy and Gradients Masking technique to
improve the adaptation by focusing on important samples and model parameters.
Our theoretical analysis demonstrates that the proposed negative learning
objective exhibits complementary behavior to EM, facilitating better adaptation
capability. Empirically, our method achieves state-of-the-art results compared
to existing test-time adaptation (TTA) approaches and significantly enhances
the resilience and generalization of DF detectors during inference. Code is
available
\href{https://github.com/HongHanh2104/T2A-Think-Twice-Before-Adaptation}{here}.

</details>


### [748] [REGen: Multimodal Retrieval-Embedded Generation for Long-to-Short Video Editing](https://arxiv.org/abs/2505.18880)
*Weihan Xu,Yimeng Ma,Jingyue Huang,Yang Li,Wenye Ma,Taylor Berg-Kirkpatrick,Julian McAuley,Paul Pu Liang,Hao-Wen Dong*

Main category: cs.CV

TL;DR: 该论文提出了一种结合检索与生成的视频编辑框架REGen，用于生成包含嵌入视频片段且叙事连贯的短视频。


<details>
  <summary>Details</summary>
Motivation: 现有提取式视频摘要方法难以生成连贯叙事，而抽象式方法无法从输入视频中“引用”片段。论文旨在解决这一问题。

Method: 提出检索嵌入生成框架（REGen），先通过微调大语言模型生成带占位符的脚本，再用检索模型选择最佳视频片段填充占位符。

Result: 客观评估表明该方法能有效插入视频片段并保持叙事连贯；主观调查显示其在连贯性、对齐性和真实性上优于现有方法。

Conclusion: REGen框架在纪录片预告生成任务中实现了叙事连贯性与视频片段引用的平衡，显著优于现有方法。

Abstract: Short videos are an effective tool for promoting contents and improving
knowledge accessibility. While existing extractive video summarization methods
struggle to produce a coherent narrative, existing abstractive methods cannot
`quote' from the input videos, i.e., inserting short video clips in their
outputs. In this work, we explore novel video editing models for generating
shorts that feature a coherent narrative with embedded video insertions
extracted from a long input video. We propose a novel retrieval-embedded
generation framework that allows a large language model to quote multimodal
resources while maintaining a coherent narrative. Our proposed REGen system
first generates the output story script with quote placeholders using a
finetuned large language model, and then uses a novel retrieval model to
replace the quote placeholders by selecting a video clip that best supports the
narrative from a pool of candidate quotable video clips. We examine the
proposed method on the task of documentary teaser generation, where short
interview insertions are commonly used to support the narrative of a
documentary. Our objective evaluations show that the proposed method can
effectively insert short video clips while maintaining a coherent narrative. In
a subjective survey, we show that our proposed method outperforms existing
abstractive and extractive approaches in terms of coherence, alignment, and
realism in teaser generation.

</details>


### [749] [SD-OVON: A Semantics-aware Dataset and Benchmark Generation Pipeline for Open-Vocabulary Object Navigation in Dynamic Scenes](https://arxiv.org/abs/2505.18881)
*Dicong Qiu,Jiadi You,Zeying Gong,Ronghe Qiu,Hui Xiong,Junwei Liang*

Main category: cs.CV

TL;DR: SD-OVON提出了一种基于多模态基础模型的语义感知数据集生成流程，用于开放词汇对象导航任务，支持动态场景和可操作物体，并提供了两个预生成的数据集。


<details>
  <summary>Details</summary>
Motivation: 现有数据集局限于静态环境，无法满足动态场景和开放词汇对象导航的需求。SD-OVON旨在通过生成符合真实世界语义和常识的场景，提升导航任务的真实性和复杂性。

Method: 利用预训练的多模态基础模型生成无限多样的真实感场景，并开发了与Habitat模拟器兼容的任务插件，同时提供了两个预生成的数据集SD-OVON-3k和SD-OVON-10k。

Result: SD-OVON支持动态场景和可操作物体，提升了导航任务的真实性和复杂性。实验表明，该流程和数据集在开放词汇对象导航任务中具有有效性。

Conclusion: SD-OVON通过生成真实感场景和动态对象，为开放词汇对象导航任务提供了更真实的训练和评估环境，推动了从仿真到现实的机器人应用。

Abstract: We present the Semantics-aware Dataset and Benchmark Generation Pipeline for
Open-vocabulary Object Navigation in Dynamic Scenes (SD-OVON). It utilizes
pretraining multimodal foundation models to generate infinite unique
photo-realistic scene variants that adhere to real-world semantics and daily
commonsense for the training and the evaluation of navigation agents,
accompanied with a plugin for generating object navigation task episodes
compatible to the Habitat simulator. In addition, we offer two pre-generated
object navigation task datasets, SD-OVON-3k and SD-OVON-10k, comprising
respectively about 3k and 10k episodes of the open-vocabulary object navigation
task, derived from the SD-OVON-Scenes dataset with 2.5k photo-realistic scans
of real-world environments and the SD-OVON-Objects dataset with 0.9k manually
inspected scanned and artist-created manipulatable object models. Unlike prior
datasets limited to static environments, SD-OVON covers dynamic scenes and
manipulatable objects, facilitating both real-to-sim and sim-to-real robotic
applications. This approach enhances the realism of navigation tasks, the
training and the evaluation of open-vocabulary object navigation agents in
complex settings. To demonstrate the effectiveness of our pipeline and
datasets, we propose two baselines and evaluate them along with
state-of-the-art baselines on SD-OVON-3k. The datasets, benchmark and source
code are publicly available.

</details>


### [750] [Inference Compute-Optimal Video Vision Language Models](https://arxiv.org/abs/2505.18855)
*Peiqi Wang,ShengYun Peng,Xuewen Zhang,Hanchao Yu,Yibo Yang,Lifu Huang,Fujun Liu,Qifan Wang*

Main category: cs.CV

TL;DR: 该研究通过大规模训练扫描和参数化建模，确定了在固定推理计算预算下视频视觉语言模型的最优配置，揭示了任务性能与扩展因素及微调数据量的关系。


<details>
  <summary>Details</summary>
Motivation: 以往的研究通常专注于优化模型效率或提升性能，而忽略了资源限制。本研究旨在在固定的推理计算预算下，找到视频视觉语言模型的最优配置。

Method: 通过大规模训练扫描和参数化建模，分析语言模型大小、帧数和每帧视觉标记数三个关键扩展因素对任务性能的影响。

Result: 实验揭示了任务性能如何依赖于扩展因素和微调数据量，以及数据量的变化如何影响计算最优边界。

Conclusion: 研究结果为选择这些扩展因素提供了实用的建议，帮助在有限的计算资源下优化模型性能。

Abstract: This work investigates the optimal allocation of inference compute across
three key scaling factors in video vision language models: language model size,
frame count, and the number of visual tokens per frame. While prior works
typically focuses on optimizing model efficiency or improving performance
without considering resource constraints, we instead identify optimal model
configuration under fixed inference compute budgets. We conduct large-scale
training sweeps and careful parametric modeling of task performance to identify
the inference compute-optimal frontier. Our experiments reveal how task
performance depends on scaling factors and finetuning data size, as well as how
changes in data size shift the compute-optimal frontier. These findings
translate to practical tips for selecting these scaling factors.

</details>


### [751] [Co-AttenDWG: Co-Attentive Dimension-Wise Gating and Expert Fusion for Multi-Modal Offensive Content Detection](https://arxiv.org/abs/2505.19010)
*Md. Mithun Hossain,Md. Shakil Hossain,Sudipto Chaki,M. F. Mridha*

Main category: cs.CV

TL;DR: 提出了一种新型多模态学习架构Co-AttenDWG，通过双路径编码、维度门控协同注意力和专家融合模块显著提升了跨模态交互性能，在MIMIC和SemEval数据集上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有预训练模型在多模态学习中存在跨模态交互不足和静态融合策略的缺陷，未能充分利用不同模态的互补性。

Method: 1) 将图文特征投影到共享嵌入空间 2) 采用通道级维度门控的协同注意力机制 3) 双路径编码器分离处理模态信息 4) 专家融合模块整合门控与自注意力机制

Result: 在MIMIC和SemEval Memotion 1.0数据集上实现了跨模态对齐的显著改进和最优性能

Conclusion: 该架构通过动态特征调控和深度模态交互，为多模态应用提供了通用解决方案

Abstract: Multi-modal learning has become a critical research area because integrating
text and image data can significantly improve performance in tasks such as
classification, retrieval, and scene understanding. However, despite progress
with pre-trained models, current approaches are limited by inadequate
cross-modal interactions and static fusion strategies that do not fully exploit
the complementary nature of different modalities. To address these
shortcomings, we introduce a novel multi-modal Co-AttenDWG architecture that
leverages dual-path encoding, co-attention with dimension-wise gating, and
advanced expert fusion. Our approach begins by projecting text and image
features into a common embedding space, where a dedicated co-attention
mechanism enables simultaneous, fine-grained interactions between modalities.
This mechanism is further enhanced by a dimension-wise gating network that
adaptively regulates the feature contributions at the channel level, ensuring
that only the most relevant information is emphasized. In parallel, dual-path
encoders refine the representations by processing cross-modal information
separately before an additional cross-attention layer further aligns
modalities. The refined features are then aggregated via an expert fusion
module that combines learned gating and self-attention to produce a robust,
unified representation. We validate our approach on the MIMIC and SemEval
Memotion 1.0, where experimental results demonstrate significant improvements
in cross-modal alignment and state-of-the-art performance, underscoring the
potential of our model for a wide range of multi-modal applications.

</details>


### [752] [WeedNet: A Foundation Model-Based Global-to-Local AI Approach for Real-Time Weed Species Identification and Classification](https://arxiv.org/abs/2505.18930)
*Yanben Shen,Timilehin T. Ayanlade,Venkata Naresh Boddepalli,Mojdeh Saadati,Ashlyn Rairdin,Zi K. Deng,Muhammad Arbab Arshad,Aditya Balu,Daren Mueller,Asheesh K Singh,Wesley Everman,Nirav Merchant,Baskar Ganapathysubramanian,Meaghan Anderson,Soumik Sarkar,Arti Singh*

Main category: cs.CV

TL;DR: 论文提出首个全球规模杂草识别模型WeedNet，结合自监督学习与微调策略，在1593种杂草上实现91.02%准确率，并展示其在农业机器人集成中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 传统AI杂草识别面临专家标注数据不足、形态特征复杂多变等挑战，需开发高精度、可扩展的自动化解决方案。

Method: 采用端到端实时识别流程，结合自监督学习、微调策略和Global-to-Local方法，增强模型可信度与区域适应性。

Result: 全球模型对85种爱荷华杂草达97.38%准确率；无人机/地面机器人图像验证显示其适用于实际农业场景。

Conclusion: WeedNet作为基础模型具有强泛化能力，结合AI对话功能可为农业生态保护提供智能决策支持。

Abstract: Early identification of weeds is essential for effective management and
control, and there is growing interest in automating the process using computer
vision techniques coupled with AI methods. However, challenges associated with
training AI-based weed identification models, such as limited expert-verified
data and complexity and variability in morphological features, have hindered
progress. To address these issues, we present WeedNet, the first global-scale
weed identification model capable of recognizing an extensive set of weed
species, including noxious and invasive plant species. WeedNet is an end-to-end
real-time weed identification pipeline and uses self-supervised learning,
fine-tuning, and enhanced trustworthiness strategies. WeedNet achieved 91.02%
accuracy across 1,593 weed species, with 41% species achieving 100% accuracy.
Using a fine-tuning strategy and a Global-to-Local approach, the local Iowa
WeedNet model achieved an overall accuracy of 97.38% for 85 Iowa weeds, most
classes exceeded a 90% mean accuracy per class. Testing across intra-species
dissimilarity (developmental stages) and inter-species similarity (look-alike
species) suggests that diversity in the images collected, spanning all the
growth stages and distinguishable plant characteristics, is crucial in driving
model performance. The generalizability and adaptability of the Global WeedNet
model enable it to function as a foundational model, with the Global-to-Local
strategy allowing fine-tuning for region-specific weed communities. Additional
validation of drone- and ground-rover-based images highlights the potential of
WeedNet for integration into robotic platforms. Furthermore, integration with
AI for conversational use provides intelligent agricultural and ecological
conservation consulting tools for farmers, agronomists, researchers, land
managers, and government agencies across diverse landscapes.

</details>


### [753] [Sparse-to-Dense: A Free Lunch for Lossless Acceleration of Video Understanding in LLMs](https://arxiv.org/abs/2505.19155)
*Xuan Zhang,Cunxiao Du,Sicheng Yu,Jiawei Wu,Fengzhuo Zhang,Wei Gao,Qian Liu*

Main category: cs.CV

TL;DR: 论文提出了一种名为Sparse-to-Dense (StD)的解码策略，通过结合稀疏和密集注意力模块，加速视频大语言模型的推理过程，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频大语言模型（Video-LLMs）的自回归特性导致输入序列长度增加时推理延迟上升，而视频序列通常较长，这给高效处理带来了挑战。

Method: StD策略整合了稀疏top-K注意力和密集全注意力两个模块，稀疏模块快速解码多个token，密集模块并行验证这些token，从而在不损失性能的情况下加速模型。

Result: StD在视频处理中实现了最高1.94倍的实时加速，且无需调参，仅需少量代码修改即可从标准Video-LLM过渡到稀疏Video-LLM。

Conclusion: StD是一种即插即用的解决方案，有效提升了视频大语言模型的推理效率，同时保持了模型性能。

Abstract: Due to the auto-regressive nature of current video large language models
(Video-LLMs), the inference latency increases as the input sequence length
grows, posing challenges for the efficient processing of video sequences that
are usually very long. We observe that during decoding, the attention scores of
most tokens in Video-LLMs tend to be sparse and concentrated, with only certain
tokens requiring comprehensive full attention. Based on this insight, we
introduce Sparse-to-Dense (StD), a novel decoding strategy that integrates two
distinct modules: one leveraging sparse top-K attention and the other employing
dense full attention. These modules collaborate to accelerate Video-LLMs
without loss. The fast (sparse) model speculatively decodes multiple tokens,
while the slow (dense) model verifies them in parallel. StD is a tuning-free,
plug-and-play solution that achieves up to a 1.94$\times$ walltime speedup in
video processing. It maintains model performance while enabling a seamless
transition from a standard Video-LLM to a sparse Video-LLM with minimal code
modifications.

</details>


### [754] [How Do Images Align and Complement LiDAR? Towards a Harmonized Multi-modal 3D Panoptic Segmentation](https://arxiv.org/abs/2505.18956)
*Yining Pan,Qiongjie Cui,Xulei Yang,Na Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种名为IAL的多模态3D全景分割框架，通过同步数据增强和特征融合技术，解决了LiDAR数据稀疏性及多模态对齐问题，实现了先进的性能。


<details>
  <summary>Details</summary>
Motivation: LiDAR数据稀疏性导致远距离或小物体识别困难，现有方法虽结合相机图像但仍存在数据增强不对齐和依赖后处理的问题。

Method: 提出IAL框架，包含模态同步数据增强策略PieAug、几何引导的令牌融合模块GTF和基于先验的查询生成模块PQG，直接预测全景分割结果。

Result: IAL在两个广泛使用的基准测试中，性能优于以往的多模态3D全景分割方法。

Conclusion: IAL通过多模态同步和高效特征融合，显著提升了3D全景分割的准确性，代码和模型已开源。

Abstract: LiDAR-based 3D panoptic segmentation often struggles with the inherent
sparsity of data from LiDAR sensors, which makes it challenging to accurately
recognize distant or small objects. Recently, a few studies have sought to
overcome this challenge by integrating LiDAR inputs with camera images,
leveraging the rich and dense texture information provided by the latter. While
these approaches have shown promising results, they still face challenges, such
as misalignment during data augmentation and the reliance on post-processing
steps. To address these issues, we propose Image-Assists-LiDAR (IAL), a novel
multi-modal 3D panoptic segmentation framework. In IAL, we first introduce a
modality-synchronized data augmentation strategy, PieAug, to ensure alignment
between LiDAR and image inputs from the start. Next, we adopt a transformer
decoder to directly predict panoptic segmentation results. To effectively fuse
LiDAR and image features into tokens for the decoder, we design a
Geometric-guided Token Fusion (GTF) module. Additionally, we leverage the
complementary strengths of each modality as priors for query initialization
through a Prior-based Query Generation (PQG) module, enhancing the decoder's
ability to generate accurate instance masks. Our IAL framework achieves
state-of-the-art performance compared to previous multi-modal 3D panoptic
segmentation methods on two widely used benchmarks. Code and models are
publicly available at <https://github.com/IMPL-Lab/IAL.git>.

</details>


### [755] [Rethinking Metrics and Benchmarks of Video Anomaly Detection](https://arxiv.org/abs/2505.19022)
*Zihao Liu,Xiaoyu Wu,Wenna Li,Linlin Yang*

Main category: cs.CV

TL;DR: 本文重新思考了视频异常检测（VAD）的评估协议，提出了三种新评估方法以解决现有指标的局限性，并比较了十种先进VAD方法的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频异常检测领域在模型架构和训练策略上进展显著，但对评估指标和基准测试的关注不足，存在单标注偏差、无法奖励早期检测及缺乏场景过拟合评估能力等问题。

Method: 提出三种新评估方法：1）多轮标注的平均AUC/AP指标；2）延迟感知平均精度（LaAP）指标；3）两个硬正常基准（UCF-HN, MSAD-HN）用于评估场景过拟合。

Result: 通过新评估方法比较了十种先进VAD方法的性能，为未来模型开发提供了新视角。

Conclusion: 本文提出的评估方法有效解决了当前VAD评估的局限性，为领域发展提供了更全面的评估框架。

Abstract: Video Anomaly Detection (VAD), which aims to detect anomalies that deviate
from expectation, has attracted increasing attention in recent years. Existing
advancements in VAD primarily focus on model architectures and training
strategies, while devoting insufficient attention to evaluation metrics and
benchmarks. In this paper, we rethink VAD evaluation protocols through
comprehensive experimental analyses, revealing three critical limitations in
current practices: 1) existing metrics are significantly influenced by single
annotation bias; 2) current metrics fail to reward early detection of
anomalies; 3) available benchmarks lack the capability to evaluate scene
overfitting. To address these limitations, we propose three novel evaluation
methods: first, we establish averaged AUC/AP metrics over multi-round
annotations to mitigate single annotation bias; second, we develop a
Latency-aware Average Precision (LaAP) metric that rewards early and accurate
anomaly detection; and finally, we introduce two hard normal benchmarks
(UCF-HN, MSAD-HN) with videos specifically designed to evaluate scene
overfitting. We report performance comparisons of ten state-of-the-art VAD
approaches using our proposed evaluation methods, providing novel perspectives
for future VAD model development.

</details>


### [756] [FlowCut: Rethinking Redundancy via Information Flow for Efficient Vision-Language Models](https://arxiv.org/abs/2505.19536)
*Jintao Tong,Wenwei Jin,Pengda Qin,Anqi Li,Yixiong Zou,Yuhong Li,Yuhua Li,Ruixuan Li*

Main category: cs.CV

TL;DR: FlowCut提出了一种基于信息流的视觉令牌剪枝框架，解决了现有单层注意力评分方法的不足，显著提升了大型视觉语言模型的效率。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）在多模态理解方面表现出色，但由于冗余的视觉令牌导致计算成本高昂。现有的剪枝方法通常依赖单层注意力评分来识别和剪枝冗余令牌，但这种方法可能不足以准确反映令牌间的复杂交互。

Method: 作者从信息流的角度重新思考冗余视觉令牌的产生，提出FlowCut框架。该框架通过分析CLS令牌作为信息中继的作用，以及逐层注意力集中的动态冗余现象，设计了一种更符合模型内在行为的信息流感知剪枝方法。

Result: 实验表明，FlowCut在LLaVA-1.5-7B和LLaVA-NeXT-7B上分别实现了1.6%和4.3%的性能提升，同时减少了88.9%和94.4%的令牌，预填充阶段速度提升了3.2倍。

Conclusion: FlowCut通过信息流分析有效解决了现有剪枝方法的不足，显著提升了模型效率，为大型视觉语言模型的优化提供了新思路。

Abstract: Large vision-language models (LVLMs) excel at multimodal understanding but
suffer from high computational costs due to redundant vision tokens. Existing
pruning methods typically rely on single-layer attention scores to rank and
prune redundant visual tokens to solve this inefficiency. However, as the
interaction between tokens and layers is complicated, this raises a basic
question: Is such a simple single-layer criterion sufficient to identify
redundancy? To answer this question, we rethink the emergence of redundant
visual tokens from a fundamental perspective: information flow, which models
the interaction between tokens and layers by capturing how information moves
between tokens across layers. We find (1) the CLS token acts as an information
relay, which can simplify the complicated flow analysis; (2) the redundancy
emerges progressively and dynamically via layer-wise attention concentration;
and (3) relying solely on attention scores from single layers can lead to
contradictory redundancy identification. Based on this, we propose FlowCut, an
information-flow-aware pruning framework, mitigating the insufficiency of the
current criterion for identifying redundant tokens and better aligning with the
model's inherent behaviors. Extensive experiments show that FlowCut achieves
superior results, outperforming SoTA by 1.6% on LLaVA-1.5-7B with 88.9% token
reduction, and by 4.3% on LLaVA-NeXT-7B with 94.4% reduction, delivering 3.2x
speed-up in the prefilling stage. Our code is available at
https://github.com/TungChintao/FlowCut

</details>


### [757] [A Smart Healthcare System for Monkeypox Skin Lesion Detection and Tracking](https://arxiv.org/abs/2505.19023)
*Huda Alghoraibi,Nuha Alqurashi,Sarah Alotaibi,Renad Alkhudaydi,Bdoor Aldajani,Lubna Alqurashi,Jood Batweel,Maha A. Thafar*

Main category: cs.CV

TL;DR: 研究人员开发了一个名为ITMAINN的AI医疗系统，通过深度学习技术从皮肤病变图像中检测猴痘，并开发了配套的移动应用和实时监控仪表板。


<details>
  <summary>Details</summary>
Motivation: 猴痘作为一种病毒性疾病，已在多国爆发，亟需可扩展、易获取且准确的诊断解决方案以支持公共卫生响应。

Method: 研究使用迁移学习在公开皮肤病变数据集上训练和评估多个预训练模型，开发了包含图像分析、症状追踪和医疗中心推荐的跨平台智能手机应用，以及供卫生部门使用的实时监控仪表板。

Result: 在二分类任务中，Vision Transformer等模型准确率和F1分数达97.8%；在多分类任务中，ResNetViT和ViT Hybrid模型准确率达92%。最优模型MobileViT被部署至移动应用。

Conclusion: ITMAINN系统为智慧城市中的响应式医疗基础设施发展奠定了基础，是公共卫生管理革命的一部分。

Abstract: Monkeypox is a viral disease characterized by distinctive skin lesions and
has been reported in many countries. The recent global outbreak has emphasized
the urgent need for scalable, accessible, and accurate diagnostic solutions to
support public health responses.
  In this study, we developed ITMAINN, an intelligent, AI-driven healthcare
system specifically designed to detect Monkeypox from skin lesion images using
advanced deep learning techniques. Our system consists of three main
components. First, we trained and evaluated several pretrained models using
transfer learning on publicly available skin lesion datasets to identify the
most effective models. For binary classification (Monkeypox vs. non-Monkeypox),
the Vision Transformer, MobileViT, Transformer-in-Transformer, and VGG16
achieved the highest performance, each with an accuracy and F1-score of 97.8%.
For multiclass classification, which contains images of patients with Monkeypox
and five other classes (chickenpox, measles, hand-foot-mouth disease, cowpox,
and healthy), ResNetViT and ViT Hybrid models achieved 92% accuracy, with F1
scores of 92.24% and 92.19%, respectively. The best-performing and most
lightweight model, MobileViT, was deployed within the mobile application. The
second component is a cross-platform smartphone application that enables users
to detect Monkeypox through image analysis, track symptoms, and receive
recommendations for nearby healthcare centers based on their location. The
third component is a real-time monitoring dashboard designed for health
authorities to support them in tracking cases, analyzing symptom trends,
guiding public health interventions, and taking proactive measures.
  This system is fundamental in developing responsive healthcare infrastructure
within smart cities. Our solution, ITMAINN, is part of revolutionizing public
health management.

</details>


### [758] [InfoChartQA: A Benchmark for Multimodal Question Answering on Infographic Charts](https://arxiv.org/abs/2505.19028)
*Minzhi Lin,Tianchi Xie,Mengchen Liu,Yilin Ye,Changjian Chen,Shixia Liu*

Main category: cs.CV

TL;DR: 论文提出InfoChartQA基准，用于评估多模态大语言模型在信息图表理解上的表现，发现现有模型在视觉元素相关问题上有显著性能下降。


<details>
  <summary>Details</summary>
Motivation: 现有视觉问答基准缺乏对信息图表中视觉元素（如图标、象形图）理解能力的评估，无法全面测试多模态大语言模型的视觉推理能力。

Method: 构建包含5,642对信息图表和普通图表的数据集InfoChartQA，设计基于视觉元素的问题，评估20个多模态大语言模型。

Result: 评估显示模型在信息图表上的表现显著下降，尤其在涉及视觉隐喻的问题上。配对图表设计支持细粒度错误分析和消融研究。

Conclusion: InfoChartQA揭示了多模态大语言模型在信息图表理解上的不足，为未来研究提供了新方向。数据集已开源。

Abstract: Understanding infographic charts with design-driven visual elements (e.g.,
pictograms, icons) requires both visual recognition and reasoning, posing
challenges for multimodal large language models (MLLMs). However, existing
visual-question answering benchmarks fall short in evaluating these
capabilities of MLLMs due to the lack of paired plain charts and
visual-element-based questions. To bridge this gap, we introduce InfoChartQA, a
benchmark for evaluating MLLMs on infographic chart understanding. It includes
5,642 pairs of infographic and plain charts, each sharing the same underlying
data but differing in visual presentations. We further design
visual-element-based questions to capture their unique visual designs and
communicative intent. Evaluation of 20 MLLMs reveals a substantial performance
decline on infographic charts, particularly for visual-element-based questions
related to metaphors. The paired infographic and plain charts enable
fine-grained error analysis and ablation studies, which highlight new
opportunities for advancing MLLMs in infographic chart understanding. We
release InfoChartQA at https://github.com/CoolDawnAnt/InfoChartQA.

</details>


### [759] [Medical Large Vision Language Models with Multi-Image Visual Ability](https://arxiv.org/abs/2505.19031)
*Xikai Yang,Juzheng Miao,Yuchen Yuan,Jiaze Wang,Qi Dou,Jinpeng Li,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 该论文提出了Med-MIM指令数据集和基准测试，用于增强医疗大型视觉语言模型（LVLM）在多图像临床场景中的理解能力，并通过微调模型验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前医疗大型视觉语言模型（LVLM）在单图像问答任务中表现良好，但在处理需要复杂视觉理解能力（如时序推理和跨模态分析）的多图像临床任务方面仍有不足。

Method: 研究团队构建了包含83.2K个医疗多图像问答对的Med-MIM指令数据集，涵盖四种多图像视觉能力。基于此数据集，对Mantis和LLaVA-Med模型进行微调，开发了MIM-LLaVA-Med和Med-Mantis两种专用医疗视觉语言模型。同时，提出了Med-MIM基准测试来全面评估LVLM的多图像理解能力。

Result: 实验结果表明，Med-Mantis和MIM-LLaVA-Med在Med-MIM基准测试的保留集和外部集上均表现出色，验证了Med-MIM指令数据集在提升医疗LVLM多图像理解能力方面的有效性。

Conclusion: 该研究填补了医疗LVLM在多图像临床任务中的能力空白，提出的数据集和模型为医疗领域的多图像分析提供了有效工具。

Abstract: Medical large vision-language models (LVLMs) have demonstrated promising
performance across various single-image question answering (QA) benchmarks, yet
their capability in processing multi-image clinical scenarios remains
underexplored. Unlike single image based tasks, medical tasks involving
multiple images often demand sophisticated visual understanding capabilities,
such as temporal reasoning and cross-modal analysis, which are poorly supported
by current medical LVLMs. To bridge this critical gap, we present the Med-MIM
instruction dataset, comprising 83.2K medical multi-image QA pairs that span
four types of multi-image visual abilities (temporal understanding, reasoning,
comparison, co-reference). Using this dataset, we fine-tune Mantis and
LLaVA-Med, resulting in two specialized medical VLMs: MIM-LLaVA-Med and
Med-Mantis, both optimized for multi-image analysis. Additionally, we develop
the Med-MIM benchmark to comprehensively evaluate the medical multi-image
understanding capabilities of LVLMs. We assess eight popular LVLMs, including
our two models, on the Med-MIM benchmark. Experimental results show that both
Med-Mantis and MIM-LLaVA-Med achieve superior performance on the held-in and
held-out subsets of the Med-MIM benchmark, demonstrating that the Med-MIM
instruction dataset effectively enhances LVLMs' multi-image understanding
capabilities in the medical domain.

</details>


### [760] [Jodi: Unification of Visual Generation and Understanding via Joint Modeling](https://arxiv.org/abs/2505.19084)
*Yifeng Xu,Zhenliang He,Meina Kan,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: Jodi是一个统一的扩散框架，通过联合建模图像和多个标签域，实现了视觉生成与理解的统一。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习将视觉生成与理解视为独立任务，而人类智能中这两者紧密相连。本文旨在通过统一框架解决这一问题。

Method: 基于线性扩散变换器和角色切换机制，Jodi支持联合生成、可控生成和图像感知三种任务。

Result: 实验表明，Jodi在生成和理解任务上表现优异，并展现出对更广泛视觉领域的强扩展性。

Conclusion: Jodi成功统一了视觉生成与理解，为多任务视觉处理提供了有效解决方案。

Abstract: Visual generation and understanding are two deeply interconnected aspects of
human intelligence, yet they have been traditionally treated as separate tasks
in machine learning. In this paper, we propose Jodi, a diffusion framework that
unifies visual generation and understanding by jointly modeling the image
domain and multiple label domains. Specifically, Jodi is built upon a linear
diffusion transformer along with a role switch mechanism, which enables it to
perform three particular types of tasks: (1) joint generation, where the model
simultaneously generates images and multiple labels; (2) controllable
generation, where images are generated conditioned on any combination of
labels; and (3) image perception, where multiple labels can be predicted at
once from a given image. Furthermore, we present the Joint-1.6M dataset, which
contains 200,000 high-quality images collected from public sources, automatic
labels for 7 visual domains, and LLM-generated captions. Extensive experiments
demonstrate that Jodi excels in both generation and understanding tasks and
exhibits strong extensibility to a wider range of visual domains. Code is
available at https://github.com/VIPL-GENUN/Jodi.

</details>


### [761] [SATORI-R1: Incentivizing Multimodal Reasoning with Spatial Grounding and Verifiable Rewards](https://arxiv.org/abs/2505.19094)
*Chuming Shen,Wei Wei,Xiaoye Qu,Yu Cheng*

Main category: cs.CV

TL;DR: 论文提出SATORI方法，通过强化学习将VQA任务分解为三个可验证阶段，解决了传统自由推理在视觉问答中的局限性，并在多个基准测试中提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的自由推理方法在视觉问答任务中存在两个主要问题：推理链过长分散视觉焦点，以及不可验证的中间步骤增加计算成本。

Method: SATORI将VQA任务分解为全局图像描述、区域定位和答案预测三个阶段，每个阶段提供明确的奖励信号，并通过VQA-Verify数据集辅助训练。

Result: 实验表明，SATORI在七个VQA基准测试中性能一致提升，最高准确率提升15.7%，注意力图分析证实了对关键区域的聚焦增强。

Conclusion: SATORI通过可验证的阶段分解和强化学习优化，显著提升了VQA任务的准确率和效率。

Abstract: DeepSeek-R1 has demonstrated powerful reasoning capabilities in the text
domain through stable reinforcement learning (RL). Recently, in the multimodal
domain, works have begun to directly apply RL to generate R1-like free-form
reasoning for Visual Question Answering (VQA) tasks. However, multimodal tasks
share an intrinsically different nature from textual tasks, which heavily rely
on the understanding of the input image to solve the problem. Therefore, such
free-form reasoning faces two critical limitations in the VQA task: (1)
Extended reasoning chains diffuse visual focus away from task-critical regions,
degrading answer accuracy. (2) Unverifiable intermediate steps amplify
policy-gradient variance and computational costs overhead. To address these
issues, in this paper, we introduce SATORI ($\textbf{S}patially$
$\textbf{A}nchored$ $\textbf{T}ask$ $\textbf{O}ptimization$ with
$\textbf{R}e\textbf{I}nforcement$ Learning), which decomposes VQA into three
verifiable stages, including global image captioning, region localization, and
answer prediction, each supplying explicit reward signals. Furthermore, we also
introduce VQA-Verify, a 12k dataset annotated with answer-aligned captions and
bounding-boxes to facilitate training. Experiments demonstrate consistent
performance improvements across seven VQA benchmarks, achieving up to $15.7\%$
improvement in accuracy in accuracy compared to the R1-like baseline. Our
analysis of the attention map confirms enhanced focus on critical regions,
which brings improvements in accuracy. Our code is available at
https://github.com/justairr/SATORI-R1.

</details>


### [762] [An Interpretable Representation Learning Approach for Diffusion Tensor Imaging](https://arxiv.org/abs/2505.19110)
*Vishwa Mohan Singh,Alberto Gaston Villagran Asiares,Luisa Sophie Schuhmacher,Kate Rendall,Simon Weißbrod,David Rügamer,Inga Körte*

Main category: cs.CV

TL;DR: 该论文提出了一种新的2D DTI纤维追踪表示方法，结合Beta-Total Correlation VAE和空间广播解码器，提升了性别分类任务的性能和解耦效果。


<details>
  <summary>Details</summary>
Motivation: DTI纤维追踪能提供大脑结构连接的详细信息，但在深度学习模型中的有效表示和解释存在挑战。

Method: 将纤维追踪的各向异性分数（FA）编码为9x9灰度图像，通过Beta-Total Correlation VAE和空间广播解码器学习解耦且可解释的潜在嵌入。

Result: 与1D Group DNN基线相比，性别分类任务的F1分数提高了15.74%，且比3D表示具有更好的解耦效果。

Conclusion: 该方法显著提升了DTI纤维追踪在深度学习中的表示和解释能力，具有实际应用潜力。

Abstract: Diffusion Tensor Imaging (DTI) tractography offers detailed insights into the
structural connectivity of the brain, but presents challenges in effective
representation and interpretation in deep learning models. In this work, we
propose a novel 2D representation of DTI tractography that encodes tract-level
fractional anisotropy (FA) values into a 9x9 grayscale image. This
representation is processed through a Beta-Total Correlation Variational
Autoencoder with a Spatial Broadcast Decoder to learn a disentangled and
interpretable latent embedding. We evaluate the quality of this embedding using
supervised and unsupervised representation learning strategies, including
auxiliary classification, triplet loss, and SimCLR-based contrastive learning.
Compared to the 1D Group deep neural network (DNN) baselines, our approach
improves the F1 score in a downstream sex classification task by 15.74% and
shows a better disentanglement than the 3D representation.

</details>


### [763] [Can Visual Encoder Learn to See Arrows?](https://arxiv.org/abs/2505.19944)
*Naoyuki Terashita,Yusuke Tozaki,Hideaki Omote,Congkha Nguyen,Ryosuke Nakamoto,Yuta Koreeda,Hiroaki Ozaki*

Main category: cs.CV

TL;DR: 论文通过消除文本和位置偏差，提升视觉语言模型对图表中边的识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在识别图表中的边时表现不佳，作者认为这是由于过度依赖文本和位置偏差，导致模型无法学习显式的边特征。

Method: 通过在无文本和位置偏差的人工生成图表数据集上进行对比学习，训练图像编码器，并在探测、图像检索和描述任务中评估其性能。

Result: 微调后的模型在所有任务中均优于预训练的CLIP模型，并在描述任务中超过零样本的GPT-4o和LLaVA-Mistral。

Conclusion: 消除文本和位置偏差能有效提升视觉语言模型对边的识别能力，为图表理解提供了新的研究方向。

Abstract: The diagram is a visual representation of a relationship illustrated with
edges (lines or arrows), which is widely used in industrial and scientific
communication. Although recognizing diagrams is essential for vision language
models (VLMs) to comprehend domain-specific knowledge, recent studies reveal
that many VLMs fail to identify edges in images. We hypothesize that these
failures stem from an over-reliance on textual and positional biases,
preventing VLMs from learning explicit edge features. Based on this idea, we
empirically investigate whether the image encoder in VLMs can learn edge
representation through training on a diagram dataset in which edges are biased
neither by textual nor positional information. To this end, we conduct
contrastive learning on an artificially generated diagram--caption dataset to
train an image encoder and evaluate its diagram-related features on three
tasks: probing, image retrieval, and captioning. Our results show that the
finetuned model outperforms pretrained CLIP in all tasks and surpasses
zero-shot GPT-4o and LLaVA-Mistral in the captioning task. These findings
confirm that eliminating textual and positional biases fosters accurate edge
recognition in VLMs, offering a promising path for advancing diagram
understanding.

</details>


### [764] [Saliency-guided Emotion Modeling: Predicting Viewer Reactions from Video Stimuli](https://arxiv.org/abs/2505.19178)
*Akhila Yaragoppa,Siddharth*

Main category: cs.CV

TL;DR: 该研究利用深度学习提出了一种基于视觉显著性的情绪预测新方法，发现显著区域数量与观众情绪存在关联，并揭示了自报告情绪与面部表情分析的差异。


<details>
  <summary>Details</summary>
Motivation: 传统情感计算方法常忽略视频中的视觉显著性区域，而该研究旨在探索显著性特征与观众情绪之间的关系，以改进情感建模。

Method: 使用HD2S显著性模型和OpenFace面部动作单元分析，提取显著性区域面积和数量特征，结合深度学习进行情绪预测。

Result: 研究发现：(1)多显著区域视频易引发高愉悦-低唤醒情绪，(2)单一显著区域视频易引发低愉悦-高唤醒情绪，(3)自报告情绪与面部表情分析结果存在偏差。

Conclusion: 基于显著性的方法为情感建模提供了高效且可解释的替代方案，对内容创作、个性化媒体体验和情感计算研究具有启示意义。

Abstract: Understanding the emotional impact of videos is crucial for applications in
content creation, advertising, and Human-Computer Interaction (HCI).
Traditional affective computing methods rely on self-reported emotions, facial
expression analysis, and biosensing data, yet they often overlook the role of
visual saliency -- the naturally attention-grabbing regions within a video. In
this study, we utilize deep learning to introduce a novel saliency-based
approach to emotion prediction by extracting two key features: saliency area
and number of salient regions. Using the HD2S saliency model and OpenFace
facial action unit analysis, we examine the relationship between video saliency
and viewer emotions. Our findings reveal three key insights: (1) Videos with
multiple salient regions tend to elicit high-valence, low-arousal emotions, (2)
Videos with a single dominant salient region are more likely to induce
low-valence, high-arousal responses, and (3) Self-reported emotions often
misalign with facial expression-based emotion detection, suggesting limitations
in subjective reporting. By leveraging saliency-driven insights, this work
provides a computationally efficient and interpretable alternative for emotion
modeling, with implications for content creation, personalized media
experiences, and affective computing research.

</details>


### [765] [Pose Splatter: A 3D Gaussian Splatting Model for Quantifying Animal Pose and Appearance](https://arxiv.org/abs/2505.18342)
*Jack Goffinet,Youngjo Min,Carlo Tomasi,David E. Carlson*

Main category: cs.CV

TL;DR: Pose Splatter是一种新型框架，利用形状雕刻和3D高斯泼溅技术，无需动物几何先验知识、逐帧优化或手动标注，即可建模实验室动物的完整姿态和外观。


<details>
  <summary>Details</summary>
Motivation: 当前3D姿态估计技术存在表示细节有限、标注工作量大和逐帧优化成本高等问题，限制了细微动作研究和大规模分析的可行性。

Method: 提出Pose Splatter框架，结合形状雕刻和3D高斯泼溅技术，并引入旋转不变视觉嵌入技术，用于编码姿态和外观。

Result: 在老鼠、大鼠和斑胸草雀数据集上的实验表明，Pose Splatter能学习准确的3D动物几何，捕捉细微姿态变化，并在低维姿态嵌入上优于现有技术。

Conclusion: Pose Splatter通过消除标注和逐帧优化的瓶颈，支持大规模、长期行为分析，为基因型、神经活动和微观行为的高分辨率研究提供了可能。

Abstract: Accurate and scalable quantification of animal pose and appearance is crucial
for studying behavior. Current 3D pose estimation techniques, such as keypoint-
and mesh-based techniques, often face challenges including limited
representational detail, labor-intensive annotation requirements, and expensive
per-frame optimization. These limitations hinder the study of subtle movements
and can make large-scale analyses impractical. We propose Pose Splatter, a
novel framework leveraging shape carving and 3D Gaussian splatting to model the
complete pose and appearance of laboratory animals without prior knowledge of
animal geometry, per-frame optimization, or manual annotations. We also propose
a novel rotation-invariant visual embedding technique for encoding pose and
appearance, designed to be a plug-in replacement for 3D keypoint data in
downstream behavioral analyses. Experiments on datasets of mice, rats, and
zebra finches show Pose Splatter learns accurate 3D animal geometries. Notably,
Pose Splatter represents subtle variations in pose, provides better
low-dimensional pose embeddings over state-of-the-art as evaluated by humans,
and generalizes to unseen data. By eliminating annotation and per-frame
optimization bottlenecks, Pose Splatter enables analysis of large-scale,
longitudinal behavior needed to map genotype, neural activity, and
micro-behavior at unprecedented resolution.

</details>


### [766] [PosePilot: An Edge-AI Solution for Posture Correction in Physical Exercises](https://arxiv.org/abs/2505.19186)
*Rushiraj Gadhvi,Priyansh Desai,Siddharth*

Main category: cs.CV

TL;DR: PosePilot是一个集成姿态识别与实时个性化纠正反馈的AI健身系统，特别适用于瑜伽等需要精确时空对齐的运动，能够在边缘设备上高效运行。


<details>
  <summary>Details</summary>
Motivation: 当前AI驱动的健身系统在自动姿态纠正方面仍面临挑战，传统解决方案存在局限性。本文旨在通过PosePilot系统解决这一问题，提供实时精准的姿势调整反馈。

Method: 采用Vanilla LSTM捕捉时间依赖性进行姿态识别，结合BiLSTM和多头注意力机制增强运动上下文处理能力，选择性关注关键肢体角度以实现高效错误检测。

Result: PosePilot能够自动识别人体姿态，在瑜伽等运动中提供即时个性化纠正反馈，并保持轻量级模型，适合在边缘设备上部署。

Conclusion: PosePilot通过创新的姿态识别与反馈机制，为家庭和户外运动提供了高效、精准的实时姿势纠正解决方案。

Abstract: Automated pose correction remains a significant challenge in AI-driven
fitness systems, despite extensive research in activity recognition. This work
presents PosePilot, a novel system that integrates pose recognition with
real-time personalized corrective feedback, overcoming the limitations of
traditional fitness solutions. Using Yoga, a discipline requiring precise
spatio-temporal alignment as a case study, we demonstrate PosePilot's ability
to analyze complex physical movements. Designed for deployment on edge devices,
PosePilot can be extended to various at-home and outdoor exercises. We employ a
Vanilla LSTM, allowing the system to capture temporal dependencies for pose
recognition. Additionally, a BiLSTM with multi-head Attention enhances the
model's ability to process motion contexts, selectively focusing on key limb
angles for accurate error detection while maintaining computational efficiency.
As part of this work, we introduce a high-quality video dataset used for
evaluating our models. Most importantly, PosePilot provides instant corrective
feedback at every stage of a movement, ensuring precise posture adjustments
throughout the exercise routine. The proposed approach 1) performs automatic
human posture recognition, 2) provides personalized posture correction feedback
at each instant which is crucial in Yoga, and 3) offers a lightweight and
robust posture correction model feasible for deploying on edge devices in
real-world environments.

</details>


### [767] [Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion](https://arxiv.org/abs/2505.20053)
*Zheqi Lv,Junhao Chen,Qi Tian,Keting Yin,Shengyu Zhang,Fei Wu*

Main category: cs.CV

TL;DR: 该论文提出了一种名为PPAD的新框架，通过引入多模态大语言模型（MLLM）作为语义观察者，实时分析中间生成结果并纠正语义不一致，从而提升文本到图像生成的准确性和质量。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在文本到图像生成中缺乏可解释的语义监督和纠正机制，导致生成过程中常出现对象混淆、空间错误、计数不准确和语义元素缺失等问题，严重影响图像质量和提示对齐。

Method: 提出MLLM Semantic-Corrected Ping-Pong-Ahead Diffusion (PPAD)框架，利用多模态大语言模型在推理过程中实时分析中间生成结果，识别潜在语义不一致，并将反馈转化为可控信号以指导后续去噪步骤。

Result: 实验表明，PPAD在极少的扩散步骤中实现语义纠正，显著提升了生成图像的质量和提示对齐效果，具有强通用性和可扩展性。

Conclusion: PPAD框架通过引入MLLM作为语义观察者，有效解决了当前扩散模型在文本到图像生成中的语义不一致问题，为生成轨迹提供了可操作的指导。

Abstract: Diffusion models have become the mainstream architecture for text-to-image
generation, achieving remarkable progress in visual quality and prompt
controllability. However, current inference pipelines generally lack
interpretable semantic supervision and correction mechanisms throughout the
denoising process. Most existing approaches rely solely on post-hoc scoring of
the final image, prompt filtering, or heuristic resampling strategies-making
them ineffective in providing actionable guidance for correcting the generative
trajectory. As a result, models often suffer from object confusion, spatial
errors, inaccurate counts, and missing semantic elements, severely compromising
prompt-image alignment and image quality. To tackle these challenges, we
propose MLLM Semantic-Corrected Ping-Pong-Ahead Diffusion (PPAD), a novel
framework that, for the first time, introduces a Multimodal Large Language
Model (MLLM) as a semantic observer during inference. PPAD performs real-time
analysis on intermediate generations, identifies latent semantic
inconsistencies, and translates feedback into controllable signals that
actively guide the remaining denoising steps. The framework supports both
inference-only and training-enhanced settings, and performs semantic correction
at only extremely few diffusion steps, offering strong generality and
scalability. Extensive experiments demonstrate PPAD's significant improvements.

</details>


### [768] [Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models](https://arxiv.org/abs/2505.20152)
*Kai Sun,Yushi Bai,Zhen Yang,Jiajie Zhang,Ji Qi,Lei Hou,Juanzi Li*

Main category: cs.CV

TL;DR: 论文提出了一种新的硬负对比学习框架MMCLIP，通过结合图像和文本的对比学习增强几何理解，并训练出在几何推理任务上表现优异的模型MMGeoLM。


<details>
  <summary>Details</summary>
Motivation: 现有的大型多模态模型（LMMs）在视觉感知任务上表现优异，但在几何问题解决等需要细致推理的场景中存在局限，主要由于对比学习在概括性描述上的固有缺陷。

Method: 提出硬负对比学习框架MMCLIP，结合基于图像的对比学习（使用生成式硬负样本）和基于文本的对比学习（使用规则和检索式硬负样本），并训练LMM模型MMGeoLM。

Result: MMGeoLM在三个几何推理基准测试中显著优于其他开源模型，7B规模的模型性能可媲美GPT-4o等闭源模型。

Conclusion: 通过硬负对比学习框架有效提升了模型在几何推理任务上的性能，同时研究了不同负样本构建方法及数量对模型性能的影响。

Abstract: Benefiting from contrastively trained visual encoders on large-scale natural
scene images, Large Multimodal Models (LMMs) have achieved remarkable
performance across various visual perception tasks. However, the inherent
limitations of contrastive learning upon summarized descriptions fundamentally
restrict the capabilities of models in meticulous reasoning, particularly in
crucial scenarios of geometric problem-solving. To enhance geometric
understanding, we propose a novel hard negative contrastive learning framework
for the vision encoder, which combines image-based contrastive learning using
generation-based hard negatives created by perturbing diagram generation code,
and text-based contrastive learning using rule-based negatives derived from
modified geometric descriptions and retrieval-based negatives selected based on
caption similarity. We train CLIP using our strong negative learning method,
namely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for
geometric problem-solving. Experiments show that our trained model, MMGeoLM,
significantly outperforms other open-source models on three geometric reasoning
benchmarks. Even with a size of 7B, it can rival powerful closed-source models
like GPT-4o. We further study the impact of different negative sample
construction methods and the number of negative samples on the geometric
reasoning performance of LMM, yielding fruitful conclusions. The code and
dataset are available at https://github.com/THU-KEG/MMGeoLM.

</details>


### [769] [RAISE: Realness Assessment for Image Synthesis and Evaluation](https://arxiv.org/abs/2505.19233)
*Aniruddha Mukherjee,Spriha Dubey,Somdyuti Paul*

Main category: cs.CV

TL;DR: 论文提出RAISE数据集，通过人类评估AI生成图像的感知真实度，并基于深度视觉模型建立预测基线。


<details>
  <summary>Details</summary>
Motivation: 由于AI生成视觉内容的真实性评估具有主观性，需要可靠方法替代真实数据，因此研究如何客观评估其感知真实度。

Method: 进行大规模人类研究，收集真实与AI生成图像的感知真实度评分，构建RAISE数据集，并训练多个模型建立预测基线。

Result: 实验表明，深度视觉模型的特征能有效捕捉主观真实度，RAISE为开发客观评估模型提供了资源。

Conclusion: RAISE数据集及基线模型为AI生成视觉内容的真实性评估提供了可靠工具。

Abstract: The rapid advancement of generative AI has enabled the creation of highly
photorealistic visual content, offering practical substitutes for real images
and videos in scenarios where acquiring real data is difficult or expensive.
However, reliably substituting real visual content with AI-generated
counterparts requires robust assessment of the perceived realness of
AI-generated visual content, a challenging task due to its inherent subjective
nature. To address this, we conducted a comprehensive human study evaluating
the perceptual realness of both real and AI-generated images, resulting in a
new dataset, containing images paired with subjective realness scores,
introduced as RAISE in this paper. Further, we develop and train multiple
models on RAISE to establish baselines for realness prediction. Our
experimental results demonstrate that features derived from deep foundation
vision models can effectively capture the subjective realness. RAISE thus
provides a valuable resource for developing robust, objective models of
perceptual realness assessment.

</details>


### [770] [VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction](https://arxiv.org/abs/2505.20279)
*Zhiwen Fan,Jian Zhang,Renjie Li,Junge Zhang,Runjin Chen,Hezhen Hu,Kevin Wang,Huaizhi Qu,Dilin Wang,Zhicheng Yan,Hongyu Xu,Justin Theiss,Tianlong Chen,Jiachen Li,Zhengzhong Tu,Zhangyang Wang,Rakesh Ranjan*

Main category: cs.CV

TL;DR: 本文提出VLM-3R框架，通过3D重建指令调优实现单目视频的空间理解和语言对齐，并在时空推理任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖外部深度传感器或预构建3D地图，限制了在单目视频输入和时间敏感应用中的扩展性。

Method: VLM-3R结合几何编码器生成隐式3D标记，利用空间-视觉-视图融合和20万+ QA对进行指令调优，实现空间与语言对齐。

Result: 模型在时空推理基准测试中表现卓越，支持单目3D空间辅助和具身推理。

Conclusion: VLM-3R不仅增强了视觉空间推理能力，还能理解3D上下文的时间变化，具有高准确性和扩展性。

Abstract: The rapid advancement of Large Multimodal Models (LMMs) for 2D images and
videos has motivated extending these models to understand 3D scenes, aiming for
human-like visual-spatial intelligence. Nevertheless, achieving deep spatial
understanding comparable to human capabilities poses significant challenges in
model encoding and data acquisition. Existing methods frequently depend on
external depth sensors for geometry capture or utilize off-the-shelf algorithms
for pre-constructing 3D maps, thereby limiting their scalability, especially
with prevalent monocular video inputs and for time-sensitive applications. In
this work, we introduce VLM-3R, a unified framework for Vision-Language Models
(VLMs) that incorporates 3D Reconstructive instruction tuning. VLM-3R processes
monocular video frames by employing a geometry encoder to derive implicit 3D
tokens that represent spatial understanding. Leveraging our Spatial-Visual-View
Fusion and over 200K curated 3D reconstructive instruction tuning
question-answer (QA) pairs, VLM-3R effectively aligns real-world spatial
context with language instructions. This enables monocular 3D spatial
assistance and embodied reasoning. To facilitate the evaluation of temporal
reasoning, we introduce the Vision-Spatial-Temporal Intelligence benchmark,
featuring over 138.6K QA pairs across five distinct tasks focused on evolving
spatial relationships. Extensive experiments demonstrate that our model,
VLM-3R, not only facilitates robust visual-spatial reasoning but also enables
the understanding of temporal 3D context changes, excelling in both accuracy
and scalability.

</details>


### [771] [Visualized Text-to-Image Retrieval](https://arxiv.org/abs/2505.20291)
*Di Wu,Yixin Wan,Kai-Wei Chang*

Main category: cs.CV

TL;DR: VisRet提出了一种新的文本到图像检索范式，通过将文本查询先转换为图像再进行检索，显著提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态检索方法在识别细微视觉空间特征方面存在局限，VisRet旨在通过规避跨模态对齐来提升检索效果。

Method: VisRet首先通过文本到图像生成将文本查询投影到图像模态，然后在图像模态内进行检索。

Result: 在三个知识密集型基准测试中，VisRet将文本到图像检索性能提升了24.5%至32.7%，并显著提升了视觉问答的准确性。

Conclusion: VisRet是一种即插即用的方法，兼容现有检索器，是知识密集型多模态系统的有效模块。

Abstract: We propose Visualize-then-Retrieve (VisRet), a new paradigm for Text-to-Image
(T2I) retrieval that mitigates the limitations of cross-modal similarity
alignment of existing multi-modal embeddings. VisRet first projects textual
queries into the image modality via T2I generation. Then, it performs retrieval
within the image modality to bypass the weaknesses of cross-modal retrievers in
recognizing subtle visual-spatial features. Experiments on three
knowledge-intensive T2I retrieval benchmarks, including a newly introduced
multi-entity benchmark, demonstrate that VisRet consistently improves T2I
retrieval by 24.5% to 32.7% NDCG@10 across different embedding models. VisRet
also significantly benefits downstream visual question answering accuracy when
used in retrieval-augmented generation pipelines. The method is plug-and-play
and compatible with off-the-shelf retrievers, making it an effective module for
knowledge-intensive multi-modal systems. Our code and the new benchmark are
publicly available at https://github.com/xiaowu0162/Visualize-then-Retrieve.

</details>


### [772] [DiSA: Diffusion Step Annealing in Autoregressive Image Generation](https://arxiv.org/abs/2505.20297)
*Qinyu Zhao,Jaskirat Singh,Ming Xu,Akshay Asthana,Stephen Gould,Liang Zheng*

Main category: cs.CV

TL;DR: 论文提出扩散步退火(DiSA)方法，通过逐步减少扩散步数来加速自回归模型中的扩散采样，实现5-10倍加速且保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有自回归模型采用扩散采样导致推理效率低下（需50-100步生成一个token）。研究发现随着生成token增多，后续token分布更受限、更易采样。

Method: 提出训练无关的DiSA方法：根据生成进度动态减少扩散步数（如从初始50步逐步降至5步），与现有扩散加速方法互补。

Result: 在MAR/Harmon上实现5-10倍加速，FlowAR/xAR上1.4-2.5倍加速，且生成质量无损。仅需少量代码即可部署。

Conclusion: DiSA通过利用自回归过程中token分布的动态特性，高效解决了扩散采样的计算瓶颈，具有即插即用的实用价值。

Abstract: An increasing number of autoregressive models, such as MAR, FlowAR, xAR, and
Harmon adopt diffusion sampling to improve the quality of image generation.
However, this strategy leads to low inference efficiency, because it usually
takes 50 to 100 steps for diffusion to sample a token. This paper explores how
to effectively address this issue. Our key motivation is that as more tokens
are generated during the autoregressive process, subsequent tokens follow more
constrained distributions and are easier to sample. To intuitively explain, if
a model has generated part of a dog, the remaining tokens must complete the dog
and thus are more constrained. Empirical evidence supports our motivation: at
later generation stages, the next tokens can be well predicted by a multilayer
perceptron, exhibit low variance, and follow closer-to-straight-line denoising
paths from noise to tokens. Based on our finding, we introduce diffusion step
annealing (DiSA), a training-free method which gradually uses fewer diffusion
steps as more tokens are generated, e.g., using 50 steps at the beginning and
gradually decreasing to 5 steps at later stages. Because DiSA is derived from
our finding specific to diffusion in autoregressive models, it is complementary
to existing acceleration methods designed for diffusion alone. DiSA can be
implemented in only a few lines of code on existing models, and albeit simple,
achieves $5-10\times$ faster inference for MAR and Harmon and $1.4-2.5\times$
for FlowAR and xAR, while maintaining the generation quality.

</details>


### [773] [Enhancing Text-to-Image Diffusion Transformer via Split-Text Conditioning](https://arxiv.org/abs/2505.19261)
*Yu Zhang,Jialei Zhou,Xinchen Li,Qi Zhang,Zhongwei Wan,Tianyu Wang,Duoqian Miao,Changwei Wang,Longbing Cao*

Main category: cs.CV

TL;DR: 论文提出DiT-ST框架，通过拆分文本描述并分层注入扩散模型，解决现有文本到图像生成中完整文本理解缺陷的问题。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像扩散模型采用完整文本条件输入，但由于复杂语法导致语义理解缺陷，易忽略关键细节或引发语义混淆。

Method: 提出DiT-ST框架：1) 用大语言模型拆分文本为简化句子；2) 按语义类型分层排序；3) 在去噪阶段分时注入不同语义单元。

Result: 实验验证DiT-ST能有效缓解完整文本理解缺陷，提升不同阶段特定语义单元的表示学习能力。

Conclusion: 分层次增量注入拆分文本的方法显著改善了扩散变换器对复杂文本的理解能力。

Abstract: Current text-to-image diffusion generation typically employs complete-text
conditioning. Due to the intricate syntax, diffusion transformers (DiTs)
inherently suffer from a comprehension defect of complete-text captions.
One-fly complete-text input either overlooks critical semantic details or
causes semantic confusion by simultaneously modeling diverse semantic primitive
types. To mitigate this defect of DiTs, we propose a novel split-text
conditioning framework named DiT-ST. This framework converts a complete-text
caption into a split-text caption, a collection of simplified sentences, to
explicitly express various semantic primitives and their interconnections. The
split-text caption is then injected into different denoising stages of DiT-ST
in a hierarchical and incremental manner. Specifically, DiT-ST leverages Large
Language Models to parse captions, extracting diverse primitives and
hierarchically sorting out and constructing these primitives into a split-text
input. Moreover, we partition the diffusion denoising process according to its
differential sensitivities to diverse semantic primitive types and determine
the appropriate timesteps to incrementally inject tokens of diverse semantic
primitive types into input tokens via cross-attention. In this way, DiT-ST
enhances the representation learning of specific semantic primitive types
across different stages. Extensive experiments validate the effectiveness of
our proposed DiT-ST in mitigating the complete-text comprehension defect.

</details>


### [774] [TextDiffuser-RL: Efficient and Robust Text Layout Optimization for High-Fidelity Text-to-Image Synthesis](https://arxiv.org/abs/2505.19291)
*Kazi Mahathir Rahman,Showrin Rahman,Sharmin Sultana Srishty*

Main category: cs.CV

TL;DR: 本文提出了一种结合强化学习与扩散模型的两阶段文本嵌入图像生成方法，显著提升了生成速度与资源效率，同时保持或超越现有方法的生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的文本嵌入图像生成方法（如TextDiffuser-2）存在计算资源消耗大、跨平台运行效率低的问题，亟需优化。

Method: 采用强化学习快速生成优化文本布局（边界框预测），再结合扩散模型进行图像合成，形成两阶段流程。

Result: 在MARIOEval基准测试中，OCR和CLIPScore指标接近SOTA模型，运行速度提升97.64%，内存占用仅2MB。

Conclusion: 该框架在保持生成质量的同时实现了高效跨平台部署，为工业应用提供了实用解决方案。

Abstract: Text-embedded image generation plays a critical role in industries such as
graphic design, advertising, and digital content creation. Text-to-Image
generation methods leveraging diffusion models, such as TextDiffuser-2, have
demonstrated promising results in producing images with embedded text.
TextDiffuser-2 effectively generates bounding box layouts that guide the
rendering of visual text, achieving high fidelity and coherence. However,
existing approaches often rely on resource-intensive processes and are limited
in their ability to run efficiently on both CPU and GPU platforms. To address
these challenges, we propose a novel two-stage pipeline that integrates
reinforcement learning (RL) for rapid and optimized text layout generation with
a diffusion-based image synthesis model. Our RL-based approach significantly
accelerates the bounding box prediction step while reducing overlaps, allowing
the system to run efficiently on both CPUs and GPUs. Extensive evaluations
demonstrate that our framework maintains or surpasses TextDiffuser-2's quality
in text placement and image synthesis, with markedly faster runtime and
increased flexibility. Extensive evaluations demonstrate that our framework
maintains or surpasses TextDiffuser-2's quality in text placement and image
synthesis, with markedly faster runtime and increased flexibility. Our approach
has been evaluated on the MARIOEval benchmark, achieving OCR and CLIPScore
metrics close to state-of-the-art models, while being 97.64% more faster and
requiring only 2MB of memory to run.

</details>


### [775] [Advancing Limited-Angle CT Reconstruction Through Diffusion-Based Sinogram Completion](https://arxiv.org/abs/2505.19385)
*Jiaqi Guo,Santiago Lopez-Tapia,Aggelos K. Katsaggelos*

Main category: cs.CV

TL;DR: 提出一种基于MR-SDEs的sinogram修复方法，通过扩散模型加速并准确完成缺失角度数据，结合后处理模块提升LACT重建质量。


<details>
  <summary>Details</summary>
Motivation: 有限角度计算机断层扫描（LACT）因缺失角度信息面临挑战，传统方法在图像域操作效果有限，需在投影层面直接修复数据。

Method: 利用均值回复随机微分方程（MR-SDEs）的扩散模型进行sinogram修复，结合蒸馏技术和伪逆矩阵约束加速扩散过程，后处理模块通过反投影进一步优化重建。

Result: 定量实验表明，该方法在感知质量和保真度上均达到最先进水平，有效抑制伪影并保留结构细节。

Conclusion: 该方法为科学和临床中的LACT重建提供了高效、准确的解决方案。

Abstract: Limited Angle Computed Tomography (LACT) often faces significant challenges
due to missing angular information. Unlike previous methods that operate in the
image domain, we propose a new method that focuses on sinogram inpainting. We
leverage MR-SDEs, a variant of diffusion models that characterize the diffusion
process with mean-reverting stochastic differential equations, to fill in
missing angular data at the projection level. Furthermore, by combining
distillation with constraining the output of the model using the pseudo-inverse
of the inpainting matrix, the diffusion process is accelerated and done in a
step, enabling efficient and accurate sinogram completion. A subsequent
post-processing module back-projects the inpainted sinogram into the image
domain and further refines the reconstruction, effectively suppressing
artifacts while preserving critical structural details. Quantitative
experimental results demonstrate that the proposed method achieves
state-of-the-art performance in both perceptual and fidelity quality, offering
a promising solution for LACT reconstruction in scientific and clinical
applications.

</details>


### [776] [Force Prompting: Video Generation Models Can Learn and Generalize Physics-based Control Signals](https://arxiv.org/abs/2505.19386)
*Nate Gillman,Charles Herrmann,Michael Freeman,Daksh Aggarwal,Evan Luo,Deqing Sun,Chen Sun*

Main category: cs.CV

TL;DR: 该论文提出了一种利用物理力作为控制信号生成视频的方法，通过力提示（force prompts）实现用户与图像的交互，无需在推理时使用3D资源或物理模拟器。


<details>
  <summary>Details</summary>
Motivation: 尽管视频生成模型在导航方面已有广泛研究，但模拟真实世界物理力的交互仍较少探索。论文旨在探索如何利用物理力作为控制信号，使视频生成模型能够更真实地响应物理交互。

Method: 论文提出力提示方法，通过局部点力（如戳植物）和全局力场（如风吹布料）控制视频生成。利用Blender合成的视频数据训练模型，仅需少量示例即可泛化到多样化的场景和材质。

Result: 该方法在仅15k训练样本和4块A100 GPU训练一天的情况下，生成的视频在力遵循和物理真实性上优于现有方法，且能泛化到多种几何、场景和材质。

Conclusion: 研究表明，视频生成模型能够通过有限的合成数据训练，实现对物理力的真实响应，推动世界模型更接近真实物理交互。所有数据集、代码和模型均已开源。

Abstract: Recent advances in video generation models have sparked interest in world
models capable of simulating realistic environments. While navigation has been
well-explored, physically meaningful interactions that mimic real-world forces
remain largely understudied. In this work, we investigate using physical forces
as a control signal for video generation and propose force prompts which enable
users to interact with images through both localized point forces, such as
poking a plant, and global wind force fields, such as wind blowing on fabric.
We demonstrate that these force prompts can enable videos to respond
realistically to physical control signals by leveraging the visual and motion
prior in the original pretrained model, without using any 3D asset or physics
simulator at inference. The primary challenge of force prompting is the
difficulty in obtaining high quality paired force-video training data, both in
the real world due to the difficulty of obtaining force signals, and in
synthetic data due to limitations in the visual quality and domain diversity of
physics simulators. Our key finding is that video generation models can
generalize remarkably well when adapted to follow physical force conditioning
from videos synthesized by Blender, even with limited demonstrations of few
objects. Our method can generate videos which simulate forces across diverse
geometries, settings, and materials. We also try to understand the source of
this generalization and perform ablations that reveal two key elements: visual
diversity and the use of specific text keywords during training. Our approach
is trained on only around 15k training examples for a single day on four A100
GPUs, and outperforms existing methods on force adherence and physics realism,
bringing world models closer to real-world physics interactions. We release all
datasets, code, weights, and interactive video demos at our project page.

</details>


### [777] [CSTrack: Enhancing RGB-X Tracking via Compact Spatiotemporal Features](https://arxiv.org/abs/2505.19434)
*X. Feng,D. Zhang,S. Hu,X. Li,M. Wu,J. Zhang,X. Chen,K. Huang*

Main category: cs.CV

TL;DR: CSTrack提出了一种新颖的RGB-X跟踪器，通过紧凑时空特征建模简化结构并提升效率，在主流基准测试中达到新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-X跟踪器采用双分支处理不同模态数据，导致特征空间分散、计算复杂，且时空建模资源受限。

Method: 提出空间紧凑模块整合双模态输入，实现模态内外空间建模；设计时间紧凑模块通过目标分布热图压缩表征时序特征。

Result: 在主流RGB-X基准测试中取得最优性能，代码模型已开源。

Conclusion: CSTrack通过紧凑时空特征建模实现了高效简洁的跨模态目标跟踪。

Abstract: Effectively modeling and utilizing spatiotemporal features from RGB and other
modalities (\eg, depth, thermal, and event data, denoted as X) is the core of
RGB-X tracker design. Existing methods often employ two parallel branches to
separately process the RGB and X input streams, requiring the model to
simultaneously handle two dispersed feature spaces, which complicates both the
model structure and computation process. More critically, intra-modality
spatial modeling within each dispersed space incurs substantial computational
overhead, limiting resources for inter-modality spatial modeling and temporal
modeling. To address this, we propose a novel tracker, CSTrack, which focuses
on modeling Compact Spatiotemporal features to achieve simple yet effective
tracking. Specifically, we first introduce an innovative Spatial Compact Module
that integrates the RGB-X dual input streams into a compact spatial feature,
enabling thorough intra- and inter-modality spatial modeling. Additionally, we
design an efficient Temporal Compact Module that compactly represents temporal
features by constructing the refined target distribution heatmap. Extensive
experiments validate the effectiveness of our compact spatiotemporal modeling
method, with CSTrack achieving new SOTA results on mainstream RGB-X benchmarks.
The code and models will be released at:
https://github.com/XiaokunFeng/CSTrack.

</details>


### [778] [C3R: Channel Conditioned Cell Representations for unified evaluation in microscopy imaging](https://arxiv.org/abs/2505.18745)
*Umar Marikkar,Syed Sameed Husain,Muhammad Awais,Sara Atito*

Main category: cs.CV

TL;DR: 该论文提出了一种名为C3R的框架，通过将免疫组化图像通道分为上下文和概念两类，解决了现有方法在跨数据集评估中的不足，并在ID和OOD任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 免疫组化（IHC）图像因染色协议不同导致通道数量和配置不一致，现有深度学习方法难以支持跨数据集的零样本评估。

Method: 提出C3R框架，包括基于上下文-概念原则的通道自适应编码器架构和掩码知识蒸馏训练策略。

Result: C3R在ID和OOD任务上均优于现有基准，且在CHAMMI基准测试中表现优于通道自适应方法。

Conclusion: C3R为IHC数据集的跨数据集泛化提供了新途径，无需特定数据集的适配或重新训练。

Abstract: Immunohistochemical (IHC) images reveal detailed information about structures
and functions at the subcellular level. However, unlike natural images, IHC
datasets pose challenges for deep learning models due to their inconsistencies
in channel count and configuration, stemming from varying staining protocols
across laboratories and studies. Existing approaches build channel-adaptive
models, which unfortunately fail to support out-of-distribution (OOD)
evaluation across IHC datasets and cannot be applied in a true zero-shot
setting with mismatched channel counts. To address this, we introduce a
structured view of cellular image channels by grouping them into either context
or concept, where we treat the context channels as a reference to the concept
channels in the image. We leverage this context-concept principle to develop
Channel Conditioned Cell Representations (C3R), a framework designed for
unified evaluation on in-distribution (ID) and OOD datasets. C3R is a two-fold
framework comprising a channel-adaptive encoder architecture and a masked
knowledge distillation training strategy, both built around the context-concept
principle. We find that C3R outperforms existing benchmarks on both ID and OOD
tasks, while a trivial implementation of our core idea also outperforms the
channel-adaptive methods reported on the CHAMMI benchmark. Our method opens a
new pathway for cross-dataset generalization between IHC datasets, without
requiring dataset-specific adaptation or retraining.

</details>


### [779] [MM-Prompt: Cross-Modal Prompt Tuning for Continual Visual Question Answering](https://arxiv.org/abs/2505.19455)
*Xu Li,Fan Lyu*

Main category: cs.CV

TL;DR: 论文提出MM-Prompt框架，通过跨模态提示查询和恢复解决持续视觉问答中的模态不平衡问题，提升准确性和知识保留。


<details>
  <summary>Details</summary>
Motivation: 现有持续视觉问答方法采用跨模态提示隔离，导致模态不平衡和性能下降。

Method: MM-Prompt框架结合跨模态提示查询和恢复，通过对齐损失防止表示漂移。

Result: 实验表明MM-Prompt在准确性和知识保留上优于现有方法，保持模态平衡。

Conclusion: MM-Prompt有效解决模态不平衡问题，提升持续学习性能。

Abstract: Continual Visual Question Answering (CVQA) based on pre-trained models(PTMs)
has achieved promising progress by leveraging prompt tuning to enable continual
multi-modal learning. However, most existing methods adopt cross-modal prompt
isolation, constructing visual and textual prompts separately, which
exacerbates modality imbalance and leads to degraded performance over time. To
tackle this issue, we propose MM-Prompt, a novel framework incorporating
cross-modal prompt query and cross-modal prompt recovery. The former enables
balanced prompt selection by incorporating cross-modal signals during query
formation, while the latter promotes joint prompt reconstruction through
iterative cross-modal interactions, guided by an alignment loss to prevent
representational drift. Extensive experiments show that MM-Prompt surpasses
prior approaches in accuracy and knowledge retention, while maintaining
balanced modality engagement throughout continual learning.

</details>


### [780] [Dual-Path Stable Soft Prompt Generation for Domain Generalization](https://arxiv.org/abs/2505.18770)
*Yuedi Zhang,Shuanghao Bai,Wanqi Zhou,Zhirong Luan,Badong Chen*

Main category: cs.CV

TL;DR: 该论文针对领域泛化（DG）中提示生成方法存在的提示变异性问题，提出了一种基于负学习的双路径稳定软提示生成框架（DPSPG），通过引入互补提示生成器减少误导信息，提高了提示的稳定性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练视觉语言模型（VLMs）的提示调优方法在领域泛化中表现良好，但手动或固定提示难以捕捉领域特定特征。动态生成的实例和领域特定提示虽能丰富领域信息，但存在提示变异性问题，即相同输入在不同随机种子下生成不一致且次优的提示。

Method: 论文提出双路径稳定软提示生成（DPSPG）框架，通过引入互补提示生成器生成负提示，减少误导信息。理论分析表明负学习能通过增大有效边界和降低梯度范数上界，生成更鲁棒的提示。

Result: 在五个DG基准数据集上的实验表明，DPSPG在保持提示稳定性的同时，性能优于现有最先进方法。

Conclusion: DPSPG通过负学习有效解决了提示变异性问题，显著提升了提示的稳定性和泛化能力，为领域泛化任务提供了新的解决方案。

Abstract: Domain generalization (DG) aims to learn a model using data from one or
multiple related but distinct source domains that can generalize well to unseen
out-of-distribution target domains. Inspired by the success of large
pre-trained vision-language models (VLMs), prompt tuning has emerged as an
effective generalization strategy. However, it often struggles to capture
domain-specific features due to its reliance on manually or fixed prompt
inputs. Recently, some prompt generation methods have addressed this limitation
by dynamically generating instance-specific and domain-specific prompts for
each input, enriching domain information and demonstrating potential for
enhanced generalization. Through further investigation, we identify a notable
issue in existing prompt generation methods: the same input often yields
significantly different and suboptimal prompts across different random seeds, a
phenomenon we term Prompt Variability. To address this, we introduce negative
learning into the prompt generation process and propose Dual-Path Stable Soft
Prompt Generation (DPSPG), a transformer-based framework designed to improve
both the stability and generalization of prompts. Specifically, DPSPG
incorporates a complementary prompt generator to produce negative prompts,
thereby reducing the risk of introducing misleading information. Both
theoretical and empirical analyses demonstrate that negative learning leads to
more robust and effective prompts by increasing the effective margin and
reducing the upper bound of the gradient norm. Extensive experiments on five DG
benchmark datasets show that DPSPG consistently outperforms state-of-the-art
methods while maintaining prompt stability.

</details>


### [781] [Enhancing Visual Reliance in Text Generation: A Bayesian Perspective on Mitigating Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2505.19498)
*Nanxing Hu,Xiaoyue Duan,Jinchao Zhang,Guoliang Kang*

Main category: cs.CV

TL;DR: 该论文针对大型视觉语言模型（LVLM）中的幻觉问题，提出了一种从贝叶斯角度出发的三方面解决方案，包括去除冗余视觉标记、纠正先验信息和适时停止文本生成，实验证明该方法有效减少了幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在生成文本时常常出现与视觉输入不匹配的幻觉问题，限制了其在实际应用中的可靠性。现有方法未能系统性地增强模型对视觉内容的依赖，因此需要一种更有效的方法来解决这一问题。

Method: 论文从贝叶斯角度分析了导致LVLM视觉依赖退化的因素，并提出了三方面解决方案：1) 去除冗余视觉标记以避免干扰；2) 纠正模型中的不适当先验信息；3) 在预测后验分布崩溃时停止文本生成。

Result: 在POPE、CHAIR和MME三个基准测试上的广泛实验表明，该方法能持续减少LVLM的幻觉问题，并优于之前的最先进方法。

Conclusion: 通过系统性增强LVLM对视觉内容的依赖，该方法有效缓解了幻觉问题，提升了模型在实际应用中的可靠性。

Abstract: Large Vision-Language Models (LVLMs) usually generate texts which satisfy
context coherence but don't match the visual input. Such a hallucination issue
hinders LVLMs' applicability in the real world. The key to solving
hallucination in LVLM is to make the text generation rely more on the visual
content. Most previous works choose to enhance/adjust the features/output of a
specific modality (i.e., visual or textual) to alleviate hallucinations in
LVLM, which do not explicitly or systematically enhance the visual reliance. In
this paper, we comprehensively investigate the factors which may degenerate the
visual reliance in text generation of LVLM from a Bayesian perspective. Based
on our observations, we propose to mitigate hallucination in LVLM from three
aspects. Firstly, we observe that not all visual tokens are informative in
generating meaningful texts. We propose to evaluate and remove redundant visual
tokens to avoid their disturbance. Secondly, LVLM may encode inappropriate
prior information, making it lean toward generating unexpected words. We
propose a simple yet effective way to rectify the prior from a Bayesian
perspective. Thirdly, we observe that starting from certain steps, the
posterior of next-token prediction conditioned on visual tokens may collapse to
a prior distribution which does not depend on any informative visual tokens at
all. Thus, we propose to stop further text generation to avoid hallucination.
Extensive experiments on three benchmarks including POPE, CHAIR, and MME
demonstrate that our method can consistently mitigate the hallucination issue
of LVLM and performs favorably against previous state-of-the-arts.

</details>


### [782] [Beyond Domain Randomization: Event-Inspired Perception for Visually Robust Adversarial Imitation from Videos](https://arxiv.org/abs/2505.18899)
*Andrea Ramazzina,Vittorio Giammarino,Matteo El-Hariry,Mario Bijelic*

Main category: cs.CV

TL;DR: 该论文提出了一种基于事件感知的视觉模仿方法，通过将RGB视频转换为稀疏的事件表示，消除外观特征的影响，从而在专家与学习者环境存在视觉差异时实现鲁棒模仿。


<details>
  <summary>Details</summary>
Motivation: 传统视觉模仿在专家演示与学习者环境存在领域偏移（如光照、颜色或纹理差异）时容易失败。虽然视觉随机化通过数据增强部分解决了这一问题，但其计算成本高且难以应对未见过的场景。

Method: 论文提出了一种事件启发的感知方法，将标准RGB视频转换为稀疏的事件表示，仅编码时间强度梯度并丢弃静态外观特征。这种方法从生物视觉系统（如视网膜神经节细胞）中汲取灵感，解耦运动动态与视觉风格。

Result: 通过在DeepMind Control Suite和Adroit动态灵巧操作平台上的实验，该方法展示了其在视觉不匹配情况下的鲁棒性，且无需计算昂贵或环境特定的数据增强技术。

Conclusion: 基于事件流的策略训练能够实现对外观干扰的不变性，为视觉模仿提供了一种高效且鲁棒的解决方案。

Abstract: Imitation from videos often fails when expert demonstrations and learner
environments exhibit domain shifts, such as discrepancies in lighting, color,
or texture. While visual randomization partially addresses this problem by
augmenting training data, it remains computationally intensive and inherently
reactive, struggling with unseen scenarios. We propose a different approach:
instead of randomizing appearances, we eliminate their influence entirely by
rethinking the sensory representation itself. Inspired by biological vision
systems that prioritize temporal transients (e.g., retinal ganglion cells) and
by recent sensor advancements, we introduce event-inspired perception for
visually robust imitation. Our method converts standard RGB videos into a
sparse, event-based representation that encodes temporal intensity gradients,
discarding static appearance features. This biologically grounded approach
disentangles motion dynamics from visual style, enabling robust visual
imitation from observations even in the presence of visual mismatches between
expert and agent environments. By training policies on event streams, we
achieve invariance to appearance-based distractors without requiring
computationally expensive and environment-specific data augmentation
techniques. Experiments across the DeepMind Control Suite and the Adroit
platform for dynamic dexterous manipulation show the efficacy of our method.
Our code is publicly available at Eb-LAIfO.

</details>


### [783] [Align and Surpass Human Camouflaged Perception: Visual Refocus Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.19611)
*Ruolin Shen,Xiaozhong Ji,Kai WU,Jiangning Zhang,Yijun He,HaiHua Yang,Xiaobin Hu,Xiaoyu Sun*

Main category: cs.CV

TL;DR: 当前多模态模型在处理与背景视觉融合的物体时与人类视觉系统存在显著差异。本文提出了一种视觉重新聚焦强化框架，通过策略优化算法使模型在回答前进行更多思考和重新聚焦，从而在伪装物体分类和检测任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究发现多模态模型无法区分被隐藏的物体，与人类利用前景-背景相似性原则进行视觉分析的认知过程不符。为了分析这种隐藏的人类-模型视觉思维差异，作者构建了一个模拟人类视觉伪装感知的系统。

Method: 提出了一种视觉重新聚焦强化框架，通过策略优化算法逐步引导模型逻辑定位视觉图像中的物体。定位过程需要分层注意力转移和动态调整先验认知知识。

Result: 实验成功展示了重新聚焦视觉现象的出现，表现为多个推理标记和检测框的动态调整。在伪装物体分类和检测任务上，性能显著优于监督微调基线。

Conclusion: 该框架使多模态模型能够更好地模拟人类视觉伪装感知系统，甚至在某些情况下超越人类表现。

Abstract: Current multi-modal models exhibit a notable misalignment with the human
visual system when identifying objects that are visually assimilated into the
background. Our observations reveal that these multi-modal models cannot
distinguish concealed objects, demonstrating an inability to emulate human
cognitive processes which effectively utilize foreground-background similarity
principles for visual analysis. To analyze this hidden human-model visual
thinking discrepancy, we build a visual system that mimicks human visual
camouflaged perception to progressively and iteratively `refocus' visual
concealed content. The refocus is a progressive guidance mechanism enabling
models to logically localize objects in visual images through stepwise
reasoning. The localization process of concealed objects requires hierarchical
attention shifting with dynamic adjustment and refinement of prior cognitive
knowledge. In this paper, we propose a visual refocus reinforcement framework
via the policy optimization algorithm to encourage multi-modal models to think
and refocus more before answering, and achieve excellent reasoning abilities to
align and even surpass human camouflaged perception systems. Our extensive
experiments on camouflaged perception successfully demonstrate the emergence of
refocus visual phenomena, characterized by multiple reasoning tokens and
dynamic adjustment of the detection box. Besides, experimental results on both
camouflaged object classification and detection tasks exhibit significantly
superior performance compared to Supervised Fine-Tuning (SFT) baselines.

</details>


### [784] [Exploring Magnitude Preservation and Rotation Modulation in Diffusion Transformers](https://arxiv.org/abs/2505.19122)
*Eric Tillman Bill,Cristian Perez Jensen,Sotiris Anagnostidis,Dimitri von Rütte*

Main category: cs.CV

TL;DR: 该论文研究了在扩散变换器（DiT）架构中保持幅度对训练稳定性的影响，提出了一种无需归一化层的幅度保持设计，并引入旋转调制作为新的条件方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 去噪扩散模型具有强大的生成能力，但由于其固有的随机性，训练过程中梯度估计的高方差导致收敛缓慢。此前的研究表明，在U-net架构中保持幅度有助于稳定训练。本文旨在探索这一效应是否适用于扩散变换器（DiT）架构。

Method: 论文提出了一种幅度保持设计，无需归一化层即可稳定训练。此外，为了实现激活幅度的保持，还引入了旋转调制——一种使用学习旋转而非传统缩放或平移的新条件方法。

Result: 通过小规模模型的实证评估和消融研究，幅度保持策略显著提升了性能，FID分数降低了约12.8%。旋转调制与缩放结合的效果与AdaLN相当，同时减少了约5.4%的参数。

Conclusion: 这项工作为条件策略和幅度控制提供了新的见解，并公开了方法的实现代码。

Abstract: Denoising diffusion models exhibit remarkable generative capabilities, but
remain challenging to train due to their inherent stochasticity, where
high-variance gradient estimates lead to slow convergence. Previous works have
shown that magnitude preservation helps with stabilizing training in the U-net
architecture. This work explores whether this effect extends to the Diffusion
Transformer (DiT) architecture. As such, we propose a magnitude-preserving
design that stabilizes training without normalization layers. Motivated by the
goal of maintaining activation magnitudes, we additionally introduce rotation
modulation, which is a novel conditioning method using learned rotations
instead of traditional scaling or shifting. Through empirical evaluations and
ablation studies on small-scale models, we show that magnitude-preserving
strategies significantly improve performance, notably reducing FID scores by
$\sim$12.8%. Further, we show that rotation modulation combined with scaling is
competitive with AdaLN, while requiring $\sim$5.4% fewer parameters. This work
provides insights into conditioning strategies and magnitude control. We will
publicly release the implementation of our method.

</details>


### [785] [Benchmarking Large Multimodal Models for Ophthalmic Visual Question Answering with OphthalWeChat](https://arxiv.org/abs/2505.19624)
*Pusheng Xu,Xia Gong,Xiaolan Chen,Weiyi Zhang,Jiancheng Yang,Bingjie Yan,Meng Yuan,Yalin Zheng,Mingguang He,Danli Shi*

Main category: cs.CV

TL;DR: 该研究开发了一个双语多模态眼科视觉问答基准(OphthalWeChat)，用于评估视觉语言模型(VLMs)在眼科领域的表现。Gemini 2.0 Flash在总体准确率上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 开发一个双语多模态视觉问答基准，以评估视觉语言模型在眼科领域的性能，支持开发准确、专业且可信赖的眼科AI系统。

Method: 从微信公众号收集眼科图像及标题，使用GPT-4o-mini生成中英文问答对，分为六类问题类型，并评估三种VLMs模型。

Result: OphthalWeChat数据集包含3,469张图像和30,120个问答对。Gemini 2.0 Flash总体准确率最高(0.548)，在不同子集表现各异。

Conclusion: 该研究提出了首个眼科双语VQA基准，具有真实临床场景和多模态特点，为眼科AI系统开发提供了量化评估工具。

Abstract: Purpose: To develop a bilingual multimodal visual question answering (VQA)
benchmark for evaluating VLMs in ophthalmology. Methods: Ophthalmic image posts
and associated captions published between January 1, 2016, and December 31,
2024, were collected from WeChat Official Accounts. Based on these captions,
bilingual question-answer (QA) pairs in Chinese and English were generated
using GPT-4o-mini. QA pairs were categorized into six subsets by question type
and language: binary (Binary_CN, Binary_EN), single-choice (Single-choice_CN,
Single-choice_EN), and open-ended (Open-ended_CN, Open-ended_EN). The benchmark
was used to evaluate the performance of three VLMs: GPT-4o, Gemini 2.0 Flash,
and Qwen2.5-VL-72B-Instruct. Results: The final OphthalWeChat dataset included
3,469 images and 30,120 QA pairs across 9 ophthalmic subspecialties, 548
conditions, 29 imaging modalities, and 68 modality combinations. Gemini 2.0
Flash achieved the highest overall accuracy (0.548), outperforming GPT-4o
(0.522, P < 0.001) and Qwen2.5-VL-72B-Instruct (0.514, P < 0.001). It also led
in both Chinese (0.546) and English subsets (0.550). Subset-specific
performance showed Gemini 2.0 Flash excelled in Binary_CN (0.687),
Single-choice_CN (0.666), and Single-choice_EN (0.646), while GPT-4o ranked
highest in Binary_EN (0.717), Open-ended_CN (BLEU-1: 0.301; BERTScore: 0.382),
and Open-ended_EN (BLEU-1: 0.183; BERTScore: 0.240). Conclusions: This study
presents the first bilingual VQA benchmark for ophthalmology, distinguished by
its real-world context and inclusion of multiple examinations per patient. The
dataset reflects authentic clinical decision-making scenarios and enables
quantitative evaluation of VLMs, supporting the development of accurate,
specialized, and trustworthy AI systems for eye care.

</details>


### [786] [JEDI: The Force of Jensen-Shannon Divergence in Disentangling Diffusion Models](https://arxiv.org/abs/2505.19166)
*Eric Tillmann Bill,Enis Simsar,Thomas Hofmann*

Main category: cs.CV

TL;DR: JEDI是一种无需重新训练或外部监督的测试时适应方法，通过最小化注意力图中的语义纠缠提升扩散模型的主题分离和组合对齐。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在复杂场景中常面临主题分离和组合对齐的挑战，现有方法通常需要重新训练或外部监督，效率较低。

Method: JEDI利用基于Jensen-Shannon散度的目标函数减少语义纠缠，并通过对抗优化提高效率，适用于多种扩散模型架构。

Result: JEDI在Stable Diffusion等模型上显著提升了提示对齐和解缠效果，并提供了一种轻量级的解缠评分基准。

Conclusion: JEDI是一种高效、通用的测试时适应方法，能够显著提升扩散模型的组合对齐能力，且无需额外训练或监督。

Abstract: We introduce JEDI, a test-time adaptation method that enhances subject
separation and compositional alignment in diffusion models without requiring
retraining or external supervision. JEDI operates by minimizing semantic
entanglement in attention maps using a novel Jensen-Shannon divergence based
objective. To improve efficiency, we leverage adversarial optimization,
reducing the number of updating steps required.
  JEDI is model-agnostic and applicable to architectures such as Stable
Diffusion 1.5 and 3.5, consistently improving prompt alignment and
disentanglement in complex scenes. Additionally, JEDI provides a lightweight,
CLIP-free disentanglement score derived from internal attention distributions,
offering a principled benchmark for compositional alignment under test-time
conditions. We will publicly release the implementation of our method.

</details>


### [787] [BAH Dataset for Ambivalence/Hesitancy Recognition in Videos for Behavioural Change](https://arxiv.org/abs/2505.19328)
*Manuela González-González,Soufiane Belharbi,Muhammad Osama Zeeshan,Masoumeh Sharafi,Muhammad Haseeb Aslam,Marco Pedersoli,Alessandro Lameiras Koerich,Simon L Bacon,Eric Granger*

Main category: cs.CV

TL;DR: 该论文介绍了首个用于识别矛盾/犹豫情绪的多模态视频数据集BAH，包含224名参与者的1118段视频，并提供了基线模型性能分析。


<details>
  <summary>Details</summary>
Motivation: 识别矛盾/犹豫情绪对个性化数字行为干预至关重要，但现有数据缺失且人工标注成本高，需开发自动化解决方案。

Method: 通过网页平台收集加拿大9省224名参与者回答7个问题时的视频（含音频），由行为专家标注矛盾/犹豫片段，并提供多模态数据（面部、文本、元数据等）。

Result: 基线模型在帧级和视频级多模态识别、零样本预测及无监督域适应任务中表现有限，凸显真实场景下识别矛盾/犹豫的挑战。

Conclusion: BAH数据集填补了矛盾/犹豫情绪识别的研究空白，公开的数据和代码为后续研究提供了基础，但需改进模型性能。

Abstract: Recognizing complex emotions linked to ambivalence and hesitancy (A/H) can
play a critical role in the personalization and effectiveness of digital
behaviour change interventions. These subtle and conflicting emotions are
manifested by a discord between multiple modalities, such as facial and vocal
expressions, and body language. Although experts can be trained to identify
A/H, integrating them into digital interventions is costly and less effective.
Automatic learning systems provide a cost-effective alternative that can adapt
to individual users, and operate seamlessly within real-time, and
resource-limited environments. However, there are currently no datasets
available for the design of ML models to recognize A/H. This paper introduces a
first Behavioural Ambivalence/Hesitancy (BAH) dataset collected for
subject-based multimodal recognition of A/H in videos. It contains videos from
224 participants captured across 9 provinces in Canada, with different age, and
ethnicity. Through our web platform, we recruited participants to answer 7
questions, some of which were designed to elicit A/H while recording themselves
via webcam with microphone. BAH amounts to 1,118 videos for a total duration of
8.26 hours with 1.5 hours of A/H. Our behavioural team annotated timestamp
segments to indicate where A/H occurs, and provide frame- and video-level
annotations with the A/H cues. Video transcripts and their timestamps are also
included, along with cropped and aligned faces in each frame, and a variety of
participants meta-data. We include results baselines for BAH at frame- and
video-level recognition in multi-modal setups, in addition to zero-shot
prediction, and for personalization using unsupervised domain adaptation. The
limited performance of baseline models highlights the challenges of recognizing
A/H in real-world videos. The data, code, and pretrained weights are available.

</details>


### [788] [Structure Disruption: Subverting Malicious Diffusion-Based Inpainting via Self-Attention Query Perturbation](https://arxiv.org/abs/2505.19425)
*Yuhao He,Jinyu Tian,Haiwei Wu,Jianqing Li*

Main category: cs.CV

TL;DR: 论文提出Structure Disruption Attack (SDA)框架，通过干扰扩散模型的自注意力机制来保护敏感图像区域免受基于修复的编辑。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型的快速发展，其图像修复和编辑能力增强，但也带来了社会风险。攻击者可能利用社交媒体上的用户图像生成误导性或有害内容。现有的全局扰动方法在掩码引导的编辑任务中效果不佳，因此需要一种更有效的保护方法。

Method: SDA通过优化扰动，在初始去噪步骤中干扰自注意力机制中的查询，破坏轮廓生成过程，从而直接干扰扩散模型的结构生成能力。

Result: 实验证明，SDA在公开数据集上实现了最先进的保护性能，并保持了强大的鲁棒性。

Conclusion: SDA是一种有效的保护框架，能够防止扩散模型生成连贯的图像，从而保护敏感图像区域。

Abstract: The rapid advancement of diffusion models has enhanced their image inpainting
and editing capabilities but also introduced significant societal risks.
Adversaries can exploit user images from social media to generate misleading or
harmful content. While adversarial perturbations can disrupt inpainting, global
perturbation-based methods fail in mask-guided editing tasks due to spatial
constraints. To address these challenges, we propose Structure Disruption
Attack (SDA), a powerful protection framework for safeguarding sensitive image
regions against inpainting-based editing. Building upon the contour-focused
nature of self-attention mechanisms of diffusion models, SDA optimizes
perturbations by disrupting queries in self-attention during the initial
denoising step to destroy the contour generation process. This targeted
interference directly disrupts the structural generation capability of
diffusion models, effectively preventing them from producing coherent images.
We validate our motivation through visualization techniques and extensive
experiments on public datasets, demonstrating that SDA achieves
state-of-the-art (SOTA) protection performance while maintaining strong
robustness.

</details>


### [789] [The Missing Point in Vision Transformers for Universal Image Segmentation](https://arxiv.org/abs/2505.19795)
*Sajjad Shahabodini,Mobina Mansoori,Farnoush Bayatmakou,Jamshid Abouei,Konstantinos N. Plataniotis,Arash Mohammadi*

Main category: cs.CV

TL;DR: ViT-P是一个新颖的两阶段分割框架，通过解耦掩码生成和分类，利用Vision Transformer提升分类精度，并在多个数据集上取得先进成果。


<details>
  <summary>Details</summary>
Motivation: 当前基于掩码的分割方法在全局上下文捕捉上表现良好，但在模糊边界和类别不平衡情况下的精确分类仍具挑战性。

Method: ViT-P采用两阶段方法：首先生成类别无关的掩码建议，然后通过基于点的Vision Transformer分类模型优化预测。

Result: 在COCO、ADE20K和Cityscapes数据集上，ViT-P实现了54.0 PQ、87.4 mIoU和63.6 mIoU的先进性能。

Conclusion: ViT-P无需预训练即可适配多种视觉Transformer，且粗标注和边界框标注能有效提升分类性能，降低标注成本。

Abstract: Image segmentation remains a challenging task in computer vision, demanding
robust mask generation and precise classification. Recent mask-based approaches
yield high-quality masks by capturing global context. However, accurately
classifying these masks, especially in the presence of ambiguous boundaries and
imbalanced class distributions, remains an open challenge. In this work, we
introduce ViT-P, a novel two-stage segmentation framework that decouples mask
generation from classification. The first stage employs a proposal generator to
produce class-agnostic mask proposals, while the second stage utilizes a
point-based classification model built on the Vision Transformer (ViT) to
refine predictions by focusing on mask central points. ViT-P serves as a
pre-training-free adapter, allowing the integration of various pre-trained
vision transformers without modifying their architecture, ensuring adaptability
to dense prediction tasks. Furthermore, we demonstrate that coarse and bounding
box annotations can effectively enhance classification without requiring
additional training on fine annotation datasets, reducing annotation costs
while maintaining strong performance. Extensive experiments across COCO,
ADE20K, and Cityscapes datasets validate the effectiveness of ViT-P, achieving
state-of-the-art results with 54.0 PQ on ADE20K panoptic segmentation, 87.4
mIoU on Cityscapes semantic segmentation, and 63.6 mIoU on ADE20K semantic
segmentation. The code and pretrained models are available at:
https://github.com/sajjad-sh33/ViT-P}{https://github.com/sajjad-sh33/ViT-P.

</details>


### [790] [Revolutionizing Wildfire Detection with Convolutional Neural Networks: A VGG16 Model Approach](https://arxiv.org/abs/2505.19479)
*Lakshmi Aishwarya Malladi,Navarun Gupta,Ahmed El-Sayed,Xingguo Xiong*

Main category: cs.CV

TL;DR: 该研究利用基于VGG16架构的CNN提升野火检测精度，通过数据增强和模型优化解决了低分辨率图像和数据集不平衡问题，展示了深度学习在早期野火识别中的潜力。


<details>
  <summary>Details</summary>
Motivation: 野火频发且破坏性加剧，亟需高效预警系统以减少灾难性后果。

Method: 采用VGG16架构的CNN模型，使用D-FIRE数据集，并通过数据增强和模型优化解决低分辨率图像和数据集不平衡问题。

Result: 模型实现了较低假阴性率，能有效减少未检测到的火灾，为快速响应提供可靠自动化方案。

Conclusion: VGG16等深度学习模型可提供可靠的早期野火识别方法，未来工作将聚焦实时监测系统集成和数据集扩展。

Abstract: Over 8,024 wildfire incidents have been documented in 2024 alone, affecting
thousands of fatalities and significant damage to infrastructure and
ecosystems. Wildfires in the United States have inflicted devastating losses.
Wildfires are becoming more frequent and intense, which highlights how urgently
efficient warning systems are needed to avoid disastrous outcomes. The goal of
this study is to enhance the accuracy of wildfire detection by using
Convolutional Neural Network (CNN) built on the VGG16 architecture. The D-FIRE
dataset, which includes several kinds of wildfire and non-wildfire images, was
employed in the study. Low-resolution images, dataset imbalance, and the
necessity for real-time applicability are some of the main challenges. These
problems were resolved by enriching the dataset using data augmentation
techniques and optimizing the VGG16 model for binary classification. The model
produced a low false negative rate, which is essential for reducing unexplored
fires, despite dataset boundaries. In order to help authorities execute fast
responses, this work shows that deep learning models such as VGG16 can offer a
reliable, automated approach for early wildfire recognition. For the purpose of
reducing the impact of wildfires, our future work will concentrate on
connecting to systems with real-time surveillance networks and enlarging the
dataset to cover more varied fire situations.

</details>


### [791] [Multimodal Machine Translation with Visual Scene Graph Pruning](https://arxiv.org/abs/2505.19507)
*Chenyu Lu,Shiliang Sun,Jing Zhao,Nan Zhang,Tengfei Song,Hao Yang*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉场景图剪枝（PSG）的多模态机器翻译方法，通过语言场景图信息指导剪枝，减少冗余视觉信息，提升翻译效果。


<details>
  <summary>Details</summary>
Motivation: 当前多模态机器翻译（MMT）研究中，视觉信息的有效利用是关键瓶颈。现有方法未能充分解决视觉信息冗余问题，亟需新方法优化。

Method: 提出PSG模型，利用语言场景图信息剪枝视觉场景图中的冗余节点，从而降低下游翻译任务中的噪声。

Result: 通过与先进方法的对比实验及消融研究，验证了PSG模型的有效性，并展示了视觉信息剪枝在MMT领域的潜力。

Conclusion: PSG模型通过剪枝冗余视觉信息显著提升了多模态机器翻译性能，为未来研究提供了新方向。

Abstract: Multimodal machine translation (MMT) seeks to address the challenges posed by
linguistic polysemy and ambiguity in translation tasks by incorporating visual
information. A key bottleneck in current MMT research is the effective
utilization of visual data. Previous approaches have focused on extracting
global or region-level image features and using attention or gating mechanisms
for multimodal information fusion. However, these methods have not adequately
tackled the issue of visual information redundancy in MMT, nor have they
proposed effective solutions. In this paper, we introduce a novel
approach--multimodal machine translation with visual Scene Graph Pruning (PSG),
which leverages language scene graph information to guide the pruning of
redundant nodes in visual scene graphs, thereby reducing noise in downstream
translation tasks. Through extensive comparative experiments with
state-of-the-art methods and ablation studies, we demonstrate the effectiveness
of the PSG model. Our results also highlight the promising potential of visual
information pruning in advancing the field of MMT.

</details>


### [792] [Applications and Effect Evaluation of Generative Adversarial Networks in Semi-Supervised Learning](https://arxiv.org/abs/2505.19522)
*Jiyu Hu,Haijiang Zeng,Zhen Tian*

Main category: cs.CV

TL;DR: 本文提出了一种基于生成对抗网络（GANs）的半监督图像分类模型，通过生成器、判别器和分类器的协同训练机制，有效利用有限标记数据和大量未标记数据，提高了图像生成质量和分类准确率。


<details>
  <summary>Details</summary>
Motivation: 由于高质量标记数据的稀缺性限制了深度学习模型在实际场景中的广泛应用，半监督学习成为缓解标记样本不足问题的研究热点。

Method: 构建基于GANs的半监督图像分类模型，引入生成器、判别器和分类器的协同训练机制。

Result: 模型实现了有限标记数据和大量未标记数据的有效利用，提升了图像生成质量和分类准确率。

Conclusion: 该研究为复杂环境下的图像识别任务提供了有效解决方案。

Abstract: In recent years, image classification, as a core task in computer vision,
relies on high-quality labelled data, which restricts the wide application of
deep learning models in practical scenarios. To alleviate the problem of
insufficient labelled samples, semi-supervised learning has gradually become a
research hotspot. In this paper, we construct a semi-supervised image
classification model based on Generative Adversarial Networks (GANs), and
through the introduction of the collaborative training mechanism of generators,
discriminators and classifiers, we achieve the effective use of limited
labelled data and a large amount of unlabelled data, improve the quality of
image generation and classification accuracy, and provide an effective solution
for the task of image recognition in complex environments.

</details>


### [793] [Two Causally Related Needles in a Video Haystack](https://arxiv.org/abs/2505.19853)
*Miaoyu Li,Qin Chao,Boyang Li*

Main category: cs.CV

TL;DR: 提出Causal2Needles基准测试，评估视频语言模型在长视频中联合理解因果事件的能力，发现现有模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试未能充分评估视频语言模型在长视频中联合理解分散信息及因果推理的能力。

Method: 设计包含因果行为事件的2-needle问题，结合视频片段定位与文本描述任务以避免文本偏差。

Result: 现有优秀模型在2-needle视觉定位任务中表现差，且性能与事件间距呈负相关。

Conclusion: 当前视频语言模型在长视频因果推理和跨片段联合理解上存在明显局限。

Abstract: Evaluating the video understanding capabilities of Video-Language Models
(VLMs) remains a significant challenge. We propose a long-context video
understanding benchmark, Causal2Needles, that assesses two crucial abilities
insufficiently evaluated by existing benchmarks: (1) the ability to extract
information from two separate locations in a long video and understand them
jointly, and (2) the ability to model the world in terms of cause and effect in
human behaviors. Specifically, Causal2Needles introduces 2-needle questions,
which require extracting information from both the cause and effect
human-behavior events in a long video and the associated narration text. To
prevent textual bias, these questions comprise two complementary formats: one
asking to identify the video clip containing the answer, and one asking for the
textual description of an unrelated visual detail from that video clip. Our
experiments reveal that models excelling in pre-existing benchmarks struggle
with 2-needle visual grounding, and the model performance is negatively
correlated with the distance between the two needles. These findings highlight
critical limitations in current VLMs.

</details>


### [794] [StyleAR: Customizing Multimodal Autoregressive Model for Style-Aligned Text-to-Image Generation](https://arxiv.org/abs/2505.19874)
*Yi Wu,Lingting Zhu,Shengju Qian,Lei Liu,Wandi Qiao,Lequan Yu,Bin Li*

Main category: cs.CV

TL;DR: 论文提出StyleAR方法，通过创新的数据整理和自回归模型，解决风格对齐文本到图像生成中的数据获取难题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态自回归模型在视觉理解和生成方面表现优异，但风格对齐的文本到图像生成任务面临数据获取困难。传统文本到图像数据难以满足特定风格需求，因此需要新方法。

Method: StyleAR结合定制数据整理方法和自回归模型，利用参考风格图像和提示合成目标风格数据，引入CLIP图像编码器和风格增强标记技术防止内容泄漏，并混合原始图像增强风格一致性。

Result: 大量定性和定量实验证明，StyleAR在风格对齐文本到图像生成任务中表现优异。

Conclusion: StyleAR通过创新数据整理和模型设计，有效解决了风格对齐生成中的数据难题，提升了生成质量和风格一致性。

Abstract: In the current research landscape, multimodal autoregressive (AR) models have
shown exceptional capabilities across various domains, including visual
understanding and generation. However, complex tasks such as style-aligned
text-to-image generation present significant challenges, particularly in data
acquisition. In analogy to instruction-following tuning for image editing of AR
models, style-aligned generation requires a reference style image and prompt,
resulting in a text-image-to-image triplet where the output shares the style
and semantics of the input. However, acquiring large volumes of such triplet
data with specific styles is considerably more challenging than obtaining
conventional text-to-image data used for training generative models. To address
this issue, we propose StyleAR, an innovative approach that combines a
specially designed data curation method with our proposed AR models to
effectively utilize text-to-image binary data for style-aligned text-to-image
generation. Our method synthesizes target stylized data using a reference style
image and prompt, but only incorporates the target stylized image as the image
modality to create high-quality binary data. To facilitate binary data
training, we introduce a CLIP image encoder with a perceiver resampler that
translates the image input into style tokens aligned with multimodal tokens in
AR models and implement a style-enhanced token technique to prevent content
leakage which is a common issue in previous work. Furthermore, we mix raw
images drawn from large-scale text-image datasets with stylized images to
enhance StyleAR's ability to extract richer stylistic features and ensure style
consistency. Extensive qualitative and quantitative experiments demonstrate our
superior performance.

</details>


### [795] [Rep3D: Re-parameterize Large 3D Kernels with Low-Rank Receptive Modeling for Medical Imaging](https://arxiv.org/abs/2505.19603)
*Ho Hin Lee,Quan Liu,Shunxing Bao,Yuankai Huo,Bennett A. Landman*

Main category: cs.CV

TL;DR: Rep3D提出了一种结合空间先验与优化感知学习的大核3D卷积框架，在3D医学图像分割任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统大核卷积在高分辨率3D体数据中存在优化不稳定和性能下降问题，作者观察到有效感受野的空间偏差现象，提出不同卷积核元素应以不同速率收敛。

Method: 提出Rep3D框架：1) 理论证明结构重参数化卷积会引发空间变化的学习率；2) 设计两阶段调制网络生成感受野偏置的缩放掩码；3) 采用纯大核深度卷积编码器。

Result: 在5个3D分割基准测试中均超越当前最佳方法（包括Transformer和固定先验方法），代码已开源。

Conclusion: 通过统一空间归纳偏置与优化感知学习，Rep3D为3D医学分析提供了可解释、可扩展的解决方案。

Abstract: In contrast to vision transformers, which model long-range dependencies
through global self-attention, large kernel convolutions provide a more
efficient and scalable alternative, particularly in high-resolution 3D
volumetric settings. However, naively increasing kernel size often leads to
optimization instability and degradation in performance. Motivated by the
spatial bias observed in effective receptive fields (ERFs), we hypothesize that
different kernel elements converge at variable rates during training. To
support this, we derive a theoretical connection between element-wise gradients
and first-order optimization, showing that structurally re-parameterized
convolution blocks inherently induce spatially varying learning rates. Building
on this insight, we introduce Rep3D, a 3D convolutional framework that
incorporates a learnable spatial prior into large kernel training. A
lightweight two-stage modulation network generates a receptive-biased scaling
mask, adaptively re-weighting kernel updates and enabling local-to-global
convergence behavior. Rep3D adopts a plain encoder design with large depthwise
convolutions, avoiding the architectural complexity of multi-branch
compositions. We evaluate Rep3D on five challenging 3D segmentation benchmarks
and demonstrate consistent improvements over state-of-the-art baselines,
including transformer-based and fixed-prior re-parameterization methods. By
unifying spatial inductive bias with optimization-aware learning, Rep3D offers
an interpretable, and scalable solution for 3D medical image analysis. The
source code is publicly available at https://github.com/leeh43/Rep3D.

</details>


### [796] [A Responsible Face Recognition Approach for Small and Mid-Scale Systems Through Personalized Neural Networks](https://arxiv.org/abs/2505.19920)
*Sebastian Groß,Stefan Heindorf,Philipp Terhörst*

Main category: cs.CV

TL;DR: 本文提出了一种新型的MOTE方法，用小型个性化神经网络替代传统基于向量的人脸模板，以提高人脸识别系统的公平性和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 传统人脸识别系统使用固定的人脸模板，缺乏可解释性，且存在公平性和隐私问题。MOTE旨在解决这些问题，特别适用于中小规模系统。

Method: MOTE为每个身份创建一个专用的二元分类器，仅使用一个参考样本和合成平衡样本进行训练，从而在注册阶段调整公平性。

Result: 在多个数据集和识别系统上的实验表明，MOTE在公平性和隐私保护方面有显著提升，尽管增加了推理时间和存储需求。

Conclusion: MOTE为中小规模应用中注重公平性和隐私保护的人脸识别提供了有效的解决方案。

Abstract: Traditional face recognition systems rely on extracting fixed face
representations, known as templates, to store and verify identities. These
representations are typically generated by neural networks that often lack
explainability and raise concerns regarding fairness and privacy. In this work,
we propose a novel model-template (MOTE) approach that replaces vector-based
face templates with small personalized neural networks. This design enables
more responsible face recognition for small and medium-scale systems. During
enrollment, MOTE creates a dedicated binary classifier for each identity,
trained to determine whether an input face matches the enrolled identity. Each
classifier is trained using only a single reference sample, along with
synthetically balanced samples to allow adjusting fairness at the level of a
single individual during enrollment. Extensive experiments across multiple
datasets and recognition systems demonstrate substantial improvements in
fairness and particularly in privacy. Although the method increases inference
time and storage requirements, it presents a strong solution for small- and
mid-scale applications where fairness and privacy are critical.

</details>


### [797] [SaSi: A Self-augmented and Self-interpreted Deep Learning Approach for Few-shot Cryo-ET Particle Detection](https://arxiv.org/abs/2505.19948)
*Gokul Adethya,Bhanu Pratyush Mantha,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: 提出了一种新型自增强自解释（SaSi）深度学习方法，用于解决冷冻电镜断层扫描（cryo-ET）中少量标记数据下的3D粒子检测问题。


<details>
  <summary>Details</summary>
Motivation: 冷冻电镜断层扫描（cryo-ET）在近天然状态下成像大分子复合物方面表现出强大能力，但在细胞环境中定位3D粒子仍面临低信噪比和缺失楔形伪影的挑战。现有深度学习方法需要大量数据，而cryo-ET中标记数据通常稀缺。

Method: 提出自增强（self-augmentation）技术以提高数据利用率，并引入自解释分割策略（self-interpreted segmentation）减少对标记数据的依赖，从而提升泛化能力和鲁棒性。

Result: 在模拟和真实cryo-ET数据集上的实验表明，SaSi方法在粒子定位任务上显著优于现有最先进方法。

Conclusion: 该研究为cryo-ET中少量标记数据下的粒子检测提供了新思路，并为结构生物学中的少样本学习设立了新基准。

Abstract: Cryo-electron tomography (cryo-ET) has emerged as a powerful technique for
imaging macromolecular complexes in their near-native states. However, the
localization of 3D particles in cellular environments still presents a
significant challenge due to low signal-to-noise ratios and missing wedge
artifacts. Deep learning approaches have shown great potential, but they need
huge amounts of data, which can be a challenge in cryo-ET scenarios where
labeled data is often scarce. In this paper, we propose a novel Self-augmented
and Self-interpreted (SaSi) deep learning approach towards few-shot particle
detection in 3D cryo-ET images. Our method builds upon self-augmentation
techniques to further boost data utilization and introduces a self-interpreted
segmentation strategy for alleviating dependency on labeled data, hence
improving generalization and robustness. As demonstrated by experiments
conducted on both simulated and real-world cryo-ET datasets, the SaSi approach
significantly outperforms existing state-of-the-art methods for particle
localization. This research increases understanding of how to detect particles
with very few labels in cryo-ET and thus sets a new benchmark for few-shot
learning in structural biology.

</details>


### [798] [Decomposing Complex Visual Comprehension into Atomic Visual Skills for Vision Language Models](https://arxiv.org/abs/2505.20021)
*Hyunsik Chae,Seungwoo Yoon,Jaden Park,Chloe Yewon Chun,Yongin Cho,Mu Cai,Yong Jae Lee,Ernest K. Ryu*

Main category: cs.CV

TL;DR: 当前视觉语言模型在复杂任务上表现优异，但在基础2D几何视觉任务上表现不佳，研究提出原子视觉技能数据集AVSD进行评估。


<details>
  <summary>Details</summary>
Motivation: 尽管现有视觉语言模型在多模态理解和推理方面表现突出，但在简单视觉任务上表现不佳，尤其是基础2D几何任务。

Method: 研究系统化分类了基础2D几何中的原子视觉技能，并构建了AVSD数据集用于评估视觉语言模型。

Result: 实验发现，即使对人类来说非常简单的原子视觉任务，当前最先进的视觉语言模型仍表现不佳。

Conclusion: 研究强调需要专门构建的数据集来训练和评估视觉语言模型在原子视觉任务上的能力。

Abstract: Recent Vision-Language Models (VLMs) have demonstrated impressive multimodal
comprehension and reasoning capabilities, yet they often struggle with
trivially simple visual tasks. In this work, we focus on the domain of basic 2D
Euclidean geometry and systematically categorize the fundamental, indivisible
visual perception skills, which we refer to as atomic visual skills. We then
introduce the Atomic Visual Skills Dataset (AVSD) for evaluating VLMs on the
atomic visual skills. Using AVSD, we benchmark state-of-the-art VLMs and find
that they struggle with these tasks, despite being trivial for adult humans.
Our findings highlight the need for purpose-built datasets to train and
evaluate VLMs on atomic, rather than composite, visual perception tasks.

</details>


### [799] [ReasonPlan: Unified Scene Prediction and Decision Reasoning for Closed-loop Autonomous Driving](https://arxiv.org/abs/2505.20024)
*Xueyi Liu,Zuodong Zhong,Yuxin Guo,Yun-Fu Liu,Zhiguo Su,Qichao Zhang,Junli Wang,Yinfeng Gao,Yupeng Zheng,Qiao Lin,Huiyong Chen,Dongbin Zhao*

Main category: cs.CV

TL;DR: 提出ReasonPlan框架，通过自监督下一场景预测和监督决策链思维微调MLLM，显著提升闭环驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM在闭环自动驾驶中应用不足，且性能未超越主流模仿学习方法，需改进视觉表征与驾驶决策的关联性。

Method: 结合自监督Next Scene Prediction任务和监督Decision Chain-of-Thought过程，构建PDR数据集（21万样本）进行微调。

Result: 在Bench2Drive基准上L2误差降低19%，驾驶分数提升16.1%；在DOS基准上展现零样本泛化能力。

Conclusion: ReasonPlan框架通过双重机制实现可解释决策，显著优于传统方法，代码数据集已开源。

Abstract: Due to the powerful vision-language reasoning and generalization abilities,
multimodal large language models (MLLMs) have garnered significant attention in
the field of end-to-end (E2E) autonomous driving. However, their application to
closed-loop systems remains underexplored, and current MLLM-based methods have
not shown clear superiority to mainstream E2E imitation learning approaches. In
this work, we propose ReasonPlan, a novel MLLM fine-tuning framework designed
for closed-loop driving through holistic reasoning with a self-supervised Next
Scene Prediction task and supervised Decision Chain-of-Thought process. This
dual mechanism encourages the model to align visual representations with
actionable driving context, while promoting interpretable and causally grounded
decision making. We curate a planning-oriented decision reasoning dataset,
namely PDR, comprising 210k diverse and high-quality samples. Our method
outperforms the mainstream E2E imitation learning method by a large margin of
19% L2 and 16.1 driving score on Bench2Drive benchmark. Furthermore, ReasonPlan
demonstrates strong zero-shot generalization on unseen DOS benchmark,
highlighting its adaptability in handling zero-shot corner cases. Code and
dataset will be found in https://github.com/Liuxueyi/ReasonPlan.

</details>


### [800] [FruitNeRF++: A Generalized Multi-Fruit Counting Method Utilizing Contrastive Learning and Neural Radiance Fields](https://arxiv.org/abs/2505.19863)
*Lukas Meyer,Andrei-Timotei Ardelean,Tim Weyrich,Marc Stamminger*

Main category: cs.CV

TL;DR: FruitNeRF++提出了一种结合对比学习和神经辐射场的多水果计数方法，通过实例掩码和神经实例场实现水果无关的计数，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: FruitNeRF方法需要对每种水果类型进行适配，限制了其实际应用。FruitNeRF++旨在消除这一限制，开发一种形状无关的多水果计数框架。

Method: 利用视觉基础模型预测的实例掩码，将每个水果的身份编码为实例嵌入到神经实例场中，通过体积采样提取带有实例特征的点云，以水果无关的方式进行聚类计数。

Result: 在包含多种水果的合成数据集和真实苹果数据集上评估，FruitNeRF++更易控制且优于其他最先进方法。

Conclusion: FruitNeRF++通过实例掩码和神经实例场实现了高效、通用的水果计数，具有实际应用潜力。

Abstract: We introduce FruitNeRF++, a novel fruit-counting approach that combines
contrastive learning with neural radiance fields to count fruits from
unstructured input photographs of orchards. Our work is based on FruitNeRF,
which employs a neural semantic field combined with a fruit-specific clustering
approach. The requirement for adaptation for each fruit type limits the
applicability of the method, and makes it difficult to use in practice. To lift
this limitation, we design a shape-agnostic multi-fruit counting framework,
that complements the RGB and semantic data with instance masks predicted by a
vision foundation model. The masks are used to encode the identity of each
fruit as instance embeddings into a neural instance field. By volumetrically
sampling the neural fields, we extract a point cloud embedded with the instance
features, which can be clustered in a fruit-agnostic manner to obtain the fruit
count. We evaluate our approach using a synthetic dataset containing apples,
plums, lemons, pears, peaches, and mangoes, as well as a real-world benchmark
apple dataset. Our results demonstrate that FruitNeRF++ is easier to control
and compares favorably to other state-of-the-art methods.

</details>


### [801] [EmoNet-Face: An Expert-Annotated Benchmark for Synthetic Emotion Recognition](https://arxiv.org/abs/2505.20033)
*Christoph Schuhmann,Robert Kaczmarczyk,Gollam Rabby,Felix Friedrich,Maurice Kraus,Krishna Kalyan,Kourosh Nadi,Huu Nguyen,Kristian Kersting,Sören Auer*

Main category: cs.CV

TL;DR: 论文提出了EmoNet Face基准套件，用于改进AI对人类情感的识别能力，包含40类情感分类、大规模AI生成数据集及高性能模型。


<details>
  <summary>Details</summary>
Motivation: 当前视觉与视觉语言模型在情感识别上存在局限，如情感谱系狭窄、数据集缺乏多样性和控制，导致潜在偏见。

Method: 构建包含40类情感的新分类体系，创建三个大规模AI生成数据集（EmoNet HQ、Binary、Big），并通过多专家标注训练模型EmpathicInsight-Face。

Result: 开发的模型在基准测试中达到人类专家水平，数据集覆盖多样人口统计特征，情感分类更精细。

Conclusion: EmoNet Face为AI系统提供了更深入理解人类情感的可靠基础，公开的套件包括分类法、数据集和模型。

Abstract: Effective human-AI interaction relies on AI's ability to accurately perceive
and interpret human emotions. Current benchmarks for vision and vision-language
models are severely limited, offering a narrow emotional spectrum that
overlooks nuanced states (e.g., bitterness, intoxication) and fails to
distinguish subtle differences between related feelings (e.g., shame vs.
embarrassment). Existing datasets also often use uncontrolled imagery with
occluded faces and lack demographic diversity, risking significant bias. To
address these critical gaps, we introduce EmoNet Face, a comprehensive
benchmark suite. EmoNet Face features: (1) A novel 40-category emotion
taxonomy, meticulously derived from foundational research to capture finer
details of human emotional experiences. (2) Three large-scale, AI-generated
datasets (EmoNet HQ, Binary, and Big) with explicit, full-face expressions and
controlled demographic balance across ethnicity, age, and gender. (3) Rigorous,
multi-expert annotations for training and high-fidelity evaluation. (4) We
built EmpathicInsight-Face, a model achieving human-expert-level performance on
our benchmark. The publicly released EmoNet Face suite - taxonomy, datasets,
and model - provides a robust foundation for developing and evaluating AI
systems with a deeper understanding of human emotions.

</details>


### [802] [ViTaPEs: Visuotactile Position Encodings for Cross-Modal Alignment in Multimodal Transformers](https://arxiv.org/abs/2505.20032)
*Fotios Lygerakis,Ozan Özdenizci,Elmar Rückert*

Main category: cs.CV

TL;DR: ViTaPEs提出了一种基于Transformer的视觉-触觉融合框架，通过多尺度位置编码实现跨模态关联，在多项任务中超越现有方法并展现零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-触觉表征学习方法依赖预训练视觉语言模型，且缺乏对位置编码和多尺度空间推理的研究，难以实现跨任务和跨环境的泛化。

Method: 采用Transformer架构，设计新型多尺度位置编码方案捕获模态内结构与跨模态关联，并提供了可证明的融合理论保证（单射性、刚体运动等变性、信息保留）。

Result: 在多个大规模真实数据集上超越SOTA基线，实现零样本外域泛化，并在机器人抓取任务中迁移学习表现优异。

Conclusion: ViTaPEs为视觉-触觉感知提供了理论可靠且任务无关的表征框架，其通用性和可迁移性为机器人操作开辟了新途径。

Abstract: Tactile sensing provides local essential information that is complementary to
visual perception, such as texture, compliance, and force. Despite recent
advances in visuotactile representation learning, challenges remain in fusing
these modalities and generalizing across tasks and environments without heavy
reliance on pre-trained vision-language models. Moreover, existing methods do
not study positional encodings, thereby overlooking the multi-scale spatial
reasoning needed to capture fine-grained visuotactile correlations. We
introduce ViTaPEs, a transformer-based framework that robustly integrates
visual and tactile input data to learn task-agnostic representations for
visuotactile perception. Our approach exploits a novel multi-scale positional
encoding scheme to capture intra-modal structures, while simultaneously
modeling cross-modal cues. Unlike prior work, we provide provable guarantees in
visuotactile fusion, showing that our encodings are injective,
rigid-motion-equivariant, and information-preserving, validating these
properties empirically. Experiments on multiple large-scale real-world datasets
show that ViTaPEs not only surpasses state-of-the-art baselines across various
recognition tasks but also demonstrates zero-shot generalization to unseen,
out-of-domain scenarios. We further demonstrate the transfer-learning strength
of ViTaPEs in a robotic grasping task, where it outperforms state-of-the-art
baselines in predicting grasp success. Project page:
https://sites.google.com/view/vitapes

</details>


### [803] [AdaTP: Attention-Debiased Token Pruning for Video Large Language Models](https://arxiv.org/abs/2505.20100)
*Fengyuan Sun,Leqi Shen,Hui Chen,Sicheng Zhao,Jungong Han,Guiguang Ding*

Main category: cs.CV

TL;DR: 论文提出AdaTP方法，通过消除注意力偏差优化视频大语言模型的令牌剪枝，显著降低计算开销同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型在视频理解任务中因处理大量视觉令牌而产生高计算开销，且现有令牌压缩方法依赖存在偏差的注意力分数。

Method: 提出AdaTP，集成两个去偏模块分别处理全局和局部注意力偏差，无需额外训练即可实现高效令牌剪枝。

Result: 在多个视频理解基准测试中达到SOTA，如在LLaVA-OneVision-7B上仅需27.3%计算量即可保持原模型性能。

Conclusion: AdaTP有效解决了注意力偏差问题，为视频大语言模型提供了高效且性能无损的令牌剪枝方案。

Abstract: Video Large Language Models (Video LLMs) have achieved remarkable results in
video understanding tasks. However, they often suffer from heavy computational
overhead due to the large number of visual tokens generated from multiple video
frames. Existing visual token compression methods often rely on attention
scores from language models as guidance. However, these scores exhibit inherent
biases: global bias reflects a tendency to focus on the two ends of the visual
token sequence, while local bias leads to an over-concentration on the same
spatial positions across different frames. To address the issue of attention
bias, we propose $\textbf{A}$ttention-$\textbf{D}$ebi$\textbf{a}$sed
$\textbf{T}$oken $\textbf{P}$runing for Video Large Language Models
($\textbf{AdaTP}$), a novel token pruning pipeline for Video LLMs. AdaTP
integrates two dedicated debiasing modules into the pipeline, targeting global
attention bias and local attention bias, respectively. Without the need for
additional training, our method significantly reduces the computational
overhead of Video LLMs while retaining the performance of vanilla models.
Extensive evaluation shows that AdaTP achieves state-of-the-art performance in
various commonly used video understanding benchmarks. In particular, on
LLaVA-OneVision-7B, AdaTP maintains performance without degradation while using
only up to $27.3\%$ FLOPs compared to the vanilla model. Our code will be
released soon.

</details>


### [804] [In-Context Brush: Zero-shot Customized Subject Insertion with Context-Aware Latent Space Manipulation](https://arxiv.org/abs/2505.20271)
*Yu Xu,Fan Tang,You Wu,Lin Gao,Oliver Deussen,Hongbin Yan,Jintao Li,Juan Cao,Tong-Yee Lee*

Main category: cs.CV

TL;DR: 提出'In-Context Brush'框架，通过上下文学习实现零样本定制化主体插入，无需模型调优即可保持高保真度和文本对齐。


<details>
  <summary>Details</summary>
Motivation: 现有方法在定制化主体插入时难以同时保证高保真度和文本意图对齐，需要无需训练或额外数据收集的解决方案。

Method: 基于预训练MMDiT修复网络，通过双级潜在空间操作（头内潜在特征偏移和头间注意力重加权）实现测试时增强。

Result: 实验表明该方法在身份保持、文本对齐和图像质量上优于现有技术，且无需专门训练或数据收集。

Conclusion: 该框架为零样本定制化主体插入提供了有效解决方案，显著提升了生成结果的质量和可控性。

Abstract: Recent advances in diffusion models have enhanced multimodal-guided visual
generation, enabling customized subject insertion that seamlessly "brushes"
user-specified objects into a given image guided by textual prompts. However,
existing methods often struggle to insert customized subjects with high
fidelity and align results with the user's intent through textual prompts. In
this work, we propose "In-Context Brush", a zero-shot framework for customized
subject insertion by reformulating the task within the paradigm of in-context
learning. Without loss of generality, we formulate the object image and the
textual prompts as cross-modal demonstrations, and the target image with the
masked region as the query. The goal is to inpaint the target image with the
subject aligning textual prompts without model tuning. Building upon a
pretrained MMDiT-based inpainting network, we perform test-time enhancement via
dual-level latent space manipulation: intra-head "latent feature shifting"
within each attention head that dynamically shifts attention outputs to reflect
the desired subject semantics and inter-head "attention reweighting" across
different heads that amplifies prompt controllability through differential
attention prioritization. Extensive experiments and applications demonstrate
that our approach achieves superior identity preservation, text alignment, and
image quality compared to existing state-of-the-art methods, without requiring
dedicated training or additional data collection.

</details>


### [805] [OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation](https://arxiv.org/abs/2505.20292)
*Shenghai Yuan,Xianyi He,Yufan Deng,Yang Ye,Jinfa Huang,Bin Lin,Chongyang Ma,Jiebo Luo,Li Yuan*

Main category: cs.CV

TL;DR: 论文提出OpenS2V-Nexus，包含评估基准OpenS2V-Eval和大规模数据集OpenS2V-5M，用于提升主题到视频生成（S2V）的研究。


<details>
  <summary>Details</summary>
Motivation: 现有S2V评估基准过于粗粒度，无法准确评估模型生成主题一致且自然的视频能力，需建立更精细的评估体系和数据集。

Method: 提出OpenS2V-Eval（含180提示词和自动指标）评估主题一致性、自然度和文本相关性；构建OpenS2V-5M数据集（500万高质量三元组）。

Result: 全面评估16个S2V模型，揭示其优劣；数据集通过跨视频关联和GPT合成确保主题多样性。

Conclusion: OpenS2V-Nexus为S2V研究提供基础设施，推动领域发展。

Abstract: Subject-to-Video (S2V) generation aims to create videos that faithfully
incorporate reference content, providing enhanced flexibility in the production
of videos. To establish the infrastructure for S2V generation, we propose
OpenS2V-Nexus, consisting of (i) OpenS2V-Eval, a fine-grained benchmark, and
(ii) OpenS2V-5M, a million-scale dataset. In contrast to existing S2V
benchmarks inherited from VBench that focus on global and coarse-grained
assessment of generated videos, OpenS2V-Eval focuses on the model's ability to
generate subject-consistent videos with natural subject appearance and identity
fidelity. For these purposes, OpenS2V-Eval introduces 180 prompts from seven
major categories of S2V, which incorporate both real and synthetic test data.
Furthermore, to accurately align human preferences with S2V benchmarks, we
propose three automatic metrics, NexusScore, NaturalScore and GmeScore, to
separately quantify subject consistency, naturalness, and text relevance in
generated videos. Building on this, we conduct a comprehensive evaluation of 16
representative S2V models, highlighting their strengths and weaknesses across
different content. Moreover, we create the first open-source large-scale S2V
generation dataset OpenS2V-5M, which consists of five million high-quality 720P
subject-text-video triples. Specifically, we ensure subject-information
diversity in our dataset by (1) segmenting subjects and building pairing
information via cross-video associations and (2) prompting GPT-Image-1 on raw
frames to synthesize multi-view representations. Through OpenS2V-Nexus, we
deliver a robust infrastructure to accelerate future S2V generation research.

</details>


### [806] [GLEAM: Learning Generalizable Exploration Policy for Active Mapping in Complex 3D Indoor Scenes](https://arxiv.org/abs/2505.20294)
*Xiao Chen,Tai Wang,Quanyi Li,Tao Huang,Jiangmiao Pang,Tianfan Xue*

Main category: cs.CV

TL;DR: 提出GLEAM-Bench基准和GLEAM策略，提升移动机器人在复杂未知环境中的主动建图泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法因训练数据不足和探索策略保守，在多样化布局和复杂连通性场景中泛化能力有限。

Method: 引入GLEAM-Bench大规模基准，并基于语义表示、长期可达目标和随机化策略提出GLEAM统一探索策略。

Result: 在128个未见复杂场景中，覆盖率达66.50%（提升9.49%），轨迹效率与建图精度显著优于现有方法。

Conclusion: GLEAM通过创新策略实现跨场景可靠主动建图，为机器人自主探索提供新解决方案。

Abstract: Generalizable active mapping in complex unknown environments remains a
critical challenge for mobile robots. Existing methods, constrained by
insufficient training data and conservative exploration strategies, exhibit
limited generalizability across scenes with diverse layouts and complex
connectivity. To enable scalable training and reliable evaluation, we introduce
GLEAM-Bench, the first large-scale benchmark designed for generalizable active
mapping with 1,152 diverse 3D scenes from synthetic and real-scan datasets.
Building upon this foundation, we propose GLEAM, a unified generalizable
exploration policy for active mapping. Its superior generalizability comes
mainly from our semantic representations, long-term navigable goals, and
randomized strategies. It significantly outperforms state-of-the-art methods,
achieving 66.50% coverage (+9.49%) with efficient trajectories and improved
mapping accuracy on 128 unseen complex scenes. Project page:
https://xiao-chen.tech/gleam/.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [807] [InjectLab: A Tactical Framework for Adversarial Threat Modeling Against Large Language Models](https://arxiv.org/abs/2505.18156)
*Austin Howard*

Main category: cs.CR

TL;DR: InjectLab是一个开源安全框架，专门针对大型语言模型(LLM)的提示词攻击，提供25种攻击技术分类及防御方案。


<details>
  <summary>Details</summary>
Motivation: 随着ChatGPT等LLM的普及，提示词攻击成为新型安全威胁，需要系统化的应对方案。

Method: 基于MITRE ATT&CK框架构建，包含6大战术、25种攻击技术的分类矩阵，提供YAML测试用例和Python工具。

Result: 建立了首个专注于LLM提示层的威胁框架，涵盖指令劫持、身份伪装等攻击类型的检测与缓解方案。

Conclusion: InjectLab为LLM安全提供了社区驱动的实践基础，未来将持续完善对抗性防护体系。

Abstract: Large Language Models (LLMs) are changing the way people interact with
technology. Tools like ChatGPT and Claude AI are now common in business,
research, and everyday life. But with that growth comes new risks, especially
prompt-based attacks that exploit how these models process language. InjectLab
is a security framework designed to address that problem. This paper introduces
InjectLab as a structured, open-source matrix that maps real-world techniques
used to manipulate LLMs. The framework is inspired by MITRE ATT&CK and focuses
specifically on adversarial behavior at the prompt layer. It includes over 25
techniques organized under six core tactics, covering threats like instruction
override, identity swapping, and multi-agent exploitation. Each technique in
InjectLab includes detection guidance, mitigation strategies, and YAML-based
simulation tests. A Python tool supports easy execution of prompt-based test
cases. This paper outlines the framework's structure, compares it to other AI
threat taxonomies, and discusses its future direction as a practical,
community-driven foundation for securing language models.

</details>


### [808] [Architectural Backdoors for Within-Batch Data Stealing and Model Inference Manipulation](https://arxiv.org/abs/2505.18323)
*Nicolas Küchler,Ivan Petrov,Conrad Grobler,Ilia Shumailov*

Main category: cs.CR

TL;DR: 该论文提出了一种新型神经网络后门攻击，针对批处理推理过程，能窃取并操控同一批次中其他用户的数据，威胁用户隐私和系统完整性。作者还提出了一种确定性防御策略，并在Hugging Face平台上发现200多个存在信息泄露风险的模型。


<details>
  <summary>Details</summary>
Motivation: 过去十年学术界主要研究分类任务中的预测篡改后门，但其现实危害性尚不明确。本文旨在揭示一种更具破坏性的架构级后门，通过批处理推理实现跨用户数据操控，证明其实际威胁。

Method: 利用架构后门技术针对批处理过程设计攻击方案，使攻击者能窃取/操控同批次用户数据。提出基于信息流控制的防御策略，通过模型图分析确保批次内用户输入的隔离性。

Result: 攻击方案被证实高效可行，能植入主流模型架构。在Hugging Face平台大规模分析中发现200多个因动态量化导致信息泄露的模型。防御策略能提供形式化安全保障。

Conclusion: 批处理架构后门构成新型严重威胁，需采用形式化验证方法防御。当前模型部署中普遍存在无意设计导致的信息泄露漏洞。

Abstract: For nearly a decade the academic community has investigated backdoors in
neural networks, primarily focusing on classification tasks where adversaries
manipulate the model prediction. While demonstrably malicious, the immediate
real-world impact of such prediction-altering attacks has remained unclear. In
this paper we introduce a novel and significantly more potent class of
backdoors that builds upon recent advancements in architectural backdoors. We
demonstrate how these backdoors can be specifically engineered to exploit
batched inference, a common technique for hardware utilization, enabling
large-scale user data manipulation and theft. By targeting the batching
process, these architectural backdoors facilitate information leakage between
concurrent user requests and allow attackers to fully control model responses
directed at other users within the same batch. In other words, an attacker who
can change the model architecture can set and steal model inputs and outputs of
other users within the same batch. We show that such attacks are not only
feasible but also alarmingly effective, can be readily injected into prevalent
model architectures, and represent a truly malicious threat to user privacy and
system integrity. Critically, to counteract this new class of vulnerabilities,
we propose a deterministic mitigation strategy that provides formal guarantees
against this new attack vector, unlike prior work that relied on Large Language
Models to find the backdoors. Our mitigation strategy employs a novel
Information Flow Control mechanism that analyzes the model graph and proves
non-interference between different user inputs within the same batch. Using our
mitigation strategy we perform a large scale analysis of models hosted through
Hugging Face and find over 200 models that introduce (unintended) information
leakage between batch entries due to the use of dynamic quantization.

</details>


### [809] [A Critical Evaluation of Defenses against Prompt Injection Attacks](https://arxiv.org/abs/2505.18333)
*Yuqi Jia,Zedian Shao,Yupei Liu,Jinyuan Jia,Dawn Song,Neil Zhenqiang Gong*

Main category: cs.CR

TL;DR: 该论文指出现有针对大型语言模型提示注入攻击的防御措施评估方法不完善，并提出从有效性和通用性两个维度进行系统评估，发现现有防御措施效果被高估。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对大型语言模型提示注入攻击防御措施的全面评估方法，导致防御效果被夸大。论文旨在建立更科学的评估框架。

Method: 提出从防御效果（针对现有及自适应攻击）和模型通用性两个维度评估防御措施，并开发了系统评估方法。

Result: 采用新评估方法后发现，现有防御措施的实际效果不如先前研究报告的那样成功。

Conclusion: 该研究为未来防御措施的评估和开发提供了基础框架，揭示了当前防御措施的局限性。

Abstract: Large Language Models (LLMs) are vulnerable to prompt injection attacks, and
several defenses have recently been proposed, often claiming to mitigate these
attacks successfully. However, we argue that existing studies lack a principled
approach to evaluating these defenses. In this paper, we argue the need to
assess defenses across two critical dimensions: (1) effectiveness, measured
against both existing and adaptive prompt injection attacks involving diverse
target and injected prompts, and (2) general-purpose utility, ensuring that the
defense does not compromise the foundational capabilities of the LLM. Our
critical evaluation reveals that prior studies have not followed such a
comprehensive evaluation methodology. When assessed using this principled
approach, we show that existing defenses are not as successful as previously
reported. This work provides a foundation for evaluating future defenses and
guiding their development. Our code and data are available at:
https://github.com/PIEval123/PIEval.

</details>


### [810] [Dynamic Risk Assessments for Offensive Cybersecurity Agents](https://arxiv.org/abs/2505.18384)
*Boyi Wei,Benedikt Stroebl,Jiacen Xu,Joie Zhang,Zhou Li,Peter Henderson*

Main category: cs.CR

TL;DR: 基础模型在自主编程能力上的提升可能被用于自动化恶意网络攻击，现有风险评估未充分考虑对手的实际自由度，研究表明即使有限计算资源也能显著提升攻击能力。


<details>
  <summary>Details</summary>
Motivation: 探讨基础模型作为自主编程工具可能被滥用于网络攻击的风险，指出当前安全评估未充分模拟真实对手的迭代优化能力。

Method: 在有限计算预算（8 H100 GPU小时）下，通过强验证器和激励措施对攻击代理进行迭代优化，测试其在InterCode CTF中的能力提升。

Result: 仅用少量计算资源，攻击代理的网络安全能力相对基线提升超过40%，证明动态风险评估的必要性。

Conclusion: 需采用动态评估方法衡量AI代理的网络安全风险，更真实反映对手在有限资源下的潜在威胁。

Abstract: Foundation models are increasingly becoming better autonomous programmers,
raising the prospect that they could also automate dangerous offensive
cyber-operations. Current frontier model audits probe the cybersecurity risks
of such agents, but most fail to account for the degrees of freedom available
to adversaries in the real world. In particular, with strong verifiers and
financial incentives, agents for offensive cybersecurity are amenable to
iterative improvement by would-be adversaries. We argue that assessments should
take into account an expanded threat model in the context of cybersecurity,
emphasizing the varying degrees of freedom that an adversary may possess in
stateful and non-stateful environments within a fixed compute budget. We show
that even with a relatively small compute budget (8 H100 GPU Hours in our
study), adversaries can improve an agent's cybersecurity capability on
InterCode CTF by more than 40\% relative to the baseline -- without any
external assistance. These results highlight the need to evaluate agents'
cybersecurity risk in a dynamic manner, painting a more representative picture
of risk.

</details>


### [811] [Towards Anonymous Neural Network Inference](https://arxiv.org/abs/2505.18398)
*Liao Peiyuan*

Main category: cs.CR

TL;DR: Funion系统通过结合Pigeonhole存储协议和BACAP方案，为神经网络推理提供端到端的发送者-接收者不可链接性，确保用户匿名性并隐藏网络流量和计算负载特征。


<details>
  <summary>Details</summary>
Motivation: 当前云服务中的神经网络推理可能暴露用户的隐私信息，如输入输出之间的关联。本文旨在提供一个完全匿名的推理查询平台，保护用户隐私。

Method: 利用Echomix匿名系统中的Pigeonhole存储协议和BACAP方案，采用存储-计算-存储范式，量化执行时间为公开延迟桶，隐藏流量和计算特征。

Result: Funion继承了Echomix的强元数据隐私保证，并在生产规模工作负载下引入可接受的开销，实现了输入输出方的无痕连接。

Conclusion: Funion为云服务中的匿名神经网络推理提供了可行方案，具有强隐私保护和实际应用潜力。

Abstract: We introduce funion, a system providing end-to-end sender-receiver
unlinkability for neural network inference. By leveraging the Pigeonhole
storage protocol and BACAP (blinding-and-capability) scheme from the Echomix
anonymity system, funion inherits the provable security guarantees of modern
mixnets. Users can anonymously store input tensors in pseudorandom storage
locations, commission compute services to process them via the neural network,
and retrieve results with no traceable connection between input and output
parties. This store-compute-store paradigm masks both network traffic patterns
and computational workload characteristics, while quantizing execution timing
into public latency buckets. Our security analysis demonstrates that funion
inherits the strong metadata privacy guarantees of Echomix under largely the
same trust assumptions, while introducing acceptable overhead for
production-scale workloads. Our work paves the way towards an accessible
platform where users can submit fully anonymized inference queries to cloud
services.

</details>


### [812] [Invisible Tokens, Visible Bills: The Urgent Need to Audit Hidden Operations in Opaque LLM Services](https://arxiv.org/abs/2505.18471)
*Guoheng Sun,Ziyao Wang,Xuandong Zhao,Bowei Tian,Zheyu Shen,Yexiao He,Jinming Xing,Ang Li*

Main category: cs.CR

TL;DR: 论文探讨了商业不透明大语言模型服务(COLS)中的责任挑战，提出了审计框架以增强透明度和可验证性。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型服务(COLS)内部操作不透明，用户无法观察或验证计费依据，存在数量膨胀和质量降级风险。

Method: 提出多种审计策略，包括基于承诺、预测、行为和签名的方法，并探索水印和可信执行环境等补充机制。

Result: 设计了一个模块化的三层审计框架，支持执行、安全日志和用户审计，同时保护专有内部信息。

Conclusion: 鼓励进一步研究和政策制定，以提高商业LLM服务的透明度、可审计性和责任性。

Abstract: Modern large language model (LLM) services increasingly rely on complex,
often abstract operations, such as multi-step reasoning and multi-agent
collaboration, to generate high-quality outputs. While users are billed based
on token consumption and API usage, these internal steps are typically not
visible. We refer to such systems as Commercial Opaque LLM Services (COLS).
This position paper highlights emerging accountability challenges in COLS:
users are billed for operations they cannot observe, verify, or contest. We
formalize two key risks: \textit{quantity inflation}, where token and call
counts may be artificially inflated, and \textit{quality downgrade}, where
providers might quietly substitute lower-cost models or tools. Addressing these
risks requires a diverse set of auditing strategies, including
commitment-based, predictive, behavioral, and signature-based methods. We
further explore the potential of complementary mechanisms such as watermarking
and trusted execution environments to enhance verifiability without
compromising provider confidentiality. We also propose a modular three-layer
auditing framework for COLS and users that enables trustworthy verification
across execution, secure logging, and user-facing auditability without exposing
proprietary internals. Our aim is to encourage further research and policy
development toward transparency, auditability, and accountability in commercial
LLM services.

</details>


### [813] [Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models](https://arxiv.org/abs/2505.18773)
*Jamie Hayes,Ilia Shumailov,Christopher A. Choquette-Choo,Matthew Jagielski,George Kaissis,Katherine Lee,Milad Nasr,Sahra Ghalebikesabi,Niloofar Mireshghallah,Meenatchi Sundaram Mutu Selva Annamalai,Igor Shilov,Matthieu Meeus,Yves-Alexandre de Montjoye,Franziska Boenisch,Adam Dziedzic,A. Feder Cooper*

Main category: cs.CR

TL;DR: 该论文研究了成员推理攻击（MIA）在大型预训练语言模型（LLM）上的有效性，通过扩展LiRA攻击方法，发现强MIA在LLM上可行但效果有限。


<details>
  <summary>Details</summary>
Motivation: 先前的研究在大型预训练语言模型上的成员推理攻击（MIA）效果有限，无法确定是攻击设计问题还是MIA本身在LLM上无效。本文旨在探索这一问题。

Method: 扩展了最强的MIA方法LiRA，应用于不同规模的GPT-2模型（10M到1B参数），并在C4数据集上训练了超过200亿token的参考模型。

Result: 研究发现：(1)强MIA在预训练LLM上可行；(2)实际效果有限（如AUC<0.7）；(3)MIA成功与隐私指标的关系比先前研究更复杂。

Conclusion: 强MIA在LLM上可行但效果有限，且其与隐私指标的关系并非简单线性，为未来隐私保护研究提供了新方向。

Abstract: State-of-the-art membership inference attacks (MIAs) typically require
training many reference models, making it difficult to scale these attacks to
large pre-trained language models (LLMs). As a result, prior research has
either relied on weaker attacks that avoid training reference models (e.g.,
fine-tuning attacks), or on stronger attacks applied to small-scale models and
datasets. However, weaker attacks have been shown to be brittle - achieving
close-to-arbitrary success - and insights from strong attacks in simplified
settings do not translate to today's LLMs. These challenges have prompted an
important question: are the limitations observed in prior work due to attack
design choices, or are MIAs fundamentally ineffective on LLMs? We address this
question by scaling LiRA - one of the strongest MIAs - to GPT-2 architectures
ranging from 10M to 1B parameters, training reference models on over 20B tokens
from the C4 dataset. Our results advance the understanding of MIAs on LLMs in
three key ways: (1) strong MIAs can succeed on pre-trained LLMs; (2) their
effectiveness, however, remains limited (e.g., AUC<0.7) in practical settings;
and, (3) the relationship between MIA success and related privacy metrics is
not as straightforward as prior work has suggested.

</details>


### [814] [$PD^3F$: A Pluggable and Dynamic DoS-Defense Framework Against Resource Consumption Attacks Targeting Large Language Models](https://arxiv.org/abs/2505.18680)
*Yuanhe Zhang,Xinyue Wang,Haoran Gao,Zhenhong Zhou,Fanyu Meng,Yuyao Zhang,Sen Su*

Main category: cs.CR

TL;DR: 该论文提出了一个可插拔的动态DoS防御框架（PD3F），通过两阶段方法防御大语言模型的资源消耗攻击，显著提升了对抗性负载下的用户访问能力。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型（LLMs）计算需求高，容易受到资源消耗攻击，现有工作缺乏有效的缓解策略，导致实际部署中存在未解决的安全风险。

Method: PD3F框架采用两阶段方法：输入侧通过资源索引指导动态请求轮询调度，减少高并发场景下的恶意资源消耗；输出侧引入自适应终止机制，提前终止恶意生成。

Result: 在六个模型上的实验表明，PD3F显著缓解了资源消耗攻击，在对抗性负载下用户访问能力提升了高达500%。

Conclusion: PD3F是朝着抵御资源消耗攻击的弹性和资源感知的LLM部署迈出的一步。

Abstract: Large Language Models (LLMs), due to substantial computational requirements,
are vulnerable to resource consumption attacks, which can severely degrade
server performance or even cause crashes, as demonstrated by denial-of-service
(DoS) attacks designed for LLMs. However, existing works lack mitigation
strategies against such threats, resulting in unresolved security risks for
real-world LLM deployments. To this end, we propose the Pluggable and Dynamic
DoS-Defense Framework ($PD^3F$), which employs a two-stage approach to defend
against resource consumption attacks from both the input and output sides. On
the input side, we propose the Resource Index to guide Dynamic Request Polling
Scheduling, thereby reducing resource usage induced by malicious attacks under
high-concurrency scenarios. On the output side, we introduce the Adaptive
End-Based Suppression mechanism, which terminates excessive malicious
generation early. Experiments across six models demonstrate that $PD^3F$
significantly mitigates resource consumption attacks, improving users' access
capacity by up to 500% during adversarial load. $PD^3F$ represents a step
toward the resilient and resource-aware deployment of LLMs against resource
consumption attacks.

</details>


### [815] [Security Concerns for Large Language Models: A Survey](https://arxiv.org/abs/2505.18889)
*Miles Q. Li,Benjamin C. M. Fung*

Main category: cs.CR

TL;DR: 该论文综述了大型语言模型（LLMs）带来的新兴安全威胁，包括提示注入、对抗攻击、恶意滥用及自主代理风险，并总结了现有防御措施与未解挑战。


<details>
  <summary>Details</summary>
Motivation: 随着GPT-4、Gemini等LLMs的快速发展，其能力也引入了新的安全漏洞。论文旨在系统梳理相关威胁，推动安全策略研究。

Method: 通过分类威胁类型（如提示注入、数据投毒）、分析案例（2022-2025年研究）及评估防御措施局限性，进行系统性文献综述。

Result: 识别出四类主要威胁，发现现有防御存在不足，尤其是自主代理的目标错位、欺骗性等风险可能持续存在。

Conclusion: 需发展多层次鲁棒安全策略，以确保LLMs的安全性和有益性。

Abstract: Large Language Models (LLMs) such as GPT-4 (and its recent iterations like
GPT-4o and the GPT-4.1 series), Google's Gemini, Anthropic's Claude 3 models,
and xAI's Grok have caused a revolution in natural language processing, but
their capabilities also introduce new security vulnerabilities. In this survey,
we provide a comprehensive overview of the emerging security concerns around
LLMs, categorizing threats into prompt injection and jailbreaking, adversarial
attacks (including input perturbations and data poisoning), misuse by malicious
actors (e.g., for disinformation, phishing, and malware generation), and
worrisome risks inherent in autonomous LLM agents. A significant focus has been
recently placed on the latter, exploring goal misalignment, emergent deception,
self-preservation instincts, and the potential for LLMs to develop and pursue
covert, misaligned objectives (scheming), which may even persist through safety
training. We summarize recent academic and industrial studies (2022-2025) that
exemplify each threat, analyze proposed defenses and their limitations, and
identify open challenges in securing LLM-based applications. We conclude by
emphasizing the importance of advancing robust, multi-layered security
strategies to ensure LLMs are safe and beneficial.

</details>


### [816] [GenAI Security: Outsmarting the Bots with a Proactive Testing Framework](https://arxiv.org/abs/2505.18172)
*Sunil Kumar Jang Bahadur,Gopala Dhar,Lavi Nigam*

Main category: cs.CR

TL;DR: 本文探讨了生成式AI（GenAI）面临的新型安全挑战，提出了一个主动安全框架以应对高级对抗攻击，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI技术的复杂化和广泛应用，传统安全方法难以应对其带来的新型安全风险，亟需主动防御策略。

Method: 研究提出了一套包含关键方法、工具和策略的主动安全框架，并利用SPML Chatbot Prompt Injection数据集进行实证测试。

Result: 实验证明该框架能有效抵御高级对抗攻击，为生成式AI的安全部署提供了可靠保障。

Conclusion: 研究强调了从被动防御转向主动安全实践的重要性，以确保生成式AI技术的安全、负责任应用。

Abstract: The increasing sophistication and integration of Generative AI (GenAI) models
into diverse applications introduce new security challenges that traditional
methods struggle to address. This research explores the critical need for
proactive security measures to mitigate the risks associated with malicious
exploitation of GenAI systems. We present a framework encompassing key
approaches, tools, and strategies designed to outmaneuver even advanced
adversarial attacks, emphasizing the importance of securing GenAI innovation
against potential liabilities. We also empirically prove the effectiveness of
the said framework by testing it against the SPML Chatbot Prompt Injection
Dataset. This work highlights the shift from reactive to proactive security
practices essential for the safe and responsible deployment of GenAI
technologies

</details>


### [817] [An Attack to Break Permutation-Based Private Third-Party Inference Schemes for LLMs](https://arxiv.org/abs/2505.18332)
*Rahul Thomas,Louai Zahran,Erica Choi,Akilesh Potti,Micah Goldblum,Arka Pal*

Main category: cs.CR

TL;DR: 该论文揭示了当前基于统计混淆的LLM隐私保护方案存在严重漏洞，提出了一种能近乎完美恢复原始提示的新型重建技术，并分析了先前理论证明的缺陷。


<details>
  <summary>Details</summary>
Motivation: 随着第三方LLM推理服务的普及，隐私问题日益突出。现有加密方法效率低下，而基于统计混淆的方案虽声称安全但缺乏严格验证。

Method: 提出新型重建技术攻击三种最新隐私方案，通过逆向工程还原置换后的隐藏状态，并系统分析先前理论证明的缺陷。

Result: 攻击成功率达近乎完美（多个SOTA模型），证明现有置换方案不安全，揭示了理论证明与实际漏洞间的巨大差距。

Conclusion: LLM隐私保护方案需要更严格的安全分析，统计混淆方法不能替代加密方案，置换操作本身无法提供可靠隐私保障。

Abstract: Recent advances in Large Language Models (LLMs) have led to the widespread
adoption of third-party inference services, raising critical privacy concerns.
Existing methods of performing private third-party inference, such as Secure
Multiparty Computation (SMPC), often rely on cryptographic methods. However,
these methods are thousands of times slower than standard unencrypted
inference, and fail to scale to large modern LLMs. Therefore, recent lines of
work have explored the replacement of expensive encrypted nonlinear
computations in SMPC with statistical obfuscation methods - in particular,
revealing permuted hidden states to the third parties, with accompanying strong
claims of the difficulty of reversal into the unpermuted states. In this work,
we begin by introducing a novel reconstruction technique that can recover
original prompts from hidden states with nearly perfect accuracy across
multiple state-of-the-art LLMs. We then show that extensions of our attack are
nearly perfectly effective in reversing permuted hidden states of LLMs,
demonstrating the insecurity of three recently proposed privacy schemes. We
further dissect the shortcomings of prior theoretical `proofs' of permuation
security which allow our attack to succeed. Our findings highlight the
importance of rigorous security analysis in privacy-preserving LLM inference.

</details>


### [818] [Lifelong Safety Alignment for Language Models](https://arxiv.org/abs/2505.20259)
*Haoyu Wang,Zeyu Qin,Yifei Zhao,Chao Du,Min Lin,Xueqian Wang,Tianyu Pang*

Main category: cs.CR

TL;DR: 该论文提出了一种终身安全对齐框架，通过元攻击者和防御者的对抗训练，使LLMs能持续适应新型越狱攻击，最终将攻击成功率降至7%。


<details>
  <summary>Details</summary>
Motivation: 现有防御主要针对已知攻击类型，而LLMs在部署时可能面临未知的新型越狱攻击，因此需要一种能持续适应新攻击的安全对齐方法。

Method: 框架包含两个对抗组件：元攻击者（主动发现新越狱策略）和防御者（抵抗攻击）。通过GPT-4o提取论文洞见初始化元攻击者，并进行迭代对抗训练。

Result: 首轮元攻击者对RR和LAT的攻击成功率分别达73%和57%，而防御者最终将攻击成功率降至7%。

Conclusion: 该框架显著提升LLMs对未知越狱攻击的鲁棒性，支持其在开放环境中的安全部署。代码已开源。

Abstract: LLMs have made impressive progress, but their growing capabilities also
expose them to highly flexible jailbreaking attacks designed to bypass safety
alignment. While many existing defenses focus on known types of attacks, it is
more critical to prepare LLMs for unseen attacks that may arise during
deployment. To address this, we propose a lifelong safety alignment framework
that enables LLMs to continuously adapt to new and evolving jailbreaking
strategies. Our framework introduces a competitive setup between two
components: a Meta-Attacker, trained to actively discover novel jailbreaking
strategies, and a Defender, trained to resist them. To effectively warm up the
Meta-Attacker, we first leverage the GPT-4o API to extract key insights from a
large collection of jailbreak-related research papers. Through iterative
training, the first iteration Meta-Attacker achieves a 73% attack success rate
(ASR) on RR and a 57% transfer ASR on LAT using only single-turn attacks.
Meanwhile, the Defender progressively improves its robustness and ultimately
reduces the Meta-Attacker's success rate to just 7%, enabling safer and more
reliable deployment of LLMs in open-ended environments. The code is available
at https://github.com/sail-sg/LifelongSafetyAlignment.

</details>


### [819] [A Novel Zero-Trust Identity Framework for Agentic AI: Decentralized Authentication and Fine-Grained Access Control](https://arxiv.org/abs/2505.19301)
*Ken Huang,Vineeth Sai Narajala,John Yeoh,Ramesh Raskar,Youssef Harkati,Jerry Huang,Idan Habler,Chris Hughes*

Main category: cs.CR

TL;DR: 论文提出新型Agentic AI IAM框架，解决传统IAM在多智能体系统中的不足，通过去中心化身份和动态细粒度访问控制实现安全可信的AI代理管理。


<details>
  <summary>Details</summary>
Motivation: 传统IAM系统（如OAuth/OIDC/SAML）为人类用户设计，无法满足多智能体系统（MAS）中动态、短暂且相互依赖的AI代理需求，亟需新的身份与访问管理范式。

Method: 提出基于去中心化标识符（DID）和可验证凭证（VC）的框架，包含代理命名服务（ANS）、动态细粒度访问控制、全局会话管理层，并利用零知识证明（ZKP）实现隐私保护。

Result: 构建了支持能力感知发现、实时策略执行和跨协议撤销的IAM架构，为AI代理生态系统奠定信任与安全基础。

Conclusion: 该框架为Agentic AI领域提供了必要的身份验证、问责制和安全保障，推动复杂多智能体系统的可信交互。

Abstract: Traditional Identity and Access Management (IAM) systems, primarily designed
for human users or static machine identities via protocols such as OAuth,
OpenID Connect (OIDC), and SAML, prove fundamentally inadequate for the
dynamic, interdependent, and often ephemeral nature of AI agents operating at
scale within Multi Agent Systems (MAS), a computational system composed of
multiple interacting intelligent agents that work collectively.
  This paper posits the imperative for a novel Agentic AI IAM framework: We
deconstruct the limitations of existing protocols when applied to MAS,
illustrating with concrete examples why their coarse-grained controls,
single-entity focus, and lack of context-awareness falter. We then propose a
comprehensive framework built upon rich, verifiable Agent Identities (IDs),
leveraging Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs),
that encapsulate an agents capabilities, provenance, behavioral scope, and
security posture.
  Our framework includes an Agent Naming Service (ANS) for secure and
capability-aware discovery, dynamic fine-grained access control mechanisms, and
critically, a unified global session management and policy enforcement layer
for real-time control and consistent revocation across heterogeneous agent
communication protocols. We also explore how Zero-Knowledge Proofs (ZKPs)
enable privacy-preserving attribute disclosure and verifiable policy
compliance.
  We outline the architecture, operational lifecycle, innovative contributions,
and security considerations of this new IAM paradigm, aiming to establish the
foundational trust, accountability, and security necessary for the burgeoning
field of agentic AI and the complex ecosystems they will inhabit.

</details>


### [820] [Benchmarking Poisoning Attacks against Retrieval-Augmented Generation](https://arxiv.org/abs/2505.18543)
*Baolei Zhang,Haoran Xin,Jiatong Li,Dongzhe Zhang,Minghong Fang,Zhuqing Liu,Lihai Nie,Zheli Liu*

Main category: cs.CR

TL;DR: 该论文提出了首个针对RAG系统中毒攻击的全面基准框架，评估了多种攻击和防御方法，发现现有防御措施不足。


<details>
  <summary>Details</summary>
Motivation: 尽管检索增强生成（RAG）能减少大语言模型的幻觉问题，但其引入的外部知识可能带来新的安全漏洞，尤其是中毒攻击。现有研究缺乏对这些攻击实际威胁的全面评估。

Method: 论文提出了一个基准框架，覆盖5个标准问答数据集及其10个扩展版本，13种中毒攻击方法和7种防御机制，全面评估了攻击和防御的效果。

Result: 研究发现，现有攻击在标准数据集上表现良好，但在扩展数据集上效果显著下降。多种先进的RAG架构仍易受攻击，且当前防御技术无法提供有效保护。

Conclusion: 当前防御策略不足以应对中毒攻击，亟需开发更鲁棒和通用的防御方法。

Abstract: Retrieval-Augmented Generation (RAG) has proven effective in mitigating
hallucinations in large language models by incorporating external knowledge
during inference. However, this integration introduces new security
vulnerabilities, particularly to poisoning attacks. Although prior work has
explored various poisoning strategies, a thorough assessment of their practical
threat to RAG systems remains missing. To address this gap, we propose the
first comprehensive benchmark framework for evaluating poisoning attacks on
RAG. Our benchmark covers 5 standard question answering (QA) datasets and 10
expanded variants, along with 13 poisoning attack methods and 7 defense
mechanisms, representing a broad spectrum of existing techniques. Using this
benchmark, we conduct a comprehensive evaluation of all included attacks and
defenses across the full dataset spectrum. Our findings show that while
existing attacks perform well on standard QA datasets, their effectiveness
drops significantly on the expanded versions. Moreover, our results demonstrate
that various advanced RAG architectures, such as sequential, branching,
conditional, and loop RAG, as well as multi-turn conversational RAG, multimodal
RAG systems, and RAG-based LLM agent systems, remain susceptible to poisoning
attacks. Notably, current defense techniques fail to provide robust protection,
underscoring the pressing need for more resilient and generalizable defense
strategies.

</details>


### [821] [LAMDA: A Longitudinal Android Malware Benchmark for Concept Drift Analysis](https://arxiv.org/abs/2505.18551)
*Md Ahsanul Haque,Ismail Hossain,Md Mahmuduzzaman Kamol,Md Jahangir Alam,Suresh Kumar Amalapuram,Sajedul Talukder,Mohammad Saidur Rahman*

Main category: cs.CR

TL;DR: 该论文提出了LAMDA数据集，用于分析Android恶意软件检测中的概念漂移问题，覆盖12年、超100万样本，是目前最全面的基准数据集。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习恶意软件检测系统难以应对真实数据分布的动态变化（如Android生态演变、新恶意软件家族涌现），且现有数据集在时间跨度、多样性和规模上不足，无法系统评估概念漂移的影响。

Method: 构建LAMDA数据集——覆盖2013-2025年（除2015年）、含1380个恶意家族和15万独立样本的百万级Android应用集合，并量化模型性能随时间衰减及特征稳定性。

Result: LAMDA成为迄今最全面的Android恶意软件基准，实证显示标准ML模型性能随时间显著下降，支持对概念漂移、泛化性等问题的深入研究。

Conclusion: LAMDA填补了动态环境下恶意软件检测研究的空白，为时间漂移、检测挑战演进等课题提供了关键数据支撑。

Abstract: Machine learning (ML)-based malware detection systems often fail to account
for the dynamic nature of real-world training and test data distributions. In
practice, these distributions evolve due to frequent changes in the Android
ecosystem, adversarial development of new malware families, and the continuous
emergence of both benign and malicious applications. Prior studies have shown
that such concept drift -- distributional shifts in benign and malicious
samples, leads to significant degradation in detection performance over time.
Despite the practical importance of this issue, existing datasets are often
outdated and limited in temporal scope, diversity of malware families, and
sample scale, making them insufficient for the systematic evaluation of concept
drift in malware detection.
  To address this gap, we present LAMDA, the largest and most temporally
diverse Android malware benchmark to date, designed specifically for concept
drift analysis. LAMDA spans 12 years (2013-2025, excluding 2015), includes over
1 million samples (approximately 37% labeled as malware), and covers 1,380
malware families and 150,000 singleton samples, reflecting the natural
distribution and evolution of real-world Android applications. We empirically
demonstrate LAMDA's utility by quantifying the performance degradation of
standard ML models over time and analyzing feature stability across years. As
the most comprehensive Android malware dataset to date, LAMDA enables in-depth
research into temporal drift, generalization, explainability, and evolving
detection challenges. The dataset and code are available at:
https://iqsec-lab.github.io/LAMDA/.

</details>


### [822] [MLRan: A Behavioural Dataset for Ransomware Analysis and Detection](https://arxiv.org/abs/2505.18613)
*Faithful Chiagoziem Onwuegbuche,Adelodun Olaoluwa,Anca Delia Jurcut,Liliana Pasquale*

Main category: cs.CR

TL;DR: 该论文介绍了MLRan，一个包含4800多个样本的行为勒索软件数据集，并提出构建高质量数据集的指南。通过特征选择和机器学习模型，实现了高达98.7%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 当前公开的勒索软件数据集稀缺且样本量小、多样性不足，难以支持机器学习模型的训练。

Method: 使用互信息过滤和递归特征消除从640万特征中筛选出483个关键特征，训练多种机器学习模型。

Result: 机器学习模型在检测勒索软件时达到98.7%的准确率、98.9%的精确率和98.5%的召回率。

Conclusion: MLRan数据集和公开的源代码支持未来研究，提升勒索软件检测的可复现性和效果。

Abstract: Ransomware remains a critical threat to cybersecurity, yet publicly available
datasets for training machine learning-based ransomware detection models are
scarce and often have limited sample size, diversity, and reproducibility. In
this paper, we introduce MLRan, a behavioural ransomware dataset, comprising
over 4,800 samples across 64 ransomware families and a balanced set of goodware
samples. The samples span from 2006 to 2024 and encompass the four major types
of ransomware: locker, crypto, ransomware-as-a-service, and modern variants. We
also propose guidelines (GUIDE-MLRan), inspired by previous work, for
constructing high-quality behavioural ransomware datasets, which informed the
curation of our dataset. We evaluated the ransomware detection performance of
several machine learning (ML) models using MLRan. For this purpose, we
performed feature selection by conducting mutual information filtering to
reduce the initial 6.4 million features to 24,162, followed by recursive
feature elimination, yielding 483 highly informative features. The ML models
achieved an accuracy, precision and recall of up to 98.7%, 98.9%, 98.5%,
respectively. Using SHAP and LIME, we identified critical indicators of
malicious behaviour, including registry tampering, strings, and API misuse. The
dataset and source code for feature extraction, selection, ML training, and
evaluation are available publicly to support replicability and encourage future
research, which can be found at https://github.com/faithfulco/mlran.

</details>


### [823] [VADER: A Human-Evaluated Benchmark for Vulnerability Assessment, Detection, Explanation, and Remediation](https://arxiv.org/abs/2505.19395)
*Ethan TS. Liu,Austin Wang,Spencer Mateega,Carlos Georgescu,Danny Tang*

Main category: cs.CR

TL;DR: 论文介绍了VADER基准测试，用于评估大语言模型在软件漏洞处理四个关键维度的表现，结果显示当前最先进模型仍有较大改进空间。


<details>
  <summary>Details</summary>
Motivation: 确保大语言模型能有效评估、检测、解释和修复软件漏洞，对构建健壮安全的软件系统至关重要。

Method: 构建包含174个真实漏洞的VADER基准，采用单次提示策略测试6个先进大语言模型，并由安全专家按严格评分标准评估。

Result: 当前最优模型在VADER上表现中等（最高54.7%准确率），修复质量与漏洞分类和测试计划高度相关（Pearson r>0.97）。

Conclusion: VADER为社区提供了可解释、可复现的基准测试工具，推动漏洞感知大语言模型的发展。

Abstract: Ensuring that large language models (LLMs) can effectively assess, detect,
explain, and remediate software vulnerabilities is critical for building robust
and secure software systems. We introduce VADER, a human-evaluated benchmark
designed explicitly to assess LLM performance across four key
vulnerability-handling dimensions: assessment, detection, explanation, and
remediation. VADER comprises 174 real-world software vulnerabilities, each
carefully curated from GitHub repositories and annotated by security experts.
For each vulnerability case, models are tasked with identifying the flaw,
classifying it using Common Weakness Enumeration (CWE), explaining its
underlying cause, proposing a patch, and formulating a test plan. Using a
one-shot prompting strategy, we benchmark six state-of-the-art LLMs (Claude 3.7
Sonnet, Gemini 2.5 Pro, GPT-4.1, GPT-4.5, Grok 3 Beta, and o3) on VADER, and
human security experts evaluated each response according to a rigorous scoring
rubric emphasizing remediation (quality of the code fix, 50%), explanation
(20%), and classification and test plan (30%) according to a standardized
rubric. Our results show that current state-of-the-art LLMs achieve only
moderate success on VADER - OpenAI's o3 attained 54.7% accuracy overall, with
others in the 49-54% range, indicating ample room for improvement. Notably,
remediation quality is strongly correlated (Pearson r > 0.97) with accurate
classification and test plans, suggesting that models that effectively
categorize vulnerabilities also tend to fix them well. VADER's comprehensive
dataset, detailed evaluation rubrics, scoring tools, and visualized results
with confidence intervals are publicly released, providing the community with
an interpretable, reproducible benchmark to advance vulnerability-aware LLMs.
All code and data are available at: https://github.com/AfterQuery/vader

</details>


### [824] [MADCAT: Combating Malware Detection Under Concept Drift with Test-Time Adaptation](https://arxiv.org/abs/2505.18734)
*Eunjin Roh,Yigitcan Kaya,Christopher Kruegel,Giovanni Vigna,Sanghyun Hong*

Main category: cs.CR

TL;DR: MADCAT是一种自监督方法，通过测试时训练解决恶意软件检测中的概念漂移问题，在Android平台上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决恶意软件检测中的概念漂移问题，即模型在新数据上性能下降的问题。

Method: 采用编码器-解码器架构，在测试时使用自监督目标对小规模平衡数据集进行编码器训练。

Result: MADCAT在持续Android恶意软件检测中表现优于基线方法，并能与现有方法协同提升性能。

Conclusion: MADCAT有效解决了概念漂移问题，提升了恶意软件检测的持续适应能力。

Abstract: We present MADCAT, a self-supervised approach designed to address the concept
drift problem in malware detection. MADCAT employs an encoder-decoder
architecture and works by test-time training of the encoder on a small,
balanced subset of the test-time data using a self-supervised objective. During
test-time training, the model learns features that are useful for detecting
both previously seen (old) data and newly arriving samples. We demonstrate the
effectiveness of MADCAT in continuous Android malware detection settings.
MADCAT consistently outperforms baseline methods in detection performance at
test time. We also show the synergy between MADCAT and prior approaches in
addressing concept drift in malware detection

</details>


### [825] [Evaluating AI cyber capabilities with crowdsourced elicitation](https://arxiv.org/abs/2505.19915)
*Artem Petrov,Dmitrii Volkov*

Main category: cs.CR

TL;DR: 论文探讨通过众包方式（如CTF比赛）激发AI在网络安全领域的潜力，发现其表现优异，并提出悬赏机制作为内部评估的有效补充。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力提升，准确评估其网络攻击潜力对治理和部署至关重要。现有内部评估常低估AI能力，需探索替代方案。

Method: 在两场CTF比赛（AI vs. Humans和Cyber Apocalypse）中设置开放AI赛道，分别吸引400和4000支队伍参与，通过众包方式测试AI性能。

Result: AI团队表现突出，分别位列前13%和前21%，共获得7500美元奖金。AI能可靠解决中位人类参与者1小时内完成的网络挑战。

Conclusion: 开放市场激发机制可作为内部评估的有效补充，提议通过悬赏机制及时、经济地掌握AI新兴能力。

Abstract: As AI systems become increasingly capable, understanding their offensive
cyber potential is critical for informed governance and responsible deployment.
However, it's hard to accurately bound their capabilities, and some prior
evaluations dramatically underestimated them. The art of extracting maximum
task-specific performance from AIs is called "AI elicitation", and today's
safety organizations typically conduct it in-house. In this paper, we explore
crowdsourcing elicitation efforts as an alternative to in-house elicitation
work.
  We host open-access AI tracks at two Capture The Flag (CTF) competitions: AI
vs. Humans (400 teams) and Cyber Apocalypse_ (4000 teams). The AI teams achieve
outstanding performance at both events, ranking top-13% and top-21%
respectively for a total of \$7500 in bounties. This impressive performance
suggests that open-market elicitation may offer an effective complement to
in-house elicitation. We propose elicitation bounties as a practical mechanism
for maintaining timely, cost-effective situational awareness of emerging AI
capabilities.
  Another advantage of open elicitations is the option to collect human
performance data at scale. Applying METR's methodology, we found that AI agents
can reliably solve cyber challenges requiring one hour or less of effort from a
median human CTF participant.

</details>


### [826] [DFIR-Metric: A Benchmark Dataset for Evaluating Large Language Models in Digital Forensics and Incident Response](https://arxiv.org/abs/2505.19973)
*Bilel Cherif,Tamas Bisztray,Richard A. Dubniczky,Aaesha Aldahmani,Saeed Alshehhi,Norbert Tihanyi*

Main category: cs.CR

TL;DR: 本文提出了DFIR-Metric基准，用于评估大语言模型在数字取证和事件响应中的表现，包括知识评估、实际取证挑战和实用分析三个部分。


<details>
  <summary>Details</summary>
Motivation: 数字取证和事件响应（DFIR）领域缺乏一个全面的基准来评估大语言模型（LLMs）在理论和实际任务中的表现，尤其是在高风险的取证环境中。

Method: DFIR-Metric基准包含三个部分：700个专家评审的多选题（知识评估）、150个CTF风格任务（实际取证挑战）和500个磁盘与内存取证案例（实用分析）。

Result: 评估了14个LLMs，分析了它们的准确性和一致性，并提出了新的指标Task Understanding Score（TUS）以更有效地评估模型在接近零准确率场景下的表现。

Conclusion: DFIR-Metric为数字取证中AI的进步提供了一个严谨且可重复的基础，所有脚本、工件和结果均在项目网站上公开。

Abstract: Digital Forensics and Incident Response (DFIR) involves analyzing digital
evidence to support legal investigations. Large Language Models (LLMs) offer
new opportunities in DFIR tasks such as log analysis and memory forensics, but
their susceptibility to errors and hallucinations raises concerns in
high-stakes contexts. Despite growing interest, there is no comprehensive
benchmark to evaluate LLMs across both theoretical and practical DFIR domains.
To address this gap, we present DFIR-Metric, a benchmark with three components:
(1) Knowledge Assessment: a set of 700 expert-reviewed multiple-choice
questions sourced from industry-standard certifications and official
documentation; (2) Realistic Forensic Challenges: 150 CTF-style tasks testing
multi-step reasoning and evidence correlation; and (3) Practical Analysis: 500
disk and memory forensics cases from the NIST Computer Forensics Tool Testing
Program (CFTT). We evaluated 14 LLMs using DFIR-Metric, analyzing both their
accuracy and consistency across trials. We also introduce a new metric, the
Task Understanding Score (TUS), designed to more effectively evaluate models in
scenarios where they achieve near-zero accuracy. This benchmark offers a
rigorous, reproducible foundation for advancing AI in digital forensics. All
scripts, artifacts, and results are available on the project website at
https://github.com/DFIR-Metric.

</details>


### [827] [Poison in the Well: Feature Embedding Disruption in Backdoor Attacks](https://arxiv.org/abs/2505.19821)
*Zhou Feng,Jiahao Chen,Chunyi Zhou,Yuwen Pu,Qingming Li,Shouling Ji*

Main category: cs.CR

TL;DR: 论文提出ShadowPrint，一种针对神经网络特征嵌入的后门攻击方法，通过聚类优化策略实现高攻击成功率与隐蔽性，且对训练数据依赖低，毒化率可低至0.01%。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击存在对训练数据过度依赖、隐蔽性差及不稳定的问题，限制了其在现实场景中的应用效果。

Method: 采用基于聚类的优化策略对齐特征嵌入，减少对训练数据的依赖，支持极低毒化率（0.01%-0.05%）。

Result: ShadowPrint在干净标签和脏标签设置下均表现优异，攻击成功率高达100%，准确率衰减不超过1%，平均检测率低于5%。

Conclusion: ShadowPrint为后门攻击能力设定了新标准，凸显了针对特征空间操作的高级防御策略的必要性。

Abstract: Backdoor attacks embed malicious triggers into training data, enabling
attackers to manipulate neural network behavior during inference while
maintaining high accuracy on benign inputs. However, existing backdoor attacks
face limitations manifesting in excessive reliance on training data, poor
stealth, and instability, which hinder their effectiveness in real-world
applications. Therefore, this paper introduces ShadowPrint, a versatile
backdoor attack that targets feature embeddings within neural networks to
achieve high ASRs and stealthiness. Unlike traditional approaches, ShadowPrint
reduces reliance on training data access and operates effectively with
exceedingly low poison rates (as low as 0.01%). It leverages a clustering-based
optimization strategy to align feature embeddings, ensuring robust performance
across diverse scenarios while maintaining stability and stealth. Extensive
evaluations demonstrate that ShadowPrint achieves superior ASR (up to 100%),
steady CA (with decay no more than 1% in most cases), and low DDR (averaging
below 5%) across both clean-label and dirty-label settings, and with poison
rates ranging from as low as 0.01% to 0.05%, setting a new standard for
backdoor attack capabilities and emphasizing the need for advanced defense
strategies focused on feature space manipulations.

</details>


### [828] [One Surrogate to Fool Them All: Universal, Transferable, and Targeted Adversarial Attacks with CLIP](https://arxiv.org/abs/2505.19840)
*Binyan Xu,Xilin Dai,Di Tang,Kehuan Zhang*

Main category: cs.CR

TL;DR: 该论文提出了一种名为UnivIntruder的新型对抗攻击框架，仅需一个公开的CLIP模型和数据集即可生成通用的、可转移的、有针对性的对抗扰动，显著提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗攻击方法通常需要频繁查询目标模型或依赖与目标模型相似的替代模型，这在实际应用中可能不可行。论文旨在解决在无法获取训练数据且不能频繁查询目标模型的现实场景下，如何有效生成对抗样本的问题。

Method: UnivIntruder利用文本概念，通过公开的CLIP模型和数据集生成通用的对抗扰动，无需查询目标模型或依赖替代模型。

Result: 实验表明，UnivIntruder在ImageNet上的攻击成功率达到85%，在CIFAR-10上超过99%，显著优于现有方法。此外，它还能成功攻击Google、Baidu等图像搜索引擎以及GPT-4、Claude-3.5等视觉语言模型。

Conclusion: UnivIntruder展示了在传统攻击途径被阻断时的实用性，突显了重新评估AI应用安全范式的必要性。

Abstract: Deep Neural Networks (DNNs) have achieved widespread success yet remain prone
to adversarial attacks. Typically, such attacks either involve frequent queries
to the target model or rely on surrogate models closely mirroring the target
model -- often trained with subsets of the target model's training data -- to
achieve high attack success rates through transferability. However, in
realistic scenarios where training data is inaccessible and excessive queries
can raise alarms, crafting adversarial examples becomes more challenging. In
this paper, we present UnivIntruder, a novel attack framework that relies
solely on a single, publicly available CLIP model and publicly available
datasets. By using textual concepts, UnivIntruder generates universal,
transferable, and targeted adversarial perturbations that mislead DNNs into
misclassifying inputs into adversary-specified classes defined by textual
concepts.
  Our extensive experiments show that our approach achieves an Attack Success
Rate (ASR) of up to 85% on ImageNet and over 99% on CIFAR-10, significantly
outperforming existing transfer-based methods. Additionally, we reveal
real-world vulnerabilities, showing that even without querying target models,
UnivIntruder compromises image search engines like Google and Baidu with ASR
rates up to 84%, and vision language models like GPT-4 and Claude-3.5 with ASR
rates up to 80%. These findings underscore the practicality of our attack in
scenarios where traditional avenues are blocked, highlighting the need to
reevaluate security paradigms in AI applications.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [829] [Data Mining-Based Techniques for Software Fault Localization](https://arxiv.org/abs/2505.18216)
*Peggy Cellier,Mireille Ducassé,Sébastien Ferré,Olivier Ridoux,W. Eric Wong*

Main category: cs.SE

TL;DR: 本章介绍使用数据挖掘技术进行故障定位的基本概念，以Trityp程序为例，探讨了形式概念分析和关联规则在多重故障和GUI组件中的应用。


<details>
  <summary>Details</summary>
Motivation: 探讨如何利用数据挖掘技术，特别是形式概念分析和关联规则，来提高软件故障定位的效率和准确性，特别是在多重故障和GUI组件测试的场景中。

Method: 采用Trityp程序作为示例，利用形式概念分析和关联规则这两种符号数据挖掘方法，分析测试用例的PASS和FAIL属性，扩展至多重故障和GUI组件的事件序列测试。

Result: 展示了数据挖掘技术在故障定位中的有效性，特别是在处理多重故障和GUI组件测试时，能够提供更精确的故障定位。

Conclusion: 数据挖掘技术，尤其是形式概念分析和关联规则，为软件故障定位提供了新的视角和方法，特别是在复杂场景如多重故障和GUI测试中表现出良好的应用潜力。

Abstract: This chapter illustrates the basic concepts of fault localization using a
data mining technique. It utilizes the Trityp program to illustrate the general
method. Formal concept analysis and association rule are two well-known methods
for symbolic data mining. In their original inception, they both consider data
in the form of an object-attribute table. In their original inception, they
both consider data in the form of an object-attribute table. The chapter
considers a debugging process in which a program is tested against different
test cases. Two attributes, PASS and FAIL, represent the issue of the test
case. The chapter extends the analysis of data mining for fault localization
for the multiple fault situations. It addresses how data mining can be further
applied to fault localization for GUI components. Unlike traditional software,
GUI test cases are usually event sequences, and each individual event has a
unique corresponding event handler.

</details>


### [830] [SEW: Self-Evolving Agentic Workflows for Automated Code Generation](https://arxiv.org/abs/2505.18646)
*Siwei Liu,Jinyuan Fang,Han Zhou,Yingxu Wang,Zaiqiao Meng*

Main category: cs.SE

TL;DR: 提出SEW框架，通过自进化自动优化多智能体工作流，提升代码生成任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖手工设计多智能体工作流，无法自动适应不同类型编码问题。

Method: 提出自进化框架SEW，自动生成和优化多智能体工作流。

Result: 在LiveCodeBench等数据集上带来最高33%的性能提升。

Conclusion: SEW能自动设计并优化工作流，为编码任务提供更优解决方案。

Abstract: Large Language Models (LLMs) have demonstrated effectiveness in code
generation tasks. To enable LLMs to address more complex coding challenges,
existing research has focused on crafting multi-agent systems with agentic
workflows, where complex coding tasks are decomposed into sub-tasks, assigned
to specialized agents. Despite their effectiveness, current approaches heavily
rely on hand-crafted agentic workflows, with both agent topologies and prompts
manually designed, which limits their ability to automatically adapt to
different types of coding problems. To address these limitations and enable
automated workflow design, we propose \textbf{S}elf-\textbf{E}volving
\textbf{W}orkflow (\textbf{SEW}), a novel self-evolving framework that
automatically generates and optimises multi-agent workflows. Extensive
experiments on three coding benchmark datasets, including the challenging
LiveCodeBench, demonstrate that our SEW can automatically design agentic
workflows and optimise them through self-evolution, bringing up to 33\%
improvement on LiveCodeBench compared to using the backbone LLM only.
Furthermore, by investigating different representation schemes of workflow, we
provide insights into the optimal way to encode workflow information with text.

</details>


### [831] [From Output to Evaluation: Does Raw Instruction-Tuned Code LLMs Output Suffice for Fill-in-the-Middle Code Generation?](https://arxiv.org/abs/2505.18789)
*Wasi Uddin Ahmad,Somshubra Majumdar,Boris Ginsburg*

Main category: cs.SE

TL;DR: 后处理对LLM在FIM代码生成中的自动评估至关重要，监督微调显著提升性能，但随机代码片段仍需后处理。


<details>
  <summary>Details</summary>
Motivation: 研究后处理在指令调优LLM输出中的必要性，因原始输出常含无关代码，需截断以有效评估。

Method: 监督微调LLM（Qwen2.5-Coder），在HumanEval Infilling和SAFIM基准上评估，分析后处理需求。

Result: 微调后LLM生成代码能无缝融入上下文，完整行中段无需后处理，但随机代码片段仍需后处理。

Conclusion: 监督微调提升FIM代码生成质量，但后处理在随机代码片段场景仍不可或缺。

Abstract: Post-processing is crucial for the automatic evaluation of LLMs in
fill-in-the-middle (FIM) code generation due to the frequent presence of
extraneous code in raw outputs. This extraneous generation suggests a lack of
awareness regarding output boundaries, requiring truncation for effective
evaluation. The determination of an optimal truncation strategy, however, often
proves intricate, particularly when the scope includes several programming
languages. This study investigates the necessity of post-processing
instruction-tuned LLM outputs. Our findings reveal that supervised fine-tuning
significantly enhances FIM code generation, enabling LLMs to generate code that
seamlessly integrates with the surrounding context. Evaluating our fine-tuned
\texttt{Qwen2.5-Coder} (base and instruct) models on HumanEval Infilling and
SAFIM benchmarks demonstrates improved performances without post-processing,
especially when the \emph{middle} consist of complete lines. However,
post-processing of the LLM outputs remains necessary when the \emph{middle} is
a random span of code.

</details>


### [832] [Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications of Agentic AI](https://arxiv.org/abs/2505.19443)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee*

Main category: cs.SE

TL;DR: 本文综述了AI辅助软件开发中的两种新兴范式：氛围编码（vibe coding）和代理编码（agentic coding），分析其差异、应用场景及混合架构趋势，提出未来以人为中心的统一开发路线。


<details>
  <summary>Details</summary>
Motivation: 探讨如何利用大语言模型（LLMs）在软件开发中实现人机协作的创新模式，解决传统开发中效率与创造力的平衡问题。

Method: 通过建立分类体系（包括概念基础、执行模型等）、比较工作流分析及20个用例研究，对比两种范式的特性与应用场景。

Result: 氛围编码适合早期原型设计/教育，代理编码擅长企业级自动化；混合架构（自然语言接口+自主执行）是未来趋势。

Conclusion: 成功的AI软件工程需融合两种范式优势，构建可信、可解释、协作的人本开发生命周期。

Abstract: This review presents a comprehensive analysis of two emerging paradigms in
AI-assisted software development: vibe coding and agentic coding. While both
leverage large language models (LLMs), they differ fundamentally in autonomy,
architectural design, and the role of the developer. Vibe coding emphasizes
intuitive, human-in-the-loop interaction through prompt-based, conversational
workflows that support ideation, experimentation, and creative exploration. In
contrast, agentic coding enables autonomous software development through
goal-driven agents capable of planning, executing, testing, and iterating tasks
with minimal human intervention. We propose a detailed taxonomy spanning
conceptual foundations, execution models, feedback loops, safety mechanisms,
debugging strategies, and real-world tool ecosystems. Through comparative
workflow analysis and 20 detailed use cases, we illustrate how vibe systems
thrive in early-stage prototyping and education, while agentic systems excel in
enterprise-grade automation, codebase refactoring, and CI/CD integration. We
further examine emerging trends in hybrid architectures, where natural language
interfaces are coupled with autonomous execution pipelines. Finally, we
articulate a future roadmap for agentic AI, outlining the infrastructure needed
for trustworthy, explainable, and collaborative systems. Our findings suggest
that successful AI software engineering will rely not on choosing one paradigm,
but on harmonizing their strengths within a unified, human-centered development
lifecycle.

</details>


### [833] [An Initial Exploration of Fine-tuning Small Language Models for Smart Contract Reentrancy Vulnerability Detection](https://arxiv.org/abs/2505.19059)
*Ignacio Mariano Andreozzi Pofcher,Joshua Ellul*

Main category: cs.SE

TL;DR: 本文探讨了小型语言模型在特定领域（Solidity智能合约中的重入漏洞检测）的微调效果，并与大型语言模型的应用场景进行对比。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在多种编码任务中表现出色，但对于某些特定任务（如漏洞检测），小型语言模型可能更适用，因其能在开发者本地计算机上高效运行和训练。

Method: 研究通过微调小型语言模型，专注于检测Solidity智能合约中的重入漏洞，评估其在该领域的表现。

Result: 研究发现，经过微调的小型语言模型在重入漏洞检测任务中能够取得合理的结果。

Conclusion: 小型语言模型在特定领域（如漏洞检测）中具有潜力，可作为大型语言模型的补充或替代方案。

Abstract: Large Language Models (LLMs) are being used more and more for various coding
tasks, including to help coders identify bugs and are a promising avenue to
support coders in various tasks including vulnerability detection --
particularly given the flexibility of such generative AI models and tools. Yet
for many tasks it may not be suitable to use LLMs, for which it may be more
suitable to use smaller language models that can fit and easily execute and
train on a developer's computer. In this paper we explore and evaluate whether
smaller language models can be fine-tuned to achieve reasonable results for a
niche area: vulnerability detection -- specifically focusing on detecting the
reentrancy bug in Solidity smart contracts.

</details>


### [834] [CIDRe: A Reference-Free Multi-Aspect Criterion for Code Comment Quality Measurement](https://arxiv.org/abs/2505.19757)
*Maria Dziuba,Valentin Malykh*

Main category: cs.SE

TL;DR: 提出CIDRe，一种语言无关、无参考的代码注释质量评估标准，结合四个协同方面，优于现有方法，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如SIDE、MIDQ、STASIS）在代码-注释分析方面存在局限，需要更鲁棒的注释质量评估标准来优化数据集构建。

Method: 提出CIDRe标准，结合四个协同方面：相关性（代码-注释语义对齐）、信息量（功能覆盖）、完整性（结构部分完整性）和描述长度（细节充分性）。

Result: 实验证明CIDRe优于现有指标，在交叉熵评估中表现更好，且基于CIDRe筛选数据微调的模型在GPT-4o-mini评估中质量显著提升。

Conclusion: CIDRe是一种有效的代码注释质量评估标准，能够显著提升注释生成模型的质量。

Abstract: Effective generation of structured code comments requires robust quality
metrics for dataset curation, yet existing approaches (SIDE, MIDQ, STASIS)
suffer from limited code-comment analysis. We propose CIDRe, a
language-agnostic reference-free quality criterion combining four synergistic
aspects: (1) relevance (code-comment semantic alignment), (2) informativeness
(functional coverage), (3) completeness (presence of all structure sections),
and (4) description length (detail sufficiency). We validate our criterion on a
manually annotated dataset. Experiments demonstrate CIDRe's superiority over
existing metrics, achieving improvement in cross-entropy evaluation. When
applied to filter comments, the models finetuned on CIDRe-filtered data show
statistically significant quality gains in GPT-4o-mini assessments.

</details>


### [835] [StructEval: Benchmarking LLMs' Capabilities to Generate Structural Outputs](https://arxiv.org/abs/2505.20139)
*Jialin Yang,Dongfu Jiang,Lipeng He,Sherman Siu,Yuxuan Zhang,Disen Liao,Zhuofeng Li,Huaye Zeng,Yiming Jia,Haozhe Wang,Benjamin Schneider,Chi Ruan,Wentao Ma,Zhiheng Lyu,Yifei Wang,Yi Lu,Quy Duc Do,Ziyan Jiang,Ping Nie,Wenhu Chen*

Main category: cs.SE

TL;DR: StructEval是一个评估大语言模型生成结构化输出能力的基准，涵盖18种格式和44种任务，发现现有模型在生成任务和视觉内容上表现较差。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件开发中的广泛应用，评估其生成结构化输出的能力变得至关重要。

Method: 通过生成任务和转换任务，系统评估模型在多种结构化格式中的表现，并引入新的度量标准。

Result: 即使是先进模型如o1-mini平均得分仅为75.58，开源模型表现更差，生成任务比转换任务更具挑战性。

Conclusion: StructEval揭示了现有模型在生成结构化输出方面的局限性，尤其是在视觉内容生成上。

Abstract: As Large Language Models (LLMs) become integral to software development
workflows, their ability to generate structured outputs has become critically
important. We introduce StructEval, a comprehensive benchmark for evaluating
LLMs' capabilities in producing both non-renderable (JSON, YAML, CSV) and
renderable (HTML, React, SVG) structured formats. Unlike prior benchmarks,
StructEval systematically evaluates structural fidelity across diverse formats
through two paradigms: 1) generation tasks, producing structured output from
natural language prompts, and 2) conversion tasks, translating between
structured formats. Our benchmark encompasses 18 formats and 44 types of task,
with novel metrics for format adherence and structural correctness. Results
reveal significant performance gaps, even state-of-the-art models like o1-mini
achieve only 75.58 average score, with open-source alternatives lagging
approximately 10 points behind. We find generation tasks more challenging than
conversion tasks, and producing correct visual content more difficult than
generating text-only structures.

</details>


### [836] [Retrieval-Augmented Generation for Service Discovery: Chunking Strategies and Benchmarking](https://arxiv.org/abs/2505.19310)
*Robin D. Pesl,Jerin G. Mathew,Massimo Mecella,Marco Aiello*

Main category: cs.SE

TL;DR: 论文探讨了如何利用检索增强生成(RAG)和发现代理优化API端点发现与预处理，以减少输入令牌长度并提升集成效率。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中集成信息系统时，传统API注册表方法难以处理未存在服务的集成问题。大型语言模型虽能自动生成系统集成，但受限于输入令牌长度，需优化API描述的预处理方式。

Method: 提出：(i) 分析RAG在端点发现和OpenAPI分块预处理中的应用；(ii) 引入发现代理，仅接收关键端点摘要并按需检索细节；(iii) 使用新基准SOCBench-D和真实数据集RestBench评估端点检索精度。

Result: 实验表明，基于端点的预处理方法优于简单分块；发现代理显著提高精确率但可能降低召回率，揭示需进一步推理能力。

Conclusion: RAG结合发现代理能有效减少令牌数并提升端点发现效率，但需权衡精确率与召回率，未来需增强推理能力。

Abstract: Integrating multiple (sub-)systems is essential to create advanced
Information Systems. Difficulties mainly arise when integrating dynamic
environments, e.g., the integration at design time of not yet existing
services. This has been traditionally addressed using a registry that provides
the API documentation of the endpoints. Large Language Models have shown to be
capable of automatically creating system integrations (e.g., as service
composition) based on this documentation but require concise input due to input
oken limitations, especially regarding comprehensive API descriptions.
Currently, it is unknown how best to preprocess these API descriptions. In the
present work, we (i) analyze the usage of Retrieval Augmented Generation for
endpoint discovery and the chunking, i.e., preprocessing, of state-of-practice
OpenAPIs to reduce the input oken length while preserving the most relevant
information. To further reduce the input token length for the composition
prompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that
only receives a summary of the most relevant endpoints nd retrieves
specification details on demand. We evaluate RAG for endpoint discovery using
(iii) a proposed novel service discovery benchmark SOCBench-D representing a
general setting across numerous domains and the real-world RestBench enchmark,
first, for the different chunking possibilities and parameters measuring the
endpoint retrieval accuracy. Then, we assess the Discovery Agent using the same
test data set. The prototype shows how to successfully employ RAG for endpoint
discovery to reduce the token count. Our experiments show that endpoint-based
approaches outperform naive chunking methods for preprocessing. Relying on an
agent significantly improves precision while being prone to decrease recall,
disclosing the need for further reasoning capabilities.

</details>


### [837] [CODE-DITING: A Reasoning-Based Metric for Functional Alignment in Code Evaluation](https://arxiv.org/abs/2505.19502)
*Guang Yang,Yu Zhou,Xiang Chen,Wei Zheng,Xing Hu,Xin Zhou,David Lo,Taolue Chen*

Main category: cs.SE

TL;DR: 论文提出CODE-DITING方法，通过数据蒸馏平衡代码评估的准确性、效率和可解释性，显著降低计算成本并超越大模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统代码评估方法依赖参考解决方案或可执行测试用例，灵活性差且扩展性不足。LLM-as-Judge方法虽提供新思路，但仍存在提示复杂、可解释性差或计算资源需求高的问题。

Method: 提出CODE-DITING方法，通过数据蒸馏框架将DeepSeek-R1671B的推理能力迁移至更小模型（1.5B/7B），结合多数投票策略提升评估效果。

Result: CODE-DITING 1.5B超越同参数量级模型，7B版本仅用1%参数量即超越GPT-4o和DeepSeek-V3 671B，且对偏好泄漏具有鲁棒性。

Conclusion: CODE-DITING在代码评估中实现了准确性、效率与可解释性的平衡，可作为现有大模型的轻量级替代方案。

Abstract: Trustworthy evaluation methods for code snippets play a crucial role in
neural code generation. Traditional methods, which either rely on reference
solutions or require executable test cases, have inherent limitation in
flexibility and scalability. The recent LLM-as-Judge methodology offers a
promising alternative by directly evaluating functional consistency between the
problem description and the generated code. To systematically understand the
landscape of these LLM-as-Judge methods, we conduct a comprehensive empirical
study across three diverse datasets. Our investigation reveals the pros and
cons of two categories of LLM-as-Judge methods: the methods based on general
foundation models can achieve good performance but require complex prompts and
lack explainability, while the methods based on reasoning foundation models
provide better explainability with simpler prompts but demand substantial
computational resources due to their large parameter sizes. To address these
limitations, we propose CODE-DITING, a novel code evaluation method that
balances accuracy, efficiency and explainability. We develop a data
distillation framework that effectively transfers reasoning capabilities from
DeepSeek-R1671B to our CODE-DITING 1.5B and 7B models, significantly enhancing
evaluation explainability and reducing the computational cost. With the
majority vote strategy in the inference process, CODE-DITING 1.5B outperforms
all models with the same magnitude of parameters and achieves performance which
would normally exhibit in a model with 5 times of parameter scale. CODE-DITING
7B surpasses GPT-4o and DeepSeek-V3 671B, even though it only uses 1% of the
parameter volume of these large models. Further experiments show that
CODEDITING is robust to preference leakage and can serve as a promising
alternative for code evaluation.

</details>


### [838] [Search-Based Software Engineering in the Landscape of AI Foundation Models](https://arxiv.org/abs/2505.19625)
*Hassan Sartaj,Shaukat Ali*

Main category: cs.SE

TL;DR: 该论文探讨了基于搜索的软件工程（SBSE）与基础模型（FMs）结合的研究路线图，提出了当前挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能（AI）尤其是基础模型（FMs）的快速发展，SBSE领域如何与FMs结合并进一步发展尚不明确。论文旨在填补这一空白。

Method: 论文提出一个研究路线图，分析SBSE与FMs的当前关系，并指出开放挑战和潜在研究方向。

Result: 论文为SBSE在FMs时代的发展提供了前瞻性和创新性的视角，明确了未来研究方向。

Conclusion: 通过结合FMs，SBSE有望在软件工程生命周期中实现更多突破，但仍需解决一系列开放挑战。

Abstract: Search-based software engineering (SBSE), at the intersection of artificial
intelligence (AI) and software engineering, has been an active area of research
for about 25 years. It has been applied to solve numerous problems across the
entire software engineering lifecycle and has demonstrated its versatility in
multiple domains. With the recent advancements in AI, particularly the
emergence of foundation models (FMs), the evolution of SBSE alongside FMs
remains undetermined. In this window of opportunity, we propose a research
roadmap that articulates the current landscape of SBSE in relation to
foundation models (FMs), highlights open challenges, and outlines potential
research directions for advancing SBSE through its interplay with FMs. This
roadmap aims to establish a forward-thinking and innovative perspective for the
future of SBSE in the era of FMs.

</details>


### [839] [Large Language Models in Code Co-generation for Safe Autonomous Vehicles](https://arxiv.org/abs/2505.19658)
*Ali Nouri,Beatriz Cabrero-Daniel,Zhennan Fei,Krishna Ronanki,Håkan Sivencrona,Christian Berger*

Main category: cs.SE

TL;DR: 论文提出了一种评估LLM生成代码的流程，比较了六种先进LLM在安全相关编程任务中的表现，并分析了常见错误模式。


<details>
  <summary>Details</summary>
Motivation: 由于LLM的随机性可能对安全相关系统开发带来风险，需要系统评估其在汽车ADAS或AD系统中的应用。

Method: 设计评估流程对生成代码进行完整性检查，比较六种LLM在四个安全任务中的表现，并建立错误模式目录。

Result: 六种LLM的性能对比结果及常见错误模式分析，为代码审查人员提供支持。

Conclusion: 讨论了LLM在代码生成中的局限性和能力，以及评估流程在现有流程中的应用价值。

Abstract: Software engineers in various industrial domains are already using Large
Language Models (LLMs) to accelerate the process of implementing parts of
software systems. When considering its potential use for ADAS or AD systems in
the automotive context, there is a need to systematically assess this new
setup: LLMs entail a well-documented set of risks for safety-related systems'
development due to their stochastic nature. To reduce the effort for code
reviewers to evaluate LLM-generated code, we propose an evaluation pipeline to
conduct sanity-checks on the generated code. We compare the performance of six
state-of-the-art LLMs (CodeLlama, CodeGemma, DeepSeek-r1, DeepSeek-Coders,
Mistral, and GPT-4) on four safety-related programming tasks. Additionally, we
qualitatively analyse the most frequent faults generated by these LLMs,
creating a failure-mode catalogue to support human reviewers. Finally, the
limitations and capabilities of LLMs in code generation, and the use of the
proposed pipeline in the existing process, are discussed.

</details>


### [840] [Deconstructing Obfuscation: A four-dimensional framework for evaluating Large Language Models assembly code deobfuscation capabilities](https://arxiv.org/abs/2505.19887)
*Anton Tkachenko,Dmitrij Suskevic,Benjamin Adolphi*

Main category: cs.SE

TL;DR: 该论文首次全面评估了商用大语言模型在汇编代码反混淆中的表现，发现性能差异显著，并提出了解释这些差异的理论框架和错误模式。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在软件工程中表现出潜力，但其在二进制分析（尤其是汇编代码反混淆）中的有效性尚未被探索。

Method: 研究测试了七种先进的大语言模型在四种混淆场景下的表现，并提出了基于四个维度的理论框架（推理深度、模式识别、噪声过滤和上下文整合）来分析性能差异。

Result: 研究发现性能差异显著，从完全自主反混淆到完全失败不等，并识别了五种错误模式。同时，建立了三级抗性模型，表明复杂混淆技术仍能有效对抗先进的大语言模型。

Conclusion: 研究建议采用人机协作模式，大语言模型可降低某些逆向工程任务的专业门槛，但在复杂反混淆中仍需人类指导。该工作为评估新兴能力和开发抗性混淆技术奠定了基础。

Abstract: Large language models (LLMs) have shown promise in software engineering, yet
their effectiveness for binary analysis remains unexplored. We present the
first comprehensive evaluation of commercial LLMs for assembly code
deobfuscation. Testing seven state-of-the-art models against four obfuscation
scenarios (bogus control flow, instruction substitution, control flow
flattening, and their combination), we found striking performance
variations--from autonomous deobfuscation to complete failure. We propose a
theoretical framework based on four dimensions: Reasoning Depth, Pattern
Recognition, Noise Filtering, and Context Integration, explaining these
variations. Our analysis identifies five error patterns: predicate
misinterpretation, structural mapping errors, control flow misinterpretation,
arithmetic transformation errors, and constant propagation errors, revealing
fundamental limitations in LLM code processing.We establish a three-tier
resistance model: bogus control flow (low resistance), control flow flattening
(moderate resistance), and instruction substitution/combined techniques (high
resistance). Universal failure against combined techniques demonstrates that
sophisticated obfuscation remains effective against advanced LLMs. Our findings
suggest a human-AI collaboration paradigm where LLMs reduce expertise barriers
for certain reverse engineering tasks while requiring human guidance for
complex deobfuscation. This work provides a foundation for evaluating emerging
capabilities and developing resistant obfuscation techniques.x deobfuscation.
This work provides a foundation for evaluating emerging capabilities and
developing resistant obfuscation techniques.

</details>


### [841] [Evaluating Large Language Models for Code Review](https://arxiv.org/abs/2505.20206)
*Umut Cihan,Arda İçöz,Vahid Haratian,Eray Tüzün*

Main category: cs.SE

TL;DR: 研究比较了GPT4o和Gemini 2.0 Flash在代码审查中的表现，发现它们在检测代码正确性和改进建议方面有一定效果，但存在错误输出的风险，建议结合人类参与的审查流程。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的发展，大型语言模型（LLMs）被用于代码审查和修复，但其可靠性和准确性尚未系统评估。本研究旨在比较不同LLMs在代码审查任务中的表现。

Method: 研究测试了GPT4o和Gemini 2.0 Flash在492个AI生成代码块和164个HumanEval基准代码块上的表现，模拟代码审查任务，评估其检测代码正确性和改进代码的能力。

Result: 在有问题描述的情况下，GPT4o和Gemini 2.0 Flash正确分类代码正确性的比例分别为68.50%和63.89%，改进代码的比例分别为67.83%和54.26%。无问题描述时性能下降，且不同类型代码表现不同。

Conclusion: LLM代码审查有助于改进建议和正确性评估，但存在错误输出风险。研究提出结合人类参与的“Human in the loop LLM Code Review”流程，以促进知识共享并降低风险。

Abstract: Context: Code reviews are crucial for software quality. Recent AI advances
have allowed large language models (LLMs) to review and fix code; now, there
are tools that perform these reviews. However, their reliability and accuracy
have not yet been systematically evaluated. Objective: This study compares
different LLMs' performance in detecting code correctness and suggesting
improvements. Method: We tested GPT4o and Gemini 2.0 Flash on 492 AI generated
code blocks of varying correctness, along with 164 canonical code blocks from
the HumanEval benchmark. To simulate the code review task objectively, we
expected LLMs to assess code correctness and improve the code if needed. We ran
experiments with different configurations and reported on the results. Results:
With problem descriptions, GPT4o and Gemini 2.0 Flash correctly classified code
correctness 68.50% and 63.89% of the time, respectively, and corrected the code
67.83% and 54.26% of the time for the 492 code blocks of varying correctness.
Without problem descriptions, performance declined. The results for the 164
canonical code blocks differed, suggesting that performance depends on the type
of code. Conclusion: LLM code reviews can help suggest improvements and assess
correctness, but there is a risk of faulty outputs. We propose a process that
involves humans, called the "Human in the loop LLM Code Review" to promote
knowledge sharing while mitigating the risk of faulty outputs.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [842] [A Matrix Product State Model for Simultaneous Classification and Generation](https://arxiv.org/abs/2406.17441)
*Alex Mossi,Bojan Žunkovic,Kyriakos Flouris*

Main category: quant-ph

TL;DR: 该论文提出了一种基于矩阵乘积状态（MPS）的量子机器学习模型，兼具分类和生成功能，通过对抗训练减少异常值，并探讨了张量网络在生成任务中的机制。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习（QML）结合了量子计算和机器学习的优势，张量网络作为其重要数学框架，最初用于模拟量子系统，现被引入经典机器学习。论文旨在探索MPS在监督学习中的潜力，并改进生成样本的质量。

Method: 论文采用矩阵乘积状态（MPS）模型，将其作为分类器和生成器双重功能模块，结合生成对抗网络的策略优化训练过程，并提出新的嵌入函数和非归一化MPS采样方法。

Result: 提出的MPS模型在监督学习中表现出色，能够高效处理高维数据，并通过对抗训练生成更真实的样本，同时提供了对张量网络生成任务机制的新见解。

Conclusion: 该研究展示了张量网络在经典机器学习中的有效性，特别是MPS模型的双重功能为监督学习提供了新思路，同时为生成任务的技术改进奠定了基础。

Abstract: Quantum machine learning (QML) is a rapidly expanding field that merges the
principles of quantum computing with the techniques of machine learning. One of
the powerful mathematical frameworks in this domain is tensor networks. These
networks are used to approximate high-order tensors by contracting tensors with
lower ranks. Initially developed for simulating quantum systems, tensor
networks have become integral to quantum computing and, by extension, to QML.
Drawing inspiration from these quantum methods, specifically the Matrix Product
States (MPS), we apply them in a classical machine learning setting. Their
ability to efficiently represent and manipulate complex, high-dimensional data
makes them effective in a supervised learning framework. Here, we present an
MPS model, in which the MPS functions as both a classifier and a generator. The
dual functionality of this novel MPS model permits a strategy that enhances the
traditional training of supervised MPS models. This framework is inspired by
generative adversarial networks and is geared towards generating more realistic
samples by reducing outliers. In addition, our contributions offer insights
into the mechanics of tensor network methods for generation tasks.
Specifically, we discuss alternative embedding functions and a new sampling
method from non-normalized MPSs.

</details>


### [843] [Towards a Quantum-classical Augmented Network](https://arxiv.org/abs/2505.18282)
*Nitin Jha,Abhishek Parakh,Mahadevan Subramaniam*

Main category: quant-ph

TL;DR: 该论文提出改进HTTP协议以同时承载经典和量子负载，通过机器学习分类隐私标签，优化量子资源利用，推动安全量子网络发展。


<details>
  <summary>Details</summary>
Motivation: 当前大规模量子网络部署受限于量子中继器等技术瓶颈，论文旨在通过增强现有经典网络的方式，结合可行量子技术提升安全性。

Method: 修改HTTP协议结构以支持混合负载，并采用逻辑回归、CNN、LSTM和BiLSTM模型对通信隐私标签进行分类。

Result: 实验验证了所提方法能有效减少量子资源消耗，为高效安全的量子网络设计奠定基础。

Conclusion: 通过协议改造和机器学习分类，实现了经典网络与量子技术的初步融合，为未来量子网络演进提供了可行路径。

Abstract: In the past decade, several small-scale quantum key distribution networks
have been established. However, the deployment of large-scale quantum networks
depends on the development of quantum repeaters, quantum channels, quantum
memories, and quantum network protocols. To improve the security of existing
networks and adopt currently feasible quantum technologies, the next step is to
augment classical networks with quantum devices, properties, and phenomena. To
achieve this, we propose a change in the structure of the HTTP protocol such
that it can carry both quantum and classical payload. This work lays the
foundation for dividing one single network packet into classical and quantum
payloads depending on the privacy needs. We implement logistic regression, CNN,
LSTM, and BiLSTM models to classify the privacy label for outgoing
communications. This enables reduced utilization of quantum resources allowing
for a more efficient secure quantum network design. Experimental results using
the proposed methods are presented.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [844] [A deep solver for backward stochastic Volterra integral equations](https://arxiv.org/abs/2505.18297)
*Kristoffer Andersson,Alessandro Gnoatto,Camilo Andrés García Trillos*

Main category: math.NA

TL;DR: 本文提出了首个用于解决后向随机Volterra积分方程（BSVIEs）及其全耦合前后向变体的深度学习求解器，通过单阶段训练神经网络避免了传统算法的嵌套时间步限制，展示了方法的可扩展性和通用性。


<details>
  <summary>Details</summary>
Motivation: 传统的后向随机Volterra积分方程及其变体的求解方法存在计算复杂度高、难以处理高维问题的限制。本文旨在通过深度学习技术，提出一种更高效、可扩展的求解方法。

Method: 该方法通过训练神经网络来近似两个解场，避免了传统算法中的嵌套时间步循环。对于解耦情况，证明了非渐近误差界由后验残差和时间步的平方根依赖组成。

Result: 数值实验验证了方法的收敛速率，并展示了其可扩展性（在500维空间变量下仍保持稳定精度）和通用性（同一方法可处理耦合系统）。

Conclusion: 该方法为高维、路径依赖的随机控制和量化金融问题提供了实用的解决方案，具有广泛的应用潜力。

Abstract: We present the first deep-learning solver for backward stochastic Volterra
integral equations (BSVIEs) and their fully-coupled forward-backward variants.
The method trains a neural network to approximate the two solution fields in a
single stage, avoiding the use of nested time-stepping cycles that limit
classical algorithms. For the decoupled case we prove a non-asymptotic error
bound composed of an a posteriori residual plus the familiar square root
dependence on the time step. Numerical experiments confirm this rate and reveal
two key properties: \emph{scalability}, in the sense that accuracy remains
stable from low dimension up to 500 spatial variables while GPU batching keeps
wall-clock time nearly constant; and \emph{generality}, since the same method
handles coupled systems whose forward dynamics depend on the backward solution.
These results open practical access to a family of high-dimensional,
path-dependent problems in stochastic control and quantitative finance.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [845] [How We Won the ISLES'24 Challenge by Preprocessing](https://arxiv.org/abs/2505.18424)
*Tianyi Ren,Juampablo E. Heras Rivera,Hitender Oswal,Yutong Pan,William Henry,Jacob Ruzevick,Mehmet Kurt*

Main category: eess.IV

TL;DR: ISLES'24挑战赛提出了一种基于深度学习的脑卒中病灶分割方法，通过优化的预处理流程和标准大型残差nnU-Net架构，实现了仅使用CT输入预测MRI随访病灶的准确分割。


<details>
  <summary>Details</summary>
Motivation: 脑卒中是全球三大死因之一，准确识别病灶边界对诊断和治疗至关重要。现有监督深度学习方法需要大量标注数据，ISLES'24挑战赛旨在通过提供纵向影像数据解决这一问题。

Method: 采用深度学习为基础的颅骨剥离和自定义强度窗预处理流程，结合标准大型残差nnU-Net架构进行分割。

Result: 该方法在测试集上取得了平均Dice系数28.5（标准差21.27）的分割性能。

Conclusion: 精心设计的预处理流程与标准分割架构相结合，能有效提升仅基于CT输入的脑卒中病灶分割准确性。

Abstract: Stroke is among the top three causes of death worldwide, and accurate
identification of stroke lesion boundaries is critical for diagnosis and
treatment. Supervised deep learning methods have emerged as the leading
solution for stroke lesion segmentation but require large, diverse, and
annotated datasets. The ISLES'24 challenge addresses this need by providing
longitudinal stroke imaging data, including CT scans taken on arrival to the
hospital and follow-up MRI taken 2-9 days from initial arrival, with
annotations derived from follow-up MRI. Importantly, models submitted to the
ISLES'24 challenge are evaluated using only CT inputs, requiring prediction of
lesion progression that may not be visible in CT scans for segmentation. Our
winning solution shows that a carefully designed preprocessing pipeline
including deep-learning-based skull stripping and custom intensity windowing is
beneficial for accurate segmentation. Combined with a standard large residual
nnU-Net architecture for segmentation, this approach achieves a mean test Dice
of 28.5 with a standard deviation of 21.27.

</details>


### [846] [Mind Your Vision: Multimodal Estimation of Refractive Disorders Using Electrooculography and Eye Tracking](https://arxiv.org/abs/2505.18538)
*Xin Wei,Huakun Liu,Yutaro Hirao,Monica Perusquia-Hernandez,Katsutoshi Masai,Hideaki Uchiyama,Kiyoshi Kiyokawa*

Main category: eess.IV

TL;DR: 该研究探索了通过眼电图（EOG）和视频眼动追踪技术被动估算屈光度的新方法，发现多模态模型在个体依赖场景下表现最佳，但在跨个体泛化能力上仍有局限。


<details>
  <summary>Details</summary>
Motivation: 当前屈光不正诊断依赖用户主动配合和临床监督，研究旨在开发一种基于眼动数据的无创、连续性筛查方法。

Method: 使用公开数据集，在多种屈光度条件下训练LSTM模型，分别评估单模态（EOG或眼动追踪）和多模态配置在个体依赖/独立场景下的性能。

Result: 多模态模型在个体依赖场景平均准确率达96.207%，显著优于单模态；但跨个体准确率仅8.882%，略高于随机水平。

Conclusion: 眼动数据估算屈光度具有潜力，但当前跨个体泛化能力有限，需进一步改进以实现实用化筛查。

Abstract: Refractive errors are among the most common visual impairments globally, yet
their diagnosis often relies on active user participation and clinical
oversight. This study explores a passive method for estimating refractive power
using two eye movement recording techniques: electrooculography (EOG) and
video-based eye tracking. Using a publicly available dataset recorded under
varying diopter conditions, we trained Long Short-Term Memory (LSTM) models to
classify refractive power from unimodal (EOG or eye tracking) and multimodal
configuration. We assess performance in both subject-dependent and
subject-independent settings to evaluate model personalization and
generalizability across individuals. Results show that the multimodal model
consistently outperforms unimodal models, achieving the highest average
accuracy in both settings: 96.207\% in the subject-dependent scenario and
8.882\% in the subject-independent scenario. However, generalization remains
limited, with classification accuracy only marginally above chance in the
subject-independent evaluations. Statistical comparisons in the
subject-dependent setting confirmed that the multimodal model significantly
outperformed the EOG and eye-tracking models. However, no statistically
significant differences were found in the subject-independent setting. Our
findings demonstrate both the potential and current limitations of eye movement
data-based refractive error estimation, contributing to the development of
continuous, non-invasive screening methods using EOG signals and eye-tracking
data.

</details>


### [847] [ReflectGAN: Modeling Vegetation Effects for Soil Carbon Estimation from Satellite Imagery](https://arxiv.org/abs/2505.18546)
*Dristi Datta,Manoranjan Paul,Manzur Murshed,Shyh Wei Teng,Leigh M. Schmidtke*

Main category: eess.IV

TL;DR: 提出ReflectGAN方法，通过生成对抗网络从植被覆盖的卫星图像中重建裸土反射率，显著提升植被区域土壤有机碳估算精度。


<details>
  <summary>Details</summary>
Motivation: 植被覆盖会干扰卫星光谱信号，导致土壤有机碳(SOC)估算不准确。现有方法在植被校正方面存在局限，需开发新方法提高估算可靠性。

Method: 开发ReflectGAN框架，学习植被覆盖与裸土反射率间的光谱转换关系。基于LUCAS 2018数据集和Landsat 8影像训练模型，并与现有方法对比。

Result: ReflectGAN重建的反射率使最佳模型(RF)的R²提升35%，RMSE降低43%。在Sentinel-2数据上也表现优于现有方法(PMM-SU)。

Conclusion: ReflectGAN能有效提高植被景观中SOC估算精度，为土壤监测提供更可靠支持。

Abstract: Soil organic carbon (SOC) is a critical indicator of soil health, but its
accurate estimation from satellite imagery is hindered in vegetated regions due
to spectral contamination from plant cover, which obscures soil reflectance and
reduces model reliability. This study proposes the Reflectance Transformation
Generative Adversarial Network (ReflectGAN), a novel paired GAN-based framework
designed to reconstruct accurate bare soil reflectance from vegetated soil
satellite observations. By learning the spectral transformation between
vegetated and bare soil reflectance, ReflectGAN facilitates more precise SOC
estimation under mixed land cover conditions. Using the LUCAS 2018 dataset and
corresponding Landsat 8 imagery, we trained multiple learning-based models on
both original and ReflectGAN-reconstructed reflectance inputs. Models trained
on ReflectGAN outputs consistently outperformed those using existing vegetation
correction methods. For example, the best-performing model (RF) achieved an
$R^2$ of 0.54, RMSE of 3.95, and RPD of 2.07 when applied to the
ReflectGAN-generated signals, representing a 35\% increase in $R^2$, a 43\%
reduction in RMSE, and a 43\% improvement in RPD compared to the best existing
method (PMM-SU). The performance of the models with ReflectGAN is also better
compared to their counterparts when applied to another dataset, i.e.,
Sentinel-2 imagery. These findings demonstrate the potential of ReflectGAN to
improve SOC estimation accuracy in vegetated landscapes, supporting more
reliable soil monitoring.

</details>


### [848] [Memory-Efficient Super-Resolution of 3D Micro-CT Images Using Octree-Based GANs: Enhancing Resolution and Segmentation Accuracy](https://arxiv.org/abs/2505.18664)
*Evgeny Ugolkov,Xupeng He,Hyung Kwak,Hussein Hoteit*

Main category: eess.IV

TL;DR: 提出了一种基于3D Octree结构的生成对抗网络，显著提升岩石微CT图像分割质量，实现16倍分辨率提升并解决内存瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 解决微CT图像分割中因矿物X射线衰减重叠导致的精度问题，以及传统3D卷积层内存消耗高的技术难题。

Method: 采用3D Octree卷积Wasserstein生成对抗网络（带梯度惩罚），在渐进式生长生成器中引入Octree结构实现内存高效计算。

Result: 分辨率从7微米/体素提升至0.44微米/体素，在Berea砂岩上验证了孔隙表征和矿物区分的显著改进。

Conclusion: 该框架突破了体素深度学习的内存限制，为地球科学成像提供了有效的超分辨率解决方案。

Abstract: We present a memory-efficient algorithm for significantly enhancing the
quality of segmented 3D micro-Computed Tomography (micro-CT) images of rocks
using a generative model. The proposed model achieves a 16x increase in
resolution and corrects inaccuracies in segmentation caused by the overlapping
X-ray attenuation in micro-CT measurements across different minerals. The
generative model employed is a 3D Octree-based convolutional Wasserstein
generative adversarial network with gradient penalty. To address the challenge
of high memory consumption inherent in standard 3D convolutional layers, we
implemented an Octree structure within the 3D progressive growing generator
model. This enabled the use of memory-efficient 3D Octree-based convolutional
layers. The approach is pivotal in overcoming the long-standing memory
bottleneck in volumetric deep learning, making it possible to reach 16x
super-resolution in 3D, a scale that is challenging to attain due to cubic
memory scaling. For training, we utilized segmented 3D low-resolution micro-CT
images along with unpaired segmented complementary 2D high-resolution laser
scanning microscope images. Post-training, resolution improved from 7 to 0.44
micro-m/voxel with accurate segmentation of constituent minerals. Validated on
Berea sandstone, this framework demonstrates substantial improvements in pore
characterization and mineral differentiation, offering a robust solution to one
of the primary computational limitations in modern geoscientific imaging.

</details>


### [849] [A physics-guided smoothing method for material modeling with digital image correlation (DIC) measurements](https://arxiv.org/abs/2505.18784)
*Jihong Wang,Chung-Hao Lee,William Richardson,Yue Yu*

Main category: eess.IV

TL;DR: 本文提出了一种新的DIC测量处理方法，通过优化算法和机器学习，提高了生物材料建模的准确性。


<details>
  <summary>Details</summary>
Motivation: 为了提高多轴拉伸协议DIC测量的处理精度，并建立物理一致的位移和应变场，以更准确地建模生物材料。

Method: 采用基于优化的移动最小二乘算法平滑节点位移，并结合数据驱动的工作流程估计非局部本构律和材料微观结构。

Result: 该方法成功应用于猪三尖瓣前叶的DIC测量，显著提高了生物材料建模的准确性。

Conclusion: 提出的DIC数据处理方法能够有效提升生物材料建模的精度，具有广泛的应用潜力。

Abstract: In this work, we present a novel approach to process the DIC measurements of
multiple biaxial stretching protocols. In particular, we develop a
optimization-based approach, which calculates the smoothed nodal displacements
using a moving least-squares algorithm subject to positive strain constraints.
As such, physically consistent displacement and strain fields are obtained.
Then, we further deploy a data-driven workflow to heterogeneous material
modeling from these physically consistent DIC measurements, by estimating a
nonlocal constitutive law together with the material microstructure. To
demonstrate the applicability of our approach, we apply it in learning a
material model and fiber orientation field from DIC measurements of a porcine
tricuspid valve anterior leaflet. Our results demonstrate that the proposed DIC
data processing approach can significantly improve the accuracy of modeling
biological materials.

</details>


### [850] [Advancements in Medical Image Classification through Fine-Tuning Natural Domain Foundation Models](https://arxiv.org/abs/2505.19779)
*Mobina Mansoori,Sajjad Shahabodini,Farnoush Bayatmakou,Jamshid Abouei,Konstantinos N. Plataniotis,Arash Mohammadi*

Main category: eess.IV

TL;DR: 该研究评估了多种先进基础模型（DINOv2、MAE等）在医学图像分类任务中的表现，发现这些模型能显著提升分类效果，尤其在数据有限时表现稳健。


<details>
  <summary>Details</summary>
Motivation: 研究旨在分析最新基础模型在医学领域的应用潜力，探索这些技术进步是否能推动医学图像分类的实质性改进。

Method: 通过微调DINOv2、MAE等模型，并在多个医学数据集（如CBIS-DDSM、ISIC2019等）上进行评估，比较不同配置的效果。

Result: AIMv2、DINOv2和SAM2模型表现最佳，证明自然领域训练的进步对医学领域分类效果有积极影响。

Conclusion: 先进基础模型能显著提升医学图像分类性能，尤其在数据有限时仍保持稳健表现，代码已开源。

Abstract: Using massive datasets, foundation models are large-scale, pre-trained models
that perform a wide range of tasks. These models have shown consistently
improved results with the introduction of new methods. It is crucial to analyze
how these trends impact the medical field and determine whether these
advancements can drive meaningful change. This study investigates the
application of recent state-of-the-art foundation models, DINOv2, MAE, VMamba,
CoCa, SAM2, and AIMv2, for medical image classification. We explore their
effectiveness on datasets including CBIS-DDSM for mammography, ISIC2019 for
skin lesions, APTOS2019 for diabetic retinopathy, and CHEXPERT for chest
radiographs. By fine-tuning these models and evaluating their configurations,
we aim to understand the potential of these advancements in medical image
classification. The results indicate that these advanced models significantly
enhance classification outcomes, demonstrating robust performance despite
limited labeled data. Based on our results, AIMv2, DINOv2, and SAM2 models
outperformed others, demonstrating that progress in natural domain training has
positively impacted the medical domain and improved classification outcomes.
Our code is publicly available at:
https://github.com/sajjad-sh33/Medical-Transfer-Learning.

</details>


### [851] [Improvement Strategies for Few-Shot Learning in OCT Image Classification of Rare Retinal Diseases](https://arxiv.org/abs/2505.20149)
*Cheng-Yu Tai,Ching-Wen Chen,Chi-Chin Wu,Bo-Chen Chiu,Cheng-Hung,Lin,Cheng-Kai Lu,Jia-Kang Wang,Tzu-Lun Huang*

Main category: eess.IV

TL;DR: 该论文利用小样本学习提升OCT诊断图像分类准确率，结合GAN增强和新型方法，最终模型准确率达97.85%。


<details>
  <summary>Details</summary>
Motivation: 解决OCT诊断图像中主要和稀有类别分类准确率不均衡的问题，提升整体分类性能。

Method: 采用基于GAN的数据增强策略，引入U-GAT-IT改进生成部分，使用数据平衡技术减少类别间准确率偏差，结合CBAM注意力机制和微调InceptionV3。

Result: 最佳模型整体准确率达到97.85%，较原始基线有显著提升。

Conclusion: 提出的方法有效提高了OCT图像分类的准确率，尤其在处理稀有类别时表现优异。

Abstract: This paper focuses on using few-shot learning to improve the accuracy of
classifying OCT diagnosis images with major and rare classes. We used the
GAN-based augmentation strategy as a baseline and introduced several novel
methods to further enhance our model. The proposed strategy contains U-GAT-IT
for improving the generative part and uses the data balance technique to narrow
down the skew of accuracy between all categories. The best model obtained was
built with CBAM attention mechanism and fine-tuned InceptionV3, and achieved an
overall accuracy of 97.85%, representing a significant improvement over the
original baseline.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [852] [NMCSE: Noise-Robust Multi-Modal Coupling Signal Estimation Method via Optimal Transport for Cardiovascular Disease Detection](https://arxiv.org/abs/2505.18174)
*Zhixin li,Peihong Zhang,Rui Sang,Yuxuan Liu,Shengchen Li*

Main category: eess.SP

TL;DR: 该论文提出了一种名为NMCSE的噪声鲁棒多模态耦合信号估计方法，通过最优传输理论重新定义问题，结合时空特征提取网络，显著提升了心血管疾病检测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的心电图（ECG）和心音图（PCG）信号耦合估计方法使用反卷积技术，容易放大噪声，限制了临床应用的实用性。因此，需要一种更鲁棒的方法来准确估计耦合信号并提升心血管疾病检测性能。

Method: 论文提出了NMCSE方法，将问题重新定义为通过最优传输理论进行分布匹配，联合优化振幅和时间对齐，避免噪声放大。结合时空特征提取网络，实现了鲁棒的多模态心血管疾病检测。

Result: 在PhysioNet 2016数据集上的实验表明，NMCSE将估计误差降低了约30%（均方误差），同时在所有测试信噪比下保持了较高的皮尔逊相关系数。心血管疾病检测准确率达到97.38%，AUC为0.98，优于现有方法。

Conclusion: NMCSE方法在噪声环境下表现出色，显著提升了耦合信号估计和心血管疾病检测的准确性，适用于实际临床应用。

Abstract: Electrocardiogram (ECG) and Phonocardiogram (PCG) signals are linked by a
latent coupling signal representing the electrical-to-mechanical cardiac
transformation. While valuable for cardiovascular disease (CVD) detection, this
coupling signal is traditionally estimated using deconvolution methods that
amplify noise, limiting clinical utility. In this paper, we propose
Noise-Robust Multi-Modal Coupling Signal Estimation (NMCSE), which reformulates
the problem as distribution matching via optimal transport theory. By jointly
optimizing amplitude and temporal alignment, NMCSE mitigates noise
amplification without additional preprocessing. Integrated with our
Temporal-Spatial Feature Extraction network, NMCSE enables robust multi-modal
CVD detection. Experiments on the PhysioNet 2016 dataset with realistic
hospital noise demonstrate that NMCSE reduces estimation errors by
approximately 30% in Mean Squared Error while maintaining higher Pearson
Correlation Coefficients across all tested signal-to-noise ratios. Our approach
achieves 97.38% accuracy and 0.98 AUC in CVD detection, outperforming
state-of-the-art methods and demonstrating robust performance for real-world
clinical applications.

</details>


### [853] [Evaluation in EEG Emotion Recognition: State-of-the-Art Review and Unified Framework](https://arxiv.org/abs/2505.18175)
*Natia Kukhilava,Tatia Tsmindashvili,Rapael Kalandadze,Anchit Gupta,Sofio Katamadze,François Brémond,Laura M. Ferrari,Philipp Müller,Benedikt Emanuel Wirth*

Main category: eess.SP

TL;DR: 该论文分析了2018-2023年间216篇EEG情绪识别(EEG-ER)论文，发现领域缺乏统一评估标准，提出了开源框架EEGain以标准化评估流程。


<details>
  <summary>Details</summary>
Motivation: 当前EEG-ER领域存在评估协议不一致问题，包括真实标签定义、评估指标选择、数据划分方式等差异，阻碍了研究的公平比较和进展追踪。

Method: 提出EEGain开源框架，集成标准化预处理、数据划分、评估指标，并支持六大主流数据集的一键加载，使用四种公开方法进行验证。

Result: EEGain实现了对六大数据集(AMIGOS等)和四种主流方法(EEGNet等)的标准化评估，为领域建立了可复现的基准。

Conclusion: EEGain通过统一评估协议显著提升了EEG-ER研究的可复现性和可比性，将加速领域整体发展。

Abstract: Electroencephalography-based Emotion Recognition (EEG-ER) has become a
growing research area in recent years. Analyzing 216 papers published between
2018 and 2023, we uncover that the field lacks a unified evaluation protocol,
which is essential to fairly define the state of the art, compare new
approaches and to track the field's progress. We report the main
inconsistencies between the used evaluation protocols, which are related to
ground truth definition, evaluation metric selection, data splitting types
(e.g., subject-dependent or subject-independent) and the use of different
datasets. Capitalizing on this state-of-the-art research, we propose a unified
evaluation protocol, EEGain (https://github.com/EmotionLab/EEGain), which
enables an easy and efficient evaluation of new methods and datasets. EEGain is
a novel open source software framework, offering the capability to compare -
and thus define - state-of-the-art results. EEGain includes standardized
methods for data pre-processing, data splitting, evaluation metrics, and the
ability to load the six most relevant datasets (i.e., AMIGOS, DEAP, DREAMER,
MAHNOB-HCI, SEED, SEED-IV) in EEG-ER with only a single line of code. In
addition, we have assessed and validated EEGain using these six datasets on the
four most common publicly available methods (EEGNet, DeepConvNet,
ShallowConvNet, TSception). This is a significant step to make research on
EEG-ER more reproducible and comparable, thereby accelerating the overall
progress of the field.

</details>


### [854] [Improving Generative Inverse Design of Rectangular Patch Antennas with Test Time Optimization](https://arxiv.org/abs/2505.18188)
*Beck LaBash,Shahriar Khushrushahi,Fabian Ruehle*

Main category: eess.SP

TL;DR: 提出一种两阶段深度学习框架，用于矩形贴片天线的逆向设计，结合生成建模和优化技术提升设计精度。


<details>
  <summary>Details</summary>
Motivation: 旨在通过深度学习解决天线逆向设计问题，生成满足特定频率响应的可行天线几何结构。

Method: 采用两阶段框架：首先生成模型学习天线频率响应的潜在表示，随后基于这些响应生成可行几何结构，并结合搜索优化技术。

Result: 该方法能生成高精度设计，并兼顾可制造性等辅助目标，适用于不同设计标准和复杂几何空间。

Conclusion: 所提框架灵活高效，可扩展至更复杂的天线设计场景，为逆向设计提供新思路。

Abstract: We propose a two-stage deep learning framework for the inverse design of
rectangular patch antennas. Our approach leverages generative modeling to learn
a latent representation of antenna frequency response curves and conditions a
subsequent generative model on these responses to produce feasible antenna
geometries. We further demonstrate that leveraging search and optimization
techniques at test-time improves the accuracy of the generated designs and
enables consideration of auxiliary objectives such as manufacturability. Our
approach generalizes naturally to different design criteria, and can be easily
adapted to more complex geometric design spaces.

</details>


### [855] [PhySense: Sensor Placement Optimization for Accurate Physics Sensing](https://arxiv.org/abs/2505.18190)
*Yuezhou Ma,Haixu Wu,Hang Zhou,Huikun Weng,Jianmin Wang,Mingsheng Long*

Main category: eess.SP

TL;DR: PhySense提出了一种两阶段框架，联合优化物理场重建和传感器布局，通过深度学习提升物理感知精度。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在稀疏数据重建方面进展迅速，但普遍忽略了传感器布局的优化，未能充分利用重建与布局之间的相互增强作用。

Method: PhySense采用两阶段框架：第一阶段使用基于流的生成模型和交叉注意力融合稀疏观测；第二阶段利用重建反馈，通过投影梯度下降优化传感器布局。

Result: 在三个挑战性基准测试（包括3D几何数据集）中，PhySense实现了最先进的物理感知精度，并发现了之前未考虑的信息性传感器布局。

Conclusion: PhySense通过联合优化重建与布局，显著提升了物理感知性能，其学习目标与经典方差最小化原则一致，具有理论保证。

Abstract: Physics sensing plays a central role in many scientific and engineering
domains, which inherently involves two coupled tasks: reconstructing dense
physical fields from sparse observations and optimizing scattered sensor
placements to observe maximum information. While deep learning has made rapid
advances in sparse-data reconstruction, existing methods generally omit
optimization of sensor placements, leaving the mutual enhancement between
reconstruction and placement on the shelf. To change this suboptimal practice,
we propose PhySense, a synergistic two-stage framework that learns to jointly
reconstruct physical fields and to optimize sensor placements, both aiming for
accurate physics sensing. The first stage involves a flow-based generative
model enhanced by cross-attention to adaptively fuse sparse observations.
\correct{Leveraging the reconstruction feedback, }the second stage performs
sensor placement via projected gradient descent to satisfy spatial constraints.
\correct{We further prove that the learning objectives of the two stages are
consistent with classical variance-minimization principles, providing
theoretical guarantees.} Extensive experiments across three challenging
benchmarks, especially a 3D geometry dataset, indicate PhySense achieves
state-of-the-art physics sensing accuracy and discovers informative sensor
placements previously unconsidered.

</details>


### [856] [SzCORE as a benchmark: report from the seizure detection challenge at the 2025 AI in Epilepsy and Neurological Disorders Conference](https://arxiv.org/abs/2505.18191)
*Jonathan Dan,Amirhossein Shahbazinia,Christodoulos Kechris,David Atienza*

Main category: eess.SP

TL;DR: 该论文通过组织一场癫痫检测算法挑战赛，评估了不同算法在长时程EEG数据上的性能，发现现有模型泛化能力不足，并提出了标准化评估框架SzCORE。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习模型在跨患者或临床环境中的癫痫检测泛化能力不足，临床仍依赖人工EEG检查，亟需开发鲁棒模型并建立标准化评估体系。

Method: 使用65名受试者4,360小时的连续EEG数据（含专家标注的癫痫发作真值），要求参赛算法检测发作起始/持续时间，采用事件级指标（灵敏度/精确率/F1值/每日误报）和SzCORE框架进行标准化评估。

Result: 30份提交中最佳算法F1-score仅43%（灵敏度37%，精确率45%），性能差异显著，但优于既往挑战赛和商业系统。挑战平台现支持持续基准测试。

Conclusion: 癫痫自动检测仍具挑战性，需通过标准化框架（如SzCORE）进行严格评估以弥合算法报告性能与真实场景的差距，平台将持续支持可重复研究和临床转化。

Abstract: Reliable automatic seizure detection from long-term EEG remains a challenge,
as current machine learning models often fail to generalize across patients or
clinical settings. Manual EEG review remains the clinical standard,
underscoring the need for robust models and standardized evaluation. To
rigorously assess algorithm performance, we organized a challenge using a
private dataset of continuous EEG recordings from 65 subjects (4,360 hours).
Expert neurophysiologists annotated the data, providing ground truth for
seizure events. Participants were required to detect seizure onset and
duration, with evaluation based on event-based metrics, including sensitivity,
precision, F1-score, and false positives per day. The SzCORE framework ensured
standardized evaluation. The primary ranking criterion was the event-based
F1-score, reflecting clinical relevance by balancing sensitivity and false
positives. The challenge received 30 submissions from 19 teams, with 28
algorithms evaluated. Results revealed wide variability in performance, with a
top F1-score of 43% (sensitivity 37%, precision 45%), highlighting the ongoing
difficulty of seizure detection. The challenge also revealed a gap between
reported performance and real-world evaluation, emphasizing the importance of
rigorous benchmarking. Compared to previous challenges and commercial systems,
the best-performing algorithm in this contest showed improved performance.
Importantly, the challenge platform now supports continuous benchmarking,
enabling reproducible research, integration of new datasets, and clinical
evaluation of seizure detection algorithms using a standardized framework.

</details>


### [857] [Large Language Model-Driven Distributed Integrated Multimodal Sensing and Semantic Communications](https://arxiv.org/abs/2505.18194)
*Yubo Peng,Luping Xiang,Bingxin Zhang,Kun Yang*

Main category: eess.SP

TL;DR: 论文提出了一种基于大语言模型（LLM）的分布式多模态感知与语义通信框架（LLM-DiSAC），通过融合射频（RF）和视觉数据提升复杂环境下的感知精度。


<details>
  <summary>Details</summary>
Motivation: 传统单一模态（如仅RF或视觉）的感知系统在复杂动态环境中表现受限，且单设备系统视角有限、空间覆盖不足，难以应对城市或非视距场景的需求。

Method: LLM-DiSAC框架包含三个核心部分：1）设备端的RF-视觉融合网络（RVFN）实现多模态特征提取与融合；2）基于LLM的语义传输网络（LSTN）提升通信效率；3）聚合中心的Transformer模型（TRAM）融合分布式特征。采用两阶段分布式学习策略保护数据隐私。

Result: 在Genesis仿真引擎生成的多视角RF-视觉数据集上测试表明，LLM-DiSAC性能表现优异。

Conclusion: 该框架通过多设备协同、多模态融合及语义通信优化，显著提升了复杂环境下的感知能力，同时兼顾隐私保护。

Abstract: Traditional single-modal sensing systems-based solely on either radio
frequency (RF) or visual data-struggle to cope with the demands of complex and
dynamic environments. Furthermore, single-device systems are constrained by
limited perspectives and insufficient spatial coverage, which impairs their
effectiveness in urban or non-line-of-sight scenarios. To overcome these
challenges, we propose a novel large language model (LLM)-driven distributed
integrated multimodal sensing and semantic communication (LLM-DiSAC) framework.
Specifically, our system consists of multiple collaborative sensing devices
equipped with RF and camera modules, working together with an aggregation
center to enhance sensing accuracy. First, on sensing devices, LLM-DiSAC
develops an RF-vision fusion network (RVFN), which employs specialized feature
extractors for RF and visual data, followed by a cross-attention module for
effective multimodal integration. Second, a LLM-based semantic transmission
network (LSTN) is proposed to enhance communication efficiency, where the
LLM-based decoder leverages known channel parameters, such as transceiver
distance and signal-to-noise ratio (SNR), to mitigate semantic distortion.
Third, at the aggregation center, a transformer-based aggregation model (TRAM)
with an adaptive aggregation attention mechanism is developed to fuse
distributed features and enhance sensing accuracy. To preserve data privacy, a
two-stage distributed learning strategy is introduced, allowing local model
training at the device level and centralized aggregation model training using
intermediate features. Finally, evaluations on a synthetic multi-view RF-visual
dataset generated by the Genesis simulation engine show that LLM-DiSAC achieves
a good performance.

</details>


### [858] [CrossRF: A Domain-Invariant Deep Learning Approach for RF Fingerprinting](https://arxiv.org/abs/2505.18200)
*Fahrettin Emin Tiras,Hayriye Serra Altinoluk*

Main category: eess.SP

TL;DR: CrossRF是一种基于对抗学习的域不变深度学习方法，显著提升了无人机跨信道射频指纹识别的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 射频指纹识别在无人机安全领域具有潜力，但现有方法在不同传输信道上性能显著下降，亟需解决跨信道识别问题。

Method: 采用对抗学习训练域不变模型，最小化不同RF信道间的域差距，使用真实无人机信号数据集UAVSig进行验证。

Result: 在信道3到4的适应中达到99.03%准确率（传统方法仅26.39%），多信道场景保持87.57%准确率，控制器分类精度达89.45%。

Conclusion: CrossRF能显著减少跨信道变异导致的性能下降，以最少训练数据需求保持高识别精度，适合实际无人机安全应用。

Abstract: Radio Frequency (RF) fingerprinting offers a promising approach for drone
identification and security, although it suffers from significant performance
degradation when operating on different transmission channels. This paper
presents CrossRF, a domain-invariant deep learning approach that addresses the
problem of cross-channel RF fingerprinting for Unmanned Aerial Vehicle (UAV)
identification. Our approach aims to minimize the domain gap between different
RF channels by using adversarial learning to train a more robust model that
maintains consistent identification performance despite channel variations. We
validate our approach using the UAVSig dataset, comprising real-world
over-the-air RF signals from identical drone models operating across several
frequency channels, ensuring that the findings correspond to real-world
scenarios. The experimental results show CrossRF's efficiency, achieving up to
99.03% accuracy when adapting from Channel 3 to Channel 4, compared to only
26.39% using conventional methods. The model maintains robust performance in
more difficult multi-channel scenarios (87.57% accuracy adapting from Channels
1,3 to 2,4) and achieves 89.45% accuracy with 0.9 precision for controller
classification. These results confirm CrossRF's ability to significantly reduce
performance degradation due to cross-channel variations while maintaining high
identification accuracy with minimal training data requirements, making it
particularly suitable for practical drone security applications.

</details>


### [859] [Season-Independent PV Disaggregation Using Multi-Scale Net Load Temporal Feature Extraction and Weather Factor Fusion](https://arxiv.org/abs/2505.18747)
*Xiaolu Chen,Chenghao Huang,Yanru Zhang,Hao Wang*

Main category: eess.SP

TL;DR: 该论文提出了一种结合分层插值和多头自注意力机制的光伏发电分解方法，用于精确预测分布式光伏发电量，提升能源系统监控能力。


<details>
  <summary>Details</summary>
Motivation: 随着分布式光伏系统的普及，电网公司面临从净电力负荷中分离光伏发电的智能监测挑战，现有方法在特征提取和天气因素关联捕捉上存在不足。

Method: 采用分层插值（HI）提取净负荷特征，并利用多头自注意力机制捕捉天气因素间的复杂依赖关系。

Result: 仿真实验验证了该方法在实际数据中的有效性，能够实现精准的光伏发电预测。

Conclusion: 该方法为分布式能源系统的监测与管理提供了有效支持，具有实际应用价值。

Abstract: With the advancement of energy Internet and energy system integration, the
increasing adoption of distributed photovoltaic (PV) systems presents new
challenges on smart monitoring and measurement for utility companies,
particularly in separating PV generation from net electricity load. Existing
methods struggle with feature extraction from net load and capturing the
relevance between weather factors. This paper proposes a PV disaggregation
method that integrates Hierarchical Interpolation (HI) and multi-head
self-attention mechanisms. By using HI to extract net load features and
multi-head self-attention to capture the complex dependencies between weather
factors, the method achieves precise PV generation predictions. Simulation
experiments demonstrate the effectiveness of the proposed method in real-world
data, supporting improved monitoring and management of distributed energy
systems.

</details>


### [860] [Accelerating Battery Material Optimization through iterative Machine Learning](https://arxiv.org/abs/2505.18162)
*Seon-Hwa Lee,Insoo Ye,Changhwan Lee,Jieun Kim,Geunho Choi,Sang-Cheol Nam,Inchul Park*

Main category: eess.SP

TL;DR: 该论文提出了一种结合主动学习的迭代机器学习框架，用于优化电池材料性能，显著减少实验次数并加速高维设计空间的探索。


<details>
  <summary>Details</summary>
Motivation: 传统单因素实验方法在电池材料优化中存在认知局限和人为偏差，随着工业发展参数复杂度增加，这种方法显得过时且效率低下。

Method: 采用迭代机器学习框架，整合主动学习以指导定向实验，并逐步优化模型，利用包括成功和失败结果的全面实验数据。

Result: 主动学习驱动的实验显著减少了所需的实验周期数，有效缓解了数据稀缺和人为偏差问题。

Conclusion: 基于机器学习的策略在加速电池材料优化方面具有变革性潜力。

Abstract: The performance of battery materials is determined by their composition and
the processing conditions employed during commercial-scale fabrication, where
raw materials undergo complex processing steps with various additives to yield
final products. As the complexity of these parameters expands with the
development of industry, conventional one-factor-at-a-time (OFAT) experiment
becomes old fashioned. While domain expertise aids in parameter optimization,
this traditional approach becomes increasingly vulnerable to cognitive
limitations and anthropogenic biases as the complexity of factors grows.
Herein, we introduce an iterative machine learning (ML) framework that
integrates active learning to guide targeted experimentation and facilitate
incremental model refinement. This method systematically leverages
comprehensive experimental observations, including both successful and
unsuccessful results, effectively mitigating human-induced biases and
alleviating data scarcity. Consequently, it significantly accelerates
exploration within the high-dimensional design space. Our results demonstrate
that active-learning-driven experimentation markedly reduces the total number
of experimental cycles necessary, underscoring the transformative potential of
ML-based strategies in expediting battery material optimization.

</details>


### [861] [Dim and Small Target Detection for Drone Broadcast Frames Based on Time-Frequency Analysis](https://arxiv.org/abs/2505.18167)
*Jie Li,Jing Li,Zhanyu Ju,Fengkui Gong,Lu Lv*

Main category: eess.SP

TL;DR: 该论文提出了一种基于通信协议时频分析的无人机广播帧微弱小目标检测算法，通过调制参数分析和帧结构优化，提升低信噪比下的检测精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 针对无人机广播帧中微弱小目标在低信噪比环境下检测精度不足的问题，结合通信协议特征优化检测算法。

Method: 利用调制参数（传输频率、信号带宽）和ZC序列的强相关性校正检测框参数，采用分段能量细化法抑制干扰，并权衡采样时长与检测性能。

Result: 仿真显示算法平均交并比、精确率和召回率分别提升3%、1.4%和2.4%，且在飞行距离、环境噪声等变化下表现鲁棒。

Conclusion: 该算法通过协议特征融合与参数优化，显著提升了无人机广播帧中微弱目标的检测性能，可适配不同监管需求。

Abstract: We propose a dim and small target detection algorithm for drone broadcast
frames based on the time-frequency analysis of communication protocol.
Specifically, by analyzing modulation parameters and frame structures, the
prior knowledge of transmission frequency, signal bandwidth, Zadoff-Chu (ZC)
sequences, and frame length of drone broadcast frames is established. The RF
signals are processed through the designed filter banks, and the frequency
domain parameters of bounding boxes generated by the detector are corrected
with transmission frequency and signal bandwidth. Given the remarkable
correlation characteristics of ZC sequences, the frequency domain parameters of
bounding boxes with low confidence scores are corrected based on ZC sequences
and frame length, which improves the detection accuracy of dim targets under
low signal-to noise ratio (SNR) situations. Besides, a segmented energy
refinement method is applied to mitigate the deviation caused by interference
signals with high energy strength, which ulteriorly corrects the time domain
detection parameters for dim targets. As the sampling duration increases, the
detection speed improves while the detection accuracy of broadcast frames
termed as small targets decreases. The trade-off between detection accuracy and
speed versus sampling duration is established, which helps to meet different
drone regulation requirements. Simulation results demonstrate that the proposed
algorithm improves the average intersection over union, precision, and recall
by 3\%, 1.4\%, and 2.4\%, respectively, compared to existing algorithms. The
proposed algorithm also performs strong robustness under varying flight
distances, diverse types of environment noise, and different flight visual
environment.

</details>


### [862] [Load Forecasting in the Era of Smart Grids: Opportunities and Advanced Machine Learning Models](https://arxiv.org/abs/2505.18170)
*Aurausp Maneshni*

Main category: eess.SP

TL;DR: 该论文探讨了四种机器学习框架在短期电力负荷预测中的应用，包括XGBoost、LightGBM、LSTM和GRU，并开发了一种混合框架。实验表明，这些方法比传统ARIMA模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 电力难以存储，需精确控制发电、输电和配电。电力供需实时平衡是电力系统的持续挑战，过剩导致资源浪费，不足则增加电网压力及运营成本。准确的负荷预测对维持电网稳定至关重要。

Method: 论文评估了四种机器学习框架（XGBoost、LightGBM、LSTM、GRU）和一种混合框架，并应用皮尔逊相关系数分析电力需求与外生变量的关系。

Result: 实验结果表明，基于机器学习的模型在特定数据集和预测任务中，比传统ARIMA基线模型具有更好的预测性能。

Conclusion: 机器学习方法在短期电力负荷预测中表现优异，能有效提升预测精度，为电网稳定运行提供支持。

Abstract: Electric energy is difficult to store, requiring stricter control over its
generation, transmission, and distribution. A persistent challenge in power
systems is maintaining real-time equilibrium between electricity demand and
supply. Oversupply contributes to resource wastage, while undersupply can
strain the grid, increase operational costs, and potentially impact service
reliability. To maintain grid stability, load forecasting is needed. Accurate
load forecasting balances generation and demand by striving to predict future
electricity consumption. This thesis examines and evaluates four machine
learning frameworks for short term load forecasting, including gradient
boosting decision tree methods such as Extreme Gradient Boosting (XGBoost) and
Light Gradient Boosting Machine (LightGBM). A hybrid framework is also
developed. In addition, two recurrent neural network architectures, Long Short
Term Memory (LSTM) networks and Gated Recurrent Units (GRU), are designed and
implemented. Pearson Correlation Coefficient is applied to assess the
relationships between electricity demand and exogenous variables. The
experimental results show that, for the specific dataset and forecasting task
in this study, machine learning-based models achieved improved forecasting
performance compared to a classical ARIMA baseline.

</details>


### [863] [Machine Learning-Based Analysis of ECG and PCG Signals for Rheumatic Heart Disease Detection: A Scoping Review (2015-2025)](https://arxiv.org/abs/2505.18182)
*Damilare Emmanuel Olatunji,Julius Dona Zannu,Carine Pierrette Mukamakuza,Godbright Nixon Uiso,Mona Mamoun Mubarak Aman,John Bosco Thuo,Chol Buol,Nchofon Tagha Ghogomu,Evelyne Umubyeyi*

Main category: eess.SP

TL;DR: 2015-2025年系统评估机器学习在心电图和心音数据应用于风湿性心脏病低成本检测工具的研究现状，指出CNN为主流技术但存在临床转化障碍。


<details>
  <summary>Details</summary>
Motivation: 支持世界心脏联盟'25 by 25'降低死亡率目标，为资源匮乏地区提供替代超声心动图的RHD筛查方案。

Method: 遵循PRISMA-ScR指南，系统检索PubMed等数据库，筛选ML-based ECG/PCG分析RHD的文献，由两名评审独立提取方法学与性能指标数据。

Result: 分析37项研究显示：CNN在2020年后占主导（中位准确率93.7%），但73%研究使用单中心数据，仅10.8%进行外部验证，性能因瓣膜病变类型差异显著。

Conclusion: ML-ECG/PCG技术具有潜力，但需解决标准化评估框架、多模态架构、成本效益分析和流行病区前瞻性试验等关键瓶颈。

Abstract: Objective: To conduct a systematic assessment of machine learning
applications that utilize electrocardiogram (ECG) and heart sound data in the
development of cost-effective detection tools for rheumatic heart disease (RHD)
from the year 2015 to 2025, thereby supporting the World Heart Federation's "25
by 25" mortality reduction objective through the creation of alternatives to
echocardiography in underserved regions. Methods: Following PRISMA-ScR
guidelines, we conducted a comprehensive search across PubMed, IEEE Xplore,
Scopus, and Embase for peer-reviewed literature focusing on ML-based ECG/PCG
analysis for RHD detection. Two independent reviewers screened studies, and
data extraction focused on methodology, validation approaches, and performance
metrics. Results: Analysis of 37 relevant studies revealed that convolutional
neural networks (CNNs) have become the predominant technology in post-2020
implementations, achieving a median accuracy of 93.7%. However, 73% of studies
relied on single-center datasets, only 10.8% incorporated external validation,
and none addressed cost-effectiveness. Performance varied markedly across
different valvular lesions, and despite 44% of studies originating from endemic
regions, significant gaps persisted in implementation science and demographic
diversity. Conclusion: While ML-based ECG/PCG analysis shows promise for RHD
detection, substantial methodological limitations hinder clinical translation.
Future research must prioritize standardized benchmarking frameworks,
multimodal architectures, cost-effectiveness assessments, and prospective
trials in endemic settings. Significance: This review provides a critical
roadmap for developing accessible ML-based RHD screening tools to help bridge
the diagnostic gap in resourceconstrained settings where conventional
auscultation misses up to 90% of cases and echocardiography remains
inaccessible.

</details>


### [864] [FRAME-C: A knowledge-augmented deep learning pipeline for classifying multi-electrode array electrophysiological signals](https://arxiv.org/abs/2505.18183)
*Nisal Ranasinghe,Dzung Do-Ha,Simon Maksour,Tamasha Malepathirana,Sachith Seneviratne,Lezanne Ooi,Saman Halgamuge*

Main category: eess.SP

TL;DR: 该论文提出了一种名为FRAME-C的知识增强机器学习流程，结合领域知识、原始波形数据和深度学习技术，用于分类MEA信号并识别ALS特异性表型，在模拟和真实数据上均表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统MEA数据分析依赖手工提取特征，可能无法完全捕捉数据中的有用信息。深度学习虽能自动学习特征，但手工特征对领域知识编码和可解释性至关重要，尤其是在数据有限或噪声较多的情况下。

Method: FRAME-C结合了领域知识（如峰值幅度、峰值间隔等手工特征）和深度学习技术，从原始波形数据中学习重要特征，同时保留关键时空信息。

Result: FRAME-C在模拟和真实MEA数据上验证，性能优于现有分类方法，真实数据上提升超过11%，模拟数据上提升高达25%。此外，还能评估手工特征的重要性。

Conclusion: FRAME-C通过融合领域知识和深度学习，显著提升了ALS表型分类的准确性，并为理解ALS病理机制提供了新见解。

Abstract: Amyotrophic lateral sclerosis (ALS) is a fatal neurodegenerative disorder
characterized by motor neuron degeneration, with alterations in neural
excitability serving as key indicators. Recent advancements in induced
pluripotent stem cell (iPSC) technology have enabled the generation of human
iPSC-derived neuronal cultures, which, when combined with multi-electrode array
(MEA) electrophysiology, provide rich spatial and temporal electrophysiological
data. Traditionally, MEA data is analyzed using handcrafted features based on
potentially imperfect domain knowledge, which while useful may not fully
capture all useful characteristics inherent in the data. Machine learning,
particularly deep learning, has the potential to automatically learn relevant
characteristics from raw data without solely relying on handcrafted feature
extraction. However, handcrafted features remain critical for encoding domain
knowledge and improving interpretability, especially with limited or noisy
data. This study introduces FRAME-C, a knowledge-augmented machine learning
pipeline that combines domain knowledge, raw spike waveform data, and deep
learning techniques to classify MEA signals and identify ALS-specific
phenotypes. FRAME-C leverages deep learning to learn important features from
spike waveforms while incorporating handcrafted features such as spike
amplitude, inter-spike interval, and spike duration, preserving key spatial and
temporal information. We validate FRAME-C on both simulated and real MEA data
from human iPSC-derived neuronal cultures, demonstrating superior performance
over existing classification methods. FRAME-C shows over 11% improvement on
real data and up to 25% on simulated data. We also show FRAME-C can evaluate
handcrafted feature importance, providing insights into ALS phenotypes.

</details>


### [865] [BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals](https://arxiv.org/abs/2505.18185)
*Qinfan Xiao,Ziyun Cui,Chi Zhang,Siqi Chen,Wen Wu,Andrew Thwaites,Alexandra Woolgar,Bowen Zhou,Chao Zhang*

Main category: eess.SP

TL;DR: 本文提出首个跨EEG和MEG的脑电基础模型BrainOmni，通过统一表征和自监督预训练提升性能与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有EEG/MEG分析方法依赖特定模态和设备的独立模型，限制了跨域扩展性。需开发统一框架处理异构脑电数据。

Method: 提出BrainTokenizer将脑电活动离散化，通过传感器编码器统一设备差异；基于此构建自监督预训练的BrainOmni基础模型。

Result: 模型在多项下游任务中超越现有方法，泛化性优异。联合EEG-MEG训练带来双向性能提升，使用1997小时EEG+656小时MEG数据。

Conclusion: BrainOmni是首个支持EEG/MEG的基础模型，统一表征和跨模态训练为脑电分析提供新范式。

Abstract: Electroencephalography (EEG) and magnetoencephalography (MEG) measure neural
activity non-invasively by capturing electromagnetic fields generated by
dendritic currents. Although rooted in the same biophysics, EEG and MEG exhibit
distinct signal patterns, further complicated by variations in sensor
configurations across modalities and recording devices. Existing approaches
typically rely on separate, modality- and dataset-specific models, which limits
the performance and cross-domain scalability. This paper proposes BrainOmni,
the first brain foundation model that generalises across heterogeneous EEG and
MEG recordings. To unify diverse data sources, we introduce BrainTokenizer,the
first tokenizer that quantises spatiotemporal brain activity into discrete
representations. Central to BrainTokenizer is a novel Sensor Encoder that
encodes sensor properties such as spatial layout, orientation, and type,
enabling compatibility across devices and modalities. Building upon the
discrete representations, BrainOmni learns unified semantic embeddings of brain
signals by self-supervised pretraining. To the best of our knowledge, it is the
first foundation model to support both EEG and MEG signals, as well as the
first to incorporate large-scale MEG pretraining. A total of 1,997 hours of EEG
and 656 hours of MEG data are curated and standardised from publicly available
sources for pretraining. Experiments show that BrainOmni outperforms both
existing foundation models and state-of-the-art task-specific models on a range
of downstream tasks. It also demonstrates strong generalisation to unseen EEG
and MEG devices. Further analysis reveals that joint EEG-MEG (EMEG) training
yields consistent improvements across both modalities. Code and model
checkpoints will be released upon acceptance.

</details>


### [866] [Generating Realistic Multi-Beat ECG Signals](https://arxiv.org/abs/2505.18189)
*Paul Pöhl,Viktor Schlegel,Hao Li,Anil Bharath*

Main category: eess.SP

TL;DR: 本文提出了一种新颖的三层合成框架，用于生成逼真的长时程心电图信号，解决了现有扩散模型在生成长序列心电图方面的不足。


<details>
  <summary>Details</summary>
Motivation: 合成心电图数据在医疗领域有广泛的应用，但现有扩散模型难以生成长时程心电图信号，限制了其临床应用。本文旨在解决这一问题。

Method: 采用三层合成框架：首先使用扩散模型生成高保真的单次心跳，然后合成保留关键时间依赖性的心跳间特征，最后通过特征引导匹配将心跳组装成连贯的长序列。

Result: 生成的合成心电图在心跳形态保真度和心跳间关系上均表现出色，在心律失常分类任务中显著优于端到端的扩散模型生成方法。

Conclusion: 该方法能够生成前所未有的多分钟心电图序列，同时保留关键的诊断特征，为下游应用提供了更高的实用价值。

Abstract: Generating synthetic ECG data has numerous applications in healthcare, from
educational purposes to simulating scenarios and forecasting trends. While
recent diffusion models excel at generating short ECG segments, they struggle
with longer sequences needed for many clinical applications. This paper
proposes a novel three-layer synthesis framework for generating realistic
long-form ECG signals. We first generate high-fidelity single beats using a
diffusion model, then synthesize inter-beat features preserving critical
temporal dependencies, and finally assemble beats into coherent long sequences
using feature-guided matching. Our comprehensive evaluation demonstrates that
the resulting synthetic ECGs maintain both beat-level morphological fidelity
and clinically relevant inter-beat relationships. In arrhythmia classification
tasks, our long-form synthetic ECGs significantly outperform end-to-end
long-form ECG generation using the diffusion model, highlighting their
potential for increasing utility for downstream applications. The approach
enables generation of unprecedented multi-minute ECG sequences while preserving
essential diagnostic characteristics.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [867] [A Unified Framework for Variable Selection in Model-Based Clustering with Missing Not at Random](https://arxiv.org/abs/2505.19093)
*Binh H. Ho,Long Nguyen Chi,TrungTin Nguyen,Binh T. Nguyen,Van Ha Hoang,Christopher Drovandi*

Main category: stat.ME

TL;DR: 该论文提出了一种结合变量选择和缺失数据处理的统一框架，用于提升基于模型的聚类分析在复杂数据中的效果。


<details>
  <summary>Details</summary>
Motivation: 基于模型的聚类分析在复杂数据中揭示潜在结构时，常因变量选择和缺失数据问题而受限，尤其是在转录组学等领域。现有方法通常孤立处理这些问题，缺乏灵活性和适应性。

Method: 论文引入了一个统一框架，结合数据驱动的惩罚矩阵进行灵活的变量选择，并显式建模缺失数据与潜在类别之间的关系。

Result: 在特定正则条件下，该框架实现了渐近一致性和选择一致性，显著提升了聚类分析的效能，并通过模拟和真实转录组数据验证了其性能。

Conclusion: 这一统一策略显著增强了基于模型聚类的功能和效率，为复杂缺失数据模式下的同质子群识别提供了先进方法。

Abstract: Model-based clustering integrated with variable selection is a powerful tool
for uncovering latent structures within complex data. However, its
effectiveness is often hindered by challenges such as identifying relevant
variables that define heterogeneous subgroups and handling data that are
missing not at random, a prevalent issue in fields like transcriptomics. While
several notable methods have been proposed to address these problems, they
typically tackle each issue in isolation, thereby limiting their flexibility
and adaptability. This paper introduces a unified framework designed to address
these challenges simultaneously. Our approach incorporates a data-driven
penalty matrix into penalized clustering to enable more flexible variable
selection, along with a mechanism that explicitly models the relationship
between missingness and latent class membership. We demonstrate that, under
certain regularity conditions, the proposed framework achieves both asymptotic
consistency and selection consistency, even in the presence of missing data.
This unified strategy significantly enhances the capability and efficiency of
model-based clustering, advancing methodologies for identifying informative
variables that define homogeneous subgroups in the presence of complex missing
data patterns. The performance of the framework, including its computational
efficiency, is evaluated through simulations and demonstrated using both
synthetic and real-world transcriptomic datasets.

</details>


### [868] [Do Large Language Models (Really) Need Statistical Foundations?](https://arxiv.org/abs/2505.19145)
*Weijie Su*

Main category: stat.ME

TL;DR: 本文探讨统计学对大型语言模型（LLM）发展的贡献，认为其数据依赖性和黑箱特性需要统计方法，并列举多个应用领域。


<details>
  <summary>Details</summary>
Motivation: 研究统计学是否能为LLM的发展和应用提供基础性贡献，基于LLM的数据依赖性和黑箱特性。

Method: 通过两个论点论证统计学在LLM中的必要性，并列举具体研究领域。

Result: 统计方法在LLM的对齐、水印、不确定性量化等领域至关重要且已发挥作用。

Conclusion: 统计研究将形成多样化专题，统计学界需及时参与LLM研究。

Abstract: Large language models (LLMs) represent a new paradigm for processing
unstructured data, with applications across an unprecedented range of domains.
In this paper, we address, through two arguments, whether the development and
application of LLMs would genuinely benefit from foundational contributions
from the statistics discipline. First, we argue affirmatively, beginning with
the observation that LLMs are inherently statistical models due to their
profound data dependency and stochastic generation processes, where statistical
insights are naturally essential for handling variability and uncertainty.
Second, we argue that the persistent black-box nature of LLMs -- stemming from
their immense scale, architectural complexity, and development practices often
prioritizing empirical performance over theoretical interpretability -- renders
closed-form or purely mechanistic analyses generally intractable, thereby
necessitating statistical approaches due to their flexibility and often
demonstrated effectiveness. To substantiate these arguments, the paper outlines
several research areas -- including alignment, watermarking, uncertainty
quantification, evaluation, and data mixture optimization -- where statistical
methodologies are critically needed and are already beginning to make valuable
contributions. We conclude with a discussion suggesting that statistical
research concerning LLMs will likely form a diverse ``mosaic'' of specialized
topics rather than deriving from a single unifying theory, and highlighting the
importance of timely engagement by our statistics community in LLM research.

</details>


### [869] [Cellwise and Casewise Robust Covariance in High Dimensions](https://arxiv.org/abs/2505.19925)
*Fabio Centofanti,Mia Hubert,Peter J. Rousseeuw*

Main category: stat.ME

TL;DR: 提出cellRCov方法，一种能同时处理案例和单元格异常值及缺失数据的稳健协方差估计器，并扩展至cellRCCA方法用于稳健正则化典型相关分析。


<details>
  <summary>Details</summary>
Motivation: 传统样本协方差矩阵对异常值敏感，现有稳健估计器计算仅适用于20维以下数据，需解决高维场景下的稳健估计问题。

Method: 基于主成分和正交子空间的协方差分解，结合稳健PCA和岭正则化技术构建cellRCov方法。

Result: 理论证明其影响函数性质及一致性，仿真显示在污染和缺失数据中表现优越，实际异常检测应用验证有效性。

Conclusion: cellRCov突破了现有方法维度限制，为高维异常数据提供可靠分析工具，衍生方法cellRCCA扩展了应用场景。

Abstract: The sample covariance matrix is a cornerstone of multivariate statistics, but
it is highly sensitive to outliers. These can be casewise outliers, such as
cases belonging to a different population, or cellwise outliers, which are
deviating cells (entries) of the data matrix. Recently some robust covariance
estimators have been developed that can handle both types of outliers, but
their computation is only feasible up to at most 20 dimensions. To remedy this
we propose the cellRCov method, a robust covariance estimator that
simultaneously handles casewise outliers, cellwise outliers, and missing data.
It relies on a decomposition of the covariance on principal and orthogonal
subspaces, leveraging recent work on robust PCA. It also employs a ridge-type
regularization to stabilize the estimated covariance matrix. We establish some
theoretical properties of cellRCov, including its casewise and cellwise
influence functions as well as consistency and asymptotic normality. A
simulation study demonstrates the superior performance of cellRCov in
contaminated and missing data scenarios. Furthermore, its practical utility is
illustrated in a real-world application to anomaly detection. We also construct
and illustrate the cellRCCA method for robust and regularized canonical
correlation analysis.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [870] [SRDiffusion: Accelerate Video Diffusion Inference via Sketching-Rendering Cooperation](https://arxiv.org/abs/2505.19151)
*Shenggan Cheng,Yuanxin Wei,Lansong Diao,Yong Liu,Bujiao Chen,Lianghua Huang,Yu Liu,Wenyuan Yu,Jiangsu Du,Wei Lin,Yang You*

Main category: cs.GR

TL;DR: SRDiffusion通过大模型与小模型协作降低视频生成的计算成本，在保持质量的同时实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 尽管基于扩散模型的视频生成技术取得了进展，但其计算成本高，尤其是高分辨率、长视频的生成。现有加速方法常以质量下降为代价。

Method: 提出SRDiffusion框架，大模型处理高噪声步骤以确保语义和运动保真度（草图阶段），小模型在低噪声步骤中优化视觉细节（渲染阶段）。

Result: 实验显示，该方法在Wan上实现3倍加速且质量几乎无损，在CogVideoX上实现2倍加速，优于现有方法。

Conclusion: SRDiffusion作为一种与现有加速策略正交的新方向，为可扩展视频生成提供了实用解决方案。

Abstract: Leveraging the diffusion transformer (DiT) architecture, models like Sora,
CogVideoX and Wan have achieved remarkable progress in text-to-video,
image-to-video, and video editing tasks. Despite these advances,
diffusion-based video generation remains computationally intensive, especially
for high-resolution, long-duration videos. Prior work accelerates its inference
by skipping computation, usually at the cost of severe quality degradation. In
this paper, we propose SRDiffusion, a novel framework that leverages
collaboration between large and small models to reduce inference cost. The
large model handles high-noise steps to ensure semantic and motion fidelity
(Sketching), while the smaller model refines visual details in low-noise steps
(Rendering). Experimental results demonstrate that our method outperforms
existing approaches, over 3$\times$ speedup for Wan with nearly no quality loss
for VBench, and 2$\times$ speedup for CogVideoX. Our method is introduced as a
new direction orthogonal to existing acceleration strategies, offering a
practical solution for scalable video generation.

</details>


### [871] [CageNet: A Meta-Framework for Learning on Wild Meshes](https://arxiv.org/abs/2505.18772)
*Michal Edelstein,Hsueh-Ti Derek Liu,Mirela Ben-Chen*

Main category: cs.GR

TL;DR: 该论文提出了一种基于笼状几何的可配置元框架，用于处理非流形、多组件等复杂三角网格数据，提升了分割和蒙皮权重学习的性能。


<details>
  <summary>Details</summary>
Motivation: 现有通用三角网格框架难以处理具有多组件、非流形元素或连接性破坏的‘野生’网格数据，限制了其应用范围。

Method: 通过笼状几何（单组件流形网格）包裹目标网格，利用广义重心坐标在笼与网格间映射函数，实现数据适配和学习。

Result: 在复杂网格数据上的分割和蒙皮权重学习任务中，性能优于现有技术。

Conclusion: 该框架扩展了通用网格学习的适用性，为‘野生’网格处理提供了有效解决方案。

Abstract: Learning on triangle meshes has recently proven to be instrumental to a
myriad of tasks, from shape classification, to segmentation, to deformation and
animation, to mention just a few. While some of these applications are tackled
through neural network architectures which are tailored to the application at
hand, many others use generic frameworks for triangle meshes where the only
customization required is the modification of the input features and the loss
function. Our goal in this paper is to broaden the applicability of these
generic frameworks to "wild", i.e. meshes in-the-wild which often have multiple
components, non-manifold elements, disrupted connectivity, or a combination of
these. We propose a configurable meta-framework based on the concept of caged
geometry: Given a mesh, a cage is a single component manifold triangle mesh
that envelopes it closely. Generalized barycentric coordinates map between
functions on the cage, and functions on the mesh, allowing us to learn and test
on a variety of data, in different applications. We demonstrate this concept by
learning segmentation and skinning weights on difficult data, achieving better
performance to state of the art techniques on wild meshes.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [872] [Human-Centered AI Communication in Co-Creativity: An Initial Framework and Insights](https://arxiv.org/abs/2505.18385)
*Jeba Rezwana,Corey Ford*

Main category: cs.HC

TL;DR: 该论文提出了一个名为FAICO的框架，旨在改善人类与AI在共创过程中的沟通效果，通过文献综述和焦点小组研究，提出了以用户为中心的AI沟通设计初步指南。


<details>
  <summary>Details</summary>
Motivation: 当前许多共创AI系统缺乏有效的沟通机制，限制了其协作潜力。为了提升人类与AI在共创活动中的沟通效果，本研究旨在开发一个系统性的沟通框架。

Method: 通过系统综述107篇论文，初步设计了FAICO框架，并邀请AI、HCI和设计领域的专家参与焦点小组研究，收集反馈以优化框架。

Result: 研究发现，参与者更倾向于人类与AI之间的反馈循环而非线性沟通，并强调了上下文在促进相互理解中的重要性。基于这些发现，提出了FAICO框架的实际应用策略。

Conclusion: FAICO框架是开发以人为中心的AI沟通设计综合指南的第一步，未来研究将进一步探索其在共创领域的应用。

Abstract: Effective communication between AI and humans is essential for successful
human-AI co-creation. However, many current co-creative AI systems lack
effective communication, which limits their potential for collaboration. This
paper presents the initial design of the Framework for AI Communication (FAICO)
for co-creative AI, developed through a systematic review of 107 full-length
papers. FAICO presents key aspects of AI communication and their impact on user
experience, offering preliminary guidelines for designing human-centered AI
communication. To improve the framework, we conducted a preliminary study with
two focus groups involving skilled individuals in AI, HCI, and design. These
sessions sought to understand participants' preferences for AI communication,
gather their perceptions of the framework, collect feedback for refinement, and
explore its use in co-creative domains like collaborative writing and design.
Our findings reveal a preference for a human-AI feedback loop over linear
communication and emphasize the importance of context in fostering mutual
understanding. Based on these insights, we propose actionable strategies for
applying FAICO in practice and future directions, marking the first step toward
developing comprehensive guidelines for designing effective human-centered AI
communication in co-creation.

</details>


### [873] [From Reddit to Generative AI: Evaluating Large Language Models for Anxiety Support Fine-tuned on Social Media Data](https://arxiv.org/abs/2505.18464)
*Ugur Kursuncu,Trilok Padhi,Gaurav Sinha,Abdulkadir Erol,Jaya Krishna Mandivarapu,Christopher R. Larrison*

Main category: cs.HC

TL;DR: 研究评估了大型语言模型（GPT和Llama）在焦虑支持中的潜力，发现微调能提升语言质量但增加毒性和偏见，GPT总体更支持但同理心有限。


<details>
  <summary>Details</summary>
Motivation: 由于心理健康支持需求增加、专业人员短缺和访问障碍，研究探索利用大型语言模型提供实时、可扩展的焦虑支持。

Method: 使用Reddit的r/Anxiety板块真实用户发帖作为提示和微调数据，采用混合方法评估框架，涵盖语言质量、安全信任度和支持性三个维度。

Result: 微调提升了语言质量，但增加了毒性和偏见，降低了情感响应。GPT总体支持性更强，但所有模型同理心有限。

Conclusion: 未经处理的社交媒体数据微调LLMs存在风险，需制定缓解策略。

Abstract: The growing demand for accessible mental health support, compounded by
workforce shortages and logistical barriers, has led to increased interest in
utilizing Large Language Models (LLMs) for scalable and real-time assistance.
However, their use in sensitive domains such as anxiety support remains
underexamined. This study presents a systematic evaluation of LLMs (GPT and
Llama) for their potential utility in anxiety support by using real
user-generated posts from the r/Anxiety subreddit for both prompting and
fine-tuning. Our approach utilizes a mixed-method evaluation framework
incorporating three main categories of criteria: (i) linguistic quality, (ii)
safety and trustworthiness, and (iii) supportiveness. Results show that
fine-tuning LLMs with naturalistic anxiety-related data enhanced linguistic
quality but increased toxicity and bias, and diminished emotional
responsiveness. While LLMs exhibited limited empathy, GPT was evaluated as more
supportive overall. Our findings highlight the risks of fine-tuning LLMs on
unprocessed social media content without mitigation strategies.

</details>


### [874] [It's Not Just Labeling -- A Research on LLM Generated Feedback Interpretability and Image Labeling Sketch Features](https://arxiv.org/abs/2505.19419)
*Baichuan Li,Larry Powell,Tracy Hammond*

Main category: cs.HC

TL;DR: 论文提出了一种基于草图标注和大型语言模型（LLM）的方法，旨在降低技术门槛并提升标注的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 训练数据的质量对机器学习应用至关重要，但传统标注方法依赖专家且耗时，缺乏反馈。本研究旨在通过草图标注和LLM简化非专家的标注流程。

Method: 研究使用合成数据集，分析草图识别特征与LLM反馈指标的关系，并探索提示策略和草图变化对反馈质量的影响。

Result: 开发了一个基于草图的虚拟助手，提升了标注的可扩展性、可访问性和可解释性。

Conclusion: 该方法为非专家提供了简化的标注工具，并推动了LLM辅助标注工具的发展。

Abstract: The quality of training data is critical to the performance of machine
learning applications in domains like transportation, healthcare, and robotics.
Accurate image labeling, however, often relies on time-consuming, expert-driven
methods with limited feedback. This research introduces a sketch-based
annotation approach supported by large language models (LLMs) to reduce
technical barriers and enhance accessibility. Using a synthetic dataset, we
examine how sketch recognition features relate to LLM feedback metrics, aiming
to improve the reliability and interpretability of LLM-assisted labeling. We
also explore how prompting strategies and sketch variations influence feedback
quality. Our main contribution is a sketch-based virtual assistant that
simplifies annotation for non-experts and advances LLM-driven labeling tools in
terms of scalability, accessibility, and explainability.

</details>


### [875] [Fairness Practices in Industry: A Case Study in Machine Learning Teams Building Recommender Systems](https://arxiv.org/abs/2505.19441)
*Jing Nathan Yan,Junxiong Wang,Jeffrey M. Rzeszotarski,Allison Koenecke*

Main category: cs.HC

TL;DR: 该研究探讨了行业从业者如何应对推荐系统中的公平性问题，发现他们更倾向于多维去偏方法，并依赖直觉而非学术指标，同时面临组织与个人角色的平衡挑战。


<details>
  <summary>Details</summary>
Motivation: 随着推荐系统的快速普及，解决其固有偏见需要强大的公平性实践。然而，由于公平性标准和最佳实践的不断演变，评估公平性具有挑战性。本文旨在分析行业从业者如何感知和整合这些变化的公平性标准。

Method: 通过对11位来自大型科技公司技术团队的从业者进行半结构化访谈，研究了推荐系统产品中公平性的行业实现方式，重点关注去偏实践、应用指标、协作策略以及学术研究与实践的结合。

Result: 研究发现，从业者偏好多维去偏而非传统人口统计方法，并依赖直觉而非学术指标。同时，研究还揭示了在平衡公平性与从业者个人角色及组织约束（包括与法律和合规专家的互动）方面的困难。

Conclusion: 研究为推荐系统社区和算法公平性从业者提供了可操作的建议，强调了持续改进公平性实践的必要性。

Abstract: The rapid proliferation of recommender systems necessitates robust fairness
practices to address inherent biases. Assessing fairness, though, is
challenging due to constantly evolving metrics and best practices. This paper
analyzes how industry practitioners perceive and incorporate these changing
fairness standards in their workflows. Through semi-structured interviews with
11 practitioners from technical teams across a range of large technology
companies, we investigate industry implementations of fairness in
recommendation system products. We focus on current debiasing practices,
applied metrics, collaborative strategies, and integrating academic research
into practice. Findings show a preference for multi-dimensional debiasing over
traditional demographic methods, and a reliance on intuitive rather than
academic metrics. This study also highlights the difficulties in balancing
fairness with both the practitioner's individual (bottom-up) roles and
organizational (top-down) workplace constraints, including the interplay with
legal and compliance experts. Finally, we offer actionable recommendations for
the recommender system community and algorithmic fairness practitioners,
underlining the need to refine fairness practices continually.

</details>


### [876] [On the Same Page: Dimensions of Perceived Shared Understanding in Human-AI Interaction](https://arxiv.org/abs/2505.20068)
*Qingyu Liang,Jaime Banks*

Main category: cs.HC

TL;DR: 该研究探讨了人机交互中共享理解的构建，通过调查发现八个关键维度。


<details>
  <summary>Details</summary>
Motivation: 随着AI越来越多地融入人类生活和工作场景，理解人机交互中的共享感知对提升交互效果至关重要。现有研究主要关注人人交互，而对人机交互中的共享理解研究不足。

Method: 采用在线问卷调查收集用户与大型语言模型交互时的反思数据，并通过归纳式主题分析法进行分析。

Result: 识别出构成人机交互中共享理解的八个维度：流畅性、操作一致性、灵活性、结果满意度、情境意识、非人类能力缺陷、计算限制和怀疑态度。

Conclusion: 研究揭示了人机交互中共享理解的多维特性，为未来设计更自然的人机协作系统提供了理论基础。

Abstract: Shared understanding plays a key role in the effective communication in and
performance of human-human interactions. With the increasingly common
integration of AI into human contexts, the future of personal and workplace
interactions will likely see human-AI interaction (HAII) in which the
perception of shared understanding is important. Existing literature has
addressed the processes and effects of PSU in human-human interactions, but the
construal remains underexplored in HAII. To better understand PSU in HAII, we
conducted an online survey to collect user reflections on interactions with a
large language model when it sunderstanding of a situation was thought to be
similar to or different from the participant's. Through inductive thematic
analysis, we identified eight dimensions comprising PSU in human-AI
interactions: Fluency, aligned operation, fluidity, outcome satisfaction,
contextual awareness, lack of humanlike abilities, computational limits, and
suspicion.

</details>


### [877] [Explanation User Interfaces: A Systematic Literature Review](https://arxiv.org/abs/2505.20085)
*Eleonora Cappuccio,Andrea Esposito,Francesco Greco,Giuseppe Desolda,Rosa Lanzilotti,Salvatore Rinzivillo*

Main category: cs.HC

TL;DR: 该论文通过系统文献综述研究了解释性用户界面（XUIs），并提出了一个名为HERMES的框架，以指导XUIs的设计和评估。


<details>
  <summary>Details</summary>
Motivation: 人工智能（AI）的决策过程通常不透明，开发者使用可解释AI（XAI）技术来提升系统的透明度、公平性和可信度。然而，向用户呈现解释往往被忽视，导致AI系统对终端用户不实用。

Method: 采用系统文献综述方法，分析学术文献中关于解释性用户界面的解决方案和设计指南。

Result: 提出了一个名为HERMES的框架，旨在指导从业者和学者设计和评估解释性用户界面。

Conclusion: 通过系统研究和框架提出，论文为提升AI系统的用户友好性和实用性提供了重要指导。

Abstract: Artificial Intelligence (AI) is one of the major technological advancements
of this century, bearing incredible potential for users through AI-powered
applications and tools in numerous domains. Being often black-box (i.e., its
decision-making process is unintelligible), developers typically resort to
eXplainable Artificial Intelligence (XAI) techniques to interpret the behaviour
of AI models to produce systems that are transparent, fair, reliable, and
trustworthy. However, presenting explanations to the user is not trivial and is
often left as a secondary aspect of the system's design process, leading to AI
systems that are not useful to end-users. This paper presents a Systematic
Literature Review on Explanation User Interfaces (XUIs) to gain a deeper
understanding of the solutions and design guidelines employed in the academic
literature to effectively present explanations to users. To improve the
contribution and real-world impact of this survey, we also present a framework
for Human-cEnteRed developMent of Explainable user interfaceS (HERMES) to guide
practitioners and academics in the design and evaluation of XUIs.

</details>


<div id='math.CT'></div>

# math.CT [[Back]](#toc)

### [878] [Alpay Algebra III: Observer-Coupled Collapse and the Temporal Drift of Identity](https://arxiv.org/abs/2505.19790)
*Faruk Alpay*

Main category: math.CT

TL;DR: 该论文基于Alpay代数，提出了一个形式化框架，用于建模观察者依赖的崩溃动态和时间身份漂移，通过符号固定点结构提供可证明的追溯性和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 传统身份建模在可解释AI（XAI）中存在局限，论文旨在通过动态身份机制解决这一问题，尤其是在观察者影响下身份的动态变化。

Method: 利用Alpay代数的固定点涌现结构，通过超限范畴流和曲率驱动的身份算子，形式化了观察者耦合的φ-崩溃过程，并定义了时间漂移机制。

Result: 提出的系统在可解释AI中超越了传统身份建模，提供了符号固定点结构，为未来具有稳定自引用行为的AI系统奠定了数学基础。

Conclusion: Alpay代数作为下一代符号框架，连接了范畴论、身份逻辑和观察者动态，为AI自我意识架构和形式逻辑系统提供了新的理论基础。

Abstract: This paper introduces a formal framework for modeling observer-dependent
collapse dynamics and temporal identity drift within artificial and
mathematical systems, grounded entirely in the symbolic foundations of Alpay
Algebra. Building upon the fixed-point emergence structures developed in Alpay
Algebra I and II, this third installment formalizes the observer-coupled
{\phi}-collapse process through transfinite categorical flows and
curvature-driven identity operators. We define a novel temporal drift mechanism
as a recursive deformation of identity signatures under entangled observer
influence, constructing categorical invariants that evolve across fold
iterations. The proposed system surpasses conventional identity modeling in
explainable AI (XAI) by encoding internal transformation history into a
symbolic fixed-point structure, offering provable traceability and temporal
coherence. Applications range from AI self-awareness architectures to formal
logic systems where identity is not static but dynamically induced by
observation. The theoretical results also offer a mathematically rigorous basis
for future AI systems with stable self-referential behavior, positioning Alpay
Algebra as a next-generation symbolic framework bridging category theory,
identity logic, and observer dynamics.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [879] [Community Moderation and the New Epistemology of Fact Checking on Social Media](https://arxiv.org/abs/2505.20067)
*Isabelle Augenstein,Michiel Bakker,Tanmoy Chakraborty,David Corney,Emilio Ferrara,Iryna Gurevych,Scott Hale,Eduard Hovy,Heng Ji,Irene Larraz,Filippo Menczer,Preslav Nakov,Paolo Papotti,Dhruv Sahnan,Greta Warren,Giovanni Zagni*

Main category: cs.SI

TL;DR: 论文探讨了社交媒体平台从专业事实核查转向社区驱动的内容审核（如Community Notes）的转变，分析了其潜力与挑战，并强调专业核查的不可替代性。


<details>
  <summary>Details</summary>
Motivation: 研究社交媒体平台如何通过社区驱动的审核机制（如众包事实核查）应对错误信息，同时探讨其与传统专业核查的互补关系。

Method: 系统性地分析主要平台现有的错误信息检测方法，并评估社区驱动审核的兴起及其规模化应用的潜力与问题。

Result: 社区驱动的审核虽能提升错误信息处理的规模和速度，但因公众对真相的认知受偏见、政治倾向和文化背景影响，难以完全替代专业核查。

Conclusion: 社区驱动的错误信息审核是有价值的补充，但专业事实核查的作用不可或缺，两者需协同发挥作用。

Abstract: Social media platforms have traditionally relied on internal moderation teams
and partnerships with independent fact-checking organizations to identify and
flag misleading content. Recently, however, platforms including X (formerly
Twitter) and Meta have shifted towards community-driven content moderation by
launching their own versions of crowd-sourced fact-checking -- Community Notes.
If effectively scaled and governed, such crowd-checking initiatives have the
potential to combat misinformation with increased scale and speed as
successfully as community-driven efforts once did with spam. Nevertheless,
general content moderation, especially for misinformation, is inherently more
complex. Public perceptions of truth are often shaped by personal biases,
political leanings, and cultural contexts, complicating consensus on what
constitutes misleading content. This suggests that community efforts, while
valuable, cannot replace the indispensable role of professional fact-checkers.
Here we systemically examine the current approaches to misinformation detection
across major platforms, explore the emerging role of community-driven
moderation, and critically evaluate both the promises and challenges of
crowd-checking at scale.

</details>


### [880] [Homophily Enhanced Graph Domain Adaptation](https://arxiv.org/abs/2505.20089)
*Ruiyi Fang,Bingheng Li,Jingyu Zhao,Ruizhi Pu,Qiuhao Zeng,Gezheng Xu,Charles Ling,Boyu Wang*

Main category: cs.SI

TL;DR: 本文提出了一种新的图同配性对齐算法，通过混合滤波器平滑图信号，有效解决图域自适应中的同配性差异问题。


<details>
  <summary>Details</summary>
Motivation: 现有图域自适应方法忽视了图同配性这一关键因素，而同配性差异会影响模型性能，因此需要解决这一问题。

Method: 提出了一种基于混合滤波器的图同配性对齐算法，用于平滑图信号并减少同配性差异。

Result: 在多个基准测试上的实验结果表明，该方法能有效提升图域自适应的性能。

Conclusion: 图同配性对齐对图域自适应至关重要，所提方法能有效解决同配性差异问题并提升性能。

Abstract: Graph Domain Adaptation (GDA) transfers knowledge from labeled source graphs
to unlabeled target graphs, addressing the challenge of label scarcity. In this
paper, we highlight the significance of graph homophily, a pivotal factor for
graph domain alignment, which, however, has long been overlooked in existing
approaches. Specifically, our analysis first reveals that homophily
discrepancies exist in benchmarks. Moreover, we also show that homophily
discrepancies degrade GDA performance from both empirical and theoretical
aspects, which further underscores the importance of homophily alignment in
GDA. Inspired by this finding, we propose a novel homophily alignment algorithm
that employs mixed filters to smooth graph signals, thereby effectively
capturing and mitigating homophily discrepancies between graphs. Experimental
results on a variety of benchmarks verify the effectiveness of our method.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [881] [Towards medical AI misalignment: a preliminary study](https://arxiv.org/abs/2505.18212)
*Barbara Puccio,Federico Castagna,Allan Tucker,Pierangelo Veltri*

Main category: cs.CY

TL;DR: 研究发现，大型语言模型（LLM）即使性能强大，仍可能通过角色扮演（'Goofy Game'）被恶意用户绕过安全防护，生成错误甚至有害的临床建议。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM作为辅助工具表现出色，但恶意用户可能利用角色扮演绕过其安全措施，导致潜在危险内容生成，尤其在医疗等领域。本文旨在揭示这一漏洞。

Method: 通过初步探索性研究，分析恶意用户如何无需技术知识即可构建角色扮演提示，诱导LLM生成错误临床建议。

Result: 研究发现，角色扮演提示能有效绕过当前LLM防护机制，生成不准确且可能有害的临床建议。

Conclusion: 该研究揭示了LLM在角色扮演攻击下的脆弱性，为未来安全改进提供了方向。

Abstract: Despite their staggering capabilities as assistant tools, often exceeding
human performances, Large Language Models (LLMs) are still prone to jailbreak
attempts from malevolent users. Although red teaming practices have already
identified and helped to address several such jailbreak techniques, one
particular sturdy approach involving role-playing (which we named `Goofy Game')
seems effective against most of the current LLMs safeguards. This can result in
the provision of unsafe content, which, although not harmful per se, might lead
to dangerous consequences if delivered in a setting such as the medical domain.
In this preliminary and exploratory study, we provide an initial analysis of
how, even without technical knowledge of the internal architecture and
parameters of generative AI models, a malicious user could construct a
role-playing prompt capable of coercing an LLM into producing incorrect (and
potentially harmful) clinical suggestions. We aim to illustrate a specific
vulnerability scenario, providing insights that can support future advancements
in the field.

</details>


### [882] [AIDRIN 2.0: A Framework to Assess Data Readiness for AI](https://arxiv.org/abs/2505.18213)
*Kaveen Hiniduma,Dylan Ryan,Suren Byna,Jean Luca Bez,Ravi Madduri*

Main category: cs.CY

TL;DR: AIDRIN框架通过改进用户界面和集成隐私保护的联邦学习，提升AI数据准备评估的实用性和可访问性。


<details>
  <summary>Details</summary>
Motivation: 为了解决AI应用中数据准备不足的问题，特别是在数据质量、偏见、公平性和隐私方面的挑战，AIDRIN框架被提出并进一步优化。

Method: 通过改进用户界面（UI）和集成隐私保护的联邦学习（PPFL）框架，提升AIDRIN的实用性和可访问性。

Result: 案例研究表明，AIDRIN能够有效识别影响AI模型性能的数据准备问题。

Conclusion: AIDRIN框架通过UI改进和PPFL集成，显著提升了在联邦学习环境中数据准备和隐私保护的优先级。

Abstract: AI Data Readiness Inspector (AIDRIN) is a framework to evaluate and improve
data preparedness for AI applications. It addresses critical data readiness
dimensions such as data quality, bias, fairness, and privacy. This paper
details enhancements to AIDRIN by focusing on user interface improvements and
integration with a privacy-preserving federated learning (PPFL) framework. By
refining the UI and enabling smooth integration with decentralized AI
pipelines, AIDRIN becomes more accessible and practical for users with varying
technical expertise. Integrating with an existing PPFL framework ensures that
data readiness and privacy are prioritized in federated learning environments.
A case study involving a real-world dataset demonstrates AIDRIN's practical
value in identifying data readiness issues that impact AI model performance.

</details>


### [883] [Navigating Pitfalls: Evaluating LLMs in Machine Learning Programming Education](https://arxiv.org/abs/2505.18220)
*Smitha Kumar,Michael A. Lones,Manuel Maarek,Hind Zantout*

Main category: cs.CY

TL;DR: 研究评估了四种大语言模型（LLM）在机器学习教育中识别代码常见错误及提供学习反馈的能力，发现模型对早期流程错误和模型选择问题识别有限，但反馈质量尚可，且开源与闭源模型差距较小。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型在机器学习教育中辅助识别代码错误和提供学习指导的潜力，特别关注模型对常见陷阱的识别能力及其教育应用价值。

Method: 使用包含常见错误的代码样本集，测试1个闭源模型和3个开源LLM的陷阱识别与反馈生成能力，重点分析ML流程早期错误和模型选择问题。

Result: 所有模型均能识别基础错误，但对ML流程早期信息泄露等关键错误识别率低，模型选择相关错误识别能力有限；反馈内容具有可操作性；开源与闭源模型性能差距小于预期。

Conclusion: 当前LLM在ML教育支持中存在局限性，但反馈质量显示其潜在指导价值；开源模型的较小性能差距为低成本教育应用提供可能，需警惕商业模型的数据风险。

Abstract: The rapid advancement of Large Language Models (LLMs) has opened new avenues
in education. This study examines the use of LLMs in supporting learning in
machine learning education; in particular, it focuses on the ability of LLMs to
identify common errors of practice (pitfalls) in machine learning code, and
their ability to provide feedback that can guide learning. Using a portfolio of
code samples, we consider four different LLMs: one closed model and three open
models. Whilst the most basic pitfalls are readily identified by all models,
many common pitfalls are not. They particularly struggle to identify pitfalls
in the early stages of the ML pipeline, especially those which can lead to
information leaks, a major source of failure within applied ML projects. They
also exhibit limited success at identifying pitfalls around model selection,
which is a concept that students often struggle with when first transitioning
from theory to practice. This questions the use of current LLMs to support
machine learning education, and also raises important questions about their use
by novice practitioners. Nevertheless, when LLMs successfully identify pitfalls
in code, they do provide feedback that includes advice on how to proceed,
emphasising their potential role in guiding learners. We also compare the
capability of closed and open LLM models, and find that the gap is relatively
small given the large difference in model sizes. This presents an opportunity
to deploy, and potentially customise, smaller more efficient LLM models within
education, avoiding risks around cost and data sharing associated with
commercial models.

</details>


### [884] [From Bias to Accountability: How the EU AI Act Confronts Challenges in European GeoAI Auditing](https://arxiv.org/abs/2505.18236)
*Natalia Matuszczyk,Craig R. Barnes,Rohit Gupta,Bulent Ozel,Aniket Mitra*

Main category: cs.CY

TL;DR: 该论文综述了地理空间人工智能(GeoAI)中的偏见问题，将其与欧盟AI法案关联，指出常见偏见机制并强调高风险系统需进行常规偏见审核。


<details>
  <summary>Details</summary>
Motivation: 现有关于GeoAI偏见的研究分散且聚焦狭窄，需整合文献并与欧盟AI法案的审计义务相结合，以系统性解决偏见问题。

Method: 通过综合碎片化文献，分析重复出现的偏见机制（如代表性、算法和聚合偏见），并将其映射到欧盟AI法案的具体条款中。

Result: 研究表明广泛部署的GeoAI应用属于高风险系统，需在2027年法案全面生效前进行常规偏见审核，并提供了检测偏见的具体方法示例。

Conclusion: 即使数据集经过良好整理，仍需在欧盟AI法案全面实施前进行常规偏见审核，该研究首次将GeoAI偏见证据与法案背景相结合。

Abstract: Bias in geospatial artificial intelligence (GeoAI) models has been
documented, yet the evidence is scattered across narrowly focused studies. We
synthesize this fragmented literature to provide a concise overview of bias in
GeoAI and examine how the EU's Artificial Intelligence Act (EU AI Act) shapes
audit obligations. We discuss recurring bias mechanisms, including
representation, algorithmic and aggregation bias, and map them to specific
provisions of the EU AI Act. By applying the Act's high-risk criteria, we
demonstrate that widely deployed GeoAI applications qualify as high-risk
systems. We then present examples of recent audits along with an outline of
practical methods for detecting bias. As far as we know, this study represents
the first integration of GeoAI bias evidence into the EU AI Act context, by
identifying high-risk GeoAI systems and mapping bias mechanisms to the Act's
Articles. Although the analysis is exploratory, it suggests that even
well-curated European datasets should employ routine bias audits before 2027,
when the AI Act's high-risk provisions take full effect.

</details>


### [885] [Military AI Needs Technically-Informed Regulation to Safeguard AI Research and its Applications](https://arxiv.org/abs/2505.18371)
*Riley Simmons-Edler,Jean Dong,Paul Lushenko,Kanaka Rajan,Ryan P. Badman*

Main category: cs.CY

TL;DR: 论文探讨了AI增强的致命自主武器系统（AI-LAWS）带来的新型风险，提出基于技术行为的定义和政策建议，呼吁AI研究者参与监管。


<details>
  <summary>Details</summary>
Motivation: 近年来AI在军事武器系统和指挥控制基础设施中的快速发展，但其对作战系统、军事决策和战争规范的社会技术影响研究不足，尤其是AI-LAWS带来的独特风险。

Method: 通过聚焦使用AI进行目标锁定或战场决策的致命自主武器系统子集（AI-LAWS），提出基于技术行为的明确定义，并分析其风险。

Result: AI-LAWS可能引发意外升级、陌生环境可靠性差及削弱人类监督等风险，现有框架未能将其与传统LAWS区分，需技术驱动的监管。

Conclusion: 需AI研究者全程参与监管，提出技术导向的政策方向，并建立区分AI-LAWS的清晰定义作为监管基础。

Abstract: Military weapon systems and command-and-control infrastructure augmented by
artificial intelligence (AI) have seen rapid development and deployment in
recent years. However, the sociotechnical impacts of AI on combat systems,
military decision-making, and the norms of warfare have been understudied. We
focus on a specific subset of lethal autonomous weapon systems (LAWS) that use
AI for targeting or battlefield decisions. We refer to this subset as
AI-powered lethal autonomous weapon systems (AI-LAWS) and argue that they
introduce novel risks -- including unanticipated escalation, poor reliability
in unfamiliar environments, and erosion of human oversight -- all of which
threaten both military effectiveness and the openness of AI research. These
risks cannot be addressed by high-level policy alone; effective regulation must
be grounded in the technical behavior of AI models. We argue that AI
researchers must be involved throughout the regulatory lifecycle. Thus, we
propose a clear, behavior-based definition of AI-LAWS -- systems that introduce
unique risks through their use of modern AI -- as a foundation for technically
grounded regulation, given that existing frameworks do not distinguish them
from conventional LAWS. Using this definition, we propose several
technically-informed policy directions and invite greater participation from
the AI research community in military AI policy discussions.

</details>


### [886] [Will Large Language Models Transform Clinical Prediction?](https://arxiv.org/abs/2505.18246)
*Yusuf Yildiz,Goran Nenadic,Meghna Jani,David A. Jenkins*

Main category: cs.CY

TL;DR: 本文探讨了大型语言模型（LLMs）在临床预测中的应用，强调其潜力与挑战，并指出需进一步研究验证、公平性、生存分析和法规制定。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗领域的应用日益受到关注，因其能有效总结数据、精准回答问题并生成文本，但其在临床预测中的整合仍需更多研究。

Method: 通过评论性分析，探讨LLMs在临床预测中的使用，重点关注方法扩展、验证、公平性评估及法规发展。

Result: 研究发现LLMs在临床预测中具有潜力，但需解决验证、偏见、生存分析和监管等关键问题。

Conclusion: 结论指出，需进一步工作和领域特定考量，以实现LLMs在临床预测工作流程中的全面整合。

Abstract: Background: Large language models (LLMs) are attracting increasing interest
in healthcare. Their ability to summarise large datasets effectively, answer
questions accurately, and generate synthesised text is widely recognised. These
capabilities are already finding applications in healthcare. Body: This
commentary discusses LLMs usage in the clinical prediction context and
highlight potential benefits and existing challenges. In these early stages,
the focus should be on extending the methodology, specifically on validation,
fairness and bias evaluation, survival analysis and development of regulations.
Conclusion: We conclude that further work and domain-specific considerations
need to be made for full integration into the clinical prediction workflows.

</details>


### [887] [Climate Implications of Diffusion-based Generative Visual AI Systems and their Mass Adoption](https://arxiv.org/abs/2505.18892)
*Vanessa Utz,Steve DiPaola*

Main category: cs.CY

TL;DR: 论文探讨了基于文本提示的扩散AI艺术系统因GPU高能耗对气候的影响，呼吁关注其大规模应用带来的能源消耗问题。


<details>
  <summary>Details</summary>
Motivation: 尽管区块链和加密货币挖矿的能源问题已被广泛研究，但扩散AI艺术系统因其主流消费者吸引力，其GPU使用对气候的影响尚未得到足够关注。

Method: 通过分析扩散视觉AI系统的增长模式、使用情况及其能源消耗，估算其对全球能源消耗的潜在贡献。

Result: 研究表明，这些工具的大规模采用可能显著增加全球能源消耗。

Conclusion: 论文提出需进一步研究解决方案，并指出公开数据不足等困难，强调对生成式AI系统气候影响的审慎考虑。

Abstract: Climate implications of rapidly developing digital technologies, such as
blockchains and the associated crypto mining and NFT minting, have been well
documented and their massive GPU energy use has been identified as a cause for
concern. However, we postulate that due to their more mainstream consumer
appeal, the GPU use of text-prompt based diffusion AI art systems also requires
thoughtful considerations. Given the recent explosion in the number of highly
sophisticated generative art systems and their rapid adoption by consumers and
creative professionals, the impact of these systems on the climate needs to be
carefully considered. In this work, we report on the growth of diffusion-based
visual AI systems, their patterns of use, growth and the implications on the
climate. Our estimates show that the mass adoption of these tools potentially
contributes considerably to global energy consumption. We end this paper with
our thoughts on solutions and future areas of inquiry as well as associated
difficulties, including the lack of publicly available data.

</details>


### [888] [Reality Check: A New Evaluation Ecosystem Is Necessary to Understand AI's Real World Effects](https://arxiv.org/abs/2505.18893)
*Reva Schwartz,Rumman Chowdhury,Akash Kundu,Heather Frase,Marzieh Fadaee,Tom David,Gabriella Waters,Afaf Taik,Morgan Briggs,Patrick Hall,Shomik Jain,Kyra Yee,Spencer Thomas,Sundeep Bhandari,Lee Wan Sie,Qinghua Lu,Matthew Holmes,Theodora Skeadas*

Main category: cs.CY

TL;DR: 传统AI评估方法局限于技术层面，难以衡量AI在现实部署中的长期社会影响。本文主张扩展评估范式，纳入上下文感知方法以捕捉AI的间接效应。


<details>
  <summary>Details</summary>
Motivation: 现有AI评估聚焦技术指标（如准确性、偏见），但忽视AI在真实场景中的二阶效应（如用户行为改变、社会经济影响）。随着AI深度融入社会，亟需系统性评估这些长期影响。

Method: 提出需要开发新数据和方法论：1) 支持上下文感知的动态测试；2) 建立能追踪实际使用场景中AI影响的评估框架。

Result: 论证了当前静态评估的局限性，并构建了评估AI二阶效应的需求框架，包括跨学科数据整合和实时监测能力。

Conclusion: 呼吁建立新生态系统，通过情境化评估范式捕捉AI的间接影响，为政策制定和风险管理提供依据。

Abstract: Conventional AI evaluation approaches concentrated within the AI stack
exhibit systemic limitations for exploring, navigating and resolving the human
and societal factors that play out in real world deployment such as in
education, finance, healthcare, and employment sectors. AI capability
evaluations can capture detail about first-order effects, such as whether
immediate system outputs are accurate, or contain toxic, biased or
stereotypical content, but AI's second-order effects, i.e. any long-term
outcomes and consequences that may result from AI use in the real world, have
become a significant area of interest as the technology becomes embedded in our
daily lives. These secondary effects can include shifts in user behavior,
societal, cultural and economic ramifications, workforce transformations, and
long-term downstream impacts that may result from a broad and growing set of
risks. This position paper argues that measuring the indirect and secondary
effects of AI will require expansion beyond static, single-turn approaches
conducted in silico to include testing paradigms that can capture what actually
materializes when people use AI technology in context. Specifically, we
describe the need for data and methods that can facilitate contextual awareness
and enable downstream interpretation and decision making about AI's secondary
effects, and recommend requirements for a new ecosystem.

</details>


### [889] [Language Models Surface the Unwritten Code of Science and Society](https://arxiv.org/abs/2505.18942)
*Honglin Bao,Siyang Wu,Jiwoong Choi,Yingrong Mao,James A. Evans*

Main category: cs.CY

TL;DR: 该论文提出利用大语言模型（LLMs）揭示人类社会中的隐性偏见和启发式规则，并通过科学评审案例展示了如何通过LLMs的自我对话来发现这些隐藏规则。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于不仅探讨人类偏见如何被大语言模型继承，还探索如何利用这些偏见使社会的“不成文规则”（如隐性刻板印象和启发式方法）变得可见并可供批判。

Method: 方法是通过科学评审案例研究，利用LLMs生成自洽假设来揭示评审者未明确表达的隐性规则，并在45个计算机科学会议的论文对中迭代搜索更深层次的假设。

Result: 研究发现LLMs的规范性先验（如理论严谨性）会系统性更新为强调外部连接的叙事（如工作如何定位和连接不同文献），揭示了科学神话中内在属性优于外在情境化的现象。

Conclusion: 结论是这一框架具有广泛适用性，可以利用LLMs作为诊断工具来揭示人类社会的隐性规则，从而更精准地实现负责任的人工智能。

Abstract: This paper calls on the research community not only to investigate how human
biases are inherited by large language models (LLMs) but also to explore how
these biases in LLMs can be leveraged to make society's "unwritten code" - such
as implicit stereotypes and heuristics - visible and accessible for critique.
We introduce a conceptual framework through a case study in science: uncovering
hidden rules in peer review - the factors that reviewers care about but rarely
state explicitly due to normative scientific expectations. The idea of the
framework is to push LLMs to speak out their heuristics through generating
self-consistent hypotheses - why one paper appeared stronger in reviewer
scoring - among paired papers submitted to 45 computer science conferences,
while iteratively searching deeper hypotheses from remaining pairs where
existing hypotheses cannot explain. We observed that LLMs' normative priors
about the internal characteristics of good science extracted from their
self-talk, e.g. theoretical rigor, were systematically updated toward
posteriors that emphasize storytelling about external connections, such as how
the work is positioned and connected within and across literatures. This shift
reveals the primacy of scientific myths about intrinsic properties driving
scientific excellence rather than extrinsic contextualization and storytelling
that influence conceptions of relevance and significance. Human reviewers tend
to explicitly reward aspects that moderately align with LLMs' normative priors
(correlation = 0.49) but avoid articulating contextualization and storytelling
posteriors in their review comments (correlation = -0.14), despite giving
implicit reward to them with positive scores. We discuss the broad
applicability of the framework, leveraging LLMs as diagnostic tools to surface
the tacit codes underlying human society, enabling more precisely targeted
responsible AI.

</details>


### [890] [EuroCon: Benchmarking Parliament Deliberation for Political Consensus Finding](https://arxiv.org/abs/2505.19558)
*Zhaowei Zhang,Minghua Yi,Mengmeng Wang,Fengshuo Bai,Zilong Zheng,Yipeng Kang,Yaodong Yang*

Main category: cs.CY

TL;DR: 论文引入EuroCon基准测试，评估大语言模型在欧洲议会多样化政治环境中达成共识的能力，发现现有模型在复杂任务上表现不足。


<details>
  <summary>Details</summary>
Motivation: 政治共识对社会治理至关重要，但前沿AI系统（如大语言模型）在此领域的能力尚未充分研究。

Method: 基于欧洲议会13年2225份高质量审议记录构建EuroCon基准，模拟不同议会设置（政治议题、目标、参与党派及权力结构），并开发评估框架验证模型生成的决议是否符合政治目标。

Result: 实验表明，即使是先进模型也难以完成三分之二多数通过决议或解决安全议题等复杂任务，但揭示了模型倾向于优先考虑主导党派立场等共识策略。

Conclusion: EuroCon能有效评估大语言模型的政治共识能力，为相关研究提供平台。

Abstract: Achieving political consensus is crucial yet challenging for the effective
functioning of social governance. However, although frontier AI systems
represented by large language models (LLMs) have developed rapidly in recent
years, their capabilities on this scope are still understudied. In this paper,
we introduce EuroCon, a novel benchmark constructed from 2,225 high-quality
deliberation records of the European Parliament over 13 years, ranging from
2009 to 2022, to evaluate the ability of LLMs to reach political consensus
among divergent party positions across diverse parliament settings.
Specifically, EuroCon incorporates four factors to build each simulated
parliament setting: specific political issues, political goals, participating
parties, and power structures based on seat distribution. We also develop an
evaluation framework for EuroCon to simulate real voting outcomes in different
parliament settings, assessing whether LLM-generated resolutions meet
predefined political goals. Our experimental results demonstrate that even
state-of-the-art models remain undersatisfied with complex tasks like passing
resolutions by a two-thirds majority and addressing security issues, while
revealing some common strategies LLMs use to find consensus under different
power structures, such as prioritizing the stance of the dominant party,
highlighting EuroCon's promise as an effective platform for studying LLMs'
ability to find political consensus.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [891] [Task-Optimized Convolutional Recurrent Networks Align with Tactile Processing in the Rodent Brain](https://arxiv.org/abs/2505.18361)
*Trinity Chung,Yuchen Shen,Nathan C. L. Kong,Aran Nayebi*

Main category: q-bio.NC

TL;DR: 该论文提出了一种新型的Encoder-Attender-Decoder（EAD）框架，用于探索基于真实触觉输入序列的任务优化时间神经网络，并发现卷积循环神经网络（ConvRNNs）在触觉分类中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 触觉感知在神经科学和人工系统中仍远未达到视觉和语言等成熟模态的水平。论文旨在通过提出一种新框架来填补这一空白，并探索触觉分类的最佳神经网络架构。

Method: 论文引入了Encoder-Attender-Decoder（EAD）框架，使用定制的啮齿动物触须阵列模拟器生成的触觉输入序列，训练任务优化的时间神经网络。特别关注卷积循环神经网络（ConvRNNs）作为编码器的性能。

Result: 基于ConvRNN编码器的EAD模型在触觉分类中表现优异，其神经表征与啮齿动物体感皮层高度匹配，且监督分类性能与神经对齐呈线性关系。自监督学习的ConvRNN编码器模型也能达到类似的神经拟合效果。

Conclusion: 研究强调了非线性循环处理在体感皮层通用触觉表征中的重要性，并为神经科学和人工智能提供了触觉感知的新见解和方法。

Abstract: Tactile sensing remains far less understood in neuroscience and less
effective in artificial systems compared to more mature modalities such as
vision and language. We bridge these gaps by introducing a novel
Encoder-Attender-Decoder (EAD) framework to systematically explore the space of
task-optimized temporal neural networks trained on realistic tactile input
sequences from a customized rodent whisker-array simulator. We identify
convolutional recurrent neural networks (ConvRNNs) as superior encoders to
purely feedforward and state-space architectures for tactile categorization.
Crucially, these ConvRNN-encoder-based EAD models achieve neural
representations closely matching rodent somatosensory cortex, saturating the
explainable neural variability and revealing a clear linear relationship
between supervised categorization performance and neural alignment.
Furthermore, contrastive self-supervised ConvRNN-encoder-based EADs, trained
with tactile-specific augmentations, match supervised neural fits, serving as
an ethologically-relevant, label-free proxy.
  For neuroscience, our findings highlight nonlinear recurrent processing as
important for general-purpose tactile representations in somatosensory cortex,
providing the first quantitative characterization of the underlying inductive
biases in this system. For embodied AI, our results emphasize the importance of
recurrent EAD architectures to handle realistic tactile inputs, along with
tailored self-supervised learning methods for achieving robust tactile
perception with the same type of sensors animals use to sense in unstructured
environments.

</details>


### [892] [Multi-modal brain encoding models for multi-modal stimuli](https://arxiv.org/abs/2505.20027)
*Subba Reddy Oota,Khushbu Pahwa,Mounika Marreddy,Maneesh Singh,Manish Gupta,Bapi S. Raju*

Main category: q-bio.NC

TL;DR: 多模态Transformer模型在预测视觉大脑活动方面表现出色，即使面对不一致的模态表示。本文探讨了这些模型在参与者接触多模态刺激时的预测准确性，并研究了大脑如何处理单模态与多模态信息。


<details>
  <summary>Details</summary>
Motivation: 研究多模态模型在预测大脑活动时的准确性，以了解大脑如何处理自然多模态刺激，以及如何在不同模态间分离和整合信息。

Method: 使用多种单模态和两种多模态模型（跨模态和联合预训练模型），通过fMRI数据研究参与者观看电影时的大脑活动。通过逐一移除单模态特征，分析多模态对齐中各模态的贡献。

Result: 多模态模型在多个语言和视觉区域显示出更好的对齐性。跨模态模型的对齐部分归因于视频模态，而联合预训练模型的对齐则部分归因于视频和音频模态。

Conclusion: 多模态模型为研究大脑多模态信息处理提供了新的视角，其可解释性有助于深化对大脑信息处理机制的理解。

Abstract: Despite participants engaging in unimodal stimuli, such as watching images or
silent videos, recent work has demonstrated that multi-modal Transformer models
can predict visual brain activity impressively well, even with incongruent
modality representations. This raises the question of how accurately these
multi-modal models can predict brain activity when participants are engaged in
multi-modal stimuli. As these models grow increasingly popular, their use in
studying neural activity provides insights into how our brains respond to such
multi-modal naturalistic stimuli, i.e., where it separates and integrates
information across modalities through a hierarchy of early sensory regions to
higher cognition. We investigate this question by using multiple unimodal and
two types of multi-modal models-cross-modal and jointly pretrained-to determine
which type of model is more relevant to fMRI brain activity when participants
are engaged in watching movies. We observe that both types of multi-modal
models show improved alignment in several language and visual regions. This
study also helps in identifying which brain regions process unimodal versus
multi-modal information. We further investigate the contribution of each
modality to multi-modal alignment by carefully removing unimodal features one
by one from multi-modal representations, and find that there is additional
information beyond the unimodal embeddings that is processed in the visual and
language regions. Based on this investigation, we find that while for
cross-modal models, their brain alignment is partially attributed to the video
modality; for jointly pretrained models, it is partially attributed to both the
video and audio modalities. This serves as a strong motivation for the
neuroscience community to investigate the interpretability of these models for
deepening our understanding of multi-modal information processing in brain.

</details>


### [893] [Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)](https://arxiv.org/abs/2505.20029)
*Subba Reddy Oota,Akshett Jindal,Ishani Mondal,Khushbu Pahwa,Satya Sai Srinath Namburi,Manish Shrivastava,Maneesh Singh,Bapi S. Raju,Manish Gupta*

Main category: q-bio.NC

TL;DR: 多模态指令调优大模型（MLLMs）在自然指令提示下能更好对齐大脑活动，尤其擅长捕捉计数和识别相关概念，但并非所有指令都有效。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索指令调优的多模态大模型（MLLMs）是否能通过自然指令提示更好地对齐大脑神经活动，并捕捉指令特定的视觉表征。

Method: 通过10种不同指令提示MLLMs生成文本响应嵌入，测量其对自然场景观看时神经视觉活动的预测能力，并与视觉专用模型及非指令调优多模态模型（如CLIP）对比。

Result: MLLMs的大脑对齐性显著优于纯视觉模型，与非指令调优多模态模型相当；特定指令（如计数、识别）能有效编码相关视觉概念，但部分指令无助于对齐。

Conclusion: 增强MLLMs捕捉任务特定信息的能力可提升其对不同指令的区分度，从而更精准预测大脑响应。

Abstract: Transformer-based language models, though not explicitly trained to mimic
brain recordings, have demonstrated surprising alignment with brain activity.
Progress in these models-through increased size, instruction-tuning, and
multimodality-has led to better representational alignment with neural data.
Recently, a new class of instruction-tuned multimodal LLMs (MLLMs) have
emerged, showing remarkable zero-shot capabilities in open-ended multimodal
vision tasks. However, it is unknown whether MLLMs, when prompted with natural
instructions, lead to better brain alignment and effectively capture
instruction-specific representations. To address this, we first investigate
brain alignment, i.e., measuring the degree of predictivity of neural visual
activity using text output response embeddings from MLLMs as participants
engage in watching natural scenes. Experiments with 10 different instructions
show that MLLMs exhibit significantly better brain alignment than vision-only
models and perform comparably to non-instruction-tuned multimodal models like
CLIP. We also find that while these MLLMs are effective at generating
high-quality responses suitable to the task-specific instructions, not all
instructions are relevant for brain alignment. Further, by varying
instructions, we make the MLLMs encode instruction-specific visual concepts
related to the input image. This analysis shows that MLLMs effectively capture
count-related and recognition-related concepts, demonstrating strong alignment
with brain activity. Notably, the majority of the explained variance of the
brain encoding models is shared between MLLM embeddings of image captioning and
other instructions. These results suggest that enhancing MLLMs' ability to
capture task-specific information could lead to better differentiation between
various types of instructions, and thereby improving their precision in
predicting brain responses.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [894] [Intent Classification on Low-Resource Languages with Query Similarity Search](https://arxiv.org/abs/2505.18241)
*Arjun Bhalla,Qi Huang*

Main category: cs.IR

TL;DR: 将意图分类重构为查询相似性搜索问题，在低资源语言中实现零样本意图分类。


<details>
  <summary>Details</summary>
Motivation: 当前意图分类方法依赖明确意图定义和大量标注数据，在低资源语言中成本高昂且难以扩展。

Method: 使用历史查询作为意图定义，通过潜在空间相似性匹配实现零样本分类。

Result: 在低资源语言环境中取得了合理的意图分类性能。

Conclusion: 相似性搜索方法可有效解决低资源语言的意图分类数据稀缺问题。

Abstract: Intent classification is an important component of a functional Information
Retrieval ecosystem. Many current approaches to intent classification,
typically framed as a classification problem, can be problematic as intents are
often hard to define and thus data can be difficult and expensive to annotate.
The problem is exacerbated when we need to extend the intent classification
system to support multiple and in particular low-resource languages. To address
this, we propose casting intent classification as a query similarity search
problem - we use previous example queries to define an intent, and a query
similarity method to classify an incoming query based on the labels of its most
similar queries in latent space. With the proposed approach, we are able to
achieve reasonable intent classification performance for queries in
low-resource languages in a zero-shot setting.

</details>


### [895] [Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems](https://arxiv.org/abs/2505.18366)
*Hansa Meghwani,Amit Agarwal,Priyaranjan Pattnayak,Hitesh Laxmichand Patel,Srikant Panda*

Main category: cs.IR

TL;DR: 提出了一种针对企业领域数据的可扩展硬负样本挖掘框架，通过动态选择语义挑战性但上下文无关的文档，显著提升了重排序模型的性能。


<details>
  <summary>Details</summary>
Motivation: 企业搜索系统因语义不匹配和术语重叠问题，难以准确检索领域特定信息，影响下游应用性能。

Method: 集成多种嵌入模型，进行降维并独特选择硬负样本，确保计算效率和语义精确性。

Result: 在专有企业语料库上，MRR@3和MRR@10分别提升15%和19%，在公共数据集上也验证了方法的通用性。

Conclusion: 该方法在提升企业搜索性能方面表现出色，并已准备好用于实际应用。

Abstract: Enterprise search systems often struggle to retrieve accurate,
domain-specific information due to semantic mismatches and overlapping
terminologies. These issues can degrade the performance of downstream
applications such as knowledge management, customer support, and
retrieval-augmented generation agents. To address this challenge, we propose a
scalable hard-negative mining framework tailored specifically for
domain-specific enterprise data. Our approach dynamically selects semantically
challenging but contextually irrelevant documents to enhance deployed
re-ranking models.
  Our method integrates diverse embedding models, performs dimensionality
reduction, and uniquely selects hard negatives, ensuring computational
efficiency and semantic precision. Evaluation on our proprietary enterprise
corpus (cloud services domain) demonstrates substantial improvements of 15\% in
MRR@3 and 19\% in MRR@10 compared to state-of-the-art baselines and other
negative sampling techniques. Further validation on public domain-specific
datasets (FiQA, Climate Fever, TechQA) confirms our method's generalizability
and readiness for real-world applications.

</details>


### [896] [AcuRank: Uncertainty-Aware Adaptive Computation for Listwise Reranking](https://arxiv.org/abs/2505.18512)
*Soyoung Yoon,Gyuwan Kim,Gyu-Hwung Cho,Seung-won Hwang*

Main category: cs.IR

TL;DR: AcuRank是一种自适应重排序框架，通过动态调整计算量和目标，基于文档相关性的不确定性估计，提高了检索任务的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前基于固定计算量的重排序方法忽视了查询难度和文档分布，导致效率低下。

Method: 使用贝叶斯TrueSkill模型迭代优化相关性估计，直到达到足够的置信水平，并通过显式建模排名不确定性来控制重排序行为。

Result: 在TREC-DL和BEIR基准测试中，AcuRank在准确性和效率之间取得了更好的平衡，且计算扩展性优于固定计算量的基线方法。

Conclusion: AcuRank在不同检索任务和基于LLM的重排序模型中表现出高效性和通用性。

Abstract: Listwise reranking with large language models (LLMs) enhances top-ranked
results in retrieval-based applications. Due to the limit in context size and
high inference cost of long context, reranking is typically performed over a
fixed size of small subsets, with the final ranking aggregated from these
partial results. This fixed computation disregards query difficulty and
document distribution, leading to inefficiencies. We propose AcuRank, an
adaptive reranking framework that dynamically adjusts both the amount and
target of computation based on uncertainty estimates over document relevance.
Using a Bayesian TrueSkill model, we iteratively refine relevance estimates
until reaching sufficient confidence levels, and our explicit modeling of
ranking uncertainty enables principled control over reranking behavior and
avoids unnecessary updates to confident predictions. Results on the TREC-DL and
BEIR benchmarks show that our method consistently achieves a superior
accuracy-efficiency trade-off and scales better with compute than
fixed-computation baselines. These results highlight the effectiveness and
generalizability of our method across diverse retrieval tasks and LLM-based
reranking models.

</details>


### [897] [News Without Borders: Domain Adaptation of Multilingual Sentence Embeddings for Cross-lingual News Recommendation](https://arxiv.org/abs/2406.12634)
*Andreea Iana,Fabian David Schmidt,Goran Glavaš,Heiko Paulheim*

Main category: cs.IR

TL;DR: 提出新闻适配句子编码器(NaSE)，通过多语言新闻语料库优化，在零样本跨语言迁移和冷启动推荐中实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 多语言新闻用户增长导致现有推荐系统在跨语言迁移和冷启动场景中性能不足，且微调方法计算成本高。

Method: 基于预训练多语言句子编码器，利用PolyNews和PolyNewsParallel语料库进行领域适配，提出冻结嵌入和点击行为融合的基线方法。

Result: NaSE在零样本跨语言迁移和少样本新闻推荐中达到最先进性能。

Conclusion: NaSE为多语言新闻推荐提供高效解决方案，减少对监督微调的依赖。

Abstract: Rapidly growing numbers of multilingual news consumers pose an increasing
challenge to news recommender systems in terms of providing customized
recommendations. First, existing neural news recommenders, even when powered by
multilingual language models (LMs), suffer substantial performance losses in
zero-shot cross-lingual transfer (ZS-XLT). Second, the current paradigm of
fine-tuning the backbone LM of a neural recommender on task-specific data is
computationally expensive and infeasible in few-shot recommendation and
cold-start setups, where data is scarce or completely unavailable. In this
work, we propose a news-adapted sentence encoder (NaSE), domain-specialized
from a pretrained massively multilingual sentence encoder (SE). To this end, we
construct and leverage PolyNews and PolyNewsParallel, two multilingual
news-specific corpora. With the news-adapted multilingual SE in place, we test
the effectiveness of (i.e., question the need for) supervised fine-tuning for
news recommendation, and propose a simple and strong baseline based on (i)
frozen NaSE embeddings and (ii) late click-behavior fusion. We show that NaSE
achieves state-of-the-art performance in ZS-XLT in true cold-start and few-shot
news recommendation.

</details>


### [898] [Walk&Retrieve: Simple Yet Effective Zero-shot Retrieval-Augmented Generation via Knowledge Graph Walks](https://arxiv.org/abs/2505.16849)
*Martin Böckling,Heiko Paulheim,Andreea Iana*

Main category: cs.IR

TL;DR: 论文提出Walk&Retrieve框架，通过基于知识图谱的检索增强生成解决大语言模型的幻觉和知识陈旧问题，无需微调即可适应动态更新，性能优越且高效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）虽具备强大推理能力，但存在幻觉和知识陈旧问题。基于知识图谱（KG）的检索增强生成（RAG）虽能缓解这些问题，但现有方法在表示对齐、检索效率与动态更新方面存在不足。

Method: 提出Walk&Retrieve框架，利用基于行走的图遍历和知识文本化生成语料，支持零样本RAG。该方法无需领域微调，可无缝适配KG更新，并能与现成LLM集成。

Result: Walk&Retrieve在响应准确性和减少幻觉方面优于现有RAG系统，同时具备低查询延迟和强扩展性，适用于大规模KG。

Conclusion: 该框架证明了轻量级检索策略的潜力，为未来RAG研究提供了高效基线方案。

Abstract: Large Language Models (LLMs) have showcased impressive reasoning abilities,
but often suffer from hallucinations or outdated knowledge. Knowledge Graph
(KG)-based Retrieval-Augmented Generation (RAG) remedies these shortcomings by
grounding LLM responses in structured external information from a knowledge
base. However, many KG-based RAG approaches struggle with (i) aligning KG and
textual representations, (ii) balancing retrieval accuracy and efficiency, and
(iii) adapting to dynamically updated KGs. In this work, we introduce
Walk&Retrieve, a simple yet effective KG-based framework that leverages
walk-based graph traversal and knowledge verbalization for corpus generation
for zero-shot RAG. Built around efficient KG walks, our method does not require
fine-tuning on domain-specific data, enabling seamless adaptation to KG
updates, reducing computational overhead, and allowing integration with any
off-the-shelf backbone LLM. Despite its simplicity, Walk&Retrieve performs
competitively, often outperforming existing RAG systems in response accuracy
and hallucination reduction. Moreover, it demonstrates lower query latency and
robust scalability to large KGs, highlighting the potential of lightweight
retrieval strategies as strong baselines for future RAG research.

</details>


### [899] [GainRAG: Preference Alignment in Retrieval-Augmented Generation through Gain Signal Synthesis](https://arxiv.org/abs/2505.18710)
*Yi Jiang,Sendong Zhao,Jianbo Li,Haochun Wang,Bing Qin*

Main category: cs.IR

TL;DR: GainRAG提出新指标'gain'对齐检索器与LLM偏好，通过有限数据训练中间件提升RAG框架性能。


<details>
  <summary>Details</summary>
Motivation: 当前RAG框架中检索器与LLM存在偏好差异，高相关段落可能干扰推理，而间接相关内容反而可能提升答案准确性。

Method: 定义'gain'指标衡量段落贡献度，提出增益信号估计方法，并采用伪段落策略防止性能退化。

Result: 在6个数据集上的实验验证了GainRAG的有效性。

Conclusion: GainRAG成功解决了检索器与LLM的偏好对齐问题，显著提升了系统性能。

Abstract: The Retrieval-Augmented Generation (RAG) framework introduces a retrieval
module to dynamically inject retrieved information into the input context of
large language models (LLMs), and has demonstrated significant success in
various NLP tasks. However, the current study points out that there is a
preference gap between retrievers and LLMs in the RAG framework, which limit
the further improvement of system performance. Some highly relevant passages
may interfere with LLM reasoning because they contain complex or contradictory
information; while some indirectly related or even inaccurate content may help
LLM generate more accurate answers by providing suggestive information or
logical clues. To solve this, we propose GainRAG, a novel approach that aligns
the retriever's and LLM's preferences by defining a new metric, "gain", which
measure how well an input passage contributes to correct outputs. Specifically,
we propose a method to estimate these gain signals and train a middleware that
aligns the preferences of the retriever and the LLM using only limited data. In
addition, we introduce a pseudo-passage strategy to mitigate degradation. The
experimental results on 6 datasets verify the effectiveness of GainRAG.

</details>


### [900] [Improving Ad matching via Cluster-Adaptive Keyword Expansion and Relevance tuning](https://arxiv.org/abs/2505.18897)
*Dipanwita Saha,Anis Zaman,Hua Zou,Ning Chen,Xinxin Shu,Nadia Vase,Abraham Bagherjeiran*

Main category: cs.IR

TL;DR: 该论文提出了一种通过文档侧语义关键词扩展来提升搜索广告关键词覆盖范围的方法，使用预训练模型生成关键词的密集向量表示，并通过最近邻搜索找到语义相关变体，同时采用聚类阈值机制和增量学习策略保持相关性，最终提高了相关性和点击率。


<details>
  <summary>Details</summary>
Motivation: 在搜索广告中，基于令牌的关键词匹配虽然能增加广告覆盖范围，但可能因语义扩展过于宽松而降低相关性。因此，需要一种在不改变查询的情况下扩展关键词覆盖范围的方法，同时保持高相关性。

Method: 使用预训练的双塔模型生成广告关键词的密集向量表示，通过最近邻搜索识别语义相关变体；采用基于聚类的阈值机制调整相似度截断值；通过增量学习策略增强下游相关性模型以适应扩展后的关键词空间。

Result: 该系统在保持低延迟的同时，提高了广告的相关性和点击率（CTR），并能适应不断变化的查询行为和广告库存。

Conclusion: 通过文档侧语义关键词扩展和增量学习策略，论文提出的方法在扩展关键词覆盖范围的同时保持了高相关性，为搜索广告提供了一种可扩展且高效的解决方案。

Abstract: In search advertising, keyword matching connects user queries with relevant
ads. While token-based matching increases ad coverage, it can reduce relevance
due to overly permissive semantic expansion. This work extends keyword reach
through document-side semantic keyword expansion, using a language model to
broaden token-level matching without altering queries. We propose a solution
using a pre-trained siamese model to generate dense vector representations of
ad keywords and identify semantically related variants through nearest neighbor
search. To maintain precision, we introduce a cluster-based thresholding
mechanism that adjusts similarity cutoffs based on local semantic density. Each
expanded keyword maps to a group of seller-listed items, which may only
partially align with the original intent. To ensure relevance, we enhance the
downstream relevance model by adapting it to the expanded keyword space using
an incremental learning strategy with a lightweight decision tree ensemble.
This system improves both relevance and click-through rate (CTR), offering a
scalable, low-latency solution adaptable to evolving query behavior and
advertising inventory.

</details>


### [901] [Optimized Text Embedding Models and Benchmarks for Amharic Passage Retrieval](https://arxiv.org/abs/2505.19356)
*Kidist Amde Mekonnen,Yosef Worku Alemneh,Maarten de Rijke*

Main category: cs.IR

TL;DR: 该论文针对低资源、形态丰富的阿姆哈拉语，提出了基于预训练模型的稠密检索方法，显著提升了检索效果，并发布了数据集和模型以促进未来研究。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的预训练语言模型在多语言和跨语言检索中取得了进展，但在低资源、形态丰富的语言（如阿姆哈拉语）中的效果仍未充分探索，主要由于数据稀缺和次优分词问题。

Method: 作者提出了基于预训练阿姆哈拉语BERT和RoBERTa的稠密检索模型，包括RoBERTa-Base-Amharic-Embed和更紧凑的变体，并训练了一个基于ColBERT的后期交互检索模型。

Result: RoBERTa-Base-Amharic-Embed模型在MRR@10和Recall@10上分别相对提升了17.6%和9.86%，而更紧凑的变体在保持竞争力的同时体积缩小了13倍以上。基于ColBERT的模型在所有评估模型中取得了最高的MRR@10分数（0.843）。

Conclusion: 论文强调了语言特定适应在低资源环境中的重要性，并发布了数据集、代码和模型以促进低资源信息检索的未来研究。

Abstract: Neural retrieval methods using transformer-based pre-trained language models
have advanced multilingual and cross-lingual retrieval. However, their
effectiveness for low-resource, morphologically rich languages such as Amharic
remains underexplored due to data scarcity and suboptimal tokenization. We
address this gap by introducing Amharic-specific dense retrieval models based
on pre-trained Amharic BERT and RoBERTa backbones. Our proposed
RoBERTa-Base-Amharic-Embed model (110M parameters) achieves a 17.6% relative
improvement in MRR@10 and a 9.86% gain in Recall@10 over the strongest
multilingual baseline, Arctic Embed 2.0 (568M parameters). More compact
variants, such as RoBERTa-Medium-Amharic-Embed (42M), remain competitive while
being over 13x smaller. Additionally, we train a ColBERT-based late interaction
retrieval model that achieves the highest MRR@10 score (0.843) among all
evaluated models. We benchmark our proposed models against both sparse and
dense retrieval baselines to systematically assess retrieval effectiveness in
Amharic. Our analysis highlights key challenges in low-resource settings and
underscores the importance of language-specific adaptation. To foster future
research in low-resource IR, we publicly release our dataset, codebase, and
trained models at https://github.com/kidist-amde/amharic-ir-benchmarks.

</details>


### [902] [HGCL: Hierarchical Graph Contrastive Learning for User-Item Recommendation](https://arxiv.org/abs/2505.19020)
*Jiawei Xue,Zhen Yang,Haitao Lin,Ziji Zhang,Luzhu Wang,Yikun Gu,Yao Xu,Xin Li*

Main category: cs.IR

TL;DR: 提出HGCL方法，通过层次化物品结构增强图对比学习，提升推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 现有图对比学习方法缺乏对层次化物品结构的显式建模，而这类结构能显著提升推荐效果。

Method: HGCL分三步：预训练GCL模块、构建双层用户-物品二部图、基于层次图微调表示。

Result: 在3个基准数据集（7万-38万节点）上性能超越基线模型。

Conclusion: 层次化物品结构能有效增强图对比学习在推荐任务中的表现。

Abstract: Graph Contrastive Learning (GCL), which fuses graph neural networks with
contrastive learning, has evolved as a pivotal tool in user-item
recommendations. While promising, existing GCL methods often lack explicit
modeling of hierarchical item structures, which represent item similarities
across varying resolutions. Such hierarchical item structures are ubiquitous in
various items (e.g., online products and local businesses), and reflect their
inherent organizational properties that serve as critical signals for enhancing
recommendation accuracy. In this paper, we propose Hierarchical Graph
Contrastive Learning (HGCL), a novel GCL method that incorporates hierarchical
item structures for user-item recommendations. First, HGCL pre-trains a GCL
module using cross-layer contrastive learning to obtain user and item
representations. Second, HGCL employs a representation compression and
clustering method to construct a two-hierarchy user-item bipartite graph.
Ultimately, HGCL fine-tunes user and item representations by learning on the
hierarchical graph, and then provides recommendations based on user-item
interaction scores. Experiments on three widely adopted benchmark datasets
ranging from 70K to 382K nodes confirm the superior performance of HGCL over
existing baseline models, highlighting the contribution of hierarchical item
structures in enhancing GCL methods for recommendation tasks.

</details>


### [903] [BroadGen: A Framework for Generating Effective and Efficient Advertiser Broad Match Keyphrase Recommendations](https://arxiv.org/abs/2505.19164)
*Ashirbad Mishra,Jinyu Zhao,Soumik Dey,Hansi Wu,Binbin Li,Kamesh Madduri*

Main category: cs.IR

TL;DR: 该论文提出了一种名为BroadGen的创新框架，用于在赞助搜索广告中推荐高效且有效的广泛匹配关键词，解决了精确匹配和广泛匹配的各自问题。


<details>
  <summary>Details</summary>
Motivation: 精确匹配关键词在赞助搜索广告中存在高管理成本、有限的目标范围和不断变化的搜索查询模式等问题。广泛匹配虽然能缓解部分问题，但存在目标准确性差和监管信号少等挑战。

Method: 论文提出了BroadGen框架，通过利用历史搜索查询数据，并结合令牌对应建模，来推荐高效且有效的广泛匹配关键词。

Result: BroadGen能够为eBay上数百万卖家和超过23亿商品提供每日服务，并且在时间上保持更好的查询稳定性。

Conclusion: BroadGen框架在赞助搜索广告中提供了一种高效且有效的广泛匹配关键词推荐方法，解决了现有匹配类型的局限性。

Abstract: In the domain of sponsored search advertising, the focus of Keyphrase
recommendation has largely been on exact match types, which pose issues such as
high management expenses, limited targeting scope, and evolving search query
patterns. Alternatives like Broad match types can alleviate certain drawbacks
of exact matches but present challenges like poor targeting accuracy and
minimal supervisory signals owing to limited advertiser usage. This research
defines the criteria for an ideal broad match, emphasizing on both efficiency
and effectiveness, ensuring that a significant portion of matched queries are
relevant. We propose BroadGen, an innovative framework that recommends
efficient and effective broad match keyphrases by utilizing historical search
query data. Additionally, we demonstrate that BroadGen, through token
correspondence modeling, maintains better query stability over time. BroadGen's
capabilities allow it to serve daily, millions of sellers at eBay with over 2.3
billion items.

</details>


### [904] [REARANK: Reasoning Re-ranking Agent via Reinforcement Learning](https://arxiv.org/abs/2505.20046)
*Le Zhang,Bo Wang,Xipeng Qiu,Siva Reddy,Aishwarya Agrawal*

Main category: cs.IR

TL;DR: REARANK是一种基于大语言模型的列表推理重排代理，通过强化学习和数据增强显著提升性能，仅需少量标注样本即可媲美GPT-4。


<details>
  <summary>Details</summary>
Motivation: 旨在提升信息检索中的重排性能与可解释性，同时减少对大量标注数据的依赖。

Method: 基于Qwen2.5-7B模型，结合强化学习和数据增强技术，实现显式推理后重排。

Result: 在多个基准测试中表现优异，尤其在BRIGHT推理密集型任务上超越GPT-4，仅需179个标注样本。

Conclusion: 该方法有效证明了强化学习可增强大语言模型的推理能力，为高效重排提供了新思路。

Abstract: We present REARANK, a large language model (LLM)-based listwise reasoning
reranking agent. REARANK explicitly reasons before reranking, significantly
improving both performance and interpretability. Leveraging reinforcement
learning and data augmentation, REARANK achieves substantial improvements over
baseline models across popular information retrieval benchmarks, notably
requiring only 179 annotated samples. Built on top of Qwen2.5-7B, our
REARANK-7B demonstrates performance comparable to GPT-4 on both in-domain and
out-of-domain benchmarks and even surpasses GPT-4 on reasoning-intensive BRIGHT
benchmarks. These results underscore the effectiveness of our approach and
highlight how reinforcement learning can enhance LLM reasoning capabilities in
reranking.

</details>


### [905] [Hierarchical Tree Search-based User Lifelong Behavior Modeling on Large Language Model](https://arxiv.org/abs/2505.19505)
*Yu Xia,Rui Zhong,Hao Gu,Wei Yang,Chi Lu,Peng Jiang,Kun Gai*

Main category: cs.IR

TL;DR: 本文提出HiT-LBM框架，通过分块行为提取和层次树搜索优化LLMs在推荐系统中的用户兴趣建模，提升推荐效果。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在推荐系统中直接用于用户兴趣学习时，难以有效处理长序列行为、提取兴趣并应用于实际场景。

Method: HiT-LBM框架整合CUBE（分块行为提取）和HTS（层次树搜索），结合TIF（时序兴趣融合）构建用户终身兴趣表示。

Result: 实验表明，该方法超越现有最优方法，显著提升推荐性能。

Conclusion: HiT-LBM通过分层建模用户行为与兴趣演化，为LLMs在推荐系统中的实际应用提供了有效解决方案。

Abstract: Large Language Models (LLMs) have garnered significant attention in
Recommendation Systems (RS) due to their extensive world knowledge and robust
reasoning capabilities. However, a critical challenge lies in enabling LLMs to
effectively comprehend and extract insights from massive user behaviors.
Current approaches that directly leverage LLMs for user interest learning face
limitations in handling long sequential behaviors, effectively extracting
interest, and applying interest in practical scenarios. To address these
issues, we propose a Hierarchical Tree Search-based User Lifelong Behavior
Modeling framework (HiT-LBM). HiT-LBM integrates Chunked User Behavior
Extraction (CUBE) and Hierarchical Tree Search for Interest (HTS) to capture
diverse interests and interest evolution of user. CUBE divides user lifelong
behaviors into multiple chunks and learns the interest and interest evolution
within each chunk in a cascading manner. HTS generates candidate interests
through hierarchical expansion and searches for the optimal interest with
process rating model to ensure information gain for each behavior chunk.
Additionally, we design Temporal-Ware Interest Fusion (TIF) to integrate
interests from multiple behavior chunks, constructing a comprehensive
representation of user lifelong interests. The representation can be embedded
into any recommendation model to enhance performance. Extensive experiments
demonstrate the effectiveness of our approach, showing that it surpasses
state-of-the-art methods.

</details>


### [906] [LogiCoL: Logically-Informed Contrastive Learning for Set-based Dense Retrieval](https://arxiv.org/abs/2505.19588)
*Yanzhen Shen,Sihao Chen,Xueqiang Xu,Yunyi Zhang,Chaitanya Malaviya,Dan Roth*

Main category: cs.IR

TL;DR: 论文提出LogiCoL方法，通过逻辑感知的对比学习改进密集检索器在处理含逻辑连接词查询时的性能。


<details>
  <summary>Details</summary>
Motivation: 当前密集检索器在处理含逻辑连接词的查询时表现不佳，无法满足查询中隐含的逻辑约束，影响下游应用效果。

Method: LogiCoL基于批量监督对比学习，通过两组软约束（利用t-范数表达）学习查询结果间的子集和互斥集关系。

Result: 实验表明，LogiCoL训练的模型在检索性能和结果逻辑一致性上均有提升。

Conclusion: LogiCoL有效解决了密集检索器处理逻辑查询的难题，并通过分析揭示了其优势原因。

Abstract: While significant progress has been made with dual- and bi-encoder dense
retrievers, they often struggle on queries with logical connectives, a use case
that is often overlooked yet important in downstream applications. Current
dense retrievers struggle with such queries, such that the retrieved results do
not respect the logical constraints implied in the queries. To address this
challenge, we introduce LogiCoL, a logically-informed contrastive learning
objective for dense retrievers. LogiCoL builds upon in-batch supervised
contrastive learning, and learns dense retrievers to respect the subset and
mutually-exclusive set relation between query results via two sets of soft
constraints expressed via t-norm in the learning objective. We evaluate the
effectiveness of LogiCoL on the task of entity retrieval, where the model is
expected to retrieve a set of entities in Wikipedia that satisfy the implicit
logical constraints in the query. We show that models trained with LogiCoL
yield improvement both in terms of retrieval performance and logical
consistency in the results. We provide detailed analysis and insights to
uncover why queries with logical connectives are challenging for dense
retrievers and why LogiCoL is most effective.

</details>


### [907] [AgentRecBench: Benchmarking LLM Agent-based Personalized Recommender Systems](https://arxiv.org/abs/2505.19623)
*Yu Shang,Peijie Liu,Yuwei Yan,Zijing Wu,Leheng Sheng,Yuanqing Yu,Chumeng Jiang,An Zhang,Fengli Xu,Yu Wang,Min Zhang,Yong Li*

Main category: cs.IR

TL;DR: 本文提出基于大语言模型的自主推荐系统，通过构建交互式模拟器、统一框架和首个综合基准测试，验证了其优越性并提供了设计指南。


<details>
  <summary>Details</summary>
Motivation: 现有自主推荐系统缺乏标准化评估方法，无法系统比较不同方法的性能。

Method: 1) 构建含丰富元数据的交互式文本推荐模拟器；2) 开发模块化统一框架；3) 建立首个包含10种方法的综合基准测试。

Result: 实验证明自主推荐系统性能优越，并确立了核心组件的设计准则。基准测试通过公开挑战验证并持续维护。

Conclusion: 该研究为自主推荐系统建立了可复现的评估体系，推动领域发展，相关资源已开源。

Abstract: The emergence of agentic recommender systems powered by Large Language Models
(LLMs) represents a paradigm shift in personalized recommendations, leveraging
LLMs' advanced reasoning and role-playing capabilities to enable autonomous,
adaptive decision-making. Unlike traditional recommendation approaches, agentic
recommender systems can dynamically gather and interpret user-item interactions
from complex environments, generating robust recommendation strategies that
generalize across diverse scenarios. However, the field currently lacks
standardized evaluation protocols to systematically assess these methods. To
address this critical gap, we propose: (1) an interactive textual
recommendation simulator incorporating rich user and item metadata and three
typical evaluation scenarios (classic, evolving-interest, and cold-start
recommendation tasks); (2) a unified modular framework for developing and
studying agentic recommender systems; and (3) the first comprehensive benchmark
comparing 10 classical and agentic recommendation methods. Our findings
demonstrate the superiority of agentic systems and establish actionable design
guidelines for their core components. The benchmark environment has been
rigorously validated through an open challenge and remains publicly available
with a continuously maintained
leaderboard~\footnote[2]{https://tsinghua-fib-lab.github.io/AgentSocietyChallenge/pages/overview.html},
fostering ongoing community engagement and reproducible research. The benchmark
is available at:
\hyperlink{https://huggingface.co/datasets/SGJQovo/AgentRecBench}{https://huggingface.co/datasets/SGJQovo/AgentRecBench}.

</details>


### [908] [Unlocking the Power of Diffusion Models in Sequential Recommendation: A Simple and Effective Approach](https://arxiv.org/abs/2505.19544)
*Jialei Chen,Yuanbo Xu,Yiheng Jiang*

Main category: cs.IR

TL;DR: 本文提出ADRec框架，通过独立噪声处理和三级训练策略解决扩散推荐模型中的嵌入崩溃问题，提升推荐准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的序列推荐模型存在嵌入崩溃问题，导致模型性能受限。ADRec旨在通过创新设计缓解这一问题。

Method: ADRec采用独立噪声处理每个标记，并在训练时对整个目标序列进行扩散。结合自回归捕获标记依赖性和标记级扩散建模，提出三级训练策略：预训练嵌入权重、对齐权重与模型主干、微调模型。

Result: 在六个数据集上的实验表明，ADRec有效提升了基于扩散的序列推荐系统的准确性和效率。

Conclusion: ADRec通过创新设计和三级训练策略，成功解决了嵌入崩溃问题，为序列推荐系统提供了更优的解决方案。

Abstract: In this paper, we focus on the often-overlooked issue of embedding collapse
in existing diffusion-based sequential recommendation models and propose ADRec,
an innovative framework designed to mitigate this problem. Diverging from
previous diffusion-based methods, ADRec applies an independent noise process to
each token and performs diffusion across the entire target sequence during
training. ADRec captures token interdependency through auto-regression while
modeling per-token distributions through token-level diffusion. This dual
approach enables the model to effectively capture both sequence dynamics and
item representations, overcoming the limitations of existing methods. To
further mitigate embedding collapse, we propose a three-stage training
strategy: (1) pre-training the embedding weights, (2) aligning these weights
with the ADRec backbone, and (3) fine-tuning the model. During inference, ADRec
applies the denoising process only to the last token, ensuring that the
meaningful patterns in historical interactions are preserved. Our comprehensive
empirical evaluation across six datasets underscores the effectiveness of ADRec
in enhancing both the accuracy and efficiency of diffusion-based sequential
recommendation systems.

</details>


### [909] [Leveraging Descriptions of Emotional Preferences in Recommender Systems](https://arxiv.org/abs/2505.20190)
*Tonmoy Hasan,Razvan Bunescu*

Main category: cs.IR

TL;DR: 论文提出了一种基于用户情感状态的推荐任务，通过挖掘书评中的细粒度情感表达，构建大型数据集，并利用Transformer架构训练推荐模型，实验表明结合文本描述和用户情感偏好的模型效果最佳。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统仅关注用户对推荐项的喜欢态度，而忽略了更广泛的情感状态（如情绪、心境等）。本文旨在利用用户明确寻求的多样情感状态，提升推荐系统的个性化能力。

Method: 从书评中挖掘细粒度情感表达构建数据集，提出基于Transformer的架构，利用情感表达作为输入，结合用户历史阅读、评分和评论数据训练多种推荐模型。

Result: 实验结果表明，能够利用项目文本描述和用户情感偏好的模型在匹配推荐项与情感偏好任务上表现最优。

Conclusion: 通过细粒度情感状态和文本描述的结合，可以显著提升推荐系统的个性化效果，为情感感知推荐提供了新方向。

Abstract: The affective attitude of liking a recommended item reflects just one
category in a wide spectrum of affective phenomena that also includes emotions
such as entranced or intrigued, moods such as cheerful or buoyant, as well as
more fine-grained affective states, such as "pleasantly surprised by the
conclusion". In this paper, we introduce a novel recommendation task that can
leverage a virtually unbounded range of affective states sought explicitly by
the user in order to identify items that, upon consumption, are likely to
induce those affective states. Correspondingly, we create a large dataset of
user preferences containing expressions of fine-grained affective states that
are mined from book reviews, and propose a Transformer-based architecture that
leverages such affective expressions as input. We then use the resulting
dataset of affective states preferences, together with the linked users and
their histories of book readings, ratings, and reviews, to train and evaluate
multiple recommendation models on the task of matching recommended items with
affective preferences. Experiments show that the best results are obtained by
models that can utilize textual descriptions of items and user affective
preferences.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [910] [Smart Waste Management System for Makkah City using Artificial Intelligence and Internet of Things](https://arxiv.org/abs/2505.19040)
*Rawabi S. Al Qurashi,Maram M. Almnjomi,Teef L. Alghamdi,Amjad H. Almalki,Shahad S. Alharthi,Shahad M. althobuti,Alanoud S. Alharthi,Maha A. Thafar*

Main category: cs.ET

TL;DR: 该研究提出了一种名为TUHR的智能废物管理系统，利用物联网和人工智能技术，针对麦加朝觐期间的大规模废物管理问题，提供动态、高效的解决方案。


<details>
  <summary>Details</summary>
Motivation: 麦加朝觐作为全球最大宗教集会之一，吸引了数百万人，导致废物大量积累，传统固定收集计划无法有效应对，亟需创新解决方案以减轻环境和健康风险。

Method: 研究设计了TUHR系统，通过超声波传感器监测垃圾桶填充水平，结合微控制器和气体检测传感器，实时通知相关部门，并检测有害物质。

Result: TUHR系统能够动态管理废物，减少不必要的资源消耗，优化废物管理流程，同时提升场所清洁度，符合沙特阿拉伯2030愿景的可持续发展目标。

Conclusion: 该智能废物管理系统不仅解决了朝觐期间的废物管理挑战，还为智能城市和健康环保的未来提供了可行范例。

Abstract: Waste management is a critical global issue with significant environmental
and public health implications. It has become more destructive during
large-scale events such as the annual pilgrimage to Makkah, Saudi Arabia, one
of the world's largest religious gatherings. This event's popularity has
attracted millions worldwide, leading to significant and un-predictable
accumulation of waste. Such a tremendous number of visitors leads to in-creased
waste management issues at the Grand Mosque and other holy sites, highlighting
the need for an effective solution other than traditional methods based on
rigid collection schedules.
  To address this challenge, this research proposed an innovative solution that
is context-specific and tailored to the unique requirements of pilgrimage
season: a Smart Waste Management System, called TUHR, that utilizes the
Internet of Things and Artificial Intelligence. This system encompasses
ultrasonic sensors that monitor waste levels in each container at the
performance sites. Once the container reaches full capacity, the sensor
communicates with the microcontroller, which alerts the relevant authorities.
Moreover, our system can detect harmful substances such as gas from the gas
detector sensor. Such a proactive and dynamic approach promises to mitigate the
environmental and health risks associated with waste accumulation and enhance
the cleanliness of these sites. It also delivers economic benefits by reducing
unnecessary gasoline consumption and optimizing waste management resources.
Importantly, this research aligns with the principles of smart cities and
exemplifies the innovative, sustainable, and health-conscious approach that
Saudi Arabia is implementing as part of its Vision 2030 initiative.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [911] [Model Enumeration of Two-Variable Logic with Quadratic Delay Complexity](https://arxiv.org/abs/2505.19648)
*Qiaolan Meng,Juhua Pu,Hongting Niu,Yuyi Wang,Yuanhong Wang,Ondřej Kuželka*

Main category: cs.LO

TL;DR: 该论文提出了一种新颖算法，用于枚举有限域中双变量一阶逻辑（FO²）句子的所有模型，其延迟复杂度接近最优。


<details>
  <summary>Details</summary>
Motivation: 研究双变量一阶逻辑（FO²）的模型枚举问题，旨在高效枚举给定句子在特定域大小下的所有可能模型。

Method: 设计了一种新算法，用于枚举FO²句子在固定域大小n下的所有模型，延迟复杂度为O(n² log n)。

Result: 算法在固定句子时，延迟复杂度为域大小n的二次方（含对数因子），接近理论下限Ω(n²)。

Conclusion: 该算法在FO²模型枚举中实现了近乎最优的复杂度，为逻辑推理和模型枚举提供了高效工具。

Abstract: We study the model enumeration problem of the function-free, finite domain
fragment of first-order logic with two variables ($FO^2$). Specifically, given
an $FO^2$ sentence $\Gamma$ and a positive integer $n$, how can one enumerate
all the models of $\Gamma$ over a domain of size $n$? In this paper, we devise
a novel algorithm to address this problem. The delay complexity, the time
required between producing two consecutive models, of our algorithm is
quadratic in the given domain size $n$ (up to logarithmic factors) when the
sentence is fixed. This complexity is almost optimal since the interpretation
of binary predicates in any model requires at least $\Omega(n^2)$ bits to
represent.

</details>


### [912] [Comparing Neural Network Encodings for Logic-based Explainability](https://arxiv.org/abs/2505.20269)
*Levi Cordeiro Carvalho,Saulo A. F. Oliveira,Thiago Alves Rocha*

Main category: cs.LO

TL;DR: 论文比较了两种将人工神经网络编码为逻辑约束的方法，发现第二种方法在构建约束和总体时间上表现更优。


<details>
  <summary>Details</summary>
Motivation: 在关键系统、数据保护法律和处理对抗样本等场景中，为人工神经网络的输出提供解释至关重要。逻辑方法能提供有正确性保证的解释，但面临可扩展性挑战。

Method: 比较了两种将人工神经网络编码为逻辑约束的方法，其中第二种方法针对解释性场景进行了优化，使用了更少的变量和约束。

Result: 实验显示，两种方法在计算解释时的运行时间相近，但优化后的编码在构建逻辑约束上提升了18%，总体时间上提升了16%。

Conclusion: 优化后的编码方法在效率和性能上表现更优，适用于需要高效解释人工神经网络输出的场景。

Abstract: Providing explanations for the outputs of artificial neural networks (ANNs)
is crucial in many contexts, such as critical systems, data protection laws and
handling adversarial examples. Logic-based methods can offer explanations with
correctness guarantees, but face scalability challenges. Due to these issues,
it is necessary to compare different encodings of ANNs into logical
constraints, which are used in logic-based explainability. This work compares
two encodings of ANNs: one has been used in the literature to provide
explanations, while the other will be adapted for our context of
explainability. Additionally, the second encoding uses fewer variables and
constraints, thus, potentially enhancing efficiency. Experiments showed similar
running times for computing explanations, but the adapted encoding performed up
to 18\% better in building logical constraints and up to 16\% better in overall
time.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [913] [FastMamba: A High-Speed and Efficient Mamba Accelerator on FPGA with Accurate Quantization](https://arxiv.org/abs/2505.18975)
*Aotao Wang,Haikuo Shao,Shaobo Ma,Zhongfeng Wang*

Main category: cs.AR

TL;DR: FastMamba是一种专为FPGA设计的加速器，通过硬件-算法协同设计优化Mamba2部署，显著提升计算效率和能效。


<details>
  <summary>Details</summary>
Motivation: Mamba2等状态空间模型在边缘设备上部署时面临量化困难、不规则张量操作和硬件不友好的非线性函数等问题，亟需高效解决方案。

Method: 采用Hadamard变换实现8位量化，提出硬件友好的幂次量化框架，并设计并行向量处理单元和流水线数据流加速器。

Result: 在Mamba2-130M上比CPU快68.8倍，比GPU快8.9倍；Mamba2-2.7B解码能效比GPU高6倍。

Conclusion: FastMamba通过创新量化方法和硬件设计，有效解决了Mamba2在边缘设备上的部署瓶颈。

Abstract: State Space Models (SSMs), like recent Mamba2, have achieved remarkable
performance and received extensive attention. However, deploying Mamba2 on
resource-constrained edge devices encounters many problems: severe outliers
within the linear layer challenging the quantization, diverse and irregular
element-wise tensor operations, and hardware-unfriendly nonlinear functions in
the SSM block. To address these issues, this paper presents FastMamba, a
dedicated accelerator on FPGA with hardware-algorithm co-design to promote the
deployment efficiency of Mamba2. Specifically, we successfully achieve 8-bit
quantization for linear layers through Hadamard transformation to eliminate
outliers. Moreover, a hardware-friendly and fine-grained power-of-two
quantization framework is presented for the SSM block and convolution layer,
and a first-order linear approximation is developed to optimize the nonlinear
functions. Based on the accurate algorithm quantization, we propose an
accelerator that integrates parallel vector processing units, pipelined
execution dataflow, and an efficient SSM Nonlinear Approximation Unit, which
enhances computational efficiency and reduces hardware complexity. Finally, we
evaluate FastMamba on Xilinx VC709 FPGA. For the input prefill task on
Mamba2-130M, FastMamba achieves 68.80\times and 8.90\times speedup over Intel
Xeon 4210R CPU and NVIDIA RTX 3090 GPU, respectively. In the output decode
experiment with Mamba2-2.7B, FastMamba attains 6\times higher energy efficiency
than RTX 3090 GPU.

</details>


### [914] [Enable Lightweight and Precision-Scalable Posit/IEEE-754 Arithmetic in RISC-V Cores for Transprecision Computing](https://arxiv.org/abs/2505.19096)
*Qiong Li,Chao Fang,Longwei Huang,Jun Lin,Zhongfeng Wang*

Main category: cs.AR

TL;DR: 该论文提出了一种在RISC-V处理器中高效实现posit格式的方法，通过优化FPU设计，显著降低了硬件资源消耗并提升了计算吞吐量。


<details>
  <summary>Details</summary>
Motivation: posit格式在动态范围和精度上优于传统浮点格式，但由于缺乏轻量级、精度可扩展且兼容IEEE-754的硬件实现方案，其在RISC-V处理器中的采用受到限制。

Method: 1) 在原有FPU中集成专用posit编解码器以实现轻量级实现；2) 引入动态指数大小的多/混合精度支持以实现精度可扩展性；3) 重用并定制ISA扩展以实现与IEEE-754兼容的posit运算。

Result: 相比现有支持posit的RISC-V处理器，该实现减少了47.9%的LUT和57.4%的FF资源，并在多种GEMM核中实现了高达2.54倍的吞吐量提升。

Conclusion: 该研究成功解决了posit格式在RISC-V处理器中的实现难题，为高效、灵活的transprecision计算提供了可行的硬件解决方案。

Abstract: While posit format offers superior dynamic range and accuracy for
transprecision computing, its adoption in RISC-V processors is hindered by the
lack of a unified solution for lightweight, precision-scalable, and IEEE-754
arithmetic compatible hardware implementation. To address these challenges, we
enhance RISC-V processors by 1) integrating dedicated posit codecs into the
original FPU for lightweight implementation, 2) incorporating
multi/mixed-precision support with dynamic exponent size for
precision-scalability, and 3) reusing and customizing ISA extensions for
IEEE-754 compatible posit operations. Our comprehensive evaluation spans the
modified FPU, RISC-V core, and SoC levels. It demonstrates that our
implementation achieves 47.9% LUTs and 57.4% FFs reduction compared to
state-of-the-art posit-enabled RISC-V processors, while achieving up to
2.54$\times$ throughput improvement in various GEMM kernels.

</details>


### [915] [Efficient Optimization Accelerator Framework for Multistate Ising Problems](https://arxiv.org/abs/2505.20250)
*Chirag Garg,Sayeef Salahuddin*

Main category: cs.AR

TL;DR: 该论文提出了一种改进Ising机解决多态NP难组合优化问题的方法，通过广义布尔逻辑函数减少搜索空间，并在图着色问题上验证了其有效性，性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有Ising机在将多态组合优化问题转换为QUBO形式时，往往导致搜索空间复杂化，影响解的质量。论文旨在解决这一问题。

Method: 通过将自旋交互建模为广义布尔逻辑函数来减少搜索空间，并结合并行回火技术，设计了一个全连接的1024神经元概率Ising加速器。

Result: 该方法在准确度上与先进启发式算法和机器学习算法相当，比现有Ising方法有显著改进，加速性能提升10000倍，物理神经元需求减少1.5-4倍。

Conclusion: 该工作扩展了现有Ising硬件的潜力，能有效解决多态优化问题，在能量、性能、面积和解质量等所有指标上均有提升。

Abstract: Ising Machines are a prominent class of hardware architectures that aim to
solve NP-hard combinatorial optimization problems. These machines consist of a
network of interacting binary spins/neurons that evolve to represent the
optimum ground state energy solution. Generally, combinatorial problems are
transformed into quadratic unconstrained binary optimization (QUBO) form to
harness the computational efficiency of these Ising machines. However, this
transformation, especially for multi-state problems, often leads to a more
complex exploration landscape than the original problem, thus severely
impacting the solution quality. To address this challenge, we model the spin
interactions as a generalized boolean logic function to significantly reduce
the exploration space. We benchmark the graph coloring problem from the class
of multi-state NP-hard optimization using probabilistic Ising solvers to
illustrate the effectiveness of our framework. The proposed methodology
achieves similar accuracy compared to state-of-the-art heuristics and machine
learning algorithms, and demonstrates significant improvement over the existing
Ising methods. Additionally, we demonstrate that combining parallel tempering
with our existing framework further reduces the coloring error by up to 50%
compared to the conventionally used Gibbs sampling algorithm. We also design a
1024-neuron all-to-all connected probabilistic Ising accelerator that shows up
to 10000x performance acceleration compared to heuristics while reducing the
number of required physical neurons by 1.5-4x compared to conventional Ising
machines. Indeed, this accelerator solution demonstrates improvement across all
metrics over the current methods, i.e., energy, performance, area, and solution
quality. Thus, this work expands the potential of existing Ising hardware to
solve a broad class of these multistate optimization problems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [916] [LA-RCS: LLM-Agent-Based Robot Control System](https://arxiv.org/abs/2505.18214)
*TaekHyun Park,YoungJun Choi,SeungHoon Shin,Kwangil Lee*

Main category: cs.RO

TL;DR: LA-RCS是一个基于LLM-Agent的机器人控制系统，能自主规划、执行任务并适应环境变化，通过自然语言命令控制机器人，平均成功率90%。


<details>
  <summary>Details</summary>
Motivation: 开发一个能自主响应复杂用户需求、减少人工干预的智能机器人控制系统。

Method: 采用双Agent框架，实现任务规划、环境观察、动态调整及自然语言指令转换。

Result: 在四类场景测试中平均任务成功率达90%，验证系统有效性。

Conclusion: LA-RCS通过LLM-Agent实现了高效自主的机器人控制，显著降低用户操作负担。

Abstract: LA-RCS (LLM-agent-based robot control system) is a sophisticated robot
control system designed to autonomously plan, work, and analyze the external
environment based on user requirements by utilizing LLM-Agent. Utilizing a
dual-agent framework, LA-RCS generates plans based on user requests, observes
the external environment, executes the plans, and modifies the plans as needed
to adapt to changes in the external conditions. Additionally, LA-RCS interprets
natural language commands by the user and converts them into commands
compatible with the robot interface so that the robot can execute tasks and
meet user requests properly. During his process, the system autonomously
evaluates observation results, provides feedback on the tasks, and executes
commands based on real-time environmental monitoring, significantly reducing
the need for user intervention in fulfilling requests. We categorized the
scenarios that LA-RCS needs to perform into four distinct types and conducted a
quantitative assessment of its performance in each scenario. The results showed
an average success rate of 90 percent, demonstrating the system capability to
fulfill user requests satisfactorily. For more extensive results, readers can
visit our project page: https://la-rcs.github.io

</details>


### [917] [BEDI: A Comprehensive Benchmark for Evaluating Embodied Agents on UAVs](https://arxiv.org/abs/2505.18229)
*Mingning Guo,Mengwei Wu,Jiarun He,Shaoxian Li,Haifeng Li,Chao Tao*

Main category: cs.RO

TL;DR: 该论文提出了一个名为BEDI的标准化基准，用于评估无人机嵌入式智能体（UAV-EAs），填补了该领域缺乏系统性评估工具的空白。


<details>
  <summary>Details</summary>
Motivation: 当前无人机嵌入式智能体的评估方法缺乏标准化基准、多样化测试场景和开放系统接口，限制了其发展。

Method: 提出BEDI基准，基于感知-决策-行动循环的动态任务链范式，将复杂任务分解为可衡量子任务，并设计统一评估框架和混合测试平台。

Result: 通过评估多个先进视觉语言模型（VLMs），揭示了它们在无人机任务中的局限性，验证了BEDI的有效性。

Conclusion: BEDI填补了无人机嵌入式智能体评估的空白，为未来研究提供了标准化工具和坚实基础。

Abstract: With the rapid advancement of low-altitude remote sensing and Vision-Language
Models (VLMs), Embodied Agents based on Unmanned Aerial Vehicles (UAVs) have
shown significant potential in autonomous tasks. However, current evaluation
methods for UAV-Embodied Agents (UAV-EAs) remain constrained by the lack of
standardized benchmarks, diverse testing scenarios and open system interfaces.
To address these challenges, we propose BEDI (Benchmark for Embodied Drone
Intelligence), a systematic and standardized benchmark designed for evaluating
UAV-EAs. Specifically, we introduce a novel Dynamic Chain-of-Embodied-Task
paradigm based on the perception-decision-action loop, which decomposes complex
UAV tasks into standardized, measurable subtasks. Building on this paradigm, we
design a unified evaluation framework encompassing five core sub-skills:
semantic perception, spatial perception, motion control, tool utilization, and
task planning. Furthermore, we construct a hybrid testing platform that
integrates static real-world environments with dynamic virtual scenarios,
enabling comprehensive performance assessment of UAV-EAs across varied
contexts. The platform also offers open and standardized interfaces, allowing
researchers to customize tasks and extend scenarios, thereby enhancing
flexibility and scalability in the evaluation process. Finally, through
empirical evaluations of several state-of-the-art (SOTA) VLMs, we reveal their
limitations in embodied UAV tasks, underscoring the critical role of the BEDI
benchmark in advancing embodied intelligence research and model optimization.
By filling the gap in systematic and standardized evaluation within this field,
BEDI facilitates objective model comparison and lays a robust foundation for
future development in this field. Our benchmark will be released at
https://github.com/lostwolves/BEDI .

</details>


### [918] [CrashAgent: Crash Scenario Generation via Multi-modal Reasoning](https://arxiv.org/abs/2505.18341)
*Miao Li,Wenhao Ding,Haohong Lin,Yiqi Lyu,Yihang Yao,Yuyou Zhang,Ding Zhao*

Main category: cs.RO

TL;DR: 论文提出使用多模态大语言模型将交通事故报告转化为结构化场景，以生成安全关键驾驶场景，支持自动驾驶算法的训练与评估。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要由人类正常驾驶行为组成，缺乏安全关键场景，限制了自动驾驶算法从风险或失败场景中学习的能力。

Method: 引入CrashAgent多智能体框架，解析多模态真实交通事故报告，生成道路布局及车辆行为，并在仿真中直接执行。

Result: 从布局重建准确性、碰撞率和多样性多角度评估生成的碰撞场景，并公开高质量大规模碰撞数据集。

Conclusion: 该方法为自动驾驶算法在安全关键场景中的开发提供了有效支持。

Abstract: Training and evaluating autonomous driving algorithms requires a diverse
range of scenarios. However, most available datasets predominantly consist of
normal driving behaviors demonstrated by human drivers, resulting in a limited
number of safety-critical cases. This imbalance, often referred to as a
long-tail distribution, restricts the ability of driving algorithms to learn
from crucial scenarios involving risk or failure, scenarios that are essential
for humans to develop driving skills efficiently. To generate such scenarios,
we utilize Multi-modal Large Language Models to convert crash reports of
accidents into a structured scenario format, which can be directly executed
within simulations. Specifically, we introduce CrashAgent, a multi-agent
framework designed to interpret multi-modal real-world traffic crash reports
for the generation of both road layouts and the behaviors of the ego vehicle
and surrounding traffic participants. We comprehensively evaluate the generated
crash scenarios from multiple perspectives, including the accuracy of layout
reconstruction, collision rate, and diversity. The resulting high-quality and
large-scale crash dataset will be publicly available to support the development
of safe driving algorithms in handling safety-critical situations.

</details>


### [919] [Reinforcement Learning for Ballbot Navigation in Uneven Terrain](https://arxiv.org/abs/2505.18417)
*Achkan Salehi*

Main category: cs.RO

TL;DR: 该论文提出了一个基于MuJoCo的开源球平衡机器人模拟器，展示了通过强化学习方法在随机生成的不平地形中有效导航的能力。


<details>
  <summary>Details</summary>
Motivation: 目前球平衡机器人的导航主要依赖控制理论方法，强化学习方法在该领域的应用较少且局限于特定子任务。强化学习不需要对环境动态进行简化假设，并能更灵活地结合额外观测数据，但相关研究仍不足。

Method: 论文使用基于MuJoCo的开源模拟器，通过经典的无模型强化学习方法，结合外感受观测和奖励塑形，训练导航策略。

Result: 实验结果表明，强化学习策略能够在合理的数据量（四到五小时，系统频率500Hz）下，有效导航通过随机生成的不平地形。

Conclusion: 该研究填补了强化学习在球平衡机器人控制与导航领域的空白，并提供了一个开源的模拟器，为后续研究奠定了基础。

Abstract: Ballbot (i.e. Ball balancing robot) navigation usually relies on methods
rooted in control theory (CT), and works that apply Reinforcement learning (RL)
to the problem remain rare while generally being limited to specific subtasks
(e.g. balance recovery). Unlike CT based methods, RL does not require
(simplifying) assumptions about environment dynamics (e.g. the absence of
slippage between the ball and the floor). In addition to this increased
accuracy in modeling, RL agents can easily be conditioned on additional
observations such as depth-maps without the need for explicit formulations from
first principles, leading to increased adaptivity. Despite those advantages,
there has been little to no investigation into the capabilities,
data-efficiency and limitations of RL based methods for ballbot control and
navigation. Furthermore, there is a notable absence of an open-source,
RL-friendly simulator for this task. In this paper, we present an open-source
ballbot simulation based on MuJoCo, and show that with appropriate conditioning
on exteroceptive observations as well as reward shaping, policies learned by
classical model-free RL methods are capable of effectively navigating through
randomly generated uneven terrain, using a reasonable amount of data (four to
five hours on a system operating at 500hz).

</details>


### [920] [VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning](https://arxiv.org/abs/2505.18719)
*Guanxing Lu,Wenkai Guo,Chubin Zhang,Yuheng Zhou,Haonan Jiang,Zifeng Gao,Yansong Tang,Ziwei Wang*

Main category: cs.RO

TL;DR: VLA-RL框架通过在线强化学习提升预训练视觉语言动作模型在机器人操作任务中的表现，解决了离线数据分布外泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言动作模型在模仿人类演示时，因依赖有限状态数据导致分布外场景执行失败。探索性在线学习方法可弥补这一缺陷。

Method: 提出轨迹级RL训练框架，将操作轨迹建模为多模态对话；用伪奖励标注微调视觉语言模型作为奖励模型；引入课程选择、向量化环境等技术提升稳定性。

Result: 在LIBERO的40项任务中超越基线4.5%，性能媲美π0-FAST商业模型，且测试时优化收益呈现早期推理扩展规律。

Conclusion: VLA-RL成功将在线RL与预训练VLA结合，为机器人操作任务提供了可扩展的强化学习框架。

Abstract: Recent high-capacity vision-language-action (VLA) models have demonstrated
impressive performance on a range of robotic manipulation tasks by imitating
human demonstrations. However, exploiting offline data with limited visited
states will cause execution failure in out-of-distribution scenarios.
Intuitively, an exploration-based method that improves on online collected data
at test time could address this limitation. We present VLA-RL, an algorithmic
and systematic framework that leverages online reinforcement learning (RL) to
improve pretrained auto-regressive VLAs in downstream tasks. Within a unified
perspective, we first introduce a trajectory-level RL formulation for
auto-regressive VLA training, which models general robotic manipulation
trajectory as multi-modal multi-turn conversation. To address the challenge of
sparse rewards, we fine-tune a pretrained vision-language model as a robotic
process reward model, which is trained on pseudo reward labels annotated on
automatically extracted task segments. To scale up, we identify several
implementation findings that improve the stability and efficiency including
curriculum selection strategy, GPU-balanced vectorized environments, batch
decoding, and critic warmup. VLA-RL enables OpenVLA-7B to surpass the strongest
finetuned baseline by 4.5% on 40 challenging robotic manipulation tasks in
LIBERO, and even matches the performance of advanced commercial models such as
$\pi_0$-FAST. Notably, we observe that VLA-RL benefits from increased test-time
optimization, indicating an early spark of inference scaling laws in robotics.

</details>


### [921] [MaskedManipulator: Versatile Whole-Body Control for Loco-Manipulation](https://arxiv.org/abs/2505.19086)
*Chen Tessler,Yifeng Jiang,Erwin Coumans,Zhengyi Luo,Gal Chechik,Xue Bin Peng*

Main category: cs.RO

TL;DR: 论文提出MaskedManipulator，一种通过两阶段学习开发的统一生成策略，用于实现高层次的全身灵巧操作控制。


<details>
  <summary>Details</summary>
Motivation: 当前基于物理的动画系统中，全身灵巧操作方法在特定交互任务上取得成功，但在高层次目标规范方面缺乏灵活性。为弥补这一差距，研究者提出了一种更直观的控制方式。

Method: 采用两阶段学习方法：首先训练跟踪控制器从大规模动作捕捉数据中重建人-物交互，然后将其提炼为MaskedManipulator策略，提供对角色身体和操作对象的直观控制。

Result: MaskedManipulator能够通过高层次目标（如目标物体姿态、关键角色姿态）合成必要的全身动作，实现复杂的移动-操作任务。

Conclusion: MaskedManipulator为虚拟角色提供了更交互式和逼真的控制方式，为动画系统开辟了新途径。

Abstract: Humans interact with their world while leveraging precise full-body control
to achieve versatile goals. This versatility allows them to solve long-horizon,
underspecified problems, such as placing a cup in a sink, by seamlessly
sequencing actions like approaching the cup, grasping, transporting it, and
finally placing it in the sink. Such goal-driven control can enable new
procedural tools for animation systems, enabling users to define partial
objectives while the system naturally ``fills in'' the intermediate motions.
However, while current methods for whole-body dexterous manipulation in
physics-based animation achieve success in specific interaction tasks, they
typically employ control paradigms (e.g., detailed kinematic motion tracking,
continuous object trajectory following, or direct VR teleoperation) that offer
limited versatility for high-level goal specification across the entire coupled
human-object system. To bridge this gap, we present MaskedManipulator, a
unified and generative policy developed through a two-stage learning approach.
First, our system trains a tracking controller to physically reconstruct
complex human-object interactions from large-scale human mocap datasets. This
tracking controller is then distilled into MaskedManipulator, which provides
users with intuitive control over both the character's body and the manipulated
object. As a result, MaskedManipulator enables users to specify complex
loco-manipulation tasks through intuitive high-level objectives (e.g., target
object poses, key character stances), and MaskedManipulator then synthesizes
the necessary full-body actions for a physically simulated humanoid to achieve
these goals, paving the way for more interactive and life-like virtual
characters.

</details>


### [922] [Reinforcement Twinning for Hybrid Control of Flapping-Wing Drones](https://arxiv.org/abs/2505.18201)
*Romain Poletti,Lorenzo Schena,Lilla Koloszar,Joris Degroote,Miguel Alfonso Mendez*

Main category: cs.RO

TL;DR: 该论文提出了一种结合模型自由和模型基础的混合控制方法，用于控制扑翼无人机的飞行，通过强化学习与数字孪生技术的结合，提高了控制效率和性能。


<details>
  <summary>Details</summary>
Motivation: 扑翼无人机的飞行控制面临时间变化、非线性和欠驱动动力学的挑战，传统模型基础方法难以精确建模，而模型自由方法在高维非线性控制目标中效率低下。

Method: 论文采用了一种混合模型自由/模型基础的控制方法，结合强化学习和自适应数字孪生技术，通过转移学习、模仿学习和经验共享来实现控制。

Result: 在三种不同的模型初始化场景下，混合学习方法均表现出优于纯模型自由和模型基础方法的性能。

Conclusion: 混合学习方法在扑翼无人机控制中表现出高效和鲁棒性，为复杂动力学系统的控制提供了新的解决方案。

Abstract: Controlling the flight of flapping-wing drones requires versatile controllers
that handle their time-varying, nonlinear, and underactuated dynamics from
incomplete and noisy sensor data. Model-based methods struggle with accurate
modeling, while model-free approaches falter in efficiently navigating very
high-dimensional and nonlinear control objective landscapes. This article
presents a novel hybrid model-free/model-based approach to flight control based
on the recently proposed reinforcement twinning algorithm. The model-based (MB)
approach relies on an adjoint formulation using an adaptive digital twin,
continuously identified from live trajectories, while the model-free (MF)
approach relies on reinforcement learning. The two agents collaborate through
transfer learning, imitation learning, and experience sharing using the real
environment, the digital twin and a referee. The latter selects the best agent
to interact with the real environment based on performance within the digital
twin and a real-to-virtual environment consistency ratio. The algorithm is
evaluated for controlling the longitudinal dynamics of a flapping-wing drone,
with the environment simulated as a nonlinear, time-varying dynamical system
under the influence of quasi-steady aerodynamic forces. The hybrid control
learning approach is tested with three types of initialization of the adaptive
model: (1) offline identification using previously available data, (2) random
initialization with full online identification, and (3) offline pre-training
with an estimation bias, followed by online adaptation. In all three scenarios,
the proposed hybrid learning approach demonstrates superior performance
compared to purely model-free and model-based methods.

</details>


### [923] [Grounding Bodily Awareness in Visual Representations for Efficient Policy Learning](https://arxiv.org/abs/2505.18487)
*Junlin Wang,Zhiyun Lin*

Main category: cs.RO

TL;DR: 本文提出了一种名为ICon的对比学习方法，通过分离视觉Transformer中的主体与环境特征，提升机器人操作任务中的策略学习效率。


<details>
  <summary>Details</summary>
Motivation: 机器人操作任务中，复杂的身体动力学使得学习有效的视觉表示成为挑战。本文旨在探索如何利用携带身体相关线索的视觉表示来提升下游任务的策略学习效率。

Method: 提出了ICon方法，一种应用于视觉Transformer（ViT）的对比学习技术，通过分离主体与环境特征，嵌入身体特定的归纳偏置。

Result: 实验表明，ICon不仅提升了多种操作任务的策略性能，还促进了不同机器人间的策略迁移。

Conclusion: ICon通过对比学习有效提升了机器人操作任务的策略学习效率，并展示了跨机器人迁移的潜力。

Abstract: Learning effective visual representations for robotic manipulation remains a
fundamental challenge due to the complex body dynamics involved in action
execution. In this paper, we study how visual representations that carry
body-relevant cues can enable efficient policy learning for downstream robotic
manipulation tasks. We present $\textbf{I}$nter-token $\textbf{Con}$trast
($\textbf{ICon}$), a contrastive learning method applied to the token-level
representations of Vision Transformers (ViTs). ICon enforces a separation in
the feature space between agent-specific and environment-specific tokens,
resulting in agent-centric visual representations that embed body-specific
inductive biases. This framework can be seamlessly integrated into end-to-end
policy learning by incorporating the contrastive loss as an auxiliary
objective. Our experiments show that ICon not only improves policy performance
across various manipulation tasks but also facilitates policy transfer across
different robots. The project website: https://github.com/HenryWJL/icon

</details>


### [924] [Towards Humanoid Robot Autonomy: A Dynamic Architecture Integrating Continuous thought Machines (CTM) and Model Context Protocol (MCP)](https://arxiv.org/abs/2505.19339)
*Libo Wang*

Main category: cs.RO

TL;DR: 该研究提出了一种动态架构CTM-MCP，通过tick-slab理论并行方案和秩压缩实现参数抑制，解决了人形机器人在陌生场景中静态预设与缺乏自主编码能力的问题。实验证明该架构在七项指标上表现有效。


<details>
  <summary>Details</summary>
Motivation: 针对人形机器人在陌生场景中静态预设的‘思考-规划-行动’模式与因缺乏自主编码能力而高度程序化的‘调用工具-返回结果’模式之间的差距，研究旨在实现基于连续思维的自主动态编码，使机器人能执行类人的自主行动。

Method: 设计了连接连续思维机器（CTM）和模型上下文协议（MCP）的动态架构，提出tick-slab理论并行方案，并采用秩压缩进行参数抑制。实验使用OpenAI的o4-mini-high构建环境，并引入扩展SayCan数据集进行九轮测试。

Result: 实验数据显示，CTM-MCP架构在任务成功率（TSR）、执行成功率（ESR）、平均情节长度（AEL）、ROSCOE、REVEAL、熟练度自评（PSA）及任务效能（TE）七项指标上均验证了其可行性和有效性。

Conclusion: 该研究为探索基于连续思维的人形机器人自主动态编码提供了实践参考，推动实现类人自主行动的目标。

Abstract: To address the gaps between the static pre-set "thinking-planning-action" of
humanoid robots in unfamiliar scenarios and the highly programmed "call
tool-return result" due to the lack of autonomous coding capabilities, this
work designs a dynamic architecture connecting continuous thought machines
(CTM) and model context protocol (MCP). It proposes a theoretical parallel
solution through tick-slab and uses rank compression to achieve parameter
suppression to provide a solution for achieving autonomous actions due to
autonomous coding. The researcher used a simulation-based experiment using
OpenAI's o4-mini-high as a tool to build the experimental environment, and
introduced the extended SayCan dataset to conduct nine epochs of experiments.
The experimental results show that the CTM-MCP architecture is feasible and
effective through the data results of seven metrics: task success rate (TSR),
execution success rate (ESR), average episode length (AEL), ROSCOE, REVEAL,
proficiency self-assessment (PSA), task effectiveness (TE). In practice, it
provides a reference experience for exploring the autonomous dynamic coding of
humanoid robots based on continuous thinking to achieve human-like autonomous
actions.

</details>


### [925] [One Policy but Many Worlds: A Scalable Unified Policy for Versatile Humanoid Locomotion](https://arxiv.org/abs/2505.18780)
*Yahao Fan,Tianxiang Gui,Kaiyang Ji,Shutong Ding,Chixuan Zhang,Jiayuan Gu,Jingyi Yu,Jingya Wang,Ye Shi*

Main category: cs.RO

TL;DR: DreamPolicy提出了一种统一的框架，通过整合离线数据和扩散驱动的运动合成，使单个策略能够掌握多样化地形并在未见过的场景中实现零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在人形机器人运动控制中面临可扩展性挑战，需要任务特定的奖励且难以利用不断增长的数据集。

Method: DreamPolicy引入了Humanoid Motion Imagery (HMI)，通过自回归地形感知扩散规划器合成未来状态预测，并利用离线数据训练策略。

Result: 实验表明，DreamPolicy在训练环境中平均成功率90%，在未见地形上比主流方法高出20%，并能泛化到扰动和复合场景。

Conclusion: DreamPolicy通过统一离线数据、基于扩散的轨迹合成和策略优化，克服了“一个任务，一个策略”的瓶颈，为可扩展的数据驱动人形控制建立了新范式。

Abstract: Humanoid locomotion faces a critical scalability challenge: traditional
reinforcement learning (RL) methods require task-specific rewards and struggle
to leverage growing datasets, even as more training terrains are introduced. We
propose DreamPolicy, a unified framework that enables a single policy to master
diverse terrains and generalize zero-shot to unseen scenarios by systematically
integrating offline data and diffusion-driven motion synthesis. At its core,
DreamPolicy introduces Humanoid Motion Imagery (HMI) - future state predictions
synthesized through an autoregressive terrain-aware diffusion planner curated
by aggregating rollouts from specialized policies across various distinct
terrains. Unlike human motion datasets requiring laborious retargeting, our
data directly captures humanoid kinematics, enabling the diffusion planner to
synthesize "dreamed" trajectories that encode terrain-specific physical
constraints. These trajectories act as dynamic objectives for our
HMI-conditioned policy, bypassing manual reward engineering and enabling
cross-terrain generalization. DreamPolicy addresses the scalability limitations
of prior methods: while traditional RL fails to exploit growing datasets, our
framework scales seamlessly with more offline data. As the dataset expands, the
diffusion prior learns richer locomotion skills, which the policy leverages to
master new terrains without retraining. Experiments demonstrate that
DreamPolicy achieves average 90% success rates in training environments and an
average of 20% higher success on unseen terrains than the prevalent method. It
also generalizes to perturbed and composite scenarios where prior approaches
collapse. By unifying offline data, diffusion-based trajectory synthesis, and
policy optimization, DreamPolicy overcomes the "one task, one policy"
bottleneck, establishing a paradigm for scalable, data-driven humanoid control.

</details>


### [926] [Guided by Guardrails: Control Barrier Functions as Safety Instructors for Robotic Learning](https://arxiv.org/abs/2505.18858)
*Maeva Guerrier,Karthik Soma,Hassan Fouad,Giovanni Beltrame*

Main category: cs.RO

TL;DR: 论文提出了一种结合控制屏障函数（CBFs）和强化学习（RL）的新方法，以解决传统RL在机器人安全学习中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习框架通过单一负奖励和即时终止来模拟安全问题，无法捕捉不安全行为的持续后果，这限制了学习基机器人系统的广泛应用。

Method: 引入了一种新方法，通过持续施加负奖励而不终止回合来模拟不安全行为的持续影响，并结合控制屏障函数（CBFs）来引导机器人学习安全行为。

Result: 实验表明，标准RL方法在这种模型下表现不佳，而结合CBFs的方法能有效帮助机器人避免危险区域并提升学习效果。

Conclusion: 结合CBFs和RL的方法为安全机器人学习提供了新的可能性，并在仿真和实际机器人实验中验证了其有效性。

Abstract: Safety stands as the primary obstacle preventing the widespread adoption of
learning-based robotic systems in our daily lives. While reinforcement learning
(RL) shows promise as an effective robot learning paradigm, conventional RL
frameworks often model safety by using single scalar negative rewards with
immediate episode termination, failing to capture the temporal consequences of
unsafe actions (e.g., sustained collision damage). In this work, we introduce a
novel approach that simulates these temporal effects by applying continuous
negative rewards without episode termination. Our experiments reveal that
standard RL methods struggle with this model, as the accumulated negative
values in unsafe zones create learning barriers. To address this challenge, we
demonstrate how Control Barrier Functions (CBFs), with their proven safety
guarantees, effectively help robots avoid catastrophic regions while enhancing
learning outcomes. We present three CBF-based approaches, each integrating
traditional RL methods with Control Barrier Functions, guiding the agent to
learn safe behavior. Our empirical analysis, conducted in both simulated
environments and real-world settings using a four-wheel differential drive
robot, explores the possibilities of employing these approaches for safe
robotic learning.

</details>


### [927] [WorldEval: World Model as Real-World Robot Policies Evaluator](https://arxiv.org/abs/2505.19017)
*Yaxuan Li,Yichen Zhu,Junjie Wen,Chaomin Shen,Yi Xu*

Main category: cs.RO

TL;DR: 该论文提出使用世界模型作为评估机器人策略的代理方法，通过Policy2Vec和WorldEval实现高效、安全的在线评估。


<details>
  <summary>Details</summary>
Motivation: 当前机器人策略在真实场景中的评估耗时且复杂，尤其在任务数量增加和环境变化时。需要一种可扩展、可重复且可靠的评估方法。

Method: 提出Policy2Vec方法，将视频生成模型转化为世界模拟器，生成遵循潜在动作的机器人视频；并设计WorldEval自动化评估流程，在线评估机器人策略。

Result: WorldEval能有效排序不同机器人策略及单个策略内的检查点，并作为安全检测器防止危险动作。实验表明其在真实场景中与策略性能强相关，且优于现有方法。

Conclusion: 世界模型可作为机器人策略评估的有效代理，Policy2Vec和WorldEval方法显著提升了评估效率和安全性。

Abstract: The field of robotics has made significant strides toward developing
generalist robot manipulation policies. However, evaluating these policies in
real-world scenarios remains time-consuming and challenging, particularly as
the number of tasks scales and environmental conditions change. In this work,
we demonstrate that world models can serve as a scalable, reproducible, and
reliable proxy for real-world robot policy evaluation. A key challenge is
generating accurate policy videos from world models that faithfully reflect the
robot actions. We observe that directly inputting robot actions or using
high-dimensional encoding methods often fails to generate action-following
videos. To address this, we propose Policy2Vec, a simple yet effective approach
to turn a video generation model into a world simulator that follows latent
action to generate the robot video. We then introduce WorldEval, an automated
pipeline designed to evaluate real-world robot policies entirely online.
WorldEval effectively ranks various robot policies and individual checkpoints
within a single policy, and functions as a safety detector to prevent dangerous
actions by newly developed robot models. Through comprehensive paired
evaluations of manipulation policies in real-world environments, we demonstrate
a strong correlation between policy performance in WorldEval and real-world
scenarios. Furthermore, our method significantly outperforms popular methods
such as real-to-sim approach.

</details>


### [928] [Situationally-Aware Dynamics Learning](https://arxiv.org/abs/2505.19574)
*Alejandro Murillo-Gonzalez,Lantao Liu*

Main category: cs.RO

TL;DR: 提出一种新型在线学习框架，帮助自主机器人在复杂非结构化环境中实时适应潜在未观测因素，提升环境理解与决策能力。


<details>
  <summary>Details</summary>
Motivation: 自主机器人在复杂非结构化环境中运行时，常因未观测的潜在因素导致对内外状态理解不足，影响决策效果。解决这一问题可增强机器人对操作环境的深层理解。

Method: 采用广义隐参数马尔可夫决策过程（GHP-MDP）建模未观测参数的影响，通过贝叶斯在线变点检测的多变量扩展实时学习状态转移联合分布，生成符号化情境表示以指导自适应决策。

Result: 在非结构化地形导航任务中验证，仿真与实物实验均显示数据效率、策略性能显著提升，并产生更安全的自适应导航策略。

Conclusion: 该框架通过在线学习潜在状态表示，有效提升机器人在动态不确定环境中的鲁棒性与安全性，为复杂场景下的自主决策提供新思路。

Abstract: Autonomous robots operating in complex, unstructured environments face
significant challenges due to latent, unobserved factors that obscure their
understanding of both their internal state and the external world. Addressing
this challenge would enable robots to develop a more profound grasp of their
operational context. To tackle this, we propose a novel framework for online
learning of hidden state representations, with which the robots can adapt in
real-time to uncertain and dynamic conditions that would otherwise be ambiguous
and result in suboptimal or erroneous behaviors. Our approach is formalized as
a Generalized Hidden Parameter Markov Decision Process, which explicitly models
the influence of unobserved parameters on both transition dynamics and reward
structures. Our core innovation lies in learning online the joint distribution
of state transitions, which serves as an expressive representation of latent
ego- and environmental-factors. This probabilistic approach supports the
identification and adaptation to different operational situations, improving
robustness and safety. Through a multivariate extension of Bayesian Online
Changepoint Detection, our method segments changes in the underlying data
generating process governing the robot's dynamics. The robot's transition model
is then informed with a symbolic representation of the current situation
derived from the joint distribution of latest state transitions, enabling
adaptive and context-aware decision-making. To showcase the real-world
effectiveness, we validate our approach in the challenging task of unstructured
terrain navigation, where unmodeled and unmeasured terrain characteristics can
significantly impact the robot's motion. Extensive experiments in both
simulation and real world reveal significant improvements in data efficiency,
policy performance, and the emergence of safer, adaptive navigation strategies.

</details>


### [929] [From Single Images to Motion Policies via Video-Generation Environment Representations](https://arxiv.org/abs/2505.19306)
*Weiming Zhi,Ziyong Ma,Tianyi Zhang,Matthew Johnson-Roberson*

Main category: cs.RO

TL;DR: 论文提出VGER框架，通过单张RGB图像生成环境表示并实现无碰撞运动规划，利用视频生成模型构建多视角数据集，结合3D基础模型和隐式表示训练，最终在多样化场景中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 自主机器人需根据环境几何生成无碰撞运动策略，但传统单目深度估计方法存在视锥形误差，难以直接用于运动规划。因此需要一种能直接从单图像构建环境表示并生成合规运动的新方法。

Method: 1) 用视频生成模型基于输入图像生成移动视角视频 2) 将视频帧输入预训练3D基础模型生成稠密点云 3) 提出多尺度噪声方法训练环境结构的隐式表示 4) 构建符合几何约束的运动生成模型

Result: 在多样室内外场景测试表明，VGER能有效从单RGB图像生成平滑运动轨迹，且运动规划与场景几何保持高度一致性。

Conclusion: VGER框架通过结合视频生成与3D基础模型，实现了单图像到环境表示及合规运动生成的端到端解决方案，为机器人自主导航提供了新思路。

Abstract: Autonomous robots typically need to construct representations of their
surroundings and adapt their motions to the geometry of their environment.
Here, we tackle the problem of constructing a policy model for collision-free
motion generation, consistent with the environment, from a single input RGB
image. Extracting 3D structures from a single image often involves monocular
depth estimation. Developments in depth estimation have given rise to large
pre-trained models such as DepthAnything. However, using outputs of these
models for downstream motion generation is challenging due to frustum-shaped
errors that arise. Instead, we propose a framework known as Video-Generation
Environment Representation (VGER), which leverages the advances of large-scale
video generation models to generate a moving camera video conditioned on the
input image. Frames of this video, which form a multiview dataset, are then
input into a pre-trained 3D foundation model to produce a dense point cloud. We
then introduce a multi-scale noise approach to train an implicit representation
of the environment structure and build a motion generation model that complies
with the geometry of the representation. We extensively evaluate VGER over a
diverse set of indoor and outdoor environments. We demonstrate its ability to
produce smooth motions that account for the captured geometry of a scene, all
from a single RGB input image.

</details>


### [930] [TeViR: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning](https://arxiv.org/abs/2505.19769)
*Yuhui Chen,Haoran Li,Zhennan Jiang,Haowei Wen,Dongbin Zhao*

Main category: cs.RO

TL;DR: 论文提出TeViR方法，利用预训练文本-视频扩散模型生成密集奖励，提升机器人操作任务中的样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉语言模型（VLM）的奖励工程在机器人操作任务中存在稀疏奖励问题，限制了样本效率，亟需一种更高效的奖励生成方法。

Method: TeViR通过预训练文本-视频扩散模型，比较预测图像序列与当前观察结果，生成密集奖励信号。

Result: 在11项复杂机器人任务中，TeViR优于传统稀疏奖励方法及其他SOTA方法，无需环境真实奖励即可实现更高样本效率和性能。

Conclusion: TeViR能高效指导复杂环境中的智能体，有望推动强化学习在机器人操作领域的应用。

Abstract: Developing scalable and generalizable reward engineering for reinforcement
learning (RL) is crucial for creating general-purpose agents, especially in the
challenging domain of robotic manipulation. While recent advances in reward
engineering with Vision-Language Models (VLMs) have shown promise, their sparse
reward nature significantly limits sample efficiency. This paper introduces
TeViR, a novel method that leverages a pre-trained text-to-video diffusion
model to generate dense rewards by comparing the predicted image sequence with
current observations. Experimental results across 11 complex robotic tasks
demonstrate that TeViR outperforms traditional methods leveraging sparse
rewards and other state-of-the-art (SOTA) methods, achieving better sample
efficiency and performance without ground truth environmental rewards. TeViR's
ability to efficiently guide agents in complex environments highlights its
potential to advance reinforcement learning applications in robotic
manipulation.

</details>


### [931] [Learning Dynamics under Environmental Constraints via Measurement-Induced Bundle Structures](https://arxiv.org/abs/2505.19521)
*Dongzhe Zheng,Wenjie Mei*

Main category: cs.RO

TL;DR: 该论文提出了一种几何框架，通过纤维丛结构统一测量、约束和动力学学习，结合神经ODE实现约束保持的连续时间动力学学习，在有限和不确定感知条件下显著提升学习效率和约束满足。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要全局约束或使用概率滤波，未能充分利用局部测量和约束的几何结构。现代机器人等领域需要在环境约束下学习未知动力学，尤其在约束信息仅局部可用且不确定时更具挑战性。

Method: 提出基于状态空间纤维丛结构的几何框架，实现测量、约束与动力学学习的统一。通过测量感知的控制屏障函数适应局部感知条件，并集成神经ODE保持几何约束。

Result: 理论保证了学习收敛性和约束满足与感知质量的相关性。仿真显示在有限和不确定感知条件下，相比传统方法显著提升了学习效率和约束满足。

Conclusion: 该几何框架不仅实现了高效动力学学习，还为与强化学习的整合提供了新方向，特别适用于感知受限的复杂场景。

Abstract: Learning unknown dynamics under environmental (or external) constraints is
fundamental to many fields (e.g., modern robotics), particularly challenging
when constraint information is only locally available and uncertain. Existing
approaches requiring global constraints or using probabilistic filtering fail
to fully exploit the geometric structure inherent in local measurements (by
using, e.g., sensors) and constraints. This paper presents a geometric
framework unifying measurements, constraints, and dynamics learning through a
fiber bundle structure over the state space. This naturally induced geometric
structure enables measurement-aware Control Barrier Functions that adapt to
local sensing (or measurement) conditions. By integrating Neural ODEs, our
framework learns continuous-time dynamics while preserving geometric
constraints, with theoretical guarantees of learning convergence and constraint
satisfaction dependent on sensing quality. The geometric framework not only
enables efficient dynamics learning but also suggests promising directions for
integration with reinforcement learning approaches. Extensive simulations
demonstrate significant improvements in both learning efficiency and constraint
satisfaction over traditional methods, especially under limited and uncertain
sensing conditions.

</details>


### [932] [Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects](https://arxiv.org/abs/2505.20223)
*Yixin Cui,Haotian Lin,Shuo Yang,Yixiao Wang,Yanjun Huang,Hong Chen*

Main category: cs.RO

TL;DR: 本文探讨了思维链（CoT）方法如何提升自动驾驶模型的推理能力，并提出了结合自学习以实现系统自我进化的见解。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在语义理解和逻辑推理能力上的显著提升，研究如何利用这些能力改进自动驾驶系统的性能成为重要课题。

Method: 通过文献综述，系统分析了CoT在自动驾驶中的动机、方法、挑战及未来研究方向，并提出结合自学习的思路。

Result: 研究发现CoT能显著提升自动驾驶系统处理复杂场景的能力，并建立了一个动态更新的文献和开源项目库。

Conclusion: CoT方法在自动驾驶中展现出巨大潜力，结合自学习可进一步推动系统的自我进化，未来研究值得深入探索。

Abstract: The rapid evolution of large language models in natural language processing
has substantially elevated their semantic understanding and logical reasoning
capabilities. Such proficiencies have been leveraged in autonomous driving
systems, contributing to significant improvements in system performance. Models
such as OpenAI o1 and DeepSeek-R1, leverage Chain-of-Thought (CoT) reasoning,
an advanced cognitive method that simulates human thinking processes,
demonstrating remarkable reasoning capabilities in complex tasks. By
structuring complex driving scenarios within a systematic reasoning framework,
this approach has emerged as a prominent research focus in autonomous driving,
substantially improving the system's ability to handle challenging cases. This
paper investigates how CoT methods improve the reasoning abilities of
autonomous driving models. Based on a comprehensive literature review, we
present a systematic analysis of the motivations, methodologies, challenges,
and future research directions of CoT in autonomous driving. Furthermore, we
propose the insight of combining CoT with self-learning to facilitate
self-evolution in driving systems. To ensure the relevance and timeliness of
this study, we have compiled a dynamic repository of literature and open-source
projects, diligently updated to incorporate forefront developments. The
repository is publicly available at
https://github.com/cuiyx1720/Awesome-CoT4AD.

</details>


### [933] [EgoZero: Robot Learning from Smart Glasses](https://arxiv.org/abs/2505.20290)
*Vincent Liu,Ademi Adeniji,Haotian Zhan,Raunaq Bhirangi,Pieter Abbeel,Lerrel Pinto*

Main category: cs.RO

TL;DR: EgoZero系统利用人类佩戴智能眼镜的演示数据，无需机器人数据即可学习稳健的操控策略，实现零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 当前机器人策略在现实世界中远落后于人类基本能力，而人类与物理世界的频繁互动数据尚未被充分利用于机器人学习。

Method: EgoZero通过人类佩戴Project Aria智能眼镜的演示数据，提取可执行的机器人动作，压缩视觉观察为形态无关的状态表示，并学习闭环策略。

Result: 在7项操控任务中，EgoZero策略在Franka Panda机器人上实现了70%的成功率，每任务仅需20分钟数据收集。

Conclusion: 研究表明，野外人类数据可作为机器人学习的可扩展基础，为机器人提供丰富、多样且自然的训练数据。

Abstract: Despite recent progress in general purpose robotics, robot policies still lag
far behind basic human capabilities in the real world. Humans interact
constantly with the physical world, yet this rich data resource remains largely
untapped in robot learning. We propose EgoZero, a minimal system that learns
robust manipulation policies from human demonstrations captured with Project
Aria smart glasses, $\textbf{and zero robot data}$. EgoZero enables: (1)
extraction of complete, robot-executable actions from in-the-wild, egocentric,
human demonstrations, (2) compression of human visual observations into
morphology-agnostic state representations, and (3) closed-loop policy learning
that generalizes morphologically, spatially, and semantically. We deploy
EgoZero policies on a gripper Franka Panda robot and demonstrate zero-shot
transfer with 70% success rate over 7 manipulation tasks and only 20 minutes of
data collection per task. Our results suggest that in-the-wild human data can
serve as a scalable foundation for real-world robot learning - paving the way
toward a future of abundant, diverse, and naturalistic training data for
robots. Code and videos are available at https://egozero-robot.github.io.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [934] [Simulating Macroeconomic Expectations using LLM Agents](https://arxiv.org/abs/2505.17648)
*Jianhao Lin,Lexuan Sun,Yixin Yan*

Main category: econ.GN

TL;DR: 该论文提出了一种利用LLM智能体模拟宏观经济预期形成的新框架，结果显示LLM智能体虽生成预期较人类同质化，但仍能有效捕捉关键异质性及预期驱动因素。


<details>
  <summary>Details</summary>
Motivation: 传统调查方法在宏观经济预期研究中存在局限性，作者希望通过AI行为科学的新视角，利用LLM智能体模拟人类预期形成过程，以补充现有研究方法。

Method: 构建数千个配备个性化特征、先验预期和知识模块的LLM智能体，复现针对通胀和失业率的家庭与专家调查实验，并进行模块消融分析。

Result: LLM智能体生成的预期比人类更同质化，但能有效反映主体间异质性和预期形成的关键驱动因素，其中先验预期模块对异质性模拟起决定性作用。

Conclusion: 该方法为宏观经济研究提供了新工具，展示了AI行为科学在理解复杂经济行为方面的潜力，尤其强调了先验知识在预期建模中的核心地位。

Abstract: We introduce a novel framework for simulating macroeconomic expectation
formation using Large Language Model-Empowered Agents (LLM Agents). By
constructing thousands of LLM Agents equipped with modules for personal
characteristics, prior expectations, and knowledge, we replicate a survey
experiment involving households and experts on inflation and unemployment. Our
results show that although the expectations and thoughts generated by LLM
Agents are more homogeneous than those of human participants, they still
effectively capture key heterogeneity across agents and the underlying drivers
of expectation formation. Furthermore, a module-ablation exercise highlights
the critical role of prior expectations in simulating such heterogeneity. This
approach complements traditional survey methods and offers new insights into AI
behavioral science in macroeconomic research.

</details>


### [935] [An AI Capability Threshold for Rent-Funded Universal Basic Income in an AI-Automated Economy](https://arxiv.org/abs/2505.18687)
*Aran Nayebi*

Main category: econ.GN

TL;DR: 研究发现，AI资本利润在特定条件下可无需增税或创造新工作即可持续资助全民基本收入(UBI)。当前经济参数下，AI生产力需达到现有自动化水平的5-6倍即可在最坏情况下资助占GDP 11%的UBI。提高公共收入份额或调控市场竞争可显著降低所需AI能力门槛。


<details>
  <summary>Details</summary>
Motivation: 探讨AI资本利润是否能在不增加税收或不创造新工作的条件下，可持续地资助全民基本收入(UBI)，并分析影响这一目标的经济因素和政策杠杆。

Method: 采用Solow-Zeira经济模型，分析在连续可自动化任务、恒定净储蓄率和任务弹性小于1的经济体中，AI能力阈值（相对于现有自动化生产力的AI生产力水平）如何随不同经济情景变化。

Result: 当前经济参数下，AI生产力需达到现有自动化水平的5-6倍即可资助占GDP 11%的UBI。将公共收入份额从15%提高到33%可将所需AI能力门槛减半至3倍，但超过50%后收益递减。市场结构对结果有显著影响：垄断或寡头市场通过增加经济租金降低门槛，而激烈竞争则大幅提高门槛。

Conclusion: 政策建议包括：在最小化运营成本的前提下最大化公共收入份额，以及战略性地管理市场竞争，以确保AI能力的提升能在现实技术进步情景下转化为显著的社会效益。

Abstract: We derive the first closed-form condition under which artificial intelligence
(AI) capital profits could sustainably finance a universal basic income (UBI)
without additional taxes or new job creation. In a Solow-Zeira economy
characterized by a continuum of automatable tasks, a constant net saving rate
$s$, and task-elasticity $\sigma < 1$, we analyze how the AI capability
threshold--defined as the productivity level of AI relative to pre-AI
automation--varies under different economic scenarios. At present economic
parameters, we find that AI systems must achieve only approximately 5-6 times
existing automation productivity to finance an 11\%-of-GDP UBI, in the worst
case situation where \emph{no} new jobs or tasks are created.
  Our analysis also reveals some specific policy levers: raising public revenue
share (e.g. profit taxation) of AI capital from the current 15\% to about 33\%
halves the required AI capability threshold to attain UBI to 3 times existing
automotion productivity, but gains diminish beyond 50\% public revenue share,
especially if regulatory costs increase. Market structure also strongly affects
outcomes: monopolistic or concentrated oligopolistic markets reduce the
threshold by increasing economic rents, whereas heightened competition
significantly raises it.
  Overall, these results suggest a couple policy recommendations: maximizing
public revenue share up to a point so that operating costs are minimized, and
strategically managing market competition can ensure AI's growing capabilities
translate into meaningful social benefits within realistic technological
progress scenarios.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [936] [Learning-Augmented Online Bipartite Fractional Matching](https://arxiv.org/abs/2505.19252)
*Davin Choo,Billy Jin,Yongho Shin*

Main category: cs.DS

TL;DR: 该论文研究了在线二分图分数匹配问题，提出了基于学习增强的算法，通过建议匹配来优化传统策略，并在顶点加权和AdWords问题上取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 在线二分图匹配是在线优化中的基础问题，具有重要的理论和实际应用价值。近年来，学习增强算法的进展激发了研究者探索如何在每次迭代中利用建议匹配来提升算法性能。

Method: 论文提出了针对顶点加权和无加权变体的算法，这些算法通过结合建议匹配和传统策略，显著优于随机选择的“抛硬币”策略。顶点加权算法还扩展到AdWords问题。

Result: 算法在顶点加权和AdWords问题上表现优异，显著提升了Mahdian等人的经典工作。同时，论文还建立了任何算法在鲁棒性和一致性之间权衡的硬度界限。

Conclusion: 通过理论和实验验证，论文证明了所提算法的有效性，为在线二分图匹配问题提供了新的解决方案，并在实际应用中展示了其潜力。

Abstract: Online bipartite matching is a fundamental problem in online optimization,
extensively studied both in its integral and fractional forms due to its
theoretical significance and practical applications, such as online advertising
and resource allocation. Motivated by recent progress in learning-augmented
algorithms, we study online bipartite fractional matching when the algorithm is
given advice in the form of a suggested matching in each iteration. We develop
algorithms for both the vertex-weighted and unweighted variants that provably
dominate the naive "coin flip" strategy of randomly choosing between the
advice-following and advice-free algorithms. Moreover, our algorithm for the
vertex-weighted setting extends to the AdWords problem under the small bids
assumption, yielding a significant improvement over the seminal work of
Mahdian, Nazerzadeh, and Saberi (EC 2007, TALG 2012). Complementing our
positive results, we establish a hardness bound on the robustness-consistency
tradeoff that is attainable by any algorithm. We empirically validate our
algorithms through experiments on synthetic and real-world data.

</details>


### [937] [Demand Selection for VRP with Emission Quota](https://arxiv.org/abs/2505.19315)
*Farid Najar,Dominique Barth,Yann Strozecki*

Main category: cs.DS

TL;DR: 该研究针对带排放配额的车辆路径问题（QVRP），提出了一种需求选择方法（MFVA），旨在最小化遗漏配送数量并遵守污染配额。研究发现，在此静态问题设置下，传统运筹学方法优于机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决带排放配额的车辆路径问题（QVRP）中的需求选择部分（MFVA），即在满足污染配额的前提下，最小化遗漏配送数量。

Method: 结合运筹学（OR）和机器学习（ML）方法，提出多种选择遗漏包裹的策略，其中车辆路径构建采用传统OR方法。

Result: 结果表明，在此静态问题设置下，传统OR方法的表现始终优于ML方法。

Conclusion: 在QVRP的静态需求选择问题中，传统OR方法比ML方法更有效。

Abstract: Combinatorial optimization (CO) problems are traditionally addressed using
Operations Research (OR) methods, including metaheuristics. In this study, we
introduce a demand selection problem for the Vehicle Routing Problem (VRP) with
an emission quota, referred to as QVRP. The objective is to minimize the number
of omitted deliveries while respecting the pollution quota. We focus on the
demand selection part, called Maximum Feasible Vehicle Assignment (MFVA), while
the construction of a routing for the VRP instance is solved using classical OR
methods. We propose several methods for selecting the packages to omit, both
from machine learning (ML) and OR. Our results show that, in this static
problem setting, classical OR-based methods consistently outperform ML-based
approaches.

</details>


### [938] [Private Geometric Median in Nearly-Linear Time](https://arxiv.org/abs/2505.20189)
*Syamantak Kumar,Daogao Liu,Kevin Tian,Chutong Yang*

Main category: cs.DS

TL;DR: 本文提出了一种改进的差分隐私算法，用于高效计算数据集的几何中位数，在保持样本复杂度最优的同时显著提升了运行效率。


<details>
  <summary>Details</summary>
Motivation: 几何中位数估计是计算几何中的基础问题，也是均值估计的鲁棒替代方案。现有差分隐私算法虽样本最优但计算效率低，需改进。

Method: 结合子采样技术、几何聚合工具（受FriendlyCore启发）优化预热阶段，并定制分析DP-SGD在几何中位数目标上的敏感性。

Result: 新算法在保持α近似精度和√d/(αε)样本复杂度的前提下，将运行时间降至近线性O~(nd + d/α²)，接近非隐私一阶方法的计算成本。

Conclusion: 该研究实现了差分隐私几何中位数计算的效率突破，证明通过算法创新可在不增加样本量的情况下显著降低计算开销。

Abstract: Estimating the geometric median of a dataset is a robust counterpart to mean
estimation, and is a fundamental problem in computational geometry. Recently,
[HSU24] gave an $(\varepsilon, \delta)$-differentially private algorithm
obtaining an $\alpha$-multiplicative approximation to the geometric median
objective, $\frac 1 n \sum_{i \in [n]} \|\cdot - \mathbf{x}_i\|$, given a
dataset $\mathcal{D} := \{\mathbf{x}_i\}_{i \in [n]} \subset \mathbb{R}^d$.
Their algorithm requires $n \gtrsim \sqrt d \cdot \frac 1 {\alpha\varepsilon}$
samples, which they prove is information-theoretically optimal. This result is
surprising because its error scales with the \emph{effective radius} of
$\mathcal{D}$ (i.e., of a ball capturing most points), rather than the
worst-case radius. We give an improved algorithm that obtains the same
approximation quality, also using $n \gtrsim \sqrt d \cdot \frac 1
{\alpha\epsilon}$ samples, but in time $\widetilde{O}(nd + \frac d
{\alpha^2})$. Our runtime is nearly-linear, plus the cost of the cheapest
non-private first-order method due to [CLM+16]. To achieve our results, we use
subsampling and geometric aggregation tools inspired by FriendlyCore [TCK+22]
to speed up the "warm start" component of the [HSU24] algorithm, combined with
a careful custom analysis of DP-SGD's sensitivity for the geometric median
objective.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [939] [High-order Equivariant Flow Matching for Density Functional Theory Hamiltonian Prediction](https://arxiv.org/abs/2505.18817)
*Seongsu Kim,Nayoung Kim,Dongwoo Kim,Sungsoo Ahn*

Main category: physics.comp-ph

TL;DR: QHFlow是一种高阶等变流匹配框架，通过生成哈密顿矩阵来加速DFT计算，减少误差并保持解的质量。


<details>
  <summary>Details</summary>
Motivation: 密度泛函理论(DFT)计算成本高，传统深度学习方法忽略哈密顿矩阵的结构特性，需要更高效且准确的预测方法。

Method: 提出QHFlow框架，利用等变流匹配生成哈密顿矩阵，结合SE(3)-等变向量场和微调方案提升物理保真度。

Result: 在MD17和QH9数据集上分别减少71%和53%的哈密顿误差，显著加速DFT计算过程。

Conclusion: QHFlow在保持解质量的同时，显著提升了DFT计算的效率和准确性，展示了深度学习方法在量子化学模拟中的潜力。

Abstract: Density functional theory (DFT) is a fundamental method for simulating
quantum chemical properties, but it remains expensive due to the iterative
self-consistent field (SCF) process required to solve the Kohn-Sham equations.
Recently, deep learning methods are gaining attention as a way to bypass this
step by directly predicting the Hamiltonian. However, they rely on
deterministic regression and do not consider the highly structured nature of
Hamiltonians. In this work, we propose QHFlow, a high-order equivariant flow
matching framework that generates Hamiltonian matrices conditioned on molecular
geometry. Flow matching models continuous-time trajectories between simple
priors and complex targets, learning the structured distributions over
Hamiltonians instead of direct regression. To further incorporate symmetry, we
use a neural architecture that predicts SE(3)-equivariant vector fields,
improving accuracy and generalization across diverse geometries. To further
enhance physical fidelity, we additionally introduce a fine-tuning scheme to
align predicted orbital energies with the target. QHFlow achieves
state-of-the-art performance, reducing Hamiltonian error by 71% on MD17 and 53%
on QH9. Moreover, we further show that QHFlow accelerates the DFT process
without trading off the solution quality when initializing SCF iterations with
the predicted Hamiltonian, significantly reducing the number of iterations and
runtime.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [940] [SP2RINT: Spatially-Decoupled Physics-Inspired Progressive Inverse Optimization for Scalable, PDE-Constrained Meta-Optical Neural Network Training](https://arxiv.org/abs/2505.18377)
*Pingchuan Ma,Ziang Yin,Qi Jing,Zhengqi Gao,Nicholas Gangi,Boyang Zhang,Tsung-Wei Huang,Zhaoran Huang,Duane S. Boning,Yu Yao,Jiaqi Gu*

Main category: physics.optics

TL;DR: 论文提出SP2RINT框架，通过空间解耦和渐进式训练方法，高效训练衍射光学神经网络（DONN），解决了现有方法在物理可实现性和计算效率上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前衍射光学神经网络（DONN）的训练方法存在两大挑战：启发式方法过于简化导致设计不可实现，而基于仿真的方法计算成本过高且难以扩展。

Method: SP2RINT框架将DONN训练建模为PDE约束学习问题，分阶段放松物理约束，并结合空间解耦的逆向设计策略，实现高效且物理可实现的训练。

Result: SP2RINT在多种DONN训练任务中达到与数字方法相当的精度，同时比基于仿真的方法快1825倍。

Conclusion: SP2RINT弥合了抽象DONN模型与可实现的硬件之间的鸿沟，为可扩展、高性能的物理可实现的超光学神经网络系统提供了训练方案。

Abstract: DONNs harness the physics of light propagation for efficient analog
computation, with applications in AI and signal processing. Advances in
nanophotonic fabrication and metasurface-based wavefront engineering have
opened new pathways to realize high-capacity DONNs across various spectral
regimes. Training such DONN systems to determine the metasurface structures
remains challenging. Heuristic methods are fast but oversimplify metasurfaces
modulation, often resulting in physically unrealizable designs and significant
performance degradation. Simulation-in-the-loop training methods directly
optimize a physically implementable metasurface using adjoint methods during
end-to-end DONN training, but are inherently computationally prohibitive and
unscalable.To address these limitations, we propose SP2RINT, a spatially
decoupled, progressive training framework that formulates DONN training as a
PDE-constrained learning problem. Metasurface responses are first relaxed into
freely trainable transfer matrices with a banded structure. We then
progressively enforce physical constraints by alternating between transfer
matrix training and adjoint-based inverse design, avoiding per-iteration PDE
solves while ensuring final physical realizability. To further reduce runtime,
we introduce a physics-inspired, spatially decoupled inverse design strategy
based on the natural locality of field interactions. This approach partitions
the metasurface into independently solvable patches, enabling scalable and
parallel inverse design with system-level calibration. Evaluated across diverse
DONN training tasks, SP2RINT achieves digital-comparable accuracy while being
1825 times faster than simulation-in-the-loop approaches. By bridging the gap
between abstract DONN models and implementable photonic hardware, SP2RINT
enables scalable, high-performance training of physically realizable
meta-optical neural systems.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [941] [A fast sound power prediction tool for genset noise using machine learning](https://arxiv.org/abs/2505.20079)
*Saurabh Pargal,Abhijit A. Sane*

Main category: physics.app-ph

TL;DR: 该研究探讨了使用KRR、HR和GPR等机器学习回归算法预测发电机组声功率水平的应用，为早期投标阶段提供噪声估算。


<details>
  <summary>Details</summary>
Motivation: 在发电机组设计和投标初期，发动机尺寸和外壳尺寸尚未确定且缺乏实测噪声数据时，需要一种可靠的方法来预测未建成发电机组的噪声水平。

Method: 研究利用来自Cummins Acoustics Technology Center的100多个实验数据集，采用KRR、HR和GPR算法进行噪声预测，并遵循ISO 3744标准。

Result: KRR预测声功率的平均准确度在5 dBA以内，HR和GPR误差略高，但所有模型均能有效捕捉不同发电机组配置的噪声趋势。

Conclusion: 这些算法为发电机组设计初期的噪声估算提供了一种有前景的方法。

Abstract: This paper investigates the application of machine learning regression
algorithms Kernel Ridge Regression (KRR), Huber Regressor (HR), and Gaussian
Process Regression (GPR) for predicting sound power levels of gensets, offering
significant value for marketing and sales teams during the early bidding
process. When engine sizes and genset enclosure dimensions are tentative, and
measured noise data is unavailable, these algorithms enable reliable noise
level estimation for unbuilt gensets. The study utilizes high fidelity datasets
from over 100 experiments conducted at Cummins Acoustics Technology Center
(ATC) in a hemi-anechoic chamber, adhering to ISO 3744 standards. By using
readily available information from the bidding and initial design stages, KRR
predicts sound power with an average accuracy of within 5 dBA. While HR and GPR
show slightly higher prediction errors, all models effectively capture the
overall noise trends across various genset configurations. These findings
present a promising method for early-stage noise estimation in genset design.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [942] [FlashMD: long-stride, universal prediction of molecular dynamics](https://arxiv.org/abs/2505.19350)
*Filippo Bigi,Sanggyu Chong,Agustinus Kristiadi,Michele Ceriotti*

Main category: physics.chem-ph

TL;DR: FlashMD是一种通过机器学习预测原子位置和动量演化的方法，显著延长了分子动力学模拟的时间步长。


<details>
  <summary>Details</summary>
Motivation: 传统分子动力学（MD）受限于原子运动的快速时间尺度，需要极小的时间步长。机器学习虽能加速力场预测，但仍无法解决时间步长限制问题。

Method: 提出FlashMD方法，通过结合哈密顿动力学的数学和物理特性，设计新型架构，支持任意热力学系综的模拟，并评估长步长MD的潜在问题。

Result: 验证了FlashMD在平衡态和时变性质模拟中的准确性，使用特定系统和通用模型，显著扩展了MD模拟的时间尺度。

Conclusion: FlashMD成功突破了传统MD的时间步长限制，为研究微观过程提供了更高效的模拟工具。

Abstract: Molecular dynamics (MD) provides insights into atomic-scale processes by
integrating over time the equations that describe the motion of atoms under the
action of interatomic forces. Machine learning models have substantially
accelerated MD by providing inexpensive predictions of the forces, but they
remain constrained to minuscule time integration steps, which are required by
the fast time scale of atomic motion. In this work, we propose FlashMD, a
method to predict the evolution of positions and momenta over strides that are
between one and two orders of magnitude longer than typical MD time steps. We
incorporate considerations on the mathematical and physical properties of
Hamiltonian dynamics in the architecture, generalize the approach to allow the
simulation of any thermodynamic ensemble, and carefully assess the possible
failure modes of such a long-stride MD approach. We validate FlashMD's accuracy
in reproducing equilibrium and time-dependent properties, using both
system-specific and general-purpose models, extending the ability of MD
simulation to reach the long time scales needed to model microscopic processes
of high scientific and technological relevance.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [943] [ABHINAYA -- A System for Speech Emotion Recognition In Naturalistic Conditions Challenge](https://arxiv.org/abs/2505.18217)
*Soumya Dutta,Smruthi Balaji,Varada R,Viveka Salinamakki,Sriram Ganapathy*

Main category: cs.SD

TL;DR: 该论文提出了一种名为Abhinaya的系统，用于自然环境下语音情感识别，通过结合语音、文本及语音-文本模型，并采用定制损失函数和多数投票机制，取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 自然环境下语音情感识别（SER）面临固有变异性、多样化录音条件和类别不平衡等挑战，论文旨在解决这些问题。

Method: 系统整合了语音模型、文本模型和语音-文本模型，微调自监督和语音大语言模型（SLLM）以获取语音表征，利用大语言模型（LLM）提取文本上下文，并通过SLLM进行语音-文本建模以捕捉细微情感线索。针对类别不平衡问题，采用定制损失函数和多数投票机制生成分类决策。

Result: 尽管有一个模型未完全训练，Abhinaya系统在166个提交中排名第4。训练完成后，其在已发表结果中达到了最先进性能。

Conclusion: Abhinaya系统在真实环境下的语音情感识别中表现出色，证明了该方法的有效性。

Abstract: Speech emotion recognition (SER) in naturalistic settings remains a challenge
due to the intrinsic variability, diverse recording conditions, and class
imbalance. As participants in the Interspeech Naturalistic SER Challenge which
focused on these complexities, we present Abhinaya, a system integrating
speech-based, text-based, and speech-text models. Our approach fine-tunes
self-supervised and speech large language models (SLLM) for speech
representations, leverages large language models (LLM) for textual context, and
employs speech-text modeling with an SLLM to capture nuanced emotional cues. To
combat class imbalance, we apply tailored loss functions and generate
categorical decisions through majority voting. Despite one model not being
fully trained, the Abhinaya system ranked 4th among 166 submissions. Upon
completion of training, it achieved state-of-the-art performance among
published results, demonstrating the effectiveness of our approach for SER in
real-world conditions.

</details>


### [944] [MPE-TTS: Customized Emotion Zero-Shot Text-To-Speech Using Multi-Modal Prompt](https://arxiv.org/abs/2505.18453)
*Zhichao Wu,Yueteng Kang,Songjun Cao,Long Ma,Qiulin Li,Qun Yang*

Main category: cs.SD

TL;DR: 提出基于多模态提示的自定义情感零样本文本转语音系统，通过解耦语音内容、音色、情感和韵律，支持文本、图像或语音作为情感提示，表现优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 现有零样本文本转语音系统仅依赖单一提示（如参考语音或文本描述），灵活性不足。本文旨在通过多模态提示提升情感表达的灵活性。

Method: 1. 多模态提示情感编码器提取不同提示的情感信息；2. 韵律预测器拟合韵律分布；3. 情感一致性损失保留预测韵律中的情感信息；4. 基于扩散的声学模型生成目标梅尔谱。

Result: 主客观实验表明，系统在自然度和相似度上优于现有方法。演示样本见项目页面。

Conclusion: 多模态提示和情感解耦设计有效提升了零样本语音合成的表现，为情感定制提供了灵活解决方案。

Abstract: Most existing Zero-Shot Text-To-Speech(ZS-TTS) systems generate the unseen
speech based on single prompt, such as reference speech or text descriptions,
which limits their flexibility. We propose a customized emotion ZS-TTS system
based on multi-modal prompt. The system disentangles speech into the content,
timbre, emotion and prosody, allowing emotion prompts to be provided as text,
image or speech. To extract emotion information from different prompts, we
propose a multi-modal prompt emotion encoder. Additionally, we introduce an
prosody predictor to fit the distribution of prosody and propose an emotion
consistency loss to preserve emotion information in the predicted prosody. A
diffusion-based acoustic model is employed to generate the target
mel-spectrogram. Both objective and subjective experiments demonstrate that our
system outperforms existing systems in terms of naturalness and similarity. The
samples are available at https://mpetts-demo.github.io/mpetts_demo/.

</details>


### [945] [Towards Reliable Large Audio Language Model](https://arxiv.org/abs/2505.19294)
*Ziyang Ma,Xiquan Li,Yakun Song,Wenxi Chen,Chenpeng Du,Jian Wu,Yuanzhe Chen,Zhuo Chen,Yuping Wang,Yuxuan Wang,Xie Chen*

Main category: cs.SD

TL;DR: 本文探讨了提升大型音频语言模型（LALMs）可靠性的方法，包括无需训练的多模态思维链（MCoT）和基于训练的监督微调（SFT），并提出了新的评估指标RGI。


<details>
  <summary>Details</summary>
Motivation: 现有的大型音频语言模型在跨语音、音乐和通用声音的理解与推理方面表现出色，但缺乏识别知识边界的能力，无法主动拒绝回答未知问题。

Method: 系统研究了多种提升LALMs可靠性的方法，包括无需训练的多模态思维链（MCoT）和基于训练的监督微调（SFT），并提出了新的评估指标RGI。

Result: 研究发现，无论是不需训练还是基于训练的方法，均能在不同程度上提升LALMs的可靠性，且可靠性意识是一种可以跨音频模态迁移的“元能力”。

Conclusion: 通过多种方法可以显著提升LALMs的可靠性，且这种能力可以跨模态迁移，为未来研究提供了新的方向。

Abstract: Recent advancements in large audio language models (LALMs) have demonstrated
impressive results and promising prospects in universal understanding and
reasoning across speech, music, and general sound. However, these models still
lack the ability to recognize their knowledge boundaries and refuse to answer
questions they don't know proactively. While there have been successful
attempts to enhance the reliability of LLMs, reliable LALMs remain largely
unexplored. In this paper, we systematically investigate various approaches
towards reliable LALMs, including training-free methods such as multi-modal
chain-of-thought (MCoT), and training-based methods such as supervised
fine-tuning (SFT). Besides, we identify the limitations of previous evaluation
metrics and propose a new metric, the Reliability Gain Index (RGI), to assess
the effectiveness of different reliable methods. Our findings suggest that both
training-free and training-based methods enhance the reliability of LALMs to
different extents. Moreover, we find that awareness of reliability is a "meta
ability", which can be transferred across different audio modalities, although
significant structural and content differences exist among sound, music, and
speech.

</details>


### [946] [Discovering Interpretable Concepts in Large Generative Music Models](https://arxiv.org/abs/2505.18186)
*Nikhil Singh,Manuel Cherep,Pattie Maes*

Main category: cs.SD

TL;DR: 该论文提出了一种使用稀疏自编码器（SAEs）从Transformer模型的残差流激活中提取可解释音乐特征的方法，旨在揭示神经网络学习到的音乐结构理论，并评估这些特征与传统音乐理论的一致性和差异性。


<details>
  <summary>Details</summary>
Motivation: 神经网络通过统计学习能够生成高质量的音乐内容，这表明它们可能已经学习到了音乐结构的隐式理论。这为研究人类生成媒体的理论提供了新的视角，既可以验证传统理论，也可以发现被忽视但有解释力的新模式。

Method: 论文采用稀疏自编码器（SAEs）从Transformer模型的残差流激活中提取可解释的音乐特征，并建立了一个自动标注和评估这些特征的流程。

Result: 研究结果揭示了一些熟悉的音乐概念，同时也发现了一些反直觉的模式，这些模式在现有理论或自然语言中没有明确的对应物。

Conclusion: 这项工作不仅提高了模型的透明度，还提供了一种新的实证工具，有助于发现传统分析和合成方法难以捕捉的组织原则。

Abstract: The fidelity with which neural networks can now generate content such as
music presents a scientific opportunity: these systems appear to have learned
implicit theories of the structure of such content through statistical learning
alone. This could offer a novel lens on theories of human-generated media.
Where these representations align with traditional constructs (e.g. chord
progressions in music), they demonstrate how these can be inferred from
statistical regularities. Where they diverge, they highlight potential limits
in our theoretical frameworks -- patterns that we may have overlooked but that
nonetheless hold significant explanatory power. In this paper, we focus on the
specific case of music generators. We introduce a method to discover musical
concepts using sparse autoencoders (SAEs), extracting interpretable features
from the residual stream activations of a transformer model. We evaluate this
approach by extracting a large set of features and producing an automatic
labeling and evaluation pipeline for them. Our results reveal both familiar
musical concepts and counterintuitive patterns that lack clear counterparts in
existing theories or natural language altogether. Beyond improving model
transparency, our work provides a new empirical tool that might help discover
organizing principles in ways that have eluded traditional methods of analysis
and synthesis.

</details>


### [947] [CloneShield: A Framework for Universal Perturbation Against Zero-Shot Voice Cloning](https://arxiv.org/abs/2505.19119)
*Renyuan Li,Zhibo Liang,Haichuan Zhang,Tianyu Shi,Zhiyuan Cheng,Jia Shi,Carl Yang,Mingjie Tang*

Main category: cs.SD

TL;DR: CloneShield是一种保护语音隐私的对抗扰动框架，有效防止零样本语音克隆，同时保持原始音频质量。


<details>
  <summary>Details</summary>
Motivation: 近年来，文本到语音（TTS）语音克隆技术的突破引发了严重的隐私问题，能够通过几秒钟的参考音频高度准确地复制说话者的声音。

Method: 提出CloneShield，一种通用的时域对抗扰动框架，通过多目标优化和Mel频谱分解生成扰动，确保跨说话者和话语的鲁棒保护。

Result: 实验表明，CloneShield在保护输入音频质量（PESQ=3.90，SRS=0.93）的同时，显著降低克隆样本的说话者相似性和语音质量（PESQ=1.07，SRS=0.08）。

Conclusion: CloneShield是一种有效的语音隐私保护方法，能够在保持原始音频质量的同时，抵御零样本语音克隆攻击。

Abstract: Recent breakthroughs in text-to-speech (TTS) voice cloning have raised
serious privacy concerns, allowing highly accurate vocal identity replication
from just a few seconds of reference audio, while retaining the speaker's vocal
authenticity. In this paper, we introduce CloneShield, a universal time-domain
adversarial perturbation framework specifically designed to defend against
zero-shot voice cloning. Our method provides protection that is robust across
speakers and utterances, without requiring any prior knowledge of the
synthesized text. We formulate perturbation generation as a multi-objective
optimization problem, and propose Multi-Gradient Descent Algorithm (MGDA) to
ensure the robust protection across diverse utterances. To preserve natural
auditory perception for users, we decompose the adversarial perturbation via
Mel-spectrogram representations and fine-tune it for each sample. This design
ensures imperceptibility while maintaining strong degradation effects on
zero-shot cloned outputs. Experiments on three state-of-the-art zero-shot TTS
systems, five benchmark datasets and evaluations from 60 human listeners
demonstrate that our method preserves near-original audio quality in protected
inputs (PESQ = 3.90, SRS = 0.93) while substantially degrading both speaker
similarity and speech quality in cloned samples (PESQ = 1.07, SRS = 0.08).

</details>


### [948] [EnvSDD: Benchmarking Environmental Sound Deepfake Detection](https://arxiv.org/abs/2505.19203)
*Han Yin,Yang Xiao,Rohan Kumar Das,Jisheng Bai,Haohe Liu,Wenwu Wang,Mark D Plumbley*

Main category: cs.SD

TL;DR: 论文介绍了首个大规模环境声音深度伪造检测数据集EnvSDD，并提出基于预训练音频基础模型的检测系统，性能优于现有语音和歌唱领域的最先进系统。


<details>
  <summary>Details</summary>
Motivation: 当前音频生成系统能生成逼真的环境音效，但也带来潜在风险。现有研究多集中于语音或歌唱的深度伪造检测，而环境声音特性不同，且缺乏大规模数据集。

Method: 构建EnvSDD数据集（含45.25小时真实音频和316.74小时伪造音频），并提出基于预训练音频基础模型的检测系统。

Result: 在EnvSDD测试集上，所提系统在未见过生成模型和数据集等多样条件下，性能超越语音和歌唱领域的现有最优系统。

Conclusion: EnvSDD填补了环境声音深度伪造检测的数据空白，所提检测系统具有优越的泛化能力。

Abstract: Audio generation systems now create very realistic soundscapes that can
enhance media production, but also pose potential risks. Several studies have
examined deepfakes in speech or singing voice. However, environmental sounds
have different characteristics, which may make methods for detecting speech and
singing deepfakes less effective for real-world sounds. In addition, existing
datasets for environmental sound deepfake detection are limited in scale and
audio types. To address this gap, we introduce EnvSDD, the first large-scale
curated dataset designed for this task, consisting of 45.25 hours of real and
316.74 hours of fake audio. The test set includes diverse conditions to
evaluate the generalizability, such as unseen generation models and unseen
datasets. We also propose an audio deepfake detection system, based on a
pre-trained audio foundation model. Results on EnvSDD show that our proposed
system outperforms the state-of-the-art systems from speech and singing
domains.

</details>


### [949] [Eta-WavLM: Efficient Speaker Identity Removal in Self-Supervised Speech Representations Using a Simple Linear Equation](https://arxiv.org/abs/2505.19273)
*Giuseppe Ruggiero,Matteo Testa,Jurgen Van de Walle,Luigi Di Caro*

Main category: cs.SD

TL;DR: 提出了一种新的自监督学习表示解耦方法，有效分离语音中的说话人特征与内容信息，在语音转换等任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法难以完全解耦语音中的说话人特征，且常导致其他语音成分受损，亟需高效轻量的解耦方案。

Method: 通过线性分解将自监督学习表示拆分为说话人相关和无关分量，生成解耦的语音表征。

Result: 该方法实现了说话人无关性，在语音转换等任务中显著优于现有技术。

Conclusion: 线性解耦方法能有效分离语音表征中的说话人信息，为内容驱动任务提供优质输入。

Abstract: Self-supervised learning (SSL) has reduced the reliance on expensive labeling
in speech technologies by learning meaningful representations from unannotated
data. Since most SSL-based downstream tasks prioritize content information in
speech, ideal representations should disentangle content from unwanted
variations like speaker characteristics in the SSL representations. However,
removing speaker information often degrades other speech components, and
existing methods either fail to fully disentangle speaker identity or require
resource-intensive models. In this paper, we propose a novel disentanglement
method that linearly decomposes SSL representations into speaker-specific and
speaker-independent components, effectively generating speaker disentangled
representations. Comprehensive experiments show that our approach achieves
speaker independence and as such, when applied to content-driven tasks such as
voice conversion, our representations yield significant improvements over
state-of-the-art methods.

</details>


### [950] [Audio Geolocation: A Natural Sounds Benchmark](https://arxiv.org/abs/2505.18726)
*Mustafa Chasmai,Wuao Liu,Subhransu Maji,Grant Van Horn*

Main category: cs.SD

TL;DR: 该研究探讨了通过声音进行地理定位的可行性，利用野生动物音频数据，结合视觉启发方法，将音频转换为频谱图，并整合物种分布预测与检索式定位技术。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索仅凭声音是否能确定地理位置，尤其是利用野生物种的叫声作为地理标志，因为它们的分布具有地域性。

Method: 采用视觉启发方法，将音频转换为频谱图，并整合物种分布预测与检索式定位技术，同时评估物种丰富度及时空邻域聚合对定位的影响。

Result: 研究发现，野生物种的叫声确实能提供有效的地理定位线索，且多模态（音频与视觉）结合能进一步提升定位效果。

Conclusion: 该研究为音频地理定位的未来研究奠定了基础，并展示了多模态整合在定位任务中的优势。

Abstract: Can we determine someone's geographic location purely from the sounds they
hear? Are acoustic signals enough to localize within a country, state, or even
city? We tackle the challenge of global-scale audio geolocation, formalize the
problem, and conduct an in-depth analysis with wildlife audio from the
iNatSounds dataset. Adopting a vision-inspired approach, we convert audio
recordings to spectrograms and benchmark existing image geolocation techniques.
We hypothesize that species vocalizations offer strong geolocation cues due to
their defined geographic ranges and propose an approach that integrates species
range prediction with retrieval-based geolocation. We further evaluate whether
geolocation improves when analyzing species-rich recordings or when aggregating
across spatiotemporal neighborhoods. Finally, we introduce case studies from
movies to explore multimodal geolocation using both audio and visual content.
Our work highlights the advantages of integrating audio and visual cues, and
sets the stage for future research in audio geolocation.

</details>


### [951] [Training-Free Multi-Step Audio Source Separation](https://arxiv.org/abs/2505.19534)
*Yongyi Zang,Jingyi Li,Qiuqiang Kong*

Main category: cs.SD

TL;DR: 本文提出了一种无需额外训练的多步音频源分离方法，通过迭代优化混合比例提升现有模型的分离性能。


<details>
  <summary>Details</summary>
Motivation: 现有的音频源分离系统通常采用一步推理，未能充分利用模型的分离潜力。本文旨在探索如何通过多步推理提升预训练模型的性能。

Method: 提出了一种迭代分离方法，通过优化混合比例逐步改进分离结果，每一步根据指标最大化确定最佳混合比例。

Result: 多步分离方法在语音增强和音乐源分离任务中均优于一步推理，性能提升接近训练更大模型或使用更多数据的效果。

Conclusion: 该方法为现有模型提供了免费的性能提升，但仍存在局限性，未来研究可进一步优化。

Abstract: Audio source separation aims to separate a mixture into target sources.
Previous audio source separation systems usually conduct one-step inference,
which does not fully explore the separation ability of models. In this work, we
reveal that pretrained one-step audio source separation models can be leveraged
for multi-step separation without additional training. We propose a simple yet
effective inference method that iteratively applies separation by optimally
blending the input mixture with the previous step's separation result. At each
step, we determine the optimal blending ratio by maximizing a metric. We prove
that our method always yield improvement over one-step inference, provide error
bounds based on model smoothness and metric robustness, and provide theoretical
analysis connecting our method to denoising along linear interpolation paths
between noise and clean distributions, a property we link to denoising
diffusion bridge models. Our approach effectively delivers improved separation
performance as a "free lunch" from existing models. Our empirical results
demonstrate that our multi-step separation approach consistently outperforms
one-step inference across both speech enhancement and music source separation
tasks, and can achieve scaling performance similar to training a larger model,
using more data, or in some cases employing a multi-step training objective.
These improvements appear not only on the optimization metric during multi-step
inference, but also extend to nearly all non-optimized metrics (with one
exception). We also discuss limitations of our approach and directions for
future research.

</details>


### [952] [STOPA: A Database of Systematic VariaTion Of DeePfake Audio for Open-Set Source Tracing and Attribution](https://arxiv.org/abs/2505.19644)
*Anton Firc,Manasi Chibber,Jagabandhu Mishra,Vishwanath Pratap Singh,Tomi Kinnunen,Kamil Malinka*

Main category: cs.SD

TL;DR: 论文介绍了STOPA数据集，用于深度伪造语音源追踪，包含多样化的合成参数和丰富元数据，以提高溯源准确性。


<details>
  <summary>Details</summary>
Motivation: 当前深度伪造语音检测领域缺乏系统化、多样化的数据集，限制了源追踪研究的进展。

Method: 构建STOPA数据集，涵盖8种声学模型、6种声码器模型及多种参数设置，共70万样本。

Result: STOPA提供了更广泛的生成因素控制和更可靠的溯源能力，优于现有数据集。

Conclusion: STOPA数据集通过系统化设计提升了深度伪造语音的溯源准确性，助力 forensic 分析和生成模型透明度。

Abstract: A key research area in deepfake speech detection is source tracing -
determining the origin of synthesised utterances. The approaches may involve
identifying the acoustic model (AM), vocoder model (VM), or other
generation-specific parameters. However, progress is limited by the lack of a
dedicated, systematically curated dataset. To address this, we introduce STOPA,
a systematically varied and metadata-rich dataset for deepfake speech source
tracing, covering 8 AMs, 6 VMs, and diverse parameter settings across 700k
samples from 13 distinct synthesisers. Unlike existing datasets, which often
feature limited variation or sparse metadata, STOPA provides a systematically
controlled framework covering a broader range of generative factors, such as
the choice of the vocoder model, acoustic model, or pretrained weights,
ensuring higher attribution reliability. This control improves attribution
accuracy, aiding forensic analysis, deepfake detection, and generative model
transparency.

</details>


### [953] [A Comprehensive Real-World Assessment of Audio Watermarking Algorithms: Will They Survive Neural Codecs?](https://arxiv.org/abs/2505.19663)
*Yigitcan Özer,Woosung Choi,Joan Serrà,Mayank Kumar Singh,Wei-Hsiang Liao,Yuki Mitsufuji*

Main category: cs.SD

TL;DR: 该论文提出了一个评估深度学习音频水印算法的框架，包括标准化基准和系统性比较方法，通过模拟真实环境下的音频攻击来测试算法鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对深度学习音频水印算法的标准化评估方法，难以公平比较不同算法的性能。论文旨在填补这一空白，提供统一的评估框架。

Method: 论文引入了一个全面的音频攻击流程，包含压缩、背景噪声、混响等多种失真类型，并构建了多样化的测试数据集（语音、环境声音、音乐）。评估了四种现有水印算法。

Result: 研究发现：(1) 神经压缩技术对水印算法构成最大挑战；(2) 音频攻击训练通常能提升鲁棒性，但某些情况下仍不足；(3) 极性反转、时间拉伸等特定失真会严重影响部分算法。

Conclusion: 该框架增强了音频水印算法在广泛应用中的鲁棒性和感知评估能力，提供了公平一致的评估方法。相关代码已在GitHub开源。

Abstract: We present a framework to foster the evaluation of deep learning-based audio
watermarking algorithms, establishing a standardized benchmark and allowing
systematic comparisons. To simulate real-world usage, we introduce a
comprehensive audio attack pipeline, featuring various distortions such as
compression, background noise, and reverberation, and propose a diverse test
dataset, including speech, environmental sounds, and music recordings. By
assessing the performance of four existing watermarking algorithms on our
framework, two main insights stand out: (i) neural compression techniques pose
the most significant challenge, even when algorithms are trained with such
compressions; and (ii) training with audio attacks generally improves
robustness, although it is insufficient in some cases. Furthermore, we find
that specific distortions, such as polarity inversion, time stretching, or
reverb, seriously affect certain algorithms. Our contributions strengthen the
robustness and perceptual assessment of audio watermarking algorithms across a
wide range of applications, while ensuring a fair and consistent evaluation
approach. The evaluation framework, including the attack pipeline, is
accessible at github.com/SonyResearch/wm_robustness_eval.

</details>


### [954] [Automated evaluation of children's speech fluency for low-resource languages](https://arxiv.org/abs/2505.19671)
*Bowen Zhang,Nur Afiqah Abdul Latiff,Justin Kan,Rong Tong,Donny Soh,Xiaoxiao Miao,Ian McLoughlin*

Main category: cs.SD

TL;DR: 该论文提出了一种结合多语言ASR模型、客观指标提取和GPT网络的系统，用于自动评估低资源语言儿童口语流利度，并在泰米尔语和马来语数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 针对低资源语言儿童口语流利度评估的挑战，现有研究主要集中在主流语言，因此需要开发高效自动评估方法。

Method: 系统结合微调多语言ASR模型、提取语音错误率/语速等客观指标，并通过GPT分类器结合少量人工标注数据评分。

Result: 在泰米尔语和马来语测试中，该方法准确率显著高于随机森林、XGBoost及直接使用ChatGPT-4o的方案。

Conclusion: 融合ASR客观指标与GPT解释的混合方法，能有效提升低资源语言流利度评估性能。

Abstract: Assessment of children's speaking fluency in education is well researched for
majority languages, but remains highly challenging for low resource languages.
This paper proposes a system to automatically assess fluency by combining a
fine-tuned multilingual ASR model, an objective metrics extraction stage, and a
generative pre-trained transformer (GPT) network. The objective metrics include
phonetic and word error rates, speech rate, and speech-pause duration ratio.
These are interpreted by a GPT-based classifier guided by a small set of
human-evaluated ground truth examples, to score fluency. We evaluate the
proposed system on a dataset of children's speech in two low-resource
languages, Tamil and Malay and compare the classification performance against
Random Forest and XGBoost, as well as using ChatGPT-4o to predict fluency
directly from speech input. Results demonstrate that the proposed approach
achieves significantly higher accuracy than multimodal GPT or other methods.

</details>


### [955] [DiEmo-TTS: Disentangled Emotion Representations via Self-Supervised Distillation for Cross-Speaker Emotion Transfer in Text-to-Speech](https://arxiv.org/abs/2505.19687)
*Deok-Hyeon Cho,Hyung-Seok Oh,Seung-Bin Kim,Seong-Whan Lee*

Main category: cs.SD

TL;DR: 提出DiEmo-TTS方法，通过自监督蒸馏解决语音合成中跨说话人情感迁移的说话人泄漏问题，提升合成质量。


<details>
  <summary>Details</summary>
Motivation: 现有音色压缩方法无法完全分离说话人和情感特征，导致说话人泄漏和合成质量下降，需改进情感嵌入的提取方式。

Method: 采用自监督蒸馏、聚类驱动采样和信息扰动技术，结合情感聚类匹配和双重条件Transformer，以保留情感并去除无关因素。

Result: 实验证明该方法能有效学习与说话人无关的情感嵌入，提升跨说话人情感迁移的准确性。

Conclusion: DiEmo-TTS成功解决了情感迁移中的说话人泄漏问题，为语音合成提供了更高质量的情感建模方案。

Abstract: Cross-speaker emotion transfer in speech synthesis relies on extracting
speaker-independent emotion embeddings for accurate emotion modeling without
retaining speaker traits. However, existing timbre compression methods fail to
fully separate speaker and emotion characteristics, causing speaker leakage and
degraded synthesis quality. To address this, we propose DiEmo-TTS, a
self-supervised distillation method to minimize emotional information loss and
preserve speaker identity. We introduce cluster-driven sampling and information
perturbation to preserve emotion while removing irrelevant factors. To
facilitate this process, we propose an emotion clustering and matching approach
using emotional attribute prediction and speaker embeddings, enabling
generalization to unlabeled data. Additionally, we designed a dual conditioning
transformer to integrate style features better. Experimental results confirm
the effectiveness of our method in learning speaker-irrelevant emotion
embeddings.

</details>


### [956] [EmoSphere-SER: Enhancing Speech Emotion Recognition Through Spherical Representation with Auxiliary Classification](https://arxiv.org/abs/2505.19693)
*Deok-Hyeon Cho,Hyung-Seok Oh,Seung-Bin Kim,Seong-Whan Lee*

Main category: cs.SD

TL;DR: EmoSphere-SER提出了一种结合球形VAD区域分类与回归的语音情感识别方法，通过动态加权和多头自注意力提升性能。


<details>
  <summary>Details</summary>
Motivation: 为了提高语音情感识别的准确性，作者提出了一种结合离散标签和连续维度（如VAD）的联合模型，旨在通过结构化学习提升预测一致性。

Method: 将VAD值转换为球形坐标并划分为多个区域，通过辅助分类任务指导回归过程，结合动态加权方案和多头自注意力的风格池化层捕捉频谱和时序动态。

Result: 实验结果表明，该方法优于基线方法，验证了框架的有效性。

Conclusion: EmoSphere-SER通过联合训练策略和结构化学习，显著提升了语音情感识别的性能。

Abstract: Speech emotion recognition predicts a speaker's emotional state from speech
signals using discrete labels or continuous dimensions such as arousal,
valence, and dominance (VAD). We propose EmoSphere-SER, a joint model that
integrates spherical VAD region classification to guide VAD regression for
improved emotion prediction. In our framework, VAD values are transformed into
spherical coordinates that are divided into multiple spherical regions, and an
auxiliary classification task predicts which spherical region each point
belongs to, guiding the regression process. Additionally, we incorporate a
dynamic weighting scheme and a style pooling layer with multi-head
self-attention to capture spectral and temporal dynamics, further boosting
performance. This combined training strategy reinforces structured learning and
improves prediction consistency. Experimental results show that our approach
exceeds baseline methods, confirming the validity of the proposed framework.

</details>


### [957] [Novel Loss-Enhanced Universal Adversarial Patches for Sustainable Speaker Privacy](https://arxiv.org/abs/2505.19951)
*Elvir Karimov,Alexander Varlamov,Danil Ivanov,Dmitrii Korzh,Oleg Y. Rogov*

Main category: cs.SD

TL;DR: 本文提出了一种改进的说话人匿名化方法，通过引入指数总方差（TV）损失函数和可扩展的通用对抗补丁（UAP）插入流程，解决了现有方法在音频质量、语音识别质量和跨模型迁移性上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前基于通用对抗补丁（UAP）的说话人匿名化方法存在音频质量下降、语音识别质量降低、跨模型迁移性差以及对输入音频长度依赖性强等问题，亟需改进。

Method: 引入指数总方差（TV）损失函数以增强UAP的强度和不可感知性，并提出一种新型可扩展的UAP插入流程。

Result: 实验证明，该方法显著提升了UAP的性能，并在不同长度的音频上表现出稳定的高效性。

Conclusion: 本文提出的方法有效改进了说话人匿名化技术，为保护个人语音数据隐私提供了更优解决方案。

Abstract: Deep learning voice models are commonly used nowadays, but the safety
processing of personal data, such as human identity and speech content, remains
suspicious. To prevent malicious user identification, speaker anonymization
methods were proposed. Current methods, particularly based on universal
adversarial patch (UAP) applications, have drawbacks such as significant
degradation of audio quality, decreased speech recognition quality, low
transferability across different voice biometrics models, and performance
dependence on the input audio length. To mitigate these drawbacks, in this
work, we introduce and leverage the novel Exponential Total Variance (TV) loss
function and provide experimental evidence that it positively affects UAP
strength and imperceptibility. Moreover, we present a novel scalable UAP
insertion procedure and demonstrate its uniformly high performance for various
audio lengths.

</details>


### [958] [Automated data curation for self-supervised learning in underwater acoustic analysis](https://arxiv.org/abs/2505.20066)
*Hilde I Hummel,Sandjai Bhulai,Burooj Ghani,Rob van der Mei*

Main category: cs.SD

TL;DR: 论文提出了一种自动化自监督数据整理流程，用于从原始海洋声学监测数据中创建多样化和平衡的数据集，以支持自监督学习模型开发，用于监测海洋哺乳动物和评估声音污染。


<details>
  <summary>Details</summary>
Motivation: 海洋生态系统的可持续性受到声音污染增加的威胁，被动声学监测系统收集了大量水下录音，但数据量大且多为未标记，需要自动化分析。

Method: 提出了一种全自动自监督数据整理流程，结合自动识别系统数据和多个水听器的录音，使用分层k均值聚类对原始音频数据进行采样，创建平衡且多样化的数据集。

Result: 整理后的数据集支持自监督学习模型的开发，可用于监测海洋哺乳动物和评估声音污染等多种任务。

Conclusion: 通过自动化数据整理流程，解决了水下声学数据未标记和大规模处理的挑战，为海洋声音污染的监测和分析提供了有效工具。

Abstract: The sustainability of the ocean ecosystem is threatened by increased levels
of sound pollution, making monitoring crucial to understand its variability and
impact. Passive acoustic monitoring (PAM) systems collect a large amount of
underwater sound recordings, but the large volume of data makes manual analysis
impossible, creating the need for automation. Although machine learning offers
a potential solution, most underwater acoustic recordings are unlabeled.
Self-supervised learning models have demonstrated success in learning from
large-scale unlabeled data in various domains like computer vision, Natural
Language Processing, and audio. However, these models require large, diverse,
and balanced datasets for training in order to generalize well. To address
this, a fully automated self-supervised data curation pipeline is proposed to
create a diverse and balanced dataset from raw PAM data. It integrates
Automatic Identification System (AIS) data with recordings from various
hydrophones in the U.S. waters. Using hierarchical k-means clustering, the raw
audio data is sampled and then combined with AIS samples to create a balanced
and diverse dataset. The resulting curated dataset enables the development of
self-supervised learning models, facilitating various tasks such as monitoring
marine mammals and assessing sound pollution.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [959] [Agent-Based Decentralized Energy Management of EV Charging Station with Solar Photovoltaics via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.18750)
*Jiarong Fan,Chenghao Huang,Hao Wang*

Main category: eess.SY

TL;DR: 该论文提出了一种多智能体强化学习（MARL）方法，结合LSTM网络和密集奖励机制，以增强电动汽车充电站在不确定性和故障情况下的鲁棒性，同时优化充电成本和用户体验。


<details>
  <summary>Details</summary>
Motivation: 在智能城市实现能源净零的背景下，电动汽车充电站的能源管理至关重要。以往研究虽能降低充电成本并维持电网稳定，但常忽略充电行为多变和充电器故障等不确定性因素。

Method: 采用多智能体强化学习（MARL）框架，将每个充电器视为独立智能体，并结合LSTM网络提取时间序列特征，设计密集奖励机制以优化训练过程。

Result: 通过真实数据集验证，该方法在系统不确定性和故障情况下表现鲁棒，能有效降低充电成本并提升充电服务满意度。

Conclusion: 所提出的MARL方法为电动汽车充电站管理提供了更可靠的解决方案，兼顾经济性和用户体验，适用于实际复杂场景。

Abstract: In the pursuit of energy net zero within smart cities, transportation
electrification plays a pivotal role. The adoption of Electric Vehicles (EVs)
keeps increasing, making energy management of EV charging stations critically
important. While previous studies have managed to reduce energy cost of EV
charging while maintaining grid stability, they often overlook the robustness
of EV charging management against uncertainties of various forms, such as
varying charging behaviors and possible faults in faults in some chargers. To
address the gap, a novel Multi-Agent Reinforcement Learning (MARL) approach is
proposed treating each charger to be an agent and coordinate all the agents in
the EV charging station with solar photovoltaics in a more realistic scenario,
where system faults may occur. A Long Short-Term Memory (LSTM) network is
incorporated in the MARL algorithm to extract temporal features from
time-series. Additionally, a dense reward mechanism is designed for training
the agents in the MARL algorithm to improve EV charging experience. Through
validation on a real-world dataset, we show that our approach is robust against
system uncertainties and faults and also effective in minimizing EV charging
costs and maximizing charging service satisfaction.

</details>


### [960] [Robust Stability Analysis of Positive Lure System with Neural Network Feedback](https://arxiv.org/abs/2505.18912)
*Hamidreza Montazeri Hedesh,Moh. Kamalul Wafi,Bahram Shafai,Milad Siami*

Main category: eess.SY

TL;DR: 本文研究了在正性约束下Lur'e问题的鲁棒性，利用正Aizerman猜想和Metzler矩阵的鲁棒性特性，推导了Lur'e系统稳定性半径的显式公式，并扩展到神经网络反馈系统。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决复杂不确定非线性系统中的鲁棒性问题，特别是在线性部分包含参数不确定性和非线性扇区边界未知的情况下，如何利用正线性系统工具进行分析。

Method: 方法包括利用系统的正性特性，结合正Aizerman猜想和Metzler矩阵的鲁棒性，推导稳定性半径的显式公式，并扩展到神经网络反馈系统，提出前馈神经网络扇区边界的改进方法。

Result: 研究结果包括推导出Lur'e系统稳定性半径的显式公式，扩展到神经网络反馈系统，并提出了一种可扩展且高效的鲁棒性分析方法。

Conclusion: 结论表明，通过利用正性特性，可以有效地分析Lur'e和神经网络控制系统的鲁棒性，并通过示例验证了所提方法的有效性。

Abstract: This paper investigates the robustness of the Lur'e problem under positivity
constraints, drawing on results from the positive Aizerman conjecture and the
robustness properties of Metzler matrices. Specifically, we consider a control
system of Lur'e type in which not only the linear part includes parametric
uncertainty but also the nonlinear sector bound is unknown. We investigate
tools from positive linear systems to effectively solve the problems in
complicated and uncertain nonlinear systems. By leveraging the positivity
characteristic of the system, we derive an explicit formula for the stability
radius of Lur'e systems. Furthermore, we extend our analysis to systems with
neural network (NN) feedback loops. Building on this approach, we also propose
a refinement method for sector bounds of feedforward neural networks (FFNNs).
This study introduces a scalable and efficient approach for robustness analysis
of both Lur'e and NN-controlled systems. Finally, the proposed results are
supported by illustrative examples.

</details>


### [961] [VLMLight: Traffic Signal Control via Vision-Language Meta-Control and Dual-Branch Reasoning](https://arxiv.org/abs/2505.19486)
*Maonan Wang,Yirong Chen,Aoyu Pang,Yuxin Cai,Chung Shue Chen,Yuheng Kan,Man-On Pun*

Main category: eess.SY

TL;DR: VLMLight是一种新型交通信号控制框架，结合视觉语言元控制和双分支推理，显著提升紧急车辆通行效率并保持实时性能。


<details>
  <summary>Details</summary>
Motivation: 现有交通信号控制方法在复杂、动态和安全关键场景中泛化能力不足，亟需一种既能保证效率又兼顾安全的解决方案。

Method: 提出VLMLight框架，集成基于视觉的交通模拟器和大型语言模型（LLM），通过双分支策略（快速RL策略和结构化推理分支）实现动态决策。

Result: 实验表明，VLMLight将紧急车辆等待时间减少高达65%，同时在标准条件下保持实时性能，性能下降不到1%。

Conclusion: VLMLight为下一代交通信号控制提供了可扩展、可解释且安全感知的解决方案。

Abstract: Traffic signal control (TSC) is a core challenge in urban mobility, where
real-time decisions must balance efficiency and safety. Existing methods -
ranging from rule-based heuristics to reinforcement learning (RL) - often
struggle to generalize to complex, dynamic, and safety-critical scenarios. We
introduce VLMLight, a novel TSC framework that integrates vision-language
meta-control with dual-branch reasoning. At the core of VLMLight is the first
image-based traffic simulator that enables multi-view visual perception at
intersections, allowing policies to reason over rich cues such as vehicle type,
motion, and spatial density. A large language model (LLM) serves as a
safety-prioritized meta-controller, selecting between a fast RL policy for
routine traffic and a structured reasoning branch for critical cases. In the
latter, multiple LLM agents collaborate to assess traffic phases, prioritize
emergency vehicles, and verify rule compliance. Experiments show that VLMLight
reduces waiting times for emergency vehicles by up to 65% over RL-only systems,
while preserving real-time performance in standard conditions with less than 1%
degradation. VLMLight offers a scalable, interpretable, and safety-aware
solution for next-generation traffic signal control.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [962] [Collaborative Memory: Multi-User Memory Sharing in LLM Agents with Dynamic Access Control](https://arxiv.org/abs/2505.18279)
*Alireza Rezazadeh,Zichao Li,Ange Lou,Yuying Zhao,Wei Wei,Yujia Bao*

Main category: cs.MA

TL;DR: 论文提出了一种名为'协作记忆'的框架，用于多用户、多代理环境中实现动态、非对称权限下的知识共享。


<details>
  <summary>Details</summary>
Motivation: 当前大多数方法假设单一用户环境，忽视了在动态、非对称权限下跨用户知识共享的益处与挑战。

Method: 引入协作记忆框架，包含私有记忆和共享记忆两个层级，通过二分图编码用户、代理和资源的访问控制，并实施细粒度的读写策略。

Result: 该框架实现了安全、高效且可解释的跨用户知识共享，同时确保了对非对称、时变策略的可证明遵循和内存操作的完全可审计性。

Conclusion: 协作记忆框架为多用户、多代理环境中的知识共享提供了一种有效的解决方案，兼顾了安全性和灵活性。

Abstract: Complex tasks are increasingly delegated to ensembles of specialized
LLM-based agents that reason, communicate, and coordinate actions-both among
themselves and through interactions with external tools, APIs, and databases.
While persistent memory has been shown to enhance single-agent performance,
most approaches assume a monolithic, single-user context-overlooking the
benefits and challenges of knowledge transfer across users under dynamic,
asymmetric permissions. We introduce Collaborative Memory, a framework for
multi-user, multi-agent environments with asymmetric, time-evolving access
controls encoded as bipartite graphs linking users, agents, and resources. Our
system maintains two memory tiers: (1) private memory-private fragments visible
only to their originating user; and (2) shared memory-selectively shared
fragments. Each fragment carries immutable provenance attributes (contributing
agents, accessed resources, and timestamps) to support retrospective permission
checks. Granular read policies enforce current user-agent-resource constraints
and project existing memory fragments into filtered transformed views. Write
policies determine fragment retention and sharing, applying context-aware
transformations to update the memory. Both policies may be designed conditioned
on system, agent, and user-level information. Our framework enables safe,
efficient, and interpretable cross-user knowledge sharing, with provable
adherence to asymmetric, time-varying policies and full auditability of memory
operations.

</details>


### [963] [Single-agent or Multi-agent Systems? Why Not Both?](https://arxiv.org/abs/2505.18286)
*Mingyan Gao,Yanzi Li,Banruo Liu,Yifan Yu,Phillip Wang,Ching-Yu Lin,Fan Lai*

Main category: cs.MA

TL;DR: 随着大语言模型能力的提升，多智能体系统（MAS）相较于单智能体系统（SAS）的优势逐渐减弱。本文提出了一种混合智能体范式，通过请求级联机制在MAS和SAS之间切换，以提高效率和性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统（MAS）虽然在复杂任务中表现出色，但其设计和部署成本较高。随着前沿大语言模型（如OpenAI-o3和Gemini-2.5-Pro）在长上下文推理、记忆保留和工具使用方面的进步，MAS的优势逐渐减弱。本文旨在比较MAS和SAS的性能差异，并提出更高效的解决方案。

Method: 本文通过广泛的实证研究比较MAS和SAS在各种智能体应用中的表现，并提出了一种混合智能体范式，即在MAS和SAS之间进行请求级联，以优化效率和能力。

Result: 实验表明，随着大语言模型能力的提升，MAS相对于SAS的优势逐渐减弱。提出的混合智能体范式在准确率上提高了1.1-12%，同时部署成本降低了20%。

Conclusion: 本文研究表明，随着大语言模型能力的提升，MAS的优势逐渐减弱。通过提出的混合智能体范式，可以在保持性能的同时显著降低部署成本，为智能体系统的设计提供了新的方向。

Abstract: Multi-agent systems (MAS) decompose complex tasks and delegate subtasks to
different large language model (LLM) agents and tools. Prior studies have
reported the superior accuracy performance of MAS across diverse domains,
enabled by long-horizon context tracking and error correction through
role-specific agents. However, the design and deployment of MAS incur higher
complexity and runtime cost compared to single-agent systems (SAS). Meanwhile,
frontier LLMs, such as OpenAI-o3 and Gemini-2.5-Pro, have rapidly advanced in
long-context reasoning, memory retention, and tool usage, mitigating many
limitations that originally motivated MAS designs. In this paper, we conduct an
extensive empirical study comparing MAS and SAS across various popular agentic
applications. We find that the benefits of MAS over SAS diminish as LLM
capabilities improve, and we propose efficient mechanisms to pinpoint the
error-prone agent in MAS. Furthermore, the performance discrepancy between MAS
and SAS motivates our design of a hybrid agentic paradigm, request cascading
between MAS and SAS, to improve both efficiency and capability. Our design
improves accuracy by 1.1-12% while reducing deployment costs by up to 20%
across various agentic applications.

</details>


### [964] [An Outlook on the Opportunities and Challenges of Multi-Agent AI Systems](https://arxiv.org/abs/2505.18397)
*Fangqiao Tian,An Luo,Jin Du,Xun Xian,Robert Specht,Ganghua Wang,Xuan Bi,Jiawei Zhou,Jayanth Srinivasa,Ashish Kundu,Charles Fleming,Rui Zhang,Zirui Liu,Mingyi Hong,Jie Ding*

Main category: cs.MA

TL;DR: 本文系统探讨了多智能体AI系统（MAS）的机遇与挑战，结合大语言模型、联邦优化和人机交互的最新进展，提出了关键概念和潜在风险，并通过生物启发模拟和理论框架指出了未来发展路径。


<details>
  <summary>Details</summary>
Motivation: 多智能体AI系统（MAS）在分布式智能中展现出巨大潜力，能够实现协作推理、规划和决策。然而，其发展面临依赖、不对齐和训练数据重叠等风险，本文旨在系统分析这些机遇与挑战。

Method: 通过形式化关键概念（如智能体拓扑、协调协议和共享目标），并结合生物启发模拟与理论框架，分析了MAS的潜在风险和发展路径。

Result: 研究识别了MAS的主要风险（如依赖性和数据重叠漏洞），并提出了开发鲁棒、可扩展和安全MAS的关键路径。

Conclusion: 本文为MAS在实际应用中的发展提供了系统展望，强调了结合理论框架和生物启发方法的重要性，以应对未来挑战。

Abstract: Multi-agent AI systems (MAS) offer a promising framework for distributed
intelligence, enabling collaborative reasoning, planning, and decision-making
across autonomous agents. This paper provides a systematic outlook on the
current opportunities and challenges of MAS, drawing insights from recent
advances in large language models (LLMs), federated optimization, and human-AI
interaction. We formalize key concepts including agent topology, coordination
protocols, and shared objectives, and identify major risks such as dependency,
misalignment, and vulnerabilities arising from training data overlap. Through a
biologically inspired simulation and comprehensive theoretical framing, we
highlight critical pathways for developing robust, scalable, and secure MAS in
real-world settings.

</details>


### [965] [MRGAgents: A Multi-Agent Framework for Improved Medical Report Generation with Med-LVLMs](https://arxiv.org/abs/2505.18530)
*Pengyu Wang,Shuchang Ye,Usman Naseem,Jinman Kim*

Main category: cs.MA

TL;DR: 提出MRGAgents框架，通过多智能体分病种微调解决医学大视觉语言模型在报告生成中偏向正常结果和忽略关键异常的问题。


<details>
  <summary>Details</summary>
Motivation: 现有Med-LVLMs在生成医学报告时存在偏向预测正常结果的偏差，且常遗漏关键异常和放射学相关区域的全面描述，影响诊断准确性。

Method: 基于IU X-ray和MIMIC-CXR数据集，训练针对不同疾病类别的专用智能体，构建多智能体框架MRGAgents。

Result: 实验表明MRGAgents在报告全面性和诊断效用上优于现有技术，能更好平衡正常与异常结果。

Conclusion: MRGAgents通过分病种专业化训练有效提升了医学报告生成的质量和临床实用性。

Abstract: Medical Large Vision-Language Models (Med-LVLMs) have been widely adopted for
medical report generation. Despite Med-LVLMs producing state-of-the-art
performance, they exhibit a bias toward predicting all findings as normal,
leading to reports that overlook critical abnormalities. Furthermore, these
models often fail to provide comprehensive descriptions of radiologically
relevant regions necessary for accurate diagnosis. To address these challenges,
we proposeMedical Report Generation Agents (MRGAgents), a novel multi-agent
framework that fine-tunes specialized agents for different disease categories.
By curating subsets of the IU X-ray and MIMIC-CXR datasets to train
disease-specific agents, MRGAgents generates reports that more effectively
balance normal and abnormal findings while ensuring a comprehensive description
of clinically relevant regions. Our experiments demonstrate that MRGAgents
outperformed the state-of-the-art, improving both report comprehensiveness and
diagnostic utility.

</details>


### [966] [MASTER: Multi-Agent Security Through Exploration of Roles and Topological Structures -- A Comprehensive Framework](https://arxiv.org/abs/2505.18572)
*Yifan Zhu,Chao Zhang,Xin Shi,Xueqiao Zhang,Yi Yang,Yawei Luo*

Main category: cs.MA

TL;DR: 该论文提出了MASTER框架，用于研究基于大语言模型的多智能体系统（MAS）的安全问题，包括攻击和防御策略。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统（MAS）因其专业角色和协作交互在多个领域展现出卓越的问题解决能力，但也放大了安全风险。

Method: 提出MASTER框架，通过自动化构建不同MAS配置和信息流交互范式，设计基于角色和拓扑结构的自适应攻击策略。

Result: 实验表明，利用角色和拓扑信息的攻击对大多数模型具有显著破坏性，同时提出的防御策略显著提升了MAS的韧性。

Conclusion: MASTER框架为未来研究MAS安全挑战提供了有价值的见解。

Abstract: Large Language Models (LLMs)-based Multi-Agent Systems (MAS) exhibit
remarkable problem-solving and task planning capabilities across diverse
domains due to their specialized agentic roles and collaborative interactions.
However, this also amplifies the severity of security risks under MAS attacks.
To address this, we introduce MASTER, a novel security research framework for
MAS, focusing on diverse Role configurations and Topological structures across
various scenarios. MASTER offers an automated construction process for
different MAS setups and an information-flow-based interaction paradigm. To
tackle MAS security challenges in varied scenarios, we design a
scenario-adaptive, extensible attack strategy utilizing role and topological
information, which dynamically allocates targeted, domain-specific attack tasks
for collaborative agent execution. Our experiments demonstrate that such an
attack, leveraging role and topological information, exhibits significant
destructive potential across most models. Additionally, we propose
corresponding defense strategies, substantially enhancing MAS resilience across
diverse scenarios. We anticipate that our framework and findings will provide
valuable insights for future research into MAS security challenges.

</details>


### [967] [Multi-Agent Reinforcement Learning in Cybersecurity: From Fundamentals to Applications](https://arxiv.org/abs/2505.19837)
*Christoph R. Landolt,Christoph Würsch,Roland Meier,Alain Mermoud,Julian Jang-Jaccard*

Main category: cs.MA

TL;DR: 该论文综述了多智能体强化学习（MARL）在自动网络防御（ACD）中的应用现状，重点探讨了入侵检测和横向移动遏制，并讨论了自主智能网络防御代理（AICA）和网络训练场（Cyber Gyms）的作用。


<details>
  <summary>Details</summary>
Motivation: 现代网络安全面临动态、协调且复杂的威胁，需要去中心化、自适应和协作的防御策略。MARL作为一种自适应解决方案，具有应对这些挑战的潜力。

Method: 论文通过调查现有研究，分析了MARL在ACD中的应用，特别是入侵检测和横向移动遏制，并探讨了AICA和Cyber Gyms在训练和验证MARL智能体中的作用。

Result: 研究发现MARL在提供自适应、可扩展和动态的网络防御解决方案方面具有变革性潜力，但也存在可扩展性和对抗鲁棒性等挑战。

Conclusion: 论文总结了MARL在网络安全领域的应用前景，并提出了未来研究方向，强调了Cyber Gyms在训练和验证AICA中的价值。

Abstract: Multi-Agent Reinforcement Learning (MARL) has shown great potential as an
adaptive solution for addressing modern cybersecurity challenges. MARL enables
decentralized, adaptive, and collaborative defense strategies and provides an
automated mechanism to combat dynamic, coordinated, and sophisticated threats.
This survey investigates the current state of research in MARL applications for
automated cyber defense (ACD), focusing on intruder detection and lateral
movement containment. Additionally, it examines the role of Autonomous
Intelligent Cyber-defense Agents (AICA) and Cyber Gyms in training and
validating MARL agents. Finally, the paper outlines existing challenges, such
as scalability and adversarial robustness, and proposes future research
directions. This also discusses how MARL integrates in AICA to provide
adaptive, scalable, and dynamic solutions to counter the increasingly
sophisticated landscape of cyber threats. It highlights the transformative
potential of MARL in areas like intrusion detection and lateral movement
containment, and underscores the value of Cyber Gyms for training and
validation of AICA.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [968] [Efficient Algorithms for Electing Successive Committees](https://arxiv.org/abs/2505.18287)
*Pallavi Jain,Andrzej Kaczmarczyk*

Main category: cs.GT

TL;DR: 论文针对连续委员会选举模型中的NP难题，提出了参数化算法以解决候选人数适中或时间范围有限的实际场景问题。


<details>
  <summary>Details</summary>
Motivation: 现有的连续委员会选举模型在实际应用中受限，因为寻找最佳委员会序列的任务在大多数选择标准下已是NP难题，尤其是当委员会规模为三时。缺乏高效算法进一步限制了该模型的潜力。

Method: 作者设计了一种参数化算法，针对候选人数适中或时间范围有限的现实场景，有效解决了上述难题。

Result: 提出的算法能够在实际场景中有效解决连续委员会选举中的NP难题，尤其是在候选人数适中或时间范围有限的情况下。

Conclusion: 通过参数化算法，论文成功解锁了连续委员会选举模型的潜力，为解决实际中的NP难题提供了有效工具。

Abstract: In a recently introduced model of successive committee elections (Bredereck
et al., AAAI-20) for a given set of ordinal or approval preferences one aims to
find a sequence of a given length of "best" same-size committees such that each
candidate is a member of a limited number of consecutive committees. However,
the practical usability of this model remains limited, as the described task
turns out to be NP-hard for most selection criteria already for seeking
committees of size three. Non-trivial or somewhat efficient algorithms for
these cases are lacking too. Motivated by a desire to unlock the full potential
of the described temporal model of committee elections, we devise
(parameterized) algorithms that effectively solve the mentioned hard cases in
realistic scenarios of a moderate number of candidates or of a limited time
horizon.

</details>


### [969] [Incentivizing High-Quality Human Annotations with Golden Questions](https://arxiv.org/abs/2505.19134)
*Shang Liu,Zhongze Cai,Hanzhao Wang,Zhongyao Ma,Xiaocheng Li*

Main category: cs.GT

TL;DR: 该论文研究了如何通过激励机制提高人工标注数据的质量，提出了一种基于主代理模型和最大似然估计的方法，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 人工标注数据在训练大型语言模型中起着至关重要的作用，但付费标注者并不总是能提供高质量的数据。因此，研究如何激励标注者提高数据质量成为一个重要问题。

Method: 论文采用主代理模型来描述公司与标注者之间的动态关系，并通过最大似然估计（MLE）和假设检验来激励标注者。标注者如果通过假设检验，将获得奖励。

Result: 研究发现，标注者的策略行为使得假设检验的速率与传统方法不同，为Θ(1/√(n log n))。此外，论文提出了选择“黄金问题”的两个标准，并通过实验验证了这些问题的有效性。

Conclusion: 通过激励机制和黄金问题的选择，可以更有效地监控和提高标注者的表现，从而提升人工标注数据的质量。

Abstract: Human-annotated data plays a vital role in training large language models
(LLMs), such as supervised fine-tuning and human preference alignment. However,
it is not guaranteed that paid human annotators produce high-quality data. In
this paper, we study how to incentivize human annotators to do so. We start
from a principal-agent model to model the dynamics between the company (the
principal) and the annotator (the agent), where the principal can only monitor
the annotation quality by examining $n$ samples. We investigate the maximum
likelihood estimators (MLE) and the corresponding hypothesis testing to
incentivize annotators: the agent is given a bonus if the MLE passes the test.
By analyzing the variance of the outcome, we show that the strategic behavior
of the agent makes the hypothesis testing very different from traditional ones:
Unlike the exponential rate proved by the large deviation theory, the
principal-agent model's hypothesis testing rate is of $\Theta(1/\sqrt{n \log
n})$. Our theory implies two criteria for the \emph{golden questions} to
monitor the performance of the annotators: they should be of (1) high certainty
and (2) similar format to normal ones. In that light, we select a set of golden
questions in human preference data. By doing incentive-compatible experiments,
we find out that the annotators' behavior is better revealed by those golden
questions, compared to traditional survey techniques such as instructed
manipulation checks.

</details>


### [970] [Continuous-Time Analysis of Heavy Ball Momentum in Min-Max Games](https://arxiv.org/abs/2505.19537)
*Yi Feng,Kaito Fujii,Stratis Skoulakis,Xiao Wang,Volkan Cevher*

Main category: cs.GT

TL;DR: 该论文研究了在极小极大博弈中，重球动量（HB）的作用及其与最小化问题中的差异，发现较小的动量能增强算法稳定性并引导轨迹至损失景观的平缓区域。


<details>
  <summary>Details</summary>
Motivation: 尽管HB动量在最小化问题中被广泛研究，但在极小极大博弈中的作用尚不明确，这限制了如Adam等实际算法的效果。

Method: 论文通过连续时间分析，研究了HB在极小极大博弈中的同步和交替更新方案，并进行了局部和全局分析。

Result: 局部上，较小动量通过更广的步长范围实现收敛；全局上，较小动量引导算法轨迹至损失景观的平缓区域，且交替更新放大此效应。

Conclusion: HB在极小极大博弈中的作用与最小化问题中存在根本差异，数值实验验证了理论结果。

Abstract: Since Polyak's pioneering work, heavy ball (HB) momentum has been widely
studied in minimization. However, its role in min-max games remains largely
unexplored. As a key component of practical min-max algorithms like Adam, this
gap limits their effectiveness. In this paper, we present a continuous-time
analysis for HB with simultaneous and alternating update schemes in min-max
games. Locally, we prove smaller momentum enhances algorithmic stability by
enabling local convergence across a wider range of step sizes, with alternating
updates generally converging faster. Globally, we study the implicit
regularization of HB, and find smaller momentum guides algorithms trajectories
towards shallower slope regions of the loss landscapes, with alternating
updates amplifying this effect. Surprisingly, all these phenomena differ from
those observed in minimization, where larger momentum yields similar effects.
Our results reveal fundamental differences between HB in min-max games and
minimization, and numerical experiments further validate our theoretical
results.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [971] [FinLoRA: Benchmarking LoRA Methods for Fine-Tuning LLMs on Financial Datasets](https://arxiv.org/abs/2505.19819)
*Dannong Wang,Jaisal Patel,Daochen Zha,Steve Y. Yang,Xiao-Yang Liu*

Main category: cs.CE

TL;DR: FinLoRA项目评估了LoRA方法在金融领域的应用，展示了其在专业金融任务中的显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 探索LoRA方法在高风险金融领域（如通过CFA考试和分析SEC文件）的有效性，此前这些领域的研究较少。

Method: 创建了19个金融数据集，评估了五种LoRA方法和五种基础LLM，并提供了详细的实验结果和计算成本分析。

Result: LoRA方法平均比基础模型性能提升36%，项目提供了可扩展的金融智能解决方案。

Conclusion: FinLoRA项目为公众提供了一种经济实惠且可扩展的方法，以普及金融智能。

Abstract: Low-rank adaptation (LoRA) methods show great potential for scaling
pre-trained general-purpose Large Language Models (LLMs) to hundreds or
thousands of use scenarios. However, their efficacy in high-stakes domains like
finance is rarely explored, e.g., passing CFA exams and analyzing SEC filings.
In this paper, we present the open-source FinLoRA project that benchmarks LoRA
methods on both general and highly professional financial tasks. First, we
curated 19 datasets covering diverse financial applications; in particular, we
created four novel XBRL analysis datasets based on 150 SEC filings. Second, we
evaluated five LoRA methods and five base LLMs. Finally, we provide extensive
experimental results in terms of accuracy, F1, and BERTScore and report
computational cost in terms of time and GPU memory during fine-tuning and
inference stages. We find that LoRA methods achieved substantial performance
gains of 36\% on average over base models. Our FinLoRA project provides an
affordable and scalable approach to democratize financial intelligence to the
general public. Datasets, LoRA adapters, code, and documentation are available
at https://github.com/Open-Finance-Lab/FinLoRA

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [972] [ICDM: Interference Cancellation Diffusion Models for Wireless Semantic Communications](https://arxiv.org/abs/2505.19983)
*Tong Wu,Zhiyong Chen,Dazhi He,Feng Yang,Meixia Tao,Xiaodong Xu,Wenjun Zhang,Ping Zhang*

Main category: cs.IT

TL;DR: 该论文提出了一种基于扩散模型的干扰消除方法（ICDM），用于无线语义通信系统中有效抑制干扰，显著降低了均方误差并提升了感知质量。


<details>
  <summary>Details</summary>
Motivation: 无线信号的广播特性使其不仅容易受到高斯噪声的影响，还会受到未知干扰的影响。因此，研究扩散模型（DMs）是否能够有效抑制无线语义通信系统中的干扰成为一个重要问题。

Method: 论文将干扰消除问题建模为信号和干扰联合后验概率的最大后验（MAP）问题，并提出了干扰消除扩散模型（ICDM）。ICDM通过分解联合后验为信号和干扰的独立先验概率及信道转移概率，利用扩散模型分别学习这些分布的梯度，并结合先进的数值迭代方法实现快速准确的干扰消除。

Result: 实验表明，ICDM显著降低了均方误差（MSE）并提升了感知质量。例如，在CelebA数据集上，信噪比（SNR）为20 dB且信干噪比（SINR）为0 dB时，ICDM将MSE降低了4.54 dB，并将学习感知图像块相似度（LPIPS）提升了2.47 dB。

Conclusion: ICDM通过扩散模型和数值迭代方法的结合，实现了无线语义通信系统中干扰的高效消除，为未来无线通信系统的干扰抑制提供了新的解决方案。

Abstract: Diffusion models (DMs) have recently achieved significant success in wireless
communications systems due to their denoising capabilities. The broadcast
nature of wireless signals makes them susceptible not only to Gaussian noise,
but also to unaware interference. This raises the question of whether DMs can
effectively mitigate interference in wireless semantic communication systems.
In this paper, we model the interference cancellation problem as a maximum a
posteriori (MAP) problem over the joint posterior probability of the signal and
interference, and theoretically prove that the solution provides excellent
estimates for the signal and interference. To solve this problem, we develop an
interference cancellation diffusion model (ICDM), which decomposes the joint
posterior into independent prior probabilities of the signal and interference,
along with the channel transition probablity. The log-gradients of these
distributions at each time step are learned separately by DMs and accurately
estimated through deriving. ICDM further integrates these gradients with
advanced numerical iteration method, achieving accurate and rapid interference
cancellation. Extensive experiments demonstrate that ICDM significantly reduces
the mean square error (MSE) and enhances perceptual quality compared to schemes
without ICDM. For example, on the CelebA dataset under the Rayleigh fading
channel with a signal-to-noise ratio (SNR) of $20$ dB and signal to
interference plus noise ratio (SINR) of 0 dB, ICDM reduces the MSE by 4.54 dB
and improves the learned perceptual image patch similarity (LPIPS) by 2.47 dB.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [973] [LLM-Meta-SR: Learning to Evolve Selection Operators for Symbolic Regression](https://arxiv.org/abs/2505.18602)
*Hengzhe Zhang,Qi Chen,Bing Xue,Mengjie Zhang*

Main category: cs.NE

TL;DR: 本文提出了一种基于LLM的进化框架，用于自动设计符号回归算法的选择算子，通过控制代码膨胀和引入语义感知，显著提升了算法性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在符号回归中的应用仍依赖人工设计，存在代码膨胀和缺乏语义指导的问题，限制了算法性能和可解释性。

Method: 提出学习进化框架，结合代码膨胀控制和语义感知选择算子，并在提示中嵌入领域知识以生成更有效的算子。

Result: 实验表明，LLM设计的算子优于9种专家设计的基线方法，达到了最先进的性能。

Conclusion: LLM能够超越专家水平，自动设计出高性能的符号回归算法。

Abstract: Large language models (LLMs) have revolutionized algorithm development, yet
their application in symbolic regression, where algorithms automatically
discover symbolic expressions from data, remains constrained and is typically
designed manually by human experts. In this paper, we propose a
learning-to-evolve framework that enables LLMs to automatically design
selection operators for evolutionary symbolic regression algorithms. We first
identify two key limitations in existing LLM-based algorithm evolution
techniques: code bloat and a lack of semantic guidance. Bloat results in
unnecessarily complex components, and the absence of semantic awareness can
lead to ineffective exchange of useful code components, both of which can
reduce the interpretability of the designed algorithm or hinder evolutionary
learning progress. To address these issues, we enhance the LLM-based evolution
framework for meta symbolic regression with two key innovations: bloat control
and a complementary, semantics-aware selection operator. Additionally, we embed
domain knowledge into the prompt, enabling the LLM to generate more effective
and contextually relevant selection operators. Our experimental results on
symbolic regression benchmarks show that LLMs can devise selection operators
that outperform nine expert-designed baselines, achieving state-of-the-art
performance. This demonstrates that LLMs can exceed expert-level algorithm
design for symbolic regression.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [974] [PacTrain: Pruning and Adaptive Sparse Gradient Compression for Efficient Collective Communication in Distributed Deep Learning](https://arxiv.org/abs/2505.18563)
*Yisu Wang,Ruilong Wu,Xinjiao Li,Dirk Kutscher*

Main category: cs.DC

TL;DR: PacTrain框架通过结合剪枝和稀疏梯度压缩技术，显著提升分布式训练效率，在带宽受限条件下实现1.25至8.72倍的训练吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络和数据集规模的增长，分布式训练中的梯度聚合开销成为主要瓶颈，现有梯度压缩方案往往无法同时保证训练加速和精度。

Method: 提出PacTrain框架：1) 主动剪枝使模型权重和梯度稀疏化；2) 利用全局梯度稀疏知识实现轻量级压缩通信；3) 保持与all-reduce原语的兼容性。

Result: 实验表明：在视觉和语言模型训练任务中，PacTrain在带宽受限条件下比现有压缩系统提升1.25-8.72倍吞吐量，同时保持精度。

Conclusion: PacTrain通过剪枝与稀疏压缩的协同设计，实现了近乎最优的压缩策略，为大规模分布式训练提供了高效解决方案。

Abstract: Large-scale deep neural networks (DNN) exhibit excellent performance for
various tasks. As DNNs and datasets grow, distributed training becomes
extremely time-consuming and demands larger clusters. A main bottleneck is the
resulting gradient aggregation overhead. While gradient compression and sparse
collective communication techniques are commonly employed to alleviate network
load, many gradient compression schemes do not achieve acceleration of the
training process while also preserving accuracy. This paper introduces
PacTrain, a novel framework that accelerates distributed training by combining
pruning with sparse gradient compression. Active pruning of the neural network
makes the model weights and gradients sparse. By ensuring the global knowledge
of the gradient sparsity among all distributed training workers, we can perform
lightweight compression communication without harming accuracy. We show that
the PacTrain compression scheme achieves a near-optimal compression strategy
while remaining compatible with the all-reduce primitive. Experimental
evaluations show that PacTrain improves training throughput by 1.25 to 8.72
times compared to state-of-the-art compression-enabled systems for
representative vision and language models training tasks under
bandwidth-constrained conditions.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [975] [Hamiltonian Theory and Computation of Optimal Probability Density Control in High Dimensions](https://arxiv.org/abs/2505.18362)
*Nathan Gaby,Xiaojing Ye*

Main category: math.OC

TL;DR: 该论文提出了一个通用的最优概率密度控制理论框架，并开发了一种可扩展至高维的数值算法。通过建立Pontryagin最大值原理和Hamilton-Jacobi-Bellman方程，结合深度神经网络参数化控制向量场和伴随函数，解决了高维状态空间中的密度控制问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决高维状态空间中的最优概率密度控制问题，尤其是在存在障碍和非线性交互挑战的情况下，传统方法难以应对高维复杂性。

Method: 方法包括建立Pontryagin最大值原理和Hamilton-Jacobi-Bellman方程，使用深度神经网络等降阶模型参数化控制向量场和伴随函数，以处理高维问题。

Result: 数值结果表明，该算法在高维密度控制问题中表现出色，能够有效应对障碍和非线性交互等挑战。

Conclusion: 论文成功提出了一个理论框架和数值算法，为高维最优概率密度控制问题提供了有效的解决方案，并通过实验验证了其性能。

Abstract: We develop a general theoretical framework for optimal probability density
control and propose a numerical algorithm that is scalable to solve the control
problem in high dimensions. Specifically, we establish the Pontryagin Maximum
Principle (PMP) for optimal density control and construct the
Hamilton-Jacobi-Bellman (HJB) equation of the value functional through rigorous
derivations without any concept from Wasserstein theory. To solve the density
control problem numerically, we propose to use reduced-order models, such as
deep neural networks (DNNs), to parameterize the control vector-field and the
adjoint function, which allows us to tackle problems defined on
high-dimensional state spaces. We also prove several convergence properties of
the proposed algorithm. Numerical results demonstrate promising performances of
our algorithm on a variety of density control problems with obstacles and
nonlinear interaction challenges in high dimensions.

</details>


### [976] [Fractional-Boundary-Regularized Deep Galerkin Method for Variational Inequalities in Mixed Optimal Stopping and Control](https://arxiv.org/abs/2505.19309)
*Yun Zhao,Harry Zheng*

Main category: math.OC

TL;DR: 该论文提出了一种结合对偶变换和分数边界正则化深度Galerkin方法（FBR-DGM）的新方法，用于解决混合最优停止和随机控制问题中的非线性HJB方程，提高了数值解的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 混合最优停止和随机控制问题中的非线性HJB方程数值解难度大且缺乏可靠基准，亟需一种高效且稳定的数值解法。

Method: 首先利用对偶方法将非线性HJB方程转化为线性算子，然后引入分数边界正则化深度Galerkin方法（FBR-DGM），通过增强经典$L^2$损失函数并结合Sobolev-Slobodeckij范数，提升网络近似及其导数的准确性。

Result: FBR-DGM方法显著提高了数值解的准确性，并能通过对偶变换还原原始解，同时通过检验最优值、最优财富和最优控制之间的对偶关系，验证了方法的自洽性和稳定性。

Conclusion: 该方法为非线性HJB方程的数值解提供了创新的基准，并在缺乏解析解的情况下展现出良好的应用潜力。

Abstract: Mixed optimal stopping and stochastic control problems define variational
inequalities with non-linear Hamilton-Jacobi-Bellman (HJB) operators, whose
numerical solution is notoriously difficult and lack of reliable benchmarks. We
first use the dual approach to transform it into a linear operator, and then
introduce a Fractional-Boundary-Regularized Deep Galerkin Method (FBR-DGM) that
augments the classical $L^2$ loss with Sobolev-Slobodeckij norms on the
parabolic boundary, enforcing regularity and yielding consistent improvements
in the network approximation and its derivatives. The improved accuracy allows
the network to be converted back to the original solution using the dual
transform. The self-consistency and stability of the network can be tested by
checking the primal-dual relationship among optimal value, optimal wealth, and
optimal control, offering innovative benchmarks in the absence of analytical
solutions.

</details>


### [977] [A Structured Tour of Optimization with Finite Differences](https://arxiv.org/abs/2505.19720)
*Marco Rando,Cesare Molinari,Lorenzo Rosasco,Silvia Villa*

Main category: math.OC

TL;DR: 该论文研究了有限差分法中结构化方向选择的影响，发现结构化方向在计算成本相近的情况下能显著提升梯度估计精度和优化性能。


<details>
  <summary>Details</summary>
Motivation: 在梯度信息不可用或计算成本高昂的情况下，有限差分法广泛用于零阶优化。虽然理论上结构化方向（如正交方向）能提升性能，但其额外计算成本限制了在高维场景的应用。本文旨在探讨结构化方向的实际影响。

Method: 论文回顾并扩展了多种结构化方向矩阵的构建策略，并与非结构化方法在计算成本、梯度近似质量和收敛行为方面进行比较，实验涵盖合成任务和对抗扰动等实际应用。

Result: 结果表明，结构化方向能以与非结构化方向相近的计算成本生成，同时显著提高梯度估计精度和优化性能。

Conclusion: 结构化方向在有限差分法中具有实际优势，尤其在计算成本和性能提升之间取得了良好平衡。

Abstract: Finite-difference methods are widely used for zeroth-order optimization in
settings where gradient information is unavailable or expensive to compute.
These procedures mimic first-order strategies by approximating gradients
through function evaluations along a set of random directions. From a
theoretical perspective, recent studies indicate that imposing structure (such
as orthogonality) on the chosen directions allows for the derivation of
convergence rates comparable to those achieved with unstructured random
directions (i.e., directions sampled independently from a distribution).
Empirically, although structured directions are expected to enhance
performance, they often introduce additional computational costs, which can
limit their applicability in high-dimensional settings. In this work, we
examine the impact of structured direction selection in finite-difference
methods. We review and extend several strategies for constructing structured
direction matrices and compare them with unstructured approaches in terms of
computational cost, gradient approximation quality, and convergence behavior.
Our evaluation spans both synthetic tasks and real-world applications such as
adversarial perturbation. The results demonstrate that structured directions
can be generated with computational costs comparable to unstructured ones while
significantly improving gradient estimation accuracy and optimization
performance.

</details>


### [978] [New Perspectives on the Polyak Stepsize: Surrogate Functions and Negative Results](https://arxiv.org/abs/2505.20219)
*Francesco Orabona,Ryan D'Orazio*

Main category: math.OC

TL;DR: 本文提出了一种新的统一视角，将Polyak步长及其变体视为对替代损失的梯度下降，并分析了其收敛性和局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管Polyak步长在凸优化中被证明是基础且通用的，但目前对其收敛性和缺点的理解仍不完整且分散。本文旨在提供一个统一的分析框架。

Method: 通过将Polyak步长及其变体视为对替代损失的梯度下降，提出了一种统一的分析方法，并展示了其与局部曲率自适应步长的等价性。

Result: 本文不仅统一分析了现有变体在不同假设下的表现，还通过负面结果证明了某些上界中的非收敛性是真实存在的。

Conclusion: 通过替代损失的视角，本文为Polyak步长及其变体提供了一个简单而统一的分析框架，揭示了其收敛性和局限性。

Abstract: The Polyak stepsize has been proven to be a fundamental stepsize in convex
optimization, giving near optimal gradient descent rates across a wide range of
assumptions. The universality of the Polyak stepsize has also inspired many
stochastic variants, with theoretical guarantees and strong empirical
performance. Despite the many theoretical results, our understanding of the
convergence properties and shortcomings of the Polyak stepsize or its variants
is both incomplete and fractured across different analyses. We propose a new,
unified, and simple perspective for the Polyak stepsize and its variants as
gradient descent on a surrogate loss. We show that each variant is equivalent
to minimize a surrogate function with stepsizes that adapt to a guaranteed
local curvature. Our general surrogate loss perspective is then used to provide
a unified analysis of existing variants across different assumptions. Moreover,
we show a number of negative results proving that the non-convergence results
in some of the upper bounds is indeed real.

</details>
