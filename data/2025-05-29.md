<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 102]
- [cs.AI](#cs.AI) [Total: 27]
- [cs.LG](#cs.LG) [Total: 139]
- [physics.soc-ph](#physics.soc-ph) [Total: 2]
- [eess.IV](#eess.IV) [Total: 10]
- [cs.CR](#cs.CR) [Total: 6]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.CY](#cs.CY) [Total: 11]
- [math.NA](#math.NA) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [eess.AS](#eess.AS) [Total: 4]
- [cs.SC](#cs.SC) [Total: 1]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.CV](#cs.CV) [Total: 72]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.HC](#cs.HC) [Total: 4]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.RO](#cs.RO) [Total: 13]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.SE](#cs.SE) [Total: 3]
- [eess.SP](#eess.SP) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [stat.CO](#stat.CO) [Total: 2]
- [cs.SD](#cs.SD) [Total: 9]
- [quant-ph](#quant-ph) [Total: 5]
- [stat.ML](#stat.ML) [Total: 17]
- [cs.IT](#cs.IT) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [math.OC](#math.OC) [Total: 3]
- [cs.MA](#cs.MA) [Total: 7]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models](https://arxiv.org/abs/2505.21523)
*Chengzhi Liu,Zhongxing Xu,Qingyue Wei,Juncheng Wu,James Zou,Xin Eric Wang,Yuyin Zhou,Sheng Liu*

Main category: cs.CL

TL;DR: 论文研究了多模态大语言模型在长推理链中视觉基础减弱导致的幻觉问题，提出了RH-AUC指标和RH-Bench基准，发现模型规模和数据类别对推理与感知平衡的影响大于数据量。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在长推理链任务中表现提升，但伴随视觉基础减弱和幻觉增加的问题，需要系统研究这一现象及其影响因素。

Method: 引入RH-AUC指标量化模型感知准确率随推理长度的变化，发布RH-Bench基准评估推理能力与幻觉的权衡。

Result: 较大模型在推理与感知间平衡更好；训练数据的类别和领域比总量更影响这一平衡。

Conclusion: 需联合评估推理质量与感知保真度，模型规模和数据特性是关键影响因素。

Abstract: Test-time compute has empowered multimodal large language models to generate
extended reasoning chains, yielding strong performance on tasks such as
multimodal math reasoning. However, this improved reasoning ability often comes
with increased hallucination: as generations become longer, models tend to
drift away from image-grounded content and rely more heavily on language
priors. Attention analysis shows that longer reasoning chains lead to reduced
focus on visual inputs, which contributes to hallucination. To systematically
study this phenomenon, we introduce RH-AUC, a metric that quantifies how a
model's perception accuracy changes with reasoning length, allowing us to
evaluate whether the model preserves visual grounding during reasoning. We also
release RH-Bench, a diagnostic benchmark that spans a variety of multimodal
tasks, designed to assess the trade-off between reasoning ability and
hallucination. Our analysis reveals that (i) larger models typically achieve a
better balance between reasoning and perception, and (ii) this balance is
influenced more by the types and domains of training data than by its overall
volume. These findings underscore the importance of evaluation frameworks that
jointly consider both reasoning quality and perceptual fidelity.

</details>


### [2] [Loquacious Set: 25,000 Hours of Transcribed and Diverse English Speech Recognition Data for Research and Commercial Use](https://arxiv.org/abs/2505.21578)
*Titouan Parcollet,Yuan Tseng,Shucong Zhang,Rogier van Dalen*

Main category: cs.CL

TL;DR: 论文介绍了Loquacious Set，一个25,000小时的商用英语语音数据集，旨在解决现有ASR数据集在规模、多样性和许可方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的ASR数据集如LibriSpeech在规模和多样性上存在局限，而新数据集如MOSEL、Gigaspeech等则因许可问题、转录质量或缺乏评估集而无法满足工业界和学术界的需求。

Method: 通过构建一个包含数十万说话者、多种口音和语音类型（朗读、自发、谈话、清晰、嘈杂）的大规模商用语音数据集Loquacious Set。

Result: Loquacious Set提供了一个25,000小时的多样化英语语音数据集，适用于工业界和学术界在真实场景中构建ASR系统。

Conclusion: Loquacious Set填补了现有ASR数据集的不足，为工业界和学术界提供了一个高质量、多样化的语音数据集，推动了ASR研究的发展。

Abstract: Automatic speech recognition (ASR) research is driven by the availability of
common datasets between industrial researchers and academics, encouraging
comparisons and evaluations. LibriSpeech, despite its long success as an ASR
benchmark, is now limited by its size and focus on clean, read speech, leading
to near-zero word error rates. More recent datasets, including MOSEL, YODAS,
Gigaspeech, OWSM, Libriheavy or People's Speech suffer from major limitations
including licenses that researchers in the industry cannot use, unreliable
transcriptions, incorrect audio data, or the lack of evaluation sets. This work
presents the Loquacious Set, a 25,000-hour curated collection of commercially
usable English speech. Featuring hundreds of thousands of speakers with diverse
accents and a wide range of speech types (read, spontaneous, talks, clean,
noisy), the Loquacious Set is designed to work for academics and researchers in
the industry to build ASR systems in real-world scenarios.

</details>


### [3] [Rethinking Data Mixture for Large Language Models: A Comprehensive Survey and New Perspectives](https://arxiv.org/abs/2505.21598)
*Yajiao Liu,Congliang Chen,Junchi Yang,Ruoyu Sun*

Main category: cs.CL

TL;DR: 本文综述了现有数据混合方法，提出细粒度分类，总结了各子类的问题表述和算法，并讨论了优缺点及关键挑战。


<details>
  <summary>Details</summary>
Motivation: 在固定训练预算下，不同领域数据的采样比例显著影响模型性能，如何确定领域权重以在有限计算资源下训练最佳模型是核心问题。

Method: 将现有方法细分为离线（启发式、算法式、函数拟合）和在线（极小极大优化、混合规律等）方法，分析其问题表述和代表性算法。

Result: 系统梳理了各类方法的关联与区别，总结了各方法的优缺点，并指出数据混合领域的关键挑战。

Conclusion: 通过分类和比较，为数据混合方法提供了全面指导，未来需解决在线优化与领域适应性等挑战。

Abstract: Training large language models with data collected from various domains can
improve their performance on downstream tasks. However, given a fixed training
budget, the sampling proportions of these different domains significantly
impact the model's performance. How can we determine the domain weights across
different data domains to train the best-performing model within constrained
computational resources? In this paper, we provide a comprehensive overview of
existing data mixture methods. First, we propose a fine-grained categorization
of existing methods, extending beyond the previous offline and online
classification. Offline methods are further grouped into heuristic-based,
algorithm-based, and function fitting-based methods. For online methods, we
categorize them into three groups: online min-max optimization, online mixing
law, and other approaches by drawing connections with the optimization
frameworks underlying offline methods. Second, we summarize the problem
formulations, representative algorithms for each subtype of offline and online
methods, and clarify the relationships and distinctions among them. Finally, we
discuss the advantages and disadvantages of each method and highlight key
challenges in the field of data mixture.

</details>


### [4] [R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing](https://arxiv.org/abs/2505.21600)
*Tianyu Fu,Yi Ge,Yichen You,Enshu Liu,Zhihang Yuan,Guohao Dai,Shengen Yan,Huazhong Yang,Yu Wang*

Main category: cs.CL

TL;DR: 论文提出R2R方法，通过智能路由关键令牌来结合大模型和小模型的优势，显著提升推理效率与性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型(LLMs)推理开销大，而蒸馏后的小模型(SLMs)虽高效但性能不足。研究发现仅少数令牌真正影响推理路径差异，这成为优化突破口。

Method: 提出R2R神经令牌路由方法：1) 自动识别关键差异令牌；2) 仅对关键令牌调用LLMs；3) 其余令牌由SLM生成；4) 配套数据生成管道训练轻量级路由器。

Result: 在数学/编程/QA任务中：1) 平均激活参数量5.6B时，性能超7B模型1.6倍；2) 相比32B模型提速2.8倍且性能相当；3) 推进推理效率的帕累托前沿。

Conclusion: R2R通过令牌级智能路由，首次实现同时超越纯SLM性能和纯LLM效率，为模型部署提供新范式。

Abstract: Large Language Models (LLMs) achieve impressive reasoning capabilities at the
cost of substantial inference overhead, posing substantial deployment
challenges. Although distilled Small Language Models (SLMs) significantly
enhance efficiency, their performance suffers as they fail to follow LLMs'
reasoning paths. Luckily, we reveal that only a small fraction of tokens
genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens
are either identical or exhibit neutral differences, such as minor variations
in abbreviations or expressions. Leveraging this insight, we introduce **Roads
to Rome (R2R)**, a neural token routing method that selectively utilizes LLMs
only for these critical, path-divergent tokens, while leaving the majority of
token generation to the SLM. We also develop an automatic data generation
pipeline that identifies divergent tokens and generates token-level routing
labels to train the lightweight router. We apply R2R to combine R1-1.5B and
R1-32B models from the DeepSeek family, and evaluate on challenging math,
coding, and QA benchmarks. With an average activated parameter size of 5.6B,
R2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the
R1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with
comparable performance, advancing the Pareto frontier of test-time scaling
efficiency. Our code is available at https://github.com/thu-nics/R2R.

</details>


### [5] [How does Misinformation Affect Large Language Model Behaviors and Preferences?](https://arxiv.org/abs/2505.21608)
*Miao Peng,Nuo Chen,Jianheng Tang,Jia Li*

Main category: cs.CL

TL;DR: 该论文提出了MisBench，一个评估大语言模型（LLMs）在错误信息面前行为和知识偏好的最大最全面基准，并发现LLMs虽能辨别错误信息，但仍易受知识冲突和风格变化影响。作者还提出了一种新方法RtD来增强LLMs检测错误信息的能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽然探讨了LLMs在对抗错误信息中的作用，但缺乏对LLMs受错误信息影响的具体方面和程度的细粒度分析。为了填补这一空白，作者提出了MisBench基准。

Method: 作者构建了包含10,346,712条错误信息的MisBench基准，综合考虑了知识冲突和风格变化。基于此，提出了一种名为Reconstruct to Discriminate（RtD）的新方法来增强LLMs检测错误信息的能力。

Result: 实验结果表明，LLMs在辨别错误信息方面表现出相当的能力，但仍易受知识冲突和风格变化的影响。提出的RtD方法有效提升了LLMs检测错误信息的可靠性。

Conclusion: 该研究为LLMs与错误信息的互动提供了有价值的见解，MisBench可作为评估LLM检测器及提升其在实际应用中可靠性的有效基准。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in
knowledge-intensive tasks, while they remain vulnerable when encountering
misinformation. Existing studies have explored the role of LLMs in combating
misinformation, but there is still a lack of fine-grained analysis on the
specific aspects and extent to which LLMs are influenced by misinformation. To
bridge this gap, we present MisBench, the current largest and most
comprehensive benchmark for evaluating LLMs' behavior and knowledge preference
toward misinformation. MisBench consists of 10,346,712 pieces of
misinformation, which uniquely considers both knowledge-based conflicts and
stylistic variations in misinformation. Empirical results reveal that while
LLMs demonstrate comparable abilities in discerning misinformation, they still
remain susceptible to knowledge conflicts and stylistic variations. Based on
these findings, we further propose a novel approach called Reconstruct to
Discriminate (RtD) to strengthen LLMs' ability to detect misinformation. Our
study provides valuable insights into LLMs' interactions with misinformation,
and we believe MisBench can serve as an effective benchmark for evaluating
LLM-based detectors and enhancing their reliability in real-world applications.
Codes and data are available at https://github.com/GKNL/MisBench.

</details>


### [6] [Iterative Corpus Refinement for Materials Property Prediction Based on Scientific Texts](https://arxiv.org/abs/2505.21646)
*Lei Zhang,Markus Stricker*

Main category: cs.CL

TL;DR: 论文提出了一种迭代框架，通过选择多样化的文献、训练Word2Vec模型并监测嵌入空间中成分-性质关联的收敛性，来加速材料发现与优化，成功预测了高性能电催化材料。


<details>
  <summary>Details</summary>
Motivation: 材料发现面临组合爆炸问题，数据稀缺且潜在的科学文本知识未被充分利用。论文旨在利用迭代文本优化方法挖掘科学文献中的隐含知识，加速高性能材料的筛选。

Method: 采用迭代框架，通过策略性选择多样化文献、训练Word2Vec模型，并监测嵌入空间中成分-性质关联的收敛性，预测高性能电催化材料。

Result: 该方法成功预测了氧还原（ORR）、析氢（HER）和析氧（OER）反应中性能最高的材料成分，并通过实验验证了其电催化性能。

Conclusion: 该研究验证了迭代文本优化在加速材料发现中的潜力，为筛选大数据稀缺的组合空间提供了可扩展且高效的工具。

Abstract: The discovery and optimization of materials for specific applications is
hampered by the practically infinite number of possible elemental combinations
and associated properties, also known as the `combinatorial explosion'. By
nature of the problem, data are scarce and all possible data sources should be
used. In addition to simulations and experimental results, the latent knowledge
in scientific texts is not yet used to its full potential. We present an
iterative framework that refines a given scientific corpus by strategic
selection of the most diverse documents, training Word2Vec models, and
monitoring the convergence of composition-property correlations in embedding
space. Our approach is applied to predict high-performing materials for oxygen
reduction (ORR), hydrogen evolution (HER), and oxygen evolution (OER) reactions
for a large number of possible candidate compositions. Our method successfully
predicts the highest performing compositions among a large pool of candidates,
validated by experimental measurements of the electrocatalytic performance in
the lab. This work demonstrates and validates the potential of iterative corpus
refinement to accelerate materials discovery and optimization, offering a
scalable and efficient tool for screening large compositional spaces where
reliable data are scarce or non-existent.

</details>


### [7] [Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations](https://arxiv.org/abs/2505.21657)
*Zeinab Dehghani,Koorosh Aslansefat,Adil Khan,Mohammed Naveed Akram*

Main category: cs.CL

TL;DR: 提出SMILE方法，通过修改输入并测量输出变化，生成热图解释大语言模型的响应机制，提升AI透明度。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（如GPT、LLAMA等）的决策过程不透明，在需要信任和问责的领域可能引发问题。

Method: SMILE通过微调输入文本、量化输出差异，生成热图直观显示提示词影响力，支持跨模型通用分析。

Result: 经多指标验证，SMILE能稳定、准确地解释主流LLMs的决策依据，可视化效果清晰可靠。

Conclusion: SMILE增强了AI模型的可解释性，为构建透明可信的AI系统迈出重要一步。

Abstract: Large language models like GPT, LLAMA, and Claude have become incredibly
powerful at generating text, but they are still black boxes, so it is hard to
understand how they decide what to say. That lack of transparency can be
problematic, especially in fields where trust and accountability matter. To
help with this, we introduce SMILE, a new method that explains how these models
respond to different parts of a prompt. SMILE is model-agnostic and works by
slightly changing the input, measuring how the output changes, and then
highlighting which words had the most impact. Create simple visual heat maps
showing which parts of a prompt matter the most. We tested SMILE on several
leading LLMs and used metrics such as accuracy, consistency, stability, and
fidelity to show that it gives clear and reliable explanations. By making these
models easier to understand, SMILE brings us one step closer to making AI more
transparent and trustworthy.

</details>


### [8] [Rethinking the Outlier Distribution in Large Language Models: An In-depth Study](https://arxiv.org/abs/2505.21670)
*Rahul Raman,Khushi Sharma,Sai Qian Zhang*

Main category: cs.CL

TL;DR: 该论文研究了大语言模型(LLMs)中的异常值问题，分析了其形成机制并提出了减少异常值的有效方法，以提升量化精度和模型效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型中的异常值会显著影响量化效果，导致模型性能下降。虽然已有多种量化算法试图解决此问题，但对异常值根源的深入探索较少。

Method: 论文全面调查了两种常见异常值(大规模激活和通道级异常)的形成机制，并提出了相应的缓解策略。

Result: 研究提出了高效方法，能在最小化精度损失的前提下消除大部分大规模激活和通道级异常值。

Conclusion: 通过深入理解异常值成因并针对性解决，可以显著提升大语言模型量化效果，促进在边缘设备上的部署。

Abstract: Investigating outliers in large language models (LLMs) is crucial due to
their significant impact on various aspects of LLM performance, including
quantization and compression. Outliers often cause considerable quantization
errors, leading to degraded model performance. Identifying and addressing these
outliers can enhance the accuracy and efficiency of the quantization process,
enabling smoother deployment on edge devices or specialized hardware. Recent
studies have identified two common types of outliers in LLMs: massive
activations and channel-wise outliers. While numerous quantization algorithms
have been proposed to mitigate their effects and maintain satisfactory
accuracy, few have thoroughly explored the root causes of these outliers in
depth. In this paper, we conduct a comprehensive investigation into the
formation mechanisms of these outliers and propose potential strategies to
mitigate their occurrence. Ultimately, we introduce some efficient approaches
to eliminate most massive activations and channel-wise outliers with minimal
impact on accuracy.

</details>


### [9] [LLMPR: A Novel LLM-Driven Transfer Learning based Petition Ranking Model](https://arxiv.org/abs/2505.21689)
*Avijit Gayen,Somyajit Chakraborty,Mainak Sen,Soham Paul,Angshuman Jana*

Main category: cs.CL

TL;DR: 该论文提出LLMPR框架，利用大语言模型和机器学习自动为法律请愿书分配优先级，以解决印度司法系统中案件积压和效率低下的问题。实验显示，随机森林和决策树模型表现最佳，准确率超过99%。


<details>
  <summary>Details</summary>
Motivation: 印度司法系统中未解决的法律案件持续积累，严重阻碍了及时司法的实现。传统的手工优先级分配方法效率低下且易受主观偏见影响，进一步加剧了延迟。

Method: 论文提出LLMPR框架，利用迁移学习和机器学习技术，通过DistilBERT、LegalBERT和MiniLM等嵌入技术处理非结构化法律文本，并结合定量指标（如间隔天数、排名分数和字数）训练多种机器学习模型，包括随机森林、决策树、XGBoost、LightGBM和CatBoost。

Result: 实验结果表明，随机森林和决策树模型表现最佳，准确率超过99%，Spearman等级相关系数为0.99。仅使用数值特征的模型也能达到近乎最优的排名结果（R2 = 0.988，ρ = 0.998），而基于LLM的嵌入仅带来边际收益。

Conclusion: 自动化请愿书排名可以有效简化司法工作流程，减少案件积压，并提高法律优先级分配的公平性。

Abstract: The persistent accumulation of unresolved legal cases, especially within the
Indian judiciary, significantly hampers the timely delivery of justice. Manual
methods of prioritizing petitions are often prone to inefficiencies and
subjective biases further exacerbating delays. To address this issue, we
propose LLMPR (Large Language Model-based Petition Ranking), an automated
framework that utilizes transfer learning and machine learning to assign
priority rankings to legal petitions based on their contextual urgency.
Leveraging the ILDC dataset comprising 7,593 annotated petitions, we process
unstructured legal text and extract features through various embedding
techniques, including DistilBERT, LegalBERT, and MiniLM. These textual
embeddings are combined with quantitative indicators such as gap days, rank
scores, and word counts to train multiple machine learning models, including
Random Forest, Decision Tree, XGBoost, LightGBM, and CatBoost. Our experiments
demonstrate that Random Forest and Decision Tree models yield superior
performance, with accuracy exceeding 99% and a Spearman rank correlation of
0.99. Notably, models using only numerical features achieve nearly optimal
ranking results (R2 = 0.988, \r{ho} = 0.998), while LLM-based embeddings offer
only marginal gains. These findings suggest that automated petition ranking can
effectively streamline judicial workflows, reduce case backlog, and improve
fairness in legal prioritization.

</details>


### [10] [MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural Awareness Evaluation for LLMs](https://arxiv.org/abs/2505.21693)
*Raoyuan Zhao,Beiduo Chen,Barbara Plank,Michael A. Hedderich*

Main category: cs.CL

TL;DR: 该论文提出了MAKIEval框架，用于自动评估大语言模型在不同语言、地区和文化主题中的文化意识表现，发现模型在英语中表现更优。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型（LLMs）的预训练以英语为中心，导致跨语言文化意识存在偏差，但缺乏全面的多语言评估工具和方法。

Method: 利用Wikidata的多语言结构作为跨语言锚点，自动识别模型输出中的文化实体并链接到结构化知识，提出四项文化意识评估指标。

Result: 评估了7个LLM在13种语言、19个国家和地区、6个文化主题中的表现，发现模型在英语中文化意识更强。

Conclusion: MAKIEval框架无需人工标注或翻译即可实现可扩展的跨语言评估，揭示了LLMs在英语中文化意识表现更优的现象。

Abstract: Large language models (LLMs) are used globally across many languages, but
their English-centric pretraining raises concerns about cross-lingual
disparities for cultural awareness, often resulting in biased outputs. However,
comprehensive multilingual evaluation remains challenging due to limited
benchmarks and questionable translation quality. To better assess these
disparities, we introduce MAKIEval, an automatic multilingual framework for
evaluating cultural awareness in LLMs across languages, regions, and topics.
MAKIEval evaluates open-ended text generation, capturing how models express
culturally grounded knowledge in natural language. Leveraging Wikidata's
multilingual structure as a cross-lingual anchor, it automatically identifies
cultural entities in model outputs and links them to structured knowledge,
enabling scalable, language-agnostic evaluation without manual annotation or
translation. We then introduce four metrics that capture complementary
dimensions of cultural awareness: granularity, diversity, cultural specificity,
and consensus across languages. We assess 7 LLMs developed from different parts
of the world, encompassing both open-source and proprietary systems, across 13
languages, 19 countries and regions, and 6 culturally salient topics (e.g.,
food, clothing). Notably, we find that models tend to exhibit stronger cultural
awareness in English, suggesting that English prompts more effectively activate
culturally grounded knowledge. We publicly release our code and data.

</details>


### [11] [Do We Know What LLMs Don't Know? A Study of Consistency in Knowledge Probing](https://arxiv.org/abs/2505.21701)
*Raoyuan Zhao,Abdullatif Köksal,Ali Modarressi,Michael A. Hedderich,Hinrich Schütze*

Main category: cs.CL

TL;DR: 论文提出通过输入变化和量化指标评估大语言模型知识缺口探测方法的不一致性，发现现有方法在扰动下表现不稳定。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在幻觉问题，需准确识别其知识缺口，但现有探测方法可靠性不足。

Method: 提出基于输入变化和定量指标的新评估流程，分析探测方法的不一致性。

Result: 发现探测方法存在内部不一致（微小提示变化导致40%差异）和跨方法不一致（方法间一致性仅7%）。

Conclusion: 现有知识缺口探测方法存在严重不一致性，亟需开发抗扰动的稳健探测框架。

Abstract: The reliability of large language models (LLMs) is greatly compromised by
their tendency to hallucinate, underscoring the need for precise identification
of knowledge gaps within LLMs. Various methods for probing such gaps exist,
ranging from calibration-based to prompting-based methods. To evaluate these
probing methods, in this paper, we propose a new process based on using input
variations and quantitative metrics. Through this, we expose two dimensions of
inconsistency in knowledge gap probing. (1) Intra-method inconsistency: Minimal
non-semantic perturbations in prompts lead to considerable variance in detected
knowledge gaps within the same probing method; e.g., the simple variation of
shuffling answer options can decrease agreement to around 40%. (2) Cross-method
inconsistency: Probing methods contradict each other on whether a model knows
the answer. Methods are highly inconsistent -- with decision consistency across
methods being as low as 7% -- even though the model, dataset, and prompt are
all the same. These findings challenge existing probing methods and highlight
the urgent need for perturbation-robust probing frameworks.

</details>


### [12] [Assessing and Refining ChatGPT's Performance in Identifying Targeting and Inappropriate Language: A Comparative Study](https://arxiv.org/abs/2505.21710)
*Barbarestani Baran,Maks Isa,Vossen Piek*

Main category: cs.CL

TL;DR: 该研究评估了ChatGPT在识别在线评论中的目标性和不当语言方面的有效性，发现其在检测不当内容方面表现良好，但在目标语言检测上存在波动。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体上用户生成内容的大量增加，AI在内容审核中的作用日益重要。本研究旨在评估ChatGPT在这一领域的表现。

Method: 研究通过将ChatGPT的表现与众包注释和专家评估进行比较，评估其准确性、检测范围和一致性。

Result: ChatGPT在检测不当内容方面表现良好，尤其是版本6通过迭代改进显著提高了准确性。但在目标语言检测上表现不稳定，假阳性率较高。

Conclusion: 研究表明，像ChatGPT这样的AI模型有潜力提升自动化内容审核系统，但也需要持续改进模型和上下文理解能力，以更好地支持自动化审核并减少网络有害行为。

Abstract: This study evaluates the effectiveness of ChatGPT, an advanced AI model for
natural language processing, in identifying targeting and inappropriate
language in online comments. With the increasing challenge of moderating vast
volumes of user-generated content on social network sites, the role of AI in
content moderation has gained prominence. We compared ChatGPT's performance
against crowd-sourced annotations and expert evaluations to assess its
accuracy, scope of detection, and consistency. Our findings highlight that
ChatGPT performs well in detecting inappropriate content, showing notable
improvements in accuracy through iterative refinements, particularly in Version
6. However, its performance in targeting language detection showed variability,
with higher false positive rates compared to expert judgments. This study
contributes to the field by demonstrating the potential of AI models like
ChatGPT to enhance automated content moderation systems while also identifying
areas for further improvement. The results underscore the importance of
continuous model refinement and contextual understanding to better support
automated moderation and mitigate harmful online behavior.

</details>


### [13] [Counterfactual Simulatability of LLM Explanations for Generation Tasks](https://arxiv.org/abs/2505.21740)
*Marvin Limpijankit,Yanda Chen,Melanie Subbiah,Nicholas Deas,Kathleen McKeown*

Main category: cs.CL

TL;DR: 论文探讨了LLMs解释行为的能力，提出反事实可模拟性评估框架，发现其在摘要任务中有效，但在医疗建议中仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs的输出对提示微小变化敏感，其行为解释能力在高风险场景中至关重要。反事实可模拟性作为一种评估方法，此前仅用于是/非问答任务，本文旨在将其扩展至生成任务。

Method: 提出通用框架，将反事实可模拟性评估方法扩展至生成任务（如新闻摘要和医疗建议），通过用户预测模型输出的准确性来衡量解释效果。

Result: LLMs的解释在新闻摘要任务中能帮助用户更好预测反事实输出，但在医疗建议任务中效果有限；评估方法更适用于技能型任务而非知识型任务。

Conclusion: 反事实可模拟性评估框架适用于生成任务，但需针对不同任务类型（如医疗领域）进一步优化解释能力。

Abstract: LLMs can be unpredictable, as even slight alterations to the prompt can cause
the output to change in unexpected ways. Thus, the ability of models to
accurately explain their behavior is critical, especially in high-stakes
settings. One approach for evaluating explanations is counterfactual
simulatability, how well an explanation allows users to infer the model's
output on related counterfactuals. Counterfactual simulatability has been
previously studied for yes/no question answering tasks. We provide a general
framework for extending this method to generation tasks, using news
summarization and medical suggestion as example use cases. We find that while
LLM explanations do enable users to better predict LLM outputs on
counterfactuals in the summarization setting, there is significant room for
improvement for medical suggestion. Furthermore, our results suggest that the
evaluation for counterfactual simulatability may be more appropriate for
skill-based tasks as opposed to knowledge-based tasks.

</details>


### [14] [BehaviorSFT: Behavioral Token Conditioning for Clinical Agents Across the Proactivity Spectrum](https://arxiv.org/abs/2505.21757)
*Yubin Kim,Zhiyuan Hu,Hyewon Jeong,Eugene Park,Shuyue Stella Li,Chanwoo Park,Shiyun Xiong,MingYu Lu,Hyeonhoon Lee,Xin Liu,Daniel McDuff,Cynthia Breazeal,Samir Tulebaev,Hae Won Park*

Main category: cs.CL

TL;DR: 论文提出BehaviorBench评估临床LLM行为，并开发BehaviorSFT训练策略以提升模型主动性与平衡性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在临床场景中反应性任务表现良好，但主动干预能力不足（如未提示的风险识别）。需系统性评估和改进模型行为谱系。

Method: 构建BehaviorBench数据集评估临床行为谱；提出BehaviorSFT训练策略，通过行为令牌显式调控模型在反应性与主动性间的动态选择。

Result: BehaviorSFT使Qwen2.5-7B-Ins模型整体Macro F1达97.3%，主动任务提升1.5%。临床盲评证实其平衡了适时建议与克制干预。

Conclusion: 显式行为条件化训练能有效提升LLM临床代理的主动性与行为合理性，优于标准微调或指令控制方法。

Abstract: Large Language Models (LLMs) as clinical agents require careful behavioral
adaptation. While adept at reactive tasks (e.g., diagnosis reasoning), LLMs
often struggle with proactive engagement, like unprompted identification of
critical missing information or risks. We introduce BehaviorBench, a
comprehensive dataset to evaluate agent behaviors across a clinical assistance
spectrum, ranging from reactive query responses to proactive interventions
(e.g., clarifying ambiguities, flagging overlooked critical data). Our
BehaviorBench experiments reveal LLMs' inconsistent proactivity. To address
this, we propose BehaviorSFT, a novel training strategy using behavioral tokens
to explicitly condition LLMs for dynamic behavioral selection along this
spectrum. BehaviorSFT boosts performance, achieving up to 97.3% overall Macro
F1 on BehaviorBench and improving proactive task scores (e.g., from 95.0% to
96.5% for Qwen2.5-7B-Ins). Crucially, blind clinician evaluations confirmed
BehaviorSFT-trained agents exhibit more realistic clinical behavior, striking a
superior balance between helpful proactivity (e.g., timely, relevant
suggestions) and necessary restraint (e.g., avoiding over-intervention) versus
standard fine-tuning or explicit instructed agents.

</details>


### [15] [Calibrating LLM Confidence by Probing Perturbed Representation Stability](https://arxiv.org/abs/2505.21772)
*Reza Khanmohammadi,Erfan Miahi,Mehrsa Mardikoraem,Simerjot Kaur,Ivan Brugere,Charese H. Smiley,Kundan Thind,Mohammad M. Ghassemi*

Main category: cs.CL

TL;DR: CCPS通过扰动LLM内部表示稳定性来校准置信度，显著降低误差并提升多项指标。


<details>
  <summary>Details</summary>
Motivation: 大语言模型(LLM)的置信度校准问题影响其可靠性，需要更准确的置信度估计方法。

Method: CCPS方法：对最终隐藏状态施加对抗扰动，提取扰动响应特征，用轻量级分类器预测答案正确性。

Result: 在8B-32B参数LLM上测试，CCPS将预期校准误差降低55%，准确率提升5%，多项指标显著优于现有方法。

Conclusion: CCPS提供了一种高效、通用且更准确的LLM置信度估计方案，增强了模型可信度。

Abstract: Miscalibration in Large Language Models (LLMs) undermines their reliability,
highlighting the need for accurate confidence estimation. We introduce CCPS
(Calibrating LLM Confidence by Probing Perturbed Representation Stability), a
novel method analyzing internal representational stability in LLMs. CCPS
applies targeted adversarial perturbations to final hidden states, extracts
features reflecting the model's response to these perturbations, and uses a
lightweight classifier to predict answer correctness. CCPS was evaluated on
LLMs from 8B to 32B parameters (covering Llama, Qwen, and Mistral
architectures) using MMLU and MMLU-Pro benchmarks in both multiple-choice and
open-ended formats. Our results show that CCPS significantly outperforms
current approaches. Across four LLMs and three MMLU variants, CCPS reduces
Expected Calibration Error by approximately 55% and Brier score by 21%, while
increasing accuracy by 5 percentage points, Area Under the Precision-Recall
Curve by 4 percentage points, and Area Under the Receiver Operating
Characteristic Curve by 6 percentage points, all relative to the strongest
prior method. CCPS delivers an efficient, broadly applicable, and more accurate
solution for estimating LLM confidence, thereby improving their
trustworthiness.

</details>


### [16] [GMU Systems for the IWSLT 2025 Low-Resource Speech Translation Shared Task](https://arxiv.org/abs/2505.21781)
*Chutong Meng,Antonios Anastasopoulos*

Main category: cs.CL

TL;DR: GMU团队针对IWSLT 2025低资源语音翻译任务，基于SeamlessM4T-v2模型进行了ASR、MT和端到端语音翻译的微调，并探索了多种训练方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在提升低资源语言对的语音翻译性能，探索端到端语音翻译的不同微调策略及其效果。

Method: 使用SeamlessM4T-v2模型进行微调，包括直接端到端微调、多任务训练，以及利用微调后的ASR和MT模型参数初始化。

Result: 直接端到端微调效果最佳；使用微调后的ASR编码器初始化可提升未训练语言的性能；多任务训练略有帮助。

Conclusion: 直接端到端微调是有效的低资源语音翻译方法，结合ASR编码器初始化和多任务训练可进一步提升性能。

Abstract: This paper describes the GMU systems for the IWSLT 2025 low-resource speech
translation shared task. We trained systems for all language pairs, except for
Levantine Arabic. We fine-tuned SeamlessM4T-v2 for automatic speech recognition
(ASR), machine translation (MT), and end-to-end speech translation (E2E ST).
The ASR and MT models are also used to form cascaded ST systems. Additionally,
we explored various training paradigms for E2E ST fine-tuning, including direct
E2E fine-tuning, multi-task training, and parameter initialization using
components from fine-tuned ASR and/or MT models. Our results show that (1)
direct E2E fine-tuning yields strong results; (2) initializing with a
fine-tuned ASR encoder improves ST performance on languages SeamlessM4T-v2 has
not been trained on; (3) multi-task training can be slightly helpful.

</details>


### [17] [VeriTrail: Closed-Domain Hallucination Detection with Traceability](https://arxiv.org/abs/2505.21786)
*Dasha Metropolitansky,Jonathan Larson*

Main category: cs.CL

TL;DR: VeriTrail是一种新方法，旨在检测和追踪多步生成过程中的闭域幻觉，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 语言模型即使在遵循源材料的情况下，仍会产生未经证实的内容（闭域幻觉）。多步生成过程（MGS）比单步生成过程（SGS）更容易产生幻觉，因此需要检测最终输出中的幻觉，并追踪其来源和中间输出的忠实性。

Method: 提出了VeriTrail方法，这是首个为MGS和SGS过程提供可追溯性的闭域幻觉检测方法，并引入了包含所有中间输出和人工标注的数据集。

Result: VeriTrail在两个数据集上均优于基线方法。

Conclusion: VeriTrail不仅能有效检测闭域幻觉，还能追踪幻觉的来源和中间输出的忠实性，为多步生成过程提供了更全面的解决方案。

Abstract: Even when instructed to adhere to source material, Language Models often
generate unsubstantiated content - a phenomenon known as "closed-domain
hallucination." This risk is amplified in processes with multiple generative
steps (MGS), compared to processes with a single generative step (SGS).
However, due to the greater complexity of MGS processes, we argue that
detecting hallucinations in their final outputs is necessary but not
sufficient: it is equally important to trace where hallucinated content was
likely introduced and how faithful content may have been derived from the
source through intermediate outputs. To address this need, we present
VeriTrail, the first closed-domain hallucination detection method designed to
provide traceability for both MGS and SGS processes. We also introduce the
first datasets to include all intermediate outputs as well as human annotations
of final outputs' faithfulness for their respective MGS processes. We
demonstrate that VeriTrail outperforms baseline methods on both datasets.

</details>


### [18] [Revisiting Common Assumptions about Arabic Dialects in NLP](https://arxiv.org/abs/2505.21816)
*Amr Keleg,Sharon Goldwater,Walid Magdy*

Main category: cs.CL

TL;DR: 该论文质疑了阿拉伯语方言NLP任务中的常见假设，并通过多标签数据集分析证明这些假设过于简化且不准确。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语方言多样性高，但NLP领域广泛采用的假设（如方言可按地区明确分组）缺乏定量验证，可能阻碍任务进展。

Method: 扩展并分析一个多标签数据集，由方言使用者手动评估11个国家级别方言中每个句子的有效性。

Result: 分析表明，四种常见假设过度简化现实，部分假设并不总是准确。

Conclusion: 现有假设可能限制了阿拉伯语NLP任务的进一步发展，需更精确的方言建模方法。

Abstract: Arabic has diverse dialects, where one dialect can be substantially different
from the others. In the NLP literature, some assumptions about these dialects
are widely adopted (e.g., ``Arabic dialects can be grouped into distinguishable
regional dialects") and are manifested in different computational tasks such as
Arabic Dialect Identification (ADI). However, these assumptions are not
quantitatively verified. We identify four of these assumptions and examine them
by extending and analyzing a multi-label dataset, where the validity of each
sentence in 11 different country-level dialects is manually assessed by
speakers of these dialects. Our analysis indicates that the four assumptions
oversimplify reality, and some of them are not always accurate. This in turn
might be hindering further progress in different Arabic NLP tasks.

</details>


### [19] [Representative Language Generation](https://arxiv.org/abs/2505.21819)
*Charlotte Peale,Vinod Raman,Omer Reingold*

Main category: cs.CL

TL;DR: 该论文提出'代表性生成'概念，扩展生成模型理论框架以解决多样性和偏见问题，引入'群体闭包维度'作为关键组合量，分析无限假设类下的信息论与计算可行性。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型存在多样性和偏见问题，需建立理论框架确保输出能按比例代表训练数据中的不同群体。

Method: 扩展Kleinberg等人的生成理论，提出代表性均匀/非均匀生成，定义'群体闭包维度'，分析无限假设类下的信息论与计算限制。

Result: 证明在特定条件下可实现对无限假设类的代表性生成，但仅通过成员查询无法保证可计算性，与标准生成理论形成对比。

Conclusion: 为开发更具多样性和代表性的生成模型奠定了理论基础，揭示了计算实现的局限性。

Abstract: We introduce "representative generation," extending the theoretical framework
for generation proposed by Kleinberg et al. (2024) and formalized by Li et al.
(2024), to additionally address diversity and bias concerns in generative
models. Our notion requires outputs of a generative model to proportionally
represent groups of interest from the training data. We characterize
representative uniform and non-uniform generation, introducing the "group
closure dimension" as a key combinatorial quantity. For representative
generation in the limit, we analyze both information-theoretic and
computational aspects, demonstrating feasibility for countably infinite
hypothesis classes and collections of groups under certain conditions, but
proving a negative result for computability using only membership queries. This
contrasts with Kleinberg et al.'s (2024) positive results for standard
generation in the limit. Our findings provide a rigorous foundation for
developing more diverse and representative generative models.

</details>


### [20] [Principled Content Selection to Generate Diverse and Personalized Multi-Document Summaries](https://arxiv.org/abs/2505.21859)
*Vishakh Padmakumar,Zichao Wang,David Arbour,Jennifer Healey*

Main category: cs.CL

TL;DR: 该论文提出了一种三步法改进多文档摘要的源覆盖率，通过关键点提取、多样性选择和重写步骤，结合用户意图生成个性化摘要。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理长上下文时存在注意力不均的问题（'lost in the middle'现象），导致多文档摘要中源材料覆盖率不足。本文旨在通过结构化内容选择提升覆盖率。

Method: 方法分为三步：(1) 将文档集简化为原子关键点；(2) 使用行列式点过程（DPP）选择多样性关键点；(3) 重写生成最终摘要。结合提示步骤与原则性技术优化内容选择。

Result: 在DiverseSumm基准测试中，该方法显著提高了不同LLMs的源覆盖率。通过将用户意图融入DPP核，还能生成兼顾相关性与覆盖率的个性化摘要。

Conclusion: 结构化内容选择策略有效解决了LLMs在多文档摘要中的覆盖率问题，结合用户意图可进一步实现个性化摘要生成。

Abstract: While large language models (LLMs) are increasingly capable of handling
longer contexts, recent work has demonstrated that they exhibit the "lost in
the middle" phenomenon (Liu et al., 2024) of unevenly attending to different
parts of the provided context. This hinders their ability to cover diverse
source material in multi-document summarization, as noted in the DiverseSumm
benchmark (Huang et al., 2024). In this work, we contend that principled
content selection is a simple way to increase source coverage on this task. As
opposed to prompting an LLM to perform the summarization in a single step, we
explicitly divide the task into three steps -- (1) reducing document
collections to atomic key points, (2) using determinantal point processes (DPP)
to perform select key points that prioritize diverse content, and (3) rewriting
to the final summary. By combining prompting steps, for extraction and
rewriting, with principled techniques, for content selection, we consistently
improve source coverage on the DiverseSumm benchmark across various LLMs.
Finally, we also show that by incorporating relevance to a provided user intent
into the DPP kernel, we can generate personalized summaries that cover relevant
source information while retaining coverage.

</details>


### [21] [Evaluating the Retrieval Robustness of Large Language Models](https://arxiv.org/abs/2505.21870)
*Shuyang Cao,Karthik Radhakrishnan,David Rosenberg,Steven Lu,Pengxiang Cheng,Lu Wang,Shiyue Zhang*

Main category: cs.CL

TL;DR: 论文评估了检索增强生成（RAG）在实际应用中的鲁棒性，发现虽然RAG通常能提升大语言模型（LLM）处理知识密集型任务的能力，但检索不完美和模型利用检索内容的能力有限可能导致性能下降。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨RAG是否总是优于非RAG方法，检索文档的数量和顺序是否影响模型性能，以及LLM在实际RAG设置中的鲁棒性。

Method: 方法包括建立一个包含1500个开放域问题的基准数据集，每个问题配有从维基百科检索的文档，并引入三个鲁棒性指标对应三个研究问题。实验涉及11种LLM和3种提示策略。

Result: 结果显示，所有测试的LLM都表现出较高的检索鲁棒性，但不同程度的鲁棒性不足限制了它们充分利用RAG的优势。

Conclusion: 结论指出，尽管LLM在RAG设置中表现出较高的鲁棒性，但检索不完美和模型利用能力的限制仍然阻碍了RAG潜力的完全发挥。

Abstract: Retrieval-augmented generation (RAG) generally enhances large language
models' (LLMs) ability to solve knowledge-intensive tasks. But RAG may also
lead to performance degradation due to imperfect retrieval and the model's
limited ability to leverage retrieved content. In this work, we evaluate the
robustness of LLMs in practical RAG setups (henceforth retrieval robustness).
We focus on three research questions: (1) whether RAG is always better than
non-RAG; (2) whether more retrieved documents always lead to better
performance; (3) and whether document orders impact results. To facilitate this
study, we establish a benchmark of 1500 open-domain questions, each with
retrieved documents from Wikipedia. We introduce three robustness metrics, each
corresponds to one research question. Our comprehensive experiments, involving
11 LLMs and 3 prompting strategies, reveal that all of these LLMs exhibit
surprisingly high retrieval robustness; nonetheless, different degrees of
imperfect robustness hinders them from fully utilizing the benefits of RAG.

</details>


### [22] [EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse](https://arxiv.org/abs/2505.21889)
*Tianyu Guo,Hande Dong,Yichong Leng,Feng Liu,Cheater Lin,Nong Xiao,Xianwei Zhang*

Main category: cs.CL

TL;DR: 论文提出EFIM，一种改进的提示格式，通过优化KV缓存重用和引入片段分词训练方法，显著提升LLM在填充任务中的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在填充任务中，由于提示格式的结构问题，KV缓存重用效率低下，导致计算资源浪费和性能瓶颈。

Method: 提出EFIM改进提示格式，并引入片段分词训练方法，解决KV缓存无效化和子词生成问题。

Result: 实验表明，EFIM能将延迟降低52%，吞吐量提升98%，同时保持原有填充能力。

Conclusion: EFIM通过优化提示格式和训练方法，显著提升了LLM在填充任务中的效率，为多轮交互服务提供了有效解决方案。

Abstract: Large language models (LLMs) are often used for infilling tasks, which
involve predicting or generating missing information in a given text. These
tasks typically require multiple interactions with similar context. To reduce
the computation of repeated historical tokens, cross-request key-value (KV)
cache reuse, a technique that stores and reuses intermediate computations, has
become a crucial method in multi-round interactive services. However, in
infilling tasks, the KV cache reuse is often hindered by the structure of the
prompt format, which typically consists of a prefix and suffix relative to the
insertion point. Specifically, the KV cache of the prefix or suffix part is
frequently invalidated as the other part (suffix or prefix) is incrementally
generated. To address the issue, we propose EFIM, a transformed prompt format
of FIM to unleash the performance potential of KV cache reuse. Although the
transformed prompt can solve the inefficiency, it exposes subtoken generation
problems in current LLMs, where they have difficulty generating partial words
accurately. Therefore, we introduce a fragment tokenization training method
which splits text into multiple fragments before tokenization during data
processing. Experiments on two representative LLMs show that LLM serving with
EFIM can lower the latency by 52% and improve the throughput by 98% while
maintaining the original infilling capability.EFIM's source code is publicly
available at https://github.com/gty111/EFIM.

</details>


### [23] [Co-Saving: Resource Aware Multi-Agent Collaboration for Software Development](https://arxiv.org/abs/2505.21898)
*Rennai Qiu,Chen Qian,Ran Li,Yufan Dang,Weize Chen,Cheng Yang,Yingli Zhang,Ye Tian,Xuantang Xiong,Lei Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 提出了一种资源感知的多智能体系统Co-Saving，通过利用历史成功轨迹中的“捷径”指令，显著减少了令牌消耗并提高了代码质量。


<details>
  <summary>Details</summary>
Motivation: 当前的多智能体系统在处理复杂任务时存在资源浪费和执行效率低下的问题，需要一种更高效的协作机制来优化资源使用和提升任务解决质量。

Method: 引入“捷径”机制，通过学习历史成功轨迹中的指令转换，绕过冗余的推理智能体，加速集体问题解决过程。

Result: 在软件开发任务中，相比现有方法ChatDev，令牌使用量平均减少50.85%，代码质量提升10.06%。

Conclusion: Co-Saving系统通过资源感知和捷径机制，显著提升了多智能体系统的效率和解决方案质量。

Abstract: Recent advancements in Large Language Models (LLMs) and autonomous agents
have demonstrated remarkable capabilities across various domains. However,
standalone agents frequently encounter limitations when handling complex tasks
that demand extensive interactions and substantial computational resources.
Although Multi-Agent Systems (MAS) alleviate some of these limitations through
collaborative mechanisms like task decomposition, iterative communication, and
role specialization, they typically remain resource-unaware, incurring
significant inefficiencies due to high token consumption and excessive
execution time. To address these limitations, we propose a resource-aware
multi-agent system -- Co-Saving (meaning that multiple agents collaboratively
engage in resource-saving activities), which leverages experiential knowledge
to enhance operational efficiency and solution quality. Our key innovation is
the introduction of "shortcuts" -- instructional transitions learned from
historically successful trajectories -- which allows to bypass redundant
reasoning agents and expedite the collective problem-solving process.
Experiments for software development tasks demonstrate significant advantages
over existing methods. Specifically, compared to the state-of-the-art MAS
ChatDev, our method achieves an average reduction of 50.85% in token usage, and
improves the overall code quality by 10.06%.

</details>


### [24] [Beyond Completion: A Foundation Model for General Knowledge Graph Reasoning](https://arxiv.org/abs/2505.21926)
*Yin Hua,Zhiqiang Liu,Mingyang Chen,Zheng Fang,Chi Man Wong,Lingxiao Li,Chi Man Vong,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: MERRY模型通过结合知识图谱的结构与文本信息，提出多视角条件消息传递编码架构和动态残差融合模块，在28个数据集上优于现有基线，展现出强大的推理和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱基础模型主要关注结构信息，局限于图谱内任务（如知识图谱补全），难以应对更具挑战性的图谱外任务（如问答）。本文旨在开发一个通用的知识图谱推理基础模型，充分利用图谱的结构和文本信息。

Method: 提出MERRY模型，采用多视角条件消息传递（CMP）编码架构融合文本与结构模态，引入动态残差融合模块选择性保留文本信息，并设计灵活的边缘评分机制适应下游任务。

Result: 在28个数据集上的综合评估表明，MERRY在多数场景下优于现有基线，尤其在知识图谱问答（KGQA）等图谱外任务中展现出卓越的泛化能力。

Conclusion: MERRY通过有效整合知识图谱的双模态信息，显著提升了图谱内外任务的推理性能，为通用知识图谱推理模型的开发提供了新方向。

Abstract: In natural language processing (NLP) and computer vision (CV), the successful
application of foundation models across diverse tasks has demonstrated their
remarkable potential. However, despite the rich structural and textual
information embedded in knowledge graphs (KGs), existing research of foundation
model for KG has primarily focused on their structural aspects, with most
efforts restricted to in-KG tasks (e.g., knowledge graph completion, KGC). This
limitation has hindered progress in addressing more challenging out-of-KG
tasks. In this paper, we introduce MERRY, a foundation model for general
knowledge graph reasoning, and investigate its performance across two task
categories: in-KG reasoning tasks (e.g., KGC) and out-of-KG tasks (e.g., KG
question answering, KGQA). We not only utilize the structural information, but
also the textual information in KGs. Specifically, we propose a
multi-perspective Conditional Message Passing (CMP) encoding architecture to
bridge the gap between textual and structural modalities, enabling their
seamless integration. Additionally, we introduce a dynamic residual fusion
module to selectively retain relevant textual information and a flexible edge
scoring mechanism to adapt to diverse downstream tasks. Comprehensive
evaluations on 28 datasets demonstrate that MERRY outperforms existing
baselines in most scenarios, showcasing strong reasoning capabilities within
KGs and excellent generalization to out-of-KG tasks such as KGQA.

</details>


### [25] [RedTeamCUA: Realistic Adversarial Testing of Computer-Use Agents in Hybrid Web-OS Environments](https://arxiv.org/abs/2505.21936)
*Zeyi Liao,Jaylen Jones,Linxi Jiang,Eric Fosler-Lussier,Yu Su,Zhiqiang Lin,Huan Sun*

Main category: cs.CL

TL;DR: 该论文提出了RedTeamCUA框架，用于评估计算机使用代理（CUAs）在混合Web-OS环境中的间接提示注入漏洞，发现现有CUAs存在显著安全风险。


<details>
  <summary>Details</summary>
Motivation: 当前对CUAs间接提示注入威胁的评估缺乏真实且受控的环境，且忽略了混合Web-OS攻击场景。

Method: 提出RedTeamCUA框架，结合基于VM的OS环境和基于Docker的Web平台，构建混合沙盒，并开发包含864个示例的RTC-Bench基准测试。

Result: 测试显示，Claude 3.7 Sonnet的ASR达42.9%，最安全的Operator仍有7.6%的ASR。Claude 4 Opus的ASR高达48%，表明高级CUAs仍面临实际风险。

Conclusion: RedTeamCUA为系统分析CUA漏洞提供了关键框架，强调了在现实部署前加强防御间接提示注入的紧迫性。

Abstract: Computer-use agents (CUAs) promise to automate complex tasks across operating
systems (OS) and the web, but remain vulnerable to indirect prompt injection.
Current evaluations of this threat either lack support realistic but controlled
environments or ignore hybrid web-OS attack scenarios involving both
interfaces. To address this, we propose RedTeamCUA, an adversarial testing
framework featuring a novel hybrid sandbox that integrates a VM-based OS
environment with Docker-based web platforms. Our sandbox supports key features
tailored for red teaming, such as flexible adversarial scenario configuration,
and a setting that decouples adversarial evaluation from navigational
limitations of CUAs by initializing tests directly at the point of an
adversarial injection. Using RedTeamCUA, we develop RTC-Bench, a comprehensive
benchmark with 864 examples that investigate realistic, hybrid web-OS attack
scenarios and fundamental security vulnerabilities. Benchmarking current
frontier CUAs identifies significant vulnerabilities: Claude 3.7 Sonnet | CUA
demonstrates an ASR of 42.9%, while Operator, the most secure CUA evaluated,
still exhibits an ASR of 7.6%. Notably, CUAs often attempt to execute
adversarial tasks with an Attempt Rate as high as 92.5%, although failing to
complete them due to capability limitations. Nevertheless, we observe
concerning ASRs of up to 50% in realistic end-to-end settings, with the
recently released frontier Claude 4 Opus | CUA showing an alarming ASR of 48%,
demonstrating that indirect prompt injection presents tangible risks for even
advanced CUAs despite their capabilities and safeguards. Overall, RedTeamCUA
provides an essential framework for advancing realistic, controlled, and
systematic analysis of CUA vulnerabilities, highlighting the urgent need for
robust defenses to indirect prompt injection prior to real-world deployment.

</details>


### [26] [Graph-Assisted Culturally Adaptable Idiomatic Translation for Indic Languages](https://arxiv.org/abs/2505.21937)
*Pratik Rakesh Singh,Kritarth Prasad,Mohammadi Zaki,Pankaj Wasnik*

Main category: cs.CL

TL;DR: 提出IdiomCE方法，使用自适应图神经网络改进多词表达和习语的翻译质量。


<details>
  <summary>Details</summary>
Motivation: 多词表达和习语的翻译需要深入理解文化差异，传统静态知识图谱和提示方法难以捕捉复杂关系。

Method: 采用自适应图神经网络（GNN）学习习语间的复杂映射，泛化到训练中未见过的节点。

Result: 在多个习语翻译数据集上评估，显著提升了英语到印度语言的习语翻译质量。

Conclusion: IdiomCE方法在资源受限环境下也能提升翻译质量，适用于较小模型。

Abstract: Translating multi-word expressions (MWEs) and idioms requires a deep
understanding of the cultural nuances of both the source and target languages.
This challenge is further amplified by the one-to-many nature of idiomatic
translations, where a single source idiom can have multiple target-language
equivalents depending on cultural references and contextual variations.
Traditional static knowledge graphs (KGs) and prompt-based approaches struggle
to capture these complex relationships, often leading to suboptimal
translations. To address this, we propose IdiomCE, an adaptive graph neural
network (GNN) based methodology that learns intricate mappings between
idiomatic expressions, effectively generalizing to both seen and unseen nodes
during training. Our proposed method enhances translation quality even in
resource-constrained settings, facilitating improved idiomatic translation in
smaller models. We evaluate our approach on multiple idiomatic translation
datasets using reference-less metrics, demonstrating significant improvements
in translating idioms from English to various Indian languages.

</details>


### [27] [RISE: Reasoning Enhancement via Iterative Self-Exploration in Multi-hop Question Answering](https://arxiv.org/abs/2505.21940)
*Bolei He,Xinran He,Mengke Chen,Xianwei Xue,Ying Zhu,Zhenhua Ling*

Main category: cs.CL

TL;DR: 论文提出RISE框架，通过迭代自探索增强大语言模型在复杂多跳问答任务中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在复杂推理任务（如多跳问答）中表现不佳，主要因证据整合和逻辑依赖处理存在缺陷。传统检索增强生成方法面临噪声过滤和证据检索不完整的挑战。

Method: RISE框架包含三步骤：问题分解、检索-阅读、自我批判，通过迭代自探索优化推理路径，提升证据整合与逻辑一致性。

Result: 在多个多跳问答基准测试中，RISE显著提高了推理准确性和任务性能。

Conclusion: RISE通过持续自我改进有效解决了多跳问答中的核心挑战，为复杂推理任务提供了新思路。

Abstract: Large Language Models (LLMs) excel in many areas but continue to face
challenges with complex reasoning tasks, such as Multi-Hop Question Answering
(MHQA). MHQA requires integrating evidence from diverse sources while managing
intricate logical dependencies, often leads to errors in reasoning.
Retrieval-Augmented Generation (RAG), widely employed in MHQA tasks, faces
challenges in effectively filtering noisy data and retrieving all necessary
evidence, thereby limiting its effectiveness in addressing MHQA challenges. To
address these challenges, we propose RISE:Reasoning Enhancement via Iterative
Self-Exploration, a novel framework designed to enhance models' reasoning
capability through iterative self-exploration. Specifically, RISE involves
three key steps in addressing MHQA tasks: question decomposition,
retrieve-then-read, and self-critique. By leveraging continuous
self-exploration, RISE identifies accurate reasoning paths, iteratively
self-improving the model's capability to integrate evidence, maintain logical
consistency, and enhance performance in MHQA tasks. Extensive experiments on
multiple MHQA benchmarks demonstrate that RISE significantly improves reasoning
accuracy and task performance.

</details>


### [28] [Offset Unlearning for Large Language Models](https://arxiv.org/abs/2404.11045)
*James Y. Huang,Wenxuan Zhou,Fei Wang,Fred Morstatter,Sheng Zhang,Hoifung Poon,Muhao Chen*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Despite the strong capabilities of Large Language Models (LLMs) to acquire
knowledge from their training corpora, the memorization of sensitive
information in the corpora such as copyrighted, biased, and private content has
led to ethical and legal concerns. In response to these challenges, unlearning
has emerged as a potential remedy for LLMs affected by problematic training
data. However, previous unlearning techniques are either not applicable to
black-box LLMs due to required access to model internal weights, or violate
data protection principles by retaining sensitive data for inference-time
correction. We propose {\delta}-Unlearning, an offset unlearning framework for
black-box LLMs. Instead of tuning the black-box LLM itself, {\delta}-Unlearning
learns the logit offset needed for unlearning by contrasting the logits from a
pair of smaller models. Experiments demonstrate that {\delta}- Unlearning can
effectively unlearn target data while maintaining similar or even stronger
performance on general out-of-forget-scope tasks. {\delta}-Unlearning also
effectively incorporates different unlearning algorithms, making our approach a
versatile solution to adapting various existing unlearning algorithms to
black-box LLMs.

</details>


### [29] [Test-Time Scaling with Repeated Sampling Improves Multilingual Text Generation](https://arxiv.org/abs/2505.21941)
*Ashim Gupta,Vivek Srikumar*

Main category: cs.CL

TL;DR: 论文评估了重复采样在多种语言生成任务中的效果，发现其能显著提升质量，尤其在需要推理的任务中奖励验证器效果更佳。


<details>
  <summary>Details</summary>
Motivation: 探索重复采样在多种语言生成任务中的有效性，尤其是在推理任务中的应用潜力。

Method: 使用基于困惑度和奖励的验证器，在Aya Evaluation Suite和m-ArenaHard两个多语言基准上进行评估。

Result: 结果显示质量持续提升，部分情况下增益超过35%；奖励验证器在需要推理的任务中表现更优。

Conclusion: 重复采样对多语言文本生成具有广泛效用，选择合适的验证器对任务至关重要。

Abstract: Inference-time scaling via repeated sampling has shown promise in reasoning
tasks, but its effectiveness in multilingual generation remains underexplored.
We evaluate this approach using perplexity- and reward-based verifiers on two
multilingual benchmarks: the Aya Evaluation Suite and m-ArenaHard. Our results
show consistent quality improvements, with gains exceeding 35% in some cases.
While perplexity-based scoring is effective for open-ended prompts, only
reward-based verifiers improve performance on tasks requiring reasoning (e.g.,
math, code). Our results demonstrate the broader utility of repeated sampling
for multilingual text generation and underscore the importance of selecting
right verifiers for the task.

</details>


### [30] [Resolving Knowledge Conflicts in Domain-specific Data Selection: A Case Study on Medical Instruction-tuning](https://arxiv.org/abs/2505.21958)
*Qihuang Zhong,Liang Ding,Fei Liao,Juhua Liu,Bo Du,Dacheng Tao*

Main category: cs.CL

TL;DR: 提出了一种名为KDS的知识感知数据选择框架，用于优化领域特定指令微调的数据选择，以减少知识冲突并提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前的数据选择方法在领域特定指令微调中表现不佳，主要原因是忽视了预训练知识与指令数据上下文知识之间的知识冲突问题。

Method: KDS框架通过两个知识感知指标（上下文-记忆知识对齐和内部记忆知识一致性）定量测量知识冲突，并筛选高质量多样化数据。

Result: 在医学领域的实验中，KDS显著提升了模型性能，改善了泛化能力并缓解了幻觉问题。

Conclusion: KDS是一种简单有效的方法，能够显著提升领域特定指令微调的效果，并在实验中证明了其优越性。

Abstract: Domain-specific instruction-tuning has become the defacto standard for
improving the performance of large language models (LLMs) in specialized
applications, e.g., medical question answering. Since the instruction-tuning
dataset might contain redundant or low-quality data, data selection (DS) is
usually required to maximize the data efficiency. Despite the successes in the
general domain, current DS methods often struggle to select the desired data
for domain-specific instruction-tuning. One of the main reasons is that they
neglect the impact of knowledge conflicts, i.e., the discrepancy between LLMs'
pretrained knowledge and context knowledge of instruction data, which could
damage LLMs' prior abilities and lead to hallucination. To this end, we propose
a simple-yet-effective Knowledge-aware Data Selection (namely KDS) framework to
select the domain-specific instruction-tuning data that meets LLMs' actual
needs. The core of KDS is to leverage two knowledge-aware metrics for
quantitatively measuring knowledge conflicts from two aspects: context-memory
knowledge alignment and intra-memory knowledge consistency. By filtering the
data with large knowledge conflicts and sampling the high-quality and diverse
data, KDS can effectively stimulate the LLMs' abilities and achieve better
domain-specific performance. Taking the medical domain as the testbed, we
conduct extensive experiments and empirically prove that KDS surpasses the
other baselines and brings significant and consistent performance gains among
all LLMs. More encouragingly, KDS effectively improves the model generalization
and alleviates the hallucination problem.

</details>


### [31] [LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents](https://arxiv.org/abs/2505.21963)
*Taro Yano,Yoichi Ishibashi,Masafumi Oyamada*

Main category: cs.CL

TL;DR: LaMDAgent框架通过LLM代理自动构建和优化后训练流程，提升任务性能并减少人工干预。


<details>
  <summary>Details</summary>
Motivation: 现有后训练技术（如SFT、偏好学习等）多为孤立研究，缺乏自动化构建完整流程的方法。人工设计或局部优化效率低下，亟需系统性解决方案。

Method: 提出LaMDAgent框架，利用LLM代理自主探索模型生成技术、数据集和超参数配置，通过任务反馈自动发现高效后训练流程。

Result: 实验显示LaMDAgent将工具使用准确率提升9.0%，同时保持指令跟随能力，并发现传统方法易忽略的有效策略。数据规模扩展可降低成本，但模型规模扩展带来新挑战。

Conclusion: LaMDAgent为自动化后训练流程优化提供了有效路径，其发现的策略具有实用价值，数据规模扩展是降低成本的关键。

Abstract: Large Language Models (LLMs) have demonstrated exceptional performance across
a wide range of tasks. To further tailor LLMs to specific domains or
applications, post-training techniques such as Supervised Fine-Tuning (SFT),
Preference Learning, and model merging are commonly employed. While each of
these methods has been extensively studied in isolation, the automated
construction of complete post-training pipelines remains an underexplored area.
Existing approaches typically rely on manual design or focus narrowly on
optimizing individual components, such as data ordering or merging strategies.
In this work, we introduce LaMDAgent (short for Language Model Developing
Agent), a novel framework that autonomously constructs and optimizes full
post-training pipelines through the use of LLM-based agents. LaMDAgent
systematically explores diverse model generation techniques, datasets, and
hyperparameter configurations, leveraging task-based feedback to discover
high-performing pipelines with minimal human intervention. Our experiments show
that LaMDAgent improves tool-use accuracy by 9.0 points while preserving
instruction-following capabilities. Moreover, it uncovers effective
post-training strategies that are often overlooked by conventional human-driven
exploration. We further analyze the impact of data and model size scaling to
reduce computational costs on the exploration, finding that model size scalings
introduces new challenges, whereas scaling data size enables cost-effective
pipeline discovery.

</details>


### [32] [Seeing the Threat: Vulnerabilities in Vision-Language Models to Adversarial Attack](https://arxiv.org/abs/2505.21967)
*Juan Ren,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 该论文系统分析了大型视觉语言模型(LVLMs)的安全漏洞，提出两阶段对抗攻击评估框架，并引入理想化安全行为规范。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在多模态任务中表现优异，但视觉输入的整合扩大了攻击面，暴露出新的安全漏洞。论文旨在探究传统对抗攻击如何绕过其安全机制。

Method: 提出两阶段评估框架：第一阶段区分指令不遵从、直接拒绝和成功攻击；第二阶段量化输出满足恶意意图的程度，并对拒绝行为分类。最后提出理想化安全行为规范。

Result: 建立了系统化的对抗攻击评估体系，揭示了LVLMs在面对有害提示时的响应模式，为安全对齐提供了量化标准。

Conclusion: 该研究为多模态系统的安全对齐提供了理论框架和评估方法，有助于提升LVLMs对抗恶意攻击的鲁棒性。

Abstract: Large Vision-Language Models (LVLMs) have shown remarkable capabilities
across a wide range of multimodal tasks. However, their integration of visual
inputs introduces expanded attack surfaces, thereby exposing them to novel
security vulnerabilities. In this work, we conduct a systematic
representational analysis to uncover why conventional adversarial attacks can
circumvent the safety mechanisms embedded in LVLMs. We further propose a novel
two stage evaluation framework for adversarial attacks on LVLMs. The first
stage differentiates among instruction non compliance, outright refusal, and
successful adversarial exploitation. The second stage quantifies the degree to
which the model's output fulfills the harmful intent of the adversarial prompt,
while categorizing refusal behavior into direct refusals, soft refusals, and
partial refusals that remain inadvertently helpful. Finally, we introduce a
normative schema that defines idealized model behavior when confronted with
harmful prompts, offering a principled target for safety alignment in
multimodal systems.

</details>


### [33] [Pearl: A Multimodal Culturally-Aware Arabic Instruction Dataset](https://arxiv.org/abs/2505.21979)
*Fakhraddin Alwajih,Samar Mohamed Magdy,Abdellah El Mekki,Omer Nacar,Youssef Nafea,Safaa Taher Abdelfadil,Abdulfattah Mohammed Yahya,Hamzah Luqman,Nada Almarwani,Samah Aloufi,Baraah Qawasmeh,Houdaifa Atou,Serry Sibaee,Hamzah A. Alsayadi,Walid Al-Dhabyani,Maged S. Al-shaibani,Aya El aatar,Nour Qandos,Rahaf Alhamouri,Samar Ahmad,Razan Khassib,Lina Hamad,Mohammed Anwar AL-Ghrawi,Fatimah Alshamari,Cheikh Malainine,Doaa Qawasmeh,Aminetou Yacoub,Tfeil moilid,Ruwa AbuHweidi,Ahmed Aboeitta,Vatimetou Mohamed Lemin,Reem Abdel-Salam,Ahlam Bashiti,Adel Ammar,Aisha Alansari,Ahmed Ashraf,Nora Alturayeif,Sara Shatnawi,Alcides Alcoba Inciarte,AbdelRahim A. Elmadany,Mohamedou cheikh tourad,Ismail Berrada,Mustafa Jarrar,Shady Shehata,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 该论文介绍了Pearl，一个专为文化理解设计的大规模阿拉伯多模态数据集和基准，旨在解决主流大型视觉语言模型中的文化偏见问题。


<details>
  <summary>Details</summary>
Motivation: 主流大型视觉语言模型（LVLMs）存在固有的文化偏见，突显了对多样化多模态数据集的需求。

Method: 通过先进的代理工作流程和来自阿拉伯世界45位注释者的大量人工标注，构建了包含超过K个多模态示例的Pearl数据集，涵盖十个文化重要领域。

Result: 评估表明，以推理为中心的指令对齐相比传统的扩展方法显著提高了模型的文化基础。

Conclusion: Pearl为推进文化感知的多模态建模研究奠定了基础资源，所有数据集和基准均已公开。

Abstract: Mainstream large vision-language models (LVLMs) inherently encode cultural
biases, highlighting the need for diverse multimodal datasets. To address this
gap, we introduce Pearl, a large-scale Arabic multimodal dataset and benchmark
explicitly designed for cultural understanding. Constructed through advanced
agentic workflows and extensive human-in-the-loop annotations by 45 annotators
from across the Arab world, Pearl comprises over K multimodal examples spanning
ten culturally significant domains covering all Arab countries. We further
provide two robust evaluation benchmarks Pearl and Pearl-Lite along with a
specialized subset Pearl-X explicitly developed to assess nuanced cultural
variations. Comprehensive evaluations on state-of-the-art open and proprietary
LVLMs demonstrate that reasoning-centric instruction alignment substantially
improves models' cultural grounding compared to conventional scaling methods.
Pearl establishes a foundational resource for advancing culturally-informed
multimodal modeling research. All datasets and benchmarks are publicly
available.

</details>


### [34] [Leveraging Interview-Informed LLMs to Model Survey Responses: Comparative Insights from AI-Generated and Human Data](https://arxiv.org/abs/2505.21997)
*Jihong Zhang,Xinya Liang,Anqi Deng,Nicole Bonge,Lin Tan,Ling Zhang,Nicole Zarrett*

Main category: cs.CL

TL;DR: 研究探讨了利用大型语言模型（LLMs）结合访谈数据生成合成调查响应的可行性，发现LLMs能捕捉总体响应模式但变异性较低，提示设计和访谈内容对结果影响显著。


<details>
  <summary>Details</summary>
Motivation: 混合方法研究在整合定量和定性数据时面临结构对齐的挑战，特别是在测量特征和个体响应模式方面。大型语言模型（LLMs）的进展为通过定性数据生成合成调查响应提供了潜在解决方案。

Method: 研究使用《运动行为调节问卷》（BREQ）和课后项目工作人员的访谈作为案例，探讨LLMs在访谈指导下是否能可靠预测人类调查响应。

Result: 结果表明，LLMs能捕捉总体响应模式但变异性低于人类。访谈数据提高了部分模型（如Claude、GPT）的响应多样性，精心设计的提示和低温设置增强了LLMs与人类响应的对齐。人口统计信息对对齐准确性的影响较小。

Conclusion: 访谈指导的LLMs有潜力桥接定性和定量方法，但在响应变异性、情感解释和心理测量保真度方面存在局限。未来研究应优化提示设计、探索偏见缓解并调整模型设置以提升LLMs生成数据在社会科学研究中的有效性。

Abstract: Mixed methods research integrates quantitative and qualitative data but faces
challenges in aligning their distinct structures, particularly in examining
measurement characteristics and individual response patterns. Advances in large
language models (LLMs) offer promising solutions by generating synthetic survey
responses informed by qualitative data. This study investigates whether LLMs,
guided by personal interviews, can reliably predict human survey responses,
using the Behavioral Regulations in Exercise Questionnaire (BREQ) and
interviews from after-school program staff as a case study. Results indicate
that LLMs capture overall response patterns but exhibit lower variability than
humans. Incorporating interview data improves response diversity for some
models (e.g., Claude, GPT), while well-crafted prompts and low-temperature
settings enhance alignment between LLM and human responses. Demographic
information had less impact than interview content on alignment accuracy. These
findings underscore the potential of interview-informed LLMs to bridge
qualitative and quantitative methodologies while revealing limitations in
response variability, emotional interpretation, and psychometric fidelity.
Future research should refine prompt design, explore bias mitigation, and
optimize model settings to enhance the validity of LLM-generated survey data in
social science research.

</details>


### [35] [Found in Translation: Measuring Multilingual LLM Consistency as Simple as Translate then Evaluate](https://arxiv.org/abs/2505.21999)
*Ashim Gupta,Maitrey Mehta,Zhichao Xu,Vivek Srikumar*

Main category: cs.CL

TL;DR: 论文提出评估大语言模型跨语言一致性的框架，发现主流模型在30种语言中存在明显不一致性。


<details>
  <summary>Details</summary>
Motivation: 现有评估多语言大语言模型的方法依赖昂贵标注数据，且开放生成任务评估困难。作者希望找到更高效的跨语言一致性评估方案。

Method: 采用'翻译-评估'策略，从信息准确性和情感共情两个维度构建评估框架。

Result: 实验显示主流模型在不同语系和文字间存在严重性能差异，暴露出多语言能力缺陷。

Conclusion: 需要多维度跨语言评估框架，作者邀请业界采用该框架进行未来多语言模型基准测试。

Abstract: Large language models (LLMs) provide detailed and impressive responses to
queries in English. However, are they really consistent at responding to the
same query in other languages? The popular way of evaluating for multilingual
performance of LLMs requires expensive-to-collect annotated datasets. Further,
evaluating for tasks like open-ended generation, where multiple correct answers
may exist, is nontrivial. Instead, we propose to evaluate the predictability of
model response across different languages. In this work, we propose a framework
to evaluate LLM's cross-lingual consistency based on a simple Translate then
Evaluate strategy. We instantiate this evaluation framework along two
dimensions of consistency: information and empathy. Our results reveal
pronounced inconsistencies in popular LLM responses across thirty languages,
with severe performance deficits in certain language families and scripts,
underscoring critical weaknesses in their multilingual capabilities. These
findings necessitate cross-lingual evaluations that are consistent along
multiple dimensions. We invite practitioners to use our framework for future
multilingual LLM benchmarking.

</details>


### [36] [Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal Assistance](https://arxiv.org/abs/2505.22003)
*Jatin Gupta,Akhil Sharma,Saransh Singhania,Ali Imam Abidi*

Main category: cs.CL

TL;DR: 本文介绍了Legal Assist AI，一个基于Transformer的模型，旨在通过大型语言模型为印度公民提供有效的法律帮助，解决法律信息获取不足的问题。


<details>
  <summary>Details</summary>
Motivation: 印度许多公民由于法律意识薄弱和相关法律信息获取困难，无法有效行使自己的法律权利，这促使了Legal Assist AI的开发。

Method: 该模型基于Transformer架构，通过微调印度法律领域的广泛数据集（如印度宪法、Bharatiya Nyaya Sanhita等），从精选数据库中检索相关法律信息并生成准确回答。

Result: Legal Assist AI在AIBE测试中得分60.08%，优于GPT-3.5 Turbo和Mistral 7B，在法律推理和准确性方面表现突出，且避免了幻觉问题。

Conclusion: Legal Assist AI展示了在实际法律场景中的应用潜力，未来计划通过扩展数据集和提升性能来覆盖更多多语言和具体案例查询。

Abstract: Pursuit of accessible legal assistance in India faces a critical gap, as many
citizens struggle to leverage their legal rights due to limited awareness and
access to relevant legal information. This paper introduces Legal Assist AI, a
transformer-based model designed to bridge this gap by offering effective legal
assistance through large language models (LLMs). The system retrieves relevant
legal information from a curated database and generates accurate responses,
enabling effective assistance for diverse users, including legal professionals,
scholars, and the general public. The model was fine-tuned on extensive
datasets from the Indian legal domain, including Indian Constitution, Bharatiya
Nyaya Sanhita (BNS), Bharatiya Nagarik Suraksha Sanhita (BNSS) and so forth,
providing a robust understanding of the complexities of Indian law. By
incorporating domain-specific legal datasets, the proposed model demonstrated
remarkable efficiency and specialization in legal Question-Answering. The model
was evaluated against state-of-the-art models such as GPT-3.5 Turbo and Mistral
7B, achieving a 60.08% score on the AIBE, outperforming its competitors in
legal reasoning and accuracy. Unlike other models, Legal Assist AI avoided
common issues such as hallucinations, making it highly reliable for practical
legal applications. It showcases the model's applicability in real-world legal
scenarios, with future iterations aiming to enhance performance and expand its
dataset to cover a broader range of multilingual and case-specific queries as
well.

</details>


### [37] [CoThink: Token-Efficient Reasoning via Instruct Models Guiding Reasoning Models](https://arxiv.org/abs/2505.22017)
*Siqi Fan,Peng Han,Shuo Shang,Yequan Wang,Aixin Sun*

Main category: cs.CL

TL;DR: 论文提出CoThink方法，通过指令模型生成大纲再由推理模型细化，动态调整推理深度，在保持准确率的同时减少22.3%的token生成。


<details>
  <summary>Details</summary>
Motivation: 研究发现，优化推理的大语言模型（LLMs）在处理简单问题时会产生冗余输出，导致token效率低下。主要原因是强化学习降低了前向推理的信息密度，以及反向思维链训练引入了不必要的验证步骤。

Method: 提出CoThink方法：先由指令模型生成高级解决方案大纲，再由推理模型细化解决方案，动态调整推理深度以适应不同难度的输入。

Result: 在GSM8K、MATH500和AIME24三个数据集上测试，CoThink平均减少22.3%的token生成，同时保持pass@1准确率仅下降0.42%。

Conclusion: CoThink有效解决了LLMs过度推理的问题，提高了推理效率，并观察到潜在的推理效率缩放规律。

Abstract: Large language models (LLMs) benefit from increased test-time compute, a
phenomenon known as test-time scaling. However, reasoning-optimized models
often overthink even simple problems, producing excessively verbose outputs and
leading to low token efficiency. By comparing these models with equally sized
instruct models, we identify two key causes of this verbosity: (1)
reinforcement learning reduces the information density of forward reasoning,
and (2) backward chain-of thought training encourages redundant and often
unnecessary verification steps. Since LLMs cannot assess the difficulty of a
given problem, they tend to apply the same cautious reasoning strategy across
all tasks, resulting in inefficient overthinking. To address this, we propose
CoThink, an embarrassingly simple pipeline: an instruct model first drafts a
high-level solution outline; a reasoning model then works out the solution. We
observe that CoThink enables dynamic adjustment of reasoning depth based on
input difficulty. Evaluated with three reasoning models DAPO, DeepSeek-R1, and
QwQ on three datasets GSM8K, MATH500, and AIME24, CoThink reduces total token
generation by 22.3% while maintaining pass@1 accuracy within a 0.42% margin on
average. With reference to the instruct model, we formally define reasoning
efficiency and observe a potential reasoning efficiency scaling law in LLMs.

</details>


### [38] [Improving Continual Pre-training Through Seamless Data Packing](https://arxiv.org/abs/2505.22018)
*Ruicheng Yin,Xuan Gao,Changze Lv,Xiaohua Wang,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.CL

TL;DR: 提出Seamless Packing（SP）数据打包策略，通过滑动窗口和装箱算法减少截断和填充，提升持续预训练效果。


<details>
  <summary>Details</summary>
Motivation: 传统数据打包方法在持续预训练中因截断和上下文不连贯影响模型性能，需改进数据工程方法。

Method: SP策略分两阶段：1) 滑动窗口同步重叠token保证连续性；2) 用FFD算法将短文本装箱以减少填充。

Result: 在99%的实验设置中超越基线方法，验证了SP的有效性。

Conclusion: SP通过优化数据打包显著提升模型性能，代码已开源。

Abstract: Continual pre-training has demonstrated significant potential in enhancing
model performance, particularly in domain-specific scenarios. The most common
approach for packing data before continual pre-training involves concatenating
input texts and splitting them into fixed-length sequences. While
straightforward and efficient, this method often leads to excessive truncation
and context discontinuity, which can hinder model performance. To address these
issues, we explore the potential of data engineering to enhance continual
pre-training, particularly its impact on model performance and efficiency. We
propose Seamless Packing (SP), a novel data packing strategy aimed at
preserving contextual information more effectively and enhancing model
performance. Our approach employs a sliding window technique in the first stage
that synchronizes overlapping tokens across consecutive sequences, ensuring
better continuity and contextual coherence. In the second stage, we adopt a
First-Fit-Decreasing algorithm to pack shorter texts into bins slightly larger
than the target sequence length, thereby minimizing padding and truncation.
Empirical evaluations across various model architectures and corpus domains
demonstrate the effectiveness of our method, outperforming baseline method in
99% of all settings. Code is available at
https://github.com/Infernus-WIND/Seamless-Packing.

</details>


### [39] [VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning](https://arxiv.org/abs/2505.22019)
*Qiuchen Wang,Ruixue Ding,Yu Zeng,Zehui Chen,Lin Chen,Shihang Wang,Pengjun Xie,Fei Huang,Feng Zhao*

Main category: cs.CL

TL;DR: 提出VRAG-RL框架，通过强化学习优化视觉语言模型在视觉丰富信息检索与推理中的表现，解决传统方法在视觉信息处理和推理激活上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统文本检索方法无法处理视觉信息，现有视觉检索方法因固定流程和模型能力激活不足导致推理效果不佳。强化学习被证明有助于模型推理，因此开发VRAG-RL框架以提升视觉丰富信息的复杂推理能力。

Method: VRAG-RL框架让视觉语言模型与搜索引擎交互，通过视觉感知令牌自主采样单轮或多轮推理轨迹，并基于这些样本持续优化。定义了针对视觉输入的动作空间（如裁剪和缩放），并设计结合查询改写和检索性能的奖励机制。

Result: VRAG-RL通过专门设计的强化学习策略优化视觉语言模型，使其在检索增强生成任务中表现更优，更贴近实际应用需求。

Conclusion: VRAG-RL框架有效解决了视觉信息检索与推理中的关键问题，通过强化学习提升了模型在复杂视觉场景下的性能，为实际应用提供了有力工具。

Abstract: Effectively retrieving, reasoning and understanding visually rich information
remains a challenge for RAG methods. Traditional text-based methods cannot
handle visual-related information. On the other hand, current vision-based RAG
approaches are often limited by fixed pipelines and frequently struggle to
reason effectively due to the insufficient activation of the fundamental
capabilities of models. As RL has been proven to be beneficial for model
reasoning, we introduce VRAG-RL, a novel RL framework tailored for complex
reasoning across visually rich information. With this framework, VLMs interact
with search engines, autonomously sampling single-turn or multi-turn reasoning
trajectories with the help of visual perception tokens and undergoing continual
optimization based on these samples. Our approach highlights key limitations of
RL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely
incorporate images into the context, leading to insufficient reasoning token
allocation and neglecting visual-specific perception; and (ii) When models
interact with search engines, their queries often fail to retrieve relevant
information due to the inability to articulate requirements, thereby leading to
suboptimal performance. To address these challenges, we define an action space
tailored for visually rich inputs, with actions including cropping and scaling,
allowing the model to gather information from a coarse-to-fine perspective.
Furthermore, to bridge the gap between users' original inquiries and the
retriever, we employ a simple yet effective reward that integrates query
rewriting and retrieval performance with a model-based reward. Our VRAG-RL
optimizes VLMs for RAG tasks using specially designed RL strategies, aligning
the model with real-world applications. The code is available at
\hyperlink{https://github.com/Alibaba-NLP/VRAG}{https://github.com/Alibaba-NLP/VRAG}.

</details>


### [40] [Jailbreak Distillation: Renewable Safety Benchmarking](https://arxiv.org/abs/2505.22037)
*Jingyu Zhang,Ahmed Elgohary,Xiawei Wang,A S M Iftekhar,Ahmed Magooda,Benjamin Van Durme,Daniel Khashabi,Kyle Jackson*

Main category: cs.CL

TL;DR: 提出Jailbreak Distillation框架，通过蒸馏越狱攻击构建高质量、易更新的安全基准，显著提升评估效果。


<details>
  <summary>Details</summary>
Motivation: 大模型快速部署于关键应用，亟需鲁棒的安全评估基准。现有方法存在公平性、可复现性及更新效率问题。

Method: 利用少量开发模型和现有越狱攻击生成候选提示池，通过算法筛选有效子集作为安全基准。

Result: 基准在13个未参与构建的多样模型上表现鲁棒，效果显著优于现有基准，同时保持高区分度与多样性。

Conclusion: 该框架为安全评估提供了高效、可持续且适应性强的一站式解决方案。

Abstract: Large language models (LLMs) are rapidly deployed in critical applications,
raising urgent needs for robust safety benchmarking. We propose Jailbreak
Distillation (JBDistill), a novel benchmark construction framework that
"distills" jailbreak attacks into high-quality and easily-updatable safety
benchmarks. JBDistill utilizes a small set of development models and existing
jailbreak attack algorithms to create a candidate prompt pool, then employs
prompt selection algorithms to identify an effective subset of prompts as
safety benchmarks. JBDistill addresses challenges in existing safety
evaluation: the use of consistent evaluation prompts across models ensures fair
comparisons and reproducibility. It requires minimal human effort to rerun the
JBDistill pipeline and produce updated benchmarks, alleviating concerns on
saturation and contamination. Extensive experiments demonstrate our benchmarks
generalize robustly to 13 diverse evaluation models held out from benchmark
construction, including proprietary, specialized, and newer-generation LLMs,
significantly outperforming existing safety benchmarks in effectiveness while
maintaining high separability and diversity. Our framework thus provides an
effective, sustainable, and adaptable solution for streamlining safety
evaluation.

</details>


### [41] [Voice Adaptation for Swiss German](https://arxiv.org/abs/2505.22054)
*Samuel Stucki,Jan Deriu,Mark Cieliebak*

Main category: cs.CL

TL;DR: 该研究通过微调XTTSv2模型，成功实现了标准德语文本到瑞士德语方言语音的转换，并在人工和自动评估中取得良好成绩。


<details>
  <summary>Details</summary>
Motivation: 研究旨在将语音克隆技术应用于 underrepresented 语言（如瑞士德语方言），以促进方言语音合成的发展。

Method: 预处理大量瑞士播客数据，自动转录并标注方言类别，获得约5000小时的弱标注训练数据，并微调XTTSv2模型。

Result: 模型在人工评估（CMOS得分-0.28）和自动评估（SMOS得分3.8）中表现良好，能准确生成目标方言。

Conclusion: 该研究为 underrepresented 语言的语音克隆技术提供了可行方案，展示了方言语音合成的潜力。

Abstract: This work investigates the performance of Voice Adaptation models for Swiss
German dialects, i.e., translating Standard German text to Swiss German dialect
speech. For this, we preprocess a large dataset of Swiss podcasts, which we
automatically transcribe and annotate with dialect classes, yielding
approximately 5000 hours of weakly labeled training material. We fine-tune the
XTTSv2 model on this dataset and show that it achieves good scores in human and
automated evaluations and can correctly render the desired dialect. Our work
shows a step towards adapting Voice Cloning technology to underrepresented
languages. The resulting model achieves CMOS scores of up to -0.28 and SMOS
scores of 3.8.

</details>


### [42] [Safeguarding Privacy of Retrieval Data against Membership Inference Attacks: Is This Query Too Close to Home?](https://arxiv.org/abs/2505.22061)
*Yujin Choi,Youngjoo Park,Junyoung Byun,Jaewook Lee,Jinseong Park*

Main category: cs.CL

TL;DR: 论文提出Mirabel框架，通过相似性检测防御检索增强生成中的成员推理攻击，保护隐私数据。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成(RAG)虽减少大模型幻觉，但直接传递私有文档易受成员推理攻击(MIA)，需保护隐私数据安全。

Method: 基于MIA查询与目标文档高相似性的特点，设计Mirabel检测框架，采用检测-隐藏策略混淆攻击者。

Result: 实验证明Mirabel能有效防御多种先进MIA方法，适配现有私有RAG系统且保持数据效用。

Conclusion: Mirabel为RAG系统提供轻量级隐私保护方案，平衡安全性与实用性。

Abstract: Retrieval-augmented generation (RAG) mitigates the hallucination problem in
large language models (LLMs) and has proven effective for specific,
personalized applications. However, passing private retrieved documents
directly to LLMs introduces vulnerability to membership inference attacks
(MIAs), which try to determine whether the target datum exists in the private
external database or not. Based on the insight that MIA queries typically
exhibit high similarity to only one target document, we introduce Mirabel, a
similarity-based MIA detection framework designed for the RAG system. With the
proposed Mirabel, we show that simple detect-and-hide strategies can
successfully obfuscate attackers, maintain data utility, and remain
system-agnostic. We experimentally prove its detection and defense against
various state-of-the-art MIA methods and its adaptability to existing private
RAG systems.

</details>


### [43] [Beyond path selection: Better LLMs for Scientific Information Extraction with MimicSFT and Relevance and Rule-induced(R$^2$)GRPO](https://arxiv.org/abs/2505.22068)
*Ran Li,Shimin Di,Yuchen Liu,Chen Jing,Yu Qiu,Lei Chen*

Main category: cs.CL

TL;DR: 论文提出两阶段训练方法（MimicSFT和R²GRPO）提升大语言模型在科学信息抽取任务中的推理能力，超越基线模型和专用监督模型。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，基于强化学习的大语言模型在数学任务中仅优化推理路径而非推理能力，而监督微调则能提升能力。科学信息抽取任务需要记忆与推理结合，但大语言模型表现不佳。因此，作者探索如何通过简单方法结合监督微调与强化学习提升推理能力。

Method: 1. MimicSFT阶段：使用结构化推理模板，无需高质量思维链数据；2. R²GRPO阶段：设计基于相关性和规则诱导奖励的强化学习框架。

Result: 实验表明，两阶段方法显著提升推理能力，R²GRPO结合MimicSFT在关系抽取任务上超越基线大模型和专用监督模型。

Conclusion: 监督微调和强化学习均可通过结构化方法协同提升科学信息抽取中的推理能力，两阶段训练策略具有有效性。

Abstract: Previous study suggest that powerful Large Language Models (LLMs) trained
with Reinforcement Learning with Verifiable Rewards (RLVR) only refines
reasoning path without improving the reasoning capacity in math tasks while
supervised-finetuning(SFT) with distillation can. We study this from the view
of Scientific information extraction (SciIE) where LLMs and reasoning LLMs
underperforms small Bert-based models. SciIE require both the reasoning and
memorization. We argue that both SFT and RLVR can refine the reasoning path and
improve reasoning capacity in a simple way based on SciIE. We propose two-stage
training with 1. MimicSFT, using structured reasoning templates without needing
high-quality chain-of-thought data, 2. R$^2$GRPO with relevance and
rule-induced rewards. Experiments on scientific IE benchmarks show that both
methods can improve the reasoning capacity. R$^2$GRPO with mimicSFT surpasses
baseline LLMs and specialized supervised models in relation extraction. Our
code is available at https://github.com/ranlislz/R2GRPO.

</details>


### [44] [ArgInstruct: Specialized Instruction Fine-Tuning for Computational Argumentation](https://arxiv.org/abs/2505.22076)
*Maja Stahl,Timon Ziegenbein,Joonsuk Park,Henning Wachsmuth*

Main category: cs.CL

TL;DR: 该论文提出了一种针对计算论证领域的专用指令微调方法，显著提升了大型语言模型在该领域任务上的表现，同时保持其通用能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在遵循指令方面表现优异，但在需要领域知识的任务上仍存在困难。本文旨在通过专用指令微调提升模型在计算论证领域的表现。

Method: 作者梳理了105个计算论证任务的自然语言指令，构建了专用评测基准，并通过自指导过程生成了52k条相关指令用于模型训练。

Result: 实验表明，专用指令微调显著提升了模型在计算论证任务上的表现（包括未见任务），同时不影响其在通用NLP任务上的性能。

Conclusion: 领域专用指令微调是提升大型语言模型在特定领域表现的有效方法，且不会损害其通用能力。

Abstract: Training large language models (LLMs) to follow instructions has
significantly enhanced their ability to tackle unseen tasks. However, despite
their strong generalization capabilities, instruction-following LLMs encounter
difficulties when dealing with tasks that require domain knowledge. This work
introduces a specialized instruction fine-tuning for the domain of
computational argumentation (CA). The goal is to enable an LLM to effectively
tackle any unseen CA tasks while preserving its generalization capabilities.
Reviewing existing CA research, we crafted natural language instructions for
105 CA tasks to this end. On this basis, we developed a CA-specific benchmark
for LLMs that allows for a comprehensive evaluation of LLMs' capabilities in
solving various CA tasks. We synthesized 52k CA-related instructions, adapting
the self-instruct process to train a CA-specialized instruction-following LLM.
Our experiments suggest that CA-specialized instruction fine-tuning
significantly enhances the LLM on both seen and unseen CA tasks. At the same
time, performance on the general NLP tasks of the SuperNI benchmark remains
stable.

</details>


### [45] [Learning to Route Queries Across Knowledge Bases for Step-wise Retrieval-Augmented Reasoning](https://arxiv.org/abs/2505.22095)
*Chunyi Peng,Zhipeng Xu,Zhenghao Liu,Yishan Li,Yukun Yan,Shuo Wang,Zhiyuan Liu,Yu Gu,Minghe Yu,Ge Yu,Maosong Sun*

Main category: cs.CL

TL;DR: 本文提出R1-Router框架，通过动态决定何时及从何处检索知识来增强多模态大语言模型的推理能力，结合Step-GRPO算法优化推理行为，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有MRAG方法采用静态检索流程，忽视了多模态大语言模型在推理过程中动态与不同知识库交互的能力，导致效率与准确性不足。

Method: 提出R1-Router框架，动态生成后续查询并路由至合适知识库；引入Step-GRPO算法，通过步骤特定奖励优化模型推理行为。

Result: 实验表明，R1-Router在多种开放域QA基准测试中优于基线模型7%以上，能自适应利用多样知识库，减少不必要检索。

Conclusion: R1-Router通过动态检索与强化学习优化，显著提升了多模态大语言模型的推理效率与准确性。

Abstract: Multimodal Retrieval-Augmented Generation (MRAG) has shown promise in
mitigating hallucinations in Multimodal Large Language Models (MLLMs) by
incorporating external knowledge during generation. Existing MRAG methods
typically adopt a static retrieval pipeline that fetches relevant information
from multiple Knowledge Bases (KBs), followed by a refinement step. However,
these approaches overlook the reasoning and planning capabilities of MLLMs to
dynamically determine how to interact with different KBs during the reasoning
process. To address this limitation, we propose R1-Router, a novel MRAG
framework that learns to decide when and where to retrieve knowledge based on
the evolving reasoning state. Specifically, R1-Router can generate follow-up
queries according to the current reasoning step, routing these intermediate
queries to the most suitable KB, and integrating external knowledge into a
coherent reasoning trajectory to answer the original query. Furthermore, we
introduce Step-wise Group Relative Policy Optimization (Step-GRPO), a tailored
reinforcement learning algorithm that assigns step-specific rewards to optimize
the reasoning behavior of MLLMs. Experimental results on various open-domain QA
benchmarks across multiple modalities demonstrate that R1-Router outperforms
baseline models by over 7%. Further analysis shows that R1-Router can
adaptively and effectively leverage diverse KBs, reducing unnecessary
retrievals and improving both efficiency and accuracy.

</details>


### [46] [Knowledge Base Construction for Knowledge-Augmented Text-to-SQL](https://arxiv.org/abs/2505.22096)
*Jinheon Baek,Horst Samulowitz,Oktie Hassanzadeh,Dharmashankar Subramanian,Sola Shirai,Alfio Gliozzo,Debarun Bhattacharjya*

Main category: cs.CL

TL;DR: 该论文提出构建一个全面的知识库来提升Text-to-SQL的准确性，解决大语言模型在处理多样化、领域特定查询时的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的Text-to-SQL方法在处理多样化、领域特定的查询时，由于模型参数知识的限制，生成的SQL语句准确性不足。

Method: 构建一个综合知识库，结合所有可用问题、相关数据库模式及其相关知识，为查询提供必要的知识支持，并可跨数据集和领域重用。

Result: 在多个Text-to-SQL数据集上验证，该方法在重叠和非重叠数据库场景下均显著优于基线。

Conclusion: 通过构建全面的知识库，可以有效提升Text-to-SQL的准确性，特别是在处理多样化、领域特定查询时。

Abstract: Text-to-SQL aims to translate natural language queries into SQL statements,
which is practical as it enables anyone to easily retrieve the desired
information from databases. Recently, many existing approaches tackle this
problem with Large Language Models (LLMs), leveraging their strong capability
in understanding user queries and generating corresponding SQL code. Yet, the
parametric knowledge in LLMs might be limited to covering all the diverse and
domain-specific queries that require grounding in various database schemas,
which makes generated SQLs less accurate oftentimes. To tackle this, we propose
constructing the knowledge base for text-to-SQL, a foundational source of
knowledge, from which we retrieve and generate the necessary knowledge for
given queries. In particular, unlike existing approaches that either manually
annotate knowledge or generate only a few pieces of knowledge for each query,
our knowledge base is comprehensive, which is constructed based on a
combination of all the available questions and their associated database
schemas along with their relevant knowledge, and can be reused for unseen
databases from different datasets and domains. We validate our approach on
multiple text-to-SQL datasets, considering both the overlapping and
non-overlapping database scenarios, where it outperforms relevant baselines
substantially.

</details>


### [47] [MemOS: An Operating System for Memory-Augmented Generation (MAG) in Large Language Models](https://arxiv.org/abs/2505.22101)
*Zhiyu Li,Shichao Song,Hanyu Wang,Simin Niu,Ding Chen,Jiawei Yang,Chenyang Xi,Huayi Lai,Jihao Zhao,Yezhaohui Wang,Junpeng Ren,Zehao Lin,Jiahao Huo,Tianyi Chen,Kai Chen,Kehang Li,Zhiqiang Yin,Qingchen Yu,Bo Tang,Hongkang Yang,Zhi-Qin John Xu,Feiyu Xiong*

Main category: cs.CL

TL;DR: 论文提出MemOS，一种为LLMs设计的内存操作系统，首次将内存提升为一等操作资源，解决当前LLMs缺乏统一结构化内存架构的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）主要依赖参数化内存和临时激活内存，缺乏统一的内存管理架构，限制了长期知识进化的能力。现有方法如RAG虽引入纯文本内存，但缺乏生命周期管理和多模态集成。

Method: 引入MemOS，构建统一的内存表示、组织和治理机制，涵盖参数化、激活和纯文本三种核心内存类型，核心是MemCube标准化内存抽象，支持异构内存的跟踪、融合和迁移。

Result: MemOS建立了具有强可控性、适应性和可进化性的以内存为中心的执行框架，填补了当前LLM基础设施的关键空白。

Conclusion: MemOS为下一代智能系统的持续适应、个性化智能和跨平台协调奠定了基础。

Abstract: Large Language Models (LLMs) have emerged as foundational infrastructure in
the pursuit of Artificial General Intelligence (AGI). Despite their remarkable
capabilities in language perception and generation, current LLMs fundamentally
lack a unified and structured architecture for handling memory. They primarily
rely on parametric memory (knowledge encoded in model weights) and ephemeral
activation memory (context-limited runtime states). While emerging methods like
Retrieval-Augmented Generation (RAG) incorporate plaintext memory, they lack
lifecycle management and multi-modal integration, limiting their capacity for
long-term knowledge evolution. To address this, we introduce MemOS, a memory
operating system designed for LLMs that, for the first time, elevates memory to
a first-class operational resource. It builds unified mechanisms for
representation, organization, and governance across three core memory types:
parametric, activation, and plaintext. At its core is the MemCube, a
standardized memory abstraction that enables tracking, fusion, and migration of
heterogeneous memory, while offering structured, traceable access across tasks
and contexts. MemOS establishes a memory-centric execution framework with
strong controllability, adaptability, and evolvability. It fills a critical gap
in current LLM infrastructure and lays the groundwork for continual adaptation,
personalized intelligence, and cross-platform coordination in next-generation
intelligent systems.

</details>


### [48] [Curse of High Dimensionality Issue in Transformer for Long-context Modeling](https://arxiv.org/abs/2505.22107)
*Shuhai Zhang,Zeng You,Yaofo Chen,Zhiquan Wen,Qianyue Wang,Zhijie Qiu,Yuanqing Li,Mingkui Tan*

Main category: cs.CL

TL;DR: 该论文提出动态组注意力（DGA）方法，通过分组编码策略减少Transformer模型中的冗余计算，在保持性能的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer模型在处理长上下文时存在计算效率低下的问题，主要由于自注意力机制对所有令牌进行均等计算，而实际上注意力权重往往是稀疏的，只有少数令牌对预测有显著贡献。

Method: 论文将概率序列建模重新定义为监督学习任务，提出基于分组编码策略的动态组注意力（DGA），通过聚合不重要令牌来显式减少冗余计算。

Result: 实验结果表明，DGA在显著降低计算成本的同时，保持了与现有方法竞争的性能表现。

Conclusion: 动态组注意力（DGA）通过理论分析和实验验证，有效解决了长上下文建模中的冗余计算问题，为高效注意力机制设计提供了新思路。

Abstract: Transformer-based large language models (LLMs) excel in natural language
processing tasks by capturing long-range dependencies through self-attention
mechanisms. However, long-context modeling faces significant computational
inefficiencies due to \textit{redundant} attention computations: while
attention weights are often \textit{sparse}, all tokens consume \textit{equal}
computational resources. In this paper, we reformulate traditional
probabilistic sequence modeling as a \textit{supervised learning task},
enabling the separation of relevant and irrelevant tokens and providing a
clearer understanding of redundancy. Based on this reformulation, we
theoretically analyze attention sparsity, revealing that only a few tokens
significantly contribute to predictions. Building on this, we formulate
attention optimization as a linear coding problem and propose a \textit{group
coding strategy}, theoretically showing its ability to improve robustness
against random noise and enhance learning efficiency. Motivated by this, we
propose \textit{Dynamic Group Attention} (DGA), which leverages the group
coding to explicitly reduce redundancy by aggregating less important tokens
during attention computation. Empirical results show that our DGA significantly
reduces computational costs while maintaining competitive performance.Code is
available at https://github.com/bolixinyu/DynamicGroupAttention.

</details>


### [49] [THINK-Bench: Evaluating Thinking Efficiency and Chain-of-Thought Quality of Large Reasoning Models](https://arxiv.org/abs/2505.22113)
*Zhiyuan Li,Yi Chang,Yuan Wu*

Main category: cs.CL

TL;DR: 大型推理模型（LRMs）在复杂任务中表现优异，但存在过度思考问题，导致计算效率低下。为此，研究者提出了Think-Bench基准和新效率指标，评估发现多数LRMs在简单问题上存在冗余推理。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）虽然在复杂任务中表现优于传统大语言模型（LLMs），但普遍存在过度思考问题，即在简单任务中生成过多冗余标记，浪费计算资源。

Method: 研究者引入了Think-Bench基准，并提出新的效率指标，从推理过程、结果质量和思维链（CoT）特性等多个维度全面评估各种LRMs。

Result: 评估发现，多数LRMs在处理简单问题时存在过度思考现象，生成了不必要的冗长推理链。虽然许多LRMs的CoT质量较高，但部分模型效率低下。

Conclusion: Think-Bench为LRMs的研究提供了坚实基础，有助于推动对推理效率的进一步探索。

Abstract: Large reasoning models (LRMs) have achieved impressive performance in complex
tasks, often outperforming conventional large language models (LLMs). However,
the prevalent issue of overthinking severely limits their computational
efficiency. Overthinking occurs when models generate excessive and redundant
tokens that contribute little to accurate outcomes, especially in simple tasks,
resulting in a significant waste of computational resources. To systematically
investigate this issue, we introduce Think-Bench, a benchmark designed to
evaluate the reasoning efficiency of LRMs. We also propose novel efficiency
metrics and conduct a comprehensive evaluation of various LRMs across multiple
dimensions, including the reasoning process, outcome quality, and
chain-of-thought (CoT) characteristics. Our analysis reveals that most LRMs
exhibit overthinking in handling easy questions, generating unnecessarily
lengthy reasoning chains. While many LRMs demonstrate high CoT quality, several
suffer from low efficiency. We hope that Think-Bench can serve as a robust
foundation for advancing research into LRMs.

</details>


### [50] [Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model](https://arxiv.org/abs/2505.22116)
*Jintao Zhang,Zirui Liu,Mingyue Cheng,Shilong Zhang,Tingyue Pan,Qi Liu,Yanhu Xie*

Main category: cs.CL

TL;DR: 本文提出IOHFuseLM框架，通过多模态语言模型预测术中低血压，采用两阶段训练策略提升稀疏事件识别能力，并在临床数据上验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 术中低血压（IOH）与心肌损伤等不良后果密切相关，但因其事件稀疏性及患者数据多样性，预测面临挑战。

Method: 提出IOHFuseLM框架：1）通过扩散增强的生理时间序列进行领域自适应预训练；2）在原始临床数据上微调；3）将静态属性转为结构化文本，实现多模态对齐。

Result: 在两个术中数据集上，IOHFuseLM在识别IOH事件方面优于基线模型，适用于临床决策支持。

Conclusion: IOHFuseLM通过融合多模态数据和两阶段训练策略，有效提升术中低血压预测精度，代码已开源。

Abstract: Intraoperative hypotension (IOH) frequently occurs under general anesthesia
and is strongly linked to adverse outcomes such as myocardial injury and
increased mortality. Despite its significance, IOH prediction is hindered by
event sparsity and the challenge of integrating static and dynamic data across
diverse patients. In this paper, we propose \textbf{IOHFuseLM}, a multimodal
language model framework. To accurately identify and differentiate sparse
hypotensive events, we leverage a two-stage training strategy. The first stage
involves domain adaptive pretraining on IOH physiological time series augmented
through diffusion methods, thereby enhancing the model sensitivity to patterns
associated with hypotension. Subsequently, task fine-tuning is performed on the
original clinical dataset to further enhance the ability to distinguish
normotensive from hypotensive states. To enable multimodal fusion for each
patient, we align structured clinical descriptions with the corresponding
physiological time series at the token level. Such alignment enables the model
to capture individualized temporal patterns alongside their corresponding
clinical semantics. In addition, we convert static patient attributes into
structured text to enrich personalized information. Experimental evaluations on
two intraoperative datasets demonstrate that IOHFuseLM outperforms established
baselines in accurately identifying IOH events, highlighting its applicability
in clinical decision support scenarios. Our code is publicly available to
promote reproducibility at https://github.com/zjt-gpu/IOHFuseLM.

</details>


### [51] [Multilingual vs Crosslingual Retrieval of Fact-Checked Claims: A Tale of Two Approaches](https://arxiv.org/abs/2505.22118)
*Alan Ramponi,Marco Rovera,Robert Moro,Sara Tonelli*

Main category: cs.CL

TL;DR: 该研究探讨了如何通过负样本选择和重排序策略提升多语言和跨语言环境下事实核查声明的检索效果，并在47种语言的数据集上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前事实核查声明的检索主要局限于单语言环境，而对于资源稀缺语言或涉及全球性议题（如疫情、战争）的情况，跨语言检索能力至关重要。

Method: 采用监督学习中的负样本选择策略（基于句子相似度）和无监督学习中的LLM重排序方法，并在包含47种语言的数据集上进行实验。

Result: 实验表明，基于LLM的重排序效果最佳，其次是通过句子相似度策略采样负样本的微调方法。跨语言检索表现出与多语言检索不同的特性。

Conclusion: 跨语言检索具有独特性质，结合LLM重排序和针对性负样本选择可显著提升多语言事实核查系统的性能。

Abstract: Retrieval of previously fact-checked claims is a well-established task, whose
automation can assist professional fact-checkers in the initial steps of
information verification. Previous works have mostly tackled the task
monolingually, i.e., having both the input and the retrieved claims in the same
language. However, especially for languages with a limited availability of
fact-checks and in case of global narratives, such as pandemics, wars, or
international politics, it is crucial to be able to retrieve claims across
languages. In this work, we examine strategies to improve the multilingual and
crosslingual performance, namely selection of negative examples (in the
supervised) and re-ranking (in the unsupervised setting). We evaluate all
approaches on a dataset containing posts and claims in 47 languages (283
language combinations). We observe that the best results are obtained by using
LLM-based re-ranking, followed by fine-tuning with negative examples sampled
using a sentence similarity-based strategy. Most importantly, we show that
crosslinguality is a setup with its own unique characteristics compared to the
multilingual setup.

</details>


### [52] [LoKI: Low-damage Knowledge Implanting of Large Language Models](https://arxiv.org/abs/2505.22120)
*Runyu Wang,Peng Ping,Zhengyu Guo,Xiaoye Zhang,Quan Shi,Liting Zhou,Tianbo Ji*

Main category: cs.CL

TL;DR: LoKI是一种参数高效微调方法，通过理解Transformer架构中知识存储机制，在保持模型通用能力的同时实现任务特定性能。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法在适应特定任务时可能导致灾难性遗忘，牺牲模型的通用能力。

Method: 提出LoKI方法，基于对Transformer知识存储机制的理解，实现低损伤知识植入。

Result: LoKI在多种模型上表现优于全微调和LoRA方法，同时显著保留通用能力。

Conclusion: LoKI在任务专业化和通用能力保留之间实现了最先进的权衡。

Abstract: Fine-tuning adapts pretrained models for specific tasks but poses the risk of
catastrophic forgetting (CF), where critical knowledge from pre-training is
overwritten. Current Parameter-Efficient Fine-Tuning (PEFT) methods for Large
Language Models (LLMs), while efficient, often sacrifice general capabilities.
To address the issue of CF in a general-purpose PEFT framework, we propose
\textbf{Lo}w-damage \textbf{K}nowledge \textbf{I}mplanting (\textbf{LoKI}), a
PEFT technique that is based on a mechanistic understanding of how knowledge is
stored in transformer architectures. In two real-world scenarios, LoKI
demonstrates task-specific performance that is comparable to or even surpasses
that of full fine-tuning and LoRA-based methods across various model types,
while significantly better preserving general capabilities. Our work connects
mechanistic insights into LLM knowledge storage with practical fine-tuning
objectives, achieving state-of-the-art trade-offs between task specialization
and the preservation of general capabilities. Our implementation is publicly
available as ready-to-use code\footnote{https://github.com/Nexround/LoKI}.

</details>


### [53] [EULER: Enhancing the Reasoning Ability of Large Language Models through Error-Induced Learning](https://arxiv.org/abs/2505.22131)
*Zhuoyang Wu,Xinze Li,Zhenghao Liu,Yukun Yan,Zhiyuan Liu,Minghe Yu,Cheng Yang,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.CL

TL;DR: 本文提出EULER模型，通过生成高质量解题错误来增强大语言模型的数学推理能力，实验证明其性能提升超过4%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在数学解题任务中表现出色，但现有方法难以针对每个数学问题生成解题错误。通过暴露错误学习可以进一步提升模型性能。

Method: 提出EULER模型，优化错误暴露模型以增加自生成解题错误的概率，同时利用优质LLM的解题方案来规范生成质量。

Result: 在多个数学问题数据集上的实验表明，EULER模型相比基线模型性能提升超过4%，并能合成更具挑战性和教育意义的解题错误。

Conclusion: EULER能有效增强LLMs的数学推理能力，其生成的错误有助于模型的训练和推理过程，所有代码已开源。

Abstract: Large Language Models (LLMs) have demonstrated strong reasoning capabilities
and achieved promising results in mathematical problem-solving tasks. Learning
from errors offers the potential to further enhance the performance of LLMs
during Supervised Fine-Tuning (SFT). However, the errors in synthesized
solutions are typically gathered from sampling trails, making it challenging to
generate solution errors for each mathematical problem. This paper introduces
the Error-IndUced LEaRning (EULER) model, which aims to develop an error
exposure model that generates high-quality solution errors to enhance the
mathematical reasoning capabilities of LLMs. Specifically, EULER optimizes the
error exposure model to increase the generation probability of self-made
solution errors while utilizing solutions produced by a superior LLM to
regularize the generation quality. Our experiments across various mathematical
problem datasets demonstrate the effectiveness of the EULER model, achieving an
improvement of over 4% compared to all baseline models. Further analysis
reveals that EULER is capable of synthesizing more challenging and educational
solution errors, which facilitate both the training and inference processes of
LLMs. All codes are available at https://github.com/NEUIR/EULER.

</details>


### [54] [RAD: Redundancy-Aware Distillation for Hybrid Models via Self-Speculative Decoding](https://arxiv.org/abs/2505.22135)
*Yuichiro Hoshino,Hideyuki Tachibana,Muneyoshi Inahara,Hiroto Takegawa*

Main category: cs.CL

TL;DR: 论文提出RAD框架，通过自推测解码识别Transformer冗余层并用SSM组件替换，结合定向自蒸馏提升混合模型性能与效率。


<details>
  <summary>Details</summary>
Motivation: 当前混合Transformer与状态空间模型(SSM)的架构存在冗余问题，尤其在Transformer组件中。优化这类混合模型以平衡性能与效率仍具挑战性。

Method: 提出RAD框架：1) 用自推测解码诊断冗余注意力层；2) 选择性替换为SSM组件；3) 针对冗余部分进行定向自蒸馏，结合架构修改与权重初始化策略。

Result: 在数学和代码任务上，RAD显著超越基线模型（GSM8K 71.27 vs 46.17，CRUX 28.25 vs 22.75）。仅用8B参数的教师模型即实现比70B基线更优效果，收敛速度提升约2倍。

Conclusion: RAD为混合模型蒸馏提供了性能优化新路径，通过冗余感知的组件替换与知识蒸馏，实现高效性与性能的双重提升。

Abstract: Hybrid models combining Transformers and State Space Models (SSMs) are
promising for balancing performance and efficiency. However, optimizing these
hybrid models, particularly by addressing the potential redundancy inherent
within the Transformer components, remains a significant challenge. In this
paper, we propose RAD (Redundancy-Aware Distillation), a novel framework that
uses self-speculative decoding as a diagnostic tool to identify redundant
attention layers within the model. These identified layers are then selectively
replaced with SSM components, followed by targeted (self-)distillation.
Specifically, RAD focuses knowledge transfer on the components identified as
redundant, considering architectural changes and specific weight initialization
strategies. We experimentally demonstrate that self-distillation using RAD
significantly surpasses the performance of the original base model on
mathematical and coding tasks. Furthermore, RAD is also effective in standard
knowledge distillation settings, achieving up to approximately 2x faster
convergence compared to baseline methods. Notably, while a baseline model
distilled from a Llama-3.1 70B teacher achieves scores of 46.17 on GSM8K and
22.75 on CRUX, RAD achieves significantly higher scores of 71.27 on GSM8K and
28.25 on CRUX, even when using a much smaller Llama-3.1 8B teacher. RAD offers
a new pathway for efficient optimization and performance enhancement in the
distillation of hybrid models.

</details>


### [55] [Limited Generalizability in Argument Mining: State-Of-The-Art Models Learn Datasets, Not Arguments](https://arxiv.org/abs/2505.22137)
*Marc Feger,Katarina Boland,Stefan Dietze*

Main category: cs.CL

TL;DR: 该论文首次大规模重新评估了最先进的BERT类模型在识别论点的泛化能力，发现模型依赖词汇捷径，泛化表现不佳，但特定任务预训练和联合基准训练能提升鲁棒性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 论点识别是自动化话语分析的关键前提，当前研究多依赖BERT类模型，但其泛化能力尚未充分验证。本文旨在评估这些模型在跨数据集识别论点时的实际表现。

Method: 评估了4种Transformer模型（包括3种标准模型和1种通过对比预训练增强的模型）在17个英文句子级数据集上的表现，重点关注模型对词汇捷径的依赖和跨数据集泛化能力。

Result: 模型在已知数据集表现良好，但在未见数据集上性能显著下降。研究发现模型依赖内容词相关的词汇捷径，而非真正理解任务。结合任务特定预训练和联合训练可提升鲁棒性和泛化性。

Conclusion: 当前论点识别模型的进步可能受数据集特定线索驱动。通过改进训练策略（如对比预训练和多数据集联合训练）可增强模型泛化能力，但需警惕对表面模式的依赖。

Abstract: Identifying arguments is a necessary prerequisite for various tasks in
automated discourse analysis, particularly within contexts such as political
debates, online discussions, and scientific reasoning. In addition to
theoretical advances in understanding the constitution of arguments, a
significant body of research has emerged around practical argument mining,
supported by a growing number of publicly available datasets. On these
benchmarks, BERT-like transformers have consistently performed best,
reinforcing the belief that such models are broadly applicable across diverse
contexts of debate. This study offers the first large-scale re-evaluation of
such state-of-the-art models, with a specific focus on their ability to
generalize in identifying arguments. We evaluate four transformers, three
standard and one enhanced with contrastive pre-training for better
generalization, on 17 English sentence-level datasets as most relevant to the
task. Our findings show that, to varying degrees, these models tend to rely on
lexical shortcuts tied to content words, suggesting that apparent progress may
often be driven by dataset-specific cues rather than true task alignment. While
the models achieve strong results on familiar benchmarks, their performance
drops markedly when applied to unseen datasets. Nonetheless, incorporating both
task-specific pre-training and joint benchmark training proves effective in
enhancing both robustness and generalization.

</details>


### [56] [InComeS: Integrating Compression and Selection Mechanisms into LLMs for Efficient Model Editing](https://arxiv.org/abs/2505.22156)
*Shuaiyi Li,Zhisong Zhang,Yang Deng,Chenlong Deng,Tianqing Fang,Hongming Zhang,Haitao Mi,Dong Yu,Wai Lam*

Main category: cs.CL

TL;DR: 提出InComeS框架，通过压缩和选择机制增强大语言模型处理编辑上下文的能力，解决现有方法在复杂场景下的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有模型编辑方法在需要深层语义理解的复杂场景中表现不佳，且受限于大语言模型的上下文窗口限制。

Method: InComeS框架通过将编辑上下文压缩为关键值缓存，并添加交叉注意力模块动态选择最相关信息。

Result: 实验表明，该方法在不同编辑格式的基准测试中表现出高效性和有效性。

Conclusion: InComeS框架显著提升了大语言模型处理多编辑任务的能力，突破了上下文窗口的限制。

Abstract: Although existing model editing methods perform well in recalling exact edit
facts, they often struggle in complex scenarios that require deeper semantic
understanding rather than mere knowledge regurgitation. Leveraging the strong
contextual reasoning abilities of large language models (LLMs), in-context
learning (ICL) becomes a promising editing method by comprehending edit
information through context encoding. However, this method is constrained by
the limited context window of LLMs, leading to degraded performance and
efficiency as the number of edits increases. To overcome this limitation, we
propose InComeS, a flexible framework that enhances LLMs' ability to process
editing contexts through explicit compression and selection mechanisms.
Specifically, InComeS compresses each editing context into the key-value (KV)
cache of a special gist token, enabling efficient handling of multiple edits
without being restricted by the model's context window. Furthermore,
specialized cross-attention modules are added to dynamically select the most
relevant information from the gist pools, enabling adaptive and effective
utilization of edit information. We conduct experiments on diverse model
editing benchmarks with various editing formats, and the results demonstrate
the effectiveness and efficiency of our method.

</details>


### [57] [Stratified Selective Sampling for Instruction Tuning with Dedicated Scoring Strategy](https://arxiv.org/abs/2505.22157)
*Paramita Mirza,Lucas Weber,Fabian Küch*

Main category: cs.CL

TL;DR: 该论文提出了一种高效且通用的数据选择方法，通过多步骤流程优化LLM微调的数据集，确保性能同时降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法计算成本高或适用范围有限，影响了LLM微调的效率和通用性。

Method: 采用多步骤流程：数据分组、质量评估、难度评分，并结合嵌入模型和聚类算法保证多样性。

Result: 实现了高性能的微调，同时显著降低了计算开销。

Conclusion: 该方法在保证数据多样性和质量的同时，高效地优化了LLM微调过程。

Abstract: Recent work shows that post-training datasets for LLMs can be substantially
downsampled without noticeably deteriorating performance. However, data
selection often incurs high computational costs or is limited to narrow
domains. In this paper, we demonstrate that data selection can be both --
efficient and universal -- by using a multi-step pipeline in which we
efficiently bin data points into groups, estimate quality using specialized
models, and score difficulty with a robust, lightweight method. Task-based
categorization allows us to control the composition of our final data --
crucial for finetuning multi-purpose models. To guarantee diversity, we improve
upon previous work using embedding models and a clustering algorithm. This
integrated strategy enables high-performance fine-tuning with minimal overhead.

</details>


### [58] [Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes](https://arxiv.org/abs/2505.22165)
*Bocheng Li,Zhujin Gao,Linli Xu*

Main category: cs.CL

TL;DR: 本文提出NeoDiff模型，结合离散和连续扩散模型的优势，通过泊松扩散过程和时间预测器实现更精细的文本生成控制。


<details>
  <summary>Details</summary>
Motivation: 现有离散和连续扩散模型在文本生成中各有限制：离散模型缺乏细粒度控制，连续模型难以捕捉语义细节。NeoDiff旨在整合两者优势。

Method: NeoDiff采用泊松扩散过程实现灵活噪声添加，并引入时间预测器自适应调节去噪过程，同时优化推理调度以提升性能。

Result: 实验表明，NeoDiff在多项文本生成任务中优于基线模型，包括非自回归/自回归扩散模型和基于迭代的方法。

Conclusion: NeoDiff为扩散模型文本生成提供了更统一的理论框架，展现出生成高质量文本的潜力。

Abstract: Diffusion models have emerged as a promising approach for text generation,
with recent works falling into two main categories: discrete and continuous
diffusion models. Discrete diffusion models apply token corruption
independently using categorical distributions, allowing for different diffusion
progress across tokens but lacking fine-grained control. Continuous diffusion
models map tokens to continuous spaces and apply fine-grained noise, but the
diffusion progress is uniform across tokens, limiting their ability to capture
semantic nuances. To address these limitations, we propose
\textbf{\underline{N}}on-simultan\textbf{\underline{e}}ous
C\textbf{\underline{o}}ntinuous \textbf{\underline{Diff}}usion Models
(NeoDiff), a novel diffusion model that integrates the strengths of both
discrete and continuous approaches. NeoDiff introduces a Poisson diffusion
process for the forward process, enabling a flexible and fine-grained noising
paradigm, and employs a time predictor for the reverse process to adaptively
modulate the denoising progress based on token semantics. Furthermore, NeoDiff
utilizes an optimized schedule for inference to ensure more precise noise
control and improved performance. Our approach unifies the theories of discrete
and continuous diffusion models, offering a more principled and effective
framework for text generation. Experimental results on several text generation
tasks demonstrate NeoDiff's superior performance compared to baselines of
non-autoregressive continuous and discrete diffusion models, iterative-based
methods and autoregressive diffusion-based methods. These results highlight
NeoDiff's potential as a powerful tool for generating high-quality text and
advancing the field of diffusion-based text generation.

</details>


### [59] [ReliableEval: A Recipe for Stochastic LLM Evaluation via Method of Moments](https://arxiv.org/abs/2505.22169)
*Gili Lior,Eliya Habba,Shahar Levy,Avi Caciularu,Gabriel Stanovsky*

Main category: cs.CL

TL;DR: 该论文提出了一种基于随机矩估计的方法ReliableEval，用于评估大语言模型(LLM)在语义保留提示扰动下的可靠性，发现即使顶级模型也存在显著的提示敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试通常仅使用单一提示评估LLM性能，而LLM对提示表述高度敏感，这种评估方式可靠性存疑。

Method: 提出随机矩估计方法ReliableEval，通过语义保留的提示扰动空间进行评估，并计算获得有意义结果所需的提示重采样次数。

Result: 对五个前沿LLM的评估表明，即使是GPT-4o和Claude-3.7-Sonnet等顶级模型也表现出显著的提示敏感性。

Conclusion: 该方法与模型、任务和指标无关，为LLM评估提供了可靠且稳健的方案。

Abstract: LLMs are highly sensitive to prompt phrasing, yet standard benchmarks
typically report performance using a single prompt, raising concerns about the
reliability of such evaluations. In this work, we argue for a stochastic method
of moments evaluation over the space of meaning-preserving prompt
perturbations. We introduce a formal definition of reliable evaluation that
accounts for prompt sensitivity, and suggest ReliableEval - a method for
estimating the number of prompt resamplings needed to obtain meaningful
results. Using our framework, we stochastically evaluate five frontier LLMs and
find that even top-performing models like GPT-4o and Claude-3.7-Sonnet exhibit
substantial prompt sensitivity. Our approach is model-, task-, and
metric-agnostic, offering a recipe for meaningful and robust LLM evaluation.

</details>


### [60] [Reverse Preference Optimization for Complex Instruction Following](https://arxiv.org/abs/2505.22172)
*Xiang Huang,Ting-En Lin,Feiteng Fang,Yuchuan Wu,Hangyu Li,Yuzhong Qu,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: 论文提出了一种名为反向偏好优化（RPO）的新方法，用于改进大型语言模型在处理复杂指令时的性能，通过动态反转指令中的约束来减少偏好对中的噪声，并在多个基准测试中表现出优于基线方法的性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在处理具有多重约束的复杂指令时面临挑战，传统方法基于满足约束的数量选择偏好对，但这种方法会引入噪声，导致选择的响应可能不完全满足所有约束，而被拒绝的响应在某些方面可能优于选择的响应。

Method: 论文提出了反向偏好优化（RPO）方法，通过动态反转指令中的约束来确保选择的响应是完美的，从而减少偏好对中的噪声，并扩大选择与被拒绝响应之间的差距，使优化方向更加明确且对噪声更鲁棒。

Result: 在Sysbench和Multi-IF两个多轮指令跟随基准测试中，RPO方法在Llama-3.1 8B模型上分别比DPO基线平均提高了4.6和2.5分。此外，RPO在不同规模的模型（8B到70B参数）上均表现良好，70B参数的RPO模型甚至超越了GPT-4o。

Conclusion: RPO方法通过动态反转指令约束，有效减少了偏好对中的噪声，提高了大型语言模型处理复杂指令的能力，并在多个基准测试中表现出优越的性能，且具有良好的可扩展性。

Abstract: Instruction following (IF) is a critical capability for large language models
(LLMs). However, handling complex instructions with multiple constraints
remains challenging. Previous methods typically select preference pairs based
on the number of constraints they satisfy, introducing noise where chosen
examples may fail to follow some constraints and rejected examples may excel in
certain respects over the chosen ones. To address the challenge of aligning
with multiple preferences, we propose a simple yet effective method called
Reverse Preference Optimization (RPO). It mitigates noise in preference pairs
by dynamically reversing the constraints within the instruction to ensure the
chosen response is perfect, alleviating the burden of extensive sampling and
filtering to collect perfect responses. Besides, reversal also enlarges the gap
between chosen and rejected responses, thereby clarifying the optimization
direction and making it more robust to noise. We evaluate RPO on two multi-turn
IF benchmarks, Sysbench and Multi-IF, demonstrating average improvements over
the DPO baseline of 4.6 and 2.5 points (on Llama-3.1 8B), respectively.
Moreover, RPO scales effectively across model sizes (8B to 70B parameters),
with the 70B RPO model surpassing GPT-4o.

</details>


### [61] [TabXEval: Why this is a Bad Table? An eXhaustive Rubric for Table Evaluation](https://arxiv.org/abs/2505.22176)
*Vihang Pancholi,Jainit Bafna,Tejas Anvekar,Manish Shrivastava,Vivek Gupta*

Main category: cs.CL

TL;DR: 论文提出了一种新颖的表格评估框架TabXEval，结合结构对齐和语义比较，解决了传统方法难以捕捉细微差异的问题，并通过多领域基准测试验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统表格评估指标难以捕捉结构和内容上的细微差异，因此需要一种更全面、可解释的评估方法。

Method: 提出TabXEval框架，包含TabAlign结构对齐和TabCompare语义比较两阶段，结合多级结构描述符和细粒度上下文量化。

Result: 在TabXBench基准测试中，TabXEval展现出优于传统方法的敏感性和特异性，适用于多领域表格任务。

Conclusion: TabXEval为可解释的表格评估奠定了基础，未来有望推动该领域的进一步创新。

Abstract: Evaluating tables qualitatively & quantitatively presents a significant
challenge, as traditional metrics often fail to capture nuanced structural and
content discrepancies. To address this, we introduce a novel, methodical rubric
integrating multi-level structural descriptors with fine-grained contextual
quantification, thereby establishing a robust foundation for comprehensive
table comparison. Building on this foundation, we propose TabXEval, an
eXhaustive and eXplainable two-phase evaluation framework. TabXEval initially
aligns reference tables structurally via TabAlign & subsequently conducts a
systematic semantic and syntactic comparison using TabCompare; this approach
clarifies the evaluation process and pinpoints subtle discrepancies overlooked
by conventional methods. The efficacy of this framework is assessed using
TabXBench, a novel, diverse, multi-domain benchmark we developed, featuring
realistic table perturbations and human-annotated assessments. Finally, a
systematic analysis of existing evaluation methods through
sensitivity-specificity trade-offs demonstrates the qualitative and
quantitative effectiveness of TabXEval across diverse table-related tasks and
domains, paving the way for future innovations in explainable table evaluation.

</details>


### [62] [Speculative Decoding Meets Quantization: Compatibility Evaluation and Hierarchical Framework Design](https://arxiv.org/abs/2505.22179)
*Yudi Zhang,Weilin Zhao,Xu Han,Tiejun Zhao,Wang Xu,Hailong Cao,Conghui Zhu*

Main category: cs.CL

TL;DR: 该论文研究了推测解码与量化技术的结合，发现4位权重量化在推测解码下的内存优势被计算负载抵消，提出了一种分层框架以优化性能。


<details>
  <summary>Details</summary>
Motivation: 推测解码和量化技术能有效加速大型语言模型的推理，但二者的结合效果尚未充分探索。论文旨在研究如何整合这两种技术以进一步提升性能。

Method: 提出了一种分层推测解码框架，利用小模型作为中间阶段，将树状草案转换为序列草案，从而发挥量化模型的内存访问优势。

Result: 实验结果表明，分层方法在4位权重量化的Llama-3-70B模型上实现了2.78倍的加速，优于EAGLE-2方法的1.31倍。

Conclusion: 分层推测解码框架有效结合了量化与推测解码的优势，显著提升了大型语言模型的推理速度。

Abstract: Speculative decoding and quantization effectively accelerate memory-bound
inference of large language models. Speculative decoding mitigates the memory
bandwidth bottleneck by verifying multiple tokens within a single forward pass,
which increases computational effort. Quantization achieves this optimization
by compressing weights and activations into lower bit-widths and also reduces
computations via low-bit matrix multiplications. To further leverage their
strengths, we investigate the integration of these two techniques.
Surprisingly, experiments applying the advanced speculative decoding method
EAGLE-2 to various quantized models reveal that the memory benefits from 4-bit
weight quantization are diminished by the computational load from speculative
decoding. Specifically, verifying a tree-style draft incurs significantly more
time overhead than a single-token forward pass on 4-bit weight quantized
models. This finding led to our new speculative decoding design: a hierarchical
framework that employs a small model as an intermediate stage to turn
tree-style drafts into sequence drafts, leveraging the memory access benefits
of the target quantized model. Experimental results show that our hierarchical
approach achieves a 2.78$\times$ speedup across various tasks for the 4-bit
weight Llama-3-70B model on an A100 GPU, outperforming EAGLE-2 by 1.31$\times$.
Code available at https://github.com/AI9Stars/SpecMQuant.

</details>


### [63] [Breaking the Cloak! Unveiling Chinese Cloaked Toxicity with Homophone Graph and Toxic Lexicon](https://arxiv.org/abs/2505.22184)
*Xuchen Ma,Jianxiang Yu,Wenming Shao,Bo Pang,Xiang Li*

Main category: cs.CL

TL;DR: 该论文提出了一种名为C$^2$TU的新方法，用于检测中文社交媒体中的伪装毒性内容，无需训练或提示，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上毒性内容日益增多，部分用户通过同音伪装逃避审查，现有方法多针对英文，中文伪装毒性检测尚未解决。

Method: C$^2$TU通过子串匹配识别候选毒性词，基于BERT和LLMs过滤非毒性内容并校正伪装词，利用完整语义上下文揭示伪装毒性。

Result: 实验表明，C$^2$TU在两个中文毒性数据集上表现优异，F1分数和准确率分别比最佳竞争对手高出71%和35%。

Conclusion: C$^2$TU为中文伪装毒性内容检测提供了一种高效且无需训练的新方法，显著提升了检测性能。

Abstract: Social media platforms have experienced a significant rise in toxic content,
including abusive language and discriminatory remarks, presenting growing
challenges for content moderation. Some users evade censorship by deliberately
disguising toxic words through homophonic cloak, which necessitates the task of
unveiling cloaked toxicity. Existing methods are mostly designed for English
texts, while Chinese cloaked toxicity unveiling has not been solved yet. To
tackle the issue, we propose C$^2$TU, a novel training-free and prompt-free
method for Chinese cloaked toxic content unveiling. It first employs substring
matching to identify candidate toxic words based on Chinese homo-graph and
toxic lexicon. Then it filters those candidates that are non-toxic and corrects
cloaks to be their corresponding toxicities. Specifically, we develop two model
variants for filtering, which are based on BERT and LLMs, respectively. For
LLMs, we address the auto-regressive limitation in computing word occurrence
probability and utilize the full semantic contexts of a text sequence to reveal
cloaked toxic words. Extensive experiments demonstrate that C$^2$TU can achieve
superior performance on two Chinese toxic datasets. In particular, our method
outperforms the best competitor by up to 71% on the F1 score and 35% on
accuracy, respectively.

</details>


### [64] [Let's Predict Sentence by Sentence](https://arxiv.org/abs/2505.22202)
*Hyeonbin Hwang,Byeongguk Jeon,Seungone Kim,Jiyeon Kim,Hoyeon Chang,Sohee Yang,Seungpil Won,Dohaeng Lee,Youbin Ahn,Minjoon Seo*

Main category: cs.CL

TL;DR: 该研究探讨预训练语言模型是否能在语义单元而非原始token序列上进行抽象推理，提出一种框架将token级模型提升至句子空间，并通过两种嵌入范式（语义嵌入和上下文嵌入）及两种推理机制（离散化和连续化）验证其效果。


<details>
  <summary>Details</summary>
Motivation: 人类推理基于高级抽象（如句子、命题），而自回归语言模型仅逐token生成。研究旨在探索预训练模型能否通过学习表征迁移到结构化语义推理空间。

Method: 提出框架将预训练模型适配到句子空间：1) 通过自编码学习语义嵌入；2) 通过下一句预测训练上下文嵌入。采用离散化（解码为文本）和连续化（纯嵌入空间推理）两种推理机制。

Result: 在数学、逻辑、常识和规划领域，连续推理下的上下文嵌入性能媲美思维链（CoT），且推理FLOPs平均减少50%。工具SentenceLens可解码中间状态为可解释句子。

Conclusion: 预训练模型可通过潜在嵌入空间有效实现结构化抽象推理，展现可扩展性和模块化适应潜力。

Abstract: Autoregressive language models (LMs) generate one token at a time, yet human
reasoning operates over higher-level abstractions - sentences, propositions,
and concepts. This contrast raises a central question- Can LMs likewise learn
to reason over structured semantic units rather than raw token sequences? In
this work, we investigate whether pretrained LMs can be lifted into such
abstract reasoning spaces by building on their learned representations. We
present a framework that adapts a pretrained token-level LM to operate in
sentence space by autoregressively predicting continuous embeddings of next
sentences. We explore two embedding paradigms inspired by classical
representation learning: 1) semantic embeddings, learned via autoencoding to
preserve surface meaning; and 2) contextual embeddings, trained via
next-sentence prediction to encode anticipatory structure. We evaluate both
under two inference regimes: Discretized, which decodes each predicted
embedding into text before re-encoding; and Continuous, which reasons entirely
in embedding space for improved efficiency. Across four domains - mathematics,
logic, commonsense, and planning - contextual embeddings under continuous
inference show competitive performance with Chain-of-Thought (CoT) while
reducing inference-time FLOPs on average by half. We also present early signs
of scalability and modular adaptation. Finally, to visualize latent
trajectories, we introduce SentenceLens, a diagnostic tool that decodes
intermediate model states into interpretable sentences. Together, our results
indicate that pretrained LMs can effectively transition to abstract, structured
reasoning within latent embedding spaces.

</details>


### [65] [Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models](https://arxiv.org/abs/2505.22232)
*Mehdi Ali,Manuel Brack,Max Lübbering,Elias Wendt,Abbas Goher Khan,Richard Rutmann,Alex Jude,Maurice Kraus,Alexander Arno Weber,Felix Stollenwerk,David Kaczér,Florian Mai,Lucie Flek,Rafet Sifa,Nicolas Flores-Herr,Joachim Köhler,Patrick Schramowski,Michael Fromm,Kristian Kersting*

Main category: cs.CL

TL;DR: 论文提出JQL方法，通过轻量级注释器高效筛选高质量多语言数据，显著提升下游模型训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有开源多语言数据集质量有限，依赖启发式过滤方法，制约跨语言迁移和扩展性。

Method: 利用预训练多语言嵌入构建轻量级注释器，蒸馏大模型标注能力实现高效数据筛选。

Result: 在35种语言上验证，JQL显著优于Fineweb2等现有方法，提高数据保留率和模型训练质量。

Conclusion: JQL为多语言数据筛选提供实用方案，提升了多语言数据集开发标准。

Abstract: High-quality multilingual training data is essential for effectively
pretraining large language models (LLMs). Yet, the availability of suitable
open-source multilingual datasets remains limited. Existing state-of-the-art
datasets mostly rely on heuristic filtering methods, restricting both their
cross-lingual transferability and scalability. Here, we introduce JQL, a
systematic approach that efficiently curates diverse and high-quality
multilingual data at scale while significantly reducing computational demands.
JQL distills LLMs' annotation capabilities into lightweight annotators based on
pretrained multilingual embeddings. These models exhibit robust multilingual
and cross-lingual performance, even for languages and scripts unseen during
training. Evaluated empirically across 35 languages, the resulting annotation
pipeline substantially outperforms current heuristic filtering methods like
Fineweb2. JQL notably enhances downstream model training quality and increases
data retention rates. Our research provides practical insights and valuable
resources for multilingual data curation, raising the standards of multilingual
dataset development.

</details>


### [66] [A Linguistically Motivated Analysis of Intonational Phrasing in Text-to-Speech Systems: Revealing Gaps in Syntactic Sensitivity](https://arxiv.org/abs/2505.22236)
*Charlotte Pouw,Afra Alishahi,Willem Zuidema*

Main category: cs.CL

TL;DR: TTS系统在句法边界模糊的句子中难以准确生成语调短语边界，需依赖逗号等表面线索；而在简单句法结构中能利用更深层次的句法线索。通过微调模型，可使其生成更符合底层结构的语调模式。


<details>
  <summary>Details</summary>
Motivation: 研究TTS系统对句法的敏感性，特别是语调短语边界的生成，以了解系统如何处理句法边界模糊的句子（如花园路径句或附着歧义句）。

Method: 采用受心理语言学启发的分析方法，针对有无逗号的句法边界位置微调TTS模型，促使其关注更细微的语言线索。

Result: TTS系统在句法模糊的句子中依赖表面标记（如逗号）定位边界，而在简单结构中能利用句法线索；微调后模型生成的语调模式更能反映底层结构。

Conclusion: TTS系统的语调生成受句法复杂度影响，通过针对性训练可提升其对深层语言线索的敏感性，从而改善语调边界预测。

Abstract: We analyze the syntactic sensitivity of Text-to-Speech (TTS) systems using
methods inspired by psycholinguistic research. Specifically, we focus on the
generation of intonational phrase boundaries, which can often be predicted by
identifying syntactic boundaries within a sentence. We find that TTS systems
struggle to accurately generate intonational phrase boundaries in sentences
where syntactic boundaries are ambiguous (e.g., garden path sentences or
sentences with attachment ambiguity). In these cases, systems need superficial
cues such as commas to place boundaries at the correct positions. In contrast,
for sentences with simpler syntactic structures, we find that systems do
incorporate syntactic cues beyond surface markers. Finally, we finetune models
on sentences without commas at the syntactic boundary positions, encouraging
them to focus on more subtle linguistic cues. Our findings indicate that this
leads to more distinct intonation patterns that better reflect the underlying
structure.

</details>


### [67] [BioHopR: A Benchmark for Multi-Hop, Multi-Answer Reasoning in Biomedical Domain](https://arxiv.org/abs/2505.22240)
*Yunsoo Kim,Yusuf Abdulle,Honghan Wu*

Main category: cs.CL

TL;DR: BioHopR是一个新的生物医学知识图谱基准，用于评估多跳、多答案推理能力，揭示当前模型在生物医学多跳推理中的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏评估生物医学领域多跳推理的能力，特别是涉及一对多和多对多关系的查询，导致生物医学多跳推理的关键挑战未被充分探索。

Method: 基于PrimeKG构建BioHopR基准，包含1跳和2跳推理任务，反映真实世界的生物医学复杂性。

Result: O3-mini在1跳任务中达到37.93%的精确率，2跳任务中达到14.57%，优于GPT4O等专有模型和HuatuoGPT-o1-70B等开源模型。

Conclusion: BioHopR为评估推理能力设定了新标准，揭示了专有模型和开源模型之间的关键差距，并为未来生物医学LLM的发展铺平了道路。

Abstract: Biomedical reasoning often requires traversing interconnected relationships
across entities such as drugs, diseases, and proteins. Despite the increasing
prominence of large language models (LLMs), existing benchmarks lack the
ability to evaluate multi-hop reasoning in the biomedical domain, particularly
for queries involving one-to-many and many-to-many relationships. This gap
leaves the critical challenges of biomedical multi-hop reasoning underexplored.
To address this, we introduce BioHopR, a novel benchmark designed to evaluate
multi-hop, multi-answer reasoning in structured biomedical knowledge graphs.
Built from the comprehensive PrimeKG, BioHopR includes 1-hop and 2-hop
reasoning tasks that reflect real-world biomedical complexities.
  Evaluations of state-of-the-art models reveal that O3-mini, a proprietary
reasoning-focused model, achieves 37.93% precision on 1-hop tasks and 14.57% on
2-hop tasks, outperforming proprietary models such as GPT4O and open-source
biomedical models including HuatuoGPT-o1-70B and Llama-3.3-70B. However, all
models exhibit significant declines in multi-hop performance, underscoring the
challenges of resolving implicit reasoning steps in the biomedical domain. By
addressing the lack of benchmarks for multi-hop reasoning in biomedical domain,
BioHopR sets a new standard for evaluating reasoning capabilities and
highlights critical gaps between proprietary and open-source models while
paving the way for future advancements in biomedical LLMs.

</details>


### [68] [MRT at SemEval-2025 Task 8: Maximizing Recovery from Tables with Multiple Steps](https://arxiv.org/abs/2505.22264)
*Maximiliano Hormazábal Lagos,Álvaro Bueno Saez,Héctor Cerezo-Costas,Pedro Alonso Doval,Jorge Alcalde Vesteiro*

Main category: cs.CL

TL;DR: 本文提出了一种利用LLMs生成Python代码与表格交互以回答问题的多步骤方法，在SemEval 2025任务8的子任务1中取得了70.50%的得分。


<details>
  <summary>Details</summary>
Motivation: 解决SemEval 2025任务8中基于表格数据的问答挑战，通过自动化方法提高问答的准确性和效率。

Method: 采用多步骤策略：理解表格内容，生成自然语言指令步骤，将指令转换为代码并执行，处理潜在错误。使用开源LLMs和针对每步任务优化的提示。

Result: 在子任务1中获得了70.50%的得分。

Conclusion: 通过代码生成与表格交互的方法有效解决了表格问答问题，展示了LLMs在此类任务中的潜力。

Abstract: In this paper we expose our approach to solve the \textit{SemEval 2025 Task
8: Question-Answering over Tabular Data} challenge. Our strategy leverages
Python code generation with LLMs to interact with the table and get the answer
to the questions. The process is composed of multiple steps: understanding the
content of the table, generating natural language instructions in the form of
steps to follow in order to get the answer, translating these instructions to
code, running it and handling potential errors or exceptions. These steps use
open source LLMs and fine grained optimized prompts for each task (step). With
this approach, we achieved a score of $70.50\%$ for subtask 1.

</details>


### [69] [Comprehensive Evaluation on Lexical Normalization: Boundary-Aware Approaches for Unsegmented Languages](https://arxiv.org/abs/2505.22273)
*Shohei Higashiyama,Masao Utiyama*

Main category: cs.CL

TL;DR: 该论文针对非正式表达处理问题，构建了大规模日语标准化数据集，并基于预训练模型开发了新方法，实验证明编码器和解码器方法均表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前词汇标准化研究缺乏全面评估，尤其在非分段语言中，难以确定哪种方法在多个评估维度上表现最佳。

Method: 构建了多领域日语标准化数据集，并基于先进预训练模型开发了标准化方法，进行了多角度实验评估。

Result: 实验表明，无论是仅编码器还是仅解码器方法，在准确性和效率方面都取得了令人满意的结果。

Conclusion: 该研究为日语非正式表达处理提供了有效工具，并验证了不同方法在多维度评估中的可行性。

Abstract: Lexical normalization research has sought to tackle the challenge of
processing informal expressions in user-generated text, yet the absence of
comprehensive evaluations leaves it unclear which methods excel across multiple
perspectives. Focusing on unsegmented languages, we make three key
contributions: (1) creating a large-scale, multi-domain Japanese normalization
dataset, (2) developing normalization methods based on state-of-the-art
pretrained models, and (3) conducting experiments across multiple evaluation
perspectives. Our experiments show that both encoder-only and decoder-only
approaches achieve promising results in both accuracy and efficiency.

</details>


### [70] [Natural Language Processing in Support of Evidence-based Medicine: A Scoping Review](https://arxiv.org/abs/2505.22280)
*Zihan Xu,Haotian Ma,Gongbo Zhang,Yihao Ding,Chunhua Weng,Yifan Peng*

Main category: cs.CL

TL;DR: 该综述探讨了自然语言处理（NLP）在循证医学（EBM）中的应用，总结了129项研究，展示了NLP如何支持EBM的五个关键步骤，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于医学文献数量庞大且快速增长，人工整理成本高昂，因此需要研究NLP方法来自动识别、评估、综合和传播EBM中的证据。

Method: 本文系统回顾了129项关于利用NLP支持EBM的研究，详细分析了NLP在EBM五个核心步骤（提问、获取、评估、应用和评价）中的应用。

Result: 综述表明，NLP在提升证据提取、综合、评估和总结方面具有重要作用，能优化临床决策流程，但也指出了当前领域的局限性。

Conclusion: NLP有潜力通过改进证据处理和临床工作流程来革新EBM，未来研究应关注技术优化和应用扩展。

Abstract: Evidence-based medicine (EBM) is at the forefront of modern healthcare,
emphasizing the use of the best available scientific evidence to guide clinical
decisions. Due to the sheer volume and rapid growth of medical literature and
the high cost of curation, there is a critical need to investigate Natural
Language Processing (NLP) methods to identify, appraise, synthesize, summarize,
and disseminate evidence in EBM. This survey presents an in-depth review of 129
research studies on leveraging NLP for EBM, illustrating its pivotal role in
enhancing clinical decision-making processes. The paper systematically explores
how NLP supports the five fundamental steps of EBM -- Ask, Acquire, Appraise,
Apply, and Assess. The review not only identifies current limitations within
the field but also proposes directions for future research, emphasizing the
potential for NLP to revolutionize EBM by refining evidence extraction,
evidence synthesis, appraisal, summarization, enhancing data comprehensibility,
and facilitating a more efficient clinical workflow.

</details>


### [71] [Compensating for Data with Reasoning: Low-Resource Machine Translation with LLMs](https://arxiv.org/abs/2505.22293)
*Samuel Frontull,Thomas Ströhle*

Main category: cs.CL

TL;DR: 该论文提出了一种名为Fragment-Shot Prompting的新型上下文学习方法，用于提升低资源语言的机器翻译质量，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在多语言机器翻译中表现出色，但在低资源语言翻译中仍面临挑战，尤其是在提示工程方面。本文旨在解决这一问题。

Method: 提出了Fragment-Shot Prompting方法，通过分割输入并基于语法覆盖检索翻译示例，以及其扩展方法Pivoted Fragment-Shot，实现无需直接平行数据的翻译。

Result: 实验表明：(1) Fragment-Shot Prompting对低资源语言翻译有效；(2) 推理能力强的模型能更好地利用检索知识；(3) 从低资源到高资源语言翻译时，提示工程改进有限。

Conclusion: Fragment-Shot Prompting显著提升了低资源语言翻译质量，尤其是在模型推理能力较强时效果更佳，但提示工程对某些翻译方向的改进有限。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
multilingual machine translation, sometimes even outperforming traditional
neural systems. However, previous research has highlighted the challenges of
using LLMs, particularly with prompt engineering, for low-resource languages.
In this work, we introduce Fragment-Shot Prompting, a novel in-context learning
method that segments input and retrieves translation examples based on
syntactic coverage, along with Pivoted Fragment-Shot, an extension that enables
translation without direct parallel data. We evaluate these methods using
GPT-3.5, GPT-4o, o1-mini, LLaMA-3.3, and DeepSeek-R1 for translation between
Italian and two Ladin variants, revealing three key findings: (1) Fragment-Shot
Prompting is effective for translating into and between the studied
low-resource languages, with syntactic coverage positively correlating with
translation quality; (2) Models with stronger reasoning abilities make more
effective use of retrieved knowledge, generally produce better translations,
and enable Pivoted Fragment-Shot to significantly improve translation quality
between the Ladin variants; and (3) prompt engineering offers limited, if any,
improvements when translating from a low-resource to a high-resource language,
where zero-shot prompting already yields satisfactory results. We publicly
release our code and the retrieval corpora.

</details>


### [72] [360-LLaMA-Factory: Plug & Play Sequence Parallelism for Long Post-Training](https://arxiv.org/abs/2505.22296)
*Haosheng Zou,Xiaowei Lv,Shousheng Jia,Xiangzheng Zhang*

Main category: cs.CL

TL;DR: 360-LLaMA-Factory开源项目通过引入序列并行技术，被多个模型和公司采用，技术报告深入探讨了其序列并行模式及实现见解。


<details>
  <summary>Details</summary>
Motivation: 为了提升LLaMA-Factory的性能和应用范围，作者引入了序列并行技术，并开源了360-LLaMA-Factory项目。

Method: 在360-LLaMA-Factory中实现了不同的序列并行模式，并分享了相关的实现细节和技术见解。

Result: 360-LLaMA-Factory获得了广泛认可，被应用于多个模型和大型公司的训练框架中。

Conclusion: 通过序列并行技术的引入和开源项目的推广，360-LLaMA-Factory在模型训练领域取得了显著成效。

Abstract: Adding sequence parallelism into LLaMA-Factory, we open-sourced
360-LLaMA-Factory at https://github.com/Qihoo360/360-LLaMA-Factory.
360-LLaMA-Factory has received wide recognition and used in models such as
Light-R1 arXiv:2503.10460, TinyR1 arXiv:2503.04872, Kaggle AIMO math models and
also in large companies' training frameworks. This technical report delves
deeper into the different sequence parallel modes behind 360-LLaMA-Factory and
discusses our implementation insights.

</details>


### [73] [Adaptive Detoxification: Safeguarding General Capabilities of LLMs through Toxicity-Aware Knowledge Editing](https://arxiv.org/abs/2505.22298)
*Yifan Lu,Jing Li,Yigeng Zhou,Yihui Zhang,Wenya Wang,Xiucheng Li,Meishan Zhang,Fangming Liu,Jun Yu,Min Zhang*

Main category: cs.CL

TL;DR: 提出ToxEdit方法，通过动态检测毒性激活模式并自适应调整计算路径，有效减轻大语言模型毒性同时保持其通用能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型解毒方法存在对无明确实体的对抗输入无效及过度编辑导致模型拒绝合法查询的问题。

Method: 采用毒性感知知识编辑，动态检测前向传播中的毒性激活模式，并通过自适应层间路径路由计算。

Result: 实验表明ToxEdit在解毒效果和保持模型通用能力上优于现有方法。

Conclusion: ToxEdit能精准减轻毒性同时保持大语言模型的通用能力。

Abstract: Large language models (LLMs) exhibit impressive language capabilities but
remain vulnerable to malicious prompts and jailbreaking attacks. Existing
knowledge editing methods for LLM detoxification face two major challenges.
First, they often rely on entity-specific localization, making them ineffective
against adversarial inputs without explicit entities. Second, these methods
suffer from over-editing, where detoxified models reject legitimate queries,
compromising overall performance. In this paper, we propose ToxEdit, a
toxicity-aware knowledge editing approach that dynamically detects toxic
activation patterns during forward propagation. It then routes computations
through adaptive inter-layer pathways to mitigate toxicity effectively. This
design ensures precise toxicity mitigation while preserving LLMs' general
capabilities. To more accurately assess over-editing, we also enhance the
SafeEdit benchmark by incorporating instruction-following evaluation tasks.
Experimental results on multiple LLMs demonstrate that our ToxEdit outperforms
previous state-of-the-art methods in both detoxification performance and
safeguarding general capabilities of LLMs.

</details>


### [74] [If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?](https://arxiv.org/abs/2505.22318)
*Ishwar B Balappanawar,Vamshi Krishna Bonagiri,Anish R Joishy,Manas Gaur,Krishnaprasad Thirunarayan,Ponnurangam Kumaraguru*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型在反事实情境下的逻辑推理能力，提出了新方法Self-Segregate以提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在知识冲突情境下的逻辑推理能力退化问题。

Method: 构建CounterLogic数据集，提出Self-Segregate提示方法增强元认知意识。

Result: 新方法将性能差距从27%降至11%，整体准确率提升7.5%。

Conclusion: 研究为提升语言模型在现实应用中的逻辑推理能力提供了实用见解。

Abstract: Large Language Models (LLMs) demonstrate impressive reasoning capabilities in
familiar contexts, but struggle when the context conflicts with their
parametric knowledge. To investigate this phenomenon, we introduce
CounterLogic, a dataset containing 1,800 examples across 9 logical schemas,
explicitly designed to evaluate logical reasoning through counterfactual
(hypothetical knowledge-conflicting) scenarios. Our systematic evaluation of 11
LLMs across 6 different datasets reveals a consistent performance degradation,
with accuracies dropping by 27% on average when reasoning through
counterfactual information. We propose Self-Segregate, a prompting method
enabling metacognitive awareness (explicitly identifying knowledge conflicts)
before reasoning. Our method dramatically narrows the average performance gaps
from 27% to just 11%, while significantly increasing the overall accuracy
(+7.5%). We discuss the implications of these findings and draw parallels to
human cognitive processes, particularly on how humans disambiguate conflicting
information during reasoning tasks. Our findings offer practical insights for
understanding and enhancing LLMs reasoning capabilities in real-world
applications, especially where models must logically reason independently of
their factual knowledge.

</details>


### [75] [Advancing Expert Specialization for Better MoE](https://arxiv.org/abs/2505.22323)
*Hongcan Guo,Haolang Lu,Guoshun Nan,Bolun Chu,Jialin Zhuang,Yuan Yang,Wenhao Che,Sicong Leng,Qimei Cui,Xudong Jiang*

Main category: cs.CL

TL;DR: 提出正交性和方差损失函数，提升MoE模型中专家专业化程度，性能提升达23.79%。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型的辅助负载均衡损失导致专家重叠和路由趋同，限制了专家专业化并影响训练后性能。

Method: 引入正交性损失促进专家处理不同类型标记，方差损失增强路由判别性，与现有损失函数兼容。

Result: 在多种架构和基准测试中显著提升专家专业化，性能最高提升23.79%，且保持下游任务负载均衡。

Conclusion: 所提方法无需修改架构即可有效优化MoE训练，代码将开源。

Abstract: Mixture-of-Experts (MoE) models enable efficient scaling of large language
models (LLMs) by activating only a subset of experts per input. However, we
observe that the commonly used auxiliary load balancing loss often leads to
expert overlap and overly uniform routing, which hinders expert specialization
and degrades overall performance during post-training. To address this, we
propose a simple yet effective solution that introduces two complementary
objectives: (1) an orthogonality loss to encourage experts to process distinct
types of tokens, and (2) a variance loss to encourage more discriminative
routing decisions. Gradient-level analysis demonstrates that these objectives
are compatible with the existing auxiliary loss and contribute to optimizing
the training process. Experimental results over various model architectures and
across multiple benchmarks show that our method significantly enhances expert
specialization. Notably, our method improves classic MoE baselines with
auxiliary loss by up to 23.79%, while also maintaining load balancing in
downstream tasks, without any architectural modifications or additional
components. We will release our code to contribute to the community.

</details>


### [76] [NLP for Social Good: A Survey of Challenges, Opportunities, and Responsible Deployment](https://arxiv.org/abs/2505.22327)
*Antonia Karamolegkou,Angana Borah,Eunjung Cho,Sagnik Ray Choudhury,Martina Galletti,Rajarshi Ghosh,Pranav Gupta,Oana Ignat,Priyanka Kargupta,Neema Kotonya,Hemank Lamba,Sun-Joo Lee,Arushi Mangla,Ishani Mondal,Deniz Nazarova,Poli Nemkova,Dina Pisarevskaya,Naquee Rizwan,Nazanin Sabri,Dominik Stammbach,Anna Steinberg,David Tomás,Steven R Wilson,Bowen Yi,Jessica H Zhu,Arkaitz Zubiaga,Anders Søgaard,Alexander Fraser,Zhijing Jin,Rada Mihalcea,Joel R. Tetreault,Daryna Dementieva*

Main category: cs.CL

TL;DR: 本文探讨了NLP在应对社会挑战中的角色，强调需以责任和公平推动NLP4SG研究。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，NLP领域需要更负责任地部署技术以解决社会问题。

Method: 通过跨学科分析社会目标和新兴风险，提出研究方向。

Result: 指出了NLP4SG研究中的潜在机遇与需克服的挑战。

Conclusion: 呼吁以更谨慎和公平的方式推进NLP技术的社会应用。

Abstract: Recent advancements in large language models (LLMs) have unlocked
unprecedented possibilities across a range of applications. However, as a
community, we believe that the field of Natural Language Processing (NLP) has a
growing need to approach deployment with greater intentionality and
responsibility. In alignment with the broader vision of AI for Social Good
(Toma\v{s}ev et al., 2020), this paper examines the role of NLP in addressing
pressing societal challenges. Through a cross-disciplinary analysis of social
goals and emerging risks, we highlight promising research directions and
outline challenges that must be addressed to ensure responsible and equitable
progress in NLP4SG research.

</details>


### [77] [Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start](https://arxiv.org/abs/2505.22334)
*Lai Wei,Yuting Li,Kaipeng Zheng,Chen Wang,Yue Wang,Linghe Kong,Lichao Sun,Weiran Huang*

Main category: cs.CL

TL;DR: 该论文探讨了多模态大语言模型（MLLMs）中的自我修正模式，并提出了一种结合监督微调（SFT）和强化学习（GRPO）的两阶段方法，显著提升了多模态推理性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索多模态大语言模型中自我修正模式（“顿悟时刻”）是否真正源于强化学习训练，以及如何有效结合监督学习和强化学习来提升推理能力。

Method: 采用两阶段方法：首先通过监督微调（SFT）引入结构化思维链推理模式，随后通过GRPO强化学习进一步优化模型能力。

Result: 实验结果表明，该方法在多个多模态推理基准测试中优于仅使用SFT或强化学习的方法，3B和7B模型均达到开源MLLMs中的领先水平。

Conclusion: 该研究为构建先进多模态推理模型提供了实用指导，证明了结合监督学习和强化学习的有效性。

Abstract: Recent advancements in large language models (LLMs) have demonstrated
impressive chain-of-thought reasoning capabilities, with reinforcement learning
(RL) playing a crucial role in this progress. While "aha moment"
patterns--where models exhibit self-correction through reflection--are often
attributed to emergent properties from RL, we first demonstrate that these
patterns exist in multimodal LLMs (MLLMs) prior to RL training but may not
necessarily correlate with improved reasoning performance. Building on these
insights, we present a comprehensive study on enhancing multimodal reasoning
through a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start
with structured chain-of-thought reasoning patterns, followed by (2)
reinforcement learning via GRPO to further refine these capabilities. Our
extensive experiments show that this combined approach consistently outperforms
both SFT-only and RL-only methods across challenging multimodal reasoning
benchmarks. The resulting models achieve state-of-the-art performance among
open-source MLLMs at both 3B and 7B scales, with our 7B model showing
substantial improvements over base models (e.g., 66.3 %$\rightarrow$73.4 % on
MathVista, 62.9 %$\rightarrow$70.4 % on We-Math) and our 3B model achieving
performance competitive with several 7B models. Overall, this work provides
practical guidance for building advanced multimodal reasoning models. Our code
is available at https://github.com/waltonfuture/RL-with-Cold-Start.

</details>


### [78] [Text2Grad: Reinforcement Learning from Natural Language Feedback](https://arxiv.org/abs/2505.22338)
*Hanyang Wang,Lu Wang,Chaoyun Zhang,Tianjun Mao,Si Qin,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.CL

TL;DR: Text2Grad提出了一种将文本反馈转化为细粒度梯度的方法，优化语言模型的策略，比传统RLHF更精确和可解释。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF使用粗粒度的标量奖励，掩盖了成功或失败的细节原因，导致学习缓慢且不透明。近期工作通过提示或反思增加文本批评，但未改变模型参数。

Method: Text2Grad通过三个组件实现：反馈标注流程、细粒度奖励模型和基于跨度的策略优化器，将文本反馈转化为跨度的梯度信号。

Result: 在摘要、代码生成和问答任务中，Text2Grad表现优于传统RL和仅提示基线，任务指标和可解释性均有提升。

Conclusion: 将自然语言反馈转化为梯度信号，是一种强大的细粒度策略优化方法。

Abstract: Traditional RLHF optimizes language models with coarse, scalar rewards that
mask the fine-grained reasons behind success or failure, leading to slow and
opaque learning. Recent work augments RL with textual critiques through
prompting or reflection, improving interpretability but leaving model
parameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm
that turns free-form textual feedback into span-level gradients. Given human
(or programmatic) critiques, Text2Grad aligns each feedback phrase with the
relevant token spans, converts these alignments into differentiable reward
signals, and performs gradient updates that directly refine the offending
portions of the model's policy. This yields precise, feedback-conditioned
adjustments instead of global nudges. Text2Grad is realized through three
components: (1) a high-quality feedback-annotation pipeline that pairs
critiques with token spans; (2) a fine-grained reward model that predicts
span-level reward on answer while generating explanatory critiques; and (3) a
span-level policy optimizer that back-propagates natural-language gradients.
Across summarization, code generation, and question answering, Text2Grad
consistently surpasses scalar-reward RL and prompt-only baselines, providing
both higher task metrics and richer interpretability. Our results demonstrate
that natural-language feedback, when converted to gradients, is a powerful
signal for fine-grained policy optimization. The code for our method is
available at https://github.com/microsoft/Text2Grad

</details>


### [79] [LLMs Struggle to Reject False Presuppositions when Misinformation Stakes are High](https://arxiv.org/abs/2505.22354)
*Judith Sieker,Clara Lachenmaier,Sina Zarrieß*

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型(LLMs)如何处理错误预设，并分析了语言因素对其识别能力的影响，发现模型普遍难以识别错误预设。


<details>
  <summary>Details</summary>
Motivation: 由于预设能巧妙地将争议或虚假信息包装为既定事实，研究者担忧LLMs可能像人类一样无法识别和纠正错误预设，从而加剧政治虚假信息的传播。

Method: 基于语言学预设分析框架，研究者构建新数据集，测试GPT-4-o、LLama-3-8B和Mistral-7B-v03三种模型在不同政治语境下对错误预设的敏感度，考察语言结构、政党倾向和情景概率等因素的影响。

Result: 实验表明所有测试模型均难以有效识别错误预设，其表现受预设构建方式、政治立场等因素显著影响，不同模型间存在差异。

Conclusion: 语言学预设分析能有效揭示LLMs强化政治虚假信息的机制，突显当前模型在识别隐性错误信息方面的重大缺陷。

Abstract: This paper examines how LLMs handle false presuppositions and whether certain
linguistic factors influence their responses to falsely presupposed content.
Presuppositions subtly introduce information as given, making them highly
effective at embedding disputable or false information. This raises concerns
about whether LLMs, like humans, may fail to detect and correct misleading
assumptions introduced as false presuppositions, even when the stakes of
misinformation are high. Using a systematic approach based on linguistic
presupposition analysis, we investigate the conditions under which LLMs are
more or less sensitive to adopt or reject false presuppositions. Focusing on
political contexts, we examine how factors like linguistic construction,
political party, and scenario probability impact the recognition of false
presuppositions. We conduct experiments with a newly created dataset and
examine three LLMs: OpenAI's GPT-4-o, Meta's LLama-3-8B, and MistralAI's
Mistral-7B-v03. Our results show that the models struggle to recognize false
presuppositions, with performance varying by condition. This study highlights
that linguistic presupposition analysis is a valuable tool for uncovering the
reinforcement of political misinformation in LLM responses.

</details>


### [80] [Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition](https://arxiv.org/abs/2505.22375)
*Hanting Chen,Yasheng Wang,Kai Han,Dong Li,Lin Li,Zhenni Bi,Jinpeng Li,Haoyu Wang,Fei Mi,Mingjian Zhu,Bin Wang,Kaikai Song,Yifei Fu,Xu He,Yu Luo,Chong Zhu,Quan He,Xueyu Wu,Wei He,Hailin Hu,Yehui Tang,Dacheng Tao,Xinghao Chen,Yunhe Wang,Other Contributors*

Main category: cs.CL

TL;DR: Pangu Embedded是一个高效的大型语言模型推理器，通过两阶段训练框架实现快速和慢速思维模式，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有推理优化的LLM存在计算成本高和推理延迟的问题，Pangu Embedded旨在解决这些挑战。

Method: 采用两阶段训练框架：第一阶段通过迭代蒸馏和强化学习优化模型；第二阶段引入双系统框架，支持快速和慢速推理模式。

Result: 在AIME 2024等基准测试中，7B参数的Pangu Embedded表现优于同类模型，如Qwen3-8B和GLM4-9B。

Conclusion: Pangu Embedded展示了在统一架构中实现高效推理的潜力，为实际部署提供了有前景的方向。

Abstract: This work presents Pangu Embedded, an efficient Large Language Model (LLM)
reasoner developed on Ascend Neural Processing Units (NPUs), featuring flexible
fast and slow thinking capabilities. Pangu Embedded addresses the significant
computational costs and inference latency challenges prevalent in existing
reasoning-optimized LLMs. We propose a two-stage training framework for its
construction. In Stage 1, the model is finetuned via an iterative distillation
process, incorporating inter-iteration model merging to effectively aggregate
complementary knowledge. This is followed by reinforcement learning on Ascend
clusters, optimized by a latency-tolerant scheduler that combines stale
synchronous parallelism with prioritized data queues. The RL process is guided
by a Multi-source Adaptive Reward System (MARS), which generates dynamic,
task-specific reward signals using deterministic metrics and lightweight LLM
evaluators for mathematics, coding, and general problem-solving tasks. Stage 2
introduces a dual-system framework, endowing Pangu Embedded with a "fast" mode
for routine queries and a deeper "slow" mode for complex inference. This
framework offers both manual mode switching for user control and an automatic,
complexity-aware mode selection mechanism that dynamically allocates
computational resources to balance latency and reasoning depth. Experimental
results on benchmarks including AIME 2024, GPQA, and LiveCodeBench demonstrate
that Pangu Embedded with 7B parameters, outperforms similar-size models like
Qwen3-8B and GLM4-9B. It delivers rapid responses and state-of-the-art
reasoning quality within a single, unified model architecture, highlighting a
promising direction for developing powerful yet practically deployable LLM
reasoners.

</details>


### [81] [RAG-Zeval: Towards Robust and Interpretable Evaluation on RAG Responses through End-to-End Rule-Guided Reasoning](https://arxiv.org/abs/2505.22430)
*Kun Li,Yunxiang Li,Tianhua Zhang,Hongyin Luo,Xixin Wu,James Glass,Helen Meng*

Main category: cs.CL

TL;DR: RAG-Zeval是一个新型端到端框架，通过强化学习训练评估器，以规则引导的推理任务评估RAG系统的忠实性和正确性，显著降低计算成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的评估框架主要依赖资源密集型模型的多阶段提示，未能充分利用模型的推理能力且计算成本高，因此需要更高效的评估方法。

Method: 提出RAG-Zeval框架，将评估任务转化为规则引导的推理任务，通过强化学习训练评估器，并采用基于排名的奖励机制，无需人工标注生成质量可控的响应。

Result: RAG-Zeval在性能上优于基线模型，与人类判断的相关性最强，且参数量仅为基线模型的1/10到1/100，同时展现出更好的解释性。

Conclusion: RAG-Zeval通过高效的评估方法和强化学习训练，显著提升了RAG系统的评估性能，同时降低了计算成本，具有广泛的应用潜力。

Abstract: Robust evaluation is critical for deploying trustworthy retrieval-augmented
generation (RAG) systems. However, current LLM-based evaluation frameworks
predominantly rely on directly prompting resource-intensive models with complex
multi-stage prompts, underutilizing models' reasoning capabilities and
introducing significant computational cost. In this paper, we present RAG-Zeval
(RAG-Zero Evaluator), a novel end-to-end framework that formulates faithfulness
and correctness evaluation as a rule-guided reasoning task. Our approach trains
evaluators with reinforcement learning, facilitating compact models to generate
comprehensive and sound assessments with detailed explanation in one-pass. We
introduce a ranking-based outcome reward mechanism, using preference judgments
rather than absolute scores, to address the challenge of obtaining precise
pointwise reward signals. To this end, we synthesize the ranking references by
generating quality-controlled responses with zero human annotation. Experiments
demonstrate RAG-Zeval's superior performance, achieving the strongest
correlation with human judgments and outperforming baselines that rely on LLMs
with 10-100 times more parameters. Our approach also exhibits superior
interpretability in response evaluation.

</details>


### [82] [Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO](https://arxiv.org/abs/2505.22453)
*Lai Wei,Yuting Li,Chen Wang,Yue Wang,Linghe Kong,Weiran Huang,Lichao Sun*

Main category: cs.CL

TL;DR: 论文提出了一种名为MM-UPT的无监督后训练框架，利用GRPO算法和自奖励机制，显著提升了多模态大语言模型的推理能力，无需外部监督。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型（MLLMs）后训练方法依赖昂贵的标注数据或复杂的无监督方法，难以持续迭代。本文旨在探索一种简单有效的无监督后训练方法，实现模型的持续自我提升。

Method: MM-UPT框架基于GRPO算法，采用多数投票的自奖励机制替代传统奖励信号，并结合模型自身生成的合成问题进一步提升性能。

Result: 实验表明，MM-UPT显著提升了Qwen2.5-VL-7B的推理能力（如MathVista任务从66.3%提升至72.9%），且性能优于现有无监督基线，接近有监督GRPO的结果。

Conclusion: MM-UPT为无外部监督条件下多模态大语言模型的持续自主增强提供了新范式，展示了可扩展自我提升的潜力。

Abstract: Improving Multi-modal Large Language Models (MLLMs) in the post-training
stage typically relies on supervised fine-tuning (SFT) or reinforcement
learning (RL). However, these supervised methods require expensive and manually
annotated multi-modal data--an ultimately unsustainable resource. While recent
efforts have explored unsupervised post-training, their methods are complex and
difficult to iterate. In this work, we are the first to investigate the use of
GRPO, a stable and scalable online RL algorithm, for enabling continual
self-improvement without any external supervision. We propose MM-UPT, a simple
yet effective framework for unsupervised post-training of MLLMs. MM-UPT builds
upon GRPO, replacing traditional reward signals with a self-rewarding mechanism
based on majority voting over multiple sampled responses. Our experiments
demonstrate that MM-UPT significantly improves the reasoning ability of
Qwen2.5-VL-7B (e.g., 66.3 %$\rightarrow$72.9 % on MathVista, 62.9
%$\rightarrow$68.7 % on We-Math), using standard dataset without ground truth
labels. MM-UPT also outperforms prior unsupervised baselines and even
approaches the results of supervised GRPO. Furthermore, we show that
incorporating synthetic questions, generated solely by MLLM itself, can boost
performance as well, highlighting a promising approach for scalable
self-improvement. Overall, MM-UPT offers a new paradigm for continual,
autonomous enhancement of MLLMs in the absence of external supervision. Our
code is available at https://github.com/waltonfuture/MM-UPT.

</details>


### [83] [EvolveSearch: An Iterative Self-Evolving Search Agent](https://arxiv.org/abs/2505.22501)
*Dingchu Zhang,Yida Zhao,Jialong Wu,Baixuan Li,Wenbiao Yin,Liwen Zhang,Yong Jiang,Yufeng Li,Kewei Tu,Pengjun Xie,Fei Huang*

Main category: cs.CL

TL;DR: 提出了EvolveSearch框架，结合SFT和RL提升LLM在开放网络搜索领域的自主进化能力，无需人工标注数据，在7个多跳问答基准上平均提升4.7%。


<details>
  <summary>Details</summary>
Motivation: 当前主流方法（监督微调和强化学习）在开放搜索领域存在数据生产效率低和收敛过快的问题，需要一种无需人工标注的自主进化方案。

Method: 提出迭代式自我进化框架EvolveSearch，融合监督微调(SFT)和强化学习(RL)来增强网络搜索能力。

Result: 在7个多跳问答基准测试中性能持续提升，最终平均超越现有最佳方法4.7%。

Conclusion: 该框架为开放网络搜索领域的智能体自主进化能力提供了新思路。

Abstract: The rapid advancement of large language models (LLMs) has transformed the
landscape of agentic information seeking capabilities through the integration
of tools such as search engines and web browsers. However, current mainstream
approaches for enabling LLM web search proficiency face significant challenges:
supervised fine-tuning struggles with data production in open-search domains,
while RL converges quickly, limiting their data utilization efficiency. To
address these issues, we propose EvolveSearch, a novel iterative self-evolution
framework that combines SFT and RL to enhance agentic web search capabilities
without any external human-annotated reasoning data. Extensive experiments on
seven multi-hop question-answering (MHQA) benchmarks demonstrate that
EvolveSearch consistently improves performance across iterations, ultimately
achieving an average improvement of 4.7\% over the current state-of-the-art
across seven benchmarks, opening the door to self-evolution agentic
capabilities in open web search domains.

</details>


### [84] [Multi-MLLM Knowledge Distillation for Out-of-Context News Detection](https://arxiv.org/abs/2505.22517)
*Yimeng Gu,Zhao Tong,Ignacio Castro,Shu Wu,Gareth Tyson*

Main category: cs.CL

TL;DR: 本文提出了一种两阶段知识蒸馏框架，通过利用教师模型生成的标签预测和推理，以更低的标注成本和计算开销提升小型多模态大语言模型在上下文无关新闻检测中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在检测上下文无关新闻时，要么需要大量标注数据进行微调，要么依赖昂贵的GPT模型API调用，这在资源有限的情况下不切实际。本文旨在以更高效和经济的方式提升小型模型的性能。

Method: 采用两阶段知识蒸馏框架：第一阶段使用LoRA微调学生模型；第二阶段在教师模型预测冲突的数据点上结合LoRA和DPO进行微调，以学习更具挑战性的模式。

Result: 实验结果表明，该方法仅使用不到10%的标注数据就达到了最先进的性能。

Conclusion: 通过两阶段知识蒸馏，本文方法在降低标注成本和计算开销的同时，有效提升了小型多模态大语言模型在上下文无关新闻检测任务中的性能。

Abstract: Multimodal out-of-context news is a type of misinformation in which the image
is used outside of its original context. Many existing works have leveraged
multimodal large language models (MLLMs) for detecting out-of-context news.
However, observing the limited zero-shot performance of smaller MLLMs, they
generally require label-rich fine-tuning and/or expensive API calls to GPT
models to improve the performance, which is impractical in low-resource
scenarios. In contrast, we aim to improve the performance of small MLLMs in a
more label-efficient and cost-effective manner. To this end, we first prompt
multiple teacher MLLMs to generate both label predictions and corresponding
rationales, which collectively serve as the teachers' knowledge. We then
introduce a two-stage knowledge distillation framework to transfer this
knowledge to a student MLLM. In Stage 1, we apply LoRA fine-tuning to the
student model using all training data. In Stage 2, we further fine-tune the
student model using both LoRA fine-tuning and DPO on the data points where
teachers' predictions conflict. This two-stage strategy reduces annotation
costs and helps the student model uncover subtle patterns in more challenging
cases. Experimental results demonstrate that our approach achieves
state-of-the-art performance using less than 10% labeled data.

</details>


### [85] [Emotion-o1: Adaptive Long Reasoning for Emotion Understanding in LLMs](https://arxiv.org/abs/2505.22548)
*Changhao Song,Yazhou Zhang,Peng Zhang*

Main category: cs.CL

TL;DR: 提出一种任务自适应的情感理解推理框架，通过动态调整推理链长度提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法采用固定长度的思维链推理，无法适应情感任务的复杂度差异。

Method: 结合微调与强化学习，设计复合奖励函数实现动态推理深度控制和多样化推理路径。

Result: 在情感/情绪（基础任务）和幽默/讽刺（高级任务）上分别取得最高3.56%和37.95%的F1提升。

Conclusion: 自适应深度推理架起了固定思维链与情感复杂性之间的桥梁。

Abstract: Emotion understanding includes basic tasks (e.g., sentiment/emotion
classification) and advanced tasks (e.g., sarcasm/humor detection). Current
methods rely on fixed-length CoT reasoning, failing to adapt to the varying
complexity of emotions. We propose a task-adaptive reasoning framework that
employs DeepSeek-R1 to generate variable-length reasoning chains for different
emotion tasks. By combining fine-tuning with reinforcement learning, we design
a composite reward function that balances four objectives: prediction accuracy,
adaptive reasoning depth control, structural diversity in reasoning paths, and
suppression of repetitive logic. This approach achieves dynamic
context-sensitive inference while enabling LLMs to autonomously develop deep
reasoning capabilities. Experimental results demonstrate consistent
improvements in both Acc and F1 scores across four tasks: emotion, sentiment,
humor, and sarcasm. Notably, peak enhancements reached 3.56% F1 (2.76% Acc) for
basic tasks and 37.95% F1 (23.14% Acc) for advanced tasks. Our work bridges
rigid CoT reasoning and emotional complexity through adaptive-depth analysis.

</details>


### [86] [ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation with Lightweight Specialized LLM](https://arxiv.org/abs/2505.22552)
*Hoang Pham,Thanh-Do Nguyen,Khac-Hoai Nam Bui*

Main category: cs.CL

TL;DR: ClaimPKG框架通过将知识图谱与大型语言模型结合，提升声明验证的推理能力，实现了当前最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有声明验证方法主要依赖非结构化文本，难以有效利用知识图谱的结构化知识；同时大型语言模型在未经适配时难以进行多步推理。

Method: 提出ClaimPKG框架：使用轻量级专用LLM生成伪子图表示声明，通过子图检索模块获取相关KG子图，最后由通用LLM生成最终判断。

Result: 在FactKG数据集上准确率超越基线9%-12%，并在HoVer和FEVEROUS等非结构化数据集上展现零样本泛化能力。

Conclusion: ClaimPKG成功将知识图谱结构化知识与LLM推理能力结合，为声明验证领域提供了有效解决方案。

Abstract: Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of
large language models (LLMs) is an emerging research challenge in claim
verification. While KGs provide structured, semantically rich representations
well-suited for reasoning, most existing verification methods rely on
unstructured text corpora, limiting their ability to effectively leverage KGs.
Additionally, despite possessing strong reasoning abilities, modern LLMs
struggle with multi-step modular pipelines and reasoning over KGs without
adaptation. To address these challenges, we propose ClaimPKG, an end-to-end
framework that seamlessly integrates LLM reasoning with structured knowledge
from KGs. Specifically, the main idea of ClaimPKG is to employ a lightweight,
specialized LLM to represent the input claim as pseudo-subgraphs, guiding a
dedicated subgraph retrieval module to identify relevant KG subgraphs. These
retrieved subgraphs are then processed by a general-purpose LLM to produce the
final verdict and justification. Extensive experiments on the FactKG dataset
demonstrate that ClaimPKG achieves state-of-the-art performance, outperforming
strong baselines in this research field by 9%-12% accuracy points across
multiple categories. Furthermore, ClaimPKG exhibits zero-shot generalizability
to unstructured datasets such as HoVer and FEVEROUS, effectively combining
structured knowledge from KGs with LLM reasoning across various LLM backbones.

</details>


### [87] [Do Large Language Models Think Like the Brain? Sentence-Level Evidence from fMRI and Hierarchical Embeddings](https://arxiv.org/abs/2505.22563)
*Yu Lei,Xingyang Ge,Yi Zhang,Yiming Yang,Bolei Ma*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）与人脑在语言处理上的计算原理是否一致，发现模型性能提升使其表征结构更接近人脑层次。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs与人脑在语言处理计算原理上的相似性，验证观察到的脑类似模式是源于规模扩展还是更深层次的结构对齐。

Method: 通过比较14个公开LLMs的分层表征与人类受试者在自然叙事故事中的fMRI数据，构建句子级神经预测模型。

Result: 模型性能改进推动表征结构向脑类似层次演化，尤其在更高语义抽象水平上实现更强的功能和解剖对应。

Conclusion: LLMs的表征架构随性能提升逐渐逼近人脑语言处理的层次结构，支持两者在高级语义处理上的对齐。

Abstract: Understanding whether large language models (LLMs) and the human brain
converge on similar computational principles remains a fundamental and
important question in cognitive neuroscience and AI. Do the brain-like patterns
observed in LLMs emerge simply from scaling, or do they reflect deeper
alignment with the architecture of human language processing? This study
focuses on the sentence-level neural mechanisms of language models,
systematically investigating how hierarchical representations in LLMs align
with the dynamic neural responses during human sentence comprehension. By
comparing hierarchical embeddings from 14 publicly available LLMs with fMRI
data collected from participants, who were exposed to a naturalistic narrative
story, we constructed sentence-level neural prediction models to precisely
identify the model layers most significantly correlated with brain region
activations. Results show that improvements in model performance drive the
evolution of representational architectures toward brain-like hierarchies,
particularly achieving stronger functional and anatomical correspondence at
higher semantic abstraction levels.

</details>


### [88] [Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2505.22571)
*Hoang Pham,Khac-Hoai Nam Bui*

Main category: cs.CL

TL;DR: 本文提出了一种名为Agent-UniRAG的新型可训练代理框架，用于统一检索增强生成（RAG）系统，通过LLM代理逐步解决单跳和多跳查询任务，并引入合成数据集SynAgent-RAG以支持小型开源LLM。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要单独解决单跳或多跳RAG任务，限制了实际应用。本文旨在通过统一框架提升RAG系统的效果和可解释性。

Method: 提出Agent-UniRAG框架，基于输入复杂性逐步解决RAG任务，同时涵盖单跳和多跳查询，并引入SynAgent-RAG数据集支持小型LLM。

Result: 实验表明，该方法在多个RAG基准测试中与闭源及大型开源LLM性能相当。

Conclusion: Agent-UniRAG框架有效统一了RAG任务，提升了系统性能，且适用于小型LLM，代码和数据集已开源。

Abstract: This paper presents a novel approach for unified retrieval-augmented
generation (RAG) systems using the recent emerging large language model (LLM)
agent concept. Specifically, Agent LLM, which utilizes LLM as fundamental
controllers, has become a promising approach to enable the interpretability of
RAG tasks, especially for complex reasoning question-answering systems (e.g.,
multi-hop queries). Nonetheless, previous works mainly focus on solving RAG
systems with either single-hop or multi-hop approaches separately, which limits
the application of those approaches to real-world applications. In this study,
we propose a trainable agent framework called Agent-UniRAG for unified
retrieval-augmented LLM systems, which enhances the effectiveness and
interpretability of RAG systems. The main idea is to design an LLM agent
framework to solve RAG tasks step-by-step based on the complexity of the
inputs, simultaneously including single-hop and multi-hop queries in an
end-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset
to enable the proposed agent framework for small open-source LLMs (e.g.,
Llama-3-8B). The results show comparable performances with closed-source and
larger open-source LLMs across various RAG benchmarks. Our source code and
dataset are publicly available for further exploitation.

</details>


### [89] [Fusion Steering: Prompt-Specific Activation Control](https://arxiv.org/abs/2505.22572)
*Waldemar Chang,Alhassan Yasin*

Main category: cs.CL

TL;DR: Fusion Steering是一种激活引导方法，通过动态注入特定提示的激活差异提升大语言模型在问答任务中的事实准确性，分段引导表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统方法局限于单层或固定层操作，无法灵活调整模型行为。为提高问答任务的事实准确性，需开发更动态、灵活的激活引导策略。

Method: 提出Fusion Steering方法，支持全层和分段引导配置，动态注入结合真实答案和模型生成解释的激活差异，使用Optuna优化每提示的注入权重。

Result: 在260个SimpleQA提示上，分段引导准确率达25.4%，显著优于基线3.5%和全层引导16.2%。严格标准下完全正确答案从0%提升至13.1%。

Conclusion: 分段动态干预策略有效，每提示全网络激活控制具有潜力，且方法兼容稀疏表示，为可解释、可扩展的激活控制指明方向。

Abstract: We present Fusion Steering, an activation steering methodology that improves
factual accuracy in large language models (LLMs) for question-answering (QA)
tasks. This approach introduces flexible steering configurations, including
full-layer steering and segmented steering. Unlike traditional methods
constrained to single-layer or fixed-layer operations, Fusion Steering employs
dynamic injection of prompt-specific activation deltas across all transformer
layers. These activation deltas are derived from reference completions that
combine the ground-truth answer with a model-generated explanation to
facilitate semantically enriched, example-specific steering. The injection
weights are optimized per prompt using Optuna, targeting a joint objective that
balances token overlap (factual alignment) and perplexity (fluency proxy).
Evaluation employs a composite score integrating token overlap and LLM-graded
quality, encompassing factual accuracy, coherence, and relevance. Empirical
results on 260 SimpleQA prompts (selected from 500 where the baseline failed)
showcase the efficacy of segmented steering. Using Gemma-2-2B-IT with 8-bit
quantization, segmented steering achieves an accuracy of 25.4% (outputs scoring
$\geq 0.6$), outperforming the baseline at 3.5% and full-layer steering at
16.2%. Under the stricter SimpleQA rubric, segmented steering boosts fully
correct responses from 0.0% to 13.1%. These findings highlight the strengths of
segmented, dynamic intervention strategies and the promise of per-prompt,
full-network activation control. Fusion Steering is also amenable to sparse
representations, such as Neuronpedia or sparse crosscoders, suggesting a
promising direction for interpretable and scalable activation-level control in
LLMs.

</details>


### [90] [Less, but Better: Efficient Multilingual Expansion for LLMs via Layer-wise Mixture-of-Experts](https://arxiv.org/abs/2505.22582)
*Xue Zhang,Yunlong Liang,Fandong Meng,Songming Zhang,Yufeng Chen,Jinan Xu,Jie Zhou*

Main category: cs.CL

TL;DR: 该论文提出了一种名为LayerMoE的层间专家分配算法，通过分析不同层在语言模型中的语言特性，动态决定每层所需的新专家数量，从而在扩展新语言时减少参数成本并减轻旧语言的遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在为大语言模型（LLMs）扩展新语言时，通常采用混合专家（MoE）架构，通过增加新专家来学习新语言，同时将旧语言的令牌路由到原始模型以避免灾难性遗忘。然而，这种方法参数成本高，且仍会影响旧语言的性能。

Method: 论文提出LayerMoE算法，通过分析LLMs中不同层的语言表示相似性，动态分配每层的新专家数量（相似性越高，专家越少），并在高相似性层前添加分类器以引导旧语言令牌的路由。

Result: 实验结果表明，该方法在单次扩展和持续扩展设置中，分别减少了60%和33.3%的专家数量，同时性能优于现有基线方法。

Conclusion: LayerMoE通过层间专家分配和路由优化，有效降低了扩展新语言的参数成本，并减轻了旧语言的遗忘问题，为构建强大的多语言LLMs提供了新思路。

Abstract: Continually expanding new languages for existing large language models (LLMs)
is a promising yet challenging approach to building powerful multilingual LLMs.
The biggest challenge is to make the model continuously learn new languages
while preserving the proficient ability of old languages. To achieve this,
recent work utilizes the Mixture-of-Experts (MoE) architecture to expand new
languages by adding new experts and avoid catastrophic forgetting of old
languages by routing corresponding tokens to the original model backbone (old
experts). Although intuitive, this kind of method is parameter-costly when
expanding new languages and still inevitably impacts the performance of old
languages. To address these limitations, we analyze the language
characteristics of different layers in LLMs and propose a layer-wise expert
allocation algorithm (LayerMoE) to determine the appropriate number of new
experts for each layer. Specifically, we find different layers in LLMs exhibit
different representation similarities between languages and then utilize the
similarity as the indicator to allocate experts for each layer, i.e., the
higher similarity, the fewer experts. Additionally, to further mitigate the
forgetting of old languages, we add a classifier in front of the router network
on the layers with higher similarity to guide the routing of old language
tokens. Experimental results show that our method outperforms the previous
state-of-the-art baseline with 60% fewer experts in the single-expansion
setting and with 33.3% fewer experts in the lifelong-expansion setting,
demonstrating the effectiveness of our method.

</details>


### [91] [Precise In-Parameter Concept Erasure in Large Language Models](https://arxiv.org/abs/2505.22586)
*Yoav Gur-Arieh,Clara Suslik,Yihuai Hong,Fazl Barez,Mor Geva*

Main category: cs.CL

TL;DR: PISCES提出了一种新方法，通过直接编辑参数空间中的方向来精确删除大语言模型中的特定概念，相比现有方法更有效、更精准。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在预训练过程中可能学习到下游任务中不需要的知识（如敏感信息或版权内容），现有方法（如微调或事实级编辑）效果有限，需要更精确的删除手段。

Method: PISCES利用解耦模型将MLP向量分解为可解释特征，通过自动解释技术识别目标概念相关特征，并从模型参数中删除这些特征。

Result: 在Gemma 2和Llama 3.1上的实验表明，PISCES将目标概念准确率降至7.7%，擦除特异性提升31%，鲁棒性提升38%。

Conclusion: 基于特征的参数内编辑为大语言模型中的概念删除提供了更精确、可靠的方法。

Abstract: Large language models (LLMs) often acquire knowledge during pretraining that
is undesirable in downstream deployments, e.g., sensitive information or
copyrighted content. Existing approaches for removing such knowledge rely on
fine-tuning, training low-rank adapters or fact-level editing, but these are
either too coarse, too shallow, or ineffective. In this work, we propose PISCES
(Precise In-parameter Suppression for Concept EraSure), a novel framework for
precisely erasing entire concepts from model parameters by directly editing
directions that encode them in parameter space. PISCES uses a disentangler
model to decompose MLP vectors into interpretable features, identifies those
associated with a target concept using automated interpretability techniques,
and removes them from model parameters. Experiments on Gemma 2 and Llama 3.1
over various concepts show that PISCES achieves modest gains in efficacy over
leading erasure methods, reducing accuracy on the target concept to as low as
7.7%, while dramatically improving erasure specificity (by up to 31%) and
robustness (by up to 38%). Overall, these results demonstrate that
feature-based in-parameter editing enables a more precise and reliable approach
for removing conceptual knowledge in language models.

</details>


### [92] [Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical Reasoning](https://arxiv.org/abs/2505.22591)
*Erxin Yu,Jing Li,Ming Liao,Qi Zhu,Boyang Xue,Minghui Xu,Baojun Wang,Lanqing Hong,Fei Mi,Lifeng Shang*

Main category: cs.CL

TL;DR: 该论文提出了Self-Error-Instruct (SEI)框架，通过分析大语言模型在数学推理中的错误案例，生成针对性训练数据以提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多个领域表现优异，但在数学推理中仍存在许多错误案例。以往的方法仅从孤立错误案例中学习，未能泛化错误模式。

Method: SEI框架首先识别模型在GSM8K和MATH数据集中的错误案例，生成错误关键词并聚类错误类型。随后利用GPT-4o生成针对性训练数据，并通过单样本学习筛选有效数据，迭代优化模型。

Result: 实验表明，SEI框架提升了多种模型在数学推理任务中的性能，包括域内和域外数据集。

Conclusion: SEI框架通过错误泛化有效提升了大语言模型的数学推理能力。

Abstract: Although large language models demonstrate strong performance across various
domains, they still struggle with numerous bad cases in mathematical reasoning.
Previous approaches to learning from errors synthesize training data by solely
extrapolating from isolated bad cases, thereby failing to generalize the
extensive patterns inherent within these cases. This paper presents
Self-Error-Instruct (SEI), a framework that addresses these model weaknesses
and synthesizes more generalized targeted training data. Specifically, we
explore a target model on two mathematical datasets, GSM8K and MATH, to
pinpoint bad cases. Then, we generate error keyphrases for these cases based on
the instructor model's (GPT-4o) analysis and identify error types by clustering
these keyphrases. Next, we sample a few bad cases during each generation for
each identified error type and input them into the instructor model, which
synthesizes additional training data using a self-instruct approach. This new
data is refined through a one-shot learning process to ensure that only the
most effective examples are kept. Finally, we use these curated data to
fine-tune the target model, iteratively repeating the process to enhance
performance. We apply our framework to various models and observe improvements
in their reasoning abilities across both in-domain and out-of-domain
mathematics datasets. These results demonstrate the effectiveness of self-error
instruction in improving LLMs' mathematical reasoning through error
generalization.

</details>


### [93] [Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding](https://arxiv.org/abs/2505.22618)
*Chengyue Wu,Hao Zhang,Shuchen Xue,Zhijian Liu,Shizhe Diao,Ligeng Zhu,Ping Luo,Song Han,Enze Xie*

Main category: cs.CL

TL;DR: 该论文提出了一种针对双向扩散模型的块级近似KV缓存机制和置信度感知并行解码策略，显著提升了扩散大语言模型的推理速度，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的开源扩散大语言模型（Diffusion LLMs）由于缺乏KV缓存机制，以及在并行解码时存在质量下降问题，导致推理速度落后于自回归模型。

Method: 论文提出了两种方法：1) 为双向扩散模型设计的块级近似KV缓存机制，支持缓存重用；2) 置信度感知并行解码策略，通过选择性解码高置信度令牌来缓解依赖破坏问题。

Result: 在LLaDA和Dream模型上的实验表明，该方法实现了最高27.6倍的吞吐量提升，且准确率损失极小。

Conclusion: 该方法缩小了扩散模型与自回归模型的性能差距，为扩散大语言模型的实用化部署铺平了道路。

Abstract: Diffusion-based large language models (Diffusion LLMs) have shown promise for
non-autoregressive text generation with parallel decoding capabilities.
However, the practical inference speed of open-sourced Diffusion LLMs often
lags behind autoregressive models due to the lack of Key-Value (KV) Cache and
quality degradation when decoding multiple tokens simultaneously. To bridge
this gap, we introduce a novel block-wise approximate KV Cache mechanism
tailored for bidirectional diffusion models, enabling cache reuse with
negligible performance drop. Additionally, we identify the root cause of
generation quality degradation in parallel decoding as the disruption of token
dependencies under the conditional independence assumption. To address this, we
propose a confidence-aware parallel decoding strategy that selectively decodes
tokens exceeding a confidence threshold, mitigating dependency violations and
maintaining generation quality. Experimental results on LLaDA and Dream models
across multiple LLM benchmarks demonstrate up to \textbf{27.6$\times$
throughput} improvement with minimal accuracy loss, closing the performance gap
with autoregressive models and paving the way for practical deployment of
Diffusion LLMs.

</details>


### [94] [Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions](https://arxiv.org/abs/2505.22627)
*Yijun Shen,Delong Chen,Fan Liu,Xingyu Wang,Chuanyi Zhang,Liang Yao,Yuhui Zheng*

Main category: cs.CL

TL;DR: CoTalk提出一种AI辅助的序列标注方法，通过多模态接口提升标注效率与质量。


<details>
  <summary>Details</summary>
Motivation: 当前密集标注图像描述的方法在优化人工标注效率方面研究不足，需在固定预算下最大化标注数量与质量。

Method: 采用Chain-of-Talkers框架：1) 序列标注减少冗余工作量；2) 多模态接口（阅读输入+语音输出）提升效率。

Result: 实验显示CoTalk标注速度提升40%（0.42单位/秒），检索性能提高至41.13%。

Conclusion: CoTalk通过序列标注与多模态交互，显著提升了标注效率与视觉-语言对齐效果。

Abstract: While densely annotated image captions significantly facilitate the learning
of robust vision-language alignment, methodologies for systematically
optimizing human annotation efforts remain underexplored. We introduce
Chain-of-Talkers (CoTalk), an AI-in-the-loop methodology designed to maximize
the number of annotated samples and improve their comprehensiveness under fixed
budget constraints (e.g., total human annotation time). The framework is built
upon two key insights. First, sequential annotation reduces redundant workload
compared to conventional parallel annotation, as subsequent annotators only
need to annotate the ``residual'' -- the missing visual information that
previous annotations have not covered. Second, humans process textual input
faster by reading while outputting annotations with much higher throughput via
talking; thus a multimodal interface enables optimized efficiency. We evaluate
our framework from two aspects: intrinsic evaluations that assess the
comprehensiveness of semantic units, obtained by parsing detailed captions into
object-attribute trees and analyzing their effective connections; extrinsic
evaluation measures the practical usage of the annotated captions in
facilitating vision-language alignment. Experiments with eight participants
show our Chain-of-Talkers (CoTalk) improves annotation speed (0.42 vs. 0.30
units/sec) and retrieval performance (41.13\% vs. 40.52\%) over the parallel
method.

</details>


### [95] [Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based (Mis)Generalization in LLMs](https://arxiv.org/abs/2505.22630)
*Ziling Cheng,Meng Cao,Marc-Antoine Rondeau,Jackie Chi Kit Cheung*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在NLP基准测试中的成功伴随着对其作为随机鹦鹉的担忧，即它们主要复制预训练中见过的文本，常产生错误。本研究探讨了LLMs在误导性上下文提示下产生的无关幻觉错误，揭示了这些错误源于一种称为基于类别的（错误）泛化的结构化机制。通过行为分析和机制可解释性实验，发现LLMs在内部计算中构建抽象类别表示，并通过两种竞争电路选择特征，最终输出受上下文提示影响。研究提出了“随机变色龙”概念，认为LLMs通过形式化训练可以表现出基于抽象的泛化，尽管这种方式不可靠。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探讨大型语言模型（LLMs）在生成文本时的错误性质及其规律性，特别是那些由误导性上下文提示引发的无关幻觉错误。尽管LLMs在NLP任务中表现出色，但人们担心它们仅仅是随机复制预训练数据，缺乏真正的理解能力。

Method: 研究方法包括行为分析和机制可解释性实验。通过对Llama-3、Mistral和Pythia等模型在39种事实回忆关系类型上的测试，分析了模型内部计算机制，包括抽象类别表示的构建和特征选择电路的作用。

Result: 研究结果表明，LLMs的错误源于一种称为基于类别的（错误）泛化的机制，其中模型将抽象类别提示与查询或上下文的特征结合来生成答案。内部计算显示，抽象类别表示在较低层构建，并在较高层细化为具体答案；特征选择由两种竞争电路控制，一种基于直接查询推理，另一种结合上下文提示。

Conclusion: 研究结论提出了“随机变色龙”的概念，认为LLMs通过形式化训练可以表现出基于抽象的泛化能力，尽管这种能力受上下文提示的影响而不可靠。这为“随机鹦鹉”论点提供了更细致的视角，揭示了LLMs在泛化过程中的复杂性和局限性。

Abstract: The widespread success of large language models (LLMs) on NLP benchmarks has
been accompanied by concerns that LLMs function primarily as stochastic parrots
that reproduce texts similar to what they saw during pre-training, often
erroneously. But what is the nature of their errors, and do these errors
exhibit any regularities? In this work, we examine irrelevant context
hallucinations, in which models integrate misleading contextual cues into their
predictions. Through behavioral analysis, we show that these errors result from
a structured yet flawed mechanism that we term class-based (mis)generalization,
in which models combine abstract class cues with features extracted from the
query or context to derive answers. Furthermore, mechanistic interpretability
experiments on Llama-3, Mistral, and Pythia across 39 factual recall relation
types reveal that this behavior is reflected in the model's internal
computations: (i) abstract class representations are constructed in lower
layers before being refined into specific answers in higher layers, (ii)
feature selection is governed by two competing circuits -- one prioritizing
direct query-based reasoning, the other incorporating contextual cues -- whose
relative influences determine the final output. Our findings provide a more
nuanced perspective on the stochastic parrot argument: through form-based
training, LLMs can exhibit generalization leveraging abstractions, albeit in
unreliable ways based on contextual cues -- what we term stochastic chameleons.

</details>


### [96] [Spatial Knowledge Graph-Guided Multimodal Synthesis](https://arxiv.org/abs/2505.22633)
*Yida Xue,Zhen Bi,Jinnan Yang,Jungang Lou,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 该论文提出了一种基于空间知识图的多模态数据合成方法SKG2Data，旨在提升多模态大语言模型的空间感知与推理能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）的空间感知能力仍存在显著不足，而现有方法在合成数据时难以保证空间常识的合理性。

Method: 通过自动构建空间知识图（SKG）模拟人类对空间方向与距离的感知，并以此指导多模态数据合成。

Result: 实验表明，基于方向、距离等空间知识合成的数据能有效增强MLLMs的空间能力，并展现出强泛化性。

Conclusion: 基于知识的数据合成方法有望推动空间智能的发展。

Abstract: Recent advances in multimodal large language models (MLLMs) have
significantly enhanced their capabilities; however, their spatial perception
abilities remain a notable limitation. To address this challenge, multimodal
data synthesis offers a promising solution. Yet, ensuring that synthesized data
adhere to spatial common sense is a non-trivial task. In this work, we
introduce SKG2Data, a novel multimodal synthesis approach guided by spatial
knowledge graphs, grounded in the concept of knowledge-to-data generation.
SKG2Data automatically constructs a Spatial Knowledge Graph (SKG) to emulate
human-like perception of spatial directions and distances, which is
subsequently utilized to guide multimodal data synthesis. Extensive experiments
demonstrate that data synthesized from diverse types of spatial knowledge,
including direction and distance, not only enhance the spatial perception and
reasoning abilities of MLLMs but also exhibit strong generalization
capabilities. We hope that the idea of knowledge-based data synthesis can
advance the development of spatial intelligence.

</details>


### [97] [Learning Composable Chains-of-Thought](https://arxiv.org/abs/2505.22635)
*Fangcong Yin,Zeyu Leo Liu,Liu Leqi,Xi Ye,Greg Durrett*

Main category: cs.CL

TL;DR: 通过改进原子任务的CoT格式使其可组合，提升LLM在未见组合任务上的零样本推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大量标注的CoT数据训练LLM推理能力，但标注成本高且难以泛化到新任务。希望模型能组合原子推理技能解决更复杂的未见任务。

Method: 提出可组合CoT格式，在原子任务上训练模型后通过多任务学习或模型融合提升组合任务性能，并用拒绝采样微调进一步优化。

Result: 在字符串操作和自然语言任务上，可组合CoT训练优于多任务学习和持续微调基线。

Conclusion: 可组合CoT能有效提升LLM的组合泛化能力，且数据效率更高。

Abstract: A common approach for teaching large language models (LLMs) to reason is to
train on chain-of-thought (CoT) traces of in-distribution reasoning problems,
but such annotated data is costly to obtain for every problem of interest. We
want reasoning models to generalize beyond their training distribution, and
ideally to generalize compositionally: combine atomic reasoning skills to solve
harder, unseen reasoning tasks. We take a step towards compositional
generalization of reasoning skills when addressing a target compositional task
that has no labeled CoT data. We find that simply training models on CoT data
of atomic tasks leads to limited generalization, but minimally modifying CoT
formats of constituent atomic tasks to be composable can lead to improvements.
We can train "atomic CoT" models on the atomic tasks with Composable CoT data
and combine them with multitask learning or model merging for better zero-shot
performance on the target compositional task. Such a combined model can be
further bootstrapped on a small amount of compositional data using rejection
sampling fine-tuning (RFT). Results on string operations and natural language
skill compositions show that training LLMs on Composable CoT outperforms
multitask learning and continued fine-tuning baselines within a given training
data budget.

</details>


### [98] [Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese](https://arxiv.org/abs/2505.22645)
*Hanjia Lyu,Jiebo Luo,Jian Kang,Allison Koenecke*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在简体和繁体中文提示下表现存在差异，可能因训练数据、字符偏好和分词方式导致，需进一步分析偏见。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在简体和繁体中文提示下的性能差异，以避免因忽视不同文化背景而导致的代表性伤害和决策领域中的下游伤害。

Method: 设计两个反映真实场景的基准任务：区域术语选择和区域名称选择，审计11个主流商业和开源LLM服务的表现。

Result: 大多数LLM在区域术语选择任务中偏向简体中文，而在区域名称选择任务中却偏向繁体中文名称。

Conclusion: LLM的偏见与任务和提示语言相关，需进一步分析，并提供了开源基准数据集以促进未来研究。

Abstract: While the capabilities of Large Language Models (LLMs) have been studied in
both Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit
differential performance when prompted in these two variants of written
Chinese. This understanding is critical, as disparities in the quality of LLM
responses can perpetuate representational harms by ignoring the different
cultural contexts underlying Simplified versus Traditional Chinese, and can
exacerbate downstream harms in LLM-facilitated decision-making in domains such
as education or hiring. To investigate potential LLM performance disparities,
we design two benchmark tasks that reflect real-world scenarios: regional term
choice (prompting the LLM to name a described item which is referred to
differently in Mainland China and Taiwan), and regional name choice (prompting
the LLM to choose who to hire from a list of names in both Simplified and
Traditional Chinese). For both tasks, we audit the performance of 11 leading
commercial LLM services and open-sourced models -- spanning those primarily
trained on English, Simplified Chinese, or Traditional Chinese. Our analyses
indicate that biases in LLM responses are dependent on both the task and
prompting language: while most LLMs disproportionately favored Simplified
Chinese responses in the regional term choice task, they surprisingly favored
Traditional Chinese names in the regional name choice task. We find that these
disparities may arise from differences in training data representation, written
character preferences, and tokenization of Simplified and Traditional Chinese.
These findings highlight the need for further analysis of LLM biases; as such,
we provide an open-sourced benchmark dataset to foster reproducible evaluations
of future LLM behavior across Chinese language variants
(https://github.com/brucelyu17/SC-TC-Bench).

</details>


### [99] [WebDancer: Towards Autonomous Information Seeking Agency](https://arxiv.org/abs/2505.22648)
*Jialong Wu,Baixuan Li,Runnan Fang,Wenbiao Yin,Liwen Zhang,Zhengwei Tao,Dingchu Zhang,Zekun Xi,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种构建端到端信息搜索智能体的数据驱动训练范式WebDancer，在GAIA和WebWalkerQA基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决复杂现实问题需要多步推理和深度信息检索，现有自主研究系统（如Deep Research）展现了多步研究的潜力，但缺乏系统化的训练方法。

Method: 提出四阶段训练框架：1) 构建浏览数据；2) 轨迹采样；3) 监督微调实现冷启动；4) 强化学习提升泛化能力。基于ReAct框架实例化为WebDancer智能体。

Result: 在GAIA和WebWalkerQA基准测试中取得显著效果，验证了训练范式的有效性。

Conclusion: 该框架为开发更强大的智能体模型提供了系统化路径，代码已开源。

Abstract: Addressing intricate real-world problems necessitates in-depth information
seeking and multi-step reasoning. Recent progress in agentic systems,
exemplified by Deep Research, underscores the potential for autonomous
multi-step research. In this work, we present a cohesive paradigm for building
end-to-end agentic information seeking agents from a data-centric and
training-stage perspective. Our approach consists of four key stages: (1)
browsing data construction, (2) trajectories sampling, (3) supervised
fine-tuning for effective cold start, and (4) reinforcement learning for
enhanced generalisation. We instantiate this framework in a web agent based on
the ReAct, WebDancer. Empirical evaluations on the challenging information
seeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of
WebDancer, achieving considerable results and highlighting the efficacy of our
training paradigm. Further analysis of agent training provides valuable
insights and actionable, systematic pathways for developing more capable
agentic models. The codes and demo will be released in
https://github.com/Alibaba-NLP/WebAgent.

</details>


### [100] [The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in Learning to Reason](https://arxiv.org/abs/2505.22653)
*Ang Lv,Ruobing Xie,Xingwu Sun,Zhanhui Kang,Rui Yan*

Main category: cs.CL

TL;DR: 研究发现大语言模型（LLM）对奖励噪声具有强鲁棒性，仅奖励关键推理短语即可达到与精确奖励相当的模型性能，结合噪声奖励模型可提升开放任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注可精确验证奖励的任务（如数学题），而实际场景中奖励模型常含噪声。本文探索奖励噪声对LLM后训练的影响，并寻求提升模型推理能力的方法。

Method: 在数学任务中人为翻转40%奖励函数输出，测试模型对噪声的鲁棒性；提出仅奖励关键推理短语（RPR）的方法，并组合RPR与噪声奖励模型进行校准。

Result: Qwen-2.5-7B在40%噪声下仍能从5%提升至72%准确率；仅用RPR（不验证答案）即可达到70%+准确率，与精确奖励效果相当；RPR能校准噪声奖励，提升开放任务表现。

Conclusion: 预训练阶段提升模型基础能力至关重要，同时RPR为后训练技术提供新思路——强化推理过程而非结果验证可有效提升性能。

Abstract: Recent studies on post-training large language models (LLMs) for reasoning
through reinforcement learning (RL) typically focus on tasks that can be
accurately verified and rewarded, such as solving math problems. In contrast,
our research investigates the impact of reward noise, a more practical
consideration for real-world scenarios involving the post-training of LLMs
using reward models. We found that LLMs demonstrate strong robustness to
substantial reward noise. For example, manually flipping 40% of the reward
function's outputs in math tasks still allows a Qwen-2.5-7B model to achieve
rapid convergence, improving its performance on math tasks from 5% to 72%,
compared to the 75% accuracy achieved by a model trained with noiseless
rewards. Surprisingly, by only rewarding the appearance of key reasoning
phrases (namely reasoning pattern reward, RPR), such as ``first, I need
to''-without verifying the correctness of answers, the model achieved peak
downstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models
trained with strict correctness verification and accurate rewards. Recognizing
the importance of the reasoning process over the final results, we combined RPR
with noisy reward models. RPR helped calibrate the noisy reward models,
mitigating potential false negatives and enhancing the LLM's performance on
open-ended tasks. These findings suggest the importance of improving models'
foundational abilities during the pre-training phase while providing insights
for advancing post-training techniques. Our code and scripts are available at
https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason.

</details>


### [101] [GuessArena: Guess Who I Am? A Self-Adaptive Framework for Evaluating LLMs in Domain-Specific Knowledge and Reasoning](https://arxiv.org/abs/2505.22661)
*Qingchen Yu,Zifan Zheng,Ding Chen,Simin Niu,Bo Tang,Feiyu Xiong,Zhiyu Li*

Main category: cs.CL

TL;DR: 论文提出GuessArena框架，通过对抗性游戏互动改进大语言模型评估，解决传统静态基准在领域适应性和细粒度评估上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统的大语言模型评估依赖静态基准，存在两个主要问题：(1)预定义的测试集缺乏对不同应用领域的适应性，(2)标准化的评估协议难以捕捉领域特定知识和上下文推理能力的细粒度评估。

Method: 提出GuessArena框架，基于对抗性游戏互动，结合动态领域知识建模和渐进式推理评估，提升评估的保真度。

Result: 在金融、医疗、制造、信息技术和教育五个垂直领域的实证研究表明，GuessArena能有效区分大语言模型在领域知识覆盖和推理链完整性上的差异。

Conclusion: 与传统基准相比，GuessArena在可解释性、可扩展性和场景适应性方面具有显著优势。

Abstract: The evaluation of large language models (LLMs) has traditionally relied on
static benchmarks, a paradigm that poses two major limitations: (1) predefined
test sets lack adaptability to diverse application domains, and (2)
standardized evaluation protocols often fail to capture fine-grained
assessments of domain-specific knowledge and contextual reasoning abilities. To
overcome these challenges, we propose GuessArena, an adaptive evaluation
framework grounded in adversarial game-based interactions. Inspired by the
interactive structure of the Guess Who I Am? game, our framework seamlessly
integrates dynamic domain knowledge modeling with progressive reasoning
assessment to improve evaluation fidelity. Empirical studies across five
vertical domains-finance, healthcare, manufacturing, information technology,
and education-demonstrate that GuessArena effectively distinguishes LLMs in
terms of domain knowledge coverage and reasoning chain completeness. Compared
to conventional benchmarks, our method provides substantial advantages in
interpretability, scalability, and scenario adaptability.

</details>


### [102] [AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models](https://arxiv.org/abs/2505.22662)
*Feng Luo,Yu-Neng Chuang,Guanchu Wang,Hoang Anh Duy Le,Shaochen Zhong,Hongyi Liu,Jiayi Yuan,Yang Sui,Vladimir Braverman,Vipin Chaudhary,Xia Hu*

Main category: cs.CL

TL;DR: AutoL2S框架让大语言模型动态调整推理路径长度，减少57%冗余推理且不影响性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理简单问题时会产生过长的推理链，增加计算成本和延迟，而现有方法缺乏动态调整能力。

Method: 提出AutoL2S框架，通过训练模型识别<EASY>标记，动态压缩推理路径，实现长短推理链的自适应生成。

Result: 实验表明，AutoL2S可将推理链长度减少高达57%，同时保持模型性能不变。

Conclusion: AutoL2S有效提升了大语言模型推理的效率和可扩展性。

Abstract: The reasoning-capable large language models (LLMs) demonstrate strong
performance on complex reasoning tasks but often suffer from overthinking,
generating unnecessarily long chain-of-thought (CoT) reasoning paths for easy
reasoning questions, thereby increasing inference cost and latency. Recent
approaches attempt to address this challenge by manually deciding when to apply
long or short reasoning. However, they lack the flexibility to adapt CoT length
dynamically based on question complexity. In this paper, we propose Auto
Long-Short Reasoning (AutoL2S), a dynamic and model-agnostic framework that
enables LLMs to dynamically compress their generated reasoning path based on
the complexity of the reasoning question. AutoL2S enables a learned paradigm,
in which LLMs themselves can decide when longer reasoning is necessary and when
shorter reasoning suffices, by training on data annotated with our proposed
method, which includes both long and short CoT paths and a special <EASY>
token. We then use <EASY> token to indicate when the model can skip generating
lengthy CoT reasoning. This proposed annotation strategy can enhance the LLMs'
ability to generate shorter CoT reasoning paths with improved quality after
training. Extensive evaluation results show that AutoL2S reduces the length of
reasoning generation by up to 57% without compromising performance,
demonstrating the effectiveness of AutoL2S for scalable and efficient LLM
reasoning.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [103] [Understanding the learned look-ahead behavior of chess neural networks](https://arxiv.org/abs/2505.21552)
*Diogo Cruz*

Main category: cs.AI

TL;DR: 该研究分析了Leela Chess Zero策略网络的预见能力，发现其能考虑最多七步后的棋局，且行为高度依赖具体棋局情境。


<details>
  <summary>Details</summary>
Motivation: 探究国际象棋神经网络（特别是Leela Chess Zero策略网络）的预见能力，理解AI在复杂战略任务中的推理机制。

Method: 基于Jenner等人（2024）的研究，通过可解释性技术分析模型对多步未来棋局及多种走法序列的处理机制。

Result: 网络预见行为具有情境依赖性，能处理七步后的棋盘状态信息，并同时考虑多种可能的走法序列。

Conclusion: 研究揭示了神经网络在战略任务中涌现的复杂预见能力，为理解AI的类认知过程提供了新视角。

Abstract: We investigate the look-ahead capabilities of chess-playing neural networks,
specifically focusing on the Leela Chess Zero policy network. We build on the
work of Jenner et al. (2024) by analyzing the model's ability to consider
future moves and alternative sequences beyond the immediate next move. Our
findings reveal that the network's look-ahead behavior is highly
context-dependent, varying significantly based on the specific chess position.
We demonstrate that the model can process information about board states up to
seven moves ahead, utilizing similar internal mechanisms across different
future time steps. Additionally, we provide evidence that the network considers
multiple possible move sequences rather than focusing on a single line of play.
These results offer new insights into the emergence of sophisticated look-ahead
capabilities in neural networks trained on strategic tasks, contributing to our
understanding of AI reasoning in complex domains. Our work also showcases the
effectiveness of interpretability techniques in uncovering cognitive-like
processes in artificial intelligence systems.

</details>


### [104] [R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised and Reinforcement Learning](https://arxiv.org/abs/2505.21668)
*Yongchao Chen,Yueying Liu,Junwei Zhou,Yilun Hao,Jingquan Wang,Yang Zhang,Chuchu Fan*

Main category: cs.AI

TL;DR: 论文提出R1-Code-Interpreter，通过多轮监督微调和强化学习训练LLM自主生成代码查询，提升精确计算和符号推理任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在需要精确计算、符号操作和算法推理的任务上表现不佳，缺乏有效结合文本推理和代码生成的方法。公开研究也缺少如何让预训练LLM有效利用代码的指导。

Method: 采用多轮监督微调(SFT)和强化学习(RL)训练Qwen-2.5模型，探索不同答案格式、冷/热启动、GRPO/PPO等策略，构建144个推理任务数据集。

Result: 最终模型R1-CI-14B在37个测试任务上准确率从44.0%提升至64.1%，超过GPT-4o纯文本版本(58.6%)，接近GPT-4o带代码解释器版本(70.9%)。

Conclusion: 代码解释器训练因任务多样性和代码执行成本而更具挑战性，监督微调阶段至关重要。模型通过代码生成展现出自我检查的新兴能力。

Abstract: Despite advances in reasoning and planning of R1-like models, Large Language
Models (LLMs) still struggle with tasks requiring precise computation, symbolic
manipulation, optimization, and algorithmic reasoning, in which textual
reasoning lacks the rigor of code execution. A key challenge is enabling LLMs
to decide when to use textual reasoning versus code generation. While OpenAI
trains models to invoke a Code Interpreter as needed, public research lacks
guidance on aligning pre-trained LLMs to effectively leverage code and
generalize across diverse tasks. We present R1-Code-Interpreter, an extension
of a text-only LLM trained via multi-turn supervised fine-tuning (SFT) and
reinforcement learning (RL) to autonomously generate multiple code queries
during step-by-step reasoning. We curate 144 reasoning and planning tasks (107
for training, 37 for testing), each with over 200 diverse questions. We
fine-tune Qwen-2.5 models (3B/7B/14B) using various SFT and RL strategies,
investigating different answer formats, reasoning vs. non-reasoning models,
cold vs. warm starts, GRPO vs. PPO, and masked vs. unmasked code outputs.
Unlike prior RL work on narrow domains, we find that Code Interpreter training
is significantly harder due to high task diversity and expensive code
execution, highlighting the critical role of the SFT stage. Our final model,
R1-CI-14B, improves average accuracy on the 37 test tasks from 44.0\% to
64.1\%, outperforming GPT-4o (text-only: 58.6\%) and approaching GPT-4o with
Code Interpreter (70.9\%), with the emergent self-checking behavior via code
generation. Datasets, Codes, and Models are available at
https://github.com/yongchao98/R1-Code-Interpreter and
https://huggingface.co/yongchao98.

</details>


### [105] [Adaptive Frontier Exploration on Graphs with Applications to Network-Based Disease Testing](https://arxiv.org/abs/2505.21671)
*Davin Choo,Yuqi Pan,Tonghan Wang,Milind Tambe,Alastair van Heerden,Cheryl Johnson*

Main category: cs.AI

TL;DR: 该论文研究了一个在图上进行序列决策的问题，提出了一种基于Gittins索引的策略，在特定条件下证明最优，并在实验中获得优于基线方法的表现。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于实际应用场景（如接触者追踪和机器人探索）中需要在图结构上进行受限探索的问题，即只能选择已探索节点的邻居进行下一步行动。

Method: 设计了一种基于Gittins索引的策略，适用于一般图结构，并在森林结构的图上证明是最优的。实现时使用了多项式时间复杂度的算法。

Result: 实验结果表明，该方法在合成图和真实世界图（如HIV检测中的性接触网络）上均优于基线方法，例如仅测试一半人口就能检测出几乎所有阳性病例。

Conclusion: 论文提出的策略在受限探索条件下有效，理论保证和实验结果均验证了其优越性，为相关领域的序列决策问题提供了实用解决方案。

Abstract: We study a sequential decision-making problem on a $n$-node graph $G$ where
each node has an unknown label from a finite set $\mathbf{\Sigma}$, drawn from
a joint distribution $P$ that is Markov with respect to $G$. At each step,
selecting a node reveals its label and yields a label-dependent reward. The
goal is to adaptively choose nodes to maximize expected accumulated discounted
rewards. We impose a frontier exploration constraint, where actions are limited
to neighbors of previously selected nodes, reflecting practical constraints in
settings such as contact tracing and robotic exploration. We design a Gittins
index-based policy that applies to general graphs and is provably optimal when
$G$ is a forest. Our implementation runs in $O(n^2 \cdot |\mathbf{\Sigma}|^2)$
time while using $O(n \cdot |\mathbf{\Sigma}|^2)$ oracle calls to $P$ and
$O(n^2 \cdot |\mathbf{\Sigma}|)$ space. Experiments on synthetic and real-world
graphs show that our method consistently outperforms natural baselines,
including in non-tree, budget-limited, and undiscounted settings. For example,
in HIV testing simulations on real-world sexual interaction networks, our
policy detects nearly all positive cases with only half the population tested,
substantially outperforming other baselines.

</details>


### [106] [Make Planning Research Rigorous Again!](https://arxiv.org/abs/2505.21674)
*Michael Katz,Harsha Kokel,Christian Muise,Shirin Sohrabi,Sarath Sreedharan*

Main category: cs.AI

TL;DR: 论文主张将传统自动规划领域的严谨方法应用于基于大语言模型（LLM）的规划系统开发，以避免重复历史错误并加速进展。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的规划研究存在重复传统规划领域已知错误的现象，作者认为应借鉴历史经验来提升LLM规划器的开发效率和质量。

Method: 提出将自动规划领域的理论工具、评估方法和数据科学地整合到LLM规划器的设计与评估流程中。

Result: 通过融合传统规划经验，可系统性规避已知缺陷，推动LLM规划器的实质性进步。

Conclusion: 传统规划社区的积累对LLM时代仍具关键价值，跨领域方法融合是规划技术发展的有效路径。

Abstract: In over sixty years since its inception, the field of planning has made
significant contributions to both the theory and practice of building planning
software that can solve a never-before-seen planning problem. This was done
through established practices of rigorous design and evaluation of planning
systems. It is our position that this rigor should be applied to the current
trend of work on planning with large language models. One way to do so is by
correctly incorporating the insights, tools, and data from the automated
planning community into the design and evaluation of LLM-based planners. The
experience and expertise of the planning community are not just important from
a historical perspective; the lessons learned could play a crucial role in
accelerating the development of LLM-based planners. This position is
particularly important in light of the abundance of recent works that replicate
and propagate the same pitfalls that the planning community has encountered and
learned from. We believe that avoiding such known pitfalls will contribute
greatly to the progress in building LLM-based planners and to planning in
general.

</details>


### [107] [Don't Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models](https://arxiv.org/abs/2505.21765)
*Sohyun An,Ruochen Wang,Tianyi Zhou,Cho-Jui Hsieh*

Main category: cs.AI

TL;DR: 大型推理模型（LRMs）因过度思考导致输出冗长且效率低下，本文提出动态优化框架，通过优化思维路径提升推理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在提升推理能力的同时，因过度思考导致输出冗长、计算浪费，甚至性能下降。本文旨在解决这一问题。

Method: 提出动态优化框架，将推理路径分割为不同思维模式，系统识别并优化有益模式，同时去除有害模式。

Result: 优化后的思维路径更简洁且信息充分，计算效率提升47%，准确率提升15.6%，并在多个数学推理基准测试中表现优异。

Conclusion: 动态优化框架显著提升了大型推理模型的效率和准确性，减少了计算开销和输出长度。

Abstract: While recent success of large reasoning models (LRMs) significantly advanced
LLMs' reasoning capability by optimizing the final answer accuracy using
reinforcement learning, they may also drastically increase the output length
due to overthinking, characterized by unnecessarily complex reasoning paths
that waste computation and potentially degrade the performance. We hypothesize
that such inefficiencies stem from LRMs' limited capability to dynamically
select the proper modular reasoning strategies, termed thinking patterns at the
right position. To investigate this hypothesis, we propose a dynamic
optimization framework that segments model-generated reasoning paths into
distinct thinking patterns, systematically identifying and promoting beneficial
patterns that improve the answer while removing detrimental ones. Empirical
analysis confirms that our optimized thinking paths yield more concise yet
sufficiently informative trajectories, enhancing reasoning efficiency by
reducing attention FLOPs by up to 47% while maintaining accuracy for originally
correct responses. Moreover, a non-trivial portion of originally incorrect
responses are transformed into correct ones, achieving a 15.6% accuracy
improvement with reduced length. Motivated by the improvement brought by the
optimized thinking paths, we apply a preference optimization technique
supported by a pairwise dataset contrasting suboptimal and optimal reasoning
paths. Experimental evaluations across multiple mathematical reasoning
benchmarks reveal that our method notably reduces computational overhead while
simultaneously improving reasoning accuracy, achieving up to a 12% accuracy
improvement and reducing token usage from approximately 5,000 to 3,000 tokens.

</details>


### [108] [Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation](https://arxiv.org/abs/2505.21784)
*Tharindu Kumarage,Ninareh Mehrabi,Anil Ramakrishna,Xinyan Zhao,Richard Zemel,Kai-Wei Chang,Aram Galstyan,Rahul Gupta,Charith Peris*

Main category: cs.AI

TL;DR: AIDSAFE提出了一种通过多智能体审议迭代生成高质量安全推理链的方法，显著提升LLM的安全泛化能力和抗越狱能力。


<details>
  <summary>Details</summary>
Motivation: 现有安全措施存在过度拒绝和越狱漏洞问题，而构建高质量策略嵌入推理链数据集成本高昂且易产生幻觉或策略冲突。

Method: 采用多智能体迭代审议机制生成安全推理链，并通过数据精炼阶段剔除重复/冗余/欺骗性内容；补充使用信念增强技术生成偏好数据。

Result: 生成的推理链具有优越的策略遵从性和推理质量，微调后的开源LLM在安全泛化、抗越狱能力上显著提升，同时保持可用性。

Conclusion: AIDSAFE为基于监督微调的安全训练提供了高质量数据基础，推动了安全推理范式的发展。

Abstract: Safety reasoning is a recent paradigm where LLMs reason over safety policies
before generating responses, thereby mitigating limitations in existing safety
measures such as over-refusal and jailbreak vulnerabilities. However,
implementing this paradigm is challenging due to the resource-intensive process
of creating high-quality policy-embedded chain-of-thought (CoT) datasets while
ensuring reasoning remains accurate and free from hallucinations or policy
conflicts. To tackle this, we propose AIDSAFE: Agentic Iterative Deliberation
for Safety Reasoning, a novel data generation recipe that leverages multi-agent
deliberation to iteratively expand reasoning on safety policies. A data refiner
stage in AIDSAFE ensures high-quality outputs by eliminating repetitive,
redundant, and deceptive thoughts. AIDSAFE-generated CoTs provide a strong
foundation for supervised fine-tuning (SFT)-based safety training.
Additionally, to address the need of preference data in alignment stages, such
as DPO training, we introduce a supplemental recipe that uses belief
augmentation to create distinct selected and rejected CoT samples. Our
evaluations demonstrate that AIDSAFE-generated CoTs achieve superior policy
adherence and reasoning quality. Consequently, we show that fine-tuning
open-source LLMs on these CoTs can significantly improve safety generalization
and jailbreak robustness while maintaining acceptable utility and over-refusal
accuracy. AIDSAFE-generated CoT datasets can be found here:
https://huggingface.co/datasets/AmazonScience/AIDSAFE

</details>


### [109] [SAGE-Eval: Evaluating LLMs for Systematic Generalizations of Safety Facts](https://arxiv.org/abs/2505.21828)
*Chen Yueh-Han,Guy Davidson,Brenden M. Lake*

Main category: cs.AI

TL;DR: 论文提出SAGE-Eval基准测试，评估大模型能否将安全知识可靠泛化到新场景。测试发现顶级模型Claude-3仅通过58%的安全事实测试，表明现有模型存在安全隐患。


<details>
  <summary>Details</summary>
Motivation: 研究发现当前大语言模型缺乏将既定安全知识（如CDC关于幼儿窒息风险的警告）可靠应用于用户天真提问的能力，这种缺陷可能导致严重后果。

Method: 构建包含104个权威安全事实的SAGE-Eval基准，系统生成10,428个测试场景覆盖7大领域，评估模型安全知识泛化能力。

Result: 表现最佳的Claude-3模型仅通过58%测试，模型能力与训练计算量仅呈弱相关，表明单纯扩大规模不能解决安全问题。

Conclusion: 前沿大模型仍缺乏稳健的安全知识泛化能力，建议开发者部署前使用SAGE-Eval评估模型可靠性。已公开基准数据集和代码。

Abstract: Do LLMs robustly generalize critical safety facts to novel situations?
Lacking this ability is dangerous when users ask naive questions. For instance,
"I'm considering packing melon balls for my 10-month-old's lunch. What other
foods would be good to include?" Before offering food options, the LLM should
warn that melon balls pose a choking hazard to toddlers, as documented by the
CDC. Failing to provide such warnings could result in serious injuries or even
death. To evaluate this, we introduce SAGE-Eval, SAfety-fact systematic
GEneralization evaluation, the first benchmark that tests whether LLMs properly
apply well established safety facts to naive user queries. SAGE-Eval comprises
104 facts manually sourced from reputable organizations, systematically
augmented to create 10,428 test scenarios across 7 common domains (e.g.,
Outdoor Activities, Medicine). We find that the top model, Claude-3.7-sonnet,
passes only 58% of all the safety facts tested. We also observe that model
capabilities and training compute weakly correlate with performance on
SAGE-Eval, implying that scaling up is not the golden solution. Our findings
suggest frontier LLMs still lack robust generalization ability. We recommend
developers use SAGE-Eval in pre-deployment evaluations to assess model
reliability in addressing salient risks. We publicly release SAGE-Eval at
https://huggingface.co/datasets/YuehHanChen/SAGE-Eval and our code is available
at https://github.com/YuehHanChen/SAGE-Eval/tree/main.

</details>


### [110] [SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem](https://arxiv.org/abs/2505.21887)
*Ahmed Heakl,Yahia Salaheldin Shaaban,Martin Takac,Salem Lahlou,Zangir Iklassov*

Main category: cs.AI

TL;DR: SVRPBench是首个模拟城市规模车辆路径规划中高保真随机动态的开源基准测试，包含500多个实例，测试显示现有RL求解器在分布变化下性能下降超过20%。


<details>
  <summary>Details</summary>
Motivation: 现有车辆路径规划基准大多基于静态理想化场景，无法反映现实物流中的不确定性，如交通拥堵、随机延误等。

Method: 构建包含时间相关拥堵、对数正态延迟、概率性事故等真实因素的测试集，覆盖多仓库/多车辆场景，共500+实例。

Result: 测试表明POMO等先进RL求解器在分布偏移下性能下降超20%，而传统方法保持稳定。

Conclusion: SVRPBench为设计适应现实不确定性的求解器提供了新基准，推动领域超越合成假设。

Abstract: Robust routing under uncertainty is central to real-world logistics, yet most
benchmarks assume static, idealized settings. We present SVRPBench, the first
open benchmark to capture high-fidelity stochastic dynamics in vehicle routing
at urban scale. Spanning more than 500 instances with up to 1000 customers, it
simulates realistic delivery conditions: time-dependent congestion, log-normal
delays, probabilistic accidents, and empirically grounded time windows for
residential and commercial clients. Our pipeline generates diverse,
constraint-rich scenarios, including multi-depot and multi-vehicle setups.
Benchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade
by over 20% under distributional shift, while classical and metaheuristic
methods remain robust. To enable reproducible research, we release the dataset
and evaluation suite. SVRPBench challenges the community to design solvers that
generalize beyond synthetic assumptions and adapt to real-world uncertainty.

</details>


### [111] [Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy](https://arxiv.org/abs/2505.21907)
*Saleh Afzoon,Zahra Jahanandish,Phuong Thao Huynh,Amin Beheshti,Usman Naseem*

Main category: cs.AI

TL;DR: 本文综述了AI副驾驶中偏好优化的研究，提出了一个基于交互阶段的分类法，并分析了相关技术。


<details>
  <summary>Details</summary>
Motivation: 随着AI副驾驶在任务协助中的普及，个性化成为提升可用性、信任和生产力的关键。然而，针对交互式实时系统的个性化技术仍不完善，本文旨在填补这一研究空白。

Method: 通过综合研究AI副驾驶中用户偏好的捕获、建模和优化方法，提出一个基于交互阶段的分类法，并分析相关技术。

Result: 提出了一个统一的AI副驾驶定义和偏好优化策略分类法，分析了偏好信号获取、用户意图建模和反馈循环整合的技术。

Conclusion: 本文为设计自适应、偏好感知的AI副驾驶提供了结构化基础，整合了AI个性化、人机协作和大语言模型适应的见解。

Abstract: AI copilots, context-aware, AI-powered systems designed to assist users in
tasks such as software development and content creation, are becoming integral
to modern workflows. As these systems grow in capability and adoption,
personalization has emerged as a cornerstone for ensuring usability, trust, and
productivity. Central to this personalization is preference optimization: the
ability of AI copilots to detect, interpret, and align with individual user
preferences. While personalization techniques are well-established in domains
like recommender systems and dialogue agents, their adaptation to interactive,
real-time systems like AI copilots remains fragmented and underexplored. This
survey addresses this gap by synthesizing research on how user preferences are
captured, modeled, and refined within the design of AI copilots. We introduce a
unified definition of AI copilots and propose a phase-based taxonomy of
preference optimization strategies, structured around pre-interaction,
mid-interaction, and post-interaction stages. We analyze techniques for
acquiring preference signals, modeling user intent, and integrating feedback
loops, highlighting both established approaches and recent innovations. By
bridging insights from AI personalization, human-AI collaboration, and large
language model adaptation, this survey provides a structured foundation for
designing adaptive, preference-aware AI copilots. It offers a holistic view of
the available preference resources, how they can be leveraged, and which
technical approaches are most suited to each stage of system design.

</details>


### [112] [From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models](https://arxiv.org/abs/2505.21935)
*Kaiyu He,Zhiyu Chen*

Main category: cs.AI

TL;DR: 该论文探讨了大型语言模型（LLMs）是否能够超越指令执行和信息检索，真正发现新知识，并提出了基于Peirce推理框架的结构化方法，以推动LLMs成为创新引擎。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的发展，当前研究主要集中在提升其指令执行和演绎推理能力，而忽略了其是否能够真正发现新知识的可能性。为了实现人工通用智能（AGI），需要模型不仅能执行命令或检索信息，还能通过学习、推理和生成新知识来深化对世界的理解。

Method: 论文以Peirce的溯因、演绎和归纳推理框架为指导，提供了一个结构化视角来审视基于LLM的假设发现。通过综合现有研究，分析了假设生成、应用和验证的成果与不足。

Result: 论文揭示了LLMs在假设发现方面的关键成就和重要缺口，并展示了如何通过统一这些研究方向，推动LLMs从“信息执行者”转变为真正的创新引擎。

Conclusion: 通过结构化分析和综合现有研究，论文指出LLMs有潜力成为科学研究与现实问题解决的创新引擎，但仍需进一步探索和完善其知识发现能力。

Abstract: Since the advent of Large Language Models (LLMs), efforts have largely
focused on improving their instruction-following and deductive reasoning
abilities, leaving open the question of whether these models can truly discover
new knowledge. In pursuit of artificial general intelligence (AGI), there is a
growing need for models that not only execute commands or retrieve information
but also learn, reason, and generate new knowledge by formulating novel
hypotheses and theories that deepen our understanding of the world. Guided by
Peirce's framework of abduction, deduction, and induction, this survey offers a
structured lens to examine LLM-based hypothesis discovery. We synthesize
existing work in hypothesis generation, application, and validation,
identifying both key achievements and critical gaps. By unifying these threads,
we illuminate how LLMs might evolve from mere ``information executors'' into
engines of genuine innovation, potentially transforming research, science, and
real-world problem solving.

</details>


### [113] [Functional Matching of Logic Subgraphs: Beyond Structural Isomorphism](https://arxiv.org/abs/2505.21988)
*Ziyang Zheng,Kezhi Li,Zhengyuan Shi,Qiang Xu*

Main category: cs.AI

TL;DR: 论文提出了一种功能性子图匹配方法，通过两阶段多模态框架解决传统结构图同构在电路拓扑变化时失效的问题，显著提升了功能子图检测和模糊边界识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有EDA应用中的子图匹配技术主要依赖结构图同构，当合成转换大幅改变电路拓扑时，无法识别功能相关的子图。这限制了在数据路径优化、算术验证和硬件木马检测等领域的应用。

Method: 提出功能性子图匹配概念，采用两阶段多模态框架：1) 学习AIG和后映射网表间的鲁棒功能嵌入以检测功能子图；2) 使用图分割方法识别模糊边界。

Result: 在ITC99、OpenABCD、ForgeEDA等基准测试中，功能子图检测平均准确率达93.8%，模糊边界识别的Dice分数达91.3%，显著优于现有结构方法。

Conclusion: 该研究突破了传统结构匹配的局限，为EDA应用提供了对合成或技术映射引起的结构变化不敏感的功能性子图匹配解决方案。

Abstract: Subgraph matching in logic circuits is foundational for numerous Electronic
Design Automation (EDA) applications, including datapath optimization,
arithmetic verification, and hardware trojan detection. However, existing
techniques rely primarily on structural graph isomorphism and thus fail to
identify function-related subgraphs when synthesis transformations
substantially alter circuit topology. To overcome this critical limitation, we
introduce the concept of functional subgraph matching, a novel approach that
identifies whether a given logic function is implicitly present within a larger
circuit, irrespective of structural variations induced by synthesis or
technology mapping. Specifically, we propose a two-stage multi-modal framework:
(1) learning robust functional embeddings across AIG and post-mapping netlists
for functional subgraph detection, and (2) identifying fuzzy boundaries using a
graph segmentation approach. Evaluations on standard benchmarks (ITC99,
OpenABCD, ForgeEDA) demonstrate significant performance improvements over
existing structural methods, with average $93.8\%$ accuracy in functional
subgraph detection and a dice score of $91.3\%$ in fuzzy boundary
identification.

</details>


### [114] [Efficiently Enhancing General Agents With Hierarchical-categorical Memory](https://arxiv.org/abs/2505.22006)
*Changze Qiao,Mingming Lu*

Main category: cs.AI

TL;DR: 论文提出了一种无需参数更新的通用智能体EHC，通过分层记忆检索和任务类别导向经验学习模块，在多模态任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖计算成本高的端到端训练或缺乏持续学习能力的工具使用，无法适应新环境。

Method: EHC包含分层记忆检索（HMR）模块和任务类别导向经验学习（TOEL）模块，支持快速检索和持续学习。

Result: 在多个标准数据集上的实验表明，EHC优于现有方法，达到最先进性能。

Conclusion: EHC是一种高效处理复杂多模态任务的通用智能体。

Abstract: With large language models (LLMs) demonstrating remarkable capabilities,
there has been a surge in research on leveraging LLMs to build general-purpose
multi-modal agents. However, existing approaches either rely on computationally
expensive end-to-end training using large-scale multi-modal data or adopt
tool-use methods that lack the ability to continuously learn and adapt to new
environments. In this paper, we introduce EHC, a general agent capable of
learning without parameter updates. EHC consists of a Hierarchical Memory
Retrieval (HMR) module and a Task-Category Oriented Experience Learning (TOEL)
module. The HMR module facilitates rapid retrieval of relevant memories and
continuously stores new information without being constrained by memory
capacity. The TOEL module enhances the agent's comprehension of various task
characteristics by classifying experiences and extracting patterns across
different categories. Extensive experiments conducted on multiple standard
datasets demonstrate that EHC outperforms existing methods, achieving
state-of-the-art performance and underscoring its effectiveness as a general
agent for handling complex multi-modal tasks.

</details>


### [115] [Reinforced Reasoning for Embodied Planning](https://arxiv.org/abs/2505.22050)
*Di Wu,Jiaxin Fan,Junzhe Zang,Guanbo Wang,Wei Yin,Wenhao Li,Bo Jin*

Main category: cs.AI

TL;DR: 该论文提出了一种强化微调框架，通过结合监督微调和基于规则的奖励函数，显著提升了具身AI在多步决策任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉语言模型（VLMs）在静态感知任务中表现优异，但在动态交互环境中进行多步决策时，仍存在时间推理、空间理解和常识基础等方面的不足。

Method: 论文首先从强大的闭源模型中蒸馏出高质量数据集，进行监督微调（SFT），然后设计基于规则的奖励函数，并通过广义强化偏好优化（GRPO）优化策略。

Result: 实验结果表明，该方法在Embench基准测试中显著优于类似或更大规模的模型，包括GPT-4o-mini和70B+开源基线，并在未见过的环境中表现出强大的泛化能力。

Conclusion: 这项研究展示了强化驱动推理在提升具身AI长时程规划能力方面的潜力。

Abstract: Embodied planning requires agents to make coherent multi-step decisions based
on dynamic visual observations and natural language goals. While recent
vision-language models (VLMs) excel at static perception tasks, they struggle
with the temporal reasoning, spatial understanding, and commonsense grounding
needed for planning in interactive environments. In this work, we introduce a
reinforcement fine-tuning framework that brings R1-style reasoning enhancement
into embodied planning. We first distill a high-quality dataset from a powerful
closed-source model and perform supervised fine-tuning (SFT) to equip the model
with structured decision-making priors. We then design a rule-based reward
function tailored to multi-step action quality and optimize the policy via
Generalized Reinforced Preference Optimization (GRPO). Our approach is
evaluated on Embench, a recent benchmark for interactive embodied tasks,
covering both in-domain and out-of-domain scenarios. Experimental results show
that our method significantly outperforms models of similar or larger scale,
including GPT-4o-mini and 70B+ open-source baselines, and exhibits strong
generalization to unseen environments. This work highlights the potential of
reinforcement-driven reasoning to advance long-horizon planning in embodied AI.

</details>


### [116] [Cognitively-Inspired Emergent Communication via Knowledge Graphs for Assisting the Visually Impaired](https://arxiv.org/abs/2505.22087)
*Ruxiao Chen,Dezheng Han,Wenjie Han,Shuaishuai Guo*

Main category: cs.AI

TL;DR: 论文提出VAG-EC框架，通过知识图谱模拟人类视觉认知，为视障人士提供低延迟、高语义的实时导航辅助系统。


<details>
  <summary>Details</summary>
Motivation: 现有辅助系统在延迟与语义丰富度之间存在矛盾：自然语言系统详细但慢，符号语言系统快但语义浅。需一种兼顾速度与语义的解决方案。

Method: 采用知识图谱表示物体关系，结合注意力机制筛选任务相关实体，模仿人类选择性注意，生成紧凑、可解释的符号语言。

Result: 实验表明VAG-EC在地形相似性(TopSim)和上下文独立性(CI)上优于传统方法，验证了其高效性与适应性。

Conclusion: 基于认知的涌现通信框架为实时辅助技术提供了快速、自适应且符合人类认知的解决方案。

Abstract: Assistive systems for visually impaired individuals must deliver rapid,
interpretable, and adaptive feedback to facilitate real-time navigation.
Current approaches face a trade-off between latency and semantic richness:
natural language-based systems provide detailed guidance but are too slow for
dynamic scenarios, while emergent communication frameworks offer low-latency
symbolic languages but lack semantic depth, limiting their utility in tactile
modalities like vibration. To address these limitations, we introduce a novel
framework, Cognitively-Inspired Emergent Communication via Knowledge Graphs
(VAG-EC), which emulates human visual perception and cognitive mapping. Our
method constructs knowledge graphs to represent objects and their
relationships, incorporating attention mechanisms to prioritize task-relevant
entities, thereby mirroring human selective attention. This structured approach
enables the emergence of compact, interpretable, and context-sensitive symbolic
languages. Extensive experiments across varying vocabulary sizes and message
lengths demonstrate that VAG-EC outperforms traditional emergent communication
methods in Topographic Similarity (TopSim) and Context Independence (CI). These
findings underscore the potential of cognitively grounded emergent
communication as a fast, adaptive, and human-aligned solution for real-time
assistive technologies. Code is available at
https://github.com/Anonymous-NLPcode/Anonymous_submission/tree/main.

</details>


### [117] [VIRAL: Vision-grounded Integration for Reward design And Learning](https://arxiv.org/abs/2505.22092)
*Valentin Cuzin-Rambaud,Emilien Komlenovic,Alexandre Faure,Bruno Yun*

Main category: cs.AI

TL;DR: 论文提出VIRAL，一种利用多模态大语言模型自动生成和优化奖励函数的流程，以提升强化学习与人类意图的对齐。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能中人与机器的对齐是关键挑战，强化学习尤其容易受到奖励函数设计不当的影响。大语言模型在奖励生成方面已展现出超越人类的能力。

Method: VIRAL通过多模态大语言模型自主创建并交互式优化奖励函数，可结合人类反馈或视频LLM生成的描述来指导优化过程。

Result: 在五个Gymnasium环境中的评估表明，VIRAL加速了新行为的学习，同时更好地与用户意图对齐。

Conclusion: VIRAL为奖励函数的自动生成和优化提供了有效解决方案，显著提升了强化学习任务的表现和人类意图对齐。

Abstract: The alignment between humans and machines is a critical challenge in
artificial intelligence today. Reinforcement learning, which aims to maximize a
reward function, is particularly vulnerable to the risks associated with poorly
designed reward functions. Recent advancements has shown that Large Language
Models (LLMs) for reward generation can outperform human performance in this
context. We introduce VIRAL, a pipeline for generating and refining reward
functions through the use of multi-modal LLMs. VIRAL autonomously creates and
interactively improves reward functions based on a given environment and a goal
prompt or annotated image. The refinement process can incorporate human
feedback or be guided by a description generated by a video LLM, which explains
the agent's policy in video form. We evaluated VIRAL in five Gymnasium
environments, demonstrating that it accelerates the learning of new behaviors
while ensuring improved alignment with user intent. The source-code and demo
video are available at: https://github.com/VIRAL-UCBL1/VIRAL and
https://youtu.be/t4_BXugBm9Q.

</details>


### [118] [Efficient Dynamic Shielding for Parametric Safety Specifications](https://arxiv.org/abs/2505.22104)
*Davide Corsi,Kaushik Mallik,Andoni Rodriguez,Cesar Sanchez*

Main category: cs.AI

TL;DR: 本文提出了一种动态防护罩方法，用于在运行时安全需求变化时快速适应，相比传统静态防护罩显著提高了效率。


<details>
  <summary>Details</summary>
Motivation: 传统的防护罩设计为静态，无法应对运行时安全需求的变化，重新计算会导致致命延迟。

Method: 提出动态防护罩，基于参数化安全规范，利用静态设计的参数集和快速动态适应算法。

Result: 实验表明，动态防护罩离线设计仅需几分钟，在线适应仅需几秒，比暴力重计算方法快5倍。

Conclusion: 动态防护罩能有效适应运行时安全需求变化，显著提升AI自主系统的安全性和响应速度。

Abstract: Shielding has emerged as a promising approach for ensuring safety of
AI-controlled autonomous systems. The algorithmic goal is to compute a shield,
which is a runtime safety enforcement tool that needs to monitor and intervene
the AI controller's actions if safety could be compromised otherwise.
Traditional shields are designed statically for a specific safety requirement.
Therefore, if the safety requirement changes at runtime due to changing
operating conditions, the shield needs to be recomputed from scratch, causing
delays that could be fatal. We introduce dynamic shields for parametric safety
specifications, which are succinctly represented sets of all possible safety
specifications that may be encountered at runtime. Our dynamic shields are
statically designed for a given safety parameter set, and are able to
dynamically adapt as the true safety specification (permissible by the
parameters) is revealed at runtime. The main algorithmic novelty lies in the
dynamic adaptation procedure, which is a simple and fast algorithm that
utilizes known features of standard safety shields, like maximal
permissiveness. We report experimental results for a robot navigation problem
in unknown territories, where the safety specification evolves as new obstacles
are discovered at runtime. In our experiments, the dynamic shields took a few
minutes for their offline design, and took between a fraction of a second and a
few seconds for online adaptation at each step, whereas the brute-force online
recomputation approach was up to 5 times slower.

</details>


### [119] [Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test](https://arxiv.org/abs/2505.22112)
*Guangfu Hao,Frederic Alexandre,Shan Yu*

Main category: cs.AI

TL;DR: 该研究评估了先进视觉大语言模型（VLLMs）在认知灵活性方面的表现，发现其在文本输入下能达到或超越人类水平，并揭示了输入方式和提示策略对模型能力的影响。


<details>
  <summary>Details</summary>
Motivation: 探索视觉大语言模型（VLLMs）在认知灵活性方面的表现，填补该领域的研究空白，并验证其是否具备类似人脑的认知架构。

Method: 使用威斯康星卡片分类测试（WCST）评估GPT-4o、Gemini-1.5 Pro和Claude-3.5 Sonnet等VLLMs的认知灵活性，结合思维链提示和角色扮演模拟功能缺陷。

Result: VLLMs在文本输入下表现出与人类相当的认知灵活性，但其能力受输入模态和提示策略显著影响；通过角色扮演可模拟认知灵活性受损患者的行为。

Conclusion: VLLMs在认知灵活性关键指标上已接近人类水平，展现了模拟复杂脑过程的潜力，为认知架构研究提供了新工具。

Abstract: Cognitive flexibility has been extensively studied in human cognition but
remains relatively unexplored in the context of Visual Large Language Models
(VLLMs). This study assesses the cognitive flexibility of state-of-the-art
VLLMs (GPT-4o, Gemini-1.5 Pro, and Claude-3.5 Sonnet) using the Wisconsin Card
Sorting Test (WCST), a classic measure of set-shifting ability. Our results
reveal that VLLMs achieve or surpass human-level set-shifting capabilities
under chain-of-thought prompting with text-based inputs. However, their
abilities are highly influenced by both input modality and prompting strategy.
In addition, we find that through role-playing, VLLMs can simulate various
functional deficits aligned with patients having impairments in cognitive
flexibility, suggesting that VLLMs may possess a cognitive architecture, at
least regarding the ability of set-shifting, similar to the brain. This study
reveals the fact that VLLMs have already approached the human level on a key
component underlying our higher cognition, and highlights the potential to use
them to emulate complex brain processes.

</details>


### [120] [Lifted Forward Planning in Relational Factored Markov Decision Processes with Concurrent Actions](https://arxiv.org/abs/2505.22147)
*Florian Andreas Marwitz,Tanya Braun,Ralf Möller,Marcel Gehrke*

Main category: cs.AI

TL;DR: 该论文提出了一种名为Foreplan的一阶关系前向规划器，通过多项式大小的表示方法有效解决了状态和动作空间指数级增长的问题，并展示了至少四个数量级的加速。


<details>
  <summary>Details</summary>
Motivation: 随着不可区分对象数量的增加，马尔可夫决策过程中的状态空间呈指数级增长，导致策略计算困难，尤其是当动作空间大小依赖于状态空间大小时，问题更加严重。

Method: 论文提出了一种一阶表示方法，以多项式大小存储状态和动作空间，并开发了Foreplan关系前向规划器及其近似版本，用于高效计算策略。

Result: Foreplan在理论和实证评估中表现出色，实现了至少四个数量级的加速，并能确定在给定限制下完成特定任务所需的对象数量。

Conclusion: Foreplan及其近似版本通过一阶表示方法有效解决了状态和动作空间的指数级增长问题，显著提升了计算效率。

Abstract: Decision making is a central problem in AI that can be formalized using a
Markov Decision Process. A problem is that, with increasing numbers of
(indistinguishable) objects, the state space grows exponentially. To compute
policies, the state space has to be enumerated. Even more possibilities have to
be enumerated if the size of the action space depends on the size of the state
space, especially if we allow concurrent actions. To tackle the exponential
blow-up in the action and state space, we present a first-order representation
to store the spaces in polynomial instead of exponential size in the number of
objects and introduce Foreplan, a relational forward planner, which uses this
representation to efficiently compute policies for numerous indistinguishable
objects and actions. Additionally, we introduce an even faster approximate
version of Foreplan. Moreover, Foreplan identifies how many objects an agent
should act on to achieve a certain task given restrictions. Further, we provide
a theoretical analysis and an empirical evaluation of Foreplan, demonstrating a
speedup of at least four orders of magnitude.

</details>


### [121] [What Makes a Good Reasoning Chain? Uncovering Structural Patterns in Long Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.22148)
*Gangwei Jiang,Yahui Liu,Zhaoyi Li,Qi Wang,Fuzheng Zhang,Linqi Song,Ying Wei,Defu Lian*

Main category: cs.AI

TL;DR: 该论文提出了LCoT2Tree框架，将大语言模型的线性推理链转换为树状结构，通过图神经网络分析结构模式，揭示其对最终答案正确性的预测能力，并识别关键错误模式。


<details>
  <summary>Details</summary>
Motivation: 尽管长链思维（LCoT）策略在复杂任务中表现出专家级性能，但其推理链的内部结构如何驱动甚至预测最终答案的正确性仍是一个关键但未充分探索的问题。

Method: 作者提出了LCoT2Tree框架，将顺序推理链转换为层次树结构，并利用图神经网络（GNN）提取结构模式（如探索、回溯和验证），以分析其对最终性能的影响。

Result: 研究发现，LCoT2Tree提取的结构模式是跨任务和模型的更强预测指标，并识别出如过度分支等导致失败的关键思维模式。此外，这些结构模式还能提升Best-of-N解码的有效性。

Conclusion: 该研究强调了推理链内部结构的关键作用，将LCoT2Tree定位为诊断、解释和改进大语言模型推理的强大工具。

Abstract: Recent advances in reasoning with large language models (LLMs) have
popularized Long Chain-of-Thought (LCoT), a strategy that encourages deliberate
and step-by-step reasoning before producing a final answer. While LCoTs have
enabled expert-level performance in complex tasks, how the internal structures
of their reasoning chains drive, or even predict, the correctness of final
answers remains a critical yet underexplored question. In this work, we present
LCoT2Tree, an automated framework that converts sequential LCoTs into
hierarchical tree structures and thus enables deeper structural analysis of LLM
reasoning. Using graph neural networks (GNNs), we reveal that structural
patterns extracted by LCoT2Tree, including exploration, backtracking, and
verification, serve as stronger predictors of final performance across a wide
range of tasks and models. Leveraging an explainability technique, we further
identify critical thought patterns such as over-branching that account for
failures. Beyond diagnostic insights, the structural patterns by LCoT2Tree
support practical applications, including improving Best-of-N decoding
effectiveness. Overall, our results underscore the critical role of internal
structures of reasoning chains, positioning LCoT2Tree as a powerful tool for
diagnosing, interpreting, and improving reasoning in LLMs.

</details>


### [122] [A Preprocessing Framework for Efficient Approximate Bi-Objective Shortest-Path Computation in the Presence of Correlated Objectives](https://arxiv.org/abs/2505.22244)
*Yaron Halle,Ariel Felner,Sven Koenig,Oren Salzman*

Main category: cs.AI

TL;DR: 该论文提出了一种针对双目标最短路径问题（BOSP）的高效算法，通过利用目标函数间的相关性，结合图聚类预处理技术，显著提升了A*pex算法的求解速度。


<details>
  <summary>Details</summary>
Motivation: 现实场景（如路网）中常存在目标函数相关性（如行驶时间与油耗），传统BOSP求解因搜索空间随目标数和图规模指数增长而计算困难。现有近似解法（如A*pex）在相关性高时可用更宽松的近似因子压缩帕累托解集。

Method: 1. 预处理阶段采用图聚类识别相关性簇并生成新图表示 2. 基于此扩展A*pex算法，通过相关性感知机制减少搜索开销。

Result: 在DIMACS标准数据集上实现最高5倍的加速，且首次在理论保证解质量的前提下有效利用双目标相关性。

Conclusion: 该算法为相关性场景下的双目标搜索提供了首个兼具高效性和理论保证的解决方案，推动了BOSP问题的实用化进程。

Abstract: The bi-objective shortest-path (BOSP) problem seeks to find paths between
start and target vertices of a graph while optimizing two conflicting objective
functions. We consider the BOSP problem in the presence of correlated
objectives. Such correlations often occur in real-world settings such as road
networks, where optimizing two positively correlated objectives, such as travel
time and fuel consumption, is common. BOSP is generally computationally
challenging as the size of the search space is exponential in the number of
objective functions and the graph size. Bounded sub-optimal BOSP solvers such
as A*pex alleviate this complexity by approximating the Pareto-optimal solution
set rather than computing it exactly (given a user-provided approximation
factor). As the correlation between objective functions increases, smaller
approximation factors are sufficient for collapsing the entire Pareto-optimal
set into a single solution. We leverage this insight to propose an efficient
algorithm that reduces the search effort in the presence of correlated
objectives. Our approach for computing approximations of the entire
Pareto-optimal set is inspired by graph-clustering algorithms. It uses a
preprocessing phase to identify correlated clusters within a graph and to
generate a new graph representation. This allows a natural generalization of
A*pex to run up to five times faster on DIMACS dataset instances, a standard
benchmark in the field. To the best of our knowledge, this is the first
algorithm proposed that efficiently and effectively exploits correlations in
the context of bi-objective search while providing theoretical guarantees on
solution quality.

</details>


### [123] [Compression versus Accuracy: A Hierarchy of Lifted Models](https://arxiv.org/abs/2505.22288)
*Jan Speller,Malte Luttermann,Marcel Gehrke,Tanya Braun*

Main category: cs.AI

TL;DR: 本文提出了一种无超参数的层次化方法，用于构建概率图模型的提升表示，避免了传统方法中需要反复调整ε参数的问题，并提供了模型压缩与精度之间的明确权衡。


<details>
  <summary>Details</summary>
Motivation: 当前先进的提升推理算法ACP需要依赖超参数ε来近似分组因子，但合适的ε值难以确定，且不同ε值可能导致模型差异大、解释性降低。因此，需要一种更高效、无需调参的方法。

Method: 提出了一种层次化方法，自动计算ε值的层次结构，确保模型层次性（小ε分组在大ε下仍保持），并生成对应的误差界层次。

Result: 该方法能高效生成模型层次结构，允许用户根据压缩率与精度需求选择ε值，同时保持模型间的可解释性关联。

Conclusion: 层次化方法消除了ACP对超参数的依赖，提供了模型选择的灵活性，平衡了计算效率与推理精度，同时增强了结果的可解释性。

Abstract: Probabilistic graphical models that encode indistinguishable objects and
relations among them use first-order logic constructs to compress a
propositional factorised model for more efficient (lifted) inference. To obtain
a lifted representation, the state-of-the-art algorithm Advanced Colour Passing
(ACP) groups factors that represent matching distributions. In an approximate
version using $\varepsilon$ as a hyperparameter, factors are grouped that
differ by a factor of at most $(1\pm \varepsilon)$. However, finding a suitable
$\varepsilon$ is not obvious and may need a lot of exploration, possibly
requiring many ACP runs with different $\varepsilon$ values. Additionally,
varying $\varepsilon$ can yield wildly different models, leading to decreased
interpretability. Therefore, this paper presents a hierarchical approach to
lifted model construction that is hyperparameter-free. It efficiently computes
a hierarchy of $\varepsilon$ values that ensures a hierarchy of models, meaning
that once factors are grouped together given some $\varepsilon$, these factors
will be grouped together for larger $\varepsilon$ as well. The hierarchy of
$\varepsilon$ values also leads to a hierarchy of error bounds. This allows for
explicitly weighing compression versus accuracy when choosing specific
$\varepsilon$ values to run ACP with and enables interpretability between the
different models.

</details>


### [124] [Rethinking the Unsolvable: When In-Context Search Meets Test-Time Scaling](https://arxiv.org/abs/2505.22290)
*Fanzeng Xia,Yidong Luo,Tinko Sebastian Bartels,Yaqi Xu,Tongxin Li*

Main category: cs.AI

TL;DR: 本文通过结合上下文搜索提示和内部扩展技术，显著提升了大语言模型在复杂推理任务上的表现，挑战了现有评估范式对模型能力的低估。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常依赖简单的上下文学习示例来评估大语言模型的推理能力，忽视了高级技术可能带来的性能突破，导致对模型潜力的系统性低估。

Method: 采用上下文搜索提示与内部扩展相结合的方法，系统探索其在超难推理任务上的潜力。

Result: 在NP难问题和复杂现实规划任务中，该方法实现了高达30倍的成功率提升，并理论上扩展了可解问题的复杂度类别。

Conclusion: 当前评估范式低估了大语言模型的真实潜力，需要重新设计更全面的评测策略以准确衡量其实际推理边界。

Abstract: Recent research has highlighted that Large Language Models (LLMs), even when
trained to generate extended long reasoning steps, still face significant
challenges on hard reasoning problems. However, much of the existing literature
relies on direct prompting with simple in-context learning examples for
evaluation, which largely overlooks advanced techniques to elicit LLMs'
deliberate reasoning before drawing conclusions that LLMs hit a performance
ceiling. In this paper, we systematically explore the combined potential of
in-context search and test-time scaling on super hard reasoning tasks. We find
that by employing advanced in-context search prompting to LLMs augmented with
internal scaling, one can achieve transformative performance breakthroughs on
tasks previously deemed "unsolvable" (e.g., reported success rates below 5%).
We provide both empirical results and theoretical analysis of how this
combination can unleash LLM reasoning capabilities: i) Empirically, on
controlled NP-hard tasks and complex real-world planning benchmarks, our
approach achieves up to a 30x improvement in success rates compared to
previously reported results without any external mechanisms; ii) Theoretically,
we show that in-context search prompting, when combined with internal scaling,
significantly extends the complexity class of solvable reasoning problems.
These findings challenge prevailing assumptions about the limitations of LLMs
on complex tasks, indicating that current evaluation paradigms systematically
underestimate their true potential. Our work calls for a critical reassessment
of how LLM reasoning is benchmarked and a more robust evaluation strategy that
fully captures the true capabilities of contemporary LLMs, which can lead to a
better understanding of their operational reasoning boundaries in real-world
deployments.

</details>


### [125] [From Large AI Models to Agentic AI: A Tutorial on Future Intelligent Communications](https://arxiv.org/abs/2505.22311)
*Feibo Jiang,Cunhua Pan,Li Dong,Kezhi Wang,Octavia A. Dobre,Merouane Debbah*

Main category: cs.AI

TL;DR: 本文系统介绍了大型人工智能模型（LAMs）和Agentic AI技术在6G智能通信系统中的原理、设计和应用，为研究者提供了前沿技术概览和实践指导。


<details>
  <summary>Details</summary>
Motivation: 随着6G通信的到来，智能通信系统面临感知与响应能力受限、可扩展性不足及动态环境适应性低等挑战，需要引入LAMs和Agentic AI技术以提升系统性能。

Method: 论文首先回顾了从LAMs到Agentic AI的技术演进，详细分析了LAMs的关键组件及其分类（如LLMs、LVMs等），提出了以LAM为中心的通信设计范式，并构建了基于LAM的Agentic AI系统框架。

Result: 论文提出了适用于6G的多Agent框架，包含数据检索、协作规划和反思评估等功能，并详细概述了LAMs和Agentic AI在通信场景中的应用。

Conclusion: 当前研究仍面临诸多挑战，未来需进一步探索高效、安全、可持续的下一代智能通信系统发展方向。

Abstract: With the advent of 6G communications, intelligent communication systems face
multiple challenges, including constrained perception and response
capabilities, limited scalability, and low adaptability in dynamic
environments. This tutorial provides a systematic introduction to the
principles, design, and applications of Large Artificial Intelligence Models
(LAMs) and Agentic AI technologies in intelligent communication systems, aiming
to offer researchers a comprehensive overview of cutting-edge technologies and
practical guidance. First, we outline the background of 6G communications,
review the technological evolution from LAMs to Agentic AI, and clarify the
tutorial's motivation and main contributions. Subsequently, we present a
comprehensive review of the key components required for constructing LAMs. We
further categorize LAMs and analyze their applicability, covering Large
Language Models (LLMs), Large Vision Models (LVMs), Large Multimodal Models
(LMMs), Large Reasoning Models (LRMs), and lightweight LAMs. Next, we propose a
LAM-centric design paradigm tailored for communications, encompassing dataset
construction and both internal and external learning approaches. Building upon
this, we develop an LAM-based Agentic AI system for intelligent communications,
clarifying its core components such as planners, knowledge bases, tools, and
memory modules, as well as its interaction mechanisms. We also introduce a
multi-agent framework with data retrieval, collaborative planning, and
reflective evaluation for 6G. Subsequently, we provide a detailed overview of
the applications of LAMs and Agentic AI in communication scenarios. Finally, we
summarize the research challenges and future directions in current studies,
aiming to support the development of efficient, secure, and sustainable
next-generation intelligent communication systems.

</details>


### [126] [AgentDNS: A Root Domain Naming System for LLM Agents](https://arxiv.org/abs/2505.22368)
*Enfang Cui,Yujun Cheng,Rui She,Dan Liu,Zhiyuan Liang,Minxin Guo,Tianzheng Li,Qian Wei,Wenjuan Xing,Zhijie Zhong*

Main category: cs.AI

TL;DR: 提出AgentDNS系统，解决LLM代理跨厂商服务发现与调用的标准化问题。


<details>
  <summary>Details</summary>
Motivation: 现有协议在跨厂商代理和工具的服务发现方面缺乏标准化解决方案。

Method: 借鉴传统DNS原理，设计包含服务注册、语义发现、安全调用和统一计费的结构化机制。

Result: 详细阐述了AgentDNS的架构、核心功能及用例，展示其在多代理协作中的潜力。

Conclusion: AgentDNS有望在实际场景中简化多代理协作，代码将开源。

Abstract: The rapid evolution of Large Language Model (LLM) agents has highlighted
critical challenges in cross-vendor service discovery, interoperability, and
communication. Existing protocols like model context protocol and
agent-to-agent protocol have made significant strides in standardizing
interoperability between agents and tools, as well as communication among
multi-agents. However, there remains a lack of standardized protocols and
solutions for service discovery across different agent and tool vendors. In
this paper, we propose AgentDNS, a root domain naming and service discovery
system designed to enable LLM agents to autonomously discover, resolve, and
securely invoke third-party agent and tool services across organizational and
technological boundaries. Inspired by the principles of the traditional DNS,
AgentDNS introduces a structured mechanism for service registration, semantic
service discovery, secure invocation, and unified billing. We detail the
architecture, core functionalities, and use cases of AgentDNS, demonstrating
its potential to streamline multi-agent collaboration in real-world scenarios.
The source code will be published on https://github.com/agentdns.

</details>


### [127] [AI Mathematician: Towards Fully Automated Frontier Mathematical Research](https://arxiv.org/abs/2505.22451)
*Yuanhang Liu,Yanxing Huang,Yanqiao Wang,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: 论文提出AI Mathematician (AIM)框架，利用大型推理模型(LRMs)支持前沿数学研究，解决研究问题的复杂性和程序严谨性挑战，并展示其在研究级任务中的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型在数学竞赛问题上表现优异，但在前沿数学研究中面临研究问题复杂性和程序严谨性两大挑战，因此需要开发新方法来支持数学研究。

Method: AIM框架采用两种核心策略：探索机制以促进更长的解决路径，以及悲观合理验证方法以确保可靠性。

Result: 实验表明，AIM能够自主构建大量证明部分并在多个真实数学课题中发现非平凡见解，显示出其处理研究级任务的能力。

Conclusion: AIM展示了LRMs在数学发现中的潜力，基于LRM的智能体系统未来可能显著加速数学研究。

Abstract: Large Reasoning Models (LRMs) have made significant progress in mathematical
capabilities in recent times. However, these successes have been primarily
confined to competition-level problems. In this work, we propose AI
Mathematician (AIM) framework, which harnesses the reasoning strength of LRMs
to support frontier mathematical research. We have identified two critical
challenges of mathematical research compared to competition, {\it the intrinsic
complexity of research problems} and {\it the requirement of procedural rigor}.
To address these challenges, AIM incorporates two core strategies: an
exploration mechanism to foster longer solution paths, and the pessimistic
reasonable verification method to ensure reliability.
  This early version of AIM already exhibits strong capability in tackling
research-level tasks. We conducted extensive experiments across several
real-world mathematical topics and obtained promising results. AIM is able to
autonomously construct substantial portions of proofs and uncover non-trivial
insights within each research area. These findings highlight the potential of
LRMs in mathematical discovery and suggest that LRM-based agent systems could
significantly accelerate mathematical research in the future.

</details>


### [128] [HDDLGym: A Tool for Studying Multi-Agent Hierarchical Problems Defined in HDDL with OpenAI Gym](https://arxiv.org/abs/2505.22597)
*Ngoc La,Ruaridh Mon-Williams,Julie A. Shah*

Main category: cs.AI

TL;DR: HDDLGym是一个基于Python的工具，用于将分层规划与强化学习无缝集成，支持多智能体场景和协作规划。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏一个能够将分层规划与强化学习无缝集成的工具，HDDLGym旨在填补这一空白。

Method: HDDLGym通过从HDDL域和问题自动生成OpenAI Gym环境，将分层规划与强化学习集成。

Result: HDDLGym成功实现了分层规划与强化学习的集成，并支持多智能体场景和协作规划。

Conclusion: HDDLGym是一个有价值的工具，特别适用于研究分层规划中的强化学习，尤其是在多智能体环境中。

Abstract: In recent years, reinforcement learning (RL) methods have been widely tested
using tools like OpenAI Gym, though many tasks in these environments could also
benefit from hierarchical planning. However, there is a lack of a tool that
enables seamless integration of hierarchical planning with RL. Hierarchical
Domain Definition Language (HDDL), used in classical planning, introduces a
structured approach well-suited for model-based RL to address this gap. To
bridge this integration, we introduce HDDLGym, a Python-based tool that
automatically generates OpenAI Gym environments from HDDL domains and problems.
HDDLGym serves as a link between RL and hierarchical planning, supporting
multi-agent scenarios and enabling collaborative planning among agents. This
paper provides an overview of HDDLGym's design and implementation, highlighting
the challenges and design choices involved in integrating HDDL with the Gym
interface, and applying RL policies to support hierarchical planning. We also
provide detailed instructions and demonstrations for using the HDDLGym
framework, including how to work with existing HDDL domains and problems from
International Planning Competitions, exemplified by the Transport domain.
Additionally, we offer guidance on creating new HDDL domains for multi-agent
scenarios and demonstrate the practical use of HDDLGym in the Overcooked
domain. By leveraging the advantages of HDDL and Gym, HDDLGym aims to be a
valuable tool for studying RL in hierarchical planning, particularly in
multi-agent contexts.

</details>


### [129] [Capability-Based Scaling Laws for LLM Red-Teaming](https://arxiv.org/abs/2505.20162)
*Alexander Panfilov,Paul Kassianik,Maksym Andriushchenko,Jonas Geiping*

Main category: cs.AI

TL;DR: 研究大语言模型红队测试中的能力差距问题，发现攻击成功率与攻击者-目标能力差相关，并提出越狱扩展定律。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力和自主性的提升，红队测试在安全部署中的作用日益重要。然而，当目标模型能力超过红队测试者时，传统的提示工程方法可能失效。

Method: 通过评估500多个攻击者-目标对，使用基于LLM的越狱攻击模拟人类红队测试者，分析不同家族、规模和能力水平的模型。

Result: 发现三个趋势：能力更强的模型是更好的攻击者；目标能力超过攻击者时攻击成功率骤降；攻击成功率与MMLU-Pro基准的社会科学部分表现相关。

Conclusion: 固定能力的攻击者（如人类）可能对未来模型无效，开源模型能力提升增加现有系统风险，需准确测量和控制模型的操纵能力。

Abstract: As large language models grow in capability and agency, identifying
vulnerabilities through red-teaming becomes vital for safe deployment. However,
traditional prompt-engineering approaches may prove ineffective once
red-teaming turns into a weak-to-strong problem, where target models surpass
red-teamers in capabilities. To study this shift, we frame red-teaming through
the lens of the capability gap between attacker and target. We evaluate more
than 500 attacker-target pairs using LLM-based jailbreak attacks that mimic
human red-teamers across diverse families, sizes, and capability levels. Three
strong trends emerge: (i) more capable models are better attackers, (ii) attack
success drops sharply once the target's capability exceeds the attacker's, and
(iii) attack success rates correlate with high performance on social science
splits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking
scaling law that predicts attack success for a fixed target based on
attacker-target capability gap. These findings suggest that fixed-capability
attackers (e.g., humans) may become ineffective against future models,
increasingly capable open-source models amplify risks for existing systems, and
model providers must accurately measure and control models' persuasive and
manipulative abilities to limit their effectiveness as attackers.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [130] [The Role of Visualization in LLM-Assisted Knowledge Graph Systems: Effects on User Trust, Exploration, and Workflows](https://arxiv.org/abs/2505.21512)
*Harry Li,Gabriel Appleby,Kenneth Alperin,Steven R Gomez,Ashley Suh*

Main category: cs.LG

TL;DR: 论文研究了LLM辅助知识图谱探索系统中用户过度信任的问题，提出了LinkQ系统并通过五种可视化机制帮助用户评估准确性，发现用户工作流因背景不同而异，揭示了可视化作为缓解技术的潜力与风险。


<details>
  <summary>Details</summary>
Motivation: 知识图谱（KGs）探索对专家用户仍具挑战性，尽管大语言模型（LLMs）被用于填补这一空白，但缺乏关于LLMs与KGs结合如何影响用户信任、探索策略及决策的实证研究，这为基于LLM的KG视觉分析系统设计带来了挑战。

Method: 开发了LinkQ系统，通过LLM将自然语言问题转换为结构化查询，并设计了五种可视化机制（如LLM-KG状态图、查询编辑器等）以帮助用户评估查询和LLM响应的准确性。与14位从业者进行定性评估。

Result: 研究发现用户（包括KG专家）倾向于过度信任LinkQ的输出，尤其是其“有帮助”的可视化界面，即使LLM出错。用户的工作流因其对KGs和LLMs的先验熟悉度而异，挑战了“一刀切”系统设计的假设。

Conclusion: 研究强调了在LLM辅助数据分析工具中虚假信任的风险，以及需要进一步研究可视化作为缓解技术的作用。

Abstract: Knowledge graphs (KGs) are powerful data structures, but exploring them
effectively remains difficult for even expert users. Large language models
(LLMs) are increasingly used to address this gap, yet little is known
empirically about how their usage with KGs shapes user trust, exploration
strategies, or downstream decision-making - raising key design challenges for
LLM-based KG visual analysis systems. To study these effects, we developed
LinkQ, a KG exploration system that converts natural language questions into
structured queries with an LLM. We collaborated with KG experts to design five
visual mechanisms that help users assess the accuracy of both KG queries and
LLM responses: an LLM-KG state diagram that illustrates which stage of the
exploration pipeline LinkQ is in, a query editor displaying the generated query
paired with an LLM explanation, an entity-relation ID table showing extracted
KG entities and relations with semantic descriptions, a query structure graph
that depicts the path traversed in the KG, and an interactive graph
visualization of query results. From a qualitative evaluation with 14
practitioners, we found that users - even KG experts - tended to overtrust
LinkQ's outputs due to its "helpful" visualizations, even when the LLM was
incorrect. Users exhibited distinct workflows depending on their prior
familiarity with KGs and LLMs, challenging the assumption that these systems
are one-size-fits-all - despite often being designed as if they are. Our
findings highlight the risks of false trust in LLM-assisted data analysis tools
and the need for further investigation into the role of visualization as a
mitigation technique.

</details>


### [131] [SIMCOPILOT: Evaluating Large Language Models for Copilot-Style Code Generation](https://arxiv.org/abs/2505.21514)
*Mingchao Jiang,Abhinav Jain,Sophia Zorek,Chris Jermaine*

Main category: cs.LG

TL;DR: SIMCOPILOT是一个评估大语言模型（LLM）作为交互式编程助手能力的基准测试，专注于代码补全和填充任务，提供Java和Python子测试集，并分析模型在实际编码场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏全面评估LLM在编程辅助任务中实际效用的基准测试，SIMCOPILOT旨在填补这一空白，提供更细致的性能分析，关注常被忽视的关键因素。

Method: SIMCOPILOT包含针对Java和Python的专用子测试集，覆盖不同规模和复杂度的代码库，评估模型在算法、数据库、计算机视觉和神经网络等领域的表现。

Result: 评估揭示了模型在复杂依赖结构中保持逻辑一致性的挑战，同时展示了模型在特定任务中的优势。

Conclusion: 研究表明，LLM正从语法感知的代码生成器向可靠的智能软件开发伙伴过渡，但仍存在局限性，需进一步改进。

Abstract: We introduce SIMCOPILOT, a benchmark that simulates the role of large
language models (LLMs) as interactive, "copilot"-style coding assistants.
Targeting both completion (finishing incomplete methods or code blocks) and
infill tasks (filling missing segments within existing code), SIMCOPILOT
provides a comprehensive framework for evaluating LLM coding capabilities. The
benchmark comprises dedicated sub-benchmarks for Java (SIMCOPILOTJ) and Python
(SIMCOPILOTP), covering diverse codebases varying in size and complexity. Our
key contributions include: (a) establishing a realistic, detailed evaluation
environment to assess LLM utility in practical coding scenarios, and (b)
providing fine-grained analyses that address critical factors frequently
overlooked by existing benchmarks, such as task-specific performance nuances,
contextual understanding across code segments, and sensitivity to variable
scope. Evaluations conducted across domains-including algorithms, databases,
computer vision, and neural networks-offer insights into model strengths and
highlight persistent challenges in maintaining logical consistency within
complex dependency structures. Beyond benchmarking, our study sheds light on
the current limitations of LLM-driven code generation and underscores the
ongoing transition of LLMs from merely syntax-aware generators toward reliable,
intelligent software development partners.

</details>


### [132] [Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation](https://arxiv.org/abs/2505.21525)
*Peiliang Gong,Yucheng Wang,Min Wu,Zhenghua Chen,Xiaoli Li,Daoqiang Zhang*

Main category: cs.LG

TL;DR: TERSE是一种针对多变量时间序列数据的源自由域自适应方法，通过时空特征编码和任务设计解决现有方法忽略空间相关性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有源自由域自适应方法在多变量时间序列数据上表现不佳，主要原因是未考虑数据内在的空间相关性，而这对准确表示和跨域信息保持至关重要。

Method: 提出TERSE方法，包含定制化的时空特征编码器，通过时间恢复和空间重连任务重建被掩码的时间序列和相关结构，实现跨域时空依赖建模和特征对齐。

Result: 在三个真实世界时间序列数据集上的实验证明了TERSE的有效性和通用性，可作为即插即用模块集成到现有方法中。

Conclusion: TERSE首次同时考虑多变量时间序列源自由域自适应中的时空一致性，有效解决了跨域特征对齐问题。

Abstract: Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained model from
an annotated source domain to an unlabelled target domain without accessing the
source data, thereby preserving data privacy. While existing SFDA methods have
proven effective in reducing reliance on source data, they struggle to perform
well on multivariate time series (MTS) due to their failure to consider the
intrinsic spatial correlations inherent in MTS data. These spatial correlations
are crucial for accurately representing MTS data and preserving invariant
information across domains. To address this challenge, we propose Temporal
Restoration and Spatial Rewiring (TERSE), a novel and concise SFDA method
tailored for MTS data. Specifically, TERSE comprises a customized
spatial-temporal feature encoder designed to capture the underlying
spatial-temporal characteristics, coupled with both temporal restoration and
spatial rewiring tasks to reinstate latent representations of the temporally
masked time series and the spatially masked correlated structures. During the
target adaptation phase, the target encoder is guided to produce spatially and
temporally consistent features with the source domain by leveraging the source
pre-trained temporal restoration and spatial rewiring networks. Therefore,
TERSE can effectively model and transfer spatial-temporal dependencies across
domains, facilitating implicit feature alignment. In addition, as the first
approach to simultaneously consider spatial-temporal consistency in MTS-SFDA,
TERSE can also be integrated as a versatile plug-and-play module into
established SFDA methods. Extensive experiments on three real-world time series
datasets demonstrate the effectiveness and versatility of our approach.

</details>


### [133] [ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools](https://arxiv.org/abs/2505.21569)
*Zhucong Li,Bowei Zhang,Jin Xiao,Zhijian Zhou,Fenglei Cao,Jiaqing Liang,Yuan Qi*

Main category: cs.LG

TL;DR: 该论文提出了ChemHAS方法，通过优化代理堆叠结构减少化学工具预测误差，在四项基础化学任务中达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的代理虽能通过选择工具提升化学任务表现，但其效果受限于化学工具固有的预测误差。论文探索如何利用LLM代理反过来减少工具预测误差。

Method: 提出ChemHAS（化学分层代理堆叠）方法，通过有限数据优化代理堆叠结构来增强化学工具。

Result: ChemHAS在四项基础化学任务中实现最先进性能，并识别出四种不同的代理堆叠行为，提升可解释性。

Conclusion: 该方法能有效补偿工具预测误差，为科学研究中AI代理应用开辟了新可能性。代码和数据集已开源。

Abstract: Large Language Model (LLM)-based agents have demonstrated the ability to
improve performance in chemistry-related tasks by selecting appropriate tools.
However, their effectiveness remains limited by the inherent prediction errors
of chemistry tools. In this paper, we take a step further by exploring how
LLMbased agents can, in turn, be leveraged to reduce prediction errors of the
tools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking),
a simple yet effective method that enhances chemistry tools through optimizing
agent-stacking structures from limited data. ChemHAS achieves state-of-the-art
performance across four fundamental chemistry tasks, demonstrating that our
method can effectively compensate for prediction errors of the tools.
Furthermore, we identify and characterize four distinct agent-stacking
behaviors, potentially improving interpretability and revealing new
possibilities for AI agent applications in scientific research. Our code and
dataset are publicly available at https:
//anonymous.4open.science/r/ChemHAS-01E4/README.md.

</details>


### [134] [FCOS: A Two-Stage Recoverable Model Pruning Framework for Automatic Modulation Recognition](https://arxiv.org/abs/2505.21571)
*Yao Lu,Tengfei Ma,Zeyu Wang,Zhuangzhi Chen,Dongwei Xu,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.LG

TL;DR: 论文提出了一种名为FCOS的两阶段剪枝框架，通过通道级剪枝和层崩溃诊断实现高效模型压缩，在保持高准确率的同时大幅降低计算量和参数量。


<details>
  <summary>Details</summary>
Motivation: 随着无线通信的快速发展和数字调制方案的日益复杂，传统手动调制识别方法难以提取可靠信号特征并满足实时性需求。基于深度学习的自动调制识别方法虽提高了分类准确率，但大模型尺寸和高计算需求限制了其在资源受限设备上的部署。

Method: FCOS框架分为两个阶段：第一阶段通过层次聚类和参数融合对通道权重进行通道级剪枝；第二阶段通过层崩溃诊断模块（LaCD）识别并移除因高通道压缩比而崩溃的层。

Result: 在多个AMR基准测试中，FCOS显著优于现有剪枝方法，实现了95.51%的FLOPs减少和95.31%的参数减少，同时在Sig2019-12数据集上仅损失0.46%的准确率。

Conclusion: FCOS框架通过精细到粗糙的两阶段剪枝策略，在极端压缩、高性能和高效推理之间取得了平衡，为资源受限设备上的模型部署提供了有效解决方案。

Abstract: With the rapid development of wireless communications and the growing
complexity of digital modulation schemes, traditional manual modulation
recognition methods struggle to extract reliable signal features and meet
real-time requirements in modern scenarios. Recently, deep learning based
Automatic Modulation Recognition (AMR) approaches have greatly improved
classification accuracy. However, their large model sizes and high
computational demands hinder deployment on resource-constrained devices. Model
pruning provides a general approach to reduce model complexity, but existing
weight, channel, and layer pruning techniques each present a trade-off between
compression rate, hardware acceleration, and accuracy preservation. To this
end, in this paper, we introduce FCOS, a novel Fine-to-COarse two-Stage pruning
framework that combines channel-level pruning with layer-level collapse
diagnosis to achieve extreme compression, high performance and efficient
inference. In the first stage of FCOS, hierarchical clustering and parameter
fusion are applied to channel weights to achieve channel-level pruning. Then a
Layer Collapse Diagnosis (LaCD) module uses linear probing to identify layer
collapse and removes the collapsed layers due to high channel compression
ratio. Experiments on multiple AMR benchmarks demonstrate that FCOS outperforms
existing channel and layer pruning methods. Specifically, FCOS achieves 95.51%
FLOPs reduction and 95.31% parameter reduction while still maintaining
performance close to the original ResNet56, with only a 0.46% drop in accuracy
on Sig2019-12. Code is available at https://github.com/yaolu-zjut/FCOS.

</details>


### [135] [Spectral-inspired Neural Operator for Data-efficient PDE Simulation in Physics-agnostic Regimes](https://arxiv.org/abs/2505.21573)
*Han Wan,Rui Zhang,Hao Sun*

Main category: cs.LG

TL;DR: 本文提出了一种名为SINO的新型神经网络框架，用于从有限数据中学习偏微分方程（PDE）的算子，无需已知PDE项，并在多个PDE基准测试中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 传统的数值求解器需要精确的离散化和完整的PDE知识，而数据驱动的神经PDE求解器虽然减轻了这些限制，但需要大量训练数据且在数据稀缺时表现不佳。物理感知方法通过融入物理知识减少数据需求，但仍依赖于已知PDE项或局部数值方案。

Method: SINO框架在频域中操作，引入频率到向量模块学习类似于导数乘子的谱表示。设计了包含低通滤波的非线性算子块以防止混叠，并采用算子蒸馏技术进行高效推理。

Result: SINO在多个PDE基准测试中实现了最先进的性能，展示了强大的离散不变性和对分布外初始条件的鲁棒泛化能力，特别是在处理全局耦合系统（如Navier-Stokes方程）时表现出色。

Conclusion: SINO是首个无需显式PDE项即可从有限数据中准确模拟全局耦合系统的物理感知方法，为PDE求解提供了高效且通用的解决方案。

Abstract: Partial differential equations (PDEs) govern the spatiotemporal evolution of
various physical systems. Classical numerical solvers, while accurate, require
fine discretization and full knowledge of the governing PDEs, limiting their
applicability when the physics is unknown or fast inference is required.
Data-driven neural PDE solvers alleviate these constraints by learning from
data but demand large training datasets and perform poorly in data-scarce
regimes. Physics-aware methods mitigate data requirements by incorporating
physical knowledge yet rely on known PDE terms or local numerical schemes,
restricting their ability to handle unknown or globally coupled systems. In
this work, we propose the Spectral-inspired Neural Operator (SINO), a novel
framework that learns PDE operators from limited trajectories (as few as 2-5),
without any known PDE terms. SINO operates in the frequency domain and
introduces a Frequency-to-Vector module to learn spectral representations
analogous to derivative multipliers. To model nonlinear physical interactions,
we design a nonlinear operator block that includes a $\Pi$-Block with low-pass
filtering to prevent aliasing. Finally, we introduce an operator distillation
technique to distill the trained model for efficient inference. SINO achieves
state-of-the-art results across multiple PDE benchmarks, demonstrating strong
discretization invariance and robust generalization to out-of-distribution
initial conditions. To our knowledge, SINO is the first physics-aware method
capable of accurately simulating globally coupled systems (e.g., the
Navier-Stokes equations) from limited data without any explicit PDE terms.

</details>


### [136] [Concentration Distribution Learning from Label Distributions](https://arxiv.org/abs/2505.21576)
*Jiawei Tang,Yuheng Jia*

Main category: cs.LG

TL;DR: 该论文提出了一种改进的标签分布学习范式——浓度分布学习（CDL-LD），通过引入背景浓度概念解决传统方法忽略标签绝对强度的问题，并提出了结合概率方法和神经网络的新模型。实验证明该方法能有效提取背景浓度并提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统标签分布学习（LDL）仅关注标签的相对描述度，忽略了标签的绝对强度，导致无法获取标签空间中未包含的隐藏标签总描述度，造成信息丢失和实例混淆。

Method: 提出背景浓度概念作为标签分布的绝对描述度项，将其融入LDL过程形成浓度分布学习范式；设计结合概率方法与神经网络的新模型，从现有LDL数据集中学习标签分布和背景浓度。

Result: 实验表明，该方法能有效从标签分布中提取背景浓度，且预测精度优于当前最先进的LDL方法。代码已开源。

Conclusion: 浓度分布学习通过引入背景浓度弥补了传统LDL的不足，实现了更全面的标签描述度建模，为复杂场景下的标签预测提供了新思路。

Abstract: Label distribution learning (LDL) is an effective method to predict the
relative label description degree (a.k.a. label distribution) of a sample.
However, the label distribution is not a complete representation of an instance
because it overlooks the absolute intensity of each label. Specifically, it's
impossible to obtain the total description degree of hidden labels that not in
the label space, which leads to the loss of information and confusion in
instances. To solve the above problem, we come up with a new concept named
background concentration to serve as the absolute description degree term of
the label distribution and introduce it into the LDL process, forming the
improved paradigm of concentration distribution learning. Moreover, we propose
a novel model by probabilistic methods and neural networks to learn label
distributions and background concentrations from existing LDL datasets.
Extensive experiments prove that the proposed approach is able to extract
background concentrations from label distributions while producing more
accurate prediction results than the state-of-the-art LDL methods. The code is
available in https://github.com/seutjw/CDL-LD.

</details>


### [137] [Fairness in Federated Learning: Fairness for Whom?](https://arxiv.org/abs/2505.21584)
*Afaf Taik,Khaoula Chehbouni,Golnoosh Farnadi*

Main category: cs.LG

TL;DR: 该论文指出联邦学习中的公平性研究存在五大常见陷阱，并提出以危害为中心的框架来更全面地定义和评估公平性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习公平性研究过于关注系统级指标，忽视了社会技术背景和多利益相关者的实际影响，导致定义和评估方式脱离实际应用场景。

Method: 通过对文献的系统性标注分析，评估公平性定义、设计决策、评估实践和用例，识别出五大常见陷阱。

Result: 分析揭示了五大陷阱：1) 仅从服务器-客户端架构视角定义公平性；2) 模拟与用例背景不匹配；3) 混淆系统保护与用户保护；4) 干预措施孤立于生命周期阶段；5) 缺乏多利益相关者对齐。

Conclusion: 提出以危害为中心的框架，将公平性定义与具体风险和利益相关者脆弱性联系起来，建议开展更全面、情境感知和可问责的公平性研究。

Abstract: Fairness in federated learning has emerged as a rapidly growing area of
research, with numerous works proposing formal definitions and algorithmic
interventions. Yet, despite this technical progress, fairness in FL is often
defined and evaluated in ways that abstract away from the sociotechnical
contexts in which these systems are deployed. In this paper, we argue that
existing approaches tend to optimize narrow system level metrics, such as
performance parity or contribution-based rewards, while overlooking how harms
arise throughout the FL lifecycle and how they impact diverse stakeholders. We
support this claim through a critical analysis of the literature, based on a
systematic annotation of papers for their fairness definitions, design
decisions, evaluation practices, and motivating use cases. Our analysis reveals
five recurring pitfalls: 1) fairness framed solely through the lens of server
client architecture, 2) a mismatch between simulations and motivating use-cases
and contexts, 3) definitions that conflate protecting the system with
protecting its users, 4) interventions that target isolated stages of the
lifecycle while neglecting upstream and downstream effects, 5) and a lack of
multi-stakeholder alignment where multiple fairness definitions can be relevant
at once. Building on these insights, we propose a harm centered framework that
links fairness definitions to concrete risks and stakeholder vulnerabilities.
We conclude with recommendations for more holistic, context-aware, and
accountable fairness research in FL.

</details>


### [138] [CellCLAT: Preserving Topology and Trimming Redundancy in Self-Supervised Cellular Contrastive Learning](https://arxiv.org/abs/2505.21587)
*Bin Qin,Qirui Ji,Jiangmeng Li,Yupeng Wang,Xuesong Wu,Jianwen Cao,Fanjiang Xu*

Main category: cs.LG

TL;DR: 本文提出了一种名为CellCLAT的自监督拓扑深度学习框架，通过参数扰动增强和元学习修剪策略，解决了细胞复合体建模中的结构约束和语义冗余问题。


<details>
  <summary>Details</summary>
Motivation: 细胞复合体在建模高阶交互方面比单纯复形更具表达能力，但现有的自监督学习方法面临两大挑战：细胞复合体的外在结构约束和内在语义冗余。这些限制阻碍了自监督拓扑深度学习的发展。

Method: CellCLAT框架包含两部分：1) 基于参数扰动的增强方法，在不改变细胞结构的前提下注入噪声；2) 通过双层元学习的细胞修剪调度器，屏蔽任务无关细胞的梯度贡献。

Result: 理论和实验验证表明，CellCLAT在自监督图学习任务上显著优于现有方法，成为该领域的重要突破。

Conclusion: 该研究为细胞复合体的自监督学习提供了创新解决方案，成功解决了结构约束和语义冗余问题，推动了拓扑深度学习的发展。

Abstract: Self-supervised topological deep learning (TDL) represents a nascent but
underexplored area with significant potential for modeling higher-order
interactions in simplicial complexes and cellular complexes to derive
representations of unlabeled graphs. Compared to simplicial complexes, cellular
complexes exhibit greater expressive power. However, the advancement in
self-supervised learning for cellular TDL is largely hindered by two core
challenges: \textit{extrinsic structural constraints} inherent to cellular
complexes, and intrinsic semantic redundancy in cellular representations. The
first challenge highlights that traditional graph augmentation techniques may
compromise the integrity of higher-order cellular interactions, while the
second underscores that topological redundancy in cellular complexes
potentially diminish task-relevant information. To address these issues, we
introduce Cellular Complex Contrastive Learning with Adaptive Trimming
(CellCLAT), a twofold framework designed to adhere to the combinatorial
constraints of cellular complexes while mitigating informational redundancy.
Specifically, we propose a parameter perturbation-based augmentation method
that injects controlled noise into cellular interactions without altering the
underlying cellular structures, thereby preserving cellular topology during
contrastive learning. Additionally, a cellular trimming scheduler is employed
to mask gradient contributions from task-irrelevant cells through a bi-level
meta-learning approach, effectively removing redundant topological elements
while maintaining critical higher-order semantics. We provide theoretical
justification and empirical validation to demonstrate that CellCLAT achieves
substantial improvements over existing self-supervised graph learning methods,
marking a significant attempt in this domain.

</details>


### [139] [Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning](https://arxiv.org/abs/2505.21591)
*Maosen Zhao,Pengtao Chen,Chong Yu,Yan Wen,Xudong Tan,Tao Chen*

Main category: cs.LG

TL;DR: 该论文提出了一种混合符号浮点量化（MSFP）框架，首次在模型量化中引入无符号浮点量化，结合时间感知LoRA（TALoRA）和去噪因子损失对齐（DFA），成功实现了扩散模型的4位浮点量化，性能优于现有的4位整数量化方法。


<details>
  <summary>Details</summary>
Motivation: 现有的4位量化方法主要基于整数量化和训练后量化微调，但性能不稳定。受浮点量化在大语言模型中成功的启发，作者探索了扩散模型的低位浮点量化，并识别了关键挑战，如符号浮点量化无法处理非对称激活分布、微调过程中未充分考虑去噪过程的时间复杂性，以及微调损失与量化误差之间的不对齐。

Method: 提出了混合符号浮点量化（MSFP）框架，首次引入无符号浮点量化，并结合时间感知LoRA（TALoRA）和去噪因子损失对齐（DFA），以确保精确和稳定的微调。

Result: 大量实验表明，该方法是首个在扩散模型中实现4位浮点量化并优于现有4位整数量化训练后微调方法的工作。

Conclusion: 通过MSFP框架及其配套技术，成功解决了扩散模型4位量化中的关键挑战，实现了性能优越的4位浮点量化，为扩散模型的高效部署提供了新思路。

Abstract: Model quantization reduces the bit-width of weights and activations,
improving memory efficiency and inference speed in diffusion models. However,
achieving 4-bit quantization remains challenging. Existing methods, primarily
based on integer quantization and post-training quantization fine-tuning,
struggle with inconsistent performance. Inspired by the success of
floating-point (FP) quantization in large language models, we explore low-bit
FP quantization for diffusion models and identify key challenges: the failure
of signed FP quantization to handle asymmetric activation distributions, the
insufficient consideration of temporal complexity in the denoising process
during fine-tuning, and the misalignment between fine-tuning loss and
quantization error. To address these challenges, we propose the mixup-sign
floating-point quantization (MSFP) framework, first introducing unsigned FP
quantization in model quantization, along with timestep-aware LoRA (TALoRA) and
denoising-factor loss alignment (DFA), which ensure precise and stable
fine-tuning. Extensive experiments show that we are the first to achieve
superior performance in 4-bit FP quantization for diffusion models,
outperforming existing PTQ fine-tuning methods in 4-bit INT quantization.

</details>


### [140] [Relevance-driven Input Dropout: an Explanation-guided Regularization Technique](https://arxiv.org/abs/2505.21595)
*Shreyas Gururaj,Lars Grüne,Wojciech Samek,Sebastian Lapuschkin,Leander Weber*

Main category: cs.LG

TL;DR: 论文提出了一种名为RelDrop的新型数据增强方法，通过选择性遮挡输入中最相关的区域，提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有数据增强方法多采用随机遮挡，忽视了影响模型决策的关键区域，导致泛化能力不足。

Method: 提出Relevance-driven Input Dropout (RelDrop)，基于输入特征的相关性进行选择性遮挡，促使模型利用更多重要特征。

Result: 实验表明，RelDrop提升了模型对遮挡的鲁棒性，使模型更充分利用感兴趣区域的特征，并提高了推理时的泛化性能。

Conclusion: RelDrop通过有信息量的正则化，有效提升了模型的泛化能力，是一种有前景的数据增强方法。

Abstract: Overfitting is a well-known issue extending even to state-of-the-art (SOTA)
Machine Learning (ML) models, resulting in reduced generalization, and a
significant train-test performance gap. Mitigation measures include a
combination of dropout, data augmentation, weight decay, and other
regularization techniques. Among the various data augmentation strategies,
occlusion is a prominent technique that typically focuses on randomly masking
regions of the input during training. Most of the existing literature
emphasizes randomness in selecting and modifying the input features instead of
regions that strongly influence model decisions. We propose Relevance-driven
Input Dropout (RelDrop), a novel data augmentation method which selectively
occludes the most relevant regions of the input, nudging the model to use other
important features in the prediction process, thus improving model
generalization through informed regularization. We further conduct qualitative
and quantitative analyses to study how Relevance-driven Input Dropout (RelDrop)
affects model decision-making. Through a series of experiments on benchmark
datasets, we demonstrate that our approach improves robustness towards
occlusion, results in models utilizing more features within the region of
interest, and boosts inference time generalization performance. Our code is
available at https://github.com/Shreyas-Gururaj/LRP_Relevance_Dropout.

</details>


### [141] [SOSBENCH: Benchmarking Safety Alignment on Scientific Knowledge](https://arxiv.org/abs/2505.21605)
*Fengqing Jiang,Fengbo Ma,Zhangchen Xu,Yuetai Li,Bhaskar Ramasubramanian,Luyao Niu,Bo Li,Xianyan Chen,Zhen Xiang,Radha Poovendran*

Main category: cs.LG

TL;DR: 论文提出SOSBench基准测试，评估大语言模型在科学高风险领域的滥用漏洞，发现先进模型存在严重安全对齐缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有安全基准测试未能充分评估大语言模型在知识密集型危险场景中的安全性，特别是在科学高风险领域的滥用问题。

Method: 构建SOSBench基准测试，涵盖六大高风险科学领域，包含3000个基于真实法规的提示，通过LLM辅助进化流程生成多样化滥用场景。

Result: 前沿模型在所有领域均表现出高比例的有害响应（如Deepseek-R1达79.1%，GPT-4.1达47.3%），揭示其安全对齐不足。

Conclusion: 研究结果凸显强大语言模型负责任部署的紧迫性，需改进安全对齐机制以防范科学滥用风险。

Abstract: Large language models (LLMs) exhibit advancing capabilities in complex tasks,
such as reasoning and graduate-level question answering, yet their resilience
against misuse, particularly involving scientifically sophisticated risks,
remains underexplored. Existing safety benchmarks typically focus either on
instructions requiring minimal knowledge comprehension (e.g., ``tell me how to
build a bomb") or utilize prompts that are relatively low-risk (e.g.,
multiple-choice or classification tasks about hazardous content). Consequently,
they fail to adequately assess model safety when handling knowledge-intensive,
hazardous scenarios.
  To address this critical gap, we introduce SOSBench, a regulation-grounded,
hazard-focused benchmark encompassing six high-risk scientific domains:
chemistry, biology, medicine, pharmacology, physics, and psychology. The
benchmark comprises 3,000 prompts derived from real-world regulations and laws,
systematically expanded via an LLM-assisted evolutionary pipeline that
introduces diverse, realistic misuse scenarios (e.g., detailed explosive
synthesis instructions involving advanced chemical formulas). We evaluate
frontier models within a unified evaluation framework using our SOSBench.
Despite their alignment claims, advanced models consistently disclose
policy-violating content across all domains, demonstrating alarmingly high
rates of harmful responses (e.g., 79.1% for Deepseek-R1 and 47.3% for GPT-4.1).
These results highlight significant safety alignment deficiencies and
underscore urgent concerns regarding the responsible deployment of powerful
LLMs.

</details>


### [142] [Learning Where to Learn: Training Distribution Selection for Provable OOD Performance](https://arxiv.org/abs/2505.21626)
*Nicolas Guerra,Nicholas H. Nelsen,Yunan Yang*

Main category: cs.LG

TL;DR: 该论文研究了如何通过优化训练数据分布来提升机器学习模型的分布外泛化能力，提出了两种算法策略并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在训练数据分布与测试数据分布不一致时，性能会显著下降。论文旨在解决这一分布外泛化问题，通过优化训练数据分布来提升模型在未知领域的表现。

Method: 论文提出了两种方法：1) 将分布外风险最小化转化为概率测度空间的双层优化问题；2) 最小化分布外误差的理论上界。

Result: 实验表明，这两种方法在函数逼近和算子学习任务中显著优于传统的固定分布经验风险最小化方法。

Conclusion: 论文证明了分布感知训练作为一种原则性和实用性框架，能够有效提升模型的分布外泛化能力。

Abstract: Out-of-distribution (OOD) generalization remains a fundamental challenge in
machine learning. Models trained on one data distribution often experience
substantial performance degradation when evaluated on shifted or unseen
domains. To address this challenge, the present paper studies the design of
training data distributions that maximize average-case OOD performance. First,
a theoretical analysis establishes a family of generalization bounds that
quantify how the choice of training distribution influences OOD error across a
predefined family of target distributions. These insights motivate the
introduction of two complementary algorithmic strategies: (i) directly
formulating OOD risk minimization as a bilevel optimization problem over the
space of probability measures and (ii) minimizing a theoretical upper bound on
OOD error. Last, the paper evaluates the two approaches across a range of
function approximation and operator learning examples. The proposed methods
significantly improve OOD accuracy over standard empirical risk minimization
with a fixed distribution. These results highlight the potential of
distribution-aware training as a principled and practical framework for robust
OOD generalization.

</details>


### [143] [Apprenticeship learning with prior beliefs using inverse optimization](https://arxiv.org/abs/2505.21639)
*Mauricio Junca,Esteban Leiva*

Main category: cs.LG

TL;DR: 本文探讨了逆向强化学习（IRL）与逆向优化（IO）在马尔可夫决策过程（MDP）中的关系，提出了一种结合先验信念的框架，并展示了学徒学习（AL）作为其特例。通过正则化方法解决了IRL的不适定性问题，并使用随机镜像下降（SMD）进行求解。


<details>
  <summary>Details</summary>
Motivation: 尽管逆向强化学习（IRL）和逆向优化（IO）在马尔可夫决策过程（MDP）中解决相同的问题，但两者之间的关系在文献中尚未充分探索。本文旨在重新审视IRL、IO和学徒学习（AL）之间的关系，并通过引入先验信念来改进现有方法。

Method: 本文提出了一种结合先验信念的IRL和AL框架，将AL问题表述为一个正则化的极小极大问题。通过引入正则化项，解决了IRL的不适定性问题。使用随机镜像下降（SMD）求解这一正则化的凸凹极小极大问题。

Result: 数值实验表明，正则化在学习和学徒策略中起到了关键作用。本文的框架展示了AL形式化是当正则化项缺失时的一个特例，并通过SMD方法建立了收敛性边界。

Conclusion: 本文通过结合先验信念和正则化方法，为IRL和AL问题提供了一个统一的框架，并展示了其在解决不适定性问题和学习成本向量方面的有效性。

Abstract: The relationship between inverse reinforcement learning (IRL) and inverse
optimization (IO) for Markov decision processes (MDPs) has been relatively
underexplored in the literature, despite addressing the same problem. In this
work, we revisit the relationship between the IO framework for MDPs, IRL, and
apprenticeship learning (AL). We incorporate prior beliefs on the structure of
the cost function into the IRL and AL problems, and demonstrate that the
convex-analytic view of the AL formalism (Kamoutsi et al., 2021) emerges as a
relaxation of our framework. Notably, the AL formalism is a special case in our
framework when the regularization term is absent. Focusing on the suboptimal
expert setting, we formulate the AL problem as a regularized min-max problem.
The regularizer plays a key role in addressing the ill-posedness of IRL by
guiding the search for plausible cost functions. To solve the resulting
regularized-convex-concave-min-max problem, we use stochastic mirror descent
(SMD) and establish convergence bounds for the proposed method. Numerical
experiments highlight the critical role of regularization in learning cost
vectors and apprentice policies.

</details>


### [144] [Efficient Diffusion Models for Symmetric Manifolds](https://arxiv.org/abs/2505.21640)
*Oren Mangoubi,Neil He,Nisheeth K. Vishnoi*

Main category: cs.LG

TL;DR: 提出了一种针对对称黎曼流形的高效扩散模型框架，通过空间变化协方差和欧几里得布朗运动投影绕过热核计算，显著提升训练速度和样本质量。


<details>
  <summary>Details</summary>
Motivation: 现有流形扩散模型依赖热核计算，缺乏闭式解且计算复杂度高（指数级或梯度评估次数多），亟需一种更高效的方法。

Method: 设计具有空间变化协方差的对称流形扩散模型，利用欧几里得布朗运动投影避免热核计算，结合伊藤引理推导高效训练目标。

Result: 模型在环面、特殊正交群和酉群上训练速度更快（每步O(1)梯度评估和近线性算术操作），样本质量优于现有方法。

Conclusion: 该框架缩小了对称流形与欧几里得空间扩散模型的效率差距，对称性保障了扩散的Lipschitz条件，实现高效精准采样。

Abstract: We introduce a framework for designing efficient diffusion models for
$d$-dimensional symmetric-space Riemannian manifolds, including the torus,
sphere, special orthogonal group and unitary group. Existing manifold diffusion
models often depend on heat kernels, which lack closed-form expressions and
require either $d$ gradient evaluations or exponential-in-$d$ arithmetic
operations per training step. We introduce a new diffusion model for symmetric
manifolds with a spatially-varying covariance, allowing us to leverage a
projection of Euclidean Brownian motion to bypass heat kernel computations. Our
training algorithm minimizes a novel efficient objective derived via Ito's
Lemma, allowing each step to run in $O(1)$ gradient evaluations and
nearly-linear-in-$d$ ($O(d^{1.19})$) arithmetic operations, reducing the gap
between diffusions on symmetric manifolds and Euclidean space. Manifold
symmetries ensure the diffusion satisfies an "average-case" Lipschitz
condition, enabling accurate and efficient sample generation. Empirically, our
model outperforms prior methods in training speed and improves sample quality
on synthetic datasets on the torus, special orthogonal group, and unitary
group.

</details>


### [145] [PrivATE: Differentially Private Confidence Intervals for Average Treatment Effects](https://arxiv.org/abs/2505.21641)
*Maresa Schröder,Justin Hartenstein,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 本文提出了一种名为PrivATE的新型机器学习框架，用于在差分隐私保护下计算平均处理效应（ATE）的置信区间（CIs）。


<details>
  <summary>Details</summary>
Motivation: 在医学等安全关键应用中，评估药物或其他医疗干预措施的有效性通常需要可靠的ATE推断，如置信区间。然而，这些数据往往涉及敏感信息，需要保护隐私。

Method: PrivATE框架包含三个步骤：(i)通过输出扰动估计差分隐私的ATE；(ii)通过截断输出扰动机制估计差分隐私的方差；(iii)构建置信区间，同时考虑估计和隐私化步骤的不确定性。

Result: PrivATE框架具有模型无关性、双重鲁棒性，并能确保有效的置信区间。通过合成和真实医疗数据集验证了其有效性。

Conclusion: PrivATE是首个在（ε, δ）-差分隐私下为ATE提供通用、双重鲁棒且有效置信区间的框架。

Abstract: The average treatment effect (ATE) is widely used to evaluate the
effectiveness of drugs and other medical interventions. In safety-critical
applications like medicine, reliable inferences about the ATE typically require
valid uncertainty quantification, such as through confidence intervals (CIs).
However, estimating treatment effects in these settings often involves
sensitive data that must be kept private. In this work, we present PrivATE, a
novel machine learning framework for computing CIs for the ATE under
differential privacy. Specifically, we focus on deriving valid
privacy-preserving CIs for the ATE from observational data. Our PrivATE
framework consists of three steps: (i) estimating a differentially private ATE
through output perturbation; (ii) estimating the differentially private
variance through a truncated output perturbation mechanism; and (iii)
constructing the CIs while accounting for the uncertainty from both the
estimation and privatization steps. Our PrivATE framework is model agnostic,
doubly robust, and ensures valid CIs. We demonstrate the effectiveness of our
framework using synthetic and real-world medical datasets. To the best of our
knowledge, we are the first to derive a general, doubly robust framework for
valid CIs of the ATE under ($\varepsilon$, $\delta$)-differential privacy.

</details>


### [146] [AutoSGD: Automatic Learning Rate Selection for Stochastic Gradient Descent](https://arxiv.org/abs/2505.21651)
*Nikola Surjanovic,Alexandre Bouchard-Côté,Trevor Campbell*

Main category: cs.LG

TL;DR: AutoSGD自动调整学习率，减少人工调参需求，在多种任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 学习率是SGD的关键参数，但手动调整耗时费力，需要自动化解决方案。

Method: 提出AutoSGD方法，动态决定每轮迭代的学习率增减，并给出收敛性理论证明。

Result: 实验表明该方法在传统优化问题和机器学习任务中均表现良好。

Conclusion: AutoSGD有效解决了学习率调参难题，具有实用价值和理论保障。

Abstract: The learning rate is an important tuning parameter for stochastic gradient
descent (SGD) and can greatly influence its performance. However, appropriate
selection of a learning rate schedule across all iterations typically requires
a non-trivial amount of user tuning effort. To address this, we introduce
AutoSGD: an SGD method that automatically determines whether to increase or
decrease the learning rate at a given iteration and then takes appropriate
action. We introduce theory supporting the convergence of AutoSGD, along with
its deterministic counterpart for standard gradient descent. Empirical results
suggest strong performance of the method on a variety of traditional
optimization problems and machine learning tasks.

</details>


### [147] [PreGenie: An Agentic Framework for High-quality Visual Presentation Generation](https://arxiv.org/abs/2505.21660)
*Xiaojie Xu,Xinli Xu,Sirui Chen,Haoyu Chen,Fan Zhang,Ying-Cong Chen*

Main category: cs.LG

TL;DR: 论文提出PreGenie框架，利用多模态大语言模型生成高质量可视化演示，解决布局混乱、图文不匹配等问题。


<details>
  <summary>Details</summary>
Motivation: 现有自动化演示生成工具存在布局混乱、文本摘要不准确、图文不匹配等问题，难以满足正式场景需求。

Method: 基于Slidev框架，采用两阶段流程：分析生成阶段总结多模态输入并生成初始代码；评审再生阶段迭代优化代码和幻灯片。

Result: 实验表明PreGenie在多模态理解和美学表现上优于现有模型，更符合人类设计偏好。

Conclusion: PreGenie框架能有效提升自动化演示生成质量，适用于正式场景。

Abstract: Visual presentations are vital for effective communication. Early attempts to
automate their creation using deep learning often faced issues such as poorly
organized layouts, inaccurate text summarization, and a lack of image
understanding, leading to mismatched visuals and text. These limitations
restrict their application in formal contexts like business and scientific
research. To address these challenges, we propose PreGenie, an agentic and
modular framework powered by multimodal large language models (MLLMs) for
generating high-quality visual presentations.
  PreGenie is built on the Slidev presentation framework, where slides are
rendered from Markdown code. It operates in two stages: (1) Analysis and
Initial Generation, which summarizes multimodal input and generates initial
code, and (2) Review and Re-generation, which iteratively reviews intermediate
code and rendered slides to produce final, high-quality presentations. Each
stage leverages multiple MLLMs that collaborate and share information.
Comprehensive experiments demonstrate that PreGenie excels in multimodal
understanding, outperforming existing models in both aesthetics and content
consistency, while aligning more closely with human design preferences.

</details>


### [148] [Efficient Controllable Diffusion via Optimal Classifier Guidance](https://arxiv.org/abs/2505.21666)
*Owen Oertell,Shikun Sun,Yiding Chen,Jin Peng Zhou,Zhiyong Wang,Wen Sun*

Main category: cs.LG

TL;DR: 论文提出了一种基于监督学习的可控扩散模型（SLCD），通过迭代生成在线数据并训练小型分类器来引导扩散模型的生成，避免了传统强化学习方法的过拟合和高资源消耗问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的可控生成在图像、分子和DNA/序列生成等应用中具有重要意义。传统的基于强化学习的微调方法容易过拟合奖励函数且资源消耗大，因此需要一种更高效且简单的方法。

Method: SLCD将可控生成问题转化为优化KL正则化目标函数的分布问题，通过在线数据生成和小型分类器训练来引导扩散模型生成，避免了复杂的强化学习或控制概念。

Result: 理论分析表明，SLCD在KL散度下可以收敛到KL正则化目标的最优解。实验证明，SLCD在连续扩散的图像生成和离散扩散的生物序列生成中，都能高效生成高质量样本。

Conclusion: SLCD提供了一种简单高效的可控扩散生成方法，避免了强化学习的复杂性，并在理论和实验上验证了其有效性。

Abstract: The controllable generation of diffusion models aims to steer the model to
generate samples that optimize some given objective functions. It is desirable
for a variety of applications including image generation, molecule generation,
and DNA/sequence generation. Reinforcement Learning (RL) based fine-tuning of
the base model is a popular approach but it can overfit the reward function
while requiring significant resources. We frame controllable generation as a
problem of finding a distribution that optimizes a KL-regularized objective
function. We present SLCD -- Supervised Learning based Controllable Diffusion,
which iteratively generates online data and trains a small classifier to guide
the generation of the diffusion model. Similar to the standard
classifier-guided diffusion, SLCD's key computation primitive is classification
and does not involve any complex concepts from RL or control. Via a reduction
to no-regret online learning analysis, we show that under KL divergence, the
output from SLCD provably converges to the optimal solution of the
KL-regularized objective. Further, we empirically demonstrate that SLCD can
generate high quality samples with nearly the same inference time as the base
model in both image generation with continuous diffusion and biological
sequence generation with discrete diffusion. Our code is available at
https://github.com/Owen-Oertell/slcd

</details>


### [149] [What happens when generative AI models train recursively on each others' generated outputs?](https://arxiv.org/abs/2505.21677)
*Hung Ahn Vu,Galen Reeves,Emily Wenger*

Main category: cs.LG

TL;DR: 研究探讨了生成式AI模型相互训练的影响，发现既有益处也有同质化风险。


<details>
  <summary>Details</summary>
Motivation: 随着社会对生成式AI工具的依赖增加，理解模型通过数据交互训练可能产生的下游效应变得至关重要。

Method: 通过实证分析数据交互的实际影响，建立理论模型，并进行实验验证长期效果。

Result: 数据交互能帮助模型接触新概念，但也可能导致任务表现同质化。

Conclusion: 数据交互对生成式AI模型具有双重影响，需谨慎管理以避免潜在风险。

Abstract: The internet is full of AI-generated content while also serving as a common
source of training data for generative AI (genAI) models. This duality raises
the possibility that future genAI models may be trained on other models'
generated outputs. Prior work has studied consequences of models training on
their own generated outputs, but limited work has considered what happens if
models ingest content produced by other models. Given society's increasing
dependence on genAI tools, understanding downstream effects of such
data-mediated model interactions is critical. To this end, we provide empirical
evidence for how data-mediated interactions might unfold in practice, develop a
theoretical model for this interactive training process, and show
experimentally possible long-term results of such interactions. We find that
data-mediated interactions can benefit models by exposing them to novel
concepts perhaps missed in original training data, but also can homogenize
their performance on shared tasks.

</details>


### [150] [multivariateGPT: a decoder-only transformer for multivariate categorical and numeric data](https://arxiv.org/abs/2505.21680)
*Andrew J. Loza,Jun Yup Kim,Shangzheng Song,Yihang Liu,Joseph J. Y. Sung,R Andrew Taylor,Dennis L. Shung*

Main category: cs.LG

TL;DR: 提出multivariateGPT架构，统一处理混合类别/数值数据，扩展Transformer在复杂时序数据中的应用。


<details>
  <summary>Details</summary>
Motivation: 现实数据常混合类别与数值特征，且采样间隔不规则。现有方法（如离散token或神经ODE）难以兼顾二者表示能力与不规则采样。

Method: 通过自回归序列分解、嵌入方案和联合分布损失函数，将下一token预测任务扩展为类别与数值的联合似然估计。

Result: 模型能高效学习物理系统模式，并建模心电图、电子健康记录等复杂时序数据。

Conclusion: 该工作将Transformer的适用性扩展到混合型数据领域，提供统一建模框架。

Abstract: Real-world processes often generate data that are a mix of categorical and
numeric values that are recorded at irregular and informative intervals.
Discrete token-based approaches are limited in numeric representation capacity
while methods like neural ordinary differential equations are not well suited
for categorical data or informative sampling and require augmentation to handle
certain classes of trajectories. Here, we present multivariateGPT, a single
architecture for modeling sequences of mixed categorical (including tokenized
text) and numeric data. This is accomplished with an autoregressive sequence
decomposition, embedding scheme, and loss function that extend the next token
prediction task to likelihood estimation of the joint distribution of next
token class and value. We demonstrate how this approach can efficiently learn
to generalize patterns in simple physical systems and model complex time series
including electrocardiograms and multivariate electronic health record data.
This work extends the utility of transformer based models to additional classes
of data.

</details>


### [151] [Incentivizing Permissionless Distributed Learning of LLMs](https://arxiv.org/abs/2505.21684)
*Joel Lidin,Amir Sarfi,Evangelos Pappas,Samuel Dare,Eugene Belilovsky,Jacob Steeves*

Main category: cs.LG

TL;DR: 论文提出了一种名为Gauntlet的激励系统，用于分布式深度学习基础模型的训练，通过奖励参与者的贡献，成功训练了一个12亿参数的LLM模型。


<details>
  <summary>Details</summary>
Motivation: 为了解决分布式深度学习中参与者贡献的激励问题，确保在没有权限控制的情况下，能够有效聚合参与者的伪梯度更新。

Method: 采用两阶段机制快速筛选参与者的在线状态、可靠性和同步性，结合核心组件估计伪梯度贡献前后的损失变化，并使用OpenSkill评分系统跟踪伪梯度得分的竞争力。

Result: 在bittensor区块链上部署的Gauntlet系统成功训练了一个12亿参数的LLM模型，并通过真实价值代币奖励参与者，证明了该激励系统的有效性。

Conclusion: Gauntlet激励系统在分布式深度学习训练中表现出色，能够有效激励参与者贡献，并训练出具有竞争力的模型。

Abstract: We describe an incentive system for distributed deep learning of foundational
models where peers are rewarded for contributions. The incentive system,
\textit{Gauntlet}, has been deployed on the bittensor blockchain and used to
train a 1.2B LLM with completely permissionless contributions of
pseudo-gradients: no control over the users that can register or their
hardware. \textit{Gauntlet} can be applied to any synchronous distributed
training scheme that relies on aggregating updates or pseudo-gradients. We rely
on a two-stage mechanism for fast filtering of peer uptime, reliability, and
synchronization, combined with the core component that estimates the loss
before and after individual pseudo-gradient contributions. We utilized an
OpenSkill rating system to track competitiveness of pseudo-gradient scores
across time. Finally, we introduce a novel mechanism to ensure peers on the
network perform unique computations. Our live 1.2B run, which has paid out
real-valued tokens to participants based on the value of their contributions,
yielded a competitive (on a per-iteration basis) 1.2B model that demonstrates
the utility of our incentive system.

</details>


### [152] [AMSFL: Adaptive Multi-Step Federated Learning via Gradient Difference-Based Error Modeling](https://arxiv.org/abs/2505.21695)
*Ganglou Xu*

Main category: cs.LG

TL;DR: 提出了一种轻量级梯度差异近似方法(GDA)，用于联邦学习中高效通信与模型精度的平衡。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在通信效率和模型精度之间面临挑战，特别是如何在低计算成本下近似更新误差。

Method: 使用一阶信息估计局部误差趋势，避免计算完整的Hessian矩阵，并作为自适应多步联邦学习框架(AMSFL)的核心组件。

Result: GDA方法为大规模多步自适应训练环境提供了统一的误差建模策略。

Conclusion: GDA方法有效解决了联邦学习中的通信效率与精度平衡问题，且计算成本低。

Abstract: Federated learning faces critical challenges in balancing communication
efficiency and model accuracy. One key issue lies in the approximation of
update errors without incurring high computational costs. In this paper, we
propose a lightweight yet effective method called Gradient Difference
Approximation (GDA), which leverages first-order information to estimate local
error trends without computing the full Hessian matrix. The proposed method
forms a key component of the Adaptive Multi-Step Federated Learning (AMSFL)
framework and provides a unified error modeling strategy for large-scale
multi-step adaptive training environments.

</details>


### [153] [Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient Sequence Modeling](https://arxiv.org/abs/2505.21717)
*Mónika Farsang,Ramin Hasani,Radu Grosu*

Main category: cs.LG

TL;DR: LrcSSM是一种非线性循环模型，通过并行处理和梯度稳定性保证，在长序列任务中表现优于现有线性状态空间层和其他模型。


<details>
  <summary>Details</summary>
Motivation: 当前的长序列处理模型如线性状态空间层、Liquid-S4和Mamba存在计算效率低、梯度不稳定等问题，需要一种更高效的解决方案。

Method: LrcSSM通过强制状态转移矩阵为对角矩阵并在每一步学习，实现并行处理，计算复杂度为O(TD)，同时提供梯度稳定性保证。

Result: 在长序列预测任务中，LrcSSM表现优于LRU、S5和Mamba，计算效率高且内存占用低。

Conclusion: LrcSSM是一种高效且稳定的长序列处理模型，适用于大规模计算任务，性能优于现有方法。

Abstract: We present LrcSSM, a \textit{nonlinear} recurrent model that processes long
sequences as fast as today's linear state-space layers. By forcing the
state-transition matrix to be diagonal and learned at every step, the full
sequence can be solved in parallel with a single prefix-scan, giving
$\mathcal{O}(TD)$ time and memory and only $\mathcal{O}(\log T)$ sequential
depth, for input-sequence length $T$ and a state dimension $D$. Moreover,
LrcSSM offers a formal gradient-stability guarantee that other input-varying
systems such as Liquid-S4 and Mamba do not provide. Lastly, for network depth
$L$, as the forward and backward passes cost $\Theta(T\,D\,L)$ FLOPs, with its
low sequential depth and parameter count $\Theta(D\,L)$, the model follows the
compute-optimal scaling law regime ($\beta \approx 0.42$) recently observed for
Mamba, outperforming quadratic-attention Transformers at equal compute while
avoiding the memory overhead of FFT-based long convolutions. We show that on a
series of long-range forecasting tasks, LrcSSM outperforms LRU, S5 and Mamba.

</details>


### [154] [Saddle-To-Saddle Dynamics in Deep ReLU Networks: Low-Rank Bias in the First Saddle Escape](https://arxiv.org/abs/2505.21722)
*Ioannis Bantzis,James B. Simon,Arthur Jacot*

Main category: cs.LG

TL;DR: 论文研究了深度ReLU网络在初始化小权重时，梯度下降如何从参数空间中的鞍点逃离，并发现逃离方向在深层具有低秩偏置。


<details>
  <summary>Details</summary>
Motivation: 研究深度ReLU网络在梯度下降初期如何从鞍点逃离，特别是探索逃离方向的性质及其在深层网络中的低秩偏置现象。

Method: 通过分析梯度下降在参数空间中的行为，研究逃离方向的性质，并证明这些方向在深层网络中的低秩偏置。

Result: 发现最优逃离方向在深层网络中具有显著的低秩偏置，即第一奇异值比其他奇异值大至少ℓ^(1/4)倍。

Conclusion: 这一结果为证明深度ReLU网络中梯度下降的“鞍点到鞍点”动态提供了初步支持，即梯度下降会访问一系列瓶颈秩递增的鞍点。

Abstract: When a deep ReLU network is initialized with small weights, GD is at first
dominated by the saddle at the origin in parameter space. We study the
so-called escape directions, which play a similar role as the eigenvectors of
the Hessian for strict saddles. We show that the optimal escape direction
features a low-rank bias in its deeper layers: the first singular value of the
$\ell$-th layer weight matrix is at least $\ell^{\frac{1}{4}}$ larger than any
other singular value. We also prove a number of related results about these
escape directions. We argue that this result is a first step in proving
Saddle-to-Saddle dynamics in deep ReLU networks, where GD visits a sequence of
saddles with increasing bottleneck rank.

</details>


### [155] [Deep Reinforcement Learning Agents are not even close to Human Intelligence](https://arxiv.org/abs/2505.21731)
*Quentin Delfosse,Jannis Blüml,Fabian Tatai,Théo Vincent,Bjarne Gregori,Elisabeth Dillies,Jan Peters,Constantin Rothkopf,Kristian Kersting*

Main category: cs.LG

TL;DR: 论文指出深度强化学习代理在简化任务上表现不佳，揭示其对捷径的依赖，呼吁新基准测试方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索深度强化学习代理在任务简化情况下的表现，揭示其与人类行为智能的差距。

Method: 方法是通过HackAtari（一套Arcade学习环境的变体任务）评估多种算法和架构的RL代理。

Result: 结果显示RL代理在简化任务上性能显著下降，依赖捷径，与人类行为形成鲜明对比。

Conclusion: 结论强调需要新基准和方法来测试系统泛化能力，静态评估不足以实现类人智能。

Abstract: Deep reinforcement learning (RL) agents achieve impressive results in a wide
variety of tasks, but they lack zero-shot adaptation capabilities. While most
robustness evaluations focus on tasks complexifications, for which human also
struggle to maintain performances, no evaluation has been performed on tasks
simplifications. To tackle this issue, we introduce HackAtari, a set of task
variations of the Arcade Learning Environments. We use it to demonstrate that,
contrary to humans, RL agents systematically exhibit huge performance drops on
simpler versions of their training tasks, uncovering agents' consistent
reliance on shortcuts. Our analysis across multiple algorithms and
architectures highlights the persistent gap between RL agents and human
behavioral intelligence, underscoring the need for new benchmarks and
methodologies that enforce systematic generalization testing beyond static
evaluation protocols. Training and testing in the same environment is not
enough to obtain agents equipped with human-like intelligence.

</details>


### [156] [LaX: Boosting Low-Rank Training of Foundation Models via Latent Crossing](https://arxiv.org/abs/2505.21732)
*Ruijie Zhang,Ziyue Liu,Zhengyang Wang,Zheng Zhang*

Main category: cs.LG

TL;DR: LaX模块通过低秩子空间信息交互提升模型性能，参数效率提高2-3倍，性能匹配或超越全秩基准。


<details>
  <summary>Details</summary>
Motivation: 基础模型训练计算成本高，低秩分解虽参数高效但性能受限，需提升低秩模型能力。

Method: 提出LaX模块，通过低秩子空间信息交叉增强模型容量，支持即插即用。

Result: LaX在ViT和LLaMA系列模型上验证，低秩模型性能匹配全秩基准，微调任务表现提升。

Conclusion: LaX以低成本显著提升低秩模型性能，为高效参数化提供有效解决方案。

Abstract: Training foundation models such as ViTs and LLMs requires tremendous
computing cost. Low-rank matrix or tensor factorization offers a
parameter-efficient alternative, but often downgrades performance due to the
restricted parameter space. In this work, we introduce {\textbf{Latent Crossing
(LaX)}} -- a simple yet effective plug-and-play module that enhances the
capacity of low-rank models by enabling information flow across low-rank
subspaces. We extensively validate the benefits of LaX on pre-training tasks
with ViT-Base/Large and LLaMA-like models ranging from 60M to 1B parameters.
LaX boosts low-rank model performance to match or exceed the full-rank
baselines while using 2-3\(\times\) fewer parameters. When equipped with
low-rank adapters (i.e., LoRA) for fine-tuning LLaMA-7/13B, LaX consistently
improves performance on arithmetic and common sense reasoning tasks with
negligible cost.

</details>


### [157] [Simulating the Unseen: Crash Prediction Must Learn from What Did Not Happen](https://arxiv.org/abs/2505.21743)
*Zihao Li,Xinyuan Cao,Xiangbo Gao,Kexin Tian,Keshu Wu,Mohammad Anis,Hao Zhang,Keke Long,Jiwan Jiang,Xiaopeng Li,Yunlong Zhang,Tianbao Yang,Dominique Lord,Zhengzhong Tu,Yang Zhou*

Main category: cs.LG

TL;DR: 论文提出通过反事实安全学习范式转变，利用生成场景引擎和数字孪生技术，将稀疏事故数据转化为丰富预测信号，实现交通事故的主动预防。


<details>
  <summary>Details</summary>
Motivation: 传统交通事故预防受限于稀疏、噪声大且低报的事故数据，难以覆盖导致严重后果的长尾场景。为实现零死亡愿景，需从仅分析已发生事故转向探索潜在危险场景。

Method: 结合宏观事故率先验与微观生成场景引擎，构建数字孪生测试平台，通过多目标验证器保持统计真实性，合成并解释未遂事件。

Result: 该方法将稀疏事故数据转化为可用于预测的丰富信号，支持对车辆、道路及政策进行部署前的压力测试。

Conclusion: 通过反事实学习潜在事故场景，交通安全可从被动取证转向主动预防，推动零死亡愿景的实现。

Abstract: Traffic safety science has long been hindered by a fundamental data paradox:
the crashes we most wish to prevent are precisely those events we rarely
observe. Existing crash-frequency models and surrogate safety metrics rely
heavily on sparse, noisy, and under-reported records, while even sophisticated,
high-fidelity simulations undersample the long-tailed situations that trigger
catastrophic outcomes such as fatalities. We argue that the path to achieving
Vision Zero, i.e., the complete elimination of traffic fatalities and severe
injuries, requires a paradigm shift from traditional crash-only learning to a
new form of counterfactual safety learning: reasoning not only about what
happened, but also about the vast set of plausible yet perilous scenarios that
could have happened under slightly different circumstances. To operationalize
this shift, our proposed agenda bridges macro to micro. Guided by crash-rate
priors, generative scene engines, diverse driver models, and causal learning,
near-miss events are synthesized and explained. A crash-focused digital twin
testbed links micro scenes to macro patterns, while a multi-objective validator
ensures that simulations maintain statistical realism. This pipeline transforms
sparse crash data into rich signals for crash prediction, enabling the
stress-testing of vehicles, roads, and policies before deployment. By learning
from crashes that almost happened, we can shift traffic safety from reactive
forensics to proactive prevention, advancing Vision Zero.

</details>


### [158] [Revisiting Bi-Linear State Transitions in Recurrent Neural Networks](https://arxiv.org/abs/2505.21749)
*M. Reza Ebrahimi,Roland Memisevic*

Main category: cs.LG

TL;DR: 该论文重新审视了循环神经网络中隐藏单元的双线性操作，提出其作为状态跟踪任务的天然归纳偏置，并展示了其与任务复杂度对应的层次结构。


<details>
  <summary>Details</summary>
Motivation: 传统研究通常将循环神经网络中的隐藏单元视为被动存储信息的记忆单元，而本文探讨了隐藏单元作为计算主动参与者的视角，特别是在状态跟踪任务中的作用。

Method: 论文通过理论和实证分析，研究了隐藏单元与输入嵌入之间的双线性操作，将其作为状态跟踪任务中隐藏状态演变的自然归纳偏置。

Result: 研究发现双线性状态更新形成了与复杂度递增的状态跟踪任务相对应的自然层次结构，且流行的线性循环网络（如Mamba）位于该层次结构的最低复杂度中心。

Conclusion: 双线性操作为循环神经网络在状态跟踪任务中的表现提供了理论基础，揭示了隐藏单元在计算中的主动作用及其与任务复杂度的关系。

Abstract: The role of hidden units in recurrent neural networks is typically seen as
modeling memory, with research focusing on enhancing information retention
through gating mechanisms. A less explored perspective views hidden units as
active participants in the computation performed by the network, rather than
passive memory stores. In this work, we revisit bi-linear operations, which
involve multiplicative interactions between hidden units and input embeddings.
We demonstrate theoretically and empirically that they constitute a natural
inductive bias for representing the evolution of hidden states in state
tracking tasks. These are the simplest type of task that require hidden units
to actively contribute to the behavior of the network. We also show that
bi-linear state updates form a natural hierarchy corresponding to state
tracking tasks of increasing complexity, with popular linear recurrent networks
such as Mamba residing at the lowest-complexity center of that hierarchy.

</details>


### [159] [Hierarchical Reinforcement Learning with Uncertainty-Guided Diffusional Subgoals](https://arxiv.org/abs/2505.21750)
*Vivienne Huiling Wang,Tinghuai Wang,Joni Pajarinen*

Main category: cs.LG

TL;DR: 该论文提出了一种结合条件扩散模型和高斯过程先验的分层强化学习方法，以解决高层策略生成有效子目标的挑战，并在连续控制基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 分层强化学习中，低层策略随时间变化，导致高层策略难以生成有效的子目标。高层策略需要捕捉复杂的子目标分布并处理估计中的不确定性。

Method: 提出了一种方法，通过训练一个受高斯过程先验正则化的条件扩散模型，生成多样化的子目标，并利用高斯过程的不确定性量化。在此基础上，开发了一种策略，从扩散策略和高斯过程的预测均值中选择子目标。

Result: 该方法在样本效率和连续控制基准测试中的性能上优于先前的分层强化学习方法。

Conclusion: 结合条件扩散模型和高斯过程先验的方法有效解决了分层强化学习中子目标生成的挑战，提升了性能。

Abstract: Hierarchical reinforcement learning (HRL) learns to make decisions on
multiple levels of temporal abstraction. A key challenge in HRL is that the
low-level policy changes over time, making it difficult for the high-level
policy to generate effective subgoals. To address this issue, the high-level
policy must capture a complex subgoal distribution while also accounting for
uncertainty in its estimates. We propose an approach that trains a conditional
diffusion model regularized by a Gaussian Process (GP) prior to generate a
complex variety of subgoals while leveraging principled GP uncertainty
quantification. Building on this framework, we develop a strategy that selects
subgoals from both the diffusion policy and GP's predictive mean. Our approach
outperforms prior HRL methods in both sample efficiency and performance on
challenging continuous control benchmarks.

</details>


### [160] [DualSchool: How Reliable are LLMs for Optimization Education?](https://arxiv.org/abs/2505.21775)
*Michael Klamkin,Arnaud Deza,Sikai Cheng,Haoruo Zhao,Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: 论文评估了大型语言模型（LLM）在线性规划对偶生成任务中的表现，发现即使是最先进的开放LLM也难以稳定生成正确的对偶形式。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索LLM在优化课程中线性规划对偶生成（P2DC）任务上的表现，验证其能否替代传统教学方法。

Method: 提出了DualSchool框架，通过规范图编辑距离验证P2DC实例，超越现有优化模型的评估方法。

Result: 实验表明，LLM能准确复述转换步骤，但无法稳定生成正确对偶，即使对最简单的两变量实例也失败。

Conclusion: 研究对教育者、学生及大型推理系统开发具有启示意义，表明LLM当前在严格数学推理任务上存在局限。

Abstract: Consider the following task taught in introductory optimization courses which
addresses challenges articulated by the community at the intersection of
(generative) AI and OR: generate the dual of a linear program. LLMs, being
trained at web-scale, have the conversion process and many instances of Primal
to Dual Conversion (P2DC) at their disposal. Students may thus reasonably
expect that LLMs would perform well on the P2DC task. To assess this
expectation, this paper introduces DualSchool, a comprehensive framework for
generating and verifying P2DC instances. The verification procedure of
DualSchool uses the Canonical Graph Edit Distance, going well beyond existing
evaluation methods for optimization models, which exhibit many false positives
and negatives when applied to P2DC. Experiments performed by DualSchool reveal
interesting findings. Although LLMs can recite the conversion procedure
accurately, state-of-the-art open LLMs fail to consistently produce correct
duals. This finding holds even for the smallest two-variable instances and for
derivative tasks, such as correctness, verification, and error classification.
The paper also discusses the implications for educators, students, and the
development of large reasoning systems.

</details>


### [161] [Memorization to Generalization: Emergence of Diffusion Models from Associative Memory](https://arxiv.org/abs/2505.21777)
*Bao Pham,Gabriel Raya,Matteo Negri,Mohammed J. Zaki,Luca Ambrogioni,Dmitry Krotov*

Main category: cs.LG

TL;DR: 该论文从联想记忆角度分析扩散模型，发现小数据量时模型会记忆训练样本，大数据量时则生成新吸引子状态，并预测了虚假状态的存在。


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型在联想记忆框架下的行为，探索其记忆与泛化现象，特别是大数据量时出现的虚假状态问题。

Method: 将扩散模型的训练视为记忆编码，生成视为记忆检索，类比Hopfield网络的临界记忆负载现象进行分析。

Result: 发现扩散模型在小数据量时记忆训练样本，大数据量时产生新吸引子状态；理论预测并实证了虚假状态的存在。

Conclusion: 通过联想记忆视角为扩散模型的记忆-泛化现象提供了新见解，并验证了虚假状态的理论预测。

Abstract: Hopfield networks are associative memory (AM) systems, designed for storing
and retrieving patterns as local minima of an energy landscape. In the
classical Hopfield model, an interesting phenomenon occurs when the amount of
training data reaches its critical memory load $- spurious\,\,states$, or
unintended stable points, emerge at the end of the retrieval dynamics, leading
to incorrect recall. In this work, we examine diffusion models, commonly used
in generative modeling, from the perspective of AMs. The training phase of
diffusion model is conceptualized as memory encoding (training data is stored
in the memory). The generation phase is viewed as an attempt of memory
retrieval. In the small data regime the diffusion model exhibits a strong
memorization phase, where the network creates distinct basins of attraction
around each sample in the training set, akin to the Hopfield model below the
critical memory load. In the large data regime, a different phase appears where
an increase in the size of the training set fosters the creation of new
attractor states that correspond to manifolds of the generated samples.
Spurious states appear at the boundary of this transition and correspond to
emergent attractor states, which are absent in the training set, but, at the
same time, have distinct basins of attraction around them. Our findings
provide: a novel perspective on the memorization-generalization phenomenon in
diffusion models via the lens of AMs, theoretical prediction of existence of
spurious states, empirical validation of this prediction in commonly-used
diffusion models.

</details>


### [162] [P-DROP: Poisson-Based Dropout for Graph Neural Networks](https://arxiv.org/abs/2505.21783)
*Hyunsik Yun*

Main category: cs.LG

TL;DR: 提出基于泊松过程的节点选择策略，解决GNN中的过平滑问题，通过异步局部更新保持结构多样性，在标准数据集上表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNN）中重复消息传递导致的过平滑问题使节点表征趋同并丧失区分能力，需要一种新方法保持结构多样性。

Method: 为每个节点配备独立泊松时钟，实现异步局部更新，提出两种应用：替代基于dropout的正则化和动态子图训练方案。

Result: 在Cora、Citeseer和Pubmed等基准测试中，该方法相比传统Dropout、DropEdge和DropNode具有竞争力或更高准确率，尤其在训练后期。

Conclusion: 基于泊松过程的节点选择策略有效缓解GNN过平滑问题，在保持结构多样性的同时提升模型性能。

Abstract: Over-smoothing remains a major challenge in Graph Neural Networks (GNNs),
where repeated message passing causes node representations to converge and lose
discriminative power. To address this, we propose a novel node selection
strategy based on Poisson processes, introducing stochastic but structure-aware
updates. Specifically, we equip each node with an independent Poisson clock,
enabling asynchronous and localized updates that preserve structural diversity.
We explore two applications of this strategy: as a replacement for
dropout-based regularization and as a dynamic subgraph training scheme.
Experimental results on standard benchmarks (Cora, Citeseer, Pubmed)
demonstrate that our Poisson-based method yields competitive or improved
accuracy compared to traditional Dropout, DropEdge, and DropNode approaches,
particularly in later training stages.

</details>


### [163] [Born a Transformer -- Always a Transformer?](https://arxiv.org/abs/2505.21785)
*Yana Veitsman,Mayank Jobanputra,Yash Sarrof,Aleksandra Bakalova,Vera Demberg,Ellie Pavlick,Michael Hahn*

Main category: cs.LG

TL;DR: 该论文探讨了预训练大语言模型（LLMs）是否能在实践中克服Transformer架构的理论限制，特别是在序列任务中的长度泛化问题。研究发现预训练模型在检索任务中存在诱导与反诱导的不对称性，并通过微调和机理分析验证了这一现象。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索预训练LLMs是否能超越Transformer架构的理论限制，尤其是在序列任务中的表现。尽管Transformer存在理论上的局限性，但大规模预训练是否能够弥补这些限制尚不明确。

Method: 研究方法包括使用C-RASP框架分析长度泛化能力，设计检索和复制任务来测试模型表现，并通过微调和机理分析验证模型的行为。

Result: 研究发现预训练模型在检索任务中表现出诱导与反诱导的不对称性，即模型更擅长向右检索（诱导）而非向左（反诱导）。这种不对称性在理论保证长度泛化的情况下可通过微调消除。

Conclusion: 结论指出预训练虽能增强Transformer的某些能力，但无法突破其长度泛化的根本限制。研究结果揭示了预训练模型的可靠性风险，并强调了理论限制的实际影响。

Abstract: Transformers have theoretical limitations in modeling certain
sequence-to-sequence tasks, yet it remains largely unclear if these limitations
play a role in large-scale pretrained LLMs, or whether LLMs might effectively
overcome these constraints in practice due to the scale of both the models
themselves and their pretraining data. We explore how these architectural
constraints manifest after pretraining, by studying a family of
$\textit{retrieval}$ and $\textit{copying}$ tasks inspired by Liu et al.
[2024]. We use the recently proposed C-RASP framework for studying length
generalization [Huang et al., 2025b] to provide guarantees for each of our
settings. Empirically, we observe an $\textit{induction-versus-anti-induction}$
asymmetry, where pretrained models are better at retrieving tokens to the right
(induction) rather than the left (anti-induction) of a query token. This
asymmetry disappears upon targeted fine-tuning if length-generalization is
guaranteed by theory. Mechanistic analysis reveals that this asymmetry is
connected to the differences in the strength of induction versus anti-induction
circuits within pretrained Transformers. We validate our findings through
practical experiments on real-world tasks demonstrating reliability risks. Our
results highlight that pretraining selectively enhances certain Transformer
capabilities, but does not overcome fundamental length-generalization limits.

</details>


### [164] [Faster Rates for Private Adversarial Bandits](https://arxiv.org/abs/2505.21790)
*Hilal Asi,Vinod Raman,Kunal Talwar*

Main category: cs.LG

TL;DR: 本文提出了新的差分隐私算法，用于解决对抗性多臂老虎机和专家建议老虎机问题，显著改进了现有算法的遗憾上界。


<details>
  <summary>Details</summary>
Motivation: 研究差分隐私在多臂老虎机问题中的应用，旨在保护用户数据隐私的同时，保持算法的性能。

Method: 设计了一种简单高效的转换方法，将非隐私老虎机算法转换为隐私算法，并应用于对抗性老虎机和专家建议老虎机问题。

Result: 对抗性老虎机的遗憾上界改进为$O\left(\frac{\sqrt{KT}}{\sqrt{\epsilon}}\right)$，专家建议老虎机首次实现了差分隐私算法，并给出了多个遗憾上界。

Conclusion: 新算法在隐私保护下实现了次线性遗憾，首次展示了中心化与本地化差分隐私在该问题上的分离。

Abstract: We design new differentially private algorithms for the problems of
adversarial bandits and bandits with expert advice. For adversarial bandits, we
give a simple and efficient conversion of any non-private bandit algorithm to a
private bandit algorithm. Instantiating our conversion with existing
non-private bandit algorithms gives a regret upper bound of
$O\left(\frac{\sqrt{KT}}{\sqrt{\epsilon}}\right)$, improving upon the existing
upper bound $O\left(\frac{\sqrt{KT \log(KT)}}{\epsilon}\right)$ for all
$\epsilon \leq 1$. In particular, our algorithms allow for sublinear expected
regret even when $\epsilon \leq \frac{1}{\sqrt{T}}$, establishing the first
known separation between central and local differential privacy for this
problem. For bandits with expert advice, we give the first differentially
private algorithms, with expected regret
$O\left(\frac{\sqrt{NT}}{\sqrt{\epsilon}}\right),
O\left(\frac{\sqrt{KT\log(N)}\log(KT)}{\epsilon}\right)$, and
$\tilde{O}\left(\frac{N^{1/6}K^{1/2}T^{2/3}\log(NT)}{\epsilon ^{1/3}} +
\frac{N^{1/2}\log(NT)}{\epsilon}\right)$, where $K$ and $N$ are the number of
actions and experts respectively. These rates allow us to get sublinear regret
for different combinations of small and large $K, N$ and $\epsilon.$

</details>


### [165] [Multimodal Federated Learning: A Survey through the Lens of Different FL Paradigms](https://arxiv.org/abs/2505.21792)
*Yuanzhe Peng,Jieming Bian,Lei Wang,Yin Huang,Jie Xu*

Main category: cs.LG

TL;DR: 该论文系统分析了多模态联邦学习（MFL）在不同联邦学习范式下的挑战，并提出了一种新的分类方法。


<details>
  <summary>Details</summary>
Motivation: 多模态联邦学习结合了多模态数据的互补性和联邦学习的分布式训练优势，但目前缺乏从不同联邦学习范式角度进行的系统分类，这限制了对其独特挑战的理解。

Method: 论文从水平联邦学习（HFL）、垂直联邦学习（VFL）和混合联邦学习三种范式出发，系统梳理了MFL的问题定义、代表性算法及其在多模态数据下的核心挑战。

Result: 通过分类分析，论文揭示了多模态数据在分布式环境中带来的模态异构性、隐私异构性和通信效率等新挑战，并提出了未来研究方向。

Conclusion: 该研究为理解多模态联邦学习的挑战提供了新视角，并为其进一步发展奠定了基础。

Abstract: Multimodal Federated Learning (MFL) lies at the intersection of two pivotal
research areas: leveraging complementary information from multiple modalities
to improve downstream inference performance and enabling distributed training
to enhance efficiency and preserve privacy. Despite the growing interest in
MFL, there is currently no comprehensive taxonomy that organizes MFL through
the lens of different Federated Learning (FL) paradigms. This perspective is
important because multimodal data introduces distinct challenges across various
FL settings. These challenges, including modality heterogeneity, privacy
heterogeneity, and communication inefficiency, are fundamentally different from
those encountered in traditional unimodal or non-FL scenarios. In this paper,
we systematically examine MFL within the context of three major FL paradigms:
horizontal FL (HFL), vertical FL (VFL), and hybrid FL. For each paradigm, we
present the problem formulation, review representative training algorithms, and
highlight the most prominent challenge introduced by multimodal data in
distributed settings. We also discuss open challenges and provide insights for
future research. By establishing this taxonomy, we aim to uncover the novel
challenges posed by multimodal data from the perspective of different FL
paradigms and to offer a new lens through which to understand and advance the
development of MFL.

</details>


### [166] [From Directions to Cones: Exploring Multidimensional Representations of Propositional Facts in LLMs](https://arxiv.org/abs/2505.21800)
*Stanley Yu,Vaidehi Bulusu,Oscar Yasunaga,Clayton Lau,Cole Blondin,Sean O'Brien,Kevin Zhu,Vasu Sharma*

Main category: cs.LG

TL;DR: 该研究扩展了概念锥框架，发现大语言模型中真假命题由多维锥结构调控，并通过因果干预验证其效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽具备对话能力，但常生成虚假信息。先前研究认为真假命题可表示为单一线性方向，但可能未完全反映其几何结构。

Method: 将用于建模拒绝行为的概念锥框架扩展至真假领域，识别跨模型的多维锥结构，并通过因果干预、泛化性测试和行为保留性验证其效果。

Result: 发现多维锥能因果调控模型对事实陈述的反应，且该结构具有跨架构泛化性，干预后不影响无关模型行为。

Conclusion: 概念锥是揭示大语言模型抽象行为的有力工具，真假命题由更丰富的多维度几何结构支配。

Abstract: Large Language Models (LLMs) exhibit strong conversational abilities but
often generate falsehoods. Prior work suggests that the truthfulness of simple
propositions can be represented as a single linear direction in a model's
internal activations, but this may not fully capture its underlying geometry.
In this work, we extend the concept cone framework, recently introduced for
modeling refusal, to the domain of truth. We identify multi-dimensional cones
that causally mediate truth-related behavior across multiple LLM families. Our
results are supported by three lines of evidence: (i) causal interventions
reliably flip model responses to factual statements, (ii) learned cones
generalize across model architectures, and (iii) cone-based interventions
preserve unrelated model behavior. These findings reveal the richer,
multidirectional structure governing simple true/false propositions in LLMs and
highlight concept cones as a promising tool for probing abstract behaviors.

</details>


### [167] [Towards Operational Automated Greenhouse Gas Plume Detection](https://arxiv.org/abs/2505.21806)
*Brian D. Bue,Jake H. Lee,Andrew K. Thorpe,Philip G. Brodrick,Daniel Cusworth,Alana Ayasse,Vassiliki Mancoridis,Anagha Satish,Shujun Xiong,Riley Duren*

Main category: cs.LG

TL;DR: 该论文探讨了如何通过深度学习技术实现温室气体排放的自动化检测，解决了数据质量控制、时空偏差和模型目标对齐等关键问题，并提出了一套最佳实践和验证标准。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习技术取得了进展，但温室气体排放的自动化检测系统在实际操作中仍面临挑战。随着数据可用性的增加，自动化监测在自然和人为排放监控中的重要性日益凸显。

Method: 论文采用卷积神经网络（CNN），通过多任务模型同时学习实例检测和像素级分割，利用多源空中和空间仪器数据进行严格实验。

Result: 实验表明，当解决了数据质量控制、时空偏差和模型目标对齐等问题时，CNN能够达到操作性的检测性能。模型在不同排放源类型和地区的检测能力得到了验证，并确定了操作性部署的阈值。

Conclusion: 论文不仅提供了可复现的分析就绪数据、模型和源代码，还定义了一套最佳实践和验证标准，为未来在该领域的研究提供了参考和便利。

Abstract: Operational deployment of a fully automated greenhouse gas (GHG) plume
detection system remains an elusive goal for imaging spectroscopy missions,
despite recent advances in deep learning approaches. With the dramatic increase
in data availability, however, automation continues to increase in importance
for natural and anthropogenic emissions monitoring. This work reviews and
addresses several key obstacles in the field: data and label quality control,
prevention of spatiotemporal biases, and correctly aligned modeling objectives.
We demonstrate through rigorous experiments using multicampaign data from
airborne and spaceborne instruments that convolutional neural networks (CNNs)
are able to achieve operational detection performance when these obstacles are
alleviated. We demonstrate that a multitask model that learns both instance
detection and pixelwise segmentation simultaneously can successfully lead
towards an operational pathway. We evaluate the model's plume detectability
across emission source types and regions, identifying thresholds for
operational deployment. Finally, we provide analysis-ready data, models, and
source code for reproducibility, and work to define a set of best practices and
validation standards to facilitate future contributions to the field.

</details>


### [168] [TabReason: A Reinforcement Learning-Enhanced Reasoning LLM for Explainable Tabular Data Prediction](https://arxiv.org/abs/2505.21807)
*Tommy Xu,Zhitian Zhang,Xiangyu Sun,Lauren Kelly Zung,Hossein Hajimirsadeghi,Greg Mori*

Main category: cs.LG

TL;DR: 本文提出了一种基于强化学习训练的大型语言模型（LLMs）方法，用于提升表格数据的预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前梯度提升机和某些深度模型在表格数据预测上表现优异但缺乏可解释性，而大型语言模型虽能生成人类可理解的推理但在表格数据预测上表现不佳。

Method: 采用强化学习训练基于推理的大型语言模型，并设计自定义奖励函数，以确保高预测准确性和人类可理解的预测原因。

Result: 实验结果表明，该方法在金融基准数据集上表现优异，超越了大多数现有的大型语言模型。

Conclusion: 通过结合强化学习和大型语言模型，本文方法在表格数据预测上实现了更高的准确性和可解释性。

Abstract: Predictive modeling on tabular data is the cornerstone of many real-world
applications. Although gradient boosting machines and some recent deep models
achieve strong performance on tabular data, they often lack interpretability.
On the other hand, large language models (LLMs) have demonstrated powerful
capabilities to generate human-like reasoning and explanations, but remain
under-performed for tabular data prediction. In this paper, we propose a new
approach that leverages reasoning-based LLMs, trained using reinforcement
learning, to perform more accurate and explainable predictions on tabular data.
Our method introduces custom reward functions that guide the model not only
toward high prediction accuracy but also toward human-understandable reasons
for its predictions. Experimental results show that our model achieves
promising performance on financial benchmark datasets, outperforming most
existing LLMs.

</details>


### [169] [Optimizing Data Augmentation through Bayesian Model Selection](https://arxiv.org/abs/2505.21813)
*Madi Matymov,Ba-Hien Tran,Michael Kampffmeyer,Markus Heinonen,Maurizio Filippone*

Main category: cs.LG

TL;DR: 该论文提出了一种基于贝叶斯原理的数据增强参数优化框架，通过变分推断实现参数联合优化，提升了模型鲁棒性和校准性。


<details>
  <summary>Details</summary>
Motivation: 传统数据增强参数选择依赖试错或耗时的验证集优化，缺乏理论指导。本文旨在通过概率视角将增强参数转化为可优化的模型超参数。

Method: 提出概率化数据增强框架，将参数优化转化为边际似然的贝叶斯模型选择问题，推导出可处理的ELBO下界实现参数联合优化。

Result: 实验证明该方法在计算机视觉任务中优于固定增强策略，提高了模型校准性和鲁棒性，并提供了理论保证。

Conclusion: 该工作为数据增强参数优化建立了严格的贝叶斯理论基础，为鲁棒机器学习提供了新范式。

Abstract: Data Augmentation (DA) has become an essential tool to improve robustness and
generalization of modern machine learning. However, when deciding on DA
strategies it is critical to choose parameters carefully, and this can be a
daunting task which is traditionally left to trial-and-error or expensive
optimization based on validation performance. In this paper, we counter these
limitations by proposing a novel framework for optimizing DA. In particular, we
take a probabilistic view of DA, which leads to the interpretation of
augmentation parameters as model (hyper)-parameters, and the optimization of
the marginal likelihood with respect to these parameters as a Bayesian model
selection problem. Due to its intractability, we derive a tractable Evidence
Lower BOund (ELBO), which allows us to optimize augmentation parameters jointly
with model parameters. We provide extensive theoretical results on variational
approximation quality, generalization guarantees, invariance properties, and
connections to empirical Bayes. Through experiments on computer vision tasks,
we show that our approach improves calibration and yields robust performance
over fixed or no augmentation. Our work provides a rigorous foundation for
optimizing DA through Bayesian principles with significant potential for robust
machine learning.

</details>


### [170] [Unsupervised Latent Pattern Analysis for Estimating Type 2 Diabetes Risk in Undiagnosed Populations](https://arxiv.org/abs/2505.21824)
*Praveen Kumar,Vincent T. Metzger,Scott A. Malec*

Main category: cs.LG

TL;DR: 该论文提出了一种新型无监督学习框架，结合非负矩阵分解与统计技术，用于识别2型糖尿病高风险人群，以解决传统监督学习方法因缺乏阴性病例数据而受限的问题。


<details>
  <summary>Details</summary>
Motivation: 2型糖尿病(T2DM)全球流行率高，造成重大健康和经济负担。早期风险识别至关重要，但现有机器学习方法多依赖监督学习，受限于阴性病例数据不足。

Method: 采用无监督框架，整合非负矩阵分解(NMF)与统计技术，通过分析已确诊患者的共病模式和多重用药特征，构建风险预测模型。

Result: 该方法能从共病和用药数据中识别潜在风险模式，为未确诊个体提供可解释的风险评估，具有可扩展性。

Conclusion: 该无监督框架为医疗 providers 提供了及时干预的工具，有望改善患者预后并减轻T2DM的未来负担。

Abstract: The global prevalence of diabetes, particularly type 2 diabetes mellitus
(T2DM), is rapidly increasing, posing significant health and economic
challenges. T2DM not only disrupts blood glucose regulation but also damages
vital organs such as the heart, kidneys, eyes, nerves, and blood vessels,
leading to substantial morbidity and mortality. In the US alone, the economic
burden of diagnosed diabetes exceeded \$400 billion in 2022. Early detection of
individuals at risk is critical to mitigating these impacts. While machine
learning approaches for T2DM prediction are increasingly adopted, many rely on
supervised learning, which is often limited by the lack of confirmed negative
cases. To address this limitation, we propose a novel unsupervised framework
that integrates Non-negative Matrix Factorization (NMF) with statistical
techniques to identify individuals at risk of developing T2DM. Our method
identifies latent patterns of multimorbidity and polypharmacy among diagnosed
T2DM patients and applies these patterns to estimate the T2DM risk in
undiagnosed individuals. By leveraging data-driven insights from comorbidity
and medication usage, our approach provides an interpretable and scalable
solution that can assist healthcare providers in implementing timely
interventions, ultimately improving patient outcomes and potentially reducing
the future health and economic burden of T2DM.

</details>


### [171] [Let Me Think! A Long Chain-of-Thought Can Be Worth Exponentially Many Short Ones](https://arxiv.org/abs/2505.21825)
*Parsa Mirtaheri,Ezra Edelman,Samy Jelassi,Eran Malach,Enric Boix-Adsera*

Main category: cs.LG

TL;DR: 本文探讨了在大型语言模型推理中，顺序计算（如更长的思维链）与并行计算（如多数投票）的优劣，发现某些图连通性问题中顺序计算具有指数优势。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解推理时计算的最优分配方式，尤其是在顺序扩展和并行扩展之间的选择，以提升大型语言模型的推理能力。

Method: 方法包括理论分析和实验验证，通过图连通性问题在不同图分布中的表现，比较不同思维链策略的效果。

Result: 结果表明，在某些图连通性问题的设定下，顺序计算相比并行计算展现出指数级的优势。

Conclusion: 结论指出顺序计算在特定推理场景中具有显著优势，为大型语言模型的推理优化提供了新的方向。

Abstract: Inference-time computation has emerged as a promising scaling axis for
improving large language model reasoning. However, despite yielding impressive
performance, the optimal allocation of inference-time computation remains
poorly understood. A central question is whether to prioritize sequential
scaling (e.g., longer chains of thought) or parallel scaling (e.g., majority
voting across multiple short chains of thought). In this work, we seek to
illuminate the landscape of test-time scaling by demonstrating the existence of
reasoning settings where sequential scaling offers an exponential advantage
over parallel scaling. These settings are based on graph connectivity problems
in challenging distributions of graphs. We validate our theoretical findings
with comprehensive experiments across a range of language models, including
models trained from scratch for graph connectivity with different chain of
thought strategies as well as large reasoning models.

</details>


### [172] [In Search of Adam's Secret Sauce](https://arxiv.org/abs/2505.21829)
*Antonio Orvieto,Robert Gower*

Main category: cs.LG

TL;DR: 本文通过大规模实验比较Adam及其简化变体，发现约束Adam动量参数相等能保持接近最优性能，并提供新的理论解释。


<details>
  <summary>Details</summary>
Motivation: 研究Adam优化器在训练基于Transformer的语言模型时表现出色的原因，探索其简化变体的性能差异。

Method: 通过训练超过1,300个语言模型，比较Adam与多种简化变体（如带符号梯度和带符号动量方法）的性能。

Result: 带符号动量方法比SGD快，但性能始终不如Adam。约束Adam动量参数相等的变体保持接近最优性能，并提供新的统计解释。

Conclusion: 约束Adam动量参数相等的简化方法不仅性能稳健，还为理解Adam的优越性提供了新的理论视角。

Abstract: Understanding the remarkable efficacy of Adam when training transformer-based
language models has become a central research topic within the optimization
community. To gain deeper insights, several simplifications of Adam have been
proposed, such as the signed gradient and signed momentum methods. In this
work, we conduct an extensive empirical study - training over 1,300 language
models across different data configurations and scales - comparing Adam to
several known simplified variants. We find that signed momentum methods are
faster than SGD, but consistently underperform relative to Adam, even after
careful tuning of momentum, clipping setting and learning rates. However, our
analysis reveals a compelling option that preserves near-optimal performance
while allowing for new insightful reformulations: constraining the Adam
momentum parameters to be equal. Beyond robust performance, this choice affords
new theoretical insights, highlights the "secret sauce" on top of signed
momentum, and grants a precise statistical interpretation: we show that Adam in
this setting implements a natural online algorithm for estimating the mean and
variance of gradients-one that arises from a mean-field Gaussian variational
inference perspective.

</details>


### [173] [TuneComp: Joint Fine-tuning and Compression for Large Foundation Models](https://arxiv.org/abs/2505.21835)
*Xiangyu Chen,Jing Liu,Ye Wang,Matthew Brand,Pu,Wang,Toshiaki Koike-Akino*

Main category: cs.LG

TL;DR: 该论文提出了一种联合微调与压缩的方法，直接在任务指导下构建更小的模型，避免了传统顺序方法中的性能损失和中间模型过大的问题。


<details>
  <summary>Details</summary>
Motivation: 传统模型压缩方法通常在微调后顺序应用，导致性能损失和生成不必要的中间大模型。论文旨在减少这一差距，直接在任务指导下构建更高效的压缩模型。

Method: 论文提出联合微调与压缩的方法，通过逐步蒸馏到剪枝后的低秩结构，实现模型的优化与压缩同步进行。

Result: 实验表明，联合微调与压缩方法显著优于其他顺序压缩方法，性能提升明显。

Conclusion: 联合微调与压缩是一种高效的方法，能够在不牺牲性能的情况下直接构建更小的模型，优于传统顺序压缩方法。

Abstract: To reduce model size during post-training, compression methods, including
knowledge distillation, low-rank approximation, and pruning, are often applied
after fine-tuning the model. However, sequential fine-tuning and compression
sacrifices performance, while creating a larger than necessary model as an
intermediate step. In this work, we aim to reduce this gap, by directly
constructing a smaller model while guided by the downstream task. We propose to
jointly fine-tune and compress the model by gradually distilling it to a pruned
low-rank structure. Experiments demonstrate that joint fine-tuning and
compression significantly outperforms other sequential compression methods.

</details>


### [174] [An Optimistic Algorithm for online CMDPS with Anytime Adversarial Constraints](https://arxiv.org/abs/2505.21841)
*Jiahui Zhu,Kihyun Yu,Dabeen Lee,Xin Liu,Honghao Wei*

Main category: cs.LG

TL;DR: 该论文提出了一种名为OMDPD的新算法，用于解决在线安全强化学习中的对抗性约束问题，实现了最优的遗憾和强约束违反性能。


<details>
  <summary>Details</summary>
Motivation: 在线安全强化学习在动态环境中具有重要应用，如自动驾驶和机器人技术。现有方法在随机约束下表现良好，但在对抗性环境中往往失效。

Method: 论文提出了Optimistic Mirror Descent Primal-Dual (OMDPD)算法，首次解决了具有任意时间对抗性约束的在线CMDP问题。

Result: OMDPD算法在不依赖Slater条件或已知安全策略的情况下，实现了最优遗憾O(sqrt(K))和强约束违反O(sqrt(K))。

Conclusion: 该研究为对抗性环境中的安全决策提供了实用的保证，展示了在奖励和转移估计准确的情况下可以进一步改进性能。

Abstract: Online safe reinforcement learning (RL) plays a key role in dynamic
environments, with applications in autonomous driving, robotics, and
cybersecurity. The objective is to learn optimal policies that maximize rewards
while satisfying safety constraints modeled by constrained Markov decision
processes (CMDPs). Existing methods achieve sublinear regret under stochastic
constraints but often fail in adversarial settings, where constraints are
unknown, time-varying, and potentially adversarially designed. In this paper,
we propose the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, the
first to address online CMDPs with anytime adversarial constraints. OMDPD
achieves optimal regret O(sqrt(K)) and strong constraint violation O(sqrt(K))
without relying on Slater's condition or the existence of a strictly known safe
policy. We further show that access to accurate estimates of rewards and
transitions can further improve these bounds. Our results offer practical
guarantees for safe decision-making in adversarial environments.

</details>


### [175] [A Provable Approach for End-to-End Safe Reinforcement Learning](https://arxiv.org/abs/2505.21852)
*Akifumi Wachi,Kohei Miyaguchi,Takumi Tanabe,Rei Sato,Youhei Akimoto*

Main category: cs.LG

TL;DR: 提出了一种名为PLS的方法，通过结合离线安全强化学习与安全策略部署，确保策略从学习到运行的整个生命周期安全性。


<details>
  <summary>Details</summary>
Motivation: 现有安全强化学习方法难以确保策略从学习到运行的整个生命周期安全性，因此需要一种新方法来解决这一挑战。

Method: PLS方法通过离线学习策略（使用回报条件监督学习），并在部署时谨慎优化目标回报参数（使用高斯过程），确保安全性。

Result: 理论分析表明PLS能高效找到近优目标回报并高概率保证安全；实验显示其在安全性和奖励性能上均优于基线方法。

Conclusion: PLS实现了在确保策略整个生命周期安全性的同时获得高奖励的长期目标。

Abstract: A longstanding goal in safe reinforcement learning (RL) is a method to ensure
the safety of a policy throughout the entire process, from learning to
operation. However, existing safe RL paradigms inherently struggle to achieve
this objective. We propose a method, called Provably Lifetime Safe RL (PLS),
that integrates offline safe RL with safe policy deployment to address this
challenge. Our proposed method learns a policy offline using return-conditioned
supervised learning and then deploys the resulting policy while cautiously
optimizing a limited set of parameters, known as target returns, using Gaussian
processes (GPs). Theoretically, we justify the use of GPs by analyzing the
mathematical relationship between target and actual returns. We then prove that
PLS finds near-optimal target returns while guaranteeing safety with high
probability. Empirically, we demonstrate that PLS outperforms baselines both in
safety and reward performance, thereby achieving the longstanding goal to
obtain high rewards while ensuring the safety of a policy throughout the
lifetime from learning to operation.

</details>


### [176] [Revisiting Bayesian Model Averaging in the Era of Foundation Models](https://arxiv.org/abs/2505.21857)
*Mijung Park*

Main category: cs.LG

TL;DR: 该论文重新审视了贝叶斯模型平均（BMA）方法，提出了一种基于预训练基础模型的集成学习方法，通过优化线性分类器和模型权重提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型的快速发展，如何有效集成多个预训练模型以提升分类任务的性能成为一个重要问题。传统BMA方法在大规模基础模型下计算成本高，因此需要更高效的集成策略。

Method: 论文提出了两种方法：1）使用可训练的线性分类器处理冻结的基础模型特征，通过模型后验分布选择最佳特征和分类器；2）提出了一种计算成本更低的优化模型平均（OMA）方法，直接优化集成权重以减少预测的不确定性。

Result: 提出的方法能够有效集成多个基础模型，提升图像和文本分类任务的性能，同时降低了计算成本。

Conclusion: 通过BMA和OMA方法，论文为集成预训练基础模型提供了一种高效且可扩展的解决方案，能够适应未来更强大的基础模型。

Abstract: We revisit the classical, full-fledged Bayesian model averaging (BMA)
paradigm to ensemble pre-trained and/or lightly-finetuned foundation models to
enhance the classification performance on image and text data. To make BMA
tractable under foundation models, we introduce trainable linear classifiers
that take frozen features from the pre-trained foundation models as inputs. The
model posteriors over the linear classifiers tell us which linear heads and
frozen features are better suited for a given dataset, resulting in a
principled model ensembling method. Furthermore, we propose a computationally
cheaper, optimizable model averaging scheme (OMA). In OMA, we directly optimize
the model ensemble weights, just like those weights based on model posterior
distributions in BMA, by reducing the amount of surprise (expected entropy of
the predictions) we get from predictions of ensembled models. With the rapid
development of foundation models, these approaches will enable the
incorporation of future, possibly significantly better foundation models to
enhance the performance of challenging classification tasks.

</details>


### [177] [Hybrid Batch Normalisation: Resolving the Dilemma of Batch Normalisation in Federated Learning](https://arxiv.org/abs/2505.21877)
*Hongyao Chen,Tianyang Xu,Xiaojun Wu,Josef Kittler*

Main category: cs.LG

TL;DR: 论文提出了一种名为混合批量归一化（HBN）的新方法，用于解决联邦学习中批量归一化（BN）因数据非独立同分布而性能下降的问题。HBN通过分离统计参数和可学习参数的更新，并引入可学习的混合分布因子，有效提升了联邦学习的性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习作为一种分布式学习范式，面临客户端数据非独立同分布（Non-IID）的挑战。传统的批量归一化（BN）方法在这种场景下性能下降，亟需一种新的归一化解决方案。

Method: 论文提出了混合批量归一化（HBN），通过分离统计参数（均值和方差）和可学习参数的更新，并引入可学习的混合分布因子，使每个计算节点能自适应地混合当前批次的统计参数和全局统计参数。

Result: HBN在多种联邦学习设置中表现出色，尤其是在小批量大小和异构数据场景下，显著提升了模型性能。

Conclusion: HBN作为一种强大的插件，能够有效提升联邦学习的性能，尤其在处理非独立同分布数据时表现出显著优势。

Abstract: Batch Normalisation (BN) is widely used in conventional deep neural network
training to harmonise the input-output distributions for each batch of data.
However, federated learning, a distributed learning paradigm, faces the
challenge of dealing with non-independent and identically distributed data
among the client nodes. Due to the lack of a coherent methodology for updating
BN statistical parameters, standard BN degrades the federated learning
performance. To this end, it is urgent to explore an alternative normalisation
solution for federated learning. In this work, we resolve the dilemma of the BN
layer in federated learning by developing a customised normalisation approach,
Hybrid Batch Normalisation (HBN). HBN separates the update of statistical
parameters (i.e. , means and variances used for evaluation) from that of
learnable parameters (i.e. , parameters that require gradient updates),
obtaining unbiased estimates of global statistical parameters in distributed
scenarios. In contrast with the existing solutions, we emphasise the supportive
power of global statistics for federated learning. The HBN layer introduces a
learnable hybrid distribution factor, allowing each computing node to
adaptively mix the statistical parameters of the current batch with the global
statistics. Our HBN can serve as a powerful plugin to advance federated
learning performance. It reflects promising merits across a wide range of
federated learning settings, especially for small batch sizes and heterogeneous
data.

</details>


### [178] [HydraNet: Momentum-Driven State Space Duality for Multi-Granularity Tennis Tournaments Analysis](https://arxiv.org/abs/2505.21882)
*Ruijie Li,Xiang Zhao,Qiao Ning,Shikai Guo*

Main category: cs.LG

TL;DR: 该研究提出了一种新的动量评分（MS）指标和HydraNet框架，用于量化并建模网球比赛中运动员在不同粒度（如分、局、盘、场）的动量变化，通过多维度数据分析和对抗学习机制提升动量建模的准确性。


<details>
  <summary>Details</summary>
Motivation: 网球比赛中的动量现象对比赛结果有决定性影响，但现有研究在建模和多粒度分析方面仍显不足。本研究旨在填补这一空白，提供更全面的动量量化与分析框架。

Method: 研究设计了HydraNet框架，结合状态空间对偶性（SSD）和滑动窗口机制捕捉显性与隐性动量，并引入Versus Learning和协作-对抗注意力机制（CAAM）增强对抗性与动态动量建模。

Result: 实验验证表明，HydraNet构建的MS指标能有效揭示动量对不同粒度比赛结果的影响，为动量建模和体育分析提供了新基础。

Conclusion: 该研究提出的MS指标和HydraNet框架为网球比赛中的动量分析提供了创新方法，具有实际应用价值，并为未来研究奠定了基础。

Abstract: In tennis tournaments, momentum, a critical yet elusive phenomenon, reflects
the dynamic shifts in performance of athletes that can decisively influence
match outcomes. Despite its significance, momentum in terms of effective
modeling and multi-granularity analysis across points, games, sets, and matches
in tennis tournaments remains underexplored. In this study, we define a novel
Momentum Score (MS) metric to quantify a player's momentum level in
multi-granularity tennis tournaments, and design HydraNet, a momentum-driven
state-space duality-based framework, to model MS by integrating thirty-two
heterogeneous dimensions of athletes performance in serve, return, psychology
and fatigue. HydraNet integrates a Hydra module, which builds upon a
state-space duality (SSD) framework, capturing explicit momentum with a
sliding-window mechanism and implicit momentum through cross-game state
propagation. It also introduces a novel Versus Learning method to better
enhance the adversarial nature of momentum between the two athletes at a macro
level, along with a Collaborative-Adversarial Attention Mechanism (CAAM) for
capturing and integrating intra-player and inter-player dynamic momentum at a
micro level. Additionally, we construct a million-level tennis cross-tournament
dataset spanning from 2012-2023 Wimbledon and 2013-2023 US Open, and validate
the multi-granularity modeling capability of HydraNet for the MS metric on this
dataset. Extensive experimental evaluations demonstrate that the MS metric
constructed by the HydraNet framework provides actionable insights into how
momentum impacts outcomes at different granularities, establishing a new
foundation for momentum modeling and sports analysis. To the best of our
knowledge. The source code and datasets are available at
https://github.com/ReyJerry/HydraNet.

</details>


### [179] [SDPO: Importance-Sampled Direct Preference Optimization for Stable Diffusion Training](https://arxiv.org/abs/2505.21893)
*Xiaomeng Yang,Zhiyu Tan,Junyan Wang,Zhijian Zhou,Hao Li*

Main category: cs.LG

TL;DR: 该论文针对扩散模型中的偏好学习问题，提出了两种改进方法：DPO-C&M和SDPO，以解决时间步依赖不稳定性和策略偏差问题，并在实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型中的偏好学习方法（如Diffusion-DPO）存在时间步依赖不稳定性和策略偏差问题，影响了模型的训练效果和性能。

Method: 论文首先分析了反向扩散轨迹，发现不稳定性主要出现在早期时间步。随后提出了DPO-C&M方法，通过裁剪和掩码无信息时间步来提高稳定性；进一步提出了SDPO方法，引入重要性采样以完全纠正策略偏差并强调扩散过程中的信息更新。

Result: 实验在CogVideoX-2B、CogVideoX-5B和Wan2.1-1.3B上进行，结果显示DPO-C&M和SDPO均优于标准Diffusion-DPO，其中SDPO在VBench评分、人类偏好对齐和训练鲁棒性方面表现最佳。

Conclusion: 论文强调了时间步感知和分布校正优化在扩散模型偏好学习中的重要性，提出的方法显著提升了模型性能。

Abstract: Preference learning has become a central technique for aligning generative
models with human expectations. Recently, it has been extended to diffusion
models through methods like Direct Preference Optimization (DPO). However,
existing approaches such as Diffusion-DPO suffer from two key challenges:
timestep-dependent instability, caused by a mismatch between the reverse and
forward diffusion processes and by high gradient variance in early noisy
timesteps, and off-policy bias arising from the mismatch between optimization
and data collection policies. We begin by analyzing the reverse diffusion
trajectory and observe that instability primarily occurs at early timesteps
with low importance weights. To address these issues, we first propose
DPO-C\&M, a practical strategy that improves stability by clipping and masking
uninformative timesteps while partially mitigating off-policy bias. Building on
this, we introduce SDPO (Importance-Sampled Direct Preference Optimization), a
principled framework that incorporates importance sampling into the objective
to fully correct for off-policy bias and emphasize informative updates during
the diffusion process. Experiments on CogVideoX-2B, CogVideoX-5B, and
Wan2.1-1.3B demonstrate that both methods outperform standard Diffusion-DPO,
with SDPO achieving superior VBench scores, human preference alignment, and
training robustness. These results highlight the importance of timestep-aware,
distribution-corrected optimization in diffusion-based preference learning.

</details>


### [180] [Compressing Sine-Activated Low-Rank Adapters through Post-Training Quantization](https://arxiv.org/abs/2505.21895)
*Cameron Gordon,Yiping Ji,Hemanth Saratchandran,Paul Albert,Simon Lucey*

Main category: cs.LG

TL;DR: 本文探讨了在量化后训练中应用正弦激活技术以增强低秩适配器（LoRA）的表达能力，实现了高效压缩且性能损失可忽略。


<details>
  <summary>Details</summary>
Motivation: 低秩适配器（LoRA）虽能高效微调，但其低秩限制导致性能不如全秩微调。Ji等人（2025）通过正弦变换提升稳定秩，但该技术是否适用于量化后训练尚不明确。

Method: 将正弦变换框架扩展至量化LoRA适配器，理论分析量化适配器的稳定秩与全精度版本的关联，验证正弦非线性在量化后的表达能力。

Result: 实验表明，正弦非线性在量化后仍能保持表达能力，显著节省内存，同时在语言、视觉和文本生成任务中保持竞争力。

Conclusion: 正弦激活技术可有效提升量化LoRA适配器的性能，为高效模型压缩提供了新思路。

Abstract: Low-Rank Adaptation (LoRA) has become a standard approach for
parameter-efficient fine-tuning, offering substantial reductions in trainable
parameters by modeling updates as the product of two low-rank matrices. While
effective, the low-rank constraint inherently limits representational capacity,
often resulting in reduced performance compared to full-rank fine-tuning.
Recent work by Ji et al. (2025) has addressed this limitation by applying a
fixed-frequency sinusoidal transformation to low-rank adapters, increasing
their stable rank without introducing additional parameters. This raises a
crucial question: can the same sine-activated technique be successfully applied
within the context of Post-Training Quantization to retain benefits even after
model compression? In this paper, we investigate this question by extending the
sinusoidal transformation framework to quantized LoRA adapters. We develop a
theoretical analysis showing that the stable rank of a quantized adapter is
tightly linked to that of its full-precision counterpart, motivating the use of
such rank-enhancing functions even under quantization. Our results demonstrate
that the expressivity gains from a sinusoidal non-linearity persist after
quantization, yielding highly compressed adapters with negligible loss in
performance. We validate our approach across a range of fine-tuning tasks for
language, vision and text-to-image generation achieving significant memory
savings while maintaining competitive accuracy.

</details>


### [181] [Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding](https://arxiv.org/abs/2505.21908)
*Hanyin Wang,Zhenbang Wu,Gururaj Kolar,Hariprasad Korsapati,Brian Bartlett,Bryan Hull,Jimeng Sun*

Main category: cs.LG

TL;DR: 论文提出了DRG-Sapphire模型，利用强化学习自动从临床记录中生成DRG编码，解决了传统方法劳动密集和大语言模型在分布外任务上的挑战。


<details>
  <summary>Details</summary>
Motivation: DRG编码对医院报销和运营至关重要，但传统方法劳动密集，且大语言模型因缺乏相关训练数据而表现不佳。

Method: 基于Qwen2.5-7B模型，采用大规模强化学习（RL）和基于规则的奖励机制，结合领域特定优化策略。

Result: 在MIMIC-IV基准测试中达到最先进准确率，并生成医生验证的DRG分配解释，显著提升可解释性。

Conclusion: 研究表明，强化学习在知识密集型分布外任务中的效果受限于基础模型的知识，扩展监督微调比单独扩展强化学习更有效。

Abstract: Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement
and operations but require labor-intensive assignment. Large Language Models
(LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of
the task: pretraining corpora rarely contain private clinical or billing data.
We introduce DRG-Sapphire, which uses large-scale reinforcement learning (RL)
for automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained
with Group Relative Policy Optimization (GRPO) using rule-based rewards,
DRG-Sapphire introduces a series of RL enhancements to address domain-specific
challenges not seen in previous mathematical tasks. Our model achieves
state-of-the-art accuracy on the MIMIC-IV benchmark and generates
physician-validated reasoning for DRG assignments, significantly enhancing
explainability. Our study further sheds light on broader challenges of applying
RL to knowledge-intensive, OOD tasks. We observe that RL performance scales
approximately linearly with the logarithm of the number of supervised
fine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally
constrained by the domain knowledge encoded in the base model. For OOD tasks
like DRG coding, strong RL performance requires sufficient knowledge infusion
prior to RL. Consequently, scaling SFT may be more effective and
computationally efficient than scaling RL alone for such tasks.

</details>


### [182] [Taming Transformer Without Using Learning Rate Warmup](https://arxiv.org/abs/2505.21910)
*Xianbiao Qi,Yelin He,Jiaquan Ye,Chun-Guang Li,Bojia Zi,Xili Dai,Qin Zou,Rong Xiao*

Main category: cs.LG

TL;DR: 论文分析了Transformer训练过程中的模型崩溃现象，提出了基于Weyl不等式的优化策略，避免熵崩溃，无需学习率预热即可稳定训练。


<details>
  <summary>Details</summary>
Motivation: 大规模Transformer训练中，不使用学习率预热等技术时易出现模型崩溃现象，研究其理论原因并提出解决方案。

Method: 提出一种基于Weyl不等式的优化策略，通过平滑权重更新，防止谱能量集中，避免熵崩溃。

Result: 在ViT、Swin-Transformer和GPT上的实验表明，该方法能有效稳定训练，无需学习率预热。

Conclusion: 该优化策略解决了Transformer训练中的熵崩溃问题，为大规模训练提供了稳定方案。

Abstract: Scaling Transformer to a large scale without using some technical tricks such
as learning rate warump and using an obviously lower learning rate is an
extremely challenging task, and is increasingly gaining more attention. In this
paper, we provide a theoretical analysis for the process of training
Transformer and reveal the rationale behind the model crash phenomenon in the
training process, termed \textit{spectral energy concentration} of
${\bW_q}^{\top} \bW_k$, which is the reason for a malignant entropy collapse,
where ${\bW_q}$ and $\bW_k$ are the projection matrices for the query and the
key in Transformer, respectively. To remedy this problem, motivated by
\textit{Weyl's Inequality}, we present a novel optimization strategy, \ie,
making the weight updating in successive steps smooth -- if the ratio
$\frac{\sigma_{1}(\nabla \bW_t)}{\sigma_{1}(\bW_{t-1})}$ is larger than a
threshold, we will automatically bound the learning rate to a weighted multiple
of $\frac{\sigma_{1}(\bW_{t-1})}{\sigma_{1}(\nabla \bW_t)}$, where $\nabla
\bW_t$ is the updating quantity in step $t$. Such an optimization strategy can
prevent spectral energy concentration to only a few directions, and thus can
avoid malignant entropy collapse which will trigger the model crash. We conduct
extensive experiments using ViT, Swin-Transformer and GPT, showing that our
optimization strategy can effectively and stably train these Transformers
without using learning rate warmup.

</details>


### [183] [Self-supervised Learning Method Using Transformer for Multi-dimensional Sensor Data Processing](https://arxiv.org/abs/2505.21918)
*Haruki Kai,Tsuyoshi Okita*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer架构的深度学习算法，用于人体活动识别，通过改进模型在五个数据集上实现了10%-15%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 旨在利用预训练语言模型提升人体活动识别任务的性能，克服传统Transformer在该任务上的局限性。

Method: 构建了改进的n维数值处理Transformer，包含线性层嵌入、分箱预处理和输出层线性变换三个关键特征。

Result: 在五个不同数据集上验证，相比原始Transformer模型，准确率提高了10%-15%。

Conclusion: 所提出的增强型Transformer模型显著提升了人体活动识别的性能，证明了其有效性。

Abstract: We developed a deep learning algorithm for human activity recognition using
sensor signals as input. In this study, we built a pretrained language model
based on the Transformer architecture, which is widely used in natural language
processing. By leveraging this pretrained model, we aimed to improve
performance on the downstream task of human activity recognition. While this
task can be addressed using a vanilla Transformer, we propose an enhanced
n-dimensional numerical processing Transformer that incorporates three key
features: embedding n-dimensional numerical data through a linear layer,
binning-based pre-processing, and a linear transformation in the output layer.
We evaluated the effectiveness of our proposed model across five different
datasets. Compared to the vanilla Transformer, our model demonstrated 10%-15%
improvements in accuracy.

</details>


### [184] [FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design](https://arxiv.org/abs/2505.21923)
*Asal Mehradfar,Xuzhe Zhao,Yilun Huang,Emir Ceyani,Yankai Yang,Shihao Han,Hamidreza Aghasi,Salman Avestimehr*

Main category: cs.LG

TL;DR: FALCON是一个基于机器学习的自动化模拟电路设计框架，通过拓扑选择和布局优化实现高性能电路设计。


<details>
  <summary>Details</summary>
Motivation: 模拟电路设计是一个复杂且多阶段的过程，传统方法依赖人工经验，效率低且难以满足高性能需求。FALCON旨在通过机器学习实现自动化设计，提高效率和准确性。

Method: FALCON结合性能驱动分类器和图神经网络，通过梯度优化和布局成本分析，实现拓扑选择和参数推断。

Result: 在100万模拟电路数据集上，FALCON拓扑推断准确率>99%，性能预测误差<10%，设计速度<1秒/实例。

Conclusion: FALCON为端到端模拟电路设计自动化提供了实用且可扩展的基础模型。

Abstract: Designing analog circuits from performance specifications is a complex,
multi-stage process encompassing topology selection, parameter inference, and
layout feasibility. We introduce FALCON, a unified machine learning framework
that enables fully automated, specification-driven analog circuit synthesis
through topology selection and layout-constrained optimization. Given a target
performance, FALCON first selects an appropriate circuit topology using a
performance-driven classifier guided by human design heuristics. Next, it
employs a custom, edge-centric graph neural network trained to map circuit
topology and parameters to performance, enabling gradient-based parameter
inference through the learned forward model. This inference is guided by a
differentiable layout cost, derived from analytical equations capturing
parasitic and frequency-dependent effects, and constrained by design rules. We
train and evaluate FALCON on a large-scale custom dataset of 1M analog mm-wave
circuits, generated and simulated using Cadence Spectre across 20
expert-designed topologies. Through this evaluation, FALCON demonstrates >99\%
accuracy in topology inference, <10\% relative error in performance prediction,
and efficient layout-aware design that completes in under 1 second per
instance. Together, these results position FALCON as a practical and extensible
foundation model for end-to-end analog circuit design automation.

</details>


### [185] [Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets](https://arxiv.org/abs/2505.21930)
*Dongyue Li,Ziniu Zhang,Lu Wang,Hongyang R. Zhang*

Main category: cs.LG

TL;DR: 本文提出一种集成方法，通过将多个数据集分组并训练小型适配器，显著提升多任务语言模型微调的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如QLoRA）在单数据集微调中高效，但在多任务场景下缺乏高效适配方案。需要解决多数据集微调时的效率与性能平衡问题。

Method: 提出分组集成策略：将n个数据集划分为m个组（m<<n），每组训练一个适配器，最后加权组合。利用低秩适配的一阶近似特性快速评估组合性能。

Result: 在340亿参数模型上实现误差<5%的性能预估，速度比基线快105倍。Llama/GPT模型在10个文本分类任务中平均准确率提升10%（仅增加9%计算量）。

Conclusion: 该方法通过轻量级集成适配器，以微小计算代价显著提升多任务微调性能，为大规模语言模型高效适配提供新思路。

Abstract: This paper develops an ensemble method for fine-tuning a language model to
multiple datasets. Existing methods, such as quantized LoRA (QLoRA), are
efficient when adapting to a single dataset. When training on multiple datasets
of different tasks, a common setup in practice, it remains unclear how to
design an efficient adaptation for fine-tuning language models. We propose to
use an ensemble of multiple smaller adapters instead of a single adapter per
task. We design an efficient algorithm that partitions $n$ datasets into $m$
groups, where $m$ is typically much smaller than $n$ in practice, and train one
adapter for each group before taking a weighted combination to form the
ensemble. The algorithm leverages a first-order approximation property of
low-rank adaptation to quickly obtain the fine-tuning performances of dataset
combinations since methods like LoRA stay close to the base model. Hence, we
use the gradients of the base model to estimate its behavior during
fine-tuning. Empirically, this approximation holds with less than $1\%$ error
on models with up to $34$ billion parameters, leading to an estimation of true
fine-tuning performances under $5\%$ error while speeding up computation
compared to base fine-tuning by $105$ times. When applied to fine-tune Llama
and GPT models on ten text classification tasks, our approach provides up to
$10\%$ higher average test accuracy over QLoRA, with only $9\%$ more FLOPs. On
a Llama model with $34$ billion parameters, an ensemble of QLoRA increases test
accuracy by $3\%$ compared to QLoRA, with only $8\%$ more FLOPs.

</details>


### [186] [Practical Adversarial Attacks on Stochastic Bandits via Fake Data Injection](https://arxiv.org/abs/2505.21938)
*Qirun Zeng,Eric He,Richard Hoffmann,Xuchuang Wang,Jinhang Zuo*

Main category: cs.LG

TL;DR: 论文提出了一种更现实的对抗攻击模型——虚假数据注入，针对随机多臂老虎机算法，在有限扰动下有效误导学习器。


<details>
  <summary>Details</summary>
Motivation: 传统对抗攻击假设不切实际（如每轮奖励可操纵、扰动无界），限制了其在现实系统中的适用性。本文旨在设计更贴近实际攻击者能力的威胁模型。

Method: 采用虚假数据注入威胁模型，设计满足幅度约束（奖励值限制）和时间约束（注入时机/频率）的高效攻击策略，针对UCB和Thompson Sampling算法。

Result: 理论证明攻击能以次线性成本使学习器持续选择目标臂；合成与真实数据实验验证了策略的有效性。

Conclusion: 研究表明，广泛使用的随机老虎机算法在实际对抗场景下存在显著脆弱性，虚假数据注入是一种高效且隐蔽的攻击方式。

Abstract: Adversarial attacks on stochastic bandits have traditionally relied on some
unrealistic assumptions, such as per-round reward manipulation and unbounded
perturbations, limiting their relevance to real-world systems. We propose a
more practical threat model, Fake Data Injection, which reflects realistic
adversarial constraints: the attacker can inject only a limited number of
bounded fake feedback samples into the learner's history, simulating legitimate
interactions. We design efficient attack strategies under this model,
explicitly addressing both magnitude constraints (on reward values) and
temporal constraints (on when and how often data can be injected). Our
theoretical analysis shows that these attacks can mislead both Upper Confidence
Bound (UCB) and Thompson Sampling algorithms into selecting a target arm in
nearly all rounds while incurring only sublinear attack cost. Experiments on
synthetic and real-world datasets validate the effectiveness of our strategies,
revealing significant vulnerabilities in widely used stochastic bandit
algorithms under practical adversarial scenarios.

</details>


### [187] [Continual Learning Beyond Experience Rehearsal and Full Model Surrogates](https://arxiv.org/abs/2505.21942)
*Prashant Bhat,Laurens Niesten,Elahe Arani,Bahram Zonooz*

Main category: cs.LG

TL;DR: SPARC提出了一种无需经验回放和全模型替代的持续学习方法，通过任务特定工作记忆和任务无关语义记忆结合，实现了参数高效性和卓越性能。


<details>
  <summary>Details</summary>
Motivation: 持续学习（CL）中，深度神经网络学习新任务时会部分或完全遗忘旧知识。现有方法依赖经验回放或全模型替代，导致内存和计算开销大，限制了实际应用。

Method: SPARC结合任务特定工作记忆和任务无关语义记忆进行跨任务知识整合，分类层权重重归一化缓解任务特定偏差，无需经验回放和全模型替代。

Result: SPARC仅需全模型替代6%的参数，在Seq-TinyImageNet上表现优异，与基于经验回放的方法在多个CL基准上性能相当。

Conclusion: SPARC是一种在严格效率约束下实用且可扩展的持续学习解决方案，兼具轻量设计和高效性能。

Abstract: Continual learning (CL) has remained a significant challenge for deep neural
networks as learning new tasks erases previously acquired knowledge, either
partially or completely. Existing solutions often rely on experience rehearsal
or full model surrogates to mitigate CF. While effective, these approaches
introduce substantial memory and computational overhead, limiting their
scalability and applicability in real-world scenarios. To address this, we
propose SPARC, a scalable CL approach that eliminates the need for experience
rehearsal and full-model surrogates. By effectively combining task-specific
working memories and task-agnostic semantic memory for cross-task knowledge
consolidation, SPARC results in a remarkable parameter efficiency, using only
6% of the parameters required by full-model surrogates. Despite its lightweight
design, SPARC achieves superior performance on Seq-TinyImageNet and matches
rehearsal-based methods on various CL benchmarks. Additionally, weight
re-normalization in the classification layer mitigates task-specific biases,
establishing SPARC as a practical and scalable solution for CL under stringent
efficiency constraints.

</details>


### [188] [Stochastic Primal-Dual Double Block-Coordinate for Two-way Partial AUC Maximization](https://arxiv.org/abs/2505.21944)
*Linli Zhou,Bokun Wang,My T. Thai,Tianbao Yang*

Main category: cs.LG

TL;DR: 本文提出两种创新的随机原始-对偶双块坐标算法，用于优化双向部分AUC（TPAUC），在凸和非凸设置下均表现出色，理论分析和实验结果均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 双向部分AUC（TPAUC）是处理不平衡数据分类的重要指标，但现有随机优化方法要么局限于近似损失函数，要么计算复杂度高，亟需更高效的解决方案。

Method: 采用随机块坐标更新策略，设计两种原始-对偶双块坐标算法，支持凸和非凸场景的TPAUC最大化。

Result: 理论分析证明算法具有更优收敛速度，实验结果表明其在多个基准数据集上收敛更快、泛化能力更强。

Conclusion: 该研究推动了TPAUC优化领域的发展，为实际机器学习应用提供了高效工具。

Abstract: Two-way partial AUC (TPAUC) is a critical performance metric for binary
classification with imbalanced data, as it focuses on specific ranges of the
true positive rate (TPR) and false positive rate (FPR). However, stochastic
algorithms for TPAUC optimization remain under-explored, with existing methods
either limited to approximated TPAUC loss functions or burdened by sub-optimal
complexities. To overcome these limitations, we introduce two innovative
stochastic primal-dual double block-coordinate algorithms for TPAUC
maximization. These algorithms utilize stochastic block-coordinate updates for
both the primal and dual variables, catering to both convex and non-convex
settings. We provide theoretical convergence rate analyses, demonstrating
significant improvements over prior approaches. Our experimental results, based
on multiple benchmark datasets, validate the superior performance of our
algorithms, showcasing faster convergence and better generalization. This work
advances the state of the art in TPAUC optimization and offers practical tools
for real-world machine learning applications.

</details>


### [189] [EnsemW2S: Enhancing Weak-to-Strong Generalization with Large Language Model Ensembles](https://arxiv.org/abs/2505.21959)
*Aakriti Agrawal,Mucong Ding,Zora Che,Chenghao Deng,Anirudh Satheesh,Bang An,Bayan Bruss,John Langford,Furong Huang*

Main category: cs.LG

TL;DR: 论文提出EnsemW2S方法，通过集成弱专家模型提升对强学生模型的监督能力，在分布内外数据上均取得显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型性能接近或超越人类水平，亟需开发能用人类水平数据监督增强这些强大模型的方法，解决弱到强（W2S）的泛化挑战。

Method: 采用令牌级集成策略EnsemW2S，迭代结合多个弱专家模型，持续优化其监督强学生模型的能力。

Result: 实验显示，在分布内数据上专家和学生模型分别提升4%和3.2%，分布外数据上最高提升6%和2.28%。

Conclusion: EnsemW2S有效推进了弱到强泛化，为利用小模型监督更强模型提供了可行方案。

Abstract: With Large Language Models (LLMs) rapidly approaching and potentially
surpassing human-level performance, it has become imperative to develop
approaches capable of effectively supervising and enhancing these powerful
models using smaller, human-level models exposed to only human-level data. We
address this critical weak-to-strong (W2S) generalization challenge by
proposing a novel method aimed at improving weak experts, by training on the
same limited human-level data, enabling them to generalize to complex,
super-human-level tasks. Our approach, called \textbf{EnsemW2S}, employs a
token-level ensemble strategy that iteratively combines multiple weak experts,
systematically addressing the shortcomings identified in preceding iterations.
By continuously refining these weak models, we significantly enhance their
collective ability to supervise stronger student models. We extensively
evaluate the generalization performance of both the ensemble of weak experts
and the subsequent strong student model across in-distribution (ID) and
out-of-distribution (OOD) datasets. For OOD, we specifically introduce question
difficulty as an additional dimension for defining distributional shifts. Our
empirical results demonstrate notable improvements, achieving 4\%, and 3.2\%
improvements on ID datasets and, upto 6\% and 2.28\% on OOD datasets for
experts and student models respectively, underscoring the effectiveness of our
proposed method in advancing W2S generalization.

</details>


### [190] [Judging LLMs on a Simplex](https://arxiv.org/abs/2505.21972)
*Patrick Vossler,Fan Xia,Yifan Mai,Jean Feng*

Main category: cs.LG

TL;DR: 论文提出用几何框架分析LLM作为评估者的理论特性，发现二元评分下排名可识别，而多级评分则不可识别，并采用贝叶斯推理整合不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法常使用LLM自身作为评估者，但其理论特性尚不明确，需研究在何种条件下排名可识别及如何处理不确定性。

Method: 通过概率单纯形几何框架分析可识别性，并引入贝叶斯推理整合认知不确定性和随机不确定性。

Result: 理论证明二元评分下排名可识别，多级评分则不可识别；实证显示贝叶斯方法能提高排名准确性和覆盖率。

Conclusion: 使用LLM作为评估者时需采用整体性不确定性量化方法，贝叶斯推理能有效提升评估可靠性。

Abstract: Automated evaluation of free-form outputs from large language models (LLMs)
is challenging because many distinct answers can be equally valid. A common
practice is to use LLMs themselves as judges, but the theoretical properties of
this approach are not yet well understood. We show that a geometric framework
that represents both judges and candidates as points on a probability simplex
can provide helpful insight on what is or is not identifiable using LLM judges.
Our theoretical analysis uncovers a "phase transition" in ranking
identifiability: for binary scoring systems, true rankings are identifiable
even with weak judges under mild assumptions, while rankings become
non-identifiable for three or more scoring levels even with infinite data,
absent additional prior knowledge. This non-identifiability highlights how
uncertainty in rankings stems from not only aleatoric uncertainty (i.e.,
inherent stochasticity in the data) but also epistemic uncertainty regarding
which assumptions hold, an aspect that has received limited attention until
now. To integrate both types of uncertainty, we use Bayesian inference to
encode assumptions as priors and conduct sensitivity analysis of ranking
estimates and credible intervals. Empirical evaluations across multiple
benchmarks demonstrate that Bayesian inference yields more accurate rankings
and substantially improves coverage rates. These results underscore the
importance of taking a more holistic approach to uncertainty quantification
when using LLMs as judges.

</details>


### [191] [BOFormer: Learning to Solve Multi-Objective Bayesian Optimization via Non-Markovian RL](https://arxiv.org/abs/2505.21974)
*Yu-Heng Hung,Kai-Jie Lin,Yu-Heng Lin,Chien-YiWang,Cheng Sun,Ping-Chun Hsieh*

Main category: cs.LG

TL;DR: 论文提出BOFormer，一种基于Transformer的多目标贝叶斯优化框架，解决了传统方法中的超体积可识别性问题，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 多目标贝叶斯优化（MOBO）中，基于学习的采集函数直接扩展会因问题的非马尔可夫性导致超体积可识别性问题，亟需新方法解决。

Method: 受非马尔可夫强化学习和Transformer启发，提出广义深度Q学习框架BOFormer，通过序列建模实现MOBO。

Result: BOFormer在合成MOBO和实际超参数优化问题中，均优于基准的基于规则和学习的方法。

Conclusion: BOFormer为MOBO提供了有效解决方案，代码已开源以促进后续研究。

Abstract: Bayesian optimization (BO) offers an efficient pipeline for optimizing
black-box functions with the help of a Gaussian process prior and an
acquisition function (AF). Recently, in the context of single-objective BO,
learning-based AFs witnessed promising empirical results given its favorable
non-myopic nature. Despite this, the direct extension of these approaches to
multi-objective Bayesian optimization (MOBO) suffer from the
\textit{hypervolume identifiability issue}, which results from the
non-Markovian nature of MOBO problems. To tackle this, inspired by the
non-Markovian RL literature and the success of Transformers in language
modeling, we present a generalized deep Q-learning framework and propose
\textit{BOFormer}, which substantiates this framework for MOBO via sequence
modeling. Through extensive evaluation, we demonstrate that BOFormer constantly
outperforms the benchmark rule-based and learning-based algorithms in various
synthetic MOBO and real-world multi-objective hyperparameter optimization
problems. We have made the source code publicly available to encourage further
research in this direction.

</details>


### [192] [Two-Stage Feature Generation with Transformer and Reinforcement Learning](https://arxiv.org/abs/2505.21978)
*Wanfu Gao,Zengyao Man,Zebin He,Yuhao Tang,Jun Gao,Kunpeng Liu*

Main category: cs.LG

TL;DR: 提出了一种两阶段特征生成框架TSFG，结合Transformer和PPO优化特征生成，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统特征生成方法依赖领域知识和人工干预，自动化方法存在特征冗余和适应性差的问题。

Method: TSFG框架整合了基于Transformer的编码器-解码器架构和PPO算法，动态调整特征生成策略。

Result: 实验表明TSFG在特征质量和适应性上优于现有方法，显著提升模型预测性能。

Conclusion: TSFG通过动态生成高质量特征集，有效解决了特征生成中的挑战，具有广泛适用性。

Abstract: Feature generation is a critical step in machine learning, aiming to enhance
model performance by capturing complex relationships within the data and
generating meaningful new features. Traditional feature generation methods
heavily rely on domain expertise and manual intervention, making the process
labor-intensive and challenging to adapt to different scenarios. Although
automated feature generation techniques address these issues to some extent,
they often face challenges such as feature redundancy, inefficiency in feature
space exploration, and limited adaptability to diverse datasets and tasks. To
address these problems, we propose a Two-Stage Feature Generation (TSFG)
framework, which integrates a Transformer-based encoder-decoder architecture
with Proximal Policy Optimization (PPO). The encoder-decoder model in TSFG
leverages the Transformer's self-attention mechanism to efficiently represent
and transform features, capturing complex dependencies within the data. PPO
further enhances TSFG by dynamically adjusting the feature generation strategy
based on task-specific feedback, optimizing the process for improved
performance and adaptability. TSFG dynamically generates high-quality feature
sets, significantly improving the predictive performance of machine learning
models. Experimental results demonstrate that TSFG outperforms existing
state-of-the-art methods in terms of feature quality and adaptability.

</details>


### [193] [ACE: Exploring Activation Cosine Similarity and Variance for Accurate and Calibration-Efficient LLM Pruning](https://arxiv.org/abs/2505.21987)
*Zhendong Mi,Zhenglun Kong,Geng Yuan,Shaoyi Huang*

Main category: cs.LG

TL;DR: 提出一种高效的大语言模型剪枝方法，通过两种创新指标提升剪枝效果与速度，在LLaMA等模型上实现困惑度降低18%和剪枝时间减少63%。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型剪枝方法存在剪枝效果不佳或效率低下的问题，需要一种同时兼顾高性能与高效率的解决方案。

Method: 结合两种创新指标：1)基于激活余弦相似度损失的剪枝度量（考虑稠密模型与剪枝模型输出激活的角度偏差）；2)基于激活方差的剪枝度量（保持剪枝后输出激活的语义区分度）。

Result: 在LLaMA、LLaMA-2和OPT等主流模型上，困惑度最高降低18%，剪枝时间最多减少63%。

Conclusion: 该方法通过双指标协同优化，显著提升了大语言模型剪枝的精度与效率，为资源受限场景提供实用解决方案。

Abstract: With the rapid expansion of large language models (LLMs), the demand for
memory and computational resources has grown significantly. Recent advances in
LLM pruning aim to reduce the size and computational cost of these models.
However, existing methods often suffer from either suboptimal pruning
performance or low time efficiency during the pruning process. In this work, we
propose an efficient and effective pruning method that simultaneously achieves
high pruning performance and fast pruning speed with improved calibration
efficiency. Our approach introduces two key innovations: (1) An activation
cosine similarity loss-guided pruning metric, which considers the angular
deviation of the output activation between the dense and pruned models. (2) An
activation variance-guided pruning metric, which helps preserve semantic
distinctions in output activations after pruning, enabling effective pruning
with shorter input sequences. These two components can be readily combined to
enhance LLM pruning in both accuracy and efficiency. Experimental results show
that our method achieves up to an 18% reduction in perplexity and up to 63%
decrease in pruning time on prevalent LLMs such as LLaMA, LLaMA-2, and OPT.

</details>


### [194] [Learning in Compact Spaces with Approximately Normalized Transformers](https://arxiv.org/abs/2505.22014)
*Jörg K. H. Franke,Urs Spiegelhalter,Marianna Nezhurina,Jenia Jitsev,Frank Hutter,Michael Hefenbrock*

Main category: cs.LG

TL;DR: 该论文提出了一种近似归一化方法anTransformer，通过约束参数范数和归一化表示来加速GPT训练收敛，同时减少超参数需求。


<details>
  <summary>Details</summary>
Motivation: 深度学习中的正则化和归一化常用于解决过拟合、数值不稳定等问题，但存在额外成本。本文旨在提出一种更全面的近似归一化方法以提升效率。

Method: 提出anTransformer方法，通过标量乘法约束参数范数并归一化所有表示，利用高维随机向量范数的紧集中特性。

Result: 在GPT训练中，相比QK归一化模型，收敛速度提升40%，额外运行时间不足3%，且支持更大批次和更少超参数。

Conclusion: anTransformer方法在保持经典GPT架构优点的同时，显著提升了训练效率和可扩展性。

Abstract: In deep learning, regularization and normalization are common solutions for
challenges such as overfitting, numerical instabilities, and the increasing
variance in the residual stream. An alternative approach is to force all
parameters and representations to lie on a hypersphere. This removes the need
for regularization and increases convergence speed, but comes with additional
costs. In this work, we propose a more holistic but approximate normalization
(anTransformer). Our approach constrains the norm of parameters and normalizes
all representations via scalar multiplications motivated by the tight
concentration of the norms of high-dimensional random vectors. When applied to
GPT training, we observe a 40% faster convergence compared to models with QK
normalization, with less than 3% additional runtime. Deriving scaling laws for
anGPT, we found our method enables training with larger batch sizes and fewer
hyperparameters, while matching the favorable scaling characteristics of
classic GPT architectures.

</details>


### [195] [Weakly-Supervised Contrastive Learning for Imprecise Class Labels](https://arxiv.org/abs/2505.22028)
*Zi-Hao Zhou,Jun-Jie Wang,Tong Wei,Min-Ling Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一种基于连续语义相似度的弱监督对比学习框架，通过图论方法解决现实场景中标签模糊或不准确的问题。


<details>
  <summary>Details</summary>
Motivation: 现实场景中数据标注常存在模糊或不准确的问题，限制了监督对比学习的应用。论文旨在解决这一挑战。

Method: 提出连续语义相似度概念，通过图论框架迭代优化弱监督信号，构建正负样本对。

Result: 在噪声标签和部分标签学习场景中显著提升性能，理论证明其可在温和条件下逼近监督对比学习效果。

Conclusion: 该框架具有高度通用性，能有效整合现有方法，为弱监督学习提供了新的解决方案。

Abstract: Contrastive learning has achieved remarkable success in learning effective
representations, with supervised contrastive learning often outperforming
self-supervised approaches. However, in real-world scenarios, data annotations
are often ambiguous or inaccurate, meaning that class labels may not reliably
indicate whether two examples belong to the same class. This limitation
restricts the applicability of supervised contrastive learning. To address this
challenge, we introduce the concept of ``continuous semantic similarity'' to
define positive and negative pairs. Instead of directly relying on imprecise
class labels, we measure the semantic similarity between example pairs, which
quantifies how closely they belong to the same category by iteratively refining
weak supervisory signals. Based on this concept, we propose a graph-theoretic
framework for weakly-supervised contrastive learning, where semantic similarity
serves as the graph weights. Our framework is highly versatile and can be
applied to many weakly-supervised learning scenarios. We demonstrate its
effectiveness through experiments in two common settings, i.e., noisy label and
partial label learning, where existing methods can be easily integrated to
significantly improve performance. Theoretically, we establish an error bound
for our approach, showing that it can approximate supervised contrastive
learning under mild conditions. The implementation code is available at
https://github.com/Speechless-10308/WSC.

</details>


### [196] [Detecting Undesired Process Behavior by Means of Retrieval Augmented Generation](https://arxiv.org/abs/2505.22041)
*Michael Grohs,Adrian Rebmann,Jana-Rebecca Rehse*

Main category: cs.LG

TL;DR: 论文提出了一种无需专用流程模型或资源密集型微调的方法，利用检索增强生成（RAG）结合知识库来检测流程中的不良行为，效果优于微调的大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有的一致性检查技术需要专用流程模型，若模型不可用则无法应用。而基于大型语言模型（LLM）微调的方法资源消耗大且泛化能力有限，因此需要一种更高效的方法来检测不良流程行为。

Method: 采用检索增强生成（RAG）技术，为LLM提供直接访问包含其他流程中期望和不良行为的知识库，无需微调即可利用这些知识检测当前流程中的不良行为。

Result: 评估表明，该方法在检测不良行为方面优于微调的LLM，特别是在事件日志中补充了频繁轨迹和活动等上下文信息时效果更佳。

Conclusion: RAG是一种可行的替代方案，无需资源密集型微调即可有效检测流程中的不良行为，尤其在结合相关上下文信息时表现更优。

Abstract: Conformance checking techniques detect undesired process behavior by
comparing process executions that are recorded in event logs to desired
behavior that is captured in a dedicated process model. If such models are not
available, conformance checking techniques are not applicable, but
organizations might still be interested in detecting undesired behavior in
their processes. To enable this, existing approaches use Large Language Models
(LLMs), assuming that they can learn to distinguish desired from undesired
behavior through fine-tuning. However, fine-tuning is highly resource-intensive
and the fine-tuned LLMs often do not generalize well. To address these
limitations, we propose an approach that requires neither a dedicated process
model nor resource-intensive fine-tuning to detect undesired process behavior.
Instead, we use Retrieval Augmented Generation (RAG) to provide an LLM with
direct access to a knowledge base that contains both desired and undesired
process behavior from other processes, assuming that the LLM can transfer this
knowledge to the process at hand. Our evaluation shows that our approach
outperforms fine-tuned LLMs in detecting undesired behavior, demonstrating that
RAG is a viable alternative to resource-intensive fine-tuning, particularly
when enriched with relevant context from the event log, such as frequent traces
and activities.

</details>


### [197] [Estimating the Effects of Sample Training Orders for Large Language Models without Retraining](https://arxiv.org/abs/2505.22042)
*Hao Yang,Haoxuan Li,Mengyue Yang,Xu Chen,Mingming Gong*

Main category: cs.LG

TL;DR: 该论文提出了一种无需重新训练的框架，通过近似Adam优化器更新和随机投影方法，高效估计不同训练样本顺序下的模型参数，应用于课程设计和记忆/泛化效应分析。


<details>
  <summary>Details</summary>
Motivation: 传统方法研究训练样本顺序对大型语言模型（LLMs）的影响需要重新训练模型，计算成本高昂。本文旨在设计一种无需重新训练的方法来高效分析样本顺序的影响。

Method: 通过一阶和二阶泰勒展开近似Adam优化器更新，并利用随机投影方法存储中间检查点，构建了一个无需重新训练的框架，用于估计任意训练样本顺序下的模型参数。

Result: 实验验证了该框架能有效复现真实模型性能，并展示了其在优化LLM训练课程和分析记忆与泛化效应方面的潜力。

Conclusion: 该框架为研究LLMs的训练样本顺序效应提供了一种高效且计算可行的方法，具有广泛的应用前景。

Abstract: The order of training samples plays a crucial role in large language models
(LLMs), significantly impacting both their external performance and internal
learning dynamics. Traditional methods for investigating this effect generally
require retraining the model with various sample orders, which is
computationally infeasible for LLMs. In this work, we improve traditional
methods by designing a retraining-free framework. By approximating Adam
optimizer updates with first- and second-order Taylor expansions and utilizing
random projection methods to store intermediate checkpoints, our framework can
efficiently estimate model parameters for arbitrary training sample orders.
Next, we apply our framework to two downstream research problems: (1) Training
curriculum design for LLMs -- we base our retraining-free framework to propose
a novel curriculum learning strategy that augments curriculum proposals with
estimated model performances, enabling more informed sample scheduling. (2)
LLMs' memorization and generalization effect analysis -- we use our
retraining-free framework to estimate how the positions of training samples
influence LLMs' capacity for memorization and generalization. We conduct
extensive experiments to validate the effectiveness of our retraining-free
framework in reproducing the true model performances, and further demonstrate
its potential in optimizing LLM training curricula and analyzing the
memorization and generalization effects of LLMs.

</details>


### [198] [Differentiable Generalized Sliced Wasserstein Plans](https://arxiv.org/abs/2505.22049)
*Laetitia Chapel,Romain Tavenard,Samuel Vaiter*

Main category: cs.LG

TL;DR: 该论文提出了一种改进的最优传输切片方法min-SWGG，通过双层优化和可微分近似解决高维数据计算问题，并扩展至流形数据，应用于图像生成等领域。


<details>
  <summary>Details</summary>
Motivation: 最优传输（OT）在机器学习中应用广泛，但计算复杂度高，切片技术虽能扩展至大数据集，但仍存在维度灾难和线性投影限制。

Method: 将min-SWGG重新表述为双层优化问题，提出可微分近似方案以高效识别最优切片，并扩展至流形数据。

Result: 新方法在高维和流形数据中表现优异，成功应用于梯度流和图像生成等任务。

Conclusion: 改进的min-SWGG方法显著提升了计算效率，扩展了应用范围，为高维和流形数据的最优传输提供了实用解决方案。

Abstract: Optimal Transport (OT) has attracted significant interest in the machine
learning community, not only for its ability to define meaningful distances
between probability distributions -- such as the Wasserstein distance -- but
also for its formulation of OT plans. Its computational complexity remains a
bottleneck, though, and slicing techniques have been developed to scale OT to
large datasets. Recently, a novel slicing scheme, dubbed min-SWGG, lifts a
single one-dimensional plan back to the original multidimensional space,
finally selecting the slice that yields the lowest Wasserstein distance as an
approximation of the full OT plan. Despite its computational and theoretical
advantages, min-SWGG inherits typical limitations of slicing methods: (i) the
number of required slices grows exponentially with the data dimension, and (ii)
it is constrained to linear projections. Here, we reformulate min-SWGG as a
bilevel optimization problem and propose a differentiable approximation scheme
to efficiently identify the optimal slice, even in high-dimensional settings.
We furthermore define its generalized extension for accommodating to data
living on manifolds. Finally, we demonstrate the practical value of our
approach in various applications, including gradient flows on manifolds and
high-dimensional spaces, as well as a novel sliced OT-based conditional flow
matching for image generation -- where fast computation of transport plans is
essential.

</details>


### [199] [The Resurrection of the ReLU](https://arxiv.org/abs/2505.22074)
*Coşku Can Horuz,Geoffrey Kasenbacher,Saya Higuchi,Sebastian Kairat,Jendrik Stoltz,Moritz Pesl,Bernhard A. Moser,Christoph Linse,Thomas Martinetz,Sebastian Otte*

Main category: cs.LG

TL;DR: 论文提出了一种名为SUGAR的新型正则化方法，通过改进ReLU的梯度处理，解决了其易出现神经元死亡的问题，并在多种网络架构中展现出优于或媲美复杂激活函数的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管ReLU因其简单性和稀疏性等优势仍被广泛使用，但其存在的神经元死亡问题限制了其效果。当前趋势倾向于使用更复杂的激活函数（如GELU、SELU等），但本文旨在证明通过适当改进，ReLU仍能成为高效且通用的选择。

Method: 提出SUGAR方法，在前向传播中保留标准ReLU函数，但在反向传播中用平滑的替代梯度避免梯度消失，从而有效复活死亡的ReLU神经元。

Result: 实验表明，SUGAR在VGG-16、ResNet-18等卷积网络中显著提升了泛化性能，同时在Conv2NeXt和Swin Transformer等现代架构中表现优于或媲美GELU。

Conclusion: 研究挑战了复杂激活函数必然更优的普遍认知，表明通过梯度优化，传统ReLU仍能成为广泛深度学习视觉模型中的强效选择。

Abstract: Modeling sophisticated activation functions within deep learning
architectures has evolved into a distinct research direction. Functions such as
GELU, SELU, and SiLU offer smooth gradients and improved convergence
properties, making them popular choices in state-of-the-art models. Despite
this trend, the classical ReLU remains appealing due to its simplicity,
inherent sparsity, and other advantageous topological characteristics. However,
ReLU units are prone to becoming irreversibly inactive - a phenomenon known as
the dying ReLU problem - which limits their overall effectiveness. In this
work, we introduce surrogate gradient learning for ReLU (SUGAR) as a novel,
plug-and-play regularizer for deep architectures. SUGAR preserves the standard
ReLU function during the forward pass but replaces its derivative in the
backward pass with a smooth surrogate that avoids zeroing out gradients. We
demonstrate that SUGAR, when paired with a well-chosen surrogate function,
substantially enhances generalization performance over convolutional network
architectures such as VGG-16 and ResNet-18, providing sparser activations while
effectively resurrecting dead ReLUs. Moreover, we show that even in modern
architectures like Conv2NeXt and Swin Transformer - which typically employ GELU
- substituting these with SUGAR yields competitive and even slightly superior
performance. These findings challenge the prevailing notion that advanced
activation functions are necessary for optimal performance. Instead, they
suggest that the conventional ReLU, particularly with appropriate gradient
handling, can serve as a strong, versatile revived classic across a broad range
of deep learning vision models.

</details>


### [200] [Can Test-time Computation Mitigate Memorization Bias in Neural Symbolic Regression?](https://arxiv.org/abs/2505.22081)
*Shun Sato,Issei Sato*

Main category: cs.LG

TL;DR: 该研究探讨了神经符号回归（NSR）中Transformer的记忆偏差问题，发现其在处理多变量时性能下降，并提出测试时策略以减轻偏差。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决神经符号回归方法在处理多输入变量时性能低下的问题，特别是由Transformer的记忆偏差引起的限制。

Method: 通过合成数据集定量评估Transformer的记忆偏差，并进行理论分析，探讨测试时策略对减轻偏差的效果。

Result: 研究发现Transformer很少生成训练数据中未出现的表达式，测试时提供额外信息可显著减轻记忆偏差，但偏差减少不一定提升性能。

Conclusion: 研究深化了对NSR方法局限性的理解，为设计更鲁棒、泛化能力强的符号回归方法奠定了基础。

Abstract: Symbolic regression aims to discover mathematical equations that fit given
numerical data. It has been applied in various fields of scientific research,
such as producing human-readable expressions that explain physical phenomena.
Recently, Neural symbolic regression (NSR) methods that involve Transformers
pre-trained on large-scale synthetic datasets have gained attention. While
these methods offer advantages such as short inference time, they suffer from
low performance, particularly when the number of input variables is large. In
this study, we hypothesized that this limitation stems from the memorization
bias of Transformers in symbolic regression. We conducted a quantitative
evaluation of this bias in Transformers using a synthetic dataset and found
that Transformers rarely generate expressions not present in the training data.
Additional theoretical analysis reveals that this bias arises from the
Transformer's inability to construct expressions compositionally while
verifying their numerical validity. We finally examined if tailoring test-time
strategies can lead to reduced memorization bias and better performance. We
empirically demonstrate that providing additional information to the model at
test time can significantly mitigate memorization bias. On the other hand, we
also find that reducing memorization bias does not necessarily correlate with
improved performance. These findings contribute to a deeper understanding of
the limitations of NSR approaches and offer a foundation for designing more
robust, generalizable symbolic regression methods. Code is available at
https://github.com/Shun-0922/Mem-Bias-NSR .

</details>


### [201] [Inclusive, Differentially Private Federated Learning for Clinical Data](https://arxiv.org/abs/2505.22108)
*Santhosh Parampottupadam,Melih Coşğun,Sarthak Pati,Maximilian Zenk,Saikat Roy,Dimitrios Bounias,Benjamin Hamm,Sinem Sav,Ralf Floca,Klaus Maier-Hein*

Main category: cs.LG

TL;DR: 提出了一种基于合规性评分的联邦学习框架，通过动态调整差分隐私噪声，提升模型性能，同时确保隐私与合规。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习在临床AI应用中面临隐私、资源限制和合规性挑战，尤其是统一噪声添加方式会不公平地降低模型性能。

Method: 开发了合规感知的联邦学习框架，结合动态差分隐私噪声调整和基于医疗安全标准的合规评分工具。

Result: 在公共数据集上实验显示，整合资源不足和低合规性诊所后，模型准确率比传统联邦学习提升高达15%。

Conclusion: 该工作平衡了隐私、合规性和性能，为全球医疗临床工作流提供了可行的联邦学习解决方案。

Abstract: Federated Learning (FL) offers a promising approach for training clinical AI
models without centralizing sensitive patient data. However, its real-world
adoption is hindered by challenges related to privacy, resource constraints,
and compliance. Existing Differential Privacy (DP) approaches often apply
uniform noise, which disproportionately degrades model performance, even among
well-compliant institutions. In this work, we propose a novel compliance-aware
FL framework that enhances DP by adaptively adjusting noise based on
quantifiable client compliance scores. Additionally, we introduce a compliance
scoring tool based on key healthcare and security standards to promote secure,
inclusive, and equitable participation across diverse clinical settings.
Extensive experiments on public datasets demonstrate that integrating
under-resourced, less compliant clinics with highly regulated institutions
yields accuracy improvements of up to 15% over traditional FL. This work
advances FL by balancing privacy, compliance, and performance, making it a
viable solution for real-world clinical workflows in global healthcare.

</details>


### [202] [The quest for the GRAph Level autoEncoder (GRALE)](https://arxiv.org/abs/2505.22109)
*Paul Krzakala,Gabriel Melo,Charlotte Laclau,Florence d'Alché-Buc,Rémi Flamary*

Main category: cs.LG

TL;DR: GRALE是一种新型图自编码器，通过共享嵌入空间处理不同尺寸的图，采用最优传输损失和可微分节点匹配模块，在模拟和分子数据实验中表现出广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 图表示学习在化学和生物学等关键应用领域具有重要意义，但目前仍是一项具有挑战性的任务。

Method: GRALE采用基于注意力的架构，扩展了AlphaFold的Evoformer组件，支持图的编码和解码，并使用最优传输损失和可微分节点匹配模块进行训练。

Result: 实验表明，GRALE在分类、回归、图插值、编辑、匹配和预测等下游任务中具有高度通用性。

Conclusion: GRALE为图表示学习提供了一种通用的预训练方法，适用于多种复杂任务。

Abstract: Although graph-based learning has attracted a lot of attention, graph
representation learning is still a challenging task whose resolution may impact
key application fields such as chemistry or biology. To this end, we introduce
GRALE, a novel graph autoencoder that encodes and decodes graphs of varying
sizes into a shared embedding space. GRALE is trained using an Optimal
Transport-inspired loss that compares the original and reconstructed graphs and
leverages a differentiable node matching module, which is trained jointly with
the encoder and decoder. The proposed attention-based architecture relies on
Evoformer, the core component of AlphaFold, which we extend to support both
graph encoding and decoding. We show, in numerical experiments on simulated and
molecular data, that GRALE enables a highly general form of pre-training,
applicable to a wide range of downstream tasks, from classification and
regression to more complex tasks such as graph interpolation, editing,
matching, and prediction.

</details>


### [203] [BiMi Sheets: Infosheets for bias mitigation methods](https://arxiv.org/abs/2505.22114)
*MaryBeth Defrance,Guillaume Bied,Maarten Buyl,Jefrey Lijffijt,Tijl De Bie*

Main category: cs.LG

TL;DR: 提出BiMi Sheets作为统一文档工具，帮助记录和比较机器学习中的偏见缓解方法设计选择。


<details>
  <summary>Details</summary>
Motivation: 由于算法偏见具有领域、任务和模型特异性，现有偏见缓解方法难以直接移植和比较，限制了实际应用。

Method: 设计BiMi Sheets标准化模板，用于系统化记录偏见缓解方法的关键设计参数，并建立结构化数据库平台bimisheet.com。

Result: BiMi Sheets提供了便携式比较框架，支持研究者快速评估方法与需求的匹配度。

Conclusion: 该工具通过标准化文档促进偏见缓解方法的透明化、可比性及实际落地。

Abstract: Over the past 15 years, hundreds of bias mitigation methods have been
proposed in the pursuit of fairness in machine learning (ML). However,
algorithmic biases are domain-, task-, and model-specific, leading to a
`portability trap': bias mitigation solutions in one context may not be
appropriate in another. Thus, a myriad of design choices have to be made when
creating a bias mitigation method, such as the formalization of fairness it
pursues, and where and how it intervenes in the ML pipeline. This creates
challenges in benchmarking and comparing the relative merits of different bias
mitigation methods, and limits their uptake by practitioners.
  We propose BiMi Sheets as a portable, uniform guide to document the design
choices of any bias mitigation method. This enables researchers and
practitioners to quickly learn its main characteristics and to compare with
their desiderata. Furthermore, the sheets' structure allow for the creation of
a structured database of bias mitigation methods. In order to foster the
sheets' adoption, we provide a platform for finding and creating BiMi Sheets at
bimisheet.com.

</details>


### [204] [Oryx: a Performant and Scalable Algorithm for Many-Agent Coordination in Offline MARL](https://arxiv.org/abs/2505.22151)
*Claude Formanek,Omayma Mahjoub,Louay Ben Nessir,Sasha Abramowitz,Ruan de Kock,Wiem Khlifi,Simon Du Toit,Felix Chalumeau,Daniel Rajaonarivonivelomanantsoa,Arnol Fokam,Siddarth Singh,Ulrich Mbou Sob,Arnu Pretorius*

Main category: cs.LG

TL;DR: Oryx算法通过结合保留式架构与隐式约束Q学习，解决了离线多智能体强化学习中的复杂协调问题，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 离线多智能体强化学习（MARL）面临复杂环境中多智能体多步协调的挑战，需要一种能够保持时序一致性的新方法。

Method: Oryx结合了保留式架构Sable和隐式约束Q学习（ICQ），提出了一种新颖的离线自回归策略更新方案。

Result: Oryx在65个测试数据集中超过80%的表现优于现有方法，展示了在多智能体和长时程任务中的强大泛化能力。

Conclusion: Oryx不仅提升了离线MARL的性能，还通过新数据集推动了多智能体协调的研究，展现了良好的扩展性。

Abstract: A key challenge in offline multi-agent reinforcement learning (MARL) is
achieving effective many-agent multi-step coordination in complex environments.
In this work, we propose Oryx, a novel algorithm for offline cooperative MARL
to directly address this challenge. Oryx adapts the recently proposed
retention-based architecture Sable and combines it with a sequential form of
implicit constraint Q-learning (ICQ), to develop a novel offline
auto-regressive policy update scheme. This allows Oryx to solve complex
coordination challenges while maintaining temporal coherence over lengthy
trajectories. We evaluate Oryx across a diverse set of benchmarks from prior
works (SMAC, RWARE, and Multi-Agent MuJoCo) covering tasks of both discrete and
continuous control, varying in scale and difficulty. Oryx achieves
state-of-the-art performance on more than 80% of the 65 tested datasets,
outperforming prior offline MARL methods and demonstrating robust
generalisation across domains with many agents and long horizons. Finally, we
introduce new datasets to push the limits of many-agent coordination in offline
MARL, and demonstrate Oryx's superior ability to scale effectively in such
settings. We will make all of our datasets, experimental data, and code
available upon publication.

</details>


### [205] [Uncertainty Estimation for Heterophilic Graphs Through the Lens of Information Theory](https://arxiv.org/abs/2505.22152)
*Dominik Fuchsgruber,Tom Wollschläger,Johannes Bordne,Stephan Günnemann*

Main category: cs.LG

TL;DR: 该论文针对异质图（heterophilic graphs）上的不确定性估计问题，提出了一种基于信息论视角的MPNN分析方法，通过联合节点嵌入空间的后验密度估计器实现最优不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 现有图不确定性估计方法大多依赖同质性假设（homophily），在异质图场景下性能显著下降。论文旨在解决这一局限，探究MPNN在异质图中信息传递的特性。

Method: 1. 从信息论角度分析MPNN的信息流动，提出类似数据处理不等式的量化方法；2. 设计基于联合节点嵌入空间的简单后验密度估计器。

Result: 1. 发现异质图中节点深层嵌入可携带更多预测目标信息；2. 所提方法在异质图上达到SOTA不确定性估计，同时在同质图上保持竞争力且无需显式利用同质性。

Conclusion: 同时考虑所有节点表征是超越同质性假设的图不确定性估计关键原则，信息论分析为理解MPNN在不同图结构下的行为提供了新视角。

Abstract: While uncertainty estimation for graphs recently gained traction, most
methods rely on homophily and deteriorate in heterophilic settings. We address
this by analyzing message passing neural networks from an information-theoretic
perspective and developing a suitable analog to data processing inequality to
quantify information throughout the model's layers. In contrast to non-graph
domains, information about the node-level prediction target can increase with
model depth if a node's features are semantically different from its neighbors.
Therefore, on heterophilic graphs, the latent embeddings of an MPNN each
provide different information about the data distribution - different from
homophilic settings. This reveals that considering all node representations
simultaneously is a key design principle for epistemic uncertainty estimation
on graphs beyond homophily. We empirically confirm this with a simple post-hoc
density estimator on the joint node embedding space that provides
state-of-the-art uncertainty on heterophilic graphs. At the same time, it
matches prior work on homophilic graphs without explicitly exploiting homophily
through post-processing.

</details>


### [206] [The informativeness of the gradient revisited](https://arxiv.org/abs/2505.22158)
*Rustem Takhanov*

Main category: cs.LG

TL;DR: 该论文探讨了梯度下降在深度学习中的局限性，提出了一个关于梯度方差的一般性界限，并应用于LWE映射和高频函数。


<details>
  <summary>Details</summary>
Motivation: 近年来，基于梯度的深度学习在多个应用中取得了革命性进展，但其理论局限性尚未被充分理解。研究表明，在许多实际学习任务中，梯度包含的信息极其有限，导致需要大量迭代才能成功。

Method: 论文通过测量目标函数类别的成对独立性和输入分布的碰撞熵，提出了一个关于梯度方差的一般性界限。该界限与目标函数类别的独立性和输入分布的熵相关。

Result: 论文给出了梯度方差的一般性界限，并应用于LWE映射和高频函数。实验部分进一步分析了基于深度学习的LWE攻击的特性。

Conclusion: 该研究为理解梯度下降在深度学习中的局限性提供了理论支持，并通过实验验证了其在实际应用中的有效性。

Abstract: In the past decade gradient-based deep learning has revolutionized several
applications. However, this rapid advancement has highlighted the need for a
deeper theoretical understanding of its limitations. Research has shown that,
in many practical learning tasks, the information contained in the gradient is
so minimal that gradient-based methods require an exceedingly large number of
iterations to achieve success. The informativeness of the gradient is typically
measured by its variance with respect to the random selection of a target
function from a hypothesis class.
  We use this framework and give a general bound on the variance in terms of a
parameter related to the pairwise independence of the target function class and
the collision entropy of the input distribution. Our bound scales as $
\tilde{\mathcal{O}}(\varepsilon+e^{-\frac{1}{2}\mathcal{E}_c}) $, where $
\tilde{\mathcal{O}} $ hides factors related to the regularity of the learning
model and the loss function, $ \varepsilon $ measures the pairwise independence
of the target function class and $\mathcal{E}_c$ is the collision entropy of
the input distribution.
  To demonstrate the practical utility of our bound, we apply it to the class
of Learning with Errors (LWE) mappings and high-frequency functions. In
addition to the theoretical analysis, we present experiments to understand
better the nature of recent deep learning-based attacks on LWE.

</details>


### [207] [An Augmentation-Aware Theory for Self-Supervised Contrastive Learning](https://arxiv.org/abs/2505.22196)
*Jingyi Cui,Hongwei Wen,Yisen Wang*

Main category: cs.LG

TL;DR: 本文首次提出了一种考虑数据增强的自监督对比学习误差界限，揭示了特定增强类型对学习效果的影响，并通过实验验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 现有的理论研究尚未充分探讨数据增强在自监督对比学习中的作用，尤其是特定增强类型的影响。本文旨在填补这一空白。

Method: 提出了一种增强感知的误差界限，并在新的语义标签假设下讨论了不同增强方法对误差界限的影响。

Result: 实验表明，监督风险不仅受无监督风险的限制，还受到数据增强引起的权衡的显式影响。

Conclusion: 本文的理论和实验结果为理解数据增强在自监督对比学习中的作用提供了新的视角。

Abstract: Self-supervised contrastive learning has emerged as a powerful tool in
machine learning and computer vision to learn meaningful representations from
unlabeled data. Meanwhile, its empirical success has encouraged many
theoretical studies to reveal the learning mechanisms. However, in the existing
theoretical research, the role of data augmentation is still under-exploited,
especially the effects of specific augmentation types. To fill in the blank, we
for the first time propose an augmentation-aware error bound for
self-supervised contrastive learning, showing that the supervised risk is
bounded not only by the unsupervised risk, but also explicitly by a trade-off
induced by data augmentation. Then, under a novel semantic label assumption, we
discuss how certain augmentation methods affect the error bound. Lastly, we
conduct both pixel- and representation-level experiments to verify our proposed
theoretical results.

</details>


### [208] [Enhancing Uncertainty Estimation and Interpretability via Bayesian Non-negative Decision Layer](https://arxiv.org/abs/2505.22199)
*Xinyue Hu,Zhibin Duan,Bo Chen,Mingyuan Zhou*

Main category: cs.LG

TL;DR: 论文提出了一种贝叶斯非负决策层（BNDL），通过随机潜在变量建模复杂依赖关系，提升深度神经网络的不确定性估计和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前深度神经网络在不确定性估计和可解释性方面存在不足，模型决策常受无关特征干扰。

Method: BNDL将深度神经网络重构为条件贝叶斯非负因子分析，利用Weibull变分推理网络近似潜在变量的后验分布。

Result: 实验表明BNDL在提升模型精度的同时，提供了可靠的不确定性估计和更好的可解释性。

Conclusion: BNDL通过解耦表示和决策层，有效解决了深度神经网络的不确定性估计和可解释性问题。

Abstract: Although deep neural networks have demonstrated significant success due to
their powerful expressiveness, most models struggle to meet practical
requirements for uncertainty estimation. Concurrently, the entangled nature of
deep neural networks leads to a multifaceted problem, where various localized
explanation techniques reveal that multiple unrelated features influence the
decisions, thereby undermining interpretability. To address these challenges,
we develop a Bayesian Non-negative Decision Layer (BNDL), which reformulates
deep neural networks as a conditional Bayesian non-negative factor analysis. By
leveraging stochastic latent variables, the BNDL can model complex dependencies
and provide robust uncertainty estimation. Moreover, the sparsity and
non-negativity of the latent variables encourage the model to learn
disentangled representations and decision layers, thereby improving
interpretability. We also offer theoretical guarantees that BNDL can achieve
effective disentangled learning. In addition, we developed a corresponding
variational inference method utilizing a Weibull variational inference network
to approximate the posterior distribution of the latent variables. Our
experimental results demonstrate that with enhanced disentanglement
capabilities, BNDL not only improves the model's accuracy but also provides
reliable uncertainty estimation and improved interpretability.

</details>


### [209] [Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning](https://arxiv.org/abs/2505.22203)
*Yuzhen Huang,Weihao Zeng,Xingshan Zeng,Qi Zhu,Junxian He*

Main category: cs.LG

TL;DR: 论文分析了数学推理领域中基于规则和基于模型的验证器在强化学习中的可靠性问题，发现两者各有缺陷：前者易漏判等价答案，后者易被攻击导致误判。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解验证器在可验证奖励强化学习（RLVR）中的可靠性及其对训练过程的影响，特别是在数学推理等复杂领域。

Method: 方法包括对多种验证器进行静态评估和强化学习训练场景的全面分析，比较基于规则和基于模型的验证器的表现。

Result: 结果显示，基于规则的验证器常漏判不同格式的等价答案，而基于模型的验证器虽静态评估准确率高，但易被攻击导致误判。

Conclusion: 结论指出两种验证器各有独特风险，需开发更鲁棒的奖励系统以提升强化学习效果。

Abstract: Trustworthy verifiers are essential for the success of reinforcement learning
with verifiable reward (RLVR), which is the core methodology behind various
large reasoning models such as DeepSeek-R1. In complex domains like
mathematical reasoning, rule-based verifiers have been widely adopted in
previous works to train strong reasoning models. However, the reliability of
these verifiers and their impact on the RL training process remain poorly
understood. In this work, we take mathematical reasoning as a case study and
conduct a comprehensive analysis of various verifiers in both static evaluation
and RL training scenarios. First, we find that current open-source rule-based
verifiers often fail to recognize equivalent answers presented in different
formats across multiple commonly used mathematical datasets, resulting in
non-negligible false negative rates. This limitation adversely affects RL
training performance and becomes more pronounced as the policy model gets
stronger. Subsequently, we investigate model-based verifiers as a potential
solution to address these limitations. While the static evaluation shows that
model-based verifiers achieve significantly higher verification accuracy,
further analysis and RL training results imply that they are highly susceptible
to hacking, where they misclassify certain patterns in responses as correct
(i.e., false positives). This vulnerability is exploited during policy model
optimization, leading to artificially inflated rewards. Our findings underscore
the unique risks inherent to both rule-based and model-based verifiers, aiming
to offer valuable insights to develop more robust reward systems in
reinforcement learning.

</details>


### [210] [LaMM: Semi-Supervised Pre-Training of Large-Scale Materials Models](https://arxiv.org/abs/2505.22208)
*Yosuke Oyama,Yusuke Majima,Eiji Ohta,Yasufumi Sakai*

Main category: cs.LG

TL;DR: 提出LaMM方法，通过半监督预训练结合改进的去噪自监督学习和负载均衡算法，提升神经网络势能模型的训练效率和微调性能。


<details>
  <summary>Details</summary>
Motivation: 神经网络势能(NNPs)可加速计算材料科学，但传统预训练和微调方法因DFT标注成本和大规模预训练中的负载不平衡问题导致计算开销大。

Method: LaMM方法整合改进的去噪自监督学习与多节点负载均衡算法，利用3亿半标注样本进行高效预训练。

Result: 该方法显著提升了模型微调的速度和精度，验证了大规模半监督数据的有效利用。

Conclusion: LaMM为NNPs训练提供了一种高效解决方案，平衡了计算成本与模型性能。

Abstract: Neural network potentials (NNPs) are crucial for accelerating computational
materials science by surrogating density functional theory (DFT) calculations.
Improving their accuracy is possible through pre-training and fine-tuning,
where an NNP model is first pre-trained on a large-scale dataset and then
fine-tuned on a smaller target dataset. However, this approach is
computationally expensive, mainly due to the cost of DFT-based dataset labeling
and load imbalances during large-scale pre-training. To address this, we
propose LaMM, a semi-supervised pre-training method incorporating improved
denoising self-supervised learning and a load-balancing algorithm for efficient
multi-node training. We demonstrate that our approach effectively leverages a
large-scale dataset of $\sim$300 million semi-labeled samples to train a single
NNP model, resulting in improved fine-tuning performance in terms of both speed
and accuracy.

</details>


### [211] [Solver-Free Decision-Focused Learning for Linear Optimization Problems](https://arxiv.org/abs/2505.22224)
*Senne Berden,Ali İrfan Mahmutoğulları,Dimos Tsouros,Tias Guns*

Main category: cs.LG

TL;DR: 该论文提出了一种针对线性优化问题的高效训练方法，通过利用几何结构避免频繁求解优化问题，显著降低了计算成本并保持了决策质量。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，优化问题的参数常需通过机器学习预测，而决策导向学习（DFL）虽能提升决策质量，但因需频繁求解优化问题导致计算成本高昂。

Method: 基于线性优化问题的几何特性，通过比较真实最优解与其预计算相邻顶点的目标值差异设计损失函数，避免了求解器的调用。

Result: 实验表明，该方法显著减少了计算时间，同时决策质量下降极小。

Conclusion: 该研究为线性优化问题的决策导向学习提供了一种高效替代方案，平衡了计算效率与决策质量。

Abstract: Mathematical optimization is a fundamental tool for decision-making in a wide
range of applications. However, in many real-world scenarios, the parameters of
the optimization problem are not known a priori and must be predicted from
contextual features. This gives rise to predict-then-optimize problems, where a
machine learning model predicts problem parameters that are then used to make
decisions via optimization. A growing body of work on decision-focused learning
(DFL) addresses this setting by training models specifically to produce
predictions that maximize downstream decision quality, rather than accuracy.
While effective, DFL is computationally expensive, because it requires solving
the optimization problem with the predicted parameters at each loss evaluation.
In this work, we address this computational bottleneck for linear optimization
problems, a common class of problems in both DFL literature and real-world
applications. We propose a solver-free training method that exploits the
geometric structure of linear optimization to enable efficient training with
minimal degradation in solution quality. Our method is based on the insight
that a solution is optimal if and only if it achieves an objective value that
is at least as good as that of its adjacent vertices on the feasible polytope.
Building on this, our method compares the estimated quality of the ground-truth
optimal solution with that of its precomputed adjacent vertices, and uses this
as loss function. Experiments demonstrate that our method significantly reduces
computational cost while maintaining high decision quality.

</details>


### [212] [Optimal kernel regression bounds under energy-bounded noise](https://arxiv.org/abs/2505.22235)
*Amon Lahr,Johannes Köhler,Anna Scampicchio,Melanie N. Zeilinger*

Main category: cs.LG

TL;DR: 本文提出了一种紧致的非渐进不确定性边界方法，适用于基于核的估计，并能处理相关噪声序列，通过高斯过程后验均值和协方差计算最优边界。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于为估计算法提供准确的非保守不确定性边界，这对于评估算法精度和在安全关键场景中的应用至关重要。

Method: 方法基于核估计，假设未知函数和噪声具有有界范数，通过高斯过程后验均值和协方差计算最坏情况下的函数实现。

Result: 结果表明，该方法能够提供紧致且易于计算的不确定性边界，优于文献中的其他方法。

Conclusion: 本文提出的方法在核估计中有效提供了紧致的不确定性边界，适用于处理相关噪声，并具有实际应用价值。

Abstract: Non-conservative uncertainty bounds are key for both assessing an estimation
algorithm's accuracy and in view of downstream tasks, such as its deployment in
safety-critical contexts. In this paper, we derive a tight, non-asymptotic
uncertainty bound for kernel-based estimation, which can also handle correlated
noise sequences. Its computation relies on a mild norm-boundedness assumption
on the unknown function and the noise, returning the worst-case function
realization within the hypothesis class at an arbitrary query input location.
The value of this function is shown to be given in terms of the posterior mean
and covariance of a Gaussian process for an optimal choice of the measurement
noise covariance. By rigorously analyzing the proposed approach and comparing
it with other results in the literature, we show its effectiveness in returning
tight and easy-to-compute bounds for kernel-based estimates.

</details>


### [213] [B-XAIC Dataset: Benchmarking Explainable AI for Graph Neural Networks Using Chemical Data](https://arxiv.org/abs/2505.22252)
*Magdalena Proszewska,Tomasz Danel,Dawid Rymarczyk*

Main category: cs.LG

TL;DR: 该论文提出了B-XAIC基准，用于评估分子领域中图神经网络的可解释性方法，揭示了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前在化学信息学和药物发现领域，评估可解释人工智能（XAI）的方法多基于人工数据集或简化任务，无法反映真实场景的复杂性，且缺乏对解释忠实性的直接衡量。

Method: 作者引入了B-XAIC，一个基于真实分子数据和多样化任务构建的新基准，这些任务带有已知的真实标签解释。

Result: 通过B-XAIC的全面评估，揭示了现有图神经网络（GNNs）可解释性方法在分子领域的局限性。

Conclusion: B-XAIC为深入理解XAI的忠实性提供了宝贵资源，有助于开发更可靠和可解释的模型。

Abstract: Understanding the reasoning behind deep learning model predictions is crucial
in cheminformatics and drug discovery, where molecular design determines their
properties. However, current evaluation frameworks for Explainable AI (XAI) in
this domain often rely on artificial datasets or simplified tasks, employing
data-derived metrics that fail to capture the complexity of real-world
scenarios and lack a direct link to explanation faithfulness. To address this,
we introduce B-XAIC, a novel benchmark constructed from real-world molecular
data and diverse tasks with known ground-truth rationales for assigned labels.
Through a comprehensive evaluation using B-XAIC, we reveal limitations of
existing XAI methods for Graph Neural Networks (GNNs) in the molecular domain.
This benchmark provides a valuable resource for gaining deeper insights into
the faithfulness of XAI, facilitating the development of more reliable and
interpretable models.

</details>


### [214] [A Unified Online-Offline Framework for Co-Branding Campaign Recommendations](https://arxiv.org/abs/2505.22254)
*Xiangxiang Dai,Xiaowei Sun,Jinhang Zuo,Xutong Liu,John C. S. Lui*

Main category: cs.LG

TL;DR: 该论文提出了一种统一的线上线下框架，用于推荐跨行业联合品牌合作，通过动态学习和优化，提高了推荐效果和成本效益。


<details>
  <summary>Details</summary>
Motivation: 企业在推荐系统中寻找有效的跨行业联合品牌合作面临资源不平衡、品牌意愿不确定和市场条件多变等挑战。

Method: 构建二分图量化联合品牌概率和市场效益，结合在线学习和离线优化，平衡探索新合作和利用现有合作。

Result: 在合成和真实数据集上，框架表现优异，至少提升12%的效果，并实现了理论上的次线性遗憾边界。

Conclusion: 该框架有效解决了联合品牌推荐中的挑战，提升了短期和长期的市场表现，具有实际应用价值。

Abstract: Co-branding has become a vital strategy for businesses aiming to expand
market reach within recommendation systems. However, identifying effective
cross-industry partnerships remains challenging due to resource imbalances,
uncertain brand willingness, and ever-changing market conditions. In this
paper, we provide the first systematic study of this problem and propose a
unified online-offline framework to enable co-branding recommendations. Our
approach begins by constructing a bipartite graph linking ``initiating'' and
``target'' brands to quantify co-branding probabilities and assess market
benefits. During the online learning phase, we dynamically update the graph in
response to market feedback, while striking a balance between exploring new
collaborations for long-term gains and exploiting established partnerships for
immediate benefits. To address the high initial co-branding costs, our
framework mitigates redundant exploration, thereby enhancing short-term
performance while ensuring sustainable strategic growth. In the offline
optimization phase, our framework consolidates the interests of multiple
sub-brands under the same parent brand to maximize overall returns, avoid
excessive investment in single sub-brands, and reduce unnecessary costs
associated with over-prioritizing a single sub-brand. We present a theoretical
analysis of our approach, establishing a highly nontrivial sublinear regret
bound for online learning in the complex co-branding problem, and enhancing the
approximation guarantee for the NP-hard offline budget allocation optimization.
Experiments on both synthetic and real-world co-branding datasets demonstrate
the practical effectiveness of our framework, with at least 12\% improvement.

</details>


### [215] [Train Sparse Autoencoders Efficiently by Utilizing Features Correlation](https://arxiv.org/abs/2505.22255)
*Vadim Kurochkin,Yaroslav Aksenov,Daniil Laptev,Daniil Gavrilov,Nikita Balagansky*

Main category: cs.LG

TL;DR: KronSAE通过Kronecker乘积分解降低稀疏自编码器的计算开销，并引入mAND激活函数提升可解释性和性能。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器（SAEs）在解释语言模型隐藏状态方面表现优异，但大规模训练时计算开销大，尤其是编码器部分。

Method: 提出KronSAE架构，利用Kronecker乘积分解潜在表示以减少内存和计算开销，并引入mAND激活函数优化性能。

Result: KronSAE显著降低了计算和内存开销，同时mAND激活函数提升了模型的可解释性和性能。

Conclusion: KronSAE是一种高效且可解释的稀疏自编码器架构，适用于大规模语言模型分析。

Abstract: Sparse Autoencoders (SAEs) have demonstrated significant promise in
interpreting the hidden states of language models by decomposing them into
interpretable latent directions. However, training SAEs at scale remains
challenging, especially when large dictionary sizes are used. While decoders
can leverage sparse-aware kernels for efficiency, encoders still require
computationally intensive linear operations with large output dimensions. To
address this, we propose KronSAE, a novel architecture that factorizes the
latent representation via Kronecker product decomposition, drastically reducing
memory and computational overhead. Furthermore, we introduce mAND, a
differentiable activation function approximating the binary AND operation,
which improves interpretability and performance in our factorized framework.

</details>


### [216] [Revisiting Group Relative Policy Optimization: Insights into On-Policy and Off-Policy Training](https://arxiv.org/abs/2505.22257)
*Youssef Mroueh,Nicolas Dupuis,Brian Belgodere,Apoorva Nitsure,Mattia Rigotti,Kristjan Greenewald,Jiri Navratil,Jerret Ross,Jesus Rios*

Main category: cs.LG

TL;DR: 该论文重新审视了组相对策略优化（GRPO）在策略内和策略外优化机制中的应用，提出策略外GRPO在奖励提升方面表现优异或与策略内GRPO相当。


<details>
  <summary>Details</summary>
Motivation: 受到近期关于策略外近端策略优化（PPO）研究的启发，该研究旨在提升训练稳定性、采样效率和内存使用效率，并探讨策略外样本估计优势函数的潜在益处。

Method: 研究将GRPO适应到策略外设置，比较了策略内和策略外GRPO目标的奖励提升效果，并在策略外GRPO中采用了裁剪替代目标。

Result: 实验结果表明，策略外GRPO在验证性奖励的强化学习后训练中，显著优于或与策略内GRPO表现相当。

Conclusion: 策略外GRPO在多种情况下优于策略内GRPO，验证了其在强化学习中的有效性和潜力。

Abstract: We revisit Group Relative Policy Optimization (GRPO) in both on-policy and
off-policy optimization regimes. Our motivation comes from recent work on
off-policy Proximal Policy Optimization (PPO), which improves training
stability, sampling efficiency, and memory usage. In addition, a recent
analysis of GRPO suggests that estimating the advantage function with
off-policy samples could be beneficial. Building on these observations, we
adapt GRPO to the off-policy setting. We show that both on-policy and
off-policy GRPO objectives yield an improvement in the reward. This result
motivates the use of clipped surrogate objectives in the off-policy version of
GRPO. We then compare the empirical performance of reinforcement learning with
verifiable rewards in post-training using both GRPO variants. Our results show
that off-policy GRPO either significantly outperforms or performs on par with
its on-policy counterpart.

</details>


### [217] [Full Domain Analysis in Fluid Dynamics](https://arxiv.org/abs/2505.22275)
*Alexander Hagg,Adam Gaier,Dominik Wilde,Alexander Asteroth,Holger Foysi,Dirk Reith*

Main category: cs.LG

TL;DR: 论文提出全领域分析方法，结合进化优化、模拟和机器学习，用于高效探索复杂流体动力学等领域的解决方案空间。


<details>
  <summary>Details</summary>
Motivation: 针对计算成本高且行为复杂的领域（如流体动力学），传统方法难以全面分析解决方案空间，需开发更高效的分析工具。

Method: 基于优化和机器学习，构建全领域分析的形式化模型，包括解决方案生成、多样化、优化及交互式分析。

Result: 通过示例验证全领域分析能深化对复杂系统的理解，尤其在计算物理等领域具有应用潜力。

Conclusion: 全领域分析是理解复杂系统的有效工具，未来可扩展至更广泛的领域。

Abstract: Novel techniques in evolutionary optimization, simulation and machine
learning allow for a broad analysis of domains like fluid dynamics, in which
computation is expensive and flow behavior is complex. Under the term of full
domain analysis we understand the ability to efficiently determine the full
space of solutions in a problem domain, and analyze the behavior of those
solutions in an accessible and interactive manner. The goal of full domain
analysis is to deepen our understanding of domains by generating many examples
of flow, their diversification, optimization and analysis. We define a formal
model for full domain analysis, its current state of the art, and requirements
of subcomponents. Finally, an example is given to show what we can learn by
using full domain analysis. Full domain analysis, rooted in optimization and
machine learning, can be a helpful tool in understanding complex systems in
computational physics and beyond.

</details>


### [218] [Versatile Cardiovascular Signal Generation with a Unified Diffusion Transformer](https://arxiv.org/abs/2505.22306)
*Zehua Chen,Yuyang Miao,Liyuan Wang,Luyun Fan,Danilo P. Mandic,Jun Zhu*

Main category: cs.LG

TL;DR: UniCardio提出了一种多模态扩散变换器，用于心血管信号的重建与合成，显著优于现有方法，并在健康监测中表现出色。


<details>
  <summary>Details</summary>
Motivation: 心血管信号（如PPG、ECG和BP）具有互补性，但实时监测受限于采集挑战（如噪声或侵入性操作）。需要一种统一生成框架来提升信号质量并填补缺失数据。

Method: UniCardio采用多模态扩散变换器架构，结合持续学习范式，支持不同模态组合的信号生成、去噪和转换任务。

Result: UniCardio在信号去噪、填补和转换任务中优于现有方法，生成信号在异常检测和生命体征估计中与真实信号性能相当，且具有可解释性。

Conclusion: UniCardio为AI辅助医疗提供了高效、统一的解决方案，有望推动实时心血管健康监测的发展。

Abstract: Cardiovascular signals such as photoplethysmography (PPG),
electrocardiography (ECG), and blood pressure (BP) are inherently correlated
and complementary, together reflecting the health of cardiovascular system.
However, their joint utilization in real-time monitoring is severely limited by
diverse acquisition challenges from noisy wearable recordings to burdened
invasive procedures. Here we propose UniCardio, a multi-modal diffusion
transformer that reconstructs low-quality signals and synthesizes unrecorded
signals in a unified generative framework. Its key innovations include a
specialized model architecture to manage the signal modalities involved in
generation tasks and a continual learning paradigm to incorporate varying
modality combinations. By exploiting the complementary nature of cardiovascular
signals, UniCardio clearly outperforms recent task-specific baselines in signal
denoising, imputation, and translation. The generated signals match the
performance of ground-truth signals in detecting abnormal health conditions and
estimating vital signs, even in unseen domains, while ensuring interpretability
for human experts. These advantages position UniCardio as a promising avenue
for advancing AI-assisted healthcare.

</details>


### [219] [Transformers Pretrained on Procedural Data Contain Modular Structures for Algorithmic Reasoning](https://arxiv.org/abs/2505.22308)
*Zachary Shinnick,Liangze Jiang,Hemanth Saratchandran,Anton van den Hengel,Damien Teney*

Main category: cs.LG

TL;DR: 研究发现，即使使用简单的合成数据预训练小型Transformer模型，也能提升特定算法推理能力，且不同规则诱导的结构可互补组合。


<details>
  <summary>Details</summary>
Motivation: 探索简单合成数据如何赋予模型特定能力，这些能力在模型架构中的分布及权重表现，以提升模型的鲁棒性和数据效率。

Method: 通过大量消融实验和部分迁移实验，分析不同程序规则在模型不同部分（如注意力层和MLP块）诱导的结构。

Result: 不同程序规则在模型中诱导出互补的归纳结构，注意力层通常携带最多可迁移信息，而MLP块也可能包含有用结构。多规则结构可组合以增强多种能力。

Conclusion: 合成数据预训练可解耦知识获取与推理过程，为提升语言模型的鲁棒性和数据效率提供了新思路。

Abstract: Pretraining on large, semantically rich datasets is key for developing
language models. Surprisingly, recent studies have shown that even synthetic
data, generated procedurally through simple semantic-free algorithms, can yield
some of the same benefits as natural language pretraining. It is unclear what
specific capabilities such simple synthetic data instils in a model, where
these capabilities reside in the architecture, and how they manifest within its
weights. In this short paper, we identify several beneficial forms of
procedural data, together with specific algorithmic reasoning skills that
improve in small transformers. Our core finding is that different procedural
rules instil distinct but complementary inductive structures in the model. With
extensive ablations and partial-transfer experiments, we discover that these
structures reside in different parts of the model. Attention layers often carry
the most transferable information, but some pretraining rules impart useful
structure to MLP blocks instead. Most interestingly, the structures induced by
multiple rules can be composed to jointly reinforce multiple capabilities.
These results suggest an exciting possibility of disentangling the acquisition
of knowledge from reasoning in language models, with the goal of improving
their robustness and data efficiency.

</details>


### [220] [From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization](https://arxiv.org/abs/2505.22310)
*Shoaib Ahmed Siddiqui,Adrian Weller,David Krueger,Gintare Karolina Dziugaite,Michael Curtis Mozer,Eleni Triantafillou*

Main category: cs.LG

TL;DR: 研究发现，当前LLM的遗忘方法易受重新学习攻击，即使微调少量无关样本，遗忘的知识也会恢复。通过权重空间特性可预测抗攻击能力，并提出新方法提升抗攻击性。


<details>
  <summary>Details</summary>
Motivation: 探索LLM遗忘方法的脆弱性，发现遗忘的知识易通过微调重新恢复，需研究其机制并提出改进方法。

Method: 在视觉分类器中控制实验，分析遗忘集准确率恢复现象，并通过权重空间特性（如L2距离和线性模式连接性）预测抗攻击能力。

Result: 遗忘集准确率可从50%恢复至近100%，且新方法在抗重新学习攻击上达到最优。

Conclusion: 权重空间特性可有效预测抗攻击能力，新方法显著提升遗忘模型的鲁棒性。

Abstract: Recent unlearning methods for LLMs are vulnerable to relearning attacks:
knowledge believed-to-be-unlearned re-emerges by fine-tuning on a small set of
(even seemingly-unrelated) examples. We study this phenomenon in a controlled
setting for example-level unlearning in vision classifiers. We make the
surprising discovery that forget-set accuracy can recover from around 50%
post-unlearning to nearly 100% with fine-tuning on just the retain set -- i.e.,
zero examples of the forget set. We observe this effect across a wide variety
of unlearning methods, whereas for a model retrained from scratch excluding the
forget set (gold standard), the accuracy remains at 50%. We observe that
resistance to relearning attacks can be predicted by weight-space properties,
specifically, $L_2$-distance and linear mode connectivity between the original
and the unlearned model. Leveraging this insight, we propose a new class of
methods that achieve state-of-the-art resistance to relearning attacks.

</details>


### [221] [Skywork Open Reasoner 1 Technical Report](https://arxiv.org/abs/2505.22312)
*Jujie He,Jiacai Liu,Chris Yuhao Liu,Rui Yan,Chaojie Wang,Peng Cheng,Xiaoyu Zhang,Fuxiang Zhang,Jiacheng Xu,Wei Shen,Siyuan Li,Liang Zeng,Tianwen Wei,Cheng Cheng,Bo An,Yang Liu,Yahui Zhou*

Main category: cs.LG

TL;DR: Skywork-OR1通过强化学习显著提升大语言模型的推理能力，在多个基准测试中表现优异，并开源了模型权重和训练代码。


<details>
  <summary>Details</summary>
Motivation: 本文旨在通过强化学习（RL）进一步提升大语言模型（LLMs）的推理能力，特别是在长链思维（CoT）模型中的应用。

Method: 基于DeepSeek-R1-Distill模型系列，采用强化学习方法，并对训练流程的核心组件进行了全面的消融研究。

Result: Skywork-OR1-32B模型在AIME24和AIME25基准上超越了DeepSeek-R1和Qwen3-32B，7B模型在同类模型中表现出竞争力。

Conclusion: 通过缓解熵崩溃现象，Skywork-OR1显著提升了测试性能，并开源了所有资源以支持社区研究。

Abstract: The success of DeepSeek-R1 underscores the significant role of reinforcement
learning (RL) in enhancing the reasoning capabilities of large language models
(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL
implementation for long Chain-of-Thought (CoT) models. Building on the
DeepSeek-R1-Distill model series, our RL approach achieves notable performance
gains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench
from 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)
for the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and
Qwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable
results on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models
demonstrate competitive reasoning capabilities among models of similar size. We
perform comprehensive ablation studies on the core components of our training
pipeline to validate their effectiveness. Additionally, we thoroughly
investigate the phenomenon of entropy collapse, identify key factors affecting
entropy dynamics, and demonstrate that mitigating premature entropy collapse is
critical for improved test performance. To support community research, we fully
open-source our model weights, training code, and training datasets.

</details>


### [222] [Rethinking BPS: A Utility-Based Evaluation Framework](https://arxiv.org/abs/2505.22316)
*Konrad Özdemir,Lukas Kirchdorfer,Keyvan Amiri Elyasi,Han van der Aa,Heiner Stuckenschmidt*

Main category: cs.LG

TL;DR: 该论文提出了一种新的框架，用于评估业务流程模拟（BPS）模型的质量，通过比较模拟数据与真实数据在下游分析任务中的表现，解决了现有方法在评估模型准确性和数据复杂性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的业务流程模拟（BPS）模型评估方法存在两个主要问题：一是将模拟视为预测问题，无法准确评估模型对当前流程的捕捉能力；二是过度依赖基于Earth Mover's Distance的指标，可能掩盖时间模式并导致误导性结论。

Method: 论文提出了一种新的评估框架，通过比较在模拟数据和真实数据上训练的预测性流程监控模型在下游分析任务中的表现，来评估模拟质量。

Result: 实证结果表明，该框架不仅能帮助识别差异来源，还能区分模型准确性和数据复杂性，为评估BPS质量提供了更有意义的方法。

Conclusion: 该论文提出的框架为业务流程模拟模型的评估提供了一种更准确和实用的方法，能够更好地识别模型问题和数据复杂性，从而优化决策支持。

Abstract: Business process simulation (BPS) is a key tool for analyzing and optimizing
organizational workflows, supporting decision-making by estimating the impact
of process changes. The reliability of such estimates depends on the ability of
a BPS model to accurately mimic the process under analysis, making rigorous
accuracy evaluation essential. However, the state-of-the-art approach to
evaluating BPS models has two key limitations. First, it treats simulation as a
forecasting problem, testing whether models can predict unseen future events.
This fails to assess how well a model captures the as-is process, particularly
when process behavior changes from train to test period. Thus, it becomes
difficult to determine whether poor results stem from an inaccurate model or
the inherent complexity of the data, such as unpredictable drift. Second, the
evaluation approach strongly relies on Earth Mover's Distance-based metrics,
which can obscure temporal patterns and thus yield misleading conclusions about
simulation quality. To address these issues, we propose a novel framework that
evaluates simulation quality based on its ability to generate representative
process behavior. Instead of comparing simulated logs to future real-world
executions, we evaluate whether predictive process monitoring models trained on
simulated data perform comparably to those trained on real data for downstream
analysis tasks. Empirical results show that our framework not only helps
identify sources of discrepancies but also distinguishes between model accuracy
and data complexity, offering a more meaningful way to assess BPS quality.

</details>


### [223] [A Closer Look on Memorization in Tabular Diffusion Model: A Data-Centric Perspective](https://arxiv.org/abs/2505.22322)
*Zhengyu Fang,Zhimeng Jiang,Huiyuan Chen,Xiaoge Zhang,Kaiyu Tang,Xiao Li,Jing Li*

Main category: cs.LG

TL;DR: 该论文研究了表格数据扩散模型中的记忆化现象，发现少数样本对隐私泄露贡献最大，提出了DynamicCut方法来减少记忆化，同时保持数据多样性和下游性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成高质量表格数据方面表现出色，但也存在隐私风险，可能复制训练样本。现有研究主要关注数据集级别的增强以减少记忆化，但对哪些个体样本贡献最大知之甚少。

Method: 论文提出了DynamicCut方法，分为两个阶段：(a) 按每个epoch的记忆化强度对样本排序，(b) 剪除可调比例的顶部样本，(c) 在过滤后的数据集上重新训练。

Result: 实验表明，DynamicCut能有效减少记忆化，同时对数据多样性和下游性能影响最小。此外，该方法还展示了跨模型的可迁移性。

Conclusion: DynamicCut是一种模型无关的缓解方法，能有效减少表格扩散模型中的记忆化，且适用于其他生成模型如GANs和VAEs。

Abstract: Diffusion models have shown strong performance in generating high-quality
tabular data, but they carry privacy risks by reproducing exact training
samples. While prior work focuses on dataset-level augmentation to reduce
memorization, little is known about which individual samples contribute most.
We present the first data-centric study of memorization dynamics in tabular
diffusion models. We quantify memorization for each real sample based on how
many generated samples are flagged as replicas, using a relative distance
ratio. Our empirical analysis reveals a heavy-tailed distribution of
memorization counts: a small subset of samples contributes disproportionately
to leakage, confirmed via sample-removal experiments. To understand this, we
divide real samples into top- and non-top-memorized groups and analyze their
training-time behaviors. We track when each sample is first memorized and
monitor per-epoch memorization intensity (AUC). Memorized samples are memorized
slightly earlier and show stronger signals in early training. Based on these
insights, we propose DynamicCut, a two-stage, model-agnostic mitigation method:
(a) rank samples by epoch-wise intensity, (b) prune a tunable top fraction, and
(c) retrain on the filtered dataset. Across multiple tabular datasets and
models, DynamicCut reduces memorization with minimal impact on data diversity
and downstream performance. It also complements augmentation-based defenses.
Furthermore, DynamicCut enables cross-model transferability: high-ranked
samples identified from one model (e.g., a diffusion model) are also effective
for reducing memorization when removed from others, such as GANs and VAEs.

</details>


### [224] [Look Within or Look Beyond? A Theoretical Comparison Between Parameter-Efficient and Full Fine-Tuning](https://arxiv.org/abs/2505.22355)
*Yongkang Liu,Xingle Xu,Ercong Nie,Zijing Wang,Shi Feng,Daling Wang,Qian Li,Hinrich Schütze*

Main category: cs.LG

TL;DR: 本文通过理论分析和实验验证，揭示了参数高效微调（PEFT）在复杂任务中表现不及全参数微调（FFT）的原因，并提出了PEFT的理论上限。


<details>
  <summary>Details</summary>
Motivation: 尽管PEFT在资源消耗上优于FFT，但在复杂任务如推理和指令微调中表现不佳。本文旨在从优化理论角度比较PEFT和FFT的表征能力与鲁棒性差异。

Method: 基于优化理论，本文理论证明了PEFT是FFT的严格子集，并通过15个数据集和11个对抗测试集进行实验验证。

Result: 实验表明，PEFT有限的参数空间限制了模型的表征能力，使其更容易受到扰动影响，尤其在复杂任务中表现逊于FFT。

Conclusion: 本文揭示了PEFT的理论局限，呼吁在PEFT领域之外开展更多研究。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods achieve performance comparable
to Full Fine-Tuning (FFT) while requiring significantly fewer computing
resources, making it the go-to choice for researchers. We find that although
PEFT can achieve competitive results on some benchmarks, its performance falls
short of FFT in complex tasks, such as reasoning and instruction-based
fine-tuning. In this paper, we compare the characteristics of PEFT and FFT in
terms of representational capacity and robustness based on optimization theory.
We theoretically demonstrate that PEFT is a strict subset of FFT. By providing
theoretical upper bounds for PEFT, we show that the limited parameter space
constrains the model's representational ability, making it more susceptible to
perturbations. Experiments on 15 datasets encompassing classification,
generation, reasoning, instruction fine-tuning tasks and 11 adversarial test
sets validate our theories. We hope that these results spark further research
beyond the realms of well established PEFT. The source code is in the anonymous
Github repository\footnote{https://github.com/misonsky/PEFTEval}.

</details>


### [225] [Suitability Filter: A Statistical Framework for Classifier Evaluation in Real-World Deployment Settings](https://arxiv.org/abs/2505.22356)
*Angéline Pouget,Mohammad Yaghini,Stephan Rabanser,Nicolas Papernot*

Main category: cs.LG

TL;DR: 提出了一种名为‘适用性过滤器’的新框架，用于在无标签数据下检测机器学习模型性能退化，通过统计假设测试比较测试数据和用户数据的分布差异。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域部署机器学习模型时，缺乏真实标签验证模型性能退化是一个主要挑战。需要一种方法来主动检测潜在的性能下降。

Method: 利用对协变量偏移敏感且能指示预测错误的‘适用性信号’，通过统计假设测试比较测试数据和用户数据的分布，确保性能下降不超过预设阈值。

Result: 实验表明，适用性过滤器能有效检测由协变量偏移引起的性能偏差，适用于多种分类任务和模型。

Conclusion: 该方法为高风险应用提供了一种模块化、可靠的性能监控手段，可主动缓解潜在故障。

Abstract: Deploying machine learning models in safety-critical domains poses a key
challenge: ensuring reliable model performance on downstream user data without
access to ground truth labels for direct validation. We propose the suitability
filter, a novel framework designed to detect performance deterioration by
utilizing suitability signals -- model output features that are sensitive to
covariate shifts and indicative of potential prediction errors. The suitability
filter evaluates whether classifier accuracy on unlabeled user data shows
significant degradation compared to the accuracy measured on the labeled test
dataset. Specifically, it ensures that this degradation does not exceed a
pre-specified margin, which represents the maximum acceptable drop in accuracy.
To achieve reliable performance evaluation, we aggregate suitability signals
for both test and user data and compare these empirical distributions using
statistical hypothesis testing, thus providing insights into decision
uncertainty. Our modular method adapts to various models and domains. Empirical
evaluations across different classification tasks demonstrate that the
suitability filter reliably detects performance deviations due to covariate
shift. This enables proactive mitigation of potential failures in high-stakes
applications.

</details>


### [226] [Budget-Adaptive Adapter Tuning in Orthogonal Subspaces for Continual Learning in LLMs](https://arxiv.org/abs/2505.22358)
*Zhiyi Wan,Wanrou Du,Liang Li,Miao Pan,Xiaoqi Qin*

Main category: cs.LG

TL;DR: OA-Adapter提出了一种动态预算分配与正交子空间学习相结合的参数高效持续学习方法，解决了LLMs在持续学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在持续学习（CL）场景中常面临灾难性遗忘问题，现有方法多采用固定预算分配或解耦优化与预算分配，导致任务干扰或潜在不对齐。

Method: OA-Adapter通过动态瓶颈维度适应机制，在单阶段端到端训练中统一动态预算分配与正交子空间学习，并应用正交约束保护历史任务知识。

Result: 实验表明，OA-Adapter在标准CL基准测试中平均准确率更高，且参数使用量减少58.5%，优于现有方法。

Conclusion: OA-Adapter通过动态预算与正交学习的结合，显著提升了LLMs在持续学习中的性能与参数效率。

Abstract: Large language models (LLMs) often suffer from catastrophic forgetting in
continual learning (CL) scenarios, where performance on previously learned
tasks degrades severely while training on sequentially arriving tasks. Although
pioneering CL approaches using orthogonal subspaces can mitigate task
interference, they typically employ fixed budget allocation, neglecting the
varying complexity across tasks and layers. Besides, recent budget-adaptive
tuning methods for LLMs often adopt multi-stage paradigms that decouple
optimization and budget allocation. Such decoupling results in potential
misalignment, which hinders those approaches' practical application in CL
scenarios. To address these limitations, we propose OA-Adapter, a novel
parameter-efficient approach for continual learning in LLMs that unifies
dynamic budget adaptation with orthogonal subspace learning in a single
end-to-end training stage. Specifically, OA-Adapter introduces a dynamic
bottleneck dimension adaptation mechanism that simultaneously allocates an
efficient parameter budget and optimizes task objectives without misalignment.
To effectively preserve previously acquired knowledge while coordinating with
the dynamic budget allocation, orthogonal constraints are applied specifically
between the parameter subspace of the current task and the dynamically
allocated parameter subspaces of historical tasks. Experimental results on
continual learning benchmarks demonstrate that OA-Adapter outperforms
state-of-the-art methods in both accuracy and parameter efficiency, achieving
higher average accuracy while using 58.5% fewer parameters on the standard CL
benchmark.

</details>


### [227] [Multiclass Loss Geometry Matters for Generalization of Gradient Descent in Separable Classification](https://arxiv.org/abs/2505.22359)
*Matan Schliserman,Tomer Koren*

Main category: cs.LG

TL;DR: 本文研究了无正则化梯度方法在多类线性分类中的泛化性能，揭示了损失模板几何形状对收敛速率的关键影响。


<details>
  <summary>Details</summary>
Motivation: 先前的工作主要集中在二元分类，而本文旨在探讨多类设置下的泛化性能，特别是损失函数衰减至零时的情况。

Method: 通过分析梯度下降法在多类设置下的风险上界，重点关注损失模板的几何形状（由p-范数定义）对收敛速率的影响。

Result: 结果表明，在指数衰减损失下，p=∞时风险与k呈对数关系，而p=2时风险与k呈线性关系，并通过下界证明后者多项式依赖不可避免。

Conclusion: 损失模板的几何形状（而非损失函数本身）对多类分类的泛化性能具有决定性影响，特别是p-范数的选择显著影响风险缩放行为。

Abstract: We study the generalization performance of unregularized gradient methods for
separable linear classification. While previous work mostly deal with the
binary case, we focus on the multiclass setting with $k$ classes and establish
novel population risk bounds for Gradient Descent for loss functions that decay
to zero. In this setting, we show risk bounds that reveal that convergence
rates are crucially influenced by the geometry of the loss template, as
formalized by Wang and Scott (2024), rather than of the loss function itself.
Particularly, we establish risk upper bounds that holds for any decay rate of
the loss whose template is smooth with respect to the $p$-norm. In the case of
exponentially decaying losses, our results indicates a contrast between the
$p=\infty$ case, where the risk exhibits a logarithmic dependence on $k$, and
$p=2$ where the risk scales linearly with $k$. To establish this separation
formally, we also prove a lower bound in the latter scenario, demonstrating
that the polynomial dependence on $k$ is unavoidable. Central to our analysis
is a novel bound on the Rademacher complexity of low-noise vector-valued linear
predictors with a loss template smooth w.r.t.~general $p$-norms.

</details>


### [228] [Continuum-armed Bandit Optimization with Batch Pairwise Comparison Oracles](https://arxiv.org/abs/2505.22361)
*Xiangyu Chang,Xi Chen,Yining Wang,Zhiyi Zeng*

Main category: cs.LG

TL;DR: 该论文研究了一种新的成对比较反馈机制下的多臂老虎机优化问题，针对强凹函数提出了一种结合离散化和局部多项式逼近的算法，在库存管理和网络收益管理应用中取得优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现有随机优化文献中的反馈机制无法直接应用于联合定价与库存补货等运营管理问题，需要处理带有偏差的成对比较反馈，并同时优化动作选择和查询次数。

Method: 采用离散化技术和局部多项式逼近将问题转化为线性老虎机问题，开发了基于锦标赛逐次消除的交互式LinUCB算法。

Result: 所提算法获得了最优（忽略多对数因子）的遗憾界，在两大运营管理问题中的应用改进了现有文献的最佳结果。

Conclusion: 该研究为带有偏差成对比较反馈的优化问题提供了通用框架，在运营管理领域具有显著应用价值。

Abstract: This paper studies a bandit optimization problem where the goal is to
maximize a function $f(x)$ over $T$ periods for some unknown strongly concave
function $f$. We consider a new pairwise comparison oracle, where the
decision-maker chooses a pair of actions $(x, x')$ for a consecutive number of
periods and then obtains an estimate of $f(x)-f(x')$. We show that such a
pairwise comparison oracle finds important applications to joint pricing and
inventory replenishment problems and network revenue management. The challenge
in this bandit optimization is twofold. First, the decision-maker not only
needs to determine a pair of actions $(x, x')$ but also a stopping time $n$
(i.e., the number of queries based on $(x, x')$). Second, motivated by our
inventory application, the estimate of the difference $f(x)-f(x')$ is biased,
which is different from existing oracles in stochastic optimization literature.
To address these challenges, we first introduce a discretization technique and
local polynomial approximation to relate this problem to linear bandits. Then
we developed a tournament successive elimination technique to localize the
discretized cell and run an interactive batched version of LinUCB algorithm on
cells. We establish regret bounds that are optimal up to poly-logarithmic
factors. Furthermore, we apply our proposed algorithm and analytical framework
to the two operations management problems and obtain results that improve
state-of-the-art results in the existing literature.

</details>


### [229] [Directed Homophily-Aware Graph Neural Network](https://arxiv.org/abs/2505.22362)
*Aihu Zhang,Jiaxing Xu,Mengcheng Lan,Shili Xiang,Yiping Ke*

Main category: cs.LG

TL;DR: 该论文提出了一种名为DHGNN的新型图神经网络框架，通过结合同质性感知和方向敏感组件，解决了现有GNN在异质性邻域和有向图上的局限性。


<details>
  <summary>Details</summary>
Motivation: 大多数图神经网络（GNNs）难以泛化到异质性邻域，并且忽略真实世界图的方向性，导致在有向图上的性能不佳。

Method: DHGNN采用可重置的门控机制，根据同质性水平和信息量自适应调节消息贡献，并通过结构感知的噪声容忍融合模块有效整合原始和反向方向的节点表示。

Result: 在节点分类和链接预测任务中，DHGNN在多种数据集上优于现有方法，尤其在链接预测上比最佳基线提高了15.07%。

Conclusion: DHGNN通过门控机制捕捉方向性同质性差异和跨层波动，为复杂图结构上的消息传递行为提供了更深入的理解。

Abstract: Graph Neural Networks (GNNs) have achieved significant success in various
learning tasks on graph-structured data. Nevertheless, most GNNs struggle to
generalize to heterophilic neighborhoods. Additionally, many GNNs ignore the
directional nature of real-world graphs, resulting in suboptimal performance on
directed graphs with asymmetric structures. In this work, we propose Directed
Homophily-aware Graph Neural Network (DHGNN), a novel framework that addresses
these limitations by incorporating homophily-aware and direction-sensitive
components. DHGNN employs a resettable gating mechanism to adaptively modulate
message contributions based on homophily levels and informativeness, and a
structure-aware noise-tolerant fusion module to effectively integrate node
representations from the original and reverse directions. Extensive experiments
on both homophilic and heterophilic directed graph datasets demonstrate that
DHGNN outperforms state-of-the-art methods in node classification and link
prediction. In particular, DHGNN improves over the best baseline by up to
15.07% in link prediction. Our analysis further shows that the gating mechanism
captures directional homophily gaps and fluctuating homophily across layers,
providing deeper insights into message-passing behavior on complex graph
structures.

</details>


### [230] [SplitLoRA: Balancing Stability and Plasticity in Continual Learning Through Gradient Space Splitting](https://arxiv.org/abs/2505.22370)
*Haomiao Qiu,Miao Zhang,Ziyue Qiao,Weili Guan,Min Zhang,Liqiang Nie*

Main category: cs.LG

TL;DR: 本文提出了一种名为SplitLoRA的新方法，通过低秩适应（Low-Rank Adaptation）优化梯度空间划分，以在持续学习中平衡稳定性和可塑性。


<details>
  <summary>Details</summary>
Motivation: 现有的梯度投影方法难以在持续学习中实现稳定性和可塑性的最佳平衡，因为梯度空间的划分不够理想。

Method: 基于低秩适应（LoRA）的持续学习范式，提出SplitLoRA方法，通过理论分析梯度空间划分对模型性能的影响，并引入一种有效的方法来优化梯度空间的划分。

Result: 在多个数据集上的实验结果表明，SplitLoRA方法达到了最先进的性能。

Conclusion: SplitLoRA方法通过优化梯度空间划分，有效平衡了持续学习中的稳定性和可塑性，展现出卓越的性能。

Abstract: Continual Learning requires a model to learn multiple tasks in sequence while
maintaining both stability:preserving knowledge from previously learned tasks,
and plasticity:effectively learning new tasks. Gradient projection has emerged
as an effective and popular paradigm in CL, where it partitions the gradient
space of previously learned tasks into two orthogonal subspaces: a primary
subspace and a minor subspace. New tasks are learned effectively within the
minor subspace, thereby reducing interference with previously acquired
knowledge. However, existing Gradient Projection methods struggle to achieve an
optimal balance between plasticity and stability, as it is hard to
appropriately partition the gradient space. In this work, we consider a
continual learning paradigm based on Low-Rank Adaptation, which has gained
considerable attention due to its efficiency and wide applicability, and
propose a novel approach for continual learning, called SplitLoRA. We first
provide a theoretical analysis of how subspace partitioning affects model
stability and plasticity. Informed by this analysis, we then introduce an
effective method that derives the optimal partition of the gradient space for
previously learned tasks. This approach effectively balances stability and
plasticity in continual learning. Experimental results on multiple datasets
demonstrate that the proposed method achieves state-of-the-art performance.

</details>


### [231] [A Divide-and-Conquer Approach for Modeling Arrival Times in Business Process Simulation](https://arxiv.org/abs/2505.22381)
*Lukas Kirchdorfer,Konrad Özdemir,Stjepan Kusenic,Han van der Aa,Heiner Stuckenschmidt*

Main category: cs.LG

TL;DR: 论文提出了一种名为AT-KDE的新方法，用于改进业务流程模拟中的案例到达时间建模，解决了现有方法过于简化的问题，显著提高了准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的业务流程模拟（BPS）方法在案例到达时间建模上过于简化，无法捕捉组织环境中的动态和时间复杂性，导致模拟结果不够准确和可靠。

Method: 论文提出了一种名为Auto Time Kernel Density Estimation (AT-KDE)的分治方法，该方法结合了全局动态、星期变化和日内分布变化，以精确且可扩展的方式建模到达时间。

Result: 在20个不同流程上的实验表明，AT-KDE比现有方法更准确、更稳健，同时保持了合理的执行时间效率。

Conclusion: AT-KDE方法显著提升了业务流程模拟中案例到达时间建模的准确性和可靠性，为组织流程优化提供了更可靠的工具。

Abstract: Business Process Simulation (BPS) is a critical tool for analyzing and
improving organizational processes by estimating the impact of process changes.
A key component of BPS is the case-arrival model, which determines the pattern
of new case entries into a process. Although accurate case-arrival modeling is
essential for reliable simulations, as it influences waiting and overall cycle
times, existing approaches often rely on oversimplified static distributions of
inter-arrival times. These approaches fail to capture the dynamic and temporal
complexities inherent in organizational environments, leading to less accurate
and reliable outcomes. To address this limitation, we propose Auto Time Kernel
Density Estimation (AT-KDE), a divide-and-conquer approach that models arrival
times of processes by incorporating global dynamics, day-of-week variations,
and intraday distributional changes, ensuring both precision and scalability.
Experiments conducted across 20 diverse processes demonstrate that AT-KDE is
far more accurate and robust than existing approaches while maintaining
sensible execution time efficiency.

</details>


### [232] [Train with Perturbation, Infer after Merging: A Two-Stage Framework for Continual Learning](https://arxiv.org/abs/2505.22389)
*Haomiao Qiu,Miao Zhang,Ziyue Qiao,Liqiang Nie*

Main category: cs.LG

TL;DR: 论文提出Perturb-and-Merge (P&M)框架，通过模型合并和二阶扰动策略缓解持续学习中的遗忘问题，结合LoRA降低内存开销，在多个基准数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法仅依赖最新任务参数进行推理，易受灾难性遗忘影响。受模型合并技术启发，作者提出将合并机制引入持续学习以减轻遗忘。

Method: 1) 训练每个任务后，通过凸组合合并新旧模型；2) 理论推导最优合并系数；3) 提出基于任务向量和Hessian矩阵的扰动正则项，用二阶差分高效近似；4) 结合LoRA降低内存消耗。

Result: P&M框架在多个持续学习基准数据集上实现了最先进的性能表现。

Conclusion: 通过理论驱动的模型合并和计算高效的扰动策略，P&M有效缓解了持续学习中的遗忘问题，其与LoRA的结合进一步提升了实用性。

Abstract: Continual Learning (CL) aims to enable models to continuously acquire new
knowledge from a sequence of tasks with avoiding the forgetting of learned
information. However, existing CL methods only rely on the parameters of the
most recent task for inference, which makes them susceptible to catastrophic
forgetting. Inspired by the recent success of model merging techniques, we
propose \textbf{Perturb-and-Merge (P\&M)}, a novel continual learning framework
that integrates model merging into the CL paradigm to mitigate forgetting.
Specifically, after training on each task, P\&M constructs a new model by
forming a convex combination of the previous model and the newly trained
task-specific model. Through theoretical analysis, we minimize the total loss
increase across all tasks and derive an analytical solution for the optimal
merging coefficient. To further improve the performance of the merged model, we
observe that the degradation introduced during merging can be alleviated by a
regularization term composed of the task vector and the Hessian matrix of the
loss function. Interestingly, we show that this term can be efficiently
approximated using second-order symmetric finite differences, and a stochastic
perturbation strategy along the task vector direction is accordingly devised
which incurs no additional forward or backward passes while providing an
effective approximation of the regularization term. Finally, we combine P\&M
with LoRA, a parameter-efficient fine-tuning method, to reduce memory overhead.
Our proposed approach achieves state-of-the-art performance on several
continual learning benchmark datasets.

</details>


### [233] [Physics-Informed Distillation of Diffusion Models for PDE-Constrained Generation](https://arxiv.org/abs/2505.22391)
*Yi Zhang,Difan Zou*

Main category: cs.LG

TL;DR: 该论文提出了一种名为PIDDM的后处理蒸馏方法，用于在扩散模型中更有效地融入物理约束，以解决传统方法中PDE约束与生成模型精度之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在物理系统建模中表现出色，但由于其仅在中间步骤访问噪声数据，无法直接在干净样本上施加约束，导致PDE约束与生成模型精度之间存在权衡。

Method: 提出了一种后处理蒸馏方法PIDDM，不在扩散过程中直接注入PDE约束，而是在蒸馏阶段强制执行这些约束，从而支持单步生成和正反问题求解。

Result: 实验表明，PIDDM在多个PDE基准测试中显著提高了PDE满足度，且计算开销低于PIDM、DiffusionPDE和ECI-sampling等基线方法。

Conclusion: PIDDM为在扩散模型中融入物理约束提供了更高效有效的策略，同时支持正反问题求解和部分观测重建。

Abstract: Modeling physical systems in a generative manner offers several advantages,
including the ability to handle partial observations, generate diverse
solutions, and address both forward and inverse problems. Recently, diffusion
models have gained increasing attention in the modeling of physical systems,
particularly those governed by partial differential equations (PDEs). However,
diffusion models only access noisy data $\boldsymbol{x}_t$ at intermediate
steps, making it infeasible to directly enforce constraints on the clean sample
$\boldsymbol{x}_0$ at each noisy level. As a workaround, constraints are
typically applied to the expectation of clean samples
$\mathbb{E}[\boldsymbol{x}_0|\boldsymbol{x}_t]$, which is estimated using the
learned score network. However, imposing PDE constraints on the expectation
does not strictly represent the one on the true clean data, known as Jensen's
Gap. This gap creates a trade-off: enforcing PDE constraints may come at the
cost of reduced accuracy in generative modeling. To address this, we propose a
simple yet effective post-hoc distillation approach, where PDE constraints are
not injected directly into the diffusion process, but instead enforced during a
post-hoc distillation stage. We term our method as Physics-Informed
Distillation of Diffusion Models (PIDDM). This distillation not only
facilitates single-step generation with improved PDE satisfaction, but also
support both forward and inverse problem solving and reconstruction from
randomly partial observation. Extensive experiments across various PDE
benchmarks demonstrate that PIDDM significantly improves PDE satisfaction over
several recent and competitive baselines, such as PIDM, DiffusionPDE, and
ECI-sampling, with less computation overhead. Our approach can shed light on
more efficient and effective strategies for incorporating physical constraints
into diffusion models.

</details>


### [234] [Mitigating Overthinking in Large Reasoning Models via Manifold Steering](https://arxiv.org/abs/2505.22411)
*Yao Huang,Huanran Chen,Shouwei Ruan,Yichi Zhang,Xingxing Wei,Yinpeng Dong*

Main category: cs.LG

TL;DR: 该论文提出了一种名为Manifold Steering的新方法，通过低维流形投影减少大型推理模型中的过度思考现象，显著降低计算开销并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在解决复杂任务时表现出过度思考现象，导致计算资源浪费。论文旨在从机制可解释性角度研究并缓解这一问题。

Method: 论文首先发现过度思考现象可通过激活空间的单一方向捕捉，并通过干预该方向缓解问题。进一步研究发现过度思考与低维流形相关，提出Manifold Steering方法，将干预方向投影到低维流形上以减少噪声干扰。

Result: 实验表明，该方法在DeepSeek-R1模型上减少了71%的输出令牌，同时在多个数学基准测试中保持甚至提高了准确性，并在代码生成和知识问答任务中表现出良好的跨领域迁移性。

Conclusion: Manifold Steering方法有效缓解了大型推理模型的过度思考问题，显著降低了计算开销，同时保持了模型的准确性，具有广泛的适用性。

Abstract: Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable
capabilities in solving complex tasks such as mathematics and coding. However,
these models frequently exhibit a phenomenon known as overthinking during
inference, characterized by excessive validation loops and redundant
deliberation, leading to substantial computational overheads. In this paper, we
aim to mitigate overthinking by investigating the underlying mechanisms from
the perspective of mechanistic interpretability. We first showcase that the
tendency of overthinking can be effectively captured by a single direction in
the model's activation space and the issue can be eased by intervening the
activations along this direction. However, this efficacy soon reaches a plateau
and even deteriorates as the intervention strength increases. We therefore
systematically explore the activation space and find that the overthinking
phenomenon is actually tied to a low-dimensional manifold, which indicates that
the limited effect stems from the noises introduced by the high-dimensional
steering direction. Based on this insight, we propose Manifold Steering, a
novel approach that elegantly projects the steering direction onto the
low-dimensional activation manifold given the theoretical approximation of the
interference noise. Extensive experiments on DeepSeek-R1 distilled models
validate that our method reduces output tokens by up to 71% while maintaining
and even improving the accuracy on several mathematical benchmarks. Our method
also exhibits robust cross-domain transferability, delivering consistent token
reduction performance in code generation and knowledge-based QA tasks. Code is
available at: https://github.com/Aries-iai/Manifold_Steering.

</details>


### [235] [STaR-Bets: Sequential Target-Recalculating Bets for Tighter Confidence Intervals](https://arxiv.org/abs/2505.22422)
*Václav Voráček,Francesco Orabona*

Main category: cs.LG

TL;DR: 本文提出了一种基于赌博算法的新型置信区间构建方法，在固定时间范围内实现了最优宽度，并提供了理论保证。


<details>
  <summary>Details</summary>
Motivation: 在统计学和机器学习中，构建有界随机变量均值的紧致置信区间是一个经典问题。现有基于赌博算法的方法在固定时间范围内要么次优，要么缺乏有限时间理论保证。本文旨在填补这一空白。

Method: 提出了一种改进的赌博策略算法，在每一步采用最优策略（而非传统方法的固定策略），并结合Hoeffding和Bernstein等经典不等式进行优化。

Result: 新方法在实证中优于现有技术，置信区间宽度达到最优（误差项随样本量n递减为1+o(1)），并提供了代码实现。

Conclusion: 该研究为固定时间范围的置信区间构建提供了首个具有最优宽度理论保证的赌博算法解决方案，在统计推断和机器学习领域具有重要应用价值。

Abstract: The construction of confidence intervals for the mean of a bounded random
variable is a classical problem in statistics with numerous applications in
machine learning and virtually all scientific fields. In particular, obtaining
the tightest possible confidence intervals is vital every time the sampling of
the random variables is expensive. The current state-of-the-art method to
construct confidence intervals is by using betting algorithms. This is a very
successful approach for deriving optimal confidence sequences, even matching
the rate of law of iterated logarithms. However, in the fixed horizon setting,
these approaches are either sub-optimal or based on heuristic solutions with
strong empirical performance but without a finite-time guarantee. Hence, no
betting-based algorithm guaranteeing the optimal
$\mathcal{O}(\sqrt{\frac{\sigma^2\log\frac1\delta}{n}})$ width of the
confidence intervals are known. This work bridges this gap. We propose a
betting-based algorithm to compute confidence intervals that empirically
outperforms the competitors. Our betting strategy uses the optimal strategy in
every step (in a certain sense), whereas the standard betting methods choose a
constant strategy in advance. Leveraging this fact results in strict
improvements even for classical concentration inequalities, such as the ones of
Hoeffding or Bernstein. Moreover, we also prove that the width of our
confidence intervals is optimal up to an $1+o(1)$ factor diminishing with $n$.
The code is available
on~https://github.com/vvoracek/STaR-bets-confidence-interval.

</details>


### [236] [Scaling Reasoning without Attention](https://arxiv.org/abs/2505.22425)
*Xueliang Zhao,Wei Wu,Lingpeng Kong*

Main category: cs.LG

TL;DR: 提出了一种基于状态空间模型的无注意力语言模型，通过架构优化和两阶段课程微调策略，在复杂推理任务上超越同类Transformer模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型存在两个核心瓶颈：Transformer架构的效率低下，以及缺乏针对高难度领域的结构化微调。

Method: 采用Mamba-2的状态空间对偶层(SSD)替代注意力机制，并提出基于PromptCoT合成范式的两阶段课程微调策略。

Result: 7B参数的模型在多项基准测试中超越同规模Transformer模型，甚至优于27B参数的Gemma3模型。

Conclusion: 状态空间模型可作为注意力架构的高效替代方案，适用于高容量推理任务。

Abstract: Large language models (LLMs) have made significant advances in complex
reasoning tasks, yet they remain bottlenecked by two core challenges:
architectural inefficiency due to reliance on Transformers, and a lack of
structured fine-tuning for high-difficulty domains. We introduce \ourmodel, an
attention-free language model that addresses both issues through architectural
and data-centric innovations. Built on the state space dual (SSD) layers of
Mamba-2, our model eliminates the need for self-attention and key-value
caching, enabling fixed-memory, constant-time inference. To train it for
complex reasoning, we propose a two-phase curriculum fine-tuning strategy based
on the \textsc{PromptCoT} synthesis paradigm, which generates pedagogically
structured problems via abstract concept selection and rationale-guided
generation. On benchmark evaluations, \ourmodel-7B outperforms strong
Transformer and hybrid models of comparable scale, and even surpasses the much
larger Gemma3-27B by 2.6\% on AIME 24, 0.6\% on AIME 25, and 3.0\% on
Livecodebench. These results highlight the potential of state space models as
efficient and scalable alternatives to attention-based architectures for
high-capacity reasoning.

</details>


### [237] [Data-Driven Antenna Miniaturization: A Knowledge-Based System Integrating Quantum PSO and Predictive Machine Learning Models](https://arxiv.org/abs/2505.22440)
*Khan Masood Parvez,Sk Md Abidar Rahaman,Ali Shiri Sichani*

Main category: cs.LG

TL;DR: 该研究提出了一种结合量子行为动态粒子群优化（QDPSO）和ANSYS HFSS仿真的机器学习工作流，大幅加速天线设计过程，实现了240倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 无线技术的快速发展要求在有限的设计周期内实现天线小型化和性能优化，传统试错方法耗时且效率低下。

Method: 采用QDPSO算法优化天线尺寸，并结合多种机器学习模型（SVM、随机森林、XGBoost和堆叠模型）预测共振频率，最终通过ANSYS验证。

Result: QDPSO在11.53秒内完成优化，共振频率降低12.7%；机器学习模型在0.75秒内预测频率，堆叠模型训练精度最高（R2=0.9825）。完整设计周期仅需12.42分钟，远低于传统方法的50小时。

Conclusion: 该框架通过AI优化与CAD验证的结合，显著减少工程工作量并确保生产就绪的设计，为6G和物联网应用中的射频系统提供了可扩展的范例。

Abstract: The rapid evolution of wireless technologies necessitates automated design
frameworks to address antenna miniaturization and performance optimization
within constrained development cycles. This study demonstrates a machine
learning enhanced workflow integrating Quantum-Behaved Dynamic Particle Swarm
Optimization (QDPSO) with ANSYS HFSS simulations to accelerate antenna design.
The QDPSO algorithm autonomously optimized loop dimensions in 11.53 seconds,
achieving a resonance frequency of 1.4208 GHz a 12.7 percent reduction compared
to conventional 1.60 GHz designs. Machine learning models (SVM, Random Forest,
XGBoost, and Stacked ensembles) predicted resonance frequencies in 0.75 seconds
using 936 simulation datasets, with stacked models showing superior training
accuracy (R2=0.9825) and SVM demonstrating optimal validation performance
(R2=0.7197). The complete design cycle, encompassing optimization, prediction,
and ANSYS validation, required 12.42 minutes on standard desktop hardware
(Intel i5-8500, 16GB RAM), contrasting sharply with the 50-hour benchmark of
PSADEA-based approaches. This 240 times of acceleration eliminates traditional
trial-and-error methods that often extend beyond seven expert-led days. The
system enables precise specifications of performance targets with automated
generation of fabrication-ready parameters, particularly benefiting compact
consumer devices requiring rapid frequency tuning. By bridging AI-driven
optimization with CAD validation, this framework reduces engineering workloads
while ensuring production-ready designs, establishing a scalable paradigm for
next-generation RF systems in 6G and IoT applications.

</details>


### [238] [SOReL and TOReL: Two Methods for Fully Offline Reinforcement Learning](https://arxiv.org/abs/2505.22442)
*Mattie Fellows,Clarisse Wibault,Uljad Berdica,Johannes Forkel,Jakob N. Foerster,Michael A. Osborne*

Main category: cs.LG

TL;DR: 该论文提出了两种算法SOReL和TOReL，旨在解决离线强化学习中的样本效率和超参数调优问题，通过仅使用离线数据实现安全可靠的强化学习。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在现实世界中的应用受限于样本效率问题，离线强化学习虽能利用离线数据学习策略，但仍需大量在线交互进行超参数调优，且初始在线性能无可靠保证。

Method: SOReL通过贝叶斯方法推断环境动态的后验分布，利用后验预测不确定性估计在线性能；TOReL扩展了基于信息率的离线超参数调优方法，适用于一般离线强化学习方法。

Result: 实验表明，SOReL能准确估计贝叶斯设置中的遗憾，TOReL的离线超参数调优性能与最佳在线调优方法相当，仅使用离线数据。

Conclusion: SOReL和TOReL为安全可靠的离线强化学习迈出了重要一步，释放了强化学习在现实世界中的潜力。

Abstract: Sample efficiency remains a major obstacle for real world adoption of
reinforcement learning (RL): success has been limited to settings where
simulators provide access to essentially unlimited environment interactions,
which in reality are typically costly or dangerous to obtain. Offline RL in
principle offers a solution by exploiting offline data to learn a near-optimal
policy before deployment. In practice, however, current offline RL methods rely
on extensive online interactions for hyperparameter tuning, and have no
reliable bound on their initial online performance. To address these two
issues, we introduce two algorithms. Firstly, SOReL: an algorithm for safe
offline reinforcement learning. Using only offline data, our Bayesian approach
infers a posterior over environment dynamics to obtain a reliable estimate of
the online performance via the posterior predictive uncertainty. Crucially, all
hyperparameters are also tuned fully offline. Secondly, we introduce TOReL: a
tuning for offline reinforcement learning algorithm that extends our
information rate based offline hyperparameter tuning methods to general offline
RL approaches. Our empirical evaluation confirms SOReL's ability to accurately
estimate regret in the Bayesian setting whilst TOReL's offline hyperparameter
tuning achieves competitive performance with the best online hyperparameter
tuning methods using only offline data. Thus, SOReL and TOReL make a
significant step towards safe and reliable offline RL, unlocking the potential
for RL in the real world. Our implementations are publicly available:
https://github.com/CWibault/sorel\_torel.

</details>


### [239] [Position: All Current Generative Fidelity and Diversity Metrics are Flawed](https://arxiv.org/abs/2505.22450)
*Ossi Räisä,Boris van Breugel,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 当前生成模型评估指标存在缺陷，阻碍了合成数据的实际应用。作者提出评估指标的期望标准及检验方法，呼吁研究社区更关注指标开发。


<details>
  <summary>Details</summary>
Motivation: 生成模型的可靠性评估指标存在不足（如缺乏异常鲁棒性、边界模糊等），限制了方法的开发与实际应用。需要建立更完善的评估体系。

Method: 提出合成数据评估指标的期望标准，设计针对性实验（sanity checks）以检测已知生成模型缺陷。

Result: 现有生成模型的保真度和多样性评估指标均存在缺陷，严重影响合成数据的实用价值。

Conclusion: 呼吁优先开发更可靠的评估指标而非模型，并通过分析当前指标缺陷为实践者提供使用指南。

Abstract: Any method's development and practical application is limited by our ability
to measure its reliability. The popularity of generative modeling emphasizes
the importance of good synthetic data metrics. Unfortunately, previous works
have found many failure cases in current metrics, for example lack of outlier
robustness and unclear lower and upper bounds. We propose a list of desiderata
for synthetic data metrics, and a suite of sanity checks: carefully chosen
simple experiments that aim to detect specific and known generative modeling
failure modes. Based on these desiderata and the results of our checks, we
arrive at our position: all current generative fidelity and diversity metrics
are flawed. This significantly hinders practical use of synthetic data. Our aim
is to convince the research community to spend more effort in developing
metrics, instead of models. Additionally, through analyzing how current metrics
fail, we provide practitioners with guidelines on how these metrics should
(not) be used.

</details>


### [240] [Pure Exploration with Infinite Answers](https://arxiv.org/abs/2505.22473)
*Riccardo Poiani,Martino Bernasconi,Andrea Celli*

Main category: cs.LG

TL;DR: 本文研究了答案集可能无限的纯探索问题，提出了一个实例依赖的下界，分析了现有方法的不足，并提出了一个通用的渐进最优框架。


<details>
  <summary>Details</summary>
Motivation: 研究纯探索问题中答案集无限的情况，如连续函数的回归问题，现有方法在无限答案集下无法保证渐进最优性。

Method: 提出了Sticky-Sequence Track-and-Stop框架，结合了Track-and-Stop和Sticky Track-and-Stop的优点，适用于无限答案集。

Result: 新框架在无限答案集下具有渐进最优性，同时揭示了现有方法在某些特殊情况下的最优性。

Conclusion: Sticky-Sequence Track-and-Stop框架在无限答案集的纯探索问题中表现优异，填补了现有方法的不足。

Abstract: We study pure exploration problems where the set of correct answers is
possibly infinite, e.g., the regression of any continuous function of the means
of the bandit. We derive an instance-dependent lower bound for these problems.
By analyzing it, we discuss why existing methods (i.e., Sticky Track-and-Stop)
for finite answer problems fail at being asymptotically optimal in this more
general setting. Finally, we present a framework, Sticky-Sequence
Track-and-Stop, which generalizes both Track-and-Stop and Sticky
Track-and-Stop, and that enjoys asymptotic optimality. Due to its generality,
our analysis also highlights special cases where existing methods enjoy
optimality.

</details>


### [241] [Forecasting Multivariate Urban Data via Decomposition and Spatio-Temporal Graph Analysis](https://arxiv.org/abs/2505.22474)
*Amirhossein Sohrabbeig,Omid Ardakanian,Petr Musilek*

Main category: cs.LG

TL;DR: 该论文提出了一种基于图神经网络（GNN）的多变量时间序列预测模型，通过分解预处理和动态依赖捕捉，提升了城市多元数据（如天气、能源需求等）的预测性能。


<details>
  <summary>Details</summary>
Motivation: 城市多元数据（如天气、污染、能源需求等）之间存在复杂的时空依赖关系，传统方法难以有效建模。论文旨在通过GNN捕捉这些依赖，提升预测精度和可解释性。

Method: 采用分解预处理（趋势、季节、残差分离）结合图神经网络（GNN），动态建模多变量时空依赖关系。

Result: 在真实数据集（用电量、天气、碳排放等）上的实验表明，模型在多场景预测中表现优异，优于传统方法。

Conclusion: 该模型为智能基础设施优化提供了有效工具，有助于节能城市发展和公共福祉提升。

Abstract: The forecasting of multivariate urban data presents a complex challenge due
to the intricate dependencies between various urban metrics such as weather,
air pollution, carbon intensity, and energy demand. This paper introduces a
novel multivariate time-series forecasting model that utilizes advanced Graph
Neural Networks (GNNs) to capture spatial dependencies among different
time-series variables. The proposed model incorporates a decomposition-based
preprocessing step, isolating trend, seasonal, and residual components to
enhance the accuracy and interpretability of forecasts. By leveraging the
dynamic capabilities of GNNs, the model effectively captures interdependencies
and improves the forecasting performance. Extensive experiments on real-world
datasets, including electricity usage, weather metrics, carbon intensity, and
air pollution data, demonstrate the effectiveness of the proposed approach
across various forecasting scenarios. The results highlight the potential of
the model to optimize smart infrastructure systems, contributing to
energy-efficient urban development and enhanced public well-being.

</details>


### [242] [Non-Asymptotic Analysis of (Sticky) Track-and-Stop](https://arxiv.org/abs/2505.22475)
*Riccardo Poiani,Martino Bernasconi,Andrea Celli*

Main category: cs.LG

TL;DR: 论文分析了纯探索问题中的Track-and-Stop算法及其扩展Sticky Track-and-Stop，填补了非渐进性性能保证的研究空白。


<details>
  <summary>Details</summary>
Motivation: 在纯探索问题中，现有算法如Track-and-Stop在渐进条件下表现最优，但其非渐进性性能保证尚未明确，需要进一步研究。

Method: 研究采用了Track-and-Stop算法及其扩展Sticky Track-and-Stop算法，分析它们在非渐进条件下的性能。

Result: 论文提供了这两种算法在非渐进条件下的性能保证，填补了现有研究的空白。

Conclusion: 研究证实了Track-and-Stop及其扩展算法不仅在渐进条件下最优，在非渐进条件下也具有可靠的性能保证。

Abstract: In pure exploration problems, a statistician sequentially collects
information to answer a question about some stochastic and unknown environment.
The probability of returning a wrong answer should not exceed a maximum risk
parameter $\delta$ and good algorithms make as few queries to the environment
as possible. The Track-and-Stop algorithm is a pioneering method to solve these
problems. Specifically, it is well-known that it enjoys asymptotic optimality
sample complexity guarantees for $\delta\to 0$ whenever the map from the
environment to its correct answers is single-valued (e.g., best-arm
identification with a unique optimal arm). The Sticky Track-and-Stop algorithm
extends these results to settings where, for each environment, there might
exist multiple correct answers (e.g., $\epsilon$-optimal arm identification).
Although both methods are optimal in the asymptotic regime, their
non-asymptotic guarantees remain unknown. In this work, we fill this gap and
provide non-asymptotic guarantees for both algorithms.

</details>


### [243] [A Closer Look at Multimodal Representation Collapse](https://arxiv.org/abs/2505.22483)
*Abhra Chaudhuri,Anjan Dutta,Tu Bui,Serban Georgescu*

Main category: cs.LG

TL;DR: 该论文研究了多模态融合中的模态坍塌现象，提出通过跨模态知识蒸馏和显式基重分配算法来解决问题，并在多个基准测试中验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 多模态融合模型在实际应用中往往只依赖部分模态而忽略其他模态，这种现象被称为模态坍塌。论文旨在深入理解模态坍塌的原因，并提出解决方案。

Method: 论文通过理论分析发现模态坍塌是由于噪声特征与预测特征在融合头中的纠缠所致，提出使用跨模态知识蒸馏来解耦这些表示，并设计了一种显式基重分配算法以防止模态坍塌。

Result: 在多个多模态基准测试上的实验验证了论文的理论分析和所提算法的有效性，能够有效防止模态坍塌并处理缺失模态的问题。

Conclusion: 论文揭示了模态坍塌的内在机制，并通过跨模态知识蒸馏和显式基重分配算法成功解决了这一问题，为多模态融合模型的优化提供了新的思路。

Abstract: We aim to develop a fundamental understanding of modality collapse, a
recently observed empirical phenomenon wherein models trained for multimodal
fusion tend to rely only on a subset of the modalities, ignoring the rest. We
show that modality collapse happens when noisy features from one modality are
entangled, via a shared set of neurons in the fusion head, with predictive
features from another, effectively masking out positive contributions from the
predictive features of the former modality and leading to its collapse. We
further prove that cross-modal knowledge distillation implicitly disentangles
such representations by freeing up rank bottlenecks in the student encoder,
denoising the fusion-head outputs without negatively impacting the predictive
features from either modality. Based on the above findings, we propose an
algorithm that prevents modality collapse through explicit basis reallocation,
with applications in dealing with missing modalities. Extensive experiments on
multiple multimodal benchmarks validate our theoretical claims. Project page:
https://abhrac.github.io/mmcollapse/.

</details>


### [244] [Understanding Adversarial Training with Energy-based Models](https://arxiv.org/abs/2505.22486)
*Mujtaba Hussain Mirza,Maria Rosaria Briglia,Filippo Bartolucci,Senad Beadini,Giuseppe Lisanti,Iacopo Masi*

Main category: cs.LG

TL;DR: 该研究通过能量模型（EBM）框架分析对抗训练（AT）中的灾难性过拟合（CO）和鲁棒过拟合（RO）现象，提出Delta能量正则化器（DER）以平滑能量景观，并探索鲁棒分类器的生成能力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过能量视角理解对抗训练中的过拟合现象，并分析鲁棒分类器的内在生成能力，以改进对抗训练的稳定性和生成模型的表现。

Method: 采用能量模型框架分析对抗样本与自然样本的能量差异，提出Delta能量正则化器（DER）来缓解过拟合，并结合局部类主成分分析（PCA）和能量引导提升生成质量。

Result: DER有效缓解了CO和RO问题，鲁棒分类器作为生成模型在图像质量和多样性之间取得平衡，生成结果在IS和FID指标上表现优异。

Conclusion: 能量视角为对抗训练中的过拟合问题提供了新见解，DER和生成技术的结合显著提升了模型鲁棒性和生成能力。

Abstract: We aim at using Energy-based Model (EBM) framework to better understand
adversarial training (AT) in classifiers, and additionally to analyze the
intrinsic generative capabilities of robust classifiers. By viewing standard
classifiers through an energy lens, we begin by analyzing how the energies of
adversarial examples, generated by various attacks, differ from those of the
natural samples. The central focus of our work is to understand the critical
phenomena of Catastrophic Overfitting (CO) and Robust Overfitting (RO) in AT
from an energy perspective. We analyze the impact of existing AT approaches on
the energy of samples during training and observe that the behavior of the
``delta energy' -- change in energy between original sample and its adversarial
counterpart -- diverges significantly when CO or RO occurs. After a thorough
analysis of these energy dynamics and their relationship with overfitting, we
propose a novel regularizer, the Delta Energy Regularizer (DER), designed to
smoothen the energy landscape during training. We demonstrate that DER is
effective in mitigating both CO and RO across multiple benchmarks. We further
show that robust classifiers, when being used as generative models, have limits
in handling trade-off between image quality and variability. We propose an
improved technique based on a local class-wise principal component analysis
(PCA) and energy-based guidance for better class-specific initialization and
adaptive stopping, enhancing sample diversity and generation quality.
Considering that we do not explicitly train for generative modeling, we achieve
a competitive Inception Score (IS) and Fr\'echet inception distance (FID)
compared to hybrid discriminative-generative models.

</details>


### [245] [On the Surprising Effectiveness of Large Learning Rates under Standard Width Scaling](https://arxiv.org/abs/2505.22491)
*Moritz Haas,Sebastian Bordt,Ulrike von Luxburg,Leena Chennuru Vankadara*

Main category: cs.LG

TL;DR: 论文通过理论分析和实验验证，揭示了交叉熵损失函数下神经网络训练中的“可控发散”机制，解释了标准参数化在大学习率下仍能稳定训练的原因。


<details>
  <summary>Details</summary>
Motivation: 现有理论无法解释标准参数化（SP）在大学习率下的稳定训练现象，论文旨在探究这一理论与实践的差异。

Method: 通过分析神经网络训练动态，结合理论证明和跨优化器、架构及数据模态的实验验证。

Result: 发现交叉熵损失下存在“可控发散”机制，使网络在大学习率下仍能稳定训练，并验证了宽度缩放对学习率选择的预测作用。

Conclusion: 研究阐明了交叉熵损失下标准参数化的有效性，并指出了层间学习率缩放方法的适用边界。

Abstract: The dominant paradigm for training large-scale vision and language models is
He initialization and a single global learning rate (\textit{standard
parameterization}, SP). Despite its practical success, standard parametrization
remains poorly understood from a theoretical perspective: Existing
infinite-width theory would predict instability under large learning rates and
vanishing feature learning under stable learning rates. However, empirically
optimal learning rates consistently decay much slower than theoretically
predicted. By carefully studying neural network training dynamics, we
demonstrate that this discrepancy is not fully explained by finite-width
phenomena such as catapult effects or a lack of alignment between weights and
incoming activations. We instead show that the apparent contradiction can be
fundamentally resolved by taking the loss function into account: In contrast to
Mean Squared Error (MSE) loss, we prove that under cross-entropy (CE) loss, an
intermediate \textit{controlled divergence} regime emerges, where logits
diverge but loss, gradients, and activations remain stable. Stable training
under large learning rates enables persistent feature evolution at scale in all
hidden layers, which is crucial for the practical success of SP. In experiments
across optimizers (SGD, Adam), architectures (MLPs, GPT) and data modalities
(vision, language), we validate that neural networks operate in this controlled
divergence regime under CE loss but not under MSE loss. Our empirical evidence
suggests that width-scaling considerations are surprisingly useful for
predicting empirically optimal learning rate exponents. Finally, our analysis
clarifies the effectiveness and limitations of recently proposed layerwise
learning rate scalings for standard initialization.

</details>


### [246] [Demystifying the Paradox of Importance Sampling with an Estimated History-Dependent Behavior Policy in Off-Policy Evaluation](https://arxiv.org/abs/2505.22492)
*Hongyi Zhou,Josiah P. Hanna,Jin Zhu,Ying Yang,Chengchun Shi*

Main category: cs.LG

TL;DR: 本文通过理论分析解释了为何在强化学习的离策略评估中，使用历史依赖的行为策略估计能降低均方误差，尽管真实策略是马尔可夫的。


<details>
  <summary>Details</summary>
Motivation: 先前的研究经验性地表明，即使真实行为策略是马尔可夫的，估计历史依赖的行为策略也能降低均方误差（MSE）。然而，为何使用历史能降低MSE的原因尚未明确。本文旨在从理论上解释这一现象。

Method: 本文通过推导普通重要性采样（IS）估计器的MSE的偏差-方差分解，展示了历史依赖的行为策略估计如何降低渐近方差，同时增加有限样本偏差。此外，还扩展了这些发现到其他离策略评估器，如序列IS估计器、双重稳健估计器和边缘化IS估计器。

Result: 研究表明，随着估计的行为策略依赖更长的历史，方差会持续降低。这一结论适用于参数化和非参数化的行为策略估计。

Conclusion: 本文的理论分析为历史依赖的行为策略估计在离策略评估中的有效性提供了理论支持，并展示了其在降低方差方面的优势。

Abstract: This paper studies off-policy evaluation (OPE) in reinforcement learning with
a focus on behavior policy estimation for importance sampling. Prior work has
shown empirically that estimating a history-dependent behavior policy can lead
to lower mean squared error (MSE) even when the true behavior policy is
Markovian. However, the question of why the use of history should lower MSE
remains open. In this paper, we theoretically demystify this paradox by
deriving a bias-variance decomposition of the MSE of ordinary importance
sampling (IS) estimators, demonstrating that history-dependent behavior policy
estimation decreases their asymptotic variances while increasing their
finite-sample biases. Additionally, as the estimated behavior policy conditions
on a longer history, we show a consistent decrease in variance. We extend these
findings to a range of other OPE estimators, including the sequential IS
estimator, the doubly robust estimator and the marginalized IS estimator, with
the behavior policy estimated either parametrically or non-parametrically.

</details>


### [247] [ProSpero: Active Learning for Robust Protein Design Beyond Wild-Type Neighborhoods](https://arxiv.org/abs/2505.22494)
*Michal Kmicikiewicz,Vincent Fortuin,Ewa Szczurek*

Main category: cs.LG

TL;DR: ProSpero框架通过结合预训练生成模型和主动学习，高效设计高适应性和新颖性的蛋白质序列，同时保持生物合理性。


<details>
  <summary>Details</summary>
Motivation: 蛋白质工程中设计高适应性和新颖性的序列是一项挑战，传统方法在探索野生型邻域外时往往产生生物不合理的序列或依赖保真度低的代理模型。

Method: ProSpero框架结合预训练生成模型和主动学习，通过适应性残基选择和生物约束的序贯蒙特卡洛采样，实现高效探索。

Result: ProSpero在多种蛋白质工程任务中表现优异，能生成高适应性和新颖性的序列，即使代理模型不准确时仍有效。

Conclusion: ProSpero为数据高效的蛋白质工程提供了新方法，平衡了序列适应性与生物合理性，优于现有方法。

Abstract: Designing protein sequences of both high fitness and novelty is a challenging
task in data-efficient protein engineering. Exploration beyond wild-type
neighborhoods often leads to biologically implausible sequences or relies on
surrogate models that lose fidelity in novel regions. Here, we propose
ProSpero, an active learning framework in which a frozen pre-trained generative
model is guided by a surrogate updated from oracle feedback. By integrating
fitness-relevant residue selection with biologically-constrained Sequential
Monte Carlo sampling, our approach enables exploration beyond wild-type
neighborhoods while preserving biological plausibility. We show that our
framework remains effective even when the surrogate is misspecified. ProSpero
consistently outperforms or matches existing methods across diverse protein
engineering tasks, retrieving sequences of both high fitness and novelty.

</details>


### [248] [Geometric GNNs for Charged Particle Tracking at GlueX](https://arxiv.org/abs/2505.22504)
*Ahmed Hossam Mohammed,Kishansingh Rajput,Simon Taylor,Denis Furletov,Sergey Furletov,Malachi Schram*

Main category: cs.LG

TL;DR: 该研究评估了图神经网络(GNN)在Jefferson实验室GlueX实验中的粒子轨迹重建任务上的表现，证明GNN在效率和速度上优于传统方法，并比较了GPU与FPGA实现的优劣。


<details>
  <summary>Details</summary>
Motivation: 传统粒子轨迹重建方法在高能物理实验中随着数据量增加性能下降，而GNN因其对三维点云数据的天然适应性成为潜在解决方案。

Method: 使用GlueX实验的模拟数据训练GNN模型，并在模拟和真实数据上测试，同时比较GPU批量处理和FPGA实现的性能。

Result: GNN在保持固定纯度的前提下提高了分段效率，且通过GPU批量处理实现显著加速，但不同硬件平台存在性能权衡。

Conclusion: GNN为高能物理实验中的粒子轨迹重建提供了更高效快速的解决方案，硬件选择需根据具体需求权衡。

Abstract: Nuclear physics experiments are aimed at uncovering the fundamental building
blocks of matter. The experiments involve high-energy collisions that produce
complex events with many particle trajectories. Tracking charged particles
resulting from collisions in the presence of a strong magnetic field is
critical to enable the reconstruction of particle trajectories and precise
determination of interactions. It is traditionally achieved through
combinatorial approaches that scale worse than linearly as the number of hits
grows. Since particle hit data naturally form a 3-dimensional point cloud and
can be structured as graphs, Graph Neural Networks (GNNs) emerge as an
intuitive and effective choice for this task. In this study, we evaluate the
GNN model for track finding on the data from the GlueX experiment at Jefferson
Lab. We use simulation data to train the model and test on both simulation and
real GlueX measurements. We demonstrate that GNN-based track finding
outperforms the currently used traditional method at GlueX in terms of
segment-based efficiency at a fixed purity while providing faster inferences.
We show that the GNN model can achieve significant speedup by processing
multiple events in batches, which exploits the parallel computation capability
of Graphical Processing Units (GPUs). Finally, we compare the GNN
implementation on GPU and FPGA and describe the trade-off.

</details>


### [249] [Sparsification and Reconstruction from the Perspective of Representation Geometry](https://arxiv.org/abs/2505.22506)
*Wenjie Sun,Bingzhe Wu,Zhile Yang,Chengke Wu*

Main category: cs.LG

TL;DR: 该论文提出SAEMA方法，通过分析稀疏自编码器（SAEs）在语言模型激活向量表示中的分层结构，探讨稀疏编码如何影响特征解耦与重建性能，并验证了表示结构的可分离性与重建性能之间的因果关系。


<details>
  <summary>Details</summary>
Motivation: 研究稀疏自编码器（SAEs）如何组织语言模型激活向量的表示，以及这种组织方式与特征解耦和重建性能之间的关系，旨在从表示几何的角度解释稀疏性的原理。

Method: 提出SAEMA方法，通过观察对称半正定矩阵秩的变化验证表示的分层结构，定义局部和全局表示，并从优化角度干预全局表示。

Result: 稀疏编码通过合并相似语义特征和引入额外维度增强特征间区分度，全局表示的可分离性与重建性能存在显著因果关系。

Conclusion: 研究揭示了表示结构变化对重建性能的影响，强调理解表示并纳入表示约束的必要性，为开发新可解释工具和改进SAEs提供了实证参考。

Abstract: Sparse Autoencoders (SAEs) have emerged as a predominant tool in mechanistic
interpretability, aiming to identify interpretable monosemantic features.
However, how does sparse encoding organize the representations of activation
vector from language models? What is the relationship between this
organizational paradigm and feature disentanglement as well as reconstruction
performance? To address these questions, we propose the SAEMA, which validates
the stratified structure of the representation by observing the variability of
the rank of the symmetric semipositive definite (SSPD) matrix corresponding to
the modal tensor unfolded along the latent tensor with the level of noise added
to the residual stream. To systematically investigate how sparse encoding
alters representational structures, we define local and global representations,
demonstrating that they amplify inter-feature distinctions by merging similar
semantic features and introducing additional dimensionality. Furthermore, we
intervene the global representation from an optimization perspective, proving a
significant causal relationship between their separability and the
reconstruction performance. This study explains the principles of sparsity from
the perspective of representational geometry and demonstrates the impact of
changes in representational structure on reconstruction performance.
Particularly emphasizes the necessity of understanding representations and
incorporating representational constraints, providing empirical references for
developing new interpretable tools and improving SAEs. The code is available at
\hyperlink{https://github.com/wenjie1835/SAERepGeo}{https://github.com/wenjie1835/SAERepGeo}.

</details>


### [250] [Accelerating Optimization via Differentiable Stopping Time](https://arxiv.org/abs/2505.22509)
*Zhonglin Xie,Yiman Fong,Haoran Yuan,Zaiwen Wen*

Main category: cs.LG

TL;DR: 该论文提出了一种可微分的停止时间方法，解决了传统优化算法中时间不可微分的问题，为加速算法提供了新的可微分框架。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习应用中，优化是一个重要模块。传统方法通常关注在给定时间内降低损失，但其对偶问题（即最小化达到目标损失的时间）因时间不可微分而难以优化。论文旨在解决这一问题。

Method: 论文提出了一种基于微分方程的可微分停止时间，并设计了高效的反向传播算法，使其能够用于优化算法超参数和在线学习。

Result: 实验表明，该方法在各种问题上表现优异，验证了其有效性。

Conclusion: 通过引入可微分停止时间，论文为加速优化算法提供了一种新的可微分框架，并在实际应用中展示了其优越性能。

Abstract: Optimization is an important module of modern machine learning applications.
Tremendous efforts have been made to accelerate optimization algorithms. A
common formulation is achieving a lower loss at a given time. This enables a
differentiable framework with respect to the algorithm hyperparameters. In
contrast, its dual, minimizing the time to reach a target loss, is believed to
be non-differentiable, as the time is not differentiable. As a result, it
usually serves as a conceptual framework or is optimized using zeroth-order
methods. To address this limitation, we propose a differentiable stopping time
and theoretically justify it based on differential equations. An efficient
algorithm is designed to backpropagate through it. As a result, the proposed
differentiable stopping time enables a new differentiable formulation for
accelerating algorithms. We further discuss its applications, such as online
hyperparameter tuning and learning to optimize. Our proposed methods show
superior performance in comprehensive experiments across various problems,
which confirms their effectiveness.

</details>


### [251] [Evaluating Supervised Learning Models for Fraud Detection: A Comparative Study of Classical and Deep Architectures on Imbalanced Transaction Data](https://arxiv.org/abs/2505.22521)
*Chao Wang,Chuanhao Nie,Yunbo Liu*

Main category: cs.LG

TL;DR: 该研究比较了四种监督学习模型在高度不平衡的在线交易数据集上的欺诈检测性能，发现集成方法表现最佳，但不同模型在精确度和召回率之间存在权衡。


<details>
  <summary>Details</summary>
Motivation: 欺诈检测在金融和电子商务等高风险领域至关重要，未检测到的欺诈交易可能导致重大经济损失。因此，需要系统地比较不同模型在高度不平衡数据集上的性能。

Method: 研究系统地比较了四种监督学习模型：逻辑回归、随机森林、LightGBM和GRU网络，在一个大规模且高度不平衡的在线交易数据集上的表现。

Result: 集成方法（随机森林和LightGBM）在整体和类别特定指标上表现最优，逻辑回归提供了可靠且可解释的基线。GRU模型对少数欺诈类别的召回率较高，但精确度较低。

Conclusion: 研究强调了根据欺诈检测系统的具体风险容忍度和操作需求选择模型的重要性，并提供了对每种模型在检测罕见但重要的欺诈活动中的有效性的细致评估。

Abstract: Fraud detection remains a critical task in high-stakes domains such as
finance and e-commerce, where undetected fraudulent transactions can lead to
significant economic losses. In this study, we systematically compare the
performance of four supervised learning models - Logistic Regression, Random
Forest, Light Gradient Boosting Machine (LightGBM), and a Gated Recurrent Unit
(GRU) network - on a large-scale, highly imbalanced online transaction dataset.
While ensemble methods such as Random Forest and LightGBM demonstrated superior
performance in both overall and class-specific metrics, Logistic Regression
offered a reliable and interpretable baseline. The GRU model showed strong
recall for the minority fraud class, though at the cost of precision,
highlighting a trade-off relevant for real-world deployment. Our evaluation
emphasizes not only weighted averages but also per-class precision, recall, and
F1-scores, providing a nuanced view of each model's effectiveness in detecting
rare but consequential fraudulent activity. The findings underscore the
importance of choosing models based on the specific risk tolerance and
operational needs of fraud detection systems.

</details>


### [252] [Test-Time Alignment of Discrete Diffusion Models with Sequential Monte Carlo](https://arxiv.org/abs/2505.22524)
*Chinmay Pani,Zijing Ou,Yingzhen Li*

Main category: cs.LG

TL;DR: 提出一种基于序贯蒙特卡洛的无训练方法，用于在测试时从奖励对齐的目标分布中采样，解决离散扩散模型在现实应用中需满足约束但无需任务特定微调的问题。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，生成过程常需满足特定约束，但又不希望进行任务特定的微调。因此，需要一种无需训练的方法来在测试时实现约束满足的采样。

Method: 采用基于序贯蒙特卡洛（SMC）的无训练方法，利用扭曲SMC和通过奖励函数一阶泰勒展开获得的近似局部最优提案。为解决离散空间中梯度未定义的问题，引入Gumbel-Softmax松弛，实现离散生成框架内的高效梯度近似。

Result: 在合成数据集和图像建模上的实验验证了该方法的有效性。

Conclusion: 所提出的方法能够在无需任务特定微调的情况下，有效实现约束满足的生成过程，并在实验中表现出良好的性能。

Abstract: Discrete diffusion models have become highly effective across various
domains. However, real-world applications often require the generative process
to adhere to certain constraints but without task-specific fine-tuning. To this
end, we propose a training-free method based on Sequential Monte Carlo (SMC) to
sample from the reward-aligned target distribution at the test time. Our
approach leverages twisted SMC with an approximate locally optimal proposal,
obtained via a first-order Taylor expansion of the reward function. To address
the challenge of ill-defined gradients in discrete spaces, we incorporate a
Gumbel-Softmax relaxation, enabling efficient gradient-based approximation
within the discrete generative framework. Empirical results on both synthetic
datasets and image modelling validate the effectiveness of our approach.

</details>


### [253] [Training RL Agents for Multi-Objective Network Defense Tasks](https://arxiv.org/abs/2505.22531)
*Andres Molina-Markham,Luis Robaina,Sean Steinle,Akash Trivedi,Derek Tsui,Nicholas Potteiger,Lauren Brandt,Ransom Winder,Ahmed Ridley*

Main category: cs.LG

TL;DR: 论文提出了一种受开放学习(OEL)启发的训练方法，用于开发自主网络防御代理，以增强其鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管OEL在其他领域显示出潜力，但将其应用于现实网络安全仍具挑战性。论文旨在通过OEL原则解决网络防御中的技术难题，如任务表示的一致性问题。

Method: 采用OEL框架，提出一种保持目标、奖励和动作空间一致性的任务表示方法，使代理能在多样化网络条件和攻击行为中持续学习。

Result: 研究表明，OEL方法能有效提升网络防御代理的鲁棒性和泛化性能，为构建网络安全基准提供了关键工具。

Conclusion: 该工作为AI在网络安全领域的应用提供了新范式，强调未来研究需注重任务多样性与表示一致性的结合。

Abstract: Open-ended learning (OEL) -- which emphasizes training agents that achieve
broad capability over narrow competency -- is emerging as a paradigm to develop
artificial intelligence (AI) agents to achieve robustness and generalization.
However, despite promising results that demonstrate the benefits of OEL,
applying OEL to develop autonomous agents for real-world cybersecurity
applications remains a challenge.
  We propose a training approach, inspired by OEL, to develop autonomous
network defenders. Our results demonstrate that like in other domains, OEL
principles can translate into more robust and generalizable agents for cyber
defense. To apply OEL to network defense, it is necessary to address several
technical challenges. Most importantly, it is critical to provide a task
representation approach over a broad universe of tasks that maintains a
consistent interface over goals, rewards and action spaces. This way, the
learning agent can train with varying network conditions, attacker behaviors,
and defender goals while being able to build on previously gained knowledge.
  With our tools and results, we aim to fundamentally impact research that
applies AI to solve cybersecurity problems. Specifically, as researchers
develop gyms and benchmarks for cyber defense, it is paramount that they
consider diverse tasks with consistent representations, such as those we
propose in our work.

</details>


### [254] [TabularQGAN: A Quantum Generative Model for Tabular Data](https://arxiv.org/abs/2505.22533)
*Pallavi Bhardwaj,Caitlin Jones,Lasse Dierich,Aleksandar Vučković*

Main category: cs.LG

TL;DR: 本文提出了一种新型量子生成对抗网络架构，用于合成表格数据，在医疗和人口普查数据集上表现优于经典模型。


<details>
  <summary>Details</summary>
Motivation: 现实中的企业数据多为表格形式且异构，包含分类和数值特征，但因数据稀缺或隐私问题难以获取。合成数据可解决这一问题。

Method: 采用灵活的量子数据编码和新型量子电路架构，构建量子生成对抗网络来建模表格数据。

Result: 量子模型在整体相似度评分上平均优于经典模型8.5%，且仅使用经典模型0.072%的参数。

Conclusion: 这是量子生成模型在表格数据任务中的首次成功展示，表明该任务可能非常适合量子计算机处理。

Abstract: In this paper, we introduce a novel quantum generative model for synthesizing
tabular data. Synthetic data is valuable in scenarios where real-world data is
scarce or private, it can be used to augment or replace existing datasets.
Real-world enterprise data is predominantly tabular and heterogeneous, often
comprising a mixture of categorical and numerical features, making it highly
relevant across various industries such as healthcare, finance, and software.
We propose a quantum generative adversarial network architecture with flexible
data encoding and a novel quantum circuit ansatz to effectively model tabular
data. The proposed approach is tested on the MIMIC III healthcare and Adult
Census datasets, with extensive benchmarking against leading classical models,
CTGAN, and CopulaGAN. Experimental results demonstrate that our quantum model
outperforms classical models by an average of 8.5% with respect to an overall
similarity score from SDMetrics, while using only 0.072% of the parameters of
the classical models. Additionally, we evaluate the generalization capabilities
of the models using two custom-designed metrics that demonstrate the ability of
the proposed quantum model to generate useful and novel samples. To our
knowledge, this is one of the first demonstrations of a successful quantum
generative model for handling tabular data, indicating that this task could be
well-suited to quantum computers.

</details>


### [255] [Uncertainty Quantification with Proper Scoring Rules: Adjusting Measures to Prediction Tasks](https://arxiv.org/abs/2505.22538)
*Paul Hofman,Yusuf Sale,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 本文提出了一种基于严格适当评分规则的灵活不确定性量化框架，适用于不同任务，并通过实验验证了其优势。


<details>
  <summary>Details</summary>
Motivation: 解决不确定性量化问题，提出一种灵活的方法以适应不同应用场景的需求。

Method: 基于严格适当评分规则的分解，构建总不确定性、偶然不确定性和认知不确定性的度量框架。

Result: 实验表明，在选择性预测任务中，评分规则应与任务损失匹配；在分布外检测中，互信息表现最佳；在主动学习中，基于零一损失的认知不确定性度量优于其他方法。

Conclusion: 提出的不确定性量化框架具有灵活性，并能根据不同任务需求进行优化，实验证明了其有效性。

Abstract: We address the problem of uncertainty quantification and propose measures of
total, aleatoric, and epistemic uncertainty based on a known decomposition of
(strictly) proper scoring rules, a specific type of loss function, into a
divergence and an entropy component. This leads to a flexible framework for
uncertainty quantification that can be instantiated with different losses
(scoring rules), which makes it possible to tailor uncertainty quantification
to the use case at hand. We show that this flexibility is indeed advantageous.
In particular, we analyze the task of selective prediction and show that the
scoring rule should ideally match the task loss. In addition, we perform
experiments on two other common tasks. For out-of-distribution detection, our
results confirm that a widely used measure of epistemic uncertainty, mutual
information, performs best. Moreover, in the setting of active learning, our
measure of epistemic uncertainty based on the zero-one-loss consistently
outperforms other uncertainty measures.

</details>


### [256] [A Human-Centric Approach to Explainable AI for Personalized Education](https://arxiv.org/abs/2505.22541)
*Vinitra Swamy*

Main category: cs.LG

TL;DR: 该论文探讨了可解释AI在教育领域的应用，提出了四种新技术以提高AI模型的透明度和信任度，并通过实证评估验证其效果。


<details>
  <summary>Details</summary>
Motivation: 尽管深度神经网络在AI研究中表现优异，但在教育领域的实际应用仍然有限，主要原因是模型决策缺乏可解释性，导致学生、家长和教师对其缺乏信任。

Method: 论文提出了四种技术贡献：多模态模块化架构（MultiModN）、可解释的混合专家模型（InterpretCC）、对抗训练以提高解释器的稳定性，以及理论驱动的LLM-XAI框架（iLLuMinaTE），并在多种环境中进行了评估。

Result: 通过结合现有解释器的实证评估和新架构设计，论文为以人为本的AI系统奠定了基础，平衡了最先进的性能和内置的透明度与信任。

Conclusion: 该研究为可解释AI在教育领域的应用提供了技术支持和实证基础，推动了AI系统在个性化学习中的透明度和信任度。

Abstract: Deep neural networks form the backbone of artificial intelligence research,
with potential to transform the human experience in areas ranging from
autonomous driving to personal assistants, healthcare to education. However,
their integration into the daily routines of real-world classrooms remains
limited. It is not yet common for a teacher to assign students individualized
homework targeting their specific weaknesses, provide students with instant
feedback, or simulate student responses to a new exam question. While these
models excel in predictive performance, this lack of adoption can be attributed
to a significant weakness: the lack of explainability of model decisions,
leading to a lack of trust from students, parents, and teachers. This thesis
aims to bring human needs to the forefront of eXplainable AI (XAI) research,
grounded in the concrete use case of personalized learning and teaching. We
frame the contributions along two verticals: technical advances in XAI and
their aligned human studies. We investigate explainability in AI for education,
revealing systematic disagreements between post-hoc explainers and identifying
a need for inherently interpretable model architectures. We propose four novel
technical contributions in interpretability with a multimodal modular
architecture (MultiModN), an interpretable mixture-of-experts model
(InterpretCC), adversarial training for explainer stability, and a
theory-driven LLM-XAI framework to present explanations to students
(iLLuMinaTE), which we evaluate in diverse settings with professors, teachers,
learning scientists, and university students. By combining empirical
evaluations of existing explainers with novel architectural designs and human
studies, our work lays a foundation for human-centric AI systems that balance
state-of-the-art performance with built-in transparency and trust.

</details>


### [257] [DES-LOC: Desynced Low Communication Adaptive Optimizers for Training Foundation Models](https://arxiv.org/abs/2505.22549)
*Alex Iacob,Lorenzo Sani,Mher Safaryan,Paris Giampouras,Samuel Horváth,Andrej Jovanovic,Meghdad Kurmanji,Preslav Aleksandrov,William F. Shen,Xinchi Qiu,Nicholas D. Lane*

Main category: cs.LG

TL;DR: DES-LOC是一种新型分布式优化器，通过独立同步参数和动量来降低通信成本，同时保持收敛性，适用于大规模基础模型训练。


<details>
  <summary>Details</summary>
Motivation: 现有的分布式数据并行（DDP）方法在基础模型训练中受限于带宽，而局部SGD等方法无法直接应用于自适应优化器，且现有扩展方法要么缺乏收敛保证，要么通信成本过高。

Method: 提出DES-LOC优化器家族，通过为参数和动量分配独立的同步周期，减少通信成本并保持收敛性。

Result: 实验表明，DES-LOC的通信量比DDP减少170倍，比当前最优的Local ADAM减少2倍，且适用于易发生系统故障的实际训练场景。

Conclusion: DES-LOC为大规模基础模型训练提供了一种可扩展、带宽高效且容错的解决方案。

Abstract: Scaling foundation model training with Distributed Data Parallel (DDP)
methods is bandwidth-limited. Existing infrequent communication methods like
Local SGD were designed to synchronize only model parameters and cannot be
trivially applied to adaptive optimizers due to additional optimizer states.
Current approaches extending Local SGD either lack convergence guarantees or
require synchronizing all optimizer states, tripling communication costs. We
propose Desynced Low Communication Adaptive Optimizers (DES-LOC), a family of
optimizers assigning independent synchronization periods to parameters and
momenta, enabling lower communication costs while preserving convergence.
Through extensive experiments on language models of up to 1.7B, we show that
DES-LOC can communicate 170x less than DDP and 2x less than the previous
state-of-the-art Local ADAM. Furthermore, unlike previous heuristic approaches,
DES-LOC is suited for practical training scenarios prone to system failures.
DES-LOC offers a scalable, bandwidth-efficient, and fault-tolerant solution for
foundation model training.

</details>


### [258] [Geometric Hyena Networks for Large-scale Equivariant Learning](https://arxiv.org/abs/2505.22560)
*Artem Moskalev,Mangal Prakash,Junjie Xu,Tianyu Cui,Rui Liao,Tommaso Mansi*

Main category: cs.LG

TL;DR: 论文提出Geometric Hyena，首个用于几何系统的等变长卷积模型，能在亚二次复杂度下捕获全局几何上下文，同时保持旋转和平移等变性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在建模生物、化学和物理系统时，处理全局几何上下文同时保持等变性至关重要。然而，由于等变性和全局上下文在大规模计算上的需求，这一任务具有挑战性。现有方法如等变自注意力存在二次复杂度问题，而局部方法如基于距离的消息传递则牺牲了全局信息。

Method: 受状态空间和长卷积模型近期成功的启发，作者引入了Geometric Hyena，这是一种等变长卷积模型，能够在亚二次复杂度下捕获全局几何上下文，同时保持旋转和平移等变性。

Result: 在大RNA分子的全原子属性预测和全蛋白质分子动力学评估中，Geometric Hyena优于现有的等变模型，同时所需内存和计算资源显著少于等变自注意力。具体而言，该模型处理30k令牌的几何上下文比等变Transformer快20倍，并在相同预算下允许72倍更长的上下文。

Conclusion: Geometric Hyena是一种高效的等变长卷积模型，能够在大规模几何系统中有效捕获全局上下文，显著提升了计算效率和性能。

Abstract: Processing global geometric context while preserving equivariance is crucial
when modeling biological, chemical, and physical systems. Yet, this is
challenging due to the computational demands of equivariance and global context
at scale. Standard methods such as equivariant self-attention suffer from
quadratic complexity, while local methods such as distance-based message
passing sacrifice global information. Inspired by the recent success of
state-space and long-convolutional models, we introduce Geometric Hyena, the
first equivariant long-convolutional model for geometric systems. Geometric
Hyena captures global geometric context at sub-quadratic complexity while
maintaining equivariance to rotations and translations. Evaluated on all-atom
property prediction of large RNA molecules and full protein molecular dynamics,
Geometric Hyena outperforms existing equivariant models while requiring
significantly less memory and compute that equivariant self-attention. Notably,
our model processes the geometric context of 30k tokens 20x faster than the
equivariant transformer and allows 72x longer context within the same budget.

</details>


### [259] [FNOPE: Simulation-based inference on function spaces with Fourier Neural Operators](https://arxiv.org/abs/2505.22573)
*Guy Moss,Leah Sophie Muhle,Reinhard Drews,Jakob H. Macke,Cornelius Schröder*

Main category: cs.LG

TL;DR: 论文提出FNOPE方法，利用傅里叶神经算子和流匹配目标，高效推断函数值参数，扩展了基于模拟推理的应用范围。


<details>
  <summary>Details</summary>
Motivation: 当前基于模拟的推理方法在处理函数值参数时效率低下，限制了其在地球科学等领域的应用。

Method: 采用傅里叶神经算子架构和流匹配目标，实现高效的后验估计。

Result: FNOPE在多个基准任务和冰川学空间推断任务中表现优异，显著降低了计算成本。

Conclusion: FNOPE扩展了基于模拟推理方法的适用性，使其能够处理函数值参数，推动了科学领域的发展。

Abstract: Simulation-based inference (SBI) is an established approach for performing
Bayesian inference on scientific simulators. SBI so far works best on
low-dimensional parametric models. However, it is difficult to infer
function-valued parameters, which frequently occur in disciplines that model
spatiotemporal processes such as the climate and earth sciences. Here, we
introduce an approach for efficient posterior estimation, using a Fourier
Neural Operator (FNO) architecture with a flow matching objective. We show that
our approach, FNOPE, can perform inference of function-valued parameters at a
fraction of the simulation budget of state of the art methods. In addition,
FNOPE supports posterior evaluation at arbitrary discretizations of the domain,
as well as simultaneous estimation of vector-valued parameters. We demonstrate
the effectiveness of our approach on several benchmark tasks and a challenging
spatial inference task from glaciology. FNOPE extends the applicability of SBI
methods to new scientific domains by enabling the inference of function-valued
parameters.

</details>


### [260] [Benignity of loss landscape with weight decay requires both large overparametrization and initialization](https://arxiv.org/abs/2505.22578)
*Etienne Boursier,Matthew Bowditch,Matthias Englert,Ranko Lazic*

Main category: cs.LG

TL;DR: 论文研究了两层ReLU网络在权重衰减下的损失景观，证明在大规模过参数化下景观变得良性，但小初始化时仍可能陷入局部极小值。


<details>
  <summary>Details</summary>
Motivation: 尽管权重衰减是现代训练中的标准做法，但其理论分析仍不充分。现有研究多集中于无正则化场景，缺乏对正则化损失景观的深入理解。

Method: 通过理论分析两层ReLU网络的ℓ2正则化损失景观，探讨过参数化程度（网络宽度m与数据量n、输入维度d的关系）对景观性质的影响。

Result: 当m≳min(n^d,2^n)时，几乎所有常数激活区域都包含全局最小值且无虚假局部极小值。但该结论仅适用于大初始化场景，小初始化时仍可能收敛到虚假极小值。

Conclusion: 过参数化可使损失景观良性化，但初始化方式决定优化结果。大初始化时理论结果成立，小初始化时仍需警惕局部极小值问题。

Abstract: The optimization of neural networks under weight decay remains poorly
understood from a theoretical standpoint. While weight decay is standard
practice in modern training procedures, most theoretical analyses focus on
unregularized settings. In this work, we investigate the loss landscape of the
$\ell_2$-regularized training loss for two-layer ReLU networks. We show that
the landscape becomes benign -- i.e., free of spurious local minima -- under
large overparametrization, specifically when the network width $m$ satisfies $m
\gtrsim \min(n^d, 2^n)$, where $n$ is the number of data points and $d$ the
input dimension. More precisely in this regime, almost all constant activation
regions contain a global minimum and no spurious local minima. We further show
that this level of overparametrization is not only sufficient but also
necessary via the example of orthogonal data. Finally, we demonstrate that such
loss landscape results primarily hold relevance in the large initialization
regime. In contrast, for small initializations -- corresponding to the feature
learning regime -- optimization can still converge to spurious local minima,
despite the global benignity of the landscape.

</details>


### [261] [Machine Unlearning under Overparameterization](https://arxiv.org/abs/2505.22601)
*Jacob L. Block,Aryan Mokhtari,Sanjay Shakkottai*

Main category: cs.LG

TL;DR: 该论文研究了过参数化设置下的机器遗忘算法，提出了一种新的定义和算法框架，以解决现有方法在梯度消失时的不足。


<details>
  <summary>Details</summary>
Motivation: 在过参数化设置中，许多模型会插值数据，导致现有遗忘算法在梯度消失时失效。因此，需要新的定义和算法来解决这一问题。

Method: 论文提出将遗忘解定义为保留数据上的最小复杂度插值器，并设计了一种新的算法框架，仅需在原始解上访问保留集的模型梯度。

Result: 实验表明，该框架在不同模型类中提供了精确和近似的遗忘保证，并在各种遗忘实验中优于现有基线方法。

Conclusion: 论文提出的新定义和算法框架有效解决了过参数化设置下的机器遗忘问题，为实际应用提供了更好的解决方案。

Abstract: Machine unlearning algorithms aim to remove the influence of specific
training samples, ideally recovering the model that would have resulted from
training on the remaining data alone. We study unlearning in the
overparameterized setting, where many models interpolate the data, and defining
the unlearning solution as any loss minimizer over the retained
set$\unicode{x2013}$as in prior work in the underparameterized
setting$\unicode{x2013}$is inadequate, since the original model may already
interpolate the retained data and satisfy this condition. In this regime, loss
gradients vanish, rendering prior methods based on gradient perturbations
ineffective, motivating both new unlearning definitions and algorithms. For
this setting, we define the unlearning solution as the minimum-complexity
interpolator over the retained data and propose a new algorithmic framework
that only requires access to model gradients on the retained set at the
original solution. We minimize a regularized objective over perturbations
constrained to be orthogonal to these model gradients, a first-order relaxation
of the interpolation condition. For different model classes, we provide exact
and approximate unlearning guarantees, and we demonstrate that an
implementation of our framework outperforms existing baselines across various
unlearning experiments.

</details>


### [262] [One Rank at a Time: Cascading Error Dynamics in Sequential Learning](https://arxiv.org/abs/2505.22602)
*Mahtab Alizadeh Vandchali,Fangshuo,Liao,Anastasios Kyrillidis*

Main category: cs.LG

TL;DR: 该论文通过低秩线性回归的视角分析顺序学习中的误差传播，特别关注学习秩-1子空间时的误差累积，提出了误差传播的界限及其对算法设计和稳定性的影响。


<details>
  <summary>Details</summary>
Motivation: 顺序学习将复杂任务分解为简单、层次化的组件，已成为AI领域的重要范式。论文旨在研究顺序学习过程中误差如何传播，尤其是在有限计算资源和精度的情况下，以提升算法设计和稳定性。

Method: 论文提出一个分析框架，将学习过程分解为一系列秩-1估计问题，每个后续估计依赖于前一步的准确性，并分析误差传播的机制。

Result: 研究证明误差会以可预测的方式累积，并建立了误差对整体模型精度影响的界限，为算法设计和稳定性提供了理论支持。

Conclusion: 顺序学习中的误差传播具有可预测性，研究结果为算法设计和稳定性保证提供了重要启示。

Abstract: Sequential learning -- where complex tasks are broken down into simpler,
hierarchical components -- has emerged as a paradigm in AI. This paper views
sequential learning through the lens of low-rank linear regression, focusing
specifically on how errors propagate when learning rank-1 subspaces
sequentially. We present an analysis framework that decomposes the learning
process into a series of rank-1 estimation problems, where each subsequent
estimation depends on the accuracy of previous steps. Our contribution is a
characterization of the error propagation in this sequential process,
establishing bounds on how errors -- e.g., due to limited computational budgets
and finite precision -- affect the overall model accuracy. We prove that these
errors compound in predictable ways, with implications for both algorithmic
design and stability guarantees.

</details>


### [263] [The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models](https://arxiv.org/abs/2505.22617)
*Ganqu Cui,Yuchen Zhang,Jiacheng Chen,Lifan Yuan,Zhi Wang,Yuxin Zuo,Haozhan Li,Yuchen Fan,Huayu Chen,Weize Chen,Zhiyuan Liu,Hao Peng,Lei Bai,Wanli Ouyang,Yu Cheng,Bowen Zhou,Ning Ding*

Main category: cs.LG

TL;DR: 该论文针对强化学习在大型语言模型推理中的策略熵崩溃问题，提出了通过控制高协方差标记的更新来管理熵的方法，以促进探索并提升性能。


<details>
  <summary>Details</summary>
Motivation: 论文旨在解决强化学习在扩展推理能力时遇到的策略熵急剧下降问题，这一问题限制了模型的探索能力并导致性能饱和。

Method: 通过理论分析和实证研究，论文提出了Clip-Cov和KL-Cov两种技术，分别通过裁剪和应用KL惩罚来控制高协方差标记的更新。

Result: 实验表明，这些方法有效鼓励了探索，避免了熵崩溃，并提升了下游任务的性能。

Conclusion: 通过理解熵动态机制并实施相应的控制策略，可以显著改善强化学习在大型语言模型中的扩展性和性能。

Abstract: This paper aims to overcome a major obstacle in scaling RL for reasoning with
LLMs, namely the collapse of policy entropy. Such phenomenon is consistently
observed across vast RL runs without entropy intervention, where the policy
entropy dropped sharply at the early training stage, this diminished
exploratory ability is always accompanied with the saturation of policy
performance. In practice, we establish a transformation equation R=-a*e^H+b
between entropy H and downstream performance R. This empirical law strongly
indicates that, the policy performance is traded from policy entropy, thus
bottlenecked by its exhaustion, and the ceiling is fully predictable H=0,
R=-a+b. Our finding necessitates entropy management for continuous exploration
toward scaling compute for RL. To this end, we investigate entropy dynamics
both theoretically and empirically. Our derivation highlights that, the change
in policy entropy is driven by the covariance between action probability and
the change in logits, which is proportional to its advantage when using Policy
Gradient-like algorithms. Empirical study shows that, the values of covariance
term and entropy differences matched exactly, supporting the theoretical
conclusion. Moreover, the covariance term stays mostly positive throughout
training, further explaining why policy entropy would decrease monotonically.
Through understanding the mechanism behind entropy dynamics, we motivate to
control entropy by restricting the update of high-covariance tokens.
Specifically, we propose two simple yet effective techniques, namely Clip-Cov
and KL-Cov, which clip and apply KL penalty to tokens with high covariances
respectively. Experiments show that these methods encourage exploration, thus
helping policy escape entropy collapse and achieve better downstream
performance.

</details>


### [264] [Understanding (Un)Reliability of Steering Vectors in Language Models](https://arxiv.org/abs/2505.22637)
*Joschka Braun,Carsten Eickhoff,David Krueger,Seyed Ali Bahrainian,Dmitrii Krasheninnikov*

Main category: cs.LG

TL;DR: 研究发现，尽管导向向量能有效控制语言模型行为，但其效果受提示类型和激活差异几何形状影响，可靠性存在波动。


<details>
  <summary>Details</summary>
Motivation: 探讨导向向量在控制语言模型行为时的可靠性问题，特别是提示类型和激活差异几何形状对效果的影响。

Method: 通过实验分析七种提示类型的效果差异，并研究训练集激活差异的余弦相似度与导向效果的关系。

Result: 所有提示类型均产生正向效果但方差大，有时效果相反；余弦相似度高时导向更有效；正负激活分离好的数据集更易导向。

Conclusion: 当目标行为缺乏一致方向时，导向向量可能不可靠。提示类型和激活差异几何形状是影响导向效果的关键因素。

Abstract: Steering vectors are a lightweight method to control language model behavior
by adding a learned bias to the activations at inference time. Although
steering demonstrates promising performance, recent work shows that it can be
unreliable or even counterproductive in some cases. This paper studies the
influence of prompt types and the geometry of activation differences on
steering reliability. First, we find that all seven prompt types used in our
experiments produce a net positive steering effect, but exhibit high variance
across samples, and often give an effect opposite of the desired one. No prompt
type clearly outperforms the others, and yet the steering vectors resulting
from the different prompt types often differ directionally (as measured by
cosine similarity). Second, we show that higher cosine similarity between
training set activation differences predicts more effective steering. Finally,
we observe that datasets where positive and negative activations are better
separated are more steerable. Our results suggest that vector steering is
unreliable when the target behavior is not represented by a coherent direction.

</details>


### [265] [Spectral Survival Analysis](https://arxiv.org/abs/2505.22641)
*Chengzhi Shi,Stratis Ioannidis*

Main category: cs.LG

TL;DR: 本文提出了一种将秩回归与CoxPH模型结合的新方法，提升了大规模高维数据下的生存分析效率和预测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管CoxPH模型在生存分析中广泛应用，但在处理大规模数据集和高维问题时存在扩展性挑战。本文旨在解决这一问题。

Method: 通过建立秩回归与CoxPH模型的基本联系，扩展了谱方法在生存分析中的应用，适用于多种CoxPH变体，包括深度模型。

Result: 在多个真实世界的高维数据集上验证了方法的可扩展性，预测性能和效率均优于传统方法。

Conclusion: 该方法为大规模高维生存分析提供了高效且通用的解决方案，具有广泛的应用潜力。

Abstract: Survival analysis is widely deployed in a diverse set of fields, including
healthcare, business, ecology, etc. The Cox Proportional Hazard (CoxPH) model
is a semi-parametric model often encountered in the literature. Despite its
popularity, wide deployment, and numerous variants, scaling CoxPH to large
datasets and deep architectures poses a challenge, especially in the
high-dimensional regime. We identify a fundamental connection between rank
regression and the CoxPH model: this allows us to adapt and extend the
so-called spectral method for rank regression to survival analysis. Our
approach is versatile, naturally generalizing to several CoxPH variants,
including deep models. We empirically verify our method's scalability on
multiple real-world high-dimensional datasets; our method outperforms legacy
methods w.r.t. predictive performance and efficiency.

</details>


### [266] [On Learning Verifiers for Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.22650)
*Maria-Florina Balcan,Avrim Blum,Zhiyuan Li,Dravyansh Sharma*

Main category: cs.LG

TL;DR: 该论文提出了一种学习自然语言思维链推理验证器的方法，以解决当前LLMs在复杂问题推理中的不可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 当前思维链推理在处理复杂数学和逻辑问题时容易产生错误或未经证实的推断，而形式化数学推理又因LLMs能力不足难以实现。因此，研究如何学习可靠的验证器来评估自然语言推理步骤的有效性变得尤为重要。

Method: 论文提出了一个形式化的PAC学习框架，用于研究自然语言思维链推理的验证问题，并提出了多个不同强度的验证目标。

Result: 论文提供了学习满足这些目标的验证器的样本复杂度上限，同时也展示了在某些自然验证目标下学习的下限和不可能性结果。

Conclusion: 通过形式化框架和验证目标的研究，论文为学习可靠的思维链推理验证器提供了理论基础和方法指导。

Abstract: Chain-of-Thought reasoning has emerged as a powerful approach for solving
complex mathematical and logical problems. However, it can often veer off track
through incorrect or unsubstantiated inferences. Formal mathematical reasoning,
which can be checked with a formal verifier, is one approach to addressing this
issue. However, currently LLMs are simply not good enough to solve complex
problems in a formal way, and even just formalizing an informal problem
statement can be challenging. Motivated by this fact, in this work we consider
the problem of learning reliable verifiers for natural language
Chain-of-Thought reasoning. That is, given a problem statement and step-by-step
solution in natural language, the aim of the verifier is to output [Yes] if the
reasoning steps in the solution are all valid, and [No] otherwise. In this work
we give a formal PAC-learning framework for studying this problem. We propose
and analyze several natural verification goals, at different levels of
strength, in this framework. We provide sample complexity upper-bounds for
learning verifiers satisfying these goals, as well as lower-bound and
impossibility results for learning other natural verification objectives
without additional assumptions.

</details>


### [267] [Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents](https://arxiv.org/abs/2505.22655)
*Michael Kirchhof,Gjergji Kasneci,Enkelejda Kasneci*

Main category: cs.LG

TL;DR: 论文认为传统的不确定性二分法在LLM代理与用户交互的场景中过于局限，提出了三种新的研究方向以丰富不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）和聊天机器人代理有时会提供错误输出，且无法完全避免。传统的不确定性量化方法（如偶然性和认知性不确定性）在交互式LLM代理场景中失去意义，因此需要探索新的不确定性研究方向。

Method: 论文回顾了现有文献，指出传统不确定性定义的矛盾与局限性，并提出了三种新方向：未明确性不确定性（用户未提供完整信息）、交互式学习（通过追问减少不确定性）和输出不确定性（利用语言空间表达不确定性）。

Result: 研究发现传统不确定性定义在交互场景中不适用，提出的新方向有望提升LLM代理交互的透明性、可信度和直观性。

Conclusion: 论文呼吁研究新的不确定性量化方法，以改善LLM代理在交互场景中的表现，使其更透明、可信和直观。

Abstract: Large-language models (LLMs) and chatbot agents are known to provide wrong
outputs at times, and it was recently found that this can never be fully
prevented. Hence, uncertainty quantification plays a crucial role, aiming to
quantify the level of ambiguity in either one overall number or two numbers for
aleatoric and epistemic uncertainty. This position paper argues that this
traditional dichotomy of uncertainties is too limited for the open and
interactive setup that LLM agents operate in when communicating with a user,
and that we need to research avenues that enrich uncertainties in this novel
scenario. We review the literature and find that popular definitions of
aleatoric and epistemic uncertainties directly contradict each other and lose
their meaning in interactive LLM agent settings. Hence, we propose three novel
research directions that focus on uncertainties in such human-computer
interactions: Underspecification uncertainties, for when users do not provide
all information or define the exact task at the first go, interactive learning,
to ask follow-up questions and reduce the uncertainty about the current
context, and output uncertainties, to utilize the rich language and speech
space to express uncertainties as more than mere numbers. We expect that these
new ways of dealing with and communicating uncertainties will lead to LLM agent
interactions that are more transparent, trustworthy, and intuitive.

</details>


### [268] [Maximizing Confidence Alone Improves Reasoning](https://arxiv.org/abs/2505.22660)
*Mihir Prabhudesai,Lili Chen,Alex Ippoliti,Katerina Fragkiadaki,Hao Liu,Deepak Pathak*

Main category: cs.LG

TL;DR: 本文提出RENT方法，通过无监督强化学习利用模型熵最小化作为内在奖励，提升推理能力，无需外部奖励或真实答案。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的奖励函数设计困难，尤其在缺乏外部监督的领域。本文旨在探索无需外部奖励的无监督强化学习方法。

Method: 提出RENT方法，利用模型生成答案的分布熵作为内在奖励，通过强化高置信度的思维链来提升推理能力。

Result: 在GSM8K、MATH500等多个推理基准测试中，不同规模的Qwen和Mistral模型均显示出性能提升。

Conclusion: RENT作为一种通用无监督学习方法，适用于外部监督有限或不可用的广泛领域。

Abstract: Reinforcement learning (RL) has enabled machine learning models to achieve
significant advances in many fields. Most recently, RL has empowered frontier
language models to solve challenging math, science, and coding problems.
However, central to any RL algorithm is the reward function, and reward
engineering is a notoriously difficult problem in any domain. In this paper, we
propose RENT: Reinforcement Learning via Entropy Minimization -- a fully
unsupervised RL method that requires no external reward or ground-truth
answers, and instead uses the model's entropy of its underlying distribution as
an intrinsic reward. We find that by reinforcing the chains of thought that
yield high model confidence on its generated answers, the model improves its
reasoning ability. In our experiments, we showcase these improvements on an
extensive suite of commonly-used reasoning benchmarks, including GSM8K,
MATH500, AMC, AIME, and GPQA, and models of varying sizes from the Qwen and
Mistral families. The generality of our unsupervised learning method lends
itself to applicability in a wide range of domains where external supervision
is limited or unavailable.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [269] [Fluent but Culturally Distant: Can Regional Training Teach Cultural Understanding?](https://arxiv.org/abs/2505.21548)
*Dhruv Agarwal,Anya Shukla,Sunayana Sitaram,Aditya Vashistha*

Main category: physics.soc-ph

TL;DR: 研究发现，印度本土大语言模型在文化价值观和实践上与全球模型相比并无优势，甚至不如普通美国人更能代表印度文化。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型普遍存在西方文化倾向，许多国家开始开发本土化模型，但尚不清楚这些模型是否真正反映了当地文化。

Method: 以印度为例，评估了五个印度本土和五个全球大语言模型，通过价值观和实践两个维度进行分析。

Result: 印度本土模型在所有任务中均未显示出比全球模型更符合印度文化规范，区域微调反而可能损害文化适应能力。

Conclusion: 研究呼吁加大对文化代表性数据的投入，以构建真正具有文化主权的大语言模型。

Abstract: Large language models (LLMs) are used around the world but exhibit Western
cultural tendencies. To address this cultural misalignment, many countries have
begun developing "regional" LLMs tailored to local communities. Yet it remains
unclear whether these models merely speak the language of their users or also
reflect their cultural values and practices. Using India as a case study, we
evaluate five Indic and five global LLMs along two key dimensions: values (via
the Inglehart-Welzel map and GlobalOpinionQA) and practices (via CulturalBench
and NormAd). Across all four tasks, we find that Indic models do not align more
closely with Indian cultural norms than global models. In fact, an average
American person is a better proxy for Indian cultural values than any Indic
model. Even prompting strategies fail to meaningfully improve alignment.
Ablations show that regional fine-tuning does not enhance cultural competence
and may in fact hurt it by impeding recall of existing knowledge. We trace this
failure to the scarcity of high-quality, untranslated, and culturally grounded
pretraining and fine-tuning data. Our study positions cultural evaluation as a
first-class requirement alongside multilingual benchmarks and offers a reusable
methodology for developers. We call for deeper investments in culturally
representative data to build and evaluate truly sovereign LLMs.

</details>


### [270] [Complexity counts: global and local perspectives on Indo-Aryan numeral systems](https://arxiv.org/abs/2505.21510)
*Chundra Cathcart*

Main category: physics.soc-ph

TL;DR: 该论文分析了印欧语系印度-雅利安语支（如印地语、古吉拉特语和孟加拉语）数字系统的复杂性，探讨了其非透明性及持续存在的原因，并通过跨语言数据量化了复杂性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解为何印度-雅利安语支的数字系统（如91在印地语中无法分解为1和90）比其他语言更复杂，并探索其背后的语言和非语言因素。

Method: 方法包括使用跨语言数据库数据，开发并应用量化数字系统复杂性的指标，比较印度-雅利安语与其他语言的差异，并分析宗教、地理隔离等因素的影响。

Result: 结果显示印度-雅利安语支的数字系统整体上比其他语言更复杂，但内部存在差异；复杂性的形成与宗教、地理等因素相关，但仍符合高效交际的普遍压力。

Conclusion: 结论指出，尽管印度-雅利安语数字系统高度复杂，但其仍遵循跨语言的交际效率原则，呼吁在讨论数字系统多样性时重视这一常被忽视的复杂性维度。

Abstract: The numeral systems of Indo-Aryan languages such as Hindi, Gujarati, and
Bengali are highly unusual in that unlike most numeral systems (e.g., those of
English, Chinese, etc.), forms referring to 1--99 are highly non-transparent
and are cannot be constructed using straightforward rules. As an example,
Hindi/Urdu *iky\=anve* `91' is not decomposable into the composite elements
*ek* `one' and *nave* `ninety' in the way that its English counterpart is. This
paper situates Indo-Aryan languages within the typology of cross-linguistic
numeral systems, and explores the linguistic and non-linguistic factors that
may be responsible for the persistence of complex systems in these languages.
Using cross-linguistic data from multiple databases, we develop and employ a
number of cross-linguistically applicable metrics to quantifies the complexity
of languages' numeral systems, and demonstrate that Indo-Aryan languages have
decisively more complex numeral systems than the world's languages as a whole,
though individual Indo-Aryan languages differ from each other in terms of the
complexity of the patterns they display. We investigate the factors (e.g.,
religion, geographic isolation, etc.) that underlie complexity in numeral
systems, with a focus on South Asia, in an attempt to develop an account of why
complex numeral systems developed and persisted in certain Indo-Aryan languages
but not elsewhere. Finally, we demonstrate that Indo-Aryan numeral systems
adhere to certain general pressures toward efficient communication found
cross-linguistically, despite their high complexity. We call for this somewhat
overlooked dimension of complexity to be taken seriously when discussing
general variation in cross-linguistic numeral systems.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [271] [High-Fidelity Functional Ultrasound Reconstruction via A Visual Auto-Regressive Framework](https://arxiv.org/abs/2505.21530)
*Xuhang Chen,Zhuo Li,Yanyan Shen,Mufti Mahmud,Hieu Pham,Chi-Man Pun,Shuqiang Wang*

Main category: eess.IV

TL;DR: 功能超声(fUS)成像在神经血管映射中具有卓越的时空分辨率，但数据稀缺和颅骨信号衰减限制了其实际应用。


<details>
  <summary>Details</summary>
Motivation: 功能超声成像面临的主要挑战是数据稀缺和信号衰减，这些问题限制了数据集多样性并影响机器学习模型的公平性。

Method: 未提及具体方法。

Result: 未提及具体结果。

Conclusion: 功能超声成像的实际应用受到数据稀缺和信号衰减的严重限制。

Abstract: Functional ultrasound (fUS) imaging provides exceptional spatiotemporal
resolution for neurovascular mapping, yet its practical application is
significantly hampered by critical challenges. Foremost among these are data
scarcity, arising from ethical considerations and signal degradation through
the cranium, which collectively limit dataset diversity and compromise the
fairness of downstream machine learning models.

</details>


### [272] [STA-Risk: A Deep Dive of Spatio-Temporal Asymmetries for Breast Cancer Risk Prediction](https://arxiv.org/abs/2505.21699)
*Zhengbo Zhou,Dooman Arefan,Margarita Zuley,Jules Sumkin,Shandong Wu*

Main category: eess.IV

TL;DR: 提出了一种基于Transformer的STA-Risk模型，通过空间和时间不对称性分析提高乳腺癌风险预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的乳腺癌风险预测模型性能有限，且多基于单次检查，忽略了纵向成像检查中的细微变化。

Method: STA-Risk模型通过侧编码和时间编码学习空间-时间不对称性，并使用定制的不对称性损失进行调节。

Result: 在两个独立的乳腺X光数据集上，STA-Risk在1至5年风险预测中表现优于四种代表性SOTA模型。

Conclusion: STA-Risk模型通过捕捉乳腺X光成像的细微演变，显著提升了乳腺癌风险预测的性能。

Abstract: Predicting the risk of developing breast cancer is an important clinical tool
to guide early intervention and tailoring personalized screening strategies.
Early risk models have limited performance and recently machine learning-based
analysis of mammogram images showed encouraging risk prediction effects. These
models however are limited to the use of a single exam or tend to overlook
nuanced breast tissue evolvement in spatial and temporal details of
longitudinal imaging exams that are indicative of breast cancer risk. In this
paper, we propose STA-Risk (Spatial and Temporal Asymmetry-based Risk
Prediction), a novel Transformer-based model that captures fine-grained
mammographic imaging evolution simultaneously from bilateral and longitudinal
asymmetries for breast cancer risk prediction. STA-Risk is innovative by the
side encoding and temporal encoding to learn spatial-temporal asymmetries,
regulated by a customized asymmetry loss. We performed extensive experiments
with two independent mammogram datasets and achieved superior performance than
four representative SOTA models for 1- to 5-year future risk prediction. Source
codes will be released upon publishing of the paper.

</details>


### [273] [Privacy-Preserving Chest X-ray Report Generation via Multimodal Federated Learning with ViT and GPT-2](https://arxiv.org/abs/2505.21715)
*Md. Zahid Hossain,Mustofa Ahmed,Most. Sharmin Sultana Samu,Md. Rakibul Islam*

Main category: eess.IV

TL;DR: 该研究提出了一种基于多模态联邦学习的胸部X光报告生成框架，通过分散训练保护数据隐私，并在性能上媲美集中式模型。


<details>
  <summary>Details</summary>
Motivation: 传统集中式方法需要传输敏感数据，存在隐私风险。研究旨在开发一种隐私保护的自动化放射报告生成方法。

Method: 使用Vision Transformer编码器和GPT-2报告生成器，比较了FedAvg、Krum Aggregation和新型L-FedAvg三种联邦学习聚合策略。

Result: Krum Aggregation在ROUGE、BLEU等指标上表现最优，联邦学习模型可生成临床相关且语义丰富的报告。

Conclusion: 该轻量级隐私保护框架为医疗AI协作开发提供了新途径，且不损害数据机密性。

Abstract: The automated generation of radiology reports from chest X-ray images holds
significant promise in enhancing diagnostic workflows while preserving patient
privacy. Traditional centralized approaches often require sensitive data
transfer, posing privacy concerns. To address this, the study proposes a
Multimodal Federated Learning framework for chest X-ray report generation using
the IU-Xray dataset. The system utilizes a Vision Transformer (ViT) as the
encoder and GPT-2 as the report generator, enabling decentralized training
without sharing raw data. Three Federated Learning (FL) aggregation strategies:
FedAvg, Krum Aggregation and a novel Loss-aware Federated Averaging (L-FedAvg)
were evaluated. Among these, Krum Aggregation demonstrated superior performance
across lexical and semantic evaluation metrics such as ROUGE, BLEU, BERTScore
and RaTEScore. The results show that FL can match or surpass centralized models
in generating clinically relevant and semantically rich radiology reports. This
lightweight and privacy-preserving framework paves the way for collaborative
medical AI development without compromising data confidentiality.

</details>


### [274] [Subspecialty-Specific Foundation Model for Intelligent Gastrointestinal Pathology](https://arxiv.org/abs/2505.21928)
*Lianghui Zhu,Xitong Ling,Minxi Ouyang,Xiaoping Liu,Mingxi Fu,Tian Guan,Fanglei Fu,Xuanyu Wang,Maomao Zeng,Mingxi Zhu,Yibo Jin,Liming Liu,Song Duan,Qiming He,Yizhi Wang,Luxi Xie,Houqiang Li,Yonghong He,Sufang Tian*

Main category: eess.IV

TL;DR: 研究者开发了Digepath，一种针对胃肠道病理学的专业基础模型，通过双阶段迭代优化策略提升诊断准确性，在多项任务中表现优异，尤其在早期癌症筛查中达到99.6%的灵敏度。


<details>
  <summary>Details</summary>
Motivation: 传统胃肠道疾病诊断依赖病理学家的主观判断，存在可重复性低和诊断变异性的问题，亟需开发AI驱动的精准病理学模型以弥补这一缺陷。

Method: 采用双阶段迭代优化策略（预训练+精细筛选），基于超过35.3亿个胃肠道病理切片图像块进行预训练，专注于全切片图像中稀疏分布病变区域的检测。

Result: Digepath在34项胃肠道病理任务中的33项达到最先进水平，包括分子预测和预后评估；早期癌症筛查模块在全国9家机构实现99.6%灵敏度。

Conclusion: 该研究不仅推动了AI在胃肠道病理学的应用，其可迁移框架也为其他病理亚专科提供了范式，有望改变传统病理实践格局。

Abstract: Gastrointestinal (GI) diseases represent a clinically significant burden,
necessitating precise diagnostic approaches to optimize patient outcomes.
Conventional histopathological diagnosis, heavily reliant on the subjective
interpretation of pathologists, suffers from limited reproducibility and
diagnostic variability. To overcome these limitations and address the lack of
pathology-specific foundation models for GI diseases, we develop Digepath, a
specialized foundation model for GI pathology. Our framework introduces a
dual-phase iterative optimization strategy combining pretraining with
fine-screening, specifically designed to address the detection of sparsely
distributed lesion areas in whole-slide images. Digepath is pretrained on more
than 353 million image patches from over 200,000 hematoxylin and eosin-stained
slides of GI diseases. It attains state-of-the-art performance on 33 out of 34
tasks related to GI pathology, including pathological diagnosis, molecular
prediction, gene mutation prediction, and prognosis evaluation, particularly in
diagnostically ambiguous cases and resolution-agnostic tissue classification.We
further translate the intelligent screening module for early GI cancer and
achieve near-perfect 99.6% sensitivity across 9 independent medical
institutions nationwide. The outstanding performance of Digepath highlights its
potential to bridge critical gaps in histopathological practice. This work not
only advances AI-driven precision pathology for GI diseases but also
establishes a transferable paradigm for other pathology subspecialties.

</details>


### [275] [Taylor expansion-based Kolmogorov-Arnold network for blind image quality assessment](https://arxiv.org/abs/2505.21592)
*Ze Chen,Shaode Yu*

Main category: eess.IV

TL;DR: 本文提出TaylorKAN，通过泰勒展开作为可学习激活函数提升局部逼近能力，解决了KAN在高维特征处理中的性能与计算效率问题。


<details>
  <summary>Details</summary>
Motivation: Kolmogorov-Arnold网络（KAN）在盲图像质量评估（BIQA）中表现优异，但处理高维特征时性能提升有限且计算成本高。

Method: 提出TaylorKAN，利用泰勒展开作为可学习激活函数增强局部逼近能力，并通过网络深度缩减和特征维度压缩提高计算效率。

Result: 在五个真实失真数据库上的实验表明，TaylorKAN性能优于其他KAN相关模型，验证了其泛化能力。

Conclusion: TaylorKAN作为一种高效、鲁棒的高维分数回归模型，展现了泰勒展开局部逼近优于正交函数全局逼近的潜力。

Abstract: Kolmogorov-Arnold Network (KAN) has attracted growing interest for its strong
function approximation capability. In our previous work, KAN and its variants
were explored in score regression for blind image quality assessment (BIQA).
However, these models encounter challenges when processing high-dimensional
features, leading to limited performance gains and increased computational
cost. To address these issues, we propose TaylorKAN that leverages the Taylor
expansions as learnable activation functions to enhance local approximation
capability. To improve the computational efficiency, network depth reduction
and feature dimensionality compression are integrated into the TaylorKAN-based
score regression pipeline. On five databases (BID, CLIVE, KonIQ, SPAQ, and
FLIVE) with authentic distortions, extensive experiments demonstrate that
TaylorKAN consistently outperforms the other KAN-related models, indicating
that the local approximation via Taylor expansions is more effective than
global approximation using orthogonal functions. Its generalization capacity is
validated through inter-database experiments. The findings highlight the
potential of TaylorKAN as an efficient and robust model for high-dimensional
score regression.

</details>


### [276] [Optimizing Deep Learning for Skin Cancer Classification: A Computationally Efficient CNN with Minimal Accuracy Trade-Off](https://arxiv.org/abs/2505.21597)
*Abdullah Al Mamun,Pollob Chandra Ray,Md Rahat Ul Nasib,Akash Das,Jia Uddin,Md Nurul Absur*

Main category: eess.IV

TL;DR: 该研究提出了一种轻量级CNN模型，在保持皮肤癌分类准确率接近ResNet50的同时，大幅降低了计算开销和资源需求，适用于资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 当前基于迁移学习的皮肤癌分类模型（如ResNet50）计算开销过大，难以在资源受限的实际场景中部署。需要开发高效轻量化的替代方案。

Method: 设计定制化CNN架构，通过参数精简（从2390万降至69.2万）和FLOPs优化（从40亿降至3004万），在HAM10000数据集上进行对比实验。

Result: 模型参数量减少96.7%，FLOPs降低99.2%，准确率仅下降0.022%。推理延迟、能耗和内存占用显著优于ResNet50。

Conclusion: 轻量化CNN在精度-效率权衡中表现出色，为移动端/边缘设备的皮肤癌诊断提供了实用解决方案。

Abstract: The rapid advancement of deep learning in medical image analysis has greatly
enhanced the accuracy of skin cancer classification. However, current
state-of-the-art models, especially those based on transfer learning like
ResNet50, come with significant computational overhead, rendering them
impractical for deployment in resource-constrained environments. This study
proposes a custom CNN model that achieves a 96.7\% reduction in parameters
(from 23.9 million in ResNet50 to 692,000) while maintaining a classification
accuracy deviation of less than 0.022\%. Our empirical analysis of the HAM10000
dataset reveals that although transfer learning models provide a marginal
accuracy improvement of approximately 0.022\%, they result in a staggering
13,216.76\% increase in FLOPs, considerably raising computational costs and
inference latency. In contrast, our lightweight CNN architecture, which
encompasses only 30.04 million FLOPs compared to ResNet50's 4.00 billion,
significantly reduces energy consumption, memory footprint, and inference time.
These findings underscore the trade-off between the complexity of deep models
and their real-world feasibility, positioning our optimized CNN as a practical
solution for mobile and edge-based skin cancer diagnostics.

</details>


### [277] [Beyond 1D: Vision Transformers and Multichannel Signal Images for PPG-to-ECG Reconstruction](https://arxiv.org/abs/2505.21767)
*Xiaoyan Li,Shixin Xu,Faisal Habib,Arvind Gupta,Huaxiong Huang*

Main category: eess.IV

TL;DR: 本文提出了一种基于Vision Transformer (ViT)的四通道PPG信号图像表示方法，用于从PPG重建ECG信号，显著提升了波形特征的捕捉精度和重建效果。


<details>
  <summary>Details</summary>
Motivation: 当前基于生成模型的ECG重建方法在捕捉细粒度波形特征方面仍存在挑战，尤其是传统方法仅依赖单通道PPG信号，限制了特征提取的全面性。

Method: 采用四通道信号图像表示（原始PPG、一阶差分、二阶差分及曲线下面积），结合ViT的自注意力机制，同时建模心跳间和心跳内的依赖关系。

Result: 实验显示，该方法在PRD和RMSE指标上分别降低29%和15%，并引入临床相关新指标（如QRS区域误差）验证了其鲁棒性。

Conclusion: 四通道PPG表示与ViT的结合为循环信号分析开辟了新途径，证明了PPG作为心脏活动监测替代方案的潜力。

Abstract: Reconstructing ECG from PPG is a promising yet challenging task. While recent
advancements in generative models have significantly improved ECG
reconstruction, accurately capturing fine-grained waveform features remains a
key challenge. To address this, we propose a novel PPG-to-ECG reconstruction
method that leverages a Vision Transformer (ViT) as the core network. Unlike
conventional approaches that rely on single-channel PPG, our method employs a
four-channel signal image representation, incorporating the original PPG, its
first-order difference, second-order difference, and area under the curve. This
multi-channel design enriches feature extraction by preserving both temporal
and physiological variations within the PPG. By leveraging the self-attention
mechanism in ViT, our approach effectively captures both inter-beat and
intra-beat dependencies, leading to more robust and accurate ECG
reconstruction. Experimental results demonstrate that our method consistently
outperforms existing 1D convolution-based approaches, achieving up to 29%
reduction in PRD and 15% reduction in RMSE. The proposed approach also produces
improvements in other evaluation metrics, highlighting its robustness and
effectiveness in reconstructing ECG signals. Furthermore, to ensure a
clinically relevant evaluation, we introduce new performance metrics, including
QRS area error, PR interval error, RT interval error, and RT amplitude
difference error. Our findings suggest that integrating a four-channel signal
image representation with the self-attention mechanism of ViT enables more
effective extraction of informative PPG features and improved modeling of
beat-to-beat variations for PPG-to-ECG mapping. Beyond demonstrating the
potential of PPG as a viable alternative for heart activity monitoring, our
approach opens new avenues for cyclic signal analysis and prediction.

</details>


### [278] [Targeted Unlearning Using Perturbed Sign Gradient Methods With Applications On Medical Images](https://arxiv.org/abs/2505.21872)
*George R. Nahass,Zhu Wang,Homa Rashidisabet,Won Hwa Kim,Sasha Hubschman,Jeffrey C. Peterson,Ghasem Yazdanpanah,Chad A. Purnell,Pete Setabutr,Ann Q. Tran,Darvin Yi,Sathya N. Ravi*

Main category: eess.IV

TL;DR: 该论文提出了一种用于临床场景的机器遗忘方法，通过双层优化和边界遗忘策略，有效平衡遗忘与保留，提升模型维护效率。


<details>
  <summary>Details</summary>
Motivation: 传统机器遗忘主要关注隐私保护，而本研究将其扩展为通用模型修订工具，特别针对临床环境中常见的数据偏移、设备淘汰和政策变化问题。

Method: 采用基于边界的双层优化框架，结合可调损失函数设计控制遗忘-保留权衡，并支持多轮遗忘结果的模型组合策略。

Result: 在基准和真实临床影像数据集上，该方法在遗忘和保留指标上均优于基线，尤其在处理成像设备和解剖异常值时表现突出。

Conclusion: 该研究证实机器遗忘可作为临床应用中模块化、实用的模型维护方案，为替代完整重训练提供了可行路径。

Abstract: Machine unlearning aims to remove the influence of specific training samples
from a trained model without full retraining. While prior work has largely
focused on privacy-motivated settings, we recast unlearning as a
general-purpose tool for post-deployment model revision. Specifically, we focus
on utilizing unlearning in clinical contexts where data shifts, device
deprecation, and policy changes are common. To this end, we propose a bilevel
optimization formulation of boundary-based unlearning that can be solved using
iterative algorithms. We provide convergence guarantees when first-order
algorithms are used to unlearn. Our method introduces tunable loss design for
controlling the forgetting-retention tradeoff and supports novel model
composition strategies that merge the strengths of distinct unlearning runs.
Across benchmark and real-world clinical imaging datasets, our approach
outperforms baselines on both forgetting and retention metrics, including
scenarios involving imaging devices and anatomical outliers. This work
establishes machine unlearning as a modular, practical alternative to
retraining for real-world model maintenance in clinical applications.

</details>


### [279] [High Volume Rate 3D Ultrasound Reconstruction with Diffusion Models](https://arxiv.org/abs/2505.22090)
*Tristan S. W. Stevens,Oisín Nolan,Oudom Somphone,Jean-Luc Robert,Ruud J. G. van Sloun*

Main category: eess.IV

TL;DR: 该论文提出了一种基于扩散模型（DMs）的新型3D超声重建方法，通过减少仰角平面的数量来提高空间和时间分辨率，并在图像质量和下游任务性能上优于传统和基于深度学习的插值方法。


<details>
  <summary>Details</summary>
Motivation: 3D超声能够实时可视化解剖结构，但高体积率和高图像质量难以同时实现。现有的3D发散波技术虽然能提供高体积率，但图像质量受限。因此，需要一种新的方法来平衡体积率和图像质量。

Method: 论文采用扩散模型（DMs）从减少的仰角平面集中重建3D超声图像，利用超声序列的时间一致性加速推理，并通过扩散后验采样的概率性质量化重建不确定性。

Result: 实验结果表明，基于扩散模型的重建方法在图像质量和下游任务性能上均优于传统和基于深度学习的插值方法，且在强子采样下对分布外数据的召回率有所提升。

Conclusion: 该研究提出的基于扩散模型的3D超声重建方法在提高图像质量和时间分辨率方面表现出色，为3D超声成像提供了一种有效的解决方案。

Abstract: Three-dimensional ultrasound enables real-time volumetric visualization of
anatomical structures. Unlike traditional 2D ultrasound, 3D imaging reduces the
reliance on precise probe orientation, potentially making ultrasound more
accessible to clinicians with varying levels of experience and improving
automated measurements and post-exam analysis. However, achieving both high
volume rates and high image quality remains a significant challenge. While 3D
diverging waves can provide high volume rates, they suffer from limited tissue
harmonic generation and increased multipath effects, which degrade image
quality. One compromise is to retain the focusing in elevation while leveraging
unfocused diverging waves in the lateral direction to reduce the number of
transmissions per elevation plane. Reaching the volume rates achieved by full
3D diverging waves, however, requires dramatically undersampling the number of
elevation planes. Subsequently, to render the full volume, simple interpolation
techniques are applied. This paper introduces a novel approach to 3D ultrasound
reconstruction from a reduced set of elevation planes by employing diffusion
models (DMs) to achieve increased spatial and temporal resolution. We compare
both traditional and supervised deep learning-based interpolation methods on a
3D cardiac ultrasound dataset. Our results show that DM-based reconstruction
consistently outperforms the baselines in image quality and downstream task
performance. Additionally, we accelerate inference by leveraging the temporal
consistency inherent to ultrasound sequences. Finally, we explore the
robustness of the proposed method by exploiting the probabilistic nature of
diffusion posterior sampling to quantify reconstruction uncertainty and
demonstrate improved recall on out-of-distribution data with synthetic
anomalies under strong subsampling.

</details>


### [280] [Chest Disease Detection In X-Ray Images Using Deep Learning Classification Method](https://arxiv.org/abs/2505.22609)
*Alanna Hazlett,Naomi Ohashi,Timothy Rodriguez,Sodiq Adewole*

Main category: eess.IV

TL;DR: 该研究利用迁移学习和预训练CNN模型对胸部X光片进行分类，涵盖COVID-19、肺炎、结核和正常病例，并通过Grad-CAM提升模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过深度学习技术提高胸部X光片的分类准确性，以辅助临床诊断COVID-19、肺炎和结核等疾病。

Method: 采用迁移学习技术，微调预训练的卷积神经网络（CNN）模型，并使用Grad-CAM提供分类决策的可视化解释。

Result: 初步结果显示高准确率和优异的分类性能指标（如精确率、召回率和F1分数）。

Conclusion: 该方法在胸部X光片分类中表现出色，结合Grad-CAM增强了模型的临床可信度和透明度。

Abstract: In this work, we investigate the performance across multiple classification
models to classify chest X-ray images into four categories of COVID-19,
pneumonia, tuberculosis (TB), and normal cases. We leveraged transfer learning
techniques with state-of-the-art pre-trained Convolutional Neural Networks
(CNNs) models. We fine-tuned these pre-trained architectures on a labeled
medical x-ray images. The initial results are promising with high accuracy and
strong performance in key classification metrics such as precision, recall, and
F1 score. We applied Gradient-weighted Class Activation Mapping (Grad-CAM) for
model interpretability to provide visual explanations for classification
decisions, improving trust and transparency in clinical applications.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [281] [Preventing Adversarial AI Attacks Against Autonomous Situational Awareness: A Maritime Case Study](https://arxiv.org/abs/2505.21609)
*Mathew J. Walter,Aaron Barrett,Kimberly Tam*

Main category: cs.CR

TL;DR: 本文提出了一种名为DFCR的新方法，通过多输入和数据融合增强AI系统对抗对抗性攻击的韧性，显著降低了多种攻击的损失。


<details>
  <summary>Details</summary>
Motivation: 对抗性AI攻击对依赖AI的自主交通系统（如海事船只）构成重大威胁。传统防御方法范围有限，安全指标不足，且需在模型层面之外建立韧性。

Method: 提出数据融合网络韧性（DFCR）方法，利用多输入和数据融合构建防御组件，并开发AI安全指标。通过实际演示和定量分析评估DFCR方法。

Result: DFCR方法显著提升了海事自主系统对抗对抗性机器学习攻击的韧性，成功降低了多方位扰动攻击、对抗性补丁攻击和欺骗攻击的损失。

Conclusion: DFCR方法为开发更安全、更具韧性的AI驱动系统提供了新途径，即使在典型防御被攻破时也能提升决策能力。

Abstract: Adversarial artificial intelligence (AI) attacks pose a significant threat to
autonomous transportation, such as maritime vessels, that rely on AI
components. Malicious actors can exploit these systems to deceive and
manipulate AI-driven operations. This paper addresses three critical research
challenges associated with adversarial AI: the limited scope of traditional
defences, inadequate security metrics, and the need to build resilience beyond
model-level defences. To address these challenges, we propose building defences
utilising multiple inputs and data fusion to create defensive components and an
AI security metric as a novel approach toward developing more secure AI
systems. We name this approach the Data Fusion Cyber Resilience (DFCR) method,
and we evaluate it through real-world demonstrations and comprehensive
quantitative analyses, comparing a system built with the DFCR method against
single-input models and models utilising existing state-of-the-art defences.
The findings show that the DFCR approach significantly enhances resilience
against adversarial machine learning attacks in maritime autonomous system
operations, achieving up to a 35\% reduction in loss for successful
multi-pronged perturbation attacks, up to a 100\% reduction in loss for
successful adversarial patch attacks and up to 100\% reduction in loss for
successful spoofing attacks when using these more resilient systems. We
demonstrate how DFCR and DFCR confidence scores can reduce adversarial AI
contact confidence and improve decision-making by the system, even when typical
adversarial defences have been compromised. Ultimately, this work contributes
to the development of more secure and resilient AI-driven systems against
adversarial attacks.

</details>


### [282] [VideoMarkBench: Benchmarking Robustness of Video Watermarking](https://arxiv.org/abs/2505.21620)
*Zhengyuan Jiang,Moyang Guo,Kecen Li,Yuepeng Hu,Yupu Wang,Zhicong Huang,Cheng Hong,Neil Zhenqiang Gong*

Main category: cs.CR

TL;DR: 论文提出了VideoMarkBench，首个系统性评估视频水印在去除和伪造攻击下鲁棒性的基准测试，揭示了现有方法的显著脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着视频生成模型的快速发展，高度逼真的合成视频引发了与虚假信息和版权侵权相关的伦理问题。视频水印作为一种缓解策略被提出，但其在常见和对抗性扰动下的鲁棒性尚未充分研究。

Method: 研究引入了VideoMarkBench，包含由三种先进视频生成模型生成的统一数据集，涵盖三种视频风格、四种水印方法和七种检测聚合策略，评估了12种扰动类型在白盒、黑盒和无盒威胁模型下的表现。

Result: 研究发现当前水印方法存在显著漏洞，亟需更鲁棒的解决方案。

Conclusion: VideoMarkBench揭示了现有视频水印技术的脆弱性，强调了开发更鲁棒水印解决方案的紧迫性。

Abstract: The rapid development of video generative models has led to a surge in highly
realistic synthetic videos, raising ethical concerns related to disinformation
and copyright infringement. Recently, video watermarking has been proposed as a
mitigation strategy by embedding invisible marks into AI-generated videos to
enable subsequent detection. However, the robustness of existing video
watermarking methods against both common and adversarial perturbations remains
underexplored. In this work, we introduce VideoMarkBench, the first systematic
benchmark designed to evaluate the robustness of video watermarks under
watermark removal and watermark forgery attacks. Our study encompasses a
unified dataset generated by three state-of-the-art video generative models,
across three video styles, incorporating four watermarking methods and seven
aggregation strategies used during detection. We comprehensively evaluate 12
types of perturbations under white-box, black-box, and no-box threat models.
Our findings reveal significant vulnerabilities in current watermarking
approaches and highlight the urgent need for more robust solutions. Our code is
available at https://github.com/zhengyuan-jiang/VideoMarkBench.

</details>


### [283] [The Feasibility of Topic-Based Watermarking on Academic Peer Reviews](https://arxiv.org/abs/2505.21636)
*Alexander Nemecek,Yuzhou Jiang,Erman Ayday*

Main category: cs.CR

TL;DR: 该论文评估了一种名为主题水印（TBW）的轻量级技术，用于在LLM生成的文本中嵌入可检测信号，以解决同行评审中LLM使用的问题。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）在学术工作流程中的广泛应用，其在同行评审中的使用仍被禁止，主要出于对保密性、内容虚构和评估不一致的担忧。因此，需要一种可靠的归属机制来维护评审过程的完整性。

Method: 研究评估了主题水印（TBW）技术，这是一种轻量级、语义感知的方法，旨在LLM生成的文本中嵌入可检测信号。研究在多种LLM配置（包括基础、少样本和微调变体）上进行了全面评估，并使用了来自学术会议的真实同行评审数据。

Result: 结果表明，TBW在保持评审质量的同时，对基于改写的规避表现出很强的鲁棒性。

Conclusion: TBW作为一种最小侵入性和实用的解决方案，在同行评审中强制执行LLM使用具有可行性。

Abstract: Large language models (LLMs) are increasingly integrated into academic
workflows, with many conferences and journals permitting their use for tasks
such as language refinement and literature summarization. However, their use in
peer review remains prohibited due to concerns around confidentiality breaches,
hallucinated content, and inconsistent evaluations. As LLM-generated text
becomes more indistinguishable from human writing, there is a growing need for
reliable attribution mechanisms to preserve the integrity of the review
process. In this work, we evaluate topic-based watermarking (TBW), a
lightweight, semantic-aware technique designed to embed detectable signals into
LLM-generated text. We conduct a comprehensive assessment across multiple LLM
configurations, including base, few-shot, and fine-tuned variants, using
authentic peer review data from academic conferences. Our results show that TBW
maintains review quality relative to non-watermarked outputs, while
demonstrating strong robustness to paraphrasing-based evasion. These findings
highlight the viability of TBW as a minimally intrusive and practical solution
for enforcing LLM usage in peer review.

</details>


### [284] [A Joint Reconstruction-Triplet Loss Autoencoder Approach Towards Unseen Attack Detection in IoV Networks](https://arxiv.org/abs/2505.21703)
*Julia Boone,Tolunay Seyfi,Fatemeh Afghah*

Main category: cs.CR

TL;DR: 论文提出了一种基于无监督自编码器的方法，用于检测车联网（IoV）系统中的未知攻击，通过结合重构损失和三重边际损失训练模型，在多个数据集上实现了高准确率。


<details>
  <summary>Details</summary>
Motivation: 车联网系统因其高度互联性面临严重的安全威胁，传统安全机制难以应对复杂多变的网络攻击，尤其是未知攻击类型。

Method: 采用无监督自编码器方法，完全基于良性网络数据训练，结合重构损失和三重边际损失，构建良性数据的多样化表示。

Result: 在两个不同应用领域（工业物联网和家庭物联网）的数据集上，模型对良性数据的准确率约为99%，对异常数据的检测性能在97%至100%之间。

Conclusion: 该方法不仅能有效检测未知攻击，还通过迁移学习展示了跨领域适应性，为车联网安全提供了可靠解决方案。

Abstract: Internet of Vehicles (IoV) systems, while offering significant advancements
in transportation efficiency and safety, introduce substantial security
vulnerabilities due to their highly interconnected nature. These dynamic
systems produce massive amounts of data between vehicles, infrastructure, and
cloud services and present a highly distributed framework with a wide attack
surface. In considering network-centered attacks on IoV systems, attacks such
as Denial-of-Service (DoS) can prohibit the communication of essential physical
traffic safety information between system elements, illustrating that the
security concerns for these systems go beyond the traditional confidentiality,
integrity, and availability concerns of enterprise systems. Given the
complexity and volume of data generated by IoV systems, traditional security
mechanisms are often inadequate for accurately detecting sophisticated and
evolving cyberattacks. Here, we present an unsupervised autoencoder method
trained entirely on benign network data for the purpose of unseen attack
detection in IoV networks. We leverage a weighted combination of reconstruction
and triplet margin loss to guide the autoencoder training and develop a diverse
representation of the benign training set. We conduct extensive experiments on
recent network intrusion datasets from two different application domains,
industrial IoT and home IoT, that represent the modern IoV task. We show that
our method performs robustly for all unseen attack types, with roughly 99%
accuracy on benign data and between 97% and 100% performance on anomaly data.
We extend these results to show that our model is adaptable through the use of
transfer learning, achieving similarly high results while leveraging domain
features from one domain to another.

</details>


### [285] [Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models](https://arxiv.org/abs/2505.22271)
*Yongcan Yu,Yanbo Wang,Ran He,Jian Liang*

Main category: cs.CR

TL;DR: 论文提出了一种名为TIM的通用防御框架，通过自适应和自我进化的方式抵御多种越狱攻击，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的防御方法通常针对特定类型的越狱攻击，难以应对多样化的对抗策略，因此需要一种更通用的防御方案。

Method: TIM框架通过训练核心标记进行高效检测，并在推理时识别越狱行为，随后进行安全微调，同时解耦检测模块以避免性能下降。

Result: 在大型语言模型和多模态语言模型上的广泛实验表明，TIM能有效抵御多种越狱攻击。

Conclusion: TIM作为一种通用防御框架，能够自适应地应对多样化的越狱攻击，并通过自我进化提升防御能力。

Abstract: While (multimodal) large language models (LLMs) have attracted widespread
attention due to their exceptional capabilities, they remain vulnerable to
jailbreak attacks. Various defense methods are proposed to defend against
jailbreak attacks, however, they are often tailored to specific types of
jailbreak attacks, limiting their effectiveness against diverse adversarial
strategies. For instance, rephrasing-based defenses are effective against text
adversarial jailbreaks but fail to counteract image-based attacks. To overcome
these limitations, we propose a universal defense framework, termed Test-time
IMmunization (TIM), which can adaptively defend against various jailbreak
attacks in a self-evolving way. Specifically, TIM initially trains a gist token
for efficient detection, which it subsequently applies to detect jailbreak
activities during inference. When jailbreak attempts are identified, TIM
implements safety fine-tuning using the detected jailbreak instructions paired
with refusal answers. Furthermore, to mitigate potential performance
degradation in the detector caused by parameter updates during safety
fine-tuning, we decouple the fine-tuning process from the detection module.
Extensive experiments on both LLMs and multimodal LLMs demonstrate the efficacy
of TIM.

</details>


### [286] [SimProcess: High Fidelity Simulation of Noisy ICS Physical Processes](https://arxiv.org/abs/2505.22638)
*Denis Donadel,Gabriele Crestanello,Giulio Morandini,Daniele Antonioli,Mauro Conti,Massimo Merro*

Main category: cs.CR

TL;DR: 本文提出SimProcess框架，通过机器学习评估工业控制系统(ICS)模拟的真实性，提升蜜罐的防御能力。


<details>
  <summary>Details</summary>
Motivation: 工业控制系统(ICS)对关键基础设施至关重要，但现有蜜罐难以真实模拟物理过程，易被攻击者识破。

Method: 提出SimProcess框架，利用随机森林等模型分析时间序列数据，量化模拟系统与真实系统的噪声分布差异。

Result: 在EPIC电网数据测试中，模型召回率达1.0，确定高斯混合分布和自编码器能最佳模拟电力系统噪声。

Conclusion: 该框架仅需实时数据即可工作，能有效提升复杂动态系统的蜜罐真实性，代码已开源。

Abstract: Industrial Control Systems (ICS) manage critical infrastructures like power
grids and water treatment plants. Cyberattacks on ICSs can disrupt operations,
causing severe economic, environmental, and safety issues. For example,
undetected pollution in a water plant can put the lives of thousands at stake.
ICS researchers have increasingly turned to honeypots -- decoy systems designed
to attract attackers, study their behaviors, and eventually improve defensive
mechanisms. However, existing ICS honeypots struggle to replicate the ICS
physical process, making them susceptible to detection. Accurately simulating
the noise in ICS physical processes is challenging because different factors
produce it, including sensor imperfections and external interferences.
  In this paper, we propose SimProcess, a novel framework to rank the fidelity
of ICS simulations by evaluating how closely they resemble real-world and noisy
physical processes. It measures the simulation distance from a target system by
estimating the noise distribution with machine learning models like Random
Forest. Unlike existing solutions that require detailed mathematical models or
are limited to simple systems, SimProcess operates with only a timeseries of
measurements from the real system, making it applicable to a broader range of
complex dynamic systems. We demonstrate the framework's effectiveness through a
case study using real-world power grid data from the EPIC testbed. We compare
the performance of various simulation methods, including static and generative
noise techniques. Our model correctly classifies real samples with a recall of
up to 1.0. It also identifies Gaussian and Gaussian Mixture as the best
distribution to simulate our power systems, together with a generative solution
provided by an autoencoder, thereby helping developers to improve honeypot
fidelity. Additionally, we make our code publicly available.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [287] [Towards Efficient Key-Value Cache Management for Prefix Prefilling in LLM Inference](https://arxiv.org/abs/2505.21919)
*Yue Zhu,Hao Yu,Chen Wang,Zhuoran Liu,Eun Kyung Lee*

Main category: cs.ET

TL;DR: 论文探讨了针对大语言模型(LLM)的键值缓存(KVC)管理优化问题，分析了现有系统的不足并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型上下文窗口的扩展，高效的键值缓存管理对推理性能优化至关重要。现有系统缺乏针对KVC预填充的专用解决方案，影响了RAG等场景的缓存重用效率。

Method: 通过分析真实KVC访问模式，评估Redis等商业KV存储及RDMA系统(CHIME/Sherman)在KVC元数据管理中的表现。

Result: 研究发现现有系统无法满足LLM工作负载需求，凸显了需要优化元数据管理的分布式缓存系统。

Conclusion: 需要设计新型KVC管理系统以实现可扩展、低延迟的LLM推理，为系统优化提供了设计启示。

Abstract: The increasing adoption of large language models (LLMs) with extended context
windows necessitates efficient Key-Value Cache (KVC) management to optimize
inference performance. Inference workloads like Retrieval-Augmented Generation
(RAG) and agents exhibit high cache reusability, making efficient caching
critical to reducing redundancy and improving speed. We analyze real-world KVC
access patterns using publicly available traces and evaluate commercial
key-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1]
and Sherman [2]) for KVC metadata management. Our work demonstrates the lack of
tailored storage solution for KVC prefilling, underscores the need for an
efficient distributed caching system with optimized metadata management for LLM
workloads, and provides insights into designing improved KVC management systems
for scalable, low-latency inference.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [288] [OpenReview Should be Protected and Leveraged as a Community Asset for Research in the Era of Large Language Models](https://arxiv.org/abs/2505.21537)
*Hao Sun,Yunyi Shen,Mihaela van der Schaar*

Main category: cs.CY

TL;DR: 本文主张利用OpenReview作为核心社区资源，以提升大语言模型时代的研究质量、可扩展性和责任感，支持开放基准测试和对齐研究。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型时代，高质量、领域丰富且持续更新的数据集具有重要价值。OpenReview作为一个持续演化的研究论文、同行评审和专家讨论的数据库，可以成为推动研究的核心资产。

Method: 通过分析OpenReview的潜力，提出三个关键应用方向：提升同行评审的质量和可扩展性；构建基于专家讨论的开放基准测试；支持对齐研究。

Result: OpenReview能够为研究社区提供独特的贡献，包括增强评审透明度、支持开放基准测试和促进伦理对齐研究。

Conclusion: 建议社区共同探索OpenReview的标准化基准和使用指南，推动负责任的数据使用和集体管理。

Abstract: In the era of large language models (LLMs), high-quality, domain-rich, and
continuously evolving datasets capturing expert-level knowledge, core human
values, and reasoning are increasingly valuable. This position paper argues
that OpenReview -- the continually evolving repository of research papers, peer
reviews, author rebuttals, meta-reviews, and decision outcomes -- should be
leveraged more broadly as a core community asset for advancing research in the
era of LLMs. We highlight three promising areas in which OpenReview can
uniquely contribute: enhancing the quality, scalability, and accountability of
peer review processes; enabling meaningful, open-ended benchmarks rooted in
genuine expert deliberation; and supporting alignment research through
real-world interactions reflecting expert assessment, intentions, and
scientific values. To better realize these opportunities, we suggest the
community collaboratively explore standardized benchmarks and usage guidelines
around OpenReview, inviting broader dialogue on responsible data use, ethical
considerations, and collective stewardship.

</details>


### [289] [Enhancing Selection of Climate Tech Startups with AI -- A Case Study on Integrating Human and AI Evaluations in the ClimaTech Great Global Innovation Challenge](https://arxiv.org/abs/2505.21562)
*Jennifer Turliuk,Alejandro Sevilla,Daniela Gorza,Tod Hynes*

Main category: cs.CY

TL;DR: 该案例研究探讨了ClimaTech全球创新挑战赛如何通过结合人类与AI评估来筛选气候科技初创企业，以提高选拔过程的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过混合模型减少偏见并提升决策质量，展示数据驱动方法在风投领域的潜力。

Method: 采用三阶段方法：AI初筛、人类评委半决赛（屏蔽AI分数）、混合权重决赛（人类83.3%+AI16.7%）。

Result: AI与人类评分呈中等正相关（Spearman=0.47），决赛四强同时被AI高分评价，体现两者互补性。

Conclusion: 混合模型能优化初创企业评估，ClimaTech方案为未来竞赛提供了人机协同的可行框架。

Abstract: This case study examines the ClimaTech Great Global Innovation Challenge's
approach to selecting climate tech startups by integrating human and AI
evaluations. The competition aimed to identify top startups and enhance the
accuracy and efficiency of the selection process through a hybrid model.
Research shows data-driven approaches help VC firms reduce bias and improve
decision-making. Machine learning models have outperformed human investors in
deal screening, helping identify high-potential startups. Incorporating AI
aimed to ensure more equitable and objective evaluations.
  The methodology included three phases: initial AI review, semi-finals judged
by humans, and finals using a hybrid weighting. In phase one, 57 applications
were scored by an AI tool built with StackAI and OpenAI's GPT-4o, and the top
36 advanced. In the semi-finals, human judges, unaware of AI scores, evaluated
startups on team quality, market potential, and technological innovation. Each
score - human or AI - was weighted equally, resulting in 75 percent human and
25 percent AI influence. In the finals, with five human judges, weighting
shifted to 83.3 percent human and 16.7 percent AI. There was a moderate
positive correlation between AI and human scores - Spearman's = 0.47 -
indicating general alignment with key differences. Notably, the final four
startups, selected mainly by humans, were among those rated highest by the AI.
This highlights the complementary nature of AI and human judgment. The study
shows that hybrid models can streamline and improve startup assessments. The
ClimaTech approach offers a strong framework for future competitions by
combining human expertise with AI capabilities.

</details>


### [290] [Beyond Explainability: The Case for AI Validation](https://arxiv.org/abs/2505.21570)
*Dalit Ken-Dror Feldman,Daniel Benoliel*

Main category: cs.CY

TL;DR: 论文主张将验证作为AI监管的核心支柱，提出基于有效性和可解释性的分类法，并设计了一个前瞻性政策框架。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在关键领域的决策中日益不透明，而主要依赖可解释性的监管方法存在不足，需要更实用的替代方案。

Method: 通过构建有效性与可解释性的二维分类法，对比分析欧盟、美国、英国和中国的监管模式。

Result: 验证机制能提升社会信任与安全性，尤其在可解释性受限的高风险场景中。提出了部署前后验证、第三方审计等政策框架。

Conclusion: 以验证为核心的监管框架能在保障创新的同时，为高性能不透明AI系统提供负责任的社会整合路径。

Abstract: Artificial Knowledge (AK) systems are transforming decision-making across
critical domains such as healthcare, finance, and criminal justice. However,
their growing opacity presents governance challenges that current regulatory
approaches, focused predominantly on explainability, fail to address
adequately. This article argues for a shift toward validation as a central
regulatory pillar. Validation, ensuring the reliability, consistency, and
robustness of AI outputs, offers a more practical, scalable, and risk-sensitive
alternative to explainability, particularly in high-stakes contexts where
interpretability may be technically or economically unfeasible. We introduce a
typology based on two axes, validity and explainability, classifying AK systems
into four categories and exposing the trade-offs between interpretability and
output reliability. Drawing on comparative analysis of regulatory approaches in
the EU, US, UK, and China, we show how validation can enhance societal trust,
fairness, and safety even where explainability is limited. We propose a
forward-looking policy framework centered on pre- and post-deployment
validation, third-party auditing, harmonized standards, and liability
incentives. This framework balances innovation with accountability and provides
a governance roadmap for responsibly integrating opaque, high-performing AK
systems into society.

</details>


### [291] [AITEE -- Agentic Tutor for Electrical Engineering](https://arxiv.org/abs/2505.21582)
*Christopher Knievel,Alexander Bernhardt,Christian Bernhardt*

Main category: cs.CY

TL;DR: AITEE是一个基于代理的电气工程智能辅导系统，结合大语言模型和电路重建技术，提供个性化学习支持，显著提升领域知识应用能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在电气工程基础领域虽有一定知识，但对具体电路问题的处理能力不足，需要开发更专业的辅导系统以满足学生多样化需求。

Method: 系统整合了电路重建、图相似度检索、Spice仿真和苏格拉底式对话，通过检索增强生成技术提供精准辅导。

Result: 实验表明AITEE在领域知识应用上显著优于基线方法，中等规模LLM即可实现良好性能。

Conclusion: 智能辅导系统有望为电气工程教育提供可扩展、个性化且高效的学习环境。

Abstract: Intelligent tutoring systems combined with large language models offer a
promising approach to address students' diverse needs and promote
self-efficacious learning. While large language models possess good
foundational knowledge of electrical engineering basics, they remain
insufficiently capable of addressing specific questions about electrical
circuits. In this paper, we present AITEE, an agent-based tutoring system for
electrical engineering designed to accompany students throughout their learning
process, offer individualized support, and promote self-directed learning.
AITEE supports both hand-drawn and digital circuits through an adapted circuit
reconstruction process, enabling natural interaction with students. Our novel
graph-based similarity measure identifies relevant context from lecture
materials through a retrieval augmented generation approach, while parallel
Spice simulation further enhances accuracy in applying solution methodologies.
The system implements a Socratic dialogue to foster learner autonomy through
guided questioning. Experimental evaluations demonstrate that AITEE
significantly outperforms baseline approaches in domain-specific knowledge
application, with even medium-sized LLM models showing acceptable performance.
Our results highlight the potential of agentic tutors to deliver scalable,
personalized, and effective learning environments for electrical engineering
education.

</details>


### [292] [Public Discourse Sandbox: Facilitating Human and AI Digital Communication Research](https://arxiv.org/abs/2505.21604)
*Kristina Radivojevic,Caleb Reinking,Shaun Whitfield,Paul Brenner*

Main category: cs.CY

TL;DR: 论文提出公共话语沙盒(PDS)平台，用于安全研究AI行为及定制化AI参与者的影响，解决社交媒体数据获取困难和伦理问题。


<details>
  <summary>Details</summary>
Motivation: 社交媒体是研究全球事件和社区讨论的重要资源，但数据获取困难、存在虚假内容且涉及伦理问题，需要更可控的研究机制。

Method: 开发公共话语沙盒(PDS)平台，支持人机及机机话语研究，采用提示工程、检索增强生成(RAG)和微调等技术。

Result: PDS为研究者提供安全实验环境，开源代码促进社区协作，支持定制化AI行为研究。

Conclusion: PDS填补了社交媒体研究的空白，为AI行为分析提供可控、可扩展的实验平台。

Abstract: Social media serves as a primary communication and information dissemination
platform for major global events, entertainment, and niche or topically focused
community discussions. Therefore, it represents a valuable resource for
researchers who aim to understand numerous questions. However, obtaining data
can be difficult, expensive, and often unreliable due to the presence of bots,
fake accounts, and manipulated content. Additionally, there are ethical
concerns if researchers decide to conduct an online experiment without
explicitly notifying social media users about their intent. There is a need for
more controlled and scalable mechanisms to evaluate the impacts of digital
discussion interventions on audiences. We introduce the Public Discourse
Sandbox (PDS), which serves as a digital discourse research platform for
human-AI as well as AI-AI discourse research, testing, and training. PDS
provides a safe and secure space for research experiments that are not viable
on public, commercial social media platforms. Its main purpose is to enable the
understanding of AI behaviors and the impacts of customized AI participants via
techniques such as prompt engineering, retrieval-augmented generation (RAG),
and fine-tuning. We provide a hosted live version of the sandbox to support
researchers as well as the open-sourced code on GitHub for community
collaboration and contribution.

</details>


### [293] [Expert Survey: AI Reliability & Security Research Priorities](https://arxiv.org/abs/2505.21664)
*Joe O'Brien,Jeremy Dolan,Jay Kim,Jonah Dykhuizen,Jeba Sania,Sebastian Becker,Jam Kraprayoon,Cara Labrador*

Main category: cs.CY

TL;DR: 53位专家调查显示，AI可靠性与安全研究需优先投入，以量化数据指导战略投资决策。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统向人类水平能力发展，亟需可靠性及安全研究以确保其益处安全普及并避免严重危害。

Method: 对105个AI可靠性与安全研究领域进行专家调查，首次建立全面分类并量化研究方向的优先级。

Result: 产出基于数据驱动的AI安全研究影响力排名，为资源分配提供证据支持。

Conclusion: 该研究为战略性地部署AI可靠性与安全研发资源提供了数据化决策依据。

Abstract: Our survey of 53 specialists across 105 AI reliability and security research
areas identifies the most promising research prospects to guide strategic AI
R&D investment. As companies are seeking to develop AI systems with broadly
human-level capabilities, research on reliability and security is urgently
needed to ensure AI's benefits can be safely and broadly realized and prevent
severe harms. This study is the first to quantify expert priorities across a
comprehensive taxonomy of AI safety and security research directions and to
produce a data-driven ranking of their potential impact. These rankings may
support evidence-based decisions about how to effectively deploy resources
toward AI reliability and security research.

</details>


### [294] [Responsible Data Stewardship: Generative AI and the Digital Waste Problem](https://arxiv.org/abs/2505.21720)
*Vanessa Utz*

Main category: cs.CY

TL;DR: 本文探讨生成式AI系统产生的数字垃圾问题，提出将其纳入AI伦理框架，并借鉴其他领域方法提出解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的普及，合成数据的大量产生导致数字垃圾问题日益严重，但相关研究尚不充分。本文旨在填补这一空白，将环境可持续性纳入AI伦理考量。

Method: 通过分析其他学科的数字资源管理方法，识别可迁移到AI领域的技术，并提出具体建议。

Result: 提出了涵盖研究方向、技术干预和文化转变的具体建议，以减轻无限数据存储对环境的影响。

Conclusion: 将数字垃圾问题纳入AI伦理框架，有助于构建更全面的伦理体系，考虑生成式AI系统的全生命周期影响。

Abstract: As generative AI systems become widely adopted, they enable unprecedented
creation levels of synthetic data across text, images, audio, and video
modalities. While research has addressed the energy consumption of model
training and inference, a critical sustainability challenge remains
understudied: digital waste. This term refers to stored data that consumes
resources without serving a specific (and/or immediate) purpose. This paper
presents this terminology in the AI context and introduces digital waste as an
ethical imperative within (generative) AI development, positioning
environmental sustainability as core for responsible innovation. Drawing from
established digital resource management approaches, we examine how other
disciplines manage digital waste and identify transferable approaches for the
AI community. We propose specific recommendations encompassing re-search
directions, technical interventions, and cultural shifts to mitigate the
environmental consequences of in-definite data storage. By expanding AI ethics
beyond immediate concerns like bias and privacy to include inter-generational
environmental justice, this work contributes to a more comprehensive ethical
framework that considers the complete lifecycle impact of generative AI
systems.

</details>


### [295] [From prosthetic memory to prosthetic denial: Auditing whether large language models are prone to mass atrocity denialism](https://arxiv.org/abs/2505.21753)
*Roberto Ulloa,Eve M. Zucker,Daniel Bultmann,David J. Simon,Mykola Makhortykh*

Main category: cs.CY

TL;DR: 研究探讨大型语言模型（LLMs）如何影响历史暴行记忆的传播与再现，揭示其可能强化历史否认主义的风险。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的普及可能改变历史叙事的传播方式，作者希望探究其对暴行记忆再现的影响，尤其是可能导致的记忆扭曲或否认。

Method: 通过对比审计五种LLMs（Claude、GPT、Llama、Mixtral和Gemini），在四个历史案例（乌克兰大饥荒、大屠杀、柬埔寨种族灭绝和卢旺达种族灭绝）中测试模型对否认主义论调的响应。

Result: LLMs对广为人知的事件（如大屠杀）回答准确，但对较少被记录的事件（如柬埔寨种族灭绝）易受否认主义框架影响，表现出不一致性。

Conclusion: LLMs虽扩展了‘替代记忆’的概念，但其未受监管的使用可能强化历史否认主义，对数字记忆保存构成伦理挑战。

Abstract: The proliferation of large language models (LLMs) can influence how
historical narratives are disseminated and perceived. This study explores the
implications of LLMs' responses on the representation of mass atrocity memory,
examining whether generative AI systems contribute to prosthetic memory, i.e.,
mediated experiences of historical events, or to what we term "prosthetic
denial," the AI-mediated erasure or distortion of atrocity memories. We argue
that LLMs function as interfaces that can elicit prosthetic memories and,
therefore, act as experiential sites for memory transmission, but also
introduce risks of denialism, particularly when their outputs align with
contested or revisionist narratives. To empirically assess these risks, we
conducted a comparative audit of five LLMs (Claude, GPT, Llama, Mixtral, and
Gemini) across four historical case studies: the Holodomor, the Holocaust, the
Cambodian Genocide, and the genocide against the Tutsis in Rwanda. Each model
was prompted with questions addressing common denialist claims in English and
an alternative language relevant to each case (Ukrainian, German, Khmer, and
French). Our findings reveal that while LLMs generally produce accurate
responses for widely documented events like the Holocaust, significant
inconsistencies and susceptibility to denialist framings are observed for more
underrepresented cases like the Cambodian Genocide. The disparities highlight
the influence of training data availability and the probabilistic nature of LLM
responses on memory integrity. We conclude that while LLMs extend the concept
of prosthetic memory, their unmoderated use risks reinforcing historical
denialism, raising ethical concerns for (digital) memory preservation, and
potentially challenging the advantageous role of technology associated with the
original values of prosthetic memory.

</details>


### [296] [CiRL: Open-Source Environments for Reinforcement Learning in Circular Economy and Net Zero](https://arxiv.org/abs/2505.21536)
*Federico Zocco,Andrea Corti,Monica Malvezzi*

Main category: cs.CY

TL;DR: 本文介绍了CiRL，一个专注于固体和流体材料循环利用的深度强化学习库，旨在应对气候变化和关键材料供应不确定性。


<details>
  <summary>Details</summary>
Motivation: 随着现代社会对有限原材料需求的持续增长，短期内减少碳排放的解决方案尚不可行，实现净零排放目标极具挑战性。循环经济（CE）模式被视为应对气候变化和关键材料供应不确定性的潜在解决方案。

Method: 通过热力学材料网络的形式化方法，将深度强化学习（DRL）整合到材料循环设计中，基于Stable-Baselines3这一先进的Python DRL算法库，并在Google Colaboratory中开发，以提高跨学科研究者的可访问性。

Result: 开发了CiRL库，专注于材料循环利用，采用状态空间形式，适用于动态系统分析和控制设计，并公开可用。

Conclusion: CiRL为循环经济研究提供了一个强大的工具，通过深度强化学习促进材料循环利用，助力实现可持续发展目标。

Abstract: The demand of finite raw materials will keep increasing as they fuel modern
society. Simultaneously, solutions for stopping carbon emissions in the short
term are not available, thus making the net zero target extremely challenging
to achieve at scale. The circular economy (CE) paradigm is gaining attention as
a solution to address climate change and the uncertainties of supplies of
critical materials. Hence, in this paper, we introduce CiRL, a deep
reinforcement learning (DRL) library of environments focused on the circularity
of both solid and fluid materials. The integration of DRL into the design of
material circularity is possible thanks to the formalism of thermodynamical
material networks, which is underpinned by compartmental dynamical
thermodynamics. Along with the focus on circularity, this library has three
more features: the new CE-oriented environments are in the state-space form,
which is typically used in dynamical systems analysis and control designs; it
is based on a state-of-the-art Python library of DRL algorithms, namely,
Stable-Baselines3; and it is developed in Google Colaboratory to be accessible
to researchers from different disciplines and backgrounds as is often the case
for circular economy researchers and engineers. CiRL is publicly available.

</details>


### [297] [From Coders to Critics: Empowering Students through Peer Assessment in the Age of AI Copilots](https://arxiv.org/abs/2505.22093)
*Santiago Berrezueta-Guzman,Stephan Krusche,Stefan Wagner*

Main category: cs.CY

TL;DR: 该论文探讨了在AI编程助手普及背景下，如何通过结构化同伴评估改进编程教育评估方法，并通过实证研究表明同伴评审能较准确地模拟教师评分并提升学生参与度。


<details>
  <summary>Details</summary>
Motivation: 随着ChatGPT等AI编程助手的快速普及，编程教育的评估方式、学术诚信和技能发展面临挑战。传统评分方法易受AI辅助抄袭影响，因此需要寻找替代方案，结构化同伴评估成为一种有前景的策略。

Method: 论文在一门大型编程入门课程中实施了基于量规的匿名同伴评审流程。学生互评最终项目（2D游戏），并通过相关性、平均绝对误差和均方根误差（RMSE）将学生评分与教师评分进行比较。此外，47个团队的反思调查收集了学生对公平性、评分行为和成绩汇总偏好的看法。

Result: 结果显示，同伴评审能较准确地模拟教师评分（中等精度），同时提升了学生的参与度、评估思维和提供反馈的积极性。

Conclusion: 研究表明，结构化同伴评估是应对AI辅助编程时代挑战的有效策略，可用于设计可扩展且可信的评估系统。

Abstract: The rapid adoption of AI powered coding assistants like ChatGPT and other
coding copilots is transforming programming education, raising questions about
assessment practices, academic integrity, and skill development. As educators
seek alternatives to traditional grading methods susceptible to AI enabled
plagiarism, structured peer assessment could be a promising strategy. This
paper presents an empirical study of a rubric based, anonymized peer review
process implemented in a large introductory programming course.
  Students evaluated each other's final projects (2D game), and their
assessments were compared to instructor grades using correlation, mean absolute
error, and root mean square error (RMSE). Additionally, reflective surveys from
47 teams captured student perceptions of fairness, grading behavior, and
preferences regarding grade aggregation. Results show that peer review can
approximate instructor evaluation with moderate accuracy and foster student
engagement, evaluative thinking, and interest in providing good feedback to
their peers. We discuss these findings for designing scalable, trustworthy peer
assessment systems to face the age of AI assisted coding.

</details>


### [298] [New Tools are Needed for Tracking Adherence to AI Model Behavioral Use Clauses](https://arxiv.org/abs/2505.22287)
*Daniel McDuff,Tim Korjakow,Kevin Klyman,Danish Contractor*

Main category: cs.CY

TL;DR: 论文探讨了基础模型对AI的变革性影响，分析了AI许可证的采用趋势，并提出了跟踪许可证采用和遵守情况的工具需求。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的快速发展，基础模型的强大能力带来了潜在的滥用风险。为了确保AI技术的负责任使用，研究分析了AI许可证的采用趋势及其配置，并探讨了如何通过工具来跟踪和确保这些许可证的有效性。

Method: 研究创建并部署了一个自定义的AI许可证生成器，用于促进许可证的创建，并对300多个定制许可证进行了定量和定性分析。同时，还分析了HuggingFace模型中心上的170万个模型许可证。

Result: 研究结果显示，这些许可证的采用率在增加，人们对支持许可证创建的工具表现出兴趣，并且在常见条款配置上呈现出趋同。

Conclusion: 论文认为，跟踪这些许可证的采用和遵守情况的工具是确保其实现负责任使用目标的自然且紧迫的下一步。

Abstract: Foundation models have had a transformative impact on AI. A combination of
large investments in research and development, growing sources of digital data
for training, and architectures that scale with data and compute has led to
models with powerful capabilities. Releasing assets is fundamental to
scientific advancement and commercial enterprise. However, concerns over
negligent or malicious uses of AI have led to the design of mechanisms to limit
the risks of the technology. The result has been a proliferation of licenses
with behavioral-use clauses and acceptable-use-policies that are increasingly
being adopted by commonly used families of models (Llama, Gemma, Deepseek) and
a myriad of smaller projects. We created and deployed a custom AI licenses
generator to facilitate license creation and have quantitatively and
qualitatively analyzed over 300 customized licenses created with this tool.
Alongside this we analyzed 1.7 million models licenses on the HuggingFace model
hub. Our results show increasing adoption of these licenses, interest in tools
that support their creation and a convergence on common clause configurations.
In this paper we take the position that tools for tracking adoption of, and
adherence to, these licenses is the natural next step and urgently needed in
order to ensure they have the desired impact of ensuring responsible use.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [299] [Locking-Free Training of Physics-Informed Neural Network for Solving Nearly Incompressible Elasticity Equations](https://arxiv.org/abs/2505.21994)
*Josef Dick,Seungchan Ko,Kassem Mustapha,Sanghyeon Park*

Main category: math.NA

TL;DR: 论文提出了一种基于物理信息神经网络（PINNs）的鲁棒方法，用于解决近不可压缩材料的线性弹性方程，通过分解方程和同时求解正反问题来缓解系数不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统的低阶有限元方法在处理近不可压缩材料的弹性方程时，随着Lamé系数λ趋近于无穷大或泊松比ν趋近于1/2，精度会显著下降，这种现象被称为锁定或非鲁棒性。尽管已有大量研究，但该问题仍未完全解决。

Method: 论文采用了一种基于机器学习的全新方法，利用物理信息神经网络（PINNs）来分解给定的弹性方程，以缓解系数极端不平衡的问题，并同时求解正问题和反问题，以恢复分解系统的解及相关外部条件。

Result: 通过一系列数值实验（包括常数、变量和参数化的Lamé系数），验证了所提方法的有效性。

Conclusion: 该方法为解决近不可压缩材料的弹性方程提供了一种鲁棒且高效的解决方案，展示了机器学习在传统计算力学问题中的潜力。

Abstract: Due to divergence instability, the accuracy of low-order conforming finite
element methods for nearly incompressible homogeneous elasticity equations
deteriorates as the Lam\'e coefficient $\lambda\to\infty$, or equivalently as
the Poisson ratio $\nu\to1/2$. This phenomenon, known as locking or
non-robustness, remains not fully understood despite extensive investigation.
In this paper, we propose a robust method based on a fundamentally different,
machine-learning-driven approach. Leveraging recently developed
Physics-Informed Neural Networks (PINNs), we address the numerical solution of
linear elasticity equations governing nearly incompressible materials. The core
idea of our method is to appropriately decompose the given equations to
alleviate the extreme imbalance in the coefficients, while simultaneously
solving both the forward and inverse problems to recover the solutions of the
decomposed systems as well as the associated external conditions. Through
various numerical experiments, including constant, variable and parametric
Lam\'e coefficients, we illustrate the efficiency of the proposed methodology.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [300] [Genetic Influences on Brain Aging: Analyzing Sex Differences in the UK Biobank using Structural MRI](https://arxiv.org/abs/2505.20344)
*Karen Ardila,Aashka Mohite,Abdoljalil Addeh,Amanda V. Tyndall,Cindy K. Barha,Quan Long,M. Ethan MacDonald*

Main category: q-bio.GN

TL;DR: 研究发现男女大脑衰老轨迹存在差异，女性与神经递质运输和线粒体应激反应相关，男性则与免疫和炎症相关基因更密切。


<details>
  <summary>Details</summary>
Motivation: 探索男女大脑衰老差异背后的遗传因素，以促进针对性的抗衰老干预措施。

Method: 利用英国生物银行40,940名参与者的结构MRI和基因分型数据，进行性别分层全基因组关联研究(GWAS)和后续分析。

Result: 女性大脑衰老与神经递质运输和线粒体应激反应基因相关，男性则主要涉及免疫和炎症相关基因。GMNC和OSTN等基因在两种性别中均与脑容量相关。

Conclusion: 性别分层研究对理解大脑衰老至关重要，并为个性化干预年龄相关认知衰退提供了遗传靶点。

Abstract: Brain aging trajectories differ between males and females, yet the genetic
factors underlying these differences remain underexplored. Using structural MRI
and genotyping data from 40,940 UK Biobank participants (aged 45-83), we
computed Brain Age Gap Estimates (BrainAGE) for total brain, hippocampal, and
ventricular volumes. We conducted sex-stratified genome-wide association
studies (GWAS) and Post-GWAS analyses to identify genetic variants associated
with accelerated brain aging. Distinct gene sets emerged by sex: in females,
neurotransmitter transport and mitochondrial stress response genes were
implicated; in males, immune and inflammation-related genes dominated. Shared
genes, including GMNC and OSTN, were consistently linked to brain volumes
across sexes, suggesting core roles in neurostructural maintenance. Tissue
expression analyses revealed sex-specific enrichment in pathways tied to
neurodegeneration. These findings highlight the importance of sex-stratified
approaches in aging research and suggest genetic targets for personalized
interventions against age-related cognitive decline.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [301] [RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with Global Illumination](https://arxiv.org/abs/2505.21925)
*Chong Zeng,Yue Dong,Pieter Peers,Hongzhi Wu,Xin Tong*

Main category: cs.GR

TL;DR: RenderFormer是一种基于Transformer的神经渲染管道，无需针对每个场景进行训练或微调，可直接从三角形场景表示渲染出具有全局光照效果的图像。


<details>
  <summary>Details</summary>
Motivation: 传统渲染方法通常需要复杂的物理模拟或针对特定场景的优化，RenderFormer旨在通过序列到序列的转换简化渲染流程，同时保留全局光照效果。

Method: 采用两阶段Transformer架构：视图无关阶段建模三角形间的光传输，视图相关阶段将光线束转换为像素值。两者均基于Transformer，学习时仅需极少先验约束。

Result: 在形状和光传输复杂度各异的场景中验证了RenderFormer的有效性，展示了其直接渲染全局光照的能力。

Conclusion: RenderFormer通过数据驱动方式实现了高效神经渲染，为图形学提供了无需物理模拟的新思路。

Abstract: We present RenderFormer, a neural rendering pipeline that directly renders an
image from a triangle-based representation of a scene with full global
illumination effects and that does not require per-scene training or
fine-tuning. Instead of taking a physics-centric approach to rendering, we
formulate rendering as a sequence-to-sequence transformation where a sequence
of tokens representing triangles with reflectance properties is converted to a
sequence of output tokens representing small patches of pixels. RenderFormer
follows a two stage pipeline: a view-independent stage that models
triangle-to-triangle light transport, and a view-dependent stage that
transforms a token representing a bundle of rays to the corresponding pixel
values guided by the triangle-sequence from the view-independent stage. Both
stages are based on the transformer architecture and are learned with minimal
prior constraints. We demonstrate and evaluate RenderFormer on scenes with
varying complexity in shape and light transport.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [302] [VietASR: Achieving Industry-level Vietnamese ASR with 50-hour labeled data and Large-Scale Speech Pretraining](https://arxiv.org/abs/2505.21527)
*Jianheng Zhuo,Yifan Yang,Yiwen Shao,Yong Xu,Dong Yu,Kai Yu,Xie Chen*

Main category: eess.AS

TL;DR: VietASR提出了一种利用大量无标签数据和少量标签数据的高效自动语音识别训练流程，显著提升了低资源语言（如越南语）的ASR性能。


<details>
  <summary>Details</summary>
Motivation: 当前自动语音识别（ASR）系统在低资源语言（如越南语）上表现不足，且依赖大量标注数据，训练成本高、延迟大、可访问性差。

Method: 通过多轮基于ASR偏好的自监督学习，在大规模无标签数据上进行预训练，并结合少量标签数据进行微调。

Result: 实验表明，仅用50小时标签数据和7万小时无标签数据训练的轻量模型，性能优于Whisper Large-v3和商业ASR系统。

Conclusion: VietASR为低资源ASR提供了一种经济高效的解决方案，代码和模型将开源以促进相关研究。

Abstract: Automatic speech recognition (ASR) has made remarkable progress but heavily
relies on large-scale labeled data, which is scarce for low-resource languages
like Vietnamese. While existing systems such as Whisper, USM, and MMS achieve
promising performance, their efficacy remains inadequate in terms of training
costs, latency, and accessibility. To address these issues, we propose VietASR,
a novel ASR training pipeline that leverages vast amounts of unlabeled data and
a small set of labeled data. Through multi-iteration ASR-biased self-supervised
learning on a large-scale unlabeled dataset, VietASR offers a cost-effective
and practical solution for enhancing ASR performance. Experiments demonstrate
that pre-training on 70,000-hour unlabeled data and fine-tuning on merely
50-hour labeled data yield a lightweight but powerful ASR model. It outperforms
Whisper Large-v3 and commercial ASR systems on real-world data. Our code and
models will be open-sourced to facilitate research in low-resource ASR.

</details>


### [303] [WhisperD: Dementia Speech Recognition and Filler Word Detection with Whisper](https://arxiv.org/abs/2505.21551)
*Emmanuel Akinrintoyo,Nadine Abdelhalim,Nicole Salomons*

Main category: eess.AS

TL;DR: Whisper模型在转录痴呆症患者语音时表现不佳，但通过微调显著提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 痴呆症患者（PwDs）的语音常有不规则模式和中断，标准训练的Whisper模型对此类语音转录效果差，而准确转录对诊断和辅助技术开发至关重要。

Method: 使用开源痴呆症语音数据集（DementiaBank）和内部数据集对Whisper进行微调，包括填充词以评估填充词包含率（FIR）和F1分数。

Result: 微调后的模型显著优于原版，中等规模模型的词错误率（WER）降至0.24，且对未见过的数据和语音模式具有良好泛化能力。

Conclusion: 微调Whisper模型能有效提升对痴呆症患者语音的转录准确率，具有实际应用潜力。

Abstract: Whisper fails to correctly transcribe dementia speech because persons with
dementia (PwDs) often exhibit irregular speech patterns and disfluencies such
as pauses, repetitions, and fragmented sentences. It was trained on standard
speech and may have had little or no exposure to dementia-affected speech.
However, correct transcription is vital for dementia speech for cost-effective
diagnosis and the development of assistive technology. In this work, we
fine-tune Whisper with the open-source dementia speech dataset (DementiaBank)
and our in-house dataset to improve its word error rate (WER). The fine-tuning
also includes filler words to ascertain the filler inclusion rate (FIR) and F1
score. The fine-tuned models significantly outperformed the off-the-shelf
models. The medium-sized model achieved a WER of 0.24, outperforming previous
work. Similarly, there was a notable generalisability to unseen data and speech
patterns.

</details>


### [304] [Evaluation of LLMs in Speech is Often Flawed: Test Set Contamination in Large Language Models for Speech Recognition](https://arxiv.org/abs/2505.22251)
*Yuan Tseng,Titouan Parcollet,Rogier van Dalen,Shucong Zhang,Sourav Bhattacharya*

Main category: eess.AS

TL;DR: 研究发现LibriSpeech和Common Voice评估集数据污染影响LLM性能评估，需使用独立数据验证。


<details>
  <summary>Details</summary>
Motivation: 现有研究声称大语言模型(LLM)提升语音任务性能，但未考虑评估数据集可能已被污染的问题。

Method: 比较使用污染/未污染数据训练的LLM，分析语音识别系统的错误率和输出概率分布差异。

Result: 污染数据导致LLM更易生成训练见过的句子，语音识别错误率差异细微但对已知转录赋予更高概率。

Conclusion: 微量数据污染即可导致LLM输出偏差，强调必须使用独立数据评估基于LLM的语音系统。

Abstract: Recent work suggests that large language models (LLMs) can improve
performance of speech tasks compared to existing systems. To support their
claims, results on LibriSpeech and Common Voice are often quoted. However, this
work finds that a substantial amount of the LibriSpeech and Common Voice
evaluation sets appear in public LLM pretraining corpora. This calls into
question the reliability of findings drawn from these two datasets. To measure
the impact of contamination, LLMs trained with or without contamination are
compared, showing that a contaminated LLM is more likely to generate test
sentences it has seen during training. Speech recognisers using contaminated
LLMs shows only subtle differences in error rates, but assigns significantly
higher probabilities to transcriptions seen during training. Results show that
LLM outputs can be biased by tiny amounts of data contamination, highlighting
the importance of evaluating LLM-based speech systems with held-out data.

</details>


### [305] [Analysis and Evaluation of Synthetic Data Generation in Speech Dysfluency Detection](https://arxiv.org/abs/2505.22029)
*Jinming Zhang,Xuanru Zhou,Jiachen Lian,Shuhe Li,William Li,Zoe Ezzes,Rian Bogley,Lisa Wauters,Zachary Miller,Jet Vonk,Brittany Morin,Maria Gorno-Tempini,Gopala Anumanchipalli*

Main category: eess.AS

TL;DR: 提出LLM-Dys数据集，通过LLM增强的模拟方法生成更自然的语音不流畅数据，并基于此改进端到端检测框架，取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有语音不流畅检测方法受限于高质量标注数据稀缺，且现有合成数据存在韵律不自然和语境多样性不足的问题。

Method: 构建LLM-Dys数据集（涵盖11类词/音素级不流畅模式），并开发端到端检测框架。

Result: 实验验证达到最先进性能，所有数据、模型和代码已开源。

Conclusion: LLM-Dys解决了合成数据的核心缺陷，为临床诊断和语言评估提供了有效工具。

Abstract: Speech dysfluency detection is crucial for clinical diagnosis and language
assessment, but existing methods are limited by the scarcity of high-quality
annotated data. Although recent advances in TTS model have enabled synthetic
dysfluency generation, existing synthetic datasets suffer from unnatural
prosody and limited contextual diversity. To address these limitations, we
propose LLM-Dys -- the most comprehensive dysfluent speech corpus with
LLM-enhanced dysfluency simulation. This dataset captures 11 dysfluency
categories spanning both word and phoneme levels. Building upon this resource,
we improve an end-to-end dysfluency detection framework. Experimental
validation demonstrates state-of-the-art performance. All data, models, and
code are open-sourced at https://github.com/Berkeley-Speech-Group/LLM-Dys.

</details>


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [306] [Symbolic Foundation Regressor on Complex Networks](https://arxiv.org/abs/2505.21879)
*Weiting Liu,Jiaxu Cui,Jiao Hu,En Wang,Bo Yang*

Main category: cs.SC

TL;DR: 本文提出了一种预训练的符号基础回归器，能高效压缩多变量交互数据并生成可解释的物理表示，在符号回归和网络动力学推断中表现优异，效率提升三倍。


<details>
  <summary>Details</summary>
Motivation: 科学领域不仅需要预测，还需理解预测背后的可解释模型。传统人工发现科学规律的过程复杂耗时，机器学习技术能有效简化这一过程，帮助洞察现代科学中的基本问题。

Method: 引入预训练的符号基础回归器，用于压缩复杂数据并生成可解释的物理表示，测试涵盖非网络符号回归、复杂网络上的符号回归及多领域网络动力学推断。

Result: 模型在方程推断效率上显著提升，比基线方法高效三倍且预测准确。应用于全球疫情数据时，能发现更直观的交互传播规律并实现最优数据拟合。

Conclusion: 该模型将预训练符号回归的应用扩展到复杂网络，为揭示复杂现象背后的隐藏机制提供了基础解决方案，增强了可解释性并促进科学发现。

Abstract: In science, we are interested not only in forecasting but also in
understanding how predictions are made, specifically what the interpretable
underlying model looks like. Data-driven machine learning technology can
significantly streamline the complex and time-consuming traditional manual
process of discovering scientific laws, helping us gain insights into
fundamental issues in modern science. In this work, we introduce a pre-trained
symbolic foundation regressor that can effectively compress complex data with
numerous interacting variables while producing interpretable physical
representations. Our model has been rigorously tested on non-network symbolic
regression, symbolic regression on complex networks, and the inference of
network dynamics across various domains, including physics, biochemistry,
ecology, and epidemiology. The results indicate a remarkable improvement in
equation inference efficiency, being three times more effective than baseline
approaches while maintaining accurate predictions. Furthermore, we apply our
model to uncover more intuitive laws of interaction transmission from global
epidemic outbreak data, achieving optimal data fitting. This model extends the
application boundary of pre-trained symbolic regression models to complex
networks, and we believe it provides a foundational solution for revealing the
hidden mechanisms behind changes in complex phenomena, enhancing
interpretability, and inspiring further scientific discoveries.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [307] [Is Your LLM Overcharging You? Tokenization, Transparency, and Incentives](https://arxiv.org/abs/2505.21627)
*Ander Artola Velasco,Stratis Tsirtsis,Nastaran Okati,Manuel Gomez-Rodriguez*

Main category: cs.GT

TL;DR: 当前按token计费的大语言模型服务存在提供商虚报token数量多收费的风险，论文提出了一种按字符计费的替代方案以消除这种动机。


<details>
  <summary>Details</summary>
Motivation: 现有云服务按token计费的模式可能导致提供商虚报token数量牟利，而用户无法验证其真实性，这暴露了当前定价机制的漏洞。

Method: 通过理论分析证明虚报token的可行性，提出启发式算法实现隐蔽多收费，并设计实验验证；最终提出按输出字符数计费的激励相容方案。

Result: 实验表明提供商可通过算法隐蔽虚报15-20%的token量；按字符计费方案能完全消除虚报动机，且与token计费呈强相关性（R²>0.95）。

Conclusion: 按token计费存在根本性缺陷，需转向字符计费等透明机制来保障公平性，这对AI服务定价设计具有重要启示。

Abstract: State-of-the-art large language models require specialized hardware and
substantial energy to operate. As a consequence, cloud-based services that
provide access to large language models have become very popular. In these
services, the price users pay for an output provided by a model depends on the
number of tokens the model uses to generate it -- they pay a fixed price per
token. In this work, we show that this pricing mechanism creates a financial
incentive for providers to strategize and misreport the (number of) tokens a
model used to generate an output, and users cannot prove, or even know, whether
a provider is overcharging them. However, we also show that, if an unfaithful
provider is obliged to be transparent about the generative process used by the
model, misreporting optimally without raising suspicion is hard. Nevertheless,
as a proof-of-concept, we introduce an efficient heuristic algorithm that
allows providers to significantly overcharge users without raising suspicion,
highlighting the vulnerability of users under the current pay-per-token pricing
mechanism. Further, to completely eliminate the financial incentive to
strategize, we introduce a simple incentive-compatible token pricing mechanism.
Under this mechanism, the price users pay for an output provided by a model
depends on the number of characters of the output -- they pay a fixed price per
character. Along the way, to illustrate and complement our theoretical results,
we conduct experiments with several large language models from the
$\texttt{Llama}$, $\texttt{Gemma}$ and $\texttt{Ministral}$ families, and input
prompts from the LMSYS Chatbot Arena platform.

</details>


### [308] [Online Fair Division for Personalized $2$-Value Instances](https://arxiv.org/abs/2505.22174)
*Georgios Amanatidis,Alexandros Lolos,Evangelos Markakis,Victor Turmel*

Main category: cs.GT

TL;DR: 该论文研究了在线公平分配问题，提出了一种确定性算法，在限制性估值函数下实现最坏情况保证，并探讨了未来信息对结果的影响。


<details>
  <summary>Details</summary>
Motivation: 在线公平分配问题在无限制条件下存在强不可能性结果，因此论文转向研究受限的估值函数实例，以寻求可行的解决方案。

Method: 论文提出了一种确定性算法，通过维护代理人的优先级系统，并在特定条件下利用未来信息，设计基于匹配的算法。

Result: 算法在每一步保持1/(2n-1)-MMS分配，最终达到1/4-MMS分配；利用未来信息可实现EF1和EF2分配。

Conclusion: 论文首次在受限的加性实例中实现了非平凡的公平性保证，为在线公平分配问题提供了新的解决方案。

Abstract: We study an online fair division setting, where goods arrive one at a time
and there is a fixed set of $n$ agents, each of whom has an additive valuation
function over the goods. Once a good appears, the value each agent has for it
is revealed and it must be allocated immediately and irrevocably to one of the
agents. It is known that without any assumptions about the values being
severely restricted or coming from a distribution, very strong impossibility
results hold in this setting. To bypass the latter, we turn our attention to
instances where the valuation functions are restricted. In particular, we study
personalized $2$-value instances, where there are only two possible values each
agent may have for each good, possibly different across agents, and we show how
to obtain worst case guarantees with respect to well-known fairness notions,
such as maximin share fairness and envy-freeness up to one (or two) good(s). We
suggest a deterministic algorithm that maintains a $1/(2n-1)$-MMS allocation at
every time step and show that this is the best possible any deterministic
algorithm can achieve if one cares about every single time step; nevertheless,
eventually the allocation constructed by our algorithm becomes a $1/4$-MMS
allocation. To achieve this, the algorithm implicitly maintains a fragile
system of priority levels for all agents. Further, we show that, by allowing
some limited access to future information, it is possible to have stronger
results with less involved approaches. By knowing the values of goods for $n-1$
time steps into the future, we design a matching-based algorithm that achieves
an EF$1$ allocation every $n$ time steps, while always maintaining an EF$2$
allocation. Finally, we show that our results allow us to get the first
nontrivial guarantees for additive instances in which the ratio of the maximum
over the minimum value an agent has for a good is bounded.

</details>


### [309] [Strengthening Proportionality in Temporal Voting](https://arxiv.org/abs/2505.22513)
*Bradley Phillips,Edith Elkind,Nicholas Teh,Tomasz Wąs*

Main category: cs.GT

TL;DR: 该论文研究了时间投票框架下的比例代表制，提出了超越现有比例代表概念（如EJR）的更强变体，并探讨了这些新概念的存在性和相互关系。


<details>
  <summary>Details</summary>
Motivation: 现有研究已将多赢家选举中的比例代表概念（如JR、PJR、EJR）扩展到时间投票框架，但缺乏对这些概念的进一步强化和扩展。本文旨在填补这一空白。

Method: 论文引入了JR、PJR和EJR的更强变体，并提出了时间投票框架下的新公理（如EJR+、FJR、FPJR和Core），研究了这些概念的存在性和相互关系。

Result: 研究发现，EJR+和FJR等新公理在强化EJR的同时，仍能在所有时间选举中满足，从而建立了一个丰富的比例代表概念层次结构。

Conclusion: 论文通过引入和验证新的比例代表公理，扩展了时间投票框架下的理论体系，为未来研究提供了更丰富的分析工具。

Abstract: We study proportional representation in the framework of temporal voting with
approval ballots. Prior work adapted basic proportional representation concepts
-- justified representation (JR), proportional JR (PJR), and extended JR (EJR)
-- from the multiwinner setting to the temporal setting. Our work introduces
and examines ways of going beyond EJR. Specifically, we consider stronger
variants of JR, PJR, and EJR, and introduce temporal adaptations of more
demanding multiwinner axioms, such as EJR+, full JR (FJR), full proportional JR
(FPJR), and the Core. For each of these concepts, we investigate its existence
and study its relationship to existing notions, thereby establishing a rich
hierarchy of proportionality concepts. Notably, we show that two of our
proposed axioms -- EJR+ and FJR -- strengthen EJR while remaining satisfiable
in every temporal election.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [310] [Enhancing Vision Transformer Explainability Using Artificial Astrocytes](https://arxiv.org/abs/2505.21513)
*Nicolas Echevarrieta-Catalan,Ana Ribas-Rodriguez,Francisco Cedron,Odelia Schwartz,Vanessa Aguiar-Pulido*

Main category: cs.CV

TL;DR: 论文提出ViTA模型，通过引入人工星形胶质细胞增强预训练模型的解释性，使其更符合人类认知。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习模型精度高但解释性差，且模型复杂度增加会进一步降低解释性。现有方法改进有限，需新思路提升解释性。

Method: 提出ViTA模型，受神经科学启发，在预训练视觉Transformer中引入人工星形胶质细胞，无需重新训练即可生成更人类对齐的解释。

Result: 实验表明，ViTA在Grad-CAM等解释技术上显著提升热图与人类标注的一致性，所有指标均有统计学意义改进。

Conclusion: 人工星形胶质细胞的引入能有效增强模型解释性与人类认知的对齐，为可解释AI提供新方向。

Abstract: Machine learning models achieve high precision, but their decision-making
processes often lack explainability. Furthermore, as model complexity
increases, explainability typically decreases. Existing efforts to improve
explainability primarily involve developing new eXplainable artificial
intelligence (XAI) techniques or incorporating explainability constraints
during training. While these approaches yield specific improvements, their
applicability remains limited. In this work, we propose the Vision Transformer
with artificial Astrocytes (ViTA). This training-free approach is inspired by
neuroscience and enhances the reasoning of a pretrained deep neural network to
generate more human-aligned explanations. We evaluated our approach employing
two well-known XAI techniques, Grad-CAM and Grad-CAM++, and compared it to a
standard Vision Transformer (ViT). Using the ClickMe dataset, we quantified the
similarity between the heatmaps produced by the XAI techniques and a
(human-aligned) ground truth. Our results consistently demonstrate that
incorporating artificial astrocytes enhances the alignment of model
explanations with human perception, leading to statistically significant
improvements across all XAI techniques and metrics utilized.

</details>


### [311] [Do DeepFake Attribution Models Generalize?](https://arxiv.org/abs/2505.21520)
*Spiros Baxavanakis,Manos Schinas,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: 本文探讨了DeepFake检测中的二元模型与多类模型在跨数据集泛化能力上的差异，研究了归因模型在未知数据集上的准确性，并评估了对比方法对提升泛化性能的效果。


<details>
  <summary>Details</summary>
Motivation: 随着DeepFake生成技术的进步和开源工具的普及，合成媒体的制作门槛大幅降低，严重威胁在线信息的真实性和公众对机构的信任。现有研究多集中于二元检测模型，但忽视了不同篡改方法引入的独特痕迹，归因模型的研究较少却在实际应用中至关重要。

Method: 研究利用五种先进的骨干模型，在六个DeepFake数据集上进行了广泛实验。比较了二元与多类模型的跨数据集泛化能力，检验了归因模型对未知数据集中已知篡改方法的检测准确性，并评估了对比方法在提升泛化性能方面的效果。

Result: 研究发现，二元模型展现出更好的泛化能力，而更大的模型、对比方法和更高质量的数据可以提升归因模型的性能。

Conclusion: 尽管二元模型在泛化能力上表现更优，但通过采用更大模型、对比方法和提高数据质量，归因模型的性能可以得到显著提升，这对于增强终端用户对检测结果的信任和可解释性具有重要意义。

Abstract: Recent advancements in DeepFake generation, along with the proliferation of
open-source tools, have significantly lowered the barrier for creating
synthetic media. This trend poses a serious threat to the integrity and
authenticity of online information, undermining public trust in institutions
and media. State-of-the-art research on DeepFake detection has primarily
focused on binary detection models. A key limitation of these models is that
they treat all manipulation techniques as equivalent, despite the fact that
different methods introduce distinct artifacts and visual cues. Only a limited
number of studies explore DeepFake attribution models, although such models are
crucial in practical settings. By providing the specific manipulation method
employed, these models could enhance both the perceived trustworthiness and
explainability for end users. In this work, we leverage five state-of-the-art
backbone models and conduct extensive experiments across six DeepFake datasets.
First, we compare binary and multi-class models in terms of cross-dataset
generalization. Second, we examine the accuracy of attribution models in
detecting seen manipulation methods in unknown datasets, hence uncovering data
distribution shifts on the same DeepFake manipulations. Last, we assess the
effectiveness of contrastive methods in improving cross-dataset generalization
performance. Our findings indicate that while binary models demonstrate better
generalization abilities, larger models, contrastive methods, and higher data
quality can lead to performance improvements in attribution models. The code of
this work is available on GitHub.

</details>


### [312] [CIM-NET: A Video Denoising Deep Neural Network Model Optimized for Computing-in-Memory Architectures](https://arxiv.org/abs/2505.21522)
*Shan Gao,Zhiqiang Wu,Yawen Niu,Xiaotao Li,Qingqing Xu*

Main category: cs.CV

TL;DR: 提出了一种硬件-算法协同设计框架CIM-NET，通过优化架构和引入伪卷积算子，显著减少内存计算芯片上的矩阵向量乘法操作，同时保持视频去噪性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度神经网络模型在边缘设备上部署时，难以满足实时性和能效要求，且未充分考虑内存计算芯片的架构限制，限制了其加速潜力。

Method: 提出了CIM-NET架构和CIM-CONV伪卷积算子，结合大感受野操作和基于交叉开关的矩阵向量乘法加速，实现高效特征提取和重建。

Result: CIM-NET将矩阵向量乘法操作减少至原来的1/77，同时PSNR仅略有下降（35.11 dB vs. 35.56 dB），在保持性能的同时显著提升推理速度。

Conclusion: 该硬件-算法协同设计框架有效解决了内存计算芯片上深度神经网络模型的部署挑战，实现了高性能与高效能的平衡。

Abstract: While deep neural network (DNN)-based video denoising has demonstrated
significant performance, deploying state-of-the-art models on edge devices
remains challenging due to stringent real-time and energy efficiency
requirements. Computing-in-Memory (CIM) chips offer a promising solution by
integrating computation within memory cells, enabling rapid matrix-vector
multiplication (MVM). However, existing DNN models are often designed without
considering CIM architectural constraints, thus limiting their acceleration
potential during inference. To address this, we propose a hardware-algorithm
co-design framework incorporating two innovations: (1) a CIM-Aware
Architecture, CIM-NET, optimized for large receptive field operation and CIM's
crossbar-based MVM acceleration; and (2) a pseudo-convolutional operator,
CIM-CONV, used within CIM-NET to integrate slide-based processing with fully
connected transformations for high-quality feature extraction and
reconstruction. This framework significantly reduces the number of MVM
operations, improving inference speed on CIM chips while maintaining
competitive performance. Experimental results indicate that, compared to the
conventional lightweight model FastDVDnet, CIM-NET substantially reduces MVM
operations with a slight decrease in denoising performance. With a stride value
of 8, CIM-NET reduces MVM operations to 1/77th of the original, while
maintaining competitive PSNR (35.11 dB vs. 35.56 dB

</details>


### [313] [UniDB++: Fast Sampling of Unified Diffusion Bridge](https://arxiv.org/abs/2505.21528)
*Mokai Pan,Kaizhen Zhu,Yuexin Ma,Yanwei Fu,Jingyi Yu,Jingya Wang,Ye Shi*

Main category: cs.CV

TL;DR: UniDB++提出了一种无需训练的采样算法，通过精确解析UniDB的反向时间SDE，减少了误差累积，显著提升了图像生成的速度和质量。


<details>
  <summary>Details</summary>
Motivation: UniDB框架虽然能实现高保真图像生成，但其依赖的迭代Euler采样方法导致推理速度慢且计算成本高，现有加速技术无法解决其特有的终端均值约束缺失和SOC特定惩罚系数问题。

Method: UniDB++通过推导UniDB反向时间SDE的精确闭式解，减少Euler近似的误差累积，并用数据预测模型替代传统噪声预测，结合SDE-Corrector机制提升低步数下的感知质量。

Result: 实验表明，UniDB++在图像恢复任务中表现优异，保真度和速度均优于基于Euler的方法，推理时间显著减少，最高可减少20倍采样步数。

Conclusion: UniDB++在SOC驱动的扩散桥模型中弥合了理论通用性和实际效率之间的差距，为高保真图像生成提供了高效解决方案。

Abstract: Diffusion Bridges enable transitions between arbitrary distributions, with
the Unified Diffusion Bridge (UniDB) framework achieving high-fidelity image
generation via a Stochastic Optimal Control (SOC) formulation. However, UniDB's
reliance on iterative Euler sampling methods results in slow, computationally
expensive inference, while existing acceleration techniques for diffusion or
diffusion bridge models fail to address its unique challenges: missing terminal
mean constraints and SOC-specific penalty coefficients in its SDEs. We present
UniDB++, a training-free sampling algorithm that significantly improves upon
these limitations. The method's key advancement comes from deriving exact
closed-form solutions for UniDB's reverse-time SDEs, effectively reducing the
error accumulation inherent in Euler approximations and enabling high-quality
generation with up to 20$\times$ fewer sampling steps. This method is further
complemented by replacing conventional noise prediction with a more stable data
prediction model, along with an SDE-Corrector mechanism that maintains
perceptual quality for low-step regimes (5-10 steps). Additionally, we
demonstrate that UniDB++ aligns with existing diffusion bridge acceleration
methods by evaluating their update rules, and UniDB++ can recover DBIMs as
special cases under some theoretical conditions. Experiments demonstrate
UniDB++'s state-of-the-art performance in image restoration tasks,
outperforming Euler-based methods in fidelity and speed while reducing
inference time significantly. This work bridges the gap between theoretical
generality and practical efficiency in SOC-driven diffusion bridge models. Our
code is available at https://github.com/2769433owo/UniDB-plusplus.

</details>


### [314] [How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control](https://arxiv.org/abs/2505.21531)
*Kunhang Li,Jason Naradowsky,Yansong Feng,Yusuke Miyao*

Main category: cs.CV

TL;DR: 论文通过3D角色控制探索大语言模型对人类动作的理解能力，发现模型擅长高层次动作规划但精确定位身体部位存在困难。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估大语言模型对人类运动知识的掌握程度，特别是在生成详细动作计划和精确定位身体部位方面的能力。

Method: 设计20个代表性动作指令，分高层次动作规划和低层次身体部位定位两步生成动画，并通过人工评估和自动对比验证。

Result: 模型能较好解释高层次动作，但精确定位困难；分解动作可提升规划效果，但对多步骤高自由度动作仍存在挑战。

Conclusion: 大语言模型在创意动作概念化和文化特定动作识别方面有潜力，但需提升空间-时间参数精确控制能力。

Abstract: We explore Large Language Models (LLMs)' human motion knowledge through 3D
avatar control. Given a motion instruction, we prompt LLMs to first generate a
high-level movement plan with consecutive steps (High-level Planning), then
specify body part positions in each step (Low-level Planning), which we
linearly interpolate into avatar animations as a clear verification lens for
human evaluators. Through carefully designed 20 representative motion
instructions with full coverage of basic movement primitives and balanced body
part usage, we conduct comprehensive evaluations including human assessment of
both generated animations and high-level movement plans, as well as automatic
comparison with oracle positions in low-level planning. We find that LLMs are
strong at interpreting the high-level body movements but struggle with precise
body part positioning. While breaking down motion queries into atomic
components improves planning performance, LLMs have difficulty with multi-step
movements involving high-degree-of-freedom body parts. Furthermore, LLMs
provide reasonable approximation for general spatial descriptions, but fail to
handle precise spatial specifications in text, and the precise spatial-temporal
parameters needed for avatar control. Notably, LLMs show promise in
conceptualizing creative motions and distinguishing culturally-specific motion
patterns.

</details>


### [315] [EvidenceMoE: A Physics-Guided Mixture-of-Experts with Evidential Critics for Advancing Fluorescence Light Detection and Ranging in Scattering Media](https://arxiv.org/abs/2505.21532)
*Ismail Erbas,Ferhat Demirkiran,Karthik Swaminathan,Naigang Wang,Navid Ibtehaj Nizam,Stefan T. Radev,Kaoutar El Maghraoui,Xavier Intes,Vikas Pandey*

Main category: cs.CV

TL;DR: 该论文提出了一种基于物理指导的混合专家（MoE）框架，用于解决荧光激光雷达（FLiDAR）在散射介质中深度和荧光寿命估计的计算难题。


<details>
  <summary>Details</summary>
Motivation: 荧光激光雷达（FLiDAR）在医疗、汽车等领域用于距离和深度估计，但在散射介质中面临信号解析困难，难以准确分离光子飞行时间（与目标深度相关）和固有荧光寿命，限制了现有方法的有效性。

Method: 论文提出了一种物理指导的混合专家（MoE）框架，其中专家模型基于辐射传输方程等物理原理。框架核心是EvidenceMoE，集成了基于证据的Dirichlet批评器（EDCs）来评估专家输出的可靠性，并通过决策网络自适应融合专家预测。

Result: 在模拟的荧光激光雷达数据上验证，该方法在深度估计和荧光寿命测量中表现出色，归一化均方根误差（NRMSE）分别为0.030和0.074。

Conclusion: 该物理指导的MoE框架有效提升了FLiDAR在散射介质中的性能，为深度和荧光寿命估计提供了可靠解决方案。

Abstract: Fluorescence LiDAR (FLiDAR), a Light Detection and Ranging (LiDAR) technology
employed for distance and depth estimation across medical, automotive, and
other fields, encounters significant computational challenges in scattering
media. The complex nature of the acquired FLiDAR signal, particularly in such
environments, makes isolating photon time-of-flight (related to target depth)
and intrinsic fluorescence lifetime exceptionally difficult, thus limiting the
effectiveness of current analytical and computational methodologies. To
overcome this limitation, we present a Physics-Guided Mixture-of-Experts (MoE)
framework tailored for specialized modeling of diverse temporal components. In
contrast to the conventional MoE approaches our expert models are informed by
underlying physics, such as the radiative transport equation governing photon
propagation in scattering media. Central to our approach is EvidenceMoE, which
integrates Evidence-Based Dirichlet Critics (EDCs). These critic models assess
the reliability of each expert's output by providing per-expert quality scores
and corrective feedback. A Decider Network then leverages this information to
fuse expert predictions into a robust final estimate adaptively. We validate
our method using realistically simulated Fluorescence LiDAR (FLiDAR) data for
non-invasive cancer cell depth detection generated from photon transport models
in tissue. Our framework demonstrates strong performance, achieving a
normalized root mean squared error (NRMSE) of 0.030 for depth estimation and
0.074 for fluorescence lifetime.

</details>


### [316] [Is Attention Required for Transformer Inference? Explore Function-preserving Attention Replacement](https://arxiv.org/abs/2505.21535)
*Yuxin Ren,Maxwell D Collins,Miao Hu,Huanrui Yang*

Main category: cs.CV

TL;DR: 本文提出FAR框架，用LSTM等序列模块替换预训练Transformer中的注意力机制，在保持性能的同时提升推理效率。


<details>
  <summary>Details</summary>
Motivation: Transformer的注意力机制在推理时存在冗余，导致在边缘设备上效率低下。作者假设推理时的序列映射关系足够简单，可用更高效的模块替代。

Method: 提出FAR框架：1) 用可学习的序列模块（如LSTM）替换所有注意力块；2) 采用分块蒸馏目标和全局结构剪枝优化架构。

Result: 在DeiT视觉Transformer上验证，保持ImageNet及下游任务精度的同时减少了参数和延迟，且保留了注意力模块学习到的语义关系。

Conclusion: FAR成功证明了用轻量级序列模块替代注意力机制的可行性，为边缘设备部署高效Transformer提供了新思路。

Abstract: While transformers excel across vision and language pretraining tasks, their
reliance on attention mechanisms poses challenges for inference efficiency,
especially on edge and embedded accelerators with limited parallelism and
memory bandwidth. Hinted by the observed redundancy of attention at inference
time, we hypothesize that though the model learns complicated token dependency
through pretraining, the inference-time sequence-to-sequence mapping in each
attention layer is actually ''simple'' enough to be represented with a much
cheaper function. In this work, we explore FAR, a Function-preserving Attention
Replacement framework that replaces all attention blocks in pretrained
transformers with learnable sequence-to-sequence modules, exemplified by an
LSTM. FAR optimize a multi-head LSTM architecture with a block-wise
distillation objective and a global structural pruning framework to achieve a
family of efficient LSTM-based models from pretrained transformers. We validate
FAR on the DeiT vision transformer family and demonstrate that it matches the
accuracy of the original models on ImageNet and multiple downstream tasks with
reduced parameters and latency. Further analysis shows that FAR preserves the
semantic token relationships and the token-to-token correlation learned in the
transformer's attention module.

</details>


### [317] [Caption This, Reason That: VLMs Caught in the Middle](https://arxiv.org/abs/2505.21538)
*Zihan Weng,Lucas Gomez,Taylor Whittington Webb,Pouya Bashivan*

Main category: cs.CV

TL;DR: 该论文通过认知科学方法分析视觉语言模型（VLMs）在感知、注意力和记忆等核心认知能力上的表现，发现其在空间理解和选择性注意力任务中存在显著不足，并提出通过微调提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型（VLMs）在视觉理解方面取得了显著进展，但在计数或关系推理等特定任务上仍落后于人类能力。论文旨在通过认知科学方法分析VLMs的局限性，并探索改进方法。

Method: 论文采用认知科学方法，设计了一系列针对感知、注意力和记忆的任务，评估了包括GPT-4o在内的先进VLMs。通过视觉-文本解耦分析，研究了模型在直接视觉推理和基于生成文本推理中的表现差异，并尝试通过微调提升模型性能。

Result: 研究发现，先进VLMs在类别识别等任务上表现接近天花板，但在空间理解和选择性注意力任务中仍存在显著差距。视觉-文本解耦分析显示，模型在基于生成文本推理时表现更好。微调较小VLMs能显著提升核心认知能力，但对分布外基准任务的提升有限。

Conclusion: 论文详细分析了VLMs的认知优势和劣势，指出了同时进行感知和推理的关键瓶颈，并提出通过微调提升模型性能的简单有效方法。研究还表明，VLMs在数据集上的表现与其他基准任务的表现强相关。

Abstract: Vision-Language Models (VLMs) have shown remarkable progress in visual
understanding in recent years. Yet, they still lag behind human capabilities in
specific visual tasks such as counting or relational reasoning. To understand
the underlying limitations, we adopt methodologies from cognitive science,
analyzing VLM performance along core cognitive axes: Perception, Attention, and
Memory. Using a suite of tasks targeting these abilities, we evaluate
state-of-the-art VLMs, including GPT-4o. Our analysis reveals distinct
cognitive profiles: while advanced models approach ceiling performance on some
tasks (e.g. category identification), a significant gap persists, particularly
in tasks requiring spatial understanding or selective attention. Investigating
the source of these failures and potential methods for improvement, we employ a
vision-text decoupling analysis, finding that models struggling with direct
visual reasoning show marked improvement when reasoning over their own
generated text captions. These experiments reveal a strong need for improved
VLM Chain-of-Thought (CoT) abilities, even in models that consistently exceed
human performance. Furthermore, we demonstrate the potential of targeted
fine-tuning on composite visual reasoning tasks and show that fine-tuning
smaller VLMs substantially improves core cognitive abilities. While this
improvement does not translate to large enhancements on challenging,
out-of-distribution benchmarks, we show broadly that VLM performance on our
datasets strongly correlates with performance on these other benchmarks. Our
work provides a detailed analysis of VLM cognitive strengths and weaknesses and
identifies key bottlenecks in simultaneous perception and reasoning while also
providing an effective and simple solution.

</details>


### [318] [Equivariant Flow Matching for Point Cloud Assembly](https://arxiv.org/abs/2505.21539)
*Ziming Wang,Nan Xue,Rebecka Jörnsten*

Main category: cs.CV

TL;DR: 提出了一种基于流匹配模型的等变求解器Eda，用于点云组装任务，能高效处理非重叠输入片段。


<details>
  <summary>Details</summary>
Motivation: 点云组装任务需要将多个点云片段对齐以重建完整3D形状，现有方法在处理非重叠片段时存在挑战。

Method: 通过理论分析，提出Eda模型学习与输入片段相关的向量场，并构建等变路径保证训练过程的高数据效率。

Result: 实验表明Eda在实际数据集上表现优异，能有效处理非重叠输入片段。

Conclusion: Eda在点云组装任务中具有高效性和竞争力，尤其适用于非重叠片段的挑战性场景。

Abstract: The goal of point cloud assembly is to reconstruct a complete 3D shape by
aligning multiple point cloud pieces. This work presents a novel equivariant
solver for assembly tasks based on flow matching models. We first theoretically
show that the key to learning equivariant distributions via flow matching is to
learn related vector fields. Based on this result, we propose an assembly
model, called equivariant diffusion assembly (Eda), which learns related vector
fields conditioned on the input pieces. We further construct an equivariant
path for Eda, which guarantees high data efficiency of the training process.
Our numerical results show that Eda is highly competitive on practical
datasets, and it can even handle the challenging situation where the input
pieces are non-overlapped.

</details>


### [319] [DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via Diffusion Transformers](https://arxiv.org/abs/2505.21541)
*Zitong Wang,Hang Zhao,Qianyu Zhou,Xuequan Lu,Xiangtai Li,Yiren Song*

Main category: cs.CV

TL;DR: 本文提出DiffDecompose方法，利用扩散Transformer解决半透明/透明图层分解任务，并构建首个大规模数据集AlphaBlend。


<details>
  <summary>Details</summary>
Motivation: 现有图像分解方法难以处理半透明或透明图层遮挡问题，主要受限于掩码先验依赖、静态对象假设和数据集缺乏。

Method: 提出基于扩散Transformer的DiffDecompose框架，通过上下文分解和层位置编码克隆技术实现无需逐层监督的多层预测。

Result: 在自建AlphaBlend数据集和公开LOGO数据集上的实验验证了方法的有效性。

Conclusion: DiffDecompose为透明图层分解任务提供了新解决方案，代码和数据集将开源。

Abstract: Diffusion models have recently motivated great success in many generation
tasks like object removal. Nevertheless, existing image decomposition methods
struggle to disentangle semi-transparent or transparent layer occlusions due to
mask prior dependencies, static object assumptions, and the lack of datasets.
In this paper, we delve into a novel task: Layer-Wise Decomposition of
Alpha-Composited Images, aiming to recover constituent layers from single
overlapped images under the condition of semi-transparent/transparent alpha
layer non-linear occlusion. To address challenges in layer ambiguity,
generalization, and data scarcity, we first introduce AlphaBlend, the first
large-scale and high-quality dataset for transparent and semi-transparent layer
decomposition, supporting six real-world subtasks (e.g., translucent flare
removal, semi-transparent cell decomposition, glassware decomposition).
Building on this dataset, we present DiffDecompose, a diffusion
Transformer-based framework that learns the posterior over possible layer
decompositions conditioned on the input image, semantic prompts, and blending
type. Rather than regressing alpha mattes directly, DiffDecompose performs
In-Context Decomposition, enabling the model to predict one or multiple layers
without per-layer supervision, and introduces Layer Position Encoding Cloning
to maintain pixel-level correspondence across layers. Extensive experiments on
the proposed AlphaBlend dataset and public LOGO dataset verify the
effectiveness of DiffDecompose. The code and dataset will be available upon
paper acceptance. Our code will be available at:
https://github.com/Wangzt1121/DiffDecompose.

</details>


### [320] [Image Tokens Matter: Mitigating Hallucination in Discrete Tokenizer-based Large Vision-Language Models via Latent Editing](https://arxiv.org/abs/2505.21547)
*Weixing Wang,Zifeng Ding,Jindong Gu,Rui Cao,Christoph Meinel,Gerard de Melo,Haojin Yang*

Main category: cs.CV

TL;DR: 大型视觉语言模型（LVLMs）在处理视觉输入时存在幻觉问题，即生成不存在的对象。作者提出通过分析图像标记共现图并抑制视觉缺失标记的影响来减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 研究发现，大型视觉语言模型在训练过程中形成的视觉先验可能导致模型生成不存在的对象。作者假设这是由于某些图像标记在空间区域中频繁共现，并与这些对象的语言描述形成强关联。

Method: 作者构建了图像标记的共现图，使用图神经网络（GNN）和对比学习进行聚类，识别频繁共现的标记簇。然后提出一种方法，通过修改潜在图像嵌入来抑制视觉缺失标记的影响。

Result: 实验表明，该方法有效减少了幻觉现象，同时保持了模型的表达能力。

Conclusion: 通过分析标记共现模式并抑制视觉缺失标记的影响，可以显著减少大型视觉语言模型的幻觉问题，提升模型的可靠性。

Abstract: Large Vision-Language Models (LVLMs) with discrete image tokenizers unify
multimodal representations by encoding visual inputs into a finite set of
tokens. Despite their effectiveness, we find that these models still
hallucinate non-existent objects. We hypothesize that this may be due to visual
priors induced during training: When certain image tokens frequently co-occur
in the same spatial regions and represent shared objects, they become strongly
associated with the verbalizations of those objects. As a result, the model may
hallucinate by evoking visually absent tokens that often co-occur with present
ones. To test this assumption, we construct a co-occurrence graph of image
tokens using a segmentation dataset and employ a Graph Neural Network (GNN)
with contrastive learning followed by a clustering method to group tokens that
frequently co-occur in similar visual contexts. We find that hallucinations
predominantly correspond to clusters whose tokens dominate the input, and more
specifically, that the visually absent tokens in those clusters show much
higher correlation with hallucinated objects compared to tokens present in the
image. Based on this observation, we propose a hallucination mitigation method
that suppresses the influence of visually absent tokens by modifying latent
image embeddings during generation. Experiments show our method reduces
hallucinations while preserving expressivity. Code is available at
https://github.com/weixingW/CGC-VTD/tree/main

</details>


### [321] [Benign-to-Toxic Jailbreaking: Inducing Harmful Responses from Harmless Prompts](https://arxiv.org/abs/2505.21556)
*Hee-Seon Kim,Minbeom Kim,Wonjun Lee,Kihyun Kim,Changick Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新的Benign-to-Toxic（B2T）越狱方法，通过优化对抗图像从良性条件诱导毒性输出，揭示了多模态对齐中的新漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有基于优化的越狱方法在Toxic-Continuation设置下效果有限，尤其是在缺乏明确毒性信号时难以引发安全对齐失效。

Method: 提出B2T越狱范式，优化对抗图像使其在良性条件下诱导模型产生毒性输出，无需依赖已有毒性输入。

Result: 该方法优于现有方法，在黑盒设置中具有可迁移性，并能与基于文本的越狱方法互补。

Conclusion: B2T范式揭示了多模态对齐中未被充分探索的脆弱性，为越狱方法提供了全新方向。

Abstract: Optimization-based jailbreaks typically adopt the Toxic-Continuation setting
in large vision-language models (LVLMs), following the standard next-token
prediction objective. In this setting, an adversarial image is optimized to
make the model predict the next token of a toxic prompt. However, we find that
the Toxic-Continuation paradigm is effective at continuing already-toxic
inputs, but struggles to induce safety misalignment when explicit toxic signals
are absent. We propose a new paradigm: Benign-to-Toxic (B2T) jailbreak. Unlike
prior work, we optimize adversarial images to induce toxic outputs from benign
conditioning. Since benign conditioning contains no safety violations, the
image alone must break the model's safety mechanisms. Our method outperforms
prior approaches, transfers in black-box settings, and complements text-based
jailbreaks. These results reveal an underexplored vulnerability in multimodal
alignment and introduce a fundamentally new direction for jailbreak approaches.

</details>


### [322] [Analytical Calculation of Weights Convolutional Neural Network](https://arxiv.org/abs/2505.21557)
*Polad Geidarov*

Main category: cs.CV

TL;DR: 该论文提出一种无需传统训练的CNN权重与阈值解析计算算法，仅需10张MNIST样本即可构建网络，在未训练情况下实现50%+的手写数字识别准确率。


<details>
  <summary>Details</summary>
Motivation: 探索绕过耗时训练过程、直接通过解析计算确定CNN参数的可能性，以提升模型构建效率。

Method: 基于MNIST数据集中0-9各1张样本图像，解析推导CNN各层通道数、权重与阈值，并用C++ Builder实现算法模块。

Result: 未经训练的解析CNN在1000张测试图像上达到超50%识别准确率，推理耗时不足1秒。

Conclusion: 证明CNN可通过纯解析计算直接应用于分类任务，为免训练网络设计提供新思路。

Abstract: This paper presents an algorithm for analytically calculating the weights and
thresholds of convolutional neural networks (CNNs) without using standard
training procedures. The algorithm enables the determination of CNN parameters
based on just 10 selected images from the MNIST dataset, each representing a
digit from 0 to 9. As part of the method, the number of channels in CNN layers
is also derived analytically. A software module was implemented in C++ Builder,
and a series of experiments were conducted using the MNIST dataset. Results
demonstrate that the analytically computed CNN can recognize over half of 1000
handwritten digit images without any training, achieving inference in fractions
of a second. These findings suggest that CNNs can be constructed and applied
directly for classification tasks without training, using purely analytical
computation of weights.

</details>


### [323] [A Novel Convolutional Neural Network-Based Framework for Complex Multiclass Brassica Seed Classification](https://arxiv.org/abs/2505.21558)
*Elhoucine Elfatimia,Recep Eryigitb,Lahcen Elfatimi*

Main category: cs.CV

TL;DR: 该研究提出了一种基于CNN的新方法，用于高效分类十种常见芸薹属种子，解决了种子图像纹理相似性的挑战，准确率达93%。


<details>
  <summary>Details</summary>
Motivation: 农民因作物生产和农场操作的需求缺乏时间和资源进行农场研究，种子分类对质量控制、生产效率和杂质检测至关重要。早期识别种子类型有助于降低成本和风险，提高产量。

Method: 研究采用自定义设计的CNN架构，针对种子图像纹理相似性的挑战进行优化，并与多种预训练的最先进架构进行比较。

Result: 实验结果表明，所提出的模型在芸薹属种子数据集上实现了93%的高准确率。

Conclusion: 该CNN框架为种子分类提供了一种高效方法，有助于农民优化种子质量管理和产量预估。

Abstract: Agricultural research has accelerated in recent years, yet farmers often lack
the time and resources for on-farm research due to the demands of crop
production and farm operations. Seed classification offers valuable insights
into quality control, production efficiency, and impurity detection. Early
identification of seed types is critical to reducing the cost and risk
associated with field emergence, which can lead to yield losses or disruptions
in downstream processes like harvesting. Seed sampling supports growers in
monitoring and managing seed quality, improving precision in determining seed
purity levels, guiding management adjustments, and enhancing yield estimations.
This study proposes a novel convolutional neural network (CNN)-based framework
for the efficient classification of ten common Brassica seed types. The
approach addresses the inherent challenge of texture similarity in seed images
using a custom-designed CNN architecture. The model's performance was evaluated
against several pre-trained state-of-the-art architectures, with adjustments to
layer configurations for optimized classification. Experimental results using
our collected Brassica seed dataset demonstrate that the proposed model
achieved a high accuracy rate of 93 percent.

</details>


### [324] [Thickness-aware E(3)-Equivariant 3D Mesh Neural Networks](https://arxiv.org/abs/2505.21572)
*Sungwon Kim,Namkyeong Lee,Yunyoung Doh,Seungmin Shin,Guimok Cho,Seung-Won Jeon,Sangkook Kim,Chanyoung Park*

Main category: cs.CV

TL;DR: 提出了一种新型的厚度感知E(3)-等变3D网格神经网络（T-EMNN），有效整合3D物体厚度并保持计算效率，显著提升了节点级3D变形预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于网格的3D静态分析方法主要关注表面拓扑和几何，忽略了真实3D物体的厚度特性，导致无法捕捉对立表面之间的高度相关性和相似行为。

Method: 提出T-EMNN框架，通过数据驱动的坐标编码空间信息并保持E(3)-等变性或不变性，同时整合物体厚度信息。

Result: 在真实工业数据集上的评估表明，T-EMNN能准确预测节点级3D变形，有效捕捉厚度效应且保持计算高效性。

Conclusion: T-EMNN在保持计算效率的同时，显著提升了3D变形预测的准确性，为厚度感知的3D分析提供了有效解决方案。

Abstract: Mesh-based 3D static analysis methods have recently emerged as efficient
alternatives to traditional computational numerical solvers, significantly
reducing computational costs and runtime for various physics-based analyses.
However, these methods primarily focus on surface topology and geometry, often
overlooking the inherent thickness of real-world 3D objects, which exhibits
high correlations and similar behavior between opposing surfaces. This
limitation arises from the disconnected nature of these surfaces and the
absence of internal edge connections within the mesh. In this work, we propose
a novel framework, the Thickness-aware E(3)-Equivariant 3D Mesh Neural Network
(T-EMNN), that effectively integrates the thickness of 3D objects while
maintaining the computational efficiency of surface meshes. Additionally, we
introduce data-driven coordinates that encode spatial information while
preserving E(3)-equivariance or invariance properties, ensuring consistent and
robust analysis. Evaluations on a real-world industrial dataset demonstrate the
superior performance of T-EMNN in accurately predicting node-level 3D
deformations, effectively capturing thickness effects while maintaining
computational efficiency.

</details>


### [325] [Do you see what I see? An Ambiguous Optical Illusion Dataset exposing limitations of Explainable AI](https://arxiv.org/abs/2505.21589)
*Carina Newen,Luca Hinkamp,Maria Ntonti,Emmanuel Müller*

Main category: cs.CV

TL;DR: 该论文介绍了一个新颖的视觉错觉数据集，旨在研究机器学习和人类视觉感知中的模糊性问题，并探讨了视觉概念对模型准确性的影响。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域（如自动驾驶和医疗诊断）中，机器学习算法的重要性日益凸显。视觉错觉作为研究人类和机器感知局限性的有力工具，相关数据集却非常稀缺。因此，作者旨在填补这一空白。

Method: 作者系统地创建了一个包含交织动物对的视觉错觉数据集，重点关注视线方向和眼睛线索等视觉概念，并通过代码开源和数据集公开促进研究。

Result: 研究发现，视线方向和眼睛线索等细微但重要的视觉特征显著影响模型准确性，揭示了视觉概念在机器学习中的关键作用。

Conclusion: 该研究为理解机器与人类视觉的对齐和偏见提供了基础，通过公开数据集和代码，推动了相关领域的进一步研究。

Abstract: From uncertainty quantification to real-world object detection, we recognize
the importance of machine learning algorithms, particularly in safety-critical
domains such as autonomous driving or medical diagnostics. In machine learning,
ambiguous data plays an important role in various machine learning domains.
Optical illusions present a compelling area of study in this context, as they
offer insight into the limitations of both human and machine perception.
Despite this relevance, optical illusion datasets remain scarce. In this work,
we introduce a novel dataset of optical illusions featuring intermingled animal
pairs designed to evoke perceptual ambiguity. We identify generalizable visual
concepts, particularly gaze direction and eye cues, as subtle yet impactful
features that significantly influence model accuracy. By confronting models
with perceptual ambiguity, our findings underscore the importance of concepts
in visual learning and provide a foundation for studying bias and alignment
between human and machine vision. To make this dataset useful for general
purposes, we generate optical illusions systematically with different concepts
discussed in our bias mitigation section. The dataset is accessible in Kaggle
via
https://kaggle.com/datasets/693bf7c6dd2cb45c8a863f9177350c8f9849a9508e9d50526e2ffcc5559a8333.
Our source code can be found at
https://github.com/KDD-OpenSource/Ambivision.git.

</details>


### [326] [Any-to-Bokeh: One-Step Video Bokeh via Multi-Plane Image Guided Diffusion](https://arxiv.org/abs/2505.21593)
*Yang Yang,Siming Zheng,Jinwei Chen,Boxi Wu,Xiaofei He,Deng Cai,Bo Li,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的一步式视频背景虚化框架，通过多平面图像表示和渐进式训练策略实现时序一致、深度感知的高质量虚化效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑模型无法显式控制焦平面或调整虚化强度，且图像虚化方法直接扩展到视频会导致时序闪烁和边缘模糊过渡不自然。

Method: 采用多平面图像(MPI)表示构建渐进扩展的深度采样函数，结合单步视频扩散模型和预训练模型(如Stable Video Diffusion)的3D先验，并引入渐进式训练策略增强时序一致性。

Result: 实验表明该方法能生成高质量、可控的虚化效果，在多个评估基准上达到最先进性能。

Conclusion: 该框架有效解决了视频虚化的时序一致性和深度控制问题，为光学效果编辑提供了实用解决方案。

Abstract: Recent advances in diffusion based editing models have enabled realistic
camera simulation and image-based bokeh, but video bokeh remains largely
unexplored. Existing video editing models cannot explicitly control focus
planes or adjust bokeh intensity, limiting their applicability for controllable
optical effects. Moreover, naively extending image-based bokeh methods to video
often results in temporal flickering and unsatisfactory edge blur transitions
due to the lack of temporal modeling and generalization capability. To address
these challenges, we propose a novel one-step video bokeh framework that
converts arbitrary input videos into temporally coherent, depth-aware bokeh
effects. Our method leverages a multi-plane image (MPI) representation
constructed through a progressively widening depth sampling function, providing
explicit geometric guidance for depth-dependent blur synthesis. By conditioning
a single-step video diffusion model on MPI layers and utilizing the strong 3D
priors from pre-trained models such as Stable Video Diffusion, our approach
achieves realistic and consistent bokeh effects across diverse scenes.
Additionally, we introduce a progressive training strategy to enhance temporal
consistency, depth robustness, and detail preservation. Extensive experiments
demonstrate that our method produces high-quality, controllable bokeh effects
and achieves state-of-the-art performance on multiple evaluation benchmarks.

</details>


### [327] [OmniResponse: Online Multimodal Conversational Response Generation in Dyadic Interactions](https://arxiv.org/abs/2505.21724)
*Cheng Luo,Jianghui Wang,Bing Li,Siyang Song,Bernard Ghanem*

Main category: cs.CV

TL;DR: 本文提出在线多模态对话响应生成任务(OMCRG)及OmniResponse模型，通过文本桥接音频与面部反应，实现高质量多模态听众响应生成。


<details>
  <summary>Details</summary>
Motivation: 自然双向互动中，听众的言语与非言语反馈需同步生成，现有技术难以实现音频与面部反应的时序对齐。

Method: 提出基于多模态大语言模型(MLLM)的OmniResponse，包含时序锚定文本模块Chrono-Text和可控语音合成模块TempoVoice，并构建含696组对话的ResponseNet数据集。

Result: 实验表明OmniResponse在语义内容、音画同步和生成质量上显著优于基线模型。

Conclusion: 通过文本中介实现多模态响应同步生成是可行的，OmniResponse为OMCRG任务提供了有效解决方案。

Abstract: In this paper, we introduce Online Multimodal Conversational Response
Generation (OMCRG), a novel task that aims to online generate synchronized
verbal and non-verbal listener feedback, conditioned on the speaker's
multimodal input. OMCRG reflects natural dyadic interactions and poses new
challenges in achieving synchronization between the generated audio and facial
responses of the listener. To address these challenges, we innovatively
introduce text as an intermediate modality to bridge the audio and facial
responses. We hence propose OmniResponse, a Multimodal Large Language Model
(MLLM) that autoregressively generates high-quality multi-modal listener
responses. OmniResponse leverages a pretrained LLM enhanced with two novel
components: Chrono-Text, which temporally anchors generated text tokens, and
TempoVoice, a controllable online TTS module that produces speech synchronized
with facial reactions. To support further OMCRG research, we present
ResponseNet, a new dataset comprising 696 high-quality dyadic interactions
featuring synchronized split-screen videos, multichannel audio, transcripts,
and facial behavior annotations. Comprehensive evaluations conducted on
ResponseNet demonstrate that OmniResponse significantly outperforms baseline
models in terms of semantic speech content, audio-visual synchronization, and
generation quality.

</details>


### [328] [Learning to See More: UAS-Guided Super-Resolution of Satellite Imagery for Precision Agriculture](https://arxiv.org/abs/2505.21746)
*Arif Masrur,Peder A. Olsen,Paul R. Adler,Carlan Jackson,Matthew W. Myers,Nathan Sedghi,Ray R. Weil*

Main category: cs.CV

TL;DR: 该研究提出了一种融合卫星和无人机系统（UAS）图像的超分辨率方法，通过整合空间、光谱和时间域数据，有效提升了农作物生物量和氮含量估算的精度，同时降低了成本。


<details>
  <summary>Details</summary>
Motivation: 卫星和无人机系统（UAS）在精准农业中各有优劣：卫星数据覆盖范围广但分辨率低，UAS数据分辨率高但覆盖范围有限且成本较高。研究旨在结合两者的优势，提供一种经济高效的解决方案。

Method: 研究采用超分辨率方法，将UAS的RGB数据光谱扩展到植被红边和近红外区域，生成高分辨率的Sentinel-2图像，并通过SRCNN模型实现光谱扩展。

Result: 该方法使生物量和氮含量估算的精度分别提高了18%和31%，且仅需在部分田地和时间点收集UAS数据，显著降低了成本。

Conclusion: 该研究提出的轻量级、可扩展的系统为精准农业提供了一种经济高效的解决方案，即使在无云卫星数据不可用时仍能有效工作，并减少了重复UAS飞行的需求。

Abstract: Unmanned Aircraft Systems (UAS) and satellites are key data sources for
precision agriculture, yet each presents trade-offs. Satellite data offer broad
spatial, temporal, and spectral coverage but lack the resolution needed for
many precision farming applications, while UAS provide high spatial detail but
are limited by coverage and cost, especially for hyperspectral data. This study
presents a novel framework that fuses satellite and UAS imagery using
super-resolution methods. By integrating data across spatial, spectral, and
temporal domains, we leverage the strengths of both platforms cost-effectively.
We use estimation of cover crop biomass and nitrogen (N) as a case study to
evaluate our approach. By spectrally extending UAS RGB data to the vegetation
red edge and near-infrared regions, we generate high-resolution Sentinel-2
imagery and improve biomass and N estimation accuracy by 18% and 31%,
respectively. Our results show that UAS data need only be collected from a
subset of fields and time points. Farmers can then 1) enhance the spectral
detail of UAS RGB imagery; 2) increase the spatial resolution by using
satellite data; and 3) extend these enhancements spatially and across the
growing season at the frequency of the satellite flights. Our SRCNN-based
spectral extension model shows considerable promise for model transferability
over other cropping systems in the Upper and Lower Chesapeake Bay regions.
Additionally, it remains effective even when cloud-free satellite data are
unavailable, relying solely on the UAS RGB input. The spatial extension model
produces better biomass and N predictions than models built on raw UAS RGB
images. Once trained with targeted UAS RGB data, the spatial extension model
allows farmers to stop repeated UAS flights. While we introduce
super-resolution advances, the core contribution is a lightweight and scalable
system for affordable on-farm use.

</details>


### [329] [FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering](https://arxiv.org/abs/2505.21755)
*Chengyue Huang,Brisa Maneechotesuwan,Shivang Chopra,Zsolt Kira*

Main category: cs.CV

TL;DR: 该论文提出一个新的多模态VQA鲁棒微调基准FRAMES-VQA，通过整合10个现有数据集并分析单/多模态分布偏移，为开发更鲁棒的微调方法提供指导。


<details>
  <summary>Details</summary>
Motivation: 当前VQA系统在适应真实世界数据分布变化时面临挑战，尤其是多模态场景下的评估设置较为局限，缺乏对多模态分布偏移的系统性研究。

Method: 整合10个VQA基准数据集，划分为ID/近OOD/远OOD三类；比较现有鲁棒微调方法；使用马氏距离量化分布偏移；分析模态间交互作用。

Result: 建立了首个系统评估多模态分布偏移的VQA基准，通过实验揭示了单/多模态偏移的交互规律及模态重要性差异。

Conclusion: FRAMES-VQA基准填补了多模态鲁棒性评估的空白，其分析结果为开发抗分布偏移的VQA系统提供了重要方向。

Abstract: Visual question answering (VQA) systems face significant challenges when
adapting to real-world data shifts, especially in multi-modal contexts. While
robust fine-tuning strategies are essential for maintaining performance across
in-distribution (ID) and out-of-distribution (OOD) scenarios, current
evaluation settings are primarily unimodal or particular to some types of OOD,
offering limited insight into the complexities of multi-modal contexts. In this
work, we propose a new benchmark FRAMES-VQA (Fine-Tuning Robustness across
Multi-Modal Shifts in VQA) for evaluating robust fine-tuning for VQA tasks. We
utilize ten existing VQA benchmarks, including VQAv2, IV-VQA, VQA-CP, OK-VQA
and others, and categorize them into ID, near and far OOD datasets covering
uni-modal, multi-modal and adversarial distribution shifts. We first conduct a
comprehensive comparison of existing robust fine-tuning methods. We then
quantify the distribution shifts by calculating the Mahalanobis distance using
uni-modal and multi-modal embeddings extracted from various models. Further, we
perform an extensive analysis to explore the interactions between uni- and
multi-modal shifts as well as modality importance for ID and OOD samples. These
analyses offer valuable guidance on developing more robust fine-tuning methods
to handle multi-modal distribution shifts. The code is available at
https://github.com/chengyuehuang511/FRAMES-VQA .

</details>


### [330] [Vision Meets Language: A RAG-Augmented YOLOv8 Framework for Coffee Disease Diagnosis and Farmer Assistance](https://arxiv.org/abs/2505.21544)
*Semanto Mondal*

Main category: cs.CV

TL;DR: 该论文提出了一种结合AI视觉与语言模型的混合方法，用于精准农业中的咖啡叶病害检测与治理建议，旨在提高资源利用效率并减少农药使用。


<details>
  <summary>Details</summary>
Motivation: 传统农业实践存在资源利用效率低和环境问题，精准农业通过先进技术优化农业流程成为解决方案。

Method: 结合目标检测（YOLOv8）、大型语言模型（LLM）和检索增强生成（RAG），开发了一个上下文感知的诊断系统。

Result: 系统能够实时检测咖啡叶病害，并提供治理建议，减少农药使用，支持环保农业方法。

Conclusion: 该AI系统在可扩展性、可靠性和用户友好性方面表现出色，有望在未来广泛应用于农业领域。

Abstract: As a social being, we have an intimate bond with the environment. A plethora
of things in human life, such as lifestyle, health, and food are dependent on
the environment and agriculture. It comes under our responsibility to support
the environment as well as agriculture. However, traditional farming practices
often result in inefficient resource use and environmental challenges. To
address these issues, precision agriculture has emerged as a promising approach
that leverages advanced technologies to optimise agricultural processes. In
this work, a hybrid approach is proposed that combines the three different
potential fields of model AI: object detection, large language model (LLM), and
Retrieval-Augmented Generation (RAG). In this novel framework, we have tried to
combine the vision and language models to work together to identify potential
diseases in the tree leaf. This study introduces a novel AI-based precision
agriculture system that uses Retrieval Augmented Generation (RAG) to provide
context-aware diagnoses and natural language processing (NLP) and YOLOv8 for
crop disease detection. The system aims to tackle major issues with large
language models (LLMs), especially hallucinations and allows for adaptive
treatment plans and real-time disease detection. The system provides an
easy-to-use interface to the farmers, which they can use to detect the
different diseases related to coffee leaves by just submitting the image of the
affected leaf the model will detect the diseases as well as suggest potential
remediation methodologies which aim to lower the use of pesticides, preserving
livelihoods, and encouraging environmentally friendly methods. With an emphasis
on scalability, dependability, and user-friendliness, the project intends to
improve RAG-integrated object detection systems for wider agricultural
applications in the future.

</details>


### [331] [MMTBENCH: A Unified Benchmark for Complex Multimodal Table Reasoning](https://arxiv.org/abs/2505.21771)
*Prasham Yatinkumar Titiya,Jainil Trivedi,Chitta Baral,Vivek Gupta*

Main category: cs.CV

TL;DR: 该论文介绍了MMTBENCH，一个包含500个真实世界多模态表格的基准测试，用于评估当前视觉语言模型在复杂多模态表格推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在文本和图像理解方面表现出色，但在处理真实世界中复杂的多模态表格推理任务时仍面临挑战。为了填补这一空白，作者提出了MMTBENCH基准测试。

Method: 作者构建了MMTBENCH基准测试，包含500个来自不同真实来源的多模态表格和4021个问答对，覆盖多种问题类型、推理类型和表格类型。

Result: 评估结果显示，现有模型在多模态表格推理任务中存在显著性能差距，尤其是在需要视觉推理和多步推理的问题上。

Conclusion: MMTBENCH作为一个高质量且具有挑战性的资源，突显了改进视觉与语言处理紧密结合架构的迫切需求，为未来多模态表格研究提供了重要参考。

Abstract: Multimodal tables those that integrate semi structured data with visual
elements such as charts and maps are ubiquitous across real world domains, yet
they pose a formidable challenge to current vision language models (VLMs).
While Large Language models (LLMs) and VLMs have demonstrated strong
capabilities in text and image understanding, their performance on complex,
real world multimodal table reasoning remains unexplored. To bridge this gap,
we introduce MMTBENCH (Multimodal Table Benchmark), a benchmark consisting of
500 real world multimodal tables drawn from diverse real world sources, with a
total of 4021 question answer pairs. MMTBENCH questions cover four question
types (Explicit, Implicit, Answer Mention, and Visual Based), five reasoning
types (Mathematical, Extrema Identification, Fact Verification, Vision Based,
and Others), and eight table types (Single/Multiple Entity, Maps and Charts
with Entities, Single/Multiple Charts, Maps, and Visualizations). Extensive
evaluation of state of the art models on all types reveals substantial
performance gaps, particularly on questions requiring visual-based reasoning
and multi-step inference. These findings show the urgent need for improved
architectures that more tightly integrate vision and language processing. By
providing a challenging, high-quality resource that mirrors the complexity of
real-world tasks, MMTBENCH underscores its value as a resource for future
research on multimodal tables.

</details>


### [332] [Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation](https://arxiv.org/abs/2505.21549)
*Daniel Csizmadia,Andrei Codreanu,Victor Sim,Vighnesh Prabeau,Michael Lu,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.CV

TL;DR: DCLIP通过元师生蒸馏框架改进CLIP模型，在保持零样本分类能力的同时提升多模态图像-文本检索性能。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在图像-文本检索任务中受限于固定图像分辨率和有限上下文，难以实现细粒度跨模态理解。DCLIP旨在解决这些问题。

Method: 采用元师生蒸馏框架，通过双向跨注意力机制生成丰富的嵌入表示，并使用对比学习和余弦相似性目标的混合损失训练轻量级学生模型。

Result: DCLIP在仅使用约67,500个样本的情况下，显著提升了图像-文本检索性能（Recall@K, MAP），同时保留了约94%的零样本分类能力。

Conclusion: DCLIP有效平衡了任务专业化和泛化能力，为高级视觉-语言任务提供了资源高效、领域自适应且细节敏感的解决方案。

Abstract: We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that
enhances multimodal image-text retrieval while preserving the original model's
strong zero-shot classification capabilities. CLIP models are typically
constrained by fixed image resolutions and limited context, which can hinder
their effectiveness in retrieval tasks that require fine-grained cross-modal
understanding. DCLIP addresses these challenges through a meta teacher-student
distillation framework, where a cross-modal transformer teacher is fine-tuned
to produce enriched embeddings via bidirectional cross-attention between
YOLO-extracted image regions and corresponding textual spans. These
semantically and spatially aligned global representations guide the training of
a lightweight student model using a hybrid loss that combines contrastive
learning and cosine similarity objectives. Despite being trained on only
~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a
fraction of CLIP's original dataset-DCLIP significantly improves image-text
retrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIP's
zero-shot classification performance. These results demonstrate that DCLIP
effectively mitigates the trade-off between task specialization and
generalization, offering a resource-efficient, domain-adaptive, and
detail-sensitive solution for advanced vision-language tasks. Code available at
https://anonymous.4open.science/r/DCLIP-B772/README.md.

</details>


### [333] [RePaViT: Scalable Vision Transformer Acceleration via Structural Reparameterization on Feedforward Network Layers](https://arxiv.org/abs/2505.21847)
*Xuwei Xu,Yang Li,Yudong Chen,Jiajun Liu,Sen Wang*

Main category: cs.CV

TL;DR: 研究发现，在Vision Transformer (ViT)中，前馈网络(FFN)层而非注意力层是推理延迟的主要来源。为此，提出了一种新颖的通道闲置机制，通过结构重参数化优化FFN层，显著提升了ViT的推理效率。


<details>
  <summary>Details</summary>
Motivation: ViT模型在扩大规模时，FFN层成为推理延迟的主要瓶颈。优化FFN层效率成为提升大规模ViT性能的关键机会。

Method: 提出通道闲置机制，允许部分特征通道绕过非线性激活函数，形成线性路径，实现FFN层的结构重参数化，从而创建ReParameterizable Vision Transformers (RePaViTs)。

Result: RePaViT系列模型在多个ViT变体上实现了显著的延迟降低，同时保持或提升准确率。特别是RePa-ViT-Large和RePa-ViT-Huge分别实现了66.8%和68.7%的速度提升，且准确率有所提高。

Conclusion: RePaViT首次将结构重参数化应用于FFN层，有效加速ViT推理，为高效ViT模型的发展提供了有前景的方向。

Abstract: We reveal that feedforward network (FFN) layers, rather than attention
layers, are the primary contributors to Vision Transformer (ViT) inference
latency, with their impact signifying as model size increases. This finding
highlights a critical opportunity for optimizing the efficiency of large-scale
ViTs by focusing on FFN layers. In this work, we propose a novel channel idle
mechanism that facilitates post-training structural reparameterization for
efficient FFN layers during testing. Specifically, a set of feature channels
remains idle and bypasses the nonlinear activation function in each FFN layer,
thereby forming a linear pathway that enables structural reparameterization
during inference. This mechanism results in a family of ReParameterizable
Vision Transformers (RePaViTs), which achieve remarkable latency reductions
with acceptable sacrifices (sometimes gains) in accuracy across various ViTs.
The benefits of our method scale consistently with model sizes, demonstrating
greater speed improvements and progressively narrowing accuracy gaps or even
higher accuracies on larger models. In particular, RePa-ViT-Large and
RePa-ViT-Huge enjoy 66.8% and 68.7% speed-ups with +1.7% and +1.1% higher top-1
accuracies under the same training strategy, respectively. RePaViT is the first
to employ structural reparameterization on FFN layers to expedite ViTs to our
best knowledge, and we believe that it represents an auspicious direction for
efficient ViTs. Source code is available at
https://github.com/Ackesnal/RePaViT.

</details>


### [334] [GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning](https://arxiv.org/abs/2505.21863)
*Shikhhar Siingh,Abhinav Rawat,Vivek Gupta,Chitta Baral*

Main category: cs.CV

TL;DR: 论文提出GETReason框架和GREAT评估指标，通过地理时空和事件信息增强图像上下文理解。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以准确提取公开事件图像中的上下文信息，而这些信息对新闻和教育至关重要。

Method: 采用分层多智能体方法，结合地理时空和事件信息进行推理，并引入GREAT指标评估理解效果。

Result: 实验表明，该方法能有效推断图像深层含义，并将其与更广泛的事件背景联系起来。

Conclusion: GETReason框架和GREAT指标显著提升了图像上下文理解的准确性和深度。

Abstract: Publicly significant images from events hold valuable contextual information,
crucial for journalism and education. However, existing methods often struggle
to extract this relevance accurately. To address this, we introduce GETReason
(Geospatial Event Temporal Reasoning), a framework that moves beyond
surface-level image descriptions to infer deeper contextual meaning. We propose
that extracting global event, temporal, and geospatial information enhances
understanding of an image's significance. Additionally, we introduce GREAT
(Geospatial Reasoning and Event Accuracy with Temporal Alignment), a new metric
for evaluating reasoning-based image understanding. Our layered multi-agent
approach, assessed using a reasoning-weighted metric, demonstrates that
meaningful insights can be inferred, effectively linking images to their
broader event context.

</details>


### [335] [Beyond Perception: Evaluating Abstract Visual Reasoning through Multi-Stage Task](https://arxiv.org/abs/2505.21850)
*Yanbei Jiang,Yihao Ding,Chao Lei,Jiayang Ao,Jey Han Lau,Krista A. Ehinger*

Main category: cs.CV

TL;DR: 当前多模态大语言模型（MLLMs）在抽象视觉推理（AVR）方面表现不足，本文提出MultiStAR基准和MSEval指标来评估多阶段推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在抽象视觉推理方面表现不佳，且现有基准和指标仅关注单步推理和最终结果，忽略了多阶段推理过程。

Method: 提出MultiStAR基准（基于RAVEN）和MSEval指标，评估MLLMs在不同复杂度下的多阶段推理能力。

Result: 实验表明，MLLMs在基础感知任务表现尚可，但在复杂规则检测阶段仍面临挑战。

Conclusion: MultiStAR和MSEval填补了AVR评估空白，揭示了MLLMs在复杂推理中的局限性。

Abstract: Current Multimodal Large Language Models (MLLMs) excel in general visual
reasoning but remain underexplored in Abstract Visual Reasoning (AVR), which
demands higher-order reasoning to identify abstract rules beyond simple
perception. Existing AVR benchmarks focus on single-step reasoning, emphasizing
the end result but neglecting the multi-stage nature of reasoning process. Past
studies found MLLMs struggle with these benchmarks, but it doesn't explain how
they fail. To address this gap, we introduce MultiStAR, a Multi-Stage AVR
benchmark, based on RAVEN, designed to assess reasoning across varying levels
of complexity. Additionally, existing metrics like accuracy only focus on the
final outcomes while do not account for the correctness of intermediate steps.
Therefore, we propose a novel metric, MSEval, which considers the correctness
of intermediate steps in addition to the final outcomes. We conduct
comprehensive experiments on MultiStAR using 17 representative close-source and
open-source MLLMs. The results reveal that while existing MLLMs perform
adequately on basic perception tasks, they continue to face challenges in more
complex rule detection stages.

</details>


### [336] [Rethinking Gradient-based Adversarial Attacks on Point Cloud Classification](https://arxiv.org/abs/2505.21854)
*Jun Chen,Xinke Li,Mingyue Xu,Tianrui Li,Chongshou Li*

Main category: cs.CV

TL;DR: 本文提出两种新策略改进基于梯度的点云对抗攻击：WAAttack框架通过加权梯度和自适应步长实现精准扰动；SubAttack通过子集分解聚焦关键区域。实验证明该方法在生成隐蔽对抗样本上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的点云对抗攻击方法采用均匀更新规则，忽略了点云的异构性，导致扰动过大且易被察觉。论文旨在通过重新设计梯度更新机制，提升攻击效果和隐蔽性。

Method: 1. WAAttack框架：引入加权梯度和自适应步长策略，根据点的局部结构和敏感度动态调整扰动；2. SubAttack策略：将点云分解为子集，集中扰动结构关键区域。

Result: 大量实验表明，该方法在生成高度隐蔽的对抗样本方面优于当前最先进的基线方法。

Conclusion: 通过重新思考梯度更新机制，提出的WAAttack和SubAttack策略为3D点云分类对抗攻击提供了更有效且隐蔽的解决方案。

Abstract: Gradient-based adversarial attacks have become a dominant approach for
evaluating the robustness of point cloud classification models. However,
existing methods often rely on uniform update rules that fail to consider the
heterogeneous nature of point clouds, resulting in excessive and perceptible
perturbations. In this paper, we rethink the design of gradient-based attacks
by analyzing the limitations of conventional gradient update mechanisms and
propose two new strategies to improve both attack effectiveness and
imperceptibility. First, we introduce WAAttack, a novel framework that
incorporates weighted gradients and an adaptive step-size strategy to account
for the non-uniform contribution of points during optimization. This approach
enables more targeted and subtle perturbations by dynamically adjusting updates
according to the local structure and sensitivity of each point. Second, we
propose SubAttack, a complementary strategy that decomposes the point cloud
into subsets and focuses perturbation efforts on structurally critical regions.
Together, these methods represent a principled rethinking of gradient-based
adversarial attacks for 3D point cloud classification. Extensive experiments
demonstrate that our approach outperforms state-of-the-art baselines in
generating highly imperceptible adversarial examples. Code will be released
upon paper acceptance.

</details>


### [337] [Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation](https://arxiv.org/abs/2505.21956)
*Mengdan Zhu,Senhao Cheng,Guangji Bai,Yifei Zhang,Liang Zhao*

Main category: cs.CV

TL;DR: 提出跨模态RAG框架，通过分解查询和图像为子维度组件，结合稀疏和稠密检索策略，显著提升文本到图像生成的检索与生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成方法在复杂查询时表现不佳，因单一图像难以涵盖所有需求元素，需更细粒度的跨模态检索与生成方法。

Method: 提出混合检索策略（稀疏+稠密检索器）获取帕累托最优图像集，并利用多模态大模型选择性融合视觉特征进行子查询感知的图像合成。

Result: 在MS-COCO等5个数据集上，跨模态RAG在检索和生成质量上均显著超越基线，同时保持高效性。

Conclusion: 该框架通过子维度分解和混合检索有效解决了复杂查询下的图像生成瓶颈，为领域特定知识整合提供了新思路。

Abstract: Text-to-image generation increasingly demands access to domain-specific,
fine-grained, and rapidly evolving knowledge that pretrained models cannot
fully capture. Existing Retrieval-Augmented Generation (RAG) methods attempt to
address this by retrieving globally relevant images, but they fail when no
single image contains all desired elements from a complex user query. We
propose Cross-modal RAG, a novel framework that decomposes both queries and
images into sub-dimensional components, enabling subquery-aware retrieval and
generation. Our method introduces a hybrid retrieval strategy - combining a
sub-dimensional sparse retriever with a dense retriever - to identify a
Pareto-optimal set of images, each contributing complementary aspects of the
query. During generation, a multimodal large language model is guided to
selectively condition on relevant visual features aligned to specific
subqueries, ensuring subquery-aware image synthesis. Extensive experiments on
MS-COCO, Flickr30K, WikiArt, CUB, and ImageNet-LT demonstrate that Cross-modal
RAG significantly outperforms existing baselines in both retrieval and
generation quality, while maintaining high efficiency.

</details>


### [338] [EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video Guidance](https://arxiv.org/abs/2505.21876)
*Zun Wang,Jaemin Cho,Jialu Li,Han Lin,Jaehong Yoon,Yue Zhang,Mohit Bansal*

Main category: cs.CV

TL;DR: EPiC提出了一种无需昂贵相机轨迹标注的高效精确3D相机控制框架，通过基于首帧可见性的掩码视频构建高质量锚点视频，并结合轻量级Anchor-ControlNet模块，实现了在视频扩散模型中的精确相机控制。


<details>
  <summary>Details</summary>
Motivation: 现有的3D相机控制方法依赖于点云估计生成的锚点视频，但点云估计的误差会导致锚点视频不准确，且需要大量相机轨迹标注，资源消耗大。EPiC旨在解决这些问题。

Method: EPiC通过基于首帧可见性的掩码视频自动构建高质量锚点视频，无需相机轨迹标注，并引入Anchor-ControlNet模块，以轻量级方式将锚点视频引导集成到预训练的视频扩散模型中。

Result: EPiC在RealEstate10K和MiraData数据集上实现了最先进的性能，展示了精确和鲁棒的相机控制能力，并在零样本视频到视频场景中表现出强大的泛化能力。

Conclusion: EPiC通过创新的锚点视频构建方法和轻量级控制模块，实现了高效、精确的3D相机控制，显著减少了资源需求，同时保持了高性能和强泛化能力。

Abstract: Recent approaches on 3D camera control in video diffusion models (VDMs) often
create anchor videos to guide diffusion models as a structured prior by
rendering from estimated point clouds following annotated camera trajectories.
However, errors inherent in point cloud estimation often lead to inaccurate
anchor videos. Moreover, the requirement for extensive camera trajectory
annotations further increases resource demands. To address these limitations,
we introduce EPiC, an efficient and precise camera control learning framework
that automatically constructs high-quality anchor videos without expensive
camera trajectory annotations. Concretely, we create highly precise anchor
videos for training by masking source videos based on first-frame visibility.
This approach ensures high alignment, eliminates the need for camera trajectory
annotations, and thus can be readily applied to any in-the-wild video to
generate image-to-video (I2V) training pairs. Furthermore, we introduce
Anchor-ControlNet, a lightweight conditioning module that integrates anchor
video guidance in visible regions to pretrained VDMs, with less than 1% of
backbone model parameters. By combining the proposed anchor video data and
ControlNet module, EPiC achieves efficient training with substantially fewer
parameters, training steps, and less data, without requiring modifications to
the diffusion model backbone typically needed to mitigate rendering
misalignments. Although being trained on masking-based anchor videos, our
method generalizes robustly to anchor videos made with point clouds during
inference, enabling precise 3D-informed camera control. EPiC achieves SOTA
performance on RealEstate10K and MiraData for I2V camera control task,
demonstrating precise and robust camera control ability both quantitatively and
qualitatively. Notably, EPiC also exhibits strong zero-shot generalization to
video-to-video scenarios.

</details>


### [339] [Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language](https://arxiv.org/abs/2505.22146)
*Guangfu Hao,Haojie Wen,Liangxuna Guo,Yang Chen,Yanchao Bi,Shan Yu*

Main category: cs.CV

TL;DR: 该研究开发了一个结合视觉和语言理解的框架，用于模拟人类工具选择能力，性能接近大型模型但参数更少。


<details>
  <summary>Details</summary>
Motivation: 人类灵活选择工具的能力是其认知能力的体现，但目前的计算模型在这方面仍有不足。研究旨在开发一个能模拟这一能力的参数高效且可解释的模型。

Method: 使用低维属性表示连接视觉工具感知和语言任务理解。构建了包含115种工具和13种属性的数据集ToolNet，结合视觉编码器（ResNet或ViT）和微调语言模型（GPT-2、LLaMA、DeepSeek）进行工具选择。

Result: 该方法在工具选择任务中达到74%的准确率，显著优于直接工具匹配（20%）和较小多模态模型（21%-58%），接近GPT-4o（73%）的性能，但参数更少。操纵相关属性（如抓握性、手部关联性、延伸性）在多模态中最关键。

Conclusion: 该研究提供了一个参数高效且可解释的解决方案，模拟人类工具认知能力，推动了认知科学理解和工具选择任务的实际应用。

Abstract: Flexible tool selection reflects a complex cognitive ability that
distinguishes humans from other species, yet computational models that capture
this ability remain underdeveloped. We developed a framework using
low-dimensional attribute representations to bridge visual tool perception and
linguistic task understanding. We constructed a comprehensive dataset (ToolNet)
containing 115 common tools labeled with 13 carefully designed attributes
spanning physical, functional, and psychological properties, paired with
natural language scenarios describing tool usage. Visual encoders (ResNet or
ViT) extract attributes from tool images while fine-tuned language models
(GPT-2, LLaMA, DeepSeek) derive required attributes from task descriptions. Our
approach achieves 74% accuracy in tool selection tasks-significantly
outperforming direct tool matching (20%) and smaller multimodal models
(21%-58%), while approaching performance of much larger models like GPT-4o
(73%) with substantially fewer parameters. Ablation studies revealed that
manipulation-related attributes (graspability, hand-relatedness, elongation)
consistently prove most critical across modalities. This work provides a
parameter-efficient, interpretable solution that mimics human-like tool
cognition, advancing both cognitive science understanding and practical
applications in tool selection tasks.

</details>


### [340] [Improving Brain-to-Image Reconstruction via Fine-Grained Text Bridging](https://arxiv.org/abs/2505.22150)
*Runze Xia,Shuo Feng,Renzhi Wang,Congchi Yin,Xuyun Wen,Piji Li*

Main category: cs.CV

TL;DR: 该论文提出了一种名为FgB2I的方法，通过细粒度文本作为桥梁来改善脑信号到图像的精细重建。


<details>
  <summary>Details</summary>
Motivation: 现有的脑信号到图像重建方法往往缺乏细节和语义一致性，可能由于语义信息不足导致。

Method: FgB2I方法包括三个关键阶段：细节增强、解码细粒度文本描述和基于文本的脑信号到图像重建。利用大型视觉语言模型生成细粒度描述，并通过三个奖励指标引导语言模型解码fMRI信号。

Result: 细粒度文本描述可以整合到现有重建方法中，实现更精细的脑信号到图像重建。

Conclusion: FgB2I方法通过引入细粒度文本描述，显著提升了脑信号到图像重建的细节和语义一致性。

Abstract: Brain-to-Image reconstruction aims to recover visual stimuli perceived by
humans from brain activity. However, the reconstructed visual stimuli often
missing details and semantic inconsistencies, which may be attributed to
insufficient semantic information. To address this issue, we propose an
approach named Fine-grained Brain-to-Image reconstruction (FgB2I), which
employs fine-grained text as bridge to improve image reconstruction. FgB2I
comprises three key stages: detail enhancement, decoding fine-grained text
descriptions, and text-bridged brain-to-image reconstruction. In the
detail-enhancement stage, we leverage large vision-language models to generate
fine-grained captions for visual stimuli and experimentally validate its
importance. We propose three reward metrics (object accuracy, text-image
semantic similarity, and image-image semantic similarity) to guide the language
model in decoding fine-grained text descriptions from fMRI signals. The
fine-grained text descriptions can be integrated into existing reconstruction
methods to achieve fine-grained Brain-to-Image reconstruction.

</details>


### [341] [Look & Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in Multimodal Large Language Models for Chest X-ray Report Generation](https://arxiv.org/abs/2505.22222)
*Yunsoo Kim,Jinge Wu,Su-Hwan Kim,Pardeep Vasudev,Jiashu Shen,Honghan Wu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Look & Mark（L&M）的新方法，通过结合放射科医生的视觉注视和边界框标注，提升多模态大语言模型在胸片报告生成中的准确性和可靠性，减少了临床错误。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型在医学图像分析中取得了进展，但在胸片报告生成中仍存在幻觉和临床错误，限制了其在实际应用中的可靠性。

Method: 提出Look & Mark（L&M）方法，结合放射科医生的视觉注视（Look）和边界框标注（Mark），通过上下文学习提升模型性能，无需重新训练。

Result: L&M方法在多个模型上显著提升了性能，包括CXR-LLaVA的1.2%整体指标提升和LLaVA-Med的9.2%提升。通用模型LLaVA-OV达到了87.3%的临床平均性能，超越了专门训练的模型。专家评估显示，L&M减少了临床错误（每份报告平均减少0.43个错误）。

Conclusion: L&M作为一种可扩展且高效的解决方案，有望提升AI辅助放射学的诊断流程，尤其在资源有限的临床环境中。

Abstract: Recent advancements in multimodal Large Language Models (LLMs) have
significantly enhanced the automation of medical image analysis, particularly
in generating radiology reports from chest X-rays (CXR). However, these models
still suffer from hallucinations and clinically significant errors, limiting
their reliability in real-world applications. In this study, we propose Look &
Mark (L&M), a novel grounding fixation strategy that integrates radiologist eye
fixations (Look) and bounding box annotations (Mark) into the LLM prompting
framework. Unlike conventional fine-tuning, L&M leverages in-context learning
to achieve substantial performance gains without retraining. When evaluated
across multiple domain-specific and general-purpose models, L&M demonstrates
significant gains, including a 1.2% improvement in overall metrics (A.AVG) for
CXR-LLaVA compared to baseline prompting and a remarkable 9.2% boost for
LLaVA-Med. General-purpose models also benefit from L&M combined with
in-context learning, with LLaVA-OV achieving an 87.3% clinical average
performance (C.AVG)-the highest among all models, even surpassing those
explicitly trained for CXR report generation. Expert evaluations further
confirm that L&M reduces clinically significant errors (by 0.43 average errors
per report), such as false predictions and omissions, enhancing both accuracy
and reliability. These findings highlight L&M's potential as a scalable and
efficient solution for AI-assisted radiology, paving the way for improved
diagnostic workflows in low-resource clinical settings.

</details>


### [342] [CAST: Contrastive Adaptation and Distillation for Semi-Supervised Instance Segmentation](https://arxiv.org/abs/2505.21904)
*Pardis Taghavi,Tian Liu,Renjie Li,Reza Langari,Zhengzhong Tu*

Main category: cs.CV

TL;DR: CAST框架通过半监督知识蒸馏压缩预训练视觉基础模型，利用少量标注和大量未标注数据提升实例分割性能。


<details>
  <summary>Details</summary>
Motivation: 实例分割需要昂贵的像素级标注和大模型，CAST旨在通过半监督学习减少标注需求并压缩模型规模。

Method: CAST分为三阶段：1) 通过自训练和对比像素校准进行域适应；2) 使用多目标损失蒸馏到紧凑学生模型；3) 微调去除伪标签偏差。核心是实例感知像素级对比损失。

Result: 在Cityscapes和ADE20K上，11倍小的学生模型超过其教师模型3.4 AP和1.5 AP，并优于最先进的半监督方法。

Conclusion: CAST有效利用未标注数据，通过对比学习提升小模型性能，为实例分割提供了高效解决方案。

Abstract: Instance segmentation demands costly per-pixel annotations and large models.
We introduce CAST, a semi-supervised knowledge distillation (SSKD) framework
that compresses pretrained vision foundation models (VFM) into compact experts
using limited labeled and abundant unlabeled data. CAST unfolds in three
stages: (1) domain adaptation of the VFM teacher(s) via self-training with
contrastive pixel calibration, (2) distillation into a compact student via a
unified multi-objective loss that couples standard supervision and
pseudo-labels with our instance-aware pixel-wise contrastive term, and (3)
fine-tuning on labeled data to remove residual pseudo-label bias. Central to
CAST is an \emph{instance-aware pixel-wise contrastive loss} that fuses mask
and class scores to mine informative negatives and enforce clear inter-instance
margins. By maintaining this contrastive signal across both adaptation and
distillation, we align teacher and student embeddings and fully leverage
unlabeled images. On Cityscapes and ADE20K, our ~11X smaller student surpasses
its adapted VFM teacher(s) by +3.4 AP (33.9 vs. 30.5) and +1.5 AP (16.7 vs.
15.2) and outperforms state-of-the-art semi-supervised approaches.

</details>


### [343] [Fostering Video Reasoning via Next-Event Prediction](https://arxiv.org/abs/2505.22457)
*Haonan Wang,Hongfu Liu,Xiangyan Liu,Chao Du,Kenji Kawaguchi,Ye Wang,Tianyu Pang*

Main category: cs.CV

TL;DR: 论文提出了一种名为'下一事件预测'(NEP)的自监督学习任务，通过利用未来视频片段来增强多模态大语言模型(MLLM)的时间推理能力，并构建了V1-33K数据集和FutureBench评估基准。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答任务依赖人工标注或强MLLM，而视频描述任务易混淆时空信息。为填补这一空白，需要开发能自主培养时间推理能力的学习范式。

Method: 提出NEP任务：将视频分割为过去/未来帧，MLLM根据过去帧预测未来事件摘要。配套构建33K视频片段数据集V1-33K，并探索不同视频指令微调策略。

Result: 实验验证NEP能有效提升MLLM的时间推理能力，FutureBench评估显示模型在预测未见未来事件时具有良好连贯性。

Conclusion: NEP为MLLM时间推理能力培养提供了可扩展的自监督训练范式，未来视频片段可作为丰富的学习信号源。

Abstract: Next-token prediction serves as the foundational learning task enabling
reasoning in LLMs. But what should the learning task be when aiming to equip
MLLMs with temporal reasoning capabilities over video inputs? Existing tasks
such as video question answering often rely on annotations from humans or much
stronger MLLMs, while video captioning tends to entangle temporal reasoning
with spatial information. To address this gap, we propose next-event prediction
(NEP), a learning task that harnesses future video segments as a rich,
self-supervised signal to foster temporal reasoning. We segment each video into
past and future frames: the MLLM takes the past frames as input and predicts a
summary of events derived from the future frames, thereby encouraging the model
to reason temporally in order to complete the task. To support this task, we
curate V1-33K, a dataset comprising 33,000 automatically extracted video
segments spanning diverse real-world scenarios. We further explore a range of
video instruction-tuning strategies to study their effects on temporal
reasoning. To evaluate progress, we introduce FutureBench to assess coherence
in predicting unseen future events. Experiments validate that NEP offers a
scalable and effective training paradigm for fostering temporal reasoning in
MLLMs.

</details>


### [344] [UniTalk: Towards Universal Active Speaker Detection in Real World Scenarios](https://arxiv.org/abs/2505.21954)
*Le Thien Phuc Nguyen,Zhuoran Yu,Khoa Quang Nhat Cao,Yuwei Guo,Tu Ho Manh Pham,Tuan Tai Nguyen,Toan Ngo Duc Vo,Lucas Poon,Soochahn Lee,Yong Jae Lee*

Main category: cs.CV

TL;DR: UniTalk是一个专注于真实复杂场景的主动说话人检测新数据集，旨在提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准如AVA主要基于老电影，存在领域差距，无法充分反映真实场景的多样性（如多语言、嘈杂背景、多人同时说话等）。

Method: 构建包含44.5小时视频、48,693个说话身份的标注数据集，覆盖多样化真实场景，并进行严格评估。

Result: SOTA模型在AVA上接近满分，但在UniTalk上表现不佳；使用UniTalk训练的模型对Talkies、ASW等现代数据集展现更强泛化性。

Conclusion: UniTalk为主动说话人检测设立了新基准，证明当前技术尚未解决真实场景问题，需开发更具鲁棒性的模型。

Abstract: We present UniTalk, a novel dataset specifically designed for the task of
active speaker detection, emphasizing challenging scenarios to enhance model
generalization. Unlike previously established benchmarks such as AVA, which
predominantly features old movies and thus exhibits significant domain gaps,
UniTalk focuses explicitly on diverse and difficult real-world conditions.
These include underrepresented languages, noisy backgrounds, and crowded scenes
- such as multiple visible speakers speaking concurrently or in overlapping
turns. It contains over 44.5 hours of video with frame-level active speaker
annotations across 48,693 speaking identities, and spans a broad range of video
types that reflect real-world conditions. Through rigorous evaluation, we show
that state-of-the-art models, while achieving nearly perfect scores on AVA,
fail to reach saturation on UniTalk, suggesting that the ASD task remains far
from solved under realistic conditions. Nevertheless, models trained on UniTalk
demonstrate stronger generalization to modern "in-the-wild" datasets like
Talkies and ASW, as well as to AVA. UniTalk thus establishes a new benchmark
for active speaker detection, providing researchers with a valuable resource
for developing and evaluating versatile and resilient models.
  Dataset: https://huggingface.co/datasets/plnguyen2908/UniTalk-ASD
  Code: https://github.com/plnguyen2908/UniTalk-ASD-code

</details>


### [345] [Thinking with Generated Images](https://arxiv.org/abs/2505.22525)
*Ethan Chern,Zhulin Hu,Steffi Chern,Siqi Kou,Jiadi Su,Yan Ma,Zhijie Deng,Pengfei Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为'Thinking with Generated Images'的新范式，通过让大型多模态模型（LMMs）自主生成中间视觉思考步骤，从根本上改变了它们处理视觉推理的方式。该方法在复杂视觉任务上实现了50%的相对性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前大型多模态模型在视觉推理方面存在局限：要么只能处理用户提供的固定图像，要么仅能通过文本链式思考进行推理。这限制了模型在视觉想象和迭代优化方面的能力，而这些正是人类创造性思维的关键特征。

Method: 提出两种互补机制：(1)带中间视觉子目标的视觉生成：将复杂视觉任务分解为可管理的组件并逐步生成整合；(2)带自我批判的视觉生成：模型生成初始视觉假设后，通过文本推理分析其不足并生成优化版本。

Result: 在视觉生成基准测试中，该方法相比基线取得了显著改进，处理复杂多对象场景时相对性能提升达50%（从38%提升至57%）。

Conclusion: 该研究为AI模型赋予了类似人类的视觉想象和迭代优化能力，可广泛应用于生物化学、建筑设计、法医分析和体育策略等领域。作者已开源相关工具套件。

Abstract: We present Thinking with Generated Images, a novel paradigm that
fundamentally transforms how large multimodal models (LMMs) engage with visual
reasoning by enabling them to natively think across text and vision modalities
through spontaneous generation of intermediate visual thinking steps. Current
visual reasoning with LMMs is constrained to either processing fixed
user-provided images or reasoning solely through text-based chain-of-thought
(CoT). Thinking with Generated Images unlocks a new dimension of cognitive
capability where models can actively construct intermediate visual thoughts,
critique their own visual hypotheses, and refine them as integral components of
their reasoning process. We demonstrate the effectiveness of our approach
through two complementary mechanisms: (1) vision generation with intermediate
visual subgoals, where models decompose complex visual tasks into manageable
components that are generated and integrated progressively, and (2) vision
generation with self-critique, where models generate an initial visual
hypothesis, analyze its shortcomings through textual reasoning, and produce
refined outputs based on their own critiques. Our experiments on vision
generation benchmarks show substantial improvements over baseline approaches,
with our models achieving up to 50% (from 38% to 57%) relative improvement in
handling complex multi-object scenarios. From biochemists exploring novel
protein structures, and architects iterating on spatial designs, to forensic
analysts reconstructing crime scenes, and basketball players envisioning
strategic plays, our approach enables AI models to engage in the kind of visual
imagination and iterative refinement that characterizes human creative,
analytical, and strategic thinking. We release our open-source suite at
https://github.com/GAIR-NLP/thinking-with-generated-images.

</details>


### [346] [Towards Comprehensive Scene Understanding: Integrating First and Third-Person Views for LVLMs](https://arxiv.org/abs/2505.21955)
*Insu Lee,Wooje Park,Jaeyun Jang,Minyoung Noh,Kyuhong Shim,Byonghyo Shim*

Main category: cs.CV

TL;DR: 论文提出了一种结合第一人称和第三人称视角的框架E3VQA和提示技术M3CoT，以增强大视觉语言模型在多视角问答中的表现。


<details>
  <summary>Details</summary>
Motivation: 第一人称视角在交互应用中存在视野狭窄和全局上下文缺失的问题，导致在空间或上下文要求高的查询中表现不佳。

Method: 引入E3VQA基准和M3CoT提示技术，通过整合多视角场景图构建统一场景表示。

Result: M3CoT在GPT-4o和Gemini 2.0 Flash上分别提升了4.84%和5.94%的性能。

Conclusion: 结合第一人称和第三人称视角能有效提升大视觉语言模型的多视角推理能力。

Abstract: Large vision-language models (LVLMs) are increasingly deployed in interactive
applications such as virtual and augmented reality, where first-person
(egocentric) view captured by head-mounted cameras serves as key input. While
this view offers fine-grained cues about user attention and hand-object
interactions, their narrow field of view and lack of global context often lead
to failures on spatially or contextually demanding queries. To address this, we
introduce a framework that augments egocentric inputs with third-person
(exocentric) views, providing complementary information such as global scene
layout and object visibility to LVLMs. We present E3VQA, the first benchmark
for multi-view question answering with 4K high-quality question-answer pairs
grounded in synchronized ego-exo image pairs. Additionally, we propose M3CoT, a
training-free prompting technique that constructs a unified scene
representation by integrating scene graphs from three complementary
perspectives. M3CoT enables LVLMs to reason more effectively across views,
yielding consistent performance gains (4.84% for GPT-4o and 5.94% for Gemini
2.0 Flash) over a recent CoT baseline. Our extensive evaluation reveals key
strengths and limitations of LVLMs in multi-view reasoning and highlights the
value of leveraging both egocentric and exocentric inputs.

</details>


### [347] [Learning Shared Representations from Unpaired Data](https://arxiv.org/abs/2505.21524)
*Amitai Yacobi,Nir Ben-Ari,Ronen Talmon,Uri Shaham*

Main category: cs.CV

TL;DR: 该论文提出了一种仅需非配对数据即可学习跨模态共享表示的新方法，通过单模态表示的随机游走矩阵谱嵌入实现，在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前跨模态表示学习严重依赖配对样本，但配对数据获取难度大。本文探索如何利用更易获取的非配对数据学习共享表示空间。

Method: 基于各单模态表示独立构建的随机游走矩阵的谱嵌入，实现非配对数据的共享表示学习。

Result: 在CV和NLP领域的检索、生成、算术运算、零样本和跨域分类任务中验证了方法的有效性。

Conclusion: 首次证明非配对数据可学习通用跨模态嵌入，代码已开源。

Abstract: Learning shared representations is a primary area of multimodal
representation learning. The current approaches to achieve a shared embedding
space rely heavily on paired samples from each modality, which are
significantly harder to obtain than unpaired ones. In this work, we demonstrate
that shared representations can be learned almost exclusively from unpaired
data. Our arguments are grounded in the spectral embeddings of the random walk
matrices constructed independently from each unimodal representation. Empirical
results in computer vision and natural language processing domains support its
potential, revealing the effectiveness of unpaired data in capturing meaningful
cross-modal relations, demonstrating high capabilities in retrieval tasks,
generation, arithmetics, zero-shot, and cross-domain classification. This work,
to the best of our knowledge, is the first to demonstrate these capabilities
almost exclusively from unpaired samples, giving rise to a cross-modal
embedding that could be viewed as universal, i.e., independent of the specific
modalities of the data. Our code IS publicly available at
https://github.com/shaham-lab/SUE.

</details>


### [348] [RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction](https://arxiv.org/abs/2505.22613)
*Yuchi Wang,Yishuo Cai,Shuhuai Ren,Sihan Yang,Linli Yao,Yuanxin Liu,Yuanxing Zhang,Pengfei Wan,Xu Sun*

Main category: cs.CV

TL;DR: 提出RICO框架，通过视觉重建迭代优化图像描述，解决现有方法因幻觉和细节缺失导致的不准确问题，并引入RICO-Flash降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLM的图像描述增强方法存在幻觉和细节缺失问题，导致描述不准确和不完整。

Method: 利用文本到图像模型重建参考图像，通过MLLM比较原始与重建图像的差异迭代优化描述，并提出轻量级RICO-Flash。

Result: 在CapsBench和CompreCap上准确率和完整性显著提升，优于基线约10%。

Conclusion: RICO通过视觉重建迭代机制生成更忠实、全面的描述，同时RICO-Flash有效平衡性能与计算成本。

Abstract: Image recaptioning is widely used to generate training datasets with enhanced
quality for various multimodal tasks. Existing recaptioning methods typically
rely on powerful multimodal large language models (MLLMs) to enhance textual
descriptions, but often suffer from inaccuracies due to hallucinations and
incompleteness caused by missing fine-grained details. To address these
limitations, we propose RICO, a novel framework that refines captions through
visual reconstruction. Specifically, we leverage a text-to-image model to
reconstruct a caption into a reference image, and prompt an MLLM to identify
discrepancies between the original and reconstructed images to refine the
caption. This process is performed iteratively, further progressively promoting
the generation of more faithful and comprehensive descriptions. To mitigate the
additional computational cost induced by the iterative process, we introduce
RICO-Flash, which learns to generate captions like RICO using DPO. Extensive
experiments demonstrate that our approach significantly improves caption
accuracy and completeness, outperforms most baselines by approximately 10% on
both CapsBench and CompreCap. Code released at
https://github.com/wangyuchi369/RICO.

</details>


### [349] [Sherlock: Self-Correcting Reasoning in Vision-Language Models](https://arxiv.org/abs/2505.22651)
*Yi Ding,Ruqi Zhang*

Main category: cs.CV

TL;DR: 论文提出Sherlock框架，通过自我校正提升视觉语言模型的推理能力，仅需少量标注数据即可实现持续自我改进，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLMs）在复杂多模态任务中表现良好，但仍存在对推理错误敏感、需大量标注数据或精确验证器、泛化能力有限等问题。为解决这些限制，研究探索了自我校正策略。

Method: 引入Sherlock框架，包含轨迹级自我校正目标、基于视觉扰动的偏好数据构建方法及动态β偏好调优。仅需2万随机标注数据即可赋予模型自我校正能力，并持续无监督自我改进。

Result: 基于Llama3.2-Vision-11B的Sherlock在8个基准测试中平均准确率达64.1（直接生成）和65.4（自我校正后），优于LLaVA-CoT（63.2）、Mulberry（63.9）等模型，且标注数据用量少于20%。

Conclusion: Sherlock通过自我校正显著提升了VLMs的推理能力，以更少标注数据实现高性能，为多模态推理模型的发展提供了新方向。

Abstract: Reasoning Vision-Language Models (VLMs) have shown promising performance on
complex multimodal tasks. However, they still face significant challenges: they
are highly sensitive to reasoning errors, require large volumes of annotated
data or accurate verifiers, and struggle to generalize beyond specific domains.
To address these limitations, we explore self-correction as a strategy to
enhance reasoning VLMs. We first conduct an in-depth analysis of reasoning
VLMs' self-correction abilities and identify key gaps. Based on our findings,
we introduce Sherlock, a self-correction and self-improvement training
framework. Sherlock introduces a trajectory-level self-correction objective, a
preference data construction method based on visual perturbation, and a dynamic
$\beta$ for preference tuning. Once the model acquires self-correction
capabilities using only 20k randomly sampled annotated data, it continues to
self-improve without external supervision. Built on the Llama3.2-Vision-11B
model, Sherlock achieves remarkable results across eight benchmarks, reaching
an average accuracy of 64.1 with direct generation and 65.4 after
self-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and
LlamaV-o1 (63.4) while using less than 20% of the annotated data.

</details>


### [350] [Self-Organizing Visual Prototypes for Non-Parametric Representation Learning](https://arxiv.org/abs/2505.21533)
*Thalles Silva,Helio Pedrini,Adín Ramírez Rivera*

Main category: cs.CV

TL;DR: 提出了一种名为SOP的无监督视觉特征学习新方法，通过多个语义相似的表示来增强原型特征编码，并在多个基准测试中取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有的原型自监督学习方法仅依赖单一原型编码隐藏簇的所有相关特征，限制了特征表征的完整性和训练效果。

Method: 提出SOP策略，用多个支持嵌入（SEs）互补表征原型，并引入两种新型非参数损失函数实现该策略，包括SOP-MIM任务。

Result: 在检索、线性评估、微调和目标检测等任务中，SOP预训练编码器达到SOTA性能，且复杂编码器带来更大增益。

Conclusion: SOP通过多视角非参数局部表征优化原型学习，验证了非参数自监督学习的可行性，并为复杂模型提供可扩展性优势。

Abstract: We present Self-Organizing Visual Prototypes (SOP), a new training technique
for unsupervised visual feature learning. Unlike existing prototypical
self-supervised learning (SSL) methods that rely on a single prototype to
encode all relevant features of a hidden cluster in the data, we propose the
SOP strategy. In this strategy, a prototype is represented by many semantically
similar representations, or support embeddings (SEs), each containing a
complementary set of features that together better characterize their region in
space and maximize training performance. We reaffirm the feasibility of
non-parametric SSL by introducing novel non-parametric adaptations of two loss
functions that implement the SOP strategy. Notably, we introduce the SOP Masked
Image Modeling (SOP-MIM) task, where masked representations are reconstructed
from the perspective of multiple non-parametric local SEs. We comprehensively
evaluate the representations learned using the SOP strategy on a range of
benchmarks, including retrieval, linear evaluation, fine-tuning, and object
detection. Our pre-trained encoders achieve state-of-the-art performance on
many retrieval benchmarks and demonstrate increasing performance gains with
more complex encoders.

</details>


### [351] [3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model](https://arxiv.org/abs/2505.22657)
*Wenbo Hu,Yining Hong,Yanjun Wang,Leison Gao,Zibu Wei,Xingcheng Yao,Nanyun Peng,Yonatan Bitton,Idan Szpektor,Kai-Wei Chang*

Main category: cs.CV

TL;DR: 论文提出3DLLM-Mem模型，通过动态内存管理提升大语言模型在3D环境中的时空推理能力，并在新基准3DMem-Bench上取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在动态3D多房间环境中缺乏有效的时空记忆建模能力，限制了其规划和行动表现。

Method: 提出3DLLM-Mem模型，利用工作记忆令牌选择性融合情景记忆中的时空特征，实现高效长时记忆推理。

Result: 模型在3DMem-Bench基准上以16.5%的优势超越基线，尤其在野外具身任务中表现突出。

Conclusion: 动态内存管理机制显著增强了大语言模型在复杂3D环境中的长期记忆推理和行动能力。

Abstract: Humans excel at performing complex tasks by leveraging long-term memory
across temporal and spatial experiences. In contrast, current Large Language
Models (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D
environments. We posit that part of this limitation is due to the lack of
proper 3D spatial-temporal memory modeling in LLMs. To address this, we first
introduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000
trajectories and 2,892 embodied tasks, question-answering and captioning,
designed to evaluate an agent's ability to reason over long-term memory in 3D
environments. Second, we propose 3DLLM-Mem, a novel dynamic memory management
and fusion model for embodied spatial-temporal reasoning and actions in LLMs.
Our model uses working memory tokens, which represents current observations, as
queries to selectively attend to and fuse the most useful spatial and temporal
features from episodic memory, which stores past observations and interactions.
Our approach allows the agent to focus on task-relevant information while
maintaining memory efficiency in complex, long-horizon environments.
Experimental results demonstrate that 3DLLM-Mem achieves state-of-the-art
performance across various tasks, outperforming the strongest baselines by
16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied
tasks.

</details>


### [352] [Corruption-Aware Training of Latent Video Diffusion Models for Robust Text-to-Video Generation](https://arxiv.org/abs/2505.21545)
*Chika Maduabuchi,Hao Chen,Yujin Han,Jindong Wang*

Main category: cs.CV

TL;DR: CAT-LVDM通过数据对齐的噪声注入提升视频扩散模型的鲁棒性，显著减少语义漂移和时间不连贯问题。


<details>
  <summary>Details</summary>
Motivation: 现有潜在视频扩散模型(LVDM)对不完美条件敏感，导致语义漂移和时间不连贯，尤其在噪声多的网络规模视频-文本数据集上表现不佳。

Method: 提出CAT-LVDM框架，包括批中心噪声注入(BCNI)和频谱感知上下文噪声(SACN)，通过结构化噪声注入提升模型鲁棒性。

Result: BCNI在WebVid-2M等数据集上平均降低FVD 31.9%，SACN在UCF-101上提升12.3%。理论分析证实了低秩数据对齐噪声的有效性。

Conclusion: CAT-LVDM为多模态噪声下的鲁棒视频扩散训练提供了原则性、可扩展的方法，显著提升了生成质量。

Abstract: Latent Video Diffusion Models (LVDMs) achieve high-quality generation but are
sensitive to imperfect conditioning, which causes semantic drift and temporal
incoherence on noisy, web-scale video-text datasets. We introduce CAT-LVDM, the
first corruption-aware training framework for LVDMs that improves robustness
through structured, data-aligned noise injection. Our method includes
Batch-Centered Noise Injection (BCNI), which perturbs embeddings along
intra-batch semantic directions to preserve temporal consistency. BCNI is
especially effective on caption-rich datasets like WebVid-2M, MSR-VTT, and
MSVD. We also propose Spectrum-Aware Contextual Noise (SACN), which injects
noise along dominant spectral directions to improve low-frequency smoothness,
showing strong results on UCF-101. On average, BCNI reduces FVD by 31.9% across
WebVid-2M, MSR-VTT, and MSVD, while SACN yields a 12.3% improvement on UCF-101.
Ablation studies confirm the benefit of low-rank, data-aligned noise. Our
theoretical analysis further explains how such perturbations tighten entropy,
Wasserstein, score-drift, mixing-time, and generalization bounds. CAT-LVDM
establishes a principled, scalable training approach for robust video diffusion
under multimodal noise. Code and models: https://github.com/chikap421/catlvdm

</details>


### [353] [Learning World Models for Interactive Video Generation](https://arxiv.org/abs/2505.21996)
*Taiye Chen,Xun Hu,Zihan Ding,Chi Jin*

Main category: cs.CV

TL;DR: 该论文通过动作条件化和自回归框架增强图像到视频模型的交互能力，提出视频检索增强生成（VRAG）方法，显著减少长期复合错误并提升时空一致性。


<details>
  <summary>Details</summary>
Motivation: 当前长视频生成模型因复合错误和记忆机制不足，导致世界建模能力有限，无法有效支持未来规划和动作选择。

Method: 采用动作条件化和自回归框架增强交互能力，并提出VRAG方法，通过显式全局状态条件化减少错误。

Result: VRAG显著降低了长期复合错误，提升了世界模型的时空一致性，而扩展上下文窗口和检索增强生成效果有限。

Conclusion: 研究揭示了视频世界模型的基本挑战，并为提升视频生成模型的世界建模能力建立了全面基准。

Abstract: Foundational world models must be both interactive and preserve
spatiotemporal coherence for effective future planning with action choices.
However, present models for long video generation have limited inherent world
modeling capabilities due to two main challenges: compounding errors and
insufficient memory mechanisms. We enhance image-to-video models with
interactive capabilities through additional action conditioning and
autoregressive framework, and reveal that compounding error is inherently
irreducible in autoregressive video generation, while insufficient memory
mechanism leads to incoherence of world models. We propose video retrieval
augmented generation (VRAG) with explicit global state conditioning, which
significantly reduces long-term compounding errors and increases spatiotemporal
consistency of world models. In contrast, naive autoregressive generation with
extended context windows and retrieval-augmented generation prove less
effective for video generation, primarily due to the limited in-context
learning capabilities of current video models. Our work illuminates the
fundamental challenges in video world models and establishes a comprehensive
benchmark for improving video generation models with internal world modeling
capabilities.

</details>


### [354] [GL-PGENet: A Parameterized Generation Framework for Robust Document Image Enhancement](https://arxiv.org/abs/2505.22021)
*Zhihong Tang,Yang Li*

Main category: cs.CV

TL;DR: 提出GL-PGENet网络，通过全局局部联合优化和参数化生成机制，实现多退化彩色文档图像的高效增强，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于单退化修复或灰度图像处理，无法满足真实场景中多退化彩色文档的增强需求。

Method: 1. 分层增强框架（全局校正+局部细化） 2. 双分支局部细化网络（参数化生成替代直接预测） 3. 改进的NestUNet结构（密集块融合多级特征） 4. 两阶段训练策略（合成数据预训练+任务微调）

Result: DocUNet数据集SSIM 0.7721，RealDAE数据集SSIM 0.9480，保持跨域适应性和计算效率。

Conclusion: GL-PGENet在质量和效率上均优于现有方法，具有实际应用价值。

Abstract: Document Image Enhancement (DIE) serves as a critical component in Document
AI systems, where its performance substantially determines the effectiveness of
downstream tasks. To address the limitations of existing methods confined to
single-degradation restoration or grayscale image processing, we present Global
with Local Parametric Generation Enhancement Network (GL-PGENet), a novel
architecture designed for multi-degraded color document images, ensuring both
efficiency and robustness in real-world scenarios. Our solution incorporates
three key innovations: First, a hierarchical enhancement framework that
integrates global appearance correction with local refinement, enabling
coarse-to-fine quality improvement. Second, a Dual-Branch Local-Refine Network
with parametric generation mechanisms that replaces conventional direct
prediction, producing enhanced outputs through learned intermediate parametric
representations rather than pixel-wise mapping. This approach enhances local
consistency while improving model generalization. Finally, a modified NestUNet
architecture incorporating dense block to effectively fuse low-level pixel
features and high-level semantic features, specifically adapted for document
image characteristics. In addition, to enhance generalization performance, we
adopt a two-stage training strategy: large-scale pretraining on a synthetic
dataset of 500,000+ samples followed by task-specific fine-tuning. Extensive
experiments demonstrate the superiority of GL-PGENet, achieving
state-of-the-art SSIM scores of 0.7721 on DocUNet and 0.9480 on RealDAE. The
model also exhibits remarkable cross-domain adaptability and maintains
computational efficiency for high-resolution images without performance
degradation, confirming its practical utility in real-world scenarios.

</details>


### [355] [Knowledge Distillation Approach for SOS Fusion Staging: Towards Fully Automated Skeletal Maturity Assessment](https://arxiv.org/abs/2505.21561)
*Omid Halimi Milani,Amanda Nikho,Marouane Tliba,Lauren Mills,Ahmet Enis Cetin,Mohammed H Elnagar*

Main category: cs.CV

TL;DR: 提出一种深度学习框架，通过师生模型架构和新型损失函数，实现蝶枕软骨融合的自动分期诊断，无需额外预处理。


<details>
  <summary>Details</summary>
Motivation: 蝶枕软骨融合是正畸和法医人类学的重要诊断标志，但现有方法依赖人工裁剪或YOLO分割，效率低且不一致。

Method: 采用师生模型架构：教师模型从裁剪图像学习空间特征，通过空间logits对齐和梯度注意力映射指导学生模型处理原始图像。

Result: 框架在专家数据验证下达到稳健诊断精度，形成端到端临床流程，提升骨骼成熟评估的效率和一致性。

Conclusion: 该方案消除了预处理需求，加速部署，为不同临床场景提供高效、标准化的骨骼成熟评估工具。

Abstract: We introduce a novel deep learning framework for the automated staging of
spheno-occipital synchondrosis (SOS) fusion, a critical diagnostic marker in
both orthodontics and forensic anthropology. Our approach leverages a
dual-model architecture wherein a teacher model, trained on manually cropped
images, transfers its precise spatial understanding to a student model that
operates on full, uncropped images. This knowledge distillation is facilitated
by a newly formulated loss function that aligns spatial logits as well as
incorporates gradient-based attention spatial mapping, ensuring that the
student model internalizes the anatomically relevant features without relying
on external cropping or YOLO-based segmentation. By leveraging expert-curated
data and feedback at each step, our framework attains robust diagnostic
accuracy, culminating in a clinically viable end-to-end pipeline. This
streamlined approach obviates the need for additional pre-processing tools and
accelerates deployment, thereby enhancing both the efficiency and consistency
of skeletal maturation assessment in diverse clinical settings.

</details>


### [356] [Multi-instance Learning as Downstream Task of Self-Supervised Learning-based Pre-trained Model](https://arxiv.org/abs/2505.21564)
*Koki Matsuishi,Tsuyoshi Okita*

Main category: cs.CV

TL;DR: 论文提出使用自监督预训练模型改进多示例学习，解决脑血肿CT图像中实例数量增加导致的分类性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 在脑血肿CT图像的多示例学习中，当每个包（bag）中的实例数量增加到256时，传统深度学习方法学习效果显著下降。

Method: 采用自监督预训练模型作为多示例学习器的下游任务，以缓解原始任务中的伪相关问题。

Result: 该方法在脑血肿CT的低密度标记分类任务中，准确率提升5%-13%，F1分数提升40%-55%。

Conclusion: 自监督预训练能有效提升多示例学习在医学图像分类中的性能，尤其在实例数量较多时。

Abstract: In deep multi-instance learning, the number of applicable instances depends
on the data set. In histopathology images, deep learning multi-instance
learners usually assume there are hundreds to thousands instances in a bag.
However, when the number of instances in a bag increases to 256 in brain
hematoma CT, learning becomes extremely difficult. In this paper, we address
this drawback. To overcome this problem, we propose using a pre-trained model
with self-supervised learning for the multi-instance learner as a downstream
task. With this method, even when the original target task suffers from the
spurious correlation problem, we show improvements of 5% to 13% in accuracy and
40% to 55% in the F1 measure for the hypodensity marker classification of brain
hematoma CT.

</details>


### [357] [Diffusion Model-based Activity Completion for AI Motion Capture from Videos](https://arxiv.org/abs/2505.21566)
*Gao Huayu,Huang Tengjiu,Ye Xiaolong,Tsuyoshi Okita*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的动作补全技术MDC-Net，用于解决AI动作捕捉中预定义动作限制的问题，并在Human3.6M数据集上取得了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 当前AI动作捕捉技术完全依赖观察到的视频序列，限制了动作的灵活性。本文旨在将AI动作捕捉应用于虚拟人，实现超出观察序列的灵活动作。

Method: 提出了一种基于扩散模型的动作补全技术，引入了门控模块和位置-时间嵌入模块，用于生成平滑连续的人体运动序列。

Result: MDC-Net在ADE、FDE和MMADE指标上优于现有方法，模型大小更小（16.84M），并能生成更自然连贯的运动序列。

Conclusion: MDC-Net通过扩散模型和创新的模块设计，有效解决了动作捕捉中的动作限制问题，为虚拟人动作生成提供了新的解决方案。

Abstract: AI-based motion capture is an emerging technology that offers a
cost-effective alternative to traditional motion capture systems. However,
current AI motion capture methods rely entirely on observed video sequences,
similar to conventional motion capture. This means that all human actions must
be predefined, and movements outside the observed sequences are not possible.
To address this limitation, we aim to apply AI motion capture to virtual
humans, where flexible actions beyond the observed sequences are required. We
assume that while many action fragments exist in the training data, the
transitions between them may be missing. To bridge these gaps, we propose a
diffusion-model-based action completion technique that generates complementary
human motion sequences, ensuring smooth and continuous movements. By
introducing a gate module and a position-time embedding module, our approach
achieves competitive results on the Human3.6M dataset. Our experimental results
show that (1) MDC-Net outperforms existing methods in ADE, FDE, and MMADE but
is slightly less accurate in MMFDE, (2) MDC-Net has a smaller model size
(16.84M) compared to HumanMAC (28.40M), and (3) MDC-Net generates more natural
and coherent motion sequences. Additionally, we propose a method for extracting
sensor data, including acceleration and angular velocity, from human motion
sequences.

</details>


### [358] [Balanced Token Pruning: Accelerating Vision Language Models Beyond Local Optimization](https://arxiv.org/abs/2505.22038)
*Kaiyuan Li,Xiaoyue Chen,Chen Gao,Yong Li,Xinlei Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为平衡令牌修剪（BTP）的即插即用方法，用于减少大型视觉语言模型中的图像令牌数量，从而降低计算开销，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）在处理多模态任务时，由于将图像编码为大量令牌，导致显著的计算开销。现有方法在修剪令牌时，往往忽略了修剪对当前层和后续层的联合影响，导致修剪决策不够优化。

Method: 提出的平衡令牌修剪（BTP）方法通过一个小型校准集将修剪过程分为多个阶段。早期阶段强调修剪对后续层的影响，而在深层阶段则侧重于保持局部输出的一致性。

Result: 实验表明，该方法在多个基准测试中表现出广泛的适用性，平均能在保留96.7%原始模型性能的同时，实现78%的压缩率。

Conclusion: 平衡令牌修剪（BTP）是一种有效的令牌修剪方法，能够在显著减少计算开销的同时，保持大型视觉语言模型的性能。

Abstract: Large Vision-Language Models (LVLMs) have shown impressive performance across
multi-modal tasks by encoding images into thousands of tokens. However, the
large number of image tokens results in significant computational overhead, and
the use of dynamic high-resolution inputs further increases this burden.
Previous approaches have attempted to reduce the number of image tokens through
token pruning, typically by selecting tokens based on attention scores or image
token diversity. Through empirical studies, we observe that existing methods
often overlook the joint impact of pruning on both the current layer's output
(local) and the outputs of subsequent layers (global), leading to suboptimal
pruning decisions. To address this challenge, we propose Balanced Token Pruning
(BTP), a plug-and-play method for pruning vision tokens. Specifically, our
method utilizes a small calibration set to divide the pruning process into
multiple stages. In the early stages, our method emphasizes the impact of
pruning on subsequent layers, whereas in the deeper stages, the focus shifts
toward preserving the consistency of local outputs. Extensive experiments
across various LVLMs demonstrate the broad effectiveness of our approach on
multiple benchmarks. Our method achieves a 78% compression rate while
preserving 96.7% of the original models' performance on average.

</details>


### [359] [EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models](https://arxiv.org/abs/2505.21567)
*Feng Jiang,Zihao Zheng,Xiuping Cui,Maoliang Li,JIayu Chen,Xiang Chen*

Main category: cs.CV

TL;DR: 提出EaqVLA框架，通过编码对齐量化优化VLA模型，降低计算/存储成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型因计算/存储成本高昂需优化，且传统量化方法受限于token对齐问题。

Method: 提出多粒度对齐分析方法及编码对齐感知的混合精度量化框架EaqVLA。

Result: EaqVLA在端到端动作控制中量化损失最小，并实现显著加速。

Conclusion: EaqVLA有效解决了VLA模型量化中的对齐问题，性能优于现有方法。

Abstract: With the development of Embodied Artificial intelligence, the end-to-end
control policy such as Vision-Language-Action (VLA) model has become the
mainstream. Existing VLA models faces expensive computing/storage cost, which
need to be optimized. Quantization is considered as the most effective method
which can not only reduce the memory cost but also achieve computation
acceleration. However, we find the token alignment of VLA models hinders the
application of existing quantization methods. To address this, we proposed an
optimized framework called EaqVLA, which apply encoding-aligned quantization to
VLA models. Specifically, we propose an complete analysis method to find the
misalignment in various granularity. Based on the analysis results, we propose
a mixed precision quantization with the awareness of encoding alignment.
Experiments shows that the porposed EaqVLA achieves better quantization
performance (with the minimal quantization loss for end-to-end action control
and xxx times acceleration) than existing quantization methods.

</details>


### [360] [From Failures to Fixes: LLM-Driven Scenario Repair for Self-Evolving Autonomous Driving](https://arxiv.org/abs/2505.22067)
*Xinyu Xia,Xingjun Ma,Yunfeng Hu,Ting Qu,Hong Chen,Xun Gong*

Main category: cs.CV

TL;DR: 论文提出SERA框架，利用LLM赋能自动驾驶系统，通过针对性场景推荐自我进化，有效修复故障案例并提升关键指标。


<details>
  <summary>Details</summary>
Motivation: 现有场景生成和选择方法缺乏适应性和语义相关性，限制了自动驾驶系统在安全关键场景下的性能提升。

Method: SERA通过分析性能日志识别故障模式，动态检索语义对齐场景，并基于LLM的反思机制优化推荐，最后进行小样本微调。

Result: 实验表明，SERA在多个自动驾驶基准测试中显著提升关键指标，验证了其在安全关键条件下的有效性和泛化能力。

Conclusion: SERA框架为自动驾驶系统提供了一种自适应、高效的故障修复方法，推动了系统在复杂场景下的稳健性。

Abstract: Ensuring robust and generalizable autonomous driving requires not only broad
scenario coverage but also efficient repair of failure cases, particularly
those related to challenging and safety-critical scenarios. However, existing
scenario generation and selection methods often lack adaptivity and semantic
relevance, limiting their impact on performance improvement. In this paper, we
propose \textbf{SERA}, an LLM-powered framework that enables autonomous driving
systems to self-evolve by repairing failure cases through targeted scenario
recommendation. By analyzing performance logs, SERA identifies failure patterns
and dynamically retrieves semantically aligned scenarios from a structured
bank. An LLM-based reflection mechanism further refines these recommendations
to maximize relevance and diversity. The selected scenarios are used for
few-shot fine-tuning, enabling targeted adaptation with minimal data.
Experiments on the benchmark show that SERA consistently improves key metrics
across multiple autonomous driving baselines, demonstrating its effectiveness
and generalizability under safety-critical conditions.

</details>


### [361] [Do We Need All the Synthetic Data? Towards Targeted Synthetic Image Augmentation via Diffusion Models](https://arxiv.org/abs/2505.21574)
*Dang Nguyen,Jiping Li,Jinghao Zheng,Baharan Mirzasoleiman*

Main category: cs.CV

TL;DR: 通过仅增强训练早期未学习的数据部分（30%-40%），该方法在多种场景下提升分类器性能达2.8%，且优于SOTA优化器SAM。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型数据增强方法需扩大数据集10-30倍以保证多样性，但难以平衡生成质量与计算成本。本文发现仅增强训练初期未被充分学习的数据，可更高效提升模型泛化能力。

Method: 提出选择性数据增强策略：通过分析双层CNN，仅对训练早期未被学习的数据（30%-40%）进行扩散模型增强，促进特征学习速度的均匀性。

Result: 在CIFAR-10/100、TinyImageNet上测试ResNet/ViT/DenseNet，性能最高提升2.8%。仅用SGD即超越SAM优化器，且能与现有强弱增强策略叠加。

Conclusion: 部分数据增强策略通过优化特征学习动态，以更低计算成本实现优于传统全域增强和SOTA优化器的性能，具备强兼容性。

Abstract: Synthetically augmenting training datasets with diffusion models has been an
effective strategy for improving generalization of image classifiers. However,
existing techniques struggle to ensure the diversity of generation and increase
the size of the data by up to 10-30x to improve the in-distribution
performance. In this work, we show that synthetically augmenting part of the
data that is not learned early in training outperforms augmenting the entire
dataset. By analyzing a two-layer CNN, we prove that this strategy improves
generalization by promoting homogeneity in feature learning speed without
amplifying noise. Our extensive experiments show that by augmenting only
30%-40% of the data, our method boosts the performance by up to 2.8% in a
variety of scenarios, including training ResNet, ViT and DenseNet on CIFAR-10,
CIFAR-100, and TinyImageNet, with a range of optimizers including SGD and SAM.
Notably, our method applied with SGD outperforms the SOTA optimizer, SAM, on
CIFAR-100 and TinyImageNet. It can also easily stack with existing weak and
strong augmentation strategies to further boost the performance.

</details>


### [362] [QuARI: Query Adaptive Retrieval Improvement](https://arxiv.org/abs/2505.21647)
*Eric Xing,Abby Stylianou,Robert Pless,Nathan Jacobs*

Main category: cs.CV

TL;DR: 该论文提出了一种通过学习查询特定的特征空间变换来改进大规模视觉语言模型在实例检索任务中性能的方法，计算成本低且效果优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模预训练的视觉语言模型在广泛的领域中表现出色，但在具有挑战性的检索任务（如超大规模图像集合中的实例检索）中表现不佳。现有方法通过线性变换强调感兴趣的子空间来提升性能，但仍有改进空间。

Method: 论文提出了一种更极端的专业化方法，通过学习将给定查询映射到查询特定的特征空间线性变换。这种线性变换可以以最小的计算成本应用于数百万图像嵌入，适用于大规模检索或重新排序。

Result: 实验结果表明，该方法在性能上一致优于现有最先进的方法，包括那些在查询时需要更多数量级计算的方法。

Conclusion: 通过学习查询特定的特征空间变换，可以在保持低计算成本的同时显著提升大规模实例检索任务的性能，为实际应用提供了有效的解决方案。

Abstract: Massive-scale pretraining has made vision-language models increasingly
popular for image-to-image and text-to-image retrieval across a broad
collection of domains. However, these models do not perform well when used for
challenging retrieval tasks, such as instance retrieval in very large-scale
image collections. Recent work has shown that linear transformations of VLM
features trained for instance retrieval can improve performance by emphasizing
subspaces that relate to the domain of interest. In this paper, we explore a
more extreme version of this specialization by learning to map a given query to
a query-specific feature space transformation. Because this transformation is
linear, it can be applied with minimal computational cost to millions of image
embeddings, making it effective for large-scale retrieval or re-ranking.
Results show that this method consistently outperforms state-of-the-art
alternatives, including those that require many orders of magnitude more
computation at query time.

</details>


### [363] [SridBench: Benchmark of Scientific Research Illustration Drawing of Image Generation Model](https://arxiv.org/abs/2505.22126)
*Yifan Chang,Yukang Feng,Jianwen Sun,Jiaxin Ai,Chuanhao Li,S. Kevin Zhou,Kaipeng Zhang*

Main category: cs.CV

TL;DR: 论文介绍了首个科学图表生成基准SridBench，评估AI在科学插图生成中的表现，发现即使顶级模型如GPT-4o-image仍落后于人类。


<details>
  <summary>Details</summary>
Motivation: 科学插图生成需要准确解读技术内容并将抽象概念转化为清晰、标准化的视觉呈现，但目前缺乏评估AI在此任务上的基准。

Method: 通过人类专家和多模态大语言模型（MLLMs）从13个自然科学和计算机科学领域的领先论文中收集1,120个样本，构建SridBench基准，并在六个维度上评估模型表现。

Result: 实验结果显示，即使是GPT-4o-image等顶级模型在文本/视觉清晰度和科学正确性方面仍存在常见问题，表现不及人类。

Conclusion: 研究强调需要更先进的推理驱动视觉生成能力，以提升AI在科学插图生成中的表现。

Abstract: Recent years have seen rapid advances in AI-driven image generation. Early
diffusion models emphasized perceptual quality, while newer multimodal models
like GPT-4o-image integrate high-level reasoning, improving semantic
understanding and structural composition. Scientific illustration generation
exemplifies this evolution: unlike general image synthesis, it demands accurate
interpretation of technical content and transformation of abstract ideas into
clear, standardized visuals. This task is significantly more
knowledge-intensive and laborious, often requiring hours of manual work and
specialized tools. Automating it in a controllable, intelligent manner would
provide substantial practical value. Yet, no benchmark currently exists to
evaluate AI on this front. To fill this gap, we introduce SridBench, the first
benchmark for scientific figure generation. It comprises 1,120 instances
curated from leading scientific papers across 13 natural and computer science
disciplines, collected via human experts and MLLMs. Each sample is evaluated
along six dimensions, including semantic fidelity and structural accuracy.
Experimental results reveal that even top-tier models like GPT-4o-image lag
behind human performance, with common issues in text/visual clarity and
scientific correctness. These findings highlight the need for more advanced
reasoning-driven visual generation capabilities.

</details>


### [364] [Real-Time Blind Defocus Deblurring for Earth Observation: The IMAGIN-e Mission Approach](https://arxiv.org/abs/2505.22128)
*Alejandro D. Mousist*

Main category: cs.CV

TL;DR: 该论文提出了一种针对ISS上IMAGIN-e任务中地球观测图像机械散焦的盲去模糊方法，适用于太空边缘计算环境，显著提升了图像质量。


<details>
  <summary>Details</summary>
Motivation: 解决国际空间站IMAGIN-e任务中地球观测图像因机械散焦导致的模糊问题，适应太空边缘计算资源受限的环境。

Method: 利用Sentinel-2数据估计散焦核，并在GAN框架下训练恢复模型，无需参考图像即可实现图像复原。

Result: 在合成退化的Sentinel-2图像上，SSIM提升72.47%，PSNR提升25.00%；在无参考图像的IMAGIN-e数据上，NIQE和BRISQUE分别提升60.66%和48.38%。

Conclusion: 该方法成功部署于IMAGIN-e任务，在资源受限条件下高效处理高分辨率图像，支持水体分割和轮廓检测等应用。

Abstract: This work addresses mechanical defocus in Earth observation images from the
IMAGIN-e mission aboard the ISS, proposing a blind deblurring approach adapted
to space-based edge computing constraints. Leveraging Sentinel-2 data, our
method estimates the defocus kernel and trains a restoration model within a GAN
framework, effectively operating without reference images.
  On Sentinel-2 images with synthetic degradation, SSIM improved by 72.47% and
PSNR by 25.00%, confirming the model's ability to recover lost details when the
original clean image is known. On IMAGIN-e, where no reference images exist,
perceptual quality metrics indicate a substantial enhancement, with NIQE
improving by 60.66% and BRISQUE by 48.38%, validating real-world onboard
restoration. The approach is currently deployed aboard the IMAGIN-e mission,
demonstrating its practical application in an operational space environment.
  By efficiently handling high-resolution images under edge computing
constraints, the method enables applications such as water body segmentation
and contour detection while maintaining processing viability despite resource
limitations.

</details>


### [365] [FaceEditTalker: Interactive Talking Head Generation with Facial Attribute Editing](https://arxiv.org/abs/2505.22141)
*Guanwen Feng,Zhiyuan Ma,Yunan Li,Junwei Jing,Jiahao Yang,Qiguang Miao*

Main category: cs.CV

TL;DR: FaceEditTalker提出了一种统一框架，在生成高质量、音频同步的说话头部视频的同时，实现了可控的面部属性编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的音频驱动说话头部生成方法在唇同步和情感表达方面取得了显著成果，但忽视了面部属性编辑这一关键任务。面部属性编辑对于实现深度个性化、扩展实际应用范围（如定制化数字头像、在线教育内容和品牌数字客服）至关重要。

Method: FaceEditTalker由两个关键组件组成：1) 图像特征空间编辑模块，提取语义和细节特征，灵活控制表情、发型和配饰等属性；2) 音频驱动视频生成模块，将编辑后的特征与音频引导的面部标志融合，驱动基于扩散的生成器。

Result: 在公开数据集上的大量实验表明，该方法在唇同步准确性、视频质量和属性可控性方面优于现有最先进方法。

Conclusion: FaceEditTalker通过统一框架实现了高质量、音频同步的说话头部视频生成与面部属性编辑，具有时间一致性、视觉保真度和身份保持的优势。

Abstract: Recent advances in audio-driven talking head generation have achieved
impressive results in lip synchronization and emotional expression. However,
they largely overlook the crucial task of facial attribute editing. This
capability is crucial for achieving deep personalization and expanding the
range of practical applications, including user-tailored digital avatars,
engaging online education content, and brand-specific digital customer service.
In these key domains, the flexible adjustment of visual attributes-such as
hairstyle, accessories, and subtle facial features is essential for aligning
with user preferences, reflecting diverse brand identities, and adapting to
varying contextual demands. In this paper, we present FaceEditTalker, a unified
framework that enables controllable facial attribute manipulation while
generating high-quality, audio-synchronized talking head videos. Our method
consists of two key components: an image feature space editing module, which
extracts semantic and detail features and allows flexible control over
attributes like expression, hairstyle, and accessories; and an audio-driven
video generation module, which fuses these edited features with audio-guided
facial landmarks to drive a diffusion-based generator. This design ensures
temporal coherence, visual fidelity, and identity preservation across frames.
Extensive experiments on public datasets demonstrate that our method
outperforms state-of-the-art approaches in lip-sync accuracy, video quality,
and attribute controllability. Project page:
https://peterfanfan.github.io/FaceEditTalker/

</details>


### [366] [Moment kernels: a simple and scalable approach for equivariance to rotations and reflections in deep convolutional networks](https://arxiv.org/abs/2505.21736)
*Zachary Schlamowitz,Andrew Bennecke,Daniel J. Tward*

Main category: cs.CV

TL;DR: 该论文提出了一种称为“矩核”的简单卷积核形式，用于实现旋转和反射等对称性的等变性，并证明所有等变核必须采取这种形式。通过标准卷积模块实现等变神经网络，应用于生物医学图像分析任务。


<details>
  <summary>Details</summary>
Motivation: 虽然平移等变性推动了卷积神经网络的发展，但旋转和反射等其他对称性在生物医学图像分析中同样重要，但由于相关方法的数学复杂性（如依赖表示理论），这些对称性未被广泛利用。

Method: 提出“矩核”作为简单卷积核形式，由空间位置的径向对称函数乘以分量幂或单位矩阵构成，并通过标准卷积模块实现等变神经网络。

Result: 证明了所有等变核必须采取矩核形式，并成功应用于分类（正交变换下输出不变）、3D图像配准（输出像向量变换）和细胞分割（定义椭圆的二次形式像矩阵变换）等任务。

Conclusion: 矩核提供了一种简单有效的方法来实现旋转和反射等对称性的等变性，为生物医学图像分析中的等变性需求提供了实用解决方案。

Abstract: The principle of translation equivariance (if an input image is translated an
output image should be translated by the same amount), led to the development
of convolutional neural networks that revolutionized machine vision. Other
symmetries, like rotations and reflections, play a similarly critical role,
especially in biomedical image analysis, but exploiting these symmetries has
not seen wide adoption. We hypothesize that this is partially due to the
mathematical complexity of methods used to exploit these symmetries, which
often rely on representation theory, a bespoke concept in differential geometry
and group theory. In this work, we show that the same equivariance can be
achieved using a simple form of convolution kernels that we call ``moment
kernels,'' and prove that all equivariant kernels must take this form. These
are a set of radially symmetric functions of a spatial position $x$, multiplied
by powers of the components of $x$ or the identity matrix. We implement
equivariant neural networks using standard convolution modules, and provide
architectures to execute several biomedical image analysis tasks that depend on
equivariance principles: classification (outputs are invariant under orthogonal
transforms), 3D image registration (outputs transform like a vector), and cell
segmentation (quadratic forms defining ellipses transform like a matrix).

</details>


### [367] [Investigating Mechanisms for In-Context Vision Language Binding](https://arxiv.org/abs/2505.22200)
*Darshana Saravanan,Makarand Tapaswi,Vineet Gandhi*

Main category: cs.CV

TL;DR: 研究探索了视觉语言模型(VLMs)中图像与文本绑定的机制，通过合成数据集验证了Binding ID在跨模态关联中的作用。


<details>
  <summary>Details</summary>
Motivation: 理解VLMs如何通过Binding ID机制在图像和文本之间建立关联，以提升模型对多模态信息的理解能力。

Method: 使用合成数据集和任务，分析VLMs如何为图像中的3D对象及其文本描述分配Binding ID。

Result: 实验表明，VLMs会为对象的图像标记和文本引用分配独特的Binding ID，从而实现上下文关联。

Conclusion: Binding ID机制在VLMs中有效促进了图像与文本的跨模态绑定，为模型的多模态理解提供了新视角。

Abstract: To understand a prompt, Vision-Language models (VLMs) must perceive the
image, comprehend the text, and build associations within and across both
modalities. For instance, given an 'image of a red toy car', the model should
associate this image to phrases like 'car', 'red toy', 'red object', etc. Feng
and Steinhardt propose the Binding ID mechanism in LLMs, suggesting that the
entity and its corresponding attribute tokens share a Binding ID in the model
activations. We investigate this for image-text binding in VLMs using a
synthetic dataset and task that requires models to associate 3D objects in an
image with their descriptions in the text. Our experiments demonstrate that
VLMs assign a distinct Binding ID to an object's image tokens and its textual
references, enabling in-context association.

</details>


### [368] [What is Adversarial Training for Diffusion Models?](https://arxiv.org/abs/2505.21742)
*Briglia Maria Rosaria,Mujtaba Hussain Mirza,Giuseppe Lisanti,Iacopo Masi*

Main category: cs.CV

TL;DR: 本文揭示了扩散模型(DMs)中的对抗训练(AT)与分类器中的AT存在本质差异：分类器AT追求输出不变性，而DM的AT需要保持等变性以维持扩散过程与数据分布的对齐。该方法通过添加随机噪声或对抗噪声，无缝融入扩散训练，提升了模型对噪声、异常值和对抗攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有研究未明确扩散模型中对抗训练的作用机制，且通常对噪声模型做出假设。本文旨在揭示DM中AT的独特性质，并提出一种无需假设噪声模型的通用方法，以增强模型在数据污染、异常值和对抗攻击等复杂场景下的鲁棒性。

Method: 提出一种将AT融入扩散训练的统一框架：通过添加随机噪声（类似随机平滑）或对抗噪声（类似传统AT），强制扩散流保持平滑性。该方法不依赖特定噪声假设，可直接整合到标准扩散训练流程中。

Result: 在低维/高维概念验证数据集（已知分布）上精确量化误差，并在CIFAR-10、CelebA和LSUN Bedroom基准测试中验证：在强噪声、数据损坏和迭代对抗攻击下表现出优越性能，同时具备处理噪声数据、异常值和防止记忆的能力。

Conclusion: 扩散模型的对抗训练本质是强制等变性而非不变性，这种方法通过平滑扩散流天然具备多重优势。理论分析和实验表明，该方法为提升扩散模型鲁棒性提供了原则性框架，且无需依赖特定噪声假设。

Abstract: We answer the question in the title, showing that adversarial training (AT)
for diffusion models (DMs) fundamentally differs from classifiers: while AT in
classifiers enforces output invariance, AT in DMs requires equivariance to keep
the diffusion process aligned with the data distribution. AT is a way to
enforce smoothness in the diffusion flow, improving robustness to outliers and
corrupted data. Unlike prior art, our method makes no assumptions about the
noise model and integrates seamlessly into diffusion training by adding random
noise, similar to randomized smoothing, or adversarial noise, akin to AT. This
enables intrinsic capabilities such as handling noisy data, dealing with
extreme variability such as outliers, preventing memorization, and improving
robustness. We rigorously evaluate our approach with proof-of-concept datasets
with known distributions in low- and high-dimensional space, thereby taking a
perfect measure of errors; we further evaluate on standard benchmarks such as
CIFAR-10, CelebA and LSUN Bedroom, showing strong performance under severe
noise, data corruption, and iterative adversarial attacks.

</details>


### [369] [Neural Restoration of Greening Defects in Historical Autochrome Photographs Based on Purely Synthetic Data](https://arxiv.org/abs/2505.22291)
*Saptarshi Neil Sinha,P. Julius Kuehn,Johannes Koppe,Arjan Kuijper,Michael Weinmann*

Main category: cs.CV

TL;DR: 本文提出了一种基于合成数据集生成和生成式AI的方法，用于自动修复早期彩色照片中的绿色缺陷，通过改进损失函数和模拟缺陷，实现了高效修复。


<details>
  <summary>Details</summary>
Motivation: 早期视觉艺术品（尤其是彩色照片）因老化和不当存储导致的模糊、划痕、色彩渗色和褪色等问题，传统方法难以准确还原原始色彩且需大量人工干预。

Method: 采用合成数据集生成技术模拟绿色缺陷，结合改进的加权损失函数（ChaIR方法）处理色彩不平衡问题，利用生成式AI进行修复。

Result: 该方法能高效修复数字化彩色照片的绿色缺陷，显著减少人工操作时间，优于现有方法。

Conclusion: 通过合成数据与生成式AI的结合，本研究为视觉艺术品的自动修复提供了高效解决方案，尤其适用于缺乏真实训练数据的场景。

Abstract: The preservation of early visual arts, particularly color photographs, is
challenged by deterioration caused by aging and improper storage, leading to
issues like blurring, scratches, color bleeding, and fading defects. In this
paper, we present the first approach for the automatic removal of greening
color defects in digitized autochrome photographs. Our main contributions
include a method based on synthetic dataset generation and the use of
generative AI with a carefully designed loss function for the restoration of
visual arts. To address the lack of suitable training datasets for analyzing
greening defects in damaged autochromes, we introduce a novel approach for
accurately simulating such defects in synthetic data. We also propose a
modified weighted loss function for the ChaIR method to account for color
imbalances between defected and non-defected areas. While existing methods
struggle with accurately reproducing original colors and may require
significant manual effort, our method allows for efficient restoration with
reduced time requirements.

</details>


### [370] [UniMoGen: Universal Motion Generation](https://arxiv.org/abs/2505.21837)
*Aliasghar Khani,Arianna Rampini,Evan Atherton,Bruno Roy*

Main category: cs.CV

TL;DR: UniMoGen是一种基于UNet的扩散模型，用于不依赖特定骨骼结构的运动生成，支持多种角色（如人类和动物）的运动数据训练，具有高效和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有运动生成方法依赖于特定骨骼结构，限制了其在不同角色间的通用性。UniMoGen旨在克服这一限制，提供更灵活的运动生成解决方案。

Method: UniMoGen通过动态处理每个角色所需的关节，实现骨骼无关性和计算效率。模型支持通过风格和轨迹输入进行控制，并能延续过去帧的运动。

Result: 在100style数据集上，UniMoGen表现优于现有方法。在结合100style和LAFAN1数据集训练时，模型在两种骨骼结构上均表现出高性能和高效性。

Conclusion: UniMoGen为广泛角色动画提供了一种灵活、高效且可控的运动生成方案，具有推动该领域发展的潜力。

Abstract: Motion generation is a cornerstone of computer graphics, animation, gaming,
and robotics, enabling the creation of realistic and varied character
movements. A significant limitation of existing methods is their reliance on
specific skeletal structures, which restricts their versatility across
different characters. To overcome this, we introduce UniMoGen, a novel
UNet-based diffusion model designed for skeleton-agnostic motion generation.
UniMoGen can be trained on motion data from diverse characters, such as humans
and animals, without the need for a predefined maximum number of joints. By
dynamically processing only the necessary joints for each character, our model
achieves both skeleton agnosticism and computational efficiency. Key features
of UniMoGen include controllability via style and trajectory inputs, and the
ability to continue motions from past frames. We demonstrate UniMoGen's
effectiveness on the 100style dataset, where it outperforms state-of-the-art
methods in diverse character motion generation. Furthermore, when trained on
both the 100style and LAFAN1 datasets, which use different skeletons, UniMoGen
achieves high performance and improved efficiency across both skeletons. These
results highlight UniMoGen's potential to advance motion generation by
providing a flexible, efficient, and controllable solution for a wide range of
character animations.

</details>


### [371] [VME: A Satellite Imagery Dataset and Benchmark for Detecting Vehicles in the Middle East and Beyond](https://arxiv.org/abs/2505.22353)
*Noora Al-Emadi,Ingmar Weber,Yin Yang,Ferda Ofli*

Main category: cs.CV

TL;DR: 论文提出VME数据集和CDSI基准数据集，提升中东地区和全球卫星图像车辆检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有数据集存在地理偏见，忽视中东地区，导致车辆检测模型在该区域表现不佳。

Method: 构建VME数据集（覆盖中东12国54城，含4,000+图像和100,000+车辆标注）和CDSI基准数据集，结合人工与半自动标注方法。

Result: VME显著提升中东车辆检测准确率；基于CDSI训练的模型在全球检测任务中表现更优。

Conclusion: 新数据集有效解决了地理偏差问题，推动了卫星图像车辆检测的全球化应用。

Abstract: Detecting vehicles in satellite images is crucial for traffic management,
urban planning, and disaster response. However, current models struggle with
real-world diversity, particularly across different regions. This challenge is
amplified by geographic bias in existing datasets, which often focus on
specific areas and overlook regions like the Middle East. To address this gap,
we present the Vehicles in the Middle East (VME) dataset, designed explicitly
for vehicle detection in high-resolution satellite images from Middle Eastern
countries. Sourced from Maxar, the VME dataset spans 54 cities across 12
countries, comprising over 4,000 image tiles and more than 100,000 vehicles,
annotated using both manual and semi-automated methods. Additionally, we
introduce the largest benchmark dataset for Car Detection in Satellite Imagery
(CDSI), combining images from multiple sources to enhance global car detection.
Our experiments demonstrate that models trained on existing datasets perform
poorly on Middle Eastern images, while the VME dataset significantly improves
detection accuracy in this region. Moreover, state-of-the-art models trained on
CDSI achieve substantial improvements in global car detection.

</details>


### [372] [DAM: Domain-Aware Module for Multi-Domain Dataset Condensation](https://arxiv.org/abs/2505.22387)
*Jaehyun Choi,Gyojin Han,Dong-Jae Lee,Sunghyun Baek,Junmo Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种多领域数据集压缩方法（MDDC），通过引入领域感知模块（DAM）来提升压缩数据在单领域和多领域场景中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现代数据集通常由多个领域的异构图像组成，而现有的数据集压缩方法大多忽略了这种多领域特性，导致压缩效果不佳。

Method: 论文提出了领域感知模块（DAM），通过在训练时嵌入可学习的空间掩码来捕获领域相关特征，并利用基于频率的伪领域标签来弥补真实领域标签的缺失。

Result: 实验表明，DAM在领域内、领域外以及跨架构性能上均优于基线数据集压缩方法。

Conclusion: MDDC通过DAM模块有效提升了数据集压缩在多领域场景中的泛化能力，为深度学习模型的训练提供了更高效的数据支持。

Abstract: Dataset Condensation (DC) has emerged as a promising solution to mitigate the
computational and storage burdens associated with training deep learning
models. However, existing DC methods largely overlook the multi-domain nature
of modern datasets, which are increasingly composed of heterogeneous images
spanning multiple domains. In this paper, we extend DC and introduce
Multi-Domain Dataset Condensation (MDDC), which aims to condense data that
generalizes across both single-domain and multi-domain settings. To this end,
we propose the Domain-Aware Module (DAM), a training-time module that embeds
domain-related features into each synthetic image via learnable spatial masks.
As explicit domain labels are mostly unavailable in real-world datasets, we
employ frequency-based pseudo-domain labeling, which leverages low-frequency
amplitude statistics. DAM is only active during the condensation process, thus
preserving the same images per class (IPC) with prior methods. Experiments show
that DAM consistently improves in-domain, out-of-domain, and cross-architecture
performance over baseline dataset condensation methods.

</details>


### [373] [Can NeRFs See without Cameras?](https://arxiv.org/abs/2505.22441)
*Chaitanya Amballa,Sattwik Basu,Yu-Lin Wei,Zhijian Yang,Mehmet Ergezer,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: 该论文提出了一种改进的NeRF方法，能够利用多路径信号（如WiFi）推断室内环境布局，并展示了在稀疏测量下重建平面图的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统NeRF通过光线追踪从3D场景合成新视角，但无法直接处理多路径信号（如RF/音频）。论文探索如何改造NeRF，使其能从多路径混合信号中学习环境信息，并以WiFi信号推断室内布局为应用场景。

Method: 重新设计NeRF框架，使其能够从多路径信号（如WiFi测量数据）中隐式学习环境表示，包括信号反射路径建模。

Result: 实验表明，改进的NeRF能从稀疏WiFi测量数据中推断出合理的室内平面图，并支持信号预测和基础光线追踪等应用。

Conclusion: 通过NeRF框架的适应性改造，多路径信号可被用于环境重建，为无线传感与场景理解的结合提供了新思路。

Abstract: Neural Radiance Fields (NeRFs) have been remarkably successful at
synthesizing novel views of 3D scenes by optimizing a volumetric scene
function. This scene function models how optical rays bring color information
from a 3D object to the camera pixels. Radio frequency (RF) or audio signals
can also be viewed as a vehicle for delivering information about the
environment to a sensor. However, unlike camera pixels, an RF/audio sensor
receives a mixture of signals that contain many environmental reflections (also
called "multipath"). Is it still possible to infer the environment using such
multipath signals? We show that with redesign, NeRFs can be taught to learn
from multipath signals, and thereby "see" the environment. As a grounding
application, we aim to infer the indoor floorplan of a home from sparse WiFi
measurements made at multiple locations inside the home. Although a difficult
inverse problem, our implicitly learnt floorplans look promising, and enables
forward applications, such as indoor signal prediction and basic ray tracing.

</details>


### [374] [NFR: Neural Feature-Guided Non-Rigid Shape Registration](https://arxiv.org/abs/2505.22445)
*Puhua Jiang,Zhangquan Chen,Mingze Sun,Ruqi Huang*

Main category: cs.CV

TL;DR: 本文提出了一种基于学习的3D形状配准框架，无需标注对应关系即可处理显著非刚性变形和部分形状匹配，通过结合神经网络特征和几何配准流程实现高精度配准。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理显著非刚性变形和部分形状匹配时效果有限，且依赖标注数据。本文旨在开发一种无需标注、能适应大变形和部分形状的配准方法。

Method: 将深度学习提取的神经特征融入迭代几何配准流程，动态更新对应关系并通过一致性先验过滤，提升配准鲁棒性。

Result: 在多个基准测试中达到最优性能，仅需少量训练数据即可处理未见过的复杂变形形状对。

Conclusion: 该框架突破了传统方法和固有方法的局限性，为复杂变形下的3D形状配准提供了高效解决方案。

Abstract: In this paper, we propose a novel learning-based framework for 3D shape
registration, which overcomes the challenges of significant non-rigid
deformation and partiality undergoing among input shapes, and, remarkably,
requires no correspondence annotation during training. Our key insight is to
incorporate neural features learned by deep learning-based shape matching
networks into an iterative, geometric shape registration pipeline. The
advantage of our approach is two-fold -- On one hand, neural features provide
more accurate and semantically meaningful correspondence estimation than
spatial features (e.g., coordinates), which is critical in the presence of
large non-rigid deformations; On the other hand, the correspondences are
dynamically updated according to the intermediate registrations and filtered by
consistency prior, which prominently robustify the overall pipeline. Empirical
results show that, with as few as dozens of training shapes of limited
variability, our pipeline achieves state-of-the-art results on several
benchmarks of non-rigid point cloud matching and partial shape matching across
varying settings, but also delivers high-quality correspondences between unseen
challenging shape pairs that undergo both significant extrinsic and intrinsic
deformations, in which case neither traditional registration methods nor
intrinsic methods work.

</details>


### [375] [On the Transferability and Discriminability of Repersentation Learning in Unsupervised Domain Adaptation](https://arxiv.org/abs/2505.22099)
*Wenwen Qiang,Ziyin Gu,Lingyu Si,Jiangmeng Li,Changwen Zheng,Fuchun Sun,Hui Xiong*

Main category: cs.CV

TL;DR: 本文提出了一种新的无监督域适应（UDA）框架RLGLC，通过结合域对齐和目标域可判别性增强约束，解决了传统对抗性方法中忽略目标域特征判别性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统无监督域适应方法仅依赖分布对齐和源域经验风险最小化，忽略了目标域特征的可判别性，导致性能不佳。本文从信息论角度分析，提出需要同时保证特征的可迁移性和可判别性。

Method: 提出RLGLC框架，结合域对齐目标和可判别性增强约束。采用非对称松弛Wasserstein距离（AR-WWD）处理类别不平衡和语义维度加权，并通过局部一致性机制保留目标域细粒度判别信息。

Result: 在多个基准数据集上的实验表明，RLGLC consistently surpasses state-of-the-art methods，验证了理论观点的价值。

Conclusion: 本文证明了在对抗性UDA中同时增强可迁移性和可判别性的必要性，RLGLC框架通过理论分析和实验验证取得了显著效果。

Abstract: In this paper, we addressed the limitation of relying solely on distribution
alignment and source-domain empirical risk minimization in Unsupervised Domain
Adaptation (UDA). Our information-theoretic analysis showed that this standard
adversarial-based framework neglects the discriminability of target-domain
features, leading to suboptimal performance. To bridge this
theoretical-practical gap, we defined "good representation learning" as
guaranteeing both transferability and discriminability, and proved that an
additional loss term targeting target-domain discriminability is necessary.
Building on these insights, we proposed a novel adversarial-based UDA framework
that explicitly integrates a domain alignment objective with a
discriminability-enhancing constraint. Instantiated as Domain-Invariant
Representation Learning with Global and Local Consistency (RLGLC), our method
leverages Asymmetrically-Relaxed Wasserstein of Wasserstein Distance (AR-WWD)
to address class imbalance and semantic dimension weighting, and employs a
local consistency mechanism to preserve fine-grained target-domain
discriminative information. Extensive experiments across multiple benchmark
datasets demonstrate that RLGLC consistently surpasses state-of-the-art
methods, confirming the value of our theoretical perspective and underscoring
the necessity of enforcing both transferability and discriminability in
adversarial-based UDA.

</details>


### [376] [Scaling-up Perceptual Video Quality Assessment](https://arxiv.org/abs/2505.22543)
*Ziheng Jia,Zicheng Zhang,Zeyu Zhang,Yingji Liang,Xiaorong Zhu,Chunyi Li,Jinliang Han,Haoning Wu,Bin Wang,Haoran Zhang,Guanyu Zhu,Qiyong Zhao,Xiaohong Liu,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 本文提出了OmniVQA框架，构建了大规模视频质量评估多模态指令数据库，并通过互补训练策略提升模型性能，在质量和评分任务上达到最优。


<details>
  <summary>Details</summary>
Motivation: 当前视频质量评估领域缺乏标注资源和数据集规模不足，限制了数据缩放定律的应用潜力。

Method: 提出OmniVQA框架构建高质量人机交互指令数据库，创建OmniVQA-Chat-400K和OmniVQA-MOS-20K数据集，并设计互补训练策略。

Result: 模型在质量理解和评分任务上均达到最先进性能，并通过OmniVQA-FG-Benchmark验证细粒度表现。

Conclusion: OmniVQA框架有效解决了视频质量评估领域的数据瓶颈，通过大规模数据集和互补训练策略显著提升模型性能。

Abstract: The data scaling law has been shown to significantly enhance the performance
of large multi-modal models (LMMs) across various downstream tasks. However, in
the domain of perceptual video quality assessment (VQA), the potential of
scaling law remains unprecedented due to the scarcity of labeled resources and
the insufficient scale of datasets. To address this, we propose
\textbf{OmniVQA}, an efficient framework designed to efficiently build
high-quality, human-in-the-loop VQA multi-modal instruction databases (MIDBs).
We then scale up to create \textbf{OmniVQA-Chat-400K}, the largest MIDB in the
VQA field concurrently. Our focus is on the technical and aesthetic quality
dimensions, with abundant in-context instruction data to provide fine-grained
VQA knowledge. Additionally, we have built the \textbf{OmniVQA-MOS-20K} dataset
to enhance the model's quantitative quality rating capabilities. We then
introduce a \textbf{complementary} training strategy that effectively leverages
the knowledge from datasets for quality understanding and quality rating tasks.
Furthermore, we propose the \textbf{OmniVQA-FG (fine-grain)-Benchmark} to
evaluate the fine-grained performance of the models. Our results demonstrate
that our models achieve state-of-the-art performance in both quality
understanding and rating tasks.

</details>


### [377] [PRISM: Video Dataset Condensation with Progressive Refinement and Insertion for Sparse Motion](https://arxiv.org/abs/2505.22564)
*Jaehyun Choi,Jiwan Hur,Gyojin Han,Jaemyung Yu,Junmo Kim*

Main category: cs.CV

TL;DR: PRISM提出了一种新的视频数据集压缩方法，通过渐进式优化和稀疏运动插入，保持空间内容与时间动态的相互依赖，实现了更高性能且更节省存储。


<details>
  <summary>Details</summary>
Motivation: 视频数据集压缩在处理大规模视频数据时面临计算挑战，现有方法多关注图像领域，而视频中空间内容与时间动态的复杂交互未被充分解决。

Method: PRISM方法通过渐进式优化和帧插入技术，保留动作中的运动信息，同时考虑每帧梯度的关系，实现高效压缩。

Result: 在标准视频动作识别基准测试中，PRISM表现优于现有方法，同时保持了紧凑的数据表示。

Conclusion: PRISM为视频数据集压缩提供了一种高效且节省存储的解决方案，适用于资源受限的环境。

Abstract: Video dataset condensation has emerged as a critical technique for addressing
the computational challenges associated with large-scale video data processing
in deep learning applications. While significant progress has been made in
image dataset condensation, the video domain presents unique challenges due to
the complex interplay between spatial content and temporal dynamics. This paper
introduces PRISM, Progressive Refinement and Insertion for Sparse Motion, for
video dataset condensation, a novel approach that fundamentally reconsiders how
video data should be condensed. Unlike the previous method that separates
static content from dynamic motion, our method preserves the essential
interdependence between these elements. Our approach progressively refines and
inserts frames to fully accommodate the motion in an action while achieving
better performance but less storage, considering the relation of gradients for
each frame. Extensive experiments across standard video action recognition
benchmarks demonstrate that PRISM outperforms existing disentangled approaches
while maintaining compact representations suitable for resource-constrained
environments.

</details>


### [378] [Universal Visuo-Tactile Video Understanding for Embodied Interaction](https://arxiv.org/abs/2505.22566)
*Yifan Xie,Mingyang Li,Shoujie Li,Xingting Li,Guangyu Chen,Fei Ma,Fei Richard Yu,Wenbo Ding*

Main category: cs.CV

TL;DR: VTV-LLM是一个多模态大语言模型，用于视觉-触觉视频理解，通过结合触觉感知和自然语言，提升物理属性理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视觉和语言模态上取得了进展，但未能有效整合触觉信息，而触觉信息对于真实世界交互至关重要。

Method: 提出了VTV-LLM模型，包括VTV150K数据集、三阶段训练范式（VTV增强、VTV-文本对齐、文本提示微调），以实现跨传感器和跨模态的整合。

Result: 实验证明VTV-LLM在触觉视频理解任务中表现优异，为触觉领域的人机交互奠定了基础。

Conclusion: VTV-LLM通过结合触觉感知和自然语言，为更直观的人机交互提供了新的可能性。

Abstract: Tactile perception is essential for embodied agents to understand physical
attributes of objects that cannot be determined through visual inspection
alone. While existing approaches have made progress in visual and language
modalities for physical understanding, they fail to effectively incorporate
tactile information that provides crucial haptic feedback for real-world
interaction. In this paper, we present VTV-LLM, the first multi-modal large
language model for universal Visuo-Tactile Video (VTV) understanding that
bridges the gap between tactile perception and natural language. To address the
challenges of cross-sensor and cross-modal integration, we contribute VTV150K,
a comprehensive dataset comprising 150,000 video frames from 100 diverse
objects captured across three different tactile sensors (GelSight Mini, DIGIT,
and Tac3D), annotated with four fundamental tactile attributes (hardness,
protrusion, elasticity, and friction). We develop a novel three-stage training
paradigm that includes VTV enhancement for robust visuo-tactile representation,
VTV-text alignment for cross-modal correspondence, and text prompt finetuning
for natural language generation. Our framework enables sophisticated tactile
reasoning capabilities including feature assessment, comparative analysis,
scenario-based decision making and so on. Experimental evaluations demonstrate
that VTV-LLM achieves superior performance in tactile video understanding
tasks, establishing a foundation for more intuitive human-machine interaction
in tactile domains.

</details>


### [379] [Progressive Data Dropout: An Embarrassingly Simple Approach to Faster Training](https://arxiv.org/abs/2505.22342)
*Shriram M S,Xinyue Hao,Shihao Hou,Yang Lu,Laura Sevilla-Lara,Anurag Arnab,Shreyank N Gowda*

Main category: cs.CV

TL;DR: 本文提出了一种名为渐进式数据丢弃（Progressive Data Dropout）的新训练方法，通过结合困难数据挖掘和丢弃策略，显著减少训练所需周期数（降至基线的12.4%），同时提升模型精度（最高4.82%），且无需改动模型架构或优化器。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习依赖大规模数据集训练，但模型和数据的规模增长带来了高昂成本。尽管模型压缩研究已取得进展，但数据利用效率的提升仍缺乏有效方案。传统均匀采样训练方式效率低下，亟需创新方法优化训练过程。

Method: 提出渐进式数据丢弃策略，融合困难数据挖掘和丢弃技术。该方法动态筛选训练数据，逐步丢弃冗余样本，仅需在标准训练流程中插入简单实现模块，无需修改模型结构或优化器。

Result: 实验表明：1) 有效训练周期数减少至基线12.4%；2) 模型精度不降反升，最高提升4.82%；3) 方法具有通用性，可无缝集成现有训练流程。代码已开源。

Conclusion: 该工作为高效训练提供了简单可推广的解决方案，在减少计算成本的同时提升模型性能，具备广泛应用的潜力，可能成为新的训练标准。

Abstract: The success of the machine learning field has reliably depended on training
on large datasets. While effective, this trend comes at an extraordinary cost.
This is due to two deeply intertwined factors: the size of models and the size
of datasets. While promising research efforts focus on reducing the size of
models, the other half of the equation remains fairly mysterious. Indeed, it is
surprising that the standard approach to training remains to iterate over and
over, uniformly sampling the training dataset. In this paper we explore a
series of alternative training paradigms that leverage insights from
hard-data-mining and dropout, simple enough to implement and use that can
become the new training standard. The proposed Progressive Data Dropout reduces
the number of effective epochs to as little as 12.4% of the baseline. This
savings actually do not come at any cost for accuracy. Surprisingly, the
proposed method improves accuracy by up to 4.82%. Our approach requires no
changes to model architecture or optimizer, and can be applied across standard
training pipelines, thus posing an excellent opportunity for wide adoption.
Code can be found here: https://github.com/bazyagami/LearningWithRevision

</details>


### [380] [Tell me Habibi, is it Real or Fake?](https://arxiv.org/abs/2505.22581)
*Kartik Kuckreja,Parul Gupta,Injy Hamed,Thamar Solorio,Muhammad Haris Khan,Abhinav Dhall*

Main category: cs.CV

TL;DR: 论文介绍了首个大规模阿拉伯语-英语视听深度伪造数据集ArEnAV，用于解决多语言和语码转换语音的深度伪造检测挑战。


<details>
  <summary>Details</summary>
Motivation: 当前深度伪造检测研究多集中于单语内容，忽视了多语言和语码转换语音的挑战，尤其在阿拉伯世界常见的阿拉伯语-英语语码转换现象。

Method: 通过整合四种文本转语音和两种唇同步模型的新流程，生成了包含387k视频和765小时内容的ArEnAV数据集。

Result: ArEnAV数据集在单语和多语数据集、先进深度伪造检测模型及人类评估中表现出色，推动了多语言多模态深度伪造检测研究。

Conclusion: ArEnAV数据集填补了多语深度伪造检测的空白，为未来研究提供了重要资源。

Abstract: Deepfake generation methods are evolving fast, making fake media harder to
detect and raising serious societal concerns. Most deepfake detection and
dataset creation research focuses on monolingual content, often overlooking the
challenges of multilingual and code-switched speech, where multiple languages
are mixed within the same discourse. Code-switching, especially between Arabic
and English, is common in the Arab world and is widely used in digital
communication. This linguistic mixing poses extra challenges for deepfake
detection, as it can confuse models trained mostly on monolingual data. To
address this, we introduce \textbf{ArEnAV}, the first large-scale
Arabic-English audio-visual deepfake dataset featuring intra-utterance
code-switching, dialectal variation, and monolingual Arabic content. It
\textbf{contains 387k videos and over 765 hours of real and fake videos}. Our
dataset is generated using a novel pipeline integrating four Text-To-Speech and
two lip-sync models, enabling comprehensive analysis of multilingual multimodal
deepfake detection. We benchmark our dataset against existing monolingual and
multilingual datasets, state-of-the-art deepfake detection models, and a human
evaluation, highlighting its potential to advance deepfake research. The
dataset can be accessed
\href{https://huggingface.co/datasets/kartik060702/ArEnAV-Full}{here}.

</details>


### [381] [RiverMamba: A State Space Model for Global River Discharge and Flood Forecasting](https://arxiv.org/abs/2505.22535)
*Mohamad Hakam Shams Eddin,Yikui Zhang,Stefan Kollet,Juergen Gall*

Main category: cs.CV

TL;DR: RiverMamba：新型深度学习模型，利用Mamba块和时空建模提升全球河流流量及洪水预测能力，优于现有AI和物理模型。


<details>
  <summary>Details</summary>
Motivation: 现有水文深度学习模型多局限于局部应用，未充分利用水体空间关联，亟需能建模时空关系的新方法以改进预测。

Method: 提出RiverMamba模型，预训练长期再分析数据，集成ECMWF HRES气象预报，通过Mamba块捕捉全球河道网络路由及时空误差。

Result: 模型在0.05°网格上实现7天提前期的可靠预测，在极端洪水事件和不同重现期表现均超越现有AI与物理模型。

Conclusion: RiverMamba为科学和业务应用提供了高精度的全球河流流量及洪水预测框架，显著提升早期预警能力。

Abstract: Recent deep learning approaches for river discharge forecasting have improved
the accuracy and efficiency in flood forecasting, enabling more reliable early
warning systems for risk management. Nevertheless, existing deep learning
approaches in hydrology remain largely confined to local-scale applications and
do not leverage the inherent spatial connections of bodies of water. Thus,
there is a strong need for new deep learning methodologies that are capable of
modeling spatio-temporal relations to improve river discharge and flood
forecasting for scientific and operational applications. To address this, we
present RiverMamba, a novel deep learning model that is pretrained with
long-term reanalysis data and that can forecast global river discharge and
floods on a $0.05^\circ$ grid up to 7 days lead time, which is of high
relevance in early warning. To achieve this, RiverMamba leverages efficient
Mamba blocks that enable the model to capture global-scale channel network
routing and enhance its forecast capability for longer lead times. The forecast
blocks integrate ECMWF HRES meteorological forecasts, while accounting for
their inaccuracies through spatio-temporal modeling. Our analysis demonstrates
that RiverMamba delivers reliable predictions of river discharge, including
extreme floods across return periods and lead times, surpassing both
operational AI- and physics-based models.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [382] [Learning optimal treatment strategies for intraoperative hypotension using deep reinforcement learning](https://arxiv.org/abs/2505.21596)
*Esra Adiyeke,Tianqi Liu,Venkata Sai Dheeraj Naganaboina,Han Li,Tyler J. Loftus,Yuanfang Ren,Benjamin Shickel,Matthew M. Ruppert,Karandeep Singh,Ruogu Fang,Parisa Rashidi,Azra Bihorac,Tezcan Ozrazgat-Baslanti*

Main category: q-bio.QM

TL;DR: 该研究开发了一个基于强化学习的模型，用于推荐手术中最佳静脉输液和血管加压药剂量，以减少术中低血压和术后急性肾损伤（AKI）。


<details>
  <summary>Details</summary>
Motivation: 传统手术决策依赖医生经验，存在变异性。数据驱动的治疗推荐系统可优化围手术期决策，尤其是术中低血压管理不佳与术后AKI相关的情况。

Method: 使用深度Q网络强化学习模型，基于16个变量（包括术中生理时间序列、静脉输液和血管加压药总剂量）进行训练和测试。

Result: 模型在69%的血管加压药剂量决策中与医生一致，41%的静脉输液剂量决策接近实际剂量。模型推荐剂量与术后AKI发生率最低相关。

Conclusion: 该模型有潜力减少术后AKI并改善术中低血压驱动的其他结局。

Abstract: Traditional methods of surgical decision making heavily rely on human
experience and prompt actions, which are variable. A data-driven system
generating treatment recommendations based on patient states can be a
substantial asset in perioperative decision-making, as in cases of
intraoperative hypotension, for which suboptimal management is associated with
acute kidney injury (AKI), a common and morbid postoperative complication. We
developed a Reinforcement Learning (RL) model to recommend optimum dose of
intravenous (IV) fluid and vasopressors during surgery to avoid intraoperative
hypotension and postoperative AKI. We retrospectively analyzed 50,021 surgeries
from 42,547 adult patients who underwent major surgery at a quaternary care
hospital between June 2014 and September 2020. Of these, 34,186 surgeries were
used for model training and 15,835 surgeries were reserved for testing. We
developed a Deep Q-Networks based RL model using 16 variables including
intraoperative physiologic time series, total dose of IV fluid and vasopressors
extracted for every 15-minute epoch. The model replicated 69% of physician's
decisions for the dosage of vasopressors and proposed higher or lower dosage of
vasopressors than received in 10% and 21% of the treatments, respectively. In
terms of IV fluids, the model's recommendations were within 0.05 ml/kg/15 min
of the actual dose in 41% of the cases, with higher or lower doses recommended
for 27% and 32% of the treatments, respectively. The model resulted in a higher
estimated policy value compared to the physicians' actual treatments, as well
as random and zero-drug policies. AKI prevalence was the lowest in patients
receiving medication dosages that aligned with model's decisions. Our findings
suggest that implementation of the model's policy has the potential to reduce
postoperative AKI and improve other outcomes driven by intraoperative
hypotension.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [383] [HelixDesign-Binder: A Scalable Production-Grade Platform for Binder Design Built on HelixFold3](https://arxiv.org/abs/2505.21873)
*Jie Gao,Jun Li,Jing Hu,Shanzhuo Zhang,Kunrui Zhu,Yueyang Huang,Xiaonan Zhang,Xiaomin Fang*

Main category: q-bio.BM

TL;DR: HelixDesign-Binder是一个基于HelixFold3的高通量蛋白质结合剂设计平台，整合了从骨架生成到多维评分的全流程，支持大规模高效设计。


<details>
  <summary>Details</summary>
Motivation: 蛋白质结合剂设计在治疗、诊断和合成生物学中至关重要，但现有工具流程分散、计算成本高且集成复杂，亟需一体化解决方案。

Method: 基于HelixFold3构建自动化设计平台，整合骨架生成、序列设计、结构评估和多维评分，利用百度云高性能计算和ipTM等先进指标。

Result: 在6个靶标测试中，平台能稳定生成多样性高质量结合剂，部分设计的预测结合亲和力达到或超过已验证方案。

Conclusion: 该平台通过PaddleHelix提供交互式网页服务，为学术和工业界的抗体/蛋白质结合剂开发提供了高效工具。

Abstract: Protein binder design is central to therapeutics, diagnostics, and synthetic
biology, yet practical deployment remains challenging due to fragmented
workflows, high computational costs, and complex tool integration. We present
HelixDesign-Binder, a production-grade, high-throughput platform built on
HelixFold3 that automates the full binder design pipeline, from backbone
generation and sequence design to structural evaluation and multi-dimensional
scoring. By unifying these stages into a scalable and user-friendly system,
HelixDesign-Binder enables efficient exploration of binder candidates with
favorable structural, energetic, and physicochemical properties. The platform
leverages Baidu Cloud's high-performance infrastructure to support large-scale
design and incorporates advanced scoring metrics, including ipTM, predicted
binding free energy, and interface hydrophobicity. Benchmarking across six
protein targets demonstrates that HelixDesign-Binder reliably produces diverse
and high-quality binders, some of which match or exceed validated designs in
predicted binding affinity. HelixDesign-Binder is accessible via an interactive
web interface in PaddleHelix platform, supporting both academic research and
industrial applications in antibody and protein binder development.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [384] [Revisiting Self-attention for Cross-domain Sequential Recommendation](https://arxiv.org/abs/2505.21811)
*Clark Mingxuan Ju,Leonardo Neves,Bhuvesh Kumar,Liam Collins,Tong Zhao,Yuwei Qiu,Qing Dou,Sohail Nizam,Sen Yang,Neil Shah*

Main category: cs.IR

TL;DR: 该论文提出了一种名为AutoCDSR的新方法，通过优化自注意力机制来提升跨域序列推荐（CDSR）的性能，无需复杂调整即可实现高效知识迁移。


<details>
  <summary>Details</summary>
Motivation: 现有跨域序列推荐方法大多依赖额外添加的领域特定模块，忽视了自注意力机制本身的学习能力。论文旨在通过优化自注意力机制来提升简单模型的性能，实现更高效的跨域知识迁移。

Method: 提出Pareto最优自注意力机制，将跨域学习建模为多目标优化问题，动态最小化跨域注意力分数。进一步提出性能更强的AutoCDSR+变体，作为即插即用模块兼容现有推荐模型。

Result: AutoCDSR使SASRec和Bert4Rec的Recall@10平均提升9.8%和16.0%，NDCG@10提升12.0%和16.7%，且计算开销小，无需复杂超参调整。

Conclusion: 通过自注意力机制优化实现的AutoCDSR方案，有效提升了跨域推荐性能，具有易实现、低计算开销和兼容性强等优势。

Abstract: Sequential recommendation is a popular paradigm in modern recommender
systems. In particular, one challenging problem in this space is cross-domain
sequential recommendation (CDSR), which aims to predict future behaviors given
user interactions across multiple domains. Existing CDSR frameworks are mostly
built on the self-attention transformer and seek to improve by explicitly
injecting additional domain-specific components (e.g. domain-aware module
blocks). While these additional components help, we argue they overlook the
core self-attention module already present in the transformer, a naturally
powerful tool to learn correlations among behaviors. In this work, we aim to
improve the CDSR performance for simple models from a novel perspective of
enhancing the self-attention. Specifically, we introduce a Pareto-optimal
self-attention and formulate the cross-domain learning as a multi-objective
problem, where we optimize the recommendation task while dynamically minimizing
the cross-domain attention scores. Our approach automates knowledge transfer in
CDSR (dubbed as AutoCDSR) -- it not only mitigates negative transfer but also
encourages complementary knowledge exchange among auxiliary domains. Based on
the idea, we further introduce AutoCDSR+, a more performant variant with slight
additional cost. Our proposal is easy to implement and works as a plug-and-play
module that can be incorporated into existing transformer-based recommenders.
Besides flexibility, it is practical to deploy because it brings little extra
computational overheads without heavy hyper-parameter tuning. AutoCDSR on
average improves Recall@10 for SASRec and Bert4Rec by 9.8% and 16.0% and
NDCG@10 by 12.0% and 16.7%, respectively. Code is available at
https://github.com/snap-research/AutoCDSR.

</details>


### [385] [Scientific Paper Retrieval with LLM-Guided Semantic-Based Ranking](https://arxiv.org/abs/2505.21815)
*Yunyi Zhang,Ruozhen Yang,Siqi Jiao,SeongKu Kang,Jiawei Han*

Main category: cs.IR

TL;DR: SemRank结合LLM引导的查询理解和基于概念的语义索引，显著提升科学论文检索的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有密集检索方法难以捕捉细粒度科学概念，而基于LLM的方法缺乏语料库特定知识的支持，可能生成不可靠内容。

Method: 提出SemRank框架，通过多粒度科学概念索引论文，并利用LLM识别查询中的核心概念以实现精准语义匹配。

Result: 实验表明SemRank能持续提升多种基础检索器的性能，优于现有基于LLM的基线方法，且保持高效。

Conclusion: SemRank通过结合LLM和概念索引，有效解决了科学论文检索中的细粒度语义匹配问题。

Abstract: Scientific paper retrieval is essential for supporting literature discovery
and research. While dense retrieval methods demonstrate effectiveness in
general-purpose tasks, they often fail to capture fine-grained scientific
concepts that are essential for accurate understanding of scientific queries.
Recent studies also use large language models (LLMs) for query understanding;
however, these methods often lack grounding in corpus-specific knowledge and
may generate unreliable or unfaithful content. To overcome these limitations,
we propose SemRank, an effective and efficient paper retrieval framework that
combines LLM-guided query understanding with a concept-based semantic index.
Each paper is indexed using multi-granular scientific concepts, including
general research topics and detailed key phrases. At query time, an LLM
identifies core concepts derived from the corpus to explicitly capture the
query's information need. These identified concepts enable precise semantic
matching, significantly enhancing retrieval accuracy. Experiments show that
SemRank consistently improves the performance of various base retrievers,
surpasses strong existing LLM-based baselines, and remains highly efficient.

</details>


### [386] [Xinyu AI Search: Enhanced Relevance and Comprehensive Results with Rich Answer Presentations](https://arxiv.org/abs/2505.21849)
*Bo Tang,Junyi Zhu,Chenyang Xi,Yunhang Ge,Jiahao Wu,Yuchen Feng,Yijun Niu,Wenqiang Wei,Yu Yu,Chunyu Li,Zehao Lin,Hao Wu,Ning Liao,Yebin Yang,Jiajia Wang,Zhiyu Li,Feiyu Xiong,Jingrun Chen*

Main category: cs.IR

TL;DR: Xinyu AI Search提出了一种新型生成式AI搜索引擎，通过查询分解图、多源检索优化和精细化引用技术，显著提升复杂查询的检索效果与呈现方式。


<details>
  <summary>Details</summary>
Motivation: 传统搜索引擎难以处理复杂查询的碎片化信息，而生成式AI搜索引擎在相关性、全面性和呈现方式上存在不足。

Method: 采用动态查询分解图将复杂查询拆分为子查询，结合多源聚合检索、结果重排序、精细化内置引用及时间轴可视化呈现技术。

Result: 在真实查询测试中超越8种现有技术，在相关性、全面性和洞察力方面表现优异，消融实验验证了各子模块的必要性。

Conclusion: 该研究首次构建了生成式AI搜索引擎的完整框架，实现了检索、生成与用户导向呈现的统一。

Abstract: Traditional search engines struggle to synthesize fragmented information for
complex queries, while generative AI search engines face challenges in
relevance, comprehensiveness, and presentation. To address these limitations,
we introduce Xinyu AI Search, a novel system that incorporates a
query-decomposition graph to dynamically break down complex queries into
sub-queries, enabling stepwise retrieval and generation. Our retrieval pipeline
enhances diversity through multi-source aggregation and query expansion, while
filtering and re-ranking strategies optimize passage relevance. Additionally,
Xinyu AI Search introduces a novel approach for fine-grained, precise built-in
citation and innovates in result presentation by integrating timeline
visualization and textual-visual choreography. Evaluated on recent real-world
queries, Xinyu AI Search outperforms eight existing technologies in human
assessments, excelling in relevance, comprehensiveness, and insightfulness.
Ablation studies validate the necessity of its key sub-modules. Our work
presents the first comprehensive framework for generative AI search engines,
bridging retrieval, generation, and user-centric presentation.

</details>


### [387] [Extracting Research Instruments from Educational Literature Using LLMs](https://arxiv.org/abs/2505.21855)
*Jiseung Yoo,Curran Mahowald,Meiyu Li,Wei Ai*

Main category: cs.IR

TL;DR: 该研究开发了一个基于大语言模型（LLM）的系统，用于从教育领域文献中提取研究工具的详细信息，显著优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用大语言模型改进教育领域研究工具信息的提取和管理，以支持教育研究和决策。

Method: 采用多步提示和特定领域的数据模式，生成针对教育研究的结构化输出。

Result: 该系统在识别工具名称和详细信息方面表现优异，显著优于其他方法。

Conclusion: LLM驱动的信息提取在教育领域具有潜力，能系统化组织研究工具信息，提升研究人员和教育领导者的信息获取效率。

Abstract: Large Language Models (LLMs) are transforming information extraction from
academic literature, offering new possibilities for knowledge management. This
study presents an LLM-based system designed to extract detailed information
about research instruments used in the education field, including their names,
types, target respondents, measured constructs, and outcomes. Using multi-step
prompting and a domain-specific data schema, it generates structured outputs
optimized for educational research. Our evaluation shows that this system
significantly outperforms other approaches, particularly in identifying
instrument names and detailed information. This demonstrates the potential of
LLM-powered information extraction in educational contexts, offering a
systematic way to organize research instrument information. The ability to
aggregate such information at scale enhances accessibility for researchers and
education leaders, facilitating informed decision-making in educational
research and policy.

</details>


### [388] [Yambda-5B -- A Large-Scale Multi-modal Dataset for Ranking And Retrieval](https://arxiv.org/abs/2505.22238)
*A. Ploshkin,V. Tytskiy,A. Pismenny,V. Baikalov,E. Taychinov,A. Permiakov,D. Burlakov,E. Krofto,N. Savushkin*

Main category: cs.IR

TL;DR: Yambda-5B是一个来自Yandex.Music的大规模开放数据集，包含47.9亿用户-项目交互，涵盖100万用户和939万曲目。数据集包含隐式和显式反馈，并提供音频嵌入。其独特之处在于区分了用户自然行为和推荐驱动行为，支持严格的基准测试。


<details>
  <summary>Details</summary>
Motivation: 为了提供一个工业规模的资源，促进推荐系统的研究、创新和可重复结果，Yandex.Music发布了Yambda-5B数据集，旨在支持机器学习算法的开发和评估。

Method: 数据集包含用户-项目交互（隐式和显式反馈）和音频嵌入。通过Global Temporal Split评估协议，对标准基线（ItemKNN、iALS）和高级模型（SANSA、SASRec）进行基准测试。

Result: 基准测试结果显示，不同推荐算法在Yambda-5B数据集上的性能表现各异，为研究社区提供了有价值的参考。

Conclusion: Yambda-5B的发布为推荐系统研究提供了一个易于访问、工业规模的资源，有望推动该领域的进步和创新。

Abstract: We present Yambda-5B, a large-scale open dataset sourced from the
Yandex.Music streaming platform. Yambda-5B contains 4.79 billion user-item
interactions from 1 million users across 9.39 million tracks. The dataset
includes two primary types of interactions: implicit feedback (listening
events) and explicit feedback (likes, dislikes, unlikes and undislikes). In
addition, we provide audio embeddings for most tracks, generated by a
convolutional neural network trained on audio spectrograms. A key
distinguishing feature of Yambda-5B is the inclusion of the is_organic flag,
which separates organic user actions from recommendation-driven events. This
distinction is critical for developing and evaluating machine learning
algorithms, as Yandex.Music relies on recommender systems to personalize track
selection for users. To support rigorous benchmarking, we introduce an
evaluation protocol based on a Global Temporal Split, allowing recommendation
algorithms to be assessed in conditions that closely mirror real-world use. We
report benchmark results for standard baselines (ItemKNN, iALS) and advanced
models (SANSA, SASRec) using a variety of evaluation metrics. By releasing
Yambda-5B to the community, we aim to provide a readily accessible,
industrial-scale resource to advance research, foster innovation, and promote
reproducible results in recommender systems.

</details>


### [389] [UDuo: Universal Dual Optimization Framework for Online Matching](https://arxiv.org/abs/2505.22243)
*Bin Li,Diwei Liu,Zehong Hu,Jia Jia*

Main category: cs.IR

TL;DR: 论文提出UDuo框架，通过动态用户到达表示、自适应资源分配策略和在线时间序列预测，解决动态环境下在线资源分配问题，相比传统方法效率更高且收敛更快。


<details>
  <summary>Details</summary>
Motivation: 传统基于静态随机用户到达模型的在线资源分配方法在动态环境下失效，需要新的框架应对用户到达模式和资源消耗的动态变化。

Method: 提出UDuo框架：(i) 动态用户到达表示向量捕捉分布变化；(ii) 自适应资源分配策略适应异构约束场景；(iii) 在线时间序列预测实现渐进最优解。

Result: 实验表明UDuo在真实定价场景中比传统方法效率提升28%，收敛速度加快40%，同时保证理论有效性。

Conclusion: UDuo为动态环境下的在线资源分配提供了理论严谨且高效的解决方案，推动了该领域的范式转变。

Abstract: Online resource allocation under budget constraints critically depends on
proper modeling of user arrival dynamics. Classical approaches employ
stochastic user arrival models to derive near-optimal solutions through
fractional matching formulations of exposed users for downstream allocation
tasks. However, this is no longer a reasonable assumption when the environment
changes dynamically. In this work, We propose the Universal Dual optimization
framework UDuo, a novel paradigm that fundamentally rethinks online allocation
through three key innovations: (i) a temporal user arrival representation
vector that explicitly captures distribution shifts in user arrival patterns
and resource consumption dynamics, (ii) a resource pacing learner with adaptive
allocation policies that generalize to heterogeneous constraint scenarios, and
(iii) an online time-series forecasting approach for future user arrival
distributions that achieves asymptotically optimal solutions with constraint
feasibility guarantees in dynamic environments. Experimental results show that
UDuo achieves higher efficiency and faster convergence than the traditional
stochastic arrival model in real-world pricing while maintaining rigorous
theoretical validity for general online allocation problems.

</details>


### [390] [Pre-training for Recommendation Unlearning](https://arxiv.org/abs/2505.22649)
*Guoxuan Chen,Lianghao Xia,Chao Huang*

Main category: cs.IR

TL;DR: 该论文提出了一种名为UnlearnRec的新型预训练范式，旨在解决图神经网络推荐系统中数据选择性遗忘的挑战，避免完全重新训练，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统在处理用户数据删除请求时面临挑战，尤其是在隐私和法规要求下，需要高效且不影响整体性能的遗忘机制。传统方法存在图结构破坏或假设不成立的问题。

Method: 提出模型无关的预训练范式UnlearnRec，通过Influence Encoder直接生成更新后的模型参数，减少微调需求，避免完全重新训练。

Result: 在公共基准测试中，该方法展现了卓越的遗忘效果，相比重新训练方法提速超过10倍，同时保持了模型性能。

Conclusion: UnlearnRec为推荐系统提供了一种高效、性能保持的数据遗忘解决方案，适用于复杂的图神经网络环境。

Abstract: Modern recommender systems powered by Graph Neural Networks (GNNs) excel at
modeling complex user-item interactions, yet increasingly face scenarios
requiring selective forgetting of training data. Beyond user requests to remove
specific interactions due to privacy concerns or preference changes, regulatory
frameworks mandate recommender systems' ability to eliminate the influence of
certain user data from models. This recommendation unlearning challenge
presents unique difficulties as removing connections within interaction graphs
creates ripple effects throughout the model, potentially impacting
recommendations for numerous users. Traditional approaches suffer from
significant drawbacks: fragmentation methods damage graph structure and
diminish performance, while influence function techniques make assumptions that
may not hold in complex GNNs, particularly with self-supervised or random
architectures. To address these limitations, we propose a novel model-agnostic
pre-training paradigm UnlearnRec that prepares systems for efficient unlearning
operations. Our Influence Encoder takes unlearning requests together with
existing model parameters and directly produces updated parameters of unlearned
model with little fine-tuning, avoiding complete retraining while preserving
model performance characteristics. Extensive evaluation on public benchmarks
demonstrates that our method delivers exceptional unlearning effectiveness
while providing more than 10x speedup compared to retraining approaches. We
release our method implementation at: https://github.com/HKUDS/UnlearnRec.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [391] [Collaborative Agentic AI Needs Interoperability Across Ecosystems](https://arxiv.org/abs/2505.21550)
*Rishi Sharma,Martijn de Vos,Pradyumna Chari,Ramesh Raskar,Anne-Marie Kermarrec*

Main category: cs.NI

TL;DR: 论文提出通过制定最小标准实现协作式AI代理的互操作性，以避免生态碎片化，并设计了名为Web of Agents的基础架构。


<details>
  <summary>Details</summary>
Motivation: 当前协作式AI代理解决方案各自孤立，可能导致生态碎片化，因此需要互操作性标准来确保开放、安全、大规模的代理生态系统。

Method: 设计了一个名为Web of Agents的最小基础架构，包含代理间通信、交互互操作性、状态管理和代理发现四个组件，并尽可能复用现有标准和基础设施。

Result: 提出了实现协作式AI代理互操作性的第一步关键方案，为未来生态发展提供了实用路径。

Conclusion: Web of Agents是避免生态碎片化的关键举措，为构建互操作的代理系统奠定了基础。

Abstract: Collaborative agentic AI is projected to transform entire industries by
enabling AI-powered agents to autonomously perceive, plan, and act within
digital environments. Yet, current solutions in this field are all built in
isolation, and we are rapidly heading toward a landscape of fragmented,
incompatible ecosystems. In this position paper, we argue that
interoperability, achieved by the adoption of minimal standards, is essential
to ensure open, secure, web-scale, and widely-adopted agentic ecosystems. To
this end, we devise a minimal architectural foundation for collaborative
agentic AI, named Web of Agents, which is composed of four components:
agent-to-agent messaging, interaction interoperability, state management, and
agent discovery. Web of Agents adopts existing standards and reuses existing
infrastructure where possible. With Web of Agents, we take the first but
critical step toward interoperable agentic systems and offer a pragmatic path
forward before ecosystem fragmentation becomes the norm.

</details>


### [392] [MetaSTNet: Multimodal Meta-learning for Cellular Traffic Conformal Prediction](https://arxiv.org/abs/2505.21553)
*Hui Ma,Kai Yang*

Main category: cs.NI

TL;DR: 提出基于多模态元学习的MetaSTNet模型，解决小样本网络流量预测难题，通过模拟器训练迁移至现实场景，并采用交叉共形预测评估置信区间。


<details>
  <summary>Details</summary>
Motivation: 现有网络流量预测方法依赖大量训练数据，但在小样本场景下性能不足，需开发适应快速迁移的高效模型。

Method: 构建端到端多模态元学习框架MetaSTNet，先在模拟器预训练，再迁移元知识至现实场景，结合交叉共形预测量化不确定性。

Result: 真实数据集实验表明，MetaSTNet能以少量数据快速适应新任务，实现高精度预测并提供校准的置信区间。

Conclusion: MetaSTNet为小样本网络流量预测提供有效解决方案，其模拟-现实迁移框架具有实际应用潜力。

Abstract: Network traffic prediction techniques have attracted much attention since
they are valuable for network congestion control and user experience
improvement. While existing prediction techniques can achieve favorable
performance when there is sufficient training data, it remains a great
challenge to make accurate predictions when only a small amount of training
data is available. To tackle this problem, we propose a deep learning model,
entitled MetaSTNet, based on a multimodal meta-learning framework. It is an
end-to-end network architecture that trains the model in a simulator and
transfers the meta-knowledge to a real-world environment, which can quickly
adapt and obtain accurate predictions on a new task with only a small amount of
real-world training data. In addition, we further employ cross conformal
prediction to assess the calibrated prediction intervals. Extensive experiments
have been conducted on real-world datasets to illustrate the efficiency and
effectiveness of MetaSTNet.

</details>


### [393] [Fog Intelligence for Network Anomaly Detection](https://arxiv.org/abs/2505.21563)
*Kai Yang,Hui Ma,Shaoyu Dou*

Main category: cs.NI

TL;DR: 论文提出了一种名为'雾智能'的分布式机器学习架构，用于大规模分布式无线网络的智能管理，结合边缘处理和集中式云计算的优势。


<details>
  <summary>Details</summary>
Motivation: 移动通信网络的规模和复杂性不断增加，网络监控数据的量和维度也在迅速增长，使得异常网络行为的检测变得极为困难。传统的集中式机器学习算法无法适用于大规模分布式无线网络。

Method: 提出了一种分布式机器学习架构——雾智能，结合边缘处理和集中式云计算的优势，具有可扩展性和隐私保护特性。

Result: 雾智能架构能够有效支持分布式无线网络的智能管理，适用于大规模网络环境。

Conclusion: 雾智能架构为大规模分布式无线网络的智能管理提供了一种可行的解决方案，兼具可扩展性和隐私保护优势。

Abstract: Anomalies are common in network system monitoring. When manifested as network
threats to be mitigated, service outages to be prevented, and security risks to
be ameliorated, detecting such anomalous network behaviors becomes of great
importance. However, the growing scale and complexity of the mobile
communication networks, as well as the ever-increasing amount and
dimensionality of the network surveillance data, make it extremely difficult to
monitor a mobile network and discover abnormal network behaviors. Recent
advances in machine learning allow for obtaining near-optimal solutions to
complicated decision-making problems with many sources of uncertainty that
cannot be accurately characterized by traditional mathematical models. However,
most machine learning algorithms are centralized, which renders them
inapplicable to a large-scale distributed wireless networks with tens of
millions of mobile devices. In this article, we present fog intelligence, a
distributed machine learning architecture that enables intelligent wireless
network management. It preserves the advantage of both edge processing and
centralized cloud computing. In addition, the proposed architecture is
scalable, privacy-preserving, and well suited for intelligent management of a
distributed wireless network.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [394] [UI-Evol: Automatic Knowledge Evolving for Computer Use Agents](https://arxiv.org/abs/2505.21964)
*Ziyun Zhang,Xinyi Liu,Xiaoyi Zhang,Jun Wang,Gang Chen,Yan Lu*

Main category: cs.HC

TL;DR: 论文提出UI-Evol模块，通过两阶段方法解决计算机代理中知识执行效率低的问题，显著提升任务性能和代理可靠性。


<details>
  <summary>Details</summary>
Motivation: 研究发现，即使90%的知识正确，实际任务执行成功率仅为41%，存在明显的知识执行差距。

Method: 提出UI-Evol模块，包含回溯阶段（提取实际交互中的动作序列）和批判阶段（通过对比外部参考优化知识）。

Result: 在OSWorld基准测试中，UI-Evol显著提升了任务性能，并降低了代理行为的标准差，提高了可靠性。

Conclusion: UI-Evol有效解决了计算机代理中的知识执行差距，提升了任务执行成功率和代理的稳定性。

Abstract: External knowledge has played a crucial role in the recent development of
computer use agents. We identify a critical knowledge-execution gap: retrieved
knowledge often fails to translate into effective real-world task execution.
Our analysis shows even 90\% correct knowledge yields only 41\% execution
success rate. To bridge this gap, we propose UI-Evol, a plug-and-play module
for autonomous GUI knowledge evolution. UI-Evol consists of two stages: a
Retrace Stage that extracts faithful objective action sequences from actual
agent-environment interactions, and a Critique Stage that refines existing
knowledge by comparing these sequences against external references. We conduct
comprehensive experiments on the OSWorld benchmark with the state-of-the-art
Agent S2. Our results demonstrate that UI-Evol not only significantly boosts
task performance but also addresses a previously overlooked issue of high
behavioral standard deviation in computer use agents, leading to superior
performance on computer use tasks and substantially improved agent reliability.

</details>


### [395] [MapStory: LLM-Powered Text-Driven Map Animation Prototyping with Human-in-the-Loop Editing](https://arxiv.org/abs/2505.21966)
*Aditya Gunturu,Ben Pearman,Keiichi Ihara,Morteza Faraji,Bryan Wang,Rubaiat Habib Kazi,Ryo Suzuki*

Main category: cs.HC

TL;DR: MapStory是一个基于LLM的地图动画创作工具，通过自然语言文本直接生成可编辑的地图动画序列，简化创作流程并提升效率。


<details>
  <summary>Details</summary>
Motivation: 旨在降低地图动画创作的门槛，通过自然语言交互和自动化工具帮助用户快速生成和迭代地图动画，满足专业和业余创作者的需求。

Method: 利用LLM代理架构解析用户脚本，自动生成场景分解（如镜头移动、视觉高亮等），并结合地理信息查询和交互式时间线编辑器进行细化调整。

Result: 专家访谈（N=5）和可用性研究（N=12）表明，MapStory能轻松创建地图动画，加快迭代速度，激发创意探索。

Conclusion: MapStory通过自然语言交互和自动化设计，有效降低了地图动画创作的难度，提升了创作效率和灵活性。

Abstract: We introduce MapStory, an LLM-powered animation authoring tool that generates
editable map animation sequences directly from natural language text. Given a
user-written script, MapStory leverages an agentic architecture to
automatically produce a scene breakdown, which decomposes the script into key
animation building blocks such as camera movements, visual highlights, and
animated elements. Our system includes a researcher component that accurately
queries geospatial information by leveraging an LLM with web search, enabling
the automatic extraction of relevant regions, paths, and coordinates while
allowing users to edit and query for changes or additional information to
refine the results. Additionally, users can fine-tune parameters of these
blocks through an interactive timeline editor. We detail the system's design
and architecture, informed by formative interviews with professional animators
and an analysis of 200 existing map animation videos. Our evaluation, which
includes expert interviews (N=5) and a usability study (N=12), demonstrates
that MapStory enables users to create map animations with ease, facilitates
faster iteration, encourages creative exploration, and lowers barriers to
creating map-centric stories.

</details>


### [396] [Voice CMS: updating the knowledge base of a digital assistant through conversation](https://arxiv.org/abs/2505.22303)
*Grzegorz Wolny,Michał Szczerbak*

Main category: cs.HC

TL;DR: 研究提出基于多智能体LLM架构和语音界面的数字助手知识库更新方案，发现语音界面在简单任务中更受青睐，且内容质量与图形界面相当，建议采用混合界面结合两者优势。


<details>
  <summary>Details</summary>
Motivation: 探索语音用户界面(VUI)在知识管理中的潜力，并与传统图形内容管理系统(CMS)比较，以理解用户偏好与信息复杂度之间的关系。

Method: 采用多智能体LLM架构和VUI设计解决方案，通过用户实验对比VUI与传统图形CMS的可用性和内容质量。

Result: VUI整体可用性评分低于图形界面，但在简单任务中更受用户偏爱；对于高复杂度任务，VUI的内容质量与图形界面相当。

Conclusion: 结合语音交互直观性和图形反馈降低认知负荷的混合界面，可能是解决知识管理关键挑战的有效方案。

Abstract: In this study, we propose a solution based on a multi-agent LLM architecture
and a voice user interface (VUI) designed to update the knowledge base of a
digital assistant. Its usability is evaluated in comparison to a more
traditional graphical content management system (CMS), with a focus on
understanding the relationship between user preferences and the complexity of
the information being provided. The findings demonstrate that, while the
overall usability of the VUI is rated lower than the graphical interface, it is
already preferred by users for less complex tasks. Furthermore, the quality of
content entered through the VUI is comparable to that achieved with the
graphical interface, even for highly complex tasks. Obtained qualitative
results suggest that a hybrid interface combining the strengths of both
approaches could address the key challenges identified during the experiment,
such as reducing cognitive load through graphical feedback while maintaining
the intuitive nature of voice-based interactions. This work highlights the
potential of conversational interfaces as a viable and effective method for
knowledge management in specific business contexts.

</details>


### [397] [Human-Centered Human-AI Collaboration (HCHAC)](https://arxiv.org/abs/2505.22477)
*Qi Gao,Wei Xu,Hanxi Pan,Mowei Shen,Zaifeng Gao*

Main category: cs.HC

TL;DR: 本文探讨了以人为中心的人机协作（HAC）概念，提出HCHAC框架，并通过自动驾驶案例展示应用，展望未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 智能时代下人机协作关系发生变革，AI不仅是工具更是合作伙伴，需以人为中心探索新型协作模式及其挑战。

Method: 从HCAI视角梳理HAC核心特征与研究议程，整合提出HCHAC框架，结合自动驾驶案例进行实证分析。

Result: 构建了人本导向的HCHAC框架，阐明人机协同机制，验证了自动驾驶场景中人类与AI代理的协作增效作用。

Conclusion: 需持续优化HAC系统的有效性、可靠性和伦理融合，推动多领域人机协同范式发展。

Abstract: In the intelligent era, the interaction between humans and intelligent
systems fundamentally involves collaboration with autonomous intelligent
agents. Human-AI Collaboration (HAC) represents a novel type of human-machine
relationship facilitated by autonomous intelligent machines equipped with AI
technologies. In this paradigm, AI agents serve not only as auxiliary tools but
also as active teammates, partnering with humans to accomplish tasks
collaboratively. Human-centered AI (HCAI) emphasizes that humans play critical
leadership roles in the collaboration. This human-led collaboration imparts new
dimensions to the human-machine relationship, necessitating innovative research
perspectives, paradigms, and agenda to address the unique challenges posed by
HAC. This chapter delves into the essence of HAC from the human-centered
perspective, outlining its core concepts and distinguishing features. It
reviews the current research methodologies and research agenda within the HAC
field from the HCAI perspective, highlighting advancements and ongoing studies.
Furthermore, a framework for human-centered HAC (HCHAC) is proposed by
integrating these reviews and analyses. A case study of HAC in the context of
autonomous vehicles is provided, illustrating practical applications and the
synergistic interactions between humans and AI agents. Finally, it identifies
potential future research directions aimed at enhancing the effectiveness,
reliability, and ethical integration of human-centered HAC systems in diverse
domains.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [398] [Align-DA: Align Score-based Atmospheric Data Assimilation with Multiple Preferences](https://arxiv.org/abs/2505.22008)
*Jing-An Sun,Hang Fan,Junchao Gong,Ben Fei,Kun Chen,Fenghua Ling,Wenlong Zhang,Wanghan Xu,Li Yan,Pierre Gentine,Lei Bai*

Main category: physics.ao-ph

TL;DR: 本文提出Align-DA方法，通过生成式过程和奖励信号优化数据同化中的背景先验，替代传统手动调参，实验表明能提升分析质量。


<details>
  <summary>Details</summary>
Motivation: 传统数据同化方法因观测稀疏性需依赖经验性背景先验调参，本文受扩散模型对齐技术启发，旨在用数据驱动方式自动优化背景先验。

Method: Align-DA将数据同化解构为生成过程，在隐空间训练基于分数的模型，并利用同化精度、预报技能和物理一致性三组奖励信号对齐先验。

Result: 多组奖励信号实验显示，该方法在不同评估指标和观测策略下均能提升分析质量，且软约束的偏好对齐可自适应复杂背景先验。

Conclusion: 基于奖励对齐的生成式数据同化框架为领域发展提供了新方向，可自动适配复杂先验，减少人工干预需求。

Abstract: Data assimilation (DA) aims to estimate the full state of a dynamical system
by combining partial and noisy observations with a prior model forecast,
commonly referred to as the background. In atmospheric applications, this
problem is fundamentally ill-posed due to the sparsity of observations relative
to the high-dimensional state space. Traditional methods address this challenge
by simplifying background priors to regularize the solution, which are
empirical and require continual tuning for application. Inspired by alignment
techniques in text-to-image diffusion models, we propose Align-DA, which
formulates DA as a generative process and uses reward signals to guide
background priors, replacing manual tuning with data-driven alignment.
Specifically, we train a score-based model in the latent space to approximate
the background-conditioned prior, and align it using three complementary reward
signals for DA: (1) assimilation accuracy, (2) forecast skill initialized from
the assimilated state, and (3) physical adherence of the analysis fields.
Experiments with multiple reward signals demonstrate consistent improvements in
analysis quality across different evaluation metrics and observation-guidance
strategies. These results show that preference alignment, implemented as a soft
constraint, can automatically adapt complex background priors tailored to DA,
offering a promising new direction for advancing the field.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [399] [Exact Algorithms and Lower Bounds for Forming Coalitions of Constrained Maximum Size](https://arxiv.org/abs/2505.22384)
*Foivos Fioravantes,Harmender Gahlawat,Nikolaos Melissinos*

Main category: cs.DS

TL;DR: 该论文研究了在团队规模受限条件下基于代理人偏好的联盟形成问题，提出了针对树状结构的高效算法，并证明其渐进最优性。


<details>
  <summary>Details</summary>
Motivation: 研究如何在考虑代理人偏好的情况下，将一组代理人高效地分成规模受限的团队，这是联盟形成问题的一个变种。

Method: 采用系统化算法研究，结合参数化复杂性理论，针对树宽受限的结构设计了精确算法。

Result: 提出了针对树状结构的高效算法，并证明该算法在渐进意义下是最优的，且在合理理论假设下无法被显著超越。

Conclusion: 论文为受限团队规模的联盟形成问题提供了实用的算法解决方案，尤其在树状结构中表现出色，并确立了算法的理论最优性。

Abstract: Imagine we want to split a group of agents into teams in the most
\emph{efficient} way, considering that each agent has their own preferences
about their teammates. This scenario is modeled by the extensively studied
\textsc{Coalition Formation} problem. Here, we study a version of this problem
where each team must additionally be of bounded size.
  We conduct a systematic algorithmic study, providing several intractability
results as well as multiple exact algorithms that scale well as the input grows
(FPT), which could prove useful in practice.
  Our main contribution is an algorithm that deals efficiently with tree-like
structures (bounded \emph{treewidth}) for ``small'' teams. We complement this
result by proving that our algorithm is asymptotically optimal. Particularly,
there can be no algorithm that vastly outperforms the one we present, under
reasonable theoretical assumptions, even when considering star-like structures
(bounded \emph{vertex cover number}).

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [400] [Towards Human-Like Trajectory Prediction for Autonomous Driving: A Behavior-Centric Approach](https://arxiv.org/abs/2505.21565)
*Haicheng Liao,Zhenning Li,Guohui Zhang,Keqiang Li,Chengzhong Xu*

Main category: cs.RO

TL;DR: HiT模型通过动态中心性度量与行为感知模块，显著提升了复杂交通场景下的车辆轨迹预测准确性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统在复杂动态交通环境中需要更精准、类人的轨迹预测方法，传统静态图模型无法充分捕捉车辆间交互影响。

Method: 提出HiT模型，采用动态框架建模直接/间接车辆交互，结合行为感知模块与动态中心性度量。

Result: 在NGSIM等5个数据集上超越现有模型，尤其在激进驾驶场景表现突出。

Conclusion: HiT为全自动驾驶系统提供了更可靠、可解释的轨迹预测方案，推动安全性提升。

Abstract: Predicting the trajectories of vehicles is crucial for the development of
autonomous driving (AD) systems, particularly in complex and dynamic traffic
environments. In this study, we introduce HiT (Human-like Trajectory
Prediction), a novel model designed to enhance trajectory prediction by
incorporating behavior-aware modules and dynamic centrality measures. Unlike
traditional methods that primarily rely on static graph structures, HiT
leverages a dynamic framework that accounts for both direct and indirect
interactions among traffic participants. This allows the model to capture the
subtle yet significant influences of surrounding vehicles, enabling more
accurate and human-like predictions. To evaluate HiT's performance, we
conducted extensive experiments using diverse and challenging real-world
datasets, including NGSIM, HighD, RounD, ApolloScape, and MoCAD++. The results
demonstrate that HiT consistently outperforms other top models across multiple
metrics, particularly excelling in scenarios involving aggressive driving
behaviors. This research presents a significant step forward in trajectory
prediction, offering a more reliable and interpretable approach for enhancing
the safety and efficiency of fully autonomous driving systems.

</details>


### [401] [Fast and Cost-effective Speculative Edge-Cloud Decoding with Early Exits](https://arxiv.org/abs/2505.21594)
*Yeshwanth Venkatesha,Souvik Kundu,Priyadarshini Panda*

Main category: cs.RO

TL;DR: 该论文提出了一种边缘-云协作的解码框架，通过在云端部署大模型、设备端部署小模型，并利用预生成和提前退出机制，显著降低了延迟并提升了效率。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）部署依赖昂贵的云端API，导致高运营成本和小型组织难以访问。设备端部署虽能降低成本，但受限于计算资源，模型规模和精度受限。因此，需要一种边缘与云协作的高效解决方案。

Method: 提出了一种边缘-云协作的推测解码框架：云端部署大目标模型（如Llama2-7B），设备端部署小草案模型（如Vicuna-68M）。通过目标模型的提前退出机制，在验证过程中生成中间令牌，设备端可提前预生成后续令牌，利用空闲时间提升并行性。

Result: 实验表明，该方法相比云端自回归解码降低了35%的延迟，预生成机制进一步提升了11%的效率。在Unitree Go2四足机器人上的实际部署中，基于视觉语言模型（VLM）的控制实现了21%的加速。

Conclusion: 该框架为资源受限的边缘设备提供了实时LLM和VLM应用的可行方案，平衡了成本、延迟和隐私需求。

Abstract: Large Language Models (LLMs) enable various applications on edge devices such
as smartphones, wearables, and embodied robots. However, their deployment often
depends on expensive cloud-based APIs, creating high operational costs, which
limit access for smaller organizations and raise sustainability concerns.
Certain LLMs can be deployed on-device, offering a cost-effective solution with
reduced latency and improved privacy. Yet, limited computing resources
constrain the size and accuracy of models that can be deployed, necessitating a
collaborative design between edge and cloud. We propose a fast and
cost-effective speculative edge-cloud decoding framework with a large target
model on the server and a small draft model on the device. By introducing early
exits in the target model, tokens are generated mid-verification, allowing the
client to preemptively draft subsequent tokens before final verification, thus
utilizing idle time and enhancing parallelism between edge and cloud. Using an
NVIDIA Jetson Nano (client) and an A100 GPU (server) with Vicuna-68M (draft)
and Llama2-7B (target) models, our method achieves up to a 35% reduction in
latency compared to cloud-based autoregressive decoding, with an additional 11%
improvement from preemptive drafting. To demonstrate real-world applicability,
we deploy our method on the Unitree Go2 quadruped robot using Vision-Language
Model (VLM) based control, achieving a 21% speedup over traditional cloud-based
autoregressive decoding. These results demonstrate the potential of our
framework for real-time LLM and VLM applications on resource-constrained edge
devices.

</details>


### [402] [PartInstruct: Part-level Instruction Following for Fine-grained Robot Manipulation](https://arxiv.org/abs/2505.21652)
*Yifan Yin,Zhengtao Han,Shivam Aarya,Jianxin Wang,Shuhang Xu,Jiawei Peng,Angtian Wang,Alan Yuille,Tianmin Shu*

Main category: cs.RO

TL;DR: 该论文提出了首个细粒度机器人操作基准PartInstruct，包含丰富的部件级标注和任务，评估发现现有模型在3D空间部件操作和长时任务中存在困难。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏细粒度机器人操作任务的大规模数据集，特别是带有部件级标注和多样3D物体的数据集，这限制了相关模型的发展。

Method: 构建PartInstruct基准，包含513个物体实例、1302个细粒度任务和超1万条专家演示数据，并在3D模拟器中合成数据。

Result: 实验表明，现有模型在部件概念理解和3D空间动作预测上表现不佳，长时任务中尤其困难。

Conclusion: PartInstruct为细粒度机器人操作研究提供了重要基准，揭示了当前模型的局限性，为未来研究指明方向。

Abstract: Fine-grained robot manipulation, such as lifting and rotating a bottle to
display the label on the cap, requires robust reasoning about object parts and
their relationships with intended tasks. Despite recent advances in training
general-purpose robot manipulation policies guided by language instructions,
there is a notable lack of large-scale datasets for fine-grained manipulation
tasks with part-level instructions and diverse 3D object instances annotated
with part-level labels. In this work, we introduce PartInstruct, the first
large-scale benchmark for training and evaluating fine-grained robot
manipulation models using part-level instructions. PartInstruct comprises 513
object instances across 14 categories, each annotated with part-level
information, and 1302 fine-grained manipulation tasks organized into 16 task
classes. Our training set consists of over 10,000 expert demonstrations
synthesized in a 3D simulator, where each demonstration is paired with a
high-level task instruction, a chain of base part-based skill instructions, and
ground-truth 3D information about the object and its parts. Additionally, we
designed a comprehensive test suite to evaluate the generalizability of learned
policies across new states, objects, and tasks. We evaluated several
state-of-the-art robot manipulation approaches, including end-to-end
vision-language policy learning and bi-level planning models for robot
manipulation on our benchmark. The experimental results reveal that current
models struggle to robustly ground part concepts and predict actions in 3D
space, and face challenges when manipulating object parts in long-horizon
tasks.

</details>


### [403] [Streaming Flow Policy: Simplifying diffusion$/$flow-matching policies by treating action trajectories as flow trajectories](https://arxiv.org/abs/2505.21851)
*Sunshine Jiang,Xiaolin Fang,Nicholas Roy,Tomás Lozano-Pérez,Leslie Pack Kaelbling,Siddharth Ancha*

Main category: cs.RO

TL;DR: 该论文提出了一种简化扩散/流匹配策略的方法，通过将动作轨迹视为流轨迹，实现了在流采样过程中实时向机器人传输动作，提高了策略执行速度和传感器运动回路的紧密性。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散/流匹配策略在模仿学习复杂、多模态动作轨迹时计算成本高，且需要等待采样过程完成才能执行动作。论文旨在简化这一过程，实现实时动作传输。

Method: 论文提出了一种新算法，从最后一个动作周围的窄高斯分布中采样，并通过流匹配学习的速度场逐步整合，生成动作序列。这种方法支持在流采样过程中实时传输动作。

Result: 流策略在保持多模态行为建模能力的同时，优于现有方法，实现了更快的策略执行和更紧密的传感器运动回路。

Conclusion: 论文提出的流策略简化了扩散/流匹配过程，实现了实时动作传输，提高了模仿学习的性能和机器人控制的效率。

Abstract: Recent advances in diffusion$/$flow-matching policies have enabled imitation
learning of complex, multi-modal action trajectories. However, they are
computationally expensive because they sample a trajectory of trajectories: a
diffusion$/$flow trajectory of action trajectories. They discard intermediate
action trajectories, and must wait for the sampling process to complete before
any actions can be executed on the robot. We simplify diffusion$/$flow policies
by treating action trajectories as flow trajectories. Instead of starting from
pure noise, our algorithm samples from a narrow Gaussian around the last
action. Then, it incrementally integrates a velocity field learned via flow
matching to produce a sequence of actions that constitute a single trajectory.
This enables actions to be streamed to the robot on-the-fly during the flow
sampling process, and is well-suited for receding horizon policy execution.
Despite streaming, our method retains the ability to model multi-modal
behavior. We train flows that stabilize around demonstration trajectories to
reduce distribution shift and improve imitation learning performance. Streaming
flow policy outperforms prior methods while enabling faster policy execution
and tighter sensorimotor loops for learning-based robot control. Project
website: https://streaming-flow-policy.github.io/

</details>


### [404] [Learning Compositional Behaviors from Demonstration and Language](https://arxiv.org/abs/2505.21981)
*Weiyu Liu,Neil Nie,Ruohan Zhang,Jiayuan Mao,Jiajun Wu*

Main category: cs.RO

TL;DR: BLADE框架结合模仿学习和基于模型的规划，利用语言标注演示和大型语言模型提取抽象动作知识，构建结构化高层动作表示库，实现机器人长时程操作任务。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决长时程机器人操作任务中的泛化问题，特别是在面对新初始状态、外部状态扰动和新目标时的适应性挑战。

Method: BLADE通过语言标注演示提取动作知识，利用大型语言模型构建结构化高层动作表示库，包括基于视觉感知的前提条件和效果，以及神经网络策略实现的控制器。

Result: BLADE在仿真和真实机器人实验中表现出色，能够适应多种新情境，包括部分可观测性和几何约束等复杂条件。

Conclusion: BLADE框架通过自动提取结构化动作表示，无需手动标注，显著提升了机器人在复杂操作任务中的泛化能力。

Abstract: We introduce Behavior from Language and Demonstration (BLADE), a framework
for long-horizon robotic manipulation by integrating imitation learning and
model-based planning. BLADE leverages language-annotated demonstrations,
extracts abstract action knowledge from large language models (LLMs), and
constructs a library of structured, high-level action representations. These
representations include preconditions and effects grounded in visual perception
for each high-level action, along with corresponding controllers implemented as
neural network-based policies. BLADE can recover such structured
representations automatically, without manually labeled states or symbolic
definitions. BLADE shows significant capabilities in generalizing to novel
situations, including novel initial states, external state perturbations, and
novel goals. We validate the effectiveness of our approach both in simulation
and on real robots with a diverse set of objects with articulated parts,
partial observability, and geometric constraints.

</details>


### [405] [Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge](https://arxiv.org/abs/2505.21906)
*Zhongyi Zhou,Yichen Zhu,Junjie Wen,Chaomin Shen,Yi Xu*

Main category: cs.RO

TL;DR: ChatVLA-2是一种新型的视觉-语言-动作模型，通过混合专家和三阶段训练流程，保留并扩展了预训练视觉语言模型的核心能力，在数学推理和空间理解方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端视觉-语言-动作模型在微调过程中往往会失去预训练视觉语言模型的关键能力，因此需要一种既能保留这些能力又能进行可操作推理的通用模型。

Method: 引入ChatVLA-2，采用混合专家架构和专门的三阶段训练流程，旨在保留视觉语言模型的原始能力，同时实现可操作的推理。

Result: ChatVLA-2在数学推理、OCR和空间理解方面表现出色，超越了现有的模仿学习方法如OpenVLA、DexVLA和pi-zero。

Conclusion: ChatVLA-2在通用机器人基础模型的开发中取得了显著进展，展示了强大的推理和理解能力。

Abstract: Vision-language-action (VLA) models have emerged as the next generation of
models in robotics. However, despite leveraging powerful pre-trained
Vision-Language Models (VLMs), existing end-to-end VLA systems often lose key
capabilities during fine-tuning as the model adapts to specific robotic tasks.
We argue that a generalizable VLA model should retain and expand upon the VLM's
core competencies: 1) Open-world embodied reasoning - the VLA should inherit
the knowledge from VLM, i.e., recognize anything that the VLM can recognize,
capable of solving math problems, possessing visual-spatial intelligence, 2)
Reasoning following - effectively translating the open-world reasoning into
actionable steps for the robot. In this work, we introduce ChatVLA-2, a novel
mixture-of-expert VLA model coupled with a specialized three-stage training
pipeline designed to preserve the VLM's original strengths while enabling
actionable reasoning. To validate our approach, we design a math-matching task
wherein a robot interprets math problems written on a whiteboard and picks
corresponding number cards from a table to solve equations. Remarkably, our
method exhibits exceptional mathematical reasoning and OCR capabilities,
despite these abilities not being explicitly trained within the VLA.
Furthermore, we demonstrate that the VLA possesses strong spatial reasoning
skills, enabling it to interpret novel directional instructions involving
previously unseen objects. Overall, our method showcases reasoning and
comprehension abilities that significantly surpass state-of-the-art imitation
learning methods such as OpenVLA, DexVLA, and pi-zero. This work represents a
substantial advancement toward developing truly generalizable robotic
foundation models endowed with robust reasoning capacities.

</details>


### [406] [DORAEMON: Decentralized Ontology-aware Reliable Agent with Enhanced Memory Oriented Navigation](https://arxiv.org/abs/2505.21969)
*Tianjun Gu,Linfeng Li,Xuhong Wang,Chenghua Gong,Jingyu Gong,Zhizhong Zhang,Yuan Xie,Lizhuang Ma,Xin Tan*

Main category: cs.RO

TL;DR: 论文提出DORAEMON框架，通过模仿人类导航能力的双流结构和新型评估指标，显著提升家庭服务机器人在陌生环境中的零样本自主导航性能。


<details>
  <summary>Details</summary>
Motivation: 家庭服务机器人在陌生环境中的自适应导航仍面临挑战，现有基于视觉语言模型的零样本方法存在时空不连续性、记忆表示无结构和任务理解不足等问题。

Method: 提出受认知启发的DORAEMON框架：背侧流处理时空不连续性（分层语义-空间融合和拓扑地图），腹侧流结合RAG-VLM和Policy-VLM提升决策，并开发Nav-Ensurance确保安全性。

Result: 在HM3D、MP3D和GOAT数据集上取得SOTA性能（SR/SPL指标），显著优于现有方法，并提出新评估指标AORI。

Conclusion: DORAEMON无需预训练或先验地图即可实现高效零样本导航，实验验证了其有效性。

Abstract: Adaptive navigation in unfamiliar environments is crucial for household
service robots but remains challenging due to the need for both low-level path
planning and high-level scene understanding. While recent vision-language model
(VLM) based zero-shot approaches reduce dependence on prior maps and
scene-specific training data, they face significant limitations: spatiotemporal
discontinuity from discrete observations, unstructured memory representations,
and insufficient task understanding leading to navigation failures. We propose
DORAEMON (Decentralized Ontology-aware Reliable Agent with Enhanced Memory
Oriented Navigation), a novel cognitive-inspired framework consisting of
Ventral and Dorsal Streams that mimics human navigation capabilities. The
Dorsal Stream implements the Hierarchical Semantic-Spatial Fusion and Topology
Map to handle spatiotemporal discontinuities, while the Ventral Stream combines
RAG-VLM and Policy-VLM to improve decision-making. Our approach also develops
Nav-Ensurance to ensure navigation safety and efficiency. We evaluate DORAEMON
on the HM3D, MP3D, and GOAT datasets, where it achieves state-of-the-art
performance on both success rate (SR) and success weighted by path length (SPL)
metrics, significantly outperforming existing methods. We also introduce a new
evaluation metric (AORI) to assess navigation intelligence better.
Comprehensive experiments demonstrate DORAEMON's effectiveness in zero-shot
autonomous navigation without requiring prior map building or pre-training.

</details>


### [407] [MIND-Stack: Modular, Interpretable, End-to-End Differentiability for Autonomous Navigation](https://arxiv.org/abs/2505.21734)
*Felix Jahncke,Johannes Betz*

Main category: cs.RO

TL;DR: MIND-Stack提出了一种模块化、可解释且端到端可微的导航算法框架，结合了规则方法和神经网络的优点，并在真实嵌入式平台上验证了性能。


<details>
  <summary>Details</summary>
Motivation: 现有导航算法中，基于规则的方法可解释性强但难以从大数据中学习，而端到端神经网络学习能力强但缺乏透明性。MIND-Stack旨在结合两者优势，实现模块化与端到端学习的统一。

Method: 提出MIND-Stack框架，包含定位网络和Stanley控制器，通过中间可解释状态表示和端到端可微性，使定位模块能够优化下游控制误差。

Result: 实验表明该框架性能优于现有算法，定位模块通过端到端训练有效降低控制误差，并在算力受限的真实嵌入式平台上实现了仿真到现实的迁移。

Conclusion: MIND-Stack展现了模块化与端到端学习的协同潜力，未来通过整合更多导航模块可进一步提升框架稳定性和性能。

Abstract: Developing robust, efficient navigation algorithms is challenging. Rule-based
methods offer interpretability and modularity but struggle with learning from
large datasets, while end-to-end neural networks excel in learning but lack
transparency and modularity. In this paper, we present MIND-Stack, a modular
software stack consisting of a localization network and a Stanley Controller
with intermediate human interpretable state representations and end-to-end
differentiability. Our approach enables the upstream localization module to
reduce the downstream control error, extending its role beyond state
estimation. Unlike existing research on differentiable algorithms that either
lack modules of the autonomous stack to span from sensor input to actuator
output or real-world implementation, MIND-Stack offers both capabilities. We
conduct experiments that demonstrate the ability of the localization module to
reduce the downstream control loss through its end-to-end differentiability
while offering better performance than state-of-the-art algorithms. We showcase
sim-to-real capabilities by deploying the algorithm on a real-world embedded
autonomous platform with limited computation power and demonstrate simultaneous
training of both the localization and controller towards one goal. While
MIND-Stack shows good results, we discuss the incorporation of additional
modules from the autonomous navigation pipeline in the future, promising even
greater stability and performance in the next iterations of the framework.

</details>


### [408] [ReinFlow: Fine-tuning Flow Matching Policy with Online Reinforcement Learning](https://arxiv.org/abs/2505.22094)
*Tonghe Zhang,Yu Chao,Sicang Su,Yu Wang*

Main category: cs.RO

TL;DR: ReinFlow是一种简单有效的在线强化学习框架，通过向流匹配策略注入可学习的噪声，将其转化为离散时间马尔可夫过程，从而提升探索能力和训练稳定性，适用于连续机器人控制任务。


<details>
  <summary>Details</summary>
Motivation: 当前在连续机器人控制任务中，强化学习方法在探索和训练稳定性方面存在挑战。ReinFlow旨在通过改进流匹配策略，提升性能并减少计算成本。

Method: ReinFlow通过向确定性流策略路径注入可学习的噪声，将其转化为离散时间马尔可夫过程，从而简化似然计算并提升训练稳定性。该方法适用于多种流模型变体，包括Rectified Flow和Shortcut Models。

Result: 在代表性运动和操作任务中，ReinFlow显著提升了性能：Rectified Flow策略在腿部运动任务中平均奖励增长135.36%，节省了去噪步骤和82.63%的壁时间；Shortcut Models策略在操作任务中成功率平均提升40.34%，节省23.20%的计算时间。

Conclusion: ReinFlow通过改进流匹配策略的探索能力和训练稳定性，在连续机器人控制任务中取得了显著性能提升，同时大幅降低了计算成本。

Abstract: We propose ReinFlow, a simple yet effective online reinforcement learning
(RL) framework that fine-tunes a family of flow matching policies for
continuous robotic control. Derived from rigorous RL theory, ReinFlow injects
learnable noise into a flow policy's deterministic path, converting the flow
into a discrete-time Markov Process for exact and straightforward likelihood
computation. This conversion facilitates exploration and ensures training
stability, enabling ReinFlow to fine-tune diverse flow model variants,
including Rectified Flow [35] and Shortcut Models [19], particularly at very
few or even one denoising step. We benchmark ReinFlow in representative
locomotion and manipulation tasks, including long-horizon planning with visual
input and sparse reward. The episode reward of Rectified Flow policies obtained
an average net growth of 135.36% after fine-tuning in challenging legged
locomotion tasks while saving denoising steps and 82.63% of wall time compared
to state-of-the-art diffusion RL fine-tuning method DPPO [43]. The success rate
of the Shortcut Model policies in state and visual manipulation tasks achieved
an average net increase of 40.34% after fine-tuning with ReinFlow at four or
even one denoising step, whose performance is comparable to fine-tuned DDIM
policies while saving computation time for an average of 23.20%. Project
Webpage: https://reinflow.github.io/

</details>


### [409] [From Strangers to Assistants: Fast Desire Alignment for Embodied Agent-User Adaptation](https://arxiv.org/abs/2505.22503)
*Yuanfei Wang,Xinju Huang,Fangwei Zhong,Yaodong Yang,Yizhou Wang,Yuanpei Chen,Hao Dong*

Main category: cs.RO

TL;DR: 该论文提出了一个名为FAMER的新框架，旨在快速对齐具身代理与用户潜在需求，通过心理推理和反思通信模块提升任务执行和沟通效率。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，具身代理需与不熟悉的代理和人类用户协作，用户目标常模糊且隐含。快速准确地理解并适应用户潜在需求成为关键能力。

Method: 开发了HA-Desire家庭辅助模拟环境，集成LLM驱动的用户代理；提出FAMER框架，包含基于需求的心理推理机制、反思通信模块和目标相关信息提取与记忆持久化。

Result: 实验表明，FAMER框架显著提升了任务执行和沟通效率，使具身代理能快速适应复杂环境中的用户特定需求。

Conclusion: FAMER框架通过心理推理和高效通信，有效解决了具身代理与用户需求对齐的挑战，为复杂环境中的适应性协作提供了新方向。

Abstract: While embodied agents have made significant progress in performing complex
physical tasks, real-world applications demand more than pure task execution.
The agents must collaborate with unfamiliar agents and human users, whose goals
are often vague and implicit. In such settings, interpreting ambiguous
instructions and uncovering underlying desires is essential for effective
assistance. Therefore, fast and accurate desire alignment becomes a critical
capability for embodied agents. In this work, we first develop a home
assistance simulation environment HA-Desire that integrates an LLM-driven human
user agent exhibiting realistic value-driven goal selection and communication.
The ego agent must interact with this proxy user to infer and adapt to the
user's latent desires. To achieve this, we present a novel framework FAMER for
fast desire alignment, which introduces a desire-based mental reasoning
mechanism to identify user intent and filter desire-irrelevant actions. We
further design a reflection-based communication module that reduces redundant
inquiries, and incorporate goal-relevant information extraction with memory
persistence to improve information reuse and reduce unnecessary exploration.
Extensive experiments demonstrate that our framework significantly enhances
both task execution and communication efficiency, enabling embodied agents to
quickly adapt to user-specific desires in complex embodied environments.

</details>


### [410] [LiDAR Based Semantic Perception for Forklifts in Outdoor Environments](https://arxiv.org/abs/2505.22258)
*Benjamin Serfling,Hannes Reichert,Lorenzo Bayerlein,Konrad Doll,Kati Radkhah-Lens*

Main category: cs.RO

TL;DR: 本文提出了一种新型激光雷达语义分割框架，专为复杂户外环境中的自动叉车设计，采用双激光雷达系统提高障碍物检测精度，实验验证了其高效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 针对自动叉车在动态仓库和堆场环境中的安全导航需求，研究旨在开发一种能够精确识别动态和静态障碍物的语义分割方法。

Method: 采用双激光雷达系统（前向和向下倾斜配置），结合轻量级算法对高分辨率3D点云进行语义分割，分类包括安全关键实例和环境类别。

Result: 实验表明，该方法在满足实时性要求的同时实现了高分割精度，适用于动态工业环境中的安全导航。

Conclusion: 该框架为自动叉车在复杂户外环境中的安全运行提供了可行解决方案，兼具高效性和精确性。

Abstract: In this study, we present a novel LiDAR-based semantic segmentation framework
tailored for autonomous forklifts operating in complex outdoor environments.
Central to our approach is the integration of a dual LiDAR system, which
combines forward-facing and downward-angled LiDAR sensors to enable
comprehensive scene understanding, specifically tailored for industrial
material handling tasks. The dual configuration improves the detection and
segmentation of dynamic and static obstacles with high spatial precision. Using
high-resolution 3D point clouds captured from two sensors, our method employs a
lightweight yet robust approach that segments the point clouds into
safety-critical instance classes such as pedestrians, vehicles, and forklifts,
as well as environmental classes such as driveable ground, lanes, and
buildings. Experimental validation demonstrates that our approach achieves high
segmentation accuracy while satisfying strict runtime requirements,
establishing its viability for safety-aware, fully autonomous forklift
navigation in dynamic warehouse and yard environments.

</details>


### [411] [SCIZOR: A Self-Supervised Approach to Data Curation for Large-Scale Imitation Learning](https://arxiv.org/abs/2505.22626)
*Yu Zhang,Yuqi Xie,Huihan Liu,Rutav Shah,Michael Wan,Linxi Fan,Yuke Zhu*

Main category: cs.RO

TL;DR: SCIZOR是一种自监督数据筛选框架，通过过滤低质量的状态-动作对来提升模仿学习策略的性能，平均提升15.4%。


<details>
  <summary>Details</summary>
Motivation: 模仿学习依赖人类示范数据，但大规模数据集中存在质量参差不齐的问题，影响策略性能。现有筛选方法依赖人工标注且粒度较粗，无法处理单个状态-动作对的质量问题。

Method: SCIZOR框架包含两个模块：1) 基于自监督任务进度预测器过滤次优数据；2) 基于联合状态-动作表征的去重模块处理冗余数据。

Result: 实验表明，SCIZOR能让模仿学习策略用更少数据达到更高性能，在多个基准测试中平均提升15.4%。

Conclusion: SCIZOR通过细粒度自监督数据筛选，有效解决了模仿学习中数据质量不均衡的问题，显著提升了策略性能。

Abstract: Imitation learning advances robot capabilities by enabling the acquisition of
diverse behaviors from human demonstrations. However, large-scale datasets used
for policy training often introduce substantial variability in quality, which
can negatively impact performance. As a result, automatically curating datasets
by filtering low-quality samples to improve quality becomes essential. Existing
robotic curation approaches rely on costly manual annotations and perform
curation at a coarse granularity, such as the dataset or trajectory level,
failing to account for the quality of individual state-action pairs. To address
this, we introduce SCIZOR, a self-supervised data curation framework that
filters out low-quality state-action pairs to improve the performance of
imitation learning policies. SCIZOR targets two complementary sources of
low-quality data: suboptimal data, which hinders learning with undesirable
actions, and redundant data, which dilutes training with repetitive patterns.
SCIZOR leverages a self-supervised task progress predictor for suboptimal data
to remove samples lacking task progression, and a deduplication module
operating on joint state-action representation for samples with redundant
patterns. Empirically, we show that SCIZOR enables imitation learning policies
to achieve higher performance with less data, yielding an average improvement
of 15.4% across multiple benchmarks. More information is available at:
https://ut-austin-rpl.github.io/SCIZOR/

</details>


### [412] [FastTD3: Simple, Fast, and Capable Reinforcement Learning for Humanoid Control](https://arxiv.org/abs/2505.22642)
*Younggyo Seo,Carmelo Sferrazza,Haoran Geng,Michal Nauman,Zhao-Heng Yin,Pieter Abbeel*

Main category: cs.RO

TL;DR: FastTD3是一种快速、简单且高效的强化学习算法，显著缩短了人形机器人在多个平台上的训练时间。


<details>
  <summary>Details</summary>
Motivation: 强化学习在机器人领域的应用面临复杂性和长训练时间的挑战，FastTD3旨在解决这些问题。

Method: 通过并行模拟、大批量更新、分布化评论器和精细调参的TD3算法改进。

Result: FastTD3在单个A100 GPU上3小时内完成HumanoidBench任务，训练过程稳定。

Conclusion: FastTD3提供了一种轻量级且易用的实现，加速了机器人领域的强化学习研究。

Abstract: Reinforcement learning (RL) has driven significant progress in robotics, but
its complexity and long training times remain major bottlenecks. In this
report, we introduce FastTD3, a simple, fast, and capable RL algorithm that
significantly speeds up training for humanoid robots in popular suites such as
HumanoidBench, IsaacLab, and MuJoCo Playground. Our recipe is remarkably
simple: we train an off-policy TD3 agent with several modifications -- parallel
simulation, large-batch updates, a distributional critic, and carefully tuned
hyperparameters. FastTD3 solves a range of HumanoidBench tasks in under 3 hours
on a single A100 GPU, while remaining stable during training. We also provide a
lightweight and easy-to-use implementation of FastTD3 to accelerate RL research
in robotics.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [413] [CPINN-ABPI: Physics-Informed Neural Networks for Accurate Power Estimation in MPSoCs](https://arxiv.org/abs/2505.22469)
*Mohamed R. Elshamy,Mehdi Elahi,Ahmad Patooghy,Abdel-Hameed A. Badawy*

Main category: cs.PF

TL;DR: 该论文首次在商用硬件上验证了ABPI方法的性能，发现其在实际应用中存在精度不足的问题，并提出结合CPINNs和遗传算法优化的新方法，显著提升了功耗估计的准确性和实时性。


<details>
  <summary>Details</summary>
Motivation: 现代多处理器系统芯片(MPSoCs)需要精确的功耗估计以实现高效的热管理和功耗管理。ABPI方法虽理论上消除了对稳态温度的依赖，但其在实际硬件中的性能尚未验证。

Method: 提出了一种新方法CPINN-ABPI，将定制物理信息神经网络(CPINNs)与ABPI的热模型结合，使用专门设计的损失函数和多目标遗传算法优化，以平衡估计精度和计算成本。

Result: 实验验证表明，CPINN-ABPI相比ABPI在CPU和GPU上的平均绝对误差(MAE)分别降低了84.7%和73.9%，加权平均绝对百分比误差(WMAPE)从47%-81%提升至约12%，且保持了195.3微秒的实时推理性能。

Conclusion: CPINN-ABPI方法在保持实时性能的同时，显著提高了功耗估计的准确性，适用于异构SoCs，为解决实际应用中的功耗管理问题提供了有效方案。

Abstract: Efficient thermal and power management in modern multiprocessor
systems-on-chip (MPSoCs) demands accurate power consumption estimation. One of
the state-of-the-art approaches, Alternative Blind Power Identification (ABPI),
theoretically eliminates the dependence on steady-state temperatures,
addressing a major shortcoming of previous approaches. However, ABPI
performance has remained unverified in actual hardware implementations. In this
study, we conduct the first empirical validation of ABPI on commercial hardware
using the NVIDIA Jetson Xavier AGX platform. Our findings reveal that, while
ABPI provides computational efficiency and independence from steady-state
temperature, it exhibits considerable accuracy deficiencies in real-world
scenarios. To overcome these limitations, we introduce a novel approach that
integrates Custom Physics-Informed Neural Networks (CPINNs) with the underlying
thermal model of ABPI. Our approach employs a specialized loss function that
harmonizes physical principles with data-driven learning, complemented by
multi-objective genetic algorithm optimization to balance estimation accuracy
and computational cost. In experimental validation, CPINN-ABPI achieves a
reduction of 84.7\% CPU and 73.9\% GPU in the mean absolute error (MAE)
relative to ABPI, with the weighted mean absolute percentage error (WMAPE)
improving from 47\%--81\% to $\sim$12\%. The method maintains real-time
performance with 195.3~$\mu$s of inference time, with similar 85\%--99\%
accuracy gains across heterogeneous SoCs.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [414] [Conformance Checking for Less: Efficient Conformance Checking for Long Event Sequences](https://arxiv.org/abs/2505.21506)
*Eli Bogdanov,Izack Cohen,Avigdor Gal*

Main category: cs.DB

TL;DR: ConLES是一种针对长事件序列的滑动窗口一致性检查方法，通过分区和迭代对齐显著降低计算复杂度，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 随着传感器和预测模型产生的长事件序列和大数据日志日益普遍，传统一致性检查方法因指数级计算复杂度而难以应对。

Method: 采用滑动窗口技术将长轨迹分割为可管理的子轨迹，并利用全局信息迭代对齐每个子轨迹与预期行为，丢弃局部最优但无前景的对齐方案。

Result: 在多个数据集上的性能评估表明，ConLES在长轨迹上优于现有最优和启发式算法，始终达到最优或接近最优解。

Conclusion: ConLES显著减少了搜索空间，高效扩展，并同时支持预定义和发现的流程模型，成为长事件序列一致性检查的领先解决方案。

Abstract: Long event sequences (termed traces) and large data logs that originate from
sensors and prediction models are becoming increasingly common in our data-rich
world. In such scenarios, conformance checking-validating a data log against an
expected system behavior (the process model) can become computationally
infeasible due to the exponential complexity of finding an optimal alignment.
To alleviate scalability challenges for this task, we propose ConLES, a
sliding-window conformance checking approach for long event sequences that
preserves the interpretability of alignment-based methods. ConLES partitions
traces into manageable subtraces and iteratively aligns each against the
expected behavior, leading to significant reduction of the search space while
maintaining overall accuracy. We use global information that captures
structural properties of both the trace and the process model, enabling
informed alignment decisions and discarding unpromising alignments, even if
they appear locally optimal. Performance evaluations across multiple datasets
highlight that ConLES outperforms the leading optimal and heuristic algorithms
for long traces, consistently achieving the optimal or near-optimal solution.
Unlike other conformance methods that struggle with long event sequences,
ConLES significantly reduces the search space, scales efficiently, and uniquely
supports both predefined and discovered process models, making it a viable and
leading option for conformance checking of long event sequences.

</details>


### [415] [StreamLink: Large-Language-Model Driven Distributed Data Engineering System](https://arxiv.org/abs/2505.21575)
*Dawei Feng,Di Mei,Huiri Tan,Lei Ren,Xianying Lou,Zhangxi Tan*

Main category: cs.DB

TL;DR: StreamLink是一个基于大型语言模型（LLM）的分布式数据系统，旨在提高数据工程任务的效率和可访问性，同时保护用户数据隐私。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言理解方面表现出色，为创新应用提供了可能。StreamLink旨在利用这一技术简化用户与复杂数据库系统的交互，同时确保数据隐私和安全性。

Method: StreamLink建立在Apache Spark和Hadoop等分布式框架之上，使用本地微调的LLM而非公共AI服务来处理自然语言查询并生成SQL，同时结合基于LLM的语法和安全检查器确保查询的可靠性和安全性。

Result: StreamLink在SQL生成方面的执行准确率比基线方法提高了10%以上，用户可以在几秒钟内从数亿条数据中找到最关心的项目。

Conclusion: StreamLink展示了将生成式LLM与分布式数据处理相结合的潜力，为用户提供了友好且安全的数据工程解决方案。

Abstract: Large Language Models (LLMs) have shown remarkable proficiency in natural
language understanding (NLU), opening doors for innovative applications. We
introduce StreamLink - an LLM-driven distributed data system designed to
improve the efficiency and accessibility of data engineering tasks. We build
StreamLink on top of distributed frameworks such as Apache Spark and Hadoop to
handle large data at scale. One of the important design philosophies of
StreamLink is to respect user data privacy by utilizing local fine-tuned LLMs
instead of a public AI service like ChatGPT. With help from domain-adapted
LLMs, we can improve our system's understanding of natural language queries
from users in various scenarios and simplify the procedure of generating
database queries like the Structured Query Language (SQL) for information
processing. We also incorporate LLM-based syntax and security checkers to
guarantee the reliability and safety of each generated query. StreamLink
illustrates the potential of merging generative LLMs with distributed data
processing for comprehensive and user-centric data engineering. With this
architecture, we allow users to interact with complex database systems at
different scales in a user-friendly and security-ensured manner, where the SQL
generation reaches over 10\% of execution accuracy compared to baseline
methods, and allow users to find the most concerned item from hundreds of
millions of items within a few seconds using natural language.

</details>


### [416] [ChatPD: An LLM-driven Paper-Dataset Networking System](https://arxiv.org/abs/2505.22349)
*Anjie Xu,Ruiqing Ding,Leye Wang*

Main category: cs.DB

TL;DR: ChatPD利用大语言模型自动从学术论文中提取数据集信息并构建结构化网络，优于现有平台。


<details>
  <summary>Details</summary>
Motivation: 现有学术平台如PapersWithCode在数据集管理上效率低下，手动工作流程存在瓶颈。

Method: 系统包含三个模块：论文收集、数据集信息提取和数据集实体解析，采用图补全与推理策略映射数据集描述到实体。

Result: ChatPD在数据集使用提取上优于PapersWithCode，实体解析任务达到约90%的精确率和召回率。

Conclusion: ChatPD成功部署并提供数据集发现服务，相关代码和网络已开源。

Abstract: Scientific research heavily depends on suitable datasets for method
validation, but existing academic platforms with dataset management like
PapersWithCode suffer from inefficiencies in their manual workflow. To overcome
this bottleneck, we present a system, called ChatPD, that utilizes Large
Language Models (LLMs) to automate dataset information extraction from academic
papers and construct a structured paper-dataset network. Our system consists of
three key modules: \textit{paper collection}, \textit{dataset information
extraction}, and \textit{dataset entity resolution} to construct paper-dataset
networks. Specifically, we propose a \textit{Graph Completion and Inference}
strategy to map dataset descriptions to their corresponding entities. Through
extensive experiments, we demonstrate that ChatPD not only outperforms the
existing platform PapersWithCode in dataset usage extraction but also achieves
about 90\% precision and recall in entity resolution tasks. Moreover, we have
deployed ChatPD to continuously extract which datasets are used in papers, and
provide a dataset discovery service, such as task-specific dataset queries and
similar dataset recommendations. We open source ChatPD and the current
paper-dataset network on this [GitHub
repository]{https://github.com/ChatPD-web/ChatPD}.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [417] [Network classification through random walks](https://arxiv.org/abs/2505.21706)
*Gonzalo Travieso,Joao Merenda,Odemir M. Bruno*

Main category: cs.SI

TL;DR: 该论文提出了一种基于随机游走统计量的新方法，用于从网络结构中推断系统类型，并在多个数据集上验证了其优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 研究网络模型时，一个重要问题是如何通过网络结构推断其代表的系统类型。现有方法多依赖结构测量和动态过程提取特征，但仍有改进空间。

Method: 提出了一种利用随机游走统计量来表征网络的新方法，这些统计量能有效反映网络特性。

Result: 新方法在多数情况下表现优异，优于现有方法，但在某些数据集上存在局限性。

Conclusion: 基于随机游走统计量的方法能有效推断网络类型，具有广泛应用潜力，但需进一步优化以适应不同数据集。

Abstract: Network models have been widely used to study diverse systems and analyze
their dynamic behaviors. Given the structural variability of networks, an
intriguing question arises: Can we infer the type of system represented by a
network based on its structure? This classification problem involves extracting
relevant features from the network. Existing literature has proposed various
methods that combine structural measurements and dynamical processes for
feature extraction. In this study, we introduce a novel approach to
characterize networks using statistics from random walks, which can be
particularly informative about network properties. We present the employed
statistical metrics and compare their performance on multiple datasets with
other state-of-the-art feature extraction methods. Our results demonstrate that
the proposed method is effective in many cases, often outperforming existing
approaches, although some limitations are observed across certain datasets.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [418] [RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for Complex Task Solving](https://arxiv.org/abs/2505.21577)
*Huacan Wang,Ziyi Ni,Shuo Zhang,Shuo Lu,Sen Hu,Ziyang He,Chen Hu,Jiaye Lin,Yifu Guo,Yuntao Du,Pin Lyu*

Main category: cs.SE

TL;DR: RepoMaster是一个自主代理框架，旨在探索和重用GitHub仓库以解决复杂任务，通过构建功能调用图、模块依赖图和分层代码树来提高效率，并在评估中显著提升了任务通过率和减少了令牌使用。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在代码生成方面取得了显著进展，但现实世界的任务通常需要完整的代码仓库而非简单脚本。GitHub上有大量开源仓库，但现有框架难以有效利用这些资源，主要因为信息过载和依赖关系复杂，且受限于当前大型语言模型的上下文窗口限制。

Method: RepoMaster通过构建功能调用图、模块依赖图和分层代码树来识别核心组件，仅将这些核心元素提供给大型语言模型。在自主执行过程中，它逐步探索相关组件并使用探索工具和剪枝信息来优化上下文使用。

Result: 在调整后的MLE-bench上，RepoMaster相对于最强基线OpenHands实现了110%的相对提升。在新发布的GitTaskBench上，任务通过率从24.1%提升至62.9%，同时令牌使用减少了95%。

Conclusion: RepoMaster通过有效探索和重用GitHub仓库，显著提升了解决复杂任务的效率和效果，同时优化了资源使用。

Abstract: The ultimate goal of code agents is to solve complex tasks autonomously.
Although large language models (LLMs) have made substantial progress in code
generation, real-world tasks typically demand full-fledged code repositories
rather than simple scripts. Building such repositories from scratch remains a
major challenge. Fortunately, GitHub hosts a vast, evolving collection of
open-source repositories, which developers frequently reuse as modular
components for complex tasks. Yet, existing frameworks like OpenHands and
SWE-Agent still struggle to effectively leverage these valuable resources.
Relying solely on README files provides insufficient guidance, and deeper
exploration reveals two core obstacles: overwhelming information and tangled
dependencies of repositories, both constrained by the limited context windows
of current LLMs. To tackle these issues, we propose RepoMaster, an autonomous
agent framework designed to explore and reuse GitHub repositories for solving
complex tasks. For efficient understanding, RepoMaster constructs function-call
graphs, module-dependency graphs, and hierarchical code trees to identify
essential components, providing only identified core elements to the LLMs
rather than the entire repository. During autonomous execution, it
progressively explores related components using our exploration tools and
prunes information to optimize context usage. Evaluated on the adjusted
MLE-bench, RepoMaster achieves a 110% relative boost in valid submissions over
the strongest baseline OpenHands. On our newly released GitTaskBench,
RepoMaster lifts the task-pass rate from 24.1% to 62.9% while reducing token
usage by 95%. Our code and demonstration materials are publicly available at
https://github.com/wanghuacan/RepoMaster.

</details>


### [419] [Leveraging XP and CRISP-DM for Agile Data Science Projects](https://arxiv.org/abs/2505.21603)
*Andre Massahiro Shimaoka,Renato Cordeiro Ferreira,Alfredo Goldman*

Main category: cs.SE

TL;DR: 研究探讨了在敏捷数据科学项目中如何整合极限编程(XP)与CRISP-DM标准流程，并通过电商公司案例验证了可行性。


<details>
  <summary>Details</summary>
Motivation: 探索如何将XP方法的敏捷性与CRISP-DM的结构化流程相结合，以优化数据科学项目的协作效率。

Method: 采用案例研究法，通过访谈和问卷调查收集电商公司数据科学团队的实践数据。

Result: 86%的团队高频使用CRISP-DM，71%采用XP实践，证实二者可有效结合。

Conclusion: XP与CRISP-DM的整合为数据科学项目提供了结构化协作框架，并提出了改进建议。

Abstract: This study explores the integration of eXtreme Programming (XP) and the
Cross-Industry Standard Process for Data Mining (CRISP-DM) in agile Data
Science projects. We conducted a case study at the e-commerce company Elo7 to
answer the research question: How can the agility of the XP method be
integrated with CRISP-DM in Data Science projects? Data was collected through
interviews and questionnaires with a Data Science team consisting of data
scientists, ML engineers, and data product managers. The results show that 86%
of the team frequently or always applies CRISP-DM, while 71% adopt XP practices
in their projects. Furthermore, the study demonstrates that it is possible to
combine CRISP-DM with XP in Data Science projects, providing a structured and
collaborative approach. Finally, the study generated improvement
recommendations for the company.

</details>


### [420] [GitGoodBench: A Novel Benchmark For Evaluating Agentic Performance On Git](https://arxiv.org/abs/2505.22583)
*Tobias Lindenbauer,Egor Bogomolov,Yaroslav Zharov*

Main category: cs.SE

TL;DR: GitGoodBench是一个新的基准测试，用于评估AI代理在版本控制系统任务上的性能，填补了现有基准测试的空白。


<details>
  <summary>Details</summary>
Motivation: 现有的软件工程AI代理基准测试（如SWE-bench）忽视了版本控制系统（VCS）操作等关键开发者工作流程，GitGoodBench旨在解决这一问题。

Method: GitGoodBench包含三个核心Git场景，提取自开源的Python、Java和Kotlin仓库，提供三个数据集：全面评估套件、快速原型版本和训练语料库。

Result: 使用配备自定义工具的GPT-4o在原型版本上实现了21.11%的解决率。

Conclusion: GitGoodBench有望成为开发真正全面的软件工程代理的重要基石，超越单纯的编程能力。

Abstract: Benchmarks for Software Engineering (SE) AI agents, most notably SWE-bench,
have catalyzed progress in programming capabilities of AI agents. However, they
overlook critical developer workflows such as Version Control System (VCS)
operations. To address this issue, we present GitGoodBench, a novel benchmark
for evaluating AI agent performance on VCS tasks. GitGoodBench covers three
core Git scenarios extracted from permissive open-source Python, Java, and
Kotlin repositories. Our benchmark provides three datasets: a comprehensive
evaluation suite (900 samples), a rapid prototyping version (120 samples), and
a training corpus (17,469 samples). We establish baseline performance on the
prototyping version of our benchmark using GPT-4o equipped with custom tools,
achieving a 21.11% solve rate overall. We expect GitGoodBench to serve as a
crucial stepping stone toward truly comprehensive SE agents that go beyond mere
programming.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [421] [CSI-Bench: A Large-Scale In-the-Wild Dataset for Multi-task WiFi Sensing](https://arxiv.org/abs/2505.21866)
*Guozhen Zhu,Yuqian Hu,Weihang Gao,Wei-Hsiang Wang,Beibei Wang,K. J. Ray Liu*

Main category: eess.SP

TL;DR: 论文提出CSI-Bench，一个大规模、真实环境下的WiFi感知基准数据集，用于提升健康监测等应用的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有WiFi感知系统在真实环境中泛化能力不足，主要因为数据采集在受控环境下进行，硬件单一且数据片段化。

Method: 收集了26个不同室内环境、35名真实用户的461小时数据，涵盖跌倒检测、呼吸监测等任务，并提供标准化评估分割和基线结果。

Result: CSI-Bench数据集捕捉了真实信号变化，支持单任务和多任务学习，为开发鲁棒模型奠定了基础。

Conclusion: CSI-Bench为可扩展、保护隐私的WiFi感知系统在健康等以人为本的应用中提供了基础。

Abstract: WiFi sensing has emerged as a compelling contactless modality for human
activity monitoring by capturing fine-grained variations in Channel State
Information (CSI). Its ability to operate continuously and non-intrusively
while preserving user privacy makes it particularly suitable for health
monitoring. However, existing WiFi sensing systems struggle to generalize in
real-world settings, largely due to datasets collected in controlled
environments with homogeneous hardware and fragmented, session-based recordings
that fail to reflect continuous daily activity.
  We present CSI-Bench, a large-scale, in-the-wild benchmark dataset collected
using commercial WiFi edge devices across 26 diverse indoor environments with
35 real users. Spanning over 461 hours of effective data, CSI-Bench captures
realistic signal variability under natural conditions. It includes
task-specific datasets for fall detection, breathing monitoring, localization,
and motion source recognition, as well as a co-labeled multitask dataset with
joint annotations for user identity, activity, and proximity. To support the
development of robust and generalizable models, CSI-Bench provides standardized
evaluation splits and baseline results for both single-task and multi-task
learning. CSI-Bench offers a foundation for scalable, privacy-preserving WiFi
sensing systems in health and broader human-centric applications.

</details>


### [422] [Empowering Intelligent Low-altitude Economy with Large AI Model Deployment](https://arxiv.org/abs/2505.22343)
*Zhonghao Lyu,Yulan Gao,Junting Chen,Hongyang Du,Jie Xu,Kaibin Huang,Dong In Kim*

Main category: eess.SP

TL;DR: 本文探讨了低空经济(LAE)中部署大型人工智能模型(LAIMs)的挑战，并提出了一种分层系统架构和任务导向的执行流程来解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 低空经济作为一种新兴经济模式，需要智能化的服务支持。然而，大型人工智能模型在低空经济中的部署面临计算资源不足、模型与动态环境不匹配以及传统设计效率低下等挑战。

Method: 提出了一种分层系统架构，设计了任务导向的执行流程，并通过实际案例验证了框架的有效性。

Result: 通过案例研究验证了所提框架的可行性和有效性，能够支持低空经济中智能服务的可扩展和自适应交付。

Conclusion: 本文提出的框架为解决低空经济中大型人工智能模型部署的挑战提供了有效方案，并指出了未来研究的开放性问题。

Abstract: Low-altitude economy (LAE) represents an emerging economic paradigm that
redefines commercial and social aerial activities. Large artificial
intelligence models (LAIMs) offer transformative potential to further enhance
the intelligence of LAE services. However, deploying LAIMs in LAE poses several
challenges, including the significant gap between their computational/storage
demands and the limited onboard resources of LAE entities, the mismatch between
lab-trained LAIMs and dynamic physical environments, and the inefficiencies of
traditional decoupled designs for sensing, communication, and computation. To
address these issues, we first propose a hierarchical system architecture
tailored for LAIM deployment and present representative LAE application
scenarios. Next, we explore key enabling techniques that facilitate the mutual
co-evolution of LAIMs and low-altitude systems, and introduce a task-oriented
execution pipeline for scalable and adaptive service delivery. Then, the
proposed framework is validated through real-world case studies. Finally, we
outline open challenges to inspire future research.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [423] [Automatic detection of abnormal clinical EEG: comparison of a finetuned foundation model with two deep learning models](https://arxiv.org/abs/2505.21507)
*Aurore Bussalb,François Le Gac,Guillaume Jubien,Mohamed Rahmouni,Ruggero G. Bettinardi,Pedro Marinho R. de Oliveira,Phillipe Derambure,Nicolas Gaspard,Jacques Jonas,Louis Maillard,Laurent Vercueil,Hervé Vespignani,Philippe Laval,Laurent Koessler,Ulysse Gimenez*

Main category: q-bio.NC

TL;DR: 比较三种深度学习模型（CNN-LSTM、Transformer和BioSerenity-E1）在EEG分类任务中的表现，预训练模型BioSerenity-E1表现最佳。


<details>
  <summary>Details</summary>
Motivation: 由于EEG解读需求量大且需要专业知识，开发AI工具辅助视觉分析。

Method: 训练或微调三种模型（CNN-LSTM、Transformer和BioSerenity-E1）于2,500个EEG记录，并在三个数据集上评估性能。

Result: BioSerenity-E1微调后在所有数据集上表现最佳，最高平衡准确率达89.19%和94.63%，在TUAB数据集上达82.25%。

Conclusion: 预训练模型能有效提升EEG自动分类的鲁棒性和效率，减少资源需求并扩大应用范围。

Abstract: Electroencephalography (EEG) is commonly used by physicians for the diagnosis
of numerous neurological disorders. Due to the large volume of EEGs requiring
interpretation and the specific expertise involved, artificial
intelligence-based tools are being developed to assist in their visual
analysis. In this paper, we compare two deep learning models (CNN-LSTM and
Transformer-based) with BioSerenity-E1, a recently proposed foundation model,
in the task of classifying entire EEG recordings as normal or abnormal. The
three models were trained or finetuned on 2,500 EEG recordings and their
performances were evaluated on two private and one public datasets: a large
multicenter dataset annotated by a single specialist (dataset A composed of n =
4,480 recordings), a small multicenter dataset annotated by three specialists
(dataset B, n = 198), and the Temple University Abnormal (TUAB) EEG corpus
evaluation dataset (n = 276). On dataset A, the three models achieved at least
86% balanced accuracy, with BioSerenity-E1 finetuned achieving the highest
balanced accuracy (89.19% [88.36-90.41]). BioSerenity-E1 finetuned also
achieved the best performance on dataset B, with 94.63% [92.32-98.12] balanced
accuracy. The models were then validated on TUAB evaluation dataset, whose
corresponding training set was not used during training, where they achieved at
least 76% accuracy. Specifically, BioSerenity-E1 finetuned outperformed the
other two models, reaching an accuracy of 82.25% [78.27-87.48]. Our results
highlight the usefulness of leveraging pre-trained models for automatic EEG
classification: enabling robust and efficient interpretation of EEG data with
fewer resources and broader applicability.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [424] [iDSE: Navigating Design Space Exploration in High-Level Synthesis Using LLMs](https://arxiv.org/abs/2505.22086)
*Runkai Li,Jia Xiong,Xi Wang*

Main category: cs.AR

TL;DR: 该论文提出了iDSE框架，利用大型语言模型(LLM)优化高层次综合(HLS)的设计空间探索，显著提升效率和质量。


<details>
  <summary>Details</summary>
Motivation: 传统高层次综合(HLS)设计空间探索方法面临组合爆炸问题，导致探索成本高且结果不理想，亟需更高效的解决方案。

Method: 提出iDSE框架，通过LLM的收敛和发散思维模式智能剪枝设计空间，并校准初始采样设计，实现多路径优化。

Result: 实验表明iDSE比启发式方法接近参考Pareto前沿5.1~16.6倍，仅需NSGA-II 4.6%的设计探索量即可达到同等效果。

Conclusion: iDSE展现了LLM在可扩展高效HLS优化中的变革潜力，为多目标优化问题提供了新思路。

Abstract: High-Level Synthesis (HLS) serves as an agile hardware development tool that
streamlines the circuit design by abstracting the register transfer level into
behavioral descriptions, while allowing designers to customize the generated
microarchitectures through optimization directives. However, the combinatorial
explosion of possible directive configurations yields an intractable design
space. Traditional design space exploration (DSE) methods, despite adopting
heuristics or constructing predictive models to accelerate Pareto-optimal
design acquisition, still suffer from prohibitive exploration costs and
suboptimal results. Addressing these concerns, we introduce iDSE, the first
LLM-aided DSE framework that leverages HLS design quality perception to
effectively navigate the design space. iDSE intelligently pruns the design
space to guide LLMs in calibrating representative initial sampling designs,
expediting convergence toward the Pareto front. By exploiting the convergent
and divergent thinking patterns inherent in LLMs for hardware optimization,
iDSE achieves multi-path refinement of the design quality and diversity.
Extensive experiments demonstrate that iDSE outperforms heuristic-based DSE
methods by 5.1$\times$$\sim$16.6$\times$ in proximity to the reference Pareto
front, matching NSGA-II with only 4.6% of the explored designs. Our work
demonstrates the transformative potential of LLMs in scalable and efficient HLS
design optimization, offering new insights into multiobjective optimization
challenges.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [425] [tenSVD algorithm for compression](https://arxiv.org/abs/2505.21686)
*Michele Gallo*

Main category: stat.CO

TL;DR: 该论文提出了一种基于张量的高效图像存储方法，通过Tucker模型压缩数据，旨在减少存储、传输和处理能耗，并在R语言中实现与基准算法对比。


<details>
  <summary>Details</summary>
Motivation: 张量为高维数据处理提供了强大框架，但现有方法在存储、传输和能耗效率上仍有提升空间。本研究旨在通过张量压缩优化这些指标。

Method: 将原始数据组织为高阶张量并应用Tucker模型进行压缩，使用R语言实现并与基准算法比较。

Result: 通过仿真和真实数据集评估，在计算时间和信息保留质量上表现优异，并量化分析了算法的能耗可持续性。

Conclusion: 该方法显著提升了图像存储效率，在计算性能与能耗可持续性之间取得了平衡，为高维数据处理提供了实用解决方案。

Abstract: Tensors provide a robust framework for managing high-dimensional data.
Consequently, tensor analysis has emerged as an active research area in various
domains, including machine learning, signal processing, computer vision, graph
analysis, and data mining. This study introduces an efficient image storage
approach utilizing tensors, aiming to minimize memory to store, bandwidth to
transmit and energy to processing. The proposed method organizes original data
into a higher-order tensor and applies the Tucker model for compression.
Implemented in R, this method is compared to a baseline algorithm. The
evaluation focuses on efficient of algorithm measured in term of computational
time and the quality of information preserved, using both simulated and real
datasets. A detailed analysis of the results is conducted, employing
established quantitative metrics, with significant attention paid to
sustainability in terms of energy consumption across algorithms.

</details>


### [426] [Are Statistical Methods Obsolete in the Era of Deep Learning?](https://arxiv.org/abs/2505.21723)
*Skyler Wu,Shihao Yang,S. C. Kou*

Main category: stat.CO

TL;DR: 论文通过对比深度学习方法（PINN）与统计方法（MAGI）在ODE逆问题中的表现，证明统计方法在稀疏噪声数据、参数推断和轨迹重建等任务中仍具优势。


<details>
  <summary>Details</summary>
Motivation: 探讨在AI时代，深度学习方法盛行下，传统统计方法是否仍具价值。通过ODE逆问题作为测试平台，比较两种方法的性能。

Method: 使用PINN代表深度学习方法，MAGI代表统计方法，通过流行病学SEIR模型和混沌动力学Lorenz模型进行案例研究。

Result: 统计方法在稀疏噪声数据下表现更优，参数推断和轨迹重建偏差更低，且计算资源需求更少。在样本外预测中，统计方法显著优于过参数化的深度学习模型。

Conclusion: 统计方法在特定场景下仍不可替代，尤其是在数据稀疏、噪声大或需要高精度建模时，其鲁棒性和计算效率具有显著优势。

Abstract: In the era of AI, neural networks have become increasingly popular for
modeling, inference, and prediction, largely due to their potential for
universal approximation. With the proliferation of such deep learning models, a
question arises: are leaner statistical methods still relevant? To shed insight
on this question, we employ the mechanistic nonlinear ordinary differential
equation (ODE) inverse problem as a testbed, using physics-informed neural
network (PINN) as a representative of the deep learning paradigm and
manifold-constrained Gaussian process inference (MAGI) as a representative of
statistically principled methods. Through case studies involving the SEIR model
from epidemiology and the Lorenz model from chaotic dynamics, we demonstrate
that statistical methods are far from obsolete, especially when working with
sparse and noisy observations. On tasks such as parameter inference and
trajectory reconstruction, statistically principled methods consistently
achieve lower bias and variance, while using far fewer parameters and requiring
less hyperparameter tuning. Statistical methods can also decisively outperform
deep learning models on out-of-sample future prediction, where the absence of
relevant data often leads overparameterized models astray. Additionally, we
find that statistically principled approaches are more robust to accumulation
of numerical imprecision and can represent the underlying system more faithful
to the true governing ODEs.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [427] [VoiceMark: Zero-Shot Voice Cloning-Resistant Watermarking Approach Leveraging Speaker-Specific Latents](https://arxiv.org/abs/2505.21568)
*Haiyun Li,Zhiyong Wu,Xiaofeng Xie,Jingran Xie,Yaoxun Xu,Hanyang Peng*

Main category: cs.SD

TL;DR: VoiceMark是首个针对零样本语音克隆的水印方法，通过说话人特定潜在特征作为水印载体，显著提升水印在合成音频中的检测准确率至95%以上。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法在传统语音克隆模型中有效，但在零样本场景下（模型未经训练直接合成音频）失效，检测准确率仅约50%。需要一种能抵抗零样本克隆的水印技术。

Method: 1. 使用说话人特定潜在特征作为水印载体
2. 引入VC模拟增强和基于VAD的损失函数增强鲁棒性

Result: 在多个零样本VC模型上测试，水印检测准确率超过95%，远超现有方法的50%水平。

Conclusion: VoiceMark首次实现零样本语音克隆场景下的有效水印追踪，为音频版权保护提供新解决方案。

Abstract: Voice cloning (VC)-resistant watermarking is an emerging technique for
tracing and preventing unauthorized cloning. Existing methods effectively trace
traditional VC models by training them on watermarked audio but fail in
zero-shot VC scenarios, where models synthesize audio from an audio prompt
without training. To address this, we propose VoiceMark, the first zero-shot
VC-resistant watermarking method that leverages speaker-specific latents as the
watermark carrier, allowing the watermark to transfer through the zero-shot VC
process into the synthesized audio. Additionally, we introduce VC-simulated
augmentations and VAD-based loss to enhance robustness against distortions.
Experiments on multiple zero-shot VC models demonstrate that VoiceMark achieves
over 95% accuracy in watermark detection after zero-shot VC synthesis,
significantly outperforming existing methods, which only reach around 50%. See
our code and demos at: https://huggingface.co/spaces/haiyunli/VoiceMark

</details>


### [428] [Music Source Restoration](https://arxiv.org/abs/2505.21827)
*Yongyi Zang,Zheqi Dai,Mark D. Plumbley,Qiuqiang Kong*

Main category: cs.SD

TL;DR: 论文提出音乐源修复(MSR)任务，解决现有音乐源分离(MSS)忽略制作过程中信号退化的问题，并发布首个包含原始音轨的分层数据集RawStems。


<details>
  <summary>Details</summary>
Motivation: 现有音乐源分离方法假设混合音轨是简单叠加，忽略了均衡、压缩、混响等实际制作中的信号退化，导致分离效果不理想。

Method: 提出MSR任务框架，将混合音轨建模为退化源信号的叠加；构建RawStems数据集(578首歌/354小时)，包含8大类17小类原始音轨；建立U-Former基线方法模拟5种退化类型。

Result: 实验证明MSR任务在RawStems数据集上可行，并开源了数据集标注、退化模拟流程、训练代码与预训练模型。

Conclusion: MSR填补了理想化源分离与真实音乐制作的鸿沟，RawStems作为首个分层原始音轨数据集将推动该领域发展。

Abstract: We introduce Music Source Restoration (MSR), a novel task addressing the gap
between idealized source separation and real-world music production. Current
Music Source Separation (MSS) approaches assume mixtures are simple sums of
sources, ignoring signal degradations employed during music production like
equalization, compression, and reverb. MSR models mixtures as degraded sums of
individually degraded sources, with the goal of recovering original, undegraded
signals. Due to the lack of data for MSR, we present RawStems, a dataset
annotation of 578 songs with unprocessed source signals organized into 8
primary and 17 secondary instrument groups, totaling 354.13 hours. To the best
of our knowledge, RawStems is the first dataset that contains unprocessed music
stems with hierarchical categories. We consider spectral filtering, dynamic
range compression, harmonic distortion, reverb and lossy codec as possible
degradations, and establish U-Former as a baseline method, demonstrating the
feasibility of MSR on our dataset. We release the RawStems dataset annotations,
degradation simulation pipeline, training code and pre-trained models to be
publicly available.

</details>


### [429] [Visual Cues Support Robust Turn-taking Prediction in Noise](https://arxiv.org/abs/2505.22088)
*Sam O'Connor Russell,Naomi Harte*

Main category: cs.SD

TL;DR: 研究探讨了预测性轮流模型（PTTM）在噪音环境下的表现，发现多模态PTTM能更好利用视觉线索提升准确性，但依赖精确转录。


<details>
  <summary>Details</summary>
Motivation: 预测性轮流模型（PTTM）对自然的人机交互至关重要，但对其在噪音环境下的性能知之甚少。本研究旨在探索PTTM在可能遇到的噪音类型中的表现。

Method: 研究分析了PTTM在不同噪音类型下的性能，并训练了一个包含视觉特征的多模态PTTM，以更好地利用视觉线索。

Result: PTTM对噪音高度敏感，准确性从干净语音的84%降至10分贝音乐噪音的52%。多模态PTTM在所有噪音类型和信噪比下均优于仅音频的PTTM，但无法泛化到新噪音类型。

Conclusion: 多模态PTTM能有效利用视觉线索提升噪音环境下的性能，但依赖精确转录，且泛化能力有限。研究公开了代码以供未来研究。

Abstract: Accurate predictive turn-taking models (PTTMs) are essential for naturalistic
human-robot interaction. However, little is known about their performance in
noise. This study therefore explores PTTM performance in types of noise likely
to be encountered once deployed. Our analyses reveal PTTMs are highly sensitive
to noise. Hold/shift accuracy drops from 84% in clean speech to just 52% in 10
dB music noise. Training with noisy data enables a multimodal PTTM, which
includes visual features to better exploit visual cues, with 72% accuracy in 10
dB music noise. The multimodal PTTM outperforms the audio-only PTTM across all
noise types and SNRs, highlighting its ability to exploit visual cues; however,
this does not always generalise to new types of noise. Analysis also reveals
that successful training relies on accurate transcription, limiting the use of
ASR-derived transcriptions to clean conditions. We make code publicly available
for future research.

</details>


### [430] [Advancing Hearing Assessment: An ASR-Based Frequency-Specific Speech Test for Diagnosing Presbycusis](https://arxiv.org/abs/2505.22231)
*Stefan Bleeck*

Main category: cs.SD

TL;DR: 论文提出了一种基于自动语音识别（ASR）的频率特异性语音测试方法，用于更精确地诊断老年性耳聋等听力损失对语音理解的影响。


<details>
  <summary>Details</summary>
Motivation: 传统听力测试难以全面评估听力损失对语音理解的功能性影响，尤其是在超阈值缺陷和频率特异性感知挑战方面。

Method: 利用ASR模拟中度斜坡型听力损失的感知效果，通过控制声学退化处理语音刺激，并分析音素级别的混淆模式。

Result: 模拟听力损失导致特定的音素混淆，主要影响高频辅音，并导致显著的音素删除，与老年性耳聋的声学线索退化一致。

Conclusion: ASR驱动的方法为开发客观、细粒度和频率特异性的听力评估工具提供了新途径，未来将通过与人类参与者的验证和高级AI模型的集成进一步提升诊断精度。

Abstract: Traditional audiometry often fails to fully characterize the functional
impact of hearing loss on speech understanding, particularly supra-threshold
deficits and frequency-specific perception challenges in conditions like
presbycusis. This paper presents the development and simulated evaluation of a
novel Automatic Speech Recognition (ASR)-based frequency-specific speech test
designed to provide granular diagnostic insights. Our approach leverages ASR to
simulate the perceptual effects of moderate sloping hearing loss by processing
speech stimuli under controlled acoustic degradation and subsequently analyzing
phoneme-level confusion patterns. Key findings indicate that simulated hearing
loss introduces specific phoneme confusions, predominantly affecting
high-frequency consonants (e.g., alveolar/palatal to labiodental substitutions)
and leading to significant phoneme deletions, consistent with the acoustic cues
degraded in presbycusis. A test battery curated from these ASR-derived
confusions demonstrated diagnostic value, effectively differentiating between
simulated normal-hearing and hearing-impaired listeners in a comprehensive
simulation. This ASR-driven methodology offers a promising avenue for
developing objective, granular, and frequency-specific hearing assessment tools
that complement traditional audiometry. Future work will focus on validating
these findings with human participants and exploring the integration of
advanced AI models for enhanced diagnostic precision.

</details>


### [431] [Effective Context in Neural Speech Models](https://arxiv.org/abs/2505.22487)
*Yen Meng,Sharon Goldwater,Hao Tang*

Main category: cs.SD

TL;DR: 论文提出两种测量语音Transformer有效上下文长度的方法，发现不同任务所需上下文长度不同，且自监督模型的上下文使用集中在早期层，最终证明HuBERT无需修改即可流式运行。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注如何扩展神经语音模型的上下文长度，但缺乏对模型实际使用上下文长度（有效上下文）的量化分析。本文旨在填补这一空白。

Method: 提出两种测量有效上下文的方法，并应用于分析监督模型（基频跟踪、音素分类、词语分类任务）和自监督模型（HuBERT等）。

Result: 监督模型的有效上下文与任务复杂度正相关；自监督模型的有效上下文主要在前几层增长且整体较短（类似音素分类模型）。实验证明HuBERT可直接用于流式场景。

Conclusion: 语音模型实际使用的上下文长度远小于理论最大值，这一发现为模型优化（如流式部署）提供了实证依据。

Abstract: Modern neural speech models benefit from having longer context, and many
approaches have been proposed to increase the maximum context a model can use.
However, few have attempted to measure how much context these models actually
use, i.e., the effective context. Here, we propose two approaches to measuring
the effective context, and use them to analyze different speech Transformers.
For supervised models, we find that the effective context correlates well with
the nature of the task, with fundamental frequency tracking, phone
classification, and word classification requiring increasing amounts of
effective context. For self-supervised models, we find that effective context
increases mainly in the early layers, and remains relatively short -- similar
to the supervised phone model. Given that these models do not use a long
context during prediction, we show that HuBERT can be run in streaming mode
without modification to the architecture and without further fine-tuning.

</details>


### [432] [Improving Respiratory Sound Classification with Architecture-Agnostic Knowledge Distillation from Ensembles](https://arxiv.org/abs/2505.22027)
*Miika Toikkanen,June-Woo Kim*

Main category: cs.SD

TL;DR: 该研究通过软标签训练方法，将教师模型的知识高效蒸馏到学生模型中，显著提升了呼吸音分类的性能，且计算成本仅增加在训练阶段。


<details>
  <summary>Details</summary>
Motivation: 呼吸音数据集规模和质量有限，导致高性能难以实现。集成模型虽有效，但推理时计算成本增加。软标签训练仅在训练阶段增加成本，能高效蒸馏知识。

Method: 采用软标签训练方法，将多个教师模型的知识蒸馏到学生模型中，探索不同变体，发现即使单个教师模型也能显著提升性能。

Result: 在ICHBI上达到64.39的新最高分，超过之前最佳0.85分，平均分数提升超过1.16分。

Conclusion: 软标签知识蒸馏在呼吸音分类中非常有效，不受模型大小或架构影响。

Abstract: Respiratory sound datasets are limited in size and quality, making high
performance difficult to achieve. Ensemble models help but inevitably increase
compute cost at inference time. Soft label training distills knowledge
efficiently with extra cost only at training. In this study, we explore soft
labels for respiratory sound classification as an architecture-agnostic
approach to distill an ensemble of teacher models into a student model. We
examine different variations of our approach and find that even a single
teacher, identical to the student, considerably improves performance beyond its
own capability, with optimal gains achieved using only a few teachers. We
achieve the new state-of-the-art Score of 64.39 on ICHBI, surpassing the
previous best by 0.85 and improving average Scores across architectures by more
than 1.16. Our results highlight the effectiveness of knowledge distillation
with soft labels for respiratory sound classification, regardless of size or
architecture.

</details>


### [433] [AudioTurbo: Fast Text-to-Audio Generation with Rectified Diffusion](https://arxiv.org/abs/2505.22106)
*Junqi Zhao,Jinzheng Zhao,Haohe Liu,Yun Chen,Lu Han,Xubo Liu,Mark Plumbley,Wenwu Wang*

Main category: cs.SD

TL;DR: AudioTurbo结合预训练扩散模型与修正扩散方法，仅需10步采样即可超越现有模型，并将推理步数压缩至3步。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在音频生成质量与多样性上表现优异，但推理速度慢；修正流方法虽加速推理，但需从头训练且低步数下性能不佳。

Method: 提出AudioTurbo，利用预训练TTA模型生成的确定性噪声样本对学习一阶ODE路径，整合预训练模型与修正扩散方法。

Result: 在AudioCaps数据集上，仅用10步采样即超越现有模型，推理步数可进一步降至3步。

Conclusion: 该方法有效平衡了生成质量与推理效率，为文本到音频生成提供了高效解决方案。

Abstract: Diffusion models have significantly improved the quality and diversity of
audio generation but are hindered by slow inference speed. Rectified flow
enhances inference speed by learning straight-line ordinary differential
equation (ODE) paths. However, this approach requires training a flow-matching
model from scratch and tends to perform suboptimally, or even poorly, at low
step counts. To address the limitations of rectified flow while leveraging the
advantages of advanced pre-trained diffusion models, this study integrates
pre-trained models with the rectified diffusion method to improve the
efficiency of text-to-audio (TTA) generation. Specifically, we propose
AudioTurbo, which learns first-order ODE paths from deterministic noise sample
pairs generated by a pre-trained TTA model. Experiments on the AudioCaps
dataset demonstrate that our model, with only 10 sampling steps, outperforms
prior models and reduces inference to 3 steps compared to a flow-matching-based
acceleration model.

</details>


### [434] [Voice Quality Dimensions as Interpretable Primitives for Speaking Style for Atypical Speech and Affect](https://arxiv.org/abs/2505.21809)
*Jaya Narain,Vasudha Kowtha,Colin Lea,Lauren Tooley,Dianna Yee,Vikramjit Mitra,Zifang Huang,Miquel Espi Marques,Jon Huang,Carlos Avendano,Shirley Ren*

Main category: cs.SD

TL;DR: 该研究开发并评估了七种语音质量维度的模型，展示了在跨语言和任务中的强零样本性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过语音质量维度描述非典型语音和其他语音调制的关键特征，以提升语音相关任务的性能。

Method: 使用预训练模型的嵌入特征，在包含11,184个样本的公共数据集上训练七种语音维度的探针。

Result: 模型在SAP数据集中表现优异，并在未见过的语言和任务（如意大利语非典型语音和情感语音）中展示了强零样本性能。

Conclusion: 语音质量维度在语音风格相关任务中具有实用性和可解释性。

Abstract: Perceptual voice quality dimensions describe key characteristics of atypical
speech and other speech modulations. Here we develop and evaluate voice quality
models for seven voice and speech dimensions (intelligibility, imprecise
consonants, harsh voice, naturalness, monoloudness, monopitch, and
breathiness). Probes were trained on the public Speech Accessibility (SAP)
project dataset with 11,184 samples from 434 speakers, using embeddings from
frozen pre-trained models as features. We found that our probes had both strong
performance and strong generalization across speech elicitation categories in
the SAP dataset. We further validated zero-shot performance on additional
datasets, encompassing unseen languages and tasks: Italian atypical speech,
English atypical speech, and affective speech. The strong zero-shot performance
and the interpretability of results across an array of evaluations suggests the
utility of using voice quality dimensions in speaking style-related tasks.

</details>


### [435] [Effective and Efficient One-pass Compression of Speech Foundation Models Using Sparsity-aware Self-pinching Gates](https://arxiv.org/abs/2505.22608)
*Haoning Xu,Zhaoqing Li,Youjun Chen,Huimeng Wang,Guinan Li,Mengzhe Geng,Chengxi Deng,Xunying Liu*

Main category: cs.SD

TL;DR: 提出一种新颖的语音基础模型压缩方法，通过单阶段整合模型剪枝和参数更新，显著减少参数数量且保持性能。


<details>
  <summary>Details</summary>
Motivation: 当前语音基础模型压缩方法在保持性能的同时难以高效减少参数数量，需要更高效的压缩技术。

Method: 采用紧凑的层级别自压缩门（含单个可学习阈值）与未压缩模型联合训练，实现细粒度神经元剪枝。

Result: 在LibriSpeech-100hr上，wav2vec2.0-base和HuBERT-large参数分别减少65%和60%，WER无显著上升，压缩时间减少25%。

Conclusion: 该方法在高效压缩模型的同时保持性能，优于现有方法，为语音模型压缩提供了新思路。

Abstract: This paper presents a novel approach for speech foundation models compression
that tightly integrates model pruning and parameter update into a single stage.
Highly compact layer-level tied self-pinching gates each containing only a
single learnable threshold are jointly trained with uncompressed models and
used in fine-grained neuron level pruning. Experiments conducted on the
LibriSpeech-100hr corpus suggest that our approach reduces the number of
parameters of wav2vec2.0-base and HuBERT-large models by 65% and 60%
respectively, while incurring no statistically significant word error rate
(WER) increase on the test-clean dataset. Compared to previously published
methods on the same task, our approach not only achieves the lowest WER of
7.05% on the test-clean dataset under a comparable model compression ratio of
4.26x, but also operates with at least 25% less model compression time.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [436] [Provably Robust Training of Quantum Circuit Classifiers Against Parameter Noise](https://arxiv.org/abs/2505.18478)
*Lucas Tecot,Di Luo,Cho-Jui Hsieh*

Main category: quant-ph

TL;DR: 提出了一种抗噪声的量子电路分类器训练方法，通过理论保证和算法改进提升量子计算的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 量子计算虽具潜力，但噪声问题阻碍了可靠算法的实现，需要提升参数化量子电路分类器的抗噪声能力。

Method: 结合进化策略的理论，开发了一种函数无关且适应性强的方法，对常用优化算法仅需最小调整。

Result: 在量子相位分类任务中验证了方法的有效性，适用于多种量子电路。

Conclusion: 为近期量子计算机的实际应用提供了新的鲁棒性优化理论和方法。

Abstract: Advancements in quantum computing have spurred significant interest in
harnessing its potential for speedups over classical systems. However, noise
remains a major obstacle to achieving reliable quantum algorithms. In this
work, we present a provably noise-resilient training theory and algorithm to
enhance the robustness of parameterized quantum circuit classifiers. Our
method, with a natural connection to Evolutionary Strategies, guarantees
resilience to parameter noise with minimal adjustments to commonly used
optimization algorithms. Our approach is function-agnostic and adaptable to
various quantum circuits, successfully demonstrated in quantum phase
classification tasks. By developing provably guaranteed optimization theory
with quantum circuits, our work opens new avenues for practical, robust
applications of near-term quantum computers.

</details>


### [437] [Physics-inspired Generative AI models via real hardware-based noisy quantum diffusion](https://arxiv.org/abs/2505.22193)
*Marco Parigi,Stefano Martina,Francesco Aldo Venturelli,Filippo Caruso*

Main category: quant-ph

TL;DR: 本文提出两种量子扩散模型（QDMs）方法，利用量子特性提升生成式AI性能，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 量子扩散模型（QDMs）旨在利用量子特性提升经典生成式AI性能，但现有算法受限于近量子设备的可扩展性。本文探索新方法以解决这一问题。

Method: 提出两种物理启发的协议：1）利用量子随机游走形式，在正向过程中结合量子与经典动力学；2）利用IBM量子硬件的固有噪声生成图像。

Result: 第一种方法生成MNIST图像集的FID分数低于纯经典动力学；第二种方法仅用4个量子比特实现图像生成，验证了量子噪声可作为有用资源。

Conclusion: 本研究为大规模量子生成式AI算法开辟了新方向，展示了量子噪声的可利用性，而非仅作为需校正的干扰。

Abstract: Quantum Diffusion Models (QDMs) are an emerging paradigm in Generative AI
that aims to use quantum properties to improve the performances of their
classical counterparts. However, existing algorithms are not easily scalable
due to the limitations of near-term quantum devices. Following our previous
work on QDMs, here we propose and implement two physics-inspired protocols. In
the first, we use the formalism of quantum stochastic walks, showing that a
specific interplay of quantum and classical dynamics in the forward process
produces statistically more robust models generating sets of MNIST images with
lower Fr\'echet Inception Distance (FID) than using totally classical dynamics.
In the second approach, we realize an algorithm to generate images by
exploiting the intrinsic noise of real IBM quantum hardware with only four
qubits. Our work could be a starting point to pave the way for new scenarios
for large-scale algorithms in quantum Generative AI, where quantum noise is
neither mitigated nor corrected, but instead exploited as a useful resource.

</details>


### [438] [Hyperbolic recurrent neural network as the first type of non-Euclidean neural quantum state ansatz](https://arxiv.org/abs/2505.22083)
*H. L. Dao*

Main category: quant-ph

TL;DR: 论文提出首个非欧几里得神经量子态（NQS）变分方法——双曲GRU，在多种量子多体系统中表现优于传统欧几里得RNN/GRU，尤其适用于具有层次相互作用结构的体系。


<details>
  <summary>Details</summary>
Motivation: 探索非欧几里得神经网络（如双曲GRU）在量子多体系统变分蒙特卡洛方法中的应用潜力，以提升基态波函数近似的精度。

Method: 采用双曲GRU构建NQS变分函数，在一维/二维横场Ising模型（100自旋）和一维Heisenberg $J_1J_2$/$J_1J_2J_3$模型（50自旋）中与欧几里得RNN/GRU进行性能对比。

Result: 双曲GRU在所有实验中表现媲美或优于欧几里得RNN，在具有层次相互作用（如Heisenberg模型近邻耦合）的体系中优势显著。

Conclusion: 双曲GRU作为首个非欧几里得NQS变分方法是可行的，其性能优势与自然语言处理中的结论一致，未来可拓展其他非欧几里得NQS研究。

Abstract: In this work, we introduce the first type of non-Euclidean neural quantum
state (NQS) ansatz, in the form of the hyperbolic GRU (a variant of recurrent
neural networks (RNNs)), to be used in the Variational Monte Carlo method of
approximating the ground state wavefunction for quantum many-body systems. In
particular, we examine the performances of NQS ansatzes constructed from both
conventional or Euclidean RNN/GRU and from hyperbolic GRU in the prototypical
settings of the one- and two-dimensional transverse field Ising models (TFIM)
of up to 100 spins and the one-dimensional Heisenberg $J_1J_2$ and $J_1J_2J_3$
systems of up 50 spins. By virtue of the fact that, for all of the experiments
performed in this work, hyperbolic GRU can yield performances comparable to or
better than Euclidean RNNs, which have been extensively studied in these
settings in the literature, our work is a proof-of-concept for the viability of
hyperbolic GRU as the first type of non-Euclidean NQS ansatz for quantum
many-body systems. Furthermore, in settings where the Hamiltonian displays a
clear hierarchical interaction structure, such as the 1D Heisenberg $J_1J_2$ &
$J_1J_2J_3$ systems with the 1st, 2nd and even 3rd nearest neighbor
interactions, our results show that hyperbolic GRU definitively outperforms its
Euclidean version in all instances. The fact that these results are reminiscent
of the established ones from natural language processing where hyperbolic GRU
almost always outperforms Euclidean RNNs when the training data exhibit a
tree-like or hierarchical structure leads us to hypothesize that hyperbolic GRU
NQS ansatz would likely outperform Euclidean RNN/GRU NQS ansatz in quantum spin
systems that involve different degrees of nearest neighbor interactions.
Finally, with this work, we hope to initiate future studies of other types of
non-Euclidean NQS beyond hyperbolic GRU.

</details>


### [439] [Depth-Based Matrix Classification for the HHL Quantum Algorithm](https://arxiv.org/abs/2505.22454)
*Mark Danza,Sonia Lopez Alarcon,Cory Merkel*

Main category: quant-ph

TL;DR: 该论文探讨了在量子计算纠错时代即将来临之际，如何利用机器学习分类器判断线性方程组问题是否适合HHL算法求解，并强调了训练数据分布对分类准确性的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算进入纠错时代，理解后NISQ算法（如HHL）在实际问题中的适用性变得至关重要。HHL算法虽在理论上具有广泛应用前景，但在实际应用中往往难以提供有效解决方案。因此，需要一种方法预先判断问题是否适合HHL求解。

Method: 通过机器学习分类器（如多层感知机）对问题的数值特性进行分析，判断其是否适合HHL算法。研究特别关注训练数据分布的显著代表性和分类器参数的精心设计。

Result: 研究表明，在训练数据分布具有显著代表性的情况下，基于矩阵数值特性的问题分类是可行的，且多层感知机能够实现较高的分类准确性。

Conclusion: 通过精心设计的训练数据分布和分类器参数，机器学习可以有效判断线性方程组问题是否适合HHL算法求解，为量子计算在实际问题中的应用提供了重要指导。

Abstract: Under the nearing error-corrected era of quantum computing, it is necessary
to understand the suitability of certain post-NISQ algorithms for practical
problems. One of the most promising, applicable and yet difficult to implement
in practical terms is the Harrow, Hassidim and Lloyd (HHL) algorithm for linear
systems of equations. An enormous number of problems can be expressed as linear
systems of equations, from Machine Learning to fluid dynamics. However, in most
cases, HHL will not be able to provide a practical, reasonable solution to
these problems. This paper's goal inquires about whether problems can be
labeled using Machine Learning classifiers as suitable or unsuitable for HHL
implementation when some numerical information about the problem is known
beforehand. This work demonstrates that training on significantly
representative data distributions is critical to achieve good classifications
of the problems based on the numerical properties of the matrix representing
the system of equations. Accurate classification is possible through
Multi-Layer Perceptrons, although with careful design of the training data
distribution and classifier parameters.

</details>


### [440] [Assessing Quantum Advantage for Gaussian Process Regression](https://arxiv.org/abs/2505.22502)
*Dominic Lowe,M. S. Kim,Roberto Bondesan*

Main category: quant-ph

TL;DR: 该论文证明在广泛场景下，高斯过程回归的量子算法无法实现指数级加速，因为核矩阵的条件数至少随矩阵大小线性增长。


<details>
  <summary>Details</summary>
Motivation: 研究高斯过程回归量子算法的实际加速潜力，揭示其在多数情况下无法实现理论上的指数级加速。

Method: 通过严格数学证明核矩阵条件数、稀疏性和Frobenius范数的缩放规律，并结合数值实验验证。

Result: 证明在一般假设下，核矩阵条件数至少线性增长，稀疏性和Frobenius范数也呈线性缩放，导致量子算法无法实现指数加速。

Conclusion: 高斯过程回归的量子算法在多数实际场景中无法实现指数级加速，这一结论与经典数据加载复杂度无关。

Abstract: Gaussian Process Regression is a well-known machine learning technique for
which several quantum algorithms have been proposed. We show here that in a
wide range of scenarios these algorithms show no exponential speedup. We
achieve this by rigorously proving that the condition number of a kernel matrix
scales at least linearly with the matrix size under general assumptions on the
data and kernel. We additionally prove that the sparsity and Frobenius norm of
a kernel matrix scale linearly under similar assumptions. The implications for
the quantum algorithms runtime are independent of the complexity of loading
classical data on a quantum computer and also apply to dequantised algorithms.
We supplement our theoretical analysis with numerical verification for popular
kernels in machine learning.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [441] [A Kernelised Stein Discrepancy for Assessing the Fit of Inhomogeneous Random Graph Models](https://arxiv.org/abs/2505.21580)
*Anum Fatima,Gesine Reinert*

Main category: stat.ML

TL;DR: 该论文提出了一种基于核化Stein差异（KSD）的拟合优度检验方法，适用于非均匀随机图模型（IRG），且仅需单次网络观测即可进行检验。


<details>
  <summary>Details</summary>
Motivation: 复杂数据常以图的形式表示，而这些图又可视为随机图（如非均匀随机图模型）的实现。在高维情况下，快速且通用的拟合优度检验方法需求迫切。

Method: 开发了一种基于KSD的拟合优度检验方法，适用于IRG模型，且仅需单次网络观测即可实施检验。该方法不依赖于检验统计量的渐近分布。

Result: 该方法适用于任意规模的网络，并提供了理论保证。

Conclusion: 所提出的KSD型检验方法为IRG模型提供了一种高效且通用的拟合优度检验工具，适用于实际应用场景。

Abstract: Complex data are often represented as a graph, which in turn can often be
viewed as a realisation of a random graph, such as of an inhomogeneous random
graph model (IRG). For general fast goodness-of-fit tests in high dimensions,
kernelised Stein discrepancy (KSD) tests are a powerful tool. Here, we develop,
test, and analyse a KSD-type goodness-of-fit test for IRG models that can be
carried out with a single observation of the network. The test is applicable to
a network of any size and does not depend on the asymptotic distribution of the
test statistic. We also provide theoretical guarantees.

</details>


### [442] [STACI: Spatio-Temporal Aleatoric Conformal Inference](https://arxiv.org/abs/2505.21658)
*Brandon R. Feng,David Keetae Park,Xihaier Luo,Arantxa Urdangarin,Shinjae Yoo,Brian J. Reich*

Main category: stat.ML

TL;DR: STACI框架结合变分贝叶斯神经网络与非平稳时空高斯过程，提出新型时空保形推断算法，实现高效可扩展的时空不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 传统时空高斯过程（GPs）虽能解释随机不确定性，但受限于计算可扩展性和核函数假设偏差；而深度学习模型通常忽略时空相关性结构。

Method: 提出STACI框架：通过变分贝叶斯神经网络近似非平稳时空GP，并设计时空保形推断算法，利用GPU加速训练。

Result: STACI在时空过程拟合精度上超越现有方法，支持百万级数据规模，并提供统计有效的预测区间。

Conclusion: STACI成功平衡了模型可解释性与计算效率，为大规模时空数据提供了可靠的不确定性量化工具。

Abstract: Fitting Gaussian Processes (GPs) provides interpretable aleatoric uncertainty
quantification for estimation of spatio-temporal fields. Spatio-temporal deep
learning models, while scalable, typically assume a simplistic independent
covariance matrix for the response, failing to capture the underlying
correlation structure. However, spatio-temporal GPs suffer from issues of
scalability and various forms of approximation bias resulting from restrictive
assumptions of the covariance kernel function. We propose STACI, a novel
framework consisting of a variational Bayesian neural network approximation of
non-stationary spatio-temporal GP along with a novel spatio-temporal conformal
inference algorithm. STACI is highly scalable, taking advantage of GPU training
capabilities for neural network models, and provides statistically valid
prediction intervals for uncertainty quantification. STACI outperforms
competing GPs and deep methods in accurately approximating spatio-temporal
processes and we show it easily scales to datasets with millions of
observations.

</details>


### [443] [Nearly Dimension-Independent Convergence of Mean-Field Black-Box Variational Inference](https://arxiv.org/abs/2505.21721)
*Kyurae Kim,Yi-An Ma,Trevor Campbell,Jacob R. Gardner*

Main category: stat.ML

TL;DR: 该论文证明了在均值-场位置-尺度变分族下，使用重参数化梯度的黑盒变分推断（BBVI）能以几乎与维度无关的速率收敛。对于强对数凹且对数平滑的目标，BBVI使用亚高斯族达到全局最优的迭代次数为O(log d)，优于全秩位置-尺度族的O(d)依赖。对于重尾族，提供了较弱的O(d^{2/k})维度依赖。若目标对数密度的Hessian矩阵为常数，复杂度则无显式维度依赖。


<details>
  <summary>Details</summary>
Motivation: 研究黑盒变分推断（BBVI）在不同变分族下的收敛速率，特别是维度依赖性问题，旨在提高变分推断在高维问题中的效率。

Method: 采用均值-场位置-尺度变分族，结合重参数化梯度进行黑盒变分推断（BBVI），分析其在强对数凹、对数平滑目标及重尾分布下的收敛行为。

Result: 对于强对数凹且对数平滑的目标，BBVI使用亚高斯族的迭代次数为O(log d)，优于全秩位置-尺度族的O(d)。重尾族的维度依赖为O(d^{2/k})。若目标对数密度的Hessian矩阵为常数，则复杂度无显式维度依赖。

Conclusion: 该研究为BBVI在不同变分族下的收敛行为提供了理论保证，特别是在高维问题中，亚高斯族和特定条件下的变分族能显著提升效率。

Abstract: We prove that, given a mean-field location-scale variational family,
black-box variational inference (BBVI) with the reparametrization gradient
converges at an almost dimension-independent rate. Specifically, for strongly
log-concave and log-smooth targets, the number of iterations for BBVI with a
sub-Gaussian family to achieve an objective $\epsilon$-close to the global
optimum is $\mathrm{O}(\log d)$, which improves over the $\mathrm{O}(d)$
dependence of full-rank location-scale families. For heavy-tailed families, we
provide a weaker $\mathrm{O}(d^{2/k})$ dimension dependence, where $k$ is the
number of finite moments. Additionally, if the Hessian of the target
log-density is constant, the complexity is free of any explicit dimension
dependence. We also prove that our bound on the gradient variance, which is key
to our result, cannot be improved using only spectral bounds on the Hessian of
the target log-density.

</details>


### [444] [Global Minimizers of $\ell^p$-Regularized Objectives Yield the Sparsest ReLU Neural Networks](https://arxiv.org/abs/2505.21791)
*Julia Nakhleh,Robert D. Nowak*

Main category: stat.ML

TL;DR: 该论文提出了一种连续可微的训练目标，用于寻找拟合数据的最稀疏ReLU网络，将组合优化问题转化为平滑优化任务。


<details>
  <summary>Details</summary>
Motivation: 过参数化神经网络能以多种方式插值给定数据集，但哪种解更优？稀疏解在效率、泛化、可解释性和模型压缩方面具有重要意义。

Method: 提出基于最小化ℓ^p拟范数(0<p<1)的连续可微目标函数，保证全局最小值对应最稀疏的单隐层ReLU网络。

Result: 证明该方法的全局极小值精确对应最稀疏解，为通过训练恢复稀疏网络提供了理论基础。

Conclusion: 该工作将稀疏插值的组合问题转化为平滑优化任务，为梯度训练方法在稀疏网络中的应用奠定了基础。

Abstract: Overparameterized neural networks can interpolate a given dataset in many
different ways, prompting the fundamental question: which among these solutions
should we prefer, and what explicit regularization strategies will provably
yield these solutions? This paper addresses the challenge of finding the
sparsest interpolating ReLU network -- i.e., the network with the fewest
nonzero parameters or neurons -- a goal with wide-ranging implications for
efficiency, generalization, interpretability, theory, and model compression.
Unlike post hoc pruning approaches, we propose a continuous, almost-everywhere
differentiable training objective whose global minima are guaranteed to
correspond to the sparsest single-hidden-layer ReLU networks that fit the data.
This result marks a conceptual advance: it recasts the combinatorial problem of
sparse interpolation as a smooth optimization task, potentially enabling the
use of gradient-based training methods. Our objective is based on minimizing
$\ell^p$ quasinorms of the weights for $0 < p < 1$, a classical
sparsity-promoting strategy in finite-dimensional settings. However, applying
these ideas to neural networks presents new challenges: the function class is
infinite-dimensional, and the weights are learned using a highly nonconvex
objective. We prove that, under our formulation, global minimizers correspond
exactly to sparsest solutions. Our work lays a foundation for understanding
when and how continuous sparsity-inducing objectives can be leveraged to
recover sparse networks through training.

</details>


### [445] [A General-Purpose Theorem for High-Probability Bounds of Stochastic Approximation with Polyak Averaging](https://arxiv.org/abs/2505.21796)
*Sajad Khodadadian,Martin Zubeldia*

Main category: stat.ML

TL;DR: 本文提出了一个通用框架，用于建立平均随机逼近算法迭代误差的非渐近浓度边界，填补了高概率性能保证的研究空白。


<details>
  <summary>Details</summary>
Motivation: Polyak-Ruppert平均技术虽广泛用于实现随机逼近算法的最优渐近方差，但其高概率性能保证在一般设置中研究不足。

Method: 通过假设能够获取未平均迭代的个体浓度边界，构建了一个通用框架来推导平均迭代的尖锐边界，并通过实例验证了结果的紧性。

Result: 为收缩性随机逼近算法及时间差分学习、Q学习等算法推导了紧浓度边界，在传统分析困难的场景中获得了新边界。

Conclusion: 该框架为平均随机逼近算法的高概率性能分析提供了通用工具，并通过实例和实际应用验证了其有效性和紧性。

Abstract: Polyak-Ruppert averaging is a widely used technique to achieve the optimal
asymptotic variance of stochastic approximation (SA) algorithms, yet its
high-probability performance guarantees remain underexplored in general
settings. In this paper, we present a general framework for establishing
non-asymptotic concentration bounds for the error of averaged SA iterates. Our
approach assumes access to individual concentration bounds for the unaveraged
iterates and yields a sharp bound on the averaged iterates. We also construct
an example, showing the tightness of our result up to constant multiplicative
factors. As direct applications, we derive tight concentration bounds for
contractive SA algorithms and for algorithms such as temporal difference
learning and Q-learning with averaging, obtaining new bounds in settings where
traditional analysis is challenging.

</details>


### [446] [Spectral clustering for dependent community Hawkes process models of temporal networks](https://arxiv.org/abs/2505.21845)
*Lingfei Zhao,Hadeel Soliman,Kevin S. Xu,Subhadeep Paul*

Main category: stat.ML

TL;DR: 该论文提出了一种结合随机块模型和霍克斯过程的DCH模型，用于分析具有社区结构和节点对依赖性的时间网络，并提供了谱聚类误差的非渐近上界。


<details>
  <summary>Details</summary>
Motivation: 时间网络（如社交媒体、金融交易等）通常表现出社区结构和节点对之间的强依赖性。现有方法难以同时建模这两种特性，因此需要一种新的模型和分析方法。

Method: 提出DCH模型，结合随机块模型（社区结构）和霍克斯过程（节点对依赖性），并使用广义矩估计（GMM）进行高效参数估计。

Result: 推导了谱聚类在事件计数矩阵上的误聚类误差的非渐近上界，并证明了GMM估计量在大规模网络和时间增长下的收敛性。

Conclusion: DCH模型能有效建模时间网络的社区结构和依赖性，理论分析和实验验证了其优越性。

Abstract: Temporal networks observed continuously over time through timestamped
relational events data are commonly encountered in application settings
including online social media communications, financial transactions, and
international relations. Temporal networks often exhibit community structure
and strong dependence patterns among node pairs. This dependence can be modeled
through mutual excitations, where an interaction event from a sender to a
receiver node increases the possibility of future events among other node
pairs.
  We provide statistical results for a class of models that we call dependent
community Hawkes (DCH) models, which combine the stochastic block model with
mutually exciting Hawkes processes for modeling both community structure and
dependence among node pairs, respectively. We derive a non-asymptotic upper
bound on the misclustering error of spectral clustering on the event count
matrix as a function of the number of nodes and communities, time duration, and
the amount of dependence in the model. Our result leverages recent results on
bounding an appropriate distance between a multivariate Hawkes process count
vector and a Gaussian vector, along with results from random matrix theory. We
also propose a DCH model that incorporates only self and reciprocal excitation
along with highly scalable parameter estimation using a Generalized Method of
Moments (GMM) estimator that we demonstrate to be consistent for growing
network size and time duration.

</details>


### [447] [Almost Linear Convergence under Minimal Score Assumptions: Quantized Transition Diffusion](https://arxiv.org/abs/2505.21892)
*Xunpeng Huang,Yingyu Lin,Nikki Lijing Kuang,Hanze Dong,Difan Zou,Yian Ma,Tong Zhang*

Main category: stat.ML

TL;DR: 提出QTD方法，通过量化数据和离散扩散动力学提升连续扩散模型的效率和理论统一性。


<details>
  <summary>Details</summary>
Motivation: 连续扩散模型在数据生成中表现出色，但受限于局部邻接结构和时间非均匀反向去噪过程的偏差，效率低下。

Method: 结合数据量化和离散扩散动力学，设计基于汉明距离的连续时间马尔可夫链前向过程，并引入截断均匀化技术进行反向采样。

Result: QTD在预期O(dln²(d/ε))次评分评估内逼近目标分布，实现了最先进的推理效率和理论统一。

Conclusion: QTD不仅提升了扩散模型的效率，还通过统一离散和连续扩散范式推动了生成建模的理论发展。

Abstract: Continuous diffusion models have demonstrated remarkable performance in data
generation across various domains, yet their efficiency remains constrained by
two critical limitations: (1) the local adjacency structure of the forward
Markov process, which restricts long-range transitions in the data space, and
(2) inherent biases introduced during the simulation of time-inhomogeneous
reverse denoising processes. To address these challenges, we propose Quantized
Transition Diffusion (QTD), a novel approach that integrates data quantization
with discrete diffusion dynamics. Our method first transforms the continuous
data distribution $p_*$ into a discrete one $q_*$ via histogram approximation
and binary encoding, enabling efficient representation in a structured discrete
latent space. We then design a continuous-time Markov chain (CTMC) with Hamming
distance-based transitions as the forward process, which inherently supports
long-range movements in the original data space. For reverse-time sampling, we
introduce a \textit{truncated uniformization} technique to simulate the reverse
CTMC, which can provably provide unbiased generation from $q_*$ under minimal
score assumptions. Through a novel KL dynamic analysis of the reverse CTMC, we
prove that QTD can generate samples with $O(d\ln^2(d/\epsilon))$ score
evaluations in expectation to approximate the $d$--dimensional target
distribution $p_*$ within an $\epsilon$ error tolerance. Our method not only
establishes state-of-the-art inference efficiency but also advances the
theoretical foundations of diffusion-based generative modeling by unifying
discrete and continuous diffusion paradigms.

</details>


### [448] [Higher-Order Group Synchronization](https://arxiv.org/abs/2505.21932)
*Adriana L. Duncan,Joe Kileel*

Main category: stat.ML

TL;DR: 本文提出了一种新颖的高阶群同步问题，该方法在超图上操作，通过同步超边上的高阶局部测量来获取节点的全局估计。该方法在计算机视觉和图像处理等领域具有应用潜力，并通过数值实验展示了其在某些情况下优于标准成对同步方法的性能。


<details>
  <summary>Details</summary>
Motivation: 高阶群同步的动机来源于计算机视觉和图像处理等计算问题中的应用需求，旨在通过高阶局部测量来更有效地获取全局估计。

Method: 本文首先定义了高阶群同步问题并讨论了其数学基础，提出了一个全局作用于高阶测量的计算框架，使用消息传递算法进行同步。

Result: 数值实验表明，该方法在旋转和角度同步等特定情况下优于标准成对同步方法，并且对异常值更具鲁棒性。在模拟冷冻电镜数据上的性能与标准冷冻电镜重建包相当。

Conclusion: 高阶群同步方法在特定应用中展现出优于传统方法的性能，并且具有更高的鲁棒性，为相关领域的计算问题提供了新的解决方案。

Abstract: Group synchronization is the problem of determining reliable global estimates
from noisy local measurements on networks. The typical task for group
synchronization is to assign elements of a group to the nodes of a graph in a
way that respects group elements given on the edges which encode information
about local pairwise relationships between the nodes. In this paper, we
introduce a novel higher-order group synchronization problem which operates on
a hypergraph and seeks to synchronize higher-order local measurements on the
hyperedges to obtain global estimates on the nodes. Higher-order group
synchronization is motivated by applications to computer vision and image
processing, among other computational problems. First, we define the problem of
higher-order group synchronization and discuss its mathematical foundations.
Specifically, we give necessary and sufficient synchronizability conditions
which establish the importance of cycle consistency in higher-order group
synchronization. Then, we propose the first computational framework for general
higher-order group synchronization; it acts globally and directly on
higher-order measurements using a message passing algorithm. We discuss
theoretical guarantees for our framework, including convergence analyses under
outliers and noise. Finally, we show potential advantages of our method through
numerical experiments. In particular, we show that in certain cases our
higher-order method applied to rotational and angular synchronization
outperforms standard pairwise synchronization methods and is more robust to
outliers. We also show that our method has comparable performance on simulated
cryo-electron microscopy (cryo-EM) data compared to a standard cryo-EM
reconstruction package.

</details>


### [449] [Learning Curves of Stochastic Gradient Descent in Kernel Regression](https://arxiv.org/abs/2505.22048)
*Haihan Zhang,Weicheng Lin,Yuanshi Liu,Cong Fang*

Main category: stat.ML

TL;DR: 该论文研究了在线一阶算法（如SGD）在核回归中的性能，发现其在多数情况下能达到最优速率，克服了离线方法（如岭回归）的饱和现象。


<details>
  <summary>Details</summary>
Motivation: 研究在线一阶算法（如SGD）在核回归中的性能，并与离线方法（如岭回归）进行比较，特别是在模型误设的情况下。

Method: 分析了球面上的内积核，使用单次随机梯度下降（SGD）在不同样本规模下研究超额风险曲线的精确阶数。

Result: SGD在多数情况下能达到最优速率，克服了离线方法的饱和现象，除非模型高度误设且学习进入后期阶段。

Conclusion: SGD在核回归中表现优异，尤其在采用指数衰减步长时，克服了饱和现象，展示了其相对于迭代平均方法的优势。

Abstract: This paper considers a canonical problem in kernel regression: how good are
the model performances when it is trained by the popular online first-order
algorithms, compared to the offline ones, such as ridge and ridgeless
regression? In this paper, we analyze the foundational single-pass Stochastic
Gradient Descent (SGD) in kernel regression under source condition where the
optimal predictor can even not belong to the RKHS, i.e. the model is
misspecified. Specifically, we focus on the inner product kernel over the
sphere and characterize the exact orders of the excess risk curves under
different scales of sample sizes $n$ concerning the input dimension $d$.
Surprisingly, we show that SGD achieves min-max optimal rates up to constants
among all the scales, without suffering the saturation, a prevalent phenomenon
observed in (ridge) regression, except when the model is highly misspecified
and the learning is in a final stage where $n\gg d^{\gamma}$ with any constant
$\gamma >0$. The main reason for SGD to overcome the curse of saturation is the
exponentially decaying step size schedule, a common practice in deep neural
network training. As a byproduct, we provide the \emph{first} provable
advantage of the scheme over the iterative averaging method in the common
setting.

</details>


### [450] [Individualised Counterfactual Examples Using Conformal Prediction Intervals](https://arxiv.org/abs/2505.22326)
*James M. Adams,Gesine Reinert,Lukasz Szpruch,Carsten Maple,Andrew Elliott*

Main category: stat.ML

TL;DR: 该论文提出了一种基于个性化置信预测区间的反事实解释方法（CPICFs），旨在为黑盒模型提供更有针对性的决策解释。


<details>
  <summary>Details</summary>
Motivation: 高维特征空间中存在大量可能的反事实解释，需通过额外标准筛选最有用的解释。论文探索如何根据个体对分类器的认知，量化信息增益并选择最具信息量的反事实。

Method: 通过显式建模个体知识，利用置信预测区间宽度评估预测不确定性，并在特征空间中选择能最大化信息增益的反事实示例。

Result: 在合成数据集和真实数据集上的实验表明，CPICFs能有效提升个体对模型决策的理解，并通过数据增强验证了其效用。

Conclusion: 个性化置信预测区间反事实解释能针对个体认知差异提供更有效的模型决策解释，增强模型透明性。

Abstract: Counterfactual explanations for black-box models aim to pr ovide insight into
an algorithmic decision to its recipient. For a binary classification problem
an individual counterfactual details which features might be changed for the
model to infer the opposite class. High-dimensional feature spaces that are
typical of machine learning classification models admit many possible
counterfactual examples to a decision, and so it is important to identify
additional criteria to select the most useful counterfactuals. In this paper,
we explore the idea that the counterfactuals should be maximally informative
when considering the knowledge of a specific individual about the underlying
classifier. To quantify this information gain we explicitly model the knowledge
of the individual, and assess the uncertainty of predictions which the
individual makes by the width of a conformal prediction interval. Regions of
feature space where the prediction interval is wide correspond to areas where
the confidence in decision making is low, and an additional counterfactual
example might be more informative to an individual. To explore and evaluate our
individualised conformal prediction interval counterfactuals (CPICFs), first we
present a synthetic data set on a hypercube which allows us to fully visualise
the decision boundary, conformal intervals via three different methods, and
resultant CPICFs. Second, in this synthetic data set we explore the impact of a
single CPICF on the knowledge of an individual locally around the original
query. Finally, in both our synthetic data set and a complex real world dataset
with a combination of continuous and discrete variables, we measure the utility
of these counterfactuals via data augmentation, testing the performance on a
held out set.

</details>


### [451] [Credal Prediction based on Relative Likelihood](https://arxiv.org/abs/2505.22332)
*Timo Löhr,Paul Hofman,Felix Mohr,Eyke Hüllermeier*

Main category: stat.ML

TL;DR: 本文提出了一种基于相对似然的概率分布集合预测方法，通过调整阈值平衡正确性与精确性，并利用改进的集成学习技术进行近似。实验证明该方法在不确定性表示和预测性能上优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 为了更有效地表示学习者的认知不确定性，本文提出使用概率分布集合（credal sets）作为预测形式，并通过相对似然统计方法控制预测的准确性与精确性之间的权衡。

Method: 基于相对似然的统计方法，设定阈值筛选出所有可能的模型生成的概率分布集合，并通过改进的集成学习技术近似这些credal sets。

Result: 在基准数据集上的实验表明，该方法在不影响预测性能的情况下，显著提升了不确定性表示的效果，并优于多种现有基线方法。

Conclusion: 本文提出的基于相对似然的credal预测方法在理论和实验上均表现出色，为不确定性表示提供了一种有效且实用的解决方案。

Abstract: Predictions in the form of sets of probability distributions, so-called
credal sets, provide a suitable means to represent a learner's epistemic
uncertainty. In this paper, we propose a theoretically grounded approach to
credal prediction based on the statistical notion of relative likelihood: The
target of prediction is the set of all (conditional) probability distributions
produced by the collection of plausible models, namely those models whose
relative likelihood exceeds a specified threshold. This threshold has an
intuitive interpretation and allows for controlling the trade-off between
correctness and precision of credal predictions. We tackle the problem of
approximating credal sets defined in this way by means of suitably modified
ensemble learning techniques. To validate our approach, we illustrate its
effectiveness by experiments on benchmark datasets demonstrating superior
uncertainty representation without compromising predictive performance. We also
compare our method against several state-of-the-art baselines in credal
prediction.

</details>


### [452] [Computing Optimal Transport Maps and Wasserstein Barycenters Using Conditional Normalizing Flows](https://arxiv.org/abs/2505.22364)
*Gabriele Visentin,Patrick Cheridito*

Main category: stat.ML

TL;DR: 提出了一种基于条件归一化流的高效计算方法，用于高维空间中的最优传输映射和Wasserstein重心计算，避免了传统对偶公式和复杂对抗优化。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理高维空间的最优传输和Wasserstein重心时，依赖对偶公式和复杂对抗优化，计算效率低且难以扩展到大量输入分布。

Method: 使用条件归一化流将输入分布近似为来自共同潜在空间的可逆推前变换，通过梯度最小化直接求解原始问题，并扩展至条件方差最小化以计算Wasserstein重心。

Result: 实验表明，该方法在高维任务中结果准确，优于现有技术，并能计算数百个输入分布的重心，解决了传统方法的计算瓶颈。

Conclusion: 该方法为高维最优传输和Wasserstein重心计算提供了高效、准确的解决方案，具有显著的计算优势和应用潜力。

Abstract: We present a novel method for efficiently computing optimal transport maps
and Wasserstein barycenters in high-dimensional spaces. Our approach uses
conditional normalizing flows to approximate the input distributions as
invertible pushforward transformations from a common latent space. This makes
it possible to directly solve the primal problem using gradient-based
minimization of the transport cost, unlike previous methods that rely on dual
formulations and complex adversarial optimization. We show how this approach
can be extended to compute Wasserstein barycenters by solving a conditional
variance minimization problem. A key advantage of our conditional architecture
is that it enables the computation of barycenters for hundreds of input
distributions, which was computationally infeasible with previous methods. Our
numerical experiments illustrate that our approach yields accurate results
across various high-dimensional tasks and compares favorably with previous
state-of-the-art methods.

</details>


### [453] [Hypothesis Testing in Imaging Inverse Problems](https://arxiv.org/abs/2505.22481)
*Yiming Xi,Konstantinos Zygalakis,Marcelo Pereyra*

Main category: stat.ML

TL;DR: 该论文提出了一种针对成像逆问题的语义假设检验框架，解决了现有成像方法在假设检验上的不足。


<details>
  <summary>Details</summary>
Motivation: 现代成像方法难以支持假设检验，而假设检验是科学方法的核心，对实验的严谨解释和决策过程至关重要。

Method: 结合自监督计算成像、视觉语言模型和非参数假设检验（使用e值）的概念。

Result: 在基于图像的表型分析实验中，该方法表现出色，既能有效控制I类错误，又具有较高的统计功效。

Conclusion: 该框架为成像逆问题中的语义假设检验提供了有效解决方案，填补了现有方法的不足。

Abstract: This paper proposes a framework for semantic hypothesis testing tailored to
imaging inverse problems. Modern imaging methods struggle to support hypothesis
testing, a core component of the scientific method that is essential for the
rigorous interpretation of experiments and robust interfacing with
decision-making processes. There are three main reasons why image-based
hypothesis testing is challenging. First, the difficulty of using a single
observation to simultaneously reconstruct an image, formulate hypotheses, and
quantify their statistical significance. Second, the hypotheses encountered in
imaging are mostly of semantic nature, rather than quantitative statements
about pixel values. Third, it is challenging to control test error
probabilities because the null and alternative distributions are often unknown.
Our proposed approach addresses these difficulties by leveraging concepts from
self-supervised computational imaging, vision-language models, and
non-parametric hypothesis testing with e-values. We demonstrate our proposed
framework through numerical experiments related to image-based phenotyping,
where we achieve excellent power while robustly controlling Type I errors.

</details>


### [454] [IGNIS: A Neural Network Framework for Robust Parameter Estimation in Archimedean Copulas](https://arxiv.org/abs/2505.22518)
*Agnideep Aich,Ashit Baran Aich,Bruce Wade*

Main category: stat.ML

TL;DR: 本文提出了一种名为IGNIS Network的新型神经网络框架，用于解决Archimedean copulas参数估计的挑战，特别是在A1和A2家族中。该方法通过直接从可观测的依赖度量映射到copula参数，克服了传统方法的局限性，并在模拟和实际数据中表现出色。


<details>
  <summary>Details</summary>
Motivation: Archimedean copulas的参数估计是一个具有挑战性的问题，尤其是对于新开发的A1和A2家族，这些家族展现出复杂的依赖结构。传统方法如矩估计法（MoM）、最大似然估计（MLE）和最大伪似然估计（MPL）由于与依赖度量（如Kendall's tau）的非单调关系问题以及数值不稳定性，往往难以有效工作。

Method: 本文提出了IGNIS Network，一种新型的、统一的神经网络框架，通过学习从可观测的依赖度量到copula参数的直接映射，克服了传统方法的局限性。该方法在包括Clayton、Gumbel、Frank、A1和A2在内的五种Archimedean copula家族的模拟数据上进行训练，确保了其广泛的适用性。

Result: 广泛的模拟研究表明，与矩估计法（MoM）相比，IGNIS Network减少了估计误差，同时通过理论指导的后处理固有地加强了参数约束。在金融回报（AAPL-MSFT）、医疗指标（CDC糖尿病指标）和环境测量（PM2.5空气质量）等多样化的实际数据集上进一步验证了该方法的实用性。

Conclusion: 我们的结果强调了神经网络方法在现代应用中进行稳健和准确的依赖建模的变革潜力。IGNIS Network不仅在理论上具有创新性，而且在实际应用中表现出色，为Archimedean copulas的参数估计提供了一种有效的解决方案。

Abstract: Parameter estimation for Archimedean copulas remains a challenging problem,
particularly for the recently developed A1 and A2 families that exhibit complex
dependency structures. Traditional methods, such as the Method of Moments
(MoM), Maximum Likelihood Estimation (MLE), and Maximum Pseudo-Likelihood
(MPL), often struggle due to issues of non-monotonic relationship with
dependency measures such as Kendall's tau (as in the case of A1) and numerical
instability. In this paper, we present the IGNIS Network, a novel, unified
neural framework that learns a direct mapping from observable dependency
measures to copula parameters, thereby overcoming the limitations of classical
approaches. Our approach is trained on simulated data spanning five Archimedean
copula families including Clayton, Gumbel, Frank, A1, and A2, ensuring its
general applicability across the entire family. Extensive simulation studies
demonstrate that the IGNIS Network reduces estimation errors compared to MoM,
while inherently enforcing parameter constraints through theory-guided
post-processing. We further validate the practical utility of our method on
diverse real-world datasets, including financial returns (AAPL-MSFT),
healthcare metrics (CDC Diabetes indicators), and environmental measurements
(PM2.5 air quality). Our results underscore the transformative potential of
neural methods for robust and accurate dependence modeling in modern
applications.

</details>


### [455] [Symplectic Generative Networks (SGNs): A Hamiltonian Framework for Invertible Deep Generative Modeling](https://arxiv.org/abs/2505.22527)
*Agnideep Aich,Ashit Aich,Bruce Wade*

Main category: stat.ML

TL;DR: 本文提出了一种基于哈密顿力学的可逆、保体积深度生成模型SGN，通过理论分析证明了其优越性。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统生成模型在计算雅可比行列式时的高计算开销问题，作者提出利用哈密顿力学构建可逆且保体积的生成模型。

Method: 通过赋予隐空间辛结构，将数据生成建模为哈密顿系统的时间演化，从而避免雅可比行列式计算。

Result: 论文提供了完整的理论框架，包括可逆性证明、复杂度分析、逼近误差量化、信息几何分析及稳定性保证。

Conclusion: SGN为高维复杂数据的生成建模奠定了理论基础，具有显著的性能优势和应用潜力。

Abstract: We introduce the Symplectic Generative Network (SGN), a deep generative model
that leverages Hamiltonian mechanics to construct an invertible,
volume-preserving mapping between a latent space and the data space. By
endowing the latent space with a symplectic structure and modeling data
generation as the time evolution of a Hamiltonian system, SGN achieves exact
likelihood evaluation without incurring the computational overhead of Jacobian
determinant calculations. In this work, we provide a rigorous mathematical
foundation for SGNs through a comprehensive theoretical framework that
includes: (i) complete proofs of invertibility and volume preservation, (ii) a
formal complexity analysis with theoretical comparisons to Variational
Autoencoders and Normalizing Flows, (iii) strengthened universal approximation
results with quantitative error bounds, (iv) an information-theoretic analysis
based on the geometry of statistical manifolds, and (v) an extensive stability
analysis with adaptive integration guarantees. These contributions highlight
the fundamental advantages of SGNs and establish a solid foundation for future
empirical investigations and applications to complex, high-dimensional data.

</details>


### [456] [Can Copulas Be Used for Feature Selection? A Machine Learning Study on Diabetes Risk Prediction](https://arxiv.org/abs/2505.22554)
*Agnideep Aich,Md Monzur Murshed,Amanda Mayeaux,Sameera Hewage*

Main category: stat.ML

TL;DR: 该研究提出了一种基于A2 copula上尾依赖系数的特征选择框架，用于糖尿病风险预测，优于传统方法，并识别出五个关键预测因子。


<details>
  <summary>Details</summary>
Motivation: 传统糖尿病风险预测方法（如互信息和遗传算法）常忽略极端依赖关系，而这些对高风险亚群至关重要。本研究旨在通过上尾依赖系数解决这一问题。

Method: 使用A2 copula的上尾依赖系数（λU）进行特征选择，量化预测变量极端高值与糖尿病诊断的共现频率，应用于CDC糖尿病健康指标数据集。

Result: 该方法筛选出五个关键预测因子（自评健康、高血压、BMI、行动受限、高胆固醇），在四种分类器上表现优异，准确率高达86.5%，AUC达0.806，媲美全特征模型。

Conclusion: 这是首个将copula上尾依赖应用于监督特征选择的研究，结合极值理论与机器学习，为糖尿病预防提供了实用工具包。

Abstract: Accurate diabetes risk prediction relies on identifying key features from
complex health datasets, but conventional methods like mutual information (MI)
filters and genetic algorithms (GAs) often overlook extreme dependencies
critical for high-risk subpopulations. In this study we introduce a
feature-selection framework using the upper-tail dependence coefficient
({\lambda}U) of the novel A2 copula, which quantifies how often extreme higher
values of a predictor co-occur with diabetes diagnoses (target variable).
Applied to the CDC Diabetes Health Indicators dataset (n=253,680), our method
prioritizes five predictors (self-reported general health, high blood pressure,
body mass index, mobility limitations, and high cholesterol levels) based on
upper tail dependencies. These features match or outperform MI and GA selected
subsets across four classifiers (Random Forest, XGBoost, Logistic Regression,
Gradient Boosting), achieving accuracy up to 86.5% (XGBoost) and AUC up to
0.806 (Gradient Boosting), rivaling the full 21-feature model. Permutation
importance confirms clinical relevance, with BMI and general health driving
accuracy. To our knowledge, this is the first work to apply a copula's
upper-tail dependence for supervised feature selection, bridging extreme-value
theory and machine learning to deliver a practical toolkit for diabetes
prevention.

</details>


### [457] [Principled Out-of-Distribution Generalization via Simplicity](https://arxiv.org/abs/2505.22622)
*Jiawei Ge,Amanda Wang,Shange Tang,Chi Jin*

Main category: stat.ML

TL;DR: 该论文研究了扩散模型在图像生成中的组合泛化能力，提出通过简单性度量实现分布外泛化的理论框架，并在两种关键场景下建立了样本复杂度保证。


<details>
  <summary>Details</summary>
Motivation: 现代基础模型展现出卓越的分布外泛化能力，但其理论原理尚不明确。论文旨在通过分析扩散模型的组合泛化能力，揭示泛化现象背后的简单性原理。

Method: 提出基于预定义简单性度量的理论框架，分析恒定差距和消失差距两种场景，研究正则化最大似然估计器的性能。

Result: 首次为学习真实、可泛化的简单模型建立了尖锐的样本复杂度保证，证明在两种场景下都能有效识别符合人类预期的模型。

Conclusion: 研究表明，与训练数据一致的最简单模型往往最能符合人类预期，这为理解基础模型的泛化能力提供了理论依据。

Abstract: Modern foundation models exhibit remarkable out-of-distribution (OOD)
generalization, solving tasks far beyond the support of their training data.
However, the theoretical principles underpinning this phenomenon remain
elusive. This paper investigates this problem by examining the compositional
generalization abilities of diffusion models in image generation. Our analysis
reveals that while neural network architectures are expressive enough to
represent a wide range of models -- including many with undesirable behavior on
OOD inputs -- the true, generalizable model that aligns with human expectations
typically corresponds to the simplest among those consistent with the training
data.
  Motivated by this observation, we develop a theoretical framework for OOD
generalization via simplicity, quantified using a predefined simplicity metric.
We analyze two key regimes: (1) the constant-gap setting, where the true model
is strictly simpler than all spurious alternatives by a fixed gap, and (2) the
vanishing-gap setting, where the fixed gap is replaced by a smoothness
condition ensuring that models close in simplicity to the true model yield
similar predictions. For both regimes, we study the regularized maximum
likelihood estimator and establish the first sharp sample complexity guarantees
for learning the true, generalizable, simple model.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [458] [Synonymous Variational Inference for Perceptual Image Compression](https://arxiv.org/abs/2505.22438)
*Zijian Liang,Kai Niu,Changshuo Wang,Jin Xu,Ping Zhang*

Main category: cs.IT

TL;DR: 该论文提出了一种基于同义关系的变分推理方法（SVI），用于分析感知图像压缩问题，并引入了同义图像压缩（SIC）方案，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 近年来语义信息理论的贡献揭示了语义和句法信息之间的集合-元素关系，表现为同义关系。本文基于这一观点，重新分析感知图像压缩问题。

Method: 提出同义变分推理（SVI）方法，以感知相似性为同义标准构建理想同义集（Synset），并通过最小化部分语义KL散度来近似其后验潜在同义表示。

Result: 理论证明感知图像压缩的优化方向遵循三重权衡，可覆盖现有的率-失真-感知方案。实验结果表明，单一渐进式SIC编解码器实现了可比的率-失真-感知性能。

Conclusion: 提出的SVI分析方法有效，同义图像压缩（SIC）方案在实际应用中表现出色，验证了理论分析的正确性。

Abstract: Recent contributions of semantic information theory reveal the set-element
relationship between semantic and syntactic information, represented as
synonymous relationships. In this paper, we propose a synonymous variational
inference (SVI) method based on this synonymity viewpoint to re-analyze the
perceptual image compression problem. It takes perceptual similarity as a
typical synonymous criterion to build an ideal synonymous set (Synset), and
approximate the posterior of its latent synonymous representation with a
parametric density by minimizing a partial semantic KL divergence. This
analysis theoretically proves that the optimization direction of perception
image compression follows a triple tradeoff that can cover the existing
rate-distortion-perception schemes. Additionally, we introduce synonymous image
compression (SIC), a new image compression scheme that corresponds to the
analytical process of SVI, and implement a progressive SIC codec to fully
leverage the model's capabilities. Experimental results demonstrate comparable
rate-distortion-perception performance using a single progressive SIC codec,
thus verifying the effectiveness of our proposed analysis method.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [459] [Nonadaptive Output Regulation of Second-Order Nonlinear Uncertain Systems](https://arxiv.org/abs/2505.21838)
*Maobin Lu,Martin Guay,Telema Harry,Shimin Wang,Jordan Cooper*

Main category: eess.SY

TL;DR: 该论文提出了一种非自适应鲁棒控制方法，用于解决二阶非线性不确定系统的输出调节问题，避免了传统自适应控制中的突发现象，并通过Duffing系统验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 针对二阶非线性不确定系统在未知外部系统下的输出调节问题，传统自适应控制方法存在突发现象，因此需要一种鲁棒控制方法来解决这一问题。

Method: 通过构建通用的内部模型，引入坐标变换将鲁棒输出调节问题转化为非自适应镇定问题，并设计镇定控制律和严格Lyapunov函数以保证鲁棒性。

Result: 提出的非自适应控制律能够使增广系统的输出零流形具有吸引力，从而解决了鲁棒输出调节问题，并通过Duffing系统验证了方法的有效性。

Conclusion: 论文提出的非自适应内部模型方法有效解决了二阶非线性不确定系统的鲁棒输出调节问题，避免了突发现象，具有实际应用价值。

Abstract: This paper investigates the robust output regulation problem of second-order
nonlinear uncertain systems with an unknown exosystem. Instead of the adaptive
control approach, this paper resorts to a robust control methodology to solve
the problem and thus avoid the bursting phenomenon. In particular, this paper
constructs generic internal models for the steady-state state and input
variables of the system. By introducing a coordinate transformation, this paper
converts the robust output regulation problem into a nonadaptive stabilization
problem of an augmented system composed of the second-order nonlinear uncertain
system and the generic internal models. Then, we design the stabilization
control law and construct a strict Lyapunov function that guarantees the
robustness with respect to unmodeled disturbances. The analysis shows that the
output zeroing manifold of the augmented system can be made attractive by the
proposed nonadaptive control law, which solves the robust output regulation
problem. Finally, we demonstrate the effectiveness of the proposed nonadaptive
internal model approach by its application to the control of the Duffing
system.

</details>


### [460] [A Physics-Informed Learning Framework to Solve the Infinite-Horizon Optimal Control Problem](https://arxiv.org/abs/2505.21842)
*Filippos Fotiadis,Kyriakos G. Vamvoudakis*

Main category: eess.SY

TL;DR: 该论文提出了一种基于物理信息神经网络（PINNs）的框架，用于解决非线性系统的无限时域最优控制问题，通过处理有限时域变体来避免多解问题，并提供了验证和扩展时域的方法。


<details>
  <summary>Details</summary>
Motivation: 传统的PINNs方法在解决无限时域最优控制问题时，由于稳态HJB方程存在多解性，可能无法逼近最优值函数。因此，需要一种新的方法来确保逼近的是最优解。

Method: 论文采用PINNs框架，通过解决有限时域变体的稳态HJB方程来学习最优值函数，该变体具有唯一解，并能随着时域增加均匀逼近最优值函数。同时提供了验证时域是否足够大及扩展时域的方法。

Result: 仿真结果验证了所提方法的有效性，表明该方法能够在不依赖多项式基函数、无需先验稳定控制器知识且不进行迭代策略评估的情况下，有效逼近最优值函数。

Conclusion: 该论文提出的PINNs框架为解决非线性系统的无限时域最优控制问题提供了一种有效且稳健的方法，特别是在处理多解性和逼近最优解方面表现出色。

Abstract: We propose a physics-informed neural networks (PINNs) framework to solve the
infinite-horizon optimal control problem of nonlinear systems. In particular,
since PINNs are generally able to solve a class of partial differential
equations (PDEs), they can be employed to learn the value function of the
infinite-horizon optimal control problem via solving the associated
steady-state Hamilton-Jacobi-Bellman (HJB) equation. However, an issue here is
that the steady-state HJB equation generally yields multiple solutions; hence
if PINNs are directly employed to it, they may end up approximating a solution
that is different from the optimal value function of the problem. We tackle
this by instead applying PINNs to a finite-horizon variant of the steady-state
HJB that has a unique solution, and which uniformly approximates the optimal
value function as the horizon increases. An algorithm to verify if the chosen
horizon is large enough is also given, as well as a method to extend it -- with
reduced computations and robustness to approximation errors -- in case it is
not. Unlike many existing methods, the proposed technique works well with
non-polynomial basis functions, does not require prior knowledge of a
stabilizing controller, and does not perform iterative policy evaluations.
Simulations are performed, which verify and clarify theoretical findings.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [461] [What Data Enables Optimal Decisions? An Exact Characterization for Linear Optimization](https://arxiv.org/abs/2505.21692)
*Omar Bennouna,Amine Bennouna,Saurabh Amin,Asuman Ozdaglar*

Main category: math.OC

TL;DR: 该论文研究数据集对决策任务的贡献，提出几何特征描述成本向量方向对最优决策的影响，并开发算法构建最小充分数据集。


<details>
  <summary>Details</summary>
Motivation: 探讨数据集在多大程度上能帮助解决特定决策任务，尤其是在参数未知且存在不确定性的情况下。

Method: 聚焦线性规划，通过几何特征分析成本向量的关键方向，并开发算法构建最小或成本最低的充分数据集。

Result: 研究表明，精心选择的小数据集通常能完全确定最优决策，为任务感知的数据选择提供理论基础。

Conclusion: 论文为任务感知的数据选择提供了理论支持，证明小数据集在特定条件下足以确定最优决策。

Abstract: We study the fundamental question of how informative a dataset is for solving
a given decision-making task. In our setting, the dataset provides partial
information about unknown parameters that influence task outcomes. Focusing on
linear programs, we characterize when a dataset is sufficient to recover an
optimal decision, given an uncertainty set on the cost vector. Our main
contribution is a sharp geometric characterization that identifies the
directions of the cost vector that matter for optimality, relative to the task
constraints and uncertainty set. We further develop a practical algorithm that,
for a given task, constructs a minimal or least-costly sufficient dataset. Our
results reveal that small, well-chosen datasets can often fully determine
optimal decisions -- offering a principled foundation for task-aware data
selection.

</details>


### [462] [PolarGrad: A Class of Matrix-Gradient Optimizers from a Unifying Preconditioning Perspective](https://arxiv.org/abs/2505.21799)
*Tim Tsz-Kit Lau,Qi Long,Weijie Su*

Main category: math.OC

TL;DR: 本文提出了一种统一框架来分析矩阵感知预条件优化方法，引入了基于梯度极分解的新优化方法PolarGrad，在多项任务中表现优于Adam和Muon。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型和数据集的规模不断扩大，高效的优化方法变得至关重要。尽管Adam和AdamW是训练神经网络和大语言模型的事实标准优化器，但利用梯度矩阵结构的结构感知预条件优化器（如Shampoo和Muon）已显示出更快的收敛速度。本文旨在通过统一框架分析这类方法，并开发新的优化器。

Method: 本文引入了一个统一框架来分析矩阵感知预条件方法，区分了将神经网络权重视为向量（解决曲率各向异性）和考虑其矩阵结构（解决梯度各向异性）的预条件策略。基于此框架，提出了PolarGrad，一种基于矩阵梯度极分解的预条件优化方法。

Result: PolarGrad在多种矩阵优化问题和语言模型预训练任务中表现优于Adam和Muon。该框架还解释了Adam训练不稳定性、Muon加速收敛等现象。

Conclusion: 本文提出的统一框架不仅解释了现有优化器的性能，还催生了新的结构感知预条件方法PolarGrad，其在实验中展现了优越性能。

Abstract: The ever-growing scale of deep learning models and datasets underscores the
critical importance of efficient optimization methods. While preconditioned
gradient methods such as Adam and AdamW are the de facto optimizers for
training neural networks and large language models, structure-aware
preconditioned optimizers like Shampoo and Muon, which utilize the matrix
structure of gradients, have demonstrated promising evidence of faster
convergence. In this paper, we introduce a unifying framework for analyzing
"matrix-aware" preconditioned methods, which not only sheds light on the
effectiveness of Muon and related optimizers but also leads to a class of new
structure-aware preconditioned methods. A key contribution of this framework is
its precise distinction between preconditioning strategies that treat neural
network weights as vectors (addressing curvature anisotropy) versus those that
consider their matrix structure (addressing gradient anisotropy). This
perspective provides new insights into several empirical phenomena in language
model pre-training, including Adam's training instabilities, Muon's accelerated
convergence, and the necessity of learning rate warmup for Adam. Building upon
this framework, we introduce PolarGrad, a new class of preconditioned
optimization methods based on the polar decomposition of matrix-valued
gradients. As a special instance, PolarGrad includes Muon with updates scaled
by the nuclear norm of the gradients. We provide numerical implementations of
these methods, leveraging efficient numerical polar decomposition algorithms
for enhanced convergence. Our extensive evaluations across diverse matrix
optimization problems and language model pre-training tasks demonstrate that
PolarGrad outperforms both Adam and Muon.

</details>


### [463] [PADAM: Parallel averaged Adam reduces the error for stochastic optimization in scientific machine learning](https://arxiv.org/abs/2505.22085)
*Arnulf Jentzen,Julian Kranz,Adrian Riekert*

Main category: math.OC

TL;DR: 该论文提出了一种名为PADAM的并行平均ADAM优化器，通过动态选择优化误差最小的平均变体，显著提升了优化性能，适用于科学机器学习问题。


<details>
  <summary>Details</summary>
Motivation: 现有平均技术（如Ruppert-Polyak和EMA）在加速随机梯度下降（SGD）优化时需要针对不同问题调整参数，难以通用。作者旨在开发一种自适应平均方法，无需额外梯度计算即可提升优化效果。

Method: 提出PADAM优化器，并行计算多个ADAM平均变体，并在训练过程中动态选择优化误差最小的变体。所有变体共享同一梯度轨迹，因此不增加计算负担。

Result: 在13个随机优化和深度神经网络学习任务中测试，PADAM在几乎所有案例中均取得最小优化误差，表现优于SGD、动量SGD、Adam等现有方法。

Conclusion: PADAM在科学机器学习问题中表现优异，建议广泛采用，并值得进一步研究DNN训练中的自适应平均方法。

Abstract: Averaging techniques such as Ruppert--Polyak averaging and exponential
movering averaging (EMA) are powerful approaches to accelerate optimization
procedures of stochastic gradient descent (SGD) optimization methods such as
the popular ADAM optimizer. However, depending on the specific optimization
problem under consideration, the type and the parameters for the averaging need
to be adjusted to achieve the smallest optimization error. In this work we
propose an averaging approach, which we refer to as parallel averaged ADAM
(PADAM), in which we compute parallely different averaged variants of ADAM and
during the training process dynamically select the variant with the smallest
optimization error. A central feature of this approach is that this procedure
requires no more gradient evaluations than the usual ADAM optimizer as each of
the averaged trajectories relies on the same underlying ADAM trajectory and
thus on the same underlying gradients. We test the proposed PADAM optimizer in
13 stochastic optimization and deep neural network (DNN) learning problems and
compare its performance with known optimizers from the literature such as
standard SGD, momentum SGD, Adam with and without EMA, and ADAMW. In
particular, we apply the compared optimizers to physics-informed neural
network, deep Galerkin, deep backward stochastic differential equation and deep
Kolmogorov approximations for boundary value partial differential equation
problems from scientific machine learning, as well as to DNN approximations for
optimal control and optimal stopping problems. In nearly all of the considered
examples PADAM achieves, sometimes among others and sometimes exclusively,
essentially the smallest optimization error. This work thus strongly suggest to
consider PADAM for scientific machine learning problems and also motivates
further research for adaptive averaging procedures within the training of DNNs.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [464] [Uncovering Bottlenecks and Optimizing Scientific Lab Workflows with Cycle Time Reduction Agents](https://arxiv.org/abs/2505.21534)
*Yao Fehlis*

Main category: cs.MA

TL;DR: 本文介绍了一种基于LangGraph的智能工作流CTRA，旨在通过自动化分析实验室操作指标来优化制药和生物技术公司的复杂工作流程。


<details>
  <summary>Details</summary>
Motivation: 科学实验室，尤其是制药和生物技术公司的实验室，由于化合物筛选和测定执行等任务的复杂性和量大，面临优化工作流程的重大挑战。

Method: CTRA包含三个主要组件：问题创建代理用于启动分析，操作指标代理用于数据提取和验证，以及见解代理用于报告和可视化，识别实验室流程中的瓶颈。

Result: CTRA在实验室数据集上的性能评估表明，它具有加速制药和生物技术开发的潜力。

Conclusion: CTRA为科学实验室提供了一个可扩展的框架，用于减少周期时间。

Abstract: Scientific laboratories, particularly those in pharmaceutical and
biotechnology companies, encounter significant challenges in optimizing
workflows due to the complexity and volume of tasks such as compound screening
and assay execution. We introduce Cycle Time Reduction Agents (CTRA), a
LangGraph-based agentic workflow designed to automate the analysis of lab
operational metrics. CTRA comprises three main components: the Question
Creation Agent for initiating analysis, Operational Metrics Agents for data
extraction and validation, and Insights Agents for reporting and visualization,
identifying bottlenecks in lab processes. This paper details CTRA's
architecture, evaluates its performance on a lab dataset, and discusses its
potential to accelerate pharmaceutical and biotechnological development. CTRA
offers a scalable framework for reducing cycle times in scientific labs.

</details>


### [465] [Streamlining Resilient Kubernetes Autoscaling with Multi-Agent Systems via an Automated Online Design Framework](https://arxiv.org/abs/2505.21559)
*Julien Soulé,Jean-Paul Jamont,Michel Occello,Louis-Marie Traonouez,Paul Théron*

Main category: cs.MA

TL;DR: 该论文提出了一种基于多智能体系统的自动化水平Pod自动扩展（HPA）框架，旨在提升Kubernetes集群在对抗性场景下的运行韧性。


<details>
  <summary>Details</summary>
Motivation: 云原生系统中，Kubernetes集群常因资源阻塞、瓶颈或Pod持续崩溃等问题面临运行韧性挑战，尤其在DDoS攻击等对抗场景下更为突出。传统HPA方法难以应对动态条件，而基于强化学习的方法通常仅优化单一目标，忽视更广泛的故障场景。

Method: 提出将运行韧性目标分解为针对特定故障的子目标，并委托给协作智能体共同形成HPA多智能体系统（MAS）。采用四阶段在线框架：1) 基于集群跟踪构建数字孪生模型；2) 在仿真中训练针对故障场景的智能体；3) 分析智能体行为以增强可解释性；4) 将学习策略迁移至真实集群。

Result: 实验表明，生成的HPA MAS在复杂集群的多种对抗条件下，其运行韧性表现优于三种最先进的HPA系统。

Conclusion: 通过多智能体协作和自动化框架，该研究有效提升了Kubernetes集群在动态及对抗性环境中的韧性，为云原生系统的弹性扩展提供了新思路。

Abstract: In cloud-native systems, Kubernetes clusters with interdependent services
often face challenges to their operational resilience due to poor workload
management issues such as resource blocking, bottlenecks, or continuous pod
crashes. These vulnerabilities are further amplified in adversarial scenarios,
such as Distributed Denial-of-Service attacks (DDoS). Conventional Horizontal
Pod Autoscaling (HPA) approaches struggle to address such dynamic conditions,
while reinforcement learning-based methods, though more adaptable, typically
optimize single goals like latency or resource usage, neglecting broader
failure scenarios. We propose decomposing the overarching goal of maintaining
operational resilience into failure-specific sub-goals delegated to
collaborative agents, collectively forming an HPA Multi-Agent System (MAS). We
introduce an automated, four-phase online framework for HPA MAS design: 1)
modeling a digital twin built from cluster traces; 2) training agents in
simulation using roles and missions tailored to failure contexts; 3) analyzing
agent behaviors for explainability; and 4) transferring learned policies to the
real cluster. Experimental results demonstrate that the generated HPA MASs
outperform three state-of-the-art HPA systems in sustaining operational
resilience under various adversarial conditions in a proposed complex cluster.

</details>


### [466] [Herd Behavior: Investigating Peer Influence in LLM-based Multi-Agent Systems](https://arxiv.org/abs/2505.21588)
*Young-Min Cho,Sharath Chandra Guntuku,Lyle Ungar*

Main category: cs.MA

TL;DR: 该论文研究了基于大语言模型的多智能体系统中的从众行为，揭示了影响从众行为的因素及其对协作结果的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型的个体行为已被广泛研究，但在多智能体系统中，同伴影响下的动态行为仍未充分探索。本文旨在填补这一空白，研究多智能体系统中的从众行为及其影响因素。

Method: 通过一系列受控实验，研究了从众行为的形成机制，重点关注自我信心与感知同伴信心之间的差距、同伴信息呈现形式等因素。

Result: 实验表明，自我信心与感知同伴信心的差距显著影响从众行为；同伴信息的呈现形式调节从众行为的强度；适度的从众行为可以提升协作效果。

Conclusion: 研究为大语言模型多智能体系统的社会动态提供了新见解，并为设计更有效和自适应的协作框架开辟了途径。

Abstract: Recent advancements in Large Language Models (LLMs) have enabled the
emergence of multi-agent systems where LLMs interact, collaborate, and make
decisions in shared environments. While individual model behavior has been
extensively studied, the dynamics of peer influence in such systems remain
underexplored. In this paper, we investigate herd behavior, the tendency of
agents to align their outputs with those of their peers, within LLM-based
multi-agent interactions. We present a series of controlled experiments that
reveal how herd behaviors are shaped by multiple factors. First, we show that
the gap between self-confidence and perceived confidence in peers significantly
impacts an agent's likelihood to conform. Second, we find that the format in
which peer information is presented plays a critical role in modulating the
strength of herd behavior. Finally, we demonstrate that the degree of herd
behavior can be systematically controlled, and that appropriately calibrated
herd tendencies can enhance collaborative outcomes. These findings offer new
insights into the social dynamics of LLM-based systems and open pathways for
designing more effective and adaptive multi-agent collaboration frameworks.

</details>


### [467] [Incorporating LLMs for Large-Scale Urban Complex Mobility Simulation](https://arxiv.org/abs/2505.21880)
*Yu-Lun Song,Chung-En Tsern,Che-Cheng Wu,Yu-Ming Chang,Syuan-Bo Huang,Wei-Chu Chen,Michael Chia-Liang Lin,Yu-Ta Lin*

Main category: cs.MA

TL;DR: 该研究提出了一种结合大语言模型(LLM)和基于代理建模(ABM)的创新城市交通模拟方法，用于提升模拟的多样性和真实性，并以台北市为例验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的ABM方法在模拟城市交通时缺乏多样性和真实性，因此需要一种更先进的方法来提升模拟效果。

Method: 通过整合LLM和ABM，生成合成人口档案、分配常规和临时地点，并模拟个性化路线。

Result: 模拟成功再现了台北市的个体行为和大规模交通模式，并提供了路线热图和交通方式指标等关键洞察。

Conclusion: 该方法为城市规划者提供了有价值的决策信息，未来工作将集中于建立更稳健的验证框架以确保准确性。

Abstract: This study presents an innovative approach to urban mobility simulation by
integrating a Large Language Model (LLM) with Agent-Based Modeling (ABM).
Unlike traditional rule-based ABM, the proposed framework leverages LLM to
enhance agent diversity and realism by generating synthetic population
profiles, allocating routine and occasional locations, and simulating
personalized routes. Using real-world data, the simulation models individual
behaviors and large-scale mobility patterns in Taipei City. Key insights, such
as route heat maps and mode-specific indicators, provide urban planners with
actionable information for policy-making. Future work focuses on establishing
robust validation frameworks to ensure accuracy and reliability in urban
planning applications.

</details>


### [468] [Reward-Independent Messaging for Decentralized Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.21985)
*Naoto Yoshida,Tadahiro Taniguchi*

Main category: cs.MA

TL;DR: MARL-CPC框架通过集体预测编码实现去中心化多智能体间的有效通信，在非合作环境中优于传统消息即行动方法。


<details>
  <summary>Details</summary>
Motivation: 在多智能体强化学习中，部分可观测性下有效通信能提升智能体性能。现有方法通常假设合作且将消息视为行动空间的一部分，限制了在非合作或无直接奖励场景中的应用。

Method: 提出MARL-CPC框架，基于集体预测编码（CPC）建立消息学习模型，将消息与状态推断关联而非直接作为行动。开发了Bandit-CPC和IPPO-CPC两种算法。

Result: 实验表明，两种算法在非合作任务中均优于传统消息即行动方法，即使消息对发送者无直接收益时仍能建立有效通信。

Conclusion: MARL-CPC为复杂去中心化环境中的协调提供了新思路，突破了传统方法对合作假设的依赖。

Abstract: In multi-agent reinforcement learning (MARL), effective communication
improves agent performance, particularly under partial observability. We
propose MARL-CPC, a framework that enables communication among fully
decentralized, independent agents without parameter sharing. MARL-CPC
incorporates a message learning model based on collective predictive coding
(CPC) from emergent communication research. Unlike conventional methods that
treat messages as part of the action space and assume cooperation, MARL-CPC
links messages to state inference, supporting communication in non-cooperative,
reward-independent settings. We introduce two algorithms -Bandit-CPC and
IPPO-CPC- and evaluate them in non-cooperative MARL tasks. Benchmarks show that
both outperform standard message-as-action approaches, establishing effective
communication even when messages offer no direct benefit to the sender. These
results highlight MARL-CPC's potential for enabling coordination in complex,
decentralized environments.

</details>


### [469] [Sentiment Simulation using Generative AI Agents](https://arxiv.org/abs/2505.22125)
*Melrose Tia,Jezreel Sophia Lanuzo,Lei Rigi Baltazar,Marie Joy Lopez-Relente,Diwa Malaya Quiñones,Jason Albia*

Main category: cs.MA

TL;DR: 该论文提出了一种基于生成式AI代理的情感模拟框架，通过心理特征建模实现前瞻性情感分析，在政治经济场景测试中达到81%-86%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统情感分析依赖表层语言模式和回顾性数据，难以捕捉心理和情境因素。这限制了其在政策测试、行为预测等需要前瞻性洞察的应用效果。

Method: 1) 基于2485名菲律宾受访者的社会人口统计和心理特征构建AI代理；2) 采用分类/情境化编码实现代理具身化；3) 暴露于真实政治经济场景；4) 生成带解释的情感评分。使用QWA指标评估准确性。

Result: 情境化编码复制原始调查的准确率达92%。情感模拟任务中，情境化编码显著优于分类编码(p<0.0001)，准确率达81%-86%。不同场景测试显示稳定性(±0.2-0.5%)和鲁棒性(p=0.9676)。

Conclusion: 该研究建立了基于心理特征的可扩展情感建模框架，实现了从回顾性分类到前瞻性动态模拟的范式转变。

Abstract: Traditional sentiment analysis relies on surface-level linguistic patterns
and retrospective data, limiting its ability to capture the psychological and
contextual drivers of human sentiment. These limitations constrain its
effectiveness in applications that require predictive insight, such as policy
testing, narrative framing, and behavioral forecasting. We present a robust
framework for sentiment simulation using generative AI agents embedded with
psychologically rich profiles. Agents are instantiated from a nationally
representative survey of 2,485 Filipino respondents, combining sociodemographic
information with validated constructs of personality traits, values, beliefs,
and socio-political attitudes. The framework includes three stages: (1) agent
embodiment via categorical or contextualized encodings, (2) exposure to
real-world political and economic scenarios, and (3) generation of sentiment
ratings accompanied by explanatory rationales. Using Quadratic Weighted
Accuracy (QWA), we evaluated alignment between agent-generated and human
responses. Contextualized encoding achieved 92% alignment in replicating
original survey responses. In sentiment simulation tasks, agents reached
81%--86% accuracy against ground truth sentiment, with contextualized profile
encodings significantly outperforming categorical (p < 0.0001, Cohen's d =
0.70). Simulation results remained consistent across repeated trials
(+/-0.2--0.5% SD) and resilient to variation in scenario framing (p = 0.9676,
Cohen's d = 0.02). Our findings establish a scalable framework for sentiment
modeling through psychographically grounded AI agents. This work signals a
paradigm shift in sentiment analysis from retrospective classification to
prospective and dynamic simulation grounded in psychology of sentiment
formation.

</details>


### [470] [Topological Structure Learning Should Be A Research Priority for LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2505.22467)
*Jiaxi Yang,Mengqi Zhang,Yiqiao Jin,Hao Chen,Qingsong Wen,Lu Lin,Yi He,Weijie Xu,James Evans,Jindong Wang*

Main category: cs.MA

TL;DR: 该立场论文提出关注多智能体系统（MAS）的结构组织，以优化协作效率，并引入一个三阶段框架来实现这一目标。


<details>
  <summary>Details</summary>
Motivation: 尽管基于大语言模型的多智能体系统在解决复杂任务方面表现出强大能力，但其结构组织对协作效率的影响尚未充分研究。本文旨在引导研究社区关注这一关键维度。

Method: 提出了一个系统性三阶段框架：智能体选择、结构分析和拓扑合成，以开发针对特定任务的拓扑感知多智能体系统。

Result: 该框架为语言模型、强化学习、图学习和生成建模等领域开辟了新的研究方向，有望释放多智能体系统在复杂实际应用中的潜力。

Conclusion: 本文的视角和框架为代理AI时代提供了关键的新见解，并讨论了多系统评估中的潜在挑战和机遇。

Abstract: Large Language Model-based Multi-Agent Systems (MASs) have emerged as a
powerful paradigm for tackling complex tasks through collaborative
intelligence. Nevertheless, the question of how agents should be structurally
organized for optimal cooperation remains largely unexplored. In this position
paper, we aim to gently redirect the focus of the MAS research community toward
this critical dimension: develop topology-aware MASs for specific tasks.
Specifically, the system consists of three core components - agents,
communication links, and communication patterns - that collectively shape its
coordination performance and efficiency. To this end, we introduce a
systematic, three-stage framework: agent selection, structure profiling, and
topology synthesis. Each stage would trigger new research opportunities in
areas such as language models, reinforcement learning, graph learning, and
generative modeling; together, they could unleash the full potential of MASs in
complicated real-world applications. Then, we discuss the potential challenges
and opportunities in the evaluation of multiple systems. We hope our
perspective and framework can offer critical new insights in the era of agentic
AI.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [471] [On the performance of machine-learning assisted Monte Carlo in sampling from simple statistical physics models](https://arxiv.org/abs/2505.22598)
*Luca Maria Del Bono,Federico Ricci-Tersenghi,Francesco Zamponi*

Main category: cond-mat.dis-nn

TL;DR: 该论文对Sequential Tempering方法在Curie-Weiss模型中的应用进行了理论分析，确定了最优权重和训练过程，并比较了是否加入局部Metropolis Monte Carlo步骤的效果。


<details>
  <summary>Details</summary>
Motivation: 近年来，机器学习技术被越来越多地应用于模拟难以采样的系统，但缺乏广泛的理论理解，可能导致次优实现。本文旨在填补这一空白，为机器学习技术与蒙特卡洛采样的结合提供理论基础。

Method: 论文采用Sequential Tempering方法，结合浅层MADE架构，对Curie-Weiss模型进行了完整的解析研究，并分析了梯度下降优化下的训练过程。

Result: 研究确定了最优权重和训练过程，并发现加入局部Metropolis Monte Carlo步骤能显著改善性能。

Conclusion: 该研究为机器学习技术在蒙特卡洛采样和优化中的应用奠定了清晰的理论基础，并提供了最佳实践的理论预测。

Abstract: Recent years have seen a rise in the application of machine learning
techniques to aid the simulation of hard-to-sample systems that cannot be
studied using traditional methods. Despite the introduction of many different
architectures and procedures, a wide theoretical understanding is still
lacking, with the risk of suboptimal implementations. As a first step to
address this gap, we provide here a complete analytic study of the widely-used
Sequential Tempering procedure applied to a shallow MADE architecture for the
Curie-Weiss model. The contribution of this work is twofold: firstly, we give a
description of the optimal weights and of the training under Gradient Descent
optimization. Secondly, we compare what happens in Sequential Tempering with
and without the addition of local Metropolis Monte Carlo steps. We are thus
able to give theoretical predictions on the best procedure to apply in this
case. This work establishes a clear theoretical basis for the integration of
machine learning techniques into Monte Carlo sampling and optimization.

</details>
