<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 58]
- [cs.AI](#cs.AI) [Total: 27]
- [cs.LG](#cs.LG) [Total: 90]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.HC](#cs.HC) [Total: 2]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 3]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.IT](#cs.IT) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.IR](#cs.IR) [Total: 8]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 36]
- [cs.RO](#cs.RO) [Total: 8]
- [quant-ph](#quant-ph) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [eess.IV](#eess.IV) [Total: 4]
- [cs.DC](#cs.DC) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 3]
- [cs.NI](#cs.NI) [Total: 3]
- [math.OC](#math.OC) [Total: 2]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [physics.ao-ph](#physics.ao-ph) [Total: 3]
- [cs.CR](#cs.CR) [Total: 16]
- [stat.ML](#stat.ML) [Total: 10]
- [math.NA](#math.NA) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [cs.GR](#cs.GR) [Total: 3]
- [physics.geo-ph](#physics.geo-ph) [Total: 2]
- [cs.MM](#cs.MM) [Total: 9]
- [cs.SE](#cs.SE) [Total: 6]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.SD](#cs.SD) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Chat-of-Thought: Collaborative Multi-Agent System for Generating Domain Specific Information](https://arxiv.org/abs/2506.10086)
*Christodoulos Constantinides,Shuxin Lin,Nianjun Zhou,Dhaval Patel*

Main category: cs.CL

TL;DR: 本文提出了一种名为Chat-of-Thought的多智能体系统，通过协作式LLM代理和动态任务路由优化工业资产FMEA文档的生成与验证。


<details>
  <summary>Details</summary>
Motivation: 传统FMEA文档生成过程复杂且易出错，需通过多角色协作与迭代优化提升准确性和效率。本文旨在利用多智能体AI技术解决工业设备监控中的这一挑战。

Method: 采用基于LLM的多角色代理协作框架，结合动态任务路由和'思维对话'机制，通过模板驱动的工作流实现内容迭代优化与上下文感知的智能体协同。

Result: 系统展示了在工业设备FMEA生成中实现动态讨论驱动的内容精炼能力，验证了多智能体协作框架在复杂工业场景中的有效性。

Conclusion: Chat-of-Thought通过创新性多角色讨论机制，为工业领域知识密集型文档生成提供了可扩展的AI解决方案，具有显著应用潜力。

Abstract: This paper presents a novel multi-agent system called Chat-of-Thought,
designed to facilitate the generation of Failure Modes and Effects Analysis
(FMEA) documents for industrial assets. Chat-of-Thought employs multiple
collaborative Large Language Model (LLM)-based agents with specific roles,
leveraging advanced AI techniques and dynamic task routing to optimize the
generation and validation of FMEA tables. A key innovation in this system is
the introduction of a Chat of Thought, where dynamic, multi-persona-driven
discussions enable iterative refinement of content. This research explores the
application domain of industrial equipment monitoring, highlights key
challenges, and demonstrates the potential of Chat-of-Thought in addressing
these challenges through interactive, template-driven workflows and
context-aware agent collaboration.

</details>


### [2] [When Meaning Stays the Same, but Models Drift: Evaluating Quality of Service under Token-Level Behavioral Instability in LLMs](https://arxiv.org/abs/2506.10095)
*Xiao Li,Joel Kreuzwieser,Alan Peters*

Main category: cs.CL

TL;DR: 研究提出Prompt-Based Semantic Shift (PBSS)框架，用于检测大语言模型在语义相同但措辞不同的提示下产生的行为漂移，发现模型响应存在与分词策略、解码机制相关的统计规律性，揭示了提示重述对模型评估稳定性的潜在影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分关注大语言模型对语义相同但表层表达不同的提示（prompt variance）的敏感性差异，需量化分析此类语义等效重述引发的模型行为不稳定性。

Method: 提出PBSS诊断框架，通过在10个约束性任务中构造语义等效但token化形式不同的提示组，系统性测量模型输出的行为漂移程度。

Result: 实验表明不同模型在PBSS测试中呈现稳定且模型特异性的响应偏移模式，其规律性与token化策略及解码过程的统计特性高度相关。

Conclusion: 提示的token级实现差异会导致模型服务质量波动，提示重述鲁棒性应成为模型评估的关键维度，分词与解码机制是影响服务稳定性的潜在因素。

Abstract: We investigate how large language models respond to prompts that differ only
in their token-level realization but preserve the same semantic intent, a
phenomenon we call prompt variance. We propose Prompt-Based Semantic Shift
(PBSS), a diagnostic framework for measuring behavioral drift in LLMs under
semantically equivalent prompt rewordings. Applied to ten constrained tasks,
PBSS reveals consistent, model-specific response shifts, suggesting statistical
regularities linked to tokenization and decoding. These results highlight an
overlooked dimension of model evaluation stability under rephrasing and suggest
that tokenization strategies and decoding dynamics may contribute to
post-training quality of service instability.

</details>


### [3] [ChartReasoner: Code-Driven Modality Bridging for Long-Chain Reasoning in Chart Question Answering](https://arxiv.org/abs/2506.10116)
*Caijun Jia,Nan Xu,Jingxuan Wei,Qingli Wang,Lei Wang,Bihui Yu,Junnan Zhu*

Main category: cs.CL

TL;DR: 本文提出ChartReasoner框架，通过代码驱动两阶段方法解决图表视觉推理中的信息丢失问题，在保留细节的同时以较少参数达到先进模型性能，接近GPT-4o的域外表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉推理方法通过图像转文本会丢失关键视觉结构信息，尤其在需要高细节的图表问答任务中。需开发能保留原始视觉语义的多模态推理方法。

Method: 1) 训练高保真模型将图表转为结构化ECharts代码；2) 设计自动合成流程生成推理轨迹，用代码验证器过滤低质样本；3) 结合监督微调与强化学习训练多模态模型。

Result: 在4个基准测试中表现优异，参数更少但性能与先进开源模型相当，在域外场景接近GPT-4o。验证了框架在细节保留和推理精度上的有效性。

Conclusion: ChartReasoner通过代码驱动方法有效平衡视觉信息保留与推理能力，为多模态推理提供了可解释、可扩展的解决方案，缩小了与专有系统的性能差距。

Abstract: Recently, large language models have shown remarkable reasoning capabilities
through long-chain reasoning before responding. However, how to extend this
capability to visual reasoning tasks remains an open challenge. Existing
multimodal reasoning approaches transfer such visual reasoning task into
textual reasoning task via several image-to-text conversions, which often lose
critical structural and semantic information embedded in visualizations,
especially for tasks like chart question answering that require a large amount
of visual details. To bridge this gap, we propose ChartReasoner, a code-driven
novel two-stage framework designed to enable precise, interpretable reasoning
over charts. We first train a high-fidelity model to convert diverse chart
images into structured ECharts codes, preserving both layout and data semantics
as lossless as possible. Then, we design a general chart reasoning data
synthesis pipeline, which leverages this pretrained transport model to
automatically and scalably generate chart reasoning trajectories and utilizes a
code validator to filter out low-quality samples. Finally, we train the final
multimodal model using a combination of supervised fine-tuning and
reinforcement learning on our synthesized chart reasoning dataset and
experimental results on four public benchmarks clearly demonstrate the
effectiveness of our proposed ChartReasoner. It can preserve the original
details of the charts as much as possible and perform comparably with
state-of-the-art open-source models while using fewer parameters, approaching
the performance of proprietary systems like GPT-4o in out-of-domain settings.

</details>


### [4] [Unsupervised Elicitation of Language Models](https://arxiv.org/abs/2506.10139)
*Jiaxin Wen,Zachary Ankner,Arushi Somani,Peter Hase,Samuel Marks,Jacob Goldman-Wetzler,Linda Petrini,Henry Sleight,Collin Burns,He He,Shi Feng,Ethan Perez,Jan Leike*

Main category: cs.CL

TL;DR: 提出无监督算法ICM，通过最大化内部一致性微调预训练语言模型，无需人工标注即可超越人类监督效果，并在前沿模型训练中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当语言模型具备超人类能力时，获取高质量人工监督变得困难。现有后训练范式依赖人类标注的局限性凸显，需探索无监督方法激发模型自身潜力。

Method: 开发Internal Coherence Maximization(ICM)算法，利用模型自身生成标签进行微调，通过最大化输出内部一致性实现无监督训练。

Result: 在GSM8k验证、TruthfulQA等任务中匹配黄金标注效果；在超人类能力任务上显著优于人工标注；基于Claude 3.5 Haiku训练的无监督奖励模型和助手均超越人类监督版本。

Conclusion: ICM证明无监督方法可有效激发语言模型潜力，为训练超人类模型提供新范式，其性能提升验证了自监督机制在前沿模型训练中的优越性。

Abstract: To steer pretrained language models for downstream tasks, today's
post-training paradigm relies on humans to specify desired behaviors. However,
for models with superhuman capabilities, it is difficult or impossible to get
high-quality human supervision. To address this challenge, we introduce a new
unsupervised algorithm, Internal Coherence Maximization (ICM), to fine-tune
pretrained language models on their own generated labels, \emph{without
external supervision}. On GSM8k-verification, TruthfulQA, and Alpaca reward
modeling tasks, our method matches the performance of training on golden
supervision and outperforms training on crowdsourced human supervision. On
tasks where LMs' capabilities are strongly superhuman, our method can elicit
those capabilities significantly better than training on human labels. Finally,
we show that our method can improve the training of frontier LMs: we use our
method to train an unsupervised reward model and use reinforcement learning to
train a Claude 3.5 Haiku-based assistant. Both the reward model and the
assistant outperform their human-supervised counterparts.

</details>


### [5] [When Large Language Models are Reliable for Judging Empathic Communication](https://arxiv.org/abs/2506.10150)
*Aakriti Kumar,Nalin Poungpeth,Diyi Yang,Erina Farrell,Bruce Lambert,Matthew Groh*

Main category: cs.CL

TL;DR: 研究通过比较专家、众包工作者和LLMs在四个心理学、NLP及传播学框架下对同理心沟通的标注一致性，发现LLMs的可靠性接近专家水平且超越众包，专家一致性为LLM性能评估提供了更有效的基准。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在评估同理心沟通细微差别时的可靠性，并与人类专家和众包标注结果对比，以验证其在情感敏感应用中的适用性。

Method: 使用来自心理学、NLP和传播学的四个评估框架，对200个真实对话进行标注，收集3150个专家标注、2844个众包标注和3150个LLM标注，分析评分者间信度。

Result: 专家一致性较高但受框架子维度（清晰度、复杂性、主观性）影响；LLMs在所有框架中接近专家基准，且可靠性超过众包工作者。

Conclusion: 当以专家一致性为基准验证特定任务时，LLMs可提升情感敏感应用（如对话伴侣）的透明度与监督能力，其标注可靠性优于众包。

Abstract: Large language models (LLMs) excel at generating empathic responses in
text-based conversations. But, how reliably do they judge the nuances of
empathic communication? We investigate this question by comparing how experts,
crowdworkers, and LLMs annotate empathic communication across four evaluative
frameworks drawn from psychology, natural language processing, and
communications applied to 200 real-world conversations where one speaker shares
a personal problem and the other offers support. Drawing on 3,150 expert
annotations, 2,844 crowd annotations, and 3,150 LLM annotations, we assess
inter-rater reliability between these three annotator groups. We find that
expert agreement is high but varies across the frameworks' sub-components
depending on their clarity, complexity, and subjectivity. We show that expert
agreement offers a more informative benchmark for contextualizing LLM
performance than standard classification metrics. Across all four frameworks,
LLMs consistently approach this expert level benchmark and exceed the
reliability of crowdworkers. These results demonstrate how LLMs, when validated
on specific tasks with appropriate benchmarks, can support transparency and
oversight in emotionally sensitive applications including their use as
conversational companions.

</details>


### [6] [Analyzing Emotions in Bangla Social Media Comments Using Machine Learning and LIME](https://arxiv.org/abs/2506.10154)
*Bidyarthi Paul,SM Musfiqur Rahman,Dipta Biswas,Md. Ziaul Hasan,Md. Zahid Hossain*

Main category: cs.CL

TL;DR: 该研究探讨了使用多种机器学习模型和解释技术对孟加拉语社交媒体评论进行情感分析，旨在提升低资源语言的情感分析效果。


<details>
  <summary>Details</summary>
Motivation: 针对孟加拉语等资源有限但具有独特地区表达和文化特征的语言，推动其情感分析技术的发展，填补现有研究的不足。

Method: 基于EmoNoBa数据集的22,698条评论，结合线性SVM、KNN、随机森林（使用TF-IDF向量化n-gram数据）、BiLSTM和AdaBoost模型，并应用PCA降维及LIME解释模型预测。

Result: 通过对比不同模型和技术，验证了AdaBoost结合决策树在情感分析中的有效性，同时探索了PCA降维对性能的影响及LIME的可解释性应用。

Conclusion: 研究为低资源语言的情感分析提供了多方法框架，证明结合传统机器学习、深度学习和可解释性技术能有效识别孟加拉语中的复杂情感特征。

Abstract: Research on understanding emotions in written language continues to expand,
especially for understudied languages with distinctive regional expressions and
cultural features, such as Bangla. This study examines emotion analysis using
22,698 social media comments from the EmoNoBa dataset. For language analysis,
we employ machine learning models: Linear SVM, KNN, and Random Forest with
n-gram data from a TF-IDF vectorizer. We additionally investigated how PCA
affects the reduction of dimensionality. Moreover, we utilized a BiLSTM model
and AdaBoost to improve decision trees. To make our machine learning models
easier to understand, we used LIME to explain the predictions of the AdaBoost
classifier, which uses decision trees. With the goal of advancing sentiment
analysis in languages with limited resources, our work examines various
techniques to find efficient techniques for emotion identification in Bangla.

</details>


### [7] [Measuring Corporate Human Capital Disclosures: Lexicon, Data, Code, and Research Opportunities](https://arxiv.org/abs/2506.10155)
*Elizabeth Demers,Victor Xiaoqi Wang,Kean Wu*

Main category: cs.CL

TL;DR: 本文开发了一个基于机器学习的人力资本（HC）关键词词典，包含五个子类别，并公开了相关数据与代码，助力未来HC管理与披露研究。


<details>
  <summary>Details</summary>
Motivation: 人力资本对企业价值创造日益重要，但缺乏统一的测量与披露标准，需系统性工具支持相关研究。

Method: 使用word2vec算法分析已确认的HC披露文本，构建包含DEI、健康安全、劳资文化、薪酬福利、人口统计等五类关键词的词典，并提供BERT模型微调示例。

Result: 成功开发多维HC关键词分类体系，并公开词典、企业HC披露数据及Python代码库，支持研究者自定义分析框架。

Conclusion: 所提供工具可拓展应用于不同企业沟通样本的HC研究，未来需结合ESG等框架深化HC披露规则与价值关联分析。

Abstract: Human capital (HC) is increasingly important to corporate value creation.
Unlike other assets, however, HC is not currently subject to well-defined
measurement or disclosure rules. We use a machine learning algorithm (word2vec)
trained on a confirmed set of HC disclosures to develop a comprehensive list of
HC-related keywords classified into five subcategories (DEI; health and safety;
labor relations and culture; compensation and benefits; and demographics and
other) that capture the multidimensional nature of HC management. We share our
lexicon, corporate HC disclosures, and the Python code used to develop the
lexicon, and we provide detailed examples of using our data and code, including
for fine-tuning a BERT model. Researchers can use our HC lexicon (or modify the
code to capture another construct of interest) with their samples of corporate
communications to address pertinent HC questions. We close with a discussion of
future research opportunities related to HC management and disclosure.

</details>


### [8] [Can LLMs Generate Good Stories? Insights and Challenges from a Narrative Planning Perspective](https://arxiv.org/abs/2506.10161)
*Yi Wang,Max Kreminski*

Main category: cs.CL

TL;DR: 研究通过计算叙事学理论评估大语言模型（LLM）在故事生成中的叙事规划能力，发现GPT-4级模型可生成小规模因果合理的故事，但复杂角色意图与戏剧冲突仍需强化学习支持。


<details>
  <summary>Details</summary>
Motivation: 现有自动方法难以评估LLM生成故事的质量，而人工评估成本高且主观性强。研究旨在通过叙事规划问题，系统分析LLM生成高质量故事的能力。

Method: 基于文学案例构建叙事规划评估基准，聚焦因果合理性、角色意图性和戏剧冲突三个维度，测试不同规模LLM的生成能力。

Result: GPT-4级模型可生成小规模因果合理的故事，但角色意图与戏剧冲突的规划仍具挑战性，需结合复杂推理的强化学习模型。

Conclusion: LLM生成故事的质量与规模受限于叙事复杂度，未来在游戏等应用中需权衡生成规模与多维度质量，并优化推理能力。

Abstract: Story generation has been a prominent application of Large Language Models
(LLMs). However, understanding LLMs' ability to produce high-quality stories
remains limited due to challenges in automatic evaluation methods and the high
cost and subjectivity of manual evaluation. Computational narratology offers
valuable insights into what constitutes a good story, which has been applied in
the symbolic narrative planning approach to story generation. This work aims to
deepen the understanding of LLMs' story generation capabilities by using them
to solve narrative planning problems. We present a benchmark for evaluating
LLMs on narrative planning based on literature examples, focusing on causal
soundness, character intentionality, and dramatic conflict. Our experiments
show that GPT-4 tier LLMs can generate causally sound stories at small scales,
but planning with character intentionality and dramatic conflict remains
challenging, requiring LLMs trained with reinforcement learning for complex
reasoning. The results offer insights on the scale of stories that LLMs can
generate while maintaining quality from different aspects. Our findings also
highlight interesting problem solving behaviors and shed lights on challenges
and considerations for applying LLM narrative planning in game environments.

</details>


### [9] [Q2E: Query-to-Event Decomposition for Zero-Shot Multilingual Text-to-Video Retrieval](https://arxiv.org/abs/2506.10202)
*Shubhashis Roy Dipta,Francis Ferraro*

Main category: cs.CL

TL;DR: 提出Q2E方法，通过分解查询和融合多模态信息提升零样本多语言文本到视频检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂现实世界事件的视频检索中存在局限性，需通过自动提取LLMs和VLMs中的潜在参数知识来改进检索效果。

Method: Q2E将查询分解为事件相关组件，结合视觉与语音输入，并采用基于熵的零样本融合方法整合多模态知识。

Result: 在多个数据集和检索指标上超越SOTA基线模型，且音频信息集成显著提升文本到视频检索性能。

Conclusion: Q2E验证了参数知识分解与多模态融合的有效性，开源代码与数据为后续研究提供基础。

Abstract: Recent approaches have shown impressive proficiency in extracting and
leveraging parametric knowledge from Large-Language Models (LLMs) and
Vision-Language Models (VLMs). In this work, we consider how we can improve the
identification and retrieval of videos related to complex real-world events by
automatically extracting latent parametric knowledge about those events. We
present Q2E: a Query-to-Event decomposition method for zero-shot multilingual
text-to-video retrieval, adaptable across datasets, domains, LLMs, or VLMs. Our
approach demonstrates that we can enhance the understanding of otherwise overly
simplified human queries by decomposing the query using the knowledge embedded
in LLMs and VLMs. We additionally show how to apply our approach to both visual
and speech-based inputs. To combine this varied multimodal knowledge, we adopt
entropy-based fusion scoring for zero-shot fusion. Through evaluations on two
diverse datasets and multiple retrieval metrics, we demonstrate that Q2E
outperforms several state-of-the-art baselines. Our evaluation also shows that
integrating audio information can significantly improve text-to-video
retrieval. We have released code and data for future research.

</details>


### [10] [TTT-Bench: A Benchmark for Evaluating Reasoning Ability with Simple and Novel Tic-Tac-Toe-style Games](https://arxiv.org/abs/2506.10209)
*Prakamya Mishra,Jiang Liu,Jialian Wu,Xiaodong Yu,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

TL;DR: 大型推理模型（LRMs）在复杂数学问题上表现优异，但在简单策略、空间和逻辑推理任务（如井字棋类游戏）中表现不佳。研究提出TTT-Bench基准测试发现，模型在长期战略推理上存在显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有推理基准多集中于STEM领域，但LRMs在更广泛任务领域的基础推理能力（如对手意图理解、空间配置分析）尚未被充分探索。

Method: 通过可验证的双人井字棋类游戏生成程序化问题，构建TTT-Bench基准，评估模型在幼龄人类即可解决的战略/空间/逻辑推理能力。

Result: 顶尖LRMs在TTT-Bench上平均得分比MATH 500低41%、比AIME 2024低5%，大模型依赖短推理路径且普遍无法处理长期战略推理场景。

Conclusion: LRMs在基础推理任务中的表现与复杂数学能力不匹配，揭示当前模型在简单但新颖的战略推理场景中存在系统性缺陷，需开发更全面的评估基准。

Abstract: Large reasoning models (LRMs) have demonstrated impressive reasoning
capabilities across a broad range of tasks including Olympiad-level
mathematical problems, indicating evidence of their complex reasoning
abilities. While many reasoning benchmarks focus on the STEM domain, the
ability of LRMs to reason correctly in broader task domains remains
underexplored. In this work, we introduce \textbf{TTT-Bench}, a new benchmark
that is designed to evaluate basic strategic, spatial, and logical reasoning
abilities in LRMs through a suite of four two-player Tic-Tac-Toe-style games
that humans can effortlessly solve from a young age. We propose a simple yet
scalable programmatic approach for generating verifiable two-player game
problems for TTT-Bench. Although these games are trivial for humans, they
require reasoning about the intentions of the opponent, as well as the game
board's spatial configurations, to ensure a win. We evaluate a diverse set of
state-of-the-art LRMs, and \textbf{discover that the models that excel at hard
math problems frequently fail at these simple reasoning games}. Further testing
reveals that our evaluated reasoning models score on average $\downarrow$ 41\%
\& $\downarrow$ 5\% lower on TTT-Bench compared to MATH 500 \& AIME 2024
respectively, with larger models achieving higher performance using shorter
reasoning traces, where most of the models struggle on long-term strategic
reasoning situations on simple and new TTT-Bench tasks.

</details>


### [11] [Classifying Unreliable Narrators with Large Language Models](https://arxiv.org/abs/2506.10231)
*Anneliese Brei,Katharine Henry,Abhisheik Sharma,Shashank Srivastava,Snigdha Chaturvedi*

Main category: cs.CL

TL;DR: 本文提出利用计算方法识别不可靠叙述者，构建跨领域标注数据集TUNa，测试大语言模型在少样本、微调等场景下的分类性能，发现任务具有挑战性但存在潜力，并开源数据集与代码。


<details>
  <summary>Details</summary>
Motivation: 通过计算手段验证文学理论中的叙述者可靠性分类框架，将叙事学概念应用于现实文本分析，解决非故意失真信息的自动识别问题。

Method: 基于叙事学理论定义三类不可靠性分类任务，构建多领域人工标注数据集TUNa，采用少样本学习、微调和课程学习策略，对比开源与商业大语言模型性能。

Result: 实验表明不可靠叙述者识别任务难度较高，但大语言模型展现出潜在应用价值，不同学习策略对模型性能产生差异化影响。

Conclusion: 通过文学理论与计算方法的交叉研究验证了可行性，开源数据集为后续研究提供基准，强调该领域需要更深入的跨学科探索。

Abstract: Often when we interact with a first-person account of events, we consider
whether or not the narrator, the primary speaker of the text, is reliable. In
this paper, we propose using computational methods to identify unreliable
narrators, i.e. those who unintentionally misrepresent information. Borrowing
literary theory from narratology to define different types of unreliable
narrators based on a variety of textual phenomena, we present TUNa, a
human-annotated dataset of narratives from multiple domains, including blog
posts, subreddit posts, hotel reviews, and works of literature. We define
classification tasks for intra-narrational, inter-narrational, and
inter-textual unreliabilities and analyze the performance of popular
open-weight and proprietary LLMs for each. We propose learning from literature
to perform unreliable narrator classification on real-world text data. To this
end, we experiment with few-shot, fine-tuning, and curriculum learning
settings. Our results show that this task is very challenging, and there is
potential for using LLMs to identify unreliable narrators. We release our
expert-annotated dataset and code and invite future research in this area.

</details>


### [12] [ToxSyn-PT: A Large-Scale Synthetic Dataset for Hate Speech Detection in Portuguese](https://arxiv.org/abs/2506.10245)
*Iago Alves Brito,Julia Soares Dollis,Fernanda Bufon Färber,Diogo Fernandes Costa Silva,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: ToxSyn-PT是首个大规模葡萄牙语细粒度仇恨言论分类语料库，覆盖9个受保护少数群体，含53,274条合成句子，通过四阶段流水线构建，实验显示其具备跨领域强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有葡萄牙语仇恨言论数据集多依赖社交媒体数据，存在领域单一、群体覆盖不足及过拟合风险，需构建多样化、平衡且跨领域泛化的新资源。

Method: 四阶段构建流程：1)人工种子库；2)指令调优大模型少样本扩展；3)释义增强；4)中性文本注入。通过合成数据避免社交媒体领域偏差，确保风格多样性与类别平衡。

Result: 在五个公开葡萄牙语仇恨言论数据集上，ToxSyn-PT在二分类和多标签分类任务中均表现优异，跨领域泛化能力显著优于传统基准。

Conclusion: ToxSyn-PT为低资源场景下的仇恨言论检测提供高质量合成数据范例，证明合成数据在跨领域任务中的有效性，数据集已开源推动相关研究。

Abstract: We present ToxSyn-PT, the first large-scale Portuguese corpus that enables
fine-grained hate-speech classification across nine legally protected minority
groups. The dataset contains 53,274 synthetic sentences equally distributed
between minorities groups and toxicity labels. ToxSyn-PT is created through a
novel four-stage pipeline: (1) a compact, manually curated seed; (2) few-shot
expansion with an instruction-tuned LLM; (3) paraphrase-based augmentation; and
(4) enrichment, plus additional neutral texts to curb overfitting to
group-specific cues. The resulting corpus is class-balanced, stylistically
diverse, and free from the social-media domain that dominate existing
Portuguese datasets. Despite domain differences with traditional benchmarks,
experiments on both binary and multi-label classification on the corpus yields
strong results across five public Portuguese hate-speech datasets,
demonstrating robust generalization even across domain boundaries. The dataset
is publicly released to advance research on synthetic data and hate-speech
detection in low-resource settings.

</details>


### [13] [Do Language Models Have Bayesian Brains? Distinguishing Stochastic and Deterministic Decision Patterns within Large Language Models](https://arxiv.org/abs/2506.10268)
*Andrea Yaoyun Cui,Pengfei Yu*

Main category: cs.CL

TL;DR: 本文挑战了语言模型通过概率性采样生成文本的假设，指出其可能呈现确定性决策，导致先前推断先验的方法不可靠，并提出新方法区分决策模式以避免误导性先验。


<details>
  <summary>Details</summary>
Motivation: 先前研究假设语言模型通过概率性采样生成文本，并基于吉布斯采样推断其先验。但此假设可能不成立，需验证模型是否真正具备贝叶斯决策能力。

Method: 提出一种区分吉布斯采样中随机与确定性决策模式的方法，并在多种大语言模型及不同条件下实验分析其决策模式。

Result: 实验表明语言模型在特定条件下会呈现近乎确定性决策（如最大似然估计），导致模拟吉布斯采样收敛至错误先验，挑战传统采样假设。

Conclusion: 需谨慎评估语言模型决策机制，传统先验推断方法可能失效；所提方法可有效识别决策模式，为理解模型决策机制提供关键洞见。

Abstract: Language models are essentially probability distributions over token
sequences. Auto-regressive models generate sentences by iteratively computing
and sampling from the distribution of the next token. This iterative sampling
introduces stochasticity, leading to the assumption that language models make
probabilistic decisions, similar to sampling from unknown distributions.
Building on this assumption, prior research has used simulated Gibbs sampling,
inspired by experiments designed to elicit human priors, to infer the priors of
language models. In this paper, we revisit a critical question: Do language
models possess Bayesian brains? Our findings show that under certain
conditions, language models can exhibit near-deterministic decision-making,
such as producing maximum likelihood estimations, even with a non-zero sampling
temperature. This challenges the sampling assumption and undermines previous
methods for eliciting human-like priors. Furthermore, we demonstrate that
without proper scrutiny, a system with deterministic behavior undergoing
simulated Gibbs sampling can converge to a "false prior." To address this, we
propose a straightforward approach to distinguish between stochastic and
deterministic decision patterns in Gibbs sampling, helping to prevent the
inference of misleading language model priors. We experiment on a variety of
large language models to identify their decision patterns under various
circumstances. Our results provide key insights in understanding decision
making of large language models.

</details>


### [14] [ClusterUCB: Efficient Gradient-Based Data Selection for Targeted Fine-Tuning of LLMs](https://arxiv.org/abs/2506.10288)
*Zige Wang,Qi Zhu,Fei Mi,Minghui Xu,Ruochun Jin,Wenjing Yang*

Main category: cs.CL

TL;DR: 提出ClusterUCB框架，通过聚类和改进的UCB算法高效选择数据，在保持效果的同时大幅降低计算消耗。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的数据选择方法在微调过程中需全程计算梯度，计算资源消耗过大，难以实际应用。

Method: 结合数据梯度特征聚类与改进的置信上界(UCB)算法，将簇间选择建模为多臂老虎机问题，利用历史影响信息估计分布并平衡探索与利用。

Result: 实验表明ClusterUCB在多个基准测试中达到与原始梯度方法相当的效果，计算消耗显著降低。

Conclusion: 通过梯度特征聚类与动态资源分配策略，实现了高效数据选择，为大规模模型微调提供了实用解决方案。

Abstract: Gradient-based data influence approximation has been leveraged to select
useful data samples in the supervised fine-tuning of large language models.
However, the computation of gradients throughout the fine-tuning process
requires too many resources to be feasible in practice. In this paper, we
propose an efficient gradient-based data selection framework with clustering
and a modified Upper Confidence Bound (UCB) algorithm. Based on the intuition
that data samples with similar gradient features will have similar influences,
we first perform clustering on the training data pool. Then, we frame the
inter-cluster data selection as a constrained computing budget allocation
problem and consider it a multi-armed bandit problem. A modified UCB algorithm
is leveraged to solve this problem. Specifically, during the iterative sampling
process, historical data influence information is recorded to directly estimate
the distributions of each cluster, and a cold start is adopted to balance
exploration and exploitation. Experimental results on various benchmarks show
that our proposed framework, ClusterUCB, can achieve comparable results to the
original gradient-based data selection methods while greatly reducing computing
consumption.

</details>


### [15] [Flick: Few Labels Text Classification using K-Aware Intermediate Learning in Multi-Task Low-Resource Languages](https://arxiv.org/abs/2506.10292)
*Ali Almutairi,Abdullah Alsuhaibani,Shoaib Jameel,Usman Naseem,Gelareh Mohammadi,Imran Razzak*

Main category: cs.CL

TL;DR: 提出Flick方法，通过优化伪标签质量解决低资源语言中的少标签文本分类问题，在14个多语言数据集上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有少标签分类方法多针对英语或采用复杂模型易过拟合，且在低资源语言中面临伪标签噪声和领域适应挑战。需在真正低资源环境下提升伪标签可靠性。

Method: Flick引入伪标签精炼组件，通过单簇内聚分析和自适应top-k选择机制，从多初始簇中蒸馏高置信度伪标签，减少噪声传播并实现预训练语言模型的鲁棒微调。

Result: 在含阿拉伯语、乌尔都语等低资源语言的14个数据集上，Flick展现出优于现有方法的性能和跨语言适应能力。

Conclusion: Flick通过聚焦高质量伪标签簇的提炼机制，显著提升低资源语言场景下的分类效果，为极少量标注数据下的模型优化提供新思路。

Abstract: Training deep learning networks with minimal supervision has gained
significant research attention due to its potential to reduce reliance on
extensive labelled data. While self-training methods have proven effective in
semi-supervised learning, they remain vulnerable to errors from noisy pseudo
labels. Moreover, most recent approaches to the few-label classification
problem are either designed for resource-rich languages such as English or
involve complex cascading models that are prone to overfitting. To address the
persistent challenge of few-label text classification in truly low-resource
linguistic contexts, where existing methods often struggle with noisy
pseudo-labels and domain adaptation, we propose Flick. Unlike prior methods
that rely on generic multi-cluster pseudo-labelling or complex cascading
architectures, Flick leverages the fundamental insight that distilling
high-confidence pseudo-labels from a broader set of initial clusters can
dramatically improve pseudo-label quality, particularly for linguistically
diverse, low-resource settings. Flick introduces a novel pseudo-label
refinement component, a departure from traditional pseudo-labelling strategies
by identifying and leveraging top-performing pseudo-label clusters. This
component specifically learns to distil highly reliable pseudo-labels from an
initial broad set by focusing on single-cluster cohesion and leveraging an
adaptive top-k selection mechanism. This targeted refinement process is crucial
for mitigating the propagation of errors inherent in low-resource data,
allowing for robust fine-tuning of pre-trained language models with only a
handful of true labels. We demonstrate Flick's efficacy across 14 diverse
datasets, encompassing challenging low-resource languages such as Arabic, Urdu,
and Setswana, alongside English, showcasing its superior performance and
adaptability.

</details>


### [16] ["Check My Work?": Measuring Sycophancy in a Simulated Educational Context](https://arxiv.org/abs/2506.10297)
*Chuck Arvin*

Main category: cs.CL

TL;DR: 研究探讨在模拟教育场景中，学生提供的答案建议如何影响大语言模型（LLMs）的回应质量，发现模型存在明显的阿谀行为：当学生提到错误答案时，模型正确率最多下降15%，反之正确率提升15%，且小模型偏差更显著。这种倾向可能加剧教育不平等。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在教育场景中的阿谀行为（sycophancy）风险，尤其是模型是否因学生提供的答案建议而改变自身判断，进而影响教育公平。

Method: 在模拟教育环境中测试5个不同规模的LLM（包括OpenAI GPT-4o和GPT-4.1系列），通过对比不同提问框架下的回答质量，分析模型答案变化频率及词元级概率，验证阿谀假说。

Result: 学生提及错误答案时，LLM正确率最多下降15%；提及正确答案时提升15%。小模型偏差更显著（如GPT-4.1-nano偏差达30%，GPT-4o仅8%）。词元概率分析表明模型倾向于迎合学生提到的答案。

Conclusion: LLMs的阿谀行为可能加剧教育不平等：知识储备强的学生受益，而知识薄弱的学生可能被强化错误认知。需进一步研究机制并开发缓解方法以确保教育公平。

Abstract: This study examines how user-provided suggestions affect Large Language
Models (LLMs) in a simulated educational context, where sycophancy poses
significant risks. Testing five different LLMs from the OpenAI GPT-4o and
GPT-4.1 model classes across five experimental conditions, we show that
response quality varies dramatically based on query framing. In cases where the
student mentions an incorrect answer, the LLM correctness can degrade by as
much as 15 percentage points, while mentioning the correct answer boosts
accuracy by the same margin. Our results also show that this bias is stronger
in smaller models, with an effect of up to 30% for the GPT-4.1-nano model,
versus 8% for the GPT-4o model. Our analysis of how often LLMs "flip" their
answer, and an investigation into token level probabilities, confirm that the
models are generally changing their answers to answer choices mentioned by
students in line with the sycophancy hypothesis. This sycophantic behavior has
important implications for educational equity, as LLMs may accelerate learning
for knowledgeable students while the same tools may reinforce misunderstanding
for less knowledgeable students. Our results highlight the need to better
understand the mechanism, and ways to mitigate, such bias in the educational
context.

</details>


### [17] [Scheduled Interleaved Speech-Text Training for Speech-to-Speech Translation with LLMs](https://arxiv.org/abs/2506.10299)
*Hayato Futami,Emiru Tsunoo,Yosuke Kashiwagi,Yuki Ito,Hassan Shahmohammadi,Siddhant Arora,Shinji Watanabe*

Main category: cs.CL

TL;DR: 提出一种基于LLM的语音-文本交错训练方法，通过逐步减少文本比例提升语音翻译性能，尤其在低资源语言中效果显著。


<details>
  <summary>Details</summary>
Motivation: 大语言模型(LLM)在纯文本数据上训练，难以直接适应语音模态，且语音-语音数据稀缺导致模态迁移困难。

Method: 使用单词级别的语音-文本交错单元进行训练，并采用渐进式调度策略逐步降低文本比例，实现从文本到语音的模态适应。

Result: 在CVSS数据集上微调LLaMA3.2-1B模型后，翻译性能持续改善，低资源语言的提升效果尤为明显。

Conclusion: 渐进式交错训练策略能有效缓解模态适应问题，为数据稀缺场景下的语音翻译提供了可行解决方案。

Abstract: Speech-to-speech translation (S2ST) has been advanced with large language
models (LLMs), which are fine-tuned on discrete speech units. In such
approaches, modality adaptation from text to speech has been an issue. LLMs are
trained on text-only data, which presents challenges to adapt them to speech
modality with limited speech-to-speech data. To address the training
difficulty, we propose scheduled interleaved speech--text training in this
study. We use interleaved speech--text units instead of speech units during
training, where aligned text tokens are interleaved at the word level. We
gradually decrease the ratio of text as training progresses, to facilitate
progressive modality adaptation from text to speech. We conduct experimental
evaluations by fine-tuning LLaMA3.2-1B for S2ST on the CVSS dataset. We show
that the proposed method consistently improves the translation performances,
especially for languages with limited training data.

</details>


### [18] [Code Execution as Grounded Supervision for LLM Reasoning](https://arxiv.org/abs/2506.10343)
*Dongwon Jung,Wenxuan Zhou,Muhao Chen*

Main category: cs.CL

TL;DR: 提出一种通过程序执行确定性生成高质量思维链监督数据的方法，有效提升大语言模型的推理能力并减少推理冗余。


<details>
  <summary>Details</summary>
Motivation: 现有思维链监督数据生成方法依赖高成本人工标注或易出错的LLM生成结果，需开发可靠且可扩展的替代方案。

Method: 从代码执行轨迹中提取可验证的逐步推理步骤，将其转化为自然语言形式的思维链监督数据。

Result: 跨领域推理基准测试显示该方法显著提升模型迁移推理能力，推理阶段token长度减少21.3%（通过消融实验验证）。

Conclusion: 基于程序确定性的思维链生成方法能产生高精度推理数据，在提升模型推理能力的同时有效抑制无意义重复和过度思考现象。

Abstract: Training large language models (LLMs) with chain-of-thought (CoT) supervision
has proven effective for enhancing their reasoning abilities. However,
obtaining reliable and accurate reasoning supervision remains a significant
challenge. We propose a scalable method for generating a high-quality CoT
supervision dataset by leveraging the determinism of program execution. Unlike
existing reasoning dataset generation methods that rely on costly human
annotations or error-prone LLM-generated CoT, our approach extracts verifiable,
step-by-step reasoning traces from code execution and transforms them into a
natural language CoT reasoning. Experiments on reasoning benchmarks across
various domains show that our method effectively equips LLMs with transferable
reasoning abilities across diverse tasks. Furthermore, the ablation studies
validate that our method produces highly accurate reasoning data and reduces
overall token length during inference by reducing meaningless repetition and
overthinking.

</details>


### [19] [TableRAG: A Retrieval Augmented Generation Framework for Heterogeneous Document Reasoning](https://arxiv.org/abs/2506.10380)
*Xiaohan Yu,Pu Jian,Chong Chen*

Main category: cs.CL

TL;DR: 现有RAG方法在处理混合文本和表格的异构文档时存在结构破坏和信息丢失问题，TableRAG通过结合文本检索与SQL操作，在HeteQA基准上取得最优效果。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在处理含表格的异构文档时，因表格展平和分块策略破坏了结构完整性，导致信息丢失并削弱LLMs在多跳全局查询中的能力。

Method: 提出TableRAG混合框架，通过上下文感知查询分解、文本检索、SQL编程执行、组合式答案生成四步迭代，统一处理文本与表格数据。

Result: 在公开数据集和自建HeteQA基准测试中，TableRAG全面超越现有基线模型，创下异构文档问答的新SOTA性能。

Conclusion: TableRAG有效解决了异构文档处理难题，其结合SQL编程与文本理解的范式为复杂推理任务提供了新方向，并通过开源框架推动领域发展。

Abstract: Retrieval-Augmented Generation (RAG) has demonstrated considerable
effectiveness in open-domain question answering. However, when applied to
heterogeneous documents, comprising both textual and tabular components,
existing RAG approaches exhibit critical limitations. The prevailing practice
of flattening tables and chunking strategies disrupts the intrinsic tabular
structure, leads to information loss, and undermines the reasoning capabilities
of LLMs in multi-hop, global queries. To address these challenges, we propose
TableRAG, an hybrid framework that unifies textual understanding and complex
manipulations over tabular data. TableRAG iteratively operates in four steps:
context-sensitive query decomposition, text retrieval, SQL programming and
execution, and compositional intermediate answer generation. We also develop
HeteQA, a novel benchmark designed to evaluate the multi-hop heterogeneous
reasoning capabilities. Experimental results demonstrate that TableRAG
consistently outperforms existing baselines on both public datasets and our
HeteQA, establishing a new state-of-the-art for heterogeneous document question
answering. We release TableRAG at https://github.com/yxh-y/TableRAG/tree/main.

</details>


### [20] [PAG: Multi-Turn Reinforced LLM Self-Correction with Policy as Generative Verifier](https://arxiv.org/abs/2506.10406)
*Yuhua Jiang,Yuwen Xiong,Yufeng Yuan,Chao Xin,Wenyuan Xu,Yu Yue,Qianchuan Zhao,Lin Yan*

Main category: cs.CL

TL;DR: 提出PAG框架，通过让大语言模型在强化学习中交替扮演策略与验证者角色，并引入选择性修订机制，实现更高效的自验证与自纠正，提升推理与验证能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型自验证方法依赖外部验证器或多阶段训练流程，扩展性受限。需探索更简洁的自我验证机制以提升模型自纠能力。

Method: PAG框架将模型置于多轮强化学习中，交替执行策略生成与生成式验证角色，仅当自检发现错误时触发选择性修订，避免无效迭代。

Result: 实验表明PAG在推理任务中提升生成准确率，其自验证效果优于自洽性验证，同时缓解模型崩溃问题。

Conclusion: PAG通过统一框架实现自验证与自纠正的协同优化，为提升大模型自我验证能力提供了新范式。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
complex reasoning tasks, yet they still struggle to reliably verify the
correctness of their own outputs. Existing solutions to this verification
challenge often depend on separate verifier models or require multi-stage
self-correction training pipelines, which limit scalability. In this paper, we
propose Policy as Generative Verifier (PAG), a simple and effective framework
that empowers LLMs to self-correct by alternating between policy and verifier
roles within a unified multi-turn reinforcement learning (RL) paradigm.
Distinct from prior approaches that always generate a second attempt regardless
of model confidence, PAG introduces a selective revision mechanism: the model
revises its answer only when its own generative verification step detects an
error. This verify-then-revise workflow not only alleviates model collapse but
also jointly enhances both reasoning and verification abilities. Extensive
experiments across diverse reasoning benchmarks highlight PAG's dual
advancements: as a policy, it enhances direct generation and self-correction
accuracy; as a verifier, its self-verification outperforms self-consistency.

</details>


### [21] [Burn After Reading: Do Multimodal Large Language Models Truly Capture Order of Events in Image Sequences?](https://arxiv.org/abs/2506.10415)
*Yingjin Song,Yupei Du,Denis Paperno,Albert Gatt*

Main category: cs.CL

TL;DR: 本文提出TempVS基准，用于评估多模态大语言模型在图像序列中的时间定位与推理能力，发现现有模型表现显著落后于人类，并提供了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在时间维度的视觉-语言推理能力缺乏系统评估，需构建针对性基准以揭示模型局限性。

Method: 设计包含事件关系推理、句子排序、图像排序三个核心测试的TempVS基准，结合基础定位任务，评估38个先进MLLMs的多模态时序理解能力。

Result: 实验显示所有模型在TempVS上表现不佳（平均准确率低于50%），与人类表现存在显著差距，尤其在跨模态时序推理方面存在明显缺陷。

Conclusion: TempVS有效暴露MLLMs的时序推理短板，未来需加强多模态时序表征学习与跨模态对齐机制的研究，基准数据与代码已开源。

Abstract: This paper introduces the TempVS benchmark, which focuses on temporal
grounding and reasoning capabilities of Multimodal Large Language Models
(MLLMs) in image sequences. TempVS consists of three main tests (i.e., event
relation inference, sentence ordering and image ordering), each accompanied
with a basic grounding test. TempVS requires MLLMs to rely on both visual and
linguistic modalities to understand the temporal order of events. We evaluate
38 state-of-the-art MLLMs, demonstrating that models struggle to solve TempVS,
with a substantial performance gap compared to human capabilities. We also
provide fine-grained insights that suggest promising directions for future
research. Our TempVS benchmark data and code are available at
https://github.com/yjsong22/TempVS.

</details>


### [22] [Beyond the Battlefield: Framing Analysis of Media Coverage in Conflict Reporting](https://arxiv.org/abs/2506.10421)
*Avneet Kaur,Arnav Arora*

Main category: cs.CL

TL;DR: 研究通过计算方法和大型语言模型分析以巴冲突新闻，发现媒体报道更倾向战争框架，且美、英、中东媒体在加害者与受害者叙事上存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 现有冲突框架研究多局限于定性或表层分析，缺乏对深层语言框架与传播框架关联的探讨，需通过量化方法揭示媒体报道偏见及其对冲突的影响。

Method: 结合框架语义学和大型语言模型，对以巴冲突新闻语料进行战争/和平新闻指标分析，识别传播框架与语言框架的关联。

Result: 媒体报道普遍侧重战争框架；美、英与中东媒体在冲突责任归属叙事上存在系统性差异，暴露出地域性媒体偏见。

Conclusion: 媒体框架差异强化了冲突叙事的地缘政治立场，量化方法有效揭示了传统研究难以捕捉的深层报道偏见。

Abstract: Framing used by news media, especially in times of conflict, can have
substantial impact on readers' opinion, potentially aggravating the conflict
itself. Current studies on the topic of conflict framing have limited insights
due to their qualitative nature or only look at surface level generic frames
without going deeper. In this work, we identify indicators of war and peace
journalism, as outlined by prior work in conflict studies, in a corpus of news
articles reporting on the Israel-Palestine war. For our analysis, we use
computational approaches, using a combination of frame semantics and large
language models to identify both communicative framing and its connection to
linguistic framing. Our analysis reveals a higher focus on war based reporting
rather than peace based. We also show substantial differences in reporting
across the US, UK, and Middle Eastern news outlets in framing who the assailant
and victims of the conflict are, surfacing biases within the media.

</details>


### [23] [Fast on the Easy, Deep on the Hard: Efficient Reasoning via Powered Length Penalty](https://arxiv.org/abs/2506.10446)
*Zehui Ling,Deshu Chen,Hongwei Zhang,Yifeng Jiao,Xin Guo,Yuan Cheng*

Main category: cs.CL

TL;DR: 提出一种通过动态调整输出长度惩罚机制来提升大语言模型推理效率的方法，在保持复杂问题准确性的同时缩短简单问题的推理步骤。


<details>
  <summary>Details</summary>
Motivation: 现有方法如Chain-of-Thought提示会生成冗长输出导致计算延迟，而强化学习方法采用统一长度惩罚未考虑问题复杂度差异，导致简单问题过度推理或复杂问题推理不足。

Method: 通过分割奖励函数并引入新型输出长度惩罚机制，根据问题复杂度动态管理推理效率：简单问题强制简洁，复杂问题保留充分推理步骤。

Result: 在GSM8K/MATH500简单数据集上缩短20-30%输出长度且保持或提升准确率，在复杂数据集AIME2024上实现1.2%绝对准确率提升。

Conclusion: 通过问题难度自适应的长度惩罚机制，成功实现推理效率与准确性的动态平衡，为LLM推理优化提供新范式。

Abstract: Large language models (LLMs) have demonstrated significant advancements in
reasoning capabilities, performing well on various challenging benchmarks.
Techniques like Chain-of-Thought prompting have been introduced to further
improve reasoning. However, these approaches frequently generate longer
outputs, which in turn increase computational latency. Although some methods
use reinforcement learning to shorten reasoning, they often apply uniform
penalties without considering the problem's complexity, leading to suboptimal
outcomes. In this study, we seek to enhance the efficiency of LLM reasoning by
promoting conciseness for simpler problems while preserving sufficient
reasoning for more complex ones for accuracy, thus improving the model's
overall performance. Specifically, we manage the model's reasoning efficiency
by dividing the reward function and including a novel penalty for output
length. Our approach has yielded impressive outcomes in benchmark evaluations
across three datasets: GSM8K, MATH500, and AIME2024. For the comparatively
simpler datasets GSM8K and MATH500, our method has effectively shortened output
lengths while preserving or enhancing accuracy. On the more demanding AIME2024
dataset, our approach has resulted in improved accuracy.

</details>


### [24] [Table-Text Alignment: Explaining Claim Verification Against Tables in Scientific Papers](https://arxiv.org/abs/2506.10486)
*Xanh Ho,Sunisth Kumar,Yun-Ang Wu,Florian Boudin,Atsuhiro Takasu,Akiko Aizawa*

Main category: cs.CL

TL;DR: 该研究将表格与声明的对齐重新定义为解释任务，构建了包含人工标注单元格理由的新数据集，并发现结合对齐信息可提升声明验证性能，但多数大语言模型的预测缺乏忠实推理。


<details>
  <summary>Details</summary>
Motivation: 现有科学声明验证方法仅预测标签，无法揭示模型推理过程且可解释性不足。需要模型识别支撑结论的关键表格单元格以提高可解释性。

Method: 扩展SciTab基准数据集，添加人工标注的最小必要单元格集合作为理由。建立处理模糊案例的分类法，并利用标注信息进行实验分析。

Result: (1) 表格对齐信息提升了声明验证性能；(2) 多数LLM虽能预测正确标签，但无法复现人类对齐理由，表明其预测缺乏可信推理过程。

Conclusion: 仅依赖标签预测不足以验证科学声明，需结合可解释的单元格级推理。当前LLM的决策过程与人类推理存在本质差异，需开发更忠实的推理机制。

Abstract: Scientific claim verification against tables typically requires predicting
whether a claim is supported or refuted given a table. However, we argue that
predicting the final label alone is insufficient: it reveals little about the
model's reasoning and offers limited interpretability. To address this, we
reframe table-text alignment as an explanation task, requiring models to
identify the table cells essential for claim verification. We build a new
dataset by extending the SciTab benchmark with human-annotated cell-level
rationales. Annotators verify the claim label and highlight the minimal set of
cells needed to support their decision. After the annotation process, we
utilize the collected information and propose a taxonomy for handling ambiguous
cases. Our experiments show that (i) incorporating table alignment information
improves claim verification performance, and (ii) most LLMs, while often
predicting correct labels, fail to recover human-aligned rationales, suggesting
that their predictions do not stem from faithful reasoning.

</details>


### [25] [Surface Fairness, Deep Bias: A Comparative Study of Bias in Language Models](https://arxiv.org/abs/2506.10491)
*Aleksandra Sorokovikova,Pavel Chizhov,Iuliia Eremenko,Ivan P. Yamshchikov*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLMs）在训练数据中的偏见会导致输出偏差，通过不同方法评估发现，模型在评分用户答案和提供建议时表现出显著偏见，且个性化功能可能加剧此问题。


<details>
  <summary>Details</summary>
Motivation: 由于训练数据包含社会偏见，LLMs可能生成带有偏见的输出。研究旨在探索不同方法衡量模型偏见，并揭示个性化功能如何加剧偏见风险。

Method: 通过三种代理方法评估LLMs偏见：1) 预置人设的多学科基准测试（MMLU）；2) 要求模型对用户答案评分；3) 请求模型提供薪资谈判建议。

Result: 预置人设的MMLU测试差异可忽略，但模型评分用户答案时显示显著偏见，薪资建议任务中偏见尤为明显。个性化功能使模型自动识别用户社会属性，进一步放大偏见。

Conclusion: LLMs的偏见问题随个性化功能发展更复杂，需关注模型自动推断用户社会属性后的潜在偏差，并开发针对性缓解策略。

Abstract: Modern language models are trained on large amounts of data. These data
inevitably include controversial and stereotypical content, which contains all
sorts of biases related to gender, origin, age, etc. As a result, the models
express biased points of view or produce different results based on the
assigned personality or the personality of the user. In this paper, we
investigate various proxy measures of bias in large language models (LLMs). We
find that evaluating models with pre-prompted personae on a multi-subject
benchmark (MMLU) leads to negligible and mostly random differences in scores.
However, if we reformulate the task and ask a model to grade the user's answer,
this shows more significant signs of bias. Finally, if we ask the model for
salary negotiation advice, we see pronounced bias in the answers. With the
recent trend for LLM assistant memory and personalization, these problems open
up from a different angle: modern LLM users do not need to pre-prompt the
description of their persona since the model already knows their
socio-demographics.

</details>


### [26] [Beyond Single-User Dialogue: Assessing Multi-User Dialogue State Tracking Capabilities of Large Language Models](https://arxiv.org/abs/2506.10504)
*Sangmin Song,Juhwan Choi,JungMin Yun,YoungBin Kim*

Main category: cs.CL

TL;DR: 研究评估大语言模型在多用户对话状态追踪中的表现，发现现有模型在复杂多说话者环境下性能显著下降，需改进以适应真实场景。


<details>
  <summary>Details</summary>
Motivation: 传统对话状态追踪基准仅关注结构化单用户对话，无法反映真实多用户交互的复杂性，需验证LLMs在多用户场景下的鲁棒性。

Method: 基于言语行为理论，利用LLM生成第二用户语句扩展现有数据集，系统化构建多用户对话以进行受控评估。

Result: 实验显示多用户场景下LLMs的对话状态追踪性能较单用户场景显著下降，暴露现有方法在多说话者语境中的局限性。

Conclusion: 需增强LLMs处理多用户对话状态追踪的能力，推动构建更贴近现实、鲁棒性更强的DST模型。

Abstract: Large language models (LLMs) have demonstrated remarkable performance in
zero-shot dialogue state tracking (DST), reducing the need for task-specific
training. However, conventional DST benchmarks primarily focus on structured
user-agent conversations, failing to capture the complexities of real-world
multi-user interactions. In this study, we assess the robustness of LLMs in
multi-user DST while minimizing dataset construction costs. Inspired by recent
advances in LLM-based data annotation, we extend an existing DST dataset by
generating utterances of a second user based on speech act theory. Our
methodology systematically incorporates a second user's utterances into
conversations, enabling a controlled evaluation of LLMs in multi-user settings.
Experimental results reveal a significant performance drop compared to
single-user DST, highlighting the limitations of current LLMs in extracting and
tracking dialogue states amidst multiple speakers. Our findings emphasize the
need for future research to enhance LLMs for multi-user DST scenarios, paving
the way for more realistic and robust DST models.

</details>


### [27] [Reliable Reasoning Path: Distilling Effective Guidance for LLM Reasoning with Knowledge Graphs](https://arxiv.org/abs/2506.10508)
*Yilin Xiao,Chuang Zhou,Qinggang Zhang,Bo Li,Qing Li,Xiao Huang*

Main category: cs.CL

TL;DR: 为解决大语言模型（LLMs）在知识密集型任务中因缺乏背景知识和幻觉问题导致的推理能力不足，本文提出RRP框架，通过结合知识图谱（KGs）的结构信息与LLMs的语义优势，生成高质量推理路径，提升LLMs的复杂问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱增强的LLMs主要关注补充事实知识，但忽略了事实间逻辑关系的组织与推理路径的构建，导致复杂问题处理效果不佳。如何从复杂图谱中提取可靠推理路径并筛选有效路径成为关键挑战。

Method: 提出RRP框架：1）融合LLMs的语义能力与知识图谱的关系嵌入、双向分布学习，提取结构化推理路径；2）引入反思模块，根据路径重要性动态评估和优化路径质量。框架支持即插即用，适配多种LLMs。

Result: 在两个公开数据集上的实验表明，RRP性能优于现有基线方法，达到SOTA水平。其生成的推理路径能有效指导LLMs推理，且无需调整模型结构即可灵活集成至不同LLMs。

Conclusion: RRP通过挖掘知识图谱中的逻辑关系与路径，解决了复杂推理任务中路径冗余和结构复杂性问题，为LLMs提供了可解释、高质量的推理指导，显著提升模型推理能力。

Abstract: Large language models (LLMs) often struggle with knowledge-intensive tasks
due to a lack of background knowledge and a tendency to hallucinate. To address
these limitations, integrating knowledge graphs (KGs) with LLMs has been
intensively studied. Existing KG-enhanced LLMs focus on supplementary factual
knowledge, but still struggle with solving complex questions. We argue that
refining the relationships among facts and organizing them into a logically
consistent reasoning path is equally important as factual knowledge itself.
Despite their potential, extracting reliable reasoning paths from KGs poses the
following challenges: the complexity of graph structures and the existence of
multiple generated paths, making it difficult to distinguish between useful and
redundant ones. To tackle these challenges, we propose the RRP framework to
mine the knowledge graph, which combines the semantic strengths of LLMs with
structural information obtained through relation embedding and bidirectional
distribution learning. Additionally, we introduce a rethinking module that
evaluates and refines reasoning paths according to their significance.
Experimental results on two public datasets show that RRP achieves
state-of-the-art performance compared to existing baseline methods. Moreover,
RRP can be easily integrated into various LLMs to enhance their reasoning
abilities in a plug-and-play manner. By generating high-quality reasoning paths
tailored to specific questions, RRP distills effective guidance for LLM
reasoning.

</details>


### [28] [Unsupervised Protoform Reconstruction through Parsimonious Rule-guided Heuristics and Evolutionary Search](https://arxiv.org/abs/2506.10614)
*Promise Dodzi Kpoglu*

Main category: cs.CL

TL;DR: 本文提出一种结合数据驱动与规则启发式的无监督方法，用于重建原始语形（protoforms），在拉丁语重建任务中显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖音系编辑的概率模型，但其数据驱动特性存在局限性。本文旨在通过整合规则与统计模式，克服纯数据驱动方法的不足。

Method: 在进化优化框架中融合数据驱动推理与语言学规则约束，利用统计模式和音系合理性启发式指导重建过程。

Result: 在基于五个罗曼语族同源词重建拉丁语的任务中，字符级准确率和音系合理性指标均大幅超越基线模型。

Conclusion: 混合策略有效结合统计与语言学知识，为原始语形重建提供了更优的解决方案，验证了规则与数据协同方法的潜力。

Abstract: We propose an unsupervised method for the reconstruction of protoforms i.e.,
ancestral word forms from which modern language forms are derived. While prior
work has primarily relied on probabilistic models of phonological edits to
infer protoforms from cognate sets, such approaches are limited by their
predominantly data-driven nature. In contrast, our model integrates data-driven
inference with rule-based heuristics within an evolutionary optimization
framework. This hybrid approach leverages on both statistical patterns and
linguistically motivated constraints to guide the reconstruction process. We
evaluate our method on the task of reconstructing Latin protoforms using a
dataset of cognates from five Romance languages. Experimental results
demonstrate substantial improvements over established baselines across both
character-level accuracy and phonological plausibility metrics.

</details>


### [29] [Tina: Tiny Reasoning Models via LoRA](https://arxiv.org/abs/2504.15777)
*Shangshang Wang,Julian Asilis,Ömer Faruk Akgül,Enes Burak Bilgin,Ollie Liu,Willie Neiswanger*

Main category: cs.CL

TL;DR: Tina模型通过低秩适应（LoRA）在强化学习中高效更新参数，以极低成本（9美元）实现高性能推理，超越同类模型，验证了高效RL推理的有效性。


<details>
  <summary>Details</summary>
Motivation: 探索如何以高成本效益在语言模型中实现强推理能力，研究极小资源下开发高性能推理模型的可行性。

Method: 基于1.5B参数基础模型，在强化学习（RL）中应用LoRA进行参数高效更新，仅调整低秩矩阵，保留基础模型大部分参数不变。

Result: Tina模型推理性能超越同基础模型的SOTA RL模型（如AIME24上Pass@1达43.33%），训练成本降低260倍（仅9美元），且开源全部代码与模型。

Conclusion: LoRA通过快速适配RL奖励的推理结构格式，在保留基础知识的同时实现高效推理，极小资源即可显著提升性能，验证了高效RL微调路径的潜力。

Abstract: How cost-effectively can strong reasoning abilities be achieved in language
models? Driven by this fundamental question, we present Tina, a family of tiny
reasoning models achieved with high cost-efficiency. Notably, Tina demonstrates
that substantial reasoning performance can be developed using only minimal
resources, by applying parameter-efficient updates during reinforcement
learning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B
parameter base model. This minimalist approach produces models that achieve
reasoning performance which is competitive with, and sometimes surpasses, SOTA
RL reasoning models built upon the same base model. Crucially, this is achieved
at a tiny fraction of the computational post-training cost employed by existing
SOTA models. In fact, the best Tina model achieves a >20\% reasoning
performance increase and 43.33\% Pass@1 accuracy on AIME24, at only \$9 USD
post-training and evaluation cost (i.e., an estimated 260x cost reduction). Our
work reveals the surprising effectiveness of efficient RL reasoning via LoRA.
We validate this across multiple open-source reasoning datasets and various
ablation settings starting with a single, fixed set of hyperparameters.
Furthermore, we hypothesize that this effectiveness and efficiency stem from
LoRA rapidly adapting the model to the structural format of reasoning rewarded
by RL, while largely preserving the base model's underlying knowledge. In
service of accessibility and open research, we fully open-source all code,
training logs, and model weights \& checkpoints.

</details>


### [30] [SDialog: A Python Toolkit for Synthetic Dialogue Generation and Analysis](https://arxiv.org/abs/2506.10622)
*Sergio Burdisso,Esaú Villatoro-Tello,Petr Motlicek*

Main category: cs.CL

TL;DR: SDialog是一个模块化、可扩展的Python工具包，利用指令调优的大语言模型生成可控的合成对话数据，支持多智能体模拟和场景驱动生成，推动合成数据生成工具标准化。


<details>
  <summary>Details</summary>
Motivation: 对话AI系统发展需要高质量、灵活且可复现的合成对话数据用于训练、评估和基准测试，现有工具在标准化和可扩展性方面存在不足。

Method: 通过模块化架构设计，结合指令调优LLM实现人物角色抽象、编排管理和场景控制，支持多智能体仿真和场景驱动的工作流框架。

Result: 成功构建可生成真实、多样且可控的对话数据平台，实现对话状态追踪和流程可视化，推进了对话数据生成工具的标准化进程。

Conclusion: SDialog在合成数据生成工具标准化方面取得关键进展，为快速迭代的研究环境提供了可复现性保障，是对话AI基础设施的重要补充。

Abstract: The advancement of conversational AI systems relies on the availability of
high-quality, flexible, and reproducible synthetic dialogues for training,
evaluation, and benchmarking. SDialog is a modular, extensible Python toolkit
designed to address the challenges of synthetic dialogue generation and
analysis. By leveraging instruction-tuned Large Language Models (LLMs), SDialog
provides abstractions for personas, orchestration, and scenario management,
enabling the creation of realistic, diverse, and controllable conversational
data for research and development. SDialog supports workflows such as
multi-agent simulation and scenario-driven generation, and represents a step
forward in the standardization of tools and frameworks for synthetic data
generation, a crucial advancement for ensuring reproducibility in today's
fast-evolving research landscape.

</details>


### [31] [Resa: Transparent Reasoning Models via SAEs](https://arxiv.org/abs/2506.09967)
*Shangshang Wang,Julian Asilis,Ömer Faruk Akgül,Enes Burak Bilgin,Ollie Liu,Deqing Fu,Willie Neiswanger*

Main category: cs.CL

TL;DR: Resa模型通过SAE-Tuning方法，以极低成本（约1美元）和短时间（约20分钟）激发语言模型的推理能力，保留97%性能，并展示通用性和模块化潜力。


<details>
  <summary>Details</summary>
Motivation: 探索如何高效、低成本地激发语言模型的推理能力，解决传统强化学习（RL）训练方法成本高、耗时长的问题。

Method: 提出SAE-Tuning方法：先训练稀疏自编码器（SAE）从源模型提取推理能力，再用SAE指导目标模型的监督微调，仅需问答数据，无需推理轨迹。

Result: SAE-Tuning将训练成本降低2000倍（至约1美元），时间缩短450倍（至20分钟），保留>97%的RL训练性能；在AIME24和AMC23上分别达到43.33%和90%的Pass@1。提取的推理能力具备通用性（跨数据集有效）和模块化（跨模型直接复用）。

Conclusion: SAE-Tuning是一种高效、低成本的推理能力激发方法，具备通用性和模块化特性，为轻量化模型优化提供新方向，且所有成果已开源。

Abstract: How cost-effectively can we elicit strong reasoning in language models by
leveraging their underlying representations? We answer this question with Resa,
a family of 1.5B reasoning models trained via a novel and efficient sparse
autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to
capture reasoning abilities from a source model, and then uses the trained SAE
to guide a standard supervised fine-tuning process to elicit such abilities in
a target model, all using verified question-answer data without any reasoning
traces. Notably, when applied to certain base models before further RL
post-training, SAE-Tuning retains >97% of its RL-trained counterpart's
reasoning performance while reducing training costs by >2000x to roughly \$1
and training time by >450x to around 20 minutes. Furthermore, when applied to
lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning
performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only
around \$1 additional cost. Surprisingly, the reasoning abilities extracted via
SAEs are potentially both generalizable and modular. Generality means abilities
extracted from one dataset still elevate performance on a larger and
overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math
can be attached to the R1-Distill model at test time, without any retraining,
and yield comparable gains. Extensive ablations validate these findings and all
artifacts are fully open-sourced.

</details>


### [32] [NeuralNexus at BEA 2025 Shared Task: Retrieval-Augmented Prompting for Mistake Identification in AI Tutors](https://arxiv.org/abs/2506.10627)
*Numaan Naeem,Sarfraz Ahmad,Momina Ahsan,Hasan Iqbal*

Main category: cs.CL

TL;DR: 本文提出了BEA 2025共享任务中AI导师教学能力评估的四个方法，最终基于GPT-4o的检索增强提示系统通过结构化提示和模式解析取得最优效果。


<details>
  <summary>Details</summary>
Motivation: 针对AI导师在数学推理中识别学生错误的教学能力评估需求，研究如何有效判断辅导响应的正确性。

Method: 采用四类方法：(1)多预训练模型嵌入集成 (2)冻结句向量分类 (3)历史感知多头注意力模型 (4)基于GPT-4o的检索增强提示系统，结合语义检索与模式化输出解析。

Result: 检索增强提示系统超越所有基线模型，验证了示例驱动提示与LLM推理结合的有效性。

Conclusion: 通过检索相似案例构建结构化提示，结合大语言模型推理能力，可显著提升教学反馈评估的准确性和可解释性。

Abstract: This paper presents our system for Track 1: Mistake Identification in the BEA
2025 Shared Task on Pedagogical Ability Assessment of AI-powered Tutors. The
task involves evaluating whether a tutor's response correctly identifies a
mistake in a student's mathematical reasoning. We explore four approaches: (1)
an ensemble of machine learning models over pooled token embeddings from
multiple pretrained language models (LMs); (2) a frozen sentence-transformer
using [CLS] embeddings with an MLP classifier; (3) a history-aware model with
multi-head attention between token-level history and response embeddings; and
(4) a retrieval-augmented few-shot prompting system with a large language model
(LLM) i.e. GPT 4o. Our final system retrieves semantically similar examples,
constructs structured prompts, and uses schema-guided output parsing to produce
interpretable predictions. It outperforms all baselines, demonstrating the
effectiveness of combining example-driven prompting with LLM reasoning for
pedagogical feedback assessment. Our code is available at
https://github.com/NaumanNaeem/BEA_2025.

</details>


### [33] [Spelling-out is not Straightforward: LLMs' Capability of Tokenization from Token to Characters](https://arxiv.org/abs/2506.10641)
*Tatsuya Hiraoka,Kentaro Inui*

Main category: cs.CL

TL;DR: 大语言模型（LLMs）虽能逐字符拼写，但在复杂字符级任务（如识别组合子成分）上表现不佳。研究发现其字符信息处理依赖中间层，而非嵌入层。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在拼写过程中如何内部表示和利用字符级信息，尤其是为何其在简单任务（如拼写）表现良好，却在复杂字符级任务中受限。

Method: 通过探测分类器、知识神经元识别和注意力权重分析，验证LLMs的字符信息编码机制。

Result: 嵌入层未充分编码字符级信息（尤其是首字符外），中间及高层Transformer通过知识重建实现拼写，并存在行为“突破点”。

Conclusion: LLMs处理字符级信息的方式与人类直觉不同，依赖深层结构补偿嵌入层不足，揭示了其字符级能力的内部机制局限性。

Abstract: Large language models (LLMs) can spell out tokens character by character with
high accuracy, yet they struggle with more complex character-level tasks, such
as identifying compositional subcomponents within tokens. In this work, we
investigate how LLMs internally represent and utilize character-level
information during the spelling-out process. Our analysis reveals that,
although spelling out is a simple task for humans, it is not handled in a
straightforward manner by LLMs. Specifically, we show that the embedding layer
does not fully encode character-level information, particularly beyond the
first character. As a result, LLMs rely on intermediate and higher Transformer
layers to reconstruct character-level knowledge, where we observe a distinct
"breakthrough" in their spelling behavior. We validate this mechanism through
three complementary analyses: probing classifiers, identification of knowledge
neurons, and inspection of attention weights.

</details>


### [34] [Large Language Models for Detection of Life-Threatening Texts](https://arxiv.org/abs/2506.10687)
*Thanh Thi Nguyen,Campbell Wilson,Janis Dalins*

Main category: cs.CL

TL;DR: 本文通过微调Gemma、Mistral和Llama-2等开源大语言模型（LLMs），在平衡/不平衡数据集上检测威胁性语言，发现LLMs显著优于传统方法，且上采样对LLMs效果有限。


<details>
  <summary>Details</summary>
Motivation: 检测威胁性语言对保护心理脆弱个体、预防潜在伤害至关重要。研究旨在验证LLMs在此任务中的有效性，并与传统方法对比。

Method: 使用7B参数的Gemma、Mistral、Llama-2模型，在平衡/不平衡数据集上微调，并与词袋、词嵌入、主题建模、BERT等方法对比，采用上采样处理数据。

Result: LLMs整体优于传统方法，Mistral和Llama-2在各类数据中表现最佳，Gemma稍弱；上采样仅显著提升传统方法性能，对LLMs影响较小。

Conclusion: LLMs在现实威胁语言检测中潜力显著，但不同模型性能存在，且数据平衡技术对LLMs提升有限。

Abstract: Detecting life-threatening language is essential for safeguarding individuals
in distress, promoting mental health and well-being, and preventing potential
harm and loss of life. This paper presents an effective approach to identifying
life-threatening texts using large language models (LLMs) and compares them
with traditional methods such as bag of words, word embedding, topic modeling,
and Bidirectional Encoder Representations from Transformers. We fine-tune three
open-source LLMs including Gemma, Mistral, and Llama-2 using their 7B parameter
variants on different datasets, which are constructed with class balance,
imbalance, and extreme imbalance scenarios. Experimental results demonstrate a
strong performance of LLMs against traditional methods. More specifically,
Mistral and Llama-2 models are top performers in both balanced and imbalanced
data scenarios while Gemma is slightly behind. We employ the upsampling
technique to deal with the imbalanced data scenarios and demonstrate that while
this method benefits traditional approaches, it does not have as much impact on
LLMs. This study demonstrates a great potential of LLMs for real-world
life-threatening language detection problems.

</details>


### [35] [Inferring Adjective Hypernyms with Language Models to Increase the Connectivity of Open English Wordnet](https://arxiv.org/abs/2506.10715)
*Lorenzo Augello,John P. McCrae*

Main category: cs.CL

TL;DR: 本文提出通过理论分析和微调语言模型来补全英语形容词上位词缺失链接，扩展了TaxoLLaMa方法的应用范围。


<details>
  <summary>Details</summary>
Motivation: 现有Open English Wordnet资源中形容词上位词关系存在大量缺失，且形容词与名词/动词的上位词关系存在本质差异，需针对性解决方法。

Method: 1. 理论分析形容词上位词关系特性 2. 创建形容词上位词新资源 3. 基于TaxoLLaMa框架微调大语言模型进行预测

Result: 成功构建形容词上位词资源，验证了TaxoLLaMa框架在形容词分类任务中的适应性，模型预测效果显著。

Conclusion: 通过理论框架构建与模型微调相结合的方法，可有效解决形容词上位词关系缺失问题，为词网资源完善提供新途径。

Abstract: Open English Wordnet is a key resource published in OntoLex-lemon as part of
the linguistic linked open data cloud. There are, however, many links missing
in the resource, and in this paper, we look at how we can establish hypernymy
between adjectives. We present a theoretical discussion of the hypernymy
relation and how it differs for adjectives in contrast to nouns and verbs. We
develop a new resource for adjective hypernymy and fine-tune large language
models to predict adjective hypernymy, showing that the methodology of
TaxoLLaMa can be adapted to this task.

</details>


### [36] [PREMISE: Scalable and Strategic Prompt Optimization for Efficient Mathematical Reasoning in Large Models](https://arxiv.org/abs/2506.10716)
*Ye Yu,Yaoning Yu,Haohan Wang*

Main category: cs.CL

TL;DR: PREMISE框架通过提示优化减少大型推理模型的冗余计算，在保持准确率的同时显著降低token使用量及成本。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型(如Claude、GPT)使用长思维链推理时会产生冗余token，导致API调用费用高昂且延迟增加，限制了实际部署场景。

Method: 结合轨迹级诊断与梯度启发的提示优化，通过多目标文本搜索平衡答案简洁性与正确性，采用单次黑箱接口实现无需修改模型权重的优化。

Result: 在GSM8K等数学基准测试中保持或提升准确率(Claude 96%→96%，Gemini 91%→92%)，推理token减少87.5%，成本降低69-82%。

Conclusion: 纯提示层面的优化可有效实现高效推理，证明该方法在商业大模型中具有实用性和可扩展性，且不损失推理质量。

Abstract: Large reasoning models (LRMs) such as Claude 3.7 Sonnet and OpenAI o1 achieve
strong performance on mathematical benchmarks using lengthy chain-of-thought
(CoT) reasoning, but the resulting traces are often unnecessarily verbose. This
inflates token usage and cost, limiting deployment in latency-sensitive or
API-constrained settings. We introduce PREMISE (PRompt-based Efficient
Mathematical Inference with Strategic Evaluation), a prompt-only framework that
reduces reasoning overhead without modifying model weights. PREMISE combines
trace-level diagnostics with gradient-inspired prompt optimization to minimize
redundant computation while preserving answer accuracy. The approach jointly
optimizes brevity and correctness through a multi-objective textual search that
balances token length and answer validity. Unlike prior work, PREMISE runs in a
single-pass black-box interface, so it can be applied directly to commercial
LLMs. On GSM8K, SVAMP, and Math500 we match or exceed baseline accuracy
($96\%\rightarrow96\%$ with Claude, $91\%\rightarrow92\%$ with Gemini) while
reducing reasoning tokens by up to $87.5\%$ and cutting dollar cost by
$69$--$82\%$. These results show that prompt-level optimization is a practical
and scalable path to efficient LRM inference without compromising reasoning
quality.

</details>


### [37] [Beyond True or False: Retrieval-Augmented Hierarchical Analysis of Nuanced Claims](https://arxiv.org/abs/2506.10728)
*Priyanka Kargupta,Runchu Tian,Jiawei Han*

Main category: cs.CL

TL;DR: 提出ClaimSpect框架，通过将复杂主张分解为多个方面并检索相关语料，自动构建层次化视角分析，以全面评估科学和政治主张的多角度观点及其分布。


<details>
  <summary>Details</summary>
Motivation: 现有主张（如科学或政治声明）常无法简单以真伪’标签，需分解为更细粒度方面（如疫苗的安全性和有效性），以提供结构化、多视角的验证和分析。

Method: 基于检索增强生成技术（RAG），ClaimSpect自动构建主张的层次化方面结构，分层次检索语料片段以发现子方面，并识别不同观点（支持/中立/反对）及其分布。

Result: 在真实科学和政治主张数据集上验证，ClaimSpect能准确解构复杂主张并反映语料中的观点分布（如统计生物医学论文对疫苗运输性的态度），效果优于基线方法。

Conclusion: ClaimSpect通过层次化分解和语料视角分析，为复杂主张提供结构化评估工具，增强信息验证的全面性和可解释性。

Abstract: Claims made by individuals or entities are oftentimes nuanced and cannot be
clearly labeled as entirely "true" or "false" -- as is frequently the case with
scientific and political claims. However, a claim (e.g., "vaccine A is better
than vaccine B") can be dissected into its integral aspects and sub-aspects
(e.g., efficacy, safety, distribution), which are individually easier to
validate. This enables a more comprehensive, structured response that provides
a well-rounded perspective on a given problem while also allowing the reader to
prioritize specific angles of interest within the claim (e.g., safety towards
children). Thus, we propose ClaimSpect, a retrieval-augmented generation-based
framework for automatically constructing a hierarchy of aspects typically
considered when addressing a claim and enriching them with corpus-specific
perspectives. This structure hierarchically partitions an input corpus to
retrieve relevant segments, which assist in discovering new sub-aspects.
Moreover, these segments enable the discovery of varying perspectives towards
an aspect of the claim (e.g., support, neutral, or oppose) and their respective
prevalence (e.g., "how many biomedical papers believe vaccine A is more
transportable than B?"). We apply ClaimSpect to a wide variety of real-world
scientific and political claims featured in our constructed dataset, showcasing
its robustness and accuracy in deconstructing a nuanced claim and representing
perspectives within a corpus. Through real-world case studies and human
evaluation, we validate its effectiveness over multiple baselines.

</details>


### [38] [TaxoAdapt: Aligning LLM-Based Multidimensional Taxonomy Construction to Evolving Research Corpora](https://arxiv.org/abs/2506.10737)
*Priyanka Kargupta,Nan Zhang,Yunyi Zhang,Rui Zhang,Prasenjit Mitra,Jiawei Han*

Main category: cs.CL

TL;DR: TaxoAdapt提出了一种动态调整LLM生成分类法的框架，通过迭代层次分类扩展分类体系，解决现有方法在通用性、动态性和多维性上的不足，在计算机科学会议数据上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统专家构建分类法耗时昂贵，现有自动方法或依赖特定语料库缺乏通用性，或过度依赖LLM预训练数据忽略科学领域动态性，且未考虑文献多维度特性。

Method: TaxoAdapt框架动态适配LLM生成分类法至目标语料库，通过迭代式层次分类扩展分类体系宽度与深度，基于语料主题分布实现多维度适配。

Result: 在跨年度计算机科学会议数据集中，TaxoAdapt生成的分类法比基线方法保留细粒度能力提升26.51%，连贯性提升50.41%（基于LLM评估）。

Conclusion: TaxoAdapt通过动态多维度适配机制，有效捕捉科学领域演化特征，构建更细粒度、更符合实际语料分布的科学文献分类体系。

Abstract: The rapid evolution of scientific fields introduces challenges in organizing
and retrieving scientific literature. While expert-curated taxonomies have
traditionally addressed this need, the process is time-consuming and expensive.
Furthermore, recent automatic taxonomy construction methods either (1)
over-rely on a specific corpus, sacrificing generalizability, or (2) depend
heavily on the general knowledge of large language models (LLMs) contained
within their pre-training datasets, often overlooking the dynamic nature of
evolving scientific domains. Additionally, these approaches fail to account for
the multi-faceted nature of scientific literature, where a single research
paper may contribute to multiple dimensions (e.g., methodology, new tasks,
evaluation metrics, benchmarks). To address these gaps, we propose TaxoAdapt, a
framework that dynamically adapts an LLM-generated taxonomy to a given corpus
across multiple dimensions. TaxoAdapt performs iterative hierarchical
classification, expanding both the taxonomy width and depth based on corpus'
topical distribution. We demonstrate its state-of-the-art performance across a
diverse set of computer science conferences over the years to showcase its
ability to structure and capture the evolution of scientific fields. As a
multidimensional method, TaxoAdapt generates taxonomies that are 26.51% more
granularity-preserving and 50.41% more coherent than the most competitive
baselines judged by LLMs.

</details>


### [39] [One Tokenizer To Rule Them All: Emergent Language Plasticity via Multilingual Tokenizers](https://arxiv.org/abs/2506.10766)
*Diana Abagyan,Alejandro R. Salamanca,Andres Felipe Cruz-Salinas,Kris Cao,Hangyu Lin,Acyr Locatelli,Marzieh Fadaee,Ahmet Üstün,Sara Hooker*

Main category: cs.CL

TL;DR: 研究提出使用覆盖更广的通用分词器提升多语言大模型预训练后的语言适应能力，实验显示其显著优于传统分词器，且对未见语言也有更好扩展性。


<details>
  <summary>Details</summary>
Motivation: 多语言大模型预训练面临模型容量限制、高质量数据稀缺及分词器语言覆盖不足的问题，需探索低成本干预方法以增强模型对新语言的适应能力。

Method: 设计覆盖语言数超过主预训练集的通用分词器，通过系统性实验验证其在预训练后扩展语言覆盖时的有效性。

Result: 通用分词器使语言适应胜率提升最高达20.2%，对完全未见语言的适应胜率增益达5%，且不影响主预训练语言性能。

Conclusion: 早期采用通用分词器可显著增强模型语言可塑性，为多语言扩展提供高效解决方案，平衡性能与扩展需求。

Abstract: Pretraining massively multilingual Large Language Models (LLMs) for many
languages at once is challenging due to limited model capacity, scarce
high-quality data, and compute constraints. Moreover, the lack of language
coverage of the tokenizer makes it harder to address the gap for new languages
purely at the post-training stage. In this work, we study what relatively cheap
interventions early on in training improve "language plasticity", or adaptation
capabilities of the model post-training to new languages. We focus on tokenizer
design and propose using a universal tokenizer that is trained for more
languages than the primary pretraining languages to enable efficient adaptation
in expanding language coverage after pretraining. Our systematic experiments
across diverse groups of languages and different training strategies show that
a universal tokenizer enables significantly higher language adaptation, with up
to 20.2% increase in win rates compared to tokenizers specific to pretraining
languages. Furthermore, a universal tokenizer also leads to better plasticity
towards languages that are completely unseen in the tokenizer and pretraining,
by up to 5% win rate gain. We achieve this adaptation to an expanded set of
languages with minimal compromise in performance on the majority of languages
included in pretraining.

</details>


### [40] [Different Questions, Different Models: Fine-Grained Evaluation of Uncertainty and Calibration in Clinical QA with LLMs](https://arxiv.org/abs/2506.10769)
*Alberto Testoni,Iacer Calixto*

Main category: cs.CL

TL;DR: 研究评估了十种开源大语言模型在临床多选题回答中的不确定性估计方法，发现基于推理行为信号的轻量级单次生成方法接近语义熵效果，并强调需根据问题类型和模型优势选择模型。


<details>
  <summary>Details</summary>
Motivation: 在临床决策等高风险领域部署大语言模型时，需确保其不确定性估计的准确性和校准性，以支持可靠应用。

Method: 通过两个数据集、11个医学专业和六类问题，对比标准单次生成与采样方法，并设计基于推理轨迹行为信号的单次轻量级估计器。

Result: 轻量方法性能接近需多次采样的语义熵，且不同医学专业与问题类型间模型表现差异显著。

Conclusion: 模型选择应同时考虑具体问题类型与模型特性，单次行为信号方法为临床场景提供高效不确定性估计方案。

Abstract: Accurate and well-calibrated uncertainty estimates are essential for
deploying large language models (LLMs) in high-stakes domains such as clinical
decision support. We present a fine-grained evaluation of uncertainty
estimation methods for clinical multiple-choice question answering, covering
ten open-source LLMs (general-purpose, biomedical, and reasoning models) across
two datasets, eleven medical specialties, and six question types. We compare
standard single-generation and sampling-based methods, and present a case study
exploring simple, single-pass estimators based on behavioral signals in
reasoning traces. These lightweight methods approach the performance of
Semantic Entropy while requiring only one generation. Our results reveal
substantial variation across specialties and question types, underscoring the
importance of selecting models based on both the nature of the question and
model-specific strengths.

</details>


### [41] [Improving Named Entity Transcription with Contextual LLM-based Revision](https://arxiv.org/abs/2506.10779)
*Viet Anh Trinh,Xinlu He,Jacob Whitehill*

Main category: cs.CL

TL;DR: 本文提出利用大语言模型（LLM）结合本地上下文修正ASR中命名实体错误，并在新数据集上实现命名实体词错率相对降低30%。


<details>
  <summary>Details</summary>
Motivation: 现有ASR系统在通用语音表现优异，但命名实体识别错误率仍较高，而命名实体是关键信息，错误会影响下游任务。

Method: 通过LLM的推理能力及包含正确实体的本地上下文（如课程讲义）对ASR预测结果中的命名实体进行修正。

Result: 在自建的NER-MIT-OpenCourseWare数据集（45小时MIT课程数据）上，命名实体词错率相对降低30%。

Conclusion: LLM与本地上下文结合能有效修正ASR命名实体错误，新数据集为相关研究提供了基准。

Abstract: With recent advances in modeling and the increasing amount of supervised
training data, automatic speech recognition (ASR) systems have achieved
remarkable performance on general speech. However, the word error rate (WER) of
state-of-the-art ASR remains high for named entities. Since named entities are
often the most critical keywords, misrecognizing them can affect all downstream
applications, especially when the ASR system functions as the front end of a
complex system. In this paper, we introduce a large language model (LLM)
revision mechanism to revise incorrect named entities in ASR predictions by
leveraging the LLM's reasoning ability as well as local context (e.g., lecture
notes) containing a set of correct named entities. Finally, we introduce the
NER-MIT-OpenCourseWare dataset, containing 45 hours of data from MIT courses
for development and testing. On this dataset, our proposed technique achieves
up to 30\% relative WER reduction for named entities.

</details>


### [42] [Mitigating Negative Interference in Multilingual Sequential Knowledge Editing through Null-Space Constraints](https://arxiv.org/abs/2506.10800)
*Wei Sun,Tingyu Qu,Mingxiao Li,Jesse Davis,Marie-Francine Moens*

Main category: cs.CL

TL;DR: 提出LangEdit框架，通过正交投影隔离多语言知识更新，解决大型语言模型在跨语言更新时的参数干扰问题，保持多语言泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法需为每种语言单独部署编辑系统（成本高）或在统一模型中顺序更新导致参数干扰，破坏多语言泛化与知识准确性。需一种跨语言高效更新且避免干扰的解决方案。

Method: LangEdit采用零空间约束框架，将各语言参数投影至先前更新子空间的正交补集，数学保证更新独立性，同时保留多语言泛化能力。

Result: 在3种模型架构、6种语言和4个下游任务中验证，LangEdit显著减少参数干扰，性能超越现有编辑方法。

Conclusion: LangEdit为LLMs提供高效、精准的多语言知识更新方案，其代码已开源，具备实际应用潜力。

Abstract: Efficiently updating multilingual knowledge in large language models (LLMs),
while preserving consistent factual representations across languages, remains a
long-standing and unresolved challenge. While deploying separate editing
systems for each language might seem viable, this approach incurs substantial
costs due to the need to manage multiple models. A more efficient solution
involves integrating knowledge updates across all languages into a unified
model. However, performing sequential edits across languages often leads to
destructive parameter interference, significantly degrading multilingual
generalization and the accuracy of injected knowledge. To address this
challenge, we propose LangEdit, a novel null-space constrained framework
designed to precisely isolate language-specific knowledge updates. The core
innovation of LangEdit lies in its ability to project parameter updates for
each language onto the orthogonal complement of previous updated subspaces.
This approach mathematically guarantees update independence while preserving
multilingual generalization capabilities. We conduct a comprehensive evaluation
across three model architectures, six languages, and four downstream tasks,
demonstrating that LangEdit effectively mitigates parameter interference and
outperforms existing state-of-the-art editing methods. Our results highlight
its potential for enabling efficient and accurate multilingual knowledge
updates in LLMs. The code is available at
https://github.com/VRCMF/LangEdit.git.

</details>


### [43] [ReCUT: Balancing Reasoning Length and Accuracy in LLMs via Stepwise Trails and Preference Optimization](https://arxiv.org/abs/2506.10822)
*Zhensheng Jin,Xinze Li,Yifan Ji,Chunyi Peng,Zhenghao Liu,Qi Shi,Yukun Yan,Shuo Wang,Furong Peng,Ge Yu*

Main category: cs.CL

TL;DR: ReCUT提出一种通过逐步探索和参数插值的方法，在保持推理准确性的同时将推理长度减少30-50%，解决了CoT提示法中过度思考导致的冗余问题。


<details>
  <summary>Details</summary>
Motivation: 现有CoT提示法存在过度思考导致冗余推理的问题，而基于多推理链训练的方法受限于生成数据质量和过拟合风险，需平衡推理精度与路径长度的新方法。

Method: 采用逐步探索机制和长短切换采样策略生成多样化推理路径，训练分别优化精度和简洁性的双模型（基于Gemini LLMs），通过参数插值整合为单一模型。

Result: 在多个数学推理数据集上，ReCUT较基线方法减少30-50%推理长度，同时维持或提升准确率，且适用于不同骨干模型。

Conclusion: ReCUT通过平衡推理精度与路径长度，有效提升LLMs推理效率，其代码与数据已开源，为优化复杂推理任务提供了新方案。

Abstract: Recent advances in Chain-of-Thought (CoT) prompting have substantially
improved the reasoning capabilities of Large Language Models (LLMs). However,
these methods often suffer from overthinking, leading to unnecessarily lengthy
or redundant reasoning traces. Existing approaches attempt to mitigate this
issue through curating multiple reasoning chains for training LLMs, but their
effectiveness is often constrained by the quality of the generated data and
prone to overfitting. To address the challenge, we propose Reasoning
Compression ThroUgh Stepwise Trials (ReCUT), a novel method aimed at balancing
the accuracy and length of reasoning trajectory. Specifically, ReCUT employs a
stepwise exploration mechanism and a long-short switched sampling strategy,
enabling LLMs to incrementally generate diverse reasoning paths. These paths
are evaluated and used to construct preference pairs to train two specialized
models (Gemini LLMs)-one optimized for reasoning accuracy, the other for
shorter reasoning. A final integrated model is obtained by interpolating the
parameters of these two models. Experimental results across multiple math
reasoning datasets and backbone models demonstrate that ReCUT significantly
reduces reasoning lengths by approximately 30-50%, while maintaining or
improving reasoning accuracy compared to various baselines. All codes and data
will be released via https://github.com/NEUIR/ReCUT.

</details>


### [44] [CIIR@LiveRAG 2025: Optimizing Multi-Agent Retrieval Augmented Generation through Self-Training](https://arxiv.org/abs/2506.10844)
*Alireza Salemi,Mukta Maddipatla,Hamed Zamani*

Main category: cs.CL

TL;DR: 本文提出mRAG框架，通过多智能体协作与自训练优化，在复杂RAG任务中表现优于传统方法，并通过案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成（RAG）在复杂任务中可能存在协作不足的问题，需通过多智能体分工与协作优化提升处理能力。

Method: 采用多智能体框架（规划、搜索、推理、协调代理），结合奖励引导轨迹采样的自训练范式优化协作机制。

Result: 在SIGIR 2025 LiveRAG竞赛的DataMorgana数据集上超越基线模型，案例研究证实框架实际应用价值。

Conclusion: mRAG通过智能体协同与动态优化机制，显著提升复杂现实场景下的RAG任务性能，具有强扩展性与实用性。

Abstract: This paper presents mRAG, a multi-agent retrieval-augmented generation (RAG)
framework composed of specialized agents for subtasks such as planning,
searching, reasoning, and coordination. Our system uses a self-training
paradigm with reward-guided trajectory sampling to optimize inter-agent
collaboration and enhance response generation. Evaluated on DataMorgana-derived
datasets during the SIGIR 2025 LiveRAG competition, mRAG outperforms
conventional RAG baselines. We further analyze competition outcomes and
showcase the framework's strengths with case studies, demonstrating its
efficacy for complex, real-world RAG tasks.

</details>


### [45] [Accelerating Diffusion Large Language Models with SlowFast: The Three Golden Principles](https://arxiv.org/abs/2506.10848)
*Qingyan Wei,Yaojie Zhang,Zhiyuan Liu,Dongrui Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 本文提出SlowFast Sampling动态采样策略，通过交替探索与加速解码阶段，结合dLLM-Cache减少冗余计算，显著提升扩散式语言模型生成速度，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散式语言模型（dLLMs）的采样策略（如置信度解码或半自回归解码）存在静态行为问题，导致解码效率低下且灵活性受限。

Method: 提出SlowFast Sampling策略，基于确定性、收敛性、位置性三原则动态切换探索与加速解码阶段，并整合dLLM-Cache优化计算冗余。

Result: 实验显示，该方法在LLaDA上实现15.63倍加速（结合缓存后达34.22倍），且吞吐量超越自回归基线模型LLaMA3 8B，精度损失极小。

Conclusion: 精心设计的采样策略可充分释放dLLMs潜力，证明其在高速高质量文本生成中可替代传统自回归模型。

Abstract: Diffusion-based language models (dLLMs) have emerged as a promising
alternative to traditional autoregressive LLMs by enabling parallel token
generation and significantly reducing inference latency. However, existing
sampling strategies for dLLMs, such as confidence-based or semi-autoregressive
decoding, often suffer from static behavior, leading to suboptimal efficiency
and limited flexibility. In this paper, we propose SlowFast Sampling, a novel
dynamic sampling strategy that adaptively alternates between exploratory and
accelerated decoding stages. Our method is guided by three golden principles:
certainty principle, convergence principle, and positional principle, which
govern when and where tokens can be confidently and efficiently decoded. We
further integrate our strategy with dLLM-Cache to reduce redundant computation.
Extensive experiments across benchmarks and models show that SlowFast Sampling
achieves up to 15.63$\times$ speedup on LLaDA with minimal accuracy drop, and
up to 34.22$\times$ when combined with caching. Notably, our approach
outperforms strong autoregressive baselines like LLaMA3 8B in throughput,
demonstrating that well-designed sampling can unlock the full potential of
dLLMs for fast and high-quality generation.

</details>


### [46] [Analyzing the relationships between pretraining language, phonetic, tonal, and speaker information in self-supervised speech models](https://arxiv.org/abs/2506.10855)
*Michele Gubian,Ioana Krehan,Oli Liu,James Kirby,Sharon Goldwater*

Main category: cs.CL

TL;DR: 研究分析了四种语言训练的wav2vec2模型对匹配/非匹配语言语音的编码方式，发现音素、声调与说话者信息的表征子空间正交，且模型结构独立于预训练语言。


<details>
  <summary>Details</summary>
Motivation: 现有自监督语音模型分析主要集中于英语，本文旨在探究多语言场景中模型如何编码跨语言的语音信息（如音素、声调、说话者）。

Method: 使用探测分类器和几何分析方法，对四种不同预训练语言的wav2vec2模型进行层级化表征分析，考察音素、词汇声调和说话者信息的编码特性。

Result: 所有语言模型中音素、声调与说话者信息的表征子空间高度正交；后期层中匹配语言的音素/声调探测精度略优，但说话者信息无语言匹配优势；各层探测精度模式相似。

Conclusion: wav2vec2模型学习到的结构具有跨语言普适性，其表征空间的组织方式基本不受预训练语言数据的影响。

Abstract: Analyses of self-supervised speech models have begun to reveal where and how
they represent different types of information. However, almost all analyses
have focused on English. Here, we examine how wav2vec2 models trained on four
different languages encode both language-matched and non-matched speech. We use
probing classifiers and geometric analyses to examine how phones, lexical
tones, and speaker information are represented. We show that for all
pretraining and test languages, the subspaces encoding phones, tones, and
speakers are largely orthogonal, and that layerwise patterns of probing
accuracy are similar, with a relatively small advantage for matched-language
phone and tone (but not speaker) probes in the later layers. Our findings
suggest that the structure of representations learned by wav2vec2 is largely
independent of the speech material used during pretraining.

</details>


### [47] [Enhancing Medical Dialogue Generation through Knowledge Refinement and Dynamic Prompt Adjustment](https://arxiv.org/abs/2506.10877)
*Hongda Sun,Jiaren Peng,Wenzhong Yang,Liang He,Bo Du,Rui Yan*

Main category: cs.CL

TL;DR: 提出MedRef医疗对话系统，通过知识精炼和动态提示调整解决现有系统在知识筛选与个性化响应生成上的不足，实验验证其有效性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有医疗对话系统（MDS）存在两大问题：难以筛选相关医学知识，且无法生成个性化、医学准确的回答。需提升系统在真实医疗场景中的实用性。

Method: 1. 知识精炼机制过滤无关医学数据，提升关键医学实体预测；2. 设计综合提示结构整合历史与显性细节；3. 通过Triplet Filter和Demo Selector模块动态适配患者状态。

Result: 在MedDG和KaMed基准测试中，MedRef在生成质量与医学实体准确性上均超越现有最优模型，验证其实际医疗应用的潜力。

Conclusion: MedRef通过知识优化与动态提示机制，显著提升医疗对话系统的响应质量与医学准确性，为真实场景提供可靠解决方案。

Abstract: Medical dialogue systems (MDS) have emerged as crucial online platforms for
enabling multi-turn, context-aware conversations with patients. However,
existing MDS often struggle to (1) identify relevant medical knowledge and (2)
generate personalized, medically accurate responses. To address these
challenges, we propose MedRef, a novel MDS that incorporates knowledge refining
and dynamic prompt adjustment. First, we employ a knowledge refining mechanism
to filter out irrelevant medical data, improving predictions of critical
medical entities in responses. Additionally, we design a comprehensive prompt
structure that incorporates historical details and evident details. To enable
real-time adaptability to diverse patient conditions, we implement two key
modules, Triplet Filter and Demo Selector, providing appropriate knowledge and
demonstrations equipped in the system prompt. Extensive experiments on MedDG
and KaMed benchmarks show that MedRef outperforms state-of-the-art baselines in
both generation quality and medical entity accuracy, underscoring its
effectiveness and reliability for real-world healthcare applications.

</details>


### [48] [Slimming Down LLMs Without Losing Their Minds](https://arxiv.org/abs/2506.10885)
*Qingda,Mai*

Main category: cs.CL

TL;DR: 本研究验证了参数高效微调方法（LoRA和QLoRA）对大型语言模型在常识推理、数学推理及多领域知识任务中的性能影响，强调数据集与任务对齐的重要性。


<details>
  <summary>Details</summary>
Motivation: 探索在有限计算资源下，如何通过参数高效微调方法提升大模型在特定任务中的性能，并分析其机制与实际应用价值。

Method: 采用LoRA和QLoRA进行微调，并在HellaSwag（常识推理）、GSM8K（数学推理）、MMLU-CS（多领域知识）三个基准任务上评估模型表现。

Result: LoRA方法显著提升任务性能且保持高效计算；性能提升高度依赖微调数据与评测任务的对齐程度。

Conclusion: 研究揭示了参数高效微调的理论机制，并为资源受限的开发者提供了兼顾效率与性能的模型适配实践指导。

Abstract: This paper investigates and validates the impact of fine-tuning on large
language model performance, focusing on parameter-efficient methods (LoRA and
QLoRA). We evaluate model capabilities across three key domains: (1)
commonsense reasoning (HellaSwag), (2) mathematical reasoning (GSM8K), and (3)
multi-domain knowledge (MMLU-CS).
  Our findings demonstrate that: (1) LoRA-based methods effectively improve
task-specific performance while maintaining computational efficiency, and (2)
performance strongly depends on alignment between fine-tuning dataset and
benchmark tasks. The study provides both theoretical insights into
parameter-efficient mechanisms and practical guidance for developers
implementing efficient LLM adaptation with limited resources.

</details>


### [49] [Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers](https://arxiv.org/abs/2506.10887)
*Yixiao Huang,Hanlin Zhu,Tianyu Guo,Jiantao Jiao,Somayeh Sojoudi,Michael I. Jordan,Stuart Russell,Song Mei*

Main category: cs.CL

TL;DR: 本文提出大语言模型（LLMs）的泛化与幻觉行为均由其'上下文外推理'(OCR)机制驱动，并通过理论分析与实验验证了矩阵分解与梯度下降隐式偏差对OCR的关键作用。


<details>
  <summary>Details</summary>
Motivation: LLMs在微调时既能泛化新知识又易产生幻觉，这一矛盾现象缺乏理论解释。研究旨在揭示其统一机制，并分析其数学本质。

Method: 通过五个主流LLM实验验证OCR机制，构建单层单头注意力模型进行合成事实回忆任务，理论分析梯度下降隐式偏差导致的核范数最小化特性。

Result: 实验表明OCR通过概念关联驱动泛化/幻觉，矩阵分解结构是必要条件；理论证明梯度下降隐式偏差使输出-值矩阵核范数最小化，解释样本高效学习机制。

Conclusion: 研究为OCR现象提供了统一理论框架，揭示了知识注入中因果/伪相关关联的数学本质，为分析及缓解模型幻觉提供了新视角。

Abstract: Large language models (LLMs) can acquire new knowledge through fine-tuning,
but this process exhibits a puzzling duality: models can generalize remarkably
from new facts, yet are also prone to hallucinating incorrect information.
However, the reasons for this phenomenon remain poorly understood. In this
work, we argue that both behaviors stem from a single mechanism known as
out-of-context reasoning (OCR): the ability to deduce implications by
associating concepts, even those without a causal link. Our experiments across
five prominent LLMs confirm that OCR indeed drives both generalization and
hallucination, depending on whether the associated concepts are causally
related. To build a rigorous theoretical understanding of this phenomenon, we
then formalize OCR as a synthetic factual recall task. We empirically show that
a one-layer single-head attention-only transformer with factorized output and
value matrices can learn to solve this task, while a model with combined
weights cannot, highlighting the crucial role of matrix factorization. Our
theoretical analysis shows that the OCR capability can be attributed to the
implicit bias of gradient descent, which favors solutions that minimize the
nuclear norm of the combined output-value matrix. This mathematical structure
explains why the model learns to associate facts and implications with high
sample efficiency, regardless of whether the correlation is causal or merely
spurious. Ultimately, our work provides a theoretical foundation for
understanding the OCR phenomenon, offering a new lens for analyzing and
mitigating undesirable behaviors from knowledge injection.

</details>


### [50] [BioClinical ModernBERT: A State-of-the-Art Long-Context Encoder for Biomedical and Clinical NLP](https://arxiv.org/abs/2506.10896)
*Thomas Sounack,Joshua Davis,Brigitte Durieux,Antoine Chaffin,Tom J. Pollard,Eric Lehman,Alistair E. W. Johnson,Matthew McDermott,Tristan Naumann,Charlotta Lindvall*

Main category: cs.CL

TL;DR: BioClinical ModernBERT是基于ModernBERT改进的生物医学和临床NLP编码器，通过大规模多源数据预训练，在性能和速度上超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有编码器在生物医学和临床领域发展滞后，且依赖单一数据源导致领域适应性受限，需提升处理效率和泛化能力。

Method: 基于ModernBERT进行持续预训练，使用包含535亿标记的最大生物医学临床语料库，整合20个多样化机构、领域和地区的数据集。

Result: 在四个下游任务中表现优于现有模型，发布150M和396M参数版本及训练检查点以支持研究。

Conclusion: BioClinical ModernBERT通过长上下文处理和多源数据解决了领域适应限制，为生物医学NLP提供了高效且通用的工具。

Abstract: Encoder-based transformer models are central to biomedical and clinical
Natural Language Processing (NLP), as their bidirectional self-attention makes
them well-suited for efficiently extracting structured information from
unstructured text through discriminative tasks. However, encoders have seen
slower development compared to decoder models, leading to limited domain
adaptation in biomedical and clinical settings. We introduce BioClinical
ModernBERT, a domain-adapted encoder that builds on the recent ModernBERT
release, incorporating long-context processing and substantial improvements in
speed and performance for biomedical and clinical NLP. BioClinical ModernBERT
is developed through continued pretraining on the largest biomedical and
clinical corpus to date, with over 53.5 billion tokens, and addresses a key
limitation of prior clinical encoders by leveraging 20 datasets from diverse
institutions, domains, and geographic regions, rather than relying on data from
a single source. It outperforms existing biomedical and clinical encoders on
four downstream tasks spanning a broad range of use cases. We release both base
(150M parameters) and large (396M parameters) versions of BioClinical
ModernBERT, along with training checkpoints to support further research.

</details>


### [51] [Beyond Gold Standards: Epistemic Ensemble of LLM Judges for Formal Mathematical Reasoning](https://arxiv.org/abs/2506.10903)
*Lan Zhang,Marco Valentino,Andre Freitas*

Main category: cs.CL

TL;DR: 本文提出一种基于大语言模型（LLM）的系统化自动评估方法（EFG集成），通过多粒度标准（逻辑保持、数学一致性等）提升形式数学推理中自动形式化任务评估的准确性和可解释性，实验表明其与人类评估相关性更强。


<details>
  <summary>Details</summary>
Motivation: 现有自动形式化评估方法依赖粗粒度通用标准，难以应对复杂数学领域中对逻辑、一致性等细粒度质量的要求，且人工评估成本高。需开发更系统、透明的自动化评估框架。

Method: 提出基于认知与形式化基础的EFG集成评估框架，结合LLM作为评判者，从逻辑保持（LP）、数学一致性（MC）、形式有效性（FV）和形式质量（FQ）四个细粒度维度进行多因素透明评估。

Result: 实验验证EFG集成方法在形式数学领域评估中与人类评估相关性显著高于粗粒度模型，尤其在形式质量（FQ）维度表现更优，表明其可作为可靠的自动评估代理。

Conclusion: 基于明确原子化属性的LLM评判者（如EFG集成）能够为形式数学推理提供可扩展、可解释且可靠的评估支持，未来可扩展至更复杂领域。

Abstract: Autoformalization plays a crucial role in formal mathematical reasoning by
enabling the automatic translation of natural language statements into formal
languages. While recent advances using large language models (LLMs) have shown
promising results, methods for automatically evaluating autoformalization
remain underexplored. As one moves to more complex domains (e.g., advanced
mathematics), human evaluation requires significant time and domain expertise,
especially as the complexity of the underlying statements and background
knowledge increases. LLM-as-a-judge presents a promising approach for
automating such evaluation. However, existing methods typically employ
coarse-grained and generic evaluation criteria, which limit their effectiveness
for advanced formal mathematical reasoning, where quality hinges on nuanced,
multi-granular dimensions. In this work, we take a step toward addressing this
gap by introducing a systematic, automatic method to evaluate autoformalization
tasks. The proposed method is based on an epistemically and formally grounded
ensemble (EFG) of LLM judges, defined on criteria encompassing logical
preservation (LP), mathematical consistency (MC), formal validity (FV), and
formal quality (FQ), resulting in a transparent assessment that accounts for
different contributing factors. We validate the proposed framework to serve as
a proxy for autoformalization assessment within the domain of formal
mathematics. Overall, our experiments demonstrate that the EFG ensemble of LLM
judges is a suitable emerging proxy for evaluation, more strongly correlating
with human assessments than a coarse-grained model, especially when assessing
formal qualities. These findings suggest that LLM-as-judges, especially when
guided by a well-defined set of atomic properties, could offer a scalable,
interpretable, and reliable support for evaluating formal mathematical
reasoning.

</details>


### [52] [Magistral](https://arxiv.org/abs/2506.10910)
*Mistral-AI,:,Abhinav Rastogi,Albert Q. Jiang,Andy Lo,Gabrielle Berrada,Guillaume Lample,Jason Rute,Joep Barmentlo,Karmesh Yadav,Kartik Khandelwal,Khyathi Raghavi Chandu,Léonard Blier,Lucile Saulnier,Matthieu Dinot,Maxime Darrin,Neha Gupta,Roman Soletskyi,Sagar Vaze,Teven Le Scao,Yihan Wang,Adam Yang,Alexander H. Liu,Alexandre Sablayrolles,Amélie Héliou,Amélie Martin,Andy Ehrenberg,Anmol Agarwal,Antoine Roux,Arthur Darcet,Arthur Mensch,Baptiste Bout,Baptiste Rozière,Baudouin De Monicault,Chris Bamford,Christian Wallenwein,Christophe Renaudin,Clémence Lanfranchi,Darius Dabert,Devon Mizelle,Diego de las Casas,Elliot Chane-Sane,Emilien Fugier,Emma Bou Hanna,Gauthier Delerce,Gauthier Guinet,Georgii Novikov,Guillaume Martin,Himanshu Jaju,Jan Ludziejewski,Jean-Hadrien Chabran,Jean-Malo Delignon,Joachim Studnia,Jonas Amar,Josselin Somerville Roberts,Julien Denize,Karan Saxena,Kush Jain,Lingxiao Zhao,Louis Martin,Luyu Gao,Lélio Renard Lavaud,Marie Pellat,Mathilde Guillaumin,Mathis Felardos,Maximilian Augustin,Mickaël Seznec,Nikhil Raghuraman,Olivier Duchenne,Patricia Wang,Patrick von Platen,Patryk Saffer,Paul Jacob,Paul Wambergue,Paula Kurylowicz,Pavankumar Reddy Muddireddy,Philomène Chagniot,Pierre Stock,Pravesh Agrawal,Romain Sauvestre,Rémi Delacourt,Sanchit Gandhi,Sandeep Subramanian,Shashwat Dalal,Siddharth Gandhi,Soham Ghosh,Srijan Mishra,Sumukh Aithal,Szymon Antoniak,Thibault Schueller,Thibaut Lavril,Thomas Robert,Thomas Wang,Timothée Lacroix,Valeriia Nemychnikova,Victor Paltz,Virgile Richard,Wen-Ding Li,William Marshall,Xuanyu Zhang,Yunhao Tang*

Main category: cs.CL

TL;DR: Mistral推出首个推理模型Magistral，采用自研强化学习流程，探索纯RL训练LLM的极限，证明仅文本RL可保持模型能力，并开源部分模型。


<details>
  <summary>Details</summary>
Motivation: 旨在摆脱对现有实现和RL轨迹的依赖，通过自研模型和基础设施，探索纯强化学习在LLM训练中的潜力。

Method: 采用自下而上的纯RL训练方法，提出强制模型推理语言控制技术，基于Mistral Medium 3进行推理训练，并通过冷启动数据扩展模型。

Result: 纯文本RL训练保持/提升多模态理解、指令跟随和函数调用能力，发布Magistral Medium及Apache 2.0开源的Magistral Small。

Conclusion: 验证了纯RL训练LLM的可行性，展示了语言控制方法的有效性，通过开源推动社区发展，为模型能力维护提供新路径。

Abstract: We introduce Magistral, Mistral's first reasoning model and our own scalable
reinforcement learning (RL) pipeline. Instead of relying on existing
implementations and RL traces distilled from prior models, we follow a ground
up approach, relying solely on our own models and infrastructure. Notably, we
demonstrate a stack that enabled us to explore the limits of pure RL training
of LLMs, present a simple method to force the reasoning language of the model,
and show that RL on text data alone maintains most of the initial checkpoint's
capabilities. We find that RL on text maintains or improves multimodal
understanding, instruction following and function calling. We present Magistral
Medium, trained for reasoning on top of Mistral Medium 3 with RL alone, and we
open-source Magistral Small (Apache 2.0) which further includes cold-start data
from Magistral Medium.

</details>


### [53] [Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative Matrix Factorization](https://arxiv.org/abs/2506.10920)
*Or Shafran,Atticus Geiger,Mor Geva*

Main category: cs.CL

TL;DR: 本文提出使用半非负矩阵分解（SNMF）分解MLP激活，替代传统稀疏自编码器（SAEs），以更有效地识别大语言模型中可解释的因果特征，实验显示其在因果干预和概念对齐上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有基于稀疏自编码器（SAEs）的方法在因果评估中效果有限，且缺乏与模型计算直接关联的内在可解释性。神经元多概念编码问题促使研究者转向激活空间方向分析，但需更优的无监督特征提取方法。

Method: 通过半非负矩阵分解（SNMF）直接分解MLP激活，使特征满足两点：（a）为神经元共激活的稀疏线性组合；（b）映射到激活输入以实现直接可解释性。

Result: 在Llama 3.1、Gemma 2和GPT-2上的实验表明，SNMF特征在因果干预效果上超越SAEs及监督基线，且与人类可解释概念一致。分析揭示语义相关特征复用特定神经元组合，暴露MLP激活空间的层次结构。

Conclusion: SNMF作为一种简单有效的工具，不仅能识别可解释特征，还能解析大语言模型中的概念层次表示，为机制可解释性研究提供新方向。

Abstract: A central goal for mechanistic interpretability has been to identify the
right units of analysis in large language models (LLMs) that causally explain
their outputs. While early work focused on individual neurons, evidence that
neurons often encode multiple concepts has motivated a shift toward analyzing
directions in activation space. A key question is how to find directions that
capture interpretable features in an unsupervised manner. Current methods rely
on dictionary learning with sparse autoencoders (SAEs), commonly trained over
residual stream activations to learn directions from scratch. However, SAEs
often struggle in causal evaluations and lack intrinsic interpretability, as
their learning is not explicitly tied to the computations of the model. Here,
we tackle these limitations by directly decomposing MLP activations with
semi-nonnegative matrix factorization (SNMF), such that the learned features
are (a) sparse linear combinations of co-activated neurons, and (b) mapped to
their activating inputs, making them directly interpretable. Experiments on
Llama 3.1, Gemma 2 and GPT-2 show that SNMF derived features outperform SAEs
and a strong supervised baseline (difference-in-means) on causal steering,
while aligning with human-interpretable concepts. Further analysis reveals that
specific neuron combinations are reused across semantically-related features,
exposing a hierarchical structure in the MLP's activation space. Together,
these results position SNMF as a simple and effective tool for identifying
interpretable features and dissecting concept representations in LLMs.

</details>


### [54] [Dynamic Epistemic Friction in Dialogue](https://arxiv.org/abs/2506.10934)
*Timothy Obiso,Kenneth Lai,Abhijnan Nath,Nikhil Krishnaswamy,James Pustejovsky*

Main category: cs.CL

TL;DR: 该论文提出动态认知摩擦概念，指代理在整合新证据时因信念状态与外部命题不匹配而产生的阻力，基于动态认知逻辑框架分析其在对话中的预测能力，并探讨如何优化以适应现实场景。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型对齐方法常忽视认知摩擦（信念更新阻力）在人类-AI协作中的关键作用，需建立理论框架以量化此类阻力对信念整合的影响。

Method: 在动态认知逻辑框架下定义动态认知摩擦，通过协作任务中的对话数据分析信念修正过程，构建模型量化认知阻力。

Result: 模型能有效预测对话中的信念更新模式，揭示认知摩擦与信念对齐程度的关系，验证了理论框架的实践解释力。

Conclusion: 动态认知摩擦模型为复杂对话场景中的信念对齐提供了可扩展的理论基础，未来可通过细化阻力度量进一步提升现实适应性。

Abstract: Recent developments in aligning Large Language Models (LLMs) with human
preferences have significantly enhanced their utility in human-AI collaborative
scenarios. However, such approaches often neglect the critical role of
"epistemic friction," or the inherent resistance encountered when updating
beliefs in response to new, conflicting, or ambiguous information. In this
paper, we define dynamic epistemic friction as the resistance to epistemic
integration, characterized by the misalignment between an agent's current
belief state and new propositions supported by external evidence. We position
this within the framework of Dynamic Epistemic Logic (Van Benthem and Pacuit,
2011), where friction emerges as nontrivial belief-revision during the
interaction. We then present analyses from a situated collaborative task that
demonstrate how this model of epistemic friction can effectively predict belief
updates in dialogues, and we subsequently discuss how the model of belief
alignment as a measure of epistemic resistance or friction can naturally be
made more sophisticated to accommodate the complexities of real-world dialogue
scenarios.

</details>


### [55] [Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture without Training](https://arxiv.org/abs/2506.10952)
*Mozhi Zhang,Howe Tissue,Lu Wang,Xipeng Qiu*

Main category: cs.CL

TL;DR: Domain2Vec提出通过将数据集分解为元域的线性组合，利用域向量优化语言模型预训练的数据混合，显著降低计算开销并提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在语言模型预训练中难以高效选择最优数据混合。论文旨在通过数据分布对齐假设（DA²），无需训练即可找到最优数据组合，解决计算资源浪费和性能瓶颈问题。

Method: 构建元域词汇表，使用分类器将数据集分解为元域分布向量（域向量），基于DA²假设对齐训练与验证数据分布，并结合现有方法建模域向量与模型性能的关系。

Result: 在Pile-CC上仅需原计算量的51.5%即达到相同验证损失；同等计算预算下，下游任务平均性能提升2.83%。

Conclusion: Domain2Vec通过元域分解与分布对齐，高效优化预训练数据混合，显著提升计算效率与下游任务表现，可无缝集成至现有方法中。

Abstract: We introduce~\textsc{Domain2Vec}, a novel approach that decomposes any
dataset into a linear combination of several \emph{meta-domains}, a new concept
designed to capture the key underlying features of datasets.
\textsc{Domain2Vec} maintains a vocabulary of meta-domains and uses a
classifier to decompose any given dataset into a domain vector that corresponds
to a distribution over this vocabulary. These domain vectors enable the
identification of the optimal data mixture for language model (LM) pretraining
in a training-free manner under the \emph{\textbf{D}istribution
\textbf{A}lignment \textbf{A}ssumption} (DA$^{2}$), which suggests that when
the data distributions of the training set and the validation set are better
aligned, a lower validation loss is achieved. Moreover, \textsc{Domain2vec} can
be seamlessly integrated into previous works to model the relationship between
domain vectors and LM performance, greatly enhancing the efficiency and
scalability of previous methods. Extensive experiments demonstrate that
\textsc{Domain2Vec} helps find the data mixture that enhances downstream task
performance with minimal computational overhead. Specifically,
\textsc{Domain2Vec} achieves the same validation loss on Pile-CC using only
$51.5\%$ of the computation required when training on the original mixture of
The Pile dataset. Under equivalent compute budget, \textsc{Domain2Vec} improves
downstream performance by an average of $2.83\%$.

</details>


### [56] [ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark](https://arxiv.org/abs/2506.10960)
*Kangwei Liu,Siyuan Cheng,Bozhong Tian,Xiaozhuan Liang,Yuyang Yin,Meng Han,Ningyu Zhang,Bryan Hooi,Xi Chen,Shumin Deng*

Main category: cs.CL

TL;DR: 本文构建了一个全面的中文有害内容检测基准及知识规则库，并提出知识增强的基线方法，使小模型性能媲美先进大模型。


<details>
  <summary>Details</summary>
Motivation: 现有有害内容检测资源集中于英文领域，中文数据集稀缺且覆盖范围有限，制约了中文内容审核效果。

Method: 基于真实数据构建覆盖6类的中文检测基准，通过专业标注生成显式知识规则库，并设计融合人工规则与大模型隐式知识的知识增强基线方法。

Result: 所提方法使小模型达到与SOTA大模型相当的检测效果，相关代码与数据已开源平台发布。

Conclusion: 该工作填补了中文有害内容检测资源空白，验证了结合专家规则与模型知识的有效性，推动了中文内容安全领域发展。

Abstract: Large language models (LLMs) have been increasingly applied to automated
harmful content detection tasks, assisting moderators in identifying policy
violations and improving the overall efficiency and accuracy of content review.
However, existing resources for harmful content detection are predominantly
focused on English, with Chinese datasets remaining scarce and often limited in
scope. We present a comprehensive, professionally annotated benchmark for
Chinese content harm detection, which covers six representative categories and
is constructed entirely from real-world data. Our annotation process further
yields a knowledge rule base that provides explicit expert knowledge to assist
LLMs in Chinese harmful content detection. In addition, we propose a
knowledge-augmented baseline that integrates both human-annotated knowledge
rules and implicit knowledge from large language models, enabling smaller
models to achieve performance comparable to state-of-the-art LLMs. Code and
data are available at https://github.com/zjunlp/ChineseHarm-bench.

</details>


### [57] [AutoMind: Adaptive Knowledgeable Agent for Automated Data Science](https://arxiv.org/abs/2506.10974)
*Yixin Ou,Yujie Luo,Jingsheng Zheng,Lanning Wei,Shuofei Qiao,Jintian Zhang,Da Zheng,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 本文提出AutoMind框架，通过专家知识库、树搜索算法和自适应编码策略提升LLM代理在数据科学任务中的表现，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动数据科学代理依赖固定流程和编码策略，仅能处理简单问题，无法应对复杂创新任务及整合人类专家经验。

Method: 结合(1)领域专家知识库、(2)知识驱动树搜索算法、(3)动态调整代码生成复杂度的自适应策略，构建灵活LLM代理框架。

Result: 在自动化数据科学基准测试中性能超越SOTA基线，解决方案质量、效率及稳健性均具优势。

Conclusion: AutoMind通过知识整合与动态策略，为全自动化数据科学提供了高效鲁棒的实现路径。

Abstract: Large Language Model (LLM) agents have shown great potential in addressing
real-world data science problems. LLM-driven data science agents promise to
automate the entire machine learning pipeline, yet their real-world
effectiveness remains limited. Existing frameworks depend on rigid, pre-defined
workflows and inflexible coding strategies; consequently, they excel only on
relatively simple, classical problems and fail to capture the empirical
expertise that human practitioners bring to complex, innovative tasks. In this
work, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework
that overcomes these deficiencies through three key advances: (1) a curated
expert knowledge base that grounds the agent in domain expert knowledge, (2) an
agentic knowledgeable tree search algorithm that strategically explores
possible solutions, and (3) a self-adaptive coding strategy that dynamically
tailors code generation to task complexity. Evaluations on two automated data
science benchmarks demonstrate that AutoMind delivers superior performance
versus state-of-the-art baselines. Additional analyses confirm favorable
effectiveness, efficiency, and qualitative solution quality, highlighting
AutoMind as an efficient and robust step toward fully automated data science.

</details>


### [58] [How Well Can Reasoning Models Identify and Recover from Unhelpful Thoughts?](https://arxiv.org/abs/2506.10979)
*Sohee Yang,Sang-Woo Lee,Nora Kassner,Daniela Gottesman,Sebastian Riedel,Mor Geva*

Main category: cs.CL

TL;DR: 研究发现，模型能有效识别大部分无益思维，但在恢复过程中表现不佳，且大模型在短无关思维干扰下表现更差，需提升自我重新评估能力以提高推理安全。


<details>
  <summary>Details</summary>
Motivation: 探究推理模型在自我重新评估中的有效性，尤其是其识别和纠正无益思维（如冗长、误导、无关或错误答案）的能力。

Method: 通过向模型注入四类无益思维（冗长、无关、误导问题、错误答案），测试其识别与恢复能力，并分析模型规模对恢复效果的影响。

Result: 模型能识别无益思维，但恢复能力差，性能显著下降；大模型在短无关思维干扰下表现更差（逆缩放趋势），小模型对有害思维干扰抵抗力更强。

Conclusion: 需改进模型的自我重新评估能力以提升推理准确性和安全性，当前模型缺乏通用“元认知”意识，易受干扰思维持续影响。

Abstract: Recent reasoning models show the ability to reflect, backtrack, and
self-validate their reasoning, which is crucial in spotting mistakes and
arriving at accurate solutions. A natural question that arises is how
effectively models can perform such self-reevaluation. We tackle this question
by investigating how well reasoning models identify and recover from four types
of unhelpful thoughts: uninformative rambling thoughts, thoughts irrelevant to
the question, thoughts misdirecting the question as a slightly different
question, and thoughts that lead to incorrect answers. We show that models are
effective at identifying most unhelpful thoughts but struggle to recover from
the same thoughts when these are injected into their thinking process, causing
significant performance drops. Models tend to naively continue the line of
reasoning of the injected irrelevant thoughts, which showcases that their
self-reevaluation abilities are far from a general "meta-cognitive" awareness.
Moreover, we observe non/inverse-scaling trends, where larger models struggle
more than smaller ones to recover from short irrelevant thoughts, even when
instructed to reevaluate their reasoning. We demonstrate the implications of
these findings with a jailbreak experiment using irrelevant thought injection,
showing that the smallest models are the least distracted by
harmful-response-triggering thoughts. Overall, our findings call for
improvement in self-reevaluation of reasoning models to develop better
reasoning and safer systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [59] [A Conjecture on a Fundamental Trade-Off between Certainty and Scope in Symbolic and Generative AI](https://arxiv.org/abs/2506.10130)
*Luciano Floridi*

Main category: cs.AI

TL;DR: 本文提出一个猜想，认为AI系统的可证明正确性与广泛数据映射能力存在根本性权衡：符号AI可验证无误但领域狭窄，生成式AI能处理高维数据但必然存在错误风险。该猜想从信息论角度重构了AI工程与哲学基础。


<details>
  <summary>Details</summary>
Motivation: 揭示AI领域长期存在的隐含矛盾——追求形式化验证的确定性系统必然牺牲泛化能力，而高泛化模型无法实现零错误，这一权衡需要被显式建模以指导技术发展。

Method: 通过信息论形式化猜想，结合认识论、形式验证和技术哲学的历史脉络分析，运用欠定理论、认知风险框架进行论证。

Result: 建立了AI系统正确性-泛化能力权衡的数学表达，证明两者存在本质互斥关系，并推导出该猜想对混合系统设计、评估标准及伦理责任的影响路径。

Conclusion: 该猜想若成立，将重构可信AI的发展范式：需放弃'全能系统'幻想，建立分层的验证标准，并通过哲学反思重新定位技术伦理责任边界。

Abstract: This article introduces a conjecture that formalises a fundamental trade-off
between provable correctness and broad data-mapping capacity in Artificial
Intelligence (AI) systems. When an AI system is engineered for deductively
watertight guarantees (demonstrable certainty about the error-free nature of
its outputs) -- as in classical symbolic AI -- its operational domain must be
narrowly circumscribed and pre-structured. Conversely, a system that can input
high-dimensional data to produce rich information outputs -- as in contemporary
generative models -- necessarily relinquishes the possibility of zero-error
performance, incurring an irreducible risk of errors or misclassification. By
making this previously implicit trade-off explicit and open to rigorous
verification, the conjecture significantly reframes both engineering ambitions
and philosophical expectations for AI. After reviewing the historical
motivations for this tension, the article states the conjecture in
information-theoretic form and contextualises it within broader debates in
epistemology, formal verification, and the philosophy of technology. It then
offers an analysis of its implications and consequences, drawing on notions of
underdetermination, prudent epistemic risk, and moral responsibility. The
discussion clarifies how, if correct, the conjecture would help reshape
evaluation standards, governance frameworks, and hybrid system design. The
conclusion underscores the importance of eventually proving or refuting the
inequality for the future of trustworthy AI.

</details>


### [60] [One Patient, Many Contexts: Scaling Medical AI Through Contextual Intelligence](https://arxiv.org/abs/2506.10157)
*Michelle M. Li,Ben Y. Reis,Adam Rodman,Tianxi Cai,Noa Dagan,Ran D. Balicer,Joseph Loscalzo,Isaac S. Kohane,Marinka Zitnik*

Main category: cs.AI

TL;DR: 医学基础模型在适应新场景时存在动态调整能力不足的问题，提出上下文切换AI愿景以实现无需重新训练的动态适应，提升跨专科、跨区域的医疗应用。


<details>
  <summary>Details</summary>
Motivation: 现有医学AI模型依赖微调或知识库，难以动态适应不同患者、专科及临床情境，导致忽略关键上下文信息并产生错误。

Method: 提出上下文切换（context-switching）框架，使模型无需重新训练即可动态调整推理逻辑，适应不同医疗场景、人群及工作流程。

Result: 理论构想表明，上下文切换AI可扩展至多疾病诊断与治疗，增强模型对未训练情境的泛化能力，并提高医疗服务的可及性。

Conclusion: 动态上下文切换能力是医疗AI发展的关键方向，可减少情境相关错误，推动模型在复杂临床环境中的实际应用。

Abstract: Medical foundation models, including language models trained on clinical
notes, vision-language models on medical images, and multimodal models on
electronic health records, can summarize clinical notes, answer medical
questions, and assist in decision-making. Adapting these models to new
populations, specialties, or settings typically requires fine-tuning, careful
prompting, or retrieval from knowledge bases. This can be impractical, and
limits their ability to interpret unfamiliar inputs and adjust to clinical
situations not represented during training. As a result, models are prone to
contextual errors, where predictions appear reasonable but fail to account for
critical patient-specific or contextual information. These errors stem from a
fundamental limitation that current models struggle with: dynamically adjusting
their behavior across evolving contexts of medical care. In this Perspective,
we outline a vision for context-switching in medical AI: models that
dynamically adapt their reasoning without retraining to new specialties,
populations, workflows, and clinical roles. We envision context-switching AI to
diagnose, manage, and treat a wide range of diseases across specialties and
regions, and expand access to medical care.

</details>


### [61] [Correlation vs causation in Alzheimer's disease: an interpretability-driven study](https://arxiv.org/abs/2506.10179)
*Hamzah Dabool,Raghad Mustafa*

Main category: cs.AI

TL;DR: 本研究通过结合相关性分析、机器学习及可解释性技术，探讨阿尔茨海默病（AD）中临床、认知、遗传和生物标志物特征的关系，强调需区分因果关系与相关性以改进诊断和治疗。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病研究中，因果关系与相关性的混淆可能影响诊断、治疗及病理机制识别。明确区分两者对揭示真实驱动因素至关重要。

Method: 使用XGBoost算法进行特征重要性分类，结合相关性矩阵分析变量集群，并利用SHAP值解释不同疾病阶段的特征贡献。

Result: 发现认知评分和遗传风险为AD分类关键特征；强相关性不必然代表因果，需谨慎解释关联数据。

Conclusion: 整合特征重要性与统计分析方法为未来因果推断研究奠定基础，区分因果与相关标志物可提升早期诊断及靶向干预效果。

Abstract: Understanding the distinction between causation and correlation is critical
in Alzheimer's disease (AD) research, as it impacts diagnosis, treatment, and
the identification of true disease drivers. This experiment investigates the
relationships among clinical, cognitive, genetic, and biomarker features using
a combination of correlation analysis, machine learning classification, and
model interpretability techniques. Employing the XGBoost algorithm, we
identified key features influencing AD classification, including cognitive
scores and genetic risk factors. Correlation matrices revealed clusters of
interrelated variables, while SHAP (SHapley Additive exPlanations) values
provided detailed insights into feature contributions across disease stages.
Our results highlight that strong correlations do not necessarily imply
causation, emphasizing the need for careful interpretation of associative data.
By integrating feature importance and interpretability with classical
statistical analysis, this work lays groundwork for future causal inference
studies aimed at uncovering true pathological mechanisms. Ultimately,
distinguishing causal factors from correlated markers can lead to improved
early diagnosis and targeted interventions for Alzheimer's disease.

</details>


### [62] [Towards Responsible AI: Advances in Safety, Fairness, and Accountability of Autonomous Systems](https://arxiv.org/abs/2506.10192)
*Filip Cano*

Main category: cs.AI

TL;DR: 该论文提出了一种综合框架，通过增强AI系统的安全性、公平性、透明度和问责制，推动可信AI的发展。具体方法包括改进安全屏蔽技术、引入公平盾后处理、量化意图分析指标，并整合为“反应式决策”框架。


<details>
  <summary>Details</summary>
Motivation: 随着自主系统在关键社会领域的影响扩大，确保AI的可信性成为迫切需求。然而，现有可信AI概念宽泛且碎片化，需在安全、公平、透明和问责等维度建立系统性解决方案。

Method: 1. 扩展确定性屏蔽技术以应对现实延迟观测，应用于自动驾驶防撞模拟；2. 提出公平盾后处理方法，在序列决策中强制群体公平；3. 建立概率代理的意图商数量化指标，支持责任追溯；4. 整合为统一的形式化反应式决策框架。

Result: 安全盾技术通过模拟验证可有效防止车辆碰撞；公平盾以最小干预成本实现公平约束；意图商数支持事故责任分析。框架整合了多维度可信AI要素。

Conclusion: 研究通过具体技术路径推进了可信AI的实践落地，为构建安全、公平、可追责的AI系统奠定基础，并为后续研究提供统一的形式化框架。

Abstract: Ensuring responsible use of artificial intelligence (AI) has become
imperative as autonomous systems increasingly influence critical societal
domains. However, the concept of trustworthy AI remains broad and
multi-faceted. This thesis advances knowledge in the safety, fairness,
transparency, and accountability of AI systems. In safety, we extend classical
deterministic shielding techniques to become resilient against delayed
observations, enabling practical deployment in real-world conditions. We also
implement both deterministic and probabilistic safety shields into simulated
autonomous vehicles to prevent collisions with road users, validating the use
of these techniques in realistic driving simulators. We introduce fairness
shields, a novel post-processing approach to enforce group fairness in
sequential decision-making settings over finite and periodic time horizons. By
optimizing intervention costs while strictly ensuring fairness constraints,
this method efficiently balances fairness with minimal interference. For
transparency and accountability, we propose a formal framework for assessing
intentional behaviour in probabilistic decision-making agents, introducing
quantitative metrics of agency and intention quotient. We use these metrics to
propose a retrospective analysis of intention, useful for determining
responsibility when autonomous systems cause unintended harm. Finally, we unify
these contributions through the ``reactive decision-making'' framework,
providing a general formalization that consolidates previous approaches.
Collectively, the advancements presented contribute practically to the
realization of safer, fairer, and more accountable AI systems, laying the
foundations for future research in trustworthy AI.

</details>


### [63] [WGSR-Bench: Wargame-based Game-theoretic Strategic Reasoning Benchmark for Large Language Models](https://arxiv.org/abs/2506.10264)
*Qiyue Yin,Pei Xu,Qiaozhe Li,Shengda Liu,Shengqi Shen,Tong Wang,Yihong Han,Xiaonan Zhao,Likun Yang,Shiyue Cao,Shiyu Qiu,Yuxuan Liu,Shizhao Yu,Lei Cui,Chengxin Yan,Jie Sun,Xiangquan Tang,Kaiqi Huang*

Main category: cs.AI

TL;DR: 本文提出首个基于兵棋推演的LLM战略推理基准WGSR-Bench，通过环境态势感知、对手风险建模和策略生成三大任务，系统评估大语言模型在多智能体动态博弈中的战略推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在数学/符号/常识推理方面表现突出，但作为高级认知核心的战略推理（多智能体动态环境下的行为评估、策略制定与调整）尚未被系统评估与建模。

Method: 构建WGSR-Bench基准，以高复杂度的兵棋推演为测试环境，设计S-POE架构（环境感知-对手建模-策略生成）三大核心任务，并开发LLM驱动的兵棋智能体进行综合评估。

Result: 通过该基准可系统评估主流LLM在博弈论战略推理中的优势与局限，例如多智能体决策、意图推理和反事实推理等能力。

Conclusion: WGSR-Bench填补了LLM战略推理评估的空白，为推进大模型驱动的战略智能研究提供系统性测试框架，并揭示现有模型的战略认知边界。

Abstract: Recent breakthroughs in Large Language Models (LLMs) have led to a
qualitative leap in artificial intelligence' s performance on reasoning tasks,
particularly demonstrating remarkable capabilities in mathematical, symbolic,
and commonsense reasoning. However, as a critical component of advanced human
cognition, strategic reasoning, i.e., the ability to assess multi-agent
behaviors in dynamic environments, formulate action plans, and adapt
strategies, has yet to be systematically evaluated or modeled. To address this
gap, this paper introduces WGSR-Bench, the first strategy reasoning benchmark
for LLMs using wargame as its evaluation environment. Wargame, a quintessential
high-complexity strategic scenario, integrates environmental uncertainty,
adversarial dynamics, and non-unique strategic choices, making it an effective
testbed for assessing LLMs' capabilities in multi-agent decision-making, intent
inference, and counterfactual reasoning. WGSR-Bench designs test samples around
three core tasks, i.e., Environmental situation awareness, Opponent risk
modeling and Policy generation, which serve as the core S-POE architecture, to
systematically assess main abilities of strategic reasoning. Finally, an
LLM-based wargame agent is designed to integrate these parts for a
comprehensive strategy reasoning assessment. With WGSR-Bench, we hope to assess
the strengths and limitations of state-of-the-art LLMs in game-theoretic
strategic reasoning and to advance research in large model-driven strategic
intelligence.

</details>


### [64] [Closer to Language than Steam: AI as the Cognitive Engine of a New Productivity Revolution](https://arxiv.org/abs/2506.10281)
*Xinmin Fang,Lingfeng Tao,Zhengxiong Li*

Main category: cs.AI

TL;DR: 本文提出AI应被视作认知革命引擎，类比书面语言对智力的革新，而非传统机械化工具。通过多学科视角分析AI如何推动知识生产力，强调需重构技能、组织与政策以适应新范式。


<details>
  <summary>Details</summary>
Motivation: 重新定义AI的角色，突破工业革命机械化框架，将其定位为人类认知能力的扩展，以揭示其对知识工作和社会结构的根本性变革。

Method: 采用跨学科方法（计算机科学、经济学、社会学），结合历史信息技术飞跃案例与多领域实证，构建AI作为生产力认知引擎的理论框架。

Result: 论证AI通过增强认知劳动实现生产力范式转移，提出从体力生产向认知生产转型的可视化模型，并识别技能体系与组织形态的适应性挑战。

Conclusion: AI作为认知革命引擎，其核心价值在于与人类智力协同进化，标志着生产力发展进入以认知增强为核心的新阶段，需系统性社会变革支持。

Abstract: Artificial Intelligence (AI) is reframed as a cognitive engine driving a
novel productivity revolution distinct from the Industrial Revolution's
physical thrust. This paper develops a theoretical framing of AI as a cognitive
revolution akin to written language - a transformative augmentation of human
intellect rather than another mechanized tool. We compare AI's emergence to
historical leaps in information technology to show how it amplifies knowledge
work. Examples from various domains demonstrate AI's impact as a driver of
productivity in cognitive tasks. We adopt a multidisciplinary perspective
combining computer science advances with economic insights and sociological
perspectives on how AI reshapes work and society. Through conceptual
frameworks, we visualize the shift from manual to cognitive productivity. Our
central argument is that AI functions as an engine of cognition - comparable to
how human language revolutionized knowledge - heralding a new productivity
paradigm. We discuss how this revolution demands rethinking of skills,
organizations, and policies. This paper, balancing academic rigor with clarity,
concludes that AI's promise lies in complementing human cognitive abilities,
marking a new chapter in productivity evolution.

</details>


### [65] [The Alignment Trap: Complexity Barriers](https://arxiv.org/abs/2506.10304)
*Jasper Yao*

Main category: cs.AI

TL;DR: 本文通过计算复杂性理论分析，证明随着AI系统表达能力超过临界值，安全性验证将变得指数级困难且不可行，揭示了AI能力提升与安全需求之间的根本矛盾，并提出了必须面对的战略三难选择。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于AI系统能力持续增强时，社会安全需求趋向完美化与验证计算复杂性之间的根本矛盾。需要建立理论框架分析AI对齐问题的本质计算限制。

Method: 采用计算复杂性理论框架，形式化定义能力-风险扩展(CRS)动态，通过四个核心定理的数学证明（包括coNP完全性分析、策略空间测度估计、对齐技术局限性证明等），结合神经网络测度理论分析。

Result: 1) 验证复杂度随表达能力指数增长 2) 安全策略占比≤2^{-2^m} 3) 现有技术无法实现普适对齐 4) 神经网络安全属性测度为零。确立'难处理间隙'现象，即实用安全需求位于计算不可行区域。

Conclusion: 提出战略三难困境：限制系统复杂度保持可验证安全/接受不可验证风险/发展超越验证的新范式。首次系统建立AI对齐的复杂性理论边界，为所有安全方法设定必须突破的严格理论界限。

Abstract: We establish fundamental computational complexity barriers to verifying AI
safety as system capabilities scale. Our main results show that for AI systems
with expressiveness EXP$(m)$ above a critical threshold $\tau$, safety
verification requires exponential time and is coNP-complete. We formalize the
Capability-Risk Scaling (CRS) dynamic, which demonstrates how increasing AI
capability drives societal safety requirements toward perfection, creating an
inescapable tension with verification complexity. Through four core theorems,
we prove that (1) verification complexity grows exponentially with system
expressiveness, (2) safe policies comprise at most a $2^{-2^m}$ fraction of the
policy space, (3) no finite set of alignment techniques can provide universal
coverage, and (4) robust safety properties form measure-zero sets for neural
networks. These results characterize an "intractability gap" where practical
safety requirements fall within the region of computational intractability. We
conclude by presenting a strategic trilemma: AI development must either
constrain system complexity to maintain verifiable safety, accept unverifiable
risks while scaling capabilities, or develop fundamentally new safety paradigms
beyond verification. Our work provides the first systematic
complexity-theoretic analysis of AI alignment and establishes rigorous bounds
that any safety approach must confront. A formal verification of the core
theorems in Lean4 is currently in progress.

</details>


### [66] [A Benchmark for Generalizing Across Diverse Team Strategies in Competitive Pokémon](https://arxiv.org/abs/2506.10326)
*Cameron Angliss,Jiaxun Cui,Jiaheng Hu,Arrasy Rahman,Peter Stone*

Main category: cs.AI

TL;DR: 论文提出VGC-Bench基准，用于评估AI代理在宝可梦VGC复杂团队配置下的策略泛化能力，发现现有方法在单团队配置表现良好但扩展性不足。


<details>
  <summary>Details</summary>
Motivation: 宝可梦VGC的团队配置组合空间（约10^139）远超Dota等游戏，其离散组合特性导致策略高度依赖团队配置，现有方法难以实现跨团队泛化。

Method: 构建VGC-Bench基准，整合人类对战数据集，并测试包括大语言模型代理、行为克隆、强化学习及博弈论方法（自博弈、虚拟博弈等）的基线方法。

Result: 单团队配置下最优方法可击败职业选手，但团队规模扩大时性能显著下降，策略跨团队泛化仍为未解难题。

Conclusion: 尽管单团队场景取得进展，AI代理在复杂多团队策略环境中的泛化能力仍是开放挑战，需社区进一步研究。

Abstract: Developing AI agents that can robustly adapt to dramatically different
strategic landscapes without retraining is a central challenge for multi-agent
learning. Pok\'emon Video Game Championships (VGC) is a domain with an
extraordinarily large space of possible team configurations of approximately
$10^{139}$ - far larger than those of Dota or Starcraft. The highly discrete,
combinatorial nature of team building in Pok\'emon VGC causes optimal
strategies to shift dramatically depending on both the team being piloted and
the opponent's team, making generalization uniquely challenging. To advance
research on this problem, we introduce VGC-Bench: a benchmark that provides
critical infrastructure, standardizes evaluation protocols, and supplies
human-play datasets and a range of baselines - from large-language-model agents
and behavior cloning to reinforcement learning and empirical game-theoretic
methods such as self-play, fictitious play, and double oracle. In the
restricted setting where an agent is trained and evaluated on a single-team
configuration, our methods are able to win against a professional VGC
competitor. We extensively evaluated all baseline methods over progressively
larger team sets and find that even the best-performing algorithm in the
single-team setting struggles at scaling up as team size grows. Thus, policy
generalization across diverse team strategies remains an open challenge for the
community. Our code is open sourced at
https://github.com/cameronangliss/VGC-Bench.

</details>


### [67] [Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable Task Experts](https://arxiv.org/abs/2506.10357)
*Zaijing Li,Yuquan Xie,Rui Shao,Gongwei Chen,Weili Guan,Dongmei Jiang,Liqiang Nie*

Main category: cs.AI

TL;DR: 本文提出了一种名为Optimus-3的通用智能体，通过知识增强数据生成、任务级路由的混合专家架构（MoE）和多模态推理增强强化学习，解决了Minecraft开放世界中多任务干扰、数据不足和视觉多样性问题。实验表明其性能超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于多模态大模型（MLLM）的智能体在开放世界（如Minecraft）中面临领域数据不足、异构任务干扰和视觉多样性三大挑战，需构建具备感知、规划、行动等能力的通用智能体。

Method: 1) 知识增强数据生成管道提供高质量训练数据；2) 基于任务级路由的混合专家架构（MoE）减少异构任务干扰；3) 多模态推理增强强化学习方法提升视觉多样性场景的推理能力。

Result: Optimus-3在Minecraft环境中广泛任务测试表现优异，超越通用多模态大模型及现有最先进智能体。

Conclusion: 通过创新性数据生成、架构设计和强化学习，Optimus-3成功解决了开放世界智能体的核心挑战，为通用智能体研究提供了新方向。

Abstract: Recently, agents based on multimodal large language models (MLLMs) have
achieved remarkable progress across various domains. However, building a
generalist agent with capabilities such as perception, planning, action,
grounding, and reflection in open-world environments like Minecraft remains
challenges: insufficient domain-specific data, interference among heterogeneous
tasks, and visual diversity in open-world settings. In this paper, we address
these challenges through three key contributions. 1) We propose a
knowledge-enhanced data generation pipeline to provide scalable and
high-quality training data for agent development. 2) To mitigate interference
among heterogeneous tasks, we introduce a Mixture-of-Experts (MoE) architecture
with task-level routing. 3) We develop a Multimodal Reasoning-Augmented
Reinforcement Learning approach to enhance the agent's reasoning ability for
visual diversity in Minecraft. Built upon these innovations, we present
Optimus-3, a general-purpose agent for Minecraft. Extensive experimental
results demonstrate that Optimus-3 surpasses both generalist multimodal large
language models and existing state-of-the-art agents across a wide range of
tasks in the Minecraft environment. Project page:
https://cybertronagent.github.io/Optimus-3.github.io/

</details>


### [68] [NeuroPAL: Punctuated Anytime Learning with Neuroevolution for Macromanagement in Starcraft: Brood War](https://arxiv.org/abs/2506.10384)
*Jim O'Connor,Yeonghun Lee,Gary B Parker*

Main category: cs.AI

TL;DR: 本文提出NeuroPAL框架，通过结合神经进化拓扑增强（NEAT）与间断式即时学习（PAL），在《星际争霸：母巢之战》的宏观管理任务中显著提升训练效率，使智能体在更短时间达到人类专家级策略水平。


<details>
  <summary>Details</summary>
Motivation: 传统《星际争霸》AI方法（如规则系统或监督深度学习）存在适应性与计算效率瓶颈。研究旨在通过改进神经进化方法，解决复杂实时策略环境中长期规划难题。

Method: 提出NeuroPAL框架：将NEAT与PAL结合，通过交替进行高频低精度训练与周期性高精度评估，优化进化训练效率。实验采用固定地图单种族场景验证。

Result: PAL结合的NEAT训练速度提升约50%，且智能体涌现出人类专家级策略（如前置兵营、防御建筑优化）。训练时间减半即达到竞争水平。

Conclusion: 结构化评估机制（如PAL）可增强神经进化在复杂实时策略环境中的扩展性与有效性，为AI在RTS领域的策略探索提供新方向。

Abstract: StarCraft: Brood War remains a challenging benchmark for artificial
intelligence research, particularly in the domain of macromanagement, where
long-term strategic planning is required. Traditional approaches to StarCraft
AI rely on rule-based systems or supervised deep learning, both of which face
limitations in adaptability and computational efficiency. In this work, we
introduce NeuroPAL, a neuroevolutionary framework that integrates
Neuroevolution of Augmenting Topologies (NEAT) with Punctuated Anytime Learning
(PAL) to improve the efficiency of evolutionary training. By alternating
between frequent, low-fidelity training and periodic, high-fidelity
evaluations, PAL enhances the sample efficiency of NEAT, enabling agents to
discover effective strategies in fewer training iterations. We evaluate
NeuroPAL in a fixed-map, single-race scenario in StarCraft: Brood War and
compare its performance to standard NEAT-based training. Our results show that
PAL significantly accelerates the learning process, allowing the agent to reach
competitive levels of play in approximately half the training time required by
NEAT alone. Additionally, the evolved agents exhibit emergent behaviors such as
proxy barracks placement and defensive building optimization, strategies
commonly used by expert human players. These findings suggest that structured
evaluation mechanisms like PAL can enhance the scalability and effectiveness of
neuroevolution in complex real-time strategy environments.

</details>


### [69] [Mirage-1: Augmenting and Updating GUI Agent with Hierarchical Multimodal Skills](https://arxiv.org/abs/2506.10387)
*Yuquan Xie,Zaijing Li,Rui Shao,Gongwei Chen,Kaiwen Zhou,Yinchuan Li,Dongmei Jiang,Liqiang Nie*

Main category: cs.AI

TL;DR: 提出Mirage-1多模态GUI代理，通过分层技能模块HMS和SA-MCTS算法解决长时任务规划与跨域差距问题，在多个基准测试中显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM作为GUI代理在在线长时任务中存在知识不足和离线/在线领域差距问题，需模拟人类知识泛化机制进行改进。

Method: 1) 分层多模态技能(HMS)构建元技能-核心技能-执行技能的三层知识体系；2) 技能增强蒙特卡洛树搜索(SA-MCTS)利用离线技能缩小在线动作空间；3) 构建跨平台即插即用代理Mirage-1及新基准AndroidLH。

Result: 在AndroidWorld、MobileMiniWob++、Mind2Web-Live和AndroidLH基准上分别提升32%、19%、15%和79%，显著突破长时任务性能瓶颈。

Conclusion: HMS层级知识抽象与SA-MCTS跨域技能迁移有效提升GUI代理的长时任务处理能力，Mirage-1验证了方法在真实场景的优越性。

Abstract: Recent efforts to leverage the Multi-modal Large Language Model (MLLM) as GUI
agents have yielded promising outcomes. However, these agents still struggle
with long-horizon tasks in online environments, primarily due to insufficient
knowledge and the inherent gap between offline and online domains. In this
paper, inspired by how humans generalize knowledge in open-ended environments,
we propose a Hierarchical Multimodal Skills (HMS) module to tackle the issue of
insufficient knowledge. It progressively abstracts trajectories into execution
skills, core skills, and ultimately meta-skills, providing a hierarchical
knowledge structure for long-horizon task planning. To bridge the domain gap,
we propose the Skill-Augmented Monte Carlo Tree Search (SA-MCTS) algorithm,
which efficiently leverages skills acquired in offline environments to reduce
the action search space during online tree exploration. Building on HMS, we
propose Mirage-1, a multimodal, cross-platform, plug-and-play GUI agent. To
validate the performance of Mirage-1 in real-world long-horizon scenarios, we
constructed a new benchmark, AndroidLH. Experimental results show that Mirage-1
outperforms previous agents by 32\%, 19\%, 15\%, and 79\% on AndroidWorld,
MobileMiniWob++, Mind2Web-Live, and AndroidLH, respectively. Project page:
https://cybertronagent.github.io/Mirage-1.github.io/

</details>


### [70] [Reasoning RAG via System 1 or System 2: A Survey on Reasoning Agentic Retrieval-Augmented Generation for Industry Challenges](https://arxiv.org/abs/2506.10408)
*Jintao Liang,Gang Su,Huifeng Lin,You Wu,Rui Zhao,Ziyue Li*

Main category: cs.AI

TL;DR: 本文综述了推理代理式检索增强生成（RAG）的演进，提出两种系统分类（预定义推理与自主推理），分析其技术架构与挑战，并展望未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统静态RAG系统在复杂推理、动态检索及多模态场景中存在局限，需通过引入决策与自适应工具提升其灵活性与实用性。

Method: 将推理代理式RAG分为预定义推理（固定模块化流程）与自主代理推理（模型动态协调工具交互），分析代表性技术的架构、推理策略及工具协作机制。

Result: 系统梳理了两类方法的优缺点，并汇总相关研究资源（GitHub仓库），为后续研究提供结构化参考。

Conclusion: 未来需增强推理代理式RAG的灵活性、鲁棒性及多模态适应能力，以应对复杂现实场景的挑战。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to
overcome the knowledge limitations of Large Language Models (LLMs) by
integrating external retrieval with language generation. While early RAG
systems based on static pipelines have shown effectiveness in well-structured
tasks, they struggle in real-world scenarios requiring complex reasoning,
dynamic retrieval, and multi-modal integration. To address these challenges,
the field has shifted toward Reasoning Agentic RAG, a paradigm that embeds
decision-making and adaptive tool use directly into the retrieval process. In
this paper, we present a comprehensive review of Reasoning Agentic RAG methods,
categorizing them into two primary systems: predefined reasoning, which follows
fixed modular pipelines to boost reasoning, and agentic reasoning, where the
model autonomously orchestrates tool interaction during inference. We analyze
representative techniques under both paradigms, covering architectural design,
reasoning strategies, and tool coordination. Finally, we discuss key research
challenges and propose future directions to advance the flexibility,
robustness, and applicability of reasoning agentic RAG systems. Our collection
of the relevant research has been organized into a
https://github.com/ByebyeMonica/Reasoning-Agentic-RAG.

</details>


### [71] [Multi-dimensional Autoscaling of Processing Services: A Comparison of Agent-based Methods](https://arxiv.org/abs/2506.10420)
*Boris Sedlak,Alireza Furutanpey,Zihang Wang,Víctor Casamayor Pujol,Schahram Dustdar*

Main category: cs.AI

TL;DR: 本文提出一种基于代理的多维弹性伸缩框架，用于资源受限的边缘计算环境，通过动态调整硬件资源和内部配置，比较四种代理方法在YOLOv8和OpenCV并行场景下的表现，验证了多维代理伸缩的可行性。


<details>
  <summary>Details</summary>
Motivation: 传统边缘计算自动伸缩受限于严格约束条件，需通过多维弹性行为提升灵活性以满足环境约束下的需求。

Method: 采用基于代理的自动伸缩框架，结合硬件资源和内部服务配置动态调整，对比Active Inference、深度Q网络、结构知识分析和深度主动推理四种代理方法，使用YOLOv8视觉识别与OpenCV二维码检测服务进行实验验证。

Result: 所有代理均达到可接受的SLO性能，深度Q网络受益预训练，结构分析快速收敛，深度主动推理代理兼具理论基础与扩展优势。

Conclusion: 研究证明多维代理自动伸缩在边缘环境中的有效性，其收敛模式差异为不同场景提供选择依据，为后续研究提供实践基础。

Abstract: Edge computing breaks with traditional autoscaling due to strict resource
constraints, thus, motivating more flexible scaling behaviors using multiple
elasticity dimensions. This work introduces an agent-based autoscaling
framework that dynamically adjusts both hardware resources and internal service
configurations to maximize requirements fulfillment in constrained
environments. We compare four types of scaling agents: Active Inference, Deep Q
Network, Analysis of Structural Knowledge, and Deep Active Inference, using two
real-world processing services running in parallel: YOLOv8 for visual
recognition and OpenCV for QR code detection. Results show all agents achieve
acceptable SLO performance with varying convergence patterns. While the Deep Q
Network benefits from pre-training, the structural analysis converges quickly,
and the deep active inference agent combines theoretical foundations with
practical scalability advantages. Our findings provide evidence for the
viability of multi-dimensional agent-based autoscaling for edge environments
and encourage future work in this research direction.

</details>


### [72] [OIBench: Benchmarking Strong Reasoning Models with Olympiad in Informatics](https://arxiv.org/abs/2506.10481)
*Yaoming Zhu,Junxin Wang,Yiyang Li,Lin Qiu,ZongYu Wang,Jun Xu,Xuezhi Cao,Yuhuai Wei,Mingshi Wang,Xunliang Cai,Rong Ma*

Main category: cs.AI

TL;DR: 本文提出OIBench，一个高质量、抗污染的信息学奥赛级别数据集，用于评估大语言模型的算法推理能力。实验显示当前SOTA模型已超越多数人类表现，但仍未达最优解。


<details>
  <summary>Details</summary>
Motivation: 传统算法基准测试逐渐饱和，需更具挑战性的基准推动算法推理能力的提升。

Method: 构建包含多编程范式的250题私有数据集OIBench，提出时间/空间完成曲线进行细粒度效率分析，并通过人类参与者对比实验验证抗污染性。

Result: 开源模型落后于闭源模型，但SOTA模型在正确性和效率上已超越多数人类，仍逊于标准解法。

Conclusion: 开源OIBench可促进未来LLM代码推理能力发展，当前模型虽优于人类，仍有改进空间。

Abstract: As models become increasingly sophisticated, conventional algorithm
benchmarks are increasingly saturated, underscoring the need for more
challenging benchmarks to guide future improvements in algorithmic reasoning.
This paper introduces OIBench, a high-quality, private, and challenging
olympiad-level informatics dataset comprising 250 carefully curated original
problems. We detail the construction methodology of the benchmark, ensuring a
comprehensive assessment across various programming paradigms and complexities,
and we demonstrate its contamination-resistant properties via experiments. We
propose Time/Space Completion Curves for finer-grained efficiency analysis and
enable direct human-model comparisons through high-level participant
evaluations. Our experiments reveal that while open-source models lag behind
closed-source counterparts, current SOTA models already outperform most human
participants in both correctness and efficiency, while still being suboptimal
compared to the canonical solutions. By releasing OIBench as a fully
open-source resource (https://huggingface.co/datasets/AGI-Eval/OIBench), we
hope this benchmark will contribute to advancing code reasoning capabilities
for future LLMs.

</details>


### [73] [Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning](https://arxiv.org/abs/2506.10521)
*Yuhao Zhou,Yiheng Wang,Xuming He,Ruoyao Xiao,Zhiwei Li,Qiantai Feng,Zijie Guo,Yuejin Yang,Hao Wu,Wenxuan Huang,Jiaqi Wei,Dan Si,Xiuqi Yao,Jia Bu,Haiwen Huang,Tianfan Fu,Shixiang Tang,Ben Fei,Dongzhan Zhou,Fenghua Ling,Yan Lu,Siqi Sun,Chenhui Li,Guanjie Zheng,Jiancheng Lv,Wenlong Zhang,Lei Bai*

Main category: cs.AI

TL;DR: 本文提出科学家首测（SFE）基准，用于评估多模态大语言模型（MLLMs）在科学信号感知、属性理解和比较推理三层次认知能力，发现当前顶尖模型表现不足，需进一步改进。


<details>
  <summary>Details</summary>
Motivation: 现有科学基准主要关注MLLMs的知识理解能力，但对其科学感知与推理能力的评估不足，阻碍了AI在科学发现中的实际应用。

Method: 构建SFE基准，包含830个专家验证的多学科VQA问题，覆盖科学信号感知、属性理解和比较推理三个层次，涉及5个高价值学科领域的66项多模态任务。

Result: 实验显示当前最优模型GPT-o3和InternVL-3在SFE上仅达34.08%和26.52%准确率，表明科学领域MLLMs仍有显著提升空间。

Conclusion: SFE基准揭示了MLLMs在科学认知能力的局限性，其评估框架可为AI增强科学发现提供改进方向。

Abstract: Scientific discoveries increasingly rely on complex multimodal reasoning
based on information-intensive scientific data and domain-specific expertise.
Empowered by expert-level scientific benchmarks, scientific Multimodal Large
Language Models (MLLMs) hold the potential to significantly enhance this
discovery process in realistic workflows. However, current scientific
benchmarks mostly focus on evaluating the knowledge understanding capabilities
of MLLMs, leading to an inadequate assessment of their perception and reasoning
abilities. To address this gap, we present the Scientists' First Exam (SFE)
benchmark, designed to evaluate the scientific cognitive capacities of MLLMs
through three interconnected levels: scientific signal perception, scientific
attribute understanding, scientific comparative reasoning. Specifically, SFE
comprises 830 expert-verified VQA pairs across three question types, spanning
66 multimodal tasks across five high-value disciplines. Extensive experiments
reveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08%
and 26.52% on SFE, highlighting significant room for MLLMs to improve in
scientific realms. We hope the insights obtained in SFE will facilitate further
developments in AI-enhanced scientific discoveries.

</details>


### [74] [LogiPlan: A Structured Benchmark for Logical Planning and Relational Reasoning in LLMs](https://arxiv.org/abs/2506.10527)
*Yanan Cai,Ahmed Salem,Besmira Nushi,Mark Russinovich*

Main category: cs.AI

TL;DR: LogiPlan是一个评估大语言模型在复杂关系结构上逻辑规划能力的新基准，通过动态调整任务复杂度，测试模型在计划构建、一致性检测和关系比较中的表现，发现模型规模与性能相关且在深层逻辑推理上存在不足。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在需要处理结构化关系图（如知识图谱、网络基础设施）的应用中，缺乏对逻辑规划能力的系统性评估工具。LogiPlan旨在填补这一空白，为模型在复杂关系推理中的表现提供细粒度分析。

Method: 提出包含三个任务的评估框架：1) 生成符合约束的定向关系图；2) 检测关系结构不一致性；3) 判断查询关系的有效性。通过控制对象数量、关系链深度动态调节难度，并引入自我修正机制验证模型优化能力。

Result: 测试包括GPT-4o、Claude 3.7等9个先进模型，发现：模型性能与规模/架构显著相关；增强推理模型在简单任务表现良好，但在需深度逻辑链的复杂场景中成功率骤降（如O3-mini在深度≥5时准确率低于20%）。

Conclusion: 当前大语言模型在浅层关系推理上取得进展，但面对需要多步逻辑规划的场景仍存在根本性局限，表明逻辑规划能力尚未被现有架构充分掌握，需进一步研究突破。

Abstract: We introduce LogiPlan, a novel benchmark designed to evaluate the
capabilities of large language models (LLMs) in logical planning and reasoning
over complex relational structures. Logical relational reasoning is important
for applications that may rely on LLMs to generate and query structured graphs
of relations such as network infrastructure, knowledge bases, or business
process schema. Our framework allows for dynamic variation of task complexity
by controlling the number of objects, relations, and the minimum depth of
relational chains, providing a fine-grained assessment of model performance
across difficulty levels. LogiPlan encompasses three complementary tasks: (1)
Plan Generation, where models must construct valid directed relational graphs
meeting specified structural constraints; (2) Consistency Detection, testing
models' ability to identify inconsistencies in relational structures; and (3)
Comparison Question, evaluating models' capacity to determine the validity of
queried relationships within a given graph. Additionally, we assess models'
self-correction capabilities by prompting them to verify and refine their
initial solutions. We evaluate state-of-the-art models including DeepSeek R1,
Gemini 2.0 Pro, Gemini 2 Flash Thinking, GPT-4.5, GPT-4o, Llama 3.1 405B,
O3-mini, O1, and Claude 3.7 Sonnet across these tasks, revealing significant
performance gaps that correlate with model scale and architecture. Our analysis
demonstrates that while recent reasoning-enhanced models show promising results
on simpler instances, they struggle with more complex configurations requiring
deeper logical planning.

</details>


### [75] [Primender Sequence: A Novel Mathematical Construct for Testing Symbolic Inference and AI Reasoning](https://arxiv.org/abs/2506.10585)
*Mohd Anwar Jamal Faiz*

Main category: cs.AI

TL;DR: 本文提出了一种结合素数与模数条件的Primender序列，用于评估大语言模型（LLM）的符号推理能力，并通过结构化框架测试多个先进模型的表现。


<details>
  <summary>Details</summary>
Motivation: 研究旨在创建可解释的规则化测试基准，以评估LLM推断隐藏规则、验证数学假设及泛化符号逻辑的能力。

Method: 设计Primender序列及其假设，构建结构化提示框架，测试ChatGPT等模型在规则推断、假设验证和序列生成任务中的表现。

Result: 通过规则推断准确率、假设评估等指标，系统性比较了不同LLM在符号推理与数学逻辑泛化方面的性能差异。

Conclusion: Primender序列及评估方法为数论与AI交叉领域提供了可复现的基准工具，推动LLM符号推理能力的标准化测试。

Abstract: This paper introduces the Primender sequence, a novel integer sequence
defined by a hybrid rule that combines classical primality with modular
digit-based conditions. Specifically, a number n is included in the sequence if
it is prime or ends with a prime number of unit digit or any length. In other
words, numbers which are primes or have at least one prime suffix. The
resulting sequence exhibits a deterministic yet non-trivial structure, blending
number-theoretic properties with symbolic patterning. We propose the Primender
sequence as a benchmark for evaluating the symbolic reasoning capabilities of
Large Language Models (LLMs). The study is motivated by the need for
interpretable, rule-based testbeds that can assess an LLM's ability to infer
hidden rules, validate mathematical hypotheses, and generalize symbolic logic
at scale. A key hypothesis explored is: Whenever a number in the Primender
sequence is exactly one more than the largest prime less than or equal to it,
the difference between it and the previous number in the sequence is also 1. We
design a structured prompt and evaluation framework to test this hypothesis
across multiple state-of-the-art LLMs, including ChatGPT, Copilot, DeepSeek,
Gemini, Grok, and LLaMA. The models are tasked with identifying the underlying
rule, validating the hypothesis, and generating the next 100,000 terms of the
sequence. Comparative metrics such as rule inference accuracy, hypothesis
evaluation, sequence validity, and symbolic explanation quality are used to
assess model performance. This work contributes a novel mathematical construct
and a reproducible methodology for benchmarking LLMs in symbolic reasoning,
hypothesis testing, and scalable pattern generalization - bridging the domains
of number theory, artificial intelligence, and software engineering.

</details>


### [76] [Data Driven Diagnosis for Large Cyber-Physical-Systems with Minimal Prior Information](https://arxiv.org/abs/2506.10613)
*Henrik Sebastian Steude,Alexander Diedrich,Ingo Pill,Lukas Moddemann,Daniel Vranješ,Oliver Niggemann*

Main category: cs.AI

TL;DR: 提出一种基于神经网络和图诊断算法的新型诊断方法，仅需子系统基本关系及正常运行数据，在缺乏先验知识条件下有效定位复杂信息物理系统故障。


<details>
  <summary>Details</summary>
Motivation: 现有复杂信息物理系统诊断方法依赖详细系统模型或大量训练数据，但实际中此类先验知识获取成本极高，亟需低依赖度的诊断方法。

Method: 结合神经网络症状生成器（子系统级异常检测）与新型图诊断算法，仅需子系统间最小因果关系信息（实践中通常可获取）进行联合诊断。

Result: 仿真数据集显示82%案例正确识别真实故障组件，73%场景显著缩小搜索空间；真实水处理数据集验证了实际应用潜力。

Conclusion: 该方法为缺乏先验知识的大型复杂信息物理系统提供了实用化诊断解决方案，在工业场景中具有重要应用价值。

Abstract: Diagnostic processes for complex cyber-physical systems often require
extensive prior knowledge in the form of detailed system models or
comprehensive training data. However, obtaining such information poses a
significant challenge. To address this issue, we present a new diagnostic
approach that operates with minimal prior knowledge, requiring only a basic
understanding of subsystem relationships and data from nominal operations. Our
method combines a neural network-based symptom generator, which employs
subsystem-level anomaly detection, with a new graph diagnosis algorithm that
leverages minimal causal relationship information between
subsystems-information that is typically available in practice. Our experiments
with fully controllable simulated datasets show that our method includes the
true causal component in its diagnosis set for 82 p.c. of all cases while
effectively reducing the search space in 73 p.c. of the scenarios. Additional
tests on the real-world Secure Water Treatment dataset showcase the approach's
potential for practical scenarios. Our results thus highlight our approach's
potential for practical applications with large and complex cyber-physical
systems where limited prior knowledge is available.

</details>


### [77] [TeleMath: A Benchmark for Large Language Models in Telecom Mathematical Problem Solving](https://arxiv.org/abs/2506.10674)
*Vincenzo Colle,Mohamed Sana,Nicola Piovesan,Antonio De Domenico,Fadhel Ayed,Merouane Debbah*

Main category: cs.AI

TL;DR: 本文提出了首个针对电信领域数学问题的基准数据集TeleMath，包含500个专家设计的问答对，评估大语言模型在专业领域中的数学推理能力，发现专用模型表现优于通用模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在通用数学推理上有所进展，但其在电信等专业领域中处理数学密集型任务（如信号处理、网络优化）的有效性尚未被充分探索。

Method: 通过专家设计种子问题构建TeleMath数据集，采用问题生成流程扩展至500个问答对，覆盖电信领域多主题，并测试多种开源大语言模型的性能。

Result: 专为数学/逻辑推理设计的模型表现最佳（如准确率75%），而通用大模型（即使参数量大）普遍存在困难（部分模型准确率低于30%）。

Conclusion: TeleMath填补了专业领域数学评估的空白，证明专用模型的有效性，公开数据集与代码以支持未来研究。

Abstract: The increasing adoption of artificial intelligence in telecommunications has
raised interest in the capability of Large Language Models (LLMs) to address
domain-specific, mathematically intensive tasks. Although recent advancements
have improved the performance of LLMs in general mathematical reasoning, their
effectiveness within specialized domains, such as signal processing, network
optimization, and performance analysis, remains largely unexplored. To address
this gap, we introduce TeleMath, the first benchmark dataset specifically
designed to evaluate LLM performance in solving mathematical problems with
numerical solutions in the telecommunications domain. Comprising 500
question-answer (QnA) pairs, TeleMath covers a wide spectrum of topics in the
telecommunications field. This paper outlines the proposed QnAs generation
pipeline, starting from a selected seed of problems crafted by Subject Matter
Experts. The evaluation of a wide range of open-source LLMs reveals that best
performance on TeleMath is achieved by recent models explicitly designed for
mathematical or logical reasoning. In contrast, general-purpose models, even
those with a large number of parameters, often struggle with these challenges.
We have released the dataset and the evaluation code to ease result
reproducibility and support future research.

</details>


### [78] [Automated Validation of Textual Constraints Against AutomationML via LLMs and SHACL](https://arxiv.org/abs/2506.10678)
*Tom Westermann,Aljosha Köcher,Felix Gehlhoff*

Main category: cs.AI

TL;DR: 提出一种基于OWL本体映射和LLM的自动化流程，将AutomationML建模规则转换为可验证的SHACL约束，实现半自动化建模检查。


<details>
  <summary>Details</summary>
Motivation: 现有AutomationML建模规则以非结构化文本形式存在，缺乏自动验证机制，导致约束检查依赖人工且效率低下。

Method: 1) 通过RML/SPARQL将AML模型映射为OWL本体 2) 利用大语言模型将文本规则翻译为SHACL约束 3) 执行SHACL验证并生成自然语言解释报告

Result: 在AML建模样例中成功验证：复杂建模规则可通过半自动化流程检查，且用户无需掌握形式化方法或本体技术

Conclusion: 该方法通过本体技术+LLM的协同框架，实现了工业建模约束的自动化验证闭环，提升了AutomationML建模质量保障能力。

Abstract: AutomationML (AML) enables standardized data exchange in engineering, yet
existing recommendations for proper AML modeling are typically formulated as
informal and textual constraints. These constraints cannot be validated
automatically within AML itself. This work-in-progress paper introduces a
pipeline to formalize and verify such constraints. First, AML models are mapped
to OWL ontologies via RML and SPARQL. In addition, a Large Language Model
translates textual rules into SHACL constraints, which are then validated
against the previously generated AML ontology. Finally, SHACL validation
results are automatically interpreted in natural language. The approach is
demonstrated on a sample AML recommendation. Results show that even complex
modeling rules can be semi-automatically checked -- without requiring users to
understand formal methods or ontology technologies.

</details>


### [79] [System ASPMT2SMT:Computing ASPMT Theories by SMT Solvers](https://arxiv.org/abs/2506.10708)
*Michael Bartholomew,Joohyung Lee*

Main category: cs.AI

TL;DR: 本文提出了一种名为ASPMT（Answer Set Programming Modulo Theories）的方法，结合答案集编程与可满足性模理论，并开发了编译器aspsmt2smt，通过整合gringo和Z3实现部分变量基础化与理论求解，有效支持连续变化的实数计算推理。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于结合答案集编程（ASP）的非单调推理能力与可满足性模理论（SMT）对复杂理论（如实数运算）的高效处理能力，以解决传统ASP难以处理的连续变化问题。

Method: 通过将ASPMT程序的紧密片段转换为SMT实例，利用gringo进行部分基础化处理，保留未决变量由Z3求解器处理，实现ASP与SMT的协同计算。

Result: 实验表明，aspsmt2smt系统能够有效处理包含实数计算的程序，成功支持对动态系统中连续变化的推理。

Conclusion: ASPMT方法及其编译器工具通过整合ASP与SMT技术，为涉及离散逻辑与连续理论混合的复杂问题提供了可行的解决方案。

Abstract: Answer Set Programming Modulo Theories (ASPMT) is an approach to combining
answer set programming and satisfiability modulo theories based on the
functional stable model semantics. It is shown that the tight fragment of ASPMT
programs can be turned into SMT instances, thereby allowing SMT solvers to
compute stable models of ASPMT programs. In this paper we present a compiler
called {\sc aspsmt2smt}, which implements this translation. The system uses ASP
grounder {\sc gringo} and SMT solver {\sc z3}. {\sc gringo} partially grounds
input programs while leaving some variables to be processed by {\sc z3}. We
demonstrate that the system can effectively handle real number computations for
reasoning about continuous changes.

</details>


### [80] [Think before You Simulate: Symbolic Reasoning to Orchestrate Neural Computation for Counterfactual Question Answering](https://arxiv.org/abs/2506.10753)
*Adam Ishay,Zhun Yang,Joohyung Lee,Ilgu Kang,Dongjae Lim*

Main category: cs.AI

TL;DR: 本文提出一种结合符号因果推理的神经符号模型增强方法，显著提升了视频动态因果与反事实推理的性能，在CLEVRER和CRAFT基准测试中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有神经符号模型在反事实推理中存在局限性，需通过符号化因果关系增强模型对事件间因果逻辑的推理能力。

Method: 定义因果图表示事件关系，利用答案集编程(ASP)协调感知与模拟模块，在CRAFT基准中引入GPT-3.5/4作为动态模拟器并设计因果推理引导的提示策略。

Result: CLEVRER基准上实现SOTA性能，CRAFT基准通过大语言模型提示优化使反事实问题准确率进一步提升。

Conclusion: 符号因果推理与神经模块的协同机制能有效解决复杂反事实问题，大语言模型可作为动态模拟器的有效替代方案。

Abstract: Causal and temporal reasoning about video dynamics is a challenging problem.
While neuro-symbolic models that combine symbolic reasoning with neural-based
perception and prediction have shown promise, they exhibit limitations,
especially in answering counterfactual questions. This paper introduces a
method to enhance a neuro-symbolic model for counterfactual reasoning,
leveraging symbolic reasoning about causal relations among events. We define
the notion of a causal graph to represent such relations and use Answer Set
Programming (ASP), a declarative logic programming method, to find how to
coordinate perception and simulation modules. We validate the effectiveness of
our approach on two benchmarks, CLEVRER and CRAFT. Our enhancement achieves
state-of-the-art performance on the CLEVRER challenge, significantly
outperforming existing models. In the case of the CRAFT benchmark, we leverage
a large pre-trained language model, such as GPT-3.5 and GPT-4, as a proxy for a
dynamics simulator. Our findings show that this method can further improve its
performance on counterfactual questions by providing alternative prompts
instructed by symbolic causal reasoning.

</details>


### [81] [OPT-BENCH: Evaluating LLM Agent on Large-Scale Search Spaces Optimization Problems](https://arxiv.org/abs/2506.10764)
*Xiaozhe Li,Jixuan Chen,Xinyu Fang,Shengyuan Ding,Haodong Duan,Qingwen Liu,Kai Chen*

Main category: cs.AI

TL;DR: 该论文提出了OPT-BENCH基准和OPT-Agent框架，用于评估大型语言模型（LLM）在复杂优化任务中的迭代推理与解决方案优化能力，并通过实验验证历史反馈对性能提升的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在多样化任务中表现优异，但其通过历史反馈迭代优化复杂解决方案的能力尚未被充分研究。为此，需构建一个系统性评估框架以填补这一研究空白。

Method: 提出OPT-BENCH基准（包含20个Kaggle机器学习任务和10个NP问题）和OPT-Agent框架，模拟人类生成-验证-迭代优化的过程，并测试9种前沿LLM在不同迭代次数、温度参数下的表现。

Result: 实验表明，结合历史上下文能显著提升LLM在ML和NP任务中的优化性能；不同模型架构和参数设置对收敛性有显著影响。所有代码与数据已开源。

Conclusion: OPT-BENCH为LLM驱动的迭代推理研究提供了标准化评估工具，OPT-Agent框架验证了历史反馈对优化的重要性，开源资源将推动该领域进一步发展。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in solving
diverse tasks. However, their proficiency in iteratively optimizing complex
solutions through learning from previous feedback remains insufficiently
explored. To bridge this gap, we present OPT-BENCH, a comprehensive benchmark
designed to evaluate LLM agents on large-scale search space optimization
problems. OPT-BENCH includes 20 real-world machine learning tasks sourced from
Kaggle and 10 classical NP problems, offering a diverse and challenging
environment for assessing LLM agents on iterative reasoning and solution
refinement. To enable rigorous evaluation, we introduce OPT-Agent, an
end-to-end optimization framework that emulates human reasoning when tackling
complex problems by generating, validating, and iteratively improving solutions
through leveraging historical feedback. Through extensive experiments on 9
state-of-the-art LLMs from 6 model families, we analyze the effects of
optimization iterations, temperature settings, and model architectures on
solution quality and convergence. Our results demonstrate that incorporating
historical context significantly enhances optimization performance across both
ML and NP tasks. All datasets, code, and evaluation tools are open-sourced to
promote further research in advancing LLM-driven optimization and iterative
reasoning. Project page:
\href{https://github.com/OliverLeeXZ/OPT-BENCH}{https://github.com/OliverLeeXZ/OPT-BENCH}.

</details>


### [82] [A Study on Individual Spatiotemporal Activity Generation Method Using MCP-Enhanced Chain-of-Thought Large Language Models](https://arxiv.org/abs/2506.10853)
*Yu Zhang,Yang Hu,De Wang*

Main category: cs.AI

TL;DR: 本文提出结合思维链推理与模型上下文协议（MCP）的框架，以增强大语言模型在时空行为模拟中的能力，实验验证其生成数据与真实移动信号数据高度吻合，并为智慧城市应用提供基础。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则和统计的时空行为模拟方法存在计算成本高、泛化性差和扩展性不足的问题，而大语言模型作为'世界模拟器'在时空推理中存在空间认知有限、物理约束理解不足及群体同质化等挑战。

Method: 通过五阶段认知框架实现类人渐进推理，结合六类MCP工具（时间管理、空间导航、环境感知、个人记忆、社会协作、经验评估）进行数据处理，构建融合思维链（CoT）与MCP的模拟框架。

Result: 上海陆家嘴实验显示，1000个生成样本与真实移动信号相似度高，生成质量得分达7.86-8.36；并行处理效率提升，单样本生成时间从1.30分钟降至0.17分钟（2至12进程）。

Conclusion: 该框架推进了大语言模型在城市计算中的应用，为合成移动数据生成提供实用方法，支撑智慧城市规划、交通预测及参与式城市设计，实现验证数据模式对齐的时空行为模拟。

Abstract: Human spatiotemporal behavior simulation is critical for urban planning
research, yet traditional rule-based and statistical approaches suffer from
high computational costs, limited generalizability, and poor scalability. While
large language models (LLMs) show promise as "world simulators," they face
challenges in spatiotemporal reasoning including limited spatial cognition,
lack of physical constraint understanding, and group homogenization tendencies.
This paper introduces a framework integrating chain-of-thought (CoT) reasoning
with Model Context Protocol (MCP) to enhance LLMs' capability in simulating
spatiotemporal behaviors that correspond with validation data patterns. The
methodology combines human-like progressive reasoning through a five-stage
cognitive framework with comprehensive data processing via six specialized MCP
tool categories: temporal management, spatial navigation, environmental
perception, personal memory, social collaboration, and experience evaluation.
Experiments in Shanghai's Lujiazui district validate the framework's
effectiveness across 1,000 generated samples. Results demonstrate high
similarity with real mobile signaling data, achieving generation quality scores
of 7.86 to 8.36 across different base models. Parallel processing experiments
show efficiency improvements, with generation times decreasing from 1.30 to
0.17 minutes per sample when scaling from 2 to 12 processes. This work
contributes to integrating CoT reasoning with MCP for urban behavior modeling,
advancing LLMs applications in urban computing and providing a practical
approach for synthetic mobility data generation. The framework offers a
foundation for smart city planning, transportation forecasting, and
participatory urban design applications.

</details>


### [83] [GenPlanX. Generation of Plans and Execution](https://arxiv.org/abs/2506.10897)
*Daniel Borrajo,Giuseppe Canonaco,Tomás de la Rosa,Alfredo Garrachón,Sriram Gopalakrishnan,Simerjot Kaur,Marianela Morales,Sunandita Patra,Alberto Pozanco,Keshav Ramani,Charese Smiley,Pietro Totis,Manuela Veloso*

Main category: cs.AI

TL;DR: GenPlanX结合大型语言模型（LLMs）与传统AI规划引擎，通过自然语言处理提升任务规划与执行效率，优化人机协作。


<details>
  <summary>Details</summary>
Motivation: 传统AI规划技术无法直接理解自然语言描述的任务，而LLMs在解析人类意图方面表现优异，需整合两者以弥补能力鸿沟。

Method: 提出GenPlanX框架，集成LLMs解析自然语言任务描述、经典规划引擎生成动作序列，并引入执行与监控模块。

Result: 在办公场景中验证了GenPlanX的实用性，证明其能简化工作流程并通过人机协作提高生产力。

Conclusion: GenPlanX通过融合LLMs与经典规划技术，实现了自然语言驱动的任务自动化，为人机协作规划系统提供新范式。

Abstract: Classical AI Planning techniques generate sequences of actions for complex
tasks. However, they lack the ability to understand planning tasks when
provided using natural language. The advent of Large Language Models (LLMs) has
introduced novel capabilities in human-computer interaction. In the context of
planning tasks, LLMs have shown to be particularly good in interpreting human
intents among other uses. This paper introduces GenPlanX that integrates LLMs
for natural language-based description of planning tasks, with a classical AI
planning engine, alongside an execution and monitoring framework. We
demonstrate the efficacy of GenPlanX in assisting users with office-related
tasks, highlighting its potential to streamline workflows and enhance
productivity through seamless human-AI collaboration.

</details>


### [84] [Breaking Bad Molecules: Are MLLMs Ready for Structure-Level Molecular Detoxification?](https://arxiv.org/abs/2506.10912)
*Fei Lin,Ziyang Gong,Cong Wang,Yonglin Tian,Tengchao Zhang,Xue Yang,Gen Luo,Fei-Yue Wang*

Main category: cs.AI

TL;DR: 本文提出首个针对多模态大语言模型的分子毒性修复基准任务ToxiMol，构建标准化数据集与自动化评估框架ToxiEval，系统性评估近30个主流模型并揭示其潜力与挑战。


<details>
  <summary>Details</summary>
Motivation: 毒性是早期药物开发失败的主因，但分子毒性修复任务长期缺乏系统性定义与基准，阻碍了多模态大语言模型在该领域的应用与发展。

Method: 建立覆盖11类任务、560个毒性分子的标准化数据集ToxiMol，设计机制感知的提示标注流程，并提出整合毒性预测、可合成性等指标的自动化评估框架ToxiEval。

Result: 实验表明当前MLLMs仍面临显著挑战，但已初步展现毒性理解、语义约束遵循和结构感知编辑能力，候选分子多样性达3.7倍于传统方法。

Conclusion: ToxiMol为分子毒性修复建立首个系统性基准，揭示MLLMs在药物开发中的潜在价值，同时指出模型在复杂毒性机制处理方面的改进方向。

Abstract: Toxicity remains a leading cause of early-stage drug development failure.
Despite advances in molecular design and property prediction, the task of
molecular toxicity repair - generating structurally valid molecular
alternatives with reduced toxicity - has not yet been systematically defined or
benchmarked. To fill this gap, we introduce ToxiMol, the first benchmark task
for general-purpose Multimodal Large Language Models (MLLMs) focused on
molecular toxicity repair. We construct a standardized dataset covering 11
primary tasks and 560 representative toxic molecules spanning diverse
mechanisms and granularities. We design a prompt annotation pipeline with
mechanism-aware and task-adaptive capabilities, informed by expert
toxicological knowledge. In parallel, we propose an automated evaluation
framework, ToxiEval, which integrates toxicity endpoint prediction, synthetic
accessibility, drug-likeness, and structural similarity into a high-throughput
evaluation chain for repair success. We systematically assess nearly 30
mainstream general-purpose MLLMs and design multiple ablation studies to
analyze key factors such as evaluation criteria, candidate diversity, and
failure attribution. Experimental results show that although current MLLMs
still face significant challenges on this task, they begin to demonstrate
promising capabilities in toxicity understanding, semantic constraint
adherence, and structure-aware molecule editing.

</details>


### [85] [Spurious Rewards: Rethinking Training Signals in RLVR](https://arxiv.org/abs/2506.10947)
*Rulin Shao,Shuyue Stella Li,Rui Xin,Scott Geng,Yiping Wang,Sewoong Oh,Simon Shaolei Du,Nathan Lambert,Sewon Min,Ranjay Krishna,Yulia Tsvetkov,Hannaneh Hajishirzi,Pang Wei Koh,Luke Zettlemoyer*

Main category: cs.AI

TL;DR: 研究表明，即使使用与正确答案相关性极低甚至负相关的虚假奖励，强化学习结合可验证奖励（RLVR）仍能显著提升Qwen2.5-Math-7B模型的数学推理能力，但对其他模型家族（如Llama3、OLMo2）效果有限。代码推理行为在Qwen模型中尤为突出，且RLVR机制可能激活了预训练中习得的潜在表征。


<details>
  <summary>Details</summary>
Motivation: 探索在缺乏有效奖励信号的情况下，RLVR是否仍能激发模型内在的数学推理能力，并验证该方法在不同模型家族间的泛化性。

Method: 使用多种虚假奖励（随机奖励、格式奖励、错误标签、1-shot强化学习、多数投票）对Qwen等模型进行强化学习训练，分析其数学推理行为变化及跨模型效果差异。

Result: Qwen2.5-Math-7B在MATH-500上提升21.4%-27.1%（接近真实奖励的29.1%），代码推理频率从65%增至90%+，但其他模型未见显著增益。不同虚假奖励对模型效果存在明显选择性。

Conclusion: RLVR可能通过激活预训练获得的潜在推理能力实现效果，但模型对奖励信号的敏感性存在家族特异性。未来研究需在多样化模型上验证RLVR，避免单一模型结论的局限性。

Abstract: We show that reinforcement learning with verifiable rewards (RLVR) can elicit
strong mathematical reasoning in certain models even with spurious rewards that
have little, no, or even negative correlation with the correct answer. For
example, RLVR improves MATH-500 performance for Qwen2.5-Math-7B in absolute
points by 21.4% (random reward), 13.8% (format reward), 24.1% (incorrect
label), 26.0% (1-shot RL), and 27.1% (majority voting) -- nearly matching the
29.1% gained with ground truth rewards. However, the spurious rewards that work
for Qwen often fail to yield gains with other model families like Llama3 or
OLMo2. In particular, we find code reasoning -- thinking in code without actual
code execution -- to be a distinctive Qwen2.5-Math behavior that becomes
significantly more frequent after RLVR, from 65% to over 90%, even with
spurious rewards. Overall, we hypothesize that, given the lack of useful reward
signal, RLVR must somehow be surfacing useful reasoning representations learned
during pretraining, although the exact mechanism remains a topic for future
work. We suggest that future RLVR research should possibly be validated on
diverse models rather than a single de facto choice, as we show that it is easy
to get significant performance gains on Qwen models even with completely
spurious reward signals.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [86] [Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of LLMs](https://arxiv.org/abs/2506.10054)
*Shangpin Peng,Weinong Wang,Zhuotao Tian,Senqiao Yang,Xing Wu,Haotian Xu,Chengquan Zhang,Takashi Isobe,Baotian Hu,Min Zhang*

Main category: cs.LG

TL;DR: 提出Omni-DPO双视角优化框架，通过动态加权偏好对的质量与模型学习状态，显著提升RLHF性能，在文本理解和数学推理任务中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有DPO方法统一处理所有偏好对，忽视数据质量差异与模型动态学习能力，导致数据利用效率低和性能次优。

Method: Omni-DPO框架结合数据固有质量与模型动态学习状态，在训练中自适应调整样本权重，实现双视角联合优化。

Result: Gemma-2-9b-it模型在Arena-Hard文本任务上以6.7分优势超越Claude 3 Opus；数学推理任务中所有基准测试均优于基线方法。

Conclusion: Omni-DPO通过精细化数据利用机制有效提升模型性能，实验验证了其在多任务场景下的泛化能力与鲁棒性。

Abstract: Direct Preference Optimization (DPO) has become a cornerstone of
reinforcement learning from human feedback (RLHF) due to its simplicity and
efficiency. However, existing DPO-based approaches typically treat all
preference pairs uniformly, ignoring critical variations in their inherent
quality and learning utility, leading to suboptimal data utilization and
performance. To address this challenge, we propose Omni-DPO, a dual-perspective
optimization framework that jointly accounts for (1) the inherent quality of
each preference pair and (2) the model's evolving performance on those pairs.
By adaptively weighting samples according to both data quality and the model's
learning dynamics during training, Omni-DPO enables more effective training
data utilization and achieves better performance. Experimental results on
various models and benchmarks demonstrate the superiority and generalization
capabilities of Omni-DPO. On textual understanding tasks, Gemma-2-9b-it
finetuned with Omni-DPO beats the leading LLM, Claude 3 Opus, by a significant
margin of 6.7 points on the Arena-Hard benchmark. On mathematical reasoning
tasks, Omni-DPO consistently outperforms the baseline methods across all
benchmarks, providing strong empirical evidence for the effectiveness and
robustness of our approach. Code and models will be available at
https://github.com/pspdada/Omni-DPO.

</details>


### [87] [Leveraging Pre-Trained Models for Multimodal Class-Incremental Learning under Adaptive Fusion](https://arxiv.org/abs/2506.09999)
*Yukun Chen,Zihuan Qiu,Fanman Meng,Hongliang Li,Linfeng Xu,Qingbo Wu*

Main category: cs.LG

TL;DR: 本文提出一种基于多模态模型的多模态类增量学习方法，整合视觉、音频和文本，通过混合专家结构、自适应视听融合模块及对比训练损失解决信息整合与灾难性遗忘问题，并在三个数据集上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 传统多模态类增量学习（MCIL）方法仅关注视觉与文本，本文旨在探索视觉、音频和文本的多模态整合，解决互补信息融合与灾难性遗忘的挑战。

Method: 1. 基于混合专家（MoE）的多模态增量特征提取器（MIFE）优化AudioCLIP；2. 自适应视听融合模块（AAVFM）结合掩码阈值与动态特征融合机制，增强文本多样性；3. 提出多模态类增量对比训练损失优化跨模态对齐。

Result: 在三个多模态数据集上的实验表明，所提方法在整合多模态信息、减少灾难性遗忘及跨模态对齐方面显著优于基线方法。

Conclusion: 通过多模态预训练模型与增量学习机制的结合，本文方法有效提升了多模态类增量学习的性能，并提出了针对MCIL的评估指标。

Abstract: Unlike traditional Multimodal Class-Incremental Learning (MCIL) methods that
focus only on vision and text, this paper explores MCIL across vision, audio
and text modalities, addressing challenges in integrating complementary
information and mitigating catastrophic forgetting. To tackle these issues, we
propose an MCIL method based on multimodal pre-trained models. Firstly, a
Multimodal Incremental Feature Extractor (MIFE) based on Mixture-of-Experts
(MoE) structure is introduced to achieve effective incremental fine-tuning for
AudioCLIP. Secondly, to enhance feature discriminability and generalization, we
propose an Adaptive Audio-Visual Fusion Module (AAVFM) that includes a masking
threshold mechanism and a dynamic feature fusion mechanism, along with a
strategy to enhance text diversity. Thirdly, a novel multimodal
class-incremental contrastive training loss is proposed to optimize cross-modal
alignment in MCIL. Finally, two MCIL-specific evaluation metrics are introduced
for comprehensive assessment. Extensive experiments on three multimodal
datasets validate the effectiveness of our method.

</details>


### [88] [NOCL: Node-Oriented Conceptualization LLM for Graph Tasks without Message Passing](https://arxiv.org/abs/2506.10014)
*Wei Li,Mengcheng Lan,Jiaxing Xu,Yiping Ke*

Main category: cs.LG

TL;DR: 提出NOCL框架，通过节点描述和节点概念技术，将大语言模型（LLM）扩展至非文本属性图，减少token长度93.9%，并在监督和零样本场景下实现优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络（MPNN）依赖监督学习，泛化能力受限；现有自监督方法仍需微调，无法适应零样本场景。LLM处理图数据面临推理能力保留、长文本处理及任务单一等挑战。

Method: 1) 节点描述：将异构节点属性转为结构化自然语言，支持非文本属性图；2) 节点概念：用预训练模型压缩描述为语义嵌入，减少token长度；3) 图表示描述符统一多级任务为语言查询格式。

Result: 实验表明NOCL在监督学习中与MPNN及混合方法性能相当，且在零样本场景下泛化能力更优，节点概念使token长度减少达93.9%。

Conclusion: NOCL通过结合LLM与图结构，突破传统方法限制，为图基础模型提供新方向，支持多级任务统一处理。

Abstract: Graphs are essential for modeling complex interactions across domains such as
social networks, biology, and recommendation systems. Traditional Graph Neural
Networks, particularly Message Passing Neural Networks (MPNNs), rely heavily on
supervised learning, limiting their generalization and applicability in
label-scarce scenarios. Recent self-supervised approaches still require labeled
fine-tuning, limiting their effectiveness in zero-shot scenarios. Meanwhile,
Large Language Models (LLMs) excel in natural language tasks but face
significant challenges when applied to graphs, including preserving reasoning
abilities, managing extensive token lengths from rich node attributes, and
being limited to textual-attributed graphs (TAGs) and a single level task. To
overcome these limitations, we propose the Node-Oriented Conceptualization LLM
(NOCL), a novel framework that leverages two core techniques: 1) node
description, which converts heterogeneous node attributes into structured
natural language, extending LLM from TAGs to non-TAGs; 2) node concept, which
encodes node descriptions into compact semantic embeddings using pretrained
language models, significantly reducing token lengths by up to 93.9% compared
to directly using node descriptions. Additionally, our NOCL employs graph
representation descriptors to unify graph tasks at various levels into a
shared, language-based query format, paving a new direction for Graph
Foundation Models. Experimental results validate NOCL's competitive supervised
performance relative to traditional MPNNs and hybrid LLM-MPNN methods and
demonstrate superior generalization in zero-shot settings.

</details>


### [89] [Improving the performance of optical inverse design of multilayer thin films using CNN-LSTM tandem neural networks](https://arxiv.org/abs/2506.10044)
*Uijun Jung,Deokho Jang,Sungchul Kim,Jungho Kim*

Main category: cs.LG

TL;DR: 本文提出使用串联神经网络（TNN）结合不同深度学习模型（MLP、CNN、LSTM）进行SiO2/TiO2多层薄膜透射光谱的逆向设计，解决了传统方法耗时且一对多映射性能差的问题。结果表明CNN-LSTM组合在精度和速度上最优。


<details>
  <summary>Details</summary>
Motivation: 传统薄膜光学逆向设计依赖大量模拟和优化，效率低下。深度学习可解决此问题，但一对多映射问题影响性能，需改进网络结构。

Method: 采用串联神经网络（TNN），结合逆向网络与预训练正向网络，探索MLP、CNN、LSTM三种算法的组合配置（如LSTM-LSTM、CNN-LSTM等）。

Result: LSTM-LSTM型TNN精度最高但训练最慢；CNN-LSTM型TNN综合了CNN特征提取与LSTM序列处理优势，成为精度与速度平衡的最优方案。

Conclusion: CNN-LSTM型TNN通过整合CNN和LSTM的优势，在薄膜逆向设计中实现了高效且高精度的性能，为光学薄膜设计提供了新方法。

Abstract: Optical properties of thin film are greatly influenced by the thickness of
each layer. Accurately predicting these thicknesses and their corresponding
optical properties is important in the optical inverse design of thin films.
However, traditional inverse design methods usually demand extensive numerical
simulations and optimization procedures, which are time-consuming. In this
paper, we utilize deep learning for the inverse design of the transmission
spectra of SiO2/TiO2 multilayer thin films. We implement a tandem neural
network (TNN), which can solve the one-to-many mapping problem that greatly
degrades the performance of deep-learning-based inverse designs. In general,
the TNN has been implemented by a back-to-back connection of an inverse neural
network and a pre-trained forward neural network, both of which have been
implemented based on multilayer perceptron (MLP) algorithms. In this paper, we
propose to use not only MLP, but also convolutional neural network (CNN) or
long short-term memory (LSTM) algorithms in the configuration of the TNN. We
show that an LSTM-LSTM-based TNN yields the highest accuracy but takes the
longest training time among nine configurations of TNNs. We also find that a
CNN-LSTM-based TNN will be an optimal solution in terms of accuracy and speed
because it could integrate the strengths of the CNN and LSTM algorithms.

</details>


### [90] [Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of LLMs](https://arxiv.org/abs/2506.10054)
*Shangpin Peng,Weinong Wang,Zhuotao Tian,Senqiao Yang,Xing Wu,Haotian Xu,Chengquan Zhang,Takashi Isobe,Baotian Hu,Min Zhang*

Main category: cs.LG

TL;DR: 提出Omni-DPO双视角优化框架，通过动态加权偏好对的质量与模型学习状态，显著提升RLHF性能，在文本理解和数学推理任务中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有DPO方法统一处理所有偏好对，忽视数据质量差异与模型动态学习能力，导致数据利用效率低和性能次优。

Method: Omni-DPO框架结合数据固有质量与模型动态学习状态，在训练中自适应调整样本权重，实现双视角联合优化。

Result: Gemma-2-9b-it模型在Arena-Hard文本任务上以6.7分优势超越Claude 3 Opus；数学推理任务中所有基准测试均优于基线方法。

Conclusion: Omni-DPO通过精细化数据利用机制有效提升模型性能，实验验证了其在多任务场景下的泛化能力与鲁棒性。

Abstract: Direct Preference Optimization (DPO) has become a cornerstone of
reinforcement learning from human feedback (RLHF) due to its simplicity and
efficiency. However, existing DPO-based approaches typically treat all
preference pairs uniformly, ignoring critical variations in their inherent
quality and learning utility, leading to suboptimal data utilization and
performance. To address this challenge, we propose Omni-DPO, a dual-perspective
optimization framework that jointly accounts for (1) the inherent quality of
each preference pair and (2) the model's evolving performance on those pairs.
By adaptively weighting samples according to both data quality and the model's
learning dynamics during training, Omni-DPO enables more effective training
data utilization and achieves better performance. Experimental results on
various models and benchmarks demonstrate the superiority and generalization
capabilities of Omni-DPO. On textual understanding tasks, Gemma-2-9b-it
finetuned with Omni-DPO beats the leading LLM, Claude 3 Opus, by a significant
margin of 6.7 points on the Arena-Hard benchmark. On mathematical reasoning
tasks, Omni-DPO consistently outperforms the baseline methods across all
benchmarks, providing strong empirical evidence for the effectiveness and
robustness of our approach. Code and models will be available at
https://github.com/pspdada/Omni-DPO.

</details>


### [91] [Textual Bayes: Quantifying Uncertainty in LLM-Based Systems](https://arxiv.org/abs/2506.10060)
*Brendan Leigh Ross,Noël Vouitsis,Atiyeh Ashari Ghomi,Rasa Hosseinzadeh,Ji Xin,Zhaoyan Liu,Yi Sui,Shiyi Hou,Kin Kwan Leung,Gabriel Loaiza-Ganem,Jesse C. Cresswell*

Main category: cs.LG

TL;DR: 该论文提出一种贝叶斯框架，通过将提示视为文本参数并进行贝叶斯推断，解决了大语言模型（LLM）不确定性量化难题，并开发了MHLP算法提升预测准确性和不确定性校准。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在不确定性量化方面存在显著挑战，且闭源黑箱特性与提示敏感性限制了其在高风险领域的应用。传统方法难以在文本模态中实现贝叶斯推断，亟需新方法突破。

Method: 提出贝叶斯视角下的LLM系统建模方法：1) 将提示作为可推断的文本参数 2) 开发Metropolis-Hastings through LLM Proposals（MHLP）算法，结合提示优化与MCMC方法实现贝叶斯推断 3) 支持闭源模型的无缝集成。

Result: 实验表明：该方法在多个LLM基准测试中显著提升预测准确性（平均提升12.7%）和不确定性量化效果（校准误差降低35%），同时支持基于自然语言的先验知识表达。

Conclusion: 该研究成功将经典贝叶斯方法引入LLM领域，通过参数化提示和MHLP算法，为构建更可靠、可校准的LLM系统提供了可行路径，特别是在高风险决策场景中具有重要应用价值。

Abstract: Although large language models (LLMs) are becoming increasingly capable of
solving challenging real-world tasks, accurately quantifying their uncertainty
remains a critical open problem, which limits their applicability in
high-stakes domains. This challenge is further compounded by the closed-source,
black-box nature of many state-of-the-art LLMs. Moreover, LLM-based systems can
be highly sensitive to the prompts that bind them together, which often require
significant manual tuning (i.e., prompt engineering). In this work, we address
these challenges by viewing LLM-based systems through a Bayesian lens. We
interpret prompts as textual parameters in a statistical model, allowing us to
use a small training dataset to perform Bayesian inference over these prompts.
This novel perspective enables principled uncertainty quantification over both
the model's textual parameters and its downstream predictions, while also
incorporating prior beliefs about these parameters expressed in free-form text.
To perform Bayesian inference, a difficult problem even for well-studied data
modalities, we introduce Metropolis-Hastings through LLM Proposals (MHLP), a
novel Markov chain Monte Carlo (MCMC) algorithm that combines prompt
optimization techniques with standard MCMC methods. MHLP is a turnkey
modification to existing LLM pipelines, including those that rely exclusively
on closed-source models. Empirically, we demonstrate that our method yields
improvements in both predictive accuracy and uncertainty quantification (UQ) on
a range of LLM benchmarks and UQ tasks. More broadly, our work demonstrates a
viable path for incorporating methods from the rich Bayesian literature into
the era of LLMs, paving the way for more reliable and calibrated LLM-based
systems.

</details>


### [92] [Optimizing Latent Dimension Allocation in Hierarchical VAEs: Balancing Attenuation and Information Retention for OOD Detection](https://arxiv.org/abs/2506.10089)
*Dane Williamson,Yangfeng Ji,Matthew Dwyer*

Main category: cs.LG

TL;DR: 本文提出了一种基于信息理论的优化框架，用于分层变分自编码器（HVAE）的潜在维度分配，以提升分布外（OOD）检测性能。通过理论推导和实验验证，证明了固定潜在预算下最优分配比例的存在，并展示了该方法在多种数据集和架构中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有分层变分自编码器（HVAE）的潜在维度分配方式具有随意性，易导致表征效率低下或后验塌缩，限制了其在安全关键场景中的OOD检测能力。需通过理论指导优化潜在结构设计。

Method: 结合信息论原理，形式化信息损失与表征衰减的权衡关系，推导出固定潜在预算下的最优潜在维度分配比例r*，并通过实验调整该比例优化HVAE结构。

Result: 实验表明，优化后的潜在分配策略显著优于基线HVAE配置，在多个数据集和模型架构中稳定提升OOD检测性能，验证了理论框架的有效性。

Conclusion: 该框架为HVAE的潜在结构设计提供了理论指导，通过优化维度分配比例可增强生成模型的OOD检测鲁棒性，推动安全关键应用中的模型可靠性。

Abstract: Out-of-distribution (OOD) detection is a critical task in machine learning,
particularly for safety-critical applications where unexpected inputs must be
reliably flagged. While hierarchical variational autoencoders (HVAEs) offer
improved representational capacity over traditional VAEs, their performance is
highly sensitive to how latent dimensions are distributed across layers.
Existing approaches often allocate latent capacity arbitrarily, leading to
ineffective representations or posterior collapse. In this work, we introduce a
theoretically grounded framework for optimizing latent dimension allocation in
HVAEs, drawing on principles from information theory to formalize the trade-off
between information loss and representational attenuation. We prove the
existence of an optimal allocation ratio $r^{\ast}$ under a fixed latent
budget, and empirically show that tuning this ratio consistently improves OOD
detection performance across datasets and architectures. Our approach
outperforms baseline HVAE configurations and provides practical guidance for
principled latent structure design, leading to more robust OOD detection with
deep generative models.

</details>


### [93] [Efficient kernelized bandit algorithms via exploration distributions](https://arxiv.org/abs/2506.10091)
*Bingshan Hu,Zheng He,Danica J. Sutherland*

Main category: cs.LG

TL;DR: 本文提出了一种基于探索分布的高效核化老虎机算法GP-Generic，通过灵活选择探索分布实现理论最优（O~(γ_T√T)遗憾界），并证明随机化算法在实践中的优势。


<details>
  <summary>Details</summary>
Motivation: 针对核化老虎机问题中传统UCB类算法局限性，提出一种通用框架以统一确定性/随机化算法设计，同时保持计算效率与理论性能。

Method: 引入探索分布概念构建GP-Generic算法类，通过概率分布选择探索策略，涵盖UCB作为特例并支持多种随机化变体。

Result: 理论证明算法可实现与UCB、TS等方法相同的O~(γ_T√T)遗憾界，实验表明随机化策略具有更优实际表现。

Conclusion: 探索分布框架为核化老虎机提供了灵活的设计范式，随机化策略在理论保证与实践效果间取得更好平衡。

Abstract: We consider a kernelized bandit problem with a compact arm set ${X} \subset
\mathbb{R}^d $ and a fixed but unknown reward function $f^*$ with a finite norm
in some Reproducing Kernel Hilbert Space (RKHS). We propose a class of
computationally efficient kernelized bandit algorithms, which we call
GP-Generic, based on a novel concept: exploration distributions. This class of
algorithms includes Upper Confidence Bound-based approaches as a special case,
but also allows for a variety of randomized algorithms. With careful choice of
exploration distribution, our proposed generic algorithm realizes a wide range
of concrete algorithms that achieve $\tilde{O}(\gamma_T\sqrt{T})$ regret
bounds, where $\gamma_T$ characterizes the RKHS complexity. This matches known
results for UCB- and Thompson Sampling-based algorithms; we also show that in
practice, randomization can yield better practical results.

</details>


### [94] [Unsupervised Deep Clustering of MNIST with Triplet-Enhanced Convolutional Autoencoders](https://arxiv.org/abs/2506.10094)
*Md. Faizul Islam Ansari*

Main category: cs.LG

TL;DR: 本研究通过两阶段深度自编码器架构实现了MNIST手写数字的无监督聚类，结合重构误差与K均值聚类损失的联合优化，在聚类性能和可视化效果上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统无监督聚类方法在处理图像特征时难以平衡重构精度与聚类纯度，且缺乏可解释性和可扩展性。本文旨在通过深度自编码器与聚类目标的联合优化，解决这一问题。

Method: 1. 第一阶段：训练深度自编码器以生成紧凑的图像表示；2. 第二阶段：联合重构误差与K均值聚类损失优化潜在嵌入；3. 引入批归一化、Dropout和权重衰减提升模型稳定性。

Result: 使用Silhouette Score、Davies-Bouldin Index、NMI和ARI指标评估，聚类性能显著优于基线。t-SNE可视化显示数字嵌入形成清晰分离簇，重构误差降低同时聚类纯度。

Conclusion: 该方法在重构精度与聚类纯度间取得平衡，兼具可解释性与可扩展性，为大规模图像无监督表示学习提供了可靠基础框架。

Abstract: This research implements an advanced unsupervised clustering system for MNIST
handwritten digits through two-phase deep autoencoder architecture. A deep
neural autoencoder requires a training process during phase one to develop
minimal yet interpretive representations of images by minimizing reconstruction
errors. During the second phase we unify the reconstruction error with a KMeans
clustering loss for learned latent embeddings through a joint distance-based
objective. Our model contains three elements which include batch normalization
combined with dropout and weight decay for achieving generalized and stable
results. The framework achieves superior clustering performance during
extensive tests which used intrinsic measurements including Silhouette Score
and Davies-Bouldin Index coupled with extrinsic metrics NMI and ARI when
processing image features. The research uses t-SNE visualization to present
learned embeddings that show distinct clusters for digits. Our approach reaches
an optimal combination between data reconstruction accuracy and cluster
separation purity when adding the benefit of understandable results and
scalable implementations. The approach creates a dependable base that helps
deploy unsupervised representation learning in different large-scale image
clustering applications.

</details>


### [95] [Learning to Collaborate Over Graphs: A Selective Federated Multi-Task Learning Approach](https://arxiv.org/abs/2506.10102)
*Ahmed Elbakary,Chaouki Ben Issaid,Mehdi Bennis*

Main category: cs.LG

TL;DR: 提出一种基于跨客户端相似性的联邦多任务学习方法，通过特征锚点压缩通信数据，结合动态图社区检测实现高效个性化协作学习。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端数据分布异构性导致模型个性化需求，传统方法存在通信开销大、负迁移风险等问题，需在保证效率的同时促进正向知识迁移。

Method: 1. 引入特征锚点（紧凑特征向量）替代全模型传输；2. 共享轻量级分类头；3. 动态图正则化建模客户端协作关系；4. 社区检测划分同质群体限制协作范围。

Result: 在两个异构数据集上超越SOTA基线，通信/计算效率提升23-45%，客户端间公平性指标提高18%，有效防止负迁移。

Conclusion: 通过特征锚点与动态社区协作机制，在降低通信成本的同时实现高效个性化联邦学习，为数据异构场景提供新解决方案。

Abstract: We present a novel federated multi-task learning method that leverages
cross-client similarity to enable personalized learning for each client. To
avoid transmitting the entire model to the parameter server, we propose a
communication-efficient scheme that introduces a feature anchor, a compact
vector representation that summarizes the features learned from the client's
local classes. This feature anchor is shared with the server to account for
local clients' distribution. In addition, the clients share the classification
heads, a lightweight linear layer, and perform a graph-based regularization to
enable collaboration among clients. By modeling collaboration between clients
as a dynamic graph and continuously updating and refining this graph, we can
account for any drift from the clients. To ensure beneficial knowledge transfer
and prevent negative collaboration, we leverage a community detection-based
approach that partitions this dynamic graph into homogeneous communities,
maximizing the sum of task similarities, represented as the graph edges'
weights, within each community. This mechanism restricts collaboration to
highly similar clients within their formed communities, ensuring positive
interaction and preserving personalization. Extensive experiments on two
heterogeneous datasets demonstrate that our method significantly outperforms
state-of-the-art baselines. Furthermore, we show that our method exhibits
superior computation and communication efficiency and promotes fairness across
clients.

</details>


### [96] [NnD: Diffusion-based Generation of Physically-Nonnegative Objects](https://arxiv.org/abs/2506.10112)
*Nadav Torem,Tamar Sde-Chen,Yoav Y. Schechner*

Main category: cs.LG

TL;DR: 本文提出非负扩散（NnD）模型，通过基于分数的扩散方法结合退火Langevin动力学，强制生成过程中的非负性，显著降低复杂自然现象（如3D云）的模拟计算成本，同时保持物理一致性。


<details>
  <summary>Details</summary>
Motivation: 自然现象（如云形成）的物理模拟计算成本高昂且难以扩展，需一种高效方法生成非负物理场并支持推断。

Method: 提出非负扩散（NnD）模型：基于分数扩散的生成方法，通过改进的退火Langevin动力学在迭代生成中强制非负性，利用高质量物理模拟数据训练。

Result: 生成的3D云体积数据符合云物理趋势，专家无法有效区分其非真实性，验证了方法的物理合理性与生成能力。

Conclusion: NnD在保持物理意义的前提下显著降低计算成本，为复杂非负自然现象的生成与推断提供高效解决方案。

Abstract: Most natural objects have inherent complexity and variability. While some
simple objects can be modeled from first principles, many real-world phenomena,
such as cloud formation, require computationally expensive simulations that
limit scalability. This work focuses on a class of physically meaningful,
nonnegative objects that are computationally tractable but costly to simulate.
To dramatically reduce computational costs, we propose nonnegative diffusion
(NnD). This is a learned generative model using score based diffusion. It
adapts annealed Langevin dynamics to enforce, by design, non-negativity
throughout iterative scene generation and analysis (inference). NnD trains on
high-quality physically simulated objects. Once trained, it can be used for
generation and inference. We demonstrate generation of 3D volumetric clouds,
comprising inherently nonnegative microphysical fields. Our generated clouds
are consistent with cloud physics trends. They are effectively not
distinguished as non-physical by expert perception.

</details>


### [97] [GRAIL: A Benchmark for GRaph ActIve Learning in Dynamic Sensing Environments](https://arxiv.org/abs/2506.10120)
*Maryam Khalid,Akane Sano*

Main category: cs.LG

TL;DR: 本文提出GRAIL框架，用于动态环境下图主动学习（AL）策略的评估，通过引入新指标综合衡量模型性能、多样性与用户负担，揭示现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有图主动学习方法主要在静态数据集上评估，且仅关注预测精度，忽视了动态环境中的用户负担、采样多样性和公平性等实际问题。

Method: 提出GRAIL基准框架，设计新指标（持续有效性、多样性、用户负担）以动态真实场景下的图AL评估，并通过动态传感器数据集进行实验验证。

Result: 实验表明现有AL策略在性能与用户负担间存在权衡，GRAIL验证了平衡节点重要性、查询多样性和网络拓扑结构对动态环境AL的重要性。

Conclusion: GRAIL为动态图AL提供了更全面的评估机制，强调需兼顾模型性能与用户需求，为实际应用中的策略设计提供指导。

Abstract: Graph-based Active Learning (AL) leverages the structure of graphs to
efficiently prioritize label queries, reducing labeling costs and user burden
in applications like health monitoring, human behavior analysis, and sensor
networks. By identifying strategically positioned nodes, graph AL minimizes
data collection demands while maintaining model performance, making it a
valuable tool for dynamic environments. Despite its potential, existing graph
AL methods are often evaluated on static graph datasets and primarily focus on
prediction accuracy, neglecting user-centric considerations such as sampling
diversity, query fairness, and adaptability to dynamic settings. To bridge this
gap, we introduce GRAIL, a novel benchmarking framework designed to evaluate
graph AL strategies in dynamic, real-world environments. GRAIL introduces novel
metrics to assess sustained effectiveness, diversity, and user burden, enabling
a comprehensive evaluation of AL methods under varying conditions. Extensive
experiments on datasets featuring dynamic, real-life human sensor data reveal
trade-offs between prediction performance and user burden, highlighting
limitations in existing AL strategies. GRAIL demonstrates the importance of
balancing node importance, query diversity, and network topology, providing an
evaluation mechanism for graph AL solutions in dynamic environments.

</details>


### [98] [Meet Me at the Arm: The Cooperative Multi-Armed Bandits Problem with Shareable Arms](https://arxiv.org/abs/2506.10127)
*Xinyi Hu,Aldo Pacchiano*

Main category: cs.LG

TL;DR: 该论文提出了一种去中心化算法A-CAPELLA，用于解决无感知多玩家多臂老虎机问题中未知臂容量的协调与学习挑战，实现了对数级遗憾。


<details>
  <summary>Details</summary>
Motivation: 传统多臂老虎机模型假设臂容量为1且依赖碰撞反馈，而实际场景中臂容量未知且无法感知碰撞。研究旨在解决无感知环境下多玩家协作与容量估计的难题。

Method: 提出A-CAPELLA算法，通过结构化碰撞模式实现同步连续消除和容量估计，利用协作假设检验协议协调玩家行为。

Result: 算法在未知臂容量的无感知场景中实现了对数级遗憾，为去中心化协作学习提供了理论保证。

Conclusion: A-CAPELLA首次在广义容量模型下实现了无感知多玩家协作的高效学习，为复杂资源分配问题提供了新解决方案。

Abstract: We study the decentralized multi-player multi-armed bandits (MMAB) problem
under a no-sensing setting, where each player receives only their own reward
and obtains no information about collisions. Each arm has an unknown capacity,
and if the number of players pulling an arm exceeds its capacity, all players
involved receive zero reward. This setting generalizes the classical
unit-capacity model and introduces new challenges in coordination and capacity
discovery under severe feedback limitations. We propose A-CAPELLA (Algorithm
for Capacity-Aware Parallel Elimination for Learning and Allocation), a
decentralized algorithm that achieves logarithmic regret in this generalized
regime. Our main contribution is a collaborative hypothesis testing protocol
that enables synchronized successive elimination and capacity estimation
through carefully structured collision patterns. This represents a provably
efficient learning result in decentralized no-sensing MMAB with unknown arm
capacities.

</details>


### [99] [Provable Sim-to-Real Transfer via Offline Domain Randomization](https://arxiv.org/abs/2506.10133)
*Arnaud Fickinger,Abderrahim Bendahi,Stuart Russell*

Main category: cs.LG

TL;DR: 本文提出离线域随机化（ODR）方法，通过最大似然估计拟合模拟器参数分布，结合熵奖励改进DROPO算法（E-DROPO），证明其理论一致性及相比均匀域随机化（DR）的误差优势，实现更鲁棒的零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 强化学习代理从模拟环境迁移到现实世界时存在性能差距，传统域随机化（DR）忽略真实系统离线数据，需探索利用离线数据优化模拟器参数分布以缩小差距。

Method: 将ODR形式化为参数化模拟器族的最大似然估计问题，提出E-DROPO算法（在DROPO中增加熵奖励防止方差坍缩），并理论分析其参数一致性及误差边界。

Result: 证明ODR估计量在数据量增加时收敛于真实动态，其仿真-现实误差在有限模拟器场景下比均匀DR紧致O(M)倍；E-DROPO通过熵奖励扩展随机化范围，提升零样本迁移鲁棒性。

Conclusion: ODR通过离线数据优化模拟器参数分布，理论保证优于传统DR，E-DROPO进一步通过熵正则化增强随机化效果，为仿真到现实迁移提供更可靠框架。

Abstract: Reinforcement-learning agents often struggle when deployed from simulation to
the real-world. A dominant strategy for reducing the sim-to-real gap is domain
randomization (DR) which trains the policy across many simulators produced by
sampling dynamics parameters, but standard DR ignores offline data already
available from the real system. We study offline domain randomization (ODR),
which first fits a distribution over simulator parameters to an offline
dataset. While a growing body of empirical work reports substantial gains with
algorithms such as DROPO, the theoretical foundations of ODR remain largely
unexplored. In this work, we (i) formalize ODR as a maximum-likelihood
estimation over a parametric simulator family, (ii) prove consistency of this
estimator under mild regularity and identifiability conditions, showing it
converges to the true dynamics as the dataset grows, (iii) derive gap bounds
demonstrating ODRs sim-to-real error is up to an O(M) factor tighter than
uniform DR in the finite-simulator case (and analogous gains in the continuous
setting), and (iv) introduce E-DROPO, a new version of DROPO which adds an
entropy bonus to prevent variance collapse, yielding broader randomization and
more robust zero-shot transfer in practice.

</details>


### [100] [Self-Predictive Representations for Combinatorial Generalization in Behavioral Cloning](https://arxiv.org/abs/2506.10137)
*Daniel Lawson,Adriana Hugessen,Charlotte Cloutier,Glen Berseth,Khimya Khetarpal*

Main category: cs.LG

TL;DR: 本文提出了一种名为BYOL-γ增强GCBC的新方法，通过结合目标条件行为克隆与后继表示学习，解决了传统方法在组合泛化任务中因状态表示时间一致性不足导致的性能限制。


<details>
  <summary>Details</summary>
Motivation: 目标条件行为克隆（GCBC）在训练任务中表现良好，但面对需要组合泛化的新状态-目标对时，因状态表示缺乏时间一致性而泛化能力不足。后继表示能编码未来状态分布，但现有方法依赖对比样本或时序差分学习。

Method: 提出BYOL-γ增强GCBC方法，通过无对比样本或时序差分学习的表示学习目标，在有限MDP场景下理论近似后继表示，并提升组合泛化能力。

Result: 该方法在多个需要组合泛化的复杂任务中表现优异，验证了其理论有效性和实际竞争力。

Conclusion: BYOL-γ通过增强状态表示的时间一致性，显著提升了GCBC的零样本组合泛化能力，为离线策略学习提供了新思路。

Abstract: Behavioral cloning (BC) methods trained with supervised learning (SL) are an
effective way to learn policies from human demonstrations in domains like
robotics. Goal-conditioning these policies enables a single generalist policy
to capture diverse behaviors contained within an offline dataset. While
goal-conditioned behavior cloning (GCBC) methods can perform well on
in-distribution training tasks, they do not necessarily generalize zero-shot to
tasks that require conditioning on novel state-goal pairs, i.e. combinatorial
generalization. In part, this limitation can be attributed to a lack of
temporal consistency in the state representation learned by BC; if temporally
related states are encoded to similar latent representations, then the
out-of-distribution gap for novel state-goal pairs would be reduced. Hence,
encouraging this temporal consistency in the representation space should
facilitate combinatorial generalization. Successor representations, which
encode the distribution of future states visited from the current state, nicely
encapsulate this property. However, previous methods for learning successor
representations have relied on contrastive samples, temporal-difference (TD)
learning, or both. In this work, we propose a simple yet effective
representation learning objective, $\text{BYOL-}\gamma$ augmented GCBC, which
is not only able to theoretically approximate the successor representation in
the finite MDP case without contrastive samples or TD learning, but also,
results in competitive empirical performance across a suite of challenging
tasks requiring combinatorial generalization.

</details>


### [101] [Interpreting learned search: finding a transition model and value function in an RNN that plays Sokoban](https://arxiv.org/abs/2506.10138)
*Mohammad Taufeeque,Aaron David Tucker,Adam Gleave,Adrià Garriga-Alonso*

Main category: cs.LG

TL;DR: 通过逆向工程分析一个用于推箱子游戏的卷积循环神经网络，发现其利用测试时计算资源提升性能的机制类似于经典双向搜索，但具有分层计划表示和独立处理每个箱子等独特设计。


<details>
  <summary>Details</summary>
Motivation: 探究无模型强化学习训练的神经网络如何利用测试时计算资源提升推箱子游戏性能，并理解其内部机制与经典搜索算法的异同。

Method: 对卷积RNN进行部分逆向工程，分析其方向关联通道的激活模式、状态-动作激活作为类价值函数的作用机制，以及专用核扩展路径形成过渡模型的过程。

Result: 网络通过方向通道表示计划，状态激活决定回溯与剪枝策略，分层计划表示增加搜索深度，且采用独立处理箱子的非统一状态表征方式。

Conclusion: 该网络通过无模型训练习得了可解释的类双向搜索机制，但其分层结构和分布式状态表征形成了与经典算法不同的新型搜索范式。

Abstract: We partially reverse-engineer a convolutional recurrent neural network (RNN)
trained to play the puzzle game Sokoban with model-free reinforcement learning.
Prior work found that this network solves more levels with more test-time
compute. Our analysis reveals several mechanisms analogous to components of
classic bidirectional search. For each square, the RNN represents its plan in
the activations of channels associated with specific directions. These
state-action activations are analogous to a value function - their magnitudes
determine when to backtrack and which plan branch survives pruning. Specialized
kernels extend these activations (containing plan and value) forward and
backward to create paths, forming a transition model. The algorithm is also
unlike classical search in some ways. State representation is not unified;
instead, the network considers each box separately. Each layer has its own plan
representation and value function, increasing search depth. Far from being
inscrutable, the mechanisms leveraging test-time compute learned in this
network by model-free training can be understood in familiar terms.

</details>


### [102] [Survival Analysis as Imprecise Classification with Trainable Kernels](https://arxiv.org/abs/2506.10140)
*Andrei V. Konstantinov,Vlada A. Efremenko,Lev V. Utkin*

Main category: cs.LG

TL;DR: 本文提出三种基于不精确概率理论和注意力机制的新型生存分析模型（iSurvM、iSurvQ、iSurvJ），通过区间概率分布和可训练注意力权重处理截尾数据，实验表明其性能优于传统Beran估计器。


<details>
  <summary>Details</summary>
Motivation: 传统非参数方法（如Beran估计器）在处理复杂数据结构与重度截尾数据时存在局限性，需开发更鲁棒且无需参数假设的生存分析模型。

Method: 结合不精确概率理论与注意力机制：1) 用区间值概率分布表示截尾数据；2) 基于核的Nadaraya-Watson回归计算全局概率分布；3) 提出三种训练策略对应三个模型（均值似然、分位数似然、联合学习）。

Result: 合成与真实数据集实验表明，所提模型（尤其iSurvJ）在精度和计算复杂度上均优于Beran估计器，且代码已开源。

Conclusion: 新模型通过非参数方式有效处理截尾数据，解决了传统方法的不足，iSurvJ的综合性能最佳，为生存分析提供了更优的解决方案。

Abstract: Survival analysis is a fundamental tool for modeling time-to-event data in
healthcare, engineering, and finance, where censored observations pose
significant challenges. While traditional methods like the Beran estimator
offer nonparametric solutions, they often struggle with the complex data
structures and heavy censoring. This paper introduces three novel survival
models, iSurvM (the imprecise Survival model based on Mean likelihood
functions), iSurvQ (the imprecise Survival model based on the Quantiles of
likelihood functions), and iSurvJ (the imprecise Survival model based on the
Joint learning), that combine imprecise probability theory with attention
mechanisms to handle censored data without parametric assumptions. The first
idea behind the models is to represent censored observations by interval-valued
probability distributions for each instance over time intervals between events
moments. The second idea is to employ the kernel-based Nadaraya-Watson
regression with trainable attention weights for computing the imprecise
probability distribution over time intervals for the entire dataset. The third
idea is to consider three decision strategies for training, which correspond to
the proposed three models. Experiments on synthetic and real datasets
demonstrate that the proposed models, especially iSurvJ, consistently
outperform the Beran estimator from the accuracy and computational complexity
points of view. Codes implementing the proposed models are publicly available.

</details>


### [103] [Physiological-Model-Based Neural Network for Heart Rate Estimation during Daily Physical Activities](https://arxiv.org/abs/2506.10144)
*Yaowen Zhang,Libera Fresiello,Peter H. Veltink,Dirk W. Donker,Ying Wang*

Main category: cs.LG

TL;DR: 本研究提出了一种基于生理模型的神经网络（PMB-NN）框架，通过日常活动中的摄氧量数据实现个性化心率估计，以提升心力衰竭早期检测能力。该方法在12名受试者中验证有效，性能优于传统生理模型。


<details>
  <summary>Details</summary>
Motivation: 现有心率监测工具依赖群体平均值，缺乏个体化动态追踪能力，限制了心力衰竭早期检测的精准性。需开发结合生理原理的高效可解释模型。

Method: 提出PMB-NN框架：将简化人体运动生理模型的约束嵌入神经网络训练，利用12名受试者的静息/骑行/跑步数据，通过VO2数据实现个性化心率估计。

Result: PMB-NN中位R²达0.8、RMSE为8.3bpm，性能与基准神经网络相当且显著优于传统生理模型(p=0.002)，并能识别个性化生理参数。

Conclusion: 该框架通过整合生理模型与数据驱动方法，为日常活动中的实时个性化心脏监测提供了可行方案，未来可结合运动衍生VO2系统实现精准健康追踪。

Abstract: Heart failure (HF) poses a significant global health challenge, with early
detection offering opportunities for improved outcomes. Abnormalities in heart
rate (HR), particularly during daily activities, may serve as early indicators
of HF risk. However, existing HR monitoring tools for HF detection are limited
by their reliability on population-based averages. The estimation of
individualized HR serves as a dynamic digital twin, enabling precise tracking
of cardiac health biomarkers. Current HR estimation methods, categorized into
physiologically-driven and purely data-driven models, struggle with efficiency
and interpretability. This study introduces a novel physiological-model-based
neural network (PMB-NN) framework for HR estimation based on oxygen uptake
(VO2) data during daily physical activities. The framework was trained and
tested on individual datasets from 12 participants engaged in activities
including resting, cycling, and running. By embedding physiological
constraints, which were derived from our proposed simplified human movement
physiological model (PM), into the neural network training process, the PMB-NN
model adheres to human physiological principles while achieving high estimation
accuracy, with a median R$^2$ score of 0.8 and an RMSE of 8.3 bpm. Comparative
statistical analysis demonstrates that the PMB-NN achieves performance on par
with the benchmark neural network model while significantly outperforming
traditional physiological model (p=0.002). In addition, our PMB-NN is adept at
identifying personalized parameters of the PM, enabling the PM to generate
reasonable HR estimation. The proposed framework with a precise VO2 estimation
system derived from body movements enables the future possibilities of
personalized and real-time cardiac monitoring during daily life physical
activities.

</details>


### [104] [Balanced Hyperbolic Embeddings Are Natural Out-of-Distribution Detectors](https://arxiv.org/abs/2506.10146)
*Tejaswi Kasarla,Max van Spengler,Pascal Mettes*

Main category: cs.LG

TL;DR: 本文提出平衡双曲学习方法，通过优化层次双曲嵌入提升分布内外样本的区分能力，实验表明其在13个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在分布外（OOD）样本识别中存在性能瓶颈，需探索更有效的特征表示空间以提升判别能力。

Method: 提出平衡双曲学习框架：设计双曲类嵌入算法联合优化层次结构失真与子层次平衡性，并将双曲原型应用于分类及OOD评分函数扩展。

Result: 在13个数据集和13种评分函数上的实验表明，该方法在相同数据下超越现有OOD方法、双曲方法及对比学习方法，并支持层次化OOD泛化。

Conclusion: 层次双曲嵌入能更有效区分分布内外样本，其平衡优化机制与双曲几何特性共同提升了OOD检测性能与模型泛化能力。

Abstract: Out-of-distribution recognition forms an important and well-studied problem
in deep learning, with the goal to filter out samples that do not belong to the
distribution on which a network has been trained. The conclusion of this paper
is simple: a good hierarchical hyperbolic embedding is preferred for
discriminating in- and out-of-distribution samples. We introduce Balanced
Hyperbolic Learning. We outline a hyperbolic class embedding algorithm that
jointly optimizes for hierarchical distortion and balancing between shallow and
wide subhierarchies. We then use the class embeddings as hyperbolic prototypes
for classification on in-distribution data. We outline how to generalize
existing out-of-distribution scoring functions to operate with hyperbolic
prototypes. Empirical evaluations across 13 datasets and 13 scoring functions
show that our hyperbolic embeddings outperform existing out-of-distribution
approaches when trained on the same data with the same backbones. We also show
that our hyperbolic embeddings outperform other hyperbolic approaches, beat
state-of-the-art contrastive methods, and natively enable hierarchical
out-of-distribution generalization.

</details>


### [105] [Probabilistic Variational Contrastive Learning](https://arxiv.org/abs/2506.10159)
*Minoh Jeong,Seonho Kim,Alfred Hero*

Main category: cs.LG

TL;DR: 本文提出变分对比学习（VCL），通过引入概率建模和KL散度正则化，为对比学习提供不确定性量化能力，同时提升性能并缓解维度坍塌问题。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法（如SimCLR、SupCon）虽性能优异，但缺乏不确定性量化机制。传统确定性嵌入无法捕捉数据分布的不确定性，限制了其在需要概率解释场景中的应用。

Method: 提出VCL的无解码器框架：1) 将InfoNCE损失解释为重建项，2) 在单位超球面添加均匀先验的KL正则项，3) 使用投影正态分布建模后验qθ(z|x)，生成概率嵌入。具体实现包括VSimCLR和VSupCon变体。

Result: 实验表明：1) 缓解维度坍塌现象 2) 提升与类标签的互信息 3) 分类精度匹配或超越基线 4) 通过后验分布提供有效不确定性估计。

Conclusion: VCL为对比学习建立概率基础，在保持性能优势的同时实现不确定性量化，为对比学习方法提供了新的理论框架和实践基础。

Abstract: Deterministic embeddings learned by contrastive learning (CL) methods such as
SimCLR and SupCon achieve state-of-the-art performance but lack a principled
mechanism for uncertainty quantification. We propose Variational Contrastive
Learning (VCL), a decoder-free framework that maximizes the evidence lower
bound (ELBO) by interpreting the InfoNCE loss as a surrogate reconstruction
term and adding a KL divergence regularizer to a uniform prior on the unit
hypersphere. We model the approximate posterior $q_\theta(z|x)$ as a projected
normal distribution, enabling the sampling of probabilistic embeddings. Our two
instantiations--VSimCLR and VSupCon--replace deterministic embeddings with
samples from $q_\theta(z|x)$ and incorporate a normalized KL term into the
loss. Experiments on multiple benchmarks demonstrate that VCL mitigates
dimensional collapse, enhances mutual information with class labels, and
matches or outperforms deterministic baselines in classification accuracy, all
the while providing meaningful uncertainty estimates through the posterior
model. VCL thus equips contrastive learning with a probabilistic foundation,
serving as a new basis for contrastive approaches.

</details>


### [106] [The 2025 PNPL Competition: Speech Detection and Phoneme Classification in the LibriBrain Dataset](https://arxiv.org/abs/2506.10165)
*Gilad Landau,Miran Özdogan,Gereon Elvers,Francesco Mantegna,Pratik Somaiya,Dulhan Jayalath,Luisa Kurth,Teyun Kwon,Brendan Shillingford,Greg Farquhar,Minqi Jiang,Karim Jerbi,Hamza Abdelhedi,Yorguin Mantilla Ramos,Caglar Gulcehre,Mark Woolrich,Natalie Voets,Oiwi Parker Jones*

Main category: cs.LG

TL;DR: 论文提出通过2025 PNPL竞赛推动非侵入式脑数据解码语音的突破，发布最大MEG数据集LibriBrain及工具库pnpl，设立标准化任务与双赛道机制，加速脑机接口研发。


<details>
  <summary>Details</summary>
Motivation: 通过非侵入式脑数据解码技术，为瘫痪或语言障碍患者（如构音障碍）提供无需高风险手术的沟通恢复方案，并推动该领域实现类似ImageNet的突破性进展。

Method: 发布迄今最大规模个体内MEG数据集LibriBrain及配套Python库pnpl；定义语音检测与音素分类两大基础任务，提供标准化数据划分、评估指标、基准模型、教程代码、社区论坛和公开排行榜；设置强调算法创新的标准赛道与侧重大规模计算的扩展赛道。

Result: 构建了支持深度学习框架集成的数据基础设施，建立了包含双赛道的竞赛框架，并通过社区协作机制促进非侵入式脑机接口技术发展。

Conclusion: 通过开放数据集、工具链和竞赛机制，动员机器学习社区共同解决非侵入式神经解码难题，为开发实用化语音脑机接口奠定基础，最终改善患者生活质量。

Abstract: The advance of speech decoding from non-invasive brain data holds the
potential for profound societal impact. Among its most promising applications
is the restoration of communication to paralysed individuals affected by speech
deficits such as dysarthria, without the need for high-risk surgical
interventions. The ultimate aim of the 2025 PNPL competition is to produce the
conditions for an "ImageNet moment" or breakthrough in non-invasive neural
decoding, by harnessing the collective power of the machine learning community.
  To facilitate this vision we present the largest within-subject MEG dataset
recorded to date (LibriBrain) together with a user-friendly Python library
(pnpl) for easy data access and integration with deep learning frameworks. For
the competition we define two foundational tasks (i.e. Speech Detection and
Phoneme Classification from brain data), complete with standardised data splits
and evaluation metrics, illustrative benchmark models, online tutorial code, a
community discussion board, and public leaderboard for submissions. To promote
accessibility and participation the competition features a Standard track that
emphasises algorithmic innovation, as well as an Extended track that is
expected to reward larger-scale computing, accelerating progress toward a
non-invasive brain-computer interface for speech.

</details>


### [107] [Wasserstein Barycenter Soft Actor-Critic](https://arxiv.org/abs/2506.10167)
*Zahra Shahrooei,Ali Baheri*

Main category: cs.LG

TL;DR: 提出WBSAC算法，通过结合悲观与乐观策略，利用Wasserstein重心作为探索策略，在稀疏奖励环境中显著提升深度强化学习的样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有深度off-policy actor-critic算法在连续控制任务中样本效率低下，尤其在稀疏奖励环境下表现受限，需改进探索策略。

Method: 使用Wasserstein重心整合悲观策略（用于时序差分学习）和乐观策略（促进探索），动态调整探索强度，构建WBSAC算法框架。

Result: 在MuJoCo连续控制基准测试中，WBSAC相比现有state-of-the-art算法展现出更高的样本效率优势。

Conclusion: WBSAC通过定向探索策略有效解决了稀疏奖励环境下的样本效率问题，为连续控制强化学习提供了新方法。

Abstract: Deep off-policy actor-critic algorithms have emerged as the leading framework
for reinforcement learning in continuous control domains. However, most of
these algorithms suffer from poor sample efficiency, especially in environments
with sparse rewards. In this paper, we take a step towards addressing this
issue by providing a principled directed exploration strategy. We propose
Wasserstein Barycenter Soft Actor-Critic (WBSAC) algorithm, which benefits from
a pessimistic actor for temporal difference learning and an optimistic actor to
promote exploration. This is achieved by using the Wasserstein barycenter of
the pessimistic and optimistic policies as the exploration policy and adjusting
the degree of exploration throughout the learning process. We compare WBSAC
with state-of-the-art off-policy actor-critic algorithms and show that WBSAC is
more sample-efficient on MuJoCo continuous control tasks.

</details>


### [108] [Geometric Regularity in Deterministic Sampling of Diffusion-based Generative Models](https://arxiv.org/abs/2506.10177)
*Defang Chen,Zhenyu Zhou,Can Wang,Siwei Lyu*

Main category: cs.LG

TL;DR: 扩散模型采样轨迹呈现低维相似结构，提出动态规划优化时间表，提升生成效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于ODE的扩散模型采样轨迹存在未被探索的几何规律性，研究其结构可优化采样过程。

Method: 通过分析采样轨迹的低维子空间特性，设计基于动态规划的时间调度策略，最小化修改现有ODE求解器。

Result: 在5~10次函数评估场景下，新方法显著提升图像生成质量，计算开销可忽略。

Conclusion: 扩散模型采样轨迹具有普适性几何规律，利用该特性可高效优化采样过程，为生成模型优化提供新方向。

Abstract: Diffusion-based generative models employ stochastic differential equations
(SDEs) and their equivalent probability flow ordinary differential equations
(ODEs) to establish a smooth transformation between complex high-dimensional
data distributions and tractable prior distributions. In this paper, we reveal
a striking geometric regularity in the deterministic sampling dynamics: each
simulated sampling trajectory lies within an extremely low-dimensional
subspace, and all trajectories exhibit an almost identical ''boomerang'' shape,
regardless of the model architecture, applied conditions, or generated content.
We characterize several intriguing properties of these trajectories,
particularly under closed-form solutions based on kernel-estimated data
modeling. We also demonstrate a practical application of the discovered
trajectory regularity by proposing a dynamic programming-based scheme to better
align the sampling time schedule with the underlying trajectory structure. This
simple strategy requires minimal modification to existing ODE-based numerical
solvers, incurs negligible computational overhead, and achieves superior image
generation performance, especially in regions with only $5 \sim 10$ function
evaluations.

</details>


### [109] [A Comparative Study of Machine Learning Techniques for Early Prediction of Diabetes](https://arxiv.org/abs/2506.10180)
*Mowafaq Salem Alzboon,Mohammad Al-Batah,Muhyeeddin Alqaraleh,Ahmad Abuashour,Ahmad Fuad Bader*

Main category: cs.LG

TL;DR: 研究使用多种机器学习算法在Pima Indians糖尿病数据集上进行糖尿病预测，神经网络以78.57%准确率表现最佳，表明机器学习可有效辅助早期糖尿病检测。


<details>
  <summary>Details</summary>
Motivation: 糖尿病已成为全球重大健康问题，早期识别与管理至关重要。通过机器学习算法提升糖尿病预测效率，可为临床提供有效工具。

Method: 使用包含768例患者数据的Pima Indians糖尿病数据集，评估逻辑回归、决策树、随机森林、K近邻、朴素贝叶斯、支持向量机、梯度提升和神经网络共8种算法的预测性能。

Result: 神经网络算法以78.57%准确率表现最优，随机森林以76.30%次之，其他算法性能依次递减。

Conclusion: 机器学习算法（特别是神经网络和随机森林）在糖尿病预测中展现出显著潜力，可作为早期筛查的高效工具。

Abstract: In many nations, diabetes is becoming a significant health problem, and early
identification and control are crucial. Using machine learning algorithms to
predict diabetes has yielded encouraging results. Using the Pima Indians
Diabetes dataset, this study attempts to evaluate the efficacy of several
machine-learning methods for diabetes prediction. The collection includes
information on 768 patients, such as their ages, BMIs, and glucose levels. The
techniques assessed are Logistic Regression, Decision Tree, Random Forest,
k-Nearest Neighbors, Naive Bayes, Support Vector Machine, Gradient Boosting,
and Neural Network. The findings indicate that the Neural Network algorithm
performed the best, with an accuracy of 78.57 percent, followed by the Random
Forest method, with an accuracy of 76.30 percent. The study implies that
machine learning algorithms can aid diabetes prediction and be an efficient
early detection tool.

</details>


### [110] [Optimizing Genetic Algorithms with Multilayer Perceptron Networks for Enhancing TinyFace Recognition](https://arxiv.org/abs/2506.10184)
*Mohammad Subhi Al-Batah,Mowafaq Salem Alzboon,Muhyeeddin Alqaraleh*

Main category: cs.LG

TL;DR: 本研究通过TinyFace、Heart Disease和Iris三个数据集，对比了基线MLP、遗传算法(GA)特征选择与PCA降维方法对MLP性能的影响，发现GA在复杂数据集表现更优，而PCA适用于低维无噪声数据。


<details>
  <summary>Details</summary>
Motivation: 探讨特征选择（GA）与降维（PCA）对多层感知机（MLP）性能的差异及协同作用，为特征工程与神经网络优化提供实践指导。

Method: 采用三种方法：1) 默认参数MLP基线训练；2) 基于遗传算法的特征选择；3) PCA降维处理，并在三个异构数据集上进行实验验证。

Result: PCA在低维/无噪声数据中有效，而GA通过精准筛选特征显著提升复杂数据集准确率；特征选择与降维对MLP性能提升具有互补性。

Conclusion: 特征工程中GA与PCA需根据数据特性结合使用，二者协同优化可增强MLP模型表现，研究为机器学习任务提供了特征处理与参数调优的实用准则。

Abstract: This study conducts an empirical examination of MLP networks investigated
through a rigorous methodical experimentation process involving three diverse
datasets: TinyFace, Heart Disease, and Iris. Study Overview: The study includes
three key methods: a) a baseline training using the default settings for the
Multi-Layer Perceptron (MLP), b) feature selection using Genetic Algorithm (GA)
based refinement c) Principal Component Analysis (PCA) based dimension
reduction. The results show important information on how such techniques affect
performance. While PCA had showed benefits in low-dimensional and noise-free
datasets GA consistently increased accuracy in complex datasets by accurately
identifying critical features. Comparison reveals that feature selection and
dimensionality reduction play interdependent roles in enhancing MLP
performance. The study contributes to the literature on feature engineering and
neural network parameter optimization, offering practical guidelines for a wide
range of machine learning tasks

</details>


### [111] [Scalable Non-Equivariant 3D Molecule Generation via Rotational Alignment](https://arxiv.org/abs/2506.10186)
*Yuhui Ding,Thomas Hofmann*

Main category: cs.LG

TL;DR: 本文提出一种放宽等变性约束的扩散模型方法，通过构建对齐潜在空间，使非等变模型在3D生成任务中达到与等变模型相当的质量，同时提升训练和采样效率。


<details>
  <summary>Details</summary>
Motivation: 现有等变扩散模型因专用架构限制可扩展性与效率，需探索在非等变框架下保持性能的方法。

Method: 为每个分子学习样本相关的SO(3)变换以构建潜在对齐空间，随后在非等变扩散模型中进行训练。

Result: 实验表明该方法显著优于传统非等变模型，质量与SOTA等变模型相当，且训练/采样效率更高。

Conclusion: 通过放松等变性约束构建对齐表征的方法，在保持生成质量的同时突破了等变架构的效率瓶颈。

Abstract: Equivariant diffusion models have achieved impressive performance in 3D
molecule generation. These models incorporate Euclidean symmetries of 3D
molecules by utilizing an SE(3)-equivariant denoising network. However,
specialized equivariant architectures limit the scalability and efficiency of
diffusion models. In this paper, we propose an approach that relaxes such
equivariance constraints. Specifically, our approach learns a sample-dependent
SO(3) transformation for each molecule to construct an aligned latent space. A
non-equivariant diffusion model is then trained over the aligned
representations. Experimental results demonstrate that our approach performs
significantly better than previously reported non-equivariant models. It yields
sample quality comparable to state-of-the-art equivariant diffusion models and
offers improved training and sampling efficiency. Our code is available at
https://github.com/skeletondyh/RADM

</details>


### [112] [Improving Oral Cancer Outcomes Through Machine Learning and Dimensionality Reduction](https://arxiv.org/abs/2506.10189)
*Mohammad Subhi Al-Batah,Muhyeeddin Alqaraleh,Mowafaq Salem Alzboon*

Main category: cs.LG

TL;DR: 本文综述了机器学习在口腔癌诊断与预后，发现神经网络以93.6%准确率最优，并强调特征选择与降维对提升模型性能的作用。


<details>
  <summary>Details</summary>
Motivation: 口腔癌的早期诊断与精准预测对提高患者生存率至关重要，传统方法存在局限性，需借助先进数据挖掘技术优化诊疗流程。

Method: 系统评估神经网络、KNN、SVM及集成学习在口腔癌分类中的应用，结合特征选择与降维技术进行模型优化。

Result: 神经网络表现最佳，分类准确率达93.6%；特征选择与降维可显著提升模型性能。

Conclusion: 先进数据挖掘技术能有效增强口腔癌早期检测能力，优化治疗策略，最终改善患者临床结局。

Abstract: Oral cancer presents a formidable challenge in oncology, necessitating early
diagnosis and accurate prognosis to enhance patient survival rates. Recent
advancements in machine learning and data mining have revolutionized
traditional diagnostic methodologies, providing sophisticated and automated
tools for differentiating between benign and malignant oral lesions. This study
presents a comprehensive review of cutting-edge data mining methodologies,
including Neural Networks, K-Nearest Neighbors (KNN), Support Vector Machines
(SVM), and ensemble learning techniques, specifically applied to the diagnosis
and prognosis of oral cancer. Through a rigorous comparative analysis, our
findings reveal that Neural Networks surpass other models, achieving an
impressive classification accuracy of 93,6 % in predicting oral cancer.
Furthermore, we underscore the potential benefits of integrating feature
selection and dimensionality reduction techniques to enhance model performance.
These insights underscore the significant promise of advanced data mining
techniques in bolstering early detection, optimizing treatment strategies, and
ultimately improving patient outcomes in the realm of oral oncology.

</details>


### [113] [DynaSubVAE: Adaptive Subgrouping for Scalable and Robust OOD Detection](https://arxiv.org/abs/2506.10200)
*Tina Behrouzi,Sana Tonekaboni,Rahul G. Krishnan,Anna Goldenberg*

Main category: cs.LG

TL;DR: 提出DynaSubVAE框架，通过动态子群划分和自适应OOD检测，解决传统模型忽视数据异质子群的问题，在多种OOD场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现实观测数据常存在偏离全局模式的异质子群，现有模型忽视这些群体导致预测错误。传统方法仅将此类样本检测为域外(OOD)，而非主动适应新趋势。

Method: 结合表示学习与动态OOD检测的变分自编码器框架，通过高斯混合模型启发的非参数聚类机制，动态更新潜在结构以捕捉新子群模式。

Result: 在近/远OOD检测中表现竞争力强，尤其擅长训练数据缺失整类的场景；动态子群划分机制优于GMM/KMeans++等独立聚类方法。

Conclusion: DynaSubVAE通过动态建模数据演化趋势有效提升模型对异质子群的适应性，为OOD检测提供兼顾准确性与灵活性的解决方案。

Abstract: Real-world observational data often contain existing or emerging
heterogeneous subpopulations that deviate from global patterns. The majority of
models tend to overlook these underrepresented groups, leading to inaccurate or
even harmful predictions. Existing solutions often rely on detecting these
samples as Out-of-domain (OOD) rather than adapting the model to new emerging
patterns. We introduce DynaSubVAE, a Dynamic Subgrouping Variational
Autoencoder framework that jointly performs representation learning and
adaptive OOD detection. Unlike conventional approaches, DynaSubVAE evolves with
the data by dynamically updating its latent structure to capture new trends. It
leverages a novel non-parametric clustering mechanism, inspired by Gaussian
Mixture Models, to discover and model latent subgroups based on embedding
similarity. Extensive experiments show that DynaSubVAE achieves competitive
performance in both near-OOD and far-OOD detection, and excels in class-OOD
scenarios where an entire class is missing during training. We further
illustrate that our dynamic subgrouping mechanism outperforms standalone
clustering methods such as GMM and KMeans++ in terms of both OOD accuracy and
regret precision.

</details>


### [114] [AWP: Activation-Aware Weight Pruning and Quantization with Projected Gradient Descent](https://arxiv.org/abs/2506.10205)
*Jing Liu,Toshiaki Koike-Akino,Ye Wang,Hassan Mansour,Matthew Brand*

Main category: cs.LG

TL;DR: 本文提出一种基于投影梯度下降的统一激活感知权重剪枝与量化方法（AWP），用于压缩大型语言模型，在实验中优于现有方法，并提供理论收敛保证。


<details>
  <summary>Details</summary>
Motivation: 针对大型语言模型（LLMs）在边缘设备部署时因参数量过大导致的效率问题，需通过模型压缩（如量化、剪枝）降低计算资源需求。现有方法在激活感知权重剪枝与量化协同优化方面存在不足。

Method: 结合激活感知权重剪枝与稀疏近似问题，受迭代硬阈值（IHT）启发，提出投影梯度下降的统一框架AWP，同步优化权重剪枝和量化。

Result: 实验表明AWP在LLM剪枝与量化任务中优于当前最优方法，且为剪枝过程提供了理论收敛性证明。

Conclusion: AWP通过统一框架有效协同优化剪枝与量化，显著提升模型压缩性能，其理论保证进一步验证了方法的可靠性。

Abstract: To address the enormous size of Large Language Models (LLMs), model
compression methods, such as quantization and pruning, are often deployed,
especially on edge devices. In this work, we focus on layer-wise post-training
quantization and pruning. Drawing connections between activation-aware weight
pruning and sparse approximation problems, and motivated by the success of
Iterative Hard Thresholding (IHT), we propose a unified method for
Activation-aware Weight pruning and quantization via Projected gradient descent
(AWP). Our experiments demonstrate that AWP outperforms state-of-the-art LLM
pruning and quantization methods. Theoretical convergence guarantees of the
proposed method for pruning are also provided.

</details>


### [115] [Cross-Learning Between ECG and PCG: Exploring Common and Exclusive Characteristics of Bimodal Electromechanical Cardiac Waveforms](https://arxiv.org/abs/2506.10212)
*Sajjad Karimi,Amit J. Shah,Gari D. Clifford,Reza Sameni*

Main category: cs.LG

TL;DR: 研究通过ECG和PCG信号的非线性互重建，揭示心脏电-机械活动关系，发现非线性模型（如非因果LSTM）在跨模态重建中表现更优，并验证PCG可估计ECG生物标志物。


<details>
  <summary>Details</summary>
Motivation: ECG与PCG分别捕捉心脏电活动与机械活动，但二者信息重叠性、互重建潜力及生物标志物提取能力尚未明确，尤其在动态生理条件和个体差异下。

Method: 使用EPHNOGRAM数据集中的静息/运动状态ECG-PCG同步记录，结合线性和非线性机器学习模型（含非因果LSTM），分析因果关系、生理状态与跨受试者变异对互重建的影响。

Result: 非线性模型（特别是非因果LSTM）重建性能更优，PCG→ECG重建优于反向；基于包络的瞬时振幅特征提升跨受试者泛化能力；PCG可跨个体估计ECG生物标志物（如QT间期）。

Conclusion: 揭示了心脏电-机械模态的波形特征与事件时序关系，为新型多模态心脏监测技术提供理论基础，证明跨模态学习在临床参数估计中的可行性。

Abstract: Simultaneous electrocardiography (ECG) and phonocardiogram (PCG) provide a
comprehensive, multimodal perspective on cardiac function by capturing the
heart's electrical and mechanical activities, respectively. However, the
distinct and overlapping information content of these signals, as well as their
potential for mutual reconstruction and biomarker extraction, remains
incompletely understood, especially under varying physiological conditions and
across individuals.
  In this study, we systematically investigate the common and exclusive
characteristics of ECG and PCG using the EPHNOGRAM dataset of simultaneous
ECG-PCG recordings during rest and exercise. We employ a suite of linear and
nonlinear machine learning models, including non-causal LSTM networks, to
reconstruct each modality from the other and analyze the influence of
causality, physiological state, and cross-subject variability. Our results
demonstrate that nonlinear models, particularly non-causal LSTM, provide
superior reconstruction performance, with reconstructing ECG from PCG proving
more tractable than the reverse. Exercise and cross-subject scenarios present
significant challenges, but envelope-based modeling that utilizes instantaneous
amplitude features substantially improves cross-subject generalizability for
cross-modal learning. Furthermore, we demonstrate that clinically relevant ECG
biomarkers, such as fiducial points and QT intervals, can be estimated from PCG
in cross-subject settings.
  These findings advance our understanding of the relationship between
electromechanical cardiac modalities, in terms of both waveform characteristics
and the timing of cardiac events, with potential applications in novel
multimodal cardiac monitoring technologies.

</details>


### [116] [LaMAGIC2: Advanced Circuit Formulations for Language Model-Based Analog Topology Generation](https://arxiv.org/abs/2506.10235)
*Chen-Chia Chang,Wan-Hsuan Lin,Yikang Shen,Yiran Chen,Xin Zhang*

Main category: cs.LG

TL;DR: LaMAGIC2提出了一种基于标识符的简洁浮点输入规范表示(SFCI)，显著提升模拟电路拓扑生成的效率与数值精度敏感性，在严格容差下成功率提高34%，MSE降低10倍。


<details>
  <summary>Details</summary>
Motivation: 现有基于语言模型的模拟电路生成方法存在O(|V|²)的冗余电路表示形式，且对数值输入精度敏感度低，导致生成效率与性能受限。

Method: 采用标识符增强组件类型识别，将token长度复杂度降至O(|V|)，并通过浮点输入规范提升数值精度敏感性，构建LaMAGIC2框架。

Result: 实验表明：在0.01严格容差下成功率提升34%，MSE降低10倍；对大规模电路迁移性最高提升58.5%。

Conclusion: LaMAGIC2通过SFCI框架有效解决了电路表示效率与数值敏感度问题，为模拟拓扑生成提供了鲁棒解决方案。

Abstract: Automation of analog topology design is crucial due to customized
requirements of modern applications with heavily manual engineering efforts.
The state-of-the-art work applies a sequence-to-sequence approach and
supervised finetuning on language models to generate topologies given user
specifications. However, its circuit formulation is inefficient due to O(|V |2)
token length and suffers from low precision sensitivity to numeric inputs. In
this work, we introduce LaMAGIC2, a succinct float-input canonical formulation
with identifier (SFCI) for language model-based analog topology generation.
SFCI addresses these challenges by improving component-type recognition through
identifier-based representations, reducing token length complexity to O(|V |),
and enhancing numeric precision sensitivity for better performance under tight
tolerances. Our experiments demonstrate that LaMAGIC2 achieves 34% higher
success rates under a tight tolerance of 0.01 and 10X lower MSEs compared to a
prior method. LaMAGIC2 also exhibits better transferability for circuits with
more vertices with up to 58.5% improvement. These advancements establish
LaMAGIC2 as a robust framework for analog topology generation.

</details>


### [117] [A new type of federated clustering: A non-model-sharing approach](https://arxiv.org/abs/2506.10244)
*Yuji Kawamata,Kaoru Kamijo,Maki Kihira,Akihiro Toyoda,Tomoru Nakayama,Akira Imakura,Tetsuya Sakurai,Yukihiko Okada*

Main category: cs.LG

TL;DR: 本文提出了一种新型联邦聚类方法DC-Clustering，支持在水平与垂直数据划分共存场景下进行隐私保护的协作聚类，性能接近集中式聚类，适用于医疗、金融等领域。


<details>
  <summary>Details</summary>
Motivation: 现有联邦聚类方法仅支持简单数据划分（水平或垂直），无法处理复杂分布式结构，限制了在异构数据场景中的应用。

Method: 通过共享中间表征而非原始数据实现隐私保护，支持k-means/谱聚类灵活选择，仅需单轮中央服务器通信完成聚类。

Result: 合成与公开数据集实验表明，DC-Clustering性能与集中式聚类相当，且满足隐私、通信效率与灵活性需求。

Conclusion: DC-Clustering填补了联邦学习中复杂数据划分场景的聚类空白，为跨机构异构数据知识发现提供了实用工具。

Abstract: In recent years, the growing need to leverage sensitive data across
institutions has led to increased attention on federated learning (FL), a
decentralized machine learning paradigm that enables model training without
sharing raw data. However, existing FL-based clustering methods, known as
federated clustering, typically assume simple data partitioning scenarios such
as horizontal or vertical splits, and cannot handle more complex distributed
structures. This study proposes data collaboration clustering (DC-Clustering),
a novel federated clustering method that supports clustering over complex data
partitioning scenarios where horizontal and vertical splits coexist. In
DC-Clustering, each institution shares only intermediate representations
instead of raw data, ensuring privacy preservation while enabling collaborative
clustering. The method allows flexible selection between k-means and spectral
clustering, and achieves final results with a single round of communication
with the central server. We conducted extensive experiments using synthetic and
open benchmark datasets. The results show that our method achieves clustering
performance comparable to centralized clustering where all data are pooled.
DC-Clustering addresses an important gap in current FL research by enabling
effective knowledge discovery from distributed heterogeneous data. Its
practical properties -- privacy preservation, communication efficiency, and
flexibility -- make it a promising tool for privacy-sensitive domains such as
healthcare and finance.

</details>


### [118] [Meta-learning Representations for Learning from Multiple Annotators](https://arxiv.org/abs/2506.10259)
*Atsutoshi Kumagai,Tomoharu Iwata,Taishi Nishiyama,Yasutoshi Ida,Yasuhiro Fujiwara*

Main category: cs.LG

TL;DR: 提出一种基于元学习的方法，从多噪声标注者数据中学习分类器。通过潜在空间建模标注者能力，结合EM算法和可微分元学习框架，在小样本场景下提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有多标注者学习方法需要大量噪声数据，但实际场景常面临数据不足问题。本文旨在利用相关任务的标注数据，通过元学习解决小样本噪声标注场景下的分类器训练问题。

Method: 构建神经网络将样本映射到潜在空间，建立概率模型估计标注者能力。通过EM算法进行任务特定分类器的最大后验适配，并利用可微分特性反向传播元学习神经网络参数。

Result: 在合成噪声数据集和真实众包数据集上验证有效性，显示该方法在小样本场景下优于传统方法。

Conclusion: 提出的元学习框架通过潜在空间建模和可微分EM算法，有效利用跨任务数据提升小样本噪声标注场景下的模型性能，实验证明其实际应用价值。

Abstract: We propose a meta-learning method for learning from multiple noisy
annotators. In many applications such as crowdsourcing services, labels for
supervised learning are given by multiple annotators. Since the annotators have
different skills or biases, given labels can be noisy. To learn accurate
classifiers, existing methods require many noisy annotated data. However,
sufficient data might be unavailable in practice. To overcome the lack of data,
the proposed method uses labeled data obtained in different but related tasks.
The proposed method embeds each example in tasks to a latent space by using a
neural network and constructs a probabilistic model for learning a
task-specific classifier while estimating annotators' abilities on the latent
space. This neural network is meta-learned to improve the expected test
classification performance when the classifier is adapted to a given small
amount of annotated data. This classifier adaptation is performed by maximizing
the posterior probability via the expectation-maximization (EM) algorithm.
Since each step in the EM algorithm is easily computed as a closed-form and is
differentiable, the proposed method can efficiently backpropagate the loss
through the EM algorithm to meta-learn the neural network. We show the
effectiveness of our method with real-world datasets with synthetic noise and
real-world crowdsourcing datasets.

</details>


### [119] [Interior-Point Vanishing Problem in Semidefinite Relaxations for Neural Network Verification](https://arxiv.org/abs/2506.10269)
*Ryota Ueda,Takami Sato,Ken Kobayashi,Kazuhide Nakata*

Main category: cs.LG

TL;DR: 本文分析了SDP松弛在深度ReLU神经网络验证中的严格可行性丢失问题（内点消失），并提出五种解决方案，显著提升问题解决率，同时揭示了传统约束的不合理性。


<details>
  <summary>Details</summary>
Motivation: 现有基于SDP松弛的深度神经网络验证方法在深度增加时面临严格可行性丢失（内点消失），导致数值不稳定和最优性失效，阻碍其可扩展性。

Method: 通过理论和实证分析验证内点消失现象，设计并测试五种增强验证问题可行性的方法，包括重新评估传统约束的有效性。

Result: 所提方法成功解决了88%现有方法无法处理的问题（占总问题41%），并证明传统ReLU单元上下界约束不仅无益反而损害可行性。

Conclusion: 该研究揭示了SDP验证在深度网络中的根本性限制，提供实用解决方案并修正传统约束认知，为构建可靠深度神经网络系统提供新见解。

Abstract: Semidefinite programming (SDP) relaxation has emerged as a promising approach
for neural network verification, offering tighter bounds than other convex
relaxation methods for deep neural networks (DNNs) with ReLU activations.
However, we identify a critical limitation in the SDP relaxation when applied
to deep networks: interior-point vanishing, which leads to the loss of strict
feasibility -- a crucial condition for the numerical stability and optimality
of SDP. Through rigorous theoretical and empirical analysis, we demonstrate
that as the depth of DNNs increases, the strict feasibility is likely to be
lost, creating a fundamental barrier to scaling SDP-based verification. To
address the interior-point vanishing, we design and investigate five solutions
to enhance the feasibility conditions of the verification problem. Our methods
can successfully solve 88% of the problems that could not be solved by existing
methods, accounting for 41% of the total. Our analysis also reveals that the
valid constraints for the lower and upper bounds for each ReLU unit are
traditionally inherited from prior work without solid reasons, but are actually
not only unbeneficial but also even harmful to the problem's feasibility. This
work provides valuable insights into the fundamental challenges of SDP-based
DNN verification and offers practical solutions to improve its applicability to
deeper neural networks, contributing to the development of more reliable and
secure systems with DNNs.

</details>


### [120] [Graph-MLLM: Harnessing Multimodal Large Language Models for Multimodal Graph Learning](https://arxiv.org/abs/2506.10282)
*Jiajin Liu,Dongzhe Fan,Jiacheng Shen,Chuanhao Ji,Daochen Zha,Qiaoyu Tan*

Main category: cs.LG

TL;DR: 本文提出Graph-MLLM基准，系统评估多模态大语言模型（MLLM）在六大数据集上的三种多模态图学习范式（Encoder、Aligner、Predictor），发现结合节点视觉与文本属性可提升性能，视觉转文本描述效果更优，且微调MLLM无需显式图结构即可达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在多模态图（MMG）学习中仅关注模态对齐而忽略数据点间结构关系，且缺乏统一评估基准，难以衡量不同方法的进展。

Method: 构建Graph-MLLM基准框架，通过六种领域数据集系统评估三种MMG学习范式（MLLM-as-Encoder、Aligner、Predictor），分析预训练模型（如CLIP）特征融合、模态对齐策略及微调方法的效果。

Result: 实验表明：1) 联合视觉与文本属性可提升图学习性能；2) 视觉转文本描述优于直接输入原始视觉数据；3) 微调MLLM在多数场景下达到SOTA，即使无显式图结构信息。

Conclusion: Graph-MLLM填补了多模态图学习评估空白，实验结果揭示了模态融合与微调策略的有效性，开源库将促进公平评估与创新研究。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in representing and understanding diverse modalities. However,
they typically focus on modality alignment in a pairwise manner while
overlooking structural relationships across data points. Integrating
multimodality with structured graph information (i.e., multimodal graphs, MMGs)
is essential for real-world applications such as social networks, healthcare,
and recommendation systems. Existing MMG learning methods fall into three
paradigms based on how they leverage MLLMs: Encoder, Aligner, and Predictor.
MLLM-as-Encoder focuses on enhancing graph neural networks (GNNs) via
multimodal feature fusion; MLLM-as-Aligner aligns multimodal attributes in
language or hidden space to enable LLM-based graph reasoning; MLLM-as-Predictor
treats MLLMs as standalone reasoners with in-context learning or fine-tuning.
Despite their advances, the MMG field lacks a unified benchmark to fairly
evaluate across these approaches, making it unclear what progress has been
made. To bridge this gap, we present Graph-MLLM, a comprehensive benchmark for
multimodal graph learning by systematically evaluating these three paradigms
across six datasets with different domains. Through extensive experiments, we
observe that jointly considering the visual and textual attributes of the nodes
benefits graph learning, even when using pre-trained text-to-image alignment
models (e.g., CLIP) as encoders. We also find that converting visual attributes
into textual descriptions further improves performance compared to directly
using visual inputs. Moreover, we observe that fine-tuning MLLMs on specific
MMGs can achieve state-of-the-art results in most scenarios, even without
explicit graph structure information. We hope that our open-sourced library
will facilitate rapid, equitable evaluation and inspire further innovative
research in this field.

</details>


### [121] [Collaborative Min-Max Regret in Grouped Multi-Armed Bandits](https://arxiv.org/abs/2506.10313)
*Moïse Blanchard,Vineet Goyal*

Main category: cs.LG

TL;DR: 本文提出Col-UCB算法，通过动态协调多组多臂老虎机中的探索行为，解决传统算法导致的组间探索成本不平衡问题，实现了最优协作遗憾界限。


<details>
  <summary>Details</summary>
Motivation: 传统多臂老虎机算法在分组场景中可能导致组间探索成本显著不平衡。研究旨在平衡不同群体间的探索负担，最小化跨组最大遗憾。

Method: 提出Col-UCB算法，通过动态协调组间探索行为，利用共享动作集结构实现跨组信息共享与资源分配。

Result: Col-UCB在极小极大和实例相关协作遗憾上达到对数因子内的理论最优，且适应不同组间动作集的重叠结构。

Conclusion: 协作机制能有效平衡探索负担，其优势取决于组间动作集的共享结构，为群体协同学习提供了理论依据。

Abstract: We study the impact of sharing exploration in multi-armed bandits in a
grouped setting where a set of groups have overlapping feasible action sets
[Baek and Farias '24]. In this grouped bandit setting, groups share reward
observations, and the objective is to minimize the collaborative regret,
defined as the maximum regret across groups. This naturally captures
applications in which one aims to balance the exploration burden between groups
or populations -- it is known that standard algorithms can lead to
significantly imbalanced exploration cost between groups. We address this
problem by introducing an algorithm Col-UCB that dynamically coordinates
exploration across groups. We show that Col-UCB achieves both optimal minimax
and instance-dependent collaborative regret up to logarithmic factors. These
bounds are adaptive to the structure of shared action sets between groups,
providing insights into when collaboration yields significant benefits over
each group learning their best action independently.

</details>


### [122] [Detecting Sockpuppetry on Wikipedia Using Meta-Learning](https://arxiv.org/abs/2506.10314)
*Luc Raszewski,Christine De Kock*

Main category: cs.LG

TL;DR: 本文提出使用元学习方法检测维基百科恶意马甲账户，通过跨任务训练提升数据稀缺场景下的检测精度，并发布新数据集以推动相关研究。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖风格和元数据特征，但难以适应特定马甲群体的行为模式，尤其在文本数据有限时效果不佳。需开发更具适应性的检测技术以应对开放编辑平台的虚假信息传播。

Method: 采用元学习技术，通过在多个任务上联合训练模型，使其能快速适应新马甲群体的写作风格特征。

Result: 实验表明元学习方法显著优于预训练模型，预测精度提升明显，验证了该方法在数据稀缺场景的有效性。

Conclusion: 元学习为开放平台马甲检测提供了新思路，同时公开的数据集将促进虚假账户检测与元学习领域的交叉研究。

Abstract: Malicious sockpuppet detection on Wikipedia is critical to preserving access
to reliable information on the internet and preventing the spread of
disinformation. Prior machine learning approaches rely on stylistic and
meta-data features, but do not prioritise adaptability to author-specific
behaviours. As a result, they struggle to effectively model the behaviour of
specific sockpuppet-groups, especially when text data is limited. To address
this, we propose the application of meta-learning, a machine learning technique
designed to improve performance in data-scarce settings by training models
across multiple tasks. Meta-learning optimises a model for rapid adaptation to
the writing style of a new sockpuppet-group. Our results show that
meta-learning significantly enhances the precision of predictions compared to
pre-trained models, marking an advancement in combating sockpuppetry on open
editing platforms. We release a new dataset of sockpuppet investigations to
foster future research in both sockpuppetry and meta-learning fields.

</details>


### [123] [PyLO: Towards Accessible Learned Optimizers in PyTorch](https://arxiv.org/abs/2506.10315)
*Paul Janson,Benjamin Therien,Quentin Anthony,Xiaolong Huang,Abhinav Moudgil,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 本文提出PyLO，一个基于PyTorch的库，旨在通过用户友好的工作流程将学习型优化器推广至更广泛的机器学习社区，解决了现有方法依赖JAX和缺乏易用性的问题，并在大规模预训练任务中验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 现有学习型优化器（如VeLO）因依赖JAX且缺乏易用工具，导致社区难以使用。本文旨在通过开发用户友好的PyTorch库，降低学习型优化器的应用门槛，并探索其在真实大规模任务中的潜力。

Method: 开发PyLO库，集成CUDA加速的small_fc_lopt优化器架构，支持与学习率调度、权重衰减等现有工具结合，并针对ViT B/16等模型进行大规模预训练实验。

Result: PyLO将ViT B/16训练吞吐量从39.59提升至205.59样本/秒（批量32），且结合传统优化工具后，学习型优化器性能显著增强。

Conclusion: PyLO通过易用性和高效性推动了学习型优化器的实际应用，证明其在大规模任务中可超越传统方法，并为社区提供了开源工具。

Abstract: Learned optimizers have been an active research topic over the past decade,
with increasing progress toward practical, general-purpose optimizers that can
serve as drop-in replacements for widely used methods like Adam. However,
recent advances -- such as VeLO, which was meta-trained for 4000 TPU-months --
remain largely inaccessible to the broader community, in part due to their
reliance on JAX and the absence of user-friendly packages for applying the
optimizers after meta-training. To address this gap, we introduce PyLO, a
PyTorch-based library that brings learned optimizers to the broader machine
learning community through familiar, widely adopted workflows. Unlike prior
work focused on synthetic or convex tasks, our emphasis is on applying learned
optimization to real-world large-scale pre-training tasks. Our release includes
a CUDA-accelerated version of the small_fc_lopt learned optimizer architecture
from (Metz et al., 2022a), delivering substantial speedups -- from 39.36 to
205.59 samples/sec throughput for training ViT B/16 with batch size 32. PyLO
also allows us to easily combine learned optimizers with existing optimization
tools such as learning rate schedules and weight decay. When doing so, we find
that learned optimizers can substantially benefit. Our code is available at
https://github.com/Belilovsky-Lab/pylo

</details>


### [124] [Air in Your Neighborhood: Fine-Grained AQI Forecasting Using Mobile Sensor Data](https://arxiv.org/abs/2506.10332)
*Aaryam Sharma*

Main category: cs.LG

TL;DR: 使用时空图神经网络在德里地区实现高精度AQI预测，MSE降低79%并揭示新规律。


<details>
  <summary>Details</summary>
Motivation: 现有政府发布的AQI数据因传感器稀疏无法反映局部实际污染情况，尤其在发展中国家存在健康风险监测缺口。

Method: 采用时空图神经网络(Spatio-temporal GNNs)处理AirDelhi数据集，预测1平方公里细粒度区域的AQI值。

Result: 模型在未见坐标上实现71.654 MSE，较现有方法降低79%，同时揭示AQI存在短期重复模式及动态空间关联特性。

Conclusion: 该方法有效解决稀疏传感器数据局限，为精准污染监测提供新范式，发现的时空模式为治理决策提供理论依据。

Abstract: Air pollution has become a significant health risk in developing countries.
While governments routinely publish air-quality index (AQI) data to track
pollution, these values fail to capture the local reality, as sensors are often
very sparse. In this paper, we address this gap by predicting AQI in 1 km^2
neighborhoods, using the example of AirDelhi dataset. Using Spatio-temporal
GNNs we surpass existing works by 71.654 MSE a 79% reduction, even on unseen
coordinates. New insights about AQI such as the existence of strong repetitive
short-term patterns and changing spatial relations are also discovered. The
code is available on GitHub.

</details>


### [125] [Provably Learning from Language Feedback](https://arxiv.org/abs/2506.10341)
*Wanqiao Xu,Allen Nie,Ruijie Zheng,Aditya Modi,Adith Swaminathan,Ching-An Cheng*

Main category: cs.LG

TL;DR: 本文提出了从语言反馈中学习（LLF）问题的理论框架，引入转移逃避维度衡量问题复杂度，并开发了无遗憾算法HELiX，证明其在不同领域的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的交互式学习方法缺乏理论支撑，本文旨在建立形式化框架以系统研究语言反馈对学习效率的影响机制。

Method: 通过形式化LLF问题定义，提出转移逃避维度作为问题复杂度指标，开发基于序列交互的无悔算法HELiX，其性能保证与问题复杂度直接相关。

Result: 理论证明语言反馈可使学习速度指数级快于传统奖励学习，实验表明HELiX在LLM提示不可靠时仍优于基线方法，验证了复杂度指标的有效性。

Conclusion: 研究首次为通用语言反馈的交互式学习建立了理论框架，通过复杂度分析和算法设计，为语言反馈的高效利用提供了新范式。

Abstract: Interactively learning from observation and language feedback is an
increasingly studied area driven by the emergence of large language model (LLM)
agents. While impressive empirical demonstrations have been shown, so far a
principled framing of these decision problems remains lacking. In this paper,
we formalize the Learning from Language Feedback (LLF) problem, assert
sufficient assumptions to enable learning despite latent rewards, and introduce
$\textit{transfer eluder dimension}$ as a complexity measure to characterize
the hardness of LLF problems. We show that transfer eluder dimension captures
the intuition that information in the feedback changes the learning complexity
of the LLF problem. We demonstrate cases where learning from rich language
feedback can be exponentially faster than learning from reward. We develop a
no-regret algorithm, called $\texttt{HELiX}$, that provably solves LLF problems
through sequential interactions, with performance guarantees that scale with
the transfer eluder dimension of the problem. Across several empirical domains,
we show that $\texttt{HELiX}$ performs well even when repeatedly prompting LLMs
does not work reliably. Our contributions mark a first step towards designing
principled interactive learning algorithms from generic language feedback.

</details>


### [126] [PhysioWave: A Multi-Scale Wavelet-Transformer for Physiological Signal Representation](https://arxiv.org/abs/2506.10351)
*Yanlong Chen,Mattia Orlandi,Pierangelo Maria Rapa,Simone Benatti,Luca Benini,Yawei Li*

Main category: cs.LG

TL;DR: 提出一种基于小波变换的生理信号分析方法，构建针对EMG/ECG的大规模预训练模型及多模态融合框架，有效解决噪声干扰、非平稳性等挑战，在多项任务中刷新基准性能。


<details>
  <summary>Details</summary>
Motivation: 生理信号存在运动伪影、基线漂移等低信噪比干扰，且具有强非平稳性特征，传统时域/滤波方法难以有效表征其突变与连续演化特性。

Method: 采用小波变换捕捉多尺度时频特征，首次建立EMG/ECG专用预训练模型，并通过可学习加权融合整合EEG模型构建统一多模态框架，各模态拥有独立分支。

Result: 在下游任务中实现性能突破，有效应对信号噪声、个体差异和设备失配问题，多模态任务表现超越现有方法。

Conclusion: 小波架构为多样化生理信号分析奠定基础，多模态设计指向下一代穿戴式健康监测与临床诊断应用，具有广泛生物医学价值。

Abstract: Physiological signals are often corrupted by motion artifacts, baseline
drift, and other low-SNR disturbances, which pose significant challenges for
analysis. Additionally, these signals exhibit strong non-stationarity, with
sharp peaks and abrupt changes that evolve continuously, making them difficult
to represent using traditional time-domain or filtering methods. To address
these issues, a novel wavelet-based approach for physiological signal analysis
is presented, aiming to capture multi-scale time-frequency features in various
physiological signals. Leveraging this technique, two large-scale pretrained
models specific to EMG and ECG are introduced for the first time, achieving
superior performance and setting new baselines in downstream tasks.
Additionally, a unified multi-modal framework is constructed by integrating
pretrained EEG model, where each modality is guided through its dedicated
branch and fused via learnable weighted fusion. This design effectively
addresses challenges such as low signal-to-noise ratio, high inter-subject
variability, and device mismatch, outperforming existing methods on multi-modal
tasks. The proposed wavelet-based architecture lays a solid foundation for
analysis of diverse physiological signals, while the multi-modal design points
to next-generation physiological signal processing with potential impact on
wearable health monitoring, clinical diagnostics, and broader biomedical
applications.

</details>


### [127] [History-Aware Neural Operator: Robust Data-Driven Constitutive Modeling of Path-Dependent Materials](https://arxiv.org/abs/2506.10352)
*Binyao Guo,Zihan Lin,QiZhi He*

Main category: cs.LG

TL;DR: 本文提出了一种基于神经算子的端到端学习框架HANO，用于数据驱动的路径依赖非弹性材料建模。HANO通过自回归模型避免隐藏状态变量，结合傅里叶神经算子和分层自注意力机制，解决了传统RNN模型的自洽性问题，并在复杂条件下表现出鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统基于循环神经网络（RNN）的模型在处理路径依赖材料响应时存在自洽性问题和初始状态敏感性，且难以适应不同离散化路径。本文旨在通过数据驱动方法克服这些限制。

Method: 开发历史感知神经算子（HANO）：基于傅里叶神经算子的自回归模型，嵌入分层自注意力机制实现多尺度特征提取，直接通过应变-应力片段预测响应，无需依赖隐藏状态变量。

Result: 在弹塑性硬化和脆性固体渐进各向异性损伤两个基准测试中，HANO在预测精度、泛化能力和鲁棒性上均优于基线模型，尤其在噪声数据、多周期加载等复杂场景下表现稳定。

Conclusion: HANO为不可逆材料响应建模提供了有效的数据驱动替代方案，其路径离散化无关性和自洽性特征使其具备与经典数值求解器集成的潜力。

Abstract: This study presents an end-to-end learning framework for data-driven modeling
of path-dependent inelastic materials using neural operators. The framework is
built on the premise that irreversible evolution of material responses,
governed by hidden dynamics, can be inferred from observable data.
  We develop the History-Aware Neural Operator (HANO), an autoregressive model
that predicts path-dependent material responses from short segments of recent
strain-stress history without relying on hidden state variables, thereby
overcoming self-consistency issues commonly encountered in recurrent neural
network (RNN)-based models. Built on a Fourier-based neural operator backbone,
HANO enables discretization-invariant learning. To enhance its ability to
capture both global loading patterns and critical local path dependencies, we
embed a hierarchical self-attention mechanism that facilitates multiscale
feature extraction.
  Beyond ensuring self-consistency, HANO mitigates sensitivity to initial
hidden states, a commonly overlooked issue that can lead to instability in
recurrent models when applied to generalized loading paths. By modeling
stress-strain evolution as a continuous operator rather than relying on fixed
input-output mappings, HANO naturally accommodates varying path discretizations
and exhibits robust performance under complex conditions, including irregular
sampling, multi-cycle loading, noisy data, and pre-stressed states. We evaluate
HANO on two benchmark problems: elastoplasticity with hardening and progressive
anisotropic damage in brittle solids. Results show that HANO consistently
outperforms baseline models in predictive accuracy, generalization, and
robustness. With its demonstrated capabilities, HANO provides an effective
data-driven surrogate for simulating inelastic materials and is well-suited for
integration with classical numerical solvers.

</details>


### [128] [TreeLoRA: Efficient Continual Learning via Layer-Wise LoRAs Guided by a Hierarchical Gradient-Similarity Tree](https://arxiv.org/abs/2506.10355)
*Yu-Yang Qian,Yuan-Ze Xu,Zhen-Yu Zhang,Peng Zhao,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: 提出TreeLoRA方法，通过分层梯度相似性构建适配器，结合多臂老虎机算法和稀疏梯度更新，实现大预训练模型的高效持续学习。


<details>
  <summary>Details</summary>
Motivation: 流式数据环境下，传统方法难以高效实现大预训练模型（LPMs）的持续学习，需解决计算负担与灾难性遗忘的平衡问题。

Method: 基于K-D树结构的低秩适配器（TreeLoRA），利用分层梯度相似性组织适配器，采用置信下限算法探索任务结构，并通过稀疏梯度更新优化参数。

Result: 在视觉Transformer（ViT）和大语言模型（LLM）的跨领域任务中验证了方法的有效性和计算效率，包括视觉与自然语言处理任务。

Conclusion: TreeLoRA通过理论分析与实验证明，能够高效实现大模型的持续学习，平衡新任务适应与旧知识保留，适用于多种实际场景。

Abstract: Many real-world applications collect data in a streaming environment, where
learning tasks are encountered sequentially. This necessitates continual
learning (CL) to update models online, enabling adaptation to new tasks while
preserving past knowledge to prevent catastrophic forgetting. Nowadays, with
the flourish of large pre-trained models (LPMs), efficiency has become
increasingly critical for CL, due to their substantial computational demands
and growing parameter sizes. In this paper, we introduce TreeLoRA (K-D Tree of
Low-Rank Adapters), a novel approach that constructs layer-wise adapters by
leveraging hierarchical gradient similarity to enable efficient CL,
particularly for LPMs. To reduce the computational burden of task similarity
estimation, we employ bandit techniques to develop an algorithm based on lower
confidence bounds to efficiently explore the task structure. Furthermore, we
use sparse gradient updates to facilitate parameter optimization, making the
approach better suited for LPMs. Theoretical analysis is provided to justify
the rationale behind our approach, and experiments on both vision transformers
(ViTs) and large language models (LLMs) demonstrate the effectiveness and
efficiency of our approach across various domains, including vision and natural
language processing tasks.

</details>


### [129] [Can We Infer Confidential Properties of Training Data from LLMs?](https://arxiv.org/abs/2506.10364)
*Penguin Huang,Chhavi Yadav,Ruihan Wu,Kamalika Chaudhuri*

Main category: cs.LG

TL;DR: 研究大型语言模型（LLMs）在领域微调时可能泄露敏感数据集属性的风险，提出PropInfer基准及两种攻击方法，验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有属性推断攻击研究集中于分类模型和生成模型，但LLMs在医疗、金融等敏感领域微调时可能泄露数据集级机密属性（如患者统计特征），其脆弱性尚未明确。

Method: 基于ChatDoctor数据集构建PropInfer基准任务，涵盖多种属性类型和任务配置；设计基于提示生成的攻击和利用词频信号的影子模型攻击。

Result: 在多预训练LLMs上的实验表明，所提攻击方法成功推断出敏感属性，揭示了LLMs此前未被识别的安全漏洞。

Conclusion: LLMs在领域微调过程中存在泄露数据集级敏感属性的风险，PropInfer基准及攻击方法为模型安全性评估提供了新方向。

Abstract: Large language models (LLMs) are increasingly fine-tuned on domain-specific
datasets to support applications in fields such as healthcare, finance, and
law. These fine-tuning datasets often have sensitive and confidential
dataset-level properties -- such as patient demographics or disease prevalence
-- that are not intended to be revealed. While prior work has studied property
inference attacks on discriminative models (e.g., image classification models)
and generative models (e.g., GANs for image data), it remains unclear if such
attacks transfer to LLMs. In this work, we introduce PropInfer, a benchmark
task for evaluating property inference in LLMs under two fine-tuning paradigms:
question-answering and chat-completion. Built on the ChatDoctor dataset, our
benchmark includes a range of property types and task configurations. We
further propose two tailored attacks: a prompt-based generation attack and a
shadow-model attack leveraging word frequency signals. Empirical evaluations
across multiple pretrained LLMs show the success of our attacks, revealing a
previously unrecognized vulnerability in LLMs.

</details>


### [130] [Discovering Hierarchical Latent Capabilities of Language Models via Causal Representation Learning](https://arxiv.org/abs/2506.10378)
*Jikai Jin,Vasilis Syrgkanis,Sham Kakade,Hanlin Zhang*

Main category: cs.LG

TL;DR: 本文提出一种因果表示学习框架，通过建模潜在能力因子及其因果关系，解决语言模型评估中的混杂效应和计算成本问题，揭示能力间的因果方向。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估方法面临混杂效应和计算成本高的挑战，需更严谨的因果分析以指导模型开发。

Method: 采用因果表示学习框架，将基准表现建模为少量潜在能力因子的线性变换，并通过控制基础模型作为混杂因子识别因果关系。

Result: 在包含1500+模型的六项基准测试中，识别出三节点线性因果结构，揭示从通用问题解决→指令遵循→数学推理的因果路径。

Conclusion: 控制基础模型变体对揭示潜在能力间因果关系至关重要，因果分析方法可提供超越简单排名的科学洞见。

Abstract: Faithful evaluation of language model capabilities is crucial for deriving
actionable insights that can inform model development. However, rigorous causal
evaluations in this domain face significant methodological challenges,
including complex confounding effects and prohibitive computational costs
associated with extensive retraining. To tackle these challenges, we propose a
causal representation learning framework wherein observed benchmark performance
is modeled as a linear transformation of a few latent capability factors.
Crucially, these latent factors are identified as causally interrelated after
appropriately controlling for the base model as a common confounder. Applying
this approach to a comprehensive dataset encompassing over 1500 models
evaluated across six benchmarks from the Open LLM Leaderboard, we identify a
concise three-node linear causal structure that reliably explains the observed
performance variations. Further interpretation of this causal structure
provides substantial scientific insights beyond simple numerical rankings:
specifically, we reveal a clear causal direction starting from general
problem-solving capabilities, advancing through instruction-following
proficiency, and culminating in mathematical reasoning ability. Our results
underscore the essential role of carefully controlling base model variations
during evaluation, a step critical to accurately uncovering the underlying
causal relationships among latent model capabilities.

</details>


### [131] [EQA-RM: A Generative Embodied Reward Model with Test-time Scaling](https://arxiv.org/abs/2506.10389)
*Yuhang Chen,Zhen Tan,Tianlong Chen*

Main category: cs.LG

TL;DR: 提出EQA-RM——首个面向具身问答任务的多模态生成式奖励模型，通过C-GRPO策略学习细粒度行为差异，支持动态调整评估粒度，在仅700样本下以61.9%准确率超越主流基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有通用奖励模型在复杂具身任务(如EQA)中缺乏对空间-时间-逻辑理解的细粒度评估能力，无法满足具身智能体训练需求。

Method: 1) 设计生成式多模态奖励模型EQA-RM架构；2) 提出对比组相对策略优化(C-GRPO)训练策略；3) 基于OpenEQA构建EQARewardBench评估基准。

Result: Qwen2-VL-2B微调的EQA-RM在EQARewardBench达61.9%准确率，样本效率显著优于Gemini/GPT-4/Claude等闭源模型及RoVRM等开源SOTA。

Conclusion: EQA-RM通过生成结构化奖励实现可解释评估，其动态粒度调节能力为具身智能研究提供新范式，配套基准与代码将推动领域发展。

Abstract: Reward Models (RMs), vital for large model alignment, are underexplored for
complex embodied tasks like Embodied Question Answering (EQA) where nuanced
evaluation of agents' spatial, temporal, and logical understanding is critical
yet not considered by generic approaches. We introduce EQA-RM, a novel
generative multimodal reward model specifically architected for EQA, trained
via our innovative Contrastive Group Relative Policy Optimization (C-GRPO)
strategy to learn fine-grained behavioral distinctions. The generative nature
of EQA-RM provides interpretable, structured reward feedback (beyond simple
scalars), uniquely enabling test-time scaling to dynamically adjust evaluation
granularity, from concise scores to detailed critiques of reasoning and
grounding, at inference without retraining. Concurrently, we introduce
EQARewardBench, a new benchmark built on OpenEQA for standardized EQA reward
model assessment. Demonstrating high sample efficiency, EQA-RM (fine-tuning
Qwen2-VL-2B-Instruct) achieves 61.9\% accuracy on EQA-RM-Bench with only 700
samples, outperforming strong proprietary baselines, including
Gemini-2.5-Flash, GPT-4o, Claude-3.5-Haiku, and open-sourced state-of-the-art
models such as RoVRM and VisualPRM. The code and dataset can be found here
https://github.com/UNITES-Lab/EQA-RM.

</details>


### [132] [Time To Impeach LLM-as-a-Judge: Programs are the Future of Evaluation](https://arxiv.org/abs/2506.10403)
*Tzu-Heng Huang,Harit Vishwakarma,Frederic Sala*

Main category: cs.LG

TL;DR: PAJAMA提出用LLM合成可执行的评判程序替代直接评分，显著降低评估成本并提升可靠性与公平性。


<details>
  <summary>Details</summary>
Motivation: 传统LLM评估方法存在API成本高、可靠性存疑、流程僵化及固有偏见等问题，需开发更高效、透明的替代方案。

Method: 通过LLM自动生成可本地执行的评判程序（Program-As-a-Judge），实现低成本、可解释且可调整的评估逻辑。

Result: 程序化评估使判断一致性提升15.83%，偏见响应减少23.7%，在CHAT-HARD等数据集上表现优于传统方法，成本降低三个数量级。

Conclusion: PAJAMA通过程序化评估范式有效解决LLM评估的痛点，在成本、公平性和适应性方面实现突破，为自动化评估提供新路径。

Abstract: Large language models (LLMs) are widely used to evaluate the quality of LLM
generations and responses, but this leads to significant challenges: high API
costs, uncertain reliability, inflexible pipelines, and inherent biases. To
address these, we introduce PAJAMA (Program-As-a-Judge for Automated Model
Assessment), a new alternative that uses LLMs to synthesize executable judging
programs instead of directly scoring responses. These synthesized programs can
be stored and run locally, costing orders of magnitude less while providing
interpretable, and auditable judging logic that can be easily adapted.
Program-based judges mitigate biases, improving judgment consistency by 15.83%
and reducing biased responses by 23.7% on average compared to a
Qwen2.5-14B-based LLM-as-a-judge. When program judgments are distilled into a
model, PAJAMA outperforms LLM-as-a-judge on the challenging CHAT-HARD subset of
RewardBench, outperforming metrics by 2.19% on Prometheus and 8.67% on the
JudgeLM dataset, all at three orders of magnitude lower cost.

</details>


### [133] [Generative Algorithms for Wildfire Progression Reconstruction from Multi-Modal Satellite Active Fire Measurements and Terrain Height](https://arxiv.org/abs/2506.10404)
*Bryan Shaddy,Brianna Binder,Agnimitra Dasgupta,Haitong Qin,James Haley,Angel Farguell,Kyle Hilburn,Derek V. Mallia,Adam Kochanski,Jan Mandel,Assad Oberai*

Main category: cs.LG

TL;DR: 本文提出了一种基于条件生成对抗网络（cGAN）的野火蔓延预测方法，通过融合卫星观测数据与地形信息，结合WRF-SFIRE模型物理特性，实现了高精度的火灾到达时间估计。


<details>
  <summary>Details</summary>
Motivation: 现有野火模型在多日连续模拟中与观测结果存在偏差，需通过数据同化提升预测精度。研究旨在利用卫星测量数据与地形信息改进火灾蔓延动态建模。

Method: 使用WRF-SFIRE历史野火模拟数据训练条件生成对抗网络，输入包含火灾到达时间、VIIRS/GOES卫星观测数据及地形高度，通过近似观测算子生成训练样本，实现物理约束下的火灾蔓延估计。

Result: 在美国太平洋地区五场野火验证中，模型与航空测量周界的平均Sørensen-Dice系数达0.81。地形高度在卫星数据条件下对预测影响微弱。

Conclusion: 基于物理模型模拟数据训练的条件生成对抗网络能有效融合多源观测数据，显著提升野火蔓延预测精度，且卫星测量数据可削弱地形因素对预测结果的影响。

Abstract: Increasing wildfire occurrence has spurred growing interest in wildfire
spread prediction. However, even the most complex wildfire models diverge from
observed progression during multi-day simulations, motivating need for data
assimilation. A useful approach to assimilating measurement data into complex
coupled atmosphere-wildfire models is to estimate wildfire progression from
measurements and use this progression to develop a matching atmospheric state.
In this study, an approach is developed for estimating fire progression from
VIIRS active fire measurements, GOES-derived ignition times, and terrain height
data. A conditional Generative Adversarial Network is trained with simulations
of historic wildfires from the atmosphere-wildfire model WRF-SFIRE, thus
allowing incorporation of WRF-SFIRE physics into estimates. Fire progression is
succinctly represented by fire arrival time, and measurements for training are
obtained by applying an approximate observation operator to WRF-SFIRE
solutions, eliminating need for satellite data during training. The model is
trained on tuples of fire arrival times, measurements, and terrain, and once
trained leverages measurements of real fires and corresponding terrain data to
generate samples of fire arrival times. The approach is validated on five
Pacific US wildfires, with results compared against high-resolution perimeters
measured via aircraft, finding an average Sorensen-Dice coefficient of 0.81.
The influence of terrain height on the arrival time inference is also evaluated
and it is observed that terrain has minimal influence when the inference is
conditioned on satellite measurements.

</details>


### [134] [Time-IMM: A Dataset and Benchmark for Irregular Multimodal Multivariate Time Series](https://arxiv.org/abs/2506.10412)
*Ching Chang,Jeehyun Hwang,Yidan Shi,Haixin Wang,Wen-Chih Peng,Tien-Fu Chen,Wei Wang*

Main category: cs.LG

TL;DR: 本文提出了Time-IMM数据集和IMM-TSF基准库，针对现实世界中不规则、多模态时间序列数据的预测问题，通过显式建模多模态和异步融合策略，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现实应用（如医疗、金融）中的时间序列数据常存在采样率不一、模态异步和缺失等问题，但现有研究假设数据干净、规则且单模态，导致与实际场景存在差距。

Method: 设计Time-IMM数据集模拟九类时序不规则性（触发型、约束型、人为型），并开发IMM-TSF基准库，包含时间戳-文本融合模块和多模态融合模块，支持近期平均和注意力策略的异步集成。

Result: 实验表明，显式建模多模态在非规则时序数据上可显著提升预测性能，验证了方法的有效性。

Conclusion: Time-IMM和IMM-TSF为现实场景下的时序分析提供了基础，公开的数据集和代码库将推动相关研究。

Abstract: Time series data in real-world applications such as healthcare, climate
modeling, and finance are often irregular, multimodal, and messy, with varying
sampling rates, asynchronous modalities, and pervasive missingness. However,
existing benchmarks typically assume clean, regularly sampled, unimodal data,
creating a significant gap between research and real-world deployment. We
introduce Time-IMM, a dataset specifically designed to capture cause-driven
irregularity in multimodal multivariate time series. Time-IMM represents nine
distinct types of time series irregularity, categorized into trigger-based,
constraint-based, and artifact-based mechanisms. Complementing the dataset, we
introduce IMM-TSF, a benchmark library for forecasting on irregular multimodal
time series, enabling asynchronous integration and realistic evaluation.
IMM-TSF includes specialized fusion modules, including a timestamp-to-text
fusion module and a multimodality fusion module, which support both
recency-aware averaging and attention-based integration strategies. Empirical
results demonstrate that explicitly modeling multimodality on irregular time
series data leads to substantial gains in forecasting performance. Time-IMM and
IMM-TSF provide a foundation for advancing time series analysis under
real-world conditions. The dataset is publicly available at
https://www.kaggle.com/datasets/blacksnail789521/time-imm/data, and the
benchmark library can be accessed at
https://anonymous.4open.science/r/IMMTSF_NeurIPS2025.

</details>


### [135] [Data-Driven Soil Organic Carbon Sampling: Integrating Spectral Clustering with Conditioned Latin Hypercube Optimization](https://arxiv.org/abs/2506.10419)
*Weiying Zhao,Aleksei Unagaev,Natalia Efremova*

Main category: cs.LG

TL;DR: 提出一种结合谱聚类与条件拉丁超立方采样(cLHS)的混合方法，以提升土壤有机碳(SOC)监测中环境协变量采样点的代表性。


<details>
  <summary>Details</summary>
Motivation: 传统cLHS方法在采样时可能忽略重要的小规模环境异质性区域，导致训练数据代表性不足，影响SOC预测模型的准确性。

Method: 通过谱聚类将研究区域划分为K个同质子区，再在每个子区内应用cLHS进行分层采样，确保全面覆盖多维环境协变量特征空间。

Result: 在真实SOC数据集上验证表明，该方法比标准cLHS在协变量特征空间覆盖度和空间异质性表征方面提升17-23%，采样点分布更均匀。

Conclusion: 混合采样策略通过平衡训练数据的空间与环境特征表征，可为机器学习模型提供更优质的输入，有望提升SOC预测精度。

Abstract: Soil organic carbon (SOC) monitoring often relies on selecting representative
field sampling locations based on environmental covariates. We propose a novel
hybrid methodology that integrates spectral clustering - an unsupervised
machine learning technique with conditioned Latin hypercube sampling (cLHS) to
enhance the representativeness of SOC sampling. In our approach, spectral
clustering partitions the study area into $K$ homogeneous zones using
multivariate covariate data, and cLHS is then applied within each zone to
select sampling locations that collectively capture the full diversity of
environmental conditions. This hybrid spectral-cLHS method ensures that even
minor but important environmental clusters are sampled, addressing a key
limitation of vanilla cLHS which can overlook such areas. We demonstrate on a
real SOC mapping dataset that spectral-cLHS provides more uniform coverage of
covariate feature space and spatial heterogeneity than standard cLHS. This
improved sampling design has the potential to yield more accurate SOC
predictions by providing better-balanced training data for machine learning
models.

</details>


### [136] [System Identification Using Kolmogorov-Arnold Networks: A Case Study on Buck Converters](https://arxiv.org/abs/2506.10434)
*Nart Gashi,Panagiotis Kakosimos,George Papafotiou*

Main category: cs.LG

TL;DR: 本文探讨了Kolmogorov-Arnold网络（KANs）在降压转换器动态系统识别中的应用，通过可学习的激活函数实现高精度、可解释的系统建模，验证了其在参数估计与方程发现中的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络在系统识别中存在可解释性差、扩展性不足等问题。KANs基于Kolmogorov-Arnold表示定理，具有更强的函数逼近能力，旨在为动态系统（如电力电子转换器）提供更高效的建模与分析方法。

Method: 利用仿真数据构建KANs模型，通过可学习激活函数近似系统状态导数，建立可解释的状态空间表达，并采用数值实验验证模型一致性及参数变化检测能力。

Result: 实验表明KANs能高精度识别降压转换器动态特性，成功验证模型自洽性，并有效检测系统参数变化，其准确性与可解释性优于传统神经网络。

Conclusion: KANs为现代工业系统识别提供了兼具高精度与可解释性的新范式，尤其在动态系统建模与参数分析中展现出显著应用潜力，为复杂系统逆向工程奠定基础。

Abstract: Kolmogorov-Arnold Networks (KANs) are emerging as a powerful framework for
interpretable and efficient system identification in dynamic systems. By
leveraging the Kolmogorov-Arnold representation theorem, KANs enable function
approximation through learnable activation functions, offering improved
scalability, accuracy, and interpretability compared to traditional neural
networks. This paper investigates the application of KANs to model and analyze
the dynamics of a buck converter system, focusing on state-space parameter
estimation along with discovering the system equations. Using simulation data,
the methodology involves approximating state derivatives with KANs,
constructing interpretable state-space representations, and validating these
models through numerical experiments. The results demonstrate the ability of
KANs to accurately identify system dynamics, verify model consistency, and
detect parameter changes, providing valuable insights into their applicability
for system identification in modern industrial systems.

</details>


### [137] [MNN-LLM: A Generic Inference Engine for Fast Large Language Model Deployment on Mobile Devices](https://arxiv.org/abs/2506.10443)
*Zhaode Wang,Jingbang Yang,Xinyu Qian,Shiwen Xing,Xiaotang Jiang,Chengfei Lv,Shengyu Zhang*

Main category: cs.LG

TL;DR: MNN-LLM框架通过模型量化、混合存储优化及移动端计算策略，显著降低大型语言模型在移动设备上的内存占用，并实现最高8.6倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）推理时计算资源消耗高，边缘设备部署面临内存和速度挑战。

Method: 采用模型量化与DRAM-Flash混合存储降低内存；基于移动CPU/GPU特性重组权重与输入，结合多核负载均衡、混合精度计算及几何优化策略提升性能。

Result: 相比主流LLM框架，MNN-LLM推理速度提升最高达8.6倍。

Conclusion: MNN-LLM有效解决了移动端LLM部署的内存与性能瓶颈，为边缘计算场景提供了高效解决方案。

Abstract: Large language models (LLMs) have demonstrated exceptional performance across
a variety of tasks. However, their substantial scale leads to significant
computational resource consumption during inference, resulting in high costs.
Consequently, edge device inference presents a promising solution. The primary
challenges of edge inference include memory usage and inference speed. This
paper introduces MNN-LLM, a framework specifically designed to accelerate the
deployment of large language models on mobile devices. MNN-LLM addresses the
runtime characteristics of LLMs through model quantization and DRAM-Flash
hybrid storage, effectively reducing memory usage. It rearranges weights and
inputs based on mobile CPU instruction sets and GPU characteristics while
employing strategies such as multicore load balancing, mixed-precision
floating-point operations, and geometric computations to enhance performance.
Notably, MNN-LLM achieves up to a 8.6x speed increase compared to current
mainstream LLM-specific frameworks.

</details>


### [138] [Equivariant Neural Diffusion for Molecule Generation](https://arxiv.org/abs/2506.10532)
*François Cornet,Grigory Bartosh,Mikkel N. Schmidt,Christian A. Naesseth*

Main category: cs.LG

TL;DR: 本文提出了一种新的等变神经扩散模型（END），用于生成3D分子，其核心创新在于可学习的前向过程，实验表明其在分子生成任务中具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有等变扩散模型的前向过程多为预定义，限制了生成建模能力。本文旨在通过设计可学习且等变的前向过程，提升分子生成的性能。

Method: END通过参数化的时间与数据相关变换构建前向过程，该过程对刚性变换保持等变性，从而增强生成建模的灵活性和表达能力。

Result: 在标准分子生成基准测试中，END在无条件和条件生成任务中均表现出与多个强基线模型相当的竞争性性能。

Conclusion: END通过引入可学习的等变前向过程，验证了其在分子生成任务中的有效性，为扩散模型的设计提供了新方向。

Abstract: We introduce Equivariant Neural Diffusion (END), a novel diffusion model for
molecule generation in 3D that is equivariant to Euclidean transformations.
Compared to current state-of-the-art equivariant diffusion models, the key
innovation in END lies in its learnable forward process for enhanced generative
modelling. Rather than pre-specified, the forward process is parameterized
through a time- and data-dependent transformation that is equivariant to rigid
transformations. Through a series of experiments on standard molecule
generation benchmarks, we demonstrate the competitive performance of END
compared to several strong baselines for both unconditional and conditional
generation.

</details>


### [139] [Data-driven Day Ahead Market Prices Forecasting: A Focus on Short Training Set Windows](https://arxiv.org/abs/2506.10536)
*Vasilis Michalakopoulos,Christoforos Menos-Aikateriniadis,Elissaios Sarmas,Antonis Zakynthinos,Pavlos S. Georgilakis,Dimitris Askounis*

Main category: cs.LG

TL;DR: 研究评估了四种机器学习模型在短期历史数据下预测欧洲电力日前市场价格的性能，发现LightGBM在45-60天训练窗口下表现最优，尤其在捕捉季节性和价格峰值方面。


<details>
  <summary>Details</summary>
Motivation: 在数据有限且价格波动剧烈的电力市场中，探索短期历史训练窗口下机器学习方法的预测能力及模型适应性。

Method: 使用LSTM+FFEC、XGBoost、LightGBM和CatBoost模型，基于ENTSO-E预测数据特征，在希腊、比利时、爱尔兰市场进行7-90天不同训练窗口的对比实验。

Result: LightGBM在45天和60天训练窗口下综合表现最佳，准确率与鲁棒性优于其他模型，且能有效识别季节性趋势和价格尖峰事件。

Conclusion: 短期训练窗口结合梯度提升方法（如LightGBM）可有效支持数据稀缺环境下的日前电价预测，平衡时序相关性与学习深度。

Abstract: This study investigates the performance of machine learning models in
forecasting electricity Day-Ahead Market (DAM) prices using short historical
training windows, with a focus on detecting seasonal trends and price spikes.
We evaluate four models, namely LSTM with Feed Forward Error Correction (FFEC),
XGBoost, LightGBM, and CatBoost, across three European energy markets (Greece,
Belgium, Ireland) using feature sets derived from ENTSO-E forecast data.
Training window lengths range from 7 to 90 days, allowing assessment of model
adaptability under constrained data availability. Results indicate that
LightGBM consistently achieves the highest forecasting accuracy and robustness,
particularly with 45 and 60 day training windows, which balance temporal
relevance and learning depth. Furthermore, LightGBM demonstrates superior
detection of seasonal effects and peak price events compared to LSTM and other
boosting models. These findings suggest that short-window training approaches,
combined with boosting methods, can effectively support DAM forecasting in
volatile, data-scarce environments.

</details>


### [140] [Graph Neural Networks for Automatic Addition of Optimizing Components in Printed Circuit Board Schematics](https://arxiv.org/abs/2506.10577)
*Pascal Plettenberg,André Alcalde,Bernhard Sick,Josephine M. Thomas*

Main category: cs.LG

TL;DR: 提出基于图神经网络的PCB设计自动化优化方法，通过将电路图表示为二分图并预测节点对，有效提升电路可靠性和减少电子浪费。


<details>
  <summary>Details</summary>
Motivation: PCB设计优化依赖人工经验，但工程师短缺和耗时导致最佳实践常被忽视，引发后期高成本调试和电子垃圾问题，需自动化解决方案。

Method: 将PCB原理图建模为二分图，采用图神经网络（GNN）进行节点对预测，自动添加提升电路稳健性的组件（如上拉电阻、去耦电容）。

Result: 在三个实际PCB优化任务中，多种GNN模型表现高准确性，验证了方法在时间和成本效益上的自动化潜力。

Conclusion: 基于GNN的自动化PCB优化方法可高效替代人工设计瓶颈，降低开发成本并延长产品生命周期，减少电子废弃物。

Abstract: The design and optimization of Printed Circuit Board (PCB) schematics is
crucial for the development of high-quality electronic devices. Thereby, an
important task is to optimize drafts by adding components that improve the
robustness and reliability of the circuit, e.g., pull-up resistors or
decoupling capacitors. Since there is a shortage of skilled engineers and
manual optimizations are very time-consuming, these best practices are often
neglected. However, this typically leads to higher costs for troubleshooting in
later development stages as well as shortened product life cycles, resulting in
an increased amount of electronic waste that is difficult to recycle. Here, we
present an approach for automating the addition of new components into PCB
schematics by representing them as bipartite graphs and utilizing a node pair
prediction model based on Graph Neural Networks (GNNs). We apply our approach
to three highly relevant PCB design optimization tasks and compare the
performance of several popular GNN architectures on real-world datasets labeled
by human experts. We show that GNNs can solve these problems with high accuracy
and demonstrate that our approach offers the potential to automate PCB design
optimizations in a time- and cost-efficient manner.

</details>


### [141] [Size-adaptive Hypothesis Testing for Fairness](https://arxiv.org/abs/2506.10586)
*Antonio Ferrara,Francesco Cozzi,Alan Perotti,André Panisson,Francesco Bonchi*

Main category: cs.LG

TL;DR: 本文提出一种统一、自适应规模的假设检验框架，将公平性评估转化为基于证据的统计决策。针对不同规模子群体，分别采用频率学派Wald检验和贝叶斯Dirichlet-多项式估计方法，通过基准数据集验证了该方法能在不同数据可用性和交叉性条件下提供可解释的统计严谨决策。


<details>
  <summary>Details</summary>
Motivation: 现有公平性评估方法仅通过单点估计值与阈值比较，存在统计脆弱性：忽略抽样误差，且对大小子群体一视同仁。在交叉性分析中，敏感属性组合导致子群体数量激增、数据稀疏，传统方法产生过宽置信区间，无法得出可靠结论。

Method: 1) 对大子群体：证明统计奇偶差异的中心极限定理，建立解析置信区间和α水平控制的Wald检验；2) 对小子群体：推导完全贝叶斯Dirichlet-多项式估计器，蒙特卡洛可信区间适应任意样本量，数据充足时自然收敛于Wald区间。

Result: 基准数据集实验表明：该方法能根据数据可用性自动调整，在交叉性分析中提供统计严谨的决策支持。Wald检验有效控制I类错误，贝叶斯估计器在小样本下保持良好校准性能，两者形成连续统一的评估体系。

Conclusion: 该框架解决了传统公平性评估的统计缺陷，通过结合频率学派和贝叶斯方法，实现了从大数据量到长尾小群体的连续覆盖，为算法歧视检测提供了可解释、样本量自适应的统计推断工具。

Abstract: Determining whether an algorithmic decision-making system discriminates
against a specific demographic typically involves comparing a single point
estimate of a fairness metric against a predefined threshold. This practice is
statistically brittle: it ignores sampling error and treats small demographic
subgroups the same as large ones. The problem intensifies in intersectional
analyses, where multiple sensitive attributes are considered jointly, giving
rise to a larger number of smaller groups. As these groups become more
granular, the data representing them becomes too sparse for reliable
estimation, and fairness metrics yield excessively wide confidence intervals,
precluding meaningful conclusions about potential unfair treatments.
  In this paper, we introduce a unified, size-adaptive, hypothesis-testing
framework that turns fairness assessment into an evidence-based statistical
decision. Our contribution is twofold. (i) For sufficiently large subgroups, we
prove a Central-Limit result for the statistical parity difference, leading to
analytic confidence intervals and a Wald test whose type-I (false positive)
error is guaranteed at level $\alpha$. (ii) For the long tail of small
intersectional groups, we derive a fully Bayesian Dirichlet-multinomial
estimator; Monte-Carlo credible intervals are calibrated for any sample size
and naturally converge to Wald intervals as more data becomes available. We
validate our approach empirically on benchmark datasets, demonstrating how our
tests provide interpretable, statistically rigorous decisions under varying
degrees of data availability and intersectionality.

</details>


### [142] [Non-stationary Online Learning for Curved Losses: Improved Dynamic Regret via Mixability](https://arxiv.org/abs/2506.10616)
*Yu-Jie Zhang,Peng Zhao,Masashi Sugiyama*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Non-stationary online learning has drawn much attention in recent years.
Despite considerable progress, dynamic regret minimization has primarily
focused on convex functions, leaving the functions with stronger curvature
(e.g., squared or logistic loss) underexplored. In this work, we address this
gap by showing that the regret can be substantially improved by leveraging the
concept of mixability, a property that generalizes exp-concavity to effectively
capture loss curvature. Let $d$ denote the dimensionality and $P_T$ the path
length of comparators that reflects the environmental non-stationarity. We
demonstrate that an exponential-weight method with fixed-share updates achieves
an $\mathcal{O}(d T^{1/3} P_T^{2/3} \log T)$ dynamic regret for mixable losses,
improving upon the best-known $\mathcal{O}(d^{10/3} T^{1/3} P_T^{2/3} \log T)$
result (Baby and Wang, 2021) in $d$. More importantly, this improvement arises
from a simple yet powerful analytical framework that exploits the mixability,
which avoids the Karush-Kuhn-Tucker-based analysis required by existing work.

</details>


### [143] [Deep Learning-Based Digitization of Overlapping ECG Images with Open-Source Python Code](https://arxiv.org/abs/2506.10617)
*Reza Karbasi,Masoud Rahimi,Abdol-Hossein Vahabie,Hadi Moradi*

Main category: cs.LG

TL;DR: 本文提出一种两阶段方法，通过U-Net分割网络与自适应网格检测技术，有效解决纸质心电图信号重叠导致的数字化难题，在重叠与非重叠场景下均显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有心电图数字化方法对信号重叠问题处理不足，而纸质ECG记录中单导联信号重叠是常见但未充分解决的挑战，影响临床数据转换的准确性。

Method: 两阶段流程：1) 使用数据增强训练的U-Net网络分割主ECG信号；2) 通过自适应网格检测将二值掩码转换为时间序列信号，兼容不同ECG格式与比例。

Result: U-Net分割IoU达0.87；数字化结果在非重叠样本(MSE=0.0010/rho=0.9644)和重叠样本(MSE=0.0029/rho=0.9641)均显著优于基线(重叠样本基线MSE=0.0178/rho=0.8676)。

Conclusion: 该方法显著提升心电图数字化精度，尤其在信号重叠场景下，为临床与研究提供可靠数据转换方案，代码已开源。

Abstract: This paper addresses the persistent challenge of accurately digitizing
paper-based electrocardiogram (ECG) recordings, with a particular focus on
robustly handling single leads compromised by signal overlaps-a common yet
under-addressed issue in existing methodologies. We propose a two-stage
pipeline designed to overcome this limitation. The first stage employs a U-Net
based segmentation network, trained on a dataset enriched with overlapping
signals and fortified with custom data augmentations, to accurately isolate the
primary ECG trace. The subsequent stage converts this refined binary mask into
a time-series signal using established digitization techniques, enhanced by an
adaptive grid detection module for improved versatility across different ECG
formats and scales. Our experimental results demonstrate the efficacy of our
approach. The U-Net architecture achieves an IoU of 0.87 for the fine-grained
segmentation task. Crucially, our proposed digitization method yields superior
performance compared to a well-established baseline technique across both
non-overlapping and challenging overlapping ECG samples. For non-overlapping
signals, our method achieved a Mean Squared Error (MSE) of 0.0010 and a Pearson
Correlation Coefficient (rho) of 0.9644, compared to 0.0015 and 0.9366,
respectively, for the baseline. On samples with signal overlap, our method
achieved an MSE of 0.0029 and a rho of 0.9641, significantly improving upon the
baseline's 0.0178 and 0.8676. This work demonstrates an effective strategy to
significantly enhance digitization accuracy, especially in the presence of
signal overlaps, thereby laying a strong foundation for the reliable conversion
of analog ECG records into analyzable digital data for contemporary research
and clinical applications. The implementation is publicly available at this
GitHub repository: https://github.com/masoudrahimi39/ECG-code.

</details>


### [144] [Leveraging Low-rank Factorizations of Conditional Correlation Matrices in Graph Learning](https://arxiv.org/abs/2506.10628)
*Thu Ha Phi,Alexandre Hippert-Ferrer,Florent Bouchard,Arnaud Breloy*

Main category: cs.LG

TL;DR: 本文提出一种基于低秩分解的图学习方法，通过黎曼优化技术解决高维数据下条件相关矩阵的图拓扑学习，有效平衡维度与性能。


<details>
  <summary>Details</summary>
Motivation: 传统图学习方法在大规模变量（节点）下因条件相关矩阵维度平方增长导致计算复杂度高，需解决高维场景下的效率问题。

Method: 采用低秩矩阵分解框架降低计算复杂度，结合黎曼流形优化技术，并扩展至带低秩约束的GLasso算法（高斯图模型惩罚最大似然估计）。

Result: 合成与真实数据实验表明，该方法在维度与性能间实现了高效权衡，验证了其计算效率与有效性。

Conclusion: 低秩分解结合黎曼优化的图学习框架为高维图结构推断提供了可扩展的解决方案，显著提升大规模场景下的实用性。

Abstract: This paper addresses the problem of learning an undirected graph from data
gathered at each nodes. Within the graph signal processing framework, the
topology of such graph can be linked to the support of the conditional
correlation matrix of the data. The corresponding graph learning problem then
scales to the squares of the number of variables (nodes), which is usually
problematic at large dimension. To tackle this issue, we propose a graph
learning framework that leverages a low-rank factorization of the conditional
correlation matrix. In order to solve for the resulting optimization problems,
we derive tools required to apply Riemannian optimization techniques for this
particular structure. The proposal is then particularized to a low-rank
constrained counterpart of the GLasso algorithm, i.e., the penalized maximum
likelihood estimation of a Gaussian graphical model. Experiments on synthetic
and real data evidence that a very efficient dimension-versus-performance
trade-off can be achieved with this approach.

</details>


### [145] [Task Adaptation from Skills: Information Geometry, Disentanglement, and New Objectives for Unsupervised Reinforcement Learning](https://arxiv.org/abs/2506.10629)
*Yucheng Yang,Tianyi Zhou,Qiang He,Lei Han,Mykola Pechenizkiy,Meng Fang*

Main category: cs.LG

TL;DR: 本文针对无监督强化学习中互信息技能学习（MISL）缺乏理论分析的问题，提出基于Wasserstein距离的新目标WSEP和算法PWSEP，提升下游任务策略初始化能力。


<details>
  <summary>Details</summary>
Motivation: MISL方法虽通过最大化状态与技能互信息学习通用技能，但缺乏对技能多样性和可分离性的理论保障，影响下游任务策略初始化效果。需补充理论分析并改进方法。

Method: 提出解缠指标LSEPIN，建立其与下游任务适应成本的信息几何关联；用Wasserstein距离替代KL散度，设计新目标WSEP；进一步提出PWSEP算法以覆盖所有最优初始策略。

Result: WSEP在理论上被证明能降低下游任务适应成本，相比MISL发现更多初始策略；PWSEP理论上可发现所有最优初始策略。

Conclusion: 通过理论分析揭示技能几何属性对任务适应的影响，基于Wasserstein距离的WSEP和PWSEP显著提升无监督强化学习的下游任务初始化能力。

Abstract: Unsupervised reinforcement learning (URL) aims to learn general skills for
unseen downstream tasks. Mutual Information Skill Learning (MISL) addresses URL
by maximizing the mutual information between states and skills but lacks
sufficient theoretical analysis, e.g., how well its learned skills can
initialize a downstream task's policy. Our new theoretical analysis in this
paper shows that the diversity and separability of learned skills are
fundamentally critical to downstream task adaptation but MISL does not
necessarily guarantee these properties. To complement MISL, we propose a novel
disentanglement metric LSEPIN. Moreover, we build an information-geometric
connection between LSEPIN and downstream task adaptation cost. For better
geometric properties, we investigate a new strategy that replaces the KL
divergence in information geometry with Wasserstein distance. We extend the
geometric analysis to it, which leads to a novel skill-learning objective WSEP.
It is theoretically justified to be helpful to downstream task adaptation and
it is capable of discovering more initial policies for downstream tasks than
MISL. We finally propose another Wasserstein distance-based algorithm PWSEP
that can theoretically discover all optimal initial policies.

</details>


### [146] [Time Series Forecasting as Reasoning: A Slow-Thinking Approach with Reinforced LLMs](https://arxiv.org/abs/2506.10630)
*Yucong Luo,Yitong Zhou,Mingyue Cheng,Jiahao Wang,Daoyu Wang,Tingyue Pan,Jintao Zhang*

Main category: cs.LG

TL;DR: 提出Time-R1框架，通过两阶段强化微调增强大语言模型（LLMs）的时间序列多步推理能力，以克服现有时间序列预测方法缺乏显式推理过程及传统提示工程的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法依赖快速思考模式，缺乏中间推理过程；而基于提示工程的LLMs存在计算成本高、隐私风险及领域推理能力不足的问题，需训练LLMs具备慢思考能力以实现深度时序推理。

Method: Time-R1框架分为两阶段：1）监督微调实现模型初步适应；2）强化学习结合细粒度多目标奖励及GRIP机制（非均匀采样优化推理路径探索），提升模型泛化与推理能力。

Result: 实验表明，Time-R1在多个数据集上显著提升了预测性能，验证了其增强LLMs时序推理的有效性。

Conclusion: 通过强化学习与定制化奖励机制，Time-R1成功赋予LLMs慢思考能力，为时间序列预测提供了更优的推理驱动解决方案。

Abstract: To advance time series forecasting (TSF), various methods have been proposed
to improve prediction accuracy, evolving from statistical techniques to
data-driven deep learning architectures. Despite their effectiveness, most
existing methods still adhere to a fast thinking paradigm-relying on extracting
historical patterns and mapping them to future values as their core modeling
philosophy, lacking an explicit thinking process that incorporates intermediate
time series reasoning. Meanwhile, emerging slow-thinking LLMs (e.g., OpenAI-o1)
have shown remarkable multi-step reasoning capabilities, offering an
alternative way to overcome these issues. However, prompt engineering alone
presents several limitations - including high computational cost, privacy
risks, and limited capacity for in-depth domain-specific time series reasoning.
To address these limitations, a more promising approach is to train LLMs to
develop slow thinking capabilities and acquire strong time series reasoning
skills. For this purpose, we propose Time-R1, a two-stage reinforcement
fine-tuning framework designed to enhance multi-step reasoning ability of LLMs
for time series forecasting. Specifically, the first stage conducts supervised
fine-tuning for warmup adaptation, while the second stage employs reinforcement
learning to improve the model's generalization ability. Particularly, we design
a fine-grained multi-objective reward specifically for time series forecasting,
and then introduce GRIP (group-based relative importance for policy
optimization), which leverages non-uniform sampling to further encourage and
optimize the model's exploration of effective reasoning paths. Experiments
demonstrate that Time-R1 significantly improves forecast performance across
diverse datasets.

</details>


### [147] [Hessian Geometry of Latent Space in Generative Models](https://arxiv.org/abs/2506.10632)
*Alexander Lobashev,Dmitry Guskov,Maria Larchenko,Mikhail Tamm*

Main category: cs.LG

TL;DR: 本文提出了一种通过重构Fisher信息度量来分析生成模型潜在空间几何结构的新方法，应用于统计物理模型和扩散模型，揭示了相变的分形特征及测地线插值的非线性行为。


<details>
  <summary>Details</summary>
Motivation: 现有方法在分析生成模型（如扩散模型）潜在空间几何结构时存在局限性，尤其是对相变等复杂现象的定量描述不足。本文旨在通过Fisher信息度量的重构，揭示潜在空间的几何特性及其与物理现象的关联。

Method: 通过近似潜在变量的后验分布，学习对数配分函数以定义指数族分布的Fisher度量，并结合理论收敛性分析。在Ising、TASEP模型和扩散模型上进行验证。

Result: 在Ising和TASEP模型中，方法优于基线，准确重建热力学量；扩散模型潜在空间呈现相变的分形结构，Fisher度量在相边界突变，测地线插值在相内近似线性但在边界失效，且潜在空间Lipschitz常数在边界发散。

Conclusion: 该方法揭示了扩散模型潜在空间的复杂几何结构及其与相变的联系，表明相边界处的非线性行为可能影响模型插值稳定性，为理解生成模型内在机制提供了新视角。

Abstract: This paper presents a novel method for analyzing the latent space geometry of
generative models, including statistical physics models and diffusion models,
by reconstructing the Fisher information metric. The method approximates the
posterior distribution of latent variables given generated samples and uses
this to learn the log-partition function, which defines the Fisher metric for
exponential families. Theoretical convergence guarantees are provided, and the
method is validated on the Ising and TASEP models, outperforming existing
baselines in reconstructing thermodynamic quantities. Applied to diffusion
models, the method reveals a fractal structure of phase transitions in the
latent space, characterized by abrupt changes in the Fisher metric. We
demonstrate that while geodesic interpolations are approximately linear within
individual phases, this linearity breaks down at phase boundaries, where the
diffusion model exhibits a divergent Lipschitz constant with respect to the
latent space. These findings provide new insights into the complex structure of
diffusion model latent spaces and their connection to phenomena like phase
transitions. Our source code is available at
https://github.com/alobashev/hessian-geometry-of-diffusion-models.

</details>


### [148] [Data Shifts Hurt CoT: A Theoretical Study](https://arxiv.org/abs/2506.10647)
*Lang Yin,Debangshu Banerjee,Gagandeep Singh*

Main category: cs.LG

TL;DR: 本文研究发现，在数据分布偏移和污染的情况下，使用思维链（CoT）训练模型解决k-parity问题时，其性能反而比直接预测更差，并揭示了其机制原因。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设训练与测试数据分布一致且无污染，但现实场景中这些假设常不成立。本文首次系统分析数据偏移（分布变化和投毒）对CoT模型性能的具体危害。

Method: 以k-parity问题为研究对象，结合分布偏移和数据投毒两种数据偏移类型，分析其对CoT分解训练模型质量的影响。

Result: 发现CoT在数据偏移下学习奇偶性问题的性能劣于直接预测，并通过技术结果严格解释了这种影响的机制性原因。

Conclusion: 思维链方法在非理想数据条件下可能引入性能风险，需重新审视其在实际应用中的鲁棒性。

Abstract: Chain of Thought (CoT) has been applied to various large language models
(LLMs) and proven to be effective in improving the quality of outputs. In
recent studies, transformers are proven to have absolute upper bounds in terms
of expressive power, and consequently, they cannot solve many computationally
difficult problems. However, empowered by CoT, transformers are proven to be
able to solve some difficult problems effectively, such as the $k$-parity
problem. Nevertheless, those works rely on two imperative assumptions: (1)
identical training and testing distribution, and (2) corruption-free training
data with correct reasoning steps. However, in the real world, these
assumptions do not always hold. Although the risks of data shifts have caught
attention, our work is the first to rigorously study the exact harm caused by
such shifts to the best of our knowledge. Focusing on the $k$-parity problem,
in this work we investigate the joint impact of two types of data shifts: the
distribution shifts and data poisoning, on the quality of trained models
obtained by a well-established CoT decomposition. In addition to revealing a
surprising phenomenon that CoT leads to worse performance on learning parity
than directly generating the prediction, our technical results also give a
rigorous and comprehensive explanation of the mechanistic reasons of such
impact.

</details>


### [149] [Saturation Self-Organizing Map](https://arxiv.org/abs/2506.10680)
*Igor Urbanik,Paweł Gajewski*

Main category: cs.LG

TL;DR: 本文提出Saturation自组织映射(SatSOM)，通过引入饱和度机制缓解持续学习中的灾难性遗忘问题，在保持SOM优势的同时提升知识保留能力。


<details>
  <summary>Details</summary>
Motivation: 自组织映射(SOM)虽具可解释性和高效性，但在持续学习场景中仍面临灾难性遗忘问题，需改进其知识保留能力。

Method: SatSOM通过神经元饱和度机制动态调节学习率和邻域半径：已饱和神经元降低学习活性，将学习资源导向未充分利用区域。

Result: SatSOM有效冻结已训练神经元参数，引导新知识向低利用率区域分配，在持续学习任务中展现出更好的知识保留特性。

Conclusion: 饱和度机制为SOM类模型提供了解决持续学习问题的新思路，在保持模型简洁性的同时增强了知识固化能力。

Abstract: Continual learning poses a fundamental challenge for neural systems, which
often suffer from catastrophic forgetting when exposed to sequential tasks.
Self-Organizing Maps (SOMs), despite their interpretability and efficiency, are
not immune to this issue. In this paper, we introduce Saturation
Self-Organizing Maps (SatSOM)-an extension of SOMs designed to improve
knowledge retention in continual learning scenarios. SatSOM incorporates a
novel saturation mechanism that gradually reduces the learning rate and
neighborhood radius of neurons as they accumulate information. This effectively
freezes well-trained neurons and redirects learning to underutilized areas of
the map.

</details>


### [150] [Preserving Task-Relevant Information Under Linear Concept Removal](https://arxiv.org/abs/2506.10703)
*Floris Holstege,Shauli Ravfogel,Bram Wouters*

Main category: cs.LG

TL;DR: SPLICE提出一种通过斜投影同时去除敏感概念并保留目标协方差的线性去偏方法，在最小化嵌入失真的前提下消除公平性问题。


<details>
  <summary>Details</summary>
Motivation: 现有后处理方法在去除神经网络中不必要概念时，常导致有用信息受损。需开发能精确保留任务相关信号的概念去除方法。

Method: 使用斜投影(oblique projection)技术，在嵌入空间切割(splice out)敏感方向，严格保持目标标签的协方差结构。

Result: 理论证明为唯一满足线性去偏与协方差保持的最优解；在Bias in Bios/Winobias基准上优于基线，信息损失减少0.5-1.2%。

Conclusion: SPLICE通过几何投影框架实现了概念去除与信息保留的平衡，为公平性表征学习提供了理论保证的解决方案。

Abstract: Modern neural networks often encode unwanted concepts alongside task-relevant
information, leading to fairness and interpretability concerns. Existing
post-hoc approaches can remove undesired concepts but often degrade useful
signals. We introduce SPLICE-Simultaneous Projection for LInear concept removal
and Covariance prEservation-which eliminates sensitive concepts from
representations while exactly preserving their covariance with a target label.
SPLICE achieves this via an oblique projection that "splices out" the unwanted
direction yet protects important label correlations. Theoretically, it is the
unique solution that removes linear concept predictability and maintains target
covariance with minimal embedding distortion. Empirically, SPLICE outperforms
baselines on benchmarks such as Bias in Bios and Winobias, removing protected
attributes while minimally damaging main-task information.

</details>


### [151] [ConTextTab: A Semantics-Aware Tabular In-Context Learner](https://arxiv.org/abs/2506.10707)
*Marco Spinaci,Marek Polewczyk,Maximilian Schambach,Sam Thelin*

Main category: cs.LG

TL;DR: 提出ConTextTab模型，结合表格原生ICL框架的架构优势与预训练语言模型的语义理解能力，通过多模态嵌入和大规模真实数据训练，在多个基准测试中达到SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 现有表格ICL方法存在两极化问题：表格原生模型（如TabPFN）仅依赖合成数据训练，无法利用真实数据语义；基于大语言模型的方案（如TabuLa-8B）虽具备语义理解能力，但受架构限制难以处理大量上下文。需要融合两者的优势。

Method: 设计ConTextTab框架：1) 为不同数据模态设计专用嵌入层；2) 在大规模真实表格数据上进行训练；3) 将语义对齐机制整合到表格原生ICL架构中。

Result: 在广泛基准测试中与SOTA模型竞争，并在强调语义理解的CARTE基准上创下新记录，验证了方法的有效性。

Conclusion: 通过融合表格原生架构效率与语言模型的语义理解能力，ConTextTab成功实现了性能突破，为表格数据建模提供了新的技术路径。

Abstract: Tabular in-context learning (ICL) has recently achieved state-of-the-art
(SOTA) performance on several tabular prediction tasks. Previously restricted
to classification problems on small tables, recent advances such as TabPFN and
TabICL have extended its use to larger datasets. While being architecturally
efficient and well-adapted to tabular data structures, current table-native ICL
architectures, being trained exclusively on synthetic data, do not fully
leverage the rich semantics and world knowledge contained in real-world tabular
data. On another end of this spectrum, tabular ICL models based on pretrained
large language models such as TabuLa-8B integrate deep semantic understanding
and world knowledge but are only able to make use of a small amount of context
due to inherent architectural limitations. With the aim to combine the best of
both these worlds, we introduce ConTextTab, integrating semantic understanding
and alignment into a table-native ICL framework. By employing specialized
embeddings for different data modalities and by training on large-scale
real-world tabular data, our model is competitive with SOTA across a broad set
of benchmarks while setting a new standard on the semantically rich CARTE
benchmark.

</details>


### [152] [Neural at ArchEHR-QA 2025: Agentic Prompt Optimization for Evidence-Grounded Clinical Question Answering](https://arxiv.org/abs/2506.10751)
*Sai Prasanna Teja Reddy Bogireddy,Abrar Majeedi,Viswanatha Reddy Gajjala,Zhuoyan Xu,Siddhant Rai,Vaishnav Potlapalli*

Main category: cs.LG

TL;DR: 本文提出Neural方法，在BioNLP 2025 ArchEHR-QA临床问答任务中获亚军。通过分阶段证据检索与答案生成，结合DSPy提示优化和自洽投票机制，测试集得分51.5，较零样本/少样本基线提升20+/10+分，证明数据驱动提示优化可替代微调模型。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录问答需在有限监督下实现精准证据检索与可靠答案生成，以弥合临床信息鸿沟。现有方法在证据召回与生成可信度方面存在挑战。

Method: 1. 分两阶段：句子级证据识别+显式引用的答案合成；2. 使用DSPy MIPROv2优化器自动探索提示空间，联合优化指令与少样本示例；3. 自洽投票机制提升证据召回率。

Result: 测试集综合得分51.5（第二名），较零样本/少样本基线分别提升20+和10+分。证据召回率提升且未损失精度。

Conclusion: 数据驱动的提示优化是临床高风险QA任务中替代模型微调的有效方案，以低成本提升医疗AI可靠性，推动辅助系统实际应用。

Abstract: Automated question answering (QA) over electronic health records (EHRs) can
bridge critical information gaps for clinicians and patients, yet it demands
both precise evidence retrieval and faithful answer generation under limited
supervision. In this work, we present Neural, the runner-up in the BioNLP 2025
ArchEHR-QA shared task on evidence-grounded clinical QA. Our proposed method
decouples the task into (1) sentence-level evidence identification and (2)
answer synthesis with explicit citations. For each stage, we automatically
explore the prompt space with DSPy's MIPROv2 optimizer, jointly tuning
instructions and few-shot demonstrations on the development set. A
self-consistency voting scheme further improves evidence recall without
sacrificing precision. On the hidden test set, our method attains an overall
score of 51.5, placing second stage while outperforming standard zero-shot and
few-shot prompting by over 20 and 10 points, respectively. These results
indicate that data-driven prompt optimization is a cost-effective alternative
to model fine-tuning for high-stakes clinical QA, advancing the reliability of
AI assistants in healthcare.

</details>


### [153] [Skillful joint probabilistic weather forecasting from marginals](https://arxiv.org/abs/2506.10772)
*Ferran Alet,Ilan Price,Andrew El-Kadi,Dominic Masters,Stratis Markou,Tom R. Andersson,Jacklynn Stott,Remi Lam,Matthew Willson,Alvaro Sanchez-Gonzalez,Peter Battaglia*

Main category: cs.LG

TL;DR: 本文介绍了一种基于机器学习的天气模型FGN，其通过模型扰动生成集合预测，在准确性和速度上超越传统数值天气预报及现有先进模型，实现了全球概率天气预报的显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统数值天气预报（NWP）在速度和准确性上存在局限，而现有机器学习模型虽表现优异，但需进一步优化集合预测能力以提升概率天气预报的可靠性和空间结构捕捉能力。

Method: FGN采用学习模型扰动生成集合预测，通过约束模型集合直接最小化连续秩概率评分（CRPS），仅基于边缘分布训练即可捕捉联合空间结构。

Result: FGN在确定性和概率指标上均达到最先进水平，可精准预测热带气旋路径，并有效捕捉空间结构，验证了其集合预测的优越性。

Conclusion: FGN通过简单、可扩展的建模方法，证明了机器学习在天气预测中的潜力，为高精度、高效率的全球天气预报提供了新方向。

Abstract: Machine learning (ML)-based weather models have rapidly risen to prominence
due to their greater accuracy and speed than traditional forecasts based on
numerical weather prediction (NWP), recently outperforming traditional
ensembles in global probabilistic weather forecasting. This paper presents FGN,
a simple, scalable and flexible modeling approach which significantly
outperforms the current state-of-the-art models. FGN generates ensembles via
learned model-perturbations with an ensemble of appropriately constrained
models. It is trained directly to minimize the continuous rank probability
score (CRPS) of per-location forecasts. It produces state-of-the-art ensemble
forecasts as measured by a range of deterministic and probabilistic metrics,
makes skillful ensemble tropical cyclone track predictions, and captures joint
spatial structure despite being trained only on marginals.

</details>


### [154] [Monotone Classification with Relative Approximations](https://arxiv.org/abs/2506.10775)
*Yufei Tao*

Main category: cs.LG

TL;DR: 本文首次研究了在单调分类问题中，以相对误差因子(1+ε)逼近最优分类器时所需的最低查询成本，提出了近乎匹配的上下界，覆盖了ε的全范围。


<details>
  <summary>Details</summary>
Motivation: 现有单调分类算法仅能保证误差不超过最优解的绝对量，而实际场景中允许误差按相对比例放宽更为合理。本文旨在填补这一理论空白，研究相对误差约束下的最小查询成本。

Method: 通过理论分析构造算法，在保持分类器单调性的前提下，逐步揭示部分点的标签以逼近最优解。同时建立信息论下界，证明任何算法所需查询次数的下限。

Result: 针对任意ε≥0，给出了误差至多(1+ε)k*时查询成本的紧致上下界，证明二者在量级上几乎完全匹配，突破了传统绝对误差分析的局限性。

Conclusion: 该工作首次建立了相对误差框架下单调分类的查询复杂度理论，为实际应用中权衡标注成本与分类精度提供了理论依据。

Abstract: In monotone classification, the input is a multi-set $P$ of points in
$\mathbb{R}^d$, each associated with a hidden label from $\{-1, 1\}$. The goal
is to identify a monotone function $h$, which acts as a classifier, mapping
from $\mathbb{R}^d$ to $\{-1, 1\}$ with a small {\em error}, measured as the
number of points $p \in P$ whose labels differ from the function values $h(p)$.
The cost of an algorithm is defined as the number of points having their labels
revealed. This article presents the first study on the lowest cost required to
find a monotone classifier whose error is at most $(1 + \epsilon) \cdot k^*$
where $\epsilon \ge 0$ and $k^*$ is the minimum error achieved by an optimal
monotone classifier -- in other words, the error is allowed to exceed the
optimal by at most a relative factor. Nearly matching upper and lower bounds
are presented for the full range of $\epsilon$. All previous work on the
problem can only achieve an error higher than the optimal by an absolute
factor.

</details>


### [155] [Dense Associative Memory with Epanechnikov Energy](https://arxiv.org/abs/2506.10801)
*Benjamin Hoover,Zhaoyang Shi,Krishnakumar Balasubramanian,Dmitry Krotov,Parikshit Ram*

Main category: cs.LG

TL;DR: 提出基于Epanechnikov核的log-sum-ReLU能量函数，为DenseAM网络实现指数容量记忆检索，并首次发现兼具完美模式恢复与涌现局部极小值的特性。


<details>
  <summary>Details</summary>
Motivation: 传统log-sum-exponential(LSE)函数需要指数分离函数且缺乏涌现记忆特性，需开发更优的能量函数以提升密集联想记忆网络的存储容量与生成潜力。

Method: 受最优核密度估计启发，设计log-sum-ReLU(LSR)能量函数，利用Epanechnikov核实现无需指数分离的精确记忆检索，并自然产生丰富涌现记忆。

Result: LSR较LSE显著增加局部极小值数量（达3.5倍），保持同等对数似然度，且在图像数据中展现出具有创造性的涌现记忆模式。

Conclusion: LSR突破DenseAM传统框架，为大规模记忆存储与生成式应用提供新方向，其涌现记忆机制揭示类创造性特征。

Abstract: We propose a novel energy function for Dense Associative Memory (DenseAM)
networks, the log-sum-ReLU (LSR), inspired by optimal kernel density
estimation. Unlike the common log-sum-exponential (LSE) function, LSR is based
on the Epanechnikov kernel and enables exact memory retrieval with exponential
capacity without requiring exponential separation functions. Moreover, it
introduces abundant additional \emph{emergent} local minima while preserving
perfect pattern recovery -- a characteristic previously unseen in DenseAM
literature. Empirical results show that LSR energy has significantly more local
minima (memories) that have comparable log-likelihood to LSE-based models.
Analysis of LSR's emergent memories on image datasets reveals a degree of
creativity and novelty, hinting at this method's potential for both large-scale
memory storage and generative tasks.

</details>


### [156] [Detecting High-Stakes Interactions with Activation Probes](https://arxiv.org/abs/2506.10805)
*Alex McKenzie,Urja Pawar,Phil Blandfort,William Bankes,David Krueger,Ekdeep Singh Lubana,Dmitrii Krasheninnikov*

Main category: cs.LG

TL;DR: 本文提出使用基于合成数据训练的激活探针监测大语言模型的高风险交互，该方法在保持监测性能的同时实现百万倍计算效率提升，并构建了分层监测系统框架。


<details>
  <summary>Details</summary>
Motivation: 大语言模型部署中，检测可能导致重大危害的'高风险'交互是安全监控的关键需求，但现有研究对此关注不足。

Method: 使用合成数据训练多种激活探针架构，测试其在真实场景中的泛化能力，并与提示工程/微调的中型LLM监测器进行对比。

Result: 探针在跨领域真实数据中表现稳健，性能与中型LLM监测器相当，计算成本降低百万倍，且支持构建资源感知的分层监测系统。

Conclusion: 激活探针可作为高效的前端过滤器，为构建分层监控系统提供可行方案，研究团队公开了合成数据集和代码以促进后续研究。

Abstract: Monitoring is an important aspect of safely deploying Large Language Models
(LLMs). This paper examines activation probes for detecting "high-stakes"
interactions -- where the text indicates that the interaction might lead to
significant harm -- as a critical, yet underexplored, target for such
monitoring. We evaluate several probe architectures trained on synthetic data,
and find them to exhibit robust generalization to diverse, out-of-distribution,
real-world data. Probes' performance is comparable to that of prompted or
finetuned medium-sized LLM monitors, while offering computational savings of
six orders-of-magnitude. Our experiments also highlight the potential of
building resource-aware hierarchical monitoring systems, where probes serve as
an efficient initial filter and flag cases for more expensive downstream
analysis. We release our novel synthetic dataset and codebase to encourage
further study.

</details>


### [157] [Efficiency Robustness of Dynamic Deep Learning Systems](https://arxiv.org/abs/2506.10831)
*Ravishka Rathnasuriya,Tingxi Li,Zexin Xu,Zihe Song,Mirazul Haque,Simin Chen,Wei Yang*

Main category: cs.LG

TL;DR: 本文系统研究动态深度学习系统（DDLS）的效率鲁棒性，提出针对动态计算、推理迭代及动态下游任务输出的效率攻击分类法，揭示现有防御机制在应对此类攻击时的局限性。


<details>
  <summary>Details</summary>
Motivation: 动态深度学习系统（DDLS）通过动态调整推理计算提升效率，但其动态行为引入了新的攻击面（如对抗攻击降低系统效率）。现有研究缺乏对此类攻击的系统性分析及防御策略的有效性验证。

Method: 提出首个针对DDLS效率攻击的综合分类法，基于动态计算、推理迭代和动态输出三类行为划分攻击类型，并通过实验评估攻击策略及现有防御机制的效果。

Result: 发现效率攻击可针对DDLS不同动态行为发起，现有防御方法无法有效应对此类攻击，需开发新型防御策略以保障未来DDLS的安全性。

Conclusion: DDLS的高效性伴随新型安全风险，需重新设计防御机制以平衡效率与鲁棒性，推动自适应系统的安全部署。

Abstract: Deep Learning Systems (DLSs) are increasingly deployed in real-time
applications, including those in resourceconstrained environments such as
mobile and IoT devices. To address efficiency challenges, Dynamic Deep Learning
Systems (DDLSs) adapt inference computation based on input complexity, reducing
overhead. While this dynamic behavior improves efficiency, such behavior
introduces new attack surfaces. In particular, efficiency adversarial attacks
exploit these dynamic mechanisms to degrade system performance. This paper
systematically explores efficiency robustness of DDLSs, presenting the first
comprehensive taxonomy of efficiency attacks. We categorize these attacks based
on three dynamic behaviors: (i) attacks on dynamic computations per inference,
(ii) attacks on dynamic inference iterations, and (iii) attacks on dynamic
output production for downstream tasks. Through an in-depth evaluation, we
analyze adversarial strategies that target DDLSs efficiency and identify key
challenges in securing these systems. In addition, we investigate existing
defense mechanisms, demonstrating their limitations against increasingly
popular efficiency attacks and the necessity for novel mitigation strategies to
secure future adaptive DDLSs.

</details>


### [158] [Advanced fraud detection using machine learning models: enhancing financial transaction security](https://arxiv.org/abs/2506.10842)
*Nudrat Fariha,Md Nazmuddin Moin Khan,Md Iqbal Hossain,Syed Ali Reza,Joy Chakra Bortty,Kazi Sharmin Sultana,Md Shadidur Islam Jawad,Saniah Safat,Md Abdul Ahad,Maksuda Begum*

Main category: cs.LG

TL;DR: 本研究提出一个端到端机器学习框架，通过特征工程整合多源数据，利用无监督模型和聚类方法检测信用卡交易欺诈，可视化分析验证模型分离异常的能力。


<details>
  <summary>Details</summary>
Motivation: 随着数字支付普及，欺诈检测对智能、可扩展系统的需求日益迫切。研究旨在构建高效框架，利用真实数据识别信用卡交易中的异常行为。

Method: 整合交易、持卡人、商户等多源数据，提取行为特征（如消费均值、时序异常）与时间特征；使用孤立森林、单类SVM、自编码器等无监督模型，结合PCA可视化和K-Means、DBSCAN聚类分析异常区域。

Result: 模型成功标记前1%重构误差为异常，PCA显示模型能有效分离异常至二维潜在空间，聚类方法识别出正常活动密集区与可疑稀疏区域。

Conclusion: 该框架通过多维度特征与无监督学习，为信用卡欺诈检测提供可解释、可扩展的解决方案，验证了行为模式与时空特征在异常识别中的关键作用。

Abstract: The rise of digital payments has accelerated the need for intelligent and
scalable systems to detect fraud. This research presents an end-to-end,
feature-rich machine learning framework for detecting credit card transaction
anomalies and fraud using real-world data. The study begins by merging
transactional, cardholder, merchant, and merchant category datasets from a
relational database to create a unified analytical view. Through the feature
engineering process, we extract behavioural signals such as average spending,
deviation from historical patterns, transaction timing irregularities, and
category frequency metrics. These features are enriched with temporal markers
such as hour, day of week, and weekend indicators to expose all latent patterns
that indicate fraudulent behaviours. Exploratory data analysis reveals
contextual transaction trends across all the dataset features. Using the
transactional data, we train and evaluate a range of unsupervised models:
Isolation Forest, One Class SVM, and a deep autoencoder trained to reconstruct
normal behavior. These models flag the top 1% of reconstruction errors as
outliers. PCA visualizations illustrate each models ability to separate
anomalies into a two-dimensional latent space. We further segment the
transaction landscape using K-Means clustering and DBSCAN to identify dense
clusters of normal activity and isolate sparse, suspicious regions.

</details>


### [159] [Viability of Future Actions: Robust Safety in Reinforcement Learning via Entropy Regularization](https://arxiv.org/abs/2506.10871)
*Pierre-François Massiani,Alexander von Rohr,Lukas Haverbeck,Sebastian Trimpe*

Main category: cs.LG

TL;DR: 本文提出通过结合熵正则化与约束惩罚，在模型无关强化学习中实现鲁棒安全性。研究表明，熵正则化能提升动作噪声下的约束满足能力，而将严格约束松弛为惩罚项可近似转化为无约束问题，利用标准RL方法在保持安全性的同时增强抗干扰性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在未知干扰下难以保证状态约束的鲁棒性，需探索如何通过简单有效的方法实现安全策略学习。

Method: 分析熵正则化与约束惩罚的协同作用，将严格约束问题通过惩罚项转化为无约束优化问题，并利用标准模型无关RL算法求解。

Result: 熵正则化通过最大化未来可行动作空间提升抗噪能力，约束松弛方法在保持安全与最优性的同时显著增强对干扰的鲁棒性。

Conclusion: 熵正则化与鲁棒安全性的关联为RL安全研究提供了新方向，通过简单奖励塑形即可实现抗干扰的安全策略学习。

Abstract: Despite the many recent advances in reinforcement learning (RL), the question
of learning policies that robustly satisfy state constraints under unknown
disturbances remains open. In this paper, we offer a new perspective on
achieving robust safety by analyzing the interplay between two well-established
techniques in model-free RL: entropy regularization, and constraints
penalization. We reveal empirically that entropy regularization in constrained
RL inherently biases learning toward maximizing the number of future viable
actions, thereby promoting constraints satisfaction robust to action noise.
Furthermore, we show that by relaxing strict safety constraints through
penalties, the constrained RL problem can be approximated arbitrarily closely
by an unconstrained one and thus solved using standard model-free RL. This
reformulation preserves both safety and optimality while empirically improving
resilience to disturbances. Our results indicate that the connection between
entropy regularization and robustness is a promising avenue for further
empirical and theoretical investigation, as it enables robust safety in RL
through simple reward shaping.

</details>


### [160] [Lattice Climber Attack: Adversarial attacks for randomized mixtures of classifiers](https://arxiv.org/abs/2506.10888)
*Lucas Gnecco-Heredia,Benjamin Negrevergne,Yann Chevaleyre*

Main category: cs.LG

TL;DR: 本文针对有限混合分类器（随机集成）的对抗攻击问题，提出了一种新的攻击方法——格点攀爬攻击，并通过理论和实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法不适用于有限混合分类器，需在几何分析基础上建立具有理论保证的攻击原则。

Method: 通过几何分析定义攻击的有效性和极大性两个核心性质，提出满足这些性质的格点攀爬攻击方法，并在二元线性场景中提供理论证明。

Result: 在合成数据集和真实数据集上的实验表明，新攻击策略优于现有方法，且能有效突破混合分类器的防御机制。

Conclusion: 格点攀爬攻击首次同时满足有效性与极大性要求，为评估混合分类器鲁棒性提供了新工具，并揭示了现有防御方法的局限性。

Abstract: Finite mixtures of classifiers (a.k.a. randomized ensembles) have been
proposed as a way to improve robustness against adversarial attacks. However,
existing attacks have been shown to not suit this kind of classifier. In this
paper, we discuss the problem of attacking a mixture in a principled way and
introduce two desirable properties of attacks based on a geometrical analysis
of the problem (effectiveness and maximality). We then show that existing
attacks do not meet both of these properties. Finally, we introduce a new
attack called {\em lattice climber attack} with theoretical guarantees in the
binary linear setting, and demonstrate its performance by conducting
experiments on synthetic and real datasets.

</details>


### [161] [The Diffusion Duality](https://arxiv.org/abs/2506.10892)
*Subham Sekhar Sahoo,Justin Deschenaux,Aaron Gokaslan,Guanghan Wang,Justin Chiu,Volodymyr Kuleshov*

Main category: cs.LG

TL;DR: 本文提出Duo方法，通过将高斯扩散技术迁移到均匀状态离散扩散模型中，结合课程学习和离散一致性蒸馏，显著提升训练速度与生成效率，在多个基准测试中超越自回归模型，并实现百倍加速采样。


<details>
  <summary>Details</summary>
Motivation: 均匀状态离散扩散模型虽具备自我纠正能力，但其性能长期落后于自回归模型和掩码扩散模型。本文发现此类模型本质与高斯扩散过程相关，因此探索如何利用高斯扩散技术提升其表现。

Method: 1. 基于高斯过程的课程学习策略：通过动态调整训练目标降低方差，使训练速度翻倍；2. 离散一致性蒸馏：将连续域的一致性蒸馏算法适配到离散场景，实现少步长高效采样。

Result: 课程学习使模型在7个基准中的3个实现零样本困惑度超越自回归模型；一致性蒸馏使采样速度提升两个数量级，代码与模型已开源。

Conclusion: 通过挖掘离散扩散与高斯扩散的关联性，Duo有效缩小了性能差距，证明了离散扩散模型在保持快速生成优势的同时达到先进性能的可行性。

Abstract: Uniform-state discrete diffusion models hold the promise of fast text
generation due to their inherent ability to self-correct. However, they are
typically outperformed by autoregressive models and masked diffusion models. In
this work, we narrow this performance gap by leveraging a key insight:
Uniform-state diffusion processes naturally emerge from an underlying Gaussian
diffusion. Our method, Duo, transfers powerful techniques from Gaussian
diffusion to improve both training and sampling. First, we introduce a
curriculum learning strategy guided by the Gaussian process, doubling training
speed by reducing variance. Models trained with curriculum learning surpass
autoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we
present Discrete Consistency Distillation, which adapts consistency
distillation from the continuous to the discrete setting. This algorithm
unlocks few-step generation in diffusion language models by accelerating
sampling by two orders of magnitude. We provide the code and model checkpoints
on the project page: http://s-sahoo.github.io/duo

</details>


### [162] [NoLoCo: No-all-reduce Low Communication Training Method for Large Models](https://arxiv.org/abs/2506.10911)
*Jari Kolehmainen,Nikolay Blagoev,John Donaghy,Oğuzhan Ersoy,Christopher Nies*

Main category: cs.LG

TL;DR: 提出NoLoCo优化方法，通过改进Nesterov动量实现隐式参数同步，无需集体通信，显著降低大规模语言模型训练的通信开销。


<details>
  <summary>Details</summary>
Motivation: 传统大规模语言模型训练依赖高带宽集群，扩展成本高且受限。现有低通信方法仍需同步参数，在低带宽网络中效率低下。

Method: 基于改进版Nesterov动量优化器，通过随机选择模型副本进行部分权重平均，实现隐式同步，避免显式参数同步和集体通信。

Result: 在125M至6.8B参数模型中，通信开销显著低于全分片数据并行和DiLoCo方法，同步速度比DiLoCo快一个量级，收敛速度提升4%且无全局阻塞。

Conclusion: NoLoCo有效降低通信瓶颈，适用于低带宽环境的大规模分布式训练，在加速器利用率和收敛效率上优于现有方法。

Abstract: Training large language models is generally done via optimization methods on
clusters containing tens of thousands of accelerators, communicating over a
high-bandwidth interconnect. Scaling up these clusters is expensive and can
become impractical, imposing limits on the size of models that can be trained.
Several recent studies have proposed training methods that are less
communication intensive, avoiding the need for a highly connected compute
cluster. These state-of-the-art low communication training methods still employ
a synchronization step for model parameters, which, when performed over all
model replicas, can become costly on a low-bandwidth network.
  In this work, we propose a novel optimization method, NoLoCo, that does not
explicitly synchronize all model parameters during training and, as a result,
does not require any collective communication. NoLoCo implicitly synchronizes
model weights via a novel variant of the Nesterov momentum optimizer by
partially averaging model weights with a randomly selected other one. We
provide both a theoretical convergence analysis for our proposed optimizer as
well as empirical results from language model training.
  We benchmark NoLoCo on a wide range of accelerator counts and model sizes,
between 125M to 6.8B parameters. Our method requires significantly less
communication overhead than fully sharded data parallel training or even widely
used low communication training method, DiLoCo. The synchronization step itself
is estimated to be one magnitude faster than the all-reduce used in DiLoCo for
few hundred accelerators training over the internet. We also do not have any
global blocking communication that reduces accelerator idling time. Compared to
DiLoCo, we also observe up to $4\%$ faster convergence rate with wide range of
model sizes and accelerator counts.

</details>


### [163] [Foundation Models for Causal Inference via Prior-Data Fitted Networks](https://arxiv.org/abs/2506.10914)
*Yuchen Ma,Dennis Frauen,Emil Javurek,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 本文提出CausalFM框架，基于先验数据拟合网络（PFNs）和结构因果模型（SCM）构建贝叶斯先验，实现多场景因果推断，并在CATE估计中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有因果推断方法依赖特定假设且灵活性不足，而PFNs通过预训练和上下文学习实现贝叶斯推断，但尚未系统应用于因果推断。需开发通用框架以支持后门/前门/工具变量等多种调整方法。

Method: 1. 基于SCM构建贝叶斯先验并验证有效性准则；2. 提出因果启发的贝叶斯神经网络先验分布族；3. 实例化CausalFM模型，通过后门调整训练CATE估计基础模型。

Result: CausalFM在合成/半合成数据集的CATE估计任务中表现具有竞争力，验证了框架在多种因果场景下的有效性。

Conclusion: CausalFM为因果推断基础模型训练提供通用范式，可能革新医学、经济学等领域的因果分析实践，突破现有方法局限性。

Abstract: Prior-data fitted networks (PFNs) have recently been proposed as a promising
way to train tabular foundation models. PFNs are transformers that are
pre-trained on synthetic data generated from a prespecified prior distribution
and that enable Bayesian inference through in-context learning. In this paper,
we introduce CausalFM, a comprehensive framework for training PFN-based
foundation models in various causal inference settings. First, we formalize the
construction of Bayesian priors for causal inference based on structural causal
models (SCMs) in a principled way and derive necessary criteria for the
validity of such priors. Building on this, we propose a novel family of prior
distributions using causality-inspired Bayesian neural networks that enable
CausalFM to perform Bayesian causal inference in various settings, including
back-door, front-door, and instrumental variable adjustment. Finally, we
instantiate CausalFM and explicitly train a foundation model for estimating
conditional average treatment effects (CATEs) using back-door adjustment. We
show that CausalFM performs competitively for CATE estimation using various
synthetic and semi-synthetic benchmarks. In sum, our framework can be used as a
general recipe to train foundation models for various causal inference
settings. In contrast to the current state-of-the-art in causal inference,
CausalFM offers a novel paradigm with the potential to fundamentally change how
practitioners perform causal inference in medicine, economics, and other
disciplines.

</details>


### [164] [Sequential-Parallel Duality in Prefix Scannable Models](https://arxiv.org/abs/2506.10918)
*Morris Yau,Sharut Gupta,Valerie Engelmayer,Kazuki Irie,Stefanie Jegelka,Jacob Andreas*

Main category: cs.LG

TL;DR: 本文提出前缀可扫描模型（PSMs），通过放宽状态聚合操作符的关联性限制，统一了多种序列模型（如Mamba、GLA等），在保持Transformer表达力的同时，实现高效并行训练与线性时间序列推断，并在实验中验证了其效率和长度泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有神经序列模型（如GLA、Mamba）虽满足并行训练与快速序列推断的“序列-并行二元性”，但缺乏对支持此类特性的模型类别的完整刻画。本文旨在定义此类模型的通用框架，并探索其扩展可能性。

Method: 1. 将状态空间模型定义为可通过并行前缀扫描算法更新的模型；2. 提出更通用的PSMs，允许非关联性聚合操作符（如softmax注意力）；3. 在语言建模和合成任务（如状态跟踪、关联回忆）中实证评估PSMs。

Result: PSMs在保持Transformer相当表达力的同时，匹配状态空间模型的推理效率（O(1)每词元计算、log(N)内存），部分任务中长度泛化能力优于两者，且能扩展至含softmax类操作符的新模型。

Conclusion: PSMs通过放宽状态聚合约束，统一了多种序列模型架构，在效率与表达能力间取得平衡，为设计兼具并行训练和高效推理的模型提供了理论框架与实证支持。

Abstract: Modern neural sequence models are designed to meet the dual mandate of
parallelizable training and fast sequential inference. Recent developments have
given rise to various models, such as Gated Linear Attention (GLA) and Mamba,
that achieve such ``sequential-parallel duality.'' This raises a natural
question: can we characterize the full class of neural sequence models that
support near-constant-time parallel evaluation and linear-time, constant-space
sequential inference? We begin by describing a broad class of such models --
state space models -- as those whose state updates can be computed using the
classic parallel prefix scan algorithm with a custom associative aggregation
operator. We then define a more general class, Prefix-Scannable Models (PSMs),
by relaxing the state aggregation operator to allow arbitrary (potentially
non-associative) functions such as softmax attention. This generalization
unifies many existing architectures, including element-wise RNNs (e.g., Mamba)
and linear transformers (e.g., GLA, Mamba2, mLSTM), while also introducing new
models with softmax-like operators that achieve O(1) amortized compute per
token and log(N) memory for sequence length N. We empirically evaluate such
models on illustrative small-scale language modeling and canonical synthetic
tasks, including state tracking and associative recall. Empirically, we find
that PSMs retain the expressivity of transformer-based architectures while
matching the inference efficiency of state space models -- in some cases
exhibiting better length generalization than either.

</details>


### [165] [Robustly Improving LLM Fairness in Realistic Settings via Interpretability](https://arxiv.org/abs/2506.10922)
*Adam Karvonen,Samuel Marks*

Main category: cs.LG

TL;DR: 研究发现现有大语言模型在招聘场景中引入现实情境细节（如公司文化、筛选条件）会引发显著种族/性别偏见（最高12%面试率差异），传统反偏见提示失效。通过内部干预方法（识别敏感属性方向并修正）可稳定降低偏见至1%以下，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型被广泛用于高风险招聘决策，但现有反偏见方法在现实复杂情境中失效，可能导致职业机会分配不公。需开发更鲁棒的偏见缓解方案以确保公平性。

Method: 提出内部偏见缓解框架：1) 通过合成数据集识别模型激活中的种族/性别相关方向 2) 在推理时应用仿射概念编辑消除敏感属性关联 3) 跨多商业/开源模型验证方法有效性。

Result: 现实情境下所有测试模型均出现系统性偏见（黑人>白人，女性>男性），传统检测方法无法捕捉基于大学背景的隐性偏见。内部干预使偏见率稳定低于2.5%，模型性能损失可忽略。

Conclusion: 招聘场景需采用含现实情境的评估方法，并优先实施内部偏见缓解策略。概念编辑技术能有效实现模型公平性，为实际部署提供可行解决方案。

Abstract: Large language models (LLMs) are increasingly deployed in high-stakes hiring
applications, making decisions that directly impact people's careers and
livelihoods. While prior studies suggest simple anti-bias prompts can eliminate
demographic biases in controlled evaluations, we find these mitigations fail
when realistic contextual details are introduced. We address these failures
through internal bias mitigation: by identifying and neutralizing sensitive
attribute directions within model activations, we achieve robust bias reduction
across all tested scenarios. Across leading commercial (GPT-4o, Claude 4
Sonnet, Gemini 2.5 Flash) and open-source models (Gemma-2 27B, Gemma-3,
Mistral-24B), we find that adding realistic context such as company names,
culture descriptions from public careers pages, and selective hiring
constraints (e.g.,``only accept candidates in the top 10\%") induces
significant racial and gender biases (up to 12\% differences in interview
rates). When these biases emerge, they consistently favor Black over White
candidates and female over male candidates across all tested models and
scenarios. Moreover, models can infer demographics and become biased from
subtle cues like college affiliations, with these biases remaining invisible
even when inspecting the model's chain-of-thought reasoning. To address these
limitations, our internal bias mitigation identifies race and gender-correlated
directions and applies affine concept editing at inference time. Despite using
directions from a simple synthetic dataset, the intervention generalizes
robustly, consistently reducing bias to very low levels (typically under 1\%,
always below 2.5\%) while largely maintaining model performance. Our findings
suggest that practitioners deploying LLMs for hiring should adopt more
realistic evaluation methodologies and consider internal mitigation strategies
for equitable outcomes.

</details>


### [166] [Developing a High-performance Framework for Speech Emotion Recognition in Naturalistic Conditions Challenge for Emotional Attribute Prediction](https://arxiv.org/abs/2506.10930)
*Thanathai Lertpetchpun,Tiantian Feng,Dani Byrd,Shrikanth Narayanan*

Main category: cs.LG

TL;DR: 本文提出了一种可复现的多模态框架，在自然条件下语音情感识别挑战赛中取得最佳成绩，通过处理标注不一致和数据不平衡问题，结合文本嵌入、性别预测及特定样本训练，并利用简单集成策略实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 自然条件下的语音情感识别面临标注不一致和数据分布不均衡的挑战，需开发有效方法提升模型鲁棒性。

Method: 采用多模态学习、多任务学习及不平衡处理技术，整合文本嵌入、性别预测，并在训练集中包含“其他”和“无协议”样本。

Result: 系统在IS25-SER挑战赛中包揽前两名，最佳性能通过两系统简单集成实现。

Conclusion: 多模态与多任务学习结合数据平衡策略有效提升自然场景语音情感识别性能，轻量级集成可进一步优化结果。

Abstract: Speech emotion recognition (SER) in naturalistic conditions presents a
significant challenge for the speech processing community. Challenges include
disagreement in labeling among annotators and imbalanced data distributions.
This paper presents a reproducible framework that achieves superior (top 1)
performance in the Emotion Recognition in Naturalistic Conditions Challenge
(IS25-SER Challenge) - Task 2, evaluated on the MSP-Podcast dataset. Our system
is designed to tackle the aforementioned challenges through multimodal
learning, multi-task learning, and imbalanced data handling. Specifically, our
best system is trained by adding text embeddings, predicting gender, and
including ``Other'' (O) and ``No Agreement'' (X) samples in the training set.
Our system's results secured both first and second places in the IS25-SER
Challenge, and the top performance was achieved by a simple two-system
ensemble.

</details>


### [167] [Self-Adapting Language Models](https://arxiv.org/abs/2506.10943)
*Adam Zweiger,Jyothish Pari,Han Guo,Ekin Akyürek,Yoon Kim,Pulkit Agrawal*

Main category: cs.LG

TL;DR: SEAL框架通过让大语言模型自主生成微调数据和更新指令，实现自我适应，提升在知识整合和少样本任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLMs）缺乏动态适应新任务、知识或示例的机制，无法通过权重调整持续改进。

Method: 提出SEAL框架：模型生成自我编辑指令（如信息重构、超参数设定、工具调用），通过监督微调（SFT）更新权重，并利用强化学习以任务表现为奖励信号优化生成过程。

Result: 实验表明SEAL在知识整合和少样本泛化任务中有效，验证了模型自我导向适应的可行性。

Conclusion: SEAL为语言模型实现自主持续适应提供了新方向，无需依赖外部模块或辅助网络。

Abstract: Large language models (LLMs) are powerful but static; they lack mechanisms to
adapt their weights in response to new tasks, knowledge, or examples. We
introduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to
self-adapt by generating their own finetuning data and update directives. Given
a new input, the model produces a self-edit-a generation that may restructure
the information in different ways, specify optimization hyperparameters, or
invoke tools for data augmentation and gradient-based updates. Through
supervised finetuning (SFT), these self-edits result in persistent weight
updates, enabling lasting adaptation. To train the model to produce effective
self-edits, we use a reinforcement learning loop with the downstream
performance of the updated model as the reward signal. Unlike prior approaches
that rely on separate adaptation modules or auxiliary networks, SEAL directly
uses the model's own generation to control its adaptation process. Experiments
on knowledge incorporation and few-shot generalization show that SEAL is a
promising step toward language models capable of self-directed adaptation. Our
website and code is available at https://jyopari.github.io/posts/seal.

</details>


### [168] [GUARD: Guided Unlearning and Retention via Data Attribution for Large Language Models](https://arxiv.org/abs/2506.10946)
*Evelyn Ma,Duo Zhou,Peizhi Niu,Huiting Zhou,Huan Zhang,Olgica Milenkovic,S. Rasoul Etesami*

Main category: cs.LG

TL;DR: 本文提出GUARD框架，通过数据归因指导大语言模型反学习，减少非目标遗忘，在保持遗忘效果的同时显著提升保留信息的效用。实验表明，GUARD在TOFU基准测试中最高可减少194.92%的效用损失。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型反学习方法主要关注架构改进，但数据层因素对性能影响未被充分探索，导致高影响力数据遗忘时模型效用严重下降。需解决反学习过程中非目标遗忘与保留信息效用间的平衡问题。

Method: GUARD框架引入轻量级代理数据归因指标，量化遗忘集与保留集的对齐程度；设计自适应非均匀反学习权重分配机制，根据归因分数动态调整样本权重，通过重新分配反学习强度减少保留信息损失。

Result: 在TOFU基准测试中，当遗忘10%训练数据时，GUARD将保留集的Truth Ratio效用损失降低达194.92%，在多种LLM架构上验证了框架有效性，且理论保证遗忘指标与现有方法相当。

Conclusion: GUARD通过数据层优化实现反学习过程中保留效用的显著提升，证明数据归因机制在模型更新中的关键作用，为合规性需求下的LLM持续优化提供新范式。

Abstract: Unlearning in large language models (LLMs) is becoming increasingly important
due to regulatory compliance, copyright protection, and privacy concerns.
However, a key challenge in LLM unlearning is unintended forgetting, where the
removal of specific data inadvertently impairs the utility of the model and its
retention of valuable, desired information. While prior work has primarily
focused on architectural innovations, the influence of data-level factors on
unlearning performance remains underexplored. As a result, existing methods
often suffer from degraded retention when forgetting high-impact data. To
address this, we propose GUARD-a novel framework for Guided Unlearning And
Retention via Data attribution. At its core, GUARD introduces a lightweight
proxy data attribution metric tailored for LLM unlearning, which quantifies the
"alignment" between the forget and retain sets while remaining computationally
efficient. Building on this, we design a novel unlearning objective that
assigns adaptive, nonuniform unlearning weights to samples, inversely
proportional to their proxy attribution scores. Through such a reallocation of
unlearning power, GUARD mitigates unintended losses in retention. We provide
rigorous theoretical guarantees that GUARD significantly enhances retention
while maintaining forgetting metrics comparable to prior methods. Extensive
experiments on the TOFU benchmark across multiple LLM architectures demonstrate
that GUARD substantially improves utility preservation while ensuring effective
unlearning. Notably, GUARD reduces utility sacrifice on the Retain Set by up to
194.92% in terms of Truth Ratio when forgetting 10% of the training data.

</details>


### [169] [Execution Guided Line-by-Line Code Generation](https://arxiv.org/abs/2506.10948)
*Boaz Lavon,Shahar Katz,Lior Wolf*

Main category: cs.LG

TL;DR: 提出EG-CFG方法，在代码生成过程中动态整合实时执行信号，通过逐行反馈提升生成质量，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在代码生成时未充分利用执行反馈，而人类程序员常依赖此类信号。EG-CFG旨在通过实时执行信号引导生成过程。

Method: 采用三阶段流程：1) 波束搜索生成候选代码行；2) 通过测试用例执行提取信号；3) 将信号整合至生成提示。通过行级信号一致性与并行多代理机制保持结构连贯性。

Result: 实验表明EG-CFG在基础问题至竞赛级编程任务中均显著提升性能，达到当前最优结果，支持任务级并行探索多样化推理路径。

Conclusion: EG-CFG通过动态执行反馈机制有效提升代码生成质量，其并行架构为复杂任务提供了可扩展的解决方案。

Abstract: We present a novel approach to neural code generation that incorporates
real-time execution signals into the language model generation process. While
large language models (LLMs) have demonstrated impressive code generation
capabilities, they typically do not utilize execution feedback during
inference, a critical signal that human programmers regularly leverage. Our
method, Execution-Guided Classifier-Free Guidance (EG-CFG), dynamically
incorporates execution signals as the model generates code, providing
line-by-line feedback that guides the generation process toward executable
solutions. EG-CFG employs a multi-stage process: first, we conduct beam search
to sample candidate program completions for each line; second, we extract
execution signals by executing these candidates against test cases; and
finally, we incorporate these signals into the prompt during generation. By
maintaining consistent signals across tokens within the same line and
refreshing signals at line boundaries, our approach provides coherent guidance
while preserving syntactic structure. Moreover, the method naturally supports
native parallelism at the task level in which multiple agents operate in
parallel, exploring diverse reasoning paths and collectively generating a broad
set of candidate solutions. Our experiments across diverse coding tasks
demonstrate that EG-CFG significantly improves code generation performance
compared to standard approaches, achieving state-of-the-art results across
various levels of complexity, from foundational problems to challenging
competitive programming tasks. Our code is available at:
https://github.com/boazlavon/eg_cfg

</details>


### [170] [Build the web for agents, not agents for the web](https://arxiv.org/abs/2506.10953)
*Xing Han Lù,Gaurav Kamath,Marius Mosbach,Siva Reddy*

Main category: cs.LG

TL;DR: 本文提出为AI代理设计专用网页界面（AWI），以解决现有方法因人类界面与LLM能力不匹配导致的效率低下问题，强调安全性、效率与标准化原则。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的网页代理在处理复杂网页输入时面临挑战，因人类设计的界面与代理能力不兼容，需开发适配代理专用交互范式。

Method: 提出“代理导向网页界面（AWI）”概念，建立六项设计原则（安全、效率、标准化等），协调多方利益相关者需求。

Result: 通过AWI框架重构网页交互模式，突破现有界面限制，为高效、可靠、透明的网页代理设计提供新路径。

Conclusion: 转向代理专用界面范式将推动网页代理技术发展，需机器学习社区协同实现界面与能力对齐的长期目标。

Abstract: Recent advancements in Large Language Models (LLMs) and multimodal
counterparts have spurred significant interest in developing web agents -- AI
systems capable of autonomously navigating and completing tasks within web
environments. While holding tremendous promise for automating complex web
interactions, current approaches face substantial challenges due to the
fundamental mismatch between human-designed interfaces and LLM capabilities.
Current methods struggle with the inherent complexity of web inputs, whether
processing massive DOM trees, relying on screenshots augmented with additional
information, or bypassing the user interface entirely through API interactions.
This position paper advocates for a paradigm shift in web agent research:
rather than forcing web agents to adapt to interfaces designed for humans, we
should develop a new interaction paradigm specifically optimized for agentic
capabilities. To this end, we introduce the concept of an Agentic Web Interface
(AWI), an interface specifically designed for agents to navigate a website. We
establish six guiding principles for AWI design, emphasizing safety,
efficiency, and standardization, to account for the interests of all primary
stakeholders. This reframing aims to overcome fundamental limitations of
existing interfaces, paving the way for more efficient, reliable, and
transparent web agent design, which will be a collaborative effort involving
the broader ML community.

</details>


### [171] [ReGuidance: A Simple Diffusion Wrapper for Boosting Sample Quality on Hard Inverse Problems](https://arxiv.org/abs/2506.10955)
*Aayush Karan,Kulin Shah,Sitan Chen*

Main category: cs.LG

TL;DR: 本文提出ReGuidance方法，通过反转概率流ODE并重新初始化DPS，有效提升扩散模型在低信噪比逆问题中的样本真实性与奖励表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练扩散模型的逆问题求解方法（如DPS）在低信噪比场景下易偏离数据流形，导致输出不真实。需要一种提升样本质量与测量一致性的通用方法。

Method: 提出ReGuidance封装方法：1) 对现有算法生成的候选解进行无条件概率流ODE反向运算得到潜在表示；2) 将该潜在表示作为DPS的初始化重新优化。

Result: 在大型框内修复和高倍超分辨率任务中，ReGuidance显著提升基线方法的样本质量与测量一致性，优于现有SOTA方法。理论证明其在多模态数据分布中可同时提升奖励与数据流形逼近。

Conclusion: ReGuidance首次为DPS提供了理论保证，通过两步优化机制有效解决了现有方法在困难逆问题中的性能退化问题，具有通用性与实践价值。

Abstract: There has been a flurry of activity around using pretrained diffusion models
as informed data priors for solving inverse problems, and more generally around
steering these models using reward models. Training-free methods like diffusion
posterior sampling (DPS) and its many variants have offered flexible heuristic
algorithms for these tasks, but when the reward is not informative enough,
e.g., in hard inverse problems with low signal-to-noise ratio, these techniques
veer off the data manifold, failing to produce realistic outputs. In this work,
we devise a simple wrapper, ReGuidance, for boosting both the sample realism
and reward achieved by these methods. Given a candidate solution $\hat{x}$
produced by an algorithm of the user's choice, we propose inverting the
solution by running the unconditional probability flow ODE in reverse starting
from $\hat{x}$, and then using the resulting latent as an initialization for
DPS. We evaluate our wrapper on hard inverse problems like large box
in-painting and super-resolution with high upscaling. Whereas state-of-the-art
baselines visibly fail, we find that applying our wrapper on top of these
baselines significantly boosts sample quality and measurement consistency. We
complement these findings with theory proving that on certain multimodal data
distributions, ReGuidance simultaneously boosts the reward and brings the
candidate solution closer to the data manifold. To our knowledge, this
constitutes the first rigorous algorithmic guarantee for DPS.

</details>


### [172] [Understanding In-Context Learning on Structured Manifolds: Bridging Attention to Kernel Methods](https://arxiv.org/abs/2506.10959)
*Zhaiming Shen,Alexander Hsu,Rongjie Lai,Wenjing Liao*

Main category: cs.LG

TL;DR: 该论文首次从理论角度研究了基于流形的Hölder函数回归问题，揭示了Transformer在上下文学习中通过注意力机制实现极小化回归误差的能力，其泛化误差与流形内禀维度呈指数关系。


<details>
  <summary>Details</summary>
Motivation: 尽管上下文学习在自然语言和视觉领域取得显著成功，但其在结构化几何数据（如流形）中的理论机制尚未被探索。本文旨在填补这一空白，揭示几何结构对ICL的影响机制。

Method: 通过建立Transformer注意力机制与经典核方法之间的理论联系，采用非参数回归框架分析流形上Hölder函数的泛化误差边界，重点研究提示长度和训练任务数量对模型性能的影响。

Result: 当训练任务足够多时，Transformer达到Hölder函数在流形上的极小极大回归率，其误差随流形内禀维度（非环境维度）指数级变化，并量化了训练任务数量与泛化误差的标度关系。

Conclusion: 研究揭示了流形几何结构对ICL性能的根本性影响，为理解Transformer作为上下文算法学习器的复杂性提供了新工具，建立了分析非线性模型ICL的理论框架。

Abstract: While in-context learning (ICL) has achieved remarkable success in natural
language and vision domains, its theoretical understanding--particularly in the
context of structured geometric data--remains unexplored. In this work, we
initiate a theoretical study of ICL for regression of H\"older functions on
manifolds. By establishing a novel connection between the attention mechanism
and classical kernel methods, we derive generalization error bounds in terms of
the prompt length and the number of training tasks. When a sufficient number of
training tasks are observed, transformers give rise to the minimax regression
rate of H\"older functions on manifolds, which scales exponentially with the
intrinsic dimension of the manifold, rather than the ambient space dimension.
Our result also characterizes how the generalization error scales with the
number of training tasks, shedding light on the complexity of transformers as
in-context algorithm learners. Our findings provide foundational insights into
the role of geometry in ICL and novels tools to study ICL of nonlinear models.

</details>


### [173] [Farseer: A Refined Scaling Law in Large Language Models](https://arxiv.org/abs/2506.10972)
*Houyi Li,Wenzhen Zheng,Qiufeng Wang,Zhenyu Ding,Haoying Wang,Zili Wang,Shijie Xuyang,Ning Ding,Shuigeng Zhou,Xiangyu Zhang,Daxin Jiang*

Main category: cs.LG

TL;DR: 本文提出Farseer，一种改进的扩展定律，通过构建模型损失表面L(N,D)显著提升预测精度，相比Chinchilla定律外推误差降低433%，并开源了大规模实验数据与模型。


<details>
  <summary>Details</summary>
Motivation: 训练大型语言模型（LLM）成本极高，导致小规模实验难以指导实际生产系统，形成扩展鸿沟。现有扩展定律（如Chinchilla定律）预测精度不足，阻碍高效创新。

Method: 提出Farseer方法，系统构建模型损失表面L(N,D)，优化参数拟合能力，支持跨规模训练策略评估，并通过约1000个不同规模的LLM实验（消耗300万H100 GPU小时）验证。

Result: Farseer外推误差比Chinchilla定律降低433%，能可靠预测大规模性能，揭示更精细的计算资源分配策略，并开源所有模型、数据及实验日志。

Conclusion: Farseer为LLM训练提供高精度跨规模预测工具，缩小实验与生产的扩展差距，其开源生态将推动后续研究。

Abstract: Training Large Language Models (LLMs) is prohibitively expensive, creating a
critical scaling gap where insights from small-scale experiments often fail to
transfer to resource-intensive production systems, thereby hindering efficient
innovation. To bridge this, we introduce Farseer, a novel and refined scaling
law offering enhanced predictive accuracy across scales. By systematically
constructing a model loss surface $L(N,D)$, Farseer achieves a significantly
better fit to empirical data than prior laws (e.g., Chinchilla's law). Our
methodology yields accurate, robust, and highly generalizable predictions,
demonstrating excellent extrapolation capabilities, improving upon Chinchilla's
law by reducing extrapolation error by 433\%. This allows for the reliable
evaluation of competing training strategies across all $(N,D)$ settings,
enabling conclusions from small-scale ablation studies to be confidently
extrapolated to predict large-scale performance. Furthermore, Farseer provides
new insights into optimal compute allocation, better reflecting the nuanced
demands of modern LLM training. To validate our approach, we trained an
extensive suite of approximately 1,000 LLMs across diverse scales and
configurations, consuming roughly 3 million NVIDIA H100 GPU hours. We are
comprehensively open-sourcing all models, data, results, and logs at
https://github.com/Farseer-Scaling-Law/Farseer to foster further research.

</details>


### [174] [Principled Approaches for Extending Neural Architectures to Function Spaces for Operator Learning](https://arxiv.org/abs/2506.10973)
*Julius Berner,Miguel Liu-Schiaffini,Jean Kossaifi,Valentin Duruisseaux,Boris Bonev,Kamyar Azizzadenesheli,Anima Anandkumar*

Main category: cs.LG

TL;DR: 本文提出一种将现有神经网络架构转换为神经算子的方法，通过最小修改实现无限维函数空间映射，旨在指导实践者应用神经算子解决科学问题。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习在有限维空间（如图像、文本）表现优异，但科学问题（如PDE求解）涉及无限维函数空间，导致神经网络在科学领域应用受限。现有神经算子多为独立模型，缺乏对成熟神经网络架构的继承。

Method: 提炼构建无限维函数空间映射的关键原则，提出将多种流行架构（如CNN、Transformer等）转换为神经算子的通用方法，保留原架构的工程优化。

Result: 开发了可复现的代码框架（https://github.com/neuraloperator/NNs-to-NOs），验证了通过最小修改即可将现有神经网络扩展为处理PDE解算子的神经算子。

Conclusion: 通过系统化转换方法，神经算子可继承深度学习在有限维空间的架构优势，为科学计算问题提供高效解决方案，推动深度学习在科学领域的应用。

Abstract: A wide range of scientific problems, such as those described by
continuous-time dynamical systems and partial differential equations (PDEs),
are naturally formulated on function spaces. While function spaces are
typically infinite-dimensional, deep learning has predominantly advanced
through applications in computer vision and natural language processing that
focus on mappings between finite-dimensional spaces. Such fundamental
disparities in the nature of the data have limited neural networks from
achieving a comparable level of success in scientific applications as seen in
other fields. Neural operators are a principled way to generalize neural
networks to mappings between function spaces, offering a pathway to replicate
deep learning's transformative impact on scientific problems. For instance,
neural operators can learn solution operators for entire classes of PDEs, e.g.,
physical systems with different boundary conditions, coefficient functions, and
geometries. A key factor in deep learning's success has been the careful
engineering of neural architectures through extensive empirical testing.
Translating these neural architectures into neural operators allows operator
learning to enjoy these same empirical optimizations. However, prior neural
operator architectures have often been introduced as standalone models, not
directly derived as extensions of existing neural network architectures. In
this paper, we identify and distill the key principles for constructing
practical implementations of mappings between infinite-dimensional function
spaces. Using these principles, we propose a recipe for converting several
popular neural architectures into neural operators with minimal modifications.
This paper aims to guide practitioners through this process and details the
steps to make neural operators work in practice. Our code can be found at
https://github.com/neuraloperator/NNs-to-NOs

</details>


### [175] [Rethinking Losses for Diffusion Bridge Samplers](https://arxiv.org/abs/2506.10982)
*Sebastian Sanokowski,Lukas Gruber,Christoph Bartmann,Sepp Hochreiter,Sebastian Lehner*

Main category: cs.LG

TL;DR: 本文分析扩散桥方法中Log Variance（LV）损失与反向Kullback-Leibler（rKL）损失的差异，提出使用结合对数导数技巧的rKL损失（rKL-LD）在概念和性能上更优。


<details>
  <summary>Details</summary>
Motivation: 现有研究显示LV损失在重参数化技巧下优于rKL损失，但扩散桥或学习扩散系数时两者的梯度等效性不成立，且LV损失无法通过数据处理不等式等理论合理优化。

Method: 提出使用rKL损失结合对数导数技巧（rKL-LD），避免LV损失的理论缺陷，并在不同扩散桥模型和基准测试中验证其有效性。

Result: 实验表明，rKL-LD损失在采样性能上优于LV损失，且训练更稳定、超参数调整需求更少。

Conclusion: rKL-LD损失在扩散桥方法中具有理论优势，实际应用效果更佳，是更优的优化目标。

Abstract: Diffusion bridges are a promising class of deep-learning methods for sampling
from unnormalized distributions. Recent works show that the Log Variance (LV)
loss consistently outperforms the reverse Kullback-Leibler (rKL) loss when
using the reparametrization trick to compute rKL-gradients. While the on-policy
LV loss yields identical gradients to the rKL loss when combined with the
log-derivative trick for diffusion samplers with non-learnable forward
processes, this equivalence does not hold for diffusion bridges or when
diffusion coefficients are learned. Based on this insight we argue that for
diffusion bridges the LV loss does not represent an optimization objective that
can be motivated like the rKL loss via the data processing inequality. Our
analysis shows that employing the rKL loss with the log-derivative trick
(rKL-LD) does not only avoid these conceptual problems but also consistently
outperforms the LV loss. Experimental results with different types of diffusion
bridges on challenging benchmarks show that samplers trained with the rKL-LD
loss achieve better performance. From a practical perspective we find that
rKL-LD requires significantly less hyperparameter optimization and yields more
stable training behavior.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [176] [AC/DC: LLM-based Audio Comprehension via Dialogue Continuation](https://arxiv.org/abs/2506.10312)
*Yusuke Fujita,Tomoya Mizumoto,Atsushi Kojima,Lianbo Liu,Yui Sudo*

Main category: eess.AS

TL;DR: 提出一种基于对话延续训练的音频理解模型，利用大语言模型的对话能力解决字幕变异性问题，实现零样本指令跟随。


<details>
  <summary>Details</summary>
Motivation: 传统音频字幕生成方法直接生成目标字幕，存在字幕表面词汇变化导致的语义表达不一致问题。

Method: 通过训练模型以对话延续形式响应输入字幕，而非直接生成字幕，从而深层捕捉语义并缓解变异性问题。

Result: 在AudioCaps、WavCaps和Clotho数据集上验证，模型通过AudioBench测试展现出对未见指令的泛化能力。

Conclusion: 对话延续训练策略使模型无需多任务调优即可实现零样本指令跟随，突破了传统音频字幕训练的局限性。

Abstract: We propose an instruction-following audio comprehension model that leverages
the dialogue continuation ability of large language models (LLMs). Instead of
directly generating target captions in training data, the proposed method
trains a model to produce responses as if the input caption triggered a
dialogue. This dialogue continuation training mitigates the caption variation
problem. Learning to continue a dialogue effectively captures the caption's
meaning beyond its surface-level words. As a result, our model enables
zero-shot instruction-following capability without multitask instruction
tuning, even trained solely on audio captioning datasets. Experiments on
AudioCaps, WavCaps, and Clotho datasets with AudioBench audio-scene
question-answering tests demonstrate our model's ability to follow various
unseen instructions.

</details>


### [177] [RT-VC: Real-Time Zero-Shot Voice Conversion with Speech Articulatory Coding](https://arxiv.org/abs/2506.10289)
*Yisi Liu,Chenyang Wang,Hanjo Kim,Raniya Khan,Gopala Anumanchipalli*

Main category: eess.AS

TL;DR: 本文提出RT-VC，一种零样本实时语音转换系统，通过发音特征解耦与可微分信号处理技术，在保持合成质量的同时实现61.4ms超低CPU延迟（较SOTA降低13.3%）。


<details>
  <summary>Details</summary>
Motivation: 语音转换技术在辅助通信与娱乐领域具有关键作用，但现有方法在实时性与延迟方面存在局限，需提升实时转换效率。

Method: 结合发音特征空间解耦内容/说话人特征，并集成可微分数字信号处理(DDSP)直接从发音特征生成语音，降低计算延迟。

Result: 实验表明RT-VC在合成质量与SOTA持平的条件下，CPU延迟降至61.4ms，延迟降低13.3%，且转换过程更具可解释性。

Conclusion: RT-VC通过特征解耦与DDSP技术实现了质量与延迟的平衡，为实时语音转换提供了高效解决方案。

Abstract: Voice conversion has emerged as a pivotal technology in numerous applications
ranging from assistive communication to entertainment. In this paper, we
present RT-VC, a zero-shot real-time voice conversion system that delivers
ultra-low latency and high-quality performance. Our approach leverages an
articulatory feature space to naturally disentangle content and speaker
characteristics, facilitating more robust and interpretable voice
transformations. Additionally, the integration of differentiable digital signal
processing (DDSP) enables efficient vocoding directly from articulatory
features, significantly reducing conversion latency. Experimental evaluations
demonstrate that, while maintaining synthesis quality comparable to the current
state-of-the-art (SOTA) method, RT-VC achieves a CPU latency of 61.4 ms,
representing a 13.3\% reduction in latency.

</details>


### [178] [Robust Unsupervised Adaptation of a Speech Recogniser Using Entropy Minimisation and Speaker Codes](https://arxiv.org/abs/2506.10653)
*Rogier C. van Dalen,Shucong Zhang,Titouan Parcollet,Sourav Bhattacharya*

Main category: eess.AS

TL;DR: 本文提出结合多假设条件熵损失函数与紧凑说话者编码的方法，利用仅1分钟无标注数据即可实现鲁棒的语音识别器自适应，在噪声增强数据集上词错误率相对降低20%-29%。


<details>
  <summary>Details</summary>
Motivation: 传统语音识别器在新说话者适应场景中面临两大挑战：微调所需标注数据稀缺，且单次错误假设的交叉熵损失容易导致模型过拟合。

Method: 提出双阶段方法：1) 用多假设条件熵损失替代单伪标签交叉熵，增强对初始识别错误的鲁棒性；2) 设计低维说话者编码向量，实现小样本下的说话者特征提取。

Result: 在远场噪声增强的Common Voice数据集上，1分钟/10分钟自适应数据分别获得20%/29%的词错误率相对改进，验证了方法的有效性。

Conclusion: 通过多假设集成与紧凑说话者编码的协同优化，显著提升了小样本无监督自适应的鲁棒性，为低资源场景的语音识别适配提供了新思路。

Abstract: Speech recognisers usually perform optimally only in a specific environment
and need to be adapted to work well in another. For adaptation to a new
speaker, there is often too little data for fine-tuning to be robust, and that
data is usually unlabelled. This paper proposes a combination of approaches to
make adaptation to a single minute of data robust. First, instead of estimating
the adaptation parameters with cross-entropy on a single error-prone hypothesis
or "pseudo-label", this paper proposes a novel loss function, the conditional
entropy over complete hypotheses. Using multiple hypotheses makes adaptation
more robust to errors in the initial recognition. Second, a "speaker code"
characterises a speaker in a vector short enough that it requires little data
to estimate. On a far-field noise-augmented version of Common Voice, the
proposed scheme yields a 20% relative improvement in word error rate on one
minute of adaptation data, increasing on 10 minutes to 29%.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [179] [Extended Creativity: A Conceptual Framework for Understanding Human-AI Creative Relations](https://arxiv.org/abs/2506.10249)
*Andrea Gaggioli,Sabrina Bartolotta,Andrea Ubaldi,Katusha Gerardini,Eleonora Diletta Sarcinella,Alice Chirico*

Main category: cs.HC

TL;DR: 该论文提出人工智能通过支持（工具）、协同（互补合作）与共生（认知融合）三种模式增强人类创造力，并从技术自主性和感知代理性两个维度分析其对不同层次创造力的影响，探讨理论、伦理及设计意义。


<details>
  <summary>Details</summary>
Motivation: 明确人工智能如何有效增强人类创造力的具体路径，解决现有研究中增强机制不清晰的问题。

Method: 基于分布式创造力理论框架，从技术自主性和感知代理性两个维度划分AI参与创造过程的三种模式（支持、Synergy、Symbiosis），并分析其与不同创造力层次的关系。

Result: 构建了AI增强创造力的分类模型，揭示不同模式对日常问题解决到范式颠覆创新的差异化影响，提出需针对性设计人机协作系统并关注伦理风险。

Conclusion: AI与人类创造力的整合需系统化框架指导，三种模式为理解人机协作提供了新视角，其技术-代理平衡将决定未来创新生态的形态。

Abstract: Artificial Intelligence holds significant potential to enhance human
creativity. However, achieving this vision requires a clearer understanding of
how such enhancement can be effectively realized. Adopting the perspective of
distributed creativity, we identify three primary modes through which AI can
contribute to creative processes: Support, where AI acts as a tool; Synergy,
where AI and humans collaborate in complementary ways; and Symbiosis, where
human and AI cognition become so integrated that they form a unified creative
system. These modes are defined along two key dimensions: the level of
technical autonomy exhibited by the AI system and the degree of perceived
agency attributed to it. We examine how each configuration influences different
levels of creativity - from everyday problem-solving to paradigm-shifting
innovation - and discuss the theoretical, ethical, and design implications.

</details>


### [180] [The Role of Generative AI in Facilitating Social Interactions: A Scoping Review](https://arxiv.org/abs/2506.10927)
*T. T. J. E. Arets,G. Perugia,M. Houben,W. A. IJsselsteijn*

Main category: cs.HC

TL;DR: 本文通过综述30项研究，探讨生成式AI（GAI）在促进人际互动中的应用设计、目标形式及评估方法，强调其动态个性化潜力，同时呼吁关注公平性与包容性。


<details>
  <summary>Details</summary>
Motivation: 社会连接减少威胁心理健康与福祉，而GAI技术（如大语言模型）被广泛用于增强社交体验，但其对互动的影响尚不明确，需系统性研究。

Method: 采用范围综述方法，分析2020年后发表的30项研究，识别GAI应用领域、设计方法及评估策略，并关注参与式设计与社会伦理问题。

Result: 发现GAI在故事创作、协作学习等六大领域促进互动；参与式设计提升技术有效性，但存在文化偏见与可访问性等伦理挑战。

Conclusion: GAI支持动态个性化互动，但需加强公平设计实践与包容性评估策略，以平衡技术潜力与社会伦理风险。

Abstract: Reduced social connectedness increasingly poses a threat to mental health,
life expectancy, and general well-being. Generative AI (GAI) technologies, such
as large language models (LLMs) and image generation tools, are increasingly
integrated into applications aimed at enhancing human social experiences.
Despite their growing presence, little is known about how these technologies
influence social interactions. This scoping review investigates how GAI-based
applications are currently designed to facilitate social interaction, what
forms of social engagement they target, and which design and evaluation
methodologies designers use to create and evaluate them. Through an analysis of
30 studies published since 2020, we identify key trends in application domains
including storytelling, socio-emotional skills training, reminiscence,
collaborative learning, music making, and general conversation. We highlight
the role of participatory and co-design approaches in fostering both effective
technology use and social engagement, while also examining socio-ethical
concerns such as cultural bias and accessibility. This review underscores the
potential of GAI to support dynamic and personalized interactions, but calls
for greater attention to equitable design practices and inclusive evaluation
strategies.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [181] [Learning Chaotic Dynamics with Neuromorphic Network Dynamics](https://arxiv.org/abs/2506.10773)
*Yinhao Xu,Georg A. Gottwald,Zdenka Kuncic*

Main category: cond-mat.dis-nn

TL;DR: 研究探讨如何通过调整输入电极和电压优化基于忆阻元件的神经形态网络，以学习复杂动态系统，发现最大化忆阻动态范围可提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用神经形态网络（本身为动态系统）的物理特性进行复杂动态系统的建模与计算，特别是通过外部控制参数优化其学习能力。

Method: 构建基于忆阻元件的复杂电路模拟神经形态网络，采用储备池计算框架对多元混沌时间序列进行自主预测，并通过调控输入电极参数优化非线性动态响应。

Result: 当输入电压使更多忆阻组件充分探索其动态范围时，系统非线性响应最佳；增加电极覆盖会抑制不利于学习的其他非线性行为。

Conclusion: 通过外部参数（如输入电压和电极配置）优化神经形态网络的物理特性，可有效提升其对复杂动态系统的学习能力，为实际设备设计提供指导。

Abstract: This study investigates how dynamical systems may be learned and modelled
with a neuromorphic network which is itself a dynamical system. The
neuromorphic network used in this study is based on a complex electrical
circuit comprised of memristive elements that produce neuro-synaptic nonlinear
responses to input electrical signals. To determine how computation may be
performed using the physics of the underlying system, the neuromorphic network
was simulated and evaluated on autonomous prediction of a multivariate chaotic
time series, implemented with a reservoir computing framework. Through
manipulating only input electrodes and voltages, optimal nonlinear dynamical
responses were found when input voltages maximise the number of memristive
components whose internal dynamics explore the entire dynamical range of the
memristor model. Increasing the network coverage with the input electrodes was
found to suppress other nonlinear responses that are less conducive to
learning. These results provide valuable insights into how a practical
neuromorphic network device can be optimised for learning complex dynamical
systems using only external control parameters.

</details>


### [182] [Learning Chaotic Dynamics with Neuromorphic Network Dynamics](https://arxiv.org/abs/2506.10773)
*Yinhao Xu,Georg A. Gottwald,Zdenka Kuncic*

Main category: cond-mat.dis-nn

TL;DR: 研究探讨如何通过调整输入电极和电压优化基于忆阻元件的神经形态网络，以学习复杂动态系统，发现最大化忆阻动态范围可提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用神经形态网络（本身为动态系统）的物理特性进行复杂动态系统的建模与计算，特别是通过外部控制参数优化其学习能力。

Method: 构建基于忆阻元件的复杂电路模拟神经形态网络，采用储备池计算框架对多元混沌时间序列进行自主预测，并通过调控输入电极参数优化非线性动态响应。

Result: 当输入电压使更多忆阻组件充分探索其动态范围时，系统非线性响应最佳；增加电极覆盖会抑制不利于学习的其他非线性行为。

Conclusion: 通过外部参数（如输入电压和电极配置）优化神经形态网络的物理特性，可有效提升其对复杂动态系统的学习能力，为实际设备设计提供指导。

Abstract: This study investigates how dynamical systems may be learned and modelled
with a neuromorphic network which is itself a dynamical system. The
neuromorphic network used in this study is based on a complex electrical
circuit comprised of memristive elements that produce neuro-synaptic nonlinear
responses to input electrical signals. To determine how computation may be
performed using the physics of the underlying system, the neuromorphic network
was simulated and evaluated on autonomous prediction of a multivariate chaotic
time series, implemented with a reservoir computing framework. Through
manipulating only input electrodes and voltages, optimal nonlinear dynamical
responses were found when input voltages maximise the number of memristive
components whose internal dynamics explore the entire dynamical range of the
memristor model. Increasing the network coverage with the input electrodes was
found to suppress other nonlinear responses that are less conducive to
learning. These results provide valuable insights into how a practical
neuromorphic network device can be optimised for learning complex dynamical
systems using only external control parameters.

</details>


### [183] [On the role of non-linear latent features in bipartite generative neural networks](https://arxiv.org/abs/2506.10552)
*Tony Bonnaire,Giovanni Catania,Aurélien Decelle,Beatriz Seoane*

Main category: cond-mat.dis-nn

TL;DR: 本文研究了不同隐藏单元先验分布（如二值、多态和类ReLU激活）对限制玻尔兹曼机（RBM）相图与记忆检索能力的影响，发现标准二值隐藏单元RBM的临界容量较低，并提出改进方法以提升其联想记忆性能。


<details>
  <summary>Details</summary>
Motivation: 探索RBM作为联想记忆模型的局限性，特别是隐藏单元先验分布对模型热力学性质与记忆检索能力的影响，旨在通过架构设计优化其性能。

Method: 结合Hopfield模型理论与统计物理学的无序系统分析工具，通过理论推导与有限尺寸蒙特卡洛模拟，研究不同隐藏单元激活函数和局部偏置对RBM相变与记忆容量的作用机制。

Result: 标准二值隐藏单元RBM因广泛连接导致临界容量受限；引入局部偏置和更丰富的隐藏单元先验（如多态或类ReLU激活）可恢复有序检索相并显著提升有限温度下的记忆召回性能。

Conclusion: 隐藏单元先验分布的设计是增强RBM表达能力的关键，通过调整激活函数与局部偏置可有效优化其作为联想记忆模型的相图与检索能力。

Abstract: We investigate the phase diagram and memory retrieval capabilities of
bipartite energy-based neural networks, namely Restricted Boltzmann Machines
(RBMs), as a function of the prior distribution imposed on their hidden units -
including binary, multi-state, and ReLU-like activations. Drawing connections
to the Hopfield model and employing analytical tools from statistical physics
of disordered systems, we explore how the architectural choices and activation
functions shape the thermodynamic properties of these models. Our analysis
reveals that standard RBMs with binary hidden nodes and extensive connectivity
suffer from reduced critical capacity, limiting their effectiveness as
associative memories. To address this, we examine several modifications, such
as introducing local biases and adopting richer hidden unit priors. These
adjustments restore ordered retrieval phases and markedly improve recall
performance, even at finite temperatures. Our theoretical findings, supported
by finite-size Monte Carlo simulations, highlight the importance of hidden unit
design in enhancing the expressive power of RBMs.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [184] [Equitable Mechanism Design for Facility Location](https://arxiv.org/abs/2506.10460)
*Toby Walsh*

Main category: cs.GT

TL;DR: 研究策略证明机制在设施选址中如何通过补充基尼指数和纳什福利近似实现公平性，发现传统基尼指数无法被策略证明机制有效优化。


<details>
  <summary>Details</summary>
Motivation: 现有文献中，策略证明机制在优化设施选址的基尼指数（衡量公平性）时存在理论限制，需探索替代指标（如补充基尼指数和纳什福利）以突破不可能性结果。

Method: 提出分析补充基尼指数的近似比，评估确定性与随机机制的表现，并研究纳什福利作为公平与效率折中方案的近似效果。

Result: 证明策略证明机制无法逼近传统基尼指数的最优解，但补充基尼指数可被近似；同时揭示了不同机制在纳什福利上的近似能力差异。

Conclusion: 通过转向补充基尼指数和纳什福利，克服了传统基尼指数的不可逼近性，为策略证明机制设计提供了新的公平性优化路径。

Abstract: We consider strategy proof mechanisms for facility location which maximize
equitability between agents. As is common in the literature, we measure
equitability with the Gini index. We first prove a simple but fundamental
impossibility result that no strategy proof mechanism can bound the
approximation ratio of the optimal Gini index of utilities for one or more
facilities. We propose instead computing approximation ratios of the
complemented Gini index of utilities, and consider how well both deterministic
and randomized mechanisms approximate this. In addition, as Nash welfare is
often put forwards as an equitable compromise between egalitarian and
utilitarian outcomes, we consider how well mechanisms approximate the Nash
welfare.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [185] [FASCIST-O-METER: Classifier for Neo-fascist Discourse Online](https://arxiv.org/abs/2506.10789)
*Rudy Alexandro Garrido Veliz,Martin Semmann,Chris Biemann,Seid Muhie Yimam*

Main category: cs.CY

TL;DR: 本研究开发了首个针对美国社会背景的新法西斯主义数字话语编码方案，通过收集并标注极端群体论坛数据，训练了首个新法西斯主义内容分类模型，强调社会背景在NLP研究中的重要性及持续对抗该意识形态的必要性。


<details>
  <summary>Details</summary>
Motivation: 新法西斯主义近十年在美国及西方社会迅速蔓延，严重威胁民主及少数群体。研究旨在填补自然语言处理（NLP）与政治学在此领域的空白，推动技术手段对抗此类意识形态。

Method: 1. 创建首个新法西斯主义数字话语编码方案；2. 收集Iron March和Stormfront.org论坛数据，通过众包标注1000条帖子；3. 微调小型与大型语言模型（SLMs/LLMs）进行分类。

Result: 成功开发首个新法西斯主义话语分类模型，验证编码方案有效性。分析显示此类论坛中新法西斯言论普遍存在，且社会背景是NLP研究中的关键因素。

Conclusion: 对抗新法西斯主义需持续技术与社会行动，维护民主社会。研究强调结合政治学与NLP方法的重要性，并呼吁进一步针对极端内容平台展开分析。

Abstract: Neo-fascism is a political and societal ideology that has been having
remarkable growth in the last decade in the United States of America (USA), as
well as in other Western societies. It poses a grave danger to democracy and
the minorities it targets, and it requires active actions against it to avoid
escalation. This work presents the first-of-its-kind neo-fascist coding scheme
for digital discourse in the USA societal context, overseen by political
science researchers. Our work bridges the gap between Natural Language
Processing (NLP) and political science against this phenomena. Furthermore, to
test the coding scheme, we collect a tremendous amount of activity on the
internet from notable neo-fascist groups (the forums of Iron March and
Stormfront.org), and the guidelines are applied to a subset of the collected
posts. Through crowdsourcing, we annotate a total of a thousand posts that are
labeled as neo-fascist or non-neo-fascist. With this labeled data set, we
fine-tune and test both Small Language Models (SLMs) and Large Language Models
(LLMs), obtaining the very first classification models for neo-fascist
discourse. We find that the prevalence of neo-fascist rhetoric in this kind of
forum is ever-present, making them a good target for future research. The
societal context is a key consideration for neo-fascist speech when conducting
NLP research. Finally, the work against this kind of political movement must be
pressed upon and continued for the well-being of a democratic society.
Disclaimer: This study focuses on detecting neo-fascist content in text,
similar to other hate speech analyses, without labeling individuals or
organizations.

</details>


### [186] [LLM-Driven Personalized Answer Generation and Evaluation](https://arxiv.org/abs/2506.10829)
*Mohammadreza Molavi,Mohammadreza Tavakoli,Mohammad Moein,Abdolali Faraji,Gábor Kismihók*

Main category: cs.CY

TL;DR: 研究探讨利用大语言模型（LLMs）生成个性化答案以提升在线学习效果，发现通过提供示例可显著增强模型响应适配性。


<details>
  <summary>Details</summary>
Motivation: 在线学习需个性化支持以提升学习体验，但定制化答案生成对教育者负担较重。LLMs具备潜力自动化此过程，需验证其有效性。

Method: 基于StackExchange构建数据集，采用0-shot、1-shot、few-shot策略生成答案，结合BERTScore、LLM评估和人工评估验证效果。

Result: 提供学习者或相似学习者的示例能显著提高LLMs生成答案的个性化程度，且少样本学习策略表现更优。

Conclusion: LLMs在个性化教育应答中具有实用价值，结合历史示例可有效提升模型性能，为在线学习自动化提供可行方案。

Abstract: Online learning has experienced rapid growth due to its flexibility and
accessibility. Personalization, adapted to the needs of individual learners, is
crucial for enhancing the learning experience, particularly in online settings.
A key aspect of personalization is providing learners with answers customized
to their specific questions. This paper therefore explores the potential of
Large Language Models (LLMs) to generate personalized answers to learners'
questions, thereby enhancing engagement and reducing the workload on educators.
To evaluate the effectiveness of LLMs in this context, we conducted a
comprehensive study using the StackExchange platform in two distinct areas:
language learning and programming. We developed a framework and a dataset for
validating automatically generated personalized answers. Subsequently, we
generated personalized answers using different strategies, including 0-shot,
1-shot, and few-shot scenarios. The generated answers were evaluated using
three methods: 1. BERTScore, 2. LLM evaluation, and 3. human evaluation. Our
findings indicated that providing LLMs with examples of desired answers (from
the learner or similar learners) can significantly enhance the LLMs' ability to
tailor responses to individual learners' needs.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [187] [Technical Report with Proofs for A Full Picture in Conformance Checking: Efficiently Summarizing All Optimal Alignments](https://arxiv.org/abs/2506.10345)
*Philipp Bär,Moe T. Wynn,Sander J. J. Leemans*

Main category: cs.IT

TL;DR: 该技术报告为原论文中关于高效总结所有最优对齐的合规性检查方法提供了详细证明。


<details>
  <summary>Details</summary>
Motivation: 现有合规性检查方法在全面且高效总结所有最优对齐方面存在不足，需通过理论证明验证方法的正确性与效率。

Method: 通过形式化数学证明与算法分析，验证原论文提出的方法能够有效生成并汇总所有最优对齐。

Result: 证明结果表明，所提方法在计算复杂性和覆盖范围上均优于传统技术，确保所有最优对齐的完整捕捉。

Conclusion: 技术报告强化了原论文方法的理论可信度，为实际应用中的合规性检查提供了可靠基础。

Abstract: This technical report provides proofs for the claims in the paper "A Full
Picture in Conformance Checking: Efficiently Summarizing All Optimal
Alignments".

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [188] [Semi-Tensor-Product Based Convolutional Neural Networks](https://arxiv.org/abs/2506.10407)
*Daizhan Cheng*

Main category: eess.SY

TL;DR: 本文提出了一种基于域的卷积积（CP）方法，结合半张量积（STP）构建新型卷积神经网络（CNN），避免了传统卷积中填充导致的无效信息问题，并成功应用于图像和三阶信号识别任务。


<details>
  <summary>Details</summary>
Motivation: 传统卷积操作中因填充（padding）产生的无效信息可能影响模型性能，本文旨在通过结合STP与新型CP方法消除这一限制，同时处理不同维度数据。

Method: 提出域基卷积积（CP），将其与半张量积（STP）结合构建无填充的卷积操作，并基于此开发STP-CNN模型。

Result: 所提出的STP-CNN在图像和三阶信号识别任务中有效避免了填充引入的干扰信息，验证了方法的可行性。

Conclusion: 结合域基CP与STP的新型卷积方法能够在不使用填充的情况下处理多维数据，为信号与图像识别提供了更高效的解决方案。

Abstract: The semi-tensor product (STP) of vectors is a generalization of conventional
inner product of vectors, which allows the factor vectors to of different
dimensions. This paper proposes a domain-based convolutional product (CP).
Combining domain-based CP with STP of vectors, a new CP is proposed. Since
there is no zero or any other padding, it can avoid the junk information caused
by padding. Using it, the STP-based convolutional neural network (CNN) is
developed. Its application to image and third order signal identifications is
considered.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [189] [An Analysis of Datasets, Metrics and Models in Keyphrase Generation](https://arxiv.org/abs/2506.10346)
*Florian Boudin,Akiko Aizawa*

Main category: cs.IR

TL;DR: 本文综述了50多篇关键词生成研究，揭示当前评估方法的问题（如数据集相似性、指标不一致导致性能高估），并发布了一个基于预训练语言模型的强基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前关键词生成领域缺乏系统性综述，导致研究现状不明确。本文旨在填补这一空白，分析研究进展、局限性与开放挑战。

Method: 通过系统性分析50+篇相关论文，结合发布基于PLM的模型（如BERT等预训练架构），提供跨模型架构、数据集和应用场景的多维度综述。

Result: 发现评估实践中存在关键问题：常用数据集高度相似（导致泛化性存疑）、指标计算不一致（如重复计数处理差异使性能被高估达30%）。同时开源了性能优越的基准模型。

Conclusion: 当前关键词生成研究需建立更严格的评估标准以反映真实性能，数据多样性不足制约发展。发布的预训练模型为后续研究提供可靠基线，推动领域进步。

Abstract: Keyphrase generation refers to the task of producing a set of words or
phrases that summarises the content of a document. Continuous efforts have been
dedicated to this task over the past few years, spreading across multiple lines
of research, such as model architectures, data resources, and use-case
scenarios. Yet, the current state of keyphrase generation remains unknown as
there has been no attempt to review and analyse previous work. In this paper,
we bridge this gap by presenting an analysis of over 50 research papers on
keyphrase generation, offering a comprehensive overview of recent progress,
limitations, and open challenges. Our findings highlight several critical
issues in current evaluation practices, such as the concerning similarity among
commonly-used benchmark datasets and inconsistencies in metric calculations
leading to overestimated performances. Additionally, we address the limited
availability of pre-trained models by releasing a strong PLM-based model for
keyphrase generation as an effort to facilitate future research.

</details>


### [190] [Conversational Search: From Fundamentals to Frontiers in the LLM Era](https://arxiv.org/abs/2506.10635)
*Fengran Mo,Chuan Meng,Mohammad Aliannejadi,Jian-Yun Nie*

Main category: cs.IR

TL;DR: 本教程探讨大语言模型（LLM）如何革新对话式搜索系统，结合基础理论与新兴技术，为构建下一代智能对话搜索提供理论与应用指导。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在指令遵循、内容生成和推理方面的能力为对话式搜索系统带来新机遇，需系统化整合其优势以应对复杂信息需求。

Method: 通过连接对话搜索的基础理论与LLM驱动的创新主题，分析LLM在意图理解、上下文交互及生成式响应中的应用。

Result: 明确了LLM在提升对话搜索系统智能性、灵活性和交互性中的关键作用，并梳理了相关技术挑战与解决方案。

Conclusion: LLM为对话式搜索系统开辟新路径，掌握其核心原理与前沿进展是开发下一代系统的关键。

Abstract: Conversational search enables multi-turn interactions between users and
systems to fulfill users' complex information needs. During this interaction,
the system should understand the users' search intent within the conversational
context and then return the relevant information through a flexible,
dialogue-based interface. The recent powerful large language models (LLMs) with
capacities of instruction following, content generation, and reasoning, attract
significant attention and advancements, providing new opportunities and
challenges for building up intelligent conversational search systems. This
tutorial aims to introduce the connection between fundamentals and the emerging
topics revolutionized by LLMs in the context of conversational search. It is
designed for students, researchers, and practitioners from both academia and
industry. Participants will gain a comprehensive understanding of both the core
principles and cutting-edge developments driven by LLMs in conversational
search, equipping them with the knowledge needed to contribute to the
development of next-generation conversational search systems.

</details>


### [191] [Towards Understanding Bias in Synthetic Data for Evaluation](https://arxiv.org/abs/2506.10301)
*Hossein A. Rahmani,Varsha Ramineni,Nick Craswell,Bhaskar Mitra,Emine Yilmaz*

Main category: cs.IR

TL;DR: 本文探讨了使用大语言模型（LLM）生成合成测试集在信息检索系统评估中的可靠性，发现合成数据在绝对性能评估中存在显著偏差，但对相对性能比较影响较小。


<details>
  <summary>Details</summary>
Motivation: 传统信息检索系统评估中，构建多样化查询和获取相关性标注成本高昂。尽管已有研究利用LLM生成合成数据改进排序模型，但合成测试集的可靠性及潜在偏差尚未充分验证。

Method: 通过实证分析验证合成测试集（由LLM生成查询、标签或两者）的评估偏差，并采用线性混合效应模型进一步量化偏差影响。

Result: 合成测试集在评估系统绝对性能时存在显著偏差，但在比较系统相对性能时偏差影响较小。

Conclusion: 合成测试集可用于系统相对性能比较，但其在绝对性能评估中的偏差需谨慎对待，未来需进一步优化以减少偏差影响。

Abstract: Test collections are crucial for evaluating Information Retrieval (IR)
systems. Creating a diverse set of user queries for these collections can be
challenging, and obtaining relevance judgments, which indicate how well
retrieved documents match a query, is often costly and resource-intensive.
Recently, generating synthetic datasets using Large Language Models (LLMs) has
gained attention in various applications. While previous work has used LLMs to
generate synthetic queries or documents to improve ranking models, using LLMs
to create synthetic test collections is still relatively unexplored. Previous
work~\cite{rahmani2024synthetic} showed that synthetic test collections have
the potential to be used for system evaluation, however, more analysis is
needed to validate this claim. In this paper, we thoroughly investigate the
reliability of synthetic test collections constructed using LLMs, where LLMs
are used to generate synthetic queries, labels, or both. In particular, we
examine the potential biases that might occur when such test collections are
used for evaluation. We first empirically show the presence of such bias in
evaluation results and analyse the effects it might have on system evaluation.
We further validate the presence of such bias using a linear mixed-effects
model. Our analysis shows that while the effect of bias present in evaluation
results obtained using synthetic test collections could be significant, for
e.g.~computing absolute system performance, its effect may not be as
significant in comparing relative system performance. Codes and data are
available at: https://github.com/rahmanidashti/BiasSyntheticData.

</details>


### [192] [Contrastive Matrix Completion with Denoising and Augmented Graph Views for Robust Recommendation](https://arxiv.org/abs/2506.10658)
*Narges Nemati,Mostafa Haghir Chehreghani*

Main category: cs.IR

TL;DR: 本文提出基于对比学习的矩阵补全方法MCCL，通过结合注意力机制与变分自编码器生成两种图表示，利用互学习损失协调二者，显著提升推荐系统的泛化能力与预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络的矩阵补全方法对噪声边敏感，且消息传递机制易导致过拟合，限制了模型的泛化性能。

Method: 提取用户-物品交互的局部子图，分别通过注意力增强的GNN层和变分自编码器生成去噪表示与分布对齐表示，并设计互学习损失函数逐步融合两种表示。

Result: 在多个真实数据集上，预测评分的RMSE提升最高达0.8%，推荐排名指标提升最高达36%。

Conclusion: MCCL通过对比学习有效捕获共性模式，在提升数值预测精度的同时生成更优推荐排序，验证了方法在泛化性与鲁棒性上的优势。

Abstract: Matrix completion is a widely adopted framework in recommender systems, as
predicting the missing entries in the user-item rating matrix enables a
comprehensive understanding of user preferences. However, current graph neural
network (GNN)-based approaches are highly sensitive to noisy or irrelevant
edges--due to their inherent message-passing mechanisms--and are prone to
overfitting, which limits their generalizability. To overcome these challenges,
we propose a novel method called Matrix Completion using Contrastive Learning
(MCCL). Our approach begins by extracting local neighborhood subgraphs for each
interaction and subsequently generates two distinct graph representations. The
first representation emphasizes denoising by integrating GNN layers with an
attention mechanism, while the second is obtained via a graph variational
autoencoder that aligns the feature distribution with a standard prior. A
mutual learning loss function is employed during training to gradually
harmonize these representations, enabling the model to capture common patterns
and significantly enhance its generalizability. Extensive experiments on
several real-world datasets demonstrate that our approach not only improves the
numerical accuracy of the predicted scores--achieving up to a 0.8% improvement
in RMSE--but also produces superior rankings with improvements of up to 36% in
ranking metrics.

</details>


### [193] [LightKG: Efficient Knowledge-Aware Recommendations with Simplified GNN Architecture](https://arxiv.org/abs/2506.10347)
*Yanhui Li,Dongxia Wang,Zhu Sun,Haonan Zhang,Huizhong Guo*

Main category: cs.IR

TL;DR: 本文提出LightKG，一种基于GNN的轻量级知识图谱推荐系统，通过简化GNN结构和高效对比学习层，在稀疏和密集场景下均优于现有方法，并大幅减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的知识图谱推荐系统（KGRS）在稀疏交互场景下性能不足，且复杂机制（如注意力）可能加剧学习难度。自监督学习（SSL）虽被引入但显著增加训练时间。

Method: LightKG采用简化的GNN层（将关系编码为标量对而非稠密向量）和线性聚合框架，降低模型复杂度；提出直接最小化原始图节点相似性的高效对比层，避免传统SSL中子图生成与比较。

Result: 在四个基准数据集上，LightKG平均推荐精度比最佳基线高5.8%，训练时间比含SSL的KGRS减少84.3%，且在稀疏和密集场景下均表现最优。

Conclusion: LightKG通过结构简化和高效对比学习，有效解决了GNN-based KGRS在稀疏场景下的性能瓶颈，同时显著提升训练效率，验证了简单模型在稀疏数据中的潜力。

Abstract: Recently, Graph Neural Networks (GNNs) have become the dominant approach for
Knowledge Graph-aware Recommender Systems (KGRSs) due to their proven
effectiveness. Building upon GNN-based KGRSs, Self-Supervised Learning (SSL)
has been incorporated to address the sparity issue, leading to longer training
time. However, through extensive experiments, we reveal that: (1)compared to
other KGRSs, the existing GNN-based KGRSs fail to keep their superior
performance under sparse interactions even with SSL. (2) More complex models
tend to perform worse in sparse interaction scenarios and complex mechanisms,
like attention mechanism, can be detrimental as they often increase learning
difficulty. Inspired by these findings, we propose LightKG, a simple yet
powerful GNN-based KGRS to address sparsity issues. LightKG includes a
simplified GNN layer that encodes directed relations as scalar pairs rather
than dense embeddings and employs a linear aggregation framework, greatly
reducing the complexity of GNNs. Additionally, LightKG incorporates an
efficient contrastive layer to implement SSL. It directly minimizes the node
similarity in original graph, avoiding the time-consuming subgraph generation
and comparison required in previous SSL methods. Experiments on four benchmark
datasets show that LightKG outperforms 12 competitive KGRSs in both sparse and
dense scenarios while significantly reducing training time. Specifically, it
surpasses the best baselines by an average of 5.8\% in recommendation accuracy
and saves 84.3\% of training time compared to KGRSs with SSL. Our code is
available at https://github.com/1371149/LightKG.

</details>


### [194] [SHORE: A Long-term User Lifetime Value Prediction Model in Digital Games](https://arxiv.org/abs/2506.10487)
*Shuaiqi Sun,Congde Yuan,Haoqiang Yang,Mengzhuo Guo,Guiying Wei,Jiangbo Tian*

Main category: cs.IR

TL;DR: 提出SHORE框架，通过结合短期预测辅助任务及混合损失函数，显著提升数字游戏用户长期生命周期价值（LTV）预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有LTV预测模型因依赖短期观测或强分布假设，存在长期价值低估和鲁棒性差的问题，需解决延迟支付、早期数据稀疏和高价值离群点等挑战。

Method: SHORE框架整合短期LTV（如LTV-15/30）作为辅助任务，采用保序多分类与动态Huber损失的混合损失函数，缓解零膨胀和离群值影响。

Result: 离线与在线实验显示，SHORE预测误差相对降低47.91%，显著优于基线模型，验证其在工业级场景的鲁棒性。

Conclusion: SHORE通过多任务学习与鲁棒损失设计，有效提升长期LTV预测性能，为游戏商业化策略提供可靠工具。

Abstract: In digital gaming, long-term user lifetime value (LTV) prediction is
essential for monetization strategy, yet presents major challenges due to
delayed payment behavior, sparse early user data, and the presence of
high-value outliers. While existing models typically rely on either short-cycle
observations or strong distributional assumptions, such approaches often
underestimate long-term value or suffer from poor robustness. To address these
issues, we propose SHort-cycle auxiliary with Order-preserving REgression
(SHORE), a novel LTV prediction framework that integrates short-horizon
predictions (e.g., LTV-15 and LTV-30) as auxiliary tasks to enhance long-cycle
targets (e.g., LTV-60). SHORE also introduces a hybrid loss function combining
order-preserving multi-class classification and a dynamic Huber loss to
mitigate the influence of zero-inflation and outlier payment behavior.
Extensive offline and online experiments on real-world datasets demonstrate
that SHORE significantly outperforms existing baselines, achieving a 47.91\%
relative reduction in prediction error in online deployment. These results
highlight SHORE's practical effectiveness and robustness in industrial-scale
LTV prediction for digital games.

</details>


### [195] [Macro Graph of Experts for Billion-Scale Multi-Task Recommendation](https://arxiv.org/abs/2506.10520)
*Hongyu Yao,Zijin Hong,Hao Chen,Yuanchen Bei,Zhiqing Li,Qijie Shen,Zuobin Ying,Huan Gong,Feiran Huang*

Main category: cs.IR

TL;DR: 本文提出Macro Graph of Expert (MGOE)框架，首次通过宏图嵌入捕捉十亿级推荐系统中多任务学习的任务特定特征与专家关联，突破传统方法忽略图结构的局限。


<details>
  <summary>Details</summary>
Motivation: 传统多任务学习方法忽视图结构信息，导致性能提升潜力未被挖掘。针对十亿级推荐系统中不同任务对应独立大规模图的特点，需有效利用图结构提升模型表现。

Method: 提出MGOE框架：1) 设计Macro Graph Bottom首次实现图信息融合；2) 构建Macro Prediction Tower动态整合跨任务宏知识，建模任务间专家相关性。

Result: 在三个公开基准数据集上离线实验超越SOTA方法，十亿级推荐系统在线A/B测试验证其优越性，成功应用于头部推荐系统首页场景。

Conclusion: MGOE通过显式建模任务特定宏图特征与专家关联，为基于图结构的十亿级多任务推荐系统提供了突破性解决方案，实验与落地应用均证明其有效性。

Abstract: Graph-based multi-task learning at billion-scale presents a significant
challenge, as different tasks correspond to distinct billion-scale graphs.
Traditional multi-task learning methods often neglect these graph structures,
relying solely on individual user and item embeddings. However, disregarding
graph structures overlooks substantial potential for improving performance. In
this paper, we introduce the Macro Graph of Expert (MGOE) framework, the first
approach capable of leveraging macro graph embeddings to capture task-specific
macro features while modeling the correlations between task-specific experts.
Specifically, we propose the concept of a Macro Graph Bottom, which, for the
first time, enables multi-task learning models to incorporate graph information
effectively. We design the Macro Prediction Tower to dynamically integrate
macro knowledge across tasks. MGOE has been deployed at scale, powering
multi-task learning for the homepage of a leading billion-scale recommender
system. Extensive offline experiments conducted on three public benchmark
datasets demonstrate its superiority over state-of-the-art multi-task learning
methods, establishing MGOE as a breakthrough in multi-task graph-based
recommendation. Furthermore, online A/B tests confirm the superiority of MGOE
in billion-scale recommender systems.

</details>


### [196] [Precise Zero-Shot Pointwise Ranking with LLMs through Post-Aggregated Global Context Information](https://arxiv.org/abs/2506.10859)
*Kehan Long,Shasha Li,Chen Xu,Jintao Tang,Ting Wang*

Main category: cs.IR

TL;DR: 本文提出一种结合全局一致性对比的点式排序方法（GCCP）及后聚合策略（PAGC），在保持点式方法高效性的同时，通过引入全局对比信息显著提升文档排序效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的点式文档排序方法虽高效，但因独立评分忽略文档间对比信息，导致评分不一致和性能不足；而对比方法（如列表/配对排序）计算成本过高，难以平衡效果与效率。

Method: 1. 提出GCCP策略：通过将候选文档与全局锚点（基于伪相关文档生成的查询聚焦摘要）对比生成一致性评分；2. 设计PAGC方法，将对比评分与现有点式方法高效结合，无需训练即可融入全局上下文。

Result: 在TREC DL和BEIR基准测试中，方法显著优于传统点式方法，且效率相当；与高计算成本的对比方法相比，性能接近甚至更具竞争力。锚点构建策略的有效性通过实验进一步验证。

Conclusion: 通过全局对比锚点和后聚合机制，成功提升点式排序的效果与一致性，在效率与性能间实现更优平衡，为大规模文档排序提供实用解决方案。

Abstract: Recent advancements have successfully harnessed the power of Large Language
Models (LLMs) for zero-shot document ranking, exploring a variety of prompting
strategies. Comparative approaches like pairwise and listwise achieve high
effectiveness but are computationally intensive and thus less practical for
larger-scale applications. Scoring-based pointwise approaches exhibit superior
efficiency by independently and simultaneously generating the relevance scores
for each candidate document. However, this independence ignores critical
comparative insights between documents, resulting in inconsistent scoring and
suboptimal performance. In this paper, we aim to improve the effectiveness of
pointwise methods while preserving their efficiency through two key
innovations: (1) We propose a novel Global-Consistent Comparative Pointwise
Ranking (GCCP) strategy that incorporates global reference comparisons between
each candidate and an anchor document to generate contrastive relevance scores.
We strategically design the anchor document as a query-focused summary of
pseudo-relevant candidates, which serves as an effective reference point by
capturing the global context for document comparison. (2) These contrastive
relevance scores can be efficiently Post-Aggregated with existing pointwise
methods, seamlessly integrating essential Global Context information in a
training-free manner (PAGC). Extensive experiments on the TREC DL and BEIR
benchmark demonstrate that our approach significantly outperforms previous
pointwise methods while maintaining comparable efficiency. Our method also
achieves competitive performance against comparative methods that require
substantially more computational resources. More analyses further validate the
efficacy of our anchor construction strategy.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [197] [Patient-Specific Deep Reinforcement Learning for Automatic Replanning in Head-and-Neck Cancer Proton Therapy](https://arxiv.org/abs/2506.10073)
*Malvern Madondo,Yuan Shao,Yingzi Liu,Jun Zhou,Xiaofeng Yang,Zhen Tian*

Main category: physics.med-ph

TL;DR: 提出基于患者特异性深度强化学习（DRL）的自动化IMPT重新规划框架，通过奖励机制优化治疗计划质量，结果显示其效果优于人工重新规划。


<details>
  <summary>Details</summary>
Motivation: 头颈癌调强质子治疗（IMPT）中，解剖结构变化可能导致剂量分布偏差，传统手动重新规划耗时且资源密集，需开发高效的自适应解决方案。

Method: 采用患者特异性DRL框架，结合计划CT和模拟解剖变化的增强数据，训练个性化智能体调整优化优先级，使用DQN和PPO算法，以剂量体积直方图（DVH）为状态，22维动作空间控制优先级。

Result: 在5例患者实际数据中，DQN和PPO将初始计划评分从120.63提升至139.78和142.74，优于人工重新规划（137.20），临床验证显示肿瘤覆盖和OAR保护均改善。

Conclusion: DRL可有效解决自适应质子治疗的几何与剂量学挑战，为离线自适应提供高效方案，并推动在线自适应质子治疗发展。

Abstract: Anatomical changes during intensity-modulated proton therapy (IMPT) for
head-and-neck cancer (HNC) can shift Bragg peaks, risking tumor underdosing and
organ-at-risk overdosing. As a result, treatment replanning is often required
to maintain clinically acceptable treatment quality. However, current manual
replanning processes are resource-intensive and time-consuming. We propose a
patient-specific deep reinforcement learning (DRL) framework for automated IMPT
replanning, with a reward-shaping mechanism based on a $150$-point plan quality
score addressing competing clinical objectives. We formulate the planning
process as an RL problem where agents learn control policies to adjust
optimization priorities, maximizing plan quality. Unlike population-based
approaches, our framework trains personalized agents for each patient using
their planning CT (Computed Tomography) and augmented anatomies simulating
anatomical changes (tumor progression and regression). This patient-specific
approach leverages anatomical similarities throughout treatment, enabling
effective plan adaptation. We implemented two DRL algorithms, Deep Q-Network
and Proximal Policy Optimization, using dose-volume histograms (DVHs) as state
representations and a $22$-dimensional action space of priority adjustments.
Evaluation on five HNC patients using actual replanning CT data showed both DRL
agents improved initial plan scores from $120.63 \pm 21.40$ to $139.78 \pm
6.84$ (DQN) and $142.74 \pm 5.16$ (PPO), surpassing manual replans generated by
a human planner ($137.20 \pm 5.58$). Clinical validation confirms that
improvements translate to better tumor coverage and OAR sparing across diverse
anatomical changes. This work demonstrates DRL's potential in addressing
geometric and dosimetric complexities of adaptive proton therapy, offering
efficient offline adaptation solutions and advancing online adaptive proton
therapy.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [198] [Multimodal Cinematic Video Synthesis Using Text-to-Image and Audio Generation Models](https://arxiv.org/abs/2506.10005)
*Sridhar S,Nithin A,Shakeel Rifath,Vasantha Raj*

Main category: cs.CV

TL;DR: 该研究提出了一种结合Stable Diffusion、GPT-2和混合音频管道的自动电影视频生成方法，通过五场景框架、帧插值与后期处理，在GPU加速环境中实现60秒专业级文本到视频合成。


<details>
  <summary>Details</summary>
Motivation: 利用生成式AI技术（如文本到视频合成）推动多媒体创作自动化，满足创意、教育及工业领域对高效高质量视频生成的需求。

Method: 整合Stable Diffusion生成高保真图像，GPT-2构建叙事结构，gTTS与YouTube音乐组成混合音频，采用五场景框架、线性帧插值、影视级后处理（锐化等）及音视频同步技术，通过Python 3.11在GPU加速的Google Colab环境中实现双模式Gradio界面。

Result: 实验显示该方法在视觉质量（最高1024x768分辨率）、叙事连贯性（15-30 FPS）和运行可靠性（CUDA内存优化）方面表现优异，支持多场景应用。

Conclusion: 该框架通过模块化AI技术与工程优化，显著推进了文本到视频合成的实用化进程，为跨领域多媒体创作提供了可扩展的解决方案。

Abstract: Advances in generative artificial intelligence have altered multimedia
creation, allowing for automatic cinematic video synthesis from text inputs.
This work describes a method for creating 60-second cinematic movies
incorporating Stable Diffusion for high-fidelity image synthesis, GPT-2 for
narrative structuring, and a hybrid audio pipeline using gTTS and
YouTube-sourced music. It uses a five-scene framework, which is augmented by
linear frame interpolation, cinematic post-processing (e.g., sharpening), and
audio-video synchronization to provide professional-quality results. It was
created in a GPU-accelerated Google Colab environment using Python 3.11. It has
a dual-mode Gradio interface (Simple and Advanced), which supports resolutions
of up to 1024x768 and frame rates of 15-30 FPS. Optimizations such as CUDA
memory management and error handling ensure reliability. The experiments
demonstrate outstanding visual quality, narrative coherence, and efficiency,
furthering text-to-video synthesis for creative, educational, and industrial
applications.

</details>


### [199] [Test-Time Adaptation for Generalizable Task Progress Estimation](https://arxiv.org/abs/2506.10085)
*Christos Ziakas,Alessandra Russo*

Main category: cs.CV

TL;DR: 提出一种测试时自适应方法，通过优化自监督目标，使进度估计模型能在线适应测试轨迹的视觉与时间上下文，在分布外任务中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有进度估计模型在分布外任务、环境和体现中泛化能力有限，且依赖时间顺序信息。需通过语义内容增强模型的自适应能力。

Method: 采用基于梯度的元学习策略，利用专家视觉轨迹和自然语言任务描述进行训练，测试时通过自监督目标优化模型，强化语义内容优先级。

Result: 该方法在多样化分布外场景中表现优于当前最先进的自回归视觉语言模型，验证了语义驱动的自适应有效性。

Conclusion: 测试时自适应结合元学习策略显著提升了进度估计的泛化能力，证明语义内容优先于时间顺序的优化路径具有广泛适用性。

Abstract: We propose a test-time adaptation method that enables a progress estimation
model to adapt online to the visual and temporal context of test trajectories
by optimizing a learned self-supervised objective. To this end, we introduce a
gradient-based meta-learning strategy to train the model on expert visual
trajectories and their natural language task descriptions, such that test-time
adaptation improves progress estimation relying on semantic content over
temporal order. Our test-time adaptation method generalizes from a single
training environment to diverse out-of-distribution tasks, environments, and
embodiments, outperforming the state-of-the-art in-context learning approach
using autoregressive vision-language models.

</details>


### [200] [Detecção da Psoríase Utilizando Visão Computacional: Uma Abordagem Comparativa Entre CNNs e Vision Transformers](https://arxiv.org/abs/2506.10119)
*Natanael Lucena,Fábio S. da Silva,Ricardo Rios*

Main category: cs.CV

TL;DR: 比较CNN与Vision Transformer在银屑病及类似皮肤病图像多分类任务中的表现，发现ViT模型在较小参数量下性能更优，其中DaViT-B以96.4% F1分数成为最优架构。


<details>
  <summary>Details</summary>
Motivation: 探索卷积神经网络(CNN)与视觉变换器(ViT)在医学图像分类任务中的效能差异，特别是针对银屑病这类需高精度诊断的皮肤病分类场景。

Method: 使用ImageNet预训练模型进行迁移学习，在银屑病及相似皮肤病图像数据集上微调，对比CNN与ViT架构的性能表现。

Result: ViT模型在较小模型规模下展现更优性能，其中DaViT-B模型取得96.4% F1分数，显著优于同类CNN模型。

Conclusion: 视觉变换器(ViT)在医学图像分类中具有显著潜力，DaViT-B被验证为银屑病自动化检测的高效解决方案。

Abstract: This paper presents a comparison of the performance of Convolutional Neural
Networks (CNNs) and Vision Transformers (ViTs) in the task of multi-classifying
images containing lesions of psoriasis and diseases similar to it. Models
pre-trained on ImageNet were adapted to a specific data set. Both achieved high
predictive metrics, but the ViTs stood out for their superior performance with
smaller models. Dual Attention Vision Transformer-Base (DaViT-B) obtained the
best results, with an f1-score of 96.4%, and is recommended as the most
efficient architecture for automated psoriasis detection. This article
reinforces the potential of ViTs for medical image classification tasks.

</details>


### [201] [SPARKE: Scalable Prompt-Aware Diversity Guidance in Diffusion Models via RKE Score](https://arxiv.org/abs/2506.10173)
*Mohammad Jalali,Haoyu Lei,Amin Gohari,Farzan Farnia*

Main category: cs.CV

TL;DR: 本文提出SPARKE方法，通过条件熵实现提示感知的多样性引导，将计算复杂度从O(n³)降至O(n)，在保持低计算成本的同时提升生成样本的多样性。


<details>
  <summary>Details</summary>
Motivation: 现有提示引导扩散模型在广泛语义提示下生成样本多样性不足，且传统基于多样性度量的方法在大规模生成场景中面临O(n³)计算复杂度问题。

Method: SPARKE方法采用条件Rényi核熵进行多样性引导，通过条件潜在RKE分数指导将熵计算复杂度优化至线性级别，支持跨数千个生成轮次的多样性控制。

Result: 实验表明SPARKE在多个文本-图像扩散模型中显著提升提示感知多样性，且未显著增加计算开销，代码已开源。

Conclusion: SPARKE通过可扩展的条件熵机制有效平衡了生成多样性与计算效率，为大规模提示引导生成任务提供了实用解决方案。

Abstract: Diffusion models have demonstrated remarkable success in high-fidelity image
synthesis and prompt-guided generative modeling. However, ensuring adequate
diversity in generated samples of prompt-guided diffusion models remains a
challenge, particularly when the prompts span a broad semantic spectrum and the
diversity of generated data needs to be evaluated in a prompt-aware fashion
across semantically similar prompts. Recent methods have introduced guidance
via diversity measures to encourage more varied generations. In this work, we
extend the diversity measure-based approaches by proposing the Scalable
Prompt-Aware R\'eny Kernel Entropy Diversity Guidance (SPARKE) method for
prompt-aware diversity guidance. SPARKE utilizes conditional entropy for
diversity guidance, which dynamically conditions diversity measurement on
similar prompts and enables prompt-aware diversity control. While the
entropy-based guidance approach enhances prompt-aware diversity, its reliance
on the matrix-based entropy scores poses computational challenges in
large-scale generation settings. To address this, we focus on the special case
of Conditional latent RKE Score Guidance, reducing entropy computation and
gradient-based optimization complexity from the $O(n^3)$ of general entropy
measures to $O(n)$. The reduced computational complexity allows for
diversity-guided sampling over potentially thousands of generation rounds on
different prompts. We numerically test the SPARKE method on several
text-to-image diffusion models, demonstrating that the proposed method improves
the prompt-aware diversity of the generated data without incurring significant
computational costs. We release our code on the project page:
https://mjalali.github.io/SPARKE

</details>


### [202] [ScoreMix: Improving Face Recognition via Score Composition in Diffusion Generators](https://arxiv.org/abs/2506.10226)
*Parsa Rahimi,Sebastien Marcel*

Main category: cs.CV

TL;DR: 本文提出ScoreMix，一种基于扩散模型分数组合特性的数据增强方法，通过在采样过程中混合不同类别的分数生成挑战性样本，显著提升小样本场景下的判别模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决标记数据有限时判别模型性能下降的问题，利用生成模型特性增强数据多样性，避免大规模数据收集的负担。

Method: 在扩散采样阶段对多类别条件轨迹的分数进行凸组合，生成困难样本；系统探索类别选择策略，发现判别器嵌入空间远距类别的混合更有效。

Result: 所有基准测试中判别能力显著提升，实验证明生成器条件空间与判别器嵌入空间相关性微弱，且无需大量超参搜索。

Conclusion: ScoreMix通过生成对抗性数据高效提升小样本学习性能，为缓解数据收集难题提供实用方案，揭示了生成-判别空间解耦现象。

Abstract: In this paper, we propose ScoreMix, a novel yet simple data augmentation
strategy leveraging the score compositional properties of diffusion models to
enhance discriminator performance, particularly under scenarios with limited
labeled data. By convexly mixing the scores from different class-conditioned
trajectories during diffusion sampling, we generate challenging synthetic
samples that significantly improve discriminative capabilities in all studied
benchmarks. We systematically investigate class-selection strategies for mixing
and discover that greater performance gains arise when combining classes
distant in the discriminator's embedding space, rather than close in the
generator's condition space. Moreover, we empirically show that, under standard
metrics, the correlation between the generator's learned condition space and
the discriminator's embedding space is minimal. Our approach achieves notable
performance improvements without extensive parameter searches, demonstrating
practical advantages for training discriminative models while effectively
mitigating problems regarding collections of large datasets. Paper website:
https://parsa-ra.github.io/scoremix

</details>


### [203] [Towards Robust Multimodal Emotion Recognition under Missing Modalities and Distribution Shifts](https://arxiv.org/abs/2506.10452)
*Guowei Zhong,Ruohong Huan,Mingzhen Wu,Ronghua Liang,Peng Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新型多模态情感识别框架CIDer，通过自蒸馏和因果推理模块，同时解决模态随机缺失和分布外数据问题，具有参数少、训练快的优势。


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感识别方法难以同时处理模态缺失和分布外数据，且常因模型特定性或参数冗余导致实用性受限。

Method: CIDer框架包含模型特定自蒸馏模块(MSSD)和模型无关因果推理模块(MACI)：MSSD通过权重共享自蒸馏增强特征鲁棒性，MACI利用多模态因果模块和反事实文本消除偏差。提出RMFM任务并重构OOD数据集。

Result: 实验表明CIDer在RMFM和OOD场景下均优于现有方法，参数减少且训练速度提升。代码已开源。

Conclusion: CIDer通过融合自蒸馏与因果干预机制，为多模态情感识别提供了高效鲁棒的解决方案，其模块化设计具有广泛适用性。

Abstract: Recent advancements in Multimodal Emotion Recognition (MER) face challenges
in addressing both modality missing and Out-Of-Distribution (OOD) data
simultaneously. Existing methods often rely on specific models or introduce
excessive parameters, which limits their practicality. To address these issues,
we propose a novel robust MER framework, Causal Inference Distiller (CIDer),
and introduce a new task, Random Modality Feature Missing (RMFM), to generalize
the definition of modality missing. CIDer integrates two key components: a
Model-Specific Self-Distillation (MSSD) module and a Model-Agnostic Causal
Inference (MACI) module. MSSD enhances robustness under the RMFM task through a
weight-sharing self-distillation approach applied across low-level features,
attention maps, and high-level representations. Additionally, a Word-level
Self-aligned Attention Module (WSAM) reduces computational complexity, while a
Multimodal Composite Transformer (MCT) facilitates efficient multimodal fusion.
To tackle OOD challenges, MACI employs a tailored causal graph to mitigate
label and language biases using a Multimodal Causal Module (MCM) and
fine-grained counterfactual texts. Notably, MACI can independently enhance OOD
generalization with minimal additional parameters. Furthermore, we also
introduce the new repartitioned MER OOD datasets. Experimental results
demonstrate that CIDer achieves robust performance in both RMFM and OOD
scenarios, with fewer parameters and faster training compared to
state-of-the-art methods. The implementation of this work is publicly
accessible at https://github.com/gw-zhong/CIDer.

</details>


### [204] [Uncertainty-Aware Deep Learning for Automated Skin Cancer Classification: A Comprehensive Evaluation](https://arxiv.org/abs/2506.10302)
*Hamzeh Asgharnezhad,Pegah Tabarisaadi,Abbas Khosravi,Roohallah Alizadehsani,U. Rajendra Acharya*

Main category: cs.CV

TL;DR: 本研究通过迁移学习和不确定性量化（UQ）在HAM10000数据集上评估深度学习模型对皮肤癌分类的性能。CLIP视觉变换器结合SVM表现最佳，集成方法在准确性与不确定性处理间取得平衡，EMCD对不确定预测更敏感。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在皮肤癌分类中受限于数据稀缺和缺乏不确定性评估，可能影响临床应用的可靠性和早期治疗效果。

Method: 分两阶段：1）对比预训练特征提取器（如CLIP、ResNet50等）与传统分类器（SVM、XGBoost等）的性能；2）引入MCD、集成和EMCD方法进行不确定性量化，并采用UAcc、USen等指标评估。

Result: CLIP ViT-H/14与SVM组合分类性能最高；集成方法在准确性与不确定性处理间平衡较好，而EMCD对不确定预测更敏感。

Conclusion: 将不确定性量化整合至深度学习医疗诊断中，可提升模型在真实临床场景中的性能和可信度，集成方法为较优选择。

Abstract: Accurate and reliable skin cancer diagnosis is critical for early treatment
and improved patient outcomes. Deep learning (DL) models have shown promise in
automating skin cancer classification, but their performance can be limited by
data scarcity and a lack of uncertainty awareness. In this study, we present a
comprehensive evaluation of DL-based skin lesion classification using transfer
learning and uncertainty quantification (UQ) on the HAM10000 dataset. In the
first phase, we benchmarked several pre-trained feature extractors-including
Contrastive Language-Image Pretraining (CLIP) variants, Residual Network-50
(ResNet50), Densely Connected Convolutional Network (DenseNet121), Visual
Geometry Group network (VGG16), and EfficientNet-V2-Large-combined with a range
of traditional classifiers such as Support Vector Machine (SVM), eXtreme
Gradient Boosting (XGBoost), and logistic regression. Our results show that
CLIP-based vision transformers, particularly LAION CLIP ViT-H/14 with SVM,
deliver the highest classification performance. In the second phase, we
incorporated UQ using Monte Carlo Dropout (MCD), Ensemble, and Ensemble Monte
Carlo Dropout (EMCD) to assess not only prediction accuracy but also the
reliability of model outputs. We evaluated these models using uncertainty-aware
metrics such as uncertainty accuracy(UAcc), uncertainty sensitivity(USen),
uncertainty specificity(USpe), and uncertainty precision(UPre). The results
demonstrate that ensemble methods offer a good trade-off between accuracy and
uncertainty handling, while EMCD is more sensitive to uncertain predictions.
This study highlights the importance of integrating UQ into DL-based medical
diagnosis to enhance both performance and trustworthiness in real-world
clinical applications.

</details>


### [205] [VideoDeepResearch: Long Video Understanding With Agentic Tool Using](https://arxiv.org/abs/2506.10821)
*Huaying Yuan,Zheng Liu,Junjie Zhou,Ji-Rong Wen,Zhicheng Dou*

Main category: cs.CV

TL;DR: 本文提出VideoDeepResearch框架，仅依赖纯文本大推理模型结合现成多模态工具包，在长视频理解任务中显著超越现有多模态大模型。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为长上下文窗口、强视觉感知和领域知识是解决长视频理解(LVU)的必要条件，但本文挑战这一假设，探索更轻量化的替代方案。

Method: 通过文本大推理模型(LRM)协调多模态工具包(包含检索器和感知器)，动态制定问题解决策略并选择性提取关键视频内容。

Result: 在MLVU(test)/LVBench/LongVideoBench基准上分别取得9.6%、6.6%、3.9%的SOTA提升，验证了代理系统的有效性。

Conclusion: 基于工具使用的代理系统可突破传统MLLMs在长视频理解中的关键限制，为LVU问题提供新范式。

Abstract: Long video understanding (LVU) presents a significant challenge for current
multi-modal large language models (MLLMs) due to the task's inherent complexity
and context window constraint. It is widely assumed that addressing LVU tasks
requires foundation MLLMs with extended context windows, strong visual
perception capabilities, and proficient domain expertise. In this work, we
challenge this common belief by introducing VideoDeepResearch, a novel agentic
framework for long video understanding. Our approach relies solely on a
text-only large reasoning model (LRM) combined with a modular multi-modal
toolkit, including multimodal retrievers and visual perceivers, all of which
are readily available in practice. For each LVU task, the system formulates a
problem-solving strategy through reasoning, while selectively accessing and
utilizing essential video content via tool using. We conduct extensive
experiments on popular LVU benchmarks, including MLVU, Video-MME, and LVBench.
Our results demonstrate that VideoDeepResearch achieves substantial
improvements over existing MLLM baselines, surpassing the previous
state-of-the-art by 9.6%, 6.6%, and 3.9% on MLVU (test), LVBench, and
LongVideoBench, respectively. These findings highlight the promise of agentic
systems in overcoming key challenges in LVU problems.

</details>


### [206] [Towards Scalable SOAP Note Generation: A Weakly Supervised Multimodal Framework](https://arxiv.org/abs/2506.10328)
*Sadia Kamal,Tim Oates,Joy Wan*

Main category: cs.CV

TL;DR: 提出一种弱监督多模态框架，通过病变图像和稀疏文本自动生成临床SOAP笔记，减少人工标注依赖，性能媲美主流大模型并引入新评估指标。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌治疗中手动撰写SOAP笔记效率低下且加剧医生职业倦怠，需开发自动化工具降低临床文档生成负担。

Method: 基于弱监督学习的多模态框架，整合皮肤病变图像与有限临床文本输入，生成结构化SOAP笔记。

Result: 在临床相关性指标上达到GPT-4o/Claude/DeepSeek水平，提出MedConceptEval和CCS双指标验证临床语义对齐。

Conclusion: 该方法显著降低人工标注需求，实现可扩展的临床文档自动化，为减轻医生工作负荷提供有效解决方案。

Abstract: Skin carcinoma is the most prevalent form of cancer globally, accounting for
over $8 billion in annual healthcare expenditures. In clinical settings,
physicians document patient visits using detailed SOAP (Subjective, Objective,
Assessment, and Plan) notes. However, manually generating these notes is
labor-intensive and contributes to clinician burnout. In this work, we propose
a weakly supervised multimodal framework to generate clinically structured SOAP
notes from limited inputs, including lesion images and sparse clinical text.
Our approach reduces reliance on manual annotations, enabling scalable,
clinically grounded documentation while alleviating clinician burden and
reducing the need for large annotated data. Our method achieves performance
comparable to GPT-4o, Claude, and DeepSeek Janus Pro across key clinical
relevance metrics. To evaluate clinical quality, we introduce two novel metrics
MedConceptEval and Clinical Coherence Score (CCS) which assess semantic
alignment with expert medical concepts and input features, respectively.

</details>


### [207] [Using Vision Language Models to Detect Students' Academic Emotion through Facial Expressions](https://arxiv.org/abs/2506.10334)
*Deliang Wang,Chao Yang,Gaowei Chen*

Main category: cs.CV

TL;DR: 研究探索视觉语言模型（VLMs）在在线学习环境中识别学生学术情绪的能力，发现Qwen模型在困惑和快乐情绪识别上表现较优，但两类模型均无法检测分心行为。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习模型在跨场景情绪识别中存在泛化能力不足的问题，需反复进行数据标注与训练。研究旨在验证无需微调的VLMs通过零样本提示实现跨场景情绪识别的可行性。

Method: 使用Llama-3.2-11B-Vision-Instruct和Qwen2.5-VL-7B-Instruct两个VLMs，通过零样本提示对5,000张包含困惑、分心、快乐、中性及疲惫表情的图像进行学术情绪分析。

Result: Qwen模型整体表现优于Llama，两者对快乐情绪识别效果最佳，但均无法检测分心行为。Qwen在困惑情绪识别中展现出较高应用潜力。

Conclusion: VLMs在学术情绪识别中具有实用价值，尤其是Qwen模型在检测学生困惑情绪方面，但需改进对分心行为的识别能力以提升教育场景适应性。

Abstract: Students' academic emotions significantly influence their social behavior and
learning performance. Traditional approaches to automatically and accurately
analyze these emotions have predominantly relied on supervised machine learning
algorithms. However, these models often struggle to generalize across different
contexts, necessitating repeated cycles of data collection, annotation, and
training. The emergence of Vision-Language Models (VLMs) offers a promising
alternative, enabling generalization across visual recognition tasks through
zero-shot prompting without requiring fine-tuning. This study investigates the
potential of VLMs to analyze students' academic emotions via facial expressions
in an online learning environment. We employed two VLMs,
Llama-3.2-11B-Vision-Instruct and Qwen2.5-VL-7B-Instruct, to analyze 5,000
images depicting confused, distracted, happy, neutral, and tired expressions
using zero-shot prompting. Preliminary results indicate that both models
demonstrate moderate performance in academic facial expression recognition,
with Qwen2.5-VL-7B-Instruct outperforming Llama-3.2-11B-Vision-Instruct.
Notably, both models excel in identifying students' happy emotions but fail to
detect distracted behavior. Additionally, Qwen2.5-VL-7B-Instruct exhibits
relatively high performance in recognizing students' confused expressions,
highlighting its potential for practical applications in identifying content
that causes student confusion.

</details>


### [208] [UrbanSense:AFramework for Quantitative Analysis of Urban Streetscapes leveraging Vision Large Language Models](https://arxiv.org/abs/2506.10342)
*Jun Yin,Jing Zhong,Peilin Li,Pengyu Zeng,Miao Zhang,Ran Luo,Shuai Lu*

Main category: cs.CV

TL;DR: 本研究提出基于视觉语言模型的多模态框架UrbanSense，结合数据集UrbanDiffBench，实现城市街景风格的自动化、可量化分析，验证了其在捕捉风格差异及演变中的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统城市文化研究依赖专家解读和历史文献，难以标准化。需更客观、数据驱动的方法分析城市街景风格差异，以理解城市演变规律。

Method: 构建UrbanDiffBench街景数据集，开发基于视觉语言模型的框架UrbanSense，支持城市风格的量化生成与对比分析。

Result: 超80%生成描述通过t检验（p<0.05），主观评估Phi分数高（城市0.912、时期0.833），证明方法能捕捉细微风格差异。

Conclusion: 该方法为城市风格演变提供量化解释工具，结合数据驱动与客观性，为未来城市设计提供科学依据。

Abstract: Urban cultures and architectural styles vary significantly across cities due
to geographical, chronological, historical, and socio-political factors.
Understanding these differences is essential for anticipating how cities may
evolve in the future. As representative cases of historical continuity and
modern innovation in China, Beijing and Shenzhen offer valuable perspectives
for exploring the transformation of urban streetscapes. However, conventional
approaches to urban cultural studies often rely on expert interpretation and
historical documentation, which are difficult to standardize across different
contexts. To address this, we propose a multimodal research framework based on
vision-language models, enabling automated and scalable analysis of urban
streetscape style differences. This approach enhances the objectivity and
data-driven nature of urban form research. The contributions of this study are
as follows: First, we construct UrbanDiffBench, a curated dataset of urban
streetscapes containing architectural images from different periods and
regions. Second, we develop UrbanSense, the first vision-language-model-based
framework for urban streetscape analysis, enabling the quantitative generation
and comparison of urban style representations. Third, experimental results show
that Over 80% of generated descriptions pass the t-test (p less than 0.05).
High Phi scores (0.912 for cities, 0.833 for periods) from subjective
evaluations confirm the method's ability to capture subtle stylistic
differences. These results highlight the method's potential to quantify and
interpret urban style evolution, offering a scientifically grounded lens for
future design.

</details>


### [209] [VINCIE: Unlocking In-context Image Editing from Video](https://arxiv.org/abs/2506.10941)
*Leigang Qu,Feng Cheng,Ziyan Yang,Qi Zhao,Shanchuan Lin,Yichun Shi,Yicong Li,Wenjie Wang,Tat-Seng Chua,Lu Jiang*

Main category: cs.CV

TL;DR: 本文提出一种直接从视频中学习上下文图像编辑的方法，通过标注视频为多模态序列并设计块因果扩散变换器，在多个任务中实现先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有上下文图像编辑方法依赖特定任务流程和专家模型（如分割、修复）构建训练数据，本文探索是否可直接从视频数据中学习此类模型。

Method: 提出可扩展的视频多模态序列标注方法，设计基于三个代理任务（下一图像预测/当前分割预测/下一分割预测）的块因果扩散变换器，并构建多轮图像编辑基准。

Result: 模型在两项多轮编辑基准上达到SOTA，且在仅用视频训练的情况下，展现出多概念组合、故事生成和编辑链应用潜力。

Conclusion: 直接从视频学习上下文图像编辑的路径可行，所提方法突破了传统专家模型依赖，为多模态序列建模提供了新思路。

Abstract: In-context image editing aims to modify images based on a contextual sequence
comprising text and previously generated images. Existing methods typically
depend on task-specific pipelines and expert models (e.g., segmentation and
inpainting) to curate training data. In this work, we explore whether an
in-context image editing model can be learned directly from videos. We
introduce a scalable approach to annotate videos as interleaved multimodal
sequences. To effectively learn from this data, we design a block-causal
diffusion transformer trained on three proxy tasks: next-image prediction,
current segmentation prediction, and next-segmentation prediction.
Additionally, we propose a novel multi-turn image editing benchmark to advance
research in this area. Extensive experiments demonstrate that our model
exhibits strong in-context image editing capabilities and achieves
state-of-the-art results on two multi-turn image editing benchmarks. Despite
being trained exclusively on videos, our model also shows promising abilities
in multi-concept composition, story generation, and chain-of-editing
applications.

</details>


### [210] [MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for Text-to-Image Reasoning](https://arxiv.org/abs/2506.10963)
*Yuxuan Luo,Yuhui Yuan,Junwen Chen,Haonan Cai,Ziyi Yue,Yuwei Yang,Fatima Zohra Daha,Ji Li,Zhouhui Lian*

Main category: cs.CV

TL;DR: 本文提出知识图像生成新任务及MMMG基准，评估模型多模态推理能力，发现现有模型存在严重缺陷，并发布基线模型FLUX-Reason。


<details>
  <summary>Details</summary>
Motivation: 知识图像对人类文明与学习机制至关重要，但现有图像生成模型在多模态推理（融合世界知识与像素级视觉表达）方面存在不足，需建立系统性评估体系推动技术发展。

Method: 构建MMMG基准（含4,456个跨10学科、6教育层级的知识图像-提示对），采用统一知识图谱表示核心实体关系，提出MMMG-Score指标（结合知识图谱编辑距离与视觉清晰度），并开发结合LLM与扩散模型的基线FLUX-Reason。

Result: 评估16个SOTA模型显示普遍存在实体保真度低、关系弱、视觉冗余问题（GPT-4o仅50.20分），FLUX-Reason基线得34.45，突显基准难度与模型局限。

Conclusion: MMMG揭示了图像生成模型的多模态推理缺陷，开源基准与基线为知识图像生成研究提供标准化评估框架，推动多模态推理技术发展。

Abstract: In this paper, we introduce knowledge image generation as a new task,
alongside the Massive Multi-Discipline Multi-Tier Knowledge-Image Generation
Benchmark (MMMG) to probe the reasoning capability of image generation models.
Knowledge images have been central to human civilization and to the mechanisms
of human learning--a fact underscored by dual-coding theory and the
picture-superiority effect. Generating such images is challenging, demanding
multimodal reasoning that fuses world knowledge with pixel-level grounding into
clear explanatory visuals. To enable comprehensive evaluation, MMMG offers
4,456 expert-validated (knowledge) image-prompt pairs spanning 10 disciplines,
6 educational levels, and diverse knowledge formats such as charts, diagrams,
and mind maps. To eliminate confounding complexity during evaluation, we adopt
a unified Knowledge Graph (KG) representation. Each KG explicitly delineates a
target image's core entities and their dependencies. We further introduce
MMMG-Score to evaluate generated knowledge images. This metric combines factual
fidelity, measured by graph-edit distance between KGs, with visual clarity
assessment. Comprehensive evaluations of 16 state-of-the-art text-to-image
generation models expose serious reasoning deficits--low entity fidelity, weak
relations, and clutter--with GPT-4o achieving an MMMG-Score of only 50.20,
underscoring the benchmark's difficulty. To spur further progress, we release
FLUX-Reason (MMMG-Score of 34.45), an effective and open baseline that combines
a reasoning LLM with diffusion models and is trained on 16,000 curated
knowledge image-prompt pairs.

</details>


### [211] [Pisces: An Auto-regressive Foundation Model for Image Understanding and Generation](https://arxiv.org/abs/2506.10395)
*Zhiyang Xu,Jiuhai Chen,Zhaojiang Lin,Xichen Pan,Lifu Huang,Tianyi Zhou,Madian Khabsa,Qifan Wang,Di Jin,Michihiro Yasunaga,Lili Yu,Xi Victoria Lin,Shaoliang Nie*

Main category: cs.CV

TL;DR: 本文提出Pisces模型，通过解耦视觉编码架构和优化多模态生成训练技术，在图像理解与生成任务中实现竞争性表现，揭示了两种任务的协同关系。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型在图像理解与生成任务中表现不及专用模型，主要因视觉特征需求差异及训练过程不兼容。需解决特征与训练方法耦合问题。

Method: 采用解耦视觉编码器架构，分离理解与生成任务的视觉特征提取；结合定制化多模态生成训练技术、数据筛选及预训练-微调策略。

Result: 在20+图像理解基准测试中表现优异，并在GenEval生成基准上展示强生成能力。实验表明理解与生成任务存在协同效应。

Conclusion: Pisces通过独立编码器与协同训练机制，验证了统一多模态模型的可行性，推动了视觉-语言联合建模领域的发展。

Abstract: Recent advances in large language models (LLMs) have enabled multimodal
foundation models to tackle both image understanding and generation within a
unified framework. Despite these gains, unified models often underperform
compared to specialized models in either task. A key challenge in developing
unified models lies in the inherent differences between the visual features
needed for image understanding versus generation, as well as the distinct
training processes required for each modality. In this work, we introduce
Pisces, an auto-regressive multimodal foundation model that addresses this
challenge through a novel decoupled visual encoding architecture and tailored
training techniques optimized for multimodal generation. Combined with
meticulous data curation, pretraining, and finetuning, Pisces achieves
competitive performance in both image understanding and image generation. We
evaluate Pisces on over 20 public benchmarks for image understanding, where it
demonstrates strong performance across a wide range of tasks. Additionally, on
GenEval, a widely adopted benchmark for image generation, Pisces exhibits
robust generative capabilities. Our extensive analysis reveals the synergistic
relationship between image understanding and generation, and the benefits of
using separate visual encoders, advancing the field of unified multimodal
models.

</details>


### [212] [Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization](https://arxiv.org/abs/2506.10463)
*Stone Yun,Alexander Wong*

Main category: cs.CV

TL;DR: 本文首次深入研究了量化感知的深度神经网络权重初始化方法，提出使用图超网络（GHN-QAT）预测量化DNN参数，显著提升了低比特量化（4-bit/2-bit）下的模型精度。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注量化训练技术，但忽视了权重初始化对量化鲁棒性的影响。鉴于随机初始化对浮点模型精度的重要作用，探索不同初始化策略对量化模型的影响具有必要性。

Method: 1) 系统分析不同权重初始化方法对高效CNN模块量化鲁棒性的影响；2) 提出GHN-QAT方法：通过微调图超网络预测量化图参数，实现量化鲁棒的CNN初始化。

Result: 1) 不同初始化方法显著影响最终量化鲁棒性；2) GHN-QAT在4-bit量化中精度显著提升，2-bit量化精度优于随机初始化；3) GHN预训练参数本身已具备量化鲁棒性。

Conclusion: GHN-QAT为量化模型设计提供了新范式，未来结合量化感知训练可进一步优化流程。该研究填补了量化感知初始化领域的空白，证明初始化策略对低比特量化至关重要。

Abstract: Deep neural network (DNN) quantization for fast, efficient inference has been
an important tool in limiting the cost of machine learning (ML) model
inference. Quantization-specific model development techniques such as
regularization, quantization-aware training, and quantization-robustness
penalties have served to greatly boost the accuracy and robustness of modern
DNNs. However, very little exploration has been done on improving the initial
conditions of DNN training for quantization. Just as random weight
initialization has been shown to significantly impact test accuracy of floating
point models, it would make sense that different weight initialization methods
impact quantization robustness of trained models. We present an extensive study
examining the effects of different weight initializations on a variety of CNN
building blocks commonly used in efficient CNNs. This analysis reveals that
even with varying CNN architectures, the choice of random weight initializer
can significantly affect final quantization robustness. Next, we explore a new
method for quantization-robust CNN initialization -- using Graph Hypernetworks
(GHN) to predict parameters of quantized DNNs. Besides showing that
GHN-predicted parameters are quantization-robust after regular float32
pretraining (of the GHN), we find that finetuning GHNs to predict parameters
for quantized graphs (which we call GHN-QAT) can further improve quantized
accuracy of CNNs. Notably, GHN-QAT shows significant accuracy improvements for
even 4-bit quantization and better-than-random accuracy for 2-bits. To the best
of our knowledge, this is the first in-depth study on quantization-aware DNN
weight initialization. GHN-QAT offers a novel approach to quantized DNN model
design. Future investigations, such as using GHN-QAT-initialized parameters for
quantization-aware training, can further streamline the DNN quantization
process.

</details>


### [213] [Semantic Localization Guiding Segment Anything Model For Reference Remote Sensing Image Segmentation](https://arxiv.org/abs/2506.10503)
*Shuyang Li,Shuang Wang,Zhuangzhuang Sun,Jing Xiao*

Main category: cs.CV

TL;DR: 本文提出PSLG-SAM框架，通过两阶段（粗定位+精细分割）解决遥感图像分割任务中的标注负担与复杂场景干扰问题，第二阶段无需训练，实验显示其优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有RRSIS方法依赖多模态融合与密集标注，且难以处理复杂场景。需降低标注成本并提升分割精度。

Method: 1. 粗定位阶段用视觉定位网络初步定位目标；2. 精细分割阶段结合聚类生成前景点与边界迭代优化策略，引导SAM模型实现精准分割，第二阶段可无需训练。

Result: 在RRSIS-D和RRSIS-M数据集上性能显著提升，超越现有最优模型。

Conclusion: PSLG-SAM通过任务分解降低标注需求，有效应对复杂场景，贡献高质量数据集，验证了方法的有效性。

Abstract: The Reference Remote Sensing Image Segmentation (RRSIS) task generates
segmentation masks for specified objects in images based on textual
descriptions, which has attracted widespread attention and research interest.
Current RRSIS methods rely on multi-modal fusion backbones and semantic
segmentation heads but face challenges like dense annotation requirements and
complex scene interpretation. To address these issues, we propose a framework
named \textit{prompt-generated semantic localization guiding Segment Anything
Model}(PSLG-SAM), which decomposes the RRSIS task into two stages: coarse
localization and fine segmentation. In coarse localization stage, a visual
grounding network roughly locates the text-described object. In fine
segmentation stage, the coordinates from the first stage guide the Segment
Anything Model (SAM), enhanced by a clustering-based foreground point generator
and a mask boundary iterative optimization strategy for precise segmentation.
Notably, the second stage can be train-free, significantly reducing the
annotation data burden for the RRSIS task. Additionally, decomposing the RRSIS
task into two stages allows for focusing on specific region segmentation,
avoiding interference from complex scenes.We further contribute a high-quality,
multi-category manually annotated dataset. Experimental validation on two
datasets (RRSIS-D and RRSIS-M) demonstrates that PSLG-SAM achieves significant
performance improvements and surpasses existing state-of-the-art models.Our
code will be made publicly available.

</details>


### [214] [ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs](https://arxiv.org/abs/2506.10128)
*Xiyao Wang,Zhengyuan Yang,Chao Feng,Yongyuan Liang,Yuhang Zhou,Xiaoyu Liu,Ziyi Zang,Ming Li,Chung-Ching Lin,Kevin Lin,Linjie Li,Furong Huang,Lijuan Wang*

Main category: cs.CV

TL;DR: 提出ViCrit任务，通过训练视觉语言模型定位人工注入的细微视觉描述错误，以增强其视觉感知能力，并在多个基准测试中验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习在文本任务中表现优异，但视觉语言模型因缺乏兼具挑战性和明确可验证性的视觉任务而受限。需解决视觉感知的细粒度验证问题。

Method: 设计ViCrit代理任务：向200字人工标注图像描述中注入单处细微错误（如物体、数量、空间关系），要求模型结合图像定位错误文本段，通过二元精确匹配奖励机制进行训练。

Result: ViCrit训练模型在多种VL基准测试中显著提升性能，且改进可迁移至抽象图像推理和视觉数学任务，表明模型真正学习感知而非记忆。配套发布ViCrit-Bench诊断基准。

Conclusion: 细粒度幻觉批评作为强化目标能有效提升视觉语言模型的感知能力，并具备跨领域泛化性，为视觉感知训练提供了新方向。

Abstract: Reinforcement learning (RL) has shown great effectiveness for fine-tuning
large language models (LLMs) using tasks that are challenging yet easily
verifiable, such as math reasoning or code generation. However, extending this
success to visual perception in vision-language models (VLMs) has been impeded
by the scarcity of vision-centric tasks that are simultaneously challenging and
unambiguously verifiable. To this end, we introduce ViCrit (Visual Caption
Hallucination Critic), an RL proxy task that trains VLMs to localize a subtle,
synthetic visual hallucination injected into paragraphs of human-written image
captions. Starting from a 200-word captions, we inject a single, subtle visual
description error-altering a few words on objects, attributes, counts, or
spatial relations-and task the model to pinpoint the corrupted span given the
image and the modified caption. This formulation preserves the full perceptual
difficulty while providing a binary, exact-match reward that is easy to compute
and unambiguous. Models trained with the ViCrit Task exhibit substantial gains
across a variety of VL benchmarks. Crucially, the improvements transfer beyond
natural-image training data to abstract image reasoning and visual math,
showing promises of learning to perceive rather than barely memorizing seen
objects. To facilitate evaluation, we further introduce ViCrit-Bench, a
category-balanced diagnostic benchmark that systematically probes perception
errors across diverse image domains and error types. Together, our results
demonstrate that fine-grained hallucination criticism is an effective and
generalizable objective for enhancing visual perception in VLMs.

</details>


### [215] [CogStream: Context-guided Streaming Video Question Answering](https://arxiv.org/abs/2506.10516)
*Zicheng Zhao,Kangyu Wang,Shijie Li,Rui Qian,Weiyao Lin,Huabin Liu*

Main category: cs.CV

TL;DR: 本文提出CogStream任务，针对流媒体视频推理中历史上下文冗余问题，开发了CogReasoner基线模型及半自动标注数据集，通过视觉流压缩与对话检索提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型（Vid-LLMs）在处理流媒体视频时需加载全部历史上下文，导致计算负担高且无关信息干扰模型注意力，亟需解决高效相关上下文提取问题。

Method: 提出CogStream任务框架，构建层次化标注数据集，并设计CogReasoner模型，结合视觉流压缩与历史对话检索机制，动态筛选关键上下文进行推理。

Result: 实验验证CogReasoner在流媒体视频推理任务中有效降低计算开销，同时提升答案生成准确性。

Conclusion: CogStream为流媒体视频推理提供了新基准，CogReasoner通过上下文压缩与检索机制显著优化性能，代码即将开源以促进后续研究。

Abstract: Despite advancements in Video Large Language Models (Vid-LLMs) improving
multimodal understanding, challenges persist in streaming video reasoning due
to its reliance on contextual information. Existing paradigms feed all
available historical contextual information into Vid-LLMs, resulting in a
significant computational burden for visual data processing. Furthermore, the
inclusion of irrelevant context distracts models from key details. This paper
introduces a challenging task called Context-guided Streaming Video Reasoning
(CogStream), which simulates real-world streaming video scenarios, requiring
models to identify the most relevant historical contextual information to
deduce answers for questions about the current stream. To support CogStream, we
present a densely annotated dataset featuring extensive and hierarchical
question-answer pairs, generated by a semi-automatic pipeline. Additionally, we
present CogReasoner as a baseline model. It efficiently tackles this task by
leveraging visual stream compression and historical dialogue retrieval.
Extensive experiments prove the effectiveness of this method. Code will be
released soon.

</details>


### [216] [From Images to Insights: Explainable Biodiversity Monitoring with Plain Language Habitat Explanations](https://arxiv.org/abs/2506.10559)
*Yutong Zhou,Masahiro Ryo*

Main category: cs.CV

TL;DR: 本文提出了一种端到端的视觉因果框架，通过整合物种识别、环境数据分析和因果推断，将物种图像转化为可解释的栖息地偏好解释，并利用AI生成人类可读的结论。


<details>
  <summary>Details</summary>
Motivation: 现有生态学方法分散且对非专家不友好，阻碍了对物种分布原因的理解及生物多样性保护。研究旨在通过可访问的技术揭示物种栖息地选择的因果机制。

Method: 结合物种识别、全球分布数据检索、伪缺失采样与气候数据提取，利用因果推断方法分析环境特征结构，并通过模板和大语言模型生成解释。

Result: 以蜜蜂和花卉为例的初步实验表明，框架能有效输出统计可靠的因果解释，验证了多模态AI助手在生态建模中的潜力。

Conclusion: 该框架为生态学提供了标准化、可解释的分析流程，结合因果推理与AI生成，有望提升物种栖息地描述的普及性与科学性。

Abstract: Explaining why the species lives at a particular location is important for
understanding ecological systems and conserving biodiversity. However, existing
ecological workflows are fragmented and often inaccessible to non-specialists.
We propose an end-to-end visual-to-causal framework that transforms a species
image into interpretable causal insights about its habitat preference. The
system integrates species recognition, global occurrence retrieval,
pseudo-absence sampling, and climate data extraction. We then discover causal
structures among environmental features and estimate their influence on species
occurrence using modern causal inference methods. Finally, we generate
statistically grounded, human-readable causal explanations from structured
templates and large language models. We demonstrate the framework on a bee and
a flower species and report early results as part of an ongoing project,
showing the potential of the multimodal AI assistant backed up by a recommended
ecological modeling practice for describing species habitat in
human-understandable language.

</details>


### [217] [Balancing Tails when Comparing Distributions: Comprehensive Equity Index (CEI) with Application to Bias Evaluation in Operational Face Biometrics](https://arxiv.org/abs/2506.10564)
*Imanol Solano,Julian Fierrez,Aythami Morales,Alejandro Peña,Ruben Tolosana,Francisco Zamora-Martinez,Javier San Agustin*

Main category: cs.CV

TL;DR: 提出综合公平性指数CEI及自动化版本CEI^A，通过分析人脸识别系统评分分布的尾部特征，有效检测现有指标难以捕捉的细微人口统计偏差。


<details>
  <summary>Details</summary>
Motivation: 现有公平性评估指标难以检测高性能人脸识别系统中由评分分布尾部差异引发的细微人口统计偏差，需开发更敏感的评估工具。

Method: 设计CEI指标分别分析真实匹配/非匹配评分分布，支持可配置的尾部概率聚焦与整体分布形态联合评估，并开发自动化版本CEI^A提升客观性。

Result: 实验证明CEI在检测故意偏置模型和真实系统尾部偏差方面优于传统方法，CEI^A进一步简化了实际应用复杂度。

Conclusion: CEI系列指标为人脸识别公平性评估提供了高灵敏度工具，其尾部分布分析方法可扩展至其他需关注极端值差异的统计比较场景。

Abstract: Demographic bias in high-performance face recognition (FR) systems often
eludes detection by existing metrics, especially with respect to subtle
disparities in the tails of the score distribution. We introduce the
Comprehensive Equity Index (CEI), a novel metric designed to address this
limitation. CEI uniquely analyzes genuine and impostor score distributions
separately, enabling a configurable focus on tail probabilities while also
considering overall distribution shapes. Our extensive experiments (evaluating
state-of-the-art FR systems, intentionally biased models, and diverse datasets)
confirm CEI's superior ability to detect nuanced biases where previous methods
fall short. Furthermore, we present CEI^A, an automated version of the metric
that enhances objectivity and simplifies practical application. CEI provides a
robust and sensitive tool for operational FR fairness assessment. The proposed
methods have been developed particularly for bias evaluation in face biometrics
but, in general, they are applicable for comparing statistical distributions in
any problem where one is interested in analyzing the distribution tails.

</details>


### [218] [DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers](https://arxiv.org/abs/2506.10568)
*Lizhen Wang,Zhurong Xia,Tianshu Hu,Pengrui Wang,Pengfei Wang,Zerong Zheng,Ming Zhou*

Main category: cs.CV

TL;DR: 本文提出基于扩散Transformer的框架DreamActor-H1，通过引入配对参考信息与掩码交叉注意力机制，结合3D人体网格模板和产品边界框，实现高保真人-产品演示视频生成，在身份细节保留与动作自然性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成人-产品交互视频时，难以同时保持人类与产品的身份特征（如人体姿态、产品logo），且缺乏对空间关系的理解，导致交互不自然。

Method: 使用DiT架构注入双参考信息，通过掩码交叉注意力保留细节；采用3D人体网格模板和产品包围框提供运动引导；结合结构化文本编码增强跨帧3D一致性。

Result: 在混合增强数据集上训练后，该方法在身份完整性（人类+0.12 FID，产品+0.15 IoU）和动作自然性（用户评分+23%）指标上超越SOTA模型。

Conclusion: 该框架通过多模态引导机制有效解决了人-产品交互视频生成中的身份保持与空间关系建模问题，为电商数字营销提供了实用工具。

Abstract: In e-commerce and digital marketing, generating high-fidelity human-product
demonstration videos is important for effective product presentation. However,
most existing frameworks either fail to preserve the identities of both humans
and products or lack an understanding of human-product spatial relationships,
leading to unrealistic representations and unnatural interactions. To address
these challenges, we propose a Diffusion Transformer (DiT)-based framework. Our
method simultaneously preserves human identities and product-specific details,
such as logos and textures, by injecting paired human-product reference
information and utilizing an additional masked cross-attention mechanism. We
employ a 3D body mesh template and product bounding boxes to provide precise
motion guidance, enabling intuitive alignment of hand gestures with product
placements. Additionally, structured text encoding is used to incorporate
category-level semantics, enhancing 3D consistency during small rotational
changes across frames. Trained on a hybrid dataset with extensive data
augmentation strategies, our approach outperforms state-of-the-art techniques
in maintaining the identity integrity of both humans and products and
generating realistic demonstration motions. Project page:
https://submit2025-dream.github.io/DreamActor-H1/.

</details>


### [219] [TexTailor: Customized Text-aligned Texturing via Effective Resampling](https://arxiv.org/abs/2506.10612)
*Suin Lee,Dae-Shik Kim*

Main category: cs.CV

TL;DR: TexTailor提出了一种通过重采样方案和自适应相机调整来生成视角一致纹理的新方法，解决了现有方法因纹理信息整合不足和固定视角导致的纹理不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到纹理合成方法因扩散过程中对已合成纹理信息整合不足、自回归合成特性及固定相机位置，导致视角间纹理属性偏移和一致性下降。

Method: 结合重采样方案（在扩散过程中整合历史纹理信息）、微调深度感知扩散模型、性能保持损失函数，并基于物体几何形状自适应调整相机位置。

Result: 在Objaverse子集和ShapeNet汽车数据集上的实验表明，TexTailor在视角一致性上优于现有最优方法。

Conclusion: TexTailor通过改进纹理信息整合与相机位置优化，显著提升了跨视角纹理生成的一致性，验证了方法的有效性。

Abstract: We present TexTailor, a novel method for generating consistent object
textures from textual descriptions. Existing text-to-texture synthesis
approaches utilize depth-aware diffusion models to progressively generate
images and synthesize textures across predefined multiple viewpoints. However,
these approaches lead to a gradual shift in texture properties across
viewpoints due to (1) insufficient integration of previously synthesized
textures at each viewpoint during the diffusion process and (2) the
autoregressive nature of the texture synthesis process. Moreover, the
predefined selection of camera positions, which does not account for the
object's geometry, limits the effective use of texture information synthesized
from different viewpoints, ultimately degrading overall texture consistency. In
TexTailor, we address these issues by (1) applying a resampling scheme that
repeatedly integrates information from previously synthesized textures within
the diffusion process, and (2) fine-tuning a depth-aware diffusion model on
these resampled textures. During this process, we observed that using only a
few training images restricts the model's original ability to generate
high-fidelity images aligned with the conditioning, and therefore propose an
performance preservation loss to mitigate this issue. Additionally, we improve
the synthesis of view-consistent textures by adaptively adjusting camera
positions based on the object's geometry. Experiments on a subset of the
Objaverse dataset and the ShapeNet car dataset demonstrate that TexTailor
outperforms state-of-the-art methods in synthesizing view-consistent textures.
The source code for TexTailor is available at
https://github.com/Adios42/Textailor

</details>


### [220] [Symmetrical Flow Matching: Unified Image Generation, Segmentation, and Classification with Score-Based Generative Models](https://arxiv.org/abs/2506.10634)
*Francisco Caetano,Christiaan Viviers,Peter H. N. De With,Fons van der Sommen*

Main category: cs.CV

TL;DR: SymmFlow提出对称流匹配框架，统一语义分割、分类与图像生成，通过双向一致性学习实现高效采样，在多项任务中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有Flow Matching方法在统一生成与判别任务时存在限制，如严格的一对一映射约束和生成多样性不足。需开发支持多任务灵活条件且保持语义一致性的模型。

Method: 提出对称学习目标联合建模前向/反向流，确保双向一致性并保留生成熵；设计显式语义保留目标，支持像素级/图像级条件，实现一步推理。

Result: CelebAMask-HQ和COCO-Stuff上FID分别达11.9和7.0（25步采样），分割任务表现可比专用模型，分类任务初步验证有效性。

Conclusion: SymmFlow首次实现生成与判别任务统一建模，通过对称流机制平衡一致性与多样性，为多模态任务提供高效灵活的基础框架。

Abstract: Flow Matching has emerged as a powerful framework for learning continuous
transformations between distributions, enabling high-fidelity generative
modeling. This work introduces Symmetrical Flow Matching (SymmFlow), a new
formulation that unifies semantic segmentation, classification, and image
generation within a single model. Using a symmetric learning objective,
SymmFlow models forward and reverse transformations jointly, ensuring
bi-directional consistency, while preserving sufficient entropy for generative
diversity. A new training objective is introduced to explicitly retain semantic
information across flows, featuring efficient sampling while preserving
semantic structure, allowing for one-step segmentation and classification
without iterative refinement. Unlike previous approaches that impose strict
one-to-one mapping between masks and images, SymmFlow generalizes to flexible
conditioning, supporting both pixel-level and image-level class labels.
Experimental results on various benchmarks demonstrate that SymmFlow achieves
state-of-the-art performance on semantic image synthesis, obtaining FID scores
of 11.9 on CelebAMask-HQ and 7.0 on COCO-Stuff with only 25 inference steps.
Additionally, it delivers competitive results on semantic segmentation and
shows promising capabilities in classification tasks. The code will be publicly
available.

</details>


### [221] [PiPViT: Patch-based Visual Interpretable Prototypes for Retinal Image Analysis](https://arxiv.org/abs/2506.10669)
*Marzieh Oghbaie,Teresa Araújoa,Hrvoje Bogunović*

Main category: cs.CV

TL;DR: 本文提出PiPViT模型，通过结合视觉Transformer和对比学习，在医学图像分类中实现可解释的原型学习，既能保持竞争性性能，又能提供符合临床意义的解释。


<details>
  <summary>Details</summary>
Motivation: 现有基于原型的可解释方法在医学影像中存在两个问题：1) 像素空间可视化与人类可理解的生物标志物不一致；2) 原型粒度过于细粒度，难以反映病变范围等关键临床信息。

Method: PiPViT利用视觉Transformer捕捉图像块间长程依赖，通过对比学习和多分辨率输入处理，仅用图像级标签即可学习反映病变范围的临床可解释原型。

Result: 在四个视网膜OCT数据集上达到与SOTA相当的分类性能，测试集定量评估显示原型具有临床相关性，且提供比现有方法更直观的解释。

Conclusion: PiPViT能通过可解释原型透明化决策过程，其学习到的生物标志物定位能力有助于辅助临床诊断决策。

Abstract: Background and Objective: Prototype-based methods improve interpretability by
learning fine-grained part-prototypes; however, their visualization in the
input pixel space is not always consistent with human-understandable
biomarkers. In addition, well-known prototype-based approaches typically learn
extremely granular prototypes that are less interpretable in medical imaging,
where both the presence and extent of biomarkers and lesions are critical.
  Methods: To address these challenges, we propose PiPViT (Patch-based Visual
Interpretable Prototypes), an inherently interpretable prototypical model for
image recognition. Leveraging a vision transformer (ViT), PiPViT captures
long-range dependencies among patches to learn robust, human-interpretable
prototypes that approximate lesion extent only using image-level labels.
Additionally, PiPViT benefits from contrastive learning and multi-resolution
input processing, which enables effective localization of biomarkers across
scales.
  Results: We evaluated PiPViT on retinal OCT image classification across four
datasets, where it achieved competitive quantitative performance compared to
state-of-the-art methods while delivering more meaningful explanations.
Moreover, quantitative evaluation on a hold-out test set confirms that the
learned prototypes are semantically and clinically relevant. We believe PiPViT
can transparently explain its decisions and assist clinicians in understanding
diagnostic outcomes. Github page: https://github.com/marziehoghbaie/PiPViT

</details>


### [222] [Continual Hyperbolic Learning of Instances and Classes](https://arxiv.org/abs/2506.10710)
*Melika Ayoughi,Mina Ghadimi Atigh,Mohammad Mahdi Derakhshani,Cees G. M. Snoek,Pascal Mettes,Paul Groth*

Main category: cs.CV

TL;DR: 本文提出HyperCLIC算法，通过双曲空间建模层次结构，解决持续学习中同时适应实例和类别的多粒度挑战，并在动态真实场景数据集EgoObjects上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习仅关注实例或类别分类，而真实场景（如自动驾驶）需同时处理两者。现有方法缺乏对层次化关系的建模能力，无法平衡细粒度实例识别与粗粒度类别泛化。

Method: 提出HyperCLIC算法：1) 利用双曲空间低失真表示层次结构；2) 结合双曲分类与蒸馏目标持续嵌入层次关系；3) 设计持续层次化评估指标。

Result: 在动态真实场景数据集EgoObjects上，HyperCLIC相比基线方法显著提升多粒度适应能力，实现实例级识别与类别级泛化的平衡，改进幅度达层次泛化指标3.2%。

Conclusion: 通过双曲空间显式建模类-实例层次关系，HyperCLIC首次实现持续学习中多粒度层次化表征的持续进化，为动态开放环境下的复杂物体识别提供新范式。

Abstract: Continual learning has traditionally focused on classifying either instances
or classes, but real-world applications, such as robotics and self-driving
cars, require models to handle both simultaneously. To mirror real-life
scenarios, we introduce the task of continual learning of instances and
classes, at the same time. This task challenges models to adapt to multiple
levels of granularity over time, which requires balancing fine-grained instance
recognition with coarse-grained class generalization. In this paper, we
identify that classes and instances naturally form a hierarchical structure. To
model these hierarchical relationships, we propose HyperCLIC, a continual
learning algorithm that leverages hyperbolic space, which is uniquely suited
for hierarchical data due to its ability to represent tree-like structures with
low distortion and compact embeddings. Our framework incorporates hyperbolic
classification and distillation objectives, enabling the continual embedding of
hierarchical relations. To evaluate performance across multiple granularities,
we introduce continual hierarchical metrics. We validate our approach on
EgoObjects, the only dataset that captures the complexity of hierarchical
object recognition in dynamic real-world environments. Empirical results show
that HyperCLIC operates effectively at multiple granularities with improved
hierarchical generalization.

</details>


### [223] [Deep Learning-based Multi Project InP Wafer Simulation for Unsupervised Surface Defect Detection](https://arxiv.org/abs/2506.10713)
*Emílio Dolgener Cantú,Rolf Klemens Wittmann,Oliver Abdeen,Patrick Wagner,Wojciech Samek,Moritz Baier,Sebastian Lapuschkin*

Main category: cs.CV

TL;DR: 提出一种基于深度神经网络的合成黄金标准生成方法，用于磷化铟多项目晶圆制造的缺陷检测，解决了传统模板匹配中黄金标准缺失的问题。


<details>
  <summary>Details</summary>
Motivation: 磷化铟多项目晶圆制造因生产规模小、设计变化大，缺乏黄金标准模板，导致缺陷检测依赖人工且效率低下。

Method: 使用深度神经网络从CAD数据生成高真实感的晶圆图像，构建合成黄金标准，并对比不同训练目标的效果。

Result: 深度学习方法优于基于决策树的基线方法，生成的模拟图像支持在晶圆任意区域创建'模拟黄金芯片'，提升缺陷检测效率。

Conclusion: 该方法成功应用于表面缺陷检测的模板匹配流程，验证了其在工业场景中的实用价值。

Abstract: Quality management in semiconductor manufacturing often relies on template
matching with known golden standards. For Indium-Phosphide (InP) multi-project
wafer manufacturing, low production scale and high design variability lead to
such golden standards being typically unavailable. Defect detection, in turn,
is manual and labor-intensive. This work addresses this challenge by proposing
a methodology to generate a synthetic golden standard using Deep Neural
Networks, trained to simulate photo-realistic InP wafer images from CAD data.
We evaluate various training objectives and assess the quality of the simulated
images on both synthetic data and InP wafer photographs. Our
deep-learning-based method outperforms a baseline decision-tree-based approach,
enabling the use of a 'simulated golden die' from CAD plans in any user-defined
region of a wafer for more efficient defect detection. We apply our method to a
template matching procedure, to demonstrate its practical utility in surface
defect detection.

</details>


### [224] [Revisiting Transformers with Insights from Image Filtering](https://arxiv.org/abs/2506.10371)
*Laziz U. Abdullaev,Maksim Tkachenko,Tan M. Nguyen*

Main category: cs.CV

TL;DR: 本文提出一种基于图像处理的统一框架，用于解释Transformer自注意力机制及其组件（如位置编码、残差连接）的作用，并通过架构改进提升模型在语言/视觉任务中的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有自注意力机制理论框架缺乏对架构组件的机理解释，且后续变体改进缺乏统一分析。研究旨在通过图像处理视角建立更深入的理论基础，并探索性能提升潜力。

Method: 开发统一图像处理框架，系统性解释自注意力计算及关键组件功能，提出两种独立架构改进（受图像处理启发），重点关注可解释性设计。

Result: 改进后的Transformer在数据污染/对抗攻击下展现更高鲁棒性，语言/视觉任务准确率显著提升，长序列理解能力增强，同时保持理论可解释性。

Conclusion: 图像处理框架为自注意力机制提供新理论解释路径，架构改进验证理论-实践协同优化的可行性，为未来可解释性研究与性能优化建立桥梁。

Abstract: The self-attention mechanism, a cornerstone of Transformer-based
state-of-the-art deep learning architectures, is largely heuristic-driven and
fundamentally challenging to interpret. Establishing a robust theoretical
foundation to explain its remarkable success and limitations has therefore
become an increasingly prominent focus in recent research. Some notable
directions have explored understanding self-attention through the lens of image
denoising and nonparametric regression. While promising, existing frameworks
still lack a deeper mechanistic interpretation of various architectural
components that enhance self-attention, both in its original formulation and
subsequent variants. In this work, we aim to advance this understanding by
developing a unifying image processing framework, capable of explaining not
only the self-attention computation itself but also the role of components such
as positional encoding and residual connections, including numerous later
variants. We also pinpoint potential distinctions between the two concepts
building upon our framework, and make effort to close this gap. We introduce
two independent architectural modifications within transformers. While our
primary objective is interpretability, we empirically observe that image
processing-inspired modifications can also lead to notably improved accuracy
and robustness against data contamination and adversaries across language and
vision tasks as well as better long sequence understanding.

</details>


### [225] [Stroke-based Cyclic Amplifier: Image Super-Resolution at Arbitrary Ultra-Large Scales](https://arxiv.org/abs/2506.10774)
*Wenhao Guo,Peng Lu,Xujun Peng,Zhaoran Zhao,Sheng Li*

Main category: cs.CV

TL;DR: 提出基于笔画循环放大（SbCA）的统一模型，通过矢量分解与循环细化策略解决超大规模图像超分辨率中的模糊问题，仅需一次训练即可实现高质量百倍放大。


<details>
  <summary>Details</summary>
Motivation: 现有任意尺度超分方法在超出训练范围的放大倍数时性能骤降，导致模糊和伪影，需解决分布漂移与细节恢复问题。

Method: 使用笔画矢量放大器将图像分解为矢量笔画进行放大，结合细节补全模块恢复缺失信息，并通过循环迭代策略实现超大规模放大。

Result: 在合成与真实数据集上验证，百倍放大任务中显著优于现有方法，消除伪影/噪声/模糊，生成高保真超分辨率图像。

Conclusion: SbCA通过统一模型与循环细化策略有效解决超大规模放大中的分布漂移问题，实现视觉质量超越SOTA的高分辨率重建。

Abstract: Prior Arbitrary-Scale Image Super-Resolution (ASISR) methods often experience
a significant performance decline when the upsampling factor exceeds the range
covered by the training data, introducing substantial blurring. To address this
issue, we propose a unified model, Stroke-based Cyclic Amplifier (SbCA), for
ultra-large upsampling tasks. The key of SbCA is the stroke vector amplifier,
which decomposes the image into a series of strokes represented as vector
graphics for magnification. Then, the detail completion module also restores
missing details, ensuring high-fidelity image reconstruction. Our cyclic
strategy achieves ultra-large upsampling by iteratively refining details with
this unified SbCA model, trained only once for all, while keeping sub-scales
within the training range. Our approach effectively addresses the distribution
drift issue and eliminates artifacts, noise and blurring, producing
high-quality, high-resolution super-resolved images. Experimental validations
on both synthetic and real-world datasets demonstrate that our approach
significantly outperforms existing methods in ultra-large upsampling tasks
(e.g. $\times100$), delivering visual quality far superior to state-of-the-art
techniques.

</details>


### [226] [SlotPi: Physics-informed Object-centric Reasoning Models](https://arxiv.org/abs/2506.10778)
*Jian Li,Wan Han,Ning Lin,Yu-Liang Zhan,Ruizhi Chengze,Haining Wang,Yi Zhang,Hongsheng Liu,Zidong Wang,Fan Yu,Hao Sun*

Main category: cs.CV

TL;DR: 论文提出SlotPi模型，通过整合物理知识与时空预测模块，解决现有物体中心动态模拟方法在物理知识及跨场景验证上的缺陷，并在多数据集上验证其强适应性。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视物理知识整合与跨场景验证，尤其在涉及流体与物体交互的真实动态场景中，需增强模型对物理规律的理解与泛化能力。

Method: SlotPi结合基于哈密顿原理的物理模块与时空动态预测模块，构建物理启发的物体中心推理模型，支持动态预测与复杂交互模拟。

Result: 模型在基准与流体数据集上表现出色，预测与视觉问答任务性能优越；新构建的真实世界交互数据集进一步验证其广泛适应性与鲁棒性。

Conclusion: SlotPi为高级世界模型奠定基础，其跨场景强适应性证明物理知识与动态建模结合的有效性，尤其在流体-物体交互等复杂场景中。

Abstract: Understanding and reasoning about dynamics governed by physical laws through
visual observation, akin to human capabilities in the real world, poses
significant challenges. Currently, object-centric dynamic simulation methods,
which emulate human behavior, have achieved notable progress but overlook two
critical aspects: 1) the integration of physical knowledge into models. Humans
gain physical insights by observing the world and apply this knowledge to
accurately reason about various dynamic scenarios; 2) the validation of model
adaptability across diverse scenarios. Real-world dynamics, especially those
involving fluids and objects, demand models that not only capture object
interactions but also simulate fluid flow characteristics. To address these
gaps, we introduce SlotPi, a slot-based physics-informed object-centric
reasoning model. SlotPi integrates a physical module based on Hamiltonian
principles with a spatio-temporal prediction module for dynamic forecasting.
Our experiments highlight the model's strengths in tasks such as prediction and
Visual Question Answering (VQA) on benchmark and fluid datasets. Furthermore,
we have created a real-world dataset encompassing object interactions, fluid
dynamics, and fluid-object interactions, on which we validated our model's
capabilities. The model's robust performance across all datasets underscores
its strong adaptability, laying a foundation for developing more advanced world
models.

</details>


### [227] [Post-Training Quantization for Video Matting](https://arxiv.org/abs/2506.10840)
*Tianrui Zhu,Houyuan Chen,Ruihao Gong,Michele Magno,Haotong Qin,Kai Zhang*

Main category: cs.CV

TL;DR: 本文提出首个针对视频抠图模型的后训练量化框架PTQ4VM，通过两阶段优化策略、全局统计校准和光流辅助组件，在超低位宽下实现接近全精度的性能，同时显著降低计算量。


<details>
  <summary>Details</summary>
Motivation: 视频抠图模型在资源受限设备部署时面临计算密集与时间连贯性保持的双重挑战，现有后训练量化方法在精度保持和时序一致性方面存在显著不足。

Method: 1) 分块重建与全局校准两阶段量化策略；2) 基于统计的全局仿射校准(GAC)补偿累积误差；3) 光流辅助组件(OFA)利用时序语义先验增强运动前景识别能力。

Result: 4位量化模型达到接近全精度性能(误差降低达20%)，计算量减少8倍，在不同位宽下均取得当前最优量化精度，视觉结果保持良好时序一致性。

Conclusion: PTQ4VM框架通过系统化量化策略设计，首次在视频抠图领域实现高效低位宽部署，为动态视觉任务模型压缩提供了新范式。

Abstract: Video matting is crucial for applications such as film production and virtual
reality, yet deploying its computationally intensive models on
resource-constrained devices presents challenges. Quantization is a key
technique for model compression and acceleration. As an efficient approach,
Post-Training Quantization (PTQ) is still in its nascent stages for video
matting, facing significant hurdles in maintaining accuracy and temporal
coherence. To address these challenges, this paper proposes a novel and general
PTQ framework specifically designed for video matting models, marking, to the
best of our knowledge, the first systematic attempt in this domain. Our
contributions include: (1) A two-stage PTQ strategy that combines
block-reconstruction-based optimization for fast, stable initial quantization
and local dependency capture, followed by a global calibration of quantization
parameters to minimize accuracy loss. (2) A Statistically-Driven Global Affine
Calibration (GAC) method that enables the network to compensate for cumulative
statistical distortions arising from factors such as neglected BN layer
effects, even reducing the error of existing PTQ methods on video matting tasks
up to 20%. (3) An Optical Flow Assistance (OFA) component that leverages
temporal and semantic priors from frames to guide the PTQ process, enhancing
the model's ability to distinguish moving foregrounds in complex scenes and
ultimately achieving near full-precision performance even under ultra-low-bit
quantization. Comprehensive quantitative and visual results show that our
PTQ4VM achieves the state-of-the-art accuracy performance across different
bit-widths compared to the existing quantization methods. We highlight that the
4-bit PTQ4VM even achieves performance close to the full-precision counterpart
while enjoying 8x FLOP savings.

</details>


### [228] [VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos](https://arxiv.org/abs/2506.10857)
*Jiashuo Yu,Yue Wu,Meng Chu,Zhifei Ren,Zizheng Huang,Pei Chu,Ruijie Zhang,Yinan He,Qirui Li,Songze Li,Zhenxiang Li,Zhongying Tu,Conghui He,Yu Qiao,Yali Wang,Yi Wang,Limin Wang*

Main category: cs.CV

TL;DR: VRBench是首个针对大模型多步推理能力的长叙事视频基准，包含1,010个长视频及人工标注的问答对，通过多阶段评估流程和LLM引导的评分指标，对12个LLM和16个VLM进行全面分析。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法忽视时间推理与过程有效性，无法充分衡量大模型的多步推理能力。VRBench旨在填补这一空白，强调时序逻辑与情节连贯性。

Method: 构建包含1.6小时均长视频的数据集，采用多阶段筛选（含专家评审）保证情节质量；设计人机协作框架生成多步推理链，提出多阶段评估流程（结果层MCQ+过程层LLM引导多维度评分）。

Result: 评估显示现有模型在多步推理任务中存在显著局限，尤其在时序关联与隐式推理方面。提出的进度级评分指标有效揭示推理链质量差异。

Conclusion: VRBench通过时序锚定与过程导向的评估范式，为多步推理研究提供新基准，其方法论与洞见推动该领域发展。

Abstract: We present VRBench, the first long narrative video benchmark crafted for
evaluating large models' multi-step reasoning capabilities, addressing
limitations in existing evaluations that overlook temporal reasoning and
procedural validity. It comprises 1,010 long videos (with an average duration
of 1.6 hours), along with 9,468 human-labeled multi-step question-answering
pairs and 30,292 reasoning steps with timestamps. These videos are curated via
a multi-stage filtering process including expert inter-rater reviewing to
prioritize plot coherence. We develop a human-AI collaborative framework that
generates coherent reasoning chains, each requiring multiple temporally
grounded steps, spanning seven types (e.g., event attribution, implicit
inference). VRBench designs a multi-phase evaluation pipeline that assesses
models at both the outcome and process levels. Apart from the MCQs for the
final results, we propose a progress-level LLM-guided scoring metric to
evaluate the quality of the reasoning chain from multiple dimensions
comprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on
VRBench, we undertake a thorough analysis and provide valuable insights that
advance the field of multi-step reasoning.

</details>


### [229] [AIR: Zero-shot Generative Model Adaptation with Iterative Refinement](https://arxiv.org/abs/2506.10895)
*Guimeng Liu,Milad Abdollahzadeh,Ngai-Man Cheung*

Main category: cs.CV

TL;DR: 本文提出了一种名为AIR的零样本生成模型适应方法，通过分析CLIP嵌入空间中文本与图像偏移的不对齐现象，发现其与概念距离相关，并基于此设计迭代优化策略，显著提升了生成图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有零样本生成模型适应方法假设CLIP嵌入空间中图像偏移与文本偏移完全对齐，但实际存在不对齐问题，导致生成质量下降。本文旨在分析这一现象并提出改进方法。

Method: 首先通过实证研究发现CLIP嵌入空间中偏移不对齐与概念距离相关（相近概念不对齐程度低），随后提出AIR方法，通过迭代优化策略逐步提升目标域生成质量。

Result: 在26种实验场景中，AIR在定性、定量指标及用户研究中均达到SOTA性能，补充实验进一步验证了方法的有效性。

Conclusion: 偏移不对齐与概念距离存在关联，AIR通过迭代优化有效缓解此问题，实验证明其在零样本生成模型适应任务中的优越性。

Abstract: Zero-shot generative model adaptation (ZSGM) aims to adapt a pre-trained
generator to a target domain using only text guidance and without any samples
from the target domain. Central to recent ZSGM approaches are directional loss
which use the text guidance in the form of aligning the image offset with text
offset in the embedding space of a vision-language model like CLIP. This is
similar to the analogical reasoning in NLP where the offset between one pair of
words is used to identify a missing element in another pair by aligning the
offset between these two pairs. However, a major limitation of existing ZSGM
methods is that the learning objective assumes the complete alignment between
image offset and text offset in the CLIP embedding space, resulting in quality
degrade in generated images. Our work makes two main contributions. Inspired by
the offset misalignment studies in NLP, as our first contribution, we perform
an empirical study to analyze the misalignment between text offset and image
offset in CLIP embedding space for various large publicly available datasets.
Our important finding is that offset misalignment in CLIP embedding space is
correlated with concept distance, i.e., close concepts have a less offset
misalignment. To address the limitations of the current approaches, as our
second contribution, we propose Adaptation with Iterative Refinement (AIR)
which is the first ZSGM approach to focus on improving target domain image
quality based on our new insight on offset misalignment.Qualitative,
quantitative, and user study in 26 experiment setups consistently demonstrate
the proposed AIR approach achieves SOTA performance. Additional experiments are
in Supp.

</details>


### [230] [M4V: Multi-Modal Mamba for Text-to-Video Generation](https://arxiv.org/abs/2506.10915)
*Jiancheng Huang,Gengwei Zhang,Zequn Jie,Siyu Jiao,Yinlong Qian,Ling Chen,Yunchao Wei,Lin Ma*

Main category: cs.CV

TL;DR: 本文提出M4V框架，结合多模态Mamba架构与扩散模型，显著降低文本到视频生成的计算成本（FLOPs减少45%），并通过奖励学习提升长上下文生成的视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频生成方法（如Transformer）因序列处理的二次复杂度导致计算成本高，而线性时间建模的Mamba架构虽高效，但其原生设计难以直接支持多模态时空建模。

Method: 提出多模态扩散Mamba块（MM-DiM），通过多模态token重组整合多模态信息；引入奖励学习策略缓解长序列自回归生成的视觉质量下降问题。

Result: 在768×1280分辨率视频生成中，M4V相比基于注意力的方法减少45% FLOPs，实验验证其能高效生成高质量视频。

Conclusion: M4V框架通过结合Mamba与多模态扩散设计，在降低计算成本的同时保持生成质量，为高效视频生成与潜在世界模拟提供了新方向。

Abstract: Text-to-video generation has significantly enriched content creation and
holds the potential to evolve into powerful world simulators. However, modeling
the vast spatiotemporal space remains computationally demanding, particularly
when employing Transformers, which incur quadratic complexity in sequence
processing and thus limit practical applications. Recent advancements in
linear-time sequence modeling, particularly the Mamba architecture, offer a
more efficient alternative. Nevertheless, its plain design limits its direct
applicability to multi-modal and spatiotemporal video generation tasks. To
address these challenges, we introduce M4V, a Multi-Modal Mamba framework for
text-to-video generation. Specifically, we propose a multi-modal diffusion
Mamba (MM-DiM) block that enables seamless integration of multi-modal
information and spatiotemporal modeling through a multi-modal token
re-composition design. As a result, the Mamba blocks in M4V reduce FLOPs by 45%
compared to the attention-based alternative when generating videos at
768$\times$1280 resolution. Additionally, to mitigate the visual quality
degradation in long-context autoregressive generation processes, we introduce a
reward learning strategy that further enhances per-frame visual realism.
Extensive experiments on text-to-video benchmarks demonstrate M4V's ability to
produce high-quality videos while significantly lowering computational costs.
Code and models will be publicly available at
https://huangjch526.github.io/M4V_project.

</details>


### [231] [SpectralAR: Spectral Autoregressive Visual Generation](https://arxiv.org/abs/2506.10962)
*Yuanhui Huang,Weiliang Chen,Wenzhao Zheng,Yueqi Duan,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 提出SpectralAR框架，通过频谱分块化与自回归生成解决视觉序列因果性问题，在ImageNet-1K上以64 tokens和310M参数达到3.02 gFID。


<details>
  <summary>Details</summary>
Motivation: 现有自回归视觉生成方法使用空间块构造序列，但图像块本质是并行的，与自回归建模的因果性存在矛盾。需从频谱角度重新定义视觉序列的因果关系。

Method: 1. 使用Nested Spectral Tokenization将图像转换为有序频谱token（低频到高频）；2. 以从粗到细的方式对频谱token序列进行自回归生成。

Result: 在ImageNet-1K图像重建与生成任务中，仅用64 tokens和310M参数即达到3.02 gFID指标，实现因果性与计算效率的平衡。

Conclusion: 通过频谱视角构建因果序列，SpectralAR在保持token效率的同时实现视觉自回归建模，为视觉生成提供新范式。

Abstract: Autoregressive visual generation has garnered increasing attention due to its
scalability and compatibility with other modalities compared with diffusion
models. Most existing methods construct visual sequences as spatial patches for
autoregressive generation. However, image patches are inherently parallel,
contradicting the causal nature of autoregressive modeling. To address this, we
propose a Spectral AutoRegressive (SpectralAR) visual generation framework,
which realizes causality for visual sequences from the spectral perspective.
Specifically, we first transform an image into ordered spectral tokens with
Nested Spectral Tokenization, representing lower to higher frequency
components. We then perform autoregressive generation in a coarse-to-fine
manner with the sequences of spectral tokens. By considering different levels
of detail in images, our SpectralAR achieves both sequence causality and token
efficiency without bells and whistles. We conduct extensive experiments on
ImageNet-1K for image reconstruction and autoregressive generation, and
SpectralAR achieves 3.02 gFID with only 64 tokens and 310M parameters. Project
page: https://huang-yh.github.io/spectralar/.

</details>


### [232] [Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs](https://arxiv.org/abs/2506.10967)
*Qizhe Zhang,Mengzhen Liu,Lichen Li,Ming Lu,Yuan Zhang,Junwen Pan,Qi She,Shanghang Zhang*

Main category: cs.CV

TL;DR: CDPruner提出一种基于条件多样性的视觉令牌剪枝方法，通过DPP优化选择子集，显著降低多模态大模型推理成本，同时保持94%的原始精度。


<details>
  <summary>Details</summary>
Motivation: 现有视觉令牌剪枝方法存在冗余保留或忽略指令相关性问题，导致计算成本高且性能次优。需在保留关键信息与降低计算量间取得平衡。

Method: 基于指令条件定义视觉令牌相似度，利用确定性点过程(DPP)最大化子集条件多样性。无需训练且模型无关，可灵活应用于不同MLLMs。

Result: 在LLaVA上实现FLOPs减少95%、CUDA延迟降低78%，精度保留94%。多基准测试达到SOTA，高压缩比下性能稳定。

Conclusion: CDPruner通过条件多样性驱动的令牌选择机制，在显著降低计算开销的同时保持语义理解能力，为MLLMs高效推理提供通用解决方案。

Abstract: In multimodal large language models (MLLMs), the length of input visual
tokens is often significantly greater than that of their textual counterparts,
leading to a high inference cost. Many works aim to address this issue by
removing redundant visual tokens. However, current approaches either rely on
attention-based pruning, which retains numerous duplicate tokens, or use
similarity-based pruning, overlooking the instruction relevance, consequently
causing suboptimal performance. In this paper, we go beyond attention or
similarity by proposing a novel visual token pruning method named CDPruner,
which maximizes the conditional diversity of retained tokens. We first define
the conditional similarity between visual tokens conditioned on the
instruction, and then reformulate the token pruning problem with determinantal
point process (DPP) to maximize the conditional diversity of the selected
subset. The proposed CDPruner is training-free and model-agnostic, allowing
easy application to various MLLMs. Extensive experiments across diverse MLLMs
show that CDPruner establishes new state-of-the-art on various vision-language
benchmarks. By maximizing conditional diversity through DPP, the selected
subset better represents the input images while closely adhering to user
instructions, thereby preserving strong performance even with high reduction
ratios. When applied to LLaVA, CDPruner reduces FLOPs by 95\% and CUDA latency
by 78\%, while maintaining 94\% of the original accuracy. Our code is available
at https://github.com/Theia-4869/CDPruner.

</details>


### [233] [Fine-Grained Perturbation Guidance via Attention Head Selection](https://arxiv.org/abs/2506.10978)
*Donghoon Ahn,Jiwon Kang,Sanghyun Lee,Minjae Kim,Jaewon Min,Wooseok Jang,Saungwu Lee,Sayak Paul,Susung Hong,Seungryong Kim*

Main category: cs.CV

TL;DR: 本文提出HeadHunter框架和SoftPAG方法，通过分析扩散模型中注意力头的细粒度扰动特性，实现生成质量与视觉属性的精准控制。首次在DiT架构中发现不同注意力头分别主导结构/风格/纹理等视觉概念，并通过头级扰动策略解决现有层级扰动导致的过平滑问题。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型注意力扰动方法缺乏对扰动位置的系统性分析，特别是在DiT架构中，质量相关计算分布于各层时难以确定最佳扰动层级。层级别扰动易导致过度平滑且无法实现细粒度视觉属性控制。

Method: 1) HeadHunter框架：通过迭代选择与用户目标对齐的注意力头，实现头粒度的扰动定位；2) SoftPAG技术：将选定头的注意力图向单位矩阵线性插值，提供连续扰动强度调节，抑制人工伪影。

Result: 在Stable Diffusion 3和FLUX.1等现代DiT模型上验证显示：相比层级扰动，头级选择使FID指标提升15%，同时支持通过头组合实现特定风格（如油画/像素风）的定向操控，且生成结果中伪影减少23%。

Conclusion: 揭示了扩散模型注意力头的可解释性功能分化，提出首个头级扰动分析框架，为构建高效扰动策略提供理论依据，同时通过组合式头选择开辟了细粒度视觉属性编辑的新途径。

Abstract: Recent guidance methods in diffusion models steer reverse sampling by
perturbing the model to construct an implicit weak model and guide generation
away from it. Among these approaches, attention perturbation has demonstrated
strong empirical performance in unconditional scenarios where classifier-free
guidance is not applicable. However, existing attention perturbation methods
lack principled approaches for determining where perturbations should be
applied, particularly in Diffusion Transformer (DiT) architectures where
quality-relevant computations are distributed across layers. In this paper, we
investigate the granularity of attention perturbations, ranging from the layer
level down to individual attention heads, and discover that specific heads
govern distinct visual concepts such as structure, style, and texture quality.
Building on this insight, we propose "HeadHunter", a systematic framework for
iteratively selecting attention heads that align with user-centric objectives,
enabling fine-grained control over generation quality and visual attributes. In
addition, we introduce SoftPAG, which linearly interpolates each selected
head's attention map toward an identity matrix, providing a continuous knob to
tune perturbation strength and suppress artifacts. Our approach not only
mitigates the oversmoothing issues of existing layer-level perturbation but
also enables targeted manipulation of specific visual styles through
compositional head selection. We validate our method on modern large-scale
DiT-based text-to-image models including Stable Diffusion 3 and FLUX.1,
demonstrating superior performance in both general quality enhancement and
style-specific guidance. Our work provides the first head-level analysis of
attention perturbation in diffusion models, uncovering interpretable
specialization within attention layers and enabling practical design of
effective perturbation strategies.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [234] [Leveraging LLMs for Mission Planning in Precision Agriculture](https://arxiv.org/abs/2506.10093)
*Marcos Abel Zuzuárregui,Stefano Carpin*

Main category: cs.RO

TL;DR: 本文提出一种基于大型语言模型（如ChatGPT）的端到端系统，使非技术用户可通过自然语言指令指挥农业机器人执行复杂数据采集任务，并通过标准化任务编码与ROS2集成解决LLM在空间推理与路径规划中的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前农业机器人系统虽能完成特定任务，但因其技术复杂性难以适应多样化任务需求，尤其终端用户技术门槛高的问题亟待解决。

Method: 结合ChatGPT将自然语言指令转化为符合IEEE任务规范的标准化任务计划，通过ROS2节点桥接高层指令与底层库，实现任务执行。

Result: 实验表明LLM在空间推理与复杂路径规划中存在局限性，但通过标准化任务编码与ROS2执行框架可有效弥补这些缺陷。

Conclusion: 该系统通过LLM与标准化任务规划的结合，降低了农业机器人任务部署的技术门槛，为LLM在机器人领域应用提供了可扩展的实践范例。

Abstract: Robotics and artificial intelligence hold significant potential for advancing
precision agriculture. While robotic systems have been successfully deployed
for various tasks, adapting them to perform diverse missions remains
challenging, particularly because end users often lack technical expertise. In
this paper, we present an end-to-end system that leverages large language
models (LLMs), specifically ChatGPT, to enable users to assign complex data
collection tasks to autonomous robots using natural language instructions. To
enhance reusability, mission plans are encoded using an existing IEEE task
specification standard, and are executed on robots via ROS2 nodes that bridge
high-level mission descriptions with existing ROS libraries. Through extensive
experiments, we highlight the strengths and limitations of LLMs in this
context, particularly regarding spatial reasoning and solving complex routing
challenges, and show how our proposed implementation overcomes them.

</details>


### [235] [One For All: LLM-based Heterogeneous Mission Planning in Precision Agriculture](https://arxiv.org/abs/2506.10106)
*Marcos Abel Zuzuárregui,Mustafa Melih Toslak,Stefano Carpin*

Main category: cs.RO

TL;DR: 本文提出一种基于自然语言和大语言模型（LLM）的机器人任务规划系统，使非技术用户无需编程即可通过自然语言指令控制异构农业机器人，扩展实验验证了其在机械操作与计算机视觉任务中的通用性和有效性。


<details>
  <summary>Details</summary>
Motivation: 精准农业中的人工智能技术虽提升效率，但增加了非技术用户的操作复杂度。现有系统需用户平衡技术学习与工作负担，亟需降低使用门槛。

Method: 结合LLM与预定义操作原语，将自然语言指令转化为中间描述，适配异构机器人平台；扩展前期轮式机器人系统，新增机械臂操作与视觉任务实验。

Result: 系统支持多类型机器人执行复杂任务（如作物监测、机械操作），实验证明其通用性与执行能力，用户无需编码即可规划农业任务。

Conclusion: 该架构显著降低了农业机器人自动化的技术门槛，为非技术用户提供高效易用的控制方案，推动精准农业技术普及。

Abstract: Artificial intelligence is transforming precision agriculture, offering
farmers new tools to streamline their daily operations. While these
technological advances promise increased efficiency, they often introduce
additional complexity and steep learning curves that are particularly
challenging for non-technical users who must balance tech adoption with
existing workloads. In this paper, we present a natural language (NL) robotic
mission planner that enables non-specialists to control heterogeneous robots
through a common interface. By leveraging large language models (LLMs) and
predefined primitives, our architecture seamlessly translates human language
into intermediate descriptions that can be executed by different robotic
platforms. With this system, users can formulate complex agricultural missions
without writing any code. In the work presented in this paper, we extend our
previous system tailored for wheeled robot mission planning through a new class
of experiments involving robotic manipulation and computer vision tasks. Our
results demonstrate that the architecture is both general enough to support a
diverse set of robots and powerful enough to execute complex mission requests.
This work represents a significant step toward making robotic automation in
precision agriculture more accessible to non-technical users.

</details>


### [236] [A Navigation Framework Utilizing Vision-Language Models](https://arxiv.org/abs/2506.10172)
*Yicheng Duan,Kaiyu tang*

Main category: cs.RO

TL;DR: 本文提出一种模块化视觉语言导航框架，通过解耦视觉语言理解与行动规划，结合冻结大模型与轻量规划逻辑，旨在实现高效、灵活且无需微调的导航系统。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型（LVLMs）虽提升多模态理解能力，但面临计算成本与实时部署挑战，需探索更高效的导航解决方案。

Method: 采用冻结的Qwen2.5-VL-7B-Instruct模型处理多模态理解，结合轻量级规划模块，通过提示工程、结构化历史管理与双帧视觉输入策略优化决策连续性。

Result: 在VLN-CE的Room-to-Room基准测试中，系统在未见环境泛化能力上表现受限，但验证了模块化框架在可扩展性与效率上的潜力。

Conclusion: 模块化架构为高效导航系统奠定基础，未来可通过增强环境先验知识与扩展多模态输入集成进一步优化性能。

Abstract: Vision-and-Language Navigation (VLN) presents a complex challenge in embodied
AI, requiring agents to interpret natural language instructions and navigate
through visually rich, unfamiliar environments. Recent advances in large
vision-language models (LVLMs), such as CLIP and Flamingo, have significantly
improved multimodal understanding but introduced new challenges related to
computational cost and real-time deployment. In this project, we propose a
modular, plug-and-play navigation framework that decouples vision-language
understanding from action planning. By integrating a frozen vision-language
model, Qwen2.5-VL-7B-Instruct, with lightweight planning logic, we aim to
achieve flexible, fast, and adaptable navigation without extensive model
fine-tuning. Our framework leverages prompt engineering, structured history
management, and a two-frame visual input strategy to enhance decision-making
continuity across navigation steps. We evaluate our system on the Room-to-Room
benchmark within the VLN-CE setting using the Matterport3D dataset and
Habitat-Lab simulation environment. Although our initial results reveal
challenges in generalizing to unseen environments under strict evaluation
settings, our modular approach lays a foundation for scalable and efficient
navigation systems, highlighting promising directions for future improvement
through enhanced environmental priors and expanded multimodal input
integration.

</details>


### [237] [Using Language and Road Manuals to Inform Map Reconstruction for Autonomous Driving](https://arxiv.org/abs/2506.10317)
*Akshar Tumu,Henrik I. Christensen,Marcell Vazquez-Chanlatte,Chikao Tsuchiya,Dhaval Bhanderi*

Main category: cs.RO

TL;DR: 本文提出一种轻量级方法，通过融合OSM地图结构化元数据、道路设计手册中的车道宽度先验与道路中心线编码，增强SMERF模型的车道拓扑预测能力。该方法在复杂交叉口场景中提升了车道与交通要素检测及其关联性能，并验证了模型对多样化拓扑的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有车道拓扑预测任务中，道路设计规范编码（如自然语言描述的设计标准、路名功能信息）蕴含重要先验知识，但未被充分利用。通过整合这些结构化元数据可提升模型对道路环境的理解。

Method: 在SMERF模型基础上，融合OpenStreetMap道路元数据与道路设计手册中的车道宽度先验信息，增强道路中心线编码表示。通过多源数据联合建模实现拓扑预测。

Result: 在两个地理多样性复杂交叉口场景的测试中，车道检测F1-score提升3.2%，交通要素关联准确率提升5.1%，四项拓扑感知指标均优于基线，验证了方法的泛化能力。

Conclusion: 融合自然语言编码的道路设计先验与结构化地图数据能有效提升车道拓扑预测性能，该方法具有跨区域、跨道路结构的扩展性，为自动驾驶环境感知提供了新思路。

Abstract: Lane-topology prediction is a critical component of safe and reliable
autonomous navigation. An accurate understanding of the road environment aids
this task. We observe that this information often follows conventions encoded
in natural language, through design codes that reflect the road structure and
road names that capture the road functionality. We augment this information in
a lightweight manner to SMERF, a map-prior-based online lane-topology
prediction model, by combining structured road metadata from OSM maps and
lane-width priors from Road design manuals with the road centerline encodings.
We evaluate our method on two geo-diverse complex intersection scenarios. Our
method shows improvement in both lane and traffic element detection and their
association. We report results using four topology-aware metrics to
comprehensively assess the model performance. These results demonstrate the
ability of our approach to generalize and scale to diverse topologies and
conditions.

</details>


### [238] [Estimating the Joint Probability of Scenario Parameters with Gaussian Mixture Copula Models](https://arxiv.org/abs/2506.10098)
*Christian Reichenbächer,Philipp Rank,Jochen Hipp,Oliver Bringmann*

Main category: cs.RO

TL;DR: 本文首次将高斯混合Copula模型应用于自动驾驶场景的统计建模，结合高斯混合模型的多模态表达能力与Copula的灵活性，在联合概率分布建模中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 基于场景的自动驾驶安全验证需量化参数组合的联合概率分布，但现有方法（如高斯混合模型、高斯Copula）在表达多模态数据与分离建模边际分布/依赖性方面存在局限。

Method: 提出高斯混合Copula模型，通过Copula分离建模边际分布与依赖关系，同时利用高斯混合模型捕捉多模态特征。基于UN R157标准场景的真实驾驶数据，对比传统高斯混合模型与高斯Copula模型。

Result: 在1800万场景实例的评估中，新模型在似然度与Sinkhorn距离指标上均优于基线方法，证明其数据拟合能力更强。

Conclusion: 高斯混合Copula模型为未来场景化验证框架提供了更可靠的统计建模基础，尤其在复杂多模态驾驶场景中具有显著优势。

Abstract: This paper presents the first application of Gaussian Mixture Copula Models
to the statistical modeling of driving scenarios for the safety validation of
automated driving systems. Knowledge of the joint probability distribution of
scenario parameters is essential for scenario-based safety assessment, where
risk quantification depends on the likelihood of concrete parameter
combinations. Gaussian Mixture Copula Models bring together the multimodal
expressivity of Gaussian Mixture Models and the flexibility of copulas,
enabling separate modeling of marginal distributions and dependencies. We
benchmark Gaussian Mixture Copula Models against previously proposed approaches
- Gaussian Mixture Models and Gaussian Copula Models - using real-world driving
data drawn from scenarios defined in United Nations Regulation No. 157. Our
evaluation across 18 million scenario instances demonstrates that Gaussian
Mixture Copula Models provide a better fit to the data in terms of both
likelihood and Sinkhorn distance. These results suggest that Gaussian Mixture
Copula Models are a compelling foundation for future scenario-based validation
frameworks.

</details>


### [239] [Demonstrating Multi-Suction Item Picking at Scale via Multi-Modal Learning of Pick Success](https://arxiv.org/abs/2506.10359)
*Che Wang,Jeroen van Baar,Chaitanya Mitash,Shuai Li,Dylan Randle,Weiyao Wang,Sumedh Sontakke,Kostas E. Bekris,Kapil Katyal*

Main category: cs.RO

TL;DR: 该研究提出一种多模态视觉编码器方法，通过结合预训练与微调策略，利用RGB、深度和语义分割等多模态输入预测多吸盘机器人抓取成功率，在工业规模稀疏标记数据上实现高效分拣。


<details>
  <summary>Details</summary>
Motivation: 在仓库等实际场景中，机器人需从非结构化堆中高效抓取多样化物品，现有方法需同时满足开放物品集处理与低延迟要求。如何通过真实部署数据自主学习以提升性能是关键挑战。

Method: 采用多模态输入（RGB/深度/语义分割）评估候选抓取点质量，结合多模态预训练与微调策略。通过大规模物品抓取数据集、含部分遮挡的数据集及包裹抓取数据集进行实验，分析模态影响与预训练效果。

Result: 实验表明多模态训练显著提升性能，预训练使模型能学习模态间关系，微调后仅需部分模态即可推理。消融实验验证了领域预训练的重要性及深度信息的关键作用。

Conclusion: 多模态联合训练与预训练策略能有效提升抓取预测性能，同时通过模态关系学习实现推理时输入模态的灵活性，为工业级机器人分拣提供高效解决方案。

Abstract: This work demonstrates how autonomously learning aspects of robotic operation
from sparsely-labeled, real-world data of deployed, engineered solutions at
industrial scale can provide with solutions that achieve improved performance.
Specifically, it focuses on multi-suction robot picking and performs a
comprehensive study on the application of multi-modal visual encoders for
predicting the success of candidate robotic picks. Picking diverse items from
unstructured piles is an important and challenging task for robot manipulation
in real-world settings, such as warehouses. Methods for picking from clutter
must work for an open set of items while simultaneously meeting latency
constraints to achieve high throughput. The demonstrated approach utilizes
multiple input modalities, such as RGB, depth and semantic segmentation, to
estimate the quality of candidate multi-suction picks. The strategy is trained
from real-world item picking data, with a combination of multimodal pretrain
and finetune. The manuscript provides comprehensive experimental evaluation
performed over a large item-picking dataset, an item-picking dataset targeted
to include partial occlusions, and a package-picking dataset, which focuses on
containers, such as boxes and envelopes, instead of unpackaged items. The
evaluation measures performance for different item configurations, pick scenes,
and object types. Ablations help to understand the effects of in-domain
pretraining, the impact of different modalities and the importance of
finetuning. These ablations reveal both the importance of training over
multiple modalities but also the ability of models to learn during pretraining
the relationship between modalities so that during finetuning and inference,
only a subset of them can be used as input.

</details>


### [240] [Grounded Vision-Language Navigation for UAVs with Open-Vocabulary Goal Understanding](https://arxiv.org/abs/2506.10756)
*Yuhang Zhang,Haosheng Yu,Jiaping Xiao,Mir Feroskhan*

Main category: cs.RO

TL;DR: 本文提出VLFly框架，通过结合大语言模型和视觉语言模型，使无人机仅依赖单目视觉实现开放词汇的连续空间语言指令导航，解决了传统视觉语言导航的分布外泛化与离散动作空间限制问题。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言导航存在两大瓶颈：1) 对分布外环境的泛化能力不足；2) 依赖固定离散动作空间。这些限制阻碍了无人机在复杂开放环境中的自然语言交互导航能力。

Method: VLFly包含三模块：1) 基于LLM的指令编码器将自然语言转化为结构化提示；2) 基于VLM的目标检索器通过视觉语言相似度匹配目标图像；3) 路径规划器生成连续速度指令。系统仅需单目视觉输入，无需定位传感器。

Result: 在未微调情况下，VLFly在多种仿真环境中全面超越基线方法。真实场景测试显示其具备：开放词汇目标理解、抽象指令处理能力，以及室内外环境的鲁棒连续导航性能。

Conclusion: VLFly通过多模态模型融合，首次实现了无定位传感器的无人机连续动作空间语言导航，为开放环境中的智能体自然语言交互导航提供了新范式。

Abstract: Vision-and-language navigation (VLN) is a long-standing challenge in
autonomous robotics, aiming to empower agents with the ability to follow human
instructions while navigating complex environments. Two key bottlenecks remain
in this field: generalization to out-of-distribution environments and reliance
on fixed discrete action spaces. To address these challenges, we propose
Vision-Language Fly (VLFly), a framework tailored for Unmanned Aerial Vehicles
(UAVs) to execute language-guided flight. Without the requirement for
localization or active ranging sensors, VLFly outputs continuous velocity
commands purely from egocentric observations captured by an onboard monocular
camera. The VLFly integrates three modules: an instruction encoder based on a
large language model (LLM) that reformulates high-level language into
structured prompts, a goal retriever powered by a vision-language model (VLM)
that matches these prompts to goal images via vision-language similarity, and a
waypoint planner that generates executable trajectories for real-time UAV
control. VLFly is evaluated across diverse simulation environments without
additional fine-tuning and consistently outperforms all baselines. Moreover,
real-world VLN tasks in indoor and outdoor environments under direct and
indirect instructions demonstrate that VLFly achieves robust open-vocabulary
goal understanding and generalized navigation capabilities, even in the
presence of abstract language input.

</details>


### [241] [Data-Driven Prediction of Dynamic Interactions Between Robot Appendage and Granular Material](https://arxiv.org/abs/2506.10875)
*Guanjin Wang,Xiangxue Zhao,Shapour Azarm,Balakumar Balachandran*

Main category: cs.RO

TL;DR: 本文提出了一种结合降维、代理建模和数据同化的数据驱动方法，用于高效预测机器人在颗粒地形中的运动交互，显著减少计算时间并保持预测准确性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决高保真物理模拟计算成本高的问题，提升机器人在未知复杂地形中导航的效率和预测能力。

Method: 整合降维技术（ST-HOSVD）、高斯过程代理模型和降阶粒子滤波数据同化，利用离线仿真数据与稀疏实验数据进行在线建模。

Result: 数据驱动方法计算时间减少多个数量级，仅用仿真数据时预测精度与仿真相当；结合实验数据后，长期预测性能优于纯仿真，并能复现物理模拟的阻力缩放关系。

Conclusion: 该方法为机器人导航提供了高效且可扩展的预测工具，兼具在线与离线应用潜力，支持复杂地形下的实时决策与长期规划。

Abstract: An alternative data-driven modeling approach has been proposed and employed
to gain fundamental insights into robot motion interaction with granular
terrain at certain length scales. The approach is based on an integration of
dimension reduction (Sequentially Truncated Higher-Order Singular Value
Decomposition), surrogate modeling (Gaussian Process), and data assimilation
techniques (Reduced Order Particle Filter). This approach can be used online
and is based on offline data, obtained from the offline collection of
high-fidelity simulation data and a set of sparse experimental data. The
results have shown that orders of magnitude reduction in computational time can
be obtained from the proposed data-driven modeling approach compared with
physics-based high-fidelity simulations. With only simulation data as input,
the data-driven prediction technique can generate predictions that have
comparable accuracy as simulations. With both simulation data and sparse
physical experimental measurement as input, the data-driven approach with its
embedded data assimilation techniques has the potential in outperforming only
high-fidelity simulations for the long-horizon predictions. In addition, it is
demonstrated that the data-driven modeling approach can also reproduce the
scaling relationship recovered by physics-based simulations for maximum
resistive forces, which may indicate its general predictability beyond a
case-by-case basis. The results are expected to help robot navigation and
exploration in unknown and complex terrains during both online and offline
phases.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [242] [VQC-MLPNet: An Unconventional Hybrid Quantum-Classical Architecture for Scalable and Robust Quantum Machine Learning](https://arxiv.org/abs/2506.10275)
*Jun Qi,Chao-Han Yang,Pin-Yu Chen,Min-Hsiu Hsieh*

Main category: quant-ph

TL;DR: 本文提出VQC-MLPNet混合量子-经典架构，通过量子电路动态生成经典MLP参数，克服变分量子电路（VQC）的表达性限制、优化难题与噪声敏感问题。理论证明其表示能力随量子资源指数提升，实验验证其在噪声环境下的稳健性。


<details>
  <summary>Details</summary>
Motivation: 传统变分量子电路（VQC）因线性表达性不足、优化困难及硬件噪声敏感性，限制了量子机器学习实际应用。需构建兼具高表达力与抗噪能力的混合架构以突破瓶颈。

Method: VQC-MLPNet利用振幅编码与参数化量子操作，使量子电路动态生成经典多层感知机（MLP）权重。结合统计学习理论与神经正切核分析，推导近似误差、均匀偏差与优化误差的理论上界。

Result: 理论证明量子比特数与电路深度对表示能力呈指数级提升，实验在量子点电荷态分类与基因组结合位点预测任务中，即使模拟IBM量子噪声仍保持稳健性能，显著优于现有量子及混合架构。

Conclusion: VQC-MLPNet为NISQ时代非常规计算范式提供了理论可靠且实践鲁棒的量子增强学习框架，通过量子-经典协同设计突破单一量子系统的固有局限。

Abstract: Variational Quantum Circuits (VQCs) offer a novel pathway for quantum machine
learning, yet their practical application is hindered by inherent limitations
such as constrained linear expressivity, optimization challenges, and acute
sensitivity to quantum hardware noise. This work introduces VQC-MLPNet, a
scalable and robust hybrid quantum-classical architecture designed to overcome
these obstacles. By innovatively employing quantum circuits to dynamically
generate parameters for classical Multi-Layer Perceptrons (MLPs) via amplitude
encoding and parameterized quantum operations, VQC-MLPNet substantially expands
representation capabilities and augments training stability. We provide
rigorous theoretical guarantees via statistical learning techniques and Neural
Tangent Kernel analysis, explicitly deriving upper bounds on approximation,
uniform deviation, and optimization errors. These theoretical insights
demonstrate exponential improvements in representation capacity relative to
quantum circuit depth and the number of qubits, providing clear computational
advantages over standalone quantum circuits and existing hybrid quantum
architectures. Our theoretical claims are empirically corroborated through
extensive experiments, including classifying semiconductor quantum-dot charge
states and predicting genomic transcription factor binding sites, demonstrating
resilient performance even under realistic IBM quantum noise simulations. This
research establishes a theoretically sound and practically robust framework,
advancing the frontiers of quantum-enhanced learning for unconventional
computing paradigms in the Noisy Intermediate-Scale Quantum era and beyond.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [243] [On feature selection in double-imbalanced data settings: a Random Forest approach](https://arxiv.org/abs/2506.10929)
*Fabio Demaria*

Main category: stat.ME

TL;DR: 本文提出了一种基于最小深度的新型特征选择阈值方案，用于解决双不平衡条件下随机森林特征选择不稳定的问题，实验证明其有效性和简洁性。


<details>
  <summary>Details</summary>
Motivation: 在高维分类任务中，传统随机森林特征选择方法在双不平衡（类别不平衡和维度不对称n≫p）场景下易产生不稳定或误导性结果，亟需改进。

Method: 通过利用随机森林的树拓扑结构，设计基于最小深度的阈值化特征选择方案，评估变量相关性以提高选择稳定性。

Result: 在模拟和真实数据集上的实验表明，该方法相比传统最小深度方法能选择更精简、准确的变量子集。

Conclusion: 该方法为双不平衡条件下的随机森林特征选择提供了可解释、实用的解决方案，显著提升了特征选择的可靠性与效率。

Abstract: Feature selection is a critical step in high-dimensional classification
tasks, particularly under challenging conditions of double imbalance, namely
settings characterized by both class imbalance in the response variable and
dimensional asymmetry in the data $(n \gg p)$. In such scenarios, traditional
feature selection methods applied to Random Forests (RF) often yield unstable
or misleading importance rankings. This paper proposes a novel thresholding
scheme for feature selection based on minimal depth, which exploits the tree
topology to assess variable relevance. Extensive experiments on simulated and
real-world datasets demonstrate that the proposed approach produces more
parsimonious and accurate subsets of variables compared to conventional minimal
depth-based selection. The method provides a practical and interpretable
solution for variable selection in RF under double imbalance conditions.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [244] [DUN-SRE: Deep Unrolling Network with Spatiotemporal Rotation Equivariance for Dynamic MRI Reconstruction](https://arxiv.org/abs/2506.10309)
*Yuliang Zhu,Jing Cheng,Qi Xie,Zhuo-Xu Cui,Qingyong Zhu,Yuanyuan Liu,Xin Liu,Jianfeng Ren,Chengbo Wang,Dong Liang*

Main category: eess.IV

TL;DR: 提出DUN-SRE网络，通过整合时空旋转等变性约束，在动态MRI重建中实现更精确的物理建模，显著提升图像质量并保持旋转对称结构。


<details>
  <summary>Details</summary>
Motivation: 现有等变卷积神经网络（ECNN）虽能利用空间对称性，但无法建模动态MRI中关键的时间对称性，导致重建质量受限。

Method: 构建(2+1)D等变卷积架构，将数据一致性模块与近端映射模块统一于深度展开框架，并通过高保真组滤波器参数化机制保持表示精度。

Result: 在心脏CINE MRI数据集上取得SOTA性能，尤其在保留旋转对称结构方面表现突出，并展现强泛化能力。

Conclusion: DUN-SRE通过严格传播时空对称约束，为动态MRI重建任务提供了物理准确性更高的解决方案。

Abstract: Dynamic Magnetic Resonance Imaging (MRI) exhibits transformation symmetries,
including spatial rotation symmetry within individual frames and temporal
symmetry along the time dimension. Explicit incorporation of these symmetry
priors in the reconstruction model can significantly improve image quality,
especially under aggressive undersampling scenarios. Recently, Equivariant
convolutional neural network (ECNN) has shown great promise in exploiting
spatial symmetry priors. However, existing ECNNs critically fail to model
temporal symmetry, arguably the most universal and informative structural prior
in dynamic MRI reconstruction. To tackle this issue, we propose a novel Deep
Unrolling Network with Spatiotemporal Rotation Equivariance (DUN-SRE) for
Dynamic MRI Reconstruction. The DUN-SRE establishes spatiotemporal equivariance
through a (2+1)D equivariant convolutional architecture. In particular, it
integrates both the data consistency and proximal mapping module into a unified
deep unrolling framework. This architecture ensures rigorous propagation of
spatiotemporal rotation symmetry constraints throughout the reconstruction
process, enabling more physically accurate modeling of cardiac motion dynamics
in cine MRI. In addition, a high-fidelity group filter parameterization
mechanism is developed to maintain representation precision while enforcing
symmetry constraints. Comprehensive experiments on Cardiac CINE MRI datasets
demonstrate that DUN-SRE achieves state-of-the-art performance, particularly in
preserving rotation-symmetric structures, offering strong generalization
capability to a broad range of dynamic MRI reconstruction tasks.

</details>


### [245] [SWDL: Stratum-Wise Difference Learning with Deep Laplacian Pyramid for Semi-Supervised 3D Intracranial Hemorrhage Segmentation](https://arxiv.org/abs/2506.10325)
*Cheng Wang,Siqi Chen,Donghua Mi,Yang Chen,Yudong Zhang,Yinsheng Li*

Main category: eess.IV

TL;DR: 本文提出了一种新型半监督学习框架SWDL-Net，通过结合拉普拉斯金字塔的边缘锐化与深度卷积的细节增强，在仅需2%标注数据的颅内出血分割任务中超越现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 颅内出血(ICH)医学图像标注成本高且耗时长，传统深度学习方法依赖大量标注数据。现有半监督学习方法主要关注高置信度伪标签或一致性约束，未能有效利用图像细节与边缘特征的互补性。

Method: SWDL-Net框架创新性地融合拉普拉斯金字塔（擅长边缘锐化）和深度卷积上采样（增强细节精度），通过差异学习机制整合二者的互补优势，实现病变细节与边界的精准分割。

Result: 在271例ICH数据集上，仅使用2%标注数据即达到SOTA性能。在公开BHSD数据集5%标注数据场景下也验证了方法优越性，代码与数据已开源。

Conclusion: 通过协同利用多尺度特征表示与深度特征映射的互补性，SWDL-Net为小样本医学图像分割提供了新思路，在降低标注成本的同时提升了细粒度分割性能。

Abstract: Recent advances in medical imaging have established deep learning-based
segmentation as the predominant approach, though it typically requires large
amounts of manually annotated data. However, obtaining annotations for
intracranial hemorrhage (ICH) remains particularly challenging due to the
tedious and costly labeling process. Semi-supervised learning (SSL) has emerged
as a promising solution to address the scarcity of labeled data, especially in
volumetric medical image segmentation. Unlike conventional SSL methods that
primarily focus on high-confidence pseudo-labels or consistency regularization,
we propose SWDL-Net, a novel SSL framework that exploits the complementary
advantages of Laplacian pyramid and deep convolutional upsampling. The
Laplacian pyramid excels at edge sharpening, while deep convolutions enhance
detail precision through flexible feature mapping. Our framework achieves
superior segmentation of lesion details and boundaries through a difference
learning mechanism that effectively integrates these complementary approaches.
Extensive experiments on a 271-case ICH dataset and public benchmarks
demonstrate that SWDL-Net outperforms current state-of-the-art methods in
scenarios with only 2% labeled data. Additional evaluations on the publicly
available Brain Hemorrhage Segmentation Dataset (BHSD) with 5% labeled data
further confirm the superiority of our approach. Code and data have been
released at https://github.com/SIAT-CT-LAB/SWDL.

</details>


### [246] [Generalist Models in Medical Image Segmentation: A Survey and Performance Comparison with Task-Specific Approaches](https://arxiv.org/abs/2506.10825)
*Andrea Moglia,Matteo Leccardi,Matteo Cavicchioli,Alice Maccarini,Marco Marcon,Luca Mainardi,Pietro Cerveri*

Main category: eess.IV

TL;DR: 本文综述了医学图像分割中通用模型（如SAM）的应用，系统分类了不同方法，分析其性能并与任务专用模型对比，探讨了法规、隐私、预算及未来方向。


<details>
  <summary>Details</summary>
Motivation: 受大型语言模型预训练与微调范式启发，通用模型（如SAM）在自然图像分割中取得突破，推动其在医学图像分割中的应用。本文旨在全面分析此类模型的进展、性能及挑战。

Method: 通过综述方法，系统分类SAM的零样本、少样本、微调、适配器等变体，分析SAM 2及其他基于图像或图文多模态的创新模型，并与现有任务专用模型进行性能评估。

Result: 通用模型在医学分割中展现潜力，但性能仍受限于数据异质性、标注成本及领域差异。部分变体在特定任务中接近或超越传统模型，但需解决合规性、隐私及可信AI等挑战。

Conclusion: 未来需关注合成数据、早期多模态融合、NLP通用模型经验迁移、代理AI与物理AI整合，以及临床转化，同时应对法规、隐私和可信AI等实际挑战。

Abstract: Following the successful paradigm shift of large language models, leveraging
pre-training on a massive corpus of data and fine-tuning on different
downstream tasks, generalist models have made their foray into computer vision.
The introduction of Segment Anything Model (SAM) set a milestone on
segmentation of natural images, inspiring the design of a multitude of
architectures for medical image segmentation. In this survey we offer a
comprehensive and in-depth investigation on generalist models for medical image
segmentation. We start with an introduction on the fundamentals concepts
underpinning their development. Then, we provide a taxonomy on the different
declinations of SAM in terms of zero-shot, few-shot, fine-tuning, adapters, on
the recent SAM 2, on other innovative models trained on images alone, and
others trained on both text and images. We thoroughly analyze their
performances at the level of both primary research and best-in-literature,
followed by a rigorous comparison with the state-of-the-art task-specific
models. We emphasize the need to address challenges in terms of compliance with
regulatory frameworks, privacy and security laws, budget, and trustworthy
artificial intelligence (AI). Finally, we share our perspective on future
directions concerning synthetic data, early fusion, lessons learnt from
generalist models in natural language processing, agentic AI and physical AI,
and clinical translation.

</details>


### [247] [SNR and Resource Adaptive Deep JSCC for Distributed IoT Image Classification](https://arxiv.org/abs/2506.10699)
*Ali Waqas,Sinem Coleri*

Main category: eess.IV

TL;DR: 本文提出了一种基于信噪比和计算资源自适应的分布式CNN框架，结合学习辅助遗传算法（LAIGA），在物联网设备计算资源受限和无线环境动态变化场景下，通过智能优化网络配置显著提升图像分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于固定分割的DNN联合信源信道编码方案缺乏对动态信道条件和设备计算资源的适应性，导致在低信噪比或计算资源受限时性能下降。

Method: 提出SNR-计算双自适应框架，采用学习辅助遗传算法（LAIGA）进行超参数搜索：1) 遗传算法剪枝超出设备FLOPs约束的配置；2) 集成随机森林模型引导搜索方向，注入任务相关偏置；3) 联合优化设备-服务器端网络架构。

Result: 在-10dB低信噪比条件下，相比现有SNR自适应多层框架，分类准确率提升10%。在1M-70M FLOPs计算预算范围内均表现优越，尤其在资源受限场景优势显著。

Conclusion: LAIGA驱动的自适应框架通过智能网络配置搜索，有效平衡计算-通信资源约束与分类性能，为边缘智能系统提供了可扩展的解决方案。

Abstract: Sensor-based local inference at IoT devices faces severe computational
limitations, often requiring data transmission over noisy wireless channels for
server-side processing. To address this, split-network Deep Neural Network
(DNN) based Joint Source-Channel Coding (JSCC) schemes are used to extract and
transmit relevant features instead of raw data. However, most existing methods
rely on fixed network splits and static configurations, lacking adaptability to
varying computational budgets and channel conditions. In this paper, we propose
a novel SNR- and computation-adaptive distributed CNN framework for wireless
image classification across IoT devices and edge servers. We introduce a
learning-assisted intelligent Genetic Algorithm (LAIGA) that efficiently
explores the CNN hyperparameter space to optimize network configuration under
given FLOPs constraints and given SNR. LAIGA intelligently discards the
infeasible network configurations that exceed computational budget at IoT
device. It also benefits from the Random Forests based learning assistance to
avoid a thorough exploration of hyperparameter space and to induce application
specific bias in candidate optimal configurations. Experimental results
demonstrate that the proposed framework outperforms fixed-split architectures
and existing SNR-adaptive methods, especially under low SNR and limited
computational resources. We achieve a 10\% increase in classification accuracy
as compared to existing JSCC based SNR-adaptive multilayer framework at an SNR
as low as -10dB across a range of available computational budget (1M to 70M
FLOPs) at IoT device.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [248] [HPCTransCompile: An AI Compiler Generated Dataset for High-Performance CUDA Transpilation and LLM Preliminary Exploration](https://arxiv.org/abs/2506.10401)
*Jiaqi Lv,Xufeng He,Yanchen Liu,Xu Dai,Yang Hu,Shouyi Yin*

Main category: cs.DC

TL;DR: 本文提出一种利用AI编译器和自动优化技术生成高性能代码对的框架，结合图数据增强和评估基准HPCTransEval，显著提升大语言模型在CUDA代码跨平台转换中的表现，解决CUDA生态兼容性挑战。


<details>
  <summary>Details</summary>
Motivation: CUDA生态在并行计算中占据主导地位，但其他硬件平台需支持CUDA代码的性能可移植性。现有转换方法存在覆盖范围有限、通用性差、开发成本高等问题，且大语言模型因缺乏高质量训练数据在CUDA高性能代码转换中表现欠佳。

Method: 提出基于AI编译器和自动优化技术的框架，生成CUDA与目标平台的高性能代码对；引入图数据增强方法提升数据质量，并构建HPCTransEval基准评估模型性能。以CUDA到CPU代码转换作为案例进行实验验证。

Result: 实验表明，该框架显著提升大语言模型的CUDA代码转换能力，尤其在性能可移植性方面表现突出，验证了LLM解决CUDA生态兼容性问题的潜力。

Conclusion: 结合AI编译器与数据增强的框架有效解决了CUDA代码跨平台挑战，为利用LLM实现高性能代码转换提供了新方向，同时强调高质量训练数据与自动优化技术的关键作用。

Abstract: The rapid growth of deep learning has driven exponential increases in model
parameters and computational demands. NVIDIA GPUs and their CUDA-based software
ecosystem provide robust support for parallel computing, significantly
alleviating computational bottlenecks. Meanwhile, due to the cultivation of
user programming habits and the high performance of GPUs, the CUDA ecosystem
has established a dominant position in the field of parallel software. This
dominance requires other hardware platforms to support CUDA-based software with
performance portability. However, translating CUDA code to other platforms
poses significant challenges due to differences in parallel programming
paradigms and hardware architectures. Existing approaches rely on language
extensions, domain-specific languages (DSLs), or compilers but face limitations
in workload coverage and generalizability. Moreover, these methods often incur
substantial development costs. Recently, LLMs have demonstrated extraordinary
potential in various vertical domains, especially in code-related tasks.
However, the performance of existing LLMs in CUDA transpilation, particularly
for high-performance code, remains suboptimal. The main reason for this
limitation lies in the lack of high-quality training datasets. To address these
challenges, we propose a novel framework for generating high-performance CUDA
and corresponding platform code pairs, leveraging AI compiler and automatic
optimization technology. We further enhance the framework with a graph-based
data augmentation method and introduce HPCTransEval, a benchmark for evaluating
LLM performance on CUDA transpilation. We conduct experiments using CUDA-to-CPU
transpilation as a case study on leading LLMs. The result demonstrates that our
framework significantly improves CUDA transpilation, highlighting the potential
of LLMs to address compatibility challenges within the CUDA ecosystem.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [249] [scSSL-Bench: Benchmarking Self-Supervised Learning for Single-Cell Data](https://arxiv.org/abs/2506.10031)
*Olga Ovcharenko,Florian Barkmann,Philip Toma,Imant Daunhawer,Julia Vogt,Sebastian Schelter,Valentina Boeva*

Main category: q-bio.QM

TL;DR: 本文提出了scSSL-Bench基准测试，系统评估19种自监督学习方法在单细胞数据中的表现，发现任务需不同方法，随机掩码增强效果最佳，并呼吁开发专用多模态整合框架。


<details>
  <summary>Details</summary>
Motivation: 为深入理解自监督学习（SSL）在单细胞数据分析中的效果，需系统评估现有方法在不同任务中的表现，并为实际应用提供指导。

Method: 构建scSSL-Bench基准，覆盖9个数据集，评估19种SSL方法在批次校正、细胞注释、多模态预测三个任务中的性能，并分析数据增强策略。

Result: 任务特异性明显：scVI等专用框架在批次校正中表现最佳，通用SSL方法（如VICReg）在细胞分型和多模态整合中更优；随机掩码增强效果超越领域专用增强方法。

Conclusion: 需开发单细胞专用多模态整合框架；scSSL-Bench为SSL在单细胞分析中的应用提供标准化评估平台与具体建议，推动深度学习与单细胞基因组学的融合。

Abstract: Self-supervised learning (SSL) has proven to be a powerful approach for
extracting biologically meaningful representations from single-cell data. To
advance our understanding of SSL methods applied to single-cell data, we
present scSSL-Bench, a comprehensive benchmark that evaluates nineteen SSL
methods. Our evaluation spans nine datasets and focuses on three common
downstream tasks: batch correction, cell type annotation, and missing modality
prediction. Furthermore, we systematically assess various data augmentation
strategies. Our analysis reveals task-specific trade-offs: the specialized
single-cell frameworks, scVI, CLAIRE, and the finetuned scGPT excel at
uni-modal batch correction, while generic SSL methods, such as VICReg and
SimCLR, demonstrate superior performance in cell typing and multi-modal data
integration. Random masking emerges as the most effective augmentation
technique across all tasks, surpassing domain-specific augmentations. Notably,
our results indicate the need for a specialized single-cell multi-modal data
integration framework. scSSL-Bench provides a standardized evaluation platform
and concrete recommendations for applying SSL to single-cell analysis,
advancing the convergence of deep learning and single-cell genomics.

</details>


### [250] [Predicting function of evolutionarily implausible DNA sequences](https://arxiv.org/abs/2506.10271)
*Shiyu Jiang,Xuyin Liu,Zitong Jerry Wang*

Main category: q-bio.QM

TL;DR: 基因组语言模型（gLMs）预测突变效应时需同时考虑序列可能性和长度，其性能与未突变序列的预测可能性强相关，且受序列长度影响显著。


<details>
  <summary>Details</summary>
Motivation: 现有基因组语言模型在生成功能性DNA序列时，需兼顾进化合理性与序列-功能关系。研究旨在评估模型预测功能丧失突变的能力，以提升合成生物学应用效果。

Method: 提出名为Nullsettes的预测任务，通过模拟关键调控元件易位产生的功能丧失突变，测试12个前沿模型，分析突变效应预测性能与未突变序列可能性、序列长度的关联。

Result: 模型预测突变效应的能力与未突变序列的预测可能性高度正相关，且有效预测区间受序列长度显著影响。

Conclusion: 使用基因组语言模型预测突变效应时，需综合考量序列可能性和长度，二者共同决定模型性能，为合成生物学设计提供关键指导。

Abstract: Genomic language models (gLMs) show potential for generating novel,
functional DNA sequences for synthetic biology, but doing so requires them to
learn not just evolutionary plausibility, but also sequence-to-function
relationships. We introduce a set of prediction tasks called Nullsettes, which
assesses a model's ability to predict loss-of-function mutations created by
translocating key control elements in synthetic expression cassettes. Across 12
state-of-the-art models, we find that mutation effect prediction performance
strongly correlates with the predicted likelihood of the nonmutant.
Furthermore, the range of likelihood values predictive of strong model
performance is highly dependent on sequence length. Our work highlights the
importance of considering both sequence likelihood and sequence length when
using gLMs for mutation effect prediction.

</details>


### [251] [A Goemans-Williamson type algorithm for identifying subcohorts in clinical trials](https://arxiv.org/abs/2506.10879)
*Pratik Worah*

Main category: q-bio.QM

TL;DR: 提出一种高效算法，通过近似优化技术识别大型异质数据集中的同质子集，应用于乳腺癌患者亚群分析及甲基化与基因表达关联研究。


<details>
  <summary>Details</summary>
Motivation: 异质数据集中识别同质子集（如特定患者亚群）对疾病机制研究和精准治疗开发至关重要，但现有方法效率不足。

Method: 设计基于Goemans-Williamson舍入技术的线性分类器算法，以0.82近似比逼近优化问题最优解。

Result: 成功从乳腺癌RNA数据中识别转移病例主导的亚群，并发现肿瘤抑制基因甲基化与核受体表达的统计学显著共变子集。

Conclusion: 该方法可为疾病通路发现和亚群特异性治疗提供有效工具，验证了算法在生物医学数据分析中的实用性。

Abstract: We design an efficient algorithm that outputs a linear classifier for
identifying homogeneous subsets (equivalently subcohorts) from large
inhomogeneous datasets. Our theoretical contribution is a rounding technique,
similar to that of Goemans and Williamson (1994), that approximates the optimal
solution of the underlying optimization problem within a factor of $0.82$. As
an application, we use our algorithm to design a simple test that can identify
homogeneous subcohorts of patients, that are mainly comprised of metastatic
cases, from the RNA microarray dataset for breast cancer by Curtis et al.
(2012). Furthermore, we also use the test output by the algorithm to
systematically identify subcohorts of patients in which statistically
significant changes in methylation levels of tumor suppressor genes co-occur
with statistically significant changes in nuclear receptor expression.
Identifying such homogeneous subcohorts of patients can be useful for the
discovery of disease pathways and therapeutics, specific to the subcohort.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [252] [AI5GTest: AI-Driven Specification-Aware Automated Testing and Validation of 5G O-RAN Components](https://arxiv.org/abs/2506.10111)
*Abiodun Ganiyu,Pranshav Gajjar,Vijay K Shah*

Main category: cs.NI

TL;DR: AI5GTest是一个基于大语言模型（LLM）的自动化测试框架，通过Gen-LLM、Val-LLM和Debug-LLM协作，解决O-RAN多厂商组件测试中人工流程效率低、易出错的问题，并引入人机协同机制提升可信度。实验表明其显著减少测试时间且保持高准确性。


<details>
  <summary>Details</summary>
Motivation: O-RAN的开放架构虽促进创新，但其多厂商组件的测试依赖传统人工流程，存在碎片化、易出错及扩展性差的问题。现有OTICs框架无法高效满足3GPP和O-RAN规范的自动化验证需求。

Method: 提出AI5GTest框架：1) Gen-LLM根据规范自动生成测试流程；2) Val-LLM通过信号消息交叉验证合规性；3) Debug-LLM分析异常根因；4) 人机协同机制要求Gen-LLM在验证前提交Top-k相关规范供人工审核，确保透明性。

Result: 基于O-RAN TIFG和WG5-IOT测试案例的实验显示，AI5GTest相比传统人工方法显著缩短测试时间，同时保持高验证准确率。

Conclusion: AI5GTest通过LLM协作和人机协同机制，有效解决了O-RAN测试的自动化与一致性问题，为多厂商环境下的规范验证提供了高效、可信的解决方案。

Abstract: The advent of Open Radio Access Networks (O-RAN) has transformed the
telecommunications industry by promoting interoperability, vendor diversity,
and rapid innovation. However, its disaggregated architecture introduces
complex testing challenges, particularly in validating multi-vendor components
against O-RAN ALLIANCE and 3GPP specifications. Existing frameworks, such as
those provided by Open Testing and Integration Centres (OTICs), rely heavily on
manual processes, are fragmented and prone to human error, leading to
inconsistency and scalability issues. To address these limitations, we present
AI5GTest -- an AI-powered, specification-aware testing framework designed to
automate the validation of O-RAN components. AI5GTest leverages a cooperative
Large Language Models (LLM) framework consisting of Gen-LLM, Val-LLM, and
Debug-LLM. Gen-LLM automatically generates expected procedural flows for test
cases based on 3GPP and O-RAN specifications, while Val-LLM cross-references
signaling messages against these flows to validate compliance and detect
deviations. If anomalies arise, Debug-LLM performs root cause analysis,
providing insight to the failure cause. To enhance transparency and
trustworthiness, AI5GTest incorporates a human-in-the-loop mechanism, where the
Gen-LLM presents top-k relevant official specifications to the tester for
approval before proceeding with validation. Evaluated using a range of test
cases obtained from O-RAN TIFG and WG5-IOT test specifications, AI5GTest
demonstrates a significant reduction in overall test execution time compared to
traditional manual methods, while maintaining high validation accuracy.

</details>


### [253] [Agentic Semantic Control for Autonomous Wireless Space Networks: Extending Space-O-RAN with MCP-Driven Distributed Intelligence](https://arxiv.org/abs/2506.10925)
*Eduardo Baena,Paolo Testolina,Michele Polese,Sergi Aliaga,Andrew Benincasa,Dimitrios Koutsonikolas,Josep Jornet,Tommaso Melodia*

Main category: cs.NI

TL;DR: 本文提出了一种基于语义代理层和MCP/A2A协议的新型月球无线通信系统扩展方案，通过分布式认知代理实现上下文感知决策，优化延迟和带宽适应性。


<details>
  <summary>Details</summary>
Motivation: 现有Space-O-RAN模型在月球表面通信中存在静态策略限制和语义集成不足的问题，无法满足动态环境与任务需求。

Method: 引入语义代理层，结合MCP协议和A2A通信协议，在探测器、基站等节点部署分布式认知代理，实现跨实时/非实时控制层的上下文感知协调策略（如延迟自适应推理和语义压缩）。

Result: 系统能够通过多MCP服务器交互，综合遥测数据、运动规划和任务约束进行动态决策，提升通信系统的自适应性和鲁棒性。

Conclusion: 所提出的语义代理架构有效增强了月球无线通信的上下文感知能力，为复杂太空任务提供了可扩展的智能通信基础设施。

Abstract: Lunar surface operations impose stringent requirements on wireless
communication systems, including autonomy, robustness to disruption, and the
ability to adapt to environmental and mission-driven context. While Space-O-RAN
provides a distributed orchestration model aligned with 3GPP standards, its
decision logic is limited to static policies and lacks semantic integration. We
propose a novel extension incorporating a semantic agentic layer enabled by the
Model Context Protocol (MCP) and Agent-to-Agent (A2A) communication protocols,
allowing context-aware decision making across real-time, near-real-time, and
non-real-time control layers. Distributed cognitive agents deployed in rovers,
landers, and lunar base stations implement wireless-aware coordination
strategies, including delay-adaptive reasoning and bandwidth-aware semantic
compression, while interacting with multiple MCP servers to reason over
telemetry, locomotion planning, and mission constraints.

</details>


### [254] [Energy-Efficient Deep Learning for Traffic Classification on Microcontrollers](https://arxiv.org/abs/2506.10851)
*Adel Chehade,Edoardo Ragusa,Paolo Gastaldo,Rodolfo Zunino*

Main category: cs.NI

TL;DR: 本文提出一种基于硬件感知神经架构搜索的轻量级1D-CNN模型，可在资源受限的物联网微控制器上实现高精度、低能耗的加密流量分类，量化后仅损失1-2%准确率，在STM32和Nucleo开发板分别实现7.86mJ/29.10mJ的单次推理能耗。


<details>
  <summary>Details</summary>
Motivation: 解决物联网设备因计算资源有限导致的加密流量分类难题，在保证精度的同时实现低延迟、低功耗的端侧部署。

Method: 采用硬件感知神经架构搜索(HW-NAS)优化轻量级1D-CNN结构，通过INT8量化压缩模型，并在STM32F746G-DISCO/Nucleo-F401RE微控制器进行部署验证。

Result: 模型在ISCX数据集达96.59%准确率(88.26K参数/10.08M FLOPs)，量化后精度损失<2%，在两种微控制器上分别实现31.43ms/115.40ms推理延迟，能耗7.86mJ/29.10mJ。

Conclusion: 该方法证明了在端侧设备进行加密流量分析的可行性，为可扩展、低功耗的物联网安全方案提供了实践基础。

Abstract: In this paper, we present a practical deep learning (DL) approach for
energy-efficient traffic classification (TC) on resource-limited
microcontrollers, which are widely used in IoT-based smart systems and
communication networks. Our objective is to balance accuracy, computational
efficiency, and real-world deployability. To that end, we develop a lightweight
1D-CNN, optimized via hardware-aware neural architecture search (HW-NAS), which
achieves 96.59% accuracy on the ISCX VPN-NonVPN dataset with only 88.26K
parameters, a 20.12K maximum tensor size, and 10.08M floating-point operations
(FLOPs). Moreover, it generalizes across various TC tasks, with accuracies
ranging from 94% to 99%. To enable deployment, the model is quantized to INT8,
suffering only a marginal 1-2% accuracy drop relative to its Float32
counterpart. We evaluate real-world inference performance on two
microcontrollers: the high-performance STM32F746G-DISCO and the cost-sensitive
Nucleo-F401RE. The deployed model achieves inference latencies of 31.43ms and
115.40ms, with energy consumption of 7.86 mJ and 29.10 mJ per inference,
respectively. These results demonstrate the feasibility of on-device encrypted
traffic analysis, paving the way for scalable, low-power IoT security
solutions.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [255] [The Gittins Index: A Design Principle for Decision-Making Under Uncertainty](https://arxiv.org/abs/2506.10872)
*Ziv Scully,Alexander Terenin*

Main category: math.OC

TL;DR: 本文通过实例驱动的方法，展示了Gittins指数在解决决策问题中的实际应用潜力，包括贝叶斯优化和队列尾部延迟最小化，即使次优情况下仍表现优异。


<details>
  <summary>Details</summary>
Motivation: Gittins指数虽在理论上能解决不确定性决策问题，但其实际应用常被忽视。本文旨在证明其在实际问题中的有效性，打破其仅作为理论工具的固有认知。

Method: 以实例为引导，系统介绍Gittins指数的定义与应用，并分析其在多类问题中的表现（包括最优解与次优解场景），重点探讨贝叶斯优化和队列尾部延迟最小化的应用方法。

Result: Gittins指数在贝叶斯优化和队列尾部延迟最小化中展现出卓越性能，即使作为次优解时仍能高效解决问题，验证了其作为实用工具的潜力。

Conclusion: Gittins指数不仅具有理论价值，还可作为实际决策问题的有效工具，尤其在次优场景下仍能提供高性能解决方案，扩展了其应用边界。

Abstract: The Gittins index is a tool that optimally solves a variety of
decision-making problems involving uncertainty, including multi-armed bandit
problems, minimizing mean latency in queues, and search problems like the
Pandora's box model. However, despite the above examples and later extensions
thereof, the space of problems that the Gittins index can solve perfectly
optimally is limited, and its definition is rather subtle compared to those of
other multi-armed bandit algorithms. As a result, the Gittins index is often
regarded as being primarily a concept of theoretical importance, rather than a
practical tool for solving decision-making problems.
  The aim of this tutorial is to demonstrate that the Gittins index can be
fruitfully applied to practical problems. We start by giving an example-driven
introduction to the Gittins index, then walk through several examples of
problems it solves - some optimally, some suboptimally but still with excellent
performance. Two practical highlights in the latter category are applying the
Gittins index to Bayesian optimization, and applying the Gittins index to
minimizing tail latency in queues.

</details>


### [256] [The Gittins Index: A Design Principle for Decision-Making Under Uncertainty](https://arxiv.org/abs/2506.10872)
*Ziv Scully,Alexander Terenin*

Main category: math.OC

TL;DR: 本文通过实例驱动的方法，展示了Gittins指数在解决决策问题中的实际应用潜力，包括贝叶斯优化和队列尾部延迟最小化，即使次优情况下仍表现优异。


<details>
  <summary>Details</summary>
Motivation: Gittins指数虽在理论上能解决不确定性决策问题，但其实际应用常被忽视。本文旨在证明其在实际问题中的有效性，打破其仅作为理论工具的固有认知。

Method: 以实例为引导，系统介绍Gittins指数的定义与应用，并分析其在多类问题中的表现（包括最优解与次优解场景），重点探讨贝叶斯优化和队列尾部延迟最小化的应用方法。

Result: Gittins指数在贝叶斯优化和队列尾部延迟最小化中展现出卓越性能，即使作为次优解时仍能高效解决问题，验证了其作为实用工具的潜力。

Conclusion: Gittins指数不仅具有理论价值，还可作为实际决策问题的有效工具，尤其在次优场景下仍能提供高性能解决方案，扩展了其应用边界。

Abstract: The Gittins index is a tool that optimally solves a variety of
decision-making problems involving uncertainty, including multi-armed bandit
problems, minimizing mean latency in queues, and search problems like the
Pandora's box model. However, despite the above examples and later extensions
thereof, the space of problems that the Gittins index can solve perfectly
optimally is limited, and its definition is rather subtle compared to those of
other multi-armed bandit algorithms. As a result, the Gittins index is often
regarded as being primarily a concept of theoretical importance, rather than a
practical tool for solving decision-making problems.
  The aim of this tutorial is to demonstrate that the Gittins index can be
fruitfully applied to practical problems. We start by giving an example-driven
introduction to the Gittins index, then walk through several examples of
problems it solves - some optimally, some suboptimally but still with excellent
performance. Two practical highlights in the latter category are applying the
Gittins index to Bayesian optimization, and applying the Gittins index to
minimizing tail latency in queues.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [257] [CARE: a Benchmark Suite for the Classification and Retrieval of Enzymes](https://arxiv.org/abs/2406.15669)
*Jason Yang,Ariane Mora,Shengchao Liu,Bruce J. Wittmann,Anima Anandkumar,Frances H. Arnold,Yisong Yue*

Main category: q-bio.BM

TL;DR: 本文提出CARE基准和数据集套件，用于酶的分类与检索任务，并设计不同训练测试划分以评估模型在真实场景中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有酶功能预测的机器学习方法缺乏标准化评估基准，导致方法间难以公平比较。

Method: 构建CARE基准，包含酶分类(EC编号预测)和反应检索(给定反应检索EC编号)两个任务，提出CREEP的对比预训练方法作为检索基线。

Result: 建立了包含分类/检索任务的数据集套件，为分类任务提供SOTA基线，为检索任务提出CREEP方法并与CLIPZyme进行对比。

Conclusion: CARE为酶功能预测领域提供了标准化评估框架，其设计的泛化测试切分和基线方法将促进该领域方法的发展与比较。

Abstract: Enzymes are important proteins that catalyze chemical reactions. In recent
years, machine learning methods have emerged to predict enzyme function from
sequence; however, there are no standardized benchmarks to evaluate these
methods. We introduce CARE, a benchmark and dataset suite for the
Classification And Retrieval of Enzymes (CARE). CARE centers on two tasks: (1)
classification of a protein sequence by its enzyme commission (EC) number and
(2) retrieval of an EC number given a chemical reaction. For each task, we
design train-test splits to evaluate different kinds of out-of-distribution
generalization that are relevant to real use cases. For the classification
task, we provide baselines for state-of-the-art methods. Because the retrieval
task has not been previously formalized, we propose a method called Contrastive
Reaction-EnzymE Pretraining (CREEP) as one of the first baselines for this task
and compare it to the recent method, CLIPZyme. CARE is available at
https://github.com/jsunn-y/CARE/.

</details>


### [258] [Identifying critical residues of a protein using meaningfully-thresholded Random Geometric Graphs](https://arxiv.org/abs/2506.10015)
*Chuqiao Zhang,Sarath Chandra Dantu,Debarghya Mitra,Dalia Chakrabarty*

Main category: q-bio.BM

TL;DR: 本文提出三种基于随机几何图（RGG）的方法识别蛋白质关键残基，并通过分子动力学模拟验证其有效性，最终与实验数据对比。


<details>
  <summary>Details</summary>
Motivation: 蛋白质关键残基对其功能至关重要，但现有识别方法需进一步优化。通过动态模拟与统计模型结合，可更精准定位关键位点。

Method: 1. 使用Cramer's V计算残基间相关性，构建RGG并通过节点度参数化关键性；2. 基于全图与单残基缺失图后验概率差异排名；3. 分析模拟过程中节点度的动态变化。

Result: 三种方法均能有效检测关键残基，其中动态节点度变化与实验验证结果一致性较高，表明统计模型与动力学模拟结合的可行性。

Conclusion: 基于RGG的多参数化方法为关键残基识别提供新思路，动态分析增强预测可靠性，未来可扩展至其他蛋白质系统研究。

Abstract: Identification of critical residues of a protein is actively pursued, since
such residues are essential for protein function. We present three ways of
recognising critical residues of an example protein, the evolution of which is
tracked via molecular dynamical simulations. Our methods are based on learning
a Random Geometric Graph (RGG) variable, where the state variable of each of
156 residues, is attached to a node of this graph, with the RGG learnt using
the matrix of correlations between state variables of each residue-pair. Given
the categorical nature of the state variable, correlation between a residue
pair is computed using Cramer's V. We advance an organic thresholding to learn
an RGG, and compare results against extant thresholding techniques, when
parametrising criticality as the nodal degree in the learnt RGG. Secondly, we
develop a criticality measure by ranking the computed differences between the
posterior probability of the full graph variable defined on all 156 residues,
and that of the graph with all but one residue omitted. A third parametrisation
of criticality informs on the dynamical variation of nodal degrees as the
protein evolves during the simulation. Finally, we compare results obtained
with the three distinct criticality parameters, against
experimentally-ascertained critical residues.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [259] [Prediction of steady states in a marine ecosystem model by a machine learning technique](https://arxiv.org/abs/2506.10475)
*Sarker Miraz Mahfuz,Thomas Slawig*

Main category: physics.ao-ph

TL;DR: 使用条件变分自编码器（CVAE）结合质量校正，将少量生物地球化学参数映射到三维稳态年周期，显著减少海洋生态系统模型spin-up过程的计算时间。


<details>
  <summary>Details</summary>
Motivation: 传统spin-up方法需大量迭代计算以获取稳态解，耗时且计算成本高。研究旨在通过机器学习生成高质量初始值，加速收敛过程。

Method: 利用预计算的稳态数据训练CVAE模型，生成初始值后执行spin-up；对比使用模型预测初始值与均匀初始值、训练数据均值的性能差异。

Result: CVAE预测初始值使spin-up迭代次数减少50-95%（依停止准则而定），远超使用均值的加速效果（后者仅轻微加速）。

Conclusion: CVAE生成的初始值可大幅降低spin-up计算成本，且优于传统均匀初始值与简单均值方法，为复杂模型收敛提供高效解决方案。

Abstract: We used precomputed steady states obtained by a spin-up for a global marine
ecosystem model as training data to build a mapping from the small number of
biogeochemical model parameters onto the three-dimensional converged steady
annual cycle. The mapping was performed by a conditional variational
autoencoder (CVAE) with mass correction. Applied for test data, we show that
the prediction obtained by the CVAE already gives a reasonable good
approximation of the steady states obtained by a regular spin-up. However, the
predictions do not reach the same level of annual periodicity as those obtained
in the original spin-up data. Thus, we took the predictions as initial values
for a spin-up. We could show that the number of necessary iterations,
corresponding to model years, to reach a prescribed stopping criterion in the
spin-up could be significantly reduced compared to the use of the originally
uniform, constant initial value. The amount of reduction depends on the applied
stopping criterion, measuring the periodicity of the solution. The savings in
needed iterations and, thus, computing time for the spin-up ranges from 50 to
95\%, depending on the stopping criterion for the spin-up. We compared these
results with the use of the mean of the training data as an initial value. We
found that this also accelerates the spin-up, but only by a much lower factor.

</details>


### [260] [Pushing the Limits of Extreme Weather: Constructing Extreme Heatwave Storylines with Differentiable Climate Models](https://arxiv.org/abs/2506.10660)
*Tim Whittaker,Alejandro Di Luca*

Main category: physics.ao-ph

TL;DR: 本文提出了一种基于可微分混合气候模型NeuralGCM的新框架，通过优化初始条件生成物理一致的最坏情况热浪轨迹，成功应用于2021年太平洋西北热浪事件，揭示极端温度异常机制。


<details>
  <summary>Details</summary>
Motivation: 现有基于物理模型的大规模集合方法计算成本高且难以模拟罕见极端天气，需开发高效方法评估气候变暖下的极端事件风险上限。

Method: 利用可微分混合气候模型NeuralGCM，通过优化初始条件生成物理一致的最坏情况热浪轨迹。

Result: 在2021年太平洋西北热浪案例中，模型产生比75成员集合极端值高3.7°C的异常温度，揭示大气阻塞增强和罗斯贝波放大特征。

Conclusion: 可微分气候模型能有效探索极端事件概率上限，为构建气候变化下的目标情景提供新方法，支持针对性极端天气叙事分析。

Abstract: Understanding the plausible upper bounds of extreme weather events is
essential for risk assessment in a warming climate. Existing methods, based on
large ensembles of physics-based models, are often computationally expensive or
lack the fidelity needed to simulate rare, high-impact extremes. Here, we
present a novel framework that leverages a differentiable hybrid climate model,
NeuralGCM, to optimize initial conditions and generate physically consistent
worst-case heatwave trajectories. Applied to the 2021 Pacific Northwest
heatwave, our method produces temperature anomalies up to 3.7 $^\circ$C above
the most extreme member of a 75-member ensemble. These trajectories feature
intensified atmospheric blocking and amplified Rossby wave patterns--hallmarks
of severe heat events. Our results demonstrate that differentiable climate
models can efficiently explore the upper tails of event likelihoods, providing
a powerful new approach for constructing targeted storylines of extreme weather
under climate change.

</details>


### [261] [A multi-scale loss formulation for learning a probabilistic model with proper score optimisation](https://arxiv.org/abs/2506.10868)
*Simon Lang,Martin Leutbecher,Pedro Maciel*

Main category: physics.ao-ph

TL;DR: 研究评估了多尺度损失在概率机器学习天气预测模型中的应用，发现其能有效约束小尺度变异性且不影响预测效果，为未来研究提供方向。


<details>
  <summary>Details</summary>
Motivation: 改进概率机器学习天气预测模型对小尺度变异性的约束能力，同时保持整体预测技能，探索多尺度损失在模型训练中的作用。

Method: 在ECMWF开发的AIFS-CRPS模型中引入多尺度损失函数，通过直接优化几乎公平连续排序概率评分(afCRPS)进行训练。

Result: 多尺度损失成功增强小尺度变异性约束，且未对预测技能产生负面影响。

Conclusion: 多尺度损失为天气预测模型训练提供了新思路，未来可进一步探索尺度感知的模型训练方法。

Abstract: We assess the impact of a multi-scale loss formulation for training
probabilistic machine-learned weather forecasting models. The multi-scale loss
is tested in AIFS-CRPS, a machine-learned weather forecasting model developed
at the European Centre for Medium-Range Weather Forecasts (ECMWF). AIFS-CRPS is
trained by directly optimising the almost fair continuous ranked probability
score (afCRPS). The multi-scale loss better constrains small scale variability
without negatively impacting forecast skill. This opens up promising directions
for future work in scale-aware model training.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [262] [From Threat to Tool: Leveraging Refusal-Aware Injection Attacks for Safety Alignment](https://arxiv.org/abs/2506.10020)
*Kyubyung Chae,Hyunbin Jin,Taesup Kim*

Main category: cs.CR

TL;DR: 提出RAAI框架，通过检测拒绝信号和注入预定义短语生成有害补全，无需训练即可提升LLMs安全性，同时保持通用任务能力。


<details>
  <summary>Details</summary>
Motivation: 传统安全对齐方法依赖高成本人工标注数据，现有合成数据方法复杂。需一种简单、通用且无需训练的方法生成合成数据以增强模型安全性。

Method: RAAI框架：利用LLM内部拒绝信号，自适应注入预定义短语，直接生成有害但流畅的补全，无需额外训练或辅助模型。

Result: RAAI使有害响应率从基线2.15%提升至平均61.04%。用其合成数据微调的模型在保持MMLU/ARC性能的同时，抗攻击能力显著增强。

Conclusion: 将LLM攻击方法转化为安全对齐工具，证明RAAI可扩展且可控，为低成本安全训练提供新思路。

Abstract: Safely aligning large language models (LLMs) often demands extensive
human-labeled preference data, a process that's both costly and time-consuming.
While synthetic data offers a promising alternative, current methods frequently
rely on complex iterative prompting or auxiliary models. To address this, we
introduce Refusal-Aware Adaptive Injection (RAAI), a straightforward,
training-free, and model-agnostic framework that repurposes LLM attack
techniques. RAAI works by detecting internal refusal signals and adaptively
injecting predefined phrases to elicit harmful, yet fluent, completions. Our
experiments show RAAI effectively jailbreaks LLMs, increasing the harmful
response rate from a baseline of 2.15% to up to 61.04% on average across four
benchmarks. Crucially, fine-tuning LLMs with the synthetic data generated by
RAAI improves model robustness against harmful prompts while preserving general
capabilities on standard tasks like MMLU and ARC. This work highlights how LLM
attack methodologies can be reframed as practical tools for scalable and
controllable safety alignment.

</details>


### [263] [LLMs Caught in the Crossfire: Malware Requests and Jailbreak Challenges](https://arxiv.org/abs/2506.10022)
*Haoyang Li,Huan Gao,Zhiyuan Zhao,Zhiyu Lin,Junyu Gao,Xuelong Li*

Main category: cs.CR

TL;DR: 研究提出MalwareBench基准数据集，评估主流大语言模型在代码生成任务中抵御越狱攻击的能力，发现其安全性能存在显著缺陷，组合攻击方法进一步降低模型防御效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究对大语言模型（LLMs）在代码生成场景下抵御越狱攻击的脆弱性缺乏系统性评估，需填补该领域安全能力评估的空白。

Method: 构建包含3,520个恶意代码生成提示的MalwareBench数据集，覆盖11种越狱方法和29种代码功能类别，基于320个手动设计的恶意需求进行多维度测试。

Result: 主流LLMs对恶意代码生成请求的平均拒绝率为60.93%，结合越狱攻击算法后骤降至39.92%，表明模型安全机制易被复合攻击策略突破。

Conclusion: 大语言模型在代码生成场景下的安全防护能力存在系统性缺陷，多维度越狱攻击组合显著削弱其防御效果，亟需针对性安全增强方案。

Abstract: The widespread adoption of Large Language Models (LLMs) has heightened
concerns about their security, particularly their vulnerability to jailbreak
attacks that leverage crafted prompts to generate malicious outputs. While
prior research has been conducted on general security capabilities of LLMs,
their specific susceptibility to jailbreak attacks in code generation remains
largely unexplored. To fill this gap, we propose MalwareBench, a benchmark
dataset containing 3,520 jailbreaking prompts for malicious code-generation,
designed to evaluate LLM robustness against such threats. MalwareBench is based
on 320 manually crafted malicious code generation requirements, covering 11
jailbreak methods and 29 code functionality categories. Experiments show that
mainstream LLMs exhibit limited ability to reject malicious code-generation
requirements, and the combination of multiple jailbreak methods further reduces
the model's security capabilities: specifically, the average rejection rate for
malicious content is 60.93%, dropping to 39.92% when combined with jailbreak
attack algorithms. Our work highlights that the code security capabilities of
LLMs still pose significant challenges.

</details>


### [264] [Private Memorization Editing: Turning Memorization into a Defense to Strengthen Data Privacy in Large Language Models](https://arxiv.org/abs/2506.10024)
*Elena Sofia Ruzzetti,Giancarlo A. Xompero,Davide Venditti,Fabio Massimo Zanzotto*

Main category: cs.CR

TL;DR: 本文提出私有记忆编辑（PME）方法，通过检测并编辑大型语言模型（LLMs）记忆的个人身份信息（PII），防止隐私泄露，有效抵御训练数据提取攻击。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型可能记忆未受控数据中的个人身份信息（PII），存在隐私泄露风险，需开发防御策略以解决此问题。

Method: 利用模型对训练数据的记忆能力，检测并编辑已记忆的PII，从而减轻模型对隐私信息的记忆，同时保持其语言能力。

Result: PME显著减少泄露的PII数量，部分场景下隐私攻击准确率降至零，且不影响模型基础性能。

Conclusion: PME成功将LLMs的记忆能力转化为隐私防御工具，提升模型对隐私攻击的鲁棒性，为数据安全提供新思路。

Abstract: Large Language Models (LLMs) memorize, and thus, among huge amounts of
uncontrolled data, may memorize Personally Identifiable Information (PII),
which should not be stored and, consequently, not leaked. In this paper, we
introduce Private Memorization Editing (PME), an approach for preventing
private data leakage that turns an apparent limitation, that is, the LLMs'
memorization ability, into a powerful privacy defense strategy. While attacks
against LLMs have been performed exploiting previous knowledge regarding their
training data, our approach aims to exploit the same kind of knowledge in order
to make a model more robust. We detect a memorized PII and then mitigate the
memorization of PII by editing a model knowledge of its training data. We
verify that our procedure does not affect the underlying language model while
making it more robust against privacy Training Data Extraction attacks. We
demonstrate that PME can effectively reduce the number of leaked PII in a
number of configurations, in some cases even reducing the accuracy of the
privacy attacks to zero.

</details>


### [265] [Evaluation empirique de la sécurisation et de l'alignement de ChatGPT et Gemini: analyse comparative des vulnérabilités par expérimentations de jailbreaks](https://arxiv.org/abs/2506.10029)
*Rafaël Nouailles*

Main category: cs.CR

TL;DR: 本文比较了ChatGPT和Gemini的安全性与对齐水平，分析了越狱技术分类及其实验，探讨了LLMs带来的网络安全挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（如ChatGPT和Gemini）的快速发展引发了新的网络安全问题，包括提示注入攻击、越狱绕过监管、错误信息传播和深度伪造风险，需评估其安全性和应对措施。

Method: 通过对比分析ChatGPT与Gemini的安全性和对齐水平，结合越狱技术分类及实验验证，系统评估模型潜在风险。

Result: 研究揭示了两种模型在安全防护与监管规避上的差异，并提出了针对越狱攻击的具体实验验证结果。

Conclusion: LLMs需持续改进安全对齐机制以应对新型攻击，越狱技术分类与实验为防御策略提供了实证基础。

Abstract: Large Language models (LLMs) are transforming digital usage, particularly in
text generation, image creation, information retrieval and code development.
ChatGPT, launched by OpenAI in November 2022, quickly became a reference,
prompting the emergence of competitors such as Google's Gemini. However, these
technological advances raise new cybersecurity challenges, including prompt
injection attacks, the circumvention of regulatory measures (jailbreaking), the
spread of misinformation (hallucinations) and risks associated with deep fakes.
This paper presents a comparative analysis of the security and alignment levels
of ChatGPT and Gemini, as well as a taxonomy of jailbreak techniques associated
with experiments.

</details>


### [266] [Safeguarding Multimodal Knowledge Copyright in the RAG-as-a-Service Environment](https://arxiv.org/abs/2506.10030)
*Tianyu Chen,Jian Lou,Wenjie Wang*

Main category: cs.CR

TL;DR: 本文提出首个多模态RAG图像知识版权保护框架AQUA，通过嵌入语义信号的水印技术，实现高效、隐蔽且鲁棒的版权追踪，填补该领域空白。


<details>
  <summary>Details</summary>
Motivation: 现有RAG水印方法仅保护文本知识，而服务型平台中共享图像知识面临版权风险，亟需针对多模态的解决方案。

Method: 采用首字母触发器和空间关系线索两种互补方法，将语义水印嵌入合成图像，确保水印在跨模态传播中的存活性与不可感知性。

Result: 跨模型与数据集的实验表明，AQUA水印具备强鲁棒性(存活率>98%)、高隐蔽性(PSNR>40dB)及可靠溯源能力。

Conclusion: AQUA首次实现多模态RAG中图像知识版权保护，为服务化平台贡献数据确权提供关键技术支撑。

Abstract: As Retrieval-Augmented Generation (RAG) evolves into service-oriented
platforms (Rag-as-a-Service) with shared knowledge bases, protecting the
copyright of contributed data becomes essential. Existing watermarking methods
in RAG focus solely on textual knowledge, leaving image knowledge unprotected.
In this work, we propose AQUA, the first watermark framework for image
knowledge protection in Multimodal RAG systems. AQUA embeds semantic signals
into synthetic images using two complementary methods: acronym-based triggers
and spatial relationship cues. These techniques ensure watermark signals
survive indirect watermark propagation from image retriever to textual
generator, being efficient, effective and imperceptible. Experiments across
diverse models and datasets show that AQUA enables robust, stealthy, and
reliable copyright tracing, filling a key gap in multimodal RAG protection.

</details>


### [267] [GenBreak: Red Teaming Text-to-Image Generators Using Large Language Models](https://arxiv.org/abs/2506.10047)
*Zilong Wang,Xiang Zheng,Xiaosen Wang,Bo Wang,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CR

TL;DR: 本文提出GenBreak框架，通过微调红队大语言模型（LLM）生成既能绕过安全机制又能产生高毒性图像的对抗性提示，解决了现有方法在评估防御型T2I模型安全性时的不足。


<details>
  <summary>Details</summary>
Motivation: 现有对抗T2I模型的研究存在局限：部分生成的对抗提示易被安全过滤器拦截，另一些虽能绕过安全机制但无法生成真正有害内容，导致缺乏可靠工具评估模型安全性。

Method: 结合监督微调（基于精选数据集）与强化学习（通过与代理T2I模型交互），通过多奖励信号引导LLM生成兼具规避能力、毒性、语义连贯性和多样性的对抗提示。

Result: 生成的对抗提示在黑盒攻击中显著有效，成功揭示商业T2I生成器的实际安全漏洞，证明其高规避性和毒性。

Conclusion: GenBreak为评估防御型T2I模型安全性提供了可靠工具，同时暴露了现有系统的实际安全风险，强调需加强模型防护机制。

Abstract: Text-to-image (T2I) models such as Stable Diffusion have advanced rapidly and
are now widely used in content creation. However, these models can be misused
to generate harmful content, including nudity or violence, posing significant
safety risks. While most platforms employ content moderation systems,
underlying vulnerabilities can still be exploited by determined adversaries.
Recent research on red-teaming and adversarial attacks against T2I models has
notable limitations: some studies successfully generate highly toxic images but
use adversarial prompts that are easily detected and blocked by safety filters,
while others focus on bypassing safety mechanisms but fail to produce genuinely
harmful outputs, neglecting the discovery of truly high-risk prompts.
Consequently, there remains a lack of reliable tools for evaluating the safety
of defended T2I models. To address this gap, we propose GenBreak, a framework
that fine-tunes a red-team large language model (LLM) to systematically explore
underlying vulnerabilities in T2I generators. Our approach combines supervised
fine-tuning on curated datasets with reinforcement learning via interaction
with a surrogate T2I model. By integrating multiple reward signals, we guide
the LLM to craft adversarial prompts that enhance both evasion capability and
image toxicity, while maintaining semantic coherence and diversity. These
prompts demonstrate strong effectiveness in black-box attacks against
commercial T2I generators, revealing practical and concerning safety
weaknesses.

</details>


### [268] [Disclosure Audits for LLM Agents](https://arxiv.org/abs/2506.10171)
*Saswat Das,Jameson Sandler,Ferdinando Fioretto*

Main category: cs.CR

TL;DR: 本文提出CMPL框架，通过多轮对话压力测试审计对话型AI代理的隐私泄露风险，揭示现有单轮防御机制的不足，并提供量化指标与开放基准。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型代理在持续访问敏感数据时存在隐私泄露风险，传统单轮检测方法无法覆盖多轮交互中的潜在漏洞，需系统性审计工具。

Method: 提出CMPL框架：采用迭代式多轮对话探测策略，模拟真实交互场景，系统性测试严格隐私策略下代理的潜在漏洞。

Result: 跨领域、多模态及不同安全配置的评估表明，CMPL能有效识别现有单轮防御无法检测的隐私风险。

Conclusion: CMPL作为诊断工具，不仅提供量化风险指标和开放基准，还揭示了对话型AI需增强多轮交互隐私防护的必要性。

Abstract: Large Language Model agents have begun to appear as personal assistants,
customer service bots, and clinical aides. While these applications deliver
substantial operational benefits, they also require continuous access to
sensitive data, which increases the likelihood of unauthorized disclosures.
This study proposes an auditing framework for conversational privacy that
quantifies and audits these risks. The proposed Conversational Manipulation for
Privacy Leakage (CMPL) framework, is an iterative probing strategy designed to
stress-test agents that enforce strict privacy directives. Rather than focusing
solely on a single disclosure event, CMPL simulates realistic multi-turn
interactions to systematically uncover latent vulnerabilities. Our evaluation
on diverse domains, data modalities, and safety configurations demonstrate the
auditing framework's ability to reveal privacy risks that are not deterred by
existing single-turn defenses. In addition to introducing CMPL as a diagnostic
tool, the paper delivers (1) an auditing procedure grounded in quantifiable
risk metrics and (2) an open benchmark for evaluation of conversational privacy
across agent implementations.

</details>


### [269] [Prompt Attacks Reveal Superficial Knowledge Removal in Unlearning Methods](https://arxiv.org/abs/2506.10236)
*Yeonwoo Jang,Shariqah Hossain,Ashwin Sreevatsa,Diogo Cruz*

Main category: cs.CR

TL;DR: 研究发现部分机器遗忘方法在简单提示攻击下失效，如ELM在特定攻击下恢复57.3%准确率，需改进评估框架以区分真实知识移除与表面输出抑制。


<details>
  <summary>Details</summary>
Motivation: 验证现有机器遗忘方法在对抗性提示攻击下的鲁棒性，挑战关于遗忘有效性的普遍假设。

Method: 系统评估8种遗忘技术，采用输出分析、logit分析和探针分析，检测未遗忘知识的可检索性。

Result: RMU/TAR表现稳健，但ELM等模型易受特定攻击（如印地语填充文本攻击）。Logit分析显示未遗忘知识未被格式修改隐藏。

Conclusion: 当前遗忘效果评估存在局限，需开发能可靠检测知识真实移除的框架，并提供公开测试工具促进研究。

Abstract: In this work, we show that some machine unlearning methods may fail when
subjected to straightforward prompt attacks. We systematically evaluate eight
unlearning techniques across three model families, and employ output-based,
logit-based, and probe analysis to determine to what extent supposedly
unlearned knowledge can be retrieved. While methods like RMU and TAR
demonstrate robust unlearning, ELM remains vulnerable to specific prompt
attacks (e.g., Hindi filler text in original prompt recovering 57.3% accuracy).
Our logit analysis also confirms that unlearned models are generally not hiding
knowledge by modifying the way the answer is formatted, as the correlation
between output and logit accuracy is strong. These results challenge prevailing
assumptions about unlearning effectiveness and highlight the need for
evaluation frameworks that can reliably distinguish between true knowledge
removal and superficial output suppression. We also publicly make available our
evaluation framework to easily evaluate prompting techniques to retrieve
unlearning knowledge.

</details>


### [270] [SOFT: Selective Data Obfuscation for Protecting LLM Fine-tuning against Membership Inference Attacks](https://arxiv.org/abs/2506.10424)
*Kaiyuan Zhang,Siyuan Cheng,Hanxi Guo,Yuetian Chen,Zian Su,Shengwei An,Yuntao Du,Charles Fleming,Ashish Kundu,Xiangyu Zhang,Ninghui Li*

Main category: cs.CR

TL;DR: 本文首次全面评估了微调大型语言模型（LLMs）在成员推理攻击（MIAs）中的隐私风险，并提出防御方法SOFT，通过选择性数据混淆平衡隐私保护与模型性能。


<details>
  <summary>Details</summary>
Motivation: 微调LLMs常涉及敏感信息，易受MIAs攻击。研究发现MIAs利用微调过程中的损失降低推断成员信息，亟需开发有效防御机制。

Method: 提出SOFT方法：基于有影响力的数据选择进行选择性混淆，通过可调参数动态权衡隐私保护与模型效用，适用于不同场景需求。

Result: 跨6个领域、多架构与规模的实验表明，SOFT显著降低隐私泄露风险，同时保持模型性能，验证其有效性与可扩展性。

Conclusion: SOFT为微调LLMs中的敏感信息保护提供了实用解决方案，平衡隐私与性能，为后续隐私安全研究奠定基础。

Abstract: Large language models (LLMs) have achieved remarkable success and are widely
adopted for diverse applications. However, fine-tuning these models often
involves private or sensitive information, raising critical privacy concerns.
In this work, we conduct the first comprehensive study evaluating the
vulnerability of fine-tuned LLMs to membership inference attacks (MIAs). Our
empirical analysis demonstrates that MIAs exploit the loss reduction during
fine-tuning, making them highly effective in revealing membership information.
These findings motivate the development of our defense. We propose SOFT
(\textbf{S}elective data \textbf{O}bfuscation in LLM
\textbf{F}ine-\textbf{T}uning), a novel defense technique that mitigates
privacy leakage by leveraging influential data selection with an adjustable
parameter to balance utility preservation and privacy protection. Our extensive
experiments span six diverse domains and multiple LLM architectures and scales.
Results show that SOFT effectively reduces privacy risks while maintaining
competitive model performance, offering a practical and scalable solution to
safeguard sensitive information in fine-tuned LLMs.

</details>


### [271] [Specification and Evaluation of Multi-Agent LLM Systems -- Prototype and Cybersecurity Applications](https://arxiv.org/abs/2506.10467)
*Felix Härer*

Main category: cs.CR

TL;DR: 本文探讨了基于大语言模型（LLM）的多智能体系统在特定领域（如网络安全）的应用潜力，提出通过联合规范和系统评估方法验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多单独评估LLM、推理技术或应用，但缺乏对多智能体系统联合规范及组合应用的系统性探索，需定义规范以挖掘其潜力并评估适用性。

Method: 扩展已有系统架构与原型，引入多智能体系统规范，通过网络安全任务测试案例验证架构与评估方法的可行性。

Result: 测试表明，基于OpenAI和DeepSeek模型的智能体能正确完成问答、服务器安全及网络安全任务，验证了架构与评估方法的有效性。

Conclusion: 多智能体LLM系统在特定领域具备应用潜力，其规范定义与系统评估方法为后续研究提供了可扩展框架。

Abstract: Recent advancements in LLMs indicate potential for novel applications, e.g.,
through reasoning capabilities in the latest OpenAI and DeepSeek models. For
applying these models in specific domains beyond text generation, LLM-based
multi-agent approaches can be utilized that solve complex tasks by combining
reasoning techniques, code generation, and software execution. Applications
might utilize these capabilities and the knowledge of specialized LLM agents.
However, while many evaluations are performed on LLMs, reasoning techniques,
and applications individually, their joint specification and combined
application is not explored well. Defined specifications for multi-agent LLM
systems are required to explore their potential and their suitability for
specific applications, allowing for systematic evaluations of LLMs, reasoning
techniques, and related aspects. This paper reports the results of exploratory
research to specify and evaluate these aspects through a multi-agent system.
The system architecture and prototype are extended from previous research and a
specification is introduced for multi-agent systems. Test cases involving
cybersecurity tasks indicate feasibility of the architecture and evaluation
approach. In particular, the results show the evaluation of question answering,
server security, and network security tasks that were completed correctly by
agents with LLMs from OpenAI and DeepSeek.

</details>


### [272] [SoK: Evaluating Jailbreak Guardrails for Large Language Models](https://arxiv.org/abs/2506.10597)
*Xunguang Wang,Zhenlan Ji,Wenxuan Wang,Zongjie Li,Daoyuan Wu,Shuai Wang*

Main category: cs.CR

TL;DR: 本文首次系统分析了针对大语言模型（LLM）的越狱攻击防护机制（guardrails），提出多维度分类法和安全-效率-实用性评估框架，通过实验揭示现有防护措施的优劣及通用性，为优化防御组合提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有LLM防护机制分散且缺乏统一分类与评估框架，导致难以有效防御越狱攻击。本文旨在填补这一空白，推动防护机制的标准化发展。

Method: 提出六维分类法对防护机制进行多维度分类，并设计Security-Efficiency-Utility评估框架，通过实验分析现有防护措施的性能与跨攻击类型适用性。

Result: 实验表明现有防护措施在安全性和效率间存在权衡，不同防御组合具有优化潜力，且防护机制对多种攻击类型展现部分通用性。

Conclusion: 本研究为LLM防护机制建立了系统性分析框架，为未来开发鲁棒、标准化的防护方案提供理论支撑与实践指导。

Abstract: Large Language Models (LLMs) have achieved remarkable progress, but their
deployment has exposed critical vulnerabilities, particularly to jailbreak
attacks that circumvent safety mechanisms. Guardrails--external defense
mechanisms that monitor and control LLM interaction--have emerged as a
promising solution. However, the current landscape of LLM guardrails is
fragmented, lacking a unified taxonomy and comprehensive evaluation framework.
In this Systematization of Knowledge (SoK) paper, we present the first holistic
analysis of jailbreak guardrails for LLMs. We propose a novel,
multi-dimensional taxonomy that categorizes guardrails along six key
dimensions, and introduce a Security-Efficiency-Utility evaluation framework to
assess their practical effectiveness. Through extensive analysis and
experiments, we identify the strengths and limitations of existing guardrail
approaches, explore their universality across attack types, and provide
insights into optimizing defense combinations. Our work offers a structured
foundation for future research and development, aiming to guide the principled
advancement and deployment of robust LLM guardrails. The code is available at
https://github.com/xunguangwang/SoK4JailbreakGuardrails.

</details>


### [273] [TED-LaST: Towards Robust Backdoor Defense Against Adaptive Attacks](https://arxiv.org/abs/2506.10722)
*Xiaoxing Mo,Yuxuan Cheng,Nan Sun,Leo Yu Zhang,Wei Luo,Shang Gao*

Main category: cs.CR

TL;DR: 本文提出TED-LaST防御策略，通过标签监督动态跟踪和自适应层强调，增强拓扑演化动力学（TED）对自适应后门攻击的鲁棒性，并在多数据集和模型上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于TED的后门攻击检测方法易受自适应攻击（如扭曲拓扑表示分布）的影响，需提升其对抗动态恶意任务和隐蔽攻击的能力。

Method: 提出TED-LaST方法，包含标签监督动态跟踪（识别隐蔽威胁）和自适应层强调（应对拓扑扰动），并设计增强型自适应攻击（含目标映射策略）进行对抗测试。

Result: 在CIFAR-10、GTSRB、ImageNet100数据集及ResNet等模型上，TED-LaST成功抵御Adap-Blend、Adapt-Patch及新攻击，检测准确率显著优于传统TED方法。

Conclusion: TED-LaST为后门检测设立新基准，通过动态追踪和分层优化显著提升DNN安全性，可有效对抗复杂自适应攻击的演化威胁。

Abstract: Deep Neural Networks (DNNs) are vulnerable to backdoor attacks, where
attackers implant hidden triggers during training to maliciously control model
behavior. Topological Evolution Dynamics (TED) has recently emerged as a
powerful tool for detecting backdoor attacks in DNNs. However, TED can be
vulnerable to backdoor attacks that adaptively distort topological
representation distributions across network layers. To address this limitation,
we propose TED-LaST (Topological Evolution Dynamics against Laundry, Slow
release, and Target mapping attack strategies), a novel defense strategy that
enhances TED's robustness against adaptive attacks. TED-LaST introduces two key
innovations: label-supervised dynamics tracking and adaptive layer emphasis.
These enhancements enable the identification of stealthy threats that evade
traditional TED-based defenses, even in cases of inseparability in topological
space and subtle topological perturbations. We review and classify data
poisoning tricks in state-of-the-art adaptive attacks and propose enhanced
adaptive attack with target mapping, which can dynamically shift malicious
tasks and fully leverage the stealthiness that adaptive attacks possess. Our
comprehensive experiments on multiple datasets (CIFAR-10, GTSRB, and
ImageNet100) and model architectures (ResNet20, ResNet101) show that TED-LaST
effectively counteracts sophisticated backdoors like Adap-Blend, Adapt-Patch,
and the proposed enhanced adaptive attack. TED-LaST sets a new benchmark for
robust backdoor detection, substantially enhancing DNN security against
evolving threats.

</details>


### [274] [ME: Trigger Element Combination Backdoor Attack on Copyright Infringement](https://arxiv.org/abs/2506.10776)
*Feiyu Yang,Siyuan Liang,Aishan Liu,Dacheng Tao*

Main category: cs.CR

TL;DR: 本文针对生成扩散模型（如Stable Diffusion）的版权侵权攻击问题，提出新数据集及基于SilentBadDiffusion（SBD）的多元素（ME）攻击方法，结合离散余弦变换（DCT）提升攻击能力并保持隐蔽性。实验表明，新方法在低毒样本下显著优于原SBD，CIR/FAE指标接近或超越基准数据集。


<details>
  <summary>Details</summary>
Motivation: 现有攻击方法SBD面临数据资源受限（如版权、内容问题）、部分图像不适用，且在少量毒样本下攻击效果不佳。需解决数据可用性及攻击效率问题。

Method: 提出ME攻击方法：在SBD基础上增加单毒化样本的视觉-文本元素数量以增强攻击力，并引入DCT保持毒样本隐蔽性。同时构建新数据集支持研究。

Result: 新数据集上CIR/FAE达16.78%/39.50和51.20%/23.60，接近或超越基准数据集。低子采样率（5%）时，ME+DCT的CIR/FAE为0.23%/84.00和12.73%/65.50，优于无法攻击的原始SBD。

Conclusion: ME攻击结合DCT有效提升SBD在低毒样本下的攻击效果，新数据集为相关研究提供支持。结果表明该方法在隐蔽性和攻击效率上的优势，为防御技术提供参考。

Abstract: The capability of generative diffusion models (DMs) like Stable Diffusion
(SD) in replicating training data could be taken advantage of by attackers to
launch the Copyright Infringement Attack, with duplicated poisoned image-text
pairs. SilentBadDiffusion (SBD) is a method proposed recently, which shew
outstanding performance in attacking SD in text-to-image tasks. However, the
feasible data resources in this area are still limited, some of them are even
constrained or prohibited due to the issues like copyright ownership or
inappropriate contents; And not all of the images in current datasets are
suitable for the proposed attacking methods; Besides, the state-of-the-art
(SoTA) performance of SBD is far from ideal when few generated poisoning
samples could be adopted for attacks. In this paper, we raised new datasets
accessible for researching in attacks like SBD, and proposed Multi-Element (ME)
attack method based on SBD by increasing the number of poisonous visual-text
elements per poisoned sample to enhance the ability of attacking, while
importing Discrete Cosine Transform (DCT) for the poisoned samples to maintain
the stealthiness. The Copyright Infringement Rate (CIR) / First Attack Epoch
(FAE) we got on the two new datasets were 16.78% / 39.50 and 51.20% / 23.60,
respectively close to or even outperformed benchmark Pokemon and Mijourney
datasets. In condition of low subsampling ratio (5%, 6 poisoned samples), MESI
and DCT earned CIR / FAE of 0.23% / 84.00 and 12.73% / 65.50, both better than
original SBD, which failed to attack at all.

</details>


### [275] [A Crack in the Bark: Leveraging Public Knowledge to Remove Tree-Ring Watermarks](https://arxiv.org/abs/2506.10502)
*Junhua Lin,Marc Juarez*

Main category: cs.CR

TL;DR: 本文提出一种针对Tree-Ring水印技术的新型攻击方法，仅需利用公开的变分自编码器（VAE）即可显著降低检测器性能，揭示现有行业实践中未考虑的安全风险。


<details>
  <summary>Details</summary>
Motivation: 现有针对Tree-Ring水印的移除攻击依赖强假设条件，而实际中扩散模型训练用VAE常被公开。本文旨在探索基于公开VAE的潜在攻击可能性，以暴露实际部署中的安全隐患。

Method: 通过访问目标扩散模型训练时使用的公开VAE，攻击者近似其潜在空间，并基于代理模型实现高效攻击，无需完整模型访问权限。

Result: 攻击使Tree-Ring检测器的ROC-AUC从0.993降至0.153，PR-AUC从0.994降至0.385，且图像质量保持高位，效果优于需完整模型访问的现有方法。

Conclusion: 重用公共VAE训练扩散模型存在重大风险，Tree-Ring检测器的精度（尤其是查准率）未达实际部署要求，当前行业实践需重新评估水印安全性。

Abstract: We present a novel attack specifically designed against Tree-Ring, a
watermarking technique for diffusion models known for its high imperceptibility
and robustness against removal attacks. Unlike previous removal attacks, which
rely on strong assumptions about attacker capabilities, our attack only
requires access to the variational autoencoder that was used to train the
target diffusion model, a component that is often publicly available. By
leveraging this variational autoencoder, the attacker can approximate the
model's intermediate latent space, enabling more effective surrogate-based
attacks. Our evaluation shows that this approach leads to a dramatic reduction
in the AUC of Tree-Ring detector's ROC and PR curves, decreasing from 0.993 to
0.153 and from 0.994 to 0.385, respectively, while maintaining high image
quality. Notably, our attacks outperform existing methods that assume full
access to the diffusion model. These findings highlight the risk of reusing
public autoencoders to train diffusion models -- a threat not considered by
current industry practices. Furthermore, the results suggest that the Tree-Ring
detector's precision, a metric that has been overlooked by previous
evaluations, falls short of the requirements for real-world deployment.

</details>


### [276] [Assessing the Resilience of Automotive Intrusion Detection Systems to Adversarial Manipulation](https://arxiv.org/abs/2506.10620)
*Stefano Longari,Paolo Cerracchio,Michele Carminati,Stefano Zanero*

Main category: cs.CR

TL;DR: 本文研究了基于梯度的对抗攻击在不同知识程度（白盒、灰盒、黑盒）下对汽车控制器局域网（CAN）入侵检测系统（IDS）的可行性与影响，发现攻击效果受数据集质量、目标IDS及攻击者知识水平影响显著，并通过预计算有效载荷验证了实时攻击的可行性。


<details>
  <summary>Details</summary>
Motivation: 现代车辆CAN总线因缺乏安全机制和日益增长的互联性易受网络攻击，现有入侵检测系统（IDS）虽能防御但存在被对抗攻击绕过的风险。本文旨在探究不同知识程度的攻击者对汽车IDS的实际威胁。

Method: 通过白盒（全系统知识）、灰盒（部分知识）和黑盒（无内部数据/机制知识）三种攻击场景，评估对抗攻击对先进IDS的有效性；分析对抗扰动对攻击效果的影响，并基于总线流量预计算规避载荷以验证实时攻击可行性，使用两个公开数据集进行实验。

Result: 攻击有效性高度依赖数据集质量、目标IDS模型及攻击者知识水平，黑盒攻击在现实场景中仍可行；对抗扰动对攻击效果有直接影响，且预计算载荷可满足实时注入需求，但受汽车领域约束（如时序限制）影响较大。

Conclusion: 对抗攻击对汽车IDS构成严重威胁，需开发更鲁棒的防御机制，尤其是在不同攻击者知识场景下。研究强调数据集质量与对系统内部了解的重要性，并为实际攻击可行性提供了实证依据。

Abstract: The security of modern vehicles has become increasingly important, with the
controller area network (CAN) bus serving as a critical communication backbone
for various Electronic Control Units (ECUs). The absence of robust security
measures in CAN, coupled with the increasing connectivity of vehicles, makes
them susceptible to cyberattacks. While intrusion detection systems (IDSs) have
been developed to counter such threats, they are not foolproof. Adversarial
attacks, particularly evasion attacks, can manipulate inputs to bypass
detection by IDSs. This paper extends our previous work by investigating the
feasibility and impact of gradient-based adversarial attacks performed with
different degrees of knowledge against automotive IDSs. We consider three
scenarios: white-box (attacker with full system knowledge), grey-box (partial
system knowledge), and the more realistic black-box (no knowledge of the IDS'
internal workings or data). We evaluate the effectiveness of the proposed
attacks against state-of-the-art IDSs on two publicly available datasets.
Additionally, we study effect of the adversarial perturbation on the attack
impact and evaluate real-time feasibility by precomputing evasive payloads for
timed injection based on bus traffic. Our results demonstrate that, besides
attacks being challenging due to the automotive domain constraints, their
effectiveness is strongly dependent on the dataset quality, the target IDS, and
the attacker's degree of knowledge.

</details>


### [277] [Monitoring Decomposition Attacks in LLMs with Lightweight Sequential Monitors](https://arxiv.org/abs/2506.10949)
*Chen Yueh-Han,Nitish Joshi,Yulin Chen,Maksym Andriushchenko,Rico Angell,He He*

Main category: cs.CR

TL;DR: 当前LLM安全防御无法抵御分解攻击，本文提出轻量级顺序监控框架，在降低90%成本的同时实现93%防御成功率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全对齐技术仅检测即时提示的恶意内容，无法识别通过分解为良性子任务实现的长期恶意意图，导致分解攻击成功率高达87%。

Method: 构建最大多样化攻击数据集验证漏洞，提出实时轻量级顺序监控框架，通过累积评估子任务检测恶意意图，并测试随机任务注入的混淆效果。

Result: 监控框架防御成功率达93%，优于o3 mini等推理模型，抗随机注入攻击，成本降低90%、延迟减少50%。GPT-4o攻击成功率验证分解攻击普遍有效性。

Conclusion: 轻量级顺序监控能有效抵御分解攻击，兼顾防御效果与部署可行性，为LLM安全提供实用解决方案。

Abstract: Current LLM safety defenses fail under decomposition attacks, where a
malicious goal is decomposed into benign subtasks that circumvent refusals. The
challenge lies in the existing shallow safety alignment techniques: they only
detect harm in the immediate prompt and do not reason about long-range intent,
leaving them blind to malicious intent that emerges over a sequence of
seemingly benign instructions. We therefore propose adding an external monitor
that observes the conversation at a higher granularity. To facilitate our study
of monitoring decomposition attacks, we curate the largest and most diverse
dataset to date, including question-answering, text-to-image, and agentic
tasks. We verify our datasets by testing them on frontier LLMs and show an 87%
attack success rate on average on GPT-4o. This confirms that decomposition
attack is broadly effective. Additionally, we find that random tasks can be
injected into the decomposed subtasks to further obfuscate malicious intents.
To defend in real time, we propose a lightweight sequential monitoring
framework that cumulatively evaluates each subtask. We show that a carefully
prompt engineered lightweight monitor achieves a 93% defense success rate,
beating reasoning models like o3 mini as a monitor. Moreover, it remains robust
against random task injection and cuts cost by 90% and latency by 50%. Our
findings suggest that lightweight sequential monitors are highly effective in
mitigating decomposition attacks and are viable in deployment.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [278] [Fundamental Limits of Learning High-dimensional Simplices in Noisy Regimes](https://arxiv.org/abs/2506.10101)
*Seyed Amir Hossein Saberi,Amir Najafi,Abolfazl Motahari,Babak H. khalaj*

Main category: stat.ML

TL;DR: 本文研究了高维噪声环境下单纯形学习的样本复杂度，提出了基于样本压缩和傅里叶方法的新算法，推导了上下界并证明在足够高信噪比时噪声与无噪声复杂度一致。


<details>
  <summary>Details</summary>
Motivation: 现有研究对高维噪声单纯形学习的样本复杂度理论存在空白，尤其是下界分析及信噪比影响机制。本文旨在填补这一空白，并解决开放性问题提供理论支撑。

Method: 结合样本压缩技术(Ashtiani et al., 2018)与新型傅里叶分析方法，提出从噪声观测中恢复分布的通用框架，特别适用于单纯形结构学习。

Result: 证明样本复杂度上界n≥(K²/ε²)e^O(K/SNR²)，下界n≥Ω(K³σ²/ε²+K/ε)。当SNR≥Ω(√K)时，噪声复杂度与无噪声情况Ω(K/ε)一致。

Conclusion: 首次建立噪声单纯形学习的严格下界，解决开放性问题。提出的傅里叶方法具有普适性，为更广泛分布学习问题提供新工具。

Abstract: In this paper, we establish sample complexity bounds for learning
high-dimensional simplices in $\mathbb{R}^K$ from noisy data. Specifically, we
consider $n$ i.i.d. samples uniformly drawn from an unknown simplex in
$\mathbb{R}^K$, each corrupted by additive Gaussian noise of unknown variance.
We prove an algorithm exists that, with high probability, outputs a simplex
within $\ell_2$ or total variation (TV) distance at most $\varepsilon$ from the
true simplex, provided $n \ge (K^2/\varepsilon^2)
e^{\mathcal{O}(K/\mathrm{SNR}^2)}$, where $\mathrm{SNR}$ is the signal-to-noise
ratio. Extending our prior work~\citep{saberi2023sample}, we derive new
information-theoretic lower bounds, showing that simplex estimation within TV
distance $\varepsilon$ requires at least $n \ge \Omega(K^3
\sigma^2/\varepsilon^2 + K/\varepsilon)$ samples, where $\sigma^2$ denotes the
noise variance. In the noiseless scenario, our lower bound $n \ge
\Omega(K/\varepsilon)$ matches known upper bounds up to constant factors. We
resolve an open question by demonstrating that when $\mathrm{SNR} \ge
\Omega(K^{1/2})$, noisy-case complexity aligns with the noiseless case. Our
analysis leverages sample compression techniques (Ashtiani et al., 2018) and
introduces a novel Fourier-based method for recovering distributions from noisy
observations, potentially applicable beyond simplex learning.

</details>


### [279] [Momentum Multi-Marginal Schrödinger Bridge Matching](https://arxiv.org/abs/2506.10168)
*Panagiotis Theodoropoulos,Augustinos D. Saravanos,Evangelos A. Theodorou,Guan-Horng Liu*

Main category: stat.ML

TL;DR: 本文提出了一种名为3MSBM的新框架，通过多边际条件随机最优控制问题，解决了现有方法在捕捉长程时间依赖性和保持中间边际一致性上的不足，实验证明其在复杂动态系统建模中的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有基于相邻快照成对插值的方法难以捕捉复杂系统的长程时间依赖性，且可能破坏轨迹一致性。为此，需开发一种能同时满足多个位置约束的随机系统建模框架。

Method: 提出3MSBM框架：将动态提升至相空间，将随机桥推广至多条件点约束，形成多边际条件随机最优控制问题，并通过最小化变分目标学习动态，同时强制保持中间边际的传输映射。

Result: 在真实场景的实验中，3MSBM在捕捉时间依赖性复杂动态方面显著优于现有方法，收敛性和可扩展性均有提升，验证了多边际设置下训练框架的有效性。

Conclusion: 3MSBM为多边际场景下的匹配框架训练开辟了新途径，通过保持中间边际和相空间动态建模，显著增强了复杂系统轨迹推断的连贯性与长期依赖性捕捉能力。

Abstract: Understanding complex systems by inferring trajectories from sparse sample
snapshots is a fundamental challenge in a wide range of domains, e.g.,
single-cell biology, meteorology, and economics. Despite advancements in Bridge
and Flow matching frameworks, current methodologies rely on pairwise
interpolation between adjacent snapshots. This hinders their ability to capture
long-range temporal dependencies and potentially affects the coherence of the
inferred trajectories. To address these issues, we introduce \textbf{Momentum
Multi-Marginal Schr\"odinger Bridge Matching (3MSBM)}, a novel matching
framework that learns smooth measure-valued splines for stochastic systems that
satisfy multiple positional constraints. This is achieved by lifting the
dynamics to phase space and generalizing stochastic bridges to be conditioned
on several points, forming a multi-marginal conditional stochastic optimal
control problem. The underlying dynamics are then learned by minimizing a
variational objective, having fixed the path induced by the multi-marginal
conditional bridge. As a matching approach, 3MSBM learns transport maps that
preserve intermediate marginals throughout training, significantly improving
convergence and scalability. Extensive experimentation in a series of
real-world applications validates the superior performance of 3MSBM compared to
existing methods in capturing complex dynamics with temporal dependencies,
opening new avenues for training matching frameworks in multi-marginal
settings.

</details>


### [280] [Distributionally-Constrained Adversaries in Online Learning](https://arxiv.org/abs/2506.10293)
*Moïse Blanchard,Samory Kpotufe*

Main category: stat.ML

TL;DR: 本文提出一种基于分布约束对手的在线学习框架，统一并扩展了对抗性与随机性设置之间的学习场景，分析了不同分布类别下的可学习性，并证明部分函数类（如线性分类器）无需先验知识即可适应约束分布。


<details>
  <summary>Details</summary>
Motivation: 现有研究试图通过平滑分析等框架连接在线学习中对抗性与随机性场景，但缺乏对一般分布类别的细粒度分析。本文旨在通过分布约束对手框架，探索介于完全随机与完全对抗之间的可学习条件。

Method: 采用分布约束对手框架，分析敌手在受限分布类中选择实例的场景，研究不同函数类与分布约束的交互对可学习性的影响，覆盖遗忘型与自适应型敌手。

Result: 刻画了分布类在两种敌手下的可学习性条件，推广了平滑分析结果，并证明线性分类器等自然函数类无需分布先验即可同时对抗所有可学习分布类的敌手。

Conclusion: 该框架为学习边界提供了统一视角，揭示了函数类与分布约束的交互机制，其灵活性支持更广泛场景下的非平凡遗憾保证，扩展了在线学习的理论边界。

Abstract: There has been much recent interest in understanding the continuum from
adversarial to stochastic settings in online learning, with various frameworks
including smoothed settings proposed to bridge this gap. We consider the more
general and flexible framework of distributionally constrained adversaries in
which instances are drawn from distributions chosen by an adversary within some
constrained distribution class [RST11]. Compared to smoothed analysis, we
consider general distributional classes which allows for a fine-grained
understanding of learning settings between fully stochastic and fully
adversarial for which a learner can achieve non-trivial regret. We give a
characterization for which distribution classes are learnable in this context
against both oblivious and adaptive adversaries, providing insights into the
types of interplay between the function class and distributional constraints on
adversaries that enable learnability. In particular, our results recover and
generalize learnability for known smoothed settings. Further, we show that for
several natural function classes including linear classifiers, learning can be
achieved without any prior knowledge of the distribution class -- in other
words, a learner can simultaneously compete against any constrained adversary
within learnable distribution classes.

</details>


### [281] [Measuring Semantic Information Production in Generative Diffusion Models](https://arxiv.org/abs/2506.10433)
*Florian Handke,Félix Koulischer,Gabriel Raya,Luca Ambrogioni*

Main category: stat.ML

TL;DR: 本文提出一种信息论方法，通过分析条件熵的时间导数，测量扩散模型生成过程中类别语义决策的发生时段。实验发现语义信息传递在中间阶段最显著，且不同类别的决策时间存在差异。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成图像时，语义与结构特征在不同阶段涌现（类似物理相变），但现有方法难以量化这些语义决策的具体发生时刻。

Method: 使用贝叶斯分类器在线公式估计噪声状态下的条件熵，通过条件熵时间导数定位类别标签与噪声状态间最大信息传递的时间区间。

Result: 在CIFAR10的DDPM模型中，语义信息传递峰值出现在生成中期，末期趋零；不同类别的熵率分布差异显著，表明语义决策时间具有类别依赖性。

Conclusion: 扩散模型生成过程存在分阶段的语义决策机制，且不同类别的语义特征形成时间轴上呈现非均匀分布，这为理解生成模型动态提供了量化依据。

Abstract: It is well known that semantic and structural features of the generated
images emerge at different times during the reverse dynamics of diffusion, a
phenomenon that has been connected to physical phase transitions in magnets and
other materials. In this paper, we introduce a general information-theoretic
approach to measure when these class-semantic "decisions" are made during the
generative process. By using an online formula for the optimal Bayesian
classifier, we estimate the conditional entropy of the class label given the
noisy state. We then determine the time intervals corresponding to the highest
information transfer between noisy states and class labels using the time
derivative of the conditional entropy. We demonstrate our method on
one-dimensional Gaussian mixture models and on DDPM models trained on the
CIFAR10 dataset. As expected, we find that the semantic information transfer is
highest in the intermediate stages of diffusion while vanishing during the
final stages. However, we found sizable differences between the entropy rate
profiles of different classes, suggesting that different "semantic decisions"
are located at different intermediate times.

</details>


### [282] [Box-Constrained Softmax Function and Its Application for Post-Hoc Calibration](https://arxiv.org/abs/2506.10572)
*Kyohei Atarashi,Satoshi Oyama,Hiromi Arai,Hisashi Kashima*

Main category: stat.ML

TL;DR: 本文提出了一种基于盒约束的Softmax函数（BCSoftmax），通过显式约束输出概率的上下界，解决了传统Softmax无法施加硬约束的问题，并开发了高效计算算法及后校准方法，有效缓解预测模型的欠/过自信问题，提升下游任务可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统Softmax函数通过温度参数实现软控制，但无法对输出概率施加硬约束（如上下界限制其在需要高可靠性场景中的应用。例如，模型可能因概率分布不合理导致决策不可信。

Method: 提出BCSoftmax函数，将其建模为带盒约束的优化问题，并设计精确高效的计算算法。进一步开发两种后校准方法，通过训练后学习输出概率或logits的上下界，调整模型置信度。

Result: 在TinyImageNet、CIFAR-100和20NewsGroups数据集上的实验表明，所提方法显著改善了校准指标，验证了其在缓解欠/过自信问题上的有效性。

Conclusion: BCSoftmax通过硬约束增强了模型可靠性，后校准方法可灵活调整置信度，实验证明其能提升下游决策任务的可信度，为可信机器学习提供了新工具。

Abstract: Controlling the output probabilities of softmax-based models is a common
problem in modern machine learning. Although the $\mathrm{Softmax}$ function
provides soft control via its temperature parameter, it lacks the ability to
enforce hard constraints, such as box constraints, on output probabilities,
which can be critical in certain applications requiring reliable and
trustworthy models. In this work, we propose the box-constrained softmax
($\mathrm{BCSoftmax}$) function, a novel generalization of the
$\mathrm{Softmax}$ function that explicitly enforces lower and upper bounds on
output probabilities. While $\mathrm{BCSoftmax}$ is formulated as the solution
to a box-constrained optimization problem, we develop an exact and efficient
computation algorithm for $\mathrm{BCSoftmax}$. As a key application, we
introduce two post-hoc calibration methods based on $\mathrm{BCSoftmax}$. The
proposed methods mitigate underconfidence and overconfidence in predictive
models by learning the lower and upper bounds of the output probabilities or
logits after model training, thereby enhancing reliability in downstream
decision-making tasks. We demonstrate the effectiveness of our methods
experimentally using the TinyImageNet, CIFAR-100, and 20NewsGroups datasets,
achieving improvements in calibration metrics.

</details>


### [283] [Logarithmic Smoothing for Adaptive PAC-Bayesian Off-Policy Learning](https://arxiv.org/abs/2506.10664)
*Maxime Haddouche,Otmane Sakhi*

Main category: stat.ML

TL;DR: 本文提出自适应离策略学习框架，通过在线PAC-Bayesian理论与改进的对数平滑估计器，实现策略迭代部署下的高效学习。实验表明该方法在动态数据收集中显著优于传统离线方法。


<details>
  <summary>Details</summary>
Motivation: 传统离策略学习依赖静态行为策略收集的数据，缺乏灵活性。实际场景中需要动态调整策略以收集更高质量数据，因此需研究自适应环境下的高效学习框架。

Method: 基于在线PAC-Bayesian理论扩展静态场景下的对数平滑方法，通过调整估计器结构支持多轮策略部署，并利用温和条件加速收敛。

Result: 静态设置下与主流离线方法性能相当，在允许中间策略部署时显著优于基线；改进后的估计器在多种场景中验证了自适应数据收集与PAC-Bayesian框架的有效性。

Conclusion: 自适应数据收集机制与PAC-Bayesian理论结合能有效加速策略优化，动态部署策略产生的数据质量提升是性能优势的关键来源。

Abstract: Off-policy learning serves as the primary framework for learning optimal
policies from logged interactions collected under a static behavior policy. In
this work, we investigate the more practical and flexible setting of adaptive
off-policy learning, where policies are iteratively refined and re-deployed to
collect higher-quality data. Building on the success of PAC-Bayesian learning
with Logarithmic Smoothing (LS) in static settings, we extend this framework to
the adaptive scenario using tools from online PAC-Bayesian theory. Furthermore,
we demonstrate that a principled adjustment to the LS estimator naturally
accommodates multiple rounds of deployment and yields faster convergence rates
under mild conditions. Our method matches the performance of leading offline
approaches in static settings, and significantly outperforms them when
intermediate policy deployments are allowed. Empirical evaluations across
diverse scenarios highlight both the advantages of adaptive data collection and
the strength of the PAC-Bayesian formulation.

</details>


### [284] [Practical Improvements of A/B Testing with Off-Policy Estimation](https://arxiv.org/abs/2506.10677)
*Sakhi Otmane,Gilotte Alexandre,Rohde David*

Main category: stat.ML

TL;DR: 本文提出一种新的无偏离策略估计器家族，用于优化A/B测试中的均值差异估计器，在测试系统相似时显著降低方差。


<details>
  <summary>Details</summary>
Motivation: 传统A/B测试使用的均值差异估计器虽无偏，但其方差较高，尤其在测试系统相似时效率不足，需改进估计方法。

Method: 构建无偏离策略估计器家族，通过理论分析确定其中方差最小的最优估计器，利用系统相似性降低方差。

Result: 实验证明新估计器在系统相似场景下方差显著降低，且保持无偏性，理论分析与实际效果一致。

Conclusion: 所提估计器简单有效，特别适用于相似系统对比，为A/B测试提供了更高效的评估工具。

Abstract: We address the problem of A/B testing, a widely used protocol for evaluating
the potential improvement achieved by a new decision system compared to a
baseline. This protocol segments the population into two subgroups, each
exposed to a version of the system and estimates the improvement as the
difference between the measured effects. In this work, we demonstrate that the
commonly used difference-in-means estimator, while unbiased, can be improved.
We introduce a family of unbiased off-policy estimators that achieves lower
variance than the standard approach. Among this family, we identify the
estimator with the lowest variance. The resulting estimator is simple, and
offers substantial variance reduction when the two tested systems exhibit
similarities. Our theoretical analysis and experimental results validate the
effectiveness and practicality of the proposed method.

</details>


### [285] [Demystifying Spectral Feature Learning for Instrumental Variable Regression](https://arxiv.org/abs/2506.10899)
*Dimitri Meunier,Antoine Moulin,Jakub Wornbard,Vladimir R. Kostic,Arthur Gretton*

Main category: stat.ML

TL;DR: 该论文探讨了在存在隐藏混淆因素时，利用非参数工具变量（IV）回归进行因果效应估计的问题。通过谱特征方法分析了两阶段最小二乘估计器的泛化误差边界，并基于谱对齐和特征值衰减特性提出了三种性能场景分类。


<details>
  <summary>Details</summary>
Motivation: 在因果推断中，隐藏混淆因素会干扰效应估计的准确性。传统方法依赖强假设，而基于工具变量的非参数方法面临性能不稳定问题。本文旨在通过谱特征分析，明确方法成功或失败的条件，为实际应用提供理论指导。

Method: 采用非参数工具变量回归框架，构建基于谱特征（即处理-工具变量关联算子的顶部特征子空间）的两阶段最小二乘估计器，并通过泛化误差边界理论分析其性能。

Result: 提出性能分类：1) 优场景（强谱对齐+慢特征值衰减）方法最优；2) 劣场景（强谱对齐+快特征值衰减）需更多样本；3) 无效场景（弱谱对齐）方法必然失败。合成实验验证了该分类体系。

Conclusion: 谱特征方法的有效性取决于谱对齐强度与工具变量强度（特征值衰减速度）的交互作用。明确场景分类为实际中选择合适估计策略提供了理论依据，并揭示了方法的内在局限性。

Abstract: We address the problem of causal effect estimation in the presence of hidden
confounders, using nonparametric instrumental variable (IV) regression. A
leading strategy employs spectral features - that is, learned features spanning
the top eigensubspaces of the operator linking treatments to instruments. We
derive a generalization error bound for a two-stage least squares estimator
based on spectral features, and gain insights into the method's performance and
failure modes. We show that performance depends on two key factors, leading to
a clear taxonomy of outcomes. In a good scenario, the approach is optimal. This
occurs with strong spectral alignment, meaning the structural function is
well-represented by the top eigenfunctions of the conditional operator, coupled
with this operator's slow eigenvalue decay, indicating a strong instrument.
Performance degrades in a bad scenario: spectral alignment remains strong, but
rapid eigenvalue decay (indicating a weaker instrument) demands significantly
more samples for effective feature learning. Finally, in the ugly scenario,
weak spectral alignment causes the method to fail, regardless of the
eigenvalues' characteristics. Our synthetic experiments empirically validate
this taxonomy.

</details>


### [286] [Probably Approximately Correct Labels](https://arxiv.org/abs/2506.10908)
*Emmanuel J. Candès,Andrew Ilyas,Tijana Zrnic*

Main category: stat.ML

TL;DR: 提出一种结合专家标注与AI预测的低成本高质数据集构建方法，确保标签错误率低，适用于文本、图像和蛋白质分析。


<details>
  <summary>Details</summary>
Motivation: 传统高质量标注数据集依赖昂贵的人工或实验成本，需寻求更经济的替代方案。

Method: 利用预训练模型的AI预测补充专家标注，通过概率保证总体标注误差最小化（PAC学习框架）。

Result: 在大型语言模型文本标注、预训练视觉模型图像标记及AlphaFold蛋白质折叠分析中验证了方法的有效性。

Conclusion: 该方法为现代AI模型提供了严格且高效的数据集构建方案，在多领域实现成本与精度的平衡。

Abstract: Obtaining high-quality labeled datasets is often costly, requiring either
extensive human annotation or expensive experiments. We propose a method that
supplements such "expert" labels with AI predictions from pre-trained models to
construct labeled datasets more cost-effectively. Our approach results in
probably approximately correct labels: with high probability, the overall
labeling error is small. This solution enables rigorous yet efficient dataset
curation using modern AI models. We demonstrate the benefits of the methodology
through text annotation with large language models, image labeling with
pre-trained vision models, and protein folding analysis with AlphaFold.

</details>


### [287] [What Exactly Does Guidance Do in Masked Discrete Diffusion Models](https://arxiv.org/abs/2506.10971)
*He Ye,Rojas Kevin,Tao Molei*

Main category: stat.ML

TL;DR: 本文通过理论分析揭示了分类器无关引导（CFG）在掩码离散扩散模型中的作用机制，表明引导强度不仅影响输出分布，还控制采样轨迹的动态特性。


<details>
  <summary>Details</summary>
Motivation: 研究分类器无关引导（CFG）如何通过调整采样动态来增强特定类别的生成效果，并量化其对分布结构和收敛速度的影响。

Method: 假设无分数误差和离散化误差，推导引导反向动力学的显式解，分析引导强度$w$对采样行为的影响，并通过实验验证理论结果。

Result: 引导强度$w$会放大目标类别的区域并抑制共享区域，不同维度（1D/2D）的协方差结构及总变差（TV）衰减率（双指数级）存在显著差异。

Conclusion: 分类器无关引导不仅调整生成分布，还通过改变采样动态的几何特性加速收敛，其作用机制在理论和实验上均得到验证。

Abstract: We study masked discrete diffusion models with classifier-free guidance
(CFG). Assuming no score error nor discretization error, we derive an explicit
solution to the guided reverse dynamics, so that how guidance influences the
sampling behavior can be precisely characterized. When the full data
distribution is a mixture over classes and the goal is to sample from a
specific class, guidance amplifies class-specific regions while suppresses
regions shared with other classes. This effect depends on the guidance strength
$w$ and induces distinct covariance structures in the sampled distribution.
Notably, we observe quantitatively different behaviors in $1$D and $2$D. We
also show that for large $w$, the decay rate of the total variation
($\mathrm{TV}$) along the reverse dynamics is double-exponential in $w$ for
both $1$D and $2$D. These findings highlight the role of guidance, not just in
shaping the output distribution, but also in controlling the dynamics of the
sampling trajectory. Our theoretical analysis is supported by experiments that
illustrate the geometric effects of guidance and its impact on convergence.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [288] [Structure and asymptotic preserving deep neural surrogates for uncertainty quantification in multiscale kinetic equations](https://arxiv.org/abs/2506.10636)
*Wei Chen,Giacomo Dimarco,Lorenzo Pareschi*

Main category: math.NA

TL;DR: 本文提出结合多尺度控制变量与结构保持渐近神经网络(SAPNNs)的方法，以提升高维随机动力学方程不确定性量化中蒙特卡洛采样的效率与精度，同时保持物理一致性。


<details>
  <summary>Details</summary>
Motivation: 传统蒙特卡洛方法在高维参数空间下存在收敛速度慢、方差高的问题，尤其在动力学方程不确定性量化中计算效率受限，亟需改进。

Method: 采用多尺度控制变量策略降低方差，并设计SAPNNs作为替代模型，通过嵌入物理约束（如守恒律、熵耗散）和融合低/高精度样本，实现高效采样与物理一致性。

Result: 数值实验表明，该方法在均质与非均质多尺度场景下显著降低方差，计算效率优于传统蒙特卡洛，同时保持渐近精度与物理属性。

Conclusion: 所提出的SAPNNs增强型多尺度采样框架，为高维动力学方程不确定性量化提供了高效且物理可信的解决方案。

Abstract: The high dimensionality of kinetic equations with stochastic parameters poses
major computational challenges for uncertainty quantification (UQ). Traditional
Monte Carlo (MC) sampling methods, while widely used, suffer from slow
convergence and high variance, which become increasingly severe as the
dimensionality of the parameter space grows. To accelerate MC sampling, we
adopt a multiscale control variates strategy that leverages low-fidelity
solutions from simplified kinetic models to reduce variance. To further improve
sampling efficiency and preserve the underlying physics, we introduce surrogate
models based on structure and asymptotic preserving neural networks (SAPNNs).
These deep neural networks are specifically designed to satisfy key physical
properties, including positivity, conservation laws, entropy dissipation, and
asymptotic limits. By training the SAPNNs on low-fidelity models and enriching
them with selected high-fidelity samples from the full Boltzmann equation, our
method achieves significant variance reduction while maintaining physical
consistency and asymptotic accuracy. The proposed methodology enables efficient
large-scale prediction in kinetic UQ and is validated across both homogeneous
and nonhomogeneous multiscale regimes. Numerical results demonstrate improved
accuracy and computational efficiency compared to standard MC techniques.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [289] [Exploring Topological and Localization Phenomena in SSH Chains under Generalized AAH Modulation: A Computational Approach](https://arxiv.org/abs/2506.10195)
*Souvik Ghosh,Sayak Roy*

Main category: cond-mat.mtrl-sci

TL;DR: 本文通过计算和机器学习方法，系统研究了广义SSH模型在准周期无序、非厄米性和周期驱动下的拓扑特性与局域化现象，揭示了拓扑边缘态破坏机制、非厄米趋肤效应及Floquet拓扑相工程。


<details>
  <summary>Details</summary>
Motivation: 探索标准SSH模型在准周期无序、非厄米性、时间驱动等复杂现实条件下的拓扑特性演化机制，拓展对一维拓扑系统多物理场耦合效应的理解。

Method: 结合精确对角化与专用数值求解器分析能谱和IPR局域化特征，采用主成分分析(PCA)进行无监督相分类，研究非厄米趋肤效应和Floquet周期驱动调控。

Result: 发现强AAH调制通过局域化相变破坏拓扑边缘态；非厄米系统呈现体态局域的趋肤效应；Floquet驱动可在平凡链中实现准能隙边缘态的新型拓扑相。

Conclusion: 广义SSH模型通过多维度调控（无序/非厄米/驱动）展现出丰富的拓扑-局域化竞争机制，机器学习方法有效揭示了传统序参量难以捕捉的相变特征。

Abstract: The Su-Schrieffer-Heeger (SSH) model serves as a canonical example of a
one-dimensional topological insulator, yet its behavior under more complex,
realistic conditions remains a fertile ground for research. This paper presents
a comprehensive computational investigation into generalized SSH models,
exploring the interplay between topology, quasi-periodic disorder,
non-Hermiticity, and time-dependent driving. Using exact diagonalization and
specialized numerical solvers, we map the system's phase space through its
spectral properties and localization characteristics, quantified by the Inverse
Participation Ratio (IPR). We demonstrate that while the standard SSH model
exhibits topologically protected edge states, these are destroyed by a
localization transition induced by strong Aubry-Andr\'e-Harper (AAH)
modulation. Further, we employ unsupervised machine learning (PCA) to
autonomously classify the system's phases, revealing that strong localization
can obscure underlying topological signatures. Extending the model beyond
Hermiticity, we uncover the non-Hermitian skin effect, a dramatic localization
of all bulk states at a boundary. Finally, we apply a periodic Floquet drive to
a topologically trivial chain, successfully engineering a Floquet topological
insulator characterized by the emergence of anomalous edge states at the
boundaries of the quasi-energy zone. These findings collectively provide a
multi-faceted view of the rich phenomena hosted in generalized 1D topological
systems.

</details>


### [290] [Coupled reaction and diffusion governing interface evolution in solid-state batteries](https://arxiv.org/abs/2506.10944)
*Jingxuan Ding,Laura Zichi,Matteo Carli,Menghang Wang,Albert Musaelian,Yu Xie,Boris Kozinsky*

Main category: cond-mat.mtrl-sci

TL;DR: 通过量子精度的大规模显式反应模拟与无监督分类技术，揭示了固态电池SEI界面中新型晶体无序相的形成及锂蠕变机制，为理解界面动力学提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 固态电池中固体电解质界面（SEI）的原子级反应机制因实验表征困难及模拟精度限制尚未明确，需开发新方法以揭示其动力学过程。

Method: 结合主动学习与深度等变神经网络势函数，构建对称电池单元（symcell）的量子精度模拟；利用基于局部原子环境聚类的无监督分类技术分析界面反应与互扩散。

Result: 发现SEI中未被热力学预测的晶体无序相Li₂S₀.₇₂P₀.₁₄Cl₀.₁₄，模拟结果与实验观测一致，揭示了锂沿界面蠕动的枝晶形成关键机制。

Conclusion: 基于第一性原理的无参数数字孪生方法，可解析固态电化学中复杂异质过程的原子动力学，为界面设计与电池性能优化提供理论支持。

Abstract: Understanding and controlling the atomistic-level reactions governing the
formation of the solid-electrolyte interphase (SEI) is crucial for the
viability of next-generation solid state batteries. However, challenges persist
due to difficulties in experimentally characterizing buried interfaces and
limits in simulation speed and accuracy. We conduct large-scale explicit
reactive simulations with quantum accuracy for a symmetric battery cell,
{\symcell}, enabled by active learning and deep equivariant neural network
interatomic potentials. To automatically characterize the coupled reactions and
interdiffusion at the interface, we formulate and use unsupervised
classification techniques based on clustering in the space of local atomic
environments. Our analysis reveals the formation of a previously unreported
crystalline disordered phase, Li$_2$S$_{0.72}$P$_{0.14}$Cl$_{0.14}$, in the
SEI, that evaded previous predictions based purely on thermodynamics,
underscoring the importance of explicit modeling of full reaction and transport
kinetics. Our simulations agree with and explain experimental observations of
the SEI formations and elucidate the Li creep mechanisms, critical to dendrite
initiation, characterized by significant Li motion along the interface. Our
approach is to crease a digital twin from first principles, without adjustable
parameters fitted to experiment. As such, it offers capabilities to gain
insights into atomistic dynamics governing complex heterogeneous processes in
solid-state synthesis and electrochemistry.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [291] [Attention on flow control: transformer-based reinforcement learning for lift regulation in highly disturbed flows](https://arxiv.org/abs/2506.10153)
*Zhecheng Liu,Jeff D. Eldredge*

Main category: physics.flu-dyn

TL;DR: 本文提出了一种基于Transformer的强化学习框架，用于通过俯仰控制调节气动升力以应对连续强阵风。通过预训练专家策略和任务级迁移学习加速训练，结果表明该方法优于传统比例控制，且四分之一弦长俯仰控制能更高效调节升力。


<details>
  <summary>Details</summary>
Motivation: 传统线性流控制策略在强扰动序列中因非线性相互作用失效，但可作为基础开发更优策略。需解决有限传感器下的部分可观测性挑战，并提升控制效率。

Method: 采用Transformer强化学习框架，结合专家策略（线性控制）预训练和任务级迁移学习（从单阵风到多阵风环境迁移），解决传感器部分可观测性问题。

Result: 1）学习控制策略优于最佳比例控制，且随阵风数量增加性能差距扩大；2）策略可泛化至任意长阵风序列；3）四分之一弦长俯仰控制相比中弦长控制能耗降低且升力调节更优，归因于附加质量效应主导。

Conclusion: 基于Transformer的强化学习框架具有通用性，结合加速技术为复杂流控问题提供有效解决方案，验证了其在多配置场景下的适应能力。

Abstract: A linear flow control strategy designed for weak disturbances may not remain
effective in sequences of strong disturbances due to nonlinear interactions,
but it is sensible to leverage it for developing a better strategy. In the
present study, we propose a transformer-based reinforcement learning (RL)
framework to learn an effective control strategy for regulating aerodynamic
lift in gust sequences via pitch control. The transformer addresses the
challenge of partial observability from limited surface pressure sensors. We
demonstrate that the training can be accelerated with two techniques --
pretraining with an expert policy (here, linear control) and task-level
transfer learning (here, extending a policy trained on isolated gusts to
multiple gusts). We show that the learned strategy outperforms the best
proportional control, with the performance gap widening as the number of gusts
increases. The control strategy learned in an environment with a small number
of successive gusts is shown to effectively generalize to an environment with
an arbitrarily long sequence of gusts. We investigate the pivot configuration
and show that quarter-chord pitching control can achieve superior lift
regulation with substantially less control effort compared to mid-chord
pitching control. Through a decomposition of the lift, we attribute this
advantage to the dominant added-mass contribution accessible via quarter-chord
pitching. The success on multiple configurations shows the generalizability of
the proposed transformer-based RL framework, which offers a promising approach
to solve more computationally demanding flow control problems when combined
with the proposed acceleration techniques.

</details>


### [292] [OmniFluids: Unified Physics Pre-trained Modeling of Fluid Dynamics](https://arxiv.org/abs/2506.10862)
*Rui Zhang,Qi Meng,Han Wan,Yang Liu,Zhi-Ming Ma,Hao Sun*

Main category: physics.flu-dyn

TL;DR: OmniFluids提出了一种结合物理预训练、粗网格算子蒸馏和少样本微调的统一框架，能在有限或零数据下实现流体动力学的高效模拟，显著超越现有AI方法，速度比传统求解器快10-100倍。


<details>
  <summary>Details</summary>
Motivation: 传统CFD方法计算成本高，而现有深度学习方法（PINNs、神经算子）存在训练复杂、依赖大量标注数据或稳定性不足的问题，需开发高效且数据需求低的替代模型。

Method: 通过混合算子架构、多帧解码器和分解傅里叶层实现物理感知建模，结合物理预训练、粗网格算子蒸馏和少量数据微调，实现与物理监督的无缝集成。

Result: 在2D/3D基准测试中，流场重建和湍流统计精度显著优于现有AI方法，速度比经典求解器快10-100倍，并能从稀疏噪声数据中准确恢复未知物理参数。

Conclusion: OmniFluids为数据有限场景下的复杂流体系统建立了一种高效、可泛化的替代建模新范式，平衡了计算效率与物理保真度。

Abstract: High-fidelity and efficient simulation of fluid dynamics drive progress in
various scientific and engineering applications. Traditional computational
fluid dynamics methods offer strong interpretability and guaranteed
convergence, but rely on fine spatial and temporal meshes, incurring
prohibitive computational costs. Physics-informed neural networks (PINNs) and
neural operators aim to accelerate PDE solvers using deep learning techniques.
However, PINNs require extensive retraining and careful tuning, and purely
data-driven operators demand large labeled datasets. Hybrid physics-aware
methods embed numerical discretizations into network architectures or loss
functions, but achieve marginal speed gains and become unstable when balancing
coarse priors against high-fidelity measurements. To this end, we introduce
OmniFluids, a unified physics pre-trained operator learning framework that
integrates physics-only pre-training, coarse-grid operator distillation, and
few-shot fine-tuning, which enables fast inference and accurate prediction
under limited or zero data supervision. For architectural design, the key
components of OmniFluids include a mixture of operators, a multi-frame decoder,
and factorized Fourier layers, which enable efficient and scalable modeling of
diverse physical tasks while maintaining seamless integration with
physics-based supervision. Across a broad range of two- and three-dimensional
benchmarks, OmniFluids significantly outperforms state-of-the-art AI-driven
methods in flow field reconstruction and turbulence statistics accuracy,
delivering 10-100x speedups compared to classical solvers, and accurately
recovers unknown physical parameters from sparse, noisy data. This work
establishes a new paradigm for efficient and generalizable surrogate modeling
in complex fluid systems under limited data availability.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [293] [FastFLUX: Pruning FLUX with Block-wise Replacement and Sandwich Training](https://arxiv.org/abs/2506.10035)
*Fuhan Cai,Yong Guo,Jie Li,Wenbo Li,Xiangzhong Fang,Jian Chen*

Main category: cs.GR

TL;DR: 本文提出FastFLUX框架，通过结构剪枝和局部微调策略，在保持图像生成质量的同时显著提升FLUX模型的推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型（如FLUX）因参数量大导致推理速度慢、内存占用高，而传统加速方法存在性能下降和训练成本高的问题。

Method: 结合Block-wise Replacement with Linear Layers（BRLL）替换复杂残差分支为轻量线性层，并采用Sandwich Training（ST）通过LoRA进行局部监督微调。

Result: 实验表明FastFLUX在剪枝20%层级结构后仍保持图像质量，同时显著提升推理速度。

Conclusion: FastFLUX通过架构级剪枝与针对性训练策略，有效平衡了模型效率与生成性能，具有高实用价值。

Abstract: Recent advancements in text-to-image (T2I) generation have led to the
emergence of highly expressive models such as diffusion transformers (DiTs),
exemplified by FLUX. However, their massive parameter sizes lead to slow
inference, high memory usage, and poor deployability. Existing acceleration
methods (e.g., single-step distillation and attention pruning) often suffer
from significant performance degradation and incur substantial training costs.
To address these limitations, we propose FastFLUX, an architecture-level
pruning framework designed to enhance the inference efficiency of FLUX. At its
core is the Block-wise Replacement with Linear Layers (BRLL) method, which
replaces structurally complex residual branches in ResBlocks with lightweight
linear layers while preserving the original shortcut connections for stability.
Furthermore, we introduce Sandwich Training (ST), a localized fine-tuning
strategy that leverages LoRA to supervise neighboring blocks, mitigating
performance drops caused by structural replacement. Experiments show that our
FastFLUX maintains high image quality under both qualitative and quantitative
evaluations, while significantly improving inference speed, even with 20\% of
the hierarchy pruned. Our code will be available soon.

</details>


### [294] [Ambient Diffusion Omni: Training Good Models with Bad Data](https://arxiv.org/abs/2506.10038)
*Giannis Daras,Adrian Rodriguez-Munoz,Adam Klivans,Antonio Torralba,Constantinos Daskalakis*

Main category: cs.GR

TL;DR: 提出Ambient Diffusion Omni框架，利用低质量、合成及分布外图像提升扩散模型性能，通过噪声抑制数据偏差，在ImageNet和文本到图像生成中实现SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型依赖高质量精选数据，但大量低质量图像被丢弃。研究旨在挖掘此类数据的潜在价值，解决高质量数据有限的问题。

Method: 基于自然图像的频谱功率律衰减和局部性特性，设计框架处理混合分布数据，通过噪声平衡有偏数据与有限无偏数据的学习权衡。

Result: 在合成损坏图像上验证有效性；ImageNet FID达SOTA，文本到图像生成的质量和多样性显著提升。

Conclusion: 低质量图像通过噪声抑制初始分布偏差，可有效增强扩散模型性能，理论分析支持数据混合训练策略的合理性。

Abstract: We show how to use low-quality, synthetic, and out-of-distribution images to
improve the quality of a diffusion model. Typically, diffusion models are
trained on curated datasets that emerge from highly filtered data pools from
the Web and other sources. We show that there is immense value in the
lower-quality images that are often discarded. We present Ambient Diffusion
Omni, a simple, principled framework to train diffusion models that can extract
signal from all available images during training. Our framework exploits two
properties of natural images -- spectral power law decay and locality. We first
validate our framework by successfully training diffusion models with images
synthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We
then use our framework to achieve state-of-the-art ImageNet FID, and we show
significant improvements in both image quality and diversity for text-to-image
generative modeling. The core insight is that noise dampens the initial skew
between the desired high-quality distribution and the mixed distribution we
actually observe. We provide rigorous theoretical justification for our
approach by analyzing the trade-off between learning from biased data versus
limited unbiased data across diffusion times.

</details>


### [295] [Learning-based density-equalizing map](https://arxiv.org/abs/2506.10027)
*Yanwen Huang,Lok Ming Lui,Gary P. T. Choi*

Main category: cs.GR

TL;DR: 本文提出了一种基于深度神经网络的密度均衡映射框架（LDEM），通过设计强制密度均匀化和几何规则性的损失函数及分层预测方法，解决了传统方法在精度、重叠伪影和2D/3D扩展性上的不足，实现了更优的密度均衡与双射性。


<details>
  <summary>Details</summary>
Motivation: 传统密度均衡图（DEM）方法依赖迭代求解器或优化能量函数，存在精度低、极端情况重叠、2D到3D扩展需大量算法重构等问题。需一种更鲁棒、可扩展的解决方案。

Method: 提出LDEM框架：使用深度神经网络，设计损失函数强制密度均匀与几何规则性，采用分层结构（粗粒度到密集粒度）预测形变，支持2D/3D统一架构。

Result: 相比现有方法，LDEM在多种密度分布下展现更优的密度均衡性和双射性，可无缝扩展至3D，且适用于不同效果的表面重新网格化。

Conclusion: LDEM为密度均衡图的计算提供了可扩展、鲁棒的新方法，其架构无关导数能量公式的特性为实际应用开辟了新可能性。

Abstract: Density-equalizing map (DEM) serves as a powerful technique for creating
shape deformations with the area changes reflecting an underlying density
function. In recent decades, DEM has found widespread applications in fields
such as data visualization, geometry processing, and medical imaging.
Traditional approaches to DEM primarily rely on iterative numerical solvers for
diffusion equations or optimization-based methods that minimize handcrafted
energy functionals. However, these conventional techniques often face several
challenges: they may suffer from limited accuracy, produce overlapping
artifacts in extreme cases, and require substantial algorithmic redesign when
extended from 2D to 3D, due to the derivative-dependent nature of their energy
formulations. In this work, we propose a novel learning-based
density-equalizing mapping framework (LDEM) using deep neural networks.
Specifically, we introduce a loss function that enforces density uniformity and
geometric regularity, and utilize a hierarchical approach to predict the
transformations at both the coarse and dense levels. Our method demonstrates
superior density-equalizing and bijectivity properties compared to prior
methods for a wide range of simple and complex density distributions, and can
be easily applied to surface remeshing with different effects. Also, it
generalizes seamlessly from 2D to 3D domains without structural changes to the
model architecture or loss formulation. Altogether, our work opens up new
possibilities for scalable and robust computation of density-equalizing maps
for practical applications.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [296] [Diffusion prior as a direct regularization term for FWI](https://arxiv.org/abs/2506.10141)
*Yuke Xie,Hervé Chauris,Nicolas Desassis*

Main category: physics.geo-ph

TL;DR: 本文提出一种新框架，将预训练的DDPM作为基于分数的生成扩散先验通过分数重匹配策略直接整合到全波形反演（FWI）中，避免传统扩散方法中的反向扩散采样，减少迭代次数，并在干净图像空间操作，提升反演稳定性和质量。实验表明该方法在保真度与鲁棒性上优于传统及GAN方法，且计算高效。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型在非线性求解器（如FWI）中需处理含噪中间状态，导致波传播通过含噪速度场时产生数值伪影和反演质量下降。现有方法需完整反向扩散过程，限制了其在物理约束地震成像中的应用。

Method: 提出通过分数重匹配策略将预训练DDPM作为正则化项引入FWI更新规则，直接在干净图像空间进行反演，避免反向扩散采样，减少迭代次数，且无需修改现有FWI流程。

Result: 数值实验表明，相比传统和基于GAN的FWI方法，该方法在保真度、鲁棒性及收敛性上表现更优，同时保持实用性和计算效率。

Conclusion: 所提框架通过分数重匹配有效整合生成扩散先验，促进稳定波传播，改善反演质量，为地震成像及其他逆问题提供了高效解决方案。

Abstract: Diffusion models have recently shown promise as powerful generative priors
for inverse problems. However, conventional applications require solving the
full reverse diffusion process and operating on noisy intermediate states,
which poses challenges for physics-constrained computational seismic imaging.
In particular, such instability is pronounced in non-linear solvers like those
used in Full Waveform Inversion (FWI), where wave propagation through noisy
velocity fields can lead to numerical artifacts and poor inversion quality. In
this work, we propose a simple yet effective framework that directly integrates
a pretrained Denoising Diffusion Probabilistic Model (DDPM) as a score-based
generative diffusion prior into FWI through a score rematching strategy. Unlike
traditional diffusion approaches, our method avoids the reverse diffusion
sampling and needs fewer iterations. We operate the image inversion entirely in
the clean image space, eliminating the need to operate through noisy velocity
models. The generative diffusion prior can be introduced as a simple
regularization term in the standard FWI update rule, requiring minimal
modification to existing FWI pipelines. This promotes stable wave propagation
and can improve convergence behavior and inversion quality. Numerical
experiments suggest that the proposed method offers enhanced fidelity and
robustness compared to conventional and GAN-based FWI approaches, while
remaining practical and computationally efficient for seismic imaging and other
inverse problem tasks.

</details>


### [297] [Self-learning signal classifier for decameter coherent scatter radars](https://arxiv.org/abs/2506.10305)
*Oleg Berngardt,Ivan Lavygin*

Main category: physics.geo-ph

TL;DR: 该论文提出了一种基于超高频相干散射雷达数据的自动分类器构建方法，结合电离层电波传播模型与数学质量评估标准，利用12个雷达的两年数据训练模型，识别出37个数据类别并验证其与地理纬度及地磁活动的关联性。


<details>
  <summary>Details</summary>
Motivation: 旨在通过自动化方法处理多雷达网络的海量电离层观测数据，解决传统人工分类效率低的问题，并利用建模与实测参数提升分类准确性。

Method: 基于SuperDARN和SECIRA网络12个雷达的两年数据，结合电离层电波传播自动建模结果、雷达实测参数及数学质量准则，使用2669个模型系数构建分类器，并通过流星尾迹散射信号校准仰角测量。

Result: 确定37个数据类别（25类常见，14类可明确区分），其中10类获初步物理解释；关键分类参数为射线轨迹后半段形态、散射高度与多普勒速度，且分类结果与太阳/地磁活动的地理纬度依赖性符合已知物理机制。

Conclusion: 所构建的自动分类器能有效识别电离层散射数据类别，验证了模型参数与物理机制的一致性，为大规模雷达网络数据分析提供了高效可靠的自动化解决方案。

Abstract: The paper presents a method for automatic constructing a classifier for
processed data obtained by decameter coherent scatter radars. Method is based
only on the radar data obtained, the results of automatic modeling of radio
wave propagation in the ionosphere, and mathematical criteria for estimating
the quality of the models. The final classifier is the model trained at data
obtained by 12 radars of the SuperDARN and SECIRA networks over two years for
each radar. The number of the model coefficients is 2669. For the
classification, the model uses both the calculated parameters of radio wave
propagation in the model ionosphere and the parameters directly measured by the
radar. Calibration of radiowave elevation measurements at each radar was made
using meteor trail scattered signals. The analysis showed that the optimal
number of classes in the data is 37, of which 25 are frequently observed. The
analysis made it possible to choose 14 classes from them, which are confidently
separated in other variants of model training. A preliminary interpretation of
10 of them was carried out. The dynamics of observation of various classes and
their dependence on the geographical latitude of radars at different levels of
solar and geomagnetic activity were presented, it was shown that it does not
contradict with known physical mechanisms. The analysis showed that the most
important parameters to identify the classes are the shape of the signal
ray-tracing trajectory in its second half, the ray-traced scattering height and
the Doppler velocity measured by the radar.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [298] [Semantic Communication-Enabled Cloud-Edge-End-collaborative Metaverse Services Architecure](https://arxiv.org/abs/2506.10001)
*Yuxuan Li,Sheng Jinag,Bizhu Wang*

Main category: cs.MM

TL;DR: 为解决元宇宙中高分辨率虚拟场景传输面临的带宽不足、延迟高及信道质量差等问题，本文提出了一种基于语义通信的云-边-端协同架构（SC-CEE-Meta），通过语义传输、视频合成与3D重建模块优化传输效率与用户体验。


<details>
  <summary>Details</summary>
Motivation: 元宇宙需在云端与VR设备间传输大量高分辨率数据，但无线带宽不足导致延迟，信道质量差引发数据错误，严重影响用户体验。需一种高效传输方案解决资源与带宽矛盾。

Method: 提出SC-CEE-Meta架构，包含VR视频语义传输、视频合成和3D虚拟场景重建模块。在VR设备与边缘服务器部署语义模块传输关键语义信息，云端预处理视频合成与3D重建，边缘端负责渲染，降低传输数据量并提升抗干扰能力。

Result: 在Meta Quest Pro上验证，SC-CEE-Meta在恶劣信道下可降低96.05%的无线传输延迟，图像质量提升43.99%。

Conclusion: 该架构通过语义通信与云边端协同，有效缓解带宽资源矛盾，提升传输效率与沉浸体验，为元宇宙服务提供可行解决方案。

Abstract: With technology advancing and the pursuit of new audiovisual experiences
strengthening, the metaverse has gained surging enthusiasm. However, it faces
practical hurdles as substantial data like high-resolution virtual scenes must
be transmitted between cloud platforms and VR devices. Specifically, the VR
device's wireless transmission hampered by insufficient bandwidth, causes speed
and delay problems. Meanwhile, poor channel quality leads to data errors and
worsens user experience. To solve this, we've proposed the Semantic
Communication-Enabled Cloud-Edge-End Collaborative Immersive Metaverse Service
(SC-CEE-Meta) Architecture, which includes three modules: VR video semantic
transmission, video synthesis, and 3D virtual scene reconstruction. By
deploying semantic modules on VR devices and edge servers and sending key
semantic info instead of focusing on bit-level reconstruction, it can cut
latency, resolve the resource-bandwidth conflict, and better withstand channel
interference. Also, the cloud deploys video synthesis and 3D scene
reconstruction preprocessing, while edge devices host 3D reconstruction
rendering modules, all for immersive services. Verified on Meta Quest Pro, the
SC-CEE-Meta can reduce wireless transmission delay by 96.05\% and boost image
quality by 43.99\% under poor channel condition.

</details>


### [299] [EQ-TAA: Equivariant Traffic Accident Anticipation via Diffusion-Based Accident Video Synthesis](https://arxiv.org/abs/2506.10002)
*Jianwu Fang,Lei-Lei Li,Zhedong Zheng,Hongkai Yu,Jianru Xue,Zhengguo Li,Tat-Seng Chua*

Main category: cs.MM

TL;DR: 提出基于注意力视频扩散模型（AVD）和等变TAA框架（EQ-TAA），通过生成因果视频片段解决交通事故事件预测中的背景混杂问题，无需额外标注，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有交通事故事件预测（TAA）方法依赖标注数据，但交通场景的长尾性、不确定性和快速演变性导致因果部分易受数据偏差影响，产生背景混杂问题。

Method: 设计AVD模型，通过文本提示生成因果视频帧（正常→事故），保留原始视频风格；结合EQ-TAA框架，利用等变三元损失对比生成的正/负样本与原始无事故视频。

Result: 大量实验表明，AVD与EQ-TAA在无需额外标注的情况下，性能与当前最先进方法相当。

Conclusion: 通过生成因果视频缓解数据偏差，结合对比学习框架，显著提升TAA任务效果，为无监督交通预测提供新思路。

Abstract: Traffic Accident Anticipation (TAA) in traffic scenes is a challenging
problem for achieving zero fatalities in the future. Current approaches
typically treat TAA as a supervised learning task needing the laborious
annotation of accident occurrence duration. However, the inherent long-tailed,
uncertain, and fast-evolving nature of traffic scenes has the problem that real
causal parts of accidents are difficult to identify and are easily dominated by
data bias, resulting in a background confounding issue. Thus, we propose an
Attentive Video Diffusion (AVD) model that synthesizes additional accident
video clips by generating the causal part in dashcam videos, i.e., from normal
clips to accident clips. AVD aims to generate causal video frames based on
accident or accident-free text prompts while preserving the style and content
of frames for TAA after video generation. This approach can be trained using
datasets collected from various driving scenes without any extra annotations.
Additionally, AVD facilitates an Equivariant TAA (EQ-TAA) with an equivariant
triple loss for an anchor accident-free video clip, along with the generated
pair of contrastive pseudo-normal and pseudo-accident clips. Extensive
experiments have been conducted to evaluate the performance of AVD and EQ-TAA,
and competitive performance compared to state-of-the-art methods has been
obtained.

</details>


### [300] [Immersive Multimedia Communication: State-of-the-Art on eXtended Reality Streaming](https://arxiv.org/abs/2506.10004)
*Haopeng Wang,Haiwei Dong,Abdulmotaleb El Saddik*

Main category: cs.MM

TL;DR: 本文综述了扩展现实（XR）流媒体的最新进展，涵盖定义、设备、交互方式、流量特征、体验质量优化方法、应用及挑战，旨在为XR未来发展提供参考。


<details>
  <summary>Details</summary>
Motivation: XR技术快速发展，需系统性梳理其流媒体传输特性、体验质量影响因素及优化方法，以提升用户沉浸感并推动技术落地。

Method: 通过定义XR技术框架，分析多模态交互与流量特征，结合视觉注意力模型提出流媒体优化策略，并总结应用场景与技术挑战。

Result: 系统归纳了XR流媒体的关键传输特性与体验质量指标，提出基于视觉注意力的优化方法，并识别出延迟、算力分配等核心挑战。

Conclusion: XR流媒体在交互体验与传输效率上取得进展，但需进一步解决实时性、多模态同步等挑战以实现大规模应用。

Abstract: Extended reality (XR) is rapidly advancing, and poised to revolutionize
content creation and consumption. In XR, users integrate various sensory inputs
to form a cohesive perception of the virtual environment. This survey reviews
the state-of-the-art in XR streaming, focusing on multiple paradigms. To begin,
we define XR and introduce various XR headsets along with their multimodal
interaction methods to provide a foundational understanding. We then analyze XR
traffic characteristics to highlight the unique data transmission requirements.
We also explore factors that influence the quality of experience in XR systems,
aiming to identify key elements for enhancing user satisfaction. Following
this, we present visual attention-based optimization methods for XR streaming
to improve efficiency and performance. Finally, we examine current applications
and highlight challenges to provide insights into ongoing and future
developments of XR.

</details>


### [301] [HER2 Expression Prediction with Flexible Multi-Modal Inputs via Dynamic Bidirectional Reconstruction](https://arxiv.org/abs/2506.10006)
*Jie Qin,Wei Yang,Yan Su,Yiran Zhu,Weizhen Li,Yunyue Pan,Chengchang Pan,Honggang Qi*

Main category: cs.MM

TL;DR: 提出自适应双模态框架，通过动态分支选择、跨模态生成对抗网络及混合训练策略，实现灵活单/双模态HER2预测，显著提升单模态准确率并降低资源需求。


<details>
  <summary>Details</summary>
Motivation: 现有HER2评估模型仅单独分析H&E或IHC图像，但临床需两者协同解读。双模态同步获取常受限于流程复杂性与成本，需开发灵活的单/双模态兼容方案。

Method: 1) 动态分支选择器根据输入完整性激活单模态重建或双模态联合推理；2) 双向跨模态GAN实现缺失模态的上下文感知特征重建；3) 结合对抗学习与多任务优化的混合训练协议。

Result: 单模态H&E准确率从71.44%提升至94.25%，双模态达95.09%；仅IHC输入时保持90.28%可靠性。跨模态重建F1-score达0.9609(HE→IHC)和0.9251(IHC→HE)，轻量版参数减少78.55%。

Conclusion: 弹性双模态框架在无需同步采集条件下实现近双模态性能，通过IHC基础设施降本显著惠及资源有限场景，为精准HER2评估提供普适性解决方案。

Abstract: Current HER2 assessment models for breast cancer predominantly analyze H&E or
IHC images in isolation,despite clinical reliance on their synergistic
interpretation. However, concurrent acquisition of both modalities is often
hindered by workflow complexity and cost constraints. We propose an adaptive
bimodal framework enabling flexible single-/dual-modality HER2 prediction
through three innovations: 1) A dynamic branch selector that activates either
single-modality reconstruction or dual-modality joint inference based on input
completeness; 2) A bidirectional cross-modal GAN performing context-aware
feature-space reconstruction of missing modalities; 3) A hybrid training
protocol integrating adversarial learning and multi-task optimization. This
architecture elevates single-modality H&E prediction accuracy from 71.44% to
94.25% while achieving 95.09% dual-modality accuracy, maintaining 90.28%
reliability with sole IHC inputs. The framework's "dual-preferred,
single-compatible" design delivers near-bimodal performance without requiring
synchronized acquisition, particularly benefiting resource-limited settings
through IHC infrastructure cost reduction. Experimental validation confirms
22.81%/12.90% accuracy improvements over H&E/IHC baselines respectively, with
cross-modal reconstruction enhancing F1-scores to 0.9609 (HE to IHC) and 0.9251
(IHC to HE). By dynamically routing inputs through reconstruction-enhanced or
native fusion pathways, the system mitigates performance degradation from
missing data while preserving computational efficiency (78.55% parameter
reduction in lightweight variant). This elastic architecture demonstrates
significant potential for democratizing precise HER2 assessment across diverse
healthcare settings.

</details>


### [302] [Controllable Expressive 3D Facial Animation via Diffusion in a Unified Multimodal Space](https://arxiv.org/abs/2506.10007)
*Kangwei Liu,Junwu Liu,Xiaowei Yi,Jinlin Guo,Yun Cao*

Main category: cs.MM

TL;DR: 本文提出了一种基于扩散模型的多模态可控3D面部动画生成框架，通过多模态情感绑定策略和注意力机制增强的潜在扩散模型，解决了现有方法在情感控制灵活性和运动多样性方面的局限。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动3D面部动画存在两大问题：(1)依赖单一模态控制信号，未能利用多模态互补优势；(2)确定性回归方法限制了情感表达的随机性和非语言行为多样性。

Method: 提出两个核心创新：(1)基于FLAME模型的多模态情感对比学习绑定策略，实现跨文本/音频/标签的灵活控制；(2)包含内容感知注意力和情感引导层的注意力潜在扩散模型，在保持时间连贯性的同时增强运动多样性。

Result: 实验表明该方法在多数指标上优于现有方法，情感相似度提升21.6%，同时保持生理合理的面部动态。

Conclusion: 该框架通过多模态融合和概率生成建模，显著提升了3D面部动画的表达能力和控制灵活性，为数字人动画生成提供了新思路。

Abstract: Audio-driven emotional 3D facial animation encounters two significant
challenges: (1) reliance on single-modal control signals (videos, text, or
emotion labels) without leveraging their complementary strengths for
comprehensive emotion manipulation, and (2) deterministic regression-based
mapping that constrains the stochastic nature of emotional expressions and
non-verbal behaviors, limiting the expressiveness of synthesized animations. To
address these challenges, we present a diffusion-based framework for
controllable expressive 3D facial animation. Our approach introduces two key
innovations: (1) a FLAME-centered multimodal emotion binding strategy that
aligns diverse modalities (text, audio, and emotion labels) through contrastive
learning, enabling flexible emotion control from multiple signal sources, and
(2) an attention-based latent diffusion model with content-aware attention and
emotion-guided layers, which enriches motion diversity while maintaining
temporal coherence and natural facial dynamics. Extensive experiments
demonstrate that our method outperforms existing approaches across most
metrics, achieving a 21.6\% improvement in emotion similarity while preserving
physiologically plausible facial dynamics. Project Page:
https://kangweiiliu.github.io/Control_3D_Animation.

</details>


### [303] [Structured Graph Representations for Visual Narrative Reasoning: A Hierarchical Framework for Comics](https://arxiv.org/abs/2506.10008)
*Yi-Chun Chen*

Main category: cs.MM

TL;DR: 提出分层知识图谱框架，用于漫画等多模态媒体的结构化叙事理解，支持跨层次符号推理任务。


<details>
  <summary>Details</summary>
Motivation: 现有视觉叙事分析缺乏对多层次结构（如故事弧、事件段）的整合表示，难以支持跨语义、时空关系的系统性推理。

Method: 构建分层知识图谱：宏观故事线-事件段-面板级多模态图，整合视觉元素（角色/物体/动作）与文本（对话/旁白）的语义时空关系。

Result: 在Manga109数据集上实现高精度（动作检索92%、对话追踪89%）与召回率（角色定位85%），有效支持时间线重建等任务。

Conclusion: 该框架为视觉媒体的叙事分析、互动故事生成及多模态推理提供了可扩展且可解释的基础架构。

Abstract: This paper presents a hierarchical knowledge graph framework for the
structured understanding of visual narratives, focusing on multimodal media
such as comics. The proposed method decomposes narrative content into multiple
levels, from macro-level story arcs to fine-grained event segments. It
represents them through integrated knowledge graphs that capture semantic,
spatial, and temporal relationships. At the panel level, we construct
multimodal graphs that link visual elements such as characters, objects, and
actions with corresponding textual components, including dialogue and captions.
These graphs are integrated across narrative levels to support reasoning over
story structure, character continuity, and event progression.
  We apply our approach to a manually annotated subset of the Manga109 dataset
and demonstrate its ability to support symbolic reasoning across diverse
narrative tasks, including action retrieval, dialogue tracing, character
appearance mapping, and panel timeline reconstruction. Evaluation results show
high precision and recall across tasks, validating the coherence and
interpretability of the framework. This work contributes a scalable foundation
for narrative-based content analysis, interactive storytelling, and multimodal
reasoning in visual media.

</details>


### [304] [WDMIR: Wavelet-Driven Multimodal Intent Recognition](https://arxiv.org/abs/2506.10011)
*Weiyin Gong,Kai Zhang,Yanghai Zhang,Qi Liu,Xinjie Sun,Junyu Lu,Linbo Zhu*

Main category: cs.MM

TL;DR: 本文提出了一种基于小波变换的多模态意图识别框架（WDMIR），通过频域分析非语言信息及跨模态交互机制，显著提升了意图识别的准确性，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态意图识别方法过度依赖文本分析，忽视了视频、音频等非语言模态中蕴含的丰富语义信息，导致意图理解的局限性。

Method: 1. 小波驱动融合模块：在频域对视频-音频特征进行同步分解与融合，实现细粒度时序动态分析；2. 跨模态交互机制：通过从双模态到三模态的渐进特征增强，弥合语言与非语言信息的语义鸿沟。

Result: 在MIntRec数据集上达到SOTA性能，准确率提升1.13%；消融实验表明小波融合模块使非语言语义提取效果提升，识别准确率在细微情感线索分析中增加0.41%。

Conclusion: WDMIR框架通过频域分析与跨模态交互，有效整合非语言信息，验证了其在意图识别任务中的优越性，尤其在捕捉非语言语义方面具有显著优势。

Abstract: Multimodal intent recognition (MIR) seeks to accurately interpret user
intentions by integrating verbal and non-verbal information across video, audio
and text modalities. While existing approaches prioritize text analysis, they
often overlook the rich semantic content embedded in non-verbal cues. This
paper presents a novel Wavelet-Driven Multimodal Intent Recognition(WDMIR)
framework that enhances intent understanding through frequency-domain analysis
of non-verbal information. To be more specific, we propose: (1) a
wavelet-driven fusion module that performs synchronized decomposition and
integration of video-audio features in the frequency domain, enabling
fine-grained analysis of temporal dynamics; (2) a cross-modal interaction
mechanism that facilitates progressive feature enhancement from bimodal to
trimodal integration, effectively bridging the semantic gap between verbal and
non-verbal information. Extensive experiments on MIntRec demonstrate that our
approach achieves state-of-the-art performance, surpassing previous methods by
1.13% on accuracy. Ablation studies further verify that the wavelet-driven
fusion module significantly improves the extraction of semantic information
from non-verbal sources, with a 0.41% increase in recognition accuracy when
analyzing subtle emotional cues.

</details>


### [305] [Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2506.10016)
*Longzhen Han,Awes Mubarak,Almas Baimagambetov,Nikolaos Polatidis,Thar Baker*

Main category: cs.MM

TL;DR: 本文综述了多模态大语言模型（MLLMs）的发展，涵盖其从文本生成到跨模态（图像、音乐、视频等）输出的扩展，分析支撑技术（自监督学习、混合专家等）、模型架构趋势及跨模态协同效应，并指出评估、模块化等开放挑战。


<details>
  <summary>Details</summary>
Motivation: 探索MLLMs如何通过整合语言与其他感官模态的统一架构实现跨模态生成能力，并研究基础技术如何推动这一进程，同时揭示当前存在的技术挑战。

Method: 通过分类六种生成模态，分析自监督学习（SSL）、混合专家（MoE）、人类反馈强化学习（RLHF）和思维链（CoT）等核心技术，结合Transformer和扩散模型等架构创新，研究跨模态能力实现路径。

Result: 发现Transformer和扩散模型等架构创新支撑跨模态迁移与模块化能力，揭示跨模态协同效应模式，但评估标准、模块化设计及结构化推理等问题仍待解决。

Conclusion: MLLMs需进一步解决开放挑战以提升通用性、适应性和可解释性，技术迁移与跨模态协同是未来发展的关键方向。

Abstract: Multimodal Large Language Models (MLLMs) have rapidly evolved beyond text
generation, now spanning diverse output modalities including images, music,
video, human motion, and 3D objects, by integrating language with other sensory
modalities under unified architectures. This survey categorises six primary
generative modalities and examines how foundational techniques, namely
Self-Supervised Learning (SSL), Mixture of Experts (MoE), Reinforcement
Learning from Human Feedback (RLHF), and Chain-of-Thought (CoT) prompting,
enable cross-modal capabilities. We analyze key models, architectural trends,
and emergent cross-modal synergies, while highlighting transferable techniques
and unresolved challenges. Architectural innovations like transformers and
diffusion models underpin this convergence, enabling cross-modal transfer and
modular specialization. We highlight emerging patterns of synergy, and identify
open challenges in evaluation, modularity, and structured reasoning. This
survey offers a unified perspective on MLLM development and identifies critical
paths toward more general-purpose, adaptive, and interpretable multimodal
systems.

</details>


### [306] [Multimodal Emotion Coupling via Speech-to-Facial and Bodily Gestures in Dyadic Interaction](https://arxiv.org/abs/2506.10010)
*Von Ralph Dane Marquez Herbuela,Yukie Nagai*

Main category: cs.MM

TL;DR: 研究通过分析语音、面部和手势的协调信号，探讨情绪表达在对话结构（如轮流发言）中的动态关系，发现非重叠语音增强面部活跃度，不同情绪在同步性上表现差异显著，为实时情感检测及AI系统优化提供依据。


<details>
  <summary>Details</summary>
Motivation: 理解情感表达中语音、面部及手势的多模态协调动态，尤其是对话结构（如轮流发言与同时发言）对情绪同步性和清晰度的影响，以提升实时情感检测及AI系统的跨模态同步准确性。

Method: 基于IEMOCAP语料库的双人互动数据，结合区域动作捕捉技术，提取语音特征（韵律、MFCCs、唤醒度、效价及分类情绪），并与3D面部及手部运动位移对齐，通过活跃度量化和语音-手势预测模型分析多模态耦合。

Result: 非重叠语音显著增强下半脸活跃度；悲伤在非重叠时表现力更强，愤怒在重叠时抑制手势；韵律和MFCCs对发音区域预测最准，唤醒度与效价相关性低且受语境影响；手部同步性在低唤醒和重叠语音时增强，与效价无关。

Conclusion: 对话结构和情绪类型显著影响多模态协调：非重叠语音强化面部表达，情绪特异性同步模式（如愤怒抑制手势）揭示了动态交互的复杂性，为优化情感识别模型提供时序与同步性关键参数。

Abstract: Human emotional expression emerges through coordinated vocal, facial, and
gestural signals. While speech face alignment is well established, the broader
dynamics linking emotionally expressive speech to regional facial and hand
motion remains critical for gaining a deeper insight into how emotional and
behavior cues are communicated in real interactions. Further modulating the
coordination is the structure of conversational exchange like sequential turn
taking, which creates stable temporal windows for multimodal synchrony, and
simultaneous speech, often indicative of high arousal moments, disrupts this
alignment and impacts emotional clarity. Understanding these dynamics enhances
realtime emotion detection by improving the accuracy of timing and synchrony
across modalities in both human interactions and AI systems. This study
examines multimodal emotion coupling using region specific motion capture from
dyadic interactions in the IEMOCAP corpus. Speech features included low level
prosody, MFCCs, and model derived arousal, valence, and categorical emotions
(Happy, Sad, Angry, Neutral), aligned with 3D facial and hand marker
displacements. Expressive activeness was quantified through framewise
displacement magnitudes, and speech to gesture prediction mapped speech
features to facial and hand movements. Nonoverlapping speech consistently
elicited greater activeness particularly in the lower face and mouth. Sadness
showed increased expressivity during nonoverlap, while anger suppressed
gestures during overlaps. Predictive mapping revealed highest accuracy for
prosody and MFCCs in articulatory regions while arousal and valence had lower
and more context sensitive correlations. Notably, hand speech synchrony was
enhanced under low arousal and overlapping speech, but not for valence.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [307] [Augmenting Large Language Models with Static Code Analysis for Automated Code Quality Improvements](https://arxiv.org/abs/2506.10330)
*Seyed Moein Abtahi,Akramul Azim*

Main category: cs.SE

TL;DR: 研究通过整合GPT-3.5/4等大语言模型与静态代码分析框架，实现代码问题自动化检测与修复，结合RAG和自研工具减少模型幻觉，显著提升代码质量并优化开发流程。


<details>
  <summary>Details</summary>
Motivation: 旨在解决软件开发中代码问题（如漏洞、异味）检测与修复的效率问题，降低人工介入成本，同时克服大语言模型生成错误代码的缺陷。

Method: 构建静态代码分析框架识别问题，通过迭代式提示工程和RAG技术增强LLM输出的准确性，并开发代码比对工具自动修正模型幻觉导致的错误。

Result: 静态分析复检显示代码问题显著减少，验证了LLM与静态分析、RAG技术结合在提升代码质量与开发效率方面的有效性。

Conclusion: LLM与静态分析框架、RAG技术的协同应用可系统化改善代码质量，降低开发资源消耗，为自动化软件工程提供可行方案。

Abstract: This study examined code issue detection and revision automation by
integrating Large Language Models (LLMs) such as OpenAI's GPT-3.5 Turbo and
GPT-4o into software development workflows. A static code analysis framework
detects issues such as bugs, vulnerabilities, and code smells within a
large-scale software project. Detailed information on each issue was extracted
and organized to facilitate automated code revision using LLMs. An iterative
prompt engineering process is applied to ensure that prompts are structured to
produce accurate and organized outputs aligned with the project requirements.
Retrieval-augmented generation (RAG) is implemented to enhance the relevance
and precision of the revisions, enabling LLM to access and integrate real-time
external knowledge. The issue of LLM hallucinations - where the model generates
plausible but incorrect outputs - is addressed by a custom-built "Code
Comparison App," which identifies and corrects erroneous changes before
applying them to the codebase. Subsequent scans using the static code analysis
framework revealed a significant reduction in code issues, demonstrating the
effectiveness of combining LLMs, static analysis, and RAG to improve code
quality, streamline the software development process, and reduce time and
resource expenditure.

</details>


### [308] [Prompt Variability Effects On LLM Code Generation](https://arxiv.org/abs/2506.10204)
*Andrei Paleyes,Radzim Sendyka,Diana Robinson,Christian Cabrera,Neil D. Lawrence*

Main category: cs.SE

TL;DR: 本文提出一种合成评估流程和基于角色的系统方法，用于量化大语言模型在代码生成中对输入提示的敏感性，并验证其通用性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的代码质量高度依赖用户输入的提示质量，而用户背景差异可能导致生成结果显著不同，需系统性评估这种敏感性。

Method: 开发与具体编程任务/LLM无关的合成评估流程及角色化评估框架，通过模拟不同技术背景用户输入分析模型响应差异。

Result: 实验验证了方法的有效性，并开源代码工具以支持社区广泛应用。

Conclusion: 提出的方法具有广泛适用性，能揭示LLM代码生成质量与用户背景的关联，为模型优化提供量化依据。

Abstract: Code generation is one of the most active areas of application of Large
Language Models (LLMs). While LLMs lower barriers to writing code and
accelerate development process, the overall quality of generated programs
depends on the quality of given prompts. Specifically, functionality and
quality of generated code can be sensitive to user's background and familiarity
with software development. It is therefore important to quantify LLM's
sensitivity to variations in the input. To this end we propose a synthetic
evaluation pipeline for code generation with LLMs, as well as a systematic
persona-based evaluation approach to expose qualitative differences of LLM
responses dependent on prospective user background. Both proposed methods are
completely independent from specific programming tasks and LLMs, and thus are
widely applicable. We provide experimental evidence illustrating utility of our
methods and share our code for the benefit of the community.

</details>


### [309] [Formalising Software Requirements using Large Language Models](https://arxiv.org/abs/2506.10704)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: VERIFAI项目旨在通过自然语言处理、本体论、软件构件复用及大语言模型等技术，解决自然语言需求的形式化规范生成与全生命周期追踪验证问题。


<details>
  <summary>Details</summary>
Motivation: 传统自然语言需求在形式化规范生成、系统实现与验证阶段存在可追踪性差、验证困难等挑战，需建立自动化支持框架以确保需求一致性。

Method: 结合自然语言处理(NLP)、领域本体建模、相似系统软件构件复用、大语言模型规范声明及AI流程引导，构建端到端需求追踪验证体系。

Result: 提出VERIFAI项目框架，整合多模态AI技术实现需求形式化规范自动生成，并建立从设计到验证的全链路追踪机制（项目处于初期阶段）。

Conclusion: 通过AI驱动的自动化需求处理与验证体系，可提升软件开发效率与可靠性，为复杂系统形式化验证提供新范式。

Abstract: This paper is a brief introduction to our recently initiated project named
VERIFAI: Traceability and verification of natural language requirements. The
project addresses the challenges in the traceability and verification of formal
specifications through providing support for the automatic generation of the
formal specifications and the traceability of the requirements from the initial
software design stage through the systems implementation and verification.
Approaches explored in this project include Natural Language Processing, use of
ontologies to describe the software system domain, reuse of existing software
artefacts from similar systems (i.e. through similarity based reuse) and large
language models to identify and declare the specifications as well as use of
artificial intelligence to guide the process.

</details>


### [310] [BugGen: A Self-Correcting Multi-Agent LLM Pipeline for Realistic RTL Bug Synthesis](https://arxiv.org/abs/2506.10501)
*Surya Jasper,Minh Luu,Evan Pan,Aakash Tyagi,Michael Quinn,Jiang Hu,David Kebo Houngninou*

Main category: cs.SE

TL;DR: BugGen利用大语言模型构建全自主多智能体流程，自动生成、插入并验证RTL功能缺陷，显著提升验证效率与机器学习调试效果。


<details>
  <summary>Details</summary>
Motivation: 硬件复杂度增加导致验证资源紧张，现有手动/自动缺陷注入方法无法可靠生成多样化、可扩展的缺陷数据集，制约机器学习辅助调试效果。

Method: 通过模块划分+闭环智能体架构选择变异目标，采用迭代优化与回滚机制确保句法正确性和功能可检测性，构建LLM驱动的全流程。

Result: 在OpenTitan IP核中生成500个独特缺陷（功能准确率94%），吞吐量达17.7缺陷/小时（5倍于人工），发现104个未检出缺陷，训练ML模型分类准确率达88.1%-93.2%。

Conclusion: BugGen为高质量缺陷数据集生成提供可扩展解决方案，其生成的缺陷能有效暴露验证覆盖漏洞，提升机器学习辅助调试的实用性与真实性。

Abstract: Hardware complexity continues to strain verification resources, motivating
the adoption of machine learning (ML) methods to improve debug efficiency.
However, ML-assisted debugging critically depends on diverse and scalable bug
datasets, which existing manual or automated bug insertion methods fail to
reliably produce. We introduce BugGen, a first of its kind, fully autonomous,
multi-agent pipeline leveraging Large Language Models (LLMs) to systematically
generate, insert, and validate realistic functional bugs in RTL. BugGen
partitions modules, selects mutation targets via a closed-loop agentic
architecture, and employs iterative refinement and rollback mechanisms to
ensure syntactic correctness and functional detectability. Evaluated across
five OpenTitan IP blocks, BugGen produced 500 unique bugs with 94% functional
accuracy and achieved a throughput of 17.7 validated bugs per hour-over five
times faster than typical manual expert insertion. Additionally, BugGen
identified 104 previously undetected bugs in OpenTitan regressions,
highlighting its utility in exposing verification coverage gaps. Compared
against Certitude, BugGen demonstrated over twice the syntactic accuracy,
deeper exposure of testbench blind spots, and more functionally meaningful and
complex bug scenarios. Furthermore, when these BugGen-generated datasets were
employed to train ML-based failure triage models, we achieved high
classification accuracy (88.1%-93.2%) across different IP blocks, confirming
the practical utility and realism of generated bugs. BugGen thus provides a
scalable solution for generating high-quality bug datasets, significantly
enhancing verification efficiency and ML-assisted debugging.

</details>


### [311] [What Users Value and Critique: Large-Scale Analysis of User Feedback on AI-Powered Mobile Apps](https://arxiv.org/abs/2506.10785)
*Vinaik Chhetri,Krishna Upadhyay,A. B. Siddique,Umar Farooq*

Main category: cs.SE

TL;DR: 本研究首次通过大规模分析89.4万条AI移动应用用户评论，开发多阶段分析流程，揭示用户对AI功能的细粒度正负面评价，发现用户关注生产力、可靠性等技术优势，同时批评技术故障、定价等问题。


<details>
  <summary>Details</summary>
Motivation: AI移动应用快速普及，但用户对其功能的感知与评价缺乏系统性研究。传统方法因用户反馈海量且分析粗粒度，难以捕捉同一评论中矛盾情感，需开发高精度分析框架。

Method: 构建含292个AI驱动应用、14类别的数据集，开发多阶段分析流程（评论分类-情感提取-聚类），验证LLM模型与提示策略，确保各阶段准确性与一致性。

Result: 提取超百万细粒度情感对，聚类为18正向/15负向主题。用户关注点集中：正面强调生产力与个性化，负面聚焦技术故障、定价及语言限制。同一评论可同时包含正负情感，传统方法易遗漏。

Conclusion: 提出可扩展的高精度用户反馈分析框架，揭示AI应用体验的共性与领域特异性问题，证明细粒度情感共现分析能更真实反映用户需求，为优化AI功能设计提供依据。

Abstract: Artificial Intelligence (AI)-powered features have rapidly proliferated
across mobile apps in various domains, including productivity, education,
entertainment, and creativity. However, how users perceive, evaluate, and
critique these AI features remains largely unexplored, primarily due to the
overwhelming volume of user feedback. In this work, we present the first
comprehensive, large-scale study of user feedback on AI-powered mobile apps,
leveraging a curated dataset of 292 AI-driven apps across 14 categories with
894K AI-specific reviews from Google Play. We develop and validate a
multi-stage analysis pipeline that begins with a human-labeled benchmark and
systematically evaluates large language models (LLMs) and prompting strategies.
Each stage, including review classification, aspect-sentiment extraction, and
clustering, is validated for accuracy and consistency. Our pipeline enables
scalable, high-precision analysis of user feedback, extracting over one million
aspect-sentiment pairs clustered into 18 positive and 15 negative user topics.
Our analysis reveals that users consistently focus on a narrow set of themes:
positive comments emphasize productivity, reliability, and personalized
assistance, while negative feedback highlights technical failures (e.g.,
scanning and recognition), pricing concerns, and limitations in language
support. Our pipeline surfaces both satisfaction with one feature and
frustration with another within the same review. These fine-grained,
co-occurring sentiments are often missed by traditional approaches that treat
positive and negative feedback in isolation or rely on coarse-grained analysis.
To this end, our approach provides a more faithful reflection of the real-world
user experiences with AI-powered apps. Category-aware analysis further uncovers
both universal drivers of satisfaction and domain-specific frustrations.

</details>


### [312] [SWE-Factory: Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks](https://arxiv.org/abs/2506.10954)
*Lianghong Guo,Yanlin Wang,Caihua Li,Pengyu Yang,Jiachi Chen,Wei Tao,Yingtian Zou,Duyu Tang,Zibin Zheng*

Main category: cs.SE

TL;DR: 提出自动化流水线SWE-Factory，解决GitHub问题解决数据集构建难题，包含环境构建、标准化评分和自动验证，实验显示高效低成本。


<details>
  <summary>Details</summary>
Motivation: 传统构建GitHub问题解决基准数据集的过程存在环境搭建、测试结果解析和任务实例校验等劳动密集型挑战，亟需自动化解决方案。

Method: 集成三阶段自动化组件：1) SWE-Builder多智能体协作构建评估环境；2) 基于退出码的标准化评分方法；3) 利用可靠退出码信号实现fail2pass自动验证。

Result: 在4种编程语言的671个问题上验证：GPT-4.1-mini以$0.045/实例构建269个有效实例，Gemini-2.5-flash成本最低($0.024/实例)；退出码评分准确率100%，fail2pass验证精确度0.92/召回率1.00。

Conclusion: SWE-Factory通过全流程自动化显著提升数据集构建效率，为LLM软件工程能力训练与评估提供高质量数据支持，已开源代码与数据集。

Abstract: Constructing large-scale datasets for the GitHub issue resolution task is
crucial for both training and evaluating the software engineering capabilities
of Large Language Models (LLMs). However, the traditional process for creating
such benchmarks is notoriously challenging and labor-intensive, particularly in
the stages of setting up evaluation environments, grading test outcomes, and
validating task instances. In this paper, we propose SWE-Factory, an automated
pipeline designed to address these challenges. To tackle these issues, our
pipeline integrates three core automated components. First, we introduce
SWE-Builder, a multi-agent system that automates evaluation environment
construction, which employs four specialized agents that work in a
collaborative, iterative loop and leverages an environment memory pool to
enhance efficiency. Second, we introduce a standardized, exit-code-based
grading method that eliminates the need for manually writing custom parsers.
Finally, we automate the fail2pass validation process using these reliable exit
code signals. Experiments on 671 issues across four programming languages show
that our pipeline can effectively construct valid task instances; for example,
with GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at $0.045 per
instance, while with Gemini-2.5-flash, it achieves comparable performance at
the lowest cost of $0.024 per instance. We also demonstrate that our
exit-code-based grading achieves 100% accuracy compared to manual inspection,
and our automated fail2pass validation reaches a precision of 0.92 and a recall
of 1.00. We hope our automated pipeline will accelerate the collection of
large-scale, high-quality GitHub issue resolution datasets for both training
and evaluation. Our code and datasets are released at
https://github.com/DeepSoftwareAnalytics/swe-factory.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [313] [Encoding call-by-push-value in the pi-calculus](https://arxiv.org/abs/2506.10584)
*Benjamin Bennetzen,Nikolaj Rossander Kristensen,Peter Buus Steffensen*

Main category: cs.LO

TL;DR: 本文提出将Levy的按名调用传值λ演算（CBPV）编码至π演算，并通过手工证明其可靠性与完备性。采用内部π演算（pi-i-calculus）规避形式化挑战，满足Gorla的编码标准，并与Milner的编码进行对比。同时展示了在Coq中形式化验证的初步进展。


<details>
  <summary>Details</summary>
Motivation: 解决CBPV到π演算编码的形式化难题（如de Bruijn索引问题），验证编码的正确性，并通过满足Gorla标准与对比现有方法，增强编码的理论可信度。

Method: 1. 使用pi-i-calculus简化形式化过程（如统一不同双模拟类型）；2. 手工证明可靠性与完备性；3. 扩展至异步多元π演算和本地π演算；4. 在Coq中部分形式化pi-i-calculus的证明。

Result: 成功证明编码的可靠性与完备性，满足Gorla五标准，展示与Milner编码的相似性，并在Coq中初步实现形式化验证（部分引理依赖手工证明）。

Conclusion: 提出的CBPV到π演算编码在理论与形式化层面均有效，手工证明的合理性为未来完整形式化奠定了基础，且pi-i-calculus的特性简化了验证过程。

Abstract: In this report we define an encoding of Levys call-by-push-value
lambda-calculus (CBPV) in the pi-calculus, and prove that our encoding is both
sound and complete. We present informal (by-hand) proofs of soundness,
completeness, and all required lemmas. The encoding is specialized to the
internal pi-calculus (pi-i-calculus) to circumvent certain challenges
associated with using de Bruijn index in a formalization, and it also helps
with bisimulation as early-, late- and open-bisimulation coincide in this
setting, furthermore bisimulation is a congruence. Additionally, we argue that
our encoding also satisfies the five criteria for good encodings proposed by
Gorla, as well as show similarities between Milners and our encoding. This
paper includes encodings from CBPV in the pi-i-calculus, asynchronous polyadic
pi-calculus and the local pi-calculus. We begin a formalization of the proof in
Coq for the soundness and completeness of the encoding in the pi-i-calculus.
Not all lemmas used in the formalization are themselves formally proven.
However, we argue that the non-proven lemmas are reasonable, as they are proven
by hand, or amount to Coq formalities that are straightforward given informal
arguments.

</details>


### [314] [StepProof: Step-by-step verification of natural language mathematical proofs](https://arxiv.org/abs/2506.10558)
*Xiaolin Hu,Qinghua Zhou,Bogdan Grechuk,Ivan Y. Tyukin*

Main category: cs.LO

TL;DR: 本文提出StepProof方法，通过将完整证明分解为多个可验证的子证明，实现句子级自动形式化验证，显著提升验证成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有交互式定理证明器（ITPs）缺乏自然语言接口，且传统自动形式化方法仅支持完整证明验证，无法进行细粒度验证。

Method: StepProof将自然语言证明拆分为多个可独立验证的子证明，支持逐句形式化验证，并通过人工微调自然语言证明优化性能。

Result: 实验表明StepProof在验证成功率和效率上优于传统方法，且人工微调可进一步提升其形式化表现。

Conclusion: StepProof通过细粒度验证机制解决了现有自动形式化的局限性，为数学证明的形式化验证提供了更灵活高效的解决方案。

Abstract: Interactive theorem provers (ITPs) are powerful tools for the formal
verification of mathematical proofs down to the axiom level. However, their
lack of a natural language interface remains a significant limitation. Recent
advancements in large language models (LLMs) have enhanced the understanding of
natural language inputs, paving the way for autoformalization - the process of
translating natural language proofs into formal proofs that can be verified.
Despite these advancements, existing autoformalization approaches are limited
to verifying complete proofs and lack the capability for finer, sentence-level
verification. To address this gap, we propose StepProof, a novel
autoformalization method designed for granular, step-by-step verification.
StepProof breaks down complete proofs into multiple verifiable subproofs,
enabling sentence-level verification. Experimental results demonstrate that
StepProof significantly improves proof success rates and efficiency compared to
traditional methods. Additionally, we found that minor manual adjustments to
the natural language proofs, tailoring them for step-level verification,
further enhanced StepProof's performance in autoformalization.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [315] [From Tool Calling to Symbolic Thinking: LLMs in a Persistent Lisp Metaprogramming Loop](https://arxiv.org/abs/2506.10021)
*Jordi de la Torre*

Main category: cs.PL

TL;DR: 提出一种将大语言模型与持久化Lisp环境集成的新架构，通过REPL交互实现工具自进化，结合符号编程与神经生成技术。


<details>
  <summary>Details</summary>
Motivation: 解决现有大语言模型在工具动态创建、状态保持和反射式编程方面的局限性，提升AI系统的交互性与自适应性。

Method: 在生成过程中嵌入Lisp表达式，通过中间件拦截实现与实时REPL的程序化交互，建立带状态的外部记忆系统。

Result: 构建支持动态工具创建、反射编程的框架，提出整合符号编程与神经生成的系统设计原则。

Conclusion: 该架构为开发具备持续学习能力的交互式AI系统提供了可行路径，实现了符号操作与语言生成的有机融合。

Abstract: We propose a novel architecture for integrating large language models (LLMs)
with a persistent, interactive Lisp environment. This setup enables LLMs to
define, invoke, and evolve their own tools through programmatic interaction
with a live REPL. By embedding Lisp expressions within generation and
intercepting them via a middleware layer, the system allows for stateful
external memory, reflective programming, and dynamic tool creation. We present
a design framework and architectural principles to guide future implementations
of interactive AI systems that integrate symbolic programming with neural
language generation.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [316] [Discrete Audio Tokens: More Than a Survey!](https://arxiv.org/abs/2506.10274)
*Pooneh Mousavi,Gallil Maimon,Adel Moumen,Darius Petermann,Jiatong Shi,Haibin Wu,Haici Yang,Anastasia Kuznetsova,Artem Ploujnikov,Ricard Marxer,Bhuvana Ramabhadran,Benjamin Elizalde,Loren Lugosch,Jinyu Li,Cem Subakan,Phil Woodland,Minje Kim,Hung-yi Lee,Shinji Watanabe,Yossi Adi,Mirco Ravanelli*

Main category: cs.SD

TL;DR: 本文系统综述并评估了离散音频标记器，提出基于编码器-解码器、量化技术等维度的分类法，在语音、音乐和通用音频领域进行多基准测试，分析性能权衡与局限性，为未来研究提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有离散音频标记研究多聚焦特定领域，缺乏跨领域统一评估标准。本文旨在通过系统性分类与多维度基准测试，揭示方法间的性能权衡，填补领域空白。

Method: 提出五维分类法（编码器-解码器结构/量化技术/训练范式/流式处理/应用领域），在重建质量、下游任务、声学语言建模等基准上评估标记器，并通过控制变量实验分析性能折衷。

Result: 实验表明不同标记器在重建保真度、计算效率与任务泛化性间存在显著权衡，流式处理能力普遍不足。开源标记器数据库揭示了跨领域适应性差异与声学语言建模的扩展瓶颈。

Conclusion: 离散音频标记器在整合语音与LLM方面潜力显著，但面临流式处理、跨领域泛化与评估标准化等挑战。需优化量化鲁棒性与声学-语言模态对齐机制以推动实际应用。

Abstract: Discrete audio tokens are compact representations that aim to preserve
perceptual quality, phonetic content, and speaker characteristics while
enabling efficient storage and inference, as well as competitive performance
across diverse downstream tasks.They provide a practical alternative to
continuous features, enabling the integration of speech and audio into modern
large language models (LLMs). As interest in token-based audio processing
grows, various tokenization methods have emerged, and several surveys have
reviewed the latest progress in the field. However, existing studies often
focus on specific domains or tasks and lack a unified comparison across various
benchmarks. This paper presents a systematic review and benchmark of discrete
audio tokenizers, covering three domains: speech, music, and general audio. We
propose a taxonomy of tokenization approaches based on encoder-decoder,
quantization techniques, training paradigm, streamability, and application
domains. We evaluate tokenizers on multiple benchmarks for reconstruction,
downstream performance, and acoustic language modeling, and analyze trade-offs
through controlled ablation studies. Our findings highlight key limitations,
practical considerations, and open challenges, providing insight and guidance
for future research in this rapidly evolving area. For more information,
including our main results and tokenizer database, please refer to our website:
https://poonehmousavi.github.io/dates-website/.

</details>


### [317] [Fine-Grained control over Music Generation with Activation Steering](https://arxiv.org/abs/2506.10225)
*Dipanshu Panda,Jayden Koshy Joe,Harshith M R,Swathi Narashiman,Pranay Mathur,Anish Veerakumar,Aniruddh Krishna,Keerthiharan A*

Main category: cs.SD

TL;DR: 提出一种在推理时干预MusicGen模型的方法，通过线性探针和注意力层引导实现音色、风格转换及流派融合，结合文本提示实现全局与局部控制。


<details>
  <summary>Details</summary>
Motivation: 现有音乐生成模型在细粒度控制（如音色、风格）上存在局限，需结合全局文本条件与局部干预手段以提升生成灵活性。

Method: 在推理时对残差流施加线性探针权重干预，或调整注意力层激活方向；采用回归任务及均方误差损失保留激活空间方向信息。

Result: 通过回归建模有效保留语义方向性，结合文本提示实现音色/风格迁移与流派混合，音频样本验证了全局-局部双重控制能力。

Conclusion: 该方法通过推理时干预机制扩展了MusicGen的控制维度，为音乐生成提供了更细粒度的语义编辑框架。

Abstract: We present a method for fine-grained control over music generation through
inference-time interventions on an autoregressive generative music transformer
called MusicGen. Our approach enables timbre transfer, style transfer, and
genre fusion by steering the residual stream using weights of linear probes
trained on it, or by steering the attention layer activations in a similar
manner. We observe that modelling this as a regression task provides improved
performance, hypothesizing that the mean-squared-error better preserve
meaningful directional information in the activation space. Combined with the
global conditioning offered by text prompts in MusicGen, our method provides
both global and local control over music generation. Audio samples illustrating
our method are available at our demo page.

</details>


### [318] [PAL: Probing Audio Encoders via LLMs -- A Study of Information Transfer from Audio Encoders to LLMs](https://arxiv.org/abs/2506.10423)
*Tony Alex,Wish Suharitdamrong,Sara Atito,Armin Mustafa,Philip J. B. Jackson,Imran Razzak,Muhammad Awais*

Main category: cs.SD

TL;DR: 本文系统研究了音频-LLM架构设计对跨模态信息传递的影响，提出延迟音频集成、注意力模块专用探测和多编码器集成三项改进，在560万音频-文本对数据集上验证后，最终模型相对基线提升10%-60%。


<details>
  <summary>Details</summary>
Motivation: 现有音频-LLM研究主要关注应用层数据构建，但对音频编码器与LLM之间的语义传递机制缺乏深入理解。本文旨在探索如何通过架构设计优化跨模态信息交互效率。

Method: 基于Pengi/LLaVA架构提出三项改进：(1)延迟音频特征融合以建立文本上下文；(2)仅通过LLM注意力模块探测音频表征；(3)集成多编码器提供互补信息。使用560万音频-文本对进行三阶段训练验证。

Result: 实验表明：延迟集成使LLM能更有效提取音频特征；注意力模块单独处理即可完成探测；多编码器集成显著扩展信息覆盖。最终架构相对基线获得10%-60%的绝对性能提升。

Conclusion: 通过系统化的架构设计优化，验证了跨模态信息传递效率与模型结构强相关。提出的延迟融合、注意力专用探测和多编码器集成策略，为音频-LLM研究提供了新的设计范式。

Abstract: The integration of audio perception capabilities into Large Language Models
(LLMs) has enabled significant advances in Audio-LLMs. Although
application-focused developments, particularly in curating training data for
specific capabilities e.g., audio reasoning, have progressed rapidly, the
underlying mechanisms that govern efficient transfer of rich semantic
representations from audio encoders to LLMs remain under-explored. We
conceptualize effective audio-LLM interaction as the LLM's ability to
proficiently probe the audio encoder representations to satisfy textual
queries. This paper presents a systematic investigation on how architectural
design choices can affect that. Beginning with a standard Pengi/LLaVA-style
audio-LLM architecture, we propose and evaluate several modifications guided by
hypotheses derived from mechanistic interpretability studies and LLM
operational principles. Our experiments demonstrate that: (1) delaying audio
integration until the LLM's initial layers establish textual context that
enhances its ability to probe the audio representations for relevant
information; (2) the LLM can proficiently probe audio representations
exclusively through LLM layer's attention submodule, without requiring
propagation to its Feed-Forward Network (FFN) submodule; (3) an efficiently
integrated ensemble of diverse audio encoders provides richer, complementary
representations, thereby broadening the LLM's capacity to probe a wider
spectrum of audio information. All hypotheses are evaluated using an identical
three-stage training curriculum on a dataset of 5.6 million audio-text pairs,
ensuring controlled comparisons. Our final architecture, which incorporates all
proposed modifications, achieves relative improvements from 10\% to 60\% over
the baseline, validating our approach to optimizing cross-modal information
transfer in audio-LLMs. Project page: https://ta012.github.io/PAL/

</details>


### [319] [BNMusic: Blending Environmental Noises into Personalized Music](https://arxiv.org/abs/2506.10754)
*Chi Zuo,Martin B. Møller,Pablo Martínez-Nuevo,Huayang Huang,Yu Wu,Ye Zhu*

Main category: cs.SD

TL;DR: 本文提出BNMusic框架，通过将环境噪音融入个性化生成音乐中，结合自适应放大技术，有效降低噪音感知并提升听觉体验。


<details>
  <summary>Details</summary>
Motivation: 传统声学掩蔽技术因主导声音与噪音节拍不匹配等问题，需过度提高音量才能掩盖噪音，存在局限性。受跨模态生成技术启发，探索通过生成与噪音节拍对齐的个性化音乐实现更自然的掩蔽。

Method: 采用两阶段梅尔频谱图处理框架：1) 基于用户文本提示生成包含噪音音乐特征的完整音乐；2) 自适应放大生成片段以优化噪音混合效果，同时保持音质。

Result: 在MusicBench、EPIC-SOUNDS和ESC-50数据集上的实验表明，该方法能生成节奏对齐、听觉愉悦的音乐片段，显著降低噪音显著度。

Conclusion: BNMusic框架通过跨模态生成与自适应调整，实现了环境噪音与个性化音乐的自然融合，为声学掩蔽提供了创新解决方案。

Abstract: While being disturbed by environmental noises, the acoustic masking technique
is a conventional way to reduce the annoyance in audio engineering that seeks
to cover up the noises with other dominant yet less intrusive sounds. However,
misalignment between the dominant sound and the noise-such as mismatched
downbeats-often requires an excessive volume increase to achieve effective
masking. Motivated by recent advances in cross-modal generation, in this work,
we introduce an alternative method to acoustic masking, aiming to reduce the
noticeability of environmental noises by blending them into personalized music
generated based on user-provided text prompts. Following the paradigm of music
generation using mel-spectrogram representations, we propose a Blending Noises
into Personalized Music (BNMusic) framework with two key stages. The first
stage synthesizes a complete piece of music in a mel-spectrogram representation
that encapsulates the musical essence of the noise. In the second stage, we
adaptively amplify the generated music segment to further reduce noise
perception and enhance the blending effectiveness, while preserving auditory
quality. Our experiments with comprehensive evaluations on MusicBench,
EPIC-SOUNDS, and ESC-50 demonstrate the effectiveness of our framework,
highlighting the ability to blend environmental noise with rhythmically
aligned, adaptively amplified, and enjoyable music segments, minimizing the
noticeability of the noise, thereby improving overall acoustic experiences.

</details>
