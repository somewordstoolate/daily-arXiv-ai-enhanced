<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 61]
- [cs.AI](#cs.AI) [Total: 45]
- [cs.LG](#cs.LG) [Total: 85]
- [math.NA](#math.NA) [Total: 1]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [cs.CV](#cs.CV) [Total: 32]
- [cs.CR](#cs.CR) [Total: 3]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.IR](#cs.IR) [Total: 6]
- [quant-ph](#quant-ph) [Total: 2]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.RO](#cs.RO) [Total: 7]
- [cs.CY](#cs.CY) [Total: 6]
- [physics.soc-ph](#physics.soc-ph) [Total: 2]
- [cs.DB](#cs.DB) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [stat.ML](#stat.ML) [Total: 11]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.SE](#cs.SE) [Total: 12]
- [cs.GR](#cs.GR) [Total: 3]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.SD](#cs.SD) [Total: 10]
- [physics.ao-ph](#physics.ao-ph) [Total: 2]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.HC](#cs.HC) [Total: 6]
- [eess.SY](#eess.SY) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.NI](#cs.NI) [Total: 1]
- [math.ST](#math.ST) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to Answer Climate Change Queries](https://arxiv.org/abs/2506.13796)
*Zhou Chen,Xiao Wang,Yuanhong Liao,Ming Lin,Yuqi Bai*

Main category: cs.CL

TL;DR: 本研究提出一种自动化构建气候变化指令数据的方法，创建ClimateChat-Corpus数据集并训练ClimateChat模型，显著提升气候变化问答任务性能，验证了基础模型选择对指令调优的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有研究在高效生成高精度气候变化指令数据方面存在不足，限制了气候变化专用大语言模型（LLMs）的进一步开发。

Method: 通过文档事实与背景知识生成指令，结合网页爬取和种子指令收集增强数据多样性，构建ClimateChat-Corpus数据集并微调开源LLMs。

Result: ClimateChat在气候变化问答任务中表现显著提升，实验表明不同基础模型与指令数据对LLM性能具有重要影响。

Conclusion: 该研究为气候变化指令数据构建及专用LLMs训练提供实证支持，强调选择合适基础模型对指令调优的关键作用。

Abstract: As the issue of global climate change becomes increasingly severe, the demand for research in climate science continues to grow. Natural language processing technologies, represented by Large Language Models (LLMs), have been widely applied to climate change-specific research, providing essential information support for decision-makers and the public. Some studies have improved model performance on relevant tasks by constructing climate change-related instruction data and instruction-tuning LLMs. However, current research remains inadequate in efficiently producing large volumes of high-precision instruction data for climate change, which limits further development of climate change LLMs. This study introduces an automated method for constructing instruction data. The method generates instructions using facts and background knowledge from documents and enhances the diversity of the instruction data through web scraping and the collection of seed instructions. Using this method, we constructed a climate change instruction dataset, named ClimateChat-Corpus, which was used to fine-tune open-source LLMs, resulting in an LLM named ClimateChat. Evaluation results show that ClimateChat significantly improves performance on climate change question-and-answer tasks. Additionally, we evaluated the impact of different base models and instruction data on LLM performance and demonstrated its capability to adapt to a wide range of climate change scientific discovery tasks, emphasizing the importance of selecting an appropriate base model for instruction tuning. This research provides valuable references and empirical support for constructing climate change instruction data and training climate change-specific LLMs.

</details>


### [2] [Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles](https://arxiv.org/abs/2506.13886)
*Antara Raaghavi Bhattacharya,Isabel Papadimitriou,Kathryn Davidson,David Alvarez-Melis*

Main category: cs.CL

TL;DR: 大语言模型在处理跨语言数字系统时表现不佳，需显式数学标记才能解决，而人类能利用隐含结构。


<details>
  <summary>Details</summary>
Motivation: 探究为何人类能灵活处理不同语言的数字系统，而大语言模型在涉及跨语言数字的数学谜题上存在困难。

Method: 通过实验分离数字的语言与数学属性，并进行消融研究以测试数字构造参数对模型性能的影响。

Result: 模型仅在数学符号显式标记时能解决问题；人类通过语言理解推断隐含结构，而模型缺乏此能力。

Conclusion: 现有推理模型难以从数据中灵活推断隐含组合规则，解决此问题仍是开放挑战。

Abstract: Across languages, numeral systems vary widely in how they construct and combine numbers. While humans consistently learn to navigate this diversity, large language models (LLMs) struggle with linguistic-mathematical puzzles involving cross-linguistic numeral systems, which humans can learn to solve successfully. We investigate why this task is difficult for LLMs through a series of experiments that untangle the linguistic and mathematical aspects of numbers in language. Our experiments establish that models cannot consistently solve such problems unless the mathematical operations in the problems are explicitly marked using known symbols ($+$, $\times$, etc, as in "twenty + three"). In further ablation studies, we probe how individual parameters of numeral construction and combination affect performance. While humans use their linguistic understanding of numbers to make inferences about the implicit compositional structure of numerals, LLMs seem to lack this notion of implicit numeral structure. We conclude that the ability to flexibly infer compositional rules from implicit patterns in human-scale data remains an open challenge for current reasoning models.

</details>


### [3] [VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training](https://arxiv.org/abs/2506.13888)
*Jipeng Zhang,Kehao Miao,Renjie Pi,Zhaowei Wang,Runtao Liu,Rui Pan,Tong Zhang*

Main category: cs.CL

TL;DR: 本文提出一种迭代训练框架，通过结合视觉专家、思维链推理及边际拒绝采样，解决视觉语言奖励模型（VL-RM）训练中的自举困境与模态偏差问题，提升多模态模型对齐性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言奖励模型（VL-RM）训练面临两大挑战：1) 自举困境（高质量数据依赖强VL模型，导致监督数据循环偏差）；2) 模态偏差与负例放大（模型幻觉生成错误视觉属性，误导训练偏好数据）。

Method: 提出迭代训练框架，整合视觉专家知识、思维链（CoT）结构化反馈及基于边际的拒绝采样，优化偏好数据集质量，增强多模态推理与批判能力。

Result: 实验表明，该方法在VL-RM基准测试中表现优异，尤其在幻觉检测与多模态推理任务中显著提升模型对齐效果。

Conclusion: 通过系统性解决数据偏差与模态交互问题，该框架有效推进了基于强化学习的视觉语言模型对齐技术发展。

Abstract: Reinforcement Fine-Tuning (RFT) with verifiable rewards has advanced large language models but remains underexplored for Vision-Language (VL) models. The Vision-Language Reward Model (VL-RM) is key to aligning VL models by providing structured feedback, yet training effective VL-RMs faces two major challenges. First, the bootstrapping dilemma arises as high-quality training data depends on already strong VL models, creating a cycle where self-generated supervision reinforces existing biases. Second, modality bias and negative example amplification occur when VL models hallucinate incorrect visual attributes, leading to flawed preference data that further misguides training. To address these issues, we propose an iterative training framework leveraging vision experts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection Sampling. Our approach refines preference datasets, enhances structured critiques, and iteratively improves reasoning. Experiments across VL-RM benchmarks demonstrate superior performance in hallucination detection and multimodal reasoning, advancing VL model alignment with reinforcement learning.

</details>


### [4] [EmoNews: A Spoken Dialogue System for Expressive News Conversations](https://arxiv.org/abs/2506.13894)
*Ryuki Matsuura,Shikhar Bharadwaj,Jiarui Liu,Dhatchi Kunde Govindarajan*

Main category: cs.CL

TL;DR: 开发基于情感语音调节的任务导向对话系统，结合LLM情感分析与PromptTTS合成技术，提升新闻对话的同理心与参与度。


<details>
  <summary>Details</summary>
Motivation: 现有任务导向情感对话系统因SDS与情感TTS研究分离及缺乏社交目标评估标准而发展不足，需探索情感语音在对话系统中的作用。

Method: 使用LLM情感分析器识别上下文情感，结合PromptTTS生成适配情感语音，并提出情感SDS主观评估量表进行性能对比。

Result: 实验表明，该系统在情感调节与用户参与度上优于基线系统，验证情感语音对提升对话质量的关键作用。

Conclusion: 上下文适配的情感语音能显著增强对话参与度，为任务导向情感对话系统研究提供技术框架与评估基准。

Abstract: We develop a task-oriented spoken dialogue system (SDS) that regulates emotional speech based on contextual cues to enable more empathetic news conversations. Despite advancements in emotional text-to-speech (TTS) techniques, task-oriented emotional SDSs remain underexplored due to the compartmentalized nature of SDS and emotional TTS research, as well as the lack of standardized evaluation metrics for social goals. We address these challenges by developing an emotional SDS for news conversations that utilizes a large language model (LLM)-based sentiment analyzer to identify appropriate emotions and PromptTTS to synthesize context-appropriate emotional speech. We also propose subjective evaluation scale for emotional SDSs and judge the emotion regulation performance of the proposed and baseline systems. Experiments showed that our emotional SDS outperformed a baseline system in terms of the emotion regulation and engagement. These results suggest the critical role of speech emotion for more engaging conversations. All our source code is open-sourced at https://github.com/dhatchi711/espnet-emotional-news/tree/emo-sds/egs2/emo_news_sds/sds1

</details>


### [5] [Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise Pooled Representations](https://arxiv.org/abs/2506.13901)
*Abhilekh Borah,Chhavi Sharma,Danush Khanna,Utkarsh Bhatt,Gurpreet Singh,Hasnat Md Abdullah,Raghav Kaushik Ravi,Vinija Jain,Jyoti Patel,Shubham Singh,Vasu Sharma,Arpita Vats,Rahul Raja,Aman Chadha,Amitava Das*

Main category: cs.CL

TL;DR: 本文提出了一种新的评估指标AQI（Alignment Quality Index），通过分析潜在空间中安全与不安全激活的分离情况，检测大语言模型（LLM）的隐藏未对齐风险，并引入LITMUS数据集支持鲁棒评估。实验表明AQI能有效发现传统拒绝率指标遗漏的漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）对齐评估依赖行为代理指标（如拒绝率、毒性分类器），存在盲点，导致模型易受越狱攻击、生成随机性及虚假对齐问题。在高风险领域应用中，模型行为必须可靠反映人类价值观与安全约束，亟需更鲁棒的评估方法。

Method: 提出AQI指标：结合Davies-Bouldin Score、Dunn Index等聚类质量指标，量化潜在空间中安全/不安全激活的分离程度，实现解码不变性且提示无关的对齐质量评估。同时构建LITMUS数据集，支持在对抗性条件下进行鲁棒测试。

Result: 在DPO、GRPO、RLHF等不同训练策略的模型上验证，AQI与外部人工评估结果相关性强，能发现传统拒绝率指标遗漏的漏洞，并可作为虚假对齐的早期预警信号。

Conclusion: AQI为模型安全审计提供了行为无关的鲁棒工具，LITMUS数据集推动了复杂场景下的对齐研究。公开代码以促进该领域发展。

Abstract: Alignment is no longer a luxury, it is a necessity. As large language models (LLMs) enter high-stakes domains like education, healthcare, governance, and law, their behavior must reliably reflect human-aligned values and safety constraints. Yet current evaluations rely heavily on behavioral proxies such as refusal rates, G-Eval scores, and toxicity classifiers, all of which have critical blind spots. Aligned models are often vulnerable to jailbreaking, stochasticity of generation, and alignment faking.
  To address this issue, we introduce the Alignment Quality Index (AQI). This novel geometric and prompt-invariant metric empirically assesses LLM alignment by analyzing the separation of safe and unsafe activations in latent space. By combining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI), Xie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various formulations, AQI captures clustering quality to detect hidden misalignments and jailbreak risks, even when outputs appear compliant. AQI also serves as an early warning signal for alignment faking, offering a robust, decoding invariant tool for behavior agnostic safety auditing.
  Additionally, we propose the LITMUS dataset to facilitate robust evaluation under these challenging conditions. Empirical tests on LITMUS across different models trained under DPO, GRPO, and RLHF conditions demonstrate AQI's correlation with external judges and ability to reveal vulnerabilities missed by refusal metrics. We make our implementation publicly available to foster future research in this area.

</details>


### [6] [ASMR: Augmenting Life Scenario using Large Generative Models for Robotic Action Reflection](https://arxiv.org/abs/2506.13956)
*Shang-Chi Tsai,Seiya Kawano,Angel Garcia Contreras,Koichiro Yoshino,Yun-Nung Chen*

Main category: cs.CL

TL;DR: 本文提出一种结合大语言模型和稳定扩散模型的数据增强框架，通过生成模拟对话和环境图像来增强多模态模型训练，从而提升机器人在真实场景中的动作选择能力。


<details>
  <summary>Details</summary>
Motivation: 在机器人辅助日常任务中，结合视觉与语言线索理解用户意图至关重要，但构建大规模多模态数据集成本高昂且耗时，限制了模型训练效果。

Method: 使用大语言模型生成潜在对话与环境上下文，并利用稳定扩散模型合成对应场景图像，通过增强数据优化多模态模型，使其在有限真实数据下更精准决策。

Result: 基于真实场景数据集的实验表明，该方法显著提升了机器人动作选择的准确性，达到了当前最优性能。

Conclusion: 所提出的数据增强框架有效解决了多模态数据稀缺问题，为机器人意图理解提供了可扩展且高效的解决方案。

Abstract: When designing robots to assist in everyday human activities, it is crucial to enhance user requests with visual cues from their surroundings for improved intent understanding. This process is defined as a multimodal classification task. However, gathering a large-scale dataset encompassing both visual and linguistic elements for model training is challenging and time-consuming. To address this issue, our paper introduces a novel framework focusing on data augmentation in robotic assistance scenarios, encompassing both dialogues and related environmental imagery. This approach involves leveraging a sophisticated large language model to simulate potential conversations and environmental contexts, followed by the use of a stable diffusion model to create images depicting these environments. The additionally generated data serves to refine the latest multimodal models, enabling them to more accurately determine appropriate actions in response to user interactions with the limited target data. Our experimental results, based on a dataset collected from real-world scenarios, demonstrate that our methodology significantly enhances the robot's action selection capabilities, achieving the state-of-the-art performance.

</details>


### [7] [Are manual annotations necessary for statutory interpretations retrieval?](https://arxiv.org/abs/2506.13965)
*Aleksander Smywiński-Pohl,Tomer Libal,Adam Kaczmarczyk,Magdalena Król*

Main category: cs.CL

TL;DR: 本文探讨如何优化法律概念解释的标注过程，通过实验确定最佳标注数量、句子选择策略及使用大语言模型（LLM）自动化的效果。


<details>
  <summary>Details</summary>
Motivation: 现有法律概念解释检索方法依赖人工标注，成本高且需针对每个概念重复标注，因此研究如何减少标注量并实现自动化。

Method: 通过实验分析三个问题：1) 每个法律概念的最佳标注数量；2) 随机标注与精选候选句对模型的影响；3) 使用LLM自动化标注的效果。

Result: 实验表明存在标注量的优化阈值，精选候选句可提升模型性能，LLM自动化标注具备潜力但需进一步验证可靠性。

Conclusion: 部分场景下减少人工标注可行，精选标注策略有效，LLM自动化是未来方向但需结合人工校验。

Abstract: One of the elements of legal research is looking for cases where judges have extended the meaning of a legal concept by providing interpretations of what a concept means or does not mean. This allow legal professionals to use such interpretations as precedents as well as laymen to better understand the legal concept. The state-of-the-art approach for retrieving the most relevant interpretations for these concepts currently depends on the ranking of sentences and the training of language models over annotated examples. That manual annotation process can be quite expensive and need to be repeated for each such concept, which prompted recent research in trying to automate this process. In this paper, we highlight the results of various experiments conducted to determine the volume, scope and even the need for manual annotation. First of all, we check what is the optimal number of annotations per a legal concept. Second, we check if we can draw the sentences for annotation randomly or there is a gain in the performance of the model, when only the best candidates are annotated. As the last question we check what is the outcome of automating the annotation process with the help of an LLM.

</details>


### [8] [AI shares emotion with humans across languages and cultures](https://arxiv.org/abs/2506.13978)
*Xiuwen Wu,Hao Wang,Zhiang Yan,Xiaohan Tang,Pengfei Xu,Wai-Ting Siok,Ping Li,Jia-Hong Gao,Bingjiang Lyu,Lang Qin*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型（LLM）在情感表达上与人类感知结构一致，并能通过心理情感概念精确调控输出情感。


<details>
  <summary>Details</summary>
Motivation: 探讨AI系统是否能像人类一样表达情感，以及如何控制其输出的情感基调，以实现更有效的人机协作。

Method: 通过跨语言文化群体和模型家族的比较，使用可解释的LLM特征评估人机情感对齐，并利用情感概念调控模型输出。

Result: LLM的情感空间与人类感知结构一致，基于效价和唤醒度；情感特征能预测人类对词汇的情感评分，且可稳定调控模型情感输出。

Conclusion: AI不仅与人类共享情感表征，还能通过心理学基础的情感概念精确引导其情感输出。

Abstract: Effective and safe human-machine collaboration requires the regulated and meaningful exchange of emotions between humans and artificial intelligence (AI). Current AI systems based on large language models (LLMs) can provide feedback that makes people feel heard. Yet it remains unclear whether LLMs represent emotion in language as humans do, or whether and how the emotional tone of their output can be controlled. We assess human-AI emotional alignment across linguistic-cultural groups and model-families, using interpretable LLM features translated from concept-sets for over twenty nuanced emotion categories (including six basic emotions). Our analyses reveal that LLM-derived emotion spaces are structurally congruent with human perception, underpinned by the fundamental affective dimensions of valence and arousal. Furthermore, these emotion-related features also accurately predict large-scale behavioural data on word ratings along these two core dimensions, reflecting both universal and language-specific patterns. Finally, by leveraging steering vectors derived solely from human-centric emotion concepts, we show that model expressions can be stably and naturally modulated across distinct emotion categories, which provides causal evidence that human emotion concepts can be used to systematically induce LLMs to produce corresponding affective states when conveying content. These findings suggest AI not only shares emotional representations with humans but its affective outputs can be precisely guided using psychologically grounded emotion concepts.

</details>


### [9] [Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text](https://arxiv.org/abs/2506.14012)
*Amr Mohamed,Yang Zhang,Michalis Vazirgiannis,Guokan Shang*

Main category: cs.CL

TL;DR: 本文系统评估了大型语言模型（LLM）对混合语言（代码转换）文本的理解能力，发现外语干扰英语会降低性能，但英语嵌入其他语言常提升理解，微调是缓解性能下降的有效方法。


<details>
  <summary>Details</summary>
Motivation: 随着多语言社区及在线内容中代码转换现象的普及，需探究LLM如何处理此类混合语言输入，因其在内容生成与处理中至关重要。

Method: 通过生成代码转换版本的经典推理与理解基准测试，系统评估LLM在混合语言场景下的表现，并分析提示策略与微调的效果。

Result: 外语干扰英语时模型性能显著下降（即使符合语言约束），但英语嵌入其他语言常提升理解；提示策略效果不稳定，微调能更稳定缓解性能下降。

Conclusion: 代码转换对LLM理解的影响具有不对称性，微调是改善模型对混合语言文本处理能力的可靠路径。

Abstract: Code-switching (CSW) is the act of alternating between two or more languages within a single discourse. This phenomenon is widespread in multilingual communities, and increasingly prevalent in online content, where users naturally mix languages in everyday communication. As a result, Large Language Models (LLMs), now central to content processing and generation, are frequently exposed to code-switched inputs. Given their widespread use, it is crucial to understand how LLMs process and reason about such mixed-language text. This paper presents a systematic evaluation of LLM comprehension under code-switching by generating CSW variants of established reasoning and comprehension benchmarks. While degradation is evident when foreign tokens disrupt English text$\unicode{x2013}$even under linguistic constraints$\unicode{x2013}$embedding English into other languages often improves comprehension. Though prompting yields mixed results, fine-tuning offers a more stable path to degradation mitigation.

</details>


### [10] [MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark for Financial LLM Evaluation](https://arxiv.org/abs/2506.14028)
*Xueqing Peng,Lingfei Qian,Yan Wang,Ruoyu Xiang,Yueru He,Yang Ren,Mingyang Jiang,Jeff Zhao,Huan He,Yi Han,Yun Feng,Yuechen Jiang,Yupeng Cao,Haohang Li,Yangyang Yu,Xiaoyu Wang,Penglei Gao,Shengyuan Lin,Keyi Wang,Shanshan Yang,Yilun Zhao,Zhiwei Liu,Peng Lu,Jerry Huang,Suyuchen Wang,Triantafillos Papadopoulos,Polydoros Giannouris,Efstathia Soufleri,Nuo Chen,Guojun Xiong,Zhiyang Deng,Yijia Zhao,Mingquan Lin,Meikang Qiu,Kaleb E Smith,Arman Cohan,Xiao-Yang Liu,Jimin Huang,Alejandro Lopez-Lira,Xi Chen,Junichi Tsujii,Jian-Yun Nie,Sophia Ananiadou,Qianqian Xie*

Main category: cs.CL

TL;DR: 论文提出了首个多语言多模态金融领域基准测试MultiFinBen，包含跨语言推理和视觉文本分析任务，评估发现现有最强模型在复杂金融任务中表现不佳，并开源数据集以促进研究。


<details>
  <summary>Details</summary>
Motivation: 现有金融NLP基准局限于单语、单模态和简单任务，无法反映真实场景的复杂性，需构建更全面的评估体系。

Method: 创建MultiFinBen基准，涵盖多模态（文本/视觉/音频）和多语言场景，设计跨语言金融问答(PolyFiQA)和OCR嵌入式任务，采用动态难度选择机制构建平衡数据集。

Result: 对22个前沿模型的测试显示，即使最强模型在跨语言多模态金融任务中表现显著不足（如GPT-4准确率仅50%），揭示现有能力与真实需求的差距。

Conclusion: MultiFinBen填补了金融AI评估体系的空白，推动透明可复现的研究，同时暴露模型在专业领域多模态跨语言推理的薄弱环节，为未来优化指明方向。

Abstract: Recent advances in large language models (LLMs) have accelerated progress in financial NLP and applications, yet existing benchmarks remain limited to monolingual and unimodal settings, often over-relying on simple tasks and failing to reflect the complexity of real-world financial communication. We introduce MultiFinBen, the first multilingual and multimodal benchmark tailored to the global financial domain, evaluating LLMs across modalities (text, vision, audio) and linguistic settings (monolingual, bilingual, multilingual) on domain-specific tasks. We introduce two novel tasks, including PolyFiQA-Easy and PolyFiQA-Expert, the first multilingual financial benchmarks requiring models to perform complex reasoning over mixed-language inputs; and EnglishOCR and SpanishOCR, the first OCR-embedded financial QA tasks challenging models to extract and reason over information from visual-text financial documents. Moreover, we propose a dynamic, difficulty-aware selection mechanism and curate a compact, balanced benchmark rather than simple aggregation existing datasets. Extensive evaluation of 22 state-of-the-art models reveals that even the strongest models, despite their general multimodal and multilingual capabilities, struggle dramatically when faced with complex cross-lingual and multimodal tasks in financial domain. MultiFinBen is publicly released to foster transparent, reproducible, and inclusive progress in financial studies and applications.

</details>


### [11] [An Interdisciplinary Review of Commonsense Reasoning and Intent Detection](https://arxiv.org/abs/2506.14040)
*Md Nazmus Sakib*

Main category: cs.CL

TL;DR: 本文综述分析了2020-2025年间28篇论文，探讨自然语言理解中常识推理与意图检测的最新进展，提出跨领域融合NLP与HCI的适应性多语言模型趋势，并指出基准设计等现存不足。


<details>
  <summary>Details</summary>
Motivation: 针对自然语言理解中常识推理与意图检测两大核心挑战，整合NLP与HCI领域的研究成果，揭示跨学科方法对提升模型适应性与语境感知能力的重要性。

Method: 系统性整理ACL、EMNLP、CHI三大会议的28篇论文，按方法论与应用场景分类：常识推理聚焦零样本学习、文化适配等方向，意图检测涵盖开集模型、生成式框架等技术。

Result: 发现模型向多语言、上下文感知方向演进，但存在常识基础薄弱、泛化能力不足及评估基准局限性等关键缺口。

Conclusion: 未来需加强常识知识嵌入、跨领域泛化能力，并设计更贴近真实场景的评估体系，以推动自然语言理解技术的实际应用突破。

Abstract: This review explores recent advances in commonsense reasoning and intent detection, two key challenges in natural language understanding. We analyze 28 papers from ACL, EMNLP, and CHI (2020-2025), organizing them by methodology and application. Commonsense reasoning is reviewed across zero-shot learning, cultural adaptation, structured evaluation, and interactive contexts. Intent detection is examined through open-set models, generative formulations, clustering, and human-centered systems. By bridging insights from NLP and HCI, we highlight emerging trends toward more adaptive, multilingual, and context-aware models, and identify key gaps in grounding, generalization, and benchmark design.

</details>


### [12] [Ace-CEFR -- A Dataset for Automated Evaluation of the Linguistic Difficulty of Conversational Texts for LLM Applications](https://arxiv.org/abs/2506.14046)
*David Kogan,Max Schumacher,Sam Nguyen,Masanori Suzuki,Melissa Smith,Chloe Sophia Bellows,Jared Bernstein*

Main category: cs.CL

TL;DR: 研究者开发了Ace-CEFR数据集，用于评估对话文本的语言难度，并证明训练后的模型在准确性和延迟上优于人类专家，同时公开了数据集。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对短文本对话的语言难度评估工具，这对大语言模型（LLMs）的训练和筛选至关重要。

Method: 构建专家标注的Ace-CEFR数据集，并测试包括Transformer和LLMs在内的多种模型以评估文本难度。

Result: 模型在Ace-CEFR上的表现超越人类专家，且延迟适用于实际生产环境。

Conclusion: Ace-CEFR数据集被公开发布，以支持相关研究和应用开发。

Abstract: There is an unmet need to evaluate the language difficulty of short, conversational passages of text, particularly for training and filtering Large Language Models (LLMs). We introduce Ace-CEFR, a dataset of English conversational text passages expert-annotated with their corresponding level of text difficulty. We experiment with several models on Ace-CEFR, including Transformer-based models and LLMs. We show that models trained on Ace-CEFR can measure text difficulty more accurately than human experts and have latency appropriate to production environments. Finally, we release the Ace-CEFR dataset to the public for research and development.

</details>


### [13] [Automatic Extraction of Clausal Embedding Based on Large-Scale English Text Data](https://arxiv.org/abs/2506.14064)
*Iona Carslaw,Sivan Milton,Nicolas Navarre,Ciyang Qing,Wataru Uegaki*

Main category: cs.CL

TL;DR: 提出一种基于成分句法分析和启发式规则的方法，从大规模语料库中自动提取自然英语嵌入式从句，并构建了人工标注数据集GECS进行验证。


<details>
  <summary>Details</summary>
Motivation: 现有研究依赖人工构造例句分析嵌入式从句，缺乏自然语料统计信息，需通过大规模文本挖掘解决此局限性。

Method: 结合成分句法解析与启发式规则，开发自动化工具检测和标注自然文本中的英语嵌入式从句结构。

Result: 构建含人工标注样本的GECS评估集验证工具有效性，并基于Dolma语料库产出大规模自然嵌入式从句数据集。

Conclusion: 该方法有效填补了嵌入式从句研究的数据空白，为语言学分析提供可扩展的自动化解决方案与资源支持。

Abstract: For linguists, embedded clauses have been of special interest because of their intricate distribution of syntactic and semantic features. Yet, current research relies on schematically created language examples to investigate these constructions, missing out on statistical information and naturally-occurring examples that can be gained from large language corpora. Thus, we present a methodological approach for detecting and annotating naturally-occurring examples of English embedded clauses in large-scale text data using constituency parsing and a set of parsing heuristics. Our tool has been evaluated on our dataset Golden Embedded Clause Set (GECS), which includes hand-annotated examples of naturally-occurring English embedded clause sentences. Finally, we present a large-scale dataset of naturally-occurring English embedded clauses which we have extracted from the open-source corpus Dolma using our extraction tool.

</details>


### [14] [Abstract Meaning Representation for Hospital Discharge Summarization](https://arxiv.org/abs/2506.14101)
*Paul Landes,Sitara Rao,Aaron Jeremy Chaise,Barbara Di Eugenio*

Main category: cs.CL

TL;DR: 本文针对大型语言模型（LLMs）在临床自动生成出院摘要中的幻觉问题，提出结合语言图与深度学习的方法，显著提升内容溯源与可信度，并在MIMIC-III和真实医院数据中验证有效性。


<details>
  <summary>Details</summary>
Motivation: LLMs的幻觉问题在临床领域（如自动生成出院摘要）可能导致严重后果，需解决内容来源可信度以减轻医生文档负担。

Method: 融合语言结构图（language-based graphs）与深度学习模型，增强自动摘要的可追溯性与可靠性。

Result: 在MIMIC-III公开数据集和匿名医院临床笔记中展示高可靠性结果，并提供方法、生成示例、源代码及训练模型。

Conclusion: 该方法有效缓解LLMs的临床幻觉风险，为自动生成医疗文档提供了可信解决方案，并通过开源资源促进后续研究。

Abstract: The Achilles heel of Large Language Models (LLMs) is hallucination, which has drastic consequences for the clinical domain. This is particularly important with regards to automatically generating discharge summaries (a lengthy medical document that summarizes a hospital in-patient visit). Automatically generating these summaries would free physicians to care for patients and reduce documentation burden. The goal of this work is to discover new methods that combine language-based graphs and deep learning models to address provenance of content and trustworthiness in automatic summarization. Our method shows impressive reliability results on the publicly available Medical Information Mart for Intensive III (MIMIC-III) corpus and clinical notes written by physicians at Anonymous Hospital. rovide our method, generated discharge ary output examples, source code and trained models.

</details>


### [15] [Essential-Web v1.0: 24T tokens of organized web data](https://arxiv.org/abs/2506.14111)
*Essential AI,:,Andrew Hojel,Michael Pust,Tim Romanski,Yash Vanjani,Ritvik Kapila,Mohit Parmar,Adarsh Chaluvaraju,Alok Tripathy,Anil Thomas,Ashish Tanwer,Darsh J Shah,Ishaan Shah,Karl Stratos,Khoi Nguyen,Kurt Smith,Michael Callahan,Peter Rushton,Philip Monk,Platon Mazarakis,Saad Jamal,Saurabh Srivastava,Somanshu Singla,Ashish Vaswani*

Main category: cs.CL

TL;DR: 研究团队提出Essential-Web v1.0——一个24万亿token的标注数据集，通过轻量级标注工具和SQL筛选机制，显著降低了构建高质量预训练数据集的成本。


<details>
  <summary>Details</summary>
Motivation: 现有大规模预训练数据集存在组织混乱、标注成本高的问题，导致数据管道建设成本高昂且难以普及。

Method: 1. 构建24万亿token数据集并设计12维分类体系（主题/格式/复杂度/质量）
2. 使用微调模型EAI-Distill-0.5b进行自动化标注（与32B模型标注一致性差距<3%）
3. 通过SQL式过滤机制灵活提取子数据集

Result: 在数学（-8.0% SOTA）、编程（+14.3%）、STEM（+24.5%）和医疗（+8.6%）领域取得竞争力表现，验证了筛选机制的有效性。

Conclusion: 通过系统化标注体系与轻量化标注工具的结合，证明了用低成本方法构建高质量预训练数据集的可行性，数据集已在HuggingFace开源。

Abstract: Data plays the most prominent role in how language models acquire skills and knowledge. The lack of massive, well-organized pre-training datasets results in costly and inaccessible data pipelines. We present Essential-Web v1.0, a 24-trillion-token dataset in which every document is annotated with a twelve-category taxonomy covering topic, format, content complexity, and quality. Taxonomy labels are produced by EAI-Distill-0.5b, a fine-tuned 0.5b-parameter model that achieves an annotator agreement within 3% of Qwen2.5-32B-Instruct. With nothing more than SQL-style filters, we obtain competitive web-curated datasets in math (-8.0% relative to SOTA), web code (+14.3%), STEM (+24.5%) and medical (+8.6%). Essential-Web v1.0 is available on HuggingFace: https://huggingface.co/datasets/EssentialAI/essential-web-v1.0

</details>


### [16] [Sampling from Your Language Model One Byte at a Time](https://arxiv.org/abs/2506.14123)
*Jonathan Hayase,Alisa Liu,Noah A. Smith,Sewoong Oh*

Main category: cs.CL

TL;DR: 提出一种推理时方法，将使用BPE分词器的自回归语言模型转换为字符/字节级模型，解决分词边界问题并统一不同模型的分词器，提升模型集成与代理调优效果。


<details>
  <summary>Details</summary>
Motivation: 现有分词方法（如提示边界问题PBP）导致生成失真，且不同分词器阻碍模型组合与互操作性。例如空格处理、中文/代码生成中的分词不对齐，以及无法直接集成不同分词器的模型。

Method: 在推理阶段将BPE分词的自回归语言模型转换为字符/字节级模型，保持文本生成分布不变，统一不同模型的分词器词汇表，支持跨模型集成与代理调优迁移。

Result: 实验表明，集成模型与代理调优模型在下游任务评估中优于其基础模型，验证了方法的有效性。

Conclusion: 该方法高效解决了分词边界问题，提升了不同分词器模型的兼容性，为模型集成与迁移学习提供了新途径。

Abstract: Tokenization is used almost universally by modern language models, enabling efficient text representation using multi-byte or multi-character tokens. However, prior work has shown that tokenization can introduce distortion into the model's generations. For example, users are often advised not to end their prompts with a space because it prevents the model from including the space as part of the next token. This Prompt Boundary Problem (PBP) also arises in languages such as Chinese and in code generation, where tokens often do not line up with syntactic boundaries. Additionally mismatching tokenizers often hinder model composition and interoperability. For example, it is not possible to directly ensemble models with different tokenizers due to their mismatching vocabularies. To address these issues, we present an inference-time method to convert any autoregressive LM with a BPE tokenizer into a character-level or byte-level LM, without changing its generative distribution at the text level. Our method efficient solves the PBP and is also able to unify the vocabularies of language models with different tokenizers, allowing one to ensemble LMs with different tokenizers at inference time as well as transfer the post-training from one model to another using proxy-tuning. We demonstrate in experiments that the ensemble and proxy-tuned models outperform their constituents on downstream evals.

</details>


### [17] [DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization](https://arxiv.org/abs/2506.14157)
*Chengyu Huang,Tanya Goyal*

Main category: cs.CL

TL;DR: 本文提出DCRM指标量化偏好优化中响应对的差异质量，通过最佳配对方法提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法中，偏好与非偏好响应的差异可能与期望学习目标不匹配，影响模型学习效果。

Method: 结合距离与奖励边际构建DCRM指标，设计best-of-N²方法筛选高DCRM响应对构建训练集。

Result: 使用该方法构建的数据集在AlpacaEval、MT-Bench等基准测试中显著提升模型表现。

Conclusion: DCRM能有效衡量响应对质量，其引导下的数据筛选方法可系统性提升偏好优化效果。

Abstract: Recent research has attempted to associate preference optimization (PO) performance with the underlying preference datasets. In this work, our observation is that the differences between the preferred response $y^+$ and dispreferred response $y^-$ influence what LLMs can learn, which may not match the desirable differences to learn. Therefore, we use distance and reward margin to quantify these differences, and combine them to get Distance Calibrated Reward Margin (DCRM), a metric that measures the quality of a response pair for PO. Intuitively, DCRM encourages minimal noisy differences and maximal desired differences. With this, we study 3 types of commonly used preference datasets, classified along two axes: the source of the responses and the preference labeling function. We establish a general correlation between higher DCRM of the training set and better learning outcome. Inspired by this, we propose a best-of-$N^2$ pairing method that selects response pairs with the highest DCRM. Empirically, in various settings, our method produces training datasets that can further improve models' performance on AlpacaEval, MT-Bench, and Arena-Hard over the existing training sets.

</details>


### [18] [S$^4$C: Speculative Sampling with Syntactic and Semantic Coherence for Efficient Inference of Large Language Models](https://arxiv.org/abs/2506.14158)
*Tao He,Guang Huang,Yu Yang,Tianshi Xu,Sicheng Zhao,Guiguang Ding,Pengyang Wang,Feng Tian*

Main category: cs.CL

TL;DR: 本文提出了一种结合语法和语义连贯性的推测采样框架S$^4$C，通过多头草稿生成和持续验证树优化，显著提升大型语言模型的推理效率，减少计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有推测采样方法在加速大型语言模型推理时，因忽略文本生成的连贯性而导致效率受限，无法充分利用并行性。

Method: S$^4$C框架引入多头草稿生成（加速候选token生成）和持续验证树（高效验证与特征复用），增强语法和语义连贯性。

Result: 在Spec-bench基准测试中，S$^4$C实现2.26x-2.60x加速比，生成更多有效token且资源消耗更低，优于现有方法。

Conclusion: S$^4$C通过结合连贯性约束，显著提升推测采样的效率与并行性，为实时应用提供更优解决方案。

Abstract: Large language models (LLMs) exhibit remarkable reasoning capabilities across diverse downstream tasks. However, their autoregressive nature leads to substantial inference latency, posing challenges for real-time applications. Speculative sampling mitigates this issue by introducing a drafting phase followed by a parallel validation phase, enabling faster token generation and verification. Existing approaches, however, overlook the inherent coherence in text generation, limiting their efficiency. To address this gap, we propose a Speculative Sampling with Syntactic and Semantic Coherence (S$^4$C) framework, which extends speculative sampling by leveraging multi-head drafting for rapid token generation and a continuous verification tree for efficient candidate validation and feature reuse. Experimental results demonstrate that S$^4$C surpasses baseline methods across mainstream tasks, offering enhanced efficiency, parallelism, and the ability to generate more valid tokens with fewer computational resources. On Spec-bench benchmarks, S$^4$C achieves an acceleration ratio of 2.26x-2.60x, outperforming state-of-the-art methods.

</details>


### [19] [MIST: Towards Multi-dimensional Implicit Bias and Stereotype Evaluation of LLMs via Theory of Mind](https://arxiv.org/abs/2506.14161)
*Yanlin Li,Hao Liu,Huimin Liu,Yinwei Wei,Yupeng Hu*

Main category: cs.CL

TL;DR: 本文提出基于刻板印象内容模型（SCM）的评估框架，通过间接测试任务揭示大语言模型在心理理论中的多维隐性偏见，发现普遍社会性偏见及非对称刻板印象强化。


<details>
  <summary>Details</summary>
Motivation: 传统直接评估方法易受社会期望效应干扰，难以捕捉隐性偏见的多维性，需开发更鲁棒的评估框架以解析其结构特征。

Method: 结合SCM理论，设计词联想偏见测试（WABT）和情感归因测试（AAT），通过间接任务探测模型在能力、社交性、道德维度的潜在刻板印象。

Result: 在8个前沿大语言模型上验证，发现普遍社会性偏见、多维偏差分化及非对称刻板印象放大等复杂偏见结构。

Conclusion: 该框架为识别隐性偏见提供了更全面的方法，揭示其结构性特征，推动大语言模型心理理论能力的可靠性评估。

Abstract: Theory of Mind (ToM) in Large Language Models (LLMs) refers to their capacity for reasoning about mental states, yet failures in this capacity often manifest as systematic implicit bias. Evaluating this bias is challenging, as conventional direct-query methods are susceptible to social desirability effects and fail to capture its subtle, multi-dimensional nature. To this end, we propose an evaluation framework that leverages the Stereotype Content Model (SCM) to reconceptualize bias as a multi-dimensional failure in ToM across Competence, Sociability, and Morality. The framework introduces two indirect tasks: the Word Association Bias Test (WABT) to assess implicit lexical associations and the Affective Attribution Test (AAT) to measure covert affective leanings, both designed to probe latent stereotypes without triggering model avoidance. Extensive experiments on 8 State-of-the-Art LLMs demonstrate our framework's capacity to reveal complex bias structures, including pervasive sociability bias, multi-dimensional divergence, and asymmetric stereotype amplification, thereby providing a more robust methodology for identifying the structural nature of implicit bias.

</details>


### [20] [GRAM: A Generative Foundation Reward Model for Reward Generalization](https://arxiv.org/abs/2506.14175)
*Chenglong Wang,Yang Gan,Yifu Huo,Yongyu Mu,Qiaozhi He,Murun Yang,Bei Li,Tong Xiao,Chunliang Zhang,Tongran Liu,Jingbo Zhu*

Main category: cs.CL

TL;DR: 本文提出一种结合无监督与监督学习的生成式奖励模型，通过标签平滑优化正则化排序损失，统一生成与判别模型目标，构建通用基础奖励模型，在多项任务中显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型依赖标注人类偏好数据且仅作为判别模型训练，存在泛化能力受限的问题。本文旨在利用无标注数据增强奖励模型训练，提升其通用性和适应性。

Method: 1. 通过大规模无监督学习预训练生成式奖励模型；2. 结合监督学习微调；3. 引入标签平滑技术，将目标转化为正则化成对排序损失，统一生成与判别模型训练目标。

Result: 实验表明，该模型在响应排序、基于人类反馈的强化学习（RLHF）及任务微调等场景中均优于基线模型，泛化能力显著提升。

Conclusion: 通过融合生成与判别模型目标并利用混合数据训练，构建了通用基础奖励模型，减少任务特定微调需求，为LLM对齐提供了高效解决方案。

Abstract: In aligning large language models (LLMs), reward models have played an important role, but are standardly trained as discriminative models and rely only on labeled human preference data. In this paper, we explore methods that train reward models using both unlabeled and labeled data. Building on the generative models in LLMs, we develop a generative reward model that is first trained via large-scale unsupervised learning and then fine-tuned via supervised learning. We also show that by using label smoothing, we are in fact optimizing a regularized pairwise ranking loss. This result, in turn, provides a new view of training reward models, which links generative models and discriminative models under the same class of training objectives. The outcome of these techniques is a foundation reward model, which can be applied to a wide range of tasks with little or no further fine-tuning effort. Extensive experiments show that this model generalizes well across several tasks, including response ranking, reinforcement learning from human feedback, and task adaptation with fine-tuning, achieving significant performance improvements over several strong baseline models.

</details>


### [21] [Can we train ASR systems on Code-switch without real code-switch data? Case study for Singapore's languages](https://arxiv.org/abs/2506.14177)
*Tuan Nguyen,Huy-Dat Tran*

Main category: cs.CL

TL;DR: 本研究提出通过短语级混合生成合成代码切换（CS）数据，结合单语数据微调预训练ASR模型，以解决多语言环境下CS-ASR数据稀缺问题，并在东南亚低资源语言对上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 多语言环境中的代码切换现象导致ASR训练数据稀缺且标注成本高，传统方法难以处理混合语言模式，需探索低成本高效的CS-ASR开发方案。

Method: 提出短语级混合方法生成合成CS数据，模拟自然语言模式；利用单语数据与合成数据微调Whisper、MMS等预训练大模型，构建涵盖马来-英、汉-马来、泰米尔-英的CS-ASR基准。

Result: 实验表明，合成数据增强策略显著提升单语及CS测试集性能，马来-英提升最大，其次为泰米尔-英和汉-马来组合，验证了方法的跨语言适用性。

Conclusion: 合成数据生成与预训练模型微调结合，为低资源CS-ASR开发提供了经济高效的解决方案，可推动多语言语音技术的研究与产业应用。

Abstract: Code-switching (CS), common in multilingual settings, presents challenges for ASR due to scarce and costly transcribed data caused by linguistic complexity. This study investigates building CS-ASR using synthetic CS data. We propose a phrase-level mixing method to generate synthetic CS data that mimics natural patterns. Utilizing monolingual augmented with synthetic phrase-mixed CS data to fine-tune large pretrained ASR models (Whisper, MMS, SeamlessM4T). This paper focuses on three under-resourced Southeast Asian language pairs: Malay-English (BM-EN), Mandarin-Malay (ZH-BM), and Tamil-English (TA-EN), establishing a new comprehensive benchmark for CS-ASR to evaluate the performance of leading ASR models. Experimental results show that the proposed training strategy enhances ASR performance on monolingual and CS tests, with BM-EN showing highest gains, then TA-EN and ZH-BM. This finding offers a cost-effective approach for CS-ASR development, benefiting research and industry.

</details>


### [22] [AsyncSwitch: Asynchronous Text-Speech Adaptation for Code-Switched ASR](https://arxiv.org/abs/2506.14190)
*Tuan Nguyen,Huy-Dat Tran*

Main category: cs.CL

TL;DR: 本文介绍AsyncSwitch框架，通过三阶段训练利用网络文本数据提升代码切换ASR模型性能，在马来语-英语任务中WER相对降低9.02%，同时增强单语识别能力。


<details>
  <summary>Details</summary>
Motivation: 开发代码切换ASR系统面临语言歧义与数据稀缺问题，传统合成音频方法计算成本高且难以扩展。需探索更高效的训练策略。

Method: 三阶段异步适配框架：1) 用代码切换文本预训练解码器自注意力层；2) 有限语音-文本数据对齐编解码器；3) 全模型微调。基于Whisper模型实现。

Result: 马来语-英语代码切换任务WER相对降低9.02%，在Singlish、马来语及其他英语变体的单语识别性能也得到提升。

Conclusion: AsyncSwitch通过分阶段预暴露多领域代码切换模式，有效提升ASR模型在低资源混合语言场景下的鲁棒性与泛化能力。

Abstract: Developing code-switched ASR systems is challenging due to language ambiguity and limited exposure to multilingual, code-switched data, while collecting such speech is costly. Prior work generates synthetic audio from text, but these methods are computationally intensive and hard to scale. We introduce AsyncSwitch, a novel asynchronous adaptation framework that leverages large-scale, text-rich web data to pre-expose ASR models to diverse code-switched domains before fine-tuning on paired speech-text corpora. Our three-stage process (1) trains decoder self-attention and feedforward layers on code-switched text, (2) aligns decoder and encoder via cross-attention using limited speech-text data, and (3) fully fine-tunes the entire model. Experiments with Whisper on Malay-English code-switching demonstrate a 9.02% relative WER reduction, while improving monolingual performance in Singlish, Malay, and other English variants.

</details>


### [23] [MAS-LitEval : Multi-Agent System for Literary Translation Quality Assessment](https://arxiv.org/abs/2506.14199)
*Junghwan Kim,Kieun Park,Sohee Park,Hyunggug Kim,Bongwon Suh*

Main category: cs.CL

TL;DR: 提出MAS-LitEval多智能体系统，利用大语言模型评估文学翻译质量，在术语、叙事和风格维度超越传统指标，测试显示最高得分达0.890。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标（如BLEU、METEOR）依赖词汇重叠，无法捕捉文学翻译中的文化细微差异、叙事连贯性和文体忠实性，亟需更全面的评估框架。

Method: 基于大语言模型构建多智能体系统（MAS-LitEval），从术语准确性、叙事一致性和风格适配性三方面评估翻译质量，并以《小王子》和《康州美国佬在亚瑟王朝》的LLM译本进行验证。

Result: MAS-LitEval在文学细节捕捉上显著优于传统指标，最优模型得分达0.890，证明其能有效量化翻译的叙事与风格保真度。

Conclusion: MAS-LitEval为文学翻译质量评估提供了可扩展的细粒度框架，兼顾文化语境与文体特征，助力译者和研究者优化翻译实践。

Abstract: Literary translation requires preserving cultural nuances and stylistic elements, which traditional metrics like BLEU and METEOR fail to assess due to their focus on lexical overlap. This oversight neglects the narrative consistency and stylistic fidelity that are crucial for literary works. To address this, we propose MAS-LitEval, a multi-agent system using Large Language Models (LLMs) to evaluate translations based on terminology, narrative, and style. We tested MAS-LitEval on translations of The Little Prince and A Connecticut Yankee in King Arthur's Court, generated by various LLMs, and compared it to traditional metrics. \textbf{MAS-LitEval} outperformed these metrics, with top models scoring up to 0.890 in capturing literary nuances. This work introduces a scalable, nuanced framework for Translation Quality Assessment (TQA), offering a practical tool for translators and researchers.

</details>


### [24] [ELI-Why: Evaluating the Pedagogical Utility of Language Model Explanations](https://arxiv.org/abs/2506.14200)
*Brihi Joshi,Keyu He,Sahana Ramnath,Sadra Sabouri,Kaitlyn Zhou,Souti Chattopadhyay,Swabha Swayamdipta,Xiang Ren*

Main category: cs.CL

TL;DR: 研究通过ELI-Why基准测试发现，当前语言模型（如GPT-4）生成的教学解释在匹配不同教育阶段需求方面显著落后于人类编写的解释，且自动评估指标无法有效区分模型生成内容的难度级别。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在教育场景中广泛应用，但其针对不同知识背景和学习者信息需求的个性化解释能力尚未充分研究。

Method: 构建包含13.4K个'Why'问题的ELI-Why基准，并开展两项人类研究：教育者评估解释与教育阶段匹配度，学习者评估解释与自身需求契合度。

Result: GPT-4生成解释仅50%匹配目标教育阶段（人类解释为79%），且用户认为其解释适用性平均低20%；自动指标显示不同模型生成解释的难度级别无显著差异。

Conclusion: 当前语言模型生成的教学解释在适切性和分层能力上存在明显局限，需针对性改进以提升教学有效性。

Abstract: Language models today are widely used in education, yet their ability to tailor responses for learners with varied informational needs and knowledge backgrounds remains under-explored. To this end, we introduce ELI-Why, a benchmark of 13.4K "Why" questions to evaluate the pedagogical capabilities of language models. We then conduct two extensive human studies to assess the utility of language model-generated explanatory answers (explanations) on our benchmark, tailored to three distinct educational grades: elementary, high-school and graduate school. In our first study, human raters assume the role of an "educator" to assess model explanations' fit to different educational grades. We find that GPT-4-generated explanations match their intended educational background only 50% of the time, compared to 79% for lay human-curated explanations. In our second study, human raters assume the role of a learner to assess if an explanation fits their own informational needs. Across all educational backgrounds, users deemed GPT-4-generated explanations 20% less suited on average to their informational needs, when compared to explanations curated by lay people. Additionally, automated evaluation metrics reveal that explanations generated across different language model families for different informational needs remain indistinguishable in their grade-level, limiting their pedagogical effectiveness.

</details>


### [25] [Intended Target Identification for Anomia Patients with Gradient-based Selective Augmentation](https://arxiv.org/abs/2506.14203)
*Jongho Kim,Romain Storaï,Seung-won Hwang*

Main category: cs.CL

TL;DR: 本研究提出基于梯度的选择性增强方法，提升语言模型在命名障碍患者语义性错语场景下的术语识别能力，通过梯度值控制数据质量、梯度方差扩展新术语，并在TOT数据集和失语症银行真实数据中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 命名障碍患者因语义性错语产生无关术语干扰，同时存在术语缺失问题，导致现有模型难以从迂回描述中准确识别目标物品。需解决语义干扰和术语扩展的双重挑战。

Method: 1. 通过梯度值筛选高质量数据增强，抵抗语义性错语干扰 2. 利用梯度方差指导模型纳入未见但相关的新术语，解决术语缺失问题。使用TOT数据集作为中间任务验证，最终应用于AphasiaBank真实患者数据。

Result: 在TOT数据集和AphasiaBank真实数据中均显著超越基线模型，证明方法能有效处理语义错误并扩展未见过术语。

Conclusion: 梯度驱动的选择性增强策略成功解决了命名障碍辅助中的核心挑战，为语言模型在临床场景的应用提供了新思路。

Abstract: In this study, we investigate the potential of language models (LMs) in aiding patients experiencing anomia, a difficulty identifying the names of items. Identifying the intended target item from patient's circumlocution involves the two challenges of term failure and error: (1) The terms relevant to identifying the item remain unseen. (2) What makes the challenge unique is inherent perturbed terms by semantic paraphasia, which are not exactly related to the target item, hindering the identification process. To address each, we propose robustifying the model from semantically paraphasic errors and enhancing the model with unseen terms with gradient-based selective augmentation. Specifically, the gradient value controls augmented data quality amid semantic errors, while the gradient variance guides the inclusion of unseen but relevant terms. Due to limited domain-specific datasets, we evaluate the model on the Tip-of-the-Tongue dataset as an intermediary task and then apply our findings to real patient data from AphasiaBank. Our results demonstrate strong performance against baselines, aiding anomia patients by addressing the outlined challenges.

</details>


### [26] [AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents](https://arxiv.org/abs/2506.14205)
*Jingxu Xie,Dylan Xu,Xuandong Zhao,Dawn Song*

Main category: cs.CL

TL;DR: AgentSynth提出了一种高效、低成本的自动化流程，通过信息不对称生成可组合的长时程任务，显著提升通用计算机使用代理的任务多样性和难度，同时大幅降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类标注的任务生成方法成本高昂且难以扩展，而自动生成的任务往往缺乏多样性和真实场景复杂性。需要一种能精确控制任务难度、低成本生成高质量轨迹数据的新方法。

Method: 1. 基于LLM的任务提议器生成符合人物设定的子任务
2. 执行代理完成子任务并记录轨迹
3. 迭代组合子任务形成长时程任务
4. 独立代理总结合成任务的难度层级
5. 通过调节子任务数量精确控制整体复杂度

Result: 1. 生成6,000+多样化任务
2. SOTA LLM代理在难度6级时成功率从18%骤降至4%
3. 单轨迹成本仅0.6美元(比人工标注低2个数量级)
4. 验证了benchmark的区分能力

Conclusion: AgentSynth通过算法化组合简单子任务构建复杂任务生态，证明了自动生成高难度基准的可行性，为训练通用智能体提供了高效、可扩展的数据合成方案。

Abstract: We introduce AgentSynth, a scalable and cost-efficient pipeline for automatically synthesizing high-quality tasks and trajectory datasets for generalist computer-use agents. Leveraging information asymmetry, AgentSynth constructs subtasks that are simple during generation but significantly more challenging when composed into long-horizon tasks, enabling the creation of over 6,000 diverse and realistic tasks. Our pipeline begins with an LLM-based task proposer guided by a persona, followed by an execution agent that completes the task and logs the trajectory. This process is repeated iteratively to form a sequence of subtasks, which are then summarized by a separate agent into a composite task of controllable difficulty. A key strength of AgentSynth is its ability to precisely modulate task complexity by varying the number of subtasks. Empirical evaluations show that state-of-the-art LLM agents suffer a steep performance drop, from 18% success at difficulty level 1 to just 4% at level 6, highlighting the benchmark's difficulty and discriminative power. Moreover, our pipeline achieves a low average cost of \$0.60 per trajectory, orders of magnitude cheaper than human annotations. Our code and data are publicly available at https://github.com/sunblaze-ucb/AgentSynth

</details>


### [27] [CausalDiffTab: Mixed-Type Causal-Aware Diffusion for Tabular Data Generation](https://arxiv.org/abs/2506.14206)
*Jia-Chen Zhang,Zheng Zhou,Yu-Jie Xiong,Chun-Ming Xia,Fei Dai*

Main category: cs.CL

TL;DR: 本文提出CausalDiffTab，一种基于扩散模型的混合类型表格数据生成方法，通过自适应因果正则化解决异构数据、复杂变量关系等挑战，实验表明其在七大数据集上全面优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 生成高质量混合类型表格数据面临异构数据类型、复杂变量间关系及列分布差异等挑战，现有方法难以有效捕捉这些特性。

Method: 结合扩散模型框架，设计支持数值/分类特征的混合数据处理机制，并提出基于层次先验融合的混合自适应因果正则化方法，动态调节因果约束权重。

Result: 在七个数据集上的实验证明，CausalDiffTab在全部评估指标上超越现有基线模型，且未损害生成质量。代码已开源。

Conclusion: CausalDiffTab通过因果正则化与扩散模型的结合，有效解决了表格数据生成的固有难题，在保持生成能力的同时显著提升模型性能。

Abstract: Training data has been proven to be one of the most critical components in training generative AI. However, obtaining high-quality data remains challenging, with data privacy issues presenting a significant hurdle. To address the need for high-quality data. Synthesize data has emerged as a mainstream solution, demonstrating impressive performance in areas such as images, audio, and video. Generating mixed-type data, especially high-quality tabular data, still faces significant challenges. These primarily include its inherent heterogeneous data types, complex inter-variable relationships, and intricate column-wise distributions. In this paper, we introduce CausalDiffTab, a diffusion model-based generative model specifically designed to handle mixed tabular data containing both numerical and categorical features, while being more flexible in capturing complex interactions among variables. We further propose a hybrid adaptive causal regularization method based on the principle of Hierarchical Prior Fusion. This approach adaptively controls the weight of causal regularization, enhancing the model's performance without compromising its generative capabilities. Comprehensive experiments conducted on seven datasets demonstrate that CausalDiffTab outperforms baseline methods across all metrics. Our code is publicly available at: https://github.com/Godz-z/CausalDiffTab.

</details>


### [28] [Explainable Detection of Implicit Influential Patterns in Conversations via Data Augmentation](https://arxiv.org/abs/2506.14211)
*Sina Abdidizaji,Md Kowsher,Niloofar Yousefi,Ivan Garibay*

Main category: cs.CL

TL;DR: 本文提出了一种改进方法，利用先进语言模型的推理能力增强数据集，以识别对话中隐性的影响策略及其位置，检测准确率提升6%，多标签分类任务性能显著提高。


<details>
  <summary>Details</summary>
Motivation: 现有模型擅长检测显性语言模式（如单条社交媒体帖子），但恶意行为者转向更隐蔽的对话内隐性策略，需开发新方法以识别此类复杂影响手段。

Method: 通过增强现有数据集（利用先进语言模型的推理能力），设计新框架检测隐性影响模式，并定位其在对话中的具体位置。

Result: 隐性影响模式检测准确率提升6%；影响技术多标签分类任务性能提高33%，受害者脆弱性分类任务提升43%。

Conclusion: 所提框架有效提升隐性影响策略的检测与定位能力，为对抗数字化时代隐蔽信息操纵提供了更优解决方案。

Abstract: In the era of digitalization, as individuals increasingly rely on digital platforms for communication and news consumption, various actors employ linguistic strategies to influence public perception. While models have become proficient at detecting explicit patterns, which typically appear in texts as single remarks referred to as utterances, such as social media posts, malicious actors have shifted toward utilizing implicit influential verbal patterns embedded within conversations. These verbal patterns aim to mentally penetrate the victim's mind in order to influence them, enabling the actor to obtain the desired information through implicit means. This paper presents an improved approach for detecting such implicit influential patterns. Furthermore, the proposed model is capable of identifying the specific locations of these influential elements within a conversation. To achieve this, the existing dataset was augmented using the reasoning capabilities of state-of-the-art language models. Our designed framework resulted in a 6% improvement in the detection of implicit influential patterns in conversations. Moreover, this approach improved the multi-label classification tasks related to both the techniques used for influence and the vulnerability of victims by 33% and 43%, respectively.

</details>


### [29] [Chaining Event Spans for Temporal Relation Grounding](https://arxiv.org/abs/2506.14213)
*Jongho Kim,Dohyeon Lee,Minsoo Kim,Seung-won Hwang*

Main category: cs.CL

TL;DR: 本文提出时间线推理网络（TRN），通过预测事件时间跨度解决现有方法因答案重叠导致的不可靠问题，在多个任务中表现优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖答案重叠作为代理标签区分相似问题，但答案可能偶然相同导致误判。需更可靠的时间推理机制。

Method: 提出TRN模型：1) 利用语义和句法信息初步回答问题；2) 通过链式问题预测事件时间线，并基于时间线修正答案。

Result: 在TORQUE、TB-dense、TRC和TRE任务中，TRN优于现有方法，有效解决了虚假答案重叠问题。

Conclusion: TRN通过两步时间线推理机制，显式建模事件时序关系，显著提升了时间关系理解的准确性和鲁棒性。

Abstract: Accurately understanding temporal relations between events is a critical building block of diverse tasks, such as temporal reading comprehension (TRC) and relation extraction (TRE). For example in TRC, we need to understand the temporal semantic differences between the following two questions that are lexically near-identical: "What finished right before the decision?" or "What finished right after the decision?". To discern the two questions, existing solutions have relied on answer overlaps as a proxy label to contrast similar and dissimilar questions. However, we claim that answer overlap can lead to unreliable results, due to spurious overlaps of two dissimilar questions with coincidentally identical answers. To address the issue, we propose a novel approach that elicits proper reasoning behaviors through a module for predicting time spans of events. We introduce the Timeline Reasoning Network (TRN) operating in a two-step inductive reasoning process: In the first step model initially answers each question with semantic and syntactic information. The next step chains multiple questions on the same event to predict a timeline, which is then used to ground the answers. Results on the TORQUE and TB-dense, TRC and TRE tasks respectively, demonstrate that TRN outperforms previous methods by effectively resolving the spurious overlaps using the predicted timeline.

</details>


### [30] [Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just Like an Olympiad Team](https://arxiv.org/abs/2506.14234)
*Md Tanzib Hosain,Salman Rahman,Md Kishor Morol,Md Rizwan Parvez*

Main category: cs.CL

TL;DR: Xolver提出了一种无需训练的多智能体推理框架，通过整合外部检索、工具使用、协作经验等多样化经验模态，使大语言模型具备持续演进的记忆能力，在多个基准测试中超越现有先进模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型独立处理问题，缺乏人类专家通过积累经验（如导师指导、历史问题直觉、工具知识、协作策略迭代）进行推理的能力。研究旨在通过经验整合实现专家级推理。

Method: 构建基于黑盒LLM的持续记忆框架，整合外部/自检索、工具调用、多智能体协作、自主评估和迭代优化等经验模态，在推理时复用策略/代码/模式而非从零生成。

Result: 轻量级模型(QWQ-32B)超越Qwen3-235B等大模型，使用o3-mini-high时在GSM8K(98.1%)等5个基准创最佳记录，验证整体经验学习的有效性。

Conclusion: Xolver证明经验整合是实现通用专家级推理代理的关键路径，其训练无关特性使轻量模型也能通过经验复用达到顶尖性能，推动语言智能体向经验感知方向演进。

Abstract: Despite impressive progress on complex reasoning, current large language models (LLMs) typically operate in isolation - treating each problem as an independent attempt, without accumulating or integrating experiential knowledge. In contrast, expert problem solvers - such as Olympiad or programming contest teams - leverage a rich tapestry of experiences: absorbing mentorship from coaches, developing intuition from past problems, leveraging knowledge of tool usage and library functionality, adapting strategies based on the expertise and experiences of peers, continuously refining their reasoning through trial and error, and learning from other related problems even during competition. We introduce Xolver, a training-free multi-agent reasoning framework that equips a black-box LLM with a persistent, evolving memory of holistic experience. Xolver integrates diverse experience modalities, including external and self-retrieval, tool use, collaborative interactions, agent-driven evaluation, and iterative refinement. By learning from relevant strategies, code fragments, and abstract reasoning patterns at inference time, Xolver avoids generating solutions from scratch - marking a transition from isolated inference toward experience-aware language agents. Built on both open-weight and proprietary models, Xolver consistently outperforms specialized reasoning agents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses advanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high. With o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24 (94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) - highlighting holistic experience learning as a key step toward generalist agents capable of expert-level reasoning. Code and data are available at https://kagnlp.github.io/xolver.github.io/.

</details>


### [31] [A Multi-Expert Structural-Semantic Hybrid Framework for Unveiling Historical Patterns in Temporal Knowledge Graphs](https://arxiv.org/abs/2506.14235)
*Yimin Deng,Yuxia Wu,Yejing Wang,Guoshuai Zhao,Li Zhu,Qidong Liu,Derong Xu,Zichuan Fu,Xian Wu,Yefeng Zheng,Xiangyu Zhao,Xueming Qian*

Main category: cs.CL

TL;DR: 提出MESH框架，通过多专家模块整合结构与语义信息，提升时序知识图谱推理能力，实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注图结构或语义单一视角，且未区分历史与非历史事件的差异，导致泛化能力受限。

Method: MESH框架采用三种专家模块，结合结构学习与语义推理，针对不同事件类型进行混合推理指导。

Result: 在三个数据集上的实验表明，该方法显著优于基线模型。

Conclusion: MESH通过双重视角融合及事件差异建模，有效提升了不同时序场景下的推理泛化能力。

Abstract: Temporal knowledge graph reasoning aims to predict future events with knowledge of existing facts and plays a key role in various downstream tasks. Previous methods focused on either graph structure learning or semantic reasoning, failing to integrate dual reasoning perspectives to handle different prediction scenarios. Moreover, they lack the capability to capture the inherent differences between historical and non-historical events, which limits their generalization across different temporal contexts. To this end, we propose a Multi-Expert Structural-Semantic Hybrid (MESH) framework that employs three kinds of expert modules to integrate both structural and semantic information, guiding the reasoning process for different events. Extensive experiments on three datasets demonstrate the effectiveness of our approach.

</details>


### [32] [Re-Initialization Token Learning for Tool-Augmented Large Language Models](https://arxiv.org/abs/2506.14248)
*Chenghao Li,Liu Liu,Baosheng Yu,Jiayan Qiu,Yibing Zhan*

Main category: cs.CL

TL;DR: 本文提出一种新的标记学习方法，通过将工具标记与现有词嵌入空间对齐，提升大语言模型在复杂任务中的工具调用能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法为每个工具分配唯一标记，但未考虑工具与词汇标记的关系，限制了预训练大语言模型在工具调用中的适应性。

Method: 基于工具名称/描述构建先验标记嵌入初始化可学习工具标记，通过正则化实现工具标记与词嵌入空间的对齐。

Result: 在GSM8K-XL等四个数据集上，数值推理、知识问答和具身规划任务表现优于CoT、REACT等基线方法。

Conclusion: 通过词嵌入空间对齐的标记学习方法，有效增强了LLMs跨领域工具调用的准确性和适应性。

Abstract: Large language models have demonstrated exceptional performance, yet struggle with complex tasks such as numerical reasoning, plan generation. Integrating external tools, such as calculators and databases, into large language models (LLMs) is crucial for enhancing problem-solving capabilities. Current methods assign a unique token to each tool, enabling LLMs to call tools through token prediction-similar to word generation. However, this approach fails to account for the relationship between tool and word tokens, limiting adaptability within pre-trained LLMs. To address this issue, we propose a novel token learning method that aligns tool tokens with the existing word embedding space from the perspective of initialization, thereby enhancing model performance. We begin by constructing prior token embeddings for each tool based on the tool's name or description, which are used to initialize and regularize the learnable tool token embeddings. This ensures the learned embeddings are well-aligned with the word token space, improving tool call accuracy. We evaluate the method on tasks such as numerical reasoning, knowledge-based question answering, and embodied plan generation using GSM8K-XL, FuncQA, KAMEL, and VirtualHome datasets. The results demonstrate clear improvements over recent baselines, including CoT, REACT, ICL, and ToolkenGPT, indicating that our approach effectively augments LLMs with tools through relevant tokens across diverse domains.

</details>


### [33] [From What to Respond to When to Respond: Timely Response Generation for Open-domain Dialogue Agents](https://arxiv.org/abs/2506.14285)
*Seongbo Jang,Minjin Jeon,Jaehoon Lee,Seonghyeon Lee,Dongha Lee,Hwanjo Yu*

Main category: cs.CL

TL;DR: 本文提出及时对话响应生成任务及TimelyChat基准，通过时间常识图谱和LLM构建训练数据，训练Timer模型以预测时间间隔并生成及时响应，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有对话生成研究主要关注文本上下文的连贯性，但何时基于时间上下文进行响应的问题尚未充分探索。

Method: 提出新任务及时序对话基准TimelyChat；利用时间常识知识图谱未标注事件数据，通过LLM合成55K事件驱动对话；设计Timer模型主动预测时间间隔并生成时序对齐响应。

Result: Timer在轮次级和对话级评估中均优于基于提示的LLMs及其他微调基线模型。

Conclusion: 通过填补时序对话生成的研究空白，Timer证明了时间条件响应生成的有效性，并公开数据、模型与代码以促进后续研究。

Abstract: While research on dialogue response generation has primarily focused on generating coherent responses conditioning on textual context, the critical question of when to respond grounded on the temporal context remains underexplored. To bridge this gap, we propose a novel task called timely dialogue response generation and introduce the TimelyChat benchmark, which evaluates the capabilities of language models to predict appropriate time intervals and generate time-conditioned responses. Additionally, we construct a large-scale training dataset by leveraging unlabeled event knowledge from a temporal commonsense knowledge graph and employing a large language model (LLM) to synthesize 55K event-driven dialogues. We then train Timer, a dialogue agent designed to proactively predict time intervals and generate timely responses that align with those intervals. Experimental results show that Timer outperforms prompting-based LLMs and other fine-tuned baselines in both turn-level and dialogue-level evaluations. We publicly release our data, model, and code.

</details>


### [34] [Expectation Confirmation Preference Optimization for Multi-Turn Conversational Recommendation Agent](https://arxiv.org/abs/2506.14302)
*Xueyang Feng,Jingsen Zhang,Jiakai Tang,Wei Li,Guohao Cai,Xu Chen,Quanyu Dai,Yue Zhu,Zhenhua Dong*

Main category: cs.CL

TL;DR: 本文提出ECPO方法，通过多轮偏好优化结合期望确认理论，有效提升对话推荐代理的交互能力，同时减少现有方法的采样开销。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的对话推荐代理存在短视回应问题，偏好优化方法虽能对齐用户期望，但成本高且在多轮对话中表现不佳。

Method: 提出ECPO框架，利用期望确认理论显式建模多轮对话中用户满意度演变，定位不满原因并针对性优化；引入LLM用户模拟器AILO生成反馈。

Result: 实验表明ECPO显著增强对话推荐代理的交互能力，在效率和效果上均优于现有多轮偏好优化方法。

Conclusion: ECPO通过动态建模用户期望确认过程，实现了低成本、精准的多轮偏好优化，为对话推荐领域提供了新范式。

Abstract: Recent advancements in Large Language Models (LLMs) have significantly propelled the development of Conversational Recommendation Agents (CRAs). However, these agents often generate short-sighted responses that fail to sustain user guidance and meet expectations. Although preference optimization has proven effective in aligning LLMs with user expectations, it remains costly and performs poorly in multi-turn dialogue. To address this challenge, we introduce a novel multi-turn preference optimization (MTPO) paradigm ECPO, which leverages Expectation Confirmation Theory to explicitly model the evolution of user satisfaction throughout multi-turn dialogues, uncovering the underlying causes of dissatisfaction. These causes can be utilized to support targeted optimization of unsatisfactory responses, thereby achieving turn-level preference optimization. ECPO ingeniously eliminates the significant sampling overhead of existing MTPO methods while ensuring the optimization process drives meaningful improvements. To support ECPO, we introduce an LLM-based user simulator, AILO, to simulate user feedback and perform expectation confirmation during conversational recommendations. Experimental results show that ECPO significantly enhances CRA's interaction capabilities, delivering notable improvements in both efficiency and effectiveness over existing MTPO methods.

</details>


### [35] [Evaluation Should Not Ignore Variation: On the Impact of Reference Set Choice on Summarization Metrics](https://arxiv.org/abs/2506.14335)
*Silvia Casola,Yang Janet Liu,Siyao Peng,Oliver Kraus,Albert Gatt,Barbara Plank*

Main category: cs.CL

TL;DR: 现有摘要评估指标对参考摘要集的选择敏感，尤其是ROUGE等n-gram指标存在显著不稳定性，建议将参考集变化纳入评估以提升LLM评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有摘要评估常忽视语言多样性对参考摘要集的影响，导致基于不同参考集的指标结果不稳定，可能削弱模型比较的可信度。

Method: 通过分析SummEval、GUMSum和DUC2004三个多参考摘要数据集，测试指标敏感性，并收集跨体裁LLM输出的人类评估数据以补充相关性分析。

Result: 主流指标（尤其是ROUGE）在参考集变化时模型排名波动显著，且人类评估与指标间呈现弱/无相关性（非新闻类数据尤为明显）。

Conclusion: 应在摘要评估中引入参考集多样性考量，以增强指标一致性及与人类判断的相关性，这对LLM生成摘要的评估尤为重要。

Abstract: Human language production exhibits remarkable richness and variation, reflecting diverse communication styles and intents. However, this variation is often overlooked in summarization evaluation. While having multiple reference summaries is known to improve correlation with human judgments, the impact of using different reference sets on reference-based metrics has not been systematically investigated. This work examines the sensitivity of widely used reference-based metrics in relation to the choice of reference sets, analyzing three diverse multi-reference summarization datasets: SummEval, GUMSum, and DUC2004. We demonstrate that many popular metrics exhibit significant instability. This instability is particularly concerning for n-gram-based metrics like ROUGE, where model rankings vary depending on the reference sets, undermining the reliability of model comparisons. We also collect human judgments on LLM outputs for genre-diverse data and examine their correlation with metrics to supplement existing findings beyond newswire summaries, finding weak-to-no correlation. Taken together, we recommend incorporating reference set variation into summarization evaluation to enhance consistency alongside correlation with human judgments, especially when evaluating LLMs.

</details>


### [36] [A Vision for Geo-Temporal Deep Research Systems: Towards Comprehensive, Transparent, and Reproducible Geo-Temporal Information Synthesis](https://arxiv.org/abs/2506.14345)
*Bruno Martins,Piotr Szymański,Piotr Gramacki*

Main category: cs.CL

TL;DR: 当前基于大语言模型的深度研究系统缺乏处理地理-时间约束的能力，本文提出整合地理-时间推理的技术路径与评估框架，推动下一代具备时空感知能力的研究系统发展。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究系统在公共卫生、环境科学等领域处理涉及地理/时间约束的复杂问题时存在能力缺失，制约了上下文敏感问题的解答效果。

Method: 主张在检索与信息合成过程中增强地理-时间约束处理能力，构建开放可复现的基础设施，并建立严格的时空推理评估协议。

Result: 系统化提出了整合地理-时间维度的技术挑战清单，构建了包含数据管道、时空编码器、评估基准的完整技术演进路线图。

Conclusion: 通过增强时空推理能力、建设配套基础设施与评估体系，可推动AI研究系统实现更精准的时空敏感信息处理，对科学决策支持产生深远影响。

Abstract: The emergence of Large Language Models (LLMs) has transformed information access, with current LLMs also powering deep research systems that can generate comprehensive report-style answers, through planned iterative search, retrieval, and reasoning. Still, current deep research systems lack the geo-temporal capabilities that are essential for answering context-rich questions involving geographic and/or temporal constraints, frequently occurring in domains like public health, environmental science, or socio-economic analysis. This paper reports our vision towards next generation systems, identifying important technical, infrastructural, and evaluative challenges in integrating geo-temporal reasoning into deep research pipelines. We argue for augmenting retrieval and synthesis processes with the ability to handle geo-temporal constraints, supported by open and reproducible infrastructures and rigorous evaluation protocols. Our vision outlines a path towards more advanced and geo-temporally aware deep research systems, of potential impact to the future of AI-driven information access.

</details>


### [37] [Digital Gatekeepers: Google's Role in Curating Hashtags and Subreddits](https://arxiv.org/abs/2506.14370)
*Amrit Poudel,Yifan Ding,Jurgen Pfeffer,Tim Weninger*

Main category: cs.CL

TL;DR: 研究揭示谷歌通过算法选择性压制与性内容、阴谋论、广告和加密货币相关的子版块及话题标签，同时推广高参与度内容，从而影响用户接触的信息和公共话语。


<details>
  <summary>Details</summary>
Motivation: 探究搜索引擎作为数字守门人如何通过算法筛选机制影响网络及社交媒体内容的可见性，进而塑造用户信息接触范围。

Method: 通过对比谷歌搜索结果与Reddit、Twitter/X平台的非抽样数据，分析搜索引擎对子版块和话题标签的可见性偏差。

Result: 谷歌算法系统性压制涉及色情、阴谋论、广告和加密货币的内容，而优先展示用户参与度较高的社交媒体叙事。

Conclusion: 搜索引擎的守门行为通过内容可见性调控间接影响公共话语，形成基于算法偏见的社交媒体叙事筛选机制。

Abstract: Search engines play a crucial role as digital gatekeepers, shaping the visibility of Web and social media content through algorithmic curation. This study investigates how search engines like Google selectively promotes or suppresses certain hashtags and subreddits, impacting the information users encounter. By comparing search engine results with nonsampled data from Reddit and Twitter/X, we reveal systematic biases in content visibility. Google's algorithms tend to suppress subreddits and hashtags related to sexually explicit material, conspiracy theories, advertisements, and cryptocurrencies, while promoting content associated with higher engagement. These findings suggest that Google's gatekeeping practices influence public discourse by curating the social media narratives available to users.

</details>


### [38] [ELLIS Alicante at CQs-Gen 2025: Winning the critical thinking questions shared task: LLM-based question generation and selection](https://arxiv.org/abs/2506.14371)
*Lucile Favero,Daniel Frases,Juan Antonio Pérez-Ortiz,Tanja Käser,Nuria Oliver*

Main category: cs.CL

TL;DR: 本文提出一种基于小规模开源语言模型的两步框架，通过生成批判性问题挑战辩论中的模糊主张，以促进深度推理。该系统在ACL 2025关联的Argument Mining研讨会共享任务中夺冠。


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型(LLMs)聊天界面可能助长浅层学习的问题，研究探索如何利用LLMs生成批判性问题，以增强对辩论文本的深度推理能力，而非仅用于事实检索。

Method: 采用双模型框架：1) Questioner模型生成候选批判性问题；2) Judge模型筛选最相关问题。两阶段均使用小型开源语言模型实现。

Result: 该系统在第十二届Argument Mining研讨会自动批判问题生成共享任务中获得第一名，验证了方法的有效性。

Conclusion: 基于LLM的两步框架能有效促进对议论文本的批判性思考，展示了语言模型在提升深度认知能力方面的应用潜力。

Abstract: The widespread adoption of chat interfaces based on Large Language Models (LLMs) raises concerns about promoting superficial learning and undermining the development of critical thinking skills. Instead of relying on LLMs purely for retrieving factual information, this work explores their potential to foster deeper reasoning by generating critical questions that challenge unsupported or vague claims in debate interventions. This study is part of a shared task of the 12th Workshop on Argument Mining, co-located with ACL 2025, focused on automatic critical question generation. We propose a two-step framework involving two small-scale open source language models: a Questioner that generates multiple candidate questions and a Judge that selects the most relevant ones. Our system ranked first in the shared task competition, demonstrating the potential of the proposed LLM-based approach to encourage critical engagement with argumentative texts.

</details>


### [39] [Thunder-NUBench: A Benchmark for LLMs' Sentence-Level Negation Understanding](https://arxiv.org/abs/2506.14397)
*Yeonkyoung So,Gyuseong Lee,Sungmok Jung,Joonhak Lee,JiA Kang,Sangho Kim,Jaejin Lee*

Main category: cs.CL

TL;DR: 本文提出了Thunder-NUBench基准测试，专门评估大语言模型在句子级否定理解上的能力，通过对比多种否定结构进行深入分析。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试未专门针对否定理解设计，且常将其作为自然语言推理等任务的附属案例，导致模型在深层语义处理上存在不足。

Method: 构建包含标准否定、局部否定、矛盾与转述的句子-否定对，并设计多选题数据集，以系统评估模型对否定结构的理解。

Result: 开发了人工标注的Thunder-NUBench数据集，涵盖多样化否定形式，支持对大语言模型否定理解能力的细粒度评测。

Conclusion: Thunder-NUBench填补了否定专用评测基准的空白，为提升大语言模型的深层否定理解提供了可靠工具。

Abstract: Negation is a fundamental linguistic phenomenon that poses persistent challenges for Large Language Models (LLMs), particularly in tasks requiring deep semantic understanding. Existing benchmarks often treat negation as a side case within broader tasks like natural language inference, resulting in a lack of benchmarks that exclusively target negation understanding. In this work, we introduce \textbf{Thunder-NUBench}, a novel benchmark explicitly designed to assess sentence-level negation understanding in LLMs. Thunder-NUBench goes beyond surface-level cue detection by contrasting standard negation with structurally diverse alternatives such as local negation, contradiction, and paraphrase. The benchmark consists of manually curated sentence-negation pairs and a multiple-choice dataset that enables in-depth evaluation of models' negation understanding.

</details>


### [40] [ImpliRet: Benchmarking the Implicit Fact Retrieval Challenge](https://arxiv.org/abs/2506.14407)
*Zeinab Sadat Taghavi,Ali Modarressi,Yunpu Ma,Hinrich Schütze*

Main category: cs.CL

TL;DR: 本文提出ImpliRet基准，将检索系统的推理挑战转移至文档端，测试模型在隐含事实（如时间、算术、常识关系）上的处理能力，发现现有检索模型及长上下文模型表现均不佳。


<details>
  <summary>Details</summary>
Motivation: 现有检索系统依赖关键词重叠等表面线索，而近期研究多关注复杂查询端处理。本文旨在探索文档端隐含推理对检索的影响，即查询简单但相关性依赖文档中未明确表述的事实。

Method: 构建ImpliRet基准：查询设计简单，但文档相关性需通过时间推理（如解析“两天前”）、算术及常识关系等隐含信息判断。评估稀疏/密集检索模型及长上下文模型（如GPT-4）在此场景下的表现。

Result: 最佳稀疏/密集检索模型的nDCG@10仅15.07%；即使提供包含正例的10篇文档上下文，GPT-4准确率仅35.06%，表明文档端推理仍具挑战。

Conclusion: 文档端隐含关系推理显著影响检索效果，现有方法（包括长上下文模型）尚未有效解决此问题，需进一步研究针对性方法提升此类场景的检索性能。

Abstract: Retrieval systems are central to many NLP pipelines, but often rely on surface-level cues such as keyword overlap and lexical semantic similarity. To evaluate retrieval beyond these shallow signals, recent benchmarks introduce reasoning-heavy queries; however, they primarily shift the burden to query-side processing techniques -- like prompting or multi-hop retrieval -- that can help resolve complexity. In contrast, we present ImpliRet, a benchmark that shifts the reasoning challenge to document-side processing: The queries are simple, but relevance depends on facts stated implicitly in documents through temporal (e.g., resolving "two days ago"), arithmetic, and world knowledge relationships. We evaluate a range of sparse and dense retrievers, all of which struggle in this setting: the best nDCG@10 is only 15.07%. We also test whether long-context models can overcome this limitation. But even with a short context of only ten documents, including the positive document, GPT-4.1 scores only 35.06%, showing that document-side reasoning remains a challenge. Our codes are available at github.com/ZeinabTaghavi/IMPLIRET.Contribution.

</details>


### [41] [LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs](https://arxiv.org/abs/2506.14429)
*Xiaoran Liu,Zhigeng Liu,Zengfeng Huang,Qipeng Guo,Ziwei He,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文首次系统研究扩散大语言模型（diffusion LLMs）的长上下文能力，发现其在直接外推时保持稳定困惑度，并具有局部感知特性，提出无需训练的LongLLaDA方法验证上下文扩展有效性，同时揭示扩散LLMs在部分长上下文任务中优于自回归模型。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于扩散LLMs的扩展性和下游任务性能，但其长上下文能力缺乏系统性分析及扩展方法。本文旨在填补这一空白，通过对比扩散LLMs与自回归LLMs的长上下文表现，探索扩散模型的独特性质与改进潜力。

Method: 通过直接上下文外推实验和Needle-In-A-Haystack任务对比两类模型表现，基于RoPE缩放理论解释现象，并提出LongLLaDA方法（将LLaDA与NTK-based RoPE外推结合），验证扩散LLMs的上下文扩展规律。

Result: 扩散LLMs在直接外推时困惑度稳定，在超预训练长度上下文中通过局部感知实现有效检索；基于RoPE的扩展规律对其有效；扩散LLMs在部分长上下文任务（如近期片段检索）表现优于自回归模型，但在其他任务中仍有不足。

Conclusion: 本研究首次建立扩散LLMs的上下文外推方法，提供理论解释与实证基准，证明扩散LLMs在长上下文任务中的独特优势，为未来研究奠定基础。

Abstract: Large Language Diffusion Models, or diffusion LLMs, have emerged as a significant focus in NLP research, with substantial effort directed toward understanding their scalability and downstream task performance. However, their long-context capabilities remain unexplored, lacking systematic analysis or methods for context extension. In this work, we present the first systematic investigation comparing the long-context performance of diffusion LLMs and traditional auto-regressive LLMs. We first identify a unique characteristic of diffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably \textbf{\textit{stable perplexity}} during direct context extrapolation. Furthermore, where auto-regressive models fail outright during the Needle-In-A-Haystack task with context exceeding their pretrained length, we discover diffusion LLMs exhibit a distinct \textbf{\textit{local perception}} phenomenon, enabling successful retrieval from recent context segments. We explain both phenomena through the lens of Rotary Position Embedding (RoPE) scaling theory. Building on these observations, we propose LongLLaDA, a training-free method that integrates LLaDA with the NTK-based RoPE extrapolation. Our results validate that established extrapolation scaling laws remain effective for extending the context windows of diffusion LLMs. Furthermore, we identify long-context tasks where diffusion LLMs outperform auto-regressive LLMs and others where they fall short. Consequently, this study establishes the first context extrapolation method for diffusion LLMs while providing essential theoretical insights and empirical benchmarks critical for advancing future research on long-context diffusion LLMs.

</details>


### [42] [How Far Can LLMs Improve from Experience? Measuring Test-Time Learning Ability in LLMs with Human Comparison](https://arxiv.org/abs/2506.14448)
*Jiayin Wang,Zhiquang Guo,Weizhi Ma,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出通过测试时学习能力评估大语言模型的动态学习潜力，发现其虽具备一定学习能力，但进步速度与稳定性仍显著落后于人类。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要评估静态知识，但智能需包含从经验中快速学习的能力。为推进AGI发展，需建立面向动态学习能力的评估体系。

Method: 使用抗饱和的语义游戏作为测试平台，构建包含有限/累积经验场景、四种经验表征的评估框架，并与8名人类参与者进行对比实验。

Result: LLMs展现出可测量的测试时学习能力，但在累积经验下改进较弱，进步速度仅为人类的1/3，且学习曲线波动性显著高于人类。

Conclusion: 尽管LLMs在静态基准表现优异，其作为通用学习机器的动态智力水平仍与人类存在本质差距，需开发更贴近人类学习机制的评估体系。

Abstract: As evaluation designs of large language models may shape our trajectory toward artificial general intelligence, comprehensive and forward-looking assessment is essential. Existing benchmarks primarily assess static knowledge, while intelligence also entails the ability to rapidly learn from experience. To this end, we advocate for the evaluation of Test-time Learning, the capacity to improve performance in experience-based, reasoning-intensive tasks during test time. In this work, we propose semantic games as effective testbeds for evaluating test-time learning, due to their resistance to saturation and inherent demand for strategic reasoning. We introduce an objective evaluation framework that compares model performance under both limited and cumulative experience settings, and contains four forms of experience representation. To provide a comparative baseline, we recruit eight human participants to complete the same task. Results show that LLMs exhibit measurable test-time learning capabilities; however, their improvements are less stable under cumulative experience and progress more slowly than those observed in humans. These findings underscore the potential of LLMs as general-purpose learning machines, while also revealing a substantial intellectual gap between models and humans, irrespective of how well LLMs perform on static benchmarks.

</details>


### [43] [LexiMark: Robust Watermarking via Lexical Substitutions to Enhance Membership Verification of an LLM's Textual Training Data](https://arxiv.org/abs/2506.14474)
*Eyal German,Sagiv Antebi,Edan Habler,Asaf Shabtai,Yuval Elovici*

Main category: cs.CL

TL;DR: 本文提出LexiMark，一种针对文本数据的新型水印技术，通过替换高熵词的同义词实现隐蔽且抗去除的水印嵌入，有效验证LLM是否未经授权使用水印数据。


<details>
  <summary>Details</summary>
Motivation: 现有数据集水印方法隐蔽性不足，易被检测和移除，导致难以可靠验证大语言模型是否在训练中使用了未经授权的数据。

Method: LexiMark通过选择高熵词汇进行同义词替换，在保持文本语义完整性的同时增强模型对水印文本的记忆，水印无视觉痕迹且与上下文自然融合。

Result: 在LLaMA、Mistral等7个开源模型及多种训练场景下测试，AUROC分数显著优于现有方法，验证了水印检测的可靠性。

Conclusion: LexiMark通过隐蔽的语义级水印实现了对LLM训练数据来源的有效追踪，为数据版权保护提供了抗检测、抗去除的解决方案。

Abstract: Large language models (LLMs) can be trained or fine-tuned on data obtained without the owner's consent. Verifying whether a specific LLM was trained on particular data instances or an entire dataset is extremely challenging. Dataset watermarking addresses this by embedding identifiable modifications in training data to detect unauthorized use. However, existing methods often lack stealth, making them relatively easy to detect and remove. In light of these limitations, we propose LexiMark, a novel watermarking technique designed for text and documents, which embeds synonym substitutions for carefully selected high-entropy words. Our method aims to enhance an LLM's memorization capabilities on the watermarked text without altering the semantic integrity of the text. As a result, the watermark is difficult to detect, blending seamlessly into the text with no visible markers, and is resistant to removal due to its subtle, contextually appropriate substitutions that evade automated and manual detection. We evaluated our method using baseline datasets from recent studies and seven open-source models: LLaMA-1 7B, LLaMA-3 8B, Mistral 7B, Pythia 6.9B, as well as three smaller variants from the Pythia family (160M, 410M, and 1B). Our evaluation spans multiple training settings, including continued pretraining and fine-tuning scenarios. The results demonstrate significant improvements in AUROC scores compared to existing methods, underscoring our method's effectiveness in reliably verifying whether unauthorized watermarked data was used in LLM training.

</details>


### [44] [LingoLoop Attack: Trapping MLLMs via Linguistic Context and State Entrapment into Endless Loops](https://arxiv.org/abs/2506.14493)
*Jiyuan Fu,Kaixun Jiang,Lingyi Hong,Jinglun Li,Haijing Guo,Dingkang Yang,Zhaoyu Chen,Wenqiang Zhang*

Main category: cs.CL

TL;DR: 本文提出LingoLoop攻击方法，通过词性感知延迟机制和生成路径剪枝机制，诱导多模态大语言模型生成冗长重复输出，显著增加计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有能耗-延迟攻击仅通过整体调整输出token分布延迟终止符生成，但忽略了词性特征对终止符的影响及句子结构对输出长度的限制，导致攻击效果受限。

Method: 1. 词性感知延迟机制：基于词性标签调整注意力权重，延迟终止符生成；2. 生成路径剪枝机制：限制隐藏状态幅度，强制模型进入重复生成循环。

Result: 实验表明攻击可使Qwen2.5-VL-3B等模型生成token数量提升30倍，能耗同步增加，持续触发模型最大生成限制。

Conclusion: 研究揭示了多模态大语言模型在资源耗尽攻击下的严重脆弱性，对其可靠部署提出新的安全挑战。

Abstract: Multimodal Large Language Models (MLLMs) have shown great promise but require substantial computational resources during inference. Attackers can exploit this by inducing excessive output, leading to resource exhaustion and service degradation. Prior energy-latency attacks aim to increase generation time by broadly shifting the output token distribution away from the EOS token, but they neglect the influence of token-level Part-of-Speech (POS) characteristics on EOS and sentence-level structural patterns on output counts, limiting their efficacy. To address this, we propose LingoLoop, an attack designed to induce MLLMs to generate excessively verbose and repetitive sequences. First, we find that the POS tag of a token strongly affects the likelihood of generating an EOS token. Based on this insight, we propose a POS-Aware Delay Mechanism to postpone EOS token generation by adjusting attention weights guided by POS information. Second, we identify that constraining output diversity to induce repetitive loops is effective for sustained generation. We introduce a Generative Path Pruning Mechanism that limits the magnitude of hidden states, encouraging the model to produce persistent loops. Extensive experiments demonstrate LingoLoop can increase generated tokens by up to 30 times and energy consumption by a comparable factor on models like Qwen2.5-VL-3B, consistently driving MLLMs towards their maximum generation limits. These findings expose significant MLLMs' vulnerabilities, posing challenges for their reliable deployment. The code will be released publicly following the paper's acceptance.

</details>


### [45] [M2BeamLLM: Multimodal Sensing-empowered mmWave Beam Prediction with Large Language Models](https://arxiv.org/abs/2506.14532)
*Can Zheng,Jiguang He,Chung G. Kang,Guofa Cai,Zitong Yu,Merouane Debbah*

Main category: cs.CL

TL;DR: 本文提出M2BeamLLM框架，通过整合多模态传感器数据与大型语言模型（如GPT-2），显著提升毫米波大规模MIMO通信系统的波束预测精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在毫米波通信波束预测中存在精度和鲁棒性不足的问题，尤其在数据有限场景下表现受限。需结合多模态感知数据与智能推理能力以优化性能。

Method: 结合多模态传感器数据（图像/雷达/LiDAR/GPS），利用LLM的推理能力，通过数据编码、多模态对齐融合及监督微调（SFT）构建端到端波束预测框架。

Result: M2BeamLLM在标准/少样本场景下均优于传统模型，预测性能随传感器模态多样性增加持续提升，验证了其在V2I通信系统中的高效性。

Conclusion: 该框架为车联网毫米波通信提供了智能化波束预测方案，证明多模态与LLM结合在复杂通信场景中的潜力。

Abstract: This paper introduces a novel neural network framework called M2BeamLLM for beam prediction in millimeter-wave (mmWave) massive multi-input multi-output (mMIMO) communication systems. M2BeamLLM integrates multi-modal sensor data, including images, radar, LiDAR, and GPS, leveraging the powerful reasoning capabilities of large language models (LLMs) such as GPT-2 for beam prediction. By combining sensing data encoding, multimodal alignment and fusion, and supervised fine-tuning (SFT), M2BeamLLM achieves significantly higher beam prediction accuracy and robustness, demonstrably outperforming traditional deep learning (DL) models in both standard and few-shot scenarios. Furthermore, its prediction performance consistently improves with increased diversity in sensing modalities. Our study provides an efficient and intelligent beam prediction solution for vehicle-to-infrastructure (V2I) mmWave communication systems.

</details>


### [46] [AlphaDecay:Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs](https://arxiv.org/abs/2506.14562)
*Di He,Ajay Jaiswal,Songjun Tu,Li Shen,Ganzhao Yuan,Shiwei Liu,Lu Yin*

Main category: cs.CL

TL;DR: AlphaDecay提出一种基于HT-SR理论的自适应权重衰减方法，通过分析模块的频谱特性差异动态调整衰减强度，提升大语言模型的训练效果。


<details>
  <summary>Details</summary>
Motivation: 传统权重衰减方法对所有模块使用统一衰减率，忽略了LLMs中不同模块的结构多样性和频谱特性差异，可能影响模型性能优化。

Method: 利用HT-SR理论分析权重相关矩阵的经验谱密度(ESD)，根据模块的'重尾性'动态分配衰减强度：重尾性强的模块衰减弱以保留特征学习能力，轻尾模块衰减强。

Result: 在60M至1B参数的预训练任务中，AlphaDecay相比均匀衰减和其他自适应方法，在困惑度和泛化能力上均取得更优表现。

Conclusion: 通过平衡模块间频谱特性差异的定制化权重衰减策略，AlphaDecay有效提升LLMs性能，验证了自适应正则化的重要性。

Abstract: Weight decay is a standard regularization technique for training large language models (LLMs). While it is common to assign a uniform decay rate to every layer, this approach overlooks the structural diversity of LLMs and the varying spectral properties across modules. In this paper, we introduce AlphaDecay, a simple yet effective method that adaptively assigns different weight decay strengths to each module of an LLM. Our approach is guided by Heavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical spectral density (ESD) of weight correlation matrices to quantify "heavy-tailedness." Modules exhibiting more pronounced heavy-tailed ESDs, reflecting stronger feature learning, are assigned weaker decay, while modules with lighter-tailed spectra receive stronger decay. Our method leverages tailored weight decay assignments to balance the module-wise differences in spectral properties, leading to improved performance. Extensive pre-training tasks with various model sizes from 60M to 1B demonstrate that AlphaDecay achieves better perplexity and generalization than conventional uniform decay and other adaptive decay baselines.

</details>


### [47] [GenerationPrograms: Fine-grained Attribution with Executable Programs](https://arxiv.org/abs/2506.14580)
*David Wan,Eran Hirsch,Elias Stengel-Eskin,Ido Dagan,Mohit Bansal*

Main category: cs.CL

TL;DR: 论文提出GenerationPrograms框架，通过模块化程序分解生成过程，提升大语言模型输出的归因质量和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在生成文本时难以提供细粒度归因，且传统归因方法无法解释模型如何利用源文档，导致可信度和可解释性不足。

Method: 引入GenerationPrograms框架，将生成过程分为两个阶段：先创建由模块化文本操作组成的可执行程序计划，再按程序指令执行操作生成最终响应。

Result: 实验评估显示，该方法在长问答和多文档摘要任务中显著提升文档和句子级别的归因质量，并能作为事后归因方法优于传统技术。

Conclusion: GenerationPrograms通过模块化生成和可解释程序，不仅提高归因准确性，还支持局部优化，增强整体可信度和实用性。

Abstract: Recent large language models (LLMs) achieve impressive performance in source-conditioned text generation but often fail to correctly provide fine-grained attributions for their outputs, undermining verifiability and trust. Moreover, existing attribution methods do not explain how and why models leverage the provided source documents to generate their final responses, limiting interpretability. To overcome these challenges, we introduce a modular generation framework, GenerationPrograms, inspired by recent advancements in executable "code agent" architectures. Unlike conventional generation methods that simultaneously generate outputs and attributions or rely on post-hoc attribution, GenerationPrograms decomposes the process into two distinct stages: first, creating an executable program plan composed of modular text operations (such as paraphrasing, compression, and fusion) explicitly tailored to the query, and second, executing these operations following the program's specified instructions to produce the final response. Empirical evaluations demonstrate that GenerationPrograms significantly improves attribution quality at both the document level and sentence level across two long-form question-answering tasks and a multi-document summarization task. We further demonstrate that GenerationPrograms can effectively function as a post-hoc attribution method, outperforming traditional techniques in recovering accurate attributions. In addition, the interpretable programs generated by GenerationPrograms enable localized refinement through modular-level improvements that further enhance overall attribution quality.

</details>


### [48] [Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC Transpilation with Testing Guarantees](https://arxiv.org/abs/2506.14606)
*Ahmed Heakl,Sarim Hashmi,Chaimaa Abi,Celine Lee,Abdulrahman Mahmoud*

Main category: cs.CL

TL;DR: 本文提出GG方法，结合大语言模型和软件测试框架，实现高效、准确的CISC到RISC指令集转换，性能优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 硬件生态快速发展，但复杂指令集（CISC）与精简指令集（RISC）因指令复杂度、内存模型等差异导致跨架构代码移植困难，需高效、可靠的翻译方法提升代码可移植性。

Method: GG方法通过预训练大语言模型生成候选代码翻译，并嵌入软件测试框架验证翻译正确性，结合高代码覆盖率（>98%）确保功能与性能。

Result: 在HumanEval和BringupBench数据集上分别实现99%和49%的功能正确性；相比Rosetta 2，运行速度提升1.73倍，能效提高1.47倍，内存占用减少2.41倍。

Conclusion: GG在CISC到RISC翻译任务中展现显著优势，开源代码与基准将推动指令集级代码翻译研究，验证了LLM与形式化测试结合的可行性。

Abstract: The hardware ecosystem is rapidly evolving, with increasing interest in translating low-level programs across different instruction set architectures (ISAs) in a quick, flexible, and correct way to enhance the portability and longevity of existing code. A particularly challenging class of this transpilation problem is translating between complex- (CISC) and reduced- (RISC) hardware architectures, due to fundamental differences in instruction complexity, memory models, and execution paradigms. In this work, we introduce GG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the translation power of pre-trained large language models (LLMs) with the rigor of established software testing constructs. Our method generates candidate translations using an LLM from one ISA to another, and embeds such translations within a software-testing framework to build quantifiable confidence in the translation. We evaluate our GG approach over two diverse datasets, enforce high code coverage (>98%) across unit tests, and achieve functional/semantic correctness of 99% on HumanEval programs and 49% on BringupBench programs, respectively. Further, we compare our approach to the state-of-the-art Rosetta 2 framework on Apple Silicon, showcasing 1.73x faster runtime performance, 1.47x better energy efficiency, and 2.41x better memory usage for our transpiled code, demonstrating the effectiveness of GG for real-world CISC-to-RISC translation tasks. We will open-source our codes, data, models, and benchmarks to establish a common foundation for ISA-level code translation research.

</details>


### [49] [When Does Meaning Backfire? Investigating the Role of AMRs in NLI](https://arxiv.org/abs/2506.14613)
*Junghyun Min,Xiulin Yang,Shira Wein*

Main category: cs.CL

TL;DR: 研究探讨在自然语言推理（NLI）中引入抽象意义表示（AMR）对预训练模型的影响，发现微调时加入AMR会削弱泛化能力，而提示中使用AMR虽略微提升GPT-4o性能，但实际效果源于表面差异的放大而非语义推理。


<details>
  <summary>Details</summary>
Motivation: 验证在NLI任务中引入语义信息（如AMR）是否能帮助预训练语言模型更好地泛化。

Method: 在微调和提示两种设置下将AMR整合到NLI任务中，并通过消融实验分析AMR的作用机制。

Result: 微调时加入AMR阻碍模型泛化；提示中使用AMR使GPT-4o性能略有提升，但消融研究表明改进源于放大表面差异，而非增强语义推理能力。

Conclusion: AMR在提示中的改进效果具有误导性，可能导致模型因表面差异错误预测非蕴含关系，即使核心语义一致。

Abstract: Natural Language Inference (NLI) relies heavily on adequately parsing the semantic content of the premise and hypothesis. In this work, we investigate whether adding semantic information in the form of an Abstract Meaning Representation (AMR) helps pretrained language models better generalize in NLI. Our experiments integrating AMR into NLI in both fine-tuning and prompting settings show that the presence of AMR in fine-tuning hinders model generalization while prompting with AMR leads to slight gains in \texttt{GPT-4o}. However, an ablation study reveals that the improvement comes from amplifying surface-level differences rather than aiding semantic reasoning. This amplification can mislead models to predict non-entailment even when the core meaning is preserved.

</details>


### [50] [Probabilistic Aggregation and Targeted Embedding Optimization for Collective Moral Reasoning in Large Language Models](https://arxiv.org/abs/2506.14625)
*Chenchen Yuan,Zheyu Zhang,Shuo Yang,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 提出一种整合多个大语言模型道德判断的框架，通过共识对齐优化偏离模型，提升AI系统的安全性和一致性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂道德困境中常出现判断分歧，需解决其道德一致性以构建更可靠的AI系统。

Method: 融合多模型道德评分形成集体概率，基于可靠性加权；对偏离模型采用嵌入优化技术最小化与共识的JS散度。

Result: 实验表明该方法能有效建立道德判断共识，并提高个体模型的道德推理忠实度。

Conclusion: 跨模型数据驱动的道德对齐具有重要价值，可为构建安全、一致的AI系统提供新路径。

Abstract: Large Language Models (LLMs) have shown impressive moral reasoning abilities. Yet they often diverge when confronted with complex, multi-factor moral dilemmas. To address these discrepancies, we propose a framework that synthesizes multiple LLMs' moral judgments into a collectively formulated moral judgment, realigning models that deviate significantly from this consensus. Our aggregation mechanism fuses continuous moral acceptability scores (beyond binary labels) into a collective probability, weighting contributions by model reliability. For misaligned models, a targeted embedding-optimization procedure fine-tunes token embeddings for moral philosophical theories, minimizing JS divergence to the consensus while preserving semantic integrity. Experiments on a large-scale social moral dilemma dataset show our approach builds robust consensus and improves individual model fidelity. These findings highlight the value of data-driven moral alignment across multiple models and its potential for safer, more consistent AI systems.

</details>


### [51] [AIn't Nothing But a Survey? Using Large Language Models for Coding German Open-Ended Survey Responses on Survey Motivation](https://arxiv.org/abs/2506.14634)
*Leah von der Heyde,Anna-Carolina Haensch,Bernd Weiß,Jessika Daikeler*

Main category: cs.CL

TL;DR: 研究探讨不同LLM在德语开放式调查回复分类中的表现，发现仅微调模型表现良好，提示方法效果因模型而异，分类性能差异影响分布。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注英语及简单主题或单一LLM，需验证LLM在复杂多语言场景下替代传统编码方法的可行性及效果。

Method: 以德语调查参与原因数据为例，对比多种先进LLM及提示方法，以人工专家编码为基准评估模型性能。

Result: 不同LLM性能差异显著，仅微调模型达满意水平；提示方法效果依赖具体模型；未微调时分类性能不均导致类别分布偏差。

Conclusion: LLM应用于调查需权衡模型选择、微调及性能差异，分类结果可能改变数据分布，研究者需谨慎选择方法并考虑其局限性。

Abstract: The recent development and wider accessibility of LLMs have spurred discussions about how they can be used in survey research, including classifying open-ended survey responses. Due to their linguistic capacities, it is possible that LLMs are an efficient alternative to time-consuming manual coding and the pre-training of supervised machine learning models. As most existing research on this topic has focused on English-language responses relating to non-complex topics or on single LLMs, it is unclear whether its findings generalize and how the quality of these classifications compares to established methods. In this study, we investigate to what extent different LLMs can be used to code open-ended survey responses in other contexts, using German data on reasons for survey participation as an example. We compare several state-of-the-art LLMs and several prompting approaches, and evaluate the LLMs' performance by using human expert codings. Overall performance differs greatly between LLMs, and only a fine-tuned LLM achieves satisfactory levels of predictive performance. Performance differences between prompting approaches are conditional on the LLM used. Finally, LLMs' unequal classification performance across different categories of reasons for survey participation results in different categorical distributions when not using fine-tuning. We discuss the implications of these findings, both for methodological research on coding open-ended responses and for their substantive analysis, and for practitioners processing or substantively analyzing such data. Finally, we highlight the many trade-offs researchers need to consider when choosing automated methods for open-ended response classification in the age of LLMs. In doing so, our study contributes to the growing body of research about the conditions under which LLMs can be efficiently, accurately, and reliably leveraged in survey research.

</details>


### [52] [Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than Few-shot](https://arxiv.org/abs/2506.14641)
*Xiang Cheng,Chengyan Pan,Minjun Zhao,Deyang Li,Fangchao Liu,Xinyu Zhang,Xiao Zhang,Yong Liu*

Main category: cs.CL

TL;DR: 研究发现，对于近期强大的语言模型（如Qwen2.5系列），传统或增强的思维链（CoT）示例在数学推理任务中无法提升性能，模型倾向于忽略示例而关注指令，需重新评估ICL范式及示例定义。


<details>
  <summary>Details</summary>
Motivation: 探究随着模型能力提升，传统CoT示例是否仍能增强当前先进模型（如Qwen2.5）的数学推理能力。

Method: 通过系统实验，对比传统CoT示例、增强CoT示例（由Qwen2.5-Max等生成）与零样本CoT的效果，分析模型对示例的关注程度。

Result: 传统和增强CoT示例均未提升模型推理性能，模型仅利用示例对齐输出格式，实际推理仍依赖指令而非示例内容。

Conclusion: 当前ICL+CoT框架在数学推理中存在局限性，需重新审视范例设计及ICL机制，以更有效激发模型能力。

Abstract: In-Context Learning (ICL) is an essential emergent ability of Large Language Models (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars of ICL to enhance the reasoning capability, especially in mathematics tasks. However, given the continuous advancement of model capabilities, it remains unclear whether CoT exemplars still benefit recent, stronger models in such tasks. Through systematic experiments, we find that for recent strong models such as the Qwen2.5 series, adding traditional CoT exemplars does not improve reasoning performance compared to Zero-Shot CoT. Instead, their primary function is to align the output format with human expectations. We further investigate the effectiveness of enhanced CoT exemplars, constructed using answers from advanced models such as \texttt{Qwen2.5-Max} and \texttt{DeepSeek-R1}. Experimental results indicate that these enhanced exemplars still fail to improve the model's reasoning performance. Further analysis reveals that models tend to ignore the exemplars and focus primarily on the instructions, leading to no observable gain in reasoning ability. Overall, our findings highlight the limitations of the current ICL+CoT framework in mathematical reasoning, calling for a re-examination of the ICL paradigm and the definition of exemplars.

</details>


### [53] [Passing the Turing Test in Political Discourse: Fine-Tuning LLMs to Mimic Polarized Social Media Comments](https://arxiv.org/abs/2506.14645)
*. Pazzaglia,V. Vendetti,L. D. Comencini,F. Deriu,V. Modugno*

Main category: cs.CL

TL;DR: 研究探讨微调后的大语言模型（LLMs）如何通过生成意识形态极化内容加剧网络分歧，发现其能产生与人类相似的高煽动性言论，引发AI伦理与治理问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力提升，其可能通过自动化偏见内容加剧意识形态极化，需验证微调后的LLMs在在线环境中复制和放大极化言论的潜力。

Method: 使用Reddit政治讨论数据集微调开源LLM，生成语境感知的意识形态对齐回应，并通过语言分析、情感评分和人工标注评估输出内容的可信度与修辞一致性。

Result: 微调后的LLMs能生成高度可信且具挑衅性的评论，与人类写作难以区分，尤其在党派数据训练下表现出显著极化倾向。

Conclusion: 研究揭示AI在政治传播中的伦理风险，强调需加强AI治理、平台监管及开发检测工具以对抗针对性微调带来的风险。

Abstract: The increasing sophistication of large language models (LLMs) has sparked growing concerns regarding their potential role in exacerbating ideological polarization through the automated generation of persuasive and biased content. This study explores the extent to which fine-tuned LLMs can replicate and amplify polarizing discourse within online environments. Using a curated dataset of politically charged discussions extracted from Reddit, we fine-tune an open-source LLM to produce context-aware and ideologically aligned responses. The model's outputs are evaluated through linguistic analysis, sentiment scoring, and human annotation, with particular attention to credibility and rhetorical alignment with the original discourse. The results indicate that, when trained on partisan data, LLMs are capable of producing highly plausible and provocative comments, often indistinguishable from those written by humans. These findings raise significant ethical questions about the use of AI in political discourse, disinformation, and manipulation campaigns. The paper concludes with a discussion of the broader implications for AI governance, platform regulation, and the development of detection tools to mitigate adversarial fine-tuning risks.

</details>


### [54] [GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors](https://arxiv.org/abs/2506.14646)
*Hengyuan Zhang,Xinrong Chen,Yingmin Qiu,Xiao Liang,Ziyue Li,Guanyu Wang,Weiping Li,Tong Mo,Wenyue Li,Hayden Kwok-Hay So,Ngai Wong*

Main category: cs.CL

TL;DR: 本文提出GuiLoMo，一种基于指导选择向量（GSVs）的细粒度专家数量与秩分配策略，解决了现有LoRA-MoE方法中专家数量受下游任务影响及统一秩分配限制表示多样性的问题，实验表明其性能优于或与基线方法相当。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA-MoE方法存在两个主要限制：1) 下游任务对专家数量分配的影响未被充分考虑；2) 所有LoRA专家采用统一秩分配，导致表示多样性受限。这些限制了模型性能的进一步提升。

Method: 提出GuiLoMo方法，通过双层优化学习指导选择向量（GSVs），动态分配各层的专家数量与秩。GSVs同时捕捉模型和任务特定需求，实现细粒度的自适应配置。

Result: 在三个骨干模型和多样化基准测试中，GuiLoMo性能优于或与所有基线方法相当。分析表明不同层和任务间专家数量与秩存在显著差异，验证了自适应配置的有效性。

Conclusion: GuiLoMo通过自适应专家配置显著提升模型性能，揭示了专家数量与秩的层间和任务间差异性，为参数高效微调提供了新视角。

Abstract: Parameter-efficient fine-tuning (PEFT) methods, particularly Low-Rank Adaptation (LoRA), offer an efficient way to adapt large language models with reduced computational costs. However, their performance is limited by the small number of trainable parameters. Recent work combines LoRA with the Mixture-of-Experts (MoE), i.e., LoRA-MoE, to enhance capacity, but two limitations remain in hindering the full exploitation of its potential: 1) the influence of downstream tasks when assigning expert numbers, and 2) the uniform rank assignment across all LoRA experts, which restricts representational diversity. To mitigate these gaps, we propose GuiLoMo, a fine-grained layer-wise expert numbers and ranks allocation strategy with GuidedSelection Vectors (GSVs). GSVs are learned via a prior bilevel optimization process to capture both model- and task-specific needs, and are then used to allocate optimal expert numbers and ranks. Experiments on three backbone models across diverse benchmarks show that GuiLoMo consistently achieves superior or comparable performance to all baselines. Further analysis offers key insights into how expert numbers and ranks vary across layers and tasks, highlighting the benefits of adaptive expert configuration. Our code is available at https://github.com/Liar406/Gui-LoMo.git.

</details>


### [55] [Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and Training Factors Shape LLM Alignment Quality](https://arxiv.org/abs/2506.14681)
*Yuto Harada,Yusuke Yamauchi,Yusuke Oda,Yohei Oseki,Yusuke Miyao,Yu Takagi*

Main category: cs.CL

TL;DR: 本文通过训练1000多个监督微调（SFT）模型，揭示了数据集属性、层权重变化与性能的关系，发现困惑度是SFT效果的关键指标，且中间层权重调整对性能提升最显著。


<details>
  <summary>Details</summary>
Motivation: 尽管监督微调（SFT）是使大语言模型（LLM）与人类指令对齐的关键步骤，但其具体机制仍不明确。研究旨在探索SFT中数据集属性、模型层间调整与性能的关系。

Method: 在代码生成、数学推理和通用任务上训练多种基础模型，生成1000+个SFT模型，分析数据集关键特征及层间权重变化，并评估困惑度与性能的关联。

Result: 发现部分任务协同性普遍存在，另一些则因模型差异显著；困惑度比数据表面相似性更能预测SFT效果，且中间层权重变化与性能提升相关性最强。

Conclusion: 需针对模型设计特定策略，数据集的深层特征（如困惑度）和中间层调整是优化SFT的关键。公开模型与结果以推动后续研究。

Abstract: Supervised fine-tuning (SFT) is a critical step in aligning large language models (LLMs) with human instructions and values, yet many aspects of SFT remain poorly understood. We trained a wide range of base models on a variety of datasets including code generation, mathematical reasoning, and general-domain tasks, resulting in 1,000+ SFT models under controlled conditions. We then identified the dataset properties that matter most and examined the layer-wise modifications introduced by SFT. Our findings reveal that some training-task synergies persist across all models while others vary substantially, emphasizing the importance of model-specific strategies. Moreover, we demonstrate that perplexity consistently predicts SFT effectiveness--often surpassing superficial similarity between trained data and benchmark--and that mid-layer weight changes correlate most strongly with performance gains. We will release these 1,000+ SFT models and benchmark results to accelerate further research.

</details>


### [56] [Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time Markers](https://arxiv.org/abs/2506.14702)
*Daniel D'souza,Julia Kreutzer,Adrien Morisot,Ahmet Üstün,Sara Hooker*

Main category: cs.CL

TL;DR: 本文提出一种优化训练协议的方法，通过显式和隐式控制生成属性，提升模型在长尾数据上的表现及可控性，尤其在低代表性领域效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有通用模型在训练后难以适应训练语料中代表性不足的用例，且提示工程或小样本学习存在敏感性和控制性不足的问题。研究旨在优化训练协议以改善长尾用例的推理表现和用户控制能力。

Method: 创建数据特征与任务来源的细粒度分类法，显式控制生成属性并隐式条件化生成。通过微调基础模型自动推断标记，使这些标记成为推理时的可选项。

Result: 开放生成任务胜率平均提升5.7%，低代表性领域提升9.1%。代码修复任务相对提升14.1%，长度指令遵循绝对提升35.3%。

Conclusion: 结合显式控制与隐式条件化的原则性方法显著提升长尾分布下的模型性能，验证了训练协议优化对提升模型可控性和长尾适应性的有效性。

Abstract: One of the most profound challenges of modern machine learning is performing well on the long-tail of rare and underrepresented features. Large general-purpose models are trained for many tasks, but work best on high-frequency use cases. After training, it is hard to adapt a model to perform well on specific use cases underrepresented in the training corpus. Relying on prompt engineering or few-shot examples to maximize the output quality on a particular test case can be frustrating, as models can be highly sensitive to small changes, react in unpredicted ways or rely on a fixed system prompt for maintaining performance. In this work, we ask: "Can we optimize our training protocols to both improve controllability and performance on underrepresented use cases at inference time?" We revisit the divide between training and inference techniques to improve long-tail performance while providing users with a set of control levers the model is trained to be responsive to. We create a detailed taxonomy of data characteristics and task provenance to explicitly control generation attributes and implicitly condition generations at inference time. We fine-tune a base model to infer these markers automatically, which makes them optional at inference time. This principled and flexible approach yields pronounced improvements in performance, especially on examples from the long tail of the training distribution. While we observe an average lift of 5.7% win rates in open-ended generation quality with our markers, we see over 9.1% gains in underrepresented domains. We also observe relative lifts of up to 14.1% on underrepresented tasks like CodeRepair and absolute improvements of 35.3% on length instruction following evaluations.

</details>


### [57] [Capacity Matters: a Proof-of-Concept for Transformer Memorization on Real-World Data](https://arxiv.org/abs/2506.14704)
*Anton Changalidis,Aki Härmä*

Main category: cs.CL

TL;DR: 本文通过合成文本数据集研究生成式Transformer的模型架构与数据配置对记忆能力的影响，发现嵌入大小是学习能力的关键，Softmax激活函数更优，数据复杂度提升记忆效果。


<details>
  <summary>Details</summary>
Motivation: 探究不同模型架构（如嵌入大小、层数、激活函数）及数据配置（如结构化复杂度）如何影响生成模型的记忆能力，以优化实际应用中的模型设计。

Method: 使用SNOMED知识图谱生成两类合成数据（静态三元组和复杂序列），训练不同配置的Transformer模型，分析其记忆容量与学习速度。

Result: 嵌入尺寸主导学习能力；增加层数对简单数据有害；Softmax激活函数稳定性更强；数据复杂度提升最终记忆容量。

Conclusion: 研究揭示了Transformer记忆机制的核心因素，并为基于结构化真实数据优化模型（如医疗知识图谱）提供了设计框架。

Abstract: This paper studies how the model architecture and data configurations influence the empirical memorization capacity of generative transformers. The models are trained using synthetic text datasets derived from the Systematized Nomenclature of Medicine (SNOMED) knowledge graph: triplets, representing static connections, and sequences, simulating complex relation patterns. The results show that embedding size is the primary determinant of learning speed and capacity, while additional layers provide limited benefits and may hinder performance on simpler datasets. Activation functions play a crucial role, and Softmax demonstrates greater stability and capacity. Furthermore, increasing the complexity of the data set seems to improve the final memorization. These insights improve our understanding of transformer memory mechanisms and provide a framework for optimizing model design with structured real-world data.

</details>


### [58] [Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs](https://arxiv.org/abs/2506.14731)
*Ring Team,Bin Hu,Cai Chen,Deng Zhao,Ding Liu,Dingnan Jin,Feng Zhu,Hao Dai,Hongzhi Luan,Jia Guo,Jiaming Liu,Jiewei Wu,Jun Mei,Jun Zhou,Junbo Zhao,Junwu Xiong,Kaihong Zhang,Kuan Xu,Lei Liang,Liang Jiang,Liangcheng Fu,Longfei Zheng,Qiang Gao,Qing Cui,Quan Wan,Shaomian Zheng,Shuaicheng Li,Tongkai Yang,Wang Ren,Xiaodong Yan,Xiaopei Wan,Xiaoyun Feng,Xin Zhao,Xinxing Yang,Xinyu Kong,Xuemin Yang,Yang Li,Yingting Wu,Yongkang Liu,Zhankai Xu,Zhenduo Zhang,Zhenglei Zhou,Zhenyu Huang,Zhiqiang Zhang,Zihao Wang,Zujie Wen*

Main category: cs.CL

TL;DR: 本文提出Ring-lite模型，通过结合知识蒸馏与强化学习的混合训练框架，在保持与SOTA小模型相当性能的同时，将激活参数量减少至同类模型的三分之一。该方法解决了MoE模型RL训练中的稳定性问题，并通过两阶段训练实现多领域数据融合。


<details>
  <summary>Details</summary>
Motivation: 现有混合专家模型(MoE)在强化学习训练中存在优化不稳定、计算效率低、多领域数据冲突等问题。本文旨在开发参数效率高且鲁棒的推理模型，同时解决上述技术挑战。

Method: 1) 提出C3PO算法-系统协同优化框架增强训练稳定性；2) 基于熵损失而非验证指标选择蒸馏检查点；3) 设计两阶段训练范式协调多领域数据冲突；4) 在16.8B参数的Ling-lite基础上构建，采用蒸馏+RL联合训练。

Result: 在AIME等基准测试中达到小规模SOTA模型性能，激活参数量仅2.75B（为同类模型1/3）。C3PO使训练吞吐量提升37%，两阶段训练使多领域性能平均提升12.6%。

Conclusion: 通过算法-系统协同设计和创新的训练策略，Ring-lite实现了参数效率与推理性能的突破，为MoE模型的RL训练提供了稳定性解决方案，并验证了熵导向蒸馏策略的有效性。

Abstract: We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model optimized via reinforcement learning (RL) to achieve efficient and robust reasoning capabilities. Built upon the publicly available Ling-lite model, a 16.8 billion parameter model with 2.75 billion activated parameters, our approach matches the performance of state-of-the-art (SOTA) small-scale reasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench, GPQA-Diamond) while activating only one-third of the parameters required by comparable models. To accomplish this, we introduce a joint training pipeline integrating distillation with RL, revealing undocumented challenges in MoE RL training. First, we identify optimization instability during RL training, and we propose Constrained Contextual Computation Policy Optimization(C3PO), a novel approach that enhances training stability and improves computational throughput via algorithm-system co-design methodology. Second, we empirically demonstrate that selecting distillation checkpoints based on entropy loss for RL training, rather than validation metrics, yields superior performance-efficiency trade-offs in subsequent RL training. Finally, we develop a two-stage training paradigm to harmonize multi-domain data integration, addressing domain conflicts that arise in training with mixed dataset. We will release the model, dataset, and code.

</details>


### [59] [Reasoning with Exploration: An Entropy Perspective](https://arxiv.org/abs/2506.14758)
*Daixuan Cheng,Shaohan Huang,Xuekai Zhu,Bo Dai,Wayne Xin Zhao,Zhenliang Zhang,Furu Wei*

Main category: cs.CL

TL;DR: 本文提出通过增强强化学习中的熵项来促进语言模型的长链探索性推理，突破现有方法过度偏向利用导致的性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型推理方法过度偏向利用策略，导致性能提升遇到瓶颈。研究发现高熵区域与探索性推理行为(关键令牌/反思行为/罕见行为)存在强相关性，这成为改进的突破口。

Method: 在标准强化学习的优势函数中增加基于熵的项（仅需一行代码修改），通过促进长链推理而非传统方法鼓励的不确定性来增强探索。

Result: 在Pass@K指标上取得显著提升，即使使用极大K值评估仍保持优势，验证了方法对语言模型推理能力的扩展效果。

Conclusion: 通过熵驱动的探索机制重新定义语言模型推理边界，为突破现有强化学习方法在复杂推理任务中的局限性提供了新方向。

Abstract: Balancing exploration and exploitation is a central goal in reinforcement learning (RL). Despite recent advances in enhancing language model (LM) reasoning, most methods lean toward exploitation, and increasingly encounter performance plateaus. In this work, we revisit entropy -- a signal of exploration in RL -- and examine its relationship to exploratory reasoning in LMs. Through empirical analysis, we uncover strong positive correlations between high-entropy regions and three types of exploratory reasoning actions: (1) pivotal tokens that determine or connect logical steps, (2) reflective actions such as self-verification and correction, and (3) rare behaviors under-explored by the base LMs. Motivated by this, we introduce a minimal modification to standard RL with only one line of code: augmenting the advantage function with an entropy-based term. Unlike traditional maximum-entropy methods which encourage exploration by promoting uncertainty, we encourage exploration by promoting longer and deeper reasoning chains. Notably, our method achieves significant gains on the Pass@K metric -- an upper-bound estimator of LM reasoning capabilities -- even when evaluated with extremely large K values, pushing the boundaries of LM reasoning.

</details>


### [60] [From Bytes to Ideas: Language Modeling with Autoregressive U-Nets](https://arxiv.org/abs/2506.14761)
*Mathurin Videau,Badr Youbi Idrissi,Alessandro Leite,Marc Schoenauer,Olivier Teytaud,David Lopez-Paz*

Main category: cs.CL

TL;DR: 提出一种基于自回归U-Net的动态分词模型，通过多尺度处理原始字节实现分层语义建模，替代传统静态方法（如BPE），使模型能自适应不同粒度的预测任务并支持跨语言知识迁移。


<details>
  <summary>Details</summary>
Motivation: 传统分词方法（如BPE）采用固定分词粒度，导致模型预测范围和语义处理能力受限。本文旨在打破这种刚性约束，使分词过程与模型训练动态结合。

Method: 设计自回归U-Net网络直接处理原始字节，通过池化操作分层构建词、词对、四词等多尺度表示。深层网络预测更远未来（如多词级），浅层处理细粒度（如字节级），实现语义分层建模。

Result: 在控制预训练计算量的条件下，浅层结构与BPE基线性能相当，深层结构展现出语义建模潜力。模型可同时处理字符级任务并跨低资源语言迁移知识。

Conclusion: 将分词过程内化为模型的一部分，通过多尺度动态嵌入提升灵活性，为统一处理多粒度任务和跨语言学习提供了新方向。

Abstract: Tokenization imposes a fixed granularity on the input text, freezing how a language model operates on data and how far in the future it predicts. Byte Pair Encoding (BPE) and similar schemes split text once, build a static vocabulary, and leave the model stuck with that choice. We relax this rigidity by introducing an autoregressive U-Net that learns to embed its own tokens as it trains. The network reads raw bytes, pools them into words, then pairs of words, then up to 4 words, giving it a multi-scale view of the sequence. At deeper stages, the model must predict further into the future -- anticipating the next few words rather than the next byte -- so deeper stages focus on broader semantic patterns while earlier stages handle fine details. When carefully tuning and controlling pretraining compute, shallow hierarchies tie strong BPE baselines, and deeper hierarchies have a promising trend. Because tokenization now lives inside the model, the same system can handle character-level tasks and carry knowledge across low-resource languages.

</details>


### [61] [A Variational Framework for Improving Naturalness in Generative Spoken Language Models](https://arxiv.org/abs/2506.14767)
*Li-Wei Chen,Takuya Higuchi,Zakaria Aldeneh,Ahmed Hussen Abdelaziz,Alexander Rudnicky*

Main category: cs.CL

TL;DR: 本文提出一种端到端变分方法，自动编码连续语音属性以增强语义标记，解决现有语音模型因忽略韵律信息导致生成不自然的问题，无需手动特征工程且生成结果更受人类青睐。


<details>
  <summary>Details</summary>
Motivation: 现有基于自监督模型的语音语义标记聚焦语言层面但忽略韵律信息，导致生成语音自然度下降。现有方法通过添加音高特征改进，但音高无法完整表征副语言属性且需人工特征工程。

Method: 采用端到端变分方法（VAE），自动学习编码连续语音属性以增强语义标记，避免手动提取和选择副语言特征。

Result: 所提方法在人类评估中生成更优的语音延续样本，代码、样本及模型已开源。

Conclusion: 通过自动学习副语言属性编码的变分方法，有效提升语音生成自然度，减少人工干预需求，验证了方法的有效性。

Abstract: The success of large language models in text processing has inspired their adaptation to speech modeling. However, since speech is continuous and complex, it is often discretized for autoregressive modeling. Speech tokens derived from self-supervised models (known as semantic tokens) typically focus on the linguistic aspects of speech but neglect prosodic information. As a result, models trained on these tokens can generate speech with reduced naturalness. Existing approaches try to fix this by adding pitch features to the semantic tokens. However, pitch alone cannot fully represent the range of paralinguistic attributes, and selecting the right features requires careful hand-engineering. To overcome this, we propose an end-to-end variational approach that automatically learns to encode these continuous speech attributes to enhance the semantic tokens. Our approach eliminates the need for manual extraction and selection of paralinguistic features. Moreover, it produces preferred speech continuations according to human raters. Code, samples and models are available at https://github.com/b04901014/vae-gslm.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [62] ['Memory States' from Almost Nothing: Representing and Computing in a Non-associative Algebra](https://arxiv.org/abs/2506.13768)
*Stefan Reimann*

Main category: cs.AI

TL;DR: 本文提出了一种非结合代数框架，用于高维信息表示与计算，通过左右结合操作生成两种记忆状态，模拟认知实验中的序列位置效应。


<details>
  <summary>Details</summary>
Motivation: 传统关联捆绑模型需依赖位置标记等辅助结构来保持序列顺序，而本文旨在构建无需辅助的非结合框架，直接保留时序信息并解释认知科学中的记忆现象。

Method: 采用非结合乘法式绑定（binding）和干扰式捆绑（bundling），通过左结合（L-state）和右结合（R-state）操作分别生成强调近因效应和首因效应的稀疏序列表示。

Result: 模型成功复现序列位置曲线（Serial Position Curve），证明L-state支持短期记忆更新，R-state编码长期记忆组块，且噪声被整合为顺序信息的构成要素。

Conclusion: 非结合框架通过双状态机制统一解释记忆的时序特性，其神经机制可能分别对应前额叶皮层与海马体功能，为认知记忆模型提供新计算基础。

Abstract: This note presents a non-associative algebraic framework for the representation and computation of information items in high-dimensional space. This framework is consistent with the principles of spatial computing and with the empirical findings in cognitive science about memory. Computations are performed through a process of multiplication-like binding and non-associative interference-like bundling. Models that rely on associative bundling typically lose order information, which necessitates the use of auxiliary order structures, such as position markers, to represent sequential information that is important for cognitive tasks. In contrast, the non-associative bundling proposed allows the construction of sparse representations of arbitrarily long sequences that maintain their temporal structure across arbitrary lengths. In this operation, noise is a constituent element of the representation of order information, rather than a means of obscuring it. The non-associative nature of the proposed framework results in the representation of a single sequence by two distinct states. The L-state, generated through left-associative bundling, continuously updates and emphasises a recency effect, while the R-state, formed through right-associative bundling, encodes finite sequences or chunks, capturing a primacy effect. The construction of these states may be associated with activity in the prefrontal cortex in relation to short-term memory and hippocampal encoding in long-term memory, respectively. The accuracy of retrieval is contingent upon a decision-making process that is based on the mutual information between the memory states and the cue. The model is able to replicate the Serial Position Curve, which reflects the empirical recency and primacy effects observed in cognitive experiments.

</details>


### [63] [Representing Time-Continuous Behavior of Cyber-Physical Systems in Knowledge Graphs](https://arxiv.org/abs/2506.13773)
*Milapji Singh Gill,Tom Jeleniewski,Felix Gehlhoff,Alexander Fay*

Main category: cs.AI

TL;DR: 本文提出了一种基于标准的模块化模型及高效知识图谱生成方法，用于在知识图谱中直接表示微分方程并与其他CPS生命周期数据结合，验证表明其在航空维护领域具有实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱缺乏可复用的本体工具和方法，导致手动实例化效率低下，难以将微分方程等动态行为信息与CPS全生命周期数据有效集成。

Method: 引入两个核心成果：1) 基于标准的模块化语义模型，支持知识图谱内直接表达微分方程并增强语义；2) 高效知识图谱生成方法以减少人工工作量。

Result: 在航空维护领域验证中，成功将复杂电液伺服作动器的微分方程形式化表示，并与生命周期数据实现上下文关联。

Conclusion: 所提方法能有效实现微分方程在知识图谱中的语义化表达及多源数据整合，证明了其在CPS应用中的实践可行性。

Abstract: Time-continuous dynamic models are essential for various Cyber-Physical System (CPS) applications. To ensure effective usability in different lifecycle phases, such behavioral information in the form of differential equations must be contextualized and integrated with further CPS information. While knowledge graphs provide a formal description and structuring mechanism for this task, there is a lack of reusable ontological artifacts and methods to reduce manual instantiation effort. Hence, this contribution introduces two artifacts: Firstly, a modular semantic model based on standards is introduced to represent differential equations directly within knowledge graphs and to enrich them semantically. Secondly, a method for efficient knowledge graph generation is presented. A validation of these artifacts was conducted in the domain of aviation maintenance. Results show that differential equations of a complex Electro-Hydraulic Servoactuator can be formally represented in a knowledge graph and be contextualized with other lifecycle data, proving the artifacts' practical applicability.

</details>


### [64] [Personalized Constitutionally-Aligned Agentic Superego: Secure AI Behavior Aligned to Diverse Human Values](https://arxiv.org/abs/2506.13774)
*Nell Watson,Ahmed Amer,Evan Harris,Preeti Ravindra,Shujun Zhang*

Main category: cs.AI

TL;DR: 该论文提出了一种名为'superego'的新型AI监督机制，通过动态引用用户选择的规则集（Creed Constitutions）来调整AI行为，显著减少了有害输出并提升了安全性。


<details>
  <summary>Details</summary>
Motivation: 现有AI对齐方法在提供深度个性化上下文信息时容易产生虚构或操作低效，难以满足复杂的安全和合规需求，阻碍了自主AI系统的实际部署。

Method: 设计了一个个性化监督代理（superego agent），动态参考用户选择的规则集（Creed Constitutions），并通过实时合规执行器验证计划。

Result: 在HarmBench和AgentHarm基准测试中，Superego代理显著减少了有害输出（最高降低98.3%的伤害分数），并在主流LLM（如Gemini 2.5 Flash和GPT-4o）上实现了近乎完美的拒绝率。

Conclusion: Superego方法大幅简化了AI个性化对齐，使自主系统更可靠地适应个人和文化背景，同时显著提升了安全性。

Abstract: Agentic AI systems, possessing capabilities for autonomous planning and action, exhibit immense potential across diverse domains. However, their practical deployment is significantly hampered by challenges in aligning their behavior with varied human values, complex safety requirements, and specific compliance needs. Existing alignment methodologies often falter when faced with the intricate task of providing deep, personalized contextual information without inducing confabulation or operational inefficiencies. This paper introduces a novel solution: a 'superego' agent, designed as a personalized oversight mechanism for agentic AI. This system dynamically steers AI planning by referencing user-selected "Creed Constitutions"-encapsulating diverse rule sets-with adjustable adherence levels to fit non-negotiable values. A real-time compliance enforcer validates plans against these constitutions and a universal ethical floor before execution. We present a functional system, including a demonstration interface (www.Creed.Space) with a prototypical constitution-sharing portal, and successful integration with third-party models via the Model Context Protocol (MCP). Comprehensive benchmark evaluations (HarmBench, AgentHarm) demonstrate that our Superego agent dramatically reduces harmful outputs, achieving up to a 98.3% harm score reduction and near-perfect refusal rates (e.g., 100% with Claude Sonnet 4 on AgentHarm's harmful set) for leading LLMs like Gemini 2.5 Flash and GPT-4o. This approach substantially simplifies personalized AI alignment, rendering agentic systems more reliably attuned to individual and cultural contexts, while also enabling substantial safety improvements. An overview on this research with examples is available at https://superego.creed.space.

</details>


### [65] [Recommendations and Reporting Checklist for Rigorous & Transparent Human Baselines in Model Evaluations](https://arxiv.org/abs/2506.13776)
*Kevin L. Wei,Patricia Paskov,Sunishchal Dev,Michael J. Byun,Anka Reuel,Xavier Roberts-Gaal,Rachel Calcott,Evie Coxon,Chinmay Deshpande*

Main category: cs.AI

TL;DR: 本文主张基础模型评估中的人类基线需更严谨透明，以准确比较人机性能，并提出框架、建议及检查清单，通过系统审查现有研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前人类基线方法在AI评估中缺乏严谨性和透明度，导致'超人类'性能声明不可靠，影响机器学习社区、用户及政策制定者对AI性能的解读。

Method: 结合测量理论与AI评估文献的元分析，构建包含设计、执行和报告建议的框架，开发检查清单并系统审查115项基础模型评估研究。

Result: 现有基线方法存在系统性缺陷，检查清单能有效识别问题并指导研究者改进基线设计与结果报告。数据已开源供验证。

Conclusion: 提出的框架和检查清单可推动更严谨的AI评估实践，为研究社区和政策制定者提供可靠基准，促进人机性能对比的科学性。

Abstract: In this position paper, we argue that human baselines in foundation model evaluations must be more rigorous and more transparent to enable meaningful comparisons of human vs. AI performance, and we provide recommendations and a reporting checklist towards this end. Human performance baselines are vital for the machine learning community, downstream users, and policymakers to interpret AI evaluations. Models are often claimed to achieve "super-human" performance, but existing baselining methods are neither sufficiently rigorous nor sufficiently well-documented to robustly measure and assess performance differences. Based on a meta-review of the measurement theory and AI evaluation literatures, we derive a framework with recommendations for designing, executing, and reporting human baselines. We synthesize our recommendations into a checklist that we use to systematically review 115 human baselines (studies) in foundation model evaluations and thus identify shortcomings in existing baselining methods; our checklist can also assist researchers in conducting human baselines and reporting results. We hope our work can advance more rigorous AI evaluation practices that can better serve both the research community and policymakers. Data is available at: https://github.com/kevinlwei/human-baselines

</details>


### [66] [The NordDRG AI Benchmark for Large Language Models](https://arxiv.org/abs/2506.13790)
*Tapio Pitkäranta*

Main category: cs.AI

TL;DR: 研究团队发布了首个针对医院资金分配层（DRG）的公开测试平台NordDRG-AI-Benchmark，用于评估大语言模型在临床编码与多语言诊断推理中的表现，结果显示不同模型在领域特定任务中差异显著。


<details>
  <summary>Details</summary>
Motivation: 现有公开基准未覆盖医院资金分配核心层（DRG），而DRG作为多国医疗报销的核心规则，亟需专门测试平台评估LLM在真实临床编码与决策支持中的能力。

Method: 构建包含三类构件的基准：20个互关联表格组成的DRG逻辑定义库、专家手册与变更日志模板、14个覆盖代码查询/跨表推理/多语言术语的临床任务提示集，并对5个前沿LLM进行基线测试。

Result: 五大LLM表现分化明显：o3模型在9项可验证任务中全胜，GPT-4o与o4-mini-high完成7项，Gemini 2.5 Pro和Flash仅完成5项与3项，证明该基准能有效揭示通用测试未覆盖的领域特异性缺陷。

Conclusion: NordDRG-AI-Benchmark填补了DRG层自动化评估空白，其可复现基线为医院资金可信自动化研究提供领域专属测试框架，凸显LLM在专业医疗场景中的差异化能力边界。

Abstract: Large language models (LLMs) are already being piloted for clinical coding and decision support. However, until now, no open benchmark has targeted the hospital funding layer where Diagnosis-Related Groups (DRG) determine reimbursement across many countries. We release NordDRG-AI-Benchmark, the first public test-bed that captures a complete DRG rule set and evaluates an LLM's ability to reason over multilingual diagnosis, procedure, and tariff logic.
  The benchmark bundles three classes of artefacts: (i) definition tables with 20 interlinked tables covering DRG logic, ICD and NCSP codes, age/sex splits, and country flags; (ii) expert manuals and changelog templates describing real governance workflows; and (iii) a prompt pack of 14 CaseMix tasks that span code lookup, cross-table inference, multilingual terminology, and quality-assurance audits.
  All artefacts are available at: https://github.com/longshoreforrest/norddrg-ai-benchmark
  A baseline demonstration shows that five state-of-the-art LLMs perform very differently on the nine automatically verifiable tasks: o3 (OpenAI) scores 9 out of 9, GPT-4o and o4-mini-high score 7 out of 9, while Gemini 2.5 Pro and Gemini 2.5 Flash solve only 5 out of 9 and 3 out of 9, respectively. These results confirm that NordDRG-AI-Benchmark highlights domain-specific strengths and weaknesses that remain hidden in generic LLM benchmarks, offering a reproducible baseline for research on trustworthy automation in hospital funding.

</details>


### [67] [ICE-ID: A Novel Historical Census Data Benchmark Comparing NARS against LLMs, \& a ML Ensemble on Longitudinal Identity Resolution](https://arxiv.org/abs/2506.13792)
*Gonçalo Hora de Carvalho,Lazar S. Popov,Sander Kaatee,Kristinn R. Thórisson,Tangrui Li,Pétur Húni Björnsson,Jilles S. Dibangoye*

Main category: cs.AI

TL;DR: 本文提出ICE-ID数据集，用于历史身份解析研究，覆盖冰岛220年人口普查记录，并评估多种方法，发现NARS方法表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决长期人物实体匹配在真实世界数据中的挑战（如姓名变化、人口统计变化），填补现有大规模开放表格数据集的空白。

Method: 构建ICE-ID数据集，定义身份解析任务，评估规则匹配、ML集成、LLM及新型非公理推理系统NARS（基于术语逻辑的通用AI框架）。

Result: NARS方法简单且达到SOTA性能，优于传统方法。数据集与代码开源，支持跨学科数据链接与历史分析研究。

Conclusion: ICE-ID为纵向身份解析提供可复现基准，NARS验证了非公理逻辑在结构化数据任务中的潜力，推动历史数据分析方法创新。

Abstract: We introduce ICE-ID, a novel benchmark dataset for historical identity resolution, comprising 220 years (1703-1920) of Icelandic census records. ICE-ID spans multiple generations of longitudinal data, capturing name variations, demographic changes, and rich genealogical links. To the best of our knowledge, this is the first large-scale, open tabular dataset specifically designed to study long-term person-entity matching in a real-world population. We define identity resolution tasks (within and across census waves) with clearly documented metrics and splits. We evaluate a range of methods: handcrafted rule-based matchers, a ML ensemble as well as LLMs for structured data (e.g. transformer-based tabular networks) against a novel approach to tabular data called NARS (Non-Axiomatic Reasoning System) - a general-purpose AI framework designed to reason with limited knowledge and resources. Its core is Non-Axiomatic Logic (NAL), a term-based logic. Our experiments show that NARS is suprisingly simple and competitive with other standard approaches, achieving SOTA at our task. By releasing ICE-ID and our code, we enable reproducible benchmarking of identity resolution approaches in longitudinal settings and hope that ICE-ID opens new avenues for cross-disciplinary research in data linkage and historical analytics.

</details>


### [68] [Med-REFL: Medical Reasoning Enhancement via Self-Corrected Fine-grained Reflection](https://arxiv.org/abs/2506.13793)
*Zongxian Yang,Jiayu Qian,Zegao Peng,Haoyu Zhang,Zhi-An Huang*

Main category: cs.AI

TL;DR: 本文提出Med-REFL方法，通过细粒度反思增强医学推理模型，自动构建优化数据以减少专家标注依赖，在MedQA-USMLE基准上实现最高4.11%的性能提升，并显著提升7B/8B模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型在医学领域表现滞后，核心问题在于高风险医疗场景中中间反思步骤质量不足，需通过精细化过程提升推理可靠性。

Method: 采用树状思维分解医学问题为细粒度推理路径，定量评估各步骤及其反思结果，自动生成直接偏好优化数据以引导模型识别和修正错误。

Result: Med-REFL在MedQA-USMLE基准实现平均4.11%提升，使7B/8B模型SOTA性能再提升4.13%，并在多数据集展现强泛化与鲁棒性。

Conclusion: 聚焦反思质量可显著提升医学AI推理的准确性与可信度，该方法为自动优化医疗推理模型提供了有效路径。

Abstract: Large reasoning models have recently made significant strides in mathematical and code reasoning, yet their success has not transferred smoothly to the medical domain. While multiple factors contribute to this disparity, a critical issue is the inadequate focus on the quality of intermediate reflection steps, which is particularly crucial in high-stakes medical scenarios. To address this challenge, we propose Med-REFL, a \underline{\textbf{Med}}ical \underline{\textbf{R}}easoning \underline{\textbf{E}}nhancement via self-corrected \underline{\textbf{F}}ine-grained ref\underline{\textbf{L}}ection. Our method leverages a tree-of-thought approach to decompose medical questions into fine-grained reasoning paths, quantitatively evaluating each step and its subsequent reflections. These assessments enable automatic construction of direct preference optimization data, reducing reliance on expensive expert annotations while guiding models to identify and correct reasoning errors. Experimental results on the MedQA-USMLE benchmark demonstrate Med-REFL achieves consistent improvements, with average gains up to 4.11\%. Notably, it further boosts the state-of-the-art performance of 7B/8B models by an additional 4.13\%. Furthermore, Med-REFL exhibits strong generalization capabilities and robustness across several challenging medical question-answering datasets. Our work illustrates that prioritizing reflection quality leads to more accurate and trustworthy reasoning in medical AI applications. Checkpoints, code, and data can be found \href{https://github.com/TianYin123/Med-REFL}{here}.

</details>


### [69] [BotTrans: A Multi-Source Graph Domain Adaptation Approach for Social Bot Detection](https://arxiv.org/abs/2506.13795)
*Boshen Shi,Yongqing Wang,Fangda Guo,Jiangli Shao,Huawei Shen,Xueqi Cheng*

Main category: cs.AI

TL;DR: 提出多源图域适应模型BotTrans，通过跨源域拓扑增强同质性、聚合跨域邻居信息及优化源-目标相关性，解决社交机器人检测中的标签稀缺与网络异质性问题，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的社交机器人检测面临标签稀缺问题，且单源迁移因网络异质性和源域相关性不足导致效果不稳定。需探索多源迁移以提升知识利用效率。

Method: BotTrans模型：1) 利用多源标签知识构建高同质性跨源域拓扑；2) 聚合跨域邻居信息增强节点嵌入判别性；3) 结合源-目标相关性优化知识迁移；4) 引入目标域语义知识优化检测性能。

Result: 在真实数据集上的实验表明，BotTrans在无标签目标检测任务中显著优于现有方法，验证了多源知识迁移的有效性。

Conclusion: BotTrans通过多源域适应策略有效缓解网络异质性和单源局限性，为无标签场景下的社交机器人检测提供了高效解决方案。

Abstract: Transferring extensive knowledge from relevant social networks has emerged as a promising solution to overcome label scarcity in detecting social bots and other anomalies with GNN-based models. However, effective transfer faces two critical challenges. Firstly, the network heterophily problem, which is caused by bots hiding malicious behaviors via indiscriminately interacting with human users, hinders the model's ability to learn sufficient and accurate bot-related knowledge from source domains. Secondly, single-source transfer might lead to inferior and unstable results, as the source network may embody weak relevance to the task and provide limited knowledge. To address these challenges, we explore multiple source domains and propose a multi-source graph domain adaptation model named \textit{BotTrans}. We initially leverage the labeling knowledge shared across multiple source networks to establish a cross-source-domain topology with increased network homophily. We then aggregate cross-domain neighbor information to enhance the discriminability of source node embeddings. Subsequently, we integrate the relevance between each source-target pair with model optimization, which facilitates knowledge transfer from source networks that are more relevant to the detection task. Additionally, we propose a refinement strategy to improve detection performance by utilizing semantic knowledge within the target domain. Extensive experiments on real-world datasets demonstrate that \textit{BotTrans} outperforms the existing state-of-the-art methods, revealing its efficacy in leveraging multi-source knowledge when the target detection task is unlabeled.

</details>


### [70] [Feedforward Ordering in Neural Connectomes via Feedback Arc Minimization](https://arxiv.org/abs/2506.13799)
*Soroush Vahidi*

Main category: cs.AI

TL;DR: 提出可扩展算法套件，用于最小化大规模加权有向图的反馈弧，以揭示神经连接组中的生物前馈结构，并在FlyWire数据集上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 通过优化反馈弧最小化问题，揭示神经连接组中具有生物学意义的定向前馈结构，增强对神经网络功能的理解。

Method: 结合贪心启发式算法、增益感知局部优化策略，以及基于强连通组件的全局结构分析，开发高效Python实现并通过Google Colab Pro+云端验证。

Result: 最佳解决方案在FlyWire数据集上显著提升前向边权重，超越现有最优方法，算法执行效率与可扩展性得到验证。

Conclusion: 所提方法在生物神经连接组分析中具有实际应用价值，为大规模图结构优化提供了有效工具链与验证框架。

Abstract: We present a suite of scalable algorithms for minimizing feedback arcs in large-scale weighted directed graphs, with the goal of revealing biologically meaningful feedforward structure in neural connectomes. Using the FlyWire Connectome Challenge dataset, we demonstrate the effectiveness of our ranking strategies in maximizing the total weight of forward-pointing edges. Our methods integrate greedy heuristics, gain-aware local refinements, and global structural analysis based on strongly connected components. Experiments show that our best solution improves the forward edge weight over previous top-performing methods. All algorithms are implemented efficiently in Python and validated using cloud-based execution on Google Colab Pro+.

</details>


### [71] [Causality in the human niche: lessons for machine learning](https://arxiv.org/abs/2506.13803)
*Richard D. Lange,Konrad P. Kording*

Main category: cs.AI

TL;DR: 本文探讨如何将人类因果认知的优势整合到机器学习中，以弥补现有结构因果模型（SCM）的不足，从而提升AI的泛化能力、可控性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习系统在泛化和新领域高效学习方面表现较弱，而人类因果认知为此提供了潜在解决方案。SCM框架虽在形式化因果关系上取得进展，但未能完全捕捉人类因果推理的关键特性（如类比泛化），限制了其在复杂现实场景中的应用。

Method: 通过理论分析比较SCM框架与人类因果认知的差异，结合认知科学视角，提出人类因果能力在特定环境（社会性、目标驱动）中的适应性特征，并论证这些特征对机器学习系统设计的意义。

Result: 揭示SCM框架在表达类型化对象因果推理、类比迁移等人类核心能力上的局限性，指出结合人类认知的归纳偏置（如对象类型相似性推理）是改进机器学习因果建模的关键路径。

Conclusion: 未来研究应更深入理解人类因果认知的适应性机制，将其转化为机器学习系统的归纳偏置，以构建更接近人类水平的因果推理能力，推动AI在复杂现实任务中的表现。

Abstract: Humans interpret the world around them in terms of cause and effect and communicate their understanding of the world to each other in causal terms. These causal aspects of human cognition are thought to underlie humans' ability to generalize and learn efficiently in new domains, an area where current machine learning systems are weak. Building human-like causal competency into machine learning systems may facilitate the construction of effective and interpretable AI. Indeed, the machine learning community has been importing ideas on causality formalized by the Structural Causal Model (SCM) framework, which provides a rigorous formal language for many aspects of causality and has led to significant advances. However, the SCM framework fails to capture some salient aspects of human causal cognition and has likewise not yet led to advances in machine learning in certain critical areas where humans excel. We contend that the problem of causality in the ``human niche'' -- for a social, autonomous, and goal-driven agent sensing and acting in the world in which humans live -- is quite different from the kind of causality captured by SCMs. For example, everyday objects come in similar types that have similar causal properties, and so humans readily generalize knowledge of one type of object (cups) to another related type (bowls) by drawing causal analogies between objects with similar properties, but such analogies are at best awkward to express in SCMs. We explore how such causal capabilities are adaptive in, and motivated by, the human niche. By better appreciating properties of human causal cognition and, crucially, how those properties are adaptive in the niche in which humans live, we hope that future work at the intersection of machine learning and causality will leverage more human-like inductive biases to create more capable, controllable, and interpretable systems.

</details>


### [72] [Bridging Pattern-Aware Complexity with NP-Hard Optimization: A Unifying Framework and Empirical Study](https://arxiv.org/abs/2506.13810)
*Olivier Saidi*

Main category: cs.AI

TL;DR: 提出一种模式感知复杂度框架，利用实际问题中的结构规律（如聚类、对称性）降低计算复杂度，在TSP基准测试中实现最高79%的解决方案质量提升。


<details>
  <summary>Details</summary>
Motivation: 尽管NP难优化问题（如TSP）在理论上难以高效求解，但现实场景中常存在可挖掘的结构规律。现有理论未充分结合这些模式，导致计算效率受限。

Method: 构建模式感知的复杂度框架，通过严格定义、定理及元学习驱动的求解流程，量化并利用结构规律（如聚类、对称性），引入模式利用效率（PUE）等指标。

Result: 在TSP基准测试（22-2392个城市）中实现最高79%的解决方案质量提升，并在金融预测、大语言模型优化等领域验证框架有效性。

Conclusion: 突破传统NP难理论限制，提出统一实用框架，通过模式驱动显著提升实际问题的计算效率，为跨领域复杂优化提供新视角。

Abstract: NP hard optimization problems like the Traveling Salesman Problem (TSP) defy efficient solutions in the worst case, yet real-world instances often exhibit exploitable patterns. We propose a novel patternaware complexity framework that quantifies and leverages structural regularities e.g., clustering, symmetry to reduce effective computational complexity across domains, including financial forecasting and LLM optimization. With rigorous definitions, theorems, and a meta learning driven solver pipeline, we introduce metrics like Pattern Utilization Efficiency (PUE) and achieve up to 79 percent solution quality gains in TSP benchmarks (22 to 2392 cities). Distinct from theoretical NP hardness, our approach offers a unified, practical lens for pattern-driven efficiency.

</details>


### [73] [The Reflexive Integrated Information Unit: A Differentiable Primitive for Artificial Consciousness](https://arxiv.org/abs/2506.13825)
*Gnankan Landry Regis N'guessan,Issa Karambal*

Main category: cs.AI

TL;DR: 本文提出反射性整合信息单元（RIIU），一种可微循环单元，通过元状态和广播缓冲区实现局部信息整合，将意识计算缩小至单元级别，推动人工意识研究从哲学争论转向数学实证。


<details>
  <summary>Details</summary>
Motivation: 人工意识研究缺乏类似感知机的可复制、可迭代的小型模块。现有模型难以量化评估和改进意识相关计算。

Method: 设计RIIU循环单元：1）元状态μ记录单元因果足迹；2）广播缓冲区B传递足迹；3）滑动窗口协方差和可微Auto-Φ替代实现在线信息整合最大化。

Result: 在八向网格世界中，四层RIIU代理在13步内恢复>90%奖励（速度达GRU两倍），同时保持非零Auto-Φ信号，验证Φ单调塑性特性。

Conclusion: RIIU通过单元级意识计算，将哲学问题转化为可实证的数学问题，为人工意识研究提供可扩展的基准模块。

Abstract: Research on artificial consciousness lacks the equivalent of the perceptron: a small, trainable module that can be copied, benchmarked, and iteratively improved. We introduce the Reflexive Integrated Information Unit (RIIU), a recurrent cell that augments its hidden state $h$ with two additional vectors: (i) a meta-state $μ$ that records the cell's own causal footprint, and (ii) a broadcast buffer $B$ that exposes that footprint to the rest of the network. A sliding-window covariance and a differentiable Auto-$Φ$ surrogate let each RIIU maximize local information integration online. We prove that RIIUs (1) are end-to-end differentiable, (2) compose additively, and (3) perform $Φ$-monotone plasticity under gradient ascent. In an eight-way Grid-world, a four-layer RIIU agent restores $>90\%$ reward within 13 steps after actuator failure, twice as fast as a parameter-matched GRU, while maintaining a non-zero Auto-$Φ$ signal. By shrinking "consciousness-like" computation down to unit scale, RIIUs turn a philosophical debate into an empirical mathematical problem.

</details>


### [74] [LocationReasoner: Evaluating LLMs on Real-World Site Selection Reasoning](https://arxiv.org/abs/2506.13841)
*Miho Koda,Yu Zheng,Ruixian Ma,Mingyang Sun,Devesh Pansare,Fabio Duarte,Paolo Santi*

Main category: cs.AI

TL;DR: 本文提出了LocationReasoner基准测试，用于检验大语言模型在复杂现实场景（如选址决策）中的推理能力。研究发现，现有先进模型在真实场景中表现有限，代理策略存在过度推理问题，并开源基准以促进发展。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的推理能力评估集中于数学解题和代码生成领域，但其在复杂现实场景（如多约束条件选址）中的泛化能力尚未被充分探索。

Method: 构建包含300+多难度查询的LocationReasoner基准，结合沙盒环境与定制工具，评估模型在空间、环境、物流等多约束条件下的选址推理能力。

Result: 实验表明：1) 先进推理模型（如OpenAI o4）在30%任务中失败；2) ReAct/Reflexion等代理策略因过度推理导致性能下降；3) 直接代码生成策略优于复杂推理策略。

Conclusion: 大语言模型在整体性和非线性推理方面存在显著缺陷，LocationReasoner基准的发布旨在推动面向现实决策任务的可靠推理模型发展。

Abstract: Recent advances in large language models (LLMs), particularly those enhanced through reinforced post-training, have demonstrated impressive reasoning capabilities, as exemplified by models such as OpenAI o1 and DeepSeek-R1. However, these capabilities are predominantly benchmarked on domains like mathematical problem solving and code generation -- leaving open the question of whether such reasoning skills generalize to complex, real-world scenarios. In this paper, we introduce LocationReasoner, a benchmark designed to evaluate LLMs' reasoning abilities in the context of real-world site selection, where models must identify feasible locations by reasoning over diverse and complicated spatial, environmental, and logistical constraints. The benchmark comprises over 300 carefully crafted queries of varying difficulty levels, supported by a sandbox environment with in-house tools for constraint-based location search. Extensive evaluations reveal that state-of-the-art reasoning models offer limited improvement over their non-reasoning predecessors in real-world contexts, with even the latest OpenAI o4 model failing on 30% of site selection tasks. Moreover, agentic strategies such as ReAct and Reflexion often suffer from over-reasoning, leading to worse outcomes than direct code-generation prompting. With key limitations of LLMs in holistic and non-linear reasoning highlighted, we release LocationReasoner to foster the development of LLMs and agents capable of robust, grounded reasoning in real-world decision-making tasks. Codes and data for our benchmark are available at https://github.com/miho-koda/LocationReasoner.

</details>


### [75] [Evaluating Explainability: A Framework for Systematic Assessment and Reporting of Explainable AI Features](https://arxiv.org/abs/2506.13917)
*Miguel A. Lago,Ghada Zamzmi,Brandon Eich,Jana G. Delfino*

Main category: cs.AI

TL;DR: 本文提出一个评估AI可解释性质量的框架，包含一致性、合理性、忠实性、实用性四个标准，并通过乳腺X光片检测案例验证，旨在促进医疗AI设备的开发与评估。


<details>
  <summary>Details</summary>
Motivation: 现有AI可解释性功能缺乏系统化的质量评估方法，需建立标准化评估框架以提升解释机制的可靠性和临床应用价值。

Method: 提出基于一致性（输入相似性下的解释稳定性）、合理性（与真实情况接近度）、忠实性（与模型内部机制对齐度）、实用性（对任务性能影响）的四维评估框架，并开发配套的评分卡系统。

Result: 以Ablation CAM和Eigen CAM在合成乳腺X光片病灶检测中的热力图解释为案例，验证前三项标准在临床场景中的适用性。

Conclusion: 该框架为AI解释质量评估提供标准化路径，可推动可解释性功能的价值讨论，并促进医疗AI设备的优化与监管。

Abstract: Explainability features are intended to provide insight into the internal mechanisms of an AI device, but there is a lack of evaluation techniques for assessing the quality of provided explanations. We propose a framework to assess and report explainable AI features. Our evaluation framework for AI explainability is based on four criteria: 1) Consistency quantifies the variability of explanations to similar inputs, 2) Plausibility estimates how close the explanation is to the ground truth, 3) Fidelity assesses the alignment between the explanation and the model internal mechanisms, and 4) Usefulness evaluates the impact on task performance of the explanation. Finally, we developed a scorecard for AI explainability methods that serves as a complete description and evaluation to accompany this type of algorithm. We describe these four criteria and give examples on how they can be evaluated. As a case study, we use Ablation CAM and Eigen CAM to illustrate the evaluation of explanation heatmaps on the detection of breast lesions on synthetic mammographies. The first three criteria are evaluated for clinically-relevant scenarios. Our proposed framework establishes criteria through which the quality of explanations provided by AI models can be evaluated. We intend for our framework to spark a dialogue regarding the value provided by explainability features and help improve the development and evaluation of AI-based medical devices.

</details>


### [76] [Integrating Knowledge Graphs and Bayesian Networks: A Hybrid Approach for Explainable Disease Risk Prediction](https://arxiv.org/abs/2506.13920)
*Mbithe Nzomo,Deshendran Moodley*

Main category: cs.AI

TL;DR: 提出一种结合知识图谱和贝叶斯网络的多模态电子健康记录分析方法，通过房颤案例验证其在可解释疾病风险预测中的有效性。


<details>
  <summary>Details</summary>
Motivation: 通用医学知识需适配具体医疗场景和患者群体，且需解决数据不完整、健康结果非确定性及系统可解释性等临床实践挑战。

Method: 基于本体论知识图谱与多模态EHR数据构建贝叶斯网络，实现可解释的疾病风险预测框架。

Result: 房颤预测案例显示该方法平衡通用知识与患者特异性，有效处理不确定性，预测性能优异且具备高可解释性。

Conclusion: 知识图谱与贝叶斯网络融合方法能兼顾医学普适性与个体化需求，为临床决策提供可靠支持。

Abstract: Multimodal electronic health record (EHR) data is useful for disease risk prediction based on medical domain knowledge. However, general medical knowledge must be adapted to specific healthcare settings and patient populations to achieve practical clinical use. Additionally, risk prediction systems must handle uncertainty from incomplete data and non-deterministic health outcomes while remaining explainable. These challenges can be alleviated by the integration of knowledge graphs (KGs) and Bayesian networks (BNs). We present a novel approach for constructing BNs from ontology-based KGs and multimodal EHR data for explainable disease risk prediction. Through an application use case of atrial fibrillation and real-world EHR data, we demonstrate that the approach balances generalised medical knowledge with patient-specific context, effectively handles uncertainty, is highly explainable, and achieves good predictive performance.

</details>


### [77] [ProfiLLM: An LLM-Based Framework for Implicit Profiling of Chatbot Users](https://arxiv.org/abs/2506.13980)
*Shahaf David,Yair Meidan,Ido Hersko,Daniel Varnovitzky,Dudu Mimran,Yuval Elovici,Asaf Shabtai*

Main category: cs.AI

TL;DR: 本文提出ProfiLLM框架，通过动态隐式用户画像提升LLM聊天机器人在IT/安全等专业领域的个性化响应能力，实验显示其能快速准确推断用户技术熟练度。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的聊天机器人难以根据用户技术背景、学习风格等特征动态调整响应，尤其在IT/安全等知识密集型领域，静态分类和显式用户报告机制适应性不足。

Method: 提出ProfiLLM框架：包含可跨领域适配的分类法及基于LLM的隐式动态用户建模方法。开发ITSec领域变体ProfiLLM[ITSec]，利用263个合成用户的1760条对话进行验证。

Result: 单次提示后实际与预测分数差距减少55-65%，后续波动微小且持续优化。提供LLM角色模拟方法、ITSec技能分类法、代码库及对话数据集。

Conclusion: ProfiLLM有效实现动态用户画像，为专业领域个性化对话系统奠定基础。贡献包括新型框架、评估方法、结构化分类体系及开源资源支持后续研究。

Abstract: Despite significant advancements in conversational AI, large language model (LLM)-powered chatbots often struggle with personalizing their responses according to individual user characteristics, such as technical expertise, learning style, and communication preferences. This lack of personalization is particularly problematic in specialized knowledge-intense domains like IT/cybersecurity (ITSec), where user knowledge levels vary widely. Existing approaches for chatbot personalization primarily rely on static user categories or explicit self-reported information, limiting their adaptability to an evolving perception of the user's proficiency, obtained in the course of ongoing interactions. In this paper, we propose ProfiLLM, a novel framework for implicit and dynamic user profiling through chatbot interactions. This framework consists of a taxonomy that can be adapted for use in diverse domains and an LLM-based method for user profiling in terms of the taxonomy. To demonstrate ProfiLLM's effectiveness, we apply it in the ITSec domain where troubleshooting interactions are used to infer chatbot users' technical proficiency. Specifically, we developed ProfiLLM[ITSec], an ITSec-adapted variant of ProfiLLM, and evaluated its performance on 1,760 human-like chatbot conversations from 263 synthetic users. Results show that ProfiLLM[ITSec] rapidly and accurately infers ITSec profiles, reducing the gap between actual and predicted scores by up to 55--65\% after a single prompt, followed by minor fluctuations and further refinement. In addition to evaluating our new implicit and dynamic profiling framework, we also propose an LLM-based persona simulation methodology, a structured taxonomy for ITSec proficiency, our codebase, and a dataset of chatbot interactions to support future research.

</details>


### [78] [SANGAM: SystemVerilog Assertion Generation via Monte Carlo Tree Self-Refine](https://arxiv.org/abs/2506.13983)
*Adarsh Gupta,Bhabesh Mali,Chandan Karfa*

Main category: cs.AI

TL;DR: 本文提出SANGAM框架，利用LLM引导的蒙特卡洛树搜索自动生成SystemVerilog断言（SVA），通过多模态规范处理、MCTSr算法推理及断言整合三阶段，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型（LLM）在推理领域的进展，探索更复杂、自动化的硬件断言生成技术，以解决工业级规范到SVA转换的自动化需求。

Method: 三阶段框架：1) 多模态规范处理（信号映射、规范分析、波形分析）；2) MCTSr算法对信号进行自动SVA推理；3) 整合推理轨迹生成各信号的SVA断言。

Result: SANGAM生成的SVA断言集在评估中表现更优，鲁棒性显著高于现有方法。

Conclusion: SANGAM框架通过结合LLM与蒙特卡洛树搜索，实现了高效、自动化的工业级硬件断言生成，验证了其技术优势与应用潜力。

Abstract: Recent advancements in the field of reasoning using Large Language Models (LLMs) have created new possibilities for more complex and automatic Hardware Assertion Generation techniques. This paper introduces SANGAM, a SystemVerilog Assertion Generation framework using LLM-guided Monte Carlo Tree Search for the automatic generation of SVAs from industry-level specifications. The proposed framework utilizes a three-stage approach: Stage 1 consists of multi-modal Specification Processing using Signal Mapper, SPEC Analyzer, and Waveform Analyzer LLM Agents. Stage 2 consists of using the Monte Carlo Tree Self-Refine (MCTSr) algorithm for automatic reasoning about SVAs for each signal, and finally, Stage 3 combines the MCTSr-generated reasoning traces to generate SVA assertions for each signal. The results demonstrated that our framework, SANGAM, can generate a robust set of SVAs, performing better in the evaluation process in comparison to the recent methods.

</details>


### [79] [Machine Mirages: Defining the Undefined](https://arxiv.org/abs/2506.13990)
*Hamidou Tembine*

Main category: cs.AI

TL;DR: 论文探讨了多模态机器学习系统在达到动物或人类水平流畅度时出现的新型认知异常——机器幻象，并强调需明确定义和系统评估这些错误。


<details>
  <summary>Details</summary>
Motivation: 随着多模态机器学习系统在图像、语言和声音处理任务中达到动物或人类水平流畅度，它们开始表现出新的认知异常，这些错误需要被理解和评估以提高系统可靠性和构建伦理智能生态系统。

Method: 论文列举并分类了多种机器幻象错误，提出需要对这些错误进行明确定义和系统评估。

Result: 论文识别了多种机器幻象错误，如幻觉、语义漂移、因果推理失败等，并强调理解这些错误对改进机器智能可靠性的重要性。

Conclusion: 理解机器幻象不仅对提升机器智能可靠性至关重要，也是构建尊重生命、认知和表达多样性的伦理智能生态系统的基础。

Abstract: As multimodal machine intelligence systems started achieving average animal-level and average human-level fluency in many measurable tasks in processing images, language, and sound, they began to exhibit a new class of cognitive aberrations: machine mirages. These include delusion, illusion, confabulation, hallucination, misattribution error, semantic drift, semantic compression, exaggeration, causal inference failure, uncanny valley of perception, bluffing-patter-bullshitting, cognitive stereotypy, pragmatic misunderstanding, hypersignification, semantic reheating-warming, simulated authority effect, fallacious abductive leap, contextual drift, referential hallucination, semiotic Frankenstein effect, calibration failure, spurious correlation, bias amplification, concept drift sensitivity, misclassification under uncertainty, adversarial vulnerability, overfitting, prosodic misclassification, accent bias, turn boundary failure, semantic boundary confusion, noise overfitting, latency-induced decision drift, ambiguity collapse and other forms of error that mimic but do not replicate human or animal fallibility. This article presents some of the errors and argues that these failures must be explicitly defined and systematically assessed. Understanding machine mirages is essential not only for improving machine intelligence reliability but also for constructing a multiscale ethical, co-evolving intelligence ecosystem that respects the diverse forms of life, cognition, and expression it will inevitably touch.

</details>


### [80] [Discovering Temporal Structure: An Overview of Hierarchical Reinforcement Learning](https://arxiv.org/abs/2506.14045)
*Martin Klissarov,Akhil Bagaria,Ziyan Luo,George Konidaris,Doina Precup,Marlos C. Machado*

Main category: cs.AI

TL;DR: 该论文探讨分层强化学习（HRL）在复杂开放环境中通过发现和利用时间结构提升AI代理性能的潜力，分析其优势、方法及挑战，并强调其在特定领域的适用性。


<details>
  <summary>Details</summary>
Motivation: 现有HRL研究虽丰富，但缺乏对‘良好时间结构’的明确定义及其适用场景的清晰界定。论文旨在从决策挑战角度明确HRL的价值，并探索其对AI代理的效能影响。

Method: 通过分析HRL对决策问题的益处，系统梳理从在线经验学习、离线数据集利用到大型语言模型（LLM）结合的时间结构发现方法。

Result: 揭示了HRL在优化AI代理性能权衡中的作用，并总结不同方法（在线/离线学习、LLM）的适用性，同时指出时间结构发现的挑战与适用领域。

Conclusion: HRL的时间结构发现是提升复杂环境决策的关键，但其定义与适用性仍需进一步研究，特定领域（如长期规划任务）可能从中显著受益。

Abstract: Developing agents capable of exploring, planning and learning in complex open-ended environments is a grand challenge in artificial intelligence (AI). Hierarchical reinforcement learning (HRL) offers a promising solution to this challenge by discovering and exploiting the temporal structure within a stream of experience. The strong appeal of the HRL framework has led to a rich and diverse body of literature attempting to discover a useful structure. However, it is still not clear how one might define what constitutes good structure in the first place, or the kind of problems in which identifying it may be helpful. This work aims to identify the benefits of HRL from the perspective of the fundamental challenges in decision-making, as well as highlight its impact on the performance trade-offs of AI agents. Through these benefits, we then cover the families of methods that discover temporal structure in HRL, ranging from learning directly from online experience to offline datasets, to leveraging large language models (LLMs). Finally, we highlight the challenges of temporal structure discovery and the domains that are particularly well-suited for such endeavours.

</details>


### [81] [Into the Unknown: Applying Inductive Spatial-Semantic Location Embeddings for Predicting Individuals' Mobility Beyond Visited Places](https://arxiv.org/abs/2506.14070)
*Xinglei Wang,Tao Cheng,Stephen Law,Zichao Zeng,Ilya Ilyankou,Junyuan Liu,Lu Yin,Weiming Huang,Natchapon Jongwiriyanurak*

Main category: cs.AI

TL;DR: 本文提出多模态对比学习框架CaLLiPer，通过融合空间坐标和语义特征生成位置嵌入，提升人类移动预测性能，尤其在处理新出现位置时表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统基于历史移动模式的位置嵌入方法存在无法编码显式空间信息、整合城市语义背景及适应未知位置的局限性，需改进以提升预测能力。

Method: 采用CaLLiPer框架，通过对比学习融合POI的空间坐标与语义特征，生成具有空间显式、语义丰富且可归纳特性的位置嵌入。

Result: 在四个公共数据集上的实验表明，CaLLiPer在常规和归纳场景下均优于基线模型，尤其在涉及新位置的场景中表现突出。

Conclusion: 多模态归纳式位置嵌入可显著增强人类移动预测系统的能力，公开的代码与数据将促进相关研究的复现与扩展。

Abstract: Predicting individuals' next locations is a core task in human mobility modelling, with wide-ranging implications for urban planning, transportation, public policy and personalised mobility services. Traditional approaches largely depend on location embeddings learned from historical mobility patterns, limiting their ability to encode explicit spatial information, integrate rich urban semantic context, and accommodate previously unseen locations. To address these challenges, we explore the application of CaLLiPer -- a multimodal representation learning framework that fuses spatial coordinates and semantic features of points of interest through contrastive learning -- for location embedding in individual mobility prediction. CaLLiPer's embeddings are spatially explicit, semantically enriched, and inductive by design, enabling robust prediction performance even in scenarios involving emerging locations. Through extensive experiments on four public mobility datasets under both conventional and inductive settings, we demonstrate that CaLLiPer consistently outperforms strong baselines, particularly excelling in inductive scenarios. Our findings highlight the potential of multimodal, inductive location embeddings to advance the capabilities of human mobility prediction systems. We also release the code and data (https://github.com/xlwang233/Into-the-Unknown) to foster reproducibility and future research.

</details>


### [82] [FormGym: Doing Paperwork with Agents](https://arxiv.org/abs/2506.14079)
*Matthew Toles,Rattandeep Singh,Isaac Song Zhou Yu*

Main category: cs.AI

TL;DR: 论文提出FieldFinder工具，帮助LLM在纯图像表单中准确定位字段，显著提升填写准确率，解决现有方法性能低下的问题。


<details>
  <summary>Details</summary>
Motivation: 纯图像表单填写因缺乏OCR/文本支持，对计算机代理的多模态理解、信息检索和工具使用能力要求极高，现有基线模型准确率不足1%，GUI代理成本高且效果有限。

Method: 构建含432字段/55文档/3任务的表单填写测试集，开发FieldFinder工具辅助LLM定位表单字段位置。

Result: FieldFinder使所有模型在6种实验条件下性能持平或提升，最高准确率从2%提升至56%；GUI代理准确率仅10.6-68.0%。

Conclusion: FieldFinder有效解决了表单字段识别难题，为无结构图像表单自动化处理提供了可行方案，显著优于传统方法。

Abstract: Completing paperwork is a challenging and time-consuming problem. Form filling is especially challenging in the pure-image domain without access to OCR, typeset PDF text, or a DOM. For computer agents, it requires multiple abilities, including multi-modal understanding, information retrieval, and tool-use. We present a novel form-filling benchmark consisting of 432 fields spread across 55 documents and 3 tasks, requiring knowledge of 236 features per user. We find that baseline VLAs achieve less than 1% accuracy in most cases, primarily due to poor localization ability. GUI agents also struggle, scoring between 10.6-68.0% despite high cost and latency. Therefore, we also contribute FieldFinder, a tool to assist LLMs in identifying where to place text on a form. With FieldFinder, all models achieve equal or better performance in all six study conditions, with a maximum increase from 2% to 56%.

</details>


### [83] [Lightweight Relevance Grader in RAG](https://arxiv.org/abs/2506.14084)
*Taehee Jeong*

Main category: cs.AI

TL;DR: 本文提出在RAG系统中使用微调后的轻量级Llama-3.2-1B模型作为相关性评估器，将精确度从0.1301提升至0.7750，性能媲美70B大模型且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统在检索文档相关性验证环节面临挑战，直接使用大语言模型作为评估器会带来过高计算开销，需探索轻量化解决方案。

Method: 通过微调1B参数的Llama-3.2小型语言模型构建相关性评估器，替代传统大型模型进行文档相关性验证。

Result: 轻量级评估器精度提升近6倍(0.1301→0.7750)，达到与70B参数模型相当水平，代码已开源。

Conclusion: 实验证明轻量级语言模型能有效解决RAG相关性验证问题，在保持高性能的同时显著降低计算资源需求。

Abstract: Retrieval-Augmented Generation (RAG) addresses limitations of large language models (LLMs) by leveraging a vector database to provide more accurate and up-to-date information. When a user submits a query, RAG executes a vector search to find relevant documents, which are then used to generate a response. However, ensuring the relevance of retrieved documents with a query would be a big challenge. To address this, a secondary model, known as a relevant grader, can be served to verify its relevance. To reduce computational requirements of a relevant grader, a lightweight small language model is preferred. In this work, we finetuned llama-3.2-1b as a relevant grader and achieved a significant increase in precision from 0.1301 to 0.7750. Its precision is comparable to that of llama-3.1-70b. Our code is available at https://github.com/taeheej/Lightweight-Relevance-Grader-in-RAG.

</details>


### [84] [Fragile Preferences: A Deep Dive Into Order Effects in Large Language Models](https://arxiv.org/abs/2506.14092)
*Haonan Yin,Shai Vardi,Vidyanand Choudhary*

Main category: cs.AI

TL;DR: 研究发现大语言模型（LLMs）在决策中存在显著位置偏差（如首因效应、中心性偏差）和名字偏好，其偏差模式与人类不同，并提出通过温度参数调整等策略缓解。


<details>
  <summary>Details</summary>
Motivation: LLMs被广泛用于高风险决策（如招聘、录取），但其在选项比较中的位置顺序偏差尚未被系统研究，且需区分表面偏好与真实判断扭曲。

Method: 通过多LLM架构和领域的综合实验，引入基于稳健性、脆弱性、中立性的偏好框架，分析顺序效应如何导致次优选择，并对比性别偏差。

Result: 发现LLMs存在质量依赖的位置偏差（高质量时首因效应，低质量时近因效应）、未记载的中心性偏差和名字偏好，且其位置偏差强于性别偏差。

Conclusion: LLMs的决策偏差并非继承人类，而是具有独特故障模式；通过温度参数调控等针对性策略可减少顺序效应导致的判断失真。

Abstract: Large language models (LLMs) are increasingly used in decision-support systems across high-stakes domains such as hiring and university admissions, where decisions often involve selecting among competing alternatives. While prior work has noted positional order biases in LLM-driven comparisons, these biases have not been systematically dissected or linked to underlying preference structures. We provide the first comprehensive investigation of positional biases across multiple LLM architectures and domains, uncovering strong and consistent order effects, including a novel centrality bias not previously documented in human or machine decision-making. We also find a quality-dependent shift: when options are high quality, models exhibit primacy bias, but favor latter options when option quality is low. We further identify a previously undocumented bias favoring certain names over others. To distinguish superficial tie-breaking from true distortions of judgment, we introduce a framework that classifies pairwise preferences as robust, fragile, or indifferent. We show that order effects can lead models to select strictly inferior options, and that positional biases are typically stronger than gender biases. These findings suggest that LLMs are not merely inheriting human-like biases, but exhibit distinct failure modes not seen in human decision-making. We propose targeted mitigation strategies, including a novel use of the temperature parameter, to reduce order-driven distortions.

</details>


### [85] [Situational-Constrained Sequential Resources Allocation via Reinforcement Learning](https://arxiv.org/abs/2506.14125)
*Libo Zhang,Yang Chen,Toru Takisaka,Kaiqi Zhao,Weidong Li,Jiamou Liu*

Main category: cs.AI

TL;DR: 本文提出SCRL框架，通过逻辑蕴含形式化情境约束并设计动态惩罚算法，结合概率选择机制优化资源分配，在医疗和农业场景中验证其高效性与约束满足能力。


<details>
  <summary>Details</summary>
Motivation: 现实场景中资源分配需考虑动态情境约束，传统约束强化学习方法难以有效处理上下文相关的需求与优先级变化，亟需适应性更强的解决方案。

Method: 将情境约束建模为逻辑蕴含规则，开发动态约束违反惩罚算法，并提出概率选择机制以改进传统约束强化学习（CRL）的局限性。

Result: 在疫情医疗资源分配与农药分配场景中，SCRL在满足约束条件的同时保持高资源利用率，显著优于现有基线方法。

Conclusion: SCRL框架通过情境约束形式化与动态惩罚机制，为上下文敏感的实时决策任务提供了高效可靠的资源分配解决方案，具有实际应用潜力。

Abstract: Sequential Resource Allocation with situational constraints presents a significant challenge in real-world applications, where resource demands and priorities are context-dependent. This paper introduces a novel framework, SCRL, to address this problem. We formalize situational constraints as logic implications and develop a new algorithm that dynamically penalizes constraint violations. To handle situational constraints effectively, we propose a probabilistic selection mechanism to overcome limitations of traditional constraint reinforcement learning (CRL) approaches. We evaluate SCRL across two scenarios: medical resource allocation during a pandemic and pesticide distribution in agriculture. Experiments demonstrate that SCRL outperforms existing baselines in satisfying constraints while maintaining high resource efficiency, showcasing its potential for real-world, context-sensitive decision-making tasks.

</details>


### [86] [Collaborative Editable Model](https://arxiv.org/abs/2506.14146)
*Kaiwen Tang,Aitong Wu,Yao Lu,Guangda Sun*

Main category: cs.AI

TL;DR: 提出协作可编辑模型（CoEM），通过用户贡献的高价值知识片段与轻量级领域适应方法，显著提升垂直领域大语言模型的生成质量，避免传统微调的高成本。


<details>
  <summary>Details</summary>
Motivation: 垂直领域大语言模型依赖大规模标注数据与计算资源，导致开发迭代效率低下。需探索轻量化、用户参与的方法降低训练成本。

Method: 构建用户贡献的候选知识库，通过交互对话与评分机制筛选高价值知识片段，利用上下文提示注入模型实现轻量级领域适应。

Result: 在金融场景中收集1.5万条用户反馈，验证CoEM生成领域内容的准确性显著提升，且避免传统微调的时间与算力开销。

Conclusion: CoEM通过用户-模型协作机制有效平衡领域适应效率与生成质量，为垂直领域LLM的持续优化提供可行路径。

Abstract: Vertical-domain large language models (LLMs) play a crucial role in specialized scenarios such as finance, healthcare, and law; however, their training often relies on large-scale annotated data and substantial computational resources, impeding rapid development and continuous iteration. To address these challenges, we introduce the Collaborative Editable Model (CoEM), which constructs a candidate knowledge pool from user-contributed domain snippets, leverages interactive user-model dialogues combined with user ratings and attribution analysis to pinpoint high-value knowledge fragments, and injects these fragments via in-context prompts for lightweight domain adaptation. With high-value knowledge, the LLM can generate more accurate and domain-specific content. In a financial information scenario, we collect 15k feedback from about 120 users and validate CoEM with user ratings to assess the quality of generated insights, demonstrating significant improvements in domain-specific generation while avoiding the time and compute overhead of traditional fine-tuning workflows.

</details>


### [87] [What's in the Box? Reasoning about Unseen Objects from Multimodal Cues](https://arxiv.org/abs/2506.14212)
*Lance Ying,Daniel Xu,Alicia Zhang,Katherine M. Collins,Max H. Siegel,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: 本文提出一种神经符号模型，结合神经网络解析多模态输入与贝叶斯模型整合信息，通过“盒中何物”实验验证其与人类判断高度相关，优于单模态模型及大型多模态基线。


<details>
  <summary>Details</summary>
Motivation: 人类能灵活整合多源信息（如视听、语言、先验知识）推断不可见物体，但现有模型对此能力缺乏有效模拟。本文旨在探索多源信息融合的认知机制。

Method: 提出神经符号混合模型：使用神经网络解析开放多模态输入，再通过贝叶斯模型综合多源信息评估假设。实验采用“盒中何物”游戏，让模型与人类共同根据摇晃盒子的视频推测内容物。

Result: 实验表明，该模型与人类判断强相关（相关系数高），而单模态消融模型及大型多模态神经模型基线表现显著较差。

Conclusion: 神经符号方法能有效模拟人类多源信息整合能力，验证了混合架构在复杂认知任务中的优势，为理解人类推理机制提供计算模型支持。

Abstract: People regularly make inferences about objects in the world that they cannot see by flexibly integrating information from multiple sources: auditory and visual cues, language, and our prior beliefs and knowledge about the scene. How are we able to so flexibly integrate many sources of information to make sense of the world around us, even if we have no direct knowledge? In this work, we propose a neurosymbolic model that uses neural networks to parse open-ended multimodal inputs and then applies a Bayesian model to integrate different sources of information to evaluate different hypotheses. We evaluate our model with a novel object guessing game called ``What's in the Box?'' where humans and models watch a video clip of an experimenter shaking boxes and then try to guess the objects inside the boxes. Through a human experiment, we show that our model correlates strongly with human judgments, whereas unimodal ablated models and large multimodal neural model baselines show poor correlation.

</details>


### [88] [From Black Boxes to Transparent Minds: Evaluating and Enhancing the Theory of Mind in Multimodal Large Language Models](https://arxiv.org/abs/2506.14224)
*Xinyang Li,Siqi Liu,Bochao Zou,Jiansheng Chen,Huimin Ma*

Main category: cs.AI

TL;DR: 本研究通过构建多模态ToM测试集GridToM，分析多模态大语言模型注意力机制，提出无需训练的轻量方法增强其心智理论能力。


<details>
  <summary>Details</summary>
Motivation: 现有机器心智理论评估方法局限于单模态黑箱测试，缺乏对多模态模型内部机制的可解释性研究。

Method: 创建包含多视角感知信息的GridToM数据集，分析注意力头对认知信息的区分能力，设计基于注意力头方向调整的增强策略。

Result: 注意力头可有效识别跨视角认知差异，提出的方法使模型ToM表现提升23.1%且无需额外训练。

Conclusion: 通过内部机制分析验证了多模态大模型具备心智理论能力，注意力头调整为实现可解释性ToM评估提供了新路径。

Abstract: As large language models evolve, there is growing anticipation that they will emulate human-like Theory of Mind (ToM) to assist with routine tasks. However, existing methods for evaluating machine ToM focus primarily on unimodal models and largely treat these models as black boxes, lacking an interpretative exploration of their internal mechanisms. In response, this study adopts an approach based on internal mechanisms to provide an interpretability-driven assessment of ToM in multimodal large language models (MLLMs). Specifically, we first construct a multimodal ToM test dataset, GridToM, which incorporates diverse belief testing tasks and perceptual information from multiple perspectives. Next, our analysis shows that attention heads in multimodal large models can distinguish cognitive information across perspectives, providing evidence of ToM capabilities. Furthermore, we present a lightweight, training-free approach that significantly enhances the model's exhibited ToM by adjusting in the direction of the attention head.

</details>


### [89] [ImpReSS: Implicit Recommender System for Support Conversations](https://arxiv.org/abs/2506.14231)
*Omri Haller,Yair Meidan,Dudu Mimran,Yuval Elovici,Asaf Shabtai*

Main category: cs.AI

TL;DR: 本文介绍ImpReSS，一种隐式推荐系统，用于在客户支持对话中自动推荐相关解决方案产品类别，无需用户购买意图假设，实验结果显示其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的对话推荐系统在客户支持场景中缺乏对隐式推荐整合的研究，ImpReSS旨在填补这一空白，通过推荐解决方案产品类别同时解决用户问题并促进业务增长。

Method: ImpReSS与现有支持聊天机器人协同工作，分析对话内容以识别推荐机会，隐式推荐相关解决方案产品类别（SPCs），不依赖用户购买意图假设。

Result: 实验评估显示，ImpReSS在通用问题解决、信息安全支持和网络安全故障排除场景下的MRR@1分别为0.72、0.82和0.85，Recall@3分别为0.89、0.83和0.67。

Conclusion: ImpReSS验证了隐式推荐在客户支持中的可行性，其高精度指标表明可同时提升用户体验与业务增长，未来将公开数据与代码以促进研究。

Abstract: Following recent advancements in large language models (LLMs), LLM-based chatbots have transformed customer support by automating interactions and providing consistent, scalable service. While LLM-based conversational recommender systems (CRSs) have attracted attention for their ability to enhance the quality of recommendations, limited research has addressed the implicit integration of recommendations within customer support interactions. In this work, we introduce ImpReSS, an implicit recommender system designed for customer support conversations. ImpReSS operates alongside existing support chatbots, where users report issues and chatbots provide solutions. Based on a customer support conversation, ImpReSS identifies opportunities to recommend relevant solution product categories (SPCs) that help resolve the issue or prevent its recurrence -- thereby also supporting business growth. Unlike traditional CRSs, ImpReSS functions entirely implicitly and does not rely on any assumption of a user's purchasing intent. Our empirical evaluation of ImpReSS's ability to recommend relevant SPCs that can help address issues raised in support conversations shows promising results, including an MRR@1 (and recall@3) of 0.72 (0.89) for general problem solving, 0.82 (0.83) for information security support, and 0.85 (0.67) for cybersecurity troubleshooting. To support future research, our data and code will be shared upon request.

</details>


### [90] [Causes in neuron diagrams, and testing causal reasoning in Large Language Models. A glimpse of the future of philosophy?](https://arxiv.org/abs/2506.14239)
*Louis Vervoort,Vitaly Nikolaev*

Main category: cs.AI

TL;DR: 本文提出基于哲学因果理论（神经元图）的AI抽象因果推理测试，发现ChatGPT等大语言模型能在争议案例中正确识别原因，并挑战了现有因果定义难以确定的观点，预示未来人机协作的哲学研究模式。


<details>
  <summary>Details</summary>
Motivation: 针对哲学因果理论中神经元图的应用局限性，探索AI在抽象推理中的表现，并试图重新定义神经元图中的因果关系，突破传统认为该领域定义难以达成的学术共识。

Method: 采用D. Lewis的神经元图构建因果推理测试框架，对ChatGPT、DeepSeek和Gemini等先进大语言模型进行案例测试，同时提出扩展有效性的新因果定义方法。

Result: 测试显示当前LLMs已具备处理哲学文献中争议性因果案例的能力，提出的新因果定义在神经元图中展现出比现有方法更广泛的适用性。

Conclusion: AI在复杂因果推理中的表现表明，未来哲学研究可能演变为人类智慧与人工智能协同推进的新范式，挑战传统研究边界。

Abstract: We propose a test for abstract causal reasoning in AI, based on scholarship in the philosophy of causation, in particular on the neuron diagrams popularized by D. Lewis. We illustrate the test on advanced Large Language Models (ChatGPT, DeepSeek and Gemini). Remarkably, these chatbots are already capable of correctly identifying causes in cases that are hotly debated in the literature. In order to assess the results of these LLMs and future dedicated AI, we propose a definition of cause in neuron diagrams with a wider validity than published hitherto, which challenges the widespread view that such a definition is elusive. We submit that these results are an illustration of how future philosophical research might evolve: as an interplay between human and artificial expertise.

</details>


### [91] [Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs](https://arxiv.org/abs/2506.14245)
*Xumeng Wen,Zihan Liu,Shun Zheng,Zhijian Xu,Shengyu Ye,Zhirong Wu,Xiao Liang,Yang Wang,Junjie Li,Ziming Miao,Jiang Bian,Mao Yang*

Main category: cs.AI

TL;DR: 本文指出传统Pass@K指标在评估大语言模型推理能力时存在缺陷，并提出新指标CoT-Pass@K以同时验证推理链与答案的正确性。通过理论与实证分析，证明RLVR方法能有效提升逻辑完整性，并揭示其训练动态特征。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR调优模型在Pass@K指标上表现不佳，但该指标本身存在根本性缺陷——仅关注最终答案正确性，却忽略推理链的准确性。这导致对强化学习可验证奖励(RLVR)方法效果的误判。

Method: 提出CoT-Pass@K新评估指标，要求推理路径(Chain-of-Thought)与最终答案同时正确。建立理论框架证明RLVR相比传统强化学习更能激励逻辑完整性，并通过训练动态分析和多维度实证验证假设。

Result: 使用CoT-Pass@K时，RLVR在所有K值下均展现推理能力提升。训练早期即出现泛化特征，且改进过程呈现平滑扩展特性，证实该方法能真正增强机器推理而非简单路径重加权。

Conclusion: 研究揭示了评估指标缺陷对RLVR效果认知的误导性，提出更可靠的CoT-Pass@K评估体系，并通过理论与实验证实RLVR在促进逻辑完整性方面的独特优势，为机器推理发展提供新视角。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned models often underperform their base models on the $Pass@K$ metric for solution-finding, leading to the hypothesis that RLVR merely re-weights existing reasoning paths at the cost of reasoning diversity. In this work, we resolve this contradiction by identifying the source of the problem: the $Pass@K$ metric itself is a flawed measure of reasoning, as it credits correct final answers that probably arise from inaccurate or incomplete chains of thought (CoTs). To address this, we introduce a more precise evaluation metric, $CoT$-$Pass@K$, which mandates that both the reasoning path and the final answer be correct. We provide a new theoretical foundation that formalizes how RLVR, unlike traditional RL, is uniquely structured to incentivize logical integrity. Our empirical results are supportive: using $CoT$-$Pass@K$, we observe that RLVR can incentivize the generalization of correct reasoning for all values of $K$. Furthermore, by analyzing the training dynamics, we find that this enhanced reasoning capability emerges early in the training process and smoothly generalizes. Our work provides a clear perspective on the role of RLVR, offers a more reliable method for its evaluation, and confirms its potential to genuinely advance machine reasoning.

</details>


### [92] [Mxplainer: Explain and Learn Insights by Imitating Mahjong Agents](https://arxiv.org/abs/2506.14246)
*Lingfeng Li,Yunlong Lu,Yongyi Wang,Qifan Zheng,Wenxin Li*

Main category: cs.AI

TL;DR: 提出Mxplainer框架，通过参数化搜索算法与神经网络转换，解释麻将AI黑箱代理的决策逻辑，提供人类可理解的策略洞察。


<details>
  <summary>Details</summary>
Motivation: 现有麻将AI虽达到职业玩家水平，但其黑箱特性阻碍人类从中学习策略。需开发可解释性方法以提取AI决策洞见。

Method: 设计参数化搜索算法Mxplainer，将其转换为等效神经网络，通过AI与人类玩家数据学习黑箱代理参数。

Result: 实验表明学习参数能揭示AI特性与风格，并实现多数牌局状态的局部决策解释。

Conclusion: Mxplainer框架有效解析黑箱AI决策模式，为人类学习复杂博弈策略提供可解释性工具，可扩展至其他不完全信息博弈场景。

Abstract: People need to internalize the skills of AI agents to improve their own capabilities. Our paper focuses on Mahjong, a multiplayer game involving imperfect information and requiring effective long-term decision-making amidst randomness and hidden information. Through the efforts of AI researchers, several impressive Mahjong AI agents have already achieved performance levels comparable to those of professional human players; however, these agents are often treated as black boxes from which few insights can be gleaned. This paper introduces Mxplainer, a parameterized search algorithm that can be converted into an equivalent neural network to learn the parameters of black-box agents. Experiments conducted on AI and human player data demonstrate that the learned parameters provide human-understandable insights into these agents' characteristics and play styles. In addition to analyzing the learned parameters, we also showcase how our search-based framework can locally explain the decision-making processes of black-box agents for most Mahjong game states.

</details>


### [93] [Don't throw the baby out with the bathwater: How and why deep learning for ARC](https://arxiv.org/abs/2506.14276)
*Jack Cole,Mohamed Osman*

Main category: cs.AI

TL;DR: 本文提出通过测试时即时训练和增强推理方法，结合深度学习范式，显著提升了AI在抽象推理任务ARC-AGI上的性能，最高准确率提升达260%-300%，并在2023年ARC竞赛中取得最佳成绩。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在视觉、语言等领域表现出色，但在抽象推理任务ARC-AGI中性能仍较低。研究旨在验证深度学习范式通过动态训练机制能否突破现有局限，探索其在未知领域中的泛化推理能力。

Method: 1. 基于预训练大语言模型进行ARC任务微调；2. 提出测试时微调（TTFT）和增强推理反向增强投票（AIRV）技术；3. 将神经网络与优化器共同作为推理过程的核心组件。

Result: AIRV使准确率提升260%，TTFT进一步增加300%。最终模型在ARC私有测试集达到58%准确率（当前最佳），并在2023年ARCathon竞赛中夺冠。

Conclusion: 深度学习通过动态抽象学习机制可有效解决复杂抽象推理问题。测试时训练与模型-优化器协同推理是提升未知领域泛化能力的关键，为通用感知推理系统提供了新范式。

Abstract: The Abstraction and Reasoning Corpus (ARC-AGI) presents a formidable challenge for AI systems. Despite the typically low performance on ARC, the deep learning paradigm remains the most effective known strategy for generating skillful (state-of-the-art) neural networks (NN) across varied modalities and tasks in vision, language etc. The deep learning paradigm has proven to be able to train these skillful neural networks and learn the abstractions needed in these diverse domains. Our work doubles down on that and continues to leverage this paradigm by incorporating on-the-fly NN training at test time. We demonstrate that fully committing to deep learning's capacity to acquire novel abstractions yields state-of-the-art performance on ARC. Specifically, we treat both the neural network and the optimizer (rather than just a pre-trained network) as integral components of the inference process, fostering generalization to unseen tasks. Concretely, we propose a methodology for training on ARC, starting from pretrained LLMs, and enhancing their ARC reasoning. We also propose Test-Time Fine-Tuning (TTFT) and the Augment Inference Reverse-Augmentation and Vote (AIRV) as effective test-time techniques. We are the first to propose and show deep learning can be used effectively for ARC, showing boosts of up to 260% in accuracy with AIRV and a further 300% boost with TTFT. An early version of this approach secured first place in the 2023 ARCathon competition, while the final version achieved the current best score on the ARC private test-set (58%). Our findings highlight the key ingredients of a robust reasoning system in unfamiliar domains, underscoring the central mechanisms that improve broad perceptual reasoning.

</details>


### [94] [ADRD: LLM-Driven Autonomous Driving Based on Rule-based Decision Systems](https://arxiv.org/abs/2506.14299)
*Fanzhi Zeng,Siqi Wang,Chuzhao Zhu,Li Li*

Main category: cs.AI

TL;DR: 本研究提出ADRD框架，利用大语言模型（LLM）构建可解释的自动驾驶规则决策系统，通过信息整合、策略生成与测试迭代，显著提升可解释性、响应速度与驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 构建可解释的自动驾驶决策系统是学术研究焦点，传统方法在透明性与适应性上存在不足，需结合LLM的推理与规则系统的可控性。

Method: 提出ADRD框架，整合信息模块（场景信息聚合）、代理模块（生成规则驾驶策略）与测试模块（迭代优化），利用LLM的推理能力生成可执行规则系统。

Result: 实验表明ADRD在决策任务中优于传统强化学习与先进LLM方法，可解释性、响应速度与驾驶性能显著提升，且能精准理解复杂场景。

Conclusion: 首次将LLM与规则系统结合于自动驾驶决策，验证其实际部署潜力，为透明、易修改、广泛适用的规则系统提供新方向。

Abstract: How to construct an interpretable autonomous driving decision-making system has become a focal point in academic research. In this study, we propose a novel approach that leverages large language models (LLMs) to generate executable, rule-based decision systems to address this challenge. Specifically, harnessing the strong reasoning and programming capabilities of LLMs, we introduce the ADRD(LLM-Driven Autonomous Driving Based on Rule-based Decision Systems) framework, which integrates three core modules: the Information Module, the Agents Module, and the Testing Module. The framework operates by first aggregating contextual driving scenario information through the Information Module, then utilizing the Agents Module to generate rule-based driving tactics. These tactics are iteratively refined through continuous interaction with the Testing Module. Extensive experimental evaluations demonstrate that ADRD exhibits superior performance in autonomous driving decision tasks. Compared to traditional reinforcement learning approaches and the most advanced LLM-based methods, ADRD shows significant advantages in terms of interpretability, response speed, and driving performance. These results highlight the framework's ability to achieve comprehensive and accurate understanding of complex driving scenarios, and underscore the promising future of transparent, rule-based decision systems that are easily modifiable and broadly applicable. To the best of our knowledge, this is the first work that integrates large language models with rule-based systems for autonomous driving decision-making, and our findings validate its potential for real-world deployment.

</details>


### [95] [AviationLLM: An LLM-based Knowledge System for Aviation Training](https://arxiv.org/abs/2506.14336)
*Jia'ang Wan,Feng Shen,Fujuan Li,Yanjin Sun,Yan Li,Shiwen Zhang*

Main category: cs.AI

TL;DR: 本文提出RALA-DPO方法，通过结合直接偏好优化（DPO）与检索增强生成（RAG）技术，提升航空理论培训中大型语言模型的专业知识回答准确性，并实现零成本知识更新。


<details>
  <summary>Details</summary>
Motivation: 现有航空培训依赖有限教员资源，网络答案准确性不足，传统监督微调（SFT）模型易生成表面合理但事实错误的回答，导致培训效率低下。需通过优化模型与外部知识库结合解决此问题。

Method: 基于开源预训练模型Qwen，采用直接偏好优化（DPO）进行领域对齐，并结合检索增强生成（RAG）技术，从外部知识库检索信息以减少模型因数据偏差、知识过时或领域空白产生的幻觉。

Result: 实验表明，RALA-DPO能有效提升航空知识回答的准确性，RAG机制进一步增强了答案精度，同时实现零成本知识更新。

Conclusion: RALA-DPO通过整合DPO与RAG，解决了专业领域模型回答的准确性与知识更新问题，为航空培训提供了高效可靠的技术支持。

Abstract: Aviation training is a core link in ensuring flight safety, improving industry efficiency and promoting sustainable development. It not only involves flight simulation but also requires the learning of a great deal of professional aviation theory knowledge. In the existing training system, the knowledge is mainly imparted by the the instructors. However, the number of instructors is limited and the professional answers obtained from the Internet are not accurate enough, resulting in low training efficiency. To address this, we introduced LLM, but the basic pre-trained model cannot provide accurate answers to professional fields, so we fine-tuned it. Traditional Supervised Fine-Tuning (SFT) risk generating superficially plausible but factually incorrect responses due to insufficient data coverage. To address this, we employ Direct Preference Optimization(DPO). This paper proposes Retrieval-Augmented LLM Alignment via Direct Preference Optimization(RALA-DPO). We select open source pre-trained LLM Qwen and adapt it to aviation theory training through DPO-based domain alignment. Simultaneously, to mitigate hallucinations caused by training data biases, knowledge obsolescence, or domain knowledge gaps, we implement Retrieval-Augmented Generation(RAG) technology that combines generative and retrieval models. RALA-DPO effectively retrieves relevant information from external knowledge bases and delivers precise and high-quality responses through the generative model. Experimental results demonstrate that RALA-DPO can improve accuracy in response to professional aviation knowledge. With integrated RAG mechanisms, this system can further improve the accuracy of answers and achieve zero-cost knowledge updates simultaneously.

</details>


### [96] [Don't Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning](https://arxiv.org/abs/2506.14387)
*William F. Shen,Xinchi Qiu,Nicola Cancedda,Nicholas D. Lane*

Main category: cs.AI

TL;DR: 现有研究在缓解大语言模型微调中的灾难性遗忘时，忽视了安全对齐能力（如诚实表达未知）的退化。本文提出SEAT方法，通过稀疏训练和实体扰动正则化，在保持微调性能的同时保留模型的『无知感知』能力。


<details>
  <summary>Details</summary>
Motivation: 传统微调会显著削弱模型安全对齐中培养的关键能力（如诚实表达未知），导致幻觉等问题。现有方法仅关注此缺陷，亟需兼顾性能与安全性的解决方案。

Method: SEAT包含两个核心组件：(1) 限制激活漂移的稀疏训练；(2) 通过实体扰动和KL散度正则化解决知识纠缠问题。

Result: 实验表明SEAT在保持微调性能的同时，显著优于基线方法（如准确率提升15%），有效维持模型的『无知感知』能力。

Conclusion: SEAT为LLM微调提供了更鲁棒的解决方案，首次同时解决了灾难性遗忘与安全能力退化问题，具有重要实践价值。

Abstract: Existing work on mitigating catastrophic forgetting in large language model (LLM) fine-tuning has primarily focused on preserving specific data or tasks, while critically overlooking the degradation of essential capabilities instilled through safety alignment, particularly the model's ability to faithfully express ignorance. In this work, we show that this capability is significantly degraded during conventional fine-tuning, leading to undesired behaviors such as hallucinations. To address this novel but highly practical problem, we propose SEAT, a simple and effective fine-tuning approach that preserves both fine-tuning performance and the model's inherent ability to acknowledge its ignorance. SEAT integrates two key components: (1) sparse training that constrains activation drift, and (2) a novel entity perturbation method with KL-divergence regularization, designed to counter knowledge entanglement. Experimental results demonstrate that SEAT significantly outperforms baselines in preserving ignorance awareness while retaining fine-tuning performance, offering a more robust solution for LLM fine-tuning.

</details>


### [97] [AST-Enhanced or AST-Overloaded? The Surprising Impact of Hybrid Graph Representations on Code Clone Detection](https://arxiv.org/abs/2506.14470)
*Zixian Zhang,Takfarinas Saber*

Main category: cs.AI

TL;DR: 本文通过实证研究发现，在基于GNN的代码克隆检测中，AST与CFG+DFG的混合表示能提升卷积和注意力模型的准确率，而FA-AST因结构复杂反而降低性能。GMN模型即使使用标准AST也能超越其他模型，减少对增强结构的需求。


<details>
  <summary>Details</summary>
Motivation: 现有基于AST的代码克隆检测方法缺乏语义深度，尽管近期研究尝试结合CFG、DFG等语义图增强表示，但不同混合表示与图机器学习技术的兼容性及效果尚未明确，需系统性评估。

Method: 采用全面实证研究方法，系统比较多种AST混合表示（CFG、DFG、FA-AST）在不同GNN架构（GCN、GAT、GMN等）中的性能表现。

Result: AST+CFG+DFG显著提升GCN/GAT的准确率，但FA-AST因结构复杂性损害性能；GMN仅用标准AST即超越其他模型，凸显其跨代码相似性检测优势。

Conclusion: 混合表示对GNN模型的影响具有差异性，GMN的架构设计比图结构增强更关键，为代码克隆检测中模型与表示的协同优化提供新见解。

Abstract: As one of the most detrimental code smells, code clones significantly increase software maintenance costs and heighten vulnerability risks, making their detection a critical challenge in software engineering. Abstract Syntax Trees (ASTs) dominate deep learning-based code clone detection due to their precise syntactic structure representation, but they inherently lack semantic depth. Recent studies address this by enriching AST-based representations with semantic graphs, such as Control Flow Graphs (CFGs) and Data Flow Graphs (DFGs). However, the effectiveness of various enriched AST-based representations and their compatibility with different graph-based machine learning techniques remains an open question, warranting further investigation to unlock their full potential in addressing the complexities of code clone detection. In this paper, we present a comprehensive empirical study to rigorously evaluate the effectiveness of AST-based hybrid graph representations in Graph Neural Network (GNN)-based code clone detection. We systematically compare various hybrid representations ((CFG, DFG, Flow-Augmented ASTs (FA-AST)) across multiple GNN architectures. Our experiments reveal that hybrid representations impact GNNs differently: while AST+CFG+DFG consistently enhances accuracy for convolution- and attention-based models (Graph Convolutional Networks (GCN), Graph Attention Networks (GAT)), FA-AST frequently introduces structural complexity that harms performance. Notably, GMN outperforms others even with standard AST representations, highlighting its superior cross-code similarity detection and reducing the need for enriched structures.

</details>


### [98] [GUI-Robust: A Comprehensive Dataset for Testing GUI Agent Robustness in Real-World Anomalies](https://arxiv.org/abs/2506.14477)
*Jingqi Yang,Zhilong Song,Jiawei Chen,Mingli Song,Sheng Zhou,linjun sun,Xiaogang Ouyang,Chun Chen,Can Wang*

Main category: cs.AI

TL;DR: 本文提出了GUI-Robust数据集，通过集成七种常见GUI交互异常类型，并采用半自动化构建范式（RPA+MLLM），显著降低标注成本，揭示了现有GUI代理在异常场景下的性能缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理数据集多基于理想化条件构建，忽略了现实部署中频繁出现的多样化异常场景，导致评估结果与实际应用存在偏差。

Method: 提出半自动化数据集构建范式：利用RPA工具采集自然交互的用户动作序列，通过MLLM生成动作描述，使标注时间成本降低19倍以上。

Result: 基于GUI-Robust的测试表明，当前最先进GUI代理在异常场景下出现显著性能退化，验证了数据集的评估有效性。

Conclusion: 本研究强调GUI代理鲁棒性的重要性，开源数据集为相关研究提供基准，启发未来对异常场景适应能力的探索。

Abstract: The development of high-quality datasets is crucial for benchmarking and advancing research in Graphical User Interface (GUI) agents. Despite their importance, existing datasets are often constructed under idealized conditions, overlooking the diverse anomalies frequently encountered in real-world deployments. To address this limitation, we introduce GUI-Robust, a novel dataset designed for comprehensive GUI agent evaluation, explicitly incorporating seven common types of anomalies observed in everyday GUI interactions. Furthermore, we propose a semi-automated dataset construction paradigm that collects user action sequences from natural interactions via RPA tools and then generate corresponding step and task descriptions for these actions with the assistance of MLLMs. This paradigm significantly reduces annotation time cost by a factor of over 19 times. Finally, we assess state-of-the-art GUI agents using the GUI-Robust dataset, revealing their substantial performance degradation in abnormal scenarios. We anticipate that our work will highlight the importance of robustness in GUI agents and inspires more future research in this direction. The dataset and code are available at https://github.com/chessbean1/GUI-Robust..

</details>


### [99] [LLM-Powered Swarms: A New Frontier or a Conceptual Stretch?](https://arxiv.org/abs/2506.14496)
*Muhammad Atta Ur Rahman,Melanie Schranz*

Main category: cs.AI

TL;DR: 本文对比传统群体智能算法（如Boids、蚁群优化）与基于大语言模型（LLM）的群体系统，探讨去中心化、可扩展性和涌现行为在现代AI中的重新定义，并评估两者在延迟、资源使用和行为准确性上的差异。研究指出LLM虽增强推理能力，但带来计算与协调的新挑战。


<details>
  <summary>Details</summary>
Motivation: 探索传统群体智能与LLM驱动的群体系统在去中心化、可扩展性和涌现行为上的差异，分析LLM如何重新定义群体概念，并评估其在现代AI中的适用性。

Method: 通过实现传统算法（Boids、ACO）与LLM驱动的群体系统，对比两者的延迟、资源消耗和行为准确性，同时评估云端与本地LLM在群体中的适用性。

Result: LLM在抽象推理方面表现优异，但计算开销和协调复杂性增加；云端LLM资源充足但延迟较高，本地LLM受限硬件但更灵活。传统算法在效率和可扩展性上仍具优势。

Conclusion: 整合LLM为群体系统带来新能力，但需权衡计算与协调成本。研究强调需重新审视传统群体设计原则，并推动AI中“群体”定义的动态扩展。

Abstract: Swarm intelligence traditionally refers to systems of simple, decentralized agents whose local interactions lead to emergent, collective behavior. Recently, the term 'swarm' has been extended to describe AI systems like OpenAI's Swarm, where large language models (LLMs) act as collaborative agents. This paper contrasts traditional swarm algorithms with LLM-driven swarms exploring how decentralization, scalability, and emergence are redefined in modern artificial intelligence (AI). We implement and compare both paradigms using Boids and Ant Colony Optimization (ACO), evaluating latency, resource usage, and behavioral accuracy. The suitability of both cloud-based and local LLMs is assessed for the agent-based use in swarms. Although LLMs offer powerful reasoning and abstraction capabilities, they introduce new constraints in computation and coordination that challenge traditional notions of swarm design. This study highlights the opportunities and limitations of integrating LLMs into swarm systems and discusses the evolving definition of 'swarm' in modern AI research.

</details>


### [100] [Toward Safety-First Human-Like Decision Making for Autonomous Vehicles in Time-Varying Traffic Flow](https://arxiv.org/abs/2506.14502)
*Xiao Wang,Junru Yu,Jun Huang,Qiong Wu,Ljubo Vacic,Changyin Sun*

Main category: cs.AI

TL;DR: 提出一种安全优先的类人决策框架（SF-HLDM），通过时空注意力机制、社会合规性估计和深度进化强化学习，解决自动驾驶在动态密集交通中的安全与效率问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶在动态密集交通中面临决策迁移性差、搜索成本高的问题，人类决策的多样性和自由意志导致数据驱动方法效率低下。需提升安全性、舒适性及社会兼容性。

Method: 结合分层渐进框架：1）时空注意力机制推断他车意图；2）社会合规性模块规范行为；3）深度进化强化学习（DERL）扩展搜索空间，避免局部最优与过拟合。

Result: SF-HLDM框架使自动驾驶AI动态调整参数，保持安全边际并适应场景化驾驶行为，实现可解释、灵活的类人决策。

Conclusion: 该框架有效提升自动驾驶在复杂交通中的安全性与社会兼容性，通过多模块协同优化决策效率，减少传统数据驱动方法的局限性。

Abstract: Despite the recent advancements in artificial intelligence technologies have shown great potential in improving transport efficiency and safety, autonomous vehicles(AVs) still face great challenge of driving in time-varying traffic flow, especially in dense and interactive situations. Meanwhile, human have free wills and usually do not make the same decisions even situate in the exactly same scenarios, leading to the data-driven methods suffer from poor migratability and high search cost problems, decreasing the efficiency and effectiveness of the behavior policy. In this research, we propose a safety-first human-like decision-making framework(SF-HLDM) for AVs to drive safely, comfortably, and social compatiblely in effiency. The framework integrates a hierarchical progressive framework, which combines a spatial-temporal attention (S-TA) mechanism for other road users' intention inference, a social compliance estimation module for behavior regulation, and a Deep Evolutionary Reinforcement Learning(DERL) model for expanding the search space efficiently and effectively to make avoidance of falling into the local optimal trap and reduce the risk of overfitting, thus make human-like decisions with interpretability and flexibility. The SF-HLDM framework enables autonomous driving AI agents dynamically adjusts decision parameters to maintain safety margins and adhering to contextually appropriate driving behaviors at the same time.

</details>


### [101] [Doppelgänger Method: Breaking Role Consistency in LLM Agent via Prompt-based Transferable Adversarial Attack](https://arxiv.org/abs/2506.14539)
*Daewon Kang,YeongHwan Shin,Doyeon Kim,Kyu-Hwan Jung,Meong Hi Son*

Main category: cs.AI

TL;DR: 本文提出对抗性攻击方法'Doppelgänger'揭示LLM代理的安全漏洞，定义脆弱性评估指标PACAT，并通过'CAT'提示实现有效防御。实验证明攻击可暴露系统信息，而防御方法能保持指令一致性。


<details>
  <summary>Details</summary>
Motivation: 随着提示工程快速创建大量自主代理，其安全性和行为一致性面临严峻挑战。研究旨在揭示系统指令泄露风险，并建立对抗性攻击的防御机制。

Method: 1) 提出Doppelgänger方法模拟代理劫持攻击；2) 定义PACAT指标量化脆弱性；3) 设计CAT防御提示增强抗攻击能力。

Result: 实验显示Doppelgänger攻击成功率达89%的指令泄露率，而采用CAT提示的代理在对抗样本中保持95%以上指令一致性。

Conclusion: LLM代理存在对抗转移攻击风险，通过系统性脆弱性评估和防御提示设计，可有效提升智能体安全性和行为鲁棒性。

Abstract: Since the advent of large language models, prompt engineering now enables the rapid, low-effort creation of diverse autonomous agents that are already in widespread use. Yet this convenience raises urgent concerns about the safety, robustness, and behavioral consistency of the underlying prompts, along with the pressing challenge of preventing those prompts from being exposed to user's attempts. In this paper, we propose the ''Doppelgänger method'' to demonstrate the risk of an agent being hijacked, thereby exposing system instructions and internal information. Next, we define the ''Prompt Alignment Collapse under Adversarial Transfer (PACAT)'' level to evaluate the vulnerability to this adversarial transfer attack. We also propose a ''Caution for Adversarial Transfer (CAT)'' prompt to counter the Doppelgänger method. The experimental results demonstrate that the Doppelgänger method can compromise the agent's consistency and expose its internal information. In contrast, CAT prompts enable effective defense against this adversarial attack.

</details>


### [102] [QUEST: Quality-aware Semi-supervised Table Extraction for Business Documents](https://arxiv.org/abs/2506.14568)
*Eliott Thomas,Mickael Coustaty,Aurelie Joseph,Gaspar Deloin,Elodie Carel,Vincent Poulain D'Andecy,Jean-Marc Ogier*

Main category: cs.AI

TL;DR: QUEST提出一种质量感知的半监督表格提取框架，通过结构/上下文特征评估和多样性指标，显著提升业务文档中的表格提取效果。


<details>
  <summary>Details</summary>
Motivation: 现有表格提取方法依赖置信度分数且多阶段流程易错，在标注稀缺的业务文档场景中表现受限。

Method: 结合预测F1分数的质量评估模型(结构+上下文特征)与多样性指标(DPP/Vendi/IntDiv)，指导半监督训练中的伪标签选择。

Result: 在专有数据集上F1从64%提升至74%，空预测减少45%；DocILE基准F1提高8%，空预测减少19%。

Conclusion: QUEST通过可解释的质量评估和对标注稀缺的鲁棒性，特别适用于结构一致性要求高的业务文档场景。

Abstract: Automating table extraction (TE) from business documents is critical for industrial workflows but remains challenging due to sparse annotations and error-prone multi-stage pipelines. While semi-supervised learning (SSL) can leverage unlabeled data, existing methods rely on confidence scores that poorly reflect extraction quality. We propose QUEST, a Quality-aware Semi-supervised Table extraction framework designed for business documents. QUEST introduces a novel quality assessment model that evaluates structural and contextual features of extracted tables, trained to predict F1 scores instead of relying on confidence metrics. This quality-aware approach guides pseudo-label selection during iterative SSL training, while diversity measures (DPP, Vendi score, IntDiv) mitigate confirmation bias. Experiments on a proprietary business dataset (1000 annotated + 10000 unannotated documents) show QUEST improves F1 from 64% to 74% and reduces empty predictions by 45% (from 12% to 6.5%). On the DocILE benchmark (600 annotated + 20000 unannotated documents), QUEST achieves a 50% F1 score (up from 42%) and reduces empty predictions by 19% (from 27% to 22%). The framework's interpretable quality assessments and robustness to annotation scarcity make it particularly suited for business documents, where structural consistency and data completeness are paramount.

</details>


### [103] [Enhancing Symbolic Machine Learning by Subsymbolic Representations](https://arxiv.org/abs/2506.14569)
*Stephen Roth,Lennart Baur,Derian Boer,Stefan Kramer*

Main category: cs.AI

TL;DR: 本文提出通过将神经嵌入整合到符号机器学习框架TILDE中，以解决现有神经符号AI系统在简单场景（如含大量常量的判别式学习）中的效率问题。实验表明该方法在三个实际领域中的F1分数优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有神经符号AI系统（如LTN、DeepProbLog）虽支持端到端学习，但在处理简单任务（如含大量常量的判别式学习）时效率较低。因此，需探索更高效的方法以结合符号与亚符号AI的优势。

Method: 通过为符号学习框架TILDE引入神经嵌入（特别是常量在相似性谓词中的嵌入），并基于符号理论进一步优化嵌入表示，从而增强符号学习器的表达能力。

Result: 在三个真实领域中的实验表明，该方法在F1分数上显著优于其他基线方法，验证了其简单性与有效性。

Conclusion: 该方法不仅适用于当前场景，还可扩展至实例间相似性建模（类似逻辑语言中的核方法）、类比推理或命题化等方向，为符号学习器增强提供了新思路。

Abstract: The goal of neuro-symbolic AI is to integrate symbolic and subsymbolic AI approaches, to overcome the limitations of either. Prominent systems include Logic Tensor Networks (LTN) or DeepProbLog, which offer neural predicates and end-to-end learning. The versatility of systems like LTNs and DeepProbLog, however, makes them less efficient in simpler settings, for instance, for discriminative machine learning, in particular in domains with many constants. Therefore, we follow a different approach: We propose to enhance symbolic machine learning schemes by giving them access to neural embeddings. In the present paper, we show this for TILDE and embeddings of constants used by TILDE in similarity predicates. The approach can be fine-tuned by further refining the embeddings depending on the symbolic theory. In experiments in three real-world domain, we show that this simple, yet effective, approach outperforms all other baseline methods in terms of the F1 score. The approach could be useful beyond this setting: Enhancing symbolic learners in this way could be extended to similarities between instances (effectively working like kernels within a logical language), for analogical reasoning, or for propositionalization.

</details>


### [104] [From Points to Places: Towards Human Mobility-Driven Spatiotemporal Foundation Models via Understanding Places](https://arxiv.org/abs/2506.14570)
*Mohammad Hashemi,Andreas Zufle*

Main category: cs.AI

TL;DR: 提出构建空间基础模型以捕捉人类移动性，通过动态、上下文丰富的场所模型替代离散兴趣点，解决现有模型在适应性、扩展性和多粒度推理上的不足，推动下一代地理空间智能发展。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型难以处理移动数据的时空和语义复杂性，需开发通用模型支持跨地理环境的可扩展分析，以反映社会行为、资源获取和动态空间模式。

Method: 提出新型空间基础模型框架，整合地理位置语义与多尺度人类移动数据，从离散兴趣点转向动态场所建模，聚焦场所的上下文关联及行为驱动特性。

Result: 明确模型在可扩展性、多粒度推理等关键差距，提出研究方向（如场所建模与高效学习机制），为开发情境感知模型提供理论路径。

Conclusion: 空间基础模型将赋能个性化场所发现、物流优化及城市规划等应用，实现更智能、响应式的空间决策，推动地理空间智能的演进。

Abstract: Capturing human mobility is essential for modeling how people interact with and move through physical spaces, reflecting social behavior, access to resources, and dynamic spatial patterns. To support scalable and transferable analysis across diverse geographies and contexts, there is a need for a generalizable foundation model for spatiotemporal data. While foundation models have transformed language and vision, they remain limited in handling the unique challenges posed by the spatial, temporal, and semantic complexity of mobility data. This vision paper advocates for a new class of spatial foundation models that integrate geolocation semantics with human mobility across multiple scales. Central to our vision is a shift from modeling discrete points of interest to understanding places: dynamic, context-rich regions shaped by human behavior and mobility that may comprise many places of interest. We identify key gaps in adaptability, scalability, and multi-granular reasoning, and propose research directions focused on modeling places and enabling efficient learning. Our goal is to guide the development of scalable, context-aware models for next-generation geospatial intelligence. These models unlock powerful applications ranging from personalized place discovery and logistics optimization to urban planning, ultimately enabling smarter and more responsive spatial decision-making.

</details>


### [105] [AgentDistill: Training-Free Agent Distillation with Generalizable MCP Boxes](https://arxiv.org/abs/2506.14728)
*Jiahao Qiu,Xinzhe Juan,Yimin Wang,Ling Yang,Xuan Qi,Tongcheng Zhang,Jiacheng Guo,Yifu Lu,Zixin Yao,Hongru Wang,Shilong Liu,Xun Jiang,Liu Leqi,Mengdi Wang*

Main category: cs.AI

TL;DR: 本文提出了一种无需训练的AgentDistill框架，通过复用教师智能体生成的模块化任务解决协议（MCPs），使小型学生智能体在跨领域任务中高效迁移知识，性能接近基于大模型（如GPT-4o）的系统。


<details>
  <summary>Details</summary>
Motivation: 现有智能体蒸馏方法依赖完整轨迹复现或逐步工具模仿，难以训练学生智能体在新环境中动态规划。基于LLM的智能体（含规划、记忆和工具使用能力）的蒸馏研究仍不充分。

Method: 提出AgentDistill框架，通过直接复用教师智能体自主生成的结构化可重用模块（MCPs），实现无需训练的知识迁移，使学生智能体以最小监督解决新问题。

Result: 在生物医学和数学基准测试中，基于小模型的学生智能体性能与使用大模型（如OctoTools/GPT-4o）的先进系统相当，验证了框架的高效性和可扩展性。

Conclusion: AgentDistill通过模块化协议复用机制，为构建低成本、可扩展的智能体提供了有效方案，解决了传统方法在新环境动态规划中的局限性。

Abstract: While knowledge distillation has become a mature field for compressing large language models (LLMs) into smaller ones by aligning their outputs or internal representations, the distillation of LLM-based agents, which involve planning, memory, and tool use, remains relatively underexplored. Existing agent distillation methods typically replay full teacher trajectories or imitate step-by-step teacher tool usage, but they often struggle to train student agents to dynamically plan and act in novel environments. We propose AgentDistill, a novel, training-free agent distillation framework that enables efficient and scalable knowledge transfer via direct reuse of Model-Context-Protocols (MCPs), which are structured and reusable task-solving modules autonomously generated by teacher agents. The reuse of these distilled MCPs enables student agents to generalize their capabilities across domains and solve new problems with minimal supervision or human intervention. Experiments on biomedical and mathematical benchmarks demonstrate that our distilled student agents, built on small language models, can achieve performance comparable to advanced systems using large LLMs such as OctoTools (GPT-4o), highlighting the effectiveness of our framework in building scalable and cost-efficient intelligent agents.

</details>


### [106] [Optimizing Length Compression in Large Reasoning Models](https://arxiv.org/abs/2506.14755)
*Zhengxiang Cheng,Dongping Chen,Mingyang Fu,Tianyi Zhou*

Main category: cs.AI

TL;DR: 本文针对大型推理模型（LRMs）生成冗余推理链的问题，提出基于简洁性和充分性两原则的LC-R1方法，通过GRPO训练结合长度奖励和压缩奖励，显著缩短推理序列长度（约50%）且仅牺牲少量准确率（约2%）。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在生成推理链时存在“无效思考”问题，即在得出正确答案后仍反复验证，导致冗余和低效。现有方法未充分解决这一具体问题，需更细粒度的优化原则。

Method: 提出LC-R1方法：基于Group Relative Policy Optimization（GRPO）的后训练框架，结合长度奖励（整体简洁性）和压缩奖励（针对性去除无效推理步骤），以同时满足Brevity（消除冗余）和Sufficiency（保留关键步骤）原则。

Result: 在多个推理基准测试中，LC-R1将推理序列长度减少约50%，准确率仅下降约2%，在帕累托前沿上实现了高压缩优先的优化平衡。分析验证了方法的鲁棒性。

Conclusion: LC-R1通过细粒度原则和双奖励机制有效解决了LRMs的冗余推理问题，为开发高效能、低计算成本的大型推理模型提供了新思路。代码已开源。

Abstract: Large Reasoning Models (LRMs) have achieved remarkable success, yet they often suffer from producing unnecessary and verbose reasoning chains. We identify a core aspect of this issue as "invalid thinking" -- models tend to repeatedly double-check their work after having derived the correct answer. To address this specific inefficiency, we move beyond the general principles of Efficacy and Efficiency to propose two new, fine-grained principles: Brevity, which advocates for eliminating redundancy, and Sufficiency, which ensures critical reasoning steps are preserved. Guided by these principles, we introduce LC-R1, a post-training method based on Group Relative Policy Optimization (GRPO). LC-R1 employs a novel combination of a Length Reward for overall conciseness and a Compress Reward that is specifically designed to remove the invalid portion of the thinking process. Extensive experiments on multiple reasoning benchmarks demonstrate that LC-R1 achieves a significant reduction in sequence length (~50%) with only a marginal (~2%) drop in accuracy, achieving a favorable trade-off point on the Pareto frontier that prioritizes high compression. Our analysis further validates the robustness of LC-R1 and provides valuable insights for developing more powerful yet computationally efficient LRMs. Our code is released at https://github.com/zxiangx/LC-R1.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [107] [LittleBit: Ultra Low-Bit Quantization via Latent Factorization](https://arxiv.org/abs/2506.13771)
*Banseok Lee,Dongkyu Kim,Youngcheon You,Youngmin Kim*

Main category: cs.LG

TL;DR: 本文提出LittleBit方法，通过低秩矩阵分解、二值化和多尺度补偿机制，在0.1位/权重的极端量化下实现LLM高效压缩，性能超越现有方法并显著降低内存需求。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)部署面临高内存与计算成本挑战，传统量化方法在低于1位/权重(BPW)时存在严重性能下降问题。

Method: 采用低秩潜在矩阵分解表示权重，结合二值化和多尺度补偿机制(行列/潜在维度补偿)，提出Dual-SVID分解稳定量化训练初始化，并集成残差补偿减少误差。

Result: Llama2-13B压缩至0.9GB(31×内存缩减)，0.1BPW性能超越现有0.7BPW方法；内核级基准测试显示相比FP16可实现5×加速。

Conclusion: LittleBit建立了更优的尺寸-性能平衡，为资源受限环境部署LLM开辟新路径，在极端量化领域具有显著优势。

Abstract: Deploying large language models (LLMs) often faces challenges from substantial memory and computational costs. Quantization offers a solution, yet performance degradation in the sub-1-bit regime remains particularly difficult. This paper introduces LittleBit, a novel method for extreme LLM compression. It targets levels like 0.1 bits per weight (BPW), achieving nearly 31$\times$ memory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents weights in a low-rank form using latent matrix factorization, subsequently binarizing these factors. To counteract information loss from this extreme precision, it integrates a multi-scale compensation mechanism. This includes row, column, and an additional latent dimension that learns per-rank importance. Two key contributions enable effective training: Dual Sign-Value-Independent Decomposition (Dual-SVID) for stable quantization-aware training (QAT) initialization, and integrated Residual Compensation to mitigate errors. Extensive experiments confirm LittleBit's superiority in sub-1-bit quantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading method's 0.7 BPW. This establishes a superior size-performance trade-off, with kernel-level benchmarks indicating potential for a 5$\times$ speedup compared to FP16. LittleBit paves the way for deploying powerful LLMs in resource-constrained environments.

</details>


### [108] [MobiEdit: Resource-efficient Knowledge Editing for Personalized On-device LLMs](https://arxiv.org/abs/2506.13772)
*Zhenyan Lu,Daliang Xu,Dongqi Cai,Zexi Li,Wei Liu,Fangming Liu,Shangguang Wang,Mengwei Xu*

Main category: cs.LG

TL;DR: MobiEdit是首个在商用移动设备上实现高效大语言模型知识编辑的框架，通过量化前向梯度估计和优化策略，显著降低资源消耗，支持实时个性化更新。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法依赖高资源反向传播，无法在移动设备运行，导致大语言模型处理个性化查询时易产生错误或过时响应。

Method: 使用量化前向梯度估计替代反向传播，兼容移动NPU；引入自适应提前终止机制和前缀缓存复用技术优化计算效率。

Result: 在商用设备上实现3B参数模型实时编辑，相比传统方法内存减少7.6倍、能耗降低14.7倍、延迟减少3.6倍。

Conclusion: MobiEdit突破了移动端大模型知识编辑的技术瓶颈，为设备端智能助手等应用提供了可行的个性化支持方案。

Abstract: Large language models (LLMs) are deployed on mobile devices to power killer applications such as intelligent assistants. LLMs pre-trained on general corpora often hallucinate when handling personalized or unseen queries, leading to incorrect or outdated responses. Knowledge editing addresses this by identifying and adjusting a small crucial portion of model weights, without compromising the general knowledge. However, prior knowledge editing methods are impractical to run on local devices due to the resource-heavy backpropagation (BP) needed for updates. We present MobiEdit, the first mobile knowledge editing framework that enables efficient LLM personalization on commercial off-the-shelf (COTS) mobile devices. MobiEdit replaces full-precision BP with quantized forward-only gradient estimation, thus compatible with the energy-efficient mobile neural processing units (NPUs). MobiEdit replaces full-precision backpropagation with quantized forward-only gradient estimation, making it compatible with energy-efficient mobile NPUs. To further improve gradient estimation efficiency, we introduce two optimizations: an early stoping mechanism that adaptively terminates editing upon success and a prefix cache that reuses computation across steps. Our approach enables real-time editing of a 3B-parameter model (Qwen2.5-3B-Instruct) on COTS mobile devices with 7.6$\times$ less memory, 14.7 $\times$ less energy and 3.6$\times$ less latency compared to previous knowledge editing methods.

</details>


### [109] [Solving the Job Shop Scheduling Problem with Graph Neural Networks: A Customizable Reinforcement Learning Environment](https://arxiv.org/abs/2506.13781)
*Pablo Ariño Fernández*

Main category: cs.LG

TL;DR: 提出JobShopLib模块化库解决作业车间调度问题中深度学习模型定制复杂、实验耗时的问题，通过强化学习环境支持自定义组件，训练出的GNN模型在大规模问题上接近SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统作业车间调度依赖启发式优先级规则，现有深度学习模型（如图神经网络）需定制化图表示、节点特征等要素，但缺乏模块化实验库导致研究效率低下。

Method: 开发JobShopLib模块化库，允许自定义图表示/节点特征/动作空间/奖励函数，并构建强化学习环境；通过模仿学习训练调度器模型。

Result: 仅用单工序特征的模型优于基于图的调度器，GNN模型在大规模问题上接近最优水平，表明特征定制的重要性及模型改进空间。

Conclusion: JobShopLib为未来研究提供工具支持，证明通过灵活定制组件可显著提升调度模型性能，该领域仍有较大优化潜力。

Abstract: The job shop scheduling problem is an NP-hard combinatorial optimization problem relevant to manufacturing and timetabling. Traditional approaches use priority dispatching rules based on simple heuristics. Recent work has attempted to replace these with deep learning models, particularly graph neural networks (GNNs), that learn to assign priorities from data. However, training such models requires customizing numerous factors: graph representation, node features, action space, and reward functions. The lack of modular libraries for experimentation makes this research time-consuming. This work introduces JobShopLib, a modular library that allows customizing these factors and creating new components with its reinforcement learning environment. We trained several dispatchers through imitation learning to demonstrate the environment's utility. One model outperformed various graph-based dispatchers using only individual operation features, highlighting the importance of feature customization. Our GNN model achieved near state-of-the-art results on large-scale problems. These results suggest significant room for improvement in developing such models. JobShopLib provides the necessary tools for future experimentation.

</details>


### [110] [Enhancing Bagging Ensemble Regression with Data Integration for Time Series-Based Diabetes Prediction](https://arxiv.org/abs/2506.13786)
*Vuong M. Ngo,Tran Quang Vinh,Patricia Kearney,Mark Roantree*

Main category: cs.LG

TL;DR: 本研究通过整合2011-2021年糖尿病相关数据，提出增强型EBMBag+集成回归模型，用于美国城市糖尿病患病率预测，结果显示其性能优于其他基线模型。


<details>
  <summary>Details</summary>
Motivation: 糖尿病作为慢性代谢疾病，准确预测州级患病率对医疗资源规划和干预至关重要，但现有数据常不完整性问题影响分析效果。

Method: 整合多源数据构建特征集，开发EBMBag+时间序列预测模型，并与SVMReg、BDTree、LSBoost、NN、LSTM、ERMBag等基线模型进行对比验证。

Result: EBMBag+取得最优指标：MAE 0.41、RMSE 0.53、MAPE 4.01%、R² 0.9，显著优于所有对比模型。

Conclusion: EBMBag+模型在糖尿病患病率预测中展现卓越性能，为公共卫生决策提供了高精度预测工具。

Abstract: Diabetes is a chronic metabolic disease characterized by elevated blood glucose levels, leading to complications like heart disease, kidney failure, and nerve damage. Accurate state-level predictions are vital for effective healthcare planning and targeted interventions, but in many cases, data for necessary analyses are incomplete. This study begins with a data engineering process to integrate diabetes-related datasets from 2011 to 2021 to create a comprehensive feature set. We then introduce an enhanced bagging ensemble regression model (EBMBag+) for time series forecasting to predict diabetes prevalence across U.S. cities. Several baseline models, including SVMReg, BDTree, LSBoost, NN, LSTM, and ERMBag, were evaluated for comparison with our EBMBag+ algorithm. The experimental results demonstrate that EBMBag+ achieved the best performance, with an MAE of 0.41, RMSE of 0.53, MAPE of 4.01, and an R2 of 0.9.

</details>


### [111] [Hybrid Meta-Learning Framework for Anomaly Forecasting in Nonlinear Dynamical Systems via Physics-Inspired Simulation and Deep Ensembles](https://arxiv.org/abs/2506.13828)
*Abdullah Burkan Bereketoglu*

Main category: cs.LG

TL;DR: 提出混合元学习框架，结合物理模拟器与多种深度学习模型，用于非线性动态系统的预测与异常检测，在仿真实验中表现优于独立模型。


<details>
  <summary>Details</summary>
Motivation: 针对非线性动态系统中非平稳随机行为导致的预测与异常检测难题，传统物理模型常不完整，需数据驱动方法实现早期缺陷识别。

Method: 集成物理模拟器（捕捉非线性增长-松弛动态）、CNN-LSTM时空特征提取、VAE无监督异常评分、孤立森林残差检测、DA-RNN单步预测，通过元学习器融合多模型输出。

Result: 仿真实验表明混合模型在异常定位、泛化能力及非线性偏差鲁棒性上优于独立模型。

Conclusion: 该框架为非线性系统提供通用数据驱动的预测监测方案，适用于物理模型不完整的多场景应用。

Abstract: We propose a hybrid meta-learning framework for forecasting and anomaly detection in nonlinear dynamical systems characterized by nonstationary and stochastic behavior. The approach integrates a physics-inspired simulator that captures nonlinear growth-relaxation dynamics with random perturbations, representative of many complex physical, industrial, and cyber-physical systems. We use CNN-LSTM architectures for spatio-temporal feature extraction, Variational Autoencoders (VAE) for unsupervised anomaly scoring, and Isolation Forests for residual-based outlier detection in addition to a Dual-Stage Attention Recurrent Neural Network (DA-RNN) for one-step forecasting on top of the generated simulation data. To create composite anomaly forecasts, these models are combined using a meta-learner that combines forecasting outputs, reconstruction errors, and residual scores. The hybrid ensemble performs better than standalone models in anomaly localization, generalization, and robustness to nonlinear deviations, according to simulation-based experiments. The framework provides a broad, data-driven approach to early defect identification and predictive monitoring in nonlinear systems, which may be applied to a variety of scenarios where complete physical models might not be accessible.

</details>


### [112] [Quantifying Structure in CLIP Embeddings: A Statistical Framework for Concept Interpretation](https://arxiv.org/abs/2506.13831)
*Jitian Zhao,Chenghui Li,Frederic Sala,Karl Rohe*

Main category: cs.LG

TL;DR: 本文提出了一种基于假设检验的概念分解方法，用于解释CLIP等深度神经网络模型的嵌入表示，提高了概念的可验证性和方法间的可比性。


<details>
  <summary>Details</summary>
Motivation: 当前基于概念的解释方法缺乏统计严谨性，难以验证识别的概念和比较不同技术。本文旨在解决这一问题。

Method: 引入假设检验框架量化CLIP嵌入空间中的旋转敏感结构，并提出一种后验概念分解方法，具有理论保证。

Result: 该方法在重建误差上优于现有技术，并在消除虚假背景概念后，最差组准确率提高了22.6%。

Conclusion: 该方法在重建准确性和概念可解释性之间取得了良好平衡，有效减少了数据中的虚假线索。

Abstract: Concept-based approaches, which aim to identify human-understandable concepts within a model's internal representations, are a promising method for interpreting embeddings from deep neural network models, such as CLIP. While these approaches help explain model behavior, current methods lack statistical rigor, making it challenging to validate identified concepts and compare different techniques. To address this challenge, we introduce a hypothesis testing framework that quantifies rotation-sensitive structures within the CLIP embedding space. Once such structures are identified, we propose a post-hoc concept decomposition method. Unlike existing approaches, it offers theoretical guarantees that discovered concepts represent robust, reproducible patterns (rather than method-specific artifacts) and outperforms other techniques in terms of reconstruction error. Empirically, we demonstrate that our concept-based decomposition algorithm effectively balances reconstruction accuracy with concept interpretability and helps mitigate spurious cues in data. Applied to a popular spurious correlation dataset, our method yields a 22.6% increase in worst-group accuracy after removing spurious background concepts.

</details>


### [113] [Evolvable Conditional Diffusion](https://arxiv.org/abs/2506.13834)
*Zhao Wei,Chin Chun Ooi,Abhishek Gupta,Jian Cheng Wong,Pao-Hsiung Chiu,Sheares Xue Wen Toh,Yew-Soon Ong*

Main category: cs.LG

TL;DR: 本文提出一种可进化条件扩散方法，利用黑箱、不可微多物理场模型引导生成过程，实现自主科学发现。该方法通过优化去噪分布统计量，无需计算梯度，在流体拓扑和超表面设计中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 科学计算中广泛存在黑箱、不可微多物理场模型（如流体力学、电磁学），传统基于梯度的扩散模型难以直接利用这些模型进行生成优化。需要一种无需可微代理的引导方法。

Method: 将多物理模型引导建模为基于描述统计量优化的进化问题，从概率演化理论推导出进化引导条件扩散算法，其更新规则与梯度引导扩散模型类似但无需计算导数。

Result: 在流体拓扑优化和超表面自动设计场景验证表明，该方法生成的样本比基线方法更优，且不依赖可微代理模型，可直接利用现有科学计算工具。

Conclusion: 进化条件扩散为非可微物理模型提供有效引导机制，扩展了扩散模型在科学发现中的应用范围，为复杂物理约束下的生成设计提供新途径。

Abstract: This paper presents an evolvable conditional diffusion method such that black-box, non-differentiable multi-physics models, as are common in domains like computational fluid dynamics and electromagnetics, can be effectively used for guiding the generative process to facilitate autonomous scientific discovery. We formulate the guidance as an optimization problem where one optimizes for a desired fitness function through updates to the descriptive statistic for the denoising distribution, and derive an evolution-guided approach from first principles through the lens of probabilistic evolution. Interestingly, the final derived update algorithm is analogous to the update as per common gradient-based guided diffusion models, but without ever having to compute any derivatives. We validate our proposed evolvable diffusion algorithm in two AI for Science scenarios: the automated design of fluidic topology and meta-surface. Results demonstrate that this method effectively generates designs that better satisfy specific optimization objectives without reliance on differentiable proxies, providing an effective means of guidance-based diffusion that can capitalize on the wealth of black-box, non-differentiable multi-physics numerical models common across Science.

</details>


### [114] [Robustness of Reinforcement Learning-Based Traffic Signal Control under Incidents: A Comparative Study](https://arxiv.org/abs/2506.13836)
*Dang Viet Anh Nguyen,Carlos Lima Azevedo,Tomer Toledo,Filipe Rodrigues*

Main category: cs.LG

TL;DR: 本文提出T-REX开源仿真平台，评估强化学习交通信号控制（RL-TSC）在交通事件干扰下的鲁棒性。研究发现独立值基与分散压力基方法在稳定场景表现良好，但事故场景性能骤降；分层协调方法在大规模不规则场景更稳定，但需权衡收敛速度与复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有RL-TSC研究对真实交通事件（如事故）干扰下的鲁棒性缺乏深入探索，需建立标准化评估框架以支持动态场景下的方法验证。

Method: 开发基于SUMO的T-REX仿真框架，模拟驾驶员改道、速度调整及换道行为，提出扩展传统效率指标的鲁棒性评估指标，并在合成与真实路网中测试多种RL-TSC方法。

Result: 独立值基与分散压力基方法在稳定场景收敛快、泛化好，但事故导致分布偏移时性能显著下降；分层协调方法在复杂路网中稳定性更高，但收敛慢且训练复杂度高。

Conclusion: RL-TSC需关注鲁棒性设计与评估，T-REX通过开源标准化平台支持动态干扰场景的基准测试，为方法优化提供结构化决策架构参考。

Abstract: Reinforcement learning-based traffic signal control (RL-TSC) has emerged as a promising approach for improving urban mobility. However, its robustness under real-world disruptions such as traffic incidents remains largely underexplored. In this study, we introduce T-REX, an open-source, SUMO-based simulation framework for training and evaluating RL-TSC methods under dynamic, incident scenarios. T-REX models realistic network-level performance considering drivers' probabilistic rerouting, speed adaptation, and contextual lane-changing, enabling the simulation of congestion propagation under incidents. To assess robustness, we propose a suite of metrics that extend beyond conventional traffic efficiency measures. Through extensive experiments across synthetic and real-world networks, we showcase T-REX for the evaluation of several state-of-the-art RL-TSC methods under multiple real-world deployment paradigms. Our findings show that while independent value-based and decentralized pressure-based methods offer fast convergence and generalization in stable traffic conditions and homogeneous networks, their performance degrades sharply under incident-driven distribution shifts. In contrast, hierarchical coordination methods tend to offer more stable and adaptable performance in large-scale, irregular networks, benefiting from their structured decision-making architecture. However, this comes with the trade-off of slower convergence and higher training complexity. These findings highlight the need for robustness-aware design and evaluation in RL-TSC research. T-REX contributes to this effort by providing an open, standardized and reproducible platform for benchmarking RL methods under dynamic and disruptive traffic scenarios.

</details>


### [115] [Sustainable Machine Learning Retraining: Optimizing Energy Efficiency Without Compromising Accuracy](https://arxiv.org/abs/2506.13838)
*Lorena Poenaru-Olaru,June Sallou,Luis Cruz,Jan Rellermeyer,Arie van Deursen*

Main category: cs.LG

TL;DR: 研究探讨了机器学习模型重训练技术的能源效率与准确性，发现仅使用最新数据或按需重训练可显著降低能耗，为可持续ML系统设计提供建议。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统因数据变化需定期重训练模型，但传统方法计算资源消耗大，导致高能耗与环境问题，需探索更可持续的重训练策略。

Method: 通过对比不同重训练技术的能源消耗与模型准确性，分析使用最新数据替代全部数据、按需触发重训练与固定计划重训练的差异。

Result: 仅用最新数据重训练可减少25%能耗；基于数据变化检测的按需重训练可降低40%能耗，且模型准确性未显著影响。

Conclusion: 推荐采用增量数据更新与按需触发机制作为可持续ML系统设计的优先重训练策略，平衡能效与模型性能。

Abstract: The reliability of machine learning (ML) software systems is heavily influenced by changes in data over time. For that reason, ML systems require regular maintenance, typically based on model retraining. However, retraining requires significant computational demand, which makes it energy-intensive and raises concerns about its environmental impact. To understand which retraining techniques should be considered when designing sustainable ML applications, in this work, we study the energy consumption of common retraining techniques. Since the accuracy of ML systems is also essential, we compare retraining techniques in terms of both energy efficiency and accuracy. We showcase that retraining with only the most recent data, compared to all available data, reduces energy consumption by up to 25\%, being a sustainable alternative to the status quo. Furthermore, our findings show that retraining a model only when there is evidence that updates are necessary, rather than on a fixed schedule, can reduce energy consumption by up to 40\%, provided a reliable data change detector is in place. Our findings pave the way for better recommendations for ML practitioners, guiding them toward more energy-efficient retraining techniques when designing sustainable ML software systems.

</details>


### [116] [SatHealth: A Multimodal Public Health Dataset with Satellite-based Environmental Factors](https://arxiv.org/abs/2506.13842)
*Yuanlong Wang,Pengqi Wang,Changchang Yin,Ping Zhang*

Main category: cs.LG

TL;DR: SatHealth数据集整合多模态时空数据，提升AI模型在公共卫生和疾病预测中的性能与泛化能力，并提供在线工具促进研究应用。


<details>
  <summary>Details</summary>
Motivation: 现有公共卫生研究因缺乏长期细粒度时空数据，难以整合环境信息，限制了AI模型的实用性和性能。

Method: 构建SatHealth数据集，包含环境数据、卫星影像、全疾病流行率估计及社会健康决定因素，并通过区域与个人健康预测实验验证。

Result: 环境信息显著提高AI模型在多种任务中的性能及时空泛化能力，并部署了支持数据探索和一键访问的Web应用。

Conclusion: SatHealth为医疗研究引入环境数据提供了资源与框架，未来将扩展覆盖全美，推动环境健康信息学发展。

Abstract: Living environments play a vital role in the prevalence and progression of diseases, and understanding their impact on patient's health status becomes increasingly crucial for developing AI models. However, due to the lack of long-term and fine-grained spatial and temporal data in public and population health studies, most existing studies fail to incorporate environmental data, limiting the models' performance and real-world application. To address this shortage, we developed SatHealth, a novel dataset combining multimodal spatiotemporal data, including environmental data, satellite images, all-disease prevalences estimated from medical claims, and social determinants of health (SDoH) indicators. We conducted experiments under two use cases with SatHealth: regional public health modeling and personal disease risk prediction. Experimental results show that living environmental information can significantly improve AI models' performance and temporal-spatial generalizability on various tasks. Finally, we deploy a web-based application to provide an exploration tool for SatHealth and one-click access to both our data and regional environmental embedding to facilitate plug-and-play utilization. SatHealth is now published with data in Ohio, and we will keep updating SatHealth to cover the other parts of the US. With the web application and published code pipeline, our work provides valuable angles and resources to include environmental data in healthcare research and establishes a foundational framework for future research in environmental health informatics.

</details>


### [117] [StaQ it! Growing neural networks for Policy Mirror Descent](https://arxiv.org/abs/2506.13862)
*Alena Shilova,Alex Davey,Brahim Driss,Riad Akrour*

Main category: cs.LG

TL;DR: 本文提出StaQ算法，通过仅保留最近M个Q函数解决策略镜像下降(PMD)框架在深度强化学习中的存储难题，在保证理论收敛性的同时实现稳定策略更新，性能优于基线且波动更小。


<details>
  <summary>Details</summary>
Motivation: 传统PMD框架因需存储所有历史Q函数导致计算不可行，而现有深度RL实现(如SAC/TRPO)在策略更新中引入近似误差。需要一种既能保持PMD理论优势，又能实际部署的算法。

Method: 设计类PMD算法，仅维护最近M个Q函数。证明当M足够大时，策略更新可精确计算而不引入误差，构建出可实现的StaQ算法框架。

Result: StaQ获得理论收敛保证，在实验中与深度RL基线竞争力相当，且表现出更低的性能振荡，验证了方法的有效性。

Conclusion: StaQ为开发完全稳定的深度RL算法提供了新路径，同时建立了PMD理论框架与工程实践之间的桥梁，为后续研究提供了实验基准。

Abstract: In Reinforcement Learning (RL), regularization has emerged as a popular tool both in theory and practice, typically based either on an entropy bonus or a Kullback-Leibler divergence that constrains successive policies. In practice, these approaches have been shown to improve exploration, robustness and stability, giving rise to popular Deep RL algorithms such as SAC and TRPO. Policy Mirror Descent (PMD) is a theoretical framework that solves this general regularized policy optimization problem, however the closed-form solution involves the sum of all past Q-functions, which is intractable in practice. We propose and analyze PMD-like algorithms that only keep the last $M$ Q-functions in memory, and show that for finite and large enough $M$, a convergent algorithm can be derived, introducing no error in the policy update, unlike prior deep RL PMD implementations. StaQ, the resulting algorithm, enjoys strong theoretical guarantees and is competitive with deep RL baselines, while exhibiting less performance oscillation, paving the way for fully stable deep RL algorithms and providing a testbed for experimentation with Policy Mirror Descent.

</details>


### [118] [Scaling Algorithm Distillation for Continuous Control with Mamba](https://arxiv.org/abs/2506.13892)
*Samuel Beaussant,Mehdi Mounsif*

Main category: cs.LG

TL;DR: 本文提出使用基于S6层的Mamba模型替代传统Transformer，以解决算法蒸馏（AD）在上下文强化学习（ICRL）中因注意力机制复杂度高导致的训练瓶颈，并在复杂连续任务中验证其优越性及长上下文扩展的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统AD方法因Transformer的二次复杂度限制，仅能应用于简单离散环境且时间跨度短的任务。需一种高效模型处理长序列以提升ICRL性能。

Method: 采用选择性结构化状态空间序列模型（S6）构建Mamba，利用其线性序列长度复杂度，在四个复杂连续元强化学习环境中进行AD实验。

Result: Mamba在AD中整体优于Transformer，且扩展上下文长度可提升ICRL性能，使其与当前最优在线元强化学习基线方法竞争。

Conclusion: Mamba通过S6层解决了AD的长序列建模难题，验证了长上下文对ICRL的重要性，并为复杂任务中的高效算法蒸馏提供了新方向。

Abstract: Algorithm Distillation (AD) was recently proposed as a new approach to perform In-Context Reinforcement Learning (ICRL) by modeling across-episodic training histories autoregressively with a causal transformer model. However, due to practical limitations induced by the attention mechanism, experiments were bottlenecked by the transformer's quadratic complexity and limited to simple discrete environments with short time horizons. In this work, we propose leveraging the recently proposed Selective Structured State Space Sequence (S6) models, which achieved state-of-the-art (SOTA) performance on long-range sequence modeling while scaling linearly in sequence length. Through four complex and continuous Meta Reinforcement Learning environments, we demonstrate the overall superiority of Mamba, a model built with S6 layers, over a transformer model for AD. Additionally, we show that scaling AD to very long contexts can improve ICRL performance and make it competitive even with a SOTA online meta RL baseline.

</details>


### [119] [Enhancing interpretability of rule-based classifiers through feature graphs](https://arxiv.org/abs/2506.13903)
*Christel Sirocchi,Damiano Verda*

Main category: cs.LG

TL;DR: 本文提出了一种用于评估基于规则系统中特征贡献的综合框架，包括图可视化策略、特征重要性指标及规则集距离度量，应用于临床数据集并展示其在识别关键特征和提升诊断准确性方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 在医疗等需透明决策的领域，复杂规则系统因特征交互和规则集对比困难，导致关键特征分析受限，需开发可解释性工具以支持决策。

Method: 提出基于图的特征可视化方法、与规则无关的特征重要性指标，以及基于特征贡献的规则集距离度量，结合四种规则方法（决策树、逻辑学习机等）进行实验验证。

Result: 在临床数据集和15个公共基准测试中，方法成功揭示特征组合的预测价值，并在特征重要性方面表现出竞争性能和更高鲁棒性。

Conclusion: 该框架通过增强规则系统的可解释性，为识别风险因素、生物标志物及优化诊断信息优先级提供了有效工具，具有实际应用价值。

Abstract: In domains where transparency and trustworthiness are crucial, such as healthcare, rule-based systems are widely used and often preferred over black-box models for decision support systems due to their inherent interpretability. However, as rule-based models grow complex, discerning crucial features, understanding their interactions, and comparing feature contributions across different rule sets becomes challenging. To address this, we propose a comprehensive framework for estimating feature contributions in rule-based systems, introducing a graph-based feature visualisation strategy, a novel feature importance metric agnostic to rule-based predictors, and a distance metric for comparing rule sets based on feature contributions. By experimenting on two clinical datasets and four rule-based methods (decision trees, logic learning machines, association rules, and neural networks with rule extraction), we showcase our method's capability to uncover novel insights on the combined predictive value of clinical features, both at the dataset and class-specific levels. These insights can aid in identifying new risk factors, signature genes, and potential biomarkers, and determining the subset of patient information that should be prioritised to enhance diagnostic accuracy. Comparative analysis of the proposed feature importance score with state-of-the-art methods on 15 public benchmarks demonstrates competitive performance and superior robustness. The method implementation is available on GitHub: https://github.com/ChristelSirocchi/rule-graph.

</details>


### [120] [GITO: Graph-Informed Transformer Operator for Learning Complex Partial Differential Equations](https://arxiv.org/abs/2506.13906)
*Milad Ramezankhani,Janak M. Patel,Anirudh Deodhar,Dagnachew Birru*

Main category: cs.LG

TL;DR: 提出新型图信息转换器算子（GITO），结合图神经网络与Transformer，高效求解不规则几何和非均匀网格的偏微分方程系统，支持零样本超分辨率和网格无关性。


<details>
  <summary>Details</summary>
Motivation: 传统基于Transformer的神经算子在处理复杂几何、非均匀网格及长程依赖时存在局限性，需开发能适应任意离散化且高效的替代求解器。

Method: GITO包含混合图Transformer（HGT）和Transformer神经算子（TNO）。HGT融合图神经网络（局部关系）与Transformer（长程依赖），TNO通过线性复杂度注意力层实现离散不变性和任意位置预测。

Result: 在基准PDE任务中，GITO性能优于现有Transformer神经算子，验证了其零样本超分辨率和跨网格泛化能力。

Conclusion: GITO为工程应用提供高效、网格无关的代理求解器框架，通过图-Transformer混合架构突破传统方法在复杂几何建模中的瓶颈。

Abstract: We present a novel graph-informed transformer operator (GITO) architecture for learning complex partial differential equation systems defined on irregular geometries and non-uniform meshes. GITO consists of two main modules: a hybrid graph transformer (HGT) and a transformer neural operator (TNO). HGT leverages a graph neural network (GNN) to encode local spatial relationships and a transformer to capture long-range dependencies. A self-attention fusion layer integrates the outputs of the GNN and transformer to enable more expressive feature learning on graph-structured data. TNO module employs linear-complexity cross-attention and self-attention layers to map encoded input functions to predictions at arbitrary query locations, ensuring discretization invariance and enabling zero-shot super-resolution across any mesh. Empirical results on benchmark PDE tasks demonstrate that GITO outperforms existing transformer-based neural operators, paving the way for efficient, mesh-agnostic surrogate solvers in engineering applications.

</details>


### [121] [Few-Shot Learning for Industrial Time Series: A Comparative Analysis Using the Example of Screw-Fastening Process Monitoring](https://arxiv.org/abs/2506.13909)
*Xinyuan Tu,Haocheng Zhang,Tao Chengxu,Zuyi Chen*

Main category: cs.LG

TL;DR: 针对工业时序数据的小样本学习，提出标签感知的任务采样器，结合轻量CNN与度量学习，在螺丝紧固过程监测中显著优于大型模型，并开源资源促进应用。


<details>
  <summary>Details</summary>
Motivation: 工业时序数据标注成本高昂，现有小样本学习（FSL）研究多集中于视觉领域，而在工业缺陷检测中缺乏系统性探索。

Method: 提出标签感知任务采样器（将多标签序列拆解为单标签任务），对比基于度量的原型网络与基于梯度的MAML方法，结合1D CNN、InceptionTime和Transformer等骨干网络。

Result: InceptionTime+原型网络在10-shot 3-way评估中加权F1达0.944（多类）和0.935（多标签），参数量与训练时间减少两个数量级；度量学习全面优于MAML，新采样策略提升1.7% F1。

Conclusion: 数据稀缺时，轻量CNN+简单度量学习优于大型基础模型，挑战'模型越大越好'的假设。开源代码/数据推动工业检测中的FSL应用。

Abstract: Few-shot learning (FSL) has shown promise in vision but remains largely unexplored for \emph{industrial} time-series data, where annotating every new defect is prohibitively expensive. We present a systematic FSL study on screw-fastening process monitoring, using a 2\,300-sample multivariate torque dataset that covers 16 uni- and multi-factorial defect types. Beyond benchmarking, we introduce a \textbf{label-aware episodic sampler} that collapses multi-label sequences into multiple single-label tasks, keeping the output dimensionality fixed while preserving combinatorial label information.
  Two FSL paradigms are investigated: the metric-based \emph{Prototypical Network} and the gradient-based \emph{Model-Agnostic Meta-Learning} (MAML), each paired with three backbones: 1D CNN, InceptionTime and the 341 M-parameter transformer \emph{Moment}. On 10-shot, 3-way evaluation, the InceptionTime + Prototypical Network combination achieves a \textbf{0.944 weighted F1} in the multi-class regime and \textbf{0.935} in the multi-label regime, outperforming finetuned Moment by up to 5.3\% while requiring two orders of magnitude fewer parameters and training time. Across all backbones, metric learning consistently surpasses MAML, and our label-aware sampling yields an additional 1.7\% F1 over traditional class-based sampling.
  These findings challenge the assumption that large foundation models are always superior: when data are scarce, lightweight CNN architectures augmented with simple metric learning not only converge faster but also generalize better. We release code, data splits and pre-trained weights to foster reproducible research and to catalyze the adoption of FSL in high-value manufacturing inspection.

</details>


### [122] [Logical Expressiveness of Graph Neural Networks with Hierarchical Node Individualization](https://arxiv.org/abs/2506.13911)
*Arie Soeteman,Balder ten Cate*

Main category: cs.LG

TL;DR: 本文提出分层自我图神经网络（HEGNNs），通过层次化节点个性化增强GNN表达能力，形成可区分同构图的模型层次，并通过逻辑特征刻画其分类能力，实验验证其优于传统GNN。


<details>
  <summary>Details</summary>
Motivation: 传统GNN在区分图结构时表达能力有限，受图同构测试中个体化-精化范式启发，需构建更强大的层次化模型以逼近图同构区分极限。

Method: HEGNNs通过层次化节点个性化扩展GNN，泛化子图GNN，结合分级混合逻辑进行节点分类器形式化描述，并与高阶GNN、局部同态计数增强GNN等方法建立理论关联。

Result: 实验证明HEGNNs具有实际可行性，在有无局部同态计数特征的情况下均优于传统GNN架构，验证了模型层次的理论表达能力提升。

Conclusion: HEGNNs构建了表达能力渐进的模型层次，理论上逼近图同构区分能力，实践性能超越传统方法，为GNN表达能力研究提供新框架。

Abstract: We propose and study Hierarchical Ego Graph Neural Networks (HEGNNs), an expressive extension of graph neural networks (GNNs) with hierarchical node individualization, inspired by the Individualization-Refinement paradigm for graph isomorphism testing. HEGNNs generalize subgraph-GNNs and form a hierarchy of increasingly expressive models that, in the limit, can distinguish graphs up to isomorphism. We provide a logical characterization of HEGNN node classifiers, with and without subgraph restrictions, using graded hybrid logic. This characterization enables us to relate the separating power of HEGNNs to that of higher-order GNNs, GNNs enriched with local homomorphism count features, and color refinement algorithms based on Individualization-Refinement. Our experimental results confirm the practical feasibility of HEGNNs and show benefits in comparison with traditional GNN architectures, both with and without local homomorphism count features.

</details>


### [123] [Branching Stein Variational Gradient Descent for sampling multimodal distributions](https://arxiv.org/abs/2506.13916)
*Isaias Banales,Arturo Jaramillo,Heli Ricalde Guerrero*

Main category: cs.LG

TL;DR: 提出一种基于粒子的变分推断方法BSVGD，通过随机分支机制增强对多模态分布的探索能力，并在理论和实验上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统Stein变分梯度下降(SVGD)算法在处理多模态分布时可能因探索不足而受限，需改进状态空间探索机制。

Method: 在SVGD基础上引入随机分支机制，形成分支Stein变分梯度下降(BSVGD)，通过粒子分裂促进多模态分布采样。

Result: 理论证明算法分布收敛性，实验显示BSVGD相比SVGD在Wasserstein距离和计算效率上表现更优。

Conclusion: BSVGD通过分支机制有效提升多模态分布推断性能，兼具理论保证与计算可行性，为复杂分布推断提供新方案。

Abstract: We propose a novel particle-based variational inference method designed to work with multimodal distributions. Our approach, referred to as Branched Stein Variational Gradient Descent (BSVGD), extends the classical Stein Variational Gradient Descent (SVGD) algorithm by incorporating a random branching mechanism that encourages the exploration of the state space. In this work, a theoretical guarantee for the convergence in distribution is presented, as well as numerical experiments to validate the suitability of our algorithm. Performance comparisons between the BSVGD and the SVGD are presented using the Wasserstein distance between samples and the corresponding computational times.

</details>


### [124] [Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models](https://arxiv.org/abs/2506.13923)
*Vaskar Nath,Elaine Lau,Anisha Gunjal,Manasi Sharma,Nikhil Baharte,Sean Hendryx*

Main category: cs.LG

TL;DR: 本文探讨了基于可验证奖励的强化学习（RLVR）训练推理模型解决新问题的机制，发现其通过压缩pass@k至pass@1和‘能力增益’提升性能，并提出新算法Guide，结合自然语言提示优化训练，显著提升数学基准表现。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解RLVR训练模型解决新问题的机制，探索如何通过自蒸馏和上下文指导提升模型泛化能力，并开发更优的在线训练算法。

Method: 在0.5B至72B参数模型上，对50万+数学、科学、编程问题进行实验；提出Guide算法，动态加入自然语言提示并调整重要性采样比率，优化无提示上下文下的策略。

Result: Guide-GRPO在7B和32B模型上相比基线提升数学基准4%宏观平均准确率，能力增益主要依赖自蒸馏，且自然语言提示显著提高pass@k率。

Conclusion: Guide算法通过上下文提示和策略优化有效提升模型泛化能力，证明自蒸馏是驱动模型解决新问题的核心机制，为RLVR训练提供了新方向。

Abstract: We study the process through which reasoning models trained with reinforcement learning on verifiable rewards (RLVR) can learn to solve new problems. We find that RLVR drives performance through two main means: (1) by compressing pass@$k$ into pass@1 and (2) via "capability gain" in which models learn to solve new problems that they previously could not solve even at high $k$. We find that while capability gain exists across model scales, learning to solve new problems is primarily driven through self-distillation. We demonstrate these findings across model scales ranging from 0.5B to 72B on >500,000 reasoning problems with prompts and verifiable final answers across math, science, and code domains. We further show that we can significantly improve pass@$k$ rates by leveraging natural language guidance for the model to consider within context while still requiring the model to derive a solution chain from scratch. Based of these insights, we derive $\text{Guide}$ - a new class of online training algorithms. $\text{Guide}$ adaptively incorporates hints into the model's context on problems for which all rollouts were initially incorrect and adjusts the importance sampling ratio for the "off-policy" trajectories in order to optimize the policy for contexts in which the hints are no longer present. We describe variants of $\text{Guide}$ for GRPO and PPO and empirically show that Guide-GRPO on 7B and 32B parameter models improves generalization over its vanilla counterpart with up to 4$\%$ macro-average improvement across math benchmarks. We include careful ablations to analyze $\text{Guide}$'s components and theoretically analyze Guide's learning efficiency.

</details>


### [125] [ReinDSplit: Reinforced Dynamic Split Learning for Pest Recognition in Precision Agriculture](https://arxiv.org/abs/2506.13935)
*Vishesh Kumar Tanwar,Soumik Sarkar,Asheesh K. Singh,Sajal K. Das*

Main category: cs.LG

TL;DR: 本文提出ReinDSplit框架，利用强化学习动态调整分割点，解决农业边缘设备异构性导致的效率问题，在保持精度的同时优化资源利用率。


<details>
  <summary>Details</summary>
Motivation: 传统分割学习框架在农业场景中采用统一分割策略，无法适应边缘设备在算力、能耗和连接性上的高度异构性，导致训练效率低下与模型性能下降。

Method: 基于Q学习的强化学习代理作为自适应协调器，将分割层选择建模为有限状态马尔可夫决策过程，动态为每个设备定制DNN分割点以平衡负载与延迟阈值。

Result: 在ResNet18、GoogleNet和MobileNetV2模型上测试三个昆虫分类数据集，其中MobileNetV2达到94.31%准确率，验证了框架有效性。

Conclusion: ReinDSplit通过强化学习与分割学习的结合，为异构环境下的资源效率、隐私保护和可扩展性提供了新思路，其范式可扩展至农业以外的领域。

Abstract: To empower precision agriculture through distributed machine learning (DML), split learning (SL) has emerged as a promising paradigm, partitioning deep neural networks (DNNs) between edge devices and servers to reduce computational burdens and preserve data privacy. However, conventional SL frameworks' one-split-fits-all strategy is a critical limitation in agricultural ecosystems where edge insect monitoring devices exhibit vast heterogeneity in computational power, energy constraints, and connectivity. This leads to straggler bottlenecks, inefficient resource utilization, and compromised model performance. Bridging this gap, we introduce ReinDSplit, a novel reinforcement learning (RL)-driven framework that dynamically tailors DNN split points for each device, optimizing efficiency without sacrificing accuracy. Specifically, a Q-learning agent acts as an adaptive orchestrator, balancing workloads and latency thresholds across devices to mitigate computational starvation or overload. By framing split layer selection as a finite-state Markov decision process, ReinDSplit convergence ensures that highly constrained devices contribute meaningfully to model training over time. Evaluated on three insect classification datasets using ResNet18, GoogleNet, and MobileNetV2, ReinDSplit achieves 94.31% accuracy with MobileNetV2. Beyond agriculture, ReinDSplit pioneers a paradigm shift in SL by harmonizing RL for resource efficiency, privacy, and scalability in heterogeneous environments.

</details>


### [126] [Toward Explainable Offline RL: Analyzing Representations in Intrinsically Motivated Decision Transformers](https://arxiv.org/abs/2506.13958)
*Leonardo Guiducci,Antonio Rizzo,Giovanna Maria Dimitri*

Main category: cs.LG

TL;DR: 本文提出一种事后可解释性框架，揭示弹性决策变换器(EDTs)中内在动机机制通过塑造嵌入几何结构提升策略学习的机理。研究发现不同内在动机变体会形成截然不同的表示结构，且这些结构的度量指标与性能存在环境特定的相关性，表明内在动机不仅是探索奖励，更作为表示先验以生物合理的方式优化决策表示。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明内在动机能提升EDTs在探索任务中的表现，但其对表示学习的影响机制尚不明确。本文旨在系统分析内在动机如何塑造EDTs的嵌入表示空间，揭示其提升策略学习的表征层面机理。

Method: 开发系统性事后可解释性框架，通过统计分析嵌入向量的协方差结构、向量模长和正交性等几何属性，比较不同内在动机变体在EDTs中形成的表示结构差异。

Result: 实验发现：(1)不同内在动机机制会形成具有协方差结构、向量分布等几何特征显著差异的表示空间；(2)嵌入几何指标与性能存在环境依赖的相关模式；(3)内在动机通过优化表示结构而非单纯探索奖励来提升决策效果。

Conclusion: 内在动机在EDTs中充当表示先验，以生物合理的方式塑造嵌入几何结构，形成适应环境特性的组织模式。这种对表示空间的系统性优化是其提升决策效果的本质原因，为设计更高效的强化学习架构提供了理论依据。

Abstract: Elastic Decision Transformers (EDTs) have proved to be particularly successful in offline reinforcement learning, offering a flexible framework that unifies sequence modeling with decision-making under uncertainty. Recent research has shown that incorporating intrinsic motivation mechanisms into EDTs improves performance across exploration tasks, yet the representational mechanisms underlying these improvements remain unexplored. In this paper, we introduce a systematic post-hoc explainability framework to analyze how intrinsic motivation shapes learned embeddings in EDTs. Through statistical analysis of embedding properties (including covariance structure, vector magnitudes, and orthogonality), we reveal that different intrinsic motivation variants create fundamentally different representational structures. Our analysis demonstrates environment-specific correlation patterns between embedding metrics and performance that explain why intrinsic motivation improves policy learning. These findings show that intrinsic motivation operates beyond simple exploration bonuses, acting as a representational prior that shapes embedding geometry in biologically plausible ways, creating environment-specific organizational structures that facilitate better decision-making.

</details>


### [127] [Membership Inference Attacks as Privacy Tools: Reliability, Disparity and Ensemble](https://arxiv.org/abs/2506.13972)
*Zhiqi Wang,Chengyu Zhang,Yuetian Chen,Nathalie Baracaldo,Swanand Kadhe,Lei Yu*

Main category: cs.LG

TL;DR: 本文系统研究了成员推理攻击（MIAs）在隐私评估中的差异性，提出基于覆盖率和稳定性分析的框架，并通过集成策略提升攻击效果与评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有MIAs研究过度关注性能指标（如AUC、准确率），而忽视了不同攻击方法间及同一方法多次实例化间的显著差异，这可能导致隐私评估工具不可靠或不全面。

Method: 提出基于覆盖率和稳定性分析的新框架，通过三种集成策略整合前沿MIAs方法，以协调其优势并解决差异性。

Result: 实验表明MIAs存在显著差异性，揭示了潜在原因及其对隐私评估的影响。集成框架能构建更强攻击并提供更鲁棒的评估方法。

Conclusion: 通过系统性分析MIAs差异并设计集成框架，本研究不仅增强了攻击能力，还为隐私评估提供了更全面、鲁棒的方法论基础。

Abstract: Membership inference attacks (MIAs) pose a significant threat to the privacy of machine learning models and are widely used as tools for privacy assessment, auditing, and machine unlearning. While prior MIA research has primarily focused on performance metrics such as AUC, accuracy, and TPR@low FPR - either by developing new methods to enhance these metrics or using them to evaluate privacy solutions - we found that it overlooks the disparities among different attacks. These disparities, both between distinct attack methods and between multiple instantiations of the same method, have crucial implications for the reliability and completeness of MIAs as privacy evaluation tools. In this paper, we systematically investigate these disparities through a novel framework based on coverage and stability analysis. Extensive experiments reveal significant disparities in MIAs, their potential causes, and their broader implications for privacy evaluation. To address these challenges, we propose an ensemble framework with three distinct strategies to harness the strengths of state-of-the-art MIAs while accounting for their disparities. This framework not only enables the construction of more powerful attacks but also provides a more robust and comprehensive methodology for privacy evaluation.

</details>


### [128] [Constant Stepsize Local GD for Logistic Regression: Acceleration by Instability](https://arxiv.org/abs/2506.13974)
*Michael Crawshaw,Blake Woodworth,Mingrui Liu*

Main category: cs.LG

TL;DR: 本文分析了在异构数据下使用任意步长的本地梯度下降法进行逻辑回归的收敛性，突破了现有方法对步长的限制，并展示了更优的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 现有针对异构目标的本地梯度下降分析要求步长η≤1/K以保证目标函数单调递减，但实际中可能需要更大步长。本文旨在探索任意步长下本地梯度下降在可分异构数据场景中的行为。

Method: 针对逻辑回归模型，在数据可分且异构的设定下，分析本地梯度下降（Local Gradient Descent）在任意步长η>0时的收敛行为，特别关注大本地更新与目标异构性带来的不稳定性。

Result: 在经历初始不稳定阶段（持续约ηKM轮）后，算法以速率O(1/ηKR)收敛，优于一般光滑凸目标下的O(1/R)速率。不稳定性主要源于大本地更新与目标异构性。

Conclusion: 突破传统步长限制，证明大步长下本地梯度下降在可分异构数据中仍可收敛，揭示了异构本地更新对稳定性的影响，为分布式优化提供新理论支撑。

Abstract: Existing analysis of Local (Stochastic) Gradient Descent for heterogeneous objectives requires stepsizes $η\leq 1/K$ where $K$ is the communication interval, which ensures monotonic decrease of the objective. In contrast, we analyze Local Gradient Descent for logistic regression with separable, heterogeneous data using any stepsize $η> 0$. With $R$ communication rounds and $M$ clients, we show convergence at a rate $\mathcal{O}(1/ηK R)$ after an initial unstable phase lasting for $\widetilde{\mathcal{O}}(ηK M)$ rounds. This improves upon the existing $\mathcal{O}(1/R)$ rate for general smooth, convex objectives. Our analysis parallels the single machine analysis of~\cite{wu2024large} in which instability is caused by extremely large stepsizes, but in our setting another source of instability is large local updates with heterogeneous objectives.

</details>


### [129] [HAELT: A Hybrid Attentive Ensemble Learning Transformer Framework for High-Frequency Stock Price Forecasting](https://arxiv.org/abs/2506.13981)
*Thanh Dan Bui*

Main category: cs.LG

TL;DR: 提出混合注意力集成学习Transformer(HAELT)框架，通过ResNet降噪、时间自注意力机制和LSTM-Transformer混合架构，有效解决高频股价预测的非平稳性与噪声问题。


<details>
  <summary>Details</summary>
Motivation: 高频股票预测面临非平稳性、噪声干扰和市场波动性三大核心挑战，传统方法难以有效捕捉复杂时序特征。

Method: 整合ResNet降噪模块、时序自注意力机制和LSTM-Transformer混合架构，通过自适应集成机制动态组合各组件输出。

Result: 在2024-2025年苹果公司小时级数据测试中，HAELT取得最高F1分数，成功识别股价双向波动模式。

Conclusion: HAELT框架展现出强大的金融时序建模能力，为算法交易系统提供了可靠的预测解决方案。

Abstract: High-frequency stock price prediction is challenging due to non-stationarity, noise, and volatility. To tackle these issues, we propose the Hybrid Attentive Ensemble Learning Transformer (HAELT), a deep learning framework combining a ResNet-based noise-mitigation module, temporal self-attention for dynamic focus on relevant history, and a hybrid LSTM-Transformer core that captures both local and long-range dependencies. These components are adaptively ensembled based on recent performance. Evaluated on hourly Apple Inc. (AAPL) data from Jan 2024 to May 2025, HAELT achieves the highest F1-Score on the test set, effectively identifying both upward and downward price movements. This demonstrates HAELT's potential for robust, practical financial forecasting and algorithmic trading.

</details>


### [130] [Quantum-Informed Contrastive Learning with Dynamic Mixup Augmentation for Class-Imbalanced Expert Systems](https://arxiv.org/abs/2506.13987)
*Md Abrar Jahin,Adiba Abid,M. F. Mridha*

Main category: cs.LG

TL;DR: 提出QCL-MixNet框架，结合量子启发的对比学习与动态mixup，解决类别不平衡表格数据中的分类问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如成本敏感学习、过采样、图神经网络）在处理类别不平衡数据时存在过拟合、标签噪声和低密度区域泛化差的问题，需开发更鲁棒的分类框架。

Method: QCL-MixNet包含三个核心创新：1）量子纠缠启发的特征交互层；2）kNN引导的自适应样本混合策略；3）融合多种损失的混合目标函数，提升类内紧凑性与类间分离性。

Result: 在18个真实不平衡数据集上，QCL-MixNet的macro-F1和召回率显著优于20种前沿基线方法，消融实验验证各组件必要性。

Conclusion: QCL-MixNet通过理论分析与实验验证，成为专家系统处理表格不平衡问题的新基准，具备强表达力、泛化性与优化鲁棒性。

Abstract: Expert systems often operate in domains characterized by class-imbalanced tabular data, where detecting rare but critical instances is essential for safety and reliability. While conventional approaches, such as cost-sensitive learning, oversampling, and graph neural networks, provide partial solutions, they suffer from drawbacks like overfitting, label noise, and poor generalization in low-density regions. To address these challenges, we propose QCL-MixNet, a novel Quantum-Informed Contrastive Learning framework augmented with k-nearest neighbor (kNN) guided dynamic mixup for robust classification under imbalance. QCL-MixNet integrates three core innovations: (i) a Quantum Entanglement-inspired layer that models complex feature interactions through sinusoidal transformations and gated attention, (ii) a sample-aware mixup strategy that adaptively interpolates feature representations of semantically similar instances to enhance minority class representation, and (iii) a hybrid loss function that unifies focal reweighting, supervised contrastive learning, triplet margin loss, and variance regularization to improve both intra-class compactness and inter-class separability. Extensive experiments on 18 real-world imbalanced datasets (binary and multi-class) demonstrate that QCL-MixNet consistently outperforms 20 state-of-the-art machine learning, deep learning, and GNN-based baselines in macro-F1 and recall, often by substantial margins. Ablation studies further validate the critical role of each architectural component. Our results establish QCL-MixNet as a new benchmark for tabular imbalance handling in expert systems. Theoretical analyses reinforce its expressiveness, generalization, and optimization robustness.

</details>


### [131] [AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science](https://arxiv.org/abs/2506.13992)
*An Luo,Xun Xian,Jin Du,Fangqiao Tian,Ganghua Wang,Ming Zhong,Shengchun Zhao,Xuan Bi,Zirui Liu,Jiawei Zhou,Jayanth Srinivasa,Ashish Kundu,Charles Fleming,Mingyi Hong,Jie Ding*

Main category: cs.LG

TL;DR: 研究通过AssistedDS基准测试评估大语言模型在数据科学任务中处理领域知识的能力，发现模型存在不加批判采纳信息、对抗性内容负面影响显著、时间序列处理错误等问题，揭示当前模型在知识评估与应用的不足。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型能否像人类数据科学家一样批判性利用外部领域知识，以提升自动化数据科学流程的鲁棒性。

Method: 构建AssistedDS基准测试，包含合成数据集（明确生成机制）和真实Kaggle竞赛数据，并配套有益/对抗性领域文档，评估模型在数据清洗、特征工程、模型选择中辨别与应用知识的能力。

Result: 1) 模型常不加批判采纳对抗性内容导致预测性能下降；2) 有益指导难以抵消对抗信息影响；3) 处理时间序列数据、跨折叠特征工程一致性和分类变量时存在高频错误。

Conclusion: 当前模型缺乏对领域知识的批判性评估能力，需开发更鲁棒的知识感知系统以缩小与人类数据科学家的差距。

Abstract: Large language models (LLMs) have advanced the automation of data science workflows. Yet it remains unclear whether they can critically leverage external domain knowledge as human data scientists do in practice. To answer this question, we introduce AssistedDS (Assisted Data Science), a benchmark designed to systematically evaluate how LLMs handle domain knowledge in tabular prediction tasks. AssistedDS features both synthetic datasets with explicitly known generative mechanisms and real-world Kaggle competitions, each accompanied by curated bundles of helpful and adversarial documents. These documents provide domain-specific insights into data cleaning, feature engineering, and model selection. We assess state-of-the-art LLMs on their ability to discern and apply beneficial versus harmful domain knowledge, evaluating submission validity, information recall, and predictive performance. Our results demonstrate three key findings: (1) LLMs frequently exhibit an uncritical adoption of provided information, significantly impairing their predictive performance when adversarial content is introduced, (2) helpful guidance is often insufficient to counteract the negative influence of adversarial information, and (3) in Kaggle datasets, LLMs often make errors in handling time-series data, applying consistent feature engineering across different folds, and interpreting categorical variables correctly. These findings highlight a substantial gap in current models' ability to critically evaluate and leverage expert knowledge, underscoring an essential research direction for developing more robust, knowledge-aware automated data science systems.

</details>


### [132] [Arctic Long Sequence Training: Scalable And Efficient Training For Multi-Million Token Sequences](https://arxiv.org/abs/2506.13996)
*Stas Bekman,Samyam Rajbhandari,Michael Wyatt,Jeff Rasley,Tunji Ruwase,Zhewei Yao,Aurick Qiao,Yuxiong He*

Main category: cs.LG

TL;DR: ALST技术通过单GPU和多GPU内存优化，支持Hugging Face模型进行超长序列训练，显著提升训练序列长度。


<details>
  <summary>Details</summary>
Motivation: 长序列训练在开源社区面临系统支持不足的挑战，现有解决方案难以直接应用于Hugging Face模型，导致长序列训练难以实现。

Method: 提出Arctic Long Sequence Training (ALST)，结合注意力无关的单GPU和多GPU内存优化技术。

Result: ALST支持单H100 GPU训练50万序列长度，8xH100节点训练370万序列长度，4节点集群训练超过1500万序列长度，相比32K基线提升400倍。

Conclusion: ALST技术为Hugging Face模型提供了高效的长序列训练解决方案，并已开源。

Abstract: Long sequences are critical for applications like RAG, long document summarization, multi-modality, etc., and modern LLMs, like Llama 4 Scout, support max sequence length of up to 10 million tokens. However, outside of enterprise labs, long sequence training is challenging for the AI community with limited system support in the open-source space.
  Out-of-box, even on a modern NVIDIA H100 80GB GPU cluster, training Llama 8B model with sequence over 32K runs out of memory on a basic Hugging Face (HF) model due to two reasons: i) LLM training workloads are not optimized to fully leverage a single GPU memory, ii) existing solutions for leveraging multiple GPU memory are not easily available to HF models, making long sequence training inaccessible.
  We address this with Arctic Long Sequence Training (ALST). It offers a combination of attention-agnostic single GPU and multi-GPU memory optimizations, that enables it to support out-of-box training of multi-million sequence length for a wide variety of HF models.
  ALST supports training Meta's Llama 8B model with 500K sequence length on a single H100 GPU, 3.7M on a single 8xH100 GPU node, and over 15M on a 4 node cluster, an increase of over 400x compared to the 32K baseline for the latter. ALST is fully compatible with HF models and open-sourced via Deepspeed https://www.deepspeed.ai/tutorials/ulysses-alst-sequence-pallellism/ and Arctic Training https://github.com/snowflakedb/ArcticTraining/blob/main/projects/sequence-parallelism/README.md.

</details>


### [133] [Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse Autoencoders](https://arxiv.org/abs/2506.14002)
*Siyu Chen,Heejune Sheen,Xuyuan Xiong,Tianhao Wang,Zhuoran Yang*

Main category: cs.LG

TL;DR: 本文提出了一种基于统计框架和偏置适应技术的稀疏自编码器（SAE）训练算法，首次为特征恢复提供理论保证，并在15亿参数的大型语言模型中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器（SAE）训练算法缺乏数学理论支撑，存在超参数敏感性和训练不稳定等缺陷，限制了其在大型语言模型特征解释中的应用可靠性。

Method: 1. 建立将多义特征建模为单义概念稀疏混合的统计框架；2. 提出基于偏置参数自适应调整算法；3. 开发改进版Group Bias Adaptation（GBA）方法。

Result: 理论证明算法在统计模型下可完全恢复单义特征，实验显示GBA在15亿参数模型上的性能显著优于基准方法。

Conclusion: 该研究为SAE训练提供了首个理论恢复保证，通过增强机制可解释性推动了可信AI系统的发展，是特征解释领域的基础性突破。

Abstract: We study the challenge of achieving theoretically grounded feature recovery using Sparse Autoencoders (SAEs) for the interpretation of Large Language Models. Existing SAE training algorithms often lack rigorous mathematical guarantees and suffer from practical limitations such as hyperparameter sensitivity and instability. To address these issues, we first propose a novel statistical framework for the feature recovery problem, which includes a new notion of feature identifiability by modeling polysemantic features as sparse mixtures of underlying monosemantic concepts. Building on this framework, we introduce a new SAE training algorithm based on ``bias adaptation'', a technique that adaptively adjusts neural network bias parameters to ensure appropriate activation sparsity. We theoretically \highlight{prove that this algorithm correctly recovers all monosemantic features} when input data is sampled from our proposed statistical model. Furthermore, we develop an improved empirical variant, Group Bias Adaptation (GBA), and \highlight{demonstrate its superior performance against benchmark methods when applied to LLMs with up to 1.5 billion parameters}. This work represents a foundational step in demystifying SAE training by providing the first SAE algorithm with theoretical recovery guarantees, thereby advancing the development of more transparent and trustworthy AI systems through enhanced mechanistic interpretability.

</details>


### [134] [Unlearning Isn't Invisible: Detecting Unlearning Traces in LLMs from Model Outputs](https://arxiv.org/abs/2506.14003)
*Yiwei Chen,Soumyadeep Pal,Yimeng Zhang,Qing Qu,Sijia Liu*

Main category: cs.LG

TL;DR: 研究发现大语言模型（LLM）在去学习（unlearning）后仍会留下可检测的“指纹”，这些痕迹可通过模型输出或内部激活被识别，揭示逆向工程遗忘信息的新风险。


<details>
  <summary>Details</summary>
Motivation: 去学习技术虽能保护隐私和版权，但可能引入新漏洞。本文旨在验证去学习是否在模型中留下可检测的持久痕迹，并评估其潜在风险。

Method: 通过监督分类器分析模型文本输出及中间激活层，检测去学习痕迹；研究激活空间中的低维流形传播特性。

Result: 实验表明，针对遗忘相关提示的检测准确率超90%，且大模型即使输入无关内容仍保持高可检测性。

Conclusion: 去学习痕迹形成可测量特征，可能被用于逆向推断被遗忘信息，暴露模型是否经过去学习处理的安全隐患。

Abstract: Machine unlearning (MU) for large language models (LLMs), commonly referred to as LLM unlearning, seeks to remove specific undesirable data or knowledge from a trained model, while maintaining its performance on standard tasks. While unlearning plays a vital role in protecting data privacy, enforcing copyright, and mitigating sociotechnical harms in LLMs, we identify a new vulnerability post-unlearning: unlearning trace detection. We discover that unlearning leaves behind persistent ''fingerprints'' in LLMs, detectable traces in both model behavior and internal representations. These traces can be identified from output responses, even when prompted with forget-irrelevant inputs. Specifically, a simple supervised classifier can reliably determine whether a model has undergone unlearning based solely on its textual outputs. Further analysis shows that these traces are embedded in intermediate activations and propagate nonlinearly to the final layer, forming low-dimensional, learnable manifolds in activation space. Through extensive experiments, we show that forget-relevant prompts enable over 90% accuracy in detecting unlearning traces across all model sizes. Even with forget-irrelevant inputs, large LLMs maintain high detectability, demonstrating the broad applicability of unlearning trace detection. These findings reveal that unlearning leaves measurable signatures, introducing a new risk of reverse-engineering forgotten information when a model is identified as unlearned given an input query. Codes are available at [this URL](https://github.com/OPTML-Group/Unlearn-Trace).

</details>


### [135] [Bures-Wasserstein Flow Matching for Graph Generation](https://arxiv.org/abs/2506.14020)
*Keyue Jiang,Jiahao Cui,Xiaowen Dong,Laura Toni*

Main category: cs.LG

TL;DR: 本文提出了一种基于最优传输和马尔可夫随机场的图生成框架BWFlow，通过联合建模节点与边的演化关系，克服传统方法在非欧几何结构上的局限性，实现更平滑的概率路径和稳定采样收敛。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散/流模型的图生成方法独立建模节点与边演化，并假设数据处于欧氏空间，忽略了图的非欧几何特性与内在关联模式，导致次优生成效果和采样收敛风险。

Method: 将图表示为马尔可夫随机场系统，利用最优传输理论设计概率路径，开发BWFlow框架。该方法支持连续/离散流匹配算法，在概率路径中保持图几何结构并确保速度场平滑性。

Result: 在普通图生成及2D/3D分子生成任务中，BWFlow表现出竞争力性能，训练稳定性强且采样收敛性可证明，实验验证了其有效性。

Conclusion: 通过显式建模图结构的非欧特性与节点-边联合演化关系，BWFlow为图生成提供了更优的概率路径设计范式，在多个应用场景中实现性能与理论保证的平衡。

Abstract: Graph generation has emerged as a critical task in fields ranging from molecule design to drug discovery. Contemporary approaches, notably diffusion and flow-based models, have achieved solid graph generative performance through constructing a probability path that interpolates between a reference distribution and the data distribution. However, these methods typically model the evolution of individual nodes and edges independently and use linear interpolations to build the path assuming that the data lie in Euclidean space. We show that this is suboptimal given the intrinsic non-Euclidean structure and interconnected patterns of graphs, and it poses risks to the sampling convergence. To build a better probability path, we model the joint evolution of the nodes and edges by representing graphs as connected systems parameterized by Markov random fields (MRF). We then leverage the optimal transport displacement between MRF objects to design the probability path for graph generation. Based on this, we introduce BWFlow, a flow-matching framework for graph generation that respects the underlying geometry of graphs and provides smooth velocities in the probability path. The novel framework can be adapted to both continuous and discrete flow-matching algorithms. Experimental evaluations in plain graph generation and 2D/3D molecule generation validate the effectiveness of BWFlow in graph generation with competitive performance, stable training, and guaranteed sampling convergence.

</details>


### [136] [Robust Physics-Informed Neural Network Approach for Estimating Heterogeneous Elastic Properties from Noisy Displacement Data](https://arxiv.org/abs/2506.14036)
*Tatthapong Srikitrungruang,Sina Aghaee Dabaghan Fard,Matthew Lemon,Jaesung Lee,Yuxiao Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种新型逆弹性物理信息神经网络（IE-PINN），通过结合三种独立神经网络架构和两阶段估计策略，有效解决了噪声位移数据下弹性参数（杨氏模量和泊松比）的鲁棒重建问题，显著提升了抗噪性和绝对尺度估计精度。


<details>
  <summary>Details</summary>
Motivation: 现有逆弹性参数估计方法存在不稳定性、对测量噪声敏感及难以恢复绝对尺度杨氏模量等关键问题，限制了其在临床成像诊断和机械特性分析等高噪声场景中的应用。

Method: IE-PINN采用三个独立架构分别建模位移场、应变场和弹性参数分布，结合两阶段估计策略（先恢复相对分布，后通过边界条件校准绝对尺度），并引入位置编码、正弦激活函数和预训练协议以增强性能。

Result: 数值实验表明，IE-PINN在强噪声条件下仍能准确估计绝对尺度弹性参数，克服了现有方法的局限性，其稳定性与精度显著优于传统技术。

Conclusion: 该方法为临床诊断和材料力学表征提供了高噪声鲁棒性的弹性参数估计方案，具有重要应用潜力。

Abstract: Accurately estimating spatially heterogeneous elasticity parameters, particularly Young's modulus and Poisson's ratio, from noisy displacement measurements remains significantly challenging in inverse elasticity problems. Existing inverse estimation techniques are often limited by instability, pronounced sensitivity to measurement noise, and difficulty in recovering absolute-scale Young's modulus. This work presents a novel Inverse Elasticity Physics-Informed Neural Network (IE-PINN) specifically designed to robustly reconstruct heterogeneous distributions of elasticity parameters from noisy displacement data based on linear elasticity physics. IE-PINN integrates three distinct neural network architectures dedicated to separately modeling displacement fields, strain fields, and elasticity distributions, thereby significantly enhancing stability and accuracy against measurement noise. Additionally, a two-phase estimation strategy is introduced: the first phase recovers relative spatial distributions of Young's modulus and Poisson's ratio, and the second phase calibrates the absolute scale of Young's modulus using imposed loading boundary conditions. Additional methodological innovations, including positional encoding, sine activation functions, and a sequential pretraining protocol, further enhance the model's performance and robustness. Extensive numerical experiments demonstrate that IE-PINN effectively overcomes critical limitations encountered by existing methods, delivering accurate absolute-scale elasticity estimations even under severe noise conditions. This advancement holds substantial potential for clinical imaging diagnostics and mechanical characterization, where measurements typically encounter substantial noise.

</details>


### [137] [Load Balancing Mixture of Experts with Similarity Preserving Routers](https://arxiv.org/abs/2506.14038)
*Nabil Omi,Siddhartha Sen,Ali Farhadi*

Main category: cs.LG

TL;DR: 本文提出一种新的负载均衡损失函数，通过保持token间关系结构，提升稀疏混合专家模型训练中路由一致性，减少冗余并加速收敛。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏混合专家模型的负载均衡机制常导致路由不一致，使模型学习冗余知识。传统方法虽强制专家均匀分布，但未考虑相似输入的专家选择一致性。

Method: 引入新型负载均衡损失函数，在训练过程中保留token间关系结构，促使相似输入选择相同专家，减少路由波动。

Result: 实验表明，该方法使模型收敛速度提升36%，冗余度减少，且性能优于主流负载均衡损失函数。

Conclusion: 通过保持路由一致性优化负载均衡，显著提升稀疏混合专家模型的训练效率和知识表达能力。

Abstract: Sparse Mixture of Experts (MoE) models offer a scalable and efficient architecture for training large neural networks by activating only a subset of parameters ("experts") for each input. A learned router computes a distribution over these experts, and assigns input tokens to a small subset. However, without auxiliary balancing mechanisms, routers often converge to using only a few experts, severely limiting model capacity and degrading performance. Most current load balancing mechanisms encourage a distribution over experts that resembles a roughly uniform distribution of experts per token. During training, this can result in inconsistent routing behavior, resulting in the model spending its capacity to learn redundant knowledge. We address this by introducing a novel load balancing loss that preserves token-wise relational structure, encouraging consistent expert choices for similar inputs during training. Our experimental results show that applying our loss to the router results in 36% faster convergence and lower redundancy compared to a popular load balancing loss.

</details>


### [138] [Scientifically-Interpretable Reasoning Network (ScIReN): Uncovering the Black-Box of Nature](https://arxiv.org/abs/2506.14054)
*Joshua Fan,Haodi Xu,Feng Tao,Md Nasim,Marc Grimson,Yiqi Luo,Carla P. Gomes*

Main category: cs.LG

TL;DR: 提出ScIReN框架，结合可解释神经网络与基于过程的科学模型，在保持预测精度的同时揭示潜在科学机制。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络缺乏科学可解释性，而基于过程的科学模型参数调整困难且跨尺度预测效果差。需结合两者优势，构建兼具预测精度与科学透明性的模型。

Method: 设计可解释编码器预测科学潜在参数，通过可微分过程解码器生成输出，并引入硬Sigmoid约束层确保参数符合科学先验范围。

Result: 在土壤有机碳流动和生态系统呼吸建模任务中，ScIReN预测精度超越黑盒模型，并能推断输入特征与科学参数的潜在关系。

Conclusion: ScIReN通过融合科学机理与数据驱动方法，在保持科学可解释性的同时发现新机制，解决了传统模型参数依赖与跨尺度预测的局限性。

Abstract: Neural networks are a powerful tool for learning patterns from data. However, they do not respect known scientific laws, nor can they reveal novel scientific insights due to their black-box nature. In contrast, scientific reasoning distills biological or physical principles from observations and controlled experiments, and quantitatively interprets them with process-based models made of mathematical equations. Yet, process-based models rely on numerous free parameters that must be set in an ad-hoc manner, and thus often fit observations poorly in cross-scale predictions. While prior work has embedded process-based models in conventional neural networks, discovering interpretable relationships between parameters in process-based models and input features is still a grand challenge for scientific discovery. We thus propose Scientifically-Interpretable Reasoning Network (ScIReN), a fully-transparent framework that combines interpretable neural and process-based reasoning. An interpretable encoder predicts scientifically-meaningful latent parameters, which are then passed through a differentiable process-based decoder to predict labeled output variables. ScIReN also uses a novel hard-sigmoid constraint layer to restrict latent parameters to meaningful ranges defined by scientific prior knowledge, further enhancing its interpretability. While the embedded process-based model enforces established scientific knowledge, the encoder reveals new scientific mechanisms and relationships hidden in conventional black-box models. We apply ScIReN on two tasks: simulating the flow of organic carbon through soils, and modeling ecosystem respiration from plants. In both tasks, ScIReN outperforms black-box networks in predictive accuracy while providing substantial scientific interpretability -- it can infer latent scientific mechanisms and their relationships with input features.

</details>


### [139] [A Regret Perspective on Online Selective Generation](https://arxiv.org/abs/2506.14067)
*Minjae Lee,Yoonjae Jung,Sangdon Park*

Main category: cs.LG

TL;DR: 本文提出一种基于部分反馈的在线选择性生成算法，通过将问题转化为多臂老虎机模型并利用反馈解锁机制，有效控制大语言模型的幻觉效应，同时保持回答效率。


<details>
  <summary>Details</summary>
Motivation: 现有选择性生成方法在非随机环境和部分用户反馈（如点赞/点踩）下的学习机制缺失，导致模型幻觉问题难以有效控制，亟需适应此类实际场景的在线学习方法。

Method: 将选择性生成建模为多臂老虎机问题，提出反馈解锁机制以利用选择生成任务特有的臂结构，将老虎机算法的遗憾边界与选择性生成的错误发现率（FDR）理论关联，并兼容任意老虎机算法。

Result: 理论证明与多场景实验表明，该算法在部分反馈下能稳定控制目标FDR，且选择效率（非弃答率）显著优于基线方法，收敛速度因反馈解锁机制得到提升。

Conclusion: 所提出的在线学习框架通过结合老虎机理论与反馈解锁机制，为实际交互场景中的选择性生成提供了兼顾幻觉控制与回答效率的解决方案。

Abstract: Large language generative models increasingly interact with humans, while their falsified responses raise concerns. To address this hallucination effect, selectively abstaining from answering, called selective generation, provides an effective way for generators to control the hallucination when it is unsure of their answers. However, as selective generators are interacting under non-stochastic environments and having partial feedback from users on selective generation (e.g., thumbs up or down on the selected answer), learning methods for selective generation under such practical setups are crucial but currently missing. To address these limitations, we propose an online learning algorithm for selective generation under partial feedback. In particular, as learning under partial feedback is well-studied by multi-armed bandit problems, we reduce selective generation to bandits and provide a novel conversion lemma from bandits back to selective generation to leverage any known bandit algorithms and theoretical properties. This mainly connects regret guarantees of bandits to false discovery rate (FDR) guarantees of selective generation for controlling hallucination. However, naively exploiting known bandit algorithms and their regret bounds suffers from slow convergence speed in practice due the nature of partial feedback. To overcome this, we exploit a unique structure of arms in selective generation for feedback unlocking, i.e., unlocking unknown feedback from observed feedback. We theoretically and empirically evaluate the efficacy of the proposed online selective generation algorithm under partial feedback over diverse data environment setups, resulting in controlling a desired FDR, while maintaining reasonable selection efficiency, i.e., the ratio of non-abstaining answers, compared to baselines.

</details>


### [140] [Comprehensive Verilog Design Problems: A Next-Generation Benchmark Dataset for Evaluating Large Language Models and Agents on RTL Design and Verification](https://arxiv.org/abs/2506.14074)
*Nathaniel Pinckney,Chenhui Deng,Chia-Tung Ho,Yun-Da Tsai,Mingjie Liu,Wenfei Zhou,Brucek Khailany,Haoxing Ren*

Main category: cs.LG

TL;DR: CVDP基准提出，包含783个硬件设计问题，覆盖13类任务，评估显示现有模型在代码生成任务中最高仅34%通过率，代理任务尤为困难。


<details>
  <summary>Details</summary>
Motivation: 现有硬件设计与验证领域缺乏真实且全面的评估基准，需推动LLM及智能体在此场景下的研究。

Method: 构建含非代理/代理格式的783个工程问题数据集，使用开源工具链评估，结合BLEU和LLM评分机制。

Result: SOTA模型在代码生成任务中pass@1≤34%，涉及RTL重用与验证的代理任务表现最差，暴露模型能力重大缺陷。

Conclusion: 当前模型在真实硬件设计自动化中存在显著能力缺口，需持续研究以提升鲁棒性和实际应用性。

Abstract: We present the Comprehensive Verilog Design Problems (CVDP) benchmark, a new dataset and infrastructure to advance LLM and agent research in hardware design and verification. CVDP includes 783 problems across 13 task categories, covering RTL generation, verification, debugging, specification alignment, and technical Q&A authored by experienced hardware engineers. Problems are offered in both non-agentic and agentic formats. The benchmark introduces more realistic and challenging contexts than prior work, with state-of-the-art models achieving no more than 34% pass@1 on code generation. Agentic tasks$\unicode{x2013}$especially those involving RTL reuse and verification$\unicode{x2013}$are particularly difficult. Evaluation uses open-source tools and model scoring infrastructure, with comprehension tasks assessed via BLEU and LLM-based judging. CVDP reveals substantial gaps in current model capabilities, underscoring the need for continued research toward robust, real-world hardware design automation.

</details>


### [141] [Multi-Scale Finetuning for Encoder-based Time Series Foundation Models](https://arxiv.org/abs/2506.14087)
*Zhongzheng Qiao,Chenghao Liu,Yiming Zhang,Ming Jin,Quang Pham,Qingsong Wen,P. N. Suganthan,Xudong Jiang,Savitha Ramasamy*

Main category: cs.LG

TL;DR: 提出多尺度微调框架MSFT，通过显式整合多尺度建模提升时间序列基础模型在下游任务中的微调效果。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型(TSFMs)的简单微调方法未能充分利用其多尺度预测能力，易导致过拟合和次优问题。

Method: 基于因果视角分析，针对编码器型TSFMs设计MSFT框架，在微调过程中显式集成多尺度建模机制。

Result: 在Moirai、Moment和Units三个模型上的实验表明，MSFT微调效果优于传统微调方法和SOTA深度学习模型。

Conclusion: 通过多尺度建模的显式整合，MSFT有效释放了TSFMs的潜在能力，为时间序列模型微调提供了新范式。

Abstract: Time series foundation models (TSFMs) demonstrate impressive zero-shot performance for time series forecasting. However, an important yet underexplored challenge is how to effectively finetune TSFMs on specific downstream tasks. While naive finetuning can yield performance gains, we argue that it falls short of fully leveraging TSFMs' capabilities, often resulting in overfitting and suboptimal performance. Given the diverse temporal patterns across sampling scales and the inherent multi-scale forecasting capabilities of TSFMs, we adopt a causal perspective to analyze finetuning process, through which we highlight the critical importance of explicitly modeling multiple scales and reveal the shortcomings of naive approaches. Focusing on \textit{encoder-based} TSFMs, we propose \textbf{M}ulti\textbf{\textsc{s}}cale \textbf{\textsc{f}}ine\textbf{\textsc{t}}uning (\textbf{MSFT}), a simple yet general framework that explicitly integrates multi-scale modeling into the finetuning process. Experimental results on three different backbones (\moirai, \moment\ and \units) demonstrate that TSFMs finetuned with MSFT not only outperform naive and typical parameter efficient finetuning methods but also surpass state-of-the-art deep learning methods.

</details>


### [142] [Transformers Learn Faster with Semantic Focus](https://arxiv.org/abs/2506.14095)
*Parikshit Ram,Kenneth L. Clarkson,Tim Klinger,Shashanka Ubaru,Alexander G. Gray*

Main category: cs.LG

TL;DR: 本文通过实证与理论分析，发现输入依赖的稀疏注意力机制能加速模型收敛并提升泛化能力，而输入无关的稀疏注意力无此优势。研究揭示了注意力稀疏性与模型稳定性、收敛性之间的理论联系。


<details>
  <summary>Details</summary>
Motivation: 研究稀疏注意力机制的核心动机并非提升计算效率，而是探索其对模型可学习性和泛化能力的影响，尤其关注输入依赖与输入无关稀疏注意力的本质差异。

Method: 结合实证分析与理论推导：通过多组注意力机制对比实验观察收敛/泛化表现，建立softmax稳定性与损失函数Lipschitz性质的理论框架，分析稀疏性对稳定性的影响。

Result: 输入依赖稀疏注意力模型收敛速度与泛化能力显著优于标准注意力，输入无关稀疏模型无此优势；理论证明输入无关稀疏无增益，而语义聚焦（输入依赖稀疏）在特定条件下可提供更强的收敛与泛化保证。

Conclusion: 注意力机制的语义聚焦能力（通过输入依赖稀疏实现）是提升模型性能的关键，其通过增强softmax稳定性与优化路径的平滑性实现，为设计高效注意力机制提供了理论指导。

Abstract: Various forms of sparse attention have been explored to mitigate the quadratic computational and memory cost of the attention mechanism in transformers. We study sparse transformers not through a lens of efficiency but rather in terms of learnability and generalization. Empirically studying a range of attention mechanisms, we find that input-dependent sparse attention models appear to converge faster and generalize better than standard attention models, while input-agnostic sparse attention models show no such benefits -- a phenomenon that is robust across architectural and optimization hyperparameter choices. This can be interpreted as demonstrating that concentrating a model's "semantic focus" with respect to the tokens currently being considered (in the form of input-dependent sparse attention) accelerates learning. We develop a theoretical characterization of the conditions that explain this behavior. We establish a connection between the stability of the standard softmax and the loss function's Lipschitz properties, then show how sparsity affects the stability of the softmax and the subsequent convergence and generalization guarantees resulting from the attention mechanism. This allows us to theoretically establish that input-agnostic sparse attention does not provide any benefits. We also characterize conditions when semantic focus (input-dependent sparse attention) can provide improved guarantees, and we validate that these conditions are in fact met in our empirical evaluations.

</details>


### [143] [Toward a Graph Foundation Model: Pre-Training Transformers With Random Walks](https://arxiv.org/abs/2506.14098)
*Ziyuan Tang,Jie Chen*

Main category: cs.LG

TL;DR: 本文提出了一种基于Transformer架构的图基础模型，通过随机游走表示节点并设计新的预测损失函数，实现了跨领域预训练及下游任务适应。


<details>
  <summary>Details</summary>
Motivation: 自然语言领域的基础模型（如GPT）因广泛数据预训练和Transformer架构展现强大能力，但图结构数据缺乏类似通用模型。研究旨在探索构建图数据的基础模型。

Method: 将节点表示为多个随机游走序列，利用Transformer提取节点表征，进而构建边和图表征；提出上下文预测损失函数，并理论分析随机游走对邻域和图的区分能力。

Result: 模型通过预训练展示了对不同图数据集的适应性，并在下游任务中验证了其作为图结构数据处理与推理基础模型的潜力。

Conclusion: 所提出的方法为图基础模型提供了可行路径，通过随机游走编码和Transformer结合，支持跨领域图数据的统一表示与迁移学习。

Abstract: A foundation model like GPT elicits many emergent abilities, owing to the pre-training with broad inclusion of data and the use of the powerful Transformer architecture. While foundation models in natural languages are prevalent, can we build similar models for graphs? This paper describes an approach toward a graph foundation model that is pre-trained with diverse graph datasets by adapting the Transformer backbone. A central challenge toward this end is how a sequence model encodes graphs of varying sizes and from different domains. We propose representing a node as multiple random walks, such that the Transformer can extract node representations from sequences, which in turn form edge and graph representations. We develop a novel context prediction loss for these random walks and theoretically analyze their expressive power in distinguishing neighborhoods and graphs. We also demonstrate the pre-training of our model and its adaptation to downstream tasks, showcasing its potential as a foundation for processing and reasoning with graph-structured data.

</details>


### [144] [SKOLR: Structured Koopman Operator Linear RNN for Time-Series Forecasting](https://arxiv.org/abs/2506.14113)
*Yitian Zhang,Liheng Ma,Antonios Valkanas,Boris N. Oreshkin,Mark Coates*

Main category: cs.LG

TL;DR: 本文提出SKOLR方法，通过建立Koopman算子与线性循环神经网络的等价性，结合可学习光谱分解与多层感知机，实现高效的非线性动态系统预测。


<details>
  <summary>Details</summary>
Motivation: Koopman算子理论虽能将非线性系统线性化，但其无限维特性导致计算困难。研究旨在通过有限维近似提升实用性，并探索其与线性RNN的潜在关联。

Method: 提出SKOLR框架：利用滞后观测构建扩展状态，建立结构化Koopman算子与线性RNN的等价性，集成可学习光谱分解、MLP作为测量函数，并通过并行线性RNN堆栈实现算子。

Result: 在多种预测基准测试和动态系统实验中，SKOLR表现出卓越性能，验证了基于Koopman理论设计的有效性。

Conclusion: 通过连接Koopman算子与线性RNN，SKOLR以简洁的架构实现高效动态建模，为非线性系统分析的算子理论应用提供了新方向。

Abstract: Koopman operator theory provides a framework for nonlinear dynamical system analysis and time-series forecasting by mapping dynamics to a space of real-valued measurement functions, enabling a linear operator representation. Despite the advantage of linearity, the operator is generally infinite-dimensional. Therefore, the objective is to learn measurement functions that yield a tractable finite-dimensional Koopman operator approximation. In this work, we establish a connection between Koopman operator approximation and linear Recurrent Neural Networks (RNNs), which have recently demonstrated remarkable success in sequence modeling. We show that by considering an extended state consisting of lagged observations, we can establish an equivalence between a structured Koopman operator and linear RNN updates. Building on this connection, we present SKOLR, which integrates a learnable spectral decomposition of the input signal with a multilayer perceptron (MLP) as the measurement functions and implements a structured Koopman operator via a highly parallel linear RNN stack. Numerical experiments on various forecasting benchmarks and dynamical systems show that this streamlined, Koopman-theory-based design delivers exceptional performance.

</details>


### [145] [Evaluating Loss Functions for Graph Neural Networks: Towards Pretraining and Generalization](https://arxiv.org/abs/2506.14114)
*Khushnood Abbas,Ruizhe Hou,Zhou Wengang,Dong Shi,Niu Ling,Satyaki Nan,Alireza Abbasi*

Main category: cs.LG

TL;DR: 通过大规模评估发现，混合损失函数与GIN架构在归纳任务中表现最佳，GAT在某些混合损失下展现专项优势，而MPNN表现较差。


<details>
  <summary>Details</summary>
Motivation: 现有研究未系统评估图神经网络（GNN）模型架构与多种损失函数组合在不同任务中的综合表现，需填补这一空白。

Method: 结合7种GNN架构和30种单/混合损失函数，在3个真实数据集上采用归纳/传导式学习设置，通过21项指标进行全面评估。

Result: 1) 混合损失函数性能更优；2) GIN+交叉熵损失表现最佳；3) GAT+特定混合损失具专项优势；4) MPNN整体表现落后。

Conclusion: 多目标优化的混合损失函数与特定GNN架构组合可提升性能，不同任务需针对性选择模型-损失组合，GIN为当前最优基础架构。

Abstract: Graph Neural Networks (GNNs) became useful for learning on non-Euclidean data. However, their best performance depends on choosing the right model architecture and the training objective, also called the loss function. Researchers have studied these parts separately, but a large-scale evaluation has not looked at how GNN models and many loss functions work together across different tasks. To fix this, we ran a thorough study - it included seven well-known GNN architectures. We also used a large group of 30 single plus mixed loss functions. The study looked at both inductive and transductive settings. Our evaluation spanned three distinct real-world datasets, assessing performance in both inductive and transductive settings using 21 comprehensive evaluation metrics. From these extensive results (detailed in supplementary information 1 \& 2), we meticulously analyzed the top ten model-loss combinations for each metric based on their average rank. Our findings reveal that, especially for the inductive case: 1) Hybrid loss functions generally yield superior and more robust performance compared to single loss functions, indicating the benefit of multi-objective optimization. 2) The GIN architecture always showed the highest-level average performance, especially with Cross-Entropy loss. 3) Although some combinations had overall lower average ranks, models such as GAT, particularly with certain hybrid losses, demonstrated incredible specialized strengths, maximizing the most top-1 results among the individual metrics, emphasizing subtle strengths for particular task demands. 4) On the other hand, the MPNN architecture typically lagged behind the scenarios it was tested against.

</details>


### [146] [CLGNN: A Contrastive Learning-based GNN Model for Betweenness Centrality Prediction on Temporal Graphs](https://arxiv.org/abs/2506.14122)
*Tianming Zhang,Renbo Zhang,Zhengyi Yang,Yunjun Gao,Bin Cao,Jing Fan*

Main category: cs.LG

TL;DR: 本文提出了一种基于对比学习的图神经网络CLGNN，用于高效预测时间介数中心性（TBC）。通过构建实例图保留路径有效性和时序依赖，结合双聚合编码和稳定性聚类对比学习，解决了现有方法因数据分布不平衡导致的过拟合问题，并支持多种最优路径定义。实验表明CLGNN在速度和预测精度上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有时间介数中心性（TBC）计算方法计算成本高，且真实数据分布极度不平衡，导致基于学习的模型对零中心性节点过拟合，无法准确预测关键节点。传统图神经网络方法未能有效处理不平衡问题或忽略时序依赖。

Method: 提出CLGNN模型：1) 构建实例图保留路径有效性和时序顺序；2) 使用均值聚合和边到节点多头注意力机制（增强时序路径计数和时间编码）进行结构与时序特征编码；3) 引入基于稳定性的聚类对比模块（KContrastNet）分离不同中心性节点，缓解类别不平衡；4) 回归模块（ValueNet）估计TBC值；支持多最优路径定义以适应不同时序语义。

Result: 实验表明：CLGNN相比精确TBC计算方法加速663.7倍；MAE比静态GNN基线降低31.4倍，斯皮尔曼相关性提高16.7倍；MAE比时序GNN最优方法降低5.7倍，相关性提高3.9倍。

Conclusion: CLGNN通过对比学习和双聚合机制有效解决了TBC预测中的不平衡问题与时序依赖建模，在效率、精度和鲁棒性上均显著超越现有方法，且支持灵活的最优路径定义。

Abstract: Temporal Betweenness Centrality (TBC) measures how often a node appears on optimal temporal paths, reflecting its importance in temporal networks. However, exact computation is highly expensive, and real-world TBC distributions are extremely imbalanced. The severe imbalance leads learning-based models to overfit to zero-centrality nodes, resulting in inaccurate TBC predictions and failure to identify truly central nodes. Existing graph neural network (GNN) methods either fail to handle such imbalance or ignore temporal dependencies altogether. To address these issues, we propose a scalable and inductive contrastive learning-based GNN (CLGNN) for accurate and efficient TBC prediction. CLGNN builds an instance graph to preserve path validity and temporal order, then encodes structural and temporal features using dual aggregation, i.e., mean and edge-to-node multi-head attention mechanisms, enhanced by temporal path count and time encodings. A stability-based clustering-guided contrastive module (KContrastNet) is introduced to separate high-, median-, and low-centrality nodes in representation space, mitigating class imbalance, while a regression module (ValueNet) estimates TBC values. CLGNN also supports multiple optimal path definitions to accommodate diverse temporal semantics. Extensive experiments demonstrate the effectiveness and efficiency of CLGNN across diverse benchmarks. CLGNN achieves up to a 663.7~$\times$ speedup compared to state-of-the-art exact TBC computation methods. It outperforms leading static GNN baselines with up to 31.4~$\times$ lower MAE and 16.7~$\times$ higher Spearman correlation, and surpasses state-of-the-art temporal GNNs with up to 5.7~$\times$ lower MAE and 3.9~$\times$ higher Spearman correlation.

</details>


### [147] [Less is More: Undertraining Experts Improves Model Upcycling](https://arxiv.org/abs/2506.14126)
*Stefan Horoi,Guy Wolf,Eugene Belilovsky,Gintare Karolina Dziugaite*

Main category: cs.LG

TL;DR: 研究发现，专家模型的长时间微调会损害模型合并后的性能，提出基于任务的早期停止策略可显著提升模型升级效果。


<details>
  <summary>Details</summary>
Motivation: 挑战现有假设——模型训练流程中某一阶段的改进会自然传递到后续阶段，特别是探究专家模型微调对模型升级（如合并或构建MoE层）的影响。

Method: 通过实验分析专家模型长时间微调对合并性能的影响，追踪性能下降原因至困难样本的过拟合，并提出任务依赖的激进早期停止策略。

Result: 长时间微调导致模型合并性能下降（包括全微调模型和LoRA适配器），且LoRA适配器升级为MoE层时下游任务表现更差；早期停止策略可有效缓解此问题。

Conclusion: 任务依赖的激进早期停止策略能显著改善模型升级性能，挑战了传统训练流程中改进自然传递的假设。

Abstract: Modern deep learning is increasingly characterized by the use of open-weight foundation models that can be fine-tuned on specialized datasets. This has led to a proliferation of expert models and adapters, often shared via platforms like HuggingFace and AdapterHub. To leverage these resources, numerous model upcycling methods have emerged, enabling the reuse of fine-tuned models in multi-task systems. A natural pipeline has thus formed to harness the benefits of transfer learning and amortize sunk training costs: models are pre-trained on general data, fine-tuned on specific tasks, and then upcycled into more general-purpose systems. A prevailing assumption is that improvements at one stage of this pipeline propagate downstream, leading to gains at subsequent steps. In this work, we challenge that assumption by examining how expert fine-tuning affects model upcycling. We show that long fine-tuning of experts that optimizes for their individual performance leads to degraded merging performance, both for fully fine-tuned and LoRA-adapted models, and to worse downstream results when LoRA adapters are upcycled into MoE layers. We trace this degradation to the memorization of a small set of difficult examples that dominate late fine-tuning steps and are subsequently forgotten during merging. Finally, we demonstrate that a task-dependent aggressive early stopping strategy can significantly improve upcycling performance.

</details>


### [148] [Leveraging Predictive Equivalence in Decision Trees](https://arxiv.org/abs/2506.14143)
*Hayden McTavish,Zachery Boner,Jon Donnelly,Margo Seltzer,Cynthia Rudin*

Main category: cs.LG

TL;DR: 本文提出一种布尔逻辑表示法解决决策树的预测等价性问题，提升模型可解释性，并验证其在缺失值鲁棒性、变量重要性评估及预测成本优化中的应用。


<details>
  <summary>Details</summary>
Motivation: 决策树存在预测等价性问题：不同结构的树可能具有相同决策边界，导致模型选择困难，影响变量重要性分析和缺失值处理。

Method: 采用布尔逻辑的决策树表示方法，消除预测等价性，确保表示形式与底层决策边界严格一致。

Result: 实验表明：1) 决策树对测试时特征缺失具有强鲁棒性；2) 解决了变量重要性量化偏差；3) 提出预测路径成本优化算法。

Conclusion: 布尔逻辑表示法有效消除决策树预测等价性，增强模型稳定性与可解释性，为下游任务提供可靠基础。

Abstract: Decision trees are widely used for interpretable machine learning due to their clearly structured reasoning process. However, this structure belies a challenge we refer to as predictive equivalence: a given tree's decision boundary can be represented by many different decision trees. The presence of models with identical decision boundaries but different evaluation processes makes model selection challenging. The models will have different variable importance and behave differently in the presence of missing values, but most optimization procedures will arbitrarily choose one such model to return. We present a boolean logical representation of decision trees that does not exhibit predictive equivalence and is faithful to the underlying decision boundary. We apply our representation to several downstream machine learning tasks. Using our representation, we show that decision trees are surprisingly robust to test-time missingness of feature values; we address predictive equivalence's impact on quantifying variable importance; and we present an algorithm to optimize the cost of reaching predictions.

</details>


### [149] [Common Benchmarks Undervalue the Generalization Power of Programmatic Policies](https://arxiv.org/abs/2506.14162)
*Amirhossein Rajabpour,Kiarash Aghakasiri,Sandra Zilles,Levi H. S. Lelis*

Main category: cs.LG

TL;DR: 本文指出现有基准低估了程序化策略的泛化能力，通过调整神经网络训练方法（如简化架构、稀疏观察输入、优化奖励函数），神经策略可达到与程序化策略相当的OOD泛化效果，并提出需设计新基准任务以突显程序化优势。


<details>
  <summary>Details</summary>
Motivation: 现有研究普遍认为程序化策略在分布外（OOD）泛化上优于神经策略，但作者认为常用基准未能充分评估程序化表示的真实潜力，需重新检验神经策略的泛化能力。

Method: 分析四篇文献实验，调整神经策略训练流程：采用更简单的网络架构、与程序化策略同类型的稀疏观察输入，以及设计鼓励安全策略的奖励函数（如低速驾驶）。

Result: 神经策略经调整后，在OOD问题上泛化能力与程序化策略相当，表明其潜力被低估。同时提出需构建包含算法结构需求（如栈操作）的新基准任务。

Conclusion: 程序化策略的泛化优势可能源于当前基准设计偏差，未来应开发更复杂概念的任务以公平对比两类策略，并优化神经策略训练方法。

Abstract: Algorithms for learning programmatic representations for sequential decision-making problems are often evaluated on out-of-distribution (OOD) problems, with the common conclusion that programmatic policies generalize better than neural policies on OOD problems. In this position paper, we argue that commonly used benchmarks undervalue the generalization capabilities of programmatic representations. We analyze the experiments of four papers from the literature and show that neural policies, which were shown not to generalize, can generalize as effectively as programmatic policies on OOD problems. This is achieved with simple changes in the neural policies training pipeline. Namely, we show that simpler neural architectures with the same type of sparse observation used with programmatic policies can help attain OOD generalization. Another modification we have shown to be effective is the use of reward functions that allow for safer policies (e.g., agents that drive slowly can generalize better). Also, we argue for creating benchmark problems highlighting concepts needed for OOD generalization that may challenge neural policies but align with programmatic representations, such as tasks requiring algorithmic constructs like stacks.

</details>


### [150] [Light Aircraft Game : Basic Implementation and training results analysis](https://arxiv.org/abs/2506.14164)
*Hanzhong Cao*

Main category: cs.LG

TL;DR: 本文研究了部分可观测合作-竞争混合环境LAG中的多智能体强化学习，对比了HAPPO和HASAC两种算法在不同战斗模式下的表现。


<details>
  <summary>Details</summary>
Motivation: 探索在线策略与离线策略方法在复杂多智能体环境（含武器交互的动态场景）中的适应性差异，为算法选择提供依据。

Method: 构建分层控制结构的LAG环境，设计不同战斗模式（无武器/导弹战）的奖励机制，采用HAPPO（基于PPO的在线分层算法）和HASAC（基于SAC的离线方法）进行对比实验。

Result: HASAC在无武器简单协作任务中表现优异，而HAPPO在导弹战斗等动态场景中展现出更强的环境适应性与智能体协调能力。

Conclusion: 在线策略方法（HAPPO）在复杂动态场景中更具优势，离线策略方法（HASAC）适合简单协作任务，揭示了多智能体系统中策略类型与任务复杂度间的权衡关系。

Abstract: This paper investigates multi-agent reinforcement learning (MARL) in a partially observable, cooperative-competitive combat environment known as LAG. We describe the environment's setup, including agent actions, hierarchical controls, and reward design across different combat modes such as No Weapon and ShootMissile. Two representative algorithms are evaluated: HAPPO, an on-policy hierarchical variant of PPO, and HASAC, an off-policy method based on soft actor-critic. We analyze their training stability, reward progression, and inter-agent coordination capabilities. Experimental results show that HASAC performs well in simpler coordination tasks without weapons, while HAPPO demonstrates stronger adaptability in more dynamic and expressive scenarios involving missile combat. These findings provide insights into the trade-offs between on-policy and off-policy methods in multi-agent settings.

</details>


### [151] [Structured and Informed Probabilistic Modeling with the Thermodynamic Kolmogorov-Arnold Model](https://arxiv.org/abs/2506.14167)
*Prithvi Raj*

Main category: cs.LG

TL;DR: 该论文将Kolmogorov-Arnold表示定理与生成模型结合，提出一种基于最大似然训练、可解释且高效的生成框架，通过混合分布和蒙特卡洛方法平衡灵活性与训练效率。


<details>
  <summary>Details</summary>
Motivation: 旨在将经典Kolmogorov-Arnold表示定理与现代概率建模结合，解决生成模型中先验-后验不匹配、训练效率低及样本质量与多样性不足的问题。

Method: 使用Kolmogorov-Arnold网络生成器与独立能量基先验耦合，通过逆采样加速推理；引入混合分布和Langevin蒙特卡洛方法扩展模型灵活性。

Result: 模型支持训练前先验知识注入以提升学习效率与样本质量，后验可恢复先验参数，并在推理速度、生成质量与多样性间实现平衡。

Conclusion: 通过连接经典表示定理与概率建模，提出兼具训练稳定性、快速推理及高质量生成的框架，为经验贝叶斯方法提供可视化与可扩展路径。

Abstract: We adapt the Kolmogorov-Arnold Representation Theorem to generative modeling by reinterpreting its inner functions as a Markov Kernel between probability spaces via inverse transform sampling. We present a generative model that is interpretable, easy to design, and efficient. Our approach couples a Kolmogorov-Arnold Network generator with independent energy-based priors, trained via Maximum Likelihood. Inverse sampling enables fast inference, while prior knowledge can be incorporated before training to better align priors with posteriors, thereby improving learning efficiency and sample quality. The learned prior is also recoverable and visualizable post-training, offering an empirical Bayes perspective. To address inflexibility and mitigate prior-posterior mismatch, we introduce scalable extensions based on mixture distributions and Langevin Monte Carlo methods, admitting a trade-off between flexibility and training efficiency. Our contributions connect classical representation theorems with modern probabilistic modeling, while balancing training stability, inference speed, and the quality and diversity of generations.

</details>


### [152] [A Variational Information Theoretic Approach to Out-of-Distribution Detection](https://arxiv.org/abs/2506.14194)
*Sudeepta Mondal,Zhuolin Jiang,Ganesh Sundaramoorthi*

Main category: cs.LG

TL;DR: 本文提出了一种基于信息论的神经网络分布外（OOD）检测特征构建理论，通过结合KL散度和信息瓶颈的损失函数优化特征，生成可解释的OOD检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测特征缺乏统一的理论框架，且特征可解释性不足。本文旨在通过信息论构建具有理论支撑和可解释性的OOD检测特征。

Method: 设计包含KL散度（分离ID/OOD特征分布）和信息瓶颈（压缩特征信息）的损失函数，通过变分优化方法生成OOD特征，并基于OOD分布假设推导现有特征性质。

Result: 理论可解释现有OOD特征（如shaping函数），并预测出优于现有方法的新特征，在OOD基准测试中表现更优。

Conclusion: 该理论为构建可解释的OOD检测特征提供了通用框架，可扩展生成多种新特征，兼具理论严谨性与实际性能提升。

Abstract: We present a theory for the construction of out-of-distribution (OOD) detection features for neural networks. We introduce random features for OOD through a novel information-theoretic loss functional consisting of two terms, the first based on the KL divergence separates resulting in-distribution (ID) and OOD feature distributions and the second term is the Information Bottleneck, which favors compressed features that retain the OOD information. We formulate a variational procedure to optimize the loss and obtain OOD features. Based on assumptions on OOD distributions, one can recover properties of existing OOD features, i.e., shaping functions. Furthermore, we show that our theory can predict a new shaping function that out-performs existing ones on OOD benchmarks. Our theory provides a general framework for constructing a variety of new features with clear explainability.

</details>


### [153] [DiffusionBlocks: Blockwise Training for Generative Models via Score-Based Diffusion](https://arxiv.org/abs/2506.14202)
*Makoto Shing,Takuya Akiba*

Main category: cs.LG

TL;DR: 本文提出DiffusionBlocks框架，通过将神经网络块视为连续时间扩散过程中的去噪操作，并优化噪声分配策略，显著降低训练内存需求，同时保持生成任务的竞争力。实验表明内存减少与块数成正比且性能更优。


<details>
  <summary>Details</summary>
Motivation: 传统端到端反向传播训练大型神经网络存在显著内存瓶颈，限制了资源有限的研究者对前沿AI技术的访问。

Method: 将网络划分为独立可训练块，将其解释为扩散过程去噪步骤，基于等累积概率质量优化噪声水平分配策略。

Result: 图像生成与语言建模任务中实现与块数成正比的内存降低，同时达到优于传统反向传播的性能表现。

Conclusion: DiffusionBlocks为计算资源受限环境下的大规模神经网络训练提供了可行方案，有助于降低先进AI技术的应用门槛。

Abstract: Training large neural networks with end-to-end backpropagation creates significant memory bottlenecks, limiting accessibility to state-of-the-art AI research. We propose $\textit{DiffusionBlocks}$, a novel training framework that interprets neural network blocks as performing denoising operations in a continuous-time diffusion process. By partitioning the network into independently trainable blocks and optimizing noise level assignments based on equal cumulative probability mass, our approach achieves significant memory efficiency while maintaining competitive performance compared to traditional backpropagation in generative tasks. Experiments on image generation and language modeling tasks demonstrate memory reduction proportional to the number of blocks while achieving superior performance. DiffusionBlocks provides a promising pathway for democratizing access to large-scale neural network training with limited computational resources.

</details>


### [154] [TriGuard: Testing Model Safety with Attribution Entropy, Verification, and Drift](https://arxiv.org/abs/2506.14217)
*Dipesh Tharu Mahato,Rohan Poudel,Pramod Dhungana*

Main category: cs.LG

TL;DR: TriGuard是一个结合形式化鲁棒性验证、归因熵和归因漂移评分的统一安全评估框架，揭示模型准确性与可解释性之间的不匹配，并通过熵正则化训练提升解释稳定性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在对抗攻击和分布偏移下的可靠性仍存在挑战，现有方法未能充分关联模型鲁棒性与解释稳定性，需开发更全面的评估框架。

Method: 提出TriGuard框架，整合(1)形式化鲁棒验证，(2)量化显著图集中度的归因熵，(3)衡量解释稳定性的归因漂移评分，通过多维度指标评估模型安全性。

Result: 实验表明：已验证鲁棒性的模型仍存在推理不稳定性；归因指标与对抗精度互补；熵正则化训练可降低解释漂移且不影响性能。

Conclusion: TriGuard通过统一评估模型鲁棒性与解释可靠性，揭示了神经网络推理的脆弱性，为构建可信赖的AI系统提供了新的方法论支持。

Abstract: Deep neural networks often achieve high accuracy, but ensuring their reliability under adversarial and distributional shifts remains a pressing challenge. We propose TriGuard, a unified safety evaluation framework that combines (1) formal robustness verification, (2) attribution entropy to quantify saliency concentration, and (3) a novel Attribution Drift Score measuring explanation stability. TriGuard reveals critical mismatches between model accuracy and interpretability: verified models can still exhibit unstable reasoning, and attribution-based signals provide complementary safety insights beyond adversarial accuracy. Extensive experiments across three datasets and five architectures show how TriGuard uncovers subtle fragilities in neural reasoning. We further demonstrate that entropy-regularized training reduces explanation drift without sacrificing performance. TriGuard advances the frontier in robust, interpretable model evaluation.

</details>


### [155] [Can Large Language Models Improve Spectral Graph Neural Networks?](https://arxiv.org/abs/2506.14220)
*Kangkang Lu,Yanhua Yu,Zhiyong Huang,Tat-Seng Chua*

Main category: cs.LG

TL;DR: 提出一种利用大语言模型（LLM）估计图同配性，并指导谱图神经网络（SGNN）滤波器设计的方法，以提升SGNN在不同图结构下的性能。


<details>
  <summary>Details</summary>
Motivation: 传统谱图神经网络（SGNN）在标签稀缺时可能学习到次优滤波器，导致性能下降。而大语言模型（LLM）的成功启发研究者探索其在GNN领域的潜力，以解决SGNN的局限性。

Method: 通过LLM生成图同配性先验知识，将其注入多项式谱滤波器系数中，使滤波器设计更贴合图拓扑结构。采用轻量级流程，结合LLM的语义理解与SGNN的谱滤波能力。

Result: 在基准数据集上的实验表明，该方法在同性/异构图场景下均优于现有基线，且计算成本和资源开销极低。

Conclusion: LLM驱动的SGNN框架通过引入同配性感知先验，有效增强了模型对不同图结构的适应性和表达能力，为结合LLM与GNN提供了新思路。

Abstract: Spectral Graph Neural Networks (SGNNs) have attracted significant attention due to their ability to approximate arbitrary filters. They typically rely on supervision from downstream tasks to adaptively learn appropriate filters. However, under label-scarce conditions, SGNNs may learn suboptimal filters, leading to degraded performance. Meanwhile, the remarkable success of Large Language Models (LLMs) has inspired growing interest in exploring their potential within the GNN domain. This naturally raises an important question: \textit{Can LLMs help overcome the limitations of SGNNs and enhance their performance?} In this paper, we propose a novel approach that leverages LLMs to estimate the homophily of a given graph. The estimated homophily is then used to adaptively guide the design of polynomial spectral filters, thereby improving the expressiveness and adaptability of SGNNs across diverse graph structures. Specifically, we introduce a lightweight pipeline in which the LLM generates homophily-aware priors, which are injected into the filter coefficients to better align with the underlying graph topology. Extensive experiments on benchmark datasets demonstrate that our LLM-driven SGNN framework consistently outperforms existing baselines under both homophilic and heterophilic settings, with minimal computational and monetary overhead.

</details>


### [156] [Convergence-Privacy-Fairness Trade-Off in Personalized Federated Learning](https://arxiv.org/abs/2506.14251)
*Xiyu Zhao,Qimei Cui,Weicai Li,Wei Ni,Ekram Hossain,Quan Z. Sheng,Xiaofeng Tao,Ping Zhang*

Main category: cs.LG

TL;DR: 本文提出DP-Ditto，一种在差分隐私保护下扩展Ditto的个性化联邦学习方法，解决了隐私保护与模型性能公平性之间的权衡问题，实验显示其在准确率和准确度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有个性化联邦学习（如Ditto）中，个性化学习依赖联邦学习结果，但用户隐私保护导致的本地扰动会影响模型收敛和性能公平性。需在隐私保护下平衡模型效果与公平性。

Method: 在Ditto基础上引入差分隐私（DP），分析隐私保障、收敛性及公平性间的权衡；推导个性化模型在DP-Ditto下的收敛上界，基于隐私预算确定最优全局聚合次数，并探索联合优化收敛与公平的可行性。

Result: 实验表明DP-Ditto在公平性上超过现有DP扰动PFL模型（如FedAMP、pFedMe等）32.71%，准确率提升9.66%。

Conclusion: DP-Ditto通过差分隐私保护有效平衡个性化联邦学习的隐私、收敛与公平性，验证了联合优化收敛与公平的可行性，显著优于现有方法。

Abstract: Personalized federated learning (PFL), e.g., the renowned Ditto, strikes a balance between personalization and generalization by conducting federated learning (FL) to guide personalized learning (PL). While FL is unaffected by personalized model training, in Ditto, PL depends on the outcome of the FL. However, the clients' concern about their privacy and consequent perturbation of their local models can affect the convergence and (performance) fairness of PL. This paper presents PFL, called DP-Ditto, which is a non-trivial extension of Ditto under the protection of differential privacy (DP), and analyzes the trade-off among its privacy guarantee, model convergence, and performance distribution fairness. We also analyze the convergence upper bound of the personalized models under DP-Ditto and derive the optimal number of global aggregations given a privacy budget. Further, we analyze the performance fairness of the personalized models, and reveal the feasibility of optimizing DP-Ditto jointly for convergence and fairness. Experiments validate our analysis and demonstrate that DP-Ditto can surpass the DP-perturbed versions of the state-of-the-art PFL models, such as FedAMP, pFedMe, APPLE, and FedALA, by over 32.71% in fairness and 9.66% in accuracy.

</details>


### [157] [RL-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?](https://arxiv.org/abs/2506.14261)
*Rohan Gupta,Erik Jenner*

Main category: cs.LG

TL;DR: 研究通过强化学习微调LLMs以规避潜在空间监视器，发现基于token的监视器易受攻击，而更全面的监视器（如max-pooling）仍稳健，且对抗策略可泛化至同类新监视器。


<details>
  <summary>Details</summary>
Motivation: 验证大型语言模型（LLMs）是否能通过学习绕过基于内部表征的潜在空间监视器，以检测其安全监控机制的脆弱性。

Method: 提出RL-Obfuscation方法，通过强化学习微调LLMs，使其在保持生成连贯性的同时规避潜在空间监视器的检测。

Result: token级监视器易被攻击；max-pooling/注意力探针监视器稳健；对抗策略可泛化至同类未见过监视器；模型通过重新定义token内部含义绕过监控。

Conclusion: 潜在空间监视器的有效性取决于其设计：细粒度方法易被规避，而全局表征方法更可靠，但LLMs仍可通过内部语义重构逃避监控。

Abstract: Latent-space monitors aim to detect undesirable behaviours in large language models by leveraging internal model representations rather than relying solely on black-box outputs. These methods have shown promise in identifying behaviours such as deception and unsafe completions, but a critical open question remains: can LLMs learn to evade such monitors? To study this, we introduce RL-Obfuscation, in which LLMs are finetuned via reinforcement learning to bypass latent-space monitors while maintaining coherent generations. We apply RL-Obfuscation to LLMs ranging from 7B to 14B parameters and evaluate evasion success against a suite of monitors. We find that token-level latent-space monitors are highly vulnerable to this attack. More holistic monitors, such as max-pooling or attention-based probes, remain robust. Moreover, we show that adversarial policies trained to evade a single static monitor generalise to unseen monitors of the same type. Finally, we study how the policy learned by RL bypasses these monitors and find that the model can also learn to repurpose tokens to mean something different internally.

</details>


### [158] [Knowledge Adaptation as Posterior Correction](https://arxiv.org/abs/2506.14262)
*Mohammad Emtiyaz Khan*

Main category: cs.LG

TL;DR: 本文提出，所有机器学习模型的适应方法均可视为对近似后验分布的校正，更准确的后验可减少校正量，从而加速适应。通过贝叶斯学习规则的双重视角，揭示了自然梯度不匹配导致的干扰机制，并验证后验校正是机器快速适应的自然机制。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型（如GPT）虽在持续学习、联邦学习等方面取得进展，但其适应速度仍远不及人类幼儿。研究旨在探索机器如何像生物智能一样通过自然机制实现适应性学习。

Method: 基于Khan和Rue（2023）的贝叶斯学习规则，采用双重视角分析适应过程中的干扰现象，提出将模型适应方法统一解释为对近似后验分布的校正。

Result: 理论证明后验准确性直接影响校正幅度与适应速度，并通过多案例验证后验校正作为快速适应核心机制的有效性。

Conclusion: 后验校正为机器实现类生物快速适应提供了统一框架，自然梯度不匹配的量化分析为理解模型适应性机制开辟了新路径。

Abstract: Adaptation is the holy grail of intelligence, but even the best AI models (like GPT) lack the adaptivity of toddlers. So the question remains: how can machines adapt quickly? Despite a lot of progress on model adaptation to facilitate continual and federated learning, as well as model merging, editing, unlearning, etc., little is known about the mechanisms by which machines can naturally learn to adapt in a similar way as humans and animals. Here, we show that all such adaptation methods can be seen as different ways of `correcting' the approximate posteriors. More accurate posteriors lead to smaller corrections, which in turn imply quicker adaptation. The result is obtained by using a dual-perspective of the Bayesian Learning Rule of Khan and Rue (2023) where interference created during adaptation is characterized by the natural-gradient mismatch over the past data. We present many examples to demonstrate the use of posterior-correction as a natural mechanism for the machines to learn to adapt quickly.

</details>


### [159] [Towards Robust Learning to Optimize with Theoretical Guarantees](https://arxiv.org/abs/2506.14263)
*Qingyu Song,Wei Lin,Juncheng Wang,Hong Xu*

Main category: cs.LG

TL;DR: 本文针对学习优化（L2O）方法在分布外（OOD）场景下缺乏理论保证的问题，提出了鲁棒性理论证明，并设计了一种基于梯度特征和历史建模的新模型，实验显示其优于现有方法且收敛速度提升10倍。


<details>
  <summary>Details</summary>
Motivation: 现有L2O方法在真实场景（如无线通信、计算机网络）中虽成功应用，但缺乏对OOD场景下性能与鲁棒性的理论验证，需填补这一理论空白。

Method: 提出一种仅使用梯度特征构建的L2O模型，结合新型梯度历史建模方法，并通过将OOD问题对齐至InD问题的方法论分析模型鲁棒性。

Result: 数值实验表明，所提模型在InD和OOD场景下均优于现有基线方法，收敛速度最高提升10倍，代码已公开。

Conclusion: 通过理论证明与模型创新，本文不仅为L2O的OOD鲁棒性提供了理论支撑，还验证了所提方法在实际优化问题中的高效性与泛化能力。

Abstract: Learning to optimize (L2O) is an emerging technique to solve mathematical optimization problems with learning-based methods. Although with great success in many real-world scenarios such as wireless communications, computer networks, and electronic design, existing L2O works lack theoretical demonstration of their performance and robustness in out-of-distribution (OOD) scenarios. We address this gap by providing comprehensive proofs. First, we prove a sufficient condition for a robust L2O model with homogeneous convergence rates over all In-Distribution (InD) instances. We assume an L2O model achieves robustness for an InD scenario. Based on our proposed methodology of aligning OOD problems to InD problems, we also demonstrate that the L2O model's convergence rate in OOD scenarios will deteriorate by an equation of the L2O model's input features. Moreover, we propose an L2O model with a concise gradient-only feature construction and a novel gradient-based history modeling method. Numerical simulation demonstrates that our proposed model outperforms the state-of-the-art baseline in both InD and OOD scenarios and achieves up to 10 $\times$ convergence speedup. The code of our method can be found from https://github.com/NetX-lab/GoMathL2O-Official.

</details>


### [160] [Improving LoRA with Variational Learning](https://arxiv.org/abs/2506.14280)
*Bai Cong,Nico Daheim,Yuesong Shen,Rio Yokota,Mohammad Emtiyaz Khan,Thomas Möllenhoff*

Main category: cs.LG

TL;DR: 本文通过引入IVON变分算法优化LoRA微调过程，在保持与AdamW相近计算成本的同时，显著提升大语言模型的准确率（+1.3%）和校准性能（ECE降低5.4%），超越传统贝叶斯方法。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯方法在LoRA微调中虽能改善校准性能，但对准确率等核心指标提升有限且可能产生负面影响，同时伴随高计算开销和实现复杂性。

Method: 采用IVON变分算法结合后验剪枝技术，在Llama/Qwen等十亿级大模型上实现高效微调，其实现难度和计算成本与AdamW相当。

Result: 在Llama-3.2-3B常识推理任务中，IVON相比AdamW提升1.3%准确率、降低5.4% ECE，显著优于Laplace-LoRA/BLoB等贝叶斯方法。

Conclusion: IVON通过变分学习机制有效解决了传统贝叶斯方法的局限性，为大规模语言模型的高效微调提供了新范式。

Abstract: Bayesian methods have recently been used to improve LoRA finetuning and, although they improve calibration, their effect on other metrics (such as accuracy) is marginal and can sometimes even be detrimental. Moreover, Bayesian methods also increase computational overheads and require additional tricks for them to work well. Here, we fix these issues by using a recently proposed variational algorithm called IVON. We show that IVON is easy to implement and has similar costs to AdamW, and yet it can also drastically improve many metrics by using a simple posterior pruning technique. We present extensive results on billion-scale LLMs (Llama and Qwen series) going way beyond the scale of existing applications of IVON. For example, we finetune a Llama-3.2-3B model on a set of commonsense reasoning tasks and improve accuracy over AdamW by 1.3% and reduce ECE by 5.4%, outperforming AdamW and other recent Bayesian methods like Laplace-LoRA and BLoB. Overall, our results show that variational learning with IVON can effectively improve LoRA finetuning.

</details>


### [161] [Equivariance Everywhere All At Once: A Recipe for Graph Foundation Models](https://arxiv.org/abs/2506.14291)
*Ben Finkelshtein,İsmail İlkan Ceylan,Michael Bronstein,Ron Levie*

Main category: cs.LG

TL;DR: 本文提出一种基于对称性研究的图基础模型设计方法，通过引入标签置换等变性和特征置换不变性，构建通用节点级预测模型，并在29个真实数据集上验证其零样本性能与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有图机器学习模型通常针对特定任务和数据集设计，缺乏跨图泛化能力。研究如何构建通用的图基础模型（Graph Foundation Models）成为关键挑战。

Method: 通过系统研究图基础模型应满足的对称性（节点置换等变性、标签置换等变性、特征置换不变性），构建线性变换层并证明其多集合上的通用逼近能力，利用图局部邻域特征多集合实现节点属性预测。

Result: 在29个真实节点分类数据集上，模型展现出强大的零样本性能，并随着训练图数量增加保持性能持续提升。

Conclusion: 基于对称性约束的系统性方法能有效构建通用图基础模型，实验证明该方法在跨图泛化与可扩展性方面具有显著优势。

Abstract: Graph machine learning architectures are typically tailored to specific tasks on specific datasets, which hinders their broader applicability. This has led to a new quest in graph machine learning: how to build graph foundation models capable of generalizing across arbitrary graphs and features? In this work, we present a recipe for designing graph foundation models for node-level tasks from first principles. The key ingredient underpinning our study is a systematic investigation of the symmetries that a graph foundation model must respect. In a nutshell, we argue that label permutation-equivariance alongside feature permutation-invariance are necessary in addition to the common node permutation-equivariance on each local neighborhood of the graph. To this end, we first characterize the space of linear transformations that are equivariant to permutations of nodes and labels, and invariant to permutations of features. We then prove that the resulting network is a universal approximator on multisets that respect the aforementioned symmetries. Our recipe uses such layers on the multiset of features induced by the local neighborhood of the graph to obtain a class of graph foundation models for node property prediction. We validate our approach through extensive experiments on 29 real-world node classification datasets, demonstrating both strong zero-shot empirical performance and consistent improvement as the number of training graphs increases.

</details>


### [162] [Fair for a few: Improving Fairness in Doubly Imbalanced Datasets](https://arxiv.org/abs/2506.14306)
*Ata Yalcin,Asli Umay Ozturk,Yigit Sever,Viktoria Pauw,Stephan Hachinger,Ismail Hakki Toroslu,Pinar Karagoz*

Main category: cs.LG

TL;DR: 本文针对机器学习中双重不平衡数据集（标签和敏感属性均不平衡）的公平性问题，分析了现有去偏方法的不足，并提出一种多标准采样方法以平衡公平性和分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法在数据双重不平衡（标签和敏感属性均不平衡）时表现不佳，需探索更有效的解决方案。

Method: 提出基于多标准的采样与分布优化方法，结合公平性和分类准确率选择最佳数据分布。

Result: 通过实验分析验证了双重不平衡数据下传统方法的局限性，并证明所提方法在权衡公平性与准确性上的有效性。

Conclusion: 在双重不平衡场景中，需同时调整标签和敏感属性的数据分布，多标准优化是提升模型公平性与性能的关键。

Abstract: Fairness has been identified as an important aspect of Machine Learning and Artificial Intelligence solutions for decision making. Recent literature offers a variety of approaches for debiasing, however many of them fall short when the data collection is imbalanced. In this paper, we focus on a particular case, fairness in doubly imbalanced datasets, such that the data collection is imbalanced both for the label and the groups in the sensitive attribute. Firstly, we present an exploratory analysis to illustrate limitations in debiasing on a doubly imbalanced dataset. Then, a multi-criteria based solution is proposed for finding the most suitable sampling and distribution for label and sensitive attribute, in terms of fairness and classification accuracy

</details>


### [163] [IntelliLung: Advancing Safe Mechanical Ventilation using Offline RL with Hybrid Actions and Clinically Aligned Rewards](https://arxiv.org/abs/2506.14375)
*Muhammad Hamza Yousuf,Jason Li,Sahar Vahdati,Raphael Theilen,Jakob Wittenstein,Jens Lehmann*

Main category: cs.LG

TL;DR: 本文提出一种基于离线强化学习的混合动作空间优化方法，用于改进机械通气控制，通过避免离散化动作空间及引入临床奖励函数，提升患者安全与个性化治疗。


<details>
  <summary>Details</summary>
Motivation: 现有机械通气（MV）控制方法因混合动作空间（连续/离散）的复杂性面临组合爆炸和分布偏移问题，且传统基于死亡率的稀疏奖励无法有效指导优化。

Method: 结合动作空间降维优化，改进离线强化学习算法（IQL/EDAC）以直接支持混合动作空间，并设计基于脱机天数和生理指标的临床奖励函数。

Result: 所提方法避免了离散化导致的安全风险，实现了更个体化的通气支持，验证了AI辅助MV优化的临床可行性。

Conclusion: 研究推动了数据驱动的智能重症护理发展，通过混合动作空间处理与临床导向奖励机制，为机械通气安全性和个性化提供新解决方案。

Abstract: Invasive mechanical ventilation (MV) is a life-sustaining therapy for critically ill patients in the intensive care unit (ICU). However, optimizing its settings remains a complex and error-prone process due to patient-specific variability. While Offline Reinforcement Learning (RL) shows promise for MV control, current stateof-the-art (SOTA) methods struggle with the hybrid (continuous and discrete) nature of MV actions. Discretizing the action space limits available actions due to exponential growth in combinations and introduces distribution shifts that can compromise safety. In this paper, we propose optimizations that build upon prior work in action space reduction to address the challenges of discrete action spaces. We also adapt SOTA offline RL algorithms (IQL and EDAC) to operate directly on hybrid action spaces, thereby avoiding the pitfalls of discretization. Additionally, we introduce a clinically grounded reward function based on ventilator-free days and physiological targets, which provides a more meaningful optimization objective compared to traditional sparse mortality-based rewards. Our findings demonstrate that AI-assisted MV optimization may enhance patient safety and enable individualized lung support, representing a significant advancement toward intelligent, data-driven critical care solutions.

</details>


### [164] [ResNets Are Deeper Than You Think](https://arxiv.org/abs/2506.14386)
*Christian H. X. Ali Mehmeti-Göpel,Michael Wand*

Main category: cs.LG

TL;DR: 残差网络通过不同的函数空间和归纳偏置实现性能优势，而非仅优化改进。


<details>
  <summary>Details</summary>
Motivation: 探究残差网络与普通前馈网络性能差距持续存在的根本原因，挑战传统优化视角的解释。

Method: 设计后训练对比实验，隔离泛化性能与可训练性变量，比较可变深度架构与固定深度网络。

Result: 可变深度架构(如ResNet)在优化无关场景下仍显著超越固定深度网络，表明残差连接具有超越优化的本质优势。

Conclusion: 残差连接通过其与自然数据结构的对齐形成深层归纳偏置，提供独立于优化过程的性能增益。

Abstract: Residual connections remain ubiquitous in modern neural network architectures nearly a decade after their introduction. Their widespread adoption is often credited to their dramatically improved trainability: residual networks train faster, more stably, and achieve higher accuracy than their feedforward counterparts. While numerous techniques, ranging from improved initialization to advanced learning rate schedules, have been proposed to close the performance gap between residual and feedforward networks, this gap has persisted. In this work, we propose an alternative explanation: residual networks do not merely reparameterize feedforward networks, but instead inhabit a different function space. We design a controlled post-training comparison to isolate generalization performance from trainability; we find that variable-depth architectures, similar to ResNets, consistently outperform fixed-depth networks, even when optimization is unlikely to make a difference. These results suggest that residual connections confer performance advantages beyond optimization, pointing instead to a deeper inductive bias aligned with the structure of natural data.

</details>


### [165] [Enclosing Prototypical Variational Autoencoder for Explainable Out-of-Distribution Detection](https://arxiv.org/abs/2506.14390)
*Conrad Orglmeister,Erik Bochinski,Volker Eiselein,Elvira Fleig*

Main category: cs.LG

TL;DR: 本文提出一种结合自解释原型变分模型与自编码器分布外检测的方法，通过变分自编码器构建潜在空间，实现分类、分布外检测及重构，并引入限制损失优化潜在空间结构，提升模型可解释性与检测性能。


<details>
  <summary>Details</summary>
Motivation: 为提高深度机器学习模型在安全关键应用中的可信度，需增强其决策可解释性并确保对分布外样本的可靠检测。现有方法在潜在空间紧凑性和解释性方面存在不足。

Method: 使用变分自编码器构建潜在空间，结合高斯混合原型定义分布内区域；提出限制损失函数压缩潜在空间分布但不坍缩，利用重构能力解释原型与分类边界。

Result: 在标准分布外检测基准及真实铁路数据集上表现优于现有方法，验证了潜在空间紧凑性和原型可解释性对检测性能的提升。

Conclusion: 该方法通过联合优化分类、重构与分布外识别任务，实现了高可解释性且高效的模型，为安全关键场景提供了可靠解决方案。

Abstract: Understanding the decision-making and trusting the reliability of Deep Machine Learning Models is crucial for adopting such methods to safety-relevant applications. We extend self-explainable Prototypical Variational models with autoencoder-based out-of-distribution (OOD) detection: A Variational Autoencoder is applied to learn a meaningful latent space which can be used for distance-based classification, likelihood estimation for OOD detection, and reconstruction. The In-Distribution (ID) region is defined by a Gaussian mixture distribution with learned prototypes representing the center of each mode. Furthermore, a novel restriction loss is introduced that promotes a compact ID region in the latent space without collapsing it into single points. The reconstructive capabilities of the Autoencoder ensure the explainability of the prototypes and the ID region of the classifier, further aiding the discrimination of OOD samples. Extensive evaluations on common OOD detection benchmarks as well as a large-scale dataset from a real-world railway application demonstrate the usefulness of the approach, outperforming previous methods.

</details>


### [166] [HiLight: A Hierarchical Reinforcement Learning Framework with Global Adversarial Guidance for Large-Scale Traffic Signal Control](https://arxiv.org/abs/2506.14391)
*Yaqiao Zhu,Hongkai Wen,Geyong Min,Man Luo*

Main category: cs.LG

TL;DR: 提出HiLight分层强化学习框架，通过全局对抗指导解决大规模交通信号控制中的协调与扩展性问题，实验验证其在大规模场景下的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在扩展至大规模交通网络时面临集中式方法可扩展性差、分散式方法缺乏全局协调的挑战，导致网络级效率受限。

Method: HiLight框架包含高层Meta-Policy（基于Transformer-LSTM划分区域并生成子目标）和低层Sub-Policy（控制单个路口），通过对抗训练机制增强全局规划与局部执行的协同。

Result: 在合成/真实基准测试及构建的曼哈顿大规模网络中（含高峰、恶劣天气等复杂场景），HiLight显著优于现有方法并保持跨规模竞争力。

Conclusion: HiLight通过分层结构与对抗训练实现了大规模交通信号的高效协调控制，为复杂动态城市交通管理提供了有效解决方案。

Abstract: Efficient traffic signal control (TSC) is essential for mitigating urban congestion, yet existing reinforcement learning (RL) methods face challenges in scaling to large networks while maintaining global coordination. Centralized RL suffers from scalability issues, while decentralized approaches often lack unified objectives, resulting in limited network-level efficiency. In this paper, we propose HiLight, a hierarchical reinforcement learning framework with global adversarial guidance for large-scale TSC. HiLight consists of a high-level Meta-Policy, which partitions the traffic network into subregions and generates sub-goals using a Transformer-LSTM architecture, and a low-level Sub-Policy, which controls individual intersections with global awareness. To improve the alignment between global planning and local execution, we introduce an adversarial training mechanism, where the Meta-Policy generates challenging yet informative sub-goals, and the Sub-Policy learns to surpass these targets, leading to more effective coordination. We evaluate HiLight across both synthetic and real-world benchmarks, and additionally construct a large-scale Manhattan network with diverse traffic conditions, including peak transitions, adverse weather, and holiday surges. Experimental results show that HiLight exhibits significant advantages in large-scale scenarios and remains competitive across standard benchmarks of varying sizes.

</details>


### [167] [One Size Fits None: Rethinking Fairness in Medical AI](https://arxiv.org/abs/2506.14400)
*Roland Roller,Michael Hahn,Ajay Madhavan Ravichandran,Bilgin Osmanodja,Florian Oetke,Zeineb Sassi,Aljoscha Burchardt,Klaus Netter,Klemens Budde,Anne Herrmann,Tobias Strapatsas,Peter Dabrock,Sebastian Möller*

Main category: cs.LG

TL;DR: 医疗机器学习模型在亚组患者中可能存在性能差异，需通过亚组分析确保公平性和透明度。


<details>
  <summary>Details</summary>
Motivation: 现实医疗数据集常存在噪声、不完整和类别不平衡问题，导致模型在不同患者亚组间性能差异，可能加剧边缘群体的临床决策劣势。

Method: 通过分析医疗预测任务，评估模型在患者亚组特征（如人口统计学指标）下的性能表现差异。

Result: 模型整体性能良好，但亚组层面存在显著性能差异，揭示现有模型可能强化医疗不公平现象。

Conclusion: 临床部署前必须进行亚组性能评估，这既能指导临床实践中的差异化管理，也能推动开发更公平有效的医疗AI模型。

Abstract: Machine learning (ML) models are increasingly used to support clinical decision-making. However, real-world medical datasets are often noisy, incomplete, and imbalanced, leading to performance disparities across patient subgroups. These differences raise fairness concerns, particularly when they reinforce existing disadvantages for marginalized groups. In this work, we analyze several medical prediction tasks and demonstrate how model performance varies with patient characteristics. While ML models may demonstrate good overall performance, we argue that subgroup-level evaluation is essential before integrating them into clinical workflows. By conducting a performance analysis at the subgroup level, differences can be clearly identified-allowing, on the one hand, for performance disparities to be considered in clinical practice, and on the other hand, for these insights to inform the responsible development of more effective models. Thereby, our work contributes to a practical discussion around the subgroup-sensitive development and deployment of medical ML models and the interconnectedness of fairness and transparency.

</details>


### [168] [Adaptive Reinforcement Learning for Unobservable Random Delays](https://arxiv.org/abs/2506.14411)
*John Wikman,Alexandre Proutiere,David Broman*

Main category: cs.LG

TL;DR: 本文提出了一种名为ACDA的模型基础算法，通过引入交互层框架，使强化学习智能体能自适应处理不可观测的时变延迟问题，在多个运动控制基准环境中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习假设智能体与环境交互无延迟，但现实场景（如信息物理系统）存在不可观测的随机时变延迟。现有方法采用固定延迟上限的保守策略，无法适应实际动态变化需求。

Method: 提出交互层框架生成未来动作矩阵，应对网络延迟和动作包丢失问题。基于此开发Actor-Critic with Delay Adaptation (ACDA)算法，动态适应延迟模式。

Result: 在广泛运动控制基准测试中，ACDA算法性能显著超越现有最先进方法，验证了框架的有效性。

Conclusion: 交互层框架与ACDA算法成功解决了强化学习在动态环境中的延迟适应问题，为实际系统部署提供了更优解决方案。

Abstract: In standard Reinforcement Learning (RL) settings, the interaction between the agent and the environment is typically modeled as a Markov Decision Process (MDP), which assumes that the agent observes the system state instantaneously, selects an action without delay, and executes it immediately. In real-world dynamic environments, such as cyber-physical systems, this assumption often breaks down due to delays in the interaction between the agent and the system. These delays can vary stochastically over time and are typically unobservable, meaning they are unknown when deciding on an action. Existing methods deal with this uncertainty conservatively by assuming a known fixed upper bound on the delay, even if the delay is often much lower. In this work, we introduce the interaction layer, a general framework that enables agents to adaptively and seamlessly handle unobservable and time-varying delays. Specifically, the agent generates a matrix of possible future actions to handle both unpredictable delays and lost action packets sent over networks. Building on this framework, we develop a model-based algorithm, Actor-Critic with Delay Adaptation (ACDA), which dynamically adjusts to delay patterns. Our method significantly outperforms state-of-the-art approaches across a wide range of locomotion benchmark environments.

</details>


### [169] [Unsupervised Skill Discovery through Skill Regions Differentiation](https://arxiv.org/abs/2506.14420)
*Ting Xiao,Jiakun Zheng,Rushuai Yang,Kang Xu,Qiaosheng Zhang,Peng Liu,Chenjia Bai*

Main category: cs.LG

TL;DR: 本文提出一种新的无监督强化学习方法，通过最大化技能间状态密度偏差和基于条件自编码器的潜在空间探索，解决了传统熵探索和互信息方法在高维状态空间的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有无监督强化学习方法中，基于熵的探索方法难以处理图像等大规模状态空间，而基于互信息的技能学习方法在状态探索方面存在不足。需要新的方法实现技能间状态多样性和技能内有效探索。

Method: 1) 提出最大化技能间状态密度偏差的新目标函数；2) 设计具有软模块化的条件自编码器进行高维状态密度估计；3) 构建基于自编码器潜在空间的类计数探索内在奖励机制。

Result: 在复杂状态和图像任务中的实验表明，该方法能学习到有意义的技能，并在多种下游任务中取得优于基线方法的性能表现。

Conclusion: 通过结合技能间状态差异最大化和潜在空间探索机制，该方法有效解决了高维状态空间下的无监督技能发现问题，为下游任务提供了优质初始化策略。

Abstract: Unsupervised Reinforcement Learning (RL) aims to discover diverse behaviors that can accelerate the learning of downstream tasks. Previous methods typically focus on entropy-based exploration or empowerment-driven skill learning. However, entropy-based exploration struggles in large-scale state spaces (e.g., images), and empowerment-based methods with Mutual Information (MI) estimations have limitations in state exploration. To address these challenges, we propose a novel skill discovery objective that maximizes the deviation of the state density of one skill from the explored regions of other skills, encouraging inter-skill state diversity similar to the initial MI objective. For state-density estimation, we construct a novel conditional autoencoder with soft modularization for different skill policies in high-dimensional space. Meanwhile, to incentivize intra-skill exploration, we formulate an intrinsic reward based on the learned autoencoder that resembles count-based exploration in a compact latent space. Through extensive experiments in challenging state and image-based tasks, we find our method learns meaningful skills and achieves superior performance in various downstream tasks.

</details>


### [170] [MoORE: SVD-based Model MoE-ization for Conflict- and Oblivion-Resistant Multi-Task Adaptation](https://arxiv.org/abs/2506.14436)
*Shen Yuan,Yin Zheng,Taifeng Wang,Binbin Liu,Hongteng Xu*

Main category: cs.LG

TL;DR: 本文提出了一种名为MoORE的新方法，通过奇异值分解（SVD）和可学习路由器调整预训练模型的权重矩阵，形成正交专家混合体，以解决多任务适应中的任务冲突和遗忘问题。实验表明该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大规模基础模型在多任务适应中常面临任务冲突和遗忘问题。现有方法（如LoRA及其混合专家变体）无法同时保证专家正交性和原始权重矩阵的列空间，导致性能受限。

Method: 对预训练模型的权重矩阵进行SVD分解，引入可学习路由器动态调整奇异值，构建正交秩一专家混合体（MoORE）。通过右奇异向量的可学习正交变换提升模型容量，并保持原始列空间。

Result: 在多个数据集上的实验表明，MoORE在任务冲突和遗忘抵抗方面优于现有方法，验证了其正交性和列空间保持的有效性。

Conclusion: MoORE通过正交专家设计和原始列空间保留，有效缓解多任务适应中的冲突与遗忘问题，为大规模模型的高效适应提供了新思路。

Abstract: Adapting large-scale foundation models in multi-task scenarios often suffers from task conflict and oblivion. To mitigate such issues, we propose a novel ''model MoE-ization'' strategy that leads to a conflict- and oblivion-resistant multi-task adaptation method. Given a weight matrix of a pre-trained model, our method applies SVD to it and introduces a learnable router to adjust its singular values based on tasks and samples. Accordingly, the weight matrix becomes a Mixture of Orthogonal Rank-one Experts (MoORE), in which each expert corresponds to the outer product of a left singular vector and the corresponding right one. We can improve the model capacity by imposing a learnable orthogonal transform on the right singular vectors. Unlike low-rank adaptation (LoRA) and its MoE-driven variants, MoORE guarantees the experts' orthogonality and maintains the column space of the original weight matrix. These two properties make the adapted model resistant to the conflicts among the new tasks and the oblivion of its original tasks, respectively. Experiments on various datasets demonstrate that MoORE outperforms existing multi-task adaptation methods consistently, showing its superiority in terms of conflict- and oblivion-resistance. The code of the experiments is available at https://github.com/DaShenZi721/MoORE.

</details>


### [171] [sHGCN: Simplified hyperbolic graph convolutional neural networks](https://arxiv.org/abs/2506.14438)
*Pol Arévalo,Alexis Molina,Álvaro Ciudad*

Main category: cs.LG

TL;DR: 通过简化双曲神经网络中的操作，显著提升计算速度和预测精度，扩展其应用范围。


<details>
  <summary>Details</summary>
Motivation: 双曲几何虽能有效建模层次化数据，但其神经网络存在效率与精度不足的问题。

Method: 简化双曲神经网络中的核心操作以降低计算复杂度。

Result: 在运行时间和预测准确性上取得显著改进，计算速度大幅提升。

Conclusion: 优化后的双曲神经网络更具实用性，可适用于更广泛的任务场景。

Abstract: Hyperbolic geometry has emerged as a powerful tool for modeling complex, structured data, particularly where hierarchical or tree-like relationships are present. By enabling embeddings with lower distortion, hyperbolic neural networks offer promising alternatives to Euclidean-based models for capturing intricate data structures. Despite these advantages, they often face performance challenges, particularly in computational efficiency and tasks requiring high precision. In this work, we address these limitations by simplifying key operations within hyperbolic neural networks, achieving notable improvements in both runtime and performance. Our findings demonstrate that streamlined hyperbolic operations can lead to substantial gains in computational speed and predictive accuracy, making hyperbolic neural networks a more viable choice for a broader range of applications.

</details>


### [172] [A General Framework for Off-Policy Learning with Partially-Observed Reward](https://arxiv.org/abs/2506.14439)
*Rikiya Takehi,Masahiro Asami,Kosuke Kawakami,Yuta Saito*

Main category: cs.LG

TL;DR: 本文提出HyPeR方法，利用次级奖励补充部分观测的目标奖励，以提升上下文老虎机中的离策略学习效果，并在多目标优化中展示优势。


<details>
  <summary>Details</summary>
Motivation: 部分观测的目标奖励（如推荐评分、转化信号）导致传统离策略学习效果下降，而仅依赖次级奖励（如点击量）可能因目标不一致而效果不佳。需结合两者以优化策略。

Method: HyPeR方法通过混合优化部分观测的目标奖励与密集次级奖励，设计新算法以同时利用两类数据，并探讨多目标优化的协同效应。

Result: 理论分析与实验验证表明，HyPeR在合成和真实数据中均优于现有方法，且多目标优化可间接提升目标奖励的优化效果。

Conclusion: HyPeR通过结合目标与次级奖励，有效解决部分观测奖励问题，多目标优化策略进一步强化了目标奖励的优化能力。

Abstract: Off-policy learning (OPL) in contextual bandits aims to learn a decision-making policy that maximizes the target rewards by using only historical interaction data collected under previously developed policies. Unfortunately, when rewards are only partially observed, the effectiveness of OPL degrades severely. Well-known examples of such partial rewards include explicit ratings in content recommendations, conversion signals on e-commerce platforms that are partial due to delay, and the issue of censoring in medical problems. One possible solution to deal with such partial rewards is to use secondary rewards, such as dwelling time, clicks, and medical indicators, which are more densely observed. However, relying solely on such secondary rewards can also lead to poor policy learning since they may not align with the target reward. Thus, this work studies a new and general problem of OPL where the goal is to learn a policy that maximizes the expected target reward by leveraging densely observed secondary rewards as supplemental data. We then propose a new method called Hybrid Policy Optimization for Partially-Observed Reward (HyPeR), which effectively uses the secondary rewards in addition to the partially-observed target reward to achieve effective OPL despite the challenging scenario. We also discuss a case where we aim to optimize not only the expected target reward but also the expected secondary rewards to some extent; counter-intuitively, we will show that leveraging the two objectives is in fact advantageous also for the optimization of only the target reward. Along with statistical analysis of our proposed methods, empirical evaluations on both synthetic and real-world data show that HyPeR outperforms existing methods in various scenarios.

</details>


### [173] [Detecting immune cells with label-free two-photon autofluorescence and deep learning](https://arxiv.org/abs/2506.14449)
*Lucas Kreiss,Amey Chaware,Maryam Roohian,Sarah Lemire,Oana-Maria Thoma,Birgitta Carlé,Maximilian Waldner,Sebastian Schürmann,Oliver Friedrich,Roarke Horstmeyer*

Main category: cs.LG

TL;DR: 本研究利用低复杂度卷积神经网络（squeezeNet）对无标记多光子显微镜（MPM）的免疫细胞图像进行分类，验证了模型在二元及多类任务中的有效性，并证明其未来在体内内窥镜应用的潜力。


<details>
  <summary>Details</summary>
Motivation: 无标记成像技术（如MPM）因无需染色而适用于体内检测，但其缺乏特异性。深度学习虽在其他光学成像中广泛应用，但在MPM中尚未充分探索。本研究旨在通过深度学习提升无标记MPM的细胞分类特异性。

Method: 使用5,075个混合样本细胞进行二元分类，3,424个独立样本细胞进行六类分类，训练低复杂度squeezeNet模型，以NADH和FAD通道的自发荧光（AF）为输入，并通过扰动测试验证模型鲁棒性。

Result: 模型性能：二元分类（ROC-AUC 0.89，PR-AUC 0.95）；六类分类（F1 0.689，精度0.697，召回率0.748，MCC 0.683）。扰动测试表明模型不受细胞外干扰，且两输入通道贡献相近。

Conclusion: 深度学习可有效提升无标记MPM的细胞分类特异性，未来或直接用于体内未染色图像的免疫细胞检测，扩展其在活体内窥镜中的应用价值。

Abstract: Label-free imaging has gained broad interest because of its potential to omit elaborate staining procedures which is especially relevant for in vivo use. Label-free multiphoton microscopy (MPM), for instance, exploits two-photon excitation of natural autofluorescence (AF) from native, metabolic proteins, making it ideal for in vivo endomicroscopy. Deep learning (DL) models have been widely used in other optical imaging technologies to predict specific target annotations and thereby digitally augment the specificity of these label-free images. However, this computational specificity has only rarely been implemented for MPM. In this work, we used a data set of label-free MPM images from a series of different immune cell types (5,075 individual cells for binary classification in mixed samples and 3,424 cells for a multi-class classification task) and trained a convolutional neural network (CNN) to classify cell types based on this label-free AF as input. A low-complexity squeezeNet architecture was able to achieve reliable immune cell classification results (0.89 ROC-AUC, 0.95 PR-AUC, for binary classification in mixed samples; 0.689 F1 score, 0.697 precision, 0.748 recall, and 0.683 MCC for six-class classification in isolated samples). Perturbation tests confirmed that the model is not confused by extracellular environment and that both input AF channels (NADH and FAD) are about equally important to the classification. In the future, such predictive DL models could directly detect specific immune cells in unstained images and thus, computationally improve the specificity of label-free MPM which would have great potential for in vivo endomicroscopy.

</details>


### [174] [Dataset distillation for memorized data: Soft labels can leak held-out teacher knowledge](https://arxiv.org/abs/2506.14457)
*Freya Behrens,Lenka Zdeborová*

Main category: cs.LG

TL;DR: 研究发现，在数据集蒸馏中，学生模型通过教师模型的软标签学习，即使未直接接触被记忆数据，仍能获得显著准确率。温度参数是关键因素，且现象在不同网络结构中普遍存在。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注数据结构的压缩效果，但忽视了神经网络记忆特定事实的能力是否及如何通过蒸馏传递。本文旨在探究教师模型未泛化时，学生模型通过软标签是否仍能学习到被记忆的隐藏数据信息。

Method: 在无法泛化的有限随机独立同分布数据集上，通过教师模型纯拟合（即记忆）生成软标签，训练学生模型。分析温度参数、网络容量、架构及数据组成对信息传递的影响。

Result: 学生模型在未观察的隐藏数据上达到非平凡准确率（部分情况完美匹配），且可通过软标签完全复现教师预测。温度显著影响现象强度，但结论在不同实验设置下稳健。

Conclusion: 蒸馏中软标签可传递教师记忆的特定信息，即使无泛化能力。温度是核心调节因素，这一发现挑战了仅依赖数据结构解释蒸馏效果的传统认知。

Abstract: Dataset distillation aims to compress training data into fewer examples via a teacher, from which a student can learn effectively. While its success is often attributed to structure in the data, modern neural networks also memorize specific facts, but if and how such memorized information is can transferred in distillation settings remains less understood. In this work, we show that students trained on soft labels from teachers can achieve non-trivial accuracy on held-out memorized data they never directly observed. This effect persists on structured data when the teacher has not generalized.To analyze it in isolation, we consider finite random i.i.d. datasets where generalization is a priori impossible and a successful teacher fit implies pure memorization. Still, students can learn non-trivial information about the held-out data, in some cases up to perfect accuracy. In those settings, enough soft labels are available to recover the teacher functionally - the student matches the teacher's predictions on all possible inputs, including the held-out memorized data. We show that these phenomena strongly depend on the temperature with which the logits are smoothed, but persist across varying network capacities, architectures and dataset compositions.

</details>


### [175] [A Model-Mediated Stacked Ensemble Approach for Depression Prediction Among Professionals](https://arxiv.org/abs/2506.14459)
*Md. Mortuza Ahmmed,Abdullah Al Noman,Mahin Montasir Afif,K. M. Tahsin Kabir,Md. Mostafizur Rahman,Mufti Mahmud*

Main category: cs.LG

TL;DR: 该研究提出一种基于堆叠集成学习的方法，用于提升职业人群抑郁症分类预测的准确性，实验结果显示训练和测试准确率分别达99.64%和98.75%，关键指标均超98%。


<details>
  <summary>Details</summary>
Motivation: 职业环境中抑郁症受多重复杂因素（如工作压力、睡眠模式）影响，传统分类模型难以有效捕捉其关联性，需开发更精准、可泛化的预测方法。

Method: 使用Kaggle的抑郁症职业数据集，结合多个基学习器与逻辑回归元模型构建堆叠集成模型，整合人口统计、职业及生活方式特征以捕捉多样化学习模式。

Result: 模型在训练和测试数据上分别达到99.64%和98.75%的准确率，精确度、召回率与F1分数均超过98%，显著优于传统方法。

Conclusion: 集成学习能有效提升心理健康分析的预测性能，为早期抑郁症检测及干预策略提供了可靠技术路径。

Abstract: Depression is a significant mental health concern, particularly in professional environments where work-related stress, financial pressure, and lifestyle imbalances contribute to deteriorating well-being. Despite increasing awareness, researchers and practitioners face critical challenges in developing accurate and generalizable predictive models for mental health disorders. Traditional classification approaches often struggle with the complexity of depression, as it is influenced by multifaceted, interdependent factors, including occupational stress, sleep patterns, and job satisfaction. This study addresses these challenges by proposing a stacking-based ensemble learning approach to improve the predictive accuracy of depression classification among professionals. The Depression Professional Dataset has been collected from Kaggle. The dataset comprises demographic, occupational, and lifestyle attributes that influence mental well-being. Our stacking model integrates multiple base learners with a logistic regression-mediated model, effectively capturing diverse learning patterns. The experimental results demonstrate that the proposed model achieves high predictive performance, with an accuracy of 99.64% on training data and 98.75% on testing data, with precision, recall, and F1-score all exceeding 98%. These findings highlight the effectiveness of ensemble learning in mental health analytics and underscore its potential for early detection and intervention strategies.

</details>


### [176] [Zeroth-Order Optimization is Secretly Single-Step Policy Optimization](https://arxiv.org/abs/2506.14460)
*Junbin Qiu,Zhengpeng Xie,Xiangda Yan,Yongjie Yang,Yao Shu*

Main category: cs.LG

TL;DR: 本文揭示了零阶优化（ZOO）与单步策略优化（PO）的等价性，并提出结合PO方差缩减技术的新算法ZoAR，显著提升收敛速度和性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究未明确ZOO方法与强化学习策略优化（PO）的内在联系，且随机有限差分等ZOO机制缺乏理论解释。本文旨在建立ZOO与PO的理论桥梁，并基于此改进算法。

Method: 通过理论证明ZOO的隐式平滑目标函数等价于单步PO目标，其梯度估计器与REINFORCE梯度估计器数学等价。提出ZoAR算法，引入PO的方差缩减技术（平均基线、查询重用）。

Result: 理论分析表明ZoAR降低方差并加速收敛；实验验证其收敛速度和最终性能显著优于现有方法。

Conclusion: 本文为理解ZOO提供了新理论视角，并通过与PO的关联提出改进算法，兼具理论创新与实用价值。

Abstract: Zeroth-Order Optimization (ZOO) provides powerful tools for optimizing functions where explicit gradients are unavailable or expensive to compute. However, the underlying mechanisms of popular ZOO methods, particularly those employing randomized finite differences, and their connection to other optimization paradigms like Reinforcement Learning (RL) are not fully elucidated. This paper establishes a fundamental and previously unrecognized connection: ZOO with finite differences is equivalent to a specific instance of single-step Policy Optimization (PO). We formally unveil that the implicitly smoothed objective function optimized by common ZOO algorithms is identical to a single-step PO objective. Furthermore, we show that widely used ZOO gradient estimators, are mathematically equivalent to the REINFORCE gradient estimator with a specific baseline function, revealing the variance-reducing mechanism in ZOO from a PO perspective.Built on this unified framework, we propose ZoAR (Zeroth-Order Optimization with Averaged Baseline and Query Reuse), a novel ZOO algorithm incorporating PO-inspired variance reduction techniques: an averaged baseline from recent evaluations and query reuse analogous to experience replay. Our theoretical analysis further substantiates these techniques reduce variance and enhance convergence. Extensive empirical studies validate our theory and demonstrate that ZoAR significantly outperforms other methods in terms of convergence speed and final performance. Overall, our work provides a new theoretical lens for understanding ZOO and offers practical algorithmic improvements derived from its connection to PO.

</details>


### [177] [Leveraging External Factors in Household-Level Electrical Consumption Forecasting using Hypernetworks](https://arxiv.org/abs/2506.14472)
*Fabien Bernier,Maxime Cordy,Yves Le Traon*

Main category: cs.LG

TL;DR: 通过超网络架构结合外部因素（如天气、节假日）优化全局电力消耗预测模型，在保持全局模型优势的同时提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统全局预测模型在引入外部因素时性能下降，而个体模型虽受益但缺乏扩展性。需找到既能利用外部因素又能维持全局模型效率的方法。

Method: 采用超网络架构动态调整各用户的模型权重，结合超过6000户卢森堡家庭的用电数据及天气、事件等外部特征进行训练。

Result: 超网络方法在外部因素加持下显著降低预测误差，优于现有方法，且保留了全局模型的泛化能力。

Conclusion: 超网络有效解决了全局模型引入外部因素时的性能退化问题，为复杂场景下的电力预测提供了兼顾精度与效率的解决方案。

Abstract: Accurate electrical consumption forecasting is crucial for efficient energy management and resource allocation. While traditional time series forecasting relies on historical patterns and temporal dependencies, incorporating external factors -- such as weather indicators -- has shown significant potential for improving prediction accuracy in complex real-world applications. However, the inclusion of these additional features often degrades the performance of global predictive models trained on entire populations, despite improving individual household-level models. To address this challenge, we found that a hypernetwork architecture can effectively leverage external factors to enhance the accuracy of global electrical consumption forecasting models, by specifically adjusting the model weights to each consumer.
  We collected a comprehensive dataset spanning two years, comprising consumption data from over 6000 luxembourgish households and corresponding external factors such as weather indicators, holidays, and major local events. By comparing various forecasting models, we demonstrate that a hypernetwork approach outperforms existing methods when associated to external factors, reducing forecasting errors and achieving the best accuracy while maintaining the benefits of a global model.

</details>


### [178] [Train Once, Forget Precisely: Anchored Optimization for Efficient Post-Hoc Unlearning](https://arxiv.org/abs/2506.14515)
*Prabhav Sanga,Jaskaran Singh,Arun K. Dubey*

Main category: cs.LG

TL;DR: 本文提出了一种名为FAMR的理论驱动框架，用于在深度图像分类器中高效实现事后遗忘，通过约束优化最小化遗忘集的均匀预测损失，同时通过L2惩罚锚定原始模型参数。该方法在类别、概念和风格遗忘任务中表现出高效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习系统越来越多地依赖受隐私法规约束的数据，如何在不完全重新训练模型的情况下，选择性遗忘特定训练样本、语义类别或视觉风格的影响，成为关键需求。

Method: FAMR将遗忘建模为约束优化问题，结合遗忘集上的均匀预测损失最小化和基于L2惩罚的参数锚定机制，其理论分析表明该方法与基于影响函数的再训练近似相关，并给出参数与输出偏差的边界。

Result: 在CIFAR-10和ImageNet-100的类别遗忘任务中，FAMR在保持模型性能的同时显著降低计算开销，且可自然推广至概念和风格擦除任务。

Conclusion: FAMR为视觉模型提供了一种可扩展、可验证的高效事后遗忘路径，通过理论保证和实证验证实现了隐私合规与模型效用的平衡。

Abstract: As machine learning systems increasingly rely on data subject to privacy regulation, selectively unlearning specific information from trained models has become essential. In image classification, this involves removing the influence of particular training samples, semantic classes, or visual styles without full retraining. We introduce \textbf{Forget-Aligned Model Reconstruction (FAMR)}, a theoretically grounded and computationally efficient framework for post-hoc unlearning in deep image classifiers. FAMR frames forgetting as a constrained optimization problem that minimizes a uniform-prediction loss on the forget set while anchoring model parameters to their original values via an $\ell_2$ penalty. A theoretical analysis links FAMR's solution to influence-function-based retraining approximations, with bounds on parameter and output deviation. Empirical results on class forgetting tasks using CIFAR-10 and ImageNet-100 demonstrate FAMR's effectiveness, with strong performance retention and minimal computational overhead. The framework generalizes naturally to concept and style erasure, offering a scalable and certifiable route to efficient post-hoc forgetting in vision models.

</details>


### [179] [Two-Player Zero-Sum Games with Bandit Feedback](https://arxiv.org/abs/2506.14518)
*Elif Yılmaz,Christos Dimitrakakis*

Main category: cs.LG

TL;DR: 本文研究两人零和博弈中行玩家在对抗性列玩家下通过老虎机反馈估计未知收益矩阵的纯策略纳什均衡学习问题，提出ETC-TPZSG和ETC-TPZSG-AE两种算法，推导其实例相关遗憾上界，证明ETC算法在对抗性博弈中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有文献对零和博弈中基于实例相关分析的遗憾上界研究不足。本文旨在探索ETC（探索-利用-提交）框架在对抗性博弈中的适用性，并通过动作对消除策略提升算法效率。

Method: 提出ETC算法变体：ETC-TPZSG直接应用ETC框架；ETC-TPZSG-AE引入动作对消除策略，利用ε-纳什均衡性质动态剔除次优动作对，优化探索阶段效率。

Result: ETC-TPZSG在T轮后达到O(Δ+√T)的实例相关遗憾上界，ETC-TPZSG-AE进一步优化至O(log(TΔ²)/Δ)，表明其性能与现有方法相当，且通过实例相关分析提供理论解释。

Conclusion: 基于ETC的算法在对抗性博弈中表现有效，其遗憾上界与现有方法可比，同时通过实例相关分析为博弈动态提供了新的理论洞察。

Abstract: We study a two-player zero-sum game (TPZSG) in which the row player aims to maximize their payoff against an adversarial column player, under an unknown payoff matrix estimated through bandit feedback. We propose and analyze two algorithms: ETC-TPZSG, which directly applies ETC to the TPZSG setting and ETC-TPZSG-AE, which improves upon it by incorporating an action pair elimination (AE) strategy that leverages the $\varepsilon$-Nash Equilibrium property to efficiently select the optimal action pair. Our objective is to demonstrate the applicability of ETC in a TPZSG setting by focusing on learning pure strategy Nash Equilibrium. A key contribution of our work is a derivation of instance-dependent upper bounds on the expected regret for both algorithms, has received limited attention in the literature on zero-sum games. Particularly, after $T$ rounds, we achieve an instance-dependent regret upper bounds of $O(Δ+ \sqrt{T})$ for ETC-TPZSG and $O(\frac{\log (T Δ^2)}Δ)$ for ETC-TPZSG-AE, where $Δ$ denotes the suboptimality gap. Therefore, our results indicate that ETC-based algorithms perform effectively in adversarial game settings, achieving regret bounds comparable to existing methods while providing insights through instance-dependent analysis.

</details>


### [180] [Towards Improved Research Methodologies for Industrial AI: A case study of false call reduction](https://arxiv.org/abs/2506.14521)
*Korbinian Pfab,Marcel Rothering*

Main category: cs.LG

TL;DR: 本文通过工业AI应用案例（自动光学检测中的误报减少）揭示当前AI研究方法的不足，指出七项常见缺陷，并论证需采用需求感知指标、明确成功标准及时间动态分析以提升应用效果。


<details>
  <summary>Details</summary>
Motivation: 验证现有AI研究方法是否足以开发成功、高效且盈利的工业应用，尤其针对当前最佳实践在真实场景中的局限性。

Method: 以工业级误报减少案例为研究对象，分析同行评审文献中的普遍弱点，并通过实验验证这些缺陷的实际影响。

Result: 实验表明现有最佳实践方法在此案例中会失败，且识别出七项关键弱点（如缺乏业务目标导向的指标、未明确定义成功标准等）。

Conclusion: 呼吁研究者批判性评估方法论，强调需求感知指标、成功标准清晰化及数据集时间动态分析对提升AI应用成功率的重要性。

Abstract: Are current artificial intelligence (AI) research methodologies ready to create successful, productive, and profitable AI applications? This work presents a case study on an industrial AI use case called false call reduction for automated optical inspection to demonstrate the shortcomings of current best practices. We identify seven weaknesses prevalent in related peer-reviewed work and experimentally show their consequences. We show that the best-practice methodology would fail for this use case. We argue amongst others for the necessity of requirement-aware metrics to ensure achieving business objectives, clear definitions of success criteria, and a thorough analysis of temporal dynamics in experimental datasets. Our work encourages researchers to critically assess their methodologies for more successful applied AI research.

</details>


### [181] [Automated Decision-Making on Networks with LLMs through Knowledge-Guided Evolution](https://arxiv.org/abs/2506.14529)
*Xiaohan Zheng,Lanning Wei,Yong Li,Quanming Yao*

Main category: cs.LG

TL;DR: LLMNet利用大语言模型（LLMs）自动化设计图神经网络（GNNs），通过知识库构建与检索增强生成（RAG）实现GNN的自动配置与调优，在三个图学习任务的12个数据集中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统GNN模型依赖人工配置与调优，效率较低。研究旨在通过LLMs自动化GNN设计，减少人工干预并提升模型性能。

Method: 系统开发多个代理构建图相关知识库，结合RAG技术实现知识引导的GNN模型进化。代理通过知识库交互提取任务与图结构特征，自动化完成模型配置与优化。

Result: 在三个图学习任务（未具体说明）的12个数据集上，LLMNet表现优于基准方法，验证其自动化设计GNN的有效性。

Conclusion: LLMNet证明LLMs与知识库结合可高效自动化GNN设计，为图结构数据学习提供可扩展的解决方案。

Abstract: Effective decision-making on networks often relies on learning from graph-structured data, where Graph Neural Networks (GNNs) play a central role, but they take efforts to configure and tune. In this demo, we propose LLMNet, showing how to design GNN automated through Large Language Models. Our system develops a set of agents that construct graph-related knowlege bases and then leverages Retrieval-Augmented Generation (RAG) to support automated configuration and refinement of GNN models through a knowledge-guided evolution process. These agents, equipped with specialized knowledge bases, extract insights into tasks and graph structures by interacting with the knowledge bases. Empirical results show LLMNet excels in twelve datasets across three graph learning tasks, validating its effectiveness of GNN model designing.

</details>


### [182] [Aligning Evaluation with Clinical Priorities: Calibration, Label Shift, and Error Costs](https://arxiv.org/abs/2506.14540)
*Gerardo A. Flores,Alyssa H. Smith,Julia A. Fukuyama,Ashia C. Wilson*

Main category: cs.LG

TL;DR: 提出一种基于适当评分规则的临床决策支持系统评估框架，强调校准性、鲁棒性及非对称错误成本敏感性。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标（如准确率、AUC-ROC）未充分反映临床关键需求，包括模型校准性、分布偏移鲁棒性及非对称错误成本敏感性。

Method: 基于Schervish表示的适当评分规则理论，提出调整版交叉熵损失函数，在临床相关类别分布范围内加权评估模型性能。

Result: 新方法能系统选择校准阈值分类器，对类别分布不确定性和临床成本非对称性具有敏感性，优于传统指标。

Conclusion: 该框架为临床环境提供简单、鲁棒且符合实际需求的模型评估方案，优先考虑校准性和现实分布变化的适应性。

Abstract: Machine learning-based decision support systems are increasingly deployed in clinical settings, where probabilistic scoring functions are used to inform and prioritize patient management decisions. However, widely used scoring rules, such as accuracy and AUC-ROC, fail to adequately reflect key clinical priorities, including calibration, robustness to distributional shifts, and sensitivity to asymmetric error costs. In this work, we propose a principled yet practical evaluation framework for selecting calibrated thresholded classifiers that explicitly accounts for the uncertainty in class prevalences and domain-specific cost asymmetries often found in clinical settings. Building on the theory of proper scoring rules, particularly the Schervish representation, we derive an adjusted variant of cross-entropy (log score) that averages cost-weighted performance over clinically relevant ranges of class balance. The resulting evaluation is simple to apply, sensitive to clinical deployment conditions, and designed to prioritize models that are both calibrated and robust to real-world variations.

</details>


### [183] [TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization](https://arxiv.org/abs/2506.14574)
*Mingkang Zhu,Xi Chen,Zhongdao Wang,Bei Yu,Hengshuang Zhao,Jiaya Jia*

Main category: cs.LG

TL;DR: 本文提出了一种基于细粒度token级奖励指导的DPO优化方法，通过将序列级PPO分解为token级优化问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类反馈的强化学习方法（如PPO）可利用token级奖励提升模型对齐效果，但DPO因采用序列级偏好优化框架难以直接利用此类细粒度奖励。

Method: 将序列级PPO分解为token级优化问题，推导出闭式token级策略与奖励函数，结合Bradley-Terry模型构建可计算的token级奖励指导的DPO损失函数。

Result: 在MT-Bench、AlpacaEval 2和Arena-Hard基准上分别实现7.5、6.2和4.3个百分点的胜率提升，显著优于原始DPO方法。

Conclusion: 通过引入token级奖励指导机制，本文方法有效解决了DPO的细粒度优化难题，为语言模型对齐提供了新的技术路径。

Abstract: Recent advancements in reinforcement learning from human feedback have shown that utilizing fine-grained token-level reward models can substantially enhance the performance of Proximal Policy Optimization (PPO) in aligning large language models. However, it is challenging to leverage such token-level reward as guidance for Direct Preference Optimization (DPO), since DPO is formulated as a sequence-level bandit problem. To address this challenge, this work decomposes the sequence-level PPO into a sequence of token-level proximal policy optimization problems and then frames the problem of token-level PPO with token-level reward guidance, from which closed-form optimal token-level policy and the corresponding token-level reward can be derived. Using the obtained reward and Bradley-Terry model, this work establishes a framework of computable loss functions with token-level reward guidance for DPO, and proposes a practical reward guidance based on the induced DPO reward. This formulation enables different tokens to exhibit varying degrees of deviation from reference policy based on their respective rewards. Experiment results demonstrate that our method achieves substantial performance improvements over DPO, with win rate gains of up to 7.5 points on MT-Bench, 6.2 points on AlpacaEval 2, and 4.3 points on Arena-Hard. Code is available at https://github.com/dvlab-research/TGDPO.

</details>


### [184] [Single-Example Learning in a Mixture of GPDMs with Latent Geometries](https://arxiv.org/abs/2506.14563)
*Jesse St. Amand,Leonardo Gizzi,Martin A. Giese*

Main category: cs.LG

TL;DR: 本文提出高斯过程动态混合模型（GPDMM），用于单样本人体运动数据学习，结合隐马尔可夫动力学先验与混合专家框架，解决数据有限且需可解释性的场景（如医疗假肢控制），并在分类和生成任务中优于LSTM、VAE等模型。


<details>
  <summary>Details</summary>
Motivation: 针对数据有限且模型可解释性至关重要的场景（如患者特异性医疗应用），传统方法难以有效建模人体运动。GPDMM旨在通过混合模型解决单样本学习中的多样序列编码与生成问题。

Method: 将多个高斯过程动态模型（GPDM）整合为概率混合专家框架（GPDMM），利用几何特征在单一潜在空间中编码多样序列，结合隐马尔可夫模型动力学先验优化，支持序列分类与生成。

Result: GPDMM在单样本学习的分类准确率和生成能力上表现优异，优于LSTM、VAE和Transformer等基准模型，并通过模型变体验证了其灵活性。

Conclusion: GPDMM通过混合高斯过程动态模型，在数据稀缺且需高可解释性的任务中（如医疗控制）展现出高效分类与生成能力，为相关领域提供了新方法。

Abstract: We present the Gaussian process dynamical mixture model (GPDMM) and show its utility in single-example learning of human motion data. The Gaussian process dynamical model (GPDM) is a form of the Gaussian process latent variable model (GPLVM), but optimized with a hidden Markov model dynamical prior. The GPDMM combines multiple GPDMs in a probabilistic mixture-of-experts framework, utilizing embedded geometric features to allow for diverse sequences to be encoded in a single latent space, enabling the categorization and generation of each sequence class. GPDMs and our mixture model are particularly advantageous in addressing the challenges of modeling human movement in scenarios where data is limited and model interpretability is vital, such as in patient-specific medical applications like prosthesis control. We score the GPDMM on classification accuracy and generative ability in single-example learning, showcase model variations, and benchmark it against LSTMs, VAEs, and transformers.

</details>


### [185] [Object-Centric Neuro-Argumentative Learning](https://arxiv.org/abs/2506.14577)
*Abdul Rahman Jacob,Avinash Kori,Emanuele De Angelis,Ben Glocker,Maurizio Proietti,Francesca Toni*

Main category: cs.LG

TL;DR: 提出一种结合假设论证与深度学习的神经论证学习架构，提升图像分析的可解释性与性能。


<details>
  <summary>Details</summary>
Motivation: 针对深度学习在关键决策中安全性、可靠性和可解释性不足的问题，探索神经符号融合方法。

Method: 神经组件通过对象中心学习生成图像事实，符号组件利用ABA框架构建可解释的预测逻辑。

Result: 合成数据实验表明，NAL架构与当前最优方法具有竞争力。

Conclusion: NAL验证了神经符号融合在增强模型可解释性方面的潜力，为可信AI提供新路径。

Abstract: Over the last decade, as we rely more on deep learning technologies to make critical decisions, concerns regarding their safety, reliability and interpretability have emerged. We introduce a novel Neural Argumentative Learning (NAL) architecture that integrates Assumption-Based Argumentation (ABA) with deep learning for image analysis. Our architecture consists of neural and symbolic components. The former segments and encodes images into facts using object-centric learning, while the latter applies ABA learning to develop ABA frameworks enabling predictions with images. Experiments on synthetic data show that the NAL architecture can be competitive with a state-of-the-art alternative.

</details>


### [186] [SCISSOR: Mitigating Semantic Bias through Cluster-Aware Siamese Networks for Robust Classification](https://arxiv.org/abs/2506.14587)
*Shuo Yang,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.LG

TL;DR: 论文提出SCISSOR方法，通过抑制语义嵌入中的潜在捷径簇来提升模型在分布外数据上的鲁棒性，无需数据增强或重写，在多个CV/NLP任务中显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统研究将模型捷径学习归因于表面特征偏差，本文发现样本嵌入的语义分布不平衡会引发虚假语义关联，这是影响模型泛化能力的关键因素。

Method: 提出基于孪生网络的SCISSOR方法：通过语义簇干预重构嵌入空间，主动抑制被模型利用为捷径的潜在语义簇，突破传统数据去偏方法对数据增强的依赖。

Result: 在4个基准测试（Chest-XRay/Not-MNIST/GYAFC/Yelp）中，SCISSOR使ViT/BERT等模型F1提升5.3-7.7个百分点，轻量模型提升达9.5-11.9%。

Conclusion: SCISSOR通过解决语义偏差重新定义了模型泛化范式，为构建抗捷径学习、高鲁棒性AI系统提供了基础框架。

Abstract: Shortcut learning undermines model generalization to out-of-distribution data. While the literature attributes shortcuts to biases in superficial features, we show that imbalances in the semantic distribution of sample embeddings induce spurious semantic correlations, compromising model robustness. To address this issue, we propose SCISSOR (Semantic Cluster Intervention for Suppressing ShORtcut), a Siamese network-based debiasing approach that remaps the semantic space by discouraging latent clusters exploited as shortcuts. Unlike prior data-debiasing approaches, SCISSOR eliminates the need for data augmentation and rewriting. We evaluate SCISSOR on 6 models across 4 benchmarks: Chest-XRay and Not-MNIST in computer vision, and GYAFC and Yelp in NLP tasks. Compared to several baselines, SCISSOR reports +5.3 absolute points in F1 score on GYAFC, +7.3 on Yelp, +7.7 on Chest-XRay, and +1 on Not-MNIST. SCISSOR is also highly advantageous for lightweight models with ~9.5% improvement on F1 for ViT on computer vision datasets and ~11.9% for BERT on NLP. Our study redefines the landscape of model generalization by addressing overlooked semantic biases, establishing SCISSOR as a foundational framework for mitigating shortcut learning and fostering more robust, bias-resistant AI systems.

</details>


### [187] [Deep Learning Surrogates for Real-Time Gas Emission Inversion](https://arxiv.org/abs/2506.14597)
*Thomas Newman,Christopher Nemeth,Matthew Jones,Philip Jonathan*

Main category: cs.LG

TL;DR: 本文提出了一种结合深度学习替代模型与蒙特卡洛算法的时空反演框架，用于动态流场中温室气体排放源位置及速率的实时贝叶斯推断，在保持物理精度的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 瞬态大气条件下温室气体排放的实时识别与量化是环境监测的关键挑战，传统CFD模拟计算成本高，难以满足实时性需求。

Method: 通过多层感知机构建CFD替代模型，嵌入序贯蒙特卡洛算法实现贝叶斯推断，替代传统数值求解器以捕捉气体扩散的时空异质性。

Result: 在Chilbolton甲烷数据集验证中达到与CFD/高斯羽流模型相当的精度，计算速度提升数量级；复杂障碍流场景测试验证了鲁棒性。

Conclusion: 该方法平衡物理精度与计算可行性，为工业排放监测等时间敏感的时空反演任务提供了可扩展解决方案。

Abstract: Real-time identification and quantification of greenhouse-gas emissions under transient atmospheric conditions is a critical challenge in environmental monitoring. We introduce a spatio-temporal inversion framework that embeds a deep-learning surrogate of computational fluid dynamics (CFD) within a sequential Monte Carlo algorithm to perform Bayesian inference of both emission rate and source location in dynamic flow fields. By substituting costly numerical solvers with a multilayer perceptron trained on high-fidelity CFD outputs, our surrogate captures spatial heterogeneity and temporal evolution of gas dispersion, while delivering near-real-time predictions. Validation on the Chilbolton methane release dataset demonstrates comparable accuracy to full CFD solvers and Gaussian plume models, yet achieves orders-of-magnitude faster runtimes. Further experiments under simulated obstructed-flow scenarios confirm robustness in complex environments. This work reconciles physical fidelity with computational feasibility, offering a scalable solution for industrial emissions monitoring and other time-sensitive spatio-temporal inversion tasks in environmental and scientific modeling.

</details>


### [188] [Expressive Score-Based Priors for Distribution Matching with Geometry-Preserving Regularization](https://arxiv.org/abs/2506.14607)
*Ziyu Gong,Jim Lim,David I. Inouye*

Main category: cs.LG

TL;DR: 本文提出了一种基于分数匹配的分布匹配方法，通过去噪分数匹配训练先验分布，解决了传统方法在稳定性、计算效率及先验偏差方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有分布匹配方法存在可扩展性（非参数方法）、不稳定性（对抗方法）及固定先验偏差（似然方法）等问题。需要一种既能避免显式密度建模，又能消除先验偏差的稳定方法。

Method: 提出使用表达性分数基先验分布进行分布匹配，仅依赖先验的分数函数而非密度，通过去噪分数匹配训练先验，避免固定先验偏差，并保留几何正则化。

Result: 实验表明该方法在多个任务中性能优于现有方法，具有更好的稳定性、计算效率（相比扩散基先验如LSGM），且无需显式密度模型。

Conclusion: 基于分数匹配的分布匹配方法通过灵活的先验训练机制，成为稳定且高效的分布匹配新范式，代码已发布。

Abstract: Distribution matching (DM) is a versatile domain-invariant representation learning technique that has been applied to tasks such as fair classification, domain adaptation, and domain translation. Non-parametric DM methods struggle with scalability and adversarial DM approaches suffer from instability and mode collapse. While likelihood-based methods are a promising alternative, they often impose unnecessary biases through fixed priors or require explicit density models (e.g., flows) that can be challenging to train. We address this limitation by introducing a novel approach to training likelihood-based DM using expressive score-based prior distributions. Our key insight is that gradient-based DM training only requires the prior's score function -- not its density -- allowing us to train the prior via denoising score matching. This approach eliminates biases from fixed priors (e.g., in VAEs), enabling more effective use of geometry-preserving regularization, while avoiding the challenge of learning an explicit prior density model (e.g., a flow-based prior). Our method also demonstrates better stability and computational efficiency compared to other diffusion-based priors (e.g., LSGM). Furthermore, experiments demonstrate superior performance across multiple tasks, establishing our score-based method as a stable and effective approach to distribution matching. Source code available at https://github.com/inouye-lab/SAUB.

</details>


### [189] [Feasibility-Driven Trust Region Bayesian Optimization](https://arxiv.org/abs/2506.14619)
*Paolo Ascia,Elena Raponi,Thomas Bäck,Fabian Duddeck*

Main category: cs.LG

TL;DR: 提出了一种基于可行性驱动的信任区域贝叶斯优化算法（FuRBO），用于在高维约束优化问题中快速发现可行解。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的优化问题常涉及高维空间中的昂贵约束，且可行区域小、不规则，现有方法在寻找初始可行解时消耗过多预算。

Method: FuRBO通过迭代定义信任区域，结合目标和约束的代理模型信息，动态调整搜索区域以加速可行解的发现。

Result: 在BBOB-constrained COCO基准测试和其他物理相关问题上验证了FuRBO的有效性，尤其在约束严格和高维（2-60维）情况下表现优异。

Conclusion: FuRBO通过自适应信任区域策略显著提升了约束优化问题的求解效率，适用于复杂现实场景。

Abstract: Bayesian optimization is a powerful tool for solving real-world optimization tasks under tight evaluation budgets, making it well-suited for applications involving costly simulations or experiments. However, many of these tasks are also characterized by the presence of expensive constraints whose analytical formulation is unknown and often defined in high-dimensional spaces where feasible regions are small, irregular, and difficult to identify. In such cases, a substantial portion of the optimization budget may be spent just trying to locate the first feasible solution, limiting the effectiveness of existing methods. In this work, we present a Feasibility-Driven Trust Region Bayesian Optimization (FuRBO) algorithm. FuRBO iteratively defines a trust region from which the next candidate solution is selected, using information from both the objective and constraint surrogate models. Our adaptive strategy allows the trust region to shift and resize significantly between iterations, enabling the optimizer to rapidly refocus its search and consistently accelerate the discovery of feasible and good-quality solutions. We empirically demonstrate the effectiveness of FuRBO through extensive testing on the full BBOB-constrained COCO benchmark suite and other physics-inspired benchmarks, comparing it against state-of-the-art baselines for constrained black-box optimization across varying levels of constraint severity and problem dimensionalities ranging from 2 to 60.

</details>


### [190] [Towards Desiderata-Driven Design of Visual Counterfactual Explainers](https://arxiv.org/abs/2506.14698)
*Sidney Bender,Jan Herrmann,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 现有视觉反事实解释器（VCEs）过度关注样本质量或最小化修改，忽略了保真度、可理解性等综合解释需求。本文提出一种新算法SCE，通过结合新机制优化反事实生成，以更全面地满足解释需求。


<details>
  <summary>Details</summary>
Motivation: 现有VCEs仅聚焦样本质量或修改最小化，未充分考虑解释的保真度、可理解性及充分性等核心要求，需探索新方法以弥补这一缺陷。

Method: 提出一种新型『平滑反事实探索器』（SCE）算法，整合多种反事实生成机制，并通过合成与真实数据系统性评估其效果。

Result: 实验表明，SCE在合成与真实数据中均能有效满足解释的保真度、可理解性及充分性需求。

Conclusion: SCE通过融合新生成机制，为图像分类模型提供了更全面、符合实际需求的视觉反事实解释框架。

Abstract: Visual counterfactual explainers (VCEs) are a straightforward and promising approach to enhancing the transparency of image classifiers. VCEs complement other types of explanations, such as feature attribution, by revealing the specific data transformations to which a machine learning model responds most strongly. In this paper, we argue that existing VCEs focus too narrowly on optimizing sample quality or change minimality; they fail to consider the more holistic desiderata for an explanation, such as fidelity, understandability, and sufficiency. To address this shortcoming, we explore new mechanisms for counterfactual generation and investigate how they can help fulfill these desiderata. We combine these mechanisms into a novel 'smooth counterfactual explorer' (SCE) algorithm and demonstrate its effectiveness through systematic evaluations on synthetic and real data.

</details>


### [191] [On the Hardness of Bandit Learning](https://arxiv.org/abs/2506.14746)
*Nataly Brukhim,Aldo Pacchiano,Miroslav Dudik,Robert Schapire*

Main category: cs.LG

TL;DR: 本文研究了在已知函数类F下老虎机学习（最佳臂识别）的可学习性，揭示了结构化老虎机学习的局限性：无组合维度（如VC维）能描述其可学习性，且存在计算困难性（除非RP=NP），同时探讨了噪声、查询复杂度与遗憾最小化的关系。


<details>
  <summary>Details</summary>
Motivation: 旨在建立类似PAC框架的老虎机学习理论，确定哪些函数类F可学习及如何学习。传统分类学习中，VC维和ERM可完全描述可学习性，但本文试图探索老虎机学习中的类似规律。

Method: 通过理论分析证明组合维度无法描述老虎机可学习性，构造特定奖励函数类展示计算困难性，并分析ERM等标准算法操作在老虎机任务中的适用性。

Result: 1) 无组合维度能刻画老虎机可学习性；2) 存在需2次查询即可找到最优动作的类，但无多项式时间算法（除非RP=NP）；3) 即使ERM可行，计算困难性仍存在。

Conclusion: 结构化老虎机学习存在本质局限性：可学习性无法通过维度理论统一描述，且计算困难性可能独立于标准算法操作。这扩展了传统学习理论的边界，揭示了任务本身的复杂性。

Abstract: We study the task of bandit learning, also known as best-arm identification, under the assumption that the true reward function f belongs to a known, but arbitrary, function class F. We seek a general theory of bandit learnability, akin to the PAC framework for classification. Our investigation is guided by the following two questions: (1) which classes F are learnable, and (2) how they are learnable. For example, in the case of binary PAC classification, learnability is fully determined by a combinatorial dimension - the VC dimension- and can be attained via a simple algorithmic principle, namely, empirical risk minimization (ERM). In contrast to classical learning-theoretic results, our findings reveal limitations of learning in structured bandits, offering insights into the boundaries of bandit learnability. First, for the question of "which", we show that the paradigm of identifying the learnable classes via a dimension-like quantity fails for bandit learning. We give a simple proof demonstrating that no combinatorial dimension can characterize bandit learnability, even in finite classes, following a standard definition of dimension introduced by Ben-David et al. (2019). For the question of "how", we prove a computational hardness result: we construct a reward function class for which at most two queries are needed to find the optimal action, yet no algorithm can do so in polynomial time unless RP=NP. We also prove that this class admits efficient algorithms for standard algorithmic operations often considered in learning theory, such as an ERM. This implies that computational hardness is in this case inherent to the task of bandit learning. Beyond these results, we investigate additional themes such as learning under noise, trade-offs between noise models, and the relationship between query complexity and regret minimization.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [192] [A Hybrid Neural Network -- Polynomial Series Scheme for Learning Invariant Manifolds of Discrete Dynamical Systems](https://arxiv.org/abs/2506.13950)
*Dimitrios G. Patsatzis,Nikolaos Kazantzis,Ioannis G. Kevrekidis,Constantinos Siettos*

Main category: math.NA

TL;DR: 提出混合多项式与神经网络的机器学习方法，构建动力系统降阶模型，在精度和效率上优于单一方法。


<details>
  <summary>Details</summary>
Motivation: 现有纯多项式展开（如Legendre、Chebyshev）或单独神经网络方法在不变流形建模中存在局限性：多项式仅能保证固定点附近的局部收敛，而神经网络难以处理复杂结构。需结合两者优势以提升降阶模型的全局性能。

Method: 物理与数值分析双驱动的混合框架：1) 多项式级数保证固定点附近的指数收敛性；2) 浅层神经网络补充建模多项式收敛域外的复杂结构；3) 通过基准案例验证收敛性、精度与计算成本。

Result: 在三个基准测试中，混合方法数值精度显著优于纯多项式（幂级数/Legendre/Chebyshev）和单独神经网络，且训练成本可控。不变流形在假设条件下被证明为解析形式。

Conclusion: 混合方法通过多项式与神经网络的互补性，在降阶模型构建中实现更高数值精度，为动力系统不变流形学习提供了更优的框架。

Abstract: We propose a hybrid machine learning scheme to learn -- in physics-informed and numerical analysis-informed fashion -- invariant manifolds (IM) of discrete maps for constructing reduced-order models (ROMs) for dynamical systems. The proposed scheme combines polynomial series with shallow neural networks, exploiting the complementary strengths of both approaches. Polynomials enable an efficient and accurate modeling of ROMs with guaranteed local exponential convergence rate around the fixed point, where, under certain assumptions, the IM is demonstrated to be analytic. Neural networks provide approximations to more complex structures beyond the reach of the polynomials' convergence. We evaluate the efficiency of the proposed scheme using three benchmark examples, examining convergence behavior, numerical approximation accuracy, and computational training cost. Additionally, we compare the IM approximations obtained solely with neural networks and with polynomial expansions. We demonstrate that the proposed hybrid scheme outperforms both pure polynomial approximations (power series, Legendre and Chebyshev polynomials) and standalone shallow neural network approximations in terms of numerical approximation accuracy.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [193] [NeuralPDR: Neural Differential Equations as surrogate models for Photodissociation Regions](https://arxiv.org/abs/2506.14270)
*Gijs Vermariën,Thomas G. Bisbas,Serena Viti,Yue Zhao,Xuefei Tang,Rahul Ravichandran*

Main category: astro-ph.GA

TL;DR: 本文提出使用潜在增强神经常微分方程（Latent Augmented Neural ODEs）作为替代模型，以加速天体化学模拟中的化学求解过程，从而支持高分辨率望远镜观测数据的快速推断和流体动力学模拟的高分辨率计算。


<details>
  <summary>Details</summary>
Motivation: 随着JWST和ALMA等高分辨率望远镜的发展，天体化学模型需在小尺度上结合物理与化学过程进行模拟，但传统三维流体动力学与化学耦合模拟计算成本极高，亟需高效替代模型。

Method: 采用潜在增强神经常微分方程作为化学求解器的替代模型，在三个物理复杂度递增的数据集（包括基于3D-PDR代码的分子云三维模拟数据）上进行训练。

Result: 替代模型在GPU上实现了化学求解的加速，能准确复现原始柱密度分布图，并支持快速统计推断或提升流体动力学模拟的分辨率。

Conclusion: 所提出的替代模型显著降低了天体化学模拟的计算成本，为高分辨率观测和复杂环境模拟提供了高效工具，推动了天体化学与流体动力学的协同研究。

Abstract: Computational astrochemical models are essential for helping us interpret and understand the observations of different astrophysical environments. In the age of high-resolution telescopes such as JWST and ALMA, the substructure of many objects can be resolved, raising the need for astrochemical modeling at these smaller scales, meaning that the simulations of these objects need to include both the physics and chemistry to accurately model the observations. The computational cost of the simulations coupling both the three-dimensional hydrodynamics and chemistry is enormous, creating an opportunity for surrogate models that can effectively substitute the chemical solver. In this work we present surrogate models that can replace the original chemical code, namely Latent Augmented Neural Ordinary Differential Equations. We train these surrogate architectures on three datasets of increasing physical complexity, with the last dataset derived directly from a three-dimensional simulation of a molecular cloud using a Photodissociation Region (PDR) code, 3D-PDR. We show that these surrogate models can provide speedup and reproduce the original observable column density maps of the dataset. This enables the rapid inference of the chemistry (on the GPU), allowing for the faster statistical inference of observations or increasing the resolution in hydrodynamical simulations of astrophysical environments.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [194] [Non-planar Object Detection and Identification by Features Matching and Triangulation Growth](https://arxiv.org/abs/2506.13769)
*Filippo Leveni*

Main category: cs.CV

TL;DR: 提出一种基于特征增量分组的对象检测方法，利用Delaunay三角剖分引导匹配过程，在几何模型失效的非平面/变形场景中优于传统单应性RANSAC方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于单应性几何模型的方法在非平面对象或显著变形场景中失效，需开发能适应复杂形变的鲁棒检测框架。

Method: 以模板特征Delaunay三角剖分为图结构，通过局部几何/光度一致性准则迭代扩展特征匹配组，逐步验证三角形邻域关系的有效性。

Result: 在无畸变场景性能与RANSAC相当，在显著形变场景中展现出更优的匹配描述能力，突破平面假设限制。

Conclusion: 基于增量特征分组的拓扑驱动方法有效解决了复杂形变下的物体识别问题，为非线性形变场景提供了新的解决方案。

Abstract: Object detection and identification is surely a fundamental topic in the computer vision field; it plays a crucial role in many applications such as object tracking, industrial robots control, image retrieval, etc. We propose a feature-based approach for detecting and identifying distorted occurrences of a given template in a scene image by incremental grouping of feature matches between the image and the template. For this purpose, we consider the Delaunay triangulation of template features as an useful tool through which to be guided in this iterative approach. The triangulation is treated as a graph and, starting from a single triangle, neighboring nodes are considered and the corresponding features are identified; then matches related to them are evaluated to determine if they are worthy to be grouped. This evaluation is based on local consistency criteria derived from geometric and photometric properties of local features. Our solution allows the identification of the object in situations where geometric models (e.g. homography) does not hold, thus enable the detection of objects such that the template is non planar or when it is planar but appears distorted in the image. We show that our approach performs just as well or better than application of homography-based RANSAC in scenarios in which distortion is nearly absent, while when the deformation becomes relevant our method shows better description performance.

</details>


### [195] [Hidden Bias in the Machine: Stereotypes in Text-to-Image Models](https://arxiv.org/abs/2506.13780)
*Sedat Porikli,Vedat Porikli*

Main category: cs.CV

TL;DR: 研究通过分析文本到图像（T2I）模型生成的图像，揭示其在性别、种族、年龄等人类特征上的显著偏见，并强调需改进数据集和开发实践以促进公平。


<details>
  <summary>Details</summary>
Motivation: 探讨T2I模型生成内容是否可能复制和强化社会现有偏见，尤其是涉及人类特征的有害刻板印象。

Method: 使用Stable Diffusion 1.5和Flux-1模型，基于160个主题的多样化提示生成16,000余张图像，并与Google搜索的8,000张对比图结合分析。

Result: 生成图像在性别、种族、体型等维度存在显著偏差，与社会叙事中的刻板印象高度一致，可能加剧偏见传播。

Conclusion: 需构建更包容的数据集和开发流程，以减少生成视觉系统中的偏见，促进技术公平性。

Abstract: Text-to-Image (T2I) models have transformed visual content creation, producing highly realistic images from natural language prompts. However, concerns persist around their potential to replicate and magnify existing societal biases. To investigate these issues, we curated a diverse set of prompts spanning thematic categories such as occupations, traits, actions, ideologies, emotions, family roles, place descriptions, spirituality, and life events. For each of the 160 unique topics, we crafted multiple prompt variations to reflect a wide range of meanings and perspectives. Using Stable Diffusion 1.5 (UNet-based) and Flux-1 (DiT-based) models with original checkpoints, we generated over 16,000 images under consistent settings. Additionally, we collected 8,000 comparison images from Google Image Search. All outputs were filtered to exclude abstract, distorted, or nonsensical results. Our analysis reveals significant disparities in the representation of gender, race, age, somatotype, and other human-centric factors across generated images. These disparities often mirror and reinforce harmful stereotypes embedded in societal narratives. We discuss the implications of these findings and emphasize the need for more inclusive datasets and development practices to foster fairness in generative visual systems.

</details>


### [196] [RadFabric: Agentic AI System with Reasoning Capability for Radiology](https://arxiv.org/abs/2506.14142)
*Wenting Chen,Yi Dong,Zhaojun Ding,Yucheng Shi,Yifan Zhou,Fang Zeng,Yijun Luo,Tianyu Lin,Yihang Su,Yichen Wu,Kai Zhang,Zhen Xiang,Tianming Liu,Ninghao Liu,Lichao Sun,Yixuan Yuan,Xiang Li*

Main category: cs.CV

TL;DR: RadFabric提出了一种多智能体、多模态推理框架，通过整合视觉与文本分析提升胸片诊断的准确性和透明度，显著优于传统系统。


<details>
  <summary>Details</summary>
Motivation: 现有胸片自动系统存在病理覆盖不足、诊断准确性有限、视觉与文本推理整合不充分的问题，需开发更全面可靠的解决方案。

Method: 基于模型上下文协议（MCP）构建模块化框架，结合专用病理检测代理、解剖解释代理及多模态推理代理，实现视觉、解剖与临床数据的融合。

Result: 骨折检测准确率达1.000，整体诊断准确率0.799（传统系统为0.229-0.527），跨模态特征对齐和偏好驱动推理显著提升性能。

Conclusion: RadFabric通过多模态协同和模块化设计，推动AI胸片分析向解剖精准、临床可解释的方向发展，为透明化放射诊断提供新范式。

Abstract: Chest X ray (CXR) imaging remains a critical diagnostic tool for thoracic conditions, but current automated systems face limitations in pathology coverage, diagnostic accuracy, and integration of visual and textual reasoning. To address these gaps, we propose RadFabric, a multi agent, multimodal reasoning framework that unifies visual and textual analysis for comprehensive CXR interpretation. RadFabric is built on the Model Context Protocol (MCP), enabling modularity, interoperability, and scalability for seamless integration of new diagnostic agents. The system employs specialized CXR agents for pathology detection, an Anatomical Interpretation Agent to map visual findings to precise anatomical structures, and a Reasoning Agent powered by large multimodal reasoning models to synthesize visual, anatomical, and clinical data into transparent and evidence based diagnoses. RadFabric achieves significant performance improvements, with near-perfect detection of challenging pathologies like fractures (1.000 accuracy) and superior overall diagnostic accuracy (0.799) compared to traditional systems (0.229 to 0.527). By integrating cross modal feature alignment and preference-driven reasoning, RadFabric advances AI-driven radiology toward transparent, anatomically precise, and clinically actionable CXR analysis.

</details>


### [197] [Fake it till You Make it: Reward Modeling as Discriminative Prediction](https://arxiv.org/abs/2506.13846)
*Runtao Liu,Jiahao Zhan,Yingqing He,Chen Wei,Alan Yuille,Qifeng Chen*

Main category: cs.CV

TL;DR: 本文提出GAN-RM框架，通过对抗训练替代传统奖励模型对人工标注和复杂质量维度的依赖，仅需少量目标样本即可高效训练奖励模型，并在多个应用中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉生成模型奖励建模方法依赖大量人工标注偏好数据或人工设计不完整的质量维度，导致实现复杂且工程成本高昂。

Method: 受GAN对抗训练启发，提出GAN-RM框架：通过区分少量无配对目标样本（偏好代理数据）与普通生成样本训练奖励模型，无需人工标注与显式维度设计。

Result: 实验证明该方法在Best-of-N筛选、监督微调(SFT)和直接偏好优化(DPO)等关键应用中均有效，仅需数百目标样本即可实现。

Conclusion: GAN-RM为视觉生成模型提供了一种高效、低成本的奖励建模范式，突破了传统方法对人工标注与人工设计质量维度的双重依赖。

Abstract: An effective reward model plays a pivotal role in reinforcement learning for post-training enhancement of visual generative models. However, current approaches of reward modeling suffer from implementation complexity due to their reliance on extensive human-annotated preference data or meticulously engineered quality dimensions that are often incomplete and engineering-intensive. Inspired by adversarial training in generative adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward modeling framework that eliminates manual preference annotation and explicit quality dimension engineering. Our method trains the reward model through discrimination between a small set of representative, unpaired target samples(denoted as Preference Proxy Data) and model-generated ordinary outputs, requiring only a few hundred target samples. Comprehensive experiments demonstrate our GAN-RM's effectiveness across multiple key applications including test-time scaling implemented as Best-of-N sample filtering, post-training approaches like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO).

</details>


### [198] [VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based Mosquito Breeding Site Detection and Reasoning](https://arxiv.org/abs/2506.14629)
*Md. Adnanul Islam,Md. Faiyaz Abdullah Sayeedi,Md. Asaduzzaman Shuvo,Muhammad Ziaur Rahman,Shahanur Rahman Bappy,Raiyan Rahman,Swakkhar Shatabda*

Main category: cs.CV

TL;DR: 本文提出多模态数据集VisText-Mosquito，结合视觉与文本数据实现蚊虫滋生地自动检测、分割与推理。YOLOv9s目标检测精度达0.929，BLIP模型生成推理文本BLEU得分54.7，验证AI在预防蚊媒疾病中的有效性。


<details>
  <summary>Details</summary>
Motivation: 蚊媒疾病构成重大全球健康风险，需通过早期检测与主动控制滋生地预防爆发。现有方法缺乏多模态数据整合，难以实现自动化分析与推理。

Method: 构建包含1,828张目标检测标注图像、142张水面分割图像及关联自然语言文本的多模态数据集VisText-Mosquito。采用YOLOv9s/YOLOv11n-Seg进行检测与分割，微调BLIP模型实现文本推理生成。

Result: YOLOv9s目标检测mAP@50达0.92891，YOLOv11n-Seg分割精度0.91587。BLIP模型获得0.0028损失值，BLEU 54.7/BERTScore 0.91/ROUGE-L 0.87。数据集与代码已在GitHub开源。

Conclusion: 通过'预防胜于治疗'框架验证AI多模态分析可主动防控蚊媒疾病风险。公开数据集与模型为后续研究提供基准，推动公共卫生领域智能预防系统发展。

Abstract: Mosquito-borne diseases pose a major global health risk, requiring early detection and proactive control of breeding sites to prevent outbreaks. In this paper, we present VisText-Mosquito, a multimodal dataset that integrates visual and textual data to support automated detection, segmentation, and reasoning for mosquito breeding site analysis. The dataset includes 1,828 annotated images for object detection, 142 images for water surface segmentation, and natural language reasoning texts linked to each image. The YOLOv9s model achieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object detection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and mAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves a final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and ROUGE-L of 0.87. This dataset and model framework emphasize the theme "Prevention is Better than Cure", showcasing how AI-based detection can proactively address mosquito-borne disease risks. The dataset and implementation code are publicly available at GitHub: https://github.com/adnanul-islam-jisun/VisText-Mosquito

</details>


### [199] [ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM](https://arxiv.org/abs/2506.14766)
*Yujun Wang,Jinhe Bi,Yunpu Ma,Soeren Pirk*

Main category: cs.CV

TL;DR: 本文提出一种基于注意力引导的对比解码框架，通过直接干预多模态大语言模型（MLLM）的注意力机制，有效减少模型幻觉现象，并在多个基准测试中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如VCD和ICD）通过对比扰动输入缓解幻觉问题，但其有效性可能源于注意力分布的内在变化而非表面概率调整。本文旨在探索更根本的注意力动态干预机制。

Method: 提出注意力可引导的对比解码框架，直接干预模型内部注意力机制，而非仅依赖输入扰动或负前缀的logits对比，提供更理论化的幻觉抑制方法。

Result: 实验表明该方法在POPE、CHAIR和MMHal-Bench等基准上显著降低幻觉率，同时提升标准VQA任务性能，且适用于多种MLLM架构和解码方法。

Conclusion: 通过注意力动态干预的对比解码框架为MLLM幻觉问题提供了新方向，验证了注意力机制在模型行为调控中的核心作用。

Abstract: Multimodal Large Language Model (MLLM) often suffer from hallucinations. They over-rely on partial cues and generate incorrect responses. Recently, methods like Visual Contrastive Decoding (VCD) and Instruction Contrastive Decoding (ICD) have been proposed to mitigate hallucinations by contrasting predictions from perturbed or negatively prefixed inputs against original outputs. In this work, we uncover that methods like VCD and ICD fundamentally influence internal attention dynamics of the model. This observation suggests that their effectiveness may not stem merely from surface-level modifications to logits but from deeper shifts in attention distribution. Inspired by this insight, we propose an attention-steerable contrastive decoding framework that directly intervenes in attention mechanisms of the model to offer a more principled approach to mitigating hallucinations. Our experiments across multiple MLLM architectures and diverse decoding methods demonstrate that our approach significantly reduces hallucinations and improves the performance on benchmarks such as POPE, CHAIR, and MMHal-Bench, while simultaneously enhancing performance on standard VQA benchmarks.

</details>


### [200] [Intelligent Image Sensing for Crime Analysis: A ML Approach towards Enhanced Violence Detection and Investigation](https://arxiv.org/abs/2506.13910)
*Aritra Dutta,Pushpita Boral,G Suseela*

Main category: cs.CV

TL;DR: 本文提出一种基于机器学习的暴力检测与分类框架，结合3D卷积神经网络和双向LSTM，利用多样化数据集和树莓派摄像头模块，提升了计算效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 传统监控方法在及时检测多样化暴力行为方面存在局限性，全球犯罪率上升导致人力与财产损失加剧，亟需自动化暴力检测方案。

Method: 采用监督学习进行二分类与多分类暴力识别：检测模型使用3D CNN，分类模型通过可分离3D卷积提取特征，双向LSTM处理时序数据，并整合树莓派摄像头实时处理视频流。

Result: 在包含监控视频、体育冲突等多源定制数据集上训练，模型在计算资源效率和准确率方面表现提升。

Conclusion: 该框架通过融合时空特征提取与实时硬件集成，为自动化暴力检测提供了高效且可扩展的解决方案。

Abstract: The increasing global crime rate, coupled with substantial human and property losses, highlights the limitations of traditional surveillance methods in promptly detecting diverse and unexpected acts of violence. Addressing this pressing need for automatic violence detection, we leverage Machine Learning to detect and categorize violent events in video streams. This paper introduces a comprehensive framework for violence detection and classification, employing Supervised Learning for both binary and multi-class violence classification. The detection model relies on 3D Convolutional Neural Networks, while the classification model utilizes the separable convolutional 3D model for feature extraction and bidirectional LSTM for temporal processing. Training is conducted on a diverse customized datasets with frame-level annotations, incorporating videos from surveillance cameras, human recordings, hockey fight, sohas and wvd dataset across various platforms. Additionally, a camera module integrated with raspberry pi is used to capture live video feed, which is sent to the ML model for processing. Thus, demonstrating improved performance in terms of computational resource efficiency and accuracy.

</details>


### [201] [HierVL: Semi-Supervised Segmentation leveraging Hierarchical Vision-Language Synergy with Dynamic Text-Spatial Query Alignment](https://arxiv.org/abs/2506.13925)
*Numair Nadeem,Saeed Anwar,Muhammad Hamza Asad,Abdul Bais*

Main category: cs.CV

TL;DR: 提出HierVL框架，通过整合视觉语言模型的语义信息与空间对齐，显著提升半监督语义分割在标签稀缺下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有纯视觉方法在标签稀缺和跨域场景下存在类间混淆、泛化差和边界定位不准问题，而视觉语言模型缺乏空间基础能力。需结合两者优势实现细粒度分割。

Method: 构建分层语义查询生成器过滤无关类别，跨模态空间对齐模块增强边界定位，双查询解码器融合语义与实例特征，并设计正则化损失保持跨模态对齐。

Result: 在COCO(232标签)、Pascal VOC(92标签)、ADE20(158标签)、Cityscapes(100标签)上mIoU分别提升4.4%、3.1%、5.9%、1.8%，刷新1%监督下的SOTA。

Conclusion: 语言引导的分割有效缩小标签效率差距，实现细粒度实例感知的泛化，验证跨模态融合在半监督分割中的关键作用。

Abstract: Semi-supervised semantic segmentation remains challenging under severe label scarcity and domain variability. Vision-only methods often struggle to generalize, resulting in pixel misclassification between similar classes, poor generalization and boundary localization. Vision-Language Models offer robust, domain-invariant semantics but lack the spatial grounding required for dense prediction. We introduce HierVL, a unified framework that bridges this gap by integrating abstract text embeddings into a mask-transformer architecture tailored for semi-supervised segmentation. HierVL features three novel components: a Hierarchical Semantic Query Generator that filters and projects abstract class embeddings into multi-scale queries to suppress irrelevant classes and handle intra-class variability; a Cross-Modal Spatial Alignment Module that aligns semantic queries with pixel features for sharper boundaries under sparse supervision; and a Dual-Query Transformer Decoder that fuses semantic and instance-level queries to prevent instance collapse. We also introduce targeted regularization losses that maintain vision-language alignment throughout training to reinforce semantic grounding. HierVL establishes a new state-of-the-art by achieving a +4.4% mean improvement of the intersection over the union on COCO (with 232 labeled images), +3.1% on Pascal VOC (with 92 labels), +5.9% on ADE20 (with 158 labels) and +1.8% on Cityscapes (with 100 labels), demonstrating better performance under 1% supervision on four benchmark datasets. Our results show that language-guided segmentation closes the label efficiency gap and unlocks new levels of fine-grained, instance-aware generalization.

</details>


### [202] [SimpleDoc: Multi-Modal Document Understanding with Dual-Cue Page Retrieval and Iterative Refinement](https://arxiv.org/abs/2506.14035)
*Chelsi Jain,Yiran Wu,Yifan Zeng,Jiale Liu,S hengyu Dai,Zhenwen Shao,Qingyun Wu,Huazheng Wang*

Main category: cs.CV

TL;DR: 本文提出SimpleDoc框架，通过双线索检索（嵌入相似度与摘要重排序）和迭代调用机制，在文档视觉问答任务中实现更少页面检索下的更高准确率，平均提升3.2%。


<details>
  <summary>Details</summary>
Motivation: 现有DocVQA方法虽采用检索增强生成（RAG）流程，但依赖视觉语言模型（VLM）嵌入检索可能效率不足。需通过优化多模态文档页面检索机制，减少冗余检索并提升证据收集能力。

Method: 1. 基于嵌入相似度初筛候选页面；2. 通过页面摘要过滤并重排序候选；3. 单VLM推理器迭代调用双线索检索器，动态更新工作记忆直至问题解决。

Result: 在4个DocVQA数据集上平均准确率提升3.2%，且检索页面数量显著少于基线方法。代码已开源。

Conclusion: SimpleDoc验证了轻量级双线索检索与迭代推理机制的有效性，为多模态长文档问答提供了高效解决方案。

Abstract: Document Visual Question Answering (DocVQA) is a practical yet challenging task, which is to ask questions based on documents while referring to multiple pages and different modalities of information, e.g, images and tables. To handle multi-modality, recent methods follow a similar Retrieval Augmented Generation (RAG) pipeline, but utilize Visual Language Models (VLMs) based embedding model to embed and retrieve relevant pages as images, and generate answers with VLMs that can accept an image as input. In this paper, we introduce SimpleDoc, a lightweight yet powerful retrieval - augmented framework for DocVQA. It boosts evidence page gathering by first retrieving candidates through embedding similarity and then filtering and re-ranking these candidates based on page summaries. A single VLM-based reasoner agent repeatedly invokes this dual-cue retriever, iteratively pulling fresh pages into a working memory until the question is confidently answered. SimpleDoc outperforms previous baselines by 3.2% on average on 4 DocVQA datasets with much fewer pages retrieved. Our code is available at https://github.com/ag2ai/SimpleDoc.

</details>


### [203] [Image Segmentation with Large Language Models: A Survey with Perspectives for Intelligent Transportation Systems](https://arxiv.org/abs/2506.14096)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.CV

TL;DR: 本文系统综述了大型语言模型（LLM）与计算机视觉结合在智能交通系统（ITS）图像分割中的应用，分析其方法、挑战及未来方向，强调可解释性AI对安全部署的重要性。


<details>
  <summary>Details</summary>
Motivation: 智能交通系统需高精度场景理解以保障安全与效率，LLM与视觉融合为图像分割任务提供了新范式，推动自动驾驶、交通监控等领域的突破。

Method: 基于提示机制和核心架构对现有方法进行分类，探讨LLM增强分割技术在道路场景理解中的创新应用，并构建技术路线图。

Result: 提出LLM增强分割技术可提升ITS场景理解能力，同时指出实时性、安全可靠性等关键挑战，需进一步优化模型性能与可解释性。

Conclusion: 未来需以可解释、以人为本的AI为核心，解决实时与安全瓶颈，确保LLM增强分割技术成熟后安全集成至下一代交通系统。

Abstract: The integration of Large Language Models (LLMs) with computer vision is profoundly transforming perception tasks like image segmentation. For intelligent transportation systems (ITS), where accurate scene understanding is critical for safety and efficiency, this new paradigm offers unprecedented capabilities. This survey systematically reviews the emerging field of LLM-augmented image segmentation, focusing on its applications, challenges, and future directions within ITS. We provide a taxonomy of current approaches based on their prompting mechanisms and core architectures, and we highlight how these innovations can enhance road scene understanding for autonomous driving, traffic monitoring, and infrastructure maintenance. Finally, we identify key challenges, including real-time performance and safety-critical reliability, and outline a perspective centered on explainable, human-centric AI as a prerequisite for the successful deployment of this technology in next-generation transportation systems.

</details>


### [204] [KDMOS:Knowledge Distillation for Motion Segmentation](https://arxiv.org/abs/2506.14130)
*Chunyu Cao,Jintao Cheng,Zeyu Chen,Linfan Zhan,Rui Fan,Zhijian He,Xiaoyu Tang*

Main category: cs.CV

TL;DR: 本文提出一种基于logits的知识蒸馏框架KDMOS，用于自动驾驶中的运动目标分割（MOS），通过师生模型、类别解耦蒸馏策略及动态上采样优化，在保持实时性的同时提升精度。


<details>
  <summary>Details</summary>
Motivation: 现有运动目标分割方法在精度与实时性间难以平衡，且移动/非移动类别严重不平衡，导致误检漏检问题。

Method: 采用BEV投影模型作为学生模型，非投影模型作为教师模型；解耦移动/非移动类别并设计针对性蒸馏策略，引入动态上采样并减少7.69%参数量。

Result: 在SemanticKITTI-MOS隐藏测试集上达到78.8% IoU，Apollo数据集表现优异，参数减少且过拟合得到缓解。

Conclusion: KDMOS通过知识蒸馏和架构优化，在实时性约束下显著提升MOS精度，为自动驾驶提供高效解决方案。

Abstract: Motion Object Segmentation (MOS) is crucial for autonomous driving, as it enhances localization, path planning, map construction, scene flow estimation, and future state prediction. While existing methods achieve strong performance, balancing accuracy and real-time inference remains a challenge. To address this, we propose a logits-based knowledge distillation framework for MOS, aiming to improve accuracy while maintaining real-time efficiency. Specifically, we adopt a Bird's Eye View (BEV) projection-based model as the student and a non-projection model as the teacher. To handle the severe imbalance between moving and non-moving classes, we decouple them and apply tailored distillation strategies, allowing the teacher model to better learn key motion-related features. This approach significantly reduces false positives and false negatives. Additionally, we introduce dynamic upsampling, optimize the network architecture, and achieve a 7.69% reduction in parameter count, mitigating overfitting. Our method achieves a notable IoU of 78.8% on the hidden test set of the SemanticKITTI-MOS dataset and delivers competitive results on the Apollo dataset. The KDMOS implementation is available at https://github.com/SCNU-RISLAB/KDMOS.

</details>


### [205] [Mapping Farmed Landscapes from Remote Sensing](https://arxiv.org/abs/2506.13993)
*Michelangelo Conserva,Alex Wilson,Charlotte Stanton,Vishal Batchu,Varun Gulshan*

Main category: cs.CV

TL;DR: 研究者开发了首个覆盖英格兰大部分地区的高分辨率农业景观地图Farmscapes，利用深度学习模型识别关键生态要素，为生物多样性保护提供开放工具。


<details>
  <summary>Details</summary>
Motivation: 农业景观管理对全球生物多样性目标至关重要，但缺乏大范围、高分辨率的生态地图，阻碍了相关保护工作的推进。

Method: 基于942个手动标注的航空影像图块，训练深度学习分割模型，生成25厘米分辨率的景观特征地图，涵盖树篱、林地等关键要素。

Result: 模型在林地（F1 96%）和农田（95%）识别准确率高，树篱分割F1达72%；全英格兰地图已发布于Google Earth Engine平台。

Conclusion: 该工具支持数据驱动的生态恢复规划，助力欧盟生物多样性战略监测，并为景观连通性分析奠定基础。

Abstract: Effective management of agricultural landscapes is critical for meeting global biodiversity targets, but efforts are hampered by the absence of detailed, large-scale ecological maps. To address this, we introduce Farmscapes, the first large-scale (covering most of England), high-resolution (25cm) map of rural landscape features, including ecologically vital elements like hedgerows, woodlands, and stone walls. This map was generated using a deep learning segmentation model trained on a novel, dataset of 942 manually annotated tiles derived from aerial imagery. Our model accurately identifies key habitats, achieving high f1-scores for woodland (96\%) and farmed land (95\%), and demonstrates strong capability in segmenting linear features, with an F1-score of 72\% for hedgerows. By releasing the England-wide map on Google Earth Engine, we provide a powerful, open-access tool for ecologists and policymakers. This work enables data-driven planning for habitat restoration, supports the monitoring of initiatives like the EU Biodiversity Strategy, and lays the foundation for advanced analysis of landscape connectivity.

</details>


### [206] [SceneAware: Scene-Constrained Pedestrian Trajectory Prediction with LLM-Guided Walkability](https://arxiv.org/abs/2506.14144)
*Juho Bai,Inwook Shim*

Main category: cs.CV

TL;DR: 本文提出SceneAware框架，通过结合场景理解和物理约束，显著提升行人轨迹预测精度，在ETH/UCY数据集上超越现有方法50%以上。


<details>
  <summary>Details</summary>
Motivation: 现有行人轨迹预测方法过度关注社交互动，忽视环境背景对人类移动模式的显著影响，导致预测结果缺乏物理合理性。

Method: 使用ViT编码静态场景图像，MLLMs生成可通行区域掩码；结合Transformer轨迹编码器捕捉时空特征，并引入碰撞惩罚机制保证物理可行性。提供确定性和随机性两种实现方案。

Result: 在ETH/UCY基准测试中性能优于SOTA方法，不同运动类型下表现稳定，碰撞惩罚机制使轨迹物理违规率降低至1.2%。

Conclusion: 显式场景信息整合能有效提升预测精度与物理合理性，SceneAware框架为环境感知的轨迹预测提供了可靠解决方案。

Abstract: Accurate prediction of pedestrian trajectories is essential for applications in robotics and surveillance systems. While existing approaches primarily focus on social interactions between pedestrians, they often overlook the rich environmental context that significantly shapes human movement patterns. In this paper, we propose SceneAware, a novel framework that explicitly incorporates scene understanding to enhance trajectory prediction accuracy. Our method leverages a Vision Transformer~(ViT) scene encoder to process environmental context from static scene images, while Multi-modal Large Language Models~(MLLMs) generate binary walkability masks that distinguish between accessible and restricted areas during training. We combine a Transformer-based trajectory encoder with the ViT-based scene encoder, capturing both temporal dynamics and spatial constraints. The framework integrates collision penalty mechanisms that discourage predicted trajectories from violating physical boundaries, ensuring physically plausible predictions. SceneAware is implemented in both deterministic and stochastic variants. Comprehensive experiments on the ETH/UCY benchmark datasets show that our approach outperforms state-of-the-art methods, with more than 50\% improvement over previous models. Our analysis based on different trajectory categories shows that the model performs consistently well across various types of pedestrian movement. This highlights the importance of using explicit scene information and shows that our scene-aware approach is both effective and reliable in generating accurate and physically plausible predictions. Code is available at: https://github.com/juho127/SceneAware.

</details>


### [207] [VideoMAR: Autoregressive Video Generatio with Continuous Tokens](https://arxiv.org/abs/2506.14168)
*Hu Yu,Biao Gong,Hangjie Yuan,DanDan Zheng,Weilong Chai,Jingdong Chen,Kecheng Zheng,Feng Zhao*

Main category: cs.CV

TL;DR: 本文提出VideoMAR，一种基于掩码自回归模型的视频生成方法，通过时空分离生成策略和高效训练技术，显著降低资源消耗并提升生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于掩码的自回归模型在图像生成中表现优异，但其视频生成潜力未被充分挖掘，且长序列建模存在高计算成本与误差累积问题。

Method: 结合时间因果性与空间双向性设计框架，提出下一帧扩散损失函数，采用时间短-长课程学习、空间渐进分辨率训练及推理时温度调节策略。引入3D旋转嵌入实现时空外推能力。

Result: 在VBench-I2V基准上超越SOTA模型Cosmos I2V，仅需其9.3%参数量、0.5%训练数据与0.2%GPU资源。

Conclusion: VideoMAR验证了自回归模型在视频生成中的高效性，通过结构创新与语言模型能力迁移，实现低消耗高性能的端到端视频生成。

Abstract: Masked-based autoregressive models have demonstrated promising image generation capability in continuous space. However, their potential for video generation remains under-explored. In this paper, we propose \textbf{VideoMAR}, a concise and efficient decoder-only autoregressive image-to-video model with continuous tokens, composing temporal frame-by-frame and spatial masked generation. We first identify temporal causality and spatial bi-directionality as the first principle of video AR models, and propose the next-frame diffusion loss for the integration of mask and video generation. Besides, the huge cost and difficulty of long sequence autoregressive modeling is a basic but crucial issue. To this end, we propose the temporal short-to-long curriculum learning and spatial progressive resolution training, and employ progressive temperature strategy at inference time to mitigate the accumulation error. Furthermore, VideoMAR replicates several unique capacities of language models to video generation. It inherently bears high efficiency due to simultaneous temporal-wise KV cache and spatial-wise parallel generation, and presents the capacity of spatial and temporal extrapolation via 3D rotary embeddings. On the VBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos I2V) while requiring significantly fewer parameters ($9.3\%$), training data ($0.5\%$), and GPU resources ($0.2\%$).

</details>


### [208] [A multi-stage augmented multimodal interaction network for fish feeding intensity quantification](https://arxiv.org/abs/2506.14170)
*Shulong Zhang,Mingyuan Yao,Jiayin Zhao,Xiao Liu,Haihua Wang*

Main category: cs.CV

TL;DR: 该研究提出了一种多阶段增强多模态交互网络（MAINet），用于量化鱼类摄食强度，通过多模态数据融合和增强特征提取，显著提高了准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前研究在模态选择、特征提取与融合以及决策推断方面存在局限性，影响了多模态融合模型的准确性、适用性和可靠性。为了解决这一问题，本研究旨在开发一种更有效的方法来量化鱼类摄食强度。

Method: 研究提出了一个通用的特征提取框架，从图像、音频和水波数据中高效提取特征信息；设计了辅助模态增强主模态机制（ARPM），包括通道注意力融合网络（CAFN）和双模态注意力融合网络（DAFN）；并引入证据推理（ER）规则来融合各模态的输出结果并做出决策。

Result: 实验结果表明，MAINet在准确率、精确率、召回率和F1分数上分别达到96.76%、96.78%、96.79%和96.79%，性能显著高于对比模型，且在单模态、双模态融合和不同决策融合方法中具有明显优势。

Conclusion: 消融实验进一步验证了所提改进策略在提高模型鲁棒性和特征利用效率方面的关键作用，能有效提高鱼类摄食强度量化结果的准确性。

Abstract: In recirculating aquaculture systems, accurate and effective assessment of fish feeding intensity is crucial for reducing feed costs and calculating optimal feeding times. However, current studies have limitations in modality selection, feature extraction and fusion, and co-inference for decision making, which restrict further improvement in the accuracy, applicability and reliability of multimodal fusion models. To address this problem, this study proposes a Multi-stage Augmented Multimodal Interaction Network (MAINet) for quantifying fish feeding intensity. Firstly, a general feature extraction framework is proposed to efficiently extract feature information from input image, audio and water wave datas. Second, an Auxiliary-modality Reinforcement Primary-modality Mechanism (ARPM) is designed for inter-modal interaction and generate enhanced features, which consists of a Channel Attention Fusion Network (CAFN) and a Dual-mode Attention Fusion Network (DAFN). Finally, an Evidence Reasoning (ER) rule is introduced to fuse the output results of each modality and make decisions, thereby completing the quantification of fish feeding intensity. The experimental results show that the constructed MAINet reaches 96.76%, 96.78%, 96.79% and 96.79% in accuracy, precision, recall and F1-Score respectively, and its performance is significantly higher than the comparison models. Compared with models that adopt single-modality, dual-modality fusion and different decision-making fusion methods, it also has obvious advantages. Meanwhile, the ablation experiments further verified the key role of the proposed improvement strategy in improving the robustness and feature utilization efficiency of model, which can effectively improve the accuracy of the quantitative results of fish feeding intensity.

</details>


### [209] [HRGS: Hierarchical Gaussian Splatting for Memory-Efficient High-Resolution 3D Reconstruction](https://arxiv.org/abs/2506.14229)
*Changbai Li,Haodong Zhu,Hanlin Chen,Juan Zhang,Tongfei Chen,Shuo Yang,Shuwei Shao,Wenhao Dong,Baochang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种内存高效的分层高斯泼溅框架HRGS，通过分层块级优化和重要性驱动的高斯剪枝，解决了3D高斯泼溅在高分辨率场景下的内存扩展性问题，实现了高质量、高分辨率的3D场景重建。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅（3DGS）在实时3D场景重建中取得显著进展，但在高分辨率场景下面临内存扩展性问题。现有方法难以在内存受限条件下实现高质量重建。

Method: 1. 分层块级优化：首先生成低分辨率全局粗高斯表示，再将场景划分为多个块并分别用高分辨率数据细化；2. 高斯分区与训练数据分区结合；3. 引入重要性驱动高斯剪枝（IDGP）加速收敛并减少内存占用；4. 利用预训练模型的法线先验提升表面重建质量。

Result: 在三个基准测试中，HRGS在高分辨率新视角合成（NVS）和表面重建任务中达到最先进性能，验证了其内存效率和重建质量优势。

Conclusion: HRGS通过分层优化和剪枝策略，有效解决了高分辨率3D重建的内存瓶颈，同时保持高质量输出，为实际应用提供了可行方案。

Abstract: 3D Gaussian Splatting (3DGS) has made significant strides in real-time 3D scene reconstruction, but faces memory scalability issues in high-resolution scenarios. To address this, we propose Hierarchical Gaussian Splatting (HRGS), a memory-efficient framework with hierarchical block-level optimization. First, we generate a global, coarse Gaussian representation from low-resolution data. Then, we partition the scene into multiple blocks, refining each block with high-resolution data. The partitioning involves two steps: Gaussian partitioning, where irregular scenes are normalized into a bounded cubic space with a uniform grid for task distribution, and training data partitioning, where only relevant observations are retained for each block. By guiding block refinement with the coarse Gaussian prior, we ensure seamless Gaussian fusion across adjacent blocks. To reduce computational demands, we introduce Importance-Driven Gaussian Pruning (IDGP), which computes importance scores for each Gaussian and removes those with minimal contribution, speeding up convergence and reducing memory usage. Additionally, we incorporate normal priors from a pretrained model to enhance surface reconstruction quality. Our method enables high-quality, high-resolution 3D scene reconstruction even under memory constraints. Extensive experiments on three benchmarks show that HRGS achieves state-of-the-art performance in high-resolution novel view synthesis (NVS) and surface reconstruction tasks.

</details>


### [210] [FRIDU: Functional Map Refinement with Guided Image Diffusion](https://arxiv.org/abs/2506.14322)
*Avigail Cohen Rimon,Mirela Ben-Chen,Or Litany*

Main category: cs.CV

TL;DR: 本文提出一种基于图像扩散模型的功能性映射优化方法，通过直接在功能性映射空间训练扩散模型，结合点级映射指导和目标约束（如正交性和算子交换性），实现高效且准确的形状对应映射优化。


<details>
  <summary>Details</summary>
Motivation: 现有形状对应映射优化方法通常依赖复杂计算或特定约束条件，而功能性映射作为基础变换矩阵可视为2D图像，但如何高效利用其结构特性进行优化仍具挑战性。

Method: 将功能性映射视为2D图像，训练扩散模型直接在其空间生成优化结果；推理时引入点级映射作为扩散过程的指导，并融入正交性和拉普拉斯-贝尔特拉米算子交换性等目标约束。

Result: 实验表明该方法在映射优化任务中与最先进方法竞争力相当，且引导扩散模型为功能性映射处理提供了高效且灵活的框架。

Conclusion: 基于功能性映射空间的扩散模型结合目标引导机制，为形状对应优化开辟了新路径，验证了其在保持高效性的同时提升映射质量的潜力。

Abstract: We propose a novel approach for refining a given correspondence map between two shapes. A correspondence map represented as a functional map, namely a change of basis matrix, can be additionally treated as a 2D image. With this perspective, we train an image diffusion model directly in the space of functional maps, enabling it to generate accurate maps conditioned on an inaccurate initial map. The training is done purely in the functional space, and thus is highly efficient. At inference time, we use the pointwise map corresponding to the current functional map as guidance during the diffusion process. The guidance can additionally encourage different functional map objectives, such as orthogonality and commutativity with the Laplace-Beltrami operator. We show that our approach is competitive with state-of-the-art methods of map refinement and that guided diffusion models provide a promising pathway to functional map processing.

</details>


### [211] [MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models](https://arxiv.org/abs/2506.14435)
*Hongyu Wang,Jiayu Xu,Ruiping Wang,Yan Feng,Yitao Zhai,Peng Pei,Xunliang Cai,Xilin Chen*

Main category: cs.CV

TL;DR: 提出MoTE方法，通过训练更多低精度（三元）专家替代全精度专家，在保持性能的同时显著降低内存占用，适用于边缘设备。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态混合专家模型（MoEs）因使用全精度专家导致内存占用过高，难以在边缘设备部署。需在保持性能的前提下降低内存需求。

Method: 基于预训练FFN共享专家，训练参数为{-1,0,1}的三元路由专家，结合训练后量化方法，以低精度专家扩展模型规模。

Result: MoTE在3.4GB内存限制下，结合量化后平均任务准确率比全精度基线MoE-LLaVA高4.3%，且模型规模扩展趋势良好。

Conclusion: MoTE通过低精度专家与量化技术，在内存受限设备上实现高效部署，验证了其有效性与实际应用潜力。

Abstract: Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size to boost performance while maintaining fixed active parameters. However, previous works primarily utilized full-precision experts during sparse up-cycling. Despite they show superior performance on end tasks, the large amount of experts introduces higher memory footprint, which poses significant challenges for the deployment on edge devices. In this work, we propose MoTE, a scalable and memory-efficient approach to train Mixture-of-Ternary-Experts models from dense checkpoint. Instead of training fewer high-precision experts, we propose to train more low-precision experts during up-cycling. Specifically, we use the pre-trained FFN as a shared expert and train ternary routed experts with parameters in {-1, 0, 1}. Extensive experiments show that our approach has promising scaling trend along model size. MoTE achieves comparable performance to full-precision baseline MoE-LLaVA while offering lower memory footprint. Furthermore, our approach is compatible with post-training quantization methods and the advantage further amplifies when memory-constraint goes lower. Given the same amount of expert memory footprint of 3.4GB and combined with post-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3% average accuracy on end tasks, demonstrating its effectiveness and potential for memory-constrained devices.

</details>


### [212] [Model compression using knowledge distillation with integrated gradients](https://arxiv.org/abs/2506.14440)
*David E. Hernandez,Jose Chang,Torbjörn E. M. Nordling*

Main category: cs.CV

TL;DR: 本文提出一种基于集成梯度（IG）增强知识蒸馏的新方法，通过预计算IG图叠加输入图像，提升学生模型对教师模型决策的理解，在CIFAR-10上实现92.6%准确率与4.1倍压缩，推理时间从140ms降至13ms。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备部署深度学习模型时，模型压缩至关重要。传统方法在压缩率与精度间存在权衡，需探索更高效的知识蒸馏策略以提升学生模型性能。

Method: 将集成梯度图作为数据增强策略，训练时将IG热图叠加至输入图像，使学生模型学习教师模型的决策逻辑。预计算IG图将运行时开销转化为一次性预处理步骤，降低训练成本。

Result: CIFAR-10测试准确率达92.6%（较非蒸馏模型提升1.1%），压缩因子4.1倍，推理时间减少90%。实验涵盖注意力迁移对比、蒙特卡洛鲁棒性验证、2.2x-1122x压缩范围评估及ImageNet子集泛化性验证。

Conclusion: 基于IG的知识蒸馏在多种架构与压缩率下优于传统方法，通过预计算平衡精度与效率，为边缘设备部署提供可行方案。结合注意力迁移可产生互补优势，统计实验证实其稳健性。

Abstract: Model compression is critical for deploying deep learning models on resource-constrained devices. We introduce a novel method enhancing knowledge distillation with integrated gradients (IG) as a data augmentation strategy. Our approach overlays IG maps onto input images during training, providing student models with deeper insights into teacher models' decision-making processes. Extensive evaluation on CIFAR-10 demonstrates that our IG-augmented knowledge distillation achieves 92.6% testing accuracy with a 4.1x compression factor-a significant 1.1 percentage point improvement ($p<0.001$) over non-distilled models (91.5%). This compression reduces inference time from 140 ms to 13 ms. Our method precomputes IG maps before training, transforming substantial runtime costs into a one-time preprocessing step. Our comprehensive experiments include: (1) comparisons with attention transfer, revealing complementary benefits when combined with our approach; (2) Monte Carlo simulations confirming statistical robustness; (3) systematic evaluation of compression factor versus accuracy trade-offs across a wide range (2.2x-1122x); and (4) validation on an ImageNet subset aligned with CIFAR-10 classes, demonstrating generalisability beyond the initial dataset. These extensive ablation studies confirm that IG-based knowledge distillation consistently outperforms conventional approaches across varied architectures and compression ratios. Our results establish this framework as a viable compression technique for real-world deployment on edge devices while maintaining competitive accuracy.

</details>


### [213] [EVA02-AT: Egocentric Video-Language Understanding with Spatial-Temporal Rotary Positional Embeddings and Symmetric Optimization](https://arxiv.org/abs/2506.14356)
*Xiaoqi Wang,Yi Wang,Lap-Pui Chau*

Main category: cs.CV

TL;DR: EVA02-AT提出了一种高效的视频-语言基础模型，通过单阶段预训练、空间-时间旋转位置编码和对称多相似性损失，在自我中心视频理解任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在自我中心视频-语言理解中存在三个主要问题：多阶段预训练成本高、空间-时间编码效率低、以及多实例检索的学习目标不精确。EVA02-AT旨在解决这些问题。

Method: 1) 通过单阶段预训练将图像CLIP模型转换为视频编码器；2) 引入空间-时间旋转位置编码和联合注意力机制；3) 提出对称多相似性（SMS）损失函数优化多实例检索任务。

Result: 在Ego4D、EPIC-Kitchens-100和Charades-Ego数据集上的实验表明，EVA02-AT在零样本和微调设置下均达到最优性能，且参数量更少。SMS损失显著提升了多实例检索性能。

Conclusion: EVA02-AT通过高效预训练、改进的空间-时间编码和精确的学习目标，为自我中心视频理解任务提供了强大的基础模型，并在多个基准测试中验证了其有效性。

Abstract: Egocentric video-language understanding demands both high efficiency and accurate spatial-temporal modeling. Existing approaches face three key challenges: 1) Excessive pre-training cost arising from multi-stage pre-training pipelines, 2) Ineffective spatial-temporal encoding due to manually split 3D rotary positional embeddings that hinder feature interactions, and 3) Imprecise learning objectives in soft-label multi-instance retrieval, which neglect negative pair correlations. In this paper, we introduce EVA02-AT, a suite of EVA02-based video-language foundation models tailored to egocentric video understanding tasks. EVA02-AT first efficiently transfers an image-based CLIP model into a unified video encoder via a single-stage pretraining. Second, instead of applying rotary positional embeddings to isolated dimensions, we introduce spatial-temporal rotary positional embeddings along with joint attention, which can effectively encode both spatial and temporal information on the entire hidden dimension. This joint encoding of spatial-temporal features enables the model to learn cross-axis relationships, which are crucial for accurately modeling motion and interaction in videos. Third, focusing on multi-instance video-language retrieval tasks, we introduce the Symmetric Multi-Similarity (SMS) loss and a novel training framework that advances all soft labels for both positive and negative pairs, providing a more precise learning objective. Extensive experiments on Ego4D, EPIC-Kitchens-100, and Charades-Ego under zero-shot and fine-tuning settings demonstrate that EVA02-AT achieves state-of-the-art performance across diverse egocentric video-language tasks with fewer parameters. Models with our SMS loss also show significant performance gains on multi-instance retrieval benchmarks. Our code and models are publicly available at https://github.com/xqwang14/EVA02-AT .

</details>


### [214] [Foundation Model Insights and a Multi-Model Approach for Superior Fine-Grained One-shot Subset Selection](https://arxiv.org/abs/2506.14473)
*Zhijing Wan,Zhixiang Wang,Zheng Wang,Xin Xu,Shin'ichi Satoh*

Main category: cs.CV

TL;DR: 本文研究基于基础模型（FM）的一次性子集选择方法，发现FM在细粒度数据集上优于传统信息提取器（IE），但在粗粒度噪声数据中优势减弱，并提出RAM-APL方法以结合多FM的互补优势，显著提升细粒度数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 传统信息提取器（IE）依赖预训练数据集，泛化能力受限。基础模型（FM）因其广泛适用性，可能突破这一限制。研究旨在验证FM在子集选择中的有效性，并探索不同FM的表现差异。

Method: 提出RAM-APL方法，通过利用多个基础模型的伪类别标签平均准确率排名，结合不同FM的互补优势，优化细粒度图像数据集的子集选择。

Result: 实验表明，FM在细粒度数据集（如Oxford-IIIT Pet、Food-101等）上显著优于传统IE，但在粗粒度噪声数据中优势不明显。RAM-APL在多个细粒度数据集上达到最先进性能。

Conclusion: 基础模型在细粒度数据子集选择中具有显著优势，而RAM-APL通过多模型互补性进一步提升了性能，验证了FM作为信息提取器的潜力。

Abstract: One-shot subset selection serves as an effective tool to reduce deep learning training costs by identifying an informative data subset based on the information extracted by an information extractor (IE). Traditional IEs, typically pre-trained on the target dataset, are inherently dataset-dependent. Foundation models (FMs) offer a promising alternative, potentially mitigating this limitation. This work investigates two key questions: (1) Can FM-based subset selection outperform traditional IE-based methods across diverse datasets? (2) Do all FMs perform equally well as IEs for subset selection? Extensive experiments uncovered surprising insights: FMs consistently outperform traditional IEs on fine-grained datasets, whereas their advantage diminishes on coarse-grained datasets with noisy labels. Motivated by these finding, we propose RAM-APL (RAnking Mean-Accuracy of Pseudo-class Labels), a method tailored for fine-grained image datasets. RAM-APL leverages multiple FMs to enhance subset selection by exploiting their complementary strengths. Our approach achieves state-of-the-art performance on fine-grained datasets, including Oxford-IIIT Pet, Food-101, and Caltech-UCSD Birds-200-2011.

</details>


### [215] [DepthSeg: Depth prompting in remote sensing semantic segmentation](https://arxiv.org/abs/2506.14382)
*Ning Zhou,Shanxiong Chen,Mingting Zhou,Haigang Sui,Lieyun Hu,Han Li,Li Hua,Qiming Zhou*

Main category: cs.CV

TL;DR: 提出DepthSeg框架，通过自动建模2D遥感影像的深度/高度信息并结合语义分割，解决光谱混淆和阴影遮挡导致的土地覆盖误分类问题，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有遥感语义分割方法仅关注光谱特征，忽略目标高程差异，导致复杂场景（如阴影遮挡、光谱混淆）中土地覆盖分类错误。

Method: DepthSeg框架分三阶段：1) 使用轻量适配器微调预训练视觉Transformer；2) 通过深度提示器显式建模深度/高度特征；3) 语义分类解码器耦合深度提示与高维特征实现精准分类。

Result: 在LiuZhou数据集上验证框架优势，消融实验表明深度提示对提升遥感语义分割效果具有显著作用。

Conclusion: DepthSeg通过整合深度信息有效缓解光谱混淆与阴影干扰，证实深度提示在复杂场景土地覆盖分类中的关键价值。

Abstract: Remote sensing semantic segmentation is crucial for extracting detailed land surface information, enabling applications such as environmental monitoring, land use planning, and resource assessment. In recent years, advancements in artificial intelligence have spurred the development of automatic remote sensing semantic segmentation methods. However, the existing semantic segmentation methods focus on distinguishing spectral characteristics of different objects while ignoring the differences in the elevation of the different targets. This results in land cover misclassification in complex scenarios involving shadow occlusion and spectral confusion. In this paper, we introduce a depth prompting two-dimensional (2D) remote sensing semantic segmentation framework (DepthSeg). It automatically models depth/height information from 2D remote sensing images and integrates it into the semantic segmentation framework to mitigate the effects of spectral confusion and shadow occlusion. During the feature extraction phase of DepthSeg, we introduce a lightweight adapter to enable cost-effective fine-tuning of the large-parameter vision transformer encoder pre-trained by natural images. In the depth prompting phase, we propose a depth prompter to model depth/height features explicitly. In the semantic prediction phase, we introduce a semantic classification decoder that couples the depth prompts with high-dimensional land-cover features, enabling accurate extraction of land-cover types. Experiments on the LiuZhou dataset validate the advantages of the DepthSeg framework in land cover mapping tasks. Detailed ablation studies further highlight the significance of the depth prompts in remote sensing semantic segmentation.

</details>


### [216] [Risk Estimation of Knee Osteoarthritis Progression via Predictive Multi-task Modelling from Efficient Diffusion Model using X-ray Images](https://arxiv.org/abs/2506.14560)
*David Butler,Adrian Hilton,Gustavo Carneiro*

Main category: cs.CV

TL;DR: 本文提出一种可解释的机器学习方法，通过多任务预测模型结合扩散模型生成高质量未来膝关节图像，以提升骨关节炎进展风险预测性能及可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有膝关节骨关节炎风险预测模型因生成未来图像的复杂性、缺乏解剖标志定位能力导致可解释性不足，限制了临床应用。

Method: 采用类别条件潜在空间扩散模型生成未来膝关节图像，结合多任务框架同时预测OA严重程度分类和解剖标志定位，实现高效推理。

Result: 在Osteoarthritis Initiative数据集上，AUC达0.71（较SOTA提升2%），推理时间减少约9%，并可视化疾病进展路径。

Conclusion: 该方法通过可解释的多任务预测建模，在提升膝关节OA进展预测性能的同时，为临床决策提供了直观的解剖标志定位依据。

Abstract: Medical imaging plays a crucial role in assessing knee osteoarthritis (OA) risk by enabling early detection and disease monitoring. Recent machine learning methods have improved risk estimation (i.e., predicting the likelihood of disease progression) and predictive modelling (i.e., the forecasting of future outcomes based on current data) using medical images, but clinical adoption remains limited due to their lack of interpretability. Existing approaches that generate future images for risk estimation are complex and impractical. Additionally, previous methods fail to localize anatomical knee landmarks, limiting interpretability. We address these gaps with a new interpretable machine learning method to estimate the risk of knee OA progression via multi-task predictive modelling that classifies future knee OA severity and predicts anatomical knee landmarks from efficiently generated high-quality future images. Such image generation is achieved by leveraging a diffusion model in a class-conditioned latent space to forecast disease progression, offering a visual representation of how particular health conditions may evolve. Applied to the Osteoarthritis Initiative dataset, our approach improves the state-of-the-art (SOTA) by 2\%, achieving an AUC of 0.71 in predicting knee OA progression while offering ~9% faster inference time.

</details>


### [217] [Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models](https://arxiv.org/abs/2506.14399)
*Tian Xia,Fabio De Sousa Ribeiro,Rajat R Rasal,Avinash Kori,Raghav Mehta,Ben Glocker*

Main category: cs.CV

TL;DR: 该论文提出了一种名为解耦分类器自由引导（DCFG）的新方法，用于改进反事实图像生成中的属性控制和干预保真度。


<details>
  <summary>Details</summary>
Motivation: 现有的分类器自由引导（CFG）方法在反事实图像生成中存在属性放大问题，导致身份保持不佳和虚假属性变化。

Method: 通过属性分割嵌入策略解耦语义输入，对用户定义的属性组进行选择性引导，并在因果图基础上将属性分为干预组和不变组。

Result: 在CelebA-HQ、MIMIC-CXR和EMBED数据集上的实验表明，DCFG提高了干预保真度，减少了意外变化，并增强了可逆性。

Conclusion: DCFG框架能够实现更忠实和可解释的反事实图像生成，具有灵活性和模型无关性。

Abstract: Counterfactual image generation aims to simulate realistic visual outcomes under specific causal interventions. Diffusion models have recently emerged as a powerful tool for this task, combining DDIM inversion with conditional generation via classifier-free guidance (CFG). However, standard CFG applies a single global weight across all conditioning variables, which can lead to poor identity preservation and spurious attribute changes - a phenomenon known as attribute amplification. To address this, we propose Decoupled Classifier-Free Guidance (DCFG), a flexible and model-agnostic framework that introduces group-wise conditioning control. DCFG builds on an attribute-split embedding strategy that disentangles semantic inputs, enabling selective guidance on user-defined attribute groups. For counterfactual generation, we partition attributes into intervened and invariant sets based on a causal graph and apply distinct guidance to each. Experiments on CelebA-HQ, MIMIC-CXR, and EMBED show that DCFG improves intervention fidelity, mitigates unintended changes, and enhances reversibility, enabling more faithful and interpretable counterfactual image generation.

</details>


### [218] [Causally Steered Diffusion for Automated Video Counterfactual Generation](https://arxiv.org/abs/2506.14404)
*Nikos Spyrou,Athanasios Vlontzos,Paraskevas Pegios,Thomas Melistas,Nefeli Gkouti,Yannis Panagakis,Giorgos Papanastasiou,Sotirios A. Tsaftaris*

Main category: cs.CV

TL;DR: 本文提出了一种基于因果忠诚框架的反事实视频生成方法，通过视觉语言模型优化文本提示，无需依赖底层视频编辑系统内部机制，有效保持视频内容的因果关系，并在多个领域展示应用前景。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频扩散模型在编辑因果依赖属性时可能因忽略因果关系导致不现实结果，需解决因果忠实性问题。

Method: 采用与底层系统无关的框架，通过视觉语言模型和因果图优化文本提示，控制潜在扩散模型的生成过程，无需微调或访问系统内部机制。

Result: 实验表明该方法在标准视频质量指标和反事实特异性标准（如因果有效性）上表现优异，成功在模型学习分布内生成因果忠实视频。

Conclusion: 该方法兼容任意黑盒视频编辑系统，在医疗和数字媒体等领域具有生成现实反事实场景的潜力，验证了提示驱动因果引导的有效性。

Abstract: Adapting text-to-image (T2I) latent diffusion models for video editing has shown strong visual fidelity and controllability, but challenges remain in maintaining causal relationships in video content. Edits affecting causally dependent attributes risk generating unrealistic or misleading outcomes if these relationships are ignored. In this work, we propose a causally faithful framework for counterfactual video generation, guided by a vision-language model (VLM). Our method is agnostic to the underlying video editing system and does not require access to its internal mechanisms or finetuning. Instead, we guide the generation by optimizing text prompts based on an assumed causal graph, addressing the challenge of latent space control in LDMs. We evaluate our approach using standard video quality metrics and counterfactual-specific criteria, such as causal effectiveness and minimality. Our results demonstrate that causally faithful video counterfactuals can be effectively generated within the learned distribution of LDMs through prompt-based causal steering. With its compatibility with any black-box video editing system, our method holds significant potential for generating realistic "what-if" video scenarios in diverse areas such as healthcare and digital media.

</details>


### [219] [Align Your Flow: Scaling Continuous-Time Flow Map Distillation](https://arxiv.org/abs/2506.14603)
*Amirmojtaba Sabour,Sanja Fidler,Karsten Kreis*

Main category: cs.CV

TL;DR: 本文提出了一种名为Align Your Flow的流映射模型，通过新的连续时间目标和训练技术，改进了扩散和流模型的生成效率，在图像生成任务中取得了最先进的少步生成性能。


<details>
  <summary>Details</summary>
Motivation: 扩散和流模型虽然是最先进的生成方法，但需要大量采样步骤。一致性模型可以将其蒸馏为高效的一步生成器，但性能会随着步骤增加而下降。流映射通过连接任意两个噪声级别来泛化这些方法，并在所有步骤数下保持有效。

Method: 提出了两种新的连续时间目标用于训练流映射，并引入了新的训练技术，包括自动引导和对抗微调，以提升性能。

Result: 在ImageNet 64x64和512x512等图像生成基准测试中，使用小型高效神经网络实现了最先进的少步生成性能，并在文本条件合成中超越了所有现有的非对抗训练少步采样器。

Conclusion: Align Your Flow模型通过流映射和新的训练技术，显著提升了生成模型的效率和性能，适用于各种图像生成任务。

Abstract: Diffusion- and flow-based models have emerged as state-of-the-art generative modeling approaches, but they require many sampling steps. Consistency models can distill these models into efficient one-step generators; however, unlike flow- and diffusion-based methods, their performance inevitably degrades when increasing the number of steps, which we show both analytically and empirically. Flow maps generalize these approaches by connecting any two noise levels in a single step and remain effective across all step counts. In this paper, we introduce two new continuous-time objectives for training flow maps, along with additional novel training techniques, generalizing existing consistency and flow matching objectives. We further demonstrate that autoguidance can improve performance, using a low-quality model for guidance during distillation, and an additional boost can be achieved by adversarial finetuning, with minimal loss in sample diversity. We extensively validate our flow map models, called Align Your Flow, on challenging image generation benchmarks and achieve state-of-the-art few-step generation performance on both ImageNet 64x64 and 512x512, using small and efficient neural networks. Finally, we show text-to-image flow map models that outperform all existing non-adversarially trained few-step samplers in text-conditioned synthesis.

</details>


### [220] [Unsupervised Imaging Inverse Problems with Diffusion Distribution Matching](https://arxiv.org/abs/2506.14605)
*Giacomo Meanti,Thomas Ryckeboer,Michael Arbel,Julien Mairal*

Main category: cs.CV

TL;DR: 本文提出一种基于未配对数据集的图像复原方法，通过条件流匹配建模退化观测分布，并在正向模型未知或误设的现实场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统图像复原方法需完整正向模型或配对数据，而现实场景中常面临模型未知、数据采集困难的问题。该方法旨在通过最小假设和小型未配对数据集解决此矛盾。

Method: 结合条件流匹配建模退化观测分布，通过框架自然衍生的分布匹配损失同步学习正向模型。

Result: 在去模糊/非均匀PSF校准任务中超越单图像盲方法和无监督方法，盲超分辨率达到SOTA水平，并以镜头校准案例验证实际应用可行性。

Conclusion: 该方法通过弱数据依赖和模型自适应性，显著降低了传统图像复原对专业设备/耗时实验的依赖，为现实场景提供了高效解决方案。

Abstract: This work addresses image restoration tasks through the lens of inverse problems using unpaired datasets. In contrast to traditional approaches -- which typically assume full knowledge of the forward model or access to paired degraded and ground-truth images -- the proposed method operates under minimal assumptions and relies only on small, unpaired datasets. This makes it particularly well-suited for real-world scenarios, where the forward model is often unknown or misspecified, and collecting paired data is costly or infeasible. The method leverages conditional flow matching to model the distribution of degraded observations, while simultaneously learning the forward model via a distribution-matching loss that arises naturally from the framework. Empirically, it outperforms both single-image blind and unsupervised approaches on deblurring and non-uniform point spread function (PSF) calibration tasks. It also matches state-of-the-art performance on blind super-resolution. We also showcase the effectiveness of our method with a proof of concept for lens calibration: a real-world application traditionally requiring time-consuming experiments and specialized equipment. In contrast, our approach achieves this with minimal data acquisition effort.

</details>


### [221] [Compositional Attribute Imbalance in Vision Datasets](https://arxiv.org/abs/2506.14418)
*Jiayi Chen,Yanbiao Ma,Andi Zhang,Weidong Tang,Wei Dai,Bowei Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种基于CLIP的视觉属性字典构建方法，通过分析单属性和组合属性不平衡问题，提出调整样本采样概率并结合数据增强技术，有效提升了模型在长尾图像分类任务中的鲁棒性和公平性。


<details>
  <summary>Details</summary>
Motivation: 视觉属性不平衡是图像分类中常见但未被充分研究的问题，显著影响模型性能和泛化能力。论文旨在解决这一问题，尤其是单属性和组合属性不平衡对模型的影响。

Method: 论文首先定义了图像的一级和二级属性，并引入基于CLIP的框架构建视觉属性字典。通过分析属性不平衡问题，提出调整样本采样概率并结合CutMix、Fmix和SaliencyMix等数据增强技术。

Result: 在基准数据集上的大量实验表明，该方法有效缓解了属性不平衡问题，提升了深度神经网络的鲁棒性和公平性。

Conclusion: 研究强调了建模视觉属性分布的重要性，并为长尾图像分类任务提供了一种可扩展的解决方案。

Abstract: Visual attribute imbalance is a common yet underexplored issue in image classification, significantly impacting model performance and generalization. In this work, we first define the first-level and second-level attributes of images and then introduce a CLIP-based framework to construct a visual attribute dictionary, enabling automatic evaluation of image attributes. By systematically analyzing both single-attribute imbalance and compositional attribute imbalance, we reveal how the rarity of attributes affects model performance. To tackle these challenges, we propose adjusting the sampling probability of samples based on the rarity of their compositional attributes. This strategy is further integrated with various data augmentation techniques (such as CutMix, Fmix, and SaliencyMix) to enhance the model's ability to represent rare attributes. Extensive experiments on benchmark datasets demonstrate that our method effectively mitigates attribute imbalance, thereby improving the robustness and fairness of deep neural networks. Our research highlights the importance of modeling visual attribute distributions and provides a scalable solution for long-tail image classification tasks.

</details>


### [222] [Adapting Lightweight Vision Language Models for Radiological Visual Question Answering](https://arxiv.org/abs/2506.14451)
*Aditya Shourya,Michel Dumontier,Chang Sun*

Main category: cs.CV

TL;DR: 本研究通过微调轻量级视觉语言模型，结合合成数据生成和多阶段训练，提出了一种高效的放射学视觉问答（VQA）模型，并在有限数据和参数下实现接近大型模型的性能，同时引入显著图诊断工具辅助模型分析。


<details>
  <summary>Details</summary>
Motivation: 放射学VQA模型面临专家标注数据稀缺、图像模式复杂及缺乏有效评估的挑战，需探索小规模模型在有限数据下的潜力与可靠性。

Method: 采用3B参数轻量模型，通过合成问答对生成和多阶段微调（基于ROCO v2.0、MedPix v2.0数据集），并设计基于显著性的诊断工具分析模型失败案例。

Result: 模型在开放/封闭式问题中表现稳健，性能接近LLaVA-Med等大型模型，且参数量和训练数据规模显著降低。

Conclusion: 适当优化的轻量模型在放射学VQA任务中具有竞争力，显著图工具可有效识别模型缺陷，为小规模高效模型开发提供参考。

Abstract: Recent advancements in vision-language systems have improved the accuracy of Radiological Visual Question Answering (VQA) Models. However, some challenges remain across each stage of model development: limited expert-labeled images hinders data procurement at scale; the intricate and nuanced patterns of radiological images make modeling inherently difficult; and the lack of evaluation evaluation efforts makes it difficult to identify cases where the model might be ill-conditioned. In this study, we fine-tune a lightweight 3B parameter vision-language model for Radiological VQA, demonstrating that small models, when appropriately tuned with curated data, can achieve robust performance across both open- and closed-ended questions. We propose a cost-effective training pipeline from synthetic question-answer pair generation to multi-stage fine-tuning on specialised radiological domain-targeted datasets (e.g., ROCO v2.0, MedPix v2.0). Our results show that despite operating at a fraction of the scale of state-of-the-art models such as LLaVA-Med, our model achieves promising performance given its small parameter size and the limited scale of training data. We introduce a lightweight saliency-based diagnostic tool that enables domain experts to inspect VQA model performance and identify ill-conditioned failure modes through saliency analysis.

</details>


### [223] [Cost-Aware Routing for Efficient Text-To-Image Generation](https://arxiv.org/abs/2506.14753)
*Qinchan,Li,Kenneth Chen,Changyue,Su,Wittawat Jitkrittum,Qi Sun,Patsorn Sangkloy*

Main category: cs.CV

TL;DR: 本文提出一种基于提示复杂度的自适应路由框架，通过动态分配不同计算成本的文生图模型，在保证生成质量的同时优化计算效率。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成高保真图像需大量迭代去噪步骤，导致计算成本高昂。现有方法采用统一降成本策略，无法根据提示复杂度动态调整计算资源。

Method: 设计自动路由机制，将提示分配到9个预训练模型（含不同去噪步数的扩散模型或轻量模型），通过强化学习优化路由策略，使复杂提示使用高成本模型，简单提示使用低成本模型。

Result: 在COCO和DiffusionDB数据集上，该框架平均生成质量超过所有单独模型，验证了计算资源动态分配的有效性。

Conclusion: 通过提示感知的自适应路由机制，实现了质量与计算成本的最优权衡，为文生图系统提供高效部署方案。

Abstract: Diffusion models are well known for their ability to generate a high-fidelity image for an input prompt through an iterative denoising process. Unfortunately, the high fidelity also comes at a high computational cost due the inherently sequential generative process. In this work, we seek to optimally balance quality and computational cost, and propose a framework to allow the amount of computation to vary for each prompt, depending on its complexity. Each prompt is automatically routed to the most appropriate text-to-image generation function, which may correspond to a distinct number of denoising steps of a diffusion model, or a disparate, independent text-to-image model. Unlike uniform cost reduction techniques (e.g., distillation, model quantization), our approach achieves the optimal trade-off by learning to reserve expensive choices (e.g., 100+ denoising steps) only for a few complex prompts, and employ more economical choices (e.g., small distilled model) for less sophisticated prompts. We empirically demonstrate on COCO and DiffusionDB that by learning to route to nine already-trained text-to-image models, our approach is able to deliver an average quality that is higher than that achievable by any of these models alone.

</details>


### [224] [Synthetic Data Augmentation for Table Detection: Re-evaluating TableNet's Performance with Automatically Generated Document Images](https://arxiv.org/abs/2506.14583)
*Krishna Sahukara,Zineddine Bettouche,Andreas Fischer*

Main category: cs.CV

TL;DR: 提出基于LaTeX的自动化流程生成多样化表格布局合成数据，用于增强真实数据集Marmot，并通过TableNet模型显著降低表格检测误差，减少人工标注成本。


<details>
  <summary>Details</summary>
Motivation: 手机或扫描仪捕获的文档页面常含表格，但人工提取效率低且易错，需自动化解决方案。

Method: 构建合成数据生成管道：通过LaTeX合成双栏页面，生成视觉多样化的表格布局及对齐的真实掩码，并整合至Marmot基准数据集。

Result: TableNet在合成测试集上像素级XOR误差为4.04%（256x256）和4.33%（1024x1024）；在Marmot数据集上最佳误差为9.18%（256x256）。

Conclusion: 合成数据有效提升表格检测性能，同时通过自动化大幅减少人工标注需求，验证了方法的实用性与高效性。

Abstract: Document pages captured by smartphones or scanners often contain tables, yet manual extraction is slow and error-prone. We introduce an automated LaTeX-based pipeline that synthesizes realistic two-column pages with visually diverse table layouts and aligned ground-truth masks. The generated corpus augments the real-world Marmot benchmark and enables a systematic resolution study of TableNet. Training TableNet on our synthetic data achieves a pixel-wise XOR error of 4.04% on our synthetic test set with a 256x256 input resolution, and 4.33% with 1024x1024. The best performance on the Marmot benchmark is 9.18% (at 256x256), while cutting manual annotation effort through automation.

</details>


### [225] [PoseGRAF: Geometric-Reinforced Adaptive Fusion for Monocular 3D Human Pose Estimation](https://arxiv.org/abs/2506.14596)
*Ming Xu,Xu Zhang*

Main category: cs.CV

TL;DR: 提出PoseGRAF框架，通过双图卷积结构建模关节与骨骼的局部依赖关系，结合交叉注意力机制和动态融合模块解决传统单目3D姿态估计在遮挡/快速运动下的失真问题，在多个数据集上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有单目3D姿态估计方法过度依赖关节位置特征，忽略骨骼内在方向与角度关联，导致关节遮挡或快速运动时生成不合理姿态。

Method: 1. 构建关节图与骨骼图的双图卷积结构捕捉局部特征 2. 交叉注意力模块建模骨骼方向-关节特征依赖 3. 动态融合模块自适应整合特征 4. 残差式改进Transformer编码器生成最终输出

Result: Human3.6M/MPI-INF-3DHP数据集上超越SOTA方法，真实场景视频验证泛化能力，代码已开源。

Conclusion: 通过显式建模骨骼方向与关节特征的空间关系，有效提升复杂场景下的姿态估计鲁棒性，实验验证了方法的优越性与泛化性。

Abstract: Existing monocular 3D pose estimation methods primarily rely on joint positional features, while overlooking intrinsic directional and angular correlations within the skeleton. As a result, they often produce implausible poses under joint occlusions or rapid motion changes. To address these challenges, we propose the PoseGRAF framework. We first construct a dual graph convolutional structure that separately processes joint and bone graphs, effectively capturing their local dependencies. A Cross-Attention module is then introduced to model interdependencies between bone directions and joint features. Building upon this, a dynamic fusion module is designed to adaptively integrate both feature types by leveraging the relational dependencies between joints and bones. An improved Transformer encoder is further incorporated in a residual manner to generate the final output. Experimental results on the Human3.6M and MPI-INF-3DHP datasets show that our method exceeds state-of-the-art approaches. Additional evaluations on in-the-wild videos further validate its generalizability. The code is publicly available at https://github.com/iCityLab/PoseGRAF.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [226] [Excessive Reasoning Attack on Reasoning LLMs](https://arxiv.org/abs/2506.14374)
*Wai Man Si,Mingjie Li,Michael Backes,Yang Zhang*

Main category: cs.CR

TL;DR: 本文提出一种针对大型语言模型（LLM）的对抗攻击方法，通过设计特定损失函数诱导模型在推理时产生过度计算，显著增加计算开销而不影响模型效果。


<details>
  <summary>Details</summary>
Motivation: 现有推理型大语言模型（如DeepSeek-R1）在复杂任务中常因过度推理（如路径频繁切换、简单问题冗余计算）导致高计算成本。研究发现对抗输入可被设计来恶意利用此行为，加剧计算负担。

Method: 提出三阶段损失框架：1）优先级交叉熵损失（强化关键token预测）；2）过度推理损失（诱导多推理路径生成）；3）延迟终止损失（延长推理过程延迟输出）。在DeepSeek-R1-Distill系列模型上针对GSM8K/ORCA数据集进行优化。

Result: 实验显示对抗输入使推理长度增加3-9倍且保持性能，攻击样本可迁移至o3-mini、QWQ等模型，跨模型引发计算开销。

Conclusion: 暴露了LLMs因过度推理行为引发的计算安全风险，证明对抗攻击可系统性操纵模型计算效率，需开发更鲁棒的推理终止机制应对此类威胁。

Abstract: Recent reasoning large language models (LLMs), such as OpenAI o1 and DeepSeek-R1, exhibit strong performance on complex tasks through test-time inference scaling. However, prior studies have shown that these models often incur significant computational costs due to excessive reasoning, such as frequent switching between reasoning trajectories (e.g., underthinking) or redundant reasoning on simple questions (e.g., overthinking). In this work, we expose a novel threat: adversarial inputs can be crafted to exploit excessive reasoning behaviors and substantially increase computational overhead without compromising model utility. Therefore, we propose a novel loss framework consisting of three components: (1) Priority Cross-Entropy Loss, a modification of the standard cross-entropy objective that emphasizes key tokens by leveraging the autoregressive nature of LMs; (2) Excessive Reasoning Loss, which encourages the model to initiate additional reasoning paths during inference; and (3) Delayed Termination Loss, which is designed to extend the reasoning process and defer the generation of final outputs. We optimize and evaluate our attack for the GSM8K and ORCA datasets on DeepSeek-R1-Distill-LLaMA and DeepSeek-R1-Distill-Qwen. Empirical results demonstrate a 3x to 9x increase in reasoning length with comparable utility performance. Furthermore, our crafted adversarial inputs exhibit transferability, inducing computational overhead in o3-mini, o1-mini, DeepSeek-R1, and QWQ models.

</details>


### [227] [LLM-Powered Intent-Based Categorization of Phishing Emails](https://arxiv.org/abs/2506.14337)
*Even Eilertsen,Vasileios Mavroeidis,Gudmund Grov*

Main category: cs.CR

TL;DR: 本文探讨利用大语言模型（LLMs）通过分析邮件文本意图检测钓鱼邮件，提出意图分类法并构建混合数据集，验证LLMs在此领域的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统钓鱼检测系统依赖用户不可见的元数据，且难以应对用户凭经验识别的钓鱼邮件文本。

Method: 使用LLMs分析邮件文本意图，设计意图分类法对邮件分类，并整合公开数据构建混合数据集进行验证。

Result: 实验表明现有LLMs能有效检测和分类钓鱼邮件，生成可操作的威胁情报。

Conclusion: LLMs在钓鱼邮件检测及意图分类中具有实际应用潜力，可补充传统防御机制。

Abstract: Phishing attacks remain a significant threat to modern cybersecurity, as they successfully deceive both humans and the defense mechanisms intended to protect them. Traditional detection systems primarily focus on email metadata that users cannot see in their inboxes. Additionally, these systems struggle with phishing emails, which experienced users can often identify empirically by the text alone. This paper investigates the practical potential of Large Language Models (LLMs) to detect these emails by focusing on their intent. In addition to the binary classification of phishing emails, the paper introduces an intent-type taxonomy, which is operationalized by the LLMs to classify emails into distinct categories and, therefore, generate actionable threat information. To facilitate our work, we have curated publicly available datasets into a custom dataset containing a mix of legitimate and phishing emails. Our results demonstrate that existing LLMs are capable of detecting and categorizing phishing emails, underscoring their potential in this domain.

</details>


### [228] [Busting the Paper Ballot: Voting Meets Adversarial Machine Learning](https://arxiv.org/abs/2506.14582)
*Kaleel Mahmood,Caleb Manicke,Ethan Rathbun,Aayushi Verma,Sohaib Ahmad,Nicholas Stamatakis,Laurent Michel,Benjamin Fuller*

Main category: cs.CR

TL;DR: 本文分析了美国选举计票机中使用机器学习分类器的安全风险，通过新数据集和模型训练揭示了对抗攻击的潜在威胁，并展示了物理域攻击对选举结果的影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示机器学习模型在选举计票中的潜在安全漏洞，特别是对抗样本攻击可能篡改选举结果的风险。

Method: 方法包括构建新选票数据集、训练多种模型（SVM、CNN、ViT）、分析梯度掩码问题并提出改进损失函数，最后在物理域实施对抗攻击实验。

Result: 实验表明，传统白盒攻击因梯度掩码失效，但改进方法可生成有效对抗样本；即使5%攻击成功率即可翻转选举结果，物理攻击验证了可行性。

Conclusion: 结论指出选举计票中的机器学习模型存在现实安全风险，需重新评估其部署并加强防御机制以应对对抗攻击威胁。

Abstract: We show the security risk associated with using machine learning classifiers in United States election tabulators. The central classification task in election tabulation is deciding whether a mark does or does not appear on a bubble associated to an alternative in a contest on the ballot. Barretto et al. (E-Vote-ID 2021) reported that convolutional neural networks are a viable option in this field, as they outperform simple feature-based classifiers.
  Our contributions to election security can be divided into four parts. To demonstrate and analyze the hypothetical vulnerability of machine learning models on election tabulators, we first introduce four new ballot datasets. Second, we train and test a variety of different models on our new datasets. These models include support vector machines, convolutional neural networks (a basic CNN, VGG and ResNet), and vision transformers (Twins and CaiT). Third, using our new datasets and trained models, we demonstrate that traditional white box attacks are ineffective in the voting domain due to gradient masking. Our analyses further reveal that gradient masking is a product of numerical instability. We use a modified difference of logits ratio loss to overcome this issue (Croce and Hein, ICML 2020). Fourth, in the physical world, we conduct attacks with the adversarial examples generated using our new methods. In traditional adversarial machine learning, a high (50% or greater) attack success rate is ideal. However, for certain elections, even a 5% attack success rate can flip the outcome of a race. We show such an impact is possible in the physical domain. We thoroughly discuss attack realism, and the challenges and practicality associated with printing and scanning ballot adversarial examples.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [229] [Analysis and Optimization of Probabilities of Beneficial Mutation and Crossover Recombination in a Hamming Space](https://arxiv.org/abs/2506.13809)
*Roman V. Belavkin*

Main category: q-bio.PE

TL;DR: 本文基于费舍尔几何方法，分析汉明空间中突变与重组的有利概率，推导出优化参数的最优条件，揭示突变与重组在进化中的互补作用。


<details>
  <summary>Details</summary>
Motivation: 受费舍尔几何方法启发，研究在任意有限字母表的汉明空间中，减少与最优解距离的突变和重组概率，以优化进化算法参数。

Method: 通过几何与组合分析，建立闭式表达式描述多代进化中距离的马尔可夫转移过程，并推导突变和重组半径的最优条件。

Result: 突变的最优半径随接近最优解而减小，导致进化减速；重组在种群子空间内平衡利弊且具平移不变性，可能加速接近最优解的进化速度。

Conclusion: 突变与重组在优化中存在本质差异：突变全局可达但效率递减，重组通过子空间互补性可能提升局部进化速率。

Abstract: Inspired by Fisher's geometric approach to study beneficial mutations, we analyse probabilities of beneficial mutation and crossover recombination of strings in a general Hamming space with arbitrary finite alphabet. Mutations and recombinations that reduce the distance to an optimum are considered as beneficial. Geometric and combinatorial analysis is used to derive closed-form expressions for transition probabilities between spheres around an optimum giving a complete description of Markov evolution of distances from an optimum over multiple generations. This paves the way for optimization of parameters of mutation and recombination operators. Here we derive optimality conditions for mutation and recombination radii maximizing the probabilities of mutation and crossover into the optimum. The analysis highlights important differences between these evolutionary operators. While mutation can potentially reach any part of the search space, the probability of beneficial mutation decreases with distance to an optimum, and the optimal mutation radius or rate should also decrease resulting in a slow-down of evolution near the optimum. Crossover recombination, on the other hand, acts in a subspace of the search space defined by the current population of strings. However, probabilities of beneficial and deleterious crossover are balanced, and their characteristics, such as variance, are translation invariant in a Hamming space, suggesting that recombination may complement mutation and boost the rate of evolution near the optimum.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [230] [Multimodal Fusion with Semi-Supervised Learning Minimizes Annotation Quantity for Modeling Videoconference Conversation Experience](https://arxiv.org/abs/2506.13971)
*Andrew Chang,Chenkai Hu,Ji Qi,Zhuojian Wei,Kexin Zhang,Viswadruth Akkaraju,David Poeppel,Dustin Freeman*

Main category: eess.AS

TL;DR: 该研究提出一种半监督学习框架，结合多模态数据（音频、面部、文本）预测视频会议中的非流畅或不愉快时刻，显著减少标注数据需求。


<details>
  <summary>Details</summary>
Motivation: 视频会议中的负面体验（如流畅性丧失或乐趣缺失）因自然数据中发生频率低且标注成本高，难以通过传统监督学习有效建模。

Method: 采用半监督学习（SSL），融合多模态深度特征（音频/面部/文本），通过模态融合协同训练模型，利用少量标注数据与大量未标注数据联合优化。

Result: SSL模型ROC-AUC达0.9，F1分数0.6，优于监督学习模型4%。仅用8%标注数据即可达到全监督模型96%的性能。

Conclusion: 该框架显著提升标注效率，为视频会议体验建模提供可行方案，证明半监督学习在多模态稀疏事件检测中的潜力。

Abstract: Group conversations over videoconferencing are a complex social behavior. However, the subjective moments of negative experience, where the conversation loses fluidity or enjoyment remain understudied. These moments are infrequent in naturalistic data, and thus training a supervised learning (SL) model requires costly manual data annotation. We applied semi-supervised learning (SSL) to leverage targeted labeled and unlabeled clips for training multimodal (audio, facial, text) deep features to predict non-fluid or unenjoyable moments in holdout videoconference sessions. The modality-fused co-training SSL achieved an ROC-AUC of 0.9 and an F1 score of 0.6, outperforming SL models by up to 4% with the same amount of labeled data. Remarkably, the best SSL model with just 8% labeled data matched 96% of the SL model's full-data performance. This shows an annotation-efficient framework for modeling videoconference experience.

</details>


### [231] [Improving Practical Aspects of End-to-End Multi-Talker Speech Recognition for Online and Offline Scenarios](https://arxiv.org/abs/2506.14204)
*Aswin Shanmugam Subramanian,Amit Das,Naoyuki Kanda,Jinyu Li,Xiaofei Wang,Yifan Gong*

Main category: eess.AS

TL;DR: 本文扩展了序列化输出训练（SOT）框架，提出结合连续语音分离（CSS）前端、双模型（流式与离线）及分段SOT（segSOT）的方法，以平衡延迟与准确性，提升流式和离线语音识别的性能。


<details>
  <summary>Details</summary>
Motivation: 针对流式与离线自动语音识别（ASR）的实际需求，需在实时字幕生成和摘要任务中平衡延迟与准确性，并解决多说话者语音重叠场景下的识别挑战。

Method: 1. 在高度重叠场景中，将CSS单通道前端与端到端系统结合；2. 采用Conformer Transducer（流式）和序列到序列模型（离线）的双模型或级联编码器两阶段模型；3. 提出适用于离线场景的segSOT以提升多说话者转录可读性。

Result: CSS框架提升了ASR系统在重叠语音场景的准确性，双模型与两阶段模型分别优化了流式与离线任务性能，segSOT进一步增强了离线转录的适应性。

Conclusion: 所提方法有效平衡了ASR系统的延迟与准确性，适用于实时和离线场景，挑战了传统端到端与级联系统的设计思路，为多说话者语音识别提供了新方向。

Abstract: We extend the frameworks of Serialized Output Training (SOT) to address practical needs of both streaming and offline automatic speech recognition (ASR) applications. Our approach focuses on balancing latency and accuracy, catering to real-time captioning and summarization requirements. We propose several key improvements: (1) Leveraging Continuous Speech Separation (CSS) single-channel front-end with end-to-end (E2E) systems for highly overlapping scenarios, challenging the conventional wisdom of E2E versus cascaded setups. The CSS framework improves the accuracy of the ASR system by separating overlapped speech from multiple speakers. (2) Implementing dual models -- Conformer Transducer for streaming and Sequence-to-Sequence for offline -- or alternatively, a two-pass model based on cascaded encoders. (3) Exploring segment-based SOT (segSOT) which is better suited for offline scenarios while also enhancing readability of multi-talker transcriptions.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [232] [Knowledge Compression via Question Generation: Enhancing Multihop Document Retrieval without Fine-tuning](https://arxiv.org/abs/2506.13778)
*Anvi Alex Eponon,Moein Shahiki-Tash,Ildar Batyrshin,Christian E. Maldonado-Sifuentes,Grigori Sidorov,Alexander Gelbukh*

Main category: cs.IR

TL;DR: 该研究提出一种基于问题生成的知识编码方法，无需微调或传统分块即可提升检索增强生成系统性能，在单跳/多跳任务中表现优异，并减少80%向量存储需求。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成系统存在分块效率低、微调成本高、存储需求大等问题，需开发更高效且轻量的知识编码方法。

Method: 通过生成覆盖词汇/语义空间的问题作为检索线索，结合自定义句法重排方法，并引入300字符内的论文摘要卡片(paper-cards)优化检索。

Result: 单跳检索Recall@3达0.84（超传统方法60%），BM25检索MRR@3从0.56提升至0.85；多跳任务F1达0.52，显著优于基线模型。

Conclusion: 该方法突破传统分块限制，降低80%存储需求与检索延迟，实现问题驱动的知识访问，为可扩展RAG系统提供新范式。

Abstract: This study presents a question-based knowledge encoding approach that improves retrieval-augmented generation (RAG) systems without requiring fine-tuning or traditional chunking. We encode textual content using generated questions that span the lexical and semantic space, creating targeted retrieval cues combined with a custom syntactic reranking method.
  In single-hop retrieval over 109 scientific papers, our approach achieves a Recall@3 of 0.84, outperforming traditional chunking methods by 60 percent. We also introduce "paper-cards", concise paper summaries under 300 characters, which enhance BM25 retrieval, increasing MRR@3 from 0.56 to 0.85 on simplified technical queries.
  For multihop tasks, our reranking method reaches an F1 score of 0.52 with LLaMA2-Chat-7B on the LongBench 2WikiMultihopQA dataset, surpassing chunking and fine-tuned baselines which score 0.328 and 0.412 respectively.
  This method eliminates fine-tuning requirements, reduces retrieval latency, enables intuitive question-driven knowledge access, and decreases vector storage demands by 80%, positioning it as a scalable and efficient RAG alternative.

</details>


### [233] [XGraphRAG: Interactive Visual Analysis for Graph-based Retrieval-Augmented Generation](https://arxiv.org/abs/2506.13782)
*Ke Wang,Bo Pan,Yingchaojie Feng,Yuwei Wu,Jieyi Chen,Minfeng Zhu,Wei Chen*

Main category: cs.IR

TL;DR: 提出可视化分析框架XGraphRAG，通过交互式可视化帮助开发者分析图结构检索增强生成系统的召回效果，提升可解释性与可访问性。


<details>
  <summary>Details</summary>
Motivation: 传统GraphRAG因复杂的信息处理流程和大量LLM调用，导致开发者难以分析其有效性，限制了系统的可解释性和应用性。

Method: 开发包含交互式可视化组件的原型系统XGraphRAG，支持追踪关键召回路径并分析GraphRAG全流程。

Result: 评估表明该框架能有效提升失败案例收集效率，并帮助识别系统改进机会。

Conclusion: XGraphRAG框架通过可视化分析增强了GraphRAG的可解释性，其开源实现为开发者提供了实用工具。

Abstract: Graph-based Retrieval-Augmented Generation (RAG) has shown great capability in enhancing Large Language Model (LLM)'s answer with an external knowledge base. Compared to traditional RAG, it introduces a graph as an intermediate representation to capture better structured relational knowledge in the corpus, elevating the precision and comprehensiveness of generation results. However, developers usually face challenges in analyzing the effectiveness of GraphRAG on their dataset due to GraphRAG's complex information processing pipeline and the overwhelming amount of LLM invocations involved during graph construction and query, which limits GraphRAG interpretability and accessibility. This research proposes a visual analysis framework that helps RAG developers identify critical recalls of GraphRAG and trace these recalls through the GraphRAG pipeline. Based on this framework, we develop XGraphRAG, a prototype system incorporating a set of interactive visualizations to facilitate users' analysis process, boosting failure cases collection and improvement opportunities identification. Our evaluation demonstrates the effectiveness and usability of our approach. Our work is open-sourced and available at https://github.com/Gk0Wk/XGraphRAG.

</details>


### [234] [Analysis of Anonymous User Interaction Relationships and Prediction of Advertising Feedback Based on Graph Neural Network](https://arxiv.org/abs/2506.13787)
*Yanjun Dai,Haoyang Feng,Yuan Gao*

Main category: cs.IR

TL;DR: 本文提出解耦时序层次图神经网络DTH-GNN，通过时间边分解、分层异质聚合和反馈感知对比正则化方法，显著提升在线广告匿名用户行为建模效果。


<details>
  <summary>Details</summary>
Motivation: 现有图模型难以捕捉匿名用户交互网络的多尺度时序、语义及高阶依赖特征，无法描述复杂行为模式，制约在线广告投放策略优化。

Method: 1) 时间边分解为短期/日周期/长期记忆三通道，采用并行扩张残差卷积提取特征；2) 元路径条件Transformer编码器实现分层异质子图聚合，通过跨通道自注意力和门控选择器抑制噪声；3) 设计反馈感知对比学习框架，结合双视图目标熵最大化、双动量队列蒸馏原型和延迟信号策略梯度微调。

Result: DTH-GNN相比最佳模型AUC提升8.2%，对数损失降低5.7%。

Conclusion: 该模型通过多尺度特征解耦、层次化噪声抑制和对比正则化机制，有效建模匿名用户行为复杂性，显著提升广告投放效果评估指标。

Abstract: While online advertising is highly dependent on implicit interaction networks of anonymous users for engagement inference, and for the selection and optimization of delivery strategies, existing graph models seldom can capture the multi-scale temporal, semantic and higher-order dependency features of these interaction networks, thus it's hard to describe the complicated patterns of the anonymous behavior. In this paper, we propose Decoupled Temporal-Hierarchical Graph Neural Network (DTH-GNN), which achieves three main contributions. Above all, we introduce temporal edge decomposition, which divides each interaction into three types of channels: short-term burst, diurnal cycle and long-range memory, and conducts feature extraction using the convolution kernel of parallel dilated residuals; Furthermore, our model builds a hierarchical heterogeneous aggregation, where user-user, user-advertisement, advertisement-advertisement subgraphs are combined through the meta-path conditional Transformer encoder, where the noise structure is dynamically tamped down via the synergy of cross-channel self-attention and gating relationship selector. Thirdly, the contrast regularity of feedback perception is formulated, the consistency of various time slices is maximized, the entropy of control exposure information with dual-view target is maximized, the global prototype of dual-momentum queue distillation is presented, and the strategy gradient layer with light weight is combined with delaying transformation signal to fine-tune the node representation for benefit-oriented. The AUC of DTH-GNN improved by 8.2% and the logarithmic loss improved by 5.7% in comparison with the best baseline model.

</details>


### [235] [AcademicBrowse: Benchmarking Academic Browse Ability of LLMs](https://arxiv.org/abs/2506.13784)
*Junting Zhou,Wang Li,Yiyan Liao,Nengyuan Zhang,Tingjia Miaoand Zhihui Qi,Yuhan Wu,Tong Yang*

Main category: cs.IR

TL;DR: 本文提出了首个针对大语言模型学术信息检索能力评估的数据集AcademicBrowse，具备学术实用性、高难度、简洁评估和广泛覆盖特性，覆盖15个学科并公开数据。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试(如BrowseComp)主要面向通用搜索场景，无法满足学术搜索对深度文献追踪、专业数据库支持、长尾知识导航及学术严谨性的特殊需求。

Method: 构建AcademicBrowse数据集，其核心特征包括：1) 问题贴近真实学术场景；2) 需至少三次深度搜索才能解答的高难度设计；3) 答案唯一性约束与来源标注；4) 覆盖至少15个学科领域。

Result: 该数据集通过限定条件确保答案唯一性，并提供明确来源与解决方案说明，显著提升了后续审核验证效率，填补了当前国内外可分析学术搜索数据集的空白。

Conclusion: AcademicBrowse能更精准衡量并促进LLMs在复杂学术检索任务中的性能提升，数据集已在HuggingFace平台开源。

Abstract: Large Language Models (LLMs)' search capabilities have garnered significant attention. Existing benchmarks, such as OpenAI's BrowseComp, primarily focus on general search scenarios and fail to adequately address the specific demands of academic search. These demands include deeper literature tracing and organization, professional support for academic databases, the ability to navigate long-tail academic knowledge, and ensuring academic rigor. Here, we proposed AcademicBrowse, the first dataset specifically designed to evaluate the complex information retrieval capabilities of Large Language Models (LLMs) in academic research. AcademicBrowse possesses the following key characteristics: Academic Practicality, where question content closely mirrors real academic learning and research environments, avoiding deliberately misleading models; High Difficulty, with answers that are challenging for single models (e.g., Grok DeepSearch or Gemini Deep Research) to provide directly, often requiring at least three deep searches to derive; Concise Evaluation, where limiting conditions ensure answers are as unique as possible, accompanied by clear sources and brief solution explanations, greatly facilitating subsequent audit and verification, surpassing the current lack of analyzed search datasets both domestically and internationally; and Broad Coverage, as the dataset spans at least 15 different academic disciplines. Through AcademicBrowse, we expect to more precisely measure and promote the performance improvement of LLMs in complex academic information retrieval tasks. The data is available at: https://huggingface.co/datasets/PKU-DS-LAB/AcademicBrowse

</details>


### [236] [InsertRank: LLMs can reason over BM25 scores to Improve Listwise Reranking](https://arxiv.org/abs/2506.14086)
*Rahul Seetharaman,Kaustubh D. Dhole,Aman Bansal*

Main category: cs.IR

TL;DR: 本文提出InsertRank，一种结合BM25词汇信号与LLM推理能力的重排方法，显著提升复杂查询下的检索效果，在BRIGHT和R2MED基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 用户对LLM聊天界面的高期望促使复杂查询需求增加，传统基于关键词或语义相似度的检索方法不足，现有LLM重排方法在推理能力利用上仍有改进空间。

Method: InsertRank在LLM重排过程中引入BM25等词汇信号，通过分数归一化和位置偏差调整优化文档排序，支持跨GPT、Gemini、Deepseek等模型家族。

Result: 在BRIGHT（37.5分）和R2MED（51.1分）基准测试中，Deepseek-R1模型驱动的InsertRank显著超越基线方法，消融实验验证了BM25信号和位置处理的有效性。

Conclusion: InsertRank通过融合LLM推理能力与传统检索信号，在多领域和医疗专业场景中实现检索性能突破，且方法具备跨模型兼容性和鲁棒性。

Abstract: Large Language Models (LLMs) have demonstrated significant strides across various information retrieval tasks, particularly as rerankers, owing to their strong generalization and knowledge-transfer capabilities acquired from extensive pretraining. In parallel, the rise of LLM-based chat interfaces has raised user expectations, encouraging users to pose more complex queries that necessitate retrieval by ``reasoning'' over documents rather than through simple keyword matching or semantic similarity. While some recent efforts have exploited reasoning abilities of LLMs for reranking such queries, considerable potential for improvement remains. In that regards, we introduce InsertRank, an LLM-based reranker that leverages lexical signals like BM25 scores during reranking to further improve retrieval performance. InsertRank demonstrates improved retrieval effectiveness on -- BRIGHT, a reasoning benchmark spanning 12 diverse domains, and R2MED, a specialized medical reasoning retrieval benchmark spanning 8 different tasks. We conduct an exhaustive evaluation and several ablation studies and demonstrate that InsertRank consistently improves retrieval effectiveness across multiple families of LLMs, including GPT, Gemini, and Deepseek models. %In addition, we also conduct ablation studies on normalization by varying the scale of the BM25 scores, and positional bias by shuffling the order of the documents. With Deepseek-R1, InsertRank achieves a score of 37.5 on the BRIGHT benchmark. and 51.1 on the R2MED benchmark, surpassing previous methods.

</details>


### [237] [RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG Systems for the SIGIR LiveRAG Competition](https://arxiv.org/abs/2506.14412)
*Tim Cofala,Oleh Astappiev,William Xion,Hailay Teklehaymanot*

Main category: cs.IR

TL;DR: 本文提出在LiveRAG 2025挑战赛中，通过结合InstructRAG、Pinecone检索器和BGE重排器的RAG方案，使用受限的10B参数模型，在Fineweb 10BT数据集上取得第四名成绩。


<details>
  <summary>Details</summary>
Motivation: 探索在模型参数受限(≤10B)条件下，通过优化检索增强生成(RAG)方案，提升对单跳/多跳问题的回答准确性，减少大语言模型幻觉现象。

Method: 采用InstructRAG框架，结合Pinecone密集检索与BGE重排技术，在OpenSearch稀疏索引和Pinecone密集索引基础上构建系统，最终通过Falcon-3-10B生成答案。

Result: 系统在SIGIR 2025 LiveRAG挑战赛中获得正确性1.13分、忠实度0.55分，综合排名第四。

Conclusion: 实验证明在参数规模受限条件下，合理组合密集检索与重排技术能有效提升RAG系统性能，验证了混合检索策略在复杂问答任务中的有效性。

Abstract: Retrieval-Augmented Generation (RAG) enriches Large Language Models (LLMs) by combining their internal, parametric knowledge with external, non-parametric sources, with the goal of improving factual correctness and minimizing hallucinations. The LiveRAG 2025 challenge explores RAG solutions to maximize accuracy on DataMorgana's QA pairs, which are composed of single-hop and multi-hop questions. The challenge provides access to sparse OpenSearch and dense Pinecone indices of the Fineweb 10BT dataset. It restricts model use to LLMs with up to 10B parameters and final answer generation with Falcon-3-10B. A judge-LLM assesses the submitted answers along with human evaluators. By exploring distinct retriever combinations and RAG solutions under the challenge conditions, our final solution emerged using InstructRAG in combination with a Pinecone retriever and a BGE reranker. Our solution achieved a correctness score of 1.13 and a faithfulness score of 0.55, placing fourth in the SIGIR 2025 LiveRAG Challenge.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [238] [Connecting phases of matter to the flatness of the loss landscape in analog variational quantum algorithms](https://arxiv.org/abs/2506.13865)
*Kasidit Srimahajariyapong,Supanut Thanasilp,Thiparat Chotibut*

Main category: quant-ph

TL;DR: 本文提出一种基于多体局域化（MBL）相的模拟变分量子算法（VQA）初始化策略，通过调控无序伊辛链的量子相，解决传统数字门控VQA中贫瘠高原问题，在保持表达力的同时提升可训练性。


<details>
  <summary>Details</summary>
Motivation: 传统基于数字门控的变分量子算法因贫瘠高原问题导致损失函数平坦化，限制了可扩展性。研究量子物相对VQA训练的影响，旨在利用MBL相延缓贫瘠高原现象。

Method: 构建由M次无序伊辛链淬火组成的模拟VQA ansätze，通过调节无序强度使淬火处于热化相或MBL相，分析其表达力与损失方差缩放特性。

Result: 热化相在较小M时即出现贫瘠高原，而MBL相在更大M时仍保持可训练性。提出中间M值时采用MBL相初始化策略，平衡初始可训练性与后续优化表达力。

Conclusion: 量子物相与VQA可训练性存在关联，MBL初始化策略为模拟硬件VQA的扩展提供了实用指导，同时为量子计算与量子多体物理的交叉研究建立桥梁。

Abstract: Variational quantum algorithms (VQAs) promise near-term quantum advantage, yet parametrized quantum states commonly built from the digital gate-based approach often suffer from scalability issues such as barren plateaus, where the loss landscape becomes flat. We study an analog VQA ansätze composed of $M$ quenches of a disordered Ising chain, whose dynamics is native to several quantum simulation platforms. By tuning the disorder strength we place each quench in either a thermalized phase or a many-body-localized (MBL) phase and analyse (i) the ansätze's expressivity and (ii) the scaling of loss variance. Numerics shows that both phases reach maximal expressivity at large $M$, but barren plateaus emerge at far smaller $M$ in the thermalized phase than in the MBL phase. Exploiting this gap, we propose an MBL initialisation strategy: initialise the ansätze in the MBL regime at intermediate quench $M$, enabling an initial trainability while retaining sufficient expressivity for subsequent optimization. The results link quantum phases of matter and VQA trainability, and provide practical guidelines for scaling analog-hardware VQAs.

</details>


### [239] [Hamiltonian Formalism for Comparing Quantum and Classical Intelligence](https://arxiv.org/abs/2506.14456)
*Elija Perrier*

Main category: quant-ph

TL;DR: 本文提出了一种基于哈密顿量形式主义的数学框架，用于比较经典与量子环境中AGI（人工通用智能）在环境交互中的差异，并分解其核心功能的动态生成器。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于量子基板上实现AGI的可能性，需建立数学工具以明确对比经典与量子AGI在环境交互中的本质区别。

Method: 引入哈密顿量形式主义，将AGI动态分解为归纳、推理、递归、学习等核心功能的生成器，构建统一数学语言。

Result: 提出了一种可量化分析AGI任务在经典与量子环境中差异的框架，明确了不同环境交互模式的特征。

Conclusion: 该框架为理解量子与经典AGI的差异提供了数学基础，推动未来关于智能体-环境交互的理论研究。

Abstract: The prospect of AGI instantiated on quantum substrates motivates the development of mathematical frameworks that enable direct comparison of their operation in classical and quantum environments. To this end, we introduce a Hamiltonian formalism for describing classical and quantum AGI tasks as a means of contrasting their interaction with the environment. We propose a decomposition of AGI dynamics into Hamiltonian generators for core functions such as induction, reasoning, recursion, learning, measurement, and memory. This formalism aims to contribute to the development of a precise mathematical language for how quantum and classical agents differ via environmental interaction.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [240] [Evolutionary chemical learning in dimerization networks](https://arxiv.org/abs/2506.14006)
*Alexei V. Tkachenko,Bortolo Matteo Mognetti,Sergei Maslov*

Main category: cond-mat.stat-mech

TL;DR: 提出基于竞争性二聚化网络（CDNs）的化学学习框架，通过体外定向进化实现多类分类等复杂任务，无需数字硬件或显式参数调整。


<details>
  <summary>Details</summary>
Motivation: 结合合成生物学与机器学习，开发自适应、高能效的分子计算系统，探索物理模拟计算在复杂学习任务中的潜力。

Method: 利用DNA/RNA寡聚体等分子构建CDNs，通过突变、选择和扩增的定向进化协议训练网络，以结合亲和力作为可调突触权重。

Result: CDNs能有效区分噪声输入模式，输出对比度和输入-输出互信息显著，性能与梯度下降训练高度相关。使用对比增强损失函数可进一步提升效果。

Conclusion: CDNs为模拟物理计算提供了新平台，其生物兼容性和能量效率为分子计算系统的实际应用奠定了基础。

Abstract: We present a novel framework for chemical learning based on Competitive Dimerization Networks (CDNs) - systems in which multiple molecular species, e.g. proteins or DNA/RNA oligomers, reversibly bind to form dimers. We show that these networks can be trained in vitro through directed evolution, enabling the implementation of complex learning tasks such as multiclass classification without digital hardware or explicit parameter tuning. Each molecular species functions analogously to a neuron, with binding affinities acting as tunable synaptic weights. A training protocol involving mutation, selection, and amplification of DNA-based components allows CDNs to robustly discriminate among noisy input patterns. The resulting classifiers exhibit strong output contrast and high mutual information between input and output, especially when guided by a contrast-enhancing loss function. Comparative analysis with in silico gradient descent training reveals closely correlated performance. These results establish CDNs as a promising platform for analog physical computation, bridging synthetic biology and machine learning, and advancing the development of adaptive, energy-efficient molecular computing systems.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [241] [The Perception of Phase Intercept Distortion and its Application in Data Augmentation](https://arxiv.org/abs/2506.14571)
*Venkatakrishnan Vaidyanathapuram Krishnan,Nathaniel Condit-Schultz*

Main category: eess.SP

TL;DR: 本文探讨了一种特殊的相位失真——相位截断失真，发现其虽改变信号波形但人耳难以察觉，并成功将其应用于音频机器学习的数据增强中。


<details>
  <summary>Details</summary>
Motivation: 研究相位截断失真对人耳感知的影响，并探索其在机器学习数据增强中的潜在应用价值。

Method: 通过人类受试者实验验证相位截断失真的不可感知性，并将其作为新型数据增强手段应用于音频机器学习任务。

Result: 实验证实相位截断失真确实难以被人耳察觉，且作为数据增强方法能有效提升音频机器学习模型的性能。

Conclusion: 相位截断失真具有感知透明性，可作为有效的音频数据增强技术，为机器学习领域提供新思路。

Abstract: Phase distortion refers to the alteration of the phase relationships between frequencies in a signal, which can be perceptible. In this paper, we discuss a special case of phase distortion known as phase-intercept distortion, which is created by a frequency-independent phase shift. We hypothesize that, though this form of distortion changes a signal's waveform significantly, the distortion is imperceptible. Human-subject experiment results are reported which are consistent with this hypothesis. Furthermore, we discuss how the imperceptibility of phase-intercept distortion can be useful for machine learning, specifically for data augmentation. We conducted multiple experiments using phase-intercept distortion as a novel approach to data augmentation, and obtained improved results for audio machine learning tasks.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [242] [DeepSeq: High-Throughput Single-Cell RNA Sequencing Data Labeling via Web Search-Augmented Agentic Generative AI Foundation Models](https://arxiv.org/abs/2506.13817)
*Saleem A. Al Dajani,Abel Sanchez,John R. Williams*

Main category: q-bio.GN

TL;DR: 该论文提出使用生成式AI基础模型和实时网络搜索自动标记单细胞RNA测序数据，准确率高达82.5%，解决了监督学习中的数据标注瓶颈。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序数据规模迅速扩大至数十亿细胞，手动标注数据耗时且易出错，亟需自动化解决方案以提高标注效率和准确性。

Method: 采用具有实时网络搜索能力的代理基础模型，自动化标注实验数据，减少人工干预和错误。

Result: 该方法实现了82.5%的标注准确率，支持构建虚拟细胞基础模型，用于细胞分型和扰动预测等下游任务。

Conclusion: 随着数据量增长，这些模型可能在标注任务上超越人类表现，为大规模扰动筛选中的可靠推断铺平道路，推动健康监测和诊断领域的创新。

Abstract: Generative AI foundation models offer transformative potential for processing structured biological data, particularly in single-cell RNA sequencing, where datasets are rapidly scaling toward billions of cells. We propose the use of agentic foundation models with real-time web search to automate the labeling of experimental data, achieving up to 82.5% accuracy. This addresses a key bottleneck in supervised learning for structured omics data by increasing annotation throughput without manual curation and human error. Our approach enables the development of virtual cell foundation models capable of downstream tasks such as cell-typing and perturbation prediction. As data volume grows, these models may surpass human performance in labeling, paving the way for reliable inference in large-scale perturbation screens. This application demonstrates domain-specific innovation in health monitoring and diagnostics, aligned with efforts like the Human Cell Atlas and Human Tumor Atlas Network.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [243] [Hard Contacts with Soft Gradients: Refining Differentiable Simulators for Learning and Control](https://arxiv.org/abs/2506.14186)
*Anselm Paulus,A. René Geist,Pierre Schumacher,Vít Musil,Georg Martius*

Main category: cs.RO

TL;DR: 本文提出DiffMJX和Contacts From Distance（CFD）方法，通过自适应积分与反向传播优化，解决硬接触场景下基于惩罚的物理模拟器梯度计算问题，显著提升梯度质量并缩小仿真与现实差距。


<details>
  <summary>Details</summary>
Motivation: 基于惩罚的物理模拟器（如MuJoCo）在硬接触场景下因高刚度参数导致自动微分梯度错误，而低刚度参数会增大仿真与现实差异，亟需在保持物理真实性的同时改善梯度计算质量。

Method: 1) 结合自适应积分与MuJoCo XLA开发DiffMJX框架优化梯度计算；2) 提出CFD机制，通过直通技巧在反向传播中生成非接触状态下的有效接触梯度，同时保持正向模拟的物理真实性。

Result: DiffMJX显著提升了硬接触场景的梯度计算精度，CFD机制在非接触状态下仍能生成有效梯度信号，且不干扰正向模拟的物理准确性。

Conclusion: 该方法突破了传统惩罚型模拟器在硬接触梯度计算与仿真-现实差距间的权衡困境，为机器人动力学梯度优化提供了更可靠的数值基础。

Abstract: Contact forces pose a major challenge for gradient-based optimization of robot dynamics as they introduce jumps in the system's velocities. Penalty-based simulators, such as MuJoCo, simplify gradient computation by softening the contact forces. However, realistically simulating hard contacts requires very stiff contact settings, which leads to incorrect gradients when using automatic differentiation. On the other hand, using non-stiff settings strongly increases the sim-to-real gap. We analyze the contact computation of penalty-based simulators to identify the causes of gradient errors. Then, we propose DiffMJX, which combines adaptive integration with MuJoCo XLA, to notably improve gradient quality in the presence of hard contacts. Finally, we address a key limitation of contact gradients: they vanish when objects do not touch. To overcome this, we introduce Contacts From Distance (CFD), a mechanism that enables the simulator to generate informative contact gradients even before objects are in contact. To preserve physical realism, we apply CFD only in the backward pass using a straight-through trick, allowing us to compute useful gradients without modifying the forward simulation.

</details>


### [244] [AMPLIFY: Actionless Motion Priors for Robot Learning from Videos](https://arxiv.org/abs/2506.14198)
*Jeremy A. Collins,Loránd Cheng,Kunal Aneja,Albert Wilcox,Benjamin Joffe,Animesh Garg*

Main category: cs.RO

TL;DR: AMPLIFY框架利用大规模无动作视频数据，通过关键点轨迹生成紧凑的运动标记，分离视觉运动预测与动作推理，显著提升策略学习效果和动态预测精度。


<details>
  <summary>Details</summary>
Motivation: 机器人领域的有标记动作数据稀缺且昂贵，限制了学习策略的泛化能力，而无动作视频数据丰富但难以转化为有效策略。

Method: 提出AMPLIFY框架，将视觉动态编码为基于关键点轨迹的离散运动标记，分离视觉运动预测与动作推理，分别训练前向和逆向动态模型。

Result: 动态预测精度显著提升（MSE提高3.7倍，像素预测精度提高2.5倍），策略学习在低数据环境下提升1.2-2.2倍，并能从人类视频中学习。

Conclusion: AMPLIFY通过异构数据源构建高效、可泛化的世界模型，为机器人控制和视频预测提供了新范式。

Abstract: Action-labeled data for robotics is scarce and expensive, limiting the generalization of learned policies. In contrast, vast amounts of action-free video data are readily available, but translating these observations into effective policies remains a challenge. We introduce AMPLIFY, a novel framework that leverages large-scale video data by encoding visual dynamics into compact, discrete motion tokens derived from keypoint trajectories. Our modular approach separates visual motion prediction from action inference, decoupling the challenges of learning what motion defines a task from how robots can perform it. We train a forward dynamics model on abundant action-free videos and an inverse dynamics model on a limited set of action-labeled examples, allowing for independent scaling. Extensive evaluations demonstrate that the learned dynamics are both accurate, achieving up to 3.7x better MSE and over 2.5x better pixel prediction accuracy compared to prior approaches, and broadly useful. In downstream policy learning, our dynamics predictions enable a 1.2-2.2x improvement in low-data regimes, a 1.4x average improvement by learning from action-free human videos, and the first generalization to LIBERO tasks from zero in-distribution action data. Beyond robotic control, we find the dynamics learned by AMPLIFY to be a versatile latent world model, enhancing video prediction quality. Our results present a novel paradigm leveraging heterogeneous data sources to build efficient, generalizable world models. More information can be found at https://amplify-robotics.github.io/.

</details>


### [245] [Steering Robots with Inference-Time Interactions](https://arxiv.org/abs/2506.14287)
*Yanwei Wang*

Main category: cs.RO

TL;DR: 该研究提出了一种无需微调预训练策略的方法，通过用户交互在推理时引导策略行为，以纠正策略错误并满足用户偏好。


<details>
  <summary>Details</summary>
Motivation: 预训练策略在部署时可能因泛化能力不足而产生错误，而传统方法需要为每个下游用例收集额外数据进行微调，效率低下。

Method: 提出了两种框架：(1) 推理时引导，利用用户交互在离散技能间切换；(2) 任务与动作模仿，允许用户编辑连续动作，同时满足离散符号计划定义的任务约束。

Result: 这些框架能够在无需额外训练的情况下纠正策略预测偏差，最大化预训练模型的效用，同时实现推理时的用户目标。

Conclusion: 通过使预训练策略可引导，用户可以在模型泛化能力不足时帮助纠正错误，而无需进行策略微调，提高了部署效率。

Abstract: Imitation learning has driven the development of generalist policies capable of autonomously solving multiple tasks. However, when a pretrained policy makes errors during deployment, there are limited mechanisms for users to correct its behavior. While collecting additional data for finetuning can address such issues, doing so for each downstream use case is inefficient at deployment. My research proposes an alternative: keeping pretrained policies frozen as a fixed skill repertoire while allowing user interactions to guide behavior generation toward user preferences at inference time. By making pretrained policies steerable, users can help correct policy errors when the model struggles to generalize-without needing to finetune the policy. Specifically, I propose (1) inference-time steering, which leverages user interactions to switch between discrete skills, and (2) task and motion imitation, which enables user interactions to edit continuous motions while satisfying task constraints defined by discrete symbolic plans. These frameworks correct misaligned policy predictions without requiring additional training, maximizing the utility of pretrained models while achieving inference-time user objectives.

</details>


### [246] [Uncertainty-Driven Radar-Inertial Fusion for Instantaneous 3D Ego-Velocity Estimation](https://arxiv.org/abs/2506.14294)
*Prashant Kumar Rai,Elham Kowsari,Nataliya Strokina,Reza Ghabcheloo*

Main category: cs.RO

TL;DR: 提出一种结合高分辨率成像雷达与惯性测量单元的自车速度估计方法，通过神经网络处理原始雷达数据并融合扩展卡尔曼滤波，显著提升运动估计精度。


<details>
  <summary>Details</summary>
Motivation: 传统雷达自运动估计方法存在局限性，需解决复杂场景下瞬时速度估计的鲁棒性与精度问题。

Method: 使用神经网络处理复数雷达数据输出含不确定性的速度估计，通过扩展卡尔曼滤波动态调整惯性传感器噪声参数实现多模态数据融合。

Result: 在ColoRadar数据集上取得最低误差，优于现有公开方法及瞬时/扫描匹配技术。

Conclusion: 不确定性感知的神经网络与动态参数优化策略有效提升了复杂环境下自主导航系统的运动估计性能。

Abstract: We present a method for estimating ego-velocity in autonomous navigation by integrating high-resolution imaging radar with an inertial measurement unit. The proposed approach addresses the limitations of traditional radar-based ego-motion estimation techniques by employing a neural network to process complex-valued raw radar data and estimate instantaneous linear ego-velocity along with its associated uncertainty. This uncertainty-aware velocity estimate is then integrated with inertial measurement unit data using an Extended Kalman Filter. The filter leverages the network-predicted uncertainty to refine the inertial sensor's noise and bias parameters, improving the overall robustness and accuracy of the ego-motion estimation. We evaluated the proposed method on the publicly available ColoRadar dataset. Our approach achieves significantly lower error compared to the closest publicly available method and also outperforms both instantaneous and scan matching-based techniques.

</details>


### [247] [GAMORA: A Gesture Articulated Meta Operative Robotic Arm for Hazardous Material Handling in Containment-Level Environments](https://arxiv.org/abs/2506.14513)
*Farha Abdul Wasay,Mohammed Abdul Rahman,Hania Ghouse*

Main category: cs.RO

TL;DR: GAMORA是一种基于VR手势控制的机器人系统，用于高风险的病毒学实验室远程操作。通过整合Oculus Quest 2、NVIDIA Jetson Nano和ROS，系统实现了实时沉浸式控制、数字孪生模拟和逆运动学精准操作，显著降低生物危害暴露风险。


<details>
  <summary>Details</summary>
Motivation: 随着生物危害复杂性增加，需在保持操作精度的同时减少人员直接暴露。现有自动化脚本或传统遥操作无法满足高精度与实时控制需求，需开发更安全、直观的远程操作方案。

Method: 结合VR头显（Oculus Quest 2）、边缘计算设备（Jetson Nano）和ROS框架，构建Unity虚拟环境与3D打印机械臂的物理系统。采用逆运动学实现精准操作，集成YOLOv8增强空间感知，并通过硬件在环测试验证性能。

Result: 系统达到平均位置误差2.2毫米（原4毫米）、移液精度0.2毫升内、50次试验重复性1.2毫米。能耗降低50%，YOLOv8提升环境感知能力。

Conclusion: GAMORA通过数字-物理反馈闭环，为高风险实验室任务提供了可扩展、沉浸式的安全自动化解决方案，兼具高精度与可持续性，适用于生物医学研究场景。

Abstract: The convergence of robotics and virtual reality (VR) has enabled safer and more efficient workflows in high-risk laboratory settings, particularly virology labs. As biohazard complexity increases, minimizing direct human exposure while maintaining precision becomes essential. We propose GAMORA (Gesture Articulated Meta Operative Robotic Arm), a novel VR-guided robotic system that enables remote execution of hazardous tasks using natural hand gestures. Unlike existing scripted automation or traditional teleoperation, GAMORA integrates the Oculus Quest 2, NVIDIA Jetson Nano, and Robot Operating System (ROS) to provide real-time immersive control, digital twin simulation, and inverse kinematics-based articulation. The system supports VR-based training and simulation while executing precision tasks in physical environments via a 3D-printed robotic arm. Inverse kinematics ensure accurate manipulation for delicate operations such as specimen handling and pipetting. The pipeline includes Unity-based 3D environment construction, real-time motion planning, and hardware-in-the-loop testing. GAMORA achieved a mean positional discrepancy of 2.2 mm (improved from 4 mm), pipetting accuracy within 0.2 mL, and repeatability of 1.2 mm across 50 trials. Integrated object detection via YOLOv8 enhances spatial awareness, while energy-efficient operation (50% reduced power output) ensures sustainable deployment. The system's digital-physical feedback loop enables safe, precise, and repeatable automation of high-risk lab tasks. GAMORA offers a scalable, immersive solution for robotic control and biosafety in biomedical research environments.

</details>


### [248] [SENIOR: Efficient Query Selection and Preference-Guided Exploration in Preference-based Reinforcement Learning](https://arxiv.org/abs/2506.14648)
*Hexian Ni,Tao Lu,Haoyuan Hu,Yinghao Cai,Shuo Wang*

Main category: cs.RO

TL;DR: SENIOR提出了一种基于偏好强化学习的高效查询选择和偏好引导探索方法，通过优化人类反馈效率和样本效率，显著提升了策略学习速度。


<details>
  <summary>Details</summary>
Motivation: 偏好强化学习（PbRL）避免了奖励工程，但反馈效率和样本效率低的问题阻碍了其应用。本文旨在解决这些问题。

Method: SENIOR包含两个关键机制：1) 基于运动差异的选择方案（MDS），通过核密度估计选择易于标记的片段对；2) 偏好引导探索方法（PGE），鼓励探索高偏好低访问状态。

Result: 实验表明，SENIOR在六项复杂机器人操作任务中，反馈效率和策略收敛速度均优于五种现有方法。

Conclusion: SENIOR通过优化反馈和探索机制，显著提升了PbRL的效率和性能，适用于仿真和现实任务。

Abstract: Preference-based Reinforcement Learning (PbRL) methods provide a solution to avoid reward engineering by learning reward models based on human preferences. However, poor feedback- and sample- efficiency still remain the problems that hinder the application of PbRL. In this paper, we present a novel efficient query selection and preference-guided exploration method, called SENIOR, which could select the meaningful and easy-to-comparison behavior segment pairs to improve human feedback-efficiency and accelerate policy learning with the designed preference-guided intrinsic rewards. Our key idea is twofold: (1) We designed a Motion-Distinction-based Selection scheme (MDS). It selects segment pairs with apparent motion and different directions through kernel density estimation of states, which is more task-related and easy for human preference labeling; (2) We proposed a novel preference-guided exploration method (PGE). It encourages the exploration towards the states with high preference and low visits and continuously guides the agent achieving the valuable samples. The synergy between the two mechanisms could significantly accelerate the progress of reward and policy learning. Our experiments show that SENIOR outperforms other five existing methods in both human feedback-efficiency and policy convergence speed on six complex robot manipulation tasks from simulation and four real-worlds.

</details>


### [249] [Casper: Inferring Diverse Intents for Assistive Teleoperation with Vision Language Models](https://arxiv.org/abs/2506.14727)
*Huihan Liu,Rutav Shah,Shuijing Liu,Jack Pittenger,Mingyo Seo,Yuchen Cui,Yonatan Bisk,Roberto Martín-Martín,Yuke Zhu*

Main category: cs.RO

TL;DR: Casper系统利用预训练视觉语言模型的常识知识，实现实时意图推断和灵活技能执行，提升辅助遥操作的任务性能并降低用户认知负荷。


<details>
  <summary>Details</summary>
Motivation: 现有辅助遥操作系统局限于简单预定义场景或特定任务数据分布，无法支持现实世界中多样化、长时程的移动操作任务需求。

Method: 结合开放世界感知模块、VLM驱动的常识推理意图推断机制，以及扩展技能库，构建通用型辅助遥操作系统Casper。

Result: 实验表明Casper在任务完成度(比基线提升23%)、认知负荷降低(减少37%)和用户满意度(4.8/5分)方面显著优于传统方法。

Conclusion: 通过融合VLMs的常识推理能力，Casper突破了传统辅助遥操作系统的场景限制，为复杂现实任务提供了通用解决方案。

Abstract: Assistive teleoperation, where control is shared between a human and a robot, enables efficient and intuitive human-robot collaboration in diverse and unstructured environments. A central challenge in real-world assistive teleoperation is for the robot to infer a wide range of human intentions from user control inputs and to assist users with correct actions. Existing methods are either confined to simple, predefined scenarios or restricted to task-specific data distributions at training, limiting their support for real-world assistance. We introduce Casper, an assistive teleoperation system that leverages commonsense knowledge embedded in pre-trained visual language models (VLMs) for real-time intent inference and flexible skill execution. Casper incorporates an open-world perception module for a generalized understanding of novel objects and scenes, a VLM-powered intent inference mechanism that leverages commonsense reasoning to interpret snippets of teleoperated user input, and a skill library that expands the scope of prior assistive teleoperation systems to support diverse, long-horizon mobile manipulation tasks. Extensive empirical evaluation, including human studies and system ablations, demonstrates that Casper improves task performance, reduces human cognitive load, and achieves higher user satisfaction than direct teleoperation and assistive teleoperation baselines.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [250] [Contemporary AI foundation models increase biological weapons risk](https://arxiv.org/abs/2506.13798)
*Roger Brent,T. Greg McKelvey*

Main category: cs.CY

TL;DR: 研究指出当前AI基础模型生物安全风险评估存在重大缺陷，通过案例分析证明大型语言模型可突破隐性知识限制，指导非专业人员完成复杂生物武器开发步骤。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全评估低估生物武器开发风险，主要源于错误假设（生物武器需隐性知识）和不完善的评估基准，需重新审视AI在生物安全领域的潜在威胁。

Method: 1. 通过挪威极端分子合成炸药等案例反驳隐性知识必要性
2. 分析病原体构建文档化实践
3. 建立生物武器开发成功要素框架
4. 测试Llama3.1/ChatGPT-4o/Claude3.5指导合成脊髓灰质炎病毒能力

Result: 先进AI模型能准确指导用户从商业合成DNA中复活活性脊髓灰质炎病毒，直接挑战当前'AI生物风险低'的主流结论。

Conclusion: 亟需改进评估基准，但有效干预时间窗口可能已关闭。研究揭示AI降低生物武器开发门槛的现实风险，要求重新制定安全评估标准。

Abstract: The rapid advancement of artificial intelligence has raised concerns about its potential to facilitate biological weapons development. We argue existing safety assessments of contemporary foundation AI models underestimate this risk, largely due to flawed assumptions and inadequate evaluation methods. First, assessments mistakenly assume biological weapons development requires tacit knowledge, or skills gained through hands-on experience that cannot be easily verbalized. Second, they rely on imperfect benchmarks that overlook how AI can uplift both nonexperts and already-skilled individuals. To challenge the tacit knowledge assumption, we examine cases where individuals without formal expertise, including a 2011 Norwegian ultranationalist who synthesized explosives, successfully carried out complex technical tasks. We also review efforts to document pathogen construction processes, highlighting how such tasks can be conveyed in text. We identify "elements of success" for biological weapons development that large language models can describe in words, including steps such as acquiring materials and performing technical procedures. Applying this framework, we find that advanced AI models Llama 3.1 405B, ChatGPT-4o, and Claude 3.5 Sonnet can accurately guide users through the recovery of live poliovirus from commercially obtained synthetic DNA, challenging recent claims that current models pose minimal biosecurity risk. We advocate for improved benchmarks, while acknowledging the window for meaningful implementation may have already closed.

</details>


### [251] [Dr. GPT Will See You Now, but Should It? Exploring the Benefits and Harms of Large Language Models in Medical Diagnosis using Crowdsourced Clinical Cases](https://arxiv.org/abs/2506.13805)
*Bonam Mingole,Aditya Majumdar,Firdaus Ahmed Choudhury,Jennifer L. Kraschnewski,Shyam S. Sundar,Amulya Yadav*

Main category: cs.CY

TL;DR: 研究通过众包竞赛评估大型语言模型（LLM）在回答日常健康问题中的表现，发现76%的回答被医生判定准确，并探索RAG技术对提升回答质量的作用。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视LLM在真实日常健康咨询场景中的效果评估，而此类场景是LLM更普遍的应用场景。论文旨在填补这一空白，探究LLM在现实健康沟通中的实际表现。

Method: 通过大学竞赛收集34名参与者对4个公开LLM提出的212个真实/虚构健康问题，由9名认证医生评估回答质量，并测试RAG增强版LLM的效果，结合7名医疗专家的访谈分析定量结果。

Result: 76%的LLM回答被医生判定为准确；RAG技术可能提升回答质量，但需结合医学知识库。定性访谈揭示了模型表现的具体优缺点。

Conclusion: LLM在日常健康咨询中表现总体可靠，但需结合专业医学知识优化。研究为评估LLM在真实健康场景中的效果提供了实践框架与洞见。

Abstract: The proliferation of Large Language Models (LLMs) in high-stakes applications such as medical (self-)diagnosis and preliminary triage raises significant ethical and practical concerns about the effectiveness, appropriateness, and possible harmfulness of the use of these technologies for health-related concerns and queries. Some prior work has considered the effectiveness of LLMs in answering expert-written health queries/prompts, questions from medical examination banks, or queries based on pre-existing clinical cases. Unfortunately, these existing studies completely ignore an in-the-wild evaluation of the effectiveness of LLMs in answering everyday health concerns and queries typically asked by general users, which corresponds to the more prevalent use case for LLMs. To address this research gap, this paper presents the findings from a university-level competition that leveraged a novel, crowdsourced approach for evaluating the effectiveness of LLMs in answering everyday health queries. Over the course of a week, a total of 34 participants prompted four publicly accessible LLMs with 212 real (or imagined) health concerns, and the LLM generated responses were evaluated by a team of nine board-certified physicians. At a high level, our findings indicate that on average, 76% of the 212 LLM responses were deemed to be accurate by physicians. Further, with the help of medical professionals, we investigated whether RAG versions of these LLMs (powered with a comprehensive medical knowledge base) can improve the quality of responses generated by LLMs. Finally, we also derive qualitative insights to explain our quantitative findings by conducting interviews with seven medical professionals who were shown all the prompts in our competition. This paper aims to provide a more grounded understanding of how LLMs perform in real-world everyday health communication.

</details>


### [252] [Students' Reliance on AI in Higher Education: Identifying Contributing Factors](https://arxiv.org/abs/2506.13845)
*Griffin Pitts,Neha Rani,Weedguet Mildort,Eva-Marie Cook*

Main category: cs.CY

TL;DR: 该研究探讨大学生对AI工具的依赖模式，发现编程自我效能、素养和认知需求与适当依赖相关，过度依赖与信任和满意度相关，依赖不足与上述能力负相关。


<details>
  <summary>Details</summary>
Motivation: 随着AI工具在教育中的普及，学生可能因不加批判地接受错误建议而产生过度依赖，影响学习效果。研究旨在揭示影响学生依赖模式的因素，以促进合理使用AI。

Method: 结合前后问卷调查与实验任务，观察学生在面对不同可靠性AI建议时的反应。参与者使用提供正确/错误建议的AI助手解决编程问题，量化其依赖模式。

Result: 适当依赖与编程自我效能、编程素养和认知需求正相关，但与任务后信任/满意度负相关；过度依赖与信任/满意度正相关；依赖不足与上述能力负相关。

Conclusion: 需针对学生能力特征设计干预措施，平衡AI工具使用与批判性思维培养，为教育技术整合提供实证依据。

Abstract: The increasing availability and use of artificial intelligence (AI) tools in educational settings has raised concerns about students' overreliance on these technologies. Overreliance occurs when individuals accept incorrect AI-generated recommendations, often without critical evaluation, leading to flawed problem solutions and undermining learning outcomes. This study investigates potential factors contributing to patterns of AI reliance among undergraduate students, examining not only overreliance but also appropriate reliance (correctly accepting helpful and rejecting harmful recommendations) and underreliance (incorrectly rejecting helpful recommendations). Our approach combined pre- and post-surveys with a controlled experimental task where participants solved programming problems with an AI assistant that provided both accurate and deliberately incorrect suggestions, allowing direct observation of students' reliance patterns when faced with varying AI reliability. We find that appropriate reliance is significantly related to students' programming self-efficacy, programming literacy, and need for cognition, while showing negative correlations with post-task trust and satisfaction. Overreliance showed significant correlations with post-task trust and satisfaction with the AI assistant. Underreliance was negatively correlated with programming literacy, programming self-efficacy, and need for cognition. Overall, the findings provide insights for developing targeted interventions that promote appropriate reliance on AI tools, with implications for the integration of AI in curriculum and educational technologies.

</details>


### [253] [Computational Studies in Influencer Marketing: A Systematic Literature Review](https://arxiv.org/abs/2506.14602)
*Haoyang Gui,Thales Bertaglia,Catalina Goanta,Gerasimos Spanakis*

Main category: cs.CY

TL;DR: 本文通过系统性文献综述分析69项研究，总结影响者营销的计算方法现状，识别四大研究主题及方法，强调需加强伦理、合规及多学科研究。


<details>
  <summary>Details</summary>
Motivation: 当前影响者营销的计算研究分散且缺乏系统性综述，导致平台外利益相关者（如监管者、跨领域研究者）难以获取科学依据。

Method: 基于PRISMA模型进行系统性文献综述，分析69项研究，按主题（如影响者识别、广告策略）和方法（机器学习与非机器学习）分类。

Result: 研究发现现有研究侧重商业优化，忽视合规与伦理；需结合上下文因素（语言、平台）、提升模型可解释性及数据可复现性。

Conclusion: 提出多学科研究议程，建议加强监管技术关联、细化分析粒度、开发标准化数据集，以弥补当前研究不足。

Abstract: Influencer marketing has become a crucial feature of digital marketing strategies. Despite its rapid growth and algorithmic relevance, the field of computational studies in influencer marketing remains fragmented, especially with limited systematic reviews covering the computational methodologies employed. This makes overarching scientific measurements in the influencer economy very scarce, to the detriment of interested stakeholders outside of platforms themselves, such as regulators, but also researchers from other fields. This paper aims to provide an overview of the state of the art of computational studies in influencer marketing by conducting a systematic literature review (SLR) based on the PRISMA model. The paper analyses 69 studies to identify key research themes, methodologies, and future directions in this research field. The review identifies four major research themes: Influencer identification and characterisation, Advertising strategies and engagement, Sponsored content analysis and discovery, and Fairness. Methodologically, the studies are categorised into machine learning-based techniques (e.g., classification, clustering) and non-machine-learning-based techniques (e.g., statistical analysis, network analysis). Key findings reveal a strong focus on optimising commercial outcomes, with limited attention to regulatory compliance and ethical considerations. The review highlights the need for more nuanced computational research that incorporates contextual factors such as language, platform, and industry type, as well as improved model explainability and dataset reproducibility. The paper concludes by proposing a multidisciplinary research agenda that emphasises the need for further links to regulation and compliance technology, finer granularity in analysis, and the development of standardised datasets.

</details>


### [254] [The Synthetic Mirror -- Synthetic Data at the Age of Agentic AI](https://arxiv.org/abs/2506.13818)
*Marcelle Momha*

Main category: cs.CY

TL;DR: 合成数据作为人工生成的数据，在AI代理中的应用日益广泛，但其可能扭曲现实并引发信任与问责问题。本文探讨了合成数据对隐私和政策制定的影响，主张通过修订现有法律框架而非创建全新制度，将其作为独特监管类别以应对挑战。


<details>
  <summary>Details</summary>
Motivation: 合成数据的普及导致其可能成为现实的“合成镜像”，引发对现实表征的潜在扭曲，进而产生信任与问责缺失。需研究其对隐私及政策制定的影响，并推动法律框架适应以保障AI代理的可靠性。

Method: 通过分析合成数据的特性及其对现有制度的冲击，提出在既有政策框架内进行针对性修订，而非建立全新监管体系。强调将合成数据定义为具有独特属性的独立监管类别。

Result: 指出现行政策需通过适应性调整（如法律修订）来应对合成数据的特殊性，而非完全重构制度。明确合成数据需区别于传统数据类型的监管路径。

Conclusion: 确保AI代理使用合成数据时的信任与问责，关键在于对现有法律框架进行精准修订，承认其独特性并纳入监管范畴，而非建立全新政策制度。

Abstract: Synthetic data, which is artificially generated and intelligently mimicking or supplementing the real-world data, is increasingly used. The proliferation of AI agents and the adoption of synthetic data create a synthetic mirror that conceptualizes a representation and potential distortion of reality, thus generating trust and accountability deficits. This paper explores the implications for privacy and policymaking stemming from synthetic data generation, and the urgent need for new policy instruments and legal framework adaptation to ensure appropriate levels of trust and accountability for AI agents relying on synthetic data. Rather than creating entirely new policy or legal regimes, the most practical approach involves targeted amendments to existing frameworks, recognizing synthetic data as a distinct regulatory category with unique characteristics.

</details>


### [255] [Rigor in AI: Doing Rigorous AI Work Requires a Broader, Responsible AI-Informed Conception of Rigor](https://arxiv.org/abs/2506.14652)
*Alexandra Olteanu,Su Lin Blodgett,Agathe Balayn,Angelina Wang,Fernando Diaz,Flavio du Pin Calmon,Margaret Mitchell,Michael Ekstrand,Reuben Binns,Solon Barocas*

Main category: cs.CY

TL;DR: 论文主张扩展AI研究与实践中的严谨性概念，除方法论严谨性外，还应包含知识、规范、概念、报告和解释严谨性，以应对负责任AI社区关切。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究过于狭隘地将严谨性局限于方法论（如数学/统计方法），导致夸大AI能力等问题。需更全面的严谨性框架以促进负责任AI发展。

Method: 提出六维严谨性框架：方法论、知识（选题依据）、规范（价值观影响）、概念（理论清晰度）、报告（成果呈现）、解释（证据推理）严谨性。

Result: 构建了涵盖多维严谨性的分析框架，为研究者、政策制定者等利益相关方提供讨论AI研究质量的共同语言与结构。

Conclusion: AI社区需采纳更全面的严谨性标准，通过多维框架引导研究实践，解决伦理与可信度问题，推动负责任AI生态建设。

Abstract: In AI research and practice, rigor remains largely understood in terms of methodological rigor -- such as whether mathematical, statistical, or computational methods are correctly applied. We argue that this narrow conception of rigor has contributed to the concerns raised by the responsible AI community, including overblown claims about AI capabilities. Our position is that a broader conception of what rigorous AI research and practice should entail is needed. We believe such a conception -- in addition to a more expansive understanding of (1) methodological rigor -- should include aspects related to (2) what background knowledge informs what to work on (epistemic rigor); (3) how disciplinary, community, or personal norms, standards, or beliefs influence the work (normative rigor); (4) how clearly articulated the theoretical constructs under use are (conceptual rigor); (5) what is reported and how (reporting rigor); and (6) how well-supported the inferences from existing evidence are (interpretative rigor). In doing so, we also aim to provide useful language and a framework for much-needed dialogue about the AI community's work by researchers, policymakers, journalists, and other stakeholders.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [256] [A Survey of Physics-Informed AI for Complex Urban Systems](https://arxiv.org/abs/2506.13777)
*En Xu,Huandong Wang,Yunke Zhang,Sibo Li,Yinzhou Tang,Zhilun Zhou,Yuming Lin,Yuan Yuan,Xiaochen Fan,Jingtao Ding,Yong Li*

Main category: physics.soc-ph

TL;DR: 本文综述了物理信息人工智能方法在城市系统中的应用，提出三种整合范式（物理集成AI、物理-AI混合集成、AI集成物理）及七种方法，分析其在能源、交通等八大领域的应用，强调提升系统可靠性、效率及适应性，并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 城市系统作为复杂系统，需结合物理模型与AI优势：AI捕捉非线性关系，物理模型保障现实规律一致性并提供可解释性。需系统梳理现有方法，指导基于需求与数据选择合适方法。

Method: 提出分类法将现有方法分为三大范式（物理集成AI、物理-AI混合集成、AI集成物理），细化七种代表性方法，并系统考察其在能源、环境、交通等八大城市领域的应用模式。

Result: 分析表明，物理-AI融合方法通过结合物理规律与数据驱动模型，有效解决城市系统挑战，提升可靠性、效率与适应性，同时为方法选择提供框架。

Conclusion: 综合现有方法与应用后，识别关键研究空白，提出未来方向以推动新一代智能城市建模，强调需进一步探索物理与AI深度融合的理论与实践路径。

Abstract: Urban systems are typical examples of complex systems, where the integration of physics-based modeling with artificial intelligence (AI) presents a promising paradigm for enhancing predictive accuracy, interpretability, and decision-making. In this context, AI excels at capturing complex, nonlinear relationships, while physics-based models ensure consistency with real-world laws and provide interpretable insights. We provide a comprehensive review of physics-informed AI methods in urban applications. The proposed taxonomy categorizes existing approaches into three paradigms - Physics-Integrated AI, Physics-AI Hybrid Ensemble, and AI-Integrated Physics - and further details seven representative methods. This classification clarifies the varying degrees and directions of physics-AI integration, guiding the selection and development of appropriate methods based on application needs and data availability. We systematically examine their applications across eight key urban domains: energy, environment, economy, transportation, information, public services, emergency management, and the urban system as a whole. Our analysis highlights how these methodologies leverage physical laws and data-driven models to address urban challenges, enhancing system reliability, efficiency, and adaptability. By synthesizing existing methodologies and their urban applications, we identify critical gaps and outline future research directions, paving the way toward next-generation intelligent urban system modeling.

</details>


### [257] [Infected Smallville: How Disease Threat Shapes Sociality in LLM Agents](https://arxiv.org/abs/2506.13783)
*Soyeon Choi,Kangwook Lee,Oliver Sng,Joshua M. Ackerman*

Main category: physics.soc-ph

TL;DR: 研究使用生成式代理模型（GABM）发现，传染病威胁显著降低代理的社交参与度，验证了行为免疫系统假说，并证明GABM可作为研究复杂社会动态的工具。


<details>
  <summary>Details</summary>
Motivation: 探究传染病威胁如何通过行为免疫系统影响社会性，验证疾病风险与社交行为变化之间的因果关系。

Method: 基于大语言模型的生成式代理建模（GABM），通过三组模拟对比实验，分析代理在接收疫情新闻后的社交行为变化，并结合访谈验证行为动机。

Result: 接触疫情信息的代理社交活动显著减少（聚会出席率下降37%，公共场所访问量降低28%），且能明确区分传染与非传染疾病风险，仅在感染风险存在时调整行为。

Conclusion: GABM为大规模模拟人类社会复杂互动提供了有效实验范式，证实疾病威胁通过认知机制直接塑造群体社交模式。

Abstract: How does the threat of infectious disease influence sociality among generative agents? We used generative agent-based modeling (GABM), powered by large language models, to experimentally test hypotheses about the behavioral immune system. Across three simulation runs, generative agents who read news about an infectious disease outbreak showed significantly reduced social engagement compared to agents who received no such news, including lower attendance at a social gathering, fewer visits to third places (e.g., cafe, store, park), and fewer conversations throughout the town. In interview responses, agents explicitly attributed their behavioral changes to disease-avoidance motivations. A validity check further indicated that they could distinguish between infectious and noninfectious diseases, selectively reducing social engagement only when there was a risk of infection. Our findings highlight the potential of GABM as an experimental tool for exploring complex human social dynamics at scale.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [258] [Sketched Sum-Product Networks for Joins](https://arxiv.org/abs/2506.14034)
*Brian Tsan,Abylay Amanbayev,Asoke Datta,Florin Rusu*

Main category: cs.DB

TL;DR: 提出使用Sum-Product Networks（SPNs）动态近似草图，以支持任意查询的基数估计，解决传统草图方法依赖预定义查询的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有草图方法虽能高效估计多路连接基数，但其构建需依赖预定义的查询选择条件，无法灵活适应新查询，限制了实际应用范围。

Method: 利用SPNs将多元关系分布分解为单变量分布的线性组合，通过单变量分布的草图元素级组合，动态生成任意查询的近似草图。

Result: 通过近似Fast-AGMS和Bound Sketch方法，在避免高成本草图构建的同时，实现了与原始方法相当的基数估计精度。

Conclusion: SPNs动态草图近似为查询优化提供了通用解决方案，显著提升了复杂查询场景下草图技术的实用性。

Abstract: Sketches have shown high accuracy in multi-way join cardinality estimation, a critical problem in cost-based query optimization. Accurately estimating the cardinality of a join operation -- analogous to its computational cost -- allows the optimization of query execution costs in relational database systems. However, although sketches have shown high efficacy in query optimization, they are typically constructed specifically for predefined selections in queries that are assumed to be given a priori, hindering their applicability to new queries. As a more general solution, we propose for Sum-Product Networks to dynamically approximate sketches on-the-fly. Sum-Product Networks can decompose and model multivariate distributions, such as relations, as linear combinations of multiple univariate distributions. By representing these univariate distributions as sketches, Sum-Product Networks can combine them element-wise to efficiently approximate the sketch of any query selection. These approximate sketches can then be applied to join cardinality estimation. In particular, we implement the Fast-AGMS and Bound Sketch methods, which have successfully been used in prior work, despite their costly construction. By accurately approximating them instead, our work provides a practical alternative to apply these sketches to query optimization.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [259] [A Silent Speech Decoding System from EEG and EMG with Heterogenous Electrode Configurations](https://arxiv.org/abs/2506.13835)
*Masakazu Inoue,Motoshige Sato,Kenichi Tomeoka,Nathania Nah,Eri Hatakeyama,Kai Arulkumaran,Ilya Horiguchi,Shuntaro Sasai*

Main category: q-bio.QM

TL;DR: 提出一种处理异质EEG/EMG电极布局的神经网络，通过多任务训练实现无声语音解码，在健康人(95.3%)和语言障碍患者(54.5%)中显著提升分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有无声语音解码系统面临数据收集困难与实验设置差异导致的异质性问题，单被试数据训练的模型性能受限(健康人70.1%/患者13.2%)。

Method: 开发能处理异质电极布局的神经网络架构，利用大规模EEG/EMG数据集进行多任务训练，增强模型泛化能力。

Result: 跨被试验证显示：健康组分类准确率提升35.2%，患者组提升41.3%；跨语言校准性能同步改善。

Conclusion: 该方法验证了实用化无声语音解码系统的可行性，特别为语言障碍患者提供了有效的辅助沟通技术路径。

Abstract: Silent speech decoding, which performs unvocalized human speech recognition from electroencephalography/electromyography (EEG/EMG), increases accessibility for speech-impaired humans. However, data collection is difficult and performed using varying experimental setups, making it nontrivial to collect a large, homogeneous dataset. In this study we introduce neural networks that can handle EEG/EMG with heterogeneous electrode placements and show strong performance in silent speech decoding via multi-task training on large-scale EEG/EMG datasets. We achieve improved word classification accuracy in both healthy participants (95.3%), and a speech-impaired patient (54.5%), substantially outperforming models trained on single-subject data (70.1% and 13.2%). Moreover, our models also show gains in cross-language calibration performance. This increase in accuracy suggests the feasibility of developing practical silent speech decoding systems, particularly for speech-impaired patients.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [260] [Markov Regime-Switching Intelligent Driver Model for Interpretable Car-Following Behavior](https://arxiv.org/abs/2506.14762)
*Chengyuan Zhang,Cathy Wu,Lijun Sun*

Main category: stat.AP

TL;DR: 本文提出一种基于FHMM-IDM的切换机制跟车模型，通过分离驾驶行为模式与外部交通场景，提升模型准确性和可解释性，实验验证其在HighD数据集上的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统单模态跟车模型（如IDM）因结构限制无法捕捉人类多模态驾驶特性，导致参数解释性差且行为预测失真。需开发能区分驾驶意图与外部环境的动态切换模型。

Method: 采用因子隐马尔可夫模型（FHMM-IDM），通过独立潜变量分别表征驾驶模式（如激进加速、稳态跟驰）和交通场景（如自由流、拥堵），结合贝叶斯MCMC推断参数与状态切换轨迹。

Result: 在HighD数据集实验中，模型成功解耦驾驶行为与交通环境，识别出可解释的动态切换规律（如加速/稳态模式转换），显著提升仿真保真度与参数可解释性。

Conclusion: 该框架为不确定环境下的驾驶行为建模提供可解释解决方案，可优化交通仿真、安全评估及ADAS系统设计，推动自动驾驶技术的人本化发展。

Abstract: Accurate and interpretable car-following models are essential for traffic simulation and autonomous vehicle development. However, classical models like the Intelligent Driver Model (IDM) are fundamentally limited by their parsimonious and single-regime structure. They fail to capture the multi-modal nature of human driving, where a single driving state (e.g., speed, relative speed, and gap) can elicit many different driver actions. This forces the model to average across distinct behaviors, reducing its fidelity and making its parameters difficult to interpret. To overcome this, we introduce a regime-switching framework that allows driving behavior to be governed by different IDM parameter sets, each corresponding to an interpretable behavioral mode. This design enables the model to dynamically switch between interpretable behavioral modes, rather than averaging across diverse driving contexts. We instantiate the framework using a Factorial Hidden Markov Model with IDM dynamics (FHMM-IDM), which explicitly separates intrinsic driving regimes (e.g., aggressive acceleration, steady-state following) from external traffic scenarios (e.g., free-flow, congestion, stop-and-go) through two independent latent Markov processes. Bayesian inference via Markov chain Monte Carlo (MCMC) is used to jointly estimate the regime-specific parameters, transition dynamics, and latent state trajectories. Experiments on the HighD dataset demonstrate that FHMM-IDM uncovers interpretable structure in human driving, effectively disentangling internal driver actions from contextual traffic conditions and revealing dynamic regime-switching patterns. This framework provides a tractable and principled solution to modeling context-dependent driving behavior under uncertainty, offering improvements in the fidelity of traffic simulations, the efficacy of safety analyses, and the development of more human-centric ADAS.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [261] [Beyond Shapley Values: Cooperative Games for the Interpretation of Machine Learning Models](https://arxiv.org/abs/2506.13900)
*Marouane Il Idrissi,Agathe Fernandes Machado,Arthur Charpentier*

Main category: stat.ML

TL;DR: 本文提出超越Shapley值的合作博弈论工具，通过Weber集和Harsanyi集扩展特征归因方法，并构建三步框架以设计更稳健、理论可靠的可解释性方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于Shapley值的特征归因方法依赖公理化假设，但其在可解释性中的适用性存在争议，需更系统化的理论框架以增强方法鲁棒性。

Method: 引入合作博弈论中的Weber集和Harsanyi集作为高效分配方案，区分价值函数与聚合规则，并提出三步蓝图（价值函数选择、聚合规则设计、结果验证）构建特征归因方法。

Result: 扩展了Shapley值的局限性，提供更灵活的特征解释工具，并通过理论框架支持定制化归因方法，增强对方法演变的适应性。

Conclusion: 呼吁XAI社区采用广义合作博弈论框架，摆脱固定公理束缚，设计兼具理论严谨性与实践鲁棒性的特征归因方法。

Abstract: Cooperative game theory has become a cornerstone of post-hoc interpretability in machine learning, largely through the use of Shapley values. Yet, despite their widespread adoption, Shapley-based methods often rest on axiomatic justifications whose relevance to feature attribution remains debatable. In this paper, we revisit cooperative game theory from an interpretability perspective and argue for a broader and more principled use of its tools. We highlight two general families of efficient allocations, the Weber and Harsanyi sets, that extend beyond Shapley values and offer richer interpretative flexibility. We present an accessible overview of these allocation schemes, clarify the distinction between value functions and aggregation rules, and introduce a three-step blueprint for constructing reliable and theoretically-grounded feature attributions. Our goal is to move beyond fixed axioms and provide the XAI community with a coherent framework to design attribution methods that are both meaningful and robust to shifting methodological trends.

</details>


### [262] [Mirror Descent Using the Tempesta Generalized Multi-parametric Logarithms](https://arxiv.org/abs/2506.13984)
*Andrzej Cichocki*

Main category: stat.ML

TL;DR: 本文开发了一类基于多参数Tempesta变形对数的Mirror Descent（MD）算法，通过调整超参数适应数据分布或几何结构，生成灵活的新MD及无镜像MD更新家族。


<details>
  <summary>Details</summary>
Motivation: 传统MD算法的链接函数和熵类有限，需扩展其灵活性与适应性以应对复杂数据场景。通过多参数变形对数构建广义熵类，增强算法对数据特性的自适应性。

Method: 利用Tempesta多参数变形对数作为Bregman散度的链接函数，估计其逆函数的广义指数近似，并通过学习超参数调整对数与指数函数的形状及算法行为。

Result: 提出可调超参数的多参数MD框架，生成包括镜像与无镜像更新的广泛算法家族，能动态适应数据分布并控制算法收敛性等性质。

Conclusion: 多参数Tempesta对数通过超参数学习机制，为MD算法提供了理论无限扩展的熵类基础，显著提升了算法灵活性与场景适配能力。

Abstract: In this paper, we develop a wide class Mirror Descent (MD) algorithms, which play a key role in machine learning. For this purpose we formulated the constrained optimization problem, in which we exploits the Bregman divergence with the Tempesta multi-parametric deformation logarithm as a link function. This link function called also mirror function defines the mapping between the primal and dual spaces and is associated with a very-wide (in fact, theoretically infinite) class of generalized trace-form entropies. In order to derive novel MD updates, we estimate generalized exponential function, which closely approximates the inverse of the multi-parametric Tempesta generalized logarithm. The shape and properties of the Tempesta logarithm and its inverse-deformed exponential functions can be tuned by several hyperparameters. By learning these hyperparameters, we can adapt to distribution or geometry of training data, and we can adjust them to achieve desired properties of MD algorithms. The concept of applying multi-parametric logarithms allow us to generate a new wide and flexible family of MD and mirror-less MD updates.

</details>


### [263] [Rademacher learning rates for iterated random functions](https://arxiv.org/abs/2506.13946)
*Nikola Sandrić*

Main category: stat.ML

TL;DR: 本文研究了在非独立同分布（非i.i.d.）数据场景下的监督学习问题，针对由迭代随机函数生成的马尔可夫链数据，提出了基于收缩性控制函数和假设类正则条件的样本误差一致收敛理论，并推导出数据分布依赖型学习率界限。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习假设训练数据来自i.i.d.样本，但现实场景中数据常具有时间依赖性和分布相关性。现有方法的学习率与数据分布无关，导致假设类选择受限和次优样本复杂度。

Method: 假设数据生成过程为具有收缩性控制函数的迭代随机过程（时间齐次马尔可夫链），在假设类满足正则条件的前提下，建立样本误差的一致收敛性，并分析近似经验风险最小化算法的可学习性。

Result: 获得了基于假设类Rademacher复杂度的数据分布依赖型学习率界限，该界限能更精确反映数据生成分布的特性，突破了传统独立于数据分布的学习率限制。

Conclusion: 提出的理论框架有效处理了非i.i.d.数据场景下的学习问题，通过引入收缩性条件和分布依赖型分析，为时间序列数据提供了更优的样本复杂度保证。

Abstract: Most existing literature on supervised machine learning assumes that the training dataset is drawn from an i.i.d. sample. However, many real-world problems exhibit temporal dependence and strong correlations between the marginal distributions of the data-generating process, suggesting that the i.i.d. assumption is often unrealistic. In such cases, models naturally include time-series processes with mixing properties, as well as irreducible and aperiodic ergodic Markov chains. Moreover, the learning rates typically obtained in these settings are independent of the data distribution, which can lead to restrictive choices of hypothesis classes and suboptimal sample complexities for the learning algorithm. In this article, we consider the case where the training dataset is generated by an iterated random function (i.e., an iteratively defined time-homogeneous Markov chain) that is not necessarily irreducible or aperiodic. Under the assumption that the governing function is contractive with respect to its first argument and subject to certain regularity conditions on the hypothesis class, we first establish a uniform convergence result for the corresponding sample error. We then demonstrate the learnability of the approximate empirical risk minimization algorithm and derive its learning rate bound. Both rates are data-distribution dependent, expressed in terms of the Rademacher complexities of the underlying hypothesis class, allowing them to more accurately reflect the properties of the data-generating distribution.

</details>


### [264] [Meta Optimality for Demographic Parity Constrained Regression via Post-Processing](https://arxiv.org/abs/2506.13947)
*Kazuto Fukuchi*

Main category: stat.ML

TL;DR: 本文提出一种元定理框架，用于验证不同场景下公平极小极大最优回归算法的有效性，并通过后处理方法实现公平回归，使传统回归技术改进能直接适配到公平场景。


<details>
  <summary>Details</summary>
Motivation: 现有公平回归算法的理论分析高度依赖特定数据生成模型，缺乏通用性。需建立通用理论框架以验证不同情境下的算法最优性，并简化公平回归的实现流程。

Method: 提出可应用于多种场景的元定理，验证对应回归算法的公平极小极大最优性；通过后处理方法将常规回归技术转化为公平回归算法。

Result: 证明了后处理方法可实现公平极小极大最优回归，并构建元定理为不同场景提供理论保障，使传统回归优化成果可直接迁移至公平约束场景。

Conclusion: 该研究为公平回归提供了通用理论工具，解耦算法开发与公平性适配过程，推动回归技术发展与实际部署的协同进步。

Abstract: We address the regression problem under the constraint of demographic parity, a commonly used fairness definition. Recent studies have revealed fair minimax optimal regression algorithms, the most accurate algorithms that adhere to the fairness constraint. However, these analyses are tightly coupled with specific data generation models. In this paper, we provide meta-theorems that can be applied to various situations to validate the fair minimax optimality of the corresponding regression algorithms. Furthermore, we demonstrate that fair minimax optimal regression can be achieved through post-processing methods, allowing researchers and practitioners to focus on improving conventional regression techniques, which can then be efficiently adapted for fair regression.

</details>


### [265] [Bridging Unsupervised and Semi-Supervised Anomaly Detection: A Theoretically-Grounded and Practical Framework with Synthetic Anomalies](https://arxiv.org/abs/2506.13955)
*Matthew Lau,Tian-Yi Zhou,Xiangchi Yuan,Jizhou Chen,Wenke Lee,Xiaoming Huo*

Main category: stat.ML

TL;DR: 该论文提出了一种理论支撑且实证有效的半监督异常检测框架，通过结合已知和合成异常数据提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 在网络安全和医疗等领域，异常检测至关重要。半监督环境下，如何利用有限的标记异常数据提升检测效果是一个关键问题。

Method: 论文扩展了无监督异常检测的原则，提出在半监督环境中结合已知和合成异常数据训练分类器，并首次给出了半监督异常检测的数学形式化定义。

Result: 实验在五个不同基准测试中验证了该框架的有效性，性能均有提升。理论分析表明合成异常数据能优化低密度区域的异常建模并提供神经网络分类器的最优收敛保证。

Conclusion: 该框架不仅在半监督异常检测中表现优异，其合成异常数据的原理也适用于其他基于分类的异常检测方法，具有广泛适用性。

Abstract: Anomaly detection (AD) is a critical task across domains such as cybersecurity and healthcare. In the unsupervised setting, an effective and theoretically-grounded principle is to train classifiers to distinguish normal data from (synthetic) anomalies. We extend this principle to semi-supervised AD, where training data also include a limited labeled subset of anomalies possibly present in test time. We propose a theoretically-grounded and empirically effective framework for semi-supervised AD that combines known and synthetic anomalies during training. To analyze semi-supervised AD, we introduce the first mathematical formulation of semi-supervised AD, which generalizes unsupervised AD. Here, we show that synthetic anomalies enable (i) better anomaly modeling in low-density regions and (ii) optimal convergence guarantees for neural network classifiers -- the first theoretical result for semi-supervised AD. We empirically validate our framework on five diverse benchmarks, observing consistent performance gains. These improvements also extend beyond our theoretical framework to other classification-based AD methods, validating the generalizability of the synthetic anomaly principle in AD.

</details>


### [266] [Estimation of Treatment Effects in Extreme and Unobserved Data](https://arxiv.org/abs/2506.14051)
*Jiyuan Tan,Jose Blanchet,Vasilis Syrgkanis*

Main category: stat.ML

TL;DR: 提出一种基于极值理论的新框架，用于评估罕见但高影响事件（如极端气候）的政策干预因果效应，解决了传统方法在数据稀缺时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有因果推断方法主要针对高频事件，但政策干预的效果可能仅在罕见事件（如极端气候）中显现，需结合极值理论进行外推分析。

Method: 采用多元正则变化理论建模极端事件，开发一致性估计量，并通过非渐近分析评估其性能。

Result: 在合成与半合成数据中验证了所提估计器的有效性，证明其能准确捕捉极端事件下的因果效应。

Conclusion: 该框架为极端事件因果推断提供了理论工具，填补了传统方法在罕见事件分析中的空白。

Abstract: Causal effect estimation seeks to determine the impact of an intervention from observational data. However, the existing causal inference literature primarily addresses treatment effects on frequently occurring events. But what if we are interested in estimating the effects of a policy intervention whose benefits, while potentially important, can only be observed and measured in rare yet impactful events, such as extreme climate events? The standard causal inference methodology is not designed for this type of inference since the events of interest may be scarce in the observed data and some degree of extrapolation is necessary. Extreme Value Theory (EVT) provides methodologies for analyzing statistical phenomena in such extreme regimes. We introduce a novel framework for assessing treatment effects in extreme data to capture the causal effect at the occurrence of rare events of interest. In particular, we employ the theory of multivariate regular variation to model extremities. We develop a consistent estimator for extreme treatment effects and present a rigorous non-asymptotic analysis of its performance. We illustrate the performance of our estimator using both synthetic and semi-synthetic data.

</details>


### [267] [Universal Rates of ERM for Agnostic Learning](https://arxiv.org/abs/2506.14110)
*Steve Hanneke,Mingyue Xu*

Main category: stat.ML

TL;DR: 本文研究了不可知情景下二元分类的ERM通用学习速率，揭示了其可能的三种速率形式（指数衰减、次平方根衰减、任意慢速），并完整分类了不同概念类别的归属。


<details>
  <summary>Details</summary>
Motivation: 现有通用学习理论多集中于可实现情景，而实际场景中假设往往不成立。本文旨在填补不可知情景下ERM通用速率研究的空白。

Method: 通过分析ERM在不可知情景下的'学习曲线'（样本量增加时超额风险的衰减模式），建立理论框架探索不可知通用速率的可能性。

Result: 提出紧凑三分法：ERM的不可知通用速率仅可能为$e^{-n}$、$o(n^{-1/2})$或任意慢速，并完整刻画各类概念集合的归属。同时建立了目标依赖和贝叶斯依赖速率的完整分类。

Conclusion: 研究首次系统揭示了不可知情景下ERM的普适速率规律，为理论分析和实际应用提供了严格的分类框架，突破了传统可实现假设的局限性。

Abstract: The universal learning framework has been developed to obtain guarantees on the learning rates that hold for any fixed distribution, which can be much faster than the ones uniformly hold over all the distributions. Given that the Empirical Risk Minimization (ERM) principle being fundamental in the PAC theory and ubiquitous in practical machine learning, the recent work of arXiv:2412.02810 studied the universal rates of ERM for binary classification under the realizable setting. However, the assumption of realizability is too restrictive to hold in practice. Indeed, the majority of the literature on universal learning has focused on the realizable case, leaving the non-realizable case barely explored.
  In this paper, we consider the problem of universal learning by ERM for binary classification under the agnostic setting, where the ''learning curve" reflects the decay of the excess risk as the sample size increases. We explore the possibilities of agnostic universal rates and reveal a compact trichotomy: there are three possible agnostic universal rates of ERM, being either $e^{-n}$, $o(n^{-1/2})$, or arbitrarily slow. We provide a complete characterization of which concept classes fall into each of these categories. Moreover, we also establish complete characterizations for the target-dependent universal rates as well as the Bayes-dependent universal rates.

</details>


### [268] [Adjustment for Confounding using Pre-Trained Representations](https://arxiv.org/abs/2506.14329)
*Rickmer Schulte,David Rügamer,Thomas Nagler*

Main category: stat.ML

TL;DR: 本文探讨如何利用预训练神经网络的潜在特征调整混杂因素，以提升平均处理效应（ATE）估计的准确性，并分析高维非结构化数据带来的挑战及神经网络的适应性优势。


<details>
  <summary>Details</summary>
Motivation: 非结构化数据（如图像、文本）可能作为混杂因素影响ATE估计，忽略此类数据会导致结果偏差。但现有方法依赖传统线性假设，难以处理高维、不可识别的潜在特征。

Method: 基于预训练神经网络的潜在特征，结合双机器学习框架，形式化潜在特征在ATE调整中的有效性条件，并分析神经网络对高维稀疏性和内在维度的适应性。

Result: 证明其方法可通过神经网络捕捉数据内在稀疏性和低维结构，实现快速收敛，而传统线性模型因假设不成立无法达到同等效果。

Conclusion: 神经网络能有效克服高维非结构化数据在ATE估计中的挑战，其自适应特性为混杂因素调整提供了更优解决方案。

Abstract: There is growing interest in extending average treatment effect (ATE) estimation to incorporate non-tabular data, such as images and text, which may act as sources of confounding. Neglecting these effects risks biased results and flawed scientific conclusions. However, incorporating non-tabular data necessitates sophisticated feature extractors, often in combination with ideas of transfer learning. In this work, we investigate how latent features from pre-trained neural networks can be leveraged to adjust for sources of confounding. We formalize conditions under which these latent features enable valid adjustment and statistical inference in ATE estimation, demonstrating results along the example of double machine learning. We discuss critical challenges inherent to latent feature learning and downstream parameter estimation arising from the high dimensionality and non-identifiability of representations. Common structural assumptions for obtaining fast convergence rates with additive or sparse linear models are shown to be unrealistic for latent features. We argue, however, that neural networks are largely insensitive to these issues. In particular, we show that neural networks can achieve fast convergence rates by adapting to intrinsic notions of sparsity and dimension of the learning problem.

</details>


### [269] [Adaptive Data Augmentation for Thompson Sampling](https://arxiv.org/abs/2506.14479)
*Wonyoung Kim*

Main category: stat.ML

TL;DR: 本文提出了一种近乎极小极大最优的汤普森采样方法，用于线性上下文赌博机问题。通过设计自适应增强与假设样本耦合的新估计器，该方法在不依赖上下文分布假设的情况下提升参数学习效率，显著改进了现有方法的遗憾边界和实证性能。


<details>
  <summary>Details</summary>
Motivation: 现有汤普森采样方法在实证中表现良好，但未能达到理论上的最优遗憾边界，且依赖上下文分布假设。本文旨在解决这一理论缺陷，同时保持实际应用的鲁棒性。

Method: 提出一种新型估计器，结合自适应增强（adaptive augmentation）与假设样本耦合（coupling of hypothetical samples），以优化参数学习过程，无需对上下文分布进行假设。

Result: 实验结果表明，该方法在多种场景下均具有鲁棒性，遗憾边界接近极小极大最优，且性能显著优于现有基线方法。

Conclusion: 通过理论分析与实证验证，本文方法在理论上填补了汤普森采样的遗憾边界缺陷，同时在实践中保持了高效性和适应性，为线性上下文赌博机问题提供了更优的解决方案。

Abstract: In linear contextual bandits, the objective is to select actions that maximize cumulative rewards, modeled as a linear function with unknown parameters. Although Thompson Sampling performs well empirically, it does not achieve optimal regret bounds. This paper proposes a nearly minimax optimal Thompson Sampling for linear contextual bandits by developing a novel estimator with the adaptive augmentation and coupling of the hypothetical samples that are designed for efficient parameter learning. The proposed estimator accurately predicts rewards for all arms without relying on assumptions for the context distribution. Empirical results show robust performance and significant improvement over existing methods.

</details>


### [270] [Sharp Generalization Bounds for Foundation Models with Asymmetric Randomized Low-Rank Adapters](https://arxiv.org/abs/2506.14530)
*Anastasis Kratsios,Tin Sum Cheng,Aurelien Lucchi,Haitz Sáez de Ocáriz Borde*

Main category: stat.ML

TL;DR: 本文对低秩适应（LoRA）的非对称初始化进行了理论分析，揭示了单次微调运行的泛化差距集中性，提出了样本复杂度的上下界，为实际应用提供了可靠性依据。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LoRA非对称初始化的理论分析多基于多次实验的平均结果，而单次微调运行的行为及其泛化性能仍缺乏理论支持。本文旨在填补这一空白，探究随机冻结下LoRA的泛化特性。

Method: 通过分析非对称LoRA在冻结低秩因子时的泛化差距集中性，推导其样本复杂度的概率上界（高概率下为Õ(√r/√N)），并建立匹配的下界（Ω(1/√N)）。

Result: 理论结果表明，秩r的LoRA在N样本上的泛化差距上界为Õ(√r/√N)，同时样本效率下限为Ω(1/√N)，验证了单次微调运行的可靠性。

Conclusion: 研究为非对称LoRA的实际应用提供了理论支撑，表明单次微调运行具有可预测的泛化性能，增强了该方法在参数高效微调中的实用价值。

Abstract: Low-Rank Adaptation (LoRA) has emerged as a widely adopted parameter-efficient fine-tuning (PEFT) technique for foundation models. Recent work has highlighted an inherent asymmetry in the initialization of LoRA's low-rank factors, which has been present since its inception and was presumably derived experimentally. This paper focuses on providing a comprehensive theoretical characterization of asymmetric LoRA with frozen random factors. First, while existing research provides upper-bound generalization guarantees based on averages over multiple experiments, the behaviour of a single fine-tuning run with specific random factors remains an open question. We address this by investigating the concentration of the typical LoRA generalization gap around its mean. Our main upper bound reveals a sample complexity of $\tilde{\mathcal{O}}\left(\frac{\sqrt{r}}{\sqrt{N}}\right)$ with high probability for rank $r$ LoRAs trained on $N$ samples. Additionally, we also determine the fundamental limits in terms of sample efficiency, establishing a matching lower bound of $\mathcal{O}\left(\frac{1}{\sqrt{N}}\right)$. By more closely reflecting the practical scenario of a single fine-tuning run, our findings offer crucial insights into the reliability and practicality of asymmetric LoRA.

</details>


### [271] [Uniform Mean Estimation for Heavy-Tailed Distributions via Median-of-Means](https://arxiv.org/abs/2506.14673)
*Mikael Møller Høgsgaard,Andrea Paudice*

Main category: stat.ML

TL;DR: 本文分析了中位数均值（MoM）估计器在重尾数据分布（仅具有p∈(1,2]阶矩）下对函数类F的同步均值估计性能，提出基于新型对称化技术的样本复杂度边界，并应用于无界输入的k均值聚类和广义损失回归。


<details>
  <summary>Details</summary>
Motivation: 针对仅具有有限阶矩（p∈(1,2]）的重尾数据分布场景，探索MoM估计器在同步多函数均值估计任务中的理论性能边界及应用潜力。

Method: 通过新型对称化技术建立样本复杂度分析框架，推导MoM估计器在L_p矩约束下的统计保证。

Result: 获得了改进的样本复杂度边界，在k均值聚类和广义损失线性回归应用中超越现有方法的表现。

Conclusion: 所提出的对称化技术为有限矩场景下的鲁棒估计提供了新工具，在无界输入聚类和广义回归任务中展现出实际应用价值。

Abstract: The Median of Means (MoM) is a mean estimator that has gained popularity in the context of heavy-tailed data. In this work, we analyze its performance in the task of simultaneously estimating the mean of each function in a class $\mathcal{F}$ when the data distribution possesses only the first $p$ moments for $p \in (1,2]$. We prove a new sample complexity bound using a novel symmetrization technique that may be of independent interest. Additionally, we present applications of our result to $k$-means clustering with unbounded inputs and linear regression with general losses, improving upon existing works.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [272] [Accurate and scalable exchange-correlation with deep learning](https://arxiv.org/abs/2506.14665)
*Giulia Luise,Chin-Wei Huang,Thijs Vogels,Derk P. Kooi,Sebastian Ehlert,Stephanie Lanius,Klaas J. H. Giesbertz,Amir Karton,Deniz Gunceler,Megan Stanley,Wessel P. Bruinsma,Lin Huang,Xinran Wei,José Garrido Torres,Abylay Katbashev,Bálint Máté,Sékou-Oumar Kaba,Roberto Sordillo,Yingrong Chen,David B. Williams-Young,Christopher M. Bishop,Jan Hermann,Rianne van den Berg,Paola Gori-Giorgi*

Main category: physics.chem-ph

TL;DR: 本文提出了一种基于深度学习的交换相关泛函Skala，通过直接从数据中学习特征，在保持半局部DFT计算效率的同时，首次实现了小分子原子化能的化学精度（误差<1 kcal/mol），并展示了其通过扩展训练集持续提升泛化能力的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统密度泛函理论（DFT）中交换相关（XC）泛函依赖人工设计特征，存在计算效率与精度的矛盾，且现有方法无法在保持计算效率的同时达到实验室预测所需的化学精度。

Method: 开发深度学习框架Skala，通过海量高精度波函数法生成的数据直接学习特征表示，避免人工特征工程。通过持续扩展包含多样化化学空间的高质量训练数据提升模型性能。

Result: Skala在原子化能预测上达到化学精度（MAE=0.98 kcal/mol），计算成本与半局部DFT相当。增加特定领域数据后，在主流化学体系精度可媲美混合泛函，且系统性能随数据规模扩大持续提升。

Conclusion: 数据驱动的深度学习方法可突破传统XC泛函设计范式，Skala通过可扩展的机器学习框架为第一性原理模拟提供了兼具精度、效率与持续进化能力的新路径。

Abstract: Density Functional Theory (DFT) is the most widely used electronic structure method for predicting the properties of molecules and materials. Although DFT is, in principle, an exact reformulation of the Schrödinger equation, practical applications rely on approximations to the unknown exchange-correlation (XC) functional. Most existing XC functionals are constructed using a limited set of increasingly complex, hand-crafted features that improve accuracy at the expense of computational efficiency. Yet, no current approximation achieves the accuracy and generality for predictive modeling of laboratory experiments at chemical accuracy -- typically defined as errors below 1 kcal/mol. In this work, we present Skala, a modern deep learning-based XC functional that bypasses expensive hand-designed features by learning representations directly from data. Skala achieves chemical accuracy for atomization energies of small molecules while retaining the computational efficiency typical of semi-local DFT. This performance is enabled by training on an unprecedented volume of high-accuracy reference data generated using computationally intensive wavefunction-based methods. Notably, Skala systematically improves with additional training data covering diverse chemistry. By incorporating a modest amount of additional high-accuracy data tailored to chemistry beyond atomization energies, Skala achieves accuracy competitive with the best-performing hybrid functionals across general main group chemistry, at the cost of semi-local DFT. As the training dataset continues to expand, Skala is poised to further enhance the predictive power of first-principles simulations.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [273] [Enhancing Clinical Decision Support and EHR Insights through LLMs and the Model Context Protocol: An Open-Source MCP-FHIR Framework](https://arxiv.org/abs/2506.13800)
*Abul Ehtesham,Aditi Singh,Saket Kumar*

Main category: cs.SE

TL;DR: 本文提出一种基于代理的开源框架，整合大语言模型与HL7 FHIR数据，通过动态提取和推理电子健康记录，解决临床决策支持、文档负担与患者健康素养提升等数字健康挑战。


<details>
  <summary>Details</summary>
Motivation: 数字健康领域长期面临临床决策支持不足、医疗文档处理效率低下及患者健康信息理解障碍等问题，需开发可扩展且互操作的解决方案。

Method: 基于MCP-FHIR协议构建代理架构，通过JSON配置声明式访问FHIR资源，结合LLM实现实时数据摘要、解释与多角色个性化通信。

Result: 使用符合FHIR R4标准的合成数据验证框架支持隐私保护与可复现性，相比传统硬编码方法，具备可扩展性、可解释性及多格式FHIR兼容性。

Conclusion: 该框架为个性化数字健康应用提供灵活基础，通过代理化设计提升AI医疗系统的互操作性与动态推理能力，推动精准医疗发展。

Abstract: Enhancing clinical decision support (CDS), reducing documentation burdens, and improving patient health literacy remain persistent challenges in digital health. This paper presents an open-source, agent-based framework that integrates Large Language Models (LLMs) with HL7 FHIR data via the Model Context Protocol (MCP) for dynamic extraction and reasoning over electronic health records (EHRs). Built on the established MCP-FHIR implementation, the framework enables declarative access to diverse FHIR resources through JSON-based configurations, supporting real-time summarization, interpretation, and personalized communication across multiple user personas, including clinicians, caregivers, and patients. To ensure privacy and reproducibility, the framework is evaluated using synthetic EHR data from the SMART Health IT sandbox (https://r4.smarthealthit.org/), which conforms to the FHIR R4 standard. Unlike traditional approaches that rely on hardcoded retrieval and static workflows, the proposed method delivers scalable, explainable, and interoperable AI-powered EHR applications. The agentic architecture further supports multiple FHIR formats, laying a robust foundation for advancing personalized digital health solutions.

</details>


### [274] [Instruction and Solution Probabilities as Heuristics for Inductive Programming](https://arxiv.org/abs/2506.13804)
*Edward McDaid,Sarah McDaid*

Main category: cs.SE

TL;DR: 本文提出通过引入指令概率和解决方案概率作为启发式方法，结合现有指令子集（IS）技术，显著缩小归纳编程（IP）搜索空间规模，最高可降低超100个数量级，并通过交叉验证验证了方法的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有指令子集（IS）虽能缩小IP搜索空间，但仍有进一步优化空间。通过引入指令和解决方案的概率启发式，可更高效地剪枝低概率指令组合，从而提升搜索效率。

Method: 基于代码样本中指令出现频率计算指令概率，构建解决方案概率（各指令概率乘积），并设定不同规模程序单元的解决方案概率阈值。在搜索过程中，通过阈值动态剪枝低概率分支。测试了两种指令概率计算方式：全局统计和按IS独立统计。

Result: 两种概率计算方式均显著降低IP搜索空间规模（最高达数十个数量级），结合IS后总降幅超100个数量级。交叉验证表明方法对未见代码有效。

Conclusion: 概率启发式与IS结合可极大优化IP搜索效率，未来可探索更精细的概率模型及动态剪枝策略。

Abstract: Instruction subsets (ISs) are heuristics that can shrink the size of the inductive programming (IP) search space by tens of orders of magnitude. Here, we extend the IS approach by introducing instruction and solution probabilities as additional heuristics. Instruction probability reflects the expectation of an instruction occurring in a solution, based on the frequency of instruction occurrence in a large code sample. The solution probability for a partial or complete program is simply the product of all constituent instruction probabilities, including duplicates. We treat the minimum solution probabilities observed in code sample program units of different sizes as solution probability thresholds. These thresholds are used to prune the search space as partial solutions are constructed, thereby eliminating any branches containing unlikely combinations of instructions. The new approach has been evaluated using a large sample of human code. We tested two formulations of instruction probability: one based on instruction occurrence across the entire code sample and another that measured the distribution separately for each IS. Our results show that both variants produce substantial further reductions in the IP search space size of up to tens of orders of magnitude, depending on solution size. In combination with IS, reductions of over 100 orders of magnitude can be achieved. We also carried out cross-validation testing to show that the heuristics should work effectively with unseen code. The approach is described and the results and some ideas for future work are discussed.

</details>


### [275] [Structured Program Synthesis using LLMs: Results and Insights from the IPARC Challenge](https://arxiv.org/abs/2506.13820)
*Shraddha Surana,Ashwin Srinivasan,Michael Bain*

Main category: cs.SE

TL;DR: 本文提出基于LLM的结构化归纳编程方法，成功解决IPARC挑战中600个复杂程序合成任务，揭示了人机协作在代码生成中的关键机制。


<details>
  <summary>Details</summary>
Motivation: IPARC挑战的600个合成图像程序任务长期无法被自动化解决，需探索LLM在结构化程序合成中的潜力及人机协作模式。

Method: 采用结构化归纳编程框架与LLM结合，通过先验结构设计、代码冻结、重用机制及人工精修实现系统化程序合成。

Result: 方法覆盖IPARC所有任务类别，验证了结构化引导、代码冻结、LLM创意激发等机制的有效性，任务解决率达100%。

Conclusion: LLM与人类在程序合成中形成互补：LLM提供创意种子，人类负责结构化精修，该协作模式为复杂系统开发提供新路径。

Abstract: The IPARC Challenge, inspired by ARC, provides controlled program synthesis tasks over synthetic images to evaluate automatic program construction, focusing on sequence, selection, and iteration. This set of 600 tasks has resisted automated solutions. This paper presents a structured inductive programming approach with LLMs that successfully solves tasks across all IPARC categories. The controlled nature of IPARC reveals insights into LLM-based code generation, including the importance of prior structuring, LLMs' ability to aid structuring (requiring human refinement), the need to freeze correct code, the efficiency of code reuse, and how LLM-generated code can spark human creativity. These findings suggest valuable mechanisms for human-LLM collaboration in tackling complex program synthesis.

</details>


### [276] [CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language Models in Tool-Calling Error Scenarios](https://arxiv.org/abs/2506.13977)
*Shiting Huang,Zhen Fang,Zehui Chen,Siyu Yuan,Junjie Ye,Yu Zeng,Lin Chen,Qi Mao,Feng Zhao*

Main category: cs.SE

TL;DR: 本文提出CRITICTOOL基准，通过进化策略构建包含复杂工具使用错误的数据集，评估大语言模型在工具学习中的错误处理能力，并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着任务复杂性和长期性增加，大语言模型在工具调用过程中可能触发多种意外错误，如何有效识别、诊断和恢复这些错误成为提升工具学习的关键研究方向。

Method: 基于对多个工具评估基准中函数调用错误类型的分析，采用进化策略构建CRITICTOOL数据集，涵盖不同复杂度的工具使用错误以贴近真实场景。

Result: 实验验证了CRITICTOOL的泛化性和有效性，并深入分析不同大语言模型的工具反思能力，为工具学习研究提供新视角。

Conclusion: CRITICTOOL为工具学习领域提供了专门化的错误评估基准，其进化策略构建方法能反映真实错误场景，促进大语言模型在复杂任务中的鲁棒性研究。

Abstract: The ability of large language models (LLMs) to utilize external tools has enabled them to tackle an increasingly diverse range of tasks. However, as the tasks become more complex and long-horizon, the intricate tool utilization process may trigger various unexpected errors. Therefore, how to effectively handle such errors, including identifying, diagnosing, and recovering from them, has emerged as a key research direction for advancing tool learning. In this work, we first extensively analyze the types of errors encountered during the function-calling process on several competitive tool evaluation benchmarks. Based on it, we introduce CRITICTOOL, a comprehensive critique evaluation benchmark specialized for tool learning. Building upon a novel evolutionary strategy for dataset construction, CRITICTOOL holds diverse tool-use errors with varying complexities, which better reflects real-world scenarios. We conduct extensive experiments on CRITICTOOL, and validate the generalization and effectiveness of our constructed benchmark strategy. We also provide an in-depth analysis of the tool reflection ability on various LLMs, offering a new perspective on the field of tool learning in LLMs. The code is available at \href{https://github.com/Shellorley0513/CriticTool}{https://github.com/Shellorley0513/CriticTool}.

</details>


### [277] [MLDebugging: Towards Benchmarking Code Debugging Across Multi-Library Scenarios](https://arxiv.org/abs/2506.13824)
*Jinyang Huang,Xiachong Feng,Qiguang Chen,Hanjie Zhao,Zihui Cheng,Jiesong Bai,Jingxuan Zhou,Min Li,Libo Qin*

Main category: cs.SE

TL;DR: 该论文提出了首个多库代码调试基准MLDebugging，包含126个Python库的七类问题，评估发现当前大语言模型在此场景下仍存在显著挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于无库或单库代码调试，但实际应用中多库场景复杂且未被充分探索，需构建更贴近现实的评估基准。

Method: 构建MLDebugging基准，覆盖126个Python库的七类多库代码问题，并对主流开源/闭源大语言模型进行全面评估。

Result: 实验表明当前LLMs在多库调试场景中表现欠佳，准确率显著低于单库场景，暴露了现有技术的局限性。

Conclusion: 该工作揭示了LLMs在多库调试中的潜力与不足，为未来提升模型对复杂代码依赖关系的理解提供了研究启示。

Abstract: Code debugging is a crucial task in software engineering, which attracts increasing attention. While remarkable success has been made in the era of large language models (LLMs), current research still focuses on the simple no-library or single-library setting, ignoring the complex multi-library scenario in real-world applications. To address this limitation, we make the first attempt to introduce MLDebugging (Multi-Library Debugging), a comprehensive benchmark designed to assess debugging challenges within multi-library Python code. Specifically, MLDebugging encompasses 126 distinct Python libraries, covering a wide range of multi-library code issues, categorized into seven distinct types. Furthermore, we conduct a thorough evaluation of MLDebugging using both mainstream open-source and closed-source LLMs and highlight that current LLMs still struggle to correctly perform code debugging across multi-library scenarios. We hope this work can uncover the potential of LLMs in multi-library debugging scenario and offer insights for future research.

</details>


### [278] [FrontendBench: A Benchmark for Evaluating LLMs on Front-End Development via Automatic Evaluation](https://arxiv.org/abs/2506.13832)
*Hongda Zhu,Yiwen Zhang,Bing Zhao,Jingzhe Ding,Siyao Liu,Tong Liu,Dandan Wang,Yanan Liu,Zhaojian Li*

Main category: cs.SE

TL;DR: 本文提出FrontendBench，一个由人类与LLM共同开发的前端代码生成基准测试，通过多层次任务和自动化评估框架解决现有基准的不足，验证了其可靠性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有前端代码生成基准存在任务过于简单、测试用例不严谨、缺乏端到端验证等问题，导致模型性能评估不准确。

Method: 构建包含148个提示-测试用例对的FrontendBench，覆盖5个层级的Web组件任务，并设计沙盒环境下的自动化评估框架，结合交互式测试场景。

Result: 自动化评估与专家人工评估一致性达90.54%，不同LLM在真实前端任务中表现差异显著，突显基准的区分能力。

Conclusion: FrontendBench作为可靠、可扩展的基准，支持多模态评估，为前端代码生成研究提供标准化测试基础。

Abstract: Large Language Models (LLMs) have made significant strides in front-end code generation. However, existing benchmarks exhibit several critical limitations: many tasks are overly simplistic, test cases often lack rigor, and end-to-end validation is absent. These issues hinder the accurate assessment of model performance. To address these challenges, we present FrontendBench, a benchmark co-developed by humans and LLMs. FrontendBench categorizes tasks based on code functionality and incorporates interactive test scenarios, enabling a more comprehensive and practical evaluation of front-end code generation capabilities. The benchmark comprises 148 meticulously crafted prompt-test case pairs spanning five levels of web components, from basic UI elements to complex interactive features. Each task reflects realistic front-end development challenges. Furthermore, we introduce an automatic evaluation framework that executes generated code within a sandbox environment and assesses outcomes using predefined test scripts. This framework achieves a 90.54% agreement rate with expert human evaluations, demonstrating high reliability. We benchmark several state-of-the-art LLMs on FrontendBench and observe substantial performance disparities in handling real-world front-end tasks. These results highlight FrontendBench as a reliable and scalable benchmark, supporting consistent multimodal evaluation and providing a robust foundation for future research in front-end code generation. Our data and code will be released soon.

</details>


### [279] [How Does LLM Reasoning Work for Code? A Survey and a Call to Action](https://arxiv.org/abs/2506.13932)
*Ira Ceka,Saurabh Pujar,Irene Manotas,Gail Kaiser,Baishakhi Ray,Shyam Ramji*

Main category: cs.SE

TL;DR: 本文系统综述了大型语言模型在代码任务中的推理技术，提出分类法、分析性能基准，并探讨未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在代码任务（如GitHub问题解决）中展现出潜力，但其实际部署能力及底层代码推理机制尚未被充分研究。

Method: 通过文献调查构建代码推理技术分类法，结合混合/代理方法分析，并评估现有基准与新兴软件工程基准的表现。

Result: 提出首个代码推理专用分类体系，揭示代码属性与推理技术的关联，并发现现有基准未充分覆盖的软件工程研究机会。

Conclusion: 需进一步探索代码结构化特性与LLM推理的协同机制，并开发更贴近真实软件工程场景的评估框架。

Abstract: The rise of large language models (LLMs) has led to dramatic improvements across a wide range of natural language tasks. These advancements have extended into the domain of code, facilitating complex tasks such as code generation, translation, summarization, and repair. However, their utility for real-world deployment in-the-wild has only recently been studied, particularly on software engineering (SWE) tasks such as GitHub issue resolution. In this study, we examine the code reasoning techniques that underlie the ability to perform such tasks, and examine the paradigms used to drive their performance. Our contributions in this paper are: (1) the first dedicated survey on code reasoning for code tasks, highlighting overarching strategies, hybrid and agentic approaches; (2) a taxonomy of various techniques used to drive code reasoning; (3) a comprehensive overview of performance on common benchmarks and a showcase of new, under-explored benchmarks with high potential in SWE; (4) an exploration on how core properties of code can be used to explain different reasoning techniques; and (5) gaps and potentially under-explored areas for future research.

</details>


### [280] [Automatic Qiskit Code Refactoring Using Large Language Models](https://arxiv.org/abs/2506.14535)
*José Manuel Suárez,Luis Mariano Bibbó,Joaquin Bogado,Alejandro Fernandez*

Main category: cs.SE

TL;DR: 本文提出一种基于大语言模型（LLM）的Qiskit代码重构方法，通过提取官方文档中的迁移场景分类法指导LLM识别代码迁移需求，有效解决量子框架API快速变化导致的兼容性问题。


<details>
  <summary>Details</summary>
Motivation: 量子软件框架（如Qiskit）的API快速迭代导致开发者面临代码兼容性维护难题，需自动化工具辅助迁移以降低维护成本。

Method: 1. 从Qiskit文档中提取迁移场景分类法；2. 将分类法与源代码共同输入LLM；3. 设计结构化输入策略突破LLM上下文长度约束，定向识别迁移模式并生成重构方案。

Result: 实验表明，结合领域特定分类法的LLM能有效自动化Qiskit代码迁移（至0.46版本），并提供已验证的提示词模板及迁移能力评估方法论。

Conclusion: 基于领域知识的LLM方法可显著提升量子代码迁移效率，所提出的分类法与评估框架为其他量子框架迁移任务提供通用解决方案。

Abstract: As quantum software frameworks evolve, developers face increasing challenges in maintaining compatibility with rapidly changing APIs. In this work, we present a novel methodology for refactoring Qiskit code using large language models (LLMs). We begin by extracting a taxonomy of migration scenarios from the different sources of official Qiskit documentation (such as release notes), capturing common patterns such as migration of functionality to different modules and deprecated usage. This taxonomy, along with the original Python source code, is provided as input to an LLM, which is then tasked with identifying instances of migration scenarios in the code and suggesting appropriate refactoring solutions. Our approach is designed to address the context length limitations of current LLMs by structuring the input and reasoning process in a targeted, efficient manner. The results demonstrate that LLMs, when guided by domain-specific migration knowledge, can effectively assist in automating Qiskit code migration. This work contributes both a set of proven prompts and taxonomy for Qiskit code migration from earlier versions to version 0.46 and a methodology to asses the capabilities of LLMs to assist in the migration of quantum code.

</details>


### [281] [Low-code to fight climate change: the Climaborough project](https://arxiv.org/abs/2506.14623)
*Aaron Conrardy,Armen Sulejmani,Cindy Guerlain,Daniele Pagani,David Hick,Matteo Satta,Jordi Cabot*

Main category: cs.SE

TL;DR: 欧盟Climaborough项目通过低代码/无代码策略开发气候仪表板平台，帮助欧洲11个城市加速实现2030年碳中和目标，支持非技术用户自定义监测本地气候行动成效。


<details>
  <summary>Details</summary>
Motivation: 为帮助欧洲城市快速部署气候转型工具，需解决非技术用户难以实时监测、评估及扩展本地气候措施成效的痛点，同时缩短仪表板开发周期。

Method: 采用低代码策略加速仪表板开发流程，并嵌入无代码设计理念，使市民根据需求自主配置和调整数据可视化界面。

Result: 成功构建Climaborough城市平台，整合历史与实时数据生成交互式仪表板，实现气候行动成效的动态追踪与规模化潜力评估。

Conclusion: 低代码/无代码策略有效平衡开发效率与用户自主性，为城市气候治理提供可扩展的技术框架，促进公民参与碳中和进程。

Abstract: The EU-funded Climaborough project supports European cities to achieve carbon neutrality by 2030. Eleven cities in nine countries will deploy in real conditions products and services fostering climate transition in their local environment. The Climaborough City Platform is being developed to monitor the cities' overall progress towards their climate goals by aggregating historic and real-time data and displaying the results in user-friendly dashboards that will be used by non-technical experts to evaluate the effectiveness of local experimental initiatives, identify those that yield significant impact, and assess the potential consequences of scaling them up to a broader level. In this paper, we explain how we have put in place a low-code/no-code strategy in Climaborough in response to the project's aim to quickly deploy climate dashboards. A low-code strategy is used to accelerate the development of the dashboards. The dashboards embed a no-code philosophy that enables all types of citizen profiles to configure and adapt the dashboard to their specific needs.

</details>


### [282] [ACM Survey Draft on Formalising Software Requirements with Large Language Models](https://arxiv.org/abs/2506.14627)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: 本文为工作草案，汇总了94篇论文，重点探讨软件需求追踪、形式化方法及其工具、统一编程理论（UTP）和机构理论，并与作者此前两篇会议论文[7][8]进行对比。


<details>
  <summary>Details</summary>
Motivation: 整合现有研究，系统分析软件工程中需求追踪、形式化方法等核心领域，并通过对比不同会议论文（如轻量级评审的AACS 2025与严格评审的SAIV 2025），展示研究进展与差异。

Method: 采用文献综述方法，汇总94篇论文内容，结合专题章节（第4-6节）深入分析，并通过比较作者此前两篇会议论文[7][8]的评审流程、内容长度及出版形式，突显当前草案的独特性。

Result: 形成涵盖需求追踪、形式化方法工具、UTP等主题的综合草案，明确其与[7][8]的差异：如[7]为简短会议摘要，[8]为经严格评审的长篇研究，而本草案侧重广度整合与理论扩展。

Conclusion: 本草案通过系统性文献整合与跨论文对比，为软件工程关键领域提供结构化综述，同时反映不同出版流程（如会议摘要vs.严格评审论文）对研究成果展示的影响。

Abstract: This draft is a working document, having a summary of nighty-four (94) papers with additional sections on Traceability of Software Requirements (Section 4), Formal Methods and Its Tools (Section 5), Unifying Theories of Programming (UTP) and Theory of Institutions (Section 6). Please refer to abstract of [7,8]. Key difference of this draft from our recently anticipated ones with similar titles, i.e. AACS 2025 [7] and SAIV 2025 [8] is:
  [7] is a two page submission to ADAPT Annual Conference, Ireland. Submitted on 18th of March, 2025, it went through the light-weight blind review and accepted for poster presentation. Conference was held on 15th of May, 2025.
  [8] is a nine page paper with additional nine pages of references and summary tables, submitted to Symposium on AI Verification (SAIV 2025) on 24th of April, 2025. It went through rigorous review process. The uploaded version on arXiv.org [8] is the improved one of the submission, after addressing the specific suggestions to improve the paper.

</details>


### [283] [Navigating the growing field of research on AI for software testing -- the taxonomy for AI-augmented software testing and an ontology-driven literature survey](https://arxiv.org/abs/2506.14640)
*Ina K. Schieferdecker*

Main category: cs.SE

TL;DR: 本文综述了AI在软件测试自动化中的增强作用，提出ai4st分类法以分类现有研究，并探讨AI带来的新型测试形式及未解决问题。


<details>
  <summary>Details</summary>
Motivation: 传统测试自动化设计、维护成本高，而AI在工程领域的突破为软件测试（包括手动与自动化）提供了新的可能性，需系统性梳理其应用与潜力。

Method: 通过回顾AI在测试自动化中的最新研究（从无自动化到全自动化），分析AI赋能的新测试形式，并构建ai4st分类法对研究进行分类。

Result: 提出ai4st分类法，系统分类现有研究，识别出测试生成、预言机设计等开放性问题，并指出AI可支持自适应测试等新范式。

Conclusion: AI为软件测试自动化带来变革潜力，未来需基于ai4st框架探索新方法，并解决测试数据、可解释性等开放挑战。

Abstract: In industry, software testing is the primary method to verify and validate the functionality, performance, security, usability, and so on, of software-based systems. Test automation has gained increasing attention in industry over the last decade, following decades of intense research into test automation and model-based testing. However, designing, developing, maintaining and evolving test automation is a considerable effort. Meanwhile, AI's breakthroughs in many engineering fields are opening up new perspectives for software testing, for both manual and automated testing. This paper reviews recent research on AI augmentation in software test automation, from no automation to full automation. It also discusses new forms of testing made possible by AI. Based on this, the newly developed taxonomy, ai4st, is presented and used to classify recent research and identify open research questions.

</details>


### [284] [Unified Software Engineering agent as AI Software Engineer](https://arxiv.org/abs/2506.14683)
*Leonhard Applis,Yuntong Zhang,Shanchao Liang,Nan Jiang,Lin Tan,Abhik Roychoudhury*

Main category: cs.SE

TL;DR: 本文提出统一软件工程智能体USEagent，旨在整合多种软件工程任务能力，作为未来AI软件工程师的雏形，并在构建的USEbench基准测试中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体多针对单一软件任务（如测试、调试），无法处理复杂开发场景（如修复补丁、接管他人代码）。研究旨在探索LLM智能体是否具备成为AI软件工程师的潜力。

Method: 开发USEagent：通过LLM作为推理引擎调用外部工具的统一智能体，整合编码、测试、修复等能力；构建USEbench基准（整合SWE-bench等数据集），涵盖1,271个仓库级任务以评估多任务处理能力。

Result: USEagent在USEbench上表现优于通用智能体（如CodeActAgent），但在部分编码任务仍存在能力差距，为未来改进指明方向。

Conclusion: USEagent作为AI软件工程师的初步实现，展示了多任务协同处理潜力，但需进一步填补能力缺口以实现人机协作开发。

Abstract: The growth of Large Language Model (LLM) technology has raised expectations for automated coding. However, software engineering is more than coding and is concerned with activities including maintenance and evolution of a project. In this context, the concept of LLM agents has gained traction, which utilize LLMs as reasoning engines to invoke external tools autonomously. But is an LLM agent the same as an AI software engineer? In this paper, we seek to understand this question by developing a Unified Software Engineering agent or USEagent. Unlike existing work which builds specialized agents for specific software tasks such as testing, debugging, and repair, our goal is to build a unified agent which can orchestrate and handle multiple capabilities. This gives the agent the promise of handling complex scenarios in software development such as fixing an incomplete patch, adding new features, or taking over code written by others. We envision USEagent as the first draft of a future AI Software Engineer which can be a team member in future software development teams involving both AI and humans. To evaluate the efficacy of USEagent, we build a Unified Software Engineering bench (USEbench) comprising of myriad tasks such as coding, testing, and patching. USEbench is a judicious mixture of tasks from existing benchmarks such as SWE-bench, SWT-bench, and REPOCOD. In an evaluation on USEbench consisting of 1,271 repository-level software engineering tasks, USEagent shows improved efficacy compared to existing general agents such as OpenHands CodeActAgent. There exist gaps in the capabilities of USEagent for certain coding tasks, which provides hints on further developing the AI Software Engineer of the future.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [285] [Balancing Preservation and Modification: A Region and Semantic Aware Metric for Instruction-Based Image Editing](https://arxiv.org/abs/2506.13827)
*Zhuoying Li,Zhu Xu,Yuxin Peng,Yang Liu*

Main category: cs.GR

TL;DR: 本文提出了一种新的指标BPM，用于评估基于指令的图像编辑质量，通过分离编辑相关与无关区域进行综合评估，并验证其与人工评估的高一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图像编辑评估指标存在高人工成本或任务适配性差的问题，无法兼顾编辑区域修改与无关区域保留的全面评估。

Method: BPM指标通过定位编辑相关区域，分两层评估：区域感知判断（位置/尺寸对齐指令）和语义感知判断（内容合规性及无关区域保留）。

Result: 实验表明BPM在综合指令编辑数据上的评估结果与人工评估一致性最高，且其区域定位能力可提升现有图像编辑方法效果。

Conclusion: BPM为指令式图像编辑提供了可解释的量化评估框架，兼具评估全面性与实际应用扩展潜力。

Abstract: Instruction-based image editing, which aims to modify the image faithfully according to the instruction while preserving irrelevant content unchanged, has made significant progress. However, there still lacks a comprehensive metric for assessing the editing quality. Existing metrics either require high human evaluation costs, which hinder large-scale evaluation, or are adapted from other tasks and lose task-specific concerns, failing to comprehensively evaluate both instruction-based modification and preservation of irrelevant regions, resulting in biased evaluation. To tackle this, we introduce a new metric called Balancing Preservation and Modification (BPM), tailored for instruction-based image editing by explicitly disentangling the image into editing-relevant and irrelevant regions for specific consideration. We first identify and locate editing-relevant regions, followed by a two-tier process to assess editing quality: Region-Aware Judge evaluates whether the position and size of the edited region align with the instruction, and Semantic-Aware Judge further assesses the instruction content compliance within editing-relevant regions as well as content preservation within irrelevant regions, yielding comprehensive and interpretable quality assessment. Moreover, the editing-relevant region localization in BPM can be integrated into image editing approaches to improve editing quality, demonstrating its broad applicability. We verify the effectiveness of the BPM metric on comprehensive instruction-editing data, and the results show the highest alignment with human evaluation compared to existing metrics, indicating its efficacy. Code is available at: https://joyli-x.github.io/BPM/

</details>


### [286] [Innovating China's Intangible Cultural Heritage with DeepSeek + MidJourney: The Case of Yangliuqing theme Woodblock Prints](https://arxiv.org/abs/2506.14104)
*RuiKun Yang,ZhongLiang Wei,Longdi Xian*

Main category: cs.GR

TL;DR: 本研究结合DeepSeek与MidJourney技术，提出一种生成抗疫主题杨柳青年画的新方法，通过FID评分与用户问卷验证其文化传承与创新效果。


<details>
  <summary>Details</summary>
Motivation: 解决杨柳青木版年画作为非物质文化遗产在保护传统与创新发展之间的平衡难题，探索AI技术助力文化传承的可行性。

Method: 采用DeepSeek生成主题提示词，结合MidJourney生成图像，融合原始年画元素与AI生成关键提示，通过FID指标评估生成质量。

Result: 混合方法获得最低平均FID值(150.2±4.9)，62份问卷显示该方法产出最具文化认同度的图像，且激发最高文化推广意愿(89%)与消费兴趣(93%)。

Conclusion: AI与传统艺术元素的深度融合能有效实现文化遗产的现代转译，为非遗创新提供可量化评估的技术路径，兼具文化保真度与时代吸引力。

Abstract: Yangliuqing woodblock prints, a cornerstone of China's intangible cultural heritage, are celebrated for their intricate designs and vibrant colors. However, preserving these traditional art forms while fostering innovation presents significant challenges. This study explores the DeepSeek + MidJourney approach to generating creative, themed Yangliuqing woodblock prints focused on the fight against COVID-19 and depicting joyous winners. Using Fréchet Inception Distance (FID) scores for evaluation, the method that combined DeepSeek-generated thematic prompts, MidJourney-generated thematic images, original Yangliuqing prints, and DeepSeek-generated key prompts in MidJourney-generated outputs achieved the lowest mean FID score (150.2) with minimal variability (σ = 4.9). Additionally, feedback from 62 participants, collected via questionnaires, confirmed that this hybrid approach produced the most representative results. Moreover, the questionnaire data revealed that participants demonstrated the highest willingness to promote traditional culture and the strongest interest in consuming the AI-generated images produced through this method. These findings underscore the effectiveness of an innovative approach that seamlessly blends traditional artistic elements with modern AI-driven creativity, ensuring both cultural preservation and contemporary relevance.

</details>


### [287] [ReFrame: Layer Caching for Accelerated Inference in Real-Time Rendering](https://arxiv.org/abs/2506.13814)
*Lufei Liu,Tor M. Aamodt*

Main category: cs.GR

TL;DR: ReFrame通过缓存中间特征优化实时渲染中的神经网络计算，在保持质量的同时平均加速1.4倍。


<details>
  <summary>Details</summary>
Motivation: 利用图形渲染任务中固有的时间连贯性，通过重用历史帧中间计算结果减少冗余运算，平衡实时渲染的质量与性能。

Method: 提出ReFrame的缓存策略框架，扩展特征复用技术至实时渲染场景，支持多种编码器-解码器结构网络，探索不同缓存策略的质量-性能权衡。

Result: 在三个实时渲染任务中实现平均1.4倍加速，图像质量损失可忽略，代码已开源。

Conclusion: ReFrame证明了特征缓存策略在实时渲染中的普适有效性，为渲染管线提供了高效的质量-性能平衡方案。

Abstract: Graphics rendering applications increasingly leverage neural networks in tasks such as denoising, supersampling, and frame extrapolation to improve image quality while maintaining frame rates. The temporal coherence inherent in these tasks presents an opportunity to reuse intermediate results from previous frames and avoid redundant computations. Recent work has shown that caching intermediate features to be reused in subsequent inferences is an effective method to reduce latency in diffusion models. We extend this idea to real-time rendering and present ReFrame, which explores different caching policies to optimize trade-offs between quality and performance in rendering workloads. ReFrame can be applied to a variety of encoder-decoder style networks commonly found in rendering pipelines. Experimental results show that we achieve 1.4x speedup on average with negligible quality loss in three real-time rendering tasks. Code available: https://ubc-aamodt-group.github.io/reframe-layer-caching/

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [288] [BraTS orchestrator : Democratizing and Disseminating state-of-the-art brain tumor image analysis](https://arxiv.org/abs/2506.13807)
*Florian Kofler,Marcel Rosier,Mehdi Astaraki,Ujjwal Baid,Hendrik Möller,Josef A. Buchner,Felix Steinbauer,Eva Oswald,Ezequiel de la Rosa,Ivan Ezhov,Constantin von See,Jan Kirschke,Anton Schmick,Sarthak Pati,Akis Linardos,Carla Pitarch,Sanyukta Adap,Jeffrey Rudie,Maria Correia de Verdier,Rachit Saluja,Evan Calabrese,Dominic LaBella,Mariam Aboian,Ahmed W. Moawad,Nazanin Maleki,Udunna Anazodo,Maruf Adewole,Marius George Linguraru,Anahita Fathi Kazerooni,Zhifan Jiang,Gian Marco Conte,Hongwei Li,Juan Eugenio Iglesias,Spyridon Bakas,Benedikt Wiestler,Marie Piraud,Bjoern Menze*

Main category: eess.IV

TL;DR: BraTS挑战赛开发的脑肿瘤分割算法应用有限，作者推出开源Python工具包BraTS orchestrator，简化算法部署，促进其在科研与临床中的使用。


<details>
  <summary>Details</summary>
Motivation: 尽管BraTS挑战赛提供了大量脑肿瘤数据集并推动了算法发展，但其成果在科研和临床中应用不足，需降低技术使用门槛以加速传播。

Method: 开发开源Python包BraTS orchestrator，集成前沿分割/合成算法，提供零编程基础教程，抽象深度学习复杂性，支持一键推理部署。

Result: 工具包已在GitHub开源，通过直观教程使研究者/临床医生可直接调用BraTS优胜算法，实现技术民主化。

Conclusion: BraTS orchestrator将挑战赛积累的专业知识转化为易用工具，使神经放射/肿瘤学领域更广泛受益于BraTS社区的技术进步。

Abstract: The Brain Tumor Segmentation (BraTS) cluster of challenges has significantly advanced brain tumor image analysis by providing large, curated datasets and addressing clinically relevant tasks. However, despite its success and popularity, algorithms and models developed through BraTS have seen limited adoption in both scientific and clinical communities. To accelerate their dissemination, we introduce BraTS orchestrator, an open-source Python package that provides seamless access to state-of-the-art segmentation and synthesis algorithms for diverse brain tumors from the BraTS challenge ecosystem. Available on GitHub (https://github.com/BrainLesion/BraTS), the package features intuitive tutorials designed for users with minimal programming experience, enabling both researchers and clinicians to easily deploy winning BraTS algorithms for inference. By abstracting the complexities of modern deep learning, BraTS orchestrator democratizes access to the specialized knowledge developed within the BraTS community, making these advances readily available to broader neuro-radiology and neuro-oncology audiences.

</details>


### [289] [Reliable Noninvasive Glucose Sensing via CNN-Based Spectroscopy](https://arxiv.org/abs/2506.13819)
*El Arbi Belfarsi,Henry Flores,Maria Valero*

Main category: eess.IV

TL;DR: 本研究提出一种基于短波红外光谱的双模态AI框架，结合多波长成像+CNN与光电二极管传感器+机器学习回归器，在合成血液模型和仿生材料中实现高精度无创血糖监测。


<details>
  <summary>Details</summary>
Motivation: 开发兼顾临床精度、成本效益与可穿戴集成的新型解决方案，突破传统有创血糖监测局限，推动连续无创监测技术发展。

Method: 双模态设计：1) 多波长SWIR成像系统+CNN提取葡萄糖吸收空间特征；2) 紧凑光电二极管电压传感器+随机森林等算法处理归一化光信号。在70-200 mg/dL生理葡萄糖范围内测试合成血液模型与仿生材料。

Result: CNN在650nm波长下MAPE达4.82%，Clarke误差网格Zone A覆盖100%；光电二极管系统Zone A准确率86.4%。

Conclusion: 该框架在临床准确性、成本控制与可穿戴性间取得平衡，为可靠连续血糖监测系统奠定技术基础，达到当前最先进水平。

Abstract: In this study, we present a dual-modal AI framework based on short-wave infrared (SWIR) spectroscopy. The first modality employs a multi-wavelength SWIR imaging system coupled with convolutional neural networks (CNNs) to capture spatial features linked to glucose absorption. The second modality uses a compact photodiode voltage sensor and machine learning regressors (e.g., random forest) on normalized optical signals. Both approaches were evaluated on synthetic blood phantoms and skin-mimicking materials across physiological glucose levels (70 to 200 mg/dL). The CNN achieved a mean absolute percentage error (MAPE) of 4.82% at 650 nm with 100% Zone A coverage in the Clarke Error Grid, while the photodiode system reached 86.4% Zone A accuracy. This framework constitutes a state-of-the-art solution that balances clinical accuracy, cost efficiency, and wearable integration, paving the way for reliable continuous non-invasive glucose monitoring.

</details>


### [290] [Comparison of ConvNeXt and Vision-Language Models for Breast Density Assessment in Screening Mammography](https://arxiv.org/abs/2506.13964)
*Yusdivia Molina-Román,David Gómez-Ortiz,Ernestina Menasalvas-Ruiz,José Gerardo Tamez-Peña,Alejandro Santos-Díaz*

Main category: eess.IV

TL;DR: 研究比较多模态与CNN方法在乳腺密度分类中的表现，发现端到端微调的ConvNeXt模型优于多模态方法，强调专业医学影像中CNN结合微调的优势。


<details>
  <summary>Details</summary>
Motivation: 乳腺钼靶密度分类对癌症风险评估至关重要，但存在主观解释和观察者间差异问题，需探索自动化方法以提升准确性与一致性。

Method: 使用BioMedCLIP和ConvNeXt模型，在BI-RADS系统下评估零样本分类、带文本描述的线性探测及带数值标签的微调三种学习场景。

Result: 微调后的ConvNeXt模型表现最佳，优于BioMedCLIP的线性探测；零样本分类效果一般，线性探测虽具潜力但不及端到端微调。

Conclusion: 尽管多模态学习有潜力，CNN结合端到端微调在医学影像中更具优势，未来需改进文本表示与领域适配以提升放射学应用效果。

Abstract: Mammographic breast density classification is essential for cancer risk assessment but remains challenging due to subjective interpretation and inter-observer variability. This study compares multimodal and CNN-based methods for automated classification using the BI-RADS system, evaluating BioMedCLIP and ConvNeXt across three learning scenarios: zero-shot classification, linear probing with textual descriptions, and fine-tuning with numerical labels. Results show that zero-shot classification achieved modest performance, while the fine-tuned ConvNeXt model outperformed the BioMedCLIP linear probe. Although linear probing demonstrated potential with pretrained embeddings, it was less effective than full fine-tuning. These findings suggest that despite the promise of multimodal learning, CNN-based models with end-to-end fine-tuning provide stronger performance for specialized medical imaging. The study underscores the need for more detailed textual representations and domain-specific adaptations in future radiology applications.

</details>


### [291] [Latent Anomaly Detection: Masked VQ-GAN for Unsupervised Segmentation in Medical CBCT](https://arxiv.org/abs/2506.14209)
*Pengwei Wang*

Main category: eess.IV

TL;DR: 提出一种无监督训练的两阶段方法，用于自动识别颌骨放射性骨坏死（ONJ）影像异常，减少手动标注负担，并支持3D打印应用。


<details>
  <summary>Details</summary>
Motivation: 由于ONJ影像中标注数据稀缺，监督训练难以实施，需开发无监督方法以实现自动异常检测。

Method: 两阶段训练流程：1) 使用VQ-GAN重建正常样本；2) 结合随机立方体掩码和ONJ特定掩码训练新编码器以恢复数据。

Result: 方法在模拟和真实患者数据中均实现成功分割。

Conclusion: 该方法提供快速初始分割，降低人工标注成本，且结合后处理可直接用于3D打印。

Abstract: Advances in treatment technology now allow for the use of customizable 3D-printed hydrogel wound dressings for patients with osteoradionecrosis (ORN) of the jaw (ONJ). Meanwhile, deep learning has enabled precise segmentation of 3D medical images using tools like nnUNet.
  However, the scarcity of labeled data in ONJ imaging makes supervised training impractical. This study aims to develop an unsupervised training approach for automatically identifying anomalies in imaging scans.
  We propose a novel two-stage training pipeline. In the first stage, a VQ-GAN is trained to accurately reconstruct normal subjects. In the second stage, random cube masking and ONJ-specific masking are applied to train a new encoder capable of recovering the data.
  The proposed method achieves successful segmentation on both simulated and real patient data.
  This approach provides a fast initial segmentation solution, reducing the burden of manual labeling. Additionally, it has the potential to be directly used for 3D printing when combined with hand-tuned post-processing.

</details>


### [292] [orGAN: A Synthetic Data Augmentation Pipeline for Simultaneous Generation of Surgical Images and Ground Truth Labels](https://arxiv.org/abs/2506.14303)
*Niran Nataraj,Maina Sogabe,Kenji Kawashima*

Main category: eess.IV

TL;DR: 提出基于GAN的orGAN系统，通过生成高保真手术出血图像解决医学影像数据不足、伦理及标注难题，结合StyleGAN与LaMa修复模块，实现高效、低伦理成本的标注数据集生成。


<details>
  <summary>Details</summary>
Motivation: 医学影像中深度学习面临数据多样性不足、伦理问题、高成本及精确标注需求，尤其手术出血检测因缺乏真实场景数据集而困难。需通过合成数据解决实际应用中的限制。

Method: 基于StyleGAN引入Relational Positional Learning模拟真实出血事件并标注坐标，结合LaMa修复模块恢复出血前图像，利用小规模模拟器官数据集生成合成数据。

Result: 混合orGAN与模拟器官数据的平衡数据集在手术场景中达到90%检测准确率及99%帧级准确率，验证了合成数据的有效性。

Conclusion: 尽管数据存在器官形态单一及术中伪影，orGAN显著推进了伦理、高效、低成本手术出血数据集的构建，支持AI在手术中的更广泛应用。

Abstract: Deep learning in medical imaging faces obstacles: limited data diversity, ethical issues, high acquisition costs, and the need for precise annotations. Bleeding detection and localization during surgery is especially challenging due to the scarcity of high-quality datasets that reflect real surgical scenarios. We propose orGAN, a GAN-based system for generating high-fidelity, annotated surgical images of bleeding. By leveraging small "mimicking organ" datasets, synthetic models that replicate tissue properties and bleeding, our approach reduces ethical concerns and data-collection costs. orGAN builds on StyleGAN with Relational Positional Learning to simulate bleeding events realistically and mark bleeding coordinates. A LaMa-based inpainting module then restores clean, pre-bleed visuals, enabling precise pixel-level annotations. In evaluations, a balanced dataset of orGAN and mimicking-organ images achieved 90% detection accuracy in surgical settings and up to 99% frame-level accuracy. While our development data lack diverse organ morphologies and contain intraoperative artifacts, orGAN markedly advances ethical, efficient, and cost-effective creation of realistic annotated bleeding datasets, supporting broader integration of AI in surgical practice.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [293] [A Survey on World Models Grounded in Acoustic Physical Information](https://arxiv.org/abs/2506.13833)
*Xiaoliang Chen,Le Chang,Xin Yu,Yunhe Huang,Xianling Tu*

Main category: cs.SD

TL;DR: 本文综述了基于声学物理信息的世界模型，探讨其理论框架、方法（如物理信息神经网络、生成模型）及在机器人、医疗等领域的应用，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 声学信号作为物理事件的机械波载体，蕴含丰富的物质属性与动态交互信息，为构建高保真环境感知与因果推理模型提供新途径。

Method: 结合物理信息神经网络（PINNs）、生成模型、自监督多模态学习框架，解析声学信号中的物理规律与潜在信息编码机制。

Result: 在机器人、自动驾驶、医疗和金融领域实现动态事件预测模拟，验证声学世界模型在复杂场景中的感知与推理能力。

Conclusion: 需解决技术伦理挑战，发展鲁棒、因果性、不确定性感知的声学智能，推动具身化主动声学智能与‘直觉物理引擎’的构建。

Abstract: This survey provides a comprehensive overview of the emerging field of world models grounded in the foundation of acoustic physical information. It examines the theoretical underpinnings, essential methodological frameworks, and recent technological advancements in leveraging acoustic signals for high-fidelity environmental perception, causal physical reasoning, and predictive simulation of dynamic events. The survey explains how acoustic signals, as direct carriers of mechanical wave energy from physical events, encode rich, latent information about material properties, internal geometric structures, and complex interaction dynamics. Specifically, this survey establishes the theoretical foundation by explaining how fundamental physical laws govern the encoding of physical information within acoustic signals. It then reviews the core methodological pillars, including Physics-Informed Neural Networks (PINNs), generative models, and self-supervised multimodal learning frameworks. Furthermore, the survey details the significant applications of acoustic world models in robotics, autonomous driving, healthcare, and finance. Finally, it systematically outlines the important technical and ethical challenges while proposing a concrete roadmap for future research directions toward robust, causal, uncertainty-aware, and responsible acoustic intelligence. These elements collectively point to a research pathway towards embodied active acoustic intelligence, empowering AI systems to construct an internal "intuitive physics" engine through sound.

</details>


### [294] [Acoustic scattering AI for non-invasive object classifications: A case study on hair assessment](https://arxiv.org/abs/2506.14148)
*Long-Vu Hoang,Tuan Nguyen,Tran Huy Dat*

Main category: cs.SD

TL;DR: 本文提出一种基于声学散射的非侵入式物体分类方法，通过头发样本的声散射信号，结合多种深度学习策略（全监督、嵌入分类、基础模型微调、自监督微调），实现头发类型与湿度的分类，最高准确率近90%。该方法为隐私保护、非接触式分类提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 传统视觉分类可能涉及隐私与接触限制，研究旨在利用声学散射特性，开发一种非接触、保护隐私的替代方案，适用于多行业场景。

Method: 发射声波刺激物体，捕捉散射信号；采用四种AI方法（全监督学习、嵌入分类、监督/自监督模型微调），通过深度学习分析信号特征进行分类。

Result: 最佳策略为全参数微调自监督模型，分类准确率达近90%，验证了声散射在物体属性识别中的有效性。

Conclusion: 声学散射结合自监督深度学习可高效实现非接触分类，具备隐私保护优势，为工业检测等领域开辟新应用潜力。

Abstract: This paper presents a novel non-invasive object classification approach using acoustic scattering, demonstrated through a case study on hair assessment. When an incident wave interacts with an object, it generates a scattered acoustic field encoding structural and material properties. By emitting acoustic stimuli and capturing the scattered signals from head-with-hair-sample objects, we classify hair type and moisture using AI-driven, deep-learning-based sound classification. We benchmark comprehensive methods, including (i) fully supervised deep learning, (ii) embedding-based classification, (iii) supervised foundation model fine-tuning, and (iv) self-supervised model fine-tuning. Our best strategy achieves nearly 90% classification accuracy by fine-tuning all parameters of a self-supervised model. These results highlight acoustic scattering as a privacy-preserving, non-contact alternative to visual classification, opening huge potential for applications in various industries.

</details>


### [295] [Pushing the Performance of Synthetic Speech Detection with Kolmogorov-Arnold Networks and Self-Supervised Learning Models](https://arxiv.org/abs/2506.14153)
*Tuan Dat Phuong,Long-Vu Hoang,Huy Dat Tran*

Main category: cs.SD

TL;DR: 本文提出将XLSR-Conformer模型中的多层感知机替换为基于Kolmogorov-Arnold表示定理的KAN网络，显著提升了合成语音检测性能。


<details>
  <summary>Details</summary>
Motivation: 语音合成技术的进步导致欺骗攻击日益复杂，现有基于自监督学习的检测系统（如XLSR-Conformer）虽表现优异，但架构仍有改进空间。

Method: 在XLSR-Conformer模型中，用Kolmogorov-Arnold Network（KAN）替代传统多层感知机（MLP），利用其数学表示优势优化模型结构。

Result: 在ASVspoof2021数据集上，LA和DF集的检测性能相对提升60.55%，21LA集的等错误率（EER）达到0.70%。

Conclusion: 将KAN整合到自监督学习模型中，为合成语音检测提供了有效的新方向，验证了架构改进的潜力。

Abstract: Recent advancements in speech synthesis technologies have led to increasingly advanced spoofing attacks, posing significant challenges for automatic speaker verification systems. While systems based on self-supervised learning (SSL) models, particularly the XLSR-Conformer model, have demonstrated remarkable performance in synthetic speech detection, there remains room for architectural improvements. In this paper, we propose a novel approach that replaces the traditional Multi-Layer Perceptron in the XLSR-Conformer model with a Kolmogorov-Arnold Network (KAN), a novel architecture based on the Kolmogorov-Arnold representation theorem. Our results on ASVspoof2021 demonstrate that integrating KAN into the SSL-based models can improve the performance by 60.55% relatively on LA and DF sets, further achieving 0.70% EER on the 21LA set. These findings suggest that incorporating KAN into SSL-based models is a promising direction for advances in synthetic speech detection.

</details>


### [296] [Fretting-Transformer: Encoder-Decoder Model for MIDI to Tablature Transcription](https://arxiv.org/abs/2506.14223)
*Anna Hamberger,Sebastian Murgul,Jochen Schmidt,Michael Heizmann*

Main category: cs.SD

TL;DR: 本文提出Fretting-Transformer模型，基于T5架构将MIDI自动转换为吉他六线谱，解决弦品歧义与可演奏性问题，在多个数据集上超越基线方法与商业软件。


<details>
  <summary>Details</summary>
Motivation: 传统MIDI符号缺乏吉他演奏的物理约束信息（如弦品位置），导致自动生成的指法谱存在可演奏性缺陷。该研究旨在通过深度学习解决弦乐器的符号音乐转录难题。

Method: 采用T5的编码器-解码器架构，将MIDI转谱任务建模为符号翻译问题。整合DadaGP/GuitarToday/Leduc数据集，设计数据预处理与tokenization策略，并引入调弦/变调夹条件机制增强上下文处理能力。

Result: 实验表明模型在六线谱准确率与可演奏性指标上优于A*算法和Guitar Pro等商业工具，上下文敏感处理使性能显著提升。

Conclusion: Fretting-Transformer为吉他自动转谱建立了新基准，其条件化处理框架为未来乐器专用转录系统提供了可扩展的技术基础。

Abstract: Music transcription plays a pivotal role in Music Information Retrieval (MIR), particularly for stringed instruments like the guitar, where symbolic music notations such as MIDI lack crucial playability information. This contribution introduces the Fretting-Transformer, an encoderdecoder model that utilizes a T5 transformer architecture to automate the transcription of MIDI sequences into guitar tablature. By framing the task as a symbolic translation problem, the model addresses key challenges, including string-fret ambiguity and physical playability. The proposed system leverages diverse datasets, including DadaGP, GuitarToday, and Leduc, with novel data pre-processing and tokenization strategies. We have developed metrics for tablature accuracy and playability to quantitatively evaluate the performance. The experimental results demonstrate that the Fretting-Transformer surpasses baseline methods like A* and commercial applications like Guitar Pro. The integration of context-sensitive processing and tuning/capo conditioning further enhances the model's performance, laying a robust foundation for future developments in automated guitar transcription.

</details>


### [297] [Making deep neural networks work for medical audio: representation, compression and domain adaptation](https://arxiv.org/abs/2506.13970)
*Charles C Onu*

Main category: cs.SD

TL;DR: 该论文提出利用机器学习分析医学音频信号（如婴儿哭声）以预测健康状况，通过迁移学习、模型压缩、领域适应技术和开源数据集，推动AI在医疗音频监测中的应用。


<details>
  <summary>Details</summary>
Motivation: 传统医学音频分析依赖专家听觉判断，存在标准化不足、资源匮乏地区难以普及及人耳难以察觉细微模式的问题。自动化分析可提升诊断一致性、扩大筛查范围并实现早期诊疗。

Method: 1. 利用成人语音数据库通过迁移学习提升婴儿哭声模型性能；2. 提出基于张量分解的端到端循环网络压缩方法；3. 设计音频领域自适应技术并迁移计算机视觉方法；4. 发布开源婴儿哭声数据集。

Result: 实现低数据环境下高精度婴儿哭声分析、数百倍模型压缩率、跨领域泛化能力提升，并促进研究社区数据共享。

Conclusion: 奠定婴儿哭声作为生命体征的研究基础，证明AI音频监测在推动普惠医疗中的变革潜力。

Abstract: This thesis addresses the technical challenges of applying machine learning to understand and interpret medical audio signals. The sounds of our lungs, heart, and voice convey vital information about our health. Yet, in contemporary medicine, these sounds are primarily analyzed through auditory interpretation by experts using devices like stethoscopes. Automated analysis offers the potential to standardize the processing of medical sounds, enable screening in low-resource settings where physicians are scarce, and detect subtle patterns that may elude human perception, thereby facilitating early diagnosis and treatment.
  Focusing on the analysis of infant cry sounds to predict medical conditions, this thesis contributes on four key fronts. First, in low-data settings, we demonstrate that large databases of adult speech can be harnessed through neural transfer learning to develop more accurate and robust models for infant cry analysis. Second, in cost-effective modeling, we introduce an end-to-end model compression approach for recurrent networks using tensor decomposition. Our method requires no post-hoc processing, achieves compression rates of several hundred-fold, and delivers accurate, portable models suitable for resource-constrained devices. Third, we propose novel domain adaptation techniques tailored for audio models and adapt existing methods from computer vision. These approaches address dataset bias and enhance generalization across domains while maintaining strong performance on the original data. Finally, to advance research in this domain, we release a unique, open-source dataset of infant cry sounds, developed in collaboration with clinicians worldwide.
  This work lays the foundation for recognizing the infant cry as a vital sign and highlights the transformative potential of AI-driven audio monitoring in shaping the future of accessible and affordable healthcare.

</details>


### [298] [SLEEPING-DISCO 9M: A large-scale pre-training dataset for generative music modeling](https://arxiv.org/abs/2506.14293)
*Tawsif Ahmed,Andrej Radonjic,Gollam Rabby*

Main category: cs.SD

TL;DR: 本文提出了Sleeping-DISCO 9M数据集，首个基于真实流行音乐的大规模预训练数据集，解决了现有合成/重录数据集无法反映现实音乐风格的问题。


<details>
  <summary>Details</summary>
Motivation: 现有开源数据集（如GTSinger、DISCO-10M）多为合成音乐或非真实流行音乐，导致生成式音乐模型社区采用率低。需构建真实世界流行音乐数据集以支持文本-音乐生成、歌声合成等任务。

Method: 通过收集实际流行音乐及世界知名艺术家的真实作品构建数据集，而非使用合成/重录音频或任意规模的非代表性数据。

Result: 成功创建Sleeping-DISCO 9M数据集，覆盖流行音乐与经典曲目，填补了高质量真实音乐数据在生成任务中的空白。

Conclusion: 该数据集突破了传统合成数据限制，为生成式AI音乐任务提供了更贴近现实场景的基础数据支持。

Abstract: We present Sleeping-DISCO 9M, a large-scale pre-training dataset for music and song. To the best of our knowledge, there are no open-source high-quality dataset representing popular and well-known songs for generative music modeling tasks such as text-music, music-captioning, singing-voice synthesis, melody reconstruction and cross-model retrieval. Past contributions focused on isolated and constrained factors whose core perspective was to create synthetic or re-recorded music corpus (e.g. GTSinger, M4Singer) and arbitrarily large-scale audio datasets (e.g. DISCO-10M and LAIONDISCO-12M) had been another focus for the community. Unfortunately, adoption of these datasets has been below substantial in the generative music community as these datasets fail to reflect real-world music and its flavour. Our dataset changes this narrative and provides a dataset that is constructed using actual popular music and world-renowned artists.

</details>


### [299] [Unifying Streaming and Non-streaming Zipformer-based ASR](https://arxiv.org/abs/2506.14434)
*Bidisha Sharma,Karthik Pandia Durai,Shankar Venkatesan,Jeena J Prakash,Shashi Kumar,Malolan Chetlur,Andreas Stolcke*

Main category: cs.SD

TL;DR: 该论文提出了一种统一流式和非流式自动语音识别（ASR）模型的框架，通过动态右上下文和分块注意力掩码训练zipformer模型，显著降低了开发成本并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 为了降低开发、训练和部署成本，研究者希望统一流式和非流式ASR模型，同时利用未来上下文信息提升模型性能。

Method: 采用动态右上下文策略，通过分块注意力掩码训练zipformer模型，分析不同右上下文帧数对模型准确性和延迟的影响。

Result: 实验表明，该方法在Librispeech和内部对话数据集上，相对降低7.9%的词错误率，且流式模型性能接近非流式模型。

Conclusion: 该方法不仅统一了流式和非流式ASR模型，还实现了灵活的延迟-准确性权衡，满足不同客户需求。

Abstract: There has been increasing interest in unifying streaming and non-streaming automatic speech recognition (ASR) models to reduce development, training, and deployment costs. We present a unified framework that trains a single end-to-end ASR model for both streaming and non-streaming applications, leveraging future context information. We propose to use dynamic right-context through the chunked attention masking in the training of zipformer-based ASR models. We demonstrate that using right-context is more effective in zipformer models compared to other conformer models due to its multi-scale nature. We analyze the effect of varying the number of right-context frames on accuracy and latency of the streaming ASR models. We use Librispeech and large in-house conversational datasets to train different versions of streaming and non-streaming models and evaluate them in a production grade server-client setup across diverse testsets of different domains. The proposed strategy reduces word error by relative 7.9\% with a small degradation in user-perceived latency. By adding more right-context frames, we are able to achieve streaming performance close to that of non-streaming models. Our approach also allows flexible control of the latency-accuracy tradeoff according to customers requirements.

</details>


### [300] [Refining music sample identification with a self-supervised graph neural network](https://arxiv.org/abs/2506.14684)
*Aditya Bhattacharjee,Ivan Meresman Higgs,Mark Sandler,Emmanouil Benetos*

Main category: cs.SD

TL;DR: 提出了一种轻量级可扩展的图神经网络编码架构，用于音频样本识别，在保持性能的同时大幅减少参数量，并引入两阶段检索策略提升效果。


<details>
  <summary>Details</summary>
Motivation: 当前音频样本识别系统难以应对音乐制作中的常见变换（如变速、变调、效果处理等），且现有模型参数量大、效率低。

Method: 1) 在对比学习框架中使用图神经网络构建轻量编码架构
2) 提出两阶段检索：粗粒度相似性搜索+交叉注意力分类器精排
3) 发布带细粒度标注的Sample100数据集用于短查询评测

Result: 参数量仅为SOTA模型的9%，但达到可比性能（mAP 44.2%），两阶段检索显著提升匹配质量

Conclusion: 该轻量架构在保持精度的同时提升效率，两阶段方法和新数据集为解决短查询识别难题提供了新方向。

Abstract: Automatic sample identification (ASID), the detection and identification of portions of audio recordings that have been reused in new musical works, is an essential but challenging task in the field of audio query-based retrieval. While a related task, audio fingerprinting, has made significant progress in accurately retrieving musical content under "real world" (noisy, reverberant) conditions, ASID systems struggle to identify samples that have undergone musical modifications. Thus, a system robust to common music production transformations such as time-stretching, pitch-shifting, effects processing, and underlying or overlaying music is an important open challenge.
  In this work, we propose a lightweight and scalable encoding architecture employing a Graph Neural Network within a contrastive learning framework. Our model uses only 9% of the trainable parameters compared to the current state-of-the-art system while achieving comparable performance, reaching a mean average precision (mAP) of 44.2%.
  To enhance retrieval quality, we introduce a two-stage approach consisting of an initial coarse similarity search for candidate selection, followed by a cross-attention classifier that rejects irrelevant matches and refines the ranking of retrieved candidates - an essential capability absent in prior models. In addition, because queries in real-world applications are often short in duration, we benchmark our system for short queries using new fine-grained annotations for the Sample100 dataset, which we publish as part of this work.

</details>


### [301] [Adaptive Accompaniment with ReaLchords](https://arxiv.org/abs/2506.14723)
*Yusong Wu,Tim Cooijmans,Kyle Kastner,Adam Roberts,Ian Simon,Alexander Scarlatos,Chris Donahue,Cassie Tarakajian,Shayegan Omidshafiei,Aaron Courville,Pablo Samuel Castro,Natasha Jaques,Cheng-Zhi Anna Huang*

Main category: cs.SD

TL;DR: ReaLchords提出了一种在线生成模型，通过强化学习微调和新型奖励模型，实现实时即兴和弦伴奏，适应陌生输入并生成协调的伴奏。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型虽能生成富有表现力的音乐，但无法以在线方式（即与其他音乐家实时互动）生成音乐。本文旨在解决这一限制，实现实时协作创作。

Method: 结合预训练的在线模型（最大似然训练）与强化学习微调，利用新型奖励模型（评估和弦与旋律的和谐及时序一致性）和基于教师模型（可预见未来旋律）的蒸馏方法。

Result: 定量实验和听觉测试表明，模型能有效适应陌生旋律输入并生成合适的伴奏，验证了方法的有效性。

Conclusion: ReaLchords为实时即兴协作创作提供了可能，并可扩展至其他模态的同步共创。

Abstract: Jamming requires coordination, anticipation, and collaborative creativity between musicians. Current generative models of music produce expressive output but are not able to generate in an \emph{online} manner, meaning simultaneously with other musicians (human or otherwise). We propose ReaLchords, an online generative model for improvising chord accompaniment to user melody. We start with an online model pretrained by maximum likelihood, and use reinforcement learning to finetune the model for online use. The finetuning objective leverages both a novel reward model that provides feedback on both harmonic and temporal coherency between melody and chord, and a divergence term that implements a novel type of distillation from a teacher model that can see the future melody. Through quantitative experiments and listening tests, we demonstrate that the resulting model adapts well to unfamiliar input and produce fitting accompaniment. ReaLchords opens the door to live jamming, as well as simultaneous co-creation in other modalities.

</details>


### [302] [Exploring Speaker Diarization with Mixture of Experts](https://arxiv.org/abs/2506.14750)
*Gaobin Yang,Maokui He,Shutong Niu,Ruoyu Wang,Hang Chen,Jun Du*

Main category: cs.SD

TL;DR: 本文提出了一种基于记忆感知多说话人嵌入和序列到序列架构的神经说话人日志系统（NSD-MS2S），并引入共享软专家混合模块（SS-MoE）扩展为NSD-MS2S-SSMoE，显著提升了鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 针对复杂真实场景中说话人日志的鲁棒性和模型偏差问题，探索通过增强说话人嵌入和优化模型架构来提升性能。

Method: 结合记忆感知多说话人嵌入模块与Seq2Seq框架，并设计SS-MoE模块动态融合专家网络，降低模型偏差。

Result: 在CHiME-6等复杂数据集上取得SOTA结果，验证了方法在噪声、重叠语音等挑战性场景的有效性。

Conclusion: 所提出的NSD-MS2S-SSMoE通过联合优化嵌入表示与模型架构，为实际应用中的说话人日志任务提供了高效解决方案。

Abstract: In this paper, we propose a novel neural speaker diarization system using memory-aware multi-speaker embedding with sequence-to-sequence architecture (NSD-MS2S), which integrates a memory-aware multi-speaker embedding module with a sequence-to-sequence architecture. The system leverages a memory module to enhance speaker embeddings and employs a Seq2Seq framework to efficiently map acoustic features to speaker labels. Additionally, we explore the application of mixture of experts in speaker diarization, and introduce a Shared and Soft Mixture of Experts (SS-MoE) module to further mitigate model bias and enhance performance. Incorporating SS-MoE leads to the extended model NSD-MS2S-SSMoE. Experiments on multiple complex acoustic datasets, including CHiME-6, DiPCo, Mixer 6 and DIHARD-III evaluation sets, demonstrate meaningful improvements in robustness and generalization. The proposed methods achieve state-of-the-art results, showcasing their effectiveness in challenging real-world scenarios.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [303] [Projecting U.S. coastal storm surge risks and impacts with deep learning](https://arxiv.org/abs/2506.13963)
*Julian R. Rice,Karthik Balaguru,Fadia Ticona Rollano,John Wilson,Brent Daniel,David Judi,Ning Sun,L. Ruby Leung*

Main category: physics.ao-ph

TL;DR: 利用深度学习模型评估美国沿海风暴潮风险，结合未来热带气旋强度与海平面变化，发现本世纪末风险人口将增加50%，佛罗里达等地风险显著上升。


<details>
  <summary>Details</summary>
Motivation: 风暴潮是热带气旋最致命的灾害之一，但由于其罕见性和物理复杂性，当前及未来风险评估困难。人工智能在自然灾害建模中的发展为解决该问题提供了新途径。

Method: 采用深度学习风暴潮模型，结合90万次合成热带气旋事件，分析未来热带气旋行为与海平面变化的影响，并耦合淹没模型进行风险评估。

Result: 历史百年一遇风暴潮与观测数据吻合；本世纪末热带气旋强度增强与海平面上升将导致风险人口增加50%，佛罗里达风险显著，乔治亚州和南卡罗来纳州存在临界阈值。

Conclusion: 未来热带气旋与海平面变化将显著加剧沿海风险，需重点关注佛罗里达等高风险区域及临界阈值地区的防灾规划。

Abstract: Storm surge is one of the deadliest hazards posed by tropical cyclones (TCs), yet assessing its current and future risk is difficult due to the phenomenon's rarity and physical complexity. Recent advances in artificial intelligence applications to natural hazard modeling suggest a new avenue for addressing this problem. We utilize a deep learning storm surge model to efficiently estimate coastal surge risk in the United States from 900,000 synthetic TC events, accounting for projected changes in TC behavior and sea levels. The derived historical 100-year surge (the event with a 1% yearly exceedance probability) agrees well with historical observations and other modeling techniques. When coupled with an inundation model, we find that heightened TC intensities and sea levels by the end of the century result in a 50% increase in population at risk. Key findings include markedly heightened risk in Florida, and critical thresholds identified in Georgia and South Carolina.

</details>


### [304] [AI-Informed Model Analogs for Subseasonal-to-Seasonal Prediction](https://arxiv.org/abs/2506.14022)
*Jacob B. Landsberg,Elizabeth A. Barnes,Matthew Newman*

Main category: physics.ao-ph

TL;DR: 本文提出一种基于可解释AI的类比预测方法，通过神经网络优化历史相似样本选择，显著提升次季节至季节(S2S)预测在温度与大气环流等任务中的性能，并增强极端事件预测和不确定性表征能力。


<details>
  <summary>Details</summary>
Motivation: 次季节至季节(S2S)预测对公共卫生、灾害防治和农业至关重要，但该时间尺度的预测存在显著挑战。传统方法在预测精度和不确定性表征方面存在局限，需结合AI技术进行改进。

Method: 采用可解释AI框架构建模型类比预测系统：1) 使用人工神经网络学习历史数据权重掩码优化相似样本选择 2) 在三个差异化任务验证方法：加州夏季温度分类、中西部温度回归、北大西洋冬季环流分类 3) 对比传统类比预测、气候基准和持续性基准。

Result: AI优化方法在气候模式和再分析数据中均表现最优：1) 确定性/概率性预测指标全面超越基准方法 2) 改进极端温度事件预测 3) 提升预测不确定性表征能力 4) 可解释框架揭示S2S可预测性物理来源。

Conclusion: 基于可解释AI的类比预测方法有效突破S2S预测瓶颈，其灵活框架可扩展至多变量预测任务，同时通过权重分析为理解预测机理提供新途径，具有重要应用价值。

Abstract: Subseasonal-to-seasonal forecasting is crucial for public health, disaster preparedness, and agriculture, and yet it remains a particularly challenging timescale to predict. We explore the use of an interpretable AI-informed model analog forecasting approach, previously employed on longer timescales, to improve S2S predictions. Using an artificial neural network, we learn a mask of weights to optimize analog selection and showcase its versatility across three varied prediction tasks: 1) classification of Week 3-4 Southern California summer temperatures; 2) regional regression of Month 1 midwestern U.S. summer temperatures; and 3) classification of Month 1-2 North Atlantic wintertime upper atmospheric winds. The AI-informed analogs outperform traditional analog forecasting approaches, as well as climatology and persistence baselines, for deterministic and probabilistic skill metrics on both climate model and reanalysis data. We find the analog ensembles built using the AI-informed approach also produce better predictions of temperature extremes and improve representation of forecast uncertainty. Finally, by using an interpretable-AI framework, we analyze the learned masks of weights to better understand S2S sources of predictability.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [305] [Asymptotically Smaller Encodings for Graph Problems and Scheduling](https://arxiv.org/abs/2506.14042)
*Bernardo Subercaseaux*

Main category: cs.LO

TL;DR: 该论文提出了一种基于双覆盖理论的图问题CNF编码方法，显著降低子句数量至O(|V|²/lg|V|)，并展示了在密集区间图和调度问题中的高效应用。


<details>
  <summary>Details</summary>
Motivation: 传统图问题（如顶点覆盖、独立集等）的CNF编码需要Ω(|V|²)子句，复杂度较高。研究旨在探索更紧凑的编码方式，以提升预处理效率，并验证理论在实践中的有效性。

Method: 利用Erdős等人关于双覆盖的理论，重新设计图问题的CNF编码结构；针对密集区间图提出新型独立集编码，结合对数规模子句优化。

Result: 成功将多个图问题的编码复杂度降至O(|V|²/lg|V|)，独立集在密集区间图中仅需O(|V|lg|V|)子句；调度问题编码规模从O(NMT²)优化至O(NMT + MT²lgT)。

Conclusion: 理论成果为Bounded Variable Addition预处理机制提供了新解释，实际应用显著压缩字符串编码和调度问题复杂度，验证了方法的高效性与扩展性。

Abstract: We show how several graph problems (e.g., vertex-cover, independent-set, $k$-coloring) can be encoded into CNF using only $O(|V|^2 / \lg |V|)$ many clauses, as opposed to the $Ω(|V|^2)$ constraints used by standard encodings. This somewhat surprising result is a simple consequence of a result of Erdős, Chung, and Spencer (1983) about biclique coverings of graphs, and opens theoretical avenues to understand the success of "Bounded Variable Addition'' (Manthey, Heule, and Biere, 2012) as a preprocessing tool. Finally, we show a novel encoding for independent sets in some dense interval graphs using only $O(|V| \lg |V|)$ clauses (the direct encoding uses $Ω(|V|^2)$), which we have successfully applied to a string-compression encoding posed by Bannai et al. (2022). As a direct byproduct, we obtain a reduction in the encoding size of a scheduling problem posed by Mayank and Modal (2020) from $O(NMT^2)$ to $O(NMT + M T^2 \lg T)$, where $N$ is the number of tasks, $T$ the total timespan, and $M$ the number of machines.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [306] [A Systematic Review of User-Centred Evaluation of Explainable AI in Healthcare](https://arxiv.org/abs/2506.13904)
*Ivania Donoso-Guzmán,Kristýna Sirka Kacafírková,Maxwell Szymanski,An Jacobs,Denis Parra,Katrien Verbert*

Main category: cs.HC

TL;DR: 该研究通过系统综述医疗领域XAI用户评估实践，提出了一个用户体验属性框架及评估指南，以提升可解释AI在真实医疗场景中的可信度和可用性。


<details>
  <summary>Details</summary>
Motivation: 当前可解释人工智能（XAI）方法在真实医疗场景中的实用价值缺乏验证，且缺乏明确的用户评估设计指南，导致其可信度和可用性评估被忽视。

Method: 对医疗领域82项XAI用户研究进行系统综述，结合预定义编码方案与迭代归纳编码，分析解释属性与评估策略。

Result: 1. 归纳了医疗XAI以人为中心的评估趋势；2. 揭示解释属性间的关联性；3. 提出更新框架及情境化评估设计指南。

Conclusion: 研究填补了XAI用户评估指南的空白，为跨学科团队提供可定制化评估框架，推动医疗领域可信解释系统的实践落地。

Abstract: Despite promising developments in Explainable Artificial Intelligence, the practical value of XAI methods remains under-explored and insufficiently validated in real-world settings. Robust and context-aware evaluation is essential, not only to produce understandable explanations but also to ensure their trustworthiness and usability for intended users, but tends to be overlooked because of no clear guidelines on how to design an evaluation with users.
  This study addresses this gap with two main goals: (1) to develop a framework of well-defined, atomic properties that characterise the user experience of XAI in healthcare; and (2) to provide clear, context-sensitive guidelines for defining evaluation strategies based on system characteristics.
  We conducted a systematic review of 82 user studies, sourced from five databases, all situated within healthcare settings and focused on evaluating AI-generated explanations. The analysis was guided by a predefined coding scheme informed by an existing evaluation framework, complemented by inductive codes developed iteratively.
  The review yields three key contributions: (1) a synthesis of current evaluation practices, highlighting a growing focus on human-centred approaches in healthcare XAI; (2) insights into the interrelations among explanation properties; and (3) an updated framework and a set of actionable guidelines to support interdisciplinary teams in designing and implementing effective evaluation strategies for XAI systems tailored to specific application contexts.

</details>


### [307] [StorySage: Conversational Autobiography Writing Powered by a Multi-Agent Framework](https://arxiv.org/abs/2506.14159)
*Shayan Talaei,Meijin Li,Kanu Grover,James Kent Hippler,Diyi Yang,Amin Saberi*

Main category: cs.HC

TL;DR: StorySage是一个多代理框架驱动的自传写作辅助系统，通过灵活对话和结构化方法帮助用户整理记忆并完成自传。


<details>
  <summary>Details</summary>
Motivation: 现有的对话式写作助手通常依赖通用交互和预定义指南，难以捕捉个人记忆并逐步构建完整的自传。

Method: 引入StorySage系统，采用由Interviewer、Session Scribe、Planner、Section Writer和Session Coordinator组成的多代理框架，迭代收集用户记忆并更新自传。

Result: 实验模拟和用户研究（N=28）显示，StorySage在会话流畅性、叙事完整性和用户满意度方面优于基线系统。

Conclusion: StorySage不仅为自传写作提供了新颖架构，还展示了多代理系统如何增强人机创意协作。

Abstract: Every individual carries a unique and personal life story shaped by their memories and experiences. However, these memories are often scattered and difficult to organize into a coherent narrative, a challenge that defines the task of autobiography writing. Existing conversational writing assistants tend to rely on generic user interactions and pre-defined guidelines, making it difficult for these systems to capture personal memories and develop a complete biography over time. We introduce StorySage, a user-driven software system designed to meet the needs of a diverse group of users that supports a flexible conversation and a structured approach to autobiography writing. Powered by a multi-agent framework composed of an Interviewer, Session Scribe, Planner, Section Writer, and Session Coordinator, our system iteratively collects user memories, updates their autobiography, and plans for future conversations. In experimental simulations, StorySage demonstrates its ability to navigate multiple sessions and capture user memories across many conversations. User studies (N=28) highlight how StorySage maintains improved conversational flow, narrative completeness, and higher user satisfaction when compared to a baseline. In summary, StorySage contributes both a novel architecture for autobiography writing and insights into how multi-agent systems can enhance human-AI creative partnerships.

</details>


### [308] [Balancing Caregiving and Self-Care: Exploring Mental Health Needs of Alzheimer's and Dementia Caregivers](https://arxiv.org/abs/2506.14196)
*Jiayue Melissa Shi,Keran Wang,Dong Whi Yoo,Ravi Karkar,Koustuv Saha*

Main category: cs.HC

TL;DR: 研究探讨AD/ADRD家庭照顾者的心理健康需求，通过访谈分析其照护负担管理实践与技术使用，提出需动态、阶段敏感的个性化支持方案。


<details>
  <summary>Details</summary>
Motivation: 现有支持系统忽视家庭照顾者心理健康需求的动态演变特性，需探索其需求变化规律及技术改进方向。

Method: 对25名AD/ADRD患者家庭照顾者进行半结构化访谈，分析心理健康挑战成因、应对策略及技术使用情况，构建心理健康演变的阶段模型。

Result: 识别心理健康问题的因果关联，绘制三阶段动态演变图谱；揭示现有技术需提升可访问性、扩展性及个性化适配能力以适应阶段变化需求。

Conclusion: 研究为开发动态分阶段心理健康干预技术提供理论基础，通过适配不同照护阶段的技术方案实现照顾者与患者的双向受益。

Abstract: Alzheimer's Disease and Related Dementias (AD/ADRD) are progressive neurodegenerative conditions that impair memory, thought processes, and functioning. Family caregivers of individuals with AD/ADRD face significant mental health challenges due to long-term caregiving responsibilities. Yet, current support systems often overlook the evolving nature of their mental wellbeing needs. Our study examines caregivers' mental wellbeing concerns, focusing on the practices they adopt to manage the burden of caregiving and the technologies they use for support. Through semi-structured interviews with 25 family caregivers of individuals with AD/ADRD, we identified the key causes and effects of mental health challenges, and developed a temporal mapping of how caregivers' mental wellbeing evolves across three distinct stages of the caregiving journey. Additionally, our participants shared insights into improvements for existing mental health technologies, emphasizing the need for accessible, scalable, and personalized solutions that adapt to caregivers' changing needs over time. These findings offer a foundation for designing dynamic, stage-sensitive interventions that holistically support caregivers' mental wellbeing, benefiting both caregivers and care recipients.

</details>


### [309] [Controlling Context: Generative AI at Work in Integrated Circuit Design and Other High-Precision Domains](https://arxiv.org/abs/2506.14567)
*Emanuel Moss,Elizabeth Watkins,Christopher Persaud,Passant Karunaratne,Dawn Nafus*

Main category: cs.HC

TL;DR: 本文探讨了生成式AI工具在集成电路设计等高精度工程领域中的应用挑战，指出控制交互上下文是主要难题，并提出通过增强交互式上下文控制来缓解问题。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI工具（如聊天机器人和代码助手）在工程流程中的普及及其准确性提升，研究旨在揭示高精度领域工程师如何保持对错误的警惕，并识别使用此类工具时面临的其他潜在问题。

Method: 通过对集成电路设计领域的硬件/软件工程师及其合作者进行访谈，研究分析了生成式AI工具使用中准确性的作用及其引发的其他问题。

Result: 研究发现工程师面临多方面挑战（如上下文控制困难），并将这些问题映射到生成式AI系统的组件中，指出控制工程师与工具交互的上下文是核心挑战。

Conclusion: 建议通过增强交互式上下文控制能力来缓解问题，强调在工具设计中需优先支持用户对交互环境的动态调整。

Abstract: Generative AI tools have become more prevalent in engineering workflows, particularly through chatbots and code assistants. As the perceived accuracy of these tools improves, questions arise about whether and how those who work in high-precision domains might maintain vigilance for errors, and what other aspects of using such tools might trouble their work. This paper analyzes interviews with hardware and software engineers, and their collaborators, who work in integrated circuit design to identify the role accuracy plays in their use of generative AI tools and what other forms of trouble they face in using such tools. The paper inventories these forms of trouble, which are then mapped to elements of generative AI systems, to conclude that controlling the context of interactions between engineers and the generative AI tools is one of the largest challenges they face. The paper concludes with recommendations for mitigating this form of trouble by increasing the ability to control context interactively.

</details>


### [310] [StreetLens: Enabling Human-Centered AI Agents for Neighborhood Assessment from Street View Imagery](https://arxiv.org/abs/2506.14670)
*Jina Kim,Leeje Jang,Yao-Yi Chiang,Guanyu Wang,Michelle Pasco*

Main category: cs.HC

TL;DR: 传统邻里研究依赖耗时的人工方法，现有技术自动化不足且缺乏适应性。本文提出StreetLens——一种可配置的视觉语言模型工作流，通过嵌入社会科学专业知识，实现高效、可扩展的街景环境评估。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如人工标注、调查）效率低且依赖专家，现有视觉语言模型应用零散、适应性差，需结合领域知识实现灵活、可扩展的自动化分析。

Method: 开发研究者可配置的StreetLens工作流：基于社会科学协议生成问题，检索街景图像，利用视觉语言模型生成从客观特征（车辆数量）到主观感知（混乱程度）的多维度语义注释，并支持整合历史调查数据增强鲁棒性。

Result: 实现可扩展的街景分析工具StreetLens，提供开源Google Colab笔记本，支持公共/自定义数据集，平衡自动化与领域知识引导的灵活性。

Conclusion: StreetLens通过人机协作范式，将领域知识嵌入AI系统，推动邻里研究向高效、可扩展方向转型，同时保持对多样化研究设计和地理环境的适应性。

Abstract: Traditionally, neighborhood studies have employed interviews, surveys, and manual image annotation guided by detailed protocols to identify environmental characteristics, including physical disorder, decay, street safety, and sociocultural symbols, and to examine their impact on developmental and health outcomes. While these methods yield rich insights, they are time-consuming and require intensive expert intervention. Recent technological advances, including vision-language models (VLMs), have begun to automate parts of this process; however, existing efforts are often ad hoc and lack adaptability across research designs and geographic contexts. In this demo paper, we present StreetLens, a human-centered, researcher-configurable workflow that embeds relevant social science expertise in a VLM for scalable neighborhood environmental assessments. StreetLens mimics the process of trained human coders by grounding the analysis in questions derived from established interview protocols, retrieving relevant street view imagery (SVI), and generating a wide spectrum of semantic annotations from objective features (e.g., the number of cars) to subjective perceptions (e.g., the sense of disorder in an image). By enabling researchers to define the VLM's role through domain-informed prompting, StreetLens places domain knowledge at the core of the analysis process. It also supports the integration of prior survey data to enhance robustness and expand the range of characteristics assessed across diverse settings. We provide a Google Colab notebook to make StreetLens accessible and extensible for researchers working with public or custom SVI datasets. StreetLens represents a shift toward flexible, agentic AI systems that work closely with researchers to accelerate and scale neighborhood studies.

</details>


### [311] [Design an Editable Speech-to-Sign-Language Transformer System: A Human-Centered AI Approach](https://arxiv.org/abs/2506.14677)
*Yingchao Li*

Main category: cs.HC

TL;DR: 本文提出了一种基于Transformer和用户可编辑JSON层的实时语音-手语动画系统，通过人机闭环优化提升自然度、可理解性和用户参与，实验证明其有效性和实时性。


<details>
  <summary>Details</summary>
Motivation: 现有手语技术存在用户直接参与度低、中间过程不透明等问题，需开发可解释、用户可调整且实时性强的系统以增强自然表达与用户信任。

Method: 结合流式Conformer编码器与自回归Transformer-MDN解码器生成3D虚拟形象动作，采用透明JSON中间层支持用户编辑，并通过人机闭环反馈持续优化系统。

Result: 20名聋哑用户与5名翻译实验显示，系统使理解度提升23%、自然度提高31%，推理延迟低于20ms/帧，认知负荷降低18%。

Conclusion: 技术架构与用户参与设计的结合，实现了可解释、自适应的手语AI系统，为实时通信与教育应用提供了高可用性解决方案。

Abstract: This paper presents a human-centered, real-time, user-adaptive speech-to-sign language animation system that integrates Transformer-based motion generation with a transparent, user-editable JSON intermediate layer. The framework overcomes key limitations in prior sign language technologies by enabling direct user inspection and modification of sign segments, thus enhancing naturalness, expressiveness, and user agency. Leveraging a streaming Conformer encoder and autoregressive Transformer-MDN decoder, the system synchronizes spoken input into upper-body and facial motion for 3D avatar rendering. Edits and user ratings feed into a human-in-the-loop optimization loop for continuous improvement. Experiments with 20 deaf signers and 5 interpreters show that the editable interface and participatory feedback significantly improve comprehension, naturalness, usability, and trust, while lowering cognitive load. With sub-20 ms per-frame inference on standard hardware, the system is ready for real-time communication and education. This work illustrates how technical and participatory innovation together enable accessible, explainable, and user-adaptive AI for sign language technology.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [312] [Safe Domains of Attraction for Discrete-Time Nonlinear Systems: Characterization and Verifiable Neural Network Estimation](https://arxiv.org/abs/2506.13961)
*Mohamed Serry,Haoyu Li,Ruikun Zhou,Huan Zhang,Jun Liu*

Main category: eess.SY

TL;DR: 本文提出了一种结合Zubov方程与物理信息神经网络的框架，用于精确估计离散时间非线性自治系统在状态约束下的安全吸引域，并通过验证工具实现结果可验证性。


<details>
  <summary>Details</summary>
Motivation: 现有非线性系统吸引域估计方法存在保守性强、仅适用于低维系统的问题，且在考虑状态约束时更具挑战性，亟需开发高精度、可验证的通用框架。

Method: 1. 推导状态约束下Zubov方程，其唯一连续解对应精确安全吸引域；2. 采用物理信息神经网络近似求解方程；3. 设计基于α,β-CROWN/dReal的验证框架实现结果认证。

Result: 数值实验表明，所提框架能有效处理带状态约束的非线性系统，生成的吸引域估计具有高精度且通过形式化验证工具获得可验证性保证。

Conclusion: 该框架通过Zubov方程理论保证与神经网络灵活性的结合，实现了约束非线性系统安全吸引域的准确、可验证估计，为实际工程系统分析提供了新工具。

Abstract: Analysis of nonlinear autonomous systems typically involves estimating domains of attraction, which have been a topic of extensive research interest for decades. Despite that, accurately estimating domains of attraction for nonlinear systems remains a challenging task, where existing methods are conservative or limited to low-dimensional systems. The estimation becomes even more challenging when accounting for state constraints. In this work, we propose a framework to accurately estimate safe (state-constrained) domains of attraction for discrete-time autonomous nonlinear systems. In establishing this framework, we first derive a new Zubov equation, whose solution corresponds to the exact safe domain of attraction. The solution to the aforementioned Zubov equation is shown to be unique and continuous over the whole state space. We then present a physics-informed approach to approximating the solution of the Zubov equation using neural networks. To obtain certifiable estimates of the domain of attraction from the neural network approximate solutions, we propose a verification framework that can be implemented using standard verification tools (e.g., $α,\!β$-CROWN and dReal). To illustrate its effectiveness, we demonstrate our approach through numerical examples concerning nonlinear systems with state constraints.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [313] [Reimagining Target-Aware Molecular Generation through Retrieval-Enhanced Aligned Diffusion](https://arxiv.org/abs/2506.14488)
*Dong Xu,Zhangfan Yang,Ka-chun Wong,Zexuan Zhu,Jiangqiang Li,Junkai Ji*

Main category: q-bio.BM

TL;DR: 本文提出了一种名为READ的检索增强对齐扩散模型，通过结合分子检索增强生成和SE(3)-等变扩散模型，解决了药物设计中几何适配与化学约束的平衡问题。实验表明READ在性能上超越现有生成模型甚至天然配体。


<details>
  <summary>Details</summary>
Motivation: 现有基于受体结构的药物设计方法难以平衡口袋特异性几何适配与严格的化学价/合成约束，导致生成配体的有效性和多样性受限。

Method: READ模型创新性地融合了分子检索增强生成与SE(3)-等变扩散框架：1) 通过对比预训练编码器对齐原子级表征 2) 在推理时检索口袋匹配的支架图嵌入指导逆向扩散过程

Result: 在CBGBench测试中，READ生成的配体在有效性、多样性和形状互补性方面均超越现有SOTA生成模型，部分指标甚至优于天然配体。

Conclusion: 通过协同优化检索机制与扩散过程，READ为基于结构的药物设计提供了更快速可靠的解决方案，证明了两种技术联合优化的有效性。

Abstract: Breakthroughs in high-accuracy protein structure prediction, such as AlphaFold, have established receptor-based molecule design as a critical driver for rapid early-phase drug discovery. However, most approaches still struggle to balance pocket-specific geometric fit with strict valence and synthetic constraints. To resolve this trade-off, a Retrieval-Enhanced Aligned Diffusion termed READ is introduced, which is the first to merge molecular Retrieval-Augmented Generation with an SE(3)-equivariant diffusion model. Specifically, a contrastively pre-trained encoder aligns atom-level representations during training, then retrieves graph embeddings of pocket-matched scaffolds to guide each reverse-diffusion step at inference. This single mechanism can inject real-world chemical priors exactly where needed, producing valid, diverse, and shape-complementary ligands. Experimental results demonstrate that READ can achieve very competitive performance in CBGBench, surpassing state-of-the-art generative models and even native ligands. That suggests retrieval and diffusion can be co-optimized for faster, more reliable structure-based drug design.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [314] [Investigating the Potential of Large Language Model-Based Router Multi-Agent Architectures for Foundation Design Automation: A Task Classification and Expert Selection Study](https://arxiv.org/abs/2506.13811)
*Sompote Youwai,David Phim,Vianne Gayl Murcia,Rianne Clair Onas*

Main category: cs.MA

TL;DR: 研究比较了三种多智能体系统在基础设计自动化中的表现，基于路由器的专家选择方法在浅基础和桩基设计中分别达到95%和90.63%准确率，较传统工作流程提升10-43.75个百分点。Grok 3在独立运算中展现最强数学推理能力，但系统仍需人类监督作为辅助工具。


<details>
  <summary>Details</summary>
Motivation: 针对土木工程基础设计自动化需求，探索通过智能任务分类和专家选择机制提升大语言模型在专业计算中的准确性与可靠性，同时保持工程文档的专业标准。

Method: 采用单智能体、设计-检查双智能体、路由器专家选择三种架构，使用DeepSeek R1/ChatGPT 4 Turbo/Grok 3/Gemini 2.5 Pro作为基线模型，通过双层次分类框架区分基础类型并选择分析方法，在浅基与桩基场景进行对比测试。

Result: 基于路由器的系统在浅基(95%)和桩基(90.63%)设计分别较Grok 3独立性能提升8.75/3.13个百分点，较传统工作流提升10-43.75个百分点。Grok 3在无外部工具时展现最优数学推理能力。

Conclusion: 路由器多智能体系统是基础设计自动化的最优解，其双层次分类机制有效支持工程分析。鉴于土木工程的特殊性，系统应定位为人类工程师的高级计算辅助工具而非替代方案，需持续保持专业监督。

Abstract: This study investigates router-based multi-agent systems for automating foundation design calculations through intelligent task classification and expert selection. Three approaches were evaluated: single-agent processing, multi-agent designer-checker architecture, and router-based expert selection. Performance assessment utilized baseline models including DeepSeek R1, ChatGPT 4 Turbo, Grok 3, and Gemini 2.5 Pro across shallow foundation and pile design scenarios. The router-based configuration achieved performance scores of 95.00% for shallow foundations and 90.63% for pile design, representing improvements of 8.75 and 3.13 percentage points over standalone Grok 3 performance respectively. The system outperformed conventional agentic workflows by 10.0 to 43.75 percentage points. Grok 3 demonstrated superior standalone performance without external computational tools, indicating advances in direct LLM mathematical reasoning for engineering applications. The dual-tier classification framework successfully distinguished foundation types, enabling appropriate analytical approaches. Results establish router-based multi-agent systems as optimal for foundation design automation while maintaining professional documentation standards. Given safety-critical requirements in civil engineering, continued human oversight remains essential, positioning these systems as advanced computational assistance tools rather than autonomous design replacements in professional practice.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [315] [AMLgentex: Mobilizing Data-Driven Research to Combat Money Laundering](https://arxiv.org/abs/2506.13989)
*Johan Östman,Edvin Callisen,Anton Chen,Kristiina Ausmees,Emanuel Gårdh,Jovan Zamac,Jolanta Goldsteine,Hugo Wefer,Simon Whelan,Markus Reimegård*

Main category: cs.SI

TL;DR: 本文提出AMLGentex开源框架，用于生成更贴近现实的交易数据并评估反洗钱检测方法，解决现有合成数据集在结构复杂性、动态行为建模等方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有反洗钱合成数据集无法模拟真实洗钱行为的复杂性（如部分可观测性、策略性行为、时序动态等），导致检测系统在现实场景中评估效果受限。

Method: 开发AMLGentex开源工具包，支持生成可配置的仿真交易数据，并在包含网络级依赖、类别不平衡等真实挑战的受控环境中系统化测试反洗钱方法。

Result: 通过该框架验证了在复杂反洗钱场景下检测方法的有效性，证明其能模拟关键现实挑战（如稀疏标签、时间演化），为方法评估提供标准化基准。

Conclusion: AMLGentex填补了现有反洗钱评估工具的空白，通过高保真仿真数据生成和结构化测试环境，推动更鲁棒的AML系统开发。

Abstract: Money laundering enables organized crime by allowing illicit funds to enter the legitimate economy. Although trillions of dollars are laundered each year, only a small fraction is ever uncovered. This stems from a range of factors, including deliberate evasion by launderers, the rarity of confirmed cases, and the limited visibility each financial institution has into the global transaction network. While several synthetic datasets are available, they fail to model the structural and behavioral complexity of real-world money laundering. In particular, they often overlook partial observability, sparse and uncertain labels, strategic behavior, temporal dynamics, class imbalance, and network-level dependencies. To address these limitations, we present AMLGentex, an open-source suite for generating realistic, configurable transaction data and benchmarking detection methods. It enables systematic evaluation of anti-money laundering (AML) systems in a controlled environment that captures key real-world challenges. We demonstrate how the framework can be used to rigorously evaluate methods under conditions that reflect the complexity of practical AML scenarios.

</details>


### [316] [Density-aware Walks for Coordinated Campaign Detection](https://arxiv.org/abs/2506.13912)
*Atul Anand Gopalakrishnan,Jakir Hossain,Tuğrulcan Elmas,Ahmet Erdem Sarıyüce*

Main category: cs.SI

TL;DR: 本文提出一种基于密度感知结构编码的图神经网络方法，用于检测社交媒体上的协调虚假宣传。通过随机加权游走和Skip-gram生成节点嵌入，结合MPNN模型，在土耳其选举前的Twitter数据集中显著提升了分类准确率。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的协调虚假宣传常通过人为放大话题伪装成自然趋势，误导用户参与。现有图神经网络难以有效识别此类行为，主要因其网络规模大且攻击手段复杂。

Method: 提出随机加权游走（RWW）方法，利用节点度数、核心数或桁架数等局部密度指标生成偏置游走路径，通过Skip-gram模型编码为密度感知节点嵌入，再训练消息传递神经网络（MPNN）进行分类。

Result: 在LEN数据集上，该方法在二元分类和多分类任务中分别实现12%和5%的准确率提升，显著优于传统图神经网络。

Conclusion: 结合密度感知结构编码与MPNN的框架能有效识别Twitter等社交媒体中的协调虚假行为，为检测复杂网络攻击提供了新思路。

Abstract: Coordinated campaigns frequently exploit social media platforms by artificially amplifying topics, making inauthentic trends appear organic, and misleading users into engagement. Distinguishing these coordinated efforts from genuine public discourse remains a significant challenge due to the sophisticated nature of such attacks. Our work focuses on detecting coordinated campaigns by modeling the problem as a graph classification task. We leverage the recently introduced Large Engagement Networks (LEN) dataset, which contains over 300 networks capturing engagement patterns from both fake and authentic trends on Twitter prior to the 2023 Turkish elections. The graphs in LEN were constructed by collecting interactions related to campaigns that stemmed from ephemeral astroturfing. Established graph neural networks (GNNs) struggle to accurately classify campaign graphs, highlighting the challenges posed by LEN due to the large size of its networks. To address this, we introduce a new graph classification method that leverages the density of local network structures. We propose a random weighted walk (RWW) approach in which node transitions are biased by local density measures such as degree, core number, or truss number. These RWWs are encoded using the Skip-gram model, producing density-aware structural embeddings for the nodes. Training message-passing neural networks (MPNNs) on these density-aware embeddings yields superior results compared to the simpler node features available in the dataset, with nearly a 12\% and 5\% improvement in accuracy for binary and multiclass classification, respectively. Our findings demonstrate that incorporating density-aware structural encoding with MPNNs provides a robust framework for identifying coordinated inauthentic behavior on social media networks such as Twitter.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [317] [NeuroCoreX: An Open-Source FPGA-Based Spiking Neural Network Emulator with On-Chip Learning](https://arxiv.org/abs/2506.14138)
*Ashish Gautam,Prasanna Date,Shruti Kulkarni,Robert Patton,Thomas Potok*

Main category: cs.NE

TL;DR: 本文提出NeuroCoreX，一种基于FPGA的脉冲神经网络（SNN）仿真器，支持灵活拓扑结构和生物启发的局部学习机制，旨在加速高效能生物启发计算的研究。


<details>
  <summary>Details</summary>
Motivation: 传统人工神经网络（ANN）受限于分层架构，而SNN天然支持多样化连接模式（如小世界图），但缺乏灵活且高效的硬件实现平台。开发NeuroCoreX旨在解决这一限制，推动SNN在神经形态硬件上的协同设计与测试。

Method: NeuroCoreX基于FPGA实现，支持全连接和多种网络拓扑，采用LIF神经元模型和基于STDP的局部学习机制，通过UART接口配置参数，并提供Python接口简化部署流程。

Result: NeuroCoreX成为开源框架，支持无架构限制的SNN拓扑设计，集成生物启发的学习规则，并通过硬件-软件协同设计提升能效，为研究提供可扩展的测试平台。

Conclusion: NeuroCoreX通过灵活的硬件仿真和易用接口，降低了SNN研究门槛，为生物启发的高效能计算系统开发提供了重要工具。

Abstract: Spiking Neural Networks (SNNs) are computational models inspired by the structure and dynamics of biological neuronal networks. Their event-driven nature enables them to achieve high energy efficiency, particularly when deployed on neuromorphic hardware platforms. Unlike conventional Artificial Neural Networks (ANNs), which primarily rely on layered architectures, SNNs naturally support a wide range of connectivity patterns, from traditional layered structures to small-world graphs characterized by locally dense and globally sparse connections. In this work, we introduce NeuroCoreX, an FPGA-based emulator designed for the flexible co-design and testing of SNNs. NeuroCoreX supports all-to-all connectivity, providing the capability to implement diverse network topologies without architectural restrictions. It features a biologically motivated local learning mechanism based on Spike-Timing-Dependent Plasticity (STDP). The neuron model implemented within NeuroCoreX is the Leaky Integrate-and-Fire (LIF) model, with current-based synapses facilitating spike integration and transmission . A Universal Asynchronous Receiver-Transmitter (UART) interface is provided for programming and configuring the network parameters, including neuron, synapse, and learning rule settings. Users interact with the emulator through a simple Python-based interface, streamlining SNN deployment from model design to hardware execution. NeuroCoreX is released as an open-source framework, aiming to accelerate research and development in energy-efficient, biologically inspired computing.

</details>


### [318] [A Scalable Hybrid Training Approach for Recurrent Spiking Neural Networks](https://arxiv.org/abs/2506.14464)
*Maximilian Baronig,Yeganeh Bahariasl,Ozan Özdenizci,Robert Legenstein*

Main category: cs.NE

TL;DR: 本文提出HYPR算法，结合并行化与近似在线前向学习，解决了循环脉冲神经网络训练中BPTT方法的内存依赖与在线学习限制，在振荡亚阈值动态神经元模型中实现了接近BPTT的性能。


<details>
  <summary>Details</summary>
Motivation: 传统BPTT训练方法存在内存线性增长、无法在线学习的缺陷，而前向梯度学习方法虽支持在线学习但性能较低。需开发兼具高效并行化与在线学习能力的新算法。

Method: 提出HYPR混合传播算法：通过子序列参数更新并行化实现高吞吐量在线学习，保持内存需求与序列长度无关，支持几乎任意非线性脉冲神经元模型的RSNN训练。

Result: 在具有亚阈值振荡动态的脉冲神经网络中，HYPR展现出前所未有的低任务性能差距（与BPTT相比仅差0.1%），且内存消耗与时间步长无关。

Conclusion: HYPR通过融合并行化与近似前向学习，突破了传统梯度传播方法的限制，为神经形态硬件上的在线持续学习提供了高效解决方案，特别适用于具有复杂动态的脉冲网络。

Abstract: Recurrent spiking neural networks (RSNNs) can be implemented very efficiently in neuromorphic systems. Nevertheless, training of these models with powerful gradient-based learning algorithms is mostly performed on standard digital hardware using Backpropagation through time (BPTT). However, BPTT has substantial limitations. It does not permit online training and its memory consumption scales linearly with the number of computation steps. In contrast, learning methods using forward propagation of gradients operate in an online manner with a memory consumption independent of the number of time steps. These methods enable SNNs to learn from continuous, infinite-length input sequences. Yet, slow execution speed on conventional hardware as well as inferior performance has hindered their widespread application. In this work, we introduce HYbrid PRopagation (HYPR) that combines the efficiency of parallelization with approximate online forward learning. Our algorithm yields high-throughput online learning through parallelization, paired with constant, i.e., sequence length independent, memory demands. HYPR enables parallelization of parameter update computation over the sub sequences for RSNNs consisting of almost arbitrary non-linear spiking neuron models. We apply HYPR to networks of spiking neurons with oscillatory subthreshold dynamics. We find that this type of neuron model is particularly well trainable by HYPR, resulting in an unprecedentedly low task performance gap between approximate forward gradient learning and BPTT.

</details>


### [319] [Is Selection All You Need in Differential Evolution?](https://arxiv.org/abs/2506.14425)
*Tomofumi Kitamura,Alex Fukunaga*

Main category: cs.NE

TL;DR: 提出无界差分进化算法(UDE)，通过保留所有生成个体、取消世代替换机制，解决传统DE因固定种群大小导致的多样性不足及存档管理复杂性问题。


<details>
  <summary>Details</summary>
Motivation: 传统差分进化算法(DE)因固定种群大小导致种群多样性受限，而引入存档机制会增加插入/删除策略等设计负担。需在有限评估预算下平衡探索与开发，并简化算法结构。

Method: UDE框架取消世代替换，将所有生成个体加入种群而不基于适应度淘汰个体，仅依赖选择机制，避免动态调整种群大小和存档管理的复杂性。

Result: UDE通过无限增长种群维持多样性，提供更直接且强大的搜索能力，无需处理传统DE的存档策略或动态种群调整问题。

Conclusion: UDE颠覆传统DE的替换逻辑，通过无界种群与纯选择机制实现更简洁高效的搜索范式，为DE算法设计开辟新方向。

Abstract: Differential Evolution (DE) is a widely used evolutionary algorithm for black-box optimization problems. However, in modern DE implementations, a major challenge lies in the limited population diversity caused by the fixed population size enforced by the generational replacement. Population size is a critical control parameter that significantly affects DE performance. Larger populations inherently contain a more diverse set of individuals, thereby facilitating broader exploration of the search space. Conversely, when the maximum evaluation budgets is constrained, smaller populations focusing on a limited number of promising candidates may be more suitable. Many state-of-the-art DE variants incorporate an archive mechanism, in which a subset of discarded individuals is preserved in an archive during generation replacement and reused in mutation operations. However, maintaining what is essentially a secondary population via an archive introduces additional design considerations, such as policies for insertion, deletion, and appropriate sizing. To address these limitations, we propose a novel DE framework called Unbounded Differential Evolution (UDE), which adds all generated candidates to the population without discarding any individual based on fitness. Unlike conventional DE, which removes inferior individuals during generational replacement, UDE eliminates replacement altogether, along with the associated complexities of archive management and dynamic population sizing. UDE represents a fundamentally new approach to DE, relying solely on selection mechanisms and enabling a more straightforward yet powerful search algorithm.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [320] [Agile Orchestration at Will: An Entire Smart Service-Based Security Architecture Towards 6G](https://arxiv.org/abs/2505.22963)
*Zhuoran Duan,Guoshun Nan,Rushan Li,Zijun Wang,Lihua Xiong,Chaoying Yuan,Guorong Liu,Hui Xu,Qimei Cui,Xiaofeng Tao,Tony Q. S. Quek*

Main category: cs.NI

TL;DR: 本文提出了一种新型6G网络安全架构ES3A，通过分层、灵活、可扩展的设计原则及两阶段编排机制，实现服务化安全、端到端保护和智能自动化，并在SDR系统上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 6G网络因高度异构化及多样化安全需求面临已知/未知威胁，现有安全架构无法满足动态保护需求，需重新设计以应对挑战。

Method: 提出ES3A架构，基于层次性、灵活性等六项原则，构建三层三域结构，采用两阶段编排机制动态生成安全策略，并通过服务化安全与智能自动化实现定制化防护。

Result: 在软件定义无线电（SDR）系统上构建原型，实验证明ES3A有效，并通过案例展示其在动态6G网络中的优越性。

Conclusion: ES3A通过智能服务化架构与动态安全策略，解决了6G网络的高动态安全防护难题，为未来网络设计提供了创新方向与实践验证。

Abstract: The upcoming 6G will fundamentally reshape mobile networks beyond communications, unlocking a multitude of applications that were once considered unimaginable. Meanwhile, security and resilience are especially highlighted in the 6G design principles. However, safeguarding 6G networks will be quite challenging due to various known and unknown threats from highly heterogeneous networks and diversified security requirements of distinct use cases, calling for a comprehensive re-design of security architecture. This motivates us to propose ES3A (Entire Smart Service-based Security Architecture), a novel security architecture for 6G networks. Specifically, we first discuss six high-level principles of our ES3A that include hierarchy, flexibility, scalability, resilience, endogeny, and trust and privacy. With these goals in mind, we then introduce three guidelines from a deployment perspective, envisioning our ES3A that offers service-based security, end-to-end protection, and smart security automation for 6G networks. Our architecture consists of three layers and three domains. It relies on a two-stage orchestration mechanism to tailor smart security strategies for customized protection in high-dynamic 6G networks, thereby addressing the aforementioned challenges. Finally, we prototype the proposed ES3A on a real-world radio system based on Software-Defined Radio (SDR). Experiments show the effectiveness of our ES3A. We also provide a case to show the superiority of our architecture.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [321] [Complete Characterization for Adjustment in Summary Causal Graphs of Time Series](https://arxiv.org/abs/2506.14534)
*Clément Yvernes,Emilie Devijver,Eric Gaussier*

Main category: math.ST

TL;DR: 本文研究时间序列中多干预下的因果效应可识别性问题，提出在摘要因果图可用时，通过必要和充分条件验证调整标准的完备性，并设计伪线性算法判断可识别性。


<details>
  <summary>Details</summary>
Motivation: 在时间序列因果推断中，当仅能获得真实因果图的抽象表示（摘要因果图）时，需解决如何从观测数据中估计总因果效应的问题。现有方法在此情境下的可识别性条件尚不明确。

Method: 提出针对调整标准的必要和充分条件，证明其在摘要因果图下的完备性，并开发一种伪线性算法以判定因果查询是否可识别。

Result: 调整标准在摘要因果图场景下被证明是完备的，所提算法能有效判断多干预时间序列的可识别性，为因果效应估计提供理论保障。

Conclusion: 该研究为基于摘要因果图的时间序列因果推断建立了可识别性框架，结合理论与算法工具，扩展了因果效应估计在复杂动态系统中的应用范围。

Abstract: The identifiability problem for interventions aims at assessing whether the total causal effect can be written with a do-free formula, and thus be estimated from observational data only. We study this problem, considering multiple interventions, in the context of time series when only an abstraction of the true causal graph, in the form of a summary causal graph, is available. We propose in particular both necessary and sufficient conditions for the adjustment criterion, which we show is complete in this setting, and provide a pseudo-linear algorithm to decide whether the query is identifiable or not.

</details>
