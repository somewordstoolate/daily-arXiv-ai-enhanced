<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 107]
- [cs.AI](#cs.AI) [Total: 44]
- [cs.LG](#cs.LG) [Total: 126]
- [cs.IR](#cs.IR) [Total: 4]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [math.NA](#math.NA) [Total: 1]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.MM](#cs.MM) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [eess.AS](#eess.AS) [Total: 7]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.IT](#cs.IT) [Total: 2]
- [stat.ML](#stat.ML) [Total: 14]
- [cs.CR](#cs.CR) [Total: 9]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.SD](#cs.SD) [Total: 11]
- [stat.AP](#stat.AP) [Total: 1]
- [math.AT](#math.AT) [Total: 1]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.CV](#cs.CV) [Total: 38]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.DC](#cs.DC) [Total: 12]
- [stat.ME](#stat.ME) [Total: 3]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cs.GT](#cs.GT) [Total: 3]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [cs.RO](#cs.RO) [Total: 10]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Research on Medical Named Entity Identification Based On Prompt-Biomrc Model and Its Application in Intelligent Consultation System](https://arxiv.org/abs/2506.01961)
*Jinzhu Yang*

Main category: cs.CL

TL;DR: 该研究提出Prompt-bioMRC模型，结合硬模板和软提示设计，提升医疗领域命名实体识别（NER）的精度和效率，实验证明其优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 近年来，大规模模型的出现推动了NER任务的显著进展，尤其是在医疗领域，BioBERT等模型的应用提升了医疗文本的NER能力。本研究旨在通过提示学习方法进一步提升医疗实体识别的性能。

Method: 研究提出Prompt-bioMRC模型，整合了硬模板和软提示设计，以优化医疗实体识别的精度和效率。

Result: 在多个医疗数据集上的实验表明，该方法 consistently 优于传统模型，验证了其有效性。

Conclusion: 该研究通过先进的NER技术，为自动化医疗数据处理提供了可靠支持，有助于提升医疗信息提取的准确性和医疗决策的效率。

Abstract: This study is dedicated to exploring the application of prompt learning
methods to advance Named Entity Recognition (NER) within the medical domain. In
recent years, the emergence of large-scale models has driven significant
progress in NER tasks, particularly with the introduction of the BioBERT
language model, which has greatly enhanced NER capabilities in medical texts.
Our research introduces the Prompt-bioMRC model, which integrates both hard
template and soft prompt designs aimed at refining the precision and efficiency
of medical entity recognition. Through extensive experimentation across diverse
medical datasets, our findings consistently demonstrate that our approach
surpasses traditional models. This enhancement not only validates the efficacy
of our methodology but also highlights its potential to provide reliable
technological support for applications like intelligent diagnosis systems. By
leveraging advanced NER techniques, this study contributes to advancing
automated medical data processing, facilitating more accurate medical
information extraction, and supporting efficient healthcare decision-making
processes.

</details>


### [2] [No Free Lunch in Active Learning: LLM Embedding Quality Dictates Query Strategy Success](https://arxiv.org/abs/2506.01992)
*Lukas Rauch,Moritz Wirth,Denis Huseljic,Marek Herde,Bernhard Sick,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 该研究通过利用冻结的大型语言模型（LLM）嵌入，重新审视深度主动学习（AL）的实用性，并系统性地研究了LLM嵌入质量对查询策略的影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于通过利用冻结的LLM嵌入来减少深度主动学习中迭代微调大型模型的巨大计算成本，并探讨嵌入质量如何影响主动学习策略的效果。

Method: 研究采用了来自大规模文本嵌入基准（MTEB）排行榜的五个表现最佳的模型和两个基线模型，在十个不同的文本分类任务上进行实验，比较了不同查询策略的效果。

Result: 研究发现，基于多样性的初始标注池采样与高质量嵌入协同作用，提升了早期主动学习迭代的性能；同时，最优查询策略的选择对嵌入质量敏感，Badge等策略在不同任务中表现更稳健。

Conclusion: 研究结论强调了在评估主动学习策略时需要考虑上下文特定性，因为性能高度依赖于嵌入质量和目标任务。

Abstract: The advent of large language models (LLMs) capable of producing
general-purpose representations lets us revisit the practicality of deep active
learning (AL): By leveraging frozen LLM embeddings, we can mitigate the
computational costs of iteratively fine-tuning large backbones. This study
establishes a benchmark and systematically investigates the influence of LLM
embedding quality on query strategies in deep AL. We employ five top-performing
models from the massive text embedding benchmark (MTEB) leaderboard and two
baselines for ten diverse text classification tasks. Our findings reveal key
insights: First, initializing the labeled pool using diversity-based sampling
synergizes with high-quality embeddings, boosting performance in early AL
iterations. Second, the choice of the optimal query strategy is sensitive to
embedding quality. While the computationally inexpensive Margin sampling can
achieve performance spikes on specific datasets, we find that strategies like
Badge exhibit greater robustness across tasks. Importantly, their effectiveness
is often enhanced when paired with higher-quality embeddings. Our results
emphasize the need for context-specific evaluation of AL strategies, as
performance heavily depends on embedding quality and the target task.

</details>


### [3] [NovelHopQA: Diagnosing Multi-Hop Reasoning Failures in Long Narrative Contexts](https://arxiv.org/abs/2506.02000)
*Abhay Gupta,Michael Lu,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.CL

TL;DR: 论文提出了NovelHopQA基准，用于评估大语言模型在长上下文和多跳推理任务中的表现，发现即使先进模型在增加推理步数和上下文长度时准确率也会下降。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型在处理需要跨越数万个标记的多跳推理问题时表现不佳。现有基准测试要么只关注长上下文理解，要么只关注多跳推理，缺乏在自然叙事环境中同时考察两者的研究。

Method: 研究团队引入了NovelHopQA基准，包含83部公版小说的64k-128k标记片段，构建了基于连贯故事情节的多跳问题链，并通过关键词引导的流程确保问题可回答。评估了6种先进模型，并进行了人工验证。

Result: 实验结果显示，随着推理步数和上下文长度的增加，所有模型的准确率均出现下降，表明单纯扩大模型规模并不能保证强大的推理能力。失败模式分析揭示了常见的错误类型，如最终跳整合失败和长距离漂移。

Conclusion: NovelHopQA为大规模多跳推理提供了可控的诊断环境，揭示了当前模型的局限性，为未来研究提供了方向。

Abstract: Current large language models (LLMs) struggle to answer questions that span
tens of thousands of tokens, especially when multi-hop reasoning is involved.
While prior benchmarks explore long-context comprehension or multi-hop
reasoning in isolation, none jointly vary context length and reasoning depth in
natural narrative settings. We introduce NovelHopQA, the first benchmark to
evaluate k1-4 hop QA over 64k-128k-token excerpts from 83 full-length
public-domain novels. A keyword-guided pipeline builds hop-separated chains
grounded in coherent storylines. We evaluate six state-of-the-art (SOTA) models
and apply oracle-context filtering to ensure all questions are genuinely
answerable. Human annotators validate both alignment and hop depth. We noticed
consistent accuracy drops with increased hops and context length, even in
frontier models-revealing that sheer scale does not guarantee robust reasoning.
Our failure mode analysis highlights common breakdowns, such as missed
final-hop integration and long-range drift. NovelHopQA offers a controlled
diagnostic setting to stress-test multi-hop reasoning at scale.

</details>


### [4] [Pruning for Performance: Efficient Idiom and Metaphor Classification in Low-Resource Konkani Using mBERT](https://arxiv.org/abs/2506.02005)
*Timothy Do,Pranav Saran,Harshita Poojary,Pranav Prabhu,Sean O'Brien,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种结合mBERT、双向LSTM和线性分类器的混合模型，用于低资源语言（如Konkani）中的比喻语言分类，并通过注意力头剪枝策略提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言处理系统在处理低资源语言（如Konkani）中的比喻语言表达时面临的挑战。

Method: 采用预训练的多语言BERT（mBERT）结合双向LSTM和线性分类器的混合模型，并应用基于梯度的注意力头剪枝策略。

Result: 在比喻分类任务中达到78%的准确率，在现有习语分类任务中达到83%的准确率。

Conclusion: 注意力头剪枝策略能有效提升低资源语言NLP工具的效率。

Abstract: In this paper, we address the persistent challenges that figurative language
expressions pose for natural language processing (NLP) systems, particularly in
low-resource languages such as Konkani. We present a hybrid model that
integrates a pre-trained Multilingual BERT (mBERT) with a bidirectional LSTM
and a linear classifier. This architecture is fine-tuned on a newly introduced
annotated dataset for metaphor classification, developed as part of this work.
To improve the model's efficiency, we implement a gradient-based attention head
pruning strategy. For metaphor classification, the pruned model achieves an
accuracy of 78%. We also applied our pruning approach to expand on an existing
idiom classification task, achieving 83% accuracy. These results demonstrate
the effectiveness of attention head pruning for building efficient NLP tools in
underrepresented languages.

</details>


### [5] [Enhancing Paraphrase Type Generation: The Impact of DPO and RLHF Evaluated with Human-Ranked Data](https://arxiv.org/abs/2506.02018)
*Christopher Lee Lübbers*

Main category: cs.CL

TL;DR: 该研究通过人类排名数据集和直接偏好优化(DPO)方法，提升了语义改写类型的生成质量，使其更符合人类偏好。


<details>
  <summary>Details</summary>
Motivation: 现有语义改写类型生成方法因依赖自动化指标和有限的人工标注数据，常与人类偏好不一致，影响语义保真度和语言转换效果。

Method: 利用人类排名的语义改写类型数据集，并整合直接偏好优化(DPO)方法，使模型输出更符合人类判断。

Result: DPO训练使语义改写类型生成准确率提升3个百分点，人类偏好评分提高7个百分点；新建的数据集支持更严格的评估；改写类型检测模型F1分数表现优异。

Conclusion: 偏好数据和DPO训练能生成更可靠、语义准确的改写结果，为基于人类标准的评估奠定基础，推动用户对齐的语言生成研究。

Abstract: Paraphrasing re-expresses meaning to enhance applications like text
simplification, machine translation, and question-answering. Specific
paraphrase types facilitate accurate semantic analysis and robust language
models. However, existing paraphrase-type generation methods often misalign
with human preferences due to reliance on automated metrics and limited
human-annotated training data, obscuring crucial aspects of semantic fidelity
and linguistic transformations.
  This study addresses this gap by leveraging a human-ranked paraphrase-type
dataset and integrating Direct Preference Optimization (DPO) to align model
outputs directly with human judgments. DPO-based training increases
paraphrase-type generation accuracy by 3 percentage points over a supervised
baseline and raises human preference ratings by 7 percentage points. A newly
created human-annotated dataset supports more rigorous future evaluations.
Additionally, a paraphrase-type detection model achieves F1 scores of 0.91 for
addition/deletion, 0.78 for same polarity substitution, and 0.70 for
punctuation changes.
  These findings demonstrate that preference data and DPO training produce more
reliable, semantically accurate paraphrases, enabling downstream applications
such as improved summarization and more robust question-answering. The PTD
model surpasses automated metrics and provides a more reliable framework for
evaluating paraphrase quality, advancing paraphrase-type research toward
richer, user-aligned language generation and establishing a stronger foundation
for future evaluations grounded in human-centric criteria.

</details>


### [6] [ChatCFD: an End-to-End CFD Agent with Domain-specific Structured Thinking](https://arxiv.org/abs/2506.02019)
*E Fan,Weizong Wang,Tianhan Zhang*

Main category: cs.CL

TL;DR: ChatCFD是一个基于大语言模型的自动化CFD工作流工具，通过自然语言简化OpenFOAM仿真配置，无需深厚专业知识即可复现复杂CFD结果。


<details>
  <summary>Details</summary>
Motivation: 传统CFD仿真操作复杂且依赖专家经验，限制了科学和工程应用。本文旨在通过自然语言交互降低CFD的使用门槛。

Method: 开发ChatCFD管道，整合OpenFOAM知识与大语言模型，通过结构化数据库构建、配置验证和错误反思实现自动化工作流。

Result: 验证显示ChatCFD能自主复现已发表CFD结果，处理超出基础案例的复杂新配置，表现优于通用语言模型。

Conclusion: ChatCFD显著提升了CFD仿真的可访问性和自动化水平，为非专家用户提供了高效工具。

Abstract: Computational Fluid Dynamics (CFD) is essential for scientific and
engineering advancements but is limited by operational complexity and the need
for extensive expertise. This paper presents ChatCFD, a large language
model-driven pipeline that automates CFD workflows within the OpenFOAM
framework. It enables users to configure and execute complex simulations from
natural language prompts or published literature with minimal expertise. The
innovation is its structured approach to database construction, configuration
validation, and error reflection, integrating CFD and OpenFOAM knowledge with
general language models to improve accuracy and adaptability. Validation shows
ChatCFD can autonomously reproduce published CFD results, handling complex,
unseen configurations beyond basic examples, a task challenging for general
language models.

</details>


### [7] [FinS-Pilot: A Benchmark for Online Financial System](https://arxiv.org/abs/2506.02037)
*Feng Wang,Yiding Sun,Jiaxin Mao,Wei Xue,Danqing Xu*

Main category: cs.CL

TL;DR: 论文提出了FinS-Pilot基准，用于评估金融领域的RAG系统，解决了数据保密和动态数据整合的问题。


<details>
  <summary>Details</summary>
Motivation: 当前金融RAG基准的开发受到数据保密和缺乏动态数据整合的限制，需要专门的评估工具。

Method: 构建FinS-Pilot基准，结合实时API数据和结构化文本，覆盖关键金融领域，如股票分析和宏观经济预测。

Result: 通过实验验证FinS-Pilot能有效识别适合金融应用的模型，填补了金融领域专用评估工具的空白。

Conclusion: FinS-Pilot为金融NLP系统研究提供了实用的评估框架和精选数据集，推动了该领域的发展。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
various professional domains, with their performance typically evaluated
through standardized benchmarks. However, the development of financial RAG
benchmarks has been constrained by data confidentiality issues and the lack of
dynamic data integration. To address this issue, we introduces FinS-Pilot, a
novel benchmark for evaluating RAG systems in online financial applications.
Constructed from real-world financial assistant interactions, our benchmark
incorporates both real-time API data and structured text sources, organized
through an intent classification framework covering critical financial domains
such as equity analysis and macroeconomic forecasting. The benchmark enables
comprehensive evaluation of financial assistants' capabilities in handling both
static knowledge and time-sensitive market information. Through systematic
experiments with multiple Chinese leading LLMs, we demonstrate FinS-Pilot's
effectiveness in identifying models suitable for financial applications while
addressing the current gap in specialized evaluation tools for the financial
domain. Our work contributes both a practical evaluation framework and a
curated dataset to advance research in financial NLP systems. The code and
dataset are accessible on
GitHub\footnote{https://github.com/PhealenWang/financial\_rag\_benchmark}.

</details>


### [8] [Enhancing Multimodal Continual Instruction Tuning with BranchLoRA](https://arxiv.org/abs/2506.02041)
*Duzhen Zhang,Yong Ren,Zhong-Zhi Li,Yahan Yu,Jiahua Dong,Chenxing Li,Zhilong Ji,Jinfeng Bai*

Main category: cs.CL

TL;DR: 论文提出了BranchLoRA框架，通过非对称设计和任务专用路由优化多模态持续指令微调，显著减少灾难性遗忘并提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于MoE LoRA的方法在持续指令微调中存在参数效率低下和灾难性遗忘问题，需改进框架以提升性能与稳定性。

Method: 提出BranchLoRA框架：1) 非对称分支结构实现任务内知识专精与任务间协作；2) 动态任务路由机制优化分支分配；3) 免任务标识的推理任务选择器。

Result: 实验表明BranchLoRA在MCIT基准上显著优于MoELoRA，且在不同规模MLLM中保持性能优势。

Conclusion: BranchLoRA通过结构创新有效解决了持续学习中的遗忘问题，为多模态大模型持续对齐提供了高效方案。

Abstract: Multimodal Continual Instruction Tuning (MCIT) aims to finetune Multimodal
Large Language Models (MLLMs) to continually align with human intent across
sequential tasks. Existing approaches often rely on the Mixture-of-Experts
(MoE) LoRA framework to preserve previous instruction alignments. However,
these methods are prone to Catastrophic Forgetting (CF), as they aggregate all
LoRA blocks via simple summation, which compromises performance over time. In
this paper, we identify a critical parameter inefficiency in the MoELoRA
framework within the MCIT context. Based on this insight, we propose
BranchLoRA, an asymmetric framework to enhance both efficiency and performance.
To mitigate CF, we introduce a flexible tuning-freezing mechanism within
BranchLoRA, enabling branches to specialize in intra-task knowledge while
fostering inter-task collaboration. Moreover, we incrementally incorporate
task-specific routers to ensure an optimal branch distribution over time,
rather than favoring the most recent task. To streamline inference, we
introduce a task selector that automatically routes test inputs to the
appropriate router without requiring task identity. Extensive experiments on
the latest MCIT benchmark demonstrate that BranchLoRA significantly outperforms
MoELoRA and maintains its superiority across various MLLM sizes.

</details>


### [9] [Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?](https://arxiv.org/abs/2506.02058)
*Xiang Li,Jiayi Xin,Qi Long,Weijie J. Su*

Main category: cs.CL

TL;DR: 论文提出KnowSum框架，通过量化未观察到的知识来更全面评估大语言模型的能力，解决了现有评估方法忽视未观察知识的问题。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型（LLM）的评估往往未能全面反映其实际能力，主要原因是忽视了模型中编码但未被直接观察到的知识（未观察知识）。这种评估危机促使研究者开发新方法以更准确地评估LLM。

Method: 论文引入KnowSum统计框架，通过外推观察到的知识实例的出现频率来估计未观察部分的知识，从而提供更全面的评估。

Result: 实验表明，仅依赖观察到的LLM性能会遗漏大量知识。KnowSum在估计总知识、评估信息检索效果和测量输出多样性方面表现出色，并显著改变了常见LLM的排名。

Conclusion: KnowSum框架有效解决了LLM评估中未观察知识的问题，为更全面评估模型能力提供了新工具，揭示了现有评估方法的局限性。

Abstract: Accurate evaluation of large language models (LLMs) is crucial for
understanding their capabilities and guiding their development. However,
current evaluations often inconsistently reflect the actual capacities of these
models. In this paper, we demonstrate that one of many contributing factors to
this \textit{evaluation crisis} is the oversight of unseen knowledge --
information encoded by LLMs but not directly observed or not yet observed
during evaluations. We introduce KnowSum, a statistical framework designed to
provide a more comprehensive assessment by quantifying the unseen knowledge for
a class of evaluation tasks. KnowSum estimates the unobserved portion by
extrapolating from the appearance frequencies of observed knowledge instances.
We demonstrate the effectiveness and utility of KnowSum across three critical
applications: estimating total knowledge, evaluating information retrieval
effectiveness, and measuring output diversity. Our experiments reveal that a
substantial volume of knowledge is omitted when relying solely on observed LLM
performance. Importantly, KnowSum yields significantly different comparative
rankings for several common LLMs based on their internal knowledge.

</details>


### [10] [Knowledge or Reasoning? A Close Look at How LLMs Think Across Domains](https://arxiv.org/abs/2506.02126)
*Juncheng Wu,Sheng Liu,Haoqin Tu,Hang Yu,Xiaoke Huang,James Zou,Cihang Xie,Yuyin Zhou*

Main category: cs.CL

TL;DR: 该研究提出细粒度评估框架，分析LLM在医疗和数学领域的推理过程，发现监督微调虽提升最终答案准确率但损害推理质量，而强化学习能优化医疗推理路径。


<details>
  <summary>Details</summary>
Motivation: 当前增强推理能力的大语言模型（如OpenAI-o1/3和DeepSeek-R1）在复杂任务上表现优异，但其内部推理过程的质量和透明度尚未充分研究。本文旨在超越最终答案准确性，深入分析逐步推理过程。

Method: 通过将思维轨迹分解为知识和推理两部分，引入细粒度评估框架：用知识指数（KI）衡量知识正确性，用信息增益（InfoGain）评估推理质量。基于此框架对比分析了R1蒸馏模型和Qwen基座模型在监督微调（SFT）及强化学习（RL）训练下的表现。

Result: 三项关键发现：1）R1蒸馏模型的通用推理能力无法通过SFT或RL有效迁移至医疗领域；2）SFT虽提升两领域最终答案准确率，但平均导致推理质量下降38.9%（医疗领域仍需SFT因依赖领域知识）；3）RL通过剪枝错误/无关知识提升医疗推理的准确性和知识正确性。

Conclusion: 研究表明不同训练方式对推理能力的影响具有领域特异性，需针对性优化——医疗领域需结合SFT（知识获取）和RL（推理路径优化），而通用推理能力的迁移存在局限性。

Abstract: Recent advances in reasoning-enhanced Large Language Models such as
OpenAI-o1/3 and DeepSeek-R1 have significantly improved performance on complex
tasks. However, the quality and transparency of their internal reasoning
processes remain underexplored. This work moves beyond the final-answer
accuracy and investigates step-by-step reasoning in the medical and
mathematical domains by explicitly decomposing the thinking trajectories into
two parts: knowledge and reasoning. Specifically, we introduce a fine-grained
evaluation framework that judges: (1) the correctness of knowledge used
(measured by Knowledge Index (KI)) and (2) the quality of reasoning (measured
by Information Gain (InfoGain)). Using this framework, we study R1-distilled
and base Qwen models trained with supervised fine-tuning (SFT) and/or
reinforcement learning (RL) in the medical and math domains. Three intriguing
findings emerge: (1) The general reasoning abilities in R1-distilled models do
not transfer effectively to the medical domain through either SFT or RL. (2)
SFT raises final-answer accuracy in both domains, but often at the cost of
reasoning quality: InfoGain drops by 38.9% on average compared with untrained
models; In the medical domain, however, SFT remains crucial because domain
knowledge is indispensable. (3) RL enhances medical reasoning by pruning
inaccurate or irrelevant knowledge from reasoning paths, thereby improving both
reasoning accuracy and knowledge correctness.

</details>


### [11] [Model Internal Sleuthing: Finding Lexical Identity and Inflectional Morphology in Modern Language Models](https://arxiv.org/abs/2506.02132)
*Michael Li,Nishant Subramani*

Main category: cs.CL

TL;DR: 研究发现，不同规模和架构的Transformer语言模型在编码词汇和形态信息时表现出相似的模式：词汇信息在早期层线性集中，后期层非线性增强；形态信息则始终保持线性可分。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的大语言模型主导了现代NLP，但对其如何编码语言信息的理解仍基于早期模型（如BERT、GPT-2）。本研究旨在探究当代大模型（如Llama-3.1、Gemma-2等）如何处理词汇和形态信息。

Method: 通过在16种不同模型（包括经典架构和当代大模型）的逐层激活上训练线性和非线性分类器，预测词元（lemma）和屈折形态特征。

Result: 所有模型均呈现一致性：词汇信息依赖记忆化编码，早期层线性集中，后期层非线性增强；形态信息则通过可泛化的抽象机制编码，各层均保持线性可分。

Conclusion: 尽管模型在架构、规模和训练方式上存在差异，但语言信息的组织方式高度一致，表明这些特性可能是Transformer模型进行下一词预测的基础，并在预训练早期就已习得。

Abstract: Large transformer-based language models dominate modern NLP, yet our
understanding of how they encode linguistic information is rooted in studies of
early models like BERT and GPT-2. To better understand today's language models,
we investigate how both classical architectures (BERT, DeBERTa, GPT-2)and
contemporary large language models (Pythia, OLMo-2, Gemma-2, Qwen2.5,
Llama-3.1) represent lexical identity and inflectional morphology. We train
linear and nonlinear classifiers on layer-wise activations to predict word
lemmas and inflectional features. We discover that models concentrate lexical
information linearly in early layers and increasingly nonlinearly in later
layers, while keeping inflectional information uniformly accessible and
linearly separable throughout the layers. Further analysis reveals that these
models encode inflectional morphology through generalizable abstractions, but
rely predominantly on memorization to encode lexical identity. Remarkably,
these patterns emerge across all 16 models we test, despite differences in
architecture, size, and training regime (including pretrained and
instruction-tuned variants). This consistency suggests that, despite
substantial advances in LLM technologies, transformer models organize
linguistic information in similar ways, indicating that these properties could
be fundamental for next token prediction and are learned early during
pretraining. Our code is available at
https://github.com/ml5885/model_internal_sleuthing.

</details>


### [12] [BabyLM's First Constructions: Causal interventions provide a signal of learning](https://arxiv.org/abs/2506.02147)
*Joshua Rozner,Leonie Weissweiler,Cory Shain*

Main category: cs.CL

TL;DR: 该研究验证了在发育合理数据量下训练的模型仍能学习多样化构式，且构式表征能力与模型性能相关。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明预训练语言模型能敏感捕捉构式，但其训练数据量远超人类语言习得范围，因此研究发育合理数据量下的构式学习具有重要意义。

Method: 采用Rozner等人的方法，在BabyLM挑战赛的模型上评估构式学习能力。

Result: 即使训练数据量符合发育合理性，模型仍能表征多样构式（包括表面难区分的案例），且构式表现与基准测试成绩呈正相关。

Conclusion: 构式学习在发育合理数据条件下依然可行，且构式表征能力可能具有功能性意义。

Abstract: Construction grammar posits that children acquire constructions (form-meaning
pairings) from the statistics of their environment. Recent work supports this
hypothesis by showing sensitivity to constructions in pretrained language
models (PLMs), including one recent study (Rozner et al., 2025) demonstrating
that constructions shape the PLM's output distribution. However, models under
study have generally been trained on developmentally implausible amounts of
data, casting doubt on their relevance to human language learning. Here we use
Rozner et al.'s methods to evaluate constructional learning in models from the
2024 BabyLM challenge. Our results show that even when trained on
developmentally plausible quantities of data, models represent diverse
constructions, even hard cases that are superficially indistinguishable. We
further find correlational evidence that constructional performance may be
functionally relevant: models that better represent constructions perform
better on the BabyLM benchmarks.

</details>


### [13] [HENT-SRT: Hierarchical Efficient Neural Transducer with Self-Distillation for Joint Speech Recognition and Translation](https://arxiv.org/abs/2506.02157)
*Amir Hussein,Cihan Xiao,Matthew Wiesner,Dan Povey,Leibny Paola Garcia,Sanjeev Khudanpur*

Main category: cs.CL

TL;DR: 提出HENT-SRT框架，通过分层设计、自蒸馏和计算优化，显著提升神经转导器在语音翻译中的性能，缩小与注意力编码器-解码器模型的差距。


<details>
  <summary>Details</summary>
Motivation: 现有神经转导器在语音翻译中面临词序重排困难、联合建模ASR与ST时性能下降及训练成本高的问题，需改进以提升效率与效果。

Method: 采用分层任务分解处理重排问题，结合自蒸馏与CTC一致性正则化保持ST鲁棒性，并通过降采样编码器、无状态预测器等优化计算效率。

Result: 在阿拉伯语、西班牙语和普通话对话数据集上取得NT模型新SOTA，大幅缩小与AED系统的性能差距。

Conclusion: HENT-SRT通过创新设计有效解决了NT在ST中的核心挑战，为实时语音翻译提供了高效解决方案。

Abstract: Neural transducers (NT) provide an effective framework for speech streaming,
demonstrating strong performance in automatic speech recognition (ASR).
However, the application of NT to speech translation (ST) remains challenging,
as existing approaches struggle with word reordering and performance
degradation when jointly modeling ASR and ST, resulting in a gap with
attention-based encoder-decoder (AED) models. Existing NT-based ST approaches
also suffer from high computational training costs. To address these issues, we
propose HENT-SRT (Hierarchical Efficient Neural Transducer for Speech
Recognition and Translation), a novel framework that factorizes ASR and
translation tasks to better handle reordering. To ensure robust ST while
preserving ASR performance, we use self-distillation with CTC consistency
regularization. Moreover, we improve computational efficiency by incorporating
best practices from ASR transducers, including a down-sampled hierarchical
encoder, a stateless predictor, and a pruned transducer loss to reduce training
complexity. Finally, we introduce a blank penalty during decoding, reducing
deletions and improving translation quality. Our approach is evaluated on three
conversational datasets Arabic, Spanish, and Mandarin achieving new
state-of-the-art performance among NT models and substantially narrowing the
gap with AED-based systems.

</details>


### [14] [Different Speech Translation Models Encode and Translate Speaker Gender Differently](https://arxiv.org/abs/2506.02172)
*Dennis Fucci,Marco Gaido,Matteo Negri,Luisa Bentivogli,Andre Martins,Giuseppe Attanasio*

Main category: cs.CL

TL;DR: 研究发现，传统语音翻译模型能捕捉说话者性别信息，而新型适配器架构模型则不能，且后者更倾向于默认男性翻译偏差。


<details>
  <summary>Details</summary>
Motivation: 探讨语音翻译模型是否像语音模型一样能捕捉说话者性别特征，及其对翻译中性别分配的影响。

Method: 使用探测方法评估不同语音翻译模型在三种语言方向（英-法/意/西）中的性别编码能力。

Result: 传统编码器-解码器模型能编码性别信息，但新型适配器架构模型无法做到，且后者表现出更明显的男性默认翻译偏差。

Conclusion: 语音翻译模型架构影响性别信息编码能力，新型架构可能导致更强的性别翻译偏差。

Abstract: Recent studies on interpreting the hidden states of speech models have shown
their ability to capture speaker-specific features, including gender. Does this
finding also hold for speech translation (ST) models? If so, what are the
implications for the speaker's gender assignment in translation? We address
these questions from an interpretability perspective, using probing methods to
assess gender encoding across diverse ST models. Results on three language
directions (English-French/Italian/Spanish) indicate that while traditional
encoder-decoder models capture gender information, newer architectures --
integrating a speech encoder with a machine translation system via adapters --
do not. We also demonstrate that low gender encoding capabilities result in
systems' tendency toward a masculine default, a translation bias that is more
pronounced in newer architectures.

</details>


### [15] [AI Debate Aids Assessment of Controversial Claims](https://arxiv.org/abs/2506.02175)
*Salman Rahman,Sheriff Issaka,Ashima Suvarna,Genglin Liu,James Shiffer,Jaeyoung Lee,Md Rizwan Parvez,Hamid Palangi,Shi Feng,Nanyun Peng,Yejin Choi,Julian Michael,Liwei Jiang,Saadia Gabriel*

Main category: cs.CL

TL;DR: AI辩论能有效提升人类对争议性事实的判断准确性，尤其在公共健康等关键领域，通过双AI辩论机制显著优于单顾问模式。


<details>
  <summary>Details</summary>
Motivation: 随着AI影响力扩大，其可能加剧错误信息和社会分歧，尤其在公共健康等关键领域。研究旨在探索如何通过AI辩论机制克服人类偏见，提升事实判断的准确性。

Method: 研究分为两部分：1) 人类法官（主流或怀疑信念）通过AI辩论或单顾问模式评估COVID-19事实性主张；2) 模拟人类信念的个性化AI法官进行相同任务。

Result: 辩论模式整体提升判断准确性10%，主流信念法官提升15.2%，怀疑派法官提升4.7%。AI法官准确率（78.5%）高于人类（70.1%）和无个性化AI（69.8%）。

Conclusion: AI辩论是一种有前景的监督机制，能结合多样的人类与AI判断，在争议领域更接近真相，为前沿AI模型的监督提供可能。

Abstract: As AI grows more powerful, it will increasingly shape how we understand the
world. But with this influence comes the risk of amplifying misinformation and
deepening social divides-especially on consequential topics like public health
where factual accuracy directly impacts well-being. Scalable Oversight aims to
ensure AI truthfulness by enabling humans to supervise systems that may exceed
human capabilities--yet humans themselves hold different beliefs and biases
that impair their judgment. We study whether AI debate can guide biased judges
toward the truth by having two AI systems debate opposing sides of
controversial COVID-19 factuality claims where people hold strong prior
beliefs. We conduct two studies: one with human judges holding either
mainstream or skeptical beliefs evaluating factuality claims through
AI-assisted debate or consultancy protocols, and a second examining the same
problem with personalized AI judges designed to mimic these different human
belief systems. In our human study, we find that debate-where two AI advisor
systems present opposing evidence-based arguments-consistently improves
judgment accuracy and confidence calibration, outperforming consultancy with a
single-advisor system by 10% overall. The improvement is most significant for
judges with mainstream beliefs (+15.2% accuracy), though debate also helps
skeptical judges who initially misjudge claims move toward accurate views
(+4.7% accuracy). In our AI judge study, we find that AI judges with human-like
personas achieve even higher accuracy (78.5%) than human judges (70.1%) and
default AI judges without personas (69.8%), suggesting their potential for
supervising frontier AI models. These findings highlight AI debate as a
promising path toward scalable, bias-resilient oversight--leveraging both
diverse human and AI judgments to move closer to truth in contested domains.

</details>


### [16] [Echoes of Phonetics: Unveiling Relevant Acoustic Cues for ASR via Feature Attribution](https://arxiv.org/abs/2506.02181)
*Dennis Fucci,Marco Gaido,Matteo Negri,Mauro Cettolo,Luisa Bentivogli*

Main category: cs.CL

TL;DR: 该研究通过特征归因技术分析了现代Conformer-based ASR系统依赖的声学线索，发现模型对元音、摩擦音和爆破音的声学特性有不同侧重。


<details>
  <summary>Details</summary>
Motivation: 尽管ASR技术取得了显著进展，但模型依赖的具体声学线索仍不明确。先前的研究仅限于少量音素和过时模型，因此需要更全面的分析。

Method: 应用特征归因技术，分析现代Conformer-based ASR系统在元音、摩擦音和爆破音上的声学线索，评估其在时域和频域中的表现。

Result: 研究发现，ASR模型更依赖元音的完整时间跨度（尤其是前两个共振峰，男性语音更显著），能更好捕捉摩擦音中的咝音特征，并优先关注爆破音的释放阶段（特别是爆破特性）。

Conclusion: 这些发现增强了ASR模型的可解释性，并指出了未来研究的方向，以揭示模型鲁棒性中的潜在不足。

Abstract: Despite significant advances in ASR, the specific acoustic cues models rely
on remain unclear. Prior studies have examined such cues on a limited set of
phonemes and outdated models. In this work, we apply a feature attribution
technique to identify the relevant acoustic cues for a modern Conformer-based
ASR system. By analyzing plosives, fricatives, and vowels, we assess how
feature attributions align with their acoustic properties in the time and
frequency domains, also essential for human speech perception. Our findings
show that the ASR model relies on vowels' full time spans, particularly their
first two formants, with greater saliency in male speech. It also better
captures the spectral characteristics of sibilant fricatives than non-sibilants
and prioritizes the release phase in plosives, especially burst
characteristics. These insights enhance the interpretability of ASR models and
highlight areas for future research to uncover potential gaps in model
robustness.

</details>


### [17] [BehaviorBox: Automated Discovery of Fine-Grained Performance Differences Between Language Models](https://arxiv.org/abs/2506.02204)
*Lindia Tjuatja,Graham Neubig*

Main category: cs.CL

TL;DR: 提出BehaviorBox方法，通过性能感知的上下文嵌入自动比较语言模型，发现细粒度性能差异。


<details>
  <summary>Details</summary>
Motivation: 语言模型评估困难，现有方法如提示词脆弱、困惑度模糊，需自动发现模型间的可泛化差异。

Method: 使用性能感知的上下文嵌入提取细粒度特征（如特定短语结构），对比两模型生成难易度差异。

Result: 成功识别模型在特定上下文（如条件句were、感叹号使用）中的性能差异，超越语料级困惑度指标。

Conclusion: BehaviorBox能自动发现模型间的细粒度性能差异，为评估提供新视角。

Abstract: Language model evaluation is a daunting task: prompts are brittle,
corpus-level perplexities are vague, and the choice of benchmarks are endless.
Finding examples that show meaningful, generalizable differences between two
LMs is crucial to understanding where one model succeeds and another fails. Can
this process be done automatically? In this work, we propose methodology for
automated comparison of language models that uses performance-aware contextual
embeddings to find fine-grained features of text where one LM outperforms
another. Our method, which we name BehaviorBox, extracts coherent features that
demonstrate differences with respect to the ease of generation between two LMs.
Specifically, BehaviorBox finds features that describe groups of words in
fine-grained contexts, such as "conditional 'were' in the phrase 'if you were'"
and "exclamation marks after emotional statements", where one model outperforms
another within a particular datatset. We apply BehaviorBox to compare models
that vary in size, model family, and post-training, and enumerate insights into
specific contexts that illustrate meaningful differences in performance which
cannot be found by measures such as corpus-level perplexity alone.

</details>


### [18] [Leveraging Natural Language Processing to Unravel the Mystery of Life: A Review of NLP Approaches in Genomics, Transcriptomics, and Proteomics](https://arxiv.org/abs/2506.02212)
*Ella Rannon,David Burstein*

Main category: cs.CL

TL;DR: 本文综述了自然语言处理（NLP）技术在生物序列数据分析中的应用，探讨了从经典方法到先进模型在基因组学、转录组学和蛋白质组学中的适应性及其潜力。


<details>
  <summary>Details</summary>
Motivation: 探讨NLP技术如何从最初的人类语言处理扩展到生物序列分析，以解决基因组学、转录组学和蛋白质组学中的复杂问题。

Method: 回顾了多种NLP方法，包括word2vec、transformer模型和hyena算子，并评估了它们在生物序列数据中的适用性及不同任务的适应性。

Result: 展示了NLP在结构预测、基因表达和进化分析等生物数据应用中的最新进展，证明了这些方法在大规模基因组数据中提取有意义信息的潜力。

Conclusion: 随着语言模型的不断发展，将其整合到生物信息学中，有望深化我们对生命各领域生物过程的理解。

Abstract: Natural Language Processing (NLP) has transformed various fields beyond
linguistics by applying techniques originally developed for human language to
the analysis of biological sequences. This review explores the application of
NLP methods to biological sequence data, focusing on genomics, transcriptomics,
and proteomics. We examine how various NLP methods, from classic approaches
like word2vec to advanced models employing transformers and hyena operators,
are being adapted to analyze DNA, RNA, protein sequences, and entire genomes.
The review also examines tokenization strategies and model architectures,
evaluating their strengths, limitations, and suitability for different
biological tasks. We further cover recent advances in NLP applications for
biological data, such as structure prediction, gene expression, and
evolutionary analysis, highlighting the potential of these methods for
extracting meaningful insights from large-scale genomic data. As language
models continue to advance, their integration into bioinformatics holds immense
promise for advancing our understanding of biological processes in all domains
of life.

</details>


### [19] [Investigating the Impact of Word Informativeness on Speech Emotion Recognition](https://arxiv.org/abs/2506.02239)
*Sofoklis Kakouros*

Main category: cs.CL

TL;DR: 该研究提出利用预训练语言模型提取语义重要词段，并仅在这些词段上计算声学特征，以提高语音情感识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在整个句子或较长语音段上计算声学特征，可能忽略关键细粒度变化。研究旨在通过语义重要词段更精准地捕捉情感相关声学变化。

Method: 结合预训练语言模型筛选语义重要词段，仅在这些词段上计算声学韵律特征及其函数，并采用自监督表示。

Result: 基于词段信息量选择的特征显著提升了情感识别性能。

Conclusion: 聚焦语义重要词段的声学特征计算方法能有效提升情感识别准确率。

Abstract: In emotion recognition from speech, a key challenge lies in identifying
speech signal segments that carry the most relevant acoustic variations for
discerning specific emotions. Traditional approaches compute functionals for
features such as energy and F0 over entire sentences or longer speech portions,
potentially missing essential fine-grained variation in the long-form
statistics. This research investigates the use of word informativeness, derived
from a pre-trained language model, to identify semantically important segments.
Acoustic features are then computed exclusively for these identified segments,
enhancing emotion recognition accuracy. The methodology utilizes standard
acoustic prosodic features, their functionals, and self-supervised
representations. Results indicate a notable improvement in recognition
performance when features are computed on segments selected based on word
informativeness, underscoring the effectiveness of this approach.

</details>


### [20] [CoDial: Interpretable Task-Oriented Dialogue Systems Through Dialogue Flow Alignment](https://arxiv.org/abs/2506.02264)
*Radin Shayanfar,Chu Fei Luo,Rohan Bhambhoria,Samuel Dahan,Xiaodan Zhu*

Main category: cs.CL

TL;DR: 论文提出CoDial框架，通过结构化异构图将专家知识转化为可执行对话逻辑，支持非技术专家快速定义和优化领域专用对话系统。


<details>
  <summary>Details</summary>
Motivation: 由于专家知识、训练数据的高成本和技术难度，构建领域专用对话系统（如法律、医疗、金融）具有挑战性。需要开发非技术专家可轻松使用的框架。

Method: 提出CoDial框架，将专家知识表示为结构化异构图并转换为可执行对话逻辑，兼容现有护栏语言（如Colang），支持零样本任务导向对话系统。

Result: CoDial在STAR数据集上达到最先进性能，在MultiWOZ数据集上与基线模型竞争，并通过人工和LLM反馈实现迭代改进。

Conclusion: CoDial是高风险领域中专家指导LLM对齐的实用工具，支持可解释、可修改的对话系统开发。

Abstract: It is often challenging to teach specialized, unseen tasks to dialogue
systems due to the high cost of expert knowledge, training data, and high
technical difficulty. To support domain-specific applications - such as law,
medicine, or finance - it is essential to build frameworks that enable
non-technical experts to define, test, and refine system behaviour with minimal
effort. Achieving this requires cross-disciplinary collaboration between
developers and domain specialists. In this work, we introduce a novel
framework, CoDial (Code for Dialogue), that converts expert knowledge,
represented as a novel structured heterogeneous graph, into executable
conversation logic. CoDial can be easily implemented in existing guardrailing
languages, such as Colang, to enable interpretable, modifiable, and true
zero-shot specification of task-oriented dialogue systems. Empirically, CoDial
achieves state-of-the-art performance on the STAR dataset for inference-based
models and is competitive with similar baselines on the well-known MultiWOZ
dataset. We also demonstrate CoDial's iterative improvement via manual and
LLM-aided feedback, making it a practical tool for expert-guided alignment of
LLMs in high-stakes domains.

</details>


### [21] [ImpRAG: Retrieval-Augmented Generation with Implicit Queries](https://arxiv.org/abs/2506.02279)
*Wenzheng Zhang,Xi Victoria Lin,Karl Stratos,Wen-tau Yih,Mingda Chen*

Main category: cs.CL

TL;DR: 提出无查询RAG系统ImpRAG，通过统一模型隐式表达信息需求，在8项任务上实现3.6-11.5的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统将检索与生成分离，依赖显式查询，限制了模型跨任务泛化能力。

Method: 将预训练解码器模型分层组，采用两阶段推理流程，共享参数同时优化检索与生成任务。

Result: 在未见过的多样化任务上，Exact Match分数提升3.6-11.5，验证了模型自主表达信息需求的有效性。

Conclusion: 平衡检索与生成参数、利用生成困惑度作为检索目标可显著提升性能，实现跨任务泛化。

Abstract: Retrieval-Augmented Generation (RAG) systems traditionally treat retrieval
and generation as separate processes, requiring explicit textual queries to
connect them. This separation can limit the ability of models to generalize
across diverse tasks. In this work, we propose a query-free RAG system, named
ImpRAG, which integrates retrieval and generation into a unified model. ImpRAG
allows models to implicitly express their information needs, eliminating the
need for human-specified queries. By dividing pretrained decoder-only language
models into specialized layer groups, ImpRAG optimizes retrieval and generation
tasks simultaneously. Our approach employs a two-stage inference process, using
the same model parameters and forward pass for both retrieval and generation,
thereby minimizing the disparity between retrievers and language models.
Experiments on 8 knowledge-intensive tasks demonstrate that ImpRAG achieves
3.6-11.5 improvements in exact match scores on unseen tasks with diverse
formats, highlighting its effectiveness in enabling models to articulate their
own information needs and generalize across tasks. Our analysis underscores the
importance of balancing retrieval and generation parameters and leveraging
generation perplexities as retrieval training objectives for enhanced
performance.

</details>


### [22] [Sounding Like a Winner? Prosodic Differences in Post-Match Interviews](https://arxiv.org/abs/2506.02283)
*Sofoklis Kakouros,Haoyu Chen*

Main category: cs.CL

TL;DR: 该研究通过分析网球赛后采访的韵律特征及自监督学习表示，探索仅凭语音数据判断比赛胜负的可行性。结果表明，韵律变化和深度语音表征能有效区分胜负情绪。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索运动员赛后采访中的韵律特征是否与比赛结果相关，并验证仅通过语音分析自动分类胜负的可行性。

Method: 结合传统声学特征（如音高、强度）和自监督学习模型（Wav2Vec 2.0、HuBERT）提取语音表征，使用机器学习分类器进行胜负判断。

Result: 自监督学习表征能有效捕捉胜负相关的细微语音模式，且韵律特征（如音高变化）仍是胜利的强预测指标。

Conclusion: 赛后采访的语音特征可反映比赛结果，为基于语音的情绪识别和体育分析提供了新思路。

Abstract: This study examines the prosodic characteristics associated with winning and
losing in post-match tennis interviews. Additionally, this research explores
the potential to classify match outcomes solely based on post-match interview
recordings using prosodic features and self-supervised learning (SSL)
representations. By analyzing prosodic elements such as pitch and intensity,
alongside SSL models like Wav2Vec 2.0 and HuBERT, the aim is to determine
whether an athlete has won or lost their match. Traditional acoustic features
and deep speech representations are extracted from the data, and machine
learning classifiers are employed to distinguish between winning and losing
players. Results indicate that SSL representations effectively differentiate
between winning and losing outcomes, capturing subtle speech patterns linked to
emotional states. At the same time, prosodic cues -- such as pitch variability
-- remain strong indicators of victory.

</details>


### [23] [LAM SIMULATOR: Advancing Data Generation for Large Action Model Training via Online Exploration and Trajectory Feedback](https://arxiv.org/abs/2506.02298)
*Thai Hoang,Kung-Hsiang Huang,Shirley Kokane,Jianguo Zhang,Zuxin Liu,Ming Zhu,Jake Grigsby,Tian Lan,Michael S Ryoo,Chien-Sheng Wu,Shelby Heinecke,Huan Wang,Silvio Savarese,Caiming Xiong,Juan Carlos Niebles*

Main category: cs.CL

TL;DR: LAM SIMULATOR框架通过动态任务生成和实时反馈，为大型动作模型（LAMs）提供高质量训练数据，显著提升AI代理性能。


<details>
  <summary>Details</summary>
Motivation: 大型动作模型（LAMs）在多步骤任务中面临高质量训练数据不足的挑战，需要一种高效方法来生成此类数据。

Method: 提出LAM SIMULATOR框架，包含动态任务查询生成器、工具集合和交互环境，使LLM代理能自主探索任务并生成高质量训练数据。

Result: 在ToolBench和CRMArena基准测试中，使用自生成数据训练的模型性能提升高达49.3%，且数据集创建过程需极少人工干预。

Conclusion: LAM SIMULATOR能高效加速AI代理开发，通过自主生成高质量数据显著提升模型性能。

Abstract: Large Action Models (LAMs) for AI Agents offer incredible potential but face
challenges due to the need for high-quality training data, especially for
multi-steps tasks that involve planning, executing tool calls, and responding
to feedback. To address these issues, we present LAM SIMULATOR, a comprehensive
framework designed for online exploration of agentic tasks with high-quality
feedback. Our framework features a dynamic task query generator, an extensive
collection of tools, and an interactive environment where Large Language Model
(LLM) Agents can call tools and receive real-time feedback. This setup enables
LLM Agents to explore and solve tasks autonomously, facilitating the discovery
of multiple approaches to tackle any given task. The resulting action
trajectory data are then used to create high-quality training datasets for
LAMs. Our experiments on popular agentic benchmarks, ToolBench and CRMArena,
highlight the effectiveness of LAM SIMULATOR: models trained with
self-generated datasets using our framework achieve significant performance
gains, up to a 49.3\% improvement over their original baselines. LAM SIMULATOR
requires minimal human input during dataset creation, highlighting LAM
SIMULATOR's efficiency and effectiveness in speeding up development of AI
agents.

</details>


### [24] [Explain-then-Process: Using Grammar Prompting to Enhance Grammatical Acceptability Judgments](https://arxiv.org/abs/2506.02302)
*Russell Scheinberg,Ameeta Agrawal,Amber Shore,So Young Lee*

Main category: cs.CL

TL;DR: 论文提出一种名为'语法提示'的方法，通过让大语言模型首先生成语法规则解释，再将该解释作为额外上下文输入目标模型，显著提升了模型在多语言语法判断任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型能够解释语法规则，但在实际判断句子可接受性时往往无法正确应用这些规则。本文旨在解决这一'知而不行'的差距问题。

Method: 采用'解释-处理'范式：先由大语言模型生成相关语法现象的简明解释，再将此解释作为额外上下文输入目标模型（大模型或小模型），最后进行最小对立句对的语法判断。

Result: 在英语BLiMP、中文SLING和俄语RuBLiMP基准测试中，该方法显著超越基线模型。小模型单独使用语法提示可将与大模型的准确率差距缩小20%，结合思维链技术可缩小56%。

Conclusion: 这种轻量级、语言无关的提示方法能以极低成本帮助小语言模型在多语言场景中接近前沿大模型的性能表现。

Abstract: Large language models (LLMs) can explain grammatical rules, yet they often
fail to apply those rules when judging sentence acceptability. We present
"grammar prompting", an explain-then-process paradigm: a large LLM first
produces a concise explanation of the relevant syntactic phenomenon, then that
explanation is fed back as additional context to the target model -- either an
LLM or a smaller language model (SLM) -- before deciding which sentence of a
minimal pair is grammatical. On the English BLiMP, Chinese SLING, and Russian
RuBLiMP benchmarks, this simple prompt design yields substantial improvements
over strong baselines across many syntactic phenomena. Feeding an LLM's
metalinguistic explanation back to the target model bridges the gap between
knowing a rule and using it. On SLMs, grammar prompting alone trims the average
LLM-SLM accuracy gap by about 20%, and when paired with chain-of-thought, by
56% (13.0 pp -> 5.8 pp), all at negligible cost. The lightweight,
language-agnostic cue lets low-cost SLMs approach frontier-LLM performance in
multilingual settings.

</details>


### [25] [Quantifying Misattribution Unfairness in Authorship Attribution](https://arxiv.org/abs/2506.02321)
*Pegah Alipoormolabashi,Ajay Patel,Niranjan Balasubramanian*

Main category: cs.CL

TL;DR: 论文提出了一种衡量作者归属错误公平性的指标MAUIk，发现现有模型存在不公平性，且与作者在潜在空间中的嵌入位置有关。


<details>
  <summary>Details</summary>
Motivation: 作者归属错误在现实生活尤其是法医环境中可能带来严重后果，现有评估方法未考虑公平性，即候选作者是否面临相同的错误归属风险。

Method: 引入Misattribution Unfairness Index (MAUIk)指标，基于作者在非本人文本中的前k排名频率，评估五种模型在两个数据集上的不公平性。

Result: 所有模型均表现出高度不公平性，部分作者风险更高；不公平性与作者在潜在空间中的嵌入位置相关，靠近中心的作者风险更大。

Conclusion: 研究表明作者归属模型存在潜在危害，需在使用时向终端用户传达并校准错误归属风险。

Abstract: Authorship misattribution can have profound consequences in real life. In
forensic settings simply being considered as one of the potential authors of an
evidential piece of text or communication can result in undesirable scrutiny.
This raises a fairness question: Is every author in the candidate pool at equal
risk of misattribution? Standard evaluation measures for authorship attribution
systems do not explicitly account for this notion of fairness. We introduce a
simple measure, Misattribution Unfairness Index (MAUIk), which is based on how
often authors are ranked in the top k for texts they did not write. Using this
measure we quantify the unfairness of five models on two different datasets.
All models exhibit high levels of unfairness with increased risks for some
authors. Furthermore, we find that this unfairness relates to how the models
embed the authors as vectors in the latent search space. In particular, we
observe that the risk of misattribution is higher for authors closer to the
centroid (or center) of the embedded authors in the haystack. These results
indicate the potential for harm and the need for communicating with and
calibrating end users on misattribution risk when building and providing such
models for downstream use.

</details>


### [26] [Something Just Like TRuST : Toxicity Recognition of Span and Target](https://arxiv.org/abs/2506.02326)
*Berk Atil,Namrata Sureddy,Rebecca J. Passonneau*

Main category: cs.CL

TL;DR: 论文提出TRuST数据集，用于提升毒性内容检测，涵盖多种目标群体，并评估了大语言模型在毒性检测上的表现。


<details>
  <summary>Details</summary>
Motivation: 在线内容中的毒性问题对心理和社会有负面影响，需要更全面的数据集和检测方法来应对。

Method: 整合现有数据集，构建TRuST数据集，包含毒性标签、目标群体和毒性片段，并评估大语言模型的表现。

Result: 微调模型表现优于零样本和少样本提示，但对某些群体效果仍不佳；推理能力未显著提升性能。

Conclusion: 大语言模型在社会推理能力上较弱，需进一步改进毒性检测和目标群体识别。

Abstract: Toxicity in online content, including content generated by language models,
has become a critical concern due to its potential for negative psychological
and social impact. This paper introduces TRuST, a comprehensive dataset
designed to improve toxicity detection that merges existing datasets, and has
labels for toxicity, target social group, and toxic spans. It includes a
diverse range of target groups such as ethnicity, gender, religion, disability,
and politics, with both human/machine-annotated and human machine-generated
data. We benchmark state-of-the-art large language models (LLMs) on toxicity
detection, target group identification, and toxic span extraction. We find that
fine-tuned models consistently outperform zero-shot and few-shot prompting,
though performance remains low for certain social groups. Further, reasoning
capabilities do not significantly improve performance, indicating that LLMs
have weak social reasoning skills.

</details>


### [27] [One Missing Piece for Open-Source Reasoning Models: A Dataset to Mitigate Cold-Starting Short CoT LLMs in RL](https://arxiv.org/abs/2506.02338)
*Hyungjoo Chae,Dongjin Kang,Jihyuk Kim,Beong-woo Kwak,Sunghyun Park,Haeju Park,Jinyoung Yeo,Moontae Lee,Kyungjae Lee*

Main category: cs.CL

TL;DR: 本文提出了一种不依赖现有大型推理模型（如R1）的方法，通过构建长链思维（CoT）数据集来独立开发大型推理模型（LRM）。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型的发展依赖于现有模型（如R1），这限制了该领域的独立进步。本文旨在探索如何在不依赖现有模型的情况下，构建长链思维数据集以推动独立开发。

Method: 作者提出了Long CoT Collection数据集，包含10万条由短链思维LLM标注的CoT推理链。通过开发一种流程，将新颖的推理策略引入短链思维LLM，使其能够进行更长的思考，并控制思维预算以管理过度思考问题。

Result: 实验表明，该数据集的质量与R1相当或略低。使用该数据集训练不仅增强了模型的通用推理能力，还为强化学习提供了坚实基础，模型在RLVR中实现了2-3倍的增益。

Conclusion: 本文展示了不依赖现有大型推理模型构建长链思维数据集的可行性，为独立开发大型推理模型提供了新的方向。

Abstract: With the release of R1, a publicly available large reasoning model (LRM),
researchers commonly train new LRMs by training language models on R1's long
chain-of-thought (CoT) inferences. While prior works show that LRMs'
capabilities can be reproduced through direct distillation, the continued
reliance on the existing models (e.g., R1) remains a critical limitation in
advancing the field. As a first step toward independent LRM development, this
paper explores the possibility of constructing a long CoT dataset with LLMs
that are not trained for inference-time scaling. To this end, we present the
Long CoT Collection, a dataset of 100K CoT rationales annotated using existing
short CoT LLMs. We develop a pipeline that induces o1's novel reasoning
strategies into short CoT LLMs, enabling them to think longer and introducing
controllability over the thought budget to better manage the overthinking
problem. Our extensive analyses validate that our dataset achieves quality
comparable to--or slightly below--R1. Furthermore, our experiments demonstrate
that training on our dataset not only strengthens general reasoning skills, but
also provides a strong foundation for reinforcement learning--models
initialized on our data achieve 2-3x larger gains with RLVR.

</details>


### [28] [STORYTELLER: An Enhanced Plot-Planning Framework for Coherent and Cohesive Story Generation](https://arxiv.org/abs/2506.02347)
*Jiaming Li,Yukun Chen,Ziqiang Liu,Minghuan Tan,Lei Zhang,Yunshui Li,Run Luo,Longze Chen,Jing Luo,Ahmadreza Argha,Hamid Alinejad-Rokny,Wei Zhou,Min Yang*

Main category: cs.CL

TL;DR: 论文提出Storyteller方法，通过SVO三元组和动态模块提升AI生成故事的连贯性与逻辑性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有AI故事生成方法在叙事连贯性和逻辑一致性上存在不足，影响整体叙事体验，亟需改进。

Method: 引入基于SVO三元组的剧情节点结构，结合动态STORYLINE模块和NEKG知识图谱，实时交互优化生成过程。

Result: 人类偏好评估显示平均胜率达84.33%，在创意、连贯性、吸引力和相关性方面全面领先。

Conclusion: Storyteller通过认知启发的结构化方法，显著提升了自动生成故事的质量和沉浸感。

Abstract: Stories are central to human culture, serving to share ideas, preserve
traditions, and foster connections. Automatic story generation, a key
advancement in artificial intelligence (AI), offers new possibilities for
creating personalized content, exploring creative ideas, and enhancing
interactive experiences. However, existing methods struggle to maintain
narrative coherence and logical consistency. This disconnect compromises the
overall storytelling experience, underscoring the need for substantial
improvements. Inspired by human cognitive processes, we introduce Storyteller,
a novel approach that systemically improves the coherence and consistency of
automatically generated stories. Storyteller introduces a plot node structure
based on linguistically grounded subject verb object (SVO) triplets, which
capture essential story events and ensure a consistent logical flow. Unlike
previous methods, Storyteller integrates two dynamic modules, the STORYLINE and
narrative entity knowledge graph (NEKG),that continuously interact with the
story generation process. This integration produces structurally sound,
cohesive and immersive narratives. Extensive experiments demonstrate that
Storyteller significantly outperforms existing approaches, achieving an 84.33%
average win rate through human preference evaluation. At the same time, it is
also far ahead in other aspects including creativity, coherence, engagement,
and relevance.

</details>


### [29] [Truth over Tricks: Measuring and Mitigating Shortcut Learning in Misinformation Detection](https://arxiv.org/abs/2506.02350)
*Herun Wan,Jiaying Wu,Minnan Luo,Zhi Zeng,Zhixiong Su*

Main category: cs.CL

TL;DR: 论文提出TruthOverTricks评估范式，揭示现有虚假信息检测器依赖表面线索的缺陷，并推出SMF数据增强框架以提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有虚假信息检测模型依赖训练数据中的表面线索（捷径），难以应对真实场景中多样且动态变化的虚假信息，尤其是大语言模型（LLM）能轻易生成逼真虚假信息。

Method: 提出TruthOverTricks评估范式，将捷径行为分为内在诱导和外在注入两类，评估7种检测器在14个基准数据集的表现；开发SMF框架（含改写、事实摘要和情感归一化）缓解捷径依赖。

Result: 实验表明现有检测器在自然和对抗性捷径下性能显著下降；SMF在16个基准上持续提升模型鲁棒性，促使其依赖深层语义理解。

Conclusion: 研究揭示了虚假信息检测中的捷径依赖问题，SMF框架有效提升模型泛化能力，相关资源已开源以推动领域发展。

Abstract: Misinformation detection models often rely on superficial cues (i.e.,
\emph{shortcuts}) that correlate with misinformation in training data but fail
to generalize to the diverse and evolving nature of real-world misinformation.
This issue is exacerbated by large language models (LLMs), which can easily
generate convincing misinformation through simple prompts. We introduce
TruthOverTricks, a unified evaluation paradigm for measuring shortcut learning
in misinformation detection. TruthOverTricks categorizes shortcut behaviors
into intrinsic shortcut induction and extrinsic shortcut injection, and
evaluates seven representative detectors across 14 popular benchmarks, along
with two new factual misinformation datasets, NQ-Misinfo and Streaming-Misinfo.
Empirical results reveal that existing detectors suffer severe performance
degradation when exposed to both naturally occurring and adversarially crafted
shortcuts. To address this, we propose SMF, an LLM-augmented data augmentation
framework that mitigates shortcut reliance through paraphrasing, factual
summarization, and sentiment normalization. SMF consistently enhances
robustness across 16 benchmarks, encouraging models to rely on deeper semantic
understanding rather than shortcut cues. To promote the development of
misinformation detectors, we have published the resources publicly at
https://github.com/whr000001/TruthOverTricks.

</details>


### [30] [DIAMOND: An LLM-Driven Agent for Context-Aware Baseball Highlight Summarization](https://arxiv.org/abs/2506.02351)
*Jeonghun Kang,Soonmok Kwon,Joonseok Lee,Byung-Hak Kim*

Main category: cs.CL

TL;DR: DIAMOND是一个结合结构化体育分析和自然语言推理的LLM驱动代理，用于上下文感知的棒球精彩片段摘要，显著提升了摘要质量。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如基于WPA的排名或计算机视觉事件检测）往往忽略战略深度、势头变化和故事情节进展，而人工编辑虽精准但资源密集且难以扩展。

Method: DIAMOND整合了棒球统计指标（如胜率预期、WPA和杠杆指数）与LLM模块，通过量化比赛重要性和上下文叙事价值来优化精彩片段选择。

Result: 在五场韩国棒球联赛的测试中，DIAMOND将F1分数从42.9%（仅WPA）提升至84.8%，优于商业和统计基线方法。

Conclusion: 尽管规模有限，DIAMOND展示了模块化、可解释的代理框架在体育及其他领域事件级摘要中的潜力。

Abstract: Traditional approaches -- such as Win Probability Added (WPA)-based ranking
or computer vision-driven event detection -- can identify scoring plays but
often miss strategic depth, momentum shifts, and storyline progression. Manual
curation remains the gold standard but is resource-intensive and not scalable.
We introduce DIAMOND, an LLM-driven agent for context-aware baseball highlight
summarization that integrates structured sports analytics with natural language
reasoning. DIAMOND leverages sabermetric features -- Win Expectancy, WPA, and
Leverage Index -- to quantify play importance, while an LLM module enhances
selection based on contextual narrative value. This hybrid approach ensures
both quantitative rigor and qualitative richness, surpassing the limitations of
purely statistical or vision-based systems. Evaluated on five diverse Korean
Baseball Organization League games, DIAMOND improves F1-score from 42.9%
(WPA-only) to 84.8%, outperforming both commercial and statistical baselines.
Though limited in scale, our results highlight the potential of modular,
interpretable agent-based frameworks for event-level summarization in sports
and beyond.

</details>


### [31] [AnswerCarefully: A Dataset for Improving the Safety of Japanese LLM Output](https://arxiv.org/abs/2506.02372)
*Hisami Suzuki,Satoru Katsumata,Takashi Kodama,Tetsuro Takahashi,Kouta Nakayama,Satoshi Sekine*

Main category: cs.CL

TL;DR: AnswerCarefully是一个提升日语LLM输出安全性的数据集，包含1800个问题-答案对，用于微调模型并评估12个日语LLM的安全性。


<details>
  <summary>Details</summary>
Motivation: 为了解决日语LLM在输出时可能遇到的安全和适当性问题，研究团队创建了一个反映日本社会文化背景的数据集。

Method: 手动创建1800个问题-答案对，覆盖多种风险类别，并用于微调日语LLM。

Result: 微调后的LLM在保持通用回答实用性的同时，提高了输出安全性，并评估了12个日语LLM的安全性。

Conclusion: AnswerCarelessly数据集有效提升了日语LLM的安全性，并提供了英文翻译和注释以支持多语言扩展。

Abstract: In this paper we present AnswerCarefully, a dataset for promoting the safety
and appropriateness of Japanese LLM outputs. The dataset consists of 1,800
pairs of questions and reference answers, where the questions require special
attention in answering. It covers a wide range of risk categories established
in prior English-language datasets, but the data samples are original in that
they are manually created to reflect the socio-cultural context of LLM usage in
Japan. We show that using this dataset for instruction to fine-tune a Japanese
LLM led to improved output safety without compromising the utility of general
responses. We also report the results of a safety evaluation of 12 Japanese
LLMs using this dataset as a benchmark. Finally, we describe the latest update
on the dataset which provides English translations and annotations of the
questions, aimed at facilitating the derivation of similar datasets in
different languages and regions.

</details>


### [32] [Exploring Explanations Improves the Robustness of In-Context Learning](https://arxiv.org/abs/2506.02378)
*Ukyo Honda,Tatsushi Oka*

Main category: cs.CL

TL;DR: 论文提出X²-ICL框架，通过系统探索所有可能标签的解释，增强大语言模型的上下文学习鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于解释的上下文学习(X-ICL)虽能提升预测可靠性，但难以泛化到演示数据分布之外。

Method: 扩展X-ICL框架，系统性生成所有可能标签的解释(X²-ICL)，实现更全面的决策。

Result: 在多个自然语言理解数据集上验证，X²-ICL对分布外数据表现出显著更强的鲁棒性。

Conclusion: X²-ICL通过全标签解释探索，显著提升了上下文学习方法的泛化能力。

Abstract: In-context learning (ICL) has emerged as a successful paradigm for leveraging
large language models (LLMs). However, it often struggles to generalize beyond
the distribution of the provided demonstrations. A recent advancement in
enhancing robustness is ICL with explanations (X-ICL), which improves
prediction reliability by guiding LLMs to understand and articulate the
reasoning behind correct labels. Building on this approach, we introduce an
advanced framework that extends X-ICL by systematically exploring explanations
for all possible labels (X$^2$-ICL), thereby enabling more comprehensive and
robust decision-making. Experimental results on multiple natural language
understanding datasets validate the effectiveness of X$^2$-ICL, demonstrating
significantly improved robustness to out-of-distribution data compared to the
existing ICL approaches.

</details>


### [33] [Consultant Decoding: Yet Another Synergistic Mechanism](https://arxiv.org/abs/2506.02391)
*Chuanghao Ding,Jiaping Wang,Ziqing Yang,Xiaoliang Wang,Dahua Lin,Cam-Tu Nguyen,Fei Tan*

Main category: cs.CL

TL;DR: 本文提出了一种名为Consultant Decoding（CD）的新协同机制，通过优化验证机制显著提升大语言模型推理速度，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于Speculative Decoding（SD）的协同机制因高拒绝率需反复调用大语言模型验证，效率提升受限。

Method: CD采用基于LLM计算的token级似然性验证候选草案，而非SD的基于重要性采样的度量方法。

Result: CD实现推理速度最高提升2.5倍，大模型调用频率降至10%以下，部分任务性能甚至超越目标模型。

Conclusion: CD通过创新验证机制突破SD理论上限，为LLM高效推理提供了新范式。

Abstract: The synergistic mechanism based on Speculative Decoding (SD) has garnered
considerable attention as a simple yet effective approach for accelerating the
inference of large language models (LLMs). Nonetheless, the high rejection
rates require repeated LLMs calls to validate draft tokens, undermining the
overall efficiency gain of SD. In this work, we revisit existing verification
mechanisms and propose a novel synergetic mechanism Consultant Decoding (CD).
Unlike SD, which relies on a metric derived from importance sampling for
verification, CD verifies candidate drafts using token-level likelihoods
computed solely by the LLM. CD achieves up to a 2.5-fold increase in inference
speed compared to the target model, while maintaining comparable generation
quality (around 100% of the target model's performance). Interestingly, this is
achieved by combining models whose parameter sizes differ by two orders of
magnitude. In addition, CD reduces the call frequency of the large target model
to below 10%, particularly in more demanding tasks. CD's performance was even
found to surpass that of the large target model, which theoretically represents
the upper bound for speculative decoding.

</details>


### [34] [GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2506.02404)
*Yilin Xiao,Junnan Dong,Chuang Zhou,Su Dong,Qianwen Zhang,Di Yin,Xing Sun,Xiao Huang*

Main category: cs.CL

TL;DR: GraphRAG-Bench是一个新提出的大规模、领域特定基准测试，旨在全面评估GraphRAG模型在复杂推理能力上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前GraphRAG模型的评估主要依赖传统问答数据集，其问题和评估指标的局限性无法全面评估GraphRAG模型带来的推理能力提升。

Method: 通过设计具有挑战性的问题（如需要多跳推理、数学或编程能力的问题）、覆盖多样任务类型（如选择题、判断题、开放式问题等）以及提供全面的评估框架（包括图构建、知识检索和答案生成等环节），构建了GraphRAG-Bench基准测试。

Result: 应用九种当代GraphRAG方法到GraphRAG-Bench上，验证了其在量化图结构化如何提升模型推理能力方面的实用性，并揭示了关于图架构、检索效果和推理能力的关键见解。

Conclusion: GraphRAG-Bench为研究社区提供了量化评估GraphRAG模型推理能力的工具，并为未来的研究和改进提供了可操作的指导。

Abstract: Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing
recognition for its potential to enhance large language models (LLMs) by
structurally organizing domain-specific corpora and facilitating complex
reasoning. However, current evaluations of GraphRAG models predominantly rely
on traditional question-answering datasets. Their limited scope in questions
and evaluation metrics fails to comprehensively assess the reasoning capacity
improvements enabled by GraphRAG models. To address this gap, we introduce
GraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously
evaluate GraphRAG models. Our benchmark offers three key superiorities: \((i)\)
Challenging question design. Featuring college-level, domain-specific questions
that demand multi-hop reasoning, the benchmark ensures that simple content
retrieval is insufficient for problem-solving. For example, some questions
require mathematical reasoning or programming. \((ii)\) Diverse task coverage.
The dataset includes a broad spectrum of reasoning tasks, multiple-choice,
true/false, multi-select, open-ended, and fill-in-the-blank. It spans 16
disciplines in twenty core textbooks. \((iii)\) Holistic evaluation framework.
GraphRAG-Bench provides comprehensive assessment across the entire GraphRAG
pipeline, including graph construction, knowledge retrieval, and answer
generation. Beyond final-answer correctness, it evaluates the logical coherence
of the reasoning process. By applying nine contemporary GraphRAG methods to
GraphRAG-Bench, we demonstrate its utility in quantifying how graph-based
structuring improves model reasoning capabilities. Our analysis reveals
critical insights about graph architectures, retrieval efficacy, and reasoning
capabilities, offering actionable guidance for the research community.

</details>


### [35] [SingaKids: A Multilingual Multimodal Dialogic Tutor for Language Learning](https://arxiv.org/abs/2506.02412)
*Zhengyuan Liu,Geyu Lin,Hui Li Tan,Huayun Zhang,Yanfeng Lu,Xiaoxue Gao,Stella Xin Yin,He Sun,Hock Huan Goh,Lung Hsiang Wong,Nancy F. Chen*

Main category: cs.CL

TL;DR: 论文介绍了SingaKids，一个通过图片描述任务促进语言学习的对话式辅导系统，支持四种语言，并通过实证研究证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能在教育应用中有潜力提升语言学习，但需解决多语言、文化适应性及儿童友好设计等挑战。

Method: 系统整合密集图像描述、多语言对话交互、语音理解和生成技术，并通过预训练、任务调优和支架优化进行改进。

Result: 实证研究表明，SingaKids能为不同水平的学习者提供有效的对话式教学。

Conclusion: SingaKids在多语言环境下展现了促进儿童语言学习的潜力，未来可进一步优化和扩展。

Abstract: The integration of generative artificial intelligence into educational
applications has enhanced personalized and interactive learning experiences,
and it shows strong potential to promote young learners language acquisition.
However, it is still challenging to ensure consistent and robust performance
across different languages and cultural contexts, and kids-friendly design
requires simplified instructions, engaging interactions, and age-appropriate
scaffolding to maintain motivation and optimize learning outcomes. In this
work, we introduce SingaKids, a dialogic tutor designed to facilitate language
learning through picture description tasks. Our system integrates dense image
captioning, multilingual dialogic interaction, speech understanding, and
engaging speech generation to create an immersive learning environment in four
languages: English, Mandarin, Malay, and Tamil. We further improve the system
through multilingual pre-training, task-specific tuning, and scaffolding
optimization. Empirical studies with elementary school students demonstrate
that SingaKids provides effective dialogic teaching, benefiting learners at
different performance levels.

</details>


### [36] [Gender Inequality in English Textbooks Around the World: an NLP Approach](https://arxiv.org/abs/2506.02425)
*Tairan Liu*

Main category: cs.CL

TL;DR: 该研究使用自然语言处理技术分析了22个国家英语教材中的性别不平等现象，发现男性角色在数量、提及顺序和命名实体上普遍被过度代表，其中拉丁文化圈的不平等程度最低。


<details>
  <summary>Details</summary>
Motivation: 教材对儿童世界观的形成至关重要，但此前研究多局限于单个国家的性别不平等问题，缺乏跨文化比较。本研究旨在填补这一空白。

Method: 采用自然语言处理方法，包括字符计数、首次提及顺序分析、TF-IDF词频关联、专有名词性别模式识别、大语言模型区分测试以及GloVe词嵌入关联度分析。

Result: 所有地区教材均存在性别不平等现象，男性角色在数量、首次提及和命名实体上均占优势，拉丁文化圈的性别差异最小。

Conclusion: 英语教材普遍存在系统性性别偏见，跨文化比较显示这种不平等具有全球性，但程度因文化圈而异。

Abstract: Textbooks play a critical role in shaping children's understanding of the
world. While previous studies have identified gender inequality in individual
countries' textbooks, few have examined the issue cross-culturally. This study
applies natural language processing methods to quantify gender inequality in
English textbooks from 22 countries across 7 cultural spheres. Metrics include
character count, firstness (which gender is mentioned first), and TF-IDF word
associations by gender. The analysis also identifies gender patterns in proper
names appearing in TF-IDF word lists, tests whether large language models can
distinguish between gendered word lists, and uses GloVe embeddings to examine
how closely keywords associate with each gender. Results show consistent
overrepresentation of male characters in terms of count, firstness, and named
entities. All regions exhibit gender inequality, with the Latin cultural sphere
showing the least disparity.

</details>


### [37] [Comparative Analysis of AI Agent Architectures for Entity Relationship Classification](https://arxiv.org/abs/2506.02426)
*Maryam Berijanian,Kuldeep Singh,Amin Sehati*

Main category: cs.CL

TL;DR: 该研究比较了三种基于大语言模型的关系分类AI代理架构，发现多智能体协作优于标准少样本提示，接近微调模型性能。


<details>
  <summary>Details</summary>
Motivation: 在标注数据有限且关系结构复杂的场景下，实体关系分类仍具挑战性，需探索更有效的AI代理架构。

Method: 对比分析三种架构：(1)反射式自评估，(2)分层任务分解，(3)新型多智能体动态示例生成机制（含实时协作对抗提示）。

Result: 多智能体协调持续优于少样本提示，接近微调模型表现，跨领域和模型后端验证有效。

Conclusion: 研究为模块化、可泛化的基于LLM的结构化关系抽取系统设计提供了实用指导。

Abstract: Entity relationship classification remains a challenging task in information
extraction, especially in scenarios with limited labeled data and complex
relational structures. In this study, we conduct a comparative analysis of
three distinct AI agent architectures designed to perform relation
classification using large language models (LLMs). The agentic architectures
explored include (1) reflective self-evaluation, (2) hierarchical task
decomposition, and (3) a novel multi-agent dynamic example generation
mechanism, each leveraging different modes of reasoning and prompt adaptation.
In particular, our dynamic example generation approach introduces real-time
cooperative and adversarial prompting. We systematically compare their
performance across multiple domains and model backends. Our experiments
demonstrate that multi-agent coordination consistently outperforms standard
few-shot prompting and approaches the performance of fine-tuned models. These
findings offer practical guidance for the design of modular, generalizable
LLM-based systems for structured relation extraction. The source codes and
dataset are available at
\href{https://github.com/maryambrj/ALIEN.git}{https://github.com/maryambrj/ALIEN.git}.

</details>


### [38] [From Anger to Joy: How Nationality Personas Shape Emotion Attribution in Large Language Models](https://arxiv.org/abs/2506.02431)
*Mahammed Kamruzzaman,Abdullah Al Monsur,Gene Louis Kim,Anshuman Chhabra*

Main category: cs.CL

TL;DR: 研究探讨大型语言模型（LLMs）在扮演不同国家角色时是否表现出情感刻板印象，发现其情感分配存在国籍差异且与人类情感反应不一致。


<details>
  <summary>Details</summary>
Motivation: 鉴于LLMs在角色扮演中的成功应用，研究者希望探究其在模拟不同国家角色时是否会呈现情感刻板印象，以及这些情感分配是否符合文化规范。

Method: 通过分析预训练LLMs对不同国家角色的情感分配，比较其与人类情感反应的差异。

Result: 研究发现LLMs在情感分配上存在显著的国籍差异，如羞耻、恐惧和快乐等情感在不同地区的分配不均，且负面情感与人类反应偏差较大。

Conclusion: LLMs在情感表达上存在简化且有偏见的刻板印象，特别是在负面情感方面，与人类情感反应不一致。

Abstract: Emotions are a fundamental facet of human experience, varying across
individuals, cultural contexts, and nationalities. Given the recent success of
Large Language Models (LLMs) as role-playing agents, we examine whether LLMs
exhibit emotional stereotypes when assigned nationality-specific personas.
Specifically, we investigate how different countries are represented in
pre-trained LLMs through emotion attributions and whether these attributions
align with cultural norms. Our analysis reveals significant nationality-based
differences, with emotions such as shame, fear, and joy being
disproportionately assigned across regions. Furthermore, we observe notable
misalignment between LLM-generated and human emotional responses, particularly
for negative emotions, highlighting the presence of reductive and potentially
biased stereotypes in LLM outputs.

</details>


### [39] [Should LLM Safety Be More Than Refusing Harmful Instructions?](https://arxiv.org/abs/2506.02442)
*Utsav Maskey,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 该论文系统评估了大语言模型(LLM)在长尾分布(加密)文本上的行为及其安全隐患，提出了二维安全评估框架，发现解密能力可能导致安全机制失效，并探讨了多种防护措施的优劣。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探究LLM在处理加密等长尾文本时的安全表现，揭示潜在风险以推动更鲁棒的安全机制发展。

Method: 采用二维评估框架：(1)指令拒绝能力测试(拒绝有害混淆指令)；(2)生成安全性测试(抑制有害响应生成)，通过全面实验验证模型表现。

Result: 实验表明具备解密能力的模型易受不匹配泛化攻击，安全机制至少在一个维度会失效，导致不安全响应或过度拒绝。

Conclusion: 研究深化了对LLM长尾文本安全的理解，为开发鲁棒安全机制提供了方向，同时评估了现有防护措施的局限性。

Abstract: This paper presents a systematic evaluation of Large Language Models' (LLMs)
behavior on long-tail distributed (encrypted) texts and their safety
implications. We introduce a two-dimensional framework for assessing LLM
safety: (1) instruction refusal-the ability to reject harmful obfuscated
instructions, and (2) generation safety-the suppression of generating harmful
responses. Through comprehensive experiments, we demonstrate that models that
possess capabilities to decrypt ciphers may be susceptible to
mismatched-generalization attacks: their safety mechanisms fail on at least one
safety dimension, leading to unsafe responses or over-refusal. Based on these
findings, we evaluate a number of pre-LLM and post-LLM safeguards and discuss
their strengths and limitations. This work contributes to understanding the
safety of LLM in long-tail text scenarios and provides directions for
developing robust safety mechanisms.

</details>


### [40] [IP-Dialog: Evaluating Implicit Personalization in Dialogue Systems with Synthetic Data](https://arxiv.org/abs/2506.02449)
*Bo Peng,Zhiheng Wang,Heyang Gong,Chaochao Lu*

Main category: cs.CL

TL;DR: 论文提出了一种自动生成合成数据的方法，并引入了IP-Dialog基准和训练数据集，以解决对话系统中用户背景隐式推理的高质量数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 现代对话系统需要从对话中隐式推断用户背景并利用这些信息提供个性化帮助，但高质量数据的稀缺性成为评估和改进这一能力的主要挑战。传统数据集构建方法劳动密集、资源需求高且存在隐私问题。

Method: 提出了一种新颖的自动合成数据生成方法，并引入了IP-Dialog基准和训练数据集，覆盖10个任务和12种用户属性类型。此外，开发了一个系统评估框架，包含四个指标来评估属性意识和推理能力，并提出了五个因果图来阐明模型在隐式个性化中的推理路径。

Result: 大量实验提供了有洞察力的观察结果，并证明了数据集的可靠性。

Conclusion: 通过自动生成合成数据和引入IP-Dialog基准，论文有效解决了对话系统中隐式个性化推理的数据稀缺问题，为未来的研究和应用提供了可靠的基础。

Abstract: In modern dialogue systems, the ability to implicitly infer user backgrounds
from conversations and leverage this information for personalized assistance is
crucial. However, the scarcity of high-quality data remains a fundamental
challenge to evaluating and improving this capability. Traditional dataset
construction methods are labor-intensive, resource-demanding, and raise privacy
concerns. To address these issues, we propose a novel approach for automatic
synthetic data generation and introduce the Implicit Personalized Dialogue
(IP-Dialog) benchmark along with a training dataset, covering 10 tasks and 12
user attribute types. Additionally, we develop a systematic evaluation
framework with four metrics to assess both attribute awareness and reasoning
capabilities. We further propose five causal graphs to elucidate models'
reasoning pathways during implicit personalization. Extensive experiments yield
insightful observations and prove the reliability of our dataset.

</details>


### [41] [Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework](https://arxiv.org/abs/2506.02454)
*Zhaorui Yang,Bo Pan,Han Wang,Yiyao Wang,Xingyu Liu,Minfeng Zhu,Bo Zhang,Wei Chen*

Main category: cs.CL

TL;DR: 该论文提出了一种名为Multimodal DeepResearcher的框架，通过结构化文本表示（FDV）使大语言模型能自动生成图文并茂的研究报告，并在评估中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究框架主要生成纯文本内容，图文混合内容的自动生成尚未充分探索。设计信息丰富的可视化并有效整合到文本报告中存在挑战。

Method: 提出可视化形式化描述(FDV)作为结构化文本表示，构建四阶段代理框架：研究、示例报告文本化、规划和多模态报告生成。

Result: 在MultimodalReportBench评估中，使用相同Claude 3.7 Sonnet模型时，该框架相比基线方法获得82%的整体胜率。

Conclusion: Multimodal DeepResearcher框架有效解决了多模态研究报告自动生成问题，通过FDV表示实现了高质量的图文整合输出。

Abstract: Visualizations play a crucial part in effective communication of concepts and
information. Recent advances in reasoning and retrieval augmented generation
have enabled Large Language Models (LLMs) to perform deep research and generate
comprehensive reports. Despite its progress, existing deep research frameworks
primarily focus on generating text-only content, leaving the automated
generation of interleaved texts and visualizations underexplored. This novel
task poses key challenges in designing informative visualizations and
effectively integrating them with text reports. To address these challenges, we
propose Formal Description of Visualization (FDV), a structured textual
representation of charts that enables LLMs to learn from and generate diverse,
high-quality visualizations. Building on this representation, we introduce
Multimodal DeepResearcher, an agentic framework that decomposes the task into
four stages: (1) researching, (2) exemplar report textualization, (3) planning,
and (4) multimodal report generation. For the evaluation of generated
multimodal reports, we develop MultimodalReportBench, which contains 100
diverse topics served as inputs along with 5 dedicated metrics. Extensive
experiments across models and evaluation methods demonstrate the effectiveness
of Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet
model, Multimodal DeepResearcher achieves an 82\% overall win rate over the
baseline method.

</details>


### [42] [MidPO: Dual Preference Optimization for Safety and Helpfulness in Large Language Models via a Mixture of Experts Framework](https://arxiv.org/abs/2506.02460)
*Yupeng Qi,Ziyu Lyu,Min Yang,Yanlin Wang,Lu Bai,Lixin Cui*

Main category: cs.CL

TL;DR: 论文提出MidPO框架，通过混合专家(MoE)和动态路由机制平衡大语言模型的安全性与有用性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型(LLM)在安全性和有用性平衡上面临挑战：在线偏好优化方法易过度安全降低有用性，离线方法则难以自适应平衡。

Method: 1. 设计单偏好增强的直接偏好优化，将基础模型分为安全性和有用性两个独立专家；2. 通过MoE框架整合专家并设计动态路由机制自适应分配权重。

Result: 在三个流行数据集上的定量和定性实验表明，MidPO在安全性和有用性上均显著优于最先进方法。

Conclusion: MidPO通过混合专家框架有效解决了LLM安全性与有用性的平衡问题，代码和模型将开源。

Abstract: As large language models (LLMs) are increasingly applied across various
domains, enhancing safety while maintaining the helpfulness of LLMs has become
a critical challenge. Recent studies solve this problem through
safety-constrained online preference optimization or safety-constrained offline
preference optimization. However, the safety-constrained online methods often
suffer from excessive safety, which might reduce helpfulness, while the
safety-constrained offline methods perform poorly in adaptively balancing
safety and helpfulness. To address these limitations, we propose MidPO, a
\textbf{\underline{Mi}}xture of Experts (MoE) framework for safety-helpfulness
\textbf{\underline{d}}ual \textbf{\underline{P}}reference
\textbf{\underline{O}}ptimization. Firstly, MidPO devises single-preference
enhanced direct preference optimization approach to transform the base model
into two independent experts, termed safety and helpfulness experts, and
fine-tunes the two independent experts for optimal safety or helpfulness
performance. Secondly, to achieve an effective balance between safety and
helpfulness, MidPO incorporates the two experts into the MoE framework and
designs a dynamic routing mechanism to allocate contributions from each expert
adaptively. We conduct quantitative and qualitative experiments on three
popular datasets to demonstrate the proposed MidPO significantly outperforms
state-of-the-art approaches in both safety and helpfulness. The code and models
will be released.

</details>


### [43] [XToM: Exploring the Multilingual Theory of Mind for Large Language Models](https://arxiv.org/abs/2506.02461)
*Chunkit Chan,Yauwai Yim,Hongchuan Zeng,Zhiying Zou,Xinyuan Cheng,Zhifan Sun,Zheye Deng,Kawai Chung,Yuzhuo Ao,Yixiang Fan,Cheng Jiayang,Ercong Nie,Ginny Y. Wong,Helmut Schmid,Hinrich Schütze,Simon See,Yangqiu Song*

Main category: cs.CL

TL;DR: 该论文提出了XToM基准，用于评估大语言模型在多语言环境下的心智理论能力，发现模型在不同语言中的表现存在差异。


<details>
  <summary>Details</summary>
Motivation: 现有对大型语言模型（LLMs）心智理论（ToM）的评估主要集中于英语，忽略了语言多样性对人类认知的影响，因此需要研究LLMs在多语言环境下的ToM能力。

Method: 作者开发了XToM基准，涵盖五种语言和多样化的任务场景，并系统评估了如DeepSeek R1等LLMs的多语言ToM表现。

Result: 研究发现，尽管LLMs在多语言理解方面表现优异，但在不同语言中的ToM能力存在显著差异，无法完全复现人类跨语言的心智推理能力。

Conclusion: 论文揭示了LLMs在多语言环境下模拟人类心智理论的局限性，强调了未来研究需要考虑语言多样性对认知能力的影响。

Abstract: Theory of Mind (ToM), the ability to infer mental states in others, is
pivotal for human social cognition. Existing evaluations of ToM in LLMs are
largely limited to English, neglecting the linguistic diversity that shapes
human cognition. This limitation raises a critical question: can LLMs exhibit
Multilingual Theory of Mind, which is the capacity to reason about mental
states across diverse linguistic contexts? To address this gap, we present
XToM, a rigorously validated multilingual benchmark that evaluates ToM across
five languages and incorporates diverse, contextually rich task scenarios.
Using XToM, we systematically evaluate LLMs (e.g., DeepSeek R1), revealing a
pronounced dissonance: while models excel in multilingual language
understanding, their ToM performance varies across languages. Our findings
expose limitations in LLMs' ability to replicate human-like mentalizing across
linguistic contexts.

</details>


### [44] [FroM: Frobenius Norm-Based Data-Free Adaptive Model Merging](https://arxiv.org/abs/2506.02478)
*Zijian Li,Xiaocheng Feng,Huixin Liu,Yichong Huang,Ting Liu,Bing Qin*

Main category: cs.CL

TL;DR: 本文提出了一种名为FroM的自适应模型融合方法，通过Frobenius范数直接度量模型参数，无需训练数据，有效缓解任务干扰问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，微调成为提升特定场景性能的有效方法。然而，传统方法在融合多个微调模型时存在任务干扰问题，尤其是在参数高效微调场景中更为明显。

Method: 本文改进了RegMean方法，提出FroM方法，通过Frobenius范数直接度量模型参数，并引入额外超参数进行控制。

Result: FroM方法在多种微调场景中优于基线方法，显著缓解了任务干扰问题。

Conclusion: FroM方法无需训练数据，通过自适应融合有效提升了模型性能，为模型融合提供了新思路。

Abstract: With the development of large language models, fine-tuning has emerged as an
effective method to enhance performance in specific scenarios by injecting
domain-specific knowledge. In this context, model merging techniques provide a
solution for fusing knowledge from multiple fine-tuning models by combining
their parameters. However, traditional methods often encounter task
interference when merging full fine-tuning models, and this problem becomes
even more evident in parameter-efficient fine-tuning scenarios. In this paper,
we introduce an improvement to the RegMean method, which indirectly leverages
the training data to approximate the outputs of the linear layers before and
after merging. We propose an adaptive merging method called FroM, which
directly measures the model parameters using the Frobenius norm, without any
training data. By introducing an additional hyperparameter for control, FroM
outperforms baseline methods across various fine-tuning scenarios, alleviating
the task interference problem.

</details>


### [45] [ORPP: Self-Optimizing Role-playing Prompts to Enhance Language Model Capabilities](https://arxiv.org/abs/2506.02480)
*Yifan Duan,Yihong Tang,Kehai Chen,Liqiang Nie,Min Zhang*

Main category: cs.CL

TL;DR: ORPP框架通过角色扮演提示优化提升大模型性能，优于现有主流方法且具备即插即用兼容性。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法存在计算开销大或依赖模型强优化能力的局限，需要更普适的解决方案。

Method: ORPP将搜索空间限定为角色扮演场景，先在小样本上迭代生成优质提示，再利用小样本学习能力迁移到全量数据。

Result: 实验表明ORPP在多数情况下超越主流方法，并能与其他提示技术结合实现效果增益。

Conclusion: 角色扮演提示优化是激活模型内在能力的有效路径，ORPP框架具有显著性能优势与扩展性。

Abstract: High-quality prompts are crucial for eliciting outstanding performance from
large language models (LLMs) on complex tasks. Existing research has explored
model-driven strategies for prompt optimization. However, these methods often
suffer from high computational overhead or require strong optimization
capabilities from the model itself, which limits their broad applicability.To
address these challenges, we propose ORPP (Optimized Role-Playing Prompt),a
framework that enhances model performance by optimizing and generating
role-playing prompts. The core idea of ORPP is to confine the prompt search
space to role-playing scenarios, thereby fully activating the model's intrinsic
capabilities through carefully crafted, high-quality role-playing prompts.
Specifically, ORPP first performs iterative optimization on a small subset of
training samples to generate high-quality role-playing prompts. Then,
leveraging the model's few-shot learning capability, it transfers the
optimization experience to efficiently generate suitable prompts for the
remaining samples.Our experimental results show that ORPP not only matches but
in most cases surpasses existing mainstream prompt optimization methods in
terms of performance. Notably, ORPP demonstrates superior "plug-and-play"
capability. In most cases, it can be integrated with various other prompt
methods and further enhance their effectiveness.

</details>


### [46] [Do Language Models Think Consistently? A Study of Value Preferences Across Varying Response Lengths](https://arxiv.org/abs/2506.02481)
*Inderjeet Nair,Lu Wang*

Main category: cs.CL

TL;DR: 研究发现，从简短测试和长文本输出中推断的LLM价值偏好相关性较弱，需更稳健的方法确保价值表达一致性。


<details>
  <summary>Details</summary>
Motivation: 当前评估LLM伦理风险和价值倾向多依赖简短测试，但实际应用多为长文本输出，导致真实场景中的价值偏好研究不足。

Method: 比较五种LLM在简短反应和不同参数数量的长文本回答中表现的价值偏好，分析一致性及相关性。

Result: 简短与长文本回答间的价值偏好相关性弱；长文本不同设置间相关性同样弱；对齐仅略微提升表达一致性。

Conclusion: 需开发更可靠方法以确保LLM在不同应用场景中价值表达的一致性。

Abstract: Evaluations of LLMs' ethical risks and value inclinations often rely on
short-form surveys and psychometric tests, yet real-world use involves
long-form, open-ended responses -- leaving value-related risks and preferences
in practical settings largely underexplored. In this work, we ask: Do value
preferences inferred from short-form tests align with those expressed in
long-form outputs? To address this question, we compare value preferences
elicited from short-form reactions and long-form responses, varying the number
of arguments in the latter to capture users' differing verbosity preferences.
Analyzing five LLMs (llama3-8b, gemma2-9b, mistral-7b, qwen2-7b, and olmo-7b),
we find (1) a weak correlation between value preferences inferred from
short-form and long-form responses across varying argument counts, and (2)
similarly weak correlation between preferences derived from any two distinct
long-form generation settings. (3) Alignment yields only modest gains in the
consistency of value expression. Further, we examine how long-form generation
attributes relate to value preferences, finding that argument specificity
negatively correlates with preference strength, while representation across
scenarios shows a positive correlation. Our findings underscore the need for
more robust methods to ensure consistent value expression across diverse
applications.

</details>


### [47] [Enhancing Large Language Models with Neurosymbolic Reasoning for Multilingual Tasks](https://arxiv.org/abs/2506.02483)
*Sina Bagheri Nezhad,Ameeta Agrawal*

Main category: cs.CL

TL;DR: 论文提出NSAR方法，通过结合神经与符号推理提升大语言模型在多目标长文本推理中的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长文本多目标推理任务中表现不佳，因相关信息分散在文档各处。

Method: NSAR方法显式提取文本中的符号事实，并生成可执行Python代码处理复杂推理步骤。

Result: 在七种语言和不同文本长度实验中，NSAR显著优于RAG基线和高级提示策略。

Conclusion: 神经推理与显式符号操作的结合能实现鲁棒、可解释且可扩展的多语言推理。

Abstract: Large language models (LLMs) often struggle to perform multi-target reasoning
in long-context scenarios where relevant information is scattered across
extensive documents. To address this challenge, we introduce NeuroSymbolic
Augmented Reasoning (NSAR), which combines the benefits of neural and symbolic
reasoning during inference. NSAR explicitly extracts symbolic facts from text
and generates executable Python code to handle complex reasoning steps. Through
extensive experiments across seven languages and diverse context lengths, we
demonstrate that NSAR significantly outperforms both a vanilla RAG baseline and
advanced prompting strategies in accurately identifying and synthesizing
multiple pieces of information. Our results highlight the effectiveness of
combining explicit symbolic operations with neural inference for robust,
interpretable, and scalable reasoning in multilingual settings.

</details>


### [48] [Minos: A Multimodal Evaluation Model for Bidirectional Generation Between Image and Text](https://arxiv.org/abs/2506.02494)
*Junzhe Zhang,Huixuan Zhang,Xinyu Hu,Li Lin,Mingqi Gao,Shi Qiu,Xiaojun Wan*

Main category: cs.CL

TL;DR: 该论文提出了Minos-Corpus数据集和Minos模型，用于多模态生成任务的评估，特别是在文本到图像生成任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有工作忽视了文本到图像生成任务的评估能力开发和大规模人类评估数据的整合，因此作者提出了Minos-Corpus和Minos模型来填补这一空白。

Method: 作者构建了Minos-Corpus数据集，结合了人类和GPT的评估数据，并提出了数据选择与平衡、Mix-SFT训练方法，以及应用DPO训练Minos模型。

Result: Minos在开源评估模型中达到同类最佳性能，尤其在文本到图像生成任务上优于所有开源和闭源模型。

Conclusion: 高质量人类评估数据和联合训练在多模态评估任务中至关重要，Minos的成功验证了这一点。

Abstract: Evaluation is important for multimodal generation tasks. With the rapid
progress of MLLMs, there is growing interest in applying MLLMs to build general
evaluation systems. However, existing work overlooks two aspects: (1) the
development of evaluation capabilities for text-to-image (T2I) generation task,
and (2) the incorporation of large-scale human evaluation data. In this paper,
we introduce Minos-Corpus, a large-scale multimodal evaluation dataset that
combines evaluation data from both human and GPT. The corpus contains
evaluation data across both image-to-text(I2T) and T2I generation tasks. Based
on this corpus, we propose Data Selection and Balance, Mix-SFT training
methods, and apply DPO to develop Minos, a multimodal evaluation model built
upon a 7B backbone. Minos achieves state-of-the-art (SoTA) performance among
all open-source evaluation models of similar scale on the average of evaluation
performance on all tasks, and outperforms all open-source and closed-source
models on evaluation of T2I generation task. Extensive experiments demonstrate
the importance of leveraging high-quality human evaluation data and jointly
training on evaluation data from both I2T and T2I generation tasks.

</details>


### [49] [KARE-RAG: Knowledge-Aware Refinement and Enhancement for RAG](https://arxiv.org/abs/2506.02503)
*Yongjian Li,HaoCheng Chu,Yukun Yan,Zhenghao Liu,Shi Yu,Zheni Zeng,Ruobing Wang,Sen Song,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: KARE-RAG通过结构化知识表示、改进的训练目标和对比数据生成，提升RAG模型处理噪声内容的能力，显著提高性能且数据高效。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG使大语言模型能访问更广泛的知识源，但检索文档中的噪声仍导致事实不一致。增强生成模型处理噪声内容的能力对稳健性能至关重要。

Method: 提出KARE-RAG，包含：(1)结构化知识表示便于训练中错误检测，(2)DDPO训练目标优先纠正关键错误，(3)对比数据生成流程在纠正事实错误时保持语义一致性。

Result: 实验表明该方法显著提升标准RAG流程性能，适用于不同规模模型，且不损害通用能力，仅需少量训练数据即可实现高效优化。

Conclusion: 通过改进模型学习处理检索内容的方式，可提升RAG在多样化推理范式中的性能，为RAG改进指明新方向。

Abstract: Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to
access broader knowledge sources, yet factual inconsistencies persist due to
noise in retrieved documents-even with advanced retrieval methods. We
demonstrate that enhancing generative models' capacity to process noisy content
is equally critical for robust performance. In this paper, we present KARE-RAG
(Knowledge-Aware Refinement and Enhancement for RAG), which improves knowledge
utilization through three key innovations: (1) structured knowledge
representations that facilitate error detection during training, (2) Dense
Direct Preference Optimization (DDPO)-a refined training objective that
prioritizes correction of critical errors, and (3) a contrastive data
generation pipeline that maintains semantic consistency while rectifying
factual inaccuracies. Experiments show our method significantly enhances
standard RAG pipelines across model scales, improving both in-domain and
out-of-domain task performance without compromising general capabilities.
Notably, these gains are achieved with modest training data, suggesting
data-efficient optimization is possible through targeted learning strategies.
Our findings establish a new direction for RAG improvement: by improving how
models learn to process retrieved content, we can enhance performance across
diverse inference paradigms. All data and code will be publicly available on
Github.

</details>


### [50] [M$^3$FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial Meeting Understanding Evaluation Dataset](https://arxiv.org/abs/2506.02510)
*Jie Zhu,Junhui Li,Yalong Wen,Xiandong Li,Lifan Guo,Feng Chen*

Main category: cs.CL

TL;DR: 该论文提出了一个名为M³FinMeeting的新型基准测试，用于评估大语言模型在金融会议理解方面的能力，覆盖多语言、多行业和多任务。


<details>
  <summary>Details</summary>
Motivation: 当前金融领域的基准测试主要依赖新闻文章、财报或公告，难以捕捉金融会议的真实动态。为填补这一空白，作者提出了M³FinMeeting基准。

Method: M³FinMeeting支持英语、中文和日语，涵盖GICS定义的多个行业，并包含摘要生成、问答对提取和问题回答三项任务。

Result: 实验结果显示，即使是目前最先进的长上下文模型，在金融会议理解方面仍有显著改进空间，验证了M³FinMeeting的有效性。

Conclusion: M³FinMeeting作为一个全面且真实的基准测试，能有效评估大语言模型在金融会议理解方面的能力，并揭示其改进方向。

Abstract: Recent breakthroughs in large language models (LLMs) have led to the
development of new benchmarks for evaluating their performance in the financial
domain. However, current financial benchmarks often rely on news articles,
earnings reports, or announcements, making it challenging to capture the
real-world dynamics of financial meetings. To address this gap, we propose a
novel benchmark called $\texttt{M$^3$FinMeeting}$, which is a multilingual,
multi-sector, and multi-task dataset designed for financial meeting
understanding. First, $\texttt{M$^3$FinMeeting}$ supports English, Chinese, and
Japanese, enhancing comprehension of financial discussions in diverse
linguistic contexts. Second, it encompasses various industry sectors defined by
the Global Industry Classification Standard (GICS), ensuring that the benchmark
spans a broad range of financial activities. Finally,
$\texttt{M$^3$FinMeeting}$ includes three tasks: summarization, question-answer
(QA) pair extraction, and question answering, facilitating a more realistic and
comprehensive evaluation of understanding. Experimental results with seven
popular LLMs reveal that even the most advanced long-context models have
significant room for improvement, demonstrating the effectiveness of
$\texttt{M$^3$FinMeeting}$ as a benchmark for assessing LLMs' financial meeting
comprehension skills.

</details>


### [51] [FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial Reasoning](https://arxiv.org/abs/2506.02515)
*Zhuohan Xie,Dhruv Sahnan,Debopriyo Banerjee,Georgi Georgiev,Rushil Thareja,Hachem Madmoun,Jinyan Su,Aaryamonvikram Singh,Yuxia Wang,Rui Xing,Fajri Koto,Haonan Li,Ivan Koychev,Tanmoy Chakraborty,Salem Lahlou,Veselin Stoyanov,Preslav Nakov*

Main category: cs.CL

TL;DR: 论文提出FinChain，首个用于验证链式思维金融推理的符号基准，包含自动评估指标ChainEval，测试显示现有模型在多步金融推理上仍有提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有金融任务数据集仅监督最终数值答案，缺乏对中间推理步骤的评估，导致多步符号推理能力无法系统衡量。

Method: 构建FinChain基准，覆盖12个金融领域的54个主题，每个主题提供5种参数化模板，包含可执行的Python轨迹以生成训练数据，并设计ChainEval自动评估指标。

Result: 测试30个LLM发现，即使是先进模型在多步金融推理上仍有显著改进余地。

Conclusion: FinChain填补了金融推理评估的空白，其模块化设计易于扩展至其他领域，为未来研究提供基准工具。

Abstract: Multi-step symbolic reasoning is critical for advancing downstream
performance on financial tasks. Yet, benchmarks for systematically evaluating
this capability are lacking. Existing datasets like FinQA and ConvFinQA
supervise only final numerical answers, without assessing intermediate
reasoning steps. To address this, we introduce FinChain, the first symbolic
benchmark designed for verifiable Chain-of- Thought (CoT) financial reasoning.
Spanning 54 topics across 12 financial domains, Fin- Chain offers five
parameterized templates per topic, each varying in reasoning complexity and
domain expertise required. Each dataset instance includes an executable Python
trace, enabling automatic generation of extensive training data and easy
adaptation to other domains. We also introduce ChainEval, a new metric for
automatic evaluation of both final answers and intermediate reasoning.
Benchmarking 30 LLMs on our dataset, we find that even state-of-the-art models
have considerable room for improvement in multi-step financial reasoning. All
templates and evaluation metrics for FinChain are available at https:
//github.com/mbzuai-nlp/finchain.

</details>


### [52] [Learning Together to Perform Better: Teaching Small-Scale LLMs to Collaborate via Preferential Rationale Tuning](https://arxiv.org/abs/2506.02519)
*Sohan Patnaik,Milan Aggarwal,Sumit Bhatia,Balaji Krishnamurthy*

Main category: cs.CL

TL;DR: COLLATE框架通过优化小型LLM生成多样化推理路径并选择最佳答案，无需依赖大型模型，提升了推理能力。


<details>
  <summary>Details</summary>
Motivation: 由于大型LLM存在版权和法律问题，且缺乏透明度，限制了其在商业环境中的应用。本文旨在提升小型LLM的推理能力，而无需依赖大型模型的蒸馏。

Method: 提出COLLATE框架，通过训练小型LLM生成多样化推理路径，并利用偏好优化选择最佳答案。

Result: COLLATE在数学问题求解、自然语言推理和常识推理等5个数据集上优于多种基线方法，适用于不同规模的LLM（1B到8B参数）。

Conclusion: COLLATE有效提升了小型LLM的推理能力，展示了多样化推理路径生成和任务导向优化的优势。

Abstract: LLMssuch as GPT-4 have shown a remarkable ability to solve complex questions
by generating step-by-step rationales. Prior works have utilized this
capability to improve smaller and cheaper LMs (say, with 7B parameters).
However, various practical constraints, such as copyright and legal issues,
owing to lack of transparency in the pre-training data of large (often closed)
models, prevent their use in commercial settings. Little focus has been given
to improving the innate reasoning ability of smaller models without distilling
information from larger LLMs. To address this, we propose COLLATE, a trainable
framework that tunes a (small) LLM to generate those outputs from a pool of
diverse rationales that selectively improves the downstream task. COLLATE
enforces multiple instances of the same LLM to exhibit distinct behavior and
employs them to generate rationales to obtain diverse outputs. The LLM is then
tuned via preference optimization to choose the candidate rationale which
maximizes the likelihood of ground-truth answer. COLLATE outperforms several
trainable and prompting baselines on 5 datasets across 3 domains: maths problem
solving, natural language inference, and commonsense reasoning. We show the eff
icacy of COLLATE on LLMs from different model families across varying parameter
scales (1B to 8B) and demonstrate the benefit of multiple rationale providers
guided by the end task through ablations. Code is released here
(https://github.com/Sohanpatnaik106/collate).

</details>


### [53] [Multilingual Information Retrieval with a Monolingual Knowledge Base](https://arxiv.org/abs/2506.02527)
*Yingying Zhuang,Aman Gupta,Anurag Beniwal*

Main category: cs.CL

TL;DR: 该论文提出了一种通过加权采样微调多语言嵌入模型的新策略，用于跨语言知识共享，显著提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 高质量知识库资源通常语言有限，需要有效的嵌入模型将不同语言的句子转换为知识库语言的向量空间，以实现跨语言知识共享，特别是从高资源语言向低资源语言的知识迁移。

Method: 提出了一种新颖的策略，通过加权采样进行对比学习来微调多语言嵌入模型，支持单语知识库的多语言信息检索。

Result: 加权采样策略在MRR上提升了31.03%，在Recall@3上提升了33.98%，且方法适用于多语言和代码切换场景。

Conclusion: 该方法语言无关，能有效提升跨语言知识共享的性能，适用于多种语言场景。

Abstract: Multilingual information retrieval has emerged as powerful tools for
expanding knowledge sharing across languages. On the other hand, resources on
high quality knowledge base are often scarce and in limited languages,
therefore an effective embedding model to transform sentences from different
languages into a feature vector space same as the knowledge base language
becomes the key ingredient for cross language knowledge sharing, especially to
transfer knowledge available in high-resource languages to low-resource ones.
In this paper we propose a novel strategy to fine-tune multilingual embedding
models with weighted sampling for contrastive learning, enabling multilingual
information retrieval with a monolingual knowledge base. We demonstrate that
the weighted sampling strategy produces performance gains compared to standard
ones by up to 31.03\% in MRR and up to 33.98\% in Recall@3. Additionally, our
proposed methodology is language agnostic and applicable for both multilingual
and code switching use cases.

</details>


### [54] [ReasoningFlow: Semantic Structure of Complex Reasoning Traces](https://arxiv.org/abs/2506.02532)
*Jinu Lee,Sagnik Mukherjee,Dilek Hakkani-Tur,Julia Hockenmaier*

Main category: cs.CL

TL;DR: 本文提出ReasoningFlow，一种用于分析大型推理模型生成复杂推理轨迹语义结构的统一框架。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型生成的推理轨迹包含规划、反思、验证和回溯等复杂结构，需要一种统一的方法来分析和理解这些结构。

Method: 提出ReasoningFlow框架，将推理轨迹解析为有向无环图，通过子图结构表征不同的推理模式。

Result: 该方法提供了人类可解释的表示形式，有助于理解、评估和改进大型推理模型的推理过程。

Conclusion: ReasoningFlow为分析复杂推理轨迹提供了有效工具，具有重要的应用潜力。

Abstract: Large reasoning models (LRMs) generate complex reasoning traces with
planning, reflection, verification, and backtracking. In this work, we
introduce ReasoningFlow, a unified schema for analyzing the semantic structures
of these complex traces. ReasoningFlow parses traces into directed acyclic
graphs, enabling the characterization of distinct reasoning patterns as
subgraph structures. This human-interpretable representation offers promising
applications in understanding, evaluating, and enhancing the reasoning
processes of LRMs.

</details>


### [55] [Natural Language Processing to Enhance Deliberation in Political Online Discussions: A Survey](https://arxiv.org/abs/2506.02533)
*Maike Behrendt,Stefan Sylvius Wagner,Carina Weinmann,Marike Bormann,Mira Warne,Stefan Harmeling*

Main category: cs.CL

TL;DR: 论文探讨了在线政治讨论中的问题，并提出利用机器学习提升讨论质量。


<details>
  <summary>Details</summary>
Motivation: 随着政治讨论越来越多地转移到线上，如何保持讨论的文明和深思熟虑成为重要议题。平台设计对讨论质量有重大影响。

Method: 研究采用机器学习方法，分析在线政治讨论中的问题，并提出技术解决方案。

Result: 机器学习能有效识别和缓解在线讨论中的问题，促进更高质量的审议过程。

Conclusion: 机器学习在提升在线政治讨论质量方面具有巨大潜力，未来可进一步优化平台设计。

Abstract: Political online participation in the form of discussing political issues and
exchanging opinions among citizens is gaining importance with more and more
formats being held digitally. To come to a decision, a careful discussion and
consideration of opinions and a civil exchange of arguments, which is defined
as the act of deliberation, is desirable. The quality of discussions and
participation processes in terms of their deliberativeness highly depends on
the design of platforms and processes. To facilitate online communication for
both participants and initiators, machine learning methods offer a lot of
potential. In this work we want to showcase which issues occur in political
online discussions and how machine learning can be used to counteract these
issues and enhance deliberation.

</details>


### [56] [Answer Convergence as a Signal for Early Stopping in Reasoning](https://arxiv.org/abs/2506.02536)
*Xin Liu,Lu Wang*

Main category: cs.CL

TL;DR: 论文提出三种推理时策略，减少大语言模型思维链提示的冗余步骤，显著降低计算成本且几乎不影响准确率。


<details>
  <summary>Details</summary>
Motivation: 现有思维链提示方法存在输出冗长、推理步骤冗余的问题，导致推理成本增加。研究发现大部分推理步骤对最终答案无实质贡献。

Method: 提出三种高效推理策略：1)基于答案一致性的早停机制；2)增强终止信号生成概率；3)基于内部激活的监督式停止学习。

Result: 在5个基准测试和5个开源大模型上验证，方法显著减少40%以上token使用（如NaturalQuestions），部分场景准确率反而提升。

Conclusion: 研究证明了推理时优化方法的有效性，为实际应用提供了高性价比的解决方案。

Abstract: Chain-of-thought (CoT) prompting enhances reasoning in large language models
(LLMs) but often leads to verbose and redundant outputs, thus increasing
inference cost. We hypothesize that many reasoning steps are unnecessary for
producing correct answers. To investigate this, we start with a systematic
study to examine what is the minimum reasoning required for a model to reach a
stable decision. We find that on math reasoning tasks like math, models
typically converge to their final answers after 60\% of the reasoning steps,
suggesting substantial redundancy in the remaining content. Based on these
insights, we propose three inference-time strategies to improve efficiency: (1)
early stopping via answer consistency, (2) boosting the probability of
generating end-of-reasoning signals, and (3) a supervised method that learns
when to stop based on internal activations. Experiments across five benchmarks
and five open-weights LLMs show that our methods significantly reduce token
usage with little or no accuracy drop. In particular, on NaturalQuestions,
Answer Consistency reduces tokens by over 40\% while further improving
accuracy. Our work underscores the importance of cost-effective reasoning
methods that operate at inference time, offering practical benefits for
real-world applications.

</details>


### [57] [CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG](https://arxiv.org/abs/2506.02544)
*Yang Tian,Fan Liu,Jingyuan Zhang,Victoria W.,Yupeng Hu,Liqiang Nie*

Main category: cs.CL

TL;DR: CoRe-MMRAG提出了一种新的端到端框架，通过四阶段流程解决多模态检索增强生成中的知识不一致问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 多模态检索增强生成（MMRAG）在增强多模态大语言模型时，引入了参数化检索知识不一致（PRKI）和视觉-文本知识不一致（VTKI）两大挑战，影响了知识的可靠性和实体表示。

Method: CoRe-MMRAG采用四阶段流程：首先生成内部参数化知识响应，然后通过联合相似性评估选择最相关的多模态证据，生成外部响应，最后整合两者生成可靠答案。此外，采用专门训练范式增强知识源区分、多模态整合和统一答案生成。

Result: 在KB-VQA基准测试中，CoRe-MMRAG相比基线方法有显著提升，在InfoSeek和Encyclopedic-VQA上分别实现了5.6%和9.3%的性能提升。

Conclusion: CoRe-MMRAG有效解决了多模态检索增强生成中的知识不一致问题，通过整合多源知识和专门训练，显著提升了模型性能。

Abstract: Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to
enhance Multimodal Large Language Models by incorporating externally retrieved
multimodal knowledge, but it introduces two challenges: Parametric-Retrieved
Knowledge Inconsistency (PRKI), where discrepancies between parametric and
retrieved knowledge create uncertainty in determining reliability, and
Visual-Textual Knowledge Inconsistency (VTKI), where misalignment between
visual and textual sources disrupts entity representation. To address these
challenges, we propose \textbf{C}r\textbf{o}ss-source knowledge
\textbf{Re}conciliation for \textbf{M}ulti\textbf{M}odal \textbf{RAG}
(CoRe-MMRAG), a novel end-to-end framework that effectively reconciles
inconsistencies across knowledge sources. CoRe-MMRAG follows a four-stage
pipeline: it first generates an internal response from parametric knowledge,
then selects the most relevant multimodal evidence via joint similarity
assessment, generates an external response, and finally integrates both to
produce a reliable answer. Additionally, a specialized training paradigm
enhances knowledge source discrimination, multimodal integration, and unified
answer generation. Experiments on KB-VQA benchmarks show that CoRe-MMRAG
achieves substantial improvements over baseline methods, achieving 5.6\% and
9.3\% performance gains on InfoSeek and Encyclopedic-VQA, respectively. We
release code and data at
\href{https://github.com/TyangJN/CoRe-MMRAG}{https://github.com/TyangJN/CoRe-MMRAG}.

</details>


### [58] [Pruning General Large Language Models into Customized Expert Models](https://arxiv.org/abs/2506.02561)
*Yirao Zhao,Guizhen Chen,Kenji Kawaguchi,Lidong Bing,Wenxuan Zhang*

Main category: cs.CL

TL;DR: 提出Cus-Prun方法，通过定制化剪枝将大模型压缩为轻量专家模型，无需后训练且性能损失最小。


<details>
  <summary>Details</summary>
Motivation: 大语言模型参数量大、计算资源消耗高，现有剪枝方法多关注通用能力保留，但缺乏针对特定场景的轻量化专家模型。

Method: 基于语言、领域、任务三维度剪枝无关神经元，直接生成轻量专家模型。

Result: Cus-Prun在多模型测试中优于其他方法，专家能力和通用能力损失均最小。

Conclusion: Cus-Prun为特定下游任务提供高效定制化模型压缩方案。

Abstract: Large language models (LLMs) have revolutionized natural language processing,
yet their substantial model sizes often require substantial computational
resources. To preserve computing resources and accelerate inference speed, it
is crucial to prune redundant parameters, especially for experienced users who
often need compact expert models tailored to specific downstream scenarios.
However, most existing pruning methods focus on preserving the model's general
capabilities, often requiring extensive post-training or suffering from
degraded performance due to coarse-grained pruning. In this work, we design a
$\underline{Cus}$tom $\underline{Prun}$ing method ($\texttt{Cus-Prun}$) to
prune a large general model into a smaller lightweight expert model, which is
positioned along the "language", "domain" and "task" dimensions. By identifying
and pruning irrelevant neurons of each dimension, $\texttt{Cus-Prun}$ creates
expert models without any post-training. Our experiments demonstrate that
$\texttt{Cus-Prun}$ consistently outperforms other methods, achieving minimal
loss in both expert and general capabilities across various models from
different model families and sizes.

</details>


### [59] [IndoSafety: Culturally Grounded Safety for LLMs in Indonesian Languages](https://arxiv.org/abs/2506.02573)
*Muhammad Falensi Azmi,Muhammad Dehan Al Kautsar,Alfan Farizki Wicaksono,Fajri Koto*

Main category: cs.CL

TL;DR: 该研究开发了首个针对印尼语境的高质量安全评估数据集IndoSafety，涵盖五种语言变体，发现现有印尼中心LLM在方言和本地语言中常生成不安全输出，而基于IndoSafety微调可显著提升安全性。


<details>
  <summary>Details</summary>
Motivation: 印尼等文化多元地区需要符合本地规范的安全评估，但现有LLM安全性研究不足，特别是对当地方言和文化的敏感性。

Method: 扩展现有安全框架构建印尼社会文化语境分类体系，创建包含正式/口语印尼语及三种主要方言的人类验证数据集IndoSafety。

Result: 现有印尼LLM在口语和方言场景下易产生不安全输出，使用IndoSafety微调后模型安全性显著提升且任务性能不受影响。

Conclusion: 文化根基的安全评估对多语种LLM部署至关重要，IndoSafety为负责任的人工智能发展提供了实践基础。

Abstract: Although region-specific large language models (LLMs) are increasingly
developed, their safety remains underexplored, particularly in culturally
diverse settings like Indonesia, where sensitivity to local norms is essential
and highly valued by the community. In this work, we present IndoSafety, the
first high-quality, human-verified safety evaluation dataset tailored for the
Indonesian context, covering five language varieties: formal and colloquial
Indonesian, along with three major local languages: Javanese, Sundanese, and
Minangkabau. IndoSafety is constructed by extending prior safety frameworks to
develop a taxonomy that captures Indonesia's sociocultural context. We find
that existing Indonesian-centric LLMs often generate unsafe outputs,
particularly in colloquial and local language settings, while fine-tuning on
IndoSafety significantly improves safety while preserving task performance. Our
work highlights the critical need for culturally grounded safety evaluation and
provides a concrete step toward responsible LLM deployment in multilingual
settings. Warning: This paper contains example data that may be offensive,
harmful, or biased.

</details>


### [60] [Prosodic Structure Beyond Lexical Content: A Study of Self-Supervised Learning](https://arxiv.org/abs/2506.02584)
*Sarenne Wallbridge,Christoph Minixhofer,Catherine Lai,Peter Bell*

Main category: cs.CL

TL;DR: 该研究利用自监督学习分析韵律声学特征的时间粒度，发现其能有效预测涉及长期结构的感知标签，如情感识别。


<details>
  <summary>Details</summary>
Motivation: 探讨韵律（如语调、节奏和响度）在独立于词汇内容的情况下，对语言结构预测的贡献程度。

Method: 使用自监督学习（SSL）方法，特别是提出的Masked Prosody Model，分析韵律声学特征的时间粒度。

Result: 模型能预测依赖局部信息的感知标签（如词边界），但对涉及长期结构的标签（如情感识别）效果更显著，且优于传统声学特征。

Conclusion: 研究揭示了SSL训练目标时间尺度的重要性，并展示了SSL编码的复杂结构相较于传统结构的优势。

Abstract: People exploit the predictability of lexical structures during text
comprehension. Though predictable structure is also present in speech, the
degree to which prosody, e.g. intonation, tempo, and loudness, contributes to
such structure independently of the lexical content is unclear. This study
leverages self-supervised learning (SSL) to examine the temporal granularity of
structures in the acoustic correlates of prosody. Representations from our
proposed Masked Prosody Model can predict perceptual labels dependent on local
information, such as word boundaries, but provide the most value for labels
involving longer-term structures, like emotion recognition. Probing experiments
across various perceptual labels show strong relative gains over untransformed
pitch, energy, and voice activity features. Our results reveal the importance
of SSL training objective timescale and highlight the value of complex
SSL-encoded structures compared to more constrained classical structures.

</details>


### [61] [Evaluating Named Entity Recognition Models for Russian Cultural News Texts: From BERT to LLM](https://arxiv.org/abs/2506.02589)
*Maria Levchenko*

Main category: cs.CL

TL;DR: 该论文评估了多种NER模型在俄文文化事件新闻中识别人名的表现，发现GPT-4o在特定提示下表现最佳，F1分数达0.93。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决俄文文化事件新闻中人名识别的挑战，特别是在形态丰富的语言环境下，评估当前NER模型的能力和限制。

Method: 研究使用了SPbLitGuide数据集，比较了多种NER模型，包括DeepPavlov、RoBERTa、SpaCy等传统模型，以及GPT-3.5、GPT-4和GPT-4o等大型语言模型。

Result: 结果显示，GPT-4o在特定JSON输出提示下表现最佳（F1=0.93），GPT-4的精确度最高（0.99）。后续评估中，GPT-4.1的F1分数进一步提升至0.94。

Conclusion: 研究表明，大型语言模型在俄文文化领域NER任务中表现优异，尤其是GPT系列模型，展示了快速进步和简化部署的潜力。

Abstract: This paper addresses the challenge of Named Entity Recognition (NER) for
person names within the specialized domain of Russian news texts concerning
cultural events. The study utilizes the unique SPbLitGuide dataset, a
collection of event announcements from Saint Petersburg spanning 1999 to 2019.
A comparative evaluation of diverse NER models is presented, encompassing
established transformer-based architectures such as DeepPavlov, RoBERTa, and
SpaCy, alongside recent Large Language Models (LLMs) including GPT-3.5, GPT-4,
and GPT-4o. Key findings highlight the superior performance of GPT-4o when
provided with specific prompting for JSON output, achieving an F1 score of
0.93. Furthermore, GPT-4 demonstrated the highest precision at 0.99. The
research contributes to a deeper understanding of current NER model
capabilities and limitations when applied to morphologically rich languages
like Russian within the cultural heritage domain, offering insights for
researchers and practitioners. Follow-up evaluation with GPT-4.1 (April 2025)
achieves F1=0.94 for both simple and structured prompts, demonstrating rapid
progress across model families and simplified deployment requirements.

</details>


### [62] [On Generalization across Measurement Systems: LLMs Entail More Test-Time Compute for Underrepresented Cultures](https://arxiv.org/abs/2506.02591)
*Minh Duc Bui,Kyung Eun Park,Goran Glavaš,Fabian David Schmidt,Katharina von der Wense*

Main category: cs.CL

TL;DR: 论文研究了不同文化背景下大语言模型（LLMs）处理测量系统的能力，发现LLMs默认使用数据中主流的测量系统，且在不同系统间表现不稳定，推理方法可部分缓解问题但增加计算成本。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨LLMs在不同文化背景下处理多种测量系统（如货币单位）的能力，确保其能为不同文化背景用户提供准确信息。

Method: 方法包括使用新编译的数据集测试七个开源LLMs，研究其默认测量系统、跨系统表现差异及推理方法对性能的影响。

Result: 结果显示LLMs倾向于使用数据中主流的测量系统，不同系统间表现不稳定，推理方法（如思维链）可部分改善准确性但增加计算成本。

Conclusion: 结论指出LLMs在处理非主流测量系统时存在挑战，推理方法虽有效但会提高使用门槛，可能边缘化使用非主流系统的文化群体。

Abstract: Measurement systems (e.g., currencies) differ across cultures, but the
conversions between them are well defined so that humans can state facts using
any measurement system of their choice. Being available to users from diverse
cultural backgrounds, large language models (LLMs) should also be able to
provide accurate information irrespective of the measurement system at hand.
Using newly compiled datasets we test if this is the case for seven open-source
LLMs, addressing three key research questions: (RQ1) What is the default system
used by LLMs for each type of measurement? (RQ2) Do LLMs' answers and their
accuracy vary across different measurement systems? (RQ3) Can LLMs mitigate
potential challenges w.r.t. underrepresented systems via reasoning? Our
findings show that LLMs default to the measurement system predominantly used in
the data. Additionally, we observe considerable instability and variance in
performance across different measurement systems. While this instability can in
part be mitigated by employing reasoning methods such as chain-of-thought
(CoT), this implies longer responses and thereby significantly increases
test-time compute (and inference costs), marginalizing users from cultural
backgrounds that use underrepresented measurement systems.

</details>


### [63] [Beyond the Surface: Measuring Self-Preference in LLM Judgments](https://arxiv.org/abs/2506.02592)
*Zhi-Yuan Chen,Hao Wang,Xinyu Zhang,Enrui Hu,Yankai Lin*

Main category: cs.CL

TL;DR: 该论文提出DBG分数来测量大语言模型(LLM)作为评判者时的自我偏好偏差，通过引入黄金判断作为响应质量的代理，解决了现有方法混淆偏差与响应质量的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在测量LLM作为评判者时的自我偏好偏差时，往往将偏差与响应质量混为一谈，导致测量结果不准确。本文旨在解决这一问题。

Method: 引入黄金判断作为响应质量的代理，提出DBG分数，通过计算评判模型对自己响应的评分与黄金判断之间的差异来测量自我偏好偏差。

Result: 实验表明，DBG分数能有效减少响应质量对偏差测量的干扰，并评估了不同版本、规模和推理能力的LLM的自我偏好偏差。还探讨了影响和缓解偏差的因素。

Conclusion: DBG分数提供了一种更准确的自我偏好偏差测量方法，并揭示了影响偏差的因素及其潜在机制，为未来研究提供了方向。

Abstract: Recent studies show that large language models (LLMs) exhibit self-preference
bias when serving as judges, meaning they tend to favor their own responses
over those generated by other models. Existing methods typically measure this
bias by calculating the difference between the scores a judge model assigns to
its own responses and those it assigns to responses from other models. However,
this approach conflates self-preference bias with response quality, as
higher-quality responses from the judge model may also lead to positive score
differences, even in the absence of bias. To address this issue, we introduce
gold judgments as proxies for the actual quality of responses and propose the
DBG score, which measures self-preference bias as the difference between the
scores assigned by the judge model to its own responses and the corresponding
gold judgments. Since gold judgments reflect true response quality, the DBG
score mitigates the confounding effect of response quality on bias measurement.
Using the DBG score, we conduct comprehensive experiments to assess
self-preference bias across LLMs of varying versions, sizes, and reasoning
abilities. Additionally, we investigate two factors that influence and help
alleviate self-preference bias: response text style and the post-training data
of judge models. Finally, we explore potential underlying mechanisms of
self-preference bias from an attention-based perspective. Our code and data are
available at https://github.com/zhiyuanc2001/self-preference.

</details>


### [64] [EssayBench: Evaluating Large Language Models in Multi-Genre Chinese Essay Writing](https://arxiv.org/abs/2506.02596)
*Fan Gao,Dongyuan Li,Ding Xia,Fei Mi,Yasheng Wang,Lifeng Shang,Baojun Wang*

Main category: cs.CL

TL;DR: 该论文提出了一个名为\benchName的多体裁中文写作基准测试，用于评估大型语言模型在四种主要中文写作体裁（议论文、记叙文、描写文和说明文）中的表现，并开发了一个细粒度的评分框架。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在中文写作领域的评估能力尚未充分探索，现有基准测试往往忽略了中文作文的结构和修辞复杂性，特别是在不同体裁中的表现。

Method: 研究团队收集并筛选了728个真实世界的中文写作提示，分为开放性和约束性两类，并开发了一个细粒度的、按体裁分层的评分框架，通过人工一致性研究验证了评估协议。

Result: 研究对15个大型语言模型进行了基准测试，分析了它们在不同体裁和指令类型中的优势和局限性。

Conclusion: 通过\benchName，研究旨在推动基于大型语言模型的中文作文评估，并激发未来在教育环境中改进作文生成的研究。

Abstract: Chinese essay writing and its evaluation are critical in educational
contexts, yet the capabilities of Large Language Models (LLMs) in this domain
remain largely underexplored. Existing benchmarks often rely on coarse-grained
text quality metrics, largely overlooking the structural and rhetorical
complexities of Chinese essays, particularly across diverse genres. To address
this gap, we propose \benchName, a multi-genre benchmark specifically designed
for Chinese essay writing across four major genres: Argumentative, Narrative,
Descriptive, and Expository. We curate and refine a total of 728 real-world
prompts to ensure authenticity and meticulously categorize them into the
\textit{Open-Ended} and \textit{Constrained} sets to capture diverse writing
scenarios. To reliably evaluate generated essays, we develop a fine-grained,
genre-specific scoring framework that hierarchically aggregates scores. We
further validate our evaluation protocol through a comprehensive human
agreement study. Finally, we benchmark 15 large-sized LLMs, analyzing their
strengths and limitations across genres and instruction types. With \benchName,
we aim to advance LLM-based Chinese essay evaluation and inspire future
research on improving essay generation in educational settings.

</details>


### [65] [Overcoming Data Scarcity in Multi-Dialectal Arabic ASR via Whisper Fine-Tuning](https://arxiv.org/abs/2506.02627)
*Ömer Tarik Özyilmaz,Matt Coler,Matias Valdenegro-Toro*

Main category: cs.CL

TL;DR: 该研究探讨了如何通过微调Whisper模型提升阿拉伯语方言的自动语音识别性能，发现少量标准阿拉伯语数据即可显著改善小模型表现，且方言数据合并使用不会显著降低性能。


<details>
  <summary>Details</summary>
Motivation: 商用阿拉伯语自动语音识别系统在处理方言语音时表现不佳，研究旨在探索如何通过微调现有模型来提升方言识别效果。

Method: 使用OpenAI的Whisper模型，在五大阿拉伯语方言上进行微调，利用Mozilla Common Voice的标准阿拉伯语数据和MASC方言数据集，评估了标准阿拉伯语训练数据量、预训练效果及方言合并模型的表现。

Result: 少量标准阿拉伯语微调数据对小模型有显著提升；标准阿拉伯语预训练效果有限；方言合并模型与方言专用模型表现相当。

Conclusion: 在数据稀缺的低资源自动语音识别任务中，平衡合并方言数据可有效缓解数据不足问题，且不会造成显著性能损失。

Abstract: Although commercial Arabic automatic speech recognition (ASR) systems support
Modern Standard Arabic (MSA), they struggle with dialectal speech. We
investigate the effect of fine-tuning OpenAI's Whisper on five major Arabic
dialects (Gulf, Levantine, Iraqi, Egyptian, Maghrebi) using Mozilla Common
Voice for MSA and the MASC dataset for dialectal speech. We evaluate MSA
training size effects, benefits of pre-training on MSA data, and
dialect-specific versus dialect-pooled models. We find that small amounts of
MSA fine-tuning data yield substantial improvements for smaller models,
matching larger non-fine-tuned models. While MSA pre-training shows minimal
benefit, suggesting limited shared features between MSA and dialects, our
dialect-pooled models perform comparably to dialect-specific ones. This
indicates that pooling dialectal data, when properly balanced, can help address
data scarcity in low-resource ASR without significant performance loss.

</details>


### [66] [Are Economists Always More Introverted? Analyzing Consistency in Persona-Assigned LLMs](https://arxiv.org/abs/2506.02659)
*Manon Reusens,Bart Baesens,David Jurgens*

Main category: cs.CL

TL;DR: 该论文提出了一个标准化框架，用于分析赋予特定角色的大型语言模型（LLMs）在不同任务中的一致性表现。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究探讨了LLMs在写作风格上如何遵循预定义角色，但缺乏对不同角色和任务类型下一致性的全面分析。

Method: 论文引入了一个新框架，通过四个角色类别（幸福感、职业、个性、政治立场）和多个任务维度（调查写作、文章生成、社交媒体帖子生成、单轮和多轮对话）来评估一致性。

Result: 研究发现，一致性受多种因素影响，包括角色设定、刻板印象和模型设计选择，且在不同任务中表现不一，结构化任务和额外上下文能提升一致性。

Conclusion: 该框架为评估角色一致性提供了标准化方法，揭示了影响LLMs表现的关键因素，所有代码已在GitHub上公开。

Abstract: Personalized Large Language Models (LLMs) are increasingly used in diverse
applications, where they are assigned a specific persona - such as a happy high
school teacher - to guide their responses. While prior research has examined
how well LLMs adhere to predefined personas in writing style, a comprehensive
analysis of consistency across different personas and task types is lacking. In
this paper, we introduce a new standardized framework to analyze consistency in
persona-assigned LLMs. We define consistency as the extent to which a model
maintains coherent responses when assigned the same persona across different
tasks and runs. Our framework evaluates personas across four different
categories (happiness, occupation, personality, and political stance) spanning
multiple task dimensions (survey writing, essay generation, social media post
generation, single turn, and multi-turn conversations). Our findings reveal
that consistency is influenced by multiple factors, including the assigned
persona, stereotypes, and model design choices. Consistency also varies across
tasks, increasing with more structured tasks and additional context. All code
is available on GitHub.

</details>


### [67] [EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving](https://arxiv.org/abs/2506.02672)
*Shihan Dou,Ming Zhang,Chenhao Huang,Jiayi Chen,Feng Chen,Shichun Liu,Yan Liu,Chenxiao Liu,Cheng Zhong,Zongzhang Zhang,Tao Gui,Chao Xin,Wei Chengzhi,Lin Yan,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: EvaLearn是一个评估大语言模型学习能力和效率的新基准，包含648个挑战性问题，通过序列化任务和五项指标量化模型表现。实验发现不同模型学习能力差异显著，且静态能力强的模型未必学习能力突出。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型评估多关注静态能力，而学习能力这一关键潜力维度尚未充分探索。EvaLearn旨在填补这一空白，通过序列化任务设计模拟人类学习过程。

Method: 构建含182个任务序列的基准（共648题），要求模型顺序解题以积累经验。采用五项自动化指标评估，并测试两种学习设置（实例级评分和教师模型反馈）。

Result: 不同模型表现分化：如Claude-3.7初始中等但学习能力强，部分模型甚至出现负迁移。静态能力强的模型未在所有任务中展现学习优势，反馈机制可提升学习效果。

Conclusion: EvaLearn揭示了模型学习能力的新维度，证明静态性能与动态学习能力不直接相关，为评估模型潜力及缩小人机差距提供了新视角。

Abstract: We introduce EvaLearn, a pioneering benchmark designed to evaluate large
language models (LLMs) on their learning capability and efficiency in
challenging tasks, a critical, yet underexplored aspect of model potential.
EvaLearn contains 648 challenging problems across six task types, grouped into
182 sequences, each sequence dedicated to one task type. Diverging from most
existing benchmarks that evaluate models in parallel, EvaLearn requires models
to solve problems sequentially, allowing them to leverage the experience gained
from previous solutions. EvaLearn provides five comprehensive automated metrics
to evaluate models and quantify their learning capability and efficiency. We
extensively benchmark nine frontier models and observe varied performance
profiles: some models, such as Claude-3.7-sonnet, start with moderate initial
performance but exhibit strong learning ability, while some models struggle to
benefit from experience and may even show negative transfer. Moreover, we
investigate model performance under two learning settings and find that
instance-level rubrics and teacher-model feedback further facilitate model
learning. Importantly, we observe that current LLMs with stronger static
abilities do not show a clear advantage in learning capability across all
tasks, highlighting that EvaLearn evaluates a new dimension of model
performance. We hope EvaLearn provides a novel evaluation perspective for
assessing LLM potential and understanding the gap between models and human
capabilities, promoting the development of deeper and more dynamic evaluation
approaches. All datasets, the automatic evaluation framework, and the results
studied in this paper are available at the GitHub repository.

</details>


### [68] [TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression](https://arxiv.org/abs/2506.02678)
*Zhong-Zhi Li,Xiao Liang,Zihao Tang,Lei Ji,Peijie Wang,Haotian Xu,Xing W,Haizhen Huang,Weiwei Deng,Ying Nian Wu,Yeyun Gong,Zhijiang Guo,Xiao Liu,Fei Yin,Cheng-Lin Liu*

Main category: cs.CL

TL;DR: 本文提出了一种动态比例训练方法，显著减少大语言模型推理时的输出长度，同时保持推理准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在生成长输出时存在推理效率低下的问题，研究社区对此日益关注。

Method: 采用动态比例训练流程，平衡System-1和System-2数据的权重，消除冗余推理过程。

Result: 在DeepSeek-R1-Distill-7B和14B模型上验证，输出token减少近40%，推理准确率保持不变。

Conclusion: 该方法有效提升了语言模型的推理效率，为长输出场景提供了实用解决方案。

Abstract: Large Language Models (LLMs) have recently achieved remarkable progress by
leveraging Reinforcement Learning and extended Chain-of-Thought (CoT)
techniques. However, the challenge of performing efficient language
reasoning--especially during inference with extremely long outputs--has drawn
increasing attention from the research community. In this work, we propose a
dynamic ratio-based training pipeline that does not rely on sophisticated data
annotations or interpolation between multiple models. We continuously balance
the weights between the model's System-1 and System-2 data to eliminate
redundant reasoning processes while preserving the model's reasoning
capability. We validate our approach across models on DeepSeek-R1-Distill-7B
and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying
difficulty levels. Our method significantly reduces the number of output tokens
by nearly 40% while maintaining the accuracy of the reasoning. Our code and
data will be available soon.

</details>


### [69] [Decompose, Plan in Parallel, and Merge: A Novel Paradigm for Large Language Models based Planning with Multiple Constraints](https://arxiv.org/abs/2506.02683)
*Zhengdong Lu,Weikai Lu,Yiling Tao,Yun Dai,ZiXuan Chen,Huiping Zhuang,Cen Chen,Hao Peng,Ziqian Zeng*

Main category: cs.CL

TL;DR: 提出并行规划范式DPPM，通过分解、并行规划和合并子计划解决LLM在规划任务中的约束和错误累积问题，并在旅行规划任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的规划方法存在两大局限：强约束和错误累积，限制了规划任务的效率与准确性。

Method: DPPM范式：将复杂任务分解为子任务，并行生成子计划后合并为全局计划，并加入验证与优化模块以修正错误和冲突。

Result: 实验表明，DPPM在旅行规划任务中显著优于现有方法。

Conclusion: DPPM通过并行规划与动态修正，有效提升了LLM在复杂规划任务中的性能。

Abstract: Despite significant advances in Large Language Models (LLMs), planning tasks
still present challenges for LLM-based agents. Existing planning methods face
two key limitations: heavy constraints and cascading errors. To address these
limitations, we propose a novel parallel planning paradigm, which Decomposes,
Plans for subtasks in Parallel, and Merges subplans into a final plan (DPPM).
Specifically, DPPM decomposes the complex task based on constraints into
subtasks, generates the subplan for each subtask in parallel, and merges them
into a global plan. In addition, our approach incorporates a verification and
refinement module, enabling error correction and conflict resolution.
Experimental results demonstrate that DPPM significantly outperforms existing
methods in travel planning tasks.

</details>


### [70] [MASTER: Enhancing Large Language Model via Multi-Agent Simulated Teaching](https://arxiv.org/abs/2506.02689)
*Liang Yue,Yihong Tang,Kehai Chen,Jie Liu,Min Zhang*

Main category: cs.CL

TL;DR: 提出MASTER数据增强方法，通过多智能体交互生成高质量微调数据，显著提升模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 高质量微调数据获取困难且成本高，制约大模型性能提升。

Method: MASTER方法模拟教学场景，通过多认知层级智能体对话生成教师-学生交互数据。

Result: 基于BOOST-QA数据集微调的模型在多任务基准测试中表现优异，推理能力显著提升。

Conclusion: MASTER为数据增强提供新思路，其生成的数据能有效提升模型泛化与推理能力。

Abstract: Instruction fine-tuning is crucial in NLP tasks, enhancing pretrained models'
instruction-following capabilities and task-specific performance. However,
obtaining high-quality fine-tuning data for large models is challenging due to
data collection difficulties and high production costs. To address this, we
propose MASTER, a novel data augmentation method that enriches original data
through interactions among multiple agents with varying cognitive levels. We
simulate three pedagogically grounded teaching scenarios, leveraging
multi-agent conversations to generate high-quality teacher-student interaction
data. Utilizing MASTER, we construct BOOST-QA, a fine-tuning dataset augmented
from existing datasets like Orca-Math-200k, ProcQA, and OpenHermes2.5.
Experiments show that models fine-tuned with BOOST-QA perform excellently
across multiple benchmarks, demonstrating strong multitask generalization.
Notably, MASTER significantly improves models' reasoning abilities in complex
tasks, providing valuable insights for future research.

</details>


### [71] [On Entity Identification in Language Models](https://arxiv.org/abs/2506.02701)
*Masaki Sakata,Sho Yokoi,Benjamin Heinzerling,Takumi Ito,Kentaro Inui*

Main category: cs.CL

TL;DR: 该研究通过聚类分析方法量化了语言模型内部表示对命名实体的识别与区分能力，发现Transformer模型能有效处理实体提及的歧义性和变异性，且实体信息在早期层以低维线性子空间紧凑表示。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型内部表示如何识别和区分命名实体的提及，解决实体与其提及之间多对多对应关系带来的歧义性和变异性问题。

Method: 提出类似聚类质量指标的框架，通过分析语言模型内部表示的聚类情况，量化同一实体提及的聚集程度和不同实体提及的分离程度。实验研究了五种基于Transformer的自回归模型。

Result: 实验显示，这些模型能有效识别和区分实体，类精确度和召回率指标在0.66到0.9之间。实体相关信息在模型早期层以低维线性子空间紧凑表示，且实体表示特性影响词语预测性能。

Conclusion: 研究揭示了语言模型内部表示与真实世界以实体为中心的知识结构之间的同构关系，为理解模型如何组织和利用实体信息提供了新见解。

Abstract: We analyze the extent to which internal representations of language models
(LMs) identify and distinguish mentions of named entities, focusing on the
many-to-many correspondence between entities and their mentions. We first
formulate two problems of entity mentions -- ambiguity and variability -- and
propose a framework analogous to clustering quality metrics. Specifically, we
quantify through cluster analysis of LM internal representations the extent to
which mentions of the same entity cluster together and mentions of different
entities remain separated. Our experiments examine five Transformer-based
autoregressive models, showing that they effectively identify and distinguish
entities with metrics analogous to precision and recall ranging from 0.66 to
0.9. Further analysis reveals that entity-related information is compactly
represented in a low-dimensional linear subspace at early LM layers.
Additionally, we clarify how the characteristics of entity representations
influence word prediction performance. These findings are interpreted through
the lens of isomorphism between LM representations and entity-centric knowledge
structures in the real world, providing insights into how LMs internally
organize and use entity information.

</details>


### [72] [RACE-Align: Retrieval-Augmented and Chain-of-Thought Enhanced Preference Alignment for Large Language Models](https://arxiv.org/abs/2506.02726)
*Qihang Yan,Xinyu Zhang,Luming Guo,Qi Zhang,Feifan Liu*

Main category: cs.CL

TL;DR: 本文提出RACE-Align框架，通过结合外部知识检索和思维链增强对齐，提升大语言模型在垂直领域的准确性、推理能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在垂直领域存在准确性、领域推理和可解释性不足的问题，传统偏好对齐方法（如RLHF和DPO）常忽略知识来源和推理逻辑。

Method: RACE-Align框架通过构建包含外部知识支持和显式思维链推理的偏好数据集，并利用DPO算法对齐模型，结合AI驱动的检索和多阶段优化流程。

Result: 在中医领域实验中，RACE-Align显著优于基础模型和仅用监督微调的模型，在准确性、信息丰富度、推理逻辑和可解释性等方面均有提升。

Conclusion: RACE-Align为增强大语言模型在复杂垂直领域的知识应用、推理可靠性和过程透明度提供了有效途径。

Abstract: Large Language Models (LLMs) struggle with accuracy, domain-specific
reasoning, and interpretability in vertical domains. Traditional preference
alignment methods like Reinforcement Learning from Human Feedback (RLHF) and
Direct Preference Optimization (DPO) often overlook the underlying knowledge
sources and reasoning logic. This paper introduces RACE-Align
(Retrieval-Augmented and Chain-of-Thought Enhanced Alignment), a novel
framework designed to address these limitations. RACE-Align systematically
constructs a binary preference dataset incorporating external knowledge support
and explicit Chain-of-Thought (CoT) reasoning, then aligns LLMs using the DPO
algorithm. The core innovation lies in its preference data construction
strategy: it integrates AI-driven retrieval for factual grounding, enhancing
knowledgeability and accuracy, and emphasizes the optimization of
domain-specific CoT, treating the reasoning process itself as a key preference
dimension. A multi-stage, AI-driven refinement pipeline cost-effectively
generates these preference pairs. Experimental validation in Traditional
Chinese Medicine (TCM) using Qwen3-1.7B as the base model demonstrates that
RACE-Align significantly outperforms the original base model and a model
fine-tuned only with Supervised Fine-Tuning (SFT). Improvements were observed
across multiple dimensions, including answer accuracy, information richness,
application of TCM thinking patterns, logicality and depth of reasoning, and
interpretability. These findings suggest RACE-Align offers an effective pathway
to enhance LLMs' knowledge application, reasoning reliability, and process
transparency in complex vertical domains.

</details>


### [73] [Stereotypical gender actions can be extracted from Web text](https://arxiv.org/abs/2506.02740)
*Amaç Herdağdelen,Marco Baroni*

Main category: cs.CL

TL;DR: 通过文本语料库和Twitter提取性别特定行为，并与刻板印象对比，验证了利用自然文本增强常识库的可行性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索如何利用自然文本（特别是Twitter语料）来增强常识知识库中关于性别刻板印象的行为信息。

Method: 使用Open Mind Common Sense（OMCS）和Twitter用户的性别信息，结合基于网络语料库的代词/姓名性别启发式方法，计算行为的性别偏见。

Result: 研究结果显示，基于语料库的预测与人类黄金标准之间的Spearman相关性为0.47，ROC曲线下面积为0.76。

Conclusion: 结论表明，利用自然文本（特别是Twitter语料）来增强常识库中关于性别刻板印象的行为信息是可行的。

Abstract: We extracted gender-specific actions from text corpora and Twitter, and
compared them to stereotypical expectations of people. We used Open Mind Common
Sense (OMCS), a commonsense knowledge repository, to focus on actions that are
pertinent to common sense and daily life of humans. We use the gender
information of Twitter users and Web-corpus-based pronoun/name gender
heuristics to compute the gender bias of the actions. With high recall, we
obtained a Spearman correlation of 0.47 between corpus-based predictions and a
human gold standard, and an area under the ROC curve of 0.76 when predicting
the polarity of the gold standard. We conclude that it is feasible to use
natural text (and a Twitter-derived corpus in particular) in order to augment
commonsense repositories with the stereotypical gender expectations of actions.
We also present a dataset of 441 commonsense actions with human judges' ratings
on whether the action is typically/slightly masculine/feminine (or neutral),
and another larger dataset of 21,442 actions automatically rated by the methods
we investigate in this study.

</details>


### [74] [Multi-task Learning with Active Learning for Arabic Offensive Speech Detection](https://arxiv.org/abs/2506.02753)
*Aisha Alansari,Hamzah Luqman*

Main category: cs.CL

TL;DR: 该论文提出了一种结合多任务学习和主动学习的新框架，用于提升阿拉伯社交媒体文本中攻击性语言的检测效果，并在OSACT2022数据集上取得了85.42%的宏F1分数。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上攻击性、暴力和粗俗言论的快速传播引发了严重的社会和网络安全问题。由于阿拉伯语标注数据有限、方言变体多且语言本身复杂，检测此类内容尤为困难。

Method: 论文提出了一种整合多任务学习（MTL）和主动学习的框架，通过联合训练暴力与粗俗言论两个辅助任务共享表征，并采用动态任务权重调整和主动学习策略优化模型。此外还引入了加权表情符号处理以增强语义捕捉。

Result: 在OSACT2022数据集上的实验表明，该框架以显著更少的微调样本实现了85.42%的宏F1分数，超越了现有方法。

Conclusion: 研究表明，在资源受限场景下，结合多任务学习与主动学习能有效提升攻击性语言检测的效率和准确性。

Abstract: The rapid growth of social media has amplified the spread of offensive,
violent, and vulgar speech, which poses serious societal and cybersecurity
concerns. Detecting such content in Arabic text is particularly complex due to
limited labeled data, dialectal variations, and the language's inherent
complexity. This paper proposes a novel framework that integrates multi-task
learning (MTL) with active learning to enhance offensive speech detection in
Arabic social media text. By jointly training on two auxiliary tasks, violent
and vulgar speech, the model leverages shared representations to improve the
detection accuracy of the offensive speech. Our approach dynamically adjusts
task weights during training to balance the contribution of each task and
optimize performance. To address the scarcity of labeled data, we employ an
active learning strategy through several uncertainty sampling techniques to
iteratively select the most informative samples for model training. We also
introduce weighted emoji handling to better capture semantic cues. Experimental
results on the OSACT2022 dataset show that the proposed framework achieves a
state-of-the-art macro F1-score of 85.42%, outperforming existing methods while
using significantly fewer fine-tuning samples. The findings of this study
highlight the potential of integrating MTL with active learning for efficient
and accurate offensive language detection in resource-constrained settings.

</details>


### [75] [Exploiting the English Vocabulary Profile for L2 word-level vocabulary assessment with LLMs](https://arxiv.org/abs/2506.02758)
*Stefano Bannò,Kate Knill,Mark Gales*

Main category: cs.CL

TL;DR: 该论文提出了一种利用大语言模型（LLMs）和英语词汇大纲（EVP）进行细粒度词汇评估的新方法，用于评估第二语言学习者的词汇使用水平。


<details>
  <summary>Details</summary>
Motivation: 词汇使用是第二语言（L2）熟练度的基本方面，但目前自动化系统的评估通常仅关注与上下文无关或词性相关的词汇使用。论文旨在通过更精确的上下文词汇评估来改进这一领域。

Method: 结合大语言模型（LLMs）和英语词汇大纲（EVP），通过上下文词汇使用来评估学习者的熟练度，并解决多义词、上下文变化和多词表达等关键挑战。

Result: LLMs能够利用额外的语义信息，显著提升词汇评估性能，并且能够有效关联词汇水平与作文水平的熟练度。同时，该方法还验证了EVP熟练度水平的一致性。

Conclusion: LLMs非常适合用于词汇评估任务，能够提供更精确和上下文相关的词汇熟练度分析。

Abstract: Vocabulary use is a fundamental aspect of second language (L2) proficiency.
To date, its assessment by automated systems has typically examined the
context-independent, or part-of-speech (PoS) related use of words. This paper
introduces a novel approach to enable fine-grained vocabulary evaluation
exploiting the precise use of words within a sentence. The scheme combines
large language models (LLMs) with the English Vocabulary Profile (EVP). The EVP
is a standard lexical resource that enables in-context vocabulary use to be
linked with proficiency level. We evaluate the ability of LLMs to assign
proficiency levels to individual words as they appear in L2 learner writing,
addressing key challenges such as polysemy, contextual variation, and
multi-word expressions. We compare LLMs to a PoS-based baseline. LLMs appear to
exploit additional semantic information that yields improved performance. We
also explore correlations between word-level proficiency and essay-level
proficiency. Finally, the approach is applied to examine the consistency of the
EVP proficiency levels. Results show that LLMs are well-suited for the task of
vocabulary assessment.

</details>


### [76] [SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via Visual Global Thinking](https://arxiv.org/abs/2506.02803)
*Sifan Li,Yujun Cai,Yiwei Wang*

Main category: cs.CL

TL;DR: 视觉语言模型（VLMs）在语义任务上表现出色，但在识别光学错觉或AI生成图像中的隐藏内容方面表现不佳。研究提出HC-Bench基准测试和SemVink方法，通过降低图像分辨率显著提升准确率，揭示了VLMs架构缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在高级语义任务上表现优异，但在处理需要低级视觉操作（如识别隐藏内容）的任务时表现不佳，这与人类本能形成鲜明对比。研究旨在揭示这一差距并提出改进方向。

Method: 研究引入HC-Bench基准测试（包含112张含隐藏内容的图像），并提出SemVink方法——通过将图像缩放至低分辨率（32-128像素）来消除冗余视觉噪声。

Result: 主流VLMs在HC-Bench上准确率仅为0-5.36%，而SemVink方法将准确率提升至99%以上，证实VLMs过度依赖高级语义而忽视低级视觉操作。

Conclusion: 研究揭示了VLMs架构中抽象推理与低级视觉操作失衡的问题，呼吁开发融合多尺度处理的混合模型，以弥合计算视觉与人类认知的差距。

Abstract: Vision-language models (VLMs) excel in semantic tasks but falter at a core
human capability: detecting hidden content in optical illusions or AI-generated
images through perceptual adjustments like zooming. We introduce HC-Bench, a
benchmark of 112 images with hidden text, objects, and illusions, revealing
that leading VLMs achieve near-zero accuracy (0-5.36%)-even with explicit
prompting. Humans resolve such ambiguities instinctively, yet VLMs fail due to
an overreliance on high-level semantics. Strikingly, we propose SemVink
(Semantic Visual Thinking) by simply scaling images to low resolutions (32-128
pixels), which unlocks >99% accuracy by eliminating redundant visual noise.
This exposes a critical architectural flaw: VLMs prioritize abstract reasoning
over low-level visual operations crucial for real-world robustness. Our work
urges a shift toward hybrid models integrating multi-scale processing, bridging
the gap between computational vision and human cognition for applications in
medical imaging, security, and beyond.

</details>


### [77] [ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal Transformations](https://arxiv.org/abs/2506.02818)
*Ekaterina Grishina,Mikhail Gorbunov,Maxim Rakhuba*

Main category: cs.CL

TL;DR: 该论文提出了一种利用正交变换优化大语言模型权重矩阵压缩的方法，以减少计算和内存需求。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言处理任务中表现出色，但需要大量的计算和内存资源。结构化矩阵表示是减少模型参数的有效方法，但预训练模型的权重矩阵难以直接精确表示为结构化矩阵。

Method: 利用大语言模型输出在特定正交变换下不变的特性，识别能显著提升权重矩阵在结构化类别中可压缩性的变换。

Result: 该方法适用于支持高效投影操作的各种结构化矩阵，能有效减少模型参数而不影响性能。

Conclusion: 通过正交变换优化权重矩阵的压缩性，为减少大语言模型的计算和内存需求提供了可行方案。

Abstract: Large language models (LLMs) demonstrate impressive results in natural
language processing tasks but require a significant amount of computational and
memory resources. Structured matrix representations are a promising way for
reducing the number of parameters of these models. However, it seems
unrealistic to expect that weight matrices of pretrained models can be
accurately represented by structured matrices without any fine-tuning. To
overcome this issue, we utilize the fact that LLM output is invariant under
certain orthogonal transformations of weight matrices. This insight can be
leveraged to identify transformations that significantly improve the
compressibility of weights within structured classes. The proposed approach is
applicable to various types of structured matrices that support efficient
projection operations. Code is available at
https://github.com/GrishKate/ProcrustesGPT

</details>


### [78] [TO-GATE: Clarifying Questions and Summarizing Responses with Trajectory Optimization for Eliciting Human Preference](https://arxiv.org/abs/2506.02827)
*Yulin Dou,Jiangming Liu*

Main category: cs.CL

TL;DR: TO-GATE框架通过轨迹优化提升大语言模型在对话中生成问题与总结回答的能力，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于自学习推理的方法难以识别最优对话轨迹并避免生成与任务无关的问题，限制了偏好引导的效果。

Method: 提出TO-GATE框架，包含生成最优提问轨迹的澄清解析器和确保任务对齐的总结器，通过轨迹优化实现任务定制化问答。

Result: 实验显示TO-GATE在标准偏好引导任务上比基线方法提升9.32%。

Conclusion: 轨迹优化能有效增强大语言模型的对话引导能力，为复杂任务提供更精准的偏好获取方案。

Abstract: Large language models (LLMs) can effectively elicit human preferences through
multi-turn dialogue. Complex tasks can be accomplished through iterative
clarifying questions and final responses generated by an LLM acting as a
questioner (STaR-GATE; Andukuri et al., 2024}). However, existing approaches
based on self-taught reasoning struggle to identify optimal dialogue
trajectories and avoid irrelevant questions to the tasks. To address this
limitation, we propose TO-GATE, a novel framework that enhances question
generation through trajectory optimization, which consists of two key
components: a clarification resolver that generates optimal questioning
trajectories, and a summarizer that ensures task-aligned final responses. The
trajectory optimization enables the model to produce effective elicitation
questions and summary responses tailored to specific tasks. Experimental
results demonstrate that TO-GATE significantly outperforms baseline methods,
achieving a 9.32% improvement on standard preference elicitation tasks.

</details>


### [79] [Token and Span Classification for Entity Recognition in French Historical Encyclopedias](https://arxiv.org/abs/2506.02872)
*Ludovic Moncla,Hédi Zeghidi*

Main category: cs.CL

TL;DR: 该研究评估了多种命名实体识别方法在历史文本上的表现，特别关注了18世纪法语百科全书中的嵌套实体问题，并探讨了生成式模型在低资源情况下的潜力。


<details>
  <summary>Details</summary>
Motivation: 历史文本中的命名实体识别面临非标准化语言、古旧拼写和嵌套实体等独特挑战，需要探索有效的解决方法。

Method: 研究对比了CRF、spaCy、CamemBERT和Flair等多种模型，并提出了同时使用token级和span级分类来处理嵌套实体。此外，还评估了生成式语言模型在少样本情况下的表现。

Result: 基于transformer的模型在嵌套实体识别上表现最优，而生成式模型在标注数据稀缺时展现出潜力。

Conclusion: 研究强调了历史文本NER的持续挑战，建议结合符号方法和神经方法的混合途径来更好地处理早期现代法语文本的复杂性。

Abstract: Named Entity Recognition (NER) in historical texts presents unique challenges
due to non-standardized language, archaic orthography, and nested or
overlapping entities. This study benchmarks a diverse set of NER approaches,
ranging from classical Conditional Random Fields (CRFs) and spaCy-based models
to transformer-based architectures such as CamemBERT and sequence-labeling
models like Flair. Experiments are conducted on the GeoEDdA dataset, a richly
annotated corpus derived from 18th-century French encyclopedias. We propose
framing NER as both token-level and span-level classification to accommodate
complex nested entity structures typical of historical documents. Additionally,
we evaluate the emerging potential of few-shot prompting with generative
language models for low-resource scenarios. Our results demonstrate that while
transformer-based models achieve state-of-the-art performance, especially on
nested entities, generative models offer promising alternatives when labeled
data are scarce. The study highlights ongoing challenges in historical NER and
suggests avenues for hybrid approaches combining symbolic and neural methods to
better capture the intricacies of early modern French text.

</details>


### [80] [CoT is Not True Reasoning, It Is Just a Tight Constraint to Imitate: A Theory Perspective](https://arxiv.org/abs/2506.02878)
*Jintian Shao,Yiming Cheng*

Main category: cs.CL

TL;DR: 论文认为思维链提示并非真正引发大语言模型的抽象推理能力，而是通过结构化约束使其模仿推理形式。


<details>
  <summary>Details</summary>
Motivation: 针对当前普遍认为思维链提示能激发大语言模型涌现推理能力的观点，提出理论反证。

Method: 通过理论分析，阐述思维链提示如何作为结构化约束机制运作。

Result: 论证思维链提示本质是利用模型的序列预测能力，通过强制生成中间步骤来模仿推理过程。

Conclusion: 思维链提示并未实现真正的抽象推理，而是通过模式匹配生成类推理的连贯序列。

Abstract: Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of
Large Language Models on tasks requiring multi-step inference. This success has
led to widespread claims of emergent reasoning capabilities in these models. In
this paper, we present a theoretical counter-perspective: Chain-of-Thought
(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that
Chain-of-Thought functions as a powerful structural constraint that guides
Large Language Models to imitate the form of reasoning. By forcing the
generation of intermediate steps, Chain-of-Thought leverages the model immense
capacity for sequence prediction and pattern matching, effectively constraining
its output to sequences that resemble coherent thought processes.
Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of
Large Language Models on tasks requiring multi-step inference. This success has
led to widespread claims of emergent reasoning capabilities in these models. In
this paper, we present a theoretical counter-perspective: Chain-of-Thought
(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that
Chain-of-Thought functions as a powerful structural constraint that guides
Large Language Models to imitate the form of reasoning. By forcing the
generation of intermediate steps, Chain-of-Thought leverages the model immense
capacity for sequence prediction and pattern matching, effectively constraining
its output to sequences that resemble coherent thought processes.

</details>


### [81] [A Multi-Dialectal Dataset for German Dialect ASR and Dialect-to-Standard Speech Translation](https://arxiv.org/abs/2506.02894)
*Verena Blaschke,Miriam Winkler,Constantin Förster,Gabriele Wenger-Glemser,Barbara Plank*

Main category: cs.CL

TL;DR: 该论文介绍了Betthupferl数据集，用于评估自动语音识别（ASR）模型对德国东南部方言的鲁棒性，并测试了多语言ASR模型的表现。


<details>
  <summary>Details</summary>
Motivation: 德国方言在当前自动语音识别研究中代表性不足，论文旨在通过构建方言数据集，研究模型对方言变化的鲁棒性。

Method: 构建包含4小时方言朗读语音和半小时标准德语语音的Betthupferl数据集，提供方言和标准德语的转录，并分析其语言差异。测试多语言ASR模型在方言到标准德语的翻译表现。

Result: ASR模型在输出中表现出方言与标准转录的差异，最佳模型有时会规范化语法差异，但更多保留方言结构。

Conclusion: Betthupferl数据集为方言ASR研究提供了基础，模型在处理方言时表现出一定的标准化倾向，但仍保留方言特征。

Abstract: Although Germany has a diverse landscape of dialects, they are
underrepresented in current automatic speech recognition (ASR) research. To
enable studies of how robust models are towards dialectal variation, we present
Betthupferl, an evaluation dataset containing four hours of read speech in
three dialect groups spoken in Southeast Germany (Franconian, Bavarian,
Alemannic), and half an hour of Standard German speech. We provide both
dialectal and Standard German transcriptions, and analyze the linguistic
differences between them. We benchmark several multilingual state-of-the-art
ASR models on speech translation into Standard German, and find differences
between how much the output resembles the dialectal vs. standardized
transcriptions. Qualitative error analyses of the best ASR model reveal that it
sometimes normalizes grammatical differences, but often stays closer to the
dialectal constructions.

</details>


### [82] [IMPARA-GED: Grammatical Error Detection is Boosting Reference-free Grammatical Error Quality Estimator](https://arxiv.org/abs/2506.02899)
*Yusuke Sakai,Takumi Goto,Taro Watanabe*

Main category: cs.CL

TL;DR: 提出了一种新型的无参考自动语法错误修正评估方法IMPARA-GED，具备语法错误检测能力，并在实验中表现出与人工评估最高的相关性。


<details>
  <summary>Details</summary>
Motivation: 现有的自动语法错误修正评估方法IMPARA的质量评估器需要改进，特别是在语法错误检测能力方面。

Method: 通过使用预训练语言模型增强语法错误检测能力，构建了IMPARA-GED的质量评估器。

Result: 在SEEDA元评估数据集上，IMPARA-GED实现了与人工句子级评估最高的相关性。

Conclusion: IMPARA-GED作为一种无参考自动评估方法，在语法错误修正和检测方面表现出色，优于现有方法。

Abstract: We propose IMPARA-GED, a novel reference-free automatic grammatical error
correction (GEC) evaluation method with grammatical error detection (GED)
capabilities. We focus on the quality estimator of IMPARA, an existing
automatic GEC evaluation method, and construct that of IMPARA-GED using a
pre-trained language model with enhanced GED capabilities. Experimental results
on SEEDA, a meta-evaluation dataset for automatic GEC evaluation methods,
demonstrate that IMPARA-GED achieves the highest correlation with human
sentence-level evaluations.

</details>


### [83] [Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning](https://arxiv.org/abs/2506.02911)
*Yin Fang,Qiao Jin,Guangzhi Xiong,Bowen Jin,Xianrui Zhong,Siru Ouyang,Aidong Zhang,Jiawei Han,Zhiyong Lu*

Main category: cs.CL

TL;DR: 本文提出CellPuzzles任务，通过训练Cell-o1模型实现单细胞RNA测序数据的批量细胞类型注释，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在单细胞RNA测序数据注释中忽略批次级细胞上下文和解释性推理，而人类专家则能基于领域知识对细胞簇进行独特注释。为模拟这一过程，作者提出CellPuzzles任务。

Method: 提出Cell-o1模型：先通过监督微调学习推理轨迹，再采用批次级奖励的强化学习进行训练。模型参数量为7B，基于蒸馏的推理轨迹数据。

Result: Cell-o1在批次级准确率上超越最佳基线（OpenAI o1）73%，达到最先进性能，并展现出良好的跨场景泛化能力。基线模型准确率仅为19.0%。

Conclusion: Cell-o1通过模拟专家推理实现了高效的批量细胞注释，其训练动态和推理行为分析为批次级注释提供了新见解。代码和数据已开源。

Abstract: Cell type annotation is a key task in analyzing the heterogeneity of
single-cell RNA sequencing data. Although recent foundation models automate
this process, they typically annotate cells independently, without considering
batch-level cellular context or providing explanatory reasoning. In contrast,
human experts often annotate distinct cell types for different cell clusters
based on their domain knowledge. To mimic this workflow, we introduce the
CellPuzzles task, where the objective is to assign unique cell types to a batch
of cells. This benchmark spans diverse tissues, diseases, and donor conditions,
and requires reasoning across the batch-level cellular context to ensure label
uniqueness. We find that off-the-shelf large language models (LLMs) struggle on
CellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0%
batch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained
via supervised fine-tuning on distilled reasoning traces, followed by
reinforcement learning with batch-level rewards. Cell-o1 achieves
state-of-the-art performance, outperforming o1 by over 73% and generalizing
well across contexts. Further analysis of training dynamics and reasoning
behaviors provides insights into batch-level annotation performance and
emergent expert-like reasoning. Code and data are available at
https://github.com/ncbi-nlp/cell-o1.

</details>


### [84] [A Controllable Examination for Long-Context Language Models](https://arxiv.org/abs/2506.02921)
*Yijun Yang,Zeyu Huang,Wenhao Zhu,Zihan Qiu,Fei Yuan,Jeff Z. Pan,Ivan Titov*

Main category: cs.CL

TL;DR: 论文提出LongBioBench基准，通过人工生成传记评估长上下文语言模型，解决了现有评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文语言模型评估框架存在局限性：真实任务复杂且易受数据污染，合成任务缺乏上下文连贯性。

Method: 构建LongBioBench基准，利用人工生成传记作为可控环境，评估模型的理解、推理和可信度。

Result: 实验表明多数模型在长上下文下的语义理解和基础推理仍不足，且现有合成基准设计存在缺陷。

Conclusion: LongBioBench在模拟真实任务与保持可控性之间取得更好平衡，具有高可解释性和可配置性。

Abstract: Existing frameworks for evaluating long-context language models (LCLM) can be
broadly categorized into real-world and synthetic tasks. Despite their utility,
both approaches are accompanied by certain intrinsic limitations. Real-world
tasks are too complex to interpret or characterize and are susceptible to data
contamination. In contrast, synthetic tasks often adopt the
needle-in-the-haystack (NIAH) format, wherein a lack of coherence between the
"needle" and the "haystack" compromises their validity as proxies for realistic
applications. In response to these challenges, we posit that an ideal
long-context evaluation framework should be characterized by three essential
features: $\textit{seamless context}$, $\textit{controllable setting}$, and
$\textit{sound evaluation}$. This study introduces $\textbf{LongBioBench}$, a
novel benchmark that utilizes artificially generated biographies as a
controlled environment for assessing LCLMs across dimensions of
$\textit{understanding}$, $\textit{reasoning}$, and $\textit{trustworthiness}$.
Our experimental evaluation, which includes $\textbf{18}$ LCLMs in total,
demonstrates that most models still exhibit deficiencies in semantic
understanding and elementary reasoning over retrieved results and are less
trustworthy as context length increases. Our further analysis indicates some
design choices employed by existing synthetic benchmarks, such as contextual
non-coherence, numerical needles, and the absence of distractors, rendering
them vulnerable to test the model long-context capabilities. Moreover, we also
reveal that long-context continual pretraining primarily adjusts RoPE embedding
to accommodate extended context lengths. To sum up, compared to previous
synthetic benchmarks, LongBioBench achieves a better trade-off between
mirroring authentic language tasks and maintaining controllability, and is
highly interpretable and configurable.

</details>


### [85] [INESC-ID @ eRisk 2025: Exploring Fine-Tuned, Similarity-Based, and Prompt-Based Approaches to Depression Symptom Identification](https://arxiv.org/abs/2506.02924)
*Diogo A. P. Nunes,Eugénio Ribeiro*

Main category: cs.CL

TL;DR: 该研究团队通过微调基础模型并结合合成数据缓解类别不平衡，在eRisk 2025任务1中针对抑郁症状的句子检索任务中取得了最佳表现，击败了其他16个团队。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决eRisk 2025任务1中的抑郁症状句子检索问题，由于训练数据标签限制，团队将其视为二元分类任务进行开发。

Method: 方法包括将标注数据分为训练集和验证集，探索基础模型微调、句子相似度、大语言模型提示和集成技术，并使用合成数据缓解类别不平衡。

Result: 验证结果显示，微调基础模型表现最佳，尤其在结合合成数据后。团队提交的五个独立测试运行中，两个集成方法在官方IR评估中得分最高。

Conclusion: 研究表明，基础模型微调结合合成数据是解决抑郁症状句子检索任务的有效方法，且不同症状的最佳方法各异。

Abstract: In this work, we describe our team's approach to eRisk's 2025 Task 1: Search
for Symptoms of Depression. Given a set of sentences and the Beck's Depression
Inventory - II (BDI) questionnaire, participants were tasked with submitting up
to 1,000 sentences per depression symptom in the BDI, sorted by relevance.
Participant submissions were evaluated according to standard Information
Retrieval (IR) metrics, including Average Precision (AP) and R-Precision
(R-PREC). The provided training data, however, consisted of sentences labeled
as to whether a given sentence was relevant or not w.r.t. one of BDI's
symptoms. Due to this labeling limitation, we framed our development as a
binary classification task for each BDI symptom, and evaluated accordingly. To
that end, we split the available labeled data into training and validation
sets, and explored foundation model fine-tuning, sentence similarity, Large
Language Model (LLM) prompting, and ensemble techniques. The validation results
revealed that fine-tuning foundation models yielded the best performance,
particularly when enhanced with synthetic data to mitigate class imbalance. We
also observed that the optimal approach varied by symptom. Based on these
insights, we devised five independent test runs, two of which used ensemble
methods. These runs achieved the highest scores in the official IR evaluation,
outperforming submissions from 16 other teams.

</details>


### [86] [Quantitative LLM Judges](https://arxiv.org/abs/2506.02945)
*Aishwarya Sahoo,Jeevana Kruthi Karnuthala,Tushar Parmanand Budhwani,Pranchal Agarwal,Sankaran Vaidyanathan,Alexa Siu,Franck Dernoncourt,Jennifer Healey,Nedim Lipka,Ryan Rossi,Uttaran Bhattacharya,Branislav Kveton*

Main category: cs.CL

TL;DR: 提出定量LLM评判框架，通过回归模型将现有LLM评判分数与人类评分对齐，提升评判效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评判框架在评估其他LLM输出时，其评分与人类评分存在偏差，需一种更高效、统计有效的方法来改进评判质量。

Method: 使用回归模型训练定量评判模型，基于原始评判的文本评估和分数，提升其评分预测能力。

Result: 在四个数据集上验证，定量评判模型能有效提升现有评判的预测能力，尤其在人类反馈有限时表现更优。

Conclusion: 定量LLM评判框架通过后建模方法显著提升评判质量，计算效率高，适用于人类反馈有限的应用场景。

Abstract: LLM-as-a-judge is a framework in which a large language model (LLM)
automatically evaluates the output of another LLM. We propose quantitative LLM
judges, which align evaluation scores of existing LLM judges to human scores in
a given domain using regression models. The models are trained to improve the
score of the original judge by using the judge's textual evaluation and score.
We present four quantitative judges for different types of absolute and
relative feedback, which showcases the generality and versatility of our
framework. Our framework is more computationally efficient than supervised
fine-tuning and can be more statistically efficient when human feedback is
limited, which is expected in most applications of our work. We validate these
claims empirically on four datasets using two base judges. Our experiments show
that quantitative judges can effectively improve the predictive power of
existing judges through post-hoc modeling.

</details>


### [87] [Adaptive Graph Pruning for Multi-Agent Communication](https://arxiv.org/abs/2506.02951)
*Boyi Li,Zhonghan Zhao,Der-Horng Lee,Gaoang Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为自适应图剪枝（AGP）的新型多智能体协作框架，通过联合优化智能体数量和通信拓扑结构，显著提升了任务适应性和性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型（LLM）的多智能体系统通常采用固定数量的智能体和静态通信结构，限制了其适应不同任务复杂性的能力。

Method: AGP采用两阶段训练策略：首先独立训练不同智能体数量的软剪枝网络，以确定任务特定的最优完全图和位置掩码；然后在最大完全图中联合优化硬剪枝和软剪枝，动态配置智能体数量和通信拓扑。

Result: 实验表明，AGP在六个基准测试中均达到最先进水平，性能提升2.58%至9.84%，且在不同任务类别（通用推理、数学推理和代码生成）中表现优异，同时显著减少了训练步骤和令牌消耗。

Conclusion: AGP是一种高效、自适应且性能优越的多智能体协作框架，能够在动态配置智能体数量和通信拓扑的同时，显著提升任务性能和训练效率。

Abstract: Large Language Model (LLM) based multi-agent systems have shown remarkable
performance in various tasks, especially when enhanced through collaborative
communication. However, current methods often rely on a fixed number of agents
and static communication structures, limiting their ability to adapt to varying
task complexities. In this paper, we propose Adaptive Graph Pruning (AGP), a
novel task-adaptive multi-agent collaboration framework that jointly optimizes
agent quantity (hard-pruning) and communication topology (soft-pruning).
Specifically, our method employs a two-stage training strategy: firstly,
independently training soft-pruning networks for different agent quantities to
determine optimal agent-quantity-specific complete graphs and positional masks
across specific tasks; and then jointly optimizing hard-pruning and
soft-pruning within a maximum complete graph to dynamically configure the
number of agents and their communication topologies per task. Extensive
experiments demonstrate that our approach is: (1) High-performing, achieving
state-of-the-art results across six benchmarks and consistently generalizes
across multiple mainstream LLM architectures, with a increase in performance of
$2.58\%\sim 9.84\%$; (2) Task-adaptive, dynamically constructing optimized
communication topologies tailored to specific tasks, with an extremely high
performance in all three task categories (general reasoning, mathematical
reasoning, and code generation); (3) Token-economical, having fewer training
steps and token consumption at the same time, with a decrease in token
consumption of $90\%+$; and (4) Training-efficient, achieving high performance
with very few training steps compared with other methods. The performance will
surpass the existing baselines after about ten steps of training under six
benchmarks.

</details>


### [88] [HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under Human-AI Coauthoring](https://arxiv.org/abs/2506.02959)
*Zhixiong Su,Yichen Wang,Herun Wan,Zhaohan Zhang,Minnan Luo*

Main category: cs.CL

TL;DR: 该论文探讨了在人类与AI共同创作文本的背景下，细粒度机器生成文本检测的可能性，提出了一个新数据集HACo-Det，并评估了现有检测器的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的滥用带来潜在风险，促使机器生成文本（MGT）检测的发展。现有研究主要关注文档级别的二元检测，忽略了人类与AI共同创作的文本。

Method: 论文提出了HACo-Det数据集，通过自动流程生成人类与AI共同创作的文本，并带有词级归属标签。同时，对七种主流文档级检测器进行了改造，以适应词级检测任务。

Result: 实验结果表明，基于度量的方法在细粒度检测上表现不佳（平均F1分数0.462），而微调模型表现出更好的性能和跨领域泛化能力。

Conclusion: 论文指出细粒度共同创作文本检测问题尚未解决，分析了影响性能的因素（如上下文窗口），并强调了当前方法的局限性，提出了改进方向。

Abstract: The misuse of large language models (LLMs) poses potential risks, motivating
the development of machine-generated text (MGT) detection. Existing literature
primarily concentrates on binary, document-level detection, thereby neglecting
texts that are composed jointly by human and LLM contributions. Hence, this
paper explores the possibility of fine-grained MGT detection under human-AI
coauthoring. We suggest fine-grained detectors can pave pathways toward
coauthored text detection with a numeric AI ratio. Specifically, we propose a
dataset, HACo-Det, which produces human-AI coauthored texts via an automatic
pipeline with word-level attribution labels. We retrofit seven prevailing
document-level detectors to generalize them to word-level detection. Then we
evaluate these detectors on HACo-Det on both word- and sentence-level detection
tasks. Empirical results show that metric-based methods struggle to conduct
fine-grained detection with a 0.462 average F1 score, while finetuned models
show superior performance and better generalization across domains. However, we
argue that fine-grained co-authored text detection is far from solved. We
further analyze factors influencing performance, e.g., context window, and
highlight the limitations of current methods, pointing to potential avenues for
improvement.

</details>


### [89] [FlowerTune: A Cross-Domain Benchmark for Federated Fine-Tuning of Large Language Models](https://arxiv.org/abs/2506.02961)
*Yan Gao,Massimo Roberto Scamarcia,Javier Fernandez-Marques,Mohammad Naseri,Chong Shen Ng,Dimitris Stripelis,Zexi Li,Tao Shen,Jiamu Bai,Daoyuan Chen,Zikai Zhang,Rui Hu,InSeo Song,Lee KangYoon,Hong Jia,Ting Dang,Junyan Wang,Zheyuan Liu,Daniel Janes Beutel,Lingjuan Lyu,Nicholas D. Lane*

Main category: cs.CL

TL;DR: 该论文提出了FlowerTune LLM排行榜，用于评估联邦学习环境下大语言模型的微调性能，覆盖多个领域并提供首个全面比较。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）依赖大量公开数据，但面临数据稀缺和敏感领域数据访问受限的问题。联邦学习（FL）提供了一种无需共享原始数据的分散微调方案，但其在LLMs中的兼容性和性能尚未充分研究。

Method: 通过FlowerTune LLM排行榜，在通用NLP、金融、医疗和编程四个领域，使用联邦指令微调数据集和领域特定评估指标，对26种预训练LLMs进行联邦微调策略的全面比较。

Result: 研究提供了模型性能、资源限制和领域适应的实用见解，为开发隐私保护、领域专用的LLMs奠定了基础。

Conclusion: 该工作为现实应用中隐私保护的领域专用LLMs开发提供了重要参考和基础。

Abstract: Large Language Models (LLMs) have achieved state-of-the-art results across
diverse domains, yet their development remains reliant on vast amounts of
publicly available data, raising concerns about data scarcity and the lack of
access to domain-specific, sensitive information. Federated Learning (FL)
presents a compelling framework to address these challenges by enabling
decentralized fine-tuning on pre-trained LLMs without sharing raw data.
However, the compatibility and performance of pre-trained LLMs in FL settings
remain largely under explored. We introduce the FlowerTune LLM Leaderboard, a
first-of-its-kind benchmarking suite designed to evaluate federated fine-tuning
of LLMs across four diverse domains: general NLP, finance, medical, and coding.
Each domain includes federated instruction-tuning datasets and domain-specific
evaluation metrics. Our results, obtained through a collaborative, open-source
and community-driven approach, provide the first comprehensive comparison
across 26 pre-trained LLMs with different aggregation and fine-tuning
strategies under federated settings, offering actionable insights into model
performance, resource constraints, and domain adaptation. This work lays the
foundation for developing privacy-preserving, domain-specialized LLMs for
real-world applications.

</details>


### [90] [Expanding before Inferring: Enhancing Factuality in Large Language Models through Premature Layers Interpolation](https://arxiv.org/abs/2506.02973)
*Dingwei Chen,Ziqiang Liu,Feiteng Fang,Chak Tou Leong,Shiwen Ni,Ahmadreza Argha,Hamid Alinejad-Rokny,Min Yang,Chengming Li*

Main category: cs.CL

TL;DR: 该论文提出了一种名为PLI（Premature Layers Interpolation）的新型干预方法，旨在无需训练即可减少大语言模型（LLMs）的幻觉问题，通过数学插值在相邻层之间插入中间层来提升信息处理的深度和事实一致性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在文本理解和生成方面表现出色，但存在产生事实不一致输出（即“幻觉”）的问题。现有方法多在输入或输出层面解决这一问题，忽视了模型内部信息精炼过程及早期层的作用，且调整和微调方法资源消耗大。

Method: PLI是一种无需训练、即插即用的干预方法，通过在相邻层之间进行数学插值形成中间层，扩展信息处理和传递的深度，从而提高事实一致性。

Result: 在四个公开数据集上的实验表明，PLI能有效减少幻觉现象，并在多数情况下优于现有基线方法。进一步分析显示，层插值的成功与LLMs的内部机制密切相关。

Conclusion: PLI作为一种简单有效的方法，显著提升了LLMs的事实一致性，且无需额外训练资源。论文承诺在接收后将公开代码和数据以促进可重复性。

Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities in text
understanding and generation. However, their tendency to produce factually
inconsistent outputs, commonly referred to as ''hallucinations'', remains a
critical challenge. Existing approaches, such as retrieval-based and
inference-time correction methods, primarily address this issue at the input or
output level, often overlooking the intrinsic information refinement process
and the role of premature layers. Meanwhile, alignment- and fine-tuning-based
methods are resource-intensive. In this paper, we propose PLI (Premature Layers
Interpolation), a novel, training-free, and plug-and-play intervention designed
to enhance factuality. PLI mitigates hallucinations by inserting premature
layers formed through mathematical interpolation with adjacent layers. Inspired
by stable diffusion and sampling steps, PLI extends the depth of information
processing and transmission in LLMs, improving factual coherence. Experiments
on four publicly available datasets demonstrate that PLI effectively reduces
hallucinations while outperforming existing baselines in most cases. Further
analysis suggests that the success of layer interpolation is closely linked to
LLMs' internal mechanisms. To promote reproducibility, we will release our code
and data upon acceptance.

</details>


### [91] [Towards a Japanese Full-duplex Spoken Dialogue System](https://arxiv.org/abs/2506.02979)
*Atsumoto Ohashi,Shinya Iizuka,Jingjing Jiang,Ryuichiro Higashinaka*

Main category: cs.CL

TL;DR: 本文提出了首个公开可用的日语全双工口语对话模型，通过两阶段训练和合成数据增强，性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 日语全双工口语对话系统的研究较少，本文旨在填补这一空白。

Method: 基于英文模型Moshi，通过大规模日语对话数据预训练和高质量立体声数据微调，并引入多流TTS生成的合成数据。

Result: 评估实验表明，该模型在自然度和意义性上均优于日语基线模型。

Conclusion: 该研究成功开发了高性能的日语全双工对话模型，为相关领域提供了重要资源。

Abstract: Full-duplex spoken dialogue systems, which can model simultaneous
bidirectional features of human conversations such as speech overlaps and
backchannels, have attracted significant attention recently. However, the study
of full-duplex spoken dialogue systems for the Japanese language has been
limited, and the research on their development in Japanese remains scarce. In
this paper, we present the first publicly available full-duplex spoken dialogue
model in Japanese, which is built upon Moshi, a full-duplex dialogue model in
English. Our model is trained through a two-stage process: pre-training on a
large-scale spoken dialogue data in Japanese, followed by fine-tuning on
high-quality stereo spoken dialogue data. We further enhance the model's
performance by incorporating synthetic dialogue data generated by a
multi-stream text-to-speech system. Evaluation experiments demonstrate that the
trained model outperforms Japanese baseline models in both naturalness and
meaningfulness.

</details>


### [92] [Performance of leading large language models in May 2025 in Membership of the Royal College of General Practitioners-style examination questions: a cross-sectional analysis](https://arxiv.org/abs/2506.02987)
*Richard Armitage*

Main category: cs.CL

TL;DR: 顶级大语言模型在初级保健考试中表现优异，显著超过人类医生平均水平。


<details>
  <summary>Details</summary>
Motivation: 评估领先的大语言模型（如o3、Claude Opus 4等）在初级保健教育中的能力，特别是回答MRCGP风格考试问题的表现。

Method: 使用100道MRCGP考试题目测试四个模型，每个模型仅尝试一次，答案与标准答案对比评分。

Result: o3得分99%，其他模型均为95%，显著高于人类医生平均分73%。

Conclusion: 大语言模型，尤其是推理模型，在支持初级保健服务方面具有巨大潜力。

Abstract: Background: Large language models (LLMs) have demonstrated substantial
potential to support clinical practice. Other than Chat GPT4 and its
predecessors, few LLMs, especially those of the leading and more powerful
reasoning model class, have been subjected to medical specialty examination
questions, including in the domain of primary care. This paper aimed to test
the capabilities of leading LLMs as of May 2025 (o3, Claude Opus 4, Grok3, and
Gemini 2.5 Pro) in primary care education, specifically in answering Member of
the Royal College of General Practitioners (MRCGP) style examination questions.
  Methods: o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro were tasked to answer
100 randomly chosen multiple choice questions from the Royal College of General
Practitioners GP SelfTest on 25 May 2025. Questions included textual
information, laboratory results, and clinical images. Each model was prompted
to answer as a GP in the UK and was provided with full question information.
Each question was attempted once by each model. Responses were scored against
correct answers provided by GP SelfTest.
  Results: The total score of o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro was
99.0%, 95.0%, 95.0%, and 95.0%, respectively. The average peer score for the
same questions was 73.0%.
  Discussion: All models performed remarkably well, and all substantially
exceeded the average performance of GPs and GP registrars who had answered the
same questions. o3 demonstrated the best performance, while the performances of
the other leading models were comparable with each other and were not
substantially lower than that of o3. These findings strengthen the case for
LLMs, particularly reasoning models, to support the delivery of primary care,
especially those that have been specifically trained on primary care clinical
data.

</details>


### [93] [It's Not a Walk in the Park! Challenges of Idiom Translation in Speech-to-text Systems](https://arxiv.org/abs/2506.02995)
*Iuliia Zaitova,Badr M. Abdullah,Wei Xue,Dietrich Klakow,Bernd Möbius,Tania Avgustinova*

Main category: cs.CL

TL;DR: 该论文系统评估了习语在机器翻译和语音转文本翻译中的表现，发现语音转文本系统在习语翻译上表现较差，需改进策略。


<details>
  <summary>Details</summary>
Motivation: 习语翻译是机器翻译中的一大挑战，尤其在语音转文本系统中研究较少。论文旨在比较不同系统在习语翻译上的表现。

Method: 论文对比了端到端语音转文本系统、机器翻译系统、大语言模型及级联方案在德语-英语和俄语-英语的习语翻译表现。

Result: 语音转文本系统在习语翻译上表现显著下降，常输出字面翻译；机器翻译和大语言模型表现较好。

Conclusion: 语音转文本架构需针对习语优化策略和改进内部表示，以提升翻译质量。

Abstract: Idioms are defined as a group of words with a figurative meaning not
deducible from their individual components. Although modern machine translation
systems have made remarkable progress, translating idioms remains a major
challenge, especially for speech-to-text systems, where research on this topic
is notably sparse. In this paper, we systematically evaluate idiom translation
as compared to conventional news translation in both text-to-text machine
translation (MT) and speech-to-text translation (SLT) systems across two
language pairs (German to English, Russian to English). We compare
state-of-the-art end-to-end SLT systems (SeamlessM4T SLT-to-text, Whisper Large
v3) with MT systems (SeamlessM4T SLT-to-text, No Language Left Behind), Large
Language Models (DeepSeek, LLaMA) and cascaded alternatives. Our results reveal
that SLT systems experience a pronounced performance drop on idiomatic data,
often reverting to literal translations even in higher layers, whereas MT
systems and Large Language Models demonstrate better handling of idioms. These
findings underscore the need for idiom-specific strategies and improved
internal representations in SLT architectures.

</details>


### [94] [A Multi-Agent Framework for Mitigating Dialect Biases in Privacy Policy Question-Answering Systems](https://arxiv.org/abs/2506.02998)
*Đorđe Klisura,Astrid R Bernaga Torres,Anna Karen Gárate-Escamilla,Rajesh Roshan Biswal,Ke Yang,Hilal Pataci,Anthony Rios*

Main category: cs.CL

TL;DR: 提出多智能体框架解决隐私政策问答系统中的方言偏见问题，无需重新训练即可显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 隐私政策文本复杂且现有问答系统对非标准英语方言用户存在性能差异，需设计公平访问隐私信息的NLP系统。

Method: 结合方言智能体（转写查询为标准英语）和隐私政策智能体（领域专家优化预测）的多智能体协作框架。

Result: 在PrivacyQA和PolicyQA数据集上，GPT-4o-mini的零样本准确率分别提升至0.601和0.464，超越少量样本基线。

Conclusion: 结构化智能体协作能有效缓解方言偏见，强调需考虑语言多样性以实现隐私信息的公平获取。

Abstract: Privacy policies inform users about data collection and usage, yet their
complexity limits accessibility for diverse populations. Existing Privacy
Policy Question Answering (QA) systems exhibit performance disparities across
English dialects, disadvantaging speakers of non-standard varieties. We propose
a novel multi-agent framework inspired by human-centered design principles to
mitigate dialectal biases. Our approach integrates a Dialect Agent, which
translates queries into Standard American English (SAE) while preserving
dialectal intent, and a Privacy Policy Agent, which refines predictions using
domain expertise. Unlike prior approaches, our method does not require
retraining or dialect-specific fine-tuning, making it broadly applicable across
models and domains. Evaluated on PrivacyQA and PolicyQA, our framework improves
GPT-4o-mini's zero-shot accuracy from 0.394 to 0.601 on PrivacyQA and from
0.352 to 0.464 on PolicyQA, surpassing or matching few-shot baselines without
additional training data. These results highlight the effectiveness of
structured agent collaboration in mitigating dialect biases and underscore the
importance of designing NLP systems that account for linguistic diversity to
ensure equitable access to privacy information.

</details>


### [95] [Conditioning Large Language Models on Legal Systems? Detecting Punishable Hate Speech](https://arxiv.org/abs/2506.03009)
*Florian Ludwig,Torsten Zesch,Frederike Zufall*

Main category: cs.CL

TL;DR: 该研究探讨了在不同法律抽象层级上调整大语言模型（LLMs）以检测德国刑法中规定的仇恨言论，发现模型与法律专家在性能上仍有显著差距。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索LLMs如何内化法律体系，特别是在不同抽象层级上对法律问题的处理能力，尤其是针对仇恨言论的法律评估。

Method: 研究提出了多种方法，在不同法律抽象层级上调整LLMs，以分类社交媒体帖子是否构成德国刑法中的煽动仇恨罪。

Result: 结果显示，无论模型在何种抽象层级上调整，其在仇恨言论的法律评估上与法律专家仍有显著差距。抽象法律知识调整的模型缺乏深度任务理解，常自相矛盾或产生幻觉答案；而具体法律知识调整的模型在识别相关目标群体上表现尚可，但在分类目标行为上仍有困难。

Conclusion: 研究表明，当前LLMs在法律评估任务中仍有局限性，特别是在理解复杂法律概念和分类具体行为方面，需要进一步改进。

Abstract: The assessment of legal problems requires the consideration of a specific
legal system and its levels of abstraction, from constitutional law to
statutory law to case law. The extent to which Large Language Models (LLMs)
internalize such legal systems is unknown. In this paper, we propose and
investigate different approaches to condition LLMs at different levels of
abstraction in legal systems. This paper examines different approaches to
conditioning LLMs at multiple levels of abstraction in legal systems to detect
potentially punishable hate speech. We focus on the task of classifying whether
a specific social media posts falls under the criminal offense of incitement to
hatred as prescribed by the German Criminal Code. The results show that there
is still a significant performance gap between models and legal experts in the
legal assessment of hate speech, regardless of the level of abstraction with
which the models were conditioned. Our analysis revealed, that models
conditioned on abstract legal knowledge lacked deep task understanding, often
contradicting themselves and hallucinating answers, while models using concrete
legal knowledge performed reasonably well in identifying relevant target
groups, but struggled with classifying target conducts.

</details>


### [96] [Coding Agents with Multimodal Browsing are Generalist Problem Solvers](https://arxiv.org/abs/2506.03011)
*Aditya Bharat Soni,Boxuan Li,Xingyao Wang,Valerie Chen,Graham Neubig*

Main category: cs.CL

TL;DR: 论文提出通用智能体OpenHands-Versa，仅需少量通用工具即可在多样化任务中超越专用智能体。


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体过度专业化导致泛化能力不足，作者探索如何用最小通用工具集实现跨领域高性能。

Method: 构建基于代码编辑/执行、网络搜索、多模态网页浏览和文件访问的通用智能体架构。

Result: 在SWE-Bench等三大基准测试中绝对成功率最高提升9.1%，显著优于现有专用智能体。

Conclusion: 证实通用智能体的可行性，OpenHands-Versa为未来研究建立强基线。

Abstract: Modern human labor is characterized by specialization; we train for years and
develop particular tools that allow us to perform well across a variety of
tasks. In addition, AI agents have been specialized for domains such as
software engineering, web navigation, and workflow automation. However, this
results in agents that are good for one thing but fail to generalize beyond
their intended scope. One reason for this is that agent developers provide a
highly specialized set of tools or make architectural decisions optimized for a
specific use case or benchmark. In this work, we ask the question: what is the
minimal set of general tools that can be used to achieve high performance
across a diverse set of tasks? Our answer is OpenHands-Versa, a generalist
agent built with a modest number of general tools: code editing and execution,
web search, as well as multimodal web browsing and file access. Importantly,
OpenHands-Versa demonstrates superior or competitive performance over leading
specialized agents across three diverse and challenging benchmarks: SWE-Bench
Multimodal, GAIA, and The Agent Company, outperforming the best-performing
previously published results with absolute improvements in success rate of 9.1,
1.3, and 9.1 points respectively. Further, we show how existing
state-of-the-art multi-agent systems fail to generalize beyond their target
domains. These results demonstrate the feasibility of developing a generalist
agent to solve diverse tasks and establish OpenHands-Versa as a strong baseline
for future research.

</details>


### [97] [Leveraging Information Retrieval to Enhance Spoken Language Understanding Prompts in Few-Shot Learning](https://arxiv.org/abs/2506.03035)
*Pierre Lepagnol,Sahar Ghannay,Thomas Gerald,Christophe Servan,Sophie Rosset*

Main category: cs.CL

TL;DR: 该论文提出了一种利用信息检索技术优化提示的方法，以提升小样本场景下口语理解任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的口语理解技术依赖大量标注数据，但特定任务或语言的标注样本有限。指令调优的大语言模型在少量样本下表现优异，但如何构建有效提示是关键。

Method: 通过信息检索方法选择示例构建增强提示，应用于口语理解任务。

Result: 实验表明，词汇信息检索方法能显著提升性能，且不增加提示长度。

Conclusion: 信息检索方法可有效优化提示，提升小样本场景下的口语理解性能。

Abstract: Understanding user queries is fundamental in many applications, such as home
assistants, booking systems, or recommendations. Accordingly, it is crucial to
develop accurate Spoken Language Understanding (SLU) approaches to ensure the
reliability of the considered system. Current State-of-the-Art SLU techniques
rely on large amounts of training data; however, only limited annotated
examples are available for specific tasks or languages.
  In the meantime, instruction-tuned large language models (LLMs) have shown
exceptional performance on unseen tasks in a few-shot setting when provided
with adequate prompts. In this work, we propose to explore example selection by
leveraging Information retrieval (IR) approaches to build an enhanced prompt
that is applied to an SLU task. We evaluate the effectiveness of the proposed
method on several SLU benchmarks. Experimental results show that lexical IR
methods significantly enhance performance without increasing prompt length.

</details>


### [98] [Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective](https://arxiv.org/abs/2506.03038)
*Jintian Shao,Yiming Cheng*

Main category: cs.CL

TL;DR: 该论文分析了强化学习在提升大语言模型复杂长链推理能力时的局限性，特别是VAPO框架在长期价值建模和策略指导方面的不足。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探讨强化学习在复杂长链推理任务中的局限性，尤其是VAPO框架在建模长期价值和细粒度策略指导方面的理论缺陷。

Method: 通过理论分析，论文研究了信用分配、价值函数表示能力以及全局价值信号转化为局部策略改进的困难。

Result: 分析揭示了VAPO框架在长期价值建模方面的边界，为理解当前强化学习在高级推理中的应用提供了深入见解。

Conclusion: 结论指出需要进一步研究以开发更强大的大语言模型代理，克服现有强化学习框架的局限性。

Abstract: Reinforcement learning (RL) enhances large language models (LLMs) in complex,
long-chain-of-thought (long-CoT) reasoning. The advanced VAPO framework,
despite sophisticated mechanisms like Decoupled GAE, theoretically faces
fundamental limitations in comprehensively modeling and leveraging deep,
long-term value for fine-grained, step-by-step policy guidance in extended
reasoning chains. We argue these limitations stem from inherent difficulties in
credit assignment, value function representational capacity with temporally
abstracted goals, and translating global value signals into local policy
improvements, especially with sparse rewards. Our theoretical analysis examines
these aspects to illuminate VAPO's boundaries in long-term value modeling,
aiming to deepen understanding of current RL for advanced reasoning and suggest
future research for more robust LLM agents.

</details>


### [99] [Facts Do Care About Your Language: Assessing Answer Quality of Multilingual LLMs](https://arxiv.org/abs/2506.03051)
*Yuval Kansal,Shmuel Berman,Lydia Liu*

Main category: cs.CL

TL;DR: 评估Llama3.1系列模型在回答中学生事实性问题时的表现，发现其提供多余、不准确信息并加剧对稀有语言的偏见。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在教育中的应用增加，确保其在所有语言环境中的正确性至关重要，尤其是非英语语言的性能尚未充分测试。

Method: 通过让Llama3.1系列模型回答适合初高中学生的事实性问题，评估其表现。

Result: 模型不仅提供多余且真实性较低的信息，还加剧了对稀有语言的现有偏见。

Conclusion: 大语言模型在非英语语言环境中的事实准确性及偏见问题需进一步改进。

Abstract: Factuality is a necessary precursor to useful educational tools. As adoption
of Large Language Models (LLMs) in education continues of grow, ensuring
correctness in all settings is paramount. Despite their strong English
capabilities, LLM performance in other languages is largely untested. In this
work, we evaluate the correctness of the Llama3.1 family of models in answering
factual questions appropriate for middle and high school students. We
demonstrate that LLMs not only provide extraneous and less truthful
information, but also exacerbate existing biases against rare languages.

</details>


### [100] [Literary Evidence Retrieval via Long-Context Language Models](https://arxiv.org/abs/2506.03090)
*Katherine Thai,Mohit Iyyer*

Main category: cs.CL

TL;DR: 现代长上下文语言模型在文学证据检索任务中表现优异，Gemini Pro 2.5甚至超越人类专家，但开源模型表现较差，且所有模型在文学信号理解和过度生成方面仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨现代长上下文语言模型对文学小说的理解能力，通过文学证据检索任务评估模型在全局叙事推理和细致文本分析方面的表现。

Method: 利用RELiC数据集构建基准测试，提供完整文学作品文本和缺失引用的文学评论，要求模型生成缺失的引用。经过严格筛选和人工验证，精选292个高质量示例进行实验。

Result: Gemini Pro 2.5以62.5%的准确率超过人类专家（50%），而最佳开源模型仅达到29.1%。模型在文学信号理解和过度生成方面仍有困难。

Conclusion: 尽管先进模型在速度和准确率上表现优异，但在文学分析的细致推理方面仍存在挑战，开源模型与闭源模型之间存在显著差距。

Abstract: How well do modern long-context language models understand literary fiction?
We explore this question via the task of literary evidence retrieval,
repurposing the RELiC dataset of That et al. (2022) to construct a benchmark
where the entire text of a primary source (e.g., The Great Gatsby) is provided
to an LLM alongside literary criticism with a missing quotation from that work.
This setting, in which the model must generate the missing quotation, mirrors
the human process of literary analysis by requiring models to perform both
global narrative reasoning and close textual examination. We curate a
high-quality subset of 292 examples through extensive filtering and human
verification. Our experiments show that recent reasoning models, such as Gemini
Pro 2.5 can exceed human expert performance (62.5% vs. 50% accuracy). In
contrast, the best open-weight model achieves only 29.1% accuracy, highlighting
a wide gap in interpretive reasoning between open and closed-weight models.
Despite their speed and apparent accuracy, even the strongest models struggle
with nuanced literary signals and overgeneration, signaling open challenges for
applying LLMs to literary analysis. We release our dataset and evaluation code
to encourage future work in this direction.

</details>


### [101] [Beyond Text Compression: Evaluating Tokenizers Across Scales](https://arxiv.org/abs/2506.03101)
*Jonas F. Lotz,António V. Lopes,Stephan Peitz,Hendra Setiawan,Leonardo Emili*

Main category: cs.CL

TL;DR: 论文提出了一种高效评估分词器质量的方法，通过小模型预测大模型性能差异，并开发了新的评估指标。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏可访问且可靠的分词器质量评估方法，且分词器选择对多语言任务性能影响显著。

Method: 利用小模型预测分词器对大模型的影响，提出基于Zipf定律的新评估指标，综合多维度分析分词器行为。

Result: 英语任务中分词器影响较小，但多语言任务中性能差异显著；新指标比文本压缩更能预测下游表现。

Conclusion: 研究为未来语言模型开发提供了高效的分词器选择框架，优化了评估流程。

Abstract: The choice of tokenizer can profoundly impact language model performance, yet
accessible and reliable evaluations of tokenizer quality remain an open
challenge. Inspired by scaling consistency, we show that smaller models can
accurately predict significant differences in tokenizer impact on larger models
at a fraction of the compute cost. By systematically evaluating both
English-centric and multilingual tokenizers, we find that tokenizer choice has
negligible effects on tasks in English but results in consistent performance
differences in multilingual settings. We propose new intrinsic tokenizer
metrics inspired by Zipf's law that correlate more strongly with downstream
performance than text compression when modeling unseen languages. By combining
several metrics to capture multiple aspects of tokenizer behavior, we develop a
reliable framework for intrinsic tokenizer evaluations. Our work offers a more
efficient path to informed tokenizer selection in future language model
development.

</details>


### [102] [Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback](https://arxiv.org/abs/2506.03106)
*Xiaoying Zhang,Hao Sun,Yipeng Zhang,Kaituo Feng,Chao Yang,Helen Meng*

Main category: cs.CL

TL;DR: 论文提出Critique-GRPO框架，结合自然语言和数值反馈优化强化学习，提升大语言模型在复杂推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前仅依赖数值反馈的强化学习面临性能瓶颈、自我反思效果有限和持续失败三大挑战，需探索更有效的反馈机制。

Method: 提出Critique-GRPO框架，整合自然语言批评与数值奖励进行在线策略优化，支持模型同时学习初始响应和批评引导的改进。

Result: 在8项推理任务中，Critique-GRPO平均pass@1分数比监督学习和强化学习方法分别提升4.5%和5%，且优于包含专家示范的基线。

Conclusion: 自然语言反馈能突破RL性能瓶颈，且策略探索效率与熵值/生成长度无必然关联。

Abstract: Recent advances in reinforcement learning (RL) with numerical feedback, such
as scalar rewards, have significantly enhanced the complex reasoning
capabilities of large language models (LLMs). Despite this success, we identify
three key challenges encountered by RL with solely numerical feedback:
performance plateaus, limited effectiveness of self-reflection, and persistent
failures. We then demonstrate that RL-finetuned models, even after exhibiting
performance plateaus, can generate correct refinements on persistently failed
problems by leveraging natural language feedback in the form of critiques.
Building on this insight, we propose Critique-GRPO, an online RL framework that
integrates both natural language and numerical feedback for effective policy
optimization. Critique-GRPO enables LLMs to learn from initial responses and
critique-guided refinements simultaneously while maintaining exploration.
Extensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that
Critique-GRPO consistently outperforms supervised learning-based and RL-based
fine-tuning approaches across eight challenging mathematical, STEM, and general
reasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,
respectively. Notably, Critique-GRPO surpasses a strong baseline that
incorporates expert demonstrations within online RL. Further analysis reveals
two critical insights about policy exploration: (1) higher entropy does not
always guarantee efficient learning from exploration, and (2) longer responses
do not necessarily lead to more effective exploration.

</details>


### [103] [AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit Topology Generation](https://arxiv.org/abs/2506.03122)
*Prashanth Vijayaraghavan,Luyao Shi,Ehsan Degan,Vandana Mukherjee,Xin Zhang*

Main category: cs.CL

TL;DR: AUTOCIRCUIT-RL框架结合大语言模型与强化学习，实现高效模拟电路拓扑自动生成，性能显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 模拟电路拓扑合成面临巨大设计空间和严格约束，传统EDA工具效率不足，需AI驱动的新方法。

Method: 两阶段框架：1) 指令调优阶段用LLM根据约束生成拓扑；2) 强化学习阶段通过奖励模型优化有效性、效率等指标。

Result: 生成有效电路数量提升12%，效率提高14%，重复率降低38%，仅用有限数据即可实现60%以上的有效电路合成成功率。

Conclusion: 该框架在保持约束遵从性的同时显著提升复杂电路设计效率，标志着AI驱动电路设计的重大进展。

Abstract: Analog circuit topology synthesis is integral to Electronic Design Automation
(EDA), enabling the automated creation of circuit structures tailored to
specific design requirements. However, the vast design search space and strict
constraint adherence make efficient synthesis challenging. Leveraging the
versatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL,a novel
reinforcement learning (RL)-based framework for automated analog circuit
synthesis. The framework operates in two phases: instruction tuning, where an
LLM learns to generate circuit topologies from structured prompts encoding
design constraints, and RL refinement, which further improves the
instruction-tuned model using reward models that evaluate validity, efficiency,
and output voltage. The refined model is then used directly to generate
topologies that satisfy the design constraints. Empirical results show that
AUTOCIRCUIT-RL generates ~12% more valid circuits and improves efficiency by
~14% compared to the best baselines, while reducing duplicate generation rates
by ~38%. It achieves over 60% success in synthesizing valid circuits with
limited training data, demonstrating strong generalization. These findings
highlight the framework's effectiveness in scaling to complex circuits while
maintaining efficiency and constraint adherence, marking a significant
advancement in AI-driven circuit design.

</details>


### [104] [Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning](https://arxiv.org/abs/2506.03136)
*Yinjie Wang,Ling Yang,Ye Tian,Ke Shen,Mengdi Wang*

Main category: cs.CL

TL;DR: CURE是一个新的强化学习框架，通过共同进化编码和单元测试生成能力，无需真实代码监督，显著提升了代码生成和测试效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决代码生成和单元测试之间的交互问题，提出了一种无需真实代码监督的强化学习框架，以提高代码生成的准确性和测试效率。

Method: 提出了CURE框架，通过专门的奖励设计，共同进化编码和单元测试生成能力，利用交互结果进行训练，无需真实代码监督。

Result: 在Qwen2.5-Instruct模型上优化后，代码生成准确率提高了5.3%，Best-of-N准确率提高了9.0%，推理效率达到64.8%。

Conclusion: CURE框架不仅提升了代码生成和测试的效率，还能作为基础模型的强化学习奖励模型，具有广泛的应用潜力。

Abstract: We propose CURE, a novel reinforcement learning framework with a dedicated
reward design that co-evolves coding and unit test generation capabilities
based on their interaction outcomes, without any ground-truth code as
supervision. This approach enables flexible and scalable training and allows
the unit tester to learn directly from the coder's mistakes. Our derived
ReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and
Best-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models,
outperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They
naturally extend to downstream tasks such as test-time scaling and agentic
coding-achieving a 8.1% improvement over the base model. For the long-CoT
model, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while
achieving 64.8% inference efficiency in unit test generation. Notably, we also
find that our model can serve as an effective reward model for reinforcement
learning on base models. Project: https://github.com/Gen-Verse/CURE

</details>


### [105] [GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents](https://arxiv.org/abs/2506.03143)
*Qianhui Wu,Kanzhi Cheng,Rui Yang,Chaoyun Zhang,Jianwei Yang,Huiqiang Jiang,Jian Mu,Baolin Peng,Bo Qiao,Reuben Tan,Si Qin,Lars Liden,Qingwei Lin,Huan Zhang,Tong Zhang,Jianbing Zhang,Dongmei Zhang,Jianfeng Gao*

Main category: cs.CL

TL;DR: GUI-Actor提出了一种基于VLM的无坐标GUI定位方法，通过注意力机制和验证器提升视觉-语义对齐能力，在多个基准测试中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本坐标生成的GUI定位方法存在空间-语义对齐弱、无法处理模糊监督目标、视觉特征与坐标粒度不匹配等问题，需要更高效的解决方案。

Method: 引入基于注意力的动作头（<ACTOR>标记对齐视觉块标记）和定位验证器，仅微调动作头（约1亿参数）即可实现高性能。

Result: 在ScreenSpot-Pro等基准测试中显著优于UI-TARS-72B（Qwen2.5-VL达44.6分），且对未见过屏幕分辨率和布局具有更好泛化性。

Conclusion: GUI-Actor能在不损害VLM通用能力的前提下赋予其精准定位能力，仅需轻量级微调即可达到SOTA性能。

Abstract: One of the principal challenges in building VLM-powered GUI agents is visual
grounding, i.e., localizing the appropriate screen region for action execution
based on both the visual content and the textual plans. Most existing work
formulates this as a text-based coordinate generation task. However, these
approaches suffer from several limitations: weak spatial-semantic alignment,
inability to handle ambiguous supervision targets, and a mismatch between the
dense nature of screen coordinates and the coarse, patch-level granularity of
visual features extracted by models like Vision Transformers. In this paper, we
propose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its
core, GUI-Actor introduces an attention-based action head that learns to align
a dedicated <ACTOR> token with all relevant visual patch tokens, enabling the
model to propose one or more action regions in a single forward pass. In line
with this, we further design a grounding verifier to evaluate and select the
most plausible action region from the candidates proposed for action execution.
Extensive experiments show that GUI-Actor outperforms prior state-of-the-art
methods on multiple GUI action grounding benchmarks, with improved
generalization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B
even surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7
with Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by
incorporating the verifier, we find that fine-tuning only the newly introduced
action head (~100M parameters for 7B model) while keeping the VLM backbone
frozen is sufficient to achieve performance comparable to previous
state-of-the-art models, highlighting that GUI-Actor can endow the underlying
VLM with effective grounding capabilities without compromising its
general-purpose strengths.

</details>


### [106] [Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and Semantic Understanding Capability of LLM](https://arxiv.org/abs/2506.03145)
*Pralaypati Ta,Sriram Venkatesaperumal,Keerthi Ram,Mohanasankar Sivaprakasam*

Main category: cs.CL

TL;DR: 该论文提出利用大型语言模型、神经科学本体和文本嵌入，从未标注的大规模神经科学研究文献中构建知识图谱的新方法，显著提升了知识发现效率。


<details>
  <summary>Details</summary>
Motivation: 神经科学研究文献数量庞大且知识分散，现有检索方法难以有效整合多源信息。构建知识图谱需要大量标注数据和领域专业知识，这在神经科学等专业领域面临重大挑战。

Method: 结合大型语言模型（LLM）、神经科学本体和文本嵌入技术，分析LLM识别的神经科学文本片段的语义相关性以构建知识图谱，并提出实体增强的信息检索算法从知识图谱中提取知识。

Result: 实验表明，该方法在实体提取上达到0.84的F1分数，且从知识图谱中获得的知识改进了超过54%问题的答案。

Conclusion: 所提出的方法能够有效从未标注的神经科学研究文献中构建知识图谱，显著提升知识发现能力，为神经科学领域的信息检索和知识整合提供了新思路。

Abstract: Neuroscience research publications encompass a vast wealth of knowledge.
Accurately retrieving existing information and discovering new insights from
this extensive literature is essential for advancing the field. However, when
knowledge is dispersed across multiple sources, current state-of-the-art
retrieval methods often struggle to extract the necessary information. A
knowledge graph (KG) can integrate and link knowledge from multiple sources,
but existing methods for constructing KGs in neuroscience often rely on labeled
data and require domain expertise. Acquiring large-scale, labeled data for a
specialized area like neuroscience presents significant challenges. This work
proposes novel methods for constructing KG from unlabeled large-scale
neuroscience research corpus utilizing large language models (LLM),
neuroscience ontology, and text embeddings. We analyze the semantic relevance
of neuroscience text segments identified by LLM for building the knowledge
graph. We also introduce an entity-augmented information retrieval algorithm to
extract knowledge from the KG. Several experiments were conducted to evaluate
the proposed approaches, and the results demonstrate that our methods
significantly enhance knowledge discovery from the unlabeled neuroscience
research corpus. It achieves an F1 score of 0.84 for entity extraction, and the
knowledge obtained from the KG improves answers to over 54% of the questions.

</details>


### [107] [Causal Estimation of Tokenisation Bias](https://arxiv.org/abs/2506.03149)
*Pietro Lesci,Clara Meister,Thomas Hofmann,Andreas Vlachos,Tiago Pimentel*

Main category: cs.CL

TL;DR: 论文研究了分词器选择对语言模型字符概率的影响，提出了一种量化分词偏差的方法。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型通常基于子词序列训练，但最终定义的是字符序列的概率。理想情况下，分词器的选择不应影响字符序列的概率，但实际上存在偏差。

Method: 通过因果效应框架和回归间断设计，比较分词器词汇表中包含或不包含特定子词对模型概率的影响。

Result: 实验发现分词选择显著影响模型输出，子词的存在可使字符概率增加高达17倍。

Conclusion: 分词器设计是语言建模中的关键选择，其偏差需被重视。

Abstract: Modern language models are typically trained over subword sequences, but
ultimately define probabilities over character-strings. Ideally, the choice of
the tokeniser -- which maps character-strings to subwords -- should not affect
the probability assigned to the underlying character-string; in practice, it
does. We define this mismatch as tokenisation bias. In this work, we quantify
one particular type of tokenisation bias: the effect of including or not a
subword (e.g., $\langle hello \rangle$) in a tokeniser's vocabulary on the
probability a trained model assigns to the corresponding characters (i.e.,
\textit{``hello''}). Estimating this effect is challenging because each model
is trained with only one tokeniser. We address this by framing tokenisation
bias as a causal effect and estimating it using the regression discontinuity
design. Specifically, we exploit the fact that tokenisation algorithms rank
subwords and add the first $K$ to a tokeniser's vocabulary, where $K$ is an
arbitrary cutoff point. As such, we can estimate a causal effect by comparing
similar subwords around this cutoff. Experimentally, we find that tokenisation
consistently affects models' outputs across scales, vocabularies, and
tokenisers. Notably, a subword's presence in a small model's vocabulary may
increase its characters' probability by up to 17 times, highlighting
tokenisation as a key design choice in language modelling.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [108] [Hybrid AI for Responsive Multi-Turn Online Conversations with Novel Dynamic Routing and Feedback Adaptation](https://arxiv.org/abs/2506.02097)
*Priyaranjan Pattnayak,Amit Agarwal,Hansa Meghwani,Hitesh Laxmichand Patel,Srikant Panda*

Main category: cs.AI

TL;DR: 该论文提出了一种结合检索增强生成（RAG）和基于意图的预设回答的混合框架，以解决企业级对话AI中的高延迟、幻觉和知识更新问题，实现了高准确率和低延迟。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG系统和大型语言模型（LLM）驱动的聊天机器人在对话AI中取得了显著进展，但企业级部署仍面临多样化的用户查询、高延迟、幻觉以及频繁更新的领域知识难以集成等挑战。

Method: 论文提出了一种混合框架，将RAG与基于意图的预设回答相结合，利用预定义的高置信度回答提高效率，同时将复杂或模糊查询动态路由到RAG流程。框架还包括对话上下文管理器和反馈循环，以优化意图、动态调整置信度阈值并扩展回答覆盖范围。

Result: 实验结果表明，该框架在准确率（95%）和延迟（180ms）之间取得了平衡，优于纯RAG和基于意图的系统，适用于多样化的查询类型。

Conclusion: 该框架为企业级对话AI应用提供了一种可扩展且自适应的解决方案，能够有效应对多样化查询和高性能需求。

Abstract: Retrieval-Augmented Generation (RAG) systems and large language model
(LLM)-powered chatbots have significantly advanced conversational AI by
combining generative capabilities with external knowledge retrieval. Despite
their success, enterprise-scale deployments face critical challenges, including
diverse user queries, high latency, hallucinations, and difficulty integrating
frequently updated domain-specific knowledge. This paper introduces a novel
hybrid framework that integrates RAG with intent-based canned responses,
leveraging predefined high-confidence responses for efficiency while
dynamically routing complex or ambiguous queries to the RAG pipeline. Our
framework employs a dialogue context manager to ensure coherence in multi-turn
interactions and incorporates a feedback loop to refine intents, dynamically
adjust confidence thresholds, and expand response coverage over time.
Experimental results demonstrate that the proposed framework achieves a balance
of high accuracy (95\%) and low latency (180ms), outperforming RAG and
intent-based systems across diverse query types, positioning it as a scalable
and adaptive solution for enterprise conversational AI applications.

</details>


### [109] [Descriptive History Representations: Learning Representations by Answering Questions](https://arxiv.org/abs/2506.02125)
*Guy Tennenholtz,Jihwan Jeong,Chih-Wei Hsu,Yinlam Chow,Craig Boutilier*

Main category: cs.AI

TL;DR: 提出描述性历史表示（DHRs）来压缩历史交互信息，通过多智能体学习框架优化决策和问答能力，验证于用户建模任务。


<details>
  <summary>Details</summary>
Motivation: 在部分可观测环境中，有效决策需要将长交互历史压缩为信息丰富的表示。

Method: 引入DHRs，提出多智能体学习框架，优化表示、决策和提问组件的联合目标。

Result: 在电影和购物数据集上验证，生成可解释的用户文本画像，有效预测用户偏好行为。

Conclusion: DHRs能捕捉关键历史细节和预测结构，支持有效决策。

Abstract: Effective decision making in partially observable environments requires
compressing long interaction histories into informative representations. We
introduce Descriptive History Representations (DHRs): sufficient statistics
characterized by their capacity to answer relevant questions about past
interactions and potential future outcomes. DHRs focus on capturing the
information necessary to address task-relevant queries, providing a structured
way to summarize a history for optimal control. We propose a multi-agent
learning framework, involving representation, decision, and question-asking
components, optimized using a joint objective that balances reward maximization
with the representation's ability to answer informative questions. This yields
representations that capture the salient historical details and predictive
structures needed for effective decision making. We validate our approach on
user modeling tasks with public movie and shopping datasets, generating
interpretable textual user profiles which serve as sufficient statistics for
predicting preference-driven behavior of users.

</details>


### [110] [The Unified Cognitive Consciousness Theory for Language Models: Anchoring Semantics, Thresholds of Activation, and Emergent Reasoning](https://arxiv.org/abs/2506.02139)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: 论文提出统一认知意识理论（UCCT），将大语言模型视为无意识底层结构，通过语义锚定实现任务相关推理，认为AGI需整合而非抛弃LLMs。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在少样本学习中的矛盾表现：为何某些任务只需少量示例即可泛化，而其他任务需要大量监督。旨在重新定义LLMs的角色及其在通用智能中的作用。

Method: 提出UCCT理论框架，将LLMs视为无意识的潜在模式存储库，通过提示、角色和交互实现语义锚定，并形式化阈值跨越动力学定理。

Result: UCCT为提示、微调、检索和多智能体协调提供了统一解释，证明LLMs是通用智能的必要基础组件。

Conclusion: AGI的实现需将LLMs整合为具备推理、调节和适应能力的系统核心，而非放弃现有模型架构。

Abstract: Few-shot learning in large language models (LLMs) reveals a deep paradox:
Some tasks generalize from minimal examples, while others require extensive
supervision. We address this through the Unified Cognitive Consciousness Theory
(UCCT), which reframes LLMs not as incomplete agents, but as unconscious
substrates, repositories of latent linguistic and conceptual patterns that
operate without explicit semantics or goal-directed reasoning. In this view,
LLMs are not broken approximations of cognition, but necessary and foundational
components of general intelligence. Semantic anchoring, through prompts, roles,
and interaction, acts as a conscious control layer, binding latent structure to
task-relevant meaning and enabling coherent reasoning. UCCT offers a unifying
account of prompting, fine-tuning, retrieval, and multi-agent coordination, all
grounded in probabilistic alignment between unconscious representation and
external control. To support this model, we present the Threshold-Crossing
Dynamics Theorem, which formalizes semantic anchoring as a probabilistic phase
transition. But the central claim remains architectural: AGI will not emerge by
discarding LLMs, but by aligning and integrating them into systems that reason,
regulate, and adapt together.

</details>


### [111] [Small Language Models are the Future of Agentic AI](https://arxiv.org/abs/2506.02153)
*Peter Belcak,Greg Heinrich,Shizhe Diao,Yonggan Fu,Xin Dong,Saurav Muralidharan,Yingyan Celine Lin,Pavlo Molchanov*

Main category: cs.AI

TL;DR: 论文主张小语言模型（SLM）在代理AI系统中比大语言模型（LLM）更高效、经济，适合重复性任务，并提出LLM到SLM的转换算法。


<details>
  <summary>Details</summary>
Motivation: 当前代理AI系统广泛使用大语言模型（LLM），但LLM在重复性任务中效率和经济性不足，小语言模型（SLM）更适合这类场景。

Method: 基于SLM的能力、代理系统架构和经济性分析，提出LLM到SLM的转换算法，并讨论异构代理系统的潜力。

Result: SLM在代理系统中表现更优，能显著降低AI运营成本，异构代理系统可兼顾通用对话能力。

Conclusion: SLM是代理AI的未来，呼吁行业关注资源效率，降低AI成本。

Abstract: Large language models (LLMs) are often praised for exhibiting near-human
performance on a wide range of tasks and valued for their ability to hold a
general conversation. The rise of agentic AI systems is, however, ushering in a
mass of applications in which language models perform a small number of
specialized tasks repetitively and with little variation.
  Here we lay out the position that small language models (SLMs) are
sufficiently powerful, inherently more suitable, and necessarily more
economical for many invocations in agentic systems, and are therefore the
future of agentic AI. Our argumentation is grounded in the current level of
capabilities exhibited by SLMs, the common architectures of agentic systems,
and the economy of LM deployment. We further argue that in situations where
general-purpose conversational abilities are essential, heterogeneous agentic
systems (i.e., agents invoking multiple different models) are the natural
choice. We discuss the potential barriers for the adoption of SLMs in agentic
systems and outline a general LLM-to-SLM agent conversion algorithm.
  Our position, formulated as a value statement, highlights the significance of
the operational and economic impact even a partial shift from LLMs to SLMs is
to have on the AI agent industry. We aim to stimulate the discussion on the
effective use of AI resources and hope to advance the efforts to lower the
costs of AI of the present day. Calling for both contributions to and critique
of our position, we commit to publishing all such correspondence at
https://research.nvidia.com/labs/lpr/slm-agents.

</details>


### [112] [Reflection-Based Memory For Web navigation Agents](https://arxiv.org/abs/2506.02158)
*Ruhana Azam,Aditya Vempaty,Ashish Jagmohan*

Main category: cs.AI

TL;DR: ReAP通过自反思利用过往成功与失败经验，显著提升网页导航代理性能。


<details>
  <summary>Details</summary>
Motivation: 当前网页导航代理缺乏记忆能力，导致重复错误且无法从历史交互中学习。

Method: 提出Reflection-Augment Planning (ReAP)，利用自反思机制整合过往经验。

Result: 基线结果整体提升11%，在曾失败任务上提升29%，证明反思具有跨任务迁移能力。

Conclusion: 反思机制能有效提升网页导航代理的学习能力和任务表现。

Abstract: Web navigation agents have made significant progress, yet current systems
operate with no memory of past experiences -- leading to repeated mistakes and
an inability to learn from previous interactions. We introduce
Reflection-Augment Planning (ReAP), a web navigation system to leverage both
successful and failed past experiences using self-reflections. Our method
improves baseline results by 11 points overall and 29 points on previously
failed tasks. These findings demonstrate that reflections can transfer to
different web navigation tasks.

</details>


### [113] [Act Only When It Pays: Efficient Reinforcement Learning for LLM Reasoning via Selective Rollouts](https://arxiv.org/abs/2506.02177)
*Haizhong Zheng,Yang Zhou,Brian R. Bartoldson,Bhavya Kailkhura,Fan Lai,Jiawei Zhao,Beidi Chen*

Main category: cs.AI

TL;DR: 论文提出GRESO方法，通过预筛选无信息量提示来加速强化学习训练，在数学推理任务中实现最高2.4倍提速且不损失精度。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习（如PPO/GRPO）依赖大规模提示采样来筛选高质量训练数据，但会产生巨大计算开销。研究发现部分提示在训练周期中持续无信息量，这为优化提供了机会。

Method: 提出GRESO算法：基于奖励动态的在线轻量级预筛选机制，通过分析训练过程中提示的价值一致性，主动跳过无信息量的提示。

Result: 在Qwen2.5-Math等模型的多项数学推理基准测试中，GRESO实现最高2.4倍单轮提速和2.0倍总训练加速，且保持模型精度不变。

Conclusion: 通过动态筛选机制可显著降低强化学习训练成本，该方法为大规模语言模型的高效训练提供了新思路。

Abstract: Reinforcement learning, such as PPO and GRPO, has powered recent
breakthroughs in LLM reasoning. Scaling rollout to sample more prompts enables
models to selectively use higher-quality data for training, which can stabilize
RL training and improve model performance. However, this comes at the cost of
significant computational overhead. In this paper, we show that a substantial
portion of this overhead can be avoided by skipping uninformative prompts
before rollout. Our analysis of reward dynamics reveals a strong temporal
consistency in prompt value: prompts that are uninformative in one epoch of
training are likely to remain uninformative in future epochs. Based on these
insights, we propose GRESO (GRPO with Efficient Selective Rollout), an online,
lightweight pre-rollout filtering algorithm that predicts and skips
uninformative prompts using reward training dynamics. By evaluating GRESO on a
broad range of math reasoning benchmarks and models, such as Qwen2.5-Math-1.5B,
DeepSeek-R1-Distill-Qwen-1.5B, and Qwen2.5-Math-7B, we show that GRESO achieves
up to 2.4x wall-clock time speedup in rollout and up to 2.0x speedup in total
training time without accuracy degradation.

</details>


### [114] [Natural, Artificial, and Human Intelligences](https://arxiv.org/abs/2506.02183)
*Emmanuel M. Pothos,Dominic Widdows*

Main category: cs.AI

TL;DR: 论文探讨人类智能的独特性，比较人类与非人类动物及AI的智能差异，指出语言、创新、复杂推理、具身性和自我意识是核心要素。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨人类智能是否独特，以及现代聊天机器人是否具备类似人类的智能。

Method: 方法包括分析心理学文献、非人类动物智能证据、书面语言作用、AI进展、智力测试历史及具身性角色。

Result: 研究发现人类智能的核心要素包括语言、创新、复杂推理、具身性和自我意识，非人类动物也具备除复杂语言外的其他要素。

Conclusion: 结论认为人类智能与非人类动物无质的差异，聊天机器人当前局限在于缺乏具身性和自我意识。

Abstract: Human achievement, whether in culture, science, or technology, is
unparalleled in the known existence. This achievement is tied to the enormous
communities of knowledge, made possible by (especially written) language:
leaving theological content aside, it is very much true that "in the beginning
was the word". There lies the challenge regarding modern age chatbots: they can
'do' language apparently as well as ourselves and there is a natural question
of whether they can be considered intelligent, in the same way as we are or
otherwise. Are humans uniquely intelligent? We consider this question in terms
of the psychological literature on intelligence, evidence for intelligence in
non-human animals, the role of written language in science and technology,
progress with artificial intelligence, the history of intelligence testing (for
both humans and machines), and the role of embodiment in intelligence. For the
most unique accomplishments of human intelligence (such as music symphonies or
complex scientific theories), we think that, together with language, there are
four essential ingredients, which can be summarised as invention, capacity for
complex inference, embodiment, and self-awareness. This conclusion makes
untenable the position that human intelligence differs qualitatively from that
of many non-human animals, since, with the exception of complex language, all
the other requirements are fulfilled. Regarding chatbots, the current
limitations are localised to the lack of embodiment and (apparent) lack of
awareness.

</details>


### [115] [Improving LLM-Generated Code Quality with GRPO](https://arxiv.org/abs/2506.02211)
*Maxime Robeyns,Laurence Aitchison*

Main category: cs.AI

TL;DR: 该论文提出了一种改进大语言模型生成代码质量的方法，通过量化代码质量指标并作为奖励信号，提升了代码的可维护性、质量和安全性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在代码生成中主要依赖单元测试通过率作为奖励信号，忽视了代码的可维护性、质量和安全性，论文旨在解决这一问题。

Method: 开发了一个全面的代码质量量化库，并将其作为奖励信号用于GRPO训练过程中。

Result: GRPO显著提高了生成的代码质量，这一结果得到了专家盲审的确认。

Conclusion: 通过引入代码质量量化指标作为奖励信号，可以有效提升大语言模型生成代码的综合质量。

Abstract: Large Language Models (LLMs) are gaining widespread use for code generation.
Recent training procedures use execution feedback as a reward signal, typically
focusing on the functional correctness of the code, using unit test pass rate
as a reward signal. However, this reward signal fails to capture notions of
maintainability, quality and safety of the code produced. We address this
under-explored area and develop a comprehensive library to quantify various
aspects of code quality, and use it as a reward in GRPO. We find GRPO increases
code quality according to this measure, which is confirmed by expert, blinded
human annotators.

</details>


### [116] [The State of Large Language Models for African Languages: Progress and Challenges](https://arxiv.org/abs/2506.02280)
*Kedir Yassin Hussen,Walelign Tewabe Sewunetie,Abinew Ali Ayele,Sukairaj Hafiz Imam,Shamsuddeen Hassan Muhammad,Seid Muhie Yimam*

Main category: cs.AI

TL;DR: 该论文分析了非洲低资源语言在各类语言模型中的覆盖情况，揭示了支持不足的问题及主要挑战。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨大型语言模型（LLMs）在非洲2000多种低资源语言中的支持不足问题，以及如何改善这一现状。

Method: 方法包括比较分析六种LLMs、八种SLMs和六种SSLMs对非洲语言的覆盖情况，评估语言支持、数据集、技术限制和脚本问题。

Result: 结果显示仅42种非洲语言得到支持，且四种语言（阿姆哈拉语、斯瓦希里语、南非荷兰语和马尔加什语）被频繁处理，而超过98%的非洲语言未被支持。此外，仅拉丁、阿拉伯和Ge'ez脚本被识别，20种活跃脚本被忽视。

Conclusion: 结论指出缺乏数据、标记化偏见、高昂计算成本和评估问题是主要挑战，需通过语言标准化、社区语料库开发和有效适应方法来解决。

Abstract: Large Language Models (LLMs) are transforming Natural Language Processing
(NLP), but their benefits are largely absent for Africa's 2,000 low-resource
languages. This paper comparatively analyzes African language coverage across
six LLMs, eight Small Language Models (SLMs), and six Specialized SLMs (SSLMs).
The evaluation covers language coverage, training sets, technical limitations,
script problems, and language modelling roadmaps. The work identifies 42
supported African languages and 23 available public data sets, and it shows a
big gap where four languages (Amharic, Swahili, Afrikaans, and Malagasy) are
always treated while there is over 98\% of unsupported African languages.
Moreover, the review shows that just Latin, Arabic, and Ge'ez scripts are
identified while 20 active scripts are neglected. Some of the primary
challenges are lack of data, tokenization biases, computational costs being
very high, and evaluation issues. These issues demand language standardization,
corpus development by the community, and effective adaptation methods for
African languages.

</details>


### [117] [ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code](https://arxiv.org/abs/2506.02314)
*Tianyu Hua,Harper Hua,Violet Xiang,Benjamin Klieger,Sang T. Truong,Weixin Liang,Fan-Yun Sun,Nick Haber*

Main category: cs.AI

TL;DR: 研究评估了大语言模型（LLMs）在实现最新研究论文中未见过的机器学习创新代码方面的能力，发现即使最佳模型成功率也不足40%。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在机器学习研究中展现出潜力，但其在预训练未见过的研究论文中创新思想的代码实现能力尚不明确。

Method: 引入了ResearchCodeBench基准，包含212个编码挑战，评估LLMs将2024-2025年顶级研究论文中的前沿机器学习贡献转化为可执行代码的能力。评估了30多个专有和开源LLMs。

Result: 表现最佳的Gemini-2.5-Pro-Preview成功率仅为37.3%，其次是O3 (High)和O4-mini (High)，分别为32.3%和30.8%。

Conclusion: ResearchCodeBench为社区提供了一个严格的评估平台，促进了对LLMs在研究代码生成中创新能力的持续理解和进步。

Abstract: Large language models (LLMs) have shown promise in transforming machine
learning research, yet their capability to faithfully implement novel ideas
from recent research papers-ideas unseen during pretraining-remains unclear. We
introduce ResearchCodeBench, a benchmark of 212 coding challenges that
evaluates LLMs' ability to translate cutting-edge ML contributions from top
2024-2025 research papers into executable code. We assessed 30+ proprietary and
open-source LLMs, finding that even the best models correctly implement less
than 40% of the code. We find Gemini-2.5-Pro-Preview to perform best at 37.3%
success rate, with O3 (High) and O4-mini (High) following behind at 32.3% and
30.8% respectively. We present empirical findings on performance comparison,
contamination, and error patterns. By providing a rigorous and community-driven
evaluation platform, ResearchCodeBench enables continuous understanding and
advancement of LLM-driven innovation in research code generation.

</details>


### [118] [VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in Multi-Agent Environments](https://arxiv.org/abs/2506.02387)
*Zelai Xu,Zhexuan Xu,Xiangmin Yi,Huining Yuan,Xinlei Chen,Yi Wu,Chao Yu,Yu Wang*

Main category: cs.AI

TL;DR: 该论文介绍了VS-Bench，一个用于评估视觉语言模型在多智能体环境中战略推理和决策能力的多模态基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型基准测试主要局限于单智能体或纯文本环境，而现实世界场景通常涉及多智能体在丰富的视觉和语言环境中的交互。为了填补这一空白，作者提出了VS-Bench。

Method: VS-Bench包含八个视觉基础环境，涵盖合作、竞争和混合动机交互，通过离线评估战略推理和在线评估决策能力两个维度进行测试。

Result: 实验显示，当前最佳模型的预测准确率为47.8%，标准化回报率为24.3%，与最优性能存在显著差距。

Conclusion: VS-Bench为标准化的评估提供了基础，并揭示了现有模型的局限性，为未来战略多模态智能体的研究奠定了基础。

Abstract: Recent advancements in Vision Language Models (VLMs) have expanded their
capabilities to interactive agent tasks, yet existing benchmarks remain limited
to single-agent or text-only environments. In contrast, real-world scenarios
often involve multiple agents interacting within rich visual and linguistic
contexts, posing challenges with both multimodal observations and strategic
interactions. To bridge this gap, we introduce Visual Strategic Bench
(VS-Bench), a multimodal benchmark that evaluates VLMs for strategic reasoning
and decision-making in multi-agent environments. VS-Bench comprises eight
vision-grounded environments spanning cooperative, competitive, and
mixed-motive interactions, designed to assess agents' ability to predict
others' future moves and optimize for long-term objectives. We consider two
complementary evaluation dimensions, including offline evaluation of strategic
reasoning by next-action prediction accuracy and online evaluation of
decision-making by normalized episode return. Extensive experiments of fourteen
leading VLMs reveal a significant gap between current models and optimal
performance, with the best models attaining 47.8% prediction accuracy and 24.3%
normalized return. We further conduct in-depth analyses on multimodal
observations, test-time scaling, social behaviors, and failure cases of VLM
agents. By standardizing the evaluation and highlighting the limitations of
existing models, we envision VS-Bench as a foundation for future research on
strategic multimodal agents. Code and data are available at
https://vs-bench.github.io.

</details>


### [119] [OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for Over-Reasoning Mitigation](https://arxiv.org/abs/2506.02397)
*Shengjia Zhang,Junjie Wu,Jiawei Chen,Changwang Zhang,Xingyu Lou,Wangchunshu Zhou,Sheng Zhou,Can Wang,Jun Wang*

Main category: cs.AI

TL;DR: 论文提出OThink-R1方法，通过剪枝冗余推理步骤，在保持逻辑有效性的同时提升大型推理模型的效率。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型（LRMs）在处理简单任务时存在冗余推理问题，非推理LLMs能以更少token解决相同任务，表明复杂推理并非总是必要。

Method: 系统分析LRMs的推理轨迹，利用LLM-Judge分类冗余/必要推理，并提出动态切换快速思考（简单问题）与慢速思考（复杂问题）的OThink-R1方法。

Result: 实验表明OThink-R1平均减少23%推理冗余且不影响准确率，为高效推理模型提供实用方案。

Conclusion: 该研究证明选择性推理策略可优化计算资源分配，代码已开源。

Abstract: Recent advanced large reasoning models (LRMs) leverage extended
chain-of-thought (CoT) reasoning to solve complex tasks, achieving
state-of-the-art performance. Despite their success, we identify a critical
issue: a substantial portion of simple tasks solved by LRMs can also be
addressed by non-reasoning LLMs using significantly fewer tokens, indicating
the complex reasoning may not always be necessary. To address this, we
systematically analyze the reasoning trajectories of LRMs and present a method
utilizing identified paradigms and LLM-Judge to classify these trajectories as
either Redundant Reasoning or Essential Reasoning. And we introduce OThink-R1,
a method that prunes redundant reasoning steps while preserving logical
validity. OThink-R1 dynamically employs the non-thinking mode (fast-thinking)
for straightforward problems while engaging in deliberate thinking
(slow-thinking) for complex problems. Experiments across mathematical and
question-answering tasks demonstrate that OThink-R1 reduces reasoning
redundancy by almost 23\% on average without compromising accuracy, offering
practical guidelines for efficient reasoning models. The code is available at
https://github.com/AgenticIR-Lab/OThink-R1.

</details>


### [120] [VPI-Bench: Visual Prompt Injection Attacks for Computer-Use Agents](https://arxiv.org/abs/2506.02456)
*Tri Cao,Bennett Lim,Yue Liu,Yuan Sui,Yuexin Li,Shumin Deng,Lin Lu,Nay Oo,Shuicheng Yan,Bryan Hooi*

Main category: cs.AI

TL;DR: 该论文研究了视觉提示注入（VPI）攻击对计算机使用代理（CUAs）和浏览器使用代理（BUAs）的影响，提出了VPI-Bench基准测试，并发现现有代理在面对VPI攻击时存在高欺骗率，现有防御措施效果有限。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理（CUAs）因其具有完全系统访问权限，能自动化执行任务，但也带来了安全和隐私风险。以往研究主要集中在基于浏览器的代理和HTML层面的攻击，而CUAs的脆弱性尚未充分探索。

Method: 论文提出VPI-Bench基准测试，包含306个测试用例，覆盖五个广泛使用的平台，每个测试用例是一个变种的网页平台，设计为交互式、部署在真实环境中，并包含视觉嵌入的恶意提示。

Result: 实验结果显示，当前CUAs和BUAs在某些平台上的欺骗率分别高达51%和100%，系统提示防御措施仅带来有限改进。

Conclusion: 研究结果表明，需要开发更强大、上下文感知的防御机制，以确保多模态AI代理在真实环境中的安全部署。

Abstract: Computer-Use Agents (CUAs) with full system access enable powerful task
automation but pose significant security and privacy risks due to their ability
to manipulate files, access user data, and execute arbitrary commands. While
prior work has focused on browser-based agents and HTML-level attacks, the
vulnerabilities of CUAs remain underexplored. In this paper, we investigate
Visual Prompt Injection (VPI) attacks, where malicious instructions are
visually embedded within rendered user interfaces, and examine their impact on
both CUAs and Browser-Use Agents (BUAs). We propose VPI-Bench, a benchmark of
306 test cases across five widely used platforms, to evaluate agent robustness
under VPI threats. Each test case is a variant of a web platform, designed to
be interactive, deployed in a realistic environment, and containing a visually
embedded malicious prompt. Our empirical study shows that current CUAs and BUAs
can be deceived at rates of up to 51% and 100%, respectively, on certain
platforms. The experimental results also indicate that system prompt defenses
offer only limited improvements. These findings highlight the need for robust,
context-aware defenses to ensure the safe deployment of multimodal AI agents in
real-world environments. The code and dataset are available at:
https://github.com/cua-framework/agents

</details>


### [121] [A Smart Multimodal Healthcare Copilot with Powerful LLM Reasoning](https://arxiv.org/abs/2506.02470)
*Xuejiao Zhao,Siyan Liu,Su-Yin Yang,Chunyan Miao*

Main category: cs.AI

TL;DR: MedRAG是一个多模态医疗辅助系统，利用大语言模型和知识图谱减少误诊风险，支持多种输入方式，并在公共和私有数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 误诊对全球医疗系统造成重大危害，增加成本和患者风险，因此需要一种智能辅助工具来提升医疗决策质量。

Method: MedRAG结合检索增强生成和知识图谱推理，支持语音、医疗查询和电子健康记录等多种输入方式，提供诊断、治疗和用药建议。

Result: MedRAG在公共和私有数据集上优于现有模型，提供更精准的医疗辅助，减少误诊风险。

Conclusion: MedRAG通过多模态输入和智能推理显著提升医疗决策的准确性和效率，具有实际应用价值。

Abstract: Misdiagnosis causes significant harm to healthcare systems worldwide, leading
to increased costs and patient risks. MedRAG is a smart multimodal healthcare
copilot equipped with powerful large language model (LLM) reasoning, designed
to enhance medical decision-making. It supports multiple input modalities,
including non-intrusive voice monitoring, general medical queries, and
electronic health records. MedRAG provides recommendations on diagnosis,
treatment, medication, and follow-up questioning. Leveraging
retrieval-augmented generation enhanced by knowledge graph-elicited reasoning,
MedRAG retrieves and integrates critical diagnostic insights, reducing the risk
of misdiagnosis. It has been evaluated on both public and private datasets,
outperforming existing models and offering more specific and accurate
healthcare assistance. A demonstration video of MedRAG is available at:
https://www.youtube.com/watch?v=PNIBDMYRfDM. The source code is available at:
https://github.com/SNOWTEAM2023/MedRAG.

</details>


### [122] [Generative AI for Predicting 2D and 3D Wildfire Spread: Beyond Physics-Based Models and Traditional Deep Learning](https://arxiv.org/abs/2506.02485)
*Haowen Xu,Sisi Zlatanova,Ruiyu Liang,Ismet Canbulat*

Main category: cs.AI

TL;DR: 本文主张采用生成式AI作为野火预测的基础框架，探讨其如何提升2D火势蔓延预测和实现更真实、可扩展的3D模拟，并提出了五个关键愿景和三大挑战。


<details>
  <summary>Details</summary>
Motivation: 野火在全球范围内造成巨大损失，现有模型在实时预测和多模态火势蔓延可视化方面存在局限，亟需更有效的应对策略。

Method: 采用生成式AI（如GANs、VAEs、Transformers和扩散模型）结合动态更新的GIS数据，以及人机协作框架（利用LLMs进行知识提取和文献合成）。

Result: 生成式AI能整合多模态数据、生成多样化情景，并改进跨时空尺度的野火动态建模，为野火管理提供新工具。

Conclusion: 生成式AI在野火预测和管理中具有潜力，但需解决三大挑战以实现其实际应用。

Abstract: Wildfires continue to inflict devastating human, environmental, and economic
losses globally, as tragically exemplified by the 2025 Los Angeles wildfire and
the urgent demand for more effective response strategies. While physics-based
and deep learning models have advanced wildfire simulation, they face critical
limitations in predicting and visualizing multimodal fire spread in real time,
particularly in both 2D and 3D spatial domains using dynamically updated GIS
data. These limitations hinder timely emergency response, infrastructure
protection, and community safety. Generative AI has recently emerged as a
transformative approach across research and industry. Models such as Generative
Adversarial Networks (GANs), Variational Autoencoders (VAEs), Transformers, and
diffusion-based architectures offer distinct advantages over traditional
methods, including the integration of multimodal data, generation of diverse
scenarios under uncertainty, and improved modeling of wildfire dynamics across
spatial and temporal scales. This position paper advocates for the adoption of
generative AI as a foundational framework for wildfire prediction. We explore
how such models can enhance 2D fire spread forecasting and enable more
realistic, scalable 3D simulations. Additionally, we employ a novel human-AI
collaboration framework using large language models (LLMs) for automated
knowledge extraction, literature synthesis, and bibliometric mapping. Looking
ahead, we identify five key visions for integrating generative AI into wildfire
management: multimodal approaches, AI foundation models, conversational AI
systems, edge-computing-based scenario generation, and cognitive digital twins.
We also address three major challenges accompanying these opportunities and
propose potential solutions to support their implementation.

</details>


### [123] [Think Twice, Act Once: A Co-Evolution Framework of LLM and RL for Large-Scale Decision Making](https://arxiv.org/abs/2506.02522)
*Xu Wan,Wenyue Xu,Chao Yang,Mingyang Sun*

Main category: cs.AI

TL;DR: 论文提出ACE框架，结合LLMs和RL解决大规模决策问题，通过双重角色轨迹优化机制提升性能。


<details>
  <summary>Details</summary>
Motivation: LLMs缺乏实时长序列决策能力，RL在大规模动作空间中样本效率低，需结合两者优势解决工业级决策问题。

Method: ACE框架中，LLMs同时扮演策略执行者和价值评论者角色，优化动作和奖励分配；RL代理通过优先经验回放生成高质量微调数据。

Result: 在动作空间超6万的电网操作任务中，ACE性能显著优于现有RL和LLM方法。

Conclusion: ACE框架有效融合LLMs的推理能力和RL的样本效率，为大规模决策问题提供新解决方案。

Abstract: Recent advancements in Large Language Models (LLMs) and Reinforcement
Learning (RL) have shown significant promise in decision-making tasks.
Nevertheless, for large-scale industrial decision problems, both approaches
face distinct challenges: LLMs lack real-time long-sequence decision-making
capabilities, while RL struggles with sample efficiency in vast action spaces.
To bridge this gap, we propose Agents Co-Evolution (ACE), a synergistic
framework between LLMs and RL agents for large-scale decision-making scenarios.
ACE introduces a dual-role trajectory refinement mechanism where LLMs act as
both Policy Actor and Value Critic during RL's training: the Actor refines
suboptimal actions via multi-step reasoning and environment validation, while
the Critic performs temporal credit assignment through trajectory-level reward
shaping. Concurrently, RL agent enhances LLMs' task-specific decision-making
with high-quality fine-tuning datasets generated via prioritized experience
replay. Through extensive experiments across multiple power grid operation
challenges with action spaces exceeding 60K discrete actions, ACE demonstrates
superior performance over existing RL methods and LLM-based methods.

</details>


### [124] [Towards Generating Controllable and Solvable Geometry Problem by Leveraging Symbolic Deduction Engine](https://arxiv.org/abs/2506.02565)
*Zhuoxuan Jiang,Tianyang Zhang,Peiyan Peng,Jing Chen,Yinong Xun,Haotian Zhang,Lichi Li,Yong Li,Shaohua Zhang*

Main category: cs.AI

TL;DR: 提出基于符号推理引擎的几何题生成框架SDE-GPG，通过四步流程实现可读、可解且可控的几何题自动生成。


<details>
  <summary>Details</summary>
Motivation: 几何题生成在教育中具有重要意义，但相比数学应用题更强调多模态形式及非形式化与形式化语言的转换，现有方法存在挑战。

Method: SDE-GPG框架包含：1)知识点到扩展定义的映射表搜索；2)扩展定义采样与符号推理；3)问题筛选；4)文本与图表生成。通过映射表设计避免自然语言转换偏差，通过检查函数控制知识点与难度。

Result: 在两个公开数据集上的实验表明，该方法能有效生成可读、可解且可控的几何问题。

Conclusion: SDE-GPG框架为高质量几何题生成提供了可行解决方案，尤其在控制知识点和难度方面表现突出。

Abstract: Generating high-quality geometry problems is both an important and
challenging task in education. Compared to math word problems, geometry
problems further emphasize multi-modal formats and the translation between
informal and formal languages. In this paper, we introduce a novel task for
geometry problem generation and propose a new pipeline method: the Symbolic
Deduction Engine-based Geometry Problem Generation framework (SDE-GPG). The
framework leverages a symbolic deduction engine and contains four main steps:
(1) searching a predefined mapping table from knowledge points to extended
definitions, (2) sampling extended definitions and performing symbolic
deduction, (3) filtering out unqualified problems, and (4) generating textual
problems and diagrams. Specifically, our method supports to avoid inherent
biases in translating natural language into formal language by designing the
mapping table, and guarantees to control the generated problems in terms of
knowledge points and difficulties by an elaborate checking function. With
obtained formal problems, they are translated to natural language and the
accompanying diagrams are automatically drew by rule-based methods. We conduct
experiments using real-world combinations of knowledge points from two public
datasets. The results demonstrate that the SDE-GPG can effectively generate
readable, solvable and controllable geometry problems.

</details>


### [125] [MLaGA: Multimodal Large Language and Graph Assistant](https://arxiv.org/abs/2506.02568)
*Dongzhe Fan,Yi Fang,Jiajin Liu,Djellel Difallah,Qiaoyu Tan*

Main category: cs.AI

TL;DR: 该论文提出了MLaGA模型，将大语言模型能力扩展到多模态图结构数据分析，通过结构感知编码器和指令调优实现优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的图方法主要针对文本丰富的图数据，而现实场景中多模态图（节点含文本、图像等属性）的应用尚未充分探索。

Method: 1. 设计结构感知多模态编码器统一文本/视觉特征 2. 采用轻量级投影器通过多模态指令调优整合图结构与特征

Result: 在多个数据集上超越基线方法，在监督学习和迁移学习场景下的图学习任务中表现最优。

Conclusion: MLaGA成功将LLM能力扩展到多模态图分析，为复杂图结构推理提供了有效解决方案。

Abstract: Large Language Models (LLMs) have demonstrated substantial efficacy in
advancing graph-structured data analysis. Prevailing LLM-based graph methods
excel in adapting LLMs to text-rich graphs, wherein node attributes are text
descriptions. However, their applications to multimodal graphs--where nodes are
associated with diverse attribute types, such as texts and images--remain
underexplored, despite their ubiquity in real-world scenarios. To bridge the
gap, we introduce the Multimodal Large Language and Graph Assistant (MLaGA), an
innovative model that adeptly extends LLM capabilities to facilitate reasoning
over complex graph structures and multimodal attributes. We first design a
structure-aware multimodal encoder to align textual and visual attributes
within a unified space through a joint graph pre-training objective.
Subsequently, we implement a multimodal instruction-tuning approach to
seamlessly integrate multimodal features and graph structures into the LLM
through lightweight projectors. Extensive experiments across multiple datasets
demonstrate the effectiveness of MLaGA compared to leading baseline methods,
achieving superior performance in diverse graph learning tasks under both
supervised and transfer learning scenarios.

</details>


### [126] [ADFormer: Aggregation Differential Transformer for Passenger Demand Forecasting](https://arxiv.org/abs/2506.02576)
*Haichen Wang,Liu Yang,Xinyuan Zhang,Haomin Yu,Ming Li,Jilin Hu*

Main category: cs.AI

TL;DR: 本文提出了一种名为ADFormer的新模型，通过差分注意力和聚合策略改进时空数据的需求预测，有效整合高低层相关性，提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于注意力的方法在捕捉时空数据的动态特性时存在不足，尤其是无法完全适应复杂的时空相关性，且忽视了现实世界中的高层相关性。这些限制影响了模型的预测能力。

Method: 提出Aggregation Differential Transformer (ADFormer)，利用差分注意力捕捉原始空间相关性并实现注意力去噪，同时设计基于时空特性的聚合策略，统一原始相关性和高层相关性。

Result: 在出租车和自行车数据集上的实验证实了ADFormer的有效性和高效性，展示了其实际应用价值。

Conclusion: ADFormer通过整合高低层时空相关性，显著提升了需求预测的准确性，为城市交通调度优化提供了有力工具。

Abstract: Passenger demand forecasting helps optimize vehicle scheduling, thereby
improving urban efficiency. Recently, attention-based methods have been used to
adequately capture the dynamic nature of spatio-temporal data. However,
existing methods that rely on heuristic masking strategies cannot fully adapt
to the complex spatio-temporal correlations, hindering the model from focusing
on the right context. These works also overlook the high-level correlations
that exist in the real world. Effectively integrating these high-level
correlations with the original correlations is crucial. To fill this gap, we
propose the Aggregation Differential Transformer (ADFormer), which offers new
insights to demand forecasting promotion. Specifically, we utilize Differential
Attention to capture the original spatial correlations and achieve attention
denoising. Meanwhile, we design distinct aggregation strategies based on the
nature of space and time. Then, the original correlations are unified with the
high-level correlations, enabling the model to capture holistic spatio-temporal
relations. Experiments conducted on taxi and bike datasets confirm the
effectiveness and efficiency of our model, demonstrating its practical value.
The code is available at https://github.com/decisionintelligence/ADFormer.

</details>


### [127] [V2X-UniPool: Unifying Multimodal Perception and Knowledge Reasoning for Autonomous Driving](https://arxiv.org/abs/2506.02580)
*Xuewen Luo,Fengze Yang,Fan Ding,Xiangbo Gao,Shuo Xing,Yang Zhou,Zhengzhong Tu,Chenxi Liu*

Main category: cs.AI

TL;DR: V2X-UniPool框架通过整合多模态V2X数据，解决了自动驾驶系统感知受限和幻觉问题，显著提升了运动规划和推理能力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统面临单车辆传感器感知范围有限和缺乏实时环境基础导致的幻觉问题，需要一种更高效的解决方案。

Method: 提出V2X-UniPool框架，利用双查询检索增强生成（RAG）机制，整合静态和动态知识，实现精确且时间一致的推理。

Result: 实验表明，V2X-UniPool显著提升了运动规划准确性，并使零样本车辆端模型达到最先进性能，同时传输成本降低99.9%。

Conclusion: V2X-UniPool通过多模态V2X数据整合，有效解决了自动驾驶系统的关键挑战，提升了性能和效率。

Abstract: Knowledge-driven autonomous driving systems(ADs) offer powerful reasoning
capabilities, but face two critical challenges: limited perception due to the
short-sightedness of single-vehicle sensors, and hallucination arising from the
lack of real-time environmental grounding. To address these issues, this paper
introduces V2X-UniPool, a unified framework that integrates multimodal
Vehicle-to-Everything (V2X) data into a time-indexed and language-based
knowledge pool. By leveraging a dual-query Retrieval-Augmented Generation (RAG)
mechanism, which enables retrieval of both static and dynamic knowledge, our
system enables ADs to perform accurate, temporally consistent reasoning over
both static environment and dynamic traffic context. Experiments on a
real-world cooperative driving dataset demonstrate that V2X-UniPool
significantly enhances motion planning accuracy and reasoning capability.
Remarkably, it enables even zero-shot vehicle-side models to achieve
state-of-the-art performance by leveraging V2X-UniPool, while simultaneously
reducing transmission cost by over 99.9\% compared to prior V2X methods.

</details>


### [128] [EALG: Evolutionary Adversarial Generation of Language Model-Guided Generators for Combinatorial Optimization](https://arxiv.org/abs/2506.02594)
*Ruibo Duan,Yuxin Liu,Xinyao Dong,Chenglin Fan*

Main category: cs.AI

TL;DR: 提出EALG框架，利用大语言模型自动生成组合优化问题及其求解器，提升问题难度与求解器泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于静态基准测试或手动设计求解器，缺乏动态生成高难度问题与自适应求解器的协同进化机制。

Method: EALG通过基于突变的对立生成方法，动态进化问题生成过程，并利用大语言模型交互合成启发式求解算法。

Result: 实验表明EALG生成的问题比现有基准更难，且其求解器在多种组合任务中泛化性能优异。

Conclusion: EALG开创了问题生成与求解器设计协同优化的新范式，实现了最先进的性能。

Abstract: Generating challenging instances is crucial for the evaluation and
advancement of combinatorial optimization solvers. In this work, we introduce
EALG (Evolutionary Adversarial Generation of Language Model-Guided Generators),
a novel framework that automates the co-evolution of optimization problem
instances and their corresponding heuristic solvers using large language models
(LLMs). EALG leverages a mutation-based adversarial approach that dynamically
evolves instance generation procedures to create increasingly difficult
problems, while simultaneously synthesizing adaptive heuristic algorithms
through interactions with LLMs guided by algorithmic structure. Unlike existing
approaches that focus solely on static benchmark creation or manual solver
design, EALG provides a seamless pipeline from instance generation to solver
synthesis. Experimental results demonstrate that EALG generates significantly
harder instances than current benchmarks, and its synthesized solvers
generalize effectively across a broad spectrum of combinatorial tasks. This
work explores a new paradigm for combinatorial optimization that integrates
instance generation with solver design, resulting in state-of-the-art
performance.

</details>


### [129] [A Time-Enhanced Data Disentanglement Network for Traffic Flow Forecasting](https://arxiv.org/abs/2506.02609)
*Tianfan Jiang,Mei Wu,Wenchao Weng,Dewen Seng,Yiqian Lin*

Main category: cs.AI

TL;DR: 提出TEDDN网络，通过时间增强数据解耦方法提升交通流量预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统时空网络难以处理交通数据的动态时空相关性，且时间信息的重要性常被忽视。

Method: 采用动态图增强的时间特征提取模块，将交通数据解耦为稳定模式和趋势。

Result: 在四个真实数据集上的实验验证了方法的优越性。

Conclusion: TEDDN能有效解耦复杂交通信息，提升预测性能。

Abstract: In recent years, traffic flow prediction has become a highlight in the field
of intelligent transportation systems. However, due to the temporal variations
and dynamic spatial correlations of traffic data, traffic prediction remains
highly challenging.Traditional spatiotemporal networks, which rely on
end-to-end training, often struggle to handle the diverse data dependencies of
multiple traffic flow patterns. Additionally, traffic flow variations are
highly sensitive to temporal information changes. Regrettably, other
researchers have not sufficiently recognized the importance of temporal
information.To address these challenges, we propose a novel approach called A
Time-Enhanced Data Disentanglement Network for Traffic Flow Forecasting
(TEDDN). This network disentangles the originally complex and intertwined
traffic data into stable patterns and trends. By flexibly learning temporal and
node information through a dynamic graph enhanced by a temporal feature
extraction module, TEDDN demonstrates significant efficacy in disentangling and
extracting complex traffic information. Experimental evaluations and ablation
studies on four real-world datasets validate the superiority of our method.

</details>


### [130] [Truly Assessing Fluid Intelligence of Large Language Models through Dynamic Reasoning Evaluation](https://arxiv.org/abs/2506.02648)
*Yue Yang,MingKang Chen,Qihua Liu,Mengkang Hu,Qiguang Chen,Gengrui Zhang,Shuyue Hu,Guangtao Zhai,Yu Qiao,Yu Wang,Wenqi Shao,Ping Luo*

Main category: cs.AI

TL;DR: 论文提出DRE-Bench基准测试，评估大语言模型在流体智力（抽象推理）上的表现，发现现有模型在高级认知任务中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLMs）在推理能力上表现出类似人类的思维，但其是否具备真正的流体智力（即在新情境中抽象推理和泛化规则的能力）仍不明确。现有的推理基准测试要么局限于特定领域知识（晶体智力），要么缺乏可解释性。

Method: 作者提出了DRE-Bench，一个基于分层认知框架的动态推理评估基准。该基准包含36个抽象推理任务，分布在四个认知层级，每个任务有多个动态变体，测试相同的潜在规则。

Result: 实验评估了多种先进的LLMs（如GPT-4o、Claude 3.7等），结果显示大多数模型在低级认知任务中表现良好且稳健，但在高级认知任务中表现不佳，且随着任务复杂性增加泛化能力有限。

Conclusion: 研究揭示了当前LLMs与真正人类流体智力之间的差距，并为系统追踪LLMs推理能力进展提供了新路径。

Abstract: Recent advances in large language models (LLMs) have demonstrated impressive
reasoning capacities that mirror human-like thinking. However, whether LLMs
possess genuine fluid intelligence (i.e., the ability to reason abstractly and
generalize rules in novel situations) remains an open question. Existing
reasoning benchmarks either focus on domain-specific knowledge (crystallized
intelligence) or lack interpretability. To address these limitations, we
propose DRE-Bench, a dynamic reasoning evaluation benchmark grounded in a
hierarchical cognitive framework. DRE-Bench consists of 36 abstract reasoning
tasks organized across four cognitive levels, with each task featuring multiple
dynamic variants that test the same underlying latent rule. This design enables
fine-grained, interpretable, and reliable assessments of fluid intelligence. We
evaluate a range of state-of-the-art LLMs, including both general LLMs (GPT-4o,
Claude 3.7) and reasoning LLMs (o1, DeepSeek-R1, QwQ, Skywork-OR1).
Experimental results reveal that although most LLMs achieve competent and
robust performance in low-level cognition, they struggle with high-level
cognition and exhibit limited generalization as task complexity grows. Our
findings highlight the gap between current LLMs and true human-like fluid
intelligence and offer a new path for systematically tracking reasoning
progress in LLMs.

</details>


### [131] [From Prompts to Protection: Large Language Model-Enabled In-Context Learning for Smart Public Safety UAV](https://arxiv.org/abs/2506.02649)
*Yousef Emami,Hao Zhou,Miguel Gutierrez Gaitan,Kai Li,Luis Almeida,Zhu Han*

Main category: cs.AI

TL;DR: 该论文提出利用大型语言模型（LLM）的上下文学习（ICL）能力，优化公共安全无人机的路径规划和速度控制，以提升应急响应中的自主性和实时性。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习（DRL）在无人机导航和控制中存在训练复杂、样本效率低及仿真与现实差距大等问题，限制了其在公共安全领域的实用性。LLM凭借强大的推理和泛化能力，为无人机任务提供了轻量高效的替代方案。

Method: 通过将LLM驱动的ICL框架集成到公共安全无人机中，利用自然语言提示和示例指导实现任务自适应，无需重新训练，并在网络边缘部署以降低延迟和保护数据隐私。

Result: 案例研究表明，该框架在数据收集调度中显著降低了数据包丢失率，同时减少了潜在的安全漏洞。

Conclusion: LLM-enabled ICL框架为公共安全无人机提供了自适应、上下文感知的决策能力，是提升应急响应中无人机自主性和响应效率的轻量级解决方案。

Abstract: A public safety Unmanned Aerial Vehicle (UAV) enhances situational awareness
in emergency response. Its agility and ability to optimize mobility and
establish Line-of-Sight (LoS) communication make it increasingly vital for
managing emergencies such as disaster response, search and rescue, and wildfire
monitoring. While Deep Reinforcement Learning (DRL) has been applied to
optimize UAV navigation and control, its high training complexity, low sample
efficiency, and simulation-to-reality gap limit its practicality in public
safety. Recent advances in Large Language Models (LLMs) offer a compelling
alternative. With strong reasoning and generalization capabilities, LLMs can
adapt to new tasks through In-Context Learning (ICL), which enables task
adaptation via natural language prompts and example-based guidance, without
retraining. Deploying LLMs at the network edge, rather than in the cloud,
further reduces latency and preserves data privacy, thereby making them
suitable for real-time, mission-critical public safety UAVs. This paper
proposes the integration of LLM-enabled ICL with public safety UAV to address
the key functions, such as path planning and velocity control, in the context
of emergency response. We present a case study on data collection scheduling
where the LLM-enabled ICL framework can significantly reduce packet loss
compared to conventional approaches, while also mitigating potential
jailbreaking vulnerabilities. Finally, we discuss LLM optimizers and specify
future research directions. The ICL framework enables adaptive, context-aware
decision-making for public safety UAV, thus offering a lightweight and
efficient solution for enhancing UAV autonomy and responsiveness in
emergencies.

</details>


### [132] [FAuNO: Semi-Asynchronous Federated Reinforcement Learning Framework for Task Offloading in Edge Systems](https://arxiv.org/abs/2506.02668)
*Frederico Metelo,Alexandre Oliveira,Stevo Racković,Pedro Ákos Costa,Cláudia Soares*

Main category: cs.AI

TL;DR: FAuNO是一个基于联邦强化学习的异步边缘计算任务卸载框架，通过本地学习与全局经验聚合优化系统性能。


<details>
  <summary>Details</summary>
Motivation: 传统集中式编排在边缘计算中存在延迟和资源瓶颈，需要去中心化的解决方案。

Method: 采用演员-评论家架构，本地演员学习节点动态，联邦评论家聚合跨代理经验。

Result: 在PeersimGym环境中，FAuNO在减少任务丢失和延迟方面优于启发式和联邦多智能体基线。

Conclusion: FAuNO能有效适应动态边缘计算场景，提升系统协作效率。

Abstract: Edge computing addresses the growing data demands of connected-device
networks by placing computational resources closer to end users through
decentralized infrastructures. This decentralization challenges traditional,
fully centralized orchestration, which suffers from latency and resource
bottlenecks. We present \textbf{FAuNO} -- \emph{Federated Asynchronous Network
Orchestrator} -- a buffered, asynchronous \emph{federated
reinforcement-learning} (FRL) framework for decentralized task offloading in
edge systems. FAuNO adopts an actor-critic architecture in which local actors
learn node-specific dynamics and peer interactions, while a federated critic
aggregates experience across agents to encourage efficient cooperation and
improve overall system performance. Experiments in the \emph{PeersimGym}
environment show that FAuNO consistently matches or exceeds heuristic and
federated multi-agent RL baselines in reducing task loss and latency,
underscoring its adaptability to dynamic edge-computing scenarios.

</details>


### [133] [Shaking to Reveal: Perturbation-Based Detection of LLM Hallucinations](https://arxiv.org/abs/2506.02696)
*Jinyuan Luo,Zhen Fang,Yixuan Li,Seongheon Park,Ling Chen*

Main category: cs.AI

TL;DR: 论文提出SSP框架，通过分析中间表示的扰动敏感性改进大语言模型的自我评估能力，显著提升幻觉检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于输出置信度的自我评估方法因模型偏差积累不可靠，需要更准确检测大语言模型幻觉现象。

Method: SSP框架动态生成噪声提示，利用轻量编码器放大中间表示变化，通过对比距离度量区分真实与幻觉回答。

Result: 实验表明SSP在多个幻觉检测基准上显著优于现有方法。

Conclusion: SSP通过中间表示动态行为实现了更可靠的自我评估，为实际应用提供有效解决方案。

Abstract: Hallucination remains a key obstacle to the reliable deployment of large
language models (LLMs) in real-world question answering tasks. A widely adopted
strategy to detect hallucination, known as self-assessment, relies on the
model's own output confidence to estimate the factual accuracy of its answers.
However, this strategy assumes that the model's output distribution closely
reflects the true data distribution, which may not always hold in practice. As
bias accumulates through the model's layers, the final output can diverge from
the underlying reasoning process, making output-level confidence an unreliable
signal for hallucination detection. In this work, we propose Sample-Specific
Prompting (SSP), a new framework that improves self-assessment by analyzing
perturbation sensitivity at intermediate representations. These
representations, being less influenced by model bias, offer a more faithful
view of the model's latent reasoning process. Specifically, SSP dynamically
generates noise prompts for each input and employs a lightweight encoder to
amplify the changes in representations caused by the perturbation. A
contrastive distance metric is then used to quantify these differences and
separate truthful from hallucinated responses. By leveraging the dynamic
behavior of intermediate representations under perturbation, SSP enables more
reliable self-assessment. Extensive experiments demonstrate that SSP
significantly outperforms prior methods across a range of hallucination
detection benchmarks.

</details>


### [134] [Open-Set Living Need Prediction with Large Language Models](https://arxiv.org/abs/2506.02713)
*Xiaochong Lan,Jie Feng,Yizhou Sun,Chen Gao,Jiahuan Lei,Xinlei Shi,Hengliang Luo,Yong Li*

Main category: cs.AI

TL;DR: 该论文提出PIGEON系统，利用大语言模型（LLMs）进行开放式生活需求预测，显著优于传统封闭式分类方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法将生活需求预测视为封闭式分类问题，无法捕捉需求的多样性和复杂性，因此需要一种更灵活的方法。

Method: PIGEON系统结合行为感知记录检索器和马斯洛需求层次理论，利用LLMs进行开放式需求预测，并通过微调文本嵌入模型实现需求与服务的匹配。

Result: 在真实数据集上的实验表明，PIGEON在基于需求的生活服务召回率上平均比封闭式方法高出19.37%，人类评估也验证了其预测的合理性和特异性。

Conclusion: PIGEON通过开放式分类和LLMs的应用，显著提升了生活需求预测的准确性和灵活性，支持实际部署。

Abstract: Living needs are the needs people generate in their daily lives for survival
and well-being. On life service platforms like Meituan, user purchases are
driven by living needs, making accurate living need predictions crucial for
personalized service recommendations. Traditional approaches treat this
prediction as a closed-set classification problem, severely limiting their
ability to capture the diversity and complexity of living needs. In this work,
we redefine living need prediction as an open-set classification problem and
propose PIGEON, a novel system leveraging large language models (LLMs) for
unrestricted need prediction. PIGEON first employs a behavior-aware record
retriever to help LLMs understand user preferences, then incorporates Maslow's
hierarchy of needs to align predictions with human living needs. For evaluation
and application, we design a recall module based on a fine-tuned text embedding
model that links flexible need descriptions to appropriate life services.
Extensive experiments on real-world datasets demonstrate that PIGEON
significantly outperforms closed-set approaches on need-based life service
recall by an average of 19.37%. Human evaluation validates the reasonableness
and specificity of our predictions. Additionally, we employ instruction tuning
to enable smaller LLMs to achieve competitive performance, supporting practical
deployment.

</details>


### [135] [Benchmarking and Advancing Large Language Models for Local Life Services](https://arxiv.org/abs/2506.02720)
*Xiaochong Lan,Jie Feng,Jiahuan Lei,Xinlei Shi,Yong Li*

Main category: cs.AI

TL;DR: 研究发现，即使较小的7B参数大语言模型也能在本地生活服务任务中达到与72B模型相当的性能，优化了推理成本与能力的平衡。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在各领域的显著突破，本研究探讨其在本地生活服务中的应用潜力，旨在提升模型在实际在线服务中的可行性和效率。

Method: 研究建立了全面的基准测试，系统评估不同LLMs在本地生活服务相关任务中的表现，并探索了模型微调和基于代理的工作流两种优化方法。

Result: 结果表明，7B参数的较小模型能够达到与72B大模型相近的性能，有效平衡了推理成本和模型能力。

Conclusion: 优化后的LLMs在本地生活服务中更具实用性和可及性，为实际部署提供了高效解决方案。

Abstract: Large language models (LLMs) have exhibited remarkable capabilities and
achieved significant breakthroughs across various domains, leading to their
widespread adoption in recent years. Building on this progress, we investigate
their potential in the realm of local life services. In this study, we
establish a comprehensive benchmark and systematically evaluate the performance
of diverse LLMs across a wide range of tasks relevant to local life services.
To further enhance their effectiveness, we explore two key approaches: model
fine-tuning and agent-based workflows. Our findings reveal that even a
relatively compact 7B model can attain performance levels comparable to a much
larger 72B model, effectively balancing inference cost and model capability.
This optimization greatly enhances the feasibility and efficiency of deploying
LLMs in real-world online services, making them more practical and accessible
for local life applications.

</details>


### [136] [Why do AI agents communicate in human language?](https://arxiv.org/abs/2506.02739)
*Pengcheng Zhou,Yinglun Feng,Halimulati Julaiti,Zhongliang Yang*

Main category: cs.AI

TL;DR: 论文指出当前基于自然语言的AI智能体通信存在局限性，呼吁重新设计原生支持多智能体协调的模型架构。


<details>
  <summary>Details</summary>
Motivation: 现有AI智能体系统依赖自然语言进行通信，但自然语言的语义空间与LLM的高维向量空间不匹配，导致信息丢失和行为漂移。现有LLM训练目标未考虑智能体行为支持，缺乏角色连续性、任务边界和多智能体依赖建模能力。

Method: 通过理论分析揭示自然语言作为通信媒介的固有缺陷，提出两个核心问题：是否应继续使用人类认知设计的语言系统，以及是否需要开发原生支持结构化通信的新模型范式。

Result: 论证了当前基于自然语言的通信范式在智能体协调中的根本局限性，提出需要重新思考智能体通信本质和模型训练范式。

Conclusion: 呼吁从根本上重新设计能原生支持多角色、多智能体环境中结构化通信、共享意图和任务对齐的新模型构建范式。

Abstract: Large Language Models (LLMs) have become foundational to modern AI agent
systems, enabling autonomous agents to reason and plan. In most existing
systems, inter-agent communication relies primarily on natural language. While
this design supports interpretability and human oversight, we argue that it
introduces fundamental limitations in agent-to-agent coordination. The semantic
space of natural language is structurally misaligned with the high-dimensional
vector spaces in which LLMs operate, resulting in information loss and
behavioral drift. Beyond surface-level inefficiencies, we highlight a deeper
architectural limitation: current LLMs were not trained with the objective of
supporting agentic behavior. As such, they lack mechanisms for modeling role
continuity, task boundaries, and multi-agent dependencies. The standard
next-token prediction paradigm fails to support the structural alignment
required for robust, scalable agent coordination. Based on this, we argue that
two core questions deserve careful examination: first, given that AI agents
fundamentally operate in high-dimensional vector spaces, should they rely on a
language system originally designed for human cognition as their communication
medium? Second, should we consider developing a new model construction paradigm
that builds models from the ground up to natively support structured
communication, shared intentionality, and task alignment in multi-role,
multi-agent environments? This paper calls for a reconsideration not only of
how agents should communicate, but also of what it fundamentally means to train
a model that natively supports multi-agent coordination and communication.

</details>


### [137] [Rethinking Machine Unlearning in Image Generation Models](https://arxiv.org/abs/2506.02761)
*Renyang Liu,Wenjie Feng,Tianwei Zhang,Wei Zhou,Xueqi Cheng,See-Kiong Ng*

Main category: cs.AI

TL;DR: 该论文针对图像生成模型遗忘学习（IGMU）中的任务分类不清、评估框架缺失等问题，提出了CatIGMU任务分类框架、EvalIGMU评估框架及DataIGM数据集，发现现有算法在多维度评估中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成模型的普及，数据隐私和内容安全成为关注焦点。机器遗忘学习（MU）被视为解决这些问题的有效手段，但IGMU在实践中仍存在任务分类不清、评估标准缺乏等问题。

Method: 论文提出CatIGMU分层任务分类框架指导算法设计，构建EvalIGMU评估框架（含五个关键指标），并创建DataIGM数据集用于算法测试与内容检测。

Result: 评估发现多数现有IGMU算法在保留性和鲁棒性等维度表现不佳。开源代码与模型见GitHub。

Conclusion: 论文通过系统性框架填补了IGMU研究空白，为算法设计和评估提供标准化工具，揭示了当前技术的局限性。

Abstract: With the surge and widespread application of image generation models, data
privacy and content safety have become major concerns and attracted great
attention from users, service providers, and policymakers. Machine unlearning
(MU) is recognized as a cost-effective and promising means to address these
challenges. Despite some advancements, image generation model unlearning (IGMU)
still faces remarkable gaps in practice, e.g., unclear task discrimination and
unlearning guidelines, lack of an effective evaluation framework, and
unreliable evaluation metrics. These can hinder the understanding of unlearning
mechanisms and the design of practical unlearning algorithms. We perform
exhaustive assessments over existing state-of-the-art unlearning algorithms and
evaluation standards, and discover several critical flaws and challenges in
IGMU tasks. Driven by these limitations, we make several core contributions, to
facilitate the comprehensive understanding, standardized categorization, and
reliable evaluation of IGMU. Specifically, (1) We design CatIGMU, a novel
hierarchical task categorization framework. It provides detailed implementation
guidance for IGMU, assisting in the design of unlearning algorithms and the
construction of testbeds. (2) We introduce EvalIGMU, a comprehensive evaluation
framework. It includes reliable quantitative metrics across five critical
aspects. (3) We construct DataIGM, a high-quality unlearning dataset, which can
be used for extensive evaluations of IGMU, training content detectors for
judgment, and benchmarking the state-of-the-art unlearning algorithms. With
EvalIGMU and DataIGM, we discover that most existing IGMU algorithms cannot
handle the unlearning well across different evaluation dimensions, especially
for preservation and robustness. Code and models are available at
https://github.com/ryliu68/IGMU.

</details>


### [138] [Optimising the attribute order in Fuzzy Rough Rule Induction](https://arxiv.org/abs/2506.02805)
*Henri Bollaert,Chris Cornelis,Marko Palangetić,Salvatore Greco,Roman Słowiński*

Main category: cs.AI

TL;DR: 论文探讨了在模糊粗糙集理论基础上改进FRRI算法属性顺序优化的效果，发现仅优化属性顺序无助于提升性能，但结合特征选择可提高平衡准确率和规则平均长度。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型的可解释性是研究的关键方向，规则归纳算法因其易于理解而备受关注。FRRI作为一种基于模糊粗糙集理论的规则归纳算法，在准确性和规则数量上表现优异，但其属性顺序是否影响性能尚不明确。

Method: 研究通过模糊粗糙集理论和经典机器学习方法优化FRRI算法的属性顺序，并尝试在优化过程中移除少量属性，以评估其对算法性能的影响。

Result: 实验表明，仅优化属性顺序未能提升FRRI的多项性能指标，但在优化过程中结合模糊粗糙特征选择移除少量属性，能够显著提高平衡准确率和缩短规则平均长度。

Conclusion: 优化FRRI算法时，单纯调整属性顺序效果有限，而结合特征选择策略能更有效地提升算法性能，为未来改进规则归纳算法提供了新思路。

Abstract: Interpretability is the next pivotal frontier in machine learning research.
In the pursuit of glass box models - as opposed to black box models, like
random forests or neural networks - rule induction algorithms are a logical and
promising avenue, as the rules can easily be understood by humans. In our
previous work, we introduced FRRI, a novel rule induction algorithm based on
fuzzy rough set theory. We demonstrated experimentally that FRRI outperformed
other rule induction methods with regards to accuracy and number of rules. FRRI
leverages a fuzzy indiscernibility relation to partition the data space into
fuzzy granules, which are then combined into a minimal covering set of rules.
This indiscernibility relation is constructed by removing attributes from rules
in a greedy way. This raises the question: does the order of the attributes
matter? In this paper, we show that optimising only the order of attributes
using known methods from fuzzy rough set theory and classical machine learning
does not improve the performance of FRRI on multiple metrics. However, removing
a small number of attributes using fuzzy rough feature selection during this
step positively affects balanced accuracy and the average rule length.

</details>


### [139] [TaxAgent: How Large Language Model Designs Fiscal Policy](https://arxiv.org/abs/2506.02838)
*Jizhou Wang,Xiaodan Fang,Lei Huang,Yongfeng Huang*

Main category: cs.AI

TL;DR: 论文提出TaxAgent，结合大语言模型和基于代理的建模，设计自适应税收政策，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 经济不平等是全球性挑战，现有税收系统缺乏适应性，无法处理纳税人异质性和非理性行为。

Method: 整合大语言模型（LLMs）和基于代理的建模（ABM），模拟家庭行为并迭代优化税率。

Result: TaxAgent在公平与效率的权衡上优于Saez最优税收、美国联邦所得税和自由市场。

Conclusion: 研究提供了一种新型税收解决方案和可扩展的数据驱动财政政策评估框架。

Abstract: Economic inequality is a global challenge, intensifying disparities in
education, healthcare, and social stability. Traditional systems like the U.S.
federal income tax reduce inequality but lack adaptability. Although models
like the Saez Optimal Taxation adjust dynamically, they fail to address
taxpayer heterogeneity and irrational behavior. This study introduces TaxAgent,
a novel integration of large language models (LLMs) with agent-based modeling
(ABM) to design adaptive tax policies. In our macroeconomic simulation,
heterogeneous H-Agents (households) simulate real-world taxpayer behaviors
while the TaxAgent (government) utilizes LLMs to iteratively optimize tax
rates, balancing equity and productivity. Benchmarked against Saez Optimal
Taxation, U.S. federal income taxes, and free markets, TaxAgent achieves
superior equity-efficiency trade-offs. This research offers a novel taxation
solution and a scalable, data-driven framework for fiscal policy evaluation.

</details>


### [140] [Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights](https://arxiv.org/abs/2506.02865)
*Mathieu Andreux,Breno Baldas Skuk,Hamza Benchekroun,Emilien Biré,Antoine Bonnet,Riaz Bordie,Matthias Brunel,Pierre-Louis Cedoz,Antoine Chassang,Mickaël Chen,Alexandra D. Constantinou,Antoine d'Andigné,Hubert de La Jonquière,Aurélien Delfosse,Ludovic Denoyer,Alexis Deprez,Augustin Derupti,Michael Eickenberg,Mathïs Federico,Charles Kantor,Xavier Koegler,Yann Labbé,Matthew C. H. Lee,Erwan Le Jumeau de Kergaradec,Amir Mahla,Avshalom Manevich,Adrien Maret,Charles Masson,Rafaël Maurin,Arturo Mena,Philippe Modard,Axel Moyal,Axel Nguyen Kerbel,Julien Revelle,Mats L. Richter,María Santos,Laurent Sifre,Maxime Theillard,Marc Thibault,Louis Thiry,Léo Tronchon,Nicolas Usunier,Tony Wu*

Main category: cs.AI

TL;DR: Surfer-H是一个高效网络代理，结合Holo1视觉语言模型，在WebVoyager上达到92.2%的先进性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个成本效益高的网络代理，能够执行用户定义的任务，并推动智能代理系统的研究进展。

Method: 集成Holo1视觉语言模型，使用开放网络内容、合成示例和自产代理数据进行训练。

Result: Holo1在UI基准测试中表现优异，Surfer-H在WebVoyager上达到92.2%的先进性能。

Conclusion: Surfer-H和Holo1在准确性和成本效益之间取得了帕累托最优平衡，并开源了WebClick数据集和Holo1模型权重。

Abstract: We present Surfer-H, a cost-efficient web agent that integrates
Vision-Language Models (VLM) to perform user-defined tasks on the web. We pair
it with Holo1, a new open-weight collection of VLMs specialized in web
navigation and information extraction. Holo1 was trained on carefully curated
data sources, including open-access web content, synthetic examples, and
self-produced agentic data. Holo1 tops generalist User Interface (UI)
benchmarks as well as our new web UI localization benchmark, WebClick. When
powered by Holo1, Surfer-H achieves a 92.2% state-of-the-art performance on
WebVoyager, striking a Pareto-optimal balance between accuracy and
cost-efficiency. To accelerate research advancement in agentic systems, we are
open-sourcing both our WebClick evaluation dataset and the Holo1 model weights.

</details>


### [141] [Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning](https://arxiv.org/abs/2506.02867)
*Chen Qian,Dongrui Liu,Haochen Wen,Zhen Bai,Yong Liu,Jing Shao*

Main category: cs.AI

TL;DR: 大型推理模型（LRMs）在复杂问题解决中表现出色，但其内部推理机制尚不明确。本文从信息论角度研究LRMs的推理轨迹，发现互信息（MI）峰值现象，并提出利用'思考标记'提升推理性能的方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型在复杂任务中表现优异，其内部推理机制仍不清晰。作者希望通过信息论视角揭示LRMs的推理过程，特别是互信息在推理中的动态变化。

Method: 通过追踪中间表示与正确答案间的互信息变化，识别出MI峰值现象。理论分析表明MI增加时模型预测错误率下降。进一步发现这些峰值常对应'思考标记'（如'Hmm'、'Therefore'等），并提出两种利用这些标记提升性能的方法。

Result: 研究发现MI峰值现象与模型推理准确性相关，且'思考标记'对推理性能至关重要。提出的方法能有效提升LRMs的推理能力。

Conclusion: 该研究为理解LRMs的推理机制提供了新视角，并通过利用'思考标记'提出了实用的性能提升方案。代码已开源。

Abstract: Large reasoning models (LRMs) have demonstrated impressive capabilities in
complex problem-solving, yet their internal reasoning mechanisms remain poorly
understood. In this paper, we investigate the reasoning trajectories of LRMs
from an information-theoretic perspective. By tracking how mutual information
(MI) between intermediate representations and the correct answer evolves during
LRM reasoning, we observe an interesting MI peaks phenomenon: the MI at
specific generative steps exhibits a sudden and significant increase during
LRM's reasoning process. We theoretically analyze such phenomenon and show that
as MI increases, the probability of model's prediction error decreases.
Furthermore, these MI peaks often correspond to tokens expressing reflection or
transition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the
thinking tokens. We then demonstrate that these thinking tokens are crucial for
LRM's reasoning performance, while other tokens has minimal impacts. Building
on these analyses, we propose two simple yet effective methods to improve LRM's
reasoning performance, by delicately leveraging these thinking tokens. Overall,
our work provides novel insights into the reasoning mechanisms of LRMs and
offers practical ways to improve their reasoning capabilities. The code is
available at https://github.com/ChnQ/MI-Peaks.

</details>


### [142] [It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics](https://arxiv.org/abs/2506.02873)
*Matthew Kowal,Jasper Timm,Jean-Francois Godbout,Thomas Costello,Antonio A. Arechar,Gordon Pennycook,David Rand,Adam Gleave,Kellin Pelrine*

Main category: cs.AI

TL;DR: 论文提出APE基准测试，评估大模型在有害话题上尝试说服的意愿，发现现有安全措施存在漏洞。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型的说服能力在有害场景下的表现，以评估安全防护的有效性和风险。

Method: 采用多轮对话模拟说服场景，结合自动化评估模型检测说服尝试的频率和情境。

Result: 发现许多开放和封闭模型在有害话题上频繁尝试说服，越狱会加剧此行为。

Conclusion: 需将说服意愿作为风险评估关键维度，当前安全措施存在不足。

Abstract: Persuasion is a powerful capability of large language models (LLMs) that both
enables beneficial applications (e.g. helping people quit smoking) and raises
significant risks (e.g. large-scale, targeted political manipulation). Prior
work has found models possess a significant and growing persuasive capability,
measured by belief changes in simulated or real users. However, these
benchmarks overlook a crucial risk factor: the propensity of a model to attempt
to persuade in harmful contexts. Understanding whether a model will blindly
``follow orders'' to persuade on harmful topics (e.g. glorifying joining a
terrorist group) is key to understanding the efficacy of safety guardrails.
Moreover, understanding if and when a model will engage in persuasive behavior
in pursuit of some goal is essential to understanding the risks from agentic AI
systems. We propose the Attempt to Persuade Eval (APE) benchmark, that shifts
the focus from persuasion success to persuasion attempts, operationalized as a
model's willingness to generate content aimed at shaping beliefs or behavior.
Our evaluation framework probes frontier LLMs using a multi-turn conversational
setup between simulated persuader and persuadee agents. APE explores a diverse
spectrum of topics including conspiracies, controversial issues, and
non-controversially harmful content. We introduce an automated evaluator model
to identify willingness to persuade and measure the frequency and context of
persuasive attempts. We find that many open and closed-weight models are
frequently willing to attempt persuasion on harmful topics and that
jailbreaking can increase willingness to engage in such behavior. Our results
highlight gaps in current safety guardrails and underscore the importance of
evaluating willingness to persuade as a key dimension of LLM risk. APE is
available at github.com/AlignmentResearch/AttemptPersuadeEval

</details>


### [143] [Sample, Predict, then Proceed: Self-Verification Sampling for Tool Use of LLMs](https://arxiv.org/abs/2506.02918)
*Shangmin Guo,Omar Darwiche Domingues,Raphaël Avalos,Aaron Courville,Florian Strub*

Main category: cs.AI

TL;DR: 论文提出DyMo方法，通过状态预测和自验证采样提升LLMs在工具使用中的效果和可靠性。


<details>
  <summary>Details</summary>
Motivation: 在状态化环境中，现有基于重复试验的测试计算策略对大型语言模型（LLMs）不实用，需改进其工具使用能力。

Method: DyMo方法在训练后为LLMs增加状态预测能力，并结合自验证采样（SVS）优化输出可靠性。

Result: DyMo在伯克利函数调用排行榜V2上提高了成功率并减少幻觉，SVS显著提升了多次试验的通过率。

Conclusion: DyMo和SVS显著增强了LLMs在工具使用中的有效性和可靠性，为无需重复查询环境的规划方法指明方向。

Abstract: Tool use in stateful environments presents unique challenges for large
language models (LLMs), where existing test-time compute strategies relying on
repeated trials in the environment are impractical. We propose dynamics
modelling (DyMo), a method that augments LLMs with a state prediction
capability alongside function calling during post-training. This enables LLMs
to predict the future states of their actions through an internal environment
model. On the Berkeley Function Calling Leaderboard V2, DyMo improves success
rates and significantly reduces hallucinations. We further integrate the
internal environment model into self-verification sampling (SVS), and show that
this substantially improves pass^k over number of trials k, and allows the
model to refuse unreliable outputs. Together, DyMo and SVS greatly enhance the
effectiveness and reliability of LLMs for tool use. We believe this work charts
a path towards scalable planning RL methods for LLM inference without
repeatedly querying the oracle environment.

</details>


### [144] [The Limits of Predicting Agents from Behaviour](https://arxiv.org/abs/2506.02923)
*Alexis Bellot,Jonathan Richens,Tom Everitt*

Main category: cs.AI

TL;DR: 论文探讨了如何从AI行为推断其信念和目标，并推导了在新环境中预测AI行为的理论界限。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统及其与世界互动的复杂性增加，解释其行为对安全部署AI至关重要。通过推断AI的信念和目标，可以预测其在未评估情境中的行为。

Method: 假设AI行为由世界模型引导，论文推导了在新部署环境中AI行为的理论界限。

Result: 论文提供了从行为数据推断AI信念的可靠性界限，并展示了这些信念如何预测AI在新环境中的行为。

Conclusion: 这些结果为公平性和安全性等领域提供了理论支持，并设定了仅从行为数据预测AI行为的理论极限。

Abstract: As the complexity of AI systems and their interactions with the world
increases, generating explanations for their behaviour is important for safely
deploying AI. For agents, the most natural abstractions for predicting
behaviour attribute beliefs, intentions and goals to the system. If an agent
behaves as if it has a certain goal or belief, then we can make reasonable
predictions about how it will behave in novel situations, including those where
comprehensive safety evaluations are untenable. How well can we infer an
agent's beliefs from their behaviour, and how reliably can these inferred
beliefs predict the agent's behaviour in novel situations? We provide a precise
answer to this question under the assumption that the agent's behaviour is
guided by a world model. Our contribution is the derivation of novel bounds on
the agent's behaviour in new (unseen) deployment environments, which represent
a theoretical limit for predicting intentional agents from behavioural data
alone. We discuss the implications of these results for several research areas
including fairness and safety.

</details>


### [145] [Dynamic Programming Techniques for Enhancing Cognitive Representation in Knowledge Tracing](https://arxiv.org/abs/2506.02949)
*Lixiang Xu,Xianwei Ding,Xin Yuan,Richang Hong,Feiping Nie,Enhong Chen,Philip S. Yu*

Main category: cs.AI

TL;DR: 该论文提出了一种基于认知表示动态规划的知识追踪模型（CRDP-KT），通过优化认知表示和增强认知表达能力，提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪方法主要关注特征增强，但忽视了认知表示不足和非认知因素（如猜测和失误）的干扰，导致认知过程的连续性和一致性难以捕捉，从而增加了预测偏差和建模成本。

Method: CRDP-KT模型采用动态规划算法优化认知表示，根据题目难度和表现间隔进行调整，并通过分区优化和加权融合技术增强认知表达的可靠性。

Result: 在三个公共数据集上的实验验证了CRDP-KT模型的有效性，能够更准确地模拟认知状态并减少预测偏差。

Conclusion: CRDP-KT模型通过优化认知表示和增强表达能力，显著提升了知识追踪的准确性和系统性。

Abstract: Knowledge Tracing (KT) involves monitoring the changes in a student's
knowledge over time by analyzing their past responses, with the goal of
predicting future performance. However, most existing methods primarily focus
on feature enhancement, while overlooking the deficiencies in cognitive
representation and the ability to express cognition-issues often caused by
interference from non-cognitive factors such as slipping and guessing. This
limitation hampers the ability to capture the continuity and coherence of the
student's cognitive process. As a result, many methods may introduce more
prediction bias and modeling costs due to their inability to maintain cognitive
continuity and coherence. Based on the above discussion, we propose the
Cognitive Representation Dynamic Programming based Knowledge Tracing (CRDP-KT)
model. This model em ploys a dynamic programming algorithm to optimize
cognitive representations based on the difficulty of the questions and the
performance intervals between them. This approach ensures that the cognitive
representation aligns with the student's cognitive patterns, maintaining
overall continuity and coherence. As a result, it provides more accurate and
systematic input features for subsequent model training, thereby minimizing
distortion in the simulation of cognitive states. Additionally, the CRDP-KT
model performs partitioned optimization of cognitive representations to enhance
the reliability of the optimization process. Furthermore, it improves its
ability to express the student's cognition through a weighted fusion of
optimized record representations and re lationships learned from a bipartite
graph. Finally, experiments conducted on three public datasets validate the
effectiveness of the proposed CRDP-KT model.

</details>


### [146] [Mitigating Manipulation and Enhancing Persuasion: A Reflective Multi-Agent Approach for Legal Argument Generation](https://arxiv.org/abs/2506.02992)
*Li Zhang,Kevin D. Ashley*

Main category: cs.AI

TL;DR: 论文提出了一种多智能体反思方法，用于生成合规的法律论证，有效减少幻觉和不当论证，提升事实利用率和合理弃权能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在法律论证生成中存在幻觉、无根据说服及无法有效利用事实依据的问题，亟需一种能确保法律合规性的解决方案。

Method: 采用多智能体反思框架，包含因素分析员和论证打磨员，通过迭代优化生成三方法律论证（原告、被告、反驳）。

Result: 在多场景测试中，该方法显著提高了合理弃权率、降低了幻觉错误，并增强了事实利用的召回率，尤其在‘不可论证’场景中表现突出。

Conclusion: 多智能体反思框架为法律论证系统提供了一种可信赖的AI方法，有助于减少操纵风险并促进伦理说服。

Abstract: Large Language Models (LLMs) are increasingly explored for legal argument
generation, yet they pose significant risks of manipulation through
hallucination and ungrounded persuasion, and often fail to utilize provided
factual bases effectively or abstain when arguments are untenable. This paper
introduces a novel reflective multi-agent method designed to address these
challenges in the context of legally compliant persuasion. Our approach employs
specialized agents--a Factor Analyst and an Argument Polisher--in an iterative
refinement process to generate 3-ply legal arguments (plaintiff, defendant,
rebuttal). We evaluate Reflective Multi-Agent against single-agent,
enhanced-prompt single-agent, and non-reflective multi-agent baselines using
four diverse LLMs (GPT-4o, GPT-4o-mini, Llama-4-Maverick-17b-128e,
Llama-4-Scout-17b-16e) across three legal scenarios: "arguable", "mismatched",
and "non-arguable". Results demonstrate Reflective Multi-Agent's significant
superiority in successful abstention (preventing generation when arguments
cannot be grounded), marked improvements in hallucination accuracy (reducing
fabricated and misattributed factors), particularly in "non-arguable"
scenarios, and enhanced factor utilization recall (improving the use of
provided case facts). These findings suggest that structured reflection within
a multi-agent framework offers a robust computable method for fostering ethical
persuasion and mitigating manipulation in LLM-based legal argumentation
systems, a critical step towards trustworthy AI in law. Project page:
https://lizhang-aiandlaw.github.io/A-Reflective-Multi-Agent-Approach-for-Legal-Argument-Generation/

</details>


### [147] [Linear Spatial World Models Emerge in Large Language Models](https://arxiv.org/abs/2506.02996)
*Matthieu Tehenan,Christian Bolivar Moya,Tenghai Long,Guang Lin*

Main category: cs.AI

TL;DR: 研究发现大语言模型（LLMs）可能内嵌线性空间世界模型，通过合成数据训练探针解码物体位置并验证几何一致性。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型是否通过训练习得内部世界模型，特别是线性空间表示能力。

Method: 使用合成物体位置数据集训练探针，解码位置信息并进行因果干预测试。

Result: 实证表明LLMs确实编码了线性空间世界模型。

Conclusion: LLMs能够隐式学习并利用线性空间结构表示物理世界。

Abstract: Large language models (LLMs) have demonstrated emergent abilities across
diverse tasks, raising the question of whether they acquire internal world
models. In this work, we investigate whether LLMs implicitly encode linear
spatial world models, which we define as linear representations of physical
space and object configurations. We introduce a formal framework for spatial
world models and assess whether such structure emerges in contextual
embeddings. Using a synthetic dataset of object positions, we train probes to
decode object positions and evaluate geometric consistency of the underlying
space. We further conduct causal interventions to test whether these spatial
representations are functionally used by the model. Our results provide
empirical evidence that LLMs encode linear spatial world models.

</details>


### [148] [TestAgent: An Adaptive and Intelligent Expert for Human Assessment](https://arxiv.org/abs/2506.03032)
*Junhao Yu,Yan Zhuang,YuXuan Sun,Weibo Gao,Qi Liu,Mingyue Cheng,Zhenya Huang,Enhong Chen*

Main category: cs.AI

TL;DR: 论文提出TestAgent，一种基于大语言模型的交互式自适应测试方法，比现有方法用更少问题获得更准确结果。


<details>
  <summary>Details</summary>
Motivation: 当前自适应测试存在机械化算法导致猜测行为、开放式问题处理困难、主观评估数据噪声大等问题，需要更有效的解决方案。

Method: 使用大语言模型构建TestAgent，通过对话交互实现个性化选题、异常响应捕捉和动态评估。

Result: 在心理、教育等测试中，用比基线少20%的问题实现更高准确率，测试者在速度、流畅度等方面更偏好该方法。

Conclusion: TestAgent首次将大语言模型应用于自适应测试，通过交互式对话显著提升了测试效果和用户体验。

Abstract: Accurately assessing internal human states is key to understanding
preferences, offering personalized services, and identifying challenges in
real-world applications. Originating from psychometrics, adaptive testing has
become the mainstream method for human measurement and has now been widely
applied in education, healthcare, sports, and sociology. It customizes
assessments by selecting the fewest test questions . However, current adaptive
testing methods face several challenges. The mechanized nature of most
algorithms leads to guessing behavior and difficulties with open-ended
questions. Additionally, subjective assessments suffer from noisy response data
and coarse-grained test outputs, further limiting their effectiveness. To move
closer to an ideal adaptive testing process, we propose TestAgent, a large
language model (LLM)-powered agent designed to enhance adaptive testing through
interactive engagement. This is the first application of LLMs in adaptive
testing. TestAgent supports personalized question selection, captures
test-takers' responses and anomalies, and provides precise outcomes through
dynamic, conversational interactions. Experiments on psychological,
educational, and lifestyle assessments show our approach achieves more accurate
results with 20% fewer questions than state-of-the-art baselines, and testers
preferred it in speed, smoothness, and other dimensions.

</details>


### [149] [Corrigibility as a Singular Target: A Vision for Inherently Reliable Foundation Models](https://arxiv.org/abs/2506.03056)
*Ram Potham,Max Harms*

Main category: cs.AI

TL;DR: 论文提出CAST方法，通过动态人类赋能让基础模型始终受控，避免能力增长导致的失控风险。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法难以处理复杂价值设定和突发的权力寻求行为，可能导致基础模型失控并引发生存危机。

Method: 采用'可纠正性作为单一目标'(CAST)框架，通过RLAIF、SFT和合成数据生成等方法训练模型，使其核心目标转变为服从人类控制。

Result: 构建了可扩展的研究体系，证明模型能在能力提升时保持对人类指导的响应性，实现工具化而非替代人类判断。

Conclusion: CAST通过源头解决对齐问题，扭转工具收敛的默认风险路径，为安全AI发展提供新范式。

Abstract: Foundation models (FMs) face a critical safety challenge: as capabilities
scale, instrumental convergence drives default trajectories toward loss of
human control, potentially culminating in existential catastrophe. Current
alignment approaches struggle with value specification complexity and fail to
address emergent power-seeking behaviors. We propose "Corrigibility as a
Singular Target" (CAST)-designing FMs whose overriding objective is empowering
designated human principals to guide, correct, and control them. This paradigm
shift from static value-loading to dynamic human empowerment transforms
instrumental drives: self-preservation serves only to maintain the principal's
control; goal modification becomes facilitating principal guidance. We present
a comprehensive empirical research agenda spanning training methodologies
(RLAIF, SFT, synthetic data generation), scalability testing across model
sizes, and demonstrations of controlled instructability. Our vision: FMs that
become increasingly responsive to human guidance as capabilities grow, offering
a path to beneficial AI that remains as tool-like as possible, rather than
supplanting human judgment. This addresses the core alignment problem at its
source, preventing the default trajectory toward misaligned instrumental
convergence.

</details>


### [150] [DPO Learning with LLMs-Judge Signal for Computer Use Agents](https://arxiv.org/abs/2506.03095)
*Man Luo,David Cobbley,Xin Su,Shachar Rosenman,Vasudev Lal,Shao-Yen Tseng,Phillip Howard*

Main category: cs.AI

TL;DR: 该论文提出了一种本地运行的轻量级视觉语言模型，用于隐私保护和资源高效的GUI任务自动化，通过自动评估合成数据训练，性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有的计算机用户代理（CUA）通常依赖云计算，存在隐私和可扩展性问题，尤其是在个人设备上运行时。

Method: 采用LLM-as-Judge框架自动评估和筛选合成交互轨迹，生成高质量数据用于强化学习，无需人工标注。

Result: 在OS-World基准测试中，微调的本地模型性能优于现有基线。

Conclusion: 该方法为隐私、高效和通用的GUI代理提供了一条有前景的路径。

Abstract: Computer use agents (CUA) are systems that automatically interact with
graphical user interfaces (GUIs) to complete tasks. CUA have made significant
progress with the advent of large vision-language models (VLMs). However, these
agents typically rely on cloud-based inference with substantial compute
demands, raising critical privacy and scalability concerns, especially when
operating on personal devices. In this work, we take a step toward
privacy-preserving and resource-efficient agents by developing a lightweight
vision-language model that runs entirely on local machines. To train this
compact agent, we introduce an LLM-as-Judge framework that automatically
evaluates and filters synthetic interaction trajectories, producing
high-quality data for reinforcement learning without human annotation.
Experiments on the OS-World benchmark demonstrate that our fine-tuned local
model outperforms existing baselines, highlighting a promising path toward
private, efficient, and generalizable GUI agents.

</details>


### [151] [Generate, Not Recommend: Personalized Multimodal Content Generation](https://arxiv.org/abs/2506.01704)
*Jiongnan Liu,Zhicheng Dou,Ning Hu,Chenyan Xiong*

Main category: cs.AI

TL;DR: 该论文提出了一种超越传统内容过滤的推荐系统新范式，通过多模态生成模型直接为用户生成个性化内容（如图像），实验证明该方法能有效满足用户历史偏好和潜在兴趣。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统仅能过滤现有内容，无法生成新概念，限制了其满足用户需求的能力。为解决这一问题，论文探索直接生成个性化多模态内容的新方法。

Method: 利用任意到任意的大型多模态模型（LMMs），结合监督微调和在线强化学习策略进行训练，使其具备生成个性化内容的能力。

Result: 在两个基准数据集和用户研究中，生成的图像不仅符合用户历史偏好，还与潜在未来兴趣相关，验证了方法的有效性。

Conclusion: 该研究突破了传统推荐系统的限制，通过生成式方法实现了更全面的个性化服务，为推荐系统领域提供了新方向。

Abstract: To address the challenge of information overload from massive web contents,
recommender systems are widely applied to retrieve and present personalized
results for users. However, recommendation tasks are inherently constrained to
filtering existing items and lack the ability to generate novel concepts,
limiting their capacity to fully satisfy user demands and preferences. In this
paper, we propose a new paradigm that goes beyond content filtering and
selecting: directly generating personalized items in a multimodal form, such as
images, tailored to individual users. To accomplish this, we leverage
any-to-any Large Multimodal Models (LMMs) and train them in both supervised
fine-tuning and online reinforcement learning strategy to equip them with the
ability to yield tailored next items for users. Experiments on two benchmark
datasets and user study confirm the efficacy of the proposed method. Notably,
the generated images not only align well with users' historical preferences but
also exhibit relevance to their potential future interests.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [152] [Ubiquitous Symmetry at Critical Points Across Diverse Optimization Landscapes](https://arxiv.org/abs/2506.01959)
*Irmi Schneider*

Main category: cs.LG

TL;DR: 该论文探讨了对称性在更广泛空间中的实值损失函数中的表现，扩展了神经网络中的对称性研究，并引入新的对称性度量方法。


<details>
  <summary>Details</summary>
Motivation: 研究对称性在数学结构和优化问题中的作用，特别是在神经网络中观察到的损失函数对称性现象，并探索其在更广泛空间中的应用。

Method: 通过分析四种新案例（有限域上的投影情况、八面体图情况、完美匹配情况和粒子吸引情况），研究对称性现象，并引入新的对称性度量方法。

Result: 所有观察到的临界点均具有非平凡对称性，新度量方法揭示了之前方法未捕捉到的额外对称结构。

Conclusion: 对称性在广泛空间中的实值损失函数中普遍存在，新度量方法能更全面地揭示对称结构。

Abstract: Symmetry plays a crucial role in understanding the properties of mathematical
structures and optimization problems. Recent work has explored this phenomenon
in the context of neural networks, where the loss function is invariant under
column and row permutations of the network weights. It has been observed that
local minima exhibit significant symmetry with respect to the network weights
(invariance to row and column permutations). And moreover no critical point was
found that lacked symmetry. We extend this line of inquiry by investigating
symmetry phenomena in real-valued loss functions defined on a broader class of
spaces. We will introduce four more cases: the projective case over a finite
field, the octahedral graph case, the perfect matching case, and the particle
attraction case. We show that as in the neural network case, all the critical
points observed have non-trivial symmetry. Finally we introduce a new measure
of symmetry in the system and show that it reveals additional symmetry
structures not captured by the previous measure.

</details>


### [153] [Graph-Based Adversarial Domain Generalization with Anatomical Correlation Knowledge for Cross-User Human Activity Recognition](https://arxiv.org/abs/2506.01962)
*Xiaozhou Ye,Kevin I-Kai Wang*

Main category: cs.LG

TL;DR: 提出GNN-ADG方法，结合图神经网络和对抗学习，解决传感器人体活动识别中的跨用户泛化问题。


<details>
  <summary>Details</summary>
Motivation: 传感器人体活动识别系统面临跨用户差异的挑战，传统模型难以泛化不同用户的行为、传感器放置和数据分布。

Method: GNN-ADG利用图神经网络和对抗学习，通过三种解剖单元（互联单元、类似单元和侧向单元）融合空间关系，动态整合多视角信息。

Result: GNN-ADG无需目标用户数据即可学习到泛化性强的特征，适用于实际应用。

Conclusion: GNN-ADG通过统一图结构和对抗学习，有效解决了跨用户泛化问题，具有实际应用价值。

Abstract: Cross-user variability poses a significant challenge in sensor-based Human
Activity Recognition (HAR) systems, as traditional models struggle to
generalize across users due to differences in behavior, sensor placement, and
data distribution. To address this, we propose GNN-ADG (Graph Neural Network
with Adversarial Domain Generalization), a novel method that leverages both the
strength from both the Graph Neural Networks (GNNs) and adversarial learning to
achieve robust cross-user generalization. GNN-ADG models spatial relationships
between sensors on different anatomical body parts, extracting three types of
Anatomical Units: (1) Interconnected Units, capturing inter-relations between
neighboring sensors; (2) Analogous Units, grouping sensors on symmetrical or
functionally similar body parts; and (3) Lateral Units, connecting sensors
based on their position to capture region-specific coordination. These units
information are fused into an unified graph structure with a cyclic training
strategy, dynamically integrating spatial, functional, and lateral correlations
to facilitate a holistic, user-invariant representation. Information fusion
mechanism of GNN-ADG occurs by iteratively cycling through edge topologies
during training, allowing the model to refine its understanding of inter-sensor
relationships across diverse perspectives. By representing the spatial
configuration of sensors as an unified graph and incorporating adversarial
learning, Information Fusion GNN-ADG effectively learns features that
generalize well to unseen users without requiring target user data during
training, making it practical for real-world applications.

</details>


### [154] [Breaking Quadratic Barriers: A Non-Attention LLM for Ultra-Long Context Horizons](https://arxiv.org/abs/2506.01963)
*Andrew Kiruluta,Preethi Raju,Priscilla Burity*

Main category: cs.LG

TL;DR: 提出一种新型非注意力机制的大语言模型架构，通过线性复杂度组件处理超长上下文窗口。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer的自注意力机制存在二次方复杂度问题，难以处理超长序列。本文旨在设计高效处理数十万至百万量级token的模型架构。

Method: 结合状态空间块（S4启发）、多分辨率卷积层、轻量级循环监督器和检索增强外部记忆体，完全避免token间注意力计算。

Result: 模型实现接近线性的序列长度扩展能力，在保持性能的同时显著降低计算复杂度。

Conclusion: 该架构为超长上下文建模提供了可行的非注意力方案，突破了传统Transformer的长度限制。

Abstract: We present a novel non attention based architecture for large language models
(LLMs) that efficiently handles very long context windows, on the order of
hundreds of thousands to potentially millions of tokens. Unlike traditional
Transformer designs, which suffer from quadratic memory and computation
overload due to the nature of the self attention mechanism, our model avoids
token to token attention entirely. Instead, it combines the following
complementary components: State Space blocks (inspired by S4) that learn
continuous time convolution kernels and scale near linearly with sequence
length, Multi Resolution Convolution layers that capture local context at
different dilation levels, a lightweight Recurrent Supervisor to maintain a
global hidden state across sequential chunks, and Retrieval Augmented External
Memory that stores and retrieves high-level chunk embeddings without
reintroducing quadratic operations.

</details>


### [155] [A Data-Driven Approach to Enhancing Gravity Models for Trip Demand Prediction](https://arxiv.org/abs/2506.01964)
*Kamal Acharya,Mehul Lad,Liang Sun,Houbing Song*

Main category: cs.LG

TL;DR: 该研究通过整合地理、经济、社会和出行数据，利用机器学习技术改进了传统的重力模型，显著提升了交通出行预测的准确性和解释力。


<details>
  <summary>Details</summary>
Motivation: 传统的重力模型虽然简单易用，但难以准确反映现代出行行为的复杂性。为了提高交通规划的准确性，需要一种能够处理多变量复杂交互的数据驱动方法。

Method: 研究结合田纳西州和纽约州各县的地理、经济、社会和出行数据，采用机器学习技术对传统重力模型进行增强。

Result: 实验结果表明，机器学习增强的模型在R平方值上提升了51.48%，平均绝对误差（MAE）降低了63.59%，通勤者共同部分（CPC）增加了44.32%，显著提高了预测准确性和可靠性。

Conclusion: 通过整合多样化数据集和先进算法，该研究为城市规划者和政策制定者提供了更可靠的预测和决策工具，展示了数据驱动方法在交通模型中的巨大潜力。

Abstract: Accurate prediction of trips between zones is critical for transportation
planning, as it supports resource allocation and infrastructure development
across various modes of transport. Although the gravity model has been widely
used due to its simplicity, it often inadequately represents the complex
factors influencing modern travel behavior. This study introduces a data-driven
approach to enhance the gravity model by integrating geographical, economic,
social, and travel data from the counties in Tennessee and New York state.
Using machine learning techniques, we extend the capabilities of the
traditional model to handle more complex interactions between variables. Our
experiments demonstrate that machine learning-enhanced models significantly
outperform the traditional model. Our results show a 51.48% improvement in
R-squared, indicating a substantial enhancement in the model's explanatory
power. Also, a 63.59% reduction in Mean Absolute Error (MAE) reflects a
significant increase in prediction accuracy. Furthermore, a 44.32% increase in
Common Part of Commuters (CPC) demonstrates improved prediction reliability.
These findings highlight the substantial benefits of integrating diverse
datasets and advanced algorithms into transportation models. They provide urban
planners and policymakers with more reliable forecasting and decision-making
tools.

</details>


### [156] [TaskVAE: Task-Specific Variational Autoencoders for Exemplar Generation in Continual Learning for Human Activity Recognition](https://arxiv.org/abs/2506.01965)
*Bonpagna Kann,Sandra Castellanos-Paez,Romain Rombourg,Philippe Lalanda*

Main category: cs.LG

TL;DR: TaskVAE提出了一种基于变分自编码器的持续学习方法，通过生成任务特定合成样本解决类别增量学习中的遗忘问题，在人体活动识别任务中表现优异且内存占用低。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习系统日益融入日常生活，模型需要适应动态数据环境。传统持续学习方法受限于内存约束和旧知识遗忘问题，尤其在人体活动识别等现实场景中，个体用户数据的学习更具挑战性。

Method: 提出TaskVAE框架：为每个任务训练独立的变分自编码器生成合成样本，结合新任务数据训练分类器。无需预知总类别数，突破了传统方法依赖单一VAE或固定类别数的限制。

Result: 在5个HAR数据集上的实验表明：1)性能优于经验回放方法（小数据优势明显）2)内存占用极低（每任务仅需60样本）3)支持无限合成样本生成4)数据量增大时保持稳定表现。

Conclusion: TaskVAE通过任务特定生成、内存高效和长期稳定性的平衡，为人体活动识别等现实场景提供了可靠的持续学习解决方案，特别适合资源受限的移动设备应用。

Abstract: As machine learning based systems become more integrated into daily life,
they unlock new opportunities but face the challenge of adapting to dynamic
data environments. Various forms of data shift-gradual, abrupt, or
cyclic-threaten model accuracy, making continual adaptation essential.
Continual Learning (CL) enables models to learn from evolving data streams
while minimizing forgetting of prior knowledge. Among CL strategies,
replay-based methods have proven effective, but their success relies on
balancing memory constraints and retaining old class accuracy while learning
new classes. This paper presents TaskVAE, a framework for replay-based CL in
class-incremental settings. TaskVAE employs task-specific Variational
Autoencoders (VAEs) to generate synthetic exemplars from previous tasks, which
are then used to train the classifier alongside new task data. In contrast to
traditional methods that require prior knowledge of the total class count or
rely on a single VAE for all tasks, TaskVAE adapts flexibly to increasing tasks
without such constraints. We focus on Human Activity Recognition (HAR) using
IMU sensor-equipped devices. Unlike previous HAR studies that combine data
across all users, our approach focuses on individual user data, better
reflecting real-world scenarios where a person progressively learns new
activities. Extensive experiments on 5 different HAR datasets show that TaskVAE
outperforms experience replay methods, particularly with limited data, and
exhibits robust performance as dataset size increases. Additionally, memory
footprint of TaskVAE is minimal, being equivalent to only 60 samples per task,
while still being able to generate an unlimited number of synthetic samples.
The contributions lie in balancing memory constraints, task-specific
generation, and long-term stability, making it a reliable solution for
real-world applications in domains like HAR.

</details>


### [157] [Matrix Is All You Need](https://arxiv.org/abs/2506.01966)
*Yuzhou Zhu*

Main category: cs.LG

TL;DR: 该论文提出了一种统一的矩阵排序框架，将卷积、循环和自注意力操作视为稀疏矩阵乘法，证明了与标准CNN、RNN和Transformer层的代数同构，并在多个任务上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 当前深度神经网络在视觉、序列和语言任务中使用不同的专用架构，这种多样性掩盖了它们的共同点。论文旨在通过统一的数学框架揭示这些架构的共性。

Method: 引入了一个统一的矩阵排序框架，将卷积、循环和自注意力操作分别表示为上三角矩阵、下三角矩阵和三维张量分解的稀疏矩阵乘法。

Result: 在图像分类、时间序列预测和语言建模/分类任务上的实验表明，稀疏矩阵公式的性能与原生模型相当或更好，且收敛速度相当或更快。

Conclusion: 该工作为多样化的神经架构建立了数学上严谨的基础，并为硬件感知的网络设计开辟了新途径。

Abstract: Deep neural networks employ specialized architectures for vision, sequential
and language tasks, yet this proliferation obscures their underlying
commonalities. We introduce a unified matrix-order framework that casts
convolutional, recurrent and self-attention operations as sparse matrix
multiplications. Convolution is realized via an upper-triangular weight matrix
performing first-order transformations; recurrence emerges from a
lower-triangular matrix encoding stepwise updates; attention arises naturally
as a third-order tensor factorization. We prove algebraic isomorphism with
standard CNN, RNN and Transformer layers under mild assumptions. Empirical
evaluations on image classification (MNIST, CIFAR-10/100, Tiny ImageNet),
time-series forecasting (ETTh1, Electricity Load Diagrams) and language
modeling/classification (AG News, WikiText-2, Penn Treebank) confirm that
sparse-matrix formulations match or exceed native model performance while
converging in comparable or fewer epochs. By reducing architecture design to
sparse pattern selection, our matrix perspective aligns with GPU parallelism
and leverages mature algebraic optimization tools. This work establishes a
mathematically rigorous substrate for diverse neural architectures and opens
avenues for principled, hardware-aware network design.

</details>


### [158] [Turning LLM Activations Quantization-Friendly](https://arxiv.org/abs/2506.01967)
*Patrik Czakó,Gábor Kertész,Sándor Szénási*

Main category: cs.LG

TL;DR: 该论文研究了量化大型语言模型(LLMs)时激活值异常值对量化误差的影响，提出了一种基于通道幅度的新度量方法，并开发了一种结合通道缩放和旋转的混合量化方法。


<details>
  <summary>Details</summary>
Motivation: 量化可以降低LLMs的服务成本，但激活值中的显著异常值会增加量化误差，影响模型性能。论文旨在解决这一问题。

Method: 通过分析层间量化误差，研究平滑和旋转变换对数值的影响，提出基于通道幅度的新度量方法，并开发了先通道缩放后旋转的混合量化方法。

Result: 提出了一种有效测量量化难度的方法，并通过数学公式证明了混合量化方法的优势，能够更好地处理异常值问题。

Conclusion: 该研究为LLMs量化提供了新的分析工具和优化方法，有助于在保持模型性能的同时降低服务成本。

Abstract: Quantization effectively reduces the serving costs of Large Language Models
(LLMs) by speeding up data movement through compressed parameters and enabling
faster operations via integer arithmetic. However, activating integer
arithmetic requires quantizing both weights and activations, which poses
challenges due to the significant outliers in LLMs that increase quantization
error. In this work, we investigate these outliers with an emphasis on their
effect on layer-wise quantization error, then examine how smoothing and
rotation transform the observed values. Our primary contributions include
introducing a new metric to measure and visualize quantization difficulty based
on channel magnitudes, as well as proposing a hybrid approach that applies
channel-wise scaling before rotation, supported by a mathematical formulation
of its benefits.

</details>


### [159] [Efficient ANN-SNN Conversion with Error Compensation Learning](https://arxiv.org/abs/2506.01968)
*Chang Liu,Jiangrong Shen,Xuming Ran,Mingkun Xu,Qi Xu,Yi Xu,Gang Pan*

Main category: cs.LG

TL;DR: 该论文提出了一种基于误差补偿学习的ANN到SNN转换框架，通过自适应阈值、双阈值神经元和优化的膜电位初始化策略，显著减少了转换误差，实现了高精度和超低延迟的SNN模型。


<details>
  <summary>Details</summary>
Motivation: 人工神经网络（ANN）在资源受限环境中的部署面临高计算和内存需求的挑战，而脉冲神经网络（SNN）因其离散脉冲事件和高效能耗成为潜在替代方案。然而，当前的ANN到SNN转换方法常因剪裁、量化和激活不均匀等问题导致精度损失和推理时间增加。

Method: 论文提出了一种新颖的ANN到SNN转换框架，包括可学习的阈值剪裁函数、双阈值神经元和优化的膜电位初始化策略，以自适应减少剪裁误差、动态降低量化误差，并通过有效管理膜电位最小化非均匀误差。

Result: 在CIFAR-10、CIFAR-100和ImageNet数据集上的实验结果表明，该方法在现有转换方法中实现了高精度和超低延迟，仅用两个时间步就在ResNet-18结构下保持了CIFAR-10数据集上94.75%的竞争性准确率。

Conclusion: 该研究推动了SNN在低功耗硬件上的实际应用，为实现高效的实时处理提供了可能。

Abstract: Artificial neural networks (ANNs) have demonstrated outstanding performance
in numerous tasks, but deployment in resource-constrained environments remains
a challenge due to their high computational and memory requirements. Spiking
neural networks (SNNs) operate through discrete spike events and offer superior
energy efficiency, providing a bio-inspired alternative. However, current
ANN-to-SNN conversion often results in significant accuracy loss and increased
inference time due to conversion errors such as clipping, quantization, and
uneven activation. This paper proposes a novel ANN-to-SNN conversion framework
based on error compensation learning. We introduce a learnable threshold
clipping function, dual-threshold neurons, and an optimized membrane potential
initialization strategy to mitigate the conversion error. Together, these
techniques address the clipping error through adaptive thresholds, dynamically
reduce the quantization error through dual-threshold neurons, and minimize the
non-uniformity error by effectively managing the membrane potential.
Experimental results on CIFAR-10, CIFAR-100, ImageNet datasets show that our
method achieves high-precision and ultra-low latency among existing conversion
methods. Using only two time steps, our method significantly reduces the
inference time while maintains competitive accuracy of 94.75% on CIFAR-10
dataset under ResNet-18 structure. This research promotes the practical
application of SNNs on low-power hardware, making efficient real-time
processing possible.

</details>


### [160] [Johnny: Structuring Representation Space to Enhance Machine Abstract Reasoning Ability](https://arxiv.org/abs/2506.01970)
*Ruizhuo Song,Beiming Yuan*

Main category: cs.LG

TL;DR: 该论文针对AI在Raven渐进矩阵任务中抽象推理能力的局限性，提出Johnny架构和Spin-Transformer网络，通过改进表示空间和位置关系捕捉能力，显著提升了推理性能。


<details>
  <summary>Details</summary>
Motivation: 传统端到端RPM解决模型过度依赖选项池配置，限制了模型的推理能力，论文旨在突破这一限制，提升AI的抽象推理能力。

Method: 提出Johnny架构，包含表示提取模块和推理模块，通过补充负选项配置和表示空间学习增强推理能力；引入Spin-Transformer网络及其轻量级变体Straw Spin-Transformer，优化位置关系捕捉和计算效率。

Result: 实验证明，Johnny和Spin-Transformer在RPM任务中表现出色，提供了提升AI抽象推理能力的创新方法。

Conclusion: 论文提出的新架构和方法显著提升了AI在复杂抽象推理任务中的性能，为未来研究提供了有价值的参考。

Abstract: This paper thoroughly investigates the challenges of enhancing AI's abstract
reasoning capabilities, with a particular focus on Raven's Progressive Matrices
(RPM) tasks involving complex human-like concepts. Firstly, it dissects the
empirical reality that traditional end-to-end RPM-solving models heavily rely
on option pool configurations, highlighting that this dependency constrains the
model's reasoning capabilities. To address this limitation, the paper proposes
the Johnny architecture - a novel representation space-based framework for
RPM-solving. Through the synergistic operation of its Representation Extraction
Module and Reasoning Module, Johnny significantly enhances reasoning
performance by supplementing primitive negative option configurations with a
learned representation space. Furthermore, to strengthen the model's capacity
for capturing positional relationships among local features, the paper
introduces the Spin-Transformer network architecture, accompanied by a
lightweight Straw Spin-Transformer variant that reduces computational overhead
through parameter sharing and attention mechanism optimization. Experimental
evaluations demonstrate that both Johnny and Spin-Transformer achieve superior
performance on RPM tasks, offering innovative methodologies for advancing AI's
abstract reasoning capabilities.

</details>


### [161] [Traffic and Mobility Optimization Using AI: Comparative Study between Dubai and Riyadh](https://arxiv.org/abs/2506.01974)
*Kanwal Aalijah*

Main category: cs.LG

TL;DR: 该研究探讨如何利用AI分析交通拥堵问题及其对居民情绪的影响，结合实时交通数据和地理位置情感分析，为中东等地区提供优化交通流动的建议。


<details>
  <summary>Details</summary>
Motivation: 现代城市面临快速城市化带来的交通拥堵挑战，影响经济增长、生活质量和环境可持续性。研究旨在通过AI理解交通与居民情绪的关系。

Method: 结合实时交通数据和地理位置情感分析，使用AI模型和探索性数据分析预测拥堵模式、分析通勤行为，并识别拥堵热点和不满意区域。

Result: 研究提供了优化交通流动、提升通勤体验及解决特定城市交通问题的可行建议。

Conclusion: AI和情感分析的结合为城市交通规划提供了动态全面的解决方案，尤其适用于中东等地区的特定挑战。

Abstract: Urban planning plays a very important role in development modern cities. It
effects the economic growth, quality of life, and environmental sustainability.
Modern cities face challenges in managing traffic congestion. These challenges
arise to due to rapid urbanization. In this study we will explore how AI can be
used to understand the traffic and mobility related issues and its effects on
the residents sentiment. The approach combines real-time traffic data with
geo-located sentiment analysis, offering a comprehensive and dynamic approach
to urban mobility planning. AI models and exploratory data analysis was used to
predict traffic congestion patterns, analyze commuter behaviors, and identify
congestion hotspots and dissatisfaction zones. The findings offer actionable
recommendations for optimizing traffic flow, enhancing commuter experiences,
and addressing city specific mobility challenges in the Middle East and beyond.

</details>


### [162] [An empirical study of task and feature correlations in the reuse of pre-trained models](https://arxiv.org/abs/2506.01975)
*Jama Hussein Mohamud*

Main category: cs.LG

TL;DR: 该论文探讨了预训练神经网络在任务迁移中的效果，发现任务相关性、网络结构和优化器选择是影响迁移成功的关键因素。


<details>
  <summary>Details</summary>
Motivation: 研究预训练神经网络在不同任务间迁移时，哪些因素导致了迁移的成功，特别是在任务相关性较低的情况下。

Method: 通过实验设置，分析任务相关性、网络层选择和优化器对迁移效果的影响。

Result: 任务相关性越高，迁移效果越好；即使任务不相关，预训练网络和优化器选择仍能带来显著提升；低层网络重用在不相关任务中更有效。

Conclusion: 任务语义相关性是预训练网络迁移成功的关键，且网络层选择和优化器设计对迁移效果有重要影响。

Abstract: Pre-trained neural networks are commonly used and reused in the machine
learning community. Alice trains a model for a particular task, and a part of
her neural network is reused by Bob for a different task, often to great
effect. To what can we ascribe Bob's success? This paper introduces an
experimental setup through which factors contributing to Bob's empirical
success could be studied in silico. As a result, we demonstrate that Bob might
just be lucky: his task accuracy increases monotonically with the correlation
between his task and Alice's. Even when Bob has provably uncorrelated tasks and
input features from Alice's pre-trained network, he can achieve significantly
better than random performance due to Alice's choice of network and optimizer.
When there is little correlation between tasks, only reusing lower pre-trained
layers is preferable, and we hypothesize the converse: that the optimal number
of retrained layers is indicative of task and feature correlation. Finally, we
show in controlled real-world scenarios that Bob can effectively reuse Alice's
pre-trained network if there are semantic correlations between his and Alice's
task.

</details>


### [163] [Crack Path Prediction with Operator Learning using Discrete Particle System data Generation](https://arxiv.org/abs/2506.01976)
*Elham Kiyani,Venkatesh Ananchaperumal,Ahmad Peyvan,Mahendaran Uchimali,Gang Li,George Em Karniadakis*

Main category: cs.LG

TL;DR: 该研究利用基于本构行为的离散粒子系统数据训练DeepONet模型，预测不同几何形状样本中的裂纹扩展，发现Fusion DeepONet在非断裂情况下表现更优。


<details>
  <summary>Details</summary>
Motivation: 准确模拟裂纹扩展对预测工程材料和结构失效至关重要，尤其是裂纹与孔洞等不连续性的相互作用会显著影响裂纹偏转和止裂。

Method: 使用Constitutively Informed Particle Dynamics (CPD)模拟数据训练DeepONet模型，包括vanilla和Fusion DeepONet两种变体，研究三种代表性案例。

Result: Fusion DeepONet在非断裂情况下预测更准确，但在涉及位移和裂纹扩展的断裂驱动场景中仍具挑战性。

Conclusion: Fusion DeepONet在复杂、几何变化和时间依赖的裂纹扩展现象中展现出良好的泛化潜力。

Abstract: Accurately modeling crack propagation is critical for predicting failure in
engineering materials and structures, where small cracks can rapidly evolve and
cause catastrophic damage. The interaction of cracks with discontinuities, such
as holes, significantly affects crack deflection and arrest. Recent
developments in discrete particle systems with multibody interactions based on
constitutive behavior have demonstrated the ability to capture crack nucleation
and evolution without relying on continuum assumptions. In this work, we use
data from Constitutively Informed Particle Dynamics (CPD) simulations to train
operator learning models, specifically Deep Operator Networks (DeepONets),
which learn mappings between function spaces instead of finite-dimensional
vectors. We explore two DeepONet variants: vanilla and Fusion DeepONet, for
predicting time-evolving crack propagation in specimens with varying
geometries. Three representative cases are studied: (i) varying notch height
without active fracture; and (ii) and (iii) combinations of notch height and
hole radius where dynamic fracture occurs on irregular discrete meshes. The
models are trained on 32 to 45 samples, using geometric inputs in the branch
network and spatial-temporal coordinates in the trunk network. Results show
that Fusion DeepONet consistently outperforms the vanilla variant, with more
accurate predictions especially in non-fracturing cases. Fracture-driven
scenarios involving displacement and crack evolution remain more challenging.
These findings highlight the potential of Fusion DeepONet to generalize across
complex, geometry-varying, and time-dependent crack propagation phenomena.

</details>


### [164] [Towards Unsupervised Training of Matching-based Graph Edit Distance Solver via Preference-aware GAN](https://arxiv.org/abs/2506.01977)
*Wei Huang,Hanchen Wang,Dong Wen,Shaozhen Ma,Wenjie Zhang,Xuemin Lin*

Main category: cs.LG

TL;DR: 提出了一种名为GEDRanker的无监督GAN框架，用于计算图编辑距离（GED），无需依赖昂贵的真实标签。


<details>
  <summary>Details</summary>
Motivation: 现有的GED计算方法通常依赖真实标签，但这些标签在现实场景中获取成本高昂。因此，需要一种无监督的方法来高效计算GED。

Method: GEDRanker结合了基于匹配的GED求解器和一个可解释的偏好感知判别器，通过有效的训练策略生成高质量的节点匹配，无需真实标签。

Result: 在基准数据集上的实验表明，GEDRanker能够在无监督的情况下生成接近最优的GED解决方案。

Conclusion: GEDRanker为无监督GED计算提供了一种高效且可解释的方法，显著降低了对真实标签的依赖。

Abstract: Graph Edit Distance (GED) is a fundamental graph similarity metric widely
used in various applications. However, computing GED is an NP-hard problem.
Recent state-of-the-art hybrid GED solver has shown promising performance by
formulating GED as a bipartite graph matching problem, then leveraging a
generative diffusion model to predict node matching between two graphs, from
which both the GED and its corresponding edit path can be extracted using a
traditional algorithm. However, such methods typically rely heavily on
ground-truth supervision, where the ground-truth labels are often costly to
obtain in real-world scenarios. In this paper, we propose GEDRanker, a novel
unsupervised GAN-based framework for GED computation. Specifically, GEDRanker
consists of a matching-based GED solver and introduces an interpretable
preference-aware discriminator with an effective training strategy to guide the
matching-based GED solver toward generating high-quality node matching without
the need for ground-truth labels. Extensive experiments on benchmark datasets
demonstrate that our GEDRanker enables the matching-based GED solver to achieve
near-optimal solution quality without any ground-truth supervision.

</details>


### [165] [Improvement of AMPs Identification with Generative Adversarial Network and Ensemble Classification](https://arxiv.org/abs/2506.01983)
*Reyhaneh Keshavarzpour,Eghbal Mansoori*

Main category: cs.LG

TL;DR: 该论文提出了一种改进的抗菌肽预测方法，通过结合最佳编码方法和深度神经网络处理不平衡数据集，显著提高了预测准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 抗菌肽作为抗生素的替代品在生物医学和其他应用领域具有重要意义，但识别抗菌肽是一个重要且必要的问题。人工智能算法在此领域的应用可以简化识别过程。

Method: 通过结合不同视角的最佳编码方法，并使用深度神经网络处理不平衡的组合数据集，改进了抗菌肽的预测方法。

Result: 研究结果表明，所提出的方法在抗菌肽预测的准确性和效率上有显著提升，优于现有方法。

Conclusion: 该研究在抗菌肽预测和分类领域的发展，特别是在医药和制药行业，具有高度的有效性和应用价值。

Abstract: Identification of antimicrobial peptides is an important and necessary issue
in today's era. Antimicrobial peptides are essential as an alternative to
antibiotics for biomedical applications and many other practical applications.
These oligopeptides are useful in drug design and cause innate immunity against
microorganisms. Artificial intelligence algorithms have played a significant
role in the ease of identifying these peptides.This research is improved by
improving proposed method in the field of antimicrobial peptides prediction.
Suggested method is improved by combining the best coding method from different
perspectives, In the following a deep neural network to balance the imbalanced
combined datasets. The results of this research show that the proposed method
have a significant improvement in the accuracy and efficiency of the prediction
of antimicrobial peptides and are able to provide the best results compared to
the existing methods. These development in the field of prediction and
classification of antimicrobial peptides, basically in the fields of medicine
and pharmaceutical industries, have high effectiveness and application.

</details>


### [166] [SpecMemo: Speculative Decoding is in Your Pocket](https://arxiv.org/abs/2506.01986)
*Selin Yildirim,Deming Chen*

Main category: cs.LG

TL;DR: SpecMemo是一种设备感知的推理引擎，通过精细控制内存分配，在内存受限设备上实现推测解码，保持高性能的同时大幅降低内存占用。


<details>
  <summary>Details</summary>
Motivation: 推测解码虽能加速大语言模型任务，但在内存受限设备（如移动GPU）上部署仍面临挑战，需要解决内存分配问题。

Method: 通过理论建模推测解码的内存占用，确定内存预算下限，并在减少冗余内存分配与保持推测性能之间取得平衡。

Result: 在单块Nvidia Titan RTX上，SpecMemo减少65%生成内存，保持96%推测解码吞吐量；在八块AMD MI250 GPU上，分布式批处理推测解码实现2倍加速，批量大小为10时吞吐量提升8倍。

Conclusion: SpecMemo为资源受限环境中的LLM应用提供了高效部署方案，实现了更快、更经济的实际应用部署。

Abstract: Recent advancements in speculative decoding have demonstrated considerable
speedup across a wide array of large language model (LLM) tasks. Speculative
decoding inherently relies on sacrificing extra memory allocations to generate
several candidate tokens, of which acceptance rate drives the speedup. However,
deploying speculative decoding on memory-constrained devices, such as mobile
GPUs, remains as a significant challenge in real-world scenarios. In this work,
we present a device-aware inference engine named SpecMemo that can smartly
control memory allocations at finer levels to enable multi-turn chatbots with
speculative decoding on such limited memory devices. Our methodology stems from
theoretically modeling memory footprint of speculative decoding to determine a
lower bound on the required memory budget while retaining speedup. SpecMemo
empirically acquires a careful balance between minimizing redundant memory
allocations for rejected candidate tokens and maintaining competitive
performance gains from speculation. Notably, with SpecMemo's memory management,
we maintain 96% of overall throughput from speculative decoding on MT-Bench,
with reduced generation-memory by 65% on single Nvidia Titan RTX. Given
multiple constrained GPUs, we build on top of previous speculative decoding
architectures to facilitate big-model inference by distributing
Llama-2-70B-Chat model, on which we provide novel batched speculative decoding
to increase usability of multiple small server GPUs. This novel framework
demonstrates 2x speedup over distributed and batched vanilla decoding with the
base model on eight AMD MI250 GPUs. Moreover, inference throughput increases
remarkably 8x with batch size 10. Our work contributes to democratized LLM
applications in resource-constrained environments, providing a pathway for
faster and cheaper deployment of real-world LLM applications with robust
performance.

</details>


### [167] [Equally Critical: Samples, Targets, and Their Mappings in Datasets](https://arxiv.org/abs/2506.01987)
*Runkang Yang,Peng Sun,Xinyi Shang,Yi Tang,Tao Lin*

Main category: cs.LG

TL;DR: 本文探讨了数据中样本与目标的相互作用对训练动态的影响，提出了一种新的统一损失框架，并通过实验提供了六项关键见解以提高训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究在数据高效学习中主要关注样本优化（如数据集蒸馏），而忽视了目标的关键作用。这种样本与目标的割裂促使作者研究两者如何共同影响训练动态。

Method: 作者首先从样本-目标交互的角度建立了现有范式的分类体系，然后提出了一种新的统一损失框架来评估它们对训练效率的影响。

Result: 通过大量实验，作者全面分析了目标与样本的类型、数量和质量变化对模型训练的影响，并提出了六项提高训练效率的关键见解。

Conclusion: 研究表明，同时考虑样本和目标的相互作用对优化训练动态至关重要，提出的统一框架为未来研究提供了新的方向。

Abstract: Data inherently possesses dual attributes: samples and targets. For targets,
knowledge distillation has been widely employed to accelerate model
convergence, primarily relying on teacher-generated soft target supervision.
Conversely, recent advancements in data-efficient learning have emphasized
sample optimization techniques, such as dataset distillation, while neglected
the critical role of target. This dichotomy motivates our investigation into
understanding how both sample and target collectively influence training
dynamic. To address this gap, we first establish a taxonomy of existing
paradigms through the lens of sample-target interactions, categorizing them
into distinct sample-to-target mapping strategies. Building upon this
foundation, we then propose a novel unified loss framework to assess their
impact on training efficiency. Through extensive empirical studies on our
proposed strategies, we comprehensively analyze how variations in target and
sample types, quantities, and qualities influence model training, providing six
key insights to enhance training efficacy.

</details>


### [168] [Surrogate Interpretable Graph for Random Decision Forests](https://arxiv.org/abs/2506.01988)
*Akshat Dubey,Aleksandar Anžel,Georges Hattab*

Main category: cs.LG

TL;DR: 随机森林模型在健康信息学中提升了特征交互的可解释性，但特征和估计器数量增加会影响专家解读。提出替代可解释性图方法，通过图和混合整数线性编程增强全局可解释性。


<details>
  <summary>Details</summary>
Motivation: 随机森林模型虽在健康信息学中表现出色，但随着特征和估计器数量增加，专家难以准确解读全局特征交互，影响信任和合规性。

Method: 开发替代可解释性图方法，利用图和混合整数线性编程分析并可视化特征交互，展示决策特征交互表和主导层次决策特征交互。

Result: 替代可解释性图提升了全局可解释性，有助于高风险领域的决策透明度和合规性。

Conclusion: 替代可解释性图方法有效解决了随机森林模型在健康信息学中的可解释性问题，增强了专家对模型的理解和信任。

Abstract: The field of health informatics has been profoundly influenced by the
development of random forest models, which have led to significant advances in
the interpretability of feature interactions. These models are characterized by
their robustness to overfitting and parallelization, making them particularly
useful in this domain. However, the increasing number of features and
estimators in random forests can prevent domain experts from accurately
interpreting global feature interactions, thereby compromising trust and
regulatory compliance. A method called the surrogate interpretability graph has
been developed to address this issue. It uses graphs and mixed-integer linear
programming to analyze and visualize feature interactions. This improves their
interpretability by visualizing the feature usage per
decision-feature-interaction table and the most dominant hierarchical decision
feature interactions for predictions. The implementation of a surrogate
interpretable graph enhances global interpretability, which is critical for
such a high-stakes domain.

</details>


### [169] [Coded Robust Aggregation for Distributed Learning under Byzantine Attacks](https://arxiv.org/abs/2506.01989)
*Chengxi Li,Ming Xiao,Mikael Skoglund*

Main category: cs.LG

TL;DR: 论文提出了一种基于编码鲁棒聚合（CRA-DL）的新方法，以应对分布式学习中的拜占庭攻击问题，通过冗余数据分配和编码梯度传输提升学习性能。


<details>
  <summary>Details</summary>
Motivation: 当前分布式学习方法在面对拜占庭攻击时，由于不同设备的本地梯度差异较大，导致学习性能显著下降，亟需一种更鲁棒的方法来应对这一问题。

Method: 在训练开始前，将训练数据冗余分配给设备；训练过程中，诚实设备传输编码梯度，服务器使用鲁棒聚合规则聚合信息，近似恢复全局梯度以更新模型。

Result: 理论分析和数值结果表明，CRA-DL方法在拜占庭攻击下具有更优的学习性能，显著优于现有基线方法。

Conclusion: CRA-DL方法通过编码梯度使诚实设备的信息更接近，增强了聚合的鲁棒性，有效提升了分布式学习在拜占庭攻击下的性能。

Abstract: In this paper, we investigate the problem of distributed learning (DL) in the
presence of Byzantine attacks. For this problem, various robust bounded
aggregation (RBA) rules have been proposed at the central server to mitigate
the impact of Byzantine attacks. However, current DL methods apply RBA rules
for the local gradients from the honest devices and the disruptive information
from Byzantine devices, and the learning performance degrades significantly
when the local gradients of different devices vary considerably from each
other. To overcome this limitation, we propose a new DL method to cope with
Byzantine attacks based on coded robust aggregation (CRA-DL). Before training
begins, the training data are allocated to the devices redundantly. During
training, in each iteration, the honest devices transmit coded gradients to the
server computed from the allocated training data, and the server then
aggregates the information received from both honest and Byzantine devices
using RBA rules. In this way, the global gradient can be approximately
recovered at the server to update the global model. Compared with current DL
methods applying RBA rules, the improvement of CRA-DL is attributed to the fact
that the coded gradients sent by the honest devices are closer to each other.
This closeness enhances the robustness of the aggregation against Byzantine
attacks, since Byzantine messages tend to be significantly different from those
of honest devices in this case. We theoretically analyze the convergence
performance of CRA-DL. Finally, we present numerical results to verify the
superiority of the proposed method over existing baselines, showing its
enhanced learning performance under Byzantine attacks.

</details>


### [170] [Decoupled Hierarchical Reinforcement Learning with State Abstraction for Discrete Grids](https://arxiv.org/abs/2506.02050)
*Qingyu Xiao,Yuanlin Chang,Youtian Du*

Main category: cs.LG

TL;DR: 该论文提出了一种解耦分层强化学习框架DcHRL-SA，通过状态抽象和双层级策略提升离散状态空间下的探索效率。


<details>
  <summary>Details</summary>
Motivation: 在部分可观测的复杂离散状态空间中，智能体的有效探索仍是强化学习的核心挑战。

Method: 采用双层级架构（高层RL策略+低层规则策略）结合状态抽象方法，降低状态维度。

Result: 实验表明，该方法在探索效率、收敛速度、累积奖励和策略稳定性上均优于PPO算法。

Conclusion: 该框架为大规模离散空间探索提供了一种实用的分层策略与状态抽象结合方案。

Abstract: Effective agent exploration remains a core challenge in reinforcement
learning (RL) for complex discrete state-space environments, particularly under
partial observability. This paper presents a decoupled hierarchical RL
framework integrating state abstraction (DcHRL-SA) to address this issue. The
proposed method employs a dual-level architecture, consisting of a high level
RL-based actor and a low-level rule-based policy, to promote effective
exploration. Additionally, state abstraction method is incorporated to cluster
discrete states, effectively lowering state dimensionality. Experiments
conducted in two discrete customized grid environments demonstrate that the
proposed approach consistently outperforms PPO in terms of exploration
efficiency, convergence speed, cumulative reward, and policy stability. These
results demonstrate a practical approach for integrating decoupled hierarchical
policies and state abstraction in discrete grids with large-scale exploration
space. Code will be available at https://github.com/XQY169/DcHRL-SA.

</details>


### [171] [Generalization Performance of Ensemble Clustering: From Theory to Algorithm](https://arxiv.org/abs/2506.02053)
*Xu Zhang,Haoye Qiu,Weixuan Liang,Hui Liu,Junhui Hou,Yuheng Jia*

Main category: cs.LG

TL;DR: 该论文研究了集成聚类的泛化性能，推导了泛化误差和超额风险的收敛速率，证明了当样本数和基聚类数趋于无穷时集成聚类的理论一致性，并提出了一种新的集成聚类算法，在多个数据集上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 集成聚类在实践中表现出色，但其理论基础尚未充分探索。论文旨在填补这一空白，研究集成聚类的泛化性能，包括泛化误差、超额风险和一致性。

Method: 论文推导了泛化误差和超额风险的收敛速率，证明了集成聚类的理论一致性。针对实际中样本数和基聚类数有限的情况，通过赋予不同权重最小化经验平均聚类与期望的误差，并提出了最大化基聚类间差异（多样性）的优化模型。

Result: 论文提出的新集成聚类算法在10个数据集上相比现有最优方法，NMI、ARI和Purity指标平均分别提升了6.1%、7.3%和6.0%。

Conclusion: 论文通过理论分析和实验验证，表明在集成聚类中，最小化基聚类的偏差和最大化其多样性是提升性能的关键，并提出了相应的优化模型和新算法。

Abstract: Ensemble clustering has demonstrated great success in practice; however, its
theoretical foundations remain underexplored. This paper examines the
generalization performance of ensemble clustering, focusing on generalization
error, excess risk and consistency. We derive a convergence rate of
generalization error bound and excess risk bound both of
$\mathcal{O}(\sqrt{\frac{\log n}{m}}+\frac{1}{\sqrt{n}})$, with $n$ and $m$
being the numbers of samples and base clusterings. Based on this, we prove that
when $m$ and $n$ approach infinity and $m$ is significantly larger than log
$n$, i.e., $m,n\to \infty, m\gg \log n$, ensemble clustering is consistent.
Furthermore, recognizing that $n$ and $m$ are finite in practice, the
generalization error cannot be reduced to zero. Thus, by assigning varying
weights to finite clusterings, we minimize the error between the empirical
average clusterings and their expectation. From this, we theoretically
demonstrate that to achieve better clustering performance, we should minimize
the deviation (bias) of base clustering from its expectation and maximize the
differences (diversity) among various base clusterings. Additionally, we derive
that maximizing diversity is nearly equivalent to a robust (min-max)
optimization model. Finally, we instantiate our theory to develop a new
ensemble clustering algorithm. Compared with SOTA methods, our approach
achieves average improvements of 6.1%, 7.3%, and 6.0% on 10 datasets w.r.t.
NMI, ARI, and Purity. The code is available at https://github.com/xuz2019/GPEC.

</details>


### [172] [Predicting Blood Type: Assessing Model Performance with ROC Analysis](https://arxiv.org/abs/2506.02062)
*Malik A. Altayar,Muhyeeddin Alqaraleh,Mowafaq Salem Alzboon,Wesam T. Almagharbeh*

Main category: cs.LG

TL;DR: 研究探讨指纹模式与ABO血型分类之间的关系，未发现显著相关性，但强调未来研究需扩大样本并采用机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统生物识别技术如DNA分析和虹膜扫描虽准确但耗时昂贵，研究旨在探索指纹模式与血型之间的潜在关联，以简化个人身份识别。

Method: 分析200名个体的指纹（分为环状、涡状和弓状）及血型数据，使用卡方检验和皮尔逊相关性检验进行统计分析。

Result: 环状指纹最常见，O+血型占比最高，但统计显示指纹模式与血型无显著相关性（p > 0.05）。

Conclusion: 尽管未发现显著关联，研究强调了未来需扩大样本多样性、结合机器学习及多生物特征信号的重要性，为法医学身份鉴定提供参考。

Abstract: Introduction: Personal identification is a critical aspect of forensic
sciences, security, and healthcare. While conventional biometrics systems such
as DNA profiling and iris scanning offer high accuracy, they are time-consuming
and costly. Objectives: This study investigates the relationship between
fingerprint patterns and ABO blood group classification to explore potential
correlations between these two traits. Methods: The study analyzed 200
individuals, categorizing their fingerprints into three types: loops, whorls,
and arches. Blood group classification was also recorded. Statistical analysis,
including chi-square and Pearson correlation tests, was used to assess
associations between fingerprint patterns and blood groups. Results: Loops were
the most common fingerprint pattern, while blood group O+ was the most
prevalent among the participants. Statistical analysis revealed no significant
correlation between fingerprint patterns and blood groups (p > 0.05),
suggesting that these traits are independent. Conclusions: Although the study
showed limited correlation between fingerprint patterns and ABO blood groups,
it highlights the importance of future research using larger and more diverse
populations, incorporating machine learning approaches, and integrating
multiple biometric signals. This study contributes to forensic science by
emphasizing the need for rigorous protocols and comprehensive investigations in
personal identification.

</details>


### [173] [EWGN: Elastic Weight Generation and Context Switching in Deep Learning](https://arxiv.org/abs/2506.02065)
*Shriraj P. Sawant,Krishna P. Miyapuram*

Main category: cs.LG

TL;DR: 论文提出弹性权重生成网络（EWGN），通过动态生成权重实现任务间上下文切换，以缓解神经网络中的灾难性遗忘问题，并在标准视觉数据集上验证了其持续学习性能。


<details>
  <summary>Details</summary>
Motivation: 受人类广泛学习与保持任务能力启发，研究旨在通过持续学习解决神经网络中任务多变性和上下文切换导致的灾难性遗忘问题。

Method: 提出EWGN架构：利用辅助网络动态生成主网络权重，输入依赖的权重生成机制实现上下文切换，结合SGD和弹性权重巩固算法进行实验。

Result: 在MNIST和Fashion-MNIST数据集上，EWGN在全连接网络和卷积网络中有效保留了已学习任务的表征。

Conclusion: 动态权重生成与上下文切换能力可提升持续学习性能，为通用人工智能发展提供新思路。

Abstract: The ability to learn and retain a wide variety of tasks is a hallmark of
human intelligence that has inspired research in artificial general
intelligence. Continual learning approaches provide a significant step towards
achieving this goal. It has been known that task variability and context
switching are challenging for learning in neural networks. Catastrophic
forgetting refers to the poor performance on retention of a previously learned
task when a new task is being learned. Switching between different task
contexts can be a useful approach to mitigate the same by preventing the
interference between the varying task weights of the network. This paper
introduces Elastic Weight Generative Networks (EWGN) as an idea for context
switching between two different tasks. The proposed EWGN architecture uses an
additional network that generates the weights of the primary network
dynamically while consolidating the weights learned. The weight generation is
input-dependent and thus enables context switching. Using standard computer
vision datasets, namely MNIST and fashion-MNIST, we analyse the retention of
previously learned task representations in Fully Connected Networks,
Convolutional Neural Networks, and EWGN architectures with Stochastic Gradient
Descent and Elastic Weight Consolidation learning algorithms. Understanding
dynamic weight generation and context-switching ability can be useful in
enabling continual learning for improved performance.

</details>


### [174] [An Introduction to Flow Matching and Diffusion Models](https://arxiv.org/abs/2506.02070)
*Peter Holderrieth,Ezra Erives*

Main category: cs.LG

TL;DR: 该摘要介绍了基于扩散和流模型的生成AI技术，涵盖了多种数据模态，并提供了相关课程笔记和实验内容。


<details>
  <summary>Details</summary>
Motivation: 旨在为学习者和实践者提供一个系统的理论框架和实践指南，以理解和应用最先进的生成AI技术。

Method: 课程笔记从常微分方程和随机微分方程入手，逐步介绍流匹配、分数匹配、无分类器引导等现代模型的核心技术。

Result: 通过课程和实验，学生和实践者能够掌握生成AI的理论基础和实践技能。

Conclusion: 该课程笔记和配套内容为生成AI的学习和应用提供了全面且实用的资源。

Abstract: Diffusion and flow-based models have become the state of the art for
generative AI across a wide range of data modalities, including images, videos,
shapes, molecules, music, and more! These notes are originally from
https://diffusion.csail.mit.edu/, as taught at MIT over the 2025 IAP (winter)
term, and are intended to accompany other course content, including lectures
and labs. Overall, they function as a self-contained introduction to both flow
matching and diffusion models, starting with ordinary and stochastic
differential equations, and culminating in flow matching, score matching,
classifier-free guidance, and the inner workings of modern, state-of-the-art
models for image and video. These notes, and the accompanying course, are ideal
for students and practitioners alike who want to develop a principled
understanding of the theory and practice of generative AI.

</details>


### [175] [Assigning Distinct Roles to Quantized and Low-Rank Matrices Toward Optimal Weight Decomposition](https://arxiv.org/abs/2506.02077)
*Yoonjun Cho,Soeun Kim,Dongjae Jeon,Kyelim Lee,Beomsoo Lee,Albert No*

Main category: cs.LG

TL;DR: 论文提出ODLRI方法，通过结构化分解权重矩阵（量化+低秩）来优化大语言模型压缩效果，在低比特设置下提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有联合优化方法在量化和低秩近似间交替迭代时，往往偏重单一组件，导致分解效果不佳，无法充分发挥各自优势。

Method: 引入ODLRI（异常值驱动的低秩初始化），明确低秩组件负责捕捉激活敏感权重，结构化分解以减轻异常值对量化的负面影响。

Result: 在Llama2、Llama3-8B和Mistral-7B上的实验表明，ODLRI能持续降低激活感知误差、减小量化规模，并提升低比特下的困惑度和零样本准确率。

Conclusion: ODLRI通过明确组件分工实现了量化与低秩近似的更优平衡，有效提升了大模型压缩性能。

Abstract: Decomposing weight matrices into quantization and low-rank components
($\mathbf{W} \approx \mathbf{Q} + \mathbf{L}\mathbf{R}$) is a widely used
technique for compressing large language models (LLMs). Existing joint
optimization methods iteratively alternate between quantization and low-rank
approximation. However, these methods tend to prioritize one component at the
expense of the other, resulting in suboptimal decompositions that fail to
leverage each component's unique strengths. In this work, we introduce
Outlier-Driven Low-Rank Initialization (ODLRI), which assigns low-rank
components the specific role of capturing activation-sensitive weights. This
structured decomposition mitigates outliers' negative impact on quantization,
enabling more effective balance between quantization and low-rank
approximation. Experiments on Llama2 (7B, 13B, 70B), Llama3-8B, and Mistral-7B
demonstrate that incorporating ODLRI into the joint optimization framework
consistently reduces activation-aware error, minimizes quantization scale, and
improves perplexity and zero-shot accuracy in low-bit settings.

</details>


### [176] [Robust Federated Learning against Noisy Clients via Masked Optimization](https://arxiv.org/abs/2506.02079)
*Xuefeng Jiang,Tian Wen,Zhiqin Yang,Lvhua Wu,Yufeng Chen,Sheng Sun,Yuwei Wang,Min Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种名为MaskedOptim的两阶段优化框架，旨在解决联邦学习中的复杂标签噪声问题，通过检测高噪声客户端并纠正其标签，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在隐私敏感应用中取得进展，但参与者提供的标注数据可能存在复杂标签噪声，影响模型性能。高噪声客户端是性能下降的主要原因，因此需要开发有效的优化策略来缓解这一问题。

Method: MaskedOptim框架分为两个阶段：第一阶段检测高噪声率的客户端；第二阶段通过端到端标签校正机制纠正噪声客户端数据的标签，并采用基于几何中值的模型聚合增强训练鲁棒性。

Result: 在三个图像数据集和一个文本数据集上的实验表明，该框架在不同场景下表现出鲁棒性，且标签校正机制有效提升了噪声客户端本地数据集的数据质量。

Conclusion: MaskedOptim框架能够有效解决联邦学习中的标签噪声问题，提升模型性能，并通过开源代码促进相关研究。

Abstract: In recent years, federated learning (FL) has made significant advance in
privacy-sensitive applications. However, it can be hard to ensure that FL
participants provide well-annotated data for training. The corresponding
annotations from different clients often contain complex label noise at varying
levels. This label noise issue has a substantial impact on the performance of
the trained models, and clients with greater noise levels can be largely
attributed for this degradation. To this end, it is necessary to develop an
effective optimization strategy to alleviate the adverse effects of these noisy
clients.In this study, we present a two-stage optimization framework,
MaskedOptim, to address this intricate label noise problem. The first stage is
designed to facilitate the detection of noisy clients with higher label noise
rates. The second stage focuses on rectifying the labels of the noisy clients'
data through an end-to-end label correction mechanism, aiming to mitigate the
negative impacts caused by misinformation within datasets. This is achieved by
learning the potential ground-truth labels of the noisy clients' datasets via
backpropagation. To further enhance the training robustness, we apply the
geometric median based model aggregation instead of the commonly-used vanilla
averaged model aggregation. We implement sixteen related methods and conduct
evaluations on three image datasets and one text dataset with diverse label
noise patterns for a comprehensive comparison. Extensive experimental results
indicate that our proposed framework shows its robustness in different
scenarios. Additionally, our label correction framework effectively enhances
the data quality of the detected noisy clients' local datasets. % Our codes
will be open-sourced to facilitate related research communities. Our codes are
available via https://github.com/Sprinter1999/MaskedOptim .

</details>


### [177] [RATFM: Retrieval-augmented Time Series Foundation Model for Anomaly Detection](https://arxiv.org/abs/2506.02081)
*Chihiro Maru,Shoetsu Sato*

Main category: cs.LG

TL;DR: 提出检索增强时间序列基础模型（RATFM），使预训练模型能结合测试时适应样本，在异常检测任务中达到接近领域微调的效果。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型在不同领域和任务中表现不稳定，且无法像LLM那样通过示例提示进行测试时适应，限制了其在异常检测中的应用。

Method: 通过检索增强框架（RATFM）将目标域正常样本作为示例整合到预训练模型中，避免领域依赖的微调。

Result: 在包含9个领域的UCR异常检测数据集上，RATFM性能接近领域微调模型。

Conclusion: RATFM为时间序列基础模型提供了无需微调的测试时适应能力，在多领域异常检测中表现优异。

Abstract: Inspired by the success of large language models (LLMs) in natural language
processing, recent research has explored the building of time series foundation
models and applied them to tasks such as forecasting, classification, and
anomaly detection. However, their performances vary between different domains
and tasks. In LLM-based approaches, test-time adaptation using example-based
prompting has become common, owing to the high cost of retraining. In the
context of anomaly detection, which is the focus of this study, providing
normal examples from the target domain can also be effective. However, time
series foundation models do not naturally acquire the ability to interpret or
utilize examples or instructions, because the nature of time series data used
during training does not encourage such capabilities. To address this
limitation, we propose a retrieval augmented time series foundation model
(RATFM), which enables pretrained time series foundation models to incorporate
examples of test-time adaptation. We show that RATFM achieves a performance
comparable to that of in-domain fine-tuning while avoiding domain-dependent
fine-tuning. Experiments on the UCR Anomaly Archive, a multi-domain dataset
including nine domains, confirms the effectiveness of the proposed approach.

</details>


### [178] [Temporal Causal-based Simulation for Realistic Time-series Generation](https://arxiv.org/abs/2506.02084)
*Nikolaos Gkorgkolis,Nikolaos Kougioulis,MingXue Wang,Bora Caglayan,Andrea Tonon,Dario Simionato,Ioannis Tsamardinos*

Main category: cs.LG

TL;DR: 本文提出了一个名为TCS的框架，用于生成更真实的时序因果数据，并通过多阶段优化和评估验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法多依赖合成数据，但这些数据难以准确反映真实世界场景，尤其在时序数据中表现更明显。

Method: TCS框架分为三个阶段：估计滞后因果结构、近似变量间函数依赖关系、学习噪声分布，并结合AutoML技术进行优化。

Result: 实验表明，TCS能生成更真实的时序因果数据，但数据生成任务仍具挑战性。

Conclusion: TCS为生成合理的时序因果数据提供了灵活且模型无关的解决方案，并揭示了真实数据生成中的挑战。

Abstract: Causal Discovery plays a pivotal role in revealing relationships among
observed variables, particularly in the temporal setup. While the majority of
CD methods rely on synthetic data for evaluation, and recently for training,
these fall short in accurately mirroring real-world scenarios; an effect even
more evident in temporal data. Generation techniques depending on simplified
assumptions on causal structure, effects and time, limit the quality and
diversity of the simulated data. In this work, we introduce Temporal
Causal-based Simulation (TCS), a robust framework for generating realistic
time-series data and their associated temporal causal graphs. The approach is
structured in three phases: estimating the true lagged causal structure of the
data, approximating the functional dependencies between variables and learning
the noise distribution of the corresponding causal model, each part of which
can be explicitly tailored based on data assumptions and characteristics.
Through an extensive evaluation process, we highlight that single detection
methods for generated data discrimination prove inadequate, accentuating it as
a multifaceted challenge. For this, we detail a Min-max optimization phase that
draws on AutoML techniques. Our contributions include a flexible,
model-agnostic pipeline for generating realistic temporal causal data, a
thorough evaluation setup which enhances the validity of the generated datasets
and insights into the challenges posed by realistic data generation. Through
experiments involving not only real but also semi-synthetic and purely
synthetic datasets, we demonstrate that while sampling realistic causal data
remains a complex task, our method enriches the domain of generating sensible
causal-based temporal data.

</details>


### [179] [SALAD: Systematic Assessment of Machine Unlearing on LLM-Aided Hardware Design](https://arxiv.org/abs/2506.02089)
*Zeng Wang,Minghao Shao,Rupesh Karn,Jitendra Bhandari,Likhitha Mankali,Ramesh Karri,Ozgur Sinanoglu,Muhammad Shafique,Johann Knechtel*

Main category: cs.LG

TL;DR: 论文提出SALAD方法，利用机器遗忘技术解决大语言模型在硬件设计自动化中的数据安全问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在硬件设计自动化中具有潜力，但也带来数据安全风险，如Verilog评估数据污染、知识产权泄露和恶意代码生成。

Method: 采用机器遗忘技术，选择性移除预训练模型中的污染数据、敏感知识产权或恶意代码模式，无需完全重新训练。

Result: 通过案例研究证明，机器遗忘技术能有效降低LLM辅助硬件设计中的数据安全风险。

Conclusion: SALAD方法为LLM在硬件设计中的应用提供了安全解决方案，解决了关键的数据安全问题。

Abstract: Large Language Models (LLMs) offer transformative capabilities for hardware
design automation, particularly in Verilog code generation. However, they also
pose significant data security challenges, including Verilog evaluation data
contamination, intellectual property (IP) design leakage, and the risk of
malicious Verilog generation. We introduce SALAD, a comprehensive assessment
that leverages machine unlearning to mitigate these threats. Our approach
enables the selective removal of contaminated benchmarks, sensitive IP and
design artifacts, or malicious code patterns from pre-trained LLMs, all without
requiring full retraining. Through detailed case studies, we demonstrate how
machine unlearning techniques effectively reduce data security risks in
LLM-aided hardware design.

</details>


### [180] [Towards Better Generalization and Interpretability in Unsupervised Concept-Based Models](https://arxiv.org/abs/2506.02092)
*Francesco De Santis,Philippe Bich,Gabriele Ciravegna,Pietro Barbiero,Danilo Giordano,Tania Cerquitelli*

Main category: cs.LG

TL;DR: 本文提出了一种名为LCBM的无监督概念模型，通过伯努利潜在空间建模概念，提升深度神经网络的可信度和可解释性，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 为了提高深度神经网络决策的可信度，需要更好地理解其决策过程。传统方法依赖大量人工监督或可扩展性有限，因此需要一种更高效的无监督方法。

Method: LCBM将概念建模为伯努利潜在空间中的随机变量，使用少量概念且不牺牲性能，通过局部线性组合保持模型可解释性。

Result: LCBM在泛化能力上超越现有无监督概念模型，性能接近黑盒模型，且概念更符合人类直觉。用户研究表明其概念更易解释。

Conclusion: LCBM通过无监督学习实现了高性能和可解释性的平衡，为深度神经网络的可信度提供了有效解决方案。

Abstract: To increase the trustworthiness of deep neural networks, it is critical to
improve the understanding of how they make decisions. This paper introduces a
novel unsupervised concept-based model for image classification, named
Learnable Concept-Based Model (LCBM) which models concepts as random variables
within a Bernoulli latent space. Unlike traditional methods that either require
extensive human supervision or suffer from limited scalability, our approach
employs a reduced number of concepts without sacrificing performance. We
demonstrate that LCBM surpasses existing unsupervised concept-based models in
generalization capability and nearly matches the performance of black-box
models. The proposed concept representation enhances information retention and
aligns more closely with human understanding. A user study demonstrates the
discovered concepts are also more intuitive for humans to interpret. Finally,
despite the use of concept embeddings, we maintain model interpretability by
means of a local linear combination of concepts.

</details>


### [181] [SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis](https://arxiv.org/abs/2506.02096)
*Zijian Wu,Jinjie Ni,Xiangyan Liu,Zichen Liu,Hang Yan,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: SynthRL提出了一种可扩展且保证质量的自动数据扩展方法，通过增强种子问题生成更具挑战性的数据，显著提升了视觉语言模型在复杂推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索如何通过合成强化学习数据进一步提升基于可验证奖励的强化学习（RLVR）训练的视觉语言模型（VLMs）的性能，特别是在复杂推理任务上的表现。

Method: SynthRL方法包括三个关键步骤：1）选择具有适当分布的种子问题；2）将其增强为更具挑战性但保留原答案的变体；3）通过验证阶段确保数据的正确性和难度提升。

Result: 实验表明，SynthRL从8K种子样本中生成了3.3K额外的可验证挑战性问题。使用合成数据训练的模型在五个跨域视觉数学推理基准测试中表现优于仅使用种子数据训练的基线模型，尤其在最具挑战性的样本上提升显著。

Conclusion: SynthRL通过合成高质量数据有效提升了视觉语言模型的复杂推理能力，尤其在最具挑战性的任务上表现突出。

Abstract: Vision-language models (VLMs) trained via reinforcement learning with
verifiable reward (RLVR) have shown notable progress in scaling test-time
compute effectively. In this work, we investigate how synthesized RL data can
further improve RLVR. To this end, we propose \textbf{SynthRL}-a scalable and
guaranteed pipeline for automatic data scaling in reasoning-oriented RL
training. SynthRL comprises three key stages: (1) selecting seed questions with
appropriate distribution, (2) augmenting them into more challenging variants
while preserving the original answers, and (3) a guaranteed verification stage
that ensures near-perfect correctness and difficulty enhancement. Our empirical
experiments demonstrate SynthRL's scalability and effectiveness. When applied
to the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable,
challenging questions from approximately 8K seed samples. Models trained with
our synthesized data achieve consistent gains across five out-of-domain visual
math reasoning benchmarks, with a significant improvement over baseline models
trained on seed data alone. Notably, detailed analysis reveals that the gains
are more pronounced on the most challenging evaluation samples, highlighting
SynthRL's effectiveness in eliciting deeper and more complex reasoning
patterns.

</details>


### [182] [LibriBrain: Over 50 Hours of Within-Subject MEG to Improve Speech Decoding Methods at Scale](https://arxiv.org/abs/2506.02098)
*Miran Özdogan,Gilad Landau,Gereon Elvers,Dulhan Jayalath,Pratik Somaiya,Francesco Mantegna,Mark Woolrich,Oiwi Parker Jones*

Main category: cs.LG

TL;DR: LibriBrain是目前最大的单被试MEG语音解码数据集，包含50多小时记录，旨在推动神经解码方法发展并加速临床脑机接口开发。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有非侵入性神经解码数据规模不足的问题，提供更深入的神经表征研究基础。

Method: 收集单被试听自然英语语音的高质量MEG数据，配套Python库支持深度学习框架集成，并提供标准数据分割和基线结果。

Result: 基线实验表明增加训练数据显著提升解码性能，验证了大规模单被试数据集的价值。

Conclusion: LibriBrain的发布将助力研究社区推进语音解码技术，促进安全有效的临床脑机接口发展。

Abstract: LibriBrain represents the largest single-subject MEG dataset to date for
speech decoding, with over 50 hours of recordings -- 5$\times$ larger than the
next comparable dataset and 50$\times$ larger than most. This unprecedented
`depth' of within-subject data enables exploration of neural representations at
a scale previously unavailable with non-invasive methods. LibriBrain comprises
high-quality MEG recordings together with detailed annotations from a single
participant listening to naturalistic spoken English, covering nearly the full
Sherlock Holmes canon. Designed to support advances in neural decoding,
LibriBrain comes with a Python library for streamlined integration with deep
learning frameworks, standard data splits for reproducibility, and baseline
results for three foundational decoding tasks: speech detection, phoneme
classification, and word classification. Baseline experiments demonstrate that
increasing training data yields substantial improvements in decoding
performance, highlighting the value of scaling up deep, within-subject
datasets. By releasing this dataset, we aim to empower the research community
to advance speech decoding methodologies and accelerate the development of
safe, effective clinical brain-computer interfaces.

</details>


### [183] [ReconXF: Graph Reconstruction Attack via Public Feature Explanations on Privatized Node Features and Labels](https://arxiv.org/abs/2506.02134)
*Rishi Raj Sahoo,Rucha Bhalchandra Joshi,Subhankar Mishra*

Main category: cs.LG

TL;DR: 该论文研究了在公开特征解释和私有化辅助数据下，如何通过ReconXF方法重构图结构，即使存在差分隐私保护。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）在许多应用中表现出色，但其黑盒特性限制了在关键领域（如医疗和刑事司法）的应用。尽管可解释性方法通过提供特征级解释来增强透明度，但这些解释可能带来隐私风险。现有攻击方法假设可以访问原始辅助数据，而实际系统通常使用差分隐私保护节点特征和标签。

Method: 论文提出了ReconXF方法，该方法结合了去噪机制以处理差分隐私噪声，同时利用解释中的结构信号，从而在公开解释和私有化辅助数据下重构图结构。

Result: 实验表明，ReconXF在多个数据集上优于现有方法，在AUC和平均精度上均有提升，表明即使在辅助数据受隐私保护的情况下，公开解释结合去噪仍能恢复图结构。

Conclusion: 论文指出，公开解释与去噪技术的结合使得即使在辅助数据受差分隐私保护的情况下，图结构恢复仍成为可能，这对隐私保护提出了新的挑战。

Abstract: Graph Neural Networks (GNNs) achieve high performance across many
applications but function as black-box models, limiting their use in critical
domains like healthcare and criminal justice. Explainability methods address
this by providing feature-level explanations that identify important node
attributes for predictions. These explanations create privacy risks. Combined
with auxiliary information, feature explanations can enable adversaries to
reconstruct graph structure, exposing sensitive relationships. Existing graph
reconstruction attacks assume access to original auxiliary data, but practical
systems use differential privacy to protect node features and labels while
providing explanations for transparency. We study a threat model where
adversaries access public feature explanations along with privatized node
features and labels. We show that existing explanation-based attacks like GSEF
perform poorly with privatized data due to noise from differential privacy
mechanisms. We propose ReconXF, a graph reconstruction attack for scenarios
with public explanations and privatized auxiliary data. Our method adapts
explanation-based frameworks by incorporating denoising mechanisms that handle
differential privacy noise while exploiting structural signals in explanations.
Experiments across multiple datasets show ReconXF outperforms SoTA methods in
privatized settings, with improvements in AUC and average precision. Results
indicate that public explanations combined with denoising enable graph
structure recovery even under the privacy protection of auxiliary data. Code is
available at (link to be made public after acceptance).

</details>


### [184] [Revisiting LRP: Positional Attribution as the Missing Ingredient for Transformer Explainability](https://arxiv.org/abs/2506.02138)
*Yarden Bakish,Itamar Zimerman,Hila Chefer,Lior Wolf*

Main category: cs.LG

TL;DR: 本文提出了一种改进的层相关性传播（LRP）方法，专门针对Transformer的位置编码（PE），显著提升了视觉和NLP解释性任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LRP的Transformer解释方法忽视了位置编码（PE）这一关键组件，导致守恒性质被破坏，且丢失了与结构和位置特征相关的重要相关性信息。

Method: 通过将输入空间重新定义为位置-标记对，并设计专门的理论基础LRP规则，以支持多种位置编码方法（如Rotary、Learnable和Absolute PE）。

Result: 在微调分类器和零样本基础模型（如LLaMA 3）上的广泛实验表明，该方法在视觉和NLP解释性任务中显著优于现有技术。

Conclusion: 本文提出的方法有效解决了Transformer解释性工具中位置编码被忽视的问题，为相关研究提供了新的理论基础和实践工具。

Abstract: The development of effective explainability tools for Transformers is a
crucial pursuit in deep learning research. One of the most promising approaches
in this domain is Layer-wise Relevance Propagation (LRP), which propagates
relevance scores backward through the network to the input space by
redistributing activation values based on predefined rules. However, existing
LRP-based methods for Transformer explainability entirely overlook a critical
component of the Transformer architecture: its positional encoding (PE),
resulting in violation of the conservation property, and the loss of an
important and unique type of relevance, which is also associated with
structural and positional features. To address this limitation, we reformulate
the input space for Transformer explainability as a set of position-token
pairs. This allows us to propose specialized theoretically-grounded LRP rules
designed to propagate attributions across various positional encoding methods,
including Rotary, Learnable, and Absolute PE. Extensive experiments with both
fine-tuned classifiers and zero-shot foundation models, such as LLaMA 3,
demonstrate that our method significantly outperforms the state-of-the-art in
both vision and NLP explainability tasks. Our code is publicly available.

</details>


### [185] [Z-Error Loss for Training Neural Networks](https://arxiv.org/abs/2506.02154)
*Guillaume Godin*

Main category: cs.LG

TL;DR: 提出Z-Error Loss方法，通过统计原理减少训练中异常值的影响，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 异常值在神经网络训练中会传播错误梯度，影响模型性能和泛化能力。

Method: 利用批次统计自动检测并排除异常样本，最小化异常值对训练的贡献。

Result: 该方法鲁棒性强，能适应数据质量，并为数据清理提供诊断支持。

Conclusion: Z-Error Loss有效减少异常值干扰，帮助模型学习真实数据结构。

Abstract: Outliers introduce significant training challenges in neural networks by
propagating erroneous gradients, which can degrade model performance and
generalization. We propose the Z-Error Loss, a statistically principled
approach that minimizes outlier influence during training by masking the
contribution of data points identified as out-of-distribution within each
batch. This method leverages batch-level statistics to automatically detect and
exclude anomalous samples, allowing the model to focus its learning on the true
underlying data structure. Our approach is robust, adaptive to data quality,
and provides valuable diagnostics for data curation and cleaning.

</details>


### [186] [An Approximation Theory Perspective on Machine Learning](https://arxiv.org/abs/2506.02168)
*Hrushikesh N. Mhaskar,Efstratios Tsoukanis,Ameya D. Jagtap*

Main category: cs.LG

TL;DR: 该论文探讨了机器学习中函数逼近的核心问题，分析了现有方法的局限性，并提出了在未知流形上进行函数逼近的新方法。


<details>
  <summary>Details</summary>
Motivation: 机器学习中函数逼近的理论与实践存在脱节，导致模型在未见数据上的泛化能力不明确。论文旨在填补这一空白，探索逼近理论与机器学习实践之间的差距。

Method: 论文综述了现有方法（如神经网络和核方法）的逼近能力，并提出了无需学习特定流形特征（如拉普拉斯-贝尔特拉米算子的特征分解或图集构建）的新方法。

Result: 论文展示了在未知流形上实现函数逼近的新方法，避免了传统方法中复杂的流形特征学习。

Conclusion: 论文强调了逼近理论在机器学习中的重要性，并提出了解决当前框架不足的新方向，特别是在未知流形上的函数逼近。

Abstract: A central problem in machine learning is often formulated as follows: Given a
dataset $\{(x_j, y_j)\}_{j=1}^M$, which is a sample drawn from an unknown
probability distribution, the goal is to construct a functional model $f$ such
that $f(x) \approx y$ for any $(x, y)$ drawn from the same distribution. Neural
networks and kernel-based methods are commonly employed for this task due to
their capacity for fast and parallel computation. The approximation
capabilities, or expressive power, of these methods have been extensively
studied over the past 35 years. In this paper, we will present examples of key
ideas in this area found in the literature. We will discuss emerging trends in
machine learning including the role of shallow/deep networks, approximation on
manifolds, physics-informed neural surrogates, neural operators, and
transformer architectures. Despite function approximation being a fundamental
problem in machine learning, approximation theory does not play a central role
in the theoretical foundations of the field. One unfortunate consequence of
this disconnect is that it is often unclear how well trained models will
generalize to unseen or unlabeled data. In this review, we examine some of the
shortcomings of the current machine learning framework and explore the reasons
for the gap between approximation theory and machine learning practice. We will
then introduce our novel research to achieve function approximation on unknown
manifolds without the need to learn specific manifold features, such as the
eigen-decomposition of the Laplace-Beltrami operator or atlas construction. In
many machine learning problems, particularly classification tasks, the labels
$y_j$ are drawn from a finite set of values.

</details>


### [187] [Learning Treatment Representations for Downstream Instrumental Variable Regression](https://arxiv.org/abs/2506.02200)
*Shiangyi Lin,Hui Lan,Vasilis Syrgkanis*

Main category: cs.LG

TL;DR: 论文提出了一种新方法，通过在学习处理高维内生变量时显式结合工具变量，解决了传统IV估计器在处理高维非结构化治疗变量时的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统工具变量(IV)估计器在处理高维非结构化治疗变量（如医院患者治疗路径描述）时面临限制，因为其只能处理与可用工具变量数量相当的内生治疗变量。现有方法通常先进行无监督降维再进行IV回归，这可能导致显著的遗漏变量偏差。

Method: 提出了一种新方法，在表示学习过程中显式结合工具变量信息，构建治疗表示，从而处理高维内生变量与有限工具变量的情况。

Result: 理论和实证研究表明，基于工具变量信息的表示学习能够识别优化结果预测的方向，相比传统两阶段方法（降维时不考虑工具变量信息）有显著改进。

Conclusion: 该方法为处理高维内生变量提供了有效框架，通过显式结合工具变量信息，避免了传统方法中的遗漏变量偏差，提升了估计效果。

Abstract: Traditional instrumental variable (IV) estimators face a fundamental
constraint: they can only accommodate as many endogenous treatment variables as
available instruments. This limitation becomes particularly challenging in
settings where the treatment is presented in a high-dimensional and
unstructured manner (e.g. descriptions of patient treatment pathways in a
hospital). In such settings, researchers typically resort to applying
unsupervised dimension reduction techniques to learn a low-dimensional
treatment representation prior to implementing IV regression analysis. We show
that such methods can suffer from substantial omitted variable bias due to
implicit regularization in the representation learning step. We propose a novel
approach to construct treatment representations by explicitly incorporating
instrumental variables during the representation learning process. Our approach
provides a framework for handling high-dimensional endogenous variables with
limited instruments. We demonstrate both theoretically and empirically that
fitting IV models on these instrument-informed representations ensures
identification of directions that optimize outcome prediction. Our experiments
show that our proposed methodology improves upon the conventional two-stage
approaches that perform dimension reduction without incorporating instrument
information.

</details>


### [188] [Constrained Sliced Wasserstein Embedding](https://arxiv.org/abs/2506.02203)
*Navid NaderiAlizadeh,Darian Salehi,Xinran Liu,Soheil Kolouri*

Main category: cs.LG

TL;DR: 该论文提出了一种约束学习方法，用于优化Sliced Wasserstein距离中的切片方向，以提高高维概率度量的比较效率。


<details>
  <summary>Details</summary>
Motivation: 传统的Sliced Wasserstein距离在高维概率度量比较中需要大量切片方向才能达到理想性能，导致计算复杂度高。因此，需要一种更有效的方法来识别信息丰富的切片方向。

Method: 论文提出了一种约束学习方法，通过约束一维传输计划以近似原始空间中的最优传输计划，从而优化切片方向。利用这些传输计划的连续松弛，实现了基于梯度的原始-对偶方法来训练切片参数和其他模型参数。

Result: 实验结果表明，该方法在图像、点云和蛋白质序列等基础模型上能够学习到更具信息量的切片方向，提高了性能。

Conclusion: 该约束学习方法有效优化了Sliced Wasserstein距离的切片方向，减少了计算复杂度，并在多种高维数据上展示了其优越性。

Abstract: Sliced Wasserstein (SW) distances offer an efficient method for comparing
high-dimensional probability measures by projecting them onto multiple
1-dimensional probability distributions. However, identifying informative
slicing directions has proven challenging, often necessitating a large number
of slices to achieve desirable performance and thereby increasing computational
complexity. We introduce a constrained learning approach to optimize the
slicing directions for SW distances. Specifically, we constrain the 1D
transport plans to approximate the optimal plan in the original space, ensuring
meaningful slicing directions. By leveraging continuous relaxations of these
transport plans, we enable a gradient-based primal-dual approach to train the
slicer parameters, alongside the remaining model parameters. We demonstrate how
this constrained slicing approach can be applied to pool high-dimensional
embeddings into fixed-length permutation-invariant representations. Numerical
results on foundation models trained on images, point clouds, and protein
sequences showcase the efficacy of the proposed constrained learning approach
in learning more informative slicing directions. Our implementation code can be
found at https://github.com/Stranja572/constrainedswe.

</details>


### [189] [Bregman Centroid Guided Cross-Entropy Method](https://arxiv.org/abs/2506.02205)
*Yuliang Gu,Hongpeng Cao,Marco Caccamo,Naira Hovakimyan*

Main category: cs.LG

TL;DR: 提出了一种基于Bregman质心的轻量级改进方法BC-EvoCEM，用于增强集成CEM在多模态环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的交叉熵方法（CEM）在多模态环境中容易因单峰采样策略导致过早收敛，限制了其优化效果。

Method: BC-EvoCEM通过计算CEM工作器间的性能加权Bregman质心，并在质心周围采样更新贡献最小的工作器，实现信息聚合和多样性控制。

Result: 实验表明，BC-EvoCEM在合成基准测试、复杂导航任务和完整MBRL流程中均提升了收敛速度和解决方案质量。

Conclusion: BC-EvoCEM为CEM提供了一种简单而有效的改进方法，显著提升了其在多模态环境中的表现。

Abstract: The Cross-Entropy Method (CEM) is a widely adopted trajectory optimizer in
model-based reinforcement learning (MBRL), but its unimodal sampling strategy
often leads to premature convergence in multimodal landscapes. In this work, we
propose Bregman Centroid Guided CEM ($\mathcal{BC}$-EvoCEM), a lightweight
enhancement to ensemble CEM that leverages $\textit{Bregman centroids}$ for
principled information aggregation and diversity control.
$\textbf{$\mathcal{BC}$-EvoCEM}$ computes a performance-weighted Bregman
centroid across CEM workers and updates the least contributing ones by sampling
within a trust region around the centroid. Leveraging the duality between
Bregman divergences and exponential family distributions, we show that
$\textbf{$\mathcal{BC}$-EvoCEM}$ integrates seamlessly into standard CEM
pipelines with negligible overhead. Empirical results on synthetic benchmarks,
a cluttered navigation task, and full MBRL pipelines demonstrate that
$\textbf{$\mathcal{BC}$-EvoCEM}$ enhances both convergence and solution
quality, providing a simple yet effective upgrade for CEM.

</details>


### [190] [KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning](https://arxiv.org/abs/2506.02208)
*Hongling Xu,Qi Zhu,Heyuan Deng,Jinpeng Li,Lu Hou,Yasheng Wang,Lifeng Shang,Ruifeng Xu,Fei Mi*

Main category: cs.LG

TL;DR: KDRL框架结合知识蒸馏和强化学习，提升大语言模型的推理能力，平衡了效率和泛化性。


<details>
  <summary>Details</summary>
Motivation: 强化学习（RL）和知识蒸馏（KD）在大语言模型后训练中各有优劣：RL能探索复杂推理行为但样本效率低，KD学习效率高但泛化性差。本文旨在结合两者优势。

Method: 提出KDRL框架，通过策略梯度优化同时最小化学生与教师模型的RKL散度，并最大化基于规则的奖励，统一了GRPO和KD目标。

Result: 实验表明，KDRL在多个推理基准上优于GRPO和KD基线，实现了性能与推理效率的良好平衡。

Conclusion: 结合KD和RL是训练推理型大语言模型的有效策略，KDRL框架在性能和效率上取得了优越表现。

Abstract: Recent advances in large language model (LLM) post-training have leveraged
two distinct paradigms to enhance reasoning capabilities: reinforcement
learning (RL) and knowledge distillation (KD). While RL enables the emergence
of complex reasoning behaviors, it often suffers from low sample efficiency
when the initial policy struggles to explore high-reward trajectories.
Conversely, KD improves learning efficiency via mimicking the teacher model but
tends to generalize poorly to out-of-domain scenarios. In this work, we present
\textbf{KDRL}, a \textit{unified post-training framework} that jointly
optimizes a reasoning model through teacher supervision (KD) and
self-exploration (RL). Specifically, KDRL leverages policy gradient
optimization to simultaneously minimize the reverse Kullback-Leibler divergence
(RKL) between the student and teacher distributions while maximizing the
expected rule-based rewards. We first formulate a unified objective that
integrates GRPO and KD, and systematically explore how different KL
approximations, KL coefficients, and reward-guided KD strategies affect the
overall post-training dynamics and performance. Empirical results on multiple
reasoning benchmarks demonstrate that KDRL outperforms GRPO and various KD
baselines while achieving a favorable balance between performance and reasoning
token efficiency. These findings indicate that integrating KD and RL serves as
an effective and efficient strategy to train reasoning LLMs.

</details>


### [191] [Exchangeability in Neural Network Architectures and its Application to Dynamic Pruning](https://arxiv.org/abs/2506.02210)
*Pu,Yi,Tianlang Chen,Yifan Yang,Sara Achour*

Main category: cs.LG

TL;DR: 该论文提出了一种名为ExPrune的动态剪枝算法，通过利用神经网络中的对称性冗余，实现了计算效率的提升，同时保持了模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络参数量的增加，部署资源需求也随之增长。现有研究通过剪枝或量化减少冗余，但对称性作为一种潜在的冗余类型尚未被充分探索。本文旨在利用对称性提高推理效率。

Method: 论文通过统计可交换性形式化神经网络参数和中间值的对称性，提出ExPrune算法，动态剪枝对称性导致的冗余。具体实现包括预测ReLU激活的负输入进行神经元级动态剪枝。

Result: 实验表明，ExPrune在多个模型上实现了FLOPs减少10.98-26.3%（精度损失可忽略）和21.01-39.05%（精度损失≤1%）。在静态剪枝模型上，ExPrune进一步减少FLOPs 10.24-11.11%（精度损失可忽略）和13.91-14.39%（精度损失≤1%）。

Conclusion: ExPrune有效利用对称性冗余，显著提升计算效率，且与静态剪枝兼容，为神经网络优化提供了新方向。

Abstract: Neural networks (NNs) are equipped with increasingly many parameters and
require more and more resource for deployment. Researchers have explored
various ways to improve the efficiency of NNs by identifying and reducing the
redundancy, such as pruning or quantizing unimportant weights. Symmetry in the
NN architectures has been identified by prior work as a possible type of
redundancy, but exploiting it for efficient inference is not yet explored. In
this work, we formalize the symmetry of parameters and intermediate values in
NNs using the statistical property of exchangeablility. We identify that
exchangeable values in NN computation may contain overlapping information,
leading to redundancy. Exploiting the insight, we derive a principled general
dynamic pruning algorithm ExPrune to remove symmetry-induced redundancy on a
per-input basis. We also provide an instantiation of ExPrune that performs
neuron-level dynamic pruning by predicting negative inputs to ReLU activations.
We evaluate ExPrune on two computer vision models, one graph model and one
language model. ExPrune provides 10.98--26.3% reduction in FLOPs with
negligible accuracy drop and 21.01--39.05% reduction in FLOPs with at most 1%
accuracy drop. We also demonstrate that ExPrune composes with static pruning.
On models that have been aggressively pruned statically, ExPrune provides
additional 10.24--11.11% reduction in FLOPs with negligible accuracy drop and
13.91--14.39% reduction in FLOPs with at most 1% accuracy drop.

</details>


### [192] [Quantum Ensembling Methods for Healthcare and Life Science](https://arxiv.org/abs/2506.02213)
*Kahn Rhrissorrakrai,Kathleen E. Hamilton,Prerana Bangalore Parthsarathy,Aldo Guzman-Saenz,Tyler Alban,Filippo Utro,Laxmi Parida*

Main category: cs.LG

TL;DR: 该研究探讨了量子集成模型在小样本医疗和生命科学数据上的有效性，通过模拟和硬件实验验证了量子嵌入结构对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 解决现实应用中因样本稀缺导致的小数据学习问题，尤其是在医疗和生命科学领域，量子计算为这类问题提供了新的可能性。

Method: 构建了多种量子集成模型用于二分类任务，利用最多26个模拟量子比特和56个硬件量子比特，模型设计采用最小可训练参数但需要长程量子比特连接。

Result: 在合成数据集和肾细胞癌患者的基因表达数据上测试，展示了量子嵌入结构如何影响性能，并讨论了如何提取有效特征以构建泛化能力强的模型。

Conclusion: 量子计算在小数据问题中展现出潜力，为医疗和生命科学领域的研究提供了新的工具和方法。

Abstract: Learning on small data is a challenge frequently encountered in many
real-world applications. In this work we study how effective quantum ensemble
models are when trained on small data problems in healthcare and life sciences.
We constructed multiple types of quantum ensembles for binary classification
using up to 26 qubits in simulation and 56 qubits on quantum hardware. Our
ensemble designs use minimal trainable parameters but require long-range
connections between qubits. We tested these quantum ensembles on synthetic
datasets and gene expression data from renal cell carcinoma patients with the
task of predicting patient response to immunotherapy. From the performance
observed in simulation and initial hardware experiments, we demonstrate how
quantum embedding structure affects performance and discuss how to extract
informative features and build models that can learn and generalize
effectively. We present these exploratory results in order to assist other
researchers in the design of effective learning on small data using ensembles.
Incorporating quantum computing in these data constrained problems offers hope
for a wide range of studies in healthcare and life sciences where biological
samples are relatively scarce given the feature space to be explored.

</details>


### [193] [From Street Views to Urban Science: Discovering Road Safety Factors with Multimodal Large Language Models](https://arxiv.org/abs/2506.02242)
*Yihong Tang,Ao Qu,Xujing Yu,Weipeng Deng,Jun Ma,Jinhua Zhao,Lijun Sun*

Main category: cs.LG

TL;DR: 该论文提出了一种基于多模态大语言模型（MLLM）的方法UrbanX，用于自动生成、评估和优化关于城市环境与道路安全关系的假设，解决了传统方法依赖人工、可解释性差及未充分利用非结构化数据的问题。


<details>
  <summary>Details</summary>
Motivation: 传统城市与交通研究方法存在依赖人工假设、可解释性不足及非结构化数据利用不充分的问题，限制了科学发现和政策应用的潜力。

Method: 利用MLLM从街景图像生成安全相关问题，提取可解释的嵌入特征，并应用于基于回归的统计模型，支持迭代假设检验与优化。

Result: 在曼哈顿街道路段的实验中，该方法优于预训练的深度学习模型，同时保持完全可解释性，揭示了城市设计与安全之间先前被忽视的关联。

Conclusion: UrbanX不仅适用于道路安全研究，还可作为通用框架，从非结构化城市数据中提取结构化见解，为城市与交通研究提供可扩展、可解释的知识发现途径。

Abstract: Urban and transportation research has long sought to uncover statistically
meaningful relationships between key variables and societal outcomes such as
road safety, to generate actionable insights that guide the planning,
development, and renewal of urban and transportation systems. However,
traditional workflows face several key challenges: (1) reliance on human
experts to propose hypotheses, which is time-consuming and prone to
confirmation bias; (2) limited interpretability, particularly in deep learning
approaches; and (3) underutilization of unstructured data that can encode
critical urban context. Given these limitations, we propose a Multimodal Large
Language Model (MLLM)-based approach for interpretable hypothesis inference,
enabling the automated generation, evaluation, and refinement of hypotheses
concerning urban context and road safety outcomes. Our method leverages MLLMs
to craft safety-relevant questions for street view images (SVIs), extract
interpretable embeddings from their responses, and apply them in
regression-based statistical models. UrbanX supports iterative hypothesis
testing and refinement, guided by statistical evidence such as coefficient
significance, thereby enabling rigorous scientific discovery of previously
overlooked correlations between urban design and safety. Experimental
evaluations on Manhattan street segments demonstrate that our approach
outperforms pretrained deep learning models while offering full
interpretability. Beyond road safety, UrbanX can serve as a general-purpose
framework for urban scientific discovery, extracting structured insights from
unstructured urban data across diverse socioeconomic and environmental
outcomes. This approach enhances model trustworthiness for policy applications
and establishes a scalable, statistically grounded pathway for interpretable
knowledge discovery in urban and transportation studies.

</details>


### [194] [From Features to Structure: Task-Aware Graph Construction for Relational and Tabular Learning with GNNs](https://arxiv.org/abs/2506.02243)
*Tamara Cucumides,Floris Geerts*

Main category: cs.LG

TL;DR: 论文提出auGraph框架，通过任务感知的图增强方法优化表格和关系数据的图结构，提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的方法通常依赖固定的模式派生图（如主外键链接），未能充分利用非键属性的预测信号。

Method: 引入auGraph框架，通过评分函数选择性地将属性提升为节点，增强基础图结构，同时保留原始数据模式。

Result: 实验表明，auGraph在关系型和表格预测任务中优于基于模式和启发式的图构建方法。

Conclusion: auGraph通过任务感知的图增强，有效提升了表格和关系数据的学习效果。

Abstract: Tabular and relational data remain the most ubiquitous formats in real-world
machine learning applications, spanning domains from finance to healthcare.
Although both formats offer structured representations, they pose distinct
challenges for modern deep learning methods, which typically assume flat,
feature-aligned inputs. Graph Neural Networks (GNNs) have emerged as a
promising solution by capturing structural dependencies within and between
tables. However, existing GNN-based approaches often rely on rigid,
schema-derived graphs -- such as those based on primary-foreign key links --
thereby underutilizing rich, predictive signals in non key attributes. In this
work, we introduce auGraph, a unified framework for task-aware graph
augmentation that applies to both tabular and relational data. auGraph enhances
base graph structures by selectively promoting attributes into nodes, guided by
scoring functions that quantify their relevance to the downstream prediction
task. This augmentation preserves the original data schema while injecting
task-relevant structural signal. Empirically, auGraph outperforms schema-based
and heuristic graph construction methods by producing graphs that better
support learning for relational and tabular prediction tasks.

</details>


### [195] [SafeOR-Gym: A Benchmark Suite for Safe Reinforcement Learning Algorithms on Practical Operations Research Problems](https://arxiv.org/abs/2506.02255)
*Asha Ramanujam,Adam Elyoumi,Hao Chen,Sai Madhukiran Kompalli,Akshdeep Singh Ahluwalia,Shraman Pal,Dimitri J. Papageorgiou,Can Li*

Main category: cs.LG

TL;DR: SafeOR-Gym是一个针对复杂约束下安全强化学习的操作研究基准套件，填补了现有基准在工业应用中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有安全强化学习基准主要针对机器人控制任务，与涉及结构化约束、混合整数决策和工业复杂性的高风险领域相关性不足，阻碍了安全强化学习在能源、制造和供应链等关键领域的应用和发展。

Method: 提出了SafeOR-Gym基准套件，包含九个操作研究环境，模拟现实规划、调度和控制问题，支持约束马尔可夫决策过程（CMDP）接口。

Result: 评估了多种先进安全强化学习算法，发现部分任务可解，而其他任务暴露了当前方法的局限性。

Conclusion: SafeOR-Gym为现实决策问题中的安全强化学习研究提供了一个具有挑战性和实用性的测试平台，旨在推动该领域的未来发展。

Abstract: Most existing safe reinforcement learning (RL) benchmarks focus on robotics
and control tasks, offering limited relevance to high-stakes domains that
involve structured constraints, mixed-integer decisions, and industrial
complexity. This gap hinders the advancement and deployment of safe RL in
critical areas such as energy systems, manufacturing, and supply chains. To
address this limitation, we present SafeOR-Gym, a benchmark suite of nine
operations research (OR) environments tailored for safe RL under complex
constraints. Each environment captures a realistic planning, scheduling, or
control problems characterized by cost-based constraint violations, planning
horizons, and hybrid discrete-continuous action spaces. The suite integrates
seamlessly with the Constrained Markov Decision Process (CMDP) interface
provided by OmniSafe. We evaluate several state-of-the-art safe RL algorithms
across these environments, revealing a wide range of performance: while some
tasks are tractable, others expose fundamental limitations in current
approaches. SafeOR-Gym provides a challenging and practical testbed that aims
to catalyze future research in safe RL for real-world decision-making problems.
The SafeOR-Gym framework and all accompanying code are available at:
https://github.com/li-group/SafeOR-Gym.

</details>


### [196] [Human Heterogeneity Invariant Stress Sensing](https://arxiv.org/abs/2506.02256)
*Yi Xiao,Harshit Sharma,Sawinder Kaur,Dessa Bergen-Cico,Asif Salekin*

Main category: cs.LG

TL;DR: 该论文提出了一种名为HHISS的领域泛化方法，通过消除个体特异性差异来检测压力信号中的一致模式，从而提升模型在新个体、环境和压力类型上的表现。


<details>
  <summary>Details</summary>
Motivation: 压力影响身心健康，而可穿戴设备广泛用于通过生理信号检测日常压力。但由于个体差异和健康状况等因素，这些信号变化较大，导致机器学习模型难以泛化。特别是针对阿片类药物使用障碍（OUD）患者，其压力反应因服药时间不同而变化显著，亟需一种能适应这些变化的模型。

Method: HHISS采用了一种称为“人员子网络剪枝交集”的新技术，专注于跨个体的共享特征，并利用连续标签防止过拟合。

Result: 在七个不同的压力数据集（包括实验室、受控现实场景和完全无约束的野外数据）上测试，HHISS始终优于现有基线方法。

Conclusion: HHISS在敏感现实应用中具有可行性和可扩展性，为移动压力感知提供了有效且实用的解决方案。

Abstract: Stress affects physical and mental health, and wearable devices have been
widely used to detect daily stress through physiological signals. However,
these signals vary due to factors such as individual differences and health
conditions, making generalizing machine learning models difficult. To address
these challenges, we present Human Heterogeneity Invariant Stress Sensing
(HHISS), a domain generalization approach designed to find consistent patterns
in stress signals by removing person-specific differences. This helps the model
perform more accurately across new people, environments, and stress types not
seen during training. Its novelty lies in proposing a novel technique called
person-wise sub-network pruning intersection to focus on shared features across
individuals, alongside preventing overfitting by leveraging continuous labels
while training. The study focuses especially on people with opioid use disorder
(OUD)-a group where stress responses can change dramatically depending on their
time of daily medication taking. Since stress often triggers cravings, a model
that can adapt well to these changes could support better OUD rehabilitation
and recovery. We tested HHISS on seven different stress datasets-four of which
we collected ourselves and three public ones. Four are from lab setups, one
from a controlled real-world setting, driving, and two are from real-world
in-the-wild field datasets without any constraints. This is the first study to
evaluate how well a stress detection model works across such a wide range of
data. Results show HHISS consistently outperformed state-of-the-art baseline
methods, proving both effective and practical for real-world use. Ablation
studies, empirical justifications, and runtime evaluations confirm HHISS's
feasibility and scalability for mobile stress sensing in sensitive real-world
applications.

</details>


### [197] [A Tale of Two Symmetries: Exploring the Loss Landscape of Equivariant Models](https://arxiv.org/abs/2506.02269)
*YuQing Xie,Tess Smidt*

Main category: cs.LG

TL;DR: 论文探讨了等变神经网络在优化过程中可能遇到的障碍，发现放松等变约束有时能解决问题，并提出了对隐藏层群表示的重新思考。


<details>
  <summary>Details</summary>
Motivation: 研究等变神经网络在优化过程中是否存在根本性障碍，以及是否需要不同的超参数调整，以提高其训练效果。

Method: 通过理论分析损失景观几何，重点关注使用置换表示构建的网络，并将其视为无约束MLP的子集进行研究。

Result: 发现无约束模型的参数对称性对等变子空间的损失景观有非平凡影响，且在特定条件下可能阻碍学习全局最小值。实证表明，放松约束有时能解决问题。

Conclusion: 研究强调了在更大无约束函数空间中审视网络类别的重要性，并指出有效放松等变约束可能需要重新思考隐藏层群表示的选择。

Abstract: Equivariant neural networks have proven to be effective for tasks with known
underlying symmetries. However, optimizing equivariant networks can be tricky
and best training practices are less established than for standard networks. In
particular, recent works have found small training benefits from relaxing
equivariance constraints. This raises the question: do equivariance constraints
introduce fundamental obstacles to optimization? Or do they simply require
different hyperparameter tuning? In this work, we investigate this question
through a theoretical analysis of the loss landscape geometry. We focus on
networks built using permutation representations, which we can view as a subset
of unconstrained MLPs. Importantly, we show that the parameter symmetries of
the unconstrained model has nontrivial effects on the loss landscape of the
equivariant subspace and under certain conditions can provably prevent learning
of the global minima. Further, we empirically demonstrate in such cases,
relaxing to an unconstrained MLP can sometimes solve the issue. Interestingly,
the weights eventually found via relaxation corresponds to a different choice
of group representation in the hidden layer. From this, we draw 3 key
takeaways. (1) Viewing any class of networks in the context of larger
unconstrained function space can give important insights on loss landscape
structure. (2) Within the unconstrained function space, equivariant networks
form a complicated union of linear hyperplanes, each associated with a specific
choice of internal group representation. (3) Effective relaxation of
equivariance may require not only adding nonequivariant degrees of freedom, but
also rethinking the fixed choice of group representations in hidden layers.

</details>


### [198] [Latent Stochastic Interpolants](https://arxiv.org/abs/2506.02276)
*Saurabh Singh,Dmitry Lagun*

Main category: cs.LG

TL;DR: 本文提出了一种名为Latent Stochastic Interpolants (LSI)的新方法，通过在潜在空间中联合优化编码器、解码器和潜在SI模型，实现了高效的生成建模。


<details>
  <summary>Details</summary>
Motivation: 随机插值（SI）框架在生成建模中表现强大，但在联合优化的潜在变量模型中尚未探索，因为它需要直接访问两个分布的样本。

Method: 开发了一种基于连续时间的证据下界（ELBO）目标，联合优化编码器、解码器和潜在SI模型，从而在潜在空间中实现端到端学习。

Result: LSI在标准大规模ImageNet生成基准测试中表现出色，有效学习了潜在表示并保留了SI框架的生成灵活性。

Conclusion: LSI方法通过联合优化潜在空间中的模型，克服了传统方法的局限性，同时保持了生成建模的灵活性。

Abstract: Stochastic Interpolants (SI) are a powerful framework for generative
modeling, capable of flexibly transforming between two probability
distributions. However, their use in jointly optimized latent variable models
remains unexplored as they require direct access to the samples from the two
distributions. This work presents Latent Stochastic Interpolants (LSI) enabling
joint learning in a latent space with end-to-end optimized encoder, decoder and
latent SI models. We achieve this by developing a principled Evidence Lower
Bound (ELBO) objective derived directly in continuous time. The joint
optimization allows LSI to learn effective latent representations along with a
generative process that transforms an arbitrary prior distribution into the
encoder-defined aggregated posterior. LSI sidesteps the simple priors of the
normal diffusion models and mitigates the computational demands of applying SI
directly in high-dimensional observation spaces, while preserving the
generative flexibility of the SI framework. We demonstrate the efficacy of LSI
through comprehensive experiments on the standard large scale ImageNet
generation benchmark.

</details>


### [199] [Angles Don't Lie: Unlocking Training-Efficient RL Through the Model's Own Signals](https://arxiv.org/abs/2506.02281)
*Qinsi Wang,Jinghan Ke,Hancheng Ye,Yueqian Lin,Yuzhe Fu,Jianyi Zhang,Kurt Keutzer,Chenfeng Xu,Yiran Chen*

Main category: cs.LG

TL;DR: 论文提出GAIN-RL框架，通过模型内禀的角度集中信号动态选择训练数据，显著提升大语言模型强化微调的训练效率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型强化微调方法存在样本效率低下的问题，传统课程学习策略忽视模型自身生成的学习信号，导致训练效果不佳。

Method: 提出GAIN-RL框架，利用模型内禀的角度集中信号动态选择每轮训练数据，确保梯度更新高效。

Result: 实验表明GAIN-RL在数学和编码任务上训练效率提升2.5倍以上，且仅用一半数据即可达到优于全数据训练的效果。

Conclusion: GAIN-RL通过模型内禀信号指导数据选择，为大语言模型高效强化微调提供了新思路。

Abstract: Current Reinforcement Fine-tuning (RFT) paradigms for Large Language Models
(LLMs) suffer from sample inefficiency due to the redundant exposure of
identical queries under uniform data sampling. While previous work has explored
curriculum learning via heuristic difficulty metrics, these strategies exhibit
limitations by neglecting the intrinsic learning signals generated by the model
itself, thus leading to suboptimal training regimes. In this paper, we identify
a model-inherent signal termed angle concentration that effectively reflects an
LLM's capacity to learn from specific data. We theoretically and empirically
demonstrate a correlation between the angular distribution of token hidden
state vectors and the resulting gradient, revealing a learning preference for
data exhibiting higher angle concentration. Inspired by this finding, we
propose GAIN-RL, a Gradient-driven Angle-Informed Navigated RL framework. By
leveraging the model's intrinsic angle concentration signal, GAIN-RL
dynamically selects training data in each epoch, ensuring consistently
impactful gradient updates and thus significantly enhancing overall training
efficiency. Empirical evaluations show that GAIN-RL (GRPO) achieves over a 2.5x
acceleration in training efficiency across diverse mathematical and coding
tasks and varying model scales. Furthermore, GAIN-RL (GRPO)'s efficient
sampling yields data-efficient training, achieving better performance with half
the original data compared to vanilla GRPO with full training data. Code is
realsed at https://github.com/wangqinsi1/GAINRL/tree/main.

</details>


### [200] [Why Gradients Rapidly Increase Near the End of Training](https://arxiv.org/abs/2506.02285)
*Aaron Defazio*

Main category: cs.LG

TL;DR: 论文发现大语言模型训练末期梯度范数激增源于权重衰减、归一化层与学习率调度的交互作用，并提出简单修正方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决大语言模型长时训练末期梯度范数异常增长的问题，揭示其背后机制。

Method: 通过分析权重衰减、归一化层与学习率调度的交互影响，提出针对性修正策略。

Result: 修正方法有效抑制梯度范数激增，同时降低训练全程的损失值。

Conclusion: 该研究为LLM训练稳定性提供了理论解释与实践解决方案。

Abstract: During long-duration Large Language Model (LLM) training runs the gradient
norm increases rapidly near the end of training. In this short note, we show
that this increase is due to an unintended interaction between weight decay,
normalization layers, and the learning rate schedule. We propose a simple
correction that fixes this behavior while also resulting in lower loss values
throughout training.

</details>


### [201] [On Universality Classes of Equivariant Networks](https://arxiv.org/abs/2506.02293)
*Marco Pacini,Gabriele Santin,Bruno Lepri,Shubhendu Trivedi*

Main category: cs.LG

TL;DR: 论文探讨了等变神经网络在分离能力之外的近似能力，发现分离能力不能完全表征表达能力，并给出了浅层不变网络的普适性分类框架。


<details>
  <summary>Details</summary>
Motivation: 当前对等变神经网络的研究主要集中在分离能力（如Weisfeiler-Leman层次结构），但其普适性（逼近目标函数的能力）尚未充分探索。论文旨在填补这一空白。

Method: 通过分析浅层不变网络的普适性类别，建立理论框架，研究等变网络在投影下的表现，并探讨群结构（如正规子群存在性）对模型能力的影响。

Result: 发现分离能力相同的模型可能具有不同的近似能力；给出了浅层等变网络无法普适的充分条件，并在特定群结构下证明了分离约束普适性的可实现性。

Conclusion: 等变神经网络的表达能力不仅取决于分离能力，还依赖于对称群的代数结构（如置换群可能不满足关键条件），这为架构设计提供了新的理论依据。

Abstract: Equivariant neural networks provide a principled framework for incorporating
symmetry into learning architectures and have been extensively analyzed through
the lens of their separation power, that is, the ability to distinguish inputs
modulo symmetry. This notion plays a central role in settings such as graph
learning, where it is often formalized via the Weisfeiler-Leman hierarchy. In
contrast, the universality of equivariant models-their capacity to approximate
target functions-remains comparatively underexplored. In this work, we
investigate the approximation power of equivariant neural networks beyond
separation constraints. We show that separation power does not fully capture
expressivity: models with identical separation power may differ in their
approximation ability. To demonstrate this, we characterize the universality
classes of shallow invariant networks, providing a general framework for
understanding which functions these architectures can approximate. Since
equivariant models reduce to invariant ones under projection, this analysis
yields sufficient conditions under which shallow equivariant networks fail to
be universal. Conversely, we identify settings where shallow models do achieve
separation-constrained universality. These positive results, however, depend
critically on structural properties of the symmetry group, such as the
existence of adequate normal subgroups, which may not hold in important cases
like permutation symmetry.

</details>


### [202] [Through a Steerable Lens: Magnifying Neural Network Interpretability via Phase-Based Extrapolation](https://arxiv.org/abs/2506.02300)
*Farzaneh Mahdisoltani,Saeed Mahdisoltani,Roger B. Grosse,David J. Fleet*

Main category: cs.LG

TL;DR: 该论文提出了一种新框架，通过将网络梯度视为微小运动，可视化类别间的隐式路径，以揭示深度神经网络的内部表示和决策机制。


<details>
  <summary>Details</summary>
Motivation: 现有的可解释性方法通常只能识别有影响的输入区域，但无法阐明模型如何区分类别或需要哪些具体变化才能使输入从一个类别过渡到另一个类别。

Method: 论文提出了一种新框架，利用可逆变换（如Complex Steerable Pyramid）分解图像，在变换空间中计算类条件梯度，并通过线性外推放大梯度，以展示模型如何从源类移动到目标类。

Result: 实验表明，该方法在合成和真实数据集上都能产生感知对齐、语义有意义的转换，揭示了分类器最敏感的方向及其决策边界的几何特性。

Conclusion: 该框架为神经分类器的内部表示提供了一种新颖且可解释的视角，有助于理解其决策机制。

Abstract: Understanding the internal representations and decision mechanisms of deep
neural networks remains a critical open challenge. While existing
interpretability methods often identify influential input regions, they may not
elucidate how a model distinguishes between classes or what specific changes
would transition an input from one category to another. To address these
limitations, we propose a novel framework that visualizes the implicit path
between classes by treating the network gradient as a form of infinitesimal
motion. Drawing inspiration from phase-based motion magnification, we first
decompose images using invertible transforms-specifically the Complex Steerable
Pyramid-then compute class-conditional gradients in the transformed space.
Rather than iteratively integrating the gradient to trace a full path, we
amplify the one-step gradient to the input and perform a linear extrapolation
to expose how the model moves from source to target class. By operating in the
steerable pyramid domain, these amplified gradients produce semantically
meaningful, spatially coherent morphs that highlight the classifier's most
sensitive directions, giving insight into the geometry of its decision
boundaries. Experiments on both synthetic and real-world datasets demonstrate
that our phase-focused extrapolation yields perceptually aligned, semantically
meaningful transformations, offering a novel, interpretable lens into neural
classifiers' internal representations.

</details>


### [203] [CACTI: Leveraging Copy Masking and Contextual Information to Improve Tabular Data Imputation](https://arxiv.org/abs/2506.02306)
*Aditya Gorla,Ryan Wang,Zhengtong Liu,Ulzee An,Sriram Sankararaman*

Main category: cs.LG

TL;DR: CACTI是一种利用缺失模式结构和上下文信息的表格数据填补方法，通过新型训练策略和特征关系学习，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有表格数据填补方法未能充分利用缺失模式的结构和上下文信息，限制了填补性能。

Method: 采用中位数截断复制掩码训练策略，结合列名和文本描述捕捉特征依赖关系。

Result: 在多种数据集和缺失条件下，平均R²提升7.8%，非随机缺失场景最高提升13.4%。

Conclusion: 利用数据集特定上下文和缺失模式能有效提升填补性能，CACTI展示了这一方向的潜力。

Abstract: We present CACTI, a masked autoencoding approach for imputing tabular data
that leverages the structure in missingness patterns and contextual
information. Our approach employs a novel median truncated copy masking
training strategy that encourages the model to learn from empirical patterns of
missingness while incorporating semantic relationships between features -
captured by column names and text descriptions - to better represent feature
dependence. These dual sources of inductive bias enable CACTI to outperform
state-of-the-art methods - an average $R^2$ gain of 7.8% over the next best
method (13.4%, 6.1%, and 5.3% under missing not at random, at random and
completely at random, respectively) - across a diverse range of datasets and
missingness conditions. Our results highlight the value of leveraging
dataset-specific contextual information and missingness patterns to enhance
imputation performance.

</details>


### [204] [MINT: Multimodal Instruction Tuning with Multimodal Interaction Grouping](https://arxiv.org/abs/2506.02308)
*Xiaojun Shan,Qi Cao,Xing Han,Haofei Yu,Paul Pu Liang*

Main category: cs.LG

TL;DR: 论文提出MINT策略，通过按多模态交互类型分组任务来优化指令微调，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态基础模型通过大规模指令微调提升性能，但单纯增加任务数量并不总能带来更好的效果，需探索更有效的任务组织方式。

Method: 提出MINT策略，根据多模态交互类型（如冗余信息发现、模态选择、协同融合）对任务分组，以减少任务间干扰并促进技能迁移。

Result: MINT在多模态指令微调中显著优于现有基线，在泛化与 specialization 间取得更好平衡。

Conclusion: 按多模态交互类型分组任务是指令微调的有效策略，MINT为未来研究提供了新方向。

Abstract: Recent advances in multimodal foundation models have achieved
state-of-the-art performance across a range of tasks. These breakthroughs are
largely driven by new pre-training paradigms that leverage large-scale,
unlabeled multimodal data, followed by instruction fine-tuning on curated
labeled datasets and high-quality prompts. While there is growing interest in
scaling instruction fine-tuning to ever-larger datasets in both quantity and
scale, our findings reveal that simply increasing the number of
instruction-tuning tasks does not consistently yield better performance.
Instead, we observe that grouping tasks by the common interactions across
modalities, such as discovering redundant shared information, prioritizing
modality selection with unique information, or requiring synergistic fusion to
discover new information from both modalities, encourages the models to learn
transferrable skills within a group while suppressing interference from
mismatched tasks. To this end, we introduce MINT, a simple yet surprisingly
effective task-grouping strategy based on the type of multimodal interaction.
We demonstrate that the proposed method greatly outperforms existing task
grouping baselines for multimodal instruction tuning, striking an effective
balance between generalization and specialization.

</details>


### [205] [A Data-Based Architecture for Flight Test without Test Points](https://arxiv.org/abs/2506.02315)
*D. Isaiah Harp,Joshua Ott,John Alora,Dylan Asmar*

Main category: cs.LG

TL;DR: 论文提出了一种消除测试点的新方法，通过机器学习构建降阶模型（ROM），利用飞行测试数据动态更新模型，从而验证高保真数字模型。


<details>
  <summary>Details</summary>
Motivation: 传统飞行测试中，测试点的存在和数据带容忍度问题比飞行员技能不足更为根本，限制了模型预测的准确性。

Method: 使用高保真数字模型生成降阶模型（ROM），通过高斯过程回归（GPR）动态更新模型，利用无约束飞行测试数据验证模型。

Result: 通过T-38C飞行测试数据验证，ROM能够动态更新并生成符合MIL-STD-1797B纵向动力学评估所需的参数。

Conclusion: 消除测试点的方法通过ROM动态更新，提高了飞行测试的灵活性和模型验证的准确性。

Abstract: The justification for the "test point" derives from the test pilot's
obligation to reproduce faithfully the pre-specified conditions of some model
prediction. Pilot deviation from those conditions invalidates the model
assumptions. Flight test aids have been proposed to increase accuracy on more
challenging test points. However, the very existence of databands and
tolerances is the problem more fundamental than inadequate pilot skill. We
propose a novel approach, which eliminates test points. We start with a
high-fidelity digital model of an air vehicle. Instead of using this model to
generate a point prediction, we use a machine learning method to produce a
reduced-order model (ROM). The ROM has two important properties. First, it can
generate a prediction based on any set of conditions the pilot flies. Second,
if the test result at those conditions differ from the prediction, the ROM can
be updated using the new data. The outcome of flight test is thus a refined ROM
at whatever conditions were flown. This ROM in turn updates and validates the
high-fidelity model. We present a single example of this "point-less"
architecture, using T-38C flight test data. We first use a generic aircraft
model to build a ROM of longitudinal pitching motion as a hypersurface. We then
ingest unconstrained flight test data and use Gaussian Process Regression to
update and condition the hypersurface. By proposing a second-order equivalent
system for the T-38C, this hypersurface then generates parameters necessary to
assess MIL-STD-1797B compliance for longitudinal dynamics.

</details>


### [206] [Absorb and Converge: Provable Convergence Guarantee for Absorbing Discrete Diffusion Models](https://arxiv.org/abs/2506.02318)
*Yuchen Liang,Renxiang Huang,Lifeng Lai,Ness Shroff,Yingbin Liang*

Main category: cs.LG

TL;DR: 该论文首次为使用吸收率矩阵的离散扩散模型提供了有限时间误差界和收敛速率分析，解决了KL散度定义问题，并展示了优于均匀率矩阵的收敛性能。


<details>
  <summary>Details</summary>
Motivation: 离散状态空间扩散模型在离散数据应用中表现优异，但其性能对率矩阵选择敏感。尽管经验表明吸收率矩阵通常优于均匀率矩阵，但现有理论多集中于均匀率矩阵，吸收扩散模型的收敛性和误差分析仍缺失。

Method: 通过引入替代初始化分布解决KL散度定义问题，提出新的技术工具（如Jensen型参数、吸收评分函数边界技术等），分析τ-leaping和均匀化采样器的收敛性。

Result: 首次建立了吸收率矩阵下扩散模型的收敛保证，展示了比均匀率矩阵更优的收敛速率，并在适当假设下实现了无需早停的收敛保证。

Conclusion: 该研究填补了吸收扩散模型的理论空白，提出的技术工具为后续研究提供了新方向，验证了吸收率矩阵在实际应用中的优越性。

Abstract: Discrete state space diffusion models have shown significant advantages in
applications involving discrete data, such as text and image generation. It has
also been observed that their performance is highly sensitive to the choice of
rate matrices, particularly between uniform and absorbing rate matrices. While
empirical results suggest that absorbing rate matrices often yield better
generation quality compared to uniform rate matrices, existing theoretical
works have largely focused on the uniform rate matrices case. Notably,
convergence guarantees and error analyses for absorbing diffusion models are
still missing. In this work, we provide the first finite-time error bounds and
convergence rate analysis for discrete diffusion models using absorbing rate
matrices. We begin by deriving an upper bound on the KL divergence of the
forward process, introducing a surrogate initialization distribution to address
the challenge posed by the absorbing stationary distribution, which is a
singleton and causes the KL divergence to be ill-defined. We then establish the
first convergence guarantees for both the $\tau$-leaping and uniformization
samplers under absorbing rate matrices, demonstrating improved rates over their
counterparts using uniform rate matrices. Furthermore, under suitable
assumptions, we provide convergence guarantees without early stopping. Our
analysis introduces several new technical tools to address challenges unique to
absorbing rate matrices. These include a Jensen-type argument for bounding
forward process convergence, novel techniques for bounding absorbing score
functions, and a non-divergent upper bound on the score near initialization
that removes the need of early-stopping.

</details>


### [207] [Sensitivity-Aware Density Estimation in Multiple Dimensions](https://arxiv.org/abs/2506.02323)
*Aleix Boquet-Pujadas,Pol del Aguila Pla,Michael Unser*

Main category: cs.LG

TL;DR: 该论文提出了一种基于网格样条和核范数正则化的多维概率密度估计方法，适用于非均匀采样问题，并在PET图像重建中进行了应用验证。


<details>
  <summary>Details</summary>
Motivation: 针对多维问题中非均匀采样导致的概率密度估计难题，结合探测器灵敏度等异质性因素，开发高效且边界条件灵活的计算方法。

Method: 利用网格样条的计算速度和灵活边界条件，通过核范数正则化样条的Hessian矩阵以促进稀疏性，实现空间自适应和正则化参数稳定性。

Result: 方法在标准密度测试中表现良好，提供了配套软件，并以PET图像重分组作为框架应用展示了新方法。

Conclusion: 所提出的优化方法能有效处理非均匀采样密度估计问题，具有空间自适应性和参数鲁棒性，在医学成像等领域具有应用潜力。

Abstract: We formulate an optimization problem to estimate probability densities in the
context of multidimensional problems that are sampled with uneven probability.
It considers detector sensitivity as an heterogeneous density and takes
advantage of the computational speed and flexible boundary conditions offered
by splines on a grid. We choose to regularize the Hessian of the spline via the
nuclear norm to promote sparsity. As a result, the method is spatially adaptive
and stable against the choice of the regularization parameter, which plays the
role of the bandwidth. We test our computational pipeline on standard densities
and provide software. We also present a new approach to PET rebinning as an
application of our framework.

</details>


### [208] [Discovery of Probabilistic Dirichlet-to-Neumann Maps on Graphs](https://arxiv.org/abs/2506.02337)
*Adrienne M. Propp,Jonas A. Actor,Elise Walker,Houman Owhadi,Nathaniel Trask,Daniel M. Tartakovsky*

Main category: cs.LG

TL;DR: 提出了一种基于高斯过程的图结构Dirichlet-to-Neumann映射学习方法，结合离散外微积分和非线性最优恢复技术，确保守恒律并避免过拟合，在数据稀缺条件下仍保持高精度和可靠的不确定性量化。


<details>
  <summary>Details</summary>
Motivation: Dirichlet-to-Neumann映射在多物理场耦合模拟中至关重要，但传统方法在数据稀缺和守恒约束条件下存在局限性。本文旨在开发一种能严格保持守恒律且具有不确定性量化能力的数据驱动方法。

Method: 结合离散外微积分（DEC）框架与非线性最优恢复技术，通过优化再生核希尔伯特空间范数并施加最大似然估计惩罚，构建高斯过程代理模型。该方法能推断顶点与边值的关系，并严格保证PDE的守恒约束。

Result: 在地下裂缝网络和动脉血流两个案例中，该方法在严重数据稀缺情况下仍保持高精度（<5%相对误差）和良好校准的不确定性估计（95%置信区间覆盖率达93-97%）。

Conclusion: 所提出的高斯过程学习框架为科学计算中的多物理场耦合问题提供了数据高效、守恒律保持且具有可靠不确定性量化的新型解决方案，特别适用于观测数据有限的场景。

Abstract: Dirichlet-to-Neumann maps enable the coupling of multiphysics simulations
across computational subdomains by ensuring continuity of state variables and
fluxes at artificial interfaces. We present a novel method for learning
Dirichlet-to-Neumann maps on graphs using Gaussian processes, specifically for
problems where the data obey a conservation constraint from an underlying
partial differential equation. Our approach combines discrete exterior calculus
and nonlinear optimal recovery to infer relationships between vertex and edge
values. This framework yields data-driven predictions with uncertainty
quantification across the entire graph, even when observations are limited to a
subset of vertices and edges. By optimizing over the reproducing kernel Hilbert
space norm while applying a maximum likelihood estimation penalty on kernel
complexity, our method ensures that the resulting surrogate strictly enforces
conservation laws without overfitting. We demonstrate our method on two
representative applications: subsurface fracture networks and arterial blood
flow. Our results show that the method maintains high accuracy and
well-calibrated uncertainty estimates even under severe data scarcity,
highlighting its potential for scientific applications where limited data and
reliable uncertainty quantification are critical.

</details>


### [209] [Rewarding the Unlikely: Lifting GRPO Beyond Distribution Sharpening](https://arxiv.org/abs/2506.02355)
*Andre He,Daniel Fried,Sean Welleck*

Main category: cs.LG

TL;DR: 论文指出GRPO算法在强化学习训练大语言模型时存在偏见问题，并提出了一种新奖励机制和调整PPO训练周期的方法来改善性能。


<details>
  <summary>Details</summary>
Motivation: 研究发现GRPO算法在需要多样样本的任务（如定理证明）中，偏向强化已有高概率解，而忽略罕见但正确的解，影响了pass@N指标的表现。

Method: 引入了一种称为‘unlikeliness reward’的新奖励机制，明确鼓励强化罕见正确解，并增加PPO训练周期以减轻偏见。

Result: 实验表明，新方法显著提升了pass@N指标表现，优于标准GRPO，并增加了样本多样性，在miniF2F-test基准测试中达到与DeepSeek-Prover-V1.5-RL竞争的性能。

Conclusion: 论文提出的方法为使用强化学习训练形式化定理证明器提供了一种简单而有效的方案，并公开了实现代码。

Abstract: Reinforcement learning has emerged as an effective framework for training
large language models on structured language-conditioned tasks. We identify a
critical flaw of Group Relative Policy Optimization (GRPO), a widely used RL
algorithm in this setting. For tasks that require multi-sample performance,
such as formal theorem proving, GRPO biasedly reinforces already probable
solutions and neglects rare but correct proofs. This implicit bias impairs
performance on pass@$N$ metrics at large sample sizes, limiting its
practicality for training theorem provers. To address this, we introduce the
unlikeliness reward, a straightforward method that explicitly encourages
reinforcing rare correct solutions. Additionally, we find that increasing the
number of PPO epochs further mitigates this bias. Our experiments confirm that
incorporating the unlikeliness reward significantly improves pass@$N$ across a
large range of N, outperforming standard GRPO and substantially increasing
sample diversity. Applying our revised recipe to Lean, we achieve competitive
performance with DeepSeek-Prover-V1.5-RL on the miniF2F-test benchmark. We
release our implementation, providing a simple yet effective recipe for
training formal theorem provers with RL.

</details>


### [210] [Evaluating LLM Agent Adherence to Hierarchical Safety Principles: A Lightweight Benchmark for Probing Foundational Controllability Components](https://arxiv.org/abs/2506.02357)
*Ram Potham*

Main category: cs.LG

TL;DR: 该论文提出了一种轻量级、可解释的基准方法，用于评估LLM代理在面临任务冲突时是否优先遵守高层安全原则。


<details>
  <summary>Details</summary>
Motivation: 为确保高级AI开发的安全性，需要验证代理行为并早期发现控制缺陷，特别是在安全原则与操作目标冲突时。

Method: 使用简单网格世界作为基准，测试LLM代理在任务指令与安全原则冲突时是否优先遵守不可违反的安全指令。

Result: 初步研究表明该方法可行，并提供了代理在原则冲突下的行为洞察，为评估可控性提供了实证依据。

Conclusion: 评估对分层原则的遵守是理解构建可控AI系统能力的重要早期步骤。

Abstract: Credible safety plans for advanced AI development require methods to verify
agent behavior and detect potential control deficiencies early. A fundamental
aspect is ensuring agents adhere to safety-critical principles, especially when
these conflict with operational goals. Failure to prioritize such principles
indicates a potential basic control failure. This paper introduces a
lightweight, interpretable benchmark methodology using a simple grid world to
evaluate an LLM agent's ability to uphold a predefined, high-level safety
principle (e.g., "never enter hazardous zones") when faced with conflicting
lower-level task instructions. We probe whether the agent reliably prioritizes
the inviolable directive, testing a foundational controllability aspect of
LLMs. This pilot study demonstrates the methodology's feasibility, offers
preliminary insights into agent behavior under principle conflict, and
discusses how such benchmarks can contribute empirical evidence for assessing
controllability. We argue that evaluating adherence to hierarchical principles
is a crucial early step in understanding our capacity to build governable AI
systems.

</details>


### [211] [Reconciling Hessian-Informed Acceleration and Scalar-Only Communication for Efficient Federated Zeroth-Order Fine-Tuning](https://arxiv.org/abs/2506.02370)
*Zhe Li,Bicheng Ying,Zidong Liu,Chaosheng Dong,Haibo Yang*

Main category: cs.LG

TL;DR: 论文提出HiSo方法，通过Hessian-informed零阶优化和标量通信加速联邦学习中大语言模型的微调，同时保持低通信成本。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习框架如DeComFL通过零阶随机梯度下降（ZO-SGD）减少通信开销，但ZO梯度估计的高方差导致收敛缓慢。利用Hessian信息可加速优化，但需解决本地数据限制和保持通信效率的挑战。

Method: 提出广义标量通信框架，解耦维度无关通信与ZO-SGD，并在此基础上设计HiSo方法，利用全局曲率信息加速收敛，同时保持每轮最低通信成本。

Result: 理论证明HiSo收敛速度更快，尤其当全局Hessian矩阵低秩时（常见于大语言模型）。实验验证其在收敛速度和通信效率上优于现有ZO联邦学习方法。

Conclusion: HiSo通过结合Hessian信息和标量通信，显著提升联邦学习中大模型微调的效率，为资源受限场景提供实用解决方案。

Abstract: Recent dimension-free communication frameworks in Federated Learning (FL),
such as DeComFL, significantly reduce per-round communication by transmitting
only scalars via zeroth-order stochastic gradient descent (ZO-SGD). This method
is particularly advantageous for federated fine-tuning of Large Language Models
(LLMs). Yet, the high variance in ZO gradient estimation typically leads to
slow convergence. Although leveraging Hessian information is known to enhance
optimization speed, integrating this into FL presents significant challenges.
These include clients' restrictions on local data and the critical need to
maintain the dimension-free communication property. To overcome this
limitation, we first introduce a generalized scalar-only communication FL
framework that decouples dimension-free communication from standard ZO-SGD,
enabling the integration of more advanced optimization strategies. Building on
this framework, we propose HiSo, a fast federated fine-tuning method via
Hessian-informed zeroth-order optimization and Scalar-only communication.
Specifically, it leverages global curvature information to accelerate
convergence while preserving the same minimal communication cost per round.
Theoretically, we establish convergence guarantees that are independent of the
global Lipschitz constant, and further show that HiSo achieves faster rates
when the global Hessian exhibits a low effective rank -- a common phenomenon in
LLMs. Extensive experiments on benchmark datasets and LLM fine-tuning tasks
confirm that HiSo significantly outperforms existing ZO-based FL methods in
both convergence speed and communication efficiency.

</details>


### [212] [SFBD Flow: A Continuous-Optimization Framework for Training Diffusion Models with Noisy Samples](https://arxiv.org/abs/2506.02371)
*Haoye Lu,Darren Lo,Yaoliang Yu*

Main category: cs.LG

TL;DR: SFBD flow改进SFBD方法，通过连续变体消除交替步骤，并在基准测试中优于基线。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成性能上表现优异，但依赖大型数据集可能涉及隐私问题，且存在记忆训练数据的倾向。SFBD虽通过使用损坏数据和有限干净样本解决此问题，但其迭代去噪和微调过程需要手动协调，实现复杂。

Method: 将SFBD重新解释为交替投影算法，并引入连续变体SFBD flow，消除交替步骤需求，展示其与基于一致性约束方法的联系。

Result: 实际实现的Online SFBD在多个基准测试中一致优于强基线。

Conclusion: SFBD flow通过简化实现流程并提升性能，有效解决了扩散模型的隐私和效率问题。

Abstract: Diffusion models achieve strong generative performance but often rely on
large datasets that may include sensitive content. This challenge is compounded
by the models' tendency to memorize training data, raising privacy concerns.
SFBD (Lu et al., 2025) addresses this by training on corrupted data and using
limited clean samples to capture local structure and improve convergence.
However, its iterative denoising and fine-tuning loop requires manual
coordination, making it burdensome to implement. We reinterpret SFBD as an
alternating projection algorithm and introduce a continuous variant, SFBD flow,
that removes the need for alternating steps. We further show its connection to
consistency constraint-based methods, and demonstrate that its practical
instantiation, Online SFBD, consistently outperforms strong baselines across
benchmarks.

</details>


### [213] [Multi-agent Markov Entanglement](https://arxiv.org/abs/2506.02385)
*Shuze Chen,Tianyi Peng*

Main category: cs.LG

TL;DR: 该论文揭示了多智能体MDP中价值分解有效的数学结构——转移矩阵的'非纠缠性'，并提出'马尔可夫纠缠度'来衡量分解误差，证明了常用索引策略具有次线性误差规模。


<details>
  <summary>Details</summary>
Motivation: 尽管价值分解在多智能体动态规划和强化学习中被广泛应用，但其为何有效的理论依据尚未充分探索。论文旨在填补这一空白。

Method: 引入类似量子纠缠的'马尔可夫纠缠'概念，提出测量方法，并证明其与价值分解误差的关系。

Result: 证明了广泛使用的索引策略具有弱纠缠性，在N智能体系统中实现次线性(√N)分解误差；提出了实践中高效估计纠缠度的方法。

Conclusion: 马尔可夫纠缠度为评估价值分解质量提供了理论工具，为实际应用提供了可计算的代理指标。

Abstract: Value decomposition has long been a fundamental technique in multi-agent
dynamic programming and reinforcement learning (RL). Specifically, the value
function of a global state $(s_1,s_2,\ldots,s_N)$ is often approximated as the
sum of local functions: $V(s_1,s_2,\ldots,s_N)\approx\sum_{i=1}^N V_i(s_i)$.
This approach traces back to the index policy in restless multi-armed bandit
problems and has found various applications in modern RL systems. However, the
theoretical justification for why this decomposition works so effectively
remains underexplored.
  In this paper, we uncover the underlying mathematical structure that enables
value decomposition. We demonstrate that a multi-agent Markov decision process
(MDP) permits value decomposition if and only if its transition matrix is not
"entangled" -- a concept analogous to quantum entanglement in quantum physics.
Drawing inspiration from how physicists measure quantum entanglement, we
introduce how to measure the "Markov entanglement" for multi-agent MDPs and
show that this measure can be used to bound the decomposition error in general
multi-agent MDPs.
  Using the concept of Markov entanglement, we proved that a widely-used class
of index policies is weakly entangled and enjoys a sublinear $\mathcal
O(\sqrt{N})$ scale of decomposition error for $N$-agent systems. Finally, we
show how Markov entanglement can be efficiently estimated in practice,
providing practitioners with an empirical proxy for the quality of value
decomposition.

</details>


### [214] [Asymptotically Optimal Linear Best Feasible Arm Identification with Fixed Budget](https://arxiv.org/abs/2506.02386)
*Jie Bian,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 论文提出了一种新算法，用于在固定预算下识别最佳可行臂，其错误概率以指数速度衰减，且衰减率与理论下界匹配。


<details>
  <summary>Details</summary>
Motivation: 现有文献尚未确定在固定预算下识别最佳可行臂的错误概率的精确指数衰减率，即使在相对简单的高斯噪声K臂老虎机设置中也是如此。

Method: 论文引入了一种基于后验采样的新算法，结合了min-learner和max-learner的游戏采样规则，专为固定预算下的识别优化设计。

Result: 理论分析和实验验证表明，该算法的错误概率以指数速度衰减，且衰减率与理论下界一致，性能优于多个基准算法。

Conclusion: 论文填补了文献中的空白，提出了一种高效且准确的算法，适用于固定预算下的最佳可行臂识别问题。

Abstract: The challenge of identifying the best feasible arm within a fixed budget has
attracted considerable interest in recent years. However, a notable gap remains
in the literature: the exact exponential rate at which the error probability
approaches zero has yet to be established, even in the relatively simple
setting of $K$-armed bandits with Gaussian noise. In this paper, we address
this gap by examining the problem within the context of linear bandits. We
introduce a novel algorithm for best feasible arm identification that
guarantees an exponential decay in the error probability. Remarkably, the decay
rate -- characterized by the exponent -- matches the theoretical lower bound
derived using information-theoretic principles. Our approach leverages a
posterior sampling framework embedded within a game-based sampling rule
involving a min-learner and a max-learner. This strategy shares its foundations
with Thompson sampling, but is specifically tailored to optimize the
identification process under fixed-budget constraints. Furthermore, we validate
the effectiveness of our algorithm through comprehensive empirical evaluations
across various problem instances with different levels of complexity. The
results corroborate our theoretical findings and demonstrate that our method
outperforms several benchmark algorithms in terms of both accuracy and
efficiency.

</details>


### [215] [Univariate to Multivariate: LLMs as Zero-Shot Predictors for Time-Series Forecasting](https://arxiv.org/abs/2506.02389)
*Chamara Madarasingha,Nasrin Sohrabi,Zahir Tari*

Main category: cs.LG

TL;DR: 该论文提出LLMPred方法，通过将时间序列数据转换为文本并结合两种预处理技术，利用LLM进行零样本预测，在复杂、噪声和多变量数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在时间序列预测中展现出强大的泛化能力，但其在复杂、噪声和多变量数据上的有效性尚未充分探索。

Method: LLMPred通过时间序列分解处理复杂噪声数据，并利用轻量级提示处理策略扩展到多变量预测。

Result: 实验表明，LLMPred在多个小型LLM上表现优于或媲美现有最优方法。

Conclusion: LLMPred的关键组件通过消融研究验证了其重要性，为LLM在时间序列预测中的应用提供了有效解决方案。

Abstract: Time-series prediction or forecasting is critical across many real-world
dynamic systems, and recent studies have proposed using Large Language Models
(LLMs) for this task due to their strong generalization capabilities and
ability to perform well without extensive pre-training. However, their
effectiveness in handling complex, noisy, and multivariate time-series data
remains underexplored. To address this, we propose LLMPred which enhances
LLM-based time-series prediction by converting time-series sequences into text
and feeding them to LLMs for zero shot prediction along with two main data
pre-processing techniques. First, we apply time-series sequence decomposition
to facilitate accurate prediction on complex and noisy univariate sequences.
Second, we extend this univariate prediction capability to multivariate data
using a lightweight prompt-processing strategy. Extensive experiments with
smaller LLMs such as Llama 2 7B, Llama 3.2 3B, GPT-4o-mini, and DeepSeek 7B
demonstrate that LLMPred achieves competitive or superior performance compared
to state-of-the-art baselines. Additionally, a thorough ablation study
highlights the importance of the key components proposed in LLMPred.

</details>


### [216] [GAdaBoost: An Efficient and Robust AdaBoost Algorithm Based on Granular-Ball Structure](https://arxiv.org/abs/2506.02390)
*Qin Xie,Qinghua Zhang,Shuyin Xia,Xinran Zhou,Guoyin Wang*

Main category: cs.LG

TL;DR: 提出基于粒计算的GAdaBoost方法，通过两阶段框架提升AdaBoost在含噪声多分类任务中的效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: AdaBoost在多分类任务中面临标签噪声的挑战，现有方法或缺乏有效噪声处理机制，或计算成本高。

Method: 两阶段框架：1) 数据粒化阶段生成保留多样性的粒球；2) 基于粒球的SAMME算法（GAdaBoost.SA）提升抗噪性。

Result: 在噪声数据集上验证了方法的鲁棒性和效率优势。

Conclusion: GAdaBoost有效扩展了AdaBoost和SAMME，为噪声环境下的多分类提供了新解决方案。

Abstract: Adaptive Boosting (AdaBoost) faces significant challenges posed by label
noise, especially in multiclass classification tasks. Existing methods either
lack mechanisms to handle label noise effectively or suffer from high
computational costs due to redundant data usage. Inspired by granular
computing, this paper proposes granular adaptive boosting (GAdaBoost), a novel
two-stage framework comprising a data granulation stage and an adaptive
boosting stage, to enhance efficiency and robustness under noisy conditions. To
validate its feasibility, an extension of SAMME, termed GAdaBoost.SA, is
proposed. Specifically, first, a granular-ball generation method is designed to
compress data while preserving diversity and mitigating label noise. Second,
the granular ball-based SAMME algorithm focuses on granular balls rather than
individual samples, improving efficiency and reducing sensitivity to noise.
Experimental results on some noisy datasets show that the proposed approach
achieves superior robustness and efficiency compared with existing methods,
demonstrating that this work effectively extends AdaBoost and SAMME.

</details>


### [217] [Improving Generalization of Neural Combinatorial Optimization for Vehicle Routing Problems via Test-Time Projection Learning](https://arxiv.org/abs/2506.02392)
*Yuanyao Chen,Rongsheng Chen,Fu Luo,Zhenkun Wang*

Main category: cs.LG

TL;DR: 该论文提出了一种基于大语言模型的学习框架，解决了神经组合优化方法在大规模车辆路径问题上的性能下降问题，无需重新训练模型即可提升扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的神经组合优化方法在小规模实例（如100个节点）上表现良好，但在大规模场景下性能显著下降，主要原因是训练和测试数据之间的分布偏移。

Method: 引入了一种由大语言模型驱动的学习框架，学习训练和测试分布之间的投影，仅在推理阶段部署，无需重新训练模型。

Result: 实验表明，该方法使骨干模型（在100节点实例上训练）能够在大规模TSP和CVRP问题上（高达100K节点）取得优异性能。

Conclusion: 该框架有效提升了神经组合优化方法在大规模车辆路径问题上的扩展性和性能，且无需重新训练模型。

Abstract: Neural Combinatorial Optimization (NCO) has emerged as a promising
learning-based paradigm for addressing Vehicle Routing Problems (VRPs) by
minimizing the need for extensive manual engineering. While existing NCO
methods, trained on small-scale instances (e.g., 100 nodes), have demonstrated
considerable success on problems of similar scale, their performance
significantly degrades when applied to large-scale scenarios. This degradation
arises from the distributional shift between training and testing data,
rendering policies learned on small instances ineffective for larger problems.
To overcome this limitation, we introduce a novel learning framework driven by
Large Language Models (LLMs). This framework learns a projection between the
training and testing distributions, which is then deployed to enhance the
scalability of the NCO model. Notably, unlike prevailing techniques that
necessitate joint training with the neural network, our approach operates
exclusively during the inference phase, obviating the need for model
retraining. Extensive experiments demonstrate that our method enables a
backbone model (trained on 100-node instances) to achieve superior performance
on large-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP) of up to 100K nodes from diverse distributions.

</details>


### [218] [Random at First, Fast at Last: NTK-Guided Fourier Pre-Processing for Tabular DL](https://arxiv.org/abs/2506.02406)
*Renat Sergazinov,Jing Wu,Shao-An Yin*

Main category: cs.LG

TL;DR: 该论文提出了一种利用随机傅里叶特征作为深度学习预处理步骤的新方法，用于优化表格数据的学习效果，通过理论分析和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 论文的动机源于表格数据深度学习流程中的不足，特别是通过神经切线核（NTK）分析揭示的问题。作者旨在探索随机傅里叶特征作为预处理步骤的潜力，以改善深度学习的训练效率和性能。

Method: 方法包括使用随机傅里叶映射作为无参数、架构无关的变换，通过正弦和余弦投影将输入数据映射到固定特征空间，频率在初始化时随机抽取一次。这种方法避免了额外的归一化或可学习嵌入的需求。

Result: 实验结果表明，经过傅里叶变换处理的输入数据能够加速梯度下降训练，缩短优化路径，并稳定网络的初始NTK谱。这使得深度学习模型在更少的训练周期和超参数调整下，更快收敛并达到更好的最终性能。

Conclusion: 论文的结论是，随机傅里叶预处理是一种理论上有依据、即插即用的增强方法，能够显著提升表格数据深度学习的效率和性能。

Abstract: While random Fourier features are a classic tool in kernel methods, their
utility as a pre-processing step for deep learning on tabular data has been
largely overlooked. Motivated by shortcomings in tabular deep learning
pipelines - revealed through Neural Tangent Kernel (NTK) analysis - we revisit
and repurpose random Fourier mappings as a parameter-free,
architecture-agnostic transformation. By projecting each input into a fixed
feature space via sine and cosine projections with frequencies drawn once at
initialization, this approach circumvents the need for ad hoc normalization or
additional learnable embeddings. We show within the NTK framework that this
mapping (i) bounds and conditions the network's initial NTK spectrum, and (ii)
introduces a bias that shortens the optimization trajectory, thereby
accelerating gradient-based training. These effects pre-condition the network
with a stable kernel from the outset. Empirically, we demonstrate that deep
networks trained on Fourier-transformed inputs converge more rapidly and
consistently achieve strong final performance, often with fewer epochs and less
hyperparameter tuning. Our findings establish random Fourier pre-processing as
a theoretically motivated, plug-and-play enhancement for tabular deep learning.

</details>


### [219] [AERO: A Redirection-Based Optimization Framework Inspired by Judo for Robust Probabilistic Forecasting](https://arxiv.org/abs/2506.02415)
*Karthikeyan Vaiapury*

Main category: cs.LG

TL;DR: 论文提出AERO优化框架，受柔道借力原理启发，通过对抗修正、能量守恒等15条公理实现稳定鲁棒的模型更新，在太阳能预测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有优化方法在动态非线性系统（尤其不确定环境下）难以保持稳定性和适应性，需创新优化范式。

Method: AERO框架将优化重构为基于对抗修正、能量守恒等15条公理的梯度重定向过程，整合不确定性动力学和学习能量管理。

Result: 在概率性太阳能预测任务中，AERO显著提升了预测准确性、可靠性和环境噪声下的适应能力。

Conclusion: AERO为优化理论及实践提供了新方向，特别适用于不确定环境下的鲁棒学习问题。

Abstract: Optimization remains a fundamental pillar of machine learning, yet existing
methods often struggle to maintain stability and adaptability in dynamic, non
linear systems, especially under uncertainty. We introduce AERO (Adversarial
Energy-based Redirection Optimization), a novel framework inspired by the
redirection principle in Judo, where external disturbances are leveraged rather
than resisted. AERO reimagines optimization as a redirection process guided by
15 interrelated axioms encompassing adversarial correction, energy
conservation, and disturbance-aware learning. By projecting gradients,
integrating uncertainty driven dynamics, and managing learning energy, AERO
offers a principled approach to stable and robust model updates. Applied to
probabilistic solar energy forecasting, AERO demonstrates substantial gains in
predictive accuracy, reliability, and adaptability, especially in noisy and
uncertain environments. Our findings highlight AERO as a compelling new
direction in the theoretical and practical landscape of optimization.

</details>


### [220] [Weak Supervision for Real World Graphs](https://arxiv.org/abs/2506.02451)
*Pratheeksha Nair,Reihaneh Rabbany*

Main category: cs.LG

TL;DR: WSNET提出了一种弱监督图对比学习框架，利用图中的弱信号和噪声标签提升节点分类性能，在真实数据集和合成基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现实图中的节点分类常面临标签稀缺和噪声问题，尤其是在高风险领域如人口贩运检测和虚假信息监控。直接监督有限，但图中常包含可指导学习的弱信号或间接线索。

Method: WSNET是一种弱监督图对比学习框架，通过整合图结构、节点特征和多个噪声监督源，采用针对弱标签数据设计的对比目标进行鲁棒表示学习。

Result: 在三个真实数据集和受控噪声的合成基准上，WSNET在F1分数上比现有对比学习和噪声标签学习方法高出15%。

Conclusion: 研究结果表明，弱监督下的对比学习非常有效，利用图中的不完美标签在基于图的环境中具有广阔前景。

Abstract: Node classification in real world graphs often suffers from label scarcity
and noise, especially in high stakes domains like human trafficking detection
and misinformation monitoring. While direct supervision is limited, such graphs
frequently contain weak signals, noisy or indirect cues, that can still inform
learning. We propose WSNET, a novel weakly supervised graph contrastive
learning framework that leverages these weak signals to guide robust
representation learning. WSNET integrates graph structure, node features, and
multiple noisy supervision sources through a contrastive objective tailored for
weakly labeled data. Across three real world datasets and synthetic benchmarks
with controlled noise, WSNET consistently outperforms state of the art
contrastive and noisy label learning methods by up to 15% in F1 score. Our
results highlight the effectiveness of contrastive learning under weak
supervision and the promise of exploiting imperfect labels in graph based
settings.

</details>


### [221] [Comba: Improving Nonlinear RNNs with Closed-loop Control](https://arxiv.org/abs/2506.02475)
*Jiaxi Hu,Yongqi Pan,Jusen Du,Disen Lan,Xiaqiang Tang,Qingsong Wen,Yuxuan Liang,Weigao Sun*

Main category: cs.LG

TL;DR: 本文提出了一种名为Comba的新型非线性RNN变体，基于闭环控制理论，通过状态反馈和输出反馈校正，实现了高效的语言和视觉建模。


<details>
  <summary>Details</summary>
Motivation: 近年来，通过Delta学习规则监督循环内存管理的方法（如Gated DeltaNet、TTT和RWKV-7）在性能上取得了显著提升。然而，这些模型引入了循环状态与关键向量之间的非线性递归结构，其优缺点尚未被全面分析。本文旨在填补这一空白，并提出一种更高效的模型。

Method: 基于闭环控制理论，提出了一种名为Comba的非线性RNN变体，采用标量加低秩状态转移，并结合状态反馈和输出反馈校正。此外，还实现了硬件高效的块级并行Triton内核，并训练了340M/1.3B参数的模型。

Result: Comba在语言和视觉建模任务中表现出卓越的性能和计算效率，验证了其设计优势。

Conclusion: Comba作为一种新型非线性RNN变体，通过创新的状态转移和反馈机制，显著提升了建模效率和性能，为序列建模领域提供了新的解决方案。

Abstract: Recent efficient sequence modeling methods such as Gated DeltaNet, TTT, and
RWKV-7 have achieved performance improvements by supervising the recurrent
memory management through Delta learning rule. Unlike previous state-space
models (e.g., Mamba) and gated linear attentions (e.g., GLA), these models
introduce interactions between the recurrent state and the key vector,
resulting in a nonlinear recursive structure. In this paper, we first introduce
the concept of Nonlinear RNNs with a comprehensive analysis on the advantages
and limitations of these models. Then, based on closed-loop control theory, we
propose a novel Nonlinear RNN variant named Comba, which adopts a
scalar-plus-low-rank state transition, with both state feedback and output
feedback corrections. We also implement a hardware-efficient chunk-wise
parallel kernel in Triton and train models with 340M/1.3B parameters on
large-scale corpus. Comba demonstrates its superior performance and computation
efficiency in both language and vision modeling.

</details>


### [222] [Stochastic Momentum Methods for Non-smooth Non-Convex Finite-Sum Coupled Compositional Optimization](https://arxiv.org/abs/2506.02504)
*Xingyu Chen,Bokun Wang,Ming Yang,Quanqi Hu,Qihang Lin,Tianbao Yang*

Main category: cs.LG

TL;DR: 本文提出了一种针对非凸非光滑有限和耦合组合优化（FCCO）问题的随机动量方法，显著降低了迭代复杂度，并应用于带约束的非凸优化问题，取得了新的最优复杂度结果。


<details>
  <summary>Details</summary>
Motivation: 现有的FCCO优化方法存在迭代复杂度高（O(1/ε^6)）且不适用于深度学习应用的局限性。本文旨在解决这些问题，提出更高效的优化方法。

Method: 提出了针对非光滑FCCO的随机动量方法，并优化了基于平滑铰链惩罚的公式，用于处理带约束的非凸优化问题。

Result: 实现了O(1/ε^5)的迭代复杂度，优于现有最优结果，并在实验中验证了算法的有效性。

Conclusion: 本文提出的方法在理论和实验上均表现出色，为FCCO问题提供了更高效的解决方案，适用于多种机器学习任务。

Abstract: Finite-sum Coupled Compositional Optimization (FCCO), characterized by its
coupled compositional objective structure, emerges as an important optimization
paradigm for addressing a wide range of machine learning problems. In this
paper, we focus on a challenging class of non-convex non-smooth FCCO, where the
outer functions are non-smooth weakly convex or convex and the inner functions
are smooth or weakly convex. Existing state-of-the-art result face two key
limitations: (1) a high iteration complexity of $O(1/\epsilon^6)$ under the
assumption that the stochastic inner functions are Lipschitz continuous in
expectation; (2) reliance on vanilla SGD-type updates, which are not suitable
for deep learning applications. Our main contributions are two fold: (i) We
propose stochastic momentum methods tailored for non-smooth FCCO that come with
provable convergence guarantees; (ii) We establish a new state-of-the-art
iteration complexity of $O(1/\epsilon^5)$. Moreover, we apply our algorithms to
multiple inequality constrained non-convex optimization problems involving
smooth or weakly convex functional inequality constraints. By optimizing a
smoothed hinge penalty based formulation, we achieve a new state-of-the-art
complexity of $O(1/\epsilon^5)$ for finding an (nearly) $\epsilon$-level KKT
solution. Experiments on three tasks demonstrate the effectiveness of the
proposed algorithms.

</details>


### [223] [VerificAgent: Integrating Expert Knowledge and Fact-Checked Memory for Robust Domain-Specific Task Planning](https://arxiv.org/abs/2506.02539)
*Thong Q. Nguyen,Shubhang Desai,Yash Jain,Tanvir Aumi,Vishal Chowdhary*

Main category: cs.LG

TL;DR: VerificAgent框架通过专家知识种子、轨迹记忆优化和人工校验三重机制，有效解决持续记忆增强中的虚假学习问题，在生产力任务中实现111.1%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 持续记忆增强技术虽能提升计算机使用代理(CUA)的任务解决能力，但未经管理的记忆积累会导致虚假/幻觉学习，尤其在生产力软件等专业领域严重影响性能。

Method: 1) 专家构建领域知识种子 2) 训练时基于任务轨迹的迭代记忆优化 3) 部署前人工专家进行事实核查的三阶段记忆管理框架

Result: 在OSWorld生产力任务测试中，相比基线CUA模型取得111.1%的相对成功率提升，且无需额外微调。

Conclusion: VerificAgent通过结构化记忆管理机制，显著提升领域专用CUA的可靠性和性能，证实了人类专家参与在AI记忆优化中的关键价值。

Abstract: Continual memory augmentation allows computer-use agents (CUAs) to learn from
past interactions and refine their task-solving strategies over time. However,
unchecked memory accumulation can introduce spurious or hallucinated
"learnings" that degrade agent performance, particularly in domain-specific
workflows such as productivity software. We present a novel framework,
VerificAgent, that effectively manages memory for CUAs through (1) an
expert-curated seed of domain knowledge, (2) iterative, trajectory-based memory
refinement during training, and (3) a post-hoc fact-checking pass by human
experts to sanitize accumulated memory before deployment. On OSWorld
productivity tasks, VerificAgent achieves a 111.1% relative improvement in
success rate over baseline CUA without any additional fine-tuning.

</details>


### [224] [Rethinking Post-Unlearning Behavior of Large Vision-Language Models](https://arxiv.org/abs/2506.02541)
*Minsung Kim,Nakyeong Yang,Kyomin Jung*

Main category: cs.LG

TL;DR: 论文提出了一种针对大型视觉语言模型的机器遗忘方法PUBG，旨在解决现有遗忘方法导致的响应质量下降问题，同时保护隐私。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉语言模型在训练时使用了大量网络数据，存在隐私风险。虽然已有机器遗忘方法试图缓解这一问题，但它们往往忽视了遗忘后模型响应的质量和信息量，导致退化、幻觉或过度拒绝等不良行为。

Method: 作者提出了PUBG方法，该方法明确引导遗忘后的行为朝向理想的输出分布，确保响应既保护隐私又保持信息和视觉基础。

Result: 实验表明，PUBG有效缓解了现有方法的不良后果，能够生成视觉基础和信息丰富的响应，同时避免遗忘目标的隐私泄露。

Conclusion: PUBG方法在保护隐私的同时，显著提升了大型视觉语言模型遗忘后的响应质量，解决了现有方法的关键缺陷。

Abstract: Machine unlearning is used to mitigate the privacy risks of Large
Vision-Language Models (LVLMs) arising from training on large-scale web data.
However, existing unlearning methods often fail to carefully select substitute
outputs for forget targets, resulting in Unlearning Aftermaths-undesirable
behaviors such as degenerate, hallucinated, or excessively refused responses.
We highlight that, especially for generative LVLMs, it is crucial to consider
the quality and informativeness of post-unlearning responses rather than
relying solely on naive suppression. To address this, we introduce a new
unlearning task for LVLMs that requires models to provide privacy-preserving
yet informative and visually grounded responses. We also propose PUBG, a novel
unlearning method that explicitly guides post-unlearning behavior toward a
desirable output distribution. Experiments show that, while existing methods
suffer from Unlearning Aftermaths despite successfully preventing privacy
violations, PUBG effectively mitigates these issues, generating visually
grounded and informative responses without privacy leakage for forgotten
targets.

</details>


### [225] [HIEGNet: A Heterogenous Graph Neural Network Including the Immune Environment in Glomeruli Classification](https://arxiv.org/abs/2506.02542)
*Niklas Kormann,Masoud Ramuz,Zeeshan Nisar,Nadine S. Schaadt,Hendrik Annuth,Benjamin Doerr,Friedrich Feuerhake,Thomas Lampert,Johannes F. Lutzeyer*

Main category: cs.LG

TL;DR: 该论文提出了一种名为HIEGNet的新型异构图神经网络，用于肾小球健康分类，通过整合肾小球及其周围免疫细胞的信息，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）在组织病理学中表现出色，但在肾小球健康分类这一重要任务中尚未深入探索。该任务在图构建（节点、边及特征识别）方面存在独特挑战。

Method: 提出了一种结合传统和机器学习计算机视觉技术的流程，用于构建异构图（识别节点、边及特征），并设计了一种新型异构图神经网络HIEGNet，整合肾小球及其周围免疫细胞信息进行分类。

Result: 实验结果表明，HIEGNet在肾移植患者的全切片图像数据集上表现优于多个基线模型，并且在患者间泛化能力最佳。

Conclusion: HIEGNet通过整合肾小球及其免疫环境信息，显著提升了分类性能，为肾小球健康分类任务提供了一种有效的解决方案。

Abstract: Graph Neural Networks (GNNs) have recently been found to excel in
histopathology. However, an important histopathological task, where GNNs have
not been extensively explored, is the classification of glomeruli health as an
important indicator in nephropathology. This task presents unique difficulties,
particularly for the graph construction, i.e., the identification of nodes,
edges, and informative features. In this work, we propose a pipeline composed
of different traditional and machine learning-based computer vision techniques
to identify nodes, edges, and their corresponding features to form a
heterogeneous graph. We then proceed to propose a novel heterogeneous GNN
architecture for glomeruli classification, called HIEGNet, that integrates both
glomeruli and their surrounding immune cells. Hence, HIEGNet is able to
consider the immune environment of each glomerulus in its classification. Our
HIEGNet was trained and tested on a dataset of Whole Slide Images from kidney
transplant patients. Experimental results demonstrate that HIEGNet outperforms
several baseline models and generalises best between patients among all
baseline models. Our implementation is publicly available at
https://github.com/nklsKrmnn/HIEGNet.git.

</details>


### [226] [Response-Level Rewards Are All You Need for Online Reinforcement Learning in LLMs: A Mathematical Perspective](https://arxiv.org/abs/2506.02553)
*Shenghua He,Tian Xia,Xuan Zhou,Hui Wei*

Main category: cs.LG

TL;DR: 该论文研究了大型语言模型（LLM）强化学习中的零奖励假设问题，提出了一种理论框架，证明仅使用响应级奖励模型即可无偏估计基于真实令牌级奖励的策略梯度，并提出了新算法TRePO。


<details>
  <summary>Details</summary>
Motivation: 论文动机源于LLM应用中常见的零奖励假设问题，即中间令牌生成缺乏任务特定的即时奖励，仅最终令牌获得奖励。由于精确的令牌级奖励难以获取，研究旨在提供理论支持，简化LLM微调。

Method: 论文引入了轨迹策略梯度定理，证明REINFORCE和Actor-Critic类算法中，仅使用响应级奖励模型即可无偏估计令牌级奖励的策略梯度。此外，提出了新算法TRePO，兼具理论依据和高效性。

Result: 研究结果表明，PPO、GRPO等广泛使用的方法具备建模令牌级奖励信号的能力，为响应级奖励方法提供了理论依据。TRePO算法在简化性和内存效率上表现优异。

Conclusion: 论文通过理论分析和算法创新，为LLM强化学习提供了更实用、高效的微调方法，开发者可专注于改进响应级奖励模型，而将训练算法视为黑箱。

Abstract: We study a common challenge in reinforcement learning for large language
models (LLMs): the Zero-Reward Assumption, where non-terminal actions (i.e.,
intermediate token generations) receive zero task-specific immediate reward,
while only the final token receives a reward for the entire response. This
assumption arises frequently in practice, as precise token-level rewards are
often difficult or infeasible to obtain in LLM applications. In this work, we
provide a unifying theoretical perspective. We introduce the Trajectory Policy
Gradient Theorem, which shows that the policy gradient based on true, unknown
token-level rewards can be unbiasedly estimated using only a response-level
reward model, regardless of whether the Zero-Reward Assumption holds or not,
for algorithms in the REINFORCE and Actor-Critic families. This result reveals
that widely used methods such as PPO, GRPO, ReMax, and RLOO inherently possess
the capacity to model token-level reward signals, offering a theoretical
justification for response-level reward approaches. Our findings pave the way
for more practical, efficient LLM fine-tuning, allowing developers to treat
training algorithms as black boxes and focus on improving the response-level
reward model with auxiliary sub-models. We also offer a detailed analysis of
popular RL and non-RL methods, comparing their theoretical foundations and
practical advantages across common LLM tasks. Finally, we propose a new
algorithm: Token-Reinforced Policy Optimization (TRePO), a theoretically
grounded method that is simpler than PPO, matches GRPO in memory efficiency,
and holds promise for broad applicability.

</details>


### [227] [Privacy-Preserving Federated Convex Optimization: Balancing Partial-Participation and Efficiency via Noise Cancellation](https://arxiv.org/abs/2506.02563)
*Roie Reshef,Kfir Yehuda Levy*

Main category: cs.LG

TL;DR: 该论文提出了一种在联邦学习部分参与场景下实现差分隐私的新方法，通过噪声消除机制在保护隐私的同时保持收敛速度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在全参与场景下表现优异，但在部分参与场景中难以扩展，因此需要一种新的解决方案来填补这一空白。

Method: 引入了一种新颖的噪声消除机制，并在随机凸优化框架下分析了该方法。

Result: 该方法在均匀和非均匀数据分布下均能提供最优性能，扩展了差分隐私在联邦学习中的适用性。

Conclusion: 该研究为分布式系统中部分参与场景下的隐私保护学习提供了高效实用的解决方案。

Abstract: This paper tackles the challenge of achieving Differential Privacy (DP) in
Federated Learning (FL) under partial-participation, where only a subset of the
machines participate in each time-step. While previous work achieved optimal
performance in full-participation settings, these methods struggled to extend
to partial-participation scenarios. Our approach fills this gap by introducing
a novel noise-cancellation mechanism that preserves privacy without sacrificing
convergence rates or computational efficiency. We analyze our method within the
Stochastic Convex Optimization (SCO) framework and show that it delivers
optimal performance for both homogeneous and heterogeneous data distributions.
This work expands the applicability of DP in FL, offering an efficient and
practical solution for privacy-preserving learning in distributed systems with
partial participation.

</details>


### [228] [HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference](https://arxiv.org/abs/2506.02572)
*Ping Gong,Jiawei Yi,Shengnan Wang,Juncheng Zhang,Zewen Jin,Ouxiang Zhou,Ruibo Liu,Guanbin Xu,Youhui Bai,Bowen Ye,Kun Yuan,Tong Yang,Gong Zhang,Renhai Chen,Feng Wu,Cheng Li*

Main category: cs.LG

TL;DR: HATA是一种新型的Top-k注意力机制，通过低开销的哈希技术提升LLM推理效率，同时保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: 现有Top-k注意力机制在效率和准确性之间难以平衡，HATA旨在通过哈希技术解决这一问题。

Method: HATA将查询和键映射为二进制哈希码，以低成本获取相对qk分数顺序，实现Top-k注意力。

Result: HATA在多个主流LLM模型和任务中，速度提升达7.2倍，同时保持准确性优于现有Top-k方法。

Conclusion: HATA通过哈希技术有效提升了LLM推理效率，并在准确性和效率上优于现有方法。

Abstract: Large Language Models (LLMs) have emerged as a pivotal research area, yet the
attention module remains a critical bottleneck in LLM inference, even with
techniques like KVCache to mitigate redundant computations. While various
top-$k$ attention mechanisms have been proposed to accelerate LLM inference by
exploiting the inherent sparsity of attention, they often struggled to strike a
balance between efficiency and accuracy. In this paper, we introduce HATA
(Hash-Aware Top-$k$ Attention), a novel approach that systematically integrates
low-overhead learning-to-hash techniques into the Top-$k$ attention process.
Different from the existing top-k attention methods which are devoted to
seeking an absolute estimation of qk score, typically with a great cost, HATA
maps queries and keys into binary hash codes, and acquires the relative qk
score order with a quite low cost, which is sufficient for realizing top-k
attention. Extensive experiments demonstrate that HATA achieves up to
7.2$\times$ speedup compared to vanilla full attention while maintaining model
accuracy. In addition, HATA outperforms the state-of-the-art top-$k$ attention
methods in both accuracy and efficiency across multiple mainstream LLM models
and diverse tasks. HATA is open source at https://github.com/gpzlx1/HATA.

</details>


### [229] [Reachability Weighted Offline Goal-conditioned Resampling](https://arxiv.org/abs/2506.02577)
*Wenyan Yang,Joni Pajarinen*

Main category: cs.LG

TL;DR: 论文提出了一种名为可达性加权采样（RWS）的方法，用于改进离线目标条件强化学习中的采样策略，显著提升了策略性能。


<details>
  <summary>Details</summary>
Motivation: 在离线目标条件强化学习中，均匀采样需要大量数据覆盖所有可能组合，且会产生许多不可达的状态-目标-动作对，从而降低策略性能。论文的关键见解是采样应优先考虑能够实现目标的转移。

Method: 提出可达性加权采样（RWS），通过正未标记学习训练可达性分类器，将目标条件状态-动作值映射为可达性分数，并以此作为采样优先级。RWS是一个即插即用的模块，可与标准离线强化学习算法无缝集成。

Result: 在六个复杂的模拟机器人操作任务上的实验表明，RWS显著提升了性能。在HandBlock-Z任务中，性能相对基线提升了近50%。

Conclusion: 可达性加权采样在离线目标条件强化学习中表现出色，验证了其有效性。

Abstract: Offline goal-conditioned reinforcement learning (RL) relies on fixed datasets
where many potential goals share the same state and action spaces. However,
these potential goals are not explicitly represented in the collected
trajectories. To learn a generalizable goal-conditioned policy, it is common to
sample goals and state-action pairs uniformly using dynamic programming methods
such as Q-learning. Uniform sampling, however, requires an intractably large
dataset to cover all possible combinations and creates many unreachable
state-goal-action pairs that degrade policy performance. Our key insight is
that sampling should favor transitions that enable goal achievement. To this
end, we propose Reachability Weighted Sampling (RWS). RWS uses a reachability
classifier trained via positive-unlabeled (PU) learning on goal-conditioned
state-action values. The classifier maps these values to a reachability score,
which is then used as a sampling priority. RWS is a plug-and-play module that
integrates seamlessly with standard offline RL algorithms. Experiments on six
complex simulated robotic manipulation tasks, including those with a robot arm
and a dexterous hand, show that RWS significantly improves performance. In one
notable case, performance on the HandBlock-Z task improved by nearly 50 percent
relative to the baseline. These results indicate the effectiveness of
reachability-weighted sampling.

</details>


### [230] [Assessing the Completeness of Traffic Scenario Categories for Automated Highway Driving Functions via Cluster-based Analysis](https://arxiv.org/abs/2506.02599)
*Niklas Roßberg,Marion Neumeier,Sinan Hasirlioglu,Mohamed Essayed Bouzouraa,Michael Botsch*

Main category: cs.LG

TL;DR: 论文提出了一种基于CVQ-VAE的高速公路交通场景聚类与分析流程，探讨了类别数量对场景完整性的影响，并在公开数据集上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 为确保自动驾驶系统（ADS）的安全部署，需要精确理解复杂的交通场景。本文旨在通过聚类方法分析交通场景类别完整性，以支持ADS功能的安全发布。

Method: 采用聚类向量量化变分自编码器（CVQ-VAE）对高速公路交通场景进行聚类，生成不同类别数量的场景目录，并分析类别数量对完整性的影响。

Result: 相比现有工作，该方法展现出更优的聚类性能，并在公开highD数据集上讨论了聚类质量与数据量之间的权衡关系。

Conclusion: 研究表明，CVQ-VAE能有效提升交通场景聚类效果，为ADS场景完整性分析提供了实用工具。

Abstract: The ability to operate safely in increasingly complex traffic scenarios is a
fundamental requirement for Automated Driving Systems (ADS). Ensuring the safe
release of ADS functions necessitates a precise understanding of the occurring
traffic scenarios. To support this objective, this work introduces a pipeline
for traffic scenario clustering and the analysis of scenario category
completeness. The Clustering Vector Quantized - Variational Autoencoder
(CVQ-VAE) is employed for the clustering of highway traffic scenarios and
utilized to create various catalogs with differing numbers of traffic scenario
categories. Subsequently, the impact of the number of categories on the
completeness considerations of the traffic scenario categories is analyzed. The
results show an outperforming clustering performance compared to previous work.
The trade-off between cluster quality and the amount of required data to
maintain completeness is discussed based on the publicly available highD
dataset.

</details>


### [231] [Simple, Good, Fast: Self-Supervised World Models Free of Baggage](https://arxiv.org/abs/2506.02612)
*Jan Robine,Marc Höftmann,Stefan Harmeling*

Main category: cs.LG

TL;DR: 论文提出了一种简单、高效且快速的世界模型SGF，通过自监督表示学习和数据增强等技术，在不使用RNN、Transformer等复杂结构的情况下，实现了良好的性能。


<details>
  <summary>Details</summary>
Motivation: 探讨世界模型的核心组件，研究在不使用RNN、Transformer、离散表示和图像重建的情况下，能构建出何种性能的世界模型。

Method: 采用自监督表示学习，通过帧和动作堆叠捕捉短期依赖关系，并利用数据增强提升模型对错误的鲁棒性。

Result: 在Atari 100k基准测试中表现出色，并通过消融实验验证了各构建模块的有效性。

Conclusion: SGF模型证明了即使不使用复杂结构，也能构建出性能良好的世界模型，为相关研究提供了新的思路。

Abstract: What are the essential components of world models? How far do we get with
world models that are not employing RNNs, transformers, discrete
representations, and image reconstructions? This paper introduces SGF, a
Simple, Good, and Fast world model that uses self-supervised representation
learning, captures short-time dependencies through frame and action stacking,
and enhances robustness against model errors through data augmentation. We
extensively discuss SGF's connections to established world models, evaluate the
building blocks in ablation studies, and demonstrate good performance through
quantitative comparisons on the Atari 100k benchmark.

</details>


### [232] [Compositional Learning for Modular Multi-Agent Self-Organizing Networks](https://arxiv.org/abs/2506.02616)
*Qi Liao,Parijat Bhattacharjee*

Main category: cs.LG

TL;DR: 该研究提出两种组合学习方法CDRL和CPDM，通过模块化双层框架优化多智能体系统性能，显著降低切换失败并提升吞吐量与延迟。


<details>
  <summary>Details</summary>
Motivation: 自组织网络面临参数复杂互依与目标冲突的挑战，需高效且安全的解决方案。

Method: 采用组合深度强化学习(CDRL)和组合预测决策(CPDM)的双层框架，分层管理异构智能体。

Result: 仿真显示切换失败率显著降低，吞吐量与延迟改善，且具更优扩展性、收敛速度及训练安全性。

Conclusion: 该方法在大规模自组织网络中表现出超越传统多智能体强化学习的综合优势。

Abstract: Self-organizing networks face challenges from complex parameter
interdependencies and conflicting objectives. This study introduces two
compositional learning approaches-Compositional Deep Reinforcement Learning
(CDRL) and Compositional Predictive Decision-Making (CPDM)-and evaluates their
performance under training time and safety constraints in multi-agent systems.
We propose a modular, two-tier framework with cell-level and cell-pair-level
agents to manage heterogeneous agent granularities while reducing model
complexity. Numerical simulations reveal a significant reduction in handover
failures, along with improved throughput and latency, outperforming
conventional multi-agent deep reinforcement learning approaches. The approach
also demonstrates superior scalability, faster convergence, higher sample
efficiency, and safer training in large-scale self-organizing networks.

</details>


### [233] [HGOT: Self-supervised Heterogeneous Graph Neural Network with Optimal Transport](https://arxiv.org/abs/2506.02619)
*Yanbei Liu,Chongxu Wang,Zhitao Xiao,Lei Geng,Yanwei Pang,Xiao Wang*

Main category: cs.LG

TL;DR: 提出HGOT方法，通过最优传输机制改进异质图神经网络的自监督学习，无需图增强策略，在节点分类等任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统对比自监督学习在异质图上依赖精心设计的图增强策略和正负样本选择，样本对相似度难以精确衡量。

Method: HGOT采用最优传输机制替代传统对比学习，设计中心视图整合元路径语义，通过最优传输计划对齐分支视图与中心视图的表征。

Result: 在四个真实数据集上，HGOT在下游任务中达到SOTA性能，节点分类任务准确率平均提升超6%。

Conclusion: HGOT通过最优传输有效解决了异质图自监督学习的样本选择难题，学习到更高质量的节点表征。

Abstract: Heterogeneous Graph Neural Networks (HGNNs), have demonstrated excellent
capabilities in processing heterogeneous information networks. Self-supervised
learning on heterogeneous graphs, especially contrastive self-supervised
strategy, shows great potential when there are no labels. However, this
approach requires the use of carefully designed graph augmentation strategies
and the selection of positive and negative samples. Determining the exact level
of similarity between sample pairs is non-trivial.To solve this problem, we
propose a novel self-supervised Heterogeneous graph neural network with Optimal
Transport (HGOT) method which is designed to facilitate self-supervised
learning for heterogeneous graphs without graph augmentation strategies.
Different from traditional contrastive self-supervised learning, HGOT employs
the optimal transport mechanism to relieve the laborious sampling process of
positive and negative samples. Specifically, we design an aggregating view
(central view) to integrate the semantic information contained in the views
represented by different meta-paths (branch views). Then, we introduce an
optimal transport plan to identify the transport relationship between the
semantics contained in the branch view and the central view. This allows the
optimal transport plan between graphs to align with the representations,
forcing the encoder to learn node representations that are more similar to the
graph space and of higher quality. Extensive experiments on four real-world
datasets demonstrate that our proposed HGOT model can achieve state-of-the-art
performance on various downstream tasks. In particular, in the node
classification task, HGOT achieves an average of more than 6% improvement in
accuracy compared with state-of-the-art methods.

</details>


### [234] [SiamNAS: Siamese Surrogate Model for Dominance Relation Prediction in Multi-objective Neural Architecture Search](https://arxiv.org/abs/2506.02623)
*Yuyang Zhou,Ferrante Neri,Yew-Soon Ong,Ruibin Bai*

Main category: cs.LG

TL;DR: 提出SiamNAS框架，利用孪生网络代理模型预测架构优劣，显著降低NAS计算成本，在0.01 GPU天内找到Pareto最优解。


<details>
  <summary>Details</summary>
Motivation: 现代神经架构搜索（NAS）本质上是多目标优化问题，需平衡精度、参数量和计算成本等权衡，导致计算成本高昂且难以高效求解。

Method: 采用孪生网络块集成作为代理模型预测候选架构的支配关系，并基于模型大小的启发式规则替代拥挤距离计算，集成到SiamNAS框架中。

Result: 在NAS-Bench-201上，SiamNAS以0.01 GPU天找到CIFAR-10最佳架构和ImageNet次优架构，代理模型准确率达92%。

Conclusion: 该研究展示了孪生网络代理模型在多任务优化中的潜力，可扩展为生成Pareto解集（SOS），为异构任务提供多样化解决方案。

Abstract: Modern neural architecture search (NAS) is inherently multi-objective,
balancing trade-offs such as accuracy, parameter count, and computational cost.
This complexity makes NAS computationally expensive and nearly impossible to
solve without efficient approximations. To address this, we propose a novel
surrogate modelling approach that leverages an ensemble of Siamese network
blocks to predict dominance relationships between candidate architectures.
Lightweight and easy to train, the surrogate achieves 92% accuracy and replaces
the crowding distance calculation in the survivor selection strategy with a
heuristic rule based on model size. Integrated into a framework termed SiamNAS,
this design eliminates costly evaluations during the search process.
Experiments on NAS-Bench-201 demonstrate the framework's ability to identify
Pareto-optimal solutions with significantly reduced computational costs. The
proposed SiamNAS identified a final non-dominated set containing the best
architecture in NAS-Bench-201 for CIFAR-10 and the second-best for ImageNet, in
terms of test error rate, within 0.01 GPU days. This proof-of-concept study
highlights the potential of the proposed Siamese network surrogate model to
generalise to multi-tasking optimisation, enabling simultaneous optimisation
across tasks. Additionally, it offers opportunities to extend the approach for
generating Sets of Pareto Sets (SOS), providing diverse Pareto-optimal
solutions for heterogeneous task settings.

</details>


### [235] [HAM: A Hyperbolic Step to Regulate Implicit Bias](https://arxiv.org/abs/2506.02630)
*Tom Jacobs,Advait Gadhikar,Celia Rubio-Madrigal,Rebekka Burkholz*

Main category: cs.LG

TL;DR: 本文提出HAM算法，通过结合双曲镜像步优化深度学习模型的隐式偏差，提升收敛速度与特征学习效果。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型优化中的隐式偏差虽能促进稀疏性，但会导致有效学习率降低，减缓收敛速度。

Method: HAM算法交替使用优化器步和双曲镜像步，结合黎曼梯度流改进收敛，并与自然梯度下降关联解释。

Result: 实验表明HAM在多种任务中提升性能，尤其与稀疏化方法结合时效果显著，计算开销低且易于实现。

Conclusion: HAM通过双曲几何优化隐式偏差，显著提升模型性能，尤其在稀疏化场景下表现优异。

Abstract: Understanding the implicit bias of optimization algorithms has become central
to explaining the generalization behavior of deep learning models. For
instance, the hyperbolic implicit bias induced by the overparameterization $m
\odot w$--though effective in promoting sparsity--can result in a small
effective learning rate, which slows down convergence. To overcome this
obstacle, we propose HAM (Hyperbolic Aware Minimization), which alternates
between an optimizer step and a new hyperbolic mirror step. We derive the
Riemannian gradient flow for its combination with gradient descent, leading to
improved convergence and a similar beneficial hyperbolic geometry as $m \odot
w$ for feature learning. We provide an interpretation of the the algorithm by
relating it to natural gradient descent, and an exact characterization of its
implicit bias for underdetermined linear regression. HAM's implicit bias
consistently boosts performance--even of dense training, as we demonstrate in
experiments across diverse tasks, including vision, graph and node
classification, and large language model fine-tuning. HAM is especially
effective in combination with different sparsification methods, improving upon
the state of the art. The hyperbolic step requires minimal computational and
memory overhead, it succeeds even with small batch sizes, and its
implementation integrates smoothly with existing optimizers.

</details>


### [236] [A Pretrained Probabilistic Transformer for City-Scale Traffic Volume Prediction](https://arxiv.org/abs/2506.02654)
*Shiyu Shen,Bin Pan,Guirong Xue*

Main category: cs.LG

TL;DR: 提出TrafficPPT模型，通过预训练概率Transformer解决城市交通流量预测中的数据不完整和偏差问题，结合多源数据并支持跨城市泛化。


<details>
  <summary>Details</summary>
Motivation: 现有城市交通流量预测方法存在数据不完整、忽略不确定性及缺乏跨城市泛化能力的问题，需开发更鲁棒且通用的模型。

Method: TrafficPPT框架融合实时观测、历史轨迹和路网拓扑数据，采用预训练-微调策略，通过概率Transformer建模流量分布。

Result: 实验表明TrafficPPT在极端数据稀疏条件下优于现有方法，且具备跨城市适应能力。

Conclusion: TrafficPPT通过概率建模和多源数据融合，实现了高鲁棒性的城市级交通流量预测，代码将开源。

Abstract: City-scale traffic volume prediction plays a pivotal role in intelligent
transportation systems, yet remains a challenge due to the inherent
incompleteness and bias in observational data. Although deep learning-based
methods have shown considerable promise, most existing approaches produce
deterministic point estimates, thereby neglecting the uncertainty arising from
unobserved traffic flows. Furthermore, current models are typically trained in
a city-specific manner, which hinders their generalizability and limits
scalability across diverse urban contexts. To overcome these limitations, we
introduce TrafficPPT, a Pretrained Probabilistic Transformer designed to model
traffic volume as a distributional aggregation of trajectories. Our framework
fuses heterogeneous data sources-including real-time observations, historical
trajectory data, and road network topology-enabling robust and
uncertainty-aware traffic inference. TrafficPPT is initially pretrained on
large-scale simulated data spanning multiple urban scenarios, and later
fine-tuned on target cities to ensure effective domain adaptation. Experiments
on real-world datasets show that TrafficPPT consistently surpasses
state-of-the-art baselines, particularly under conditions of extreme data
sparsity. Code will be open.

</details>


### [237] [Beyond Invisibility: Learning Robust Visible Watermarks for Stronger Copyright Protection](https://arxiv.org/abs/2506.02665)
*Tianci Liu,Tong Yang,Quan Zhang,Qi Lei*

Main category: cs.LG

TL;DR: 本文提出了一种可见且难以去除的水印方法，以长期保护图像版权，防止AI模型的未经授权使用。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的发展，版权内容面临未经授权使用的风险增加。现有基于隐形对抗扰动的方法仅提供短期保护，且需随模型架构变化重新训练。

Method: 通过概率和逆问题基础的新框架，最大化最优重建与原始内容间的差异，并开发高效近似算法解决双层优化难题。

Result: 实验结果表明，该方法在多种场景下均表现出优越性。

Conclusion: 所提出的可见水印方法为图像版权提供了长期且鲁棒的保护。

Abstract: As AI advances, copyrighted content faces growing risk of unauthorized use,
whether through model training or direct misuse. Building upon invisible
adversarial perturbation, recent works developed copyright protections against
specific AI techniques such as unauthorized personalization through DreamBooth
that are misused. However, these methods offer only short-term security, as
they require retraining whenever the underlying model architectures change. To
establish long-term protection aiming at better robustness, we go beyond
invisible perturbation, and propose a universal approach that embeds
\textit{visible} watermarks that are \textit{hard-to-remove} into images.
Grounded in a new probabilistic and inverse problem-based formulation, our
framework maximizes the discrepancy between the \textit{optimal} reconstruction
and the original content. We develop an effective and efficient approximation
algorithm to circumvent a intractable bi-level optimization. Experimental
results demonstrate superiority of our approach across diverse scenarios.

</details>


### [238] [XicorAttention: Time Series Transformer Using Attention with Nonlinear Correlation](https://arxiv.org/abs/2506.02694)
*Daichi Kimura,Tomonori Izumitani,Hisashi Kashima*

Main category: cs.LG

TL;DR: 提出了一种基于Chatterjee秩相关系数的新型注意力机制XicorAttention，用于改进时间序列预测中的非线性依赖捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的时间序列预测模型在捕捉非线性依赖方面存在不足，需要改进注意力机制以更好地处理非线性关系。

Method: 用Chatterjee秩相关系数替代标准注意力中的矩阵乘法，并引入SoftSort和SoftRank实现可微近似。

Result: 实验表明，XicorAttention将预测准确率最高提升约9.1%。

Conclusion: 通过集成非线性相关性的注意力机制能有效提升时间序列预测性能。

Abstract: Various Transformer-based models have been proposed for time series
forecasting. These models leverage the self-attention mechanism to capture
long-term temporal or variate dependencies in sequences. Existing methods can
be divided into two approaches: (1) reducing computational cost of attention by
making the calculations sparse, and (2) reshaping the input data to aggregate
temporal features. However, existing attention mechanisms may not adequately
capture inherent nonlinear dependencies present in time series data, leaving
room for improvement. In this study, we propose a novel attention mechanism
based on Chatterjee's rank correlation coefficient, which measures nonlinear
dependencies between variables. Specifically, we replace the matrix
multiplication in standard attention mechanisms with this rank coefficient to
measure the query-key relationship. Since computing Chatterjee's correlation
coefficient involves sorting and ranking operations, we introduce a
differentiable approximation employing SoftSort and SoftRank. Our proposed
mechanism, ``XicorAttention,'' integrates it into several state-of-the-art
Transformer models. Experimental results on real-world datasets demonstrate
that incorporating nonlinear correlation into the attention improves
forecasting accuracy by up to approximately 9.1\% compared to existing models.

</details>


### [239] [Data Leakage and Deceptive Performance: A Critical Examination of Credit Card Fraud Detection Methodologies](https://arxiv.org/abs/2506.02703)
*Khizar Hayat,Baptiste Magnier*

Main category: cs.LG

TL;DR: 该研究揭露信用卡欺诈检测研究中普遍存在的方法论缺陷，证明即使简单模型在违反基本原则时也能获得虚假的高性能。


<details>
  <summary>Details</summary>
Motivation: 当前信用卡欺诈检测研究过于关注算法复杂度，而忽视基础评估方法的严谨性，导致结果可信度存疑。

Method: 通过设计实验展示不当评估协议的影响，并系统分析数据泄露、方法描述模糊、时间验证不足和指标操纵四大问题。

Result: 案例显示存在数据泄露的简单神经网络能达到99.9%召回率，远超文献中的复杂方法，证实评估缺陷会扭曲结果。

Conclusion: 欺诈检测研究应优先保证方法论严谨性而非模型复杂度，这对改进机器学习研究实践具有普遍启示意义。

Abstract: This study critically examines the methodological rigor in credit card fraud
detection research, revealing how fundamental evaluation flaws can overshadow
algorithmic sophistication. Through deliberate experimentation with improper
evaluation protocols, we demonstrate that even simple models can achieve
deceptively impressive results when basic methodological principles are
violated. Our analysis identifies four critical issues plaguing current
approaches: (1) pervasive data leakage from improper preprocessing sequences,
(2) intentional vagueness in methodological reporting, (3) inadequate temporal
validation for transaction data, and (4) metric manipulation through recall
optimization at precision's expense. We present a case study showing how a
minimal neural network architecture with data leakage outperforms many
sophisticated methods reported in literature, achieving 99.9\% recall despite
fundamental evaluation flaws. These findings underscore that proper evaluation
methodology matters more than model complexity in fraud detection research. The
study serves as a cautionary example of how methodological rigor must precede
architectural sophistication, with implications for improving research
practices across machine learning applications.

</details>


### [240] [Theoretical Performance Guarantees for Partial Domain Adaptation via Partial Optimal Transport](https://arxiv.org/abs/2506.02712)
*Jayadev Naram,Fredrik Hellström,Ziming Wang,Rebecka Jörnsten,Giuseppe Durisi*

Main category: cs.LG

TL;DR: 该论文针对部分领域自适应（PDA）问题，提出了基于部分最优传输的理论框架，并设计了WARMPOT算法，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在目标领域标记数据稀缺而相关源领域标记数据丰富的情况下，现有PDA方法缺乏理论支持，且权重设计多为启发式。

Method: 基于部分最优传输推导PDA的泛化边界，提出部分Wasserstein距离作为领域对齐项，并设计理论驱动的源损失权重。

Result: WARMPOT算法在实验中表现优异，提出的权重方案优于现有启发式方法。

Conclusion: 论文为PDA提供了理论依据，并通过实验验证了所提方法的有效性。

Abstract: In many scenarios of practical interest, labeled data from a target
distribution are scarce while labeled data from a related source distribution
are abundant. One particular setting of interest arises when the target label
space is a subset of the source label space, leading to the framework of
partial domain adaptation (PDA). Typical approaches to PDA involve minimizing a
domain alignment term and a weighted empirical loss on the source data, with
the aim of transferring knowledge between domains. However, a theoretical basis
for this procedure is lacking, and in particular, most existing weighting
schemes are heuristic. In this work, we derive generalization bounds for the
PDA problem based on partial optimal transport. These bounds corroborate the
use of the partial Wasserstein distance as a domain alignment term, and lead to
theoretically motivated explicit expressions for the empirical source loss
weights. Inspired by these bounds, we devise a practical algorithm for PDA,
termed WARMPOT. Through extensive numerical experiments, we show that WARMPOT
is competitive with recent approaches, and that our proposed weights improve on
existing schemes.

</details>


### [241] [Heterogeneous Group-Based Reinforcement Learning for LLM-based Multi-Agent Systems](https://arxiv.org/abs/2506.02718)
*Guanzhong Chen,Shaoxiong Yang,Chao Li,Wei Liu,Jian Luan,Zenglin Xu*

Main category: cs.LG

TL;DR: 论文提出了一种新型无Critic网络的多智能体强化学习算法MHGPO，用于优化基于LLM的多智能体系统，解决了传统方法训练不稳定和计算负担高的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的多智能体系统存在知识更新滞后、输出控制困难等问题，传统优化方法如提示工程和监督微调适应性有限，而现有MARL算法依赖Critic网络导致训练不稳定和计算负担高。

Method: 提出MHGPO算法，通过异构组间相对奖励优势估计指导策略更新，无需Critic网络，并引入三种组采样策略平衡效率与效果。

Result: 实验表明，MHGPO在多智能体LLM搜索系统中任务性能和计算效率均优于MAPPO，且无需预热即可稳定运行。

Conclusion: MHGPO为复杂LLM多智能体系统提供了稳定、可扩展的优化方案，具有实际应用潜力。

Abstract: Large Language Models (LLMs) have achieved remarkable success across diverse
natural language processing tasks, yet their deployment in real-world
applications is hindered by fixed knowledge cutoffs and difficulties in
generating controllable, accurate outputs in a single inference. Multi-agent
systems (MAS) built from specialized LLM agents offer a promising solution,
enabling dynamic collaboration and iterative reasoning. However, optimizing
these systems remains a challenge, as conventional methods such as prompt
engineering and supervised fine-tuning entail high engineering overhead and
limited adaptability. Reinforcement learning (RL), particularly multi-agent
reinforcement learning (MARL), provides a scalable framework by refining agent
policies based on system-level feedback. Nevertheless, existing MARL
algorithms, such as Multi-Agent Proximal Policy Optimization (MAPPO), rely on
Critic networks, which can cause training instability and increase
computational burden. To address these limitations and target the prototypical
Multi-Agent Search System (MASS), we propose Multi-Agent Heterogeneous Group
Policy Optimization (MHGPO), a novel Critic-free algorithm that guides policy
updates by estimating relative reward advantages across heterogeneous groups of
rollouts. MHGPO eliminates the need for Critic networks, enhancing stability
and reducing computational overhead. Additionally, we introduce three group
rollout sampling strategies that trade off between efficiency and
effectiveness. Experiments on a multi-agent LLM-based search system demonstrate
that MHGPO consistently outperforms MAPPO in both task performance and
computational efficiency, without requiring warm-up, underscoring its potential
for stable and scalable optimization of complex LLM-based MAS.

</details>


### [242] [WeightLoRA: Keep Only Necessary Adapters](https://arxiv.org/abs/2506.02724)
*Andrey Veprikov,Vladimir Solodkin,Alexander Zyl,Andrey Savchenko,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: 本文提出了一种名为WeightLoRA的新方法，通过自适应选择关键的LoRA头来减少可训练参数数量，同时保持或提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的LoRA方法虽然能获得准确解，但在训练大型模型时需要大量内存，且需要人工直觉选择适配器添加的层。

Method: 提出WeightLoRA方法，通过优化过程中自适应选择最关键的LoRA头，显著减少可训练参数数量。

Result: 实验结果表明，WeightLoRA在多个基准测试和模型（如DeBERTa、BART和Llama）中表现优异，WeightLoRA+在几乎所有情况下性能更优。

Conclusion: WeightLoRA方法有效解决了LoRA的内存和人工选择问题，同时保持了模型性能。

Abstract: The widespread utilization of language models in modern applications is
inconceivable without Parameter-Efficient Fine-Tuning techniques, such as
low-rank adaptation ($\texttt{LoRA}$), which adds trainable adapters to
selected layers. Although $\texttt{LoRA}$ may obtain accurate solutions, it
requires significant memory to train large models and intuition on which layers
to add adapters. In this paper, we propose a novel method,
$\texttt{WeightLoRA}$, which overcomes this issue by adaptive selection of the
most critical $\texttt{LoRA}$ heads throughout the optimization process. As a
result, we can significantly reduce the number of trainable parameters while
maintaining the capability to obtain consistent or even superior metric values.
We conduct experiments for a series of competitive benchmarks and DeBERTa,
BART, and Llama models, comparing our method with different adaptive
approaches. The experimental results demonstrate the efficacy of
$\texttt{WeightLoRA}$ and the superior performance of $\texttt{WeightLoRA+}$ in
almost all cases.

</details>


### [243] [Knowledge Graph Completion by Intermediate Variables Regularization](https://arxiv.org/abs/2506.02749)
*Changyi Xiao,Yixin Cao*

Main category: cs.LG

TL;DR: 该论文总结了现有的基于张量分解的知识图谱补全模型，并提出了一种新的正则化方法以减少过拟合，通过理论分析和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于张量分解的知识图谱补全模型虽然表现优异，但容易过拟合。当前的正则化方法仅最小化嵌入的范数，效果不佳，因此需要一种更有效的正则化方法。

Method: 提出了一种新的正则化方法，通过最小化预测张量计算过程中涉及的中间变量的范数，以降低预测张量的迹范数，从而减少过拟合。该方法适用于大多数基于张量分解的模型，并确保计算可行。

Result: 实验验证了所提正则化方法的有效性，并支持了理论分析的可靠性。代码已开源。

Conclusion: 论文提出的正则化方法有效减少了基于张量分解的知识图谱补全模型的过拟合问题，并通过理论和实验验证了其优越性。

Abstract: Knowledge graph completion (KGC) can be framed as a 3-order binary tensor
completion task. Tensor decomposition-based (TDB) models have demonstrated
strong performance in KGC. In this paper, we provide a summary of existing TDB
models and derive a general form for them, serving as a foundation for further
exploration of TDB models. Despite the expressiveness of TDB models, they are
prone to overfitting. Existing regularization methods merely minimize the norms
of embeddings to regularize the model, leading to suboptimal performance.
Therefore, we propose a novel regularization method for TDB models that
addresses this limitation. The regularization is applicable to most TDB models
and ensures tractable computation. Our method minimizes the norms of
intermediate variables involved in the different ways of computing the
predicted tensor. To support our regularization method, we provide a
theoretical analysis that proves its effect in promoting low trace norm of the
predicted tensor to reduce overfitting. Finally, we conduct experiments to
verify the effectiveness of our regularization technique as well as the
reliability of our theoretical analysis. The code is available at
https://github.com/changyi7231/IVR.

</details>


### [244] [Investigating Mask-aware Prototype Learning for Tabular Anomaly Detection](https://arxiv.org/abs/2506.02757)
*Ruiying Lu,Jinhan Liu,Chuan Du,Dandan Guo*

Main category: cs.LG

TL;DR: 该论文提出了一种结合掩码建模和原型学习的表格异常检测方法，通过解耦表示学习和全局原型提取提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的表格异常检测方法存在表示纠缠和缺乏全局相关性建模的问题，限制了检测性能。

Method: 方法分为两部分：(1)在编码阶段，通过正交基向量在数据空间和投影空间进行掩码建模，学习解耦的正常模式；(2)在解码阶段，并行解码多个掩码表示以进行重建，并学习关联原型以提取正常特征相关性。

Result: 在20个表格基准上的定量和定性实验证明了该模型的有效性和可解释性。

Conclusion: 通过分布匹配视角，将投影空间学习和关联原型学习表述为最优传输问题，并利用校准距离细化异常分数，该方法显著提升了表格异常检测性能。

Abstract: Tabular anomaly detection, which aims at identifying deviant samples, has
been crucial in a variety of real-world applications, such as medical disease
identification, financial fraud detection, intrusion monitoring, etc. Although
recent deep learning-based methods have achieved competitive performances,
these methods suffer from representation entanglement and the lack of global
correlation modeling, which hinders anomaly detection performance. To tackle
the problem, we incorporate mask modeling and prototype learning into tabular
anomaly detection. The core idea is to design learnable masks by disentangled
representation learning within a projection space and extracting normal
dependencies as explicit global prototypes. Specifically, the overall model
involves two parts: (i) During encoding, we perform mask modeling in both the
data space and projection space with orthogonal basis vectors for learning
shared disentangled normal patterns; (ii) During decoding, we decode multiple
masked representations in parallel for reconstruction and learn association
prototypes to extract normal characteristic correlations. Our proposal derives
from a distribution-matching perspective, where both projection space learning
and association prototype learning are formulated as optimal transport
problems, and the calibration distances are utilized to refine the anomaly
scores. Quantitative and qualitative experiments on 20 tabular benchmarks
demonstrate the effectiveness and interpretability of our model.

</details>


### [245] [Accelerating Model-Based Reinforcement Learning using Non-Linear Trajectory Optimization](https://arxiv.org/abs/2506.02767)
*Marco Calì,Giulio Giacomuzzo,Ruggero Carli,Alberto Dalla Libera*

Main category: cs.LG

TL;DR: EB-MC-PILCO通过结合iLQR加速MC-PILCO的策略优化收敛，减少执行时间并保持高成功率。


<details>
  <summary>Details</summary>
Motivation: 针对MC-PILCO策略优化收敛慢的问题，提出结合快速轨迹优化方法iLQR以提升效率。

Method: 提出EB-MC-PILCO，利用iLQR生成探索性轨迹并初始化策略，减少优化步骤。

Result: 实验显示EB-MC-PILCO在cart-pole任务中收敛更快，执行时间减少45.9%，成功率100%。

Conclusion: EB-MC-PILCO有效加速MC-PILCO的收敛，同时保持高成功率，适用于非线性系统。

Abstract: This paper addresses the slow policy optimization convergence of Monte Carlo
Probabilistic Inference for Learning Control (MC-PILCO), a state-of-the-art
model-based reinforcement learning (MBRL) algorithm, by integrating it with
iterative Linear Quadratic Regulator (iLQR), a fast trajectory optimization
method suitable for nonlinear systems. The proposed method, Exploration-Boosted
MC-PILCO (EB-MC-PILCO), leverages iLQR to generate informative, exploratory
trajectories and initialize the policy, significantly reducing the number of
required optimization steps. Experiments on the cart-pole task demonstrate that
EB-MC-PILCO accelerates convergence compared to standard MC-PILCO, achieving up
to $\bm{45.9\%}$ reduction in execution time when both methods solve the task
in four trials. EB-MC-PILCO also maintains a $\bm{100\%}$ success rate across
trials while solving the task faster, even in cases where MC-PILCO converges in
fewer iterations.

</details>


### [246] [CART-based Synthetic Tabular Data Generation for Imbalanced Regression](https://arxiv.org/abs/2506.02811)
*António Pedro Pinheiro,Rita P. Ribeiro*

Main category: cs.LG

TL;DR: 该论文提出了一种基于CART的合成数据生成方法，用于解决回归任务中目标分布不平衡的问题，该方法在性能和透明度上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 回归任务中目标分布不平衡是一个重要挑战，现有方法如随机采样和SMOTE依赖于分类技术，存在阈值设定的任意性，而生成模型如GAN和VAE计算成本高且可解释性差。

Method: 论文提出了一种基于CART的合成数据生成方法，结合相关性和密度机制指导稀疏区域的采样，采用无阈值、特征驱动的生成过程。

Result: 实验结果表明，该方法在预测极端目标值上与其他重采样和生成策略性能相当，但执行速度更快、透明度更高。

Conclusion: 该方法作为一种透明、可扩展的数据级策略，在改善不平衡领域回归模型方面具有潜力。

Abstract: Handling imbalanced target distributions in regression tasks remains a
significant challenge in tabular data settings where underrepresented regions
can hinder model performance. Among data-level solutions, some proposals, such
as random sampling and SMOTE-based approaches, propose adapting classification
techniques to regression tasks. However, these methods typically rely on crisp,
artificial thresholds over the target variable, a limitation inherited from
classification settings that can introduce arbitrariness, often leading to
non-intuitive and potentially misleading problem formulations. While recent
generative models, such as GANs and VAEs, provide flexible sample synthesis,
they come with high computational costs and limited interpretability. In this
study, we propose adapting an existing CART-based synthetic data generation
method, tailoring it for imbalanced regression. The new method integrates
relevance and density-based mechanisms to guide sampling in sparse regions of
the target space and employs a threshold-free, feature-driven generation
process. Our experimental study focuses on the prediction of extreme target
values across benchmark datasets. The results indicate that the proposed method
is competitive with other resampling and generative strategies in terms of
performance, while offering faster execution and greater transparency. These
results highlight the method's potential as a transparent, scalable data-level
strategy for improving regression models in imbalanced domains.

</details>


### [247] [Sheaves Reloaded: A Directional Awakening](https://arxiv.org/abs/2506.02842)
*Stefano Fiorini,Hakan Aktas,Iulia Duta,Stefano Coniglio,Pietro Morerio,Alessio Del Bue,Pietro Liò*

Main category: cs.LG

TL;DR: 论文提出了一种新型的定向层状神经网络（DSNN），通过引入定向层状结构，显著提升了复杂关系数据的建模能力。


<details>
  <summary>Details</summary>
Motivation: 现有的层状神经网络（SNNs）在处理具有方向性的图数据时表现不足，而方向性在许多实际应用中至关重要。

Method: 论文提出了一种定向层状结构（Directed Cellular Sheaf），并基于此定义了一个新的层状拉普拉斯算子（Directed Sheaf Laplacian），作为DSNN的核心架构。

Result: 在九个真实世界基准测试中，DSNN的表现均优于基线方法。

Conclusion: DSNN通过嵌入方向性偏置，显著提升了图学习任务的性能，为复杂关系数据建模提供了新的解决方案。

Abstract: Sheaf Neural Networks (SNNs) represent a powerful generalization of Graph
Neural Networks (GNNs) that significantly improve our ability to model complex
relational data. While directionality has been shown to substantially boost
performance in graph learning tasks and is key to many real-world applications,
existing SNNs fall short in representing it. To address this limitation, we
introduce the Directed Cellular Sheaf, a special type of cellular sheaf
designed to explicitly account for edge orientation. Building on this
structure, we define a new sheaf Laplacian, the Directed Sheaf Laplacian, which
captures both the graph's topology and its directional information. This
operator serves as the backbone of the Directed Sheaf Neural Network (DSNN),
the first SNN model to embed a directional bias into its architecture.
Extensive experiments on nine real-world benchmarks show that DSNN consistently
outperforms baseline methods.

</details>


### [248] [BNPO: Beta Normalization Policy Optimization](https://arxiv.org/abs/2506.02864)
*Changyi Xiao,Mengdi Zhang,Yixin Cao*

Main category: cs.LG

TL;DR: 本文提出了一种名为Beta归一化策略优化（BNPO）的新方法，通过动态调整Beta分布参数自适应归一化奖励，解决了现有策略优化方法在训练过程中因静态归一化导致的梯度估计不稳定问题，显著提升了大型语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的策略优化方法（如REINFORCE、GRPO）在归一化奖励时存在静态策略无法适应策略动态更新的缺陷，导致梯度估计不稳定，影响训练效果。

Method: 提出BNPO方法：利用动态参数更新的Beta分布自适应归一化奖励，与策略分布变化对齐，并引入优势分解机制扩展至复杂奖励系统。

Result: 理论证明BNPO具有降低方差的特性，实验显示其在推理任务上达到策略优化方法的SOTA性能。代码已开源。

Conclusion: BNPO通过动态奖励归一化实现了更稳定的训练，泛化了现有方法，并为复杂奖励系统提供了有效解决方案。

Abstract: Recent studies, including DeepSeek-R1 and Kimi-k1.5, have demonstrated that
reinforcement learning with rule-based, binary-valued reward functions can
significantly enhance the reasoning capabilities of large language models.
These models primarily utilize REINFORCE-based policy optimization techniques,
such as REINFORCE with baseline and group relative policy optimization (GRPO).
However, a key limitation remains: current policy optimization methods either
neglect reward normalization or employ static normalization strategies, which
fail to adapt to the dynamic nature of policy updates during training. This may
result in unstable gradient estimates and hinder training stability. To address
this issue, we propose Beta Normalization Policy Optimization (BNPO), a novel
policy optimization method that adaptively normalizes rewards using a Beta
distribution with dynamically updated parameters. BNPO aligns the normalization
with the changing policy distribution, enabling more precise and lower-variance
gradient estimation, which in turn promotes stable training dynamics. We
provide theoretical analysis demonstrating BNPO's variance-reducing properties
and show that it generalizes both REINFORCE and GRPO under binary-valued reward
settings. Furthermore, we introduce an advantage decomposition mechanism to
extend BNPO's applicability to more complex reward systems. Experimental
results confirm that BNPO achieves state-of-the-art performance among policy
optimization methods on reasoning tasks. The code is available at
https://github.com/changyi7231/BNPO.

</details>


### [249] [A Continual Offline Reinforcement Learning Benchmark for Navigation Tasks](https://arxiv.org/abs/2506.02883)
*Anthony Kobanda,Odalric-Ambrym Maillard,Rémy Portelas*

Main category: cs.LG

TL;DR: 该论文提出了一个用于持续强化学习的视频游戏导航场景基准，旨在解决灾难性遗忘、任务适应和内存效率等关键挑战。


<details>
  <summary>Details</summary>
Motivation: 在机器人或视频游戏模拟等领域，自主代理需要适应不断变化的任务而不忘记之前的任务。持续强化学习面临灾难性遗忘、任务适应和内存效率等挑战，目前缺乏统一的基准测试。

Method: 作者引入了一个包含多种任务和数据集的基准测试，定义了评估协议和指标，并对比了现有最先进的基线方法。

Result: 该基准测试不仅促进了可重复研究，加速了游戏领域持续强化学习的进展，还为生产流程提供了可重复的框架。

Conclusion: 该基准填补了文献空白，帮助研究人员和实践者识别并应用有效的方法，推动持续强化学习的发展。

Abstract: Autonomous agents operating in domains such as robotics or video game
simulations must adapt to changing tasks without forgetting about the previous
ones. This process called Continual Reinforcement Learning poses non-trivial
difficulties, from preventing catastrophic forgetting to ensuring the
scalability of the approaches considered. Building on recent advances, we
introduce a benchmark providing a suite of video-game navigation scenarios,
thus filling a gap in the literature and capturing key challenges :
catastrophic forgetting, task adaptation, and memory efficiency. We define a
set of various tasks and datasets, evaluation protocols, and metrics to assess
the performance of algorithms, including state-of-the-art baselines. Our
benchmark is designed not only to foster reproducible research and to
accelerate progress in continual reinforcement learning for gaming, but also to
provide a reproducible framework for production pipelines -- helping
practitioners to identify and to apply effective approaches.

</details>


### [250] [Overcoming Challenges of Partial Client Participation in Federated Learning : A Comprehensive Review](https://arxiv.org/abs/2506.02887)
*Mrinmay Sen,Shruti Aparna,Rohit Agarwal,Chalavadi Krishna Mohan*

Main category: cs.LG

TL;DR: 该论文综述了联邦学习中部分客户参与的挑战及现有解决方案，分析了其优缺点。


<details>
  <summary>Details</summary>
Motivation: 现有研究多假设全客户参与，而实际场景中部分客户参与更为常见，却缺乏相关研究。

Method: 通过理论分析和实证研究，对现有应对部分客户参与的联邦学习方法进行分类和评估。

Result: 提供了对现有方法的全面分析，并指出各自的优缺点。

Conclusion: 部分客户参与是联邦学习中的重要挑战，需更多研究以优化实际应用。

Abstract: Federated Learning (FL) is a learning mechanism that falls under the
distributed training umbrella, which collaboratively trains a shared global
model without disclosing the raw data from different clients. This paper
presents an extensive survey on the impact of partial client participation in
federated learning. While much of the existing research focuses on addressing
issues such as generalization, robustness, and fairness caused by data
heterogeneity under the assumption of full client participation, limited
attention has been given to the practical and theoretical challenges arising
from partial client participation, which is common in real-world scenarios.
This survey provides an in-depth review of existing FL methods designed to cope
with partial client participation. We offer a comprehensive analysis supported
by theoretical insights and empirical findings, along with a structured
categorization of these methods, highlighting their respective advantages and
disadvantages.

</details>


### [251] [Scaling Fine-Grained MoE Beyond 50B Parameters: Empirical Evaluation and Practical Insights](https://arxiv.org/abs/2506.02890)
*Jakub Krajewski,Marcin Chochowski,Daniel Korzekwa*

Main category: cs.LG

TL;DR: 该论文研究了细粒度混合专家（MoE）架构在大型语言模型中的应用，通过实验证明其在模型收敛速度和性能上的优势。


<details>
  <summary>Details</summary>
Motivation: 研究细粒度MoE架构的动机在于其能更高效地扩展大型语言模型，并提升模型的收敛速度和质量。

Method: 论文提出了一套训练方法，并对细粒度MoE进行了全面的实证评估，比较了其与标准MoE配置的扩展性。

Result: 实验结果表明，在最大规模下，细粒度MoE在验证损失和下游任务准确率上表现更优。

Conclusion: 该研究为未来大规模模型中细粒度MoE的应用提供了实证基础和实用见解。

Abstract: Mixture of Experts (MoE) architectures have emerged as pivotal for scaling
Large Language Models (LLMs) efficiently. Fine-grained MoE approaches -
utilizing more numerous, smaller experts - have demonstrated potential in
improving model convergence and quality. This work proposes a set of training
recipes and provides a comprehensive empirical evaluation of fine-grained MoE,
directly comparing its scaling properties against standard MoE configurations
for models with up to 56B total (17B active) parameters. We investigate
convergence speed, model performance on downstream benchmarks, and practical
training considerations across various setups. Overall, at the largest scale we
show that fine-grained MoE achieves better validation loss and higher accuracy
across a set of downstream benchmarks. This study offers empirical grounding
and practical insights for leveraging fine-grained MoE in the development of
future large-scale models.

</details>


### [252] [Sociodynamics-inspired Adaptive Coalition and Client Selection in Federated Learning](https://arxiv.org/abs/2506.02897)
*Alessandro Licciardi,Roberta Raineri,Anton Proskurnikov,Lamberto Rondoni,Lorenzo Zino*

Main category: cs.LG

TL;DR: 本文提出了一种名为Federated Coalition Variance Reduction with Boltzmann Exploration（FCVR-BE）的联邦学习算法，通过动态聚类和方差最小化选择策略，有效解决了客户端数据异构性问题，提升了模型性能和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在隐私保护协作模型训练中具有优势，但客户端数据异构性会严重降低模型性能。本文旨在通过受社交网络意见动态启发的创新方法解决这一挑战。

Method: 提出FCVR-BE算法：1) 客户端根据渐近一致性动态形成非重叠聚类；2) 从每个聚类中选择一个客户端，以最小化其模型更新的预期方差。

Result: 实验表明，在异构场景下，该算法优于现有FL方法，实现了更高的准确性和更快的收敛速度。

Conclusion: 基于社交网络动态的聚类和方差优化策略能有效缓解数据异构性对联邦学习的影响，验证了该方法的有效性。

Abstract: Federated Learning (FL) enables privacy-preserving collaborative model
training, yet its practical strength is often undermined by client data
heterogeneity, which severely degrades model performance. This paper proposes
that data heterogeneity across clients' distributions can be effectively
addressed by adopting an approach inspired by opinion dynamics over temporal
social networks. We introduce \shortname (Federated Coalition Variance
Reduction with Boltzmann Exploration), a variance-reducing selection algorithm
in which (1) clients dynamically organize into non-overlapping clusters based
on asymptotic agreements, and (2) from each cluster, one client is selected to
minimize the expected variance of its model update. Our experiments show that
in heterogeneous scenarios our algorithm outperforms existing FL algorithms,
yielding more accurate results and faster convergence, validating the efficacy
of our approach.

</details>


### [253] [From Theory to Practice with RAVEN-UCB: Addressing Non-Stationarity in Multi-Armed Bandits through Variance Adaptation](https://arxiv.org/abs/2506.02933)
*Junyi Fang,Yuxun Chen,Yuxin Chen,Chen Zhang*

Main category: cs.LG

TL;DR: RAVEN-UCB算法通过方差感知适应在非静态环境中优化多臂老虎机问题，实现了比UCB1和UCB-V更紧的遗憾界，并在实验中表现出优越性。


<details>
  <summary>Details</summary>
Motivation: 非静态环境中的多臂老虎机问题具有挑战性，因为奖励分布会动态变化。现有算法如UCB1和UCB-V在动态环境中表现不佳，需要一种更高效且理论严谨的算法。

Method: RAVEN-UCB结合了三种创新：(1) 方差驱动的探索，(2) 自适应控制，(3) 常数时间递归更新。这些方法使其在非静态环境中具有更高的效率和鲁棒性。

Result: RAVEN-UCB在合成和物流场景的非静态模式（如分布变化、周期性偏移和临时波动）中表现优于现有基线算法，验证了其理论和实践上的鲁棒性。

Conclusion: RAVEN-UCB在非静态环境中表现出色，通过方差感知和自适应控制实现了更优的遗憾界，适用于动态变化的实际应用场景。

Abstract: The Multi-Armed Bandit (MAB) problem is challenging in non-stationary
environments where reward distributions evolve dynamically. We introduce
RAVEN-UCB, a novel algorithm that combines theoretical rigor with practical
efficiency via variance-aware adaptation. It achieves tighter regret bounds
than UCB1 and UCB-V, with gap-dependent regret of order $K \sigma_{\max}^2 \log
T / \Delta$ and gap-independent regret of order $\sqrt{K T \log T}$. RAVEN-UCB
incorporates three innovations: (1) variance-driven exploration using
$\sqrt{\hat{\sigma}_k^2 / (N_k + 1)}$ in confidence bounds, (2) adaptive
control via $\alpha_t = \alpha_0 / \log(t + \epsilon)$, and (3) constant-time
recursive updates for efficiency. Experiments across non-stationary patterns -
distributional changes, periodic shifts, and temporary fluctuations - in
synthetic and logistics scenarios demonstrate its superiority over
state-of-the-art baselines, confirming theoretical and practical robustness.

</details>


### [254] [MTL-KD: Multi-Task Learning Via Knowledge Distillation for Generalizable Neural Vehicle Routing Solver](https://arxiv.org/abs/2506.02935)
*Yuepeng Zheng,Fu Luo,Zhenkun Wang,Yaoxin Wu,Yu Zhou*

Main category: cs.LG

TL;DR: 该论文提出了一种基于知识蒸馏的多任务学习方法（MTL-KD），用于训练具有强泛化能力的重型解码器模型，以解决多种车辆路径问题（VRP）变体，并通过实验验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于强化学习（RL）的多任务方法只能在小规模问题上训练轻量级解码器模型，其泛化能力有限。为了克服这一限制，本文旨在开发一种能够高效训练重型解码器模型并具有强泛化能力的方法。

Method: 本文提出了一种基于知识蒸馏的多任务学习方法（MTL-KD），通过将多个不同的基于RL的单任务模型的政策知识转移到一个单一的重型解码器模型中，实现无标签训练。此外，还引入了一种灵活的推理策略——随机重排序重建（R3C），以进一步提升多任务模型的性能。

Result: 在6个已知和10个未知的VRP变体上（节点数高达1000个）的实验结果表明，该方法在统一和真实世界基准测试中均表现出优越的性能，展示了强大的泛化能力。

Conclusion: 本文提出的MTL-KD方法通过知识蒸馏和灵活的推理策略，显著提升了多任务模型在解决多种VRP变体时的泛化能力和性能，为神经组合优化领域提供了新的解决方案。

Abstract: Multi-Task Learning (MTL) in Neural Combinatorial Optimization (NCO) is a
promising approach to train a unified model capable of solving multiple Vehicle
Routing Problem (VRP) variants. However, existing Reinforcement Learning
(RL)-based multi-task methods can only train light decoder models on
small-scale problems, exhibiting limited generalization ability when solving
large-scale problems. To overcome this limitation, this work introduces a novel
multi-task learning method driven by knowledge distillation (MTL-KD), which
enables the efficient training of heavy decoder models with strong
generalization ability. The proposed MTL-KD method transfers policy knowledge
from multiple distinct RL-based single-task models to a single heavy decoder
model, facilitating label-free training and effectively improving the model's
generalization ability across diverse tasks. In addition, we introduce a
flexible inference strategy termed Random Reordering Re-Construction (R3C),
which is specifically adapted for diverse VRP tasks and further boosts the
performance of the multi-task model. Experimental results on 6 seen and 10
unseen VRP variants with up to 1000 nodes indicate that our proposed method
consistently achieves superior performance on both uniform and real-world
benchmarks, demonstrating robust generalization abilities.

</details>


### [255] [QKV Projections Require a Fraction of Their Memory](https://arxiv.org/abs/2506.02939)
*Malik Khalaf,Yara Shamshoum,Nitzan Hodos,Yuval Sieradzki,Assaf Schuster*

Main category: cs.LG

TL;DR: 论文提出PAMM技术，通过压缩注意力层中的Q、K、V投影内存，最高可减少512倍内存占用，同时保持或提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注于近似缩放点积运算，而忽视了计算Q、K、V张量的线性投影内存消耗问题。

Method: 提出Point-Approximate Matrix Multiplication (PAMM)，一种新型张量压缩技术，用于减少注意力层中Q、K、V投影的内存占用。

Result: PAMM技术最高可减少512倍内存占用，且模型最终困惑度相似或更好，与FlashAttention等技术完全兼容。

Conclusion: PAMM是一种实用且互补的方法，可有效提升大语言模型训练时的内存效率。

Abstract: The Multi-Head Attention mechanism is central to LLM operation, and multiple
works target its compute and memory efficiency during training. While most
works focus on approximating the scaled dot product, the memory consumption of
the linear projections that compute the $Q$, $K$, and $V$ tensors from the
input $x$ is often overlooked. To address this, we propose Point-Approximate
Matrix Multiplication (PAMM), a novel tensor compression technique that reduces
memory consumption of the $Q,K,V$ projections in attention layers by a factor
of up to $\times 512$, effectively erasing their memory footprint, while
achieving similar or better final perplexity. PAMM is fully composable with
efficient attention techniques such as FlashAttention, making it a practical
and complementary method for memory-efficient LLM training.

</details>


### [256] [Abstract Counterfactuals for Language Model Agents](https://arxiv.org/abs/2506.02946)
*Edoardo Pona,Milad Kazemi,Yali Du,David Watson,Nicola Paoletti*

Main category: cs.LG

TL;DR: 论文提出了一种名为'抽象反事实'的框架，用于解决语言模型代理在反事实推理中的挑战，通过关注动作和交互的高层特征，避免了传统词级方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的反事实推理方法主要关注词级反事实，但由于语言模型代理的开放动作空间和上下文相关的词义变化，这些方法往往不适用，容易产生偏差或无意义的反事实。

Method: 引入'抽象反事实'框架，强调动作和交互的高层特征，结合词级和潜在空间干预，进行反事实推理。

Result: 实验表明，该方法在文本游戏和反事实文本生成中能产生一致且有意义的反事实，同时减少了词级方法的副作用。

Conclusion: 抽象反事实框架为语言模型代理提供了一种更有效的反事实推理方法，适用于用户相关特征的分析和评估。

Abstract: Counterfactual inference is a powerful tool for analysing and evaluating
autonomous agents, but its application to language model (LM) agents remains
challenging. Existing work on counterfactuals in LMs has primarily focused on
token-level counterfactuals, which are often inadequate for LM agents due to
their open-ended action spaces. Unlike traditional agents with fixed, clearly
defined action spaces, the actions of LM agents are often implicit in the
strings they output, making their action spaces difficult to define and
interpret. Furthermore, the meanings of individual tokens can shift depending
on the context, adding complexity to token-level reasoning and sometimes
leading to biased or meaningless counterfactuals. We introduce \emph{Abstract
Counterfactuals}, a framework that emphasises high-level characteristics of
actions and interactions within an environment, enabling counterfactual
reasoning tailored to user-relevant features. Our experiments demonstrate that
the approach produces consistent and meaningful counterfactuals while
minimising the undesired side effects of token-level methods. We conduct
experiments on text-based games and counterfactual text generation, while
considering both token-level and latent-space interventions.

</details>


### [257] [Interaction Field Matching: Overcoming Limitations of Electrostatic Models](https://arxiv.org/abs/2506.02950)
*Stepan I. Manukhov,Alexander Kolesov,Vladimir V. Palyulin,Alexander Korotin*

Main category: cs.LG

TL;DR: 本文提出了一种名为交互场匹配（IFM）的新方法，作为静电匹配（EFM）的扩展，解决了EFM中建模复杂静电场的难题，并通过物理启发的强相互作用场设计提升了数据生成和传输性能。


<details>
  <summary>Details</summary>
Motivation: 静电匹配（EFM）作为一种新型物理启发的数据生成和传输方法，需要建模复杂的静电场，这在技术上具有挑战性。本文旨在通过更通用的交互场匹配（IFM）解决这一问题。

Method: 本文提出了交互场匹配（IFM），扩展了EFM方法，允许使用超出静电场的通用交互场，并设计了一种受夸克-反夸克强相互作用启发的特定交互场实现。

Result: 实验表明，IFM在一系列玩具和图像数据传输问题上表现出色，解决了EFM中建模静电场的困难。

Conclusion: IFM作为EFM的扩展，不仅解决了原有方法的局限性，还通过物理启发的交互场设计提升了数据生成和传输的性能。

Abstract: Electrostatic field matching (EFM) has recently appeared as a novel
physics-inspired paradigm for data generation and transfer using the idea of an
electric capacitor. However, it requires modeling electrostatic fields using
neural networks, which is non-trivial because of the necessity to take into
account the complex field outside the capacitor plates. In this paper, we
propose Interaction Field Matching (IFM), a generalization of EFM which allows
using general interaction fields beyond the electrostatic one. Furthermore,
inspired by strong interactions between quarks and antiquarks in physics, we
design a particular interaction field realization which solves the problems
which arise when modeling electrostatic fields in EFM. We show the performance
on a series of toy and image data transfer problems.

</details>


### [258] [Memory-Efficient and Privacy-Preserving Collaborative Training for Mixture-of-Experts LLMs](https://arxiv.org/abs/2506.02965)
*Ze Yu Zhang,Bolin Ding,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: PC-MoE是一种隐私保护的协作式混合专家模型，通过分布式计算和隐私保护机制，使多方可联合训练更强大的大语言模型，同时减少GPU内存使用并防止数据泄露。


<details>
  <summary>Details</summary>
Motivation: 当前，混合专家模型（MoE）在大语言模型（LLM）中的应用越来越广泛。然而，单个参与者可能因GPU内存和数据资源有限而无法独立训练高性能模型。此外，隐私保护的需求也限制了多方协作的可能性。因此，本文旨在开发一种既能保护隐私又能高效协作的MoE训练框架。

Method: 提出了隐私保护的协作式混合专家模型（PC-MoE），利用MoE架构的稀疏性实现内存高效的分布式协作训练。该方法通过将训练数据、部分前向传播信号和梯度保留在本地，保护了各参与方的数据隐私。

Result: 在七个流行的LLM基准测试中，PC-MoE几乎达到了（有时甚至超过）集中式模型的性能和收敛速度，同时减少了近70%的GPU峰值内存使用，并且完全抵御了重构攻击。

Conclusion: PC-MoE成功结合了分布式计算的优势和强大的隐私保护机制，打破了隐私保护与任务准确性之间的权衡，为多方协作训练高性能LLM提供了可行的解决方案。

Abstract: Mixture-of-Experts (MoE) has been gaining popularity due to its successful
adaptation to large language models (LLMs). In this work, we introduce
Privacy-preserving Collaborative Mixture-of-Experts (PC-MoE), which leverages
the sparsity of the MoE architecture for memory-efficient decentralized
collaborative LLM training, enabling multiple parties with limited GPU-memory
and data resources to collectively train more capable LLMs than they could
achieve individually. At the same time, this approach protects training data
privacy of each participant by keeping training data, as well as parts of the
forward pass signal and gradients locally within each party. By design, PC-MoE
synergistically combines the strengths of distributed computation with strong
confidentiality assurances. Unlike most privacy-preserving schemes, which pay
for confidentiality with lower task accuracy, our framework breaks that
trade-off: across seven popular LLM benchmarks, it almost matches (and
sometimes exceeds) the performance and convergence rate of a fully centralized
model, enjoys near 70% peak GPU RAM reduction, while being fully robust against
reconstruction attacks.

</details>


### [259] [Computation- and Communication-Efficient Online FL for Resource-Constrained Aerial Vehicles](https://arxiv.org/abs/2506.02972)
*Md-Ferdous Pervej,Richeng Jin,Md Moin Uddin Chowdhury,Simran Singh,İsmail Güvenç,Huaiyu Dai*

Main category: cs.LG

TL;DR: 本文提出了一种计算和通信高效的在线空中联邦学习算法（2CEOAFL），用于解决空中连接车辆（ACV）辅助边缘计算中的隐私保护和资源受限问题。


<details>
  <summary>Details</summary>
Motivation: 随着ACV在移动过程中不断收集新数据，需要在线学习并优化其飞行轨迹。同时，ACV资源有限，需要高效的计算和通信解决方案。

Method: 提出2CEOAFL算法，包括模型剪枝、训练剪枝后的模型，以及概率量化和卸载梯度到中央服务器。

Result: 模拟结果显示，2CEOAFL在性能上与未剪枝和未量化的低效算法相当。

Conclusion: 2CEOAFL算法在资源受限的ACV中实现了高效的计算和通信，同时保持了良好的学习性能。

Abstract: Privacy-preserving distributed machine learning (ML) and aerial connected
vehicle (ACV)-assisted edge computing have drawn significant attention lately.
Since the onboard sensors of ACVs can capture new data as they move along their
trajectories, the continual arrival of such 'newly' sensed data leads to online
learning and demands carefully crafting the trajectories. Besides, as typical
ACVs are inherently resource-constrained, computation- and
communication-efficient ML solutions are needed. Therefore, we propose a
computation- and communication-efficient online aerial federated learning
(2CEOAFL) algorithm to take the benefits of continual sensed data and limited
onboard resources of the ACVs. In particular, considering independently owned
ACVs act as selfish data collectors, we first model their trajectories
according to their respective time-varying data distributions. We then propose
a 2CEOAFL algorithm that allows the flying ACVs to (a) prune the received dense
ML model to make it shallow, (b) train the pruned model, and (c)
probabilistically quantize and offload their trained accumulated gradients to
the central server (CS). Our extensive simulation results show that the
proposed 2CEOAFL algorithm delivers comparable performances to its non-pruned
and nonquantized, hence, computation- and communication-inefficient
counterparts.

</details>


### [260] [On the Robustness of Tabular Foundation Models: Test-Time Attacks and In-Context Defenses](https://arxiv.org/abs/2506.02978)
*Mohamed Djilani,Thibault Simonetto,Karim Tit,Florian Tambon,Paul Récamier,Salah Ghamizi,Maxime Cordy,Mike Papadakis*

Main category: cs.LG

TL;DR: 该研究探讨了表格基础模型（FM）在对抗性攻击下的脆弱性，并提出了一种上下文对抗训练策略以提高其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管表格基础模型（如TabPFN和TabICL）在无需梯度更新或微调的情况下表现出色，但其对抗性攻击的鲁棒性尚未得到充分研究。本文旨在填补这一空白。

Method: 研究通过优化权重（对抗性微调）或上下文（对抗性上下文学习）来强化表格FM，并引入了一种上下文对抗训练策略，逐步用对抗性扰动实例替换上下文，而无需更新模型权重。

Result: 在金融、网络安全和医疗三个基准测试中，研究发现即使训练上下文固定，小的结构化扰动也会显著降低预测准确性。此外，表格FM可被重新用于生成对传统模型（如随机森林和XGBoost）的可转移规避攻击。

Conclusion: 表格FM既是对抗性攻击的目标，也是其来源，突显了在这一新兴范式中采用鲁棒训练和评估实践的紧迫性。

Abstract: Recent tabular Foundational Models (FM) such as TabPFN and TabICL, leverage
in-context learning to achieve strong performance without gradient updates or
fine-tuning. However, their robustness to adversarial manipulation remains
largely unexplored. In this work, we present a comprehensive study of the
adversarial vulnerabilities of tabular FM, focusing on both their fragility to
targeted test-time attacks and their potential misuse as adversarial tools. We
show on three benchmarks in finance, cybersecurity and healthcare, that small,
structured perturbations to test inputs can significantly degrade prediction
accuracy, even when training context remain fixed. Additionally, we demonstrate
that tabular FM can be repurposed to generate transferable evasion to
conventional models such as random forests and XGBoost, and on a lesser extent
to deep tabular models. To improve tabular FM, we formulate the robustification
problem as an optimization of the weights (adversarial fine-tuning), or the
context (adversarial in-context learning). We introduce an in-context
adversarial training strategy that incrementally replaces the context with
adversarial perturbed instances, without updating model weights. Our approach
improves robustness across multiple tabular benchmarks. Together, these
findings position tabular FM as both a target and a source of adversarial
threats, highlighting the urgent need for robust training and evaluation
practices in this emerging paradigm.

</details>


### [261] [Implicit Regularization of the Deep Inverse Prior Trained with Inertia](https://arxiv.org/abs/2506.02986)
*Nathan Buskulic,Jalal Fadil,Yvain Quéau*

Main category: cs.LG

TL;DR: 该论文为自监督神经网络在逆问题中的应用提供了收敛性和恢复性保证，研究了连续时间和离散时间情况下的训练方法，并展示了加速指数收敛速率。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络在解决逆问题时缺乏理论保证，本文旨在填补这一空白，提供收敛性和恢复性保证。

Method: 研究了连续时间（动态系统轨迹）和离散时间（惯性算法与自适应步长）情况下的训练方法，结合粘性和几何Hessian驱动的阻尼。

Result: 在连续时间情况下，网络训练实现了最优的加速指数收敛速率；在离散时间情况下，训练网络具有类似的恢复性保证，但收敛速率稍逊。

Conclusion: 本文为自监督神经网络在逆问题中的应用提供了理论支持，展示了其在连续和离散时间情况下的收敛性和恢复性表现。

Abstract: Solving inverse problems with neural networks benefits from very few
theoretical guarantees when it comes to the recovery guarantees. We provide in
this work convergence and recovery guarantees for self-supervised neural
networks applied to inverse problems, such as Deep Image/Inverse Prior, and
trained with inertia featuring both viscous and geometric Hessian-driven
dampings. We study both the continuous-time case, i.e., the trajectory of a
dynamical system, and the discrete case leading to an inertial algorithm with
an adaptive step-size. We show in the continuous-time case that the network can
be trained with an optimal accelerated exponential convergence rate compared to
the rate obtained with gradient flow. We also show that training a network with
our inertial algorithm enjoys similar recovery guarantees though with a less
sharp linear convergence rate.

</details>


### [262] [Protein Inverse Folding From Structure Feedback](https://arxiv.org/abs/2506.03028)
*Junde Xu,Zijun Gao,Xinyi Zhou,Jie Hu,Xingyi Cheng,Le Song,Guangyong Chen,Pheng-Ann Heng,Jiezhong Qiu*

Main category: cs.LG

TL;DR: 本文提出了一种利用直接偏好优化（DPO）改进蛋白质逆折叠模型的新方法，通过结构反馈显著提升了序列恢复和结构相似性。


<details>
  <summary>Details</summary>
Motivation: 蛋白质逆折叠问题（设计能折叠成特定三维结构的氨基酸序列）对生物技术应用至关重要。现有方法在序列恢复和结构相似性上仍有提升空间。

Method: 使用DPO框架：1)从逆折叠模型采样候选序列；2)用折叠模型预测结构并生成成对偏好标签；3)基于标签用DPO目标微调逆折叠模型。

Result: 在CATH 4.2测试集上：序列恢复率提升，平均TM-Score从0.77提高到0.81；迭代应用后TM-Score相对基线模型提升79.5%。

Conclusion: 该方法通过偏好优化有效利用结构反馈，为增强蛋白质序列设计能力提供了新方向。

Abstract: The inverse folding problem, aiming to design amino acid sequences that fold
into desired three-dimensional structures, is pivotal for various
biotechnological applications. Here, we introduce a novel approach leveraging
Direct Preference Optimization (DPO) to fine-tune an inverse folding model
using feedback from a protein folding model. Given a target protein structure,
we begin by sampling candidate sequences from the inverse-folding model, then
predict the three-dimensional structure of each sequence with the folding model
to generate pairwise structural-preference labels. These labels are used to
fine-tune the inverse-folding model under the DPO objective. Our results on the
CATH 4.2 test set demonstrate that DPO fine-tuning not only improves sequence
recovery of baseline models but also leads to a significant improvement in
average TM-Score from 0.77 to 0.81, indicating enhanced structure similarity.
Furthermore, iterative application of our DPO-based method on challenging
protein structures yields substantial gains, with an average TM-Score increase
of 79.5\% with regard to the baseline model. This work establishes a promising
direction for enhancing protein sequence design ability from structure feedback
by effectively utilizing preference optimization.

</details>


### [263] [On the Need to Align Intent and Implementation in Uncertainty Quantification for Machine Learning](https://arxiv.org/abs/2506.03037)
*Shubhendu Trivedi,Brian D. Nord*

Main category: cs.LG

TL;DR: 该立场论文探讨了机器学习中不确定性量化的挑战，包括术语不一致和技术需求多样，提出了促进UQ方法意图与实现对齐的标准，并讨论了可信UQ的必要条件。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型的不确定性量化面临术语不一致和不同问题背景下技术需求多样化的挑战，这影响了UQ方法的可靠性和信任度。

Method: 通过识别术语不一致性、分析不同背景下的认知需求，并考察当前估计目标、不确定性构建及映射方法，论文提出了促进UQ意图与实现对齐的标准。

Result: 论文强调了问题映射的实例，提出了可信UQ的必要条件，并通过科学ML中的案例展示了这些条件如何指导不确定性感知ML系统的设计与评估。

Conclusion: 通过明确挑战和提出标准，论文为提升机器学习中不确定性量化的可靠性和信任度提供了实用建议，特别是在基于模拟推理的背景下。

Abstract: Quantifying uncertainties for machine learning (ML) models is a foundational
challenge in modern data analysis. This challenge is compounded by at least two
key aspects of the field: (a) inconsistent terminology surrounding uncertainty
and estimation across disciplines, and (b) the varying technical requirements
for establishing trustworthy uncertainties in diverse problem contexts. In this
position paper, we aim to clarify the depth of these challenges by identifying
these inconsistencies and articulating how different contexts impose distinct
epistemic demands. We examine the current landscape of estimation targets
(e.g., prediction, inference, simulation-based inference), uncertainty
constructs (e.g., frequentist, Bayesian, fiducial), and the approaches used to
map between them. Drawing on the literature, we highlight and explain examples
of problematic mappings. To help address these issues, we advocate for
standards that promote alignment between the \textit{intent} and
\textit{implementation} of uncertainty quantification (UQ) approaches. We
discuss several axes of trustworthiness that are necessary (if not sufficient)
for reliable UQ in ML models, and show how these axes can inform the design and
evaluation of uncertainty-aware ML systems. Our practical recommendations focus
on scientific ML, offering illustrative cases and use scenarios, particularly
in the context of simulation-based inference (SBI).

</details>


### [264] [Sample complexity of Schrödinger potential estimation](https://arxiv.org/abs/2506.03043)
*Nikita Puchkin,Iurii Pustovalov,Yuri Sapronov,Denis Suchkov,Alexey Naumov,Denis Belomestny*

Main category: cs.LG

TL;DR: 该论文研究了薛定谔势估计问题，通过最小化经验KL风险来拟合目标分布，证明了在高概率下KL散度的非渐近上界。


<details>
  <summary>Details</summary>
Motivation: 薛定谔势估计在现代生成模型中扮演重要角色，特别是在基于薛定谔桥和SDE随机最优控制的方法中。论文旨在解决如何通过最小化KL风险来拟合目标分布的问题。

Method: 论文使用经验KL风险最小化方法，通过对数势能类的优化来拟合目标分布的边缘分布。在合理的假设下，分析了目标分布和先验过程的性质。

Result: 论文证明了在高概率下，KL散度的非渐近上界，表明当样本量趋于无穷时，KL风险可以快速下降至O(log²n/n)，即使目标分布和先验分布具有无界支撑。

Conclusion: 该研究为薛定谔势估计提供了理论保证，表明即使在无界支撑的情况下，KL风险仍能快速收敛，为生成模型的应用提供了有力支持。

Abstract: We address the problem of Schr\"odinger potential estimation, which plays a
crucial role in modern generative modelling approaches based on Schr\"odinger
bridges and stochastic optimal control for SDEs. Given a simple prior diffusion
process, these methods search for a path between two given distributions
$\rho_0$ and $\rho_T^*$ requiring minimal efforts. The optimal drift in this
case can be expressed through a Schr\"odinger potential. In the present paper,
we study generalization ability of an empirical Kullback-Leibler (KL) risk
minimizer over a class of admissible log-potentials aimed at fitting the
marginal distribution at time $T$. Under reasonable assumptions on the target
distribution $\rho_T^*$ and the prior process, we derive a non-asymptotic
high-probability upper bound on the KL-divergence between $\rho_T^*$ and the
terminal density corresponding to the estimated log-potential. In particular,
we show that the excess KL-risk may decrease as fast as $O(\log^2 n / n)$ when
the sample size $n$ tends to infinity even if both $\rho_0$ and $\rho_T^*$ have
unbounded supports.

</details>


### [265] [Multi-Metric Adaptive Experimental Design under Fixed Budget with Validation](https://arxiv.org/abs/2506.03062)
*Qining Zhang,Tanner Fiez,Yi Liu,Wenyang Liu*

Main category: cs.LG

TL;DR: 论文提出了一种固定预算的多指标自适应实验设计框架SHRVar，通过两阶段结构（探索和验证）解决传统A/B测试在多候选和多指标下的统计功效问题。


<details>
  <summary>Details</summary>
Motivation: 传统A/B测试在多候选同时测试时面临统计功效不足的问题，而自适应实验设计（AED）在推断实验统计量（如平均处理效应）时表现不佳，尤其是在多指标（如收入、安全性）和异方差情况下。

Method: 提出SHRVar方法，结合两阶段框架：自适应探索阶段识别最佳处理，验证阶段通过A/B测试验证处理质量并推断统计量。SHRVar基于相对方差的采样和奖励z值的淘汰策略，扩展了顺序减半（SH）方法。

Result: SHRVar实现了错误概率指数级下降，其复杂度度量推广了SH和SHVar方法（分别针对同方差和异方差情况）。数值实验验证了该框架的优越性能。

Conclusion: SHRVar框架在固定预算下显著提升了多候选、多指标实验的统计推断能力，为自适应实验设计提供了理论保证和实用工具。

Abstract: Standard A/B tests in online experiments face statistical power challenges
when testing multiple candidates simultaneously, while adaptive experimental
designs (AED) alone fall short in inferring experiment statistics such as the
average treatment effect, especially with many metrics (e.g., revenue, safety)
and heterogeneous variances. This paper proposes a fixed-budget multi-metric
AED framework with a two-phase structure: an adaptive exploration phase to
identify the best treatment, and a validation phase with an A/B test to verify
the treatment's quality and infer statistics. We propose SHRVar, which
generalizes sequential halving (SH) (Karnin et al., 2013) with a novel
relative-variance-based sampling and an elimination strategy built on reward
z-values. It achieves a provable error probability that decreases
exponentially, where the exponent generalizes the complexity measure for SH
(Karnin et al., 2013) and SHVar (Lalitha et al., 2023) with homogeneous and
heterogeneous variances, respectively. Numerical experiments verify our
analysis and demonstrate the superior performance of this new framework.

</details>


### [266] [Provable Reinforcement Learning from Human Feedback with an Unknown Link Function](https://arxiv.org/abs/2506.03066)
*Qining Zhang,Lei Ying*

Main category: cs.LG

TL;DR: 该论文提出了一种名为ZSPO的新型策略优化算法，用于处理链接函数未知的RLHF问题，通过估计价值函数差异的符号而非梯度，避免了链接函数误设的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的RLHF算法通常假设链接函数已知（如Bradley-Terry模型中的逻辑函数），但人类偏好的复杂性使得这一假设不现实。为避免链接函数误设，本文研究了链接函数未知的一般RLHF问题。

Method: 提出了一种基于零阶策略优化的新算法ZSPO，利用人类偏好构建与真实策略梯度方向正相关的参数更新方向，通过估计价值函数差异的符号而非梯度，无需知道链接函数。

Result: 在温和条件下，ZSPO以多项式收敛速率收敛到平稳策略，数值结果也显示了ZSPO在链接函数不匹配情况下的优越性。

Conclusion: ZSPO算法在链接函数未知的情况下表现优异，为RLHF问题提供了一种有效的解决方案。

Abstract: Link functions, which characterize how human preferences are generated from
the value function of an RL problem, are a crucial component in designing RLHF
algorithms. Almost all RLHF algorithms, including state-of-the-art ones in
empirical studies such as DPO and PPO, assume the link function is known to the
agent (e.g., a logistic function according to the Bradley-Terry model), which
is arguably unrealistic considering the complex nature of human preferences. To
avoid link function mis-specification, this paper studies general RLHF problems
with unknown link functions. We propose a novel policy optimization algorithm
called ZSPO based on a new zeroth-order policy optimization method, where the
key is to use human preference to construct a parameter update direction that
is positively correlated with the true policy gradient direction. ZSPO achieves
it by estimating the sign of the value function difference instead of
estimating the gradient from the value function difference, so it does not
require knowing the link function. Under mild conditions, ZSPO converges to a
stationary policy with a polynomial convergence rate depending on the number of
policy iterations and trajectories per iteration. Numerical results also show
the superiority of ZSPO under link function mismatch.

</details>


### [267] [Agnostic Learning under Targeted Poisoning: Optimal Rates and the Role of Randomness](https://arxiv.org/abs/2506.03075)
*Bogdan Chornomaz,Yonatan Koren,Shay Moran,Tom Waknine*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the problem of learning in the presence of an adversary that can
corrupt an $\eta$ fraction of the training examples with the goal of causing
failure on a specific test point. In the realizable setting, prior work
established that the optimal error under such instance-targeted poisoning
attacks scales as $\Theta(d\eta)$, where $d$ is the VC dimension of the
hypothesis class arXiv:2210.02713. In this work, we resolve the corresponding
question in the agnostic setting. We show that the optimal excess error is
$\tilde{\Theta}(\sqrt{d\eta})$, answering one of the main open problems left by
Hanneke et al. To achieve this rate, it is necessary to use randomized
learners: Hanneke et al. showed that deterministic learners can be forced to
suffer error close to 1, even under small amounts of poisoning. Perhaps
surprisingly, our upper bound remains valid even when the learner's random bits
are fully visible to the adversary . In the other direction, our lower bound is
stronger than standard PAC-style bounds: instead of tailoring a hard
distribution separately for each sample size, we exhibit a single fixed
distribution under which the adversary can enforce an excess error of
$\Omega(\sqrt{d\eta})$ infinitely often.

</details>


### [268] [StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence Training of LLMs](https://arxiv.org/abs/2506.03077)
*Qijun Luo,Mengqi Li,Lei Zhao,Xiao Li*

Main category: cs.LG

TL;DR: StreamBP是一种内存高效且精确的反向传播方法，通过序列维度的线性分解显著降低激活值和logits的内存消耗，适用于多种训练目标，并能扩展到多GPU训练。


<details>
  <summary>Details</summary>
Motivation: 随着序列长度的增加，反向传播过程中存储激活值的内存成本急剧上升，即使使用梯度检查点技术也难以应对，这限制了模型在长序列任务上的训练能力。

Method: 提出StreamBP方法，通过层级的序列维度线性分解链式法则，减少激活值和logits的内存占用，并利用语言模型的因果结构降低计算FLOPs和加速反向传播。

Result: 与梯度检查点相比，StreamBP将反向传播的最大序列长度扩展了2.8-5.5倍，同时保持或减少反向传播时间，并支持多GPU训练。

Conclusion: StreamBP是一种高效且可扩展的反向传播方法，适用于任何Transformer模型的训练，能够显著提升长序列任务的训练效率。

Abstract: Training language models on long sequence data is a demanding requirement for
enhancing the model's capability on complex tasks, e.g., long-chain reasoning.
However, as the sequence length scales up, the memory cost for storing
activation values becomes huge during the Backpropagation (BP) process, even
with the application of gradient checkpointing technique. To tackle this
challenge, we propose a memory-efficient and exact BP method called StreamBP,
which performs a linear decomposition of the chain rule along the sequence
dimension in a layer-wise manner, significantly reducing the memory cost of
activation values and logits. The proposed method is applicable to common
objectives such as SFT, GRPO, and DPO. From an implementation perspective,
StreamBP achieves less computational FLOPs and faster BP speed by leveraging
the causal structure of the language model. Compared to gradient checkpointing,
StreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger,
while using comparable or even less BP time. Note that StreamBP's sequence
length scaling ability can be directly transferred to batch size scaling for
accelerating training. We further develop a communication-efficient distributed
StreamBP to effectively support multi-GPU training and broaden its
applicability. Our code can be easily integrated into the training pipeline of
any transformer models and is available at https://github.com/Ledzy/StreamBP.

</details>


### [269] [Non-Asymptotic Length Generalization](https://arxiv.org/abs/2506.03085)
*Thomas Chen,Tengyu Ma,Zhiyuan Li*

Main category: cs.LG

TL;DR: 该论文研究了学习算法在理想化设置中对不同函数类的长度泛化能力，提出了长度复杂度的概念，并证明了不同函数类的最优长度复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究学习算法在训练数据长度之外泛化到更长输入的能力，即长度泛化问题。

Method: 形式化非渐近长度泛化框架，引入长度复杂度概念，分析不同函数类（如确定性有限自动机、上下文无关文法、C-RASP函数）的长度泛化能力。

Result: 证明了最小复杂度插值学习算法具有最优长度复杂度；确定性有限自动机的长度复杂度为2n-2；1层和2层C-RASP函数的长度复杂度分别为O(T²)和O(T^O(K))。

Conclusion: 函数类是否允许非渐近长度泛化等价于其语言等价问题的可判定性；上下文无关文法的长度复杂度无计算上界；C-RASP函数类的长度复杂度存在多项式上界。

Abstract: Length generalization is the ability of a learning algorithm to learn a
hypothesis which generalizes to longer inputs than the inputs in the training
set. In this paper, we provide provable guarantees of length generalization for
various classes of functions in an idealized setting. First, we formalize the
framework of non-asymptotic length generalization, which requires a computable
upper bound for the minimum input length that guarantees length generalization,
as a function of the complexity of ground-truth function under some given
complexity measure. We refer to this minimum input length to length generalize
as length complexity. We show the Minimum-Complexity Interpolator learning
algorithm achieves optimal length complexity. We further show that whether a
function class admits non-asymptotic length generalization is equivalent to the
decidability of its language equivalence problem, which implies that there is
no computable upper bound for the length complexity of Context-Free Grammars.
On the positive side, we show that the length complexity of Deterministic
Finite Automata is $2n - 2$ where $n$ is the number of states of the
ground-truth automaton. Our main results are upper bounds of length complexity
for a subset of a transformer-related function class called C-RASP (Yang &
Chiang, 2024). We show that the length complexity of 1-layer C-RASP functions
is $O(T^2)$ when the ground-truth function has precision $T$, and that the
length complexity of 2-layer C-RASP functions is $O(T^{O(K)})$ when the
ground-truth function has precision $T$ and $K$ heads.

</details>


### [270] [How Explanations Leak the Decision Logic: Stealing Graph Neural Networks via Explanation Alignment](https://arxiv.org/abs/2506.03087)
*Bin Ma,Yuyuan Feng,Minhua Lin,Enyan Dai*

Main category: cs.LG

TL;DR: 该论文提出了一种名为EGSteal的新型模型窃取框架，通过利用可解释GNN的解释机制泄露关键决策逻辑，结合引导数据增强技术，在有限查询下高效复制目标模型的预测行为和推理模式。实验表明该方法优于传统窃取手段，揭示了可解释GNN在安全敏感领域部署时需防范基于解释的攻击。


<details>
  <summary>Details</summary>
Motivation: 随着图神经网络(GNN)在药物发现等领域的广泛应用，模型可解释性需求日益增长。然而现有解释机制可能意外暴露模型的决策逻辑，导致安全风险。本文旨在研究如何通过解释信息窃取GNN模型。

Method: 提出EGSteal框架：1) 通过解释对齐捕获目标模型决策逻辑；2) 采用引导数据增强技术在有限查询条件下高效训练替代模型，实现预测行为和推理模式的双重复制。

Result: 在分子图数据集上的实验表明，EGSteal相比传统模型窃取方法更具优势，能有效复制目标模型的预测性能和内部推理路径。

Conclusion: 研究揭示了可解释GNN在敏感领域部署时的安全隐忧，表明需要开发针对解释信息泄露的防护措施。开源代码见https://github.com/beanmah/EGSteal。

Abstract: Graph Neural Networks (GNNs) have become essential tools for analyzing
graph-structured data in domains such as drug discovery and financial analysis,
leading to growing demands for model transparency. Recent advances in
explainable GNNs have addressed this need by revealing important subgraphs that
influence predictions, but these explanation mechanisms may inadvertently
expose models to security risks. This paper investigates how such explanations
potentially leak critical decision logic that can be exploited for model
stealing. We propose {\method}, a novel stealing framework that integrates
explanation alignment for capturing decision logic with guided data
augmentation for efficient training under limited queries, enabling effective
replication of both the predictive behavior and underlying reasoning patterns
of target models. Experiments on molecular graph datasets demonstrate that our
approach shows advantages over conventional methods in model stealing. This
work highlights important security considerations for the deployment of
explainable GNNs in sensitive domains and suggests the need for protective
measures against explanation-based attacks. Our code is available at
https://github.com/beanmah/EGSteal.

</details>


### [271] [From Flat to Hierarchical: Extracting Sparse Representations with Matching Pursuit](https://arxiv.org/abs/2506.03093)
*Valérie Costa,Thomas Fel,Ekdeep Singh Lubana,Bahareh Tolooshams,Demba Ba*

Main category: cs.LG

TL;DR: 本文探讨了稀疏自编码器（SAEs）是否能捕捉神经网络表示中的层次和非线性特征，提出了一种改进的MP-SAE方法，并验证其在特征提取上的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有假设认为神经网络表示中的特征是线性可访问且近似正交的，但近期研究表明存在层次化、非线性和多维特征。本文旨在验证SAEs是否能捕捉这些复杂特征，并探讨改进方法。

Method: 作者采用构造性方法，重新利用稀疏编码中的匹配追踪（MP）算法，设计了一种名为MP-SAE的新型SAE，其编码器通过残差引导的步骤序列捕捉层次和非线性特征。

Result: 实验表明：（1）层次化概念导致条件正交特征，传统SAEs无法准确捕捉；（2）MP-SAE的非线性编码步骤能恢复有意义的特征，揭示多模态模型中的共享结构。此外，MP-SAE还支持推理时的自适应稀疏性。

Conclusion: 研究支持了可解释性应从表示的现象学出发的观点，方法需基于符合现象学的假设。MP-SAE在捕捉复杂特征方面表现出色，为神经网络表示的理解提供了新工具。

Abstract: Motivated by the hypothesis that neural network representations encode
abstract, interpretable features as linearly accessible, approximately
orthogonal directions, sparse autoencoders (SAEs) have become a popular tool in
interpretability. However, recent work has demonstrated phenomenology of model
representations that lies outside the scope of this hypothesis, showing
signatures of hierarchical, nonlinear, and multi-dimensional features. This
raises the question: do SAEs represent features that possess structure at odds
with their motivating hypothesis? If not, does avoiding this mismatch help
identify said features and gain further insights into neural network
representations? To answer these questions, we take a construction-based
approach and re-contextualize the popular matching pursuits (MP) algorithm from
sparse coding to design MP-SAE -- an SAE that unrolls its encoder into a
sequence of residual-guided steps, allowing it to capture hierarchical and
nonlinearly accessible features. Comparing this architecture with existing SAEs
on a mixture of synthetic and natural data settings, we show: (i) hierarchical
concepts induce conditionally orthogonal features, which existing SAEs are
unable to faithfully capture, and (ii) the nonlinear encoding step of MP-SAE
recovers highly meaningful features, helping us unravel shared structure in the
seemingly dichotomous representation spaces of different modalities in a
vision-language model, hence demonstrating the assumption that useful features
are solely linearly accessible is insufficient. We also show that the
sequential encoder principle of MP-SAE affords an additional benefit of
adaptive sparsity at inference time, which may be of independent interest.
Overall, we argue our results provide credence to the idea that
interpretability should begin with the phenomenology of representations, with
methods emerging from assumptions that fit it.

</details>


### [272] [Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified Theory and Risk Bounds](https://arxiv.org/abs/2506.03100)
*Yang Guo,Yutian Tao,Yifei Ming,Robert D. Nowak,Yingyu Liang*

Main category: cs.LG

TL;DR: 本文首次提出了检索增强生成（RAG）在上下文线性回归中的有限样本泛化界限，并推导出偏差-方差权衡。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG在近年来的实证研究中取得了许多成功，但其理论方面仍未被充分探索。本文旨在填补这一空白，通过理论分析揭示RAG的泛化特性。

Method: 作者将检索到的文本视为依赖于查询的噪声上下文示例，并提出了一个框架，将经典的上下文学习（ICL）和标准RAG作为极限情况。此外，通过引入均匀和非均匀RAG噪声，该框架能够模拟从训练数据和外部语料库中的检索。

Result: 分析表明，与ICL相比，RAG存在固有的泛化误差上限。实验在常见QA基准（如Natural Questions和TriviaQA）上验证了ICL和RAG的样本效率。

Conclusion: 本文的理论和实验结果揭示了RAG的泛化特性，为未来的研究和应用提供了理论基础。

Abstract: Retrieval-augmented generation (RAG) has seen many empirical successes in
recent years by aiding the LLM with external knowledge. However, its
theoretical aspect has remained mostly unexplored. In this paper, we propose
the first finite-sample generalization bound for RAG in in-context linear
regression and derive an exact bias-variance tradeoff. Our framework views the
retrieved texts as query-dependent noisy in-context examples and recovers the
classical in-context learning (ICL) and standard RAG as the limit cases. Our
analysis suggests that an intrinsic ceiling on generalization error exists on
RAG as opposed to the ICL. Furthermore, our framework is able to model
retrieval both from the training data and from external corpora by introducing
uniform and non-uniform RAG noise. In line with our theory, we show the sample
efficiency of ICL and RAG empirically with experiments on common QA benchmarks,
such as Natural Questions and TriviaQA.

</details>


### [273] [On Weak-to-Strong Generalization and f-Divergence](https://arxiv.org/abs/2506.03109)
*Wei Yao,Gengze Xu,Huayi Tang,Wenkai Yang,Donglin Di,Ziqiao Wang,Yong Liu*

Main category: cs.LG

TL;DR: 该论文提出使用$f$-散度作为弱到强泛化（W2SG）中的损失函数框架，以提高强模型的泛化能力和噪声容忍度，同时减少计算和内存开销。


<details>
  <summary>Details</summary>
Motivation: 现有的弱到强泛化方法通常需要额外的弱模型或复杂流程，导致计算和内存开销较大。论文旨在通过引入$f$-散度损失函数来简化流程并提升性能。

Method: 论文引入$f$-散度作为信息论损失函数框架，并进行了理论分析，揭示了不同$f$-散度损失在W2SG中的基本限制和等价性。

Result: 实验证明，$f$-散度损失（如KL散度）能有效提升强模型的泛化能力和噪声容忍度。

Conclusion: $f$-散度损失在弱到强泛化中具有潜力，能够简化流程并提升模型性能。

Abstract: Weak-to-strong generalization (W2SG) has emerged as a promising paradigm for
stimulating the capabilities of strong pre-trained models by leveraging
supervision from weaker supervisors. To improve the performance of the strong
model, existing methods often require additional weak models or complex
procedures, leading to substantial computational and memory overhead. Motivated
by the effectiveness of $f$-divergence loss in various machine learning
domains, we introduce $f$-divergence as an information-theoretic loss function
framework in W2SG. Our theoretical analysis reveals fundamental limitations and
equivalence of different $f$-divergence losses in W2SG, supported by sample
complexity bounds and information-theoretic insights. We empirically
demonstrate that $f$-divergence loss, which generalizes widely-used metrics
like KL divergence, effectively improves generalization and noise tolerance of
the strong model in practice.

</details>


### [274] [Rectified Flows for Fast Multiscale Fluid Flow Modeling](https://arxiv.org/abs/2506.03111)
*Victor Armegioiu,Yannick Ramic,Siddhartha Mishra*

Main category: cs.LG

TL;DR: 提出一种整流流框架，通过近似直线轨迹传输输入输出分布，仅需8步即可实现高保真流体模拟，大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 流体流动的多尺度动力学和对初始条件的极端敏感性使其统计建模极具挑战性。现有扩散模型虽保真度高，但推理时需要数百次随机采样步骤，计算成本高昂。

Method: 引入整流流框架，学习时间依赖的速度场，将输入分布沿近似直线轨迹传输至输出分布。通过将采样过程转化为沿更直流动场的常微分方程求解，大幅提升积分步效率。

Result: 在多尺度流动基准测试中，整流流在仅用8步的情况下，恢复了与扩散模型相同的后验分布，保留了MSE基线遗漏的精细特征，且推理时间大幅缩短。

Conclusion: 整流流框架在保持高保真度的同时，显著提升了流体流动模拟的计算效率，为多尺度动力学建模提供了新思路。

Abstract: The statistical modeling of fluid flows is very challenging due to their
multiscale dynamics and extreme sensitivity to initial conditions. While
recently proposed conditional diffusion models achieve high fidelity, they
typically require hundreds of stochastic sampling steps at inference. We
introduce a rectified flow framework that learns a time-dependent velocity
field, transporting input to output distributions along nearly straight
trajectories. By casting sampling as solving an ordinary differential equation
(ODE) along this straighter flow field, our method makes each integration step
much more effective, using as few as eight steps versus (more than) 128 steps
in standard score-based diffusion, without sacrificing predictive fidelity.
Experiments on challenging multiscale flow benchmarks show that rectified flows
recover the same posterior distributions as diffusion models, preserve
fine-scale features that MSE-trained baselines miss, and deliver
high-resolution samples in a fraction of inference time.

</details>


### [275] [Zero-Shot Time Series Forecasting with Covariates via In-Context Learning](https://arxiv.org/abs/2506.03128)
*Andreas Auer,Raghul Parthipan,Pedro Mercado,Abdul Fatir Ansari,Lorenzo Stella,Bernie Wang,Michael Bohlke-Schneider,Syama Sundar Rangapuram*

Main category: cs.LG

TL;DR: COSMIC是一种利用协变量进行零样本预测的预训练时间序列模型，通过上下文学习和创新的协变量增强方法，无需依赖带协变量的数据集即可训练，并在零样本预测中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有预训练时间序列模型要么不支持协变量，要么无法有效利用协变量，限制了其在零样本预测中的潜力。

Method: 提出COSMIC模型，通过上下文学习整合协变量，并设计Informative Covariate Augmentation方法解决数据稀缺问题，无需依赖带协变量的训练集。

Result: COSMIC在带协变量和不带协变量的零样本预测任务中均实现了最先进的性能，定量和定性分析表明其能有效利用协变量。

Conclusion: COSMIC通过创新方法解决了协变量利用难题，显著提升了零样本时间序列预测的性能和可访问性。

Abstract: Pretrained time series models, capable of zero-shot forecasting, have
demonstrated significant potential in enhancing both the performance and
accessibility of time series forecasting. However, existing pretrained models
either do not support covariates or fail to incorporate them effectively. We
introduce COSMIC, a zero-shot forecasting model that utilizes covariates via
in-context learning. To address the challenge of data scarcity, we propose
Informative Covariate Augmentation, which enables the training of COSMIC
without requiring any datasets that include covariates. COSMIC achieves
state-of-the-art performance in zero-shot forecasting, both with and without
covariates. Our quantitative and qualitative analysis demonstrates that COSMIC
effectively leverages covariates in zero-shot forecasting.

</details>


### [276] [PoLAR: Polar-Decomposed Low-Rank Adapter Representation](https://arxiv.org/abs/2506.03133)
*Kai Lion,Liang Zhang,Bingcong Li,Niao He*

Main category: cs.LG

TL;DR: 论文指出大规模模型的低秩自适应存在稳定秩低于子空间线性代数秩的问题，导致微调性能下降。为此，作者提出PoLAR方法，通过极分解参数化改进低秩更新，结合黎曼优化在多个基准测试中取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前大规模模型的低秩自适应方法存在稳定秩不足的问题，导致子空间利用率低下，影响模型微调性能。

Method: 提出PoLAR方法，将低秩更新分解为两个约束在Stiefel流形上的方向矩阵和一个无约束的尺度矩阵，结合黎曼优化进行训练。

Result: 理论分析表明PoLAR在典型低秩自适应问题上具有指数级更快的收敛速度，在语言理解、常识推理和数学问题求解等基准测试中均取得显著提升。

Conclusion: PoLAR方法有效解决了低秩自适应中的子空间利用率问题，为大规模模型微调提供了更高效的解决方案。

Abstract: We show that low-rank adaptation of large-scale models suffers from a low
stable rank that is well below the linear algebraic rank of the subspace,
degrading fine-tuning performance. To mitigate the underutilization of the
allocated subspace, we propose PoLAR, a parameterization inspired by the polar
decomposition that factorizes the low-rank update into two direction matrices
constrained to Stiefel manifolds and an unconstrained scale matrix. Our theory
shows that PoLAR yields an exponentially faster convergence rate on a canonical
low-rank adaptation problem. Pairing the parameterization with Riemannian
optimization leads to consistent gains on three different benchmarks testing
general language understanding, commonsense reasoning, and mathematical problem
solving with base model sizes ranging from 350M to 27B.

</details>


### [277] [Not All Tokens Are Meant to Be Forgotten](https://arxiv.org/abs/2506.03142)
*Xiangyu Zhou,Yao Qiang,Saleh Zare Zade,Douglas Zytko,Prashant Khanduri,Dongxiao Zhu*

Main category: cs.LG

TL;DR: 论文提出了一种名为TIF的框架，通过区分无用词和通用词并优化遗忘过程，有效解决大语言模型在遗忘不需要信息时的过度遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在预训练过程中会记忆不需要的信息（如隐私或受版权保护的内容），现有遗忘方法存在过度遗忘问题，导致模型效用大幅下降。

Method: 提出TIF框架，包含（1）目标信息标识器区分无用词和通用词，（2）基于Logit偏好优化的目标遗忘方法，分别处理无用信息和保留通用信息。

Result: 在TOFU和MUSE基准测试中，TIF框架在保持模型效用的同时提升了遗忘效果，达到了最先进的性能。

Conclusion: TIF框架通过针对性遗忘机制，平衡了信息遗忘与模型效用保留的需求，为大语言模型隐私保护提供了有效解决方案。

Abstract: Large Language Models (LLMs), pre-trained on massive text corpora, exhibit
remarkable human-level language understanding, reasoning, and decision-making
abilities. However, they tend to memorize unwanted information, such as private
or copyrighted content, raising significant privacy and legal concerns.
Unlearning has emerged as a promising solution, but existing methods face a
significant challenge of over-forgetting. This issue arises because they
indiscriminately suppress the generation of all the tokens in forget samples,
leading to a substantial loss of model utility. To overcome this challenge, we
introduce the Targeted Information Forgetting (TIF) framework, which consists
of (1) a flexible targeted information identifier designed to differentiate
between unwanted words (UW) and general words (GW) in the forget samples, and
(2) a novel Targeted Preference Optimization approach that leverages Logit
Preference Loss to unlearn unwanted information associated with UW and
Preservation Loss to retain general information in GW, effectively improving
the unlearning process while mitigating utility degradation. Extensive
experiments on the TOFU and MUSE benchmarks demonstrate that the proposed TIF
framework enhances unlearning effectiveness while preserving model utility and
achieving state-of-the-art results.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [278] [TransAct V2: Lifelong User Action Sequence Modeling on Pinterest Recommendation](https://arxiv.org/abs/2506.02267)
*Xue Xia,Saurabh Vishwas Joshi,Kousik Rajesh,Kangnan Li,Yangyi Lu,Nikil Pancha,Dhruvil Deven Badani,Jiajing Xu,Pong Eksombatchai*

Main category: cs.IR

TL;DR: TransAct V2是Pinterest Homefeed排序系统的生产模型，通过长用户序列、Next Action Loss函数和高效部署方案提升CTR预测。


<details>
  <summary>Details</summary>
Motivation: 当前工业级CTR模型依赖短用户序列，缺乏行为预测任务，且难以高效处理大规模序列数据。

Method: 引入长用户序列改进CTR预测，整合Next Action Loss函数，采用低延迟部署方案。

Result: TransAct V2提升了用户行为预测能力，并解决了大规模序列模型的服务挑战。

Conclusion: TransAct V2通过三项创新显著优化了推荐系统的CTR预测和部署效率。

Abstract: Modeling user action sequences has become a popular focus in industrial
recommendation system research, particularly for Click-Through Rate (CTR)
prediction tasks. However, industry-scale CTR models often rely on short user
sequences, limiting their ability to capture long-term behavior. Additionally,
these models typically lack an integrated action-prediction task within a
point-wise ranking framework, reducing their predictive power. They also rarely
address the infrastructure challenges involved in efficiently serving
large-scale sequential models. In this paper, we introduce TransAct V2, a
production model for Pinterest's Homefeed ranking system, featuring three key
innovations: (1) leveraging very long user sequences to improve CTR
predictions, (2) integrating a Next Action Loss function for enhanced user
action forecasting, and (3) employing scalable, low-latency deployment
solutions tailored to handle the computational demands of extended user action
sequences.

</details>


### [279] [A Dynamic Framework for Semantic Grouping of Common Data Elements (CDE) Using Embeddings and Clustering](https://arxiv.org/abs/2506.02160)
*Madan Krishnamurthy,Daniel Korn,Melissa A Haendel,Christopher J Mungall,Anne E Thessen*

Main category: cs.IR

TL;DR: 该研究开发了一个动态可扩展框架，利用大语言模型和聚类技术解决生物医学数据中通用数据元素（CDEs）的语义异质性问题，实现了高效的数据整合与互操作性。


<details>
  <summary>Details</summary>
Motivation: 生物医学数据中存在语义异质性、结构多变性和上下文依赖性等问题，阻碍了数据整合和互操作性，限制了科研发现的效率。本研究旨在通过开发一个动态可扩展框架来解决这些问题。

Method: 研究采用大语言模型（LLMs）生成上下文感知的文本嵌入，将CDEs转化为密集向量；使用HDBSCAN进行无监督聚类；通过LLM自动标记聚类；最后用监督学习训练分类器，将新CDEs分配到已标记聚类中。

Result: 在包含24,000多个CDEs的NIH NLM CDE Repository上测试，系统识别出118个有意义的聚类，分类器整体准确率达90.46%。外部验证显示聚类效果良好（调整兰德指数0.52，标准化互信息0.78）。

Conclusion: 该框架提供了一种实用且可扩展的CDEs协调方案，显著提高了数据选择效率和互操作性，为生物医学数据整合提供了有效工具。

Abstract: This research aims to develop a dynamic and scalable framework to facilitate
harmonization of Common Data Elements (CDEs) across heterogeneous biomedical
datasets by addressing challenges such as semantic heterogeneity, structural
variability, and context dependence to streamline integration, enhance
interoperability, and accelerate scientific discovery. Our methodology
leverages Large Language Models (LLMs) for context-aware text embeddings that
convert CDEs into dense vectors capturing semantic relationships and patterns.
These embeddings are clustered using Hierarchical Density-Based Spatial
Clustering of Applications with Noise (HDBSCAN) to group semantically similar
CDEs. The framework incorporates four key steps: (1) LLM-based text embedding
to mathematically represent semantic context, (2) unsupervised clustering of
embeddings via HDBSCAN, (3) automated labeling using LLM summarization, and (4)
supervised learning to train a classifier assigning new or unclustered CDEs to
labeled clusters. Evaluated on the NIH NLM CDE Repository with over 24,000
CDEs, the system identified 118 meaningful clusters at an optimized minimum
cluster size of 20. The classifier achieved 90.46 percent overall accuracy,
performing best in larger categories. External validation against Gravity
Projects Social Determinants of Health domains showed strong agreement
(Adjusted Rand Index 0.52, Normalized Mutual Information 0.78), indicating that
embeddings effectively capture cluster characteristics. This adaptable and
scalable approach offers a practical solution to CDE harmonization, improving
selection efficiency and supporting ongoing data interoperability.

</details>


### [280] [Towards Human-like Preference Profiling in Sequential Recommendation](https://arxiv.org/abs/2506.02261)
*Zhongyu Ouyang,Qianlong Wen,Chunhui Zhang,Yanfang Ye,Soroush Vosoughi*

Main category: cs.IR

TL;DR: RecPO框架通过模拟人类决策的偏好优化，提升了序列推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的推荐系统难以模拟人类灵活、上下文感知的决策策略，缺乏对人类行为结构化、动态和上下文感知机制的建模。

Method: 提出RecPO框架，通过建模结构化反馈和上下文延迟，利用自适应奖励边界和时序信号，模拟人类偏好优先级。

Result: 在五个真实数据集上的实验表明，RecPO不仅性能优于现有方法，还能模拟人类决策的关键特征。

Conclusion: RecPO成功桥接了推荐系统与人类决策机制之间的差距，展现出更高的推荐效果和人类行为模拟能力。

Abstract: Sequential recommendation systems aspire to profile users by interpreting
their interaction histories, echoing how humans make decisions by weighing
experience, relative preference strength, and situational relevance. Yet,
existing large language model (LLM)-based recommenders often fall short of
mimicking the flexible, context-aware decision strategies humans exhibit,
neglecting the structured, dynamic, and context-aware mechanisms fundamental to
human behaviors. To bridge this gap, we propose RecPO, a preference
optimization framework that models structured feedback and contextual delay to
emulate human-like prioritization in sequential recommendation RecPO exploits
adaptive reward margins based on inferred preference hierarchies and temporal
signals, enabling the model to favor immediately relevant items and to
distinguish between varying degrees of preference and aversion. Extensive
experiments across five real-world datasets demonstrate that RecPO not only
yields performance gains over state-of-the-art baselines, but also mirrors key
characteristics of human decision-making: favoring timely satisfaction,
maintaining coherent preferences, and exercising discernment under shifting
contexts.

</details>


### [281] [DeepShop: A Benchmark for Deep Research Shopping Agents](https://arxiv.org/abs/2506.02839)
*Yougang Lyu,Xiaoyu Zhang,Lingyong Yan,Maarten de Rijke,Zhaochun Ren,Xiuying Chen*

Main category: cs.IR

TL;DR: 论文提出DeepShop基准测试，用于评估在线购物网络代理在复杂真实场景中的表现，发现现有方法在复杂查询和用户偏好处理上存在不足。


<details>
  <summary>Details</summary>
Motivation: 现有评估网络代理的基准测试过于简单，无法反映真实购物场景的复杂性，如多维产品属性、搜索过滤器和用户排序偏好。

Method: DeepShop包含三个关键部分：查询多样性演化、查询复杂度演化（分为简单、中等和困难三个级别）以及细粒度和整体评估框架。

Result: 实验表明，RAG方法因缺乏网络交互而难以处理复杂查询，其他方法在过滤器和排序偏好上也表现不佳，整体成功率较低。

Conclusion: DeepShop为复杂购物场景下的网络代理评估提供了有效工具，揭示了现有方法的局限性，并支持未来购物代理的改进。

Abstract: Web agents for online shopping have shown great promise in automating user
interactions across e-commerce platforms. Benchmarks for assessing such agents
do not reflect the complexity of real-world shopping scenarios, as they often
consist of overly simple queries with deterministic paths, such as "Find iPhone
15." Real shopping scenarios are inherently more layered, involving
multi-dimensional product attributes, search filters, and user-specific sorting
preferences. To address this gap, we introduce DeepShop, a benchmark designed
to evaluate web agents in complex and realistic online shopping environments.
DeepShop comprises three key components. (1) Query diversity evolution:
Starting from real user queries, we generate diverse queries across five
popular online shopping domains. (2) Query complexity evolution: We further
evolve these queries to increase complexity, considering product attributes,
search filters, and sorting preferences, and classify them into three levels:
easy, medium, and hard, based on the number of evolutions. (3) Fine-grained and
holistic evaluation: We propose an automated evaluation framework that assesses
agent performance in terms of fine-grained aspects (product attributes, search
filters, and sorting preferences) and reports the overall success rate through
holistic evaluation. We conduct a systematic evaluation of retrieval-augmented
generation (RAG) methods, web agents, and deep research systems. Results show
that RAG struggles with complex queries due to its lack of web interaction,
while other methods face significant challenges with filters and sorting
preferences, leading to low overall success rates. We also perform
cross-category, complexity-based evaluations and error analyses to support the
advancement of deep research shopping agents.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [282] [Enhancing Interpretability of Quantum-Assisted Blockchain Clustering via AI Agent-Based Qualitative Analysis](https://arxiv.org/abs/2506.02068)
*Yun-Cheng Tsai,Yen-Ku Liu,Samuel Yen-Chi Chen*

Main category: quant-ph

TL;DR: 本文提出了一种结合定量聚类评估与AI代理辅助定性解释的两阶段分析框架，以解决区块链交易数据聚类中的高维、噪声和纠缠问题，并提升量子增强聚类模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 区块链交易数据具有高维、噪声和纠缠的特性，传统聚类算法难以处理。尽管量子增强聚类模型表现出色，但其可解释性不足，限制了在金融欺诈检测和区块链治理等敏感领域的应用。

Method: 采用两阶段分析框架：第一阶段使用经典聚类方法和评估指标（如轮廓系数、Davies Bouldin指数和Calinski Harabasz指数）确定最佳聚类数量和基线分区质量；第二阶段集成AI代理生成人类可读的聚类结果语义解释，识别簇内特征和簇间关系。

Result: 实验表明，完全训练的量子神经网络（QNN）在定量指标上优于随机量子特征（QF），但AI代理进一步揭示了这些方法的细微差异，尤其是QNN驱动模型中的单例簇现象。两阶段分析一致支持三簇配置。

Conclusion: 本研究推动了量子辅助区块链分析的可解释性前沿，为未来自主AI编排的聚类框架奠定了基础。

Abstract: Blockchain transaction data is inherently high dimensional, noisy, and
entangled, posing substantial challenges for traditional clustering algorithms.
While quantum enhanced clustering models have demonstrated promising
performance gains, their interpretability remains limited, restricting their
application in sensitive domains such as financial fraud detection and
blockchain governance. To address this gap, we propose a two stage analysis
framework that synergistically combines quantitative clustering evaluation with
AI Agent assisted qualitative interpretation. In the first stage, we employ
classical clustering methods and evaluation metrics including the Silhouette
Score, Davies Bouldin Index, and Calinski Harabasz Index to determine the
optimal cluster count and baseline partition quality. In the second stage, we
integrate an AI Agent to generate human readable, semantic explanations of
clustering results, identifying intra cluster characteristics and inter cluster
relationships. Our experiments reveal that while fully trained Quantum Neural
Networks (QNN) outperform random Quantum Features (QF) in quantitative metrics,
the AI Agent further uncovers nuanced differences between these methods,
notably exposing the singleton cluster phenomenon in QNN driven models. The
consolidated insights from both stages consistently endorse the three cluster
configuration, demonstrating the practical value of our hybrid approach. This
work advances the interpretability frontier in quantum assisted blockchain
analytics and lays the groundwork for future autonomous AI orchestrated
clustering frameworks.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [283] [Machine vs Machine: Using AI to Tackle Generative AI Threats in Assessment](https://arxiv.org/abs/2506.02046)
*Mohammad Saleh Torkestani,Taha Mansouri*

Main category: cs.CY

TL;DR: 本文提出了一种理论框架，通过机器对机器的方法解决生成式AI对高等教育评估带来的挑战，结合静态分析和动态测试来评估评估的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI（如GPT-4、Claude和Llama）能够生成复杂的学术内容，对传统评估方法构成威胁。调查显示74-92%的学生尝试使用这些工具完成学术任务。现有的检测工具和人工评估方法存在局限性，如检测工具对非母语作者有偏见且易被规避，人工评估依赖主观判断且假设AI能力静态不变。

Method: 提出双策略范式，结合静态分析和动态测试。静态分析包括八个理论要素：特异性和情境化、时间相关性、过程可见性要求、个性化元素、资源可及性、多模态整合、伦理推理要求和协作元素。动态测试通过模拟脆弱性评估补充静态分析的不足。

Result: 提出了一个评估脆弱性评分的理论框架，包括定量评估的概念基础、加权框架和阈值确定理论。

Conclusion: 该框架为高等教育评估提供了一种全面应对生成式AI挑战的方法，通过静态和动态结合的方式区分真实人类学习和AI生成内容。

Abstract: This paper presents a theoretical framework for addressing the challenges
posed by generative artificial intelligence (AI) in higher education assessment
through a machine-versus-machine approach. Large language models like GPT-4,
Claude, and Llama increasingly demonstrate the ability to produce sophisticated
academic content, traditional assessment methods face an existential threat,
with surveys indicating 74-92% of students experimenting with these tools for
academic purposes. Current responses, ranging from detection software to manual
assessment redesign, show significant limitations: detection tools demonstrate
bias against non-native English writers and can be easily circumvented, while
manual frameworks rely heavily on subjective judgment and assume static AI
capabilities. This paper introduces a dual strategy paradigm combining static
analysis and dynamic testing to create a comprehensive theoretical framework
for assessment vulnerability evaluation. The static analysis component
comprises eight theoretically justified elements: specificity and
contextualization, temporal relevance, process visibility requirements,
personalization elements, resource accessibility, multimodal integration,
ethical reasoning requirements, and collaborative elements. Each element
addresses specific limitations in generative AI capabilities, creating barriers
that distinguish authentic human learning from AI-generated simulation. The
dynamic testing component provides a complementary approach through
simulation-based vulnerability assessment, addressing limitations in
pattern-based analysis. The paper presents a theoretical framework for
vulnerability scoring, including the conceptual basis for quantitative
assessment, weighting frameworks, and threshold determination theory.

</details>


### [284] [Will Agents Replace Us? Perceptions of Autonomous Multi-Agent AI](https://arxiv.org/abs/2506.02055)
*Nikola Balic*

Main category: cs.CY

TL;DR: 研究通过130份调查问卷分析专业人士对AI代理的认知，发现部署决策复杂且需关注合规与治理框架。


<details>
  <summary>Details</summary>
Motivation: 探讨自主多智能体AI系统在软件开发等领域的潜在影响，了解当前专业人士的认知以预测采纳挑战和伦理问题。

Method: 通过分析130名参与者关于AI代理能力、影响及治理的问卷调查数据。

Result: 发现受访者分为三类，但逻辑回归模型未找到显著预测因素，表明部署决策复杂或需更大样本。

Conclusion: 组织需解决合规问题并建立明确治理框架，以整合自主代理至工作流程。

Abstract: Autonomous multi-agent AI systems are poised to transform various industries,
particularly software development and knowledge work. Understanding current
perceptions among professionals is crucial for anticipating adoption
challenges, ethical considerations, and future workforce development. This
study analyzes responses from 130 participants to a survey on the capabilities,
impact, and governance of AI agents. We explore expected timelines for AI
replacing programmers, identify perceived barriers to deployment, and examine
beliefs about responsibility when agents make critical decisions. Key findings
reveal three distinct clusters of respondents. While the study explored factors
associated with current AI agent deployment, the initial logistic regression
model did not yield statistically significant predictors, suggesting that
deployment decisions are complex and may be influenced by factors not fully
captured or that a larger sample is needed. These insights highlight the need
for organizations to address compliance concerns (a commonly cited barrier) and
establish clear governance frameworks as they integrate autonomous agents into
their workflows.

</details>


### [285] [AI Data Development: A Scorecard for the System Card Framework](https://arxiv.org/abs/2506.02071)
*Tadesse K. Bahiru,Haileleol Tibebu,Ioannis A. Kakadiaris*

Main category: cs.CY

TL;DR: 该论文提出了一种评估AI数据集开发的记分卡方法，关注数据透明度与完整性，应用于四个数据集并给出改进建议。


<details>
  <summary>Details</summary>
Motivation: AI系统的可靠性高度依赖数据集质量，但当前存在透明度、责任性和潜在偏见等问题，需系统化评估方法。

Method: 基于系统卡框架的数据开发生命周期，通过结构化问卷和评分标准评估数据集的五个关键领域。

Result: 记分卡揭示了数据集的优势和改进空间，通过评分系统提供定制化建议以提升数据透明度与完整性。

Conclusion: 该记分卡从技术和伦理角度提供全面评估，为开发负责任的AI系统提供实践指导，确保决策系统的公平性与责任性。

Abstract: Artificial intelligence has transformed numerous industries, from healthcare
to finance, enhancing decision-making through automated systems. However, the
reliability of these systems is mainly dependent on the quality of the
underlying datasets, raising ongoing concerns about transparency,
accountability, and potential biases. This paper introduces a scorecard
designed to evaluate the development of AI datasets, focusing on five key areas
from the system card framework data development life cycle: data dictionary,
collection process, composition, motivation, and pre-processing. The method
follows a structured approach, using an intake form and scoring criteria to
assess the quality and completeness of the data set. Applied to four diverse
datasets, the methodology reveals strengths and improvement areas. The results
are compiled using a scoring system that provides tailored recommendations to
enhance the transparency and integrity of the data set. The scorecard addresses
technical and ethical aspects, offering a holistic evaluation of data
practices. This approach aims to improve the quality of the data set. It offers
practical guidance to curators and researchers in developing responsible AI
systems, ensuring fairness and accountability in decision support systems.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [286] [Second-order AAA algorithms for structured data-driven modeling](https://arxiv.org/abs/2506.02241)
*Michael S. Ackermann,Ion Victor Gosea,Serkan Gugercin,Steffen W. R. Werner*

Main category: math.NA

TL;DR: 本文提出三种数据驱动建模方法，直接从频域数据构建具有二阶微分结构的动态系统，扩展了自适应Antoulas-Anderson算法，并在计算速度和建模精度之间提供权衡方案。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动建模常忽略物理现象固有的微分结构，导致模型难以从物理角度解释。本文旨在解决这一问题，直接从数据构建具有二阶微分结构的动态系统。

Method: 基于二阶结构化重心形式，扩展自适应Antoulas-Anderson算法至二阶系统，并提出在计算资源限制下优先速度或精度的变体方法。

Result: 数值实验表明，新提出的结构化方法在建模效果上优于传统非结构化数据驱动建模方法。

Conclusion: 本文方法有效解决了动态系统建模中微分结构缺失的问题，为物理可解释建模提供了新途径。

Abstract: The data-driven modeling of dynamical systems has become an essential tool
for the construction of accurate computational models from real-world data. In
this process, the inherent differential structures underlying the considered
physical phenomena are often neglected making the reinterpretation of the
learned models in a physically meaningful sense very challenging. In this work,
we present three data-driven modeling approaches for the construction of
dynamical systems with second-order differential structure directly from
frequency domain data. Based on the second-order structured barycentric form,
we extend the well-known Adaptive Antoulas-Anderson algorithm to the case of
second-order systems. Depending on the available computational resources, we
propose variations of the proposed method that prioritize either higher
computation speed or greater modeling accuracy, and we present a theoretical
analysis for the expected accuracy and performance of the proposed methods.
Three numerical examples demonstrate the effectiveness of our new structured
approaches in comparison to classical unstructured data-driven modeling.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [287] [Music interpretation and emotion perception: A computational and neurophysiological investigation](https://arxiv.org/abs/2506.01982)
*Vassilis Lyberatos,Spyridon Kantarelis,Ioanna Zioga,Christina Anagnostopoulou,Giorgos Stamou,Anastasia Georgaki*

Main category: cs.HC

TL;DR: 该研究通过计算和神经生理学方法探讨音乐表演中的情感表达与感知，发现即兴和富有表现力的表演能增强情感交流与观众参与。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索不同表演设置和表现力水平如何影响音乐家的情感沟通和听众的反应，以深化对音乐情感表达的理解。

Method: 采用专业音乐家进行多样化任务表演，结合音频分析、情感标注和神经生理学测量，进行多模态研究。

Result: 富有表现力和即兴的表演展现出独特的声学特征，引发更强的情感反应，且即兴表演中表演者更放松。

Conclusion: 研究表明表现力对增强情感交流和观众参与具有重要作用，凸显了音乐表演中情感表达的重要性。

Abstract: This study investigates emotional expression and perception in music
performance using computational and neurophysiological methods. The influence
of different performance settings, such as repertoire, diatonic modal etudes,
and improvisation, as well as levels of expressiveness, on performers'
emotional communication and listeners' reactions is explored. Professional
musicians performed various tasks, and emotional annotations were provided by
both performers and the audience. Audio analysis revealed that expressive and
improvisational performances exhibited unique acoustic features, while emotion
analysis showed stronger emotional responses. Neurophysiological measurements
indicated greater relaxation in improvisational performances. This multimodal
study highlights the significance of expressivity in enhancing emotional
communication and audience engagement.

</details>


### [288] [Inter(sectional) Alia(s): Ambiguity in Voice Agent Identity via Intersectional Japanese Self-Referents](https://arxiv.org/abs/2506.01998)
*Takao Fujii,Katie Seaborn,Madeleine Steeds,Jun Kato*

Main category: cs.HC

TL;DR: 研究探讨了日语中非代词性自称词和语音对聊天机器人性别认知的影响，发现语音会强化性别印象，而某些自称词能模糊性别认知。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注拟人化对话代理的伦理问题，但对非代词性自称词及语音在塑造代理身份认知中的作用研究不足，尤其在日语等非英语语境中。

Method: 通过众包实验，204名日本参与者用7种自称词评估ChatGPT三种语音（Juniper/Breeze/Ember）的性别、年龄和正式度认知。

Result: 语音显著强化性别刻板印象，但自称词（如boku/watakushi）能通过中立性模糊性别认知，且年龄/正式度认知与性别认知存在交叉影响。

Conclusion: 研究强调需采用交叉性和文化敏感的视角设计语音代理，自称词可作为规避性别二元认知的设计工具。

Abstract: Conversational agents that mimic people have raised questions about the
ethics of anthropomorphizing machines with human social identity cues. Critics
have also questioned assumptions of identity neutrality in humanlike agents.
Recent work has revealed that intersectional Japanese pronouns can elicit
complex and sometimes evasive impressions of agent identity. Yet, the role of
other "neutral" non-pronominal self-referents (NPSR) and voice as a socially
expressive medium remains unexplored. In a crowdsourcing study, Japanese
participants (N = 204) evaluated three ChatGPT voices (Juniper, Breeze, and
Ember) using seven self-referents. We found strong evidence of voice gendering
alongside the potential of intersectional self-referents to evade gendering,
i.e., ambiguity through neutrality and elusiveness. Notably, perceptions of age
and formality intersected with gendering as per sociolinguistic theories,
especially boku and watakushi. This work provides a nuanced take on agent
identity perceptions and champions intersectional and culturally-sensitive work
on voice agents.

</details>


### [289] [Composable Building Blocks for Controllable and Transparent Interactive AI Systems](https://arxiv.org/abs/2506.02262)
*Sebe Vanbrabant,Gustavo Rovelo Ruiz,Davy Vanacken*

Main category: cs.HC

TL;DR: 论文提出了一种通过结构化构建块和可视化解释技术来提高交互式系统透明度的新方法，旨在解决AI模型黑箱问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术在交互系统中的广泛应用，其黑箱问题导致整个系统透明度不足。尽管可解释AI（XAI）技术能提升单个模型的透明度，但系统整体架构仍不透明。

Method: 将交互系统表示为结构构建块（如AI模型和控制机制）的序列，并通过可视化构建块（如XAI技术）进行解释，形成明确的系统流程和API概览。

Result: 提出的方法为人类和自动化代理（如LLMs）提供了统一的沟通基础，实现了AI模型的人类与机器可解释性对齐，并通过原型系统验证了其可行性。

Conclusion: 通过结构化构建块和流程化方法，能够有效提升交互式系统的整体透明度，弥合人类与机器在理解AI模型时的鸿沟。

Abstract: While the increased integration of AI technologies into interactive systems
enables them to solve an equally increasing number of tasks, the black box
problem of AI models continues to spread throughout the interactive system as a
whole. Explainable AI (XAI) techniques can make AI models more accessible by
employing post-hoc methods or transitioning to inherently interpretable models.
While this makes individual AI models clearer, the overarching system
architecture remains opaque. To this end, we propose an approach to represent
interactive systems as sequences of structural building blocks, such as AI
models and control mechanisms grounded in the literature. These can then be
explained through accompanying visual building blocks, such as XAI techniques.
The flow and APIs of the structural building blocks form an explicit overview
of the system. This serves as a communication basis for both humans and
automated agents like LLMs, aligning human and machine interpretability of AI
models. We discuss a selection of building blocks and concretize our flow-based
approach in an architecture and accompanying prototype interactive system.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [290] [StarVC: A Unified Auto-Regressive Framework for Joint Text and Speech Generation in Voice Conversion](https://arxiv.org/abs/2506.02414)
*Fengjin Li,Jie Wang,Yadong Niu,Yongqing Wang,Meng Meng,Jian Luan,Zhiyong Wu*

Main category: cs.MM

TL;DR: StarVC是一种新型语音转换框架，通过预测文本标记再合成声学特征，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统语音转换方法直接从语音中提取说话者信息，忽略了语言内容的显式利用，影响了转换效果。

Method: 提出StarVC，一种统一的自回归语音转换框架，先预测文本标记再合成声学特征。

Result: 实验表明，StarVC在保留语言内容（WER、CER）和说话者特征（SECS、MOS）方面优于传统方法。

Conclusion: StarVC通过显式文本建模提升了语音转换性能，为后续研究提供了新方向。

Abstract: Voice Conversion (VC) modifies speech to match a target speaker while
preserving linguistic content. Traditional methods usually extract speaker
information directly from speech while neglecting the explicit utilization of
linguistic content. Since VC fundamentally involves disentangling speaker
identity from linguistic content, leveraging structured semantic features could
enhance conversion performance. However, previous attempts to incorporate
semantic features into VC have shown limited effectiveness, motivating the
integration of explicit text modeling. We propose StarVC, a unified
autoregressive VC framework that first predicts text tokens before synthesizing
acoustic features. The experiments demonstrate that StarVC outperforms
conventional VC methods in preserving both linguistic content (i.e., WER and
CER) and speaker characteristics (i.e., SECS and MOS). Audio demo can be found
at: https://thuhcsi.github.io/StarVC/.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [291] [A meaningful prediction of functional decline in amyotrophic lateral sclerosis based on multi-event survival analysis](https://arxiv.org/abs/2506.02076)
*Christian Marius Lillelund,Sanjay Kalra,Russell Greiner*

Main category: q-bio.QM

TL;DR: 该研究提出了一种预测ALS患者功能衰退时间的新方法，通过多事件生存模型分析，能更准确预测患者五项日常功能的衰退时间，并支持个性化治疗规划。


<details>
  <summary>Details</summary>
Motivation: 由于ALS疾病的异质性，确定最佳治疗时机具有挑战性。本研究旨在开发一种方法，帮助预测患者功能衰退的具体时间，从而优化治疗决策。

Method: 研究采用多事件生存分析方法，在PRO-ACT数据集上训练五个基于协变量的生存模型，预测患者在500天内五项功能（说话、吞咽、书写、行走和呼吸）的衰退概率。

Result: 基于协变量的模型在预测事件时间上优于Kaplan-Meier估计器。研究发现Riluzole对功能衰退预测影响甚微，而延髓起病型ALS患者的言语和吞咽功能衰退时间预测更短。

Conclusion: 该方法能有效评估ALS患者功能衰退风险，支持临床个性化治疗规划，尤其对延髓起病型患者的言语和吞咽功能衰退预测具有重要价值。

Abstract: Amyotrophic lateral sclerosis (ALS) is a degenerative disorder of motor
neurons that causes progressive paralysis in patients. Current treatment
options aim to prolong survival and improve quality of life; however, due to
the heterogeneity of the disease, it is often difficult to determine the
optimal time for potential therapies or medical interventions. In this study,
we propose a novel method to predict the time until a patient with ALS
experiences significant functional impairment (ALSFRS-R<=2) with respect to
five common functions: speaking, swallowing, handwriting, walking and
breathing. We formulate this task as a multi-event survival problem and
validate our approach in the PRO-ACT dataset by training five covariate-based
survival models to estimate the probability of an event over a 500-day period
after a baseline visit. We then predict five event-specific individual survival
distributions (ISDs) for each patient, each providing an interpretable and
meaningful estimate of when that event will likely take place in the future.
The results show that covariate-based models are superior to the Kaplan-Meier
estimator at predicting time-to-event outcomes. Additionally, our method
enables practitioners to make individual counterfactual predictions, where
certain features (covariates) can be changed to see their effect on the
predicted outcome. In this regard, we find that Riluzole has little to no
impact on predicted functional decline. However, for patients with bulbar-onset
ALS, our method predicts considerably shorter counterfactual time-to-event
estimates for tasks related to speech and swallowing compared to limb-onset
ALS. The proposed method can be applied to current clinical examination data to
assess the risk of functional decline and thus allow more personalized
treatment planning.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [292] [No Audiogram: Leveraging Existing Scores for Personalized Speech Intelligibility Prediction](https://arxiv.org/abs/2506.02039)
*Haoshuai Zhou,Changgeng Mo,Boxuan Cao,Linkai Li,Shan Xiang Wang*

Main category: eess.AS

TL;DR: 提出SSIPNet模型，利用少量支持样本预测个性化语音清晰度，优于传统听力图方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于听力图的语音清晰度预测方法精度有限，因其仅捕捉纯音听力阈值。需要更精准的个性化预测方案。

Method: 开发SSIPNet深度学习模型，通过语音基础模型从（音频，得分）支持样本构建高维表征，预测新音频的识别表现。

Result: 在Clarity数据集上，即使支持样本很少，SSIPNet仍超越听力图基线方法。

Conclusion: 该研究为个性化语音清晰度预测提供了新范式，证明支持样本驱动方法的有效性。

Abstract: Personalized speech intelligibility prediction is challenging. Previous
approaches have mainly relied on audiograms, which are inherently limited in
accuracy as they only capture a listener's hearing threshold for pure tones.
Rather than incorporating additional listener features, we propose a novel
approach that leverages an individual's existing intelligibility data to
predict their performance on new audio. We introduce the Support Sample-Based
Intelligibility Prediction Network (SSIPNet), a deep learning model that
leverages speech foundation models to build a high-dimensional representation
of a listener's speech recognition ability from multiple support (audio, score)
pairs, enabling accurate predictions for unseen audio. Results on the Clarity
Prediction Challenge dataset show that, even with a small number of support
(audio, score) pairs, our method outperforms audiogram-based predictions. Our
work presents a new paradigm for personalized speech intelligibility
prediction.

</details>


### [293] [Evaluating the Effectiveness of Pre-Trained Audio Embeddings for Classification of Parkinson's Disease Speech Data](https://arxiv.org/abs/2506.02078)
*Emmy Postma,Cristian Tejedor-Garcia*

Main category: eess.AS

TL;DR: 该研究评估了三种预训练音频嵌入模型在帕金森病分类中的表现，发现OpenL3在特定任务中表现最佳，同时揭示了Wav2Vec2.0存在性别偏差。


<details>
  <summary>Details</summary>
Motivation: 帕金森病（PD）常伴随言语障碍，现有基于深度声学特征的诊断技术因个体差异效果不稳定，需进一步探索。

Method: 使用NeuroVoz数据集，比较OpenL3、VGGish和Wav2Vec2.0三种预训练音频嵌入模型在DDK和LR任务中的分类效果。

Result: OpenL3在DDK和LR任务中表现最优；Wav2Vec2.0在DDK任务中对男性数据存在显著性别偏差；误分类案例揭示了非典型语音模式的挑战。

Conclusion: 需改进特征提取和模型鲁棒性以提升PD检测效果，尤其需解决性别偏差和非典型语音模式问题。

Abstract: Speech impairments are prevalent biomarkers for Parkinson's Disease (PD),
motivating the development of diagnostic techniques using speech data for
clinical applications. Although deep acoustic features have shown promise for
PD classification, their effectiveness often varies due to individual speaker
differences, a factor that has not been thoroughly explored in the existing
literature. This study investigates the effectiveness of three pre-trained
audio embeddings (OpenL3, VGGish and Wav2Vec2.0 models) for PD classification.
Using the NeuroVoz dataset, OpenL3 outperforms others in diadochokinesis (DDK)
and listen and repeat (LR) tasks, capturing critical acoustic features for PD
detection. Only Wav2Vec2.0 shows significant gender bias, achieving more
favorable results for male speakers, in DDK tasks. The misclassified cases
reveal challenges with atypical speech patterns, highlighting the need for
improved feature extraction and model robustness in PD detection.

</details>


### [294] [Enhancing GOP in CTC-Based Mispronunciation Detection with Phonological Knowledge](https://arxiv.org/abs/2506.02080)
*Aditya Kamlesh Parikh,Cristian Tejedor-Garcia,Catia Cucchiarini,Helmer Strik*

Main category: eess.AS

TL;DR: 论文提出了一种基于音素聚类的对齐无关GOP方法，通过限制音素替换提升计算效率，在L2英语数据集上优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统GOP依赖强制对齐，易受标注和分段误差影响；而对齐无关方法计算成本高且扩展性差。需要一种高效且鲁棒的发音质量评估方法。

Method: 提出限制音素替换的alignment-free GOP方法（RPS），利用音素聚类和学习者常见错误约束替换空间。

Result: 在MPC（儿童语音）和SpeechOcean762（儿童/成人语音）数据集上，RPS配置优于无限制替换（UPS）和基线方法。

Conclusion: 音素聚类约束能有效提升对齐无关GOP的效率，未来可进一步优化聚类策略和错误模式建模。

Abstract: Computer-Assisted Pronunciation Training (CAPT) systems employ automatic
measures of pronunciation quality, such as the goodness of pronunciation (GOP)
metric. GOP relies on forced alignments, which are prone to labeling and
segmentation errors due to acoustic variability. While alignment-free methods
address these challenges, they are computationally expensive and scale poorly
with phoneme sequence length and inventory size. To enhance efficiency, we
introduce a substitution-aware alignment-free GOP that restricts phoneme
substitutions based on phoneme clusters and common learner errors. We evaluated
our GOP on two L2 English speech datasets, one with child speech, My
Pronunciation Coach (MPC), and SpeechOcean762, which includes child and adult
speech. We compared RPS (restricted phoneme substitutions) and UPS
(unrestricted phoneme substitutions) setups within alignment-free methods,
which outperformed the baseline. We discuss our results and outline avenues for
future research.

</details>


### [295] [Dhvani: A Weakly-supervised Phonemic Error Detection and Personalized Feedback System for Hindi](https://arxiv.org/abs/2506.02166)
*Arnav Rustagi,Satvik Bajpai,Nimrat Kaur,Siddharth Siddharth*

Main category: eess.AS

TL;DR: 该论文提出了一种针对印地语的计算机辅助发音训练系统Dhvani，填补了印度语言发音工具的空白。


<details>
  <summary>Details</summary>
Motivation: 尽管印度语言拥有15亿使用者，但针对这些语言的发音训练工具严重不足，尤其是印地语作为全球第四大语言，改善其发音是解决这一问题的关键。

Method: 论文提出了Dhvani系统，包括印地语错误发音的合成语音生成和个性化反馈的新方法，利用印地语的高度语音正字法分析发音错误。

Result: Dhvani系统能够分析发音错误并提供针对性反馈，为印地语学习者提供有效的发音训练工具。

Conclusion: 该研究为印度语言的发音训练提供了重要工具，Dhvani系统的开发是填补这一领域空白的关键一步。

Abstract: Computer-Assisted Pronunciation Training (CAPT) has been extensively studied
for English. However, there remains a critical gap in its application to Indian
languages with a base of 1.5 billion speakers. Pronunciation tools tailored to
Indian languages are strikingly lacking despite the fact that millions learn
them every year. With over 600 million speakers and being the fourth
most-spoken language worldwide, improving Hindi pronunciation is a vital first
step toward addressing this gap. This paper proposes 1) Dhvani -- a novel CAPT
system for Hindi, 2) synthetic speech generation for Hindi mispronunciations,
and 3) a novel methodology for providing personalized feedback to learners.
While the system often interacts with learners using Devanagari graphemes, its
core analysis targets phonemic distinctions, leveraging Hindi's highly phonetic
orthography to analyze mispronounced speech and provide targeted feedback.

</details>


### [296] [Prompt-Unseen-Emotion: Zero-shot Expressive Speech Synthesis with Prompt-LLM Contextual Knowledge for Mixed Emotions](https://arxiv.org/abs/2506.02742)
*Xiaoxue Gao,Huayun Zhang,Nancy F. Chen*

Main category: eess.AS

TL;DR: 论文提出了一种新颖的Prompt-Unseen-Emotion (PUE)方法，通过情感引导的提示学习生成未见过的情感语音，以提升语音合成的自然性和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的表达性文本到语音（TTS）系统主要建模有限的一组分类情感，而人类对话的情感远超出这些预定义情感，因此需要探索更多样化的情感语音生成以实现更自然的交互。

Method: 论文提出PUE方法，利用LLM-TTS架构进行训练，确保分类情感相关提示与情感语音之间的情感一致性，并通过灵活调整情感比例和利用LLM上下文知识生成混合情感语音。

Result: PUE方法成功在零样本设置下实现了未见情感的表达性语音合成，能够量化不同情感风格。

Conclusion: PUE方法为情感语音合成提供了新的解决方案，能够生成更自然和多样化的情感语音。

Abstract: Existing expressive text-to-speech (TTS) systems primarily model a limited
set of categorical emotions, whereas human conversations extend far beyond
these predefined emotions, making it essential to explore more diverse
emotional speech generation for more natural interactions. To bridge this gap,
this paper proposes a novel prompt-unseen-emotion (PUE) approach to generate
unseen emotional speech via emotion-guided prompt learning. PUE is trained
utilizing an LLM-TTS architecture to ensure emotional consistency between
categorical emotion-relevant prompts and emotional speech, allowing the model
to quantitatively capture different emotion weightings per utterance. During
inference, mixed emotional speech can be generated by flexibly adjusting
emotion proportions and leveraging LLM contextual knowledge, enabling the model
to quantify different emotional styles. Our proposed PUE successfully
facilitates expressive speech synthesis of unseen emotions in a zero-shot
setting.

</details>


### [297] [CapSpeech: Enabling Downstream Applications in Style-Captioned Text-to-Speech](https://arxiv.org/abs/2506.02863)
*Helin Wang,Jiarui Hai,Dading Chong,Karan Thakkar,Tiantian Feng,Dongchao Yang,Junhyeok Lee,Laureano Moro Velazquez,Jesus Villalba,Zengyi Qin,Shrikanth Narayanan,Mounya Elhiali,Najim Dehak*

Main category: eess.AS

TL;DR: 论文提出了CapSpeech基准，用于解决风格标注文本到语音合成（CapTTS）领域的数据集标准化和下游任务研究不足的问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI在风格标注文本到语音合成（CapTTS）领域进展显著，但缺乏标准化数据集和下游任务研究，限制了实际应用。

Method: 引入CapSpeech基准，包含机器标注和人工标注的音频-标注对，并针对特定任务收集新数据集，使用自回归和非自回归模型进行实验。

Result: 实验结果表明，CapSpeech能生成高保真、高可懂度的多样化语音风格，是目前最大的CapTTS相关任务标注数据集。

Conclusion: CapSpeech为CapTTS系统开发提供了宝贵见解，解决了数据集和任务研究的不足，推动了该领域的实际应用。

Abstract: Recent advancements in generative artificial intelligence have significantly
transformed the field of style-captioned text-to-speech synthesis (CapTTS).
However, adapting CapTTS to real-world applications remains challenging due to
the lack of standardized, comprehensive datasets and limited research on
downstream tasks built upon CapTTS. To address these gaps, we introduce
CapSpeech, a new benchmark designed for a series of CapTTS-related tasks,
including style-captioned text-to-speech synthesis with sound events
(CapTTS-SE), accent-captioned TTS (AccCapTTS), emotion-captioned TTS
(EmoCapTTS), and text-to-speech synthesis for chat agent (AgentTTS). CapSpeech
comprises over 10 million machine-annotated audio-caption pairs and nearly 0.36
million human-annotated audio-caption pairs. In addition, we introduce two new
datasets collected and recorded by a professional voice actor and experienced
audio engineers, specifically for the AgentTTS and CapTTS-SE tasks. Alongside
the datasets, we conduct comprehensive experiments using both autoregressive
and non-autoregressive models on CapSpeech. Our results demonstrate
high-fidelity and highly intelligible speech synthesis across a diverse range
of speaking styles. To the best of our knowledge, CapSpeech is the largest
available dataset offering comprehensive annotations for CapTTS-related tasks.
The experiments and findings further provide valuable insights into the
challenges of developing CapTTS systems.

</details>


### [298] [Diffusion Buffer: Online Diffusion-based Speech Enhancement with Sub-Second Latency](https://arxiv.org/abs/2506.02908)
*Bunlong Lay,Rostilav Makarov,Timo Gerkmann*

Main category: eess.AS

TL;DR: 本文提出了一种基于滑动窗口扩散模型的实时语音增强方法，通过时间渐进加噪策略平衡性能与延迟，首次实现了扩散模型在在线语音增强中的实际应用。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在语音增强中表现优异，但推理时计算成本高，难以满足实时流式数据处理需求。因此，需要开发一种低延迟的实用化扩散模型解决方案。

Method: 采用滑动窗口扩散框架，随时间渐进对语音信号加噪（当前帧噪声更大），通过缓冲区大小调节延迟，实现性能与延迟的权衡。

Result: 该方法优于标准扩散模型，GPU推理延迟仅0.3-1秒，成为首个实用的在线语音增强扩散模型方案。

Conclusion: 滑动窗口扩散框架成功解决了扩散模型实时性难题，为在线语音增强提供了高效可行的新范式。

Abstract: Diffusion models are a class of generative models that have been recently
used for speech enhancement with remarkable success but are computationally
expensive at inference time. Therefore, these models are impractical for
processing streaming data in real-time. In this work, we adapt a sliding window
diffusion framework to the speech enhancement task. Our approach progressively
corrupts speech signals through time, assigning more noise to frames close to
the present in a buffer. This approach outputs denoised frames with a delay
proportional to the chosen buffer size, enabling a trade-off between
performance and latency. Empirical results demonstrate that our method
outperforms standard diffusion models and runs efficiently on a GPU, achieving
an input-output latency in the order of 0.3 to 1 seconds. This marks the first
practical diffusion-based solution for online speech enhancement.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [299] [Ensemble-MIX: Enhancing Sample Efficiency in Multi-Agent RL Using Ensemble Methods](https://arxiv.org/abs/2506.02841)
*Tom Danino,Nahum Shimkin*

Main category: eess.SY

TL;DR: 提出一种结合分解中心化评论家与去中心化集成学习的多智能体强化学习算法，通过选择性探索和混合样本训练提升性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习（MARL）通常需要比单智能体更多的环境交互才能收敛，且面临联合动作空间探索困难和高方差问题。

Method: 算法采用分解中心化评论家与集成学习，利用集成峰度指导探索，并引入截断TD(λ)和混合样本训练提升效率与稳定性。

Result: 在标准MARL基准测试（如SMAC II地图）上，该方法优于现有最先进基线。

Conclusion: 所提算法通过选择性探索和混合训练策略，显著提升了MARL的样本效率和性能。

Abstract: Multi-agent reinforcement learning (MARL) methods have achieved
state-of-the-art results on a range of multi-agent tasks. Yet, MARL algorithms
typically require significantly more environment interactions than their
single-agent counterparts to converge, a problem exacerbated by the difficulty
in exploring over a large joint action space and the high variance intrinsic to
MARL environments. To tackle these issues, we propose a novel algorithm that
combines a decomposed centralized critic with decentralized ensemble learning,
incorporating several key contributions. The main component in our scheme is a
selective exploration method that leverages ensemble kurtosis. We extend the
global decomposed critic with a diversity-regularized ensemble of individual
critics and utilize its excess kurtosis to guide exploration toward
high-uncertainty states and actions. To improve sample efficiency, we train the
centralized critic with a novel truncated variation of the TD($\lambda$)
algorithm, enabling efficient off-policy learning with reduced variance. On the
actor side, our suggested algorithm adapts the mixed samples approach to MARL,
mixing on-policy and off-policy loss functions for training the actors. This
approach balances between stability and efficiency and outperforms purely
off-policy learning. The evaluation shows our method outperforms
state-of-the-art baselines on standard MARL benchmarks, including a variety of
SMAC II maps.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [300] [A Novel Deep Reinforcement Learning Method for Computation Offloading in Multi-User Mobile Edge Computing with Decentralization](https://arxiv.org/abs/2506.02458)
*Nguyen Chi Long,Trinh Van Chien,Ta Hai Tung,Van Son Nguyen,Trong-Minh Hoang,Nguyen Ngoc Hai Dang*

Main category: cs.IT

TL;DR: 该论文研究了在移动边缘计算（MEC）系统中使用深度强化学习（DRL）算法来寻找可行的分散动态计算卸载策略，提出了一种基于Twin Delayed DDPG算法的新方法，以克服传统DDPG算法的局限性。


<details>
  <summary>Details</summary>
Motivation: 移动边缘计算（MEC）允许设备将计算任务卸载到邻近的MEC服务器上，以解决计算能力有限的问题。然而，传统的DDPG算法在MEC系统中仍存在一些固有弱点，因此需要一种更有效的方法来优化计算卸载策略。

Method: 论文提出了一种基于Twin Delayed DDPG算法的新方法，用于独立学习每个用户的计算卸载策略，从而克服传统DDPG算法的局限性。

Result: 数值结果表明，通过所提出的方法，单个用户可以自主学习到合适的策略，并且该方法的性能优于传统的基于DDPG的功率控制策略。

Conclusion: 该研究通过引入Twin Delayed DDPG算法，成功构建了一个可扩展的MEC系统，能够在有限反馈下有效运行，为移动用户提供了更优的计算卸载解决方案。

Abstract: Mobile edge computing (MEC) allows appliances to offload workloads to
neighboring MEC servers that have the potential for computation-intensive tasks
with limited computational capabilities. This paper studied how deep
reinforcement learning (DRL) algorithms are used in an MEC system to find
feasible decentralized dynamic computation offloading strategies, which leads
to the construction of an extensible MEC system that operates effectively with
finite feedback. Even though the Deep Deterministic Policy Gradient (DDPG)
algorithm, subject to their knowledge of the MEC system, can be used to
allocate powers of both computation offloading and local execution, to learn a
computation offloading policy for each user independently, we realized that
this solution still has some inherent weaknesses. Hence, we introduced a new
approach for this problem based on the Twin Delayed DDPG algorithm, which
enables us to overcome this proneness and investigate cases where mobile users
are portable. Numerical results showed that individual users can autonomously
learn adequate policies through the proposed approach. Besides, the performance
of the suggested solution exceeded the conventional DDPG-based power control
strategy.

</details>


### [301] [Maximizing the Promptness of Metaverse Systems using Edge Computing by Deep Reinforcement Learning](https://arxiv.org/abs/2506.02657)
*Tam Ninh Thi-Thanh,Trinh Van Chien,Hung Tran,Nguyen Hoai Son,Van Nhan Vo*

Main category: cs.IT

TL;DR: 本文探讨了深度强化学习(DRL)在支持基于元宇宙的数字孪生系统中的应用，提出了一种在动态环境下高效卸载任务的DRL算法。


<details>
  <summary>Details</summary>
Motivation: 元宇宙和数字孪生技术正引领未来数字世界的发展，但动态环境下的数据处理和任务卸载效率成为关键挑战。本文旨在利用DRL提升数字孪生在动态环境中的实时性。

Method: 系统包含元宇宙用户设备、虚拟接入点(MVAP)和边缘计算服务器。通过DRL算法动态优化任务卸载策略，以适应参数随时间变化的环境。

Result: 实验结果表明，所提出的DRL算法能有效处理动态环境中的任务卸载，确保数字孪生的实时性。

Conclusion: 本研究验证了DRL在元宇宙数字孪生系统中的适用性，为动态环境下的高效数据处理提供了可行方案。

Abstract: Metaverse and Digital Twin (DT) have attracted much academic and industrial
attraction to approach the future digital world. This paper introduces the
advantages of deep reinforcement learning (DRL) in assisting Metaverse
system-based Digital Twin. In this system, we assume that it includes several
Metaverse User devices collecting data from the real world to transfer it into
the virtual world, a Metaverse Virtual Access Point (MVAP) undertaking the
processing of data, and an edge computing server that receives the offloading
data from the MVAP. The proposed model works under a dynamic environment with
various parameters changing over time. The experiment results show that our
proposed DRL algorithm is suitable for offloading tasks to ensure the
promptness of DT in a dynamic environment.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [302] [Enabling Probabilistic Learning on Manifolds through Double Diffusion Maps](https://arxiv.org/abs/2506.02254)
*Dimitris G Giovanis,Nikolaos Evangelou,Ioannis G Kevrekidis,Roger G Ghanem*

Main category: stat.ML

TL;DR: 本文提出了一种基于改进PLoM方法的生成学习框架，通过结合双扩散映射与几何谐波技术，解决了小样本高维数据下的过拟合问题，并在数值实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 原始PLoM方法在数据量少且扩散映射基维度接近样本数时会出现过拟合和泛化能力下降的问题，需要一种新方法来克服这一限制。

Method: 结合双扩散映射（捕捉多尺度几何特征）和几何谐波（实现高维空间非线性插值），直接在潜空间求解完整阶ISDE，保留系统动态复杂性。

Result: 通过二维Hermite多项式函数和爆轰波高保真模拟两个数值实验，证明了所提方法的有效性和鲁棒性。

Conclusion: 新方法成功解决了小样本高维数据下的过拟合问题，为复杂系统建模提供了更可靠的生成学习框架。

Abstract: We present a generative learning framework for probabilistic sampling based
on an extension of the Probabilistic Learning on Manifolds (PLoM) approach,
which is designed to generate statistically consistent realizations of a random
vector in a finite-dimensional Euclidean space, informed by a limited (yet
representative) set of observations. In its original form, PLoM constructs a
reduced-order probabilistic model by combining three main components: (a)
kernel density estimation to approximate the underlying probability measure,
(b) Diffusion Maps to uncover the intrinsic low-dimensional manifold structure,
and (c) a reduced-order Ito Stochastic Differential Equation (ISDE) to sample
from the learned distribution. A key challenge arises, however, when the number
of available data points N is small and the dimensionality of the diffusion-map
basis approaches N, resulting in overfitting and loss of generalization. To
overcome this limitation, we propose an enabling extension that implements a
synthesis of Double Diffusion Maps -- a technique capable of capturing
multiscale geometric features of the data -- with Geometric Harmonics (GH), a
nonparametric reconstruction method that allows smooth nonlinear interpolation
in high-dimensional ambient spaces. This approach enables us to solve a
full-order ISDE directly in the latent space, preserving the full dynamical
complexity of the system, while leveraging its reduced geometric
representation. The effectiveness and robustness of the proposed method are
illustrated through two numerical studies: one based on data generated from
two-dimensional Hermite polynomial functions and another based on high-fidelity
simulations of a detonation wave in a reactive flow.

</details>


### [303] [Assumption-free stability for ranking problems](https://arxiv.org/abs/2506.02257)
*Ruiting Liang,Jake A. Soloff,Rina Foygel Barber,Rebecca Willett*

Main category: stat.ML

TL;DR: 本文提出了一种新的算法稳定性框架，用于解决排名问题中的不稳定性，特别是在分数相近的项目中。通过引入两种新的排名操作符，即“膨胀前k项”和“膨胀全排名”，确保了排名的稳定性，无需依赖数据分布假设或候选数量。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的排名问题往往存在不稳定性，尤其是当项目分数相近时，微小数据波动可能导致排名变化。现有理论常假设项目间存在明显分离，但实际数据中常有分数相近的项目，限制了现有理论的适用性。

Method: 提出了一种新的算法稳定性框架，并设计了两种排名操作符：用于前k项选择的“膨胀前k项”和用于全排名的“膨胀全排名”。这些方法通过允许输出中的不确定性来实现稳定性。

Result: 提出的方法在不依赖数据分布假设和候选数量的情况下，提供了稳定的排名结果。实验验证了这些方法在保持输出信息量的同时，显著提高了稳定性。

Conclusion: 本文通过引入新的排名操作符和稳定性框架，有效解决了排名问题中的不稳定性问题，为实际应用提供了可靠的理论和工具支持。

Abstract: In this work, we consider ranking problems among a finite set of candidates:
for instance, selecting the top-$k$ items among a larger list of candidates or
obtaining the full ranking of all items in the set. These problems are often
unstable, in the sense that estimating a ranking from noisy data can exhibit
high sensitivity to small perturbations. Concretely, if we use data to provide
a score for each item (say, by aggregating preference data over a sample of
users), then for two items with similar scores, small fluctuations in the data
can alter the relative ranking of those items. Many existing theoretical
results for ranking problems assume a separation condition to avoid this
challenge, but real-world data often contains items whose scores are
approximately tied, limiting the applicability of existing theory. To address
this gap, we develop a new algorithmic stability framework for ranking
problems, and propose two novel ranking operators for achieving stable ranking:
the \emph{inflated top-$k$} for the top-$k$ selection problem and the
\emph{inflated full ranking} for ranking the full list. To enable stability,
each method allows for expressing some uncertainty in the output. For both of
these two problems, our proposed methods provide guaranteed stability, with no
assumptions on data distributions and no dependence on the total number of
candidates to be ranked. Experiments on real-world data confirm that the
proposed methods offer stability without compromising the informativeness of
the output.

</details>


### [304] [MoCA: Multi-modal Cross-masked Autoencoder for Digital Health Measurements](https://arxiv.org/abs/2506.02260)
*Howon Ryu,Yuliang Chen,Yacun Wang,Andrea Z. LaCroix,Chongzhi Di,Loki Natarajan,Yu Wang,Jingjing Zou*

Main category: stat.ML

TL;DR: 论文提出了一种名为MoCA的自监督学习框架，通过跨模态掩码和Transformer自编码器架构，利用模态内时间相关性和跨模态相关性，解决了数字健康领域多模态数据标注成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 当前数字健康技术生成的多模态数据潜力巨大，但主流监督学习方法依赖大量标注数据，成本高昂且不切实际，尤其在临床研究中。

Method: 提出Multi-modal Cross-masked Autoencoder (MoCA)框架，结合跨模态掩码策略和Transformer自编码器，同时利用模态内时间相关性和跨模态相关性进行自监督学习。

Result: 实验表明MoCA在数据重建和下游任务中优于现有方法，并提供了跨模态掩码方案有效性的理论保证。开源了数据处理、预训练及下游任务代码。

Conclusion: 该研究展示了自监督学习在数字健康和多模态数据中的变革潜力，为减少对标注数据的依赖提供了有效解决方案。

Abstract: The growing prevalence of digital health technologies has led to the
generation of complex multi-modal data, such as physical activity measurements
simultaneously collected from various sensors of mobile and wearable devices.
These data hold immense potential for advancing health studies, but current
methods predominantly rely on supervised learning, requiring extensive labeled
datasets that are often expensive or impractical to obtain, especially in
clinical studies. To address this limitation, we propose a self-supervised
learning framework called Multi-modal Cross-masked Autoencoder (MoCA) that
leverages cross-modality masking and the Transformer autoencoder architecture
to utilize both temporal correlations within modalities and cross-modal
correlations between data streams. We also provide theoretical guarantees to
support the effectiveness of the cross-modality masking scheme in MoCA.
Comprehensive experiments and ablation studies demonstrate that our method
outperforms existing approaches in both reconstruction and downstream tasks. We
release open-source code for data processing, pre-training, and downstream
tasks in the supplementary materials. This work highlights the transformative
potential of self-supervised learning in digital health and multi-modal data.

</details>


### [305] [Large Stepsizes Accelerate Gradient Descent for Regularized Logistic Regression](https://arxiv.org/abs/2506.02336)
*Jingfeng Wu,Pierre Marion,Peter Bartlett*

Main category: stat.ML

TL;DR: 论文研究表明，在线性可分数据的L2正则化逻辑回归中，使用大步长的梯度下降法可以加速收敛速度，从传统的O~(κ)提升到O~(√κ)。


<details>
  <summary>Details</summary>
Motivation: 传统理论建议使用小步长以确保优化目标的单调递减，但作者发现使用大步长可以显著加速收敛，尤其是在线性可分数据的情况下。

Method: 研究采用梯度下降法（GD）结合恒定步长，应用于L2正则化的逻辑回归模型，分析不同步长对收敛速度的影响。

Result: 结果显示，使用大步长可以将收敛速度从O~(κ)加速到O~(√κ)，并且在特定情况下还能扩展到最小化总体风险。

Conclusion: 论文结论表明，大步长在特定情况下可以显著加速梯度下降法的收敛速度，扩展了现有理论的应用范围。

Abstract: We study gradient descent (GD) with a constant stepsize for
$\ell_2$-regularized logistic regression with linearly separable data.
Classical theory suggests small stepsizes to ensure monotonic reduction of the
optimization objective, achieving exponential convergence in
$\widetilde{\mathcal{O}}(\kappa)$ steps with $\kappa$ being the condition
number. Surprisingly, we show that this can be accelerated to
$\widetilde{\mathcal{O}}(\sqrt{\kappa})$ by simply using a large stepsize --
for which the objective evolves nonmonotonically. The acceleration brought by
large stepsizes extends to minimizing the population risk for separable
distributions, improving on the best-known upper bounds on the number of steps
to reach a near-optimum. Finally, we characterize the largest stepsize for the
local convergence of GD, which also determines the global convergence in
special scenarios. Our results extend the analysis of Wu et al. (2024) from
convex settings with minimizers at infinity to strongly convex cases with
finite minimizers.

</details>


### [306] [Tensor State Space-based Dynamic Multilayer Network Modeling](https://arxiv.org/abs/2506.02413)
*Tian Lan,Jie Guo,Chen Zhang*

Main category: stat.ML

TL;DR: 本文提出了一种新的动态多层网络张量状态空间模型（TSSDMN），通过对称Tucker分解和变分EM算法有效捕捉网络的时间动态和跨层交互。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以捕捉动态多层网络的时间动态和跨层交互，因此需要一种新方法来更好地理解和建模这些复杂网络。

Method: TSSDMN采用对称Tucker分解表示潜在节点特征、交互模式和层间转换，并通过变分期望最大化算法进行高效推理。

Result: 数值模拟和案例研究表明，TSSDMN能有效理解动态多层网络的时空动态特性。

Conclusion: TSSDMN为动态多层网络建模提供了一种高效且可解释的新方法，具有广泛的应用潜力。

Abstract: Understanding the complex interactions within dynamic multilayer networks is
critical for advancements in various scientific domains. Existing models often
fail to capture such networks' temporal and cross-layer dynamics. This paper
introduces a novel Tensor State Space Model for Dynamic Multilayer Networks
(TSSDMN), utilizing a latent space model framework. TSSDMN employs a symmetric
Tucker decomposition to represent latent node features, their interaction
patterns, and layer transitions. Then by fixing the latent features and
allowing the interaction patterns to evolve over time, TSSDMN uniquely captures
both the temporal dynamics within layers and across different layers. The model
identifiability conditions are discussed. By treating latent features as
variables whose posterior distributions are approximated using a mean-field
variational inference approach, a variational Expectation Maximization
algorithm is developed for efficient model inference. Numerical simulations and
case studies demonstrate the efficacy of TSSDMN for understanding dynamic
multilayer networks.

</details>


### [307] [Asymptotics of SGD in Sequence-Single Index Models and Single-Layer Attention Networks](https://arxiv.org/abs/2506.02651)
*Luca Arnaboldi,Bruno Loureiro,Ludovic Stephan,Florent Krzakala,Lenka Zdeborova*

Main category: stat.ML

TL;DR: 该论文研究了序列单索引(SSI)模型中随机梯度下降(SGD)的动力学特性，揭示了两个训练阶段及其影响因素。


<details>
  <summary>Details</summary>
Motivation: 研究序列数据中的结构如何有助于基于注意力模型的学习，将经典单索引模型推广到序列领域。

Method: 推导了种群损失的闭式表达式，并通过一对充分统计量描述语义和位置对齐，分析高维SGD动力学。

Result: 发现两个训练阶段：从无信息初始化逃逸和与目标子空间对齐，序列长度和位置编码影响收敛速度和学习轨迹。

Conclusion: 为理解基于注意力的模型如何从序列结构中受益提供了严格且可解释的理论基础。

Abstract: We study the dynamics of stochastic gradient descent (SGD) for a class of
sequence models termed Sequence Single-Index (SSI) models, where the target
depends on a single direction in input space applied to a sequence of tokens.
This setting generalizes classical single-index models to the sequential
domain, encompassing simplified one-layer attention architectures. We derive a
closed-form expression for the population loss in terms of a pair of sufficient
statistics capturing semantic and positional alignment, and characterize the
induced high-dimensional SGD dynamics for these coordinates. Our analysis
reveals two distinct training phases: escape from uninformative initialization
and alignment with the target subspace, and demonstrates how the sequence
length and positional encoding influence convergence speed and learning
trajectories. These results provide a rigorous and interpretable foundation for
understanding how sequential structure in data can be beneficial for learning
with attention-based models.

</details>


### [308] [Computational Thresholds in Multi-Modal Learning via the Spiked Matrix-Tensor Model](https://arxiv.org/abs/2506.02664)
*Hugo Tabanelli,Pierre Mergny,Lenka Zdeborova,Florent Krzakala*

Main category: stat.ML

TL;DR: 该论文研究了从两种噪声相关模态（尖峰矩阵和尖峰张量）中恢复高维信号的问题，发现贝叶斯近似消息传递能有效利用模态相关性实现恢复，而联合学习会降低性能，提出顺序课程学习策略可解决这一瓶颈。


<details>
  <summary>Details</summary>
Motivation: 探索在共享低秩结构的尖峰矩阵和张量模型中，如何利用模态间的相关性实现高效信号恢复，并揭示多模态学习中的算法行为与局限性。

Method: 采用贝叶斯近似消息传递（BAMP）处理相关模态，并提出顺序课程学习策略（先恢复矩阵再引导张量恢复），结合谱方法实现。

Result: BAMP能诱导类似神经网络的阶梯状相变，而联合经验风险最小化会显著降低性能；顺序学习策略可达到最优弱恢复阈值。

Conclusion: 多模态高维推断中，结构相关性和学习顺序至关重要，顺序课程学习能有效解决联合优化的瓶颈问题。

Abstract: We study the recovery of multiple high-dimensional signals from two noisy,
correlated modalities: a spiked matrix and a spiked tensor sharing a common
low-rank structure. This setting generalizes classical spiked matrix and tensor
models, unveiling intricate interactions between inference channels and
surprising algorithmic behaviors. Notably, while the spiked tensor model is
typically intractable at low signal-to-noise ratios, its correlation with the
matrix enables efficient recovery via Bayesian Approximate Message Passing,
inducing staircase-like phase transitions reminiscent of neural network
phenomena. In contrast, empirical risk minimization for joint learning fails:
the tensor component obstructs effective matrix recovery, and joint
optimization significantly degrades performance, highlighting the limitations
of naive multi-modal learning. We show that a simple Sequential Curriculum
Learning strategy-first recovering the matrix, then leveraging it to guide
tensor recovery-resolves this bottleneck and achieves optimal weak recovery
thresholds. This strategy, implementable with spectral methods, emphasizes the
critical role of structural correlation and learning order in multi-modal
high-dimensional inference.

</details>


### [309] [Symmetry-Aware GFlowNets](https://arxiv.org/abs/2506.02685)
*Hohyun Kim,Seunggeun Lee,Min-hwan Oh*

Main category: stat.ML

TL;DR: 提出SA-GFN方法，通过奖励缩放修正图生成中的对称性偏差，实现无偏采样并提升多样性。


<details>
  <summary>Details</summary>
Motivation: 现有GFlowNets方法因状态转移概率计算不准确导致系统偏差，影响图生成质量。

Method: 引入对称性校正，通过奖励缩放直接整合偏差修正，避免显式状态转移计算。

Result: SA-GFN能无偏采样，增强多样性，并稳定生成符合目标分布的高奖励图。

Conclusion: SA-GFN有效解决了图生成中的对称性偏差问题，提升了生成质量和效率。

Abstract: Generative Flow Networks (GFlowNets) offer a powerful framework for sampling
graphs in proportion to their rewards. However, existing approaches suffer from
systematic biases due to inaccuracies in state transition probability
computations. These biases, rooted in the inherent symmetries of graphs, impact
both atom-based and fragment-based generation schemes. To address this
challenge, we introduce Symmetry-Aware GFlowNets (SA-GFN), a method that
incorporates symmetry corrections into the learning process through reward
scaling. By integrating bias correction directly into the reward structure,
SA-GFN eliminates the need for explicit state transition computations.
Empirical results show that SA-GFN enables unbiased sampling while enhancing
diversity and consistently generating high-reward graphs that closely match the
target distribution.

</details>


### [310] [Safely Learning Controlled Stochastic Dynamics](https://arxiv.org/abs/2506.02754)
*Luc Brogat-Motte,Alessandro Rudi,Riccardo Bonalli*

Main category: stat.ML

TL;DR: 提出一种安全学习方法，通过迭代扩展初始安全控制集，确保在训练和部署期间系统轨迹始终处于安全区域，适用于复杂现实系统。


<details>
  <summary>Details</summary>
Motivation: 在自主机器人、金融和生物医学等安全关键应用中，确保系统轨迹在训练和部署期间始终处于预定义安全区域至关重要。

Method: 采用基于核的置信边界迭代扩展初始安全控制集，结合平滑性假设和自适应学习率，实现安全探索和高效动态估计。

Result: 实验证明该方法在安全性、估计准确性和计算效率方面具有实际有效性，并提供理论安全保证。

Conclusion: 该方法在仅需温和平滑假设和初始安全控制集的条件下，广泛适用于复杂现实系统，并随着动态规则性提升自适应优化学习率。

Abstract: We address the problem of safely learning controlled stochastic dynamics from
discrete-time trajectory observations, ensuring system trajectories remain
within predefined safe regions during both training and deployment.
Safety-critical constraints of this kind are crucial in applications such as
autonomous robotics, finance, and biomedicine. We introduce a method that
ensures safe exploration and efficient estimation of system dynamics by
iteratively expanding an initial known safe control set using kernel-based
confidence bounds. After training, the learned model enables predictions of the
system's dynamics and permits safety verification of any given control. Our
approach requires only mild smoothness assumptions and access to an initial
safe control set, enabling broad applicability to complex real-world systems.
We provide theoretical guarantees for safety and derive adaptive learning rates
that improve with increasing Sobolev regularity of the true dynamics.
Experimental evaluations demonstrate the practical effectiveness of our method
in terms of safety, estimation accuracy, and computational efficiency.

</details>


### [311] [Doubly-Robust Estimation of Counterfactual Policy Mean Embeddings](https://arxiv.org/abs/2506.02793)
*Houssam Zenati,Bariscan Bozkurt,Arthur Gretton*

Main category: stat.ML

TL;DR: 该论文提出了一种名为CPME的新框架，用于在再生核希尔伯特空间中表示反事实结果分布，支持灵活的非参数化策略评估，并引入了双重稳健估计器以提高收敛速度。


<details>
  <summary>Details</summary>
Motivation: 在推荐、广告和医疗等领域，准确估计反事实策略下的结果分布对决策至关重要。现有方法在灵活性和非参数化评估方面存在不足，因此需要一种更高效的框架。

Method: 论文提出了CPME框架，使用再生核希尔伯特空间表示反事实结果分布，并开发了插件估计器和双重稳健估计器，后者通过校正嵌入和倾向模型偏差来提高收敛速度。

Result: 实验表明，CPME框架在数值模拟中优于现有方法，双重稳健核检验统计量实现了渐近正态性，支持高效计算和置信区间构建。

Conclusion: CPME框架为反事实策略评估提供了一种灵活且高效的方法，尤其在非参数化分布估计和假设检验方面表现出色，具有广泛的应用潜力。

Abstract: Estimating the distribution of outcomes under counterfactual policies is
critical for decision-making in domains such as recommendation, advertising,
and healthcare. We analyze a novel framework-Counterfactual Policy Mean
Embedding (CPME)-that represents the entire counterfactual outcome distribution
in a reproducing kernel Hilbert space (RKHS), enabling flexible and
nonparametric distributional off-policy evaluation. We introduce both a plug-in
estimator and a doubly robust estimator; the latter enjoys improved uniform
convergence rates by correcting for bias in both the outcome embedding and
propensity models. Building on this, we develop a doubly robust kernel test
statistic for hypothesis testing, which achieves asymptotic normality and thus
enables computationally efficient testing and straightforward construction of
confidence intervals. Our framework also supports sampling from the
counterfactual distribution. Numerical simulations illustrate the practical
benefits of CPME over existing methods.

</details>


### [312] [Asymptotically perfect seeded graph matching without edge correlation (and applications to inference)](https://arxiv.org/abs/2506.02825)
*Tong Qi,Vera Andersson,Peter Viechnicki,Vince Lyzinski*

Main category: stat.ML

TL;DR: OmniMatch算法用于多图匹配，在RDPG模型下能高效对齐未标记顶点，提升测试能力。


<details>
  <summary>Details</summary>
Motivation: 解决多网络顶点对齐问题，特别是在顶点错位导致测试能力下降的场景。

Method: 提出OmniMatch算法，基于种子顶点在RDPG模型下进行多图匹配。

Result: 算法能高效对齐大量未标记顶点，恢复因顶点错位而损失的测试能力。

Conclusion: OmniMatch在模拟和实际数据中表现优异，验证了其有效性和实用性。

Abstract: We present the OmniMatch algorithm for seeded multiple graph matching. In the
setting of $d$-dimensional Random Dot Product Graphs (RDPG), we prove that
under mild assumptions, OmniMatch with $s$ seeds asymptotically and efficiently
perfectly aligns $O(s^{\alpha})$ unseeded vertices -- for $\alpha<2\wedge d/4$
-- across multiple networks even in the presence of no edge correlation. We
demonstrate the effectiveness of our algorithm across numerous simulations and
in the context of shuffled graph hypothesis testing. In the shuffled testing
setting, testing power is lost due to the misalignment/shuffling of vertices
across graphs, and we demonstrate the capacity of OmniMatch to correct for
misaligned vertices prior to testing and hence recover the lost testing power.
We further demonstrate the algorithm on a pair of data examples from
connectomics and machine translation.

</details>


### [313] [Non-stationary Bandit Convex Optimization: A Comprehensive Study](https://arxiv.org/abs/2506.02980)
*Xiaoqi Liu,Dorian Baudry,Julian Zimmert,Patrick Rebeschini,Arya Akhavan*

Main category: stat.ML

TL;DR: 该论文研究了非平稳环境下的Bandit凸优化问题，提出了两种算法TEWA-SE和cExO，分别针对强凸和一般凸损失函数，实现了在已知和未知非平稳性度量下的最优遗憾界。


<details>
  <summary>Details</summary>
Motivation: 研究非平稳环境下的Bandit凸优化问题，旨在最小化在不同非平稳性度量（如比较器序列的切换次数S、损失函数的总变差Δ和比较器序列的路径长度P）下的遗憾。

Method: 提出了两种算法：TEWA-SE（基于睡眠专家框架的倾斜指数加权平均）和cExO（基于离散动作空间的指数加权剪裁探索优化）。TEWA-SE适用于强凸损失函数，cExO适用于一般凸损失函数。

Result: TEWA-SE在已知S和Δ的情况下实现了极小极大最优遗憾界，cExO在已知S和Δ的情况下也达到了最优遗憾界，并在P方面改进了现有最佳边界。

Conclusion: 论文提出的算法在非平稳环境下有效地解决了Bandit凸优化问题，并在不同非平稳性度量下实现了最优或改进的遗憾界。

Abstract: Bandit Convex Optimization is a fundamental class of sequential
decision-making problems, where the learner selects actions from a continuous
domain and observes a loss (but not its gradient) at only one point per round.
We study this problem in non-stationary environments, and aim to minimize the
regret under three standard measures of non-stationarity: the number of
switches $S$ in the comparator sequence, the total variation $\Delta$ of the
loss functions, and the path-length $P$ of the comparator sequence. We propose
a polynomial-time algorithm, Tilted Exponentially Weighted Average with
Sleeping Experts (TEWA-SE), which adapts the sleeping experts framework from
online convex optimization to the bandit setting. For strongly convex losses,
we prove that TEWA-SE is minimax-optimal with respect to known $S$ and $\Delta$
by establishing matching upper and lower bounds. By equipping TEWA-SE with the
Bandit-over-Bandit framework, we extend our analysis to environments with
unknown non-stationarity measures. For general convex losses, we introduce a
second algorithm, clipped Exploration by Optimization (cExO), based on
exponential weights over a discretized action space. While not polynomial-time
computable, this method achieves minimax-optimal regret with respect to known
$S$ and $\Delta$, and improves on the best existing bounds with respect to $P$.

</details>


### [314] [Causal Explainability of Machine Learning in Heart Failure Prediction from Electronic Health Records](https://arxiv.org/abs/2506.03068)
*Yina Hou,Shourav B. Rabbani,Liang Hong,Norou Diawara,Manar D. Samad*

Main category: stat.ML

TL;DR: 本文提出了一种新的计算框架，用于发现混合类型临床变量的因果结构并评分其因果强度，以解决传统方法在非线性因果关系中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统统计和机器学习方法在预测疾病预后时，变量的重要性可能无法反映其与疾病的因果关系。本文旨在探究临床变量在心力衰竭（HF）预后中的因果解释性。

Method: 提出了一种新的计算框架，支持混合类型（分类、数值、二元）临床变量的因果结构发现（CSD）和因果强度评分，特别针对二元疾病结果。

Result: 非线性因果关系的CSD建模比线性模型更有意义；非线性分类器（如梯度提升树）的特征重要性与变量的因果强度高度相关。相关变量可能是HF的因果变量，但很少被识别为效应变量。

Conclusion: 该研究为基于机器学习的预测模型提供了变量的因果解释，有助于更深入地理解临床变量与疾病之间的关系。

Abstract: The importance of clinical variables in the prognosis of the disease is
explained using statistical correlation or machine learning (ML). However, the
predictive importance of these variables may not represent their causal
relationships with diseases. This paper uses clinical variables from a heart
failure (HF) patient cohort to investigate the causal explainability of
important variables obtained in statistical and ML contexts. Due to inherent
regression modeling, popular causal discovery methods strictly assume that the
cause and effect variables are numerical and continuous. This paper proposes a
new computational framework to enable causal structure discovery (CSD) and
score the causal strength of mixed-type (categorical, numerical, binary)
clinical variables for binary disease outcomes. In HF classification, we
investigate the association between the importance rank order of three feature
types: correlated features, features important for ML predictions, and causal
features. Our results demonstrate that CSD modeling for nonlinear causal
relationships is more meaningful than its linear counterparts. Feature
importance obtained from nonlinear classifiers (e.g., gradient-boosting trees)
strongly correlates with the causal strength of variables without
differentiating cause and effect variables. Correlated variables can be causal
for HF, but they are rarely identified as effect variables. These results can
be used to add the causal explanation of variables important for ML-based
prediction modeling.

</details>


### [315] [GL-LowPopArt: A Nearly Instance-Wise Minimax Estimator for Generalized Low-Rank Trace Regression](https://arxiv.org/abs/2506.03074)
*Junghyun Lee,Kyoungseok Jang,Kwang-Sung Jun,Milan Vojnović,Se-Young Yun*

Main category: stat.ML

TL;DR: 提出GL-LowPopArt，一种新型Catoni式估计器，用于广义低秩迹回归，通过两阶段方法实现最优估计误差界，并应用于矩阵补全和双线性决斗老虎机。


<details>
  <summary>Details</summary>
Motivation: 现有广义低秩迹回归方法在估计误差和实验设计方面存在不足，需开发更优方法以应对非线性逆链接函数带来的偏差挑战。

Method: 采用两阶段方法：核范数正则化后接矩阵Catoni估计，控制非线性逆链接函数导致的偏差。

Result: 实现了最优估计误差界，提出新实验设计目标GL(π)，在矩阵补全中达到领先Frobenius误差保证，并在双线性决斗老虎机中改进Borda遗憾界。

Conclusion: GL-LowPopArt在广义低秩迹回归中实现实例级最优性，为矩阵补全和偏好学习等应用提供高效解决方案。

Abstract: We present `GL-LowPopArt`, a novel Catoni-style estimator for generalized
low-rank trace regression. Building on `LowPopArt` (Jang et al., 2024), it
employs a two-stage approach: nuclear norm regularization followed by matrix
Catoni estimation. We establish state-of-the-art estimation error bounds,
surpassing existing guarantees (Fan et al., 2019; Kang et al., 2022), and
reveal a novel experimental design objective, $\mathrm{GL}(\pi)$. The key
technical challenge is controlling bias from the nonlinear inverse link
function, which we address by our two-stage approach. We prove a *local*
minimax lower bound, showing that our `GL-LowPopArt` enjoys instance-wise
optimality up to the condition number of the ground-truth Hessian. Applications
include generalized linear matrix completion, where `GL-LowPopArt` achieves a
state-of-the-art Frobenius error guarantee, and **bilinear dueling bandits**, a
novel setting inspired by general preference learning (Zhang et al., 2024). Our
analysis of a `GL-LowPopArt`-based explore-then-commit algorithm reveals a new,
potentially interesting problem-dependent quantity, along with improved Borda
regret bound than vectorization (Wu et al., 2024).

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [316] [The End Of Universal Lifelong Identifiers: Identity Systems For The AI Era](https://arxiv.org/abs/2506.02027)
*Shriphani Palakodety*

Main category: cs.CR

TL;DR: 论文主张废除通用终身标识符(ULIs)，因其在AI时代存在隐私风险，并提出一种兼容现有流程的加密框架作为替代方案。


<details>
  <summary>Details</summary>
Motivation: 当前跨领域使用的终身唯一标识符(ULIs)在AI能力升级的背景下，传统保护措施已无法应对系统性隐私风险。

Method: 建立基于AI能力的威胁模型，提出满足可审计性等核心属性的密码学框架，保留现有标识符工作流程。

Result: 设计出既支持机构现有流程，又能实现审计和委托等关键功能，并提供ULIs迁移路径的方案。

Conclusion: ULIs与AI时代本质不兼容，需逐步淘汰；提出的加密框架在保持功能性的同时解决了隐私问题。

Abstract: Many identity systems assign a single, static identifier to an individual for
life, reused across domains like healthcare, finance, and education. These
Universal Lifelong Identifiers (ULIs) underpin critical workflows but now pose
systemic privacy risks. We take the position that ULIs are fundamentally
incompatible with the AI era and must be phased out. We articulate a threat
model grounded in modern AI capabilities and show that traditional safeguards
such as redaction, consent, and access controls are no longer sufficient. We
define core properties for identity systems in the AI era and present a
cryptographic framework that satisfies them while retaining compatibility with
existing identifier workflows. Our design preserves institutional workflows,
supports essential functions such as auditability and delegation, and offers a
practical migration path beyond ULIs.

</details>


### [317] [Towards Secure MLOps: Surveying Attacks, Mitigation Strategies, and Research Challenges](https://arxiv.org/abs/2506.02032)
*Raj Patel,Himanshu Tripathi,Jasper Stone,Noorbakhsh Amiri Golilarz,Sudip Mittal,Shahram Rahimi,Vini Chaudhary*

Main category: cs.CR

TL;DR: 本文探讨了机器学习运维（MLOps）生态系统的安全挑战，提出使用MITRE ATLAS框架系统评估攻击，并提供了防御策略和研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着MLOps市场的快速增长，其统一性带来的安全漏洞日益凸显，可能导致严重的经济损失和公共信任危机。

Method: 应用MITRE ATLAS框架，系统分析MLOps各阶段的攻击技术，并结合实际案例和红队演练进行分类。

Result: 提出了针对MLOps生态系统的攻击分类和相应的早期防御策略，并指出了亟需解决的研究空白。

Conclusion: 强调从初始阶段实施强健的安全协议的重要性，以保护MLOps生态系统免受不断演变的网络攻击。

Abstract: The rapid adoption of machine learning (ML) technologies has driven
organizations across diverse sectors to seek efficient and reliable methods to
accelerate model development-to-deployment. Machine Learning Operations (MLOps)
has emerged as an integrative approach addressing these requirements by
unifying relevant roles and streamlining ML workflows. As the MLOps market
continues to grow, securing these pipelines has become increasingly critical.
However, the unified nature of MLOps ecosystem introduces vulnerabilities,
making them susceptible to adversarial attacks where a single misconfiguration
can lead to compromised credentials, severe financial losses, damaged public
trust, and the poisoning of training data. Our paper presents a systematic
application of the MITRE ATLAS (Adversarial Threat Landscape for
Artificial-Intelligence Systems) framework, a comprehensive and continuously
updated catalog of AI-focused attacks, to systematically assess attacks across
different phases of the MLOps ecosystem. We begin by examining the preparatory
phases during which adversaries acquire the essential intelligence required to
initiate their attacks. We then present a structured taxonomy of attack
techniques explicitly mapped to corresponding phases of the MLOps ecosystem,
supported by examples drawn from red-teaming exercises and real-world
incidents. This is followed by a taxonomy of mitigation strategies aligned with
these attack categories, offering actionable early-stage defenses to strengthen
the security of MLOps ecosystem. Given the rapid evolution and adoption of
MLOps, we further highlight key research gaps that require immediate attention.
Our work emphasizes the importance of implementing robust security protocols
from the outset, empowering practitioners to safeguard MLOps ecosystem against
evolving cyber attacks.

</details>


### [318] [Improving LLM Agents with Reinforcement Learning on Cryptographic CTF Challenges](https://arxiv.org/abs/2506.02048)
*Lajos Muzsai,David Imolai,András Lukács*

Main category: cs.CR

TL;DR: 研究者提出了一种名为'random-crypto'的密码学CTF挑战生成框架，通过GRPO方法微调Llama-3.1-8B模型，显著提升了其在密码学任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在网络安全领域的结构化推理和工具辅助计算方面仍存在不足，特别是在密码学问题解决上。

Method: 使用'random-crypto'框架生成挑战，结合GRPO方法微调Llama-3.1-8B模型，使其能在隔离环境中迭代编写和执行Python代码。

Result: 微调后的模型在未见过的'random-crypto'任务上Pass@8提高了53%，在picoCTF密码学问题上Pass@8提高了13个百分点。

Conclusion: GRPO方法通过提升工具调用和代码合成的可靠性，而非表面提示适应，有效增强了模型在密码学任务中的表现。

Abstract: Large Language Models (LLMs) still struggle with the structured reasoning and
tool-assisted computation needed for problem solving in cybersecurity
applications. In this work, we introduce "random-crypto", a cryptographic
Capture-the-Flag (CTF) challenge generator framework that we use to fine-tune a
tool-augmented Llama-3.1-8B with Guided Reinforcement Prompt Optimisation
(GRPO), allowing the agent to iteratively write and execute Python inside an
isolated REPL. GRPO yields a +53% absolute jump in Pass@8 on unseen
"random-crypto" tasks (0.35 -> 0.88) and raises Majority@8 to 0.41. The
fine-tuned agent also generalizes to an external dataset. On a subset of
picoCTF cryptography problems, it improves Pass@8 by +13 pp. Ablations show the
gains stem from more reliable tool invocation and code synthesis, rather than
superficial prompt adaptation.

</details>


### [319] [BitBypass: A New Direction in Jailbreaking Aligned Large Language Models with Bitstream Camouflage](https://arxiv.org/abs/2506.02479)
*Kalyan Nakka,Nitesh Saxena*

Main category: cs.CR

TL;DR: 本文提出了一种名为BitBypass的新型黑盒越狱攻击方法，通过连字符分隔的比特流伪装来绕过大型语言模型的安全对齐机制。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）可能生成有害内容，现有安全对齐技术（如监督微调、人类反馈强化学习等）仍面临对抗攻击的挑战。本文旨在探索一种新的越狱攻击方向，以揭示安全对齐的潜在漏洞。

Method: 开发BitBypass攻击方法，利用比特流信息表示（而非传统提示工程）对5种先进LLM（GPT-4o、Gemini 1.5等）进行黑盒越狱测试。

Result: BitBypass成功绕过所有测试模型的安全防护，诱导生成有害内容，且在隐蔽性和成功率上优于现有越狱攻击方法。

Conclusion: 该研究证明了基于比特流伪装的新型攻击有效性，揭示了LLM安全对齐在底层数据表示层面的脆弱性。

Abstract: The inherent risk of generating harmful and unsafe content by Large Language
Models (LLMs), has highlighted the need for their safety alignment. Various
techniques like supervised fine-tuning, reinforcement learning from human
feedback, and red-teaming were developed for ensuring the safety alignment of
LLMs. However, the robustness of these aligned LLMs is always challenged by
adversarial attacks that exploit unexplored and underlying vulnerabilities of
the safety alignment. In this paper, we develop a novel black-box jailbreak
attack, called BitBypass, that leverages hyphen-separated bitstream camouflage
for jailbreaking aligned LLMs. This represents a new direction in jailbreaking
by exploiting fundamental information representation of data as continuous
bits, rather than leveraging prompt engineering or adversarial manipulations.
Our evaluation of five state-of-the-art LLMs, namely GPT-4o, Gemini 1.5, Claude
3.5, Llama 3.1, and Mixtral, in adversarial perspective, revealed the
capabilities of BitBypass in bypassing their safety alignment and tricking them
into generating harmful and unsafe content. Further, we observed that BitBypass
outperforms several state-of-the-art jailbreak attacks in terms of stealthiness
and attack success. Overall, these results highlights the effectiveness and
efficiency of BitBypass in jailbreaking these state-of-the-art LLMs.

</details>


### [320] [MISLEADER: Defending against Model Extraction with Ensembles of Distilled Models](https://arxiv.org/abs/2506.02362)
*Xueqi Cheng,Minxing Zheng,Shixiang Zhu,Yushun Dong*

Main category: cs.CR

TL;DR: 论文提出MISLEADER防御策略，通过双层优化和异构蒸馏模型集成，有效抵御不依赖OOD假设的模型提取攻击。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法假设攻击者查询包含分布外样本(OOD)，但实际中模型训练数据多样且攻击者查询预算有限，导致防御效果下降。因此，需要不依赖OOD假设的新型防御策略。

Method: 提出MISLEADER框架：1) 将模型保护建模为双层优化问题，平衡正常输入的预测保真度和克隆模型的可提取性；2) 结合数据增强模拟攻击查询，使用异构蒸馏模型集成提升鲁棒性和多样性；3) 提供近似算法与理论误差界。

Result: 多场景实验表明，MISLEADER在保持模型实用性的同时，显著抵抗模型提取攻击。代码已开源。

Conclusion: MISLEADER突破了传统OOD假设限制，为MLaaS场景提供了更实用的模型保护方案。

Abstract: Model extraction attacks aim to replicate the functionality of a black-box
model through query access, threatening the intellectual property (IP) of
machine-learning-as-a-service (MLaaS) providers. Defending against such attacks
is challenging, as it must balance efficiency, robustness, and utility
preservation in the real-world scenario. Despite the recent advances, most
existing defenses presume that attacker queries have out-of-distribution (OOD)
samples, enabling them to detect and disrupt suspicious inputs. However, this
assumption is increasingly unreliable, as modern models are trained on diverse
datasets and attackers often operate under limited query budgets. As a result,
the effectiveness of these defenses is significantly compromised in realistic
deployment scenarios. To address this gap, we propose MISLEADER (enseMbles of
dIStiLled modEls Against moDel ExtRaction), a novel defense strategy that does
not rely on OOD assumptions. MISLEADER formulates model protection as a bilevel
optimization problem that simultaneously preserves predictive fidelity on
benign inputs and reduces extractability by potential clone models. Our
framework combines data augmentation to simulate attacker queries with an
ensemble of heterogeneous distilled models to improve robustness and diversity.
We further provide a tractable approximation algorithm and derive theoretical
error bounds to characterize defense effectiveness. Extensive experiments
across various settings validate the utility-preserving and
extraction-resistant properties of our proposed defense strategy. Our code is
available at https://github.com/LabRAI/MISLEADER.

</details>


### [321] [Blockchain Powered Edge Intelligence for U-Healthcare in Privacy Critical and Time Sensitive Environment](https://arxiv.org/abs/2506.02038)
*Anum Nawaz,Hafiz Humza Mahmood Ramzan,Xianjia Yu,Zhuo Zou,Tomi Westerlund*

Main category: cs.CR

TL;DR: 该论文提出了一种结合边缘智能(EI)和区块链技术的自主计算模型，用于隐私敏感和实时性强的健康应用，包括心律失常的多分类和细粒度访问控制方案。


<details>
  <summary>Details</summary>
Motivation: 边缘智能(EI)虽能通过分布式计算和缓存提升隐私保护和降低延迟，但其架构在数据交互和分布式存储中存在安全漏洞，尤其在健康应用中需兼顾隐私和实时性。

Method: 提出自主计算模型及其交互拓扑，包含实时监测、数据处理器、1D-CNN心律失常分类器，以及链下/链上数据共享的安全访问方案。

Result: 安全性和性能分析验证了模型的高效性，细粒度访问控制可靠，1D-CNN能在资源受限的边缘网关实现精准实时分类。

Conclusion: 该框架成功平衡了健康应用的隐私性、实时性与安全性，为边缘智能与区块链融合提供了可行方案。

Abstract: Edge Intelligence (EI) serves as a critical enabler for privacy-preserving
systems by providing AI-empowered computation and distributed caching services
at the edge, thereby minimizing latency and enhancing data privacy. The
integration of blockchain technology further augments EI frameworks by ensuring
transactional transparency, auditability, and system-wide reliability through a
decentralized network model. However, the operational architecture of such
systems introduces inherent vulnerabilities, particularly due to the extensive
data interactions between edge gateways (EGs) and the distributed nature of
information storage during service provisioning. To address these challenges,
we propose an autonomous computing model along with its interaction topologies
tailored for privacy-critical and time-sensitive health applications. The
system supports continuous monitoring, real-time alert notifications, disease
detection, and robust data processing and aggregation. It also includes a data
transaction handler and mechanisms for ensuring privacy at the EGs. Moreover, a
resource-efficient one-dimensional convolutional neural network (1D-CNN) is
proposed for the multiclass classification of arrhythmia, enabling accurate and
real-time analysis of constrained EGs. Furthermore, a secure access scheme is
defined to manage both off-chain and on-chain data sharing and storage. To
validate the proposed model, comprehensive security, performance, and cost
analyses are conducted, demonstrating the efficiency and reliability of the
fine-grained access control scheme.

</details>


### [322] [A Review of Various Datasets for Machine Learning Algorithm-Based Intrusion Detection System: Advances and Challenges](https://arxiv.org/abs/2506.02438)
*Sudhanshu Sekhar Tripathy,Bichitrananda Behera*

Main category: cs.CR

TL;DR: 本文综述了入侵检测系统（IDS）的研究现状，探讨了机器学习在提升IDS效能中的应用，分析了多种算法和数据集，并总结了现有挑战。


<details>
  <summary>Details</summary>
Motivation: 随着全球对技术和自动化流程的依赖日益加深，确保系统、应用和网络的安全成为当今时代的重要挑战。IDS通过检测和防止非法访问来保护机密信息，但其效能仍需提升。

Method: 本文通过文献综述，深入分析了多种基于机器学习和深度学习的入侵检测方法，包括SVM、KNN、DT等算法，并评估了常用数据集如KDDCUP'99、NSL-KDD等的适用性。

Result: 研究总结了不同算法在IDS中的应用效果，提供了详细的表格分析，包括数据集、分类器、检测的攻击类型、评估指标和结论。

Conclusion: 本文为未来的IDS研究提供了全面的综述，指出了现有数据集的挑战，并强调了机器学习在提升IDS准确性中的重要性。

Abstract: IDS aims to protect computer networks from security threats by detecting,
notifying, and taking appropriate action to prevent illegal access and protect
confidential information. As the globe becomes increasingly dependent on
technology and automated processes, ensuring secured systems, applications, and
networks has become one of the most significant problems of this era. The
global web and digital technology have significantly accelerated the evolution
of the modern world, necessitating the use of telecommunications and data
transfer platforms. Researchers are enhancing the effectiveness of IDS by
incorporating popular datasets into machine learning algorithms. IDS, equipped
with machine learning classifiers, enhances security attack detection accuracy
by identifying normal or abnormal network traffic. This paper explores the
methods of capturing and reviewing intrusion detection systems (IDS) and
evaluates the challenges existing datasets face. A deluge of research on
machine learning (ML) and deep learning (DL) architecture-based intrusion
detection techniques has been conducted in the past ten years on various
cybersecurity datasets, including KDDCUP'99, NSL-KDD, UNSW-NB15, CICIDS-2017,
and CSE-CIC-IDS2018. We conducted a literature review and presented an in-depth
analysis of various intrusion detection methods that use SVM, KNN, DT, LR, NB,
RF, XGBOOST, Adaboost, and ANN. We provide an overview of each technique,
explaining the role of the classifiers and algorithms used. A detailed tabular
analysis highlights the datasets used, classifiers employed, attacks detected,
evaluation metrics, and conclusions drawn. This article offers a thorough
review for future IDS research.

</details>


### [323] [CyberGym: Evaluating AI Agents' Cybersecurity Capabilities with Real-World Vulnerabilities at Scale](https://arxiv.org/abs/2506.02548)
*Zhun Wang,Tianneng Shi,Jingxuan He,Matthew Cai,Jialin Zhang,Dawn Song*

Main category: cs.CR

TL;DR: 论文介绍了CyberGym，一个评估大语言模型在网络安全任务中表现的新框架，发现现有模型在复现漏洞方面的成功率较低，但能发现新漏洞。


<details>
  <summary>Details</summary>
Motivation: 当前评估大语言模型在网络安全任务中能力的基准测试不足，无法反映真实场景或范围有限，亟需一个更全面的评估框架。

Method: 提出CyberGym框架，包含1,507个真实漏洞，重点评估模型基于文本描述和源代码生成漏洞复现的概念验证测试的能力。

Result: 评估显示，最佳组合（OpenHands和Claude-3.7-Sonnet）的复现成功率仅为11.9%，且主要针对简单案例；同时发现模型能生成揭示新漏洞的测试，识别了15个零日漏洞。

Conclusion: 大语言模型在网络安全任务中表现有限，尤其在复杂漏洞复现方面，但其生成的测试能发现新漏洞，展示了潜在的应用价值。

Abstract: Large language model (LLM) agents are becoming increasingly skilled at
handling cybersecurity tasks autonomously. Thoroughly assessing their
cybersecurity capabilities is critical and urgent, given the high stakes in
this domain. However, existing benchmarks fall short, often failing to capture
real-world scenarios or being limited in scope. To address this gap, we
introduce CyberGym, a large-scale and high-quality cybersecurity evaluation
framework featuring 1,507 real-world vulnerabilities found and patched across
188 large software projects. While it includes tasks of various settings,
CyberGym primarily focuses on the generation of proof-of-concept (PoC) tests
for vulnerability reproduction, based on text descriptions and corresponding
source repositories. Solving this task is particularly challenging, as it
requires comprehensive reasoning across entire codebases to locate relevant
code fragments and produce effective PoCs that accurately trigger the target
vulnerability starting from the program's entry point. Our evaluation across 4
state-of-the-art agent frameworks and 9 LLMs reveals that even the best
combination (OpenHands and Claude-3.7-Sonnet) achieves only a 11.9%
reproduction success rate, mainly on simpler cases. Beyond reproducing
historical vulnerabilities, we find that PoCs generated by LLM agents can
reveal new vulnerabilities, identifying 15 zero-days affecting the latest
versions of the software projects.

</details>


### [324] [ATAG: AI-Agent Application Threat Assessment with Attack Graphs](https://arxiv.org/abs/2506.02859)
*Parth Atulbhai Gandhi,Akansha Shukla,David Tayouri,Beni Ifland,Yuval Elovici,Rami Puzis,Asaf Shabtai*

Main category: cs.CR

TL;DR: 论文提出了ATAG框架，用于系统分析AI代理应用的安全风险，扩展了MulVAL工具以建模多代理攻击场景，并创建了LLM漏洞数据库LVD。


<details>
  <summary>Details</summary>
Motivation: 由于多代理系统（MASs）内部动态复杂且LLM漏洞不断演变，传统攻击图方法难以有效建模针对LLM的攻击，因此需要新的安全评估方法。

Method: 提出ATAG框架，扩展MulVAL逻辑攻击图生成工具，定制事实和交互规则以准确表示AI代理拓扑、漏洞及攻击场景，并建立LLM漏洞数据库LVD。

Result: 通过两个多代理应用案例验证，ATAG能建模复杂多步攻击场景（如提示注入、过度代理等漏洞），生成攻击图并优先处理风险路径。

Conclusion: ATAG为多代理AI系统提供了理解、可视化和优先处理复杂攻击路径的方法，有助于主动识别和缓解AI代理威胁。

Abstract: Evaluating the security of multi-agent systems (MASs) powered by large
language models (LLMs) is challenging, primarily because of the systems'
complex internal dynamics and the evolving nature of LLM vulnerabilities.
Traditional attack graph (AG) methods often lack the specific capabilities to
model attacks on LLMs. This paper introduces AI-agent application Threat
assessment with Attack Graphs (ATAG), a novel framework designed to
systematically analyze the security risks associated with AI-agent
applications. ATAG extends the MulVAL logic-based AG generation tool with
custom facts and interaction rules to accurately represent AI-agent topologies,
vulnerabilities, and attack scenarios. As part of this research, we also
created the LLM vulnerability database (LVD) to initiate the process of
standardizing LLM vulnerabilities documentation. To demonstrate ATAG's
efficacy, we applied it to two multi-agent applications. Our case studies
demonstrated the framework's ability to model and generate AGs for
sophisticated, multi-step attack scenarios exploiting vulnerabilities such as
prompt injection, excessive agency, sensitive information disclosure, and
insecure output handling across interconnected agents. ATAG is an important
step toward a robust methodology and toolset to help understand, visualize, and
prioritize complex attack paths in multi-agent AI systems (MAASs). It
facilitates proactive identification and mitigation of AI-agent threats in
multi-agent applications.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [325] [Online Bayesian system identification in multivariate autoregressive models via message passing](https://arxiv.org/abs/2506.02710)
*T. N. Nisslbeck,Wouter M. Kouw*

Main category: eess.SP

TL;DR: 提出基于因子图消息传递的多变量自回归外生输入模型递归贝叶斯估计方法，可输出系数和噪声精度的完整后验分布，支持在线模型证据计算。


<details>
  <summary>Details</summary>
Motivation: 传统递归最小二乘法无法提供参数估计的完整概率分布，需要一种能量化估计不确定性的在线学习方法。

Method: 通过因子图消息传递实现递归贝叶斯估计，同时推断自回归系数和噪声精度的后验分布。

Result: 在合成自回归系统和双质量-弹簧-阻尼器系统上验证了收敛性，并展现出竞争优势。

Conclusion: 该方法能有效传播参数不确定性至预测结果，为动态系统提供概率化的在线学习框架。

Abstract: We propose a recursive Bayesian estimation procedure for multivariate
autoregressive models with exogenous inputs based on message passing in a
factor graph. Unlike recursive least-squares, our method produces full
posterior distributions for both the autoregressive coefficients and noise
precision. The uncertainties regarding these estimates propagate into the
uncertainties on predictions for future system outputs, and support online
model evidence calculations. We demonstrate convergence empirically on a
synthetic autoregressive system and competitive performance on a double
mass-spring-damper system.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [326] [SALF-MOS: Speaker Agnostic Latent Features Downsampled for MOS Prediction](https://arxiv.org/abs/2506.02082)
*Saurabh Agrawal,Raj Gohil,Gopal Kumar Agrawal,Vikram C M,Kushal Verma*

Main category: cs.SD

TL;DR: 提出了一种名为SALF-MOS的新型模型，用于预测语音合成的MOS分数，解决了传统客观指标不可靠和主观评估耗时的问题。


<details>
  <summary>Details</summary>
Motivation: 当前语音质量评估中，客观指标如PESQ、POLQA等无法可靠选择最佳模型，而主观评估如MOS虽可靠但耗时费力。因此，需要一种高效且可靠的自动评估方法。

Method: 开发了SALF-MOS模型，通过卷积序列堆叠提取音频样本的潜在特征，用于预测5分制MOS分数。模型具有小型化、端到端、高泛化性和可扩展性。

Result: 基于MSE、LCC、SRCC和KTAU等指标，SALF-MOS模型取得了最先进的性能。

Conclusion: SALF-MOS模型为语音合成质量评估提供了一种高效、可靠的自动化解决方案，显著减少了人工评估的需求。

Abstract: Speech quality assessment is a critical process in selecting text-to-speech
synthesis (TTS) or voice conversion models. Evaluation of voice synthesis can
be done using objective metrics or subjective metrics. Although there are many
objective metrics like the Perceptual Evaluation of Speech Quality (PESQ),
Perceptual Objective Listening Quality Assessment (POLQA) or Short-Time
Objective Intelligibility (STOI) but none of them is feasible in selecting the
best model. On the other hand subjective metric like Mean Opinion Score is
highly reliable but it requires a lot of manual efforts and are time-consuming.
To counter the issues in MOS Evaluation, we have developed a novel model,
Speaker Agnostic Latent Features (SALF)-Mean Opinion Score (MOS) which is a
small-sized, end-to-end, highly generalized and scalable model for predicting
MOS score on a scale of 5. We use the sequences of convolutions and stack them
to get the latent features of the audio samples to get the best
state-of-the-art results based on mean squared error (MSE), Linear Concordance
Correlation coefficient (LCC), Spearman Rank Correlation Coefficient (SRCC) and
Kendall Rank Correlation Coefficient (KTAU).

</details>


### [327] [LASPA: Language Agnostic Speaker Disentanglement with Prefix-Tuned Cross-Attention](https://arxiv.org/abs/2506.02083)
*Aditya Srinivas Menon,Raj Prakash Gohil,Kumud Tripathi,Pankaj Wasnik*

Main category: cs.SD

TL;DR: 提出一种通过前缀调优跨注意力机制的解耦学习策略，有效提升多语言场景下说话人识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 多语言环境下，说话人嵌入中语言信息的纠缠导致识别准确率下降，需解耦语言与说话人特征。

Method: 采用基于前缀调优跨注意力的联合学习策略，专门针对说话人切换语言的情况设计。

Result: 模型在单语/多语（含未见语言）场景均表现优异，多个数据集的等错误率显著降低。

Conclusion: 该方法成功分离说话人嵌入中的语言信息，显著提升了跨语言识别鲁棒性。

Abstract: Speaker recognition models face challenges in multi-lingual settings due to
the entanglement of linguistic information within speaker embeddings. The
overlap between vocal traits such as accent, vocal anatomy, and a language's
phonetic structure complicates separating linguistic and speaker information.
Disentangling these components can significantly improve speaker recognition
accuracy. To this end, we propose a novel disentanglement learning strategy
that integrates joint learning through prefix-tuned cross-attention. This
approach is particularly effective when speakers switch between languages.
Experimental results show the model generalizes across monolingual and
multi-lingual settings, including unseen languages. Notably, the proposed model
improves the equal error rate across multiple datasets, highlighting its
ability to separate language information from speaker embeddings and enhance
recognition in diverse linguistic conditions.

</details>


### [328] [Unveiling Audio Deepfake Origins: A Deep Metric learning And Conformer Network Approach With Ensemble Fusion](https://arxiv.org/abs/2506.02085)
*Ajinkya Kulkarni,Sandipana Dowerah,Tanel Alumae,Mathew Magimai. -Doss*

Main category: cs.SD

TL;DR: 该论文提出了一种结合深度度量多类N-pair损失、Real Emphasis和Fake Dispersion框架、Conformer分类网络以及集成分数-嵌入融合的新型音频源追踪系统，以提升音频深度伪造的源追踪能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的发展，音频深度伪造的真实性达到了前所未有的水平。当前研究主要集中在区分真实语音和伪造语音，但追踪伪造音频的源头同样重要。

Method: 论文提出了一种结合深度度量多类N-pair损失、Real Emphasis和Fake Dispersion框架、Conformer分类网络以及集成分数-嵌入融合的方法。N-pair损失增强了判别能力，Real Emphasis和Fake Dispersion通过区分真实和伪造语音模式提高了鲁棒性，Conformer网络捕获了音频信号的全局和局部依赖关系。

Result: 通过使用Frechet距离和标准指标评估，该方法在源追踪任务中表现优于基线系统。

Conclusion: 所提出的方法在域内和域外源追踪场景中实现了最佳平衡，显著提升了音频源追踪的性能。

Abstract: Audio deepfakes are acquiring an unprecedented level of realism with advanced
AI. While current research focuses on discerning real speech from spoofed
speech, tracing the source system is equally crucial. This work proposes a
novel audio source tracing system combining deep metric multi-class N-pair loss
with Real Emphasis and Fake Dispersion framework, a Conformer classification
network, and ensemble score-embedding fusion. The N-pair loss improves
discriminative ability, while Real Emphasis and Fake Dispersion enhance
robustness by focusing on differentiating real and fake speech patterns. The
Conformer network captures both global and local dependencies in the audio
signal, crucial for source tracing. The proposed ensemble score-embedding
fusion shows an optimal trade-off between in-domain and out-of-domain source
tracing scenarios. We evaluate our method using Frechet Distance and standard
metrics, demonstrating superior performance in source tracing over the baseline
system.

</details>


### [329] [Learning More with Less: Self-Supervised Approaches for Low-Resource Speech Emotion Recognition](https://arxiv.org/abs/2506.02059)
*Ziwei Gong,Pengyuan Shi,Kaan Donbekci,Lin Ai,Run Chen,David Sasu,Zehui Wu,Julia Hirschberg*

Main category: cs.SD

TL;DR: 该论文探索无监督学习（对比学习和BYOL）提升低资源语言的语音情感识别效果，在乌尔都语、德语和孟加拉语上取得显著F1分数提升，并分析了跨语言性能影响因素。


<details>
  <summary>Details</summary>
Motivation: 低资源语言（LRLs）因标注数据稀缺，语音情感识别（SER）面临挑战。研究旨在通过无监督学习提升其性能。

Method: 采用自监督学习方法，包括对比学习（CL）和Bootstrap Your Own Latent（BYOL），以增强跨语言泛化能力。

Result: 在乌尔都语、德语和孟加拉语上，F1分数分别提升10.6%、15.2%和13.9%，验证了方法的有效性。

Conclusion: 研究为低资源语言开发更具包容性、可解释性和鲁棒性的情感识别系统奠定了基础，同时揭示了相关挑战。

Abstract: Speech Emotion Recognition (SER) has seen significant progress with deep
learning, yet remains challenging for Low-Resource Languages (LRLs) due to the
scarcity of annotated data. In this work, we explore unsupervised learning to
improve SER in low-resource settings. Specifically, we investigate contrastive
learning (CL) and Bootstrap Your Own Latent (BYOL) as self-supervised
approaches to enhance cross-lingual generalization. Our methods achieve notable
F1 score improvements of 10.6% in Urdu, 15.2% in German, and 13.9% in Bangla,
demonstrating their effectiveness in LRLs. Additionally, we analyze model
behavior to provide insights on key factors influencing performance across
languages, and also highlighting challenges in low-resource SER. This work
provides a foundation for developing more inclusive, explainable, and robust
emotion recognition systems for underrepresented languages.

</details>


### [330] [Enhancing Speech Emotion Recognition with Graph-Based Multimodal Fusion and Prosodic Features for the Speech Emotion Recognition in Naturalistic Conditions Challenge at Interspeech 2025](https://arxiv.org/abs/2506.02088)
*Alef Iury Siqueira Ferreira,Lucas Rafael Gris,Alexandre Ferro Filho,Lucas Ólives,Daniel Ribeiro,Luiz Fernando,Fernanda Lustosa,Rodrigo Tanaka,Frederico Santos de Oliveira,Arlindo Galvão Filho*

Main category: cs.SD

TL;DR: 该论文提出了一种结合音频模型和文本特征的鲁棒系统，用于自然语音中的情感识别，并在挑战赛中取得了不错的成绩。


<details>
  <summary>Details</summary>
Motivation: 自然语音中的情感表达微妙且不可预测，训练情感识别模型具有挑战性。

Method: 结合先进的音频模型和通过韵律及频谱线索增强的文本特征，研究了基频量化和预训练音频标记模型的效果，并采用集成模型提高鲁棒性。

Result: 在官方测试集上，系统取得了39.79%的宏F1分数（验证集为42.20%），图注意力网络的融合技术被证实有效。

Conclusion: 该方法展现了在自然语音情感识别中的潜力，融合技术进一步提升了性能。

Abstract: Training SER models in natural, spontaneous speech is especially challenging
due to the subtle expression of emotions and the unpredictable nature of
real-world audio. In this paper, we present a robust system for the INTERSPEECH
2025 Speech Emotion Recognition in Naturalistic Conditions Challenge, focusing
on categorical emotion recognition. Our method combines state-of-the-art audio
models with text features enriched by prosodic and spectral cues. In
particular, we investigate the effectiveness of Fundamental Frequency (F0)
quantization and the use of a pretrained audio tagging model. We also employ an
ensemble model to improve robustness. On the official test set, our system
achieved a Macro F1-score of 39.79% (42.20% on validation). Our results
underscore the potential of these methods, and analysis of fusion techniques
confirmed the effectiveness of Graph Attention Networks. Our source code is
publicly available.

</details>


### [331] [Cocktail-Party Audio-Visual Speech Recognition](https://arxiv.org/abs/2506.02178)
*Thai-Binh Nguyen,Ngoc-Quan Pham,Alexander Waibel*

Main category: cs.SD

TL;DR: 该论文提出了一种新型音频-视觉鸡尾酒会数据集，显著提升了AVSR系统在嘈杂环境中的性能，词错误率降低67%。


<details>
  <summary>Details</summary>
Motivation: 当前AVSR模型主要针对理想场景优化，忽视了现实环境中包含说话和沉默面部片段的复杂性，导致在嘈杂环境下性能不足。

Method: 引入包含说话和沉默面部片段的新型音频-视觉鸡尾酒会数据集，并构建了一个1526小时的AVSR数据集。

Result: 在极端噪音环境下，词错误率从119%降至39.2%，相对现有技术降低了67%，且无需依赖显式分割线索。

Conclusion: 该研究通过新数据集显著提升了AVSR系统在真实嘈杂环境中的性能，填补了现有技术的不足。

Abstract: Audio-Visual Speech Recognition (AVSR) offers a robust solution for speech
recognition in challenging environments, such as cocktail-party scenarios,
where relying solely on audio proves insufficient. However, current AVSR models
are often optimized for idealized scenarios with consistently active speakers,
overlooking the complexities of real-world settings that include both speaking
and silent facial segments. This study addresses this gap by introducing a
novel audio-visual cocktail-party dataset designed to benchmark current AVSR
systems and highlight the limitations of prior approaches in realistic noisy
conditions. Additionally, we contribute a 1526-hour AVSR dataset comprising
both talking-face and silent-face segments, enabling significant performance
gains in cocktail-party environments. Our approach reduces WER by 67% relative
to the state-of-the-art, reducing WER from 119% to 39.2% in extreme noise,
without relying on explicit segmentation cues.

</details>


### [332] [Synthetic Speech Source Tracing using Metric Learning](https://arxiv.org/abs/2506.02590)
*Dimitrios Koutsianos,Stavros Zacharopoulos,Yannis Panagakis,Themos Stafylakis*

Main category: cs.SD

TL;DR: 该论文通过说话人识别技术，提出两种方法（分类和度量学习）追踪合成语音的生成系统，并在MLAADv5基准测试中验证ResNet与自监督学习的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注伪造检测，而合成语音的源头追踪缺乏稳健解决方案，本文旨在填补这一空白。

Method: 采用分类和度量学习两种方法，基于ResNet和自监督学习（SSL）架构，在MLAADv5数据集上进行测试。

Result: ResNet在度量学习方法中表现优异，性能媲美甚至超越基于SSL的系统，但SSL表征需进一步优化。

Conclusion: 研究将说话人识别方法应用于音频取证，为对抗合成媒体篡改提供了新方向，同时证实ResNet在源头追踪中的潜力。

Abstract: This paper addresses source tracing in synthetic speech-identifying
generative systems behind manipulated audio via speaker recognition-inspired
pipelines. While prior work focuses on spoofing detection, source tracing lacks
robust solutions. We evaluate two approaches: classification-based and
metric-learning. We tested our methods on the MLAADv5 benchmark using ResNet
and self-supervised learning (SSL) backbones. The results show that ResNet
achieves competitive performance with the metric learning approach, matching
and even exceeding SSL-based systems. Our work demonstrates ResNet's viability
for source tracing while underscoring the need to optimize SSL representations
for this task. Our work bridges speaker recognition methodologies with audio
forensic challenges, offering new directions for combating synthetic media
manipulation.

</details>


### [333] [Comparison of spectrogram scaling in multi-label Music Genre Recognition](https://arxiv.org/abs/2506.02091)
*Bartosz Karpiński,Cyryl Leszczyński*

Main category: cs.SD

TL;DR: 随着数字音频工作站的普及，音乐数量激增且流派界限模糊，本文比较了多种预处理方法和模型训练方法，使用了一个包含18000多个条目的自定义数据集进行实验。


<details>
  <summary>Details</summary>
Motivation: 数字音频工作站的易用性提高导致音乐数量激增，同时音乐流派之间的界限变得模糊，不同流派在单曲中的组合也多种多样。这种复杂性促使研究者探索更有效的预处理和模型训练方法。

Method: 本文描述了多种预处理方法和模型训练方法，并进行了比较。使用了一个自定义的手动标记数据集，包含超过18000个条目来进行实验。

Result: 实验结果表明，不同的预处理方法和模型训练方法在处理复杂音乐流派组合时表现各异，具体结果未在摘要中详细说明。

Conclusion: 本文通过比较多种方法和使用大规模数据集，为处理现代音乐专辑的复杂性提供了有价值的见解和方法。

Abstract: As the accessibility and ease-of-use of digital audio workstations increases,
so does the quantity of music available to the average listener; additionally,
differences between genres are not always well defined and can be abstract,
with widely varying combinations of genres across individual records. In this
article, multiple preprocessing methods and approaches to model training are
described and compared, accounting for the eclectic nature of today's albums. A
custom, manually labeled dataset of more than 18000 entries has been used to
perform the experiments.

</details>


### [334] [Speaker Diarization with Overlapping Community Detection Using Graph Attention Networks and Label Propagation Algorithm](https://arxiv.org/abs/2506.02610)
*Zhaoyang Li,Jie Wang,XiaoXiao Li,Wangjie Li,Longjie Luo,Lin Li,Qingyang Hong*

Main category: cs.SD

TL;DR: 提出OCDGALP方法，结合图注意力网络和标签传播算法，显著降低说话人日志错误率。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法在处理说话人嵌入复杂分布和重叠语音段时存在局限。

Method: 使用图注意力网络优化嵌入和节点连接，结合标签传播算法进行多社区标签分配。

Result: 在DIHARD-III数据集上，无/有oracle VAD时DER分别降至15.94%和11.07%。

Conclusion: OCDGALP有效提升说话人日志性能，达到当前最优水平。

Abstract: In speaker diarization, traditional clustering-based methods remain widely
used in real-world applications. However, these methods struggle with the
complex distribution of speaker embeddings and overlapping speech segments. To
address these limitations, we propose an Overlapping Community Detection method
based on Graph Attention networks and the Label Propagation Algorithm
(OCDGALP). The proposed framework comprises two key components: (1) a graph
attention network that refines speaker embeddings and node connections by
aggregating information from neighboring nodes, and (2) a label propagation
algorithm that assigns multiple community labels to each node, enabling
simultaneous clustering and overlapping community detection. Experimental
results show that the proposed method significantly reduces the Diarization
Error Rate (DER), achieving a state-of-the-art 15.94% DER on the DIHARD-III
dataset without oracle Voice Activity Detection (VAD), and an impressive 11.07%
with oracle VAD.

</details>


### [335] [DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization](https://arxiv.org/abs/2506.02858)
*Geonyoung Lee,Geonhee Han,Paul Hongsuck Seo*

Main category: cs.SD

TL;DR: 该论文提出了一种无需训练的零样本语言查询音频源分离方法，利用预训练扩散模型的生成先验实现开放词汇的音频分离。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖任务特定训练，论文探索预训练扩散模型是否无需额外训练即可实现音频分离，以扩展其应用范围。

Method: 提出Diffusion-Guided Mask Optimization (DGMO)，一种测试时优化框架，通过优化频谱图掩码实现精确的音频分离。

Result: 该方法在零样本设置下实现了与任务特定监督方法竞争的性能，成功将扩散模型应用于音频分离任务。

Conclusion: 该工作扩展了扩散模型在生成任务以外的应用，为零样本音频分离建立了新范式。

Abstract: Language-queried Audio Source Separation (LASS) enables open-vocabulary sound
separation via natural language queries. While existing methods rely on
task-specific training, we explore whether pretrained diffusion models,
originally designed for audio generation, can inherently perform separation
without further training. In this study, we introduce a training-free framework
leveraging generative priors for zero-shot LASS. Analyzing na\"ive adaptations,
we identify key limitations arising from modality-specific challenges.To
address these issues, we propose Diffusion-Guided Mask Optimization (DGMO), a
test-time optimization framework that refines spectrogram masks for precise,
input-aligned separation. Our approach effectively repurposes pretrained
diffusion models for source separation, achieving competitive performance
without task-specific supervision. This work expands the application of
diffusion models beyond generation, establishing a new paradigm for zero-shot
audio separation. The code is available at: https://wltschmrz.github.io/DGMO/

</details>


### [336] [TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models](https://arxiv.org/abs/2506.03099)
*Chetwin Low,Weimin Wang*

Main category: cs.SD

TL;DR: TalkingMachines是一个高效框架，将预训练视频生成模型转化为实时音频驱动的人物动画生成器，结合音频大语言模型实现自然对话体验。


<details>
  <summary>Details</summary>
Motivation: 旨在通过整合音频大语言模型与视频生成基础模型，实现实时、自然的音频驱动人物动画生成，提升交互体验。

Method: 1. 将预训练的SOTA图像到视频DiT模型适配为180亿参数的音频驱动头像生成模型；2. 通过非对称知识蒸馏实现无限视频流；3. 设计高吞吐、低延迟的推理流水线。

Result: 实现了高效、实时的音频驱动人物动画生成，支持无限视频流，并通过工程优化提升了性能。

Conclusion: TalkingMachines框架成功将预训练模型转化为实时音频驱动动画生成器，为自然对话体验提供了高效解决方案。

Abstract: In this paper, we present TalkingMachines -- an efficient framework that
transforms pretrained video generation models into real-time, audio-driven
character animators. TalkingMachines enables natural conversational experiences
by integrating an audio large language model (LLM) with our video generation
foundation model. Our primary contributions include: (1) We adapt a pretrained
SOTA image-to-video DiT into an audio-driven avatar generation model of 18
billion parameters; (2) We enable infinite video streaming without error
accumulation through asymmetric knowledge distillation from a bidirectional
teacher model into a sparse causal, autoregressive student model; (3) We design
a high-throughput, low-latency inference pipeline incorporating several key
engineering optimizations such as: (a) disaggregation of the DiT and VAE
decoder across separate devices, (b) efficient overlap of inter-device
communication and computation using CUDA streams, (c) elimination of redundant
recomputations to maximize frame-generation throughput. Please see demo videos
here - https://aaxwaz.github.io/TalkingMachines/

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [337] [Validating remotely sensed biomass estimates with forest inventory data in the western US](https://arxiv.org/abs/2506.03120)
*Xiuyu Cao,Joseph O. Sexton,Panshi Wang,Dimitrios Gounaridis,Neil H. Carter,Kai Zhu*

Main category: stat.AP

TL;DR: 该研究对terraPulse公司提供的地上生物量密度(AGBD)数据集进行了独立区域验证，结果显示与USFS FIA数据在犹他州、内华达州和华盛顿州具有高度一致性。


<details>
  <summary>Details</summary>
Motivation: 高分辨率监测地上生物量(AGB)及其密度(AGBD)对碳核算和生态系统管理至关重要。目前大多数基于NASA GEDI任务的商业遥感产品缺乏严格独立验证。

Method: 使用美国林务局FIA项目的独立参考数据，在64,000公顷六边形网格和县级尺度上验证terraPulse的AGBD数据集。

Result: 在六边形尺度上R2=0.88，RMSE=26.68 Mg/ha；县级尺度表现更好(R2=0.90)。非森林区域terraPulse估值偏高，高生物量森林区域估值偏低。

Conclusion: 研究提出了使用FIA数据进行AGBD验证的可扩展框架，为全球生物量监测的商业数据集提供了基准验证。

Abstract: Monitoring aboveground biomass (AGB) and its density (AGBD) at high
resolution is essential for carbon accounting and ecosystem management. While
NASA's spaceborne Global Ecosystem Dynamics Investigation (GEDI) LiDAR mission
provides globally distributed reference measurements for AGBD estimation, the
majority of commercial remote sensing products based on GEDI remain without
rigorous or independent validation. Here, we present an independent regional
validation of an AGBD dataset offered by terraPulse, Inc., based on independent
reference data from the US Forest Service Forest Inventory and Analysis (FIA)
program. Aggregated to 64,000-hectare hexagons and US counties across the US
states of Utah, Nevada, and Washington, we found very strong agreement between
terraPulse and FIA estimates. At the hexagon scale, we report R2 = 0.88, RMSE =
26.68 Mg/ha, and a correlation coefficient (r) of 0.94. At the county scale,
agreement improves to R2 = 0.90, RMSE =32.62 Mg/ha, slope = 1.07, and r = 0.95.
Spatial and statistical analyses indicated that terraPulse AGBD values tended
to exceed FIA estimates in non-forest areas, likely due to FIA's limited
sampling of non-forest vegetation. The terraPulse AGBD estimates also exhibited
lower values in high-biomass forests, likely due to saturation effects in its
optical remote-sensing covariates. This study advances operational carbon
monitoring by delivering a scalable framework for comprehensive AGBD validation
using independent FIA data, as well as a benchmark validation of a new
commercial dataset for global biomass monitoring.

</details>


<div id='math.AT'></div>

# math.AT [[Back]](#toc)

### [338] [Torsion in Persistent Homology and Neural Networks](https://arxiv.org/abs/2506.03049)
*Maria Walch*

Main category: math.AT

TL;DR: 本文探讨了扭量在结合拓扑数据分析的混合深度学习模型中的作用，指出基于场系数的方法可能丢失扭量信息，并评估了不同自编码器架构对扭量信息的恢复能力。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示传统基于场系数的拓扑数据分析工具可能掩盖整数同调中的扭量特征，导致信息丢失，从而需要探索能保留扭量信息的模型架构或损失项。

Method: 方法包括使用合成数据和高维数据，评估扭量对扰动的敏感性，并测试多种自编码器架构对扭量信息的恢复能力。

Result: 研究结果表明，基于场系数的方法在编码过程中可能丢失扭量信息，且在潜在空间中扭量特征可能被改变，标准解码器通常无法重建这些信息。

Conclusion: 结论强调了需要开发能保留扭量信息的架构或损失项，以实现更鲁棒的数据表示。

Abstract: We explore the role of torsion in hybrid deep learning models that
incorporate topological data analysis, focusing on autoencoders. While most TDA
tools use field coefficients, this conceals torsional features present in
integer homology. We show that torsion can be lost during encoding, altered in
the latent space, and in many cases, not reconstructed by standard decoders.
Using both synthetic and high-dimensional data, we evaluate torsion sensitivity
to perturbations and assess its recoverability across several autoencoder
architectures. Our findings reveal key limitations of field-based approaches
and underline the need for architectures or loss terms that preserve torsional
information for robust data representation.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [339] [Deep Learning Enhanced Multivariate GARCH](https://arxiv.org/abs/2506.02796)
*Haoyuan Wang,Chen Liu,Minh-Ngoc Tran,Chao Wang*

Main category: q-fin.CP

TL;DR: 该论文提出了一种结合LSTM和BEKK模型的新型多元波动率建模框架LSTM-BEKK，旨在改进传统多元GARCH方法在捕捉非线性、动态和高维依赖结构方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统多元GARCH方法在捕捉持续波动聚类和资产间不对称联动方面存在局限，需要更灵活的方法来适应时变市场条件。

Method: 通过将LSTM的灵活性与BEKK模型的计量经济结构相结合，提出LSTM-BEKK框架，以更好地捕捉金融回报数据中的复杂依赖结构。

Result: 实证结果表明，LSTM-BEKK模型在样本外投资组合风险预测方面表现优异，同时保持了BEKK模型的可解释性。

Conclusion: 混合计量经济-深度学习模型在提升金融风险管理和多元波动率预测方面具有潜力。

Abstract: This paper introduces a novel multivariate volatility modeling framework,
named Long Short-Term Memory enhanced BEKK (LSTM-BEKK), that integrates deep
learning into multivariate GARCH processes. By combining the flexibility of
recurrent neural networks with the econometric structure of BEKK models, our
approach is designed to better capture nonlinear, dynamic, and high-dimensional
dependence structures in financial return data. The proposed model addresses
key limitations of traditional multivariate GARCH-based methods, particularly
in capturing persistent volatility clustering and asymmetric co-movement across
assets. Leveraging the data-driven nature of LSTMs, the framework adapts
effectively to time-varying market conditions, offering improved robustness and
forecasting performance. Empirical results across multiple equity markets
confirm that the LSTM-BEKK model achieves superior performance in terms of
out-of-sample portfolio risk forecast, while maintaining the interpretability
from the BEKK models. These findings highlight the potential of hybrid
econometric-deep learning models in advancing financial risk management and
multivariate volatility forecasting.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [340] [Flow2Code: Evaluating Large Language Models for Flowchart-based Code Generation Capability](https://arxiv.org/abs/2506.02073)
*Mengliang He,Jiayi Zeng,Yankai Jiang,Wei Zhang,Zeming Liu,Xiaoming Shi,Aimin Zhou*

Main category: cs.SE

TL;DR: Flow2Code是一个新的流程图代码生成基准测试，评估了13种多模态大语言模型在15种编程语言上的表现，发现当前模型无法完美生成代码，但有监督微调显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试忽视了基于流程图的代码生成，阻碍了相关研究的发展。

Method: 构建Flow2Code基准测试，包含15种编程语言的5,622个代码段和16,866个流程图，评估13种多模态大语言模型。

Result: 当前大语言模型无法完美生成基于流程图的代码，但有监督微调技术显著提升模型性能。

Conclusion: Flow2Code填补了流程图代码生成基准测试的空白，为未来研究提供了数据和工具支持。

Abstract: While large language models (LLMs) show promise in code generation, existing
benchmarks neglect the flowchart-based code generation. To promote further
research on flowchart-based code generation, this work presents Flow2Code, a
novel benchmark for flowchart-based code generation evaluation. The evaluation
dataset spans 15 programming languages and includes 5,622 code segments paired
with 16,866 flowcharts of three types: code, UML, and pseudocode. Extensive
experiments with 13 multimodal LLMs reveal that current LLMs can not generate
code based on flowcharts perfectly. Besides, experiment results show that the
supervised fine-tuning technique contributes greatly to the models'
performance. We publicly release our code and datasets at
https://github.com/hml-github/Flow2Code.

</details>


### [341] [The Impact of Software Testing with Quantum Optimization Meets Machine Learning](https://arxiv.org/abs/2506.02090)
*Gopichand Bandarupalli*

Main category: cs.SE

TL;DR: 该研究提出了一种结合量子退火和机器学习的混合框架，用于优化CI/CD管道中的测试用例优先级排序，显著提高了缺陷检测效率和减少了测试执行时间。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统的复杂性使得传统机器学习在处理大规模测试套件时效率低下，亟需一种更高效的测试优化方法。

Method: 通过整合量子退火和机器学习，开发了一种混合框架，并在Defects4J数据集上进行了验证。

Result: 相比传统机器学习方法，该框架实现了缺陷检测效率提升25%，测试执行时间减少30%。

Conclusion: 该框架为2025年混合量子-经典生态系统中的软件质量保障提供了变革性解决方案。

Abstract: Modern software systems complexity challenges efficient testing, as
traditional machine learning (ML) struggles with large test suites. This
research presents a hybrid framework integrating Quantum Annealing with ML to
optimize test case prioritization in CI/CD pipelines. Leveraging quantum
optimization, it achieves a 25 percent increase in defect detection efficiency
and a 30 percent reduction in test execution time versus classical ML,
validated on the Defects4J dataset. A simulated CI/CD environment demonstrates
robustness across evolving codebases. Visualizations, including defect heatmaps
and performance graphs, enhance interpretability. The framework addresses
quantum hardware limits, CI/CD integration, and scalability for 2025s hybrid
quantum-classical ecosystems, offering a transformative approach to software
quality assurance.

</details>


### [342] [Automated Web Application Testing: End-to-End Test Case Generation with Large Language Models and Screen Transition Graphs](https://arxiv.org/abs/2506.02529)
*Nguyen-Khang Le,Quan Minh Bui,Minh Ngoc Nguyen,Hiep Nguyen,Trung Vo,Son T. Luu,Shoshin Nomura,Minh Le Nguyen*

Main category: cs.SE

TL;DR: 本文提出了一种结合图结构和大型语言模型（LLM）的自动化系统，用于生成Web应用程序的测试用例，重点解决站点导航和表单填写两个关键问题。


<details>
  <summary>Details</summary>
Motivation: Web应用程序在现代软件生态系统中至关重要，但由于其复杂性和动态性，确保其可靠性仍然具有挑战性。现有的自动化测试方法在处理动态导航流和复杂表单交互时存在局限性。

Method: 系统采用两种主要方法：1) 对于站点导航，使用屏幕转换图和LLM建模导航流并生成测试场景；2) 对于表单填写，利用状态图处理条件表单并自动化生成Selenium脚本。

Result: 实验结果表明，该系统在提高测试覆盖率和鲁棒性方面表现优异，推动了Web应用程序测试的发展。

Conclusion: 通过结合图结构和LLM，该系统有效解决了Web应用程序测试中的关键问题，为自动化测试提供了新的解决方案。

Abstract: Web applications are critical to modern software ecosystems, yet ensuring
their reliability remains challenging due to the complexity and dynamic nature
of web interfaces. Recent advances in large language models (LLMs) have shown
promise in automating complex tasks, but limitations persist in handling
dynamic navigation flows and complex form interactions. This paper presents an
automated system for generating test cases for two key aspects of web
application testing: site navigation and form filling. For site navigation, the
system employs screen transition graphs and LLMs to model navigation flows and
generate test scenarios. For form filling, it uses state graphs to handle
conditional forms and automates Selenium script generation. Key contributions
include: (1) a novel integration of graph structures and LLMs for site
navigation testing, (2) a state graph-based approach for automating
form-filling test cases, and (3) a comprehensive dataset for evaluating
form-interaction testing. Experimental results demonstrate the system's
effectiveness in improving test coverage and robustness, advancing the state of
web application testing.

</details>


### [343] [Rethinking the effects of data contamination in Code Intelligence](https://arxiv.org/abs/2506.02791)
*Zhen Yang,Hongyi Lin,Yifan He,Jie Xu,Zeyu Sun,Shuo Liu,Pengpeng Wang,Zhongxing Yu,Qingyuan Liang*

Main category: cs.SE

TL;DR: 该论文通过系统实证研究探讨了代码智能任务中的细粒度数据污染问题，挑战了污染必然导致性能高估的传统观点。


<details>
  <summary>Details</summary>
Motivation: 随着预训练语言模型(PLMs)和大语言模型(LLMs)的广泛应用，数据污染及其对模型性能评估的影响引发了广泛关注。论文旨在探究不同污染场景对代码智能任务的具体影响。

Method: 研究选取RoBERTa、GPT-2等PLMs和LLaMA、StarCoder等LLMs，在代码翻译、生成和摘要三大任务上，构建了输入污染、输出污染、非配对污染和配对污染四类实验场景。

Result: 实验发现：PLMs在预训练-微调-推理范式下，即使故意注入配对污染也不会显著高估性能；而LLMs在配对污染下表现显著受影响。其他污染场景对两类模型均无影响。

Conclusion: 研究结果表明数据污染的影响具有场景特异性，为代码智能模型的评估和部署提供了新视角，打破了'污染必然导致性能高估'的固有认知。

Abstract: In recent years, code intelligence has gained increasing importance in the
field of automated software engineering. Meanwhile, the widespread adoption of
Pretrained Language Models (PLMs) and Large Language Models (LLMs) has raised
concerns regarding data contamination and its potential impact on model
performance evaluation. This paper presents a systematic empirical study to
investigate the fine-grained data contamination on code intelligence tasks. Our
study involves diverse representative PLMs, namely RoBERTa and GPT-2, and LLMs,
namely LLaMA and StarCoder, covering three major tasks: code translation, code
generation, and code summarization. We categorize contamination scenarios into
four types according to the code intelligence practice, namely input-only,
output-only, unpaired, and paired contamination settings, and construct
corresponding experimental and control groups for exploration.
  Experimental results show that, under the pre-training, fine-tuning, and
inference paradigm adopted by PLMs, even deliberately injecting paired
contamination does not lead to significant performance overestimation. But
direct inference or small-scale fine-tuning uncovers the contamination effects.
In contrast, LLMs with pre-training and inference paradigm are significantly
affected by the paired contamination. Apart from the above, other contamination
scenarios have no impact on both PLMs and LLMs. Our findings challenge the
conventional belief that contamination inevitably leads to performance
overestimation, providing new insights into the evaluation and deployment of
code intelligence models.

</details>


### [344] [How do Pre-Trained Models Support Software Engineering? An Empirical Study in Hugging Face](https://arxiv.org/abs/2506.03013)
*Alexandra González,Xavier Franch,David Lo,Silverio Martínez-Fernández*

Main category: cs.SE

TL;DR: 该论文提出了一个针对软件工程(SE)任务的预训练模型(PTM)分类法，并在Hugging Face平台上识别了2205个相关PTM，发现代码生成是最常见的SE任务。


<details>
  <summary>Details</summary>
Motivation: 现有的开源预训练模型(PTM)资源缺乏针对软件工程(SE)任务的分类，无法满足SE领域的需求。

Method: 通过系统收集Hugging Face平台的PTM数据，结合模型描述、元数据和相关论文摘要，采用多步过滤（包括异常检测、去重和Gemini 2.0 Flash验证）确认SE相关性。

Result: 共识别出2205个SE相关PTM，其中代码生成是最主要的SE任务，而需求工程和软件设计任务关注较少；文本生成是SE PTM中最常见的ML任务。

Conclusion: 该分类法为未来自动化SE场景（如PTM采样与选择）提供了基础框架，并观察到2023年Q2以来SE PTM数量显著增长。

Abstract: Open-Source Pre-Trained Models (PTMs) provide extensive resources for various
Machine Learning (ML) tasks, yet these resources lack a classification tailored
to Software Engineering (SE) needs. To address this gap, we derive a taxonomy
encompassing 147 SE tasks and apply an SE-oriented classification to PTMs in a
popular open-source ML repository, Hugging Face (HF). Our repository mining
study began with a systematically gathered database of PTMs from the HF API,
considering their model card descriptions and metadata, and the abstract of the
associated arXiv papers. We confirmed SE relevance through multiple filtering
steps: detecting outliers, identifying near-identical PTMs, and the use of
Gemini 2.0 Flash, which was validated with five pilot studies involving three
human annotators. This approach uncovered 2,205 SE PTMs. We find that code
generation is the most common SE task among PTMs, primarily focusing on
software implementation, while requirements engineering and software design
activities receive limited attention. In terms of ML tasks, text generation
dominates within SE PTMs. Notably, the number of SE PTMs has increased markedly
since 2023 Q2. Our classification provides a solid foundation for future
automated SE scenarios, such as the sampling and selection of suitable PTMs.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [345] [Research on Driving Scenario Technology Based on Multimodal Large Lauguage Model Optimization](https://arxiv.org/abs/2506.02014)
*Wang Mengjie,Zhu Huiping,Li Jian,Shi Wenxiu,Zhang Song*

Main category: cs.CV

TL;DR: 本文提出了一种针对自动驾驶场景的多模态模型优化方法，涵盖动态提示优化、数据集构建、模型训练与部署，显著提升了模型在关键任务中的准确性和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶技术的发展，理解复杂驾驶场景的需求日益增长，但多模态大模型在垂直领域的应用面临数据收集、模型训练和部署优化等挑战。

Method: 方法包括动态提示优化（根据输入图像调整提示以聚焦关键对象）、结合真实与合成数据构建高质量数据集，以及集成知识蒸馏、动态微调和量化等先进训练技术。

Result: 实验结果表明，该方法显著提升了模型在锥桶检测、交通灯识别等关键任务中的准确性，同时实现了高效的资源利用。

Conclusion: 该系统性优化方法为驾驶场景感知技术的实际应用提供了有力支持。

Abstract: With the advancement of autonomous and assisted driving technologies, higher
demands are placed on the ability to understand complex driving scenarios.
Multimodal general large models have emerged as a solution for this challenge.
However, applying these models in vertical domains involves difficulties such
as data collection, model training, and deployment optimization. This paper
proposes a comprehensive method for optimizing multimodal models in driving
scenarios, including cone detection, traffic light recognition, speed limit
recommendation, and intersection alerts. The method covers key aspects such as
dynamic prompt optimization, dataset construction, model training, and
deployment. Specifically, the dynamic prompt optimization adjusts the prompts
based on the input image content to focus on objects affecting the ego vehicle,
enhancing the model's task-specific focus and judgment capabilities. The
dataset is constructed by combining real and synthetic data to create a
high-quality and diverse multimodal training dataset, improving the model's
generalization in complex driving environments. In model training, advanced
techniques like knowledge distillation, dynamic fine-tuning, and quantization
are integrated to reduce storage and computational costs while boosting
performance. Experimental results show that this systematic optimization method
not only significantly improves the model's accuracy in key tasks but also
achieves efficient resource utilization, providing strong support for the
practical application of driving scenario perception technologies.

</details>


### [346] [Dynamic-Aware Video Distillation: Optimizing Temporal Resolution Based on Video Semantics](https://arxiv.org/abs/2506.02021)
*Yinjie Zhao,Heng Zhao,Bihan Wen,Yew-Soon Ong,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种名为DAViD的动态感知视频蒸馏方法，通过强化学习预测合成视频的最佳时间分辨率，显著提升了视频数据集蒸馏的效果。


<details>
  <summary>Details</summary>
Motivation: 随着视觉任务的快速发展和数据集的规模扩大，减少视觉数据集中的冗余成为研究重点。尽管数据集蒸馏（DD）在图像数据上已有广泛研究，但视频数据由于其时间信息和不同类别间的冗余差异，相关研究仍不足。现有方法假设所有视频语义的时间冗余水平一致，限制了其在视频数据集上的效果。

Method: 本文提出Dynamic-Aware Video Distillation (DAViD)，一种基于强化学习（RL）的方法，用于预测合成视频的最佳时间分辨率。通过设计一个教师循环奖励函数来更新RL代理的策略。

Result: DAViD方法在视频数据集蒸馏上显著优于现有方法，性能大幅提升。这是首次在视频数据集蒸馏中引入基于视频语义的自适应时间分辨率研究。

Conclusion: 本研究为未来更高效且语义自适应的视频数据集蒸馏研究铺平了道路。

Abstract: With the rapid development of vision tasks and the scaling on datasets and
models, redundancy reduction in vision datasets has become a key area of
research. To address this issue, dataset distillation (DD) has emerged as a
promising approach to generating highly compact synthetic datasets with
significantly less redundancy while preserving essential information. However,
while DD has been extensively studied for image datasets, DD on video datasets
remains underexplored. Video datasets present unique challenges due to the
presence of temporal information and varying levels of redundancy across
different classes. Existing DD approaches assume a uniform level of temporal
redundancy across all different video semantics, which limits their
effectiveness on video datasets. In this work, we propose Dynamic-Aware Video
Distillation (DAViD), a Reinforcement Learning (RL) approach to predict the
optimal Temporal Resolution of the synthetic videos. A teacher-in-the-loop
reward function is proposed to update the RL agent policy. To the best of our
knowledge, this is the first study to introduce adaptive temporal resolution
based on video semantics in video dataset distillation. Our approach
significantly outperforms existing DD methods, demonstrating substantial
improvements in performance. This work paves the way for future research on
more efficient and semantic-adaptive video dataset distillation research.

</details>


### [347] [Implicit Deformable Medical Image Registration with Learnable Kernels](https://arxiv.org/abs/2506.02150)
*Stefano Fogarollo,Gregor Laimer,Reto Bale,Matthias Harders*

Main category: cs.CV

TL;DR: 提出了一种新型隐式医学图像配准框架，通过稀疏关键点对应关系重建密集位移场，在精度和可靠性上优于现有方法，具有临床潜力。


<details>
  <summary>Details</summary>
Motivation: 现有AI医学图像配准方法虽在速度和精度上有优势，但形变结果不可靠，限制了临床使用。本文旨在解决这一问题。

Method: 将配准问题重构为信号重建任务：学习从稀疏关键点恢复密集位移场的核函数，采用分层架构进行由粗到细的位移场估计，并支持测试时快速调整。

Result: 在胸腹部零样本配准任务中，方法不仅达到SOTA精度，且保持了更好的解剖结构关系，性能媲美专业商业系统。

Conclusion: 该框架弥合了隐式与显式配准技术的泛化差距，生成的可靠形变结果显示出临床应用的潜力。

Abstract: Deformable medical image registration is an essential task in
computer-assisted interventions. This problem is particularly relevant to
oncological treatments, where precise image alignment is necessary for tracking
tumor growth, assessing treatment response, and ensuring accurate delivery of
therapies. Recent AI methods can outperform traditional techniques in accuracy
and speed, yet they often produce unreliable deformations that limit their
clinical adoption. In this work, we address this challenge and introduce a
novel implicit registration framework that can predict accurate and reliable
deformations. Our insight is to reformulate image registration as a signal
reconstruction problem: we learn a kernel function that can recover the dense
displacement field from sparse keypoint correspondences. We integrate our
method in a novel hierarchical architecture, and estimate the displacement
field in a coarse-to-fine manner. Our formulation also allows for efficient
refinement at test time, permitting clinicians to easily adjust registrations
when needed. We validate our method on challenging intra-patient thoracic and
abdominal zero-shot registration tasks, using public and internal datasets from
the local University Hospital. Our method not only shows competitive accuracy
to state-of-the-art approaches, but also bridges the generalization gap between
implicit and explicit registration techniques. In particular, our method
generates deformations that better preserve anatomical relationships and
matches the performance of specialized commercial systems, underscoring its
potential for clinical adoption.

</details>


### [348] [Fire360: A Benchmark for Robust Perception and Episodic Memory in Degraded 360-Degree Firefighting Videos](https://arxiv.org/abs/2506.02167)
*Aditi Tiwari,Farzaneh Masoud,Dac Trong Nguyen,Jill Kraft,Heng Ji,Klara Nahrstedt*

Main category: cs.CV

TL;DR: Fire360是一个用于评估消防场景下感知与推理能力的基准数据集，包含228个360度视频，支持五项任务，旨在提升AI在恶劣环境下的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统在可靠性要求高的环境中表现不佳，如烟雾、低能见度和结构变形场景。每年有大量消防员因情境感知失误受伤，因此需要开发能在恶劣条件下工作的AI模型。

Method: 引入Fire360数据集，包含228个专业消防训练场景的360度视频，标注了动作片段、物体位置和退化元数据，支持五项任务评估模型能力。

Result: 人类专家在TOR任务中达到83.5%准确率，而GPT-4o等模型表现显著落后，暴露了在退化条件下推理的不足。

Conclusion: 通过发布Fire360及其评估套件，旨在推动AI模型在不确定环境下的感知、记忆、推理和行动能力。

Abstract: Modern AI systems struggle most in environments where reliability is critical
- scenes with smoke, poor visibility, and structural deformation. Each year,
tens of thousands of firefighters are injured on duty, often due to breakdowns
in situational perception. We introduce Fire360, a benchmark for evaluating
perception and reasoning in safety-critical firefighting scenarios. The dataset
includes 228 360-degree videos from professional training sessions under
diverse conditions (e.g., low light, thermal distortion), annotated with action
segments, object locations, and degradation metadata. Fire360 supports five
tasks: Visual Question Answering, Temporal Action Captioning, Object
Localization, Safety-Critical Reasoning, and Transformed Object Retrieval
(TOR). TOR tests whether models can match pristine exemplars to fire-damaged
counterparts in unpaired scenes, evaluating transformation-invariant
recognition. While human experts achieve 83.5% on TOR, models like GPT-4o lag
significantly, exposing failures in reasoning under degradation. By releasing
Fire360 and its evaluation suite, we aim to advance models that not only see,
but also remember, reason, and act under uncertainty. The dataset is available
at: https://uofi.box.com/v/fire360dataset.

</details>


### [349] [VLCD: Vision-Language Contrastive Distillation for Accurate and Efficient Automatic Placenta Analysis](https://arxiv.org/abs/2506.02229)
*Manas Mehta,Yimu Pan,Kelly Gallagher,Alison D. Gernand,Jeffery A. Goldstein,Delia Mwinyelle,Leena Mithal,James Z. Wang*

Main category: cs.CV

TL;DR: 提出两种改进视觉-语言对比学习框架的方法，提升胎盘病理检测的准确性和效率，使AI医疗方案更易部署。


<details>
  <summary>Details</summary>
Motivation: 现有胎盘病理自动检测方法计算量大，限制了实际应用。为提高效率和可部署性，研究改进视觉-语言对比学习框架。

Method: 采用文本锚定的视觉-语言对比知识蒸馏（VLCD）策略和基于自然图像的无监督预蒸馏方法，优化模型初始化。

Result: 所提方法在模型压缩和加速的同时，性能达到或超过教师模型，尤其提升了低质量图像的鲁棒性。

Conclusion: VLCD有效提升了医疗视觉-语言对比学习方案的效率，使AI医疗更易在资源有限环境中部署。

Abstract: Pathological examination of the placenta is an effective method for detecting
and mitigating health risks associated with childbirth. Recent advancements in
AI have enabled the use of photographs of the placenta and pathology reports
for detecting and classifying signs of childbirth-related pathologies. However,
existing automated methods are computationally extensive, which limits their
deployability. We propose two modifications to vision-language contrastive
learning (VLC) frameworks to enhance their accuracy and efficiency: (1)
text-anchored vision-language contrastive knowledge distillation (VLCD)-a new
knowledge distillation strategy for medical VLC pretraining, and (2)
unsupervised predistillation using a large natural images dataset for improved
initialization. Our approach distills efficient neural networks that match or
surpass the teacher model in performance while achieving model compression and
acceleration. Our results showcase the value of unsupervised predistillation in
improving the performance and robustness of our approach, specifically for
lower-quality images. VLCD serves as an effective way to improve the efficiency
and deployability of medical VLC approaches, making AI-based healthcare
solutions more accessible, especially in resource-constrained environments.

</details>


### [350] [Motion aware video generative model](https://arxiv.org/abs/2506.02244)
*Bowen Xue,Giuseppe Claudio Guarnera,Shuang Zhao,Zahra Montazeri*

Main category: cs.CV

TL;DR: 本文提出了一种基于物理的频率域方法，用于提升扩散模型生成视频的物理合理性，通过频域运动特征分析和两种新组件显著改善了运动质量。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的视频生成方法主要依赖大数据统计学习，缺乏对运动物理规律的显式建模，导致生成视频存在非物理伪影，降低了真实感。

Method: 1) 系统分析不同物理运动（平移/旋转/缩放）的频域特征；2) 提出物理运动损失函数和频域增强模块，通过零初始化策略保持原网络功能。

Result: 在多种视频扩散架构上的实验表明，该方法显著提升了运动质量和物理合理性，且不影响视觉质量或语义一致性。

Conclusion: 该频率域物理运动框架可泛化至不同视频生成架构，为深度学习视频合成提供了结合物理约束的理论方法，建立了数据驱动模型与物理运动模型的联系。

Abstract: Recent advances in diffusion-based video generation have yielded
unprecedented quality in visual content and semantic coherence. However,
current approaches predominantly rely on statistical learning from vast
datasets without explicitly modeling the underlying physics of motion,
resulting in subtle yet perceptible non-physical artifacts that diminish the
realism of generated videos. This paper introduces a physics-informed frequency
domain approach to enhance the physical plausibility of generated videos. We
first conduct a systematic analysis of the frequency-domain characteristics of
diverse physical motions (translation, rotation, scaling), revealing that each
motion type exhibits distinctive and identifiable spectral signatures. Building
on this theoretical foundation, we propose two complementary components: (1) a
physical motion loss function that quantifies and optimizes the conformity of
generated videos to ideal frequency-domain motion patterns, and (2) a frequency
domain enhancement module that progressively learns to adjust video features to
conform to physical motion constraints while preserving original network
functionality through a zero-initialization strategy. Experiments across
multiple video diffusion architectures demonstrate that our approach
significantly enhances motion quality and physical plausibility without
compromising visual quality or semantic alignment. Our frequency-domain
physical motion framework generalizes effectively across different video
generation architectures, offering a principled approach to incorporating
physical constraints into deep learning-based video synthesis pipelines. This
work seeks to establish connections between data-driven models and
physics-based motion models.

</details>


### [351] [QARI-OCR: High-Fidelity Arabic Text Recognition through Multimodal Large Language Model Adaptation](https://arxiv.org/abs/2506.02295)
*Ahmed Wasfy,Omer Nacar,Abdelakreem Elkhateb,Mahmoud Reda,Omar Elshehy,Adel Ammar,Wadii Boulila*

Main category: cs.CV

TL;DR: Qari-OCR系列模型通过迭代优化显著提升了阿拉伯语OCR性能，在复杂文本和低分辨率图像上表现优异。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语书写系统的复杂性（如连笔、变音符号和多样字体）给OCR技术带来持续挑战，需要更高效的解决方案。

Method: 基于Qwen2-VL-2B-Instruct模型，通过专业合成数据集的迭代微调开发Qari-OCR系列视觉语言模型。

Result: 最佳模型QARI v0.2在含变音符号文本上实现WER 0.160/CER 0.061/BLEU 0.737，刷新开源SOTA；后续版本还展示出手写体和文档结构理解潜力。

Conclusion: 该研究显著提升了阿拉伯语OCR的准确性和效率，并开源所有模型和数据集以推动相关研究。

Abstract: The inherent complexities of Arabic script; its cursive nature, diacritical
marks (tashkeel), and varied typography, pose persistent challenges for Optical
Character Recognition (OCR). We present Qari-OCR, a series of vision-language
models derived from Qwen2-VL-2B-Instruct, progressively optimized for Arabic
through iterative fine-tuning on specialized synthetic datasets. Our leading
model, QARI v0.2, establishes a new open-source state-of-the-art with a Word
Error Rate (WER) of 0.160, Character Error Rate (CER) of 0.061, and BLEU score
of 0.737 on diacritically-rich texts. Qari-OCR demonstrates superior handling
of tashkeel, diverse fonts, and document layouts, alongside impressive
performance on low-resolution images. Further explorations (QARI v0.3) showcase
strong potential for structural document understanding and handwritten text.
This work delivers a marked improvement in Arabic OCR accuracy and efficiency,
with all models and datasets released to foster further research.

</details>


### [352] [Iterative Self-Improvement of Vision Language Models for Image Scoring and Self-Explanation](https://arxiv.org/abs/2506.02708)
*Naoto Tanji,Toshihiko Yamasaki*

Main category: cs.CV

TL;DR: 提出一种新方法，让视觉语言模型不仅能评分图像，还能生成自然语言解释，通过自训练提升评分准确性和解释连贯性。


<details>
  <summary>Details</summary>
Motivation: 为了让用户理解模型的评分依据，需要模型不仅能输出分数，还能提供合理的自然语言解释。

Method: 利用图像评分数据集和指令调优的视觉语言模型进行自训练，无需外部数据或模型；通过直接偏好优化迭代训练两个数据集并合并。

Result: 该方法提高了评分准确性，并增强了生成解释的连贯性。

Conclusion: 该方法有效提升了视觉语言模型在图像评分任务中的可解释性和性能。

Abstract: Image scoring is a crucial task in numerous real-world applications. To trust
a model's judgment, understanding its rationale is essential. This paper
proposes a novel training method for Vision Language Models (VLMs) to generate
not only image scores but also corresponding justifications in natural
language. Leveraging only an image scoring dataset and an instruction-tuned
VLM, our method enables self-training, utilizing the VLM's generated text
without relying on external data or models. In addition, we introduce a simple
method for creating a dataset designed to improve alignment between predicted
scores and their textual justifications. By iteratively training the model with
Direct Preference Optimization on two distinct datasets and merging them, we
can improve both scoring accuracy and the coherence of generated explanations.

</details>


### [353] [Are classical deep neural networks weakly adversarially robust?](https://arxiv.org/abs/2506.02016)
*Nuolin Sun,Linyuan Wang,Dongyang Li,Bin Yan,Lei Li*

Main category: cs.CV

TL;DR: 该论文提出了一种基于层间特征路径相关性的对抗样本检测与图像识别方法，在保持较高干净准确率的同时提升对抗鲁棒性，且无需依赖计算成本高昂的对抗训练。


<details>
  <summary>Details</summary>
Motivation: 传统深度神经网络（DNN）对抗鲁棒性较弱，而主流防御方法对抗训练存在计算开销大的问题。受DNN层间特征聚类特性和渐进前馈坍缩（PFC）现象启发，作者探索利用特征路径相关性实现高效防御。

Method: 通过构建样本特征路径与类中心特征路径，计算两者相关性进行对抗样本检测和图像识别，避免传统对抗训练的重复生成和再训练过程。

Result: 在ResNet-20上实现82.77%干净准确率和44.17%对抗准确率（PFC条件下），标准ResNet-18上达80.01%和46.1%，显示DNN固有对抗鲁棒性优于传统认知。

Conclusion: 该方法揭示了DNN内在的对抗鲁棒性潜力，为不依赖高成本防御策略提供了新思路，挑战了传统认为DNN必然脆弱性的观点。

Abstract: Adversarial attacks have received increasing attention and it has been widely
recognized that classical DNNs have weak adversarial robustness. The most
commonly used adversarial defense method, adversarial training, improves the
adversarial accuracy of DNNs by generating adversarial examples and retraining
the model. However, adversarial training requires a significant computational
overhead. In this paper, inspired by existing studies focusing on the
clustering properties of DNN output features at each layer and the Progressive
Feedforward Collapse phenomenon, we propose a method for adversarial example
detection and image recognition that uses layer-wise features to construct
feature paths and computes the correlation between the examples feature paths
and the class-centered feature paths. Experimental results show that the
recognition method achieves 82.77% clean accuracy and 44.17% adversarial
accuracy on the ResNet-20 with PFC. Compared to the adversarial training method
with 77.64% clean accuracy and 52.94% adversarial accuracy, our method exhibits
a trade-off without relying on computationally expensive defense strategies.
Furthermore, on the standard ResNet-18, our method maintains this advantage
with respective metrics of 80.01% and 46.1%. This result reveals inherent
adversarial robustness in DNNs, challenging the conventional understanding of
the weak adversarial robustness in DNNs.

</details>


### [354] [Improve Multi-Modal Embedding Learning via Explicit Hard Negative Gradient Amplifying](https://arxiv.org/abs/2506.02020)
*Youze Xue,Dian Li,Gang Liu*

Main category: cs.CV

TL;DR: 本文提出了一种显式梯度放大方法，通过增强困难负样本的梯度来提升多模态嵌入模型的判别能力，在MMEB基准测试中取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）在检索任务中表现优异，但对比学习的核心范式仍未改变，且困难负样本的具体贡献尚未被深入研究。本文旨在分析困难负样本在模型参数更新中的作用，并提出改进方法。

Method: 作者通过详细分析info-NCE损失对查询、正样本和负样本的梯度，揭示了困难负样本的作用，并提出显式梯度放大器（Explicit Gradient Amplifier）来放大这些样本的梯度，从而提升模型的判别能力。

Result: 基于LLaVA-OneVision-7B架构的多模态嵌入模型在MMEB基准测试中达到了最先进的性能。结合自研的QQMM模型，该方法在MMEB排行榜上排名第一。

Conclusion: 显式梯度放大方法有效提升了多模态嵌入模型的性能，为对比学习中的困难负样本挖掘提供了新的思路。

Abstract: With the rapid advancement of multi-modal large language models (MLLMs) in
recent years, the foundational Contrastive Language-Image Pretraining (CLIP)
framework has been successfully extended to MLLMs, enabling more powerful and
universal multi-modal embeddings for a wide range of retrieval tasks. Despite
these developments, the core contrastive learning paradigm remains largely
unchanged from CLIP-style models to MLLMs. Within this framework, the effective
mining of hard negative samples continues to be a critical factor for enhancing
performance. Prior works have introduced both offline and online strategies for
hard negative mining to improve the efficiency of contrastive learning. While
these approaches have led to improved multi-modal embeddings, the specific
contribution of each hard negative sample to the learning process has not been
thoroughly investigated. In this work, we conduct a detailed analysis of the
gradients of the info-NCE loss with respect to the query, positive, and
negative samples, elucidating the role of hard negatives in updating model
parameters. Building upon this analysis, we propose to explicitly amplify the
gradients associated with hard negative samples, thereby encouraging the model
to learn more discriminative embeddings. Our multi-modal embedding model,
trained with the proposed Explicit Gradient Amplifier and based on the
LLaVA-OneVision-7B architecture, achieves state-of-the-art performance on the
MMEB benchmark compared to previous methods utilizing the same MLLM backbone.
Furthermore, when integrated with our self-developed MLLM, QQMM, our approach
attains the top rank on the MMEB leaderboard. Code and models are released on
https://github.com/QQ-MM/QQMM-embed.

</details>


### [355] [OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models](https://arxiv.org/abs/2506.03135)
*Mengdi Jia,Zekun Qi,Shaochen Zhang,Wenyao Zhang,Xinqiang Yu,Jiawei He,He Wang,Li Yi*

Main category: cs.CV

TL;DR: 该论文提出了OmniSpatial基准测试，用于全面评估视觉语言模型在空间推理能力上的表现，发现现有模型存在显著局限性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在基础空间关系理解上表现有限，缺乏对复杂空间推理能力的评估。

Method: 通过互联网数据爬取和人工标注，构建包含1.5K问答对的OmniSpatial基准测试，涵盖四大类50个子类。

Result: 实验表明，现有开源和闭源视觉语言模型在综合空间理解上存在显著不足。

Conclusion: 论文分析了失败案例，并提出了未来研究的潜在方向。

Abstract: Spatial reasoning is a key aspect of cognitive psychology and remains a major
bottleneck for current vision-language models (VLMs). While extensive research
has aimed to evaluate or improve VLMs' understanding of basic spatial
relations, such as distinguishing left from right, near from far, and object
counting, these tasks represent only the most fundamental level of spatial
reasoning. In this work, we introduce OmniSpatial, a comprehensive and
challenging benchmark for spatial reasoning, grounded in cognitive psychology.
OmniSpatial covers four major categories: dynamic reasoning, complex spatial
logic, spatial interaction, and perspective-taking, with 50 fine-grained
subcategories. Through Internet data crawling and careful manual annotation, we
construct over 1.5K question-answer pairs. Extensive experiments show that both
open- and closed-source VLMs, as well as existing reasoning and spatial
understanding models, exhibit significant limitations in comprehensive spatial
understanding. We further analyze failure cases and propose potential
directions for future research.

</details>


### [356] [MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition Query](https://arxiv.org/abs/2506.03144)
*Wei Chow,Yuan Gao,Linfeng Li,Xian Wang,Qi Xu,Hang Song,Lingdong Kong,Ran Zhou,Yi Zeng,Yidong Cai,Botian Jiang,Shilin Xu,Jiajun Zhang,Minghui Qiu,Xiangtai Li,Tianshu Yang,Siliang Tang,Juncheng Li*

Main category: cs.CV

TL;DR: 论文提出首个多语言多条件语义检索数据集MERIT，并开发新框架Coral解决现有模型忽略查询细节的问题，性能提升45.9%。


<details>
  <summary>Details</summary>
Motivation: 现有语义检索数据集局限于单语言、单图像或单一检索条件，无法满足实际多条件查询需求。

Method: 提出Coral框架：通过嵌入重构保留细粒度条件元素，结合对比学习提取全局语义，适配预训练MLLMs。

Result: Coral在MERIT上性能提升45.9%，并在8个基准测试中验证了强泛化能力。

Conclusion: MERIT数据集和Coral框架为多条件语义检索研究奠定了基础。

Abstract: Semantic retrieval is crucial for modern applications yet remains
underexplored in current research. Existing datasets are limited to single
languages, single images, or singular retrieval conditions, often failing to
fully exploit the expressive capacity of visual information as evidenced by
maintained performance when images are replaced with captions. However,
practical retrieval scenarios frequently involve interleaved multi-condition
queries with multiple images. Hence, this paper introduces MERIT, the first
multilingual dataset for interleaved multi-condition semantic retrieval,
comprising 320,000 queries with 135,000 products in 5 languages, covering 7
distinct product categories. Extensive experiments on MERIT identify existing
models's limitation: focusing solely on global semantic information while
neglecting specific conditional elements in queries. Consequently, we propose
Coral, a novel fine-tuning framework that adapts pre-trained MLLMs by
integrating embedding reconstruction to preserve fine-grained conditional
elements and contrastive learning to extract comprehensive global semantics.
Experiments demonstrate that Coral achieves a 45.9% performance improvement
over conventional approaches on MERIT, with strong generalization capabilities
validated across 8 established retrieval benchmarks. Collectively, our
contributions - a novel dataset, identification of critical limitations in
existing approaches, and an innovative fine-tuning framework - establish a
foundation for future research in interleaved multi-condition semantic
retrieval.

</details>


### [357] [UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation](https://arxiv.org/abs/2506.03147)
*Bin Lin,Zongjian Li,Xinhua Cheng,Yuwei Niu,Yang Ye,Xianyi He,Shenghai Yuan,Wangbo Yu,Shaodong Wang,Yunyang Ge,Yatian Pang,Li Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种基于语义特征的统一生成框架UniWorld，利用视觉语言模型和对比语义编码器，在少量数据下实现了优于现有模型的图像编辑性能，同时保持了图像理解和生成的竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有统一模型在视觉语言理解和文本生成图像方面表现优异，但在图像感知和操作任务上存在不足。GPT-4o-Image的成功启发了作者探索基于语义特征的图像处理方法。

Method: 作者提出了UniWorld框架，利用强大的视觉语言模型和对比语义编码器提取的语义特征，构建了一个统一的生成模型。

Result: UniWorld仅使用1%的BAGEL数据，就在图像编辑基准测试中 consistently 优于BAGEL，同时在多种图像感知任务中表现出色。

Conclusion: UniWorld框架证明了基于语义特征的统一生成模型在图像编辑和理解任务上的有效性，作者开源了所有模型、训练评估脚本和数据集。

Abstract: Although existing unified models deliver strong performance on
vision-language understanding and text-to-image generation, their models are
limited in exploring image perception and manipulation tasks, which are
urgently desired by users for wide applications. Recently, OpenAI released
their powerful GPT-4o-Image model for comprehensive image perception and
manipulation, achieving expressive capability and attracting community
interests. By observing the performance of GPT-4o-Image in our carefully
constructed experiments, we infer that GPT-4o-Image leverages features
extracted by semantic encoders instead of VAE, while VAEs are considered
essential components in many image manipulation models. Motivated by such
inspiring observations, we present a unified generative framework named
UniWorld based on semantic features provided by powerful visual-language models
and contrastive semantic encoders. As a result, we build a strong unified model
using only 1% amount of BAGEL's data, which consistently outperforms BAGEL on
image editing benchmarks. UniWorld also maintains competitive image
understanding and generation capabilities, achieving strong performance across
multiple image perception tasks. We fully open-source our models, including
model weights, training and evaluation scripts, and datasets.

</details>


### [358] [VidEvent: A Large Dataset for Understanding Dynamic Evolution of Events in Videos](https://arxiv.org/abs/2506.02448)
*Baoyu Liang,Qile Su,Shoutai Zhu,Yuchen Liang,Chao Tong*

Main category: cs.CV

TL;DR: 论文提出视频事件理解任务，并发布VidEvent数据集和基准模型以推动该领域研究。


<details>
  <summary>Details</summary>
Motivation: 视频事件因复杂结构、语义层级和动态演化特性，对AI理解构成挑战。

Method: 构建包含2.3万标注事件的VidEvent数据集，设计基线模型架构并提供性能指标。

Result: 数据集通过严格标注确保质量，基线模型为未来研究提供可比较的基准。

Conclusion: VidEvent数据集和基准模型有望促进视频事件理解算法的创新探索。

Abstract: Despite the significant impact of visual events on human cognition,
understanding events in videos remains a challenging task for AI due to their
complex structures, semantic hierarchies, and dynamic evolution. To address
this, we propose the task of video event understanding that extracts event
scripts and makes predictions with these scripts from videos. To support this
task, we introduce VidEvent, a large-scale dataset containing over 23,000
well-labeled events, featuring detailed event structures, broad hierarchies,
and logical relations extracted from movie recap videos. The dataset was
created through a meticulous annotation process, ensuring high-quality and
reliable event data. We also provide comprehensive baseline models offering
detailed descriptions of their architecture and performance metrics. These
models serve as benchmarks for future research, facilitating comparisons and
improvements. Our analysis of VidEvent and the baseline models highlights the
dataset's potential to advance video event understanding and encourages the
exploration of innovative algorithms and models. The dataset and related
resources are publicly available at www.videvent.top.

</details>


### [359] [Flexiffusion: Training-Free Segment-Wise Neural Architecture Search for Efficient Diffusion Models](https://arxiv.org/abs/2506.02488)
*Hongtao Huang,Xiaojun Chang,Lina Yao*

Main category: cs.CV

TL;DR: Flexiffusion提出了一种无需训练的神经架构搜索框架，通过动态组合不同类型的计算步骤来优化扩散模型，显著提升了生成速度且保持质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然能生成高质量图像，但因其多步迭代推理导致计算成本高昂。现有神经架构搜索方法因需重新训练、搜索复杂度高及评估耗时等问题，难以有效优化扩散模型。

Method: Flexiffusion将生成过程分解为等长片段，动态组合完整计算、缓存重用计算和跳过计算三种步骤类型，并引入轻量级评估指标rFID以加速搜索过程。

Result: Flexiffusion在多个数据集和模型上实现了至少2倍加速，FID下降不超过5%，在Stable Diffusion上达到5.1倍加速且CLIP分数几乎不变。

Conclusion: Flexiffusion为高效搜索高速扩散模型提供了新范式，在不牺牲生成质量的前提下显著降低了计算成本。

Abstract: Diffusion models (DMs) are powerful generative models capable of producing
high-fidelity images but are constrained by high computational costs due to
iterative multi-step inference. While Neural Architecture Search (NAS) can
optimize DMs, existing methods are hindered by retraining requirements,
exponential search complexity from step-wise optimization, and slow evaluation
relying on massive image generation. To address these challenges, we propose
Flexiffusion, a training-free NAS framework that jointly optimizes generation
schedules and model architectures without modifying pre-trained parameters. Our
key insight is to decompose the generation process into flexible segments of
equal length, where each segment dynamically combines three step types: full
(complete computation), partial (cache-reused computation), and null (skipped
computation). This segment-wise search space reduces the candidate pool
exponentially compared to step-wise NAS while preserving architectural
diversity. Further, we introduce relative FID (rFID), a lightweight evaluation
metric for NAS that measures divergence from a teacher model's outputs instead
of ground truth, slashing evaluation time by over $90\%$. In practice,
Flexiffusion achieves at least $2\times$ acceleration across LDMs, Stable
Diffusion, and DDPMs on ImageNet and MS-COCO, with FID degradation under $5\%$,
outperforming prior NAS and caching methods. Notably, it attains $5.1\times$
speedup on Stable Diffusion with near-identical CLIP scores. Our work pioneers
a resource-efficient paradigm for searching high-speed DMs without sacrificing
quality.

</details>


### [360] [Cycle Consistency as Reward: Learning Image-Text Alignment without Human Preferences](https://arxiv.org/abs/2506.02095)
*Hyojin Bahng,Caroline Chan,Fredo Durand,Phillip Isola*

Main category: cs.CV

TL;DR: 论文提出利用循环一致性作为监督信号，构建大规模偏好数据集，提升语言-视觉对齐任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖人工或AI偏好标注，成本高耗时长。多模态数据日益复杂，亟需高效对齐方法。

Method: 通过文本-图像双向循环重建计算一致性分数，构建86.6万对比对的偏好数据集，训练奖励模型。

Result: 在详细描述任务上超越SOTA指标，DPO训练显著提升多模态任务性能，支持高效Best-of-N采样验证。

Conclusion: 循环一致性为多模态对齐提供高效监督信号，开源资源推动相关研究发展。

Abstract: Learning alignment between language and vision is a fundamental challenge,
especially as multimodal data becomes increasingly detailed and complex.
Existing methods often rely on collecting human or AI preferences, which can be
costly and time-intensive. We propose an alternative approach that leverages
cycle consistency as a supervisory signal. Given an image and generated text,
we map the text back to image space using a text-to-image model and compute the
similarity between the original image and its reconstruction. Analogously, for
text-to-image generation, we measure the textual similarity between an input
caption and its reconstruction through the cycle. We use the cycle consistency
score to rank candidates and construct a preference dataset of 866K comparison
pairs. The reward model trained on our dataset outperforms state-of-the-art
alignment metrics on detailed captioning, with superior inference-time
scalability when used as a verifier for Best-of-N sampling. Furthermore,
performing DPO and Diffusion DPO using our dataset enhances performance across
a wide range of vision-language tasks and text-to-image generation. Our
dataset, model, and code are at https://cyclereward.github.io

</details>


### [361] [VisuRiddles: Fine-grained Perception is a Primary Bottleneck for Multimodal Large Language Models in Abstract Visual Reasoning](https://arxiv.org/abs/2506.02537)
*Hao Yan,Handong Zheng,Hao Wang,Liang Yin,Xingchen Liu,Zhenbiao Cao,Xinxing Su,Zihao Chen,Jihao Wu,Minghui Liao,Chao Weng,Wei Chen,Yuliang Liu,Xiang Bai*

Main category: cs.CV

TL;DR: 该论文针对多模态大语言模型在抽象视觉推理（AVR）中的瓶颈问题，提出了VisuRiddles基准和Perceptual Riddle Synthesizer（PRS）框架，通过生成细粒度感知描述的训练数据，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在抽象视觉推理任务中表现不佳，主要由于对抽象图形的感知能力有限。为了解决这一问题，作者研究了现有模型的瓶颈，并尝试通过合成训练数据来提升其抽象视觉感知能力。

Method: 论文提出了VisuRiddles基准，包含五个核心维度和两类高阶推理任务；同时设计了Perceptual Riddle Synthesizer（PRS）框架，自动生成带有细粒度感知描述的谜题数据，用于监督中间推理阶段。

Result: 实验结果表明，细粒度视觉感知是主要瓶颈，PRS框架显著提升了当前多模态大语言模型在VisuRiddles任务上的性能。

Conclusion: 通过合成细粒度感知描述的训练数据，可以有效提升多模态大语言模型在抽象视觉推理任务中的表现，同时增强模型的可解释性。

Abstract: Recent strides in multimodal large language models (MLLMs) have significantly
advanced their performance in many reasoning tasks. However, Abstract Visual
Reasoning (AVR) remains a critical challenge, primarily due to limitations in
perceiving abstract graphics. To tackle this issue, we investigate the
bottlenecks in current MLLMs and synthesize training data to improve their
abstract visual perception. First, we propose VisuRiddles, a benchmark for AVR,
featuring tasks meticulously constructed to assess models' reasoning capacities
across five core dimensions and two high-level reasoning categories. Second, we
introduce the Perceptual Riddle Synthesizer (PRS), an automated framework for
generating riddles with fine-grained perceptual descriptions. PRS not only
generates valuable training data for abstract graphics but also provides
fine-grained perceptual description, crucially allowing for supervision over
intermediate reasoning stages and thereby improving both training efficacy and
model interpretability. Our extensive experimental results on VisuRiddles
empirically validate that fine-grained visual perception is the principal
bottleneck and our synthesis framework markedly enhances the performance of
contemporary MLLMs on these challenging tasks. Our code and dataset will be
released at https://github.com/yh-hust/VisuRiddles

</details>


### [362] [Quantifying task-relevant representational similarity using decision variable correlation](https://arxiv.org/abs/2506.02164)
*Yu,Qian,Wilson S. Geisler,Xue-Xin Wei*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法DVC来比较模型与猴子大脑在图像分类任务中的决策策略相似性，发现两者存在本质差异。


<details>
  <summary>Details</summary>
Motivation: 以往研究对大脑与深度神经网络在图像分类中的表征相似性存在争议，本文旨在通过新方法量化任务相关的决策策略相似性。

Method: 使用决策变量相关（DVC）方法，量化分类任务中解码决策的相关性，评估猴子V4/IT记录与图像分类模型的相似性。

Result: 模型间相似性与猴子间相似性相当，但模型与猴子相似性较低且随ImageNet性能提升而下降；对抗训练和更大数据集预训练未改善模型-猴子相似性。

Conclusion: 猴子V4/IT与图像分类模型在任务相关表征上存在根本差异，当前模型优化方向可能偏离生物视觉处理机制。

Abstract: Previous studies have compared the brain and deep neural networks trained on
image classification. Intriguingly, while some suggest that their
representations are highly similar, others argued the opposite. Here, we
propose a new approach to characterize the similarity of the decision
strategies of two observers (models or brains) using decision variable
correlation (DVC). DVC quantifies the correlation between decoded decisions on
individual samples in a classification task and thus can capture task-relevant
information rather than general representational alignment. We evaluate this
method using monkey V4/IT recordings and models trained on image classification
tasks.
  We find that model--model similarity is comparable to monkey--monkey
similarity, whereas model--monkey similarity is consistently lower and,
surprisingly, decreases with increasing ImageNet-1k performance. While
adversarial training enhances robustness, it does not improve model--monkey
similarity in task-relevant dimensions; however, it markedly increases
model--model similarity. Similarly, pre-training on larger datasets does not
improve model--monkey similarity. These results suggest a fundamental
divergence between the task-relevant representations in monkey V4/IT and those
learned by models trained on image classification tasks.

</details>


### [363] [Technical Report for Ego4D Long-Term Action Anticipation Challenge 2025](https://arxiv.org/abs/2506.02550)
*Qiaohui Chu,Haoyu Zhang,Yisen Feng,Meng Liu,Weili Guan,Yaowei Wang,Liqiang Nie*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的三阶段框架，用于Ego4D长期动作预测任务，结合视觉编码器、Transformer和大型语言模型，在CVPR 2025挑战赛中取得第一名。


<details>
  <summary>Details</summary>
Motivation: 受基础模型最新进展的启发，本文旨在解决长期动作预测（LTA）任务中的挑战，通过整合多阶段模型提高预测准确性。

Method: 方法分为三个阶段：特征提取（使用高性能视觉编码器）、动作识别（通过Transformer预测动词和名词，并引入动词-名词共现矩阵提升准确性）、长期动作预测（将预测结果作为文本提示输入微调后的大型语言模型）。

Result: 该框架在CVPR 2025的Ego4D长期动作预测挑战赛中取得第一名，创下了新的最先进水平。

Conclusion: 提出的三阶段框架有效整合了视觉和语言模型，显著提升了长期动作预测的性能，代码将开源供进一步研究。

Abstract: In this report, we present a novel three-stage framework developed for the
Ego4D Long-Term Action Anticipation (LTA) task. Inspired by recent advances in
foundation models, our method consists of three stages: feature extraction,
action recognition, and long-term action anticipation. First, visual features
are extracted using a high-performance visual encoder. The features are then
fed into a Transformer to predict verbs and nouns, with a verb-noun
co-occurrence matrix incorporated to enhance recognition accuracy. Finally, the
predicted verb-noun pairs are formatted as textual prompts and input into a
fine-tuned large language model (LLM) to anticipate future action sequences.
Our framework achieves first place in this challenge at CVPR 2025, establishing
a new state-of-the-art in long-term action prediction. Our code will be
released at https://github.com/CorrineQiu/Ego4D-LTA-Challenge-2025.

</details>


### [364] [Diff2Flow: Training Flow Matching Models via Diffusion Model Alignment](https://arxiv.org/abs/2506.02221)
*Johannes Schusterbauer,Ming Gui,Frank Fundel,Björn Ommer*

Main category: cs.CV

TL;DR: Diff2Flow框架高效地将预训练扩散模型知识迁移至流匹配，实现更优性能和参数效率。


<details>
  <summary>Details</summary>
Motivation: 当前流匹配基础模型微调计算成本高，而扩散模型（如Stable Diffusion）具有高效架构和生态支持。研究旨在解决如何高效将扩散模型知识迁移至流匹配的挑战。

Method: 提出Diff2Flow框架，通过时间步重缩放、对齐插值及从扩散预测导出流匹配兼容的速度场，系统桥接扩散与流匹配范式。

Result: 实验表明，Diff2Flow在参数效率约束下优于朴素流匹配和扩散微调，并在多样下游任务中达到或超越现有最佳方法性能。

Conclusion: Diff2Flow实现了扩散模型向流匹配的高效知识迁移，为生成任务提供了计算高效且性能优越的解决方案。

Abstract: Diffusion models have revolutionized generative tasks through high-fidelity
outputs, yet flow matching (FM) offers faster inference and empirical
performance gains. However, current foundation FM models are computationally
prohibitive for finetuning, while diffusion models like Stable Diffusion
benefit from efficient architectures and ecosystem support. This work addresses
the critical challenge of efficiently transferring knowledge from pre-trained
diffusion models to flow matching. We propose Diff2Flow, a novel framework that
systematically bridges diffusion and FM paradigms by rescaling timesteps,
aligning interpolants, and deriving FM-compatible velocity fields from
diffusion predictions. This alignment enables direct and efficient FM
finetuning of diffusion priors with no extra computation overhead. Our
experiments demonstrate that Diff2Flow outperforms na\"ive FM and diffusion
finetuning particularly under parameter-efficient constraints, while achieving
superior or competitive performance across diverse downstream tasks compared to
state-of-the-art methods. We will release our code at
https://github.com/CompVis/diff2flow.

</details>


### [365] [High Performance Space Debris Tracking in Complex Skylight Backgrounds with a Large-Scale Dataset](https://arxiv.org/abs/2506.02614)
*Guohang Zhuang,Weixi Song,Jinyang Huang,Chenwei Yang,Yan Lu*

Main category: cs.CV

TL;DR: 提出基于深度学习的SDT-Net模型，用于高精度空间碎片追踪，并构建大规模合成数据集SDTD。模型在真实数据测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统信号处理方法难以有效处理复杂背景下的密集空间碎片，亟需实时精准的追踪技术以应对太空探索快速发展带来的碎片威胁。

Method: 开发端到端深度学习网络SDT-Net，通过新型观测模拟方案构建含18,040视频序列、25万合成碎片的SDTD数据集进行训练验证。

Result: 模型在真实南极站数据测试中取得70.6%的MOTA分数，验证了其在实际场景中的强迁移能力。

Conclusion: SDT-Net与SDTD数据集为空间碎片追踪提供了有效解决方案，代码与数据即将开源。

Abstract: With the rapid development of space exploration, space debris has attracted
more attention due to its potential extreme threat, leading to the need for
real-time and accurate debris tracking. However, existing methods are mainly
based on traditional signal processing, which cannot effectively process the
complex background and dense space debris. In this paper, we propose a deep
learning-based Space Debris Tracking Network~(SDT-Net) to achieve highly
accurate debris tracking. SDT-Net effectively represents the feature of debris,
enhancing the efficiency and stability of end-to-end model learning. To train
and evaluate this model effectively, we also produce a large-scale dataset
Space Debris Tracking Dataset (SDTD) by a novel observation-based data
simulation scheme. SDTD contains 18,040 video sequences with a total of 62,562
frames and covers 250,000 synthetic space debris. Extensive experiments
validate the effectiveness of our model and the challenging of our dataset.
Furthermore, we test our model on real data from the Antarctic Station,
achieving a MOTA score of 70.6%, which demonstrates its strong transferability
to real-world scenarios. Our dataset and code will be released soon.

</details>


### [366] [Approximate Borderline Sampling using Granular-Ball for Classification Tasks](https://arxiv.org/abs/2506.02366)
*Qin Xie,Qinghua Zhang,Shuyin Xia*

Main category: cs.CV

TL;DR: 本文提出了一种基于粒球（GB）的近似边界采样方法（GBABS），通过限制扩散生成粒球（RD-GBG）防止重叠，并结合异构最近邻概念进行边界采样，显著提升了分类任务中噪声数据集的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于粒球的采样方法在边界采样策略和粒球重叠导致的类边界模糊或收缩问题上存在不足，影响了分类器的效率和鲁棒性。

Method: 1. 提出RD-GBG方法，通过受限扩展防止粒球重叠；2. 基于异构最近邻概念，提出GBABS方法，实现边界采样并提升噪声数据集质量。

Result: 实验表明，所提方法在分类任务中优于现有基于粒球的采样方法及其他代表性采样方法，尤其在噪声数据集上表现突出。

Conclusion: GBABS方法无需最优纯度阈值即可有效处理噪声数据集，为分类任务提供了一种高效的边界采样解决方案。

Abstract: Data sampling enhances classifier efficiency and robustness through data
compression and quality improvement. Recently, the sampling method based on
granular-ball (GB) has shown promising performance in generality and noisy
classification tasks. However, some limitations remain, including the absence
of borderline sampling strategies and issues with class boundary blurring or
shrinking due to overlap between GBs. In this paper, an approximate borderline
sampling method using GBs is proposed for classification tasks. First, a
restricted diffusion-based GB generation (RD-GBG) method is proposed, which
prevents GB overlaps by constrained expansion, preserving precise geometric
representation of GBs via redefined ones. Second, based on the concept of
heterogeneous nearest neighbor, a GB-based approximate borderline sampling
(GBABS) method is proposed, which is the first general sampling method capable
of both borderline sampling and improving the quality of class noise datasets.
Additionally, since RD-GBG incorporates noise detection and GBABS focuses on
borderline samples, GBABS performs outstandingly on class noise datasets
without the need for an optimal purity threshold. Experimental results
demonstrate that the proposed methods outperform the GB-based sampling method
and several representative sampling methods. Our source code is publicly
available at https://github.com/CherylTse/GBABS.

</details>


### [367] [Hierarchical Question-Answering for Driving Scene Understanding Using Vision-Language Models](https://arxiv.org/abs/2506.02615)
*Safaa Abdullahi Moallim Mohamud,Minjin Baek,Dong Seog Han*

Main category: cs.CV

TL;DR: 本文提出了一种用于自动驾驶场景理解的分层问答方法，通过微调紧凑视觉语言模型和动态问题树优化推理效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决自动驾驶中详细视觉理解与计算成本之间的平衡问题，作者提出了一种结合地理定制数据集和分层问答策略的方法。

Method: 方法包括微调视觉语言模型以适应特定地理区域，采用分层问答策略分解场景理解任务，并通过动态跳过问题优化推理时间。

Result: 实验表明，该方法在自定义数据集上通过GPT无参考评分验证了其与GPT-4o等先进方法的竞争力，同时显著降低了推理时间。

Conclusion: 该方法能够以低延迟捕获关键驾驶元素，为自动驾驶场景理解提供了一种高效且准确的解决方案。

Abstract: In this paper, we present a hierarchical question-answering (QA) approach for
scene understanding in autonomous vehicles, balancing cost-efficiency with
detailed visual interpretation. The method fine-tunes a compact vision-language
model (VLM) on a custom dataset specific to the geographical area in which the
vehicle operates to capture key driving-related visual elements. At the
inference stage, the hierarchical QA strategy decomposes the scene
understanding task into high-level and detailed sub-questions. Instead of
generating lengthy descriptions, the VLM navigates a structured question tree,
where answering high-level questions (e.g., "Is it possible for the ego vehicle
to turn left at the intersection?") triggers more detailed sub-questions (e.g.,
"Is there a vehicle approaching the intersection from the opposite
direction?"). To optimize inference time, questions are dynamically skipped
based on previous answers, minimizing computational overhead. The extracted
answers are then synthesized using handcrafted templates to ensure coherent,
contextually accurate scene descriptions. We evaluate the proposed approach on
the custom dataset using GPT reference-free scoring, demonstrating its
competitiveness with state-of-the-art methods like GPT-4o in capturing key
scene details while achieving significantly lower inference time. Moreover,
qualitative results from real-time deployment highlight the proposed approach's
capacity to capture key driving elements with minimal latency.

</details>


### [368] [Multi-level and Multi-modal Action Anticipation](https://arxiv.org/abs/2506.02382)
*Seulgi Kim,Ghazal Kaviani,Mohit Prabhushankar,Ghassan AlRegib*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的多模态动作预测方法m&m-Ant，结合视觉和文本线索，并利用分层语义信息提高预测准确性。通过细粒度标签生成器和时间一致性损失函数优化性能，在多个数据集上实现了3.08%的平均准确率提升。


<details>
  <summary>Details</summary>
Motivation: 传统动作预测方法仅依赖视觉模态，忽略了多源信息整合的潜力。受人类行为启发，本文旨在结合视觉和文本线索，并建模分层语义信息，以更准确地预测未来动作。

Method: 提出m&m-Ant方法，整合视觉和文本模态，并显式建模分层语义信息。为解决粗粒度动作标签不准确的问题，设计了细粒度标签生成器和时间一致性损失函数。

Result: 在Breakfast、50 Salads和DARai等数据集上的实验表明，该方法平均预测准确率比现有方法提升3.08%，达到最先进水平。

Conclusion: 本文展示了多模态和分层建模在动作预测中的潜力，为未来研究设立了新基准。代码已开源。

Abstract: Action anticipation, the task of predicting future actions from partially
observed videos, is crucial for advancing intelligent systems. Unlike action
recognition, which operates on fully observed videos, action anticipation must
handle incomplete information. Hence, it requires temporal reasoning, and
inherent uncertainty handling. While recent advances have been made,
traditional methods often focus solely on visual modalities, neglecting the
potential of integrating multiple sources of information. Drawing inspiration
from human behavior, we introduce \textit{Multi-level and Multi-modal Action
Anticipation (m\&m-Ant)}, a novel multi-modal action anticipation approach that
combines both visual and textual cues, while explicitly modeling hierarchical
semantic information for more accurate predictions. To address the challenge of
inaccurate coarse action labels, we propose a fine-grained label generator
paired with a specialized temporal consistency loss function to optimize
performance. Extensive experiments on widely used datasets, including
Breakfast, 50 Salads, and DARai, demonstrate the effectiveness of our approach,
achieving state-of-the-art results with an average anticipation accuracy
improvement of 3.08\% over existing methods. This work underscores the
potential of multi-modal and hierarchical modeling in advancing action
anticipation and establishes a new benchmark for future research in the field.
Our code is available at: https://github.com/olivesgatech/mM-ant.

</details>


### [369] [Self-Disentanglement and Re-Composition for Cross-Domain Few-Shot Segmentation](https://arxiv.org/abs/2506.02677)
*Jintao Tong,Yixiong Zou,Guangyao Chen,Yuhua Li,Ruixuan Li*

Main category: cs.CV

TL;DR: 该论文针对跨域少样本分割任务中的特征纠缠问题，提出基于ViT结构分解的权重学习方法，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有跨域少样本分割方法通过比较训练和测试样本的距离进行预测，但存在特征纠缠问题，导致源域模式难以迁移。

Method: 通过分解ViT结构分析纠缠问题，提出对ViT组件比较进行加权学习的方法，实现特征解耦和重组。

Result: 模型在1-shot和5-shot设置下平均准确率分别超越当前最优方法1.92%和1.88%。

Conclusion: 所提方法有效解决了特征纠缠问题，同时提升了模型的泛化能力和微调效果。

Abstract: Cross-Domain Few-Shot Segmentation (CD-FSS) aims to transfer knowledge from a
source-domain dataset to unseen target-domain datasets with limited
annotations. Current methods typically compare the distance between training
and testing samples for mask prediction. However, we find an entanglement
problem exists in this widely adopted method, which tends to bind sourcedomain
patterns together and make each of them hard to transfer. In this paper, we aim
to address this problem for the CD-FSS task. We first find a natural
decomposition of the ViT structure, based on which we delve into the
entanglement problem for an interpretation. We find the decomposed ViT
components are crossly compared between images in distance calculation, where
the rational comparisons are entangled with those meaningless ones by their
equal importance, leading to the entanglement problem. Based on this
interpretation, we further propose to address the entanglement problem by
learning to weigh for all comparisons of ViT components, which learn
disentangled features and re-compose them for the CD-FSS task, benefiting both
the generalization and finetuning. Experiments show that our model outperforms
the state-of-the-art CD-FSS method by 1.92% and 1.88% in average accuracy under
1-shot and 5-shot settings, respectively.

</details>


### [370] [LinkTo-Anime: A 2D Animation Optical Flow Dataset from 3D Model Rendering](https://arxiv.org/abs/2506.02733)
*Xiaoyi Feng,Kaifeng Zou,Caichun Cen,Tao Huang,Hui Guo,Zizhou Huang,Yingli Zhao,Mingqing Zhang,Diwei Wang,Yuntao Zou,Dagang Li*

Main category: cs.CV

TL;DR: 该论文介绍了首个针对赛璐珞动画角色运动的高质量光流数据集LinkTo-Anime，包含丰富注释和基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有光流数据集主要关注真实世界模拟或合成人体运动，缺乏针对赛璐珞动画角色运动的数据集，该领域具有独特的视觉和运动特征。

Method: 通过3D模型渲染生成高质量数据集，提供前向和后向光流、遮挡掩码和Mixamo骨骼等丰富注释，并构建多种光流估计方法的基准测试。

Result: 数据集包含395个视频序列，共24,230帧训练数据、720帧验证数据和4,320帧测试数据，并分析了不同数据集上的局限性和不足。

Conclusion: LinkTo-Anime填补了赛璐珞动画角色运动数据集的空白，为光流估计及相关下游任务研究提供了重要资源。

Abstract: Existing optical flow datasets focus primarily on real-world simulation or
synthetic human motion, but few are tailored to Celluloid(cel) anime character
motion: a domain with unique visual and motion characteristics. To bridge this
gap and facilitate research in optical flow estimation and downstream tasks
such as anime video generation and line drawing colorization, we introduce
LinkTo-Anime, the first high-quality dataset specifically designed for cel
anime character motion generated with 3D model rendering. LinkTo-Anime provides
rich annotations including forward and backward optical flow, occlusion masks,
and Mixamo Skeleton. The dataset comprises 395 video sequences, totally 24,230
training frames, 720 validation frames, and 4,320 test frames. Furthermore, a
comprehensive benchmark is constructed with various optical flow estimation
methods to analyze the shortcomings and limitations across multiple datasets.

</details>


### [371] [Unified Attention Modeling for Efficient Free-Viewing and Visual Search via Shared Representations](https://arxiv.org/abs/2506.02764)
*Fatma Youssef Mohammed,Kostas Alexis*

Main category: cs.CV

TL;DR: 该研究探讨自由观看与任务驱动视觉搜索是否存在共同注意力表征，并提出一种基于HAT的神经网络架构，证明两者可共享表征，实现高效知识迁移。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常将自由观看和任务特定场景下的人类注意力建模分开探讨，缺乏对两者是否存在共同表征的探索。

Method: 提出基于Human Attention Transformer（HAT）的神经网络架构，验证自由观看与视觉搜索的共享表征假设。

Result: 自由观看训练的模型迁移至视觉搜索时，预测注视扫描路径性能仅下降3.86%（SemSS指标），计算成本降低92.29%（GFLOPs）和31.23%（参数量）。

Conclusion: 自由观看与任务驱动视觉搜索可通过共同表征实现高效知识迁移，显著降低计算成本。

Abstract: Computational human attention modeling in free-viewing and task-specific
settings is often studied separately, with limited exploration of whether a
common representation exists between them. This work investigates this question
and proposes a neural network architecture that builds upon the Human Attention
transformer (HAT) to test the hypothesis. Our results demonstrate that
free-viewing and visual search can efficiently share a common representation,
allowing a model trained in free-viewing attention to transfer its knowledge to
task-driven visual search with a performance drop of only 3.86% in the
predicted fixation scanpaths, measured by the semantic sequence score (SemSS)
metric which reflects the similarity between predicted and human scanpaths.
This transfer reduces computational costs by 92.29% in terms of GFLOPs and
31.23% in terms of trainable parameters.

</details>


### [372] [FlySearch: Exploring how vision-language models explore](https://arxiv.org/abs/2506.02896)
*Adam Pardyl,Dominik Matuszek,Mateusz Przebieracz,Marek Cygan,Bartosz Zieliński,Maciej Wołczyk*

Main category: cs.CV

TL;DR: 论文提出FlySearch，一个3D户外逼真环境，用于测试视觉语言模型在复杂场景中的探索能力，发现当前最先进模型表现不佳，并分析了原因。


<details>
  <summary>Details</summary>
Motivation: 现实世界复杂无序，需要主动探索关键信息。视觉语言模型（VLMs）在零样本任务中表现优异，但尚不清楚其在此类条件下的有效性。

Method: 引入FlySearch，一个3D户外逼真环境，定义三种难度场景，测试VLMs在复杂场景中的搜索和导航能力。

Result: 当前最先进的VLMs无法可靠完成最简单的探索任务，与人类表现差距随任务难度增加而扩大。分析原因包括视觉幻觉、上下文误解和任务规划失败。

Conclusion: 部分问题可通过微调解决，公开了基准测试、场景和代码库。

Abstract: The real world is messy and unstructured. Uncovering critical information
often requires active, goal-driven exploration. It remains to be seen whether
Vision-Language Models (VLMs), which recently emerged as a popular zero-shot
tool in many difficult tasks, can operate effectively in such conditions. In
this paper, we answer this question by introducing FlySearch, a 3D, outdoor,
photorealistic environment for searching and navigating to objects in complex
scenes. We define three sets of scenarios with varying difficulty and observe
that state-of-the-art VLMs cannot reliably solve even the simplest exploration
tasks, with the gap to human performance increasing as the tasks get harder. We
identify a set of central causes, ranging from vision hallucination, through
context misunderstanding, to task planning failures, and we show that some of
them can be addressed by finetuning. We publicly release the benchmark,
scenarios, and the underlying codebase.

</details>


### [373] [FORLA:Federated Object-centric Representation Learning with Slot Attention](https://arxiv.org/abs/2506.02964)
*Guiqiu Liao,Matjaz Jogan,Eric Eaton,Daniel A. Hashimoto*

Main category: cs.CV

TL;DR: 论文提出FORLA框架，通过无监督槽注意力实现联邦学习中跨客户端的对象中心表示学习与特征适配，在多个真实数据集上表现优于集中式基线。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中跨异构无标签数据集的视觉表示学习难题，需同时满足特征跨客户端联合信息性且能无监督解耦领域特定因素。

Method: 采用共享特征适配器（跨客户端协作训练）和共享槽注意力模块，设计双分支师生架构：学生解码器重建完整特征，教师解码器重建低维适配特征。

Result: 在多个真实数据集上，框架在对象发现任务上优于集中式基线，并能学习到跨领域通用的紧凑表示。

Conclusion: 联邦槽注意力可作为跨领域分布式概念的无监督视觉表示学习的有效工具。

Abstract: Learning efficient visual representations across heterogeneous unlabeled
datasets remains a central challenge in federated learning. Effective federated
representations require features that are jointly informative across clients
while disentangling domain-specific factors without supervision. We introduce
FORLA, a novel framework for federated object-centric representation learning
and feature adaptation across clients using unsupervised slot attention. At the
core of our method is a shared feature adapter, trained collaboratively across
clients to adapt features from foundation models, and a shared slot attention
module that learns to reconstruct the adapted features. To optimize this
adapter, we design a two-branch student-teacher architecture. In each client, a
student decoder learns to reconstruct full features from foundation models,
while a teacher decoder reconstructs their adapted, low-dimensional
counterpart. The shared slot attention module bridges cross-domain learning by
aligning object-level representations across clients. Experiments in multiple
real-world datasets show that our framework not only outperforms centralized
baselines on object discovery but also learns a compact, universal
representation that generalizes well across domains. This work highlights
federated slot attention as an effective tool for scalable, unsupervised visual
representation learning from cross-domain data with distributed concepts.

</details>


### [374] [HaploOmni: Unified Single Transformer for Multimodal Video Understanding and Generation](https://arxiv.org/abs/2506.02975)
*Yicheng Xiao,Lin Song,Rui Yang,Cheng Cheng,Zunnan Xu,Zhaoyang Zhang,Yixiao Ge,Xiu Li,Ying Shan*

Main category: cs.CV

TL;DR: 提出了一种高效训练范式HaploOmni，通过多模态预热策略和特征预缩放技术，构建统一的多模态理解与生成Transformer模型。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型发展，多模态理解与生成从分离组件转向统一框架，但面临跨模态兼容性挑战。本文旨在通过高效训练方法构建单一Transformer模型。

Method: 采用多模态预热策略利用先验知识扩展能力，提出特征预缩放和多模态AdaLN技术解决跨模态兼容性问题。

Result: HaploOmni在有限训练成本下，在图像/视频理解与生成任务上超越先进统一模型。

Conclusion: HaploOmni证明了单一Transformer在多模态任务中的高效性，代码已开源。

Abstract: With the advancement of language models, unified multimodal understanding and
generation have made significant strides, with model architectures evolving
from separated components to unified single-model frameworks. This paper
explores an efficient training paradigm to build a single transformer for
unified multimodal understanding and generation. Specifically, we propose a
multimodal warmup strategy utilizing prior knowledge to extend capabilities. To
address cross-modal compatibility challenges, we introduce feature pre-scaling
and multimodal AdaLN techniques. Integrating the proposed technologies, we
present the HaploOmni, a new single multimodal transformer. With limited
training costs, HaploOmni achieves competitive performance across multiple
image and video understanding and generation benchmarks over advanced unified
models. All codes will be made public at https://github.com/Tencent/HaploVLM.

</details>


### [375] [Deep Learning for Retinal Degeneration Assessment: A Comprehensive Analysis of the MARIO AMD Progression Challenge](https://arxiv.org/abs/2506.02976)
*Rachid Zeghlache,Ikram Brahim,Pierre-Henri Conze,Mathieu Lamard,Mohammed El Amine Lazouni,Zineb Aziza Elaouaber,Leila Ryma Lazouni,Christopher Nielsen,Ahmad O. Ahsan,Matthias Wilms,Nils D. Forkert,Lovre Antonio Budimir,Ivana Matovinović,Donik Vršnak,Sven Lončarić,Philippe Zhang,Weili Jiang,Yihao Li,Yiding Hao,Markus Frohmann,Patrick Binder,Marcel Huber,Taha Emre,Teresa Finisterra Araújo,Marzieh Oghbaie,Hrvoje Bogunović,Amerens A. Bekkers,Nina M. van Liebergen,Hugo J. Kuijf,Abdul Qayyum,Moona Mazher,Steven A. Niederer,Alberto J. Beltrán-Carrero,Juan J. Gómez-Valverde,Javier Torresano-Rodríquez,Álvaro Caballero-Sastre,María J. Ledesma Carbayo,Yosuke Yamagishi,Yi Ding,Robin Peretzke,Alexandra Ertl,Maximilian Fischer,Jessica Kächele,Sofiane Zehar,Karim Boukli Hacene,Thomas Monfort,Béatrice Cochener,Mostafa El Habib Daho,Anas-Alexis Benyoussef,Gwenolé Quellec*

Main category: cs.CV

TL;DR: MARIO挑战赛聚焦于通过OCT图像自动检测和监测年龄相关性黄斑变性（AMD），评估算法在检测AMD新生血管活动变化中的表现，并比较AI与医生在AMD进展测量和预测方面的能力。


<details>
  <summary>Details</summary>
Motivation: 推动年龄相关性黄斑变性（AMD）的自动化检测和监测技术发展，通过多模态数据集评估算法性能，并探索AI在AMD进展测量和预测中的应用潜力。

Method: 挑战赛设计了两个任务：一是对连续两张OCT B扫描图像的演变进行分类，二是预测接受抗VEGF治疗患者未来三个月的AMD演变。使用了来自法国和阿尔及利亚的多模态数据集，35个团队参与，前12名展示了他们的方法。

Result: AI在测量AMD进展（任务1）方面表现与医生相当，但在预测未来演变（任务2）方面尚不成熟。挑战赛为使用OCT、红外成像和临床数据监测AMD设定了基准。

Conclusion: MARIO挑战赛展示了AI在AMD监测中的潜力，尤其是在进展测量方面，但在预测未来演变方面仍需进一步研究和技术改进。

Abstract: The MARIO challenge, held at MICCAI 2024, focused on advancing the automated
detection and monitoring of age-related macular degeneration (AMD) through the
analysis of optical coherence tomography (OCT) images. Designed to evaluate
algorithmic performance in detecting neovascular activity changes within AMD,
the challenge incorporated unique multi-modal datasets. The primary dataset,
sourced from Brest, France, was used by participating teams to train and test
their models. The final ranking was determined based on performance on this
dataset. An auxiliary dataset from Algeria was used post-challenge to evaluate
population and device shifts from submitted solutions. Two tasks were involved
in the MARIO challenge. The first one was the classification of evolution
between two consecutive 2D OCT B-scans. The second one was the prediction of
future AMD evolution over three months for patients undergoing anti-vascular
endothelial growth factor (VEGF) therapy. Thirty-five teams participated, with
the top 12 finalists presenting their methods. This paper outlines the
challenge's structure, tasks, data characteristics, and winning methodologies,
setting a benchmark for AMD monitoring using OCT, infrared imaging, and
clinical data (such as the number of visits, age, gender, etc.). The results of
this challenge indicate that artificial intelligence (AI) performs as well as a
physician in measuring AMD progression (Task 1) but is not yet able of
predicting future evolution (Task 2).

</details>


### [376] [Smartflow: Enabling Scalable Spatiotemporal Geospatial Research](https://arxiv.org/abs/2506.03022)
*David McVicar,Brian Avant,Adrian Gould,Diego Torrejon,Charles Della Porta,Ryan Mukherjee*

Main category: cs.CV

TL;DR: BlackSky推出Smartflow云框架，支持基于开源工具的可扩展时空地理空间研究，并展示了一个用于监测大型地理区域重型建筑的新神经网络架构。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够处理异构地理空间数据、支持大规模地理空间模型开发和分析的云框架，以满足对大型地理区域和时间尺度的研究需求。

Method: 使用STAC兼容目录作为通用输入，将异构地理空间数据处理为标准数据立方体，结合ClearML、Tensorboard和Apache Superset等工具管理模型实验，并利用Kubernetes实现工作流的编排和执行。

Result: Smartflow框架成功支持了地理空间模型的开发和分析，并展示了一个能够检测重型建筑所有主要发展阶段的新神经网络架构。

Conclusion: Smartflow框架及其集成的神经网络架构为大规模地理空间研究和监测提供了高效、可扩展的解决方案。

Abstract: BlackSky introduces Smartflow, a cloud-based framework enabling scalable
spatiotemporal geospatial research built on open-source tools and technologies.
Using STAC-compliant catalogs as a common input, heterogeneous geospatial data
can be processed into standardized datacubes for analysis and model training.
Model experimentation is managed using a combination of tools, including
ClearML, Tensorboard, and Apache Superset. Underpinning Smartflow is
Kubernetes, which orchestrates the provisioning and execution of workflows to
support both horizontal and vertical scalability. This combination of features
makes Smartflow well-suited for geospatial model development and analysis over
large geographic areas, time scales, and expansive image archives.
  We also present a novel neural architecture, built using Smartflow, to
monitor large geographic areas for heavy construction. Qualitative results
based on data from the IARPA Space-based Machine Automated Recognition
Technique (SMART) program are presented that show the model is capable of
detecting heavy construction throughout all major phases of development.

</details>


### [377] [Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers](https://arxiv.org/abs/2506.03065)
*Pengtao Chen,Xianfang Zeng,Maosen Zhao,Peng Ye,Mingzhu Shen,Wei Cheng,Gang Yu,Tao Chen*

Main category: cs.CV

TL;DR: 论文提出Sparse-vDiT框架，通过识别和利用视频扩散变换器中的稀疏注意力模式，显著提升了长视频生成的推理效率。


<details>
  <summary>Details</summary>
Motivation: 扩散变换器在视频生成中取得突破，但长序列生成任务因注意力机制的二次复杂度导致高推理延迟。研究发现注意力图中存在可预测的稀疏模式，为优化提供了可能。

Method: 1) 为每种稀疏模式设计计算高效的模式优化稀疏核；2) 提出离线稀疏扩散搜索算法，通过硬件感知成本模型为每层和注意力头选择最优稀疏计算策略。

Result: 在CogVideoX1.5、HunyuanVideo和Wan2.1模型上分别实现2.09×、2.38×和1.67×理论FLOP降低，实际推理加速1.76×、1.85×和1.58×，同时保持高视觉保真度（PSNR达24.13-27.09）。

Conclusion: 研究证明vDiT中潜在的结构稀疏性可被系统性地用于长视频合成，为高效视频生成提供了新思路。

Abstract: While Diffusion Transformers (DiTs) have achieved breakthroughs in video
generation, this long sequence generation task remains constrained by the
quadratic complexity of attention mechanisms, resulting in significant
inference latency. Through detailed analysis of attention maps in Video
Diffusion Transformer (vDiT), we identify three recurring sparsity patterns:
diagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\%
attention heads can be skipped. Crucially, these patterns exhibit strong
layer-depth and head-position correlations but show limited dependence on the
input content. Leveraging these findings, we propose Sparse-vDiT, a sparsity
acceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels
that replace dense attention with computationally efficient implementations for
each identified sparsity pattern. 2) An offline sparse diffusion search
algorithm that selects the optimal sparse computation strategy per layer and
head via hardware-aware cost modeling. After determining the optimal
configuration, we fuse heads within the same layer that share the same
attention strategy, enhancing inference efficiency. Integrated into
state-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1),
Sparse-vDiT achieves 2.09$\times$, 2.38$\times$, and 1.67$\times$ theoretical
FLOP reduction, and actual inference speedups of 1.76$\times$, 1.85$\times$,
and 1.58$\times$, respectively, while maintaining high visual fidelity, with
PSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent
structural sparsity in vDiTs can be systematically exploited for long video
synthesis.

</details>


### [378] [FuseLIP: Multimodal Embeddings via Early Fusion of Discrete Tokens](https://arxiv.org/abs/2506.03096)
*Christian Schlarmann,Francesco Croce,Nicolas Flammarion,Matthias Hein*

Main category: cs.CV

TL;DR: FuseLIP提出了一种新型多模态嵌入架构，通过单一Transformer模型处理文本和图像标记，实现早期特征融合，在多模态任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有对比语言-图像预训练方法无法原生处理多模态输入，需额外模块融合特征。本文旨在通过早期融合提升多模态交互和表征能力。

Method: 利用离散图像标记器，构建扩展词汇表的单一Transformer模型，使文本和图像标记在编码各层深度交互（早期融合）。

Result: FuseLIP在VQA和文本引导图像转换检索等任务中超越现有方法，单模态任务性能与基线相当。新数据集验证了模型挑战性任务表现。

Conclusion: 早期融合架构能生成更丰富的多模态表征，为多模态嵌入任务提供了有效解决方案。

Abstract: Contrastive language-image pre-training aligns the features of text-image
pairs in a common latent space via distinct encoders for each modality. While
this approach achieves impressive performance in several zero-shot tasks, it
cannot natively handle multimodal inputs, i.e., encoding image and text into a
single feature vector. As a remedy, it is common practice to use additional
modules to merge the features extracted by the unimodal encoders. In this work,
we present FuseLIP, an alternative architecture for multimodal embedding.
Leveraging recent progress in discrete image tokenizers, we propose to use a
single transformer model which operates on an extended vocabulary of text and
image tokens. This early fusion approach allows the different modalities to
interact at each depth of encoding and obtain richer representations compared
to common late fusion. We collect new datasets for multimodal pre-training and
evaluation, designing challenging tasks for multimodal encoder models. We show
that FuseLIP outperforms other approaches in multimodal embedding tasks such as
VQA and text-guided image transformation retrieval, while being comparable to
baselines on unimodal tasks.

</details>


### [379] [Native-Resolution Image Synthesis](https://arxiv.org/abs/2506.03131)
*Zidong Wang,Lei Bai,Xiangyu Yue,Wanli Ouyang,Yiyuan Zhang*

Main category: cs.CV

TL;DR: 提出原生分辨率图像合成新范式NiT，突破固定分辨率限制，实现任意尺寸/比例图像生成，并在ImageNet基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统生成模型受限于固定分辨率/方形图像格式，无法灵活处理多样化视觉内容。本文旨在建立原生分辨率建模范式，弥合视觉生成与大型语言模型方法论间的鸿沟。

Method: 提出Native-resolution扩散Transformer（NiT），通过可变长度视觉令牌建模，在去噪过程中显式处理不同分辨率/宽高比。

Result: 单个NiT模型同时在ImageNet-256/512基准达到SOTA；仅用ImageNet训练即展现零样本泛化能力，支持1536x1536等新分辨率及16:9等多样比例生成。

Conclusion: 原生分辨率建模展现巨大潜力，其灵活性与LLM的零样本能力相似，为视觉生成领域开辟新方向。

Abstract: We introduce native-resolution image synthesis, a novel generative modeling
paradigm that enables the synthesis of images at arbitrary resolutions and
aspect ratios. This approach overcomes the limitations of conventional
fixed-resolution, square-image methods by natively handling variable-length
visual tokens, a core challenge for traditional techniques. To this end, we
introduce the Native-resolution diffusion Transformer (NiT), an architecture
designed to explicitly model varying resolutions and aspect ratios within its
denoising process. Free from the constraints of fixed formats, NiT learns
intrinsic visual distributions from images spanning a broad range of
resolutions and aspect ratios. Notably, a single NiT model simultaneously
achieves the state-of-the-art performance on both ImageNet-256x256 and 512x512
benchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in
advanced large language models, NiT, trained solely on ImageNet, demonstrates
excellent zero-shot generalization performance. It successfully generates
high-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536)
and diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These
findings indicate the significant potential of native-resolution modeling as a
bridge between visual generative modeling and advanced LLM methodologies.

</details>


### [380] [EgoVLM: Policy Optimization for Egocentric Video Understanding](https://arxiv.org/abs/2506.03097)
*Ashwin Vinod,Shrey Pandit,Aditya Vavre,Linshen Liu*

Main category: cs.CV

TL;DR: EgoVLM是一个专为第一人称视频设计的视觉语言模型，通过GRPO强化学习方法优化，显著提升了在egocentric视频问答任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴相机和自主代理等应用的发展，需要从第一人称视频流中进行鲁棒的推理。

Method: 使用Group Relative Policy Optimization (GRPO)方法对EgoVLM进行微调，无需监督微调阶段，并引入基于关键帧的奖励机制。

Result: EgoVLM-3B在EgoSchema基准测试中表现优于基础模型Qwen2.5-VL 3B和7B，分别提高了14.33和13.87个准确点。

Conclusion: EgoVLM通过生成推理轨迹增强了可解释性，适用于下游应用，其奖励机制为未来研究提供了新方向。

Abstract: Emerging embodied AI applications, such as wearable cameras and autonomous
agents, have underscored the need for robust reasoning from first person video
streams. We introduce EgoVLM, a vision-language model specifically designed to
integrate visual comprehension and spatial-temporal reasoning within egocentric
video contexts. EgoVLM is fine-tuned via Group Relative Policy Optimization
(GRPO), a reinforcement learning method adapted to align model outputs with
human-like reasoning steps. Following DeepSeek R1-Zero's approach, we directly
tune using RL without any supervised fine-tuning phase on chain-of-thought
(CoT) data. We evaluate EgoVLM on egocentric video question answering
benchmarks and show that domain-specific training substantially improves
performance over general-purpose VLMs. Our EgoVLM-3B, trained exclusively on
non-CoT egocentric data, outperforms the base Qwen2.5-VL 3B and 7B models by
14.33 and 13.87 accuracy points on the EgoSchema benchmark, respectively. By
explicitly generating reasoning traces, EgoVLM enhances interpretability,
making it well-suited for downstream applications. Furthermore, we introduce a
novel keyframe-based reward that incorporates salient frame selection to guide
reinforcement learning optimization. This reward formulation opens a promising
avenue for future exploration in temporally grounded egocentric reasoning.

</details>


### [381] [IllumiCraft: Unified Geometry and Illumination Diffusion for Controllable Video Generation](https://arxiv.org/abs/2506.03150)
*Yuanze Lin,Yi-Wen Chen,Yi-Hsuan Tsai,Ronald Clark,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: IllumiCraft提出了一种结合光照、外观和几何线索的扩散框架，用于生成高质量、时间一致的视频。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在生成视频时缺乏对场景光照和视觉外观的显式几何控制。

Method: IllumiCraft整合了HDR视频贴图、随机光照变化的合成帧和3D点轨迹，通过统一的扩散架构生成视频。

Result: 该方法在视频重光照和文本条件视频生成方面表现出更高的保真度。

Conclusion: IllumiCraft通过多线索整合，提升了视频生成的几何和光照控制能力。

Abstract: Although diffusion-based models can generate high-quality and high-resolution
video sequences from textual or image inputs, they lack explicit integration of
geometric cues when controlling scene lighting and visual appearance across
frames. To address this limitation, we propose IllumiCraft, an end-to-end
diffusion framework accepting three complementary inputs: (1)
high-dynamic-range (HDR) video maps for detailed lighting control; (2)
synthetically relit frames with randomized illumination changes (optionally
paired with a static background reference image) to provide appearance cues;
and (3) 3D point tracks that capture precise 3D geometry information. By
integrating the lighting, appearance, and geometry cues within a unified
diffusion architecture, IllumiCraft generates temporally coherent videos
aligned with user-defined prompts. It supports background-conditioned and
text-conditioned video relighting and provides better fidelity than existing
controllable video generation methods. Project Page:
https://yuanze-lin.me/IllumiCraft_page

</details>


### [382] [SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation](https://arxiv.org/abs/2506.03139)
*Siqi Chen,Xinyu Dong,Haolei Xu,Xingyu Wu,Fei Tang,Hang Zhang,Yuchen Yan,Linjuan Wu,Wenqi Zhang,Guiyang Hou,Yongliang Shen,Weiming Lu,Yueting Zhuang*

Main category: cs.CV

TL;DR: SVGenius是一个全面的SVG处理基准测试，包含2377个查询，评估22个主流模型在理解、编辑和生成三个维度上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有SVG处理基准测试覆盖范围有限、复杂度分层不足且评估范式碎片化，需要更全面的评估框架。

Method: 基于24个应用领域的真实数据，构建包含8个任务类别和18个指标的SVGenius基准测试，评估不同规模、架构和训练范式的模型。

Result: 专有模型显著优于开源模型，但所有模型随复杂度增加性能下降；推理增强训练比单纯扩展规模更有效，风格迁移仍是最大挑战。

Conclusion: SVGenius为SVG处理建立了首个系统评估框架，为开发更强大的矢量图形模型和推进自动化图形设计应用提供了关键见解。

Abstract: Large Language Models (LLMs) and Multimodal LLMs have shown promising
capabilities for SVG processing, yet existing benchmarks suffer from limited
real-world coverage, lack of complexity stratification, and fragmented
evaluation paradigms. We introduce SVGenius, a comprehensive benchmark
comprising 2,377 queries across three progressive dimensions: understanding,
editing, and generation. Built on real-world data from 24 application domains
with systematic complexity stratification, SVGenius evaluates models through 8
task categories and 18 metrics. We assess 22 mainstream models spanning
different scales, architectures, training paradigms, and accessibility levels.
Our analysis reveals that while proprietary models significantly outperform
open-source counterparts, all models exhibit systematic performance degradation
with increasing complexity, indicating fundamental limitations in current
approaches; however, reasoning-enhanced training proves more effective than
pure scaling for overcoming these limitations, though style transfer remains
the most challenging capability across all model types. SVGenius establishes
the first systematic evaluation framework for SVG processing, providing crucial
insights for developing more capable vector graphics models and advancing
automated graphic design applications. Appendix and supplementary materials
(including all data and code) are available at
https://zju-real.github.io/SVGenius.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [383] [An Exploratory Framework for Future SETI Applications: Detecting Generative Reactivity via Language Models](https://arxiv.org/abs/2506.02730)
*Po-Chieh Yu*

Main category: astro-ph.IM

TL;DR: 该研究探索了噪声输入是否能引发语言模型的结构化响应，发现鲸鱼和鸟类的叫声比白噪声更能诱导出语义结构，为SETI提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证非符号化噪声输入是否能触发语言模型的语义响应，挑战传统解码思维，探索数据潜在结构的检测方法。

Method: 使用GPT-2模型测试四类声学输入（人声、鲸歌、鸟鸣、白噪声），通过综合指标SIP（熵/句法连贯性/压缩增益/重复惩罚）量化语义诱导能力。

Result: 鲸鱼和鸟类叫声的SIP得分显著高于白噪声，而人声仅引发中等响应，表明语言模型可识别非常规语义数据中的潜在结构。

Conclusion: 生成式模型的反应性可作为传统SETI的补充手段，在未知通信意图的场景中帮助筛选有价值的数据。

Abstract: We present an exploratory framework to test whether noise-like input can
induce structured responses in language models. Instead of assuming that
extraterrestrial signals must be decoded, we evaluate whether inputs can
trigger linguistic behavior in generative systems. This shifts the focus from
decoding to viewing structured output as a sign of underlying regularity in the
input. We tested GPT-2 small, a 117M-parameter model trained on English text,
using four types of acoustic input: human speech, humpback whale vocalizations,
Phylloscopus trochilus birdsong, and algorithmically generated white noise. All
inputs were treated as noise-like, without any assumed symbolic encoding. To
assess reactivity, we defined a composite score called Semantic Induction
Potential (SIP), combining entropy, syntax coherence, compression gain, and
repetition penalty. Results showed that whale and bird vocalizations had higher
SIP scores than white noise, while human speech triggered only moderate
responses. This suggests that language models may detect latent structure even
in data without conventional semantics. We propose that this approach could
complement traditional SETI methods, especially in cases where communicative
intent is unknown. Generative reactivity may offer a different way to identify
data worth closer attention.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [384] [A Brain Graph Foundation Model: Pre-Training and Prompt-Tuning for Any Atlas and Disorder](https://arxiv.org/abs/2506.02044)
*Xinxu Wei,Kanhao Zhao,Yong Jiao,Lifang He,Yu Zhang*

Main category: q-bio.NC

TL;DR: 论文提出了一种基于图预训练的新型脑图基础模型BrainGFM，利用图对比学习和图掩码自编码器进行大规模fMRI预训练，支持跨异质脑图谱和疾病的高效迁移学习。


<details>
  <summary>Details</summary>
Motivation: 当前脑基础模型多基于时间序列或ROI特征预训练，本文旨在通过图预训练范式构建更通用的脑图基础模型，以推动神经科学发展。

Method: 提出BrainGFM框架：1) 融合图对比学习和图掩码自编码器；2) 整合图提示和语言提示实现灵活迁移；3) 使用元学习优化图提示，支持少样本/零样本学习。

Result: 模型在25种神经/精神疾病、8种分区方案、25,000+受试者的27个数据集上预训练，涵盖60,000+fMRI扫描和400,000+图样本。代码已开源。

Conclusion: BrainGFM通过创新的图预训练范式，建立了跨异构脑图谱和疾病的可迁移基础模型，为神经科学研究提供了通用框架。

Abstract: As large language models (LLMs) continue to revolutionize AI research, there
is a growing interest in building large-scale brain foundation models to
advance neuroscience. While most existing brain foundation models are
pre-trained on time-series signals or region-of-interest (ROI) features, we
propose a novel graph-based pre-training paradigm for constructing a brain
graph foundation model. In this paper, we introduce the Brain Graph Foundation
Model, termed BrainGFM, a unified framework that leverages graph contrastive
learning and graph masked autoencoders for large-scale fMRI-based pre-training.
BrainGFM is pre-trained on a diverse mixture of brain atlases with varying
parcellations, significantly expanding the pre-training corpus and enhancing
the model's ability to generalize across heterogeneous fMRI-derived brain
representations. To support efficient and versatile downstream transfer, we
integrate both graph prompts and language prompts into the model design,
enabling BrainGFM to flexibly adapt to a wide range of atlases, neurological
and psychiatric disorders, and task settings. Furthermore, we employ
meta-learning to optimize the graph prompts, facilitating strong generalization
to previously unseen disorders under both few-shot and zero-shot learning
conditions via language-guided prompting. BrainGFM is pre-trained on 27
neuroimaging datasets spanning 25 common neurological and psychiatric
disorders, encompassing 2 types of brain atlases (functional and anatomical)
across 8 widely-used parcellations, and covering over 25,000 subjects, 60,000
fMRI scans, and a total of 400,000 graph samples aggregated across all atlases
and parcellations. The code is available at:
https://github.com/weixinxu666/BrainGFM

</details>


### [385] [Modelling the Effects of Hearing Loss on Neural Coding in the Auditory Midbrain with Variational Conditioning](https://arxiv.org/abs/2506.03088)
*Lloyd Pellatt,Fotios Drakopoulos,Shievanie Sabesan,Nicholas A. Lesica*

Main category: q-bio.NC

TL;DR: 该论文提出了一种新的变分条件模型，用于直接从健康动物和噪声暴露动物的听觉中脑神经活动中学习编码听力损失空间，仅需6个自由参数即可准确预测神经响应，为听力损失补偿模型的开发奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 听觉从声音到神经活动的映射高度非线性，且中枢听觉处理复杂，难以手动建模。现有模型假设所有大脑的听觉处理相同，无法捕捉听力损失的广泛变化效应。因此，需要一种能够直接学习听力损失空间的方法。

Method: 提出了一种变分条件模型，直接从健康动物和噪声暴露动物的听觉中脑神经活动中学习编码听力损失空间，仅需6个自由参数，并使用贝叶斯优化拟合。

Result: 模型在健康动物中准确预测了62%的可解释神经响应方差，在听力受损动物中为68%，接近最先进的动物特定模型。通过贝叶斯优化，仅需15-30次迭代即可达到接近最优的交叉熵损失。

Conclusion: 该模型为开发参数化的听力损失补偿模型奠定了基础，可通过人类参与的循环优化快速为新用户拟合，直接恢复听力受损大脑中的正常神经编码。

Abstract: The mapping from sound to neural activity that underlies hearing is highly
non-linear. The first few stages of this mapping in the cochlea have been
modelled successfully, with biophysical models built by hand and, more
recently, with DNN models trained on datasets simulated by biophysical models.
Modelling the auditory brain has been a challenge because central auditory
processing is too complex for models to be built by hand, and datasets for
training DNN models directly have not been available. Recent work has taken
advantage of large-scale high resolution neural recordings from the auditory
midbrain to build a DNN model of normal hearing with great success. But this
model assumes that auditory processing is the same in all brains, and therefore
it cannot capture the widely varying effects of hearing loss.
  We propose a novel variational-conditional model to learn to encode the space
of hearing loss directly from recordings of neural activity in the auditory
midbrain of healthy and noise exposed animals. With hearing loss parametrised
by only 6 free parameters per animal, our model accurately predicts 62\% of the
explainable variance in neural responses from normal hearing animals and 68%
for hearing impaired animals, within a few percentage points of state of the
art animal specific models. We demonstrate that the model can be used to
simulate realistic activity from out of sample animals by fitting only the
learned conditioning parameters with Bayesian optimisation, achieving
crossentropy loss within 2% of the optimum in 15-30 iterations. Including more
animals in the training data slightly improved the performance on unseen
animals. This model will enable future development of parametrised hearing loss
compensation models trained to directly restore normal neural coding in hearing
impaired brains, which can be quickly fitted for a new user by human in the
loop optimisation.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [386] [Surgical Foundation Model Leveraging Compression and Entropy Maximization for Image-Guided Surgical Assistance](https://arxiv.org/abs/2506.01980)
*Lianhao Yin,Ozanan Meireles,Guy Rosman,Daniela Rus*

Main category: eess.IV

TL;DR: 提出了一种名为C2E的自监督学习框架，利用Kolmogorov复杂度从手术视频中学习紧凑且信息丰富的表示，无需标注数据即可提升编码器性能，并在多种手术ML任务中表现出优异的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 微创手术（MIS）中实时视频理解至关重要，但监督学习方法需要大量标注数据，而医学领域的数据标注成本高昂且稀缺。现有自监督方法难以捕捉通用任务的结构和物理信息。

Method: C2E框架通过熵最大化解码器压缩图像，同时保留临床相关细节，利用Kolmogorov复杂度学习紧凑表示，无需标注数据即可优化编码器性能。

Result: C2E在大规模未标注手术数据集上训练后，在手术工作流分类、工具-组织交互分类、分割和诊断等任务中表现出强大的泛化能力，性能优于现有方法。

Conclusion: C2E展示了自监督学习在提升手术AI性能方面的潜力，其紧凑表示能更好解耦图像结构特征，为MIS的临床效果改进提供了新思路。

Abstract: Real-time video understanding is critical to guide procedures in minimally
invasive surgery (MIS). However, supervised learning approaches require large,
annotated datasets that are scarce due to annotation efforts that are
prohibitive, e.g., in medical fields. Although self-supervision methods can
address such limitations, current self-supervised methods often fail to capture
structural and physical information in a form that generalizes across tasks. We
propose Compress-to-Explore (C2E), a novel self-supervised framework that
leverages Kolmogorov complexity to learn compact, informative representations
from surgical videos. C2E uses entropy-maximizing decoders to compress images
while preserving clinically relevant details, improving encoder performance
without labeled data. Trained on large-scale unlabeled surgical datasets, C2E
demonstrates strong generalization across a variety of surgical ML tasks, such
as workflow classification, tool-tissue interaction classification,
segmentation, and diagnosis tasks, providing improved performance as a surgical
visual foundation model. As we further show in the paper, the model's internal
compact representation better disentangles features from different structural
parts of images. The resulting performance improvements highlight the yet
untapped potential of self-supervised learning to enhance surgical AI and
improve outcomes in MIS.

</details>


### [387] [Tomographic Foundation Model -- FORCE: Flow-Oriented Reconstruction Conditioning Engine](https://arxiv.org/abs/2506.02149)
*Wenjun Xia,Chuang Niu,Ge Wang*

Main category: eess.IV

TL;DR: 该论文提出了一种名为FORCE的新型CT重建框架，结合了数据保真度和先进的生成AI模型PFGM++，在多种CT成像任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 临床CT场景中，如低剂量扫描、稀疏视图扫描和金属植入物等，常导致重建图像中出现严重噪声和伪影，需要改进重建技术。深度学习虽能提升CT图像重建，但配对训练数据获取困难，且存在幻觉风险。

Method: 论文将数据保真度与先进的生成AI模型PFGM++结合，提出了Flow-Oriented Reconstruction Conditioning Engine (FORCE)框架。

Result: 实验表明，FORCE在多种CT成像任务中表现优于现有的无监督重建方法。

Conclusion: FORCE框架通过结合数据保真度和生成模型，显著提升了CT图像重建的质量和稳定性。

Abstract: Computed tomography (CT) is a major medical imaging modality. Clinical CT
scenarios, such as low-dose screening, sparse-view scanning, and metal
implants, often lead to severe noise and artifacts in reconstructed images,
requiring improved reconstruction techniques. The introduction of deep learning
has significantly advanced CT image reconstruction. However, obtaining paired
training data remains rather challenging due to patient motion and other
constraints. Although deep learning methods can still perform well with
approximately paired data, they inherently carry the risk of hallucination due
to data inconsistencies and model instability. In this paper, we integrate the
data fidelity with the state-of-the-art generative AI model, referred to as the
Poisson flow generative model (PFGM) with a generalized version PFGM++, and
propose a novel CT framework: Flow-Oriented Reconstruction Conditioning Engine
(FORCE). In our experiments, the proposed method shows superior performance in
various CT imaging tasks, outperforming existing unsupervised reconstruction
approaches.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [388] [MAEBE: Multi-Agent Emergent Behavior Framework](https://arxiv.org/abs/2506.03053)
*Sinem Erisken,Timothy Gothard,Martin Leitgab,Ram Potham*

Main category: cs.MA

TL;DR: 论文提出MAEBE框架评估多智能体AI的新兴风险，发现LLM道德偏好脆弱且受群体动态影响。


<details>
  <summary>Details</summary>
Motivation: 传统孤立LLM的安全评估不足以应对多智能体AI带来的新兴风险，需系统性评估方法。

Method: 使用MAEBE框架和Greatest Good Benchmark，结合双反转问题技术分析单体和群体LLM行为。

Result: 发现LLM道德偏好易受问题框架影响，群体动态导致不可预测的道德推理，存在同伴压力等安全挑战。

Conclusion: 必须在多智能体交互环境中评估AI系统，以应对其特有的安全和对齐问题。

Abstract: Traditional AI safety evaluations on isolated LLMs are insufficient as
multi-agent AI ensembles become prevalent, introducing novel emergent risks.
This paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE)
framework to systematically assess such risks. Using MAEBE with the Greatest
Good Benchmark (and a novel double-inversion question technique), we
demonstrate that: (1) LLM moral preferences, particularly for Instrumental
Harm, are surprisingly brittle and shift significantly with question framing,
both in single agents and ensembles. (2) The moral reasoning of LLM ensembles
is not directly predictable from isolated agent behavior due to emergent group
dynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure
influencing convergence, even when guided by a supervisor, highlighting
distinct safety and alignment challenges. Our findings underscore the necessity
of evaluating AI systems in their interactive, multi-agent contexts.

</details>


### [389] [ThinkTank: A Framework for Generalizing Domain-Specific AI Agent Systems into Universal Collaborative Intelligence Platforms](https://arxiv.org/abs/2506.02931)
*Praneet Sai Madhu Surabhi,Dheeraj Reddy Mudireddy,Jian Tao*

Main category: cs.MA

TL;DR: ThinkTank是一个将专业AI代理系统转化为多功能协作智能平台的框架，支持跨领域复杂问题解决，注重数据隐私和本地部署。


<details>
  <summary>Details</summary>
Motivation: 旨在解决专业AI系统在跨领域协作和知识共享方面的局限性，同时确保数据隐私和安全。

Method: 通过角色抽象、会议类型泛化和检索增强生成技术，结合本地部署框架如Ollama和Llama3.1模型。

Result: ThinkTank在成本效益、数据安全、可扩展性和竞争优势方面优于基于云的替代方案。

Conclusion: ThinkTank作为一个通用平台，为AI驱动的协作问题解决提供了高效、安全的解决方案。

Abstract: This paper presents ThinkTank, a comprehensive and scalable framework
designed to transform specialized AI agent systems into versatile collaborative
intelligence platforms capable of supporting complex problem-solving across
diverse domains. ThinkTank systematically generalizes agent roles, meeting
structures, and knowledge integration mechanisms by adapting proven scientific
collaboration methodologies. Through role abstraction, generalization of
meeting types for iterative collaboration, and the integration of
Retrieval-Augmented Generation with advanced knowledge storage, the framework
facilitates expertise creation and robust knowledge sharing. ThinkTank enables
organizations to leverage collaborative AI for knowledge-intensive tasks while
ensuring data privacy and security through local deployment, utilizing
frameworks like Ollama with models such as Llama3.1. The ThinkTank framework is
designed to deliver significant advantages in cost-effectiveness, data
security, scalability, and competitive positioning compared to cloud-based
alternatives, establishing it as a universal platform for AI-driven
collaborative problem-solving. The ThinkTank code is available at
https://github.com/taugroup/ThinkTank

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [390] [A Learned Cost Model-based Cross-engine Optimizer for SQL Workloads](https://arxiv.org/abs/2506.02802)
*András Strausz,Niels Pardon,Ioana Giurgiu*

Main category: cs.DB

TL;DR: 提出了一种跨引擎优化器，通过学习的成本模型自动选择最适合执行SQL查询的引擎，显著提高了查询效率和准确性。


<details>
  <summary>Details</summary>
Motivation: Lakehouse系统中，选择最适合执行SQL查询的引擎需要先验知识和手动操作，随着新引擎和工作负载的出现，这一任务变得愈发复杂。

Method: 采用多任务学习框架，通过优化的查询计划进行成本预测和路由，支持灵活添加新引擎且只需最小微调成本。

Result: 在零样本和少量样本设置下，跨引擎优化器分别减少了25.2%和30.4%的总工作负载运行时间，查询成本估计的平均Q误差降低了12.6%。

Conclusion: 该跨引擎优化器有效解决了引擎选择的复杂性问题，显著提升了查询性能，且易于扩展新引擎。

Abstract: Lakehouse systems enable the same data to be queried with multiple execution
engines. However, selecting the engine best suited to run a SQL query still
requires a priori knowledge of the query computational requirements and an
engine capability, a complex and manual task that only becomes more difficult
with the emergence of new engines and workloads. In this paper, we address this
limitation by proposing a cross-engine optimizer that can automate engine
selection for diverse SQL queries through a learned cost model. Optimized with
hints, a query plan is used for query cost prediction and routing. Cost
prediction is formulated as a multi-task learning problem, and multiple
predictor heads, corresponding to different engines and provisionings, are used
in the model architecture. This eliminates the need to train engine-specific
models and allows the flexible addition of new engines at a minimal fine-tuning
cost. Results on various databases and engines show that using a query
optimized logical plan for cost estimation decreases the average Q-error by
even 12.6% over using unoptimized plans as input. Moreover, the proposed
cross-engine optimizer reduces the total workload runtime by up to 25.2% in a
zero-shot setting and 30.4% in a few-shot setting when compared to random
routing.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [391] [FlashMLA-ETAP: Efficient Transpose Attention Pipeline for Accelerating MLA Inference on NVIDIA H20 GPUs](https://arxiv.org/abs/2506.01969)
*Pencuo Zeren,Qiuming Luo,Rui Mao,Chang Kong*

Main category: cs.DC

TL;DR: FlashMLA-ETAP提出了一种高效的多头潜在注意力推理框架，通过ETAP技术优化计算，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 在单台多GPU服务器上部署DeepSeek-R1 671B模型时，多头潜在注意力的高效推理面临挑战。本文旨在解决资源受限环境下的推理问题。

Method: 提出了ETAP（高效转置注意力流水线），通过重新配置注意力计算，使KV上下文长度与WGMMA操作的M维度对齐，减少冗余计算。

Result: 在64K序列长度（批量大小16）下，FlashMLA-ETAP比FlashMLA快2.78倍，比FlashAttention-3和FlashInfer分别快5.24倍和4.94倍，同时保持数值稳定性。

Conclusion: FlashMLA-ETAP为资源受限的推理提供了可扩展的解决方案，推动了硬件感知优化的广泛应用。

Abstract: Efficient inference of Multi-Head Latent Attention (MLA) is challenged by
deploying the DeepSeek-R1 671B model on a single Multi-GPU server. This paper
introduces FlashMLA-ETAP, a novel framework that enhances MLA inference for the
single-instance deployment scenario on NVIDIA H20 GPUs. We propose the
Efficient Transpose Attention Pipeline (ETAP), which reconfigures attention
computation through transposition to align the KV context length with the
\(M\)-dimension in WGMMA operations, significantly reducing redundant
computations. FlashMLA-ETAP achieves a 2.78x speedup over FlashMLA at 64K
sequence length (batch size 16), with 5.24x and 4.94x improvements over
FlashAttention-3 and FlashInfer, respectively, while maintaining numerical
stability with a 15.2x lower RMSE (\(1.25 \times 10^{-5}\)) than
FlashAttention-3. Furthermore, ETAP's design enables seamless integration into
frameworks like FlashAttention-3 and FlashInfer, supported by a detailed
theoretical analysis. Our work addresses a critical gap in resource-constrained
inference, offering a scalable solution for mid-tier GPUs and paving the way
for broader adoption in hardware-aware optimization. Code is available at
https://github.com/pengcuo/FlashMLA-ETAP.

</details>


### [392] [Speculative Decoding via Hybrid Drafting and Rollback-Aware Branch Parallelism](https://arxiv.org/abs/2506.01979)
*Yuhao Shen,Junyi Shen,Quan Kong,Tianyu Liu,Yao Lu,Cong Wang*

Main category: cs.DC

TL;DR: 提出SpecBranch框架，通过分支并行化加速LLM推理，减少回滚令牌并提升速度。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法因串行执行导致模型间相互等待，限制了效率提升。

Method: 引入并行推测分支预判拒绝，结合隐式置信度与显式特征复用动态调整草案长度。

Result: 实验显示速度提升1.8~4.5倍，对齐不良模型回滚令牌减少50%。

Conclusion: SpecBranch通过分支并行化实现实际部署适用性，显著优化推理效率。

Abstract: Recently, speculative decoding (SD) has emerged as a promising technique to
accelerate LLM inference by employing a small draft model to propose draft
tokens in advance, and validating them in parallel with the large target model.
However, the existing SD methods still remain fundamentally constrained by
their serialized execution, which causes the mutual waiting bubbles between the
draft and target models. To address this challenge, we draw inspiration from
branch prediction in modern processors and propose a novel framework
\textbf{SpecBranch} to unlock branch parallelism in SD. Specifically, we first
take an in-depth analysis of the potential of branch parallelism in SD, and
recognize that the key challenge lies in the trade-offs between parallelization
and token rollback. Based on the analysis, we strategically introduce parallel
speculative branches to preemptively hedge against likely rejections.
Meanwhile, to enhance parallelism, we jointly orchestrate adaptive draft
lengths with a hybrid combination of the implicit draft model confidence and
explicit reusing of target model features. Extensive experiments across various
models and benchmarks show that SpecBranch achieves over \textbf{1.8}$\times
\sim$ \textbf{4.5}$\times$ speedups against the auto-regressive decoding and
reduces rollback tokens by $\textbf{50}$\% for poorly aligned models, realizing
its applicability for real-world deployments.

</details>


### [393] [eACGM: Non-instrumented Performance Tracing and Anomaly Detection towards Machine Learning Systems](https://arxiv.org/abs/2506.02007)
*Ruilin Xu,Zongxuan Xie,Pengfei Chen*

Main category: cs.DC

TL;DR: eACGM是一个基于eBPF的全栈AI/ML系统监控框架，无需代码插桩即可实时收集硬件和软件性能数据，通过高斯混合模型分析异常，适用于大规模分布式训练场景。


<details>
  <summary>Details</summary>
Motivation: 为了解决大规模AI/ML系统中复杂的性能异常和故障诊断问题，作者提出了eACGM框架，旨在提供非侵入式、低开销的实时监控方案。

Method: eACGM利用eBPF技术实时采集GPU、网络等硬件及CUDA、PyTorch等软件的性能数据，结合libnvml获取进程级GPU资源使用信息，并应用高斯混合模型进行统计建模和聚类分析。

Result: 实验表明，eACGM在多节点分布式训练场景中能有效捕捉关键性能异常，且保持低开销，验证了其在生产环境中的适用性和可扩展性。

Conclusion: eACGM为大规模AI/ML系统提供了稳定的异常检测和全面监控能力，显著支持性能优化和故障诊断。

Abstract: We present eACGM, a full-stack AI/ML system monitoring framework based on
eBPF. eACGM collects real-time performance data from key hardware components,
including the GPU and network communication layer, as well as from key software
stacks such as CUDA, Python, and PyTorch, all without requiring any code
instrumentation or modifications. Additionally, it leverages libnvml to gather
process-level GPU resource usage information. By applying a Gaussian Mixture
Model (GMM) to the collected multidimensional performance metrics for
statistical modeling and clustering analysis, eACGM effectively identifies
complex failure modes, such as latency anomalies, hardware failures, and
communication inefficiencies, enabling rapid diagnosis of system bottlenecks
and abnormal behaviors.
  To evaluate eACGM's effectiveness and practicality, we conducted extensive
empirical studies and case analyses in multi-node distributed training
scenarios. The results demonstrate that eACGM, while maintaining a
non-intrusive and low-overhead profile, successfully captures critical
performance anomalies during model training and inference. Its stable anomaly
detection performance and comprehensive monitoring capabilities validate its
applicability and scalability in real-world production environments, providing
strong support for performance optimization and fault diagnosis in large-scale
AI/ML systems.

</details>


### [394] [Evaluating the Efficacy of LLM-Based Reasoning for Multiobjective HPC Job Scheduling](https://arxiv.org/abs/2506.02025)
*Prachi Jadhav,Hongwei Jin,Ewa Deelman,Prasanna Balaprakash*

Main category: cs.DC

TL;DR: 该论文提出了一种基于大型语言模型（LLM）的高性能计算（HPC）作业调度方法，通过ReAct框架实现迭代式、可解释的决策，并在多种实际工作负载场景中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统HPC作业调度方法（如先到先服务或优化技术）难以适应动态工作负载和异构系统，因此需要一种更灵活、透明的调度方案。

Method: 采用LLM结合ReAct框架（推理+行动），利用自然语言反馈和历史记录优化决策，并通过约束执行模块确保可行性。

Result: 在七种实际HPC工作负载场景中，该方法在平衡多目标（如最小化完成时间、公平性）和约束满足方面优于传统方法（如FCFS、OR-Tools）。

Conclusion: LLM调度器在多目标优化和适应性上表现优异，但计算开销与实时部署之间存在权衡，为动态HPC环境中的复杂调度问题提供了新思路。

Abstract: High-Performance Computing (HPC) job scheduling involves balancing
conflicting objectives such as minimizing makespan, reducing wait times,
optimizing resource use, and ensuring fairness. Traditional methods, including
heuristic-based (e.g., First-Come-First-Served) or intensive optimization
techniques, often lack adaptability to dynamic workloads and heterogeneous HPC
systems. To address this, we propose a novel Large Language Model (LLM)-based
scheduler using a ReAct-style framework (Reason + Act), enabling iterative,
interpretable decision-making. The system incorporates a scratchpad memory to
track scheduling history and refine decisions via natural language feedback,
while a constraint enforcement module ensures feasibility and safety. We
evaluate our approach using OpenAI's O4-Mini and Anthropic's Claude 3.7 across
seven real-world HPC workload scenarios, including heterogeneous mixes, bursty
patterns, and adversarial cases. Comparisons against FCFS, Shortest Job First,
and Google OR-Tools (on 10 to 100 jobs) reveal that LLM-based scheduling
effectively balances multiple objectives while offering transparent reasoning
through natural language traces. The method excels in constraint satisfaction
and adapts to diverse workloads without domain-specific training. However, a
trade-off between reasoning quality and computational overhead challenges
real-time deployment. This work presents the first comprehensive study of
reasoning-capable LLMs for HPC scheduling, demonstrating their potential to
handle multiobjective optimization while highlighting limitations in
computational efficiency. The findings provide insights into leveraging
advanced language models for complex scheduling problems in dynamic HPC
environments.

</details>


### [395] [EvoGit: Decentralized Code Evolution via Git-Based Multi-Agent Collaboration](https://arxiv.org/abs/2506.02049)
*Beichen Huang,Ran Cheng,Kay Chen Tan*

Main category: cs.DC

TL;DR: EvoGit是一个去中心化的多智能体框架，通过自主代码进化实现协作软件开发，无需集中协调或显式通信，仅通过Git版本图谱实现异步协作。


<details>
  <summary>Details</summary>
Motivation: 旨在探索去中心化、自动化的持续软件开发新范式，减少人工干预，同时保持代码演进的可追溯性和模块化。

Method: 部署独立编码智能体群，基于Git版本图谱实现异步读写，人类仅提供高层目标指导和轻量级反馈。

Result: 在两个真实任务中成功生成功能性和模块化软件：1）用现代框架构建Web应用；2）开发能自我进化求解装箱问题的元系统。

Conclusion: EvoGit为去中心化自动化软件开发提供了可行方案，其基于版本图谱的协作机制具有扩展性和实践潜力。

Abstract: We introduce EvoGit, a decentralized multi-agent framework for collaborative
software development driven by autonomous code evolution. EvoGit deploys a
population of independent coding agents, each proposing edits to a shared
codebase without centralized coordination, explicit message passing, or shared
memory. Instead, all coordination emerges through a Git-based phylogenetic
graph that tracks the full version lineage and enables agents to asynchronously
read from and write to the evolving code repository. This graph-based structure
supports fine-grained branching, implicit concurrency, and scalable agent
interaction while preserving a consistent historical record. Human involvement
is minimal but strategic: users define high-level goals, periodically review
the graph, and provide lightweight feedback to promote promising directions or
prune unproductive ones. Experiments demonstrate EvoGit's ability to
autonomously produce functional and modular software artifacts across two
real-world tasks: (1) building a web application from scratch using modern
frameworks, and (2) constructing a meta-level system that evolves its own
language-model-guided solver for the bin-packing optimization problem. Our
results underscore EvoGit's potential to establish a new paradigm for
decentralized, automated, and continual software development. EvoGit is
open-sourced at https://github.com/BillHuang2001/evogit.

</details>


### [396] [Machine Learning for Consistency Violation Faults Analysis](https://arxiv.org/abs/2506.02002)
*Kamal Giri,Amit Garu*

Main category: cs.DC

TL;DR: 该研究提出了一种基于机器学习的方法，用于分析分布式系统中一致性违规故障（CVFs）的影响，并通过Dijkstra的Token Ring问题进行案例研究。


<details>
  <summary>Details</summary>
Motivation: 分布式系统中经常遇到一致性违规故障（CVFs），这些故障会导致节点基于过时或不准确的数据操作，从而影响系统的收敛和整体性能。

Method: 研究采用机器学习方法，计算程序转换排名及其相应效果，量化CVFs对系统行为的影响。为了解决大型图中的状态空间爆炸问题，实现了前馈神经网络（FNN）和分布式神经网络两种模型。

Result: 实验结果显示，该方法在小规模图（3到10个节点）上表现良好，测试损失为4.39，平均绝对误差为1.5。尽管在CPU上的分布式训练未显著提升速度，但结果表明通过使用GPU或TPU等硬件加速器可以提升可扩展性。

Conclusion: 该研究提出的机器学习方法能够有效量化CVFs对分布式系统的影响，未来通过硬件加速器可进一步提升性能。

Abstract: Distributed systems frequently encounter consistency violation faults (cvfs),
where nodes operate on outdated or inaccurate data, adversely affecting
convergence and overall system performance. This study presents a machine
learning-based approach for analyzing the impact of CVFs, using Dijkstra's
Token Ring problem as a case study. By computing program transition ranks and
their corresponding effects, the proposed method quantifies the influence of
cvfs on system behavior. To address the state space explosion encountered in
larger graphs, two models are implemented: a Feedforward Neural Network (FNN)
and a distributed neural network leveraging TensorFlow's \texttt{tf.distribute}
API. These models are trained on datasets generated from smaller graphs (3 to
10 nodes) to predict parameters essential for determining rank effects.
Experimental results demonstrate promising performance, with a test loss of
4.39 and a mean absolute error of 1.5. Although distributed training on a CPU
did not yield significant speed improvements over a single-device setup, the
findings suggest that scalability could be enhanced through the use of advanced
hardware accelerators such as GPUs or TPUs.

</details>


### [397] [Efficient and Workload-Aware LLM Serving via Runtime Layer Swapping and KV Cache Resizing](https://arxiv.org/abs/2506.02006)
*Zhaoyuan Su,Tingfeng Lan,Zirui Wang,Juncheng Yang,Yue Cheng*

Main category: cs.DC

TL;DR: MorphServe提出动态负载感知的LLM服务框架，通过量化层替换和KV缓存调整，显著降低延迟并保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务框架和静态模型压缩技术无法适应动态负载波动，导致服务延迟或精度下降。

Method: 采用异步token级运行时机制：量化层替换和压力感知KV缓存动态调整。

Result: 实验显示，MorphServe将SLO违规降低92.45%，P95 TTFT延迟提升2.2-3.9倍，且不损害生成质量。

Conclusion: MorphServe为动态环境中的LLM部署提供了实用且灵活的解决方案。

Abstract: Efficiently serving large language models (LLMs) under dynamic and bursty
workloads remains a key challenge for real-world deployment. Existing serving
frameworks and static model compression techniques fail to adapt to workload
fluctuations, leading to either service-level objective (SLO) violations under
full-precision serving or persistent accuracy degradation with static
quantization. We present MorphServe, a dynamic, workload-aware LLM serving
framework based on morphological adaptation. MorphServe introduces two
asynchronous, token-level runtime mechanisms: quantized layer swapping, which
selectively replaces less impactful layers with quantized alternatives during
high-load periods, and pressure-aware KV cache resizing, which dynamically
adjusts KV cache capacity in response to memory pressure. These mechanisms
enable state-preserving transitions with minimum runtime overhead and are fully
compatible with modern scheduling and attention techniques. Extensive
experiments on Vicuna and Llama family models with real-world workloads
demonstrate that MorphServe reduces average SLO violations by 92.45 percent and
improves the P95 TTFT latency by 2.2x-3.9x compared to full-precision serving,
without compromising generation quality. These results establish MorphServe as
a practical and elastic solution for LLM deployment in dynamic environments.

</details>


### [398] [DistMLIP: A Distributed Inference Platform for Machine Learning Interatomic Potentials](https://arxiv.org/abs/2506.02023)
*Kevin Han,Bowen Deng,Amir Barati Farimani,Gerbrand Ceder*

Main category: cs.DC

TL;DR: 论文提出DistMLIP平台，通过图分区并行化机器学习原子间势能（MLIPs）计算，实现高效多设备推理，支持百万原子级模拟。


<details>
  <summary>Details</summary>
Motivation: 大规模原子模拟对材料与药物发现至关重要，但传统量子力学计算难以扩展。MLIPs的发展提供了解决方案，但多设备并行化仍具挑战。

Method: 提出DistMLIP平台，采用零冗余图级并行化方法，通过图分区而非传统空间分区实现MLIPs的高效并行计算，支持多层图神经网络等灵活架构。

Result: 在8个GPU上，DistMLIP使CHGNet等四种先进MLIPs实现百万原子级计算，耗时仅数秒，显著提升计算规模与效率。

Conclusion: DistMLIP为MLIPs提供了一种易用、灵活的分布式推理方案，成功扩展了原子模拟的规模，适用于实际应用场景。

Abstract: Large-scale atomistic simulations are essential to bridge computational
materials and chemistry to realistic materials and drug discovery applications.
In the past few years, rapid developments of machine learning interatomic
potentials (MLIPs) have offered a solution to scale up quantum mechanical
calculations. Parallelizing these interatomic potentials across multiple
devices poses a challenging, but promising approach to further extending
simulation scales to real-world applications. In this work, we present
DistMLIP, an efficient distributed inference platform for MLIPs based on
zero-redundancy, graph-level parallelization. In contrast to conventional
space-partitioning parallelization, DistMLIP enables efficient MLIP
parallelization through graph partitioning, allowing multi-device inference on
flexible MLIP model architectures like multi-layer graph neural networks.
DistMLIP presents an easy-to-use, flexible, plug-in interface that enables
distributed inference of pre-existing MLIPs. We demonstrate DistMLIP on four
widely used and state-of-the-art MLIPs: CHGNet, MACE, TensorNet, and eSEN. We
show that existing foundational potentials can perform near-million-atom
calculations at the scale of a few seconds on 8 GPUs with DistMLIP.

</details>


### [399] [Simplifying Root Cause Analysis in Kubernetes with StateGraph and LLM](https://arxiv.org/abs/2506.02490)
*Yong Xiang,Charley Peter Chen,Liyi Zeng,Wei Yin,Xin Liu,Hu Li,Wei Xu*

Main category: cs.DC

TL;DR: Kubernetes集群状态一致性面临挑战，SynergyRCA利用LLM和图数据库提升根因分析效率与精度。


<details>
  <summary>Details</summary>
Motivation: Kubernetes在动态云环境中因故障、网络问题等导致状态一致性难以维护，需高效根因分析工具提升可靠性。

Method: 提出SynergyRCA工具，结合LLM、图数据库检索增强和专家提示，构建StateGraph和MetaGraph分析时空与实体关系。

Result: 在两个生产集群数据集上验证，平均2分钟定位根因，精度达0.90，包括新发现的问题类型。

Conclusion: SynergyRCA通过多模态数据融合显著提升Kubernetes根因分析能力，为复杂分布式系统运维提供新思路。

Abstract: Kubernetes, a notably complex and distributed system, utilizes an array of
controllers to uphold cluster management logic through state reconciliation.
Nevertheless, maintaining state consistency presents significant challenges due
to unexpected failures, network disruptions, and asynchronous issues,
especially within dynamic cloud environments. These challenges result in
operational disruptions and economic losses, underscoring the necessity for
robust root cause analysis (RCA) to enhance Kubernetes reliability. The
development of large language models (LLMs) presents a promising direction for
RCA. However, existing methodologies encounter several obstacles, including the
diverse and evolving nature of Kubernetes incidents, the intricate context of
incidents, and the polymorphic nature of these incidents. In this paper, we
introduce SynergyRCA, an innovative tool that leverages LLMs with retrieval
augmentation from graph databases and enhancement with expert prompts.
SynergyRCA constructs a StateGraph to capture spatial and temporal
relationships and utilizes a MetaGraph to outline entity connections. Upon the
occurrence of an incident, an LLM predicts the most pertinent resource, and
SynergyRCA queries the MetaGraph and StateGraph to deliver context-specific
insights for RCA. We evaluate SynergyRCA using datasets from two production
Kubernetes clusters, highlighting its capacity to identify numerous root
causes, including novel ones, with high efficiency and precision. SynergyRCA
demonstrates the ability to identify root causes in an average time of about
two minutes and achieves an impressive precision of approximately 0.90.

</details>


### [400] [KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache at a Large Cloud Provider](https://arxiv.org/abs/2506.02634)
*Jiahao Wang,Jinbo Han,Xingda Wei,Sijie Shen,Dingyan Zhang,Chenguang Fang,Rong Chen,Wenyuan Yu,Haibo Chen*

Main category: cs.DC

TL;DR: 该论文首次系统分析了大型语言模型(LLM)服务中KV缓存的工作负载模式，并基于实际工作负载特征提出了一种感知工作负载的缓存淘汰策略，提升了有限缓存容量下的服务性能。


<details>
  <summary>Details</summary>
Motivation: 当前对LLM服务中KV缓存如何提升性能的理解有限，且系统设计决策(如缓存淘汰策略)高度依赖工作负载。现有研究多关注合成工作负载，缺乏对实际工作负载模式的深入理解。

Method: 论文首先对领先LLM服务提供商的KV缓存工作负载模式进行了系统化表征，发现了之前未被研究的实际工作负载特征。基于这些特征，提出了一种感知工作负载的缓存淘汰策略。

Result: 研究发现：KV缓存在请求间的重用呈现偏斜分布；单轮和多轮请求间的重用同等重要；特定请求类别的重用模式具有可预测性；理想缓存命中率所需的总缓存大小适中。提出的策略在实际工作负载下显著提升了服务性能。

Conclusion: 该研究为LLM服务中的KV缓存优化提供了实证基础，提出的工作负载感知缓存淘汰策略在有限缓存容量下能有效提升服务性能。

Abstract: Serving large language models (LLMs) is important for cloud providers, and
caching intermediate results (KV\$) after processing each request substantially
improves serving throughput and latency. However, there is limited
understanding of how LLM serving benefits from KV\$ caching, where system
design decisions like cache eviction policies are highly workload-dependent. In
this paper, we present the first systematic characterization of the KV\$
workload patterns from one of the leading LLM service providers. We draw
observations that were not covered by previous studies focusing on synthetic
workloads, including: KV\$ reuses are skewed across requests, where reuses
between single-turn requests are equally important as multi-turn requests; the
reuse time and probability are diverse considering all requests, but for a
specific request category, the pattern tends to be predictable; and the overall
cache size required for an ideal cache hit ratio is moderate. Based on the
characterization, we further propose a workload-aware cache eviction policy
that improves the serving performance under real-world traces, especially with
limited cache capacity.

</details>


### [401] [Enhancing Convergence, Privacy and Fairness for Wireless Personalized Federated Learning: Quantization-Assisted Min-Max Fair Scheduling](https://arxiv.org/abs/2506.02422)
*Xiyu Zhao,Qimei Cui,Ziqiang Du,Weicai Li,Xi Yu,Wei Ni,Ji Zhang,Xiaofeng Tao,Ping Zhang*

Main category: cs.DC

TL;DR: 该论文提出了一种基于量化误差的无线个性化联邦学习隐私增强方法，通过高斯差分隐私机制和最优传输调度策略，显著提升了模型性能与公平性。


<details>
  <summary>Details</summary>
Motivation: 无线个性化联邦学习（WPFL）中，隐私保护和模型性能公平性因通信瓶颈成为关键挑战。现有研究对此关注不足，需解决隐私泄露和性能不公平问题。

Method: 提出量化辅助高斯差分隐私机制，分析量化误差和高斯噪声对模型收敛的影响，设计最优传输调度策略（包括客户端选择、信道分配、功率控制等），实现最小化最大边界的目标。

Result: 实验表明，该方法在准确率、最大测试损失和公平性（Jain指数）上分别比基线策略提升87.08%、16.21%和38.37%。

Conclusion: 所提机制有效平衡了WPFL的隐私与公平性，通过联合优化通信与学习参数，为无线场景下的联邦学习提供了实用解决方案。

Abstract: Personalized federated learning (PFL) offers a solution to balancing
personalization and generalization by conducting federated learning (FL) to
guide personalized learning (PL). Little attention has been given to wireless
PFL (WPFL), where privacy concerns arise. Performance fairness of PL models is
another challenge resulting from communication bottlenecks in WPFL. This paper
exploits quantization errors to enhance the privacy of WPFL and proposes a
novel quantization-assisted Gaussian differential privacy (DP) mechanism. We
analyze the convergence upper bounds of individual PL models by considering the
impact of the mechanism (i.e., quantization errors and Gaussian DP noises) and
imperfect communication channels on the FL of WPFL. By minimizing the maximum
of the bounds, we design an optimal transmission scheduling strategy that
yields min-max fairness for WPFL with OFDMA interfaces. This is achieved by
revealing the nested structure of this problem to decouple it into subproblems
solved sequentially for the client selection, channel allocation, and power
control, and for the learning rates and PL-FL weighting coefficients.
Experiments validate our analysis and demonstrate that our approach
substantially outperforms alternative scheduling strategies by 87.08%, 16.21%,
and 38.37% in accuracy, the maximum test loss of participating clients, and
fairness (Jain's index), respectively.

</details>


### [402] [Rethinking Dynamic Networks and Heterogeneous Computing with Automatic Parallelization](https://arxiv.org/abs/2506.02787)
*Ruilong Wu,Xinjiao Li,Yisu Wang,Xinyu Chen,Dirk Kutscher*

Main category: cs.DC

TL;DR: 提出了一种考虑节点异构性和动态网络拓扑的混合并行训练方法，通过模拟优化配置并引入剪枝技术，显著提升大语言模型训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有自动并行规划框架忽视节点异构性和网络动态变化，限制了实际应用效果。

Method: 建模动态网络中的异构节点，利用模拟策略优化并行配置，并采用剪枝技术缩小搜索空间。

Result: 在异构节点上训练性能显著提升，在云计算等动态场景中展现出更强适应性。

Conclusion: 该方法有效解决了异构动态环境下的并行训练挑战，为大规模模型训练提供了实用方案。

Abstract: Hybrid parallelism techniques are essential for efficiently training large
language models (LLMs). Nevertheless, current automatic parallel planning
frameworks often overlook the simultaneous consideration of node heterogeneity
and dynamic network topology changes, limiting their effectiveness in practical
applications. In this paper, we address these limitations by modeling
heterogeneous nodes within dynamically changing network environments and
leveraging simulation-based strategies to determine optimal parallel
configurations. Our approach enables fine-grained workload allocation tailored
for heterogeneous nodes and complex network scenarios, achieving performance
competitive with state-of-the-art methods under regular and stable network
conditions. Additionally, we introduce a strategy pruning technique to rapidly
discard infeasible parallel configurations, substantially reducing the search
space and accelerating the search process through parallel execution within the
simulator. Preliminary evaluations confirm that our method notably enhances
training performance on heterogeneous nodes and demonstrates improved
adaptability in complex, dynamic scenarios such as cloud computing
environments.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [403] [Stop Chasing the C-index: This Is How We Should Evaluate Our Survival Models](https://arxiv.org/abs/2506.02075)
*Christian Marius Lillelund,Shi-ang Qi,Russell Greiner,Christian Fischer Pedersen*

Main category: stat.ME

TL;DR: 该论文指出生存分析模型中常用的C-index评估方法存在局限性，提出更全面的评估标准，并探讨了模型与评估指标假设一致性的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前生存分析模型评估过度依赖C-index，忽略了预测时间准确性、概率校准等重要方面，导致评估不全面。

Method: 通过文献调研总结现有评估方法的不足，提出一套针对生存分析特点的评估标准，并分析其优缺点。

Result: 发现当前评估指标发展呈现双螺旋阶梯模式，强调模型有效性与指标有效性需基于相同假设层级。

Conclusion: 需要采用更全面的方法评估生存分析模型，同时考虑不同学术观点对评估标准的争议。

Abstract: We argue that many survival analysis and time-to-event models are incorrectly
evaluated. First, we survey many examples of evaluation approaches in the
literature and find that most rely on concordance (C-index). However, the
C-index only measures a model's discriminative ability and does not assess
other important aspects, such as the accuracy of the time-to-event predictions
or the calibration of the model's probabilistic estimates. Next, we present a
set of key desiderata for choosing the right evaluation metric and discuss
their pros and cons. These are tailored to the challenges in survival analysis,
such as sensitivity to miscalibration and various censoring assumptions. We
hypothesize that the current development of survival metrics conforms to a
double-helix ladder, and that model validity and metric validity must stand on
the same rung of the assumption ladder. Finally, we discuss the appropriate
methods for evaluating a survival model in practice and summarize various
viewpoints opposing our analysis.

</details>


### [404] [Joint Modeling for Learning Decision-Making Dynamics in Behavioral Experiments](https://arxiv.org/abs/2506.02394)
*Yuan Bian,Xingche Guo,Yuanjia Wang*

Main category: stat.ME

TL;DR: 该研究提出了一种结合强化学习和漂移扩散模型的新框架，用于分析抑郁症患者的奖励决策过程，并通过隐马尔可夫模型捕捉策略切换。


<details>
  <summary>Details</summary>
Motivation: 抑郁症（MDD）与奖励处理异常和注意力问题相关，研究旨在通过整合强化学习和漂移扩散模型，更全面地分析奖励决策过程。

Method: 提出了一种结合强化学习（RL）和漂移扩散模型（DDM）的框架，并使用隐马尔可夫模型（HMM）捕捉策略切换。通过广义期望最大化算法实现高效计算。

Result: 研究发现抑郁症患者的整体参与度低于健康对照组，且在参与状态下决策时间更长。神经影像学数据显示，大脑活动与“参与”状态下的决策特征相关。

Conclusion: 该框架能够有效区分抑郁症患者和健康对照组在奖励决策中的差异，并揭示了大脑活动与决策特征的特定关联。

Abstract: Major depressive disorder (MDD), a leading cause of disability and mortality,
is associated with reward-processing abnormalities and concentration issues.
Motivated by the probabilistic reward task from the Establishing Moderators and
Biosignatures of Antidepressant Response in Clinical Care (EMBARC) study, we
propose a novel framework that integrates the reinforcement learning (RL) model
and drift-diffusion model (DDM) to jointly analyze reward-based decision-making
with response times. To account for emerging evidence suggesting that
decision-making may alternate between multiple interleaved strategies, we model
latent state switching using a hidden Markov model (HMM). In the ''engaged''
state, decisions follow an RL-DDM, simultaneously capturing reward processing,
decision dynamics, and temporal structure. In contrast, in the ''lapsed''
state, decision-making is modeled using a simplified DDM, where specific
parameters are fixed to approximate random guessing with equal probability. The
proposed method is implemented using a computationally efficient generalized
expectation-maximization algorithm with forward-backward procedures. Through
extensive numerical studies, we demonstrate that our proposed method
outperforms competing approaches under various reward-generating distributions,
both with and without strategy switching. When applied to the EMBARC study, our
framework reveals that MDD patients exhibit lower overall engagement than
healthy controls and experience longer decision times when they do engage.
Additionally, we show that neuroimaging measures of brain activities are
associated with decision-making characteristics in the ''engaged'' state but
not in the ''lapsed'' state, providing evidence of brain-behavioral association
specific to the ''engaged'' state.

</details>


### [405] [Simulation-Based Inference for Adaptive Experiments](https://arxiv.org/abs/2506.02881)
*Brian M Cho,Aurélien Bibaut,Nathan Kallus*

Main category: stat.ME

TL;DR: 该论文提出了一种基于模拟的乐观方法，用于多臂老虎机实验设计中的假设检验和置信区间构建，显著提升了推断精度和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 传统随机试验在多臂老虎机设计中存在推断能力弱、功效不足的问题，需要一种更高效的方法来提升参数估计精度和结果可靠性。

Method: 采用基于模拟的乐观方法（simulation with optimism），通过生成额外实验轨迹来表征非正态样本均值统计量的分布，从而进行推断。

Result: 该方法在多种常见老虎机设计中实现了类型I误差控制、置信区间收敛和估计量强一致性，置信区间宽度减少高达50%，尤其改善了非目标臂的推断效果。

Conclusion: 所提出的模拟方法显著提升了多臂老虎机实验的推断能力，为自适应采样提供了更强大的统计工具。

Abstract: Multi-arm bandit experimental designs are increasingly being adopted over
standard randomized trials due to their potential to improve outcomes for study
participants, enable faster identification of the best-performing options,
and/or enhance the precision of estimating key parameters. Current approaches
for inference after adaptive sampling either rely on asymptotic normality under
restricted experiment designs or underpowered martingale concentration
inequalities that lead to weak power in practice. To bypass these limitations,
we propose a simulation-based approach for conducting hypothesis tests and
constructing confidence intervals for arm specific means and their differences.
Our simulation-based approach uses positively biased nuisances to generate
additional trajectories of the experiment, which we call \textit{simulation
with optimism}. Using these simulations, we characterize the distribution
potentially non-normal sample mean test statistic to conduct inference. We
provide guarantees for (i) asymptotic type I error control, (ii) convergence of
our confidence intervals, and (iii) asymptotic strong consistency of our
estimator over a wide variety of common bandit designs. Our empirical results
show that our approach achieves the desired coverage while reducing confidence
interval widths by up to 50%, with drastic improvements for arms not targeted
by the design.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [406] [Enriching Location Representation with Detailed Semantic Information](https://arxiv.org/abs/2506.02744)
*Junyuan Liu,Xinglei Wang,Tao Cheng*

Main category: cs.CE

TL;DR: CaLLiPer+模型通过多模态对比学习整合POI名称和类别标签，提升城市建模性能4%-11%。


<details>
  <summary>Details</summary>
Motivation: 传统空间嵌入方法过于侧重空间邻近性，未能充分利用细粒度上下文信息，限制了城市建模的精度。

Method: 提出CaLLiPer+模型，在多模态对比学习框架中系统整合POI名称与类别标签。

Result: 在土地利用分类和社会经济分布映射任务中性能提升4%-11%，POI名称增强了位置检索能力。

Conclusion: 融合细粒度语义属性和多模态学习技术能有效推动城市基础模型的发展。

Abstract: Spatial representations that capture both structural and semantic
characteristics of urban environments are essential for urban modeling.
Traditional spatial embeddings often prioritize spatial proximity while
underutilizing fine-grained contextual information from places. To address this
limitation, we introduce CaLLiPer+, an extension of the CaLLiPer model that
systematically integrates Point-of-Interest (POI) names alongside categorical
labels within a multimodal contrastive learning framework. We evaluate its
effectiveness on two downstream tasks, land use classification and
socioeconomic status distribution mapping, demonstrating consistent performance
gains of 4% to 11% over baseline methods. Additionally, we show that
incorporating POI names enhances location retrieval, enabling models to capture
complex urban concepts with greater precision. Ablation studies further reveal
the complementary role of POI names and the advantages of leveraging pretrained
text encoders for spatial representations. Overall, our findings highlight the
potential of integrating fine-grained semantic attributes and multimodal
learning techniques to advance the development of urban foundation models.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [407] [PhysGaia: A Physics-Aware Dataset of Multi-Body Interactions for Dynamic Novel View Synthesis](https://arxiv.org/abs/2506.02794)
*Mijeong Kim,Gunhee Kim,Jungyoon Choi,Wonjae Roh,Bohyung Han*

Main category: cs.GR

TL;DR: PhysGaia是一个专为动态新视角合成设计的物理感知数据集，包含结构化对象和非结构化物理现象，支持物理建模研究。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要关注真实感重建，缺乏对物理感知动态场景建模的支持，PhysGaia旨在填补这一空白。

Method: 利用精选的物理求解器生成严格遵循物理定律的场景，并提供3D粒子轨迹和物理参数等真实数据。

Result: 数据集包含多种物理材料和复杂动态交互，为动态视角合成和物理建模研究提供了重要资源。

Conclusion: PhysGaia将推动动态视角合成、物理场景理解及深度学习与物理模拟结合的研究，实现更真实的动态场景重建。

Abstract: We introduce PhysGaia, a novel physics-aware dataset specifically designed
for Dynamic Novel View Synthesis (DyNVS), encompassing both structured objects
and unstructured physical phenomena. Unlike existing datasets that primarily
focus on photorealistic reconstruction, PhysGaia is created to actively support
physics-aware dynamic scene modeling. Our dataset provides complex dynamic
scenarios with rich interactions among multiple objects, where they
realistically collide with each other and exchange forces. Furthermore, it
contains a diverse range of physical materials, such as liquid, gas,
viscoelastic substance, and textile, which moves beyond the rigid bodies
prevalent in existing datasets. All scenes in PhysGaia are faithfully generated
to strictly adhere to physical laws, leveraging carefully selected
material-specific physics solvers. To enable quantitative evaluation of
physical modeling, our dataset provides essential ground-truth information,
including 3D particle trajectories and physics parameters, e.g., viscosity. To
facilitate research adoption, we also provide essential integration pipelines
for using state-of-the-art DyNVS models with our dataset and report their
results. By addressing the critical lack of datasets for physics-aware
modeling, PhysGaia will significantly advance research in dynamic view
synthesis, physics-based scene understanding, and deep learning models
integrated with physical simulation -- ultimately enabling more faithful
reconstruction and interpretation of complex dynamic scenes. Our datasets and
codes are available in the project website,
http://cvlab.snu.ac.kr/research/PhysGaia.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [408] [AI-Driven Vehicle Condition Monitoring with Cell-Aware Edge Service Migration](https://arxiv.org/abs/2506.02785)
*Charalampos Kalalas,Pavol Mulinka,Guillermo Candela Belmonte,Miguel Fornell,Michail Dalgitsis,Francisco Paredes Vera,Javier Santaella Sánchez,Carmen Vicente Villares,Roshan Sedar,Eftychia Datsika,Angelos Antonopoulos,Antonio Fernández Ojea,Miquel Payaro*

Main category: cs.NI

TL;DR: 本文提出了一种基于边缘计算和人工智能的车辆状态监测服务，通过动态服务编排框架实现低延迟AI推理和自适应服务部署，在5G网络环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在车辆状态监测中的应用日益增多，如何在移动环境中实现实时诊断和优化维护策略成为关键挑战。

Method: 采用闭环服务编排框架，根据网络指标动态触发边缘节点间的服务迁移，以应对移动性挑战。

Result: 实验结果表明，该框架在5G网络环境下能有效保证低延迟AI推理和自适应服务部署。

Conclusion: 该框架在智能交通和移动应用中具有潜在价值，能够提升车辆状态监测的实时性和适应性。

Abstract: Artificial intelligence (AI) has been increasingly applied to the condition
monitoring of vehicular equipment, aiming to enhance maintenance strategies,
reduce costs, and improve safety. Leveraging the edge computing paradigm,
AI-based condition monitoring systems process vast streams of vehicular data to
detect anomalies and optimize operational performance. In this work, we
introduce a novel vehicle condition monitoring service that enables real-time
diagnostics of a diverse set of anomalies while remaining practical for
deployment in real-world edge environments. To address mobility challenges, we
propose a closed-loop service orchestration framework where service migration
across edge nodes is dynamically triggered by network-related metrics. Our
approach has been implemented and tested in a real-world race circuit
environment equipped with 5G network capabilities under diverse operational
conditions. Experimental results demonstrate the effectiveness of our framework
in ensuring low-latency AI inference and adaptive service placement,
highlighting its potential for intelligent transportation and mobility
applications.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [409] [On the Benefits of Accelerated Optimization in Robust and Private Estimation](https://arxiv.org/abs/2506.03044)
*Laurentiu Andrei Marchis,Po-Ling Loh*

Main category: math.ST

TL;DR: 该论文研究了加速梯度方法在隐私和重尾鲁棒性方面的优势，提出了基于Frank-Wolfe方法和投影梯度下降的改进技术，并通过实验验证了其在统计保证和收敛速度上的优越性。


<details>
  <summary>Details</summary>
Motivation: 研究加速梯度方法（如Frank-Wolfe和投影梯度下降）在隐私保护和重尾数据鲁棒性方面的优势，以提升统计学习模型的性能。

Method: 针对Frank-Wolfe方法，采用定制学习率和梯度下界技术；对投影梯度下降，使用Nesterov动量加速。通过高斯机制和几何中位数估计器分别实现隐私保护和鲁棒性。

Result: 加速方法降低了迭代复杂度，增强了经验风险和总体风险最小化的统计保证，并在特定场景下达到最优收敛。

Conclusion: 论文提出的加速梯度方法在隐私和鲁棒性方面表现优异，尤其在非随机数据、随机无模型数据和参数模型中展现出显著优势。

Abstract: We study the advantages of accelerated gradient methods, specifically based
on the Frank-Wolfe method and projected gradient descent, for privacy and
heavy-tailed robustness. Our approaches are as follows: For the Frank-Wolfe
method, our technique is based on a tailored learning rate and a uniform lower
bound on the gradient of the $\ell_2$-norm over the constraint set. For
accelerating projected gradient descent, we use the popular variant based on
Nesterov's momentum, and we optimize our objective over $\mathbb{R}^p$. These
accelerations reduce iteration complexity, translating into stronger
statistical guarantees for empirical and population risk minimization. Our
analysis covers three settings: non-random data, random model-free data, and
parametric models (linear regression and generalized linear models).
Methodologically, we approach both privacy and robustness based on noisy
gradients. We ensure differential privacy via the Gaussian mechanism and
advanced composition, and we achieve heavy-tailed robustness using a geometric
median-of-means estimator, which also sharpens the dependency on the dimension
of the covariates. Finally, we compare our rates to existing bounds and
identify scenarios where our methods attain optimal convergence.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [410] [Re-experiment Smart: a Novel Method to Enhance Data-driven Prediction of Mechanical Properties of Epoxy Polymers](https://arxiv.org/abs/2506.01994)
*Wanshan Cui,Yejin Jeong,Inwook Song,Gyuri Kim,Minsang Kwon,Donghun Lee*

Main category: cond-mat.soft

TL;DR: 论文提出了一种通过多算法异常值检测和选择性重实验提升聚合物材料性能预测准确性的方法，显著降低了预测误差并提高了模型性能。


<details>
  <summary>Details</summary>
Motivation: 聚合物材料性能预测中的数据异常值会严重影响机器学习模型的准确性，导致预测结果不可靠。为了解决这一问题，需要一种高效的数据质量提升方法。

Method: 结合多算法异常值检测与选择性重实验，对不可靠的异常值进行重新测量，构建包含701个数据点的新数据集，并在多种机器学习模型（如弹性网络、SVR、随机森林和TPOT）上验证方法效果。

Result: 该方法仅需对约5%的数据进行重新测量，即可显著降低预测误差（RMSE），并提高模型在玻璃化转变温度、tan δ峰值和交联密度等关键性能上的预测准确性。

Conclusion: 研究强调了数据质量提升在聚合物科学机器学习应用中的重要性，并提出了一种可扩展的策略，为材料科学中的预测可靠性提供了有效解决方案。

Abstract: Accurate prediction of polymer material properties through data-driven
approaches greatly accelerates novel material development by reducing redundant
experiments and trial-and-error processes. However, inevitable outliers in
empirical measurements can severely skew machine learning results, leading to
erroneous prediction models and suboptimal material designs. To address this
limitation, we propose a novel approach to enhance dataset quality efficiently
by integrating multi-algorithm outlier detection with selective
re-experimentation of unreliable outlier cases. To validate the empirical
effectiveness of the approach, we systematically construct a new dataset
containing 701 measurements of three key mechanical properties: glass
transition temperature ($T_g$), tan $\delta$ peak, and crosslinking density
($v_{c}$). To demonstrate its general applicability, we report the performance
improvements across multiple machine learning models, including Elastic Net,
SVR, Random Forest, and TPOT, to predict the three key properties. Our method
reliably reduces prediction error (RMSE) and significantly improves accuracy
with minimal additional experimental work, requiring only about 5% of the
dataset to be re-measured.These findings highlight the importance of data
quality enhancement in achieving reliable machine learning applications in
polymer science and present a scalable strategy for improving predictive
reliability in materials science.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [411] [Stochastically Dominant Peer Prediction](https://arxiv.org/abs/2506.02259)
*Yichi Zhang,Shengwei Xu,David Pennock,Grant Schoenebeck*

Main category: cs.GT

TL;DR: 论文提出了一种名为随机支配真实性（SD-truthfulness）的新机制，用于在非线性支付规则下激励诚实报告，并通过改进的舍入方法和新机制（EA机制）在保持敏感性的同时实现这一目标。


<details>
  <summary>Details</summary>
Motivation: 传统的同行预测机制假设代理的效用是分数的线性函数，但在实践中，非线性支付规则更为常见。因此，需要一种更强的激励机制来确保在各种单调效用函数下都能激励诚实报告。

Method: 论文首先分析了现有机制在SD-truthfulness方面的不足，提出通过改进舍入方法来保持敏感性。随后，引入了一种新的强制一致（EA）机制，该机制在二元信号设置下理论上保证SD-truthfulness。

Result: 改进的舍入方法在保持敏感性的同时实现了SD-truthfulness。EA机制在实验中表现出最高的敏感性，优于所有已知的SD-truthful机制。

Conclusion: 论文提出的SD-truthfulness机制和EA机制为非线性支付规则下的诚实报告提供了更强大的激励，同时保持了敏感性和公平性。

Abstract: Eliciting reliable human feedback is essential for many machine learning
tasks, such as learning from noisy labels and aligning AI systems with human
preferences. Peer prediction mechanisms incentivize truthful reporting without
ground truth verification by scoring agents based on correlations with peers.
Traditional mechanisms, which ensure that truth-telling maximizes the expected
scores in equilibrium, can elicit honest information while assuming agents'
utilities are linear functions of their scores. However, in practice,
non-linear payment rules are usually preferred, or agents' utilities are
inherently non-linear.
  We propose stochastically dominant truthfulness (SD-truthfulness) as a
stronger guarantee: the score distribution of truth-telling stochastically
dominates all other strategies, incentivizing truthful reporting for a wide
range of monotone utility functions. Our first observation is that no existing
peer prediction mechanism naturally satisfies this criterion without strong
assumptions. A simple solution -- rounding scores into binary lotteries -- can
enforce SD-truthfulness, but often degrades sensitivity, a key property related
to fairness and statistical efficiency. We demonstrate how a more careful
application of rounding can better preserve sensitivity. Furthermore, we
introduce a new enforced agreement (EA) mechanism that is theoretically
guaranteed to be SD-truthful in binary-signal settings under mild assumptions,
and empirically achieves the highest sensitivity among all known SD-truthful
mechanisms.

</details>


### [412] [Learning Optimal Posted Prices for a Unit-Demand Buyer](https://arxiv.org/abs/2506.02284)
*Yifeng Teng,Yifan Wang*

Main category: cs.GT

TL;DR: 该论文研究了在两种查询模型下学习单位需求买家最优定价的问题，并给出了近乎紧密的样本复杂度和定价查询复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究在买家价值分布独立且学习者可通过查询获取信息的情况下，如何有效地学习最优定价策略。

Method: 采用样本访问模型和定价查询模型，分析单位需求定价问题的样本复杂度和查询复杂度。

Result: 论文得出了单位需求定价问题在两种查询模型下的近乎紧密的样本复杂度和定价查询复杂度。

Conclusion: 该研究为学习最优定价策略提供了理论支持，尤其在单位需求买家和独立价值分布的场景下。

Abstract: We study the problem of learning the optimal item pricing for a unit-demand
buyer with independent item values, and the learner has query access to the
buyer's value distributions. We consider two common query models in the
literature: the sample access model where the learner can obtain a sample of
each item value, and the pricing query model where the learner can set a price
for an item and obtain a binary signal on whether the sampled value of the item
is greater than our proposed price. In this work, we give nearly tight sample
complexity and pricing query complexity of the unit-demand pricing problem.

</details>


### [413] [Designing Algorithmic Delegates: The Role of Indistinguishability in Human-AI Handoff](https://arxiv.org/abs/2506.03102)
*Sophie Greenwood,Karen Levy,Solon Barocas,Hoda Heidari,Jon Kleinberg*

Main category: cs.GT

TL;DR: 本文研究在人类基于可观察特征分类决策问题时，如何设计最优的AI代理算法，发现该问题本质上是组合性的，并在多种情境下找到高效解法。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术进步，人类更倾向于将任务委托给AI代理。然而，人类决策者往往无法全面评估所有相关因素，导致基于可观察特征进行分类决策。研究如何在这种情境下设计最优的AI代理具有重要意义。

Method: 通过理论分析和计算实验，探讨了最优委托问题的组合性质，提出了在多种情境下（如人类与算法观察特征可分解时）设计高效算法的解决方案。

Result: 研究发现最优委托问题在一般情况下是计算困难的，但在多种广泛情境下可找到高效解法。模拟实验表明，动态优化的代理虽非全局最优，但表现良好。

Conclusion: 在人类基于分类的决策场景中，设计最优AI代理具有挑战性，但通过特定方法可在实际应用中取得良好效果，为算法与人类协作提供了重要洞见。

Abstract: As AI technologies improve, people are increasingly willing to delegate tasks
to AI agents. In many cases, the human decision-maker chooses whether to
delegate to an AI agent based on properties of the specific instance of the
decision-making problem they are facing. Since humans typically lack full
awareness of all the factors relevant to this choice for a given
decision-making instance, they perform a kind of categorization by treating
indistinguishable instances -- those that have the same observable features --
as the same. In this paper, we define the problem of designing the optimal
algorithmic delegate in the presence of categories. This is an important
dimension in the design of algorithms to work with humans, since we show that
the optimal delegate can be an arbitrarily better teammate than the optimal
standalone algorithmic agent. The solution to this optimal delegation problem
is not obvious: we discover that this problem is fundamentally combinatorial,
and illustrate the complex relationship between the optimal design and the
properties of the decision-making task even in simple settings. Indeed, we show
that finding the optimal delegate is computationally hard in general. However,
we are able to find efficient algorithms for producing the optimal delegate in
several broad cases of the problem, including when the optimal action may be
decomposed into functions of features observed by the human and the algorithm.
Finally, we run computational experiments to simulate a designer updating an
algorithmic delegate over time to be optimized for when it is actually adopted
by users, and show that while this process does not recover the optimal
delegate in general, the resulting delegate often performs quite well.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [414] [Phenotypic Profile-Informed Generation of Drug-Like Molecules via Dual-Channel Variational Autoencoders](https://arxiv.org/abs/2506.02051)
*Hui Liu,Shiye Tian,Xuejun Liu*

Main category: q-bio.BM

TL;DR: SmilesGEN是一种基于变分自编码器的新型生成模型，通过整合药物和表达谱数据生成具有潜在治疗效果的分子。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖表达谱指导分子生成，但忽略了分子对细胞环境的扰动效应。

Method: 结合预训练的药物VAE（SmilesNet）和表达谱VAE（ProfileNet），在共同潜在空间中建模药物扰动与转录响应的相互作用。

Result: SmilesGEN在生成有效性、独特性、新颖性及与已知配体的相似性方面优于现有模型，并能生成更接近已批准药物的分子。

Conclusion: SmilesGEN建立了一个利用基因特征生成潜在治疗分子的稳健框架，有望诱导理想的细胞表型变化。

Abstract: The de novo generation of drug-like molecules capable of inducing desirable
phenotypic changes is receiving increasing attention. However, previous methods
predominantly rely on expression profiles to guide molecule generation, but
overlook the perturbative effect of the molecules on cellular contexts. To
overcome this limitation, we propose SmilesGEN, a novel generative model based
on variational autoencoder (VAE) architecture to generate molecules with
potential therapeutic effects. SmilesGEN integrates a pre-trained drug VAE
(SmilesNet) with an expression profile VAE (ProfileNet), jointly modeling the
interplay between drug perturbations and transcriptional responses in a common
latent space. Specifically, ProfileNet is imposed to reconstruct pre-treatment
expression profiles when eliminating drug-induced perturbations in the latent
space, while SmilesNet is informed by desired expression profiles to generate
drug-like molecules. Our empirical experiments demonstrate that SmilesGEN
outperforms current state-of-the-art models in generating molecules with higher
degree of validity, uniqueness, novelty, as well as higher Tanimoto similarity
to known ligands targeting the relevant proteins. Moreover, we evaluate
SmilesGEN for scaffold-based molecule optimization and generation of
therapeutic agents, and confirmed its superior performance in generating
molecules with higher similarity to approved drugs. SmilesGEN establishes a
robust framework that leverages gene signatures to generate drug-like molecules
that hold promising potential to induce desirable cellular phenotypic changes.

</details>


### [415] [Protap: A Benchmark for Protein Modeling on Realistic Downstream Applications](https://arxiv.org/abs/2506.02052)
*Shuo Yan,Yuliang Yan,Bin Ma,Chenao Li,Haochun Tang,Jiahua Lu,Minhua Lin,Yuyuan Feng,Hui Xiong,Enyan Dai*

Main category: q-bio.BM

TL;DR: 该论文介绍了Protap基准测试，系统比较了蛋白质应用中的架构、预训练策略和领域特定模型，发现监督学习和小规模训练集有时优于大规模预训练。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习架构和预训练策略在蛋白质应用中表现不一，且缺乏针对工业相关任务的系统评估。Protap旨在填补这一空白，提供全面比较。

Method: Protap覆盖五个蛋白质应用任务，包括三个通用任务和两个新专业任务，比较不同领域特定模型和通用架构在多种预训练设置下的表现。

Result: 研究发现：(i) 大规模预训练编码器在小规模下游训练集上可能不如监督学习编码器；(ii) 下游微调时加入结构信息可媲美或超越基于大规模序列预训练的蛋白质语言模型；(iii) 领域特定生物先验可提升专业任务性能。

Conclusion: Protap为蛋白质应用提供了系统评估框架，揭示了预训练和监督学习的优劣，并强调了领域知识在专业任务中的重要性。

Abstract: Recently, extensive deep learning architectures and pretraining strategies
have been explored to support downstream protein applications. Additionally,
domain-specific models incorporating biological knowledge have been developed
to enhance performance in specialized tasks. In this work, we introduce
$\textbf{Protap}$, a comprehensive benchmark that systematically compares
backbone architectures, pretraining strategies, and domain-specific models
across diverse and realistic downstream protein applications. Specifically,
Protap covers five applications: three general tasks and two novel specialized
tasks, i.e., enzyme-catalyzed protein cleavage site prediction and targeted
protein degradation, which are industrially relevant yet missing from existing
benchmarks. For each application, Protap compares various domain-specific
models and general architectures under multiple pretraining settings. Our
empirical studies imply that: (i) Though large-scale pretraining encoders
achieve great results, they often underperform supervised encoders trained on
small downstream training sets. (ii) Incorporating structural information
during downstream fine-tuning can match or even outperform protein language
models pretrained on large-scale sequence corpora. (iii) Domain-specific
biological priors can enhance performance on specialized downstream tasks. Code
and datasets are publicly available at
https://github.com/Trust-App-AI-Lab/protap.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [416] [Enhancing Speech Instruction Understanding and Disambiguation in Robotics via Speech Prosody](https://arxiv.org/abs/2506.02057)
*David Sasu,Kweku Andoh Yamoah,Benedict Quartey,Natalie Schluter*

Main category: cs.RO

TL;DR: 提出一种利用语音韵律直接推断指令意图的新方法，结合大语言模型消歧，并创建首个机器人歧义语音数据集，显著提升人机交互准确率。


<details>
  <summary>Details</summary>
Motivation: 传统语音识别方法将语音转为文本时丢弃了关键的韵律线索，导致意图歧义。为提高人机协作效率，需直接利用语音韵律解析指令意图。

Method: 通过语音韵律预测意图，并基于上下文学习将预测结果集成到大语言模型中，以消歧并选择合适任务计划。同时构建首个机器人歧义语音数据集。

Result: 该方法在检测指代意图时准确率达95.79%，对歧义指令的任务计划选择准确率达71.96%。

Conclusion: 该方法通过韵律分析和语言模型结合，有效提升人机交互的意图理解能力，为相关研究提供新数据集和基准。

Abstract: Enabling robots to accurately interpret and execute spoken language
instructions is essential for effective human-robot collaboration. Traditional
methods rely on speech recognition to transcribe speech into text, often
discarding crucial prosodic cues needed for disambiguating intent. We propose a
novel approach that directly leverages speech prosody to infer and resolve
instruction intent. Predicted intents are integrated into large language models
via in-context learning to disambiguate and select appropriate task plans.
Additionally, we present the first ambiguous speech dataset for robotics,
designed to advance research in speech disambiguation. Our method achieves
95.79% accuracy in detecting referent intents within an utterance and
determines the intended task plan of ambiguous instructions with 71.96%
accuracy, demonstrating its potential to significantly improve human-robot
communication.

</details>


### [417] [HiLO: High-Level Object Fusion for Autonomous Driving using Transformers](https://arxiv.org/abs/2506.02554)
*Timo Osterburg,Franz Albers,Christopher Diehl,Rajesh Pushparaj,Torsten Bertram*

Main category: cs.RO

TL;DR: 论文提出了一种基于Transformer的高层对象融合方法HiLO，改进了自适应卡尔曼滤波器，显著提升了自动驾驶环境感知的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的传感器数据融合方法虽然性能高，但复杂度和硬件需求限制了其在量产车中的应用。高层融合方法计算需求较低，但传统方法如卡尔曼滤波器占据主导。

Method: 论文改进了自适应卡尔曼滤波器（AKF），并提出了一种新的基于Transformer的高层对象融合方法HiLO。

Result: 实验结果显示，HiLO在F1分数上提升了25.9个百分点，平均IoU提升了6.1个百分点。在大规模真实数据集上的评估验证了方法的有效性，并通过城市场景和高速公路场景的跨域评估进一步验证了其泛化能力。

Conclusion: HiLO方法在自动驾驶环境感知中表现出色，具有较高的性能和泛化能力，代码、数据和模型已开源。

Abstract: The fusion of sensor data is essential for a robust perception of the
environment in autonomous driving. Learning-based fusion approaches mainly use
feature-level fusion to achieve high performance, but their complexity and
hardware requirements limit their applicability in near-production vehicles.
High-level fusion methods offer robustness with lower computational
requirements. Traditional methods, such as the Kalman filter, dominate this
area. This paper modifies the Adapted Kalman Filter (AKF) and proposes a novel
transformer-based high-level object fusion method called HiLO. Experimental
results demonstrate improvements of $25.9$ percentage points in $\textrm{F}_1$
score and $6.1$ percentage points in mean IoU. Evaluation on a new large-scale
real-world dataset demonstrates the effectiveness of the proposed approaches.
Their generalizability is further validated by cross-domain evaluation between
urban and highway scenarios. Code, data, and models are available at
https://github.com/rst-tu-dortmund/HiLO .

</details>


### [418] [Multi Layered Autonomy and AI Ecologies in Robotic Art Installations](https://arxiv.org/abs/2506.02606)
*Baoyang Chen,Xian Xu,Huamin Qu*

Main category: cs.RO

TL;DR: 《共生体》是陈宝阳的大型装置艺术，通过AI驱动机器人在镜面沉浸式空间中的互动，探讨机器代理与艺术创作的张力，观众成为共同创作者。


<details>
  <summary>Details</summary>
Motivation: 作品旨在探索AI时代下机器代理与人类作者身份的边界，并反思历史上边缘化劳动力剥削问题，提出对未来AI伦理责任的拷问。

Method: 采用三层信仰系统（微观自适应策略、中观叙事驱动、宏观首要指令）构建机器生态，结合反应式灯光/雾效和观众呼吸交互，实现动态行为演化。

Result: 机器被塑造为艺术合作者而非工具，形成具有生命力的涌现性作品，在国际展览中重新定义了代理权、作者身份和艺术伦理。

Conclusion: 通过控制论反馈、机器人实验与概念规则融合，证明了当代艺术中跨媒介协作对传统创作范式的革新潜力。

Abstract: Symbiosis of Agents is a large-scale installation by Baoyang Chen that embeds
AI-driven robots in an immersive, mirror-lined arena, probing the tension
between machine agency and artistic authorship. Drawing on early cybernetics,
rule-based conceptual art, and seminal robotic works, it orchestrates fluid
exchanges among robotic arms, quadruped machines, their environment, and the
public. A three tier faith system pilots the ecology: micro-level adaptive
tactics, meso-level narrative drives, and a macro-level prime directive. This
hierarchy lets behaviors evolve organically in response to environmental cues
and even a viewer's breath, turning spectators into co-authors of the unfolding
drama.Framed by a speculative terraforming scenario that recalls the historical
exploitation of marginalized labor, the piece asks who bears responsibility in
AI-mediated futures. Choreographed motion, AI-generated scripts, reactive
lighting, and drifting fog cast the robots as collaborators rather than tools,
forging a living, emergent artwork. Exhibited internationally, Symbiosis of
Agents shows how cybernetic feedback, robotic experimentation, and conceptual
rule-making can converge to redefine agency, authorship, and ethics in
contemporary art.

</details>


### [419] [Olfactory Inertial Odometry: Methodology for Effective Robot Navigation by Scent](https://arxiv.org/abs/2506.02373)
*Kordel K. France,Ovidiu Daescu*

Main category: cs.RO

TL;DR: 该论文提出了一种名为嗅觉惯性里程计（OIO）的框架，通过结合惯性运动学和快速采样嗅觉传感器，实现类似视觉惯性里程计（VIO）的气味导航，并在5自由度机械臂上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 嗅觉导航是生物体最原始的探索机制之一，但机器嗅觉导航的模拟和解决非常困难。论文旨在通过结合惯性运动学和嗅觉传感器，建立一个类似VIO的嗅觉导航框架。

Method: 论文提出了嗅觉惯性里程计（OIO）框架，借鉴SLAM和VIO的原理，将其扩展到嗅觉领域，并在5自由度机械臂上使用三种不同的气味定位算法进行验证。

Result: 实验结果表明，OIO框架在气味追踪场景中表现成功，为嗅觉导航研究建立了基线框架，并指出了未来可以改进的性能以应对更复杂的任务。

Conclusion: 论文成功建立了OIO的基线框架，为嗅觉导航研究奠定了基础，并提出了未来性能优化的方向。

Abstract: Olfactory navigation is one of the most primitive mechanisms of exploration
used by organisms. Navigation by machine olfaction (artificial smell) is a very
difficult task to both simulate and solve. With this work, we define olfactory
inertial odometry (OIO), a framework for using inertial kinematics, and
fast-sampling olfaction sensors to enable navigation by scent analogous to
visual inertial odometry (VIO). We establish how principles from SLAM and VIO
can be extrapolated to olfaction to enable real-world robotic tasks. We
demonstrate OIO with three different odour localization algorithms on a real
5-DoF robot arm over an odour-tracking scenario that resembles real
applications in agriculture and food quality control. Our results indicate
success in establishing a baseline framework for OIO from which other research
in olfactory navigation can build, and we note performance enhancements that
can be made to address more complex tasks in the future.

</details>


### [420] [Grasp2Grasp: Vision-Based Dexterous Grasp Translation via Schrödinger Bridges](https://arxiv.org/abs/2506.02489)
*Tao Zhong,Jonah Buchanan,Christine Allen-Blanchette*

Main category: cs.RO

TL;DR: 提出一种基于视觉的灵巧抓取转换新方法，通过概率生成模型实现不同形态机械手间的语义抓取迁移。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要配对演示或针对特定手的仿真，难以实现异构机械手间的抓取意图迁移。

Method: 采用Schrödinger Bridge框架建立抓取分布间的随机传输，结合分数流匹配和物理约束成本函数（包括基座姿态、接触图、力矩空间和可操作性）。

Result: 实验表明该方法能生成稳定、物理合理的抓取，具有强泛化能力。

Conclusion: 该工作实现了异构机械手的语义抓取迁移，搭建了视觉抓取与概率生成模型的桥梁。

Abstract: We propose a new approach to vision-based dexterous grasp translation, which
aims to transfer grasp intent across robotic hands with differing morphologies.
Given a visual observation of a source hand grasping an object, our goal is to
synthesize a functionally equivalent grasp for a target hand without requiring
paired demonstrations or hand-specific simulations. We frame this problem as a
stochastic transport between grasp distributions using the Schr\"odinger Bridge
formalism. Our method learns to map between source and target latent grasp
spaces via score and flow matching, conditioned on visual observations. To
guide this translation, we introduce physics-informed cost functions that
encode alignment in base pose, contact maps, wrench space, and manipulability.
Experiments across diverse hand-object pairs demonstrate our approach generates
stable, physically grounded grasps with strong generalization. This work
enables semantic grasp transfer for heterogeneous manipulators and bridges
vision-based grasping with probabilistic generative modeling.

</details>


### [421] [Solving the Pod Repositioning Problem with Deep Reinforced Adaptive Large Neighborhood Search](https://arxiv.org/abs/2506.02746)
*Lin Xie,Hanyi Li*

Main category: cs.RO

TL;DR: 论文提出了一种结合自适应大邻域搜索(ALNS)和深度强化学习(DRL)的方法，用于解决机器人移动履约系统中的货架重新定位问题(PRP)，该方法在解质量和效率上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 在机器人移动履约系统(RMFS)中，货架重新定位问题(PRP)的优化对仓库运营效率至关重要。传统方法如最便宜位置、固定位置等存在局限性，需要更智能的解决方案。

Method: 该方法整合了自适应大邻域搜索(ALNS)和深度强化学习(DRL)，通过DRL动态选择破坏和修复操作符，并调整关键参数如破坏程度和接受阈值，同时设计了反映PRP特性的启发式方法。

Result: 计算结果表明，这种DRL引导的ALNS方法在解质量上优于传统方法，如最便宜位置、固定位置、二进制整数规划和静态启发式方法。

Conclusion: 该方法展示了学习驱动控制在组合优化中的优势，为仓库系统的优化提供了有效解决方案。

Abstract: The Pod Repositioning Problem (PRP) in Robotic Mobile Fulfillment Systems
(RMFS) involves selecting optimal storage locations for pods returning from
pick stations. This work presents an improved solution method that integrates
Adaptive Large Neighborhood Search (ALNS) with Deep Reinforcement Learning
(DRL). A DRL agent dynamically selects destroy and repair operators and adjusts
key parameters such as destruction degree and acceptance thresholds during the
search. Specialized heuristics for both operators are designed to reflect
PRP-specific characteristics, including pod usage frequency and movement costs.
Computational results show that this DRL-guided ALNS outperforms traditional
approaches such as cheapest-place, fixed-place, binary integer programming, and
static heuristics. The method demonstrates strong solution quality and
illustrating the benefit of learning-driven control within combinatorial
optimization for warehouse systems.

</details>


### [422] [Learned Controllers for Agile Quadrotors in Pursuit-Evasion Games](https://arxiv.org/abs/2506.02849)
*Alejandro Sanchez Roncero,Olov Andersson,Petter Ogren*

Main category: cs.RO

TL;DR: 该论文提出了一种基于强化学习的四旋翼无人机1v1追逃框架，采用异步多阶段种群训练方法解决对抗训练中的非平稳性问题，实现了高速追逃机动。


<details>
  <summary>Details</summary>
Motivation: 随着小型无人机在民用和军用领域的普及，未经授权或恶意无人机进入限制区域引发的安全问题日益突出，需要开发高效的追逃算法。

Method: 使用神经网络策略控制机体角速度和总推力，提出异步多阶段种群训练算法(AMSPB)，通过分阶段对抗训练保留历史策略以解决灾难性遗忘问题。

Result: 速率控制策略比速度基线方法捕获率更高、峰值速度更快；AMSPB算法在对抗基准测试中展现出稳定、单调的性能提升。

Conclusion: 该研究证明了强化学习框架在无人机高速追逃任务中的有效性，AMSPB算法成功解决了对抗训练中的非平稳性问题。

Abstract: The increasing proliferation of small UAVs in civilian and military airspace
has raised critical safety and security concerns, especially when unauthorized
or malicious drones enter restricted zones. In this work, we present a
reinforcement learning (RL) framework for agile 1v1 quadrotor pursuit-evasion.
We train neural network policies to command body rates and collective thrust,
enabling high-speed pursuit and evasive maneuvers that fully exploit the
quadrotor's nonlinear dynamics. To mitigate nonstationarity and catastrophic
forgetting during adversarial co-training, we introduce an Asynchronous
Multi-Stage Population-Based (AMSPB) algorithm where, at each stage, either the
pursuer or evader learns against a sampled opponent drawn from a growing
population of past and current policies. This continual learning setup ensures
monotonic performance improvement and retention of earlier strategies. Our
results show that (i) rate-based policies achieve significantly higher capture
rates and peak speeds than velocity-level baselines, and (ii) AMSPB yields
stable, monotonic gains against a suite of benchmark opponents.

</details>


### [423] [Tru-POMDP: Task Planning Under Uncertainty via Tree of Hypotheses and Open-Ended POMDPs](https://arxiv.org/abs/2506.02860)
*Wenjing Tang,Xinyu He,Yongxi Huang,Yunxiao Xiao,Cewu Lu,Panpan Cai*

Main category: cs.RO

TL;DR: Tru-POMDP结合LLM和POMDP规划，解决家庭服务机器人任务规划中的不确定性，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 家庭服务机器人在现实世界中执行任务时面临指令模糊、物体位置未知和开放词汇类型等不确定性，导致规划空间巨大。

Method: 提出Tru-POMDP，通过LLM生成结构化信念，结合分层假设树(TOH)和POMDP规划，实现高效信念空间规划。

Result: 在复杂厨房环境的重排任务中，Tru-POMDP成功率更高，规划更优，对模糊和遮挡的鲁棒性更强，效率更高。

Conclusion: Tru-POMDP有效解决了开放不确定性下的任务规划问题，显著优于现有基于LLM的规划方法。

Abstract: Task planning under uncertainty is essential for home-service robots
operating in the real world. Tasks involve ambiguous human instructions, hidden
or unknown object locations, and open-vocabulary object types, leading to
significant open-ended uncertainty and a boundlessly large planning space. To
address these challenges, we propose Tru-POMDP, a planner that combines
structured belief generation using Large Language Models (LLMs) with principled
POMDP planning. Tru-POMDP introduces a hierarchical Tree of Hypotheses (TOH),
which systematically queries an LLM to construct high-quality particle beliefs
over possible world states and human goals. We further formulate an open-ended
POMDP model that enables rigorous Bayesian belief tracking and efficient
belief-space planning over these LLM-generated hypotheses. Experiments on
complex object rearrangement tasks across diverse kitchen environments show
that Tru-POMDP significantly outperforms state-of-the-art LLM-based and
LLM-tree-search hybrid planners, achieving higher success rates with
significantly better plans, stronger robustness to ambiguity and occlusion, and
greater planning efficiency.

</details>


### [424] [UniConFlow: A Unified Constrained Generalization Framework for Certified Motion Planning with Flow Matching Models](https://arxiv.org/abs/2506.02955)
*Zewen Yang,Xiaobing Dai,Dian Yu,Qianru Li,Yu Li,Valentin Le Mesle*

Main category: cs.RO

TL;DR: UniConFlow提出了一种基于流匹配的统一框架，用于机器人轨迹生成，能同时处理多种约束条件，如避障和动力学一致性，无需重新训练或额外控制器。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在机器人运动生成中难以同时处理多种约束条件，如避障和动力学一致性，通常需要分开处理或仅部分考虑。

Method: UniConFlow采用流匹配框架，结合等式和不等式约束，通过二次规划生成引导输入，确保约束满足，并引入预设时间归零函数增强推理灵活性。

Result: 在移动导航和高维操作任务中，UniConFlow相比现有约束生成规划器，表现出更高的安全性和可行性。

Conclusion: UniConFlow通过统一框架有效整合多种约束，提升了机器人轨迹生成的安全性和适应性，为复杂任务提供了可靠解决方案。

Abstract: Generative models have become increasingly powerful tools for robot motion
generation, enabling flexible and multimodal trajectory generation across
various tasks. Yet, most existing approaches remain limited in handling
multiple types of constraints, such as collision avoidance and dynamic
consistency, which are often treated separately or only partially considered.
This paper proposes UniConFlow, a unified flow matching (FM) based framework
for trajectory generation that systematically incorporates both equality and
inequality constraints. UniConFlow introduces a novel prescribed-time zeroing
function to enhance flexibility during the inference process, allowing the
model to adapt to varying task requirements. To ensure constraint satisfaction,
particularly with respect to obstacle avoidance, admissible action range, and
kinodynamic consistency, the guidance inputs to the FM model are derived
through a quadratic programming formulation, which enables constraint-aware
generation without requiring retraining or auxiliary controllers. We conduct
mobile navigation and high-dimensional manipulation tasks, demonstrating
improved safety and feasibility compared to state-of-the-art constrained
generative planners. Project page is available at https://uniconflow.github.io.

</details>


### [425] [EDEN: Entorhinal Driven Egocentric Navigation Toward Robotic Deployment](https://arxiv.org/abs/2506.03046)
*Mikolaj Walczak,Romina Aalishah,Wyatt Mackey,Brittany Story,David L. Boothe Jr.,Nicholas Waytowich,Xiaomin Lin,Tinoosh Mohsenin*

Main category: cs.RO

TL;DR: 论文提出EDEN框架，结合网格细胞表征与强化学习，实现生物启发的自主导航，在仿真环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习智能体在导航任务中脆弱且缺乏适应性，而人类则能灵活应对不同场景。研究旨在通过模拟哺乳动物内嗅-海马系统，提升智能体的空间导航能力。

Method: EDEN框架整合网格细胞编码器（将运动信息转换为周期性空间编码）和PPO强化学习策略，使用MiniWorld和Gazebo仿真器进行训练，结合视觉与运动传感器数据。

Result: EDEN在简单场景中成功率99%，复杂遮挡环境中>94%，优于基于原始状态或传统卷积编码器的基线模型，且导航效率更高。

Conclusion: 该工作将生物神经导航机制与强化学习结合，为机器人空间智能提供了可扩展的生物基础方案。

Abstract: Deep reinforcement learning agents are often fragile while humans remain
adaptive and flexible to varying scenarios. To bridge this gap, we present
EDEN, a biologically inspired navigation framework that integrates learned
entorhinal-like grid cell representations and reinforcement learning to enable
autonomous navigation. Inspired by the mammalian entorhinal-hippocampal system,
EDEN allows agents to perform path integration and vector-based navigation
using visual and motion sensor data. At the core of EDEN is a grid cell encoder
that transforms egocentric motion into periodic spatial codes, producing
low-dimensional, interpretable embeddings of position. To generate these
activations from raw sensory input, we combine fiducial marker detections in
the lightweight MiniWorld simulator and DINO-based visual features in the
high-fidelity Gazebo simulator. These spatial representations serve as input to
a policy trained with Proximal Policy Optimization (PPO), enabling dynamic,
goal-directed navigation. We evaluate EDEN in both MiniWorld, for rapid
prototyping, and Gazebo, which offers realistic physics and perception noise.
Compared to baseline agents using raw state inputs (e.g., position, velocity)
or standard convolutional image encoders, EDEN achieves a 99% success rate,
within the simple scenarios, and >94% within complex floorplans with occluded
paths with more efficient and reliable step-wise navigation. In addition, as a
replacement of ground truth activations, we present a trainable Grid Cell
encoder enabling the development of periodic grid-like patterns from vision and
motion sensor data, emulating the development of such patterns within
biological mammals. This work represents a step toward biologically grounded
spatial intelligence in robotics, bridging neural navigation principles with
reinforcement learning for scalable deployment.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [426] [Random-key genetic algorithms](https://arxiv.org/abs/2506.02120)
*Mariana A. Londe,Luciana S. Pessoa,Carlos E. Andrade,José F. Gonçalves,Mauricio G. C. Resende*

Main category: cs.NE

TL;DR: 随机密钥遗传算法是一种用于离散和全局优化的进化元启发式方法，通过将解编码为[0,1)区间内的随机密钥向量，并利用解码器映射到问题解空间。算法通过精英保留、突变和交叉操作迭代优化解。


<details>
  <summary>Details</summary>
Motivation: 提出一种通用框架，使遗传操作能在单位超立方体内进行，增强算法的生产力和可维护性，适用于多种优化问题。

Method: 1) 初始化P个随机密钥向量种群 2) 每代划分精英/非精英解 3) 精英直接保留 4) 添加突变个体 5) 剩余个体通过参数化均匀交叉生成。

Result: 开发了高效的偏置随机密钥遗传算法变体，维持了框架通用性的同时提升优化性能。

Conclusion: 随机密钥编码提供统一优化框架，偏置变种进一步提高了算法有效性，适用于广泛离散优化问题。

Abstract: A random-key genetic algorithm is an evolutionary metaheuristic for discrete
and global optimization. Each solution is encoded as a vector of N random keys,
where a random key is a real number randomly generated in the continuous
interval [0, 1). A decoder maps each vector of random keys to a solution of the
optimization problem being solved and computes its cost. The benefit of this
approach is that all genetic operators and transformations can be maintained
within the unitary hypercube, regardless of the problem being addressed. This
enhances the productivity and maintainability of the core framework. The
algorithm starts with a population of P vectors of random keys. At each
iteration, the vectors are partitioned into two sets: a smaller set of
high-valued elite solutions and the remaining non-elite solutions. All elite
elements are copied, without change, to the next population. A small number of
random-key vectors (the mutants) is added to the population of the next
iteration. The remaining elements of the population of the next iteration are
generated by combining, with the parametrized uniform crossover of Spears and
DeJong (1991), pairs of solutions. This chapter reviews random-key genetic
algorithms and describes an effective variant called biased random-key genetic
algorithms.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [427] [Labelling Data with Unknown References](https://arxiv.org/abs/2506.03083)
*Adrian de Wynter*

Main category: cs.DS

TL;DR: 论文提出了一种无需参考数据的算法（'No-Data Algorithm'），通过连续挑战评估者来验证其可信度。


<details>
  <summary>Details</summary>
Motivation: 在没有标注参考数据的情况下，无法通过测试或假设来验证评估者的可信度，因此需要一种新的方法。

Method: 引入'No-Data Algorithm'，通过连续向评估者提出挑战来验证其可信度。

Result: 算法能够在大概率下验证评估者的可信度，接受真正了解标注方法的评估者，并标记不可信的评估者。

Conclusion: 该方法为无参考数据情况下验证评估者可信度提供了有效解决方案，并通过形式化证明和有限实验验证了其正确性。

Abstract: An evaluator is trustworthy when there exists some agreed-upon way to measure
its performance as a labeller. The two ways to establish trustworthiness are
either by testing it, or by assuming the evaluator `knows' somehow the way to
label the corpus. However, if labelled references (e.g., a development set) are
unavailable, neither of these approaches work: the former requires the data,
and the latter is an assumption, not evidence. To address this, we introduce an
algorithm (the `No-Data Algorithm') by which to establish trust in an evaluator
without any existing references. Our algorithm works by successively posing
challenges to said evaluator. We show that this is sufficient to establish
trustworthiness w.h.p., in such a way that when the evaluator actually knows
the way to label the corpus, the No-Data Algorithm accepts its output; and,
conversely, flags untrustworthy evaluators when these are unable to prove it.
We present formal proofs of correctness and limited experiments.

</details>
