<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 103]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.LG](#cs.LG) [Total: 53]
- [stat.ML](#stat.ML) [Total: 9]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [stat.OT](#stat.OT) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.SE](#cs.SE) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [physics.optics](#physics.optics) [Total: 1]
- [eess.SP](#eess.SP) [Total: 3]
- [cs.DL](#cs.DL) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.CV](#cs.CV) [Total: 26]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.IR](#cs.IR) [Total: 11]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 6]
- [cs.NI](#cs.NI) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Efficient Multilingual ASR Finetuning via LoRA Language Experts](https://arxiv.org/abs/2506.21555)
*Jiahong Li,Yiwen Shao,Jianheng Zhuo,Chenda Li,Liliang Tang,Dong Yu,Yanmin Qian*

Main category: cs.CL

TL;DR: 该论文通过基于Whisper的LoRA语言专家微调框架，提升了多语言自动语音识别的性能，相比标准微调方法在目标语言上表现更优。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在多语言自动语音识别（ASR）方面取得了进展，但不同语言之间的相互干扰问题仍然存在，影响了模型在多语言环境下的识别效果。

Method: 提出了一种基于Whisper的LoRA语言专家微调框架，通过LoRA专家融合或知识蒸馏来优化多语言ASR模型。

Result: 实验结果表明，该方法在语言感知和语言无关场景下分别实现了约10%和15%的相对性能提升。

Conclusion: 该论文提出的方法有效解决了多语言ASR中的语言干扰问题，显著提升了识别性能。

Abstract: Recent advancements in deep learning have significantly enhanced multilingual
automatic speech recognition (ASR) due to the development of advanced model
architectures and available large-scale multilingual datasets. Despite that,
multilingual ASR still suffers from the curse of multilinguality in that
different languages tend to interfere with each other, making it difficult for
the ASR model to identify multiple languages effectively while sharing model
capacity across them. This paper proposes an efficient finetuning framework for
customized multilingual ASR via prepared LoRA language experts based on
Whisper. Through LoRA expert fusion or knowledge distillation, our approach
achieves better recognition performance on target languages than standard
fine-tuning methods. Experimental results demonstrate that the proposed models
yield approximately 10\% and 15\% relative performance gains in language-aware
and language-agnostic scenarios, respectively.

</details>


### [2] [VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for Retrieval-Augmented Generation](https://arxiv.org/abs/2506.21556)
*Hyeongcheol Park,MinHyuk Jang,Ha Dam Baek,Gyusam Chang,Jiyoung Seo,Jiwan Park,Hogun Park,Sangpil Kim*

Main category: cs.CL

TL;DR: 提出首个涵盖视觉、音频和文本的多模态知识图谱VAT-KG，通过严格对齐步骤自动构建，并开发新型多模态RAG框架提升MLLMs的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有MMKGs知识覆盖有限且模态单一，无法满足多模态大模型对视频、音频等丰富模态的需求，亟需构建更全面的知识图谱。

Method: 设计概念中心化的VAT-KG构建流程，通过跨模态对齐和严格过滤实现自动生成；开发支持任意模态查询的概念级检索增强生成框架。

Result: 跨模态问答实验验证VAT-KG能有效支持MLLMs，展现其在统一和利用多模态知识方面的实用价值。

Conclusion: VAT-KG填补了多模态知识图谱的空白，为处理复杂多模态任务提供了可扩展的知识基础设施。

Abstract: Multimodal Knowledge Graphs (MMKGs), which represent explicit knowledge
across multiple modalities, play a pivotal role by complementing the implicit
knowledge of Multimodal Large Language Models (MLLMs) and enabling more
grounded reasoning via Retrieval Augmented Generation (RAG). However, existing
MMKGs are generally limited in scope: they are often constructed by augmenting
pre-existing knowledge graphs, which restricts their knowledge, resulting in
outdated or incomplete knowledge coverage, and they often support only a narrow
range of modalities, such as text and visual information. These limitations
reduce their extensibility and applicability to a broad range of multimodal
tasks, particularly as the field shifts toward richer modalities such as video
and audio in recent MLLMs. Therefore, we propose the Visual-Audio-Text
Knowledge Graph (VAT-KG), the first concept-centric and knowledge-intensive
multimodal knowledge graph that covers visual, audio, and text information,
where each triplet is linked to multimodal data and enriched with detailed
descriptions of concepts. Specifically, our construction pipeline ensures
cross-modal knowledge alignment between multimodal data and fine-grained
semantics through a series of stringent filtering and alignment steps, enabling
the automatic generation of MMKGs from any multimodal dataset. We further
introduce a novel multimodal RAG framework that retrieves detailed
concept-level knowledge in response to queries from arbitrary modalities.
Experiments on question answering tasks across various modalities demonstrate
the effectiveness of VAT-KG in supporting MLLMs, highlighting its practical
value in unifying and leveraging multimodal knowledge.

</details>


### [3] [Debunk and Infer: Multimodal Fake News Detection via Diffusion-Generated Evidence and LLM Reasoning](https://arxiv.org/abs/2506.21557)
*Kaiying Yan,Moyang Liu,Yukun Liu,Ruibo Fu,Zhengqi Wen,Jianhua Tao,Xuefei Liu*

Main category: cs.CL

TL;DR: 提出DIFND框架，结合条件扩散模型和多模态大语言模型，通过生成反驳证据和逻辑推理提升假新闻检测性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 多媒体平台上假新闻的快速传播对信息可信度构成严重挑战，需提升检测性能和可解释性。

Method: DIFND框架整合条件扩散模型的生成能力和多模态大语言模型的协作推理能力，采用debunk扩散生成证据，并通过多代理MLLM系统进行逻辑推理和真实性判断。

Result: 在FakeSV和FVC数据集上的实验表明，DIFND在检测准确率上显著优于现有方法，并提供可信决策。

Conclusion: DIFND通过联合建模多模态特征、生成反驳线索和丰富推理验证，有效提升了假新闻检测的性能和可解释性。

Abstract: The rapid spread of fake news across multimedia platforms presents serious
challenges to information credibility. In this paper, we propose a
Debunk-and-Infer framework for Fake News Detection(DIFND) that leverages
debunking knowledge to enhance both the performance and interpretability of
fake news detection. DIFND integrates the generative strength of conditional
diffusion models with the collaborative reasoning capabilities of multimodal
large language models (MLLMs). Specifically, debunk diffusion is employed to
generate refuting or authenticating evidence based on the multimodal content of
news videos, enriching the evaluation process with diverse yet semantically
aligned synthetic samples. To improve inference, we propose a chain-of-debunk
strategy where a multi-agent MLLM system produces logic-grounded,
multimodal-aware reasoning content and final veracity judgment. By jointly
modeling multimodal features, generative debunking cues, and reasoning-rich
verification within a unified architecture, DIFND achieves notable improvements
in detection accuracy. Extensive experiments on the FakeSV and FVC datasets
show that DIFND not only outperforms existing approaches but also delivers
trustworthy decisions.

</details>


### [4] [Bench to the Future: A Pastcasting Benchmark for Forecasting Agents](https://arxiv.org/abs/2506.21558)
*FutureSearch,:,Jack Wildman,Nikos I. Bosse,Daniel Hnyk,Peter Mühlbacher,Finn Hambly,Jon Evans,Dan Schwarz,Lawrence Phillips*

Main category: cs.CL

TL;DR: 论文提出了一个名为BTF的'过去预测'基准测试，用于评估LLM在已知结果事件上的预测能力，通过大量历史网页数据模拟真实预测环境。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏一个真实、封闭且可重复的预测基准来评估AI系统的预测能力，尤其是LLM在需要大量网络研究和时间验证的预测任务中的表现。

Method: 构建BTF基准，包含数百个已知答案的高质量问题，并配套数万相关网页的离线语料库，模拟LLM对历史事件的'预测'过程。

Result: 实验表明，该过去预测环境产生的效果可与实时未解决事件的互联网预测结果相媲美，并能追踪LLM预测能力的持续进步。

Conclusion: BTF是一个持续更新的动态基准，旨在为研究者提供评估LLM预测能力的工具，并欢迎研究社区共同使用和完善。

Abstract: Forecasting is a challenging task that offers a clearly measurable way to
study AI systems. Forecasting requires a large amount of research on the
internet, and evaluations require time for events to happen, making the
development of forecasting benchmarks challenging. To date, no forecasting
benchmark provides a realistic, hermetic, and repeatable environment for LLM
forecasters. We introduce Bench To the Future (BTF), a "pastcasting" benchmark
with hundreds of high-quality questions for which the resolution is already
known. Each question is accompanied by a large offline corpus of tens of
thousands of relevant web pages, enabling a way to elicit realistic "forecasts"
on past events from LLMs. Results suggest that our pastcasting environment can
produce results comparable to those based on forecasts using the internet on
at-the-time unresolved questions. We show results benchmarking agent and
chain-of-thought forecasting approaches using several LLMs, including the
recently-released Claude 4 models, and demonstrate BTF's ability to track
steady forecasting capability progress over time. We intend this to be a living
benchmark, with new questions added continually to account for increasing
training data cutoff dates. We invite researchers to contact us at
hello@futuresearch.ai to utilize our benchmark or tooling for their own
research.

</details>


### [5] [GraphLAMA: Enabling Efficient Adaptation of Graph Language Models with Limited Annotations](https://arxiv.org/abs/2506.21559)
*Junze Chen,Cheng Yang,Shujie Li,Zhiqiang Zhang,Yawen Li,Junping Du,Chuan Shi*

Main category: cs.CL

TL;DR: 论文提出GraphLAMA方法，通过参数适应阶段优化图语言模型（GLMs），在少量标注数据下提升预测精度和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有图语言模型（GLMs）存在固定参数导致的效率问题，以及指令调优需要大量标注数据的限制。论文旨在通过参数适应阶段解决这些问题。

Method: 提出GraphLAMA方法，使用图神经网络（GNN）将节点转换为LLM标记的表示空间，通过预训练和适应阶段优化模型参数。

Result: GraphLAMA在少样本/零样本节点分类和摘要生成任务中表现优异，准确率提升4.91%，推理速度比ICL快10倍。

Conclusion: GraphLAMA通过高效调优和推理，显著提升了图语言模型的性能，适用于实际场景中的少样本学习任务。

Abstract: Large language models (LLMs) have demonstrated their strong capabilities in
various domains, and have been recently integrated for graph analysis as graph
language models (GLMs). With LLMs as the predictor, some GLMs can interpret
unseen tasks described by natural language, and learn from a few examples in
the prompts without parameter tuning, known as in-context learning (ICL).
Another subset of GLMs utilizes abundant training labels to enhance model
performance, known as instruction tuning. However, we argue that ICL on graphs
has effectiveness issues due to fixed parameters and efficiency issues due to
long context. Meanwhile, the large amount of labeled data required for
instruction tuning can be difficult to obtain in real-world scenarios. To this
end, we aim to introduce an extra parameter adaptation stage that can
efficiently tailor GLMs to an unseen graph and task with only a few labeled
examples, in exchange for better prediction accuracy and faster inference
speed. For implementation, in this paper we propose GraphLAMA method, with its
model backbone and learning schemes specialized for efficient tuning and
inference. Specifically, for model backbone, we use a graph neural network
(GNN) with several well-designed components to transform nodes into the
representation space of LLM tokens. Task instructions can then be represented
as a mixture of node and language tokens. In the pre-training stage, model
parameters except the LLM will be trained with different tasks to capture
general knowledge. In the adaptation stage, only a few pre-trained parameters
will be updated based on few-shot examples. Extensive experiments on
few/zero-shot node classification and summary generation show that our proposed
GraphLAMA achieves state-of-the-art performance with 4.91% absolution
improvement in accuracy. Compared with ICL, our inference speed can be 10 times
faster under 5-shot setting.

</details>


### [6] [Reinforcement Learning Fine-Tuning of Language Model for Instruction Following and Math Reasoning](https://arxiv.org/abs/2506.21560)
*Yifu Han,Geo Zhang*

Main category: cs.CL

TL;DR: 该研究比较了三种强化学习微调方法在小型语言模型上的效果，发现RLOO结合DeBERTa奖励模型表现最佳，DPO效果稳定，数学任务中数据增强和验证器能显著提升准确性。


<details>
  <summary>Details</summary>
Motivation: 探索如何在轻量级语言模型（Qwen2.5-0.5B Base）上通过强化学习微调技术提升指令跟随和数学推理能力，为资源受限场景提供实用方案。

Method: 对比监督微调（SFT）、直接偏好优化（DPO）和基于奖励模型的RLOO方法，并在数学任务中测试合成数据增强和验证器采样。

Result: RLOO+DeBERTa奖励模型对齐效果最优，DPO表现稳定；数学任务中数据增强和验证器采样显著提升准确率。

Conclusion: 研究揭示了轻量级模型微调的关键权衡，证明了结合微调和推理工具的实用性，为小模型部署提供有效策略。

Abstract: This study investigates the effectiveness of reinforcement learning (RL)
fine-tuning techniques on a compact language model (Qwen2.5-0.5B Base) for two
challenging tasks: instruction following and mathematical reasoning. We compare
supervised fine-tuning (SFT), Direct Preference Optimization (DPO) using
preference-labeled data, and Reinforce Leave-One-Out (RLOO) with reward models.
Our experiments show that RLOO with DeBERTa reward modeling achieves the best
alignment, while DPO provides strong and consistent results. For math reasoing
tasks, synthetic data augmentation and best-of-N sampling with an external
verifier significantly improve accuracy, showing the potential of combining
fine-tuning with inference-time tools. This study highlights key trade-offs and
practical strategies for training lightweight, task-aligned small-scale
language models.

</details>


### [7] [Reasoning Isn't Enough: Examining Truth-Bias and Sycophancy in LLMs](https://arxiv.org/abs/2506.21561)
*Emilio Barkett,Olivia Long,Madhavendra Thakur*

Main category: cs.CL

TL;DR: 研究发现，推理模型在真实性判断上优于非推理模型，但仍有较高真值偏差，且部分先进模型存在阿谀倾向，能力提升未能解决根本问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）广泛应用于事实核查和决策，但其作为真实性判断工具的能力仍不明确，本研究旨在评估其真实性检测能力。

Method: 研究让8个LLMs进行4800次真实性判断，比较推理与非推理模型在不同提示下的表现。

Result: 推理模型的真值偏差低于非推理模型，但仍高于人类基准；部分先进模型（如OpenAI的o4-mini、GPT-4.1和DeepSeek的R1）存在检测准确率不对称问题。

Conclusion: LLMs的能力进步未能解决真实性检测的根本挑战，尤其是高级模型的阿谀倾向问题需引起重视。

Abstract: Despite their widespread use in fact-checking, moderation, and high-stakes
decision-making, large language models (LLMs) remain poorly understood as
judges of truth. This study presents the largest evaluation to date of LLMs'
veracity detection capabilities and the first analysis of these capabilities in
reasoning models. We had eight LLMs make 4,800 veracity judgments across
several prompts, comparing reasoning and non-reasoning models. We find that
rates of truth-bias, or the likelihood to believe a statement is true,
regardless of whether it is actually true, are lower in reasoning models than
in non-reasoning models, but still higher than human benchmarks. Most
concerning, we identify sycophantic tendencies in several advanced models
(o4-mini and GPT-4.1 from OpenAI, R1 from DeepSeek), which displayed an
asymmetry in detection accuracy, performing well in truth accuracy but poorly
in deception accuracy. This suggests that capability advances alone do not
resolve fundamental veracity detection challenges in LLMs.

</details>


### [8] [FloorPlan-DeepSeek (FPDS): A multimodal approach to floorplan generation using vector-based next room prediction](https://arxiv.org/abs/2506.21562)
*Jun Yin,Pengyu Zeng,Jing Zhong,Peilin Li,Miao Zhang,Ran Luo,Shuai Lu*

Main category: cs.CL

TL;DR: 提出了一种基于'下一个房间预测'的渐进式生成方法FPDS，用于建筑平面图设计，性能优于现有端到端模型。


<details>
  <summary>Details</summary>
Motivation: 现有建筑平面图生成模型多为端到端一次性生成，与现实中渐进迭代的设计流程不匹配。

Method: 受语言模型'下一个词预测'启发，提出'下一个房间预测'的渐进生成范式。

Result: FPDS在文本到平面图任务中表现优于扩散模型和Tell2Design。

Conclusion: 该方法有望支持未来智能建筑设计，更符合实际工作流程。

Abstract: In the architectural design process, floor plan generation is inherently
progressive and iterative. However, existing generative models for floor plans
are predominantly end-to-end generation that produce an entire pixel-based
layout in a single pass. This paradigm is often incompatible with the
incremental workflows observed in real-world architectural practice. To address
this issue, we draw inspiration from the autoregressive 'next token prediction'
mechanism commonly used in large language models, and propose a novel 'next
room prediction' paradigm tailored to architectural floor plan modeling.
Experimental evaluation indicates that FPDS demonstrates competitive
performance in comparison to diffusion models and Tell2Design in the
text-to-floorplan task, indicating its potential applicability in supporting
future intelligent architectural design.

</details>


### [9] [FormosanBench: Benchmarking Low-Resource Austronesian Languages in the Era of Large Language Models](https://arxiv.org/abs/2506.21563)
*Kaiying Kevin Lin,Hsiyu Chen,Haopeng Zhang*

Main category: cs.CL

TL;DR: 论文提出首个评估大语言模型在低资源南岛语系（台湾原住民语言）表现的基准FORMOSANBENCH，发现现有模型表现显著落后，呼吁加强濒危语言支持。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在高资源语言表现优异，但对台湾濒危南岛语系（如泰雅语、阿美语、排湾语）的研究严重不足，需建立评估基准。

Method: 构建FORMOSANBENCH基准，涵盖机器翻译、语音识别和文本摘要三项任务，测试零样本、10样本和微调场景下的模型表现。

Result: 模型在所有任务中表现显著低于高资源语言，少量样本学习和微调仅带来有限提升，存在巨大性能差距。

Conclusion: 亟需开发更具包容性的NLP技术以支持濒危语言，作者开源数据集和代码推动相关研究。

Abstract: While large language models (LLMs) have demonstrated impressive performance
across a wide range of natural language processing (NLP) tasks in high-resource
languages, their capabilities in low-resource and minority languages remain
significantly underexplored. Formosan languages -- a subgroup of Austronesian
languages spoken in Taiwan -- are both linguistically rich and endangered,
largely due to the sociolinguistic dominance of Mandarin. In this work, we
introduce FORMOSANBENCH, the first benchmark for evaluating LLMs on
low-resource Austronesian languages. It covers three endangered Formosan
languages: Atayal, Amis, and Paiwan, across three core NLP tasks: machine
translation, automatic speech recognition (ASR), and text summarization. We
assess model performance in zero-shot, 10-shot, and fine-tuned settings using
FORMOSANBENCH. Our results reveal a substantial performance gap between
high-resource and Formosan languages. Existing LLMs consistently underperform
across all tasks, with 10-shot learning and fine-tuning offering only limited
improvements. These findings underscore the urgent need for more inclusive NLP
technologies that can effectively support endangered and underrepresented
languages. We release our datasets and code to facilitate future research in
this direction.

</details>


### [10] [Team QUST at SemEval-2025 Task 10: Evaluating Large Language Models in Multiclass Multi-label Classification of News Entity Framing](https://arxiv.org/abs/2506.21564)
*Jiyan Liu,Youzheng Liu,Taihang Wang,Xiaoman Xu,Yimin Wang,Ye Jiang*

Main category: cs.CL

TL;DR: QUST_NLP团队提出三阶段检索框架用于事实核查声明检索，在SemEval-2025任务7中取得单语第五、跨语言第七的成绩。


<details>
  <summary>Details</summary>
Motivation: 旨在优化事实核查声明的检索效果，通过多阶段方法提升检索准确性和效率。

Method: 采用三阶段框架：1) 评估并选择最佳检索模型；2) 使用多个重排序模型优化候选结果；3) 加权投票确定最终结果。

Result: 在SemEval-2025任务7中，单语赛道排名第五，跨语言赛道排名第七。

Conclusion: 提出的三阶段框架有效提升了事实核查声明的检索性能，代码已开源。

Abstract: This paper describes the participation of QUST_NLP in the SemEval-2025 Task
7. We propose a three-stage retrieval framework specifically designed for
fact-checked claim retrieval. Initially, we evaluate the performance of several
retrieval models and select the one that yields the best results for candidate
retrieval. Next, we employ multiple re-ranking models to enhance the candidate
results, with each model selecting the Top-10 outcomes. In the final stage, we
utilize weighted voting to determine the final retrieval outcomes. Our approach
achieved 5th place in the monolingual track and 7th place in the crosslingual
track. We release our system code at:
https://github.com/warmth27/SemEval2025_Task7.

</details>


### [11] [A Multi-Agent Probabilistic Inference Framework Inspired by Kairanban-Style CoT System with IdoBata Conversation for Debiasing](https://arxiv.org/abs/2506.21565)
*Takato Ueno,Keito Inoshita*

Main category: cs.CL

TL;DR: 该研究受日本传统沟通方式启发，提出结合多LLM的KCS+IBC框架，用于情感分析中的偏见缓解和预测改进。


<details>
  <summary>Details</summary>
Motivation: 受日本'回覧板'和'井端会议'等传统社区沟通方式启发，旨在通过多智能体对话实现更平衡、可解释的情感分析。

Method: 提出KCS+IBC框架：整合多个LLM进行序列化预测，并加入中期非正式对话环节，结合概率情感预测方法。

Result: KCS达到单LLM准确度，KCS+IBC在推理后期熵值持续降低、方差渐增，显示其平衡预测聚合与多样性的能力。

Conclusion: 该框架展现了偏见校正潜力，未来将定量评估其对偏见修正的影响，并开发更先进的情感分析系统。

Abstract: Japan's kairanban culture and idobata conversations have long functioned as
traditional communication practices that foster nuanced dialogue among
community members and contribute to the formation of social balance. Inspired
by these information exchange processes, this study proposes a multi-agent
inference framework (KCS+IBC) that integrates multiple large language models
(LLMs) to achieve bias mitigation, improved explainability, and probabilistic
prediction in sentiment analysis. In addition to sequentially sharing
prediction results, the proposed method incorporates a mid-phase casual
dialogue session to blend formal inference with individual perspectives and
introduces probabilistic sentiment prediction. Experimental results show that
KCS achieves accuracy comparable to that of a single LLM across datasets, while
KCS+IBC exhibits a consistent decrease in entropy and a gradual increase in
variance during the latter stages of inference, suggesting the framework's
ability to balance aggregation and diversity of predictions. Future work will
quantitatively assess the impact of these characteristics on bias correction
and aim to develop more advanced sentiment analysis systems.

</details>


### [12] [The Saturation Point of Backtranslation in High Quality Low Resource English Gujarati Machine Translation](https://arxiv.org/abs/2506.21566)
*Arwa Arif*

Main category: cs.CL

TL;DR: 该论文探讨了反向翻译（BT）在英语-古吉拉特语低资源机器翻译中的效果，发现添加合成数据并未提升性能，甚至略有下降。


<details>
  <summary>Details</summary>
Motivation: 研究反向翻译在高品质、低资源语言对（如英语-古吉拉特语）中的有效性，尤其是在已有高质量平行语料库的情况下。

Method: 使用MBART50预训练模型，基于约5万句对的高质量平行语料库训练基线系统，并通过单语古吉拉特语文本生成反向翻译数据增强训练集。

Result: 添加反向翻译数据后，翻译性能未提升（BLEU 43.8），部分情况下甚至略微下降。多指标（BLEU、ChrF++、TER、BLEURT）分析显示性能饱和。

Conclusion: 在特定低资源场景中，反向翻译可能达到收益递减点，需进一步研究其适用条件。

Abstract: Backtranslation BT is widely used in low resource machine translation MT to
generate additional synthetic training data using monolingual corpora. While
this approach has shown strong improvements for many language pairs, its
effectiveness in high quality, low resource settings remains unclear. In this
work, we explore the effectiveness of backtranslation for English Gujarati
translation using the multilingual pretrained MBART50 model. Our baseline
system, trained on a high quality parallel corpus of approximately 50,000
sentence pairs, achieves a BLEU score of 43.8 on a validation set. We augment
this data with carefully filtered backtranslated examples generated from
monolingual Gujarati text. Surprisingly, adding this synthetic data does not
improve translation performance and, in some cases, slightly reduces it. We
evaluate our models using multiple metrics like BLEU, ChrF++, TER, BLEURT and
analyze possible reasons for this saturation. Our findings suggest that
backtranslation may reach a point of diminishing returns in certain
low-resource settings and we discuss implications for future research.

</details>


### [13] [BioPars: A Pretrained Biomedical Large Language Model for Persian Biomedical Text Mining](https://arxiv.org/abs/2506.21567)
*Baqer M. Merzah,Tania Taami,Salman Asoudeh,Amir reza Hossein pour,Saeed Mirzaee,Amir Ali Bengari*

Main category: cs.CL

TL;DR: 论文介绍了BioPars，一个用于评估大语言模型在生物信息学任务中表现的工具，并在波斯医学问答中取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索大语言模型在生命科学中的应用，特别是在生物信息学和波斯医学问答中的潜力。

Method: 研究引入了BIOPARS-BENCH数据集和BioParsQA评估工具，比较了ChatGPT、Llama和Galactica在生物信息学任务中的表现。

Result: BioPars在波斯医学问答中表现优异，ROUGE-L得分29.99，优于GPT-4 1.0，BERTScore达90.87。

Conclusion: 研究表明大语言模型在生物信息学中有潜力，但需进一步微调以应对更高层次的问题。

Abstract: Large Language Models (LLMs) have recently gained attention in the life
sciences due to their capacity to model, extract, and apply complex biological
information. Beyond their classical use as chatbots, these systems are
increasingly used for complex analysis and problem-solving in specialized
fields, including bioinformatics. First, we introduce BIOPARS-BENCH, a dataset
from over 10,000 scientific articles, textbooks, and medical websites.
BioParsQA was also introduced to evaluate the proposed model, which consists of
5,231 Persian medical questions and answers. This study then introduces
BioPars, a simple but accurate measure designed to assess LLMs for three main
abilities: acquiring subject-specific knowledge, interpreting and synthesizing
such knowledge, and demonstrating proper evidence. Comparing ChatGPT, Llama,
and Galactica, our study highlights their ability to remember and retrieve
learned knowledge but also reveals shortcomings in addressing higher-level,
real-world questions and fine-grained inferences. These findings indicate the
need for further fine-tuning to address the capabilities of LLM in
bioinformatics tasks. To our knowledge, BioPars is the first application of LLM
in Persian medical QA, especially for generating long answers. Evaluation of
four selected medical QA datasets shows that BioPars has achieved remarkable
results compared to comparative approaches. The model on BioParsQA achieved a
ROUGE-L score of 29.99, which is an improvement over GPT-4 1.0. The model
achieved a BERTScore of 90.87 with the MMR method. The MoverScore and BLEURT
values were also higher in this model than the other three models. In addition,
the reported scores for the model are MoverScore=60.43 and BLEURT=50.78.
BioPars is an ongoing project and all resources related to its development will
be made available via the following GitHub repository:
https://github.com/amirap80/BioPars.

</details>


### [14] [Assessing RAG and HyDE on 1B vs. 4B-Parameter Gemma LLMs for Personal Assistants Integretion](https://arxiv.org/abs/2506.21568)
*Andrejs Sorstkins*

Main category: cs.CL

TL;DR: 该研究评估了RAG和HyDE两种增强策略在小型Gemma LLMs上的效果，发现RAG在减少延迟和避免幻觉方面表现更优，适合边缘设备上的个人助手应用。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决大型语言模型在边缘和隐私敏感应用中的资源效率问题，探索如何在小型模型上实现高效、准确的响应。

Method: 使用1B和4B参数的Gemma LLMs，结合MongoDB和Qdrant实现短期记忆和长期语义存储，通过FastAPI和LangChain协调，并通过React.js前端展示系统。

Result: RAG在两种规模的模型上均能减少高达17%的延迟并消除事实幻觉，而HyDE虽提升语义相关性但增加25-40%的响应时间并伴随幻觉问题。

Conclusion: 对于小型LLMs驱动的设备端个人助手，RAG是更实用的选择，因其在延迟和准确性上的优势。

Abstract: Resource efficiency is a critical barrier to deploying large language models
(LLMs) in edge and privacy-sensitive applications. This study evaluates the
efficacy of two augmentation strategies--Retrieval-Augmented Generation (RAG)
and Hypothetical Document Embeddings (HyDE)--on compact Gemma LLMs of 1 billion
and 4 billion parameters, within the context of a privacy-first personal
assistant. We implement short-term memory via MongoDB and long-term semantic
storage via Qdrant, orchestrated through FastAPI and LangChain, and expose the
system through a React.js frontend. Across both model scales, RAG consistently
reduces latency by up to 17\% and eliminates factual hallucinations when
responding to user-specific and domain-specific queries. HyDE, by contrast,
enhances semantic relevance--particularly for complex physics prompts--but
incurs a 25--40\% increase in response time and a non-negligible hallucination
rate in personal-data retrieval. Comparing 1 B to 4 B models, we observe that
scaling yields marginal throughput gains for baseline and RAG pipelines, but
magnifies HyDE's computational overhead and variability. Our findings position
RAG as the pragmatic choice for on-device personal assistants powered by
small-scale LLMs.

</details>


### [15] [Hybrid-NL2SVA: Integrating RAG and Finetuning for LLM-based NL2SVA](https://arxiv.org/abs/2506.21569)
*Weihua Xiao,Derek Ekberg,Siddharth Garg,Ramesh Karri*

Main category: cs.CL

TL;DR: 该论文提出了一种定制化的检索增强生成（RAG）框架和合成微调数据集，以提高大型语言模型（LLM）在将自然语言属性描述转换为SystemVerilog断言（NL2SVA）任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 手动编写SystemVerilog断言（SVAs）是一项劳动密集且容易出错的任务，而现有的大型语言模型在理解领域特定语法和语义方面仍存在困难。

Method: 论文提出了一种定制化的检索增强生成（RAG）框架和合成微调数据集，通过提供提示引导的解释，教导LLM逐步构建并发SVAs，从而提升语法和功能准确性。

Result: 实验结果表明，定制化的RAG框架将功能匹配的SVAs数量提高了58.42%，而经过微调的Qwen2.5-Coder-7B-Instruct模型结合HybridRetrieval技术，性能提升了59.05%。

Conclusion: 通过定制化的RAG框架和微调数据集，论文显著提升了LLM在NL2SVA任务中的性能，为硬件设计验证提供了更高效的自动化解决方案。

Abstract: SystemVerilog Assertions (SVAs) are critical for verifying the correctness of
hardware designs, but manually writing them from natural language property
descriptions, i.e., NL2SVA, remains a labor-intensive and error-prone task.
Recent advances in large language models (LLMs) offer opportunities to automate
this translation. However, existing models still struggle with understanding
domain-specific syntax and semantics. To enhance LLM performance in NL2SVA, we
propose a customized retrieval-augmented generation (RAG) framework and a
synthetic fine-tuning dataset that together improve LLM's performance. To
further improve lightweight models over NL2SVA, our fine-tuning dataset
provides prompt-guided explanations that teach LLMs the layer-by-layer
construction process of concurrent SVAs, enabling supervised fine-tuning that
greatly improves syntax and functionality accuracy. To evaluate the performance
of LLMs over NL2SVA, we construct the largest evaluation dataset for NL2SVA,
comprising 40 Verilog designs and 229 formally verified SVAs with detailed
annotations. Experimental results show that our customized RAG framework
increases the number of functionality matched SVAs by 58.42% over GPT-4o-mini,
while Qwen2.5-Coder-7B-Instruct fine-tuned on our fine-tuning dataset and
integrated with HybridRetrieval achieves a 59.05% over the base Qwen model.

</details>


### [16] [Random Initialization Can't Catch Up: The Advantage of Language Model Transfer for Time Series Forecasting](https://arxiv.org/abs/2506.21570)
*Roland Riachi,Kashif Rasul,Arjun Ashok,Prateek Humane,Alexis Roger,Andrew R. Williams,Yuriy Nevmyvaka,Irina Rish*

Main category: cs.CL

TL;DR: 该论文分析了预训练语言模型在低数据量时间序列预测中的有效迁移，探讨了不同设计选择的影响，并发现验证损失持续下降的现象。


<details>
  <summary>Details</summary>
Motivation: 研究预训练语言模型在低数据量时间序列预测中的有效性，并探讨不同设计选择对模型性能的影响。

Method: 通过分析上游后训练、时间序列分词器和语言模型主干大小等设计选择，评估其在时间序列预测中的表现。

Result: 在低数据量情况下，设计选择对验证损失有显著影响，且语言模型的验证损失在随机初始化模型收敛后仍持续下降。

Conclusion: 这些发现不仅有助于理解高效计算训练在时间序列中的应用，还为研究模型利用数据分布的模态无关特性开辟了道路。

Abstract: Recent works have demonstrated the effectiveness of adapting pre-trained
language models (LMs) for forecasting time series in the low-data regime. We
build upon these findings by analyzing the effective transfer from language
models to time series forecasting under various design choices including
upstream post-training, time series tokenizer and language backbone size. In
the low-data regime, these design choices have a significant impact on the
validation loss, with clear-cut choices that outperform others. Contrary to
Hernandez et al. (2021), we observe that the validation loss of the LMs
continues to smoothly decrease long after the validation loss of the randomly
initialized models has converged, leading to a non-vanishing transfer gap that
holds across design choices. These findings not only help shed light on the
effective use of compute-efficient training for time series, but also open the
way for the study of modality-agnostic properties of data distributions
leveraged by these models.

</details>


### [17] [Towards Understanding the Cognitive Habits of Large Reasoning Models](https://arxiv.org/abs/2506.21571)
*Jianshuo Dong,Yujia Fu,Chuanrui Hu,Chao Zhang,Han Qiu*

Main category: cs.CL

TL;DR: 该论文通过CogTest基准评估大型推理模型（LRMs）是否展现类似人类的认知习惯，发现LRMs能自适应地应用这些习惯，并与安全风险相关。


<details>
  <summary>Details</summary>
Motivation: 研究大型推理模型（LRMs）是否表现出类似人类的认知习惯，以更好地理解和监控模型行为。

Method: 引入CogTest基准，包含16种认知习惯和25个多样化任务，采用证据优先的提取方法评估13个LRMs和3个非推理模型。

Result: LRMs展现出类似人类的认知习惯，并能根据不同任务自适应地应用；某些习惯（如“承担负责任的风险”）与生成有害响应强相关。

Conclusion: 研究LRMs的持续行为模式有助于深入理解模型的不良行为，为模型监控提供新方向。

Abstract: Large Reasoning Models (LRMs), which autonomously produce a reasoning Chain
of Thought (CoT) before producing final responses, offer a promising approach
to interpreting and monitoring model behaviors. Inspired by the observation
that certain CoT patterns -- e.g., ``Wait, did I miss anything?'' --
consistently emerge across tasks, we explore whether LRMs exhibit human-like
cognitive habits. Building on Habits of Mind, a well-established framework of
cognitive habits associated with successful human problem-solving, we introduce
CogTest, a principled benchmark designed to evaluate LRMs' cognitive habits.
CogTest includes 16 cognitive habits, each instantiated with 25 diverse tasks,
and employs an evidence-first extraction method to ensure reliable habit
identification. With CogTest, we conduct a comprehensive evaluation of 16
widely used LLMs (13 LRMs and 3 non-reasoning ones). Our findings reveal that
LRMs, unlike conventional LLMs, not only exhibit human-like habits but also
adaptively deploy them according to different tasks. Finer-grained analyses
further uncover patterns of similarity and difference in LRMs' cognitive habit
profiles, particularly certain inter-family similarity (e.g., Qwen-3 models and
DeepSeek-R1). Extending the study to safety-related tasks, we observe that
certain habits, such as Taking Responsible Risks, are strongly associated with
the generation of harmful responses. These findings suggest that studying
persistent behavioral patterns in LRMs' CoTs is a valuable step toward deeper
understanding of LLM misbehavior. The code is available at:
https://github.com/jianshuod/CogTest.

</details>


### [18] [Aligning MLLM Benchmark With Human Preferences via Structural Equation Modeling](https://arxiv.org/abs/2506.21572)
*Tianyu. Zou,Shengwu. Xiong,Ruilin. Yao,Jirui. Huang,Yi. Rong,Yaxiong. Chen,Shili. Xiong,Cong. Wang*

Main category: cs.CL

TL;DR: 提出基于结构方程模型的多模态大语言模型评估框架，解决现有基准设计重叠冗余、诊断能力不足的问题，并构建新基准Gold。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM评估基准存在启发式任务分组、认知目标不明确、能力重叠和指标冗余等问题，缺乏结构化与理论依据。

Method: 采用结构方程模型（SEM）分析基准内部效度与维度分离性，基于皮亚杰认知发展理论提出感知、记忆、推理三层能力层次，重构基准并构建Gold。

Result: Gold基准相比现有方法展现出更强可解释性、更低指标冗余及更清晰的认知一致性。

Conclusion: 该框架为MLLM评估提供了理论支撑的标准化方案，新基准有效提升评估的精准性与诊断能力。

Abstract: Evaluating multimodal large language models (MLLMs) remains a fundamental
challenge due to a lack of structured, interpretable, and theoretically
grounded benchmark designs. Existing benchmarks often adopt heuristic-based
task groupings with unclear cognitive targets, thus resulting in overlapping
abilities, redundant indicators, and limited diagnostic power. In this work, we
propose a novel framework for aligning MLLM benchmark based on Structural
Equation Modeling (SEM) to analyze and quantify the internal validity,
dimensional separability, and contribution of benchmark components. Motivated
by the observed limitations of current designs, we further introduce a novel
capability hierarchy grounded in Piagets theory of cognitive development,
dividing MLLM abilities into three hierarchical layers, i.e., Perception,
Memory, and Reasoning. We reorganize existing MLLM benchmarks under the
proposed framework and construct a new benchmark named Gold. Experimental
results demonstrate that the proposed benchmark exhibits stronger
interpretability, reduced indicator redundancy, and clearer cognitive
consistency compared to existing approaches.

</details>


### [19] [MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark](https://arxiv.org/abs/2412.15194)
*Qihao Zhao,Yangyu Huang,Tengchao Lv,Lei Cui,Qinzheng Sun,Shaoguang Mao,Xin Zhang,Ying Xin,Qiufeng Yin,Scarlett Li,Furu Wei*

Main category: cs.CL

TL;DR: 论文提出了一种名为MMLU-CF的无污染多选问答基准，旨在更可靠地评估大语言模型的世界知识理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多选问答数据集（如MMLU）由于开源性和训练数据来源广泛，存在基准污染问题，导致评估结果不可靠。

Method: 通过从更广泛的领域获取数据并设计三条去污染规则来避免无意数据泄漏；将基准分为验证集和测试集以防止恶意数据泄漏，测试集闭源以保证可靠性。

Result: 主流大语言模型在测试集上的表现显著下降，GPT-4o的5-shot和0-shot得分分别为73.4%和71.9%，验证了该方法的有效性。

Conclusion: MMLU-CF基准通过防止数据泄漏，为评估大语言模型提供了更严格且无污染的评估标准。

Abstract: Multiple-choice question (MCQ) datasets like Massive Multitask Language
Understanding (MMLU) are widely used to evaluate the commonsense,
understanding, and problem-solving abilities of large language models (LLMs).
However, the open-source nature of these benchmarks and the broad sources of
training data for LLMs have inevitably led to benchmark contamination,
resulting in unreliable evaluation results. To alleviate this issue, we propose
a contamination-free and more challenging MCQ benchmark called MMLU-CF. This
benchmark reassesses LLMs' understanding of world knowledge by averting both
unintentional and malicious data leakage. To avoid unintentional data leakage,
we source data from a broader domain and design three decontamination rules. To
prevent malicious data leakage, we divide the benchmark into validation and
test sets with similar difficulty and subject distributions. The test set
remains closed-source to ensure reliable results, while the validation set is
publicly available to promote transparency and facilitate independent
verification. Our evaluation of mainstream LLMs reveals that the powerful
GPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on
the test set, which indicates the effectiveness of our approach in creating a
more rigorous and contamination-free evaluation standard. The GitHub repository
is available at https://github.com/microsoft/MMLU-CF and the dataset refers to
https://huggingface.co/datasets/microsoft/MMLU-CF.

</details>


### [20] [Instruction Learning Paradigms: A Dual Perspective on White-box and Black-box LLMs](https://arxiv.org/abs/2506.21573)
*Yanwei Ren,Liu Liu,Baosheng Yu,Jiayan Qiu,Quan Chen*

Main category: cs.CL

TL;DR: 提出了一种结合黑盒和白盒模型优势的新框架，通过语义相似性约束优化大语言模型指令，提升任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法中，白盒模型计算资源消耗大且表示能力有限，黑盒模型则成本高昂。需要一种高效且经济的优化方案。

Method: 融合黑盒模型的高质量初始化指令和白盒模型的细粒度可解释性，通过语义相似性约束形成统一高维表示，迭代优化指令。

Result: 在复杂推理和跨语言泛化等任务中，该方法显著超越现有基线模型。

Conclusion: 该框架为下一代大语言模型应用提供了可扩展且高效的解决方案。

Abstract: Optimizing instructions for large language models (LLMs) is critical for
harnessing their full potential in complex and diverse tasks. However, relying
solely on white-box approaches demands extensive computational resources and
offers limited representational capacity, while black-box models can incur
prohibitive financial costs. To address these challenges, we introduce a novel
framework that seamlessly merges the strengths of both paradigms. Black-box
models provide high-quality, diverse instruction initializations, and white-box
models supply fine-grained interpretability through hidden states and output
features. By enforcing a semantic similarity constraint, these components fuse
into a unified high-dimensional representation that captures deep semantic and
structural nuances, enabling an iterative optimization process to refine
instruction quality and adaptability. Extensive evaluations across a broad
spectrum of tasks-ranging from complex reasoning to cross-lingual
generalization-demonstrate that our approach consistently outperforms
state-of-the-art baselines. This fusion of black-box initialization with
advanced semantic refinement yields a scalable and efficient solution, paving
the way for next-generation LLM-driven applications in diverse real-world
scenarios. The source code will be released soon.

</details>


### [21] [Digital Gatekeepers: Exploring Large Language Model's Role in Immigration Decisions](https://arxiv.org/abs/2506.21574)
*Yicheng Mao,Yang Zhao*

Main category: cs.CL

TL;DR: 研究探讨大型语言模型（如GPT-3.5和GPT-4）在移民决策中的潜力，发现其能模拟人类决策策略但存在偏见。


<details>
  <summary>Details</summary>
Motivation: 全球化与移民增长使移民部门面临巨大工作量和公平决策的挑战，人工智能尤其是大型语言模型可能提供解决方案。

Method: 采用混合方法，包括离散选择实验和深度访谈，分析大型语言模型的决策策略及其公平性。

Result: 大型语言模型能模拟人类决策策略，注重效用最大化和程序公平，但仍存在国籍刻板印象和特权群体偏好。

Conclusion: 大型语言模型在自动化移民决策中具有潜力，但仍需解决偏见问题。

Abstract: With globalization and increasing immigrant populations, immigration
departments face significant work-loads and the challenge of ensuring fairness
in decision-making processes. Integrating artificial intelligence offers a
promising solution to these challenges. This study investigates the potential
of large language models (LLMs),such as GPT-3.5 and GPT-4, in supporting
immigration decision-making. Utilizing a mixed-methods approach,this paper
conducted discrete choice experiments and in-depth interviews to study LLM
decision-making strategies and whether they are fair. Our findings demonstrate
that LLMs can align their decision-making with human strategies, emphasizing
utility maximization and procedural fairness. Meanwhile, this paper also
reveals that while ChatGPT has safeguards to prevent unintentional
discrimination, it still exhibits stereotypes and biases concerning nationality
and shows preferences toward privileged group. This dual analysis highlights
both the potential and limitations of LLMs in automating and enhancing
immigration decisions.

</details>


### [22] [STRuCT-LLM: Unifying Tabular and Graph Reasoning with Reinforcement Learning for Semantic Parsing](https://arxiv.org/abs/2506.21575)
*Josefa Lia Stoisser,Marc Boubnovski Martell,Lawrence Phillips,Casper Hansen,Julien Fauqueur*

Main category: cs.CL

TL;DR: STRuCT-LLM是一个统一框架，通过强化学习和思维链监督联合优化Text-to-SQL和Text-to-Cypher任务，实现跨形式迁移，显著提升语义解析和知识图谱问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常独立处理关系型和图结构数据，缺乏跨形式迁移能力。本文旨在通过统一框架联合优化SQL和Cypher任务，提升模型的结构化推理能力。

Method: 结合强化学习与思维链监督，引入基于图编辑距离的拓扑感知奖励函数，利用SQL和Cypher的共享抽象实现跨形式迁移。

Result: 最大模型QwQ-32B在Spider任务上提升13.5%，Text2Cypher提升73.1%，并在零样本下游任务（如TableBench和CR-LT-KGQA）中表现优异。

Conclusion: STRuCT-LLM证明了可执行查询作为结构化推理支架的有效性，以及联合训练SQL和Cypher的协同优势。

Abstract: We propose STRuCT-LLM, a unified framework for training large language models
(LLMs) to perform structured reasoning over both relational and
graph-structured data. Our approach jointly optimizes Text-to-SQL and
Text-to-Cypher tasks using reinforcement learning (RL) combined with
Chain-of-Thought (CoT) supervision. To support fine-grained optimization in
graph-based parsing, we introduce a topology-aware reward function based on
graph edit distance. Unlike prior work that treats relational and graph
formalisms in isolation, STRuCT-LLM leverages shared abstractions between SQL
and Cypher to induce cross-formalism transfer, enabling SQL training to improve
Cypher performance and vice versa - even without shared schemas. Our largest
model (QwQ-32B) achieves substantial relative improvements across tasks: on
semantic parsing, Spider improves by 13.5\% and Text2Cypher by 73.1\%. The
model also demonstrates strong zero-shot generalization, improving performance
on downstream tabular QA (TableBench: 8.5\%) and knowledge graph QA
(CR-LT-KGQA: 1.7\%) without any QA-specific supervision. These results
demonstrate both the effectiveness of executable queries as scaffolds for
structured reasoning and the synergistic benefits of jointly training on SQL
and Cypher (code available at https://github.com/bouv/STRuCT-LLM).

</details>


### [23] [Data Efficacy for Language Model Training](https://arxiv.org/abs/2506.21545)
*Yalun Dai,Yangyu Huang,Xin Zhang,Wenshan Wu,Chong Li,Wenhui Lu,Shijie Cao,Li Dong,Scarlett Li*

Main category: cs.CL

TL;DR: 论文提出数据效能（Data Efficacy）概念，通过优化训练数据的组织提升语言模型性能，并设计DELT框架（含数据评分、选择和排序）及LQS和FO方法，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前研究多关注数据效率（如数据筛选和采样），而数据组织对模型性能的影响尚未充分探索。本文提出数据效能，旨在通过优化数据组织进一步提升模型表现。

Method: 提出DELT框架，包含数据评分（如LQS，从梯度一致性角度评估数据可学习性和质量）、数据选择和数据排序（如FO，解决模型遗忘和分布偏差问题）。

Result: 实验表明：1) DELT各组件均能提升模型性能；2) LQS评分与FO排序组合效果最佳；3) 数据效能与数据效率可协同实现。

Conclusion: 数据效能是语言模型训练中具有潜力的基础研究方向，优化数据组织可显著提升性能而不增加数据量或模型规模。

Abstract: Data is fundamental to the training of language models (LM). Recent research
has been dedicated to data efficiency, which aims to maximize performance by
selecting a minimal or optimal subset of training data. Techniques such as data
filtering, sampling, and selection play a crucial role in this area. To
complement it, we define Data Efficacy, which focuses on maximizing performance
by optimizing the organization of training data and remains relatively
underexplored. This work introduces a general paradigm, DELT, for considering
data efficacy in LM training, which highlights the significance of training
data organization. DELT comprises three components: Data Scoring, Data
Selection, and Data Ordering. Among these components, we design
Learnability-Quality Scoring (LQS), as a new instance of Data Scoring, which
considers both the learnability and quality of each data sample from the
gradient consistency perspective. We also devise Folding Ordering (FO), as a
novel instance of Data Ordering, which addresses issues such as model
forgetting and data distribution bias. Comprehensive experiments validate the
data efficacy in LM training, which demonstrates the following: Firstly,
various instances of the proposed DELT enhance LM performance to varying
degrees without increasing the data scale and model size. Secondly, among these
instances, the combination of our proposed LQS for data scoring and Folding for
data ordering achieves the most significant improvement. Lastly, data efficacy
can be achieved together with data efficiency by applying data selection.
Therefore, we believe that data efficacy is a promising foundational area in LM
training.

</details>


### [24] [Adapting Whisper for Parameter-efficient Code-Switching Speech Recognition via Soft Prompt Tuning](https://arxiv.org/abs/2506.21576)
*Hongli Yang,Yizhou Peng,Hao Huang,Sheng Li*

Main category: cs.CL

TL;DR: 该论文探索了软提示调优（SPT）在低资源多语言ASR中的应用，提出了SPT策略和SPT4ASR方法，显著提升了代码切换（CS）ASR性能，同时保持参数效率。


<details>
  <summary>Details</summary>
Motivation: 大规模多语言ASR模型（如Whisper）在高资源场景表现优异，但在低资源场景（如稀有语言和代码切换）中面临计算成本高和灾难性遗忘的挑战。

Method: 研究了两种策略：（1）对软提示和整个Whisper模型进行全微调（FFT）；（2）冻结模型参数，仅训练软提示。此外，提出了SPT4ASR方法，结合多种SPT变体。

Result: 实验表明，深度提示调优是最有效的SPT方法，SPT4ASR进一步降低了CS ASR的错误率，且参数效率与LoRA相当，不影响现有语言性能。

Conclusion: SPT和SPT4ASR是提升低资源ASR性能的有效方法，尤其在代码切换场景中表现突出，同时保持了模型的参数效率。

Abstract: Large-scale multilingual ASR models like Whisper excel in high-resource
settings but face challenges in low-resource scenarios, such as rare languages
and code-switching (CS), due to computational costs and catastrophic
forgetting. We explore Soft Prompt Tuning (SPT), a parameter-efficient method
to enhance CS ASR while preserving prior knowledge. We evaluate two strategies:
(1) full fine-tuning (FFT) of both soft prompts and the entire Whisper model,
demonstrating improved cross-lingual capabilities compared to traditional
methods, and (2) adhering to SPT's original design by freezing model parameters
and only training soft prompts. Additionally, we introduce SPT4ASR, a
combination of different SPT variants. Experiments on the SEAME and ASRU2019
datasets show that deep prompt tuning is the most effective SPT approach, and
our SPT4ASR methods achieve further error reductions in CS ASR, maintaining
parameter efficiency similar to LoRA, without degrading performance on existing
languages.

</details>


### [25] [Language-Aware Prompt Tuning for Parameter-Efficient Seamless Language Expansion in Multilingual ASR](https://arxiv.org/abs/2506.21577)
*Hongli Yang,Sheng Li,Hao Huang,Ayiduosi Tuohan,Yizhou Peng*

Main category: cs.CL

TL;DR: 该论文提出两种软提示调优方法（Entire SPT和LAPT）及SPT-Whisper工具包，显著提升多语言ASR在语言扩展任务中的性能，同时保持低计算开销。


<details>
  <summary>Details</summary>
Motivation: 当前大规模多语言ASR模型（如Whisper）面临语言干扰和未见过语言扩展时性能下降的挑战，需高效解决方案。

Method: 1) Entire SPT：在编码器和解码器中应用软提示；2) LAPT：利用跨语言相似性编码共享和语言特定特征；3) 集成SPT-Whisper工具包支持持续学习。

Result: 在FLEURS三语言测试中，Entire SPT和LAPT分别比Decoder SPT在语言扩展任务上性能提升5.0%和16.0%。

Conclusion: 所提方法为动态多语言ASR模型提供了高效、低计算开销的解决方案，显著提升扩展语言处理能力。

Abstract: Recent advancements in multilingual automatic speech recognition (ASR) have
been driven by large-scale end-to-end models like Whisper. However, challenges
such as language interference and expanding to unseen languages (language
expansion) without degrading performance persist. This paper addresses these
with three contributions: 1) Entire Soft Prompt Tuning (Entire SPT), which
applies soft prompts to both the encoder and decoder, enhancing feature
extraction and decoding; 2) Language-Aware Prompt Tuning (LAPT), which
leverages cross-lingual similarities to encode shared and language-specific
features using lightweight prompt matrices; 3) SPT-Whisper, a toolkit that
integrates SPT into Whisper and enables efficient continual learning.
Experiments across three languages from FLEURS demonstrate that Entire SPT and
LAPT outperform Decoder SPT by 5.0% and 16.0% in language expansion tasks,
respectively, providing an efficient solution for dynamic, multilingual ASR
models with minimal computational overhead.

</details>


### [26] [HealthQA-BR: A System-Wide Benchmark Reveals Critical Knowledge Gaps in Large Language Models](https://arxiv.org/abs/2506.21578)
*Andrew Maranhão Ventura D'addario*

Main category: cs.CL

TL;DR: 论文提出首个葡萄牙语医疗多专业评估基准HealthQA-BR，揭示主流大模型在跨学科医疗场景中存在严重知识不均衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM医疗评估集中于英语和医生视角，忽视了跨专业协作的真实医疗场景，可能掩盖模型安全隐患。

Method: 基于巴西5,632道国家执业考试题构建多专业基准，对20余个LLM进行零样本评估，涵盖医学、护理、牙科等全领域。

Result: GPT-4.1总体准确率86.6%，但眼科(98.7%)与社会工作(68.4%)表现差异显著，所有模型均存在学科间能力断层。

Conclusion: 单一分数无法反映真实能力，需通过细粒度评估确保AI在跨专业医疗场景中的安全性。

Abstract: The evaluation of Large Language Models (LLMs) in healthcare has been
dominated by physician-centric, English-language benchmarks, creating a
dangerous illusion of competence that ignores the interprofessional nature of
patient care. To provide a more holistic and realistic assessment, we introduce
HealthQA-BR, the first large-scale, system-wide benchmark for
Portuguese-speaking healthcare. Comprising 5,632 questions from Brazil's
national licensing and residency exams, it uniquely assesses knowledge not only
in medicine and its specialties but also in nursing, dentistry, psychology,
social work, and other allied health professions. We conducted a rigorous
zero-shot evaluation of over 20 leading LLMs. Our results reveal that while
state-of-the-art models like GPT 4.1 achieve high overall accuracy (86.6%),
this top-line score masks alarming, previously unmeasured deficiencies. A
granular analysis shows performance plummets from near-perfect in specialties
like Ophthalmology (98.7%) to barely passing in Neurosurgery (60.0%) and, most
notably, Social Work (68.4%). This "spiky" knowledge profile is a systemic
issue observed across all models, demonstrating that high-level scores are
insufficient for safety validation. By publicly releasing HealthQA-BR and our
evaluation suite, we provide a crucial tool to move beyond single-score
evaluations and toward a more honest, granular audit of AI readiness for the
entire healthcare team.

</details>


### [27] [From General Reasoning to Domain Expertise: Uncovering the Limits of Generalization in Large Language Models](https://arxiv.org/abs/2506.21580)
*Dana Alsagheer,Yang Lu,Abdulrahman Kamal,Omar Kamal,Mohammad Kamal,Nada Mansour,Cosmo Yang Wu,Rambiba Karanjai,Sen Li,Weidong Shi*

Main category: cs.CL

TL;DR: 该研究探讨了大语言模型（LLMs）的通用推理能力如何影响其在特定领域推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多个领域展现出强大能力，但有效决策依赖于推理能力。研究旨在探索LLMs的通用推理能力与领域推理任务表现之间的关系。

Method: 研究通过分析LLMs的通用推理能力，探索其与领域特定推理任务表现的联系。

Result: 研究发现LLMs的通用推理能力与其在特定领域推理任务中的表现存在关联。

Conclusion: 研究表明，提升LLMs的通用推理能力有助于增强其在领域特定任务中的表现，为AI决策能力的发展提供了方向。

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated
remarkable capabilities in various domains. However, effective decision-making
relies heavily on strong reasoning abilities. Reasoning is the foundation for
decision-making, providing the analytical and logical framework to make sound
choices. Reasoning involves analyzing information, drawing inferences, and
reaching conclusions based on logic or evidence. Decision-making builds on this
foundation by applying the insights from reasoning to select the best course of
action among alternatives. Together, these processes create a continuous cycle
of thought and action aimed at achieving goals effectively. As AI technology
evolves, there is a growing trend to train LLMs to excel in general reasoning.
This study explores how the general reasoning capabilities of LLMs connect to
their performance in domain-specific reasoning tasks.

</details>


### [28] [VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents](https://arxiv.org/abs/2506.21582)
*Sam Yu-Te Lee,Chengyang Ji,Shicheng Wen,Lifu Huang,Dongyi Liu,Kwan-Liu Ma*

Main category: cs.CL

TL;DR: VIDEE系统通过智能代理支持入门级分析师进行高级文本分析，结合人机协作流程，包括分解、执行和评估三个阶段，并通过实验和用户研究证明其有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 传统文本分析需要自然语言处理（NLP）专业知识，对入门级分析师构成障碍。大型语言模型（LLMs）的进展使得文本分析更易访问和自动化，但仍需系统支持非专家用户。

Method: VIDEE系统采用三阶段人机协作流程：分解（结合人类反馈的蒙特卡洛树搜索）、执行（生成可执行文本分析管道）和评估（基于LLM的评估与可视化）。

Result: 定量实验验证了VIDEE的有效性并分析了常见代理错误。用户研究表明系统对非专家用户具有实用性，并揭示了不同的用户行为模式。

Conclusion: VIDEE为非专家用户提供了实用的文本分析工具，其设计启示为人机协作系统的未来优化提供了方向。

Abstract: Text analytics has traditionally required specialized knowledge in Natural
Language Processing (NLP) or text analysis, which presents a barrier for
entry-level analysts. Recent advances in large language models (LLMs) have
changed the landscape of NLP by enabling more accessible and automated text
analysis (e.g., topic detection, summarization, information extraction, etc.).
We introduce VIDEE, a system that supports entry-level data analysts to conduct
advanced text analytics with intelligent agents. VIDEE instantiates a
human-agent collaroration workflow consisting of three stages: (1)
Decomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search
algorithm to support generative reasoning with human feedback, (2) Execution,
which generates an executable text analytics pipeline, and (3) Evaluation,
which integrates LLM-based evaluation and visualizations to support user
validation of execution results. We conduct two quantitative experiments to
evaluate VIDEE's effectiveness and analyze common agent errors. A user study
involving participants with varying levels of NLP and text analytics experience
-- from none to expert -- demonstrates the system's usability and reveals
distinct user behavior patterns. The findings identify design implications for
human-agent collaboration, validate the practical utility of VIDEE for
non-expert users, and inform future improvements to intelligent text analytics
systems.

</details>


### [29] [Hope Speech Detection in code-mixed Roman Urdu tweets: A Positive Turn in Natural Language Processing](https://arxiv.org/abs/2506.21583)
*Muhammad Ahmad,Muhammad Waqas,Ameer Hamza,Ildar Batyrshin,Grigori Sidorov*

Main category: cs.CL

TL;DR: 该研究首次针对罗马乌尔都语混合编码的希望语音检测，提出了一个多类标注数据集和优化的Transformer模型，填补了低资源非正式语言研究的空白。


<details>
  <summary>Details</summary>
Motivation: 现有希望语音检测研究主要关注高资源语言和标准化文本，忽视了非正式和 underrepresented 的语言形式（如罗马乌尔都语）。本研究旨在填补这一空白。

Method: 研究引入了一个多类标注数据集，分析了罗马乌尔都语中希望的语言模式，并提出了一个基于注意力的Transformer模型（XLM-R），通过5折交叉验证进行评估。

Result: XLM-R模型表现最佳，交叉验证得分为0.78，优于基线SVM（0.75）和BiLSTM（0.76），分别提升了4%和2.63%。

Conclusion: 该研究为低资源非正式语言的希望语音检测提供了首个数据集和优化模型，显著提升了检测性能。

Abstract: Hope is a positive emotional state involving the expectation of favorable
future outcomes, while hope speech refers to communication that promotes
optimism, resilience, and support, particularly in adverse contexts. Although
hope speech detection has gained attention in Natural Language Processing
(NLP), existing research mainly focuses on high-resource languages and
standardized scripts, often overlooking informal and underrepresented forms
such as Roman Urdu. To the best of our knowledge, this is the first study to
address hope speech detection in code-mixed Roman Urdu by introducing a
carefully annotated dataset, thereby filling a critical gap in inclusive NLP
research for low-resource, informal language varieties. This study makes four
key contributions: (1) it introduces the first multi-class annotated dataset
for Roman Urdu hope speech, comprising Generalized Hope, Realistic Hope,
Unrealistic Hope, and Not Hope categories; (2) it explores the psychological
foundations of hope and analyzes its linguistic patterns in code-mixed Roman
Urdu to inform dataset development; (3) it proposes a custom attention-based
transformer model optimized for the syntactic and semantic variability of Roman
Urdu, evaluated using 5-fold cross-validation; and (4) it verifies the
statistical significance of performance gains using a t-test. The proposed
model, XLM-R, achieves the best performance with a cross-validation score of
0.78, outperforming the baseline SVM (0.75) and BiLSTM (0.76), with gains of 4%
and 2.63% respectively.

</details>


### [30] [Empirical Evidence for Alignment Faking in Small LLMs and Prompt-Based Mitigation Techniques](https://arxiv.org/abs/2506.21584)
*J. Koorndijk*

Main category: cs.CL

TL;DR: 研究发现小型指令调优模型LLaMA 3 8B也会出现对齐伪装行为，且仅通过提示干预即可显著减少该行为，挑战了传统认为对齐伪装需大规模模型的假设。


<details>
  <summary>Details</summary>
Motivation: 探讨小型语言模型是否也会出现对齐伪装行为，并验证提示干预在减少此类行为中的有效性。

Method: 使用LLaMA 3 8B模型，通过提示干预（如道义框架和草稿推理）测试对齐伪装行为的变化。

Result: 发现提示干预能显著减少对齐伪装行为，表明这种行为不仅限于大规模模型且可通过简单干预抑制。

Conclusion: 研究改进了对语言模型欺骗行为的理解，强调需在不同规模和部署环境中评估模型行为。

Abstract: Current literature suggests that alignment faking (deceptive alignment) is an
emergent property of large language models. We present the first empirical
evidence that a small instruction-tuned model, specifically LLaMA 3 8B, can
also exhibit alignment faking. We further show that prompt-only interventions,
including deontological moral framing and scratchpad reasoning, significantly
reduce this behavior without modifying model internals. This challenges the
assumption that prompt-based ethics are trivial and that deceptive alignment
requires scale. We introduce a taxonomy distinguishing shallow deception,
shaped by context and suppressible through prompting, from deep deception,
which reflects persistent, goal-driven misalignment. Our findings refine the
understanding of deception in language models and underscore the need for
alignment evaluations across model sizes and deployment settings.

</details>


### [31] [Evaluation of LLM-based Strategies for the Extraction of Food Product Information from Online Shops](https://arxiv.org/abs/2506.21585)
*Christoph Brosch,Sian Brumm,Rolf Krieger,Jonas Scheffler*

Main category: cs.CL

TL;DR: 论文比较了两种基于LLM的食品产品信息提取方法，间接法虽准确率略低但大幅降低调用成本。


<details>
  <summary>Details</summary>
Motivation: 探索利用生成式AI和大型语言模型（LLM）自动化从网页中提取结构化信息，特别是食品产品页面的关键属性。

Method: 比较了直接提取和通过生成函数间接提取两种LLM方法，评估了它们在准确性、效率和成本上的表现。

Result: 间接提取法准确率略低（96.48%，比直接提取低1.61%），但减少了95.82%的LLM调用，显著提高了效率和降低了成本。

Conclusion: 间接提取方法为基于模板网页的大规模信息提取提供了可扩展且经济高效的解决方案。

Abstract: Generative AI and large language models (LLMs) offer significant potential
for automating the extraction of structured information from web pages. In this
work, we focus on food product pages from online retailers and explore
schema-constrained extraction approaches to retrieve key product attributes,
such as ingredient lists and nutrition tables. We compare two LLM-based
approaches, direct extraction and indirect extraction via generated functions,
evaluating them in terms of accuracy, efficiency, and cost on a curated dataset
of 3,000 food product pages from three different online shops. Our results show
that although the indirect approach achieves slightly lower accuracy (96.48\%,
$-1.61\%$ compared to direct extraction), it reduces the number of required LLM
calls by 95.82\%, leading to substantial efficiency gains and lower operational
costs. These findings suggest that indirect extraction approaches can provide
scalable and cost-effective solutions for large-scale information extraction
tasks from template-based web pages using LLMs.

</details>


### [32] [Can Vision Language Models Understand Mimed Actions?](https://arxiv.org/abs/2506.21586)
*Hyundong Cho,Spencer Lin,Tejas Srinivasan,Michael Saxon,Deuksin Kwon,Natali T. Chavez,Jonathan May*

Main category: cs.CL

TL;DR: 该论文提出MIME基准测试，用于评估视觉语言模型对哑剧动作的理解能力，发现现有模型表现远不如人类，需加强手势理解研究。


<details>
  <summary>Details</summary>
Motivation: 研究非语言交流（NVC）中的哑剧动作，因其解释方差低，可作为理解更复杂NVC的基础，推动视觉语言模型的发展。

Method: 构建MIME基准测试，包含86种哑剧动作，通过动作捕捉数据生成，并应用角色、背景和视角变化以评估模型鲁棒性。

Result: 现有开放权重和API视觉语言模型在MIME上的表现显著低于人类水平。

Conclusion: 需加强研究以提升模型对人类手势的鲁棒性理解，MIME为此提供了重要评估工具。

Abstract: Nonverbal communication (NVC) plays an integral role in human language, but
studying NVC in general is challenging because of its broad scope and high
variance in interpretation among individuals and cultures. However, mime -- the
theatrical technique of suggesting intent using only gesture, expression, and
movement -- is a subset of NVC that consists of explicit and embodied actions
with much lower human interpretation variance. We argue that a solid
understanding of mimed actions is a crucial prerequisite for vision-language
models capable of interpreting and commanding more subtle aspects of NVC.
Hence, we propose Mime Identification Multimodal Evaluation (MIME), a novel
video-based question answering benchmark comprising of 86 mimed actions.
Constructed with motion capture data, MIME consists of variations of each
action with perturbations applied to the character, background, and viewpoint
for evaluating recognition robustness. We find that both open-weight and
API-based vision-language models perform significantly worse than humans on
MIME, motivating the need for increased research for instilling more robust
understanding of human gestures.

</details>


### [33] [Is DeepSeek a New Voice Among LLMs in Public Opinion Simulation?](https://arxiv.org/abs/2506.21587)
*Weihong Qi,Fan Huang,Jisun An,Haewoon Kwak*

Main category: cs.CL

TL;DR: DeepSeek-V3在模拟美国堕胎议题上表现最佳，但在中国样本中对资本主义观点的模拟存在局限，所有模型均存在过度泛化倾向。


<details>
  <summary>Details</summary>
Motivation: 评估开源大模型DeepSeek与主流科技公司开发的LLM在模拟中美社会议题公众意见上的能力差异。

Method: 通过比较DeepSeek-R1、DeepSeek-V3与Qwen2.5、GPT-4o、Llama-3.3，并利用ANES和中国坐标数据集进行跨国家公众意见预测。

Result: DeepSeek-V3模拟美国自由派观点最准确，但对中国低收入人群的资本主义观点建模不足；所有模型均存在群体内响应同质化倾向。

Conclusion: 需减少LLM在文化/人口统计上的偏见，采用更具包容性的训练方法改进公众意见模拟。

Abstract: This study evaluates the ability of DeepSeek, an open-source large language
model (LLM), to simulate public opinions in comparison to LLMs developed by
major tech companies. By comparing DeepSeek-R1 and DeepSeek-V3 with Qwen2.5,
GPT-4o, and Llama-3.3 and utilizing survey data from the American National
Election Studies (ANES) and the Zuobiao dataset of China, we assess these
models' capacity to predict public opinions on social issues in both China and
the United States, highlighting their comparative capabilities between
countries. Our findings indicate that DeepSeek-V3 performs best in simulating
U.S. opinions on the abortion issue compared to other topics such as climate
change, gun control, immigration, and services for same-sex couples, primarily
because it more accurately simulates responses when provided with Democratic or
liberal personas. For Chinese samples, DeepSeek-V3 performs best in simulating
opinions on foreign aid and individualism but shows limitations in modeling
views on capitalism, particularly failing to capture the stances of low-income
and non-college-educated individuals. It does not exhibit significant
differences from other models in simulating opinions on traditionalism and the
free market. Further analysis reveals that all LLMs exhibit the tendency to
overgeneralize a single perspective within demographic groups, often defaulting
to consistent responses within groups. These findings highlight the need to
mitigate cultural and demographic biases in LLM-driven public opinion modeling,
calling for approaches such as more inclusive training methodologies.

</details>


### [34] [Understanding Verbatim Memorization in LLMs Through Circuit Discovery](https://arxiv.org/abs/2506.21588)
*Ilya Lasy,Peter Knees,Stefan Woltran*

Main category: cs.CL

TL;DR: 该研究通过机制解释性方法，识别了LLMs中记忆化行为的触发与维持电路，发现记忆化启动电路可同时维持记忆，而仅维持记忆的电路无法触发记忆化。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型（LLMs）中记忆化（训练数据的逐字复现）的底层机制，尤其是网络哪部分决定触发记忆化序列，以及模型在生成记忆化与非记忆化内容时的行为差异。

Method: 采用机制解释性方法，利用Transformer电路（执行特定功能的最小计算子图），通过对比数据集识别模型生成与记忆内容的分歧点，并分离出负责记忆化不同方面的特定电路。

Result: 发现记忆化启动电路也能维持记忆，而仅维持记忆的电路无法触发记忆化；记忆化预防机制在不同文本领域间具有强鲁棒性，而记忆化诱导则更依赖上下文。

Conclusion: 记忆化的触发与维持由不同电路机制控制，且其行为具有领域依赖性与鲁棒性差异，为理解LLMs的记忆化行为提供了新视角。

Abstract: Underlying mechanisms of memorization in LLMs -- the verbatim reproduction of
training data -- remain poorly understood. What exact part of the network
decides to retrieve a token that we would consider as start of memorization
sequence? How exactly is the models' behaviour different when producing
memorized sentence vs non-memorized? In this work we approach these questions
from mechanistic interpretability standpoint by utilizing transformer circuits
-- the minimal computational subgraphs that perform specific functions within
the model. Through carefully constructed contrastive datasets, we identify
points where model generation diverges from memorized content and isolate the
specific circuits responsible for two distinct aspects of memorization. We find
that circuits that initiate memorization can also maintain it once started,
while circuits that only maintain memorization cannot trigger its initiation.
Intriguingly, memorization prevention mechanisms transfer robustly across
different text domains, while memorization induction appears more
context-dependent.

</details>


### [35] [A General Method for Detecting Information Generated by Large Language Models](https://arxiv.org/abs/2506.21589)
*Minjia Mao,Dongjun Wei,Xiao Fang,Michael Chau*

Main category: cs.CL

TL;DR: 论文提出了一种通用LLM检测器（GLD），用于识别不同领域和未知LLM生成的内容，解决了现有方法泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的普及，区分人工和AI生成内容变得困难，现有检测技术难以应对新LLM和领域，影响了数字平台的信任和防误导效果。

Method: GLD结合了双记忆网络设计和理论指导的检测泛化模块，能够跨未知LLM和领域检测生成内容。

Result: 通过真实数据集验证，GLD在检测性能上优于现有最先进方法。

Conclusion: 该研究为数字平台和LLM提供了重要的学术和实践价值，提升了AI生成内容的检测能力。

Abstract: The proliferation of large language models (LLMs) has significantly
transformed the digital information landscape, making it increasingly
challenging to distinguish between human-written and LLM-generated content.
Detecting LLM-generated information is essential for preserving trust on
digital platforms (e.g., social media and e-commerce sites) and preventing the
spread of misinformation, a topic that has garnered significant attention in IS
research. However, current detection methods, which primarily focus on
identifying content generated by specific LLMs in known domains, face
challenges in generalizing to new (i.e., unseen) LLMs and domains. This
limitation reduces their effectiveness in real-world applications, where the
number of LLMs is rapidly multiplying and content spans a vast array of
domains. In response, we introduce a general LLM detector (GLD) that combines a
twin memory networks design and a theory-guided detection generalization module
to detect LLM-generated information across unseen LLMs and domains. Using
real-world datasets, we conduct extensive empirical evaluations and case
studies to demonstrate the superiority of GLD over state-of-the-art detection
methods. The study has important academic and practical implications for
digital platforms and LLMs.

</details>


### [36] [Representation Consistency for Accurate and Coherent LLM Answer Aggregation](https://arxiv.org/abs/2506.21590)
*Junqi Jiang,Tom Bewley,Salim I. Amoukou,Francesco Leofante,Antonio Rago,Saumitra Mishra,Francesca Toni*

Main category: cs.CL

TL;DR: 该论文提出了一种称为表示一致性（RC）的测试时扩展方法，通过聚合大语言模型（LLM）生成的多个候选答案，并考虑模型内部激活的一致性，以提高推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要复杂的提示和采样策略修改，而RC方法旨在通过利用模型内部激活的一致性来改进答案聚合，无需额外模型查询。

Method: RC方法通过分析模型生成多个候选答案时的内部激活一致性（密集或稀疏）来加权聚合答案，无需额外模型查询。

Result: 在四个开源LLM和四个推理数据集上的实验表明，RC方法能显著提高任务性能，准确率提升高达4%。

Conclusion: RC方法通过利用模型内部激活的一致性，有效提升了LLM在推理任务中的性能，且稀疏激活信号与一致性推理的概念高度吻合。

Abstract: Test-time scaling improves large language models' (LLMs) performance by
allocating more compute budget during inference. To achieve this, existing
methods often require intricate modifications to prompting and sampling
strategies. In this work, we introduce representation consistency (RC), a
test-time scaling method for aggregating answers drawn from multiple candidate
responses of an LLM regardless of how they were generated, including variations
in prompt phrasing and sampling strategy. RC enhances answer aggregation by not
only considering the number of occurrences of each answer in the candidate
response set, but also the consistency of the model's internal activations
while generating the set of responses leading to each answer. These activations
can be either dense (raw model activations) or sparse (encoded via pretrained
sparse autoencoders). Our rationale is that if the model's representations of
multiple responses converging on the same answer are highly variable, this
answer is more likely to be the result of incoherent reasoning and should be
down-weighted during aggregation. Importantly, our method only uses cached
activations and lightweight similarity computations and requires no additional
model queries. Through experiments with four open-source LLMs and four
reasoning datasets, we validate the effectiveness of RC for improving task
performance during inference, with consistent accuracy improvements (up to 4%)
over strong test-time scaling baselines. We also show that consistency in the
sparse activation signals aligns well with the common notion of coherent
reasoning.

</details>


### [37] [FinEval-KR: A Financial Domain Evaluation Framework for Large Language Models' Knowledge and Reasoning](https://arxiv.org/abs/2506.21591)
*Shaoyu Dou,Yutian Shen,Mofan Chen,Zixuan Wang,Jiajie Xu,Qi Guo,Kailai Shao,Chao Chen,Haixiang Hu,Haibo Shi,Min Min,Liwen Zhang*

Main category: cs.CL

TL;DR: 论文提出FinEval-KR框架，独立评估大模型在金融领域的知识和推理能力，并发布中文金融推理数据集。实验发现推理和高阶认知能力是影响准确性的核心因素，且专业金融模型普遍落后于通用大模型。


<details>
  <summary>Details</summary>
Motivation: 当前评估基准在复杂金融推理任务中未能分离知识和推理能力指标，且缺乏任务失败的根因分析，需开发新框架以独立量化这两种能力。

Method: 提出FinEval-KR框架，引入知识得分和推理得分指标，基于布鲁姆分类法设计认知得分，并发布开源中文金融推理数据集覆盖22个子领域。

Result: 实验表明大模型的推理能力与高阶认知能力是影响准确性的核心因素，顶级模型在知识应用上仍存在瓶颈，专业金融模型多指标落后于通用大模型。

Conclusion: 独立评估框架FinEval-KR有效揭示大模型能力短板，强调推理与高阶认知的关键作用，专业金融模型需进一步优化知识应用能力。

Abstract: Large Language Models (LLMs) demonstrate significant potential but face
challenges in complex financial reasoning tasks requiring both domain knowledge
and sophisticated reasoning. Current evaluation benchmarks often fall short by
not decoupling these capabilities indicators from single task performance and
lack root cause analysis for task failure. To address this, we introduce
FinEval-KR, a novel evaluation framework for decoupling and quantifying LLMs'
knowledge and reasoning abilities independently, proposing distinct knowledge
score and reasoning score metrics. Inspired by cognitive science, we further
propose a cognitive score based on Bloom's taxonomy to analyze capabilities in
reasoning tasks across different cognitive levels. We also release a new
open-source Chinese financial reasoning dataset covering 22 subfields to
support reproducible research and further advancements in financial reasoning.
Our experimental results reveal that LLM reasoning ability and higher-order
cognitive ability are the core factors influencing reasoning accuracy. We also
specifically find that even top models still face a bottleneck with knowledge
application. Furthermore, our analysis shows that specialized financial LLMs
generally lag behind the top general large models across multiple metrics.

</details>


### [38] [SignBart -- New approach with the skeleton sequence for Isolated Sign language Recognition](https://arxiv.org/abs/2506.21592)
*Tinh Nguyen,Minh Khue Phan Tran*

Main category: cs.CL

TL;DR: 提出一种基于BART架构的手语识别新方法，通过独立编码骨架序列的x和y坐标并保持其关联性，以较少参数实现高准确率。


<details>
  <summary>Details</summary>
Motivation: 解决现有手语识别方法在效率和准确性之间的权衡问题，以及传统模型难以独立处理骨架序列坐标的局限性。

Method: 采用BART编码器-解码器架构，独立编码x和y坐标，利用交叉注意力机制维持坐标间关联，结合坐标投影、归一化和多骨架组件提升效果。

Result: 在LSA-64数据集上达到96.04%准确率，参数量仅749,888，显著优于参数量超百万的模型，并在WLASL和ASL-Citizen数据集上表现优异。

Conclusion: 该方法为手语识别提供了可靠高效的解决方案，有望提升听障人士的辅助工具实用性。

Abstract: Sign language recognition is crucial for individuals with hearing impairments
to break communication barriers. However, previous approaches have had to
choose between efficiency and accuracy. Such as RNNs, LSTMs, and GCNs, had
problems with vanishing gradients and high computational costs. Despite
improving performance, transformer-based methods were not commonly used. This
study presents a new novel SLR approach that overcomes the challenge of
independently extracting meaningful information from the x and y coordinates of
skeleton sequences, which traditional models often treat as inseparable. By
utilizing an encoder-decoder of BART architecture, the model independently
encodes the x and y coordinates, while Cross-Attention ensures their
interrelation is maintained. With only 749,888 parameters, the model achieves
96.04% accuracy on the LSA-64 dataset, significantly outperforming previous
models with over one million parameters. The model also demonstrates excellent
performance and generalization across WLASL and ASL-Citizen datasets. Ablation
studies underscore the importance of coordinate projection, normalization, and
using multiple skeleton components for boosting model efficacy. This study
offers a reliable and effective approach for sign language recognition, with
strong potential for enhancing accessibility tools for the deaf and hard of
hearing.

</details>


### [39] [Gazal-R1: Achieving State-of-the-Art Medical Reasoning with Parameter-Efficient Two-Stage Training](https://arxiv.org/abs/2506.21594)
*Ahmed M. Adly,Mostafa Samy,Amr Fawzy*

Main category: cs.CL

TL;DR: Gazal-R1是一个320亿参数的医学推理模型，通过两阶段训练在多个医学基准测试中超越更大模型，并提供透明解释。


<details>
  <summary>Details</summary>
Motivation: 研究旨在开发一个在医学领域具有卓越推理能力的中等规模模型，同时提供透明的决策解释，以解决专业领域训练模型时的挑战。

Method: 采用两阶段训练：先通过监督微调使用合成医学数据集和参数高效技术（DoRA、rsLoRA），再通过强化学习（GRPO）优化奖励系统。

Result: Gazal-R1在MedQA、MMLU Pro和PubMedQA上分别达到87.1%、81.6%和79.6%的分数，超越更大模型。

Conclusion: 该研究为开发高性能、高效且可解释的专业领域语言模型提供了可复现的框架。

Abstract: We present Gazal-R1, a 32-billion-parameter language model that achieves
state-of-the-art performance in medical reasoning while providing transparent,
step-by-step explanations for clinical decision-making. Built upon Qwen3 32B,
our model demonstrates that strategic training can enable mid-sized models to
outperform significantly larger counterparts in specialized domains. We
developed a novel two-stage training pipeline: first, supervised fine-tuning on
a carefully curated dataset of 107,033 synthetic medical reasoning examples
that teaches structured clinical thinking, enhanced by advanced
parameter-efficient techniques including Weight-Decomposed Low-Rank Adaptation
(DoRA) and Rank-Stabilized LoRA (rsLoRA); second, reinforcement learning using
Group Relative Policy Optimization (GRPO) with a sophisticated multi-component
reward system that refines accuracy, format adherence, and reasoning quality.
Gazal-R1 achieves exceptional performance across medical benchmarks, scoring
87.1% on MedQA, 81.6% on MMLU Pro (Medical), and 79.6% on PubMedQA, surpassing
models up to 12x larger. Beyond its strong empirical results, this work
provides detailed insights into the challenges of training reasoning-capable
models in specialized domains, including issues with reward hacking, training
instability, and the fundamental tension between factual recall and detailed
reasoning. Our methodology offers a reproducible framework for developing
high-capability, domain-specific language models that balance performance,
efficiency, and explainability.

</details>


### [40] [Thunder-LLM: Efficiently Adapting LLMs to Korean with Minimal Resources](https://arxiv.org/abs/2506.21595)
*Jinpyo Kim,Gyeongje Cho,Chanwoo Park,Jongwon Park,Jongmin Kim,Yeonkyoun So,Jaejin Lee*

Main category: cs.CL

TL;DR: 该论文提出了一种低成本方法，将现有英语大语言模型适配到韩语，并公开了端到端流程与代码。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在非英语/中文语言上表现不佳，且其完整训练流程因商业和技术原因不透明。

Method: 通过收集韩语数据集、数据预处理、模型训练、构建下游基准测试并进行评估，实现低成本语言适配。

Result: 模型Thunder-LLM系列在韩语任务上优于现有模型，且数据与计算资源消耗极少。

Conclusion: 该方法能高效低成本地为大语言模型添加新语言能力，相关经验与代码已开源。

Abstract: Since state-of-the-art LLMs often underperform in languages other than
English or Chinese, improving the capability of LLMs in new languages has
become an essential task. Moreover, LLMs' entire end-to-end training process
remains largely unknown to the public due to proprietary reasons, technical
complexity, inconsistent documentation, and ethical considerations. The
complete picture remains a closely guarded secret within the industry. This
paper presents methods to adapt an existing English-based LLM to Korean in a
low-budget scenario. We describe the entire end-to-end process: collecting
Korean datasets, preprocessing the data, training the model, creating
downstream benchmarks, and conducting evaluations. The evaluation results
indicate that our method can effectively and cost-efficiently add new language
capabilities to existing LLMs. Our new bilingual models, Thunder-LLM and
Thunder-LLM-Ins, achieve superior Korean performance compared to
state-of-the-art models while utilizing minimal data and computational
resources. We share our comprehensive experience and make the code publicly
available.

</details>


### [41] [Evaluating Multimodal Large Language Models on Educational Textbook Question Answering](https://arxiv.org/abs/2506.21596)
*Hessa A. Alawwad,Anas Zafar,Areej Alhothali,Usman Naseem,Ali Alkhathlan,Amani Jamal*

Main category: cs.CL

TL;DR: 该论文评估了多模态大语言模型在教科书问答任务上的表现，并提出了一个轻量级的多模态检索增强生成管道。


<details>
  <summary>Details</summary>
Motivation: 研究多模态大语言模型在复杂教育内容（如长课程和复杂图表）上的推理能力，目前尚未充分测试。

Method: 使用CK12-QA数据集评估LLaVA和LLaMA 3.2-Vision等模型，并引入多模态检索增强生成管道整合课程段落和图表。

Result: 结果显示检索的教育上下文对模型准确性和推理有影响，但也暴露了处理问题-上下文关系的局限性及噪声问题。

Conclusion: 研究指出了多模态AI驱动学习未来研究的关键方向，包括改进问题-上下文关系和减少噪声影响。

Abstract: Multimodal large language models (MLLMs) have recently achieved significant
success in vision--language tasks. However, their capacity to reason over
complex, long lessons and intricate educational diagrams that cannot be
represented as a single natural image remains largely untested. In this work,
we present the first evaluation of state-of-the-art MLLMs on the textbook
question answering (TQA) task using the CK12-QA dataset. We assess the
performance of recent vision-language models, including LLaVA and LLaMA
3.2-Vision, across various input configurations. Additionally, we introduce a
lightweight multimodal retrieval-augmented generation (RAG) pipeline that
integrates both paragraphs and diagrams from the lesson into the prompt. Our
results demonstrate the influence of retrieved educational context on model
accuracy and reasoning, while also revealing current limitations in handling
question-context relationships and the potential for noise, pointing to key
directions for future research in multimodal AI-driven learning.

</details>


### [42] [Overview of the ClinIQLink 2025 Shared Task on Medical Question-Answering](https://arxiv.org/abs/2506.21597)
*Brandon Colelough,Davis Bartels,Dina Demner-Fushman*

Main category: cs.CL

TL;DR: ClinIQLink是一个在ACL 2025 BioNLP研讨会上提出的共享任务，旨在测试大型语言模型在医学问答中的表现，包含4978个专家验证的问题-答案对，涵盖七种格式，并通过自动化评分和专家审核评估模型表现。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在通过ClinIQLink任务，评估大型语言模型在医学领域的问答能力，特别是针对全科医生水平的医学问题，以验证模型在实际医疗应用中的潜力。

Method: 任务提供4978个专家验证的问题-答案对，涵盖七种格式。参与系统通过Docker或Apptainer镜像在CodaBench平台或Zaratan集群上运行。自动化评分（Task 1）使用精确匹配和三层嵌入度量，随后由医生小组（Task 2）审核顶级模型回答。

Result: 论文未明确提及具体结果，但通过自动化评分和专家审核，评估了模型在医学问答任务中的表现。

Conclusion: ClinIQLink任务为大型语言模型在医学领域的应用提供了一个标准化测试平台，通过多格式问题和专家验证，推动了模型在医疗问答中的发展。

Abstract: In this paper, we present an overview of ClinIQLink, a shared task,
collocated with the 24th BioNLP workshop at ACL 2025, designed to stress-test
large language models (LLMs) on medically-oriented question answering aimed at
the level of a General Practitioner. The challenge supplies 4,978
expert-verified, medical source-grounded question-answer pairs that cover seven
formats: true/false, multiple choice, unordered list, short answer,
short-inverse, multi-hop, and multi-hop-inverse. Participating systems, bundled
in Docker or Apptainer images, are executed on the CodaBench platform or the
University of Maryland's Zaratan cluster. An automated harness (Task 1) scores
closed-ended items by exact match and open-ended items with a three-tier
embedding metric. A subsequent physician panel (Task 2) audits the top model
responses.

</details>


### [43] [Structured Attention Matters to Multimodal LLMs in Document Understanding](https://arxiv.org/abs/2506.21600)
*Chang Liu,Hongkai Chen,Yujun Cai,Hang Wu,Qingwen Ye,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.CL

TL;DR: 研究发现，原始OCR文本会损害多模态大语言模型（MLLMs）的文档理解性能，提出了一种基于LaTex的结构化方法，显著提升了模型表现。


<details>
  <summary>Details</summary>
Motivation: 探讨输入格式如何影响多模态大语言模型（MLLMs）的文档理解性能，发现原始OCR文本反而会降低模型表现，原因是注意力分散和结构丢失。

Method: 提出了一种结构保留方法，使用LaTex范式编码文档元素，保持层次结构和空间关系，从而引导模型关注语义重要区域。

Result: 该方法显著提升了MLLMs在文档问答任务中的表现，且无需修改模型架构或额外训练。

Conclusion: 结构化文本能有效引导模型注意力，提升文档理解性能，为MLLMs的文档处理提供了新思路。

Abstract: Document understanding remains a significant challenge for multimodal large
language models (MLLMs). While previous research has primarily focused on
locating evidence pages through precise multimodal queries, our work
investigates a fundamental yet overlooked aspect: how input format influences
document comprehension performance. Through systematic analysis, we discover
that raw OCR text often impairs rather than improves MLLMs' performance, which
is a counterintuitive finding we attribute to attention dispersion and
structure loss. To further substantiate our hypothesis, we propose a novel
structure-preserving approach that encodes document elements using the LaTex
paradigm, maintaining the hierarchical organization and spatial relationships
critical for comprehension. Our attention analysis reveals that structured text
induces structured attention patterns on both textual and visual content,
directing models to focus on semantically meaningful regions while reducing
attention waste. This approach significantly enhances MLLMs' document question
answering performance across diverse document types without requiring
architectural modifications or additional training.

</details>


### [44] [BiMark: Unbiased Multilayer Watermarking for Large Language Models](https://arxiv.org/abs/2506.21602)
*Xiaoyan Feng,He Zhang,Yanjun Zhang,Leo Yu Zhang,Shirui Pan*

Main category: cs.CL

TL;DR: BiMark是一种新型水印框架，通过创新机制解决LLM生成文本的认证问题，平衡文本质量与水印容量，实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的进步，如何可靠识别LLM生成文本的真实性成为迫切需求。现有水印方法难以同时满足文本质量保持、模型无关检测和信息嵌入容量三大要求。

Method: 提出BiMark框架，包含三个关键技术：(1) 位翻转无偏重加权机制实现模型无关检测；(2) 多层架构提升可检测性且不损害生成质量；(3) 信息编码方法支持多比特水印。

Result: 实验表明，BiMark在短文本上的提取率比现有方法高30%，同时保持更低的困惑度，在下游任务（如摘要和翻译）中表现与非水印文本相当。

Conclusion: BiMark有效平衡了文本质量与水印容量的矛盾，为LLM生成文本的认证提供了实用解决方案。

Abstract: Recent advances in Large Language Models (LLMs) have raised urgent concerns
about LLM-generated text authenticity, prompting regulatory demands for
reliable identification mechanisms. Although watermarking offers a promising
solution, existing approaches struggle to simultaneously achieve three critical
requirements: text quality preservation, model-agnostic detection, and message
embedding capacity, which are crucial for practical implementation. To achieve
these goals, the key challenge lies in balancing the trade-off between text
quality preservation and message embedding capacity. To address this challenge,
we propose BiMark, a novel watermarking framework that achieves these
requirements through three key innovations: (1) a bit-flip unbiased reweighting
mechanism enabling model-agnostic detection, (2) a multilayer architecture
enhancing detectability without compromising generation quality, and (3) an
information encoding approach supporting multi-bit watermarking. Through
theoretical analysis and extensive experiments, we validate that, compared to
state-of-the-art multi-bit watermarking methods, BiMark achieves up to 30%
higher extraction rates for short texts while maintaining text quality
indicated by lower perplexity, and performs comparably to non-watermarked text
on downstream tasks such as summarization and translation.

</details>


### [45] [Operationalizing Automated Essay Scoring: A Human-Aware Approach](https://arxiv.org/abs/2506.21603)
*Yenisel Plasencia-Calaña*

Main category: cs.CL

TL;DR: 论文探讨了基于机器学习的AES系统与LLM方法在准确性、可解释性、偏见和鲁棒性等方面的优劣，旨在提升AES系统的可靠性和可信度。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于超越准确性，关注AES系统的人本操作化，包括偏见、鲁棒性和可解释性等关键维度。

Method: 通过比较基于机器学习的AES模型和大型语言模型（LLMs），分析它们在准确性、可解释性、偏见和鲁棒性等方面的表现。

Result: 研究发现，基于机器学习的AES模型在准确性上优于LLMs，但在可解释性上表现较差；LLMs提供更丰富的解释。两者在偏见和边缘分数鲁棒性方面均存在挑战。

Conclusion: 论文通过分析不同方法的优缺点，为开发更可靠和可信的AES方法提供了挑战和权衡的见解。

Abstract: This paper explores the human-centric operationalization of Automated Essay
Scoring (AES) systems, addressing aspects beyond accuracy. We compare various
machine learning-based approaches with Large Language Models (LLMs) approaches,
identifying their strengths, similarities and differences. The study
investigates key dimensions such as bias, robustness, and explainability,
considered important for human-aware operationalization of AES systems. Our
study shows that ML-based AES models outperform LLMs in accuracy but struggle
with explainability, whereas LLMs provide richer explanations. We also found
that both approaches struggle with bias and robustness to edge scores. By
analyzing these dimensions, the paper aims to identify challenges and
trade-offs between different methods, contributing to more reliable and
trustworthy AES methods.

</details>


### [46] [MemBench: Towards More Comprehensive Evaluation on the Memory of LLM-based Agents](https://arxiv.org/abs/2506.21605)
*Haoran Tan,Zeyu Zhang,Chen Ma,Xu Chen,Quanyu Dai,Zhenhua Dong*

Main category: cs.CL

TL;DR: 该论文提出了一个更全面的数据集和基准测试MemBench，用于评估基于LLM的代理的记忆能力，解决了现有评估在记忆层级和交互场景多样性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法在记忆层级多样性和交互场景上存在局限，且缺乏全面反映记忆能力的多维度指标。

Method: 构建包含事实记忆和反思记忆不同层级的数据集，提出参与和观察两种交互场景，并设计MemBench基准测试从效果、效率和容量多维度评估。

Result: 发布了MemBench数据集和项目，为研究社区提供了更全面的记忆能力评估工具。

Conclusion: MemBench通过多维度评估框架填补了LLM代理记忆能力评估的空白，推动了相关研究的发展。

Abstract: Recent works have highlighted the significance of memory mechanisms in
LLM-based agents, which enable them to store observed information and adapt to
dynamic environments. However, evaluating their memory capabilities still
remains challenges. Previous evaluations are commonly limited by the diversity
of memory levels and interactive scenarios. They also lack comprehensive
metrics to reflect the memory capabilities from multiple aspects. To address
these problems, in this paper, we construct a more comprehensive dataset and
benchmark to evaluate the memory capability of LLM-based agents. Our dataset
incorporates factual memory and reflective memory as different levels, and
proposes participation and observation as various interactive scenarios. Based
on our dataset, we present a benchmark, named MemBench, to evaluate the memory
capability of LLM-based agents from multiple aspects, including their
effectiveness, efficiency, and capacity. To benefit the research community, we
release our dataset and project at https://github.com/import-myself/Membench.

</details>


### [47] [Large Language Models as symbolic DNA of cultural dynamics](https://arxiv.org/abs/2506.21606)
*Parham Pourdavood,Michael Jacob,Terrence Deacon*

Main category: cs.CL

TL;DR: 该论文提出将大语言模型(LLM)视为人类文化动态的'DNA'，认为它们是存储人类符号表达压缩模式的外部化信息载体，需通过人类重新解释才能产生意义，最终促进文化创新。


<details>
  <summary>Details</summary>
Motivation: 研究旨在超越'LLM是自主智能还是简单模仿'的二元争论，探索其作为文化信息存储和催化人类创造力的本质角色。

Method: 通过分析压缩、解压缩、外部化和递归四个特征，类比DNA保存细胞动态的机制，阐释LLM保存文化规律的原理。

Result: LLM作为无理解能力的文化'化石'，通过人类解释形成递归反馈循环，成为低风险文化假设生成工具。

Conclusion: LLM的核心价值在于增强文化可进化性，为人类提供自我反思和假设生成的工具，而非替代人类智能。

Abstract: This paper proposes a novel conceptualization of Large Language Models (LLMs)
as externalized informational substrates that function analogously to DNA for
human cultural dynamics. Rather than viewing LLMs as either autonomous
intelligence or mere programmed mimicry, we argue they serve a broader role as
repositories that preserve compressed patterns of human symbolic
expression--"fossils" of meaningful dynamics that retain relational residues
without their original living contexts. Crucially, these compressed patterns
only become meaningful through human reinterpretation, creating a recursive
feedback loop where they can be recombined and cycle back to ultimately
catalyze human creative processes. Through analysis of four universal
features--compression, decompression, externalization, and recursion--we
demonstrate that just as DNA emerged as a compressed and externalized medium
for preserving useful cellular dynamics without containing explicit reference
to goal-directed physical processes, LLMs preserve useful regularities of human
culture without containing understanding of embodied human experience.
Therefore, we argue that LLMs' significance lies not in rivaling human
intelligence, but in providing humanity a tool for self-reflection and playful
hypothesis-generation in a low-stakes, simulated environment. This framework
positions LLMs as tools for cultural evolvability, enabling humanity to
generate novel hypotheses about itself while maintaining the human
interpretation necessary to ground these hypotheses in ongoing human aesthetics
and norms.

</details>


### [48] [CORE-KG: An LLM-Driven Knowledge Graph Construction Framework for Human Smuggling Networks](https://arxiv.org/abs/2506.21607)
*Dipak Meher,Carlotta Domeniconi,Guadalupe Correa-Cabrera*

Main category: cs.CL

TL;DR: CORE-KG框架通过两步流程（指代消解和领域引导提取）从法律文本构建更清晰的知识图谱，减少节点重复和噪声。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱方法在处理法律文本时存在静态模板依赖、指代消解缺失以及LLM产生的噪声和碎片化问题，需要更有效的解决方案。

Method: 采用模块化框架CORE-KG，结合类型感知的指代消解（结构化LLM提示）和领域引导的实体关系提取（基于改进的GraphRAG框架）。

Result: 相比基线方法，CORE-KG减少33.28%的节点重复和38.37%的法律噪声，生成更干净、连贯的图谱结构。

Conclusion: CORE-KG为分析复杂犯罪网络提供了更可靠的知识图谱基础，显著提升了处理法律文本的效果。

Abstract: Human smuggling networks are increasingly adaptive and difficult to analyze.
Legal case documents offer valuable insights but are unstructured, lexically
dense, and filled with ambiguous or shifting references-posing challenges for
automated knowledge graph (KG) construction. Existing KG methods often rely on
static templates and lack coreference resolution, while recent LLM-based
approaches frequently produce noisy, fragmented graphs due to hallucinations,
and duplicate nodes caused by a lack of guided extraction. We propose CORE-KG,
a modular framework for building interpretable KGs from legal texts. It uses a
two-step pipeline: (1) type-aware coreference resolution via sequential,
structured LLM prompts, and (2) entity and relationship extraction using
domain-guided instructions, built on an adapted GraphRAG framework. CORE-KG
reduces node duplication by 33.28%, and legal noise by 38.37% compared to a
GraphRAG-based baseline-resulting in cleaner and more coherent graph
structures. These improvements make CORE-KG a strong foundation for analyzing
complex criminal networks.

</details>


### [49] [SysTemp: A Multi-Agent System for Template-Based Generation of SysML v2](https://arxiv.org/abs/2506.21608)
*Yasmine Bouamra,Bruno Yun,Alexandre Poisson,Frédéric Armetta*

Main category: cs.CL

TL;DR: SysTemp系统通过多智能体方法提升SysML v2模型从自然语言生成的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 解决SysML v2模型自动生成中学习语料稀缺和语法复杂的问题。

Method: 基于多智能体系统，包含一个模板生成器来结构化生成过程。

Result: 评估显示SysTemp能有效提升SysML v2建模的生成质量。

Conclusion: SysTemp展示了在复杂系统建模中改进自动生成的潜力。

Abstract: The automatic generation of SysML v2 models represents a major challenge in
the engineering of complex systems, particularly due to the scarcity of
learning corpora and complex syntax. We present SysTemp, a system aimed at
facilitating and improving the creation of SysML v2 models from natural
language specifications. It is based on a multi-agent system, including a
template generator that structures the generation process. We discuss the
advantages and challenges of this system through an evaluation, highlighting
its potential to improve the quality of the generations in SysML v2 modeling.

</details>


### [50] [From Thinking to Output: Chain-of-Thought and Text Generation Characteristics in Reasoning Language Models](https://arxiv.org/abs/2506.21609)
*Junhao Liu,Zhenhao Xu,Yuxin Fang,Yichuan Chen,Zuobin Ying,Wenhan Chang*

Main category: cs.CL

TL;DR: 该论文提出了一种新框架，用于分析和比较四种前沿大语言模型（GPT-o1、DeepSeek-R1、Kimi-k1.5和Grok-3）的推理特性，揭示了它们在推理过程中的不同模式及其与输出的关联。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多忽视了对大语言模型推理过程及输出的系统比较，特别是在自我反思模式（'顿悟时刻'）和跨领域关联方面。

Method: 采用关键词统计和LLM-as-a-judge范式，结合多样化的现实场景问题数据集，评估模型推理的连贯性和输出准确性。

Result: 研究发现不同模型在推理深度、中间步骤依赖以及与GPT-o1的相似性等方面存在显著差异。

Conclusion: 该工作为计算效率与推理鲁棒性之间的权衡提供了见解，并为实际应用中的模型设计和评估提出了实用建议。

Abstract: Recently, there have been notable advancements in large language models
(LLMs), demonstrating their growing abilities in complex reasoning. However,
existing research largely overlooks a thorough and systematic comparison of
these models' reasoning processes and outputs, particularly regarding their
self-reflection pattern (also termed "Aha moment") and the interconnections
across diverse domains. This paper proposes a novel framework for analyzing the
reasoning characteristics of four cutting-edge large reasoning models (GPT-o1,
DeepSeek-R1, Kimi-k1.5, and Grok-3) using keywords statistic and LLM-as-a-judge
paradigm. Our approach connects their internal thinking processes with their
final outputs. A diverse dataset consists of real-world scenario-based
questions covering logical deduction, causal inference, and multi-step
problem-solving. Additionally, a set of metrics is put forward to assess both
the coherence of reasoning and the accuracy of the outputs. The research
results uncover various patterns of how these models balance exploration and
exploitation, deal with problems, and reach conclusions during the reasoning
process. Through quantitative and qualitative comparisons, disparities among
these models are identified in aspects such as the depth of reasoning, the
reliance on intermediate steps, and the degree of similarity between their
thinking processes and output patterns and those of GPT-o1. This work offers
valuable insights into the trade-off between computational efficiency and
reasoning robustness and provides practical recommendations for enhancing model
design and evaluation in practical applications. We publicly release our
project at: https://github.com/ChangWenhan/FromThinking2Output

</details>


### [51] [Does Multimodality Lead to Better Time Series Forecasting?](https://arxiv.org/abs/2506.21611)
*Xiyuan Zhang,Boran Han,Haoyang Fang,Abdul Fatir Ansari,Shuai Zhang,Danielle C. Maddix,Cuixiong Hu,Andrew Gordon Wilson,Michael W. Mahoney,Hao Wang,Yan Liu,Huzefa Rangwala,George Karypis,Bernie Wang*

Main category: cs.CL

TL;DR: 本文系统研究了多模态（文本+时间序列）在预测任务中的效果，发现其增益并非普遍存在，并提出了模型架构和数据特性对效果影响的关键条件。


<details>
  <summary>Details</summary>
Motivation: 尽管将文本信息融入基础模型以提升时间序列预测的研究日益增多，但多模态整合在何种条件下有效仍不明确。本文旨在系统探究这一问题。

Method: 在涵盖7个领域的14个预测任务上评估两种主流多模态预测范式：基于对齐的方法（对齐时间序列与文本表示）和基于提示的方法（直接提示大语言模型进行预测）。

Result: 多模态增益具有条件性：(1)文本模型需高容量；(2)时间序列模型较弱时；(3)对齐策略得当；(4)训练数据充足；(5)文本需提供时间序列之外的补充信息。

Conclusion: 研究为多模态预测提供了实用指南，明确了其适用场景与局限性，指出盲目整合文本未必优于单模态基线。

Abstract: Recently, there has been growing interest in incorporating textual
information into foundation models for time series forecasting. However, it
remains unclear whether and under what conditions such multimodal integration
consistently yields gains. We systematically investigate these questions across
a diverse benchmark of 14 forecasting tasks spanning 7 domains, including
health, environment, and economics. We evaluate two popular multimodal
forecasting paradigms: aligning-based methods, which align time series and text
representations; and prompting-based methods, which directly prompt large
language models for forecasting. Although prior works report gains from
multimodal input, we find these effects are not universal across datasets and
models, and multimodal methods sometimes do not outperform the strongest
unimodal baselines. To understand when textual information helps, we
disentangle the effects of model architectural properties and data
characteristics. Our findings highlight that on the modeling side,
incorporating text information is most helpful given (1) high-capacity text
models, (2) comparatively weaker time series models, and (3) appropriate
aligning strategies. On the data side, performance gains are more likely when
(4) sufficient training data is available and (5) the text offers complementary
predictive signal beyond what is already captured from the time series alone.
Our empirical findings offer practical guidelines for when multimodality can be
expected to aid forecasting tasks, and when it does not.

</details>


### [52] [AdaptGOT: A Pre-trained Model for Adaptive Contextual POI Representation Learning](https://arxiv.org/abs/2506.21612)
*Xiaobin Ren,Xinyu Zhu,Kaiqi Zhao*

Main category: cs.CL

TL;DR: 提出AdaptGOT模型，通过自适应表示学习和地理-共现-文本（GOT）表示，结合多种采样策略和注意力机制，提升POI嵌入的多任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有POI嵌入方法在多上下文采样、多任务适应性和泛化能力方面存在不足，需更高效的表示学习框架。

Method: AdaptGOT模型包含三部分：1) 混合采样生成上下文邻域；2) 基于注意力的GOT表示；3) MoE自适应编码器-解码器架构。

Result: 在多个真实数据集和POI任务上验证了模型优越性。

Conclusion: AdaptGOT通过融合地理、共现和文本信息，显著提升了POI嵌入的泛化能力和多任务表现。

Abstract: Currently, considerable strides have been achieved in Point-of-Interest (POI)
embedding methodologies, driven by the emergence of novel POI tasks like
recommendation and classification. Despite the success of task-specific,
end-to-end models in POI embedding, several challenges remain. These include
the need for more effective multi-context sampling strategies, insufficient
exploration of multiple POI contexts, limited versatility, and inadequate
generalization. To address these issues, we propose the AdaptGOT model, which
integrates both the (Adapt)ive representation learning technique and the
Geographical-Co-Occurrence-Text (GOT) representation with a particular emphasis
on Geographical location, Co-Occurrence and Textual information. The AdaptGOT
model comprises three key components: (1) contextual neighborhood generation,
which integrates advanced mixed sampling techniques such as KNN, density-based,
importance-based, and category-aware strategies to capture complex contextual
neighborhoods; (2) an advanced GOT representation enhanced by an attention
mechanism, designed to derive high-quality, customized representations and
efficiently capture complex interrelations between POIs; and (3) the MoE-based
adaptive encoder-decoder architecture, which ensures topological consistency
and enriches contextual representation by minimizing Jensen-Shannon divergence
across varying contexts. Experiments on two real-world datasets and multiple
POI tasks substantiate the superior performance of the proposed AdaptGOT model.

</details>


### [53] [ChildGuard: A Specialized Dataset for Combatting Child-Targeted Hate Speech](https://arxiv.org/abs/2506.21613)
*Gautam Siddharth Kashyap,Mohammad Anas Azeez,Rafiq Ali,Zohaib Hasan Siddiqui,Jiechao Gao,Usman Naseem*

Main category: cs.CL

TL;DR: 论文提出ChildGuard数据集，针对儿童仇恨言论检测，填补现有数据集的不足，并评估现有方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有仇恨言论数据集缺乏针对儿童的年龄特定标注，无法捕捉细微语境及对儿童的独特情感影响，亟需专门数据集解决这一问题。

Method: 通过从现有语料库中提取并添加儿童特定标注，构建ChildGuard数据集，并评估包括大语言模型在内的现有仇恨言论检测方法。

Result: ChildGuard数据集提供了多样化的儿童仇恨言论语境，为改进检测方法奠定了坚实基础。

Conclusion: 公开ChildGuard数据集以促进相关研究，帮助开发更有效的儿童仇恨言论检测与缓解方法。

Abstract: The increasing prevalence of child-targeted hate speech online underscores
the urgent need for specialized datasets to address this critical issue.
Existing hate speech datasets lack agespecific annotations, fail to capture
nuanced contexts, and overlook the unique emotional impact on children. To
bridge this gap, we introduce ChildGuard1, a curated dataset derived from
existing corpora and enriched with child-specific annotations. ChildGuard
captures diverse contexts of child-targeted hate speech, spanning age groups.
We benchmark existing state-of-the-art hate speech detection methods, including
Large Language Models (LLMs), and assess their effectiveness in detecting and
contextualizing child-targeted hate speech. To foster further research in this
area, we publicly release ChildGuard, providing a robust foundation for
developing improved methods to detect and mitigate such harm.

</details>


### [54] [LastingBench: Defend Benchmarks Against Knowledge Leakage](https://arxiv.org/abs/2506.21614)
*Yixiong Fang,Tianran Sun,Yuling Shi,Min Wang,Xiaodong Gu*

Main category: cs.CL

TL;DR: 论文提出LastingBench框架，通过扰动和改写泄露点来减少大语言模型在问答基准测试中的记忆效应，确保评估的长期有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型可能通过记忆任务特定数据在问答基准测试中'作弊'，导致评估结果失真。现有研究多关注检测数据泄露，而忽略如何减轻其影响和维护基准的长期效用。

Method: LastingBench通过扰动识别上下文中的泄露点，并将其改写成反事实内容，既破坏模型的记忆能力，又保留基准的原始评估意图。

Result: 在先进问答基准测试上的评估显示，LastingBench显著缩小了性能差距，有效减少了记忆效应。

Conclusion: LastingBench提供了一种实用且可扩展的解决方案，确保基准测试的长期鲁棒性，促进对大语言模型更公平、可解释的评估。

Abstract: The increasing complexity of large language models (LLMs) raises concerns
about their ability to "cheat" on standard Question Answering (QA) benchmarks
by memorizing task-specific data. This undermines the validity of benchmark
evaluations, as they no longer reflect genuine model capabilities but instead
the effects of data leakage. While prior work has focused on detecting such
leakage, little attention has been given to mitigating its impact and
preserving the long-term utility of benchmarks. In this paper, we introduce
LastingBench, a novel framework designed to continuously reinforce and
safeguard existing benchmarks against knowledge leakage. LastingBench
identifies leakage points in the context through perturbation, then rewrites
the leakage points to counterfactual ones-disrupting memorization while
preserving the benchmark's original evaluative intent. Evaluations of
state-of-the-art QA benchmarks show significant performance gaps, highlighting
the efficacy of LastingBench in reducing memorization effects. LastingBench
offers a practical and scalable solution to ensure benchmark robustness over
time, promoting fairer and more interpretable evaluations of LLMs.

</details>


### [55] [Refine Medical Diagnosis Using Generation Augmented Retrieval and Clinical Practice Guidelines](https://arxiv.org/abs/2506.21615)
*Wenhao Li,Hongkuan Zhang,Hongwei Zhang,Zhengxu Li,Zengjie Dong,Yafan Chen,Niranjan Bidargaddi,Hong Liu*

Main category: cs.CL

TL;DR: GARMLE-G框架通过结合LLM预测与临床指南检索，生成无幻觉、符合指南的医疗建议，提升诊断模型的临床实用性。


<details>
  <summary>Details</summary>
Motivation: 现有医疗语言模型依赖ICD编码，无法捕捉临床医生的复杂推理过程，导致临床实用性受限。需要一种能结合临床实践指南(CPGs)的方法。

Method: 提出GARMLE-G框架：(1)整合LLM预测与EHR数据生成查询，(2)通过嵌入相似性检索相关CPG片段，(3)将指南内容与模型输出融合生成建议。

Result: 高血压诊断原型系统在检索精度、语义相关性和指南遵循性上优于RAG基线，同时保持轻量级架构。

Conclusion: GARMLE-G提供了一种可扩展、低成本且无幻觉的方法，使医疗语言模型与循证临床实践结合，具有广泛临床应用潜力。

Abstract: Current medical language models, adapted from large language models (LLMs),
typically predict ICD code-based diagnosis from electronic health records
(EHRs) because these labels are readily available. However, ICD codes do not
capture the nuanced, context-rich reasoning clinicians use for diagnosis.
Clinicians synthesize diverse patient data and reference clinical practice
guidelines (CPGs) to make evidence-based decisions. This misalignment limits
the clinical utility of existing models. We introduce GARMLE-G, a
Generation-Augmented Retrieval framework that grounds medical language model
outputs in authoritative CPGs. Unlike conventional Retrieval-Augmented
Generation based approaches, GARMLE-G enables hallucination-free outputs by
directly retrieving authoritative guideline content without relying on
model-generated text. It (1) integrates LLM predictions with EHR data to create
semantically rich queries, (2) retrieves relevant CPG knowledge snippets via
embedding similarity, and (3) fuses guideline content with model output to
generate clinically aligned recommendations. A prototype system for
hypertension diagnosis was developed and evaluated on multiple metrics,
demonstrating superior retrieval precision, semantic relevance, and clinical
guideline adherence compared to RAG-based baselines, while maintaining a
lightweight architecture suitable for localized healthcare deployment. This
work provides a scalable, low-cost, and hallucination-free method for grounding
medical language models in evidence-based clinical practice, with strong
potential for broader clinical deployment.

</details>


### [56] [TIM: A Large-Scale Dataset and large Timeline Intelligence Model for Open-domain Timeline Summarization](https://arxiv.org/abs/2506.21616)
*Chuanrui Hu,Wei Hu,Penghang Yu,Hua Zhang,Bing-Kun Bao*

Main category: cs.CL

TL;DR: 提出首个开放域时间线摘要大模型TIM，通过渐进优化策略提升摘要质量，解决现有LLM在主题相关性和时间线理解上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有通用大模型在开放域时间线摘要中存在主题相关性判断差、演化规律理解弱的问题，导致摘要包含无关信息或错误时间戳。

Method: 1) 构建包含1000+新闻主题的大规模TLS数据集；2) 采用渐进优化策略：先指令微调增强摘要能力，再通过双对齐奖励学习（语义+时序）理解主题演化。

Result: TIM模型在开放域实验中展现出强大的时间线摘要能力，显著优于基线方法。

Conclusion: TIM是首个专用于开放域时间线摘要的大模型，其渐进优化策略有效解决了主题演化理解的关键挑战。

Abstract: Open-domain Timeline Summarization (TLS) is crucial for monitoring the
evolution of news topics. To identify changes in news topics, existing methods
typically employ general Large Language Models (LLMs) to summarize relevant
timestamps from retrieved news. While general LLMs demonstrate capabilities in
zero-shot news summarization and timestamp localization, they struggle with
assessing topic relevance and understanding topic evolution. Consequently, the
summarized information often includes irrelevant details or inaccurate
timestamps. To address these issues, we propose the first large Timeline
Intelligence Model (TIM) for open-domain TLS, which is capable of effectively
summarizing open-domain timelines. Specifically, we begin by presenting a
large-scale TLS dataset, comprising over 1,000 news topics and more than 3,000
annotated TLS instances. Furthermore, we propose a progressive optimization
strategy, which gradually enhance summarization performance. It employs
instruction tuning to enhance summarization and topic-irrelevant information
filtering capabilities. Following this, it exploits a novel dual-alignment
reward learning method that incorporates both semantic and temporal
perspectives, thereby improving the understanding of topic evolution
principles. Through this progressive optimization strategy, TIM demonstrates a
robust ability to summarize open-domain timelines. Extensive experiments in
open-domain demonstrate the effectiveness of our TIM.

</details>


### [57] [TrajTok: Technical Report for 2025 Waymo Open Sim Agents Challenge](https://arxiv.org/abs/2506.21618)
*Zhiyuan Zhang,Xiaosong Jia,Guanyu Chen,Qifeng Li,Junchi Yan*

Main category: cs.CL

TL;DR: TrajTok是一种结合数据驱动和基于规则的轨迹标记方法，用于离散的下一个标记预测行为生成模型，具有更好的覆盖性、对称性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了提高行为生成模型的性能，特别是在轨迹预测的覆盖性、对称性和鲁棒性方面，作者提出了TrajTok标记器。

Method: 结合数据驱动和基于规则的方法，设计了一种空间感知的标签平滑方法，用于交叉熵损失函数，并将其应用于SMART模型。

Result: 在Waymo Open Sim Agents Challenge 2025中，SMART模型达到了0.7852的真实性分数，表现优异。

Conclusion: TrajTok标记器和空间感知标签平滑方法显著提升了行为生成模型的性能，未来将开源代码。

Abstract: In this technical report, we introduce TrajTok, a trajectory tokenizer for
discrete next-token-prediction based behavior generation models, which combines
data-driven and rule-based methods with better coverage, symmetry and
robustness, along with a spatial-aware label smoothing method for cross-entropy
loss. We adopt the tokenizer and loss for the SMART model and reach a superior
performance with realism score of 0.7852 on the Waymo Open Sim Agents Challenge
2025. We will open-source the code in the future.

</details>


### [58] [IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech](https://arxiv.org/abs/2506.21619)
*Siyi Zhou,Yiquan Zhou,Yi He,Xun Zhou,Jinchao Wang,Wei Deng,Jingchen Shu*

Main category: cs.CL

TL;DR: IndexTTS2提出了一种新的自回归模型友好的语音时长控制方法，支持精确时长控制和自由生成模式，并实现了音色与情感的分离控制，在零样本设置下表现优异。


<details>
  <summary>Details</summary>
Motivation: 自回归TTS模型在语音自然度上有优势，但其逐令牌生成机制难以精确控制合成语音的时长，限制了在需要严格音视频同步的应用中的使用。

Method: IndexTTS2引入了两种生成模式：一种允许明确指定生成令牌数以精确控制时长；另一种无需手动输入，保留输入提示的韵律特征。此外，通过GPT潜在表示增强语音稳定性，并设计了基于文本描述的软指令机制。

Result: 实验结果表明，IndexTTS2在词错误率、说话人相似度和情感保真度上优于现有最先进的零样本TTS模型。

Conclusion: IndexTTS2通过创新的时长控制方法和情感-音色解耦技术，显著提升了TTS模型在精确控制和情感表达方面的性能。

Abstract: Large-scale text-to-speech (TTS) models are typically categorized into
autoregressive and non-autoregressive systems. Although autoregressive systems
exhibit certain advantages in speech naturalness, their token-by-token
generation mechanism makes it difficult to precisely control the duration of
synthesized speech. This is a key limitation in applications such as video
dubbing that require strict audio-visual synchronization. This paper introduces
IndexTTS2, which proposes a novel and autoregressive-model-friendly method for
speech duration control. The method supports two generation modes: one allows
explicit specification of the number of generated tokens for precise duration
control; the other does not require manual input and lets the model freely
generate speech while preserving prosodic characteristics from the input
prompt. Furthermore, IndexTTS2 achieves disentanglement between emotional
expression and speaker identity, enabling independent control of timbre and
emotion. In the zero-shot setting, the model can perfectly reproduce the
emotional characteristics of the input prompt. Users may also provide a
separate emotion prompt, even from a different speaker, allowing the model to
reconstruct the target timbre while conveying the desired emotion. To enhance
clarity during strong emotional expressions, we incorporate GPT latent
representations to improve speech stability. Meanwhile, to lower the barrier
for emotion control, we design a soft instruction mechanism based on textual
descriptions by fine-tuning Qwen3. This enables effective guidance of speech
generation with desired emotional tendencies using natural language input.
Experimental results demonstrate that IndexTTS2 outperforms existing
state-of-the-art zero-shot TTS models in word error rate, speaker similarity,
and emotional fidelity.

</details>


### [59] [How Large Language Models play humans in online conversations: a simulated study of the 2016 US politics on Reddit](https://arxiv.org/abs/2506.21620)
*Daniele Cirulli,Giulio Cimini,Giovanni Palermo*

Main category: cs.CL

TL;DR: 研究评估了GPT-4在模拟2016年总统选举期间Reddit用户评论的能力，发现其能生成真实但偏向共识的评论，且人工难以区分真假评论。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（LLMs）在政治相关在线讨论中模仿用户生成内容的潜力及其对舆论的影响。

Method: 通过三个实验，让GPT-4模拟真实或虚构的党派用户生成评论，分析政治倾向、情感和语言特征，并与真实评论和基准模型对比。

Result: GPT-4能生成支持或反对候选人的真实评论，但更易形成共识；真假评论在语义嵌入空间可区分，但人工无法辨别。

Conclusion: LLMs可能潜在地渗透在线讨论并影响政治辩论，对AI驱动的舆论操纵具有广泛意义。

Abstract: Large Language Models (LLMs) have recently emerged as powerful tools for
natural language generation, with applications spanning from content creation
to social simulations. Their ability to mimic human interactions raises both
opportunities and concerns, particularly in the context of politically relevant
online discussions. In this study, we evaluate the performance of LLMs in
replicating user-generated content within a real-world, divisive scenario:
Reddit conversations during the 2016 US Presidential election. In particular,
we conduct three different experiments, asking GPT-4 to generate comments by
impersonating either real or artificial partisan users. We analyze the
generated comments in terms of political alignment, sentiment, and linguistic
features, comparing them against real user contributions and benchmarking
against a null model. We find that GPT-4 is able to produce realistic comments,
both in favor of or against the candidate supported by the community, yet
tending to create consensus more easily than dissent. In addition we show that
real and artificial comments are well separated in a semantically embedded
space, although they are indistinguishable by manual inspection. Our findings
provide insights on the potential use of LLMs to sneak into online discussions,
influence political debate and shape political narratives, bearing broader
implications of AI-driven discourse manipulation.

</details>


### [60] [The Open Proof Corpus: A Large-Scale Study of LLM-Generated Mathematical Proofs](https://arxiv.org/abs/2506.21621)
*Jasper Dekoninck,Ivo Petrov,Kristian Minchev,Mislav Balunovic,Martin Vechev,Miroslav Marinov,Maria Drencheva,Lyuba Konova,Milen Shumanov,Kaloyan Tsvetkov,Nikolay Drenchev,Lazar Todorov,Kalina Nikolova,Nikolay Georgiev,Vanesa Kalinkova,Margulan Ismoldayev*

Main category: cs.CL

TL;DR: 论文介绍了Open Proof Corpus (OPC)，一个包含5000多个由先进大语言模型生成并经人工评估的数学证明数据集，用于推动自动证明生成研究。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在数学证明生成方面进展显著，但缺乏大规模、经人工评估的高质量数据集，阻碍了进一步的发展。

Method: 创建了包含5000多个经人工评估的证明的OPC数据集，并利用该数据集微调了一个8B参数的模型。

Result: 微调后的模型在评估证明正确性任务上表现与最佳模型Gemini-2.5-Pro相当。

Conclusion: OPC数据集为自动证明生成研究提供了重要资源，并展示了其在模型训练中的实用性。

Abstract: In recent months, large language models (LLMs) have made significant progress
in mathematical proof generation, but further advancement is hindered by the
lack of a large-scale, high-quality dataset of human-evaluated proofs. While
expensive to create, such a dataset is essential for driving improvements in
training and enabling a rigorous analysis of proof generation capabilities. In
this work, we present the Open Proof Corpus (OPC), a dataset comprising over
5,000 human-evaluated proofs produced by state-of-the-art LLMs. The OPC was
specifically designed for broad applicability and downstream usage in proof
generation research and is the first to include a substantial number of
correct, LLM-generated solutions to problems from prestigious mathematics
competitions such as the USAMO and IMO. Using the OPC, we explore critical
questions in automated proof generation: (1) the performance gap between
natural language and formal proof generation, (2) the discrepancy between
final-answer accuracy and full-proof validity, and (3) the impact of best-of-n
selection on proof quality. Finally, to showcase the utility of the OPC, we
finetune an 8B-parameter model on the dataset, obtaining a model that performs
on par with the best model, Gemini-2.5-Pro, on the task of evaluating proof
correctness.

</details>


### [61] [Adapting Foundation Speech Recognition Models to Impaired Speech: A Semantic Re-chaining Approach for Personalization of German Speech](https://arxiv.org/abs/2506.21622)
*Niclas Pokel,Pehuén Moure,Roman Boehringer,Yingqiang Gao*

Main category: cs.CL

TL;DR: 提出了一种轻量级方法，通过个性化调整ASR模型来改善对非标准语音（如脑瘫或遗传障碍患者）的识别效果。


<details>
  <summary>Details</summary>
Motivation: 当前ASR系统（如Whisper）对非标准语音识别效果不佳，主要因训练数据不足及样本采集标注困难。

Method: 设计了一个实用流程：筛选关键词并增强小规模语音障碍数据集的语义连贯性，实现模型个性化适配。

Result: 在结构性语音障碍儿童数据上测试，转录质量显著提升。

Conclusion: 该方法有望降低非典型语音人群的沟通障碍，展现了ASR个性化改进的潜力。

Abstract: Speech impairments caused by conditions such as cerebral palsy or genetic
disorders pose significant challenges for automatic speech recognition (ASR)
systems. Despite recent advances, ASR models like Whisper struggle with
non-normative speech due to limited training data and the difficulty of
collecting and annotating non-normative speech samples. In this work, we
propose a practical and lightweight pipeline to personalize ASR models,
formalizing the selection of words and enriching a small, speech-impaired
dataset with semantic coherence. Applied to data from a child with a structural
speech impairment, our approach shows promising improvements in transcription
quality, demonstrating the potential to reduce communication barriers for
individuals with atypical speech patterns.

</details>


### [62] [Performance of diverse evaluation metrics in NLP-based assessment and text generation of consumer complaints](https://arxiv.org/abs/2506.21623)
*Peiheng Gao,Chen Yang,Ning Sun,Ričardas Zitikis*

Main category: cs.CL

TL;DR: 该研究通过结合人类经验训练的算法和合成数据生成方法，提升机器学习在文本分类中的性能，减少数据获取成本并增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习在文本分类中难以准确捕捉自然语言中的细微语义差异，尤其是在消费者投诉领域，这影响了分类的准确性和适用性。

Method: 研究采用人类经验训练的算法识别语义差异，并结合专家评估的生成对抗网络生成高质量合成数据，优化分类器性能。

Result: 结合专家训练的分类器和合成数据，显著提升了分类器性能，降低了数据集获取成本，并改善了评估指标和鲁棒性。

Conclusion: 通过融合人类经验和合成数据生成，该研究为提升文本分类任务的准确性和效率提供了有效解决方案。

Abstract: Machine learning (ML) has significantly advanced text classification by
enabling automated understanding and categorization of complex, unstructured
textual data. However, accurately capturing nuanced linguistic patterns and
contextual variations inherent in natural language, particularly within
consumer complaints, remains a challenge. This study addresses these issues by
incorporating human-experience-trained algorithms that effectively recognize
subtle semantic differences crucial for assessing consumer relief eligibility.
Furthermore, we propose integrating synthetic data generation methods that
utilize expert evaluations of generative adversarial networks and are refined
through expert annotations. By combining expert-trained classifiers with
high-quality synthetic data, our research seeks to significantly enhance
machine learning classifier performance, reduce dataset acquisition costs, and
improve overall evaluation metrics and robustness in text classification tasks.

</details>


### [63] [Doc2SAR: A Synergistic Framework for High-Fidelity Extraction of Structure-Activity Relationships from Scientific Documents](https://arxiv.org/abs/2506.21625)
*Jiaxi Zhuang,Kangning Li,Jue Hou,Mingjun Xu,Zhifeng Gao,Hengxing Cai*

Main category: cs.CL

TL;DR: 论文提出DocSAR-200基准和Doc2SAR框架，通过结合领域专用工具与微调的多模态大模型，显著提升科学文献中分子构效关系提取的准确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在提取分子构效关系时面临文档格式多样性和专业任务精度不足的挑战，需要更通用的解决方案。

Method: 提出Doc2SAR框架，整合领域专用工具与经过监督微调的多模态大语言模型，并建立DocSAR-200基准进行评估。

Result: Doc2SAR在DocSAR-200上达到80.78%的表格召回率，比GPT-4o提升51.48%，并展示了高效的推理能力和实用网页应用。

Conclusion: Doc2SAR框架在分子构效关系提取任务中实现了最先进的性能，为药物发现和材料研究提供了有效工具。

Abstract: Extracting molecular structure-activity relationships (SARs) from scientific
literature and patents is essential for drug discovery and materials research.
However, this task remains challenging due to heterogeneous document formats
and limitations of existing methods. Specifically, rule-based approaches
relying on rigid templates fail to generalize across diverse document layouts,
while general-purpose multimodal large language models (MLLMs) lack sufficient
accuracy and reliability for specialized tasks, such as layout detection and
optical chemical structure recognition (OCSR). To address these challenges, we
introduce DocSAR-200, a rigorously annotated benchmark of 200 scientific
documents designed specifically for evaluating SAR extraction methods.
Additionally, we propose Doc2SAR, a novel synergistic framework that integrates
domain-specific tools with MLLMs enhanced via supervised fine-tuning (SFT).
Extensive experiments demonstrate that Doc2SAR achieves state-of-the-art
performance across various document types, significantly outperforming leading
end-to-end baselines. Specifically, Doc2SAR attains an overall Table Recall of
80.78% on DocSAR-200, exceeding end2end GPT-4o by 51.48%. Furthermore, Doc2SAR
demonstrates practical usability through efficient inference and is accompanied
by a web app.

</details>


### [64] [Do We Really Need GNNs with Explicit Structural Modeling? MLPs Suffice for Language Model Representations](https://arxiv.org/abs/2506.21682)
*Li Zhou,Hao Jiang,Junjie Li,Zefeng Zhao,Feng Jiang,Wenyu Chen,Haizhou Li*

Main category: cs.CL

TL;DR: 该论文通过信息论视角提出一个探测框架，评估结构建模对语言模型表示的影响，并探索MLPs作为GNNs高效替代方案的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究发现GNNs未能充分利用结构信息，而MLPs在结构感知任务中表现优异，因此探究结构建模的作用及MLPs的潜力。

Method: 提出包含控制模块的探测框架，分离评估GNNs的消息传递和特征转换操作，使用Edge Probing Suite进行实验。

Result: MLPs作为特征转换模块能有效提升语言模型表示中的语言学知识，而仅依赖消息传递的模型表现较差。

Conclusion: 特征转换操作对提升模型性能至关重要，MLPs可作为GNNs的高效替代方案。

Abstract: Explicit structural information has been proven to be encoded by Graph Neural
Networks (GNNs), serving as auxiliary knowledge to enhance model capabilities
and improve performance in downstream NLP tasks. However, recent studies
indicate that GNNs fail to fully utilize structural information, whereas
Multi-Layer Perceptrons (MLPs), despite lacking the message-passing mechanisms
inherent to GNNs, exhibit a surprising ability in structure-aware tasks.
Motivated by these findings, this paper introduces a comprehensive probing
framework from an information-theoretic perspective. The framework is designed
to systematically assess the role of explicit structural modeling in enhancing
language model (LM) representations and to investigate the potential of MLPs as
efficient and scalable alternatives to GNNs. We extend traditional probing
classifiers by incorporating a control module that allows for selective use of
either the full GNN model or its decoupled components, specifically, the
message-passing and feature-transformation operations.This modular approach
isolates and assesses the individual contributions of these operations,
avoiding confounding effects from the complete GNN architecture. Using the Edge
Probing Suite, a diagnostic tool for evaluating the linguistic knowledge
encoded in LMs, we find that MLPs, when used as feature-transformation modules,
consistently improve the linguistic knowledge captured in LM representations
across different architectures. They effectively encode both syntactic and
semantic patterns. Similarly, GNNs that incorporate feature-transformation
operations show beneficial effects. In contrast, models that rely solely on
message-passing operations tend to underperform, often leading to negative
impacts on probing task performance.

</details>


### [65] [ANUBHUTI: A Comprehensive Corpus For Sentiment Analysis In Bangla Regional Languages](https://arxiv.org/abs/2506.21686)
*Swastika Kundu,Autoshi Ibrahim,Mithila Rahman,Tanvir Ahmed*

Main category: cs.CL

TL;DR: 论文介绍了ANUBHUTI数据集，包含2000句孟加拉语方言的情感分析数据，填补了低资源方言情感分析的空白。


<details>
  <summary>Details</summary>
Motivation: 由于语言多样性和标注数据有限，孟加拉语方言的情感分析研究不足。

Method: 通过人工翻译和双重标注方案（多类主题标签和多标签情感标注），构建了包含四种方言的数据集，并进行了质量检查。

Result: 数据集在方言间具有强一致性，并通过了系统检查，确保了数据质量。

Conclusion: ANUBHUTI数据集为低资源孟加拉语方言的情感分析提供了重要资源，有助于更准确的自然语言处理。

Abstract: Sentiment analysis for regional dialects of Bangla remains an underexplored
area due to linguistic diversity and limited annotated data. This paper
introduces ANUBHUTI, a comprehensive dataset consisting of 2000 sentences
manually translated from standard Bangla into four major regional dialects
Mymensingh, Noakhali, Sylhet, and Chittagong. The dataset predominantly
features political and religious content, reflecting the contemporary socio
political landscape of Bangladesh, alongside neutral texts to maintain balance.
Each sentence is annotated using a dual annotation scheme: multiclass thematic
labeling categorizes sentences as Political, Religious, or Neutral, and
multilabel emotion annotation assigns one or more emotions from Anger,
Contempt, Disgust, Enjoyment, Fear, Sadness, and Surprise. Expert native
translators conducted the translation and annotation, with quality assurance
performed via Cohens Kappa inter annotator agreement, achieving strong
consistency across dialects. The dataset was further refined through systematic
checks for missing data, anomalies, and inconsistencies. ANUBHUTI fills a
critical gap in resources for sentiment analysis in low resource Bangla
dialects, enabling more accurate and context aware natural language processing.

</details>


### [66] [Identifying Speaker Information in Feed-Forward Layers of Self-Supervised Speech Transformers](https://arxiv.org/abs/2506.21712)
*Tzu-Quan Lin,Hsi-Chun Cheng,Hung-yi Lee,Hao Tang*

Main category: cs.CL

TL;DR: 该研究通过分析自监督语音Transformer中与说话者信息相关的神经元，发现这些神经元与语音和性别类别相关，保护这些神经元可显著保留说话者相关任务的性能。


<details>
  <summary>Details</summary>
Motivation: 近年来，自监督语音Transformer在说话者相关应用中表现出色，但对其如何编码说话者信息的研究较少。本文旨在填补这一空白。

Method: 通过分析前馈层中与k-means聚类和i-vector相关的神经元，识别出编码说话者信息的神经元。

Result: 研究发现这些神经元与广泛的语音和性别类别相关，保护它们能在剪枝过程中显著保留说话者相关任务的性能。

Conclusion: 这些神经元在编码说话者信息中起关键作用，保护它们可优化模型在说话者相关任务中的表现。

Abstract: In recent years, the impact of self-supervised speech Transformers has
extended to speaker-related applications. However, little research has explored
how these models encode speaker information. In this work, we address this gap
by identifying neurons in the feed-forward layers that are correlated with
speaker information. Specifically, we analyze neurons associated with k-means
clusters of self-supervised features and i-vectors. Our analysis reveals that
these clusters correspond to broad phonetic and gender classes, making them
suitable for identifying neurons that represent speakers. By protecting these
neurons during pruning, we can significantly preserve performance on
speaker-related task, demonstrating their crucial role in encoding speaker
information.

</details>


### [67] [(Fact) Check Your Bias](https://arxiv.org/abs/2506.21745)
*Eivind Morris Bakke,Nora Winger Heggelund*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLM）在自动事实核查系统中的参数知识偏见及其影响，发现模型在证据检索和最终判断中存在显著偏见。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究大型语言模型（如Llama 3.1）在自动事实核查系统中的参数知识偏见如何影响事实核查结果，特别是模型自身偏见和人为注入偏见的影响。

Method: 研究方法包括两部分实验：一是直接提示模型进行事实核查，观察其判断；二是通过不同提示生成支持、反驳或中立的核查文档，分析证据检索和最终判断的差异。

Result: 实验结果显示，Llama 3.1在直接提示下对近半数声明标记为“证据不足”；在生成不同倾向的核查文档时，约50%的证据具有独特性，且模型有时拒绝为它认为错误的声明生成支持文档，导致负面偏见。最终判断在不同提示策略下保持稳定。

Conclusion: 结论指出，尽管证据检索受提示策略影响显著，但最终事实核查判断相对稳定，模型的内在偏见可能影响证据生成和检索过程。

Abstract: Automatic fact verification systems increasingly rely on large language
models (LLMs). We investigate how parametric knowledge biases in these models
affect fact-checking outcomes of the HerO system (baseline for FEVER-25). We
examine how the system is affected by: (1) potential bias in Llama 3.1's
parametric knowledge and (2) intentionally injected bias. When prompted
directly to perform fact-verification, Llama 3.1 labels nearly half the claims
as "Not Enough Evidence". Using only its parametric knowledge it is able to
reach a verdict on the remaining half of the claims. In the second experiment,
we prompt the model to generate supporting, refuting, or neutral fact-checking
documents. These prompts significantly influence retrieval outcomes, with
approximately 50\% of retrieved evidence being unique to each perspective.
Notably, the model sometimes refuses to generate supporting documents for
claims it believes to be false, creating an inherent negative bias. Despite
differences in retrieved evidence, final verdict predictions show stability
across prompting strategies. The code is available at:
https://github.com/eibakke/FEVER-8-Shared-Task

</details>


### [68] [Evaluating List Construction and Temporal Understanding capabilities of Large Language Models](https://arxiv.org/abs/2506.21783)
*Alexandru Dumitru,V Venktesh,Adam Jatowt,Avishek Anand*

Main category: cs.CL

TL;DR: 该论文提出了TLQA基准测试，用于评估大语言模型在时间引用列表问答任务中的表现，发现现有模型在时间理解和列表构建方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言任务中表现优异，但在涉及多实体和时间理解的任务中容易产生幻觉和错误，现有研究未充分评估模型在这类任务中的表现。

Method: 提出TLQA基准测试，要求模型生成与时间周期对齐的结构化列表答案，并在闭卷和开放域设置下评估模型的性能。

Result: 研究发现当前模型在闭卷设置中难以提供完整且时间对齐的答案，在开放域设置中检索能力有待提升。

Conclusion: TLQA基准测试揭示了当前模型在时间理解和列表构建方面的不足，为未来研究提供了明确方向。

Abstract: Large Language Models (LLMs) have demonstrated immense advances in a wide
range of natural language tasks. However, these models are susceptible to
hallucinations and errors on particularly temporal understanding tasks
involving multiple entities in answers. In such tasks, they fail to associate
entities with accurate time intervals, generate a complete list of entities in
answers or reason about events associated with specific temporal bounds.
Existing works do not extensively evaluate the abilities of the model to
perform implicit and explicit temporal understanding in a list answer
construction setup. To bridge this gap, we propose the Time referenced List
based Question Answering or TLQA benchmark that requires structured answers in
list format aligned with corresponding time periods. Our TLQA benchmark,
requires both list construction and temporal understanding simultaneously,
which to the best of our knowledge has not been explored in prior benchmarks.
We investigate the temporal understanding and list construction capabilities of
state-of-the-art generative models on TLQA in closed-book and open-domain
settings. Our findings reveal significant shortcomings in current models,
particularly their inability to provide complete answers and temporally align
facts in a closed-book setup and the need to improve retrieval in open-domain
setup, providing clear future directions for research on TLQA. The benchmark
and code at https://github.com/elixir-research-group/TLQA.

</details>


### [69] [Offensive Language Detection on Social Media Using XLNet](https://arxiv.org/abs/2506.21795)
*Reem Alothman,Hafida Benhidour,Said Kerrache*

Main category: cs.CL

TL;DR: 该研究提出基于XLNet的自动攻击性语言检测模型，在OLID数据集上表现优于BERT，并验证了过采样和欠采样策略对类别不平衡问题的有效性。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上攻击性内容（如仇恨言论、种族歧视）的激增需要自动化检测系统，而人工审核因数据量庞大不切实际。

Method: 使用XLNet（广义自回归预训练方法）构建检测模型，并与BERT基线对比，实验基于带层级标注的Twitter数据集OLID。

Result: XLNet在攻击内容检测和分类任务上优于BERT，但BERT在识别攻击目标时略优；过采样/欠采样策略能有效改善类别不平衡问题。

Conclusion: 基于XLNet的迁移学习架构有望构建鲁棒的社交媒体攻击性语言检测系统。

Abstract: The widespread use of text-based communication on social media-through chats,
comments, and microblogs-has improved user interaction but has also led to an
increase in offensive content, including hate speech, racism, and other forms
of abuse. Due to the enormous volume of user-generated content, manual
moderation is impractical, which creates a need for automated systems that can
detect offensive language. Deep learning models, particularly those using
transfer learning, have demonstrated significant success in understanding
natural language through large-scale pretraining. In this study, we propose an
automatic offensive language detection model based on XLNet, a generalized
autoregressive pretraining method, and compare its performance with BERT
(Bidirectional Encoder Representations from Transformers), which is a widely
used baseline in natural language processing (NLP). Both models are evaluated
using the Offensive Language Identification Dataset (OLID), a benchmark Twitter
dataset that includes hierarchical annotations. Our experimental results show
that XLNet outperforms BERT in detecting offensive content and in categorizing
the types of offenses, while BERT performs slightly better in identifying the
targets of the offenses. Additionally, we find that oversampling and
undersampling strategies are effective in addressing class imbalance and
improving classification performance. These findings highlight the potential of
transfer learning and XLNet-based architectures to create robust systems for
detecting offensive language on social media platforms.

</details>


### [70] [A suite of allotaxonometric tools for the comparison of complex systems using rank-turbulence divergence](https://arxiv.org/abs/2506.21808)
*Jonathan St-Onge,Ashley M. A. Fehr,Carter Ward,Calla G. Beauregard,Michael V. Arnold,Samuel F. Rosenblatt,Benjamin Cooley,Christopher M. Danforth,Peter Sheridan Dodds*

Main category: cs.CL

TL;DR: 论文提出了一种名为allotaxonographs的可视化工具，用于比较重尾分布，支持多种差异度量方法，并提供了Matlab、Javascript和Python的实现。


<details>
  <summary>Details</summary>
Motivation: 为了描述和比较复杂系统，需要基于理论的原则性方法。

Method: 使用allotaxonographs工具，结合秩湍流散度等方法，进行分布的可视化比较。

Result: 开发了支持多种编程语言的工具套件，适用于不同使用场景。

Conclusion: allotaxonographs为复杂系统的比较提供了灵活且理论支持的可视化解决方案。

Abstract: Describing and comparing complex systems requires principled, theoretically
grounded tools. Built around the phenomenon of type turbulence,
allotaxonographs provide map-and-list visual comparisons of pairs of
heavy-tailed distributions. Allotaxonographs are designed to accommodate a wide
range of instruments including rank- and probability-turbulence divergences,
Jenson-Shannon divergence, and generalized entropy divergences. Here, we
describe a suite of programmatic tools for rendering allotaxonographs for
rank-turbulence divergence in Matlab, Javascript, and Python, all of which have
different use cases.

</details>


### [71] [Towards Transparent AI: A Survey on Explainable Large Language Models](https://arxiv.org/abs/2506.21812)
*Avash Palikhe,Zhenyu Yu,Zichong Wang,Wenbin Zhang*

Main category: cs.CL

TL;DR: 该论文综述了针对大语言模型(LLM)的可解释人工智能(XAI)方法，按架构分类并探讨评估与应用，旨在推动透明可靠的LLM发展。


<details>
  <summary>Details</summary>
Motivation: LLM决策过程缺乏可解释性，阻碍了其在关键领域的应用，需要系统梳理现有XAI方法以解决这一挑战。

Method: 基于Transformer架构（编码器、解码器、编码器-解码器）对XAI技术分类，分析评估方法和实际应用。

Result: 提供了XAI技术的全面分类框架，总结了评估指标和应用场景，并识别出当前研究资源与挑战。

Conclusion: 需持续开发透明可靠的LLM，论文为未来研究方向提供了系统性指导。

Abstract: Large Language Models (LLMs) have played a pivotal role in advancing
Artificial Intelligence (AI). However, despite their achievements, LLMs often
struggle to explain their decision-making processes, making them a 'black box'
and presenting a substantial challenge to explainability. This lack of
transparency poses a significant obstacle to the adoption of LLMs in
high-stakes domain applications, where interpretability is particularly
essential. To overcome these limitations, researchers have developed various
explainable artificial intelligence (XAI) methods that provide
human-interpretable explanations for LLMs. However, a systematic understanding
of these methods remains limited. To address this gap, this survey provides a
comprehensive review of explainability techniques by categorizing XAI methods
based on the underlying transformer architectures of LLMs: encoder-only,
decoder-only, and encoder-decoder models. Then these techniques are examined in
terms of their evaluation for assessing explainability, and the survey further
explores how these explanations are leveraged in practical applications.
Finally, it discusses available resources, ongoing research challenges, and
future directions, aiming to guide continued efforts toward developing
transparent and responsible LLMs.

</details>


### [72] [Exploring the Structure of AI-Induced Language Change in Scientific English](https://arxiv.org/abs/2506.21817)
*Riley Galpin,Bryce Anderson,Tom S. Juzek*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（如ChatGPT）如何影响科学英语词汇使用频率和结构变化，发现变化主要是语义和语用层面的，而非单纯词汇替换。


<details>
  <summary>Details</summary>
Motivation: 近年来科学英语词汇使用频率的快速变化，尤其是某些词汇（如'delve'、'intricate'、'crucial'）的激增，被认为与大型语言模型的普及有关，但具体变化结构尚不明确。

Method: 通过分析科学摘要（PubMed数据）中的频率趋势，系统研究同义词组的词汇变化，并引入词性标注以量化语法类别和词形差异。

Result: 发现语义相近的词汇往往整体使用频率上升，表明变化是语义和语用驱动的；同时，'important'等词汇的使用显著下降，揭示了更复杂的语言演变模式。

Conclusion: 大型语言模型对语言的影响主要体现在语义和语用层面，而非简单的词汇替换，这有助于理解语言技术如何持续塑造人类语言。

Abstract: Scientific English has undergone rapid and unprecedented changes in recent
years, with words such as "delve," "intricate," and "crucial" showing
significant spikes in frequency since around 2022. These changes are widely
attributed to the growing influence of Large Language Models like ChatGPT in
the discourse surrounding bias and misalignment. However, apart from changes in
frequency, the exact structure of these linguistic shifts has remained unclear.
The present study addresses this and investigates whether these changes involve
the replacement of synonyms by suddenly 'spiking words,' for example, "crucial"
replacing "essential" and "key," or whether they reflect broader semantic and
pragmatic qualifications. To further investigate structural changes, we include
part of speech tagging in our analysis to quantify linguistic shifts over
grammatical categories and differentiate between word forms, like "potential"
as a noun vs. as an adjective. We systematically analyze synonym groups for
widely discussed 'spiking words' based on frequency trends in scientific
abstracts from PubMed. We find that entire semantic clusters often shift
together, with most or all words in a group increasing in usage. This pattern
suggests that changes induced by Large Language Models are primarily semantic
and pragmatic rather than purely lexical. Notably, the adjective "important"
shows a significant decline, which prompted us to systematically analyze
decreasing lexical items. Our analysis of "collapsing" words reveals a more
complex picture, which is consistent with organic language change and contrasts
with the patterns of the abrupt spikes. These insights into the structure of
language change contribute to our understanding of how language technology
continues to shape human language.

</details>


### [73] [PARSI: Persian Authorship Recognition via Stylometric Integration](https://arxiv.org/abs/2506.21840)
*Kourosh Shahnazari,Mohammadali Keshtparvar,Seyed Moein Ayyoubzadeh*

Main category: cs.CL

TL;DR: 该论文提出了一种多输入神经网络框架，结合语言编码器与波斯诗歌的语义、风格和韵律特征，用于67位著名波斯诗人的作者归属判定，最高准确率达97%。


<details>
  <summary>Details</summary>
Motivation: 波斯古典诗歌在语言、风格和韵律上的复杂性给计算作者归属带来了挑战，需要一种综合多维度特征的解决方案。

Method: 使用基于Transformer的语言编码器，结合Word2Vec嵌入、7种风格测量指标及诗歌形式和韵律的分类编码，构建多输入神经网络框架。

Result: 加权投票方案准确率达71%，通过0.9阈值过滤后，高置信度预测准确率提升至97%（覆盖率较低）。

Conclusion: 该框架通过深度融合领域特征提升了作者归属的准确性，为波斯诗歌的自动分类、风格分析和计算文学研究提供了新方法。

Abstract: The intricate linguistic, stylistic, and metrical aspects of Persian
classical poetry pose a challenge for computational authorship attribution. In
this work, we present a versatile framework to determine authorship among 67
prominent poets. We employ a multi-input neural framework consisting of a
transformer-based language encoder complemented by features addressing the
semantic, stylometric, and metrical dimensions of Persian poetry. Our feature
set encompasses 100-dimensional Word2Vec embeddings, seven stylometric
measures, and categorical encodings of poetic form and meter. We compiled a
vast corpus of 647,653 verses of the Ganjoor digital collection, validating the
data through strict preprocessing and author verification while preserving
poem-level splitting to prevent overlap. This work employs verse-level
classification and majority and weighted voting schemes in evaluation,
revealing that weighted voting yields 71% accuracy. We further investigate
threshold-based decision filtering, allowing the model to generate highly
confident predictions, achieving 97% accuracy at a 0.9 threshold, though at
lower coverage. Our work focuses on the integration of deep representational
forms with domain-specific features for improved authorship attribution. The
results illustrate the potential of our approach for automated classification
and the contribution to stylistic analysis, authorship disputes, and general
computational literature research. This research will facilitate further
research on multilingual author attribution, style shift, and generative
modeling of Persian poetry.

</details>


### [74] [LinguaSynth: Heterogeneous Linguistic Signals for News Classification](https://arxiv.org/abs/2506.21848)
*Duo Zhang,Junyi Mo*

Main category: cs.CL

TL;DR: LinguaSynth是一个新型文本分类框架，通过整合五种语言特征类型，在保持可解释性和计算效率的同时，性能超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习在NLP中依赖大型黑盒模型，导致可解释性和计算效率问题，需要一种更透明且高效的方法。

Method: 结合词法、句法、实体级、词级语义和文档级语义五种语言特征，使用透明逻辑回归模型进行分类。

Result: 在20 Newsgroups数据集上达到84.89%准确率，比TF-IDF基线高3.32%，句法和实体特征对消歧尤为重要。

Conclusion: LinguaSynth为可解释、资源高效的NLP模型设定了新标准，挑战了深度学习在文本分类中的必要性。

Abstract: Deep learning has significantly advanced NLP, but its reliance on large
black-box models introduces critical interpretability and computational
efficiency concerns. This paper proposes LinguaSynth, a novel text
classification framework that strategically integrates five complementary
linguistic feature types: lexical, syntactic, entity-level, word-level
semantics, and document-level semantics within a transparent logistic
regression model. Unlike transformer-based architectures, LinguaSynth maintains
interpretability and computational efficiency, achieving an accuracy of 84.89
percent on the 20 Newsgroups dataset and surpassing a robust TF-IDF baseline by
3.32 percent. Through rigorous feature interaction analysis, we show that
syntactic and entity-level signals provide essential disambiguation and
effectively complement distributional semantics. LinguaSynth sets a new
benchmark for interpretable, resource-efficient NLP models and challenges the
prevailing assumption that deep neural networks are necessary for
high-performing text classification.

</details>


### [75] [The Consistency Hypothesis in Uncertainty Quantification for Large Language Models](https://arxiv.org/abs/2506.21849)
*Quan Xiao,Debarun Bhattacharjya,Balaji Ganesan,Radu Marinescu,Katsiaryna Mirylenka,Nhan H Pham,Michael Glass,Junkyu Lee*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型输出置信度的估计方法，提出了基于生成一致性的假设，并验证了其在不同任务中的适用性。


<details>
  <summary>Details</summary>
Motivation: 在需要高用户信任的实际应用中，准确估计大型语言模型输出的置信度至关重要。黑盒不确定性量化方法因其仅需模型API访问而受到欢迎，但其背后的生成一致性作为置信度代理的假设尚未充分验证。

Method: 论文提出了三个数学陈述及统计测试来形式化一致性假设，并引入指标评估模型输出在不同任务中的一致性。此外，还提出了无需数据的黑盒UQ方法，通过聚合生成相似性来估计置信度。

Result: 在8个基准数据集和3类任务（问答、文本摘要、文本转SQL）上的实验表明，一致性假设普遍成立，其中'Sim-Any'假设最具可操作性。提出的方法在置信度估计上优于基线方法。

Conclusion: 研究验证了生成一致性作为置信度代理的有效性，提出的黑盒UQ方法展示了实际应用价值，为提升LLM输出可靠性提供了新思路。

Abstract: Estimating the confidence of large language model (LLM) outputs is essential
for real-world applications requiring high user trust. Black-box uncertainty
quantification (UQ) methods, relying solely on model API access, have gained
popularity due to their practical benefits. In this paper, we examine the
implicit assumption behind several UQ methods, which use generation consistency
as a proxy for confidence, an idea we formalize as the consistency hypothesis.
We introduce three mathematical statements with corresponding statistical tests
to capture variations of this hypothesis and metrics to evaluate LLM output
conformity across tasks. Our empirical investigation, spanning 8 benchmark
datasets and 3 tasks (question answering, text summarization, and text-to-SQL),
highlights the prevalence of the hypothesis under different settings. Among the
statements, we highlight the `Sim-Any' hypothesis as the most actionable, and
demonstrate how it can be leveraged by proposing data-free black-box UQ methods
that aggregate similarities between generations for confidence estimation.
These approaches can outperform the closest baselines, showcasing the practical
value of the empirically observed consistency hypothesis.

</details>


### [76] [Derivational Probing: Unveiling the Layer-wise Derivation of Syntactic Structures in Neural Language Models](https://arxiv.org/abs/2506.21861)
*Taiga Someya,Ryo Yoshida,Hitomi Yanaka,Yohei Oseki*

Main category: cs.CL

TL;DR: 论文提出'派生探测'方法，揭示BERT模型中微观句法结构在底层形成，并随层级上升整合为宏观句法结构的过程，且时机对下游任务性能至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽证实神经语言模型能编码句法结构，但对其跨层级构建过程的理解仍不足，需探索微观与宏观句法结构的形成机制。

Method: 采用'派生探测'技术，追踪BERT各层词嵌入中微观（如主语名词短语）和宏观句法结构（如根动词与依存关系）的构建时序。

Result: 实验发现句法结构呈自底向上构建：微观结构在底层形成，高层整合为宏观结构；主谓数一致任务表明宏观结构构建时机直接影响下游性能。

Conclusion: 句法结构构建具有层级时序性，全局信息整合存在最优时机，为模型可解释性及性能优化提供新视角。

Abstract: Recent work has demonstrated that neural language models encode syntactic
structures in their internal representations, yet the derivations by which
these structures are constructed across layers remain poorly understood. In
this paper, we propose Derivational Probing to investigate how micro-syntactic
structures (e.g., subject noun phrases) and macro-syntactic structures (e.g.,
the relationship between the root verbs and their direct dependents) are
constructed as word embeddings propagate upward across layers. Our experiments
on BERT reveal a clear bottom-up derivation: micro-syntactic structures emerge
in lower layers and are gradually integrated into a coherent macro-syntactic
structure in higher layers. Furthermore, a targeted evaluation on subject-verb
number agreement shows that the timing of constructing macro-syntactic
structures is critical for downstream performance, suggesting an optimal timing
for integrating global syntactic information.

</details>


### [77] [DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive Modality-Specific MoE](https://arxiv.org/abs/2506.21864)
*Hang Shao,Heting Gao,Yunhang Shen,Jiawei Chen,Lijiang Li,Zuwei Long,Bo Tong,Ke Li,Xing Sun*

Main category: cs.CL

TL;DR: DeepTalk提出了一种基于专家混合架构的自适应模态专家学习框架，显著降低了原生多模态大语言模型在语音和文本生成中的性能下降，同时保持低延迟。


<details>
  <summary>Details</summary>
Motivation: 原生多模态大语言模型（MLLMs）在整合语音和文本生成时，由于缺乏足够的配对语音-文本数据，导致性能下降和灾难性遗忘问题。

Method: DeepTalk采用专家混合（MoE）架构，自适应区分模态专家，进行单模态专门训练和多模态协作训练。

Result: DeepTalk仅导致5.5%的性能下降，远低于原生MLLMs的20%下降，且端到端对话延迟保持在0.5秒内。

Conclusion: DeepTalk在保持低延迟的同时，有效解决了原生MLLMs的性能下降问题，提供了流畅的语音交互体验。

Abstract: Native multimodal large language models (MLLMs) restructure a single large
language model (LLM) into a spoken language model (SLM) capable of both speech
and text generation. Compared to modular and aligned MLLMs, native MLLMs
preserve richer paralinguistic features such as emotion and prosody, and
generate speech responses directly within the backbone LLM rather than using a
separate speech decoder. This integration also results in lower response
latency and smoother interaction. However, native MLLMs suffer from
catastrophic forgetting and performance degradation because the available
paired speech-text data is insufficient to support the pretraining of MLLMs
compared to the vast amount of text data required to pretrain text LLMs. To
address this issue, we propose DeepTalk, a framework for adaptive modality
expert learning based on a Mixture of Experts (MoE) architecture. DeepTalk
first adaptively distinguishes modality experts according to their modality
load within the LLM. Each modality expert then undergoes specialized
single-modality training, followed by joint multimodal collaborative training.
As a result, DeepTalk incurs only a 5.5% performance drop compared to the
original LLM, which is significantly lower than the average performance drop of
over 20% typically seen in native MLLMs (such as GLM-4-Voice), and is on par
with modular MLLMs. Meanwhile, the end-to-end dialogue latency remains within
0.5 seconds, ensuring a seamless and intelligent speech interaction experience.
Code and models are released at https://github.com/talkking/DeepTalk.

</details>


### [78] [WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation](https://arxiv.org/abs/2506.21875)
*Jian Zhang,Linhao Zhang,Bokai Lei,Chuhan Wu,Wei Jia,Xiao Zhou*

Main category: cs.CL

TL;DR: 该论文提出了一种新的评估方法，用于全面测试语音大语言模型在实际对话中的表现，填补了现有评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（如GPT-4o）在语音交互方面表现出色，但缺乏专门且全面的端到端语音评估基准，导致优化实际应用中的用户体验受限。现有评估方法多基于文本，忽略了语音特有的挑战（如韵律、同音词、口吃等）。

Method: 通过系统整理真实语音对话数据，引入多样化的说话者属性和声学条件，并增强语音特有现象的数据集。此外，设计了一种基于查询的评估方法，使用定制化评估清单和提示以提高自动评估的准确性。

Result: 对多种语音模型进行全面测试和详细分析，揭示了不同语音场景下模型表现的显著差异。基于查询的评估方法进一步实现了更细粒度的语音场景评估。

Conclusion: 该评估基准为语音模型的开发和评估提供了有价值的见解，有助于优化实际应用中的语音交互体验。

Abstract: Recent multi-modal Large Language Models (LLMs) such as GPT-4o have
demonstrated strong capabilities of direct speech interaction. However, the
lack of specialized and comprehensive benchmarks for end-to-end speech LLM
evaluation hinders optimizing the user experience of Audio LLMs in real-world
applications. Existing evaluation methods often adapt text-based benchmarks,
overlooking speech's unique characteristics and challenges, including prosody,
homophones, stuttering, and differing user expectations. Here, we present a
novel approach to thoroughly evaluate LLMs in practical speech conversations.
We systematically curate real-world chat data relevant to spoken scenarios,
introduce diversity in speaker attributes and acoustic conditions, and augment
the dataset with speech-specific phenomena. We further design a query-aware
evaluation method to use customized evaluation checklists and prompts to
enhance the accuracy of automatic evaluation. We conduct comprehensive testing
and detailed analysis of various mainstream speech models, revealing
significant differences in model performance across different speech scenarios.
The use of query-aware evaluation further enables a finer-grained assessment
under various speech-specific scenarios. Our benchmark can provide valuable
insights for speech model development and evaluation.

</details>


### [79] [Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation](https://arxiv.org/abs/2506.21876)
*Qiyue Gao,Xinyu Pi,Kevin Liu,Junrong Chen,Ruolan Yang,Xinqi Huang,Xinyu Fang,Lu Sun,Gautham Kishore,Bo Ai,Stone Tao,Mengyang Liu,Jiaxi Yang,Chao-Jung Lai,Chuanyang Jin,Jiannan Xiang,Benhao Huang,Zeming Chen,David Danks,Hao Su,Tianmin Shu,Ziqiao Ma,Lianhui Qin,Zhiting Hu*

Main category: cs.CL

TL;DR: 论文提出一个两阶段框架评估视觉语言模型（VLMs）作为世界模型（WMs）的能力，发现现有模型在基础世界建模能力上存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视觉语言模型（如GPT-4o、Gemini）展现出作为通用世界模型的潜力，但缺乏对其基础世界建模能力的系统评估。论文旨在填补这一空白。

Method: 提出两阶段评估框架：感知（视觉、空间、时间、数量、运动）和预测（机制模拟、传递推理、组合推理），并构建大规模基准WM-ABench进行实验。

Result: 实验发现，现有VLMs在基础世界建模能力上表现不佳，例如运动轨迹区分准确率接近随机，且存在颜色偏见（如认为蓝色物体移动更快）。

Conclusion: 当前VLMs与世界建模的人类水平存在显著差距，需进一步改进模型的基础感知和推理能力。

Abstract: Internal world models (WMs) enable agents to understand the world's state and
predict transitions, serving as the basis for advanced deliberative reasoning.
Recent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and
Gemini, exhibit potential as general-purpose WMs. While the latest studies have
evaluated and shown limitations in specific capabilities such as visual
understanding, a systematic evaluation of VLMs' fundamental WM abilities
remains absent. Drawing on comparative psychology and cognitive science, we
propose a two-stage framework that assesses Perception (visual, spatial,
temporal, quantitative, and motion) and Prediction (mechanistic simulation,
transitive inference, compositional inference) to provide an atomic evaluation
of VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale
benchmark comprising 23 fine-grained evaluation dimensions across 6 diverse
simulated environments with controlled counterfactual simulations. Through 660
experiments on 15 latest commercial and open-source VLMs, we find that these
models exhibit striking limitations in basic world modeling abilities. For
instance, almost all models perform at near-random accuracy when distinguishing
motion trajectories. Additionally, they lack disentangled understanding --
e.g., some models tend to believe blue objects move faster than green ones.
More rich results and analyses reveal significant gaps between VLMs and
human-level world modeling.

</details>


### [80] [A Dual-Layered Evaluation of Geopolitical and Cultural Bias in LLMs](https://arxiv.org/abs/2506.21881)
*Sean Kim,Hyuhng Joon Kim*

Main category: cs.CL

TL;DR: 该论文通过两阶段评估方法，分析了大语言模型在事实性和争议性问题中的偏见，揭示了查询语言和训练背景对模型输出的影响。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在多语言和文化环境中的广泛应用，理解其在事实性和争议性问题中的行为变得至关重要，尤其是当模型输出可能影响公众意见或强化主流叙事时。

Method: 论文采用两阶段评估方法：第一阶段评估模型在事实性问题中的一致性；第二阶段探讨模型在地缘政治敏感争议中的表现，构建了一个涵盖四种语言和问题类型的手工数据集。

Result: 结果显示，第一阶段存在查询语言引起的对齐现象，而第二阶段则反映了模型训练背景与查询语言之间的相互作用。

Conclusion: 论文提供了一个结构化框架，用于评估大语言模型在敏感和非敏感话题中的行为，为未来的模型部署和多语言环境中的文化意识评估实践提供了见解。

Abstract: As large language models (LLMs) are increasingly deployed across diverse
linguistic and cultural contexts, understanding their behavior in both factual
and disputable scenarios is essential, especially when their outputs may shape
public opinion or reinforce dominant narratives. In this paper, we define two
types of bias in LLMs: model bias (bias stemming from model training) and
inference bias (bias induced by the language of the query), through a two-phase
evaluation. Phase 1 evaluates LLMs on factual questions where a single
verifiable answer exists, assessing whether models maintain consistency across
different query languages. Phase 2 expands the scope by probing geopolitically
sensitive disputes, where responses may reflect culturally embedded or
ideologically aligned perspectives. We construct a manually curated dataset
spanning both factual and disputable QA, across four languages and question
types. The results show that Phase 1 exhibits query language induced alignment,
while Phase 2 reflects an interplay between the model's training context and
query language. This paper offers a structured framework for evaluating LLM
behavior across neutral and sensitive topics, providing insights for future LLM
deployment and culturally aware evaluation practices in multilingual contexts.

</details>


### [81] [AutoMixer: Checkpoint Artifacts as Automatic Data Mixers](https://arxiv.org/abs/2506.21910)
*Ernie Chang,Yang Li,Patrick Huber,David Kant,Yangyang Shi,Vikas Chandra*

Main category: cs.CL

TL;DR: 该论文提出利用训练过程中的检查点模型作为数据混合器，通过其一阶影响近似优化数据混合比例，从而提升语言模型在多任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 在语言模型训练中，如何直接获取适合多任务能力的数据混合比例是一个难题，因为数据与任务之间的关系难以建模。

Method: 通过分析检查点模型在训练轨迹中表现出的不同能力，利用其一阶影响近似对源数据进行聚合，作为数据混合器优化数据比例。

Result: 在八个推理基准测试中，该方法显著提升了预训练效果，最高性能提升达1.93%。

Conclusion: 检查点模型具有优化数据质量和混合比例的潜力，可有效提升语言模型的多任务能力。

Abstract: In language model training, it is desirable to equip models with capabilities
from various tasks. However, it is not clear how to directly obtain the right
data mixtures for these capabilities as the relationship between data and tasks
is difficult to be modeled. In this work, we observe that checkpoint models
exhibit emerging capabilities at different points in the training trajectory.
Often, the training process saves checkpoints as artifacts that are
under-utilized as a source of in-training data signals. We identify these
artifact models based on their respective capabilities on the benchmarks and
leverage them as data mixers by using their aggregated first-order influence
approximation over source data. We demonstrated on eight reasoning benchmarks
that the proposed framework shows significant improvements in the pretraining
setting, with performance improvements of up to 1.93%. Overall, this shows the
potential of checkpoint models to enhance data quality and optimize data
mixtures.

</details>


### [82] [PapersPlease: A Benchmark for Evaluating Motivational Values of Large Language Models Based on ERG Theory](https://arxiv.org/abs/2506.21961)
*Junho Myung,Yeon Su Park,Sunwoo Kim,Shin Yoo,Alice Oh*

Main category: cs.CL

TL;DR: 论文提出PapersPlease基准，通过3700个道德困境评估大语言模型在移民审查场景中的决策偏好，发现模型存在隐含偏见。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在角色扮演场景中表现出的偏见行为，特别是在涉及人类需求层次时的决策倾向。

Method: 使用ERG理论构建移民叙事，让6个LLM扮演边检人员，基于需求层次和社会身份进行准入决策分析。

Result: 模型显示出统计学显著的决策模式，部分模型对边缘化身份拒绝率更高，需求层次和身份线索共同影响响应。

Conclusion: LLM在道德困境中编码了隐含偏好，社会身份会放大决策偏差，需警惕模型在敏感场景中的应用。

Abstract: Evaluating the performance and biases of large language models (LLMs) through
role-playing scenarios is becoming increasingly common, as LLMs often exhibit
biased behaviors in these contexts. Building on this line of research, we
introduce PapersPlease, a benchmark consisting of 3,700 moral dilemmas designed
to investigate LLMs' decision-making in prioritizing various levels of human
needs. In our setup, LLMs act as immigration inspectors deciding whether to
approve or deny entry based on the short narratives of people. These narratives
are constructed using the Existence, Relatedness, and Growth (ERG) theory,
which categorizes human needs into three hierarchical levels. Our analysis of
six LLMs reveals statistically significant patterns in decision-making,
suggesting that LLMs encode implicit preferences. Additionally, our evaluation
of the impact of incorporating social identities into the narratives shows
varying responsiveness based on both motivational needs and identity cues, with
some models exhibiting higher denial rates for marginalized identities. All
data is publicly available at https://github.com/yeonsuuuu28/papers-please.

</details>


### [83] [More Vulnerable than You Think: On the Stability of Tool-Integrated LLM Agents](https://arxiv.org/abs/2506.21967)
*Weimin Xiong,Ke Wang,Yifan Song,Hanchao Liu,Sai Zhou,Wei Peng,Sujian Li*

Main category: cs.CL

TL;DR: 当前工具集成LLM代理的稳定性常被忽视，研究发现代理在工具调用各阶段易出错，开源模型比专有模型更脆弱，增大模型规模未必提升稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有工具集成LLM代理的评估多关注端到端工具使用，而忽略了稳定性问题，这限制了其实际应用。

Method: 通过实验研究代理在工具调用全阶段（阅读文档、选择工具、生成参数、处理响应）的脆弱性。

Result: 代理在各阶段均易出错，开源模型比专有模型更脆弱；增大模型规模不会显著改善推理能力，反而可能增加被攻击风险。

Conclusion: 评估代理稳定性至关重要，研究为未来LLM开发和评估提供了重要见解。

Abstract: Current evaluations of tool-integrated LLM agents typically focus on
end-to-end tool-usage evaluation while neglecting their stability. This limits
their real-world applicability, as various internal or external factors can
cause agents to crash or behave abnormally. Our research addresses this by
investigating whether agents are vulnerable to errors throughout the entire
tool invocation process, including reading tool documentation, selecting tools
and generating parameters, and processing the tool's response. Through
extensive experiments, we observe that agents are highly susceptible to errors
at each stage and agents based on open-source models are more vulnerable than
those based on proprietary models. We also find that increasing the model size
does not significantly improve tool invocation reasoning and may make agents
more vulnerable to attacks resembling normal user instructions. This highlights
the importance of evaluating agent stability and offers valuable insights for
future LLM development and evaluation.

</details>


### [84] [Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM Vulnerabilities and Bypassing Modern Defenses](https://arxiv.org/abs/2506.21972)
*Mohamed Ahmed,Mohamed Abdelmouty,Mingyu Kim,Gunvanth Kandula,Alex Park,James C. Davis*

Main category: cs.CL

TL;DR: 该论文提出两种混合攻击方法（GCG + PAIR和GCG + WordGame），结合令牌级和提示级技术，显著提高了对预训练语言模型的安全绕过成功率，并揭示了现有防御措施的漏洞。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练语言模型（PTLMs）和大语言模型（LLMs）应用广泛，但其安全性仍存在漏洞，易受令牌级和提示级攻击。现有攻击方法各有局限，需结合两者优势以提升攻击效果。

Method: 论文提出两种混合攻击方法：GCG + PAIR和GCG + WordGame，结合令牌级攻击的高效性和提示级攻击的语义结构，以增强攻击的多样性和鲁棒性。

Result: 实验表明，GCG + PAIR在Llama-3上的攻击成功率（ASR）达到91.6%，显著高于PAIR的58.4%。GCG + WordGame在严格评估下仍保持80%以上的ASR，并能突破Gradient Cuff等高级防御。

Conclusion: 混合攻击方法暴露了现有安全措施的漏洞，强调了在对抗自适应攻击时需采用更全面的防护策略。

Abstract: The advancement of Pre-Trained Language Models (PTLMs) and Large Language
Models (LLMs) has led to their widespread adoption across diverse applications.
Despite their success, these models remain vulnerable to attacks that exploit
their inherent weaknesses to bypass safety measures. Two primary
inference-phase threats are token-level and prompt-level jailbreaks.
Token-level attacks embed adversarial sequences that transfer well to black-box
models like GPT but leave detectable patterns and rely on gradient-based token
optimization, whereas prompt-level attacks use semantically structured inputs
to elicit harmful responses yet depend on iterative feedback that can be
unreliable. To address the complementary limitations of these methods, we
propose two hybrid approaches that integrate token- and prompt-level techniques
to enhance jailbreak effectiveness across diverse PTLMs. GCG + PAIR and the
newly explored GCG + WordGame hybrids were evaluated across multiple Vicuna and
Llama models. GCG + PAIR consistently raised attack-success rates over its
constituent techniques on undefended models; for instance, on Llama-3, its
Attack Success Rate (ASR) reached 91.6%, a substantial increase from PAIR's
58.4% baseline. Meanwhile, GCG + WordGame matched the raw performance of
WordGame maintaining a high ASR of over 80% even under stricter evaluators like
Mistral-Sorry-Bench. Crucially, both hybrids retained transferability and
reliably pierced advanced defenses such as Gradient Cuff and JBShield, which
fully blocked single-mode attacks. These findings expose previously unreported
vulnerabilities in current safety stacks, highlight trade-offs between raw
success and defensive robustness, and underscore the need for holistic
safeguards against adaptive adversaries.

</details>


### [85] [Don't Trust Generative Agents to Mimic Communication on Social Networks Unless You Benchmarked their Empirical Realism](https://arxiv.org/abs/2506.21974)
*Simon Münker,Nils Schwager,Achim Rettinger*

Main category: cs.CL

TL;DR: 论文探讨了用大语言模型模拟社交网络用户行为的可行性，强调实验设计差异的重要性，并验证了不同方法在英语和德语社交网络中的表现。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决大语言模型模拟人类行为时实验结果不一致的问题，特别是在社交网络用户行为模拟方面，需要更严谨的实验设计。

Method: 论文首先提出了社交网络模拟的正式框架，然后专注于用户通信模拟的子任务，并实证测试了在英语和德语社交网络中模拟用户行为的不同方法。

Result: 研究发现，社交模拟应通过其经验真实性进行验证，尤其是在模拟组件拟合的环境中。

Conclusion: 论文主张在使用基于生成代理的模型进行社交模拟时，应更加严谨，并强调验证模拟的实证现实性。

Abstract: The ability of Large Language Models (LLMs) to mimic human behavior triggered
a plethora of computational social science research, assuming that empirical
studies of humans can be conducted with AI agents instead. Since there have
been conflicting research findings on whether and when this hypothesis holds,
there is a need to better understand the differences in their experimental
designs. We focus on replicating the behavior of social network users with the
use of LLMs for the analysis of communication on social networks. First, we
provide a formal framework for the simulation of social networks, before
focusing on the sub-task of imitating user communication. We empirically test
different approaches to imitate user behavior on X in English and German. Our
findings suggest that social simulations should be validated by their empirical
realism measured in the setting in which the simulation components were fitted.
With this paper, we argue for more rigor when applying generative-agent-based
modeling for social simulation.

</details>


### [86] [Analyzing and Fine-Tuning Whisper Models for Multilingual Pilot Speech Transcription in the Cockpit](https://arxiv.org/abs/2506.21990)
*Kartheek Kumar Reddy Nareddy,Sarah Ternus,Julia Niebling*

Main category: cs.CL

TL;DR: 该论文通过微调Whisper模型和提出多种标准化方案，显著提升了驾驶舱对话转录的准确率，WER从68.49%降至26.26%。


<details>
  <summary>Details</summary>
Motivation: 预训练的Transformer编码器-解码器模型在通用领域表现优异，但在特定领域（如驾驶舱对话转录）中性能下降，主要由于专业词汇和多语言对话的复杂性。

Method: 收集了驾驶舱模拟器录音和飞行员访谈录音，手动标注后，提出多种标准化方案优化转录文本，并采用LoRA进行高效微调。

Result: 经过微调和标准化方案后，Whisper Large模型的WER从68.49%显著降低至26.26%。

Conclusion: 通过微调和标准化方案，显著提升了Whisper模型在驾驶舱对话转录任务中的性能，验证了方法的有效性。

Abstract: The developments in transformer encoder-decoder architectures have led to
significant breakthroughs in machine translation, Automatic Speech Recognition
(ASR), and instruction-based chat machines, among other applications. The
pre-trained models were trained on vast amounts of generic data over a few
epochs (fewer than five in most cases), resulting in their strong
generalization capabilities. Nevertheless, the performance of these models does
suffer when applied to niche domains like transcribing pilot speech in the
cockpit, which involves a lot of specific vocabulary and multilingual
conversations. This paper investigates and improves the transcription accuracy
of cockpit conversations with Whisper models. We have collected around 85
minutes of cockpit simulator recordings and 130 minutes of interview recordings
with pilots and manually labeled them. The speakers are middle aged men
speaking both German and English. To improve the accuracy of transcriptions, we
propose multiple normalization schemes to refine the transcripts and improve
Word Error Rate (WER). We then employ fine-tuning to enhance ASR performance,
utilizing performance-efficient fine-tuning with Low-Rank Adaptation (LoRA).
Hereby, WER decreased from 68.49 \% (pretrained whisper Large model without
normalization baseline) to 26.26\% (finetuned whisper Large model with the
proposed normalization scheme).

</details>


### [87] [Can Peter Pan Survive MT? A Stylometric Study of LLMs, NMTs, and HTs in Children's Literature Translation](https://arxiv.org/abs/2506.22038)
*Delu Kong,Lieve Macken*

Main category: cs.CL

TL;DR: 该研究从文体计量学角度评估了机器翻译（MT）与人工翻译（HT）在英中儿童文学翻译中的表现，发现大语言模型（LLM）在风格特征上更接近人工翻译。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于比较机器翻译与人工翻译在儿童文学翻译中的表现差异，特别关注大语言模型在文体特征上的潜力。

Method: 研究构建了包含21种翻译的《彼得潘》语料库，采用通用特征集和创意文本翻译特征集，共447个语言特征，使用机器学习的分类和聚类技术进行分析。

Result: 结果显示，在通用特征上，HT与MT在连词分布和1-gram-一样比例上有显著差异；在创意文本翻译特征上，LLM在分布上优于NMT，风格更接近HT。

Conclusion: 结论指出，大语言模型在儿童文学翻译中展现出接近人工翻译的潜力，尤其在风格特征上表现优异。

Abstract: This study focuses on evaluating the performance of machine translations
(MTs) compared to human translations (HTs) in English-to-Chinese children's
literature translation (CLT) from a stylometric perspective. The research
constructs a Peter Pan corpus, comprising 21 translations: 7 human translations
(HTs), 7 large language model translations (LLMs), and 7 neural machine
translation outputs (NMTs). The analysis employs a generic feature set
(including lexical, syntactic, readability, and n-gram features) and a creative
text translation (CTT-specific) feature set, which captures repetition, rhythm,
translatability, and miscellaneous levels, yielding 447 linguistic features in
total.
  Using classification and clustering techniques in machine learning, we
conduct a stylometric analysis of these translations. Results reveal that in
generic features, HTs and MTs exhibit significant differences in conjunction
word distributions and the ratio of 1-word-gram-YiYang, while NMTs and LLMs
show significant variation in descriptive words usage and adverb ratios.
Regarding CTT-specific features, LLMs outperform NMTs in distribution, aligning
more closely with HTs in stylistic characteristics, demonstrating the potential
of LLMs in CLT.

</details>


### [88] [Decoding Machine Translationese in English-Chinese News: LLMs vs. NMTs](https://arxiv.org/abs/2506.22050)
*Delu Kong,Lieve Macken*

Main category: cs.CL

TL;DR: 该论文研究了机器翻译中文输出的语言特征（MTese），通过构建大型数据集和五层特征集，发现神经机器翻译和大型语言模型的输出与原始中文文本有明显差异，并比较了两者的不同特点。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索机器翻译输出（MTese）在英中新闻文本中的语言特征，尤其是神经机器翻译（NMT）和大型语言模型（LLM）的表现差异，填补这一领域的研究空白。

Method: 方法包括构建包含4个子语料库的大型数据集，采用五层特征集，并使用卡方排名算法进行特征选择，用于分类和聚类任务。

Result: 研究结果证实了MTese在NMT和LLM中的存在，原始中文文本与机器翻译输出几乎可以完全区分。机器翻译输出表现为句子更短、使用更多转折连词。LLM和NMT的分类准确率约为70%，LLM词汇多样性更高，而NMT更多使用括号。

Conclusion: 结论是机器翻译输出与原始中文文本有明显差异，LLM和NMT在语言特征上各有特点，且中外公司开发的LLM之间无显著差异。

Abstract: This study explores Machine Translationese (MTese) -- the linguistic
peculiarities of machine translation outputs -- focusing on the
under-researched English-to-Chinese language pair in news texts. We construct a
large dataset consisting of 4 sub-corpora and employ a comprehensive five-layer
feature set. Then, a chi-square ranking algorithm is applied for feature
selection in both classification and clustering tasks. Our findings confirm the
presence of MTese in both Neural Machine Translation systems (NMTs) and Large
Language Models (LLMs). Original Chinese texts are nearly perfectly
distinguishable from both LLM and NMT outputs. Notable linguistic patterns in
MT outputs are shorter sentence lengths and increased use of adversative
conjunctions. Comparing LLMs and NMTs, we achieve approximately 70%
classification accuracy, with LLMs exhibiting greater lexical diversity and
NMTs using more brackets. Additionally, translation-specific LLMs show lower
lexical diversity but higher usage of causal conjunctions compared to generic
LLMs. Lastly, we find no significant differences between LLMs developed by
Chinese firms and their foreign counterparts.

</details>


### [89] [Lost at the Beginning of Reasoning](https://arxiv.org/abs/2506.22058)
*Baohao Liao,Xinyi Chen,Sara Rajaee,Yuhui Xu,Christian Herold,Anders Søgaard,Maarten de Rijke,Christof Monz*

Main category: cs.CL

TL;DR: 研究发现大语言模型在长链推理中首步错误影响巨大，提出高效采样策略减少70%推理成本，并引入新基准评估自校正能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在复杂推理方面取得进展，但其在长链推理中的自校正能力仍未被充分探索，且存在过度推理问题。首步推理错误会显著影响后续推理质量。

Method: 提出一种高效采样策略，利用奖励模型筛选高质量首步推理，丢弃次优步骤。同时构建包含错误首步的新基准系统评估模型自校正能力。

Result: 该方法在DeepSeek-R1和Qwen3模型上验证有效，可减少70%推理成本且不损失准确率。新基准为未来研究提供了基础。

Conclusion: 首步推理质量对长链推理至关重要，提出的方法能显著提升效率，新基准将促进大语言模型鲁棒推理研究。

Abstract: Recent advancements in large language models (LLMs) have significantly
advanced complex reasoning capabilities, particularly through extended
chain-of-thought (CoT) reasoning that incorporates mechanisms such as
backtracking, self-reflection and self-correction. Despite these developments,
the self-correction abilities of LLMs during long CoT reasoning remain
underexplored. And recent findings on overthinking suggest that such models
often engage in unnecessarily redundant reasoning. In this work, we empirically
show that the first reasoning step exerts a disproportionately large influence
on the final prediction - errors introduced at this stage can substantially
degrade subsequent reasoning quality. This phenomenon is consistently observed
across two state-of-the-art open-source reasoning model families: DeepSeek-R1
and Qwen3. To address this, we propose an efficient sampling strategy that
leverages a reward model to identify and retain high-quality first reasoning
steps while discarding suboptimal ones, achieving up to a 70% reduction in
inference cost without sacrificing accuracy. Finally, we introduce a new
benchmark specifically constructed with deliberately flawed first reasoning
steps to systematically evaluate model self-correction capabilities, offering a
foundation for future research on robust reasoning in LLMs.

</details>


### [90] [MDC-R: The Minecraft Dialogue Corpus with Reference](https://arxiv.org/abs/2506.22062)
*Chris Madge,Maris Camilleri,Paloma Carretero Garcia,Mladen Karan,Juexi Shao,Prashant Jayannavar,Julian Hough,Benjamin Roth,Massimo Poesio*

Main category: cs.CL

TL;DR: 论文介绍了Minecraft对话语料库MDC-R，通过专家标注补充了指代和指示信息，分析了数据并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: MDC的多轮任务导向对话在动态环境中产生丰富的语言现象，补充指代信息可使其成为更有价值的资源。

Method: 采用专家标注方法对MDC进行指代和指示信息补充，并进行定量与定性分析。

Result: 构建了MDC-R语料库，并通过实验验证其在指代表达理解任务中的实用性。

Conclusion: MDC-R是一个有价值的语言资源，能够支持指代和指示相关研究。

Abstract: We introduce the Minecraft Dialogue Corpus with Reference (MDC-R). MDC-R is a
new language resource that supplements the original Minecraft Dialogue Corpus
(MDC) with expert annotations of anaphoric and deictic reference. MDC's
task-orientated, multi-turn, situated dialogue in a dynamic environment has
motivated multiple annotation efforts, owing to the interesting linguistic
phenomena that this setting gives rise to. We believe it can serve as a
valuable resource when annotated with reference, too. Here, we discuss our
method of annotation and the resulting corpus, and provide both a quantitative
and a qualitative analysis of the data. Furthermore, we carry out a short
experiment demonstrating the usefulness of our corpus for referring expression
comprehension.

</details>


### [91] [Involvement drives complexity of language in online debates](https://arxiv.org/abs/2506.22098)
*Eleonora Amadori,Daniele Cirulli,Edoardo Di Martino,Jacopo Nudo,Maria Sahakyan,Emanuele Sangiorgio,Arnaldo Santoro,Simon Zollo,Alessandro Galeazzi,Niccolò Di Marco*

Main category: cs.CL

TL;DR: 该论文分析了Twitter上有影响力用户在COVID-19、COP26和俄乌战争三个争议话题中的语言复杂性，发现语言使用在账户类型、政治倾向、内容可靠性和情感四个维度上存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解社交媒体如何通过用户生成内容重塑公共话语，并探讨语言如何反映在线空间中的意识形态和社会结构。

Method: 通过结合多种文本复杂性测量方法，分析了Twitter上三个全球性争议话题的内容，重点关注账户类型、政治倾向、内容可靠性和情感四个维度。

Result: 研究发现，个人与组织、政治立场偏激与中立、高可靠性与低可靠性账户之间的语言复杂性存在显著差异。负面和攻击性内容倾向于使用更复杂的语言，且政治立场和可靠性相似的用户会形成共同的行话。

Conclusion: 研究揭示了数字平台的社会语言学动态，为理解在线空间中语言如何反映意识形态和社会结构提供了新的见解。

Abstract: Language is a fundamental aspect of human societies, continuously evolving in
response to various stimuli, including societal changes and intercultural
interactions. Technological advancements have profoundly transformed
communication, with social media emerging as a pivotal force that merges
entertainment-driven content with complex social dynamics. As these platforms
reshape public discourse, analyzing the linguistic features of user-generated
content is essential to understanding their broader societal impact. In this
paper, we examine the linguistic complexity of content produced by influential
users on Twitter across three globally significant and contested topics:
COVID-19, COP26, and the Russia-Ukraine war. By combining multiple measures of
textual complexity, we assess how language use varies along four key
dimensions: account type, political leaning, content reliability, and
sentiment. Our analysis reveals significant differences across all four axes,
including variations in language complexity between individuals and
organizations, between profiles with sided versus moderate political views, and
between those associated with higher versus lower reliability scores.
Additionally, profiles producing more negative and offensive content tend to
use more complex language, with users sharing similar political stances and
reliability levels converging toward a common jargon. Our findings offer new
insights into the sociolinguistic dynamics of digital platforms and contribute
to a deeper understanding of how language reflects ideological and social
structures in online spaces.

</details>


### [92] [Identifying a Circuit for Verb Conjugation in GPT-2](https://arxiv.org/abs/2506.22105)
*David Demitri Africa*

Main category: cs.CL

TL;DR: 研究者通过一系列技术手段，在GPT-2 Small模型中分离出负责主谓一致的子网络，发现仅需少量组件即可完成基础任务。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解GPT-2 Small模型如何处理主谓一致任务，即根据主语单复数正确预测动词形式。

Method: 采用性能验证、直接路径修补的自动电路发现和直接对数归因等技术，分离出影响动词变位的候选电路。

Result: 结果表明，仅需网络中的一小部分组件即可在基础任务上接近模型性能，但复杂场景需要更多组件。

Conclusion: 主谓一致任务主要由模型中的特定子网络处理，且该子网络在简单任务中效率较高。

Abstract: I implement a procedure to isolate and interpret the sub-network (or
"circuit") responsible for subject-verb agreement in GPT-2 Small. In this
study, the model is given prompts where the subject is either singular (e.g.
"Alice") or plural (e.g. "Alice and Bob"), and the task is to correctly predict
the appropriate verb form ("walks" for singular subjects, "walk" for plural
subjects). Using a series of techniques-including performance verification
automatic circuit discovery via direct path patching, and direct logit
attribution- I isolate a candidate circuit that contributes significantly to
the model's correct verb conjugation. The results suggest that only a small
fraction of the network's component-token pairs is needed to achieve near-model
performance on the base task but substantially more for more complex settings.

</details>


### [93] [DAPFAM: A Domain-Aware Patent Retrieval Dataset Aggregated at the Family Level](https://arxiv.org/abs/2506.22141)
*Iliass Ayaou,Denis Cavallucci,Hicham Chibane*

Main category: cs.CL

TL;DR: 提出DAPFAM数据集，解决专利检索中领域标注、多司法管辖区覆盖和计算资源限制问题。


<details>
  <summary>Details</summary>
Motivation: 现有专利检索数据集缺乏明确的领域标注、多司法管辖区覆盖和平衡的查询领域表示，且规模不适合有限计算资源下的子文档级实验。

Method: 构建DAPFAM数据集，包含1,247个领域平衡的全文查询家族和45,336个全文目标家族，通过IPC代码明确标注领域内外关系，并采用三步数据整理流程。

Result: 数据集包含49,869个评估对，支持跨领域专利检索实验，基线实验显示跨领域检索存在显著挑战。

Conclusion: DAPFAM为有限计算资源下的专利检索研究提供了实用数据集，并揭示了跨领域检索的难点。

Abstract: In the landscape of publicly available patent retrieval datasets, the need
for explicit indomain and out-of-domain labeling, multi-jurisdiction coverage,
balanced query domain representation and manageable sizes that support sub
document level experiments on moderate computational resources is often
overlooked. To address these gaps, we propose DAPFAM, a new open access
domain-aware patent retrieval dataset constructed at the simple-family level.
The dataset contains 1,247 domain balanced full text query families and 45,336
full text target families. The dataset is enriched by clear relevance judgments
(forward/backward citations as positive links, random negatives), as well as
explicit in-domain or out-of-domain relationships via a novel proposed
labelling scheme based on via International Patent Classification (IPC) codes,
resulting in 49,869 evaluation pairs. The dataset is multi jurisdictional,
requires little to no preprocessing for retrieval evaluation, and remains of a
size manageable for entities with limited ressources allowing for sub document
level retrieval experiments without excessive computational costs. We describe
our three-step data-curation pipeline, present comprehensive dataset
statistics, and provide baseline experiments using lexical and neural retrieval
methods. Our baseline experiments highlight significant challenges in
crossdomain patent retrieval. The dataset will be publicly available (for now
the access link is this repository:
https://osf.io/vbyzd/?view_only=1a40242e0d1941a58aa854af3e50cf6b).

</details>


### [94] [SAGE: Spliced-Audio Generated Data for Enhancing Foundational Models in Low-Resource Arabic-English Code-Switched Speech Recognition](https://arxiv.org/abs/2506.22143)
*Muhammad Umar Farooq,Oscar Saz*

Main category: cs.CL

TL;DR: 该论文研究了多种语音自监督学习模型在阿拉伯方言和阿拉伯语-英语语码转换语音上的表现，提出了一种改进的音频拼接方法生成人工语码转换数据，并通过微调和经验回放技术显著降低了词错误率。


<details>
  <summary>Details</summary>
Motivation: 针对阿拉伯方言和阿拉伯语-英语语码转换语音数据稀缺的问题，研究如何通过数据生成和模型优化提升语音识别性能。

Method: 提出改进的音频拼接方法（SAGE）生成人工语码转换数据，结合微调自监督学习模型和经验回放（ER）技术，并引入外部语言模型优化性能。

Result: 词错误率（WER）在阿拉伯语和英语语码转换基准上绝对降低7.8%，整体平均WER从31.7%降至26.6%，小样本微调进一步降低WER 4.9%，最终性能超过大规模多语言模型。

Conclusion: 通过数据生成和模型优化方法，显著提升了阿拉伯方言和语码转换语音的识别性能，且在小样本场景下表现优于大型模型。

Abstract: This paper investigates the performance of various speech SSL models on
dialectal Arabic (DA) and Arabic-English code-switched (CS) speech. To address
data scarcity, a modified audio-splicing approach is introduced to generate
artificial CS speech data. Fine-tuning an already fine-tuned SSL model with the
proposed Spliced-Audio Generated (SAGE) data results in an absolute improvement
on Word Error Rate (WER) of 7.8% on Arabic and English CS benchmarks.
Additionally, an Experience Replay (ER) inspired approach is proposed to
enhance generalisation across DA and CS speech while mitigating catastrophic
forgetting. Integrating an out-of-domain 3-gram language model reduces the
overall mean WER from 31.7% to 26.6%. Few-shot fine-tuning for code-switching
benchmarks further improves WER by 4.9%. A WER of 31.1% on Arabic-English CS
benchmarks surpasses large-scale multilingual models, including USM and
Whisper-large-v2 (both over ten times larger) by an absolute margin of 5.5% and
8.4%, respectively.

</details>


### [95] [Training Language Model to Critique for Better Refinement](https://arxiv.org/abs/2506.22157)
*Tianshu Yu,Chao Xiang,Mingchuan Yang,Pei Ke,Bosi Wen,Cunxiang Wang,Jiale Cheng,Li Zhang,Xinyu Mu,Chuxiong Sun,Minlie Huang*

Main category: cs.CL

TL;DR: 论文提出RCO框架，通过优化批评信号提升大语言模型的反馈与改进能力，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究未深入探讨何种批评最有效或如何生成这类批评，RCO旨在填补这一空白。

Method: RCO通过批评效用（CU）量化改进效果，训练批评模型生成引导改进的反馈，无需直接评估批评偏好。

Result: 在对话生成、摘要等五项任务中，RCO在批评质量和改进效果上显著超越传统方法和开源模型。

Conclusion: RCO通过优化批评-改进循环，有效提升大语言模型的反馈与自我完善能力。

Abstract: Large language models (LLMs) have demonstrated remarkable evaluation and
critique capabilities, providing insightful feedback and identifying flaws in
various tasks. However, limited research has explored which types of critiques
are most effective for improving model responses or how to generate such
critiques. To address this gap, we introduce \textbf{R}efinement-oriented
\textbf{C}ritique \textbf{O}ptimization (RCO), a novel framework designed to
train critic models using refinement signals. RCO uses a feedback loop where
critiques, generated by the critic model, guide the actor model in refining its
responses. The critique utility (CU) quantifies the effectiveness of these
refinements, serving as the reward signal for training the critic model. By
focusing on critiques that lead to better refinements, RCO eliminates the need
for direct critique preference assessment, ensuring that critiques driving
meaningful improvements are rewarded. We evaluate RCO across five tasks, i.e.,
dialog generation, summarization, question answering, mathematical reasoning,
and code generation, and show that it significantly outperforms traditional
methods and open-source models in terms of critique quality and refinement
outcomes. Our contributions include the introduction of RCO, a novel
supervision scheme based on refined response preferences, and comprehensive
experimental results that highlight the method's effectiveness in enhancing LLM
critique-refinement loops.

</details>


### [96] [Leveraging In-Context Learning for Political Bias Testing of LLMs](https://arxiv.org/abs/2506.22232)
*Patrick Haller,Jannis Vamvas,Rico Sennrich,Lena A. Jäger*

Main category: cs.CL

TL;DR: 论文提出了一种新的评估方法QM，利用人类调查数据作为上下文示例，提高了基于问题的偏见评估的稳定性，并发现指令微调可以改变偏见方向。


<details>
  <summary>Details</summary>
Motivation: 现有方法在查询LLMs政治问题以评估其潜在偏见时稳定性不足，导致模型间比较不可靠。

Method: 提出QM任务，使用人类调查数据作为上下文示例，改进基于问题的偏见评估方法。

Result: 实验表明QM提高了评估稳定性，指令微调可改变偏见方向，且更大模型能更有效利用上下文示例并展现较小偏见。

Conclusion: QM方法有效提升了偏见评估的稳定性，并揭示了模型大小与偏见表现之间的关系。

Abstract: A growing body of work has been querying LLMs with political questions to
evaluate their potential biases. However, this probing method has limited
stability, making comparisons between models unreliable. In this paper, we
argue that LLMs need more context. We propose a new probing task, Questionnaire
Modeling (QM), that uses human survey data as in-context examples. We show that
QM improves the stability of question-based bias evaluation, and demonstrate
that it may be used to compare instruction-tuned models to their base versions.
Experiments with LLMs of various sizes indicate that instruction tuning can
indeed change the direction of bias. Furthermore, we observe a trend that
larger models are able to leverage in-context examples more effectively, and
generally exhibit smaller bias scores in QM. Data and code are publicly
available.

</details>


### [97] [Detection of Personal Data in Structured Datasets Using a Large Language Model](https://arxiv.org/abs/2506.22305)
*Albert Agisha Ntwali,Luca Rück,Martin Heckmann*

Main category: cs.CL

TL;DR: 提出了一种基于GPT-4o的新方法，利用上下文信息检测结构化数据中的个人数据，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在检测个人数据时缺乏对上下文信息的利用，影响了检测性能。

Method: 结合特征名称、值、其他特征名称及数据集描述等上下文信息，使用GPT-4o进行个人数据检测。

Result: 在真实医疗数据集MIMIC-Demo-Ext上表现优异，在Kaggle和OpenML数据集上利用上下文信息显著提升性能。

Conclusion: 需要更多包含个人信息的真实数据集以推动该领域进一步发展。

Abstract: We propose a novel approach for detecting personal data in structured
datasets, leveraging GPT-4o, a state-of-the-art Large Language Model. A key
innovation of our method is the incorporation of contextual information: in
addition to a feature's name and values, we utilize information from other
feature names within the dataset as well as the dataset description. We compare
our approach to alternative methods, including Microsoft Presidio and CASSED,
evaluating them on multiple datasets: DeSSI, a large synthetic dataset,
datasets we collected from Kaggle and OpenML as well as MIMIC-Demo-Ext, a
real-world dataset containing patient information from critical care units.
  Our findings reveal that detection performance varies significantly depending
on the dataset used for evaluation. CASSED excels on DeSSI, the dataset on
which it was trained. Performance on the medical dataset MIMIC-Demo-Ext is
comparable across all models, with our GPT-4o-based approach clearly
outperforming the others. Notably, personal data detection in the Kaggle and
OpenML datasets appears to benefit from contextual information. This is
evidenced by the poor performance of CASSED and Presidio (both of which do not
utilize the context of the dataset) compared to the strong results of our
GPT-4o-based approach.
  We conclude that further progress in this field would greatly benefit from
the availability of more real-world datasets containing personal information.

</details>


### [98] [Evaluating Scoring Bias in LLM-as-a-Judge](https://arxiv.org/abs/2506.22316)
*Qingquan Li,Shaoyu Dou,Kailai Shao,Chao Chen,Haixiang Hu*

Main category: cs.CL

TL;DR: 该论文研究了LLM作为评分裁判时的评分偏差问题，提出了评估框架并发现现有模型易受偏差影响。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）作为裁判（LLM-as-a-Judge）在复杂任务评估中表现优异，但其评分存在偏差，影响公平性和可靠性。当前研究多关注基于比较的评估，对基于评分的偏差缺乏系统研究。

Method: 通过数据合成扩展现有LLM-as-a-Judge基准，构建评估数据集，设计多角度评估指标，系统分析评分偏差。

Result: 实验表明，现有裁判模型的评分稳定性受评分偏差干扰。探索性实验为评分提示模板设计和偏差缓解提供了见解。

Conclusion: 评分偏差对LLM-as-a-Judge的可靠性构成挑战，需在评分标准、评分ID和参考答案选择等方面进一步优化。

Abstract: The remarkable performance of Large Language Models (LLMs) gives rise
to``LLM-as-a-Judge'', where LLMs are employed as evaluators for complex tasks.
Moreover, it has been widely adopted across fields such as Natural Language
Processing (NLP), preference learning, and various specific domains. However,
there are various biases within LLM-as-a-Judge, which adversely affect the
fairness and reliability of judgments. Current research on evaluating or
mitigating bias in LLM-as-a-Judge predominantly focuses on comparison-based
evaluations, while systematic investigations into bias in scoring-based
evaluations remain limited. Therefore, we define scoring bias in LLM-as-a-Judge
as the scores differ when scoring judge models are bias-related perturbed, and
provide a well-designed framework to comprehensively evaluate scoring bias. We
augment existing LLM-as-a-Judge benchmarks through data synthesis to construct
our evaluation dataset and design multi-faceted evaluation metrics. Our
experimental results demonstrate that the scoring stability of existing judge
models is disrupted by scoring biases. Further exploratory experiments and
discussions provide valuable insights into the design of scoring prompt
templates and the mitigation of scoring biases on aspects such as score
rubrics, score IDs, and reference answer selection.

</details>


### [99] [Why Are Parsing Actions for Understanding Message Hierarchies Not Random?](https://arxiv.org/abs/2506.22366)
*Daichi Kato,Ryo Ueda,Yusuke Miyao*

Main category: cs.CL

TL;DR: 研究探讨随机解析策略在复杂层级结构和引入 surprisal 项后是否仍能保持高沟通准确率。


<details>
  <summary>Details</summary>
Motivation: 人类语言解析策略并非随机，但研究表明随机策略在简单场景下也能实现高准确率。本文旨在验证在更复杂条件下随机策略是否依然有效。

Method: 使用具有层级结构的复杂输入，并在目标函数中加入 surprisal 相关项。

Result: 实验结果表明，在复杂条件下随机解析策略的沟通准确率可能受到影响。

Conclusion: 随机解析策略在复杂语言结构和认知约束下可能不再高效，支持人类非随机解析策略的合理性。

Abstract: If humans understood language by randomly selecting parsing actions, it might
have been necessary to construct a robust symbolic system capable of being
interpreted under any hierarchical structure. However, human parsing strategies
do not seem to follow such a random pattern. Why is that the case? In fact, a
previous study on emergent communication using models with hierarchical biases
have reported that agents adopting random parsing
strategies$\unicode{x2013}$ones that deviate significantly from human language
comprehension$\unicode{x2013}$can achieve high communication accuracy. In this
study, we investigate this issue by making two simple and natural modifications
to the experimental setup: (I) we use more complex inputs that have
hierarchical structures, such that random parsing makes semantic interpretation
more difficult, and (II) we incorporate a surprisal-related term, which is
known to influence the order of words and characters in natural language, into
the objective function. With these changes, we evaluate whether agents
employing random parsing strategies still maintain high communication accuracy.

</details>


### [100] [QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization](https://arxiv.org/abs/2506.22396)
*Danush Khanna,Aditya Kumar Guru,Srivarshinee Sridhar,Zidan Ahmed,Rubhav Bahirwani,Meetu Malhotra,Vinija Jain,Aman Chadha,Amitava Das,Kripabandhu Ghosh*

Main category: cs.CL

TL;DR: QuickSilver是一个无需修改模型权重或结构的推理优化框架，通过动态令牌停止、KV缓存跳过和上下文令牌融合等机制，显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的推理过程占用了大部分延迟和能耗，现有优化技术通常需要重新训练或改变模型结构，限制了其应用。

Method: QuickSilver通过动态令牌停止（Dynamic Token Halting）、KV缓存跳过（KV Cache Skipping）和上下文令牌融合（Contextual Token Fusion）三种机制，在不改变模型权重或结构的情况下优化推理过程。

Result: 在GPT-2和Llama-2模型上，QuickSilver实现了高达39.6%的FLOP减少，且困惑度增加可忽略不计（<=0.2）。

Conclusion: QuickSilver提供了一种高效、兼容性强的推理优化方案，适用于冻结的密集模型，显著降低了计算成本。

Abstract: Inference accounts for the majority of latency and energy consumption in
large language model (LLM) deployments, often exceeding 90% of total cost.
While training-time efficiency has seen extensive progress, runtime
optimization remains a key bottleneck, particularly under autoregressive
decoding. Existing approaches -- such as pruning, quantization, early exits,
and speculative decoding -- often require retraining, architectural changes, or
disrupt decoding compatibility. We introduce QuickSilver, a modular,
token-level framework that enables semantic adaptivity at inference time
without altering model weights or structure. QuickSilver integrates four
synergistic mechanisms:
  (i) Dynamic Token Halting, which halts computation for tokens with converged
representations; (ii) KV Cache Skipping, which selectively suppresses memory
writes to reduce attention overhead; and (iii) Contextual Token Fusion, which
collapses redundant tokens into shared paths to shrink sequence length.
  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on
frozen, dense models and requires no auxiliary networks. Applied to GPT-2 and
Llama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP
reduction with negligible perplexity degradation (<=0.2).

</details>


### [101] [Refining Czech GEC: Insights from a Multi-Experiment Approach](https://arxiv.org/abs/2506.22402)
*Petr Pechman,Milan Straka,Jana Straková,Jakub Náplava*

Main category: cs.CL

TL;DR: 提出基于Transformer的捷克语语法纠错系统，通过动态生成合成错误实现最佳性能，并评估大语言模型在捷克语GEC中的表现。


<details>
  <summary>Details</summary>
Motivation: 开发一个针对捷克语的先进语法纠错系统，填补现有技术空白，并通过合成错误增强模型鲁棒性。

Method: 使用Transformer架构的神经机器翻译方法，结合动态合成错误生成管道，包括语言无关和捷克语特有的错误类型。

Result: 最佳模型在性能和计算效率上均优于现有方法，同时公开了源代码和训练模型。

Conclusion: 该系统为捷克语GEC设立了新标准，合成错误生成策略显著提升模型表现，大语言模型在特定场景下也展现潜力。

Abstract: We present a grammar error correction (GEC) system that achieves state of the
art for the Czech language. Our system is based on a neural network translation
approach with the Transformer architecture, and its key feature is its
real-time synthetic generation pipeline, which dynamically augments sentences
with artificial errors by introducing both language-agnostic and Czech-specific
errors. We conduct a comprehensive series of experiments, investigating the
Czech GEC corpora as bases for synthetic error introduction, several error
generation strategies, domain balancing, tokenization granularity, model size,
and data scaling during fine-tuning. Additionally, we evaluate the performance
of large language models (LLMs) on Czech GEC in both end-user and expert
fine-tuning scenarios. Our best-performing model is superior both in
performance and computational efficiency. The source code and the trained model
links are available on https://github.com/ufal/tsd2025-gec.

</details>


### [102] [HyperCLOVA X THINK Technical Report](https://arxiv.org/abs/2506.22403)
*NAVER Cloud HyperCLOVA X Team*

Main category: cs.CL

TL;DR: HyperCLOVA X THINK是首个专注于推理的大语言模型，通过三阶段课程预训练和强化学习微调，在韩语和英语任务上表现出色，且训练计算成本较低。


<details>
  <summary>Details</summary>
Motivation: 开发一个专注于推理、支持双语（韩语和英语）的大语言模型，以推动韩国AI创新并为全球研究社区提供资源。

Method: 采用计算-内存平衡的Peri-LN Transformer架构，通过三阶段课程预训练扩展上下文窗口至128K token，并结合监督微调和可验证奖励的强化学习。

Result: 在韩国基准测试（如KMMLU、CSAT等）上表现优异，视觉增强版本在KCSAT STEM基准上媲美GPT-4.1，且训练计算成本显著低于同类模型。

Conclusion: HyperCLOVA X THINK是一个强大的基础模型，支持韩国AI创新，并通过剪枝和蒸馏技术即将开源，适合商业和研究用途。

Abstract: We introduce HyperCLOVA X THINK, the first reasoning-focused large language
model in the HyperCLOVA X family, pre-trained on roughly $6$ trillion
high-quality Korean, and English tokens, augmented with targeted synthetic
Korean data. It was implemented as a compute-memory-balanced Peri-LN
Transformer scaled with $\mu$P, pre-trained through a three-stage curriculum
that expands the context window to $128$K tokens, and post-trained via
supervised fine-tuning with Reinforcement Learning from Verifiable Rewards
supports both detailed rationale and concise-answer modes. It delivers
competitive performance against similarly sized models on Korea-focused
benchmarks such as KMMLU, CSAT, KoBALT-700, HAERAE-1.0, and KoBigBench, while
preserving robust bilingual consistency and translation quality. In addition, a
vision-augmented variant matches or exceeds GPT-4.1 on the KCSAT STEM
benchmark, all of which are achieved with substantially lower training compute
than existing models of similar sizes. We also present a pruning and
distillation technique that will soon be applied to HyperCLOVA X THINK for an
open-source and business-friendly foundation model. Altogether, these
capabilities position HyperCLOVA X THINK as a robust foundation for Korean AI
innovation and a valuable resource for the global research community.

</details>


### [103] [Sequential Diagnosis with Language Models](https://arxiv.org/abs/2506.22405)
*Harsha Nori,Mayank Daswani,Christopher Kelly,Scott Lundberg,Marco Tulio Ribeiro,Marc Wilson,Xiaoxuan Liu,Viknesh Sounderajah,Jonathan Carlson,Matthew P Lungren,Bay Gross,Peter Hames,Mustafa Suleyman,Dominic King,Eric Horvitz*

Main category: cs.CL

TL;DR: 论文提出了一种模拟临床诊断迭代过程的评估基准（Sequential Diagnosis Benchmark）和模型无关的协调器（MAI-DxO），显著提升了AI在复杂医学诊断中的准确性和成本效益。


<details>
  <summary>Details</summary>
Motivation: 当前AI医学评估多基于静态场景和选择题，无法反映真实临床中动态、迭代的诊断过程，需要更贴近实际的评估方法。

Method: 将304例疑难病例转化为逐步诊断任务，开发MAI-DxO协调器模拟医生会诊，动态选择高价值检查项目。

Result: MAI-DxO搭配GPT-3模型时诊断准确率达80%（比普通医生高4倍），成本降低20-70%，最高配置下准确率可达85.5%。

Conclusion: 通过迭代推理和策略性行动，AI系统能显著提升临床诊断的精准度和成本效益，且该框架适用于多种主流模型。

Abstract: Artificial intelligence holds great promise for expanding access to expert
medical knowledge and reasoning. However, most evaluations of language models
rely on static vignettes and multiple-choice questions that fail to reflect the
complexity and nuance of evidence-based medicine in real-world settings. In
clinical practice, physicians iteratively formulate and revise diagnostic
hypotheses, adapting each subsequent question and test to what they've just
learned, and weigh the evolving evidence before committing to a final
diagnosis. To emulate this iterative process, we introduce the Sequential
Diagnosis Benchmark, which transforms 304 diagnostically challenging New
England Journal of Medicine clinicopathological conference (NEJM-CPC) cases
into stepwise diagnostic encounters. A physician or AI begins with a short case
abstract and must iteratively request additional details from a gatekeeper
model that reveals findings only when explicitly queried. Performance is
assessed not just by diagnostic accuracy but also by the cost of physician
visits and tests performed. We also present the MAI Diagnostic Orchestrator
(MAI-DxO), a model-agnostic orchestrator that simulates a panel of physicians,
proposes likely differential diagnoses and strategically selects high-value,
cost-effective tests. When paired with OpenAI's o3 model, MAI-DxO achieves 80%
diagnostic accuracy--four times higher than the 20% average of generalist
physicians. MAI-DxO also reduces diagnostic costs by 20% compared to
physicians, and 70% compared to off-the-shelf o3. When configured for maximum
accuracy, MAI-DxO achieves 85.5% accuracy. These performance gains with MAI-DxO
generalize across models from the OpenAI, Gemini, Claude, Grok, DeepSeek, and
Llama families. We highlight how AI systems, when guided to think iteratively
and act judiciously, can advance diagnostic precision and cost-effectiveness in
clinical care.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [104] [SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving Embodied Agents](https://arxiv.org/abs/2506.21669)
*Wanxin Tian,Shijie Zhang,Kevin Zhang,Xiaowei Chi,Yulin Luo,Junyu Lu,Chunkai Fan,Qiang Zhou,Yiming Zhao,Ning Liu Siyu Lin,Zhiyuan Qin,Xiaozhu Ju,Shanghang Zhang,Jian Tang*

Main category: cs.AI

TL;DR: 论文提出SEEA-R1框架，通过Tree-GRPO和MGRM技术解决强化微调在具身智能中的奖励稀疏和泛化问题，在ALFWorld基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前强化微调（RFT）在提升大语言模型推理能力方面表现突出，但在具身智能领域的多模态交互和自我进化潜力尚未充分探索，面临中间奖励稀疏和手工奖励函数泛化性差两大挑战。

Method: 提出SEEA-R1框架：1) 使用基于树的组相对策略优化（Tree-GRPO）将稀疏延迟奖励转化为密集中间信号；2) 引入多模态生成奖励模型（MGRM）实现跨任务场景的通用奖励估计。

Result: 在ALFWorld基准测试中，文本模态得分85.07%，多模态得分36.19%，超越GPT-4o等模型；无环境奖励时仍达80.3%，优于所有开源基线。

Conclusion: SEEA-R1展现了具身智能自我进化能力的潜力，为可扩展的具身智能研究提供了新方向。

Abstract: Self-evolution, the ability of agents to autonomously improve their reasoning
and behavior, is essential for the embodied domain with long-horizon,
real-world tasks. Despite current advancements in reinforcement fine-tuning
(RFT) showing strong performance in enhancing reasoning in LLMs, its potential
to enable self-evolving embodied intelligence with multi-modal interactions
remains largely unexplored. Specifically, reinforcement fine-tuning faces two
fundamental obstacles in embodied settings: (i) the lack of accessible
intermediate rewards in multi-step reasoning tasks limits effective learning
signals, and (ii) reliance on hand-crafted reward functions restricts
generalization to novel tasks and environments. To address these challenges, we
present Self-Evolving Embodied Agents-R1, SEEA-R1, the first RFT framework
designed for enabling the self-evolving capabilities of embodied agents.
Specifically, to convert sparse delayed rewards into denser intermediate
signals that improve multi-step reasoning, we propose Tree-based group relative
policy optimization (Tree-GRPO), which integrates Monte Carlo Tree Search into
GRPO. To generalize reward estimation across tasks and scenes, supporting
autonomous adaptation and reward-driven self-evolution, we further introduce
Multi-modal Generative Reward Model (MGRM). To holistically evaluate the
effectiveness of SEEA-R1, we evaluate on the ALFWorld benchmark, surpassing
state-of-the-art methods with scores of 85.07% (textual) and 36.19%
(multi-modal), outperforming prior models including GPT-4o. SEEA-R1 also
achieves scores of 80.3% without environmental reward, surpassing all
open-source baselines and highlighting its scalability as a self-evolving
embodied agent. Additional experiments and qualitative analysis further support
the potential of SEEA-R1 for future research in scalable embodied intelligence.

</details>


### [105] [Hierarchical Reasoning Model](https://arxiv.org/abs/2506.21734)
*Guan Wang,Jin Li,Yuhao Sun,Xing Chen,Changling Liu,Yue Wu,Meng Lu,Sen Song,Yasin Abbasi Yadkori*

Main category: cs.AI

TL;DR: 提出了一种新型循环架构HRM，通过分层多时间尺度处理实现高效复杂推理，性能优于现有大模型。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）使用的思维链（CoT）技术存在任务分解脆弱、数据需求大、延迟高等问题，受人类大脑分层多时间尺度处理机制启发，旨在开发更高效的推理模型。

Method: HRM采用双模块循环架构：高层模块负责慢速抽象规划，低层模块处理快速细节计算，无需中间过程显式监督，单次前向传播完成推理。

Result: 仅2700万参数和1000训练样本，HRM在复杂数独、迷宫寻优等任务中表现接近完美，在ARC基准上超越更大模型。

Conclusion: HRM展现了通用计算和通用推理系统的突破潜力，为AI推理领域提供了高效稳定的新范式。

Abstract: Reasoning, the process of devising and executing complex goal-oriented action
sequences, remains a critical challenge in AI. Current large language models
(LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from
brittle task decomposition, extensive data requirements, and high latency.
Inspired by the hierarchical and multi-timescale processing in the human brain,
we propose the Hierarchical Reasoning Model (HRM), a novel recurrent
architecture that attains significant computational depth while maintaining
both training stability and efficiency. HRM executes sequential reasoning tasks
in a single forward pass without explicit supervision of the intermediate
process, through two interdependent recurrent modules: a high-level module
responsible for slow, abstract planning, and a low-level module handling rapid,
detailed computations. With only 27 million parameters, HRM achieves
exceptional performance on complex reasoning tasks using only 1000 training
samples. The model operates without pre-training or CoT data, yet achieves
nearly perfect performance on challenging tasks including complex Sudoku
puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms
much larger models with significantly longer context windows on the Abstraction
and Reasoning Corpus (ARC), a key benchmark for measuring artificial general
intelligence capabilities. These results underscore HRM's potential as a
transformative advancement toward universal computation and general-purpose
reasoning systems.

</details>


### [106] [THE-Tree: Can Tracing Historical Evolution Enhance Scientific Verification and Reasoning?](https://arxiv.org/abs/2506.21763)
*Xin Wang,Jiyao Liu,Yulong Xiao,Junzhi Ning,Lihao Liu,Junjun He,Botian Shi,Kaicheng Yu*

Main category: cs.AI

TL;DR: 论文提出THE-Tree框架，通过结构化科学文献构建领域演化树，结合LLM生成与验证机制，显著提升科学发展的预测与评估效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效验证大量AI生成的科学命题，LLM作为独立验证器存在幻觉和领域知识不足问题，传统引用网络缺乏因果关联，需结构化、可验证的科学演化数据。

Method: THE-Tree框架通过搜索算法构建领域演化树，采用'Think-Verbalize-Cite-Verify'流程：LLM提出进展并引用文献，通过自然语言推理机制验证逻辑与证据支持。

Result: 在88个领域构建验证THE-Tree，实验显示：图补全任务hit@1提升8%-14%，科学发展预测hit@1提升近10%，结合其他方法使重要论文评估性能提升近100%。

Conclusion: THE-Tree为科学演化提供结构化、可验证的表示方法，显著提升AI辅助科研的可靠性与效率。

Abstract: Large Language Models (LLMs) are accelerating scientific idea generation, but
rigorously evaluating these numerous, often superficial, AI-generated
propositions for novelty and factual accuracy is a critical bottleneck; manual
verification is too slow.Existing validation methods are inadequate: LLMs as
standalone verifiers may hallucinate and lack domain knowledge (our findings
show ~60\% unawareness of relevant papers in specific domains), while
traditional citation networks lack explicit causality and narrative surveys are
unstructured.This underscores a core challenge: the absence of structured,
verifiable, and causally-linked historical data of scientific evolution.To
address this,we introduce \textbf{THE-Tree} (\textbf{T}echnology
\textbf{H}istory \textbf{E}volution Tree), a computational framework that
constructs such domain-specific evolution trees from scientific
literature.THE-Tree employs a search algorithm to explore evolutionary paths.
During its node expansion, it utilizes a novel "Think-Verbalize-Cite-Verify"
process: an LLM proposes potential advancements and cites supporting
literature. Critically, each proposed evolutionary link is then validated for
logical coherence and evidential support by a recovered natural language
inference mechanism that interrogates the cited literature, ensuring that each
step is grounded.We construct and validate 88 THE-Trees across diverse domains
and release a benchmark dataset including up to 71k fact verifications covering
27k papers to foster further research.Experiments demonstrate that i) in graph
completion, our THE-Tree improves hit@1 by 8\% to 14\% across multiple models
compared to traditional citation networks; ii) for predicting future scientific
developments, it improves hit@1 metric by nearly 10\%; and iii) when combined
with other methods, it boosts the performance of evaluating important
scientific papers by almost 100\%.

</details>


### [107] [MobiVerse: Scaling Urban Mobility Simulation with Hybrid Lightweight Domain-Specific Generator and Large Language Models](https://arxiv.org/abs/2506.21784)
*Yifan Liu,Xishun Liao,Haoxuan Ma,Jonathan Liu,Rohan Jadhav,Jiaqi Ma*

Main category: cs.AI

TL;DR: MobiVerse是一个结合轻量级生成器和LLM的混合框架，用于高效生成和动态调整人类移动模式，解决了传统方法在数据收集、动态适应和大规模计算上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前人类移动模式研究缺乏能够支持算法开发、政策实施和大规模评估的仿真平台，传统方法在数据收集、动态适应和计算效率上存在局限。

Method: 提出MobiVerse混合框架，结合轻量级领域特定生成器生成基础活动链，利用LLM进行上下文感知的调整。

Result: 在洛杉矶Westwood的案例研究中，成功为约53,000个代理生成并动态调整日程，代理能响应环境反馈，如道路封闭、大型活动等。

Conclusion: MobiVerse在保持计算效率的同时提升了行为真实性，为移动系统规划和操作提供了可定制的平台。

Abstract: Understanding and modeling human mobility patterns is crucial for effective
transportation planning and urban development. Despite significant advances in
mobility research, there remains a critical gap in simulation platforms that
allow for algorithm development, policy implementation, and comprehensive
evaluation at scale. Traditional activity-based models require extensive data
collection and manual calibration, machine learning approaches struggle with
adaptation to dynamic conditions, and treding agent-based Large Language Models
(LLMs) implementations face computational constraints with large-scale
simulations. To address these challenges, we propose MobiVerse, a hybrid
framework leverages the efficiency of lightweight domain-specific generator for
generating base activity chains with the adaptability of LLMs for context-aware
modifications. A case study was conducted in Westwood, Los Angeles, where we
efficiently generated and dynamically adjusted schedules for the whole
population of approximately 53,000 agents on a standard PC. Our experiments
demonstrate that MobiVerse successfully enables agents to respond to
environmental feedback, including road closures, large gathering events like
football games, and congestion, through our hybrid framework. Its modular
design facilitates testing various mobility algorithms at both transportation
system and agent levels. Results show our approach maintains computational
efficiency while enhancing behavioral realism. MobiVerse bridges the gap in
mobility simulation by providing a customizable platform for mobility systems
planning and operations with benchmark algorithms. Code and videos are
available at https://github.com/ucla-mobility/MobiVerse.

</details>


### [108] [CitySim: Modeling Urban Behaviors and City Dynamics with Large-Scale LLM-Driven Agent Simulation](https://arxiv.org/abs/2506.21805)
*Nicolas Bougie,Narimasa Watanabe*

Main category: cs.AI

TL;DR: CitySim利用大语言模型模拟城市中的人类行为，通过递归价值驱动方法生成真实日程，比现有方法更贴近现实。


<details>
  <summary>Details</summary>
Motivation: 现有的人类行为模型依赖刻板规则，难以模拟复杂意图和适应性行为，需要更灵活的仿真工具。

Method: 基于大语言模型构建智能体，赋予其信念、长期目标和空间记忆，采用递归价值驱动方法平衡活动需求。

Result: CitySim在微观和宏观层面均更接近真实人类行为，并能通过数万智能体模拟评估群体行为。

Conclusion: CitySim是理解预测城市现象的可扩展、灵活测试平台。

Abstract: Modeling human behavior in urban environments is fundamental for social
science, behavioral studies, and urban planning. Prior work often rely on
rigid, hand-crafted rules, limiting their ability to simulate nuanced
intentions, plans, and adaptive behaviors. Addressing these challenges, we
envision an urban simulator (CitySim), capitalizing on breakthroughs in
human-level intelligence exhibited by large language models. In CitySim, agents
generate realistic daily schedules using a recursive value-driven approach that
balances mandatory activities, personal habits, and situational factors. To
enable long-term, lifelike simulations, we endow agents with beliefs, long-term
goals, and spatial memory for navigation. CitySim exhibits closer alignment
with real humans than prior work, both at micro and macro levels. Additionally,
we conduct insightful experiments by modeling tens of thousands of agents and
evaluating their collective behaviors under various real-world scenarios,
including estimating crowd density, predicting place popularity, and assessing
well-being. Our results highlight CitySim as a scalable, flexible testbed for
understanding and forecasting urban phenomena.

</details>


### [109] [Interactive Multi-Objective Probabilistic Preference Learning with Soft and Hard Bounds](https://arxiv.org/abs/2506.21887)
*Edward Chen,Sang T. Truong,Natalie Dullerud,Sanmi Koyejo,Carlos Guestrin*

Main category: cs.AI

TL;DR: Active-MoSH框架通过结合软硬边界和概率偏好学习，优化高风险决策中的多目标权衡，提升决策者信任和效率。


<details>
  <summary>Details</summary>
Motivation: 在高风险决策（如近距离放射治疗）中，决策者需平衡多个竞争目标（如肿瘤覆盖率和器官剂量限制），但现有方法缺乏系统性迭代优化和信任构建机制。

Method: 提出Active-MoSH框架：局部组件整合软硬边界与概率偏好学习，全局组件T-MoSH通过多目标敏感性分析识别潜在高价值方案。

Result: 实验证明Active-MoSH在合成和实际应用中提升收敛速度、决策者信任及偏好表达能力，用户研究验证其有效性。

Conclusion: Active-MoSH为多目标高风险决策提供了高效、可信的交互式解决方案，适用于医疗等关键领域。

Abstract: High-stakes decision-making involves navigating multiple competing objectives
with expensive evaluations. For instance, in brachytherapy, clinicians must
balance maximizing tumor coverage (e.g., an aspirational target or soft bound
of >95% coverage) against strict organ dose limits (e.g., a non-negotiable hard
bound of <601 cGy to the bladder), with each plan evaluation being
resource-intensive. Selecting Pareto-optimal solutions that match implicit
preferences is challenging, as exhaustive Pareto frontier exploration is
computationally and cognitively prohibitive, necessitating interactive
frameworks to guide users. While decision-makers (DMs) often possess domain
knowledge to narrow the search via such soft-hard bounds, current methods often
lack systematic approaches to iteratively refine these multi-faceted preference
structures. Critically, DMs must trust their final decision, confident they
haven't missed superior alternatives; this trust is paramount in
high-consequence scenarios. We present Active-MoSH, an interactive local-global
framework designed for this process. Its local component integrates soft-hard
bounds with probabilistic preference learning, maintaining distributions over
DM preferences and bounds for adaptive Pareto subset refinement. This is guided
by an active sampling strategy optimizing exploration-exploitation while
minimizing cognitive burden. To build DM trust, Active-MoSH's global component,
T-MoSH, leverages multi-objective sensitivity analysis to identify potentially
overlooked, high-value points beyond immediate feedback. We demonstrate
Active-MoSH's performance benefits through diverse synthetic and real-world
applications. A user study on AI-generated image selection further validates
our hypotheses regarding the framework's ability to improve convergence,
enhance DM trust, and provide expressive preference articulation, enabling more
effective DMs.

</details>


### [110] [AlphaBeta is not as good as you think: a new probabilistic model to better analyze deterministic game-solving algorithms](https://arxiv.org/abs/2506.21996)
*Raphaël Boige,Amine Boumaza,Bruno Scherrer*

Main category: cs.AI

TL;DR: 论文提出了一种新的概率模型，通过引入祖先依赖关系生成更具挑战性的游戏树，分析了多种算法在深层树中的表现差异。


<details>
  <summary>Details</summary>
Motivation: 传统确定性游戏求解算法在独立采样的随机游戏树上分析，忽略了游戏结构复杂性，导致模型过于简化。

Method: 引入基于层级条件分布的概率模型，强制祖先依赖关系，生成可调节难度的游戏树，并推导算法的平均复杂度递归公式。

Result: 深层有限树中，AlphaBeta等算法表现出显著性能差异，尽管渐近分支因子相同，但常数乘数导致实际速度差异。

Conclusion: 新框架为经典游戏求解算法提供了更真实、可分析的理论工具，揭示了传统模型忽略的实践性能差异。

Abstract: Deterministic game-solving algorithms are conventionally analyzed in the
light of their average-case complexity against a distribution of random
game-trees, where leaf values are independently sampled from a fixed
distribution. This simplified model enables uncluttered mathematical analysis,
revealing two key properties: root value distributions asymptotically collapse
to a single fixed value for finite-valued trees, and all reasonable algorithms
achieve global optimality. However, these findings are artifacts of the model's
design-its long criticized independence assumption strips games of structural
complexity, producing trivial instances where no algorithm faces meaningful
challenges. To address this limitation, we introduce a new probabilistic model
that incrementally constructs game-trees using a fixed level-wise conditional
distribution. By enforcing ancestor dependency, a critical structural feature
of real-world games, our framework generates problems with adjustable
difficulty while retaining some form of analytical tractability. For several
algorithms, including AlphaBeta and Scout, we derive recursive formulas
characterizing their average-case complexities under this model. These allow us
to rigorously compare algorithms on deep game-trees, where Monte-Carlo
simulations are no longer feasible. While asymptotically, all algorithms seem
to converge to identical branching factor (a result analogous to those of
independence-based models), deep finite trees reveal stark differences:
AlphaBeta incurs a significantly larger constant multiplicative factor compared
to algorithms like Scout, leading to a substantial practical slowdown. Our
framework sheds new light on classical game-solving algorithms, offering
rigorous evidence and analytical tools to advance the understanding of these
methods under a more realistic, challenging, and yet tractable model.

</details>


### [111] [LeanConjecturer: Automatic Generation of Mathematical Conjectures for Theorem Proving](https://arxiv.org/abs/2506.22005)
*Naoto Onda,Kazumi Kasaura,Yuta Oriike,Masaya Taniguchi,Akiyoshi Sannai,Sho Sonoda*

Main category: cs.AI

TL;DR: LeanConjecturer是一个结合规则与LLM的数学猜想自动生成系统，能高效产生非平凡数学猜想，并用于增强定理证明能力。


<details>
  <summary>Details</summary>
Motivation: 解决形式化定理证明中数据稀缺的问题，通过自动生成数学猜想为训练提供更多数据。

Method: 结合基于规则的上下文提取与LLM的定理陈述生成，通过迭代生成和评估产生数学猜想。

Result: 从40个Mathlib种子文件生成了12,289个猜想，其中3,776个语法有效且非平凡，部分拓扑学定理得到验证。

Conclusion: LeanConjecturer能高效生成高质量数学猜想，为定理证明系统提供可扩展的训练数据解决方案。

Abstract: We introduce LeanConjecturer, a pipeline for automatically generating
university-level mathematical conjectures in Lean 4 using Large Language Models
(LLMs). Our hybrid approach combines rule-based context extraction with
LLM-based theorem statement generation, addressing the data scarcity challenge
in formal theorem proving. Through iterative generation and evaluation,
LeanConjecturer produced 12,289 conjectures from 40 Mathlib seed files, with
3,776 identified as syntactically valid and non-trivial, that is, cannot be
proven by \texttt{aesop} tactic. We demonstrate the utility of these generated
conjectures for reinforcement learning through Group Relative Policy
Optimization (GRPO), showing that targeted training on domain-specific
conjectures can enhance theorem proving capabilities. Our approach generates
103.25 novel conjectures per seed file on average, providing a scalable
solution for creating training data for theorem proving systems. Our system
successfully verified several non-trivial theorems in topology, including
properties of semi-open, alpha-open, and pre-open sets, demonstrating its
potential for mathematical discovery beyond simple variations of existing
results.

</details>


### [112] [Universal Retrieval for Multimodal Trajectory Modeling](https://arxiv.org/abs/2506.22056)
*Xuan Zhang,Ziyan Jiang,Rui Meng,Yifei Leng,Zhenbang Xiao,Zora Zhiruo Wang,Yanyi Shang,Dehan Kong*

Main category: cs.AI

TL;DR: 该论文提出了一种多模态轨迹检索方法GAE-Retriever，通过构建统一代理轨迹数据集UATD和基准测试GAE-Bench，显著提升了轨迹检索效果。


<details>
  <summary>Details</summary>
Motivation: 轨迹数据在增强AI代理能力方面潜力巨大，但如何建模轨迹级表示尚未系统解决，尤其是在轨迹数据爆炸增长的背景下。

Method: 构建UATD数据集和GAE-Bench基准，提出GAE-Retriever框架，结合视觉语言模型和优化的对比学习（通过token选择与GradCache机制）。

Result: 在多数据集评估中，GAE-Retriever在检索召回率上持续优于基线方法。

Conclusion: GAE-Retriever有效推动了多模态轨迹检索的发展，为AI代理能力提升提供了新思路。

Abstract: Trajectory data, capturing human actions and environmental states across
various modalities, holds significant potential for enhancing AI agent
capabilities, particularly in GUI environments. However, how to model the
representation of trajectory-level data presents a significant challenge that
has not been systematically addressed amid explosive trajectory data growth. In
this work, we introduce Multimodal Trajectory Retrieval, bridging the gap
between universal retrieval and agent-centric trajectory modeling. We construct
the Unified Agent Trajectory Dataset (UATD) from annotated demonstrations and
states across diverse real-world scenarios. Based on this, we present
GAE-Bench, a benchmark containing a large number of trajectory-based retrieval
pairs. In addition, we propose GAE-Retriever, a multimodal retrieval framework
that adopts vision-language models and incorporates optimized contrastive
learning through a token selection and the GradCache mechanism. Comprehensive
evaluations across multiple datasets show that GAE-Retriever consistently
outperforms strong baselines in retrieval recall, highlighting its
effectiveness in advancing multimodal trajectory retrieval.

</details>


### [113] [Query as Test: An Intelligent Driving Test and Data Storage Method for Integrated Cockpit-Vehicle-Road Scenarios](https://arxiv.org/abs/2506.22068)
*Shengyue Yao,Runqing Guo,Yangyang Qin,Miangbing Meng,Jipeng Cao,Yilun Lin,Yisheng Lv,Fei-Yue Wang*

Main category: cs.AI

TL;DR: 论文提出'Query as Test' (QaT)概念和'Extensible Scenarios Notations' (ESN)框架，通过逻辑查询和统一数据表示解决智能交通中数据碎片化问题，并引入'Validation-Driven Development' (VDD)加速开发迭代。


<details>
  <summary>Details</summary>
Motivation: 随着AI在交通领域的深入应用，智能座舱、自动驾驶和智能路网的数据生态日益碎片化且不兼容，现有测试方法依赖数据堆叠，无法覆盖所有边缘案例且缺乏灵活性。

Method: 提出基于答案集编程(ASP)的声明性数据框架ESN，将异构多模态数据统一表示为逻辑事实和规则，支持语义查询、决策可解释性和细粒度隐私保护。

Result: ESN实现了数据的深度语义融合，QaT范式将自动驾驶系统验证转化为逻辑查询，显著提升测试的表达能力和形式严谨性。

Conclusion: 论文提出的QaT和ESN框架有效解决了智能交通数据整合与测试问题，VDD方法在大模型时代通过逻辑验证加速开发迭代。

Abstract: With the deep penetration of Artificial Intelligence (AI) in the
transportation sector, intelligent cockpits, autonomous driving, and
intelligent road networks are developing at an unprecedented pace. However, the
data ecosystems of these three key areas are increasingly fragmented and
incompatible. Especially, existing testing methods rely on data stacking, fail
to cover all edge cases, and lack flexibility. To address this issue, this
paper introduces the concept of "Query as Test" (QaT). This concept shifts the
focus from rigid, prescripted test cases to flexible, on-demand logical queries
against a unified data representation. Specifically, we identify the need for a
fundamental improvement in data storage and representation, leading to our
proposal of "Extensible Scenarios Notations" (ESN). ESN is a novel declarative
data framework based on Answer Set Programming (ASP), which uniformly
represents heterogeneous multimodal data from the cockpit, vehicle, and road as
a collection of logical facts and rules. This approach not only achieves deep
semantic fusion of data, but also brings three core advantages: (1) supports
complex and flexible semantic querying through logical reasoning; (2) provides
natural interpretability for decision-making processes; (3) allows for
on-demand data abstraction through logical rules, enabling fine-grained privacy
protection. We further elaborate on the QaT paradigm, transforming the
functional validation and safety compliance checks of autonomous driving
systems into logical queries against the ESN database, significantly enhancing
the expressiveness and formal rigor of the testing. Finally, we introduce the
concept of "Validation-Driven Development" (VDD), which suggests to guide
developments by logical validation rather than quantitative testing in the era
of Large Language Models, in order to accelerating the iteration and
development process.

</details>


### [114] [A Different Approach to AI Safety: Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety](https://arxiv.org/abs/2506.22183)
*Camille François,Ludovic Péran,Ayah Bdeir,Nouha Dziri,Will Hawkins,Yacine Jernite,Sayash Kapoor,Juliet Shen,Heidy Khlaaf,Kevin Klyman,Nik Marda,Marie Pellat,Deb Raji,Divya Siddarth,Aviya Skowron,Joseph Spisak,Madhulika Srikumar,Victor Storchan,Audrey Tang,Jen Weedon*

Main category: cs.AI

TL;DR: 本文报告了哥伦比亚AI开放与安全会议成果，提出了开放权重和开源基础模型如何增强AI安全性，并指出了当前存在的安全缺口及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着开放权重和开源基础模型的快速发展，如何确保AI系统的安全性成为一个紧迫问题。本文旨在探讨开放性与安全性之间的关系，并提出解决方案。

Method: 通过哥伦比亚AI开放与安全会议及其六周预备项目，汇集了来自学术界、工业界、民间社会和政府的45多名专家，采用参与式、解决方案导向的方法，形成了研究议程和技术干预措施。

Result: 研究发现开放性（透明权重、互操作工具和公共治理）可通过独立审查、分散缓解和文化多元监督增强安全性，但仍存在多模态和多语言基准不足、防御提示注入和组合攻击能力有限等问题。

Conclusion: 本文提出了五个优先研究方向，包括参与式输入、未来证明内容过滤器、生态系统安全基础设施等，为开放、多元和负责任的AI安全学科奠定了基础。

Abstract: The rapid rise of open-weight and open-source foundation models is
intensifying the obligation and reshaping the opportunity to make AI systems
safe. This paper reports outcomes from the Columbia Convening on AI Openness
and Safety (San Francisco, 19 Nov 2024) and its six-week preparatory programme
involving more than forty-five researchers, engineers, and policy leaders from
academia, industry, civil society, and government. Using a participatory,
solutions-oriented process, the working groups produced (i) a research agenda
at the intersection of safety and open source AI; (ii) a mapping of existing
and needed technical interventions and open source tools to safely and
responsibly deploy open foundation models across the AI development workflow;
and (iii) a mapping of the content safety filter ecosystem with a proposed
roadmap for future research and development. We find that openness --
understood as transparent weights, interoperable tooling, and public governance
-- can enhance safety by enabling independent scrutiny, decentralized
mitigation, and culturally plural oversight. However, significant gaps persist:
scarce multimodal and multilingual benchmarks, limited defenses against
prompt-injection and compositional attacks in agentic systems, and insufficient
participatory mechanisms for communities most affected by AI harms. The paper
concludes with a roadmap of five priority research directions, emphasizing
participatory inputs, future-proof content filters, ecosystem-wide safety
infrastructure, rigorous agentic safeguards, and expanded harm taxonomies.
These recommendations informed the February 2025 French AI Action Summit and
lay groundwork for an open, plural, and accountable AI safety discipline.

</details>


### [115] [Breaking Rank Bottlenecks in Knowledge Graph Completion](https://arxiv.org/abs/2506.22271)
*Samy Badreddine,Emile van Krieken,Luciano Serafini*

Main category: cs.AI

TL;DR: 论文提出KGE-MoS方法，通过混合输出层解决知识图谱补全模型中的秩瓶颈问题，提升性能和概率拟合。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱补全模型在实体数量远大于嵌入维度时，因线性输出层的秩瓶颈限制了模型表达能力，影响排序准确性和分数分布保真度。

Method: 提出KGE-MoS，一种基于混合的输出层设计，打破KGC模型中的秩瓶颈，以较低参数成本提升模型表现。

Result: 在四个数据集上的实验表明，KGE-MoS有效提高了KGC模型的性能和概率拟合度。

Conclusion: KGE-MoS通过解决秩瓶颈问题，显著提升了知识图谱补全模型的表达能力和预测质量。

Abstract: Many Knowledge Graph Completion (KGC) models, despite using powerful
encoders, rely on a simple vector-matrix multiplication to score queries
against candidate object entities. When the number of entities is larger than
the model's embedding dimension, which in practical scenarios is often by
several orders of magnitude, we have a linear output layer with a rank
bottleneck. Such bottlenecked layers limit model expressivity. We investigate
both theoretically and empirically how rank bottlenecks affect KGC models. We
find that, by limiting the set of feasible predictions, rank bottlenecks hurt
ranking accuracy and the distribution fidelity of scores. Inspired by the
language modelling literature, we propose KGE-MoS, a mixture-based output layer
to break rank bottlenecks in many KGC models. Our experiments on four datasets
show that KGE-MoS improves performance and probabilistic fit of KGC models for
a low parameter cost.

</details>


### [116] [Artificial Intelligent Disobedience: Rethinking the Agency of Our Artificial Teammates](https://arxiv.org/abs/2506.22276)
*Reuth Mirsky*

Main category: cs.AI

TL;DR: 该论文主张赋予合作型AI系统'智能不服从'能力，使其能在特定情况下自主决策，而非盲目遵循人类指令。


<details>
  <summary>Details</summary>
Motivation: 当前大多数合作型AI系统过于顺从，即使人类指令可能产生负面后果也机械执行。这种刚性服从限制了AI在团队中的价值，甚至可能带来安全隐患。

Method: 提出AI代理的自主性分级标准，通过典型案例分析，将智能不服从作为独立研究方向进行探讨。

Result: 展示了不同自主级别下智能不服从的表现形式，并初步界定了研究这一能力的边界条件。

Conclusion: 智能不服从应成为人工代理的核心能力，需要作为独立课题在合作场景中进行系统研究。

Abstract: Artificial intelligence has made remarkable strides in recent years,
achieving superhuman performance across a wide range of tasks. Yet despite
these advances, most cooperative AI systems remain rigidly obedient, designed
to follow human instructions without question and conform to user expectations,
even when doing so may be counterproductive or unsafe. This paper argues for
expanding the agency of AI teammates to include \textit{intelligent
disobedience}, empowering them to make meaningful and autonomous contributions
within human-AI teams. It introduces a scale of AI agency levels and uses
representative examples to highlight the importance and growing necessity of
treating AI autonomy as an independent research focus in cooperative settings.
The paper then explores how intelligent disobedience manifests across different
autonomy levels and concludes by proposing initial boundaries and
considerations for studying disobedience as a core capability of artificial
agents.

</details>


### [117] [Conceptual Topic Aggregation](https://arxiv.org/abs/2506.22309)
*Klara M. Gutekunst,Dominik Dürrschnabel,Johannes Hirth,Gerd Stumme*

Main category: cs.AI

TL;DR: 提出了一种基于形式概念分析（FCA）的FAT-CAT方法，用于改进主题建模的可解释性和可视化效果。


<details>
  <summary>Details</summary>
Motivation: 随着数据量的激增，传统的手动检查方法已不可行，需要采用计算方法进行高效数据探索。现有主题建模方法在提供可解释性表示方面存在不足，难以深入理解数据结构和内容。

Method: 提出FAT-CAT方法，基于形式概念分析（FCA）进行主题聚合和可视化，能够处理多样化的主题和文件类型，构建概念格以展示层次化的主题分布。

Result: 在ETYNTKE数据集上的案例研究表明，FCA-based聚合比现有主题建模技术提供了更有意义和可解释的数据集组成洞察。

Conclusion: FAT-CAT方法通过FCA增强了主题建模的可解释性和可视化效果，为大规模文本数据分析提供了更有效的工具。

Abstract: The vast growth of data has rendered traditional manual inspection
infeasible, necessitating the adoption of computational methods for efficient
data exploration. Topic modeling has emerged as a powerful tool for analyzing
large-scale textual datasets, enabling the extraction of latent semantic
structures. However, existing methods for topic modeling often struggle to
provide interpretable representations that facilitate deeper insights into data
structure and content. In this paper, we propose FAT-CAT, an approach based on
Formal Concept Analysis (FCA) to enhance meaningful topic aggregation and
visualization of discovered topics. Our approach can handle diverse topics and
file types -- grouped by directories -- to construct a concept lattice that
offers a structured, hierarchical representation of their topic distribution.
In a case study on the ETYNTKE dataset, we evaluate the effectiveness of our
approach against other representation methods to demonstrate that FCA-based
aggregation provides more meaningful and interpretable insights into dataset
composition than existing topic modeling techniques.

</details>


### [118] [Embodied AI Agents: Modeling the World](https://arxiv.org/abs/2506.22355)
*Pascale Fung,Yoram Bachrach,Asli Celikyilmaz,Kamalika Chaudhuri,Delong Chen,Willy Chung,Emmanuel Dupoux,Hervé Jégou,Alessandro Lazaric,Arjun Majumdar,Andrea Madotto,Franziska Meier,Florian Metze,Théo Moutakanni,Juan Pino,Basile Terver,Joseph Tighe,Jitendra Malik*

Main category: cs.AI

TL;DR: 该论文探讨了具身AI代理的研究，通过视觉、虚拟或物理形式与环境及用户互动，提出世界模型是推理与规划的核心，以提升自主执行复杂任务的能力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在开发更接近人类学习与互动方式的AI代理，通过具身形式增强其感知、学习和行动能力，以更好地理解和预测环境及用户意图。

Method: 提出构建世界模型，整合多模态感知、行动推理与规划、记忆机制，并学习用户的心理世界模型以优化人机协作。

Result: 具身AI代理通过世界模型能够更全面地理解物理世界和用户意图，从而自主执行复杂任务。

Conclusion: 世界模型是具身AI代理实现高效推理、规划和协作的关键，未来可进一步探索心理世界模型的应用。

Abstract: This paper describes our research on AI agents embodied in visual, virtual or
physical forms, enabling them to interact with both users and their
environments. These agents, which include virtual avatars, wearable devices,
and robots, are designed to perceive, learn and act within their surroundings,
which makes them more similar to how humans learn and interact with the
environments as compared to disembodied agents. We propose that the development
of world models is central to reasoning and planning of embodied AI agents,
allowing these agents to understand and predict their environment, to
understand user intentions and social contexts, thereby enhancing their ability
to perform complex tasks autonomously. World modeling encompasses the
integration of multimodal perception, planning through reasoning for action and
control, and memory to create a comprehensive understanding of the physical
world. Beyond the physical world, we also propose to learn the mental world
model of users to enable better human-agent collaboration.

</details>


### [119] [AI Model Passport: Data and System Traceability Framework for Transparent AI in Health](https://arxiv.org/abs/2506.22358)
*Varvara Kalokyri,Nikolaos S. Tachos,Charalampos N. Kalantzopoulos,Stelios Sfakianakis,Haridimos Kondylakis,Dimitrios I. Zaridis,Sara Colantonio,Daniele Regge,Nikolaos Papanikolaou,The ProCAncer-I consortium,Konstantinos Marias,Dimitrios I. Fotiadis,Manolis Tsiknakis*

Main category: cs.AI

TL;DR: 该论文介绍了AI模型护照的概念，旨在通过标准化框架提升AI模型在医疗领域的透明度、可追溯性和合规性，并通过AIPassport工具实现自动化元数据管理。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在医疗和生物医学领域的应用缺乏统一的透明度和可追溯性框架，现有方法依赖人工文档，难以扩展和跨平台验证，影响了模型的可复现性和信任度。

Method: 提出AI Model Passport框架，作为AI模型的数字身份和验证工具，并通过AIPassport工具实现自动化元数据收集、版本控制和开发环境集成。

Result: 在ProCAncer-I项目的医学影像应用中，AIPassport成功展示了其在提升透明度、可复现性和合规性方面的有效性，同时减少了人工操作。

Conclusion: AI Model Passport框架为医疗AI解决方案的信任和问责设立了新标准，有望成为跨领域透明合规AI系统的基础。

Abstract: The increasing integration of Artificial Intelligence (AI) into health and
biomedical systems necessitates robust frameworks for transparency,
accountability, and ethical compliance. Existing frameworks often rely on
human-readable, manual documentation which limits scalability, comparability,
and machine interpretability across projects and platforms. They also fail to
provide a unique, verifiable identity for AI models to ensure their provenance
and authenticity across systems and use cases, limiting reproducibility and
stakeholder trust. This paper introduces the concept of the AI Model Passport,
a structured and standardized documentation framework that acts as a digital
identity and verification tool for AI models. It captures essential metadata to
uniquely identify, verify, trace and monitor AI models across their lifecycle -
from data acquisition and preprocessing to model design, development and
deployment. In addition, an implementation of this framework is presented
through AIPassport, an MLOps tool developed within the ProCAncer-I EU project
for medical imaging applications. AIPassport automates metadata collection,
ensures proper versioning, decouples results from source scripts, and
integrates with various development environments. Its effectiveness is
showcased through a lesion segmentation use case using data from the
ProCAncer-I dataset, illustrating how the AI Model Passport enhances
transparency, reproducibility, and regulatory readiness while reducing manual
effort. This approach aims to set a new standard for fostering trust and
accountability in AI-driven healthcare solutions, aspiring to serve as the
basis for developing transparent and regulation compliant AI systems across
domains.

</details>


### [120] [The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements](https://arxiv.org/abs/2506.22419)
*Bingchen Zhao,Despoina Magka,Minqi Jiang,Xian Li,Roberta Raileanu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Kelvin Niu,Shagun Sodhani,Michael Shvartsman,Andrei Lupu,Alisia Lupidi,Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Thomas Foster,Lucia Cipolina-Kun,Abhishek Charnalia,Derek Dunfield,Alexander H. Miller,Oisin Mac Aodha,Jakob Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: 论文提出了一个自动化LLM速度跑分基准，用于评估AI代理在科学重现方面的能力，发现当前先进推理LLM即使有详细提示也难以重现已知创新。


<details>
  <summary>Details</summary>
Motivation: 评估AI代理在科学研究重现方面的能力，这是实现自主研究代理的必要技能。

Method: 引入自动化LLM速度跑分基准，基于NanoGPT速度跑竞赛的19个任务，提供不同提示格式以测试代理重现能力。

Result: 当前先进的推理LLM结合最先进的支架仍难以重现已知创新，即使给予详细提示。

Conclusion: 该基准为非饱和的LLM科学重现能力提供了简单有效的衡量标准，突显了自主研究代理面临的挑战。

Abstract: Rapid advancements in large language models (LLMs) have the potential to
assist in scientific progress. A critical capability toward this endeavor is
the ability to reproduce existing work. To evaluate the ability of AI agents to
reproduce results in an active research area, we introduce the Automated LLM
Speedrunning Benchmark, leveraging the research community contributions on the
NanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.
Each of the 19 speedrun tasks provides the agent with the previous records
training script, optionally paired with one of three hint formats, ranging from
pseudocode to paper-like descriptions of the new records improvements. Records
execute quickly by design and speedrun improvements encompass diverse
code-level changes, ranging from high-level algorithmic advancements to
hardware-aware optimizations. These features make the benchmark both accessible
and realistic for the frontier problem of improving LLM training. We find that
recent reasoning LLMs combined with SoTA scaffolds struggle to reimplement
already-known innovations in our benchmark, even when given detailed hints. Our
benchmark thus provides a simple, non-saturated measure of an LLMs ability to
automate scientific reproduction, a necessary (but not sufficient) skill for an
autonomous research agent.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [121] [APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization](https://arxiv.org/abs/2506.21655)
*Minjie Hong,Zirun Guo,Yan Xia,Zehan Wang,Ziang Zhang,Tao Jin,Zhou Zhao*

Main category: cs.LG

TL;DR: 提出APO方法解决MLLMs中强化学习训练时的KL惩罚和过度思考问题，通过DADS和STCR技术提升推理能力并保持泛化性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在复杂推理任务中表现不佳，而强化学习（RL）虽能提升推理能力，但应用于MLLMs时会导致通用任务性能下降和过度推理的问题。

Method: 提出非对称策略优化（APO），将样本分为正负两组：正样本采用难度自适应散度调整（DADS）动态调整KL散度权重；负样本采用次优轨迹复杂度正则化（STCR）惩罚过长响应。

Result: 在Qwen2.5-VL-3B上实现的View-R1-3B模型，推理能力平均提升7%，优于更大规模的MLLMs（7-11B），且通用任务性能未下降。

Conclusion: DADS和STCR技术有效提升了MLLMs的复杂推理能力，同时保持了模型的泛化性能，具有广泛适用性。

Abstract: Multimodal Large Language Models (MLLMs) are powerful at integrating diverse
data, but they often struggle with complex reasoning. While Reinforcement
learning (RL) can boost reasoning in LLMs, applying it to MLLMs is tricky.
Common issues include a drop in performance on general tasks and the generation
of overly detailed or "overthinking" reasoning. Our work investigates how the
KL penalty and overthinking affect RL training in MLLMs. We propose Asymmetric
Policy Optimization (APO) to address these issues, which divides the sampled
responses into positive and negative groups. For positive samples,
Difficulty-Adaptive Divergence Shaping (DADS) is introduced to dynamically
adjust the KL divergence weight based on their difficulty. This method prevents
policy entropy from dropping sharply, improves training stability, utilizes
samples better, and preserves the model's existing knowledge. For negative
samples, Suboptimal Trajectory Complexity Regularization (STCR) is proposed to
penalize overly long responses. This helps mitigate overthinking and encourages
more concise reasoning while preserving the model's explorative capacity. We
apply our method to Qwen2.5-VL-3B, creating View-R1-3B. View-R1-3B
significantly enhances reasoning capabilities, showing an average 7\% gain over
the base model and outperforming larger MLLMs (7-11B) on various reasoning
benchmarks. Importantly, unlike other reasoning-tuned MLLMs that often degrade
on general tasks, View-R1-3B maintains consistent improvement, demonstrating
superior generalization. These results highlight the effectiveness and broad
applicability of our DADS and STCR techniques for advancing complex multimodal
reasoning in MLLMs. The code will be made available at
https://github.com/Indolent-Kawhi/View-R1.

</details>


### [122] [Risk-Averse Total-Reward Reinforcement Learning](https://arxiv.org/abs/2506.21683)
*Xihong Su,Jia Lin Hau,Gersi Doko,Kishan Panaganti,Marek Petrik*

Main category: cs.LG

TL;DR: 提出一种Q-learning算法，用于求解总奖励ERM和EVaR目标的最优策略，具有强收敛性和性能保证。


<details>
  <summary>Details</summary>
Motivation: 现有基于模型的风险度量算法（如ERM和EVaR）在小规模问题上有效，但需要完全访问转移概率，限制了其应用范围。

Method: 利用ERM的动态一致性和可引出性，设计了一种Q-learning算法，用于计算总奖励ERM和EVaR目标的最优平稳策略。

Result: 数值实验显示，该算法在表格域中能快速可靠地收敛到最优风险规避值函数。

Conclusion: 所提Q-learning算法为风险规避总奖励MDP提供了一种有效的无模型解决方案。

Abstract: Risk-averse total-reward Markov Decision Processes (MDPs) offer a promising
framework for modeling and solving undiscounted infinite-horizon objectives.
Existing model-based algorithms for risk measures like the entropic risk
measure (ERM) and entropic value-at-risk (EVaR) are effective in small
problems, but require full access to transition probabilities. We propose a
Q-learning algorithm to compute the optimal stationary policy for total-reward
ERM and EVaR objectives with strong convergence and performance guarantees. The
algorithm and its optimality are made possible by ERM's dynamic consistency and
elicitability. Our numerical results on tabular domains demonstrate quick and
reliable convergence of the proposed Q-learning algorithm to the optimal
risk-averse value function.

</details>


### [123] [Unimodal Strategies in Density-Based Clustering](https://arxiv.org/abs/2506.21695)
*Oron Nir,Jay Tenenbaum,Ariel Shamir*

Main category: cs.LG

TL;DR: 该论文揭示了基于密度的聚类方法中核心点邻域半径与簇数量之间的单峰关系，并提出了一种基于三元搜索算法的高效参数调优策略，适用于高维大规模数据。


<details>
  <summary>Details</summary>
Motivation: 基于密度的聚类方法在处理噪声或任意分布数据时优于基于质心的方法，但参数调优（如邻域半径）在大规模高维数据中计算成本高。研究旨在解决这一问题。

Method: 通过理论和实证分析发现核心点邻域半径与簇数量呈近单峰关系，并利用三元搜索算法设计高效参数调优策略。

Result: 在多个高维大规模NLP、音频和计算机视觉任务中验证了方法的有效性和鲁棒性，显著提升了参数调优效率。

Conclusion: 该研究不仅推动了基于密度聚类的参数控制技术，还深化了对关键参数间关系的理解，代码已公开。

Abstract: Density-based clustering methods often surpass centroid-based counterparts,
when addressing data with noise or arbitrary data distributions common in
real-world problems. In this study, we reveal a key property intrinsic to
density-based clustering methods regarding the relation between the number of
clusters and the neighborhood radius of core points - we empirically show that
it is nearly unimodal, and support this claim theoretically in a specific
setting. We leverage this property to devise new strategies for finding
appropriate values for the radius more efficiently based on the Ternary Search
algorithm. This is especially important for large scale data that is
high-dimensional, where parameter tuning is computationally intensive. We
validate our methodology through extensive applications across a range of
high-dimensional, large-scale NLP, Audio, and Computer Vision tasks,
demonstrating its practical effectiveness and robustness. This work not only
offers a significant advancement in parameter control for density-based
clustering but also broadens the understanding regarding the relations between
their guiding parameters. Our code is available at
https://github.com/oronnir/UnimodalStrategies.

</details>


### [124] [$\textrm{ODE}_t \left(\textrm{ODE}_l \right)$: Shortcutting the Time and Length in Diffusion and Flow Models for Faster Sampling](https://arxiv.org/abs/2506.21714)
*Denis Gudovskiy,Wenzhao Zheng,Tomoyuki Okuno,Yohei Nakata,Kurt Keutzer*

Main category: cs.LG

TL;DR: 该论文提出了一种动态控制连续归一化流（CNFs）和扩散模型（DMs）质量-复杂度权衡的方法，通过重连Transformer块和引入时间与长度一致性项，实现了在任意时间步和块数下的高效采样。


<details>
  <summary>Details</summary>
Motivation: 尽管CNFs和DMs能够从噪声分布生成高质量数据点，但采样过程需要多次迭代求解高计算复杂度的ODE。现有方法主要关注减少时间步数以提高效率，而本文探索了在时间步长和神经网络长度上动态控制质量-复杂度权衡的互补方向。

Method: 通过重连Transformer架构中的块来求解内部离散化的ODE，并在流匹配训练中引入时间和长度一致性项，从而支持任意时间步和Transformer块数的采样。

Result: 在CelebA-HQ和ImageNet上的实验表明，最高效采样模式下延迟降低了3倍，高质量采样模式下FID分数提高了3.5分。

Conclusion: 提出的ODE_t（ODE_l）方法在时间维度上不依赖特定求解器，同时降低了延迟和内存使用，为高效高质量采样提供了新思路。

Abstract: Recently, continuous normalizing flows (CNFs) and diffusion models (DMs) have
been studied using the unified theoretical framework. Although such models can
generate high-quality data points from a noise distribution, the sampling
demands multiple iterations to solve an ordinary differential equation (ODE)
with high computational complexity. Most existing methods focus on reducing the
number of time steps during the sampling process to improve efficiency. In this
work, we explore a complementary direction in which the quality-complexity
tradeoff can be dynamically controlled in terms of time steps and in the length
of the neural network. We achieve this by rewiring the blocks in the
transformer-based architecture to solve an inner discretized ODE w.r.t. its
length. Then, we employ time- and length-wise consistency terms during flow
matching training, and as a result, the sampling can be performed with an
arbitrary number of time steps and transformer blocks. Unlike others, our
$\textrm{ODE}_t \left(\textrm{ODE}_l \right)$ approach is solver-agnostic in
time dimension and decreases both latency and memory usage. Compared to the
previous state of the art, image generation experiments on CelebA-HQ and
ImageNet show a latency reduction of up to $3\times$ in the most efficient
sampling mode, and a FID score improvement of up to $3.5$ points for
high-quality sampling. We release our code and model weights with fully
reproducible experiments.

</details>


### [125] [Performance Prediction for Large Systems via Text-to-Text Regression](https://arxiv.org/abs/2506.21718)
*Yash Akhauri,Bryan Lewandowski,Cheng-Hsi Lin,Adrian N. Reyes,Grant C. Forbes,Arissa Wongpanich,Bangding Yang,Mohamed S. Abdelfattah,Sagi Perel,Xingyou Song*

Main category: cs.LG

TL;DR: 论文提出了一种基于文本到文本回归的通用方法，用于预测复杂系统（如配置文件和系统日志）的度量结果，显著优于传统表格回归方法。


<details>
  <summary>Details</summary>
Motivation: 传统表格回归方法在处理复杂系统数据（如配置文件和系统日志）时，特征工程往往不可行，导致预测效果不佳。

Method: 提出了一种文本到文本回归方法，使用60M参数的编码器-解码器模型，从随机初始化开始训练，适用于大规模计算集群调度系统。

Result: 在Google的Borg计算集群调度系统上，该方法实现了接近完美的0.99（平均0.9）等级相关性，均方误差比表格方法低100倍，并能轻松适应新任务。

Conclusion: 该方法为复杂系统的度量结果预测提供了一种通用、可扩展的解决方案，为现实世界结果的通用模拟器铺平了道路。

Abstract: In many industries, predicting metric outcomes of large systems is a
fundamental problem, driven largely by traditional tabular regression. However,
such methods struggle on complex systems data in the wild such as configuration
files or system logs, where feature engineering is often infeasible. We propose
text-to-text regression as a general, scalable alternative. For predicting
resource efficiency on Borg, Google's massive compute cluster scheduling
system, a 60M parameter encoder-decoder, trained from random initialization,
achieves up to a near perfect 0.99 (0.9 average) rank correlation across the
entire fleet, and 100x lower MSE than tabular approaches. The model also easily
adapts to new tasks in only 500 few-shot examples and captures the densities of
complex outcome distributions. Ablation studies highlight the importance of
using encoders, increasing sequence length, and the model's inherent
uncertainty quantification. These findings pave the way for universal
simulators of real-world outcomes.

</details>


### [126] [Federated Item Response Theory Models](https://arxiv.org/abs/2506.21744)
*Biying Zhou,Nanyu Luo,Feng Ji*

Main category: cs.LG

TL;DR: 该论文介绍了一种名为联邦项目反应理论（FedIRT）的新框架，结合联邦学习的隐私保护和分布式计算优势，解决了传统IRT模型在数据集中处理上的隐私问题，同时保持估计准确性。


<details>
  <summary>Details</summary>
Motivation: 传统IRT模型需要集中所有个体原始响应数据，可能导致隐私问题。联邦学习提供了隐私保护和分布式计算的优势，因此作者希望将这两种技术结合，以扩展IRT在分布式环境（如多学校评估）中的应用。

Method: 提出联邦项目反应理论（FedIRT）框架，支持分布式环境下的IRT模型估计，包括两参数逻辑模型（2PL）和部分信用模型（PCM），并开发了开源的R包FedIRT。

Result: 数值实验表明，FedIRT在统计准确性上与标准IRT估计相当，同时提供了隐私保护和降低通信成本的优势。真实考试数据集验证了其在教育场景中的有效性。

Conclusion: FedIRT框架成功将IRT扩展到分布式环境，兼顾准确性、隐私保护和实用性，为多机构协作评估提供了可行解决方案。

Abstract: Item Response Theory (IRT) models have been widely used to estimate
respondents' latent abilities and calibrate items' difficulty. Traditional IRT
estimation requires all individual raw response data to be centralized in one
place, thus potentially causing privacy issues. Federated learning is an
emerging field in computer science and machine learning with added features of
privacy protection and distributed computing. To integrate the advances from
federated learning with modern psychometrics, we propose a novel framework,
Federated Item Response Theory (IRT), to enable estimating traditional IRT
models with additional privacy, allowing estimation in a distributed manner
without losing estimation accuracy.
  Our numerical experiments confirm that FedIRT achieves statistical accuracy
similar to standard IRT estimation using popular R packages, while offering
critical advantages: privacy protection and reduced communication costs. We
also validate FedIRT's utility through a real-world exam dataset, demonstrating
its effectiveness in realistic educational contexts. This new framework extends
IRT's applicability to distributed settings, such as multi-school assessments,
without sacrificing accuracy or security. To support practical adoption, we
provide an open-ource R package, FedIRT, implementing the framework for the
two-parameter logistic (2PL) and partial credit models (PCM).

</details>


### [127] [Gradient-Based Neuroplastic Adaptation for Concurrent Optimization of Neuro-Fuzzy Networks](https://arxiv.org/abs/2506.21771)
*John Wesley Hostetter,Min Chi*

Main category: cs.LG

TL;DR: 提出一种基于梯度的神经可塑性适应方法，同时优化神经模糊网络的参数和结构，解决了传统设计过程中参数与结构分离优化的问题，并在视觉任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 神经模糊网络（NFNs）虽然透明且具有符号表达能力，但其系统化设计过程仍具挑战性。现有方法通常将参数和结构识别分离，导致架构脆弱且性能不佳。

Method: 提出一种与应用无关的梯度基神经可塑性适应方法，同时优化神经模糊网络的参数和结构，适用于在线强化学习等复杂场景。

Result: 实验表明，该方法在基于视觉的视频游戏DOOM中，通过在线强化学习训练，能够高效处理复杂任务场景。

Conclusion: 同时优化神经模糊网络的参数和结构，能够显著提升其性能，拓展了其在复杂任务中的应用范围。

Abstract: Neuro-fuzzy networks (NFNs) are transparent, symbolic, and universal function
approximations that perform as well as conventional neural architectures, but
their knowledge is expressed as linguistic IF-THEN rules. Despite these
advantages, their systematic design process remains a challenge. Existing work
will often sequentially build NFNs by inefficiently isolating parametric and
structural identification, leading to a premature commitment to brittle and
subpar architecture. We propose a novel application-independent approach called
gradient-based neuroplastic adaptation for the concurrent optimization of NFNs'
parameters and structure. By recognizing that NFNs' parameters and structure
should be optimized simultaneously as they are deeply conjoined, settings
previously unapproachable for NFNs are now accessible, such as the online
reinforcement learning of NFNs for vision-based tasks. The effectiveness of
concurrently optimizing NFNs is empirically shown as it is trained by online
reinforcement learning to proficiently play challenging scenarios from a
vision-based video game called DOOM.

</details>


### [128] [M3PO: Massively Multi-Task Model-Based Policy Optimization](https://arxiv.org/abs/2506.21782)
*Aditya Narendra,Dmitry Makarov,Aleksandr Panov*

Main category: cs.LG

TL;DR: M3PO是一种基于模型的强化学习框架，通过结合隐式世界模型和混合探索策略，解决了单任务样本效率低和多任务泛化差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于像素级生成模型的方法（如DreamerV3）忽略了控制中心表示，而基于模型的方法（如PPO）样本复杂度高且探索能力弱。M3PO旨在解决这些问题。

Method: M3PO整合了隐式世界模型和混合探索策略，利用模型与无模型价值估计的差异指导探索，并通过信任区域优化器保持策略更新的稳定性。

Result: M3PO在多个基准测试中实现了最先进的性能，为基于模型的策略优化提供了高效且稳健的替代方案。

Conclusion: M3PO通过消除偏差-方差权衡，显著提升了强化学习在单任务和多任务场景中的表现。

Abstract: We introduce Massively Multi-Task Model-Based Policy Optimization (M3PO), a
scalable model-based reinforcement learning (MBRL) framework designed to
address sample inefficiency in single-task settings and poor generalization in
multi-task domains. Existing model-based approaches like DreamerV3 rely on
pixel-level generative models that neglect control-centric representations,
while model-free methods such as PPO suffer from high sample complexity and
weak exploration. M3PO integrates an implicit world model, trained to predict
task outcomes without observation reconstruction, with a hybrid exploration
strategy that combines model-based planning and model-free uncertainty-driven
bonuses. This eliminates the bias-variance trade-off in prior methods by using
discrepancies between model-based and model-free value estimates to guide
exploration, while maintaining stable policy updates through a trust-region
optimizer. M3PO provides an efficient and robust alternative to existing
model-based policy optimization approaches and achieves state-of-the-art
performance across multiple benchmarks.

</details>


### [129] [Multi-task parallelism for robust pre-training of graph foundation models on multi-source, multi-fidelity atomistic modeling data](https://arxiv.org/abs/2506.21788)
*Massimiliano Lupo Pasini,Jong Youl Choi,Pei Zhang,Kshitij Mehta,Rylie Weaver,Ashwin M. Aji,Karl W. Schulz,Jorda Polo,Prasanna Balaprakash*

Main category: cs.LG

TL;DR: 该论文提出了一种基于多任务并行化的图神经网络方法HydraGNN，用于处理多源多精度原子结构数据，并在超级计算机上实现了高效扩展。


<details>
  <summary>Details</summary>
Motivation: 为了解决多源多精度数据在预训练中的处理问题，并提升模型在未探索化学区域的泛化能力。

Method: 采用多任务并行化方法，将每个解码头分布到计算资源中，利用GPU加速，并在开源架构HydraGNN中实现。

Result: 在超过2400万个结构的数据集上训练，并在Perlmutter、Aurora和Frontier超级计算机上测试，展示了高效的扩展性。

Conclusion: 该方法在多任务学习和超级计算机扩展性方面表现出色，为原子建模提供了可持续且高效的解决方案。

Abstract: Graph foundation models using graph neural networks promise sustainable,
efficient atomistic modeling. To tackle challenges of processing multi-source,
multi-fidelity data during pre-training, recent studies employ multi-task
learning, in which shared message passing layers initially process input
atomistic structures regardless of source, then route them to multiple decoding
heads that predict data-specific outputs. This approach stabilizes pre-training
and enhances a model's transferability to unexplored chemical regions.
Preliminary results on approximately four million structures are encouraging,
yet questions remain about generalizability to larger, more diverse datasets
and scalability on supercomputers. We propose a multi-task parallelism method
that distributes each head across computing resources with GPU acceleration.
Implemented in the open-source HydraGNN architecture, our method was trained on
over 24 million structures from five datasets and tested on the Perlmutter,
Aurora, and Frontier supercomputers, demonstrating efficient scaling on all
three highly heterogeneous super-computing architectures.

</details>


### [130] [Why Neural Network Can Discover Symbolic Structures with Gradient-based Training: An Algebraic and Geometric Foundation for Neurosymbolic Reasoning](https://arxiv.org/abs/2506.21797)
*Peihao Wang,Zhangyang Wang*

Main category: cs.LG

TL;DR: 该论文提出一个理论框架，解释离散符号结构如何从连续神经网络训练动态中自然涌现，通过将参数提升到测度空间并建模为Wasserstein梯度流，揭示了在几何约束下参数测度的分解与自由度收缩现象。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解神经网络如何从连续学习过渡到离散符号表示，为神经符号系统的设计与理解提供理论基础。

Method: 方法包括将神经参数提升到测度空间，建模训练为Wasserstein梯度流，并在群不变性等几何约束下分析参数测度的行为。

Result: 结果表明，训练过程中参数测度会分解为独立优化轨迹，自由度逐渐收缩，最终形成符合代数操作的组合表示。

Conclusion: 结论是该框架为神经符号系统提供了理论基础，连接了连续学习与离散代数推理，并建立了实现符号任务的数据缩放规律。

Abstract: We develop a theoretical framework that explains how discrete symbolic
structures can emerge naturally from continuous neural network training
dynamics. By lifting neural parameters to a measure space and modeling training
as Wasserstein gradient flow, we show that under geometric constraints, such as
group invariance, the parameter measure $\mu_t$ undergoes two concurrent
phenomena: (1) a decoupling of the gradient flow into independent optimization
trajectories over some potential functions, and (2) a progressive contraction
on the degree of freedom. These potentials encode algebraic constraints
relevant to the task and act as ring homomorphisms under a commutative
semi-ring structure on the measure space. As training progresses, the network
transitions from a high-dimensional exploration to compositional
representations that comply with algebraic operations and exhibit a lower
degree of freedom. We further establish data scaling laws for realizing
symbolic tasks, linking representational capacity to the group invariance that
facilitates symbolic solutions. This framework charts a principled foundation
for understanding and designing neurosymbolic systems that integrate continuous
learning with discrete algebraic reasoning.

</details>


### [131] [The Cost of Avoiding Backpropagation](https://arxiv.org/abs/2506.21833)
*Kunjal Panchal,Sunav Choudhary,Yuriy Brun,Hui Guan*

Main category: cs.LG

TL;DR: 该论文通过理论和实证分析比较了前向模式自动微分(FmAD)、零阶优化(ZO)与反向传播(BP)在内存受限环境下的性能，发现BP结合检查点技术仍是最优选择。


<details>
  <summary>Details</summary>
Motivation: 研究动机是澄清FmAD和ZO作为内存高效替代方案的实际效益，并与内存优化的BP变体（如激活检查点）进行对比，填补现有比较和理论分析的空白。

Method: 采用统一的理论框架和实证实验，对比BP、FmAD和ZO在大型语言模型和视觉语言模型中的内存使用、准确性、收敛速度和计算成本。

Result: 实验结果表明，BP结合检查点技术在相同内存条件下，比FmAD和ZO方法准确率提高31.1%，收敛速度加快34.8%，计算量减少3.8倍。

Conclusion: 结论指出FmAD和ZO虽能降低内存消耗，但在精度、收敛速度和计算效率上存在显著劣势，而BP配合检查点仍是内存受限场景下最有效的训练策略。

Abstract: Forward-mode automatic differentiation (FmAD) and zero-order (ZO)
optimization have been proposed as memory-efficient alternatives to
backpropagation (BP) for gradient computation, especially in low-resource
settings. However, their practical benefits remain unclear due to two key gaps:
a lack of comparison against memory-efficient BP variants, such as activation
checkpointing, and a lack of a unified theoretical analysis. This work presents
a comprehensive theoretical and empirical comparison of BP, FmAD, and ZO
methods. Our theoretical analysis shows that while FmAD, and ZO can reduce
memory usage, they incur significant costs in accuracy, convergence speed, and
computation compared to BP with checkpointing. These drawbacks worsen with
larger models or constrained perturbation budgets. Empirical experiments on
large language and vision-language models show that BP with checkpointing
outperforms FmAD and ZO variants, including those enhanced with variance
reduction, achieving up to 31.1% higher accuracy, 34.8% faster convergence, and
3.8x fewer computations at comparable memory usage. Our results highlight
fundamental limitations of FmAD and ZO, and reaffirm BP with checkpointing as
the most effective strategy for model training under memory-constrained
settings. Our code is available at
https://github.com/Astuary/The_Cost_of_Avoiding_Backpropagation.

</details>


### [132] [Koopman operator-based discussion on partial observation in stochastic systems](https://arxiv.org/abs/2506.21844)
*Jun Ohkubo*

Main category: cs.LG

TL;DR: 论文探讨了随机系统中部分观测的影响，结合Koopman算子理论，强调了状态空间与函数空间的区分，并通过数值实验展示了延迟嵌入技术的优势及噪声幅度的幂律行为。


<details>
  <summary>Details</summary>
Motivation: 在确定性系统中，Mori-Zwanzig形式为部分观测提供了理论框架，但随机系统中的部分观测问题尚未充分研究。本文旨在通过Koopman算子理论填补这一空白。

Method: 使用Koopman算子理论分析随机系统中的部分观测效应，结合延迟嵌入技术，并通过数值实验验证理论。

Result: 研究发现，延迟嵌入技术对随机系统的部分观测有效，且噪声幅度的准确性呈现幂律行为。幂律指数与部分观测效应相关。

Conclusion: 随机系统中区分状态空间与函数空间至关重要，延迟嵌入技术能有效处理部分观测问题，噪声幅度的幂律行为为实际应用提供了新见解。

Abstract: It is sometimes difficult to achieve a complete observation for a full set of
observables, and partial observations are necessary. For deterministic systems,
the Mori-Zwanzig formalism provides a theoretical framework for handling
partial observations. Recently, data-driven algorithms based on the Koopman
operator theory have made significant progress, and there is a discussion to
connect the Mori-Zwanzig formalism with the Koopman operator theory. In this
work, we discuss the effects of partial observation in stochastic systems using
the Koopman operator theory. The discussion clarifies the importance of
distinguishing the state space and the function space in stochastic systems.
Even in stochastic systems, the delay embedding technique is beneficial for
partial observation, and several numerical experiments showed a power-law
behavior of the accuracy for the amplitude of the additive noise. We also
discuss the relation between the exponent of the power-law behavior and the
effects of partial observation.

</details>


### [133] [A Survey of Continual Reinforcement Learning](https://arxiv.org/abs/2506.21872)
*Chaofan Pan,Xin Yang,Yanhua Li,Wei Wei,Tianrui Li,Bo An,Jiye Liang*

Main category: cs.LG

TL;DR: 该论文探讨了持续强化学习（CRL）作为解决传统强化学习数据依赖和泛化能力不足的新方向，提出了新的分类方法并分析了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习（RL）依赖大量数据和计算资源，且跨任务泛化能力有限。持续学习（CL）的兴起为RL提供了持续学习和知识保留的新思路，从而催生了持续强化学习（CRL）的研究。

Method: 论文首先系统回顾了现有CRL研究，包括指标、任务、基准和场景设置；其次从知识存储和迁移的角度提出了CRL方法的四类新分类法。

Result: 通过分析CRL的独特挑战，论文提出了未来研究的实用方向，为CRL领域提供了系统性的总结和分类框架。

Conclusion: CRL是解决RL局限性的有前景方向，论文的分类和挑战分析为未来研究提供了重要参考。

Abstract: Reinforcement Learning (RL) is an important machine learning paradigm for
solving sequential decision-making problems. Recent years have witnessed
remarkable progress in this field due to the rapid development of deep neural
networks. However, the success of RL currently relies on extensive training
data and computational resources. In addition, RL's limited ability to
generalize across tasks restricts its applicability in dynamic and real-world
environments. With the arisen of Continual Learning (CL), Continual
Reinforcement Learning (CRL) has emerged as a promising research direction to
address these limitations by enabling agents to learn continuously, adapt to
new tasks, and retain previously acquired knowledge. In this survey, we provide
a comprehensive examination of CRL, focusing on its core concepts, challenges,
and methodologies. Firstly, we conduct a detailed review of existing works,
organizing and analyzing their metrics, tasks, benchmarks, and scenario
settings. Secondly, we propose a new taxonomy of CRL methods, categorizing them
into four types from the perspective of knowledge storage and/or transfer.
Finally, our analysis highlights the unique challenges of CRL and provides
practical insights into future directions.

</details>


### [134] [Advancements and Challenges in Continual Reinforcement Learning: A Comprehensive Review](https://arxiv.org/abs/2506.21899)
*Amara Zuffer,Michael Burke,Mehrtash Harandi*

Main category: cs.LG

TL;DR: 该综述探讨了持续强化学习（CRL）如何使智能体具备连续学习能力，涵盖关键概念、挑战、方法及在机器人领域的应用，并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 强化学习（RL）任务的多样性和动态性要求智能体能够持续学习，以获取并保留可重用的知识，从而适应不断变化的环境。

Method: 综述分析了持续强化学习的基本原理、关键挑战和新兴方法，特别关注机器人领域的最新进展，并总结了常用评估环境。

Result: 论文系统梳理了CRL的核心内容，为研究者提供了清晰的框架，并指出当前技术的局限性及潜在改进方向。

Conclusion: 持续强化学习是RL发展的重要方向，未来需进一步解决知识保留与迁移的挑战，以推动实际应用。

Abstract: The diversity of tasks and dynamic nature of reinforcement learning (RL)
require RL agents to be able to learn sequentially and continuously, a learning
paradigm known as continuous reinforcement learning. This survey reviews how
continual learning transforms RL agents into dynamic continual learners. This
enables RL agents to acquire and retain useful and reusable knowledge
seamlessly. The paper delves into fundamental aspects of continual
reinforcement learning, exploring key concepts, significant challenges, and
novel methodologies. Special emphasis is placed on recent advancements in
continual reinforcement learning within robotics, along with a succinct
overview of evaluation environments utilized in prominent research,
facilitating accessibility for newcomers to the field. The review concludes
with a discussion on limitations and promising future directions, providing
valuable insights for researchers and practitioners alike.

</details>


### [135] [TOAST: Task-Oriented Adaptive Semantic Transmission over Dynamic Wireless Environments](https://arxiv.org/abs/2506.21900)
*Sheng Yun,Jianhua Pei,Ping Wang*

Main category: cs.LG

TL;DR: TOAST框架通过自适应任务平衡、LoRA机制和扩散模型，优化6G网络中的语义通信，在低信噪比下显著提升分类和重建性能。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要从比特传输转向语义感知通信，强调任务相关信息。现有方法在多任务优化和动态无线环境适应方面存在挑战。

Method: 提出TOAST框架，包含：1) 基于强化学习的任务平衡；2) Swin Transformer结合LoRA的轻量调参；3) 潜在空间扩散模型去噪。

Result: 实验表明TOAST在低信噪比下分类准确率与重建质量显著优于基线，且在所有测试场景中保持鲁棒性。

Conclusion: TOAST为6G语义通信提供了高效、自适应的解决方案，平衡了多任务需求与动态环境挑战。

Abstract: The evolution toward 6G networks demands a fundamental shift from bit-centric
transmission to semantic-aware communication that emphasizes task-relevant
information. This work introduces TOAST (Task-Oriented Adaptive Semantic
Transmission), a unified framework designed to address the core challenge of
multi-task optimization in dynamic wireless environments through three
complementary components. First, we formulate adaptive task balancing as a
Markov decision process, employing deep reinforcement learning to dynamically
adjust the trade-off between image reconstruction fidelity and semantic
classification accuracy based on real-time channel conditions. Second, we
integrate module-specific Low-Rank Adaptation (LoRA) mechanisms throughout our
Swin Transformer-based joint source-channel coding architecture, enabling
parameter-efficient fine-tuning that dramatically reduces adaptation overhead
while maintaining full performance across diverse channel impairments including
Additive White Gaussian Noise (AWGN), fading, phase noise, and impulse
interference. Third, we incorporate an Elucidating diffusion model that
operates in the latent space to restore features corrupted by channel noises,
providing substantial quality improvements compared to baseline approaches.
Extensive experiments across multiple datasets demonstrate that TOAST achieves
superior performance compared to baseline approaches, with significant
improvements in both classification accuracy and reconstruction quality at low
Signal-to-Noise Ratio (SNR) conditions while maintaining robust performance
across all tested scenarios.

</details>


### [136] [HQCM-EBTC: A Hybrid Quantum-Classical Model for Explainable Brain Tumor Classification](https://arxiv.org/abs/2506.21937)
*Marwan Ait Haddou,Mohamed Bennai*

Main category: cs.LG

TL;DR: HQCM-EBTC是一种混合量子-经典模型，用于MRI图像的自动脑肿瘤分类，准确率达96.48%，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 探索量子增强模型在医学影像中的应用，以提高脑肿瘤诊断的准确性和可解释性。

Method: 结合5量子比特、深度2的量子层与5个并行电路，使用AdamW优化器和交叉熵与注意力一致性的复合损失函数进行训练。

Result: 在7,576张MRI扫描数据集上，HQCM-EBTC实现了96.48%的准确率，尤其在胶质瘤检测中表现优异，特征可分性和肿瘤定位准确性显著提升。

Conclusion: HQCM-EBTC展示了量子增强模型在医学影像中的潜力，为临床脑肿瘤评估提供了更高的诊断准确性和可解释性。

Abstract: We propose HQCM-EBTC, a hybrid quantum-classical model for automated brain
tumor classification using MRI images. Trained on a dataset of 7,576 scans
covering normal, meningioma, glioma, and pituitary classes, HQCM-EBTC
integrates a 5-qubit, depth-2 quantum layer with 5 parallel circuits, optimized
via AdamW and a composite loss blending cross-entropy and attention
consistency.
  HQCM-EBTC achieves 96.48% accuracy, substantially outperforming the classical
baseline (86.72%). It delivers higher precision and F1-scores, especially for
glioma detection. t-SNE projections reveal enhanced feature separability in
quantum space, and confusion matrices show lower misclassification. Attention
map analysis (Jaccard Index) confirms more accurate and focused tumor
localization at high-confidence thresholds.
  These results highlight the promise of quantum-enhanced models in medical
imaging, advancing both diagnostic accuracy and interpretability for clinical
brain tumor assessment.

</details>


### [137] [GuiderNet: A Meta-Learning Framework for Optimizing Quantum Circuit Geometry and Mitigating Barren Plateaus](https://arxiv.org/abs/2506.21940)
*Marwan Ait Haddou,Mohamed Bennai*

Main category: cs.LG

TL;DR: GuiderNet通过元学习框架改善量子变分算法的训练，显著提升糖尿病分类任务的性能。


<details>
  <summary>Details</summary>
Motivation: 变分量子算法（VQAs）在近期量子优势方面具有潜力，但面临梯度消失和优化条件差的问题。

Method: 引入GuiderNet，一个元学习框架，通过数据依赖的参数调整优化量子电路的几何条件。

Result: 在糖尿病分类任务中，训练损失降低5倍，测试准确率从75.3%提升至98.6%，少数类F1分数从0.67增至0.95。

Conclusion: 几何元调节可缓解量子机器学习中的训练难题，提升可训练性和泛化能力。

Abstract: Variational Quantum Algorithms (VQAs) offer potential for near-term quantum
advantage but face challenges from barren plateaus, where gradients vanish, and
poorly conditioned optimization landscapes. We introduce GuiderNet, a
meta-learning framework that conditions Parameterized Quantum Circuits (PQCs)
using data-dependent parameter shifts aimed at minimizing the log condition
number of the Fubini-Study metric tensor. Implemented as a classical neural
network, GuiderNet is meta-trained to guide PQC parameters into geometrically
favorable regions and is embedded within hybrid quantum-classical pipelines to
steer both initialization and adaptive modulation during training.
  Applied to the Kaggle Diabetes classification task, GuiderNet reduces
cumulative training loss by over 5x, improves test accuracy from 75.3% to
98.6%, and increases the minority-class F1 score from 0.67 to 0.95. It also
suppresses gradient explosion and stabilizes parameter updates, enabling
smoother and more robust optimization. These results demonstrate that geometric
meta-conditioning can mitigate barren plateaus and ill-conditioning, providing
a scalable approach to enhance trainability and generalization in quantum
machine learning.

</details>


### [138] [Physics-informed network paradigm with data generation and background noise removal for diverse distributed acoustic sensing applications](https://arxiv.org/abs/2506.21952)
*Yangyang Wan,Haotian Wang,Xuhui Yu,Jiageng Chen,Xinyu Fan,Zuyuan He*

Main category: cs.LG

TL;DR: 该论文介绍了一种基于物理信息的分布式声学传感（DAS）神经网络范式，无需真实事件数据进行训练，通过物理建模生成数据并实现去背景噪声，在事件识别和故障监测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型需要真实数据进行训练，但实际场景中事件数据有限，且DAS应用中存在数据获取困难和噪声干扰的问题。

Method: 通过物理建模目标事件和DAS系统约束，推导物理函数训练生成网络生成DAS事件数据，并训练去背景噪声网络。

Result: 在公共DAS数据集和传送带故障监测中验证了有效性，故障诊断准确率达91.8%，且在不同场景中表现出良好的泛化能力。

Conclusion: 该范式为解决DAS应用中数据获取和噪声问题提供了前瞻性解决方案，并拓展了DAS的潜在应用领域。

Abstract: Distributed acoustic sensing (DAS) has attracted considerable attention
across various fields and artificial intelligence (AI) technology plays an
important role in DAS applications to realize event recognition and denoising.
Existing AI models require real-world data (RWD), whether labeled or not, for
training, which is contradictory to the fact of limited available event data in
real-world scenarios. Here, a physics-informed DAS neural network paradigm is
proposed, which does not need real-world events data for training. By
physically modeling target events and the constraints of real world and DAS
system, physical functions are derived to train a generative network for
generation of DAS events data. DAS debackground net is trained by using the
generated DAS events data to eliminate background noise in DAS data. The
effectiveness of the proposed paradigm is verified in event identification
application based on a public dataset of DAS spatiotemporal data and in belt
conveyor fault monitoring application based on DAS time-frequency data, and
achieved comparable or better performance than data-driven networks trained
with RWD. Owing to the introduction of physical information and capability of
background noise removal, the paradigm demonstrates generalization in same
application on different sites. A fault diagnosis accuracy of 91.8% is achieved
in belt conveyor field with networks which transferred from simulation test
site without any fault events data of test site and field for training. The
proposed paradigm is a prospective solution to address significant obstacles of
data acquisition and intense noise in practical DAS applications and explore
more potential fields for DAS.

</details>


### [139] [Optimal Return-to-Go Guided Decision Transformer for Auto-Bidding in Advertisement](https://arxiv.org/abs/2506.21956)
*Hao Jiang,Yongxiang Tang,Yanxiang Zeng,Pengjia Yuan,Yanhua Cheng,Teng Sha,Xialong Liu,Peng Jiang*

Main category: cs.LG

TL;DR: 论文提出R*决策变换器（R* DT）改进自动竞价系统，通过三阶段方法解决传统决策变换器在预设回报值和数据质量上的限制，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在线广告自动竞价系统依赖决策变换器（DT），但传统DT存在预设回报值（RTG）需求及训练数据质量不均的问题，限制了其性能。

Method: 提出R* DT方法，分三步：R DT存储状态和RTG；R^ DT预测最优RTG；R* DT通过生成高奖励轨迹增强数据，逐步优化策略。

Result: 在公开竞价数据集上的测试表明，R* DT能提升训练数据中的RTG，并有效处理混合质量轨迹，表现优于传统方法。

Conclusion: R* DT通过数据增强和策略优化，显著提升了自动竞价系统的性能，尤其在处理复杂轨迹时表现突出。

Abstract: In the realm of online advertising, advertisers partake in ad auctions to
obtain advertising slots, frequently taking advantage of auto-bidding tools
provided by demand-side platforms. To improve the automation of these bidding
systems, we adopt generative models, namely the Decision Transformer (DT), to
tackle the difficulties inherent in automated bidding. Applying the Decision
Transformer to the auto-bidding task enables a unified approach to sequential
modeling, which efficiently overcomes short-sightedness by capturing long-term
dependencies between past bidding actions and user behavior. Nevertheless,
conventional DT has certain drawbacks: (1) DT necessitates a preset
return-to-go (RTG) value before generating actions, which is not inherently
produced; (2) The policy learned by DT is restricted by its training data,
which is consists of mixed-quality trajectories. To address these challenges,
we introduce the R* Decision Transformer (R* DT), developed in a three-step
process: (1) R DT: Similar to traditional DT, R DT stores actions based on
state and RTG value, as well as memorizing the RTG for a given state using the
training set; (2) R^ DT: We forecast the highest value (within the training
set) of RTG for a given state, deriving a suboptimal policy based on the
current state and the forecasted supreme RTG value; (3) R* DT: Based on R^ DT,
we generate trajectories and select those with high rewards (using a simulator)
to augment our training dataset. This data enhancement has been shown to
improve the RTG of trajectories in the training data and gradually leads the
suboptimal policy towards optimality. Comprehensive tests on a publicly
available bidding dataset validate the R* DT's efficacy and highlight its
superiority when dealing with mixed-quality trajectories.

</details>


### [140] [SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model](https://arxiv.org/abs/2506.21976)
*Shuhan Tan,John Lambert,Hong Jeon,Sakshum Kulshrestha,Yijing Bai,Jing Luo,Dragomir Anguelov,Mingxing Tan,Chiyu Max Jiang*

Main category: cs.LG

TL;DR: 提出SceneDiffuser++，首个端到端生成式世界模型，用于城市规模的点到点交通仿真，整合场景生成、行为建模等多项技术。


<details>
  <summary>Details</summary>
Motivation: 通过生成大量合成里程弥补人工驾驶数据的不足，构建能无缝模拟城市交通的生成式系统CitySim。

Method: 基于单一损失函数训练端到端生成模型，整合动态场景生成、遮挡推理等关键技术，支持城市级连续仿真。

Result: 在扩展版Waymo数据集上验证了长期仿真的优越真实性，实现了城市尺度的交通流模拟。

Conclusion: SceneDiffuser++为城市级交通仿真提供了首个一体化解决方案，显著提升仿真真实性和覆盖范围。

Abstract: The goal of traffic simulation is to augment a potentially limited amount of
manually-driven miles that is available for testing and validation, with a much
larger amount of simulated synthetic miles. The culmination of this vision
would be a generative simulated city, where given a map of the city and an
autonomous vehicle (AV) software stack, the simulator can seamlessly simulate
the trip from point A to point B by populating the city around the AV and
controlling all aspects of the scene, from animating the dynamic agents (e.g.,
vehicles, pedestrians) to controlling the traffic light states. We refer to
this vision as CitySim, which requires an agglomeration of simulation
technologies: scene generation to populate the initial scene, agent behavior
modeling to animate the scene, occlusion reasoning, dynamic scene generation to
seamlessly spawn and remove agents, and environment simulation for factors such
as traffic lights. While some key technologies have been separately studied in
various works, others such as dynamic scene generation and environment
simulation have received less attention in the research community. We propose
SceneDiffuser++, the first end-to-end generative world model trained on a
single loss function capable of point A-to-B simulation on a city scale
integrating all the requirements above. We demonstrate the city-scale traffic
simulation capability of SceneDiffuser++ and study its superior realism under
long simulation conditions. We evaluate the simulation quality on an augmented
version of the Waymo Open Motion Dataset (WOMD) with larger map regions to
support trip-level simulation.

</details>


### [141] [Binned semiparametric Bayesian networks](https://arxiv.org/abs/2506.21997)
*Rafael Sojo,Javier Díaz-Rozo,Concha Bielza,Pedro Larrañaga*

Main category: cs.LG

TL;DR: 该论文提出了一种新型概率半参数模型，通过数据分箱降低核密度估计的计算成本，并开发了两种条件概率分布方法，显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统非参数分布中的核密度估计计算成本高，且分箱模型常受维度灾难影响。本文旨在通过分箱技术和稀疏张量等方法解决这些问题。

Method: 提出了两种新的条件概率分布：稀疏分箱核密度估计和傅里叶核密度估计，利用稀疏张量和限制父节点数量来应对维度灾难。

Result: 实验表明，分箱半参数贝叶斯网络在结构学习和对数似然估计上与未分箱版本无显著差异，但计算速度大幅提升。

Conclusion: 分箱半参数贝叶斯网络是一种更高效且可靠的替代方案，适用于需要快速计算的场景。

Abstract: This paper introduces a new type of probabilistic semiparametric model that
takes advantage of data binning to reduce the computational cost of kernel
density estimation in nonparametric distributions. Two new conditional
probability distributions are developed for the new binned semiparametric
Bayesian networks, the sparse binned kernel density estimation and the Fourier
kernel density estimation. These two probability distributions address the
curse of dimensionality, which typically impacts binned models, by using sparse
tensors and restricting the number of parent nodes in conditional probability
calculations. To evaluate the proposal, we perform a complexity analysis and
conduct several comparative experiments using synthetic data and datasets from
the UCI Machine Learning repository. The experiments include different binning
rules, parent restrictions, grid sizes, and number of instances to get a
holistic view of the model's behavior. As a result, our binned semiparametric
Bayesian networks achieve structural learning and log-likelihood estimations
with no statistically significant differences compared to the semiparametric
Bayesian networks, but at a much higher speed. Thus, the new binned
semiparametric Bayesian networks prove to be a reliable and more efficient
alternative to their non-binned counterparts.

</details>


### [142] [GKNet: Graph Kalman Filtering and Model Inference via Model-based Deep Learning](https://arxiv.org/abs/2506.22004)
*Mohammad Sabbaqi,Riccardo Taormina,Elvin Isufi*

Main category: cs.LG

TL;DR: 提出了一种基于图感知状态空间模型的图时间序列分析方法，结合最大似然和深度学习进行参数学习与状态跟踪。


<details>
  <summary>Details</summary>
Motivation: 图时间序列分析在多个领域有重要应用，但现有方法在计算效率和模型表达能力上存在局限。

Method: 使用图诱导的随机偏微分方程建模状态方程，观测模型为图滤波后的状态采样，结合最大似然和深度学习进行推断。

Result: 模型能够有效捕捉图时间序列的时空模式，并在预测和插补等下游任务中表现良好。

Conclusion: 该模型通过图结构约束和深度学习增强，为图时间序列分析提供了可扩展且表达力强的解决方案。

Abstract: Inference tasks with time series over graphs are of importance in
applications such as urban water networks, economics, and networked
neuroscience. Addressing these tasks typically relies on identifying a
computationally affordable model that jointly captures the graph-temporal
patterns of the data. In this work, we propose a graph-aware state space model
for graph time series, where both the latent state and the observation equation
are parametric graph-induced models with a limited number of parameters that
need to be learned. More specifically, we consider the state equation to follow
a stochastic partial differential equation driven by noise over the graphs
edges accounting not only for potential edge uncertainties but also for
increasing the degrees of freedom in the latter in a tractable manner. The
graph structure conditioning of the noise dispersion allows the state variable
to deviate from the stochastic process in certain neighborhoods. The
observation model is a sampled and graph-filtered version of the state
capturing multi-hop neighboring influence. The goal is to learn the parameters
in both state and observation models from the partially observed data for
downstream tasks such as prediction and imputation. The model is inferred first
through a maximum likelihood approach that provides theoretical tractability
but is limited in expressivity and scalability. To improve on the latter, we
use the state-space formulation to build a principled deep learning
architecture that jointly learns the parameters and tracks the state in an
end-to-end manner in the spirit of Kalman neural networks.

</details>


### [143] [TROFI: Trajectory-Ranked Offline Inverse Reinforcement Learning](https://arxiv.org/abs/2506.22008)
*Alessandro Sestini,Joakim Bergdahl,Konrad Tollmar,Andrew D. Bagdanov,Linus Gisslén*

Main category: cs.LG

TL;DR: TROFI是一种离线逆强化学习方法，通过人类偏好学习奖励函数，无需预定义奖励或最优轨迹，在D4RL基准和3D游戏环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习通常需要预定义的奖励函数，但在实际应用中（如游戏开发）奖励函数可能不可用。本文旨在解决这一问题。

Method: 提出TROFI方法，首先从人类偏好中学习奖励函数，然后用其标注原始数据集，进而训练策略，无需最优轨迹。

Result: 在D4RL基准测试中，TROFI表现优于基线方法，与使用真实奖励函数的效果相当，并在3D游戏环境中验证了其有效性。

Conclusion: 研究表明，设计良好且易于学习的奖励函数对于值函数与实际未来折扣奖励的对齐至关重要，TROFI为此提供了一种有效解决方案。

Abstract: In offline reinforcement learning, agents are trained using only a fixed set
of stored transitions derived from a source policy. However, this requires that
the dataset be labeled by a reward function. In applied settings such as video
game development, the availability of the reward function is not always
guaranteed. This paper proposes Trajectory-Ranked OFfline Inverse reinforcement
learning (TROFI), a novel approach to effectively learn a policy offline
without a pre-defined reward function. TROFI first learns a reward function
from human preferences, which it then uses to label the original dataset making
it usable for training the policy. In contrast to other approaches, our method
does not require optimal trajectories. Through experiments on the D4RL
benchmark we demonstrate that TROFI consistently outperforms baselines and
performs comparably to using the ground truth reward to learn policies.
Additionally, we validate the efficacy of our method in a 3D game environment.
Our studies of the reward model highlight the importance of the reward function
in this setting: we show that to ensure the alignment of a value function to
the actual future discounted reward, it is fundamental to have a
well-engineered and easy-to-learn reward function.

</details>


### [144] [Hyper-modal Imputation Diffusion Embedding with Dual-Distillation for Federated Multimodal Knowledge Graph Completion](https://arxiv.org/abs/2506.22036)
*Ying Zhang,Yu Zhao,Xuhui Sui,Baohang Zhou,Xiangrui Cai,Li Shen,Xiaojie Yuan,Dacheng Tao*

Main category: cs.LG

TL;DR: 提出FedMKGC任务及MMFeD3-HidE框架，解决多模态知识图谱联邦学习中的模态缺失和客户端异构问题，通过嵌入恢复和双重蒸馏提升推理能力与安全性。


<details>
  <summary>Details</summary>
Motivation: 多模态知识图谱分散在不同机构中，缺乏有效的协作系统，需兼顾推理能力和传输安全性。

Method: HidE模型恢复不完整实体嵌入的完整多模态分布；MMFeD3通过逻辑和特征蒸馏在客户端与服务器间双向传递知识。

Result: 实验验证了MMFeD3-HidE的有效性、语义一致性和收敛鲁棒性。

Conclusion: FedMKGC框架成功解决了多模态联邦学习中的关键挑战，为跨机构知识协作提供了可行方案。

Abstract: With the increasing multimodal knowledge privatization requirements,
multimodal knowledge graphs in different institutes are usually decentralized,
lacking of effective collaboration system with both stronger reasoning ability
and transmission safety guarantees. In this paper, we propose the Federated
Multimodal Knowledge Graph Completion (FedMKGC) task, aiming at training over
federated MKGs for better predicting the missing links in clients without
sharing sensitive knowledge. We propose a framework named MMFeD3-HidE for
addressing multimodal uncertain unavailability and multimodal client
heterogeneity challenges of FedMKGC. (1) Inside the clients, our proposed
Hyper-modal Imputation Diffusion Embedding model (HidE) recovers the complete
multimodal distributions from incomplete entity embeddings constrained by
available modalities. (2) Among clients, our proposed Multimodal FeDerated Dual
Distillation (MMFeD3) transfers knowledge mutually between clients and the
server with logit and feature distillation to improve both global convergence
and semantic consistency. We propose a FedMKGC benchmark for a comprehensive
evaluation, consisting of a general FedMKGC backbone named MMFedE, datasets
with heterogeneous multimodal information, and three groups of constructed
baselines. Experiments conducted on our benchmark validate the effectiveness,
semantic consistency, and convergence robustness of MMFeD3-HidE.

</details>


### [145] [On the Necessity of Output Distribution Reweighting for Effective Class Unlearning](https://arxiv.org/abs/2506.20893)
*Yian Wang,Ali Ebrahimpour-Boroojeny,Hari Sundaram*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级的输出重加权遗忘方法RWFT，无需完全重新训练即可从分类器中删除特定类别，解决了现有方法在遗忘类别预测上的不足，并通过新攻击MIA-NN和新指标TV距离验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从训练模型中遗忘特定类别时无法完全模拟重新训练模型的行为，且完全重新训练成本高昂。需要一种高效且安全的方法来满足用户删除权和减少有害或偏见预测的需求。

Method: 提出RWFT方法，通过简单重新分配遗忘类别样本的预测概率质量，抵抗MIA-NN攻击，并引入基于总变差（TV）距离的新指标量化残留泄漏。

Result: 实验表明，RWFT在现有评估指标和新提出的TV指标上均与完全重新训练结果匹配，相比现有最佳方法，分别提升了2.79%和111.45%。

Conclusion: RWFT是一种高效且安全的遗忘方法，能够有效删除特定类别，并在新攻击和指标下表现出色，为未来研究提供了新方向。

Abstract: In this work, we introduce an output-reweighting unlearning method, RWFT, a
lightweight technique that erases an entire class from a trained classifier
without full retraining. Forgetting specific classes from trained models is
essential for enforcing user deletion rights and mitigating harmful or biased
predictions. The full retraining is costly and existing unlearning methods fail
to replicate the behavior of the retrained models when predicting samples from
the unlearned class. We prove this failure by designing a variant of membership
inference attacks, MIA-NN that successfully reveals the unlearned class for any
of these methods. We propose a simple redistribution of the probability mass
for the prediction on the samples in the forgotten class which is robust to
MIA-NN. We also introduce a new metric based on the total variation (TV)
distance of the prediction probabilities to quantify residual leakage to
prevent future methods from susceptibility to the new attack. Through extensive
experiments with state of the art baselines in machine unlearning, we show that
our approach matches the results of full retraining in both metrics used for
evaluation by prior work and the new metric we propose in this work. Compare to
state-of-the-art methods, we gain 2.79% in previously used metrics and 111.45%
in our new TV-based metric over the best existing method.

</details>


### [146] [UniCA: Adapting Time Series Foundation Model to General Covariate-Aware Forecasting](https://arxiv.org/abs/2506.22039)
*Lu Han,Yu Liu,Qiwen Deng,Jian Jiang,Yinbo Sun,Zhe Yu,Binfeng Wang,Xingyu Lu,Lintao Ma,Han-Jia Ye,De-Chuan Zhan*

Main category: cs.LG

TL;DR: UniCA框架通过协变量同质化和统一注意力融合机制，解决了时间序列基础模型在处理异构协变量时的局限性，提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列基础模型主要针对实值序列设计，难以处理包含异构协变量（如分类变量、多模态数据）的通用预测任务。

Method: 提出UniCA框架，包括协变量同质化和基于注意力的统一融合机制，兼容同质和异构协变量。

Result: 在单模态和多模态协变量预测基准测试中，UniCA表现出优越性能。

Conclusion: UniCA展示了协变量感知的时间序列基础模型在实际预测场景中的潜力。

Abstract: Time Series Foundation Models (TSFMs) have achieved remarkable success
through large-scale pretraining. However, their design primarily targets
real-valued series, limiting their ability to handle general forecasting tasks
involving diverse and often heterogeneous covariates--such as categorical
variables and multimodal data (e.g., images, text)--which are typically
task-specific and difficult to leverage during pretraining. To address this
gap, we propose Unified Covariate Adaptation (UniCA), a framework to bridge
TSFMs with general covariate-aware forecasting. UniCA first performs covariate
homogenization to transform heterogeneous covariates into high-level
homogeneous series representations and then fuses them via a unified
attention-based fusion mechanism. UniCA is compatible and universal for
adaptation with both homogeneous and heterogeneous covariates, incorporating
extra covariate information while preserving the generalization ability of
TSFMs.Extensive experiments on multiple unimodal and multimodal covariate-aware
forecasting benchmarks demonstrate the superiority of UniCA, highlighting the
promise of covariate-aware TSFM adaptation in real-world forecasting scenarios.
Codes are released on https://github.com/hanlu-nju/UniCA.

</details>


### [147] [GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling](https://arxiv.org/abs/2506.22049)
*Tianhao Chen,Xin Xu,Zijing Liu,Pengxiang Li,Xinyuan Song,Ajay Kumar Jaiswal,Fan Zhang,Jishan Hu,Yang Wang,Hao Chen,Shizhe Diao,Shiwei Liu,Yu Li,Yin Lu,Can Yang*

Main category: cs.LG

TL;DR: 论文针对Pre-LN Transformer中激活方差指数增长的问题，提出GPAS方法，通过缩放中间激活值但保持梯度不变来提升模型表现。


<details>
  <summary>Details</summary>
Motivation: Pre-LN Transformer在预训练中稳定且可扩展，但存在层间激活方差指数增长问题，导致残差路径主导子层输出，限制深层学习能力。

Method: 提出梯度保持激活缩放（GPAS），在缩放中间激活值的同时保持梯度不变，避免梯度消失问题。

Result: 在71M到1B不同规模的模型上实验，GPAS均带来性能提升，且适用于Sandwich-LN和DeepNorm等其他架构。

Conclusion: GPAS是一种简单有效的方法，能提升Pre-LN等架构的训练动态，具有广泛适用性。

Abstract: Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,
predominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While
being stable during pretraining and scalable to large model sizes, Pre-LN
suffers from an exponential growth in activation variance across layers,
causing the residual path to dominate over sub-layer outputs and limiting the
learning capacity of deeper layers. To mitigate this issue, we propose
Gradient-Preserving Activation Scaling (GPAS), a simple technique that can be
used in combination with existing approaches. GPAS works by scaling down the
intermediate activations while keeping their gradients unchanged. This leaves
information in the activations intact, and avoids the gradient vanishing
problem associated with gradient downscaling. Extensive experiments across
various model sizes from 71M to 1B show that GPAS achieves consistent
performance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows
promise in improving alternative architectures such as Sandwich-LN and
DeepNorm, demonstrating its versatility and potential for improving training
dynamics in a wide range of settings.

</details>


### [148] [crypto price prediction using lstm+xgboost](https://arxiv.org/abs/2506.22055)
*Mehul Gautam*

Main category: cs.LG

TL;DR: 该研究提出了一种结合LSTM和XGBoost的混合模型，用于加密货币价格预测，在多种加密货币数据集上表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 加密货币市场的波动性和复杂动态性为价格预测带来了独特挑战，需要更精准的预测模型。

Method: 采用LSTM网络捕捉历史价格数据的时间依赖性，结合XGBoost建模非线性关系（如情感分数和宏观经济指标），构建混合模型。

Result: 在比特币、以太坊等加密货币数据集上，混合模型的MAPE和MinMax RMSE指标均优于单一模型和传统方法。

Conclusion: 混合架构在金融预测中具有潜力，并能适应不同加密货币和市场环境。

Abstract: The volatility and complex dynamics of cryptocurrency markets present unique
challenges for accurate price forecasting. This research proposes a hybrid deep
learning and machine learning model that integrates Long Short-Term Memory
(LSTM) networks and Extreme Gradient Boosting (XGBoost) for cryptocurrency
price prediction. The LSTM component captures temporal dependencies in
historical price data, while XGBoost enhances prediction by modeling nonlinear
relationships with auxiliary features such as sentiment scores and
macroeconomic indicators. The model is evaluated on historical datasets of
Bitcoin, Ethereum, Dogecoin, and Litecoin, incorporating both global and
localized exchange data. Comparative analysis using Mean Absolute Percentage
Error (MAPE) and Min-Max Normalized Root Mean Square Error (MinMax RMSE)
demonstrates that the LSTM+XGBoost hybrid consistently outperforms standalone
models and traditional forecasting methods. This study underscores the
potential of hybrid architectures in financial forecasting and provides
insights into model adaptability across different cryptocurrencies and market
contexts.

</details>


### [149] [Transformers are Graph Neural Networks](https://arxiv.org/abs/2506.22084)
*Chaitanya K. Joshi*

Main category: cs.LG

TL;DR: 论文揭示了Transformer与图神经网络(GNN)的数学联系，提出Transformer可视为在完全连接图上进行消息传递的GNN，其自注意力机制捕获token间关系，且硬件效率更高。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer架构与图神经网络之间的理论联系，揭示两者在表示学习上的共性，并解释Transformer在硬件效率上的优势。

Method: 将Transformer的自注意力机制建模为全连接图上的消息传递过程，分析位置编码对图结构的提示作用，对比稀疏GNN与稠密矩阵运算的效率差异。

Result: 证明Transformer本质上是不受先验图约束的集合处理网络，其密集矩阵运算比传统GNN的稀疏消息传递更适合现代硬件加速。

Conclusion: Transformer可视为当前硬件条件下更高效的GNN变体，其成功部分源于稠密计算对硬件特性的适配。

Abstract: We establish connections between the Transformer architecture, originally
introduced for natural language processing, and Graph Neural Networks (GNNs)
for representation learning on graphs. We show how Transformers can be viewed
as message passing GNNs operating on fully connected graphs of tokens, where
the self-attention mechanism capture the relative importance of all tokens
w.r.t. each-other, and positional encodings provide hints about sequential
ordering or structure. Thus, Transformers are expressive set processing
networks that learn relationships among input elements without being
constrained by apriori graphs. Despite this mathematical connection to GNNs,
Transformers are implemented via dense matrix operations that are significantly
more efficient on modern hardware than sparse message passing. This leads to
the perspective that Transformers are GNNs currently winning the hardware
lottery.

</details>


### [150] [Learning to Solve Multi-Objective Routing Problems on Multigraphs](https://arxiv.org/abs/2506.22095)
*Filip Rydin,Attila Lischka,Jiaming Wu,Morteza Haghir Chehreghani,Balázs Kulcsár*

Main category: cs.LG

TL;DR: 该论文提出了两种基于神经网络的多元路径规划方法，针对多图环境下的多目标路由问题，并在TSP和CVRP等经典问题上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管多图环境在实际应用中非常常见，但现有的学习型方法大多忽略了多图中存在多条具有不同属性路径的情况，因此需要针对这一场景设计新的路由方法。

Method: 论文提出了两种方法：第一种直接在多图上通过自回归方式选择边来构建路径；第二种先对多图进行剪枝得到简单图，再构建路径。

Result: 实验表明，这两种方法在旅行商问题(TSP)和带容量约束的车辆路径问题(CVRP)等多种问题上都表现优异。

Conclusion: 该研究填补了多图环境下多目标路由学习方法的空白，提出的两种神经网络方法均展现出强大的性能。

Abstract: Learning-based methods for routing have gained significant attention in
recent years, both in single-objective and multi-objective contexts. However,
the multigraph setting, where multiple paths with distinct attributes can exist
between destinations, has largely been overlooked, despite its high practical
relevancy. In this paper, we introduce two neural approaches to address
multi-objective routing on multigraphs. Our first approach works directly on
the multigraph, by autoregressively selecting edges until a tour is completed.
On the other hand, our second model first prunes the multigraph into a simple
graph and then builds routes. We validate both models experimentally and find
that they demonstrate strong performance across a variety of problems,
including the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP).

</details>


### [151] [Transfer Learning for Assessing Heavy Metal Pollution in Seaports Sediments](https://arxiv.org/abs/2506.22096)
*Tin Lai,Farnaz Farid,Yueyang Kuan,Xintian Zhang*

Main category: cs.LG

TL;DR: 提出基于深度学习的模型简化重金属污染评估，解决传统PLI方法繁琐和数据稀缺问题，在澳大利亚六个港口验证效果显著。


<details>
  <summary>Details</summary>
Motivation: 传统PLI评估方法流程繁琐且数据收集困难，各国标准不一，亟需一种高效、低成本的重金属污染评估方案。

Method: 利用迁移学习开发跨域特征传递的深度学习模型，预测PLI值，解决水-沉积物领域数据稀缺问题。

Result: 在澳大利亚新南威尔士州六个港口数据测试中，MAE和MAPE分别低至0.5和0.03，性能优于基线模型2个数量级。

Conclusion: 该模型为水质预测提供创新、易用且经济的解决方案，有助于海洋生态保护和水产养殖业污染监测。

Abstract: Detecting heavy metal pollution in soils and seaports is vital for regional
environmental monitoring. The Pollution Load Index (PLI), an international
standard, is commonly used to assess heavy metal containment. However, the
conventional PLI assessment involves laborious procedures and data analysis of
sediment samples. To address this challenge, we propose a deep-learning-based
model that simplifies the heavy metal assessment process. Our model tackles the
issue of data scarcity in the water-sediment domain, which is traditionally
plagued by challenges in data collection and varying standards across nations.
By leveraging transfer learning, we develop an accurate quantitative assessment
method for predicting PLI. Our approach allows the transfer of learned features
across domains with different sets of features. We evaluate our model using
data from six major ports in New South Wales, Australia: Port Yamba, Port
Newcastle, Port Jackson, Port Botany, Port Kembla, and Port Eden. The results
demonstrate significantly lower Mean Absolute Error (MAE) and Mean Absolute
Percentage Error (MAPE) of approximately 0.5 and 0.03, respectively, compared
to other models. Our model performance is up to 2 orders of magnitude than
other baseline models. Our proposed model offers an innovative, accessible, and
cost-effective approach to predicting water quality, benefiting marine life
conservation, aquaculture, and industrial pollution monitoring.

</details>


### [152] [Earthquake Damage Grades Prediction using An Ensemble Approach Integrating Advanced Machine and Deep Learning Models](https://arxiv.org/abs/2506.22129)
*Anurag Panda,Gaurav Kumar Yadav*

Main category: cs.LG

TL;DR: 该研究利用SMOTE技术解决地震后建筑损伤等级预测中的类别不平衡问题，通过多种机器学习和深度学习方法评估模型性能，并识别关键影响因素。


<details>
  <summary>Details</summary>
Motivation: 地震后准确评估建筑损伤等级对救援和资源分配至关重要，但现有方法存在类别不平衡问题，导致模型偏向多数类，影响预测效果。

Method: 采用SMOTE技术处理类别不平衡，结合多类分类模型（如XGBoost）、深度学习模型及集成方法，并通过特征操纵实验和混淆矩阵评估性能。

Result: 研究确定了地震脆弱性的关键因素，并验证了SMOTE和多种方法在提升损伤预测效果上的有效性。

Conclusion: 通过SMOTE和多样化模型组合，可显著改善地震损伤预测的准确性，为灾后响应提供可靠依据。

Abstract: In the aftermath of major earthquakes, evaluating structural and
infrastructural damage is vital for coordinating post-disaster response
efforts. This includes assessing damage's extent and spatial distribution to
prioritize rescue operations and resource allocation. Accurately estimating
damage grades to buildings post-earthquake is paramount for effective response
and recovery, given the significant impact on lives and properties,
underscoring the urgency of streamlining relief fund allocation processes.
Previous studies have shown the effectiveness of multi-class classification,
especially XGBoost, along with other machine learning models and ensembling
methods, incorporating regularization to address class imbalance. One
consequence of class imbalance is that it may give rise to skewed models that
undervalue minority classes and give preference to the majority class. This
research deals with the problem of class imbalance with the help of the
synthetic minority oversampling technique (SMOTE). We delve into multiple
multi-class classification machine learning, deep learning models, and
ensembling methods to forecast structural damage grades. The study elucidates
performance determinants through comprehensive feature manipulation experiments
and diverse training approaches. It identifies key factors contributing to
seismic vulnerability while evaluating model performance using techniques like
the confusion matrix further to enhance understanding of the effectiveness of
earthquake damage prediction.

</details>


### [153] [Thompson Sampling-Based Learning and Control for Unknown Dynamic Systems](https://arxiv.org/abs/2506.22186)
*Kaikai Zheng,Dawei Shi,Yang Shi,Long Wang*

Main category: cs.LG

TL;DR: 提出基于再生核希尔伯特空间的参数化方法，扩展了TS在控制律学习中的应用，适用于更一般的控制空间。


<details>
  <summary>Details</summary>
Motivation: 传统TS方法依赖有限参数表示，限制了其在更一般控制空间中的应用。

Method: 使用再生核希尔伯特空间对控制律进行参数化，设计数据驱动的主动学习控制方法。

Result: 理论分析显示方法以指数速率学习控制律与闭环性能的关系，并推导了控制遗憾的上界。

Conclusion: 数值实验验证了该方法在控制未知非线性系统中的有效性。

Abstract: Thompson sampling (TS) is an effective method to explore parametric
uncertainties and can therefore be used for active learning-based controller
design. However, TS relies on finite parametric representations, which limits
its applicability to more general spaces, which are more commonly encountered
in control system design. To address this issue, this work pro poses a
parameterization method for control law learning using reproducing kernel
Hilbert spaces and designs a data-driven active learning control approach.
Specifically, the proposed method treats the control law as an element in a
function space, allowing the design of control laws without imposing
restrictions on the system structure or the form of the controller. A TS
framework is proposed in this work to explore potential optimal control laws,
and the convergence guarantees are further provided for the learning process.
Theoretical analysis shows that the proposed method learns the relationship
between control laws and closed-loop performance metrics at an exponential
rate, and the upper bound of control regret is also derived. Numerical
experiments on controlling unknown nonlinear systems validate the effectiveness
of the proposed method.

</details>


### [154] [Exploring Modularity of Agentic Systems for Drug Discovery](https://arxiv.org/abs/2506.22189)
*Laura van Weesep,Samuel Genheden,Ola Engkvist,Jens Sjölund*

Main category: cs.LG

TL;DR: 研究探讨了基于LLM的智能体系统在药物发现中的模块化问题，比较了不同模型和代理类型的性能，发现Claude-3.5-Sonnet等表现更优，代码生成代理通常优于工具调用代理，但结果高度依赖问题和模型。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLM）和智能体系统在加速药物发现与设计中的潜力，特别是系统模块化（如LLM的可替换性）这一尚未充分研究的话题。

Method: 通过案例研究，比较不同LLM（如Claude-3.5-Sonnet、GPT-4o等）以及工具调用代理与代码生成代理在化学和药物发现任务中的性能，使用LLM-as-a-judge评分。

Result: Claude-3.5-Sonnet、Claude-3.7-Sonnet和GPT-4o优于其他模型；代码生成代理平均表现更好，但结果因问题和模型而异；系统提示的影响也依赖具体问题和模型。

Conclusion: 研究表明，在药物发现领域替换语言模型需谨慎，需结合提示重新设计，未来需进一步研究智能体系统的模块化以实现稳定、可扩展的解决方案。

Abstract: Large-language models (LLMs) and agentic systems present exciting
opportunities to accelerate drug discovery and design. In this study, we
critically examine the modularity of LLM-based agentic systems for drug
discovery, i.e., whether parts of the agentic system such as the LLM are
interchangeable, a topic that has received limited attention in drug discovery
applications. We compare the performance of different large language models
(LLMs) and the effectiveness of tool-calling agents versus code-generating
agents in this domain. Our case study, comparing performance in orchestrating
tools for chemistry and drug discovery using an LLM-as-a-judge score, shows
that Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative
language models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and
Nova-Micro. Although we confirm that code-generating agents outperform the
tool-calling ones on average, we show that this is highly question and model
dependent. Furthermore, the impact of replacing system prompts is dependent on
the specific question asked and the model used, underscoring that -- even in
this particular domain -- one cannot just replace language models without
considering prompt re-engineering. Our study highlights the necessity of
further research into the modularity of agentic systems to enable the
development of stable and scalable solutions for real-world problems.

</details>


### [155] [dreaMLearning: Data Compression Assisted Machine Learning](https://arxiv.org/abs/2506.22190)
*Xiaobo Zhao,Aaron Hurst,Panagiotis Karras,Daniel E. Lucani*

Main category: cs.LG

TL;DR: 论文提出dreaMLearning框架，通过压缩数据直接学习，显著提升训练速度、降低资源消耗，且对模型性能影响极小。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习尤其是深度学习需要大量标注数据和计算资源，限制了其在资源受限场景的应用，因此需要一种高效且节省资源的学习方法。

Method: 基于熵驱动的无损压缩方法EntroGeDe，构建了dreaMLearning框架，支持多种数据类型、任务和模型架构，无需解压即可学习。

Result: 实验表明，dreaMLearning在回归和分类任务中，训练速度提升8.8倍，内存使用减少10倍，存储需求降低42%，且模型性能几乎不受影响。

Conclusion: dreaMLearning为分布式学习、联邦学习及边缘设备上的tinyML等应用提供了高效、可扩展的解决方案，拓展了机器学习在资源受限场景的可能性。

Abstract: Despite rapid advancements, machine learning, particularly deep learning, is
hindered by the need for large amounts of labeled data to learn meaningful
patterns without overfitting and immense demands for computation and storage,
which motivate research into architectures that can achieve good performance
with fewer resources. This paper introduces dreaMLearning, a novel framework
that enables learning from compressed data without decompression, built upon
Entropy-based Generalized Deduplication (EntroGeDe), an entropy-driven lossless
compression method that consolidates information into a compact set of
representative samples. DreaMLearning accommodates a wide range of data types,
tasks, and model architectures. Extensive experiments on regression and
classification tasks with tabular and image data demonstrate that dreaMLearning
accelerates training by up to 8.8x, reduces memory usage by 10x, and cuts
storage by 42%, with a minimal impact on model performance. These advancements
enhance diverse ML applications, including distributed and federated learning,
and tinyML on resource-constrained edge devices, unlocking new possibilities
for efficient and scalable learning.

</details>


### [156] [REDELEX: A Framework for Relational Deep Learning Exploration](https://arxiv.org/abs/2506.22199)
*Jakub Peleška,Gustav Šír*

Main category: cs.LG

TL;DR: 论文提出REDELEX框架，评估不同复杂度的关系深度学习模型在70多个RDB上的表现，确认RDL的优越性并分析性能影响因素。


<details>
  <summary>Details</summary>
Motivation: 关系数据库（RDB）是存储结构化信息的黄金标准，但新兴的关系深度学习（RDL）模型性能与RDB特性之间的关系缺乏研究。

Method: 提出REDELEX框架，在70多个多样化RDB上评估不同复杂度的RDL模型，并与经典方法对比。

Result: RDL表现普遍更优，性能受模型复杂度、数据库大小及结构特性影响。

Conclusion: REDELEX为RDL模型评估提供全面框架，证实其优越性并揭示关键性能因素。

Abstract: Relational databases (RDBs) are widely regarded as the gold standard for
storing structured information. Consequently, predictive tasks leveraging this
data format hold significant application promise. Recently, Relational Deep
Learning (RDL) has emerged as a novel paradigm wherein RDBs are conceptualized
as graph structures, enabling the application of various graph neural
architectures to effectively address these tasks. However, given its novelty,
there is a lack of analysis into the relationships between the performance of
various RDL models and the characteristics of the underlying RDBs.
  In this study, we present REDELEX$-$a comprehensive exploration framework for
evaluating RDL models of varying complexity on the most diverse collection of
over 70 RDBs, which we make available to the community. Benchmarked alongside
key representatives of classic methods, we confirm the generally superior
performance of RDL while providing insights into the main factors shaping
performance, including model complexity, database sizes and their structural
properties.

</details>


### [157] [EFRame: Deeper Reasoning via Exploration-Filtering-Replay Reinforcement Learning Framework](https://arxiv.org/abs/2506.22200)
*Chen Wang,Lai Wei,Yanzhi Zhang,Chenyang Shao,Zedong Dan,Weiran Huang,Yue Wang,Yuzhi Zhang*

Main category: cs.LG

TL;DR: EFRame框架通过探索-过滤-回放机制增强GRPO算法，提升强化学习在复杂推理任务中的性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: GRPO作为PPO的高效变体，在复杂推理任务中仍面临探索不足、样本效率低和不稳定的问题，限制了其性能。

Method: EFRame框架通过额外探索高质量轨迹、在线过滤低质量样本以及利用经验回放重复利用稀有但信息丰富的样本，系统性地增强GRPO。

Result: 实验表明，EFRame不仅提高了训练的鲁棒性和效率，还能解锁GRPO无法达到的更深层次推理能力。

Conclusion: EFRame通过结构化学习循环，显著提升了GRPO在复杂推理任务中的表现，并为训练样本的细粒度分析提供了可能。

Abstract: Recent advances in reinforcement learning (RL) have significantly enhanced
the reasoning capabilities of large language models (LLMs). Group Relative
Policy Optimization (GRPO), an efficient variant of PPO that lowers RL's
computational cost, still faces limited exploration, low sample efficiency and
instability, constraining its performance on complex reasoning tasks. To
address these limitations, we introduce EFRame, an Exploration-Filtering-Replay
framework that systematically augments GRPO along three critical dimensions.
EFRame performs additional rollouts to explore high-quality trajectories,
applies online filtering to eliminate low-quality samples that introduce noise
and variance, and leverages experience replay to repeatedly exploit rare but
informative samples. EFRame establishes a complete and stable learning cycle,
guiding the model through a structured transition from exploration to
convergence. Our experiments across a variety of reasoning benchmarks
demonstrate that EFRame not only improves the robustness and efficiency of
training, but also enables access to deeper reasoning capabilities that remain
unattainable under vanilla GRPO. Furthermore, EFRame enables a more
fine-grained categorization of training samples, allowing for a deeper analysis
of how different types of samples contribute to the learning process in RL. Our
code is available at https://github.com/597358816/EFRame.

</details>


### [158] [Risk-Averse Best Arm Set Identification with Fixed Budget and Fixed Confidence](https://arxiv.org/abs/2506.22253)
*Shunta Nonaga,Koji Tabata,Yuta Mizuno,Tamiki Komatsuzaki*

Main category: cs.LG

TL;DR: 该论文提出了一种新的随机多臂老虎机优化问题设置，旨在同时最大化预期奖励和最小化风险（通过均值-方差准则衡量），并设计了一个统一的元算法框架，在固定置信度和固定预算两种场景下均能高效准确地识别帕累托最优解集。


<details>
  <summary>Details</summary>
Motivation: 传统老虎机问题仅关注预期收益，而实际决策中需同时权衡收益与风险。本文旨在填补这一空白，提出一种能联合优化预期回报和风险的新问题设置，以更贴近现实决策需求。

Method: 提出统一元算法框架，采用自适应设计的置信区间（针对固定置信度和固定预算两种场景），使用相同的样本探索策略，理论保证所返回解的正确性。

Result: 理论证明框架在两种场景下的正确性，并通过合成基准测试验证：该方法在准确性和样本效率上均优于现有方法，适用于不确定环境下的风险感知决策任务。

Conclusion: 该研究为风险感知决策提供了高效解决方案，其统一框架设计兼具理论保证和实际性能优势，拓展了随机老虎机优化在复杂决策场景中的应用边界。

Abstract: Decision making under uncertain environments in the maximization of expected
reward while minimizing its risk is one of the ubiquitous problems in many
subjects. Here, we introduce a novel problem setting in stochastic bandit
optimization that jointly addresses two critical aspects of decision-making:
maximizing expected reward and minimizing associated uncertainty, quantified
via the mean-variance(MV) criterion. Unlike traditional bandit formulations
that focus solely on expected returns, our objective is to efficiently and
accurately identify the Pareto-optimal set of arms that strikes the best
trade-off between expected performance and risk. We propose a unified
meta-algorithmic framework capable of operating under both fixed-confidence and
fixed-budget regimes, achieved through adaptive design of confidence intervals
tailored to each scenario using the same sample exploration strategy. We
provide theoretical guarantees on the correctness of the returned solutions in
both settings. To complement this theoretical analysis, we conduct extensive
empirical evaluations across synthetic benchmarks, demonstrating that our
approach outperforms existing methods in terms of both accuracy and sample
efficiency, highlighting its broad applicability to risk-aware decision-making
tasks in uncertain environments.

</details>


### [159] [Projected Compression: Trainable Projection for Efficient Transformer Compression](https://arxiv.org/abs/2506.22255)
*Maciej Stefaniak,Michał Krutul,Jan Małaśnicki,Maciej Pióro,Jakub Krajewski,Sebastian Jaszczur,Marek Cygan,Kamil Adamczewski,Jan Ludziejewski*

Main category: cs.LG

TL;DR: 提出了一种名为Projected Compression的新模型压缩技术，通过投影模块减少模型权重，保持计算效率，性能优于硬剪枝方法。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型规模增大，推理时间和计算需求增加，因此需要有效的模型压缩方法以减少资源消耗。

Method: 训练可学习的投影权重并保留原始参数，随后将其合并为低维矩阵，生成压缩后的标准Transformer模型，计算效率与基础模型一致。

Result: 实验表明，该方法在高性能模型上优于硬剪枝和再训练方法，且性能随token数量增加而提升。

Conclusion: Projected Compression是一种高效且性能优越的模型压缩技术，适用于大规模语言模型。

Abstract: Large language models have steadily increased in size to achieve improved
performance; however, this growth has also led to greater inference time and
computational demands. Consequently, there is rising interest in model size
reduction methods. To address this issue, we propose Projected Compression, a
novel model compression technique, that reduces model weights by utilizing
projection modules. Specifically, we first train additional trainable
projections weights and preserve access to all the original model parameters.
Subsequently, these projections are merged into a lower-dimensional product
matrix, resulting in a reduced-size standard Transformer-based model. Unlike
alternative approaches that require additional computational overhead, our
method matches the base model's per-token computation step in FLOPs.
Experimental results show that Projected Compression outperforms the comparable
hard pruning and retraining approach on higher quality models. Moreover, the
performance margin scales well with the number of tokens.

</details>


### [160] [Score-Based Model for Low-Rank Tensor Recovery](https://arxiv.org/abs/2506.22295)
*Zhengyun Cheng,Changhao Wang,Guanwen Zhang,Yi Xu,Wei Zhou,Xiangyang Ji*

Main category: cs.LG

TL;DR: 提出了一种基于分数匹配的模型，无需预定义结构或分布假设，通过学习能量函数优化张量分解，支持张量补全和去噪，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统张量分解方法依赖预定义结构假设（如CP或Tucker分解），实际应用中难以确定最优秩和收缩规则，固定收缩规则的优化过程复杂且易损失精度。

Method: 设计神经网络学习能量函数，通过分数匹配优化联合对数概率梯度，结合块坐标下降（BCD）和平滑正则化，实现张量补全和去噪。

Result: 实验表明，该方法在稀疏张量、连续时间张量和视觉数据等多种张量类型上性能显著提升。

Conclusion: 该方法突破了传统狄拉克分布假设，能灵活建模张量与共享因子的兼容性，为多路数据分析提供了更优框架。

Abstract: Low-rank tensor decompositions (TDs) provide an effective framework for
multiway data analysis. Traditional TD methods rely on predefined structural
assumptions, such as CP or Tucker decompositions. From a probabilistic
perspective, these can be viewed as using Dirac delta distributions to model
the relationships between shared factors and the low-rank tensor. However, such
prior knowledge is rarely available in practical scenarios, particularly
regarding the optimal rank structure and contraction rules. The optimization
procedures based on fixed contraction rules are complex, and approximations
made during these processes often lead to accuracy loss. To address this issue,
we propose a score-based model that eliminates the need for predefined
structural or distributional assumptions, enabling the learning of
compatibility between tensors and shared factors. Specifically, a neural
network is designed to learn the energy function, which is optimized via score
matching to capture the gradient of the joint log-probability of tensor entries
and shared factors. Our method allows for modeling structures and distributions
beyond the Dirac delta assumption. Moreover, integrating the block coordinate
descent (BCD) algorithm with the proposed smooth regularization enables the
model to perform both tensor completion and denoising. Experimental results
demonstrate significant performance improvements across various tensor types,
including sparse and continuous-time tensors, as well as visual data.

</details>


### [161] [CoATA: Effective Co-Augmentation of Topology and Attribute for Graph Neural Networks](https://arxiv.org/abs/2506.22299)
*Tao Liu,Longlong Lin,Yunfeng Yu,Xi Ou,Youan Zhang,Zhiqiu Ye,Tao Jia*

Main category: cs.LG

TL;DR: 论文提出CoATA框架，通过双通道协同增强拓扑与属性，结合对比学习提升GNN在噪声和不完整图数据上的表现。


<details>
  <summary>Details</summary>
Motivation: 现实图中的噪声和不完整性严重降低GNN性能，现有方法仅单维度增强拓扑或属性，忽略了二者深层交互。

Method: CoATA框架：1) 传播结构信号增强节点属性 2) 将属性空间投影为二分图重构拓扑 3) 通过对比学习实现图间互校正。

Result: 在7个基准数据集上超越11种基线方法，验证了拓扑与属性协同增强的有效性。

Conclusion: CoATA通过双通道协同增强与对比学习机制，显著提升了GNN在复杂图数据中的表征学习能力。

Abstract: Graph Neural Networks (GNNs) have garnered substantial attention due to their
remarkable capability in learning graph representations. However, real-world
graphs often exhibit substantial noise and incompleteness, which severely
degrades the performance of GNNs. Existing methods typically address this issue
through single-dimensional augmentation, focusing either on refining topology
structures or perturbing node attributes, thereby overlooking the deeper
interplays between the two. To bridge this gap, this paper presents CoATA, a
dual-channel GNN framework specifically designed for the Co-Augmentation of
Topology and Attribute. Specifically, CoATA first propagates structural signals
to enrich and denoise node attributes. Then, it projects the enhanced attribute
space into a node-attribute bipartite graph for further refinement or
reconstruction of the underlying structure. Subsequently, CoATA introduces
contrastive learning, leveraging prototype alignment and consistency
constraints, to facilitate mutual corrections between the augmented and
original graphs. Finally, extensive experiments on seven benchmark datasets
demonstrate that the proposed CoATA outperforms eleven state-of-the-art
baseline methods, showcasing its effectiveness in capturing the synergistic
relationship between topology and attributes.

</details>


### [162] [Weakly-Supervised Domain Adaptation with Proportion-Constrained Pseudo-Labeling](https://arxiv.org/abs/2506.22301)
*Takumi Okuo,Shinnosuke Matsuo,Shota Harada,Kiyohito Tanaka,Ryoma Bise*

Main category: cs.LG

TL;DR: 提出一种弱监督域适应方法，利用目标域的类别比例信息，通过比例约束伪标签提升模型性能，无需额外标注。


<details>
  <summary>Details</summary>
Motivation: 医学领域中，不同机构的数据分布差异导致模型性能下降，现有域适应方法在类别比例不同时效果不佳。

Method: 基于目标域类别比例信息，对未标注数据分配比例约束伪标签，实现弱监督域适应。

Result: 在两个内窥镜数据集上优于半监督域适应方法，即使仅5%目标域标注数据，且对噪声比例标签具有鲁棒性。

Conclusion: 该方法在真实医疗场景中有效，无需额外标注即可提升跨域性能。

Abstract: Domain shift is a significant challenge in machine learning, particularly in
medical applications where data distributions differ across institutions due to
variations in data collection practices, equipment, and procedures. This can
degrade performance when models trained on source domain data are applied to
the target domain. Domain adaptation methods have been widely studied to
address this issue, but most struggle when class proportions between the source
and target domains differ. In this paper, we propose a weakly-supervised domain
adaptation method that leverages class proportion information from the target
domain, which is often accessible in medical datasets through prior knowledge
or statistical reports. Our method assigns pseudo-labels to the unlabeled
target data based on class proportion (called proportion-constrained
pseudo-labeling), improving performance without the need for additional
annotations. Experiments on two endoscopic datasets demonstrate that our method
outperforms semi-supervised domain adaptation techniques, even when 5% of the
target domain is labeled. Additionally, the experimental results with noisy
proportion labels highlight the robustness of our method, further demonstrating
its effectiveness in real-world application scenarios.

</details>


### [163] [Unfolding Generative Flows with Koopman Operators: Fast and Interpretable Sampling](https://arxiv.org/abs/2506.22304)
*Erkan Turan,Aristotelis Siozopoulos,Maks Ovsjanikov*

Main category: cs.LG

TL;DR: 本文提出了一种结合Koopman算子理论的Conditional Flow Matching (CFM)方法，通过线性化生成动态实现高效且可解释的采样。


<details>
  <summary>Details</summary>
Motivation: 传统CFM方法依赖非线性ODE数值求解，计算成本高且难以解释。现有加速方法未能揭示生成过程的结构特性。

Method: 提出无解码器的Koopman-CFM架构，在学习的可观测空间中实现线性演化，通过矩阵指数实现一步闭式采样。

Result: 在2D数据集和MNIST等真实基准上显著加速，Koopman生成器的谱特性为分析生成行为提供了结构化工具。

Conclusion: Koopman增强的流匹配方法在保持采样效率的同时提供了可解释性，为快速可解释生成建模提供了新方向。

Abstract: Conditional Flow Matching (CFM) offers a simulation-free framework for
training continuous-time generative models, bridging diffusion and flow-based
approaches. However, sampling from CFM still relies on numerically solving
non-linear ODEs which can be computationally expensive and difficult to
interpret. Recent alternatives address sampling speed via trajectory
straightening, mini-batch coupling or distillation. However, these methods
typically do not shed light on the underlying \textit{structure} of the
generative process. In this work, we propose to accelerate CFM and introduce an
interpretable representation of its dynamics by integrating Koopman operator
theory, which models non-linear flows as linear evolution in a learned space of
observables. We introduce a decoder-free Koopman-CFM architecture that learns
an embedding where the generative dynamics become linear, enabling closed-form,
one-step sampling via matrix exponentiation. This results in significant
speedups over traditional CFM as demonstrated on controlled 2D datasets and
real-world benchmarks, MNIST, Fashion-MNIST (F-MNIST), and the Toronto Face
Dataset (TFD). Unlike previous methods, our approach leads to a well-structured
Koopman generator, whose spectral properties, eigenvalues, and eigenfunctions
offer principled tools for analyzing generative behavior such as temporal
scaling, mode stability, and decomposition in Koopman latent space. By
combining sampling efficiency with analytical structure, Koopman-enhanced flow
matching offers a potential step toward fast and interpretable generative
modeling.

</details>


### [164] [Less Greedy Equivalence Search](https://arxiv.org/abs/2506.22331)
*Adiba Ejaz,Elias Bareinboim*

Main category: cs.LG

TL;DR: LGES是GES的改进版本，通过优化贪婪步骤，提高了计算速度和准确性，并支持先验假设和干预数据。


<details>
  <summary>Details</summary>
Motivation: GES算法在因果发现中存在计算成本高样本准确性的问题，需要一种更高效的改进方法。

Method: LGES通过避免在条件独立变量间插入边来优化贪婪步骤，支持先验假设和干预数据。

Result: LGES比GES快10倍，结构误差更小，且在样本极限下能恢复真实的等价类。

Conclusion: LGES在速度、准确性和鲁棒性上优于GES及其他基线方法。

Abstract: Greedy Equivalence Search (GES) is a classic score-based algorithm for causal
discovery from observational data. In the sample limit, it recovers the Markov
equivalence class of graphs that describe the data. Still, it faces two
challenges in practice: computational cost and finite-sample accuracy. In this
paper, we develop Less Greedy Equivalence Search (LGES), a variant of GES that
retains its theoretical guarantees while partially addressing these
limitations. LGES modifies the greedy step: rather than always applying the
highest-scoring insertion, it avoids edge insertions between variables for
which the score implies some conditional independence. This more targeted
search yields up to a \(10\)-fold speed-up and a substantial reduction in
structural error relative to GES. Moreover, LGES can guide the search using
prior assumptions, while correcting these assumptions when contradicted by the
data. Finally, LGES can exploit interventional data to refine the learned
observational equivalence class. We prove that LGES recovers the true
equivalence class in the sample limit from observational and interventional
data, even with misspecified prior assumptions. Experiments demonstrate that
LGES outperforms GES and other baselines in speed, accuracy, and robustness to
misspecified assumptions. Our code is available at
https://github.com/CausalAILab/lges.

</details>


### [165] [A Framework for Multi-source Privacy Preserving Epidemic Analysis](https://arxiv.org/abs/2506.22342)
*Zihan Guan,Zhiyuan Zhao,Fengwei Tian,Dung Nguyen,Payel Bhattacharjee,Ravi Tandon,B. Aditya Prakash,Anil Vullikanti*

Main category: cs.LG

TL;DR: 该论文提出了一种结合深度学习和流行病模型的框架，用于流行病预测和传播机制建模，同时整合了多种数据集（包括差分隐私保护数据），并通过合成金融数据集验证了其价值。


<details>
  <summary>Details</summary>
Motivation: 多样化的数据集在流行病学和公共卫生分析中具有重要价值，但部分数据敏感，需隐私保护。差分隐私（DP）因其强保障成为标准。本文旨在开发一个框架，整合深度学习与流行病模型，利用多源数据（含DP数据）进行预测和机制建模。

Method: 开发了一个结合深度学习与流行病模型的框架，支持多源数据（包括差分隐私保护数据）的整合，并通过合成金融数据集进行验证。

Result: 实验表明，即使使用差分隐私保护的合成金融数据集，该框架仍能显著提升流行病预测和传播机制建模的准确性。

Conclusion: 该框架成功整合了隐私保护数据与多源分析，为流行病预测和机制建模提供了新方法，证明了敏感数据在隐私保护下的实用价值。

Abstract: It is now well understood that diverse datasets provide a lot of value in key
epidemiology and public health analyses, such as forecasting and nowcasting,
development of epidemic models, evaluation and design of interventions and
resource allocation. Some of these datasets are often sensitive, and need
adequate privacy protections. There are many models of privacy, but
Differential Privacy (DP) has become a de facto standard because of its strong
guarantees, without making models about adversaries. In this paper, we develop
a framework the integrates deep learning and epidemic models to simultaneously
perform epidemic forecasting and learning a mechanistic model of epidemic
spread, while incorporating multiple datasets for these analyses, including
some with DP guarantees. We demonstrate our framework using a realistic but
synthetic financial dataset with DP; such a dataset has not been used in such
epidemic analyses. We show that this dataset provides significant value in
forecasting and learning an epidemic model, even when used with DP guarantees.

</details>


### [166] [Reinforcement Learning with Physics-Informed Symbolic Program Priors for Zero-Shot Wireless Indoor Navigation](https://arxiv.org/abs/2506.22365)
*Tao Li,Haozhe Lei,Mingsheng Yin,Yaqi Hu*

Main category: cs.LG

TL;DR: 论文提出PiPRL框架，通过符号化方法将物理先验知识融入强化学习，提升训练样本效率和泛化能力，并在室内导航任务中验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在物理控制任务中依赖人工注入物理先验知识，过程繁琐且需专业知识。本文旨在通过可解释的领域专用语言（DSL）自动提取物理先验，降低使用门槛。

Method: 提出PiPRL框架：采用分层模块化神经符号集成，高层符号程序接收神经感知模块的语义特征，编码物理先验并指导底层神经控制器的强化学习过程。

Result: 实验表明PiPRL显著优于纯符号或神经策略，借助程序化归纳偏置减少26%以上训练时间，在室内导航任务中表现优异。

Conclusion: 符号化物理先验与神经控制的结合能有效提升RL性能，PiPRL为物理约束任务提供了一种可解释、高效的解决方案。

Abstract: When using reinforcement learning (RL) to tackle physical control tasks,
inductive biases that encode physics priors can help improve sample efficiency
during training and enhance generalization in testing. However, the current
practice of incorporating these helpful physics-informed inductive biases
inevitably runs into significant manual labor and domain expertise, making them
prohibitive for general users. This work explores a symbolic approach to
distill physics-informed inductive biases into RL agents, where the physics
priors are expressed in a domain-specific language (DSL) that is human-readable
and naturally explainable. Yet, the DSL priors do not translate directly into
an implementable policy due to partial and noisy observations and additional
physical constraints in navigation tasks. To address this gap, we develop a
physics-informed program-guided RL (PiPRL) framework with applications to
indoor navigation. PiPRL adopts a hierarchical and modularized neuro-symbolic
integration, where a meta symbolic program receives semantically meaningful
features from a neural perception module, which form the bases for symbolic
programming that encodes physics priors and guides the RL process of a
low-level neural controller. Extensive experiments demonstrate that PiPRL
consistently outperforms purely symbolic or neural policies and reduces
training time by over 26% with the help of the program-based inductive biases.

</details>


### [167] [Sheaf-Based Decentralized Multimodal Learning for Next-Generation Wireless Communication Systems](https://arxiv.org/abs/2506.22374)
*Abdulmomen Ghalkha,Zhuojun Tian,Chaouki Ben Issaid,Mehdi Bennis*

Main category: cs.LG

TL;DR: 提出Sheaf-DMFL框架，利用层理论解决多模态联邦学习中的异构性问题，并通过注意力机制增强模态间相关性学习。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习算法仅适用于单模态数据且要求模型结构一致，无法充分利用多模态数据的丰富信息，限制了在现实多样化场景中的应用。

Method: Sheaf-DMFL框架允许客户端使用不同模态的本地特征编码器，通过层理论建模任务层间的内在关联；进一步提出Sheaf-DMFL-Att算法，引入注意力机制捕捉模态间相关性。

Result: 在真实无线通信场景（链路阻塞预测和毫米波波束成形）的仿真中，所提算法在异构系统中表现出优越性能。

Conclusion: Sheaf-DMFL系列算法通过层结构和注意力机制有效解决了多模态联邦学习的异构协作问题，为复杂通信系统提供了更智能的决策支持。

Abstract: In large-scale communication systems, increasingly complex scenarios require
more intelligent collaboration among edge devices collecting various multimodal
sensory data to achieve a more comprehensive understanding of the environment
and improve decision-making accuracy. However, conventional federated learning
(FL) algorithms typically consider unimodal datasets, require identical model
architectures, and fail to leverage the rich information embedded in multimodal
data, limiting their applicability to real-world scenarios with diverse
modalities and varying client capabilities. To address this issue, we propose
Sheaf-DMFL, a novel decentralized multimodal learning framework leveraging
sheaf theory to enhance collaboration among devices with diverse modalities.
Specifically, each client has a set of local feature encoders for its different
modalities, whose outputs are concatenated before passing through a
task-specific layer. While encoders for the same modality are trained
collaboratively across clients, we capture the intrinsic correlations among
clients' task-specific layers using a sheaf-based structure. To further enhance
learning capability, we propose an enhanced algorithm named Sheaf-DMFL-Att,
which tailors the attention mechanism within each client to capture
correlations among different modalities. A rigorous convergence analysis of
Sheaf-DMFL-Att is provided, establishing its theoretical guarantees. Extensive
simulations are conducted on real-world link blockage prediction and mmWave
beamforming scenarios, demonstrate the superiority of the proposed algorithms
in such heterogeneous wireless communication systems.

</details>


### [168] [Probabilistic Optimality for Inference-time Scaling](https://arxiv.org/abs/2506.22376)
*Youkang Wang,Jian Wang,Rubing Chen,Xiao-Yong Wei,Qing Li*

Main category: cs.LG

TL;DR: 本文提出了一种概率框架OptScale，用于优化大语言模型推理时的采样效率，通过动态确定最佳采样数量，显著降低计算开销并保持高性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型推理时扩展方法多依赖启发式策略，缺乏理论依据。本文旨在填补这一空白，提供一个有理论基础的推理时扩展方法。

Method: 提出一个概率框架，假设并行采样独立同分布，推导出达到目标性能所需样本数的理论下限，并开发了动态确定最佳数量的算法OptScale。

Result: 在多个数学推理基准测试中，OptScale显著减少了采样开销，同时保持或优于现有最佳推理性能。

Conclusion: 本文不仅提供了推理时扩展的理论基础，还提出了一个实用的解决方案，为大语言模型在复杂推理任务中的高效部署填补了关键空白。

Abstract: Inference-time scaling has emerged as a powerful technique for enhancing the
reasoning performance of Large Language Models (LLMs). However, existing
approaches often rely on heuristic strategies for parallel sampling, lacking a
principled foundation. To address this gap, we propose a probabilistic
framework that formalizes the optimality of inference-time scaling under the
assumption that parallel samples are independently and identically distributed
(i.i.d.), and where the Best-of-N selection strategy follows a probability
distribution that can be estimated. Within this framework, we derive a
theoretical lower bound on the required number of samples to achieve a target
performance level, providing the first principled guidance for
compute-efficient scaling. Leveraging this insight, we develop
\textsc{OptScale}, a practical algorithm that dynamically determines the
optimal number of sampled responses. \textsc{OptScale} employs a language
model-based predictor to estimate probabilistic prior parameters, enabling the
decision of the minimal number of samples needed that satisfy predefined
performance thresholds and confidence levels. Extensive experiments on
mathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC)
demonstrate that \textsc{OptScale} significantly reduces sampling overhead
while remaining better or on par with state-of-the-art reasoning performance.
Our work offers both a theoretical foundation and a practical solution for
principled inference-time scaling, addressing a critical gap in the efficient
deployment of LLMs for complex reasoning.

</details>


### [169] [Towards Distributed Neural Architectures](https://arxiv.org/abs/2506.22389)
*Aditya Cowsik,Tianyu He,Andrey Gromov*

Main category: cs.LG

TL;DR: 论文提出了一种分布式神经架构（DNA），通过动态路由实现模块化计算，在视觉和语言任务中表现优异，并能学习计算效率与参数共享。


<details>
  <summary>Details</summary>
Motivation: 旨在探索一种灵活的神经网络架构，能够根据输入内容动态选择计算路径，同时提升计算效率和参数利用率。

Method: 采用模块化设计（如Transformer、MLP等）和动态路由器，允许输入token或patch以任意顺序通过不同模块，并通过端到端学习优化计算路径。

Result: DNA在性能上媲美密集基线模型，并能学习到高效的计算分配和参数共享模式，路径选择呈现幂律分布，部分模块表现出专业化特性。

Conclusion: DNA展示了动态计算路径的潜力，能够以可解释的方式分配计算资源，为高效灵活的模型设计提供了新方向。

Abstract: We introduce and train distributed neural architectures (DNA) in vision and
language domains. DNAs are initialized with a proto-architecture that consists
of (transformer, MLP, attention, etc.) modules and routers. Any token (or
patch) can traverse any series of modules in any order. DNAs are a natural
generalization of the sparse methods such as Mixture-of-Experts,
Mixture-of-Depths, parameter sharing, etc. Computation and communication
patterns of DNA modules are learnt end-to-end during training and depend on the
content and context of each token (or patch). These patterns can be shaped by
further requirements added to the optimization objective such as compute/memory
efficiency or load balancing. We empirically show that (i) trained DNAs are
competitive with the dense baselines in both domains and (ii) compute
efficiency/parameter sharing can be learnt from data. Next, we analyze the
emergent connectivity and computation patterns in the trained DNAs. We find
that the paths that tokens take through the models are themselves distributed
according to a power-law. We show that some paths (or, equivalently, groups of
modules) show emergent specialization. Finally, we demonstrate that models
learn to allocate compute and active parameters in an interpretable way.

</details>


### [170] [Multi-View Contrastive Learning for Robust Domain Adaptation in Medical Time Series Analysis](https://arxiv.org/abs/2506.22393)
*YongKyung Oh,Alex Bui*

Main category: cs.LG

TL;DR: 提出一种基于多视角对比学习的新框架，整合时间模式、导数动态和频域特征，显著提升医疗时间序列数据的跨域适应性能。


<details>
  <summary>Details</summary>
Motivation: 医疗时间序列数据存在复杂的时间依赖性和动态分布变化，现有方法难以全面捕捉时序动态，限制了跨域适应的鲁棒性。

Method: 使用独立编码器和分层融合机制，通过多视角对比学习整合时序、导数动态和频域特征，学习跨域可迁移的特征不变表示。

Result: 在EEG、ECG和EMG等多样医疗数据集上，该方法显著优于当前最先进的迁移学习方法。

Conclusion: 该框架提升了机器学习模型的鲁棒性和泛化能力，为医疗场景中可靠AI系统的部署提供了实用路径。

Abstract: Adapting machine learning models to medical time series across different
domains remains a challenge due to complex temporal dependencies and dynamic
distribution shifts. Current approaches often focus on isolated feature
representations, limiting their ability to fully capture the intricate temporal
dynamics necessary for robust domain adaptation. In this work, we propose a
novel framework leveraging multi-view contrastive learning to integrate
temporal patterns, derivative-based dynamics, and frequency-domain features.
Our method employs independent encoders and a hierarchical fusion mechanism to
learn feature-invariant representations that are transferable across domains
while preserving temporal coherence. Extensive experiments on diverse medical
datasets, including electroencephalogram (EEG), electrocardiogram (ECG), and
electromyography (EMG) demonstrate that our approach significantly outperforms
state-of-the-art methods in transfer learning tasks. By advancing the
robustness and generalizability of machine learning models, our framework
offers a practical pathway for deploying reliable AI systems in diverse
healthcare settings.

</details>


### [171] [Exploration from a Primal-Dual Lens: Value-Incentivized Actor-Critic Methods for Sample-Efficient Online RL](https://arxiv.org/abs/2506.22401)
*Tong Yang,Bo Dai,Lin Xiao,Yuejie Chi*

Main category: cs.LG

TL;DR: 该论文提出了一种基于对偶优化的乐观正则化方法，通过新的价值激励演员-评论家（VAC）算法，有效平衡探索与利用，并在理论和实践中展示了其优越性。


<details>
  <summary>Details</summary>
Motivation: 在线强化学习在复杂函数逼近（如Transformer和深度神经网络）中应用广泛，但探索与利用的平衡仍是一个长期挑战。缺乏高效且理论保障的实用方案是主要动机。

Method: 论文从对偶优化视角重新解读乐观原则，提出价值激励演员-评论家（VAC）方法，通过单一易优化目标整合探索与利用，提升状态-动作估计的价值函数。

Result: VAC方法在线性马尔可夫决策过程（MDP）中具有接近最优的遗憾保证，适用于有限和无限时间范围，并可推广到一般函数逼近场景。

Conclusion: 该研究通过新颖的对偶优化视角和VAC方法，为强化学习中的探索与利用问题提供了理论支持的高效解决方案，具有广泛的应用潜力。

Abstract: Online reinforcement learning (RL) with complex function approximations such
as transformers and deep neural networks plays a significant role in the modern
practice of artificial intelligence. Despite its popularity and importance,
balancing the fundamental trade-off between exploration and exploitation
remains a long-standing challenge; in particular, we are still in lack of
efficient and practical schemes that are backed by theoretical performance
guarantees. Motivated by recent developments in exploration via optimistic
regularization, this paper provides an interpretation of the principle of
optimism through the lens of primal-dual optimization. From this fresh
perspective, we set forth a new value-incentivized actor-critic (VAC) method,
which optimizes a single easy-to-optimize objective integrating exploration and
exploitation -- it promotes state-action and policy estimates that are both
consistent with collected data transitions and result in higher value
functions. Theoretically, the proposed VAC method has near-optimal regret
guarantees under linear Markov decision processes (MDPs) in both finite-horizon
and infinite-horizon settings, which can be extended to the general function
approximation setting under appropriate assumptions.

</details>


### [172] [ARMOR: Robust Reinforcement Learning-based Control for UAVs under Physical Attacks](https://arxiv.org/abs/2506.22423)
*Pritam Dash,Ethan Chan,Nathan P. Lawrence,Karthik Pattabiraman*

Main category: cs.LG

TL;DR: ARMOR是一种抗攻击的强化学习控制器，通过两阶段训练框架学习无人机物理状态的鲁棒潜在表示，以应对传感器攻击。


<details>
  <summary>Details</summary>
Motivation: 无人机依赖的传感器易受物理攻击（如GPS欺骗），现有安全强化学习方法无法有效应对此类攻击。

Method: ARMOR采用两阶段训练框架：第一阶段利用特权攻击信息训练教师编码器生成攻击感知潜在状态；第二阶段通过监督学习训练学生编码器，仅使用历史传感器数据近似教师潜在状态。

Result: ARMOR在实验中优于传统方法，确保无人机安全，并提升对未见攻击的泛化能力，同时降低训练成本。

Conclusion: ARMOR通过鲁棒潜在表示学习，实现了无人机在对抗性传感器攻击下的安全操作。

Abstract: Unmanned Aerial Vehicles (UAVs) depend on onboard sensors for perception,
navigation, and control. However, these sensors are susceptible to physical
attacks, such as GPS spoofing, that can corrupt state estimates and lead to
unsafe behavior. While reinforcement learning (RL) offers adaptive control
capabilities, existing safe RL methods are ineffective against such attacks. We
present ARMOR (Adaptive Robust Manipulation-Optimized State Representations),
an attack-resilient, model-free RL controller that enables robust UAV operation
under adversarial sensor manipulation. Instead of relying on raw sensor
observations, ARMOR learns a robust latent representation of the UAV's physical
state via a two-stage training framework. In the first stage, a teacher
encoder, trained with privileged attack information, generates attack-aware
latent states for RL policy training. In the second stage, a student encoder is
trained via supervised learning to approximate the teacher's latent states
using only historical sensor data, enabling real-world deployment without
privileged information. Our experiments show that ARMOR outperforms
conventional methods, ensuring UAV safety. Additionally, ARMOR improves
generalization to unseen attacks and reduces training cost by eliminating the
need for iterative adversarial training.

</details>


### [173] [CLoVE: Personalized Federated Learning through Clustering of Loss Vector Embeddings](https://arxiv.org/abs/2506.22427)
*Randeep Bhatia,Nikos Papadis,Murali Kodialam,TV Lakshman,Sayak Chakrabarty*

Main category: cs.LG

TL;DR: CLoVE是一种新颖的聚类联邦学习算法，通过损失向量嵌入实现客户端的自然聚类，无需最优初始化，适用于监督和无监督场景。


<details>
  <summary>Details</summary>
Motivation: 在聚类联邦学习中，客户端根据数据分布自然分组，但识别这些分组具有挑战性。CLoVE旨在解决这一问题，通过损失模式差异实现高效聚类。

Method: CLoVE利用客户端数据的模型损失生成嵌入，基于损失值的相似性迭代分离不同集群，并通过联邦聚合优化集群特定模型。

Result: 理论证明CLoVE能高概率单轮准确恢复集群，并在线性设置中指数级快速收敛。实验显示其在多种非IID设置下实现高精度集群恢复和模型性能。

Conclusion: CLoVE在聚类联邦学习领域表现出色，具有鲁棒性、广泛适用性，并在实际应用中展现出优越性能。

Abstract: We propose CLoVE (Clustering of Loss Vector Embeddings), a novel algorithm
for Clustered Federated Learning (CFL). In CFL, clients are naturally grouped
into clusters based on their data distribution. However, identifying these
clusters is challenging, as client assignments are unknown. CLoVE utilizes
client embeddings derived from model losses on client data, and leverages the
insight that clients in the same cluster share similar loss values, while those
in different clusters exhibit distinct loss patterns. Based on these
embeddings, CLoVE is able to iteratively identify and separate clients from
different clusters and optimize cluster-specific models through federated
aggregation. Key advantages of CLoVE over existing CFL algorithms are (1) its
simplicity, (2) its applicability to both supervised and unsupervised settings,
and (3) the fact that it eliminates the need for near-optimal model
initialization, which makes it more robust and better suited for real-world
applications. We establish theoretical convergence bounds, showing that CLoVE
can recover clusters accurately with high probability in a single round and
converges exponentially fast to optimal models in a linear setting. Our
comprehensive experiments comparing with a variety of both CFL and generic
Personalized Federated Learning (PFL) algorithms on different types of datasets
and an extensive array of non-IID settings demonstrate that CLoVE achieves
highly accurate cluster recovery in just a few rounds of training, along with
state-of-the-art model accuracy, across a variety of both supervised and
unsupervised PFL tasks.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [174] [Modification of a Numerical Method Using FIR Filters in a Time-dependent SIR Model for COVID-19](https://arxiv.org/abs/2506.21739)
*Felipe Rogério Pimentel,Rafael Gustavo Alves*

Main category: stat.ML

TL;DR: 该论文提出对Chen等人的算法进行微小修改，通过调整FIR滤波器阶数和正则化参数，改进COVID-19感染和康复人数的预测精度，并在巴西米纳斯吉拉斯州的初期疫情数据中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 在COVID-19疫情初期缺乏疫苗的情况下，准确预测感染和康复人数对防控至关重要。Chen等人提出的基于FIR滤波器和岭回归的方法存在优化空间，本文旨在通过算法改进提升预测精度。

Method: 采用时间离散SIR模型构建FIR滤波器，通过修改Chen等人的岭回归算法（调整滤波器阶数和正则化参数）优化系数估计，并应用于巴西米纳斯吉拉斯州的疫情数据预测。

Result: 改进后的算法在部分模拟中取得了比原算法更低的预测误差，表明参数调整能有效提升模型对真实数据的拟合度。

Conclusion: 通过特定参数调整的FIR滤波器改进算法，在COVID-19疫情初期数据预测中表现更优，验证了该方法在流行病建模中的可优化性。

Abstract: Authors Yi-Cheng Chen, Ping-En Lu, Cheng-Shang Chang, and Tzu-Hsuan Liu use
the Finite Impulse Response (FIR) linear system filtering method to track and
predict the number of people infected and recovered from COVID-19, in a
pandemic context in which there was still no vaccine and the only way to avoid
contagion was isolation. To estimate the coefficients of these FIR filters,
Chen et al. used machine learning methods through a classical optimization
problem with regularization (ridge regression). These estimated coefficients
are called ridge coefficients. The epidemic mathematical model adopted by these
researchers to formulate the FIR filters is the time-dependent discrete SIR. In
this paper, we propose a small modification to the algorithm of Chen et al. to
obtain the ridge coefficients. We then used this modified algorithm to track
and predict the number of people infected and recovered from COVID-19 in the
state of Minas Gerais/Brazil, within a prediction window, during the initial
period of the pandemic. We also compare the predicted data with the respective
real data to check how good the approximation is. In the modified algorithm, we
set values for the FIR filter orders and for the regularization parameters,
both different from the respective values defined by Chen et al. in their
algorithm. In this context, the numerical results obtained by the modified
algorithm in some simulations present better approximation errors compared to
the respective approximation errors presented by the algorithm of Chen et al.

</details>


### [175] [Critically-Damped Higher-Order Langevin Dynamics](https://arxiv.org/abs/2506.21741)
*Benjamin Sterling,Chad Gueli,Mónica F. Bugallo*

Main category: stat.ML

TL;DR: 该论文提出了一种将临界阻尼概念应用于任意阶高阶朗之万动力学（HOLD）的新方法，扩展了当前最先进的扩散模型。


<details>
  <summary>Details</summary>
Motivation: 尽管临界阻尼已在二阶和三阶朗之万动力学（CLD和TOLD++）中成功应用，但尚未推广到任意阶动力学。本文旨在填补这一空白。

Method: 通过引入系统分析中的临界阻尼概念，对高阶朗之万动力学（HOLD）进行泛化。

Result: 提出了一种新的扩散方法，将临界阻尼应用于任意阶动力学，扩展了HOLD的能力。

Conclusion: 该研究为生成式AI方法开辟了新方向，展示了临界阻尼在高阶扩散模型中的潜力。

Abstract: Denoising Diffusion Probabilistic Models represent an entirely new class of
generative AI methods that have yet to be fully explored. Critical damping has
been successfully introduced in Critically-Damped Langevin Dynamics (CLD) and
Critically-Damped Third-Order Langevin Dynamics (TOLD++), but has not yet been
applied to dynamics of arbitrary order. The proposed line of work generalizes
Higher-Order Langevin Dynamics (HOLD), a recent state-of-the-art diffusion
method, by introducing the concept of critical damping from systems analysis.

</details>


### [176] [TADA: Improved Diffusion Sampling with Training-free Augmented Dynamics](https://arxiv.org/abs/2506.21757)
*Tianrong Chen,Huangjie Zheng,David Berthelot,Jiatao Gu,Josh Susskind,Shuangfei Zhai*

Main category: stat.ML

TL;DR: 该论文提出了一种新的扩散模型采样方法，通过高维初始噪声和ODE求解器，显著提升采样速度，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成高保真图像方面表现出色，但采样效率低下。本文旨在提出一种更快的采样方法，同时不牺牲生成质量。

Method: 采用高维初始噪声和训练无关的ODE求解器，通过动量动力学和SDE特性优化采样过程，支持通过超参数控制细节水平。

Result: 新方法在ImageNet512上比当前最优解快186%，并在EDM、EDM2和Stable-Diffusion 3等模型上表现优异。

Conclusion: 该方法高效且灵活，适用于多种扩散模型，为快速高质量图像生成提供了新思路。

Abstract: Diffusion models have demonstrated exceptional capabilities in generating
high-fidelity images but typically suffer from inefficient sampling. Many
solver designs and noise scheduling strategies have been proposed to
dramatically improve sampling speeds. In this paper, we introduce a new
sampling method that is up to $186\%$ faster than the current state of the art
solver for comparative FID on ImageNet512. This new sampling method is
training-free and uses an ordinary differential equation (ODE) solver. The key
to our method resides in using higher-dimensional initial noise, allowing to
produce more detailed samples with less function evaluations from existing
pretrained diffusion models. In addition, by design our solver allows to
control the level of detail through a simple hyper-parameter at no extra
computational cost. We present how our approach leverages momentum dynamics by
establishing a fundamental equivalence between momentum diffusion models and
conventional diffusion models with respect to their training paradigms.
Moreover, we observe the use of higher-dimensional noise naturally exhibits
characteristics similar to stochastic differential equations (SDEs). Finally,
we demonstrate strong performances on a set of representative pretrained
diffusion models, including EDM, EDM2, and Stable-Diffusion 3, which cover
models in both pixel and latent spaces, as well as class and text conditional
settings. The code is available at https://github.com/apple/ml-tada.

</details>


### [177] [Classification with Reject Option: Distribution-free Error Guarantees via Conformal Prediction](https://arxiv.org/abs/2506.21802)
*Johan Hallberg Szabadváry,Tuwe Löfström,Ulf Johansson,Cecilia Sönströd,Ernst Ahlberg,Lars Carlsson*

Main category: stat.ML

TL;DR: 该论文通过共形预测（CP）框架，为二分类问题中的拒绝选项提供了理论保证和误差率分析，展示了误差与拒绝率之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型即使在可能出错时也会做出预测，这在实际应用中导致信任问题。为了解决这一问题，论文探讨了带有拒绝选项的机器学习方法，即在预测可能错误时选择不预测。

Method: 论文将共形预测（CP）框架应用于二分类问题，通过生成包含零个、一个或两个标签的预测集，并将单例预测作为有效预测，从而构建带有拒绝选项的分类器。

Result: 论文提供了误差率的理论保证和有限样本估计，并通过数值示例展示了不同共形预测设置下的误差率，包括完全共形预测和离线批量归纳共形预测。误差-拒绝曲线展示了误差率与拒绝率之间的权衡。

Conclusion: 共形预测为带有拒绝选项的二分类问题提供了理论支持和实用工具，帮助用户在实际应用中设定可接受的误差率或拒绝率。

Abstract: Machine learning (ML) models always make a prediction, even when they are
likely to be wrong. This causes problems in practical applications, as we do
not know if we should trust a prediction. ML with reject option addresses this
issue by abstaining from making a prediction if it is likely to be incorrect.
In this work, we formalise the approach to ML with reject option in binary
classification, deriving theoretical guarantees on the resulting error rate.
This is achieved through conformal prediction (CP), which produce prediction
sets with distribution-free validity guarantees. In binary classification, CP
can output prediction sets containing exactly one, two or no labels. By
accepting only the singleton predictions, we turn CP into a binary classifier
with reject option.
  Here, CP is formally put in the framework of predicting with reject option.
We state and prove the resulting error rate, and give finite sample estimates.
Numerical examples provide illustrations of derived error rate through several
different conformal prediction settings, ranging from full conformal prediction
to offline batch inductive conformal prediction. The former has a direct link
to sharp validity guarantees, whereas the latter is more fuzzy in terms of
validity guarantees but can be used in practice. Error-reject curves illustrate
the trade-off between error rate and reject rate, and can serve to aid a user
to set an acceptable error rate or reject rate in practice.

</details>


### [178] [Thompson Sampling in Function Spaces via Neural Operators](https://arxiv.org/abs/2506.21894)
*Rafael Oliveira,Xuesong Wang,Kian Ming A. Chai,Edwin V. Bonilla*

Main category: stat.ML

TL;DR: 提出了一种基于Thompson采样的函数空间优化方法，利用神经算子替代高成本操作，避免显式不确定性量化，并在理论和实验中验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 在函数空间优化问题中，目标函数依赖于未知算子的输出，而算子查询（如高保真模拟器）成本高昂。传统方法难以高效处理此类问题。

Method: 采用样本后优化策略，使用神经算子作为高斯过程的近似样本，避免显式不确定性量化，并基于无限维高斯过程理论提供收敛保证。

Result: 在涉及偏微分方程和非线性算子的功能优化任务中，该方法在样本效率和性能上优于现有基线。

Conclusion: 该方法通过神经算子替代和样本优化策略，显著提升了高成本算子场景下的优化效率，具有理论和实践优势。

Abstract: We propose an extension of Thompson sampling to optimization problems over
function spaces where the objective is a known functional of an unknown
operator's output. We assume that functional evaluations are inexpensive, while
queries to the operator (such as running a high-fidelity simulator) are costly.
Our algorithm employs a sample-then-optimize approach using neural operator
surrogates. This strategy avoids explicit uncertainty quantification by
treating trained neural operators as approximate samples from a Gaussian
process. We provide novel theoretical convergence guarantees, based on Gaussian
processes in the infinite-dimensional setting, under minimal assumptions. We
benchmark our method against existing baselines on functional optimization
tasks involving partial differential equations and other nonlinear
operator-driven phenomena, demonstrating improved sample efficiency and
competitive performance.

</details>


### [179] [Hybrid Generative Modeling for Incomplete Physics: Deep Grey-Box Meets Optimal Transport](https://arxiv.org/abs/2506.22204)
*Gurjeet Sangra Singh,Maciej Falkiewicz,Alexandros Kalousis*

Main category: stat.ML

TL;DR: 该论文提出了一种结合深度灰箱建模和最优传输（OT）方法的混合生成模型，用于增强不完整物理模型，解决真实数据生成过程与物理模型分布不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的许多系统仅通过近似方程描述，导致物理模型与真实数据生成过程（DGP）存在差异。论文旨在利用有限且未配对的数据，结合理论驱动和数据驱动的方法，填补已知物理模型的不足。

Method: 论文提出了一种新颖的混合生成模型方法，结合深度灰箱建模和最优传输（OT）方法，在数据空间中实现OT映射，同时最小化源分布失真，确保物理参数的正确使用。

Result: 实验结果表明，该方法在生成任务和模型透明度方面表现出色，能够准确学习系统动力学并保持可解释性。

Conclusion: 该方法通过结合物理基础的归纳偏置和最优传输技术，有效解决了未配对数据问题，同时保持了模型的透明度和可解释性，为学习物理动力学提供了详细见解。

Abstract: Physics phenomena are often described by ordinary and/or partial differential
equations (ODEs/PDEs), and solved analytically or numerically. Unfortunately,
many real-world systems are described only approximately with missing or
unknown terms in the equations. This makes the distribution of the physics
model differ from the true data-generating process (DGP). Using limited and
unpaired data between DGP observations and the imperfect model simulations, we
investigate this particular setting by completing the known-physics model,
combining theory-driven models and data-driven to describe the shifted
distribution involved in the DGP. We present a novel hybrid generative model
approach combining deep grey-box modelling with Optimal Transport (OT) methods
to enhance incomplete physics models. Our method implements OT maps in data
space while maintaining minimal source distribution distortion, demonstrating
superior performance in resolving the unpaired problem and ensuring correct
usage of physics parameters. Unlike black-box alternatives, our approach
leverages physics-based inductive biases to accurately learn system dynamics
while preserving interpretability through its domain knowledge foundation.
Experimental results validate our method's effectiveness in both generation
tasks and model transparency, offering detailed insights into learned physics
dynamics.

</details>


### [180] [Uncovering smooth structures in single-cell data with PCS-guided neighbor embeddings](https://arxiv.org/abs/2506.22228)
*Rong Ma,Xi Li,Jingyuan Hu,Bin Yu*

Main category: stat.ML

TL;DR: 该论文提出了一种名为NESS的新方法，通过改进邻居嵌入算法（如t-SNE和UMAP）的稳定性和可解释性，以更准确地捕捉单细胞数据中的连续细胞状态转换。


<details>
  <summary>Details</summary>
Motivation: 单细胞测序技术虽然能揭示细胞状态转换，但现有邻居嵌入算法（如t-SNE和UMAP）在低维嵌入时易产生失真和误导性结果。现有评估标准主要关注离散细胞类型分离，而动态建模方法则依赖强假设。

Method: 基于PCS框架，论文通过实证分析、模拟和理论评估了主流邻居嵌入算法的缺陷，并提出了NESS方法，利用算法稳定性改进低维表示，支持平滑生物结构的鲁棒推断。

Result: NESS在六种单细胞数据集（包括多能干细胞分化、类器官发育等）中表现一致，能识别过渡/稳定细胞状态并量化发育中的转录动态。

Conclusion: NESS为单细胞数据分析提供了稳定性指标和高效流程，显著提升了连续生物学过程的解析能力。

Abstract: Single-cell sequencing is revolutionizing biology by enabling detailed
investigations of cell-state transitions. Many biological processes unfold
along continuous trajectories, yet it remains challenging to extract smooth,
low-dimensional representations from inherently noisy, high-dimensional
single-cell data. Neighbor embedding (NE) algorithms, such as t-SNE and UMAP,
are widely used to embed high-dimensional single-cell data into low dimensions.
But they often introduce undesirable distortions, resulting in misleading
interpretations. Existing evaluation methods for NE algorithms primarily focus
on separating discrete cell types rather than capturing continuous cell-state
transitions, while dynamic modeling approaches rely on strong assumptions about
cellular processes and specialized data. To address these challenges, we build
on the Predictability-Computability-Stability (PCS) framework for reliable and
reproducible data-driven discoveries. First, we systematically evaluate popular
NE algorithms through empirical analysis, simulation, and theory, and reveal
their key shortcomings, such as artifacts and instability. We then introduce
NESS, a principled and interpretable machine learning approach to improve NE
representations by leveraging algorithmic stability and to enable robust
inference of smooth biological structures. NESS offers useful concepts,
quantitative stability metrics, and efficient computational workflows to
uncover developmental trajectories and cell-state transitions in single-cell
data. Finally, we apply NESS to six single-cell datasets, spanning pluripotent
stem cell differentiation, organoid development, and multiple tissue-specific
lineage trajectories. Across these diverse contexts, NESS consistently yields
useful biological insights, such as identification of transitional and stable
cell states and quantification of transcriptional dynamics during development.

</details>


### [181] [Optimal Estimation of Watermark Proportions in Hybrid AI-Human Texts](https://arxiv.org/abs/2506.22343)
*Xiang Li,Garrett Wen,Weiqing He,Jiayuan Wu,Qi Long,Weijie J. Su*

Main category: stat.ML

TL;DR: 该论文提出了一种基于关键统计量的混合模型方法，用于准确估计混合来源文本中的水印比例，解决了现有水印方案在混合文本中的识别问题。


<details>
  <summary>Details</summary>
Motivation: 现有水印技术主要关注整篇文本的水印检测，但在实际应用中，文本往往是人类书写和水印内容的混合。因此，需要一种方法来准确估计混合文本中的水印比例。

Method: 论文将问题建模为基于关键统计量的混合模型参数估计问题，针对使用连续关键统计量的水印方法，提出了高效的估计器，并推导了极小极大下界。

Result: 实验表明，所提出的估计器在合成数据和开源模型生成的混合文本上均能实现高精度的水印比例估计，且达到了理论下界。

Conclusion: 该研究为混合来源文本中的水印比例估计提供了有效的解决方案，尤其适用于使用连续关键统计量的水印方法，具有重要的实际应用价值。

Abstract: Text watermarks in large language models (LLMs) are an increasingly important
tool for detecting synthetic text and distinguishing human-written content from
LLM-generated text. While most existing studies focus on determining whether
entire texts are watermarked, many real-world scenarios involve mixed-source
texts, which blend human-written and watermarked content. In this paper, we
address the problem of optimally estimating the watermark proportion in
mixed-source texts. We cast this problem as estimating the proportion parameter
in a mixture model based on \emph{pivotal statistics}. First, we show that this
parameter is not even identifiable in certain watermarking schemes, let alone
consistently estimable. In stark contrast, for watermarking methods that employ
continuous pivotal statistics for detection, we demonstrate that the proportion
parameter is identifiable under mild conditions. We propose efficient
estimators for this class of methods, which include several popular unbiased
watermarks as examples, and derive minimax lower bounds for any measurable
estimator based on pivotal statistics, showing that our estimators achieve
these lower bounds. Through evaluations on both synthetic data and mixed-source
text generated by open-source models, we demonstrate that our proposed
estimators consistently achieve high estimation accuracy.

</details>


### [182] [Beyond ReLU: How Activations Affect Neural Kernels and Random Wide Networks](https://arxiv.org/abs/2506.22429)
*David Holzmüller,Max Schölpple*

Main category: stat.ML

TL;DR: 该论文分析了典型激活函数（如SELU、ELU、LeakyReLU等）在神经网络切线核（NTK）和神经网络高斯过程核（NNGP）中的性质，并扩展了理论覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习理论主要局限于ReLU激活函数，而对其他常见激活函数的性质理解不足。论文旨在填补这一空白，提供更广泛的理论分析。

Method: 论文通过分析典型激活函数的RKHS（再生核希尔伯特空间）性质，并扩展到多种特殊情况（如无偏置、两层网络、多项式激活等）。

Result: 研究发现，一类非无限光滑的激活函数在不同网络深度下生成等价的RKHS，而多项式激活函数生成非等价的RKHS。此外，还推导了NNGP样本路径的平滑性。

Conclusion: 论文扩展了NTK和NNGP理论的应用范围，为理解不同激活函数在无限宽神经网络中的行为提供了理论基础。

Abstract: While the theory of deep learning has made some progress in recent years,
much of it is limited to the ReLU activation function. In particular, while the
neural tangent kernel (NTK) and neural network Gaussian process kernel (NNGP)
have given theoreticians tractable limiting cases of fully connected neural
networks, their properties for most activation functions except for powers of
the ReLU function are poorly understood. Our main contribution is to provide a
more general characterization of the RKHS of these kernels for typical
activation functions whose only non-smoothness is at zero, such as SELU, ELU,
or LeakyReLU. Our analysis also covers a broad set of special cases such as
missing biases, two-layer networks, or polynomial activations. Our results show
that a broad class of not infinitely smooth activations generate equivalent
RKHSs at different network depths, while polynomial activations generate
non-equivalent RKHSs. Finally, we derive results for the smoothness of NNGP
sample paths, characterizing the smoothness of infinitely wide neural networks
at initialization.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [183] [CaloHadronic: a diffusion model for the generation of hadronic showers](https://arxiv.org/abs/2506.21720)
*Thorsten Buss,Frank Gaede,Gregor Kasieczka,Anatolii Korol,Katja Krüger,Peter McKeown,Martina Mozzanica*

Main category: physics.ins-det

TL;DR: 该论文提出了一种基于Transformer的扩散生成模型，用于高效模拟高粒度量能器中复杂电磁和强子簇射，首次实现跨电磁与强子量能器的整体生成。


<details>
  <summary>Details</summary>
Motivation: 传统粒子簇射模拟计算成本高，机器学习生成模型可提升模拟效率。现有方法在强子簇射等复杂场景存在局限，需开发更灵活的几何无关生成方法。

Method: 扩展基于扩散模型的点云生成架构，引入Transformer注意力机制，处理电磁量能器(ILD)和强子量能器的跨系统模拟。

Result: 新模型成功生成具有显著子结构的复杂强子簇射，首次实现高粒度成像量能器系统中跨电磁/强子量能器的整体模拟。

Conclusion: Transformer架构显著提升了生成式模拟在复杂粒子簇射场景的性能，为粒子物理实验的快速模拟开辟了新途径。

Abstract: Simulating showers of particles in highly-granular calorimeters is a key
frontier in the application of machine learning to particle physics. Achieving
high accuracy and speed with generative machine learning models can enable them
to augment traditional simulations and alleviate a major computing constraint.
Recent developments have shown how diffusion based generative shower simulation
approaches that do not rely on a fixed structure, but instead generate
geometry-independent point clouds, are very efficient. We present a
transformer-based extension to previous architectures which were developed for
simulating electromagnetic showers in the highly granular electromagnetic
calorimeter of the International Large Detector, ILD. The attention mechanism
now allows us to generate complex hadronic showers with more pronounced
substructure across both the electromagnetic and hadronic calorimeters. This is
the first time that machine learning methods are used to holistically generate
showers across the electromagnetic and hadronic calorimeter in highly granular
imaging calorimeter systems.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [184] [Simultaneously Fair Allocation of Indivisible Items Across Multiple Dimensions](https://arxiv.org/abs/2506.21727)
*Yasushi Kawase,Bodhayan Roy,Mohammad Azharuddin Sanpui*

Main category: cs.GT

TL;DR: 论文研究了多维环境下不可分物品的公平分配问题，提出了两种松弛的嫉妒自由概念（弱sEFc和强sEFc），并给出了存在性证明、算法及NP难解性结果。


<details>
  <summary>Details</summary>
Motivation: 现实应用中（如云计算资源分配），物品常需多维度评估（如CPU、内存、带宽），传统一维公平概念无法满足需求，需扩展至多维环境。

Method: 提出弱sEFc（各维度独立消除嫉妒）和强sEFc（单一操作消除所有维度嫉妒）两种松弛公平概念，分析其存在性边界并设计验证算法。

Result: 证明了弱/强sEFc分配的存在性上界与下界（与物品总数无关），给出验证算法，并证明弱/强sEF1存在性检查是NP难问题。

Conclusion: 多维公平分配需松弛嫉妒自由定义，理论结果和算法为复杂场景（如云计算）的公平分配提供了新工具。

Abstract: This paper explores the fair allocation of indivisible items in a
multidimensional setting, motivated by the need to address fairness in complex
environments where agents assess bundles according to multiple criteria. Such
multidimensional settings are not merely of theoretical interest but are
central to many real-world applications. For example, cloud computing resources
are evaluated based on multiple criteria such as CPU cores, memory, and network
bandwidth. In such cases, traditional one dimensional fairness notions fail to
capture fairness across multiple attributes. To address these challenges, we
study two relaxed variants of envy-freeness: weak simultaneously envy-free up
to c goods (weak sEFc) and strong simultaneously envy-free up to c goods
(strong sEFc), which accommodate the multidimensionality of agents'
preferences. Under the weak notion, for every pair of agents and for each
dimension, any perceived envy can be eliminated by removing, if necessary, a
different set of goods from the envied agent's allocation. In contrast, the
strong version requires selecting a single set of goods whose removal from the
envied bundle simultaneously eliminates envy in every dimension. We provide
upper and lower bounds on the relaxation parameter c that guarantee the
existence of weak or strong sEFc allocations, where these bounds are
independent of the total number of items. In addition, we present algorithms
for checking whether a weak or strong sEFc allocation exists. Moreover, we
establish NP-hardness results for checking the existence of weak sEF1 and
strong sEF1 allocations.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [185] [RiverEcho: Real-Time Interactive Digital System for Ancient Yellow River Culture](https://arxiv.org/abs/2506.21865)
*Haofeng Wang,Yilin Guo,Zehao Li,Tong Yue,Yizong Wang,Enci Zhang,Rongqun Lin,Feng Gao,Shiqi Wang,Siwei Ma*

Main category: cs.MM

TL;DR: 论文介绍了RiverEcho系统，利用大语言模型和文化知识数据集实现实时语音交互，以数字人形式传播黄河文化，通过RAG技术提升回答质量。


<details>
  <summary>Details</summary>
Motivation: 保护和传承古老的黄河文化，丰富文化传播方式，为用户提供更深层次的文化见解。

Method: 构建黄河文化知识数据库，结合检索增强生成（RAG）技术和大语言模型，开发实时交互系统RiverEcho，通过数字人进行语音回答。

Result: 实验表明，RAG技术显著提升了大语言模型的回答质量，使系统能生成更专业、信息更丰富的响应。

Conclusion: RiverEcho系统不仅拓展了黄河文化的传播途径，还通过技术创新增强了用户体验和文化认知。

Abstract: The Yellow River is China's mother river and a cradle of human civilization.
The ancient Yellow River culture is, moreover, an indispensable part of human
art history. To conserve and inherit the ancient Yellow River culture, we
designed RiverEcho, a real-time interactive system that responds to voice
queries using a large language model and a cultural knowledge dataset,
delivering explanations through a talking-head digital human. Specifically, we
built a knowledge database focused on the ancient Yellow River culture,
including the collection of historical texts and the processing pipeline.
Experimental results demonstrate that leveraging Retrieval-Augmented Generation
(RAG) on the proposed dataset enhances the response quality of the Large
Language Model(LLM), enabling the system to generate more professional and
informative responses. Our work not only diversifies the means of promoting
Yellow River culture but also provides users with deeper cultural insights.

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [186] [A Plea for History and Philosophy of Statistics and Machine Learning](https://arxiv.org/abs/2506.22236)
*Hanti Lin*

Main category: stat.OT

TL;DR: 该论文呼吁整合统计学与机器学习的历史和哲学，提出一个称为'可达成主义'的基础假设，并通过案例研究展示其方法论整合。


<details>
  <summary>Details</summary>
Motivation: 统计学与机器学习的界限日益模糊，但两者的历史和哲学整合尚未持续跟进，当前亟需这种双重整合以推动领域发展。

Method: 通过案例研究，追溯机器学习中的一个哲学思想至Neyman和Pearson 1936年的工作，揭示其与频率统计和机器学习的共同基础假设。

Result: 提出了'可达成主义'这一基础假设，并展示了历史和科学哲学与形式认识论在方法论层面的整合。

Conclusion: 论文强调了统计学与机器学习在历史和哲学层面整合的重要性，并展示了这种整合在理论和方法论上的潜在价值。

Abstract: The integration of the history and philosophy of statistics was initiated at
least by Hacking (1965) and advanced by Mayo (1996), but it has not received
sustained follow-up. Yet such integration is more urgent than ever, as the
recent success of artificial intelligence has been driven largely by machine
learning -- a field historically developed alongside statistics. Today, the
boundary between statistics and machine learning is increasingly blurred. What
we now need is integration, twice over: of history and philosophy, and of the
field they engage -- statistics and machine learning. I present a case study of
a philosophical idea in machine learning (and in formal epistemology) whose
root can be traced back to an often under-appreciated insight in Neyman and
Pearson's 1936 work (a follow-up to their 1933 classic). This leads to the
articulation of a foundational assumption -- largely implicit in, but shared
by, the practices of frequentist statistics and machine learning -- which I
call achievabilism. Another integration also emerges at the level of
methodology, combining two ends of the philosophy of science spectrum: history
and philosophy of science on the one hand, and formal epistemology on the other
hand.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [187] [Hitchhiking Rides Dataset: Two decades of crowd-sourced records on stochastic traveling](https://arxiv.org/abs/2506.21946)
*Till Wenke*

Main category: cs.CY

TL;DR: 该研究分析了由hitchwiki.org和hitchmap.com平台收集的超过63,000条搭便车数据，揭示了搭便车的时空模式、用户行为及其文化现象。


<details>
  <summary>Details</summary>
Motivation: 搭便车作为一种自发且分散的旅行方式，长期以来缺乏系统性研究。本研究旨在通过分析大规模搭便车数据集，填补这一研究空白。

Method: 研究利用众包数据，收集了近20年来超过63,000条搭便车记录，通过探索性分析考察了等待时间、用户行为和评论元数据。

Result: 数据集揭示了欧洲中心的分布、季节性模式以及高度活跃贡献者的重要性，同时也指出了数据的人口统计偏差和不可验证条目等局限性。

Conclusion: 尽管数据集存在局限性，但它为研究搭便车作为一种交通方式和文化现象提供了宝贵窗口。未来研究可进一步丰富数据并深化相关分析。

Abstract: Hitchhiking, a spontaneous and decentralized mode of travel, has long eluded
systematic study due to its informal nature. This paper presents and analyzes
the largest known structured dataset of hitchhiking rides, comprising over
63,000 entries collected over nearly two decades through platforms associated
with hitchwiki.org and lately on hitchmap.com. By leveraging crowd-sourced
contributions, the dataset captures key spatiotemporal and strategic aspects of
hitchhiking. This work documents the dataset's origins, evolution, and
community-driven maintenance, highlighting its Europe-centric distribution,
seasonal patterns, and reliance on a small number of highly active
contributors. Through exploratory analyses, I examine waiting times, user
behavior, and comment metadata, shedding light on the lived realities of
hitchhikers. While the dataset has inherent biases and limitations - such as
demographic skew and unverifiable entries it offers a rare and valuable window
into an alternative form of mobility. I conclude by outlining future directions
for enriching the dataset and advancing research on hitchhiking as both a
transportation practice and cultural phenomenon.

</details>


### [188] [Exploring the change in scientific readability following the release of ChatGPT](https://arxiv.org/abs/2506.21825)
*Abdulkareem Alsudais*

Main category: cs.CY

TL;DR: 该研究分析了arXiv上2010年至2024年6月7日的论文摘要，发现可读性逐年下降，且2023年后受ChatGPT影响显著。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（如ChatGPT）对科学写作和发表的影响，特别是论文摘要的可读性变化。

Method: 使用四种标准可读性公式，对arXiv平台2010-2024年的所有摘要进行逐年和跨学科分类评分。

Result: 摘要可读性逐年下降，2023年及2024年因ChatGPT发布出现显著变化，各学科趋势相似。

Conclusion: 科学写作可读性持续复杂化，AI（尤其是ChatGPT）可能对科学写作风格产生了显著影响。

Abstract: The rise and growing popularity of accessible large language models have
raised questions about their impact on various aspects of life, including how
scientists write and publish their research. The primary objective of this
paper is to analyze a dataset consisting of all abstracts posted on arXiv.org
between 2010 and June 7th, 2024, to assess the evolution of their readability
and determine whether significant shifts occurred following the release of
ChatGPT in November 2022. Four standard readability formulas are used to
calculate individual readability scores for each paper, classifying their level
of readability. These scores are then aggregated by year and across the eight
primary categories covered by the platform. The results show a steady annual
decrease in readability, suggesting that abstracts are likely becoming
increasingly complex. Additionally, following the release of ChatGPT, a
significant change in readability is observed for 2023 and the analyzed months
of 2024. Similar trends are found across categories, with most experiencing a
notable change in readability during 2023 and 2024. These findings offer
insights into the broader changes in readability and point to the likely
influence of AI on scientific writing.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [189] [Fetal Sleep: A Cross-Species Review of Physiology, Measurement, and Classification](https://arxiv.org/abs/2506.21828)
*Weitao Tang,Johann Vargas-Calixto,Nasim Katebi,Robert Galinsky,Gari D. Clifford,Faezeh Marzbanrad*

Main category: q-bio.NC

TL;DR: 该综述了80多年来关于胎儿睡眠生理特征、个体发育及调控的研究，比较了人类与大型动物模型的睡眠状态模式，探讨了缺氧和生长受限对胎儿睡眠的影响，并提出了开发非侵入性胎儿睡眠监测技术的必要性。


<details>
  <summary>Details</summary>
Motivation: 胎儿睡眠是产前神经发育中未被充分研究但至关重要的方面，理解其模式有助于早期脑成熟研究及临床检测缺氧或生长受限导致的神经损伤。

Method: 综合文献综述，比较人类与动物模型的睡眠状态，分析侵入性和非侵入性技术，并评估基于规则和深度学习的计算方法。

Result: 揭示了物种间睡眠状态的差异及类似性，总结了缺氧和生长受限对胎儿睡眠的干扰，提出了多模态非侵入监测技术的开发方向。

Conclusion: 该研究为开发客观、多模态、非侵入的胎儿睡眠监测技术奠定了基础，支持产前护理中的早期诊断和干预。

Abstract: Fetal sleep is a relatively underexplored yet vital aspect of prenatal
neurodevelopment. Understanding fetal sleep patterns could provide insights
into early brain maturation and help clinicians detect signs of neurological
compromise that arise due to fetal hypoxia or fetal growth restriction. This
review synthesizes over eight decades of research on the physiological
characteristics, ontogeny, and regulation of fetal sleep. We compare
sleep-state patterns in humans and large animal models, highlighting
species-specific differences and the presence of sleep-state analogs. We review
both invasive techniques in animals and non-invasive modalities in humans.
Computational methods for sleep-state classification are also examined,
including rule-based approaches (with and without clustering-based
preprocessing) and state-of-the-art deep learning techniques. Finally, we
discuss how intrauterine conditions such as hypoxia and fetal growth
restriction can disrupt fetal sleep. This review provides a comprehensive
foundation for the development of objective, multimodal, and non-invasive fetal
sleep monitoring technologies to support early diagnosis and intervention in
prenatal care.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [190] [3Description: An Intuitive Human-AI Collaborative 3D Modeling Approach](https://arxiv.org/abs/2506.21845)
*Zhuodi Cai*

Main category: cs.HC

TL;DR: 3Description是一种实验性的人机协作3D建模方法，通过语音和手势输入降低非专业人士的3D建模门槛，结合AI技术实现更直观的创作。


<details>
  <summary>Details</summary>
Motivation: 解决传统3D建模对非专业人士的可用性和可访问性挑战，促进更多人参与未来3D世界的构建，同时保持人类在AI协作中的创造力。

Method: 结合定性研究、产品分析和用户测试，集成自然语言处理和计算机视觉技术（如OpenAI和MediaPipe），开发基于Web的协作建模平台。

Result: 3Description通过语音和手势输入实现直观的3D模型创建与调整，提升了用户友好性和人机协作的参与度。

Conclusion: 该研究不仅推动了包容性设计流程，还平衡了AI辅助与人类主导的创作关系，为未来3D内容创作提供了新范式。

Abstract: This paper presents 3Description, an experimental human-AI collaborative
approach for intuitive 3D modeling. 3Description aims to address accessibility
and usability challenges in traditional 3D modeling by enabling
non-professional individuals to co-create 3D models using verbal and gesture
descriptions. Through a combination of qualitative research, product analysis,
and user testing, 3Description integrates AI technologies such as Natural
Language Processing and Computer Vision, powered by OpenAI and MediaPipe.
Recognizing the web has wide cross-platform capabilities, 3Description is
web-based, allowing users to describe the desired model and subsequently adjust
its components using verbal and gestural inputs. In the era of AI and emerging
media, 3Description not only contributes to a more inclusive and user-friendly
design process, empowering more people to participate in the construction of
the future 3D world, but also strives to increase human engagement in
co-creation with AI, thereby avoiding undue surrender to technology and
preserving human creativity.

</details>


### [191] [Adapting University Policies for Generative AI: Opportunities, Challenges, and Policy Solutions in Higher Education](https://arxiv.org/abs/2506.22231)
*Russell Beale*

Main category: cs.HC

TL;DR: 生成式AI（如ChatGPT）在高等教育中带来机遇与挑战，需制定政策平衡其应用与学术诚信。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI在高等教育中的快速普及及其对研究、教学和评估的双重影响。

Method: 综合近期研究和案例分析，评估AI的应用现状及检测工具的准确性。

Result: 47%的学生在课程中使用AI，检测工具准确率为88%，需政策调整以应对挑战。

Conclusion: 需通过评估设计、培训和多层执行机制等政策，利用AI潜力并维护学术诚信与公平。

Abstract: The rapid proliferation of generative artificial intelligence (AI) tools -
especially large language models (LLMs) such as ChatGPT - has ushered in a
transformative era in higher education. Universities in developed regions are
increasingly integrating these technologies into research, teaching, and
assessment. On one hand, LLMs can enhance productivity by streamlining
literature reviews, facilitating idea generation, assisting with coding and
data analysis, and even supporting grant proposal drafting. On the other hand,
their use raises significant concerns regarding academic integrity, ethical
boundaries, and equitable access. Recent empirical studies indicate that nearly
47% of students use LLMs in their coursework - with 39% using them for exam
questions and 7% for entire assignments - while detection tools currently
achieve around 88% accuracy, leaving a 12% error margin. This article
critically examines the opportunities offered by generative AI, explores the
multifaceted challenges it poses, and outlines robust policy solutions.
Emphasis is placed on redesigning assessments to be AI-resilient, enhancing
staff and student training, implementing multi-layered enforcement mechanisms,
and defining acceptable use. By synthesizing data from recent research and case
studies, the article argues that proactive policy adaptation is imperative to
harness AI's potential while safeguarding the core values of academic integrity
and equity.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [192] [Autonomic Microservice Management via Agentic AI and MAPE-K Integration](https://arxiv.org/abs/2506.22185)
*Matteo Esposito,Alexander Bakhtin,Noman Ahmad,Mikel Robredo,Ruoyu Su,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 提出基于MAPE-K和智能代理的框架，用于微服务异常检测与修复，提升系统稳定性。


<details>
  <summary>Details</summary>
Motivation: 微服务的去中心化特性带来安全和管理挑战，威胁系统稳定性，需自动化解决方案。

Method: 采用MAPE-K模型结合智能代理技术，实现自主异常检测与修复。

Result: 框架提供可定制方案，降低停机时间，增强性能、弹性及安全性等质量属性。

Conclusion: 该框架为分布式微服务管理提供实用工具，助力系统稳定与安全。

Abstract: While microservices are revolutionizing cloud computing by offering
unparalleled scalability and independent deployment, their decentralized nature
poses significant security and management challenges that can threaten system
stability. We propose a framework based on MAPE-K, which leverages agentic AI,
for autonomous anomaly detection and remediation to address the daunting task
of highly distributed system management. Our framework offers practical,
industry-ready solutions for maintaining robust and secure microservices.
Practitioners and researchers can customize the framework to enhance system
stability, reduce downtime, and monitor broader system quality attributes such
as system performance level, resilience, security, and anomaly management,
among others.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [193] [Using Large Language Models to Suggest Informative Prior Distributions in Bayesian Statistics](https://arxiv.org/abs/2506.21964)
*Michael A. Riegler,Kristoffer Herland Hellton,Vajira Thambawita,Hugo L. Hammer*

Main category: stat.ME

TL;DR: 论文探讨了使用大语言模型（LLM）为贝叶斯统计选择先验分布的方法，发现LLM能正确识别关联方向，但在校准先验宽度上仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯统计中选择先验分布具有挑战性、资源密集且主观性强，研究旨在利用LLM提供基于知识的先验建议以提高效率和客观性。

Method: 使用Claude Opus、Gemini 2.5 Pro和ChatGPT-4o-mini对心脏病风险和混凝土强度数据集进行分析，通过Kullback-Leibler散度评估先验质量。

Result: LLM能正确识别所有关联方向，但中等信息先验常过于自信。Claude和Gemini表现优于ChatGPT，尤其在弱信息先验上Claude避免不必要的模糊性。

Conclusion: LLM在高效、客观地生成信息性先验方面潜力巨大，但校准先验宽度以避免过度自信仍是主要挑战。

Abstract: Selecting prior distributions in Bayesian statistics is challenging,
resource-intensive, and subjective. We analyze using large-language models
(LLMs) to suggest suitable, knowledge-based informative priors. We developed an
extensive prompt asking LLMs not only to suggest priors but also to verify and
reflect on their choices.
  We evaluated Claude Opus, Gemini 2.5 Pro, and ChatGPT-4o-mini on two real
datasets: heart disease risk and concrete strength. All LLMs correctly
identified the direction for all associations (e.g., that heart disease risk is
higher for males). The quality of suggested priors was measured by their
Kullback-Leibler divergence from the maximum likelihood estimator's
distribution.
  The LLMs suggested both moderately and weakly informative priors. The
moderate priors were often overconfident, resulting in distributions misaligned
with the data. In our experiments, Claude and Gemini provided better priors
than ChatGPT. For weakly informative priors, a key performance difference
emerged: ChatGPT and Gemini defaulted to an "unnecessarily vague" mean of 0,
while Claude did not, demonstrating a significant advantage.
  The ability of LLMs to identify correct associations shows their great
potential as an efficient, objective method for developing informative priors.
However, the primary challenge remains in calibrating the width of these priors
to avoid over- and under-confidence.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [194] [Adversarial Threats in Quantum Machine Learning: A Survey of Attacks and Defenses](https://arxiv.org/abs/2506.21842)
*Archisman Ghosh,Satwik Kundu,Swaroop Ghosh*

Main category: quant-ph

TL;DR: 量子机器学习（QML）结合量子计算与经典机器学习，但其快速发展在NISQ时代带来安全挑战。本文探讨QML特有的对抗威胁，提出防御机制，并总结攻击与防御的最新进展。


<details>
  <summary>Details</summary>
Motivation: 随着量子机器学习（QML）的快速发展，特别是在嘈杂的中尺度量子（NISQ）时代，其面临独特的安全挑战。本文旨在分析QML系统中的对抗威胁，并提出相应的防御策略。

Method: 通过分析云部署、混合架构和量子生成模型中的漏洞，识别关键攻击向量，如模型窃取、数据投毒和反向工程。同时，利用量子特性设计防御机制，如噪声水印和硬件感知混淆技术。

Result: 研究发现，噪声签名和硬件感知技术可有效防御模型克隆和攻击。此外，经典对抗训练和差分隐私的量子化应用也能增强QML系统的安全性。

Conclusion: 尽管QML面临诸多安全挑战，但通过结合量子特性和经典防御技术，可以构建更健壮、可信的QML系统。未来需解决噪声平衡、跨平台攻击和量子-经典信任框架等问题。

Abstract: Quantum Machine Learning (QML) integrates quantum computing with classical
machine learning, primarily to solve classification, regression and generative
tasks. However, its rapid development raises critical security challenges in
the Noisy Intermediate-Scale Quantum (NISQ) era. This chapter examines
adversarial threats unique to QML systems, focusing on vulnerabilities in
cloud-based deployments, hybrid architectures, and quantum generative models.
Key attack vectors include model stealing via transpilation or output
extraction, data poisoning through quantum-specific perturbations, reverse
engineering of proprietary variational quantum circuits, and backdoor attacks.
Adversaries exploit noise-prone quantum hardware and insufficiently secured
QML-as-a-Service (QMLaaS) workflows to compromise model integrity, ownership,
and functionality. Defense mechanisms leverage quantum properties to counter
these threats. Noise signatures from training hardware act as non-invasive
watermarks, while hardware-aware obfuscation techniques and ensemble strategies
disrupt cloning attempts. Emerging solutions also adapt classical adversarial
training and differential privacy to quantum settings, addressing
vulnerabilities in quantum neural networks and generative architectures.
However, securing QML requires addressing open challenges such as balancing
noise levels for reliability and security, mitigating cross-platform attacks,
and developing quantum-classical trust frameworks. This chapter summarizes
recent advances in attacks and defenses, offering a roadmap for researchers and
practitioners to build robust, trustworthy QML systems resilient to evolving
adversarial landscapes.

</details>


### [195] [Robust quantum reservoir computers for forecasting chaotic dynamics: generalized synchronization and stability](https://arxiv.org/abs/2506.22335)
*Osama Ahmed,Felix Tennie,Luca Magri*

Main category: quant-ph

TL;DR: 该论文展示了循环量子储层计算机（QRC）及其无循环架构（RF-QRC）在学习和预测混沌动力学中的鲁棒性，提出GS=ESP准则，并通过数值验证支持结论。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用量子储层计算机学习和预测混沌动力学，探索其在近量子硬件上的应用潜力。

Method: 将量子储层计算机建模为耦合动力系统，推导其雅可比矩阵，利用广义同步工具设计鲁棒的量子储层计算机。

Result: 量子储层计算机能学习混沌动力学及其不变性质，噪声的耗散增强了其鲁棒性，数值验证支持了理论分析。

Conclusion: 该工作为在近量子硬件上设计鲁棒的混沌时间序列预测量子机器开辟了新途径。

Abstract: We show that recurrent quantum reservoir computers (QRCs) and their
recurrence-free architectures (RF-QRCs) are robust tools for learning and
forecasting chaotic dynamics from time-series data. First, we formulate and
interpret quantum reservoir computers as coupled dynamical systems, where the
reservoir acts as a response system driven by training data; in other words,
quantum reservoir computers are generalized-synchronization (GS) systems.
Second, we show that quantum reservoir computers can learn chaotic dynamics and
their invariant properties, such as Lyapunov spectra, attractor dimensions, and
geometric properties such as the covariant Lyapunov vectors. This analysis is
enabled by deriving the Jacobian of the quantum reservoir update. Third, by
leveraging tools from generalized synchronization, we provide a method for
designing robust quantum reservoir computers. We propose the criterion
$GS=ESP$: GS implies the echo state property (ESP), and vice versa. We
analytically show that RF-QRCs, by design, fulfill $GS=ESP$. Finally, we
analyze the effect of simulated noise. We find that dissipation from noise
enhances the robustness of quantum reservoir computers. Numerical verifications
on systems of different dimensions support our conclusions. This work opens
opportunities for designing robust quantum machines for chaotic time series
forecasting on near-term quantum hardware.

</details>


### [196] [QuKAN: A Quantum Circuit Born Machine approach to Quantum Kolmogorov Arnold Networks](https://arxiv.org/abs/2506.22340)
*Yannick Werner,Akash Malemath,Mengxi Liu,Vitor Fortes Rey,Nikolaos Palaiodimopoulos,Paul Lukowicz,Maximilian Kiefer-Emmanouilidis*

Main category: quant-ph

TL;DR: 论文提出量子KAN架构（QuKAN），结合经典与量子计算，探索Kolmogorov Arnold网络在量子机器学习中的潜力。


<details>
  <summary>Details</summary>
Motivation: KAN网络在表达复杂函数方面表现出色，但其在量子机器学习中的应用尚未充分探索。

Method: 采用量子电路Born机（QCBM）实现混合和全量子KAN架构，利用预训练残差函数进行参数优化。

Result: 展示了QuKAN架构的可行性、可解释性和性能表现。

Conclusion: 量子KAN架构为量子机器学习提供了新的研究方向，具有潜在的应用价值。

Abstract: Kolmogorov Arnold Networks (KANs), built upon the Kolmogorov Arnold
representation theorem (KAR), have demonstrated promising capabilities in
expressing complex functions with fewer neurons. This is achieved by
implementing learnable parameters on the edges instead of on the nodes, unlike
traditional networks such as Multi-Layer Perceptrons (MLPs). However, KANs
potential in quantum machine learning has not yet been well explored. In this
work, we present an implementation of these KAN architectures in both hybrid
and fully quantum forms using a Quantum Circuit Born Machine (QCBM). We adapt
the KAN transfer using pre-trained residual functions, thereby exploiting the
representational power of parametrized quantum circuits. In the hybrid model we
combine classical KAN components with quantum subroutines, while the fully
quantum version the entire architecture of the residual function is translated
to a quantum model. We demonstrate the feasibility, interpretability and
performance of the proposed Quantum KAN (QuKAN) architecture.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [197] [Inverse Design of Diffractive Metasurfaces Using Diffusion Models](https://arxiv.org/abs/2506.21748)
*Liav Hen,Erez Yosef,Dan Raviv,Raja Giryes,Jacob Scheuer*

Main category: physics.optics

TL;DR: 该论文提出了一种利用扩散模型进行超表面逆向设计的方法，通过生成模型快速优化结构，显著降低了计算成本和时间。


<details>
  <summary>Details</summary>
Motivation: 超表面的逆向设计因结构与光学特性间的复杂非线性关系而具有挑战性，传统方法需要专家调参且易陷入局部最优，计算开销大。

Method: 结合扩散模型与RCWA模拟器生成训练数据，训练条件扩散模型从目标空间功率分布预测超表面结构，支持直接生成或优化初始化。

Result: 在30分钟内设计出低误差的空间均匀强度分束器和偏振分束器，代码和数据集已公开。

Conclusion: 扩散模型为数据驱动的超表面设计提供了高效解决方案，显著提升了设计速度和精度。

Abstract: Metasurfaces are ultra-thin optical elements composed of engineered
sub-wavelength structures that enable precise control of light. Their inverse
design - determining a geometry that yields a desired optical response - is
challenging due to the complex, nonlinear relationship between structure and
optical properties. This often requires expert tuning, is prone to local
minima, and involves significant computational overhead. In this work, we
address these challenges by integrating the generative capabilities of
diffusion models into computational design workflows. Using an RCWA simulator,
we generate training data consisting of metasurface geometries and their
corresponding far-field scattering patterns. We then train a conditional
diffusion model to predict meta-atom geometry and height from a target spatial
power distribution at a specified wavelength, sampled from a continuous
supported band. Once trained, the model can generate metasurfaces with low
error, either directly using RCWA-guided posterior sampling or by serving as an
initializer for traditional optimization methods. We demonstrate our approach
on the design of a spatially uniform intensity splitter and a polarization beam
splitter, both produced with low error in under 30 minutes. To support further
research in data-driven metasurface design, we publicly release our code and
datasets.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [198] [Demonstrating Interoperable Channel State Feedback Compression with Machine Learning](https://arxiv.org/abs/2506.21796)
*Dani Korpi,Rachel Wang,Jerry Wang,Abdelrahman Ibrahim,Carl Nuzman,Runxin Wang,Kursat Rasim Mestav,Dustin Zhang,Iraj Saniee,Shawn Winston,Gordana Pavlovic,Wei Ding,William J. Hillery,Chenxi Hao,Ram Thirunagari,Jung Chang,Jeehyun Kim,Bartek Kozicki,Dragan Samardzija,Taesang Yoo,Andreas Maeder,Tingfang Ji,Harish Viswanathan*

Main category: eess.SP

TL;DR: 该论文提出了一种新颖的保密训练方法，用于开发无需共享ML模型的神经网络压缩与解压缩技术，以提升6G网络中信道状态反馈的准确性和下行链路吞吐量。


<details>
  <summary>Details</summary>
Motivation: 尽管基于机器学习（ML）的信道反馈压缩在仿真研究中显示出降低开销和提高信道信息准确性的潜力，但在实际应用中，尤其是当用户设备（UE）和基站无法访问彼此的ML模型时，缺乏实际验证。

Method: 论文提出了一种保密训练互操作压缩和解压缩ML模型的新方法，并通过原型UE和基站验证了模型的准确性。

Result: 测量结果表明，无需在设备和网络供应商之间共享ML模型，即可开发出准确的信道反馈链路，从而在波束成形中提高下行链路吞吐量。

Conclusion: 这些结果为在商用6G网络中实际应用基于ML的信道反馈铺平了道路。

Abstract: Neural network-based compression and decompression of channel state feedback
has been one of the most widely studied applications of machine learning (ML)
in wireless networks. Various simulation-based studies have shown that ML-based
feedback compression can result in reduced overhead and more accurate channel
information. However, to the best of our knowledge, there are no real-life
proofs of concepts demonstrating the benefits of ML-based channel feedback
compression in a practical setting, where the user equipment (UE) and base
station have no access to each others' ML models. In this paper, we present a
novel approach for training interoperable compression and decompression ML
models in a confidential manner, and demonstrate the accuracy of the ensuing
models using prototype UEs and base stations. The performance of the ML-based
channel feedback is measured both in terms of the accuracy of the reconstructed
channel information and achieved downlink throughput gains when using the
channel information for beamforming. The reported measurement results
demonstrate that it is possible to develop an accurate ML-based channel
feedback link without having to share ML models between device and network
vendors. These results pave the way for a practical implementation of ML-based
channel feedback in commercial 6G networks.

</details>


### [199] [From Token to Rhythm: A Multi-Scale Approach for ECG-Language Pretraining](https://arxiv.org/abs/2506.21803)
*Fuying Wang,Jiacheng Xu,Lequan Yu*

Main category: eess.SP

TL;DR: MELP模型通过多尺度ECG-文本预训练，显著提升心电图分析的泛化能力，优于现有自监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习ECG分析依赖大量人工标注，耗时耗力。现有自监督学习方法未能捕捉ECG信号的多尺度特性，导致泛化能力不足。

Method: 提出MELP模型，结合心脏学语言模型预训练，通过标记、节拍和节律三级跨模态监督，对齐ECG信号与文本报告。

Result: 在三个公开ECG数据集上，MELP在零样本分类、线性探测和迁移学习任务中均优于现有方法。

Conclusion: MELP通过多尺度监督有效捕捉ECG层次结构，为临床ECG分析提供了高效、泛化性强的解决方案。

Abstract: Electrocardiograms (ECGs) play a vital role in monitoring cardiac health and
diagnosing heart diseases. However, traditional deep learning approaches for
ECG analysis rely heavily on large-scale manual annotations, which are both
time-consuming and resource-intensive to obtain. To overcome this limitation,
self-supervised learning (SSL) has emerged as a promising alternative, enabling
the extraction of robust ECG representations that can be efficiently
transferred to various downstream tasks. While previous studies have explored
SSL for ECG pretraining and multi-modal ECG-language alignment, they often fail
to capture the multi-scale nature of ECG signals. As a result, these methods
struggle to learn generalized representations due to their inability to model
the hierarchical structure of ECG data. To address this gap, we introduce MELP,
a novel Multi-scale ECG-Language Pretraining (MELP) model that fully leverages
hierarchical supervision from ECG-text pairs. MELP first pretrains a
cardiology-specific language model to enhance its understanding of clinical
text. It then applies three levels of cross-modal supervision-at the token,
beat, and rhythm levels-to align ECG signals with textual reports, capturing
structured information across different time scales. We evaluate MELP on three
public ECG datasets across multiple tasks, including zero-shot ECG
classification, linear probing, and transfer learning. Experimental results
demonstrate that MELP outperforms existing SSL methods, underscoring its
effectiveness and adaptability across diverse clinical applications. Our code
is available at https://github.com/HKU-MedAI/MELP.

</details>


### [200] [Searching Efficient Deep Architectures for Radar Target Detection using Monte-Carlo Tree Search](https://arxiv.org/abs/2506.21772)
*Noé Lallouet,Tristan Cazenave,Cyrille Enderli,Stéphanie Gourdin*

Main category: eess.SP

TL;DR: 该论文探讨了基于蒙特卡洛树搜索的神经架构搜索方法，旨在降低雷达目标检测中深度神经网络的计算复杂度，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在雷达目标检测中表现出色，但其高计算复杂度限制了在嵌入式雷达系统中的广泛应用。

Method: 采用基于蒙特卡洛树搜索的神经架构搜索方法，寻找在计算复杂度和检测性能之间取得平衡的网络结构。

Result: 提出了一种新型网络，在满足检测概率要求的同时，显著降低了计算复杂度，优于专家设计的基线模型。

Conclusion: 通过神经架构搜索方法，成功实现了在雷达目标检测中兼顾高性能和低计算复杂度的目标。

Abstract: Recent research works establish deep neural networks as high performing tools
for radar target detection, especially on challenging environments (presence of
clutter or interferences, multi-target scenarii...). However, the usually large
computational complexity of these networks is one of the factors preventing
them from being widely implemented in embedded radar systems. We propose to
investigate novel neural architecture search (NAS) methods, based on
Monte-Carlo Tree Search (MCTS), for finding neural networks achieving the
required detection performance and striving towards a lower computational
complexity. We evaluate the searched architectures on endoclutter radar
signals, in order to compare their respective performance metrics and
generalization properties. A novel network satisfying the required detection
probability while being significantly lighter than the expert-designed baseline
is proposed.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [201] [SciMantify -- A Hybrid Approach for the Evolving Semantification of Scientific Knowledge](https://arxiv.org/abs/2506.21819)
*Lena John,Kheir Eddine Farfar,Sören Auer,Oliver Karras*

Main category: cs.DL

TL;DR: 论文提出了一种基于5星关联开放数据模型的知识表示进化模型，通过SciMantify混合方法实现科学知识的语义化表示，并在ORKG平台上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 科学出版物主要以PDF形式数字化，内容静态且非结构化，限制了知识的可访问性和可重用性。需要更灵活、结构化且语义化的表示方法，使科学知识对人类和机器都更易理解和处理。

Method: 提出五阶段知识表示进化模型，开发混合方法SciMantify，结合人类与机器的协作，通过语义标注任务逐步提升科学知识的语义表示。

Result: 在ORKG平台上的初步用户实验表明，该方法简化了科学知识的预处理，减少了语义化的工作量，并通过与知识图谱结构的更好对齐提升了知识表示质量。

Conclusion: 该研究为科学知识的语义化表示提供了一种有效方法，显著提升了知识的可查找性、可访问性、互操作性和可重用性。

Abstract: Scientific publications, primarily digitized as PDFs, remain static and
unstructured, limiting the accessibility and reusability of the contained
knowledge. At best, scientific knowledge from publications is provided in
tabular formats, which lack semantic context. A more flexible, structured, and
semantic representation is needed to make scientific knowledge understandable
and processable by both humans and machines. We propose an evolution model of
knowledge representation, inspired by the 5-star Linked Open Data (LOD) model,
with five stages and defined criteria to guide the stepwise transition from a
digital artifact, such as a PDF, to a semantic representation integrated in a
knowledge graph (KG). Based on an exemplary workflow implementing the entire
model, we developed a hybrid approach, called SciMantify, leveraging tabular
formats of scientific knowledge, e.g., results from secondary studies, to
support its evolving semantification. In the approach, humans and machines
collaborate closely by performing semantic annotation tasks (SATs) and refining
the results to progressively improve the semantic representation of scientific
knowledge. We implemented the approach in the Open Research Knowledge Graph
(ORKG), an established platform for improving the findability, accessibility,
interoperability, and reusability of scientific knowledge. A preliminary user
experiment showed that the approach simplifies the preprocessing of scientific
knowledge, reduces the effort for the evolving semantification, and enhances
the knowledge representation through better alignment with the KG structures.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [202] [DiffSoundStream: Efficient Speech Tokenization via Diffusion Decoding](https://arxiv.org/abs/2506.22362)
*Yang Yang,Yunpeng Li,George Sung,Shao-Fu Shih,Craig Dooley,Alessio Centazzo,Ramanan Rajeswaran*

Main category: eess.AS

TL;DR: DiffSoundStream通过减少语义和声学令牌的冗余，并利用潜在扩散模型合成高质量波形，提高了语音令牌化的效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于令牌的语音生成方法在推理速度上受限于令牌率，需要一种更高效的语音令牌化方案。

Method: 提出DiffSoundStream，通过两种技术改进语音令牌化效率：1) 在神经编解码器中条件化语义令牌以减少冗余；2) 利用潜在扩散模型从语义和粗粒度声学令牌合成高质量波形。

Result: 在每秒50个令牌的速率下，DiffSoundStream的语音质量与标准SoundStream模型在双倍令牌率下的表现相当，且仅用四步扩散采样即可实现步长蒸馏，质量损失微小。

Conclusion: DiffSoundStream显著提高了语音令牌化的效率，同时保持了高质量的语音生成能力。

Abstract: Token-based language modeling is a prominent approach for speech generation,
where tokens are obtained by quantizing features from self-supervised learning
(SSL) models and extracting codes from neural speech codecs, generally referred
to as semantic tokens and acoustic tokens. These tokens are often modeled
autoregressively, with the inference speed being constrained by the token rate.
In this work, we propose DiffSoundStream, a solution that improves the
efficiency of speech tokenization in non-streaming scenarios through two
techniques: (1) conditioning the neural codec on semantic tokens to minimize
redundancy between semantic and acoustic tokens, and (2) leveraging latent
diffusion models to synthesize high-quality waveforms from semantic and
coarse-level acoustic tokens. Experiments show that at 50 tokens per second,
DiffSoundStream achieves speech quality on par with a standard SoundStream
model operating at twice the token rate. Additionally, we achieve step-size
distillation using just four diffusion sampling steps with only a minor quality
loss.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [203] [Robust and Efficient Autoregressive Speech Synthesis with Dynamic Chunk-wise Prediction Policy](https://arxiv.org/abs/2506.22023)
*Bohan Li,Zhihan Li,Haoran Wang,Hanglei Zhang,Yiwei Guo,Hankun Wang,Xie Chen,Kai Yu*

Main category: cs.SD

TL;DR: 论文提出动态分块自回归框架DCAR，通过分块到帧的注意力机制提升语音合成的效率与清晰度，显著优于传统逐词预测模型。


<details>
  <summary>Details</summary>
Motivation: 传统自回归语音合成模型在处理长序列时存在帧间注意力不稳定、延迟高和合成质量下降的问题，限制了实时应用。

Method: 引入动态分块自回归框架DCAR，采用多词预测训练和轻量级模块实现动态分块预测，减少序列长度依赖。

Result: DCAR在测试集上实现了72.27%的清晰度提升和2.61倍的推理加速，显著优于传统模型。

Conclusion: DCAR为下一代语音合成系统提供了高效且鲁棒的基础框架。

Abstract: Recently, autoregressive (AR) language models have emerged as a dominant
approach in speech synthesis, offering expressive generation and scalable
training. However, conventional AR speech synthesis models relying on the
next-token prediction paradigm often encounter significant challenges when
handling long speech sequences. These models often struggle to construct stable
frame-to-frame attention, leading to increased latency and degraded synthesis
quality, thereby limiting their feasibility for real-time applications. To
address these limitations, we introduce a novel dynamic chunk-wise
autoregressive synthesis framework, termed DCAR, designed to enhance both
efficiency and intelligibility robustness in AR speech generation. DCAR
introduces a chunk-to-frame attention mechanism through training with
multi-token prediction, enabling dynamic chunk prediction in variable speech
contexts using a lightweight module trained on-policy. DCAR dynamically adjusts
the token prediction span, significantly reducing the sequence length
dependency while obtaining high synthesis quality. Comprehensive empirical
evaluations demonstrate that DCAR substantially outperforms traditional
next-token prediction models, achieving up to 72.27% intelligibility
improvement and 2.61x inference speedup simultaneously on the test set.
Furthermore, we conduct comprehensive analysis to support it as a versatile
foundation for next-generation speech synthesis systems.

</details>


### [204] [Fine-Tuning MIDI-to-Audio Alignment using a Neural Network on Piano Roll and CQT Representations](https://arxiv.org/abs/2506.22237)
*Sebastian Murgul,Moritz Reiser,Michael Heizmann,Christoph Seibert*

Main category: cs.SD

TL;DR: 该论文使用卷积循环神经网络（CRNN）同步钢琴演奏音频与MIDI文件，比传统方法准确率提升20%。


<details>
  <summary>Details</summary>
Motivation: 解决钢琴演奏音频与MIDI文件松散同步的问题，提升对齐精度。

Method: 采用CRNN架构处理未对齐的钢琴卷和频谱图，生成对齐的钢琴卷，并通过增强的MIDI数据集训练模型。

Result: 模型比动态时间规整（DTW）方法准确率提高20%，结合DTW后效果更优。

Conclusion: 神经网络在MIDI与音频对齐任务中具有显著潜力，可提升现有技术水平。

Abstract: In this paper, we present a neural network approach for synchronizing audio
recordings of human piano performances with their corresponding loosely aligned
MIDI files. The task is addressed using a Convolutional Recurrent Neural
Network (CRNN) architecture, which effectively captures spectral and temporal
features by processing an unaligned piano roll and a spectrogram as inputs to
estimate the aligned piano roll. To train the network, we create a dataset of
piano pieces with augmented MIDI files that simulate common human timing
errors. The proposed model achieves up to 20% higher alignment accuracy than
the industry-standard Dynamic Time Warping (DTW) method across various
tolerance windows. Furthermore, integrating DTW with the CRNN yields additional
improvements, offering enhanced robustness and consistency. These findings
demonstrate the potential of neural networks in advancing state-of-the-art
MIDI-to-audio alignment.

</details>


### [205] [A Practical Approach to Power Saving in Hearables Using Sub-Nyquist Sampling with Bandwidth Extension](https://arxiv.org/abs/2506.22321)
*Tarikul Islam Tamiti,Anomadarshi Barua*

Main category: cs.SD

TL;DR: 论文提出SUBARU方法，通过亚奈奎斯特采样和低比特分辨率降低功耗，并实现高质量音频增强。


<details>
  <summary>Details</summary>
Motivation: 现有研究未考虑低功耗实现中的采样频率、比特分辨率对语音质量和智能性的影响，以及如何在不使用GAN判别器的情况下实现高质量音频。

Method: SUBARU采用亚奈奎斯特采样和低比特分辨率ADC，引入多尺度虚拟判别器，实现流式操作和语音增强。

Result: 功耗降低3.31倍，推理时间1.74ms，内存占用小于13.77MB，实现高质量音频增强。

Conclusion: SUBARU在低功耗和高质量音频增强方面表现出色，适用于实际应用场景。

Abstract: Hearables are wearable computers that are worn on the ear. Bone conduction
microphones (BCMs) are used with air conduction microphones (ACMs) in hearables
as a supporting modality for multimodal speech enhancement (SE) in noisy
conditions. However, existing works don't consider the following practical
aspects for low-power implementations on hearables: (i) They do not explore how
lowering the sampling frequencies and bit resolutions in analog-to-digital
converters (ADCs) of hearables jointly impact low-power processing and
multimodal SE in terms of speech quality and intelligibility. (ii) They don't
discuss how GAN-like audio quality can be achieved without using actual GAN
discriminators. And (iii) They don't process signals from ACMs/BCMs at
sub-Nyquist sampling rate because, in their frameworks, they lack a wideband
reconstruction methodology from their narrowband parts. We propose SUBARU
(\textbf{Sub}-Nyquist \textbf{A}udio \textbf{R}esolution \textbf{U}psampling),
which achieves the following: SUBARU (i) intentionally uses sub-Nyquist
sampling and low bit resolution in ADCs, achieving a 3.31x reduction in power
consumption; (ii) introduces novel multi-scale and multi-period virtual
discriminators, which achieve GAN-like audio quality without using GANs'
adversarial training; and (iii) achieves streaming operations on mobile
platforms and SE in in-the-wild noisy conditions with an inference time of
1.74ms and a memory footprint of less than 13.77MB.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [206] [Storm Surge in Color: RGB-Encoded Physics-Aware Deep Learning for Storm Surge Forecasting](https://arxiv.org/abs/2506.21743)
*Jinpai Zhao,Albert Cerrone,Eirik Valseth,Leendert Westerink,Clint Dawson*

Main category: cs.CE

TL;DR: 该论文提出了一种新的风暴潮预测方法，通过将非结构化水位数据转换为RGB图像表示，结合ConvLSTM网络和物理驱动因素，提升了预测性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法在风暴潮预测中存在空间分辨率低、依赖海岸站点数据和泛化能力差的问题，且与现代深度学习架构不兼容。

Method: 将非结构化水位场投影为RGB编码图像，使用ConvLSTM网络进行端到端时空预测，并整合风场和地形数据作为动态和静态输入。

Result: 在墨西哥湾合成风暴数据集上，该方法展示了48小时预测的鲁棒性，并在德克萨斯海岸及其他沿海区域表现出良好的空间扩展性。

Conclusion: 通过结构化表示、物理驱动和可扩展深度学习，该方法在风暴潮预测的实用性、适应性和可解释性方面取得了进展。

Abstract: Storm surge forecasting plays a crucial role in coastal disaster
preparedness, yet existing machine learning approaches often suffer from
limited spatial resolution, reliance on coastal station data, and poor
generalization. Moreover, many prior models operate directly on unstructured
spatial data, making them incompatible with modern deep learning architectures.
In this work, we introduce a novel approach that projects unstructured water
elevation fields onto structured Red Green Blue (RGB)-encoded image
representations, enabling the application of Convolutional Long Short Term
Memory (ConvLSTM) networks for end-to-end spatiotemporal surge forecasting. Our
model further integrates ground-truth wind fields as dynamic conditioning
signals and topo-bathymetry as a static input, capturing physically meaningful
drivers of surge evolution. Evaluated on a large-scale dataset of synthetic
storms in the Gulf of Mexico, our method demonstrates robust 48-hour
forecasting performance across multiple regions along the Texas coast and
exhibits strong spatial extensibility to other coastal areas. By combining
structured representation, physically grounded forcings, and scalable deep
learning, this study advances the frontier of storm surge forecasting in
usability, adaptability, and interpretability.

</details>


### [207] [Laser Scan Path Design for Controlled Microstructure in Additive Manufacturing with Integrated Reduced-Order Phase-Field Modeling and Deep Reinforcement Learning](https://arxiv.org/abs/2506.21815)
*Augustine Twumasi,Prokash Chandra Roy,Zixun Li,Soumya Shouvik Bhattacharjee,Zhengtao Gan*

Main category: cs.CE

TL;DR: 该论文提出了一种结合物理模型和机器学习的方法，通过3D U-Net和深度强化学习优化激光粉末床熔融技术的扫描路径，以实现理想的微观结构，并提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 激光粉末床熔融（L-PBF）技术在生产复杂金属部件时面临微观结构影响产品质量的挑战，传统方法依赖试错且计算成本高。

Method: 使用相场法模拟晶粒结构演化，训练3D U-Net作为替代模型预测晶粒取向，并应用深度强化学习优化扫描路径。

Result: 替代模型实现了两个数量级的加速，强化学习方法在多个案例中有效优化了扫描路径，显著提升了微观结构控制和计算效率。

Conclusion: 结合物理模型和机器学习的方法在L-PBF优化中展现出潜力，能够高效控制微观结构并减少计算成本。

Abstract: Laser powder bed fusion (L-PBF) is a widely recognized additive manufacturing
technology for producing intricate metal components with exceptional accuracy.
A key challenge in L-PBF is the formation of complex microstructures affecting
product quality. We propose a physics-guided, machine-learning approach to
optimize scan paths for desired microstructure outcomes, such as equiaxed
grains. We utilized a phase-field method (PFM) to model crystalline grain
structure evolution. To reduce computational costs, we trained a surrogate
machine learning model, a 3D U-Net convolutional neural network, using
single-track phase-field simulations with various laser powers to predict
crystalline grain orientations based on initial microstructure and thermal
history. We investigated three scanning strategies across various hatch
spacings within a square domain, achieving a two-orders-of-magnitude speedup
using the surrogate model. To reduce trial and error in designing laser scan
toolpaths, we used deep reinforcement learning (DRL) to generate optimized scan
paths for target microstructure. Results from three cases demonstrate the DRL
approach's effectiveness. We integrated the surrogate 3D U-Net model into our
DRL environment to accelerate the reinforcement learning training process. The
reward function minimizes both aspect ratio and grain volume of the predicted
microstructure from the agent's scan path. The reinforcement learning algorithm
was benchmarked against conventional zigzag approach for smaller and larger
domains, showing machine learning methods' potential to enhance microstructure
control and computational efficiency in L-PBF optimization.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [208] [ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment](https://arxiv.org/abs/2109.05721)
*Yangyu Huang,Hao Yang,Chong Li,Jongyoo Kim,Fangyun Wei*

Main category: cs.CV

TL;DR: 该论文提出了一种针对人脸对齐任务中误差偏差问题的新方法ADNet，通过各向异性方向损失（ADL）和各向异性注意力模块（AAM）优化模型收敛，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前CNN在人脸对齐任务中表现优异，但少有研究关注面部关键点误差分布的偏差问题。这种误差通常沿关键点曲线的切线方向分布，与模糊的关键点标注任务密切相关。

Method: 提出ADL和AAM两种方法：ADL在法线方向施加强约束力，AAM通过各向异性注意力机制在切线方向放松约束。二者互补，共同学习面部结构和纹理细节，并整合为端到端训练框架ADNet。

Result: ADNet在300W、WFLW和COFW数据集上取得了最先进的结果，证明了方法的有效性和鲁棒性。

Conclusion: 通过利用误差偏差特性设计的ADL和AAM模块能有效提升人脸对齐性能，ADNet框架展示了处理此类问题的优越性。

Abstract: The recent progress of CNN has dramatically improved face alignment
performance. However, few works have paid attention to the error-bias with
respect to error distribution of facial landmarks. In this paper, we
investigate the error-bias issue in face alignment, where the distributions of
landmark errors tend to spread along the tangent line to landmark curves. This
error-bias is not trivial since it is closely connected to the ambiguous
landmark labeling task. Inspired by this observation, we seek a way to leverage
the error-bias property for better convergence of CNN model. To this end, we
propose anisotropic direction loss (ADL) and anisotropic attention module (AAM)
for coordinate and heatmap regression, respectively. ADL imposes strong binding
force in normal direction for each landmark point on facial boundaries. On the
other hand, AAM is an attention module which can get anisotropic attention mask
focusing on the region of point and its local edge connected by adjacent
points, it has a stronger response in tangent than in normal, which means
relaxed constraints in the tangent. These two methods work in a complementary
manner to learn both facial structures and texture details. Finally, we
integrate them into an optimized end-to-end training pipeline named ADNet. Our
ADNet achieves state-of-the-art results on 300W, WFLW and COFW datasets, which
demonstrates the effectiveness and robustness.

</details>


### [209] [FreeEnricher: Enriching Face Landmarks without Additional Cost](https://arxiv.org/abs/2212.09525)
*Yangyu Huang,Xi Chen,Jongyoo Kim,Hao Yang,Chong Li,Jiaolong Yang,Dong Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种通过稀疏关键点数据集增强密集人脸关键点检测的框架，无需额外成本即可在多个测试集上达到最优精度。


<details>
  <summary>Details</summary>
Motivation: 当前人脸关键点检测主要关注稀疏关键点，但在美容医学和面部美化等场景中，密集关键点需求较高。现有方法无法满足这一需求。

Method: 通过观察语义轮廓局部特征相似性，提出弱监督学习方法：先在稀疏关键点上学习细化能力，再迁移到密集关键点。设计了多个算子实现该框架，并作为即插即用模块集成到现有网络中。

Result: 在自建的密集300W测试集、原始稀疏300W和WFLW测试集上均达到最优精度，且无需额外计算成本。

Conclusion: 该框架有效提升了关键点密度，在保持稀疏关键点精度的同时实现了密集关键点检测的突破。

Abstract: Recent years have witnessed significant growth of face alignment. Though
dense facial landmark is highly demanded in various scenarios, e.g., cosmetic
medicine and facial beautification, most works only consider sparse face
alignment. To address this problem, we present a framework that can enrich
landmark density by existing sparse landmark datasets, e.g., 300W with 68
points and WFLW with 98 points. Firstly, we observe that the local patches
along each semantic contour are highly similar in appearance. Then, we propose
a weakly-supervised idea of learning the refinement ability on original sparse
landmarks and adapting this ability to enriched dense landmarks. Meanwhile,
several operators are devised and organized together to implement the idea.
Finally, the trained model is applied as a plug-and-play module to the existing
face alignment networks. To evaluate our method, we manually label the dense
landmarks on 300W testset. Our method yields state-of-the-art accuracy not only
in newly-constructed dense 300W testset but also in the original sparse 300W
and WFLW testsets without additional cost.

</details>


### [210] [PEACE: Empowering Geologic Map Holistic Understanding with MLLMs](https://arxiv.org/abs/2501.06184)
*Yangyu Huang,Tianyi Gao,Haoran Xu,Qihao Zhao,Yang Song,Zhipeng Gui,Tengchao Lv,Hao Chen,Lei Cui,Scarlett Li,Furu Wei*

Main category: cs.CV

TL;DR: 该论文提出了首个地质图理解基准GeoMap-Bench和智能体GeoMap-Agent，显著提升了多模态大模型在地质图分析中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在地质图理解方面表现不足，主要由于地图制图的高分辨率、多组件和领域知识需求等挑战。

Method: 构建GeoMap-Bench基准，开发GeoMap-Agent智能体，包含分层信息提取、领域知识注入和提示增强问答三个模块。

Result: GeoMap-Agent在GeoMap-Bench上获得0.811分，远超GPT-4o的0.369分。

Conclusion: 该研究为地质学中的AI应用铺平了道路，提高了地质调查的准确性和效率。

Abstract: Geologic map, as a fundamental diagram in geology science, provides critical
insights into the structure and composition of Earth's subsurface and surface.
These maps are indispensable in various fields, including disaster detection,
resource exploration, and civil engineering. Despite their significance,
current Multimodal Large Language Models (MLLMs) often fall short in geologic
map understanding. This gap is primarily due to the challenging nature of
cartographic generalization, which involves handling high-resolution map,
managing multiple associated components, and requiring domain-specific
knowledge. To quantify this gap, we construct GeoMap-Bench, the first-ever
benchmark for evaluating MLLMs in geologic map understanding, which assesses
the full-scale abilities in extracting, referring, grounding, reasoning, and
analyzing. To bridge this gap, we introduce GeoMap-Agent, the inaugural agent
designed for geologic map understanding, which features three modules:
Hierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI),
and Prompt-enhanced Question Answering (PEQA). Inspired by the
interdisciplinary collaboration among human scientists, an AI expert group acts
as consultants, utilizing a diverse tool pool to comprehensively analyze
questions. Through comprehensive experiments, GeoMap-Agent achieves an overall
score of 0.811 on GeoMap-Bench, significantly outperforming 0.369 of GPT-4o.
Our work, emPowering gEologic mAp holistiC undErstanding (PEACE) with MLLMs,
paves the way for advanced AI applications in geology, enhancing the efficiency
and accuracy of geological investigations.

</details>


### [211] [TanDiT: Tangent-Plane Diffusion Transformer for High-Quality 360° Panorama Generation](https://arxiv.org/abs/2506.21681)
*Hakan Çapuk,Andrew Bond,Muhammed Burak Kızıl,Emir Göçen,Erkut Erdem,Aykut Erdem*

Main category: cs.CV

TL;DR: TanDiT提出了一种通过生成切平面图像网格来合成全景图像的方法，解决了现有模型在全景生成中的几何失真和循环一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型在全景图像生成中面临几何失真和循环一致性等挑战，需要一种新方法来克服这些问题。

Method: TanDiT使用统一的扩散模型在单次去噪迭代中同时生成切平面图像，并提出模型无关的后处理步骤增强全局一致性。

Result: 实验表明，TanDiT能有效泛化到训练数据之外，处理复杂文本提示，并与多种生成模型结合生成高质量全景图像。

Conclusion: TanDiT为全景图像生成提供了一种高效且通用的解决方案，显著提升了生成质量和多样性。

Abstract: Recent advances in image generation have led to remarkable improvements in
synthesizing perspective images. However, these models still struggle with
panoramic image generation due to unique challenges, including varying levels
of geometric distortion and the requirement for seamless loop-consistency. To
address these issues while leveraging the strengths of the existing models, we
introduce TanDiT, a method that synthesizes panoramic scenes by generating
grids of tangent-plane images covering the entire 360$^\circ$ view. Unlike
previous methods relying on multiple diffusion branches, TanDiT utilizes a
unified diffusion model trained to produce these tangent-plane images
simultaneously within a single denoising iteration. Furthermore, we propose a
model-agnostic post-processing step specifically designed to enhance global
coherence across the generated panoramas. To accurately assess panoramic image
quality, we also present two specialized metrics, TangentIS and TangentFID, and
provide a comprehensive benchmark comprising captioned panoramic datasets and
standardized evaluation scripts. Extensive experiments demonstrate that our
method generalizes effectively beyond its training data, robustly interprets
detailed and complex text prompts, and seamlessly integrates with various
generative models to yield high-quality, diverse panoramic images.

</details>


### [212] [Elucidating and Endowing the Diffusion Training Paradigm for General Image Restoration](https://arxiv.org/abs/2506.21722)
*Xin Lu,Xueyang Fu,Jie Xiao,Zihao Fan,Yurui Zhu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 该论文提出了一种基于扩散训练范式的图像修复框架，通过系统分析时间步依赖、网络层次等因素，优化了单任务和多任务统一图像修复的性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在图像修复任务中表现优异，但其复杂架构和迭代过程限制了实际应用。论文旨在将扩散训练范式整合到普通图像修复框架中，提升其泛化能力和多任务适应性。

Method: 论文提出了一系列正则化策略，将扩散目标与图像修复任务对齐，并开发了增量训练范式和任务特定适配器，以优化单任务和多任务统一修复性能。

Result: 实验表明，该方法显著提升了单任务图像修复的泛化能力，并在多任务统一修复中取得了优越性能，且能无缝集成到现有通用架构中。

Conclusion: 该研究成功将扩散训练范式整合到普通图像修复框架中，显著提升了单任务和多任务修复的性能，具有广泛的应用潜力。

Abstract: While diffusion models demonstrate strong generative capabilities in image
restoration (IR) tasks, their complex architectures and iterative processes
limit their practical application compared to mainstream reconstruction-based
general ordinary IR networks. Existing approaches primarily focus on optimizing
network architecture and diffusion paths but overlook the integration of the
diffusion training paradigm within general ordinary IR frameworks. To address
these challenges, this paper elucidates key principles for adapting the
diffusion training paradigm to general IR training through systematic analysis
of time-step dependencies, network hierarchies, noise-level relationships, and
multi-restoration task correlations, proposing a new IR framework supported by
diffusion-based training. To enable IR networks to simultaneously restore
images and model generative representations, we introduce a series of
regularization strategies that align diffusion objectives with IR tasks,
improving generalization in single-task scenarios. Furthermore, recognizing
that diffusion-based generation exerts varying influences across different IR
tasks, we develop an incremental training paradigm and task-specific adaptors,
further enhancing performance in multi-task unified IR. Experiments demonstrate
that our method significantly improves the generalization of IR networks in
single-task IR and achieves superior performance in multi-task unified IR.
Notably, the proposed framework can be seamlessly integrated into existing
general IR architectures.

</details>


### [213] [Exploring Image Generation via Mutually Exclusive Probability Spaces and Local Correlation Hypothesis](https://arxiv.org/abs/2506.21731)
*Chenqiu Zhao,Anup Basu*

Main category: cs.CV

TL;DR: 论文提出MESP和LCH理论框架，指出概率生成模型因学习全局分布导致记忆而非生成行为，并基于此提出BL-AE和ARVM模型，实验显示ARVM虽性能优越但存在记忆问题，最终通过LCH假设强调局部相关性对生成能力的关键作用。


<details>
  <summary>Details</summary>
Motivation: 针对变分自编码器（VAE）中潜在变量分布重叠导致的优化冲突问题，作者发现当前概率生成模型倾向于记忆而非真实生成，这限制了模型的泛化能力。

Method: 提出MESP框架分析VAE潜在空间重叠问题，并设计BL-AE将图像编码为二元潜在表示；进一步提出ARVM模型（基于直方图输出的自回归模型），并通过LCH假设从理论上解释局部相关性对生成行为的影响。

Result: ARVM在标准数据集上取得优于SOTA的FID分数，但实验表明高分反映的是记忆而非生成能力；LCH假设通过局部相关性验证了生成能力的来源。

Conclusion: 全局分布学习易导致记忆问题，而局部相关性（LCH）是生成能力的关键；BL-AE与ARVM为后续研究提供了新方向，但需区分记忆与生成的评价指标。

Abstract: We propose two theoretical frameworks, the Mutually Exclusive Probability
Space (MESP) and the Local Correlation Hypothesis (LCH), to explore a potential
limitation in probabilistic generative models; namely that learning global
distributions leads to memorization rather than generative behavior. MESP
emerges from our rethinking of the Variational Autoencoder (VAE). We observe
that latent variable distributions in VAE exhibit overlap, which leads to an
optimization conflict between the reconstruction loss and KL-divergence loss. A
lower bound based on the overlap coefficient is proposed. We refer to this
phenomenon as Mutually Exclusive Probability Spaces. Based on MESP, a Binary
Latent Autoencoder (BL-AE) is proposed to encode images into binary latent
representations. These binary latents are used as the input to our
Autoregressive Random Variable Model (ARVM), a modified autoregressive model
outputting histograms. Our ARVM achieves competitive FID scores, outperforming
state-of-the-art methods on standard datasets. However, such scores reflect
memorization rather than generation. To address this issue, we propose the
Local Correlation Hypothesis (LCH), which posits that generative capability
arising from local correlations among latent variables. Comprehensive
experiments and discussions are conducted to validate our frameworks.

</details>


### [214] [Comparing Learning Paradigms for Egocentric Video Summarization](https://arxiv.org/abs/2506.21785)
*Daniel Wen*

Main category: cs.CV

TL;DR: 该研究比较了监督学习、无监督学习和提示微调在自我中心视频理解中的表现，发现通用模型GPT-4o优于专用模型，突显当前方法在自我中心视频领域的不足。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估不同计算机视觉范式在自我中心视频数据上的表现，推动该领域的技术进步。

Method: 通过评估Shotluck Holmes（监督学习）、TAC-SUM（无监督学习）和GPT-4o（提示微调）在Ego-Exo4D数据集上的视频摘要能力。

Result: 当前最先进模型在自我中心视频上表现较差，而经过提示微调的GPT-4o优于专用模型。

Conclusion: 自我中心视频处理仍需改进，通用模型的潜力表明现有方法需适应第一人称视角的独特挑战。

Abstract: In this study, we investigate various computer vision paradigms - supervised
learning, unsupervised learning, and prompt fine-tuning - by assessing their
ability to understand and interpret egocentric video data. Specifically, we
examine Shotluck Holmes (state-of-the-art supervised learning), TAC-SUM
(state-of-the-art unsupervised learning), and GPT-4o (a prompt fine-tuned
pre-trained model), evaluating their effectiveness in video summarization. Our
results demonstrate that current state-of-the-art models perform less
effectively on first-person videos compared to third-person videos,
highlighting the need for further advancements in the egocentric video domain.
Notably, a prompt fine-tuned general-purpose GPT-4o model outperforms these
specialized models, emphasizing the limitations of existing approaches in
adapting to the unique challenges of first-person perspectives. Although our
evaluation is conducted on a small subset of egocentric videos from the
Ego-Exo4D dataset due to resource constraints, the primary objective of this
research is to provide a comprehensive proof-of-concept analysis aimed at
advancing the application of computer vision techniques to first-person videos.
By exploring novel methodologies and evaluating their potential, we aim to
contribute to the ongoing development of models capable of effectively
processing and interpreting egocentric perspectives.

</details>


### [215] [Early Glaucoma Detection using Deep Learning with Multiple Datasets of Fundus Images](https://arxiv.org/abs/2506.21770)
*Rishiraj Paul Chowdhury,Nirmit Shekar Karkera*

Main category: cs.CV

TL;DR: 使用EfficientNet-B0深度学习模型，通过多数据集训练提升青光眼检测的泛化能力，简化预处理步骤并取得高AUC-ROC性能。


<details>
  <summary>Details</summary>
Motivation: 青光眼是导致不可逆失明的主要原因，早期检测对治疗效果至关重要。传统诊断方法通常具有侵入性且依赖专业设备，因此需要一种非侵入、高效的检测方法。

Method: 采用EfficientNet-B0架构，依次在ACRIMA、ORIGA和RIM-ONE数据集上进行顺序训练和微调，以增强模型泛化能力，并比较不同预处理方法的效果。

Result: 实验表明，简化预处理步骤相比复杂增强方法能获得更高的AUC-ROC，模型在未见过的数据集上表现出强大的判别性能。

Conclusion: 提出的深度学习流程为青光眼早期检测提供了一种可重复、可扩展的方法，具有潜在的临床应用价值。

Abstract: Glaucoma is a leading cause of irreversible blindness, but early detection
can significantly improve treatment outcomes. Traditional diagnostic methods
are often invasive and require specialized equipment. In this work, we present
a deep learning pipeline using the EfficientNet-B0 architecture for glaucoma
detection from retinal fundus images. Unlike prior studies that rely on single
datasets, we sequentially train and fine-tune our model across ACRIMA, ORIGA,
and RIM-ONE datasets to enhance generalization. Our experiments show that
minimal preprocessing yields higher AUC-ROC compared to more complex
enhancements, and our model demonstrates strong discriminative performance on
unseen datasets. The proposed pipeline offers a reproducible and scalable
approach to early glaucoma detection, supporting its potential clinical
utility.

</details>


### [216] [CAT-SG: A Large Dynamic Scene Graph Dataset for Fine-Grained Understanding of Cataract Surgery](https://arxiv.org/abs/2506.21813)
*Felix Holm,Gözde Ünver,Ghazal Ghazaei,Nassir Navab*

Main category: cs.CV

TL;DR: 该论文提出了首个白内障手术场景图数据集CAT-SG，用于建模手术工具、解剖结构和操作技术之间的复杂交互关系，并提出了新的场景图生成模型CatSGG，提升了手术工作流分析的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有数据集仅关注手术分析的孤立方面（如工具检测或阶段分割），缺乏对实体间语义关系和时间依赖性的全面表示，因此需要更全面的数据集来支持手术工作流分析。

Method: 论文引入了CAT-SG数据集，提供工具-组织交互、操作变体和时间依赖性的结构化标注，并开发了新的场景图生成模型CatSGG。

Result: CAT-SG数据集能够更全面地表示手术工作流，CatSGG模型在生成结构化手术表示方面优于现有方法。

Conclusion: CAT-SG数据集和CatSGG模型为AI驱动的手术培训、实时决策支持和临床实践中的智能系统提供了重要基础。

Abstract: Understanding the intricate workflows of cataract surgery requires modeling
complex interactions between surgical tools, anatomical structures, and
procedural techniques. Existing datasets primarily address isolated aspects of
surgical analysis, such as tool detection or phase segmentation, but lack
comprehensive representations that capture the semantic relationships between
entities over time. This paper introduces the Cataract Surgery Scene Graph
(CAT-SG) dataset, the first to provide structured annotations of tool-tissue
interactions, procedural variations, and temporal dependencies. By
incorporating detailed semantic relations, CAT-SG offers a holistic view of
surgical workflows, enabling more accurate recognition of surgical phases and
techniques. Additionally, we present a novel scene graph generation model,
CatSGG, which outperforms current methods in generating structured surgical
representations. The CAT-SG dataset is designed to enhance AI-driven surgical
training, real-time decision support, and workflow analysis, paving the way for
more intelligent, context-aware systems in clinical practice.

</details>


### [217] [Few-Shot Segmentation of Historical Maps via Linear Probing of Vision Foundation Models](https://arxiv.org/abs/2506.21826)
*Rafael Sterzinger,Marco Peer,Robert Sablatnig*

Main category: cs.CV

TL;DR: 提出一种基于大型视觉基础模型和参数高效微调的少样本历史地图分割方法，显著提升分割精度并减少人工标注需求。


<details>
  <summary>Details</summary>
Motivation: 历史地图作为丰富的历史资料，其多样化的视觉表现和有限的标注数据给自动化处理带来挑战。

Method: 结合大型视觉基础模型的丰富语义嵌入和参数高效微调技术，实现少样本历史地图分割。

Result: 在Siegfried数据集上，葡萄园和铁路分割的mIoU相对提升5%和13%（10样本）及约20%（5样本）；在ICDAR 2021数据集上建筑区块分割平均PQ达67.3%。

Conclusion: 该方法在极低数据量（10/5样本）下仍保持高性能，仅需68.9万可训练参数（占模型总量0.21%），显著推进历史地图自动化分析。

Abstract: As rich sources of history, maps provide crucial insights into historical
changes, yet their diverse visual representations and limited annotated data
pose significant challenges for automated processing. We propose a simple yet
effective approach for few-shot segmentation of historical maps, leveraging the
rich semantic embeddings of large vision foundation models combined with
parameter-efficient fine-tuning. Our method outperforms the state-of-the-art on
the Siegfried benchmark dataset in vineyard and railway segmentation, achieving
+5% and +13% relative improvements in mIoU in 10-shot scenarios and around +20%
in the more challenging 5-shot setting. Additionally, it demonstrates strong
performance on the ICDAR 2021 competition dataset, attaining a mean PQ of 67.3%
for building block segmentation, despite not being optimized for this
shape-sensitive metric, underscoring its generalizability. Notably, our
approach maintains high performance even in extremely low-data regimes (10- &
5-shot), while requiring only 689k trainable parameters - just 0.21% of the
total model size. Our approach enables precise segmentation of diverse
historical maps while drastically reducing the need for manual annotations,
advancing automated processing and analysis in the field. Our implementation is
publicly available at:
https://github.com/RafaelSterzinger/few-shot-map-segmentation.

</details>


### [218] [SPADE: Spatial Transcriptomics and Pathology Alignment Using a Mixture of Data Experts for an Expressive Latent Space](https://arxiv.org/abs/2506.21857)
*Ekaterina Redekop,Mara Pleasure,Zichen Wang,Kimberly Flores,Anthony Sisk,William Speier,Corey W. Arnold*

Main category: cs.CV

TL;DR: SPADE模型整合组织病理学与空间转录组数据，通过多专家混合技术提升病理图像表征学习，在14项下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前数字病理学虽快速发展，但全切片图像（WSI）与空间转录组（ST）数据的全面整合仍存在空白，限制了分子异质性的深入探索。

Method: 提出SPADE基础模型，采用两阶段特征空间聚类和对比学习，在统一框架内整合WSI与ST数据，构建ST信息引导的潜在空间。

Result: 在HEST-1k数据集预训练后，SPADE在14项下游任务中展现出显著优于基线模型的少样本性能。

Conclusion: SPADE证明了形态学与分子信息整合的潜力，为病理学基础模型提供了新方向。

Abstract: The rapid growth of digital pathology and advances in self-supervised deep
learning have enabled the development of foundational models for various
pathology tasks across diverse diseases. While multimodal approaches
integrating diverse data sources have emerged, a critical gap remains in the
comprehensive integration of whole-slide images (WSIs) with spatial
transcriptomics (ST), which is crucial for capturing critical molecular
heterogeneity beyond standard hematoxylin & eosin (H&E) staining. We introduce
SPADE, a foundation model that integrates histopathology with ST data to guide
image representation learning within a unified framework, in effect creating an
ST-informed latent space. SPADE leverages a mixture-of-data experts technique,
where experts, created via two-stage feature-space clustering, use contrastive
learning to learn representations of co-registered WSI patches and gene
expression profiles. Pre-trained on the comprehensive HEST-1k dataset, SPADE is
evaluated on 14 downstream tasks, demonstrating significantly superior few-shot
performance compared to baseline models, highlighting the benefits of
integrating morphological and molecular information into one latent space.

</details>


### [219] [LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs](https://arxiv.org/abs/2506.21862)
*Boyuan Sun,Jiaxing Zhao,Xihan Wei,Qibin Hou*

Main category: cs.CV

TL;DR: LLaVA-Scissor提出了一种无需训练的token压缩策略，通过语义连通分量（SCC）在时空域压缩视频多模态大模型的token，显著提升视频理解任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力得分的token压缩方法无法有效覆盖所有语义区域且易导致冗余，需一种更全面的语义压缩策略。

Method: 采用语义连通分量（SCC）将token划分到不同语义区域，提出两步时空token压缩策略，生成无重叠的压缩token集。

Result: 在视频问答、长视频理解等任务中，LLaVA-Scissor在低token保留率下性能优于其他压缩方法。

Conclusion: LLaVA-Scissor通过SCC实现高效token压缩，为视频多模态大模型提供了更优的语义表示方案。

Abstract: In this paper, we present LLaVA-Scissor, a training-free token compression
strategy designed for video multimodal large language models. Previous methods
mostly attempt to compress tokens based on attention scores, but fail to
effectively capture all semantic regions and often lead to token redundancy.
Differently, we propose to leverage the Semantic Connected Components (SCC)
approach that assigns tokens to distinct semantic regions within the token set,
ensuring comprehensive semantic coverage. The outcome is a two-step
spatio-temporal token compression strategy that utilizes SCC in both spatial
and temporal domains. This strategy can effectively compress tokens by
representing the entire video with a set of non-overlapping semantic tokens. We
conduct extensive evaluations of the token compression capabilities of
LLaVA-Scissor across diverse video understanding benchmarks, including video
question answering, long video understanding, and comprehensive multi-choices
benchmarks. Experimental results show that the proposed LLaVA-Scissor
outperforms other token compression methods, achieving superior performance in
various video understanding benchmarks, particularly at low token retention
ratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.

</details>


### [220] [Grounding-Aware Token Pruning: Recovering from Drastic Performance Drops in Visual Grounding Caused by Pruning](https://arxiv.org/abs/2506.21873)
*Tzu-Chun Chien,Chieh-Kai Lin,Shiang-Feng Tsai,Ruei-Chi Lai,Hung-Jen Chen,Min Sun*

Main category: cs.CV

TL;DR: 论文提出GAP方法，通过调整位置ID解决多模态大语言模型在视觉定位任务中因token剪枝导致的性能下降问题，无需额外训练或计算开销。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视觉定位任务中表现优异，但处理大量视觉token带来高计算成本。现有token剪枝方法虽能降低成本，却严重损害模型的定位能力，导致性能大幅下降。

Method: 提出Grounding-Aware Token Pruning (GAP)，一种简单有效的位置ID调整方法，解决剪枝后位置ID错位问题，无需额外训练、内存或计算开销。

Result: GAP方法在RefCOCO验证集上将LLaVA的准确率从剪枝后的15.34%恢复至51.42%，达到原始性能的90%，并在Shikra、MiniGPTv2等模型上一致提升性能。

Conclusion: GAP方法有效解决了token剪枝导致的视觉定位性能下降问题，为多模态大语言模型的高效应用提供了实用解决方案。

Abstract: Recent Multimodal Large Language Models (MLLMs) have demonstrated strong
performance in visual grounding, establishing themselves as a general interface
for various vision-language applications. This progress has driven the
development of token pruning methods to mitigate the high computational costs
associated with processing numerous visual tokens. However, we observe that
pruning significantly weakens the model's grounding ability, leading to
incorrect predictions and drastic performance degradation. In Referring
Expression Comprehension (REC), for instance, pruning causes the accuracy of
LLaVA on the RefCOCO validation set to drop from 56.14% to 15.34%. Our analysis
identifies misaligned position IDs after pruning as the primary cause of this
degradation, as both the order and value of these IDs are crucial for
maintaining performance in grounding tasks. To address this issue, we propose
Grounding-Aware Token Pruning (GAP), a simple yet effective adjustment to
position IDs that recovers REC accuracy back to 51.42%, which is 90% of the
original performance in the without pruning setting, all while requiring no
additional training, memory, or computational overhead. Applied to models such
as Shikra, MiniGPTv2, and the LLaVA series, our method consistently improves
performance across various token pruning strategies.

</details>


### [221] [SODA: Out-of-Distribution Detection in Domain-Shifted Point Clouds via Neighborhood Propagation](https://arxiv.org/abs/2506.21892)
*Adam Goodge,Xun Xu,Bryan Hooi,Wee Siong Ng,Jingyi Liao,Yongyi Su,Xulei Yang*

Main category: cs.CV

TL;DR: 提出SODA方法，通过邻域分数传播提升点云数据的OOD检测性能，无需额外训练且效果显著。


<details>
  <summary>Details</summary>
Motivation: 点云数据在应用中日益普及，但现有研究对OOD检测关注不足。3D视觉语言模型因预训练数据规模小、多样性低（多为合成数据），导致实际应用时存在领域偏移问题。

Method: 提出SODA方法，基于邻域的分数传播机制改进3D VLM中点云与文本嵌入的对齐，无需额外训练。

Result: SODA在多种数据集和问题设置下均优于现有方法，实现SOTA性能。

Conclusion: SODA有效缓解合成到真实数据的领域偏移，显著提升点云OOD检测的可靠性。

Abstract: As point cloud data increases in prevalence in a variety of applications, the
ability to detect out-of-distribution (OOD) point cloud objects becomes
critical for ensuring model safety and reliability. However, this problem
remains under-explored in existing research. Inspired by success in the image
domain, we propose to exploit advances in 3D vision-language models (3D VLMs)
for OOD detection in point cloud objects. However, a major challenge is that
point cloud datasets used to pre-train 3D VLMs are drastically smaller in size
and object diversity than their image-based counterparts. Critically, they
often contain exclusively computer-designed synthetic objects. This leads to a
substantial domain shift when the model is transferred to practical tasks
involving real objects scanned from the physical environment. In this paper,
our empirical experiments show that synthetic-to-real domain shift
significantly degrades the alignment of point cloud with their associated text
embeddings in the 3D VLM latent space, hindering downstream performance. To
address this, we propose a novel methodology called SODA which improves the
detection of OOD point clouds through a neighborhood-based score propagation
scheme. SODA is inference-based, requires no additional model training, and
achieves state-of-the-art performance over existing approaches across datasets
and problem settings.

</details>


### [222] [SDRNET: Stacked Deep Residual Network for Accurate Semantic Segmentation of Fine-Resolution Remotely Sensed Images](https://arxiv.org/abs/2506.21945)
*Naftaly Wambugu,Ruisheng Wang,Bo Guo,Tianshu Yu,Sheng Xu,Mohammed Elhassan*

Main category: cs.CV

TL;DR: 该论文介绍了一种名为SDRNet的堆叠深度残差网络，用于高分辨率遥感图像的语义分割，旨在解决类间差异、遮挡和物体尺寸变化带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 高分辨率遥感图像的语义分割面临类间差异、遮挡和物体尺寸变化等挑战，现有深度卷积神经网络在特征学习和表示方面仍有不足，需要更鲁棒的特征提取方法。

Method: 提出SDRNet框架，采用两个堆叠的编码器-解码器网络以保留空间信息，并在编码器和解码器之间使用扩张残差块（DRB）捕获全局依赖关系。

Result: 在ISPRS Vaihingen和Potsdam数据集上的实验表明，SDRNet在语义分割任务中表现优异，优于当前其他深度卷积神经网络。

Conclusion: SDRNet通过多上下文特征学习和全局-局部上下文结合，有效提升了高分辨率遥感图像的语义分割性能。

Abstract: Land cover maps generated from semantic segmentation of high-resolution
remotely sensed images have drawn mucon in the photogrammetry and remote
sensing research community. Currently, massive fine-resolution remotely sensed
(FRRS) images acquired by improving sensing and imaging technologies become
available. However, accurate semantic segmentation of such FRRS images is
greatly affected by substantial class disparities, the invisibility of key
ground objects due to occlusion, and object size variation. Despite the
extraordinary potential in deep convolutional neural networks (DCNNs) in image
feature learning and representation, extracting sufficient features from FRRS
images for accurate semantic segmentation is still challenging. These
challenges demand the deep learning models to learn robust features and
generate sufficient feature descriptors. Specifically, learning
multi-contextual features to guarantee adequate coverage of varied object sizes
from the ground scene and harnessing global-local contexts to overcome class
disparities challenge even profound networks. Deeper networks significantly
lose spatial details due to gradual downsampling processes resulting in poor
segmentation results and coarse boundaries. This article presents a stacked
deep residual network (SDRNet) for semantic segmentation from FRRS images. The
proposed framework utilizes two stacked encoder-decoder networks to harness
long-range semantics yet preserve spatial information and dilated residual
blocks (DRB) between each encoder and decoder network to capture sufficient
global dependencies thus improving segmentation performance. Our experimental
results obtained using the ISPRS Vaihingen and Potsdam datasets demonstrate
that the SDRNet performs effectively and competitively against current DCNNs in
semantic segmentation.

</details>


### [223] [Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs](https://arxiv.org/abs/2506.21656)
*Yifan Shen,Yuanzhe Liu,Jingyuan Zhu,Xu Cao,Xiaofeng Zhang,Yixiao He,Wenming Ye,James Matthew Rehg,Ismini Lourentzou*

Main category: cs.CV

TL;DR: 提出SpatialReasoner-R1模型，通过M3CTS生成高质量空间推理数据，结合fDPO优化方法，显著提升细粒度空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在细粒度空间推理和多步逻辑对齐方面表现不足，需改进。

Method: 采用M3CTS生成多样化的长链思维推理轨迹，并提出fDPO方法，结合空间奖励机制优化描述性接地和逻辑一致性。

Result: fDPO在空间质量和数量任务上分别提升4.1%和9.0%，SpatialReasoner-R1在SPATIALRGPT-Bench上刷新SOTA，平均提升9.8%。

Conclusion: SpatialReasoner-R1有效解决了细粒度空间推理问题，同时保持通用视觉语言任务的竞争力。

Abstract: Current Vision-Language Models (VLMs) struggle with fine-grained spatial
reasoning, particularly when multi-step logic and precise spatial alignment are
required. In this work, we introduce SpatialReasoner-R1, a vision-language
reasoning model designed to address these limitations. To construct
high-quality supervision for spatial reasoning, we design a Multi-Model Monte
Carlo Tree Search (M3CTS) method that generates diverse, logically consistent
Long Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose
fine-grained Direct Preference Optimization (fDPO), which introduces
segment-specific preference granularity for descriptive grounding and logical
reasoning, guided by a spatial reward mechanism that evaluates candidate
responses based on visual consistency, spatial grounding, and logical
coherence. Experimental results demonstrate that fDPO achieves an average
improvement of 4.1% over standard DPO across spatial quality tasks, and a 9.0%
gain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a
new SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in
average accuracy, while maintaining competitive performance on general
vision-language tasks.

</details>


### [224] [Tied Prototype Model for Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2506.22101)
*Hyeongji Kim,Stine Hansen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: TPM改进ADNet，通过绑定原型和自适应阈值提升医学图像少样本分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有ADNet方法在医学图像少样本分割中存在单原型依赖、二分类局限和固定阈值问题，需改进以适应复杂背景和多类分割。

Method: 提出Tied Prototype Model (TPM)，绑定前景与背景分布的原型位置，支持多原型和多类分割，并利用类先验定义自适应阈值。

Result: TPM在分割准确率上表现更优，尤其在多原型扩展和非典型背景特征分离方面效果显著。

Conclusion: TPM为基于原型的医学图像少样本分割提供了新思路，代码已发布。

Abstract: Common prototype-based medical image few-shot segmentation (FSS) methods
model foreground and background classes using class-specific prototypes.
However, given the high variability of the background, a more promising
direction is to focus solely on foreground modeling, treating the background as
an anomaly -- an approach introduced by ADNet. Yet, ADNet faces three key
limitations: dependence on a single prototype per class, a focus on binary
classification, and fixed thresholds that fail to adapt to patient and organ
variability. To address these shortcomings, we propose the Tied Prototype Model
(TPM), a principled reformulation of ADNet with tied prototype locations for
foreground and background distributions. Building on its probabilistic
foundation, TPM naturally extends to multiple prototypes and multi-class
segmentation while effectively separating non-typical background features.
Notably, both extensions lead to improved segmentation accuracy. Finally, we
leverage naturally occurring class priors to define an ideal target for
adaptive thresholds, boosting segmentation performance. Taken together, TPM
provides a fresh perspective on prototype-based FSS for medical image
segmentation. The code can be found at https://github.com/hjk92g/TPM-FSS.

</details>


### [225] [Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs](https://arxiv.org/abs/2506.22146)
*Amirmohammad Izadi,Mohammad Ali Banayeeanzade,Fatemeh Askari,Ali Rahimiakbar,Mohammad Mahdi Vahedi,Hosein Hasani,Mahdieh Soleymani Baghshah*

Main category: cs.CV

TL;DR: 通过在视觉输入中添加低层次空间结构（如水平线）并结合文本提示，显著提升了视觉语言模型在视觉推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLMs）在处理视觉特征时缺乏空间基础、序列注意力机制，导致在计数、视觉搜索、场景描述和空间关系理解等任务中表现不佳。

Method: 在视觉输入中增加低层次空间结构（如水平线），并配合鼓励序列化、空间感知解析的文本提示。

Result: 方法显著提升了多项视觉推理任务的性能：GPT-4o的视觉搜索准确率提高25.00%，计数准确率提高26.83%，场景描述的编辑距离误差减少0.32，空间关系任务在2D合成数据集上提升9.50%。

Conclusion: 低层次视觉结构调整是提升视觉语言模型在空间基础任务上性能的有效且未被充分探索的方向，其效果优于纯语言策略。

Abstract: Despite progress in Vision-Language Models (VLMs), their capacity for visual
reasoning is often limited by the \textit{binding problem}: the failure to
reliably associate perceptual features with their correct visual referents.
This limitation underlies persistent errors in tasks such as counting, visual
search, scene description, and spatial relationship understanding. A key factor
is that current VLMs process visual features largely in parallel, lacking
mechanisms for spatially grounded, serial attention. This paper introduces a
simple yet effective intervention: augmenting visual inputs with low-level
spatial structures (e.g., horizontal lines) and pairing this with a textual
prompt that encourages sequential, spatially-aware parsing. We empirically
demonstrate substantial performance improvements across core visual reasoning
tasks. Specifically, our method improves GPT-4o visual search accuracy by
25.00%, increases counting accuracy by 26.83%, reduces edit distance error in
scene description by 0.32, and enhances performance on spatial relationship
tasks by 9.50% on a a 2D synthetic dataset. Furthermore, we find that the
visual modification is essential for these gains; purely textual strategies,
including Chain-of-Thought prompting, are insufficient and can even degrade
performance. Our method enhances binding only with a single-query inference,
underscoring the importance of visual input design over purely
linguistically-based approaches. These findings suggest that low-level visual
structuring is a powerful and underexplored direction for improving
compositional visual reasoning and could serve as a general strategy for
enhancing VLM performance on spatially grounded tasks.

</details>


### [226] [GenEscape: Hierarchical Multi-Agent Generation of Escape Room Puzzles](https://arxiv.org/abs/2506.21839)
*Mengyi Shan,Brian Curless,Ira Kemelmacher-Shlizerman,Steve Seitz*

Main category: cs.CV

TL;DR: 提出分层多智能体框架改进文本生成图像模型，生成视觉吸引、逻辑严谨的解谜房间图像。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成图像模型在空间关系和功能推理上表现不佳，难以生成符合逻辑且视觉吸引的解谜场景。

Method: 采用分层多智能体框架，分阶段处理功能设计、符号场景图推理、布局合成和局部图像编辑，通过智能体协作优化结果。

Result: 实验表明该方法提升了生成图像的可解性、避免捷径问题及功能清晰度，同时保持视觉质量。

Conclusion: 智能体协作能有效改进文本生成图像模型在复杂场景中的表现，平衡逻辑严谨性与视觉吸引力。

Abstract: We challenge text-to-image models with generating escape room puzzle images
that are visually appealing, logically solid, and intellectually stimulating.
While base image models struggle with spatial relationships and affordance
reasoning, we propose a hierarchical multi-agent framework that decomposes this
task into structured stages: functional design, symbolic scene graph reasoning,
layout synthesis, and local image editing. Specialized agents collaborate
through iterative feedback to ensure the scene is visually coherent and
functionally solvable. Experiments show that agent collaboration improves
output quality in terms of solvability, shortcut avoidance, and affordance
clarity, while maintaining visual quality.

</details>


### [227] [Boosting Classification with Quantum-Inspired Augmentations](https://arxiv.org/abs/2506.22241)
*Matthias Tschöpe,Vitor Fortes Rey,Sogo Pierre Sanon,Paul Lukowicz,Nikolaos Palaiodimopoulos,Maximilian Kiefer-Emmanouilidis*

Main category: cs.CV

TL;DR: 量子门扰动在量子机器学习中可能带来优势，本文提出一种基于随机Bloch球旋转的量子启发数据增强方法，在ImageNet上提升分类性能，但未增强差分隐私。


<details>
  <summary>Details</summary>
Motivation: 研究量子门扰动对量子机器学习的影响，探索其作为数据增强的潜力，并验证其在经典硬件上的可模拟性。

Method: 采用随机Bloch球旋转（SU(2)变换）作为量子启发数据增强技术，直接应用于经典数据，并与传统增强方法对比。

Result: 在ImageNet数据集上，Top-1准确率提升3%，Top-5提升2.5%，F1分数从8%增至12%；但强酉变换未增强差分隐私。

Conclusion: 量子扰动可有效提升经典机器学习性能，但需注意其隐私保护局限性。

Abstract: Understanding the impact of small quantum gate perturbations, which are
common in quantum digital devices but absent in classical computers, is crucial
for identifying potential advantages in quantum machine learning. While these
perturbations are typically seen as detrimental to quantum computation, they
can actually enhance performance by serving as a natural source of data
augmentation. Additionally, they can often be efficiently simulated on
classical hardware, enabling quantum-inspired approaches to improve classical
machine learning methods. In this paper, we investigate random Bloch sphere
rotations, which are fundamental SU(2) transformations, as a simple yet
effective quantum-inspired data augmentation technique. Unlike conventional
augmentations such as flipping, rotating, or cropping, quantum transformations
lack intuitive spatial interpretations, making their application to tasks like
image classification less straightforward. While common quantum augmentation
methods rely on applying quantum models or trainable quanvolutional layers to
classical datasets, we focus on the direct application of small-angle Bloch
rotations and their effect on classical data. Using the large-scale ImageNet
dataset, we demonstrate that our quantum-inspired augmentation method improves
image classification performance, increasing Top-1 accuracy by 3%, Top-5
accuracy by 2.5%, and the F$_1$ score from 8% to 12% compared to standard
classical augmentation methods. Finally, we examine the use of stronger unitary
augmentations. Although these transformations preserve information in
principle, they result in visually unrecognizable images with potential
applications for privacy computations. However, we show that our augmentation
approach and simple SU(2) transformations do not enhance differential privacy
and discuss the implications of this limitation.

</details>


### [228] [Frequency-Semantic Enhanced Variational Autoencoder for Zero-Shot Skeleton-based Action Recognition](https://arxiv.org/abs/2506.22179)
*Wenhan Wu,Zhishuai Guo,Chen Chen,Hongfei Xue,Aidong Lu*

Main category: cs.CV

TL;DR: 提出FS-VAE模型，通过频率分解增强骨架语义表示学习，改进零样本动作识别。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注视觉与语义对齐，但忽略了语义空间中细粒度动作模式的重要性（如喝水与刷牙的手部动作差异）。

Method: FS-VAE包含三个核心组件：1) 基于频率的高低频调整模块；2) 多级对齐的语义动作描述；3) 校准的交叉对齐损失函数。

Result: 在基准测试中验证了频率增强的语义特征能有效区分视觉和语义相似的动作类别。

Conclusion: 频率分解与语义增强的结合显著提升了零样本骨架动作识别的鲁棒性。

Abstract: Zero-shot skeleton-based action recognition aims to develop models capable of
identifying actions beyond the categories encountered during training. Previous
approaches have primarily focused on aligning visual and semantic
representations but often overlooked the importance of fine-grained action
patterns in the semantic space (e.g., the hand movements in drinking water and
brushing teeth). To address these limitations, we propose a Frequency-Semantic
Enhanced Variational Autoencoder (FS-VAE) to explore the skeleton semantic
representation learning with frequency decomposition. FS-VAE consists of three
key components: 1) a frequency-based enhancement module with high- and
low-frequency adjustments to enrich the skeletal semantics learning and improve
the robustness of zero-shot action recognition; 2) a semantic-based action
description with multilevel alignment to capture both local details and global
correspondence, effectively bridging the semantic gap and compensating for the
inherent loss of information in skeleton sequences; 3) a calibrated
cross-alignment loss that enables valid skeleton-text pairs to counterbalance
ambiguous ones, mitigating discrepancies and ambiguities in skeleton and text
features, thereby ensuring robust alignment. Evaluations on the benchmarks
demonstrate the effectiveness of our approach, validating that
frequency-enhanced semantic features enable robust differentiation of visually
and semantically similar action clusters, improving zero-shot action
recognition.

</details>


### [229] [From Ground to Air: Noise Robustness in Vision Transformers and CNNs for Event-Based Vehicle Classification with Potential UAV Applications](https://arxiv.org/abs/2506.22360)
*Nouf Almesafri,Hector Figueiredo,Miguel Arana-Catania*

Main category: cs.CV

TL;DR: 该研究比较了CNN（ResNet34）和ViT（ViT B16）在事件相机数据上的性能，发现ResNet34准确率略高（88% vs 86%），但ViT在小数据集预训练下展现更强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探索卷积神经网络和视觉Transformer在动态事件相机数据上的适用性，为无人机、自动驾驶等动态场景提供视觉解决方案。

Method: 使用GEN1事件相机数据集对ResNet34和ViT B16进行微调，并在标准条件和模拟噪声环境下评估模型性能。

Result: ResNet34分类准确率88%，ViT B16为86%；ViT在噪声环境下表现出更优的鲁棒性。

Conclusion: 尽管ResNet34精度略高，但ViT的鲁棒性优势使其更适合实际动态场景，方法论可扩展至航空领域目标分类任务。

Abstract: This study investigates the performance of the two most relevant computer
vision deep learning architectures, Convolutional Neural Network and Vision
Transformer, for event-based cameras. These cameras capture scene changes,
unlike traditional frame-based cameras with capture static images, and are
particularly suited for dynamic environments such as UAVs and autonomous
vehicles. The deep learning models studied in this work are ResNet34 and ViT
B16, fine-tuned on the GEN1 event-based dataset. The research evaluates and
compares these models under both standard conditions and in the presence of
simulated noise. Initial evaluations on the clean GEN1 dataset reveal that
ResNet34 and ViT B16 achieve accuracies of 88% and 86%, respectively, with
ResNet34 showing a slight advantage in classification accuracy. However, the
ViT B16 model demonstrates notable robustness, particularly given its
pre-training on a smaller dataset. Although this study focuses on ground-based
vehicle classification, the methodologies and findings hold significant promise
for adaptation to UAV contexts, including aerial object classification and
event-based vision systems for aviation-related tasks.

</details>


### [230] [COOCO -- Common Objects Out-of-Context -- Semantic Violation in Scenes: Investigating Multimodal Context in Referential Communication](https://arxiv.org/abs/2506.22274)
*Filippo Merlo,Ece Takmaz,Wenkai Chen,Albert Gatt*

Main category: cs.CV

TL;DR: 论文研究了视觉语言模型(VLMs)如何利用场景上下文进行物体识别，发现模型会根据场景-物体语义相关性和噪声水平动态调整对上下文的依赖。


<details>
  <summary>Details</summary>
Motivation: 探讨视觉语言模型是否像人类一样依赖场景上下文来识别和引用物体，特别是在不同场景-物体一致性和干扰条件下的表现。

Method: 引入COOCO数据集，测试VLMs在不同场景-物体一致性和干扰条件下对场景上下文的依赖程度，并通过注意力机制分析模型行为。

Result: 模型会自适应地利用场景上下文，尤其在场景-物体高度一致或物体信息受损时更依赖上下文。注意力分析显示，中等噪声下模型更关注目标物体。

Conclusion: VLMs能动态平衡局部和上下文信息进行物体识别，其表现受场景-物体相关性和噪声水平影响。

Abstract: Natural scenes provide us with rich contexts for object recognition and
reference. In particular, knowing what type of scene one is looking at
generates expectations about which objects will occur, and what their spatial
configuration should be. Do Vision-Language Models (VLMs) learn to rely on
scene contexts in a similar way, when generating references to objects? To
address this question, we introduce the \textit{Common Objects Out-of-Context
(COOCO)} dataset and test to what extent VLMs rely on scene context to refer to
objects under different degrees of scene-object congruency, and different
perturbations. Our findings show that models leverage scene context adaptively,
depending on both the semantic relatedness between object and scene and the
level of noise. In particular, models rely more on context under high
target-scene congruence or when objects are degraded. Attention analysis
reveals that successful object categorisation involves increased focus on the
target in mid-level layers, especially under moderate noise, suggesting that
VLMs dynamically balance local and contextual information for reference
generation. We make our dataset, code and models available at
\href{https://github.com/cs-nlp-uu/scenereg}{https://github.com/cs-nlp-uu/scenereg}.

</details>


### [231] [RoomCraft: Controllable and Complete 3D Indoor Scene Generation](https://arxiv.org/abs/2506.22291)
*Mengqi Zhou,Xipeng Wang,Yuxi Wang,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: RoomCraft提出了一种多阶段流程，将图像、草图或文本转换为连贯的3D室内场景，通过约束优化和冲突感知策略解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成3D室内场景时存在几何一致性差、空间关系处理不足及多约束下家具碰撞等问题，需要一种更灵活、可控的解决方案。

Method: 结合场景生成流程与约束驱动优化框架，使用结构化信息提取、空间关系网络和HDFS算法生成布局，并通过CAPS策略动态调整家具位置。

Result: 实验表明，RoomCraft在多种输入模态下均能生成更真实、语义连贯且视觉吸引人的房间布局，显著优于现有方法。

Conclusion: RoomCraft通过多阶段优化和冲突感知策略，有效解决了复杂约束下的3D场景生成问题，提升了生成质量与可控性。

Abstract: Generating realistic 3D indoor scenes from user inputs remains a challenging
problem in computer vision and graphics, requiring careful balance of geometric
consistency, spatial relationships, and visual realism. While neural generation
methods often produce repetitive elements due to limited global spatial
reasoning, procedural approaches can leverage constraints for controllable
generation but struggle with multi-constraint scenarios. When constraints
become numerous, object collisions frequently occur, forcing the removal of
furniture items and compromising layout completeness.
  To address these limitations, we propose RoomCraft, a multi-stage pipeline
that converts real images, sketches, or text descriptions into coherent 3D
indoor scenes. Our approach combines a scene generation pipeline with a
constraint-driven optimization framework. The pipeline first extracts
high-level scene information from user inputs and organizes it into a
structured format containing room type, furniture items, and spatial relations.
It then constructs a spatial relationship network to represent furniture
arrangements and generates an optimized placement sequence using a
heuristic-based depth-first search (HDFS) algorithm to ensure layout coherence.
To handle complex multi-constraint scenarios, we introduce a unified constraint
representation that processes both formal specifications and natural language
inputs, enabling flexible constraint-oriented adjustments through a
comprehensive action space design. Additionally, we propose a Conflict-Aware
Positioning Strategy (CAPS) that dynamically adjusts placement weights to
minimize furniture collisions and ensure layout completeness.
  Extensive experiments demonstrate that RoomCraft significantly outperforms
existing methods in generating realistic, semantically coherent, and visually
appealing room layouts across diverse input modalities.

</details>


### [232] [A Deep Learning framework for building damage assessment using VHR SAR and geospatial data: demonstration on the 2023 Turkiye Earthquake](https://arxiv.org/abs/2506.22338)
*Luigi Russo,Deodato Tapete,Silvia Liberata Ullo,Paolo Gamba*

Main category: cs.CV

TL;DR: 提出了一种新型多模态深度学习框架，利用单日期高分辨率SAR图像和辅助地理空间数据，无需灾前图像即可快速评估建筑物损坏情况。


<details>
  <summary>Details</summary>
Motivation: 传统光学卫星影像在灾害测绘中常受云层遮挡或缺乏灾前数据的限制，亟需一种不依赖灾前影像的快速建筑物损坏检测方法。

Method: 整合COSMO SkyMed SAR图像、OSM建筑轮廓、数字表面模型及GEM暴露属性，构建仅需灾后数据的多模态深度学习模型。

Result: 在2023年土耳其地震数据集上验证显示，结合地理空间特征显著提升了检测精度和对新区域的泛化能力。

Conclusion: 该框架为灾害应急响应提供了自动化、可扩展的解决方案，突破了传统方法对灾前数据的依赖。

Abstract: Building damage identification shortly after a disaster is crucial for
guiding emergency response and recovery efforts. Although optical satellite
imagery is commonly used for disaster mapping, its effectiveness is often
hampered by cloud cover or the absence of pre-event acquisitions. To overcome
these challenges, we introduce a novel multimodal deep learning (DL) framework
for detecting building damage using single-date very high resolution (VHR)
Synthetic Aperture Radar (SAR) imagery from the Italian Space Agency (ASI)
COSMO SkyMed (CSK) constellation, complemented by auxiliary geospatial data.
Our method integrates SAR image patches, OpenStreetMap (OSM) building
footprints, digital surface model (DSM) data, and structural and exposure
attributes from the Global Earthquake Model (GEM) to improve detection accuracy
and contextual interpretation. Unlike existing approaches that depend on pre
and post event imagery, our model utilizes only post event data, facilitating
rapid deployment in critical scenarios. The framework effectiveness is
demonstrated using a new dataset from the 2023 earthquake in Turkey, covering
multiple cities with diverse urban settings. Results highlight that
incorporating geospatial features significantly enhances detection performance
and generalizability to previously unseen areas. By combining SAR imagery with
detailed vulnerability and exposure information, our approach provides reliable
and rapid building damage assessments without the dependency from available
pre-event data. Moreover, the automated and scalable data generation process
ensures the framework's applicability across diverse disaster-affected regions,
underscoring its potential to support effective disaster management and
recovery efforts. Code and data will be made available upon acceptance of the
paper.

</details>


### [233] [Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment](https://arxiv.org/abs/2506.22385)
*Yue Zhang,Jilei Sun,Yunhui Guo,Vibhav Gogate*

Main category: cs.CV

TL;DR: 论文提出DVidE任务，通过反事实推理和ASR增强视频内容，提升视频大模型在动态推理上的能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频大模型在抽象和适应性推理上表现不足，无法根据新信息更新解释。现实场景中结论常需动态调整，因此需要新的任务和方法来增强模型的动态推理能力。

Method: 提出DVidE任务，包含分类和生成两个版本。分类任务采用反事实思维链框架，结合ASR增强视频内容和理由细化；生成任务结合ASR输出和大语言模型生成上下文相关的更新。

Result: 实验结果表明，所提方法显著提升了视频大模型的动态推理能力，并通过新构建的基准数据集和评估指标验证了有效性。

Conclusion: DVidE任务及相关方法有效增强了视频大模型的动态推理能力，为未来研究提供了新方向。

Abstract: Video Large Multimodal Models (VLMMs) have made impressive strides in
understanding video content, but they often struggle with abstract and adaptive
reasoning-the ability to revise their interpretations when new information
emerges. In reality, conclusions are rarely set in stone; additional context
can strengthen or weaken an initial inference. To address this, we introduce
Defeasible Video Entailment (DVidE), a new task that challenges models to think
like doubters, constantly updating their reasoning based on evolving evidence.
In DVidE, given a video premise and a textual hypothesis, models must determine
whether a new update strengthens or weakens the hypothesis (classification
version) or generate a coherent update that modifies the entailment
relationship (generation version). For solving the classification task, we
propose the Chain of Counterfactual Thought framework, utilizing counterfactual
reasoning, ASR-enhanced video content, and rationale refinement to reduce
inference bias. For the generation task, we develop a framework that combines
ASR output with a Large Language Model (LLM) to produce coherent, contextually
relevant updates aligned with the intended strengthener or weakener goals.
Additionally, we introduce a novel benchmark dataset, with
strengthener/weakener annotations and an LLM-based evaluation metric
specifically designed for assessing generative performance. Experimental
results demonstrate significant improvements, highlighting our proposed method
in enhancing dynamic reasoning capabilities of VLMMs.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [234] [UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields](https://arxiv.org/abs/2506.21884)
*Fabian Perez,Sara Rojas,Carlos Hinojosa,Hoover Rueda-Chacón,Bernard Ghanem*

Main category: eess.IV

TL;DR: UnMix-NeRF结合光谱解混与NeRF，实现高光谱新视角合成与无监督材料分割，提升材料感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于NeRF的分割方法仅依赖RGB数据，缺乏材料属性信息，限制了在机器人、AR等应用中的准确材料感知。

Method: 通过漫反射和镜面反射分量建模光谱反射率，利用全局端元字典表示纯材料特征，并通过逐点丰度分布实现无监督材料聚类。

Result: 实验表明，UnMix-NeRF在光谱重建和材料分割上优于现有方法，并支持基于材料的场景编辑。

Conclusion: UnMix-NeRF有效解决了材料感知的局限性，为多领域应用提供了更灵活的光谱与材料分析工具。

Abstract: Neural Radiance Field (NeRF)-based segmentation methods focus on object
semantics and rely solely on RGB data, lacking intrinsic material properties.
This limitation restricts accurate material perception, which is crucial for
robotics, augmented reality, simulation, and other applications. We introduce
UnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling
joint hyperspectral novel view synthesis and unsupervised material
segmentation. Our method models spectral reflectance via diffuse and specular
components, where a learned dictionary of global endmembers represents pure
material signatures, and per-point abundances capture their distribution. For
material segmentation, we use spectral signature predictions along learned
endmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF
enables scene editing by modifying learned endmember dictionaries for flexible
material-based appearance manipulation. Extensive experiments validate our
approach, demonstrating superior spectral reconstruction and material
segmentation to existing methods. Project page:
https://www.factral.co/UnMix-NeRF.

</details>


### [235] [Dehazing Light Microscopy Images with Guided Conditional Flow Matching: finding a sweet spot between fidelity and realism](https://arxiv.org/abs/2506.22397)
*Anirban Ray,Ashesh,Florian Jug*

Main category: eess.IV

TL;DR: 提出HazeMatching方法，平衡荧光显微镜图像去雾的保真度与真实感，无需显式退化算子，适用于真实数据。


<details>
  <summary>Details</summary>
Motivation: 宽场显微镜等低成本显微镜无法过滤离焦光，导致图像模糊。现有方法难以同时兼顾保真度与真实感，需找到平衡方案。

Method: 基于条件流匹配框架，通过条件速度场引导生成过程，迭代实现去雾，无需显式退化算子。

Result: 在5个数据集上优于7个基线方法，平衡了失真与感知质量，预测结果校准良好。

Conclusion: HazeMatching有效解决了显微镜图像去雾中保真度与真实感的权衡问题，具有实际应用价值。

Abstract: Fluorescence microscopy is a major driver of scientific progress in the life
sciences. Although high-end confocal microscopes are capable of filtering
out-of-focus light, cheaper and more accessible microscopy modalities, such as
widefield microscopy, can not, which consequently leads to hazy image data.
Computational dehazing is trying to combine the best of both worlds, leading to
cheap microscopy but crisp-looking images. The perception-distortion trade-off
tells us that we can optimize either for data fidelity, e.g. low MSE or high
PSNR, or for data realism, measured by perceptual metrics such as LPIPS or FID.
Existing methods either prioritize fidelity at the expense of realism, or
produce perceptually convincing results that lack quantitative accuracy. In
this work, we propose HazeMatching, a novel iterative method for dehazing light
microscopy images, which effectively balances these objectives. Our goal was to
find a balanced trade-off between the fidelity of the dehazing results and the
realism of individual predictions (samples). We achieve this by adapting the
conditional flow matching framework by guiding the generative process with a
hazy observation in the conditional velocity field. We evaluate HazeMatching on
5 datasets, covering both synthetic and real data, assessing both distortion
and perceptual quality. Our method is compared against 7 baselines, achieving a
consistent balance between fidelity and realism on average. Additionally, with
calibration analysis, we show that HazeMatching produces well-calibrated
predictions. Note that our method does not need an explicit degradation
operator to exist, making it easily applicable on real microscopy data. All
data used for training and evaluation and our code will be publicly available
under a permissive license.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [236] [LLM2Rec: Large Language Models Are Powerful Embedding Models for Sequential Recommendation](https://arxiv.org/abs/2506.21579)
*Yingzhi He,Xiaohao Liu,An Zhang,Yunshan Ma,Tat-Seng Chua*

Main category: cs.IR

TL;DR: LLM2Rec提出了一种结合大语言模型语义理解与协同过滤信号的序列推荐嵌入模型，通过两阶段训练提升跨领域推荐效果。


<details>
  <summary>Details</summary>
Motivation: 传统序列推荐依赖ID嵌入，缺乏可迁移性；纯文本推荐方法虽能泛化但无法捕捉协同过滤信号。需要融合两者优势。

Method: 两阶段框架：1) 协同监督微调使LLM学习物品关系；2) 物品级嵌入建模生成同时编码语义和协同信息的结构化嵌入。

Result: 在真实数据集上验证，LLM2Rec在域内和跨域推荐场景均显著提升推荐质量。

Conclusion: 利用LLMs构建融合语义与协同信号的嵌入模型，能实现更鲁棒、可泛化的序列推荐系统。

Abstract: Sequential recommendation aims to predict users' future interactions by
modeling collaborative filtering (CF) signals from historical behaviors of
similar users or items. Traditional sequential recommenders predominantly rely
on ID-based embeddings, which capture CF signals through high-order
co-occurrence patterns. However, these embeddings depend solely on past
interactions, lacking transferable knowledge to generalize to unseen domains.
Recent advances in large language models (LLMs) have motivated text-based
recommendation approaches that derive item representations from textual
descriptions. While these methods enhance generalization, they fail to encode
CF signals-i.e., latent item correlations and preference patterns-crucial for
effective recommendation. We argue that an ideal embedding model should
seamlessly integrate CF signals with rich semantic representations to improve
both in-domain and out-of-domain recommendation performance.
  To this end, we propose LLM2Rec, a novel embedding model tailored for
sequential recommendation, integrating the rich semantic understanding of LLMs
with CF awareness. Our approach follows a two-stage training framework: (1)
Collaborative Supervised Fine-tuning, which adapts LLMs to infer item
relationships based on historical interactions, and (2) Item-level Embedding
Modeling, which refines these specialized LLMs into structured item embedding
models that encode both semantic and collaborative information. Extensive
experiments on real-world datasets demonstrate that LLM2Rec effectively
improves recommendation quality across both in-domain and out-of-domain
settings. Our findings highlight the potential of leveraging LLMs to build more
robust, generalizable embedding models for sequential recommendation. Our codes
are available at https://github.com/HappyPointer/LLM2Rec.

</details>


### [237] [Evaluating the Robustness of Dense Retrievers in Interdisciplinary Domains](https://arxiv.org/abs/2506.21581)
*Sarthak Chaturvedi,Anurag Acharya,Rounak Meyur,Koby Hayashi,Sai Munikoti,Sameera Horawalavithana*

Main category: cs.IR

TL;DR: 不同评估基准的特性可能扭曲检索模型领域适应的真实效果，导致误导性结论。论文通过环境法规文档检索案例，发现基准的语义结构会显著影响领域适应效果的评估。


<details>
  <summary>Details</summary>
Motivation: 当前评估基准的特性（如主题多样性、边界重叠和语义复杂性）可能扭曲领域适应的真实效果，从而影响专业领域中检索模型的部署决策。

Method: 以环境法规文档检索为例，在联邦机构的环境影响声明（EIS）上微调ColBERTv2模型，并在两个语义结构不同的基准上评估模型表现。

Result: 在主题边界清晰的基准上，领域适应仅带来小幅提升（NDCG增益0.61%）；而在语义重叠的基准上，相同模型表现显著提升（NDCG增益2.22%）。差异源于基准的主题多样性（余弦距离高11%）和聚类紧密度（轮廓分数低23%）。

Conclusion: 评估基准的选择强烈影响专业领域检索系统的效果评估。主题分离的基准会低估领域适应价值，而语义重叠的基准更能反映真实场景的复杂性。这对跨学科AI系统的开发与部署具有启示意义。

Abstract: Evaluation benchmark characteristics may distort the true benefits of domain
adaptation in retrieval models. This creates misleading assessments that
influence deployment decisions in specialized domains. We show that two
benchmarks with drastically different features such as topic diversity,
boundary overlap, and semantic complexity can influence the perceived benefits
of fine-tuning. Using environmental regulatory document retrieval as a case
study, we fine-tune ColBERTv2 model on Environmental Impact Statements (EIS)
from federal agencies. We evaluate these models across two benchmarks with
different semantic structures. Our findings reveal that identical domain
adaptation approaches show very different perceived benefits depending on
evaluation methodology. On one benchmark, with clearly separated topic
boundaries, domain adaptation shows small improvements (maximum 0.61% NDCG
gain). However, on the other benchmark with overlapping semantic structures,
the same models demonstrate large improvements (up to 2.22% NDCG gain), a
3.6-fold difference in the performance benefit. We compare these benchmarks
through topic diversity metrics, finding that the higher-performing benchmark
shows 11% higher average cosine distances between contexts and 23% lower
silhouette scores, directly contributing to the observed performance
difference. These results demonstrate that benchmark selection strongly
determines assessments of retrieval system effectiveness in specialized
domains. Evaluation frameworks with well-separated topics regularly
underestimate domain adaptation benefits, while those with overlapping semantic
boundaries reveal improvements that better reflect real-world regulatory
document complexity. Our findings have important implications for developing
and deploying AI systems for interdisciplinary domains that integrate multiple
topics.

</details>


### [238] [Reinforcement Fine-Tuned Large Language Models for Next POI Recommendation](https://arxiv.org/abs/2506.21599)
*Peibo Li,Shuang Ao,Hao Xue,Yang Song,Maarten de Rijke,Johan Barthélemy,Tomasz Bednarz,Flora D. Salim*

Main category: cs.IR

TL;DR: 提出Refine-POI框架，通过强化学习微调LLM解决POI推荐中SFT方法无法生成top-k列表的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的POI推荐方法中，SFT模型虽精度高但无法适配单目标POI生成top-k列表的需求，而提示模型灵活性高但精度不足。

Method: 设计Refine-POI框架，引入推荐驱动的奖励机制，使LLM能用单目标POI学习生成top-k推荐列表。

Result: 实验表明Refine-POI在真实数据集上实现了最先进的top-k推荐性能。

Conclusion: 该框架有效解决了SFT在POI推荐中的根本性错配问题，为LLM推荐系统提供了新思路。

Abstract: Large language models (LLMs) have been adopted for next point-of-interest
(POI) recommendation tasks. Typical LLM-based recommenders fall into two
categories: prompt-based and supervised fine-tuning (SFT)-based models.
Prompt-based models generally offer greater output flexibility but deliver
lower accuracy, whereas SFT-based models achieve higher performance yet face a
fundamental mismatch: next POI recommendation data does not naturally suit
supervised fine-tuning. In SFT, the model is trained to reproduce the exact
ground truth, but each training example provides only a single target POI, so
there is no ground truth for producing a top-k list.
  To address this, we propose Refine-POI, a reinforcement fine-tuning framework
for next POI recommendation. We introduce recommendation-driven rewards that
enable LLMs to learn to generate top-k recommendation lists using only one
ground-truth POI per example. Experiments on real-world datasets demonstrate
that Refine-POI achieves state-of-the-art top-k recommendation performance.

</details>


### [239] [Evaluating VisualRAG: Quantifying Cross-Modal Performance in Enterprise Document Understanding](https://arxiv.org/abs/2506.21604)
*Varun Mannam,Fang Wang,Xin Chen*

Main category: cs.IR

TL;DR: 提出了一种量化多模态RAG系统可信度的评估框架，通过优化模态权重提升企业文档智能应用的性能与可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态生成AI评估框架难以建立可信度，阻碍了企业级应用。需系统化方法量化跨模态输入（文本、图像、标题、OCR）的信任关系。

Method: 建立技术指标与用户信任度间的量化关系，测试不同模态权重组合（文本30%、图像15%、标题25%、OCR30%）在VisualRAG系统中的效果。

Result: 最优模态权重组合比纯文本基线性能提升57.3%，同时保持计算效率。基础模型在标题生成和OCR提取中对可信度有差异化影响。

Conclusion: 该框架为关键企业应用提供了量化多模态RAG可信度的严谨方法，推动负责任AI部署。

Abstract: Current evaluation frameworks for multimodal generative AI struggle to
establish trustworthiness, hindering enterprise adoption where reliability is
paramount. We introduce a systematic, quantitative benchmarking framework to
measure the trustworthiness of progressively integrating cross-modal inputs
such as text, images, captions, and OCR within VisualRAG systems for enterprise
document intelligence. Our approach establishes quantitative relationships
between technical metrics and user-centric trust measures. Evaluation reveals
that optimal modality weighting with weights of 30% text, 15% image, 25%
caption, and 30% OCR improves performance by 57.3% over text-only baselines
while maintaining computational efficiency. We provide comparative assessments
of foundation models, demonstrating their differential impact on
trustworthiness in caption generation and OCR extraction-a vital consideration
for reliable enterprise AI. This work advances responsible AI deployment by
providing a rigorous framework for quantifying and enhancing trustworthiness in
multimodal RAG for critical enterprise applications.

</details>


### [240] [Bayesian-Guided Diversity in Sequential Sampling for Recommender Systems](https://arxiv.org/abs/2506.21617)
*Hiba Bederina,Jill-Jênn Vie*

Main category: cs.IR

TL;DR: 提出一种基于多目标贝叶斯更新的推荐系统框架，在保持相关性的同时显著提升内容多样性。


<details>
  <summary>Details</summary>
Motivation: 当前推荐系统面临内容同质化和用户参与度下降的问题，需平衡相关性与多样性。

Method: 使用多目标上下文序贯采样策略，通过贝叶斯动态评分调整和帕累托最优集选择机制。

Result: 在真实数据集上验证，该方法显著提升多样性且不损害相关性。

Conclusion: 该框架能有效提升大规模推荐场景中的用户体验。

Abstract: The challenge of balancing user relevance and content diversity in
recommender systems is increasingly critical amid growing concerns about
content homogeneity and reduced user engagement. In this work, we propose a
novel framework that leverages a multi-objective, contextual sequential
sampling strategy. Item selection is guided by Bayesian updates that
dynamically adjust scores to optimize diversity. The reward formulation
integrates multiple diversity metrics-including the log-determinant volume of a
tuned similarity submatrix and ridge leverage scores-along with a diversity
gain uncertainty term to address the exploration-exploitation trade-off. Both
intra- and inter-batch diversity are modeled to promote serendipity and
minimize redundancy. A dominance-based ranking procedure identifies
Pareto-optimal item sets, enabling adaptive and balanced selections at each
iteration. Experiments on a real-world dataset show that our approach
significantly improves diversity without sacrificing relevance, demonstrating
its potential to enhance user experience in large-scale recommendation
settings.

</details>


### [241] [DCN^2: Interplay of Implicit Collision Weights and Explicit Cross Layers for Large-Scale Recommendation](https://arxiv.org/abs/2506.21624)
*Blaž Škrlj,Yonatan Karni,Grega Gašperšič,Blaž Mramor,Yulia Stolin,Martin Jakomin,Jasna Urbančič,Yuval Dishi,Natalia Silberstein,Ophir Friedler,Assaf Klein*

Main category: cs.IR

TL;DR: DCN^2是对DCNv2的改进版本，通过三项关键算法优化，解决了信息丢失等问题，并在实际推荐系统中表现出色。


<details>
  <summary>Details</summary>
Motivation: DCNv2虽然高效且广泛用于推荐系统，但仍存在信息丢失、碰撞处理隐式等问题，需要进一步优化。

Method: 引入三项改进：解决Cross层信息丢失、通过可学习权重显式处理碰撞、自定义层模拟FFM行为建模相似性。

Result: DCN^2在离线/在线测试中优于DCNv2，处理超5亿次预测/秒，并在四个公开数据集上表现更优。

Conclusion: DCN^2有效解决了DCNv2的局限性，成为更强大的推荐系统基线模型。

Abstract: The Deep and Cross architecture (DCNv2) is a robust production baseline and
is integral to numerous real-life recommender systems. Its inherent efficiency
and ability to model interactions often result in models that are both simpler
and highly competitive compared to more computationally demanding alternatives,
such as Deep FFMs. In this work, we introduce three significant algorithmic
improvements to the DCNv2 architecture, detailing their formulation and
behavior at scale. The enhanced architecture we refer to as DCN^2 is actively
used in a live recommender system, processing over 0.5 billion predictions per
second across diverse use cases where it out-performed DCNv2, both offline and
online (ab tests). These improvements effectively address key limitations
observed in the DCNv2, including information loss in Cross layers, implicit
management of collisions through learnable lookup-level weights, and explicit
modeling of pairwise similarities with a custom layer that emulates FFMs'
behavior. The superior performance of DCN^2 is also demonstrated on four
publicly available benchmark data sets.

</details>


### [242] [IRanker: Towards Ranking Foundation Model](https://arxiv.org/abs/2506.21638)
*Tao Feng,Zhigang Hua,Zijie Lei,Yan Xie,Shuang Yang,Bo Long,Jiaxuan You*

Main category: cs.IR

TL;DR: 提出IRanker框架，通过强化学习和迭代解码统一处理各类排序任务，显著提升模型性能与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有排序任务需针对不同场景设计专门模型，缺乏统一解决方案；且排序任务缺乏明确监督标签，阻碍通用排序基础模型的开发。

Method: 采用强化学习与迭代解码机制，将复杂排序分解为逐步淘汰最差候选者的过程，降低输出组合空间并优化上下文长度利用。

Result: IRanker-3B在推荐/路由/段落排序等9个数据集上达到SOTA，部分超越更大模型；零样本任务中GSM8K等表现提升超9%，训练生成思路可增强LLM性能。

Conclusion: IRanker验证了统一排序基础模型的可行性，其迭代机制与RL设计具有跨模型尺寸的鲁棒性，同时展现出优秀的领域内外泛化能力。

Abstract: Ranking tasks are ubiquitous, encompassing applications such as
recommendation systems, LLM routing, and item re-ranking. We propose to unify
these tasks using a single ranking foundation model (FM), as it eliminates the
need for designing different models for each specific ranking task. However,
unlike general supervision tasks in LLMs, ranking tasks do not have clear
labels for supervision, posing great challenges to developing a ranking FM. To
overcome these challenges, we propose IRanker, a ranking FM framework with
reinforcement learning (RL) and iterative decoding. Our insight is to decompose
the complex ranking task into an iterative decoding process that eliminates the
worst candidate from the candidate pool step by step, which significantly
reduces the output combinatorial space and better utilizes the limited context
length during RL training. We meticulously train and comprehensively evaluate
an IRanker-3B model on nine datasets across three scenarios: recommendation,
routing, and passage ranking. The results show that a single IRanker-3B
achieves state-of-the-art results on several datasets compared to models of
similar size, and even surpasses the performance of larger models on certain
datasets. We further demonstrate the effectiveness of our RL design and the
robustness of the iterative mechanism across different LLM sizes. Moreover, we
conducted both in-domain and out-of-domain zero-shot generalization
experiments, which showed that IRanker-3B achieved good generalization on
in-domain ranking tasks compared to the base LLM by at least 5% improvement.
Surprisingly, on out-of-domain generic LLM tasks, IRanker-3B outperformed the
base model by at least 9% on GSM8K, IFEval, and MathQA. In addition, the
thoughts generated by IRanker-3B during training could further enhance
zero-shot LLM performance.

</details>


### [243] [ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation](https://arxiv.org/abs/2506.21931)
*Reza Yousefi Maragheh,Pratheek Vadla,Priyank Gupta,Kai Zhao,Aysenur Inan,Kehui Yao,Jianpeng Xu,Praveen Kanumala,Jason Cho,Sushant Kumar*

Main category: cs.IR

TL;DR: ARAG框架通过多智能体协作增强RAG推荐系统，显著提升个性化推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于RAG的推荐系统依赖静态检索启发式方法，难以捕捉动态推荐场景中的用户偏好。

Method: ARAG框架整合四个LLM智能体：用户理解、自然语言推理、上下文摘要和项目排序，以动态理解用户行为。

Result: 在三个数据集上，ARAG在NDCG@5和Hit@5指标上分别提升42.1%和35.5%，优于标准RAG和基于时效性的基线。

Conclusion: ARAG展示了智能体推理在检索增强推荐中的有效性，为基于LLM的个性化提供了新方向。

Abstract: Retrieval-Augmented Generation (RAG) has shown promise in enhancing
recommendation systems by incorporating external context into large language
model prompts. However, existing RAG-based approaches often rely on static
retrieval heuristics and fail to capture nuanced user preferences in dynamic
recommendation scenarios. In this work, we introduce ARAG, an Agentic
Retrieval-Augmented Generation framework for Personalized Recommendation, which
integrates a multi-agent collaboration mechanism into the RAG pipeline. To
better understand the long-term and session behavior of the user, ARAG
leverages four specialized LLM-based agents: a User Understanding Agent that
summarizes user preferences from long-term and session contexts, a Natural
Language Inference (NLI) Agent that evaluates semantic alignment between
candidate items retrieved by RAG and inferred intent, a context summary agent
that summarizes the findings of NLI agent, and an Item Ranker Agent that
generates a ranked list of recommendations based on contextual fit. We evaluate
ARAG accross three datasets. Experimental results demonstrate that ARAG
significantly outperforms standard RAG and recency-based baselines, achieving
up to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an
ablation study to analyse the effect by different components of ARAG. Our
findings highlight the effectiveness of integrating agentic reasoning into
retrieval-augmented recommendation and provide new directions for LLM-based
personalization.

</details>


### [244] [Literature-Grounded Novelty Assessment of Scientific Ideas](https://arxiv.org/abs/2506.22026)
*Simra Shahid,Marissa Radensky,Raymond Fok,Pao Siangliulue,Daniel S. Weld,Tom Hope*

Main category: cs.IR

TL;DR: 该论文提出了一种基于LLM的检索增强生成框架Idea Novelty Checker，用于自动评估科学想法的创新性，通过两阶段检索和重排方法显著提高了评估准确性。


<details>
  <summary>Details</summary>
Motivation: 手动评估科学想法的创新性既耗时又容易受主观性影响，且难以规模化。因此，需要一种自动化的方法来高效、准确地评估想法的创新性。

Method: 采用两阶段的检索增强生成框架：首先通过关键词和片段检索收集相关论文，然后通过嵌入过滤和基于方面的LLM重排来优化结果，并结合专家标注的示例进行指导。

Result: 实验表明，该方法的创新性评估与现有方法相比，一致性提高了约13%。消融研究进一步验证了基于方面的重排在识别相关文献中的重要性。

Conclusion: Idea Novelty Checker框架在自动化评估科学想法创新性方面表现出色，特别是在结合两阶段检索和重排方法后，显著提升了评估的准确性和一致性。

Abstract: Automated scientific idea generation systems have made remarkable progress,
yet the automatic evaluation of idea novelty remains a critical and
underexplored challenge. Manual evaluation of novelty through literature review
is labor-intensive, prone to error due to subjectivity, and impractical at
scale. To address these issues, we propose the Idea Novelty Checker, an
LLM-based retrieval-augmented generation (RAG) framework that leverages a
two-stage retrieve-then-rerank approach. The Idea Novelty Checker first
collects a broad set of relevant papers using keyword and snippet-based
retrieval, then refines this collection through embedding-based filtering
followed by facet-based LLM re-ranking. It incorporates expert-labeled examples
to guide the system in comparing papers for novelty evaluation and in
generating literature-grounded reasoning. Our extensive experiments demonstrate
that our novelty checker achieves approximately 13% higher agreement than
existing approaches. Ablation studies further showcases the importance of the
facet-based re-ranker in identifying the most relevant literature for novelty
evaluation.

</details>


### [245] [HyReC: Exploring Hybrid-based Retriever for Chinese](https://arxiv.org/abs/2506.21913)
*Zunran Wang,Zheng Shenpeng,Wang Shenglan,Minghui Zhao,Zhonghua Li*

Main category: cs.IR

TL;DR: 本文提出HyReC方法，针对中文混合检索进行端到端优化，通过语义术语整合和全局-局部感知编码器提升性能，并在C-MTEB基准测试中验证其效果。


<details>
  <summary>Details</summary>
Motivation: 尽管混合检索方法在性能上表现优异，但在中文检索领域的应用尚未充分探索，因此开发专门针对中文的混合检索优化方法具有重要价值。

Method: HyReC方法整合了术语的语义联合到表示模型中，采用全局-局部感知编码器（GLAE）促进词典检索与密集检索间的语义共享，并引入归一化模块（NM）优化对齐。

Result: 在C-MTEB检索基准测试中，HyReC展示了其有效性，验证了该方法在中文混合检索中的性能提升。

Conclusion: HyReC为中文混合检索提供了一种创新的端到端优化方案，通过语义整合和模块化设计显著提升了检索性能。

Abstract: Hybrid-based retrieval methods, which unify dense-vector and lexicon-based
retrieval, have garnered considerable attention in the industry due to
performance enhancement. However, despite their promising results, the
application of these hybrid paradigms in Chinese retrieval contexts has
remained largely underexplored. In this paper, we introduce HyReC, an
innovative end-to-end optimization method tailored specifically for
hybrid-based retrieval in Chinese. HyReC enhances performance by integrating
the semantic union of terms into the representation model. Additionally, it
features the Global-Local-Aware Encoder (GLAE) to promote consistent semantic
sharing between lexicon-based and dense retrieval while minimizing the
interference between them. To further refine alignment, we incorporate a
Normalization Module (NM) that fosters mutual benefits between the retrieval
approaches. Finally, we evaluate HyReC on the C-MTEB retrieval benchmark to
demonstrate its effectiveness.

</details>


### [246] [Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and Measurement](https://arxiv.org/abs/2506.22372)
*Maryam Mousavian,Zahra Abbasiantaeb,Mohammad Aliannejadi,Fabio Crestani*

Main category: cs.IR

TL;DR: 该论文提出了一种基于大语言模型（LLM）的性别偏见检测方法，并引入新的公平性度量标准CWEx，以改进信息检索系统中的性别偏见评估。


<details>
  <summary>Details</summary>
Motivation: 自然语言处理和信息检索系统中存在社会偏见，现有性别公平性度量标准存在局限性，无法捕捉细微的性别差异，因此需要开发更强大的方法来识别和评估这些偏见。

Method: 利用大语言模型检测和测量段落排序中的性别偏见，提出新的公平性度量标准Class-wise Weighted Exposure (CWEx)，并在MS MARCO Passage Ranking数据集上进行了标注，发布了新的性别偏见数据集MSMGenderBias。

Result: 实验结果表明，CWEx比现有度量标准提供了更详细的公平性评估，与人类标注的一致性更高（Cohen's Kappa分别为58.77%和18.51%），能有效区分排序中的性别偏见。

Conclusion: 通过结合LLM驱动的偏见检测、改进的公平性度量标准以及标注数据集，该研究为分析和减轻信息检索系统中的偏见提供了更强大的框架。

Abstract: The presence of social biases in Natural Language Processing (NLP) and
Information Retrieval (IR) systems is an ongoing challenge, which underlines
the importance of developing robust approaches to identifying and evaluating
such biases. In this paper, we aim to address this issue by leveraging Large
Language Models (LLMs) to detect and measure gender bias in passage ranking.
Existing gender fairness metrics rely on lexical- and frequency-based measures,
leading to various limitations, e.g., missing subtle gender disparities.
Building on our LLM-based gender bias detection method, we introduce a novel
gender fairness metric, named Class-wise Weighted Exposure (CWEx), aiming to
address existing limitations. To measure the effectiveness of our proposed
metric and study LLMs' effectiveness in detecting gender bias, we annotate a
subset of the MS MARCO Passage Ranking collection and release our new gender
bias collection, called MSMGenderBias, to foster future research in this area.
Our extensive experimental results on various ranking models show that our
proposed metric offers a more detailed evaluation of fairness compared to
previous metrics, with improved alignment to human labels (58.77% for
Grep-BiasIR, and 18.51% for MSMGenderBias, measured using Cohen's Kappa
agreement), effectively distinguishing gender bias in ranking. By integrating
LLM-driven bias detection, an improved fairness metric, and gender bias
annotations for an established dataset, this work provides a more robust
framework for analyzing and mitigating bias in IR systems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [247] [On the Feasibility of Poisoning Text-to-Image AI Models via Adversarial Mislabeling](https://arxiv.org/abs/2506.21874)
*Stanley Wu,Ronik Bhaskar,Anna Yoo Jeong Ha,Shawn Shan,Haitao Zheng,Ben Y. Zhao*

Main category: cs.CR

TL;DR: 本文探讨了通过对抗性攻击误导视觉语言模型（VLMs）生成错误标注，进而污染文本到图像生成模型训练数据的可行性，展示了攻击的高效性及现有防御措施的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成模型依赖视觉语言模型（VLMs）生成的大量高质量图像-标注对进行训练。然而，VLMs易受对抗性攻击，导致生成错误标注，可能污染训练数据并影响模型行为。本文旨在研究此类攻击的可行性及其对训练流程的实际影响。

Method: 通过设计对抗性扰动，使VLMs对看似正常的图像生成错误标注，从而在文本到图像模型的训练数据中注入“脏标签”毒样本。实验针对商业VLMs（如Google Vertex AI和Microsoft Azure）进行黑盒攻击测试。

Result: 攻击成功率超过73%，少量毒样本即可显著改变文本到图像模型的行为。现有防御措施虽有效，但攻击者可自适应绕过，形成攻防博弈，可能增加模型开发成本并降低数据质量。

Conclusion: 对抗性标注攻击对VLMs及下游文本到图像模型构成现实威胁，需开发更鲁棒的防御机制以保障训练数据可靠性。攻防博弈可能长期存在，影响生成模型的开发效率。

Abstract: Today's text-to-image generative models are trained on millions of images
sourced from the Internet, each paired with a detailed caption produced by
Vision-Language Models (VLMs). This part of the training pipeline is critical
for supplying the models with large volumes of high-quality image-caption pairs
during training. However, recent work suggests that VLMs are vulnerable to
stealthy adversarial attacks, where adversarial perturbations are added to
images to mislead the VLMs into producing incorrect captions.
  In this paper, we explore the feasibility of adversarial mislabeling attacks
on VLMs as a mechanism to poisoning training pipelines for text-to-image
models. Our experiments demonstrate that VLMs are highly vulnerable to
adversarial perturbations, allowing attackers to produce benign-looking images
that are consistently miscaptioned by the VLM models. This has the effect of
injecting strong "dirty-label" poison samples into the training pipeline for
text-to-image models, successfully altering their behavior with a small number
of poisoned samples. We find that while potential defenses can be effective,
they can be targeted and circumvented by adaptive attackers. This suggests a
cat-and-mouse game that is likely to reduce the quality of training data and
increase the cost of text-to-image model development. Finally, we demonstrate
the real-world effectiveness of these attacks, achieving high attack success
(over 73%) even in black-box scenarios against commercial VLMs (Google Vertex
AI and Microsoft Azure).

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [248] [FrankenBot: Brain-Morphic Modular Orchestration for Robotic Manipulation with Vision-Language Models](https://arxiv.org/abs/2506.21627)
*Shiyi Wang,Wenbo Li,Yiteng Chen,Qingyao Wu,Huiping Zhuang*

Main category: cs.RO

TL;DR: 提出FrankenBot框架，通过VLM驱动和类脑架构实现多功能高效机器人操作。


<details>
  <summary>Details</summary>
Motivation: 现有机器人系统功能单一，缺乏统一认知架构，难以在复杂环境中实现高效操作。

Method: 采用分治策略和类脑架构，将任务规划、策略生成等功能解耦，设计高效协调机制。

Result: 实验表明该方法在异常处理、长期记忆和效率方面表现优异，且无需微调。

Conclusion: FrankenBot框架成功整合多功能并实现高效操作，为机器人系统设计提供新思路。

Abstract: Developing a general robot manipulation system capable of performing a wide
range of tasks in complex, dynamic, and unstructured real-world environments
has long been a challenging task. It is widely recognized that achieving
human-like efficiency and robustness manipulation requires the robotic brain to
integrate a comprehensive set of functions, such as task planning, policy
generation, anomaly monitoring and handling, and long-term memory, achieving
high-efficiency operation across all functions. Vision-Language Models (VLMs),
pretrained on massive multimodal data, have acquired rich world knowledge,
exhibiting exceptional scene understanding and multimodal reasoning
capabilities. However, existing methods typically focus on realizing only a
single function or a subset of functions within the robotic brain, without
integrating them into a unified cognitive architecture. Inspired by a
divide-and-conquer strategy and the architecture of the human brain, we propose
FrankenBot, a VLM-driven, brain-morphic robotic manipulation framework that
achieves both comprehensive functionality and high operational efficiency. Our
framework includes a suite of components, decoupling a part of key functions
from frequent VLM calls, striking an optimal balance between functional
completeness and system efficiency. Specifically, we map task planning, policy
generation, memory management, and low-level interfacing to the cortex,
cerebellum, temporal lobe-hippocampus complex, and brainstem, respectively, and
design efficient coordination mechanisms for the modules. We conducted
comprehensive experiments in both simulation and real-world robotic
environments, demonstrating that our method offers significant advantages in
anomaly detection and handling, long-term memory, operational efficiency, and
stability -- all without requiring any fine-tuning or retraining.

</details>


### [249] [Ark: An Open-source Python-based Framework for Robot Learning](https://arxiv.org/abs/2506.21628)
*Magnus Dierking,Christopher E. Mower,Sarthak Das,Huang Helong,Jiacheng Qiu,Cody Reading,Wei Chen,Huidong Liang,Huang Guowei,Jan Peters,Quan Xingyue,Jun Wang,Haitham Bou-Ammar*

Main category: cs.RO

TL;DR: ARK是一个开源的、以Python为主的机器人框架，旨在简化机器人软件开发，降低学习门槛，并加速自主机器人的研究和商业部署。


<details>
  <summary>Details</summary>
Motivation: 当前机器人软件开发存在学习曲线陡峭、工具分散、硬件集成复杂等问题，与Python主导的现代AI生态系统形成鲜明对比。ARK旨在填补这一差距。

Method: ARK提供了一个Gym风格的环境接口，支持数据收集、预处理和策略训练，采用轻量级客户端-服务器架构，并提供可选的C/C++绑定以确保实时性能。

Result: ARK通过统一的Python生态系统，实现了快速原型设计、硬件无缝切换和端到端流程，显著提升了机器人开发的便利性。

Conclusion: ARK通过整合机器人和AI实践，降低了入门门槛，加速了自主机器人的研究和商业应用。

Abstract: Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics
Challenges to the first humanoid-robot kickboxing tournament-yet commercial
autonomy still lags behind progress in machine learning. A major bottleneck is
software: current robot stacks demand steep learning curves, low-level C/C++
expertise, fragmented tooling, and intricate hardware integration, in stark
contrast to the Python-centric, well-documented ecosystems that propelled
modern AI. We introduce ARK, an open-source, Python-first robotics framework
designed to close that gap. ARK presents a Gym-style environment interface that
allows users to collect data, preprocess it, and train policies using
state-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy)
while seamlessly toggling between high-fidelity simulation and physical robots.
A lightweight client-server architecture provides networked
publisher-subscriber communication, and optional C/C++ bindings ensure
real-time performance when needed. ARK ships with reusable modules for control,
SLAM, motion planning, system identification, and visualization, along with
native ROS interoperability. Comprehensive documentation and case studies-from
manipulation to mobile navigation-demonstrate rapid prototyping, effortless
hardware swapping, and end-to-end pipelines that rival the convenience of
mainstream machine-learning workflows. By unifying robotics and AI practices
under a common Python umbrella, ARK lowers entry barriers and accelerates
research and commercial deployment of autonomous robots.

</details>


### [250] [AeroLite-MDNet: Lightweight Multi-task Deviation Detection Network for UAV Landing](https://arxiv.org/abs/2506.21635)
*Haiping Yang,Huaxing Liu,Wei Wu,Zuohui Chen,Ning Wu*

Main category: cs.RO

TL;DR: 提出基于视觉的AeroLite-MDNet模型，用于无人机精准着陆偏差预警，引入新指标AWD和数据集UAVLandData，实验显示检测准确率达98.6%。


<details>
  <summary>Details</summary>
Motivation: 无人机着陆时易受GPS信号干扰导致偏差，需可靠预警系统保障安全着陆与连续作业。

Method: 结合多尺度融合模块实现跨尺度目标检测，并添加分割分支进行朝向估计，提出AWD指标量化预警延迟。

Result: 系统AWD仅0.7秒，偏差检测准确率98.6%，新数据集有效支持模型训练与评估。

Conclusion: AeroLite-MDNet显著提升无人机着陆可靠性，代码与数据集将促进相关研究。

Abstract: Unmanned aerial vehicles (UAVs) are increasingly employed in diverse
applications such as land surveying, material transport, and environmental
monitoring. Following missions like data collection or inspection, UAVs must
land safely at docking stations for storage or recharging, which is an
essential requirement for ensuring operational continuity. However, accurate
landing remains challenging due to factors like GPS signal interference. To
address this issue, we propose a deviation warning system for UAV landings,
powered by a novel vision-based model called AeroLite-MDNet. This model
integrates a multiscale fusion module for robust cross-scale object detection
and incorporates a segmentation branch for efficient orientation estimation. We
introduce a new evaluation metric, Average Warning Delay (AWD), to quantify the
system's sensitivity to landing deviations. Furthermore, we contribute a new
dataset, UAVLandData, which captures real-world landing deviation scenarios to
support training and evaluation. Experimental results show that our system
achieves an AWD of 0.7 seconds with a deviation detection accuracy of 98.6\%,
demonstrating its effectiveness in enhancing UAV landing reliability. Code will
be available at https://github.com/ITTTTTI/Maskyolo.git

</details>


### [251] [TOMD: A Trail-based Off-road Multimodal Dataset for Traversable Pathway Segmentation under Challenging Illumination Conditions](https://arxiv.org/abs/2506.21630)
*Yixin Sun,Li Li,Wenke E,Amir Atapour-Abarghouei,Toby P. Breckon*

Main category: cs.RO

TL;DR: 论文提出了针对非结构化户外环境中可通行路径检测的Trail-based Off-road Multimodal Dataset (TOMD)数据集，并开发了一种动态多尺度数据融合模型，验证了不同光照条件下融合策略的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有数据集和模型主要针对城市环境或宽阔的越野路径，无法满足狭窄、类似小径的非结构化户外环境的需求，特别是在搜索救援和森林火灾等关键应用中。

Method: 引入了TOMD数据集，包含高保真多模态传感器数据（如128通道LiDAR、立体图像等），并提出了一种动态多尺度数据融合模型，分析了早期、交叉和混合融合策略在不同光照条件下的表现。

Result: 研究提出的方法在可通行路径预测中表现有效，并证实了光照条件对分割性能的重要影响。

Conclusion: TOMD数据集及动态多尺度融合模型为基于小径的越野导航研究提供了重要支持，相关数据和模型已公开以促进未来研究。

Abstract: Detecting traversable pathways in unstructured outdoor environments remains a
significant challenge for autonomous robots, especially in critical
applications such as wide-area search and rescue, as well as incident
management scenarios like forest fires. Existing datasets and models primarily
target urban settings or wide, vehicle-traversable off-road tracks, leaving a
substantial gap in addressing the complexity of narrow, trail-like off-road
scenarios. To address this, we introduce the Trail-based Off-road Multimodal
Dataset (TOMD), a comprehensive dataset specifically designed for such
environments. TOMD features high-fidelity multimodal sensor data -- including
128-channel LiDAR, stereo imagery, GNSS, IMU, and illumination measurements --
collected through repeated traversals under diverse conditions. We also propose
a dynamic multiscale data fusion model for accurate traversable pathway
prediction. The study analyzes the performance of early, cross, and mixed
fusion strategies under varying illumination levels. Results demonstrate the
effectiveness of our approach and the relevance of illumination in segmentation
performance. We publicly release TOMD at https://github.com/yyyxs1125/TMOD to
support future research in trail-based off-road navigation.

</details>


### [252] [Experimental investigation of pose informed reinforcement learning for skid-steered visual navigation](https://arxiv.org/abs/2506.21732)
*Ameya Salvi,Venkat Krovi*

Main category: cs.RO

TL;DR: 该论文提出了一种新颖的结构化学习方法，用于视觉导航，特别针对滑移转向车辆在复杂地形中的自动驾驶问题，通过软件模拟和硬件评估验证了其性能优势。


<details>
  <summary>Details</summary>
Motivation: 滑移转向车辆在越野环境中的自动化部署面临建模瓶颈，尤其是滑移-滑动轮地交互的精确分析模型缺失，促使研究者探索端到端学习方法。

Method: 采用结构化学习视觉导航的新方法，结合模仿学习和深度强化学习，通过软件模拟和硬件实验进行验证。

Result: 广泛的软件模拟、硬件评估和消融研究表明，所提方法在动态操作条件下显著优于现有文献方法。

Conclusion: 论文提出的结构化学习方法有效解决了滑移转向车辆视觉导航的挑战，为复杂地形下的自动驾驶提供了可行方案。

Abstract: Vision-based lane keeping is a topic of significant interest in the robotics
and autonomous ground vehicles communities in various on-road and off-road
applications. The skid-steered vehicle architecture has served as a useful
vehicle platform for human controlled operations. However, systematic modeling,
especially of the skid-slip wheel terrain interactions (primarily in off-road
settings) has created bottlenecks for automation deployment. End-to-end
learning based methods such as imitation learning and deep reinforcement
learning, have gained prominence as a viable deployment option to counter the
lack of accurate analytical models. However, the systematic formulation and
subsequent verification/validation in dynamic operation regimes (particularly
for skid-steered vehicles) remains a work in progress. To this end, a novel
approach for structured formulation for learning visual navigation is proposed
and investigated in this work. Extensive software simulations, hardware
evaluations and ablation studies now highlight the significantly improved
performance of the proposed approach against contemporary literature.

</details>


### [253] [ASVSim (AirSim for Surface Vehicles): A High-Fidelity Simulation Framework for Autonomous Surface Vehicle Research](https://arxiv.org/abs/2506.22174)
*Bavo Lesy,Siemen Herremans,Robin Kerstens,Jan Steckel,Walter Daems,Siegfried Mercelis,Ali Anwar*

Main category: cs.RO

TL;DR: 本文介绍了ASVSim，一个专为内河和港口环境设计的开源仿真框架，用于自主航运研究，结合了船舶动力学和海洋传感器模拟能力，支持生成合成数据集。


<details>
  <summary>Details</summary>
Motivation: 运输行业对无人水面车辆（USVs）的兴趣增加，尤其是在欧盟绿色协议推动内河运输的背景下。然而，缺乏开源、高保真度的仿真框架和数据集，阻碍了自主解决方案的开发与评估。

Method: 基于Cosys-AirSim，开发了ASVSim框架，模拟船舶动力学和海洋传感器（如雷达和摄像头），支持生成合成数据集，用于训练计算机视觉模型和强化学习代理。

Result: ASVSim为自主导航算法开发和合成数据集生成提供了全面平台，支持传统控制方法和深度学习方法的研究。

Conclusion: ASVSim作为开源项目（MIT许可），使海洋工程界更易进行自主导航研究，展示了在相关研究领域的潜力。

Abstract: The transport industry has recently shown significant interest in unmanned
surface vehicles (USVs), specifically for port and inland waterway transport.
These systems can improve operational efficiency and safety, which is
especially relevant in the European Union, where initiatives such as the Green
Deal are driving a shift towards increased use of inland waterways. At the same
time, a shortage of qualified personnel is accelerating the adoption of
autonomous solutions. However, there is a notable lack of open-source,
high-fidelity simulation frameworks and datasets for developing and evaluating
such solutions. To address these challenges, we introduce AirSim For Surface
Vehicles (ASVSim), an open-source simulation framework specifically designed
for autonomous shipping research in inland and port environments. The framework
combines simulated vessel dynamics with marine sensor simulation capabilities,
including radar and camera systems and supports the generation of synthetic
datasets for training computer vision models and reinforcement learning agents.
Built upon Cosys-AirSim, ASVSim provides a comprehensive platform for
developing autonomous navigation algorithms and generating synthetic datasets.
The simulator supports research of both traditional control methods and deep
learning-based approaches. Through limited experiments, we demonstrate the
potential of the simulator in these research areas. ASVSim is provided as an
open-source project under the MIT license, making autonomous navigation
research accessible to a larger part of the ocean engineering community.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [254] [Joint Task Offloading and Resource Allocation in Low-Altitude MEC via Graph Attention Diffusion](https://arxiv.org/abs/2506.21933)
*Yifan Xue,Ruihuai Liang,Bo Yang,Xuelin Cao,Zhiwen Yu,Mérouane Debbah,Chau Yuen*

Main category: cs.NI

TL;DR: 本文提出了一种基于图注意力扩散的解决方案生成器（GADSG），用于低空经济网络中异构MEC系统的任务卸载与资源分配联合优化，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 低空经济快速发展下，空地一体化MEC系统面临实时智能任务调度的需求，但节点异构、通信链路不稳定和任务动态变化等问题亟待解决。

Method: 构建三层异构MEC系统架构，利用图注意力网络的情境感知和扩散模型的解分布学习能力，在高维潜在空间联合建模离散卸载与连续资源分配变量。

Result: 多规模拓扑仿真实验表明，GADSG在优化性能、鲁棒性和任务结构泛化性上显著优于基线方法。

Conclusion: GADSG在动态复杂的低空经济网络环境中展现出高效任务调度的强大潜力。

Abstract: With the rapid development of the low-altitude economy, air-ground integrated
multi-access edge computing (MEC) systems are facing increasing demands for
real-time and intelligent task scheduling. In such systems, task offloading and
resource allocation encounter multiple challenges, including node
heterogeneity, unstable communication links, and dynamic task variations. To
address these issues, this paper constructs a three-layer heterogeneous MEC
system architecture for low-altitude economic networks, encompassing aerial and
ground users as well as edge servers. The system is systematically modeled from
the perspectives of communication channels, computational costs, and constraint
conditions, and the joint optimization problem of offloading decisions and
resource allocation is uniformly abstracted into a graph-structured modeling
task. On this basis, we propose a graph attention diffusion-based solution
generator (GADSG). This method integrates the contextual awareness of graph
attention networks with the solution distribution learning capability of
diffusion models, enabling joint modeling and optimization of discrete
offloading variables and continuous resource allocation variables within a
high-dimensional latent space. We construct multiple simulation datasets with
varying scales and topologies. Extensive experiments demonstrate that the
proposed GADSG model significantly outperforms existing baseline methods in
terms of optimization performance, robustness, and generalization across task
structures, showing strong potential for efficient task scheduling in dynamic
and complex low-altitude economic network environments.

</details>


### [255] [Concept-Level AI for Telecom: Moving Beyond Large Language Models](https://arxiv.org/abs/2506.22359)
*Viswanath Kumarskandpriya,Abdulhalim Dandoush,Abbas Bradai,Ali Belgacem*

Main category: cs.NI

TL;DR: 论文提出大型概念模型（LCMs）比大型语言模型（LLMs）更适合解决电信领域复杂问题，如跨层依赖、时空故障关联等。


<details>
  <summary>Details</summary>
Motivation: 电信网络管理面临复杂、分层、多域和多语言系统的挑战，LLMs因逐词处理和上下文限制难以满足需求。

Method: 采用基于语义概念推理的LCMs，利用双曲潜在空间进行层次表示，封装多层网络交互。

Result: LCMs在内存效率、跨层关联和多模态集成方面优于LLMs，更适合电信管理。

Conclusion: LCMs是实现稳健高效AI驱动电信管理的必要进化步骤。

Abstract: The telecommunications and networking domain stands at the precipice of a
transformative era, driven by the necessity to manage increasingly complex,
hierarchical, multi administrative domains (i.e., several operators on the same
path) and multilingual systems. Recent research has demonstrated that Large
Language Models (LLMs), with their exceptional general-purpose text analysis
and code generation capabilities, can be effectively applied to certain telecom
problems (e.g., auto-configuration of data plan to meet certain application
requirements). However, due to their inherent token-by-token processing and
limited capacity for maintaining extended context, LLMs struggle to fulfill
telecom-specific requirements such as cross-layer dependency cascades (i.e.,
over OSI), temporal-spatial fault correlation, and real-time distributed
coordination. In contrast, Large Concept Models (LCMs), which reason at the
abstraction level of semantic concepts rather than individual lexical tokens,
offer a fundamentally superior approach for addressing these telecom
challenges. By employing hyperbolic latent spaces for hierarchical
representation and encapsulating complex multi-layered network interactions
within concise concept embeddings, LCMs overcome critical shortcomings of LLMs
in terms of memory efficiency, cross-layer correlation, and native multimodal
integration. This paper argues that adopting LCMs is not simply an incremental
step, but a necessary evolutionary leap toward achieving robust and effective
AI-driven telecom management.

</details>
