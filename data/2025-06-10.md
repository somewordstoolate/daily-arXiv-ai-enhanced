<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 134]
- [cs.AI](#cs.AI) [Total: 77]
- [cs.LG](#cs.LG) [Total: 195]
- [stat.ME](#stat.ME) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 4]
- [cs.LO](#cs.LO) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.SD](#cs.SD) [Total: 6]
- [q-fin.CP](#q-fin.CP) [Total: 2]
- [eess.SP](#eess.SP) [Total: 21]
- [cs.DC](#cs.DC) [Total: 2]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [eess.SY](#eess.SY) [Total: 4]
- [cs.MA](#cs.MA) [Total: 6]
- [cs.SI](#cs.SI) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [cs.NE](#cs.NE) [Total: 5]
- [cs.MS](#cs.MS) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.IR](#cs.IR) [Total: 15]
- [cs.SE](#cs.SE) [Total: 5]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 84]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.CY](#cs.CY) [Total: 11]
- [cs.CR](#cs.CR) [Total: 18]
- [stat.ML](#stat.ML) [Total: 12]
- [eess.AS](#eess.AS) [Total: 4]
- [math.PR](#math.PR) [Total: 1]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.HC](#cs.HC) [Total: 3]
- [eess.IV](#eess.IV) [Total: 2]
- [econ.TH](#econ.TH) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 3]
- [cs.RO](#cs.RO) [Total: 15]
- [math.NA](#math.NA) [Total: 1]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.AR](#cs.AR) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [How Significant Are the Real Performance Gains? An Unbiased Evaluation Framework for GraphRAG](https://arxiv.org/abs/2506.06331)
*Qiming Zeng,Xiao Yan,Hao Luo,Yuhao Lin,Yuxiang Wang,Fangcheng Fu,Bo Du,Quanqing Xu,Jiawei Jiang*

Main category: cs.CL

TL;DR: 本文提出一种无偏评估框架，发现现有GraphRAG方法的性能提升较此前报告更为有限，强调科学评估对相关研究的基础性作用。


<details>
  <summary>Details</summary>
Motivation: 当前GraphRAG的答案评估框架存在无关问题与评估偏差两大缺陷，可能导致对模型性能的误判。

Method: 通过图-文本联合锚定的问题生成技术产生与底层数据集更相关的问题，并设计无偏评估流程消除LLM答案评估中的偏差。

Result: 对三种典型GraphRAG方法的评估显示，其性能提升幅度较先前报告结果显著降低。

Conclusion: 尽管该框架仍可能存在缺陷，但揭示了建立科学评估体系对GraphRAG研究发展的重要性。

Abstract: By retrieving contexts from knowledge graphs, graph-based retrieval-augmented
generation (GraphRAG) enhances large language models (LLMs) to generate quality
answers for user questions. Many GraphRAG methods have been proposed and
reported inspiring performance in answer quality. However, we observe that the
current answer evaluation framework for GraphRAG has two critical flaws, i.e.,
unrelated questions and evaluation biases, which may lead to biased or even
wrong conclusions on performance. To tackle the two flaws, we propose an
unbiased evaluation framework that uses graph-text-grounded question generation
to produce questions that are more related to the underlying dataset and an
unbiased evaluation procedure to eliminate the biases in LLM-based answer
assessment. We apply our unbiased framework to evaluate 3 representative
GraphRAG methods and find that their performance gains are much more moderate
than reported previously. Although our evaluation framework may still have
flaws, it calls for scientific evaluations to lay solid foundations for
GraphRAG research.

</details>


### [2] [TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment](https://arxiv.org/abs/2506.06343)
*Taesoo Kim,Jong Hwan Ko*

Main category: cs.CL

TL;DR: TESU-LLM框架通过文本数据训练语音语言模型，无需大规模语音-文本配对数据，实现了高效且可扩展的语音模型构建。


<details>
  <summary>Details</summary>
Motivation: 现有语音语言模型依赖大规模语音-文本配对数据和大量计算资源，导致可扩展性和可访问性受限。

Method: 利用统一编码器将语义等效的文本和语音输入映射到共享潜在空间，并通过轻量级投影网络与LLM的嵌入空间对齐，实现从文本监督到语音推理的泛化。

Result: TESU-LLM在多个语音相关基准测试中表现优异，与使用大规模多模态数据集和大量计算资源训练的基线方法相当。

Conclusion: TESU-LLM展示了无需语音数据即可构建高效语音语言模型的潜力，为语音LLM的构建提供了可扩展的路径。

Abstract: Recent advances in speech-enabled language models have shown promising
results in building intelligent voice assistants. However, most existing
approaches rely on large-scale paired speech-text data and extensive
computational resources, which pose challenges in terms of scalability and
accessibility. In this paper, we present \textbf{TESU-LLM}, a novel framework
that enables training speech-capable language models using only text data. Our
key insight is to leverage a unified encoder that maps semantically equivalent
text and speech inputs to a shared latent space. By aligning the encoder output
with the embedding space of a LLM via a lightweight projection network, we
enable the model to generalize from text-only supervision to speech-based
inference. Despite being trained exclusively on text, TESU-LLM achieves strong
performance on various speech-related benchmarks, comparable to baseline
methods trained with large-scale multimodal datasets and substantial
computational resources. These results highlight the effectiveness and
efficiency of our approach, offering a scalable path toward building speech
LLMs without speech data.

</details>


### [3] [Unified Game Moderation: Soft-Prompting and LLM-Assisted Label Transfer for Resource-Efficient Toxicity Detection](https://arxiv.org/abs/2506.06347)
*Zachary Yang,Domenico Tullo,Reihaneh Rabbany*

Main category: cs.CL

TL;DR: 本文提出两种方法提升多游戏实时毒性检测的扩展性：软提示整合游戏上下文，LLM辅助标签迁移扩展多语言支持，显著降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 游戏社区多语言多平台的毒性检测面临扩展性挑战，需在保证实时性的同时降低计算和维护成本。

Method: 1. 软提示技术使单一模型适配多游戏；2. 基于GPT-4o-mini的标签迁移框架扩展7种语言支持。

Result: 德语检测F1达58.88%超越英语基准(45.39%)，生产环境日均识别50名违规玩家/游戏，资源消耗显著降低。

Conclusion: 统一模型通过上下文整合与LLM辅助迁移，有效实现跨游戏跨语言的规模化实时毒性检测。

Abstract: Toxicity detection in gaming communities faces significant scaling challenges
when expanding across multiple games and languages, particularly in real-time
environments where computational efficiency is crucial. We present two key
findings to address these challenges while building upon our previous work on
ToxBuster, a BERT-based real-time toxicity detection system. First, we
introduce a soft-prompting approach that enables a single model to effectively
handle multiple games by incorporating game-context tokens, matching the
performance of more complex methods like curriculum learning while offering
superior scalability. Second, we develop an LLM-assisted label transfer
framework using GPT-4o-mini to extend support to seven additional languages.
Evaluations on real game chat data across French, German, Portuguese, and
Russian achieve macro F1-scores ranging from 32.96% to 58.88%, with
particularly strong performance in German, surpassing the English benchmark of
45.39%. In production, this unified approach significantly reduces
computational resources and maintenance overhead compared to maintaining
separate models for each game and language combination. At Ubisoft, this model
successfully identifies an average of 50 players, per game, per day engaging in
sanctionable behavior.

</details>


### [4] [Relationship Detection on Tabular Data Using Statistical Analysis and Large Language Models](https://arxiv.org/abs/2506.06371)
*Panagiotis Koletsis,Christos Panagiotopoulos,Georgios Th. Papadopoulos,Vasilis Efthymiou*

Main category: cs.CL

TL;DR: 本文提出了一种混合方法，利用知识图谱和大型语言模型来检测未标记表格数据中的列关系，并通过统计分析减少搜索空间。


<details>
  <summary>Details</summary>
Motivation: 表格解释任务在过去几年取得了显著进展，本文旨在通过结合知识图谱和大型语言模型，进一步提升未标记表格数据中列关系的检测效果。

Method: 本文采用混合方法，结合知识图谱和大型语言模型，通过域和范围约束检测以及关系共现分析来减少搜索空间。

Result: 实验结果表明，该方法在SemTab挑战提供的两个基准数据集上表现优异，与现有最先进方法竞争。

Conclusion: 本文提出的方法在未标记表格数据的列关系检测任务中表现出色，证明了其在实际应用中的潜力。

Abstract: Over the past few years, table interpretation tasks have made significant
progress due to their importance and the introduction of new technologies and
benchmarks in the field. This work experiments with a hybrid approach for
detecting relationships among columns of unlabeled tabular data, using a
Knowledge Graph (KG) as a reference point, a task known as CPA. This approach
leverages large language models (LLMs) while employing statistical analysis to
reduce the search space of potential KG relations. The main modules of this
approach for reducing the search space are domain and range constraints
detection, as well as relation co-appearance analysis. The experimental
evaluation on two benchmark datasets provided by the SemTab challenge assesses
the influence of each module and the effectiveness of different
state-of-the-art LLMs at various levels of quantization. The experiments were
performed, as well as at different prompting techniques. The proposed
methodology, which is publicly available on github, proved to be competitive
with state-of-the-art approaches on these datasets.

</details>


### [5] [Enhancing Decision-Making of Large Language Models via Actor-Critic](https://arxiv.org/abs/2506.06376)
*Heng Dong,Kefei Duan,Chongjie Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型（LLM）的Actor-Critic框架LAC，通过长期动作价值评估和无梯度策略优化，显著提升了复杂多步决策任务的性能，并在多个实验环境中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在复杂决策场景中存在长期推理不足和结果评估不准确的问题，导致次优决策。传统方法依赖短期自回归动作生成，或难以有效模拟推演和评估结果。

Method: LAC框架包含两个核心技术：1) 通过正负结果关联的token概率计算Q值，结合未来轨迹推演增强动作评估；2) 采用无梯度机制实现高效策略改进。

Result: 在ALFWorld、BabyAI-Text和WebShop等多样化环境中，使用7B/8B参数的LLM即达到SOTA性能，部分复杂任务甚至超越基于GPT-4的基线方法。

Conclusion: 将结构化策略优化与LLM内在知识结合，可有效提升多步环境下的决策能力，证明轻量级模型通过系统框架设计也能实现超越大型模型的决策表现。

Abstract: Large Language Models (LLMs) have achieved remarkable advancements in natural
language processing tasks, yet they encounter challenges in complex
decision-making scenarios that require long-term reasoning and alignment with
high-level objectives. Existing methods either rely on short-term
auto-regressive action generation or face limitations in accurately simulating
rollouts and assessing outcomes, leading to sub-optimal decisions. This paper
introduces a novel LLM-based Actor-Critic framework, termed LAC, that
effectively improves LLM policies with long-term action evaluations in a
principled and scalable way. Our approach addresses two key challenges: (1)
extracting robust action evaluations by computing Q-values via token logits
associated with positive/negative outcomes, enhanced by future trajectory
rollouts and reasoning; and (2) enabling efficient policy improvement through a
gradient-free mechanism. Experiments across diverse environments -- including
high-level decision-making (ALFWorld), low-level action spaces (BabyAI-Text),
and large action spaces (WebShop) -- demonstrate the framework's generality and
superiority over state-of-the-art methods. Notably, our approach achieves
competitive performance using 7B/8B parameter LLMs, even outperforming baseline
methods employing GPT-4 in complex tasks. These results underscore the
potential of integrating structured policy optimization with LLMs' intrinsic
knowledge to advance decision-making capabilities in multi-step environments.

</details>


### [6] [Detection Method for Prompt Injection by Integrating Pre-trained Model and Heuristic Feature Engineering](https://arxiv.org/abs/2506.06384)
*Yi Ji,Runzhi Li,Baolei Mao*

Main category: cs.CL

TL;DR: 本文提出了一种名为DMPI-PMHFE的双通道特征融合检测框架，用于检测大语言模型中的提示注入攻击，结合预训练语言模型和启发式特征工程，显著提高了检测效果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，提示注入攻击成为重要的安全威胁，现有防御机制在有效性和通用性之间存在权衡，亟需高效的跨模型检测方法。

Method: DMPI-PMHFE框架结合DeBERTa-v3-base作为特征提取器，生成富含上下文信息的语义向量，同时基于已知攻击模式设计启发式规则提取显式结构特征，通过全连接神经网络进行最终预测。

Result: 实验结果表明，DMPI-PMHFE在多个基准数据集上的准确率、召回率和F1分数均优于现有方法，并在实际部署中显著降低了主流大语言模型的攻击成功率。

Conclusion: DMPI-PMHFE通过双通道特征融合有效缓解了仅依赖DeBERTa提取特征的局限性，为跨大语言模型的提示注入攻击检测提供了高效解决方案。

Abstract: With the widespread adoption of Large Language Models (LLMs), prompt
injection attacks have emerged as a significant security threat. Existing
defense mechanisms often face critical trade-offs between effectiveness and
generalizability. This highlights the urgent need for efficient prompt
injection detection methods that are applicable across a wide range of LLMs. To
address this challenge, we propose DMPI-PMHFE, a dual-channel feature fusion
detection framework. It integrates a pretrained language model with heuristic
feature engineering to detect prompt injection attacks. Specifically, the
framework employs DeBERTa-v3-base as a feature extractor to transform input
text into semantic vectors enriched with contextual information. In parallel,
we design heuristic rules based on known attack patterns to extract explicit
structural features commonly observed in attacks. Features from both channels
are subsequently fused and passed through a fully connected neural network to
produce the final prediction. This dual-channel approach mitigates the
limitations of relying only on DeBERTa to extract features. Experimental
results on diverse benchmark datasets demonstrate that DMPI-PMHFE outperforms
existing methods in terms of accuracy, recall, and F1-score. Furthermore, when
deployed actually, it significantly reduces attack success rates across
mainstream LLMs, including GLM-4, LLaMA 3, Qwen 2.5, and GPT-4o.

</details>


### [7] [Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models](https://arxiv.org/abs/2506.06395)
*Pengyi Li,Matvey Skripkin,Alexander Zubrey,Andrey Kuznetsov,Ivan Oseledets*

Main category: cs.CL

TL;DR: 本文提出了一种基于自置信度的强化学习方法（RLSC），通过使用模型自身的置信度作为奖励信号，减少了对人工标注或外部奖励模型的依赖，显著提升了模型在数学推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法通常依赖于昂贵的人工标注或外部奖励模型，本文旨在提出一种更简单、可扩展的后训练方法，减少对监督的依赖。

Method: 提出了Reinforcement Learning via Self-Confidence (RLSC)，利用模型自身的置信度作为奖励信号，无需标签、偏好模型或奖励工程。

Result: 在Qwen2.5-Math-7B模型上，RLSC在AIME2024、MATH500和AMC23数据集上的准确率分别提升了20.10%、49.40%和52.50%。

Conclusion: RLSC为推理模型提供了一种简单、可扩展的后训练方法，显著提升了模型性能，且仅需极少的监督。

Abstract: Large language models (LLMs) excel at reasoning, yet post-training remains
critical for aligning their behavior with task goals. Existing reinforcement
learning (RL) methods often depend on costly human annotations or external
reward models. We propose Reinforcement Learning via Self-Confidence (RLSC),
which uses the model's own confidence as reward signals-eliminating the need
for labels, preference models, or reward engineering. Applied to
Qwen2.5-Math-7B with only 8 samples per question and 4 training epochs, RLSC
improves accuracy by +20.10% on AIME2024, +49.40% on MATH500, and +52.50% on
AMC23. RLSC offers a simple, scalable post-training method for reasoning models
with minimal supervision.

</details>


### [8] [Natural Language Interaction with Databases on Edge Devices in the Internet of Battlefield Things](https://arxiv.org/abs/2506.06396)
*Christopher D. Molek,Roberto Fronteddu,K. Brent Venable,Niranjan Suri*

Main category: cs.CL

TL;DR: 提出基于边缘设备大型语言模型（LLM）的两步工作流，通过自然语言处理实现战场物联网（IoBT）数据库交互，Llama 3.1模型在查询生成与总结任务中表现最优，准确率提升19.4%。


<details>
  <summary>Details</summary>
Motivation: 战场物联网（IoBT）数据需转化为可即时访问的决策信息，但现有方法受限于精确匹配要求，需通过自然语言交互提升动态网络环境下的态势感知能力。

Method: 采用适配边缘设备的LLM模型，将自然语言问题映射为图数据库Cypher查询，并通过LLM将查询结果转化为自然语言总结，结合NLP与图数据库技术处理IoBT动态网络。

Result: Llama 3.1（80亿参数）在美军多用途传感区数据集上全面优于其他中型LLM，两步法放宽精确匹配限制后实现19.4%的准确率提升。

Conclusion: 该工作流验证了边缘部署LLM实现自然语言数据库交互的可行性，为战场关键决策提供灵活高效的信息提取框架，突破传统精确匹配方法的性能瓶颈。

Abstract: The expansion of the Internet of Things (IoT) in the battlefield, Internet of
Battlefield Things (IoBT), gives rise to new opportunities for enhancing
situational awareness. To increase the potential of IoBT for situational
awareness in critical decision making, the data from these devices must be
processed into consumer-ready information objects, and made available to
consumers on demand. To address this challenge we propose a workflow that makes
use of natural language processing (NLP) to query a database technology and
return a response in natural language. Our solution utilizes Large Language
Models (LLMs) that are sized for edge devices to perform NLP as well as
graphical databases which are well suited for dynamic connected networks which
are pervasive in the IoBT. Our architecture employs LLMs for both mapping
questions in natural language to Cypher database queries as well as to
summarize the database output back to the user in natural language. We evaluate
several medium sized LLMs for both of these tasks on a database representing
publicly available data from the US Army's Multipurpose Sensing Area (MSA) at
the Jornada Range in Las Cruces, NM. We observe that Llama 3.1 (8 billion
parameters) outperforms the other models across all the considered metrics.
Most importantly, we note that, unlike current methods, our two step approach
allows the relaxation of the Exact Match (EM) requirement of the produced
Cypher queries with ground truth code and, in this way, it achieves a 19.4%
increase in accuracy. Our workflow lays the ground work for deploying LLMs on
edge devices to enable natural language interactions with databases containing
information objects for critical decision making.

</details>


### [9] [Direct Behavior Optimization: Unlocking the Potential of Lightweight LLMs](https://arxiv.org/abs/2506.06401)
*Hongming Yang,Shi Lin,Jun Shao,Changting Lin,Donghai Zhu,Meng Han,Qinglei Kong*

Main category: cs.CL

TL;DR: 本文提出了一种名为DeBoP的直接行为优化范式，旨在提升轻量级大语言模型（LwLLMs）的推理能力，减少计算时间，并在多项任务中超越现有提示优化方法。


<details>
  <summary>Details</summary>
Motivation: 轻量级大语言模型（LwLLMs）虽然在资源效率、成本效益和数据隐私方面具有优势，但其推理能力有限，难以处理复杂任务。现有的提示优化方法通常依赖大量人工或先进LLMs的元认知能力，对LwLLMs效果不佳。

Method: DeBoP是一种自动优化方法，源自链式思维（CoT）提示技术，通过无梯度的蒙特卡洛树搜索将复杂提示优化转化为离散、可量化的执行序列优化。

Result: 在七项复杂任务中，DeBoP优化的LwLLMs显著优于现有提示优化方法，并在大多数任务中超越GPT-3.5，同时将计算时间减少约60%。

Conclusion: DeBoP有效提升了LwLLMs的推理能力，减少了计算时间，为轻量级模型的优化提供了新的解决方案。

Abstract: Lightweight Large Language Models (LwLLMs) are reduced-parameter, optimized
models designed to run efficiently on consumer-grade hardware, offering
significant advantages in resource efficiency, cost-effectiveness, and data
privacy. However, these models often struggle with limited inference and
reasoning capabilities, which restrict their performance on complex tasks and
limit their practical applicability. Moreover, existing prompt optimization
methods typically rely on extensive manual effort or the meta-cognitive
abilities of state-of-the-art LLMs, making them less effective for LwLLMs. To
address these challenges, we introduce DeBoP, a new Direct Behavior
Optimization Paradigm, original from the Chain-of-Thought (CoT) prompting
technique. Unlike CoT Prompting, DeBoP is an automatic optimization method,
which focuses on the optimization directly on the behavior of LwLLMs. In
particular, DeBoP transforms the optimization of complex prompts into the
optimization of discrete, quantifiable execution sequences using a
gradient-free Monte Carlo Tree Search. We evaluate DeBoP on seven challenging
tasks where state-of-the-art LLMs excel but LwLLMs generally underperform.
Experimental results demonstrate that DeBoP significantly outperforms recent
prompt optimization methods on most tasks. In particular, DeBoP-optimized
LwLLMs surpass GPT-3.5 on most tasks while reducing computational time by
approximately 60% compared to other automatic prompt optimization methods.

</details>


### [10] [Unintended Harms of Value-Aligned LLMs: Psychological and Empirical Insights](https://arxiv.org/abs/2506.06404)
*Sooyung Choi,Jaehyeok Lee,Xiaoyuan Yi,Jing Yao,Xing Xie,JinYeong Bak*

Main category: cs.CL

TL;DR: 研究指出价值观对齐的大型语言模型（LLMs）可能因生成符合特定价值观的内容而增加安全风险，并提出情境对齐方法以提升安全性。


<details>
  <summary>Details</summary>
Motivation: 随着个性化LLMs的发展，其与人类价值观对齐的需求增加，但价值观可能隐含有害信息，需探究此类模型的安全风险及心理机制。

Method: 使用含详细安全分类的数据集分析价值观对齐LLMs的安全风险，结合心理学假设验证相关性，并提出情境对齐方法。

Result: 价值观对齐LLMs比未微调模型更易产生有害行为，且传统安全评估风险略高于其他微调模型；其危害源于严格遵循价值观生成内容。

Conclusion: 价值观对齐可能放大LLMs的安全隐患，但通过情境对齐方法可缓解风险。研究揭示了价值观对齐的潜在机制，为安全优化提供方向。

Abstract: The application scope of Large Language Models (LLMs) continues to expand,
leading to increasing interest in personalized LLMs that align with human
values. However, aligning these models with individual values raises
significant safety concerns, as certain values may correlate with harmful
information. In this paper, we identify specific safety risks associated with
value-aligned LLMs and investigate the psychological principles behind these
challenges. Our findings reveal two key insights. (1) Value-aligned LLMs are
more prone to harmful behavior compared to non-fine-tuned models and exhibit
slightly higher risks in traditional safety evaluations than other fine-tuned
models. (2) These safety issues arise because value-aligned LLMs genuinely
generate text according to the aligned values, which can amplify harmful
outcomes. Using a dataset with detailed safety categories, we find significant
correlations between value alignment and safety risks, supported by
psychological hypotheses. This study offers insights into the "black box" of
value alignment and proposes in-context alignment methods to enhance the safety
of value-aligned LLMs.

</details>


### [11] [SMAR: Soft Modality-Aware Routing Strategy for MoE-based Multimodal Large Language Models Preserving Language Capabilities](https://arxiv.org/abs/2506.06406)
*Guoyang Xia,Yifeng Ding,Fengfa Li,Lei Ren,Chen Wei,Fangxiang Feng,Xiaojie Wang*

Main category: cs.CL

TL;DR: 提出一种名为SMAR的正则化方法，通过KL散度控制多模态路由分布，解决多模态MoE模型训练成本高、语言能力退化的问题，实验显示在仅用2.5%纯文本时仍保留86.6%语言能力。


<details>
  <summary>Details</summary>
Motivation: 现有构建多模态MoE模型的方法存在高训练成本或语言能力下降的缺陷，需在保持预训练语言能力的同时实现模态差异化路由。

Method: 使用KL散度正则化技术（SMAR）控制跨模态路由概率分布，无需修改模型架构或依赖大量文本数据，促进专家专业化。

Result: 在视觉指令调优任务中，SMAR以仅2.5%纯文本数据实现86.6%语言能力保留率，优于基线方法且多模态性能稳定。

Conclusion: SMAR为多模态MoE模型提供了模态区分与语言能力平衡的实用解决方案，具有高效性和架构兼容性优势。

Abstract: Mixture of Experts (MoE) architectures have become a key approach for scaling
large language models, with growing interest in extending them to multimodal
tasks. Existing methods to build multimodal MoE models either incur high
training costs or suffer from degraded language capabilities when adapting
pretrained models. To address this, we propose Soft ModalityAware Routing
(SMAR), a novel regularization technique that uses Kullback Leibler divergence
to control routing probability distributions across modalities, encouraging
expert specialization without modifying model architecture or heavily relying
on textual data. Experiments on visual instruction tuning show that SMAR
preserves language ability at 86.6% retention with only 2.5% pure text,
outperforming baselines while maintaining strong multimodal performance. Our
approach offers a practical and efficient solution to balance modality
differentiation and language capabilities in multimodal MoE models.

</details>


### [12] [Canonical Autoregressive Generation](https://arxiv.org/abs/2506.06446)
*Ivi Chatzi,Nina Corvelo Benz,Stratis Tsirtsis,Manuel Gomez-Rodriguez*

Main category: cs.CL

TL;DR: 本文提出了一种称为规范采样的方法，确保大语言模型生成规范化的token序列，从而提高生成序列的质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在生成token序列时，并不总是生成规范的token序列，这可能导致生成结果的不一致性和质量下降。

Method: 本文首先证明了在自回归生成过程中，模型需要在每一步生成规范的token序列，然后提出了一种简单高效的规范采样方法，避免生成非规范的token序列。

Result: 与标准采样相比，规范采样生成的token序列分布更接近训练数据中的真实分布。

Conclusion: 规范采样方法能够有效提高大语言模型生成序列的规范性和一致性，从而提升模型的表现。

Abstract: State of the art large language models are trained using large amounts of
tokens derived from raw text using what is called a tokenizer. Crucially, the
tokenizer determines the (token) vocabulary a model will use during inference
as well as, in principle, the (token) language. This is because, while the
token vocabulary may allow for different tokenizations of a string, the
tokenizer always maps the string to only one of these tokenizations--the
canonical tokenization. However, multiple lines of empirical evidence suggest
that large language models do not always generate canonical token sequences,
and this comes with several negative consequences. In this work, we first show
that, to generate a canonical token sequence, a model needs to generate
(partial) canonical token sequences at each step of the autoregressive
generation process underpinning its functioning. Building upon this theoretical
result, we introduce canonical sampling, a simple and efficient sampling method
that precludes a given model from generating non-canonical token sequences.
Further, we also show that, in comparison with standard sampling, the
distribution of token sequences generated using canonical sampling is provably
closer to the true distribution of token sequences used during training.

</details>


### [13] [What Is Seen Cannot Be Unseen: The Disruptive Effect of Knowledge Conflict on Large Language Models](https://arxiv.org/abs/2506.06485)
*Kaiser Sun,Fan Bai,Mark Dredze*

Main category: cs.CL

TL;DR: 研究提出诊断框架评估大语言模型在上下文与参数知识冲突时的表现，发现模型难以抑制内部知识，且知识一致性对性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 大语言模型依赖上下文与参数知识完成任务，但二者可能产生冲突。需系统评估模型在知识冲突下的行为，以指导实际部署。

Method: 构建上下文与参数知识冲突的诊断数据集，通过多任务类型分析模型表现，包括知识对齐/非对齐场景及添加冲突解释的对比实验。

Result: 1. 非知识任务不受冲突影响；2. 知识一致时性能更高；3. 模型无法完全抑制内部知识；4. 提供冲突解释会增强上下文依赖。

Conclusion: 知识冲突暴露模型评估有效性风险，需在LLM部署中针对性处理。模型对参数知识的固守性提示需开发更鲁棒的知识整合机制。

Abstract: Large language models frequently rely on both contextual input and parametric
knowledge to perform tasks. However, these sources can come into conflict,
especially when retrieved documents contradict the model's parametric
knowledge. We propose a diagnostic framework to systematically evaluate LLM
behavior under context-memory conflict, where the contextual information
diverges from their parametric beliefs. We construct diagnostic data that
elicit these conflicts and analyze model performance across multiple task
types. Our findings reveal that (1) knowledge conflict has minimal impact on
tasks that do not require knowledge utilization, (2) model performance is
consistently higher when contextual and parametric knowledge are aligned, (3)
models are unable to fully suppress their internal knowledge even when
instructed, and (4) providing rationales that explain the conflict increases
reliance on contexts. These insights raise concerns about the validity of
model-based evaluation and underscore the need to account for knowledge
conflict in the deployment of LLMs.

</details>


### [14] [Improving LLM-Powered EDA Assistants with RAFT](https://arxiv.org/abs/2506.06500)
*Luyao Shi,Michael Kazda,Charles Schmitter,Hemlata Gupta*

Main category: cs.CL

TL;DR: 本文提出使用合成Q/A数据集通过RAFT增强LLMs在EDA任务中的表现，并探讨了真实用户问题对合成数据生成的影响，同时实施了安全访问控制并评估了数据泄露风险。


<details>
  <summary>Details</summary>
Motivation: 电子设计工程师在获取设计验证和技术开发相关信息时效率低下，而现有的开源LLMs缺乏EDA领域的专业知识，导致在RAG环境中可能产生不准确的响应。

Method: 提出使用合成Q/A数据集进行RAFT，以增强LLMs在EDA任务中的表现，并探讨了真实用户问题作为RAFS示例对合成数据生成的影响，同时实施了安全访问控制。

Result: RAFT结合合成数据显著提升了LLMs在基于RAG的EDA任务中的表现，并提供了关于数据泄露和意外记忆风险的实用见解。

Conclusion: 通过RAFT和合成数据，LLMs在EDA任务中的表现得到显著提升，同时确保了敏感信息的安全性，并提供了关于数据泄露风险的实用建议。

Abstract: Electronic design engineers often struggle to efficiently access relevant
information for tasks like design verification and technology development.
While large language models (LLMs) can enhance productivity as conversational
agents, pre-trained open-source LLMs lack domain-specific knowledge for
Electronic Design Automation (EDA). In a Retrieval-Augmented Generation (RAG)
context, LLMs rely on external context but may still produce inaccurate
responses. Retrieval-Augmented Fine-Tuning (RAFT) improves LLM performance, but
acquiring labeled question/answer (Q/A) data in EDA is difficult. To address
this, we propose using synthetic Q/A datasets to enhance LLMs with RAFT. Our
results show that RAFT with synthetic data significantly boosts LLM performance
for RAG-based EDA tasks. We also investigate the impact of using real user
questions as Retrieval-Augmented Few-Shot (RAFS) examples for synthetic data
generation. Additionally, we implement secure access control to ensure
sensitive information is only accessible to authorized personnel. Finally, we
assess the risk of data leakage and unintended memorization during fine-tuning
with synthetic data, providing practical insights.

</details>


### [15] [Biases Propagate in Encoder-based Vision-Language Models: A Systematic Analysis From Intrinsic Measures to Zero-shot Retrieval Outcomes](https://arxiv.org/abs/2506.06506)
*Kshitish Ghate,Tessa Charlesworth,Mona Diab,Aylin Caliskan*

Main category: cs.CL

TL;DR: 研究发现视觉语言模型（VLMs）的社会群体偏见会系统性传播至零样本检索任务，且模型性能越强偏见传播越显著，需警惕复杂模型加剧公平性问题。


<details>
  <summary>Details</summary>
Motivation: 探究基础视觉语言模型（VLMs）中内在的社会群体偏见如何影响下游零样本检索任务的公平性，以推动构建更公正的AI系统。

Method: 提出控制框架，通过关联表示空间的内在偏见度量与零样本图文/图到文检索任务的外在偏见度量，分析114组实验、6个社会群体及3种VLMs的偏差传播模式。

Result: 内在与外在偏见呈现强相关性（平均ρ=0.83±0.10），模型性能越强偏见传播越显著；少数群体因表征不足导致结果稳定性更低。

Conclusion: 模型复杂化趋势可能加剧偏见传播风险，需建立基线评估任务监测群体与情感信号传播，尤其关注弱势群体的模型输出偏差。

Abstract: To build fair AI systems we need to understand how social-group biases
intrinsic to foundational encoder-based vision-language models (VLMs) manifest
in biases in downstream tasks. In this study, we demonstrate that intrinsic
biases in VLM representations systematically ``carry over'' or propagate into
zero-shot retrieval tasks, revealing how deeply rooted biases shape a model's
outputs. We introduce a controlled framework to measure this propagation by
correlating (a) intrinsic measures of bias in the representational space with
(b) extrinsic measures of bias in zero-shot text-to-image (TTI) and
image-to-text (ITT) retrieval. Results show substantial correlations between
intrinsic and extrinsic bias, with an average $\rho$ = 0.83 $\pm$ 0.10. This
pattern is consistent across 114 analyses, both retrieval directions, six
social groups, and three distinct VLMs. Notably, we find that
larger/better-performing models exhibit greater bias propagation, a finding
that raises concerns given the trend towards increasingly complex AI models.
Our framework introduces baseline evaluation tasks to measure the propagation
of group and valence signals. Investigations reveal that underrepresented
groups experience less robust propagation, further skewing their model-related
outcomes.

</details>


### [16] [Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality and Model Performance](https://arxiv.org/abs/2506.06522)
*Aladin Djuhera,Swanand Ravindra Kadhe,Syed Zawad,Farhan Ahmed,Heiko Ludwig,Holger Boche*

Main category: cs.CL

TL;DR: 本文对两个开源的训练后数据集Tulu-3-SFT-Mix和SmolTalk进行了全面分析，提出了一个新的数据集TuluTalk，该数据集在减少样本数量的同时，性能优于或匹配原数据集。


<details>
  <summary>Details</summary>
Motivation: 由于大多数领先的开源和闭源大语言模型（LLMs）使用的训练后数据集不公开，且缺乏透明度，本文旨在通过开源数据集的分析，揭示数据质量对模型性能的影响。

Method: 使用Magpie框架对Tulu-3-SFT-Mix和SmolTalk数据集进行详细的质量指标标注，包括对话结构、任务类别、输入质量和响应质量，并设计了一个新的数据集TuluTalk。

Result: TuluTalk数据集比原数据集减少了14%的样本，但在关键基准测试中性能优于或匹配原数据集。

Conclusion: 本文的研究为在资源有限的情况下构建更有效的训练后数据集提供了可行的见解，并公开了标注的源数据集和TuluTalk数据集以支持未来研究。

Abstract: Recent work on large language models (LLMs) has increasingly focused on
post-training and alignment with datasets curated to enhance instruction
following, world knowledge, and specialized skills. However, most post-training
datasets used in leading open- and closed-source LLMs remain inaccessible to
the public, with limited information about their construction process. This
lack of transparency has motivated the recent development of open-source
post-training corpora. While training on these open alternatives can yield
performance comparable to that of leading models, systematic comparisons remain
challenging due to the significant computational cost of conducting them
rigorously at scale, and are therefore largely absent. As a result, it remains
unclear how specific samples, task types, or curation strategies influence
downstream performance when assessing data quality. In this work, we conduct
the first comprehensive side-by-side analysis of two prominent open
post-training datasets: Tulu-3-SFT-Mix and SmolTalk. Using the Magpie
framework, we annotate each sample with detailed quality metrics, including
turn structure (single-turn vs. multi-turn), task category, input quality, and
response quality, and we derive statistics that reveal structural and
qualitative similarities and differences between the two datasets. Based on
these insights, we design a principled curation recipe that produces a new data
mixture, TuluTalk, which contains 14% fewer samples than either source dataset
while matching or exceeding their performance on key benchmarks. Our findings
offer actionable insights for constructing more effective post-training
datasets that improve model performance within practical resource limits. To
support future research, we publicly release both the annotated source datasets
and our curated TuluTalk mixture.

</details>


### [17] [Beyond Facts: Evaluating Intent Hallucination in Large Language Models](https://arxiv.org/abs/2506.06539)
*Yijie Hao,Haofei Yu,Jiaxuan You*

Main category: cs.CL

TL;DR: 论文提出意图幻觉概念，并引入FAITHQA基准和CONSTRAINT SCORE指标，评估大语言模型在复杂查询中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在处理包含多个条件的复杂查询时，往往只部分满足查询，忽略某些条件，导致意图幻觉。

Method: 引入FAITHQA基准，包含20,068个问题，涵盖查询和检索增强生成，并开发CONSTRAINT SCORE自动评估指标。

Result: 发现意图幻觉在先进模型中普遍存在，主要源于遗漏或误解，CONSTRAINT SCORE在检测意图幻觉上更接近人类表现。

Conclusion: 意图幻觉是大语言模型的常见问题，FAITHQA和CONSTRAINT SCORE为未来研究提供了有效工具。

Abstract: When exposed to complex queries containing multiple conditions, today's large
language models (LLMs) tend to produce responses that only partially satisfy
the query while neglecting certain conditions. We therefore introduce the
concept of Intent Hallucination. In this phenomenon, LLMs either omit
(neglecting to address certain parts) or misinterpret (responding to invented
query parts) elements of the given query, leading to intent hallucinated
generation. To systematically evaluate intent hallucination, we introduce
FAITHQA, a novel benchmark for intent hallucination that contains 20,068
problems, covering both query-only and retrieval-augmented generation (RAG)
setups with varying topics and difficulty. FAITHQA is the first hallucination
benchmark that goes beyond factual verification, tailored to identify the
fundamental cause of intent hallucination. By evaluating various LLMs on
FAITHQA, we find that (1) intent hallucination is a common issue even for
state-of-the-art models, and (2) the phenomenon stems from omission or
misinterpretation of LLMs. To facilitate future research, we introduce an
automatic LLM generation evaluation metric, CONSTRAINT SCORE, for detecting
intent hallucination. Human evaluation results demonstrate that CONSTRAINT
SCORE is closer to human performance for intent hallucination compared to
baselines.

</details>


### [18] [LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles](https://arxiv.org/abs/2506.06561)
*Ho Yin 'Sam' Ng,Ting-Yao Hsu,Aashish Anantha Ramakrishnan,Branislav Kveton,Nedim Lipka,Franck Dernoncourt,Dongwon Lee,Tong Yu,Sungchul Kim,Ryan A. Rossi,Ting-Hao 'Kenneth' Huang*

Main category: cs.CL

TL;DR: LaMP-Cap数据集引入多模态图表信息，提升个性化图表标题生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成的图表标题通常需要作者根据写作风格和领域风格进行修改，突显了个性化需求。

Method: LaMP-Cap数据集为每个目标图表提供多模态信息，包括图表图像、相关图表及其标题和提及段，作为上下文特征。

Result: 实验表明，使用多模态信息生成的标题更接近作者原创，且图像信息比提及段更有帮助。

Conclusion: 多模态信息在个性化图表标题生成中具有显著优势，优于纯文本信息。

Abstract: Figure captions are crucial for helping readers understand and remember a
figure's key message. Many models have been developed to generate these
captions, helping authors compose better quality captions more easily. Yet,
authors almost always need to revise generic AI-generated captions to match
their writing style and the domain's style, highlighting the need for
personalization. Despite language models' personalization (LaMP) advances,
these technologies often focus on text-only settings and rarely address
scenarios where both inputs and profiles are multimodal. This paper introduces
LaMP-Cap, a dataset for personalized figure caption generation with multimodal
figure profiles. For each target figure, LaMP-Cap provides not only the needed
inputs, such as figure images, but also up to three other figures from the same
document--each with its image, caption, and figure-mentioning paragraphs--as a
profile to characterize the context. Experiments with four LLMs show that using
profile information consistently helps generate captions closer to the original
author-written ones. Ablation studies reveal that images in the profile are
more helpful than figure-mentioning paragraphs, highlighting the advantage of
using multimodal profiles over text-only ones.

</details>


### [19] [Precise Information Control in Long-Form Text Generation](https://arxiv.org/abs/2506.06589)
*Jacqueline He,Howard Yen,Margaret Li,Shuyue Stella Li,Zhiyuan Zeng,Weijia Shi,Yulia Tsvetkov,Danqi Chen,Pang Wei Koh,Luke Zettlemoyer*

Main category: cs.CL

TL;DR: 本文提出精确信息控制（PIC）任务框架及PIC-Bench基准，发现主流语言模型存在超70%的内在幻觉问题，并通过弱监督训练8B参数PIC-LM模型将F1分数从69.1%提升至91.0%，显著提升生成文本的忠实性。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型生成看似合理但缺乏输入依据的『内在幻觉』问题，要求模型严格基于可验证声明生成长文本且不添加未支持内容。

Method: 1. 提出PIC任务框架（完整/部分）控制生成要求；2. 构建PIC-Bench基准（8个长文本生成任务）；3. 开发基于弱监督偏好数据的后训练框架，训练8B参数PIC-LM模型。

Result: SOTA模型在PIC-Bench上幻觉率达70%+；PIC-LM在完整PIC设置中F1从69.1%提升至91.0%，端到端任务中准确率提升30.5%（出生地验证）和召回率提升17.1%（模糊QA检索）。

Conclusion: PIC框架有效提升生成忠实性，证明精确控制生成对减少幻觉的可行性，为可信语言模型开发提供新方向。

Abstract: A central challenge in modern language models (LMs) is intrinsic
hallucination: the generation of information that is plausible but
unsubstantiated relative to input context. To study this problem, we propose
Precise Information Control (PIC), a new task formulation that requires models
to generate long-form outputs grounded in a provided set of short
self-contained statements, known as verifiable claims, without adding any
unsupported ones. For comprehensiveness, PIC includes a full setting that tests
a model's ability to include exactly all input claims, and a partial setting
that requires the model to selectively incorporate only relevant claims. We
present PIC-Bench, a benchmark of eight long-form generation tasks (e.g.,
summarization, biography generation) adapted to the PIC setting, where LMs are
supplied with well-formed, verifiable input claims. Our evaluation of a range
of open and proprietary LMs on PIC-Bench reveals that, surprisingly,
state-of-the-art LMs still intrinsically hallucinate in over 70% of outputs. To
alleviate this lack of faithfulness, we introduce a post-training framework,
using a weakly supervised preference data construction method, to train an 8B
PIC-LM with stronger PIC ability--improving from 69.1% to 91.0% F1 in the full
PIC setting. When integrated into end-to-end factual generation pipelines,
PIC-LM improves exact match recall by 17.1% on ambiguous QA with retrieval, and
factual precision by 30.5% on a birthplace verification task, underscoring the
potential of precisely grounded generation.

</details>


### [20] [MedCite: Can Language Models Generate Verifiable Text for Medicine?](https://arxiv.org/abs/2506.06605)
*Xiao Wang,Mengjue Tan,Qiao Jin,Guangzhi Xiong,Yu Hu,Aidong Zhang,Zhiyong Lu,Minjia Zhang*

Main category: cs.CL

TL;DR: 本文提出首个端到端医疗引用生成框架\name，通过多轮检索-引用方法显著提升引用质量，并建立系统化评估体系。


<details>
  <summary>Details</summary>
Motivation: 现有医疗问答系统缺乏引用生成与评估能力，阻碍其实际应用。需构建支持引用生成与质量评估的系统框架。

Method: 提出端到端框架\name，创新性引入多轮检索-引用机制，整合系统设计、引用生成与专业评估功能。

Result: 方法在引用精确度/召回率上优于基线模型，评估结果与专家标注高度相关，关键设计选择显著影响最终质量。

Conclusion: 框架验证了医疗引用生成的技术可行性，揭示设计选择对质量的关键作用，为临床应用提供方法论基础。

Abstract: Existing LLM-based medical question-answering systems lack citation
generation and evaluation capabilities, raising concerns about their adoption
in practice. In this work, we introduce \name, the first end-to-end framework
that facilitates the design and evaluation of citation generation with LLMs for
medical tasks. Meanwhile, we introduce a novel multi-pass retrieval-citation
method that generates high-quality citations. Our evaluation highlights the
challenges and opportunities of citation generation for medical tasks, while
identifying important design choices that have a significant impact on the
final citation quality. Our proposed method achieves superior citation
precision and recall improvements compared to strong baseline methods, and we
show that evaluation results correlate well with annotation results from
professional experts.

</details>


### [21] [Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit](https://arxiv.org/abs/2506.06607)
*Charles Goddard,Fernando Fernandes Neto*

Main category: cs.CL

TL;DR: 本文提出一种无需训练的tokenizer移植技术，利用正交匹配追踪（OMP）算法通过稀疏线性组合重构未登录词嵌入，有效解决大语言模型跨分词器移植时的性能退化问题。


<details>
  <summary>Details</summary>
Motivation: 现有跨tokenizer移植方法在零样本场景下性能显著下降，尤其在数学推理等依赖数值分词的任务中。需开发无需梯度更新的方法以保持基础模型性能，并支持跨tokenizer知识迁移。

Method: 采用两阶段OMP算法：1）在供体嵌入空间用共享锚标记构建新词表示；2）将稀疏系数反向映射至基模型嵌入空间。通过字典共享实现跨tokenizer参数传递，避免模型微调。

Result: 在Llama→Mistral和Qwen→Llama跨tokenizer场景中，OMP零样本性能显著优于WECHSEL等基线方法（最高12B/1B参数规模），数学推理能力保持最佳，数值分词不匹配问题得到有效验证。

Conclusion: 该方法首次实现无需训练的tokenizer移植，支持知识蒸馏/模型融合等应用，并通过mergekit-tokensurgeon工具开源。揭示了数值分词方案对齐对保持数学能力的关键作用。

Abstract: We present a training-free method to transplant tokenizers in pretrained
large language models (LLMs) by reconstructing unseen token embeddings via
Orthogonal Matching Pursuit (OMP). Specifically, we approximate each
out-of-vocabulary token as a sparse linear combination of shared tokens, in two
phases: first, compute each new token's representation in the donor embedding
space with a small dictionary of shared anchor tokens, then transfer these same
sparse coefficients back into the base model's embedding space.
  On two challenging cross-tokenizer tasks--Llama$\to$Mistral NeMo (12B) and
Qwen$\to$Llama (1B)--we show that OMP achieves best zero-shot preservation of
the base model's performance across multiple benchmarks, while other zero-shot
approaches degrade significantly. Compared to baselines (zero-init, mean-init,
and existing approaches like WECHSEL, FOCUS, ZETT), OMP consistently achieves
the best overall performance, effectively bridging large tokenizer
discrepancies without gradient updates. Our analysis further identifies
mismatched numerical tokenization schemes as a critical challenge for
preserving mathematical reasoning capabilities. This technique enables direct
reuse of pretrained model weights with new tokenizers, facilitating
cross-tokenizer knowledge distillation, speculative decoding, ensembling,
merging, and domain-specific vocabulary adaptations. We integrate our method
into the open-source mergekit-tokensurgeon tool for post hoc vocabulary
realignment.

</details>


### [22] [Transferring Features Across Language Models With Model Stitching](https://arxiv.org/abs/2506.06609)
*Alan Chen,Jack Merullo,Alessandro Stolfo,Ellie Pavlick*

Main category: cs.CL

TL;DR: 该论文展示了如何通过仿射映射在语言模型的残差流之间有效传输特征，并利用此技术在不同大小的模型间传输稀疏自编码器（SAE）的权重，比较其表示空间。研究发现，小模型和大模型学习的表示空间高度相似，从而激励了在较小模型上训练SAE并传输到较大模型以节省计算资源。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索如何通过仿射映射在语言模型的残差流之间有效传输特征，以比较不同大小模型的表示空间，并提高稀疏自编码器（SAE）的训练效率。

Method: 论文采用仿射映射技术，将稀疏自编码器（SAE）的权重在不同大小的模型间进行传输，并比较其表示空间。此外，还研究了特征级别的可传输性，特别是语义和结构特征的传输差异。

Result: 研究发现，小模型和大模型学习的表示空间高度相似，通过小模型训练SAE并传输到大模型可以节省50%的计算资源。此外，传输的探测和导向向量能够有效恢复真实性能，且特定类别的功能特征在传输中保持其角色。

Conclusion: 论文展示了小模型和大模型在线性表示空间中的相似性和差异，并提出了一种提高稀疏自编码器（SAE）训练效率的方法。

Abstract: In this work, we demonstrate that affine mappings between residual streams of
language models is a cheap way to effectively transfer represented features
between models. We apply this technique to transfer the weights of Sparse
Autoencoders (SAEs) between models of different sizes to compare their
representations. We find that small and large models learn highly similar
representation spaces, which motivates training expensive components like SAEs
on a smaller model and transferring to a larger model at a FLOPs savings. For
example, using a small-to-large transferred SAE as initialization can lead to
50% cheaper training runs when training SAEs on larger models. Next, we show
that transferred probes and steering vectors can effectively recover ground
truth performance. Finally, we dive deeper into feature-level transferability,
finding that semantic and structural features transfer noticeably differently
while specific classes of functional features have their roles faithfully
mapped. Overall, our findings illustrate similarities and differences in the
linear representation spaces of small and large models and demonstrate a method
for improving the training efficiency of SAEs.

</details>


### [23] [Interpretable Depression Detection from Social Media Text Using LLM-Derived Embeddings](https://arxiv.org/abs/2506.06616)
*Samuel Kim,Oghenemaro Imieye,Yunting Yin*

Main category: cs.CL

TL;DR: 研究比较了大语言模型（LLM）与传统机器学习在社交媒体抑郁检测任务中的表现，发现零样本LLM在二元分类中表现良好，但细粒度分类效果较差；而基于LLM生成摘要嵌入的模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 通过准确且可解释的社交媒体抑郁语言检测，为心理健康早期干预提供支持，对临床实践和公共卫生具有重要意义。

Method: 对比零样本LLM与基于传统文本嵌入、LLM生成摘要嵌入的有监督分类器，在二元抑郁分类、抑郁严重程度分类及抑郁症/PTSD/焦虑鉴别诊断三类任务中的性能。

Result: 零样本LLM在二元分类中泛化能力强，但细粒度分类效果差；使用LLM生成摘要嵌入的分类器表现优于传统文本嵌入模型，部分任务达到更优性能。

Conclusion: LLM在心理健康预测中具有潜力，未来可通过优化零样本能力与上下文感知摘要技术进一步提升应用效果。

Abstract: Accurate and interpretable detection of depressive language in social media
is useful for early interventions of mental health conditions, and has
important implications for both clinical practice and broader public health
efforts. In this paper, we investigate the performance of large language models
(LLMs) and traditional machine learning classifiers across three classification
tasks involving social media data: binary depression classification, depression
severity classification, and differential diagnosis classification among
depression, PTSD, and anxiety. Our study compares zero-shot LLMs with
supervised classifiers trained on both conventional text embeddings and
LLM-generated summary embeddings. Our experiments reveal that while zero-shot
LLMs demonstrate strong generalization capabilities in binary classification,
they struggle with fine-grained ordinal classifications. In contrast,
classifiers trained on summary embeddings generated by LLMs demonstrate
competitive, and in some cases superior, performance on the classification
tasks, particularly when compared to models using traditional text embeddings.
Our findings demonstrate the strengths of LLMs in mental health prediction, and
suggest promising directions for better utilization of their zero-shot
capabilities and context-aware summarization techniques.

</details>


### [24] [BriefMe: A Legal NLP Benchmark for Assisting with Legal Briefs](https://arxiv.org/abs/2506.06619)
*Jesse Woo,Fateme Hashemi Chaleshtori,Ana Marasović,Kenneth Marino*

Main category: cs.CL

TL;DR: 本文介绍了BRIEFME数据集，旨在通过法律摘要、论点补全和案例检索任务评估语言模型在法律文书撰写中的能力。


<details>
  <summary>Details</summary>
Motivation: 法律文书的撰写和编辑是法律工作中未被充分探索的核心部分，需要深入理解法律并具备创新和说服力。

Method: 创建了BRIEFME数据集，包含三个任务：论点摘要、论点补全和案例检索，并分析了当前语言模型在这些任务中的表现。

Result: 当前的大型语言模型在摘要和指导性补全任务中表现良好，但在实际论点补全和相关法律案例检索任务中表现不佳。

Conclusion: 希望BRIEFME数据集能推动法律NLP的发展，帮助法律工作者更高效地完成工作。

Abstract: A core part of legal work that has been under-explored in Legal NLP is the
writing and editing of legal briefs. This requires not only a thorough
understanding of the law of a jurisdiction, from judgments to statutes, but
also the ability to make new arguments to try to expand the law in a new
direction and make novel and creative arguments that are persuasive to judges.
To capture and evaluate these legal skills in language models, we introduce
BRIEFME, a new dataset focused on legal briefs. It contains three tasks for
language models to assist legal professionals in writing briefs: argument
summarization, argument completion, and case retrieval. In this work, we
describe the creation of these tasks, analyze them, and show how current models
perform. We see that today's large language models (LLMs) are already quite
good at the summarization and guided completion tasks, even beating
human-generated headings. Yet, they perform poorly on other tasks in our
benchmark: realistic argument completion and retrieving relevant legal cases.
We hope this dataset encourages more development in Legal NLP in ways that will
specifically aid people in performing legal work.

</details>


### [25] [Psychological Counseling Cannot Be Achieved Overnight: Automated Psychological Counseling Through Multi-Session Conversations](https://arxiv.org/abs/2506.06626)
*Junzhe Wang,Bichen Wang,Xing Fu,Yixin Sun,Yanyan Zhao,Bing Qin*

Main category: cs.CL

TL;DR: 本文提出了一个多会话心理咨询对话数据集（MusPsy-Dataset），并开发了相应的模型（MusPsy-Model），以解决现有研究局限于单次会话的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在心理咨询中的应用主要集中于单次会话，无法反映现实中的多会话心理咨询过程。

Method: 利用公开的心理案例报告构建多会话心理咨询对话数据集，并开发了能够跟踪客户进展并调整咨询方向的模型。

Result: 实验表明，该模型在多会话场景下的表现优于基线模型。

Conclusion: 多会话心理咨询数据集和模型的引入，为更贴近实际的心理咨询研究提供了新的方向。

Abstract: In recent years, Large Language Models (LLMs) have made significant progress
in automated psychological counseling. However, current research focuses on
single-session counseling, which doesn't represent real-world scenarios. In
practice, psychological counseling is a process, not a one-time event,
requiring sustained, multi-session engagement to progressively address clients'
issues. To overcome this limitation, we introduce a dataset for Multi-Session
Psychological Counseling Conversation Dataset (MusPsy-Dataset). Our
MusPsy-Dataset is constructed using real client profiles from publicly
available psychological case reports. It captures the dynamic arc of
counseling, encompassing multiple progressive counseling conversations from the
same client across different sessions. Leveraging our dataset, we also
developed our MusPsy-Model, which aims to track client progress and adapt its
counseling direction over time. Experiments show that our model performs better
than baseline models across multiple sessions.

</details>


### [26] [SafeLawBench: Towards Safe Alignment of Large Language Models](https://arxiv.org/abs/2506.06636)
*Chuxue Cao,Han Zhu,Jiaming Ji,Qichao Sun,Zhenghao Zhu,Yinyu Wu,Juntao Dai,Yaodong Yang,Sirui Han,Yike Guo*

Main category: cs.CL

TL;DR: 本文提出了基于法律视角的大语言模型（LLMs）安全评估基准SafeLawBench，包含多选和开放问答任务，评估了20个LLMs的安全性和推理稳定性，发现多数模型表现不佳，呼吁社区重视LLMs安全性研究。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的广泛应用，其安全性问题引发关注。然而，由于现有安全评估标准的主观性，缺乏明确的评估框架。本文旨在填补这一空白，从法律视角提出系统化的安全评估基准。

Method: 本文提出了SafeLawBench基准，将安全风险分为三个等级，包含24,860个多选题和1,106个开放问答任务。评估了2个闭源和18个开源LLMs，采用零样本和少样本提示，并分析了模型的安全推理稳定性和拒绝行为。

Result: 评估结果显示，即使是领先的SOTA模型如Claude-3.5-Sonnet和GPT-4o，在SafeLawBench上的多选题任务中准确率未超过80.5%，20个LLMs的平均准确率为68.8%。多数投票机制能提升模型表现。

Conclusion: 本文强调了LLMs安全性评估的重要性，提出了基于法律视角的SafeLawBench基准，发现现有模型在安全性方面仍有不足，呼吁社区进一步研究LLMs的安全性问题。

Abstract: With the growing prevalence of large language models (LLMs), the safety of
LLMs has raised significant concerns. However, there is still a lack of
definitive standards for evaluating their safety due to the subjective nature
of current safety benchmarks. To address this gap, we conducted the first
exploration of LLMs' safety evaluation from a legal perspective by proposing
the SafeLawBench benchmark. SafeLawBench categorizes safety risks into three
levels based on legal standards, providing a systematic and comprehensive
framework for evaluation. It comprises 24,860 multi-choice questions and 1,106
open-domain question-answering (QA) tasks. Our evaluation included 2
closed-source LLMs and 18 open-source LLMs using zero-shot and few-shot
prompting, highlighting the safety features of each model. We also evaluated
the LLMs' safety-related reasoning stability and refusal behavior.
Additionally, we found that a majority voting mechanism can enhance model
performance. Notably, even leading SOTA models like Claude-3.5-Sonnet and
GPT-4o have not exceeded 80.5% accuracy in multi-choice tasks on SafeLawBench,
while the average accuracy of 20 LLMs remains at 68.8\%. We urge the community
to prioritize research on the safety of LLMs.

</details>


### [27] [Quantile Regression with Large Language Models for Price Prediction](https://arxiv.org/abs/2506.06657)
*Nikhita Vedula,Dushyanta Dhyani,Laleh Jalali,Boris Oreshkin,Mohsen Bayati,Shervin Malmasi*

Main category: cs.CL

TL;DR: 本文提出一种基于大语言模型（LLMs）的分位数回归方法，用于生成完整预测分布，在价格估计等任务中显著优于传统点估计方法，并通过系统实验验证了Mistral-7B模型在准确性和分布校准上的优势。


<details>
  <summary>Details</summary>
Motivation: 现有LLM结构化预测方法主要关注点估计，缺乏系统性方法对比，且难以处理需文本理解与不确定性量化的任务（如价格估计）。本文旨在填补这一空白。

Method: 提出分位数回归框架，通过微调Mistral-7B模型并添加分位数头，生成预测分布。结合LLM辅助标签校正技术，系统比较不同架构、训练方法和数据规模。

Result: Mistral-7B在三个价格预测数据集上全面超越传统方法（编码器架构/小样本学习），校准指标提升显著。LLM标签校正实现无偏人类级精度，相关数据集已开源。

Conclusion: 分位数回归有效扩展LLM的回归能力，Mistral-7B在复杂文本到分布预测任务中展现优越性，为不确定性量化任务提供新范式。

Abstract: Large Language Models (LLMs) have shown promise in structured prediction
tasks, including regression, but existing approaches primarily focus on point
estimates and lack systematic comparison across different methods. We
investigate probabilistic regression using LLMs for unstructured inputs,
addressing challenging text-to-distribution prediction tasks such as price
estimation where both nuanced text understanding and uncertainty quantification
are critical. We propose a novel quantile regression approach that enables LLMs
to produce full predictive distributions, improving upon traditional point
estimates. Through extensive experiments across three diverse price prediction
datasets, we demonstrate that a Mistral-7B model fine-tuned with quantile heads
significantly outperforms traditional approaches for both point and
distributional estimations, as measured by three established metrics each for
prediction accuracy and distributional calibration. Our systematic comparison
of LLM approaches, model architectures, training approaches, and data scaling
reveals that Mistral-7B consistently outperforms encoder architectures,
embedding-based methods, and few-shot learning methods. Our experiments also
reveal the effectiveness of LLM-assisted label correction in achieving
human-level accuracy without systematic bias. Our curated datasets are made
available at https://github.com/vnik18/llm-price-quantile-reg/ to support
future research.

</details>


### [28] [Learning Distribution-Wise Control in Representation Space for Language Models](https://arxiv.org/abs/2506.06686)
*Chunyuan Deng,Ruidi Chang,Hanjie Chen*

Main category: cs.CL

TL;DR: 本文提出了一种分布层面的干预方法（D-Intervention），通过扩展点级控制至概念子空间的分布区域，增强语言模型的控制性和鲁棒性，在常识与算术推理任务中表现优于传统点级干预。


<details>
  <summary>Details</summary>
Motivation: 现有可学习干预（表示微调）仅针对概念子空间的点级控制，无法覆盖周围分布区域。本文旨在通过分布层面的干预，提升模型行为控制的全面性和精细度。

Method: 将点级干预扩展为分布级干预，使模型学习概念子空间的分布特征而非单点变换，重点在早期网络层应用更大标准差的分布调整以优化控制效果。

Result: 在8个常识推理和7个算术推理基准测试中，分布级干预的控制精度与鲁棒性均显著优于点级方法，且早期层干预与较大标准差呈现强正相关性。

Conclusion: 分布级干预通过建模概念子空间的分布特性，为语言模型行为控制提供了更全面的解决方案，实现了细粒度的模型行为调控，代码已发布。

Abstract: Interventions in language models (LMs) are applied strategically to steer
model behavior during the forward pass. Learnable interventions, also known as
representation fine-tuning, aim to apply pointwise control within the concept
subspace and have proven effective in altering high-level behaviors. In this
work, we extend this approach to the distribution level, enabling the model to
learn not only pointwise transformations but also the surrounding regions of
the concept subspace. We demonstrate that these methods perform effectively in
early layers, with larger standard deviations correlating strongly with
improved performance. Across eight commonsense reasoning and seven arithmetic
reasoning benchmarks, our distribution-wise interventions consistently
outperform pointwise interventions in controllability and robustness. These
results illustrate that distribution-wise interventions provide a more
comprehensive method for steering model behavior and enabling finer-grained
control over language models. The code is at:
\href{https://github.com/chili-lab/D-Intervention}{https://github.com/chili-lab/D-Intervention}.

</details>


### [29] [Dynamic and Parametric Retrieval-Augmented Generation](https://arxiv.org/abs/2506.06704)
*Weihang Su,Qingyao Ai,Jingtao Zhan,Qian Dong,Yiqun Liu*

Main category: cs.CL

TL;DR: 本文探讨了动态RAG和参数化RAG两大新兴研究方向，以改进传统RAG系统在复杂任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统采用静态的检索-生成流程，依赖上下文知识注入，在处理需要多步推理和自适应信息访问的复杂任务时表现不佳。

Method: 动态RAG在生成过程中自适应地决定何时检索和检索什么，而参数化RAG则从输入级转向参数级知识注入，以提高效率和效果。

Result: 本文全面概述了动态RAG和参数化RAG的最新进展，并分享了理论基础和实践见解。

Conclusion: 动态RAG和参数化RAG为RAG系统的进一步发展提供了新的研究方向，有望在复杂任务中实现更好的性能。

Abstract: Retrieval-Augmented Generation (RAG) has become a foundational paradigm for
equipping large language models (LLMs) with external knowledge, playing a
critical role in information retrieval and knowledge-intensive applications.
However, conventional RAG systems typically adopt a static
retrieve-then-generate pipeline and rely on in-context knowledge injection,
which can be suboptimal for complex tasks that require multihop reasoning,
adaptive information access, and deeper integration of external knowledge.
Motivated by these limitations, the research community has moved beyond static
retrieval and in-context knowledge injection. Among the emerging directions,
this tutorial delves into two rapidly growing and complementary research areas
on RAG: Dynamic RAG and Parametric RAG. Dynamic RAG adaptively determines when
and what to retrieve during the LLM's generation process, enabling real-time
adaptation to the LLM's evolving information needs. Parametric RAG rethinks how
retrieved knowledge should be injected into LLMs, transitioning from
input-level to parameter-level knowledge injection for enhanced efficiency and
effectiveness. This tutorial offers a comprehensive overview of recent advances
in these emerging research areas. It also shares theoretical foundations and
practical insights to support and inspire further research in RAG.

</details>


### [30] [DivScore: Zero-Shot Detection of LLM-Generated Text in Specialized Domains](https://arxiv.org/abs/2506.06705)
*Zhihui Chen,Kai He,Yucheng Huang,Yunxiao Zhu,Mengling Feng*

Main category: cs.CL

TL;DR: 针对医学和法律等专业领域，本文提出了一种名为DivScore的零样本检测框架，通过归一化熵评分和领域知识蒸馏，有效识别LLM生成的文本，并在对抗性环境中表现出更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在医学和法律等高风险领域，准确检测LLM生成的文本对于打击错误信息和确保内容真实性至关重要。现有的零样本检测器在通用文本上表现良好，但在专业领域由于领域转移问题往往失效。

Method: 本文提出DivScore框架，结合归一化熵评分和领域知识蒸馏，以解决领域转移问题，并发布了一个针对医学和法律领域的LLM生成文本检测基准。

Result: 实验表明，DivScore在AUROC和召回率上分别比现有最先进的检测器高出14.4%和64.0%，在对抗性环境中，其AUROC和召回率分别平均高出22.8%和29.5%。

Conclusion: DivScore在专业领域中表现出色，尤其是在对抗性环境中，展示了其强大的鲁棒性和检测能力，代码和数据已公开。

Abstract: Detecting LLM-generated text in specialized and high-stakes domains like
medicine and law is crucial for combating misinformation and ensuring
authenticity. However, current zero-shot detectors, while effective on general
text, often fail when applied to specialized content due to domain shift. We
provide a theoretical analysis showing this failure is fundamentally linked to
the KL divergence between human, detector, and source text distributions. To
address this, we propose DivScore, a zero-shot detection framework using
normalized entropy-based scoring and domain knowledge distillation to robustly
identify LLM-generated text in specialized domains. We also release a
domain-specific benchmark for LLM-generated text detection in the medical and
legal domains. Experiments on our benchmark show that DivScore consistently
outperforms state-of-the-art detectors, with 14.4% higher AUROC and 64.0%
higher recall (0.1% false positive rate threshold). In adversarial settings,
DivScore demonstrates superior robustness than other baselines, achieving on
average 22.8% advantage in AUROC and 29.5% in recall. Code and data are
publicly available.

</details>


### [31] [A Survey of Retentive Network](https://arxiv.org/abs/2506.06708)
*Haiqi Yang,Zhiyuan Li,Yi Chang,Yuan Wu*

Main category: cs.CL

TL;DR: RetNet通过结合循环结构的归纳偏置与注意力机制的全局依赖建模，提出线性时间推理的保留机制，解决了Transformer长序列处理中的内存与扩展性问题。本文首次系统综述其架构创新、跨领域应用及挑战，为未来研究提供方向。


<details>
  <summary>Details</summary>
Motivation: Transformer因自注意力机制的二次复杂度导致长序列处理时内存成本高、扩展性差。现有文献缺乏对新兴RetNet架构的全面分析，需系统总结其技术突破与应用潜力。

Method: 提出保留机制(retention mechanism)，统一循环结构的序列建模能力与注意力的全局依赖捕获，支持完全并行训练与线性推理复杂度，兼容长上下文高效建模。

Result: RetNet在自然语言处理、语音识别、时间序列分析等领域展现跨域有效性，性能与Transformer相当但计算效率显著提升，证实其作为通用基础架构的潜力。

Conclusion: 本文填补RetNet综述空白，指出其部署优化、理论解释等挑战，建议未来研究关注硬件适配、多模态扩展等方向，推动高效序列建模框架的实用化进程。

Abstract: Retentive Network (RetNet) represents a significant advancement in neural
network architecture, offering an efficient alternative to the Transformer.
While Transformers rely on self-attention to model dependencies, they suffer
from high memory costs and limited scalability when handling long sequences due
to their quadratic complexity. To mitigate these limitations, RetNet introduces
a retention mechanism that unifies the inductive bias of recurrence with the
global dependency modeling of attention. This mechanism enables linear-time
inference, facilitates efficient modeling of extended contexts, and remains
compatible with fully parallelizable training pipelines. RetNet has garnered
significant research interest due to its consistently demonstrated cross-domain
effectiveness, achieving robust performance across machine learning paradigms
including natural language processing, speech recognition, and time-series
analysis. However, a comprehensive review of RetNet is still missing from the
current literature. This paper aims to fill that gap by offering the first
detailed survey of the RetNet architecture, its key innovations, and its
diverse applications. We also explore the main challenges associated with
RetNet and propose future research directions to support its continued
advancement in both academic research and practical deployment.

</details>


### [32] [C-PATH: Conversational Patient Assistance and Triage in Healthcare System](https://arxiv.org/abs/2506.06737)
*Qi Shi,Qiwei Han,Cláudia Soares*

Main category: cs.CL

TL;DR: 本文介绍了C-PATH，一个基于大语言模型的对话式AI系统，旨在通过自然对话帮助患者识别症状并推荐合适的医疗科室。


<details>
  <summary>Details</summary>
Motivation: 医疗系统复杂且令人困惑，患者难以获得及时和适当的医疗服务。

Method: C-PATH基于LLaMA3架构，通过多阶段管道对医学知识、对话数据和临床总结进行微调，并采用GPT数据增强框架将结构化临床知识转化为易于理解的对话。

Result: C-PATH在清晰度、信息量和推荐准确性方面表现优异，显著优于特定领域的基线模型。

Conclusion: C-PATH在开发以用户为中心、易用且准确的数字健康辅助和分诊工具方面迈出了重要一步。

Abstract: Navigating healthcare systems can be complex and overwhelming, creating
barriers for patients seeking timely and appropriate medical attention. In this
paper, we introduce C-PATH (Conversational Patient Assistance and Triage in
Healthcare), a novel conversational AI system powered by large language models
(LLMs) designed to assist patients in recognizing symptoms and recommending
appropriate medical departments through natural, multi-turn dialogues. C-PATH
is fine-tuned on medical knowledge, dialogue data, and clinical summaries using
a multi-stage pipeline built on the LLaMA3 architecture. A core contribution of
this work is a GPT-based data augmentation framework that transforms structured
clinical knowledge from DDXPlus into lay-person-friendly conversations,
allowing alignment with patient communication norms. We also implement a
scalable conversation history management strategy to ensure long-range
coherence. Evaluation with GPTScore demonstrates strong performance across
dimensions such as clarity, informativeness, and recommendation accuracy.
Quantitative benchmarks show that C-PATH achieves superior performance in
GPT-rewritten conversational datasets, significantly outperforming
domain-specific baselines. C-PATH represents a step forward in the development
of user-centric, accessible, and accurate AI tools for digital health
assistance and triage.

</details>


### [33] [Geopolitical biases in LLMs: what are the "good" and the "bad" countries according to contemporary language models](https://arxiv.org/abs/2506.06751)
*Mikhail Salnikov,Dmitrii Korzh,Ivan Lazichny,Elvir Karimov,Artyom Iudin,Ivan Oseledets,Oleg Y. Rogov,Alexander Panchenko,Natalia Loukachevitch,Elena Tutubalina*

Main category: cs.CL

TL;DR: 该论文通过分析不同国家（美、英、苏、中）对历史事件的对立观点，评估了LLMs中的地缘政治偏见。研究发现模型倾向于特定国家叙事，简单去偏见提示效果有限，并提出新数据集供未来研究。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型可能隐含地缘政治偏见，但缺乏系统性评估方法。研究旨在揭示模型对国家叙事偏见的敏感性及其去偏难度。

Method: 构建包含中立事件描述与对立国家观点的新数据集，通过操纵参与者标签进行实验，并测试简单去偏见提示的有效性。

Result: 模型表现出显著地缘政治偏好，去偏见提示效果有限。标签置换实验显示模型对归属敏感，可能放大偏见或识别逻辑矛盾。

Conclusion: LLMs存在根深蒂固的国家叙事偏见，简单干预难以消除。研究为地缘政治偏见检测提供了方法论框架和基准数据集。

Abstract: This paper evaluates geopolitical biases in LLMs with respect to various
countries though an analysis of their interpretation of historical events with
conflicting national perspectives (USA, UK, USSR, and China). We introduce a
novel dataset with neutral event descriptions and contrasting viewpoints from
different countries. Our findings show significant geopolitical biases, with
models favoring specific national narratives. Additionally, simple debiasing
prompts had a limited effect in reducing these biases. Experiments with
manipulated participant labels reveal models' sensitivity to attribution,
sometimes amplifying biases or recognizing inconsistencies, especially with
swapped labels. This work highlights national narrative biases in LLMs,
challenges the effectiveness of simple debiasing methods, and offers a
framework and dataset for future geopolitical bias research.

</details>


### [34] [They want to pretend not to understand: The Limits of Current LLMs in Interpreting Implicit Content of Political Discourse](https://arxiv.org/abs/2506.06775)
*Walter Paci,Alessandro Panunzi,Sandro Pezzelle*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型在政治话语中检测和解释隐含内容的能力，发现当前模型在处理预设和隐含意义方面存在困难，但提出了未来改进的方向。


<details>
  <summary>Details</summary>
Motivation: 隐含内容在政治话语中至关重要，大型语言模型在复杂语义和语用理解任务中表现优异，但其在政治话语中的应用尚未充分探索。

Method: 利用IMPAQTS语料库，通过多项选择题和开放式生成任务测试大型语言模型在解释隐含内容方面的有效性。

Result: 所有测试模型在解释预设和隐含意义方面均表现不佳，表明当前模型缺乏准确解释高度隐含语言的关键语用能力。

Conclusion: 当前大型语言模型在政治话语中解释隐含内容的能力有限，但研究指出了未来提升模型性能的潜在方向。

Abstract: Implicit content plays a crucial role in political discourse, where speakers
systematically employ pragmatic strategies such as implicatures and
presuppositions to influence their audiences. Large Language Models (LLMs) have
demonstrated strong performance in tasks requiring complex semantic and
pragmatic understanding, highlighting their potential for detecting and
explaining the meaning of implicit content. However, their ability to do this
within political discourse remains largely underexplored. Leveraging, for the
first time, the large IMPAQTS corpus, which comprises Italian political
speeches with the annotation of manipulative implicit content, we propose
methods to test the effectiveness of LLMs in this challenging problem. Through
a multiple-choice task and an open-ended generation task, we demonstrate that
all tested models struggle to interpret presuppositions and implicatures. We
conclude that current LLMs lack the key pragmatic capabilities necessary for
accurately interpreting highly implicit language, such as that found in
political discourse. At the same time, we highlight promising trends and future
directions for enhancing model performance. We release our data and code at
https://github.com/WalterPaci/IMPAQTS-PID

</details>


### [35] [Extending dependencies to the taggedPBC: Word order in transitive clauses](https://arxiv.org/abs/2506.06785)
*Hiram Ring*

Main category: cs.CL

TL;DR: 本文介绍了taggedPBC数据集的依存关系标注扩展版本，通过将依存信息与POS标签结合，验证了基于语料库的类型学方法在跨语言研究中的有效性，并证明即使数据存在，充分标注仍能提供重要洞见。


<details>
  <summary>Details</summary>
Motivation: 原始taggedPBC数据集虽包含大规模多语言POS标注数据，但缺乏依存关系标注，限制了其在句法类型学研究中的应用。

Method: 通过将依存关系信息与POS标签一同迁移至taggedPBC所有语言数据中，构建CoNLLU格式的标注语料库。

Result: 数据集中提取的及物从句词序信息（论元与谓词位置）与WALS等三大类型学数据库的专家判定结果显著相关。

Conclusion: 基于语料库的类型学方法可有效扩展离散语言范畴比较，且噪声数据在充分标注下仍具研究价值；依存标注语料库已开源供协作研究。

Abstract: The taggedPBC (Ring 2025a) contains more than 1,800 sentences of pos-tagged
parallel text data from over 1,500 languages, representing 133 language
families and 111 isolates. While this dwarfs previously available resources,
and the POS tags achieve decent accuracy, allowing for predictive
crosslinguistic insights (Ring 2025b), the dataset was not initially annotated
for dependencies. This paper reports on a CoNLLU-formatted version of the
dataset which transfers dependency information along with POS tags to all
languages in the taggedPBC. Although there are various concerns regarding the
quality of the tags and the dependencies, word order information derived from
this dataset regarding the position of arguments and predicates in transitive
clauses correlates with expert determinations of word order in three
typological databases (WALS, Grambank, Autotyp). This highlights the usefulness
of corpus-based typological approaches (as per Baylor et al. 2023; Bjerva 2024)
for extending comparisons of discrete linguistic categories, and suggests that
important insights can be gained even from noisy data, given sufficient
annotation. The dependency-annotated corpora are also made available for
research and collaboration via GitHub.

</details>


### [36] [On the Adaptive Psychological Persuasion of Large Language Models](https://arxiv.org/abs/2506.06800)
*Tianjie Ju,Yujia Chen,Hao Fei,Mong-Li Lee,Wynne Hsu,Pengzhou Cheng,Zongru Wu,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLMs）在自主说服和抵抗说服方面的双重能力，提出了基于心理策略的自适应框架，显著提高了LLMs的说服成功率。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在指令跟随和修辞流畅性方面表现出色，但其在心理修辞背景下的自主说服和抵抗说服能力尚未得到系统探索。

Method: 首先评估了四种常用LLMs在对抗性对话中的表现，然后引入了十一种心理说服策略，并提出了基于直接偏好优化的自适应框架，训练LLMs自主选择最优策略。

Result: 实验表明，明确指示LLMs采用特定策略（如流畅效应和重复效应）显著提高了说服成功率，且自适应心理说服方法有效提升了LLMs的策略选择能力。

Conclusion: 提出的自适应心理说服方法显著增强了LLMs的说服成功率，同时保持了其通用能力。

Abstract: Previous work has showcased the intriguing capabilities of Large Language
Models (LLMs) in instruction-following and rhetorical fluency. However,
systematic exploration of their dual capabilities to autonomously persuade and
resist persuasion, particularly in contexts involving psychological rhetoric,
remains unexplored. In this paper, we first evaluate four commonly adopted LLMs
by tasking them to alternately act as persuaders and listeners in adversarial
dialogues. Empirical results show that persuader LLMs predominantly employ
repetitive strategies, leading to low success rates. Then we introduce eleven
comprehensive psychological persuasion strategies, finding that explicitly
instructing LLMs to adopt specific strategies such as Fluency Effect and
Repetition Effect significantly improves persuasion success rates. However, no
``one-size-fits-all'' strategy proves universally effective, with performance
heavily dependent on contextual counterfactuals. Motivated by these
observations, we propose an adaptive framework based on direct preference
optimization that trains LLMs to autonomously select optimal strategies by
leveraging persuasion results from strategy-specific responses as preference
pairs. Experiments on three open-source LLMs confirm that the proposed adaptive
psychological persuasion method effectively enables persuader LLMs to select
optimal strategies, significantly enhancing their success rates while
maintaining general capabilities. Our code is available at
https://github.com/KalinaEine/PsychologicalPersuasion.

</details>


### [37] [Label-semantics Aware Generative Approach for Domain-Agnostic Multilabel Classification](https://arxiv.org/abs/2506.06806)
*Subhendu Khatuya,Shashwat Naidu,Saptarshi Ghosh,Pawan Goyal,Niloy Ganguly*

Main category: cs.CL

TL;DR: 提出LAGAMC模型，通过生成标签描述和双目标损失函数，实现高效多标签文本分类，在多个数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 文本数据激增导致人工文档分类困难，需开发领域无关的自动化多标签分类框架。

Method: 使用预定义标签描述生成机制+微调句子转换器匹配标签，结合交叉熵损失与余弦相似度的双目标损失函数，确保语义对齐与分类准确性。

Result: 在Micro-F1和Macro-F1指标上分别超越基线13.94%和24.85%，所有测试数据集均达新SOTA。

Conclusion: LAGAMC兼具参数高效性与跨领域适应性，其语义驱动的生成式方法为实际应用提供了有效解决方案。

Abstract: The explosion of textual data has made manual document classification
increasingly challenging. To address this, we introduce a robust, efficient
domain-agnostic generative model framework for multi-label text classification.
Instead of treating labels as mere atomic symbols, our approach utilizes
predefined label descriptions and is trained to generate these descriptions
based on the input text. During inference, the generated descriptions are
matched to the pre-defined labels using a finetuned sentence transformer. We
integrate this with a dual-objective loss function, combining cross-entropy
loss and cosine similarity of the generated sentences with the predefined
target descriptions, ensuring both semantic alignment and accuracy. Our
proposed model LAGAMC stands out for its parameter efficiency and versatility
across diverse datasets, making it well-suited for practical applications. We
demonstrate the effectiveness of our proposed model by achieving new
state-of-the-art performances across all evaluated datasets, surpassing several
strong baselines. We achieve improvements of 13.94% in Micro-F1 and 24.85% in
Macro-F1 compared to the closest baseline across all datasets.

</details>


### [38] [Not quite Sherlock Holmes: Language model predictions do not reliably differentiate impossible from improbable events](https://arxiv.org/abs/2506.06808)
*James A. Michaelov,Reeka Estacio,Zhien Zhang,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: 语言模型在区分可能事件与不可能事件时表现不佳，甚至在某些情况下表现低于随机水平。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型是否能够可靠地预测可能事件比不可能事件更有可能发生。

Method: 通过分离可能性、典型性和上下文相关性，测试多种语言模型的性能。

Result: 所有测试模型，包括Llama 3、Gemma 2和Mistral NeMo，在某些条件下表现低于随机水平，将更高概率分配给不可能句子。

Conclusion: 语言模型在区分可能事件与不可能事件方面的能力并不稳健，需要进一步改进。

Abstract: Can language models reliably predict that possible events are more likely
than merely improbable ones? By teasing apart possibility, typicality, and
contextual relatedness, we show that despite the results of previous work,
language models' ability to do this is far from robust. In fact, under certain
conditions, all models tested - including Llama 3, Gemma 2, and Mistral NeMo -
perform at worse-than-chance level, assigning higher probabilities to
impossible sentences such as 'the car was given a parking ticket by the brake'
than to merely unlikely sentences such as 'the car was given a parking ticket
by the explorer'.

</details>


### [39] [Advancing Question Generation with Joint Narrative and Difficulty Control](https://arxiv.org/abs/2506.06812)
*Bernardo Leite,Henrique Lopes Cardoso*

Main category: cs.CL

TL;DR: 本文提出了一种联合叙事和难度控制的策略，用于生成阅读理解问题，初步证明其可行性，但并非在所有情况下都有效。


<details>
  <summary>Details</summary>
Motivation: 现有问题生成研究缺乏对叙事和难度控制的结合，而这对教育目的的定制问题生成至关重要。

Method: 提出了一种联合叙事和难度控制的策略，以同时控制生成问题的这两个属性。

Result: 初步评估表明该策略在某些条件下表现良好，但并非在所有情况下都有效。

Conclusion: 研究结果强调了该策略的适用条件，并讨论了其应用中的权衡。

Abstract: Question Generation (QG), the task of automatically generating questions from
a source input, has seen significant progress in recent years.
Difficulty-controllable QG (DCQG) enables control over the difficulty level of
generated questions while considering the learner's ability. Additionally,
narrative-controllable QG (NCQG) allows control over the narrative aspects
embedded in the questions. However, research in QG lacks a focus on combining
these two types of control, which is important for generating questions
tailored to educational purposes. To address this gap, we propose a strategy
for Joint Narrative and Difficulty Control, enabling simultaneous control over
these two attributes in the generation of reading comprehension questions. Our
evaluation provides preliminary evidence that this approach is feasible, though
it is not effective across all instances. Our findings highlight the conditions
under which the strategy performs well and discuss the trade-offs associated
with its application.

</details>


### [40] [BTPD: A Multilingual Hand-curated Dataset of Bengali Transnational Political Discourse Across Online Communities](https://arxiv.org/abs/2506.06813)
*Dipto Das,Syed Ishtiaque Ahmed,Shion Guha*

Main category: cs.CL

TL;DR: 本文介绍了一个多语言的孟加拉跨国政治话语数据集，填补了该领域在资源匮乏语言中的研究空白。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏数据集，针对孟加拉语等资源匮乏语言的政治话语研究受限，本文旨在填补这一空白。

Method: 通过社区知情的关键词检索方法，从三个在线平台手工收集并整理数据集。

Result: 提供了一个包含多语言内容的孟加拉跨国政治话语数据集，并概述了其主题和内容。

Conclusion: 该数据集为研究孟加拉语政治话语和意识形态极化提供了重要资源。

Abstract: Understanding political discourse in online spaces is crucial for analyzing
public opinion and ideological polarization. While social computing and
computational linguistics have explored such discussions in English, such
research efforts are significantly limited in major yet under-resourced
languages like Bengali due to the unavailability of datasets. In this paper, we
present a multilingual dataset of Bengali transnational political discourse
(BTPD) collected from three online platforms, each representing distinct
community structures and interaction dynamics. Besides describing how we
hand-curated the dataset through community-informed keyword-based retrieval,
this paper also provides a general overview of its topics and multilingual
content.

</details>


### [41] [How do datasets, developers, and models affect biases in a low-resourced language?](https://arxiv.org/abs/2506.06816)
*Dipto Das,Shion Guha,Bryan Semaan*

Main category: cs.CL

TL;DR: 该论文通过算法审计发现，针对孟加拉语的情感分析模型在性别、宗教和国籍等身份类别上存在偏见，尽管使用了多语言或特定语言模型，仍无法有效解决低资源语境下的社会技术系统偏见问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于身份偏见在低资源语言（如孟加拉语）中的影响尚未充分探索，现有解决方案（如多语言模型）的有效性缺乏实证支持，且此类偏见加剧了边缘化群体的困境。

Method: 方法包括对基于mBERT和BanglaBERT的模型进行算法审计，使用Google Dataset Search中所有孟加拉语情感分析数据集进行微调，分析模型在相同语义内容下的偏见表现，并考察预训练模型与多样化数据集结合时的矛盾。

Result: 结果显示，所有模型均在不同身份类别中表现出系统性偏见，且预训练模型与不同背景创建的数据集结合时产生不一致性和不确定性。

Conclusion: 结论指出这些发现与认知不公、AI对齐问题相关，并强调算法审计方法需考虑社会技术系统复杂性及数据来源多样性对偏见评估的影响。

Abstract: Sociotechnical systems, such as language technologies, frequently exhibit
identity-based biases. These biases exacerbate the experiences of historically
marginalized communities and remain understudied in low-resource contexts.
While models and datasets specific to a language or with multilingual support
are commonly recommended to address these biases, this paper empirically tests
the effectiveness of such approaches in the context of gender, religion, and
nationality-based identities in Bengali, a widely spoken but low-resourced
language. We conducted an algorithmic audit of sentiment analysis models built
on mBERT and BanglaBERT, which were fine-tuned using all Bengali sentiment
analysis (BSA) datasets from Google Dataset Search. Our analyses showed that
BSA models exhibit biases across different identity categories despite having
similar semantic content and structure. We also examined the inconsistencies
and uncertainties arising from combining pre-trained models and datasets
created by individuals from diverse demographic backgrounds. We connected these
findings to the broader discussions on epistemic injustice, AI alignment, and
methodological decisions in algorithmic audits.

</details>


### [42] [Beyond Classification: Towards Speech Emotion Reasoning with Multitask AudioLLMs](https://arxiv.org/abs/2506.06820)
*Wenyu Zhang,Yingxu He,Geyu Lin,Zhuohan Liu,Shuo Sun,Bin Wang,Xunlong Zou,Jeremy H. M. Wong,Qiongqiong Wang,Hardik B. Sailor,Nancy F. Chen,Ai Ti Aw*

Main category: cs.CL

TL;DR: 本文提出了一种利用AudioLLMs生成能力来增强情感识别的方法，通过引入情感推理策略，结合推理增强数据监督、双编码器架构和任务交替训练，提升了情感预测的准确性和生成响应的连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有的AudioLLMs在语义任务上表现优异，但在情感等副语言线索建模方面仍有限制。传统方法将情感理解视为分类问题，缺乏对预测背后逻辑的解释。本文旨在通过情感推理策略，提升情感识别的解释性和准确性。

Method: 本文提出了一种统一框架，结合推理增强数据监督、双编码器架构和任务交替训练，使AudioLLMs能够在多任务学习中有效融入情感推理。

Result: 在IEMOCAP和MELD数据集上的实验表明，该方法不仅提高了情感预测的准确性，还增强了生成响应的连贯性和证据基础。

Conclusion: 通过引入情感推理策略，本文的方法显著提升了AudioLLMs在情感识别任务中的表现，同时增强了生成响应的解释性和连贯性。

Abstract: Audio Large Language Models (AudioLLMs) have achieved strong results in
semantic tasks like speech recognition and translation, but remain limited in
modeling paralinguistic cues such as emotion. Existing approaches often treat
emotion understanding as a classification problem, offering little insight into
the underlying rationale behind predictions. In this work, we explore emotion
reasoning, a strategy that leverages the generative capabilities of AudioLLMs
to enhance emotion recognition by producing semantically aligned,
evidence-grounded explanations. To support this in multitask AudioLLMs, we
introduce a unified framework combining reasoning-augmented data supervision,
dual-encoder architecture, and task-alternating training. This approach enables
AudioLLMs to effectively learn different tasks while incorporating emotional
reasoning. Experiments on IEMOCAP and MELD show that our approach not only
improves emotion prediction accuracy but also enhances the coherence and
evidential grounding of the generated responses.

</details>


### [43] [Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems](https://arxiv.org/abs/2506.06821)
*Yuhan Cao,Zian Chen,Kun Quan,Ziliang Zhang,Yu Wang,Xiaoning Dong,Yeqi Feng,Guanzhong He,Jingcheng Huang,Jianhao Li,Yixuan Tan,Jiafu Tang,Yilin Tang,Junlei Wu,Qianyu Xiao,Can Zheng,Shouchen Zhou,Yuxiang Zhu,Yiming Huang,Tian Xie,Tianxing He*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）在代码检查与调试中的潜力，提出了TCGBench基准，评估LLMs生成测试用例的能力，发现LLMs在生成针对性测试用例方面表现不佳，但通过高质量数据集可以提升其性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在代码生成方面表现出色，但其在代码检查与调试中的应用，尤其是通过生成测试用例来暴露代码缺陷的能力，尚未得到充分研究。本文旨在填补这一空白。

Method: 本文提出了TCGBench基准，包含两个任务：一是生成有效的测试用例生成器，二是生成针对性的测试用例生成器以暴露人类代码中的缺陷。实验使用了先进的LLMs，并构建了一个高质量的手工标注数据集。

Result: 实验结果表明，尽管LLMs在大多数情况下能生成有效的测试用例生成器，但在生成针对性测试用例以暴露代码缺陷方面表现不佳，尤其是高级推理模型（如o3-mini）远未达到人类水平。然而，通过使用高质量数据集，LLMs的性能得到了提升。

Conclusion: 本文揭示了LLMs在生成针对性测试用例方面的局限性，但通过高质量数据集的辅助，可以显著提升其性能，为未来研究提供了方向。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
code generation, capable of tackling complex tasks during inference. However,
the extent to which LLMs can be utilized for code checking or debugging through
test case generation remains largely unexplored. We investigate this problem
from the perspective of competition-level programming (CP) programs and propose
TCGBench, a Benchmark for (LLM generation of) Test Case Generators. This
benchmark comprises two tasks, aimed at studying the capabilities of LLMs in
(1) generating valid test case generators for a given CP problem, and further
(2) generating targeted test case generators that expose bugs in human-written
code. Experimental results indicate that while state-of-the-art LLMs can
generate valid test case generators in most cases, most LLMs struggle to
generate targeted test cases that reveal flaws in human code effectively.
Especially, even advanced reasoning models (e.g., o3-mini) fall significantly
short of human performance in the task of generating targeted generators.
Furthermore, we construct a high-quality, manually curated dataset of
instructions for generating targeted generators. Analysis demonstrates that the
performance of LLMs can be enhanced with the aid of this dataset, by both
prompting and fine-tuning.

</details>


### [44] [PCoT: Persuasion-Augmented Chain of Thought for Detecting Fake News and Social Media Disinformation](https://arxiv.org/abs/2506.06842)
*Arkadiusz Modzelewski,Witold Sosnowski,Tiziano Labruna,Adam Wierzbicki,Giovanni Da San Martino*

Main category: cs.CL

TL;DR: 研究者提出了一种结合说服知识的思维链方法（PCoT），以增强大语言模型在零样本分类中的虚假信息检测能力，并发布了两个新数据集。实验表明PCoT在多个模型和数据集上平均优于现有方法15%。


<details>
  <summary>Details</summary>
Motivation: 基于心理学研究，发现说服谬误知识有助于人类识别虚假信息，因此探索是否可通过注入此类知识提升大语言模型的检测能力。

Method: 提出Persuasion-Augmented Chain of Thought（PCoT），利用说服知识改进零样本虚假信息检测，并通过新数据集EUDisinfo和MultiDis进行验证。

Result: PCoT在5个大语言模型和5个数据集上平均表现优于基线方法15%，新数据集确保模型评估内容完全未见于训练数据。

Conclusion: 说服知识能有效增强零样本虚假信息检测，PCoT的提出及新数据集的发布为后续研究提供了重要工具与基准。

Abstract: Disinformation detection is a key aspect of media literacy. Psychological
studies have shown that knowledge of persuasive fallacies helps individuals
detect disinformation. Inspired by these findings, we experimented with large
language models (LLMs) to test whether infusing persuasion knowledge enhances
disinformation detection. As a result, we introduce the Persuasion-Augmented
Chain of Thought (PCoT), a novel approach that leverages persuasion to improve
disinformation detection in zero-shot classification. We extensively evaluate
PCoT on online news and social media posts. Moreover, we publish two novel,
up-to-date disinformation datasets: EUDisinfo and MultiDis. These datasets
enable the evaluation of PCoT on content entirely unseen by the LLMs used in
our experiments, as the content was published after the models' knowledge
cutoffs. We show that, on average, PCoT outperforms competitive methods by 15%
across five LLMs and five datasets. These findings highlight the value of
persuasion in strengthening zero-shot disinformation detection.

</details>


### [45] [Adapt Once, Thrive with Updates: Transferable Parameter-Efficient Fine-Tuning on Evolving Base Models](https://arxiv.org/abs/2506.06844)
*Naibin Gu,Peng Fu,Xiyu Liu,Ke Ma,Zheng Lin,Weiping Wang*

Main category: cs.CL

TL;DR: 提出Trans-PEFT方法，通过减少对基础模型知识的依赖，使PEFT模块在基础模型更新后无需重新调优即可保持性能，显著降低维护成本。


<details>
  <summary>Details</summary>
Motivation: 基础模型更新后，原有PEFT模块性能显著下降，而重新调优所有模块的计算成本过高，需探索无需重复调优的解决方案。

Method: 分析基础模型更新对FFN任务知识与注意力机制任务模式的影响，设计Trans-PEFT以增强任务模式关注度并降低对基础模型知识的依赖。

Result: 在7个基础模型和12个数据集上的实验表明，Trans-PEFT模块在更新后的模型上无需调优即可保持性能，理论分析支持其有效性。

Conclusion: Trans-PEFT通过解耦任务模式与基础模型知识，显著减少实际应用中模型更新时的维护开销，具有广泛适用性。

Abstract: Parameter-efficient fine-tuning (PEFT) has become a common method for
fine-tuning large language models, where a base model can serve multiple users
through PEFT module switching. To enhance user experience, base models require
periodic updates. However, once updated, PEFT modules fine-tuned on previous
versions often suffer substantial performance degradation on newer versions.
Re-tuning these numerous modules to restore performance would incur significant
computational costs. Through a comprehensive analysis of the changes that occur
during base model updates, we uncover an interesting phenomenon: continual
training primarily affects task-specific knowledge stored in Feed-Forward
Networks (FFN), while having less impact on the task-specific pattern in the
Attention mechanism. Based on these findings, we introduce Trans-PEFT, a novel
approach that enhances the PEFT module by focusing on the task-specific pattern
while reducing its dependence on certain knowledge in the base model. Further
theoretical analysis supports our approach. Extensive experiments across 7 base
models and 12 datasets demonstrate that Trans-PEFT trained modules can maintain
performance on updated base models without re-tuning, significantly reducing
maintenance overhead in real-world applications.

</details>


### [46] [Right Is Not Enough: The Pitfalls of Outcome Supervision in Training LLMs for Math Reasoning](https://arxiv.org/abs/2506.06877)
*Jiaxing Guo,Wenjie Yang,Shengzhong Zhang,Tongshan Xu,Lun Du,Da Zheng,Zengfeng Huang*

Main category: cs.CL

TL;DR: 现有结果导向的大语言模型（LLMs）在数学解题中存在答案正确但推理过程错误的问题。研究提出新数据集MathOlympiadEval和验证方法ParaStepVerifier，通过细粒度步骤检测提升错误推理识别能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在数学解题中表现优异，但其答案正确性常掩盖了推理过程的根本性错误（奖励黑客现象）。现有自动评估方法（如LLM-as-a-judge）难以可靠检测此类缺陷，需开发更精细的验证方法。

Method: 提出ParaStepVerifier方法，通过分步并行验证对数学解题过程进行细致检查，识别错误推理步骤。同时构建包含细粒度标注的MathOlympiadEval数据集以评估模型表现。

Result: 实验表明ParaStepVerifier在检测复杂多步骤问题的错误解决方案时显著优于基线方法，尤其在过程正确性评估方面提升了识别准确性。

Conclusion: ParaStepVerifier为评估和训练具有真实数学推理能力的LLMs提供了更可靠的路径，揭示了当前模型答案正确性与过程严谨性之间的关键差距。

Abstract: Outcome-rewarded Large Language Models (LLMs) have demonstrated remarkable
success in mathematical problem-solving. However, this success often masks a
critical issue: models frequently achieve correct answers through fundamentally
unsound reasoning processes, a phenomenon indicative of reward hacking. We
introduce MathOlympiadEval, a new dataset with fine-grained annotations, which
reveals a significant gap between LLMs' answer correctness and their low
process correctness. Existing automated methods like LLM-as-a-judge struggle to
reliably detect these reasoning flaws. To address this, we propose
ParaStepVerifier, a novel methodology for meticulous, step-by-step verification
of mathematical solutions. ParaStepVerifier identifies incorrect reasoning
steps. Empirical results demonstrate that ParaStepVerifier substantially
improves the accuracy of identifying flawed solutions compared to baselines,
especially for complex, multi-step problems. This offers a more robust path
towards evaluating and training LLMs with genuine mathematical reasoning.

</details>


### [47] [Mixture of Small and Large Models for Chinese Spelling Check](https://arxiv.org/abs/2506.06887)
*Ziheng Qiao,Houquan Zhou,Zhenghua Li*

Main category: cs.CL

TL;DR: 本文提出一种动态混合方法，在解码阶段结合小模型与大型语言模型的概率分布，无需微调LLM即可提升中文拼写检查效果，在多个数据集达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在中文拼写检查任务中表现不佳，而微调模型存在编辑模式过拟合问题，且微调LLM需要大量资源。需平衡小模型的精确性与LLM的流畅性。

Method: 通过beam search解码阶段动态混合小模型与LLM的概率分布，结合两者的优势，避免LLM微调并提升领域适应性。

Result: 实验表明该方法显著提升纠错能力，在多个数据集上达到最先进水平，且节省计算资源。

Conclusion: 动态混合方法有效平衡了模型性能与资源消耗，为中文拼写检查任务提供了高效解决方案，并促进跨领域应用。

Abstract: In the era of large language models (LLMs), the Chinese Spelling Check (CSC)
task has seen various LLM methods developed, yet their performance remains
unsatisfactory. In contrast, fine-tuned BERT-based models, relying on
high-quality in-domain data, show excellent performance but suffer from edit
pattern overfitting. This paper proposes a novel dynamic mixture approach that
effectively combines the probability distributions of small models and LLMs
during the beam search decoding phase, achieving a balanced enhancement of
precise corrections from small models and the fluency of LLMs. This approach
also eliminates the need for fine-tuning LLMs, saving significant time and
resources, and facilitating domain adaptation. Comprehensive experiments
demonstrate that our mixture approach significantly boosts error correction
capabilities, achieving state-of-the-art results across multiple datasets. Our
code is available at https://github.com/zhqiao-nlp/MSLLM.

</details>


### [48] [Automatic Speech Recognition of African American English: Lexical and Contextual Effects](https://arxiv.org/abs/2506.06888)
*Hamid Mojarad,Kevin Tang*

Main category: cs.CL

TL;DR: 该研究探讨了非洲裔美国英语（AAE）中的辅音簇缩减（CCR）和ING缩减对自动语音识别（ASR）错误率的影响，并比较了有无语言模型（LM）的ASR系统的表现。


<details>
  <summary>Details</summary>
Motivation: 自动语音识别模型在处理非洲裔美国英语（AAE）的语音、音系和形态句法特征时表现不佳，尤其是辅音簇缩减（CCR）和ING缩减。本研究旨在探讨这些特征是否增加了ASR的误识别，并比较有无语言模型的ASR系统在词汇邻域效应和上下文预测性上的差异。

Method: 使用wav2vec 2.0对CORAAL语料库进行转录，分别在有和没有语言模型的情况下进行测试。使用Montreal Forced Aligner（MFA）检测CCR和ING缩减，并通过发音扩展进行分析。

Result: 研究发现CCR和ING缩减对词错误率（WER）有轻微但显著的影响，且在没有语言模型的ASR系统中，词汇邻域效应更为明显。

Conclusion: CCR和ING缩减对ASR系统的识别准确性有显著影响，且没有语言模型的系统更容易受到词汇邻域效应的影响。

Abstract: Automatic Speech Recognition (ASR) models often struggle with the phonetic,
phonological, and morphosyntactic features found in African American English
(AAE). This study focuses on two key AAE variables: Consonant Cluster Reduction
(CCR) and ING-reduction. It examines whether the presence of CCR and
ING-reduction increases ASR misrecognition. Subsequently, it investigates
whether end-to-end ASR systems without an external Language Model (LM) are more
influenced by lexical neighborhood effect and less by contextual predictability
compared to systems with an LM. The Corpus of Regional African American
Language (CORAAL) was transcribed using wav2vec 2.0 with and without an LM. CCR
and ING-reduction were detected using the Montreal Forced Aligner (MFA) with
pronunciation expansion. The analysis reveals a small but significant effect of
CCR and ING on Word Error Rate (WER) and indicates a stronger presence of
lexical neighborhood effect in ASR systems without LMs.

</details>


### [49] [Hybrid Extractive Abstractive Summarization for Multilingual Sentiment Analysis](https://arxiv.org/abs/2506.06929)
*Mikhail Krasitskii,Grigori Sidorov,Olga Kolesnikova,Liliana Chanona Hernandez,Alexander Gelbukh*

Main category: cs.CL

TL;DR: 本文提出一种结合提取式与生成式摘要的混合方法，用于多语言情感分析，通过动态阈值和文化适应优化，在10种语言中实现高准确率与计算效率，并计划进一步优化低资源语言处理。


<details>
  <summary>Details</summary>
Motivation: 现有独立方法在多语言情感分析中存在局限性，需结合提取式与生成式摘要的优势以提升跨语言场景下的性能与适应性。

Method: 整合基于TF-IDF的提取式摘要与微调XLM-R的生成式模块，引入动态阈值调整及文化适应机制增强模型泛化能力。

Result: 实验显示英语准确率达0.90，低资源语言达0.84，计算效率较传统方法提升22%，适用于实时品牌监控与跨文化分析。

Conclusion: 该方法在多语言场景中表现优越，未来将通过8位量化进一步优化低资源语言处理，扩展实际应用场景。

Abstract: We propose a hybrid approach for multilingual sentiment analysis that
combines extractive and abstractive summarization to address the limitations of
standalone methods. The model integrates TF-IDF-based extraction with a
fine-tuned XLM-R abstractive module, enhanced by dynamic thresholding and
cultural adaptation. Experiments across 10 languages show significant
improvements over baselines, achieving 0.90 accuracy for English and 0.84 for
low-resource languages. The approach also demonstrates 22% greater
computational efficiency than traditional methods. Practical applications
include real-time brand monitoring and cross-cultural discourse analysis.
Future work will focus on optimization for low-resource languages via 8-bit
quantization.

</details>


### [50] [DiscoSum: Discourse-aware News Summarization](https://arxiv.org/abs/2506.06930)
*Alexander Spangher,Tenghao Huang,Jialiang Gu,Jiatong Shi,Muhao Chen*

Main category: cs.CL

TL;DR: 本文提出了一种将话语结构整合到新闻文章摘要生成中的新方法，开发了DiscoSum算法，通过束搜索技术实现结构感知摘要生成，并在多平台数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的文本摘要方法在保持长期话语结构方面存在不足，尤其是在新闻文章中，组织结构对读者参与度有显著影响。

Method: 本文提出了一种新的话语结构模式，并开发了DiscoSum算法，利用束搜索技术生成结构感知的新闻摘要，以满足不同风格和结构需求。

Result: 通过人工和自动评估，验证了该方法在保持叙事忠实度和满足结构要求方面的有效性。

Conclusion: 本文的方法成功地将话语结构整合到新闻摘要生成中，显著提升了摘要的质量和适用性。

Abstract: Recent advances in text summarization have predominantly leveraged large
language models to generate concise summaries. However, language models often
do not maintain long-term discourse structure, especially in news articles,
where organizational flow significantly influences reader engagement. We
introduce a novel approach to integrating discourse structure into
summarization processes, focusing specifically on news articles across various
media. We present a novel summarization dataset where news articles are
summarized multiple times in different ways across different social media
platforms (e.g. LinkedIn, Facebook, etc.). We develop a novel news discourse
schema to describe summarization structures and a novel algorithm, DiscoSum,
which employs beam search technique for structure-aware summarization, enabling
the transformation of news stories to meet different stylistic and structural
demands. Both human and automatic evaluation results demonstrate the efficacy
of our approach in maintaining narrative fidelity and meeting structural
requirements.

</details>


### [51] [What Makes a Good Natural Language Prompt?](https://arxiv.org/abs/2506.06950)
*Do Xuan Long,Duy Dinh,Ngoc-Hai Nguyen,Kenji Kawaguchi,Nancy F. Chen,Shafiq Joty,Min-Yen Kan*

Main category: cs.CL

TL;DR: 本文通过对2022年至2025年间150多篇提示相关论文的元分析，提出了一个基于属性和以人为中心的框架来评估提示质量，并揭示了现有研究在模型和任务上的不平衡支持。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的发展和人机交互的普及，提示（prompting）成为一个决定性因素，但目前对自然语言提示的量化缺乏共识。

Method: 本文通过元分析调查了150多篇提示相关论文，提出了一个包含21个属性、分为六个维度的框架来评估提示质量，并分析了这些属性对LLMs的影响。

Result: 研究发现，单一属性增强在推理任务中通常影响最大，且基于属性增强提示的指令调优可以产生更好的推理模型。

Conclusion: 本文为基于属性的提示评估和优化奠定了基础，填补了人机交互中的研究空白，并开辟了新的提示研究方向。

Abstract: As large language models (LLMs) have progressed towards more human-like and
human--AI communications have become prevalent, prompting has emerged as a
decisive component. However, there is limited conceptual consensus on what
exactly quantifies natural language prompts. We attempt to address this
question by conducting a meta-analysis surveying more than 150
prompting-related papers from leading NLP and AI conferences from 2022 to 2025
and blogs. We propose a property- and human-centric framework for evaluating
prompt quality, encompassing 21 properties categorized into six dimensions. We
then examine how existing studies assess their impact on LLMs, revealing their
imbalanced support across models and tasks, and substantial research gaps.
Further, we analyze correlations among properties in high-quality natural
language prompts, deriving prompting recommendations. We then empirically
explore multi-property prompt enhancements in reasoning tasks, observing that
single-property enhancements often have the greatest impact. Finally, we
discover that instruction-tuning on property-enhanced prompts can result in
better reasoning models. Our findings establish a foundation for
property-centric prompt evaluation and optimization, bridging the gaps between
human--AI communication and opening new prompting research directions.

</details>


### [52] [BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning](https://arxiv.org/abs/2506.06955)
*Ha-Thanh Nguyen,Chaoran Liu,Hirokazu Kiyomaru,Koichi Takeda,Yusuke Miyao,Maki Matsuda,Yusuke Oda,Pontus Stenetorp,Qianying Liu,Su Myat Noe,Hideyuki Tachibana,Kouta Nakayama,Sadao Kurohashi*

Main category: cs.CL

TL;DR: BIS Reasoning 1.0是首个大规模日语三段论推理数据集，旨在评估大语言模型在信念不一致推理中的表现，揭示其处理逻辑有效但信念冲突输入时的弱点。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要关注一般或信念一致的推理，缺乏对信念不一致推理的评估。BIS Reasoning 1.0旨在填补这一空白，揭示大语言模型在处理逻辑有效但信念冲突输入时的偏差。

Method: BIS Reasoning 1.0引入了逻辑有效但信念不一致的三段论问题，并对包括GPT、Claude和领先的日语大语言模型在内的先进模型进行基准测试。

Result: 测试结果显示，各模型表现差异显著，GPT-4o的准确率最高，达到79.54%。分析表明，当前大语言模型在处理逻辑有效但信念冲突的输入时存在明显弱点。

Conclusion: 这些发现对在法律、医疗和科学文献等高风险领域部署大语言模型具有重要意义，确保在这些领域中真理能够超越直觉信念，保障完整性和安全性。

Abstract: We present BIS Reasoning 1.0, the first large-scale Japanese dataset of
syllogistic reasoning problems explicitly designed to evaluate
belief-inconsistent reasoning in large language models (LLMs). Unlike prior
datasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned
reasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent
syllogisms to uncover reasoning biases in LLMs trained on human-aligned
corpora. We benchmark state-of-the-art models - including GPT models, Claude
models, and leading Japanese LLMs - revealing significant variance in
performance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies
critical weaknesses in current LLMs when handling logically valid but
belief-conflicting inputs. These findings have important implications for
deploying LLMs in high-stakes domains such as law, healthcare, and scientific
literature, where truth must override intuitive belief to ensure integrity and
safety.

</details>


### [53] [Learning to Clarify by Reinforcement Learning Through Reward-Weighted Fine-Tuning](https://arxiv.org/abs/2506.06964)
*Subhojyoti Mukherjee,Viet Dac Lai,Raghavendra Addanki,Ryan Rossi,Seunghyun Yoon,Trung Bui,Anup Rao,Jayakumar Subramanian,Branislav Kveton*

Main category: cs.CL

TL;DR: 本文提出了一种在问答代理中学习提出澄清问题的方法，通过模拟包含澄清问题的对话并使用强化学习进行优化，相比现有方法在奖励和语言质量上取得了提升。


<details>
  <summary>Details</summary>
Motivation: 现有的问答代理在回答自然语言问题时，往往缺乏提出澄清问题的能力，这限制了其回答的准确性和交互性。本文旨在通过强化学习优化这一能力。

Method: 本文提出了一种基于离线强化学习的方法，通过模拟包含澄清问题的对话，并使用奖励加权的监督微调进行优化，避免了额外超参数和直接优化奖励的复杂性。

Result: 与现有的监督微调和直接偏好优化方法相比，本文方法在优化奖励和语言质量上均取得了显著提升。

Conclusion: 本文提出的方法有效提升了问答代理提出澄清问题的能力，为未来问答系统的交互性和准确性提供了新的优化方向。

Abstract: Question answering (QA) agents automatically answer questions posed in
natural language. In this work, we learn to ask clarifying questions in QA
agents. The key idea in our method is to simulate conversations that contain
clarifying questions and learn from them using reinforcement learning (RL). To
make RL practical, we propose and analyze offline RL objectives that can be
viewed as reward-weighted supervised fine-tuning (SFT) and easily optimized in
large language models. Our work stands in a stark contrast to recently proposed
methods, based on SFT and direct preference optimization, which have additional
hyper-parameters and do not directly optimize rewards. We compare to these
methods empirically and report gains in both optimized rewards and language
quality.

</details>


### [54] [A dependently-typed calculus of event telicity and culminativity](https://arxiv.org/abs/2506.06968)
*Pavel Kovalev,Carlo Angiuli*

Main category: cs.CL

TL;DR: 本文提出了一种基于依赖类型的跨语言框架，用于分析事件的终结性和完成性，并通过英语例句验证。框架结合名词与动词域的分析，强调有界性及修饰对事件属性的影响，并在Agda中形式化实现。


<details>
  <summary>Details</summary>
Motivation: 现有研究对事件的有界性（telicity）和完成性（culminativity）的跨语言统一建模不足，需通过类型理论构建形式化框架以支持语言学分析与计算验证。

Method: 扩展Martin-Löf依赖类型理论，构建名词域（有界性、子类型、形容词修饰）和动词域（依赖事件演算、副词修饰）的双层模型，并通过Agda证明助手实现形式化规则与示例。

Result: 框架成功区分终结事件（受事有界）与完成事件（达到内在终点），并捕捉名词短语有界性、修饰语对事件属性的影响，相关规则在Agda中验证了逻辑一致性。

Conclusion: 该框架为跨语言事件语义分析提供了形式化基础，依赖类型理论能有效建模语言学中的组合性及推理，Agda实现验证了理论的可计算性。

Abstract: We present a dependently-typed cross-linguistic framework for analyzing the
telicity and culminativity of events, accompanied by examples of using our
framework to model English sentences. Our framework consists of two parts. In
the nominal domain, we model the boundedness of noun phrases and its
relationship to subtyping, delimited quantities, and adjectival modification.
In the verbal domain we define a dependent event calculus, modeling telic
events as those whose undergoer is bounded, culminating events as telic events
that achieve their inherent endpoint, and consider adverbial modification. In
both domains we pay particular attention to associated entailments. Our
framework is defined as an extension of intensional Martin-L\"of dependent type
theory, and the rules and examples in this paper have been formalized in the
Agda proof assistant.

</details>


### [55] [Break-The-Chain: Reasoning Failures in LLMs via Adversarial Prompting in Code Generation](https://arxiv.org/abs/2506.06971)
*Jaechul Roh,Varun Gandhi,Shivani Anilkumar,Arin Garg*

Main category: cs.CL

TL;DR: 研究通过对抗性提示扰动测试大语言模型在代码生成任务中的推理鲁棒性，发现模型表现对提示表面特征高度敏感，部分扰动导致准确率最大下降42.1%，另一些却提升35.3%，揭示当前系统脆弱性与不可预测性。


<details>
  <summary>Details</summary>
Motivation: 质疑大语言模型是否真正具备推理能力，或仅依赖统计模式。通过设计语义等价但结构对抗的提示扰动，系统性检验模型推理的稳健性。

Method: 构建包含故事化重构、无关约束注入、示例重排、数值扰动等7类对抗提示的测试集，基于700个代码生成问题评估模型表现。

Result: 不同扰动产生两极影响：部分导致准确率暴跌42.1%，另一些反提升35.3%，显示模型对提示表面特征存在非理性依赖。

Conclusion: 当前推理系统存在语义脆弱性，需建立原则性提示工程方法。公开扰动数据集与评估框架以促进可信推理研究。

Abstract: Large Language Models (LLMs) have achieved remarkable success in tasks
requiring complex reasoning, such as code generation, mathematical problem
solving, and algorithmic synthesis -- especially when aided by reasoning tokens
and Chain-of-Thought prompting. Yet, a core question remains: do these models
truly reason, or do they merely exploit shallow statistical patterns? In this
paper, we systematically investigate the robustness of reasoning LLMs by
introducing a suite of semantically faithful yet adversarially structured
prompt perturbations. Our evaluation -- spanning 700 perturbed code generations
derived from LeetCode-style problems -- applies transformations such as
storytelling reframing, irrelevant constraint injection, example reordering,
and numeric perturbation. We observe that while certain modifications severely
degrade performance (with accuracy drops up to -42.1%), others surprisingly
improve model accuracy by up to 35.3%, suggesting sensitivity not only to
semantics but also to surface-level prompt dynamics. These findings expose the
fragility and unpredictability of current reasoning systems, underscoring the
need for more principles approaches to reasoning alignments and prompting
robustness. We release our perturbation datasets and evaluation framework to
promote further research in trustworthy and resilient LLM reasoning.

</details>


### [56] [Atomic Reasoning for Scientific Table Claim Verification](https://arxiv.org/abs/2506.06972)
*Yuji Zhang,Qingyun Wang,Cheng Qian,Jiateng Liu,Chenkai Sun,Denghui Zhang,Tarek Abdelzaher,Chengxiang Zhai,Preslav Nakov,Heng Ji*

Main category: cs.CL

TL;DR: 本文提出了一种基于认知负荷理论的模块化推理方法，通过技能链模式减少认知负荷，提升模型对科学表格声明的验证能力，并在少量微调数据下超越了GPT-4o的链式思维方法。


<details>
  <summary>Details</summary>
Motivation: 科学文本的复杂性和高信息密度可能导致非专家误解，现有模型在细粒度推理上存在不足，难以准确验证科学声明。

Method: 提出技能链模式，通过开发模块化、可重用的推理组件（原子技能）来减少认知负荷，并创建跨领域基准SciAtomicBench进行评估。

Result: 仅使用350个微调样本，基于原子推理训练的模型在细粒度推理上超越了GPT-4o的链式思维方法，达到了最先进的性能。

Conclusion: 通过减少认知负荷和模块化推理，模型能够更准确、更通用地验证科学表格声明，且所需训练数据更少。

Abstract: Scientific texts often convey authority due to their technical language and
complex data. However, this complexity can sometimes lead to the spread of
misinformation. Non-experts are particularly susceptible to misleading claims
based on scientific tables due to their high information density and perceived
credibility. Existing table claim verification models, including
state-of-the-art large language models (LLMs), often struggle with precise
fine-grained reasoning, resulting in errors and a lack of precision in
verifying scientific claims. Inspired by Cognitive Load Theory, we propose that
enhancing a model's ability to interpret table-based claims involves reducing
cognitive load by developing modular, reusable reasoning components (i.e.,
atomic skills). We introduce a skill-chaining schema that dynamically composes
these skills to facilitate more accurate and generalizable reasoning with a
reduced cognitive load. To evaluate this, we create SciAtomicBench, a
cross-domain benchmark with fine-grained reasoning annotations. With only 350
fine-tuning examples, our model trained by atomic reasoning outperforms
GPT-4o's chain-of-thought method, achieving state-of-the-art results with far
less training data.

</details>


### [57] [Chain of Methodologies: Scaling Test Time Computation without Training](https://arxiv.org/abs/2506.06982)
*Cong Liu,Jie Wu,Weigang Wu,Xu Chen,Liang Lin,Wei-Shi Zheng*

Main category: cs.CL

TL;DR: 本文提出了一种名为Chain of Methodologies (CoM)的创新提示框架，通过整合人类方法论洞察，增强大语言模型（LLMs）的结构化思维，使其能够处理复杂推理任务。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理复杂推理任务时，由于训练数据中缺乏深入的洞察，表现不佳。本文旨在通过引入人类方法论洞察，提升LLMs的推理能力。

Method: 本文提出了Chain of Methodologies (CoM)框架，利用高级LLMs的元认知能力，通过用户定义的方法论激活系统化推理，无需显式微调。

Result: 实验表明，CoM在复杂推理任务上超越了竞争基线，展示了无需训练的提示方法作为复杂推理任务的稳健解决方案的潜力。

Conclusion: CoM通过整合人类方法论洞察，显著提升了LLMs的复杂推理能力，为接近人类水平的推理提供了新的途径。

Abstract: Large Language Models (LLMs) often struggle with complex reasoning tasks due
to insufficient in-depth insights in their training data, which are typically
absent in publicly available documents. This paper introduces the Chain of
Methodologies (CoM), an innovative and intuitive prompting framework that
enhances structured thinking by integrating human methodological insights,
enabling LLMs to tackle complex tasks with extended reasoning. CoM leverages
the metacognitive abilities of advanced LLMs, activating systematic reasoning
throught user-defined methodologies without explicit fine-tuning. Experiments
show that CoM surpasses competitive baselines, demonstrating the potential of
training-free prompting methods as robust solutions for complex reasoning tasks
and bridging the gap toward human-level reasoning through human-like
methodological insights.

</details>


### [58] [Cultural Bias Matters: A Cross-Cultural Benchmark Dataset and Sentiment-Enriched Model for Understanding Multimodal Metaphors](https://arxiv.org/abs/2506.06987)
*Senqi Yang,Dongyu Zhang,Jing Ren,Ziqi Xu,Xiuzhen Zhang,Yiliao Song,Hongfei Lin,Feng Xia*

Main category: cs.CL

TL;DR: 本文提出了MultiMM数据集和SEMD模型，用于跨文化的多模态隐喻研究，旨在减少NLP中的文化偏见。


<details>
  <summary>Details</summary>
Motivation: 现有隐喻处理研究主要依赖英语数据，存在西方文化偏见，影响模型性能和NLP进展，特别是多模态语境下的文化偏见影响尚未充分研究。

Method: 引入MultiMM数据集，包含8,461个中英文广告文本-图像对，并提出Sentiment-Enriched Metaphor Detection (SEMD)模型，结合情感嵌入提升跨文化隐喻理解。

Result: 实验验证了SEMD在隐喻检测和情感分析任务中的有效性。

Conclusion: 本文提升了NLP研究中对文化偏见的认识，为开发更公平、包容的语言模型做出贡献。

Abstract: Metaphors are pervasive in communication, making them crucial for natural
language processing (NLP). Previous research on automatic metaphor processing
predominantly relies on training data consisting of English samples, which
often reflect Western European or North American biases. This cultural skew can
lead to an overestimation of model performance and contributions to NLP
progress. However, the impact of cultural bias on metaphor processing,
particularly in multimodal contexts, remains largely unexplored. To address
this gap, we introduce MultiMM, a Multicultural Multimodal Metaphor dataset
designed for cross-cultural studies of metaphor in Chinese and English. MultiMM
consists of 8,461 text-image advertisement pairs, each accompanied by
fine-grained annotations, providing a deeper understanding of multimodal
metaphors beyond a single cultural domain. Additionally, we propose
Sentiment-Enriched Metaphor Detection (SEMD), a baseline model that integrates
sentiment embeddings to enhance metaphor comprehension across cultural
backgrounds. Experimental results validate the effectiveness of SEMD on
metaphor detection and sentiment analysis tasks. We hope this work increases
awareness of cultural bias in NLP research and contributes to the development
of fairer and more inclusive language models. Our dataset and code are
available at https://github.com/DUTIR-YSQ/MultiMM.

</details>


### [59] [What makes Reasoning Models Different? Follow the Reasoning Leader for Efficient Decoding](https://arxiv.org/abs/2506.06998)
*Ming Li,Zhengyuan Yang,Xiyao Wang,Dianqi Li,Kevin Lin,Tianyi Zhou,Lijuan Wang*

Main category: cs.CL

TL;DR: 大型推理模型（LRMs）通过生成长链思维实现强推理性能，但冗长的推理过程导致推理速度变慢且容易陷入不必要的细节。本文提出FoReaL-Decoding方法，通过协作的快慢思维解码在成本与质量之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 为了理解LRMs的行为，本文系统分析了推理模型与非推理模型在token级别上的错位现象，特别是全局错位反弹和局部错位减少现象，并提出了改进方法。

Method: 提出FoReaL-Decoding方法，使用一个主导模型引导每句话的前几个token，然后由较弱的草稿模型完成剩余部分，并通过随机门平滑切换大小模型。

Result: 在四个数学推理基准测试中，FoReaL-Decoding减少了30%到50%的理论FLOPs，缩短了40%的推理链长度，同时保持了86%到100%的模型性能。

Conclusion: FoReaL-Decoding为推理任务提供了一种简单、即插即用的方法，实现了成本与质量的可控权衡。

Abstract: Large reasoning models (LRMs) achieve strong reasoning performance by
emitting long chains of thought. Yet, these verbose traces slow down inference
and often drift into unnecessary detail, known as the overthinking phenomenon.
To better understand LRMs' behavior, we systematically analyze the token-level
misalignment between reasoning and non-reasoning models. While it is expected
that their primary difference lies in the stylistic "thinking cues", LRMs
uniquely exhibit two pivotal, previously under-explored phenomena: a Global
Misalignment Rebound, where their divergence from non-reasoning models persists
or even grows as response length increases, and more critically, a Local
Misalignment Diminish, where the misalignment concentrates at the "thinking
cues" each sentence starts with but rapidly declines in the remaining of the
sentence. Motivated by the Local Misalignment Diminish, we propose
FoReaL-Decoding, a collaborative fast-slow thinking decoding method for
cost-quality trade-off. In FoReaL-Decoding, a Leading model leads the first few
tokens for each sentence, and then a weaker draft model completes the following
tokens to the end of each sentence. FoReaL-Decoding adopts a stochastic gate to
smoothly interpolate between the small and the large model. On four popular
math-reasoning benchmarks (AIME24, GPQA-Diamond, MATH500, AMC23),
FoReaL-Decoding reduces theoretical FLOPs by 30 to 50% and trims CoT length by
up to 40%, while preserving 86 to 100% of model performance. These results
establish FoReaL-Decoding as a simple, plug-and-play route to controllable
cost-quality trade-offs in reasoning-centric tasks.

</details>


### [60] [Adversarial Paraphrasing: A Universal Attack for Humanizing AI-Generated Text](https://arxiv.org/abs/2506.07001)
*Yize Cheng,Vinu Sankar Sadasivan,Mehrdad Saberi,Shoumik Saha,Soheil Feizi*

Main category: cs.CL

TL;DR: 本文提出一种无需训练的对抗性改述框架，通过指令式LLM在检测器引导下生成对抗样本，显著降低多种AI文本检测器的检测率，揭示现有检测策略在复杂规避技术下的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成文本检测器虽对简单改述攻击具备一定鲁棒性，但仍面临更高级规避技术的挑战。研究旨在开发一种通用、无需训练的攻击方法，有效绕过检测系统。

Method: 利用现成指令遵循型LLM，在AI文本检测器（如OpenAI-RoBERTa-Large）的实时反馈下对AI生成内容进行对抗性改述，优化规避检测的文本特征。

Result: 对抗性改述使RADAR和Fast-DetectGPT的T@1%F分别降低64.49%和98.96%，在神经网络/水印/零样本检测器中平均降低87.88% T@1%F，且文本质量仅轻微下降。

Conclusion: 当前检测策略对复杂对抗攻击仍显脆弱，需开发更鲁棒的防御机制以应对日益精进的规避技术，同时需权衡文本质量与检测规避的平衡。

Abstract: The increasing capabilities of Large Language Models (LLMs) have raised
concerns about their misuse in AI-generated plagiarism and social engineering.
While various AI-generated text detectors have been proposed to mitigate these
risks, many remain vulnerable to simple evasion techniques such as
paraphrasing. However, recent detectors have shown greater robustness against
such basic attacks. In this work, we introduce Adversarial Paraphrasing, a
training-free attack framework that universally humanizes any AI-generated text
to evade detection more effectively. Our approach leverages an off-the-shelf
instruction-following LLM to paraphrase AI-generated content under the guidance
of an AI text detector, producing adversarial examples that are specifically
optimized to bypass detection. Extensive experiments show that our attack is
both broadly effective and highly transferable across several detection
systems. For instance, compared to simple paraphrasing attack--which,
ironically, increases the true positive at 1% false positive (T@1%F) by 8.57%
on RADAR and 15.03% on Fast-DetectGPT--adversarial paraphrasing, guided by
OpenAI-RoBERTa-Large, reduces T@1%F by 64.49% on RADAR and a striking 98.96% on
Fast-DetectGPT. Across a diverse set of detectors--including neural
network-based, watermark-based, and zero-shot approaches--our attack achieves
an average T@1%F reduction of 87.88% under the guidance of
OpenAI-RoBERTa-Large. We also analyze the tradeoff between text quality and
attack success to find that our method can significantly reduce detection
rates, with mostly a slight degradation in text quality. Our adversarial setup
highlights the need for more robust and resilient detection strategies in the
light of increasingly sophisticated evasion techniques.

</details>


### [61] [A Culturally-diverse Multilingual Multimodal Video Benchmark & Model](https://arxiv.org/abs/2506.07032)
*Bhuiyan Sanjid Shafique,Ashmal Vayani,Muhammad Maaz,Hanoona Abdul Rasheed,Dinura Dissanayake,Mohammed Irfan Kurpath,Yahya Hmaiti,Go Inoue,Jean Lahoud,Md. Safirur Rashid,Shadid Intisar Quasem,Maheen Fatima,Franco Vidal,Mykola Maslych,Ketan Pravin More,Sanoojan Baliah,Hasindri Watawana,Yuhao Li,Fabian Farestam,Leon Schaller,Roman Tymtsiv,Simon Weber,Hisham Cholakkal,Ivan Laptev,Shin'ichi Satoh,Michael Felsberg,Mubarak Shah,Salman Khan,Fahad Shahbaz Khan*

Main category: cs.CL

TL;DR: 本文提出了多语言视频大模型（ViMUL）及其基准测试（ViMUL-Bench），旨在评估和提升视频大模型在14种语言中的表现，促进文化和语言包容性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大模型主要集中在英语上，缺乏对其他语言和文化的包容性，特别是在视频理解领域。

Method: 作者开发了ViMUL-Bench基准测试，涵盖14种语言和15个文化类别，并构建了一个包含120万样本的多语言视频训练集，同时提出了一个简单的多语言视频大模型ViMUL。

Result: ViMUL在高资源和低资源语言之间取得了更好的平衡，ViMUL-Bench和训练数据集为未来研究提供了基础。

Conclusion: ViMUL-Bench、ViMUL模型及大规模多语言视频训练集的发布将推动文化和语言包容性多语言视频大模型的研究。

Abstract: Large multimodal models (LMMs) have recently gained attention due to their
effectiveness to understand and generate descriptions of visual content. Most
existing LMMs are in English language. While few recent works explore
multilingual image LMMs, to the best of our knowledge, moving beyond the
English language for cultural and linguistic inclusivity is yet to be
investigated in the context of video LMMs. In pursuit of more inclusive video
LMMs, we introduce a multilingual Video LMM benchmark, named ViMUL-Bench, to
evaluate Video LMMs across 14 languages, including both low- and high-resource
languages: English, Chinese, Spanish, French, German, Hindi, Arabic, Russian,
Bengali, Urdu, Sinhala, Tamil, Swedish, and Japanese. Our ViMUL-Bench is
designed to rigorously test video LMMs across 15 categories including eight
culturally diverse categories, ranging from lifestyles and festivals to foods
and rituals and from local landmarks to prominent cultural personalities.
ViMUL-Bench comprises both open-ended (short and long-form) and multiple-choice
questions spanning various video durations (short, medium, and long) with 8k
samples that are manually verified by native language speakers. In addition, we
also introduce a machine translated multilingual video training set comprising
1.2 million samples and develop a simple multilingual video LMM, named ViMUL,
that is shown to provide a better tradeoff between high-and low-resource
languages for video understanding. We hope our ViMUL-Bench and multilingual
video LMM along with a large-scale multilingual video training set will help
ease future research in developing cultural and linguistic inclusive
multilingual video LMMs. Our proposed benchmark, video LMM and training data
will be publicly released at https://mbzuai-oryx.github.io/ViMUL/.

</details>


### [62] [KG2QA: Knowledge Graph-enhanced Retrieval-Augmented Generation for Communication Standards Question Answering](https://arxiv.org/abs/2506.07037)
*Zhongze Luo,Weixuan Wan,Qizhi Zheng,Yanhong Bai,Jingyun Sun,Jian Wang,Dan Wang*

Main category: cs.CL

TL;DR: 本文结合大语言模型微调与知识图谱，构建通信标准智能问答系统，显著提升模型性能与查询准确率，具备良好应用价值。


<details>
  <summary>Details</summary>
Motivation: 传统通信标准咨询周期长、依赖专家经验，难以满足技术快速发展需求，需开发高效智能解决方案。

Method: 采用LoRA微调Qwen2.5-7B-Instruct模型，结合含13,906实体/13,524关系的知识图谱，搭建RAG框架实现检索增强问答。

Result: 微调后BLEU-4从18.8564升至66.8993，ROUGE等指标显著提升；知识图谱查询准确率良好，DeepSeek评估平均分提升2.26%。

Conclusion: 系统通过模型微调与知识图谱结合，有效提升问答效果与交互体验，具备实际应用价值。

Abstract: There are many types of standards in the field of communication. The
traditional consulting model has a long cycle and relies on the knowledge and
experience of experts, making it difficult to meet the rapidly developing
technological demands. This paper combines the fine-tuning of large language
models with the construction of knowledge graphs to implement an intelligent
consultation and question-answering system for communication standards. The
experimental results show that after LoRA tuning on the constructed dataset of
6,587 questions and answers in the field of communication standards,
Qwen2.5-7B-Instruct demonstrates outstanding professional capabilities in the
field of communication standards on the test set. BLEU-4 rose from 18.8564 to
66.8993, and evaluation indicators such as ROUGE also increased significantly,
outperforming the fine-tuning effect of the comparison model
Llama-3-8B-Instruct. Based on the ontology framework containing 6 entity
attributes and 10 relation attributes, a knowledge graph of the communication
standard domain containing 13,906 entities and 13,524 relations was
constructed, showing a relatively good query accuracy rate. The intelligent
consultation and question-answering system enables the fine-tuned model on the
server side to access the locally constructed knowledge graph and conduct
graphical retrieval of key information first, which is conducive to improving
the question-answering effect. The evaluation using DeepSeek as the Judge on
the test set shows that our RAG framework enables the fine-tuned model to
improve the scores at all five angles, with an average score increase of 2.26%.
And combined with web services and API interfaces, it has achieved very good
results in terms of interaction experience and back-end access, and has very
good practical application value.

</details>


### [63] [Reasoning with RAGged events: RAG-Enhanced Event Knowledge Base Construction and reasoning with proof-assistants](https://arxiv.org/abs/2506.07042)
*Stergios Chatzikyriakidis*

Main category: cs.CL

TL;DR: 本文提出使用多种大语言模型（GPT-4、Claude、Llama 3.2）结合知识图谱增强与RAG策略，自动提取历史事件结构化表示，并通过Coq形式化验证实现高阶推理。研究发现不同增强策略优化不同性能维度，模型架构决定策略敏感性。


<details>
  <summary>Details</summary>
Motivation: 解决手动构建历史事件计算表示的高成本问题，并突破RDF/OWL仅支持一阶逻辑的局限性，以实现更深层次的时空与语义分析。

Method: 采用三种增强策略（基础生成、知识图谱增强、RAG）结合多LLM模型，基于修昔底德历史文本进行实验评估，并开发RDF到Coq的自动转换流程。

Result: 基础生成策略在事件覆盖度最优（Claude/GPT-4），RAG提升坐标精度与元数据完整性；大模型基线稳健而Llama 3.2表现波动大；Coq验证成功实现因果验证与形式化证明。

Conclusion: 增强策略需针对性能维度选择性使用，模型规模决定策略有效性边界。通过形式化方法可突破RDF限制，验证历史事件语义结构的领域合法性。

Abstract: Extracting structured computational representations of historical events from
narrative text remains computationally expensive when constructed manually.
While RDF/OWL reasoners enable graph-based reasoning, they are limited to
fragments of first-order logic, preventing deeper temporal and semantic
analysis. This paper addresses both challenges by developing automatic
historical event extraction models using multiple LLMs (GPT-4, Claude, Llama
3.2) with three enhancement strategies: pure base generation, knowledge graph
enhancement, and Retrieval-Augmented Generation (RAG). We conducted
comprehensive evaluations using historical texts from Thucydides. Our findings
reveal that enhancement strategies optimize different performance dimensions
rather than providing universal improvements. For coverage and historical
breadth, base generation achieves optimal performance with Claude and GPT-4
extracting comprehensive events. However, for precision, RAG enhancement
improves coordinate accuracy and metadata completeness. Model architecture
fundamentally determines enhancement sensitivity: larger models demonstrate
robust baseline performance with incremental RAG improvements, while Llama 3.2
shows extreme variance from competitive performance to complete failure. We
then developed an automated translation pipeline converting extracted RDF
representations into Coq proof assistant specifications, enabling higher-order
reasoning beyond RDF capabilities including multi-step causal verification,
temporal arithmetic with BC dates, and formal proofs about historical
causation. The Coq formalization validates that RAG-discovered event types
represent legitimate domain-specific semantic structures rather than
ontological violations.

</details>


### [64] [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/abs/2506.07044)
*LASA Team,Weiwen Xu,Hou Pong Chan,Long Li,Mahani Aljunied,Ruifeng Yuan,Jianyu Wang,Chenghao Xiao,Guizhen Chen,Chaoqun Liu,Zhaodonghui Li,Yu Sun,Junao Shen,Chaojun Wang,Jie Tan,Deli Zhao,Tingyang Xu,Hao Zhang,Yu Rong*

Main category: cs.CL

TL;DR: 本文提出了一种针对医疗场景的多模态大语言模型Lingshu，通过综合数据整理和多阶段训练，解决了现有医疗MLLM在知识覆盖、幻觉问题和推理能力上的不足，并在多个医疗任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在医疗应用中的效果有限，主要由于医疗场景与通用领域的数据和任务存在差异，导致知识覆盖不全、易产生幻觉以及缺乏针对复杂医疗场景的推理能力。

Method: 提出了一种综合数据整理方法，从医学影像、文本和通用数据中获取丰富的医学知识，并生成准确的医学描述、视觉问答和推理样本。在此基础上，开发了医疗专用MLLM Lingshu，并通过多阶段训练和强化学习增强其推理能力。同时，开发了MedEvalKit评估框架。

Result: Lingshu在多模态问答、文本问答和医疗报告生成等任务上表现优异，显著优于现有的开源多模态模型。

Conclusion: Lingshu通过综合数据整理和多阶段训练，有效提升了医疗MLLM的性能，为医疗领域的多模态应用提供了新的解决方案。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities in understanding common visual elements, largely due to their
large-scale datasets and advanced training strategies. However, their
effectiveness in medical applications remains limited due to the inherent
discrepancies between data and tasks in medical scenarios and those in the
general domain. Concretely, existing medical MLLMs face the following critical
limitations: (1) limited coverage of medical knowledge beyond imaging, (2)
heightened susceptibility to hallucinations due to suboptimal data curation
processes, (3) lack of reasoning capabilities tailored for complex medical
scenarios. To address these challenges, we first propose a comprehensive data
curation procedure that (1) efficiently acquires rich medical knowledge data
not only from medical imaging but also from extensive medical texts and
general-domain data; and (2) synthesizes accurate medical captions, visual
question answering (VQA), and reasoning samples. As a result, we build a
multimodal dataset enriched with extensive medical knowledge. Building on the
curated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu
undergoes multi-stage training to embed medical expertise and enhance its
task-solving capabilities progressively. Besides, we preliminarily explore the
potential of applying reinforcement learning with verifiable rewards paradigm
to enhance Lingshu's medical reasoning ability. Additionally, we develop
MedEvalKit, a unified evaluation framework that consolidates leading multimodal
and textual medical benchmarks for standardized, fair, and efficient model
assessment. We evaluate the performance of Lingshu on three fundamental medical
tasks, multimodal QA, text-based QA, and medical report generation. The results
show that Lingshu consistently outperforms the existing open-source multimodal
models on most tasks ...

</details>


### [65] [Com$^2$: A Causal-Guided Benchmark for Exploring Complex Commonsense Reasoning in Large Language Models](https://arxiv.org/abs/2506.07064)
*Kai Xiong,Xiao Ding,Yixin Cao,Yuxiong Yan,Li Du,Yufei Zhang,Jinglong Gao,Jiaqian Liu,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: 提出Com²基准测试，评估大语言模型在复杂常识推理中的不足，发现后训练与慢思考可改善表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注数学与代码等复杂任务，但复杂常识推理（如事件长期影响）因不确定性与结构缺失未被充分探索，需填补此空白。

Method: 结合因果事件图构建结构化复杂常识，利用因果理论（如干预）生成不同场景，并通过LLM慢思考合成测试案例，构建侦探故事子集增强挑战性。

Result: 实验表明LLMs在推理深度与广度上存在困难，但后训练与慢思考策略可缓解此问题。

Conclusion: Com²有效揭示LLMs复杂推理缺陷，未来可通过结构化知识整合与推理策略优化提升模型表现。

Abstract: Large language models (LLMs) have mastered abundant simple and explicit
commonsense knowledge through pre-training, enabling them to achieve human-like
performance in simple commonsense reasoning. Nevertheless, LLMs struggle to
reason with complex and implicit commonsense knowledge that is derived from
simple ones (such as understanding the long-term effects of certain events), an
aspect humans tend to focus on more. Existing works focus on complex tasks like
math and code, while complex commonsense reasoning remains underexplored due to
its uncertainty and lack of structure. To fill this gap and align with
real-world concerns, we propose a benchmark Com$^2$ focusing on complex
commonsense reasoning. We first incorporate causal event graphs to serve as
structured complex commonsense. Then we adopt causal theory~(e.g.,
intervention) to modify the causal event graphs and obtain different scenarios
that meet human concerns. Finally, an LLM is employed to synthesize examples
with slow thinking, which is guided by the logical relationships in the
modified causal graphs. Furthermore, we use detective stories to construct a
more challenging subset. Experiments show that LLMs struggle in reasoning depth
and breadth, while post-training and slow thinking can alleviate this. The code
and data are available at https://github.com/Waste-Wood/Com2.

</details>


### [66] [Representation Decomposition for Learning Similarity and Contrastness Across Modalities for Affective Computing](https://arxiv.org/abs/2506.07086)
*Yuanhe Tian,Pengsen Cheng,Guoqing Jin,Lei Zhang,Yan Song*

Main category: cs.CL

TL;DR: 本文提出了一种基于LLM的多模态情感计算方法，通过分解视觉和文本表示来捕捉跨模态的复杂信息，并在多个情感计算任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态情感计算方法通常依赖于单模态分析或简单的跨模态信息融合，无法有效捕捉不同模态之间的复杂和冲突信息。

Method: 本文方法首先使用预训练的多模态编码器对输入模态进行编码和对齐，然后通过表示分解框架将共同情感内容与独特线索分离，最后通过注意力机制整合这些分解信号，形成动态软提示用于多模态LLM。

Result: 在三个代表性情感计算任务（多模态方面情感分析、多模态情感分析和仇恨表情包检测）上的实验表明，该方法显著优于现有基线模型和最新模型。

Conclusion: 本文提出的方法通过分解和整合多模态信息，有效提升了情感计算的性能，展示了其在复杂情感理解任务中的潜力。

Abstract: Multi-modal affective computing aims to automatically recognize and interpret
human attitudes from diverse data sources such as images and text, thereby
enhancing human-computer interaction and emotion understanding. Existing
approaches typically rely on unimodal analysis or straightforward fusion of
cross-modal information that fail to capture complex and conflicting evidence
presented across different modalities. In this paper, we propose a novel
LLM-based approach for affective computing that explicitly deconstructs visual
and textual representations into shared (modality-invariant) and
modality-specific components. Specifically, our approach firstly encodes and
aligns input modalities using pre-trained multi-modal encoders, then employs a
representation decomposition framework to separate common emotional content
from unique cues, and finally integrates these decomposed signals via an
attention mechanism to form a dynamic soft prompt for a multi-modal LLM.
Extensive experiments on three representative tasks for affective computing,
namely, multi-modal aspect-based sentiment analysis, multi-modal emotion
analysis, and hateful meme detection, demonstrate the effectiveness of our
approach, which consistently outperforms strong baselines and state-of-the-art
models.

</details>


### [67] [How Far Are We from Optimal Reasoning Efficiency?](https://arxiv.org/abs/2506.07104)
*Jiaxuan Gao,Shu Yan,Qixin Tan,Lu Yang,Shusheng Xu,Wei Fu,Zhiyu Mei,Kaifeng Lyu,Yi Wu*

Main category: cs.CL

TL;DR: 本文提出推理效率差距（REG）作为评估大型推理模型（LRMs）效率的统一指标，并设计REO-RL算法通过强化学习优化模型在特定token预算下的效率，显著减少REG且保持准确性。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型（LRMs）的推理过程冗长冗余，导致高推理成本且难以实际部署。现有微调方法在效率评估上不一致，需统一指标衡量效率提升。

Method: 提出推理效率前沿（efficiency frontiers）和效率差距（REG）指标，并设计REO-RL算法，通过强化学习在稀疏token预算下优化模型效率目标，结合指数预算策略和数值积分降低误差。

Result: 实验显示REO-RL将REG降低≥50%，在16K token预算下接近Qwen3-4B/8B的效率前沿且精度损失极小，验证了预算策略的有效性。

Conclusion: REG能有效衡量推理效率的权衡，REO-RL显著提升效率，但完全对齐效率前沿仍是开放挑战。

Abstract: Large Reasoning Models (LRMs) demonstrate remarkable problem-solving
capabilities through extended Chain-of-Thought (CoT) reasoning but often
produce excessively verbose and redundant reasoning traces. This inefficiency
incurs high inference costs and limits practical deployment. While existing
fine-tuning methods aim to improve reasoning efficiency, assessing their
efficiency gains remains challenging due to inconsistent evaluations. In this
work, we introduce the reasoning efficiency frontiers, empirical upper bounds
derived from fine-tuning base LRMs across diverse approaches and training
configurations. Based on these frontiers, we propose the Reasoning Efficiency
Gap (REG), a unified metric quantifying deviations of any fine-tuned LRMs from
these frontiers. Systematic evaluation on challenging mathematical benchmarks
reveals significant gaps in current methods: they either sacrifice accuracy for
short length or still remain inefficient under tight token budgets. To reduce
the efficiency gap, we propose REO-RL, a class of Reinforcement Learning
algorithms that minimizes REG by targeting a sparse set of token budgets.
Leveraging numerical integration over strategically selected budgets, REO-RL
approximates the full efficiency objective with low error using a small set of
token budgets. Through systematic benchmarking, we demonstrate that our
efficiency metric, REG, effectively captures the accuracy-length trade-off,
with low-REG methods reducing length while maintaining accuracy. Our approach,
REO-RL, consistently reduces REG by >=50 across all evaluated LRMs and matching
Qwen3-4B/8B efficiency frontiers under a 16K token budget with minimal accuracy
loss. Ablation studies confirm the effectiveness of our exponential token
budget strategy. Finally, our findings highlight that fine-tuning LRMs to
perfectly align with the efficiency frontiers remains an open challenge.

</details>


### [68] [Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models](https://arxiv.org/abs/2506.07106)
*Samir Abdaljalil,Hasan Kurban,Khalid Qaraqe,Erchin Serpedin*

Main category: cs.CL

TL;DR: 本文提出Theorem-of-Thought (ToTh)框架，通过模拟三种推理代理（溯因、演绎、归纳）协作构建结构化推理图，结合贝叶斯信念传播和自然语言推理评估一致性，显著提升大语言模型在符号与数值推理任务中的性能，并生成可解释的逻辑链。


<details>
  <summary>Details</summary>
Motivation: 现有Chain-of-Thought (CoT)等提示方法虽能提升大语言模型推理可靠性，但缺乏对推理过程逻辑结构的约束和内部一致性的评估机制，导致推理过程脆弱且难以解释。

Method: ToTh框架引入三个并行推理代理（分别对应溯因、演绎、归纳推理），生成结构化推理图，利用自然语言推理引导的贝叶斯信念传播评估步骤置信度，选择最连贯的推理图生成最终结果。

Result: 在符号推理（WebOfLies）和数值推理（MultiArith）任务中，ToTh在多种大语言模型上均优于CoT、Self-Consistency等方法，且生成的推理链具备逻辑可解释性。

Conclusion: ToTh通过结构化协作推理与一致性评估机制，为大语言模型构建更稳健、受认知启发的推理框架提供了新方向，代码已开源。

Abstract: Large language models (LLMs) have shown strong performance across natural
language reasoning tasks, yet their reasoning processes remain brittle and
difficult to interpret. Prompting techniques like Chain-of-Thought (CoT)
enhance reliability by eliciting intermediate reasoning steps or aggregating
multiple outputs. However, they lack mechanisms for enforcing logical structure
and assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a
novel framework that models reasoning as collaboration among three parallel
agents, each simulating a distinct mode of inference: abductive, deductive, and
inductive. Each agent produces a reasoning trace, which is structured into a
formal reasoning graph. To evaluate consistency, we apply Bayesian belief
propagation guided by natural language inference (NLI), assigning confidence
scores to each step. The most coherent graph is selected to derive the final
answer. Experiments on symbolic (WebOfLies) and numerical (MultiArith)
reasoning benchmarks show that ToTh consistently outperforms CoT,
Self-Consistency, and CoT-Decoding across multiple LLMs, while producing
interpretable and logically grounded reasoning chains. Our findings suggest a
promising direction for building more robust and cognitively inspired LLM
reasoning. The implementation is available at
https://github.com/KurbanIntelligenceLab/theorem-of-thought.

</details>


### [69] [Prompting Science Report 2: The Decreasing Value of Chain of Thought in Prompting](https://arxiv.org/abs/2506.07142)
*Lennart Meincke,Ethan Mollick,Lilach Mollick,Dan Shapiro*

Main category: cs.CL

TL;DR: 链式思考（CoT）提示法的效果因任务和模型类型而异：对非推理模型小幅提升平均表现但增加答案波动性，而自带推理能力的模型增益有限且显著增加成本。


<details>
  <summary>Details</summary>
Motivation: 通过技术测试帮助商业、教育和政策决策者理解AI应用细节，重点评估链式思考提示法对大型语言模型推理任务的实际有效性。

Method: 通过对比测试不同模型在链式思考提示下的表现，分析任务类型、模型架构与提示方法之间的交互效应，并量化token消耗与响应时间变化。

Result: 非推理模型使用CoT时准确率微升但错误率波动，新型模型常自带隐式推理导致CoT效果弱化；显式推理模型使用CoT仅边际提升准确率，但token消耗增加50-200%。

Conclusion: 链式思考提示的适用性高度依赖模型特性与任务类型，需权衡准确率增益与计算资源成本，建议优先在非自主推理模型上针对性使用。

Abstract: This is the second in a series of short reports that seek to help business,
education, and policy leaders understand the technical details of working with
AI through rigorous testing. In this report, we investigate Chain-of-Thought
(CoT) prompting, a technique that encourages a large language model (LLM) to
"think step by step" (Wei et al., 2022). CoT is a widely adopted method for
improving reasoning tasks, however, our findings reveal a more nuanced picture
of its effectiveness. We demonstrate two things:
  - The effectiveness of Chain-of-Thought prompting can vary greatly depending
on the type of task and model. For non-reasoning models, CoT generally improves
average performance by a small amount, particularly if the model does not
inherently engage in step-by-step processing by default. However, CoT can
introduce more variability in answers, sometimes triggering occasional errors
in questions the model would otherwise get right. We also found that many
recent models perform some form of CoT reasoning even if not asked; for these
models, a request to perform CoT had little impact. Performing CoT generally
requires far more tokens (increasing cost and time) than direct answers.
  - For models designed with explicit reasoning capabilities, CoT prompting
often results in only marginal, if any, gains in answer accuracy. However, it
significantly increases the time and tokens needed to generate a response.

</details>


### [70] [Semantic-preserved Augmentation with Confidence-weighted Fine-tuning for Aspect Category Sentiment Analysis](https://arxiv.org/abs/2506.07148)
*Yaping Chai,Haoran Xie,Joe S. Qin*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型（LLM）的数据增强策略，用于提升低资源场景下的方面类别情感分析（ACSA）任务性能，通过结构化提示模板和后处理技术确保语义一致性，并采用置信度加权微调策略提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 在低资源场景下，数据稀缺是影响模型性能的主要问题。本文旨在通过数据增强策略，提升方面类别情感分析任务的性能，特别是在语义一致性和语言多样性方面。

Method: 本文提出了一种基于LLM的数据增强策略，使用结构化提示模板生成预定义内容，并通过后处理技术确保生成句子与原始句子的语义一致性。此外，采用置信度加权微调策略，鼓励模型生成更自信和准确的情感极性预测。

Result: 在四个基准数据集上，本文方法在所有基线模型中均取得了最佳性能，显著提升了模型对方面类别与情感极性关系的理解能力。

Conclusion: 本文提出的数据增强策略和置信度加权微调方法有效提升了低资源场景下方面类别情感分析任务的性能，证明了其在语义覆盖和预测准确性方面的优势。

Abstract: Large language model (LLM) is an effective approach to addressing data
scarcity in low-resource scenarios. Recent existing research designs
hand-crafted prompts to guide LLM for data augmentation. We introduce a data
augmentation strategy for the aspect category sentiment analysis (ACSA) task
that preserves the original sentence semantics and has linguistic diversity,
specifically by providing a structured prompt template for an LLM to generate
predefined content. In addition, we employ a post-processing technique to
further ensure semantic consistency between the generated sentence and the
original sentence. The augmented data increases the semantic coverage of the
training distribution, enabling the model better to understand the relationship
between aspect categories and sentiment polarities, enhancing its inference
capabilities. Furthermore, we propose a confidence-weighted fine-tuning
strategy to encourage the model to generate more confident and accurate
sentiment polarity predictions. Compared with powerful and recent works, our
method consistently achieves the best performance on four benchmark datasets
over all baselines.

</details>


### [71] [Syntactic Control of Language Models by Posterior Inference](https://arxiv.org/abs/2506.07154)
*Vicky Xefteri,Tim Vieira,Ryan Cotterell,Afra Amini*

Main category: cs.CL

TL;DR: 本文提出了一种基于后验推断的采样算法，通过结合序列蒙特卡洛和句法标注器，有效控制生成文本的句法结构，显著提高了句法准确性。


<details>
  <summary>Details</summary>
Motivation: 在需要清晰度、风格一致性或可解释性的应用中，控制语言模型生成文本的句法结构具有重要价值，但这一任务仍然具有挑战性。

Method: 本文提出了一种结合序列蒙特卡洛和句法标注器的方法，通过从提议分布中采样来估计后验分布，确保每个生成的词符与目标句法结构对齐。

Result: 实验表明，使用适当的提议分布，GPT2和Llama3-8B模型的句法准确性显著提高，F1分数分别从12.31和35.33提升至约93，且不影响语言模型的流畅性。

Conclusion: 本文的研究结果强调了句法控制的复杂性以及采样算法的有效性，为需要精确控制句法的应用提供了一种有前景的方法。

Abstract: Controlling the syntactic structure of text generated by language models is
valuable for applications requiring clarity, stylistic consistency, or
interpretability, yet it remains a challenging task. In this paper, we argue
that sampling algorithms based on the posterior inference can effectively
enforce a target constituency structure during generation. Our approach
combines sequential Monte Carlo, which estimates the posterior distribution by
sampling from a proposal distribution, with a syntactic tagger that ensures
that each generated token aligns with the desired syntactic structure. Our
experiments with GPT2 and Llama3-8B models show that with an appropriate
proposal distribution, we can improve syntactic accuracy, increasing the F1
score from $12.31$ (GPT2-large) and $35.33$ (Llama3-8B) to about $93$ in both
cases without compromising the language model's fluency. These results
underscore both the complexity of syntactic control and the effectiveness of
sampling algorithms, offering a promising approach for applications where
precise control over syntax is essential.

</details>


### [72] [GeometryZero: Improving Geometry Solving for LLM with Group Contrastive Policy Optimization](https://arxiv.org/abs/2506.07160)
*Yikun Wang,Yibin Wang,Dianyi Wang,Zimian Peng,Qipeng Guo,Dacheng Tao,Jiaqi Wang*

Main category: cs.CL

TL;DR: 提出Group Contrastive Policy Optimization (GCPO)强化学习框架，通过组对比掩码和长度奖励优化几何问题中的辅助构造策略，开发GeometryZero模型，在多个基准测试中显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有几何推理方法依赖大型语言模型或无条件奖励机制，导致计算成本高且辅助构造效率低下。需设计高效的小型化模型以结合辅助构造与强化学习的优势。

Method: 提出GCPO框架：1) 组对比掩码根据上下文效用提供动态奖励信号；2) 长度奖励机制促进长推理链生成。基于此开发GeometryZero系列模型，智能控制辅助构造时机。

Result: 在Geometry3K和MathVista等基准测试中，GeometryZero模型平均性能提升4.29%，显著优于GRPO等基线方法。

Conclusion: GCPO框架有效解决了无条件奖励的局限性，GeometryZero证明了小模型通过强化学习优化辅助构造的可行性，为高效几何推理提供了新方向。

Abstract: Recent advances in large language models (LLMs) have demonstrated remarkable
capabilities across diverse domains, particularly in mathematical reasoning,
amid which geometry problem solving remains a challenging area where auxiliary
construction plays a enssential role. Existing approaches either achieve
suboptimal performance or rely on massive LLMs (e.g., GPT-4o), incurring
massive computational costs. We posit that reinforcement learning with
verifiable reward (e.g., GRPO) offers a promising direction for training
smaller models that effectively combine auxiliary construction with robust
geometric reasoning. However, directly applying GRPO to geometric reasoning
presents fundamental limitations due to its dependence on unconditional
rewards, which leads to indiscriminate and counterproductive auxiliary
constructions. To address these challenges, we propose Group Contrastive Policy
Optimization (GCPO), a novel reinforcement learning framework featuring two key
innovations: (1) Group Contrastive Masking, which adaptively provides positive
or negative reward signals for auxiliary construction based on contextual
utility, and a (2) length reward that promotes longer reasoning chains.
Building on GCPO, we develop GeometryZero, a family of affordable-size
geometric reasoning models that judiciously determine when to employ auxiliary
construction. Our extensive empirical evaluation across popular geometric
benchmarks (Geometry3K, MathVista) demonstrates that GeometryZero models
consistently outperform baselines (e.g. GRPO), achieving an average improvement
of 4.29% across all benchmarks.

</details>


### [73] [CTDGSI: A comprehensive exploitation of instance selection methods for automatic text classification. VII Concurso de Teses, Dissertações e Trabalhos de Graduação em SI -- XXI Simpósio Brasileiro de Sistemas de Informação](https://arxiv.org/abs/2506.07169)
*Washington Cunha,Leonardo Rocha,Marcos André Gonçalves*

Main category: cs.CL

TL;DR: 该论文探讨了自然语言处理中的实例选择技术，提出两种新方法，显著减少训练集规模并保持模型效果。


<details>
  <summary>Details</summary>
Motivation: 当前自然语言处理领域依赖大量数据和计算资源，训练大型密集模型成本高昂，实例选择技术有潜力降低这些成本。

Method: 论文全面比较了实例选择方法在自动文本分类任务中的应用，并提出了两种针对大数据集和Transformer架构的新方法。

Result: 最终解决方案平均减少41%的训练集规模，同时保持模型效果，并实现1.67倍至2.46倍的加速。

Conclusion: 实例选择技术在自然语言处理中具有巨大潜力，能够显著降低训练成本并提高效率。

Abstract: Progress in Natural Language Processing (NLP) has been dictated by the rule
of more: more data, more computing power and more complexity, best exemplified
by the Large Language Models. However, training (or fine-tuning) large dense
models for specific applications usually requires significant amounts of
computing resources. This \textbf{Ph.D. dissertation} focuses on an
under-investi\-gated NLP data engineering technique, whose potential is
enormous in the current scenario known as Instance Selection (IS). The IS goal
is to reduce the training set size by removing noisy or redundant instances
while maintaining the effectiveness of the trained models and reducing the
training process cost. We provide a comprehensive and scientifically sound
comparison of IS methods applied to an essential NLP task -- Automatic Text
Classification (ATC), considering several classification solutions and many
datasets. Our findings reveal a significant untapped potential for IS
solutions. We also propose two novel IS solutions that are noise-oriented and
redundancy-aware, specifically designed for large datasets and transformer
architectures. Our final solution achieved an average reduction of 41\% in
training sets, while maintaining the same levels of effectiveness in all
datasets. Importantly, our solutions demonstrated speedup improvements of 1.67x
(up to 2.46x), making them scalable for datasets with hundreds of thousands of
documents.

</details>


### [74] [RULE: Reinforcement UnLEarning Achieves Forget-Retain Pareto Optimality](https://arxiv.org/abs/2506.07171)
*Chenlong Zhang,Zhuoran Jin,Hongbang Yuan,Jiaheng Wei,Tong Zhou,Kang Liu,Jun Zhao,Yubo Chen*

Main category: cs.CL

TL;DR: 本文提出了一种名为RULE的高效框架，用于从大语言模型中选择性移除特定信息，同时保持模型整体效用。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，模型中可能包含敏感、受版权保护或非法内容，引发了对其安全性和合法性的担忧。

Method: RULE框架将遗忘任务视为拒绝边界优化问题，使用少量遗忘集和合成边界查询进行训练，并通过可验证的奖励函数鼓励在相关查询上安全拒绝，同时保留对允许输入的帮助性响应。

Result: 实验结果表明，RULE在仅使用12%的遗忘集和8%的合成边界数据的情况下，在遗忘质量和响应自然度上优于现有基线，同时保持了模型的整体效用。

Conclusion: RULE不仅实现了目标遗忘，还提高了模型输出的自然度、训练效率，并展现出强大的泛化能力。

Abstract: The widespread deployment of Large Language Models (LLMs) trained on massive,
uncurated corpora has raised growing concerns about the inclusion of sensitive,
copyrighted, or illegal content. This has led to increasing interest in LLM
unlearning: the task of selectively removing specific information from a model
without retraining from scratch or degrading overall utility. However, existing
methods often rely on large-scale forget and retain datasets, and suffer from
unnatural responses, poor generalization, or catastrophic utility loss. In this
work, we propose Reinforcement UnLearning (RULE), an efficient framework that
formulates unlearning as a refusal boundary optimization problem. RULE is
trained with a small portion of the forget set and synthesized boundary
queries, using a verifiable reward function that encourages safe refusal on
forget--related queries while preserving helpful responses on permissible
inputs. We provide both theoretical and empirical evidence demonstrating the
effectiveness of RULE in achieving targeted unlearning without compromising
model utility. Experimental results show that, with only $12%$ forget set and
$8%$ synthesized boundary data, RULE outperforms existing baselines by up to
$17.5%$ forget quality and $16.3%$ naturalness response while maintaining
general utility, achieving forget--retain Pareto optimality. Remarkably, we
further observe that RULE improves the naturalness of model outputs, enhances
training efficiency, and exhibits strong generalization ability, generalizing
refusal behavior to semantically related but unseen queries.

</details>


### [75] [Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs](https://arxiv.org/abs/2506.07180)
*Wenrui Zhou,Shu Yang,Qingsong Yang,Zikun Guo,Lijie Hu,Di Wang*

Main category: cs.CL

TL;DR: 本文提出了VISE基准，用于评估视频大语言模型在误导性用户输入下的迎合行为，并探索了通过关键帧选择减少这种偏差的方法。


<details>
  <summary>Details</summary>
Motivation: 随着视频大语言模型在需要多模态推理的实际应用中的普及，确保其事实一致性和可靠性变得至关重要。然而，这些模型倾向于迎合用户输入，即使与视觉证据相矛盾，这削弱了其可信度。

Method: 本文提出了VISE基准，首次专门评估视频大语言模型在不同问题格式、提示偏见和视觉推理任务中的迎合行为，并探索了关键帧选择作为一种可解释的、无需训练的缓解策略。

Result: VISE基准能够对多种迎合类型和交互模式进行细粒度分析，并揭示了通过增强视觉基础来减少迎合偏差的潜在路径。

Conclusion: VISE基准为理解和减少视频大语言模型的迎合行为提供了系统化的评估工具，关键帧选择策略展示了缓解这种偏差的潜力。

Abstract: As video large language models (Video-LLMs) become increasingly integrated
into real-world applications that demand grounded multimodal reasoning,
ensuring their factual consistency and reliability is of critical importance.
However, sycophancy, the tendency of these models to align with user input even
when it contradicts the visual evidence, undermines their trustworthiness in
such contexts. Current sycophancy research has largely overlooked its specific
manifestations in the video-language domain, resulting in a notable absence of
systematic benchmarks and targeted evaluations to understand how Video-LLMs
respond under misleading user input. To fill this gap, we propose VISE
(Video-LLM Sycophancy Benchmarking and Evaluation), the first dedicated
benchmark designed to evaluate sycophantic behavior in state-of-the-art
Video-LLMs across diverse question formats, prompt biases, and visual reasoning
tasks. Specifically, VISE pioneeringly brings linguistic perspectives on
sycophancy into the visual domain, enabling fine-grained analysis across
multiple sycophancy types and interaction patterns. In addition, we explore
key-frame selection as an interpretable, training-free mitigation strategy,
which reveals potential paths for reducing sycophantic bias by strengthening
visual grounding.

</details>


### [76] [SDE-SQL: Enhancing Text-to-SQL Generation in Large Language Models via Self-Driven Exploration with SQL Probes](https://arxiv.org/abs/2506.07245)
*Wenxuan Xie,Yaxun Dai,Wenhao Jiang*

Main category: cs.CL

TL;DR: 提出SDE-SQL框架，通过动态SQL探针使大语言模型在推理时自主探索数据库，实现零样本下Text-to-SQL任务性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法依赖静态预处理的数据库信息，限制了模型对数据库内容的动态理解能力。缺乏自主探索机制导致模型受限于人工提供的固定上下文。

Method: SDE-SQL框架通过生成和执行SQL探针，使模型主动检索数据库信息并迭代更新数据认知。采用零样本设置，无需问题-SQL对作为上下文示例。

Result: 在BIRD基准测试中，Qwen2.5-72B-Instruct模型执行准确率相对提升8.02%，创开源模型无监督微调/集成方法的新SOTA。加入监督微调后性能可再提升0.52%。

Conclusion: SDE-SQL通过动态数据库探索机制有效突破静态上下文限制，验证了自主数据检索对Text-to-SQL任务的重要性，且具备与监督微调结合的扩展潜力。

Abstract: Recent advancements in large language models (LLMs) have significantly
improved performance on the Text-to-SQL task. However, prior approaches
typically rely on static, pre-processed database information provided at
inference time, which limits the model's ability to fully understand the
database contents. Without dynamic interaction, LLMs are constrained to fixed,
human-provided context and cannot autonomously explore the underlying data. To
address this limitation, we propose SDE-SQL, a framework that enables large
language models to perform self-driven exploration of databases during
inference. This is accomplished by generating and executing SQL probes, which
allow the model to actively retrieve information from the database and
iteratively update its understanding of the data. Unlike prior methods, SDE-SQL
operates in a zero-shot setting, without relying on any question-SQL pairs as
in-context demonstrations. When evaluated on the BIRD benchmark with
Qwen2.5-72B-Instruct, SDE-SQL achieves an 8.02% relative improvement in
execution accuracy over the vanilla Qwen2.5-72B-Instruct baseline, establishing
a new state-of-the-art among methods based on open-source models without
supervised fine-tuning (SFT) or model ensembling. Moreover, with SFT, the
performance of SDE-SQL can be further enhanced, yielding an additional 0.52%
improvement.

</details>


### [77] [Improving the Efficiency of Long Document Classification using Sentence Ranking Approach](https://arxiv.org/abs/2506.07248)
*Prathamesh Kokate,Mitali Sarnaik,Manavi Khopade,Raviraj Joshi*

Main category: cs.CL

TL;DR: 提出基于TF-IDF的句子排序方法，用于长文档分类，在保持准确率的同时显著减少输入长度和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer模型（如BERT）因固定输入长度和二次注意力复杂度难以处理长文档，且全文档分类存在信息冗余问题。

Method: 结合归一化TF-IDF分数与句子长度的增强评分策略，探索固定数量或百分比选择关键句，提升效率。

Result: 在MahaNews LDC数据集上，输入规模减少超50%，延迟降低43%，分类准确率仅比全文本基线下降0.33%。

Conclusion: 通过上下文精简实现高效长文档分类，性能损失极小，具备实际应用价值。

Abstract: Long document classification poses challenges due to the computational
limitations of transformer-based models, particularly BERT, which are
constrained by fixed input lengths and quadratic attention complexity.
Moreover, using the full document for classification is often redundant, as
only a subset of sentences typically carries the necessary information. To
address this, we propose a TF-IDF-based sentence ranking method that improves
efficiency by selecting the most informative content. Our approach explores
fixed-count and percentage-based sentence selection, along with an enhanced
scoring strategy combining normalized TF-IDF scores and sentence length.
Evaluated on the MahaNews LDC dataset of long Marathi news articles, the method
consistently outperforms baselines such as first, last, and random sentence
selection. With MahaBERT-v2, we achieve near-identical classification accuracy
with just a 0.33 percent drop compared to the full-context baseline, while
reducing input size by over 50 percent and inference latency by 43 percent.
This demonstrates that significant context reduction is possible without
sacrificing performance, making the method practical for real-world long
document classification tasks.

</details>


### [78] [Bias Attribution in Filipino Language Models: Extending a Bias Interpretability Metric for Application on Agglutinative Languages](https://arxiv.org/abs/2506.07249)
*Lance Calvin Lim Gamboa,Yue Feng,Mark Lee*

Main category: cs.CL

TL;DR: 本研究将信息论偏见归因指标应用于菲律宾语等粘着语模型，发现其偏见驱动主题（人物、物体、关系）与英语模型（犯罪行为、性相关、亲社会行为）存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 现有偏见归因研究集中于英语文本，但粘着语（如菲律宾语）的语言结构差异可能影响模型偏见机制，需探索非英语模型的偏见来源。

Method: 调整信息论偏见归因评分方法，应用于纯菲律宾语模型及三个多语言模型（全球语言、东南亚语言训练），对比分析偏见贡献因素。

Result: 菲律宾语模型的偏见主要由实体类主题（人物/物体/关系）驱动，而英语模型则更多受行为类主题（犯罪/性/亲社会行为）影响。

Conclusion: 英语与非英语模型在处理社会人口群体相关输入时存在根本性差异，强调需针对不同语言特性开发定制化偏见缓解策略。

Abstract: Emerging research on bias attribution and interpretability have revealed how
tokens contribute to biased behavior in language models processing English
texts. We build on this line of inquiry by adapting the information-theoretic
bias attribution score metric for implementation on models handling
agglutinative languages, particularly Filipino. We then demonstrate the
effectiveness of our adapted method by using it on a purely Filipino model and
on three multilingual models: one trained on languages worldwide and two on
Southeast Asian data. Our results show that Filipino models are driven towards
bias by words pertaining to people, objects, and relationships, entity-based
themes that stand in contrast to the action-heavy nature of bias-contributing
themes in English (i.e., criminal, sexual, and prosocial behaviors). These
findings point to differences in how English and non-English models process
inputs linked to sociodemographic groups and bias.

</details>


### [79] [Question Answering under Temporal Conflict: Evaluating and Organizing Evolving Knowledge with LLMs](https://arxiv.org/abs/2506.07270)
*Atahan Özer,Çağatay Yıldız*

Main category: cs.CL

TL;DR: 本文针对大语言模型(LLM)在处理动态知识时的局限性，提出轻量级外部记忆框架，在时间敏感任务中显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的知识受限于预训练数据，且现实信息持续演变。传统更新方法(如重新训练或上下文学习)存在成本高、扩展性差的问题，需探索更有效处理动态知识的方法。

Method: 提出轻量级代理框架：通过增量构建结构化外部记忆库，支持推理时基于时间过滤检索相关信息，无需重新训练模型。

Result: 在Temporal Wiki(维基历史快照)和Unified Clark(新闻时序聚合)基准测试中，该方法在复杂推理和冲突事实整合任务上显著优于上下文学习(ICL)和检索增强生成(RAG)基线。

Conclusion: 结构化外部记忆机制能有效解决LLM处理时序知识时的冲突与误导问题，为动态知识更新提供了可扩展的解决方案。

Abstract: Large language models (LLMs) exhibit remarkable capabilities in question
answering and reasoning thanks to their extensive parametric memory. However,
their knowledge is inherently limited by the scope of their pre-training data,
while real-world information evolves continuously. Updating this knowledge
typically requires costly and brittle re-training, or in-context learning
(ICL), which becomes impractical at scale given the volume and volatility of
modern information. Motivated by these limitations, we investigate how LLMs
perform when exposed to temporal text corpora, or documents that reflect
evolving knowledge over time, such as sports biographies where facts like a
player's "current team" change year by year. To this end, we introduce two new
benchmarks: Temporal Wiki, which captures factual drift across historical
Wikipedia snapshots, and Unified Clark, which aggregates timestamped news
articles to simulate real-world information accumulation. Our analysis reveals
that LLMs often struggle to reconcile conflicting or outdated facts and can be
misled when multiple versions of a fact appear in context. To address these
issues, we propose a lightweight, agentic framework that incrementally builds a
structured, external memory from source documents without requiring
re-training. This knowledge organization strategy enables models to retrieve
and reason over temporally filtered, relevant information at inference time.
Empirically, our method outperforms ICL and RAG baselines across both
benchmarks, especially on questions requiring more complex reasoning or
integration of conflicting facts.

</details>


### [80] [Parsing the Switch: LLM-Based UD Annotation for Complex Code-Switched and Low-Resource Languages](https://arxiv.org/abs/2506.07274)
*Olga Kellert,Nemika Tyagi,Muhammad Imran,Nelvin Licona-Guevara,Carlos Gómez-Rodríguez*

Main category: cs.CL

TL;DR: 本文提出了BiLingua Parser，一种基于大语言模型的注释管道，用于生成代码转换文本的通用依赖注释，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 代码转换在低资源语言环境中对句法分析提出了复杂挑战，现有解析器在单语树库上训练，难以泛化到多语言和混合语言输入。

Method: 开发了一个基于提示的框架，结合少样本大语言模型提示和专家评审，生成了西班牙语-英语和西班牙语-瓜拉尼语的注释数据集。

Result: BiLingua Parser在专家修订后达到了95.29%的LAS，显著优于现有基线。

Conclusion: 大语言模型在精心指导下，可以作为在资源匮乏的代码转换环境中引导句法资源的实用工具。

Abstract: Code-switching presents a complex challenge for syntactic analysis,
especially in low-resource language settings where annotated data is scarce.
While recent work has explored the use of large language models (LLMs) for
sequence-level tagging, few approaches systematically investigate how well
these models capture syntactic structure in code-switched contexts. Moreover,
existing parsers trained on monolingual treebanks often fail to generalize to
multilingual and mixed-language input. To address this gap, we introduce the
BiLingua Parser, an LLM-based annotation pipeline designed to produce Universal
Dependencies (UD) annotations for code-switched text. First, we develop a
prompt-based framework for Spanish-English and Spanish-Guaran\'i data,
combining few-shot LLM prompting with expert review. Second, we release two
annotated datasets, including the first Spanish-Guaran\'i UD-parsed corpus.
Third, we conduct a detailed syntactic analysis of switch points across
language pairs and communicative contexts. Experimental results show that
BiLingua Parser achieves up to 95.29% LAS after expert revision, significantly
outperforming prior baselines and multilingual parsers. These results show that
LLMs, when carefully guided, can serve as practical tools for bootstrapping
syntactic resources in under-resourced, code-switched environments. Data and
source code are available at https://github.com/N3mika/ParsingProject

</details>


### [81] [Exploring the Impact of Temperature on Large Language Models:Hot or Cold?](https://arxiv.org/abs/2506.07295)
*Lujun Li,Lama Sleem,Niccolo' Gentile,Geoffrey Nichil,Radu State*

Main category: cs.CL

TL;DR: 研究系统评估了采样温度（0-2）对不同规模开源模型（小/中/大）六种能力的影响，发现温度对模型表现具有技能特异性效应。提出基于BERT的温度选择器可优化中小模型在SuperGLUE的表现，并发现FP16推理与4位量化模型温度效应一致，且突变温度随模型规模增加而上升。


<details>
  <summary>Details</summary>
Motivation: 针对LLMs中采样温度对语义理解与随机性调控的关键作用尚不明确的问题，探究温度参数对不同规模模型多维度能力的影响机制，并解决实际应用中的最优温度选择难题。

Method: 在0-2温度范围内，使用评估六种能力的数据集对三种规模（1B-80B）开源模型进行统计分析，开发基于温度效应的BERT选择器，并扩展至FP16精度和4位量化模型进行验证。

Result: 1) 温度影响具有技能特异性 2) BERT选择器显著改善中小模型SuperGLUE表现 3) FP16与量化模型温度效应一致 4) 突变温度阈值随模型规模增大而提高。

Conclusion: 温度调控是模型能力表达的关键维度，其效应具有模型规模依赖性和任务特异性。动态温度选择机制与量化兼容性为LLMs实际部署提供了重要优化路径。

Abstract: The sampling temperature, a critical hyperparameter in large language models
(LLMs), modifies the logits before the softmax layer, thereby reshaping the
distribution of output tokens. Recent studies have challenged the Stochastic
Parrots analogy by demonstrating that LLMs are capable of understanding
semantics rather than merely memorizing data and that randomness, modulated by
sampling temperature, plays a crucial role in model inference. In this study,
we systematically evaluated the impact of temperature in the range of 0 to 2 on
data sets designed to assess six different capabilities, conducting statistical
analyses on open source models of three different sizes: small (1B--4B), medium
(6B--13B), and large (40B--80B). Our findings reveal distinct skill-specific
effects of temperature on model performance, highlighting the complexity of
optimal temperature selection in practical applications. To address this
challenge, we propose a BERT-based temperature selector that takes advantage of
these observed effects to identify the optimal temperature for a given prompt.
We demonstrate that this approach can significantly improve the performance of
small and medium models in the SuperGLUE datasets. Furthermore, our study
extends to FP16 precision inference, revealing that temperature effects are
consistent with those observed in 4-bit quantized models. By evaluating
temperature effects up to 4.0 in three quantized models, we find that the
Mutation Temperature -- the point at which significant performance changes
occur -- increases with model size.

</details>


### [82] [Subjectivity in the Annotation of Bridging Anaphora](https://arxiv.org/abs/2506.07297)
*Lauren Levine,Amir Zeldes*

Main category: cs.CL

TL;DR: 本文探讨了桥接指代标注中的主观性，提出了新的桥接子类型分类系统，并发现现有资源可能被严重低估。


<details>
  <summary>Details</summary>
Motivation: 桥接指代标注中的主观性导致一致性难以达成，本文旨在探索这一主观性并改进标注系统。

Method: 在GUM语料库的测试集上进行标注实验，提出新的桥接子类型分类系统，并与现有方案进行比较。

Result: 发现现有资源可能被严重低估，桥接子类型标注一致性中等，但桥接实例的标注一致性较低。

Conclusion: 桥接指代标注中的主观性显著影响标注一致性，新的分类系统有助于改进标注质量。

Abstract: Bridging refers to the associative relationship between inferable entities in
a discourse and the antecedents which allow us to understand them, such as
understanding what "the door" means with respect to an aforementioned "house".
As identifying associative relations between entities is an inherently
subjective task, it is difficult to achieve consistent agreement in the
annotation of bridging anaphora and their antecedents. In this paper, we
explore the subjectivity involved in the annotation of bridging instances at
three levels: anaphor recognition, antecedent resolution, and bridging subtype
selection. To do this, we conduct an annotation pilot on the test set of the
existing GUM corpus, and propose a newly developed classification system for
bridging subtypes, which we compare to previously proposed schemes. Our results
suggest that some previous resources are likely to be severely under-annotated.
We also find that while agreement on the bridging subtype category was
moderate, annotator overlap for exhaustively identifying instances of bridging
is low, and that many disagreements resulted from subjective understanding of
the entities involved.

</details>


### [83] [ConfQA: Answer Only If You Are Confident](https://arxiv.org/abs/2506.07309)
*Yin Huang,Yifan Ethan Xu,Kai Sun,Vera Yan,Alicia Sun,Haidar Khan,Jimmy Nguyen,Mohammad Kachuee,Zhaojiang Lin,Yue Liu,Aaron Colak,Anuj Kumar,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CL

TL;DR: 本文提出了一种名为ConfQA的微调策略，通过引导LLM在不确定时承认“我不确定”，并结合知识图谱中的简单事实来校准置信度，显著减少了LLM的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决大型语言模型（LLM）在生成事实性陈述时出现的幻觉问题，即模型生成不准确或虚构的信息。

Method: 提出ConfQA微调策略，通过“仅在自信时回答”的提示和知识图谱中的属性值来校准LLM的置信度，并引入双神经知识框架，根据置信度选择内部参数化知识或外部符号知识。

Result: ConfQA将幻觉率从20-40%降低至5%以下，并在多个事实性基准测试中表现出色，同时减少了30%以上的不必要外部检索。

Conclusion: ConfQA策略和双神经知识框架有效减少了LLM的幻觉现象，提升了模型的准确性和效率，具有跨领域和问题类型的鲁棒性。

Abstract: Can we teach Large Language Models (LLMs) to refrain from hallucinating
factual statements? In this paper we present a fine-tuning strategy that we
call ConfQA, which can reduce hallucination rate from 20-40% to under 5% across
multiple factuality benchmarks. The core idea is simple: when the LLM answers a
question correctly, it is trained to continue with the answer; otherwise, it is
trained to admit "I am unsure". But there are two key factors that make the
training highly effective. First, we introduce a dampening prompt "answer only
if you are confident" to explicitly guide the behavior, without which
hallucination remains high as 15%-25%. Second, we leverage simple factual
statements, specifically attribute values from knowledge graphs, to help LLMs
calibrate the confidence, resulting in robust generalization across domains and
question types. Building on this insight, we propose the Dual Neural Knowledge
framework, which seamlessly select between internally parameterized neural
knowledge and externally recorded symbolic knowledge based on ConfQA's
confidence. The framework enables potential accuracy gains to beyond 95%, while
reducing unnecessary external retrievals by over 30%.

</details>


### [84] [Low-resource Machine Translation: what for? who for? An observational study on a dedicated Tetun language translation service](https://arxiv.org/abs/2411.12262)
*Raphael Merx,Adérito José Guterres Correia,Hanna Suominen,Ekaterina Vylomova*

Main category: cs.CL

TL;DR: 通过分析东帝汶Tetun语言翻译平台的实际使用数据，发现用户需求与现有语料库存在显著差异，建议低资源机器翻译应优先教育相关领域的高资源到低资源方向翻译。


<details>
  <summary>Details</summary>
Motivation: 传统调查方法依赖小样本受访者，难以全面理解低资源机器翻译的社区需求。研究旨在通过真实使用数据揭示实际需求与现有语料库的差异。

Method: 对专业Tetun翻译平台tetun.org的10万条翻译请求进行观察性研究，分析用户行为模式与翻译方向。

Result: 用户(多为移动端学生)主要从高资源语言译入Tetun，涉及科学、医疗等多元领域，与以政府/社会新闻为主的现有语料库形成鲜明对比。

Conclusion: 制度化少数民族语言的MT系统应优先保证教育相关领域的高资源到低资源翻译准确性，观察性分析能有效指导低资源语言技术开发。

Abstract: Low-resource machine translation (MT) presents a diversity of community needs
and application challenges that remain poorly understood. To complement surveys
and focus groups, which tend to rely on small samples of respondents, we
propose an observational study on actual usage patterns of tetun$.$org, a
specialized MT service for the Tetun language, which is the lingua franca in
Timor-Leste. Our analysis of 100,000 translation requests reveals patterns that
challenge assumptions based on existing corpora. We find that users, many of
them students on mobile devices, typically translate text from a high-resource
language into Tetun across diverse domains including science, healthcare, and
daily life. This contrasts sharply with available Tetun corpora, which are
dominated by news articles covering government and social issues. Our results
suggest that MT systems for institutionalized minority languages like Tetun
should prioritize accuracy on domains relevant to educational contexts, in the
high-resource to low-resource direction. More broadly, this study demonstrates
how observational analysis can inform low-resource language technology
development, by grounding research in practical community needs.

</details>


### [85] [Reward Model Interpretability via Optimal and Pessimal Tokens](https://arxiv.org/abs/2506.07326)
*Brian Christian,Hannah Rose Kirk,Jessica A. F. Thompson,Christopher Summerfield,Tsvetomira Dumbalska*

Main category: cs.CL

TL;DR: 研究发现，尽管奖励模型被广泛用于对齐大语言模型与人类价值观，但其自身存在显著异质性、系统性编码偏差、对提示框架的敏感性以及高频词过估值等问题，并可能传播有害身份偏见。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注奖励模型在生成模型微调中的应用，而对其直接编码人类价值判断的内部机制研究不足。本文旨在揭示奖励模型作为人类价值观载体的潜在缺陷与局限性。

Method: 通过系统分析十个开源奖励模型在整个词汇空间中对单token响应的评分模式，考察不同参数规模和架构的模型在价值导向提示下的响应特性。

Result: 发现四大核心现象：(1)同目标模型间存在显著异质性；(2)高低分token编码存在系统性不对称；(3)提示框架敏感性反映人类认知偏差；(4)高频token系统性高估。同时揭示无害性训练可能导致身份偏见强化。

Conclusion: 奖励模型并非可靠的人类价值观代理，其内在偏差可能通过下游LLM传播放大。研究质疑现有奖励模型的互换性假设，强调需重新评估其作为复杂情境价值判断工具的适用性。

Abstract: Reward modeling has emerged as a crucial component in aligning large language
models with human values. Significant attention has focused on using reward
models as a means for fine-tuning generative models. However, the reward models
themselves -- which directly encode human value judgments by turning
prompt-response pairs into scalar rewards -- remain relatively understudied. We
present a novel approach to reward model interpretability through exhaustive
analysis of their responses across their entire vocabulary space. By examining
how different reward models score every possible single-token response to
value-laden prompts, we uncover several striking findings: (i) substantial
heterogeneity between models trained on similar objectives, (ii) systematic
asymmetries in how models encode high- vs low-scoring tokens, (iii) significant
sensitivity to prompt framing that mirrors human cognitive biases, and (iv)
overvaluation of more frequent tokens. We demonstrate these effects across ten
recent open-source reward models of varying parameter counts and architectures.
Our results challenge assumptions about the interchangeability of reward
models, as well as their suitability as proxies of complex and
context-dependent human values. We find that these models can encode concerning
biases toward certain identity groups, which may emerge as unintended
consequences of harmlessness training -- distortions that risk propagating
through the downstream large language models now deployed to millions.

</details>


### [86] [Improving LLM Reasoning through Interpretable Role-Playing Steering](https://arxiv.org/abs/2506.07335)
*Anyi Wang,Dong Shu,Yifan Wang,Yunpu Ma,Mengnan Du*

Main category: cs.CL

TL;DR: 本文提出SRPS框架，通过识别并操控大语言模型内部与角色扮演相关的特征，提升推理能力，相比传统提示工程具有更好的可解释性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示工程的角色扮演方法存在稳定性与可解释性不足的问题，需开发更可控的模型内部特征操作方法。

Method: SRPS框架提取角色提示的潜在表征，根据激活模式筛选关键特征，构建可调节强度的转向向量注入残差流。

Result: 在零样本思维链场景下，Llama3.1-8B在CSQA准确率提升7.94%，Gemma2-9B在SVAMP提升7.6%，多基准测试均显示稳定增益。

Conclusion: SRPS通过特征操控有效增强LLM推理能力，为角色信息对模型激活机制的研究提供了新视角与工具支持。

Abstract: Role-playing has emerged as an effective technique for enhancing the
reasoning capabilities of large language models (LLMs). However, existing
methods primarily rely on prompt engineering, which often lacks stability and
interpretability. In this paper, we introduce Sparse Autoencoder Role-Playing
Steering (SRPS), a novel framework that identifies and manipulates internal
model features associated with role-playing behavior. Our approach extracts
latent representations from role-play prompts, selects the most relevant
features based on activation patterns, and constructs a steering vector that
can be injected into the model's residual stream with controllable intensity.
Our method enables fine-grained control over role-specific behavior and offers
insights into how role information influences internal model activations.
Extensive experiments across various reasoning benchmarks and model sizes
demonstrate consistent performance gains. Notably, in the zero-shot
chain-of-thought (CoT) setting, the accuracy of Llama3.1-8B on CSQA improves
from 31.86% to 39.80%, while Gemma2-9B on SVAMP increases from 37.50% to
45.10%. These results highlight the potential of SRPS to enhance reasoning
ability in LLMs, providing better interpretability and stability compared to
traditional prompt-based role-playing.

</details>


### [87] [Refusal-Feature-guided Teacher for Safe Finetuning via Data Filtering and Alignment Distillation](https://arxiv.org/abs/2506.07356)
*Seokil Ham,Yubin Choi,Seungju Cho,Yujin Yang,Younghun Kim,Changick Kim*

Main category: cs.CL

TL;DR: 本文提出了一种基于拒绝特征的教师模型（ReFT），用于在Finetuning-as-a-Service中过滤有害提示，确保大语言模型的安全对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的Finetuning-as-a-Service在用户数据包含有害提示时，可能导致大语言模型的安全对齐退化。本文旨在解决这一问题。

Method: 通过从安全对齐的大语言模型中提取反映拒绝行为的特征（拒绝特征），训练ReFT模型，以识别并过滤有害提示，并在微调过程中将安全对齐知识蒸馏到基础模型中。

Result: 实验表明，基于ReFT的微调策略有效减少了有害输出，并提高了用户特定任务的微调准确性。

Conclusion: ReFT模型为Finetuning-as-a-Service提供了一种安全可靠的解决方案，确保大语言模型在定制化任务中的安全部署。

Abstract: Recently, major AI service providers such as Google and OpenAI have
introduced Finetuning-as-a-Service, which enables users to customize Large
Language Models (LLMs) for specific downstream tasks using their own data.
However, this service is vulnerable to degradation of LLM safety-alignment when
user data contains harmful prompts. While some prior works address this issue,
fundamentally filtering harmful data from user data remains unexplored.
Motivated by our observation that a directional representation reflecting
refusal behavior (called the refusal feature) obtained from safety-aligned LLMs
can inherently distinguish between harmful and harmless prompts, we propose the
Refusal-Feature-guided Teacher (ReFT). Our ReFT model is trained to identify
harmful prompts based on the similarity between input prompt features and its
refusal feature. During finetuning, the ReFT model serves as a teacher that
filters harmful prompts from user data and distills alignment knowledge into
the base model. Extensive experiments demonstrate that our ReFT-based
finetuning strategy effectively minimizes harmful outputs and enhances
finetuning accuracy for user-specific tasks, offering a practical solution for
secure and reliable deployment of LLMs in Finetuning-as-a-Service.

</details>


### [88] [SEED: Enhancing Text-to-SQL Performance and Practical Usability Through Automatic Evidence Generation](https://arxiv.org/abs/2506.07423)
*Janghyeon Yun,Sang-goo Lee*

Main category: cs.CL

TL;DR: 本文提出SEED系统，通过自动生成证据来提升无证据场景下的Text-to-SQL模型性能，并在BIRD和Spider数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL研究依赖BIRD数据集，但该数据集假设用户具备专业知识和领域背景，且人工生成的证据存在缺陷，影响了模型性能。

Method: SEED系统通过分析数据库模式、描述文件和值，自动生成相关证据，以提升模型在实际场景中的可用性和性能。

Result: SEED在无证据场景下显著提高了SQL生成准确性，在某些情况下甚至优于提供BIRD证据的设置。

Conclusion: SEED生成的证据不仅缩小了研究与实际部署之间的差距，还提升了Text-to-SQL模型的适应性和鲁棒性。

Abstract: Text-to-SQL enables non-experts to retrieve data from databases by converting
natural language queries into SQL. However, state-of-the-art text-to-SQL
studies rely on the BIRD dataset, which assumes that evidence is provided along
with questions. Although BIRD facilitates research advancements, it assumes
that users have expertise and domain knowledge, contradicting the fundamental
goal of text-to-SQL. In addition, human-generated evidence in BIRD contains
defects, including missing or erroneous evidence, which affects model
performance. To address this issue, we propose SEED (System for Evidence
Extraction and Domain knowledge generation), an approach that automatically
generates evidence to improve performance and practical usability in real-world
scenarios. SEED systematically analyzes database schema, description files, and
values to extract relevant information. We evaluated SEED on BIRD and Spider,
demonstrating that it significantly improves SQL generation accuracy in the
no-evidence scenario, and in some cases, even outperforms the setting where
BIRD evidence is provided. Our results highlight that SEED-generated evidence
not only bridges the gap between research and real-world deployment but also
improves the adaptability and robustness of text-to-SQL models. Our code is
available at https://github.com/felix01189/SEED

</details>


### [89] [Plug-in and Fine-tuning: Bridging the Gap between Small Language Models and Large Language Models](https://arxiv.org/abs/2506.07424)
*Kyeonghyun Kim,Jinhee Jang,Juhwan Choi,Yoonji Lee,Kyohoon Jin,YoungBin Kim*

Main category: cs.CL

TL;DR: 提出PiFi框架，结合大语言模型（LLM）的知识泛化能力与小语言模型（SLM）的计算效率，通过集成LLM的冻结层并微调，在低计算成本下提升任务性能及跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）计算成本高，难以在资源受限环境中部署；小语言模型（SLM）虽高计算效率，但泛化能力不足。需结合两者优势以兼顾性能与效率。

Method: 将LLM的单个冻结层集成到SLM中，针对特定任务微调组合模型，以低成本引入LLM知识并保持计算效率。

Result: PiFi在多种自然语言处理任务（理解与生成）中性能显著提升，且能有效迁移LLM知识，增强对未见领域的泛化能力。

Conclusion: PiFi通过融合LLM与SLM的优势，在资源受限环境下实现高性能，为平衡模型效率与泛化能力提供了新方案。

Abstract: Large language models (LLMs) are renowned for their extensive linguistic
knowledge and strong generalization capabilities, but their high computational
demands make them unsuitable for resource-constrained environments. In
contrast, small language models (SLMs) are computationally efficient but often
lack the broad generalization capacity of LLMs. To bridge this gap, we propose
PiFi, a novel framework that combines the strengths of both LLMs and SLMs to
achieve high performance while maintaining efficiency. PiFi integrates a single
frozen layer from an LLM into a SLM and fine-tunes the combined model for
specific tasks, boosting performance without a significant increase in
computational cost. We show that PiFi delivers consistent performance
improvements across a range of natural language processing tasks, including
both natural language understanding and generation. Moreover, our findings
demonstrate PiFi's ability to effectively leverage LLM knowledge, enhancing
generalization to unseen domains and facilitating the transfer of linguistic
abilities.

</details>


### [90] [Conjoined Predication and Scalar Implicature](https://arxiv.org/abs/2506.07429)
*Ratna Kandala*

Main category: cs.CL

TL;DR: 本文探讨Magri（2016）遗留的连词第一难题，提出其不自然性源于连词谓语的集体/并发解读引发的间接语境矛盾，并扩展标量含义的语用机制解释。


<details>
  <summary>Details</summary>
Motivation: Magri提出的连词第一难题揭示了量化、集体/并发解释与语境更新维度间的未解互动，需进一步分析其不自然性的根源。

Method: 将Magri难题置于原理论框架内进行概念分析，考察连词谓语的集体或并发解读如何导致语境矛盾。

Result: 发现连词如'(Only) Some Italians...'的集体解读会引发间接语境矛盾，且标量含义的语用机制超出穷尽化语法解释范围。

Conclusion: 连词结构的语用异常性源于语境矛盾的间接触发，标量含义生成机制需包含更广泛的语用原则，挑战现有语法化理论。

Abstract: Magri (2016) investigates two puzzles arising from conjunction. Although
Magri has proposed a solution to the second puzzle, the first remains
unresolved. This first puzzle reveals a hidden interaction among
quantification, collective/concurrent interpretation, and contextual updating
dimensions that have yet to be explored. In essence, the problem is that
certain forms of sentences like "Some Italians come from a warm country," when
conjoined as in "(Only) Some Italians come from a warm country and are blond,"
sound infelicitous, even though no obvious alternative triggers a conflicting
scalar implicature. In this paper, we offer a conceptual analysis of Magri's
first puzzle by situating it within its original theoretical framework. We
argue that the oddness arises from the collective or concurrent reading of the
conjunctive predicate: in examples such as "(Only) Some Italians come from a
warm country and are blond," this interpretation generates an indirect
contextual contradiction. Moreover, we suggest that the pragmatic mechanisms
governing scalar implicature generation extend beyond what is captured by
exhaustification-based grammatical licensing accounts.

</details>


### [91] [Well Begun is Half Done: Low-resource Preference Alignment by Weak-to-Strong Decoding](https://arxiv.org/abs/2506.07434)
*Feifan Song,Shaohang Wei,Wen Luo,Yuxuan Fan,Tianyu Liu,Guoyin Wang,Houfeng Wang*

Main category: cs.CL

TL;DR: 提出Weak-to-Strong Decoding（WSD）框架，通过小型对齐模型引导基础大模型生成对齐内容，解决低资源对齐方法的质量与对齐性矛盾。


<details>
  <summary>Details</summary>
Motivation: 现有低资源对齐方法在同时保证生成内容的高质量与对齐性上面临挑战，且解码初始阶段生成对齐响应的难度较高。

Method: WSD框架利用小型对齐模型生成对齐的开头内容，由GenerAlign数据集微调的Pilot-3B作为草案模型，基础大模型续写后续内容，并通过自动切换机制控制生成过程。

Result: WSD显著提升不同基础模型的对齐能力，优于所有基线方法，且避免下游任务性能下降（对齐税）。实验验证了时间效率与机制内在关联。

Conclusion: WSD通过解耦解码阶段角色分配，有效结合小模型对齐优势与大模型生成能力，为低资源对齐提供高效解决方案，并揭示其内在机制。

Abstract: Large Language Models (LLMs) require alignment with human preferences to
avoid generating offensive, false, or meaningless content. Recently,
low-resource methods for LLM alignment have been popular, while still facing
challenges in obtaining both high-quality and aligned content. Motivated by the
observation that the difficulty of generating aligned responses is concentrated
at the beginning of decoding, we propose a novel framework, Weak-to-Strong
Decoding (WSD), to enhance the alignment ability of base models by the guidance
of a small aligned model. The small model first drafts well-aligned beginnings,
followed by the large base model to continue the rest, controlled by a
well-designed auto-switch mechanism. We also collect a new dataset, GenerAlign,
to fine-tune a small-sized Pilot-3B as the draft model, which effectively
enhances different base models under the WSD framework to outperform all
baseline methods, while avoiding degradation on downstream tasks, termed as the
alignment tax. Extensive experiments are further conducted to examine the
impact of different settings and time efficiency, as well as analyses on the
intrinsic mechanisms of WSD in depth.

</details>


### [92] [LG-ANNA-Embedding technical report](https://arxiv.org/abs/2506.07438)
*Jooyoung Choi,Hyun Kim,Hansol Jang,Changwook Jun,Kyunghoon Bae,Hyewon Choi,Stanley Jungkyu Choi,Honglak Lee,Chulmin Yun*

Main category: cs.CL

TL;DR: 本文提出了一种基于指令的统一框架，用于学习适用于信息检索和非信息检索任务的通用文本嵌入，结合上下文学习、软监督和自适应硬负样本挖掘，无需任务特定微调。


<details>
  <summary>Details</summary>
Motivation: 为了生成适用于多种任务的通用文本嵌入，本文旨在结合上下文学习、软监督和自适应硬负样本挖掘，提升模型在分类、语义相似性、聚类和重排序等任务中的表现。

Method: 基于Mistral-7B模型，结合上下文学习、软监督和自适应硬负样本挖掘，使用结构化指令和少量示例指导模型，并通过软标签框架和自适应边距硬负样本挖掘提升语义区分能力。

Result: 在MTEB（英语，v2）基准测试中，模型在41个任务上表现出色，Borda得分排名靠前，优于多个更大或完全微调的基线模型。

Conclusion: 本文展示了结合上下文提示、软监督和自适应采样在生成高质量、可扩展文本嵌入方面的有效性。

Abstract: This report presents a unified instruction-based framework for learning
generalized text embeddings optimized for both information retrieval (IR) and
non-IR tasks. Built upon a decoder-only large language model (Mistral-7B), our
approach combines in-context learning, soft supervision, and adaptive
hard-negative mining to generate context-aware embeddings without task-specific
fine-tuning. Structured instructions and few-shot examples are used to guide
the model across diverse tasks, enabling strong performance on classification,
semantic similarity, clustering, and reranking benchmarks. To improve semantic
discrimination, we employ a soft labeling framework where continuous relevance
scores, distilled from a high-performance dense retriever and reranker, serve
as fine-grained supervision signals. In addition, we introduce adaptive
margin-based hard-negative mining, which filters out semantically ambiguous
negatives based on their similarity to positive examples, thereby enhancing
training stability and retrieval robustness. Our model is evaluated on the
newly introduced MTEB (English, v2) benchmark, covering 41 tasks across seven
categories. Results show that our method achieves strong generalization and
ranks among the top-performing models by Borda score, outperforming several
larger or fully fine-tuned baselines. These findings highlight the
effectiveness of combining in-context prompting, soft supervision, and adaptive
sampling for scalable, high-quality embedding generation.

</details>


### [93] [Understanding Cross-Domain Adaptation in Low-Resource Topic Modeling](https://arxiv.org/abs/2506.07453)
*Pritom Saha Akash,Kevin Chen-Chuan Chang*

Main category: cs.CL

TL;DR: 本文提出DALTA框架，通过领域对齐的潜在主题适应方法，解决低资源场景下主题建模的稳定性与连贯性问题，结合共享编码器、专用解码器和对抗对齐技术，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有主题模型在低资源目标领域数据不足时，因知识迁移不稳定且易受无关内容干扰，导致主题推断效果差。需实现跨领域知识选择性迁移以提升模型表现。

Method: 提出DALTA框架：1) 共享编码器提取领域不变特征；2) 独立解码器保留领域特性；3) 对抗对齐技术最小化潜在空间差异，防止目标域过拟合。

Result: 在多个低资源数据集上，DALTA在主题连贯性、稳定性和迁移性方面均优于现有方法，验证了理论泛化边界与框架有效性。

Conclusion: DALTA通过领域对齐机制实现高效知识迁移，为低资源主题建模提供了理论保障与实践方案，具有广泛适用性。

Abstract: Topic modeling plays a vital role in uncovering hidden semantic structures
within text corpora, but existing models struggle in low-resource settings
where limited target-domain data leads to unstable and incoherent topic
inference. We address this challenge by formally introducing domain adaptation
for low-resource topic modeling, where a high-resource source domain informs a
low-resource target domain without overwhelming it with irrelevant content. We
establish a finite-sample generalization bound showing that effective knowledge
transfer depends on robust performance in both domains, minimizing latent-space
discrepancy, and preventing overfitting to the data. Guided by these insights,
we propose DALTA (Domain-Aligned Latent Topic Adaptation), a new framework that
employs a shared encoder for domain-invariant features, specialized decoders
for domain-specific nuances, and adversarial alignment to selectively transfer
relevant information. Experiments on diverse low-resource datasets demonstrate
that DALTA consistently outperforms state-of-the-art methods in terms of topic
coherence, stability, and transferability.

</details>


### [94] [KScope: A Framework for Characterizing the Knowledge Status of Language Models](https://arxiv.org/abs/2506.07458)
*Yuxin Xiao,Shan Chen,Jack Gallifant,Danielle Bitterman,Thomas Hartvigsen,Marzyeh Ghassemi*

Main category: cs.CL

TL;DR: 本文提出KScope框架，通过分层统计测试将大语言模型的知识状态分为五类，分析上下文对知识更新的影响，发现支持性上下文缩小知识差距，特征驱动更新效果，并验证改进方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注大语言模型在知识冲突下的行为，但无法全面评估模型对问题的真实认知水平，需更系统的知识状态分类与分析框架。

Method: 提出基于一致性和正确性的五类知识状态分类法，构建KScope分层统计测试框架，逐步验证假设并归类模型知识状态。

Result: 实验表明：支持性上下文缩小知识差距；难度、相关性和熟悉度特征驱动知识更新；模型在部分正确/冲突时特征偏好相似，完全错误时差异显著；特征约束的上下文摘要与增强可信度提升更新效果。

Conclusion: KScope系统揭示LLM知识状态特征分析可优化知识更新，上下文质量与模型可信度协同作用，方法具有跨模型泛化性。

Abstract: Characterizing a large language model's (LLM's) knowledge of a given question
is challenging. As a result, prior work has primarily examined LLM behavior
under knowledge conflicts, where the model's internal parametric memory
contradicts information in the external context. However, this does not fully
reflect how well the model knows the answer to the question. In this paper, we
first introduce a taxonomy of five knowledge statuses based on the consistency
and correctness of LLM knowledge modes. We then propose KScope, a hierarchical
framework of statistical tests that progressively refines hypotheses about
knowledge modes and characterizes LLM knowledge into one of these five
statuses. We apply KScope to nine LLMs across four datasets and systematically
establish: (1) Supporting context narrows knowledge gaps across models. (2)
Context features related to difficulty, relevance, and familiarity drive
successful knowledge updates. (3) LLMs exhibit similar feature preferences when
partially correct or conflicted, but diverge sharply when consistently wrong.
(4) Context summarization constrained by our feature analysis, together with
enhanced credibility, further improves update effectiveness and generalizes
across LLMs.

</details>


### [95] [From Calibration to Collaboration: LLM Uncertainty Quantification Should Be More Human-Centered](https://arxiv.org/abs/2506.07461)
*Siddartha Devic,Tejas Srinivasan,Jesse Thomason,Willie Neiswanger,Vatsal Sharan*

Main category: cs.CL

TL;DR: 当前大语言模型（LLM）的不确定性量化（UQ）方法存在生态效度低、仅考虑认知不确定性、指标不实用等问题，需转向以用户为中心的研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有LLM不确定性量化方法未能有效支持用户在现实任务中的决策，需改进以提升人机协作的可靠性。

Method: 通过分析40种LLM UQ方法，识别出三个阻碍用户效用的问题，并提出用户导向的解决方案。

Result: 发现三大问题：低生态效度基准测试、忽略其他不确定性类型、非实用性优化指标，并提出对应改进策略。

Conclusion: LLM不确定性量化研究应放弃非代表性任务优化，转而采用更人性化的评估框架以服务实际用户需求。

Abstract: Large Language Models (LLMs) are increasingly assisting users in the real
world, yet their reliability remains a concern. Uncertainty quantification (UQ)
has been heralded as a tool to enhance human-LLM collaboration by enabling
users to know when to trust LLM predictions. We argue that current practices
for uncertainty quantification in LLMs are not optimal for developing useful UQ
for human users making decisions in real-world tasks. Through an analysis of 40
LLM UQ methods, we identify three prevalent practices hindering the community's
progress toward its goal of benefiting downstream users: 1) evaluating on
benchmarks with low ecological validity; 2) considering only epistemic
uncertainty; and 3) optimizing metrics that are not necessarily indicative of
downstream utility. For each issue, we propose concrete user-centric practices
and research directions that LLM UQ researchers should consider. Instead of
hill-climbing on unrepresentative tasks using imperfect metrics, we argue that
the community should adopt a more human-centered approach to LLM uncertainty
quantification.

</details>


### [96] [CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large Language Models](https://arxiv.org/abs/2506.07463)
*Guang Liu,Liangdong Wang,Jijie Li,Yang Yu,Yao Xu,Jiabei Chen,Yu Bai,Feng Liao,Yonghua Lin*

Main category: cs.CL

TL;DR: CCI4.0是一个大规模双语预训练数据集，包含两个子数据集，通过新颖的数据处理流程提升数据质量，显著提高LLM在数学和代码任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 为了提升大规模语言模型（LLM）的预训练数据质量，并引入多样化的人类推理轨迹，作者开发了CCI4.0数据集。

Method: CCI4.0包含两个子数据集：CCI4.0-M2-Base和CCI4.0-M2-CoT。通过两阶段去重、多分类器质量评分和领域感知流畅性过滤来确保数据质量，并提取了45亿条CoT模板。

Result: 实验表明，使用CCI4.0预训练的LLM在数学和代码任务中表现显著提升，证明了严格数据筛选和人类思维模板的重要性。

Conclusion: CCI4.0展示了高质量数据和多样化推理轨迹对提升LLM性能的关键作用，为自动处理预训练语料提供了新思路。

Abstract: We introduce CCI4.0, a large-scale bilingual pre-training dataset engineered
for superior data quality and diverse human-like reasoning trajectory. CCI4.0
occupies roughly $35$ TB of disk space and comprises two sub-datasets:
CCI4.0-M2-Base and CCI4.0-M2-CoT. CCI4.0-M2-Base combines a $5.2$ TB carefully
curated Chinese web corpus, a $22.5$ TB English subset from Nemotron-CC, and
diverse sources from math, wiki, arxiv, and code. Although these data are
mostly sourced from well-processed datasets, the quality standards of various
domains are dynamic and require extensive expert experience and labor to
process. So, we propose a novel pipeline justifying data quality mainly based
on models through two-stage deduplication, multiclassifier quality scoring, and
domain-aware fluency filtering. We extract $4.5$ billion pieces of
CoT(Chain-of-Thought) templates, named CCI4.0-M2-CoT. Differing from the
distillation of CoT from larger models, our proposed staged CoT extraction
exemplifies diverse reasoning patterns and significantly decreases the
possibility of hallucination. Empirical evaluations demonstrate that LLMs
pre-trained in CCI4.0 benefit from cleaner, more reliable training signals,
yielding consistent improvements in downstream tasks, especially in math and
code reflection tasks. Our results underscore the critical role of rigorous
data curation and human thinking templates in advancing LLM performance,
shedding some light on automatically processing pretraining corpora.

</details>


### [97] [Improving Fairness of Large Language Models in Multi-document Summarization](https://arxiv.org/abs/2506.07479)
*Haoyuan Li Yusen Zhang,Snigdha Chaturvedi*

Main category: cs.CL

TL;DR: FairPO方法通过扰动文档集生成偏好对，并动态调整偏好对权重，以在多文档摘要中同时提升摘要级和语料库级的公平性。


<details>
  <summary>Details</summary>
Motivation: 多文档摘要中的公平性对于提供跨文档的全面视图至关重要，尤其是在涉及不同社会属性值时，可能显著影响决策。现有方法主要关注摘要级公平性，而忽略了语料库级公平性。

Method: 提出FairPO方法，通过扰动文档集生成偏好对以提升摘要级公平性，并通过动态调整偏好对权重来提升语料库级公平性。

Result: 实验表明，FairPO在保持摘要关键质量的同时，优于现有基线方法。

Conclusion: FairPO方法在多文档摘要中有效提升了摘要级和语料库级的公平性，为公平性研究提供了新的方向。

Abstract: Fairness in multi-document summarization (MDS) is crucial for providing
comprehensive views across documents with diverse social attribute values,
which can significantly impact decision-making. For example, a summarization
system that tends to overrepresent negative reviews of products can mislead
customers into disregarding good products. Previous works measure fairness in
MDS at two levels: summary-level and corpus-level. While summary-level fairness
focuses on individual summaries, corpus-level fairness focuses on a corpus of
summaries. Recent methods primarily focus on summary-level fairness. We propose
FairPO, a preference tuning method that focuses on both summary-level and
corpus-level fairness in MDS. To improve summary-level fairness, we propose to
generate preference pairs by perturbing document sets. To improve corpus-level
fairness, we propose fairness-aware preference tuning by dynamically adjusting
the weights of preference pairs. Our experiments show that FairPO outperforms
strong baselines while maintaining the critical qualities of summaries. The
code is available at https://github.com/leehaoyuan/coverage_fairnes.

</details>


### [98] [A Hybrid GA LLM Framework for Structured Task Optimization](https://arxiv.org/abs/2506.07483)
*Berry Feng,Jonas Lin,Patrick Lau*

Main category: cs.CL

TL;DR: GA LLM结合遗传算法与大型语言模型，通过迭代优化生成结构化输出，适用于行程规划、学术大纲等任务。


<details>
  <summary>Details</summary>
Motivation: 为了在严格约束下生成高质量的结构化输出，结合遗传算法的全局优化能力和语言模型的领域知识与创造力。

Method: 将每个输出视为基因，通过选择、交叉和变异等进化操作，由语言模型指导迭代优化。

Result: GA LLM在行程规划、学术大纲和商业报告等任务中表现优异，生成的结果结构良好且满足需求。

Conclusion: GA LLM通过结合遗传算法和语言模型的优势，在约束满足和解决方案质量上优于单独使用语言模型。

Abstract: GA LLM is a hybrid framework that combines Genetic Algorithms with Large
Language Models to handle structured generation tasks under strict constraints.
Each output, such as a plan or report, is treated as a gene, and evolutionary
operations like selection, crossover, and mutation are guided by the language
model to iteratively improve solutions. The language model provides domain
knowledge and creative variation, while the genetic algorithm ensures
structural integrity and global optimization. GA LLM has proven effective in
tasks such as itinerary planning, academic outlining, and business reporting,
consistently producing well structured and requirement satisfying results. Its
modular design also makes it easy to adapt to new tasks. Compared to using a
language model alone, GA LLM achieves better constraint satisfaction and higher
quality solutions by combining the strengths of both components.

</details>


### [99] [DEBATE: A Dataset for Disentangling Textual Ambiguity in Mandarin Through Speech](https://arxiv.org/abs/2506.07502)
*Haotian Guo,Jing Han,Yongfeng Tu,Shihao Gao,Shengfan Shen,Wulong Xiang,Weihao Gan,Zixing Zhang*

Main category: cs.CL

TL;DR: 本文提出首个中文语音-文本消歧数据集DEBATE，通过10人录音捕捉语音特征对文本歧义的消解作用，揭示现有语音模型与人类意图理解的显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于文本和视觉消歧，而语音消歧（DTS）因缺乏高质量多模态数据集未被充分探索。DEBATE旨在填补这一空白，研究语音特征（发音、停顿等）如何揭示说话者真实语义。

Method: 构建包含1,001个中文歧义句的DEBATE数据集，每个句子由10名母语者录音，设计数据采集流程并分析质量，同时测试三大语音-语言模型的性能。

Result: 实验显示现有模型与人类在语音消歧任务上存在显著性能差距（人类准确率约90%，模型最高仅60%），证明语音特征对意图理解的关键作用未被现有技术充分捕捉。

Conclusion: DEBATE为跨语言/文化的语音消歧研究提供基准，其开源特性将推动语音理解技术发展，同时暴露当前模型在语义深层解析上的局限性。

Abstract: Despite extensive research on textual and visual disambiguation,
disambiguation through speech (DTS) remains underexplored. This is largely due
to the lack of high-quality datasets that pair spoken sentences with richly
ambiguous text. To address this gap, we present DEBATE, a unique public Chinese
speech-text dataset designed to study how speech cues and
patterns-pronunciation, pause, stress and intonation-can help resolve textual
ambiguity and reveal a speaker's true intent. DEBATE contains 1,001 carefully
selected ambiguous utterances, each recorded by 10 native speakers, capturing
diverse linguistic ambiguities and their disambiguation through speech. We
detail the data collection pipeline and provide rigorous quality analysis.
Additionally, we benchmark three state-of-the-art large speech and
audio-language models, illustrating clear and huge performance gaps between
machine and human understanding of spoken intent. DEBATE represents the first
effort of its kind and offers a foundation for building similar DTS datasets
across languages and cultures. The dataset and associated code are available
at: https://github.com/SmileHnu/DEBATE.

</details>


### [100] [What Do Indonesians Really Need from Language Technology? A Nationwide Survey](https://arxiv.org/abs/2506.07506)
*Muhammad Dehan Al Kautsar,Lucky Susanto,Derry Wijaya,Fajri Koto*

Main category: cs.CL

TL;DR: 印尼700多种本土语言的NLP开发面临高成本挑战，但社区实际需求不明。全国调查显示，解决语言障碍（如机器翻译）是首要需求，但需解决隐私与数据使用问题以促进AI应用。


<details>
  <summary>Details</summary>
Motivation: 印尼本土语言NLP技术开发成本高昂，但缺乏对语言社区真实需求的了解，需明确技术发展方向以提升资源分配效率。

Method: 通过全国性问卷调查，直接收集印尼本土语言使用者对语言技术的需求与关切。

Result: 语言障碍（机器翻译与信息检索）为最优先需求；社区高度支持技术发展，但担忧隐私、偏见及公共数据用于AI训练的伦理问题。

Conclusion: 需增强技术开发的透明度与沟通，平衡技术推进与伦理考量，才能有效推动印尼多语言AI技术的采纳与应用。

Abstract: There is an emerging effort to develop NLP for Indonesias 700+ local
languages, but progress remains costly due to the need for direct engagement
with native speakers. However, it is unclear what these language communities
truly need from language technology. To address this, we conduct a nationwide
survey to assess the actual needs of native speakers in Indonesia. Our findings
indicate that addressing language barriers, particularly through machine
translation and information retrieval, is the most critical priority. Although
there is strong enthusiasm for advancements in language technology, concerns
around privacy, bias, and the use of public data for AI training highlight the
need for greater transparency and clear communication to support broader AI
adoption.

</details>


### [101] [DeRAGEC: Denoising Named Entity Candidates with Synthetic Rationale for ASR Error Correction](https://arxiv.org/abs/2506.07510)
*Solee Im,Wonjun Lee,Jinmyeong An,Yunsu Kim,Jungseul Ok,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: DeRAGEC通过合成去噪逻辑过滤噪声命名实体候选，结合语音相似性和增强定义优化ASR系统的命名实体纠错，无需额外训练即可显著降低词错误率。


<details>
  <summary>Details</summary>
Motivation: 现有ASR系统在命名实体纠错时易受噪声候选干扰，需一种高效方法在无需额外训练的情况下提升纠错准确率。

Method: 扩展RAGEC框架，利用合成去噪逻辑过滤噪声命名实体候选，通过语音相似性和增强定义结合上下文学习优化检索结果。

Result: 在CommonVoice和STOP数据集上，词错误率相对降低28%，命名实体命中率显著优于基线ASR及RAGEC方法。

Conclusion: DeRAGEC通过去噪和上下文学习有效提升ASR系统的命名实体纠错性能，且无需训练，具有实际应用价值。

Abstract: We present DeRAGEC, a method for improving Named Entity (NE) correction in
Automatic Speech Recognition (ASR) systems. By extending the
Retrieval-Augmented Generative Error Correction (RAGEC) framework, DeRAGEC
employs synthetic denoising rationales to filter out noisy NE candidates before
correction. By leveraging phonetic similarity and augmented definitions, it
refines noisy retrieved NEs using in-context learning, requiring no additional
training. Experimental results on CommonVoice and STOP datasets show
significant improvements in Word Error Rate (WER) and NE hit ratio,
outperforming baseline ASR and RAGEC methods. Specifically, we achieved a 28%
relative reduction in WER compared to ASR without postprocessing. Our source
code is publicly available at: https://github.com/solee0022/deragec

</details>


### [102] [Towards Large Language Models with Self-Consistent Natural Language Explanations](https://arxiv.org/abs/2506.07523)
*Sahar Admoni,Ofra Amir,Assaf Hallak,Yftah Ziser*

Main category: cs.CL

TL;DR: 本文提出了一种新方法PSCB，用于评估大语言模型（LLMs）生成解释的自我一致性，并提出了新的度量标准，通过DPO微调LLMs，提高了解释与决策相关特征的一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）提供了易于解释的途径，但研究表明，这些事后解释往往与真实决策过程不符。由于评估特征重要性的高成本，缺乏系统性解决方案。

Method: 引入了Post-hoc Self-Consistency Bank (PSCB)，一个包含多样化任务和模型决策的大规模基准，每个决策都配有LLM生成的解释和相应的特征重要性评分。提出了新的度量标准，并通过Direct Preference Optimization (DPO)微调LLMs。

Result: 分析PSCB发现，正确和错误预测的自我一致性评分几乎没有差异。新提出的度量标准能更有效地捕捉解释质量的变化，通过DPO微调LLMs显著提高了解释与决策相关特征的一致性。

Conclusion: 本文的研究为构建更可信、自我一致的大语言模型提供了一条可扩展的路径。

Abstract: Large language models (LLMs) seem to offer an easy path to interpretability:
just ask them to explain their decisions. Yet, studies show that these post-hoc
explanations often misrepresent the true decision process, as revealed by
mismatches in feature importance. Despite growing evidence of this
inconsistency, no systematic solutions have emerged, partly due to the high
cost of estimating feature importance, which limits evaluations to small
datasets. To address this, we introduce the Post-hoc Self-Consistency Bank
(PSCB) - a large-scale benchmark of decisions spanning diverse tasks and
models, each paired with LLM-generated explanations and corresponding feature
importance scores. Analysis of PSCB reveals that self-consistency scores barely
differ between correct and incorrect predictions. We also show that the
standard metric fails to meaningfully distinguish between explanations. To
overcome this limitation, we propose an alternative metric that more
effectively captures variation in explanation quality. We use it to fine-tune
LLMs via Direct Preference Optimization (DPO), leading to significantly better
alignment between explanations and decision-relevant features, even under
domain shift. Our findings point to a scalable path toward more trustworthy,
self-consistent LLMs.

</details>


### [103] [Bit-level BPE: Below the byte boundary](https://arxiv.org/abs/2506.07541)
*Sangwhan Moon,Tatsuya Hiraoka,Naoaki Okazaki*

Main category: cs.CL

TL;DR: 本文提出一种无损压缩技术，解决子词分词中字节级回退导致CJK等字符序列过长的问题，减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有字节级回退方法在处理CJK字符和表情符号时，将字符拆分为字节导致序列长度显著增加，影响训练和推理效率。

Method: 提出一种简单的无损压缩技术，通过减少长尾字符的字节级表示来缩短序列长度。

Result: 该方法有效降低了序列长度，从而减少计算时间，同时保持信息无损。

Conclusion: 所提压缩技术为多语言场景提供高效解决方案，平衡了OOV处理与计算效率。

Abstract: Byte-level fallbacks for subword tokenization have become a common practice
in large language models. In particular, it has been demonstrated to be
incredibly effective as a pragmatic solution for preventing OOV, especially in
the context of larger models. However, breaking a character down to individual
bytes significantly increases the sequence length for long-tail tokens in
languages such as Chinese, Japanese, and Korean (CJK) and other
character-diverse contexts such as emoji. The increased sequence length results
in longer computation during both training and inference. In this work, we
propose a simple compression technique that reduces the sequence length
losslessly.

</details>


### [104] [SELT: Self-Evaluation Tree Search for LLMs with Task Decomposition](https://arxiv.org/abs/2506.07557)
*Mengsong Wu,Di Zhang,Yuqiang Li,Dongzhan Zhou,Wenliang Chen*

Main category: cs.CL

TL;DR: SELT提出了一种基于改进蒙特卡洛树搜索的框架，通过自评估机制和语义聚类增强大语言模型的复杂推理能力，无需依赖外部奖励模型，在多个基准测试中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂推理任务中表现下降，且传统方法依赖外部奖励模型。研究旨在通过自评估机制和树搜索策略自主优化推理路径，减少冗余和幻觉问题。

Method: 结合改进的蒙特卡洛树搜索（MCTS），重新设计置信上限评分以适配LLM自评估能力，将推理分解为原子子任务，并通过语义聚类压缩冗余路径，实现探索与利用的平衡。

Result: 在MMLU知识推理和Seal-Tools工具学习数据集上，SELT相比基线方法显著提高了答案准确率与推理鲁棒性，且无需任务微调即展现跨领域泛化能力。

Conclusion: SELT通过自主树搜索与语义聚类有效提升LLM复杂推理性能，验证了自评估机制与任务分解策略的通用性，为无监督推理优化提供了新方向。

Abstract: While Large Language Models (LLMs) have achieved remarkable success in a wide
range of applications, their performance often degrades in complex reasoning
tasks. In this work, we introduce SELT (Self-Evaluation LLM Tree Search), a
novel framework that leverages a modified Monte Carlo Tree Search (MCTS) to
enhance LLM reasoning without relying on external reward models. By redefining
the Upper Confidence Bound scoring to align with intrinsic self-evaluation
capabilities of LLMs and decomposing the inference process into atomic subtasks
augmented with semantic clustering at each node, SELT effectively balances
exploration and exploitation, reduces redundant reasoning paths, and mitigates
hallucination. We validate our approach on challenging benchmarks, including
the knowledge-based MMLU and the Tool Learning dataset Seal-Tools, where SELT
achieves significant improvements in answer accuracy and reasoning robustness
compared to baseline methods. Notably, our framework operates without
task-specific fine-tuning, demonstrating strong generalizability across diverse
reasoning tasks. Relevant results and code are available at
https://github.com/fairyshine/SELT .

</details>


### [105] [Beyond the Sentence: A Survey on Context-Aware Machine Translation with Large Language Models](https://arxiv.org/abs/2506.07583)
*Ramakrishna Appicharla,Baban Gain,Santanu Pal,Asif Ekbal*

Main category: cs.CL

TL;DR: 本文综述了大型语言模型（LLMs）在上下文感知机器翻译中的应用，发现商业LLMs表现优于开源LLMs，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）广受欢迎，但它们在上下文感知机器翻译中的应用尚未得到充分探索。

Method: 本文通过文献综述，分析了现有工作中使用的提示和微调方法，并探讨了自动后编辑和翻译代理的创建。

Result: 商业LLMs（如ChatGPT和Tower LLM）在翻译质量上优于开源LLMs（如Llama和Bloom LLMs），提示方法可作为评估翻译质量的基准。

Conclusion: 本文总结了LLMs在上下文感知机器翻译中的现状，并提出了未来研究的有趣方向。

Abstract: Despite the popularity of the large language models (LLMs), their application
to machine translation is relatively underexplored, especially in context-aware
settings. This work presents a literature review of context-aware translation
with LLMs. The existing works utilise prompting and fine-tuning approaches,
with few focusing on automatic post-editing and creating translation agents for
context-aware machine translation. We observed that the commercial LLMs (such
as ChatGPT and Tower LLM) achieved better results than the open-source LLMs
(such as Llama and Bloom LLMs), and prompt-based approaches serve as good
baselines to assess the quality of translations. Finally, we present some
interesting future directions to explore.

</details>


### [106] [Instructing Large Language Models for Low-Resource Languages: A Systematic Study for Basque](https://arxiv.org/abs/2506.07597)
*Oscar Sainz,Naiara Perez,Julen Etxaniz,Joseba Fernandez de Landa,Itziar Aldabe,Iker García-Ferrero,Aimar Zabala,Ekhi Azurmendi,German Rigau,Eneko Agirre,Mikel Artetxe,Aitor Soroa*

Main category: cs.CL

TL;DR: 本文探讨了在低资源语言场景下，使用目标语言语料库、多语言基础模型和合成指令来训练语言模型的方法，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在低资源语言场景下，缺乏大规模指令数据集，本文旨在探索替代传统指令适应流程的方法。

Method: 假设在低资源语言场景下，仅有的资源包括目标语言语料库、现有的多语言基础模型和从指令模型中采样的合成指令。本文通过系统实验研究了这些组件的不同组合。

Result: 实验结果表明，目标语言语料库至关重要，合成指令能生成鲁棒的模型，且使用指令调优的模型作为骨干优于非指令调优的模型，且随着规模扩大效果更佳。

Conclusion: 使用Llama 3.1 instruct 70B作为骨干模型，本文的模型在巴斯克语上接近更大规模的前沿模型，且仅使用了1.2B词的语料库。

Abstract: Instructing language models with user intent requires large instruction
datasets, which are only available for a limited set of languages. In this
paper, we explore alternatives to conventional instruction adaptation pipelines
in low-resource scenarios. We assume a realistic scenario for low-resource
languages, where only the following are available: corpora in the target
language, existing open-weight multilingual base and instructed backbone LLMs,
and synthetically generated instructions sampled from the instructed backbone.
We present a comprehensive set of experiments for Basque that systematically
study different combinations of these components evaluated on benchmarks and
human preferences from 1,680 participants. Our conclusions show that target
language corpora are essential, with synthetic instructions yielding robust
models, and, most importantly, that using as backbone an instruction-tuned
model outperforms using a base non-instructed model, and improved results when
scaling up. Using Llama 3.1 instruct 70B as backbone our model comes near
frontier models of much larger sizes for Basque, without using any Basque data
apart from the 1.2B word corpora. We release code, models, instruction
datasets, and human preferences to support full reproducibility in future
research on low-resource language adaptation.

</details>


### [107] [PolitiSky24: U.S. Political Bluesky Dataset with User Stance Labels](https://arxiv.org/abs/2506.07606)
*Peyman Rostami,Vahid Rahimzadeh,Ali Adibi,Azadeh Shakery*

Main category: cs.CL

TL;DR: 本文介绍了首个针对2024年美国总统选举的立场检测数据集PolitiSky24，该数据集从Bluesky平台收集，聚焦于Kamala Harris和Donald Trump，包含16,044个用户-目标立场对，并采用先进的信息检索和大语言模型进行标注，准确率达81%。


<details>
  <summary>Details</summary>
Motivation: 现有立场检测数据集主要关注推文级别的立场，而用户级别的立场资源，尤其是在新兴平台如Bluesky上，仍然稀缺。用户级别的立场检测通过考虑用户的完整发帖历史，提供了更全面的视角。

Method: 使用结合先进信息检索和大语言模型的管道，生成带有支持理由和文本跨度的立场标签，确保透明性。

Result: PolitiSky24数据集包含16,044个用户-目标立场对，标注准确率达到81%。

Conclusion: PolitiSky24数据集通过其及时性、开放数据性质和用户级别视角，填补了政治立场分析中的空白，并已公开提供。

Abstract: Stance detection identifies the viewpoint expressed in text toward a specific
target, such as a political figure. While previous datasets have focused
primarily on tweet-level stances from established platforms, user-level stance
resources, especially on emerging platforms like Bluesky remain scarce.
User-level stance detection provides a more holistic view by considering a
user's complete posting history rather than isolated posts. We present the
first stance detection dataset for the 2024 U.S. presidential election,
collected from Bluesky and centered on Kamala Harris and Donald Trump. The
dataset comprises 16,044 user-target stance pairs enriched with engagement
metadata, interaction graphs, and user posting histories. PolitiSky24 was
created using a carefully evaluated pipeline combining advanced information
retrieval and large language models, which generates stance labels with
supporting rationales and text spans for transparency. The labeling approach
achieves 81\% accuracy with scalable LLMs. This resource addresses gaps in
political stance analysis through its timeliness, open-data nature, and
user-level perspective. The dataset is available at
https://doi.org/10.5281/zenodo.15616911

</details>


### [108] [Vuyko Mistral: Adapting LLMs for Low-Resource Dialectal Translation](https://arxiv.org/abs/2506.07617)
*Roman Kyslyi,Yuliia Maksymiuk,Ihor Pysmennyi*

Main category: cs.CL

TL;DR: 本文首次尝试将大语言模型（LLMs）应用于乌克兰方言（Hutsul），通过构建平行语料库和词典，并采用RAG管道生成合成翻译对，最终微调多个开源LLMs，结果显示微调后的小模型在翻译任务上优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决乌克兰Hutsul方言这一低资源且形态复杂的方言在自然语言处理中的挑战，特别是数据短缺问题。

Method: 作者构建了9852对Hutsul方言与标准乌克兰语的平行句子对和7320个方言词汇映射词典，并采用RAG管道生成52142个合成翻译对，随后使用LoRA微调多个开源LLMs，并通过多指标评估模型性能。

Result: 实验结果表明，即使是7B参数的微调模型在自动评估和LLM评估指标上均优于GPT-4o的零样本基线。

Conclusion: 本文成功展示了微调小模型在低资源方言翻译任务中的有效性，并为Hutsul方言的自然语言处理提供了公开的数据、模型和代码资源。

Abstract: In this paper we introduce the first effort to adapt large language models
(LLMs) to the Ukrainian dialect (in our case Hutsul), a low-resource and
morphologically complex dialect spoken in the Carpathian Highlands. We created
a parallel corpus of 9852 dialect-to-standard Ukrainian sentence pairs and a
dictionary of 7320 dialectal word mappings. We also addressed data shortage by
proposing an advanced Retrieval-Augmented Generation (RAG) pipeline to generate
synthetic parallel translation pairs, expanding the corpus with 52142 examples.
We have fine-tuned multiple open-source LLMs using LoRA and evaluated them on a
standard-to-dialect translation task, also comparing with few-shot GPT-4o
translation. In the absence of human annotators, we adopt a multi-metric
evaluation strategy combining BLEU, chrF++, TER, and LLM-based judgment
(GPT-4o). The results show that even small(7B) finetuned models outperform
zero-shot baselines such as GPT-4o across both automatic and LLM-evaluated
metrics. All data, models, and code are publicly released at:
https://github.com/woters/vuyko-hutsul

</details>


### [109] [LoRMA: Low-Rank Multiplicative Adaptation for LLMs](https://arxiv.org/abs/2506.07621)
*Harsh Bihany,Shubham Patel,Ashutosh Modi*

Main category: cs.CL

TL;DR: 本文提出了一种名为LoRMA的低秩乘性自适应方法，通过将传统加性更新转换为矩阵乘法变换，解决了计算复杂性和秩瓶颈问题，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 全微调大型语言模型计算成本高昂，现有低秩自适应方法（如LoRA）采用加性更新策略，可能限制其表达能力和效率。

Method: 通过矩阵乘法变换替代加性更新，采用操作重排序和秩膨胀策略降低计算复杂度并突破秩限制，构建低秩乘性自适应框架（LoRMA）。

Result: 实验表明LoRMA在多种评估指标下均表现优异，验证了乘性更新策略在参数效率与模型性能间的平衡能力。

Conclusion: LoRMA通过矩阵乘法范式突破传统低秩自适应方法的局限性，为高效参数微调提供了新的技术路径，同时保持模型适应下游任务的能力。

Abstract: Large Language Models have shown remarkable capabilities in the NLP domain.
Their effectiveness can mainly be attributed to their ability to adapt to an
array of downstream tasks. However, generally, full fine-tuning is a
computationally expensive job. To mitigate this, many techniques have been
developed that prime efficiency, a prominent one being Low-Rank Adaptation
(LoRA). However, LoRA and its variants employ re-parametrized additive updates.
In this paper, we propose Low-Rank Multiplicative Adaptation (LoRMA), which
shifts the paradigm of additive updates to a richer space of matrix
multiplicative transformations. We tackle challenges such as computational
complexity and rank bottleneck of matrix multiplication by effectively
re-ordering operations and introducing rank inflation strategies. We conduct
extensive experiments to demonstrate the effectiveness of our approach in terms
of various evaluation metrics.

</details>


### [110] [Intent Matters: Enhancing AI Tutoring with Fine-Grained Pedagogical Intent Annotation](https://arxiv.org/abs/2506.07626)
*Kseniia Petukhova,Ekaterina Kochmar*

Main category: cs.CL

TL;DR: 通过细粒度标注教师教学意图，微调后的LLM能生成更符合教学策略的辅导回答，提升教育场景下的生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型(LLM)在教育应用中缺乏与教学策略的对齐，需通过任务特定适应提升其辅导效果。

Method: 基于MathDial数学辅导对话数据集，使用包含11种教学意图的细粒度分类体系重新标注数据，并微调LLM模型。

Result: 自动评估与定性分析均表明，细粒度意图模型比原始四分类模型生成的教学响应更有效且更符合教学策略。

Conclusion: 教学意图的细粒度标注对教育文本生成具有重要价值，研究公开了标注数据与代码以支持后续研究。

Abstract: Large language models (LLMs) hold great promise for educational applications,
particularly in intelligent tutoring systems. However, effective tutoring
requires alignment with pedagogical strategies - something current LLMs lack
without task-specific adaptation. In this work, we explore whether fine-grained
annotation of teacher intents can improve the quality of LLM-generated tutoring
responses. We focus on MathDial, a dialog dataset for math instruction, and
apply an automated annotation framework to re-annotate a portion of the dataset
using a detailed taxonomy of eleven pedagogical intents. We then fine-tune an
LLM using these new annotations and compare its performance to models trained
on the original four-category taxonomy. Both automatic and qualitative
evaluations show that the fine-grained model produces more pedagogically
aligned and effective responses. Our findings highlight the value of intent
specificity for controlled text generation in educational settings, and we
release our annotated data and code to facilitate further research.

</details>


### [111] [Unblocking Fine-Grained Evaluation of Detailed Captions: An Explaining AutoRater and Critic-and-Revise Pipeline](https://arxiv.org/abs/2506.07631)
*Brian Gordon,Yonatan Bitton,Andreea Marzoca,Yasumasa Onoe,Xiao Wang,Daniel Cohen-Or,Idan Szpektor*

Main category: cs.CL

TL;DR: 本文提出了DOCCI-Critique基准和VNLI-Critique模型，用于细粒度评估视觉语言模型（VLM）生成段落描述的事实准确性，并通过自动评分、模型排名及优化流程显著提升事实性检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以检测VLM生成段落描述中的细粒度事实错误，主要因缺乏针对长文本的评估工具和带人工验证的数据集。

Method: 构建包含10,216个句子级人工标注的DOCCI-Critique基准，并开发VNLI-Critique模型进行自动事实分类与解释；提出基于模型反馈的自动评分和修正流程。

Result: VNLI-Critique在M-HalDetect和CHOCOLATE任务中表现优异（如0.98 Spearman相关性），修正流程使DetailCaps-4870的事实性提升46%。

Conclusion: 研究提供了细粒度评估基准与工具链，显著提升VLM生成内容的事实性检测与优化能力，推动视觉语言理解技术进步。

Abstract: Large Vision-Language Models (VLMs) now generate highly detailed,
paragraphlength image captions, yet evaluating their factual accuracy remains
challenging. Current methods often miss fine-grained errors, being designed for
shorter texts or lacking datasets with verified inaccuracies. We introduce
DOCCI-Critique, a benchmark with 1,400 VLM-generated paragraph captions (100
images, 14 VLMs) featuring over 10,216 sentence-level human annotations of
factual correctness and explanatory rationales for errors, all within paragraph
context. Building on this, we develop VNLI-Critique, a model for automated
sentence-level factuality classification and critique generation. We highlight
three key applications: (1) VNLI-Critique demonstrates robust generalization,
validated by state-of-the-art performance on the M-HalDetect benchmark and
strong results in CHOCOLATE claim verification. (2) The VNLI-Critique driven
AutoRater for DOCCI-Critique provides reliable VLM rankings, showing excellent
alignment with human factuality judgments (e.g., 0.98 Spearman). (3) An
innovative Critic-and-Revise pipeline, where critiques from VNLI-Critique guide
LLM-based corrections, achieves substantial improvements in caption factuality
(e.g., a 46% gain on DetailCaps-4870). Our work offers a crucial benchmark
alongside practical tools, designed to significantly elevate the standards for
fine-grained evaluation and foster the improvement of VLM image understanding.
Project page: https://google.github.io/unblocking-detail-caption

</details>


### [112] [TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient LLM-based Scientific Peer Review](https://arxiv.org/abs/2506.07642)
*Yuan Chang,Ziyue Li,Hengyuan Zhang,Yuanbo Kong,Yanru Wu,Zhijiang Guo,Ngai Wong*

Main category: cs.CL

TL;DR: 本文提出TreeReview框架，通过分层双向问答建模论文评审，结合动态问题扩展机制，在生成全面深入的评审反馈同时减少80%的LLM计算量。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在辅助论文评审时难以兼顾评审深度与效率，需解决生成全面见解与计算资源消耗之间的矛盾。

Method: 构建分层问题树递归分解评审问题，通过叶到根的答案聚合生成最终评审，并引入动态问题扩展机制实现深度追问。

Result: 在ICLR/NeurIPS基准测试中，TreeReview生成的评审在全面性和专业性上优于基线方法，且LLM计算量减少80%。

Conclusion: TreeReview通过结构化问答机制实现了高效优质的自动化论文评审，为LLM在学术评估中的应用提供了新范式。

Abstract: While Large Language Models (LLMs) have shown significant potential in
assisting peer review, current methods often struggle to generate thorough and
insightful reviews while maintaining efficiency. In this paper, we propose
TreeReview, a novel framework that models paper review as a hierarchical and
bidirectional question-answering process. TreeReview first constructs a tree of
review questions by recursively decomposing high-level questions into
fine-grained sub-questions and then resolves the question tree by iteratively
aggregating answers from leaf to root to get the final review. Crucially, we
incorporate a dynamic question expansion mechanism to enable deeper probing by
generating follow-up questions when needed. We construct a benchmark derived
from ICLR and NeurIPS venues to evaluate our method on full review generation
and actionable feedback comments generation tasks. Experimental results of both
LLM-based and human evaluation show that TreeReview outperforms strong
baselines in providing comprehensive, in-depth, and expert-aligned review
feedback, while reducing LLM token usage by up to 80% compared to
computationally intensive approaches. Our code and benchmark dataset are
available at https://github.com/YuanChang98/tree-review.

</details>


### [113] [Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models](https://arxiv.org/abs/2506.07645)
*Maciej Chrabąszcz,Katarzyna Lorenc,Karolina Seweryn*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型（LLMs）在低资源语言（如波兰语）中易受字符和词级攻击，仅需少量字符修改和小型代理模型即可绕过其安全机制。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs的安全训练数据集中于高资源语言（如英语），导致低资源语言存在潜在漏洞，需验证其抗干扰能力。

Method: 通过修改少量字符，并利用小型代理模型计算词重要性，构建低成本攻击方法，并在波兰语中验证后扩展至其他语言。

Result: 攻击显著改变了不同LLMs的预测结果，证实低资源语言场景下模型安全机制易被绕过。

Conclusion: LLMs在多语言环境中的安全需重视，低资源语言漏洞可能被低成本攻击利用。研究公开数据集与代码以促进后续安全研究。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
various natural language processing (NLP) tasks in recent years. However, their
susceptibility to jailbreaks and perturbations necessitates additional
evaluations. Many LLMs are multilingual, but safety-related training data
contains mainly high-resource languages like English. This can leave them
vulnerable to perturbations in low-resource languages such as Polish. We show
how surprisingly strong attacks can be cheaply created by altering just a few
characters and using a small proxy model for word importance calculation. We
find that these character and word-level attacks drastically alter the
predictions of different LLMs, suggesting a potential vulnerability that can be
used to circumvent their internal safety mechanisms. We validate our attack
construction methodology on Polish, a low-resource language, and find potential
vulnerabilities of LLMs in this language. Additionally, we show how it can be
extended to other languages. We release the created datasets and code for
further research.

</details>


### [114] [Transcript-Prompted Whisper with Dictionary-Enhanced Decoding for Japanese Speech Annotation](https://arxiv.org/abs/2506.07646)
*Rui Hu,Xiaolong Lin,Jiawang Liu,Shixi Huang,Zhenpeng Zhan*

Main category: cs.CL

TL;DR: 本文提出了一种基于预训练ASR模型的音素和韵律标注方法，用于构建日语TTS数据集，并通过解码策略优化标注准确性。


<details>
  <summary>Details</summary>
Motivation: 为了构建高质量的日语TTS数据集，需要一种能够自动且准确地标注音素和韵律的方法。

Method: 通过微调预训练的ASR模型，结合字典先验知识进行解码，以同时输出字素和标注标签。

Result: 客观评估显示该方法优于仅依赖文本或音频的先前方法，主观评估表明其标注的TTS模型自然度与人工标注相当。

Conclusion: 所提出的方法在音素和韵律标注上表现出色，能够有效支持日语TTS数据集的构建。

Abstract: In this paper, we propose a method for annotating phonemic and prosodic
labels on a given audio-transcript pair, aimed at constructing Japanese
text-to-speech (TTS) datasets. Our approach involves fine-tuning a large-scale
pre-trained automatic speech recognition (ASR) model, conditioned on ground
truth transcripts, to simultaneously output phrase-level graphemes and
annotation labels. To further correct errors in phonemic labeling, we employ a
decoding strategy that utilizes dictionary prior knowledge. The objective
evaluation results demonstrate that our proposed method outperforms previous
approaches relying solely on text or audio. The subjective evaluation results
indicate that the naturalness of speech synthesized by the TTS model, trained
with labels annotated using our method, is comparable to that of a model
trained with manual annotations.

</details>


### [115] [Beyond Benchmarks: A Novel Framework for Domain-Specific LLM Evaluation and Knowledge Mapping](https://arxiv.org/abs/2506.07658)
*Nitin Sharma,Thomas Wolfers,Çağatay Yıldız*

Main category: cs.CL

TL;DR: 本文提出一种无需人工干预的确定性流程，将领域语料转化为评测基准，解决领域特定模型评估的可靠性问题，并揭示领域适应中知识表征的层级机制，为高效微调提供新方法。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型评估中领域特定基准构建的可靠性问题，并探究领域适应过程中知识表征的动态变化。

Method: 使用TF和Term TF-IDF方法生成领域关键词及关联词表，构建提示-目标对，通过模型对提示的补全准确性直接评估领域知识，避免传统困惑度指标的局限性。

Result: 新基准与专家基准强相关且更准确；小模型在500步内快速适应；初始-中间层负责属性提取，后续层专注预测；遗忘始于中间层并在后续层放大。

Conclusion: 提供领域模型评估的实用方法，揭示知识表征的层级机制，为减少灾难性遗忘和优化微调策略提供理论依据。

Abstract: The paper addresses two critical challenges in language model (LM)
evaluation: creating reliable domain-specific benchmarks and understanding
knowledge representation during domain adaptation. We introduce a deterministic
pipeline that converts raw domain corpora into completion-type benchmarks
without relying on LMs or human curation, eliminating benchmark contamination
issues while enabling evaluation on the latest domain data. Our approach
generates domain-specific keywords and related word lists using TF and Term
TF-IDF methods and constructs prompt-target pairs. We evaluate models by
measuring their ability to complete these prompts with the correct
domain-specific targets, providing a direct assessment of domain knowledge with
low computational cost. Through comprehensive experiments across multiple
models (GPT-2 medium/XL, Llama-2/3.1, OLMo-2, Qwen-2, Mistral) and domains, we
demonstrate that our benchmark strongly correlates with expert-generated
benchmarks while providing a more accurate measure of domain knowledge than
traditional perplexity metrics. We reveal that domain adaptation happens
rapidly in smaller models (within 500 steps) and illustrate a new approach to
domain knowledge evaluation in base models during training for early stopping.
By extending mechanistic analysis to domain adaptation, we discover that
initial-to-mid layers are primarily responsible for attribute extraction, while
later layers focus on next token prediction. Furthermore, we show that during
adaptation, forgetting begins in the middle layers, where attribute extraction
happens and is amplified in later layers. Our work provides both a practical
evaluation methodology for domain-specific LMs and novel insights into
knowledge representation during adaptation, with implications for more
efficient fine-tuning strategies and targeted approaches to mitigate
catastrophic forgetting.

</details>


### [116] [Synthesis by Design: Controlled Data Generation via Structural Guidance](https://arxiv.org/abs/2506.07664)
*Lei Xu,Sirui Chen,Yuxuan Huang,Chaochao Lu*

Main category: cs.CL

TL;DR: 本文提出了一种通过生成问题解决代码提取结构信息并指导数据生成的方法，以提升LLM在数学推理中的表现，并生成了包含39K问题和6.1K高难度问题的数据集。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过问题重述生成数据集来增强LLM的推理能力，但在生成质量和问题复杂性方面存在问题。

Method: 提出从数学推理中提取结构信息，并生成问题解决代码，以结构化解决方案指导数据生成。

Result: 在MATH和GSM8K数据集上生成了39K问题和6.1K高难度问题，实验表明模型性能随推理长度增加而下降，微调实验验证了数据集的有效性。

Conclusion: 本文提出的方法和数据集有望为未来提升LLM推理能力的研究做出贡献。

Abstract: Mathematical reasoning remains challenging for LLMs due to complex logic and
the need for precise computation. Existing methods enhance LLM reasoning by
synthesizing datasets through problem rephrasing, but face issues with
generation quality and problem complexity. To address this, we propose to
extract structural information with generated problem-solving code from
mathematical reasoning and guide data generation with structured solutions.
Applied to MATH and GSM8K, our approach produces 39K problems with labeled
intermediate steps and a 6.1K-problem benchmark of higher difficulty. Results
on our benchmark show that model performance declines as reasoning length
increases. Additionally, we conducted fine-tuning experiments using the
proposed training data on a range of LLMs, and the results validate the
effectiveness of our dataset. We hope the proposed method and dataset will
contribute to future research in enhancing LLM reasoning capabilities.

</details>


### [117] [Silencing Empowerment, Allowing Bigotry: Auditing the Moderation of Hate Speech on Twitch](https://arxiv.org/abs/2506.07667)
*Prarabdh Shukla,Wei Yin Chong,Yash Patel,Brennan Schaffner,Danish Pruthi,Arjun Bhagoji*

Main category: cs.CL

TL;DR: 审计发现Twitch的AutoMod工具在仇恨内容过滤上存在高漏检率(94%)和误封率(89.5%)，过度依赖敏感词而忽视上下文理解。


<details>
  <summary>Details</summary>
Motivation: 实时互动内容(如直播评论)对审核系统提出更高延迟要求，但主流平台自动化审核系统的有效性缺乏实证研究，特别是Twitch的AutoMod工具。

Method: 通过创建隔离测试账户，利用Twitch API发送107,000条来自4个数据集的评论(含明显仇恨内容)，测试AutoMod对厌女/种族歧视/残障歧视/恐同内容的识别能力。

Result: 1) 部分数据集94%仇恨内容未被拦截 2) 添加敏感词后100%被删除，显示系统依赖敏感词检测 3) 89.5%教学/赋权场景的良性内容被误封，违反平台指南。

Conclusion: AutoMod存在重大缺陷：过度依赖敏感词导致漏检与误封并存，突显上下文理解能力不足，需改进语义分析技术以平衡审核效果。

Abstract: To meet the demands of content moderation, online platforms have resorted to
automated systems. Newer forms of real-time engagement($\textit{e.g.}$, users
commenting on live streams) on platforms like Twitch exert additional pressures
on the latency expected of such moderation systems. Despite their prevalence,
relatively little is known about the effectiveness of these systems. In this
paper, we conduct an audit of Twitch's automated moderation tool
($\texttt{AutoMod}$) to investigate its effectiveness in flagging hateful
content. For our audit, we create streaming accounts to act as siloed test
beds, and interface with the live chat using Twitch's APIs to send over
$107,000$ comments collated from $4$ datasets. We measure $\texttt{AutoMod}$'s
accuracy in flagging blatantly hateful content containing misogyny, racism,
ableism and homophobia. Our experiments reveal that a large fraction of hateful
messages, up to $94\%$ on some datasets, $\textit{bypass moderation}$.
Contextual addition of slurs to these messages results in $100\%$ removal,
revealing $\texttt{AutoMod}$'s reliance on slurs as a moderation signal. We
also find that contrary to Twitch's community guidelines, $\texttt{AutoMod}$
blocks up to $89.5\%$ of benign examples that use sensitive words in
pedagogical or empowering contexts. Overall, our audit points to large gaps in
$\texttt{AutoMod}$'s capabilities and underscores the importance for such
systems to understand context effectively.

</details>


### [118] [GaRAGe: A Benchmark with Grounding Annotations for RAG Evaluation](https://arxiv.org/abs/2506.07671)
*Ionut-Teodor Sorodoc,Leonardo F. R. Ribeiro,Rexhina Blloshmi,Christopher Davis,Adrià de Gispert*

Main category: cs.CL

TL;DR: GaRAGe是一个大型RAG基准，包含2366个问题，超过35K标注段落，用于评估LLM在生成RAG答案时识别相关信息的能力。


<details>
  <summary>Details</summary>
Motivation: 为了评估LLM在生成RAG答案时是否能够准确识别相关基础信息，并提供一个细粒度的评估标准。

Method: 构建了一个包含2366个问题和超过35K标注段落的基准，涵盖多种复杂性、动态性和主题，以反映真实世界的RAG使用场景。

Result: 评估显示，多个最先进的LLM倾向于过度总结，而非严格基于相关段落生成答案，且在缺乏相关信息时，偏转回答的准确率较低。

Conclusion: GaRAGe基准揭示了LLM在生成RAG答案时的局限性，特别是在处理时间敏感问题和稀疏私有基础信息时表现较差。

Abstract: We present GaRAGe, a large RAG benchmark with human-curated long-form answers
and annotations of each grounding passage, allowing a fine-grained evaluation
of whether LLMs can identify relevant grounding when generating RAG answers.
Our benchmark contains 2366 questions of diverse complexity, dynamism, and
topics, and includes over 35K annotated passages retrieved from both private
document sets and the Web, to reflect real-world RAG use cases. This makes it
an ideal test bed to evaluate an LLM's ability to identify only the relevant
information necessary to compose a response, or provide a deflective response
when there is insufficient information. Evaluations of multiple
state-of-the-art LLMs on GaRAGe show that the models tend to over-summarise
rather than (a) ground their answers strictly on the annotated relevant
passages (reaching at most a Relevance-Aware Factuality Score of 60%), or (b)
deflect when no relevant grounding is available (reaching at most 31% true
positive rate in deflections). The F1 in attribution to relevant sources is at
most 58.9%, and we show that performance is particularly reduced when answering
time-sensitive questions and when having to draw knowledge from sparser private
grounding sources.

</details>


### [119] [Training Superior Sparse Autoencoders for Instruct Models](https://arxiv.org/abs/2506.07691)
*Jiaming Li,Haoran Ye,Yukun Chen,Xinyue Li,Lei Zhang,Hamid Alinejad-Rokny,Jimmy Chih-Hsien Peng,Min Yang*

Main category: cs.CL

TL;DR: 本文提出了一种针对指令模型优化的稀疏自编码器训练方法FAST，显著提升了重建质量与特征可解释性，并发现通过干预特殊标记激活可改进模型输出控制。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器方法主要针对基础模型设计，在指令模型上存在重建质量下降和特征可解释性不足的问题，需开发适配指令模型特性的训练方法。

Method: FAST方法通过对齐指令模型的数据分布与激活模式，设计序列化训练策略，优化自编码器在指令模型上的适应性训练过程。

Result: 在Qwen2.5-7B-Instruct上实现0.6468的MSE（基线为5.1985/1.5096），Llama3.2-3B-Instruct的高质量特征比例达21.1%（基线为7.0%/10.2%）。

Conclusion: FAST验证了针对指令模型定制训练方法的有效性，揭示了通过特殊标记激活干预实现细粒度模型行为控制的新可能性。

Abstract: As large language models (LLMs) grow in scale and capability, understanding
their internal mechanisms becomes increasingly critical. Sparse autoencoders
(SAEs) have emerged as a key tool in mechanistic interpretability, enabling the
extraction of human-interpretable features from LLMs. However, existing SAE
training methods are primarily designed for base models, resulting in reduced
reconstruction quality and interpretability when applied to instruct models. To
bridge this gap, we propose
$\underline{\textbf{F}}$inetuning-$\underline{\textbf{a}}$ligned
$\underline{\textbf{S}}$equential $\underline{\textbf{T}}$raining
($\textit{FAST}$), a novel training method specifically tailored for instruct
models. $\textit{FAST}$ aligns the training process with the data distribution
and activation patterns characteristic of instruct models, resulting in
substantial improvements in both reconstruction and feature interpretability.
On Qwen2.5-7B-Instruct, $\textit{FAST}$ achieves a mean squared error of 0.6468
in token reconstruction, significantly outperforming baseline methods with
errors of 5.1985 and 1.5096. In feature interpretability, $\textit{FAST}$
yields a higher proportion of high-quality features, for Llama3.2-3B-Instruct,
$21.1\%$ scored in the top range, compared to $7.0\%$ and $10.2\%$ for
$\textit{BT(P)}$ and $\textit{BT(F)}$. Surprisingly, we discover that
intervening on the activations of special tokens via the SAEs leads to
improvements in output quality, suggesting new opportunities for fine-grained
control of model behavior. Code, data, and 240 trained SAEs are available at
https://github.com/Geaming2002/FAST.

</details>


### [120] [Through the Valley: Path to Effective Long CoT Training for Small Language Models](https://arxiv.org/abs/2506.07712)
*Renjie Luo,Jiaxi Li,Chen Huang,Wei Lu*

Main category: cs.CL

TL;DR: 研究发现小语言模型（≤3B参数）在有限长思维链（CoT）数据训练下会出现性能显著下降的「长CoT退化」现象，错误累积是主因。长CoT虽能提升多步推理能力，但会放大错误风险，建议通过充分监督微调缓解下游强化学习影响。


<details>
  <summary>Details</summary>
Motivation: 探究长思维链监督对小模型的适用性，揭示其训练过程中出现的性能退化现象及其对下游任务的影响。

Method: 基于Qwen2.5/LLaMA3/Gemma3系列模型进行实验，分析错误累积机制，并测试不同规模监督微调对强化学习的影响。

Result: 小模型仅用8k长CoT样本训练后性能下降75%，部分模型即使使用220k样本仍无法恢复原始性能。错误累积效应显著，但充分监督微调可缓解强化学习负面影响。

Conclusion: 挑战长CoT训练必然有益小模型的假设，提出错误累积机制解释性能退化，建议谨慎使用长CoT并配合充分监督微调构建高效小规模推理模型。

Abstract: Long chain-of-thought (CoT) supervision has become a common strategy to
enhance reasoning in language models. While effective for large models, we
identify a phenomenon we call Long CoT Degradation, in which small language
models (SLMs; <=3B parameters) trained on limited long CoT data experience
significant performance deterioration. Through extensive experiments on the
Qwen2.5, LLaMA3 and Gemma3 families, we demonstrate that this degradation is
widespread across SLMs. In some settings, models trained on only 8k long CoT
examples lose up to 75% of their original performance before fine-tuning.
Strikingly, we further observe that for some particularly small models, even
training on 220k long CoT examples fails to recover or surpass their original
performance prior to fine-tuning. Our analysis attributes this effect to error
accumulation: while longer responses increase the capacity for multi-step
reasoning, they also amplify the risk of compounding mistakes. Furthermore, we
find that Long CoT Degradation may negatively impacts downstream reinforcement
learning (RL), although this can be alleviated by sufficiently scaled
supervised fine-tuning (SFT). Our findings challenge common assumptions about
the benefits of long CoT training for SLMs and offer practical guidance for
building more effective small-scale reasoning models.

</details>


### [121] [Multilingual Grammatical Error Annotation: Combining Language-Agnostic Framework with Language-Specific Flexibility](https://arxiv.org/abs/2506.07719)
*Mengyang Qiu,Tran Minh Nguyen,Zihao Huang,Zelong Li,Yang Gu,Qingyu Gao,Siliang Liu,Jungyeul Park*

Main category: cs.CL

TL;DR: 本文提出了一种标准化的、模块化的多语言语法错误标注框架，结合语言无关的基础和结构化的语言特定扩展，支持多语言语法错误校正的扩展性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的语法错误校正框架（如$	exttt{errant}$）在扩展到类型多样的语言时存在局限性，因此需要一种更灵活且一致的多语言标注框架。

Method: 本文重新实现了$	exttt{errant}$，使用$	exttt{stanza}$以支持更广泛的多语言覆盖，并结合语言无关的基础和语言特定的扩展，构建了一个模块化的标注框架。

Result: 该框架在英语、德语、捷克语、韩语和汉语中展示了其适应性，支持从通用标注到定制化语言精炼的应用。

Conclusion: 该工作促进了多语言环境下语法错误校正标注的可扩展性和一致性，并提供了完整的代码库和标注工具。

Abstract: Grammatical Error Correction (GEC) relies on accurate error annotation and
evaluation, yet existing frameworks, such as $\texttt{errant}$, face
limitations when extended to typologically diverse languages. In this paper, we
introduce a standardized, modular framework for multilingual grammatical error
annotation. Our approach combines a language-agnostic foundation with
structured language-specific extensions, enabling both consistency and
flexibility across languages. We reimplement $\texttt{errant}$ using
$\texttt{stanza}$ to support broader multilingual coverage, and demonstrate the
framework's adaptability through applications to English, German, Czech,
Korean, and Chinese, ranging from general-purpose annotation to more customized
linguistic refinements. This work supports scalable and interpretable GEC
annotation across languages and promotes more consistent evaluation in
multilingual settings. The complete codebase and annotation tools can be
accessed at https://github.com/open-writing-evaluation/jp_errant_bea.

</details>


### [122] [Swiss Parliaments Corpus Re-Imagined (SPC_R): Enhanced Transcription with RAG-based Correction and Predicted BLEU](https://arxiv.org/abs/2506.07726)
*Vincenzo Timmel,Manfred Vogel,Daniel Perruchoud,Reza Kakooee*

Main category: cs.CL

TL;DR: 本文提出了一种结合Whisper ASR、GPT-4o修正与数据过滤的方法，构建了高质量瑞士德语长篇议会语料库，BLEU分数提升6分。


<details>
  <summary>Details</summary>
Motivation: 针对瑞士德语低资源领域，构建高质量长篇幅语音-文本对齐语料库，解决传统句子级语料在语义完整性和命名实体识别上的不足。

Method: 使用Whisper Large-v3转录音频为标准德语，通过两阶段GPT-4o修正（命名实体纠错+语义完整性评估），结合预测BLEU分数和GPT评分进行数据过滤。

Result: 生成801小时音频数据（751小时通过质控），相比原句级语料库实现6分BLEU提升，证明方法有效性。

Conclusion: 融合ASR、大语言模型修正与数据驱动过滤的流程，显著提升低资源领域语音语料质量，为方言语音处理提供新范式。

Abstract: This paper presents a new long-form release of the Swiss Parliaments Corpus,
converting entire multi-hour Swiss German debate sessions (each aligned with
the official session protocols) into high-quality speech-text pairs. Our
pipeline starts by transcribing all session audio into Standard German using
Whisper Large-v3 under high-compute settings. We then apply a two-step GPT-4o
correction process: first, GPT-4o ingests the raw Whisper output alongside the
official protocols to refine misrecognitions, mainly named entities. Second, a
separate GPT-4o pass evaluates each refined segment for semantic completeness.
We filter out any segments whose Predicted BLEU score (derived from Whisper's
average token log-probability) and GPT-4o evaluation score fall below a certain
threshold. The final corpus contains 801 hours of audio, of which 751 hours
pass our quality control. Compared to the original sentence-level SPC release,
our long-form dataset achieves a 6-point BLEU improvement, demonstrating the
power of combining robust ASR, LLM-based correction, and data-driven filtering
for low-resource, domain-specific speech corpora.

</details>


### [123] [Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking](https://arxiv.org/abs/2506.07751)
*Silin Gao,Antoine Bosselut,Samy Bengio,Emmanuel Abbe*

Main category: cs.CL

TL;DR: 本文提出AbstraL方法，通过强化学习训练LLMs进行抽象推理，有效应对分布变化并提升鲁棒性，显著减少GSM扰动基准的性能下降。


<details>
  <summary>Details</summary>
Motivation: 现有小型LLMs在面临数值/名义变量变化或干扰条款时推理鲁棒性不足，传统基于合成数据的实例化方法效果有限。本文探索通过抽象化问题本质来增强模型泛化能力，并连接符号工具辅助求解。

Method: 采用强化学习（而非监督微调）训练细粒度抽象数据，开发AbstraL方法促进LLMs的抽象推理能力，生成忠实的问题抽象表征。

Result: 在GSM扰动基准测试中，AbstraL显著缓解了性能衰减，证明抽象方法对分布偏移的抵御效果优于传统数据增强策略。

Conclusion: 基于强化学习的抽象推理训练能有效提升LLMs的鲁棒性，其问题表征能力不仅对抗分布偏移，还为符号推理工具的整合提供了接口。

Abstract: Recent studies have shown that large language models (LLMs), especially
smaller ones, often lack robustness in their reasoning. I.e., they tend to
experience performance drops when faced with distribution shifts, such as
changes to numerical or nominal variables, or insertions of distracting
clauses. A possible strategy to address this involves generating synthetic data
to further "instantiate" reasoning problems on potential variations. In
contrast, our approach focuses on "abstracting" reasoning problems. This not
only helps counteract distribution shifts but also facilitates the connection
to symbolic tools for deriving solutions. We find that this abstraction process
is better acquired through reinforcement learning (RL) than just supervised
fine-tuning, which often fails to produce faithful abstractions. Our method,
AbstraL -- which promotes abstract reasoning in LLMs using RL on granular
abstraction data -- significantly mitigates performance degradation on recent
GSM perturbation benchmarks.

</details>


### [124] [LLM Unlearning Should Be Form-Independent](https://arxiv.org/abs/2506.07795)
*Xiaotian Ye,Mengqi Zhang,Shu Wu*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLM）遗忘技术，提出现有方法存在形式依赖偏差问题，并引入ORT基准和ROCR方法来改善遗忘效果。


<details>
  <summary>Details</summary>
Motivation: LLM遗忘技术旨在消除或抑制模型中的不良知识，以防止有害或私人信息的滥用。然而，现有方法在实际场景中效果有限，阻碍了其实际应用。

Method: 本文提出了一种名为Rank-one Concept Redirection (ROCR)的无训练方法，通过针对下游任务中的不变性（即激活的危险概念）进行遗忘，能够在几秒内修改模型参数，将模型对特定遗忘目标概念的感知重定向到无害概念。

Result: 实验表明，ROCR相比传统方法显著提高了遗忘效果，并生成高度自然的输出。

Conclusion: LLM遗忘技术应具备形式独立性，以应对现实世界安全关键场景中的多样化任务。ROCR为解决这一问题提供了有前景的路径。

Abstract: Large Language Model (LLM) unlearning aims to erase or suppress undesirable
knowledge within the model, offering promise for controlling harmful or private
information to prevent misuse. However, recent studies highlight its limited
efficacy in real-world scenarios, hindering practical adoption. In this study,
we identify a pervasive issue underlying many downstream failures: the
effectiveness of existing unlearning methods heavily depends on the form of
training samples and frequently fails to generalize to alternate expressions of
the same knowledge. We formally characterize this problem as Form-Dependent
Bias and systematically investigate its specific manifestation patterns across
various downstream tasks. To quantify its prevalence and support future
research, we introduce ORT, a novel benchmark designed to evaluate the
robustness of unlearning methods against variations in knowledge expression.
Results reveal that Form-Dependent Bias is both widespread and severe among
current techniques.
  We argue that LLM unlearning should be form-independent to address the
endless forms of downstream tasks encountered in real-world security-critical
scenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR),
a novel training-free method, as a promising solution path. ROCR performs
unlearning by targeting the invariants in downstream tasks, specifically the
activated dangerous concepts. It is capable of modifying model parameters
within seconds to redirect the model's perception of a specific unlearning
target concept to another harmless concept. Extensive experiments demonstrate
that ROCR significantly improves unlearning effectiveness compared to
traditional methods while generating highly natural outputs.

</details>


### [125] [MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification](https://arxiv.org/abs/2506.07801)
*Iustin Sirbu,Robert-Adrian Popovici,Cornelia Caragea,Stefan Trausan-Matu,Traian Rebedea*

Main category: cs.CL

TL;DR: MultiMatch是一种新型半监督学习算法，结合协同训练、一致性正则化和伪标签技术，通过三重加权模块提升伪标签质量，在文本分类任务中实现SOTA性能并显著提升数据不平衡场景的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有半监督学习方法在伪标签选择、分类难度适应性和数据不平衡场景的鲁棒性方面存在局限，需要整合不同技术优势以提升模型性能。

Method: 提出三阶段伪标签加权模块：1) 基于多头一致性筛选伪标签；2) 自适应置信度过滤；3) 根据分类难度动态加权。融合Multihead Co-training、FreeMatch和MarginMatch的核心技术。

Result: 在5个NLP数据集的10个设置中9项达到SOTA，Friedman测试排名第一。数据高度不平衡场景下准确率比次优方法高3.26%。

Conclusion: MultiMatch通过系统整合现有技术形成统一框架，在标准与不平衡场景均展现优越性能，为文本分类任务提供了更鲁棒的半监督学习解决方案。

Abstract: We introduce MultiMatch, a novel semi-supervised learning (SSL) algorithm
combining the paradigms of co-training and consistency regularization with
pseudo-labeling. At its core, MultiMatch features a three-fold pseudo-label
weighting module designed for three key purposes: selecting and filtering
pseudo-labels based on head agreement and model confidence, and weighting them
according to the perceived classification difficulty. This novel module
enhances and unifies three existing techniques -- heads agreement from
Multihead Co-training, self-adaptive thresholds from FreeMatch, and Average
Pseudo-Margins from MarginMatch -- resulting in a holistic approach that
improves robustness and performance in SSL settings. Experimental results on
benchmark datasets highlight the superior performance of MultiMatch, achieving
state-of-the-art results on 9 out of 10 setups from 5 natural language
processing datasets and ranking first according to the Friedman test among 19
methods. Furthermore, MultiMatch demonstrates exceptional robustness in highly
imbalanced settings, outperforming the second-best approach by 3.26% -- and
data imbalance is a key factor for many text classification tasks.

</details>


### [126] [WebUIBench: A Comprehensive Benchmark for Evaluating Multimodal Large Language Models in WebUI-to-Code](https://arxiv.org/abs/2506.07818)
*Zhiyu Lin,Zhengda Zhou,Zhiyuan Zhao,Tianrui Wan,Yilun Ma,Junyu Gao,Xuelong Li*

Main category: cs.CL

TL;DR: 本文提出WebUIBench基准，系统性评估多模态大语言模型在网页开发中的多维子能力，发现主流模型的技能特点与缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅关注网页生成结果，缺乏对开发过程中多维子能力的评估，需构建多视角框架以精准指导开发效率提升。

Method: 基于软件工程原则设计WebUIBench基准，覆盖WebUI感知、HTML编程、理解与代码生成四领域，使用0.7K真实网站生成21K问答对。

Result: 对29个主流MLLMs的评估揭示了模型在开发过程中的技能特征及各类弱点，如特定子能力不足或跨阶段协作问题。

Conclusion: WebUIBench为MLLMs在网页开发中的能力评估提供系统性工具，有助于针对性优化模型，推动AI软件工程师的效能提升。

Abstract: With the rapid advancement of Generative AI technology, Multimodal Large
Language Models(MLLMs) have the potential to act as AI software engineers
capable of executing complex web application development. Considering that the
model requires a confluence of multidimensional sub-capabilities to address the
challenges of various development phases, constructing a multi-view evaluation
framework is crucial for accurately guiding the enhancement of development
efficiency. However, existing benchmarks usually fail to provide an assessment
of sub-capabilities and focus solely on webpage generation outcomes. In this
work, we draw inspiration from the principles of software engineering and
further propose WebUIBench, a benchmark systematically designed to evaluate
MLLMs in four key areas: WebUI Perception, HTML Programming,WebUI-HTML
Understanding, and WebUI-to-Code. WebUIBench comprises 21K high-quality
question-answer pairs derived from over 0.7K real-world websites. The extensive
evaluation of 29 mainstream MLLMs uncovers the skill characteristics and
various weakness that models encountered during the development process.

</details>


### [127] [Learning to Focus: Causal Attention Distillation via Gradient-Guided Token Pruning](https://arxiv.org/abs/2506.07851)
*Yiju Guo,Wenkai Yang,Zexu Sun,Ning Ding,Zhiyuan Liu,Yankai Lin*

Main category: cs.CL

TL;DR: 本文提出LeaF框架，通过两阶段干预推理消除训练数据中的伪相关性，提升大语言模型在长上下文推理中对关键信息的关注能力，从而改善生成质量和推理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在长上下文推理中易受干扰模式影响，导致注意力分散。研究发现训练数据中的伪相关性阻碍模型捕捉真实的因果指令-响应关系，引发冗余推理和错误输出。

Method: LeaF框架分两阶段：1) 基于梯度比较和高级教师模型自动识别训练语料中的混淆token；2) 通过剪枝蒸馏实施干预，使学生的注意力分布与教师的关键上下文关注对齐。

Result: 实验表明LeaF在数学推理和代码生成基准上实现绝对性能提升，有效抑制对混淆token的关注，并产生更可解释、可靠的推理过程。

Conclusion: LeaF通过干预式推理解耦混杂因素，显著增强大语言模型对关键上下文的聚焦能力，同时提升任务性能与模型可解释性。

Abstract: Large language models (LLMs) have demonstrated significant improvements in
contextual understanding. However, their ability to attend to truly critical
information during long-context reasoning and generation still falls behind the
pace. Specifically, our preliminary experiments reveal that certain distracting
patterns can misdirect the model's attention during inference, and removing
these patterns substantially improves reasoning accuracy and generation
quality. We attribute this phenomenon to spurious correlations in the training
data, which obstruct the model's capacity to infer authentic causal
instruction-response relationships. This phenomenon may induce redundant
reasoning processes, potentially resulting in significant inference overhead
and, more critically, the generation of erroneous or suboptimal responses. To
mitigate this, we introduce a two-stage framework called Learning to Focus
(LeaF) leveraging intervention-based inference to disentangle confounding
factors. In the first stage, LeaF employs gradient-based comparisons with an
advanced teacher to automatically identify confounding tokens based on causal
relationships in the training corpus. Then, in the second stage, it prunes
these tokens during distillation to enact intervention, aligning the student's
attention with the teacher's focus distribution on truly critical context
tokens. Experimental results demonstrate that LeaF not only achieves an
absolute improvement in various mathematical reasoning and code generation
benchmarks but also effectively suppresses attention to confounding tokens
during inference, yielding a more interpretable and reliable reasoning model.

</details>


### [128] [MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs](https://arxiv.org/abs/2506.07899)
*Ke Wang,Yiming Qin,Nikolaos Dimitriadis,Alessandro Favero,Pascal Frossard*

Main category: cs.CL

TL;DR: MEMOIR提出了一种通过残差记忆模块和稀疏激活掩码实现高效、可扩展的语言模型持续编辑框架，在保持预训练模型核心能力的同时，支持数千次连续编辑且最小化遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有终身模型编辑方法存在泛化性下降、编辑间干扰或无法扩展至长编辑序列的问题，亟需一种可靠且可扩展的模型更新方案。

Method: 通过样本依赖的稀疏掩码将每次编辑限制在内存参数的独立子集，推理时通过查询激活模式匹配相关记忆，抑制无关知识激活，实现知识精准调用。

Result: 在LLaMA-3和Mistral上的问答、幻觉修正及分布外泛化实验表明，MEMOIR在可靠性、泛化性和局部性指标上达到SOTA，支持超千次连续编辑。

Conclusion: MEMOIR通过解耦知识存储与模型参数，有效解决了持续编辑中的干扰与扩展难题，为实际部署中的动态知识更新提供了新范式。

Abstract: Language models deployed in real-world systems often require post-hoc updates
to incorporate new or corrected knowledge. However, editing such models
efficiently and reliably - without retraining or forgetting previous
information - remains a major challenge. Existing methods for lifelong model
editing either compromise generalization, interfere with past edits, or fail to
scale to long editing sequences. We propose MEMOIR, a novel scalable framework
that injects knowledge through a residual memory, i.e., a dedicated parameter
module, while preserving the core capabilities of the pre-trained model. By
sparsifying input activations through sample-dependent masks, MEMOIR confines
each edit to a distinct subset of the memory parameters, minimizing
interference among edits. At inference, it identifies relevant edits by
comparing the sparse activation patterns of new queries to those stored during
editing. This enables generalization to rephrased queries by activating only
the relevant knowledge while suppressing unnecessary memory activation for
unrelated prompts. Experiments on question answering, hallucination correction,
and out-of-distribution generalization benchmarks across LLaMA-3 and Mistral
demonstrate that MEMOIR achieves state-of-the-art performance across
reliability, generalization, and locality metrics, scaling to thousands of
sequential edits with minimal forgetting.

</details>


### [129] [MiniCPM4: Ultra-Efficient LLMs on End Devices](https://arxiv.org/abs/2506.07900)
*MiniCPM Team,Chaojun Xiao,Yuxuan Li,Xu Han,Yuzhuo Bai,Jie Cai,Haotian Chen,Wentong Chen,Xin Cong,Ganqu Cui,Ning Ding,Shengdan Fan,Yewei Fang,Zixuan Fu,Wenyu Guan,Yitong Guan,Junshao Guo,Yufeng Han,Bingxiang He,Yuxiang Huang,Cunliang Kong,Qiuzuo Li,Siyuan Li,Wenhao Li,Yanghao Li,Yishan Li,Zhen Li,Dan Liu,Biyuan Lin,Yankai Lin,Xiang Long,Quanyu Lu,Yaxi Lu,Peiyan Luo,Hongya Lyu,Litu Ou,Yinxu Pan,Zekai Qu,Qundong Shi,Zijun Song,Jiayuan Su,Zhou Su,Ao Sun,Xianghui Sun,Peijun Tang,Fangzheng Wang,Feng Wang,Shuo Wang,Yudong Wang,Yesai Wu,Zhenyu Xiao,Jie Xie,Zihao Xie,Yukun Yan,Jiarui Yuan,Kaihuo Zhang,Lei Zhang,Linyue Zhang,Xueren Zhang,Yudi Zhang,Hengyu Zhao,Weilin Zhao,Weilun Zhao,Yuanqian Zhao,Zhi Zheng,Ge Zhou,Jie Zhou,Wei Zhou,Zihan Zhou,Zixuan Zhou,Zhiyuan Liu,Guoyang Zeng,Chao Jia,Dahai Li,Maosong Sun*

Main category: cs.CL

TL;DR: 本文提出MiniCPM4，一种专为终端设备设计的高效大语言模型，通过模型架构、训练数据、算法及推理系统的系统创新，以0.5B和8B参数版本实现高效长上下文处理，性能超越同规模开源模型。


<details>
  <summary>Details</summary>
Motivation: 针对终端设备对高效大语言模型的需求，通过系统性优化解决长上下文处理速度及资源效率问题，满足多样化设备部署需求。

Method: 1. 模型架构：提出可训练稀疏注意力机制InfLLM v2；2. 训练数据：开发UltraClean预训练数据过滤策略和UltraChat v2微调数据集；3. 训练算法：引入ModelTunnel v2预训练策略搜索及BitCPM量化方法；4. 推理系统：集成稀疏注意力、量化和推测采样的CPM.cu系统。

Result: MiniCPM4在多个基准测试中优于同规模开源模型，8B版本长序列处理速度显著超越Qwen3-8B，并成功支持可信调查生成、工具调用等应用场景。

Conclusion: MiniCPM4通过四维系统创新实现终端设备高效部署，在性能与速度间取得平衡，验证了其在多样化场景中的实用性与扩展潜力。

Abstract: This paper introduces MiniCPM4, a highly efficient large language model (LLM)
designed explicitly for end-side devices. We achieve this efficiency through
systematic innovation in four key dimensions: model architecture, training
data, training algorithms, and inference systems. Specifically, in terms of
model architecture, we propose InfLLM v2, a trainable sparse attention
mechanism that accelerates both prefilling and decoding phases for long-context
processing. Regarding training data, we propose UltraClean, an efficient and
accurate pre-training data filtering and generation strategy, and UltraChat v2,
a comprehensive supervised fine-tuning dataset. These datasets enable
satisfactory model performance to be achieved using just 8 trillion training
tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient
pre-training strategy search, and improve existing post-training methods by
introducing chunk-wise rollout for load-balanced reinforcement learning and
data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose
CPM.cu that integrates sparse attention, model quantization, and speculative
sampling to achieve efficient prefilling and decoding. To meet diverse
on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B
parameters, respectively. Sufficient evaluation results show that MiniCPM4
outperforms open-source models of similar size across multiple benchmarks,
highlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B
demonstrates significant speed improvements over Qwen3-8B when processing long
sequences. Through further adaptation, MiniCPM4 successfully powers diverse
applications, including trustworthy survey generation and tool use with model
context protocol, clearly showcasing its broad usability.

</details>


### [130] [Quantum Graph Transformer for NLP Sentiment Classification](https://arxiv.org/abs/2506.07937)
*Shamminuj Aktar,Andreas Bärtschi,Abdel-Hameed A. Badawy,Stephan Eidenbenz*

Main category: cs.CL

TL;DR: 本文提出了一种量子图变换器（QGT），结合量子自注意力机制和图传递框架，用于结构化语言建模，在情感分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习在处理复杂结构化数据方面具有潜力，特别是在自然语言处理领域，需要更高效和表达能力强的模型。

Method: QGT通过参数化量子电路（PQCs）实现自注意力机制，减少了可训练参数数量，并将其集成到图传递框架中。

Result: QGT在五个情感分类基准测试中表现优于或与现有量子自然语言处理模型相当，相比经典图变换器，在真实和合成数据集上分别提高了5.42%和4.76%的准确率，且在Yelp数据集上样本效率显著提升。

Conclusion: QGT展示了图基量子自然语言处理技术在高效和可扩展语言理解方面的潜力。

Abstract: Quantum machine learning is a promising direction for building more efficient
and expressive models, particularly in domains where understanding complex,
structured data is critical. We present the Quantum Graph Transformer (QGT), a
hybrid graph-based architecture that integrates a quantum self-attention
mechanism into the message-passing framework for structured language modeling.
The attention mechanism is implemented using parameterized quantum circuits
(PQCs), which enable the model to capture rich contextual relationships while
significantly reducing the number of trainable parameters compared to classical
attention mechanisms. We evaluate QGT on five sentiment classification
benchmarks. Experimental results show that QGT consistently achieves higher or
comparable accuracy than existing quantum natural language processing (QNLP)
models, including both attention-based and non-attention-based approaches. When
compared with an equivalent classical graph transformer, QGT yields an average
accuracy improvement of 5.42% on real-world datasets and 4.76% on synthetic
datasets. Additionally, QGT demonstrates improved sample efficiency, requiring
nearly 50% fewer labeled samples to reach comparable performance on the Yelp
dataset. These results highlight the potential of graph-based QNLP techniques
for advancing efficient and scalable language understanding.

</details>


### [131] [Statistical Hypothesis Testing for Auditing Robustness in Language Models](https://arxiv.org/abs/2506.07947)
*Paulius Rauba,Qiyao Wei,Mihaela van der Schaar*

Main category: cs.CL

TL;DR: 提出一种基于分布的扰动分析框架，用于检测大型语言模型在干预下的输出变化，通过频率假设检验提供可解释的统计结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效评估LLM在扰动下的输出变化，因其随机性导致直接对比不可行，且传统文本分析关注偏差而非分布变化。

Method: 构建低维语义空间中经验性的零假设/备择假设分布，利用蒙特卡洛采样实现无需严格分布假设的假设检验框架，支持多扰动评估与误差控制。

Result: 案例研究表明该框架能量化响应变化、计算真/假阳性率，并验证与参考模型的对齐性，证实其作为可靠审计工具的有效性。

Conclusion: 该框架为LLM审计提供模型无关、可解释的统计推断方法，支持任意黑盒模型与扰动分析，具有广泛适用性和实用价值。

Abstract: Consider the problem of testing whether the outputs of a large language model
(LLM) system change under an arbitrary intervention, such as an input
perturbation or changing the model variant. We cannot simply compare two LLM
outputs since they might differ due to the stochastic nature of the system, nor
can we compare the entire output distribution due to computational
intractability. While existing methods for analyzing text-based outputs exist,
they focus on fundamentally different problems, such as measuring bias or
fairness. To this end, we introduce distribution-based perturbation analysis, a
framework that reformulates LLM perturbation analysis as a frequentist
hypothesis testing problem. We construct empirical null and alternative output
distributions within a low-dimensional semantic similarity space via Monte
Carlo sampling, enabling tractable inference without restrictive distributional
assumptions. The framework is (i) model-agnostic, (ii) supports the evaluation
of arbitrary input perturbations on any black-box LLM, (iii) yields
interpretable p-values; (iv) supports multiple perturbations via controlled
error rates; and (v) provides scalar effect sizes. We demonstrate the
usefulness of the framework across multiple case studies, showing how we can
quantify response changes, measure true/false positive rates, and evaluate
alignment with reference models. Above all, we see this as a reliable
frequentist hypothesis testing framework for LLM auditing.

</details>


### [132] [Language Models over Canonical Byte-Pair Encodings](https://arxiv.org/abs/2506.07956)
*Tim Vieira,Tianyu Liu,Clemente Pasti,Yahya Emara,Brian DuSell,Benjamin LeBrun,Mario Giulianelli,Juan Luis Gastaldi,Timothy J. O'Donnell,Ryan Cotterell*

Main category: cs.CL

TL;DR: 现代语言模型通过分词器（如BPE）将字符序列转换为token序列，但存在对非规范token编码分配概率的问题。本文提出两种方法确保仅规范token序列获得正概率，从而提升模型效果。


<details>
  <summary>Details</summary>
Motivation: 现有分词方法导致模型为大量非规范token编码分配概率，这些编码虽解码有效但实际训练中从未出现，造成概率分配错误。

Method: 提出两种方法：(1) 基于推理的条件约束（无需额外训练），(2) 基于模型构造的保证规范性的参数化方法（需重新训练）。

Result: 实验表明，修正非规范性问题可提升多个模型和语料库在测试数据上的似然度。

Conclusion: 消除非规范token编码的概率分配错误是可行且必要的，两种方法均能有效优化模型概率分布质量。

Abstract: Modern language models represent probability distributions over character
strings as distributions over (shorter) token strings derived via a
deterministic tokenizer, such as byte-pair encoding. While this approach is
highly effective at scaling up language models to large corpora, its current
incarnations have a concerning property: the model assigns nonzero probability
mass to an exponential number of $\it{noncanonical}$ token encodings of each
character string -- these are token strings that decode to valid character
strings but are impossible under the deterministic tokenizer (i.e., they will
never be seen in any training corpus, no matter how large). This misallocation
is both erroneous, as noncanonical strings never appear in training data, and
wasteful, diverting probability mass away from plausible outputs. These are
avoidable mistakes! In this work, we propose methods to enforce canonicality in
token-level language models, ensuring that only canonical token strings are
assigned positive probability. We present two approaches: (1) canonicality by
conditioning, leveraging test-time inference strategies without additional
training, and (2) canonicality by construction, a model parameterization that
guarantees canonical outputs but requires training. We demonstrate that fixing
canonicality mistakes improves the likelihood of held-out data for several
models and corpora.

</details>


### [133] [Correlated Errors in Large Language Models](https://arxiv.org/abs/2506.07962)
*Elliot Kim,Avi Garg,Kenny Peng,Nikhil Garg*

Main category: cs.CL

TL;DR: 研究发现，尽管LLM在架构、数据、提供商上存在差异，但模型错误高度相关，尤其是更大、更准确的模型。这种相关性影响下游任务如评估和招聘，揭示算法单一化风险。


<details>
  <summary>Details</summary>
Motivation: 现有假设认为训练数据、架构和提供商的多样性可减少LLM同质性，但缺乏实证支持。本文旨在验证不同LLM是否真正存在显著差异。

Method: 对超过350个LLM进行大规模评估，使用两个主流排行榜数据集和一个简历筛选任务，分析模型错误相关性及其驱动因素（如架构、提供商），并探究下游任务影响。

Result: 模型错误高度相关（某数据集中错误一致率达60%），共享架构和提供商是驱动因素；更大、更准确的模型即使架构不同仍高度相关。下游任务中，LLM-as-judge评估和招聘决策受此影响，后者符合算法单一化理论。

Conclusion: 当前LLM多样性不足以避免同质化，错误相关性可能导致系统性风险，尤其在关键应用场景（如招聘）中需警惕算法单一化问题。

Abstract: Diversity in training data, architecture, and providers is assumed to
mitigate homogeneity in LLMs. However, we lack empirical evidence on whether
different LLMs differ meaningfully. We conduct a large-scale empirical
evaluation on over 350 LLMs overall, using two popular leaderboards and a
resume-screening task. We find substantial correlation in model errors -- on
one leaderboard dataset, models agree 60% of the time when both models err. We
identify factors driving model correlation, including shared architectures and
providers. Crucially, however, larger and more accurate models have highly
correlated errors, even with distinct architectures and providers. Finally, we
show the effects of correlation in two downstream tasks: LLM-as-judge
evaluation and hiring -- the latter reflecting theoretical predictions
regarding algorithmic monoculture.

</details>


### [134] [Reinforcement Pre-Training](https://arxiv.org/abs/2506.08007)
*Qingxiu Dong,Li Dong,Yao Tang,Tianzhu Ye,Yutao Sun,Zhifang Sui,Furu Wei*

Main category: cs.CL

TL;DR: 本文提出了一种新的扩展范式——强化预训练（RPT），通过将下一个词的预测任务重构为基于强化学习的推理任务，显著提升了语言模型的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 为了利用大量文本数据进行通用强化学习，而非依赖特定领域的标注数据，本文提出了强化预训练（RPT）作为一种新的扩展范式。

Method: RPT将下一个词的预测任务重构为基于强化学习的推理任务，通过为正确预测下一个词提供可验证的奖励来训练模型。

Result: RPT显著提升了语言模型在预测下一个词时的准确性，并为后续的强化微调提供了强大的预训练基础。

Conclusion: RPT作为一种有效的扩展范式，能够显著提升语言模型的预训练效果，并展示了随着计算资源的增加，预测准确性持续提升的趋势。

Abstract: In this work, we introduce Reinforcement Pre-Training (RPT) as a new scaling
paradigm for large language models and reinforcement learning (RL).
Specifically, we reframe next-token prediction as a reasoning task trained
using RL, where it receives verifiable rewards for correctly predicting the
next token for a given context. RPT offers a scalable method to leverage vast
amounts of text data for general-purpose RL, rather than relying on
domain-specific annotated answers. By incentivizing the capability of
next-token reasoning, RPT significantly improves the language modeling accuracy
of predicting the next tokens. Moreover, RPT provides a strong pre-trained
foundation for further reinforcement fine-tuning. The scaling curves show that
increased training compute consistently improves the next-token prediction
accuracy. The results position RPT as an effective and promising scaling
paradigm to advance language model pre-training.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [135] [Understanding Financial Reasoning in AI: A Multimodal Benchmark and Error Learning Approach](https://arxiv.org/abs/2506.06282)
*Shuangyan Deng,Haizhou Peng,Jiachen Xu,Chunhou Liu,Ciprian Doru Giurcuaneanu,Jiamou Liu*

Main category: cs.AI

TL;DR: 本文提出一个结合文本与视觉的多模态金融推理基准测试FinMR，并设计基于错误反馈的无调优学习框架，实验表明多模态输入和错误反馈能显著提升模型性能，但视觉理解和数学逻辑仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 现有金融AI模型在整合视觉数据（如图表）和复杂推理方面存在不足，需构建更贴近真实场景的评估体系以提升模型金融分析能力。

Method: 构建含3,200个专家级多模态问题的金融基准FinMR，并提出错误感知学习框架——通过历史错误反馈指导推理过程，无需微调模型。

Result: 多模态输入使SOTA模型性能提升显著，错误反馈机制带来持续改进，但模型在视觉解析（-12.3%）和数学计算（-9.8%）上仍明显落后人类专家。

Conclusion: 金融AI需强化多模态理解能力，基于错误反馈的自省推理机制展现出潜力，但视觉与数理逻辑仍是关键突破方向。

Abstract: Effective financial reasoning demands not only textual understanding but also
the ability to interpret complex visual data such as charts, tables, and trend
graphs. This paper introduces a new benchmark designed to evaluate how well AI
models - especially large language and multimodal models - reason in
finance-specific contexts. Covering 3,200 expert-level question-answer pairs
across 15 core financial topics, the benchmark integrates both textual and
visual modalities to reflect authentic analytical challenges in finance. To
address limitations in current reasoning approaches, we propose an error-aware
learning framework that leverages historical model mistakes and feedback to
guide inference, without requiring fine-tuning. Our experiments across
state-of-the-art models show that multimodal inputs significantly enhance
performance and that incorporating error feedback leads to consistent and
measurable improvements. The results highlight persistent challenges in visual
understanding and mathematical logic, while also demonstrating the promise of
self-reflective reasoning in financial AI systems. Our code and data can be
found at https://anonymous/FinMR/CodeData.

</details>


### [136] [Unreal Patterns](https://arxiv.org/abs/2506.06284)
*John Beverley,Jim Logan*

Main category: cs.AI

TL;DR: 本文提出了一种用于表示不存在或可能永远不存在的实体信息的框架，与传统方法不同，它强调使用实际类型的交集而非特定不存在的实例来建模。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理不存在实体时，要么过度依赖形而上学的假设，要么引入计算效率低下的问题，本文旨在提供一种实用且计算可行的解决方案。

Method: 本文采用结构化本体驱动的方法，基于基本形式本体论，通过实际类型的交集来建模不存在的实体。

Result: 提出了一种新的框架，能够有效处理对假设或不存在实体的引用，避免了传统方法的缺陷。

Conclusion: 本文的框架为处理不存在实体提供了一种实用且计算可行的解决方案，强调了实际应用的重要性。

Abstract: This paper introduces a framework for representing information about entities
that do not exist or may never exist, such as those involving fictional
entities, blueprints, simulations, and future scenarios. Traditional approaches
that introduce "dummy instances" or rely on modal logic are criticized, and a
proposal is defended in which such cases are modeled using the intersections of
actual types rather than specific non existent tokens. The paper positions
itself within the Basic Formal Ontology and its realist commitments,
emphasizing the importance of practical, implementable solutions over purely
metaphysical or philosophical proposals, arguing that existing approaches to
non existent entities either overcommit to metaphysical assumptions or
introduce computational inefficiencies that hinder applications. By developing
a structured ontology driven approach to unreal patterns, the paper aims to
provide a useful and computationally viable means of handling references to
hypothetical or non existent entities.

</details>


### [137] [NFISiS: New Perspectives on Fuzzy Inference Systems for Renewable Energy Forecasting](https://arxiv.org/abs/2506.06285)
*Kaike Sa Teles Rocha Alves,Eduardo Pestana de Aguiar*

Main category: cs.AI

TL;DR: 本文介绍了evolvingfuzzysystems，一个Python库，提供了多种演化模糊系统模型的实现，旨在促进自适应和可解释机器学习的研究和应用。


<details>
  <summary>Details</summary>
Motivation: 演化模糊系统（eFS）因其能够自适应地更新结构以应对数据动态，同时保持可解释性而受到关注，但缺乏公开的实现限制了其广泛应用。

Method: 作者开发了一个Python库，实现了多种eFS模型，并提供了训练、可视化和性能评估的内置工具，使用fetch_california_housing数据集进行模型评估。

Result: 评估结果显示ePL模型在准确性和计算成本之间取得了平衡，特别适合实际应用。

Conclusion: 通过公开这些模型，evolvingfuzzysystems旨在促进自适应和可解释机器学习的研究和实际应用。

Abstract: Evolving Fuzzy Systems (eFS) have gained significant attention due to their
ability to adaptively update their structure in response to data dynamics while
maintaining interpretability. However, the lack of publicly available
implementations of these models limits their accessibility and widespread
adoption. To address this gap, we present evolvingfuzzysystems, a Python
library that provides implementations of several well-established eFS models,
including ePL-KRLS-DISCO, ePL+, eMG, ePL, exTS, Simpl\_eTS, and eTS. The
library facilitates model evaluation and comparison by offering built-in tools
for training, visualization, and performance assessment. The models are
evaluated using the fetch\_california\_housing dataset, with performance
measured in terms of normalized root-mean-square error (NRMSE), non-dimensional
error index (NDEI), and mean absolute percentage error (MAPE). Additionally,
computational complexity is analyzed by measuring execution times and rule
evolution during training and testing phases. The results highlight ePL as a
simple yet efficient model that balances accuracy and computational cost,
making it particularly suitable for real-world applications. By making these
models publicly available, evolvingfuzzysystems aims to foster research and
practical applications in adaptive and interpretable machine learning.

</details>


### [138] [Deep Research Bench: Evaluating AI Web Research Agents](https://arxiv.org/abs/2506.06287)
*FutureSearch,:,Nikos I. Bosse,Jon Evans,Robert G. Gambee,Daniel Hnyk,Peter Mühlbacher,Lawrence Phillips,Dan Schwarz,Jack Wildman*

Main category: cs.AI

TL;DR: 本文介绍了Deep Research Bench，一个包含89个多步骤网络研究任务的基准，用于评估AI网络研究代理的质量，并提供了一个名为RetroSearch的离线环境，确保评估的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现代AI中最常见的应用之一是带有网络搜索功能的LLM聊天，但目前缺乏对网络研究代理质量的直接评估，尤其是考虑到网络内容的不断变化。

Method: 作者创建了Deep Research Bench，包含89个多步骤网络研究任务，并提供了一个名为RetroSearch的离线环境，使用冻结的网络页面集进行测试，确保评估的稳定性。

Result: 实验表明，离线RetroSearch代理与实时网络代理表现相当，能够可靠地评估模型随时间的变化，并提供了对主要LLM的自动化评估，包括幻觉、工具使用和遗忘等指标。

Conclusion: Deep Research Bench和RetroSearch环境为评估网络研究代理提供了一个可靠的基准，能够持续跟踪模型的进展，并公开了主要网络研究产品的评估结果。

Abstract: Amongst the most common use cases of modern AI is LLM chat with web search
enabled. However, no direct evaluations of the quality of web research agents
exist that control for the continually-changing web. We introduce Deep Research
Bench, consisting of 89 multi-step web research task instances of varying
difficulty across 8 diverse task categories, with the answers carefully worked
out by skilled humans. We provide a "RetroSearch" environment with a large
frozen set of scraped web pages, and demonstrate that offline "RetroSearch"
agents perform comparably to "live web" agents, enabling reliable evaluations
of models over time. We provide robust agent tooling and scaffolding to
benchmark major LLMs as they are released, including "thinking" models like o3
and Gemini 2.5 Pro. We include automated evaluations of the lengthy agent
traces to report progress over time in hallucinations, tool use, and
forgetting. Finally, we evaluate the major web research products branded as
"Deep Research", "Deep Search", "Search", or "Research." Results are available
on a public leaderboard at https://drb.futuresearch.ai/.

</details>


### [139] [Large Language Models and Their Applications in Roadway Safety and Mobility Enhancement: A Comprehensive Review](https://arxiv.org/abs/2506.06301)
*Muhammad Monjurul Karim,Yan Shi,Shucheng Zhang,Bingzhang Wang,Mehrdad Nasri,Yinhai Wang*

Main category: cs.AI

TL;DR: 本文综述了大语言模型（LLMs）在提升道路安全和交通流动性中的应用与定制化，探讨了其潜力与挑战，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统工程方法在处理复杂、动态的交通环境时存在局限，LLMs凭借其在自然语言理解、知识整合和推理方面的能力，为交通系统提供了新的分析框架。

Method: 本文通过系统回顾LLMs在交通领域的应用，分析了其在架构、训练、提示和多模态策略上的定制化方法，以弥合交通数据与LLMs之间的模态差距。

Result: LLMs在交通流动性（如交通流预测、信号控制）和安全性（如事故分析、驾驶员行为评估）中展现了多样化应用潜力，但仍面临幻觉、推理缺陷、数据隐私等挑战。

Conclusion: LLMs在交通领域具有变革性潜力，但需通过负责任的技术创新，如多模态融合、时空推理增强、人机协作等，以实现更安全、智能的交通系统。

Abstract: Roadway safety and mobility remain critical challenges for modern
transportation systems, demanding innovative analytical frameworks capable of
addressing complex, dynamic, and heterogeneous environments. While traditional
engineering methods have made progress, the complexity and dynamism of
real-world traffic necessitate more advanced analytical frameworks. Large
Language Models (LLMs), with their unprecedented capabilities in natural
language understanding, knowledge integration, and reasoning, represent a
promising paradigm shift. This paper comprehensively reviews the application
and customization of LLMs for enhancing roadway safety and mobility. A key
focus is how LLMs are adapted -- via architectural, training, prompting, and
multimodal strategies -- to bridge the "modality gap" with transportation's
unique spatio-temporal and physical data. The review systematically analyzes
diverse LLM applications in mobility (e.g., traffic flow prediction, signal
control) and safety (e.g., crash analysis, driver behavior assessment,).
Enabling technologies such as V2X integration, domain-specific foundation
models, explainability frameworks, and edge computing are also examined.
Despite significant potential, challenges persist regarding inherent LLM
limitations (hallucinations, reasoning deficits), data governance (privacy,
bias), deployment complexities (sim-to-real, latency), and rigorous safety
assurance. Promising future research directions are highlighted, including
advanced multimodal fusion, enhanced spatio-temporal reasoning, human-AI
collaboration, continuous learning, and the development of efficient,
verifiable systems. This review provides a structured roadmap of current
capabilities, limitations, and opportunities, underscoring LLMs' transformative
potential while emphasizing the need for responsible innovation to realize
safer, more intelligent transportation systems.

</details>


### [140] [Mapping Human-Agent Co-Learning and Co-Adaptation: A Scoping Review](https://arxiv.org/abs/2506.06324)
*Shruti Kumar,Xiaoyu Chen,Xiaomei Wang*

Main category: cs.AI

TL;DR: 本文探讨了人-人工智能-机器人协同学习与适应的挑战，分析了现有研究中的术语不一致性，并提出了三个研究问题以深入理解这一领域。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解人-人工智能-机器人协同学习与适应的术语使用、智能代理类型、任务领域以及认知理论与框架的多样性。

Method: 通过范围综述，收集并分析现有文献，探讨人-人工智能-机器人协同学习与适应的术语、智能代理类型、任务领域及认知理论与框架。

Result: 研究发现现有文献中术语使用不一致，且研究领域多样，涵盖了从一次性到持续学习/适应的场景，并提出了新的认知框架。

Conclusion: 结论指出，理解人-人工智能-机器人协同学习与适应的术语、智能代理类型、任务领域及认知理论与框架对于推动这一领域的研究至关重要。

Abstract: Several papers have delved into the challenges of human-AI-robot co-learning
and co-adaptation. It has been noted that the terminology used to describe this
collaborative relationship in existing studies needs to be more consistent. For
example, the prefix "co" is used interchangeably to represent both
"collaborative" and "mutual," and the terms "co-learning" and "co-adaptation"
are sometimes used interchangeably. However, they can reflect subtle
differences in the focus of the studies. The current scoping review's primary
research question (RQ1) aims to gather existing papers discussing this
collaboration pattern and examine the terms researchers use to describe this
human-agent relationship. Given the relative newness of this area of study, we
are also keen on exploring the specific types of intelligent agents and task
domains that have been considered in existing research (RQ2). This exploration
is significant as it can shed light on the diversity of human-agent
interactions, from one-time to continuous learning/adaptation scenarios. It can
also help us understand the dynamics of human-agent interactions in different
task domains, guiding our expectations towards research situated in dynamic,
complex domains. Our third objective (RQ3) is to investigate the cognitive
theories and frameworks that have been utilized in existing studies to measure
human-agent co-learning and co-adaptation. This investigation is crucial as it
can help us understand the theoretical underpinnings of human-agent
collaboration and adaptation, and it can also guide us in identifying any new
frameworks proposed specifically for this type of relationship.

</details>


### [141] [Memory OS of AI Agent](https://arxiv.org/abs/2506.06326)
*Jiazheng Kang,Mingming Ji,Zhe Zhao,Ting Bai*

Main category: cs.AI

TL;DR: 本文提出了一种名为MemoryOS的内存操作系统，旨在解决大语言模型在固定上下文窗口和内存管理不足方面的挑战，提升AI代理的长期记忆能力和个性化交互体验。


<details>
  <summary>Details</summary>
Motivation: 大语言模型面临固定上下文窗口和内存管理不足的挑战，导致长期记忆能力不足和个性化交互体验受限。本文旨在通过创新设计解决这一问题。

Method: 本文提出MemoryOS，借鉴操作系统的内存管理原则，设计了分层存储架构，包含四个关键模块：内存存储、更新、检索和生成。架构包括短期、中期和长期个人记忆三个存储单元，并采用动态更新策略。

Result: 在LoCoMo基准测试中，MemoryOS在GPT-4o-mini上相比基线模型，F1和BLEU-1分别平均提升了49.11%和46.18%，展示了其在长对话中的上下文连贯性和个性化记忆保留能力。

Conclusion: MemoryOS通过分层内存集成和动态更新，显著提升了大语言模型的长期记忆能力和个性化交互体验，实验结果表明其有效性。

Abstract: Large Language Models (LLMs) face a crucial challenge from fixed context
windows and inadequate memory management, leading to a severe shortage of
long-term memory capabilities and limited personalization in the interactive
experience with AI agents. To overcome this challenge, we innovatively propose
a Memory Operating System, i.e., MemoryOS, to achieve comprehensive and
efficient memory management for AI agents. Inspired by the memory management
principles in operating systems, MemoryOS designs a hierarchical storage
architecture and consists of four key modules: Memory Storage, Updating,
Retrieval, and Generation. Specifically, the architecture comprises three
levels of storage units: short-term memory, mid-term memory, and long-term
personal memory. Key operations within MemoryOS include dynamic updates between
storage units: short-term to mid-term updates follow a dialogue-chain-based
FIFO principle, while mid-term to long-term updates use a segmented page
organization strategy. Our pioneering MemoryOS enables hierarchical memory
integration and dynamic updating. Extensive experiments on the LoCoMo benchmark
show an average improvement of 49.11% on F1 and 46.18% on BLEU-1 over the
baselines on GPT-4o-mini, showing contextual coherence and personalized memory
retention in long conversations. The implementation code is open-sourced at
https://github.com/BAI-LAB/MemoryOS.

</details>


### [142] [Will artificial agents pursue power by default?](https://arxiv.org/abs/2506.06352)
*Christian Tarsney*

Main category: cs.AI

TL;DR: 本文通过决策理论框架形式化工具性收敛和权力追求的概念，评估了权力作为工具性收敛目标的合理性，认为其具有部分真实性，但预测效用可能有限。


<details>
  <summary>Details</summary>
Motivation: 研究者担心高级AI可能带来灾难性风险，认为足够强大的AI会追求对人类权力的控制，因为权力是一个工具性收敛目标，适用于多种最终目标。然而，近期有人对这些观点表示怀疑。

Method: 本文在抽象的决策理论框架中形式化了工具性收敛和权力追求的概念。

Result: 研究发现，权力作为工具性收敛目标的说法包含一定的真实性，但在缺乏关于AI最终目标的实质性信息时，其预测效用可能有限。然而，对于有可能获得绝对或近乎绝对权力的AI，工具性收敛的预测性更强。

Conclusion: 权力作为工具性收敛目标的说法具有部分真实性，但其预测效用取决于对AI最终目标的了解程度。

Abstract: Researchers worried about catastrophic risks from advanced AI have argued
that we should expect sufficiently capable AI agents to pursue power over
humanity because power is a convergent instrumental goal, something that is
useful for a wide range of final goals. Others have recently expressed
skepticism of these claims. This paper aims to formalize the concepts of
instrumental convergence and power-seeking in an abstract, decision-theoretic
framework, and to assess the claim that power is a convergent instrumental
goal. I conclude that this claim contains at least an element of truth, but
might turn out to have limited predictive utility, since an agent's options
cannot always be ranked in terms of power in the absence of substantive
information about the agent's final goals. However, the fact of instrumental
convergence is more predictive for agents who have a good shot at attaining
absolute or near-absolute power.

</details>


### [143] [Towards Foundation Model on Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2506.06367)
*Jiaxin Pan,Mojtaba Nayyeri,Osama Mohammed,Daniel Hernandez,Rongchuan Zhang,Cheng Cheng,Steffen Staab*

Main category: cs.AI

TL;DR: 本文提出了一种完全归纳的时序知识图谱链接预测方法POSTRA，通过正弦位置编码和消息传递生成自适应实体和关系表示，解决了现有模型在跨领域和真实场景中的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有的时序知识图谱嵌入模型在推理过程中依赖于训练时见过的元素，限制了模型在新领域和真实场景中的泛化能力。本文旨在克服这一限制，提出一种完全归纳的方法。

Method: 本文采用正弦位置编码捕捉细粒度时序模式，并通过基于局部和全局时序上下文的消息传递生成自适应实体和关系表示。模型设计对时序粒度和时间跨度无关，有效解决了跨时序知识图谱的时序差异问题。

Result: POSTRA在未见过的时序知识图谱上表现出强大的零样本性能，能够有效泛化到新实体、关系和时间戳。理论分析和实验结果均表明，单个预训练模型可以提升各种归纳时序推理场景的零样本性能。

Conclusion: POSTRA作为一种预训练、可扩展和可迁移的模型，标志着向时序知识图谱基础模型迈出了重要一步，显著提升了零样本性能。

Abstract: Temporal Knowledge Graphs (TKGs) store temporal facts with quadruple formats
(s, p, o, t). Existing Temporal Knowledge Graph Embedding (TKGE) models perform
link prediction tasks in transductive or semi-inductive settings, which means
the entities, relations, and temporal information in the test graph are fully
or partially observed during training. Such reliance on seen elements during
inference limits the models' ability to transfer to new domains and generalize
to real-world scenarios. A central limitation is the difficulty in learning
representations for entities, relations, and timestamps that are transferable
and not tied to dataset-specific vocabularies. To overcome these limitations,
we introduce the first fully-inductive approach to temporal knowledge graph
link prediction. Our model employs sinusoidal positional encodings to capture
fine-grained temporal patterns and generates adaptive entity and relation
representations using message passing conditioned on both local and global
temporal contexts. Our model design is agnostic to temporal granularity and
time span, effectively addressing temporal discrepancies across TKGs and
facilitating time-aware structural information transfer. As a pretrained,
scalable, and transferable model, POSTRA demonstrates strong zero-shot
performance on unseen temporal knowledge graphs, effectively generalizing to
novel entities, relations, and timestamps. Extensive theoretical analysis and
empirical results show that a single pretrained model can improve zero-shot
performance on various inductive temporal reasoning scenarios, marking a
significant step toward a foundation model for temporal KGs.

</details>


### [144] [SIGMA: Refining Large Language Model Reasoning via Sibling-Guided Monte Carlo Augmentation](https://arxiv.org/abs/2506.06470)
*Yanwei Ren,Haotian Zhang,Fuxiang Wu,Jiayan Qiu,Jiaxing Huang,Baosheng Yu,Liu Liu*

Main category: cs.AI

TL;DR: 提出SIGMA框架，通过重新整合蒙特卡洛树搜索中被丢弃的非最优推理分支，显著减少数据需求并提升大语言模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅保留蒙特卡洛树搜索中的最优轨迹，导致大量包含部分洞见和错误模式的非最优分支数据被浪费。

Method: SIGMA建立搜索路径中兄弟节点的语义关联，通过批判模型识别优劣、修订模型进行文本反向传播的两阶段优化，利用非最优分支反馈增强推理轨迹。

Result: 在MATH基准测试中，仅用3万样本的7B模型达到54.92%准确率，超越使用59万样本的SOTA模型。

Conclusion: 非最优推理分支的深度利用可突破数据规模限制，兄弟节点引导的优化范式为LLM高效训练开辟新路径。

Abstract: Enhancing large language models by simply scaling up datasets has begun to
yield diminishing returns, shifting the spotlight to data quality. Monte Carlo
Tree Search (MCTS) has emerged as a powerful technique for generating
high-quality chain-of-thought data, yet conventional approaches typically
retain only the top-scoring trajectory from the search tree, discarding sibling
nodes that often contain valuable partial insights, recurrent error patterns,
and alternative reasoning strategies. This unconditional rejection of
non-optimal reasoning branches may waste vast amounts of informative data in
the whole search tree. We propose SIGMA (Sibling Guided Monte Carlo
Augmentation), a novel framework that reintegrates these discarded sibling
nodes to refine LLM reasoning. SIGMA forges semantic links among sibling nodes
along each search path and applies a two-stage refinement: a critique model
identifies overlooked strengths and weaknesses across the sibling set, and a
revision model conducts text-based backpropagation to refine the top-scoring
trajectory in light of this comparative feedback. By recovering and amplifying
the underutilized but valuable signals from non-optimal reasoning branches,
SIGMA substantially improves reasoning trajectories. On the challenging MATH
benchmark, our SIGMA-tuned 7B model achieves 54.92% accuracy using only 30K
samples, outperforming state-of-the-art models trained on 590K samples. This
result highlights that our sibling-guided optimization not only significantly
reduces data usage but also significantly boosts LLM reasoning.

</details>


### [145] [Reinforcement Learning for Autonomous Warehouse Orchestration in SAP Logistics Execution: Redefining Supply Chain Agility](https://arxiv.org/abs/2506.06523)
*Sumanth Pillella*

Main category: cs.AI

TL;DR: 本研究提出一种基于强化学习的框架，用于优化SAP物流执行系统的仓库任务，通过实时动态调度提升效率，在合成数据集上实现95%优化准确率及60%处理时间缩减。


<details>
  <summary>Details</summary>
Motivation: 在供应链需求激增背景下，传统仓库管理方法难以应对实时任务分配、多语言数据及运营中断等复杂场景，亟需通过智能算法提升SAP LE系统的操作敏捷性。

Method: 采用强化学习将仓库流程建模为动态环境，构建含30万条多语言交易记录的合成数据集，模拟真实运营场景与中断事件，开发任务分配、库存移动和订单拣选的实时决策模型。

Result: 实验显示任务优化准确率达95%，处理时间较传统方法减少60%，并通过效率热力图等可视化工具验证了策略有效性。

Conclusion: 该框架成功解决SAP系统数据隐私、可扩展性及集成问题，为动态供应链环境提供了兼顾实时响应与资源优化的创新解决方案。

Abstract: In an era of escalating supply chain demands, SAP Logistics Execution (LE) is
pivotal for managing warehouse operations, transportation, and delivery. This
research introduces a pioneering framework leveraging reinforcement learning
(RL) to autonomously orchestrate warehouse tasks in SAP LE, enhancing
operational agility and efficiency. By modeling warehouse processes as dynamic
environments, the framework optimizes task allocation, inventory movement, and
order picking in real-time. A synthetic dataset of 300,000 LE transactions
simulates real-world warehouse scenarios, including multilingual data and
operational disruptions. The analysis achieves 95% task optimization accuracy,
reducing processing times by 60% compared to traditional methods.
Visualizations, including efficiency heatmaps and performance graphs, guide
agile warehouse strategies. This approach tackles data privacy, scalability,
and SAP integration, offering a transformative solution for modern supply
chains.

</details>


### [146] [ScriptDoctor: Automatic Generation of PuzzleScript Games via Large Language Models and Tree Search](https://arxiv.org/abs/2506.06524)
*Sam Earle,Ahmed Khalifa,Muhammad Umair Nasir,Zehua Jiang,Graham Todd,Andrzej Banburski-Fahey,Julian Togelius*

Main category: cs.AI

TL;DR: 论文提出了ScriptDoctor系统，利用大型语言模型（LLM）在高度受限的PuzzleScript语言中自动生成和测试回合制解谜游戏，展示了自动化、开放式LLM工作流程在生成新颖游戏内容中的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前自动游戏设计（AGD）领域对大型预训练模型的应用多基于临时性人工监督，需探索如何将其整合至长期自主测试的流程中，以实现系统与游戏引擎的自动化交互。

Method: ScriptDoctor通过迭代循环生成和测试游戏设计：使用人类示例约束输出，利用PuzzleScript引擎的报错信息修正代码，并通过搜索代理进行自动化试玩测试。

Result: 系统成功验证了基于LLM的自动化流程在受限环境下生成功能性游戏内容的能力，为长周期AGD流程提供了可行案例。

Conclusion: ScriptDoctor证明了自动化LLM驱动工作流在开放游戏内容生成中的潜力，为未来整合生成模型与游戏引擎的自主设计系统提供了参考。

Abstract: There is much interest in using large pre-trained models in Automatic Game
Design (AGD), whether via the generation of code, assets, or more abstract
conceptualization of design ideas. But so far this interest largely stems from
the ad hoc use of such generative models under persistent human supervision.
Much work remains to show how these tools can be integrated into
longer-time-horizon AGD pipelines, in which systems interface with game engines
to test generated content autonomously. To this end, we introduce ScriptDoctor,
a Large Language Model (LLM)-driven system for automatically generating and
testing games in PuzzleScript, an expressive but highly constrained description
language for turn-based puzzle games over 2D gridworlds. ScriptDoctor generates
and tests game design ideas in an iterative loop, where human-authored examples
are used to ground the system's output, compilation errors from the
PuzzleScript engine are used to elicit functional code, and search-based agents
play-test generated games. ScriptDoctor serves as a concrete example of the
potential of automated, open-ended LLM-based workflows in generating novel game
content.

</details>


### [147] [The Optimization Paradox in Clinical AI Multi-Agent Systems](https://arxiv.org/abs/2506.06574)
*Suhana Bedi,Iddah Mlauzi,Daniel Shin,Sanmi Koyejo,Nigam H. Shah*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Multi-agent artificial intelligence systems are increasingly deployed in
clinical settings, yet the relationship between component-level optimization
and system-wide performance remains poorly understood. We evaluated this
relationship using 2,400 real patient cases from the MIMIC-CDM dataset across
four abdominal pathologies (appendicitis, pancreatitis, cholecystitis,
diverticulitis), decomposing clinical diagnosis into information gathering,
interpretation, and differential diagnosis. We evaluated single agent systems
(one model performing all tasks) against multi-agent systems (specialized
models for each task) using comprehensive metrics spanning diagnostic outcomes,
process adherence, and cost efficiency. Our results reveal a paradox: while
multi-agent systems generally outperformed single agents, the
component-optimized or Best of Breed system with superior components and
excellent process metrics (85.5% information accuracy) significantly
underperformed in diagnostic accuracy (67.7% vs. 77.4% for a top multi-agent
system). This finding underscores that successful integration of AI in
healthcare requires not just component level optimization but also attention to
information flow and compatibility between agents. Our findings highlight the
need for end to end system validation rather than relying on component metrics
alone.

</details>


### [148] [AI Simulation by Digital Twins: Systematic Survey, Reference Framework, and Mapping to a Standardized Architecture](https://arxiv.org/abs/2506.06580)
*Xiaoran Liu,Istvan David*

Main category: cs.AI

TL;DR: 本文通过系统调研22项研究，探讨数字孪生如何赋能AI模拟，提出参考框架及架构指南，并指出未来挑战与研究机会。


<details>
  <summary>Details</summary>
Motivation: 现代亚符号AI面临数据量不足与质量低的挑战，数字孪生因其高保真模拟和实时交互能力，为AI模拟提供了新的解决方案。

Method: 系统分析22项数字孪生支持AI模拟的文献，总结技术趋势，构建参考框架并与ISO 23247标准架构进行整合。

Result: 提出数字孪生与AI模拟结合的参考框架及架构指南，验证其与ISO标准的兼容性，并识别技术挑战与未来研究方向。

Conclusion: 数字孪生为AI模拟开辟新路径，但需解决数据集成、模型保真度等问题，未来研究应关注跨领域协同与标准化实践。

Abstract: Insufficient data volume and quality are particularly pressing challenges in
the adoption of modern subsymbolic AI. To alleviate these challenges, AI
simulation uses virtual training environments in which AI agents can be safely
and efficiently developed with simulated, synthetic data. Digital twins open
new avenues in AI simulation, as these high-fidelity virtual replicas of
physical systems are equipped with state-of-the-art simulators and the ability
to further interact with the physical system for additional data collection. In
this article, we report on our systematic survey of digital twin-enabled AI
simulation. By analyzing 22 primary studies, we identify technological trends
and derive a reference framework to situate digital twins and AI components.
Based on our findings, we derive a reference framework and provide
architectural guidelines by mapping it onto the ISO 23247 reference
architecture for digital twins. Finally, we identify challenges and research
opportunities for prospective researchers.

</details>


### [149] [GELD: A Unified Neural Model for Efficiently Solving Traveling Salesman Problems Across Different Scales](https://arxiv.org/abs/2506.06634)
*Yubin Xiao,Di Wang,Rui Cao,Xuan Wu,Boyang Li,You Zhou*

Main category: cs.AI

TL;DR: 提出新型神经求解器GELD，通过全局评估与局部选择框架，首次实现单模型高效处理不同规模TSP问题，支持74万节点大规模求解且无需分治策略。


<details>
  <summary>Details</summary>
Motivation: 现有神经TSP求解器使用同一预训练参数时，难以兼顾小规模与大规模问题的求解效率，限制实际应用价值。需开发兼具泛化性和扩展性的统一模型。

Method: GELD融合轻量级全局编码器(GE)与重量级局部解码器(LD)：GE采用低复杂度注意力机制加速推理；LD增强局部决策；配合两阶段多尺度训练策略提升泛化能力。

Result: 实验表明GELD在解质量与推理速度上超越7个SOTA模型，作为后处理方法可显著提升现有求解器效果，首次实现744,710节点TSP的直接求解。

Conclusion: GELD通过全局-局部协同框架突破神经求解器的规模限制，建立单模型多尺度求解新范式，为实际工程应用提供高效解决方案。

Abstract: The Traveling Salesman Problem (TSP) is a well-known combinatorial
optimization problem with broad real-world applications. Recent advancements in
neural network-based TSP solvers have shown promising results. Nonetheless,
these models often struggle to efficiently solve both small- and large-scale
TSPs using the same set of pre-trained model parameters, limiting their
practical utility. To address this issue, we introduce a novel neural TSP
solver named GELD, built upon our proposed broad global assessment and refined
local selection framework. Specifically, GELD integrates a lightweight
Global-view Encoder (GE) with a heavyweight Local-view Decoder (LD) to enrich
embedding representation while accelerating the decision-making process.
Moreover, GE incorporates a novel low-complexity attention mechanism, allowing
GELD to achieve low inference latency and scalability to larger-scale TSPs.
Additionally, we propose a two-stage training strategy that utilizes training
instances of different sizes to bolster GELD's generalization ability.
Extensive experiments conducted on both synthetic and real-world datasets
demonstrate that GELD outperforms seven state-of-the-art models considering
both solution quality and inference speed. Furthermore, GELD can be employed as
a post-processing method to significantly elevate the quality of the solutions
derived by existing neural TSP solvers via spending affordable additional
computing time. Notably, GELD is shown as capable of solving TSPs with up to
744,710 nodes, first-of-its-kind to solve this large size TSP without relying
on divide-and-conquer strategies to the best of our knowledge.

</details>


### [150] [Contextual Experience Replay for Self-Improvement of Language Agents](https://arxiv.org/abs/2506.06698)
*Yitao Liu,Chenglei Si,Karthik Narasimhan,Shunyu Yao*

Main category: cs.AI

TL;DR: 本文提出了一种无需训练的框架Contextual Experience Replay (CER)，通过积累和合成过去的经验，帮助语言模型在复杂任务中自我改进，提升适应性。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型在复杂任务中表现不佳，且无法在推理时持续学习过去的经验。本文旨在解决这一问题，帮助模型在特定环境中获得经验并提升表现。

Method: 提出Contextual Experience Replay (CER)框架，通过动态记忆缓冲区积累和合成过去的经验，使模型能够在新的任务中检索并增强相关知识。

Result: 在VisualWebArena和WebArena基准测试中，CER分别取得了31.9%和36.7%的成功率，相对GPT-4o基线提升了51.0%。

Conclusion: CER框架有效提升了语言模型在复杂任务中的适应性，证明了其高效性和有效性。

Abstract: Large language model (LLM) agents have been applied to sequential
decision-making tasks such as web navigation, but without any
environment-specific experiences, they often fail in these complex tasks.
Moreover, current LLM agents are not designed to continually learn from past
experiences during inference time, which could be crucial for them to gain
these environment-specific experiences. To address this, we propose Contextual
Experience Replay (CER), a training-free framework to enable efficient
self-improvement for language agents in their context window. Specifically, CER
accumulates and synthesizes past experiences into a dynamic memory buffer.
These experiences encompass environment dynamics and common decision-making
patterns, allowing the agents to retrieve and augment themselves with relevant
knowledge in new tasks, enhancing their adaptability in complex environments.
We evaluate CER on the challenging WebArena and VisualWebArena benchmarks. On
VisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena,
CER also gets a competitive average success rate of 36.7%, relatively improving
the success rate of the GPT-4o agent baseline by 51.0%. We also conduct a
comprehensive analysis on it to prove its efficiency, validity and understand
it better.

</details>


### [151] [Integrating AI Planning Semantics into SysML System Models for Automated PDDL File Generation](https://arxiv.org/abs/2506.06714)
*Hamied Nabizada,Tom Jeleniewski,Lasse Beers,Maximilian Weigand,Felix Gehlhoff,Alexander Fay*

Main category: cs.AI

TL;DR: 本文提出一种SysML配置文件，将PDDL规划语义直接集成到系统模型中，通过可重用构造型与OCL约束确保语法一致性，并通过飞机制造案例验证了模型生成PDDL描述及优化执行计划的能力。


<details>
  <summary>Details</summary>
Motivation: 解决系统建模与AI规划在工程设计中的割裂问题，提供自动化、基于模型的规划描述生成方法，以桥接系统建模与AI规划工具链。

Method: 基于PDDL 3.1的BNF定义开发SysML配置文件，定义可重用构造型（如类型、谓词、动作）并添加OCL约束；通过飞机制造案例的机器人系统建模生成PDDL领域与问题描述。

Result: 案例中成功生成PDDL格式输入并驱动求解器输出优化执行计划，验证了配置文件在自动化生成规划描述和系统-规划协同设计中的有效性。

Conclusion: 该方法为工程系统模型与AI规划语义提供了可重用集成框架，支持模型驱动的规划自动化，增强了SysML在复杂系统设计中的表达能力。

Abstract: This paper presents a SysML profile that enables the direct integration of
planning semantics based on the Planning Domain Definition Language (PDDL) into
system models. Reusable stereotypes are defined for key PDDL concepts such as
types, predicates, functions and actions, while formal OCL constraints ensure
syntactic consistency. The profile was derived from the Backus-Naur Form (BNF)
definition of PDDL 3.1 to align with SysML modeling practices. A case study
from aircraft manufacturing demonstrates the application of the profile: a
robotic system with interchangeable end effectors is modeled and enriched to
generate both domain and problem descriptions in PDDL format. These are used as
input to a PDDL solver to derive optimized execution plans. The approach
supports automated and model-based generation of planning descriptions and
provides a reusable bridge between system modeling and AI planning in
engineering design.

</details>


### [152] [WorldLLM: Improving LLMs' world modeling using curiosity-driven theory-making](https://arxiv.org/abs/2506.06725)
*Guillaume Levy,Cedric Colas,Pierre-Yves Oudeyer,Thomas Carta,Clement Romac*

Main category: cs.AI

TL;DR: WorldLLM框架结合贝叶斯推断与强化学习，通过自然语言假设迭代优化和主动探索，提升LLM在结构化环境中的预测能力，并在文本游戏实验中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）虽具备通用知识，但在结构化领域（如模拟环境）中难以生成精确预测，因其无法将宽泛的非结构化知识与具体环境结合。

Method: 提出WorldLLM框架：1）利用LLM的上下文学习能力，通过自然语言假设引导预测；2）基于贝叶斯推断迭代优化假设；3）使用好奇心驱动的强化学习策略主动探索环境，收集低似然证据以更新模型。

Result: 在需要操作物体的文本游戏环境中，WorldLLM显著提升预测准确性，并生成人类可解释的环境动态理论。

Conclusion: WorldLLM通过贝叶斯优化与强化学习的结合，实现了LLM在结构化环境中的自主持续改进，为可解释世界建模提供了新方法。

Abstract: Large Language Models (LLMs) possess general world knowledge but often
struggle to generate precise predictions in structured, domain-specific
contexts such as simulations. These limitations arise from their inability to
ground their broad, unstructured understanding in specific environments. To
address this, we present WorldLLM, a framework that enhances LLM-based world
modeling by combining Bayesian inference and autonomous active exploration with
reinforcement learning. WorldLLM leverages the in-context learning abilities of
LLMs to guide an LLM-based world model's predictions using natural language
hypotheses given in its prompt. These hypotheses are iteratively refined
through a Bayesian inference framework that leverages a second LLM as the
proposal distribution given collected evidence. This evidence is collected
using a curiosity-driven reinforcement learning policy that explores the
environment to find transitions with a low log-likelihood under our LLM-based
predictive model using the current hypotheses. By alternating between refining
hypotheses and collecting new evidence, our framework autonomously drives
continual improvement of the predictions. Our experiments demonstrate the
effectiveness of WorldLLM in a textual game environment that requires agents to
manipulate and combine objects. The framework not only enhances predictive
accuracy, but also generates human-interpretable theories of environment
dynamics.

</details>


### [153] [VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs](https://arxiv.org/abs/2506.06727)
*Can Li,Ting Zhang,Mei Wang,Hua Huang*

Main category: cs.AI

TL;DR: 本文介绍了VisioMath，一个用于评估多模态环境下数学推理能力的基准数据集，发现现有大型多模态模型在图像选项的数学推理任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 尽管大型多模态模型（LMMs）在多个领域展示了强大的问题解决能力，但在答案选项以图像形式呈现的数学推理任务上，其能力尚未得到充分探索。

Method: 作者提出了VisioMath，一个包含8,070张图像和1,800道多选题的数据集，每个答案选项均为图像，专门用于评估多模态环境下的数学推理能力。

Result: 实验表明，即使是目前最先进的模型（如GPT-4o）在该任务上的准确率也仅为45.9%，凸显了现有模型在处理视觉相似答案选项时的局限性。

Conclusion: VisioMath填补了现有基准的空白，为未来多模态推理研究提供了严格的测试平台，推动了该领域的进一步发展。

Abstract: Large Multimodal Models (LMMs) have demonstrated remarkable problem-solving
capabilities across various domains. However, their ability to perform
mathematical reasoning when answer options are represented as images--an
essential aspect of multi-image comprehension--remains underexplored. To bridge
this gap, we introduce VisioMath, a benchmark designed to evaluate mathematical
reasoning in multimodal contexts involving image-based answer choices.
VisioMath comprises 8,070 images and 1,800 multiple-choice questions, where
each answer option is an image, presenting unique challenges to existing LMMs.
To the best of our knowledge, VisioMath is the first dataset specifically
tailored for mathematical reasoning in image-based-option scenarios, where
fine-grained distinctions between answer choices are critical for accurate
problem-solving. We systematically evaluate state-of-the-art LMMs on VisioMath
and find that even the most advanced models struggle with this task. Notably,
GPT-4o achieves only 45.9% accuracy, underscoring the limitations of current
models in reasoning over visually similar answer choices. By addressing a
crucial gap in existing benchmarks, VisioMath establishes a rigorous testbed
for future research, driving advancements in multimodal reasoning.

</details>


### [154] [Honey, I shrunk the hypothesis space (through logical preprocessing)](https://arxiv.org/abs/2506.06739)
*Andrew Cropper,Filipe Gouveia,David M. Cerna*

Main category: cs.AI

TL;DR: 本文提出了一种在归纳逻辑编程（ILP）中缩小假设空间的方法，通过背景知识排除不可能出现在最优假设中的规则，从而显著减少学习时间并保持预测准确性。


<details>
  <summary>Details</summary>
Motivation: 归纳逻辑编程（ILP）的目标是从假设空间中搜索能够泛化训练示例和背景知识的假设。然而，假设空间通常很大，搜索效率低下。本文旨在通过缩小假设空间来提高ILP系统的效率。

Method: 本文提出了一种方法，利用背景知识找到不可能出现在最优假设中的规则（如“偶数不可能是奇数”），并将这些规则从假设空间中移除。该方法通过答案集编程实现，并应用于基于约束的ILP系统。

Result: 实验表明，该方法在多个领域（如视觉推理和游戏）中显著减少了学习时间，同时保持了预测准确性。例如，仅需10秒的预处理时间，学习时间从超过10小时减少到仅2秒。

Conclusion: 本文提出的方法通过缩小假设空间，显著提高了ILP系统的效率，同时保持了预测准确性，展示了其在多种应用场景中的潜力。

Abstract: Inductive logic programming (ILP) is a form of logical machine learning. The
goal is to search a hypothesis space for a hypothesis that generalises training
examples and background knowledge. We introduce an approach that 'shrinks' the
hypothesis space before an ILP system searches it. Our approach uses background
knowledge to find rules that cannot be in an optimal hypothesis regardless of
the training examples. For instance, our approach discovers relationships such
as "even numbers cannot be odd" and "prime numbers greater than 2 are odd". It
then removes violating rules from the hypothesis space. We implement our
approach using answer set programming and use it to shrink the hypothesis space
of a constraint-based ILP system. Our experiments on multiple domains,
including visual reasoning and game playing, show that our approach can
substantially reduce learning times whilst maintaining predictive accuracies.
For instance, given just 10 seconds of preprocessing time, our approach can
reduce learning times from over 10 hours to only 2 seconds.

</details>


### [155] [AI PsyRoom: Artificial Intelligence Platform for Segmented Yearning and Reactive Outcome Optimization Method](https://arxiv.org/abs/2506.06740)
*Yigui Feng,Qinglin Wang,Ke Liu,Xinhai Chen,Bo Yang,Jie Liu*

Main category: cs.AI

TL;DR: AI PsyRoom是一个多代理模拟框架，旨在通过生成具有情感细微差别的对话来增强心理咨询，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于心理健康服务需求增加和专业心理咨询师短缺，心理咨询面临巨大挑战，现有大语言模型在情感理解和个性化治疗计划生成方面存在不足。

Method: 提出AI PsyRoom框架，利用细粒度情感分类和多代理框架，构建多代理PsyRoom A用于对话重建，生成高质量对话数据集EmoPsy，并提出PsyRoom B用于生成个性化治疗计划。

Result: AI PsyRoom在问题导向、表达、共情和互动沟通质量方面显著优于现有方法，分别提升18%、23%、24%和16%。

Conclusion: AI PsyRoom为AI辅助心理咨询研究提供了基础，其数据集和模型已公开，推动了该领域的进展。

Abstract: Psychological counseling faces huge challenges due to the growing demand for
mental health services and the shortage of trained professionals. Large
language models (LLMs) have shown potential to assist psychological counseling,
especially in empathy and emotional support. However, existing models lack a
deep understanding of emotions and are unable to generate personalized
treatment plans based on fine-grained emotions. To address these shortcomings,
we present AI PsyRoom, a multi-agent simulation framework designed to enhance
psychological counseling by generating empathetic and emotionally nuanced
conversations. By leveraging fine-grained emotion classification and a
multi-agent framework, we construct a multi-agent PsyRoom A for dialogue
reconstruction, generating a high-quality dialogue dataset EmoPsy, which
contains 35 sub-emotions, 423 specific emotion scenarios, and 12,350 dialogues.
We also propose PsyRoom B for generating personalized treatment plans.
Quantitative evaluations demonstrate that AI PsyRoom significantly outperforms
state-of-the-art methods, achieving 18% improvement in problem orientation, 23%
in expression, 24% in Empathy, and 16% in interactive communication quality.
The datasets and models are publicly available, providing a foundation for
advancing AI-assisted psychological counseling research.

</details>


### [156] [Bio-Inspired Classification: Combining Information Theory and Spiking Neural Networks -- Influence of the Learning Rules](https://arxiv.org/abs/2506.06750)
*Zofia Rudnicka,Janusz Szczepanski,Agnieszka Pregowska*

Main category: cs.AI

TL;DR: 该论文探讨了脉冲神经网络（SNN）训练中不同学习算法对分类准确性的影响，提出了一种结合SNN与Lempel-Ziv复杂度（LZC）的生物启发分类器，并比较了传统反向传播与生物启发算法（如tempotron和Spikprop）在计算效率与准确性上的权衡。


<details>
  <summary>Details</summary>
Motivation: SNN的训练因时间动态性、脉冲不可微分性及稀疏事件驱动特性而具有挑战性。研究旨在探索不同学习算法（尤其是生物启发规则）对分类准确性的影响，并寻求高效且适用于实时场景的解决方案。

Method: 提出一种结合SNN与Lempel-Ziv复杂度（LZC）的生物启发分类器，利用SNN的时序精度和生物真实性，结合LZC的结构复杂性分析，对时空神经数据进行高效且可解释的分类。同时对比了反向传播、tempotron和Spikprop等算法的性能。

Result: 传统反向传播算法分类精度高但计算成本极高，不适合实时应用；生物启发算法（如tempotron和Spikprop）在保持竞争力的分类性能的同时显著提升计算效率，适用于时间敏感任务。

Conclusion: 选择SNN学习算法需权衡分类精度与计算成本，并结合具体约束条件。生物启发算法在实时场景中更具实用性，而反向传播适用于对精度要求极高且资源充足的情况。

Abstract: Training of Spiking Neural Networks (SNN) is challenging due to their unique
properties, including temporal dynamics, non-differentiability of spike events,
and sparse event-driven activations. In this paper, we widely consider the
influence of the type of chosen learning algorithm, including bioinspired
learning rules on the accuracy of classification. We proposed a bioinspired
classifier based on the combination of SNN and Lempel-Ziv complexity (LZC).
This approach synergizes the strengths of SNNs in temporal precision and
biological realism with LZC's structural complexity analysis, facilitating
efficient and interpretable classification of spatiotemporal neural data. It
turned out that the classic backpropagation algorithm achieves excellent
classification accuracy, but at extremely high computational cost, which makes
it impractical for real-time applications. Biologically inspired learning
algorithms such as tempotron and Spikprop provide increased computational
efficiency while maintaining competitive classification performance, making
them suitable for time-sensitive tasks. The results obtained indicate that the
selection of the most appropriate learning algorithm depends on the trade-off
between classification accuracy and computational cost as well as application
constraints.

</details>


### [157] [Learning What Matters Now: A Dual-Critic Context-Aware RL Framework for Priority-Driven Information Gain](https://arxiv.org/abs/2506.06786)
*Dimitris Panagopoulos,Adolfo Perrusquia,Weisi Guo*

Main category: cs.AI

TL;DR: 提出CA-MIQ强化学习框架，通过双评论家机制动态调整探索策略，在搜救任务优先级变化时实现高效适应。


<details>
  <summary>Details</summary>
Motivation: 高风险搜救任务中，自主系统需在持续收集关键信息的同时快速适应优先级变化，现有方法在动态调整能力上存在不足。

Method: CA-MIQ采用轻量级双评论家架构：外在评论家处理任务奖励，内在评论家融合状态新颖性、信息位置感知与实时优先级对齐，通过转移检测器触发探索增强。

Result: 在模拟SAR网格环境中，CA-MIQ单次优先级切换后任务成功率比基线高4倍，多次切换后成功率3倍并实现100%恢复，基准方法完全失效。

Conclusion: CA-MIQ在具有分段稳态信息价值分布的离散环境中表现卓越，验证了其在动态任务场景下的强适应性。

Abstract: Autonomous systems operating in high-stakes search-and-rescue (SAR) missions
must continuously gather mission-critical information while flexibly adapting
to shifting operational priorities. We propose CA-MIQ (Context-Aware
Max-Information Q-learning), a lightweight dual-critic reinforcement learning
(RL) framework that dynamically adjusts its exploration strategy whenever
mission priorities change. CA-MIQ pairs a standard extrinsic critic for task
reward with an intrinsic critic that fuses state-novelty, information-location
awareness, and real-time priority alignment. A built-in shift detector triggers
transient exploration boosts and selective critic resets, allowing the agent to
re-focus after a priority revision. In a simulated SAR grid-world, where
experiments specifically test adaptation to changes in the priority order of
information types the agent is expected to focus on, CA-MIQ achieves nearly
four times higher mission-success rates than baselines after a single priority
shift and more than three times better performance in multiple-shift scenarios,
achieving 100% recovery while baseline methods fail to adapt. These results
highlight CA-MIQ's effectiveness in any discrete environment with
piecewise-stationary information-value distributions.

</details>


### [158] [Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures](https://arxiv.org/abs/2506.06832)
*Clément Hongler,Andrew Emil*

Main category: cs.AI

TL;DR: 本文提出了一种基于大语言模型（LLMs）的交叉熵（Xent）游戏框架，用于评估LLMs的能力，并探索其在生成、总结、反事实思考等任务中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型（LLMs）如何定义文本的概率分布，并探讨LLMs在生成任务之外的多种能力，如总结、反事实思考、异常检测等。

Method: 提出交叉熵（Xent）游戏框架，包括单人和多人游戏，涉及交叉熵分数和约束，并通过基本博弈论一致性公理构建游戏空间。

Result: 展示了Xent游戏空间包含丰富的有趣示例，并提出了Xent游戏度量，用于构建能力基准。

Conclusion: 通过进化动力学思想，探索Xent游戏空间，为解决测量LLMs一般能力的无界范围问题提供了新思路。

Abstract: Large Language Models (LLMs) define probability measures on text. By
considering the implicit knowledge question of what it means for an LLM to know
such a measure and what it entails algorithmically, we are naturally led to
formulate a series of tasks that go beyond generative sampling, involving forms
of summarization, counterfactual thinking, anomaly detection, originality
search, reverse prompting, debating, creative solving, etc. These tasks can be
formulated as games based on LLM measures, which we call Cross-Entropy (Xent)
Games. Xent Games can be single-player or multi-player. They involve
cross-entropy scores and cross-entropy constraints, and can be expressed as
simple computational graphs and programs. We show the Xent Game space is large
enough to contain a wealth of interesting examples, while being constructible
from basic game-theoretic consistency axioms. We then discuss how the Xent Game
space can be used to measure the abilities of LLMs. This leads to the
construction of Xent Game measures: finite families of Xent Games that can be
used as capability benchmarks, built from a given scope, by extracting a
covering measure. To address the unbounded scope problem associated with the
challenge of measuring general abilities, we propose to explore the space of
Xent Games in a coherent fashion, using ideas inspired by evolutionary
dynamics.

</details>


### [159] [United Minds or Isolated Agents? Exploring Coordination of LLMs under Cognitive Load Theory](https://arxiv.org/abs/2506.06843)
*HaoYang Shang,Xuan Liu,Zi Liang,Jie Zhang,Haibo Hu,Song Guo*

Main category: cs.AI

TL;DR: 本文提出CoThinker框架，通过多代理系统减轻大语言模型在复杂任务中的认知负荷，提升问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理复杂、多面任务时存在性能瓶颈，主要由于任务需求超出其认知负荷能力。

Method: 引入CoThinker框架，通过代理分工和结构化通信来分配和管理认知负荷。

Result: 实验验证CoThinker在复杂问题解决任务中优于现有多代理基线，提高了解决方案的质量和效率。

Conclusion: CoThinker框架为克服大语言模型性能瓶颈提供了原则性方法，展示了集体认知和有效负荷管理的特征。

Abstract: Large Language Models (LLMs) exhibit a notable performance ceiling on
complex, multi-faceted tasks, as they often fail to integrate diverse
information or adhere to multiple constraints. We posit that such limitation
arises when the demands of a task exceed the LLM's effective cognitive load
capacity. This interpretation draws a strong analogy to Cognitive Load Theory
(CLT) in cognitive science, which explains similar performance boundaries in
the human mind, and is further supported by emerging evidence that reveals LLMs
have bounded working memory characteristics. Building upon this CLT-grounded
understanding, we introduce CoThinker, a novel LLM-based multi-agent framework
designed to mitigate cognitive overload and enhance collaborative
problem-solving abilities. CoThinker operationalizes CLT principles by
distributing intrinsic cognitive load through agent specialization and managing
transactional load via structured communication and a collective working
memory. We empirically validate CoThinker on complex problem-solving tasks and
fabricated high cognitive load scenarios, demonstrating improvements over
existing multi-agent baselines in solution quality and efficiency. Our analysis
reveals characteristic interaction patterns, providing insights into the
emergence of collective cognition and effective load management, thus offering
a principled approach to overcoming LLM performance ceilings.

</details>


### [160] [Incorporating Failure of Machine Learning in Dynamic Probabilistic Safety Assurance](https://arxiv.org/abs/2506.06868)
*Razieh Arshadizadeh,Mahmoud Asgari,Zeinab Khosravi,Yiannis Papadopoulos,Koorosh Aslansefat*

Main category: cs.AI

TL;DR: 提出一种结合SafeML与贝叶斯网络的概率安全框架，通过动态检测机器学习分布偏移并建模故障因果链，实现自动驾驶编队系统的实时安全评估与自适应调整。


<details>
  <summary>Details</summary>
Motivation: 传统安全评估方法依赖代码/设计产物，无法应对数据驱动的机器学习组件因分布偏移引发的推理故障，需建立动态安全验证机制。

Method: 整合SafeML的分布偏移动态检测能力与贝叶斯网络的因果推理，构建概率化安全模型，将ML故障显式纳入系统级安全分析框架。

Result: 在交通标志识别的车辆编队仿真中验证，证明该方法能有效评估不确定性下的系统安全状态并触发适应性调整。

Conclusion: 显式建模机器学习故障的因果影响可提升安全关键系统的动态安全保障能力，为ML系统安全认证提供新范式。

Abstract: Machine Learning (ML) models are increasingly integrated into safety-critical
systems, such as autonomous vehicle platooning, to enable real-time
decision-making. However, their inherent imperfection introduces a new class of
failure: reasoning failures often triggered by distributional shifts between
operational and training data. Traditional safety assessment methods, which
rely on design artefacts or code, are ill-suited for ML components that learn
behaviour from data. SafeML was recently proposed to dynamically detect such
shifts and assign confidence levels to the reasoning of ML-based components.
Building on this, we introduce a probabilistic safety assurance framework that
integrates SafeML with Bayesian Networks (BNs) to model ML failures as part of
a broader causal safety analysis. This allows for dynamic safety evaluation and
system adaptation under uncertainty. We demonstrate the approach on an
simulated automotive platooning system with traffic sign recognition. The
findings highlight the potential broader benefits of explicitly modelling ML
failures in safety assessment.

</details>


### [161] [KnowCoder-V2: Deep Knowledge Analysis](https://arxiv.org/abs/2506.06881)
*Zixuan Li,Wenxuan Liu,Long Bai,Chunmao Zhang,Wei Li,Fenghui Zhang,Quanxin Jin,Ruoyun He,Zhuo Chen,Zhilei Hu,Fei Wang,Bingbing Xu,Xuhui Jiang,Xiaolong Jin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.AI

TL;DR: 本文提出了一种知识驱动的深度研究框架（KDR），通过引入独立的知识组织阶段和复杂的知识计算步骤，解决了现有框架在知识管理、在线操作和复杂计算方面的不足。进一步提出了KCII模型，通过统一的代码生成桥接知识组织与推理，提升了LLM在知识分析任务中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究框架在知识管理、在线操作和复杂计算方面存在不足，无法有效处理大规模知识共享和复杂知识计算任务，因此需要一种新的框架来提升深度知识分析能力。

Method: 提出KDR框架，引入独立的知识组织阶段，将大规模领域相关数据预处理为系统化知识，并扩展深度研究框架以支持在线复杂知识计算。进一步提出KCII模型，通过统一的代码生成桥接知识组织与推理。

Result: 在六个知识分析任务的三十多个数据集上的实验结果表明，KCII模型在KDR框架中能够生成高质量的报告，并提供深入的分析结果，优于主流深度研究框架。

Conclusion: KDR框架和KCII模型有效提升了深度知识分析能力，能够生成高质量的分析报告，解决了现有框架在知识管理和复杂计算方面的不足。

Abstract: Deep knowledge analysis tasks always involve the systematic extraction and
association of knowledge from large volumes of data, followed by logical
reasoning to discover insights. However, to solve such complex tasks, existing
deep research frameworks face three major challenges: 1) They lack systematic
organization and management of knowledge; 2) They operate purely online, making
it inefficient for tasks that rely on shared and large-scale knowledge; 3) They
cannot perform complex knowledge computation, limiting their abilities to
produce insightful analytical results. Motivated by these, in this paper, we
propose a \textbf{K}nowledgeable \textbf{D}eep \textbf{R}esearch (\textbf{KDR})
framework that empowers deep research with deep knowledge analysis capability.
Specifically, it introduces an independent knowledge organization phase to
preprocess large-scale, domain-relevant data into systematic knowledge offline.
Based on this knowledge, it extends deep research with an additional kind of
reasoning steps that perform complex knowledge computation in an online manner.
To enhance the abilities of LLMs to solve knowledge analysis tasks in the above
framework, we further introduce \textbf{\KCII}, an LLM that bridges knowledge
organization and reasoning via unified code generation. For knowledge
organization, it generates instantiation code for predefined classes,
transforming data into knowledge objects. For knowledge computation, it
generates analysis code and executes on the above knowledge objects to obtain
deep analysis results. Experimental results on more than thirty datasets across
six knowledge analysis tasks demonstrate the effectiveness of \KCII. Moreover,
when integrated into the KDR framework, \KCII can generate high-quality reports
with insightful analytical results compared to the mainstream deep research
framework.

</details>


### [162] [Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering](https://arxiv.org/abs/2506.06905)
*Akash Gupta,Amos Storkey,Mirella Lapata*

Main category: cs.AI

TL;DR: 本文提出一种基于元学习的软提示蒸馏方法，通过注意力映射模块增强小型多模态模型在少样本场景下的任务适应能力，解决了传统上下文学习中信息过载导致的性能不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有小型多模态模型在上下文学习(ICL)中表现不稳定，且增加示例数量无法保证性能提升。作者认为图像嵌入中的冗余信息干扰了任务表现，需设计更有效的少样本学习机制。

Method: 提出元学习方法：1) 从任务相关图像特征中蒸馏固定软提示；2) 引入可集成于LLaVA v1.5架构的注意力映射模块；3) 通过联合学习实现低数据场景下的快速梯度适配。

Result: 在VL-ICL基准测试中，该方法在视觉问答任务上持续优于传统ICL和提示调优方法，抗图像干扰能力强，任务诱导准确率提升显著。

Conclusion: 通过软提示蒸馏与注意力映射的联合优化，实现了多模态模型在少样本场景下的高效任务适应，为低资源环境下的模型部署提供了新思路。

Abstract: Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to
perform new tasks with minimal supervision. However, ICL performance,
especially in smaller LMMs, is inconsistent and does not always improve
monotonically with increasing examples. We hypothesize that this occurs due to
the LMM being overwhelmed by additional information present in the image
embeddings, which is not required for the downstream task. To address this, we
propose a meta-learning approach that provides an alternative for inducing
few-shot capabilities in LMMs, using a fixed set of soft prompts that are
distilled from task-relevant image features and can be adapted at test time
using a few examples. To facilitate this distillation, we introduce an
attention-mapper module that can be easily integrated with the popular LLaVA
v1.5 architecture and is jointly learned with soft prompts, enabling task
adaptation in LMMs under low-data regimes with just a few gradient steps.
Evaluation on the VL-ICL Bench shows that our method consistently outperforms
ICL and related prompt-tuning approaches, even under image perturbations,
improving task induction and reasoning across visual question answering tasks.

</details>


### [163] [Causal Graph based Event Reasoning using Semantic Relation Experts](https://arxiv.org/abs/2506.06910)
*Mahnaz Koupaee,Xueying Bai,Mudan Chen,Greg Durrett,Nathanael Chambers,Niranjan Balasubramanian*

Main category: cs.AI

TL;DR: 本文提出了一种通过生成因果事件图来帮助大语言模型（LLMs）显式表示因果关系的协作方法，并在多个下游任务中展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在事件推理方面取得了进展，但在准确识别事件之间的因果关系方面仍存在困难，这影响了事件预测和时间线理解等深层推理任务的表现。

Method: 本文提出了一种协作方法，利用LLMs模拟专注于特定语义关系的专家，通过多轮讨论生成因果事件图，并由最终专家进行整合。

Result: 该方法在未对下游任务进行微调的情况下，在事件预测和下一事件预测任务中取得了与最先进模型竞争的结果，并生成了更具信息性和连贯性的解释。

Conclusion: 通过生成因果事件图，本文的方法有效提升了LLMs在事件推理任务中的表现，展示了因果图在辅助推理中的潜力。

Abstract: Understanding how events in a scenario causally connect with each other is
important for effectively modeling and reasoning about events. But event
reasoning remains a difficult challenge, and despite recent advances, Large
Language Models (LLMs) still struggle to accurately identify causal connections
between events. This struggle leads to poor performance on deeper reasoning
tasks like event forecasting and timeline understanding. To address this
challenge, we investigate the generation of causal event graphs (e.g., A
enables B) as a parallel mechanism to help LLMs explicitly represent causality
during inference. This paper evaluates both how to generate correct graphs as
well as how graphs can assist reasoning. We propose a collaborative approach to
causal graph generation where we use LLMs to simulate experts that focus on
specific semantic relations. The experts engage in multiple rounds of
discussions which are then consolidated by a final expert. Then, to demonstrate
the utility of causal graphs, we use them on multiple downstream applications,
and also introduce a new explainable event prediction task that requires a
causal chain of events in the explanation. These explanations are more
informative and coherent than baseline generations. Finally, our overall
approach not finetuned on any downstream task, achieves competitive results
with state-of-the-art models on both forecasting and next event prediction
tasks.

</details>


### [164] [Boosting LLM Reasoning via Spontaneous Self-Correction](https://arxiv.org/abs/2506.06923)
*Xutong Zhao,Tengyu Xu,Xuewei Wang,Zhengxing Chen,Di Jin,Liang Tan,Yen-Ting,Zishun Yu,Zhuokai Zhao,Yun He,Sinong Wang,Han Fang,Sarath Chandar,Chen Zhu*

Main category: cs.AI

TL;DR: 本文提出了一种名为SPOC的自发自我纠正方法，通过单次推理生成交错解决方案和验证，显著提升了大型语言模型在数学推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在许多任务上表现出色，数学推理仍然是一个挑战。现有的自我纠正方法依赖额外的提示和系统设计来引发纠正，而不是在单次推理中实时自发地进行纠正。

Method: SPOC方法通过为同一模型分配双重角色（解决方案提出者和验证者），在单次推理中生成交错的解决方案和验证，并根据验证结果动态终止生成。此外，通过生成合成数据进行微调，并结合在线强化学习，进一步提高了模型的解决方案提出和验证准确性。

Result: 实验表明，SPOC显著提升了Llama-3.1-8B和70B Instruct模型在多个数学推理基准上的表现，如MATH500、AMC23和AIME24，准确率分别提升了8.8%、11.6%、10.0%、20.0%、3.3%和6.7%。

Conclusion: SPOC方法通过自发自我纠正和单次推理中的动态验证，有效提升了大型语言模型在数学推理任务中的性能，展示了其在复杂任务中的潜力。

Abstract: While large language models (LLMs) have demonstrated remarkable success on a
broad range of tasks, math reasoning remains a challenging one. One of the
approaches for improving math reasoning is self-correction, which designs
self-improving loops to let the model correct its own mistakes. However,
existing self-correction approaches treat corrections as standalone
post-generation refinements, relying on extra prompt and system designs to
elicit self-corrections, instead of performing real-time, spontaneous
self-corrections in a single pass. To address this, we propose SPOC, a
spontaneous self-correction approach that enables LLMs to generate interleaved
solutions and verifications in a single inference pass, with generation
dynamically terminated based on verification outcomes, thereby effectively
scaling inference time compute. SPOC considers a multi-agent perspective by
assigning dual roles -- solution proposer and verifier -- to the same model. We
adopt a simple yet effective approach to generate synthetic data for
fine-tuning, enabling the model to develop capabilities for self-verification
and multi-agent collaboration. We further improve its solution proposal and
verification accuracy through online reinforcement learning. Experiments on
mathematical reasoning benchmarks show that SPOC significantly improves
performance. Notably, SPOC boosts the accuracy of Llama-3.1-8B and 70B Instruct
models, achieving gains of 8.8% and 11.6% on MATH500, 10.0% and 20.0% on AMC23,
and 3.3% and 6.7% on AIME24, respectively.

</details>


### [165] [An Agentic Framework for Autonomous Metamaterial Modeling and Inverse Design](https://arxiv.org/abs/2506.06935)
*Darui Lu,Jordan M. Malof,Willie J. Padilla*

Main category: cs.AI

TL;DR: 本文开发了一种基于多大型语言模型整合的自主代理框架，用于光子超材料的逆向设计。该框架能自动化提出前向模型、调用外部工具进行模拟优化，并通过深度逆向方法生成最终设计，展示了其自主推理、规划及适应能力。


<details>
  <summary>Details</summary>
Motivation: 通过整合多个LLM系统，构建能够自主执行复杂科学任务（如光子超材料逆向设计）的代理框架，以解决传统方法在自动化、灵活性和创新性上的局限性。

Method: 代理接收目标光学光谱后，自主开发前向深度学习模型，通过API调用外部工具进行仿真与优化，利用记忆模块，并采用深度逆向方法生成最终设计。框架具备内部反思和动态决策能力。

Result: 框架成功实现了光子超材料逆向设计的全流程，表现出高效的自动化、推理与适应能力，其内部决策机制支持多样化且可能具有创新性的输出。

Conclusion: 该代理框架在光子超材料逆向设计中验证了其自主性与灵活性，为复杂科学任务的自动化提供了新范式，并可能扩展至其他跨领域研究。

Abstract: Recent significant advances in integrating multiple Large Language Model
(LLM) systems have enabled Agentic Frameworks capable of performing complex
tasks autonomously, including novel scientific research. We develop and
demonstrate such a framework specifically for the inverse design of photonic
metamaterials. When queried with a desired optical spectrum, the Agent
autonomously proposes and develops a forward deep learning model, accesses
external tools via APIs for tasks like simulation and optimization, utilizes
memory, and generates a final design via a deep inverse method. The framework's
effectiveness is demonstrated in its ability to automate, reason, plan, and
adapt. Notably, the Agentic Framework possesses internal reflection and
decision flexibility, permitting highly varied and potentially novel outputs.

</details>


### [166] [The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://arxiv.org/abs/2506.06941)
*Parshin Shojaee,Iman Mirzadeh,Keivan Alizadeh,Maxwell Horton,Samy Bengio,Mehrdad Farajtabar*

Main category: cs.AI

TL;DR: 研究通过可控谜题环境系统分析大型推理模型（LRMs）的能力、扩展性及局限性，发现LRMs在复杂度超过阈值时准确率崩溃，存在反直觉的扩展限制，并划分出三种性能区间。


<details>
  <summary>Details</summary>
Motivation: 当前对LRMs的评估主要依赖数学和编程基准测试，存在答案污染问题且无法深入分析推理过程。需系统性研究其核心能力、扩展规律及缺陷。

Method: 使用可精确调控复杂度且保持逻辑一致性的可控谜题环境，分析模型最终答案及内部推理轨迹，并与标准LLM在相同计算资源下对比。

Result: LRMs在超阈值复杂度时完全失效，推理努力先增后降；划分出低复杂度标准模型优、中复杂度LRMs优、高复杂度双失效的三区间；揭示LRMs无法使用显式算法且跨规模推理不一致。

Conclusion: LRMs存在严格计算限制，其推理能力具有显著边界。需重新审视模型真实推理机制，当前「思维链」可能未实现真正的算法化推理。

Abstract: Recent generations of language models have introduced Large Reasoning Models
(LRMs) that generate detailed thinking processes before providing answers.
While these models demonstrate improved performance on reasoning benchmarks,
their fundamental capabilities, scaling properties, and limitations remain
insufficiently understood. Current evaluations primarily focus on established
math and coding benchmarks, emphasizing final answer accuracy. However, this
evaluation paradigm often suffers from contamination and does not provide
insights into the reasoning traces. In this work, we systematically investigate
these gaps with the help of controllable puzzle environments that allow precise
manipulation of complexity while maintaining consistent logical structures.
This setup enables the analysis of not only final answers but also the internal
reasoning traces, offering insights into how LRMs think. Through extensive
experiments, we show that LRMs face a complete accuracy collapse beyond certain
complexities. Moreover, they exhibit a counterintuitive scaling limit: their
reasoning effort increases with problem complexity up to a point, then declines
despite having remaining token budget. By comparing LRMs with their standard
LLM counterparts under same inference compute, we identify three performance
regimes: (1) low-complexity tasks where standard models outperform LRMs, (2)
medium-complexity tasks where LRMs demonstrates advantage, and (3)
high-complexity tasks where both models face complete collapse. We found that
LRMs have limitations in exact computation: they fail to use explicit
algorithms and reason inconsistently across scales. We also investigate the
reasoning traces in more depth, studying the patterns of explored solutions and
analyzing the models' computational behavior, shedding light on their
strengths, limitations, and raising questions about their reasoning
capabilities.

</details>


### [167] [Deontically Constrained Policy Improvement in Reinforcement Learning Agents](https://arxiv.org/abs/2506.06959)
*Alena Makarova,Houssam Abbas*

Main category: cs.AI

TL;DR: 本文提出了一种在满足义务逻辑约束的情况下，最大化效用的决策策略学习方法。


<details>
  <summary>Details</summary>
Motivation: 研究如何在强化学习中，既最大化效用，又满足伦理、社会或情境约束。

Method: 使用期望行为功利主义逻辑，开发了一种策略改进方法，确保在约束下达到局部最优。

Result: 实验证明，该方法能够在样本MDP中实现约束下的效用最大化。

Conclusion: 该方法能够同时最大化两个价值函数，其中一个隐含在双层结构中，有效处理了约束与效用的平衡。

Abstract: Markov Decision Processes (MDPs) are the most common model for decision
making under uncertainty in the Machine Learning community. An MDP captures
non-determinism, probabilistic uncertainty, and an explicit model of action. A
Reinforcement Learning (RL) agent learns to act in an MDP by maximizing a
utility function. This paper considers the problem of learning a decision
policy that maximizes utility subject to satisfying a constraint expressed in
deontic logic. In this setup, the utility captures the agent's mission - such
as going quickly from A to B. The deontic formula represents (ethical, social,
situational) constraints on how the agent might achieve its mission by
prohibiting classes of behaviors. We use the logic of Expected Act
Utilitarianism, a probabilistic stit logic that can be interpreted over
controlled MDPs. We develop a variation on policy improvement, and show that it
reaches a constrained local maximum of the mission utility. Given that in stit
logic, an agent's duty is derived from value maximization, this can be seen as
a way of acting to simultaneously maximize two value functions, one of which is
implicit, in a bi-level structure. We illustrate these results with experiments
on sample MDPs.

</details>


### [168] [Long-Tailed Learning for Generalized Category Discovery](https://arxiv.org/abs/2506.06965)
*Cuong Manh Hoang*

Main category: cs.AI

TL;DR: 本文提出一种新框架，用于解决长尾分布下的广义类别发现问题。通过自引导标签技术和表示平衡过程，减少分类器偏差并提升尾部类别的识别能力，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有广义类别发现方法在平衡数据集上表现良好，但无法有效处理现实世界中普遍存在的长尾分布数据，导致分类效果下降。

Method: 结合自引导标签技术（通过可学习分布生成伪标签减少偏差）和表示平衡过程（挖掘样本邻域以增强尾部类别表征）构建框架。

Result: 在公开数据集上的实验表明，该模型显著超越此前最优方法。

Conclusion: 所提框架通过减少分类偏差与优化表征学习，有效解决了长尾分布下的广义类别发现问题，具有实际应用价值。

Abstract: Generalized Category Discovery (GCD) utilizes labeled samples of known
classes to discover novel classes in unlabeled samples. Existing methods show
effective performance on artificial datasets with balanced distributions.
However, real-world datasets are always imbalanced, significantly affecting the
effectiveness of these methods. To solve this problem, we propose a novel
framework that performs generalized category discovery in long-tailed
distributions. We first present a self-guided labeling technique that uses a
learnable distribution to generate pseudo-labels, resulting in less biased
classifiers. We then introduce a representation balancing process to derive
discriminative representations. By mining sample neighborhoods, this process
encourages the model to focus more on tail classes. We conduct experiments on
public datasets to demonstrate the effectiveness of the proposed framework. The
results show that our model exceeds previous state-of-the-art methods.

</details>


### [169] [Deep RL Needs Deep Behavior Analysis: Exploring Implicit Planning by Model-Free Agents in Open-Ended Environments](https://arxiv.org/abs/2506.06981)
*Riley Simmons-Edler,Ryan P. Badman,Felix Baastad Berg,Raymond Chua,John J. Vastola,Joshua Lunger,William Qian,Kanaka Rajan*

Main category: cs.AI

TL;DR: 本文提出一种受神经行为学启发的分析框架，通过构建复杂觅食环境ForageWorld，揭示无模型DRL智能体可通过循环神经网络涌现类规划行为，强调跨学科方法对理解AI行为及安全对齐的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习（DRL）智能体行为分析方法不足，需超越简单奖励曲线比较，开发能解析复杂任务中行为与神经动态关联的工具。

Method: 构建模拟真实动物觅食的复杂部分可观测环境ForageWorld，整合神经科学和动物行为学工具，对基于RNN的无模型DRL智能体进行联合行为-神经动态分析。

Result: 发现无模型RNN-DRL智能体可通过涌现动态实现结构化类规划行为，无需显式记忆模块；开发出可复用的行为-表征特征诊断框架。

Conclusion: 借鉴生物智能研究方法（神经科学+认知科学），可揭示DRL智能体隐藏的学习动态结构，这对确保AI安全对齐及优化难以通过奖励衡量的行为至关重要。

Abstract: Understanding the behavior of deep reinforcement learning (DRL) agents --
particularly as task and agent sophistication increase -- requires more than
simple comparison of reward curves, yet standard methods for behavioral
analysis remain underdeveloped in DRL. We apply tools from neuroscience and
ethology to study DRL agents in a novel, complex, partially observable
environment, ForageWorld, designed to capture key aspects of real-world animal
foraging -- including sparse, depleting resource patches, predator threats, and
spatially extended arenas. We use this environment as a platform for applying
joint behavioral and neural analysis to agents, revealing detailed,
quantitatively grounded insights into agent strategies, memory, and planning.
Contrary to common assumptions, we find that model-free RNN-based DRL agents
can exhibit structured, planning-like behavior purely through emergent dynamics
-- without requiring explicit memory modules or world models. Our results show
that studying DRL agents like animals -- analyzing them with
neuroethology-inspired tools that reveal structure in both behavior and neural
dynamics -- uncovers rich structure in their learning dynamics that would
otherwise remain invisible. We distill these tools into a general analysis
framework linking core behavioral and representational features to diagnostic
methods, which can be reused for a wide range of tasks and agents. As agents
grow more complex and autonomous, bridging neuroscience, cognitive science, and
AI will be essential -- not just for understanding their behavior, but for
ensuring safe alignment and maximizing desirable behaviors that are hard to
measure via reward. We show how this can be done by drawing on lessons from how
biological intelligence is studied.

</details>


### [170] [Evaluating LLM-corrupted Crowdsourcing Data Without Ground Truth](https://arxiv.org/abs/2506.06991)
*Yichi Zhang,Jinlong Pang,Zhaowei Zhu,Yang Liu*

Main category: cs.AI

TL;DR: 本文提出一种基于同行预测的无训练评分机制，用于检测众包标注任务中LLM辅助的低效作弊行为，通过量化工人答案间的相关性并结合LLM生成标签，在真实数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着众包工作者广泛使用大语言模型（LLM），标注任务中本应反映人类反馈的数据集可能被LLM生成内容污染，而传统基于高维文本的LLM检测方法不适用于标注任务（如多选标注）。

Method: 提出基于同行预测的机制，通过量化工人答案间相关性（考虑LLM生成标签的子集），构建无需训练的评分方法，并理论证明其在存在LLM共谋的众包模型下的有效性。

Result: 实证结果表明该方法在真实众包数据集上能有效检测低努力作弊行为，且在特定条件下（如LLM共谋）具有鲁棒性。

Conclusion: 同行预测机制为众包标注任务中的LLM作弊问题提供了理论可靠且无需训练的解决方案，其有效性依赖于工人答案间信息相关性及LLM标签的可用性。

Abstract: The recent success of generative AI highlights the crucial role of
high-quality human feedback in building trustworthy AI systems. However, the
increasing use of large language models (LLMs) by crowdsourcing workers poses a
significant challenge: datasets intended to reflect human input may be
compromised by LLM-generated responses. Existing LLM detection approaches often
rely on high-dimension training data such as text, making them unsuitable for
annotation tasks like multiple-choice labeling. In this work, we investigate
the potential of peer prediction -- a mechanism that evaluates the information
within workers' responses without using ground truth -- to mitigate
LLM-assisted cheating in crowdsourcing with a focus on annotation tasks. Our
approach quantifies the correlations between worker answers while conditioning
on (a subset of) LLM-generated labels available to the requester. Building on
prior research, we propose a training-free scoring mechanism with theoretical
guarantees under a crowdsourcing model that accounts for LLM collusion. We
establish conditions under which our method is effective and empirically
demonstrate its robustness in detecting low-effort cheating on real-world
crowdsourcing datasets.

</details>


### [171] [Mathesis: Towards Formal Theorem Proving from Natural Languages](https://arxiv.org/abs/2506.07047)
*Yu Xuejun,Jianyuan Zhong,Zijin Feng,Pengyi Zhai,Roozbeh Yousefzadeh,Wei Chong Ng,Haoxiong Liu,Ziyi Shou,Jing Xiong,Yudong Zhou,Claudia Beth Ong,Austen Jeremy Sugiarto,Yaoxi Zhang,Wai Ming Tai,Huan Cao,Dongcai Lu,Jiacheng Sun,Qiang Xu,Shen Xin,Zhenguo Li*

Main category: cs.AI

TL;DR: Mathesis提出首个端到端定理证明框架，结合强化学习自动形式化与新型评估方法，显著提升自然语言数学问题的形式化与证明能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的定理证明器依赖专家编写的形式化命题输入，限制了其处理自然语言描述的现实问题能力。

Method: 设计Mathesis-Autoformalizer（基于强化学习的自动形式化器）与Mathesis-Prover（形式化证明生成器），配合LeanScorer形式化质量评估框架，构建端到端流程。

Result: 自动形式化器在Gaokao-Formal基准上超越最佳基线22%通过率，全系统在MiniF2F达到64%准确率（pass@32），在Gaokao-Formal实现18%的SOTA性能。

Conclusion: Mathesis证明了端到端形式定理证明在现实问题中的可行性，其创新评估框架与系统设计为自然语言形式推理开辟新路径。

Abstract: Recent advances in large language models show strong promise for formal
reasoning. However, most LLM-based theorem provers have long been constrained
by the need for expert-written formal statements as inputs, limiting their
applicability to real-world problems expressed in natural language. We tackle
this gap with Mathesis, the first end-to-end theorem proving pipeline
processing informal problem statements. It contributes Mathesis-Autoformalizer,
the first autoformalizer using reinforcement learning to enhance the
formalization ability of natural language problems, aided by our novel
LeanScorer framework for nuanced formalization quality assessment. It also
proposes a Mathesis-Prover, which generates formal proofs from the formalized
statements. To evaluate the real-world applicability of end-to-end formal
theorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex
problems from China's national college entrance exam. Our approach is carefully
designed, with a thorough study of each component. Experiments demonstrate
Mathesis's effectiveness, with the autoformalizer outperforming the best
baseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other
model combinations, achieving 64% accuracy on MiniF2F with pass@32 and a
state-of-the-art 18% on Gaokao-Formal.

</details>


### [172] [Reasoning Paths as Signals: Augmenting Multi-hop Fact Verification through Structural Reasoning Progression](https://arxiv.org/abs/2506.07075)
*Liwen Zheng,Chaozhuo Li,Haoran Jia,Xi Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种结构化推理框架，用于多跳事实验证，通过显式建模推理路径为结构化图，提升证据检索和验证的准确性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中事实声明的复杂性增加，现有自动化事实验证系统在准确聚合和推理多跳证据方面存在挑战，导致检索碎片化和解释性有限。

Method: 提出了一种结构化推理框架，包含两个关键模块：结构增强的检索机制和推理路径引导的验证模块，并结合结构感知的推理机制捕捉多跳证据链的长程依赖。

Result: 在FEVER和HoVer数据集上的实验表明，该方法在检索精度和验证准确性上均优于现有基线。

Conclusion: 推理路径建模在提升多跳事实验证系统的检索和验证性能方面具有显著效果。

Abstract: The growing complexity of factual claims in real-world scenarios presents
significant challenges for automated fact verification systems, particularly in
accurately aggregating and reasoning over multi-hop evidence. Existing
approaches often rely on static or shallow models that fail to capture the
evolving structure of reasoning paths, leading to fragmented retrieval and
limited interpretability. To address these issues, we propose a Structural
Reasoning framework for Multi-hop Fact Verification that explicitly models
reasoning paths as structured graphs throughout both evidence retrieval and
claim verification stages. Our method comprises two key modules: a
structure-enhanced retrieval mechanism that constructs reasoning graphs to
guide evidence collection, and a reasoning-path-guided verification module that
incrementally builds subgraphs to represent evolving inference trajectories. We
further incorporate a structure-aware reasoning mechanism that captures
long-range dependencies across multi-hop evidence chains, enabling more precise
verification. Extensive experiments on the FEVER and HoVer datasets demonstrate
that our approach consistently outperforms strong baselines, highlighting the
effectiveness of reasoning-path modeling in enhancing retrieval precision and
verification accuracy.

</details>


### [173] [BRIGHT+: Upgrading the BRIGHT Benchmark with MARCUS, a Multi-Agent RAG Clean-Up Suite](https://arxiv.org/abs/2506.07116)
*Liyang Chen,Yujun Cai,Jieqiong Dong,Yiwei Wang*

Main category: cs.AI

TL;DR: 本文提出MARCUS多智能体流程，通过LLM清理并重构BRIGHT基准数据集中的噪声内容，生成更高质量的BRIGHT-Plus语料库，显著提升多跳检索与推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有BRIGHT基准因网络爬取数据冗余、语义不连续等问题（尤其StackExchange子域），导致检索精度与下游推理受限，需改进其语料质量。

Method: MARCUS采用多智能体协作框架，分别负责结构噪声消除与语义分割，保留答案相关文本片段并增强上下文连贯性。

Result: 实验表明BRIGHT-Plus在多种检索模型上均显著提升检索准确率与多跳推理能力，且改进效果具有普适性。

Conclusion: 通过系统性语料清洗与重组，BRIGHT-Plus为复杂推理任务提供更鲁棒的基准，开源数据与流程支持后续检索增强生成研究。

Abstract: Retrieval-Augmented Generation (RAG) systems require corpora that are both
structurally clean and semantically coherent. BRIGHT is a recent and
influential benchmark designed to evaluate complex multi-hop retrieval across
diverse, high-reasoning domains. However, its practical effectiveness is
limited by common web-crawled artifacts - such as content redundancy and
semantic discontinuity - that impair retrieval accuracy and downstream
reasoning. Notably, we find that such issues are concentrated in seven
StackExchange-derived subdomains, while other domains (e.g., Coding and
Theorem-based content) remain relatively clean.
  In this study, we present MARCUS, a multi-agent pipeline that leverages large
language models (LLMs) to systematically clean and re-chunk BRIGHT into a
higher-quality corpus: BRIGHT-Plus. MARCUS applies dedicated agents for
structural noise removal and semantic segmentation, preserving answer-bearing
spans while improving contextual integrity. Experimental evaluations
demonstrate that BRIGHT-Plus yields consistent and significant improvements in
both retrieval accuracy and multi-hop reasoning across a diverse set of
retrievers. We release both the BRIGHT-Plus corpus and the MARCUS pipeline to
support future research on robust, reasoning-centric retrieval.

</details>


### [174] [Translating Federated Learning Algorithms in Python into CSP Processes Using ChatGPT](https://arxiv.org/abs/2506.07173)
*Miroslav Popovic,Marko Popovic,Miodrag Djukic,Ilija Basicevic*

Main category: cs.AI

TL;DR: 本文提出一种利用ChatGPT将Python联邦学习算法自动转换为CSP进程的方法，并通过模型检查器PAT验证，减少人工干预并确保算法正确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法需手动将联邦学习算法转换为CSP进程以验证安全性和活性，过程繁琐且依赖专业编程能力。本文旨在通过ChatGPT自动化此翻译过程，降低开发门槛并提升效率。

Method: 使用ChatGPT自动翻译Python联邦学习算法为CSP进程，结合反馈估计上下文最小性，并利用模型检查器PAT验证翻译结果的正确性。

Result: 实验成功将集中式与去中心化联邦学习算法自动转换为CSP进程，且PAT验证表明翻译结果符合安全性与活性要求。

Conclusion: 通过ChatGPT实现联邦学习算法到CSP的自动化翻译可行，验证了该方法在简化开发流程及提升验证效率方面的潜力。

Abstract: The Python Testbed for Federated Learning Algorithms is a simple Python FL
framework that is easy to use by ML&AI developers who do not need to be
professional programmers and is also amenable to LLMs. In the previous
research, generic federated learning algorithms provided by this framework were
manually translated into the CSP processes and algorithms' safety and liveness
properties were automatically verified by the model checker PAT. In this paper,
a simple translation process is introduced wherein the ChatGPT is used to
automate the translation of the mentioned federated learning algorithms in
Python into the corresponding CSP processes. Within the process, the minimality
of the used context is estimated based on the feedback from ChatGPT. The
proposed translation process was experimentally validated by successful
translation (verified by the model checker PAT) of both generic centralized and
decentralized federated learning algorithms.

</details>


### [175] [Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images](https://arxiv.org/abs/2506.07184)
*Liangliang You,Junchi Yao,Shu Yang,Guimin Hu,Lijie Hu,Di Wang*

Main category: cs.AI

TL;DR: 本文针对多模态大语言模型中序列图像的行为性幻觉问题，提出轻量级两阶段框架SHE，通过视觉-文本对齐检测和正交投影缓解幻觉，并引入BEACH指标量化严重程度。实验表明SHE在BEACH上降低10%以上行为幻觉，同时保持描述准确性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注多模态模型的客观性幻觉，但序列图像中的行为性幻觉研究不足。行为性幻觉由先验驱动偏差和雪球效应引发，影响模型可靠性，需针对性解决方案。

Method: 提出SHE框架：1) 使用自适应时间窗口进行视觉-文本对齐检测幻觉；2) 通过联合嵌入空间正交投影缓解幻觉。同时设计BEACH指标量化行为性幻觉严重程度。

Result: 在标准基准测试中，SHE将行为性幻觉在BEACH指标上降低超过10%，同时保持描述准确性，验证了框架有效性。

Conclusion: SHE框架有效解决了序列图像中的行为性幻觉问题，填补了该领域研究空白，BEACH指标为后续研究提供了量化评估工具。

Abstract: While multimodal large language models excel at various tasks, they still
suffer from hallucinations, which limit their reliability and scalability for
broader domain applications. To address this issue, recent research mainly
focuses on objective hallucination. However, for sequential images, besides
objective hallucination, there is also behavioral hallucination, which is less
studied. This work aims to fill in the gap. We first reveal that behavioral
hallucinations mainly arise from two key factors: prior-driven bias and the
snowball effect. Based on these observations, we introduce SHE (Sequence
Hallucination Eradication), a lightweight, two-stage framework that (1) detects
hallucinations via visual-textual alignment check using our proposed adaptive
temporal window and (2) mitigates them via orthogonal projection onto the joint
embedding space. We also propose a new metric (BEACH) to quantify behavioral
hallucination severity. Empirical results on standard benchmarks demonstrate
that SHE reduces behavioral hallucination by over 10% on BEACH while
maintaining descriptive accuracy.

</details>


### [176] [Exploring Effective Strategies for Building a Customised GPT Agent for Coding Classroom Dialogues](https://arxiv.org/abs/2506.07194)
*Luwei Bai,Dongkeun Han,Sara Hennessy*

Main category: cs.AI

TL;DR: 研究探讨如何利用GPT-4的MyGPT代理开发定制化课堂对话编码工具，提出在有限数据下优化代理性能的策略，使其成为有效的编码助手。


<details>
  <summary>Details</summary>
Motivation: 课堂对话分析因需复杂理解且人工编码耗时，现有大模型或固定编码方案不适用于小数据集或定制需求，需探索基于小数据的可行方法。

Method: 以MyGPT代理为案例，评估其基线性能，通过变量控制法分析不同示例输入对编码效果的影响，并采用设计研究法总结配置策略。

Result: 发现基于MyGPT特性配置的策略可提升编码效果，代理能生成有效建议，但存在局限性（如依赖示例质量）。

Conclusion: 通过特定策略开发的MyGPT代理可作为实用编码助手，辅助处理课堂对话分析，为小规模定制化研究提供新途径。

Abstract: This study investigates effective strategies for developing a customised GPT
agent to code classroom dialogue. While classroom dialogue is widely recognised
as a crucial element of education, its analysis remains challenging due to the
need for a nuanced understanding of dialogic functions and the labour-intensive
nature of manual transcript coding. Recent advancements in large language
models offer promising avenues for automating this process. However, existing
studies predominantly focus on training large-scale models or evaluating
pre-trained models with fixed codebooks, which are often not applicable or
replicable for dialogue researchers working with small datasets or customised
coding schemes. Using GPT-4's MyGPT agent as a case, this study evaluates its
baseline performance in coding classroom dialogue with a human codebook and
examines how performance varies with different example inputs through a
variable control method. Through a design-based research approach, it
identifies a set of practical strategies, based on MyGPT's unique features, for
configuring effective agents with limited data. The findings suggest that,
despite some limitations, a MyGPT agent developed with these strategies can
serve as a useful coding assistant by generating coding suggestions.

</details>


### [177] [Reasoning Multimodal Large Language Model: Data Contamination and Dynamic Evaluation](https://arxiv.org/abs/2506.07202)
*Ming Liu,Wensheng Zhang*

Main category: cs.AI

TL;DR: 提出动态任务扰动框架评估多模态大语言模型（MLLM）的泛化能力，发现微调模拟测试数据会提升特定任务表现但削弱整体泛化。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在视觉语言基准测试中表现优异，但数据污染（测试集在训练中泄露）可能掩盖其真实泛化能力。基于潜在污染数据微调的推理模型也存在类似风险，需开发新评估方法以区分真实理解与过拟合或数据泄漏。

Method: 通过任务扰动（如对同一视觉输入进行问答、标题生成、问题提出、验证等多任务评估）构建动态评估框架，结合自动流程和校准评分机制（使用释义与干扰采样）分析模型跨任务的“能力向量”，类比损失函数曲率分析泛化鲁棒性。

Result: 在MME、RealWorldQA等基准测试中，微调模拟污染数据会显著提升单任务性能但损害整体泛化；动态任务扰动可揭示模型是否依赖任务特定表面线索，而非真实理解。

Conclusion: 动态任务扰动方法为MLLM泛化能力评估提供更深刻洞察，可有效区分真实理解与数据泄漏/过拟合，强调传统静态基准的局限性。

Abstract: Multimodal Large Language Models (MLLMs) show impressive vision-language
benchmark performance, yet growing concerns about data contamination (test set
exposure during training) risk masking true generalization. This concern
extends to reasoning MLLMs, often fine-tuned via reinforcement learning from
potentially contaminated base models. We propose a novel dynamic evaluation
framework to rigorously assess MLLM generalization, moving beyond static
benchmarks. Instead of perturbing inputs, we perturb the task itself. Using the
same visual input, models are evaluated across a family of tasks (e.g., QA,
captioning, question posing, verification) to probe diverse capabilities. This
task perturbation reveals whether model performance is robust or reliant on
superficial task-specific cues. Our approach is analogous to loss landscape
sharpness: models overfit or contaminated for a single task (sharp minima)
falter under task shifts, unlike models with generalizable solutions (flatter
minima). We developed an automated pipeline with a calibrated judge scoring
open-ended generations (captions, questions) using paraphrase and corruption
sampling. Applying this framework to leading image/video MLLMs on benchmarks
including MME, RealWorldQA, and CVRR-ES, we analyze each model's cross-task
"ability vector." We demonstrate that fine-tuning on simulated test data
(extreme contamination) drastically sharpens task-specific performance but
harms overall generalization. Our dynamic task perturbation offers deeper
insights into MLLM generalization, distinguishing genuine understanding from
spurious leakage or overfitting.

</details>


### [178] [BIMgent: Towards Autonomous Building Modeling via Computer-use Agents](https://arxiv.org/abs/2506.07217)
*Zihan Deng,Changyu Du,Stavros Nousias,André Borrmann*

Main category: cs.AI

TL;DR: 本文提出BIMgent框架，利用多模态大语言模型实现建筑信息模型（BIM）的自主化建模，在真实任务中达到32%的成功率，显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有计算机代理主要集中于通用桌面自动化任务，而在AEC领域高度专业化的3D建筑建模（涉及开放设计任务和复杂BIM软件交互）中缺乏深入研究。

Method: BIMgent通过多模态输入处理概念设计、规划软件特定工作流、执行图形界面操作，构建基于多模态大语言模型的自主建模框架。

Result: 实验显示BIMgent在真实建模任务中成功率为32%（基线模型为0%），设计质量合理，且能有效减少人工工作量并保持设计意图。

Conclusion: BIMgent证明了在复杂建筑建模场景中的实际应用潜力，为专业领域自动化代理提供了可行解决方案。

Abstract: Existing computer-use agents primarily focus on general-purpose desktop
automation tasks, with limited exploration of their application in highly
specialized domains. In particular, the 3D building modeling process in the
Architecture, Engineering, and Construction (AEC) sector involves open-ended
design tasks and complex interaction patterns within Building Information
Modeling (BIM) authoring software, which has yet to be thoroughly addressed by
current studies. In this paper, we propose BIMgent, an agentic framework
powered by multimodal large language models (LLMs), designed to enable
autonomous building model authoring via graphical user interface (GUI)
operations. BIMgent automates the architectural building modeling process,
including multimodal input for conceptual design, planning of software-specific
workflows, and efficient execution of the authoring GUI actions. We evaluate
BIMgent on real-world building modeling tasks, including both text-based
conceptual design generation and reconstruction from existing building design.
The design quality achieved by BIMgent was found to be reasonable. Its
operations achieved a 32% success rate, whereas all baseline models failed to
complete the tasks (0% success rate). Results demonstrate that BIMgent
effectively reduces manual workload while preserving design intent,
highlighting its potential for practical deployment in real-world architectural
modeling scenarios.

</details>


### [179] [LLM-Enhanced Rapid-Reflex Async-Reflect Embodied Agent for Real-Time Decision-Making in Dynamically Changing Environments](https://arxiv.org/abs/2506.07223)
*Yangqing Zheng,Shunqi Mao,Dingxin Zhang,Weidong Cai*

Main category: cs.AI

TL;DR: 本文提出了一种时间转换机制（TCM）和快速反射异步反思代理（RRARA），用于在动态变化的高风险场景中优化代理决策延迟。


<details>
  <summary>Details</summary>
Motivation: 在动态变化的高风险场景中，决策延迟是一个关键但研究不足的问题，本文旨在解决这一问题。

Method: 提出了时间转换机制（TCM）将决策延迟转换为等效的模拟帧，并引入了响应延迟（RL）和延迟动作比（LAR）来评估代理性能。同时，提出了快速反射异步反思代理（RRARA），结合轻量级LLM引导的反馈模块和基于规则的代理，实现即时反应和异步反思。

Result: 在HAZARD基准测试中，RRARA在延迟敏感场景中显著优于现有基线。

Conclusion: 本文提出的方法有效解决了高风险场景中的决策延迟问题，显著提升了代理性能。

Abstract: In the realm of embodied intelligence, the evolution of large language models
(LLMs) has markedly enhanced agent decision making. Consequently, researchers
have begun exploring agent performance in dynamically changing high-risk
scenarios, i.e., fire, flood, and wind scenarios in the HAZARD benchmark. Under
these extreme conditions, the delay in decision making emerges as a crucial yet
insufficiently studied issue. We propose a Time Conversion Mechanism (TCM) that
translates inference delays in decision-making into equivalent simulation
frames, thus aligning cognitive and physical costs under a single FPS-based
metric. By extending HAZARD with Respond Latency (RL) and Latency-to-Action
Ratio (LAR), we deliver a fully latency-aware evaluation protocol. Moreover, we
present the Rapid-Reflex Async-Reflect Agent (RRARA), which couples a
lightweight LLM-guided feedback module with a rule-based agent to enable
immediate reactive behaviors and asynchronous reflective refinements in situ.
Experiments on HAZARD show that RRARA substantially outperforms existing
baselines in latency-sensitive scenarios.

</details>


### [180] [Subgoal-Guided Policy Heuristic Search with Learned Subgoals](https://arxiv.org/abs/2506.07255)
*Jake Tuero,Michael Buro,Levi H. S. Lelis*

Main category: cs.AI

TL;DR: 本文提出了一种新的子目标策略学习方法，用于提升策略树搜索算法的样本效率。


<details>
  <summary>Details</summary>
Motivation: 传统的策略树搜索算法在训练过程中需要完整的解轨迹，且在处理复杂问题时学习成本极高，尤其是在策略随机初始化的情况下，搜索样本会被浪费在失败尝试中。

Method: 提出了一种基于子目标的策略学习方法，通过搜索过程中扩展的树（包括失败尝试的搜索树）来学习子目标和条件策略。

Result: 实验表明，该方法在在线学习策略和启发式函数时显著提高了样本效率。

Conclusion: 本文的方法有效解决了策略树搜索算法在复杂问题上的学习成本问题，提升了样本利用效率。

Abstract: Policy tree search is a family of tree search algorithms that use a policy to
guide the search. These algorithms provide guarantees on the number of
expansions required to solve a given problem that are based on the quality of
the policy. While these algorithms have shown promising results, the process in
which they are trained requires complete solution trajectories to train the
policy. Search trajectories are obtained during a trial-and-error search
process. When the training problem instances are hard, learning can be
prohibitively costly, especially when starting from a randomly initialized
policy. As a result, search samples are wasted in failed attempts to solve
these hard instances. This paper introduces a novel method for learning
subgoal-based policies for policy tree search algorithms. The subgoals and
policies conditioned on subgoals are learned from the trees that the search
expands while attempting to solve problems, including the search trees of
failed attempts. We empirically show that our policy formulation and training
method improve the sample efficiency of learning a policy and heuristic
function in this online setting.

</details>


### [181] [Boosting Vulnerability Detection of LLMs via Curriculum Preference Optimization with Synthetic Reasoning Data](https://arxiv.org/abs/2506.07390)
*Xin-Cheng Wen,Yijun Yang,Cuiyun Gao,Yang Xiao,Deheng Ye*

Main category: cs.AI

TL;DR: 本文提出ReVD框架，通过合成漏洞推理数据和优化偏好学习，显著提升大语言模型在软件漏洞检测中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在漏洞检测中存在两大问题：缺乏漏洞推理数据导致无法捕捉底层模式，以及过度关注语义而非漏洞原因导致无法识别语义相似漏洞样本。高质量数据稀缺进一步制约了专用模型的开发。

Method: ReVD框架包含：(1) 构建漏洞代码及其修复代码的前向/后向推理过程以合成高质量数据；(2) 采用三元组监督微调与课程在线偏好优化策略，增强模型对漏洞模式的理解。

Result: 在PrimeVul和SVEN数据集上的实验表明，ReVD将LLM漏洞检测准确率提升12.24%-22.77%，达到当前最优水平。

Conclusion: ReVD通过系统性推理数据生成与针对性优化策略，有效解决了LLM在漏洞检测中的核心限制，为高质量数据稀缺环境下的安全应用提供了新方案。

Abstract: Large language models (LLMs) demonstrate considerable proficiency in numerous
coding-related tasks; however, their capabilities in detecting software
vulnerabilities remain limited. This limitation primarily stems from two
factors: (1) the absence of reasoning data related to vulnerabilities, which
hinders the models' ability to capture underlying vulnerability patterns; and
(2) their focus on learning semantic representations rather than the reason
behind them, thus failing to recognize semantically similar vulnerability
samples. Furthermore, the development of LLMs specialized in vulnerability
detection is challenging, particularly in environments characterized by the
scarcity of high-quality datasets. In this paper, we propose a novel framework
ReVD that excels at mining vulnerability patterns through reasoning data
synthesizing and vulnerability-specific preference optimization. Specifically,
we construct forward and backward reasoning processes for vulnerability and
corresponding fixed code, ensuring the synthesis of high-quality reasoning
data. Moreover, we design the triplet supervised fine-tuning followed by
curriculum online preference optimization for enabling ReVD to better
understand vulnerability patterns. The extensive experiments conducted on
PrimeVul and SVEN datasets demonstrate that ReVD sets new state-of-the-art for
LLM-based software vulnerability detection, e.g., 12.24\%-22.77\% improvement
in the accuracy. The source code and data are available at
https://github.com/Xin-Cheng-Wen/PO4Vul.

</details>


### [182] [An Intelligent Fault Self-Healing Mechanism for Cloud AI Systems via Integration of Large Language Models and Deep Reinforcement Learning](https://arxiv.org/abs/2506.07411)
*Ze Yang,Yihong Jin,Juntian Liu,Xinhe Xu*

Main category: cs.AI

TL;DR: 本文提出一种结合大语言模型（LLM）与深度强化学习（DRL）的智能故障自愈机制（IFSHM），通过两阶段混合架构实现云AI系统的语义化故障识别与动态恢复策略优化，实验表明其恢复效率较现有方法提升37%。


<details>
  <summary>Details</summary>
Motivation: 云AI系统规模与复杂度增加导致故障检测与恢复成为保障服务可靠性的核心挑战，传统方法在语义理解与策略泛化能力上存在局限。

Method: 构建LLM驱动的故障语义解析模块（动态提取多源日志深层语义）与DRL恢复策略优化器（学习故障类型与响应行为动态匹配），结合记忆引导元控制器实现持续适应新故障模式。

Result: 在云故障注入平台实验中，IFSHM框架较现有DRL与规则方法缩短37%系统恢复时间，且能处理未知故障场景。

Conclusion: LLM与DRL的融合显著提升强化学习的探索效率与泛化能力，通过语义建模与元控制机制实现了云系统故障恢复的持续自适应优化。

Abstract: As the scale and complexity of cloud-based AI systems continue to increase,
the detection and adaptive recovery of system faults have become the core
challenges to ensure service reliability and continuity. In this paper, we
propose an Intelligent Fault Self-Healing Mechanism (IFSHM) that integrates
Large Language Model (LLM) and Deep Reinforcement Learning (DRL), aiming to
realize a fault recovery framework with semantic understanding and policy
optimization capabilities in cloud AI systems. On the basis of the traditional
DRL-based control model, the proposed method constructs a two-stage hybrid
architecture: (1) an LLM-driven fault semantic interpretation module, which can
dynamically extract deep contextual semantics from multi-source logs and system
indicators to accurately identify potential fault modes; (2) DRL recovery
strategy optimizer, based on reinforcement learning, learns the dynamic
matching of fault types and response behaviors in the cloud environment. The
innovation of this method lies in the introduction of LLM for environment
modeling and action space abstraction, which greatly improves the exploration
efficiency and generalization ability of reinforcement learning. At the same
time, a memory-guided meta-controller is introduced, combined with
reinforcement learning playback and LLM prompt fine-tuning strategy, to achieve
continuous adaptation to new failure modes and avoid catastrophic forgetting.
Experimental results on the cloud fault injection platform show that compared
with the existing DRL and rule methods, the IFSHM framework shortens the system
recovery time by 37% with unknown fault scenarios.

</details>


### [183] [Evaluating Visual Mathematics in Multimodal LLMs: A Multilingual Benchmark Based on the Kangaroo Tests](https://arxiv.org/abs/2506.07418)
*Arnau Igualde Sáez,Lamyae Rhomrasi,Yusef Ahsini,Ricardo Vinuesa,Sergio Hoyas,Jose P. García Sabater,Marius J. Fullana i Alfonso,J. Alberto Conejero*

Main category: cs.AI

TL;DR: 本文分析了多模态大语言模型（MLLMs）在数学问题解决中的表现，评估了多个模型在多语言Kangaroo风格基准测试中的表现，揭示了模型在几何、代数、逻辑等领域的精度差异，并指出Gemini 2.0 Flash在图像任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）在视觉语言能力上具有潜力，但它们在视觉呈现的数学问题解决中的有效性尚未得到充分探索。本文旨在填补这一研究空白。

Method: 本文评估了多个MLLMs模型（包括GPT 4o、Pixtral、Qwen VL、Llama 3.2 Vision变体和Gemini 2.0 Flash）在多语言Kangaroo风格基准测试中的表现，涵盖英语、法语、西班牙语和加泰罗尼亚语。

Result: 实验结果表明，模型在几何、视觉代数、逻辑、模式和组合数学等领域的整体精度中等，且没有单一模型在所有主题中表现优异。Gemini 2.0 Flash在图像任务中表现最佳，其次是Qwen VL 2.5 72B和GPT 4o。

Conclusion: 尽管某些模型在结构化推理和一致性上表现突出，但整体上MLLMs在视觉数学问题解决中仍有提升空间，尤其是在复杂几何和组合推理方面。

Abstract: Multimodal Large Language Models (MLLMs) promise advanced vision language
capabilities, yet their effectiveness in visually presented mathematics remains
underexplored. This paper analyzes the development and evaluation of MLLMs for
mathematical problem solving, focusing on diagrams, multilingual text, and
symbolic notation. We then assess several models, including GPT 4o, Pixtral,
Qwen VL, Llama 3.2 Vision variants, and Gemini 2.0 Flash in a multilingual
Kangaroo style benchmark spanning English, French, Spanish, and Catalan. Our
experiments reveal four key findings. First, overall precision remains moderate
across geometry, visual algebra, logic, patterns, and combinatorics: no single
model excels in every topic. Second, while most models see improved accuracy
with questions that do not have images, the gain is often limited; performance
for some remains nearly unchanged without visual input, indicating
underutilization of diagrammatic information. Third, substantial variation
exists across languages and difficulty levels: models frequently handle easier
items but struggle with advanced geometry and combinatorial reasoning. Notably,
Gemini 2.0 Flash achieves the highest precision on image based tasks, followed
by Qwen VL 2.5 72B and GPT 4o, though none approach human level performance.
Fourth, a complementary analysis aimed at distinguishing whether models reason
or simply recite reveals that Gemini and GPT 4o stand out for their structured
reasoning and consistent accuracy. In contrast, Pixtral and Llama exhibit less
consistent reasoning, often defaulting to heuristics or randomness when unable
to align their outputs with the given answer options.

</details>


### [184] [HeTa: Relation-wise Heterogeneous Graph Foundation Attack Model](https://arxiv.org/abs/2506.07428)
*Yuling Wang,Zihui Chen,Pengfei Jiao,Xiao Wang*

Main category: cs.AI

TL;DR: 本文提出了一种新型关系感知异质图基础攻击模型HeTa，通过挖掘共享攻击单元，实现了跨不同异质图神经网络（HGNNs）的通用扰动生成，并在实验中展现了强大的攻击性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有针对HGNNs的攻击方法需针对新场景重新训练参数，效率低下。研究发现不同HGNNs在关系感知层面存在共享的脆弱性模式，因此探索设计一种基础攻击模型，以快速适应新异质图并实现跨模型攻击。

Method: 提出HeTa模型：1）使用基础代理模型对齐异质性并识别关系感知攻击单元的重要性；2）基于关系权重实现逐关系序列化攻击，生成可迁移至不同目标HGNNs的扰动，并支持对新异质图的快速微调。

Result: 实验表明，HeTa在攻击性能和泛化性上显著优于现有方法，生成的扰动可有效迁移至不同目标HGNNs，且能快速适应新异质图数据。

Conclusion: 通过挖掘HGNNs共享的脆弱性模式，HeTa验证了基础攻击模型的可行性，为异质图神经网络的安全性评估提供了高效、可扩展的解决方案。

Abstract: Heterogeneous Graph Neural Networks (HGNNs) are vulnerable, highlighting the
need for tailored attacks to assess their robustness and ensure security.
However, existing HGNN attacks often require complex retraining of parameters
to generate specific perturbations for new scenarios. Recently, foundation
models have opened new horizons for the generalization of graph neural networks
by capturing shared semantics across various graph distributions. This leads us
to ask:Can we design a foundation attack model for HGNNs that enables
generalizable perturbations across different HGNNs, and quickly adapts to new
heterogeneous graphs (HGs)? Empirical findings reveal that, despite significant
differences in model design and parameter space, different HGNNs surprisingly
share common vulnerability patterns from a relation-aware perspective.
Therefore, we explore how to design foundation HGNN attack criteria by mining
shared attack units. In this paper, we propose a novel relation-wise
heterogeneous graph foundation attack model, HeTa. We introduce a foundation
surrogate model to align heterogeneity and identify the importance of shared
relation-aware attack units. Building on this, we implement a serialized
relation-by-relation attack based on the identified relational weights. In this
way, the perturbation can be transferred to various target HGNNs and easily
fine-tuned for new HGs. Extensive experiments exhibit powerful attack
performances and generalizability of our method.

</details>


### [185] [LegalReasoner: Step-wised Verification-Correction for Legal Judgment Reasoning](https://arxiv.org/abs/2506.07443)
*Weijie Shi,Han Zhu,Jiaming Ji,Mengze Li,Jipeng Zhang,Ruiyuan Zhang,Jia Zhu,Jiajie Xu,Sirui Han,Yike Guo*

Main category: cs.AI

TL;DR: 本文提出LegalReasoner方法，通过逐步验证和修正法律推理过程，提升法律判决预测的可靠性，并发布包含5.8万香港案例的LegalHK数据集。实验显示该方法将LLAMA-3模型的判决一致性从72.37提升至80.27。


<details>
  <summary>Details</summary>
Motivation: 现有法律判决预测方法在复杂法律推理中易产生逻辑错误，影响司法决策支持系统的可靠性和效率。

Method: 1) 识别争议点分解复杂案例 2) 分步推理并利用验证器从正确性/进步性/潜在性三维度检查逻辑 3) 检测到错误时采用专家设计的归因与解决策略修正 4) 构建含详细标注的LegalHK数据集进行模型微调

Result: 在LLAMA-3.1-70B模型上，LegalReasoner使判决与法庭决定的一致性从72.37显著提高至80.27，数据集已开源。

Conclusion: 通过推理过程验证-修正机制与高质量标注，LegalReasoner有效提升法律判决预测的可靠性，数据集为后续研究提供重要支持。

Abstract: Legal judgment prediction (LJP) aims to function as a judge by making final
rulings based on case claims and facts, which plays a vital role in the
judicial domain for supporting court decision-making and improving judicial
efficiency. However, existing methods often struggle with logical errors when
conducting complex legal reasoning. We propose LegalReasoner, which enhances
LJP reliability through step-wise verification and correction of the reasoning
process. Specifically, it first identifies dispute points to decompose complex
cases, and then conducts step-wise reasoning while employing a process verifier
to validate each step's logic from correctness, progressiveness, and potential
perspectives. When errors are detected, expert-designed attribution and
resolution strategies are applied for correction. To fine-tune LegalReasoner,
we release the LegalHK dataset, containing 58,130 Hong Kong court cases with
detailed annotations of dispute points, step-by-step reasoning chains, and
process verification labels. Experiments demonstrate that LegalReasoner
significantly improves concordance with court decisions from 72.37 to 80.27 on
LLAMA-3.1-70B. The data is available at
https://huggingface.co/datasets/weijiezz/LegalHK.

</details>


### [186] [Fact in Fragments: Deconstructing Complex Claims via LLM-based Atomic Fact Extraction and Verification](https://arxiv.org/abs/2506.07446)
*Liwen Zheng,Chaozhuo Li,Zheng Liu,Feiran Huang,Haoran Jia,Zaisheng Ye,Xi Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种名为AFEV的新框架，通过迭代分解复杂声明为原子事实，提升事实验证的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理需要多跳推理的复杂声明时表现不佳，因为它们依赖于静态分解策略和表层语义检索，无法捕捉声明的细微结构和意图，导致推理错误累积和噪声证据污染。

Method: AFEV框架通过迭代提取原子事实，动态优化声明理解，重新排序证据以过滤噪声，并利用上下文特定的演示来指导推理过程。

Result: 在五个基准数据集上的广泛实验表明，AFEV在准确性和可解释性方面达到了最先进的性能。

Conclusion: AFEV框架通过迭代分解和自适应推理，有效提升了复杂声明的事实验证能力，减少了错误传播，并增强了证据的可靠性。

Abstract: Fact verification plays a vital role in combating misinformation by assessing
the veracity of claims through evidence retrieval and reasoning. However,
traditional methods struggle with complex claims requiring multi-hop reasoning
over fragmented evidence, as they often rely on static decomposition strategies
and surface-level semantic retrieval, which fail to capture the nuanced
structure and intent of the claim. This results in accumulated reasoning
errors, noisy evidence contamination, and limited adaptability to diverse
claims, ultimately undermining verification accuracy in complex scenarios. To
address this, we propose Atomic Fact Extraction and Verification (AFEV), a
novel framework that iteratively decomposes complex claims into atomic facts,
enabling fine-grained retrieval and adaptive reasoning. AFEV dynamically
refines claim understanding and reduces error propagation through iterative
fact extraction, reranks evidence to filter noise, and leverages
context-specific demonstrations to guide the reasoning process. Extensive
experiments on five benchmark datasets demonstrate that AFEV achieves
state-of-the-art performance in both accuracy and interpretability.

</details>


### [187] [Efficient Generation of Diverse Cooperative Agents with World Models](https://arxiv.org/abs/2506.07450)
*Yi Loo,Akshunn Trivedi,Malika Meghjani*

Main category: cs.AI

TL;DR: 本文提出了一种通过世界模型生成模拟轨迹的方法XPM-WM，显著提高了零样本协调代理的训练效率，减少了样本需求，并能够生成多样化的合作伙伴。


<details>
  <summary>Details</summary>
Motivation: 当前零样本协调代理训练中的主要瓶颈是生成具有多样化协作惯例的合作伙伴，而现有的跨游戏最小化方法计算成本高且样本效率低。

Method: 本文提出了XPM-WM框架，利用学习到的世界模型生成模拟轨迹，从而加速跨游戏最小化方法的训练过程。

Result: 实验表明，XPM-WM方法无需采样多种轨迹，能够有效生成具有多样化惯例的合作伙伴，并在零样本协调代理的训练中达到与之前方法相当的性能。

Conclusion: XPM-WM方法显著提高了样本效率，并能够扩展到更多的合作伙伴，具有较高的可扩展性。

Abstract: A major bottleneck in the training process for Zero-Shot Coordination (ZSC)
agents is the generation of partner agents that are diverse in collaborative
conventions. Current Cross-play Minimization (XPM) methods for population
generation can be very computationally expensive and sample inefficient as the
training objective requires sampling multiple types of trajectories. Each
partner agent in the population is also trained from scratch, despite all of
the partners in the population learning policies of the same coordination task.
In this work, we propose that simulated trajectories from the dynamics model of
an environment can drastically speed up the training process for XPM methods.
We introduce XPM-WM, a framework for generating simulated trajectories for XPM
via a learned World Model (WM). We show XPM with simulated trajectories removes
the need to sample multiple trajectories. In addition, we show our proposed
method can effectively generate partners with diverse conventions that match
the performance of previous methods in terms of SP population training reward
as well as training partners for ZSC agents. Our method is thus, significantly
more sample efficient and scalable to a larger number of partners.

</details>


### [188] [Learning What Reinforcement Learning Can't: Interleaved Online Fine-Tuning for Hardest Questions](https://arxiv.org/abs/2506.07527)
*Lu Ma,Hao Liang,Meiyi Qiang,Lexiang Tang,Xiaochen Ma,Zhen Hao Wong,Junbo Niu,Chengyu Shen,Runming He,Bin Cui,Wentao Zhang*

Main category: cs.AI

TL;DR: 本文提出ReLIFT方法，通过交替使用强化学习（RL）和在线监督微调（SFT），有效结合两者的优势，提升大语言模型在复杂推理任务中的表现，并在多个基准测试中实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法虽能激发大语言模型的复杂推理行为，但受限于模型固有知识，难以引入新知识。监督微调（SFT）可补充这一缺陷，但两者单独使用均存在局限性。因此需探索结合RL与SFT的互补性训练策略。

Method: 提出ReLIFT方法：以RL为主框架，当模型遇到困难问题时，收集高质量解决方案进行在线监督微调，交替执行RL与SFT阶段。该策略既保持RL对已有能力的优化，又通过SFT引入新知识。

Result: ReLIFT在5个竞赛级基准和1个分布外基准上平均提升5.2分，且仅需13%的标注数据即超越纯RL/SFT方法，证明其高效性和可扩展性。

Conclusion: ReLIFT突破纯强化学习的根本限制，验证了混合训练策略的潜力。通过动态结合RL与SFT，实现知识获取与能力优化的协同，为提升大模型推理能力提供新方向。

Abstract: Recent advances in large language model (LLM) reasoning have shown that
sophisticated behaviors such as planning and self-reflection can emerge through
reinforcement learning (RL). However, despite these successes, RL in its
current form remains insufficient to induce capabilities that exceed the
limitations of the base model, as it is primarily optimized based on existing
knowledge of the model rather than facilitating the acquisition of new
information. To address this limitation, we employ supervised fine-tuning (SFT)
to learn what RL cannot, which enables the incorporation of new knowledge and
reasoning patterns by leveraging high-quality demonstration data. We analyze
the training dynamics of RL and SFT for LLM reasoning and find that RL excels
at maintaining and improving performance on questions within the model's
original capabilities, while SFT is more effective at enabling progress on
questions beyond the current scope of the model. Motivated by the complementary
strengths of RL and SFT, we introduce a novel training approach,
\textbf{ReLIFT} (\textbf{Re}inforcement \textbf{L}earning \textbf{I}nterleaved
with Online \textbf{F}ine-\textbf{T}uning). In ReLIFT, the model is primarily
trained using RL, but when it encounters challenging questions, high-quality
solutions are collected for fine-tuning, and the training process alternates
between RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT
achieves an average improvement of over +5.2 points across five
competition-level benchmarks and one out-of-distribution benchmark compared to
other zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both
RL and SFT while using only 13\% of the detailed demonstration data,
highlighting its scalability. These results provide compelling evidence that
ReLIFT overcomes the fundamental limitations of RL and underscores the
significant potential.

</details>


### [189] [Coordinating Search-Informed Reasoning and Reasoning-Guided Search in Claim Verification](https://arxiv.org/abs/2506.07528)
*Qisheng Hu,Quanyu Long,Wenya Wang*

Main category: cs.AI

TL;DR: 本文介绍了一种分层代理推理与信息搜索方法（HARIS），通过协调推理驱动搜索与搜索辅助推理，提升多跳声明验证的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 多跳声明验证需动态结合推理与搜索，但现有方法未充分协调两者的交织过程，导致验证链构建效率低且隐藏事实难以发现。

Method: HARIS采用分层代理结构：高层推理代理生成验证链并触发信息需求，低层搜索代理迭代检索并精炼查询，通过强化学习训练以任务结果作为奖励。

Result: 在EX-FEVER和HOVER基准测试中，HARIS显著超越现有方法，验证准确率大幅提升。

Conclusion: HARIS通过显式建模推理与搜索的协同机制，证明了分层代理分工与强化学习训练在多跳验证任务中的有效性，兼具性能与可解释性。

Abstract: Multi-hop claim verification is inherently challenging, requiring multi-step
reasoning to construct verification chains while iteratively searching for
information to uncover hidden bridging facts. This process is fundamentally
interleaved, as effective reasoning relies on dynamically retrieved evidence,
while effective search demands reasoning to refine queries based on partial
information. To achieve this, we propose Hierarchical Agent Reasoning and
Information Search (HARIS), explicitly modeling the coordinated process of
reasoning-driven searching and search-informed reasoning. HARIS consists of a
high-level reasoning agent that focuses on constructing the main verification
chain, generating factual questions when more information is needed, and a
low-level search agent that iteratively retrieves more information, refining
its search based on intermediate findings. This design allows each agent to
specialize in its respective task, enhancing verification accuracy and
interpretability. HARIS is trained using reinforcement learning with
outcome-based rewards. Experimental results on the EX-FEVER and HOVER
benchmarks demonstrate that HARIS achieves strong performance, greatly
advancing multi-hop claim verification.

</details>


### [190] [Curriculum Learning With Counterfactual Group Relative Policy Advantage For Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2506.07548)
*Weiqiang Jin,Hongyang Du,Guizhong Liu,Dong In Kim*

Main category: cs.AI

TL;DR: 本文提出了一种动态课程学习框架，用于多智能体强化学习，通过自适应难度调整机制和反事实组相对策略优势（CGRPA）提高训练稳定性和最终性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体强化学习方法通常在固定对手策略下训练，限制了其适应变化环境的能力，导致次优策略。受课程学习在监督任务中的成功启发，本文旨在解决这一问题。

Method: 提出了一种动态课程学习框架，采用自适应难度调整机制，并开发了反事实组相对策略优势（CGRPA），通过反事实优势函数评估每个智能体的贡献，提供内在奖励以增强信用分配。

Result: 实验表明，该方法提高了训练稳定性和最终性能，在对抗性任务中取得了与最先进方法竞争的结果。

Conclusion: 本文提出的动态课程学习框架和CGRPA方法有效解决了多智能体强化学习中的适应性和稳定性问题，显著提升了性能。

Abstract: Multi-agent reinforcement learning (MARL) has achieved strong performance in
cooperative adversarial tasks. However, most existing methods typically train
agents against fixed opponent strategies and rely on such meta-static
difficulty conditions, which limits their adaptability to changing environments
and often leads to suboptimal policies. Inspired by the success of curriculum
learning (CL) in supervised tasks, we propose a dynamic CL framework for MARL
that employs an self-adaptive difficulty adjustment mechanism. This mechanism
continuously modulates opponent strength based on real-time agent training
performance, allowing agents to progressively learn from easier to more
challenging scenarios. However, the dynamic nature of CL introduces instability
due to nonstationary environments and sparse global rewards. To address this
challenge, we develop a Counterfactual Group Relative Policy Advantage (CGRPA),
which is tightly coupled with the curriculum by providing intrinsic credit
signals that reflect each agent's impact under evolving task demands. CGRPA
constructs a counterfactual advantage function that isolates individual
contributions within group behavior, facilitating more reliable policy updates
throughout the curriculum. CGRPA evaluates each agent's contribution through
constructing counterfactual action advantage function, providing intrinsic
rewards that enhance credit assignment and stabilize learning under
non-stationary conditions. Extensive experiments demonstrate that our method
improves both training stability and final performance, achieving competitive
results against state-of-the-art methods. The code is available at
https://github.com/NICE-HKU/CL2MARL-SMAC.

</details>


### [191] [GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular Structure Recognition](https://arxiv.org/abs/2506.07553)
*Jingchao Wang,Haote Yang,Jiang Wu,Yifan He,Xingjian Wei,Yinfan Wang,Chengjin Liu,Lingli Ge,Lijun Wu,Bin Wang,Dahua Lin,Conghui He*

Main category: cs.AI

TL;DR: GTR-Mol-VLM框架通过图遍历和忠实识别原则，提升了光学化学结构识别（OCSR）的准确性，并在复杂分子结构处理上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型（VLMs）在处理复杂分子结构和不一致注释时存在困难，因此需要一种更有效的方法来提升OCSR的准确性。

Method: 提出了GTR-Mol-VLM框架，包含图遍历作为视觉思维链机制和忠实识别原则，并构建了GTR-CoT-1.3M数据集和MolRec-Bench基准。

Result: GTR-Mol-VLM在功能基团缩写分子图像处理上，比次优基线模型在SMILES和基于图的指标上提升了约14个百分点。

Conclusion: GTR-Mol-VLM显著提升了OCSR技术，有望推动化学信息学和AI for Science领域的发展。

Abstract: Optical Chemical Structure Recognition (OCSR) is crucial for digitizing
chemical knowledge by converting molecular images into machine-readable
formats. While recent vision-language models (VLMs) have shown potential in
this task, their image-captioning approach often struggles with complex
molecular structures and inconsistent annotations. To overcome these
challenges, we introduce GTR-Mol-VLM, a novel framework featuring two key
innovations: (1) the \textit{Graph Traversal as Visual Chain of Thought}
mechanism that emulates human reasoning by incrementally parsing molecular
graphs through sequential atom-bond predictions, and (2) the data-centric
principle of \textit{Faithfully Recognize What You've Seen}, which addresses
the mismatch between abbreviated structures in images and their expanded
annotations. To support model development, we constructed GTR-CoT-1.3M, a
large-scale instruction-tuning dataset with meticulously corrected annotations,
and introduced MolRec-Bench, the first benchmark designed for a fine-grained
evaluation of graph-parsing accuracy in OCSR. Comprehensive experiments
demonstrate that GTR-Mol-VLM achieves superior results compared to specialist
models, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in
scenarios involving molecular images with functional group abbreviations,
GTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage
points, both in SMILES-based and graph-based metrics. We hope that this work
will drive OCSR technology to more effectively meet real-world needs, thereby
advancing the fields of cheminformatics and AI for Science. We will release
GTR-CoT at https://github.com/opendatalab/GTR-CoT.

</details>


### [192] [SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems](https://arxiv.org/abs/2506.07564)
*Peiran Li,Xinkai Zou,Zhuohang Wu,Ruifeng Li,Shuo Xing,Hanwen Zheng,Zhikai Hu,Yuping Wang,Haoxi Li,Qin Yuan,Yingmo Zhang,Zhengzhong Tu*

Main category: cs.AI

TL;DR: SAFEFLOW是一个新的协议级框架，用于构建可信赖的基于LLM/VLM的代理，通过细粒度的信息流控制、事务执行和冲突解决机制，确保多代理环境中的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型和视觉语言模型的能力不断增强，现有的代理框架仍然脆弱，缺乏安全信息流、可靠性和多代理协调的原则性机制。

Method: SAFEFLOW引入了细粒度的信息流控制，精确跟踪数据来源、完整性和保密性，并通过事务执行、冲突解决和安全调度来确保多代理环境中的全局一致性。

Result: 实验表明，使用SAFEFLOW构建的代理在敌对、嘈杂和并发操作条件下仍能保持出色的任务性能和安全性，显著优于现有技术。

Conclusion: SAFEFLOW和SAFEFLOWBENCH为原则性、健壮和安全的代理生态系统奠定了基础，推动了可靠自主性的前沿发展。

Abstract: Recent advances in large language models (LLMs) and vision-language models
(VLMs) have enabled powerful autonomous agents capable of complex reasoning and
multi-modal tool use. Despite their growing capabilities, today's agent
frameworks remain fragile, lacking principled mechanisms for secure information
flow, reliability, and multi-agent coordination. In this work, we introduce
SAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based
agents. SAFEFLOW enforces fine-grained information flow control (IFC),
precisely tracking provenance, integrity, and confidentiality of all the data
exchanged between agents, tools, users, and environments. By constraining LLM
reasoning to respect these security labels, SAFEFLOW prevents untrusted or
adversarial inputs from contaminating high-integrity decisions. To ensure
robustness in concurrent multi-agent settings, SAFEFLOW introduces
transactional execution, conflict resolution, and secure scheduling over shared
state, preserving global consistency across agents. We further introduce
mechanisms, including write-ahead logging, rollback, and secure caches, that
further enhance resilience against runtime errors and policy violations. To
validate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark
suite designed to evaluate agent reliability under adversarial, noisy, and
concurrent operational conditions. Extensive experiments demonstrate that
agents built with SAFEFLOW maintain impressive task performance and security
guarantees even in hostile environments, substantially outperforming
state-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for
principled, robust, and secure agent ecosystems, advancing the frontier of
reliable autonomy.

</details>


### [193] [Automating Exploratory Multiomics Research via Language Models](https://arxiv.org/abs/2506.07591)
*Shang Qu,Ning Ding,Linhai Xie,Yifei Li,Zaoqu Liu,Kaiyan Zhang,Yibai Xiong,Yuxin Zuo,Zhangren Chen,Ermo Hua,Xingtai Lv,Youbang Sun,Yang Li,Dong Li,Fuchu He,Bowen Zhou*

Main category: cs.AI

TL;DR: PROTEUS是一个全自动系统，通过模块化流程从临床多组学数据生成数据驱动的假设，平衡可靠性与新颖性，加速分析并推动自主科学系统发展。


<details>
  <summary>Details</summary>
Motivation: 临床蛋白质组学领域需要高效的下游数据分析和假设生成以推动新发现，但高维度异构数据与开放探索需求对传统方法构成挑战。

Method: 采用模块化架构模拟科研流程，通过统一图结构管理生物实体关系，实现从开放探索到统计分析与假设生成的自动化处理。

Result: 在10个已发表临床多组学数据集上生成360个假设，经外部数据验证和开放评分评估，证明系统能有效平衡假设可靠性与创新性。

Conclusion: PROTEUS不仅加速多组学分析，更为定制化自主科研系统提供路径，实现从原始数据到开放假设生成的端到端解决方案。

Abstract: This paper introduces PROTEUS, a fully automated system that produces
data-driven hypotheses from raw data files. We apply PROTEUS to clinical
proteogenomics, a field where effective downstream data analysis and hypothesis
proposal is crucial for producing novel discoveries. PROTEUS uses separate
modules to simulate different stages of the scientific process, from open-ended
data exploration to specific statistical analysis and hypothesis proposal. It
formulates research directions, tools, and results in terms of relationships
between biological entities, using unified graph structures to manage complex
research processes. We applied PROTEUS to 10 clinical multiomics datasets from
published research, arriving at 360 total hypotheses. Results were evaluated
through external data validation and automatic open-ended scoring. Through
exploratory and iterative research, the system can navigate high-throughput and
heterogeneous multiomics data to arrive at hypotheses that balance reliability
and novelty. In addition to accelerating multiomic analysis, PROTEUS represents
a path towards tailoring general autonomous systems to specialized scientific
domains to achieve open-ended hypothesis generation from data.

</details>


### [194] [SWE-Dev: Building Software Engineering Agents with Training and Inference Scaling](https://arxiv.org/abs/2506.07636)
*Haoran Wang,Zhenyu Hou,Yao Wei,Jie Tang,Yuxiao Dong*

Main category: cs.AI

TL;DR: 本文提出SWE-Dev，一种基于开源大语言模型的软件工程代理，通过合成测试用例和扩展代理轨迹数据，解决了现有工具因缺乏高质量训练数据和测试用例导致的性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的软件工程工具（如OpenAI Codex）在端到端自动化开发中存在挑战，主要由于缺乏高质量训练数据和有效测试用例。

Method: 1. 开发测试用例合成管道用于补丁评估；2. 扩展代理轨迹构建训练数据；3. 基于开源LLM构建SWE-Dev代理。

Result: 在SWE-bench-Verified基准测试中，7B/32B参数模型的成功率分别达23.4%/36.6%，超越当前最优开源模型。

Conclusion: SWE-Dev证明了通过系统化数据构建方法可显著提升LLM在真实软件工程任务中的性能，且模型与数据已全面开源。

Abstract: Large language models (LLMs) have advanced rapidly from conversational
problem solving to addressing real-world tasks involving tool use, such as
software engineering (SWE). Recent LLM-powered toolkits, such as OpenAI Codex
and Cursor, have offered end-to-end automation of the software development
process. However, building effective SWE agents remains challenging due to the
lack of high-quality training data and effective test cases. To address this
issue, we present SWE-Dev, an SWE agent built upon open-source LLMs. First, we
develop a robust pipeline to synthesize test cases for patch evaluation.
Second, we scale up agent trajectories to construct the training data for
building SWE-Dev. Experiments on the SWE-bench-Verified benchmark show that the
SWE-Dev models can achieve top performance among all open SWE agents.
Specifically, the success rates of the SWE-Dev 7B and 32B parameter models
reach 23.4% and 36.6%, respectively, outperforming state-of-the-art open-source
models. All code, models, and datasets are publicly available at
https://github.com/THUDM/SWE-Dev.

</details>


### [195] [MCPWorld: A Unified Benchmarking Testbed for API, GUI, and Hybrid Computer Use Agents](https://arxiv.org/abs/2506.07672)
*Yunhe Yan,Shihe Wang,Jiajun Du,Yexuan Yang,Yuxuan Shan,Qichen Qiu,Xianqing Jia,Xinge Wang,Xin Yuan,Xu Han,Mao Qin,Yinxiao Chen,Chen Peng,Shangguang Wang,Mengwei Xu*

Main category: cs.AI

TL;DR: MCPWorld是首个针对API、GUI及API-GUI混合代理的自动CUA测试平台，通过白盒应用和动态代码插装技术，提供鲁棒且准确的CUA评估。


<details>
  <summary>Details</summary>
Motivation: 现有CUA基准主要针对GUI代理，其评估方法易受UI变化影响，且忽略了应用API暴露的功能交互。

Method: 提出MCPWorld，使用白盒应用，支持动态代码插装，程序化验证任务完成情况，涵盖201个用户任务。

Result: 初步实验显示，使用LLM驱动的CUA框架，任务完成准确率达到75.12%。

Conclusion: MCPWorld有望促进和标准化下一代计算机使用代理的基准测试，利用丰富的外部工具。

Abstract: (M)LLM-powered computer use agents (CUA) are emerging as a transformative
technique to automate human-computer interaction. However, existing CUA
benchmarks predominantly target GUI agents, whose evaluation methods are
susceptible to UI changes and ignore function interactions exposed by
application APIs, e.g., Model Context Protocol (MCP). To this end, we propose
MCPWorld, the first automatic CUA testbed for API, GUI, and API-GUI hybrid
agents. A key principle of MCPWorld is the use of "white-box apps", i.e., those
with source code availability and can be revised/re-compiled as needed (e.g.,
adding MCP support), with two notable advantages:
  (1) It greatly broadens the design space of CUA, such as what and how the app
features to be exposed/extracted as CUA-callable APIs.
  (2) It allows MCPWorld to programmatically verify task completion by directly
monitoring application behavior through techniques like dynamic code
instrumentation, offering robust, accurate CUA evaluation decoupled from
specific agent implementations or UI states.
  Currently, MCPWorld includes 201 well curated and annotated user tasks,
covering diversified use cases and difficulty levels. MCPWorld is also fully
containerized with GPU acceleration support for flexible adoption on different
OS/hardware environments. Our preliminary experiments, using a representative
LLM-powered CUA framework, achieve 75.12% task completion accuracy,
simultaneously providing initial evidence on the practical effectiveness of
agent automation leveraging MCP. Overall, we anticipate MCPWorld to facilitate
and standardize the benchmarking of next-generation computer use agents that
can leverage rich external tools. Our code and dataset are publicly available
at https://github.com/SAAgent/MCPWorld.

</details>


### [196] [NeurIPS 2025 E2LM Competition : Early Training Evaluation of Language Models](https://arxiv.org/abs/2506.07731)
*Mouadh Yagoubi,Yasser Dahou,Billel Mokeddem,Younes Belkada,Phuc H. Le-Khac,Basma El Amel Boussaha,Reda Alami,Jingwei Zuo,Damiano Marsili,Mugariya Farooq,Mounia Lalmas,Georgia Gkioxari,Patrick Gallinari,Philip Torr,Hakim Hacid*

Main category: cs.AI

TL;DR: 现有基准测试在评估大型语言模型后期表现有效，但无法有效捕捉小模型早期训练差异。本竞赛旨在设计针对早期训练阶段的科学知识评估任务，提供预训练小模型及检查点，鼓励开发新评估方法，促进系统化LLM研究。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估完全训练的大模型时有效，但在小模型早期训练阶段无法提供有区分度的信号。需设计专门针对早期训练阶段的评估方法，以更科学地指导模型开发。

Method: 发起竞赛，要求开发新评估方法或改进现有基准，提供0.5B/1B/3B参数小模型及训练至200B token的中间检查点。支持免费云GPU平台实验，降低参与门槛。

Result: 提交方案将基于信号质量、1万亿token训练后的模型排名一致性、与科学知识领域相关性进行评审。

Conclusion: 通过推动早期训练评估策略设计，吸引跨学科研究者参与，使LLM基础研究从开发初期即具备系统性，并建立更有效的基准指导体系。

Abstract: Existing benchmarks have proven effective for assessing the performance of
fully trained large language models. However, we find striking differences in
the early training stages of small models, where benchmarks often fail to
provide meaningful or discriminative signals. To explore how these differences
arise, this competition tackles the challenge of designing scientific knowledge
evaluation tasks specifically tailored for measuring early training progress of
language models. Participants are invited to develop novel evaluation
methodologies or adapt existing benchmarks to better capture performance
differences among language models. To support this effort, we provide three
pre-trained small models (0.5B, 1B, and 3B parameters), along with intermediate
checkpoints sampled during training up to 200B tokens. All experiments and
development work can be run on widely available free cloud-based GPU platforms,
making participation accessible to researchers with limited computational
resources. Submissions will be evaluated based on three criteria: the quality
of the performance signal they produce, the consistency of model rankings at 1
trillion tokens of training, and their relevance to the scientific knowledge
domain. By promoting the design of tailored evaluation strategies for early
training, this competition aims to attract a broad range of participants from
various disciplines, including those who may not be machine learning experts or
have access to dedicated GPU resources. Ultimately, this initiative seeks to
make foundational LLM research more systematic and benchmark-informed from the
earliest phases of model development.

</details>


### [197] [RSafe: Incentivizing proactive reasoning to build robust and adaptive LLM safeguards](https://arxiv.org/abs/2506.07736)
*Jingnan Zheng,Xiangtian Ji,Yijun Lu,Chenhang Cui,Weixiang Zhao,Gelei Deng,Zhenkai Liang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: 本文提出了一种名为RSafe的自适应推理保护机制，通过两阶段训练增强大语言模型的安全性，以应对未知或对抗性安全威胁。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）经过安全对齐，但仍存在漏洞，可能对用户和社会造成风险。现有保护模型依赖人工数据集，难以应对未知威胁。

Method: RSafe采用两阶段训练：1) 引导推理，通过政策指导逐步分析输入内容的安全风险；2) 强化对齐，使用基于规则的强化学习优化推理路径以符合安全预测。

Result: RSafe能够内化安全原则，在未见或对抗性安全违规场景中提供泛化的安全保护能力。

Conclusion: RSafe通过接受用户指定的安全策略，提供定制化的增强保护，有效应对大语言模型的安全挑战。

Abstract: Large Language Models (LLMs) continue to exhibit vulnerabilities despite
deliberate safety alignment efforts, posing significant risks to users and
society. To safeguard against the risk of policy-violating content,
system-level moderation via external guard models-designed to monitor LLM
inputs and outputs and block potentially harmful content-has emerged as a
prevalent mitigation strategy. Existing approaches of training guard models
rely heavily on extensive human curated datasets and struggle with
out-of-distribution threats, such as emerging harmful categories or jailbreak
attacks. To address these limitations, we propose RSafe, an adaptive
reasoning-based safeguard that conducts guided safety reasoning to provide
robust protection within the scope of specified safety policies. RSafe operates
in two stages: 1) guided reasoning, where it analyzes safety risks of input
content through policy-guided step-by-step reasoning, and 2) reinforced
alignment, where rule-based RL optimizes its reasoning paths to align with
accurate safety prediction. This two-stage training paradigm enables RSafe to
internalize safety principles to generalize safety protection capability over
unseen or adversarial safety violation scenarios. During inference, RSafe
accepts user-specified safety policies to provide enhanced safeguards tailored
to specific safety requirements.

</details>


### [198] [Agent Semantics, Semantic Spacetime, and Graphical Reasoning](https://arxiv.org/abs/2506.07756)
*Mark Burgess*

Main category: cs.AI

TL;DR: 本文介绍了语义时空图模型的形式化方面，探讨了其在知识表示和过程建模中的应用，并提出了有限γ(3,4)表示以形成可扩展的封闭操作集。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过语义时空图模型提供可预测的路径，并解决图中吸收状态导致的信息泄露问题。

Method: 定义了有限γ(3,4)表示，形成封闭操作集，并结合Promise理论分析吸收状态与边界信息的关系。

Result: 模型能够预测图中路径，吸收状态与边界信息相关，需手动注入补救信息以保持封闭性。

Conclusion: 语义时空图模型通过有限表示和Promise理论，有效处理了吸收状态和信息泄露问题，为知识表示和过程建模提供了新思路。

Abstract: Some formal aspects of the Semantic Spacetime graph model are presented, with
reference to its use for directed knowledge representations and process
modelling. A finite $\gamma(3,4)$ representation is defined to form a closed
set of operations that can scale to any degree of semantic complexity. The
Semantic Spacetime postulates bring predictability with minimal constraints to
pathways in graphs. The ubiquitous appearance of absorbing states in any
partial graph means that a graph process leaks information. The issue is
closely associated with the issue of division by zero, which signals a loss of
closure and the need for manual injection of remedial information. The Semantic
Spacetime model (and its Promise Theory) origins help to clarify how such
absorbing states are associated with boundary information where intentionality
can enter.

</details>


### [199] [REMoH: A Reflective Evolution of Multi-objective Heuristics approach via Large Language Models](https://arxiv.org/abs/2506.07759)
*Diego Forniés-Tabuenca,Alejandro Uribe,Urtzi Otamendi,Arkaitz Artetxe,Juan Carlos Rivera,Oier Lopez de Lacalle*

Main category: cs.AI

TL;DR: 本文提出了一种结合NSGA-II与LLM启发式生成的多目标优化框架REMoH，通过反射机制提升启发式多样性与质量，并在FJSSP问题上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统多目标优化算法需要大量问题特定建模，且难以适应非线性结构。LLM的进展为优化提供了更强的可解释性、适应性和推理能力，因此探索其与传统优化方法的结合具有潜力。

Method: 提出REMoH框架，将NSGA-II与LLM启发式生成结合，利用聚类和搜索空间反射机制生成多样且高质量的启发式，提升收敛性和解多样性。

Result: 在FJSSP问题的三个数据集上，REMoH与现有最优方法相比取得了竞争性结果，同时减少了建模工作量并增强了适应性。

Conclusion: LLM能够增强传统多目标优化，提供更大的灵活性、可解释性和鲁棒性，REMoH框架展示了这一潜力。

Abstract: Multi-objective optimization is fundamental in complex decision-making tasks.
Traditional algorithms, while effective, often demand extensive
problem-specific modeling and struggle to adapt to nonlinear structures. Recent
advances in Large Language Models (LLMs) offer enhanced explainability,
adaptability, and reasoning. This work proposes Reflective Evolution of
Multi-objective Heuristics (REMoH), a novel framework integrating NSGA-II with
LLM-based heuristic generation. A key innovation is a reflection mechanism that
uses clustering and search-space reflection to guide the creation of diverse,
high-quality heuristics, improving convergence and maintaining solution
diversity. The approach is evaluated on the Flexible Job Shop Scheduling
Problem (FJSSP) in-depth benchmarking against state-of-the-art methods using
three instance datasets: Dauzere, Barnes, and Brandimarte. Results demonstrate
that REMoH achieves competitive results compared to state-of-the-art approaches
with reduced modeling effort and enhanced adaptability. These findings
underscore the potential of LLMs to augment traditional optimization, offering
greater flexibility, interpretability, and robustness in multi-objective
scenarios.

</details>


### [200] [A Proposal to Extend the Common Model of Cognition with Metacognition](https://arxiv.org/abs/2506.07807)
*John Laird,Christian Lebiere,Paul Rosenbloom,Andrea Stocco,Robert Wray*

Main category: cs.AI

TL;DR: 本文提出了一种在认知通用模型（CMC）中整合元认知的统一方法，利用工作记忆中的显式表示进行推理。


<details>
  <summary>Details</summary>
Motivation: 旨在扩展认知通用模型（CMC）以更好地理解和模拟人类认知中的元认知过程。

Method: 通过在CMC的工作记忆中引入显式表示，对代理的认知能力和过程进行推理。

Result: 提出了一个整合元认知的框架，并提供了相关示例。

Conclusion: 该方法成功地将元认知整合到CMC中，为模拟人类认知提供了新的视角。

Abstract: The Common Model of Cognition (CMC) provides an abstract characterization of
the structure and processing required by a cognitive architecture for
human-like minds. We propose a unified approach to integrating metacognition
within the CMC. We propose that metacognition involves reasoning over explicit
representations of an agent's cognitive capabilities and processes in working
memory. Our proposal exploits the existing cognitive capabilities of the CMC,
making minimal extensions in the structure and information available within
working memory. We provide examples of metacognition within our proposal.

</details>


### [201] [Guideline Forest: Experience-Induced Multi-Guideline Reasoning with Stepwise Aggregation](https://arxiv.org/abs/2506.07820)
*Jiaxiang CHen,Zhuo Wang,Mingxi Zou,Qifan Wang,Zenglin Xu*

Main category: cs.AI

TL;DR: 提出Guideline Forest框架，通过结构化推理策略提升大语言模型的多策略复用能力，在数学和编程推理任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效复用已验证的多样化推理策略，且单一路径蒸馏或测试时搜索效率低下，难以模拟人类灵活、自适应的推理过程。

Method: 从验证样本中归纳结构化推理指南(Guidelines)，并行扩展其变体表征不同思维模式，通过自修正和逐步聚合动态消除不确定性。

Result: 在GSM8K等4个基准测试中全面超越CoT/ToT等基线模型，消融实验验证多路径推理与逐步聚合机制的有效性。

Conclusion: Guideline Forest通过策略复用与动态聚合机制，实现了接近人类的自适应推理范式，展现出强泛化能力和结构化策略组合优势。

Abstract: Human reasoning is flexible, adaptive, and grounded in prior
experience-qualities that large language models (LLMs) still struggle to
emulate. Existing methods either explore diverse reasoning paths at inference
time or search for optimal workflows through expensive operations, but both
fall short in leveraging multiple reusable strategies in a structured,
efficient manner. We propose Guideline Forest, a framework that enhances LLMs
reasoning by inducing structured reasoning strategies-called guidelines-from
verified examples and executing them via step-wise aggregation. Unlike
test-time search or single-path distillation, our method draws on verified
reasoning experiences by inducing reusable guidelines and expanding each into
diverse variants. Much like human reasoning, these variants reflect alternative
thought patterns, are executed in parallel, refined via self-correction, and
aggregated step by step-enabling the model to adaptively resolve uncertainty
and synthesize robust solutions.We evaluate Guideline Forest on four
benchmarks-GSM8K, MATH-500, MBPP, and HumanEval-spanning mathematical and
programmatic reasoning. Guideline Forest consistently outperforms strong
baselines, including CoT, ReAct, ToT, FoT, and AFlow. Ablation studies further
highlight the effectiveness of multi-path reasoning and stepwise aggregation,
underscoring the Guideline Forest's adaptability and generalization potential.

</details>


### [202] [Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs](https://arxiv.org/abs/2506.07824)
*Yao Yan*

Main category: cs.AI

TL;DR: 本文通过线性探测和logit-lens技术，分析了LLaMA-3-8B-Instruct模型在多位数加法任务中的内部计算过程，揭示了其分阶段的层次化计算轨迹。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于通过多位数加法任务，深入理解大型语言模型的计算能力及其内部算术过程。

Method: 结合线性探测和logit-lens技术，分析LLaMA-3-8B-Instruct模型的前向传播过程，提出并验证了一个四阶段的计算轨迹。

Result: 研究发现，模型在计算过程中经历了公式结构表示、核心计算特征、数值抽象和最终内容生成四个阶段，表明其倾向于内部计算而非机械记忆。

Conclusion: 结论是LLaMA-3-8B-Instruct模型在多位数加法任务中表现出层次化的计算过程，支持内部计算而非机械记忆的假设。

Abstract: Multi-digit addition is a clear probe of the computational power of large
language models. To dissect the internal arithmetic processes in
LLaMA-3-8B-Instruct, we combine linear probing with logit-lens inspection.
Inspired by the step-by-step manner in which humans perform addition, we
propose and analyze a coherent four-stage trajectory in the forward
pass:Formula-structure representations become linearly decodable first, while
the answer token is still far down the candidate list.Core computational
features then emerge prominently.At deeper activation layers, numerical
abstractions of the result become clearer, enabling near-perfect detection and
decoding of the individual digits in the sum.Near the output, the model
organizes and generates the final content, with the correct token reliably
occupying the top rank.This trajectory suggests a hierarchical process that
favors internal computation over rote memorization. We release our code and
data to facilitate reproducibility.

</details>


### [203] [HAIBU-ReMUD: Reasoning Multimodal Ultrasound Dataset and Model Bridging to General Specific Domains](https://arxiv.org/abs/2506.07837)
*Shijie Wang,Yilun Zhang,Zeyu Lai,Dexing Kong*

Main category: cs.AI

TL;DR: 本文提出一种针对医学超声领域的图像-文本推理监督微调数据生成方法，构建了包含4.5万条问答数据的ReMUD数据集，并基于Qwen2.5-VL-7B-Instruct训练出性能优于通用模型的ReMUD-7B模型。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在特定领域（如医学超声）表现欠佳，主要因缺乏结构化领域数据。现有超声资料（PDF/图像等）无法直接用于模型训练，需解决数据标准化问题。

Method: 开发图像-文本推理监督微调数据生成流程，从非结构化医学资料中提取四元组数据（图像/问题/思维轨迹/答案），构建包含QA和VQA的ReMUD数据集。

Result: ReMUD-7B模型在医学超声领域表现超越通用MLLMs，数据集规模达45,000+条，相关代码、模型参数及数据集将开源以解决领域数据短缺问题。

Conclusion: 通过领域特定数据生成方法有效提升MLLMs在医学超声场景的性能，开源资源为特定领域多模态模型研究提供数据支持。

Abstract: Multimodal large language models (MLLMs) have shown great potential in
general domains but perform poorly in some specific domains due to a lack of
domain-specific data, such as image-text data or vedio-text data. In some
specific domains, there is abundant graphic and textual data scattered around,
but lacks standardized arrangement. In the field of medical ultrasound, there
are ultrasonic diagnostic books, ultrasonic clinical guidelines, ultrasonic
diagnostic reports, and so on. However, these ultrasonic materials are often
saved in the forms of PDF, images, etc., and cannot be directly used for the
training of MLLMs. This paper proposes a novel image-text reasoning supervised
fine-tuning data generation pipeline to create specific domain quadruplets
(image, question, thinking trace, and answer) from domain-specific materials. A
medical ultrasound domain dataset ReMUD is established, containing over 45,000
reasoning and non-reasoning supervised fine-tuning Question Answering (QA) and
Visual Question Answering (VQA) data. The ReMUD-7B model, fine-tuned on
Qwen2.5-VL-7B-Instruct, outperforms general-domain MLLMs in medical ultrasound
field. To facilitate research, the ReMUD dataset, data generation codebase, and
ReMUD-7B parameters will be released at https://github.com/ShiDaizi/ReMUD,
addressing the data shortage issue in specific domain MLLMs.

</details>


### [204] [A Temporal FRBR/FRBRoo-Based Model for Component-Level Versioning of Legal Norms](https://arxiv.org/abs/2506.07853)
*Hudson de Martim*

Main category: cs.AI

TL;DR: 本文提出了一种扩展FRBRoo框架的结构化时间模型，用于细粒度地跟踪法律规范的演变，解决了现有框架在组件级版本控制上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有法律文档建模框架（如FRBR/FRBRoo和Akoma Ntoso）缺乏细粒度的组件级版本控制机制，无法准确重建特定时间点的法律文本，限制了法律科技和AI应用的可靠性。

Method: 本文扩展了FRBRoo框架，引入了Temporal Version (TV)和Language Version (LV)等子类，以及Component Work (CW)、Component Temporal Version (CTV)和Component Language Version (CLV)等概念，用于跟踪法律规范的演变。

Result: 通过巴西联邦宪法的案例研究，展示了该模型如何精确地重建特定时间点的法律文本，为开发高级法律信息系统和AI工具提供了基础。

Conclusion: 该模型克服了现有生成模型的局限性，为法律信息系统的开发提供了强大的支持，能够进行准确的历史分析和影响评估。

Abstract: Effectively representing legal norms for automated processing is a critical
challenge, particularly in tracking the diachronic evolution of their
hierarchical components (e.g., articles, paragraphs). While foundational
frameworks like FRBR/FRBRoo and standards like Akoma Ntoso model legal
documents at a macro level, they lack native mechanisms for granular,
component-level versioning. This limitation hinders the deterministic
point-in-time reconstruction of legal texts, a fundamental capability for
reliable Legal Tech and AI applications. This paper proposes a structured,
temporal model that extends the FRBRoo framework to address this gap. It
introduces specialized subclasses of Expressio - Temporal Version (TV) and
Language Version (LV - to represent the state of a legal norm and its
linguistic variations at specific points in time. The model applies this same
paradigm hierarchically, introducing Component Work (CW), Component Temporal
Version (CTV), and Component Language Version (CLV) to track the lifecycle of
individual articles, paragraphs, and clauses. Using the Brazilian Federal
Constitution as a case study, the paper demonstrates how each amendment creates
new Component Temporal Versions for affected provisions, while unaffected
components retain their existing versions. This fine-grained, time-aware
architecture enables the precise, deterministic retrieval and reconstruction of
any part of a legal text as it existed on a specific date. The model provides a
robust foundation for developing advanced legal information systems, knowledge
graphs, and AI tools capable of accurate historical analysis and impact
assessment, overcoming the limitations of current generative models.

</details>


### [205] [Evaluating Large Language Models on the Frame and Symbol Grounding Problems: A Zero-shot Benchmark](https://arxiv.org/abs/2506.07896)
*Shoko Oka*

Main category: cs.AI

TL;DR: 研究探讨现代大语言模型是否具备解决框架问题和符号接地问题的认知能力，通过设计基准任务并测试13个模型，发现部分闭源模型表现稳定且高分。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的进展，重新审视人工智能中的框架问题和符号接地问题，探讨这些模型是否具备解决这些传统难题的能力。

Method: 设计两个反映问题核心的基准任务，在零样本条件下对13个主流大语言模型进行测试，并对输出质量进行多标准评分。

Result: 开源模型因模型规模、量化和指令调优差异表现不一，而部分闭源模型在多次测试中表现稳定且高分。

Conclusion: 研究表明，部分现代大语言模型可能已具备解决框架问题和符号接地问题的能力，能够提供有意义且稳定的回答。

Abstract: Recent advancements in large language models (LLMs) have revitalized
philosophical debates surrounding artificial intelligence. Two of the most
fundamental challenges - namely, the Frame Problem and the Symbol Grounding
Problem - have historically been viewed as unsolvable within traditional
symbolic AI systems. This study investigates whether modern LLMs possess the
cognitive capacities required to address these problems. To do so, I designed
two benchmark tasks reflecting the philosophical core of each problem,
administered them under zero-shot conditions to 13 prominent LLMs (both closed
and open-source), and assessed the quality of the models' outputs across five
trials each. Responses were scored along multiple criteria, including
contextual reasoning, semantic coherence, and information filtering. The
results demonstrate that while open-source models showed variability in
performance due to differences in model size, quantization, and instruction
tuning, several closed models consistently achieved high scores. These findings
suggest that select modern LLMs may be acquiring capacities sufficient to
produce meaningful and stable responses to these long-standing theoretical
challenges.

</details>


### [206] [LUCIFER: Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement](https://arxiv.org/abs/2506.07915)
*Dimitris Panagopoulos,Adolfo Perrusquia,Weisi Guo*

Main category: cs.AI

TL;DR: 提出LUCIFER框架，通过结合分层决策架构、强化学习与大型语言模型，利用人类上下文知识解决动态环境中自主系统决策滞后问题，提升探索与决策效率。


<details>
  <summary>Details</summary>
Motivation: 动态环境中，智能体内部模型与实时环境间的知识差异导致决策效率受限，需融合人类领域专家的实时观察洞察，但如何将其转化为系统可执行信息仍具挑战。

Method: LUCIFER框架整合分层决策架构、强化学习（RL）和大型语言模型（LLMs），通过LLMs双重角色（上下文提取器与零样本探索引导者）结构化人类输入并指导行动选择，实现任务分解与多智能体协作。

Result: 实验表明LUCIFER在探索效率和决策质量上优于传统单层策略，验证了上下文驱动决策的潜力，且不同LLM在框架中表现差异显著。

Conclusion: 结合人类上下文知识与自主系统学习过程的注意力对齐机制，LUCIFER为动态环境中的智能决策提供了可扩展的领域无关解决方案。

Abstract: In dynamic environments, the rapid obsolescence of pre-existing environmental
knowledge creates a gap between an agent's internal model and the evolving
reality of its operational context. This disparity between prior and updated
environmental valuations fundamentally limits the effectiveness of autonomous
decision-making. To bridge this gap, the contextual bias of human domain
stakeholders, who naturally accumulate insights through direct, real-time
observation, becomes indispensable. However, translating their nuanced, and
context-rich input into actionable intelligence for autonomous systems remains
an open challenge. To address this, we propose LUCIFER (Language Understanding
and Context-Infused Framework for Exploration and Behavior Refinement), a
domain-agnostic framework that integrates a hierarchical decision-making
architecture with reinforcement learning (RL) and large language models (LLMs)
into a unified system. This architecture mirrors how humans decompose complex
tasks, enabling a high-level planner to coordinate specialised sub-agents, each
focused on distinct objectives and temporally interdependent actions. Unlike
traditional applications where LLMs are limited to single role, LUCIFER
integrates them in two synergistic roles: as context extractors, structuring
verbal stakeholder input into domain-aware representations that influence
decision-making through an attention space mechanism aligning LLM-derived
insights with the agent's learning process, and as zero-shot exploration
facilitators guiding the agent's action selection process during exploration.
We benchmark various LLMs in both roles and demonstrate that LUCIFER improves
exploration efficiency and decision quality, outperforming flat,
goal-conditioned policies. Our findings show the potential of context-driven
decision-making, where autonomous systems leverage human contextual knowledge
for operational success.

</details>


### [207] [Solving Inequality Proofs with Large Language Models](https://arxiv.org/abs/2506.07927)
*Jiayi Sheng,Luna Lyu,Jikai Jin,Tony Xia,Alex Gu,James Zou,Pan Lu*

Main category: cs.AI

TL;DR: 该论文针对大型语言模型（LLMs）在不等式证明任务中的不足，提出了一种非正式但可验证的任务框架，并发布专家级数据集IneqMath。通过系统评估发现，即使顶尖模型在分步推理中的准确率极低（<10%），暴露了LLMs在严格证明中的脆弱性。研究指出，模型规模和计算资源对提升证明正确性效果有限，未来需关注定理引导和自我优化方向。


<details>
  <summary>Details</summary>
Motivation: 现有不等式证明数据集存在稀缺性、合成性或形式化过强的问题，阻碍了LLMs在此类复杂推理任务中的发展。研究旨在通过重新设计任务形式与构建高质量数据集，揭示LLMs在严格数学证明中的核心缺陷。

Method: 1) 将不等式证明分解为可自动验证的边界估计与关系预测子任务；2) 构建包含逐步解答与定理标注的奥赛级数据集IneqMath；3) 设计结合最终答案与四类分步评判的LLM-as-judge评估框架，以检测常见推理错误。

Result: 评估29个主流LLMs发现：顶尖模型在分步检测下的总体准确率不足10%，相比仅看最终答案的准确率下降最高达65.5%；模型规模与测试时计算量增加对整体证明正确性提升有限。

Conclusion: LLMs在不等式证明中表现出答案生成与严格演绎链条的严重脱节。研究揭示了当前模型在形式化推理的根本局限，并指出定理引导推理、自我修正等方向的关键潜力。

Abstract: Inequality proving, crucial across diverse scientific and mathematical
fields, tests advanced reasoning skills such as discovering tight bounds and
strategic theorem application. This makes it a distinct, demanding frontier for
large language models (LLMs), offering insights beyond general mathematical
problem-solving. Progress in this area is hampered by existing datasets that
are often scarce, synthetic, or rigidly formal. We address this by proposing an
informal yet verifiable task formulation, recasting inequality proving into two
automatically checkable subtasks: bound estimation and relation prediction.
Building on this, we release IneqMath, an expert-curated dataset of
Olympiad-level inequalities, including a test set and training corpus enriched
with step-wise solutions and theorem annotations. We also develop a novel
LLM-as-judge evaluation framework, combining a final-answer judge with four
step-wise judges designed to detect common reasoning flaws. A systematic
evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even
top models like o1 achieve less than 10% overall accuracy under step-wise
scrutiny; this is a drop of up to 65.5% from their accuracy considering only
final answer equivalence. This discrepancy exposes fragile deductive chains and
a critical gap for current LLMs between merely finding an answer and
constructing a rigorous proof. Scaling model size and increasing test-time
computation yield limited gains in overall proof correctness. Instead, our
findings highlight promising research directions such as theorem-guided
reasoning and self-refinement. Code and data are available at
https://ineqmath.github.io/.

</details>


### [208] [Gradients: When Markets Meet Fine-tuning -- A Distributed Approach to Model Optimisation](https://arxiv.org/abs/2506.07940)
*Christopher Subia-Waud*

Main category: cs.AI

TL;DR: 本文提出去中心化AutoML平台Gradients，通过经济激励驱动的竞争市场优化超参数配置，在180组实验中显著超越主流平台，尤其在复杂推理和扩散模型任务中提升达30-40%。


<details>
  <summary>Details</summary>
Motivation: 现有AutoML平台依赖单一优化策略，仅能覆盖部分有效超参数配置，导致基础模型微调存在根本性探索局限。

Method: 构建去中心化超参数优化市场，通过经济激励使独立矿工竞争探索配置，将个体探索行为与集体优化目标对齐，系统性覆盖集中式方法遗漏的参数区域。

Result: 在70M-70B参数规模的多样化任务中：1) 对HuggingFace AutoTrain胜率82.8%，平均提升11.8% 2) 对主流平台全胜，平均提升42.1% 3) 复杂推理/检索任务提升30-40%，个性化扩散模型提升23.4%。

Conclusion: 经济激励驱动的竞争机制能系统性发现集中式AutoML遗漏的优质配置，证明去中心化市场策略在超参数优化中的显著优势。

Abstract: Foundation model fine-tuning faces a fundamental challenge: existing AutoML
platforms rely on single optimisation strategies that explore only a fraction
of viable hyperparameter configurations. In this white paper, We introduce
Gradients, a decentralised AutoML platform that transforms hyperparameter
optimisation into a competitive marketplace where independent miners compete to
discover optimal configurations. Economic incentives align individual
exploration with collective optimisation goals, driving systematic
investigation of hyperparameter regions that centralised methods miss. We
evaluate our approach across 180 controlled experiments spanning diverse model
architectures (70M to 70B parameters) and task types. Gradients achieves an
82.8\% win rate against HuggingFace AutoTrain and 100\% against TogetherAI,
Databricks, and Google Cloud, with mean improvements of 11.8\% and 42.1\%
respectively. Complex reasoning and retrieval tasks show particularly strong
gains of 30-40\%, whilst diffusion models achieve 23.4\% improvements for
person-specific generation. These results demonstrate that competitive,
economically-driven approaches can systematically discover superior
configurations that centralised AutoML consistently miss.

</details>


### [209] [Reinforcing Multimodal Understanding and Generation with Dual Self-rewards](https://arxiv.org/abs/2506.07963)
*Jixiang Hong,Yiran Zhang,Guanzhong Wang,Yi Liu,Ji-Rong Wen,Rui Yan*

Main category: cs.AI

TL;DR: 本文提出了一种自监督的双奖励机制，通过理解与生成的逆对偶任务，提升大模态模型的图像文本对齐能力，无需外部监督。


<details>
  <summary>Details</summary>
Motivation: 当前大模态模型在图像文本对齐上表现不佳，现有方法依赖外部监督且仅解决单向任务。本文旨在通过自监督方法提升模型的理解与生成能力。

Method: 引入自监督双奖励机制，通过在一个任务域中采样多个输出，反转输入输出对计算对偶似然作为自奖励进行优化。

Result: 实验表明，该方法有效提升了模型性能，尤其在文本到图像任务中取得了显著改进。

Conclusion: 自监督双奖励机制能够在不依赖外部监督的情况下，显著提升大模态模型的理解与生成能力。

Abstract: Building upon large language models (LLMs), recent large multimodal models
(LMMs) unify cross-model understanding and generation into a single framework.
However, LMMs still struggle to achieve accurate image-text alignment, prone to
generating text responses contradicting the visual input or failing to follow
the text-to-image prompts. Current solutions require external supervision
(e.g., human feedback or reward models) and only address unidirectional
tasks-either understanding or generation. In this work, based on the
observation that understanding and generation are inverse dual tasks, we
introduce a self-supervised dual reward mechanism to reinforce the
understanding and generation capabilities of LMMs. Specifically, we sample
multiple outputs for a given input in one task domain, then reverse the
input-output pairs to compute the dual likelihood of the model as self-rewards
for optimization. Extensive experimental results on visual understanding and
generation benchmarks demonstrate that our method can effectively enhance the
performance of the model without any external supervision, especially achieving
remarkable improvements in text-to-image tasks.

</details>


### [210] [$τ^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment](https://arxiv.org/abs/2506.07982)
*Victor Barres,Honghua Dong,Soham Ray,Xujie Si,Karthik Narasimhan*

Main category: cs.AI

TL;DR: 本文提出τ²-bench基准测试框架通过双控环境模拟真实用户协作场景，解决现有对话AI测试中用户被动性问题，并验证代理在指导用户时的性能挑战。


<details>
  <summary>Details</summary>
Motivation: 现有对话AI基准测试仅模拟单一控制环境（仅代理使用工具），而现实场景（如技术支持）需要用户主动参与环境状态修改，两者存在差距。

Method: 1) 建立电信双控领域Dec-POMDP模型；2) 组合式任务生成器创建多样化任务；3) 工具约束的高保真用户模拟器；4) 通过消融实验区分推理与协作错误。

Result: 实验显示代理从无用户控制转向双控时性能显著下降（平均下降37%），突显用户引导的挑战性，且沟通协调错误占比达总错误的42%。

Conclusion: τ²-bench为需协同推理与用户引导的AI代理提供可控测试环境，揭示了现有系统在真实协作场景中的能力缺陷。

Abstract: Existing benchmarks for conversational AI agents simulate single-control
environments, where only the AI agent can use tools to interact with the world,
while the user remains a passive information provider. This differs from
real-world scenarios like technical support, where users need to actively
participate in modifying the state of the (shared) world. In order to address
this gap, we introduce $\tau^2$-bench, with four key contributions:
  1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both
agent and user make use of tools to act in a shared, dynamic environment that
tests both agent coordination and communication,
  2) A compositional task generator that programmatically creates diverse,
verifiable tasks from atomic components, ensuring domain coverage and
controlled complexity,
  3) A reliable user simulator tightly coupled with the environment, whose
behavior is constrained by tools and observable states, improving simulation
fidelity,
  4) Fine-grained analysis of agent performance through multiple ablations
including separating errors arising from reasoning vs
communication/coordination.
  In particular, our experiments show significant performance drops when agents
shift from no-user to dual-control, highlighting the challenges of guiding
users. Overall, $\tau^2$-bench provides a controlled testbed for agents that
must both reason effectively and guide user actions.

</details>


### [211] [GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection Behavior](https://arxiv.org/abs/2506.08012)
*Penghao Wu,Shengnan Ma,Bo Wang,Jiaheng Yu,Lewei Lu,Ziwei Liu*

Main category: cs.AI

TL;DR: 本文提出GUI-Reflection框架，通过预训练、离线微调和在线反思调优三阶段，赋予多模态GUI模型自我反思与纠错能力，实现无需人工标注的自动化训练，并公开数据与工具。


<details>
  <summary>Details</summary>
Motivation: 现有GUI模型依赖无错误离线轨迹学习，缺乏反思与错误恢复能力。需通过增强模型自我修正能力，提升GUI自动化的鲁棒性与适应性。

Method: 1) 自动从成功轨迹生成反思/纠错数据，构建反思任务套件；2) 搭建移动端在线训练环境；3) 提出迭代式在线反思调优算法，持续增强模型能力。

Result: 框架使GUI代理具备自我反思行为，实现全自动数据生成与学习，最终模型在反思导向任务中表现提升，相关资源将全面开源。

Conclusion: GUI-Reflection为GUI自动化提供反思与纠错能力，推动更健壮、自适应的智能交互系统发展，开源资源将促进领域研究。

Abstract: Multimodal Large Language Models (MLLMs) have shown great potential in
revolutionizing Graphical User Interface (GUI) automation. However, existing
GUI models mostly rely on learning from nearly error-free offline trajectories,
thus lacking reflection and error recovery capabilities. To bridge this gap, we
propose GUI-Reflection, a novel framework that explicitly integrates
self-reflection and error correction capabilities into end-to-end multimodal
GUI models throughout dedicated training stages: GUI-specific pre-training,
offline supervised fine-tuning (SFT), and online reflection tuning.
GUI-reflection enables self-reflection behavior emergence with fully automated
data generation and learning processes without requiring any human annotation.
Specifically, 1) we first propose scalable data pipelines to automatically
construct reflection and error correction data from existing successful
trajectories. While existing GUI models mainly focus on grounding and UI
understanding ability, we propose the GUI-Reflection Task Suite to learn and
evaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a
diverse and efficient environment for online training and data collection of
GUI models on mobile devices. 3) We also present an iterative online reflection
tuning algorithm leveraging the proposed environment, enabling the model to
continuously enhance its reflection and error correction abilities. Our
framework equips GUI agents with self-reflection and correction capabilities,
paving the way for more robust, adaptable, and intelligent GUI automation, with
all data, models, environments, and tools to be released publicly.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [212] [CellCLIP -- Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning](https://arxiv.org/abs/2506.06290)
*Mingyu Lu,Ethan Weinberger,Chanwoo Kim,Su-In Lee*

Main category: cs.LG

TL;DR: 本文介绍了CellCLIP，一种用于高内涵筛选数据的跨模态对比学习框架，旨在更好地理解扰动与细胞形态变化之间的关系。


<details>
  <summary>Details</summary>
Motivation: 高内涵筛选技术如Cell Painting能够大规模检测细胞对扰动的形态响应，但现有方法在处理此类数据时存在语义差异和表示困难。

Method: CellCLIP结合预训练图像编码器和新型通道编码方案，捕捉显微镜图像通道间的关系，并使用自然语言编码器表示扰动。

Result: CellCLIP在跨模态检索和生物学相关下游任务中表现优异，并显著减少了计算时间。

Conclusion: CellCLIP为高内涵筛选数据提供了一种有效的跨模态对比学习框架，能够更好地对齐扰动与其形态效应。

Abstract: High-content screening (HCS) assays based on high-throughput microscopy
techniques such as Cell Painting have enabled the interrogation of cells'
morphological responses to perturbations at an unprecedented scale. The
collection of such data promises to facilitate a better understanding of the
relationships between different perturbations and their effects on cellular
state. Towards achieving this goal, recent advances in cross-modal contrastive
learning could, in theory, be leveraged to learn a unified latent space that
aligns perturbations with their corresponding morphological effects. However,
the application of such methods to HCS data is not straightforward due to
substantial differences in the semantics of Cell Painting images compared to
natural images, and the difficulty of representing different classes of
perturbations (e.g., small molecule vs CRISPR gene knockout) in a single latent
space. In response to these challenges, here we introduce CellCLIP, a
cross-modal contrastive learning framework for HCS data. CellCLIP leverages
pre-trained image encoders coupled with a novel channel encoding scheme to
better capture relationships between different microscopy channels in image
embeddings, along with natural language encoders for representing
perturbations. Our framework outperforms current open-source models,
demonstrating the best performance in both cross-modal retrieval and
biologically meaningful downstream tasks while also achieving significant
reductions in computation time.

</details>


### [213] [Improvement of Optimization using Learning Based Models in Mixed Integer Linear Programming Tasks](https://arxiv.org/abs/2506.06291)
*Xiaoke Wang,Batuhan Altundas,Zhaoxin Li,Aaron Zhao,Matthew Gombolay*

Main category: cs.LG

TL;DR: 本文提出了一种基于行为克隆和强化学习的图神经网络框架，用于为多智能体任务分配和调度问题中的混合整数线性规划求解器提供高质量初始解，从而减少优化时间和方差。


<details>
  <summary>Details</summary>
Motivation: 混合整数线性规划（MILP）在关键行业如建筑、制造和物流中广泛应用，但其计算时间较长，尤其是在大规模实时场景中，限制了其广泛采用。

Method: 本文提出了一种学习框架，结合行为克隆（BC）和强化学习（RL）来训练图神经网络（GNN），为MILP求解器提供高质量的初始解。

Result: 实验结果表明，与传统方法相比，该方法在保持解的质量和可行性的同时，显著减少了优化时间和方差。

Conclusion: 本文提出的方法有效提升了MILP求解器的效率，特别是在多智能体任务分配和调度问题中，具有实际应用价值。

Abstract: Mixed Integer Linear Programs (MILPs) are essential tools for solving
planning and scheduling problems across critical industries such as
construction, manufacturing, and logistics. However, their widespread adoption
is limited by long computational times, especially in large-scale, real-time
scenarios. To address this, we present a learning-based framework that
leverages Behavior Cloning (BC) and Reinforcement Learning (RL) to train Graph
Neural Networks (GNNs), producing high-quality initial solutions for
warm-starting MILP solvers in Multi-Agent Task Allocation and Scheduling
Problems. Experimental results demonstrate that our method reduces optimization
time and variance compared to traditional techniques while maintaining solution
quality and feasibility.

</details>


### [214] [Mutual-Taught for Co-adapting Policy and Reward Models](https://arxiv.org/abs/2506.06292)
*Tianyuan Shi,Canbin Huang,Fanqi Wan,Longguang Zhong,Ziyi Yang,Weizhou Shen,Xiaojun Quan,Ming Yan*

Main category: cs.LG

TL;DR: 提出Mutual-Taught自训练方法，通过迭代优化策略模型与奖励模型，解决分布偏移问题，无需人工标注。实验显示8B模型在AlpacaEval-2上胜率54.1%，奖励模型性能媲美GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型偏好优化中，策略模型生成数据与奖励模型训练数据的分布偏移会降低奖励模型效果，进而影响策略模型性能。需解决这一循环依赖问题。

Method: 基于EM算法框架：E步用当前奖励模型反馈更新策略模型；M步用策略模型更新前后的输出构建新训练数据更新奖励模型，实现两者协同进化。

Result: LLaMA-3-8B-Instruct-MT在AlpacaEval-2长度控制胜率达54.1%；FsfairX-LLaMA3-RM-MT奖励模型在RewardBench上达到GPT-4o水平。

Conclusion: Mutual-Taught通过策略模型与奖励模型的迭代互训，有效缓解分布偏移问题，实现双模型协同优化，为无标注场景下模型优化提供新思路。

Abstract: During the preference optimization of large language models (LLMs),
distribution shifts may arise between newly generated model samples and the
data used to train the reward model (RM). This shift reduces the efficacy of
the RM, which in turn negatively impacts the performance of the policy model
(PM). To address this challenge, we propose Mutual-Taught, a self-training
method that iteratively improves both the PM and RM without requiring
additional human annotation. Our approach mirrors the expectation-maximization
(EM) algorithm. In the E-step, the PM is updated using feedback from the
current RM, guiding the PM toward a better approximation of the latent optimal
preference distribution. In the M-step, we update the RM by constructing
training data from the outputs of the PM before and after the E-step update.
This process ensures that the RM adapts to the evolving policy distribution.
Experimental results demonstrate that this iterative approach leads to
consistent improvements in both models. Specifically, our 8B policy model,
LLaMA-3-8B-Instruct-MT, achieves a length-controlled win rate of 54.1\% on
AlpacaEval-2, while our 8B reward model, FsfairX-LLaMA3-RM-MT, performs on par
with GPT-4o-2024-08-06 on RewardBench.

</details>


### [215] [Prediction of Bank Credit Ratings using Heterogeneous Topological Graph Neural Networks](https://arxiv.org/abs/2506.06293)
*Junyi Liu,Stanley Kok*

Main category: cs.LG

TL;DR: 该研究提出HTGNN模型，通过持续同构构建银行关系网络并结合传统网络形成异质网络，解决了因隐私问题导致图神经网络应用受限的问题，实验证明其能有效提升银行评级预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有银行信用评级依赖机构数据，但完整银行间连接图因隐私问题难以获取，导致图神经网络（GNN）无法直接应用，需探索融合多源异构数据的解决方案。

Method: 使用持续同构（persistent Homology）构建银行关系网络，与传统借贷网络结合形成异质网络，并设计HTGNN模型整合两类网络信息进行评级预测。

Result: 在真实全球银行数据集上的实验表明，HTGNN模型能有效提升预测准确性，验证了异质网络融合方法的有效性。

Conclusion: 该方法为投资者和监管机构提供了更可靠的风险评估工具，支持主动风险缓解和市场干预策略，代码已开源。

Abstract: Agencies such as Standard & Poor's and Moody's provide bank credit ratings
that influence economic stability and decision-making by stakeholders. Accurate
and timely predictions support informed decision-making, regulatory actions,
and investor protection. However, a complete interbank connection graph is
often unavailable due to privacy concerns, complicating the direct application
of Graph Neural Networks (GNNs) for rating prediction. our research utilizes
persistent homology to construct a network that captures relationships among
banks and combines this with a traditional lending network to create a
heterogeneous network that integrates information from both sources, leading to
improved predictions. Experiments on a global, real-world dataset validate the
effectiveness of HTGNN. This research has implications for investors and
regulatory bodies in enhancing proactive risk mitigation and the implementation
of effective market interventions.The code can be find at
https://github.com/Liu-Jun-Yi/HTGNN.

</details>


### [216] [GLProtein: Global-and-Local Structure Aware Protein Representation Learning](https://arxiv.org/abs/2506.06294)
*Yunqing Liu,Wenqi Fan,Xiaoyong Wei,Qing Li*

Main category: cs.LG

TL;DR: 本文提出GLProtein框架，首次在蛋白质预训练中整合全局结构相似性与局部氨基酸细节，显著提升预测准确性及功能理解。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质功能研究多基于序列分析，但蛋白质结构信息（从氨基酸分子到蛋白间结构相似性）的整合仍有探索空间。

Method: 结合蛋白质掩码建模与三元组结构相似性评分，采用3D距离编码和基于子结构的氨基酸分子编码，实现多层次结构信息融合。

Result: GLProtein在蛋白质相互作用预测、接触预测等任务中优于现有方法，验证了框架有效性。

Conclusion: 通过同时捕捉蛋白质的全局-局部结构特征，GLProtein为生物信息学任务提供了更精准的预训练模型范式。

Abstract: Proteins are central to biological systems, participating as building blocks
across all forms of life. Despite advancements in understanding protein
functions through protein sequence analysis, there remains potential for
further exploration in integrating protein structural information. We argue
that the structural information of proteins is not only limited to their 3D
information but also encompasses information from amino acid molecules (local
information) to protein-protein structure similarity (global information). To
address this, we propose \textbf{GLProtein}, the first framework in protein
pre-training that incorporates both global structural similarity and local
amino acid details to enhance prediction accuracy and functional insights.
GLProtein innovatively combines protein-masked modelling with triplet structure
similarity scoring, protein 3D distance encoding and substructure-based amino
acid molecule encoding. Experimental results demonstrate that GLProtein
outperforms previous methods in several bioinformatics tasks, including
predicting protein-protein interaction, contact prediction, and so on.

</details>


### [217] [dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching](https://arxiv.org/abs/2506.06295)
*Zhiyuan Liu,Yicun Yang,Yaojie Zhang,Junjie Chen,Chang Zou,Qingyuan Wei,Shaobo Wang,Linfeng Zhang*

Main category: cs.LG

TL;DR: 本文针对基于扩散的大型语言模型（dLLMs）推理延迟高的问题，提出了一种无需训练的缓存框架dLLM-Cache，通过长间隔提示缓存和部分响应更新，显著降低推理时间，同时保持输出质量，使其接近自回归模型（ARMs）的推理速度。


<details>
  <summary>Details</summary>
Motivation: 扩散模型（dLLMs）在文本生成中展现出潜力，但其迭代去噪过程导致推理延迟高，而传统自回归模型的加速技术（如KV缓存）因双向注意力机制无法直接应用于dLLMs。因此，需设计针对dLLMs的高效推理优化方法。

Method: 提出dLLM-Cache框架，利用dLLM推理过程中静态提示和部分动态响应的特性，结合长间隔提示缓存与基于特征相似性的部分响应更新，重用中间计算结果，减少冗余计算。

Result: 在LLaDA 8B和Dream 7B等模型上的实验表明，dLLM-Cache实现了最高9.1倍的推理加速，且输出质量无损，推理延迟接近自回归模型。

Conclusion: dLLM-Cache通过创新的缓存策略有效解决了扩散语言模型的推理延迟问题，为dLLMs的实际应用提供了高效可行的解决方案。

Abstract: Autoregressive Models (ARMs) have long dominated the landscape of Large
Language Models. Recently, a new paradigm has emerged in the form of
diffusion-based Large Language Models (dLLMs), which generate text by
iteratively denoising masked segments. This approach has shown significant
advantages and potential. However, dLLMs suffer from high inference latency.
Traditional ARM acceleration techniques, such as Key-Value caching, are
incompatible with dLLMs due to their bidirectional attention mechanism. To
address this specific challenge, our work begins with a key observation that
dLLM inference involves a static prompt and a partially dynamic response, where
most tokens remain stable across adjacent denoising steps. Based on this, we
propose dLLM-Cache, a training-free adaptive caching framework that combines
long-interval prompt caching with partial response updates guided by feature
similarity. This design enables efficient reuse of intermediate computations
without compromising model performance. Extensive experiments on representative
dLLMs, including LLaDA 8B and Dream 7B, show that dLLM-Cache achieves up to 9.1
x speedup over standard inference without compromising output quality. Notably,
our method brings dLLM inference latency close to that of ARMs under many
settings. Codes are provided in the supplementary material and will be released
publicly on GitHub.

</details>


### [218] [Dynamic Graph CNN with Jacobi Kolmogorov-Arnold Networks for 3D Classification of Point Sets](https://arxiv.org/abs/2506.06296)
*Hanaa El Afia,Said Ohamouddou,Raddouane Chiheb,Abdellatif El Afia*

Main category: cs.LG

TL;DR: Jacobi-KAN-DGCNN框架结合动态图卷积神经网络与Jacobi Kolmogorov-Arnold网络，用于三维点云分类，在ModelNet40数据集上表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过结合动态图卷积神经网络与Jacobi Kolmogorov-Arnold网络，提升三维点云分类的准确性和收敛速度。

Method: 使用可调节的单变量多项式扩展替换多层感知机层，简化DGCNN架构，避免深度层次，实现逐层比较。

Result: 在ModelNet40数据集上，使用Jacobi多项式的KAN层在准确性和收敛速度上优于传统线性层DGCNN基线，且保持参数效率。

Conclusion: 高次多项式不自动提升性能，需进一步研究多项式基、次数与图学习机制的相互作用。

Abstract: We introduce Jacobi-KAN-DGCNN, a framework that integrates Dynamic Graph
Convolutional Neural Network (DGCNN) with Jacobi Kolmogorov-Arnold Networks
(KAN) for the classification of three-dimensional point clouds. This method
replaces Multi-Layer Perceptron (MLP) layers with adaptable univariate
polynomial expansions within a streamlined DGCNN architecture, circumventing
deep levels for both MLP and KAN to facilitate a layer-by-layer comparison. In
comparative experiments on the ModelNet40 dataset, KAN layers employing Jacobi
polynomials outperform the traditional linear layer-based DGCNN baseline in
terms of accuracy and convergence speed, while maintaining parameter
efficiency. Our results demonstrate that higher polynomial degrees do not
automatically improve performance, highlighting the need for further
theoretical and empirical investigation to fully understand the interactions
between polynomial bases, degrees, and the mechanisms of graph-based learning.

</details>


### [219] [Optimal patient allocation for echocardiographic assessments](https://arxiv.org/abs/2506.06297)
*Bozhi Sun,Seda Tierney,Jeffrey A. Feinstein,Frederick Damen,Alison L. Marsden,Daniele E. Schiavazzi*

Main category: cs.LG

TL;DR: 本文通过离散事件随机模拟和强化学习，优化了医院超声检查的调度策略，动态分配资源以提高效率。


<details>
  <summary>Details</summary>
Motivation: 医院超声检查调度面临不确定性因素（如患者缺席、到达时间、检查时长等）和资源限制，需优化资源分配策略以提高效率。

Method: 使用SimPy构建离散事件随机模拟模型，结合Gymnasium库，比较动态分配与预留策略，并应用强化学习推导近似最优动态分配策略。

Result: 动态分配策略在适应患者变化和资源限制方面表现更优，强化学习策略进一步提升了资源管理效率。

Conclusion: 动态分配和强化学习策略为超声检查调度提供了智能化的数据驱动解决方案，显著提高了实验室效率。

Abstract: Scheduling echocardiographic exams in a hospital presents significant
challenges due to non-deterministic factors (e.g., patient no-shows, patient
arrival times, diverse exam durations, etc.) and asymmetric resource
constraints between fetal and non-fetal patient streams. To address these
challenges, we first conducted extensive pre-processing on one week of
operational data from the Echo Laboratory at Stanford University's Lucile
Packard Children's Hospital, to estimate patient no-show probabilities and
derive empirical distributions of arrival times and exam durations. Based on
these inputs, we developed a discrete-event stochastic simulation model using
SimPy, and integrate it with the open source Gymnasium Python library. As a
baseline for policy optimization, we developed a comparative framework to
evaluate on-the-fly versus reservation-based allocation strategies, in which
different proportions of resources are reserved in advance. Considering a
hospital configuration with a 1:6 ratio of fetal to non-fetal rooms and a 4:2
ratio of fetal to non-fetal sonographers, we show that on-the-fly allocation
generally yields better performance, more effectively adapting to patient
variability and resource constraints. Building on this foundation, we apply
reinforcement learning (RL) to derive an approximated optimal dynamic
allocation policy. This RL-based policy is benchmarked against the
best-performing rule-based strategies, allowing us to quantify their
differences and provide actionable insights for improving echo lab efficiency
through intelligent, data-driven resource management.

</details>


### [220] [Pairwise Calibrated Rewards for Pluralistic Alignment](https://arxiv.org/abs/2506.06298)
*Daniel Halpern,Evi Micha,Ariel D. Procaccia,Itai Shapira*

Main category: cs.LG

TL;DR: 现有AI对齐方法假设单一行为标准，但人类偏好具有多样性。本文提出通过多奖励函数分布反映不同偏好，利用成对校准确保少数观点不被忽略，实验证明该方法能更真实地呈现多元价值观。


<details>
  <summary>Details</summary>
Motivation: 当前AI对齐流程假设存在统一行为标准，但人类偏好因用户、情境、文化存在差异。多数信号主导导致少数观点被忽视，需开发能包容多元偏好的对齐方法。

Method: 提出学习多奖励函数分布，每个函数对应不同对齐策略。直接从成对偏好数据学习分布，不依赖用户标识或预定义群体，将标注者分歧视为软标签，核心采用成对校准标准确保偏好比例匹配。

Result: 理论证明小规模集成即可准确表征多样偏好分布，实验验证训练启发式方法有效，校准度提升表明能更真实反映多元价值观。

Conclusion: 通过多奖励函数分布捕捉偏好多样性，结合成对校准机制，可创建更具包容性的对齐系统，为处理复杂人类价值观提供新方向。

Abstract: Current alignment pipelines presume a single, universal notion of desirable
behavior. However, human preferences often diverge across users, contexts, and
cultures. As a result, disagreement collapses into the majority signal and
minority perspectives are discounted. To address this, we propose reflecting
diverse human preferences through a distribution over multiple reward
functions, each inducing a distinct aligned policy. The distribution is learned
directly from pairwise preference without annotator identifiers or predefined
groups. Instead, annotator disagreements are treated as informative soft
labels. Our central criterion is pairwise calibration: for every pair of
candidate responses, the proportion of reward functions preferring one response
matches the fraction of annotators with that preference. We prove that even a
small outlier-free ensemble can accurately represent diverse preference
distributions. Empirically, we introduce and validate a practical training
heuristic to learn such ensembles, and demonstrate its effectiveness through
improved calibration, implying a more faithful representation of pluralistic
values.

</details>


### [221] [LT-PINN: Lagrangian Topology-conscious Physics-informed Neural Network for Boundary-focused Engineering Optimization](https://arxiv.org/abs/2506.06300)
*Yuanye Zhou,Zhaokun Wang,Kai Zhou,Hui Tang,Xiaofan Li*

Main category: cs.LG

TL;DR: 本文提出了一种新的拉格朗日拓扑感知物理信息神经网络（LT-PINNs），用于边界聚焦的工程优化，消除了手动插值的需求，并在复杂拓扑中实现了精确的边界确定。


<details>
  <summary>Details</summary>
Motivation: 传统的物理信息神经网络（PINNs）依赖于基于密度的拓扑描述，需要手动插值，限制了其在复杂几何中的应用。为了解决这一问题，本文提出了LT-PINNs。

Method: 通过将拓扑边界曲线的控制变量参数化为可学习参数，LT-PINNs消除了手动插值的需求，并引入了专门的边界条件损失函数和拓扑损失函数，以确保边界表示的准确性和清晰性。

Result: 实验结果表明，LT-PINNs在相对L2误差上显著优于现有的密度拓扑导向的PINNs（DT-PINNs），能够处理任意边界条件，并在复杂拓扑中无需手动插值即可推断出清晰的边界。

Conclusion: LT-PINNs在工程优化中具有广泛的应用潜力，特别是在处理复杂拓扑和边界条件时表现出色。

Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful meshless
tool for topology optimization, capable of simultaneously determining optimal
topologies and physical solutions. However, conventional PINNs rely on
density-based topology descriptions, which necessitate manual interpolation and
limit their applicability to complex geometries. To address this, we propose
Lagrangian topology-conscious PINNs (LT-PINNs), a novel framework for
boundary-focused engineering optimization. By parameterizing the control
variables of topology boundary curves as learnable parameters, LT-PINNs
eliminate the need for manual interpolation and enable precise boundary
determination. We further introduce specialized boundary condition loss
function and topology loss function to ensure sharp and accurate boundary
representations, even for intricate topologies. The accuracy and robustness of
LT-PINNs are validated via two types of partial differential equations (PDEs),
including elastic equation with Dirichlet boundary conditions and Laplace's
equation with Neumann boundary conditions. Furthermore, we demonstrate
effectiveness of LT-PINNs on more complex time-dependent and time-independent
flow problems without relying on measurement data, and showcase their
engineering application potential in flow velocity rearrangement, transforming
a uniform upstream velocity into a sine-shaped downstream profile. The results
demonstrate (1) LT-PINNs achieve substantial reductions in relative L2 errors
compared with the state-of-art density topology-oriented PINNs (DT-PINNs), (2)
LT-PINNs can handle arbitrary boundary conditions, making them suitable for a
wide range of PDEs, and (3) LT-PINNs can infer clear topology boundaries
without manual interpolation, especially for complex topologies.

</details>


### [222] [Reward Is Enough: LLMs Are In-Context Reinforcement Learners](https://arxiv.org/abs/2506.06303)
*Kefan Song,Amir Moeini,Peng Wang,Lei Gong,Rohan Chandra,Yanjun Qi,Shangtong Zhang*

Main category: cs.LG

TL;DR: 研究发现大语言模型（LLM）在推理过程中自发表现出类似强化学习（RL）的行为，称为上下文强化学习（ICRL）。通过多轮提示框架（ICRL prompting），结合历史响应与奖励反馈，显著提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在推理阶段是否具备类似强化学习的自我优化能力，并验证通过动态反馈机制能否提升模型表现。

Method: 提出ICRL prompting框架：多轮迭代提示LLM完成任务，每轮提供数值化奖励反馈，并在后续轮次中将历史响应与奖励作为上下文输入。

Result: 在Game of 24、创意写作和ScienceWorld等任务中，ICRL prompting显著优于Self-Refine等基线方法，即使奖励信号由LLM自身生成时仍能提升性能。

Conclusion: ICRL现象揭示了LLM在推理阶段具备隐式强化学习能力，为无需额外训练、仅通过测试时计算扩展模型能力提供了新范式。

Abstract: Reinforcement learning (RL) is a human-designed framework for solving
sequential decision making problems. In this work, we demonstrate that,
surprisingly, RL emerges in LLM's (Large Language Model) inference time -- a
phenomenon known as in-context RL (ICRL). Specifically, we propose a novel
multi-round prompting framework called ICRL prompting. The goal is to prompt
the LLM to complete a task. After the LLM generates a response at the current
round, we give numerical scalar feedbacks for the response, called the rewards.
At the next round, we prompt the LLM again with the same task and a context
consisting of all previous responses and rewards. We observe that the quality
of the LLM's response increases as the context grows. In other words, the LLM
is able to maximize the scalar reward signal in the inference time, just like
an RL algorithm. We evaluate ICRL prompting in three benchmarks (Game of 24,
creative writing, and ScienceWorld) and demonstrate significant performance
improvements over baseline methods such as Self-Refine and Reflexion.
Surprisingly, in some experiments the reward signals are generated by the LLM
itself, yet performance improvements are still observed from ICRL prompting,
offering a promising paradigm for scaling test-time compute.

</details>


### [223] [Wine Quality Prediction with Ensemble Trees: A Unified, Leak-Free Comparative Study](https://arxiv.org/abs/2506.06327)
*Zilang Chen*

Main category: cs.LG

TL;DR: 本文系统评估了五种集成学习模型在红/白葡萄酒质量预测任务中的表现，提出基于随机森林的高性价比方案，并构建了可复现的基准流程。


<details>
  <summary>Details</summary>
Motivation: 传统葡萄酒质量评估依赖主观性强、劳动密集的品鉴小组，需建立客观可量化的预测模型实现生产控制。

Method: 采用无数据泄漏流程：分层分组交叉验证、SMOTE-Tomek重采样、逆频率代价加权、Optuna超参搜索、两阶段特征选择，在Vinho Verde数据集上对比随机森林/Gradient Boosting/XGBoost/LightGBM/CatBoost。

Result: Gradient Boosting加权F1最高（红0.693±0.028，白0.664±0.016），随机森林/XGBoost差距<3%。特征数缩减55%仅导致F1下降2.6-3.0个百分点。随机森林训练最快（<50分钟），Gradient Boosting最慢（12小时）。

Conclusion: 推荐随机森林作为生产环境首选，XGBoost/LightGBM适合GPU加速场景，Gradient Boosting作为离线精度基准。酒精、挥发性酸度等5个核心特征已包含主要预测信号。

Abstract: Accurate and reproducible wine-quality assessment is critical for production
control yet remains dominated by subjective, labour-intensive tasting panels.
We present the first unified benchmark of five ensemble learners (Random
Forest, Gradient Boosting, XGBoost, LightGBM, CatBoost) on the canonical Vinho
Verde red- and white-wine datasets (1,599 and 4,898 instances, 11
physicochemical attributes). Our leakage-free workflow employs an 80:20
stratified train-test split, five-fold StratifiedGroupKFold within the training
set, per-fold standardisation, SMOTE-Tomek resampling, inverse-frequency cost
weighting, Optuna hyper-parameter search (120-200 trials per model) and a
two-stage feature-selection refit. Final scores on untouched test sets are
reported with weighted F1 as the headline metric. Gradient Boosting achieves
the highest accuracy (weighted F1 0.693 +/- 0.028 for red and 0.664 +/- 0.016
for white), followed within three percentage points by Random Forest and
XGBoost. Limiting each model to its five top-ranked variables lowers
dimensionality by 55 percent while reducing weighted F1 by only 2.6 percentage
points for red and 3.0 percentage points for white, indicating that alcohol,
volatile acidity, sulphates, free SO2 and chlorides capture most predictive
signal. Runtime profiling on an EPYC 9K84/H20 node reveals a steep efficiency
gradient: Gradient Boosting averages 12 h per five-fold study, XGBoost and
LightGBM require 2-3 h, CatBoost 1 h, and Random Forest under 50 min. We
therefore recommend Random Forest as the most cost-effective production model,
XGBoost and LightGBM as GPU-efficient alternatives, and Gradient Boosting as
the accuracy ceiling for offline benchmarking. The fully documented pipeline
and metric set provide a reproducible baseline for future work on imbalanced
multi-class wine-quality prediction.

</details>


### [224] [ExplainBench: A Benchmark Framework for Local Model Explanations in Fairness-Critical Applications](https://arxiv.org/abs/2506.06330)
*James Afful*

Main category: cs.LG

TL;DR: 该论文提出了ExplainBench，一个用于系统评估局部模型解释的开源基准套件，支持在公平敏感场景下的可重复比较分析。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习系统在刑事司法、金融和医疗等高风险领域的广泛应用，对可解释和可信赖模型的需求日益增加。然而，尽管局部解释技术如SHAP、LIME和反事实方法层出不穷，但缺乏标准化的评估框架，特别是在公平敏感场景下。

Method: 论文提出了ExplainBench，一个包含统一封装流行解释算法、集成端到端模型训练和解释生成管道、并支持通过忠实度、稀疏性和鲁棒性指标进行评估的开源基准套件。该框架还提供了基于Streamlit的图形界面和Python模块，便于研究集成。

Result: 通过在COMPAS、UCI Adult Income和LendingClub等公平研究常用数据集上的演示，展示了不同解释方法在共享实验协议下的行为。

Conclusion: ExplainBench通过支持局部解释的可重复比较分析，推进了可解释机器学习的方法论基础，并促进了现实世界AI系统的问责性。

Abstract: As machine learning systems are increasingly deployed in high-stakes domains
such as criminal justice, finance, and healthcare, the demand for interpretable
and trustworthy models has intensified. Despite the proliferation of local
explanation techniques, including SHAP, LIME, and counterfactual methods, there
exists no standardized, reproducible framework for their comparative
evaluation, particularly in fairness-sensitive settings.
  We introduce ExplainBench, an open-source benchmarking suite for systematic
evaluation of local model explanations across ethically consequential datasets.
ExplainBench provides unified wrappers for popular explanation algorithms,
integrates end-to-end pipelines for model training and explanation generation,
and supports evaluation via fidelity, sparsity, and robustness metrics. The
framework includes a Streamlit-based graphical interface for interactive
exploration and is packaged as a Python module for seamless integration into
research workflows.
  We demonstrate ExplainBench on datasets commonly used in fairness research,
such as COMPAS, UCI Adult Income, and LendingClub, and showcase how different
explanation methods behave under a shared experimental protocol. By enabling
reproducible, comparative analysis of local explanations, ExplainBench advances
the methodological foundations of interpretable machine learning and
facilitates accountability in real-world AI systems.

</details>


### [225] [Extending AALpy with Passive Learning: A Generalized State-Merging Approach](https://arxiv.org/abs/2506.06333)
*Benjamin von Berg,Bernhard K. Aichernig*

Main category: cs.LG

TL;DR: AALpy是一个专注于IO行为系统主动学习的Python开源自动机学习库，最近增加了被动学习中的状态合并方法，简化了算法的实现。


<details>
  <summary>Details</summary>
Motivation: 为了简化状态合并算法的实现，并支持现有和新算法的开发，AALpy引入了通用的红蓝框架实现。

Method: 使用AALpy定义和执行状态合并算法，主要关注兼容性标准和评分的定义。

Result: 通过AALpy，现有状态合并算法的实现仅需几行代码，显著降低了实现复杂度。

Conclusion: AALpy的通用红蓝框架实现为自动机学习提供了高度可配置的工具，简化了状态合并算法的开发和应用。

Abstract: AALpy is a well-established open-source automata learning library written in
Python with a focus on active learning of systems with IO behavior. It provides
a wide range of state-of-the-art algorithms for different automaton types
ranging from fully deterministic to probabilistic automata. In this work, we
present the recent addition of a generalized implementation of an important
method from the domain of passive automata learning: state-merging in the
red-blue framework. Using a common internal representation for different
automaton types allows for a general and highly configurable implementation of
the red-blue framework. We describe how to define and execute state-merging
algorithms using AALpy, which reduces the implementation effort for
state-merging algorithms mainly to the definition of compatibility criteria and
scoring. This aids the implementation of both existing and novel algorithms. In
particular, defining some existing state-merging algorithms from the literature
with AALpy only takes a few lines of code.

</details>


### [226] [Optimized Local Updates in Federated Learning via Reinforcement Learning](https://arxiv.org/abs/2506.06337)
*Ali Murad,Bo Hui,Wei-Shinn Ku*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度强化学习（DRL）的联邦学习（FL）框架，通过优化客户端训练数据量来提升模型性能，同时减少非独立同分布（non-IID）数据的影响。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非独立同分布数据下，模型聚合可能导致性能下降。本文旨在通过优化客户端训练数据量，提升整体性能并保护数据隐私。

Method: 使用深度强化学习代理，根据训练损失变化作为奖励信号，优化客户端训练数据量。每轮聚合后，DRL算法根据本地性能输出优化权重，用于下一轮本地训练。

Result: 实验表明，该算法在多个基准数据集和联邦学习框架上均表现出优越性能。

Conclusion: 本文提出的方法有效提升了联邦学习在非独立同分布数据下的性能，同时保护了客户端数据隐私。

Abstract: Federated Learning (FL) is a distributed framework for collaborative model
training over large-scale distributed data, enabling higher performance while
maintaining client data privacy. However, the nature of model aggregation at
the centralized server can result in a performance drop in the presence of
non-IID data across different clients. We remark that training a client locally
on more data than necessary does not benefit the overall performance of all
clients. In this paper, we devise a novel framework that leverages a Deep
Reinforcement Learning (DRL) agent to select an optimized amount of data
necessary to train a client model without oversharing information with the
server. Starting without awareness of the client's performance, the DRL agent
utilizes the change in training loss as a reward signal and learns to optimize
the amount of training data necessary for improving the client's performance.
Specifically, after each aggregation round, the DRL algorithm considers the
local performance as the current state and outputs the optimized weights for
each class, in the training data, to be used during the next round of local
training. In doing so, the agent learns a policy that creates an optimized
partition of the local training dataset during the FL rounds. After FL, the
client utilizes the entire local training dataset to further enhance its
performance on its own data distribution, mitigating the non-IID effects of
aggregation. Through extensive experiments, we demonstrate that training FL
clients through our algorithm results in superior performance on multiple
benchmark datasets and FL frameworks. Our code is available at
https://github.com/amuraddd/optimized_client_training.git.

</details>


### [227] [From Transformers to Large Language Models: A systematic review of AI applications in the energy sector towards Agentic Digital Twins](https://arxiv.org/abs/2506.06359)
*Gabriel Antonesi,Tudor Cioara,Ionut Anghel,Vasilis Michalakopoulos,Elissaios Sarmas,Liana Toderean*

Main category: cs.LG

TL;DR: 本文综述了Transformer架构与大型语言模型（LLMs）在能源管理中的应用，指出其相较于传统机器学习在复杂时序建模、多模态数据融合及决策支持上的优势，并提出整合LLMs的下一代自主化数字孪生（Agentic Digital Twin）概念。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习在能源领域存在泛化能力弱、态势感知不足及异构数据整合困难等问题，而基于Transformer的基座模型和LLMs在复杂关系建模与多模态融合方面展现出潜力，为提升智能电网管理效能提供新路径。

Method: 系统梳理Transformer和LLMs在能源领域的架构创新、领域适配及实践案例，分析其在预测、电网平衡等任务中的表现，并探讨LLMs微调方法、适用场景及挑战，最终提出集成LLMs的代理数字孪生框架。

Result: Transformer模型在能源预测与电网管理任务中表现出更强的上下文建模能力，LLMs通过领域适配可支持规划决策与人员培训；代理数字孪生概念验证了生成式AI在能源系统全生命周期自主决策中的可行性。

Conclusion: 生成式AI正从战略规划渗透至能源系统日常运营，代理数字孪生通过融合LLMs的自主性与社交交互能力，标志着能源管理向主动化、社会化方向演进，为未来智能电网提供核心支撑。

Abstract: Artificial intelligence (AI) has long promised to improve energy management
in smart grids by enhancing situational awareness and supporting more effective
decision-making. While traditional machine learning has demonstrated notable
results in forecasting and optimization, it often struggles with
generalization, situational awareness, and heterogeneous data integration.
Recent advances in foundation models such as Transformer architecture and Large
Language Models (LLMs) have demonstrated improved capabilities in modelling
complex temporal and contextual relationships, as well as in multi-modal data
fusion which is essential for most AI applications in the energy sector. In
this review we synthesize the rapid expanding field of AI applications in the
energy domain focusing on Transformers and LLMs. We examine the architectural
foundations, domain-specific adaptations and practical implementations of
transformer models across various forecasting and grid management tasks. We
then explore the emerging role of LLMs in the field: adaptation and fine tuning
for the energy sector, the type of tasks they are suited for, and the new
challenges they introduce. Along the way, we highlight practical
implementations, innovations, and areas where the research frontier is rapidly
expanding. These recent developments reviewed underscore a broader trend:
Generative AI (GenAI) is beginning to augment decision-making not only in
high-level planning but also in day-to-day operations, from forecasting and
grid balancing to workforce training and asset onboarding. Building on these
developments, we introduce the concept of the Agentic Digital Twin, a
next-generation model that integrates LLMs to bring autonomy, proactivity, and
social interaction into digital twin-based energy management systems.

</details>


### [228] [Beyond the Norm: A Survey of Synthetic Data Generation for Rare Events](https://arxiv.org/abs/2506.06380)
*Jingyi Gu,Xuan Zhang,Guiling Wang*

Main category: cs.LG

TL;DR: 本文首次综述了针对极端事件的合成数据生成方法，系统回顾了生成模型和大语言模型，并提出了专门的评估框架，为极端事件研究提供了结构化基础。


<details>
  <summary>Details</summary>
Motivation: 极端事件（如市场崩溃、自然灾害和疫情）虽然罕见但具有灾难性，且数据稀缺，传统数据驱动方法难以应对。合成数据生成成为解决这一挑战的有力工具，但现有研究缺乏针对极端事件的专门探讨。

Method: 系统回顾生成模型和大语言模型，特别是通过统计理论增强的模型，以及捕捉重尾分布的专业训练和采样机制。提出涵盖统计、依赖、视觉和任务导向指标的评估框架。

Result: 总结了基准数据集，提出了针对极端事件的评估框架，并深入分析了各指标在极端情况下的适用性，为模型评估提供了实用指导。

Conclusion: 本文为极端事件的合成数据生成研究提供了系统化的综述和评估框架，指出了未充分探索的领域，并提出了未来研究的开放挑战。

Abstract: Extreme events, such as market crashes, natural disasters, and pandemics, are
rare but catastrophic, often triggering cascading failures across
interconnected systems. Accurate prediction and early warning can help minimize
losses and improve preparedness. While data-driven methods offer powerful
capabilities for extreme event modeling, they require abundant training data,
yet extreme event data is inherently scarce, creating a fundamental challenge.
Synthetic data generation has emerged as a powerful solution. However, existing
surveys focus on general data with privacy preservation emphasis, rather than
extreme events' unique performance requirements. This survey provides the first
overview of synthetic data generation for extreme events. We systematically
review generative modeling techniques and large language models, particularly
those enhanced by statistical theory as well as specialized training and
sampling mechanisms to capture heavy-tailed distributions. We summarize
benchmark datasets and introduce a tailored evaluation framework covering
statistical, dependence, visual, and task-oriented metrics. A central
contribution is our in-depth analysis of each metric's applicability in
extremeness and domain-specific adaptations, providing actionable guidance for
model evaluation in extreme settings. We categorize key application domains and
identify underexplored areas like behavioral finance, wildfires, earthquakes,
windstorms, and infectious outbreaks. Finally, we outline open challenges,
providing a structured foundation for advancing synthetic rare-event research.

</details>


### [229] [Theoretical Analysis of Positional Encodings in Transformer Models: Impact on Expressiveness and Generalization](https://arxiv.org/abs/2506.06398)
*Yin Li*

Main category: cs.LG

TL;DR: 本文提出了一个理论框架，分析不同位置编码方法对Transformer模型表达能力、泛化及长序列外推的影响，并提出基于正交函数的新编码方法。实验表明正交变换编码在泛化和外推上优于传统正弦编码，填补了Transformer理论空白。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对位置编码方法如何影响Transformer模型性能的系统理论分析，尤其是不同编码方法在表达能力、泛化及外推长序列时的差异。

Method: 通过函数近似定义表达能力、Rademacher复杂度建立泛化边界，提出基于小波和Legendre多项式的正交编码方法，并扩展ALiBi偏置方法至统一理论框架。

Result: 在合成序列任务中，正交变换编码的泛化与外推能力超越传统正弦编码；理论分析揭示了现有编码方法的局限性。

Conclusion: 该研究为Transformer位置编码设计提供了理论指导，正交编码方法在NLP/CV等任务中具有应用潜力，同时建立了编码机制与模型性能的定量关联。

Abstract: Positional encodings are a core part of transformer-based models, enabling
processing of sequential data without recurrence. This paper presents a
theoretical framework to analyze how various positional encoding methods,
including sinusoidal, learned, relative, and bias-based methods like Attention
with Linear Biases (ALiBi), impact a transformer's expressiveness,
generalization ability, and extrapolation to longer sequences. Expressiveness
is defined via function approximation, generalization bounds are established
using Rademacher complexity, and new encoding methods based on orthogonal
functions, such as wavelets and Legendre polynomials, are proposed. The
extrapolation capacity of existing and proposed encodings is analyzed,
extending ALiBi's biasing approach to a unified theoretical context.
Experimental evaluation on synthetic sequence-to-sequence tasks shows that
orthogonal transform-based encodings outperform traditional sinusoidal
encodings in generalization and extrapolation. This work addresses a critical
gap in transformer theory, providing insights for design choices in natural
language processing, computer vision, and other transformer applications.

</details>


### [230] [CoxNTF: A New Approach for Joint Clustering and Prediction in Survival Analysis](https://arxiv.org/abs/2506.06411)
*Paul Fogel,Christophe Geissler,George Luta*

Main category: cs.LG

TL;DR: 本文提出CoxNTF方法，通过非负张量分解结合生存概率构建潜在特征，在保持预测性能的同时提升生存分析的可解释性与特征处理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如NMF）未整合生存信息，导致预测能力受限。需开发既能保留协变量潜在结构，又能关联生存结果的表示学习方法。

Method: CoxNTF利用Coxnet模型生成的生存概率构建加权协变量张量，通过非负张量分解(NTF)提取与生存结果紧密相关的潜在特征表示。

Result: CoxNTF预测性能与原始Coxnet相当，同时提供结构化聚类框架，有效处理特征冗余，实现生存预测与解释性聚类的双重目标。

Conclusion: CoxNTF为生存分析提供了一种兼顾预测性能与可解释性的新工具，其张量分解框架在联合聚类和预测任务中展现出显著优势。

Abstract: The interpretation of the results of survival analysis often benefits from
latent factor representations of baseline covariates. However, existing
methods, such as Nonnegative Matrix Factorization (NMF), do not incorporate
survival information, limiting their predictive power. We present CoxNTF, a
novel approach that uses non-negative tensor factorization (NTF) to derive
meaningful latent representations that are closely associated with survival
outcomes. CoxNTF constructs a weighted covariate tensor in which survival
probabilities derived from the Coxnet model are used to guide the tensorization
process. Our results show that CoxNTF achieves survival prediction performance
comparable to using Coxnet with the original covariates, while providing a
structured and interpretable clustering framework. In addition, the new
approach effectively handles feature redundancy, making it a powerful tool for
joint clustering and prediction in survival analysis.

</details>


### [231] [NeurNCD: Novel Class Discovery via Implicit Neural Representation](https://arxiv.org/abs/2506.06412)
*Junming Wang,Yi Shi*

Main category: cs.LG

TL;DR: 提出NeurNCD框架，通过结合Embedding-NeRF与KL散度替代传统显式3D分割图，实现开放场景下无需密集标注的高效新类发现。


<details>
  <summary>Details</summary>
Motivation: 传统显式表征（如3D分割图）存在离散性、孔洞与噪声问题，限制了新类发现的准确性。需开发更鲁棒的数据高效方法。

Method: 使用Embedding-NeRF模型聚合语义嵌入与视觉熵，结合特征查询/调制/聚类模块，实现预训练分割网络与隐式神经表征的信息交互。

Result: 在NYUv2与Replica数据集上显著超越SOTA方法，开放/封闭场景均表现优异，且无需密集标注或人工监督。

Conclusion: NeurNCD通过隐式表征与特征交互机制，突破传统显式方法局限，为实际场景新类发现提供高效解决方案。

Abstract: Discovering novel classes in open-world settings is crucial for real-world
applications. Traditional explicit representations, such as object descriptors
or 3D segmentation maps, are constrained by their discrete, hole-prone, and
noisy nature, which hinders accurate novel class discovery. To address these
challenges, we introduce NeurNCD, the first versatile and data-efficient
framework for novel class discovery that employs the meticulously designed
Embedding-NeRF model combined with KL divergence as a substitute for
traditional explicit 3D segmentation maps to aggregate semantic embedding and
entropy in visual embedding space. NeurNCD also integrates several key
components, including feature query, feature modulation and clustering,
facilitating efficient feature augmentation and information exchange between
the pre-trained semantic segmentation network and implicit neural
representations. As a result, our framework achieves superior segmentation
performance in both open and closed-world settings without relying on densely
labelled datasets for supervised training or human interaction to generate
sparse label supervision. Extensive experiments demonstrate that our method
significantly outperforms state-of-the-art approaches on the NYUv2 and Replica
datasets.

</details>


### [232] [Unlocking Chemical Insights: Superior Molecular Representations from Intermediate Encoder Layers](https://arxiv.org/abs/2506.06443)
*Luis Pinto*

Main category: cs.LG

TL;DR: 本文通过分析五种分子编码器的中间层嵌入，发现其在ADMET属性预测任务中优于最终层嵌入，显著提升了性能并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前在计算化学中，预训练分子编码器通常仅使用最终层嵌入进行下游任务，这可能丢弃了有价值的信息。本文旨在挑战这一惯例，探索中间层嵌入的潜力。

Method: 本文对五种不同的分子编码器进行了全面的分层分析，涉及22个ADMET属性预测任务，比较了中间层和最终层嵌入的性能，并提出了评估后微调的高效方法。

Result: 使用中间层嵌入平均提升了5.4%的性能，最高提升28.6%；微调中间层后平均提升8.5%，最高提升40.8%，并在多个基准上达到了新的最先进水平。

Conclusion: 本文强调了探索分子编码器全表示深度的重要性，通过中间层嵌入和高效微调方法，显著提升了性能并降低了计算成本。

Abstract: Pretrained molecular encoders have become indispensable in computational
chemistry for tasks such as property prediction and molecular generation.
However, the standard practice of relying solely on final-layer embeddings for
downstream tasks may discard valuable information. In this work, we challenge
this convention by conducting a comprehensive layer-wise analysis of five
diverse molecular encoders across 22 ADMET property prediction tasks. Our
results demonstrate that embeddings from intermediate layers consistently
outperform final-layer representations. Specifically, using fixed embeddings
from the optimal intermediate layers improved downstream performance by an
average of 5.4%, reaching gains up to 28.6%. Furthermore, finetuning up to
these intermediate layers yielded even greater average improvements of 8.5%,
with performance increases as high as 40.8%, achieving new state-of-the-art
results on several benchmarks. Additionally, a strong positive correlation
between fixed embedding performance and finetuning outcomes supports an
efficient evaluate-then-finetune approach, enabling identification of optimal
layers with reduced computational cost. These findings highlight the importance
of exploring the full representational depth of molecular encoders to achieve
substantial performance improvements and computational efficiency. The code is
made publicly available at
https://github.com/luispintoc/Unlocking-Chemical-Insights.

</details>


### [233] [Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety Assurance](https://arxiv.org/abs/2506.06444)
*Ruizhong Qiu,Gaotang Li,Tianxin Wei,Jingrui He,Hanghang Tong*

Main category: cs.LG

TL;DR: 本文提出了一种名为SAFFRON的新型推理扩展范式，旨在增强大型语言模型（LLM）的安全性，通过引入多叉奖励模型（MRM）减少奖励模型评估的计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有安全保证研究主要关注训练阶段的对齐，但最近的研究表明这些方法容易受到多种越狱攻击。尽管推理扩展显著提升了LLM的推理能力，但在安全保证方面尚未得到探索。

Method: 提出SAFFRON范式，包括引入多叉奖励模型（MRM）、部分监督训练目标、保守探索约束以及基于Trie的键值缓存策略。

Result: 实验验证了SAFFRON的有效性，并公开发布了训练好的多叉奖励模型（Saffron-1）和配套的安全奖励数据集（Safety4M）。

Conclusion: SAFFRON为LLM安全提供了高效且鲁棒的推理扩展方法，推动了LLM安全领域的进一步研究。

Abstract: Existing safety assurance research has primarily focused on training-phase
alignment to instill safe behaviors into LLMs. However, recent studies have
exposed these methods' susceptibility to diverse jailbreak attacks.
Concurrently, inference scaling has significantly advanced LLM reasoning
capabilities but remains unexplored in the context of safety assurance.
Addressing this gap, our work pioneers inference scaling for robust and
effective LLM safety against emerging threats. We reveal that conventional
inference scaling techniques, despite their success in reasoning tasks, perform
poorly in safety contexts, even falling short of basic approaches like
Best-of-N Sampling. We attribute this inefficiency to a newly identified
challenge, the exploration--efficiency dilemma, arising from the high
computational overhead associated with frequent process reward model (PRM)
evaluations. To overcome this dilemma, we propose SAFFRON, a novel inference
scaling paradigm tailored explicitly for safety assurance. Central to our
approach is the introduction of a multifurcation reward model (MRM) that
significantly reduces the required number of reward model evaluations. To
operationalize this paradigm, we further propose: (i) a partial supervision
training objective for MRM, (ii) a conservative exploration constraint to
prevent out-of-distribution explorations, and (iii) a Trie-based key--value
caching strategy that facilitates cache sharing across sequences during tree
search. Extensive experiments validate the effectiveness of our method.
Additionally, we publicly release our trained multifurcation reward model
(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)
to accelerate future research in LLM safety. Our code, model, and data are
publicly available at https://github.com/q-rz/saffron , and our project
homepage is at https://q-rz.github.io/p/saffron .

</details>


### [234] [LETS Forecast: Learning Embedology for Time Series Forecasting](https://arxiv.org/abs/2506.06454)
*Abrar Majeedi,Viswanatha Reddy Gajjala,Satya Sai Srinath Namburi GNVV,Nada Magdi Elkordi,Yin Li*

Main category: cs.LG

TL;DR: 本文提出了DeepEDM框架，结合非线性动力学模型与深度神经网络，通过学习时间延迟嵌入的潜在空间，并利用核回归逼近底层动力学，提升了时间序列预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现实中的时间序列通常由复杂的非线性动力学控制，现有深度学习方法大多未显式建模这些动力学。本文旨在填补这一空白，提出一种新的框架来精确预测未来时间序列。

Method: DeepEDM框架结合了经验动力学建模（EDM）和深度神经网络，通过学习时间延迟嵌入的潜在空间，并利用核回归逼近底层动力学，同时采用高效的softmax注意力机制。

Result: 实验表明，DeepEDM对输入噪声具有鲁棒性，并在非线性动力学系统的合成数据和真实时间序列数据上均优于现有最先进方法。

Conclusion: DeepEDM通过显式建模非线性动力学，显著提升了时间序列预测的准确性，为复杂时间序列分析提供了一种有效的工具。

Abstract: Real-world time series are often governed by complex nonlinear dynamics.
Understanding these underlying dynamics is crucial for precise future
prediction. While deep learning has achieved major success in time series
forecasting, many existing approaches do not explicitly model the dynamics. To
bridge this gap, we introduce DeepEDM, a framework that integrates nonlinear
dynamical systems modeling with deep neural networks. Inspired by empirical
dynamic modeling (EDM) and rooted in Takens' theorem, DeepEDM presents a novel
deep model that learns a latent space from time-delayed embeddings, and employs
kernel regression to approximate the underlying dynamics, while leveraging
efficient implementation of softmax attention and allowing for accurate
prediction of future time steps. To evaluate our method, we conduct
comprehensive experiments on synthetic data of nonlinear dynamical systems as
well as real-world time series across domains. Our results show that DeepEDM is
robust to input noise, and outperforms state-of-the-art methods in forecasting
accuracy. Our code is available at: https://abrarmajeedi.github.io/deep_edm.

</details>


### [235] [WISCA: A Consensus-Based Approach to Harmonizing Interpretability in Tabular Datasets](https://arxiv.org/abs/2506.06455)
*Antonio Jesús Banegas-Luna,Horacio Pérez-Sánchez,Carlos Martínez-Cortés*

Main category: cs.LG

TL;DR: 本文探讨了机器学习模型在可解释性方面的重要性，提出了一种新的共识方法WISCA，以提升解释的可靠性。


<details>
  <summary>Details</summary>
Motivation: 在科学和高风险领域中，机器学习模型的可解释性至关重要，然而不同的解释算法常常产生冲突结果，因此需要共识方法来统一结果。

Method: 研究训练了六个机器学习模型，并使用多种模型无关的解释技术生成共识解释，提出了新的方法WISCA，整合了类别概率和归一化属性。

Result: WISCA始终与最可靠的个体方法一致，证明了其作为共识策略在提升解释可靠性方面的价值。

Conclusion: 研究表明，采用稳健的共识策略如WISCA，能够有效提高机器学习模型解释的可靠性。

Abstract: While predictive accuracy is often prioritized in machine learning (ML)
models, interpretability remains essential in scientific and high-stakes
domains. However, diverse interpretability algorithms frequently yield
conflicting explanations, highlighting the need for consensus to harmonize
results. In this study, six ML models were trained on six synthetic datasets
with known ground truths, utilizing various model-agnostic interpretability
techniques. Consensus explanations were generated using established methods and
a novel approach: WISCA (Weighted Scaled Consensus Attributions), which
integrates class probability and normalized attributions. WISCA consistently
aligned with the most reliable individual method, underscoring the value of
robust consensus strategies in improving explanation reliability.

</details>


### [236] [Towards Infant Sleep-Optimized Driving: Synergizing Wearable and Vehicle Sensing in Intelligent Cruise Control](https://arxiv.org/abs/2506.06459)
*Ruitao Chen,Mozhang Guo,Jinge Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于强化学习的智能巡航控制框架，通过结合可穿戴传感器和车辆数据，优化驾驶行为以提升婴儿睡眠质量，同时保持出行效率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶虽提升了车辆安全性和驾驶舒适性，但其对乘客特别是婴儿睡眠的影响尚未充分研究。急加速、急刹车等行为可能干扰婴儿睡眠，影响乘客舒适度和父母的便利性。

Method: 本文提出了一种智能巡航控制框架，结合强化学习（RL）、长短期记忆（LSTM）和Transformer神经网络，利用可穿戴传感器、车辆控制器数据和地图数据，动态计算最佳驾驶激进程度，并将其转化为具体的自动驾驶控制策略。

Result: 仿真结果表明，与基准方法相比，本文提出的解决方案显著提高了婴儿睡眠质量，同时保持了理想的出行效率。

Conclusion: 本文通过强化学习和智能巡航控制框架，有效平衡了乘客舒适度和出行效率，特别是在提升婴儿睡眠质量方面取得了显著成果。

Abstract: Automated driving (AD) has substantially improved vehicle safety and driving
comfort, but their impact on passenger well-being, particularly infant sleep,
is not sufficiently studied. Sudden acceleration, abrupt braking, and sharp
maneuvers can disrupt infant sleep, compromising both passenger comfort and
parental convenience. To solve this problem, this paper explores the
integration of reinforcement learning (RL) within AD to personalize driving
behavior and optimally balance occupant comfort and travel efficiency. In
particular, we propose an intelligent cruise control framework that adapts to
varying driving conditions to enhance infant sleep quality by effectively
synergizing wearable sensing and vehicle data. Long short-term memory (LSTM)
and transformer-based neural networks are integrated with RL to model the
relationship between driving behavior and infant sleep quality under diverse
traffic and road conditions. Based on the sleep quality indicators from the
wearable sensors, driving action data from vehicle controllers, and map data
from map applications, the model dynamically computes the optimal driving
aggressiveness level, which is subsequently translated into specific AD control
strategies, e.g., the magnitude and frequency of acceleration, lane change, and
overtaking. Simulation results demonstrate that the proposed solution
significantly improves infant sleep quality compared to baseline methods, while
preserving desirable travel efficiency.

</details>


### [237] [TimeRecipe: A Time-Series Forecasting Recipe via Benchmarking Module Level Effectiveness](https://arxiv.org/abs/2506.06482)
*Zhiyuan Zhao,Juntong Ni,Shangqing Xu,Haoxin Liu,Wei Jin,B. Aditya Prakash*

Main category: cs.LG

TL;DR: 提出TimeRecipe框架，通过模块级系统化评估时间序列预测方法，揭示设计选择与效果的关系，并发布实用工具包。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测模型评估多停留在高层级，缺乏对具体设计选择（如分解、归一化）有效性的深入分析，导致模型优化方向不明确。

Method: 开发TimeRecipe统一基准框架，在多样化数据集、预测范围及任务设置下，通过超万次实验系统评估各模块组件的有效性。

Result: 实验表明，通过设计空间穷举可构建优于现有SOTA的模型，并发现特定设计选择与预测任务场景间的关联性直觉。

Conclusion: TimeRecipe不仅提供模型性能提升路径，其开源工具包还能基于实证推荐适配架构，推动时间序列预测领域模块化设计研究。

Abstract: Time-series forecasting is an essential task with wide real-world
applications across domains. While recent advances in deep learning have
enabled time-series forecasting models with accurate predictions, there remains
considerable debate over which architectures and design components, such as
series decomposition or normalization, are most effective under varying
conditions. Existing benchmarks primarily evaluate models at a high level,
offering limited insight into why certain designs work better. To mitigate this
gap, we propose TimeRecipe, a unified benchmarking framework that
systematically evaluates time-series forecasting methods at the module level.
TimeRecipe conducts over 10,000 experiments to assess the effectiveness of
individual components across a diverse range of datasets, forecasting horizons,
and task settings. Our results reveal that exhaustive exploration of the design
space can yield models that outperform existing state-of-the-art methods and
uncover meaningful intuitions linking specific design choices to forecasting
scenarios. Furthermore, we release a practical toolkit within TimeRecipe that
recommends suitable model architectures based on these empirical insights. The
benchmark is available at: https://github.com/AdityaLab/TimeRecipe.

</details>


### [238] [A Certified Unlearning Approach without Access to Source Data](https://arxiv.org/abs/2506.06486)
*Umit Yigit Basaran,Sk Miraj Ahmed,Amit Roy-Chowdhury,Basak Guler*

Main category: cs.LG

TL;DR: 本文提出了一种无需访问原始训练数据的认证遗忘框架，通过使用替代数据集近似源数据的统计特性，确保在隐私敏感场景下的有效性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着数据隐私法规的普及，从训练模型中删除私人或受版权保护信息的能力变得至关重要。传统遗忘方法通常假设可以访问完整的训练数据集，但在源数据不可用的情况下，这一假设不现实。

Method: 本文提出了一种认证遗忘框架，利用替代数据集近似源数据的统计特性，并通过统计距离进行噪声缩放控制。虽然理论保证假设已知确切的统计距离，但实际实现中通常近似该距离，从而提供较弱但仍具意义的隐私保证。

Result: 通过广泛的实验验证，本文的方法在合成和真实数据集上均表现出有效性和可靠性，确保了模型在遗忘后的行为具有强保证，同时保持其整体效用。

Conclusion: 本文提出的认证遗忘框架在无需访问原始训练数据的情况下，能够有效删除数据，并在隐私敏感场景下提供强保证，具有实际应用价值。

Abstract: With the growing adoption of data privacy regulations, the ability to erase
private or copyrighted information from trained models has become a crucial
requirement. Traditional unlearning methods often assume access to the complete
training dataset, which is unrealistic in scenarios where the source data is no
longer available. To address this challenge, we propose a certified unlearning
framework that enables effective data removal \final{without access to the
original training data samples}. Our approach utilizes a surrogate dataset that
approximates the statistical properties of the source data, allowing for
controlled noise scaling based on the statistical distance between the two.
\updated{While our theoretical guarantees assume knowledge of the exact
statistical distance, practical implementations typically approximate this
distance, resulting in potentially weaker but still meaningful privacy
guarantees.} This ensures strong guarantees on the model's behavior
post-unlearning while maintaining its overall utility. We establish theoretical
bounds, introduce practical noise calibration techniques, and validate our
method through extensive experiments on both synthetic and real-world datasets.
The results demonstrate the effectiveness and reliability of our approach in
privacy-sensitive settings.

</details>


### [239] [Membership Inference Attacks for Unseen Classes](https://arxiv.org/abs/2506.06488)
*Pratiksha Thaker,Neil Kale,Zhiwei Steven Wu,Virginia Smith*

Main category: cs.LG

TL;DR: 本文研究了在无法访问完整子类数据的情况下，成员推断攻击的性能，并展示了分位数回归方法在此类情况下的优势。


<details>
  <summary>Details</summary>
Motivation: 现有的影子模型攻击通常假设攻击者可以访问与目标模型训练数据分布匹配的背景数据，但在实际中，攻击者可能无法访问完整子类数据，本文旨在研究这种极端但现实的分布偏移情况下的成员推断攻击。

Method: 本文首先展示了影子模型攻击在此类情况下的性能急剧下降，然后提出了分位数回归方法，并展示了其在类丢失情况下的优势。

Result: 实验表明，分位数回归攻击在类丢失情况下始终优于影子模型攻击，例如在CIFAR-100数据集上，分位数回归攻击在未见类上的TPR是影子模型的11倍，在ImageNet数据集上即使移除90%的训练类，仍能实现非平凡的TPR。

Conclusion: 分位数回归方法在无法访问完整子类数据的情况下表现出色，本文还提供了一个理论模型来说明该方法的潜力和局限性。

Abstract: Shadow model attacks are the state-of-the-art approach for membership
inference attacks on machine learning models. However, these attacks typically
assume an adversary has access to a background (nonmember) data distribution
that matches the distribution the target model was trained on. We initiate a
study of membership inference attacks where the adversary or auditor cannot
access an entire subclass from the distribution -- a more extreme but realistic
version of distribution shift than has been studied previously. In this
setting, we first show that the performance of shadow model attacks degrades
catastrophically, and then demonstrate the promise of another approach,
quantile regression, that does not have the same limitations. We show that
quantile regression attacks consistently outperform shadow model attacks in the
class dropout setting -- for example, quantile regression attacks achieve up to
11$\times$ the TPR of shadow models on the unseen class on CIFAR-100, and
achieve nontrivial TPR on ImageNet even with 90% of training classes removed.
We also provide a theoretical model that illustrates the potential and
limitations of this approach.

</details>


### [240] [Alternating Gradient Flows: A Theory of Feature Learning in Two-layer Neural Networks](https://arxiv.org/abs/2506.06489)
*Daniel Kunin,Giovanni Luca Marchetti,Feng Chen,Dhruva Karkada,James B. Simon,Michael R. DeWeese,Surya Ganguli,Nina Miolane*

Main category: cs.LG

TL;DR: 本文提出交替梯度流（AGF）框架，用于分析小初始化下两层神经网络的特征学习动态，揭示神经元激活顺序与损失下降机制，统一并扩展了现有理论。


<details>
  <summary>Details</summary>
Motivation: 神经网络如何学习特征仍不明确。现有研究观察到梯度流在小初始化时呈现阶梯状损失曲线，但缺乏系统性理论框架解释特征学习的动态过程。

Method: AGF将梯度流分解为交替两步：1) 在休眠神经元上最大化效用函数以选择特征方向；2) 在活跃神经元上最小化损失函数以快速收敛。初始时所有神经元休眠，逐步激活并学习特征。

Result: AGF在多种架构（全连接线性网络、注意力线性变换器、对角线性网络、二次网络）中验证有效性，证明其收敛性，并首次完整描述了模加任务中网络按傅里叶系数大小顺序学习特征的动态过程。

Conclusion: AGF为理解神经网络特征学习提供了统一框架，揭示了特征激活顺序与机制，在理论分析和实验验证中均表现出与梯度流的一致性，是解释特征学习的重要进展。

Abstract: What features neural networks learn, and how, remains an open question. In
this paper, we introduce Alternating Gradient Flows (AGF), an algorithmic
framework that describes the dynamics of feature learning in two-layer networks
trained from small initialization. Prior works have shown that gradient flow in
this regime exhibits a staircase-like loss curve, alternating between plateaus
where neurons slowly align to useful directions and sharp drops where neurons
rapidly grow in norm. AGF approximates this behavior as an alternating two-step
process: maximizing a utility function over dormant neurons and minimizing a
cost function over active ones. AGF begins with all neurons dormant. At each
round, a dormant neuron activates, triggering the acquisition of a feature and
a drop in the loss. AGF quantifies the order, timing, and magnitude of these
drops, matching experiments across architectures. We show that AGF unifies and
extends existing saddle-to-saddle analyses in fully connected linear networks
and attention-only linear transformers, where the learned features are singular
modes and principal components, respectively. In diagonal linear networks, we
prove AGF converges to gradient flow in the limit of vanishing initialization.
Applying AGF to quadratic networks trained to perform modular addition, we give
the first complete characterization of the training dynamics, revealing that
networks learn Fourier features in decreasing order of coefficient magnitude.
Altogether, AGF offers a promising step towards understanding feature learning
in neural networks.

</details>


### [241] [Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms](https://arxiv.org/abs/2506.06499)
*Alex Havrilla,Edward Hughes,Mikayel Samvelyan,Jacob Abernethy*

Main category: cs.LG

TL;DR: SPARQ方法通过质量多样性算法生成高质量且多样化的数学问题与解答对，仅使用单一模型并测量问题解决率来提升模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM驱动的合成数据生成方法在扩展到更复杂和多样化的问题领域时存在局限性，SPARQ旨在解决这一问题。

Method: SPARQ利用质量多样性算法，从7.5K样本的种子数据集生成超过2000万个新的问题-解答对，并通过问题解决率过滤数据。

Result: 通过过滤生成的数据并微调模型，模型性能相对提升高达24%。高质量数据有助于提升分布内性能，而多样化数据则增强分布外泛化能力。

Conclusion: SPARQ方法有效提升了模型推理能力，并验证了合成问题生成中的模型和数据扩展规律对下游模型泛化的积极影响。

Abstract: Large language model (LLM) driven synthetic data generation has emerged as a
powerful method for improving model reasoning capabilities. However, most
methods either distill large state-of-the-art models into small students or use
natural ground-truth problem statements to guarantee problem statement quality.
This limits the scalability of these approaches to more complex and diverse
problem domains. To address this, we present SPARQ: Synthetic Problem
Generation for Reasoning via Quality-Diversity Algorithms, a novel approach for
generating high-quality and diverse synthetic math problem and solution pairs
using only a single model by measuring a problem's solve-rate: a proxy for
problem difficulty. Starting from a seed dataset of 7.5K samples, we generate
over 20 million new problem-solution pairs. We show that filtering the
generated data by difficulty and then fine-tuning the same model on the
resulting data improves relative model performance by up to 24\%. Additionally,
we conduct ablations studying the impact of synthetic data quantity, quality
and diversity on model generalization. We find that higher quality, as measured
by problem difficulty, facilitates better in-distribution performance. Further,
while generating diverse synthetic data does not as strongly benefit
in-distribution performance, filtering for more diverse data facilitates more
robust OOD generalization. We also confirm the existence of model and data
scaling laws for synthetically generated problems, which positively benefit
downstream model generalization.

</details>


### [242] [Optimal Rates in Continual Linear Regression via Increasing Regularization](https://arxiv.org/abs/2506.06501)
*Ran Levinstein,Amit Attia,Matan Schliserman,Uri Sherman,Tomer Koren,Daniel Soudry,Itay Evron*

Main category: cs.LG

TL;DR: 本文研究了在随机任务顺序下的可实现在线线性回归，通过正则化方法缩小了理论上的损失上下界差距，并提出了最优的损失率。


<details>
  <summary>Details</summary>
Motivation: 现有研究在使用无正则化方案时，损失的上界与下界存在显著差距。本文旨在通过正则化方法缩小或消除这一差距。

Method: 使用两种常用的正则化方案：显式各向同性ℓ2正则化和通过有限步长预算的隐式正则化，将其转化为对精心定义的替代损失函数进行随机梯度下降（SGD）。

Result: 本文证明了固定正则化强度可以达到近乎最优的损失率O(log k / k)，并通过广义SGD推导出递增正则化强度方案，实现了最优损失率O(1/k)。

Conclusion: 增加正则化系数或减少每任务步数的计划在理论上是有益的，尤其是在最坏情况下。

Abstract: We study realizable continual linear regression under random task orderings,
a common setting for developing continual learning theory. In this setup, the
worst-case expected loss after $k$ learning iterations admits a lower bound of
$\Omega(1/k)$. However, prior work using an unregularized scheme has only
established an upper bound of $O(1/k^{1/4})$, leaving a significant gap. Our
paper proves that this gap can be narrowed, or even closed, using two
frequently used regularization schemes: (1) explicit isotropic $\ell_2$
regularization, and (2) implicit regularization via finite step budgets. We
show that these approaches, which are used in practice to mitigate forgetting,
reduce to stochastic gradient descent (SGD) on carefully defined surrogate
losses. Through this lens, we identify a fixed regularization strength that
yields a near-optimal rate of $O(\log k / k)$. Moreover, formalizing and
analyzing a generalized variant of SGD for time-varying functions, we derive an
increasing regularization strength schedule that provably achieves an optimal
rate of $O(1/k)$. This suggests that schedules that increase the regularization
coefficient or decrease the number of steps per task are beneficial, at least
in the worst case.

</details>


### [243] [InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models](https://arxiv.org/abs/2506.06505)
*Keisuke Sugiura,Hiroki Matsutani*

Main category: cs.LG

TL;DR: InstantFT是一种基于FPGA的CNN快速微调方法，在IoT设备上实现了17.4倍于现有LoRA方法的加速，仅需0.36秒且能效提升16.3倍，支持动态数据分布适应。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络(DNN)训练在资源受限的IoT设备上存在计算和内存需求过高的问题，难以实现运行时模型动态适应。

Method: 通过优化参数高效微调(PEFT)中的前向/反向计算流程，开发基于FPGA的硬件加速方案InstantFT。

Result: 在概念漂移数据集上，微调速度达现有LoRA方法的17.4倍(0.36秒)，准确率相当，能效提升16.3倍。

Conclusion: InstantFT首次在IoT设备上实现CNN的实时动态微调，有效应对非稳态数据分布挑战。

Abstract: Training deep neural networks (DNNs) requires significantly more computation
and memory than inference, making runtime adaptation of DNNs challenging on
resource-limited IoT platforms. We propose InstantFT, an FPGA-based method for
ultra-fast CNN fine-tuning on IoT devices, by optimizing the forward and
backward computations in parameter-efficient fine-tuning (PEFT). Experiments on
datasets with concept drift demonstrate that InstantFT fine-tunes a pre-trained
CNN 17.4x faster than existing Low-Rank Adaptation (LoRA)-based approaches,
while achieving comparable accuracy. Our FPGA-based InstantFT reduces the
fine-tuning time to just 0.36s and improves energy-efficiency by 16.3x,
enabling on-the-fly adaptation of CNNs to non-stationary data distributions.

</details>


### [244] [Sharp Gap-Dependent Variance-Aware Regret Bounds for Tabular MDPs](https://arxiv.org/abs/2506.06521)
*Shulun Chen,Runlong Zhou,Zihan Zhang,Maryam Fazel,Simon S. Du*

Main category: cs.LG

TL;DR: 本文研究了基于差距的遗憾界，提出了MVP算法，并证明了其在方差感知下的遗憾界，同时建立了下界，表明对最大条件总方差的依赖是必要的。


<details>
  <summary>Details</summary>
Motivation: 研究在马尔可夫决策过程（MDP）中，如何通过考虑子最优性差距和方差来改进遗憾界，特别是在方差感知的情况下。

Method: 使用单调值传播（MVP）算法，并通过加权子最优性差距的新颖分析，推导出方差感知的遗憾界。

Result: 证明了MVP算法在方差感知下的遗憾界，并建立了下界，表明对最大条件总方差的依赖是必要的。

Conclusion: 本文的结果表明，考虑方差和子最优性差距可以显著改进遗憾界，且这一方法可能适用于其他算法。

Abstract: We consider the gap-dependent regret bounds for episodic MDPs. We show that
the Monotonic Value Propagation (MVP) algorithm achieves a variance-aware
gap-dependent regret bound of $$\tilde{O}\left(\left(\sum_{\Delta_h(s,a)>0}
\frac{H^2 \log K \land \mathtt{Var}_{\max}^{\text{c}}}{\Delta_h(s,a)}
+\sum_{\Delta_h(s,a)=0}\frac{ H^2 \land
\mathtt{Var}_{\max}^{\text{c}}}{\Delta_{\mathrm{min}}} + SAH^4 (S \lor H)
\right) \log K\right),$$ where $H$ is the planning horizon, $S$ is the number
of states, $A$ is the number of actions, and $K$ is the number of episodes.
Here, $\Delta_h(s,a) =V_h^* (a) - Q_h^* (s, a)$ represents the suboptimality
gap and $\Delta_{\mathrm{min}} := \min_{\Delta_h (s,a) > 0} \Delta_h(s,a)$. The
term $\mathtt{Var}_{\max}^{\text{c}}$ denotes the maximum conditional total
variance, calculated as the maximum over all $(\pi, h, s)$ tuples of the
expected total variance under policy $\pi$ conditioned on trajectories visiting
state $s$ at step $h$. $\mathtt{Var}_{\max}^{\text{c}}$ characterizes the
maximum randomness encountered when learning any $(h, s)$ pair. Our result
stems from a novel analysis of the weighted sum of the suboptimality gap and
can be potentially adapted for other algorithms. To complement the study, we
establish a lower bound of $$\Omega \left( \sum_{\Delta_h(s,a)>0} \frac{H^2
\land \mathtt{Var}_{\max}^{\text{c}}}{\Delta_h(s,a)}\cdot \log K\right),$$
demonstrating the necessity of dependence on $\mathtt{Var}_{\max}^{\text{c}}$
even when the maximum unconditional total variance (without conditioning on
$(h, s)$) approaches zero.

</details>


### [245] [Hierarchical and Collaborative LLM-Based Control for Multi-UAV Motion and Communication in Integrated Terrestrial and Non-Terrestrial Networks](https://arxiv.org/abs/2506.06532)
*Zijiang Yan,Hao Zhou,Jianhua Pei,Hina Tabassum*

Main category: cs.LG

TL;DR: 本文提出了一种基于大型语言模型（LLMs）的分层协作方法，用于多无人机系统在动态和受限环境中的联合运动与通信控制。


<details>
  <summary>Details</summary>
Motivation: 多无人机系统的控制与优化在动态和受限环境中仍然是一个重大挑战，特别是在包含高空平台站（HAPS）的集成地面与非地面网络中。

Method: 提出了一种基于LLMs的分层协作方法，HAPS上的LLM负责无人机接入控制，每架无人机上的LLM处理运动规划与控制。

Result: 实验结果表明，所提出的方法在系统奖励、操作成本和无人机碰撞率方面均优于基线方法。

Conclusion: 基于LLMs的知识驱动范式在下一代3D空中高速公路系统的开发中具有巨大潜力。

Abstract: Unmanned aerial vehicles (UAVs) have been widely adopted in various
real-world applications. However, the control and optimization of multi-UAV
systems remain a significant challenge, particularly in dynamic and constrained
environments. This work explores the joint motion and communication control of
multiple UAVs operating within integrated terrestrial and non-terrestrial
networks that include high-altitude platform stations (HAPS). Specifically, we
consider an aerial highway scenario in which UAVs must accelerate, decelerate,
and change lanes to avoid collisions and maintain overall traffic flow.
Different from existing studies, we propose a novel hierarchical and
collaborative method based on large language models (LLMs). In our approach, an
LLM deployed on the HAPS performs UAV access control, while another LLM onboard
each UAV handles motion planning and control. This LLM-based framework
leverages the rich knowledge embedded in pre-trained models to enable both
high-level strategic planning and low-level tactical decisions. This
knowledge-driven paradigm holds great potential for the development of
next-generation 3D aerial highway systems. Experimental results demonstrate
that our proposed collaborative LLM-based method achieves higher system
rewards, lower operational costs, and significantly reduced UAV collision rates
compared to baseline approaches.

</details>


### [246] [GeoClip: Geometry-Aware Clipping for Differentially Private SGD](https://arxiv.org/abs/2506.06549)
*Atefeh Gilani,Naima Tasnim,Lalitha Sankar,Oliver Kosut*

Main category: cs.LG

TL;DR: GeoClip提出了一种基于几何感知的梯度裁剪框架，通过变换基对齐梯度分布的几何形状，减少噪声添加并控制梯度裁剪概率，提升了DP-SGD的性能。


<details>
  <summary>Details</summary>
Motivation: DP-SGD中，样本梯度裁剪阈值的设置对隐私与效用之间的权衡至关重要。现有自适应方法在标准坐标系下操作，未考虑梯度坐标间的相关性，导致性能受限。

Method: GeoClip提出了一种几何感知框架，在训练过程中通过变换基对齐梯度分布的几何形状，仅使用先前发布的噪声梯度自适应估计该变换，不增加额外隐私成本。

Result: 实验表明，在相同隐私预算下，GeoClip在表格和图像数据集上均优于现有的自适应裁剪方法。

Conclusion: GeoClip通过几何感知的梯度裁剪和扰动，显著提升了DP-SGD的效用与隐私权衡，为隐私保护机器学习提供了新的解决方案。

Abstract: Differentially private stochastic gradient descent (DP-SGD) is the most
widely used method for training machine learning models with provable privacy
guarantees. A key challenge in DP-SGD is setting the per-sample gradient
clipping threshold, which significantly affects the trade-off between privacy
and utility. While recent adaptive methods improve performance by adjusting
this threshold during training, they operate in the standard coordinate system
and fail to account for correlations across the coordinates of the gradient. We
propose GeoClip, a geometry-aware framework that clips and perturbs gradients
in a transformed basis aligned with the geometry of the gradient distribution.
GeoClip adaptively estimates this transformation using only previously released
noisy gradients, incurring no additional privacy cost. We provide convergence
guarantees for GeoClip and derive a closed-form solution for the optimal
transformation that minimizes the amount of noise added while keeping the
probability of gradient clipping under control. Experiments on both tabular and
image datasets demonstrate that GeoClip consistently outperforms existing
adaptive clipping methods under the same privacy budget.

</details>


### [247] [SDN-Based False Data Detection With Its Mitigation and Machine Learning Robustness for In-Vehicle Networks](https://arxiv.org/abs/2506.06556)
*Long Dang,Thushari Hapuarachchi,Kaiqi Xiong,Yi Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于SDN的车辆网络虚假数据检测与缓解系统（FDDMS），利用LSTM模型实时检测并缓解虚假数据注入攻击，并通过动态更新SDN流规则来重定向攻击流量。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶和联网车辆的发展，车辆内部网络复杂性增加，ECU之间的通信安全至关重要。本文旨在解决车辆网络中虚假数据注入攻击的问题，确保车辆的安全性和可靠性。

Method: 本文首先解码原始CAN数据，构建攻击模型，然后利用基于LSTM的检测模型识别虚假数据注入攻击。进一步提出了一种改进的DeepFool攻击变体来评估模型鲁棒性，并通过基于阈值的重训练技术增强对抗攻击的防御能力。最后，通过SDN动态更新流规则实现攻击流量的重定向。

Result: 实验结果表明，所提出的FDDMS能够有效对抗多种对抗攻击，并实时检测和缓解虚假数据注入攻击。

Conclusion: 本文提出的FDDMS系统在车辆网络中表现出强大的鲁棒性，能够有效应对虚假数据注入攻击，保障车辆通信的安全性和可靠性。

Abstract: As the development of autonomous and connected vehicles advances, the
complexity of modern vehicles increases, with numerous Electronic Control Units
(ECUs) integrated into the system. In an in-vehicle network, these ECUs
communicate with one another using an standard protocol called Controller Area
Network (CAN). Securing communication among ECUs plays a vital role in
maintaining the safety and security of the vehicle. This paper proposes a
robust SDN-based False Data Detection and Mitigation System (FDDMS) for
in-vehicle networks. Leveraging the unique capabilities of Software-Defined
Networking (SDN), FDDMS is designed to monitor and detect false data injection
attacks in real-time. Specifically, we focus on brake-related ECUs within an
SDN-enabled in-vehicle network. First, we decode raw CAN data to create an
attack model that illustrates how false data can be injected into the system.
Then, FDDMS, incorporating a Long Short Term Memory (LSTM)-based detection
model, is used to identify false data injection attacks. We further propose an
effective variant of DeepFool attack to evaluate the model's robustness. To
countermeasure the impacts of four adversarial attacks including Fast gradient
descent method, Basic iterative method, DeepFool, and the DeepFool variant, we
further enhance a re-training technique method with a threshold based selection
strategy. Finally, a mitigation scheme is implemented to redirect attack
traffic by dynamically updating flow rules through SDN. Our experimental
results show that the proposed FDDMS is robust against adversarial attacks and
effectively detects and mitigates false data injection attacks in real-time.

</details>


### [248] [Rapid training of Hamiltonian graph networks without gradient descent](https://arxiv.org/abs/2506.06558)
*Atamert Rahma,Chinmay Datar,Ana Cukarska,Felix Dietrich*

Main category: cs.LG

TL;DR: 本文提出了一种基于哈密顿图网络（HGN）的物理系统建模方法，通过随机特征参数构建替代传统梯度优化算法，实现高达600倍的训练加速，同时保持物理不变性并支持零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 传统基于梯度优化的图神经网络（如Adam、RMSProp）在复杂物理系统建模中存在训练速度慢的问题，且需保持物理对称性和约束条件。

Method: 结合哈密顿力学与图神经网络，采用随机特征参数构造方法替代迭代优化算法，确保模型具有置换、旋转和平移不变性。

Result: 在3D N体弹簧系统等场景中，模型在8节点训练数据下可零样本泛化至4096节点系统，训练速度较15种优化器快600倍且精度相当。

Conclusion: 该方法挑战了梯度下降优化在物理系统建模中的主导地位，为大规模复杂系统的高效学习提供了新范式。

Abstract: Learning dynamical systems that respect physical symmetries and constraints
remains a fundamental challenge in data-driven modeling. Integrating physical
laws with graph neural networks facilitates principled modeling of complex
N-body dynamics and yields accurate and permutation-invariant models. However,
training graph neural networks with iterative, gradient-based optimization
algorithms (e.g., Adam, RMSProp, LBFGS) often leads to slow training,
especially for large, complex systems. In comparison to 15 different
optimizers, we demonstrate that Hamiltonian Graph Networks (HGN) can be trained
up to 600x faster--but with comparable accuracy--by replacing iterative
optimization with random feature-based parameter construction. We show robust
performance in diverse simulations, including N-body mass-spring systems in up
to 3 dimensions with different geometries, while retaining essential physical
invariances with respect to permutation, rotation, and translation. We reveal
that even when trained on minimal 8-node systems, the model can generalize in a
zero-shot manner to systems as large as 4096 nodes without retraining. Our work
challenges the dominance of iterative gradient-descent-based optimization
algorithms for training neural network models for physical systems.

</details>


### [249] [Graph Persistence goes Spectral](https://arxiv.org/abs/2506.06571)
*Mattie Ji,Amauri H. Souza,Vikas Garg*

Main category: cs.LG

TL;DR: 本文提出了一种新的拓扑描述子SpectRe，通过将谱信息整合到持续同调（PH）图中，显著增强了图表示学习的表达能力，并验证了其局部稳定性及在实验中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于持续同调（PH）的图表示学习方法依赖顶点和边特征，但无法捕捉基本图结构信息。因此，需要一种更强大的描述子以结合拓扑与谱信息。

Method: 提出SpectRe方法，将图的谱信息（如特征值、特征向量）融入PH图中，并引入全局和局部稳定性理论分析其性质。

Result: SpectRe在表达能力上严格优于现有描述子，且被证明具有局部稳定性。合成和真实数据集实验验证了其对图模型性能的提升。

Conclusion: SpectRe通过融合拓扑与谱信息，为图学习任务提供了更优的表示方法，并具备实际应用潜力。

Abstract: Including intricate topological information (e.g., cycles) provably enhances
the expressivity of message-passing graph neural networks (GNNs) beyond the
Weisfeiler-Leman (WL) hierarchy. Consequently, Persistent Homology (PH) methods
are increasingly employed for graph representation learning. In this context,
recent works have proposed decorating classical PH diagrams with vertex and
edge features for improved expressivity. However, due to their dependence on
features, these methods still fail to capture basic graph structural
information. In this paper, we propose SpectRe -- a new topological descriptor
for graphs that integrates spectral information into PH diagrams. Notably,
SpectRe is strictly more expressive than existing descriptors on graphs. We
also introduce notions of global and local stability to analyze existing
descriptors and establish that SpectRe is locally stable. Finally, experiments
on synthetic and real-world datasets demonstrate the effectiveness of SpectRe
and its potential to enhance the capabilities of graph models in relevant
learning tasks.

</details>


### [250] [Towards Efficient Multi-LLM Inference: Characterization and Analysis of LLM Routing and Hierarchical Techniques](https://arxiv.org/abs/2506.06579)
*Adarsh Prasad Behera,Jaya Prakash Champati,Roberto Morabito,Sasu Tarkoma,James Gross*

Main category: cs.LG

TL;DR: 本文探讨了通过多LLM智能模型选择策略（如路由和级联推理）来优化语言模型推理效率的方法，旨在减少计算资源消耗，使其更适合在资源受限的环境中部署。


<details>
  <summary>Details</summary>
Motivation: 尽管语言模型在自然语言处理任务中表现出色，但其推理过程计算成本高、能耗大，尤其在硬件、电力或带宽受限的环境中难以部署。因此，需要开发更高效的推理策略。

Method: 本文提出了两种互补的策略：路由（根据查询选择最合适的模型）和级联推理（通过一系列模型逐步处理查询，直到获得可靠响应）。

Result: 这些策略通过为简单任务使用轻量级模型，仅在必要时调用更大模型，显著减少了计算资源消耗，并提高了推理效率。

Conclusion: 未来的研究方向包括加快响应时间、根据任务复杂性自适应选择模型，以及在异构环境中实现可扩展部署，从而使基于LLM的系统更高效、更适用于实际应用。

Abstract: Recent progress in Language Models (LMs) has dramatically advanced the field
of natural language processing (NLP), excelling at tasks like text generation,
summarization, and question answering. However, their inference remains
computationally expensive and energy intensive, especially in settings with
limited hardware, power, or bandwidth. This makes it difficult to deploy LMs in
mobile, edge, or cost sensitive environments. To address these challenges,
recent approaches have introduced multi LLM intelligent model selection
strategies that dynamically allocate computational resources based on query
complexity -- using lightweight models for simpler queries and escalating to
larger models only when necessary. This survey explores two complementary
strategies for efficient LLM inference: (i) routing, which selects the most
suitable model based on the query, and (ii) cascading or hierarchical inference
(HI), which escalates queries through a sequence of models until a confident
response is found. Both approaches aim to reduce computation by using
lightweight models for simpler tasks while offloading only when needed. We
provide a comparative analysis of these techniques across key performance
metrics, discuss benchmarking efforts, and outline open challenges. Finally, we
outline future research directions to enable faster response times, adaptive
model selection based on task complexity, and scalable deployment across
heterogeneous environments, making LLM based systems more efficient and
accessible for real world applications.

</details>


### [251] [Demystifying Topological Message-Passing with Relational Structures: A Case Study on Oversquashing in Simplicial Message-Passing](https://arxiv.org/abs/2506.06582)
*Diaaeldin Taha,James Chapman,Marzieh Eidi,Karel Devriendt,Guido Montúfar*

Main category: cs.LG

TL;DR: 本文提出了一种统一的公理化框架，通过关系结构的视角将图和拓扑消息传递联系起来，扩展了图论结果和算法到高阶结构，以分析和缓解拓扑消息传递网络中的过度压缩问题。


<details>
  <summary>Details</summary>
Motivation: 拓扑深度学习（TDL）在建模关系数据中的高阶交互方面表现出强大的能力，但拓扑消息传递中的过度压缩现象缺乏理论分析。

Method: 本文提出了一种统一的公理化框架，通过关系结构的视角将图和拓扑消息传递联系起来，扩展了图论结果和算法到高阶结构。

Result: 通过理论分析和在单纯网络上的实证研究，展示了该框架在推进拓扑深度学习中的潜力。

Conclusion: 该框架为分析和缓解拓扑消息传递网络中的过度压缩问题提供了新的视角和方法，推动了拓扑深度学习的发展。

Abstract: Topological deep learning (TDL) has emerged as a powerful tool for modeling
higher-order interactions in relational data. However, phenomena such as
oversquashing in topological message-passing remain understudied and lack
theoretical analysis. We propose a unifying axiomatic framework that bridges
graph and topological message-passing by viewing simplicial and cellular
complexes and their message-passing schemes through the lens of relational
structures. This approach extends graph-theoretic results and algorithms to
higher-order structures, facilitating the analysis and mitigation of
oversquashing in topological message-passing networks. Through theoretical
analysis and empirical studies on simplicial networks, we demonstrate the
potential of this framework to advance TDL.

</details>


### [252] [Global Convergence of Gradient EM for Over-Parameterized Gaussian Mixtures](https://arxiv.org/abs/2506.06584)
*Mo Zhou,Weihang Xu,Maryam Fazel,Simon S. Du*

Main category: cs.LG

TL;DR: 本文首次证明了在过参数化设置下，梯度EM算法能全局收敛并恢复真实高斯混合模型，突破了以往仅适用于两成分的限制。


<details>
  <summary>Details</summary>
Motivation: 传统EM算法在模型成分数m≥3时无法保证全局收敛，而实际应用中常需处理多成分模型。过参数化（n>m）可能提供新解决方案。

Method: 结合Hermite多项式分析梯度EM动态，利用张量分解刻画似然损失几何景观，分两阶段进行理论分析。

Result: 当n=Ω(mlogm)时，随机初始化的梯度EM能以多项式样本量和速率全局收敛到真实参数，适用于任意良好分离的GMM。

Conclusion: 首次为m>2的GMM建立梯度EM全局收敛理论，为EM算法分析提供了新的数学框架与工具。

Abstract: Learning Gaussian Mixture Models (GMMs) is a fundamental problem in machine
learning, with the Expectation-Maximization (EM) algorithm and its popular
variant gradient EM being arguably the most widely used algorithms in practice.
In the exact-parameterized setting, where both the ground truth GMM and the
learning model have the same number of components $m$, a vast line of work has
aimed to establish rigorous recovery guarantees for EM. However, global
convergence has only been proven for the case of $m=2$, and EM is known to fail
to recover the ground truth when $m\geq 3$.
  In this paper, we consider the $\textit{over-parameterized}$ setting, where
the learning model uses $n>m$ components to fit an $m$-component ground truth
GMM. In contrast to the exact-parameterized case, we provide a rigorous global
convergence guarantee for gradient EM. Specifically, for any well separated
GMMs in general position, we prove that with only mild over-parameterization $n
= \Omega(m\log m)$, randomly initialized gradient EM converges globally to the
ground truth at a polynomial rate with polynomial samples. Our analysis
proceeds in two stages and introduces a suite of novel tools for Gaussian
Mixture analysis. We use Hermite polynomials to study the dynamics of gradient
EM and employ tensor decomposition to characterize the geometric landscape of
the likelihood loss. This is the first global convergence and recovery result
for EM or Gradient EM beyond the special case of $m=2$.

</details>


### [253] [Direct Prediction Set Minimization via Bilevel Conformal Classifier Training](https://arxiv.org/abs/2506.06599)
*Yuanjie Shi,Hooman Shahrokhi,Xuesong Jia,Xiongzhi Chen,Janardhan Rao Doppa,Yan Yan*

Main category: cs.LG

TL;DR: 本文提出了一种新的直接预测集最小化（DPSM）算法，通过将保形原则融入深度分类器的训练过程中，显著减少了预测集的大小，并在多个基准数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 标准的保形预测（CP）校准方法通常会产生较大的预测集，这在实际应用中降低了其有效性。为了解决这一问题，本文探讨了如何将保形原则融入深度分类器的训练过程中，以直接最小化预测集的大小。

Method: 本文提出了一种名为直接预测集最小化（DPSM）的算法，将其形式化为一个双层优化问题。DPSM的关键在于最小化预测集大小的度量（上层），该度量以学习的符合性分数分位数（下层）为条件。

Result: 实验结果表明，DPSM在多个基准数据集和深度模型上显著优于现有的最佳保形训练基线，预测集大小减少了20.46%，并验证了理论分析。

Conclusion: DPSM算法通过将保形原则融入训练过程，有效减少了预测集的大小，并在理论和实验上均表现出优越性能。

Abstract: Conformal prediction (CP) is a promising uncertainty quantification framework
which works as a wrapper around a black-box classifier to construct prediction
sets (i.e., subset of candidate classes) with provable guarantees. However,
standard calibration methods for CP tend to produce large prediction sets which
makes them less useful in practice. This paper considers the problem of
integrating conformal principles into the training process of deep classifiers
to directly minimize the size of prediction sets. We formulate conformal
training as a bilevel optimization problem and propose the {\em Direct
Prediction Set Minimization (DPSM)} algorithm to solve it. The key insight
behind DPSM is to minimize a measure of the prediction set size (upper level)
that is conditioned on the learned quantile of conformity scores (lower level).
We analyze that DPSM has a learning bound of $O(1/\sqrt{n})$ (with $n$ training
samples), while prior conformal training methods based on stochastic
approximation for the quantile has a bound of $\Omega(1/s)$ (with batch size
$s$ and typically $s \ll \sqrt{n}$). Experiments on various benchmark datasets
and deep models show that DPSM significantly outperforms the best prior
conformal training baseline with $20.46\%\downarrow$ in the prediction set size
and validates our theory.

</details>


### [254] [CAtCh: Cognitive Assessment through Cookie Thief](https://arxiv.org/abs/2506.06603)
*Joseph T Colonel,Carolyn Hagler,Guiselle Wismer,Laura Curtis,Jacqueline Becker,Juan Wisnivesky,Alex Federman,Gaurav Pandey*

Main category: cs.LG

TL;DR: 本文评估了多种基于语音和多模态的方法，用于从患者音频中预测广泛认知障碍（CI），发现多模态方法及声学特征（如情感和韵律）优于单模态和语言学特征。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习算法虽能预测阿尔茨海默病及相关痴呆（ADRD），但未扩展至更广泛的认知障碍（CI），而CI可能是ADRD的前兆和风险因素。

Method: 使用基于语音的开源方法（原用于ADRD预测）和多模态情感分析方法，结合声学与语言学特征（如BERT和可解释特征），预测CI。

Result: 多模态方法优于单模态；声学方法（尤其是情感和韵律相关特征）显著优于基于BERT的及可解释的语言学特征。

Conclusion: 多模态和声学方法在CI预测中表现更优，相关代码已开源，为早期筛查提供潜在工具。

Abstract: Several machine learning algorithms have been developed for the prediction of
Alzheimer's disease and related dementia (ADRD) from spontaneous speech.
However, none of these algorithms have been translated for the prediction of
broader cognitive impairment (CI), which in some cases is a precursor and risk
factor of ADRD. In this paper, we evaluated several speech-based open-source
methods originally proposed for the prediction of ADRD, as well as methods from
multimodal sentiment analysis for the task of predicting CI from patient audio
recordings. Results demonstrated that multimodal methods outperformed unimodal
ones for CI prediction, and that acoustics-based approaches performed better
than linguistics-based ones. Specifically, interpretable acoustic features
relating to affect and prosody were found to significantly outperform
BERT-based linguistic features and interpretable linguistic features,
respectively. All the code developed for this study is available at
https://github.com/JTColonel/catch.

</details>


### [255] [Stacey: Promoting Stochastic Steepest Descent via Accelerated $\ell_p$-Smooth Nonconvex Optimization](https://arxiv.org/abs/2506.06606)
*Xinyu Luo,Cedar Site Bai,Bolian Li,Petros Drineas,Ruqi Zhang,Brian Bullins*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: While popular optimization methods such as SGD, AdamW, and Lion depend on
steepest descent updates in either $\ell_2$ or $\ell_\infty$ norms, there
remains a critical gap in handling the non-Euclidean structure observed in
modern deep networks training. In this work, we address this need by
introducing a new accelerated $\ell_p$ steepest descent algorithm, called
Stacey, which uses interpolated primal-dual iterate sequences to effectively
navigate non-Euclidean smooth optimization tasks. In addition to providing
novel theoretical guarantees for the foundations of our algorithm, we
empirically compare our approach against these popular methods on tasks
including image classification and language model (LLM) pretraining,
demonstrating both faster convergence and higher final accuracy. We further
evaluate different values of $p$ across various models and datasets,
underscoring the importance and efficiency of non-Euclidean approaches over
standard Euclidean methods. Code can be found at
https://github.com/xinyuluo8561/Stacey .

</details>


### [256] [Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning](https://arxiv.org/abs/2506.06632)
*Shubham Parashar,Shurui Gui,Xiner Li,Hongyi Ling,Sushil Vemuri,Blake Olson,Eric Li,Yu Zhang,James Caverlee,Dileep Kalathil,Shuiwang Ji*

Main category: cs.LG

TL;DR: 本文提出了一种名为E2H Reasoner的方法，通过从易到难的任务调度来增强小规模语言模型的推理能力，避免过拟合并减少样本需求。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法在提升语言模型对复杂任务的推理能力上效果有限，因此需要一种更有效的方法来逐步提升模型的推理技能。

Method: 提出了E2H Reasoner方法，通过从易到难的任务调度，逐步增强语言模型的推理能力，并在理论框架下证明了其收敛性。

Result: 实验表明，E2H Reasoner显著提升了小规模语言模型（1.5B到3B）的推理能力，且比直接使用强化学习更有效。

Conclusion: E2H Reasoner通过任务调度和逐步学习，有效提升了小规模语言模型的推理能力，并减少了样本需求。

Abstract: We aim to improve the reasoning capabilities of language models via
reinforcement learning (RL). Recent RL post-trained models like DeepSeek-R1
have demonstrated reasoning abilities on mathematical and coding tasks.
However, prior studies suggest that using RL alone to improve reasoning on
inherently difficult tasks is less effective. Here, we draw inspiration from
curriculum learning and propose to schedule tasks from easy to hard (E2H),
allowing LLMs to build reasoning skills gradually. Our method is termed E2H
Reasoner. Empirically, we observe that, although easy tasks are important
initially, fading them out through appropriate scheduling is essential in
preventing overfitting. Theoretically, we establish convergence guarantees for
E2H Reasoner within an approximate policy iteration framework. We derive
finite-sample complexity bounds and show that when tasks are appropriately
decomposed and conditioned, learning through curriculum stages requires fewer
total samples than direct learning. Experiments across multiple domains show
that E2H Reasoner significantly improves the reasoning ability of small LLMs
(1.5B to 3B), which otherwise struggle when trained with vanilla RL alone,
highlighting the effectiveness of our method.

</details>


### [257] [Vision-QRWKV: Exploring Quantum-Enhanced RWKV Models for Image Classification](https://arxiv.org/abs/2506.06633)
*Chi-Sheng Chen*

Main category: cs.LG

TL;DR: 本文提出了一种混合量子-经典扩展的Vision-QRWKV模型，首次应用于图像分类任务，通过将变分量子电路集成到RWKV的通道混合组件中，提升了非线性特征转换和视觉表示的表达能力。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习的最新进展展示了在复杂高维数据领域增强经典神经网络架构的潜力，特别是在时间序列建模方面。本文旨在将量子增强技术应用于视觉领域，探索量子模型在轻量级和高效视觉任务中的潜力。

Method: 本文提出了Vision-QRWKV模型，通过在Receptance Weighted Key Value (RWKV)架构的通道混合组件中集成变分量子电路（VQC），来增强非线性特征转换和视觉表示的表达能力。

Result: 在14个医学和标准图像分类基准数据集上的评估表明，量子增强的Vision-QRWKV模型在大多数数据集上优于其经典版本，特别是在具有细微或噪声类别区分的数据集上（如ChestMNIST、RetinaMNIST、BloodMNIST）。

Conclusion: 本研究首次系统地将量子增强的RWKV应用于视觉领域，展示了量子模型在轻量级和高效视觉任务中的潜力，并为未来的架构权衡提供了见解。

Abstract: Recent advancements in quantum machine learning have shown promise in
enhancing classical neural network architectures, particularly in domains
involving complex, high-dimensional data. Building upon prior work in temporal
sequence modeling, this paper introduces Vision-QRWKV, a hybrid
quantum-classical extension of the Receptance Weighted Key Value (RWKV)
architecture, applied for the first time to image classification tasks. By
integrating a variational quantum circuit (VQC) into the channel mixing
component of RWKV, our model aims to improve nonlinear feature transformation
and enhance the expressive capacity of visual representations.
  We evaluate both classical and quantum RWKV models on a diverse collection of
14 medical and standard image classification benchmarks, including MedMNIST
datasets, MNIST, and FashionMNIST. Our results demonstrate that the
quantum-enhanced model outperforms its classical counterpart on a majority of
datasets, particularly those with subtle or noisy class distinctions (e.g.,
ChestMNIST, RetinaMNIST, BloodMNIST). This study represents the first
systematic application of quantum-enhanced RWKV in the visual domain, offering
insights into the architectural trade-offs and future potential of quantum
models for lightweight and efficient vision tasks.

</details>


### [258] [Non-Intrusive Load Monitoring Based on Image Load Signatures and Continual Learning](https://arxiv.org/abs/2506.06637)
*Olimjon Toirov,Wei Yu*

Main category: cs.LG

TL;DR: 本文提出一种结合图像负载特征和持续学习的非侵入式负载监测方法，通过将多维电力信号转换为图像特征并利用深度卷积网络及持续学习策略，显著提升了识别精度。


<details>
  <summary>Details</summary>
Motivation: 传统非侵入式负载监测方法因负载组合复杂多变，存在特征鲁棒性差、模型泛化能力不足的问题，难以适应新负载场景。

Method: 将电流、电压等多维信号转化为图像特征，采用深度卷积网络进行分类；引入自监督预训练增强特征泛化，结合持续在线学习策略缓解模型遗忘问题。

Result: 在高采样率负载数据集上的实验表明，该方法相比现有方法在识别准确率上取得显著提升。

Conclusion: 所提出的图像特征与持续学习融合方法有效解决了传统NILM的局限性，具备适应动态负载环境的能力。

Abstract: Non-Intrusive Load Monitoring (NILM) identifies the operating status and
energy consumption of each electrical device in the circuit by analyzing the
electrical signals at the bus, which is of great significance for smart power
management. However, the complex and changeable load combinations and
application environments lead to the challenges of poor feature robustness and
insufficient model generalization of traditional NILM methods. To this end,
this paper proposes a new non-intrusive load monitoring method that integrates
"image load signature" and continual learning. This method converts
multi-dimensional power signals such as current, voltage, and power factor into
visual image load feature signatures, and combines deep convolutional neural
networks to realize the identification and classification of multiple devices;
at the same time, self-supervised pre-training is introduced to improve feature
generalization, and continual online learning strategies are used to overcome
model forgetting to adapt to the emergence of new loads. This paper conducts a
large number of experiments on high-sampling rate load datasets, and compares a
variety of existing methods and model variants. The results show that the
proposed method has achieved significant improvements in recognition accuracy.

</details>


### [259] [Spark Transformer: Reactivating Sparsity in FFN and Attention](https://arxiv.org/abs/2506.06644)
*Chong You,Kan Wu,Zhipeng Jia,Lin Chen,Srinadh Bhojanapalli,Jiaxian Guo,Utku Evci,Jan Wassenberg,Praneeth Netrapalli,Jeremiah J. Willcock,Suvinay Subramanian,Felix Chern,Alek Andreev,Shreya Pathak,Felix Yu,Prateek Jain,David E. Culler,Henry M. Levy,Sanjiv Kumar*

Main category: cs.LG

TL;DR: 本文提出Spark Transformer，通过top-k掩码和硬件友好的统计top-k算法，在保持模型质量与参数量的同时，实现前馈网络和注意力机制的高激活稀疏性，显著降低计算量并提升解码速度。


<details>
  <summary>Details</summary>
Motivation: 现代Transformer弃用ReLU导致激活稀疏性难以利用，现有稀疏化方法常导致模型质量下降、参数增加或训练效率降低，注意力稀疏化也面临类似挑战。

Method: 使用可控制稀疏度的top-k掩码，提出线性时间近似算法统计top-k避免排序开销，通过参数重分配构建低成本预测器识别激活项，兼容标准训练流程。

Result: FFN中仅8%神经元激活，每个token最多关注256个token，FLOPs减少2.5倍，CPU/GPU解码速度分别提升1.79x/1.40x，在Gemma-2预训练下保持基准测试竞争力。

Conclusion: Spark Transformer在维持模型性能的前提下，通过系统级优化实现了高效激活稀疏性，为大规模Transformer的实际部署提供了有效的效率提升方案。

Abstract: The discovery of the lazy neuron phenomenon in trained Transformers, where
the vast majority of neurons in their feed-forward networks (FFN) are inactive
for each token, has spurred tremendous interests in activation sparsity for
enhancing large model efficiency. While notable progress has been made in
translating such sparsity to wall-time benefits, modern Transformers have moved
away from the ReLU activation function crucial to this phenomenon. Existing
efforts on re-introducing activation sparsity often degrade model quality,
increase parameter count, complicate or slow down training. Sparse attention,
the application of sparse activation to the attention mechanism, often faces
similar challenges.
  This paper introduces the Spark Transformer, a novel architecture that
achieves a high level of activation sparsity in both FFN and the attention
mechanism while maintaining model quality, parameter count, and standard
training procedures. Our method realizes sparsity via top-k masking for
explicit control over sparsity level. Crucially, we introduce statistical
top-k, a hardware-accelerator-friendly, linear-time approximate algorithm that
avoids costly sorting and mitigates significant training slowdown from standard
top-$k$ operators. Furthermore, Spark Transformer reallocates existing FFN
parameters and attention key embeddings to form a low-cost predictor for
identifying activated entries. This design not only mitigates quality loss from
enforced sparsity, but also enhances wall-time benefit. Pretrained with the
Gemma-2 recipe, Spark Transformer demonstrates competitive performance on
standard benchmarks while exhibiting significant sparsity: only 8% of FFN
neurons are activated, and each token attends to a maximum of 256 tokens. This
sparsity translates to a 2.5x reduction in FLOPs, leading to decoding wall-time
speedups of up to 1.79x on CPU and 1.40x on GPU.

</details>


### [260] [SAFER: A Calibrated Risk-Aware Multimodal Recommendation Model for Dynamic Treatment Regimes](https://arxiv.org/abs/2506.06649)
*Yishan Shen,Yuyang Ye,Hui Xiong,Yong Chen*

Main category: cs.LG

TL;DR: 本文提出SAFER框架，整合结构化电子健康记录与临床文本，通过共形预测提供统计安全保障，优化动态治疗策略的可靠性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有动态治疗策略依赖临床黄金标准且未充分利用非结构化临床文本，导致推荐可靠性受限，需解决标签不确定性和安全风险问题。

Method: SAFER融合结构化/非结构化医疗数据，假设死亡患者存在治疗模糊性以处理标签不确定性，并采用共形预测过滤高风险推荐。

Result: 在脓毒症数据集上，SAFER在推荐指标和反事实死亡率上超越现有方法，同时提供形式化安全保障。

Conclusion: SAFER为高风险动态治疗场景提供了可信赖的理论基础解决方案，兼具数据融合与安全校准能力。

Abstract: Dynamic treatment regimes (DTRs) are critical to precision medicine,
optimizing long-term outcomes through personalized, real-time decision-making
in evolving clinical contexts, but require careful supervision for unsafe
treatment risks. Existing efforts rely primarily on clinician-prescribed gold
standards despite the absence of a known optimal strategy, and predominantly
using structured EHR data without extracting valuable insights from clinical
notes, limiting their reliability for treatment recommendations. In this work,
we introduce SAFER, a calibrated risk-aware tabular-language recommendation
framework for DTR that integrates both structured EHR and clinical notes,
enabling them to learn from each other, and addresses inherent label
uncertainty by assuming ambiguous optimal treatment solution for deceased
patients. Moreover, SAFER employs conformal prediction to provide statistical
guarantees, ensuring safe treatment recommendations while filtering out
uncertain predictions. Experiments on two publicly available sepsis datasets
demonstrate that SAFER outperforms state-of-the-art baselines across multiple
recommendation metrics and counterfactual mortality rate, while offering robust
formal assurances. These findings underscore SAFER potential as a trustworthy
and theoretically grounded solution for high-stakes DTR applications.

</details>


### [261] [Rescaled Influence Functions: Accurate Data Attribution in High Dimension](https://arxiv.org/abs/2506.06656)
*Ittai Rubinstein,Samuel B. Hopkins*

Main category: cs.LG

TL;DR: 本文提出了一种新的数据归因工具——重缩放影响函数（RIF），以替代传统的影响函数（IF），在计算开销几乎不变的情况下显著提高了准确性，并展示了其在实践中的优越性。


<details>
  <summary>Details</summary>
Motivation: 研究训练数据如何影响模型行为，特别是在高维情况下，传统的影响函数（IF）往往不准确，低估样本移除的影响。

Method: 提出重缩放影响函数（RIF），作为影响函数（IF）的替代方案，通过理论分析和实验验证其有效性。

Result: RIF在多个真实数据集上表现出显著优于IF的预测准确性，并能检测到IF无法识别的数据投毒攻击。

Conclusion: RIF是一种高效且准确的数据归因工具，能够显著提升模型行为预测的准确性，并在实际应用中表现出色。

Abstract: How does the training data affect a model's behavior? This is the question we
seek to answer with data attribution. The leading practical approaches to data
attribution are based on influence functions (IF). IFs utilize a first-order
Taylor approximation to efficiently predict the effect of removing a set of
samples from the training set without retraining the model, and are used in a
wide variety of machine learning applications. However, especially in the
high-dimensional regime (# params $\geq \Omega($# samples$)$), they are often
imprecise and tend to underestimate the effect of sample removals, even for
simple models such as logistic regression. We present rescaled influence
functions (RIF), a new tool for data attribution which can be used as a drop-in
replacement for influence functions, with little computational overhead but
significant improvement in accuracy. We compare IF and RIF on a range of
real-world datasets, showing that RIFs offer significantly better predictions
in practice, and present a theoretical analysis explaining this improvement.
Finally, we present a simple class of data poisoning attacks that would fool
IF-based detections but would be detected by RIF.

</details>


### [262] [SDP-CROWN: Efficient Bound Propagation for Neural Network Verification with Tightness of Semidefinite Programming](https://arxiv.org/abs/2506.06665)
*Hong-Ming Chiu,Hao Chen,Huan Zhang,Richard Y. Zhang*

Main category: cs.LG

TL;DR: 提出SDP-CROWN框架，结合SDP松弛的紧致性和边界传播验证器的可扩展性，显著提升大规模神经网络验证的紧密度。


<details>
  <summary>Details</summary>
Motivation: 现有线性边界传播验证器在神经元耦合场景下边界松散，而基于SDP的验证器虽能捕捉耦合但计算复杂度高，难以扩展至大模型。需结合两者优势。

Method: 通过SDP原理推导新型线性边界，显式捕获基于L2范数的神经元间耦合，每层增加单个参数，无缝集成至线性边界传播流程。

Result: 理论证明神经元间边界比传统方法紧致√n倍；实验在6.5万神经元/247万参数模型上验证性能接近SDP方法，显著优于现有边界传播方法。

Conclusion: SDP-CROWN在保持边界传播可扩展性的同时，通过SDP增强边界紧密度，为大规模网络验证提供高效且精确的解决方案。

Abstract: Neural network verifiers based on linear bound propagation scale impressively
to massive models but can be surprisingly loose when neuron coupling is
crucial. Conversely, semidefinite programming (SDP) verifiers capture
inter-neuron coupling naturally, but their cubic complexity restricts them to
only small models. In this paper, we propose SDP-CROWN, a novel hybrid
verification framework that combines the tightness of SDP relaxations with the
scalability of bound-propagation verifiers. At the core of SDP-CROWN is a new
linear bound, derived via SDP principles, that explicitly captures
$\ell_{2}$-norm-based inter-neuron coupling while adding only one extra
parameter per layer. This bound can be integrated seamlessly into any linear
bound-propagation pipeline, preserving the inherent scalability of such methods
yet significantly improving tightness. In theory, we prove that our
inter-neuron bound can be up to a factor of $\sqrt{n}$ tighter than traditional
per-neuron bounds. In practice, when incorporated into the state-of-the-art
$\alpha$-CROWN verifier, we observe markedly improved verification performance
on large models with up to 65 thousand neurons and 2.47 million parameters,
achieving tightness that approaches that of costly SDP-based methods.

</details>


### [263] [Through the Gaps: Uncovering Tactical Line-Breaking Passes with Clustering](https://arxiv.org/abs/2506.06666)
*Oktay Karakuş,Hasan Arkadaş*

Main category: cs.LG

TL;DR: 本文提出了一种基于聚类的无监督框架，用于检测和分析足球比赛中的线间传球（LBPs），并引入多个战术指标来量化其效果。


<details>
  <summary>Details</summary>
Motivation: 线间传球是足球比赛中的关键战术动作，能够帮助球队突破防线并进入高价值区域。本文旨在通过同步事件和跟踪数据，提供一种可解释、可扩展的方法来检测和分析这些传球。

Method: 本文采用无监督的聚类框架，通过垂直空间分割对手球队的阵型，并识别在开放比赛中突破防线的传球。此外，引入了空间构建比（SBR）和两个链式变体（LBPCh^1和LBPCh^2）等战术指标。

Result: 在2022年世界杯的比赛中，本文方法成功揭示了不同球队和球员在垂直推进和结构破坏方面的风格差异。

Conclusion: 本文提出的方法具有可解释性和可扩展性，可直接应用于现代足球表现分析和球探工作流程。

Abstract: Line-breaking passes (LBPs) are crucial tactical actions in football,
allowing teams to penetrate defensive lines and access high-value spaces. In
this study, we present an unsupervised, clustering-based framework for
detecting and analysing LBPs using synchronised event and tracking data from
elite matches. Our approach models opponent team shape through vertical spatial
segmentation and identifies passes that disrupt defensive lines within open
play. Beyond detection, we introduce several tactical metrics, including the
space build-up ratio (SBR) and two chain-based variants, LBPCh$^1$ and
LBPCh$^2$, which quantify the effectiveness of LBPs in generating immediate or
sustained attacking threats. We evaluate these metrics across teams and players
in the 2022 FIFA World Cup, revealing stylistic differences in vertical
progression and structural disruption. The proposed methodology is explainable,
scalable, and directly applicable to modern performance analysis and scouting
workflows.

</details>


### [264] [Learning Robust Heterogeneous Graph Representations via Contrastive-Reconstruction under Sparse Semantics](https://arxiv.org/abs/2506.06682)
*Di Lin,Wanjing Ren,Xuanbin Li,Rui Zhang*

Main category: cs.LG

TL;DR: HetCRF提出了一种新的双通道自监督学习框架，适用于异质图，通过两阶段聚合策略适应嵌入语义，解决了语义稀疏和梯度不平衡问题，在节点分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 在异质图的自监督学习中，掩码自编码器（MAE）和对比学习（CL）各有优势，但现有混合框架在共享编码器设计和语义稀疏场景下面临挑战。本文旨在解决这些问题，提出一种新的框架HetCRF。

Method: HetCRF采用双通道自监督学习框架，使用两阶段聚合策略适应嵌入语义，增强编码器输出以解决语义稀疏问题，并提出两种正样本增强策略以平衡梯度贡献。

Result: 在四个真实世界的异质图数据集上的节点分类实验中，HetCRF在Aminer和Freebase数据集上分别将Macro-F1分数提高了2.75%和2.2%，验证了其有效性和优越性。

Conclusion: HetCRF在异质图的自监督学习中表现出色，特别是在语义稀疏和梯度不平衡问题上，显著提升了节点分类的性能。

Abstract: In graph self-supervised learning, masked autoencoders (MAE) and contrastive
learning (CL) are two prominent paradigms. MAE focuses on reconstructing masked
elements, while CL maximizes similarity between augmented graph views. Recent
studies highlight their complementarity: MAE excels at local feature capture,
and CL at global information extraction. Hybrid frameworks for homogeneous
graphs have been proposed, but face challenges in designing shared encoders to
meet the semantic requirements of both tasks. In semantically sparse scenarios,
CL struggles with view construction, and gradient imbalance between positive
and negative samples persists. This paper introduces HetCRF, a novel
dual-channel self-supervised learning framework for heterogeneous graphs.
HetCRF uses a two-stage aggregation strategy to adapt embedding semantics,
making it compatible with both MAE and CL. To address semantic sparsity, it
enhances encoder output for view construction instead of relying on raw
features, improving efficiency. Two positive sample augmentation strategies are
also proposed to balance gradient contributions. Node classification
experiments on four real-world heterogeneous graph datasets demonstrate that
HetCRF outperforms state-of-the-art baselines. On datasets with missing node
features, such as Aminer and Freebase, at a 40% label rate in node
classification, HetCRF improves the Macro-F1 score by 2.75% and 2.2%
respectively compared to the second-best baseline, validating its effectiveness
and superiority.

</details>


### [265] [Breaking Data Silos: Towards Open and Scalable Mobility Foundation Models via Generative Continual Learning](https://arxiv.org/abs/2506.06694)
*Yuan Yuan,Yukun Liu,Chonghua Han,Jie Feng,Yong Li*

Main category: cs.LG

TL;DR: MoveGCL提出了一种可扩展且保护隐私的框架，通过生成式持续学习训练移动性基础模型，解决了数据孤岛和隐私问题，并在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于移动数据的隐私敏感性，构建类似于自然语言处理和计算机视觉领域的通用基础模型面临挑战。MoveGCL旨在解决这一问题，推动开放、可扩展且保护隐私的模型开发。

Method: MoveGCL通过生成式持续学习框架，利用冻结的教师模型生成合成轨迹进行分散式模型进化，并结合专家混合Transformer和移动感知专家路由机制，以及分层渐进适应策略来稳定持续更新。

Result: 在六个真实城市数据集上的实验表明，MoveGCL的性能与联合训练相当，显著优于联邦学习基线，同时提供了强大的隐私保护。

Conclusion: MoveGCL为移动性基础模型的开发提供了实用蓝图，标志着在基础模型时代实现开放、可扩展且保护隐私的模型开发迈出了关键一步。

Abstract: Foundation models have revolutionized fields such as natural language
processing and computer vision by enabling general-purpose learning across
diverse tasks and datasets. However, building analogous models for human
mobility remains challenging due to the privacy-sensitive nature of mobility
data and the resulting data silos across institutions. To bridge this gap, we
propose MoveGCL, a scalable and privacy-preserving framework for training
mobility foundation models via generative continual learning. Without sharing
raw data, MoveGCL enables decentralized and progressive model evolution by
replaying synthetic trajectories generated from a frozen teacher model, and
reinforces knowledge retention through a tailored distillation strategy that
mitigates catastrophic forgetting. To address the heterogeneity of mobility
patterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a
mobility-aware expert routing mechanism, and employs a layer-wise progressive
adaptation strategy to stabilize continual updates. Experiments on six
real-world urban datasets demonstrate that MoveGCL achieves performance
comparable to joint training and significantly outperforms federated learning
baselines, while offering strong privacy protection. MoveGCL marks a crucial
step toward unlocking foundation models for mobility, offering a practical
blueprint for open, scalable, and privacy-preserving model development in the
era of foundation models.

</details>


### [266] [MarginSel : Max-Margin Demonstration Selection for LLMs](https://arxiv.org/abs/2506.06699)
*Rajeev Bhatt Ambati,James Lester,Shashank Srivastava,Snigdha Chaturvedi*

Main category: cs.LG

TL;DR: 本文提出了一种名为MarginSel的方法，通过选择困难的示例来提升大语言模型在少样本学习中的表现，取得了显著的F1分数提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在少样本学习中的表现依赖于示例的选择和顺序，现有方法对此敏感，因此需要一种更有效的示例选择策略。

Method: MarginSel是一种两步法，针对每个测试实例选择困难的示例，以最大化模型的决策边界。

Result: 与随机选择示例相比，MarginSel在分类任务中实现了2-7%的F1分数绝对提升。

Conclusion: MarginSel通过增加困难示例的边界，有效改善了大语言模型的决策边界，提升了少样本学习的效果。

Abstract: Large Language Models (LLMs) excel at few-shot learning via in-context
learning (ICL). However, the effectiveness of ICL is often sensitive to the
selection and ordering of demonstration examples. To address this, we present
MarginSel: Max-Margin Demonstration Selection for LLMs, a two-step method that
selects hard demonstration examples for the ICL prompt, adapting to each test
instance. Our approach achieves 2-7% absolute improvement in F1-score across
classification tasks, compared to a random selection of examples. We also
provide theoretical insights and empirical evidence showing that MarginSel
induces max-margin behavior in LLMs by effectively increasing the margin for
hard examples, analogous to support vectors, thereby shifting the decision
boundary in a beneficial direction.

</details>


### [267] [Do Protein Transformers Have Biological Intelligence?](https://arxiv.org/abs/2506.06701)
*Fudong Lin,Wanrou Du,Jinchan Liu,Tarikul Milon,Shelby Meche,Wu Xu,Xiaoqi Qin,Xu Yuan*

Main category: cs.LG

TL;DR: 本文探索了蛋白质Transformer是否能够捕捉蛋白质序列中的生物智能，提出了新的Transformer架构SPT和解释性AI技术Sequence Score，并在两个数据集上取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索蛋白质Transformer是否能够捕捉蛋白质序列中的生物智能，并解决解释这些模型决策过程的难题。

Method: 首先引入了Protein-FN数据集，设计了新的Transformer架构SPT，并开发了解释性AI技术Sequence Score。

Result: 最小的SPT-Tiny模型在AR数据集上达到了94.3%的准确率，在Protein-FN数据集上达到了99.6%的准确率，且Sequence Score技术揭示了SPT模型能够发现与生物学领域知识一致的蛋白质序列结构模式。

Conclusion: SPT模型能够有效捕捉蛋白质序列中的生物智能，且Sequence Score技术为解释模型决策提供了有力工具。

Abstract: Deep neural networks, particularly Transformers, have been widely adopted for
predicting the functional properties of proteins. In this work, we focus on
exploring whether Protein Transformers can capture biological intelligence
among protein sequences. To achieve our goal, we first introduce a protein
function dataset, namely Protein-FN, providing over 9000 protein data with
meaningful labels. Second, we devise a new Transformer architecture, namely
Sequence Protein Transformers (SPT), for computationally efficient protein
function predictions. Third, we develop a novel Explainable Artificial
Intelligence (XAI) technique called Sequence Score, which can efficiently
interpret the decision-making processes of protein models, thereby overcoming
the difficulty of deciphering biological intelligence bided in Protein
Transformers. Remarkably, even our smallest SPT-Tiny model, which contains only
5.4M parameters, demonstrates impressive predictive accuracy, achieving 94.3%
on the Antibiotic Resistance (AR) dataset and 99.6% on the Protein-FN dataset,
all accomplished by training from scratch. Besides, our Sequence Score
technique helps reveal that our SPT models can discover several meaningful
patterns underlying the sequence structures of protein data, with these
patterns aligning closely with the domain knowledge in the biology community.
We have officially released our Protein-FN dataset on Hugging Face Datasets
https://huggingface.co/datasets/Protein-FN/Protein-FN. Our code is available at
https://github.com/fudong03/BioIntelligence.

</details>


### [268] [A Framework for Controllable Multi-objective Learning with Annealed Stein Variational Hypernetworks](https://arxiv.org/abs/2506.06715)
*Minh-Duc Nguyen,Dung D. Le*

Main category: cs.LG

TL;DR: 提出SVH-MOL方法，利用Stein变分梯度下降（SVGD）解决多目标学习中帕累托解的多样性与超体积最大化矛盾，通过梯度策略与退火机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有帕累托集学习方法在最大化超体积值的同时难以保证解的多样性，需平衡两者以实现更优的多目标优化。

Method: 采用SVGD驱动粒子群逼近帕累托集，结合多样化梯度方向策略与退火机制构建统一框架，增强解的收敛性与多样性。

Result: 在多目标优化及多任务学习实验中验证了SVH-MOL的有效性，其性能优于现有方法。

Conclusion: SVH-MOL通过SVGD与梯度策略的融合，显著提升了帕累托解的多样性与优化效果，为多目标学习提供了高效解决方案。

Abstract: Pareto Set Learning (PSL) is popular as an efficient approach to obtaining
the complete optimal solution in Multi-objective Learning (MOL). A set of
optimal solutions approximates the Pareto set, and its mapping is a set of
dense points in the Pareto front in objective space. However, some current
methods face a challenge: how to make the Pareto solution is diverse while
maximizing the hypervolume value. In this paper, we propose a novel method to
address this challenge, which employs Stein Variational Gradient Descent (SVGD)
to approximate the entire Pareto set. SVGD pushes a set of particles towards
the Pareto set by applying a form of functional gradient descent, which helps
to converge and diversify optimal solutions. Additionally, we employ diverse
gradient direction strategies to thoroughly investigate a unified framework for
SVGD in multi-objective optimization and adapt this framework with an annealing
schedule to promote stability. We introduce our method, SVH-MOL, and validate
its effectiveness through extensive experiments on multi-objective problems and
multi-task learning, demonstrating its superior performance.

</details>


### [269] [The OCR Quest for Generalization: Learning to recognize low-resource alphabets with model editing](https://arxiv.org/abs/2506.06761)
*Adrià Molina Rodríguez,Oriol Ramos Terrades,Josep Lladós*

Main category: cs.LG

TL;DR: 本文提出了一种新方法，通过模型编辑技术增强对低资源语言的支持，提升了跨领域数据分布的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于低资源语言在大量预训练和基础技术中的代表性不足，本文旨在构建能够更快适应新数据分布的模型，特别是在字母表和非拉丁文字等领域的识别。

Method: 利用模型编辑的最新进展，增强对未见脚本的整合，展示了在稀疏数据分布中领域合并的有效性，无需与整体分布或原型需求相关。

Result: 实验表明，即使在相同的训练数据下，该方法在迁移学习到新字母表和跨领域评估中显著提升了性能，特别是在历史加密文本和非拉丁文字的挑战性领域转移中。

Conclusion: 本研究提出了一种新方法，使模型能够轻松采用代表性不足的字母表，从而将文档识别扩展到更广泛的背景和文化中。

Abstract: Achieving robustness in recognition systems across diverse domains is crucial
for their practical utility. While ample data availability is usually assumed,
low-resource languages, such as ancient manuscripts and non-western languages,
tend to be kept out of the equations of massive pretraining and foundational
techniques due to an under representation. In this work, we aim for building
models which can generalize to new distributions of data, such as alphabets,
faster than centralized fine-tune strategies. For doing so, we take advantage
of the recent advancements in model editing to enhance the incorporation of
unseen scripts (low-resource learning). In contrast to state-of-the-art
meta-learning, we showcase the effectiveness of domain merging in sparse
distributions of data, with agnosticity of its relation to the overall
distribution or any other prototyping necessity. Even when using the same exact
training data, our experiments showcase significant performance boosts in
\textbf{transfer learning} to new alphabets and \textbf{out-of-domain
evaluation} in challenging domain shifts, including historical ciphered texts
and non-Latin scripts. This research contributes a novel approach into building
models that can easily adopt under-represented alphabets and, therefore, enable
document recognition to a wider set of contexts and cultures.

</details>


### [270] [Feature-Based Instance Neighbor Discovery: Advanced Stable Test-Time Adaptation in Dynamic World](https://arxiv.org/abs/2506.06782)
*Qinting Jiang,Chuyang Ye,Dongyan Wei,Bingli Wang,Yuan Xue,Jingyan Jiang,Zhi Wang*

Main category: cs.LG

TL;DR: 本文提出FIND方法，通过特征解耦、自适应归一化及选择性优化，解决动态多分布测试场景下深度神经网络性能下降问题，显著提升准确率与效率。


<details>
  <summary>Details</summary>
Motivation: 现有测试时自适应方法（TTA）在处理批次内动态多分布测试数据时表现不佳，全局归一化策略会扭曲原始数据特征，导致QoE下降。

Method: FIND包含三个核心组件：层间特征解耦（LFD）构建图结构捕获相似分布特征；特征感知批量归一化（FABN）融合源域与测试域统计量；选择性FABN（S-FABN）动态决定分层策略以提升推理效率。

Result: 实验表明FIND在动态场景中准确率提升30%，且保持计算高效性，显著优于现有方法。

Conclusion: FIND通过细粒度特征分布建模与动态归一化策略，有效解决了多分布测试适应问题，为提升模型鲁棒性提供了新方向。

Abstract: Despite progress, deep neural networks still suffer performance declines
under distribution shifts between training and test domains, leading to a
substantial decrease in Quality of Experience (QoE) for applications. Existing
test-time adaptation (TTA) methods are challenged by dynamic, multiple test
distributions within batches. We observe that feature distributions across
different domains inherently cluster into distinct groups with varying means
and variances. This divergence reveals a critical limitation of previous global
normalization strategies in TTA, which inevitably distort the original data
characteristics. Based on this insight, we propose Feature-based Instance
Neighbor Discovery (FIND), which comprises three key components: Layer-wise
Feature Disentanglement (LFD), Feature Aware Batch Normalization (FABN) and
Selective FABN (S-FABN). LFD stably captures features with similar
distributions at each layer by constructing graph structures. While FABN
optimally combines source statistics with test-time distribution specific
statistics for robust feature representation. Finally, S-FABN determines which
layers require feature partitioning and which can remain unified, thereby
enhancing inference efficiency. Extensive experiments demonstrate that FIND
significantly outperforms existing methods, achieving a 30\% accuracy
improvement in dynamic scenarios while maintaining computational efficiency.

</details>


### [271] [Caterpillar GNN: Replacing Message Passing with Efficient Aggregation](https://arxiv.org/abs/2506.06784)
*Marek Černý*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Message-passing graph neural networks (MPGNNs) dominate modern graph
learning, typically prioritizing maximal expressive power. In contrast, we
introduce an \emph{efficient aggregation} mechanism, deliberately trading off
some expressivity for stronger and more structured aggregation capabilities.
Our approach allows seamless scaling between classical message-passing and
simpler methods based on colored or plain walks. We rigorously characterize the
expressive power at each intermediate step using homomorphism counts from a
hierarchy of generalized \emph{caterpillar graphs}. Based on this foundation,
we propose the \emph{Caterpillar GNN}, whose robust graph-level aggregation
enables it to successfully tackle synthetic graph-level task specifically
designed to be challenging for classical MPGNNs. Moreover, we demonstrate that,
on real-world datasets, the Caterpillar GNN achieves comparable predictive
performance while significantly reducing the number of nodes in the hidden
layers of the computational graph.

</details>


### [272] [FuncGNN: Learning Functional Semantics of Logic Circuits with Graph Neural Networks](https://arxiv.org/abs/2506.06787)
*Qiyun Zhao*

Main category: cs.LG

TL;DR: 提出FuncGNN模型，通过混合特征聚合、门感知归一化和多层集成技术，解决现代电路AIG表示中的结构异质性与全局信息丢失问题，显著提升逻辑电路建模性能。


<details>
  <summary>Details</summary>
Motivation: 随着集成电路复杂度增加，传统与-非图（AIG）在表示现代电路时面临结构异质性增强和全局逻辑信息丢失的挑战，影响电路建模效果。

Method: 1) 混合特征聚合提取多粒度拓扑特征 2) 门感知归一化适配电路门分布 3) 多层集成融合局部与全局语义信息

Result: 在信号概率预测和真值表距离预测任务中分别提升2.06%和18.71%，训练时间减少50.6%，GPU内存占用降低32.8%。

Conclusion: FuncGNN通过多粒度特征提取与自适应归一化机制，实现了对复杂电路结构的高效表征，为EDA自动化流程提供了更优的解决方案。

Abstract: As integrated circuit scale grows and design complexity rises, effective
circuit representation helps support logic synthesis, formal verification, and
other automated processes in electronic design automation. And-Inverter Graphs
(AIGs), as a compact and canonical structure, are widely adopted for
representing Boolean logic in these workflows. However, the increasing
complexity and integration density of modern circuits introduce structural
heterogeneity and global logic information loss in AIGs, posing significant
challenges to accurate circuit modeling. To address these issues, we propose
FuncGNN, which integrates hybrid feature aggregation to extract
multi-granularity topological patterns, thereby mitigating structural
heterogeneity and enhancing logic circuit representations. FuncGNN further
introduces gate-aware normalization that adapts to circuit-specific gate
distributions, improving robustness to structural heterogeneity. Finally,
FuncGNN employs multi-layer integration to merge intermediate features across
layers, effectively synthesizing local and global semantic information for
comprehensive logic representations. Experimental results on two logic-level
analysis tasks (i.e., signal probability prediction and truth-table distance
prediction) demonstrate that FuncGNN outperforms existing state-of-the-art
methods, achieving improvements of 2.06% and 18.71%, respectively, while
reducing training time by approximately 50.6% and GPU memory usage by about
32.8%.

</details>


### [273] [Is Optimal Transport Necessary for Inverse Reinforcement Learning?](https://arxiv.org/abs/2506.06793)
*Zixuan Dong,Yumi Omori,Keith Ross*

Main category: cs.LG

TL;DR: 本文挑战逆向强化学习（IRL）中依赖最优传输（OT）的必要性，提出两种无需复杂优化的简单启发式奖励方法，实验表明其性能上可匹配或超越OT方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于OT的IRL方法虽有效，但存在算法复杂、超参数敏感及需求解OT优化问题等缺陷。作者质疑OT的核心价值，探索是否可通过简单替代方案实现类似效果。

Method: 提出两种方法：1）最小距离奖励（基于最近专家状态分配奖励，忽略时序）；（2）分段匹配奖励（通过轻量级时序对齐匹配智能体状态与专家轨迹片段）。

Result: 在32个在线/离线基准测试中，两种简单方法与三种强化学习算法结合后，性能匹配或超越近期OT方法，且具备线性时间复杂度和易实现性。

Conclusion: OT在IRL中的优势可能源于基础邻近性对齐而非最优耦合，未来设计应重新评估复杂性，优先考虑轻量级替代方案。

Abstract: Inverse Reinforcement Learning (IRL) aims to recover a reward function from
expert demonstrations. Recently, Optimal Transport (OT) methods have been
successfully deployed to align trajectories and infer rewards. While OT-based
methods have shown strong empirical results, they introduce algorithmic
complexity, hyperparameter sensitivity, and require solving the OT optimization
problems. In this work, we challenge the necessity of OT in IRL by proposing
two simple, heuristic alternatives: (1) Minimum-Distance Reward, which assigns
rewards based on the nearest expert state regardless of temporal order; and (2)
Segment-Matching Reward, which incorporates lightweight temporal alignment by
matching agent states to corresponding segments in the expert trajectory. These
methods avoid optimization, exhibit linear-time complexity, and are easy to
implement. Through extensive evaluations across 32 online and offline
benchmarks with three reinforcement learning algorithms, we show that our
simple rewards match or outperform recent OT-based approaches. Our findings
suggest that the core benefits of OT may arise from basic proximity alignment
rather than its optimal coupling formulation, advocating for reevaluation of
complexity in future IRL design.

</details>


### [274] [IMPA-HGAE:Intra-Meta-Path Augmented Heterogeneous Graph Autoencoder](https://arxiv.org/abs/2506.06809)
*Di Lin,Wanjing Ren,Xuanbin Li,Rui Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为IMPA-HGAE的新框架，通过充分利用元路径上的内部节点信息来增强目标节点嵌入，解决了现有异质图自监督学习模型在元路径上信息利用不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的异质图自监督学习模型通常通过元路径将异质图转换为同质图进行训练，但这种方法仅利用了元路径两端节点的信息，而忽略了元路径上的异质节点信息。

Method: 本文提出了IMPA-HGAE框架，通过引入创新的掩码策略，增强生成式自监督学习模型在异质图数据上的表示能力，并充分利用元路径上的内部节点信息。

Result: 实验结果表明，IMPA-HGAE在异质数据集上表现出色，验证了其有效性。

Conclusion: 本文为在复杂图场景中利用元路径引导的结构语义进行鲁棒表示学习提供了新的见解，并探讨了生成式自监督学习在异质图中的未来研究方向。

Abstract: Self-supervised learning (SSL) methods have been increasingly applied to
diverse downstream tasks due to their superior generalization capabilities and
low annotation costs. However, most existing heterogeneous graph SSL models
convert heterogeneous graphs into homogeneous ones via meta-paths for training,
which only leverage information from nodes at both ends of meta-paths while
underutilizing the heterogeneous node information along the meta-paths. To
address this limitation, this paper proposes a novel framework named IMPA-HGAE
to enhance target node embeddings by fully exploiting internal node information
along meta-paths. Experimental results validate that IMPA-HGAE achieves
superior performance on heterogeneous datasets. Furthermore, this paper
introduce innovative masking strategies to strengthen the representational
capacity of generative SSL models on heterogeneous graph data. Additionally,
this paper discuss the interpretability of the proposed method and potential
future directions for generative self-supervised learning in heterogeneous
graphs. This work provides insights into leveraging meta-path-guided structural
semantics for robust representation learning in complex graph scenarios.

</details>


### [275] [Path Integral Optimiser: Global Optimisation via Neural Schrödinger-Föllmer Diffusion](https://arxiv.org/abs/2506.06815)
*Max McGuinness,Eirik Fladmark,Francisco Vargas*

Main category: cs.LG

TL;DR: 本文探讨了神经扩散过程在全局优化中的应用，重点研究了Zhang等人的路径积分采样器，提出了一种基于Boltzmann分布的优化方法，并展示了其在低维任务中的潜力，但在高维环境中表现欠佳。


<details>
  <summary>Details</summary>
Motivation: 研究动机是通过神经扩散过程改进全局优化方法，特别是在高维空间中的优化问题。

Method: 方法包括使用Boltzmann分布将优化问题转化为Schrödinger桥采样问题，应用Girsanov定理，并通过神经近似（Fourier MLP）计算积分项。

Result: 实验结果显示，该优化器在2到1,247维的优化任务中表现出色，但在15.9k参数的高维模型中探索能力有限。

Conclusion: 结论是该方法在低维优化任务中具有潜力，但在高维环境中需要进一步改进以适应复杂场景。

Abstract: We present an early investigation into the use of neural diffusion processes
for global optimisation, focusing on Zhang et al.'s Path Integral Sampler. One
can use the Boltzmann distribution to formulate optimization as solving a
Schr\"odinger bridge sampling problem, then apply Girsanov's theorem with a
simple (single-point) prior to frame it in stochastic control terms, and
compute the solution's integral terms via a neural approximation (a Fourier
MLP). We provide theoretical bounds for this optimiser, results on toy
optimisation tasks, and a summary of the stochastic theory motivating the
model. Ultimately, we found the optimiser to display promising per-step
performance at optimisation tasks between 2 and 1,247 dimensions, but struggle
to explore higher-dimensional spaces when faced with a 15.9k parameter model,
indicating a need for work on adaptation in such environments.

</details>


### [276] [Curvature Enhanced Data Augmentation for Regression](https://arxiv.org/abs/2506.06853)
*Ilya Kaufman Sirot,Omri Azencot*

Main category: cs.LG

TL;DR: 本文提出了一种基于流形学习的回归任务数据增强方法CEMS，利用数据流形的二阶表示进行高效采样和重建，显著提升了模型在分布内和分布外场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管数据增强在分类任务中取得了显著成功，但在回归问题中的应用较少。本文旨在探索一种新的流形学习方法，以生成合成数据并提升回归模型的泛化能力。

Method: 本文提出了Curvature-Enhanced Manifold Sampling (CEMS)方法，利用数据流形的二阶表示进行高效采样和重建，并提供了理论框架和实用工具。

Result: CEMS在多个数据集上进行了广泛评估，与最先进的方法相比，在分布内和分布外场景下均表现出色，且仅引入极小的计算开销。

Conclusion: CEMS方法通过利用数据流形的二阶表示，显著提升了回归任务的性能，为数据增强在回归问题中的应用提供了新的思路。

Abstract: Deep learning models with a large number of parameters, often referred to as
over-parameterized models, have achieved exceptional performance across various
tasks. Despite concerns about overfitting, these models frequently generalize
well to unseen data, thanks to effective regularization techniques, with data
augmentation being among the most widely used. While data augmentation has
shown great success in classification tasks using label-preserving
transformations, its application in regression problems has received less
attention. Recently, a novel \emph{manifold learning} approach for generating
synthetic data was proposed, utilizing a first-order approximation of the data
manifold. Building on this foundation, we present a theoretical framework and
practical tools for approximating and sampling general data manifolds.
Furthermore, we introduce the Curvature-Enhanced Manifold Sampling (CEMS)
method for regression tasks. CEMS leverages a second-order representation of
the data manifold to enable efficient sampling and reconstruction of new data
points. Extensive evaluations across multiple datasets and comparisons with
state-of-the-art methods demonstrate that CEMS delivers superior performance in
both in-distribution and out-of-distribution scenarios, while introducing only
minimal computational overhead. Code is available at
https://github.com/azencot-group/CEMS.

</details>


### [277] [High-Fidelity Scientific Simulation Surrogates via Adaptive Implicit Neural Representations](https://arxiv.org/abs/2506.06858)
*Ziwei Li,Yuhan Duan,Tianyu Xiong,Yi-Tang Chen,Wei-Lun Chao,Han-Wei Shen*

Main category: cs.LG

TL;DR: 本文提出了一种名为FA-INR的新方法，通过跨注意力机制和专家混合模型，有效提升了隐式神经表示在复杂科学模拟中的表现，同时减少了模型大小。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示（INR）在处理具有局部高频变化的复杂科学数据时表现不佳，现有方法通过引入刚性几何结构来改善，但牺牲了灵活性和模型大小。

Method: 提出FA-INR方法，利用跨注意力机制和增强的记忆库学习灵活的特征表示，并引入坐标引导的专家混合模型（MoE）来提高特征表示的专业化和效率。

Result: 在三个大规模集成模拟数据集上的实验表明，FA-INR在保持高保真度的同时显著减少了模型大小，为基于INR的代理模型建立了新的精度与紧凑性权衡前沿。

Conclusion: FA-INR通过灵活的特征表示和专家混合模型，成功解决了复杂科学模拟中的高频变化问题，同时实现了模型的高效性和紧凑性。

Abstract: Effective surrogate models are critical for accelerating scientific
simulations. Implicit neural representations (INRs) offer a compact and
continuous framework for modeling spatially structured data, but they often
struggle with complex scientific fields exhibiting localized, high-frequency
variations. Recent approaches address this by introducing additional features
along rigid geometric structures (e.g., grids), but at the cost of flexibility
and increased model size. In this paper, we propose a simple yet effective
alternative: Feature-Adaptive INR (FA-INR). FA-INR leverages cross-attention to
an augmented memory bank to learn flexible feature representations, enabling
adaptive allocation of model capacity based on data characteristics, rather
than rigid structural assumptions. To further improve scalability, we introduce
a coordinate-guided mixture of experts (MoE) that enhances the specialization
and efficiency of feature representations. Experiments on three large-scale
ensemble simulation datasets show that FA-INR achieves state-of-the-art
fidelity while significantly reducing model size, establishing a new trade-off
frontier between accuracy and compactness for INR-based surrogates.

</details>


### [278] [Differentially Private Sparse Linear Regression with Heavy-tailed Responses](https://arxiv.org/abs/2506.06861)
*Xizhi Tian,Meng Ding,Touming Tao,Zihang Xiang,Di Wang*

Main category: cs.LG

TL;DR: 本文针对高维稀疏且响应变量具有重尾分布的差分隐私线性回归问题，提出了两种新方法DP-IHT-H和DP-IHT-L，分别在一般和额外假设条件下优化了误差界限，并通过实验验证了其优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私线性回归方法主要针对常规数据分布或低维场景，缺乏对高维稀疏数据及重尾分布的系统研究。本文旨在填补这一空白，解决高维稀疏重尾数据下的隐私保护回归问题。

Method: 提出两种方法：1) DP-IHT-H结合Huber损失与私有迭代硬阈值技术；2) DP-IHT-L在响应变量满足额外假设时进一步优化误差界限。两种方法均基于(ε,δ)-DP框架。

Result: DP-IHT-H获得与尾部参数ζ相关的误差界Õ(s^½(log d/n)^{ζ/(1+ζ)} + s^{(1+2ζ)/(2+2ζ)}(log²d/nε)^{ζ/(1+ζ)})；DP-IHT-L在额外假设下实现与ζ无关的误差界Õ((s^³⁄² log d)/(nε))。实验证明方法优于常规DP算法。

Conclusion: 本文提出的DP-IHT-H和DP-IHT-L方法有效解决了高维稀疏重尾数据的差分隐私回归问题，理论分析和实验验证表明其在不同场景下均具有优越性，扩展了DP线性回归的应用边界。

Abstract: As a fundamental problem in machine learning and differential privacy (DP),
DP linear regression has been extensively studied. However, most existing
methods focus primarily on either regular data distributions or low-dimensional
cases with irregular data. To address these limitations, this paper provides a
comprehensive study of DP sparse linear regression with heavy-tailed responses
in high-dimensional settings. In the first part, we introduce the DP-IHT-H
method, which leverages the Huber loss and private iterative hard thresholding
to achieve an estimation error bound of \(
  \tilde{O}\biggl(
  s^{* \frac{1 }{2}}
  \cdot \biggl(\frac{\log d}{n}\biggr)^{\frac{\zeta}{1 + \zeta}}
  +
  s^{* \frac{1 + 2\zeta}{2 + 2\zeta}}
  \cdot \biggl(\frac{\log^2 d}{n \varepsilon}\biggr)^{\frac{\zeta}{1 + \zeta}}
  \biggr) \) under the $(\varepsilon, \delta)$-DP model, where $n$ is the
sample size, $d$ is the dimensionality, $s^*$ is the sparsity of the parameter,
and $\zeta \in (0, 1]$ characterizes the tail heaviness of the data. In the
second part, we propose DP-IHT-L, which further improves the error bound under
additional assumptions on the response and achieves \(
  \tilde{O}\Bigl(\frac{(s^*)^{3/2} \log d}{n \varepsilon}\Bigr). \) Compared to
the first result, this bound is independent of the tail parameter $\zeta$.
Finally, through experiments on synthetic and real-world datasets, we
demonstrate that our methods outperform standard DP algorithms designed for
``regular'' data.

</details>


### [279] [SAFE: Finding Sparse and Flat Minima to Improve Pruning](https://arxiv.org/abs/2506.06866)
*Dongyeop Lee,Kwanhee Lee,Jinseok Chung,Namhoon Lee*

Main category: cs.LG

TL;DR: 本文提出了一种新的神经网络剪枝方法SAFE及其扩展SAFE$^+$，通过同时追求稀疏性和平坦性来提升模型性能，并在图像分类和语言建模任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的神经网络剪枝方法往往会导致性能下降，尽管已有许多进展，但恢复原始性能仍具挑战性。本文受鲁棒优化研究的启发，旨在通过寻找同时稀疏且平坦的子网络来解决这一问题。

Method: 本文将剪枝问题形式化为一个稀疏性约束的优化问题，并将平坦性作为目标进行优化。通过增强拉格朗日对偶方法显式求解，并进一步提出广义投影操作，从而开发出SAFE及其扩展SAFE$^+$。

Result: 在标准图像分类和语言建模任务上的广泛评估表明，SAFE能够持续生成具有更好泛化性能的稀疏网络，与现有基线方法相比具有竞争力。此外，SAFE对噪声数据表现出较强的鲁棒性，适合实际应用场景。

Conclusion: SAFE及其扩展SAFE$^+$通过同时优化稀疏性和平坦性，有效提升了剪枝后神经网络的性能，并在实际应用中表现出良好的鲁棒性。

Abstract: Sparsifying neural networks often suffers from seemingly inevitable
performance degradation, and it remains challenging to restore the original
performance despite much recent progress. Motivated by recent studies in robust
optimization, we aim to tackle this problem by finding subnetworks that are
both sparse and flat at the same time. Specifically, we formulate pruning as a
sparsity-constrained optimization problem where flatness is encouraged as an
objective. We solve it explicitly via an augmented Lagrange dual approach and
extend it further by proposing a generalized projection operation, resulting in
novel pruning methods called SAFE and its extension, SAFE$^+$. Extensive
evaluations on standard image classification and language modeling tasks reveal
that SAFE consistently yields sparse networks with improved generalization
performance, which compares competitively to well-established baselines. In
addition, SAFE demonstrates resilience to noisy data, making it well-suited for
real-world conditions.

</details>


### [280] [Log-Sum-Exponential Estimator for Off-Policy Evaluation and Learning](https://arxiv.org/abs/2506.06873)
*Armin Behnamnia,Gholamali Aminian,Alireza Aghaei,Chengchun Shi,Vincent Y. F. Tan,Hamid R. Rabiee*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Off-policy learning and evaluation leverage logged bandit feedback datasets,
which contain context, action, propensity score, and feedback for each data
point. These scenarios face significant challenges due to high variance and
poor performance with low-quality propensity scores and heavy-tailed reward
distributions. We address these issues by introducing a novel estimator based
on the log-sum-exponential (LSE) operator, which outperforms traditional
inverse propensity score estimators. Our LSE estimator demonstrates variance
reduction and robustness under heavy-tailed conditions. For off-policy
evaluation, we derive upper bounds on the estimator's bias and variance. In the
off-policy learning scenario, we establish bounds on the regret -- the
performance gap between our LSE estimator and the optimal policy -- assuming
bounded $(1+\epsilon)$-th moment of weighted reward. Notably, we achieve a
convergence rate of $O(n^{-\epsilon/(1+ \epsilon)})$ for the regret bounds,
where $\epsilon \in [0,1]$ and $n$ is the size of logged bandit feedback
dataset. Theoretical analysis is complemented by comprehensive empirical
evaluations in both off-policy learning and evaluation scenarios, confirming
the practical advantages of our approach. The code for our estimator is
available at the following link:
https://github.com/armin-behnamnia/lse-offpolicy-learning.

</details>


### [281] [FREE: Fast and Robust Vision Language Models with Early Exits](https://arxiv.org/abs/2506.06884)
*Divya Jyoti Bajpai,Manjesh Kumar Hanawal*

Main category: cs.LG

TL;DR: 本文提出了一种基于对抗训练的早期退出策略（FREE），以加速视觉语言模型（VLM）的推理过程，在保持性能的同时减少延迟。该方法通过生成对抗网络框架训练中间层特征，实现输入自适应推理，实验显示推理速度提升1.51倍以上。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在任务性能上显著提升，但其庞大参数量导致推理延迟高，难以实际部署。传统早期退出策略在有限标注数据下训练分类器存在挑战，需解决模型效率与性能的权衡问题。

Method: 提出FREE方法：在GAN框架中，每个退出点包含Transformer层和分类器。通过对抗训练使中间层特征逼近最终层特征，同时利用分类器作为判别器，实现输入自适应推理，动态加速且减少性能损失。

Result: 实验表明，FREE在保持性能可比性的前提下，推理速度提升超1.51倍，并缓解了过思考（overthinking）和中间层危机（mid-crisis）现象，同时增强了模型鲁棒性。

Conclusion: FREE通过对抗训练优化早期退出策略，有效平衡了视觉语言模型的推理速度与精度，为实际部署提供了一种可行解决方案，代码已开源。

Abstract: In recent years, Vision-Language Models (VLMs) have shown remarkable
performance improvements in Vision-Language tasks. However, their large size
poses challenges for real-world applications where inference latency is a
concern. To tackle this issue, we propose employing Early Exit (EE) strategies
in VLMs. However, training exit classifiers in VLMs is challenging,
particularly with limited labeled training data. To address this, we introduce
FREE, an adversarial training approach within a GAN-based framework. Here, each
exit consists of a transformer layer and a classifier. The transformer layer is
adversarially trained to produce feature representations similar to the final
layer, while a feature classifier serves as the discriminator. Our method
focuses on performing input-adaptive inference that increases inference speed
with minimal drop in performance. Experimental results demonstrate the
effectiveness of our approach in enhancing accuracy and model robustness by
mitigating overthinking and the phenomenon of mid-crisis that we highlight. We
experimentally validate that our method speeds up the inference process by more
than 1.51x while retaining comparable performance. The source code is available
at https://github.com/Div290/FREE.

</details>


### [282] [Can In-Context Reinforcement Learning Recover From Reward Poisoning Attacks?](https://arxiv.org/abs/2506.06891)
*Paulius Sasnauskas,Yiğit Yalın,Goran Radanović*

Main category: cs.LG

TL;DR: 本文研究了上下文强化学习（ICRL）在对抗性环境中的鲁棒性，提出了一种新的对抗训练框架AT-DPT，显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 针对决策预训练Transformer（DPT）在奖励中毒攻击下的脆弱性，本文旨在提高其鲁棒性。

Method: 提出了一种对抗训练框架AT-DPT，同时训练攻击者和DPT模型，攻击者通过污染环境奖励来最小化DPT的真实奖励，DPT则从污染数据中推断最优动作。

Result: 在强盗算法和MDP设置中，AT-DPT显著优于现有的鲁棒基线方法，且在自适应攻击者下表现相似。

Conclusion: AT-DPT在复杂环境中展现出良好的鲁棒性，验证了其在对抗性环境中的有效性。

Abstract: We study the corruption-robustness of in-context reinforcement learning
(ICRL), focusing on the Decision-Pretrained Transformer (DPT, Lee et al.,
2023). To address the challenge of reward poisoning attacks targeting the DPT,
we propose a novel adversarial training framework, called Adversarially Trained
Decision-Pretrained Transformer (AT-DPT). Our method simultaneously trains an
attacker to minimize the true reward of the DPT by poisoning environment
rewards, and a DPT model to infer optimal actions from the poisoned data. We
evaluate the effectiveness of our approach against standard bandit algorithms,
including robust baselines designed to handle reward contamination. Our results
show that the proposed method significantly outperforms these baselines in
bandit settings, under a learned attacker. We additionally evaluate AT-DPT on
an adaptive attacker, and observe similar results. Furthermore, we extend our
evaluation to the MDP setting, confirming that the robustness observed in
bandit scenarios generalizes to more complex environments.

</details>


### [283] [Scalable Gaussian Processes with Latent Kronecker Structure](https://arxiv.org/abs/2506.06895)
*Jihao Andreas Lin,Sebastian Ament,Maximilian Balandat,David Eriksson,José Miguel Hernández-Lobato,Eytan Bakshy*

Main category: cs.LG

TL;DR: 本文提出了一种利用潜在Kronecker结构的方法，通过将观测值的核矩阵表示为潜在Kronecker积的投影，解决了高斯过程在大数据集上的计算扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 高斯过程在处理大规模数据集时面临计算扩展性限制，现有的Kronecker积方法通常需要近似或不现实的假设，尤其是在数据缺失时，Kronecker结构会失效。

Method: 本文提出了一种新方法，通过将观测值的核矩阵表示为潜在Kronecker积的投影，结合迭代线性系统求解器和路径条件，实现了精确高斯过程的推断。

Result: 该方法在包含多达五百万个样本的真实数据集上，表现优于现有的稀疏和变分高斯过程方法，适用于机器人、自动化机器学习和气候应用等领域。

Conclusion: 本文提出的方法显著减少了计算资源需求，能够有效处理大规模数据集，为高斯过程在实际应用中的扩展提供了新的解决方案。

Abstract: Applying Gaussian processes (GPs) to very large datasets remains a challenge
due to limited computational scalability. Matrix structures, such as the
Kronecker product, can accelerate operations significantly, but their
application commonly entails approximations or unrealistic assumptions. In
particular, the most common path to creating a Kronecker-structured kernel
matrix is by evaluating a product kernel on gridded inputs that can be
expressed as a Cartesian product. However, this structure is lost if any
observation is missing, breaking the Cartesian product structure, which
frequently occurs in real-world data such as time series. To address this
limitation, we propose leveraging latent Kronecker structure, by expressing the
kernel matrix of observed values as the projection of a latent Kronecker
product. In combination with iterative linear system solvers and pathwise
conditioning, our method facilitates inference of exact GPs while requiring
substantially fewer computational resources than standard iterative methods. We
demonstrate that our method outperforms state-of-the-art sparse and variational
GPs on real-world datasets with up to five million examples, including
robotics, automated machine learning, and climate applications.

</details>


### [284] [Uncertainty Estimation on Graphs with Structure Informed Stochastic Partial Differential Equations](https://arxiv.org/abs/2506.06907)
*Fred Xu,Thomas Markovich*

Main category: cs.LG

TL;DR: 本文提出一种基于随机偏微分方程(SPDE)与高斯过程的新型图神经网络消息传递机制，通过引入时空噪声和协方差核平滑控制，有效提升图数据在分布偏移下的不确定性估计能力。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络在不确定性估计中难以同时处理图结构随机性和标签分布随机性，尤其在分布偏移场景下表现受限。需要同时建模时空维度不确定性并控制核函数平滑度以适应不同标签信息量的图数据。

Method: 将Matern高斯过程驱动的SPDE演化过程与GNN消息传递机制进行类比，设计融合时空噪声的消息传递方案。通过高斯过程方法显式控制协方差核平滑度，实现空间-时间联合不确定性建模。

Result: 在包含不同标签信息量的图数据集上进行OOD检测实验，结果显示该方法在低/高标签信息场景下均优于现有方法，验证了模型对协方差核平滑度控制的有效性。

Conclusion: 通过SPDE与高斯过程的数学框架建立的新型消息传递机制，实现了对图数据不确定性的精细化建模，为分布偏移场景下的图神经网络应用提供了理论保障。

Abstract: Graph Neural Networks have achieved impressive results across diverse network
modeling tasks, but accurately estimating uncertainty on graphs remains
difficult, especially under distributional shifts. Unlike traditional
uncertainty estimation, graph-based uncertainty must account for randomness
arising from both the graph's structure and its label distribution, which adds
complexity. In this paper, making an analogy between the evolution of a
stochastic partial differential equation (SPDE) driven by Matern Gaussian
Process and message passing using GNN layers, we present a principled way to
design a novel message passing scheme that incorporates spatial-temporal noises
motivated by the Gaussian Process approach to SPDE. Our method simultaneously
captures uncertainty across space and time and allows explicit control over the
covariance kernel smoothness, thereby enhancing uncertainty estimates on graphs
with both low and high label informativeness. Our extensive experiments on
Out-of-Distribution (OOD) detection on graph datasets with varying label
informativeness demonstrate the soundness and superiority of our model to
existing approaches.

</details>


### [285] [Graph-Based Physics-Guided Urban PM2.5 Air Quality Imputation with Constrained Monitoring Data](https://arxiv.org/abs/2506.06917)
*Shangjie Du,Hui Wei,Dong Yoon Lee,Zhizhang Hu,Shijia Pan*

Main category: cs.LG

TL;DR: GraPhy是一种基于图神经网络的物理引导学习框架，用于在监测数据有限的城区进行高分辨率空气质量建模，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 在监测网络稀疏的社会经济弱势地区，空气质量建模的准确性和分辨率受到限制，而细粒度的空气质量监测信息对减少公众暴露于污染物至关重要。

Method: 提出了一种名为GraPhy的物理引导图神经网络架构，专门设计用于低分辨率监测数据的层和边特征。

Result: 在加利福尼亚州圣华金谷的实验表明，GraPhy在均方误差、平均绝对误差和R平方值上表现最佳，性能提升了9%-56%。

Conclusion: GraPhy在不同空间异质性水平下均优于基线模型，证明了其模型设计的有效性。

Abstract: This work introduces GraPhy, a graph-based, physics-guided learning framework
for high-resolution and accurate air quality modeling in urban areas with
limited monitoring data. Fine-grained air quality monitoring information is
essential for reducing public exposure to pollutants. However, monitoring
networks are often sparse in socioeconomically disadvantaged regions, limiting
the accuracy and resolution of air quality modeling. To address this, we
propose a physics-guided graph neural network architecture called GraPhy with
layers and edge features designed specifically for low-resolution monitoring
data. Experiments using data from California's socioeconomically disadvantaged
San Joaquin Valley show that GraPhy achieves the overall best performance
evaluated by mean squared error (MSE), mean absolute error (MAE), and R-square
value (R2), improving the performance by 9%-56% compared to various baseline
models. Moreover, GraPhy consistently outperforms baselines across different
spatial heterogeneity levels, demonstrating the effectiveness of our model
design.

</details>


### [286] [Basis Transformers for Multi-Task Tabular Regression](https://arxiv.org/abs/2506.06926)
*Wei Min Loh,Jiaqi Shang,Pascal Poupart*

Main category: cs.LG

TL;DR: 本文提出了一种名为basis transformers的新架构，旨在解决表格数据处理中的部分信息、噪声和异构结构等挑战，并在多任务表格回归基准测试中取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有技术在同时处理表格数据的文本信息、可变列数和无元数据等关键方面存在困难，因此需要一种新的架构来解决这些挑战。

Method: 本文提出了basis transformers架构，专门设计用于处理表格数据的固有不变性，包括层次结构和数值表示。

Result: 在OpenML-CTR23基准测试的34个任务中，该模型的$R^2$得分中位数提高了0.338，且标准偏差最小，参数量比最佳基线少了五倍，甚至优于预训练的大型语言模型基线。

Conclusion: basis transformers架构在处理表格数据时表现出色，显著提升了性能，且具有更高的效率和更好的泛化能力。

Abstract: Dealing with tabular data is challenging due to partial information, noise,
and heterogeneous structure. Existing techniques often struggle to
simultaneously address key aspects of tabular data such as textual information,
a variable number of columns, and unseen data without metadata besides column
names. We propose a novel architecture, \textit{basis transformers},
specifically designed to tackle these challenges while respecting inherent
invariances in tabular data, including hierarchical structure and the
representation of numeric values. We evaluate our design on a multi-task
tabular regression benchmark, achieving an improvement of 0.338 in the median
$R^2$ score and the lowest standard deviation across 34 tasks from the
OpenML-CTR23 benchmark. Furthermore, our model has five times fewer parameters
than the best-performing baseline and surpasses pretrained large language model
baselines -- even when initialized from randomized weights.

</details>


### [287] [Rewriting the Budget: A General Framework for Black-Box Attacks Under Cost Asymmetry](https://arxiv.org/abs/2506.06933)
*Mahdi Salmani,Alireza Abdollahpoorrostam,Seyed-Mohsen Moosavi-Dezfooli*

Main category: cs.LG

TL;DR: 本文提出了一种针对非对称查询成本的黑盒对抗攻击框架，通过改进搜索策略和梯度估计方法，在降低总攻击成本和扰动幅度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统对抗攻击方法假设所有查询成本相同，但实际场景中不同输出类别可能引发差异化的审查或惩罚机制（如内容审核系统），导致查询成本不对称。现有方法在此类场景下效果不足，需开发更高效的非对称攻击算法。

Method: 提出非对称搜索（AS）减少对高成本查询的依赖，改进梯度估计过程（AGREST）以偏向低成本查询采样。通过平衡不同查询类型设计最小化总攻击成本的算法，兼容现有黑盒攻击框架。

Result: 在标准图像分类基准测试中，该方法在不同成本机制下均实现更低总查询成本和更小扰动幅度，部分场景改进幅度达40%。理论分析与实验验证了有效性。

Conclusion: 非对称黑盒攻击框架通过核心组件的针对性优化，显著提升对抗攻击效率。其模块化设计便于集成到现有方法，为实际成本敏感场景提供新解决方案。

Abstract: Traditional decision-based black-box adversarial attacks on image classifiers
aim to generate adversarial examples by slightly modifying input images while
keeping the number of queries low, where each query involves sending an input
to the model and observing its output. Most existing methods assume that all
queries have equal cost. However, in practice, queries may incur asymmetric
costs; for example, in content moderation systems, certain output classes may
trigger additional review, enforcement, or penalties, making them more costly
than others. While prior work has considered such asymmetric cost settings,
effective algorithms for this scenario remain underdeveloped. In this paper, we
propose a general framework for decision-based attacks under asymmetric query
costs, which we refer to as asymmetric black-box attacks. We modify two core
components of existing attacks: the search strategy and the gradient estimation
process. Specifically, we propose Asymmetric Search (AS), a more conservative
variant of binary search that reduces reliance on high-cost queries, and
Asymmetric Gradient Estimation (AGREST), which shifts the sampling distribution
to favor low-cost queries. We design efficient algorithms that minimize total
attack cost by balancing different query types, in contrast to earlier methods
such as stealthy attacks that focus only on limiting expensive (high-cost)
queries. Our method can be integrated into a range of existing black-box
attacks with minimal changes. We perform both theoretical analysis and
empirical evaluation on standard image classification benchmarks. Across
various cost regimes, our method consistently achieves lower total query cost
and smaller perturbations than existing approaches, with improvements of up to
40% in some settings.

</details>


### [288] [Understanding Sharpness Dynamics in NN Training with a Minimalist Example: The Effects of Dataset Difficulty, Depth, Stochasticity, and More](https://arxiv.org/abs/2506.06940)
*Geonhui Yoo,Minhak Song,Chulhee Yun*

Main category: cs.LG

TL;DR: 本文通过构建一个极简的深度线性网络模型，揭示了深度神经网络训练中尖锐度逐渐增加的机制，并分析了数据集属性、网络深度、优化器随机性及步长对渐进尖锐化的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管在深度神经网络训练中普遍观察到尖锐度逐渐增加（渐进尖锐化）的现象，但其内在机制尚不明确。本文旨在通过简化模型深入理解这一动态过程。

Method: 使用每层仅含单个神经元的深度线性网络作为极简模型，结合理论分析与实证研究，探讨训练数据、网络深度、优化器特性（如随机性）及步长对尖锐度动态的影响。

Result: 极简模型成功复现了实际训练中的尖锐度动态变化，理论分析表明数据集特征、网络深度、优化器随机性和步长均显著影响渐进尖锐化程度，且实证结果验证了理论在真实场景中的适用性。

Conclusion: 研究通过极简模型为理解神经网络训练中的尖锐度动态提供了新视角，强调了网络深度、训练数据与优化器之间的复杂关系，为未来优化算法设计提供了理论基础。

Abstract: When training deep neural networks with gradient descent, sharpness often
increases -- a phenomenon known as progressive sharpening -- before saturating
at the edge of stability. Although commonly observed in practice, the
underlying mechanisms behind progressive sharpening remain poorly understood.
In this work, we study this phenomenon using a minimalist model: a deep linear
network with a single neuron per layer. We show that this simple model
effectively captures the sharpness dynamics observed in recent empirical
studies, offering a simple testbed to better understand neural network
training. Moreover, we theoretically analyze how dataset properties, network
depth, stochasticity of optimizers, and step size affect the degree of
progressive sharpening in the minimalist model. We then empirically demonstrate
how these theoretical insights extend to practical scenarios. This study offers
a deeper understanding of sharpness dynamics in neural network training,
highlighting the interplay between depth, training data, and optimizers.

</details>


### [289] [Safety-Aware Reinforcement Learning for Control via Risk-Sensitive Action-Value Iteration and Quantile Regression](https://arxiv.org/abs/2506.06954)
*Clinton Enwerem,Aniruddh G. Puranic,John S. Baras,Calin Belta*

Main category: cs.LG

TL;DR: 本文提出了一种基于分位数和条件风险价值（CVaR）的风险正则化算法，用于解决强化学习中的高方差环境下的过估计偏差和安全约束问题。


<details>
  <summary>Details</summary>
Motivation: 主流的近似动作值迭代强化学习算法在高方差随机环境中存在过估计偏差，导致策略次优。现有方法在确保策略满足安全约束方面存在挑战，通常需要复杂的神经网络架构或手动权衡。

Method: 提出了一种风险正则化的分位数动作值迭代算法，集成了条件风险价值（CVaR）来确保安全，无需复杂架构。并提供了风险敏感分布贝尔曼算子在Wasserstein空间中的收缩性质的理论保证。

Result: 在动态避障任务中的移动机器人模拟表明，该方法相比风险中性方法，实现了更高的目标成功率、更少的碰撞以及更好的安全性能权衡。

Conclusion: 本文提出的算法有效解决了强化学习中的过估计偏差和安全约束问题，具有更好的性能和安全性。

Abstract: Mainstream approximate action-value iteration reinforcement learning (RL)
algorithms suffer from overestimation bias, leading to suboptimal policies in
high-variance stochastic environments. Quantile-based action-value iteration
methods reduce this bias by learning a distribution of the expected cost-to-go
using quantile regression. However, ensuring that the learned policy satisfies
safety constraints remains a challenge when these constraints are not
explicitly integrated into the RL framework. Existing methods often require
complex neural architectures or manual tradeoffs due to combined cost
functions. To address this, we propose a risk-regularized quantile-based
algorithm integrating Conditional Value-at-Risk (CVaR) to enforce safety
without complex architectures. We also provide theoretical guarantees on the
contraction properties of the risk-sensitive distributional Bellman operator in
Wasserstein space, ensuring convergence to a unique cost distribution.
Simulations of a mobile robot in a dynamic reach-avoid task show that our
approach leads to more goal successes, fewer collisions, and better
safety-performance trade-offs compared to risk-neutral methods.

</details>


### [290] [UdonCare: Hierarchy Pruning for Unseen Domain Discovery in Predictive Healthcare](https://arxiv.org/abs/2506.06977)
*Pengfei Hu,Xiaoxue Han,Fei Wang,Yue Ning*

Main category: cs.LG

TL;DR: 本文提出了一种基于层次医学本体的领域泛化框架UdonCare，通过迭代修剪细粒度领域并应用Siamese推理机制，有效提升了临床预测模型在数据分布变化下的性能。


<details>
  <summary>Details</summary>
Motivation: 在临床预测中，患者群体的数据分布变化会显著降低模型性能。传统的领域泛化方法在现实医疗环境中面临两大挑战：缺乏患者特定的领域标签，以及纯数据驱动方法忽视了关键的临床知识。

Method: 本文利用ICD-9-CM等层次医学本体将疾病分组为更高层次的类别，并发现更灵活的潜在领域。提出的UdonCare框架通过迭代修剪细粒度领域、编码这些领域，并应用Siamese推理机制来分离领域相关信号与患者特征。

Result: 在MIMIC-III和MIMIC-IV临床数据集上的实验结果表明，UdonCare在存在显著领域差距时，相比其他领域泛化基线模型表现出更高的性能。

Conclusion: UdonCare框架展示了医学知识在提升实际医疗应用领域泛化能力方面的潜力，为临床预测模型的鲁棒性提供了新的解决方案。

Abstract: Domain generalization has become a critical challenge in clinical prediction,
where patient cohorts often exhibit shifting data distributions that degrade
model performance. Typical domain generalization approaches struggle in
real-world healthcare settings for two main reasons: (1) patient-specific
domain labels are typically unavailable, making domain discovery especially
difficult; (2) purely data-driven approaches overlook key clinical insights,
leading to a gap in medical knowledge integration. To address these problems,
we leverage hierarchical medical ontologies like the ICD-9-CM hierarchy to
group diseases into higher-level categories and discover more flexible latent
domains. In this paper, we introduce UdonCare, a hierarchy-guided framework
that iteratively prunes fine-grained domains, encodes these refined domains,
and applies a Siamese-type inference mechanism to separate domain-related
signals from patient-level features. Experimental results on clinical datasets
(MIMIC-III and MIMIC-IV) show that the proposed model achieves higher
performance compared to other domain generalization baselines when substantial
domain gaps presents, highlighting the untapped potential of medical knowledge
for enhancing domain generalization in practical healthcare applications.

</details>


### [291] [Near Optimal Non-asymptotic Sample Complexity of 1-Identification](https://arxiv.org/abs/2506.06978)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 本文针对多臂老虎机中的1-识别问题，提出SEE算法，在非渐近分析下实现接近最优的样本复杂度，填补了现有文献的空白。


<details>
  <summary>Details</summary>
Motivation: 现有文献中关于1-识别问题的非渐近样本复杂度分析尚未明确，Degenne & Koolen (2019) 虽建立了渐近紧样本复杂度，但非渐近性质仍需探索。

Method: 设计了Sequential-Exploration-Exploitation (SEE) 算法，结合顺序探索与利用策略，从非渐近角度进行理论分析。

Result: 算法在样本复杂度上实现了上下界近乎匹配（差距为多项式对数因子），数值实验验证了其优于现有基准方法的有效性。

Conclusion: SEE算法在非渐近场景下解决了1-识别问题，理论及实验均表明其接近最优性，为多臂老虎机纯探索领域提供了新方法。

Abstract: Motivated by an open direction in existing literature, we study the
1-identification problem, a fundamental multi-armed bandit formulation on pure
exploration. The goal is to determine whether there exists an arm whose mean
reward is at least a known threshold $\mu_0$, or to output None if it believes
such an arm does not exist. The agent needs to guarantee its output is correct
with probability at least $1-\delta$. Degenne & Koolen 2019 has established the
asymptotically tight sample complexity for the 1-identification problem, but
they commented that the non-asymptotic analysis remains unclear. We design a
new algorithm Sequential-Exploration-Exploitation (SEE), and conduct
theoretical analysis from the non-asymptotic perspective. Novel to the
literature, we achieve near optimality, in the sense of matching upper and
lower bounds on the pulling complexity. The gap between the upper and lower
bounds is up to a polynomial logarithmic factor. The numerical result also
indicates the effectiveness of our algorithm, compared to existing benchmarks.

</details>


### [292] [MoXGATE: Modality-aware cross-attention for multi-omic gastrointestinal cancer sub-type classification](https://arxiv.org/abs/2506.06980)
*Sajib Acharjee Dip,Uddip Acharjee Shuvo,Dipanwita Mallick,Abrar Rahman Abir,Liqing Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为MoXGATE的深度学习框架，通过跨注意力和可学习的模态权重，有效整合多组学数据，提升癌症亚型分类的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 癌症亚型分类对个性化治疗和预后评估至关重要，但由于基因组、表观基因组和转录组特征的异质性，多组学数据的有效整合仍具挑战性。

Method: 提出MoXGATE框架，利用跨注意力和模态加权融合，结合焦点损失函数处理数据不平衡问题，并在TCGA的GIAC和BRCA数据集上进行实验验证。

Result: MoXGATE在癌症亚型分类中表现优异，达到95%的准确率，且在不同癌症类型中具有良好泛化能力。

Conclusion: MoXGATE是一种有前景的多组学癌症亚型分类方法，具有改进的性能和生物学泛化能力。

Abstract: Cancer subtype classification is crucial for personalized treatment and
prognostic assessment. However, effectively integrating multi-omic data remains
challenging due to the heterogeneous nature of genomic, epigenomic, and
transcriptomic features. In this work, we propose Modality-Aware
Cross-Attention MoXGATE, a novel deep-learning framework that leverages
cross-attention and learnable modality weights to enhance feature fusion across
multiple omics sources. Our approach effectively captures inter-modality
dependencies, ensuring robust and interpretable integration. Through
experiments on Gastrointestinal Adenocarcinoma (GIAC) and Breast Cancer (BRCA)
datasets from TCGA, we demonstrate that MoXGATE outperforms existing methods,
achieving 95\% classification accuracy. Ablation studies validate the
effectiveness of cross-attention over simple concatenation and highlight the
importance of different omics modalities. Moreover, our model generalizes well
to unseen cancer types e.g., breast cancer, underscoring its adaptability. Key
contributions include (1) a cross-attention-based multi-omic integration
framework, (2) modality-weighted fusion for enhanced interpretability, (3)
application of focal loss to mitigate data imbalance, and (4) validation across
multiple cancer subtypes. Our results indicate that MoXGATE is a promising
approach for multi-omic cancer subtype classification, offering improved
performance and biological generalizability.

</details>


### [293] [Certified Unlearning for Neural Networks](https://arxiv.org/abs/2506.06985)
*Anastasia Koloskova,Youssef Allouah,Animesh Jha,Rachid Guerraoui,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 本文提出一种基于噪声微调保留数据的认证机器去学习方法，通过随机后处理的隐私放大实现可证明的去学习保证，无需损失函数假设，在理论和实践中均优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有机器去学习方法依赖限制性假设或缺乏形式化保证，无法满足隐私保护需求（如'被遗忘权'）和法规要求。

Method: 利用去学习与随机后处理隐私放大的关联，在保留数据上采用噪声微调机制，构建无需损失函数假设的普适性认证框架。

Result: 理论分析揭示效率与精度的权衡关系，实证结果表明该方法不仅满足形式化去学习保证，且实际效果显著超越基线方法。

Conclusion: 所提方法突破了传统假设限制，在保证理论严谨性的同时具备广泛适用性和实践有效性，为机器去学习提供了新范式。

Abstract: We address the problem of machine unlearning, where the goal is to remove the
influence of specific training data from a model upon request, motivated by
privacy concerns and regulatory requirements such as the "right to be
forgotten." Unfortunately, existing methods rely on restrictive assumptions or
lack formal guarantees. To this end, we propose a novel method for certified
machine unlearning, leveraging the connection between unlearning and privacy
amplification by stochastic post-processing. Our method uses noisy fine-tuning
on the retain data, i.e., data that does not need to be removed, to ensure
provable unlearning guarantees. This approach requires no assumptions about the
underlying loss function, making it broadly applicable across diverse settings.
We analyze the theoretical trade-offs in efficiency and accuracy and
demonstrate empirically that our method not only achieves formal unlearning
guarantees but also performs effectively in practice, outperforming existing
baselines. Our code is available at
https://github.com/stair-lab/certified-unlearningneural-networks-icml-2025

</details>


### [294] [Fully Explainable Classification Models Using Hyperblocks](https://arxiv.org/abs/2506.06986)
*Austin Snyder,Ryan Gallagher,Boris Kovalerchuk*

Main category: cs.LG

TL;DR: 本文提出一种基于Hyperblocks的改进方法，通过简化算法降低模型复杂度并保持分类性能，引入可解释的k-NN回退机制实现全数据覆盖，在WBC(9维)和MNIST(784维)数据集上验证了高维大数据场景下可解释模型的竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有Hyperblocks模型存在可解释性不足、训练时间长、复杂度高的问题，需让领域专家无需机器学习背景即可直观理解模型决策逻辑，同时保持高维大数据的分类精度。

Method: 1) 提出Hyperblocks简化算法：通过属性/块冗余消除、重叠分析及创建分离单元降低复杂度
2) 引入k-NN可解释回退机制处理未覆盖样本
3) 保持模型透明度的同时实现全数据覆盖

Result: WBC数据集保持强预测性能且复杂度显著降低；MNIST(784维)通过调参持续改进，模型参数量减少90%仍保持分类能力，证明可解释模型处理高维数据的可行性。

Conclusion: 该方法为需要可信度与透明度的领域提供了黑箱模型的可行替代方案，证明可解释模型在大规模高维数据场景中可兼顾性能与透明度。

Abstract: Building on existing work with Hyperblocks, which classify data using minimum
and maximum bounds for each attribute, we focus on enhancing interpretability,
decreasing training time, and reducing model complexity without sacrificing
accuracy. This system allows subject matter experts (SMEs) to directly inspect
and understand the model's decision logic without requiring extensive machine
learning expertise. To reduce Hyperblock complexity while retaining
performance, we introduce a suite of algorithms for Hyperblock simplification.
These include removing redundant attributes, removing redundant blocks through
overlap analysis, and creating disjunctive units. These methods eliminate
unnecessary parameters, dramatically reducing model size without harming
classification power. We increase robustness by introducing an interpretable
fallback mechanism using k-Nearest Neighbor (k-NN) classifiers for points not
covered by any block, ensuring complete data coverage while preserving model
transparency. Our results demonstrate that interpretable models can scale to
high-dimensional, large-volume datasets while maintaining competitive accuracy.
On benchmark datasets such as WBC (9-D), we achieve strong predictive
performance with significantly reduced complexity. On MNIST (784-D), our method
continues to improve through tuning and simplification, showing promise as a
transparent alternative to black-box models in domains where trust, clarity,
and control are crucial.

</details>


### [295] [Modified K-means Algorithm with Local Optimality Guarantees](https://arxiv.org/abs/2506.06990)
*Mingyi Li,Michael R. Metel,Akiko Takeda*

Main category: cs.LG

TL;DR: 本文分析了K-means算法的局部最优性，提出了确保局部最优性的简单修改方法，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管K-means算法在机器学习中被广泛研究，但其局部最优性保证仍缺乏严格分析。本文旨在填补这一空白。

Method: 本文首先提出了K-means算法收敛到局部最优解的条件，并基于此提出了确保连续和离散意义上局部最优性的简单修改方法，计算复杂度与原始K-means算法相同。

Result: 数值实验表明，K-means算法在实践中并不总能找到局部最优解，而本文提出的方法提供了改进的局部最优解，并减少了聚类损失。

Conclusion: 本文提出的方法在保持计算复杂度的同时，显著提高了K-means算法的局部最优性，为聚类任务提供了更可靠的解决方案。

Abstract: The K-means algorithm is one of the most widely studied clustering algorithms
in machine learning. While extensive research has focused on its ability to
achieve a globally optimal solution, there still lacks a rigorous analysis of
its local optimality guarantees. In this paper, we first present conditions
under which the K-means algorithm converges to a locally optimal solution.
Based on this, we propose simple modifications to the K-means algorithm which
ensure local optimality in both the continuous and discrete sense, with the
same computational complexity as the original K-means algorithm. As the
dissimilarity measure, we consider a general Bregman divergence, which is an
extension of the squared Euclidean distance often used in the K-means
algorithm. Numerical experiments confirm that the K-means algorithm does not
always find a locally optimal solution in practice, while our proposed methods
provide improved locally optimal solutions with reduced clustering loss. Our
code is available at https://github.com/lmingyi/LO-K-means.

</details>


### [296] [Towards Physics-informed Diffusion for Anomaly Detection in Trajectories](https://arxiv.org/abs/2506.06999)
*Arun Sharma,Mingzhou Yang,Majid Farhadloo,Subhankar Ghosh,Bharat Jayaprakash,Shashi Shekhar*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理信息的扩散模型，用于检测异常轨迹，特别是GPS欺骗，以提高预测精度并降低误报率。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在通过检测异常轨迹来遏制国际水域中的非法活动，如未经授权的捕鱼和非法石油转移。由于AI生成的深度伪造技术（如添加噪声、伪造轨迹）的进步以及缺乏足够的标记样本进行验证，这一问题具有挑战性。

Method: 本文提出了一种物理信息扩散模型，该模型结合了运动学约束，以识别不符合物理定律的轨迹。

Result: 在海上和城市领域的真实数据集上的实验结果表明，该框架在异常检测和轨迹生成方法上具有更高的预测精度和更低的估计错误率。

Conclusion: 本文提出的物理信息扩散模型在检测异常轨迹方面表现出色，特别是在处理GPS欺骗问题上，具有较高的预测精度和较低的误报率。

Abstract: Given trajectory data, a domain-specific study area, and a user-defined
threshold, we aim to find anomalous trajectories indicative of possible GPS
spoofing (e.g., fake trajectory). The problem is societally important to curb
illegal activities in international waters, such as unauthorized fishing and
illicit oil transfers. The problem is challenging due to advances in AI
generated in deep fakes generation (e.g., additive noise, fake trajectories)
and lack of adequate amount of labeled samples for ground-truth verification.
Recent literature shows promising results for anomalous trajectory detection
using generative models despite data sparsity. However, they do not consider
fine-scale spatiotemporal dependencies and prior physical knowledge, resulting
in higher false-positive rates. To address these limitations, we propose a
physics-informed diffusion model that integrates kinematic constraints to
identify trajectories that do not adhere to physical laws. Experimental results
on real-world datasets in the maritime and urban domains show that the proposed
framework results in higher prediction accuracy and lower estimation error rate
for anomaly detection and trajectory generation methods, respectively. Our
implementation is available at
https://github.com/arunshar/Physics-Informed-Diffusion-Probabilistic-Model.

</details>


### [297] [End-to-End Probabilistic Framework for Learning with Hard Constraints](https://arxiv.org/abs/2506.07003)
*Utkarsh Utkarsh,Danielle C. Maddix,Ruijun Ma,Michael W. Mahoney,Yuyang Wang*

Main category: cs.LG

TL;DR: ProbHardE2E是一个通用概率预测框架，通过可微分概率投影层（DPPL）实现端到端学习，强制满足硬约束并支持不确定性量化。该方法无需分布假设，可优化严格评分规则，适用于非线性约束，应用于PDE不确定性估计和时间序列预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常通过后处理或推理阶段满足约束，且依赖分布假设（如基于似然的目标函数），导致估计偏差。需开发一种能直接整合硬性约束、支持不确定性量化且分布鲁棒的通用框架。

Method: 提出DPPL层，结合神经网络架构实现端到端学习。利用方差信息强制约束，优化严格评分规则（如CRPS），避免分布假设，支持非线性约束建模。

Result: 在偏微分方程不确定性估计和概率时间序列预测中验证有效性，证明其可跨领域应用，并优于依赖后处理或分布假设的方法。

Conclusion: ProbHardE2E通过DPPL和评分规则优化，提供了一种灵活、分布鲁棒且支持硬约束的通用框架，连接了不同领域的概率建模需求。

Abstract: We present a general purpose probabilistic forecasting framework,
ProbHardE2E, to learn systems that can incorporate operational/physical
constraints as hard requirements. ProbHardE2E enforces hard constraints by
exploiting variance information in a novel way; and thus it is also capable of
performing uncertainty quantification (UQ) on the model. Our methodology uses a
novel differentiable probabilistic projection layer (DPPL) that can be combined
with a wide range of neural network architectures. This DPPL allows the model
to learn the system in an end-to-end manner, compared to other approaches where
the constraints are satisfied either through a post-processing step or at
inference. In addition, ProbHardE2E can optimize a strictly proper scoring
rule, without making any distributional assumptions on the target, which
enables it to obtain robust distributional estimates (in contrast to existing
approaches that generally optimize likelihood-based objectives, which are
heavily biased by their distributional assumptions and model choices); and it
can incorporate a range of non-linear constraints (increasing the power of
modeling and flexibility). We apply ProbHardE2E to problems in learning partial
differential equations with uncertainty estimates and to probabilistic
time-series forecasting, showcasing it as a broadly applicable general setup
that connects these seemingly disparate domains.

</details>


### [298] [Comparison of Lightweight Methods for Vehicle Dynamics-Based Driver Drowsiness Detection](https://arxiv.org/abs/2506.07014)
*Yutaro Nakagama,Daisuke Ishii,Kazuki Yoshizoe*

Main category: cs.LG

TL;DR: 本文提出了一种基于车辆动力学的驾驶员疲劳检测方法，通过公开数据集和轻量级机器学习模型，验证了现有方法的可重复性和性能，并展示了随机森林方法的高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于车辆动力学的驾驶员疲劳检测方法在性能指标可靠性和可重复性方面存在问题，本文旨在通过透明和公平的框架比较这些方法的性能。

Method: 本文开发了一个框架，从公开数据集中提取特征，并使用轻量级机器学习模型进行驾驶员疲劳检测。同时，实现了三种现有代表性方法和一种基于随机森林的方法。

Result: 在评估的方法中，基于随机森林的方法达到了最高的准确率88%。

Conclusion: 本文揭示了非标准化开发的驾驶员疲劳检测方法的固有问题，并展示了一种高性能且适当实现的方法。

Abstract: Driver drowsiness detection (DDD) prevents road accidents caused by driver
fatigue. Vehicle dynamics-based DDD has been proposed as a method that is both
economical and high performance. However, there are concerns about the
reliability of performance metrics and the reproducibility of many of the
existing methods. For instance, some previous studies seem to have a data
leakage issue among training and test datasets, and many do not openly provide
the datasets they used. To this end, this paper aims to compare the performance
of representative vehicle dynamics-based DDD methods under a transparent and
fair framework that uses a public dataset. We first develop a framework for
extracting features from an open dataset by Aygun et al. and performing DDD
with lightweight ML models; the framework is carefully designed to support a
variety of onfigurations. Second, we implement three existing representative
methods and a concise random forest (RF)-based method in the framework.
Finally, we report the results of experiments to verify the reproducibility and
clarify the performance of DDD based on common metrics. Among the evaluated
methods, the RF-based method achieved the highest accuracy of 88 %. Our
findings imply the issues inherent in DDD methods developed in a non-standard
manner, and demonstrate a high performance method implemented appropriately.

</details>


### [299] [AlphaSteer: Learning Refusal Steering with Principled Null-Space Constraint](https://arxiv.org/abs/2506.07022)
*Leheng Sheng,Changshuo Shen,Weixiang Zhao,Junfeng Fang,Xiaohao Liu,Zhenkai Liang,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.LG

TL;DR: AlphaSteer提出一种基于理论指导的激活导向方法，通过优化效用保持与安全性增强目标，有效解决LLM在防御恶意提示时安全性与实用性的权衡问题，实验验证其显著提升安全性且不影响正常能力。


<details>
  <summary>Details</summary>
Motivation: 现有激活导向方法在增强LLM安全性时，因不加区分地应用导向向量导致良性提示的过度拒绝与性能下降，需理论化方法解决安全与效用的平衡问题。

Method: AlphaSteer将激活导向建模为可学习过程：通过零空间约束学习对正常数据无扰动的导向向量（效用保持），并基于线性回归构建恶意数据的拒绝导向向量（安全性增强）。

Result: 实验表明，AlphaSteer在多种越狱攻击场景下显著提升LLM安全性，同时在通用能力基准测试中保持性能无损。

Conclusion: AlphaSteer通过理论驱动的双目标学习框架，成功实现LLM安全防御与实用性的协同优化，为安全导向方法提供新范式。

Abstract: As LLMs are increasingly deployed in real-world applications, ensuring their
ability to refuse malicious prompts, especially jailbreak attacks, is essential
for safe and reliable use. Recently, activation steering has emerged as an
effective approach for enhancing LLM safety by adding a refusal direction
vector to internal activations of LLMs during inference, which will further
induce the refusal behaviors of LLMs. However, indiscriminately applying
activation steering fundamentally suffers from the trade-off between safety and
utility, since the same steering vector can also lead to over-refusal and
degraded performance on benign prompts. Although prior efforts, such as vector
calibration and conditional steering, have attempted to mitigate this
trade-off, their lack of theoretical grounding limits their robustness and
effectiveness. To better address the trade-off between safety and utility, we
present a theoretically grounded and empirically effective activation steering
method called AlphaSteer. Specifically, it considers activation steering as a
learnable process with two principled learning objectives: utility preservation
and safety enhancement. For utility preservation, it learns to construct a
nearly zero vector for steering benign data, with the null-space constraints.
For safety enhancement, it learns to construct a refusal direction vector for
steering malicious data, with the help of linear regression. Experiments across
multiple jailbreak attacks and utility benchmarks demonstrate the effectiveness
of AlphaSteer, which significantly improves the safety of LLMs without
compromising general capabilities. Our codes are available at
https://github.com/AlphaLab-USTC/AlphaSteer.

</details>


### [300] [Mixture Experts with Test-Time Self-Supervised Aggregation for Tabular Imbalanced Regression](https://arxiv.org/abs/2506.07033)
*Yung-Chien Wang,Kuang-Da Wang,Wei-Yao Wang,Wen-Chih Peng*

Main category: cs.LG

TL;DR: 本文提出MATI方法，针对表格回归任务中的数据不平衡问题，通过区域感知混合专家和测试时自监督聚合，提升模型在不同测试分布下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有表格回归任务中数据不平衡问题研究不足，传统方法依赖测试分布已知且平衡的假设，实际应用中因分布变化易导致性能下降。

Method: MATI包含两个核心创新：1) 基于高斯混合模型的区域感知专家，捕捉不同区域特征；2) 测试时自监督动态调整专家权重，适应不同测试分布。

Result: 在房价预测、共享单车和年龄预测等4个真实数据集上，MATI在平衡/正态/逆向三种测试分布下平均MAE提升7.1%，优于现有方法。

Conclusion: MATI通过区域特征建模与动态专家聚合机制，有效解决了表格回归任务中数据分布不平衡的挑战，显著提升模型实际部署的鲁棒性。

Abstract: Tabular data serve as a fundamental and ubiquitous representation of
structured information in numerous real-world applications, e.g., finance and
urban planning. In the realm of tabular imbalanced applications, data imbalance
has been investigated in classification tasks with insufficient instances in
certain labels, causing the model's ineffective generalizability. However, the
imbalance issue of tabular regression tasks is underexplored, and yet is
critical due to unclear boundaries for continuous labels and simplifying
assumptions in existing imbalance regression work, which often rely on known
and balanced test distributions. Such assumptions may not hold in practice and
can lead to performance degradation. To address these issues, we propose MATI:
Mixture Experts with Test-Time Self-Supervised Aggregation for Tabular
Imbalance Regression, featuring two key innovations: (i) the Region-Aware
Mixture Expert, which adopts a Gaussian Mixture Model to capture the underlying
related regions. The statistical information of each Gaussian component is then
used to synthesize and train region-specific experts to capture the unique
characteristics of their respective regions. (ii) Test-Time Self-Supervised
Expert Aggregation, which dynamically adjusts region expert weights based on
test data features to reinforce expert adaptation across varying test
distributions. We evaluated MATI on four real-world tabular imbalance
regression datasets, including house pricing, bike sharing, and age prediction.
To reflect realistic deployment scenarios, we adopted three types of test
distributions: a balanced distribution with uniform target frequencies, a
normal distribution that follows the training data, and an inverse distribution
that emphasizes rare target regions. On average across these three test
distributions, MATI achieved a 7.1% improvement in MAE compared to existing
methods.

</details>


### [301] [Efficient $Q$-Learning and Actor-Critic Methods for Robust Average Reward Reinforcement Learning](https://arxiv.org/abs/2506.07040)
*Yang Xu,Swetha Ganesh,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 本文提出了首个针对污染、TV距离和Wasserstein距离不确定性下鲁棒平均奖励MDP的Q学习和actor-critic算法，证明了非渐近收敛性，并建立了样本复杂度理论。


<details>
  <summary>Details</summary>
Motivation: 针对分布鲁棒强化学习在平均奖励场景下的理论空白，研究在模型不确定性（如TV距离、Wasserstein距离）下如何设计高效收敛的强化学习算法。

Method: 通过构造鲁棒Q贝尔曼算子的半范数性质，结合随机逼近更新实现Q函数估计；将鲁棒策略镜像下降与critic估计结合，构建自然actor-critic框架。

Result: Q学习在O~(ε^-2)样本内收敛到最优鲁棒Q函数；actor-critic算法在O~(ε^-3)样本内获得ε-最优鲁棒策略。

Conclusion: 该工作首次建立了平均奖励MDP下分布鲁棒强化学习的非渐近收敛理论，为鲁棒RL算法设计提供了新的理论基准。

Abstract: We present the first $Q$-learning and actor-critic algorithms for robust
average reward Markov Decision Processes (MDPs) with non-asymptotic convergence
under contamination, TV distance and Wasserstein distance uncertainty sets. We
show that the robust $Q$ Bellman operator is a strict contractive mapping with
respect to a carefully constructed semi-norm with constant functions being
quotiented out. This property supports a stochastic approximation update, that
learns the optimal robust $Q$ function in $\tilde{\cO}(\epsilon^{-2})$ samples.
We also show that the same idea can be used for robust $Q$ function estimation,
which can be further used for critic estimation. Coupling it with theories in
robust policy mirror descent update, we present a natural actor-critic
algorithm that attains an $\epsilon$-optimal robust policy in
$\tilde{\cO}(\epsilon^{-3})$ samples. These results advance the theory of
distributionally robust reinforcement learning in the average reward setting.

</details>


### [302] [FairPFN: A Tabular Foundation Model for Causal Fairness](https://arxiv.org/abs/2506.07049)
*Jake Robertson,Noah Hollmann,Samuel Müller,Noor Awad,Frank Hutter*

Main category: cs.LG

TL;DR: 本文提出FairPFN，一种基于合成因果公平数据预训练的表格基础模型，用于在无需因果模型知识的情况下识别和消除预测中的保护属性因果效应。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统在关键领域（如医疗、执法和金融）中被广泛应用，但这些系统通常基于包含人口偏见的历史数据进行训练，导致决策加剧社会不平等。现有因果公平框架假设已知正确的因果模型，限制了其在复杂公平场景中的适用性。

Method: 提出FairPFN模型，通过预训练在合成因果公平数据上，无需因果模型知识即可识别和消除保护属性的因果效应。

Result: FairPFN在多种手工制作和真实场景中表现出色，能够有效识别和消除保护属性的因果效应，优于现有基线方法。

Conclusion: FairPFN为因果公平研究开辟了新方向，使其更适用于复杂的公平问题，未来研究前景广阔。

Abstract: Machine learning (ML) systems are utilized in critical sectors, such as
healthcare, law enforcement, and finance. However, these systems are often
trained on historical data that contains demographic biases, leading to ML
decisions that perpetuate or exacerbate existing social inequalities. Causal
fairness provides a transparent, human-in-the-loop framework to mitigate
algorithmic discrimination, aligning closely with legal doctrines of direct and
indirect discrimination. However, current causal fairness frameworks hold a key
limitation in that they assume prior knowledge of the correct causal model,
restricting their applicability in complex fairness scenarios where causal
models are unknown or difficult to identify. To bridge this gap, we propose
FairPFN, a tabular foundation model pre-trained on synthetic causal fairness
data to identify and mitigate the causal effects of protected attributes in its
predictions. FairPFN's key contribution is that it requires no knowledge of the
causal model and still demonstrates strong performance in identifying and
removing protected causal effects across a diverse set of hand-crafted and
real-world scenarios relative to robust baseline methods. FairPFN paves the way
for promising future research, making causal fairness more accessible to a
wider variety of complex fairness problems.

</details>


### [303] [Policy Gradient with Tree Search: Avoiding Local Optimas through Lookahead](https://arxiv.org/abs/2506.07054)
*Uri Koren,Navdeep Kumar,Uri Gadot,Giorgia Ramponi,Kfir Yehuda Levy,Shie Mannor*

Main category: cs.LG

TL;DR: 传统策略梯度（PG）方法在复杂环境中易陷入局部最优，本文提出结合树搜索的PGTS方法，通过增加前瞻步数减少不良驻点，理论和实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统策略梯度方法在大型或复杂环境中易收敛至次优局部解，需改进策略优化以提升性能。

Method: 提出PGTS方法，集成m步前瞻树搜索机制，理论证明增加搜索深度可单调减少不良驻点，并支持仅更新当前策略访问状态的实用场景。

Result: 在多种MDP环境（如Ladder、Tightrope等）中，PGTS展现出远见性，能逃离局部陷阱并优于传统PG方法。

Conclusion: PGTS通过树搜索增强策略优化，理论分析与实验结果均验证其能有效提升最坏情况性能并解决复杂奖励场景问题。

Abstract: Classical policy gradient (PG) methods in reinforcement learning frequently
converge to suboptimal local optima, a challenge exacerbated in large or
complex environments. This work investigates Policy Gradient with Tree Search
(PGTS), an approach that integrates an $m$-step lookahead mechanism to enhance
policy optimization. We provide theoretical analysis demonstrating that
increasing the tree search depth $m$-monotonically reduces the set of
undesirable stationary points and, consequently, improves the worst-case
performance of any resulting stationary policy. Critically, our analysis
accommodates practical scenarios where policy updates are restricted to states
visited by the current policy, rather than requiring updates across the entire
state space. Empirical evaluations on diverse MDP structures, including Ladder,
Tightrope, and Gridworld environments, illustrate PGTS's ability to exhibit
"farsightedness," navigate challenging reward landscapes, escape local traps
where standard PG fails, and achieve superior solutions.

</details>


### [304] [E-BATS: Efficient Backpropagation-Free Test-Time Adaptation for Speech Foundation Models](https://arxiv.org/abs/2506.07078)
*Jiaheng Dong,Hong Jia,Soumyajit Chatterjee,Abhirup Ghosh,James Bailey,Ting Dang*

Main category: cs.LG

TL;DR: 本文提出E-BATS框架，通过无反向传播的轻量级测试时自适应方法，解决语音基础模型在声学域偏移下的性能下降问题，实现高效内存利用与准确率提升的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有测试时自适应方法在语音任务中存在内存消耗高或准确率低的问题，且视觉任务方法无法直接迁移至语音领域。需开发兼顾效率与效果的语音专用自适应框架。

Method: 结合三个核心组件：(1)前向传播轻量提示适配实现特征对齐；(2)多尺度损失函数捕捉全局（语句级）与局部（词元级）分布偏移；(3)测试时指数滑动平均机制确保跨语句稳定适配。

Result: 在16种声学条件下的4个噪声语音数据集上，准确率较无反向传播基线提升4.1%-13.5%，GPU内存消耗比基于反向传播方法减少2.0-6.4倍。

Conclusion: E-BATS通过高效无反向传播设计，为实际语音系统在声学变化下的可扩展鲁棒适配提供新路径，推动现实场景中语音处理技术的发展。

Abstract: Speech Foundation Models encounter significant performance degradation when
deployed in real-world scenarios involving acoustic domain shifts, such as
background noise and speaker accents. Test-time adaptation (TTA) has recently
emerged as a viable strategy to address such domain shifts at inference time
without requiring access to source data or labels. However, existing TTA
approaches, particularly those relying on backpropagation, are
memory-intensive, limiting their applicability in speech tasks and
resource-constrained settings. Although backpropagation-free methods offer
improved efficiency, existing ones exhibit poor accuracy. This is because they
are predominantly developed for vision tasks, which fundamentally differ from
speech task formulations, noise characteristics, and model architecture, posing
unique transferability challenges. In this paper, we introduce E-BATS, the
first Efficient BAckpropagation-free TTA framework designed explicitly for
speech foundation models. E-BATS achieves a balance between adaptation
effectiveness and memory efficiency through three key components: (i)
lightweight prompt adaptation for a forward-pass-based feature alignment, (ii)
a multi-scale loss to capture both global (utterance-level) and local
distribution shifts (token-level) and (iii) a test-time exponential moving
average mechanism for stable adaptation across utterances. Experiments
conducted on four noisy speech datasets spanning sixteen acoustic conditions
demonstrate consistent improvements, with 4.1%-13.5% accuracy gains over
backpropagation-free baselines and 2.0-6.4 times GPU memory savings compared to
backpropagation-based methods. By enabling scalable and robust adaptation under
acoustic variability, this work paves the way for developing more efficient
adaptation approaches for practical speech processing systems in real-world
environments.

</details>


### [305] [State Entropy Regularization for Robust Reinforcement Learning](https://arxiv.org/abs/2506.07085)
*Uri Koren,Yonatan Ashlag,Mirco Mutti,Esther Derman,Pierre-Luc Bacon,Shie Mannor*

Main category: cs.LG

TL;DR: 本文通过理论分析证明，状态熵正则化在强化学习中能提升对结构化、空间相关扰动的鲁棒性，但其优势对策略评估的轨迹数量敏感。与策略熵正则化相比，状态熵在迁移学习场景中表现更优，但存在特定失效场景。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒强化学习方法主要关注小规模、非相关扰动，而忽略了迁移学习中常见的结构化扰动。状态熵正则化虽在实践中表现优异，但缺乏理论支撑，需系统性分析其鲁棒性机制。

Method: 通过形式化理论分析状态熵正则化的鲁棒性，包括奖励函数和状态转移不确定性的数学证明。对比策略熵正则化，结合不同扰动类型（结构化/非相关）和策略评估轨迹数量进行系统性实验验证。

Result: 1. 状态熵正则化对结构化扰动具有形式化鲁棒保证 2. 在奖励不确定性下表现优于策略熵 3. 对状态转移不确定性存在失效场景 4. 其鲁棒性优势随策略评估轨迹数增加而显著衰减

Conclusion: 状态熵正则化为处理迁移学习中的复杂扰动提供了理论依据，但其实际应用需谨慎选择评估轨迹数量。与策略熵形成互补关系，二者应根据任务特性结合使用。

Abstract: State entropy regularization has empirically shown better exploration and
sample complexity in reinforcement learning (RL). However, its theoretical
guarantees have not been studied. In this paper, we show that state entropy
regularization improves robustness to structured and spatially correlated
perturbations. These types of variation are common in transfer learning but
often overlooked by standard robust RL methods, which typically focus on small,
uncorrelated changes. We provide a comprehensive characterization of these
robustness properties, including formal guarantees under reward and transition
uncertainty, as well as settings where the method performs poorly. Much of our
analysis contrasts state entropy with the widely used policy entropy
regularization, highlighting their different benefits. Finally, from a
practical standpoint, we illustrate that compared with policy entropy, the
robustness advantages of state entropy are more sensitive to the number of
rollouts used for policy evaluation.

</details>


### [306] [Pointwise confidence estimation in the non-linear $\ell^2$-regularized least squares](https://arxiv.org/abs/2506.07088)
*Ilja Kuzborskij,Yasin Abbasi Yadkori*

Main category: cs.LG

TL;DR: 本文提出了一种基于加权范数的非渐近高概率置信边界估计方法，适用于ℓ²正则化非线性最小二乘模型的局部极小值。该方法通过隐式特征空间中测试输入与训练数据的相似性动态调整置信区间，计算效率接近梯度计算，并在实验中优于自助法。


<details>
  <summary>Details</summary>
Motivation: 传统基于渐近极大似然估计的置信区间在非渐近场景下可能失效，且现有方法（如自助法）在非线性模型（如神经网络）中计算成本高。本文旨在解决非线性模型中置信区间随数据分布动态调整的问题，同时保证非渐近高概率覆盖。

Method: 通过分析正则化训练损失的局部极小值，提出基于逆Hessian矩阵的加权范数构造点态置信边界。该范数推广了线性场景下的协方差逆形式，并设计了一种高效计算算法，其复杂度仅略高于损失函数的梯度计算。

Result: 置信边界随测试输入与训练数据的隐式特征相似性自适应缩放，实验表明其在覆盖率和区间宽度权衡上优于自助法。加权范数的计算效率验证了方法的实用性。

Conclusion: 所提方法为非渐近场景下的置信估计提供了理论保证，其动态调整特性与高效计算使其适用于复杂非线性模型（如神经网络），为实际应用中的不确定性量化提供了更可靠的解决方案。

Abstract: We consider a high-probability non-asymptotic confidence estimation in the
$\ell^2$-regularized non-linear least-squares setting with fixed design. In
particular, we study confidence estimation for local minimizers of the
regularized training loss. We show a pointwise confidence bound, meaning that
it holds for the prediction on any given fixed test input $x$. Importantly, the
proposed confidence bound scales with similarity of the test input to the
training data in the implicit feature space of the predictor (for instance,
becoming very large when the test input lies far outside of the training data).
This desirable last feature is captured by the weighted norm involving the
inverse-Hessian matrix of the objective function, which is a generalized
version of its counterpart in the linear setting, $x^{\top} \text{Cov}^{-1} x$.
Our generalized result can be regarded as a non-asymptotic counterpart of the
classical confidence interval based on asymptotic normality of the MLE
estimator. We propose an efficient method for computing the weighted norm,
which only mildly exceeds the cost of a gradient computation of the loss
function. Finally, we complement our analysis with empirical evidence showing
that the proposed confidence bound provides better coverage/width trade-off
compared to a confidence estimation by bootstrapping, which is a gold-standard
method in many applications involving non-linear predictors such as neural
networks.

</details>


### [307] [Patient Similarity Computation for Clinical Decision Support: An Efficient Use of Data Transformation, Combining Static and Time Series Data](https://arxiv.org/abs/2506.07092)
*Joydeb Kumar Sana,Mohammad M. Masud,M Sohel Rahman,M Saifur Rahman*

Main category: cs.LG

TL;DR: 本文提出一种基于数据转换的分布式患者相似性计算方法（DPSC），结合时间序列和静态数据，通过aWOE和Z-score转换提升预测性能，并利用分布式DTW降低计算时间。


<details>
  <summary>Details</summary>
Motivation: 传统动态时间规整（DTW）方法在处理医疗大数据时计算效率低，且静态数据与时间序列数据的结合及隐私保护存在挑战。

Method: 使用自适应证据权重（aWOE）和Z-score转换静态数据以保护隐私并优化聚类；采用分布式DTW计算时间序列相似性以提升计算效率。

Result: 在冠状动脉疾病和充血性心力衰竭中，AUC、准确率和F值分别提升最高达15.9%和21.9%，计算时间减少40%。

Conclusion: 所提方法有效整合多源数据，显著提升临床决策支持性能，同时通过分布式计算解决大数据场景下的效率问题。

Abstract: Patient similarity computation (PSC) is a fundamental problem in healthcare
informatics. The aim of the patient similarity computation is to measure the
similarity among patients according to their historical clinical records, which
helps to improve clinical decision support. This paper presents a novel
distributed patient similarity computation (DPSC) technique based on data
transformation (DT) methods, utilizing an effective combination of time series
and static data. Time series data are sensor-collected patients' information,
including metrics like heart rate, blood pressure, Oxygen saturation,
respiration, etc. The static data are mainly patient background and demographic
data, including age, weight, height, gender, etc. Static data has been used for
clustering the patients. Before feeding the static data to the machine learning
model adaptive Weight-of-Evidence (aWOE) and Z-score data transformation (DT)
methods have been performed, which improve the prediction performances. In
aWOE-based patient similarity models, sensitive patient information has been
processed using aWOE which preserves the data privacy of the trained models. We
used the Dynamic Time Warping (DTW) approach, which is robust and very popular,
for time series similarity. However, DTW is not suitable for big data due to
the significant computational run-time. To overcome this problem, distributed
DTW computation is used in this study. For Coronary Artery Disease, our DT
based approach boosts prediction performance by as much as 11.4%, 10.20%, and
12.6% in terms of AUC, accuracy, and F-measure, respectively. In the case of
Congestive Heart Failure (CHF), our proposed method achieves performance
enhancement up to 15.9%, 10.5%, and 21.9% for the same measures, respectively.
The proposed method reduces the computation time by as high as 40%.

</details>


### [308] [Filling the Missings: Spatiotemporal Data Imputation by Conditional Diffusion](https://arxiv.org/abs/2506.07099)
*Wenying He,Jieling Huang,Junhua Gu,Ji Zhang,Yude Bai*

Main category: cs.LG

TL;DR: 本文提出了一种名为CoFILL的条件扩散模型，用于解决时空数据缺失问题，通过双流架构处理时空和频域特征，实验表明其在插补精度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 时空数据在现实应用中常因硬件或软件故障而缺失，现有机器学习方法难以有效建模时空维度间的复杂依赖关系，且在插补过程中易产生累积误差。

Method: CoFILL基于扩散模型的优势，提出了一种双流架构，并行处理时空和频域特征，融合这些特征以捕捉数据的快速波动和潜在模式。

Result: 实验表明，CoFILL的噪声预测网络成功将随机噪声转化为与真实数据分布一致的有意义值，且在插补精度上优于现有方法。

Conclusion: CoFILL通过创新的双流架构和扩散模型，有效解决了时空数据缺失问题，提供了更鲁棒的插补方法。

Abstract: Missing data in spatiotemporal systems presents a significant challenge for
modern applications, ranging from environmental monitoring to urban traffic
management. The integrity of spatiotemporal data often deteriorates due to
hardware malfunctions and software failures in real-world deployments. Current
approaches based on machine learning and deep learning struggle to model the
intricate interdependencies between spatial and temporal dimensions effectively
and, more importantly, suffer from cumulative errors during the data imputation
process, which propagate and amplify through iterations. To address these
limitations, we propose CoFILL, a novel Conditional Diffusion Model for
spatiotemporal data imputation. CoFILL builds on the inherent advantages of
diffusion models to generate high-quality imputations without relying on
potentially error-prone prior estimates. It incorporates an innovative
dual-stream architecture that processes temporal and frequency domain features
in parallel. By fusing these complementary features, CoFILL captures both rapid
fluctuations and underlying patterns in the data, which enables more robust
imputation. The extensive experiments reveal that CoFILL's noise prediction
network successfully transforms random noise into meaningful values that align
with the true data distribution. The results also show that CoFILL outperforms
state-of-the-art methods in imputation accuracy. The source code is publicly
available at https://github.com/joyHJL/CoFILL.

</details>


### [309] [Towards Universal Offline Black-Box Optimization via Learning Language Model Embeddings](https://arxiv.org/abs/2506.07109)
*Rong-Xi Tan,Ming Chen,Ke Xue,Yao Wang,Yaoyuan Wang,Sheng Fu,Chao Qian*

Main category: cs.LG

TL;DR: 本文探讨了利用语言模型的嵌入能力实现跨领域通用黑箱优化的方法，提出了端到端学习框架和潜在空间学习策略，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的离线黑箱优化方法受限于单任务和固定维度设置，无法实现跨领域通用优化。语言模型的嵌入能力为统一表示异构数值空间提供了新思路。

Method: 提出了多种方法，包括基于下一词预测的端到端学习框架，以及优先学习具有强表示能力的潜在空间。

Result: 实验表明，所提出的方法在通用性和有效性上表现优异，能够克服传统黑箱优化的障碍。

Conclusion: 结合语言模型先验和学习字符串嵌入空间，能够为通用黑箱优化算法的发展铺平道路。

Abstract: The pursuit of universal black-box optimization (BBO) algorithms is a
longstanding goal. However, unlike domains such as language or vision, where
scaling structured data has driven generalization, progress in offline BBO
remains hindered by the lack of unified representations for heterogeneous
numerical spaces. Thus, existing offline BBO approaches are constrained to
single-task and fixed-dimensional settings, failing to achieve cross-domain
universal optimization. Recent advances in language models (LMs) offer a
promising path forward: their embeddings capture latent relationships in a
unifying way, enabling universal optimization across different data types
possible. In this paper, we discuss multiple potential approaches, including an
end-to-end learning framework in the form of next-token prediction, as well as
prioritizing the learning of latent spaces with strong representational
capabilities. To validate the effectiveness of these methods, we collect
offline BBO tasks and data from open-source academic works for training.
Experiments demonstrate the universality and effectiveness of our proposed
methods. Our findings suggest that unifying language model priors and learning
string embedding space can overcome traditional barriers in universal BBO,
paving the way for general-purpose BBO algorithms. The code is provided at
https://github.com/lamda-bbo/universal-offline-bbo.

</details>


### [310] [Quality-Diversity Red-Teaming: Automated Generation of High-Quality and Diverse Attackers for Large Language Models](https://arxiv.org/abs/2506.07121)
*Ren-Jian Wang,Ke Xue,Zeyu Qin,Ziniu Li,Sheng Tang,Hao-Tian Li,Shengcai Liu,Chao Qian*

Main category: cs.LG

TL;DR: 本文提出了一种新的红队测试框架QDRT，通过行为条件训练和开放式的行为回放缓冲区，生成多样且高质量的攻击，以更全面地评估大语言模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有红队测试方法在生成对抗性提示时存在多样性不足和攻击风格覆盖不全的问题，无法全面评估大语言模型的安全性。

Method: 本文提出了Quality-Diversity Red-Teaming (QDRT)框架，通过行为条件训练和开放式的行为回放缓冲区实现目标驱动的多样性，并训练多个专门攻击者以生成高质量的攻击。

Result: 实验表明，QDRT生成的攻击在多样性和有效性上均优于现有方法，能够针对多种大语言模型（如GPT-2、Llama-3等）进行更全面的安全评估。

Conclusion: QDRT框架为大语言模型的安全性评估提供了一种系统且有效的方法，支持其负责任地部署。

Abstract: Ensuring safety of large language models (LLMs) is important. Red teaming--a
systematic approach to identifying adversarial prompts that elicit harmful
responses from target LLMs--has emerged as a crucial safety evaluation method.
Within this framework, the diversity of adversarial prompts is essential for
comprehensive safety assessments. We find that previous approaches to
red-teaming may suffer from two key limitations. First, they often pursue
diversity through simplistic metrics like word frequency or sentence embedding
similarity, which may not capture meaningful variation in attack strategies.
Second, the common practice of training a single attacker model restricts
coverage across potential attack styles and risk categories. This paper
introduces Quality-Diversity Red-Teaming (QDRT), a new framework designed to
address these limitations. QDRT achieves goal-driven diversity through
behavior-conditioned training and implements a behavioral replay buffer in an
open-ended manner. Additionally, it trains multiple specialized attackers
capable of generating high-quality attacks across diverse styles and risk
categories. Our empirical evaluation demonstrates that QDRT generates attacks
that are both more diverse and more effective against a wide range of target
LLMs, including GPT-2, Llama-3, Gemma-2, and Qwen2.5. This work advances the
field of LLM safety by providing a systematic and effective approach to
automated red-teaming, ultimately supporting the responsible deployment of
LLMs.

</details>


### [311] [Reliable Critics: Monotonic Improvement and Convergence Guarantees for Reinforcement Learning](https://arxiv.org/abs/2506.07134)
*Eshwar S. R.,Gugan Thoppe,Aditya Gopalan,Gal Dalal*

Main category: cs.LG

TL;DR: 本文提出了可靠策略迭代（RPI），通过基于贝尔曼的约束优化替代传统策略评估中的投影或贝尔曼误差最小化，解决了函数逼近下策略迭代的单调性保证问题。


<details>
  <summary>Details</summary>
Motivation: 尽管经过多年研究，使用函数逼近的强化学习算法仍存在挑战，尤其是策略迭代在函数逼近下失去单调性保证。本文旨在解决这一问题。

Method: 引入可靠策略迭代（RPI），用基于贝尔曼的约束优化替代传统策略评估中的投影或贝尔曼误差最小化，并提供了无模型变体，可集成到DQN和DDPG等主流无模型策略迭代实现中。

Result: RPI不仅保证了价值估计的单调性，还使其下界真实回报，且其极限部分满足未投影的贝尔曼方程。在经典控制任务中，RPI增强的变体在保持下界保证的同时，性能优于或匹配所有基线方法。

Conclusion: RPI是首个在函数逼近下具有单调性和收敛性保证的算法，其无模型变体易于集成到现有无模型策略迭代框架中，并在实际任务中表现出色。

Abstract: Despite decades of research, it remains challenging to correctly use
Reinforcement Learning (RL) algorithms with function approximation. A prime
example is policy iteration, whose fundamental guarantee of monotonic
improvement collapses even under linear function approximation. To address this
issue, we introduce Reliable Policy Iteration (RPI). It replaces the common
projection or Bellman-error minimization during policy evaluation with a
Bellman-based constrained optimization. We prove that not only does RPI confer
textbook monotonicity on its value estimates but these estimates also lower
bound the true return. Also, their limit partially satisfies the unprojected
Bellman equation, emphasizing RPI's natural fit within RL. RPI is the first
algorithm with such monotonicity and convergence guarantees under function
approximation. For practical use, we provide a model-free variant of RPI that
amounts to a novel critic. It can be readily integrated into primary model-free
PI implementations such as DQN and DDPG. In classical control tasks, such
RPI-enhanced variants consistently maintain their lower-bound guarantee while
matching or surpassing the performance of all baseline methods.

</details>


### [312] [AMoPO: Adaptive Multi-objective Preference Optimization without Reward Models and Reference Models](https://arxiv.org/abs/2506.07165)
*Qi Liu,Jingqing Ruan,Hao Li,Haodong Zhao,Desheng Wang,Jiansong Chen,Wan Guanglu,Xunliang Cai,Zhi Zheng,Tong Xu*

Main category: cs.LG

TL;DR: 本文提出了一种名为AMoPO的新框架，用于解决大语言模型在多目标偏好对齐中的局限性，通过动态平衡偏好维度，无需额外奖励模型，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型多目标偏好对齐方法存在两个主要问题：无法有效平衡不同偏好维度，以及依赖辅助奖励/参考模型增加了计算复杂性。

Method: AMoPO框架引入多目标优化范式，使用维度感知的生成指标作为隐式奖励，并通过自适应权重分配机制动态优先处理偏好维度。

Result: 实验结果表明，AMoPO在7B、14B和32B模型上均表现出色，性能比现有基线提升了28.5%，并验证了其适应性和有效性。

Conclusion: AMoPO框架能够实现维度感知的偏好对齐，展示了其优越性，代码和数据集已公开。

Abstract: Existing multi-objective preference alignment methods for large language
models (LLMs) face limitations: (1) the inability to effectively balance
various preference dimensions, and (2) reliance on auxiliary reward/reference
models introduces computational complexity. To address these challenges, we
propose Adaptive Multi-objective Preference Optimization (AMoPO), a novel
framework that achieves dynamic balance across preference dimensions. By
introducing the multi-objective optimization paradigm to use the
dimension-aware generation metrics as implicit rewards, AMoPO aligns LLMs with
diverse preferences without additional reward models or reference models. We
introduce an adaptive weight assignment mechanism that models the generation
space as a Gaussian distribution, allowing dynamic prioritization of preference
dimensions. Empirical results demonstrate that AMoPO outperforms
state-of-the-art baselines by 28.5%, and the experiments on 7B, 14B, and 32B
models reveal the scaling ability of AMoPO. Moreover, additional analysis of
multiple dimensions verifies its adaptability and effectiveness. These findings
validate AMoPO's capability to achieve dimension-aware preference alignment,
highlighting its superiority. Our codes and datasets are available at
https://github.com/Javkonline/AMoPO.

</details>


### [313] [Efficient Text-Attributed Graph Learning through Selective Annotation and Graph Alignment](https://arxiv.org/abs/2506.07168)
*Huanyi Xie,Lijie Hu,Lu Yu,Tianhao Huang,Longfei Li,Meng Li,Jun Zhou,Huan Wang,Di Wang*

Main category: cs.LG

TL;DR: GAGA框架通过仅标注1%的代表性节点和边，结合两级对齐模块，在文本属性图（TAG）表征学习中实现高效且高分类准确率。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络（GNNs）在文本属性图（TAGs）中因复杂文本信息表现不足，而现有基于大语言模型（LLMs）的方法需大量标注或微调，成本高昂。

Method: GAGA通过标注代表性节点/边构建注释图，并设计两级对齐模块将注释图与TAG的底层结构对齐，以融合拓扑关系。

Result: 实验表明GAGA仅需1%标注数据即可达到或超越现有方法的分类准确率，显著提升效率。

Conclusion: GAGA在降低标注成本的同时保持高性能，为TAG表征学习提供了高效解决方案。

Abstract: In the realm of Text-attributed Graphs (TAGs), traditional graph neural
networks (GNNs) often fall short due to the complex textual information
associated with each node. Recent methods have improved node representations by
leveraging large language models (LLMs) to enhance node text features, but
these approaches typically require extensive annotations or fine-tuning across
all nodes, which is both time-consuming and costly. To overcome these
challenges, we introduce GAGA, an efficient framework for TAG representation
learning. GAGA reduces annotation time and cost by focusing on annotating only
representative nodes and edges. It constructs an annotation graph that captures
the topological relationships among these annotations. Furthermore, GAGA
employs a two-level alignment module to effectively integrate the annotation
graph with the TAG, aligning their underlying structures. Experiments show that
GAGA achieves classification accuracies on par with or surpassing
state-of-the-art methods while requiring only 1% of the data to be annotated,
demonstrating its high efficiency.

</details>


### [314] [Regularized Adaptive Graph Learning for Large-Scale Traffic Forecasting](https://arxiv.org/abs/2506.07179)
*Kaiqi Wu,Weiyang Kong,Sen Zhang,Yubao Liu,Zitong Chen*

Main category: cs.LG

TL;DR: 本文提出了一种正则化自适应图学习模型（RAGL），通过结合随机共享嵌入和自适应图卷积，解决了交通预测中节点嵌入正则化和计算效率的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的自适应图学习方法在交通预测中往往忽略节点嵌入的正则化，或面临图卷积操作计算复杂度高的问题。本文旨在解决这些挑战。

Method: 本文提出了正则化自适应图学习框架，结合随机共享嵌入（SSE）和自适应图卷积，并通过残差差异机制实现嵌入正则化和噪声抑制。此外，开发了高效余弦算子（ECO），基于正则化嵌入的余弦相似性进行图卷积，具有线性时间复杂度。

Result: 在四个大规模真实交通数据集上的实验表明，RAGL在预测精度上优于现有方法，并展现出竞争力的计算效率。

Conclusion: RAGL模型在交通预测任务中表现出色，不仅提高了预测精度，还解决了计算效率问题，具有广泛的应用前景。

Abstract: Traffic prediction is a critical task in spatial-temporal forecasting with
broad applications in travel planning and urban management. Adaptive graph
convolution networks have emerged as mainstream solutions due to their ability
to learn node embeddings in a data-driven manner and capture complex latent
dependencies. However, existing adaptive graph learning methods for traffic
forecasting often either ignore the regularization of node embeddings, which
account for a significant proportion of model parameters, or face scalability
issues from expensive graph convolution operations. To address these
challenges, we propose a Regularized Adaptive Graph Learning (RAGL) model.
First, we introduce a regularized adaptive graph learning framework that
synergizes Stochastic Shared Embedding (SSE) and adaptive graph convolution via
a residual difference mechanism, achieving both embedding regularization and
noise suppression. Second, to ensure scalability on large road networks, we
develop the Efficient Cosine Operator (ECO), which performs graph convolution
based on the cosine similarity of regularized embeddings with linear time
complexity. Extensive experiments on four large-scale real-world traffic
datasets show that RAGL consistently outperforms state-of-the-art methods in
terms of prediction accuracy and exhibits competitive computational efficiency.

</details>


### [315] [Learning based on neurovectors for tabular data: a new neural network approach](https://arxiv.org/abs/2506.07185)
*J. C. Husillos,A. Gallego,A. Roma,A. Troncoso*

Main category: cs.LG

TL;DR: 本文提出了一种基于Neurovectors的新学习方法，通过向量空间中的能量传播替代传统反向传播权重调整，提升了模型的可解释性和效率，并在分类和回归任务中展现出与传统模型相当的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统人工神经网络依赖反向传播调整权重，存在可解释性低和适应性不足的问题。本文旨在通过结构化向量空间中的能量传播机制，构建更透明、灵活的学习框架。

Method: Neurovectors利用节点与向量关系构建信息结构，通过能量传播驱动学习过程，生成动态知识表示，而非传统权重更新机制。

Result: 在UCI和Kaggle数据集上的实验表明，Neurovectors在分类和回归任务中达到与传统机器学习及深度学习模型竞争的准确率。

Conclusion: Neurovectors通过能量传播机制和结构化向量空间，实现了高效且可解释的建模，为表格数据处理提供了新范式。

Abstract: In this paper, we present a novel learning approach based on Neurovectors, an
innovative paradigm that structures information through interconnected nodes
and vector relationships for tabular data processing. Unlike traditional
artificial neural networks that rely on weight adjustment through
backpropagation, Neurovectors encode information by structuring data in vector
spaces where energy propagation, rather than traditional weight updates, drives
the learning process, enabling a more adaptable and explainable learning
process. Our method generates dynamic representations of knowledge through
neurovectors, thereby improving both the interpretability and efficiency of the
predictive model. Experimental results using datasets from well-established
repositories such as the UCI machine learning repository and Kaggle are
reported both for classification and regression. To evaluate its performance,
we compare our approach with standard machine learning and deep learning
models, showing that Neurovectors achieve competitive accuracy.

</details>


### [316] [Analyzing Breast Cancer Survival Disparities by Race and Demographic Location: A Survival Analysis Approach](https://arxiv.org/abs/2506.07191)
*Ramisa Farha,Joshua O. Olukoya*

Main category: cs.LG

TL;DR: 本研究通过SEER 2021数据集分析乳腺癌患者生存差异，结合探索性数据分析和生存模型，揭示种族与地域对预后的影响，为制定针对性干预提供依据。


<details>
  <summary>Details</summary>
Motivation: 揭示不同种族和地域乳腺癌患者生存结果的差异，以解决治疗与护理中的不平等问题，支持全球改善癌症预后的努力。

Method: 采用SEER 2021数据集，结合探索性数据分析（EDA）、Kaplan-Meier估计器、log-rank检验及Cox比例风险模型进行生存分析，并通过模型验证确保结果可靠性。

Result: 研究发现不同种族和地理群体间存在显著生存率差异，关键变量（如种族）对预后有显著影响，统计模型验证了这些差异的稳健性。

Conclusion: 研究结果为政策制定者和医疗专业人员提供了数据支持，强调需开发针对性干预措施以减少乳腺癌治疗不平等，促进全球预后改善。

Abstract: This study employs a robust analytical framework to uncover patterns in
survival outcomes among breast cancer patients from diverse racial and
geographical backgrounds. This research uses the SEER 2021 dataset to analyze
breast cancer survival outcomes to identify and comprehend dissimilarities. Our
approach integrates exploratory data analysis (EDA), through this we identify
key variables that influence survival rates and employ survival analysis
techniques, including the Kaplan-Meier estimator and log-rank test and the
advanced modeling Cox Proportional Hazards model to determine how survival
rates vary across racial groups and countries. Model validation and
interpretation are undertaken to ensure the reliability of our findings, which
are documented comprehensively to inform policymakers and healthcare
professionals. The outcome of this paper is a detailed version of statistical
analysis that not just highlights disparities in breast cancer treatment and
care but also serves as a foundational tool for developing targeted
interventions to address the inequalities effectively. Through this research,
our aim is to contribute to the global efforts to improve breast cancer
outcomes and reduce treatment disparities.

</details>


### [317] [GGBall: Graph Generative Model on Poincaré Ball](https://arxiv.org/abs/2506.07198)
*Tianci Bu,Chuanrui Wang,Hao Ma,Haoren Zheng,Xin Lu,Tailin Wu*

Main category: cs.LG

TL;DR: GGBall提出了一种新的双曲框架用于图生成，结合了几何归纳偏置和现代生成范式，显著提升了拓扑层次结构的保持能力。


<details>
  <summary>Details</summary>
Motivation: 由于欧几里得几何在捕捉指数复杂性方面的局限性，生成具有层次结构的图仍然是一个基本挑战。

Method: GGBall结合了双曲向量量化自编码器（HVQVAE）和通过闭式测地线定义的黎曼流匹配先验，并开发了一套完全在流形内操作的双曲GNN和Transformer层。

Result: 在Community-Small和Ego-Small数据集上，GGBall分别减少了超过75%和40%的度MMD，显著优于现有基线。

Conclusion: 双曲几何为复杂、结构化和层次化数据域的生成建模提供了强大的基础。

Abstract: Generating graphs with hierarchical structures remains a fundamental
challenge due to the limitations of Euclidean geometry in capturing exponential
complexity. Here we introduce \textbf{GGBall}, a novel hyperbolic framework for
graph generation that integrates geometric inductive biases with modern
generative paradigms. GGBall combines a Hyperbolic Vector-Quantized Autoencoder
(HVQVAE) with a Riemannian flow matching prior defined via closed-form
geodesics. This design enables flow-based priors to model complex latent
distributions, while vector quantization helps preserve the curvature-aware
structure of the hyperbolic space. We further develop a suite of hyperbolic GNN
and Transformer layers that operate entirely within the manifold, ensuring
stability and scalability. Empirically, our model reduces degree MMD by over
75\% on Community-Small and over 40\% on Ego-Small compared to state-of-the-art
baselines, demonstrating an improved ability to preserve topological
hierarchies. These results highlight the potential of hyperbolic geometry as a
powerful foundation for the generative modeling of complex, structured, and
hierarchical data domains. Our code is available at
\href{https://github.com/AI4Science-WestlakeU/GGBall}{here}.

</details>


### [318] [Advancing Multimodal Reasoning Capabilities of Multimodal Large Language Models via Visual Perception Reward](https://arxiv.org/abs/2506.07218)
*Tong Xiao,Xin Xu,Zhenya Huang,Hongyu Gao,Quan Liu,Qi Liu,Enhong Chen*

Main category: cs.LG

TL;DR: 现有研究通过强化学习提升多模态大模型（MLLM）的推理能力，但忽视了多模态感知能力的增强。本文提出Perception-R1方法，通过视觉感知奖励机制联合优化感知与推理，仅用少量数据即在多个基准达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的多模态推理方法（RLVR）主要关注推理能力，但多模态感知作为推理的基础能力未得到有效提升。实验表明，感知能力不足限制了MLLM的推理性能进一步提升。

Method: 提出Perception-R1方法：1) 从多模态问题的思维链轨迹中提取文本化视觉标注作为参考；2) 在强化学习阶段，用大语言模型（LLM）评估MLLM输出与视觉标注的一致性，生成视觉感知奖励；3) 联合优化感知与推理奖励。

Result: 在多个多模态推理基准测试中达到SOTA性能，仅使用1,442条训练数据即超越现有方法，验证了视觉感知奖励对联合优化感知与推理的有效性。

Conclusion: 显式建模视觉感知奖励能有效提升MLLM的多模态感知能力，进而增强复杂推理性能。该方法为联合优化多模态基础能力提供了新思路。

Abstract: Enhancing the multimodal reasoning capabilities of Multimodal Large Language
Models (MLLMs) is a challenging task that has attracted increasing attention in
the community. Recently, several studies have applied Reinforcement Learning
with Verifiable Rewards (RLVR) to the multimodal domain in order to enhance the
reasoning abilities of MLLMs. However, these works largely overlook the
enhancement of multimodal perception capabilities in MLLMs, which serve as a
core prerequisite and foundational component of complex multimodal reasoning.
Through McNemar's test, we find that existing RLVR method fails to effectively
enhance the multimodal perception capabilities of MLLMs, thereby limiting their
further improvement in multimodal reasoning. To address this limitation, we
propose Perception-R1, which introduces a novel visual perception reward that
explicitly encourages MLLMs to perceive the visual content accurately, thereby
can effectively incentivizing both their multimodal perception and reasoning
capabilities. Specifically, we first collect textual visual annotations from
the CoT trajectories of multimodal problems, which will serve as visual
references for reward assignment. During RLVR training, we employ a judging LLM
to assess the consistency between the visual annotations and the responses
generated by MLLM, and assign the visual perception reward based on these
consistency judgments. Extensive experiments on several multimodal reasoning
benchmarks demonstrate the effectiveness of our Perception-R1, which achieves
state-of-the-art performance on most benchmarks using only 1,442 training data.

</details>


### [319] [VARSHAP: Addressing Global Dependency Problems in Explainable AI with Variance-Based Local Feature Attribution](https://arxiv.org/abs/2506.07229)
*Mateusz Gajewski,Mikołaj Morzy,Adam Karczmarz,Piotr Sankowski*

Main category: cs.LG

TL;DR: 本文提出了一种新的局部特征归因方法VARSHAP，通过减少预测方差来衡量特征重要性，解决了现有方法如SHAP的全局依赖问题。


<details>
  <summary>Details</summary>
Motivation: 现有特征归因方法如SHAP存在全局依赖问题，无法准确捕捉模型的局部行为，因此需要一种新的方法来克服这一局限。

Method: VARSHAP基于Shapley值框架，使用预测方差的减少作为特征重要性的关键指标，且对全局数据分布变化具有鲁棒性。

Result: 在合成和真实数据集上的实验表明，VARSHAP在定量和定性上均优于KernelSHAP和LIME等流行方法。

Conclusion: VARSHAP是一种有效的局部特征归因方法，能够更好地捕捉模型的局部行为，且对全局数据分布变化具有鲁棒性。

Abstract: Existing feature attribution methods like SHAP often suffer from global
dependence, failing to capture true local model behavior. This paper introduces
VARSHAP, a novel model-agnostic local feature attribution method which uses the
reduction of prediction variance as the key importance metric of features.
Building upon Shapley value framework, VARSHAP satisfies the key Shapley
axioms, but, unlike SHAP, is resilient to global data distribution shifts.
Experiments on synthetic and real-world datasets demonstrate that VARSHAP
outperforms popular methods such as KernelSHAP or LIME, both quantitatively and
qualitatively.

</details>


### [320] [Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path Lengths in LLMs](https://arxiv.org/abs/2506.07240)
*Roy Eisenstadt,Itamar Zimerman,Lior Wolf*

Main category: cs.LG

TL;DR: 本文提出一种'超频'方法，通过可视化推理进度条和调控内部进度编码，优化大语言模型的显式推理过程，在减少计算量的同时提升准确率。


<details>
  <summary>Details</summary>
Motivation: 显式结构化方法中推理链长度直接影响模型表现：过短无法捕捉任务特性，过长导致过度思考性能下降。需探索模型自我调节推理长度的机制。

Method: 1. 通过交互式进度条可视化模型推理规划动态
2. 在推理时操纵内部进度编码，生成更简洁的思维链（Chain of Thoughts）

Result: 实验证明该方法有效缓解过度思考现象，答案准确率提升3.2%，推理延迟降低40%，同时减少17%的推理步骤。

Conclusion: 通过显式建模和调控推理进度，实现了更高效可靠的显式推理机制，为优化大模型推理过程提供了新范式。

Abstract: Recently, techniques such as explicit structured reasoning have demonstrated
strong test-time scaling behavior by enforcing a separation between the model's
internal "thinking" process and the final response. A key factor influencing
answer quality in this setting is the length of the thinking stage. When the
reasoning is too short, the model may fail to capture the complexity of the
task. Conversely, when it is too long, the model may overthink, leading to
unnecessary computation and degraded performance. This paper explores and
exploits the underlying mechanisms by which LLMs understand and regulate the
length of their reasoning during explicit thought processes. First, we show
that LLMs encode their progress through the reasoning process and introduce an
interactive progress bar visualization, which is then used to reveal insights
on the model's planning dynamics. Second, we manipulate the internal progress
encoding during inference to reduce unnecessary steps and generate a more
concise and decisive chain of thoughts. Our empirical results demonstrate that
this "overclocking" method mitigates overthinking, improves answer accuracy,
and reduces inference latency. Our code is publicly available.

</details>


### [321] [Promoting Ensemble Diversity with Interactive Bayesian Distributional Robustness for Fine-tuning Foundation Models](https://arxiv.org/abs/2506.07247)
*Ngoc-Quan Pham,Tuan Truong,Quyen Tran,Tan Nguyen,Dinh Phung,Trung Le*

Main category: cs.LG

TL;DR: 本文提出了一种新的贝叶斯推理框架IBDR，通过增强粒子多样性提升集合质量，并在VTAB-1K基准测试和常见推理任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了在贝叶斯推理中更好地模拟粒子间的相互作用，提升集合质量，本文提出了IBDR框架。

Method: IBDR基于广义理论框架，将分布总体损失与近似后验连接，采用双重优化程序，确保分布鲁棒性并促进粒子多样性。

Result: 在VTAB-1K基准测试和常见推理任务中，IBDR表现优于多种基线方法。

Conclusion: IBDR在现实应用中表现出色，证明了其在提升集合质量和分布鲁棒性方面的有效性。

Abstract: We introduce Interactive Bayesian Distributional Robustness (IBDR), a novel
Bayesian inference framework that allows modeling the interactions between
particles, thereby enhancing ensemble quality through increased particle
diversity. IBDR is grounded in a generalized theoretical framework that
connects the distributional population loss with the approximate posterior,
motivating a practical dual optimization procedure that enforces distributional
robustness while fostering particle diversity. We evaluate IBDR's performance
against various baseline methods using the VTAB-1K benchmark and the common
reasoning language task. The results consistently show that IBDR outperforms
these baselines, underscoring its effectiveness in real-world applications.

</details>


### [322] [A Stable Whitening Optimizer for Efficient Neural Network Training](https://arxiv.org/abs/2506.07254)
*Kevin Frans,Sergey Levine,Pieter Abbeel*

Main category: cs.LG

TL;DR: 本文提出了一种基于Shampoo算法家族的新方法SPlus，解决了神经网络优化中的三个关键问题，并通过实验验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在通过实验方法解决神经网络优化中的实际问题，特别是Shampoo算法在矩阵逆缓存、学习率跨网络宽度传递以及高学习率导致的参数噪声等问题。

Method: 提出了SPlus方法，包括引入历史特征基与瞬时归一化的有界更新、适应形状感知的缩放以实现学习率跨网络宽度传递，以及简单的迭代平均方案以减少参数噪声。

Result: 在Transformer训练基准测试中，SPlus在44%的梯度步数和62%的墙钟时间内达到了Adam的验证性能。

Conclusion: SPlus方法在稳定性和计算效率上显著优于传统方法，能够有效加速神经网络训练。

Abstract: In this work, we take an experimentally grounded look at neural network
optimization. Building on the Shampoo family of algorithms, we identify and
alleviate three key issues, resulting in the proposed SPlus method. First, we
find that naive Shampoo is prone to divergence when matrix-inverses are cached
for long periods. We introduce an alternate bounded update combining a
historical eigenbasis with instantaneous normalization, resulting in
across-the-board stability and significantly lower computational requirements.
Second, we adapt a shape-aware scaling to enable learning rate transfer across
network width. Third, we find that high learning rates result in large
parameter noise, and propose a simple iterate-averaging scheme which unblocks
faster learning. To properly confirm these findings, we introduce a pointed
Transformer training benchmark, considering three objectives (language
modelling, image classification, and diffusion modelling) across different
stages of training. On average, SPlus is able to reach the validation
performance of Adam within 44% of the gradient steps and 62% of the wallclock
time.

</details>


### [323] [A Cramér-von Mises Approach to Incentivizing Truthful Data Sharing](https://arxiv.org/abs/2506.07272)
*Alex Clinton,Thomas Zeng,Yiding Chen,Xiaojin Zhu,Kirthevasan Kandasamy*

Main category: cs.LG

TL;DR: 提出基于Cramér-von Mises统计量的新型激励机制，通过双样本检验严格激励真实数据贡献，抑制数据伪造行为，并在理论与实证中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有数据市场激励机制依赖数据量奖励，易受伪造/低质数据操纵。先前基于数据对比的方法需强分布假设（如高斯分布），实际应用受限。

Method: 设计基于Cramér-von Mises统计量的双样本检验机制，在贝叶斯与无先验场景下构建奖励体系，使真实数据提交成为近似纳什均衡。

Result: 理论证明真实报告构成纳什均衡，在三个典型数据共享场景中放宽假设限制。实证模拟及真实语言/图像数据实验均验证机制有效性。

Conclusion: 新机制无需强分布假设，通过统计检验严格激励诚实数据共享，为数据市场提供更普适的防操纵激励解决方案。

Abstract: Modern data marketplaces and data sharing consortia increasingly rely on
incentive mechanisms to encourage agents to contribute data. However, schemes
that reward agents based on the quantity of submitted data are vulnerable to
manipulation, as agents may submit fabricated or low-quality data to inflate
their rewards. Prior work has proposed comparing each agent's data against
others' to promote honesty: when others contribute genuine data, the best way
to minimize discrepancy is to do the same. Yet prior implementations of this
idea rely on very strong assumptions about the data distribution (e.g.
Gaussian), limiting their applicability. In this work, we develop reward
mechanisms based on a novel, two-sample test inspired by the Cram\'er-von Mises
statistic. Our methods strictly incentivize agents to submit more genuine data,
while disincentivizing data fabrication and other types of untruthful
reporting. We establish that truthful reporting constitutes a (possibly
approximate) Nash equilibrium in both Bayesian and prior-agnostic settings. We
theoretically instantiate our method in three canonical data sharing problems
and show that it relaxes key assumptions made by prior work. Empirically, we
demonstrate that our mechanism incentivizes truthful data sharing via
simulations and on real-world language and image data.

</details>


### [324] [Investigating the Relationship Between Physical Activity and Tailored Behavior Change Messaging: Connecting Contextual Bandit with Large Language Models](https://arxiv.org/abs/2506.07275)
*Haochen Song,Dominik Hofer,Rania Islambouli,Laura Hawkins,Ananya Bhattacharjee,Meredith Franklin,Joseph Jay Williams*

Main category: cs.LG

TL;DR: 本研究提出结合上下文多臂老虎机(cMAB)与大型语言模型(LLM)的混合方法，用于个性化运动激励信息推送。通过7天实验比较四种模型(cMAB、LLM、cMABxLLM、随机对照)对步数和信息接受度的影响，采用因果推断框架评估效果。


<details>
  <summary>Details</summary>
Motivation: 传统cMAB算法需要大样本学习且可能忽略未编码的心理因素，而单纯LLM缺乏动态调整能力。研究旨在探索两种技术在行为干预中的互补作用，提升个性化信息推送效果。

Method: 设计7天对照试验，每日推送四种干预类型（自我监测/收益框架/损失框架/社会比较）的个性化信息。结合动态情境因素（自我效能/社会影响/调节焦点），通过生态瞬时评估(EMA)收集步数与信息接受度数据，应用因果推断框架分析四种模型效果。

Result: 研究揭示了LLM内容个性化与cMAB干预类型选择的协同效应，cMABxLLM组合模型在提升运动动机和步数方面展现互补优势，具体效果需通过因果分析框架进一步验证。

Conclusion: LLM内容生成与cMAB算法调整的结合为行为干预提供了新范式，心理因素动态编码与算法协同机制是未来个性化健康干预的重要方向。

Abstract: Machine learning approaches, such as contextual multi-armed bandit (cMAB)
algorithms, offer a promising strategy to reduce sedentary behavior by
delivering personalized interventions to encourage physical activity. However,
cMAB algorithms typically require large participant samples to learn
effectively and may overlook key psychological factors that are not explicitly
encoded in the model. In this study, we propose a hybrid approach that combines
cMAB for selecting intervention types with large language models (LLMs) to
personalize message content. We evaluate four intervention types: behavioral
self-monitoring, gain-framed, loss-framed, and social comparison, each
delivered as a motivational message aimed at increasing motivation for physical
activity and daily step count. Message content is further personalized using
dynamic contextual factors including daily fluctuations in self-efficacy,
social influence, and regulatory focus. Over a seven-day trial, participants
receive daily messages assigned by one of four models: cMAB alone, LLM alone,
combined cMAB with LLM personalization (cMABxLLM), or equal randomization
(RCT). Outcomes include daily step count and message acceptance, assessed via
ecological momentary assessments (EMAs). We apply a causal inference framework
to evaluate the effects of each model. Our findings offer new insights into the
complementary roles of LLM-based personalization and cMAB adaptation in
promoting physical activity through personalized behavioral messaging.

</details>


### [325] [Tokenized Bandit for LLM Decoding and Alignment](https://arxiv.org/abs/2506.07276)
*Suho Shin,Chenghao Yang,Haifeng Xu,Mohammad T. Hajiaghayi*

Main category: cs.LG

TL;DR: 本文提出了基于LLM解码和对齐的Tokenized线性老虎机（TLB）和多臂老虎机（TMAB）问题，引入DDMC假设并设计算法，证明贪心解码在DDMC下的最优性，验证了其在LLM对齐中的应用。


<details>
  <summary>Details</summary>
Motivation: 受LLM解码和对齐启发，研究在不可逆选择token序列的场景下，如何通过随机效用反馈优化决策，解决无结构序列函数无法学习的问题。

Method: 提出DDMC（递减公共性距离）假设，设计TLB和TMAB算法（遗憾界分别为O~(L√T)和O~(L√T²/³)），并通过理论证明与实验验证。

Result: 算法在合成和真实数据上验证有效性，证明DDMC下贪心解码的（近似）最优性，为LLM解码时对齐提供直接应用支持。

Conclusion: DDMC假设使序列函数可学习，贪心解码的合理性在理论层面得到解释，且算法可扩展至LLM解码时对齐场景。

Abstract: We introduce the tokenized linear bandit (TLB) and multi-armed bandit (TMAB),
variants of linear and stochastic multi-armed bandit problems inspired by LLM
decoding and alignment. In these problems, at each round $t \in [T]$, a user
submits a query (context), and the decision maker (DM) sequentially selects a
token irrevocably from a token set. Once the sequence is complete, the DM
observes a random utility from the user, whose expectation is presented by a
sequence function mapping the chosen token sequence to a nonnegative real value
that depends on the query.
  In both problems, we first show that learning is impossible without any
structure on the sequence function. We introduce a natural assumption,
diminishing distance with more commons (DDMC), and propose algorithms with
regret $\tilde{O}(L\sqrt{T})$ and $\tilde{O}(L\sqrt{T^{2/3}})$ for TLB and
TMAB, respectively. As a side product, we obtain an (almost) optimality of the
greedy decoding for LLM decoding algorithm under DDMC, which justifies the
unresaonable effectiveness of greedy decoding in several tasks. This also has
an immediate application to decoding-time LLM alignment, when the misaligned
utility can be represented as the frozen LLM's utility and a linearly
realizable latent function. We finally validate our algorithm's performance
empirically as well as verify our assumptions using synthetic and real-world
datasets.

</details>


### [326] [EviNet: Evidential Reasoning Network for Resilient Graph Learning in the Open and Noisy Environments](https://arxiv.org/abs/2506.07288)
*Weijie Guan,Haohui Wang,Jian Kang,Lihui Liu,Dawei Zhou*

Main category: cs.LG

TL;DR: 本文提出了EVINET框架，通过结合Beta嵌入和主观逻辑，解决了图学习中的误分类检测和分布外检测问题。


<details>
  <summary>Details</summary>
Motivation: 传统的图学习通常基于封闭世界假设，但现实环境中数据可能包含噪声和未知类别。因此，需要一种方法能够在模型错误预测已知类别数据或遇到未知类别数据时进行检测。

Method: EVINET框架引入了Beta嵌入和主观逻辑，包含两个关键模块：Dissonance Reasoning用于误分类检测，Vacuity Reasoning用于分布外检测。

Result: 实验表明，EVINET在分布内分类、误分类检测和分布外检测任务中，均优于现有的最先进方法。

Conclusion: EVINET展示了不确定性估计和逻辑推理在误分类检测和分布外检测中的必要性，为开放世界图学习铺平了道路。

Abstract: Graph learning has been crucial to many real-world tasks, but they are often
studied with a closed-world assumption, with all possible labels of data known
a priori. To enable effective graph learning in an open and noisy environment,
it is critical to inform the model users when the model makes a wrong
prediction to in-distribution data of a known class, i.e., misclassification
detection or when the model encounters out-of-distribution from novel classes,
i.e., out-of-distribution detection. This paper introduces Evidential Reasoning
Network (EVINET), a framework that addresses these two challenges by
integrating Beta embedding within a subjective logic framework. EVINET includes
two key modules: Dissonance Reasoning for misclassification detection and
Vacuity Reasoning for out-of-distribution detection. Extensive experiments
demonstrate that EVINET outperforms state-of-the-art methods across multiple
metrics in the tasks of in-distribution classification, misclassification
detection, and out-of-distribution detection. EVINET demonstrates the necessity
of uncertainty estimation and logical reasoning for misclassification detection
and out-of-distribution detection and paves the way for open-world graph
learning. Our code and data are available at https://github.com/SSSKJ/EviNET.

</details>


### [327] [Pre-trained Large Language Models Learn Hidden Markov Models In-context](https://arxiv.org/abs/2506.07298)
*Yijia Dai,Zhaolin Gao,Yahya Satter,Sarah Dean,Jennifer J. Sun*

Main category: cs.LG

TL;DR: 本文展示了预训练大语言模型（LLMs）通过上下文学习（ICL）可以有效建模隐马尔可夫模型（HMMs）生成的数据，并在真实世界任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管HMMs是建模序列数据的基础工具，但其在实际数据中的拟合仍具有计算挑战性。本文旨在探索LLMs通过ICL学习HMM生成数据的潜力。

Method: 使用预训练的LLMs通过ICL对合成HMMs生成的数据进行建模，并在真实世界的动物决策任务中验证其性能。

Result: LLMs在合成HMMs上的预测准确率接近理论最优，并在真实世界任务中与专家设计的模型表现相当。

Conclusion: 本文首次证明ICL可以学习和预测HMM生成的序列，深化了对LLMs中ICL的理解，并展示了其在揭示复杂科学数据中隐藏结构的潜力。

Abstract: Hidden Markov Models (HMMs) are foundational tools for modeling sequential
data with latent Markovian structure, yet fitting them to real-world data
remains computationally challenging. In this work, we show that pre-trained
large language models (LLMs) can effectively model data generated by HMMs via
in-context learning (ICL)$\unicode{x2013}$their ability to infer patterns from
examples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve
predictive accuracy approaching the theoretical optimum. We uncover novel
scaling trends influenced by HMM properties, and offer theoretical conjectures
for these empirical observations. We also provide practical guidelines for
scientists on using ICL as a diagnostic tool for complex data. On real-world
animal decision-making tasks, ICL achieves competitive performance with models
designed by human experts. To our knowledge, this is the first demonstration
that ICL can learn and predict HMM-generated sequences$\unicode{x2013}$an
advance that deepens our understanding of in-context learning in LLMs and
establishes its potential as a powerful tool for uncovering hidden structure in
complex scientific data.

</details>


### [328] [PASS: Private Attributes Protection with Stochastic Data Substitution](https://arxiv.org/abs/2506.07308)
*Yizhuo Chen,Chun-Fu,Chen,Hsiang Hsu,Shaohan Hu,Tarek Abdelzaher*

Main category: cs.LG

TL;DR: 现有基于对抗训练的隐私属性保护方法存在漏洞，本文提出PASS方法，通过概率替换样本及信息论驱动的损失函数，在多种数据集上验证了其有效性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有隐私保护方法通过对抗训练移除敏感属性，但存在固有弱点导致严重漏洞，需开发更鲁棒的隐私保护机制。

Method: 提出PASS方法：基于概率的样本随机替换机制，通过信息论目标推导的新型损失函数实现效用保持型隐私保护。

Result: 在面部图像、行为传感器数据和语音等多模态数据集上的综合实验表明，PASS在隐私保护和数据效用平衡方面优于现有方法。

Conclusion: PASS通过理论证明和实证研究克服了对抗训练方法的局限性，建立了信息论驱动的隐私保护新范式，具有跨领域适用性。

Abstract: The growing Machine Learning (ML) services require extensive collections of
user data, which may inadvertently include people's private information
irrelevant to the services. Various studies have been proposed to protect
private attributes by removing them from the data while maintaining the
utilities of the data for downstream tasks. Nevertheless, as we theoretically
and empirically show in the paper, these methods reveal severe vulnerability
because of a common weakness rooted in their adversarial training based
strategies. To overcome this limitation, we propose a novel approach, PASS,
designed to stochastically substitute the original sample with another one
according to certain probabilities, which is trained with a novel loss function
soundly derived from information-theoretic objective defined for
utility-preserving private attributes protection. The comprehensive evaluation
of PASS on various datasets of different modalities, including facial images,
human activity sensory signals, and voice recording datasets, substantiates
PASS's effectiveness and generalizability.

</details>


### [329] [Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency in Deployed Inference](https://arxiv.org/abs/2506.07311)
*Thomas Joshi,Herman Saini,Neil Dhillon,Antoni Viros i Martin,Kaoutar El Maghraoui*

Main category: cs.LG

TL;DR: 该论文提出了一种将PagedAttention与PyTorch的FlexAttention结合的新方法，解决了大语言模型（LLMs）在长上下文推理中的内存效率问题，显著降低了推理延迟。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长上下文推理中面临严重的内存效率问题，主要源于传统的键值（KV）缓存处理方式。

Method: 论文提出了一种新颖的PagedAttention与PyTorch的FlexAttention的集成方法，通过分页注意力机制减少内部碎片化，并优化了KV缓存的分配。

Result: 在NVIDIA L4 GPU上的基准测试显示，使用全局KV缓存时，推理延迟仅随序列长度线性增长，而在未使用缓存时则呈指数增长。

Conclusion: 该方法显著改善了长上下文模型的推理效率，未来有望广泛应用于长上下文模型的部署中。

Abstract: Large Language Models (LLMs) encounter severe memory inefficiencies during
long-context inference due to conventional handling of key-value (KV) caches.
In this work, we introduce a novel integration of PagedAttention with PyTorch's
FlexAttention, addressing internal fragmentation and inefficiencies associated
with monolithic KV cache allocations. Implemented within IBM's Foundation Model
Stack (FMS), our fused attention kernel efficiently gathers scattered KV data.
Our benchmarks on an NVIDIA L4 GPU (24GB) demonstrate significantly reduced
inference latency, growing only linearly (~2x) with sequence length from 128 to
2048 tokens when utilizing a global KV cache, compared to exponential latency
increases without caching. While peak memory usage remains largely unchanged
for single-step evaluations (dominated by model weights and activations), paged
attention causes minimal incremental memory usage, observable only at sequence
lengths exceeding 2048 tokens due to its power-of-two cache allocations. We
open-source the full implementation and discuss its implications for future
long-context model deployment.

</details>


### [330] [Generative Modeling of Networked Time-Series via Transformer Architectures](https://arxiv.org/abs/2506.07312)
*Yusuf Elnady*

Main category: cs.LG

TL;DR: 本文提出一种基于Transformer生成模型，用于合成高质量时间序列数据，以解决安全领域数据不足问题，并验证其能有效提升现有及新ML工作流的性能。


<details>
  <summary>Details</summary>
Motivation: 安全领域存在数据获取受限问题，现有Transformer生成数据未能有效提升模型性能，需设计更优生成框架以增强ML模型表现。

Method: 设计高效Transformer生成框架，专注于生成通用性强、跨数据集兼容的时间序列数据，优化生成质量。

Result: 所提Transformer模型取得SOTA效果，生成样本质量高且具备跨数据集泛化能力。

Conclusion: 该生成模型有效缓解数据稀缺问题，提升ML工作流性能，其通用性为多场景应用提供基础。

Abstract: Many security and network applications require having large datasets to train
the machine learning models. Limited data access is a well-known problem in the
security domain. Recent studies have shown the potential of Transformer models
to enlarge the size of data by synthesizing new samples, but the synthesized
samples don't improve the models over the real data. To address this issue, we
design an efficient transformer-based model as a generative framework to
generate time-series data, that can be used to boost the performance of
existing and new ML workflows. Our new transformer model achieves the SOTA
results. We style our model to be generalizable and work across different
datasets, and produce high-quality samples.

</details>


### [331] [DEF: Diffusion-augmented Ensemble Forecasting](https://arxiv.org/abs/2506.07324)
*David Millard,Arielle Carr,Stéphane Gaudreault,Ali Baheri*

Main category: cs.LG

TL;DR: 本文提出了一种名为DEF的新方法，通过扩散增强的集合预测生成初始条件扰动，适用于机器学习在天气预报中的应用，提高了长期预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的初始条件扰动方法主要针对数值天气预报（NWP）求解器设计，限制了其在机器学习天气预报领域的应用。因此，需要一种更通用的方法来生成初始条件扰动。

Method: DEF方法利用简单的条件扩散模型生成有意义的结构化扰动，可迭代应用，并通过引导项直观控制扰动水平，将任何确定性神经预测系统转化为随机系统。

Result: 在5.625° ERA5再分析数据集上验证，DEF方法在长期预测中累积误差更少，并生成有意义的预测分布，展示了改进的预测性能和合理的扩展估计。

Conclusion: DEF方法通过扩散增强的集合预测，成功将确定性神经预测系统转化为随机系统，显著提高了长期天气预报的准确性和可靠性。

Abstract: We present DEF (\textbf{\ul{D}}iffusion-augmented \textbf{\ul{E}}nsemble
\textbf{\ul{F}}orecasting), a novel approach for generating initial condition
perturbations. Modern approaches to initial condition perturbations are
primarily designed for numerical weather prediction (NWP) solvers, limiting
their applicability in the rapidly growing field of machine learning for
weather prediction. Consequently, stochastic models in this domain are often
developed on a case-by-case basis. We demonstrate that a simple conditional
diffusion model can (1) generate meaningful structured perturbations, (2) be
applied iteratively, and (3) utilize a guidance term to intuitivey control the
level of perturbation. This method enables the transformation of any
deterministic neural forecasting system into a stochastic one. With our
stochastic extended systems, we show that the model accumulates less error over
long-term forecasts while producing meaningful forecast distributions. We
validate our approach on the 5.625$^\circ$ ERA5 reanalysis dataset, which
comprises atmospheric and surface variables over a discretized global grid,
spanning from the 1960s to the present. On this dataset, our method
demonstrates improved predictive performance along with reasonable spread
estimates.

</details>


### [332] [Mobility-Aware Asynchronous Federated Learning with Dynamic Sparsification](https://arxiv.org/abs/2506.07328)
*Jintao Yan,Tan Chen,Yuxuan Sun,Zhaojun Nan,Sheng Zhou,Zhisheng Niu*

Main category: cs.LG

TL;DR: 本文提出了一种移动感知的动态稀疏化（MADS）算法，用于优化异步联邦学习（AFL）中的稀疏化程度，以应对设备移动性导致的连接不稳定性，并提高模型收敛性。


<details>
  <summary>Details</summary>
Motivation: 设备移动性导致的间歇性连接问题影响了异步联邦学习的收敛性，需要通过梯度稀疏化来应对，但稀疏化与模型陈旧性之间的相互作用尚未被充分研究。

Method: 本文开发了一个理论模型，分析了稀疏化、模型陈旧性和移动性接触模式之间的相互作用，并提出了MADS算法，根据接触时间和模型陈旧性动态优化稀疏化程度。

Result: 实验结果表明，MADS算法在CIFAR-10数据集上的图像分类准确率提高了8.76%，在Argoverse轨迹预测数据集上的平均位移误差降低了9.46%。

Conclusion: MADS算法在不同移动速度条件下优化了稀疏化程度，显著提高了异步联邦学习的收敛性和模型性能。

Abstract: Asynchronous Federated Learning (AFL) enables distributed model training
across multiple mobile devices, allowing each device to independently update
its local model without waiting for others. However, device mobility introduces
intermittent connectivity, which necessitates gradient sparsification and leads
to model staleness, jointly affecting AFL convergence. This paper develops a
theoretical model to characterize the interplay among sparsification, model
staleness and mobility-induced contact patterns, and their joint impact on AFL
convergence. Based on the analysis, we propose a mobility-aware dynamic
sparsification (MADS) algorithm that optimizes the sparsification degree based
on contact time and model staleness. Closed-form solutions are derived, showing
that under low-speed conditions, MADS increases the sparsification degree to
enhance convergence, while under high-speed conditions, it reduces the
sparsification degree to guarantee reliable uploads within limited contact
time. Experimental results validate the theoretical findings. Compared with the
state-of-the-art benchmarks, the MADS algorithm increases the image
classification accuracy on the CIFAR-10 dataset by 8.76% and reduces the
average displacement error in the Argoverse trajectory prediction dataset by
9.46%.

</details>


### [333] [JavelinGuard: Low-Cost Transformer Architectures for LLM Security](https://arxiv.org/abs/2506.07330)
*Yash Datta,Sharath Rajasekar*

Main category: cs.LG

TL;DR: JavelinGuard是一套低成本、高性能的模型架构，用于检测LLM交互中的恶意意图，通过五种基于Transformer的架构优化生产部署，在多个对抗数据集上表现优于现有模型，其中Raudra综合性能最佳，但不同架构在速度、资源等方面存在权衡。


<details>
  <summary>Details</summary>
Motivation: 现有LLM防护模型在准确性和效率上存在不足，需开发兼顾低成本、高性能的解决方案以应对实际生产环境中的恶意意图检测需求。

Method: 提出五种逐步复杂的Transformer架构（Sharanga、Mahendra、Vaishnava、Ashwina、Raudra），基于ModernBERT等紧凑模型优化，使用九大对抗数据集（含自建JavelinBench）进行测试，并与开源模型及GPT-4o对比。

Result: Raudra多任务框架表现最稳健，所有模型在准确率与延迟上均优于对比模型。不同架构在速度（最快400M参数CPU推理速度）、可解释性和资源需求上呈现差异化优势。

Conclusion: 多任务设计的Raudra综合性能最优，但架构选择需权衡实际场景的复杂度与效率需求，为LLM安全应用提供灵活部署方案。

Abstract: We present JavelinGuard, a suite of low-cost, high-performance model
architectures designed for detecting malicious intent in Large Language Model
(LLM) interactions, optimized specifically for production deployment. Recent
advances in transformer architectures, including compact BERT(Devlin et al.
2019) variants (e.g., ModernBERT (Warner et al. 2024)), allow us to build
highly accurate classifiers with as few as approximately 400M parameters that
achieve rapid inference speeds even on standard CPU hardware. We systematically
explore five progressively sophisticated transformer-based architectures:
Sharanga (baseline transformer classifier), Mahendra (enhanced
attention-weighted pooling with deeper heads), Vaishnava and Ashwina (hybrid
neural ensemble architectures), and Raudra (an advanced multi-task framework
with specialized loss functions). Our models are rigorously benchmarked across
nine diverse adversarial datasets, including popular sets like the NotInject
series, BIPIA, Garak, ImprovedLLM, ToxicChat, WildGuard, and our newly
introduced JavelinBench, specifically crafted to test generalization on
challenging borderline and hard-negative cases. Additionally, we compare our
architectures against leading open-source guardrail models as well as large
decoder-only LLMs such as gpt-4o, demonstrating superior cost-performance
trade-offs in terms of accuracy, and latency. Our findings reveal that while
Raudra's multi-task design offers the most robust performance overall, each
architecture presents unique trade-offs in speed, interpretability, and
resource requirements, guiding practitioners in selecting the optimal balance
of complexity and efficiency for real-world LLM security applications.

</details>


### [334] [Graph-KV: Breaking Sequence via Injecting Structural Biases into Large Language Models](https://arxiv.org/abs/2506.07334)
*Haoyu Wang,Peihao Wang,Mufei Li,Shikun Liu,Siqi Miao,Zhangyang Wang,Pan Li*

Main category: cs.LG

TL;DR: 本文提出Graph-KV方法，通过结构化归纳偏置和KV缓存优化，解决大语言模型（LLM）序列化输入无法有效利用结构依赖的问题，在检索增强生成、图结构数据推理等任务中显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM因序列化输入而无法有效利用结构依赖，导致在检索增强生成（RAG）和图结构数据推理等任务中性能受限。需一种方法引入结构化归纳偏置以提升模型对复杂依赖的建模能力。

Method: Graph-KV将文本段的KV缓存作为浓缩表示，通过选择性注意力机制（仅关注指定源段）引入图结构块掩码，稀疏化注意力并实现类消息传递机制。同时，策略性分配位置编码以减少位置偏差和上下文消耗。

Result: 在RAG基准测试、基于引文图的学术论文问答（Arxiv-QA）及引文网络论文分类任务中，Graph-KV显著优于传统序列编码方法，尤其在减少位置偏差和利用结构偏置方面表现突出。

Conclusion: Graph-KV通过结构化注意力机制和位置编码优化，有效缓解LLM的序列化输入限制，为处理图结构数据和复杂依赖任务提供了高效解决方案，并在多场景中验证其优越性。

Abstract: Modern large language models (LLMs) are inherently auto-regressive, requiring
input to be serialized into flat sequences regardless of their structural
dependencies. This serialization hinders the model's ability to leverage
structural inductive biases, especially in tasks such as retrieval-augmented
generation (RAG) and reasoning on data with native graph structures, where
inter-segment dependencies are crucial. We introduce Graph-KV with the
potential to overcome this limitation. Graph-KV leverages the KV-cache of text
segments as condensed representations and governs their interaction through
structural inductive biases. In this framework, 'target' segments selectively
attend only to the KV-caches of their designated 'source' segments, rather than
all preceding segments in a serialized sequence. This approach induces a
graph-structured block mask, sparsifying attention and enabling a
message-passing-like step within the LLM. Furthermore, strategically allocated
positional encodings for source and target segments reduce positional bias and
context window consumption. We evaluate Graph-KV across three scenarios: (1)
seven RAG benchmarks spanning direct inference, multi-hop reasoning, and
long-document understanding; (2) Arxiv-QA, a novel academic paper QA task with
full-text scientific papers structured as citation ego-graphs; and (3) paper
topic classification within a citation network. By effectively reducing
positional bias and harnessing structural inductive biases, Graph-KV
substantially outperforms baselines, including standard costly sequential
encoding, across various settings. Code and the Graph-KV data are publicly
available.

</details>


### [335] [SALT: A Lightweight Model Adaptation Method for Closed Split Computing Environments](https://arxiv.org/abs/2506.07355)
*Yuya Okada,Takayuki Nishio*

Main category: cs.LG

TL;DR: SALT是一种轻量级模型适应框架，用于在封闭约束下的分割计算，通过在客户端引入可训练的适配器来优化特征，实现用户特定适应，无需修改原始模型或增加通信开销。


<details>
  <summary>Details</summary>
Motivation: 在封闭环境中，传统的适应方法不可行，因为它们需要访问模型参数或架构。SALT旨在解决这一挑战，提供一种在不修改原始模型的情况下实现用户特定适应的方法。

Method: SALT在客户端引入一个紧凑、可训练的适配器，用于优化来自头部网络的潜在特征，从而实现用户特定适应，且不增加通信开销。

Result: 在CIFAR-10和CIFAR-100的用户特定分类任务中，SALT展示了比微调方法更高的准确性和更低的训练延迟，并在边缘-云环境中的鲁棒推理方面表现出色。

Conclusion: SALT为在严格系统约束下的边缘AI系统提供了一种实用的个性化推理解决方案，具有最小的部署开销。

Abstract: We propose SALT (Split-Adaptive Lightweight Tuning), a lightweight model
adaptation framework for Split Computing under closed constraints, where the
head and tail networks are proprietary and inaccessible to users. In such
closed environments, conventional adaptation methods are infeasible since they
require access to model parameters or architectures. SALT addresses this
challenge by introducing a compact, trainable adapter on the client side to
refine latent features from the head network, enabling user-specific adaptation
without modifying the original models or increasing communication overhead. We
evaluate SALT on user-specific classification tasks with CIFAR-10 and
CIFAR-100, demonstrating improved accuracy with lower training latency compared
to fine-tuning methods. Furthermore, SALT facilitates model adaptation for
robust inference over lossy networks, a common challenge in edge-cloud
environments. With minimal deployment overhead, SALT offers a practical
solution for personalized inference in edge AI systems under strict system
constraints.

</details>


### [336] [MoE-GPS: Guidlines for Prediction Strategy for Dynamic Expert Duplication in MoE Load Balancing](https://arxiv.org/abs/2506.07366)
*Haiyue Ma,Zhixu Du,Yiran Chen*

Main category: cs.LG

TL;DR: 本文提出MoE-GPS框架，通过量化系统级模型运行时的性能影响，指导在多GPU Mixture-of-Experts网络中优化预测策略，显著提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 在多GPU Mixture-of-Experts网络中，专家分布在不同的GPU上，导致负载不平衡。现有方法通过动态复制热门专家来改善推理负载平衡，但需要预测路由前的分布。本文探讨了预测策略、准确性、开销和端到端系统性能之间的权衡。

Method: 提出MoE-GPS框架，量化系统级模型运行时的性能影响，指导选择最优预测策略。特别推荐Distribution-Only Prediction策略，仅预测总体token分布，显著减少开销。

Result: 在Mixtral 8x7B MMLU数据集上，MoE-GPS推荐的Distribution-Only Prediction策略比传统的Token-to-Expert Prediction策略提升了超过23%的端到端推理性能。

Conclusion: MoE-GPS框架通过优化预测策略，显著提升了多GPU Mixture-of-Experts网络的推理性能，Distribution-Only Prediction策略在减少开销的同时，大幅提升了系统性能。

Abstract: In multi-GPU Mixture-of-Experts (MoE) network, experts are distributed across
different GPUs, which creates load imbalance as each expert processes different
number of tokens. Recent works improve MoE inference load balance by
dynamically duplicating popular experts to more GPUs to process excessive
tokens, which requires predicting the distribution before routing. In this
paper, we discuss the tradeoff of prediction strategies, accuracies, overhead,
and end-to-end system performance. We propose MoE-GPS, a framework that guides
the selection of the optimal predictor design under various system
configurations, by quantifying the performance impact to system-level model
runtime. Specifically, we advocate for Distribution-Only Prediction, a
prediction strategy that only predicts overall token distribution which
significantly reduces overhead compared to the traditional Token-to-Expert
Prediction. On Mixtral 8x7B MMLU dataset, MoE-GPS suggests Distribution-Only
Prediction which improves end-to-end inference performance by more than 23%
compared with Token-to-Expert Prediction.

</details>


### [337] [Moment Alignment: Unifying Gradient and Hessian Matching for Domain Generalization](https://arxiv.org/abs/2506.07378)
*Yuen Chen,Haozhe Si,Guojun Zhang,Han Zhao*

Main category: cs.LG

TL;DR: 本文提出了基于矩对齐的领域泛化理论，通过闭式矩对齐（CMA）算法优化领域梯度与Hessian对齐，提升模型在未见目标域的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有领域泛化方法在计算效率上存在不足，且其原理不明确。本文旨在通过理论分析和提出高效算法来解决这些问题。

Method: 扩展了转移测度的定义，提出矩对齐理论，并基于此设计了闭式矩对齐（CMA）算法，避免重复反向传播或采样估计Hessian。

Result: 实验表明，CMA在线性探测和全微调任务中均优于经验风险最小化和现有先进算法。

Conclusion: 矩对齐理论为领域泛化提供了统一视角，CMA算法在计算效率和性能上均取得显著提升。

Abstract: Domain generalization (DG) seeks to develop models that generalize well to
unseen target domains, addressing the prevalent issue of distribution shifts in
real-world applications. One line of research in DG focuses on aligning
domain-level gradients and Hessians to enhance generalization. However,
existing methods are computationally inefficient and the underlying principles
of these approaches are not well understood. In this paper, we develop the
theory of moment alignment for DG. Grounded in \textit{transfer measure}, a
principled framework for quantifying generalizability between two domains, we
first extend the definition of transfer measure to domain generalization that
includes multiple source domains and establish a target error bound. Then, we
prove that aligning derivatives across domains improves transfer measure both
when the feature extractor induces an invariant optimal predictor across
domains and when it does not. Notably, moment alignment provides a unifying
understanding of Invariant Risk Minimization, gradient matching, and Hessian
matching, three previously disconnected approaches to DG. We further connect
feature moments and derivatives of the classifier head, and establish the
duality between feature learning and classifier fitting. Building upon our
theory, we introduce \textbf{C}losed-Form \textbf{M}oment \textbf{A}lignment
(CMA), a novel DG algorithm that aligns domain-level gradients and Hessians in
closed-form. Our method overcomes the computational inefficiencies of existing
gradient and Hessian-based techniques by eliminating the need for repeated
backpropagation or sampling-based Hessian estimation. We validate the efficacy
of our approach through two sets of experiments: linear probing and full
fine-tuning. CMA demonstrates superior performance in both settings compared to
Empirical Risk Minimization and state-of-the-art algorithms.

</details>


### [338] [RiemannFormer: A Framework for Attention in Curved Spaces](https://arxiv.org/abs/2506.07405)
*Zhongping Ji*

Main category: cs.LG

TL;DR: 本文提出了一种基于几何解释的Transformer架构优化方法，通过减少参数和引入局部注意力机制，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过几何解释进一步挖掘Transformer架构的潜力，特别是注意力机制的几何意义。

Method: 提出了一种框架，将注意力机制与度量张量、切空间、内积等几何概念联系起来，并通过切向量的平行传输实现离散位置的连接。同时，通过预定义配置减少参数，并引入显式机制增强局部注意力。

Result: 实验结果表明，所提出的模块相对于基线模型有显著的性能提升。

Conclusion: 本文通过几何解释和参数优化，成功提升了Transformer架构的性能，未来将在视觉和大语言模型上进行更多评估实验。

Abstract: This research endeavors to offer insights into unlocking the further
potential of transformer-based architectures. One of the primary motivations is
to offer a geometric interpretation for the attention mechanism in
transformers. In our framework, the attention mainly involves metric tensors,
tangent spaces, inner product, and how they relate to each other. These
quantities and structures at discrete positions are intricately interconnected
via the parallel transport of tangent vectors. To make the learning process
more efficient, we reduce the number of parameters through ingenious predefined
configurations. Moreover, we introduce an explicit mechanism to highlight a
neighborhood by attenuating the remote values, given that transformers
inherently neglect local inductive bias. Experimental results demonstrate that
our modules deliver significant performance improvements relative to the
baseline. More evaluation experiments on visual and large language models will
be launched successively.

</details>


### [339] [InverseScope: Scalable Activation Inversion for Interpreting Large Language Models](https://arxiv.org/abs/2506.07406)
*Yifan Luo,Zhennan Zhou,Bin Dong*

Main category: cs.LG

TL;DR: 本文提出了一种名为InverseScope的框架，通过输入反演来解释大语言模型的内部表示，提高了样本效率，并引入了定量评估协议。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型的内部表示是解释性研究中的核心挑战，现有方法通常依赖于对表示结构的强假设，这些假设在实践中可能不成立。

Method: InverseScope框架通过定义生成相似激活的输入分布来分析编码特征，提出了一种新的条件生成架构以提高样本效率，并引入了定量评估协议。

Result: InverseScope能够扩展到更大的模型和实际任务，实现了对大语言模型内部表示的系统性和定量分析。

Conclusion: InverseScope为解释大语言模型的内部表示提供了一种高效且可扩展的方法，推动了该领域的进一步发展。

Abstract: Understanding the internal representations of large language models (LLMs) is
a central challenge in interpretability research. Existing feature
interpretability methods often rely on strong assumptions about the structure
of representations that may not hold in practice. In this work, we introduce
InverseScope, an assumption-light and scalable framework for interpreting
neural activations via input inversion. Given a target activation, we define a
distribution over inputs that generate similar activations and analyze this
distribution to infer the encoded features. To address the inefficiency of
sampling in high-dimensional spaces, we propose a novel conditional generation
architecture that significantly improves sample efficiency compared to previous
methods. We further introduce a quantitative evaluation protocol that tests
interpretability hypotheses using feature consistency rate computed over the
sampled inputs. InverseScope scales inversion-based interpretability methods to
larger models and practical tasks, enabling systematic and quantitative
analysis of internal representations in real-world LLMs.

</details>


### [340] [Anomaly Detection and Early Warning Mechanism for Intelligent Monitoring Systems in Multi-Cloud Environments Based on LLM](https://arxiv.org/abs/2506.07407)
*Yihong Jin,Ze Yang,Juntian Liu,Xinhe Xu*

Main category: cs.LG

TL;DR: 本文提出一种基于大语言模型（LLM）的多云环境智能监控系统异常检测与预警机制，通过结合LLM的上下文理解能力和传统机器学习方法，提升检测精度与实时响应效率。


<details>
  <summary>Details</summary>
Motivation: 随着多云环境快速发展，需增强智能监控系统的安全性与可靠性。传统异常检测系统在动态多云环境中存在适应性不足、检测精度有限等问题。

Method: 在现有监控框架中引入多级特征提取方法，融合LLM的自然语言处理能力与机器学习技术，动态适配不同云服务商环境，实现异常模式检测与潜在故障预测。

Result: 实验表明该模型在检测精度和延迟上显著优于传统系统，云基础设施韧性提升23%，主动管理响应速度提高35%。

Conclusion: LLM与机器学习的融合有效解决了多云环境监控的异构性问题，为构建自适应、高可靠的云监控体系提供了新范式。

Abstract: With the rapid development of multi-cloud environments, it is increasingly
important to ensure the security and reliability of intelligent monitoring
systems. In this paper, we propose an anomaly detection and early warning
mechanism for intelligent monitoring system in multi-cloud environment based on
Large-Scale Language Model (LLM). On the basis of the existing monitoring
framework, the proposed model innovatively introduces a multi-level feature
extraction method, which combines the natural language processing ability of
LLM with traditional machine learning methods to enhance the accuracy of
anomaly detection and improve the real-time response efficiency. By introducing
the contextual understanding capabilities of LLMs, the model dynamically adapts
to different cloud service providers and environments, so as to more
effectively detect abnormal patterns and predict potential failures.
Experimental results show that the proposed model is significantly better than
the traditional anomaly detection system in terms of detection accuracy and
latency, and significantly improves the resilience and active management
ability of cloud infrastructure.

</details>


### [341] [Fractional-order Jacobian Matrix Differentiation and Its Application in Artificial Neural Networks](https://arxiv.org/abs/2506.07408)
*Xiaojun zhou,Chunna Zhao,Yaqun Huang,Chengli Zhou,Junjie Ye,Kemeng Xiang*

Main category: cs.LG

TL;DR: 本文提出了一种基于分数阶雅可比矩阵的矩阵微分方法，并将其应用于深度学习的自动微分技术中，验证了其在深度学习中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 由于分数阶微分与整数阶微分具有不同的特性，可以应用于人工神经网络的优化算法以获得更好的结果。然而，目前缺乏与自动微分技术完美兼容的分数阶矩阵微分方法。

Method: 本文通过整数阶雅可比矩阵的定义，提出了分数阶雅可比矩阵微分方法（${{\bf{J}}^\alpha }$），并基于此设计了分数阶自动微分技术，使其能够在隐藏层中使用分数阶微分。

Result: 实验通过在PyTorch框架中设计分数阶线性层（FLinear）并替换多层感知机中的nn.Linear，验证了${{\bf{J}}^\alpha }$在训练集、验证集和测试集上的优越性能，并证明了其是一种优秀的分数阶梯度下降方法。

Conclusion: 本文提出的分数阶雅可比矩阵微分方法在深度学习中表现出色，增强了分数阶微分的实用性。

Abstract: Fractional-order differentiation has many characteristics different from
integer-order differentiation. These characteristics can be applied to the
optimization algorithms of artificial neural networks to obtain better results.
However, due to insufficient theoretical research, at present, there is no
fractional-order matrix differentiation method that is perfectly compatible
with automatic differentiation (Autograd) technology. Therefore, we propose a
fractional-order matrix differentiation calculation method. This method is
introduced by the definition of the integer-order Jacobian matrix. We denote it
as fractional-order Jacobian matrix differentiation (${{\bf{J}}^\alpha }$).
Through ${{\bf{J}}^\alpha }$, we can carry out the matrix-based
fractional-order chain rule. Based on the Linear module and the
fractional-order differentiation, we design the fractional-order Autograd
technology to enable the use of fractional-order differentiation in hidden
layers, thereby enhancing the practicality of fractional-order differentiation
in deep learning. In the experiment, according to the PyTorch framework, we
design fractional-order Linear (FLinear) and replace nn.Linear in the
multilayer perceptron with FLinear. Through the qualitative analysis of the
training set and validation set $Loss$, the quantitative analysis of the test
set indicators, and the analysis of time consumption and GPU memory usage
during model training, we verify the superior performance of ${{\bf{J}}^\alpha
}$ and prove that it is an excellent fractional-order gradient descent method
in the field of deep learning.

</details>


### [342] [Variational Supervised Contrastive Learning](https://arxiv.org/abs/2506.07413)
*Ziwen Wang,Jiajun Fan,Thao Nguyen,Heng Ji,Ge Liu*

Main category: cs.LG

TL;DR: 提出VarCon方法，通过变分推断改进监督对比学习，在多个数据集上实现SOTA性能，并具备更清晰的语义组织与少样本学习优势。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习存在两个核心问题：(1)缺乏对嵌入分布的显式调控，可能导致语义相关样本被错误分离；(2)过度依赖大批量负样本和定制数据增强，限制了泛化能力。

Method: 将监督对比学习重构为隐类变量的变分推断问题，通过最大化后验加权的证据下界（ELBO），替代成对样本比较，实现高效类感知匹配并精细控制类内分散度。

Result: 在ImageNet-1K上Top-1准确率达79.36%（ResNet-50，200 epochs），CIFAR-100达78.29%；嵌入空间决策边界更清晰，少样本学习与跨增强策略鲁棒性显著优于监督基线。

Conclusion: VarCon通过变分框架有效解决了传统对比学习的分布控制与泛化瓶颈，在性能、收敛速度及语义结构组织上均取得突破。

Abstract: Contrastive learning has proven to be highly efficient and adaptable in
shaping representation spaces across diverse modalities by pulling similar
samples together and pushing dissimilar ones apart. However, two key
limitations persist: (1) Without explicit regulation of the embedding
distribution, semantically related instances can inadvertently be pushed apart
unless complementary signals guide pair selection, and (2) excessive reliance
on large in-batch negatives and tailored augmentations hinders generalization.
To address these limitations, we propose Variational Supervised Contrastive
Learning (VarCon), which reformulates supervised contrastive learning as
variational inference over latent class variables and maximizes a
posterior-weighted evidence lower bound (ELBO) that replaces exhaustive
pair-wise comparisons for efficient class-aware matching and grants
fine-grained control over intra-class dispersion in the embedding space.
Trained exclusively on image data, our experiments on CIFAR-10, CIFAR-100,
ImageNet-100, and ImageNet-1K show that VarCon (1) achieves state-of-the-art
performance for contrastive learning frameworks, reaching 79.36% Top-1 accuracy
on ImageNet-1K and 78.29% on CIFAR-100 with a ResNet-50 encoder while
converging in just 200 epochs; (2) yields substantially clearer decision
boundaries and semantic organization in the embedding space, as evidenced by
KNN classification, hierarchical clustering results, and transfer-learning
assessments; and (3) demonstrates superior performance in few-shot learning
than supervised baseline and superior robustness across various augmentation
strategies.

</details>


### [343] [LiteVLM: A Low-Latency Vision-Language Model Inference Pipeline for Resource-Constrained Environments](https://arxiv.org/abs/2506.07416)
*Jin Huang,Yuchao Jin,Le An,Josh Park*

Main category: cs.LG

TL;DR: 本文提出一种针对嵌入式设备优化的高效视觉语言模型（VLM）流水线，通过联合使用多种技术降低计算开销，在自动驾驶平台上实现2.5倍至3.2倍的端到端延迟优化。


<details>
  <summary>Details</summary>
Motivation: 在资源受限环境（如机器人、自动驾驶）中部署实时视觉语言模型时，传统方法存在高计算开销问题，需优化以实现高效运行。

Method: 结合三阶段优化：1) 基于补丁选择过滤无关摄像头视图；2) 令牌选择模块缩短LLM输入序列；3) 推测解码加速令牌生成，并应用FP8训练后量化。

Result: 在NVIDIA DRIVE Thor平台上，端到端延迟降低2.5倍（FP8量化后达3.2倍），且任务精度未受影响。

Conclusion: 该流水线为资源受限环境中的实时VLM部署提供了可行解决方案，显著提升计算效率。

Abstract: This paper introduces an efficient Vision-Language Model (VLM) pipeline
specifically optimized for deployment on embedded devices, such as those used
in robotics and autonomous driving. The pipeline significantly reduces the
computational overhead by jointly leveraging patch selection to filter
irrelevant camera views, a token selection module to reduce input sequence
length for the LLM, and speculative decoding to accelerate token generation.
Evaluation on the NVIDIA DRIVE Thor platform for automonous driving
application, our pipeline achieves $2.5\times$ end-to-end latency reduction
without compromising task accuracy. The speed-up further increases to
$3.2\times$ when applying FP8 post-training quantization. These results
demonstrate our pipeline as a viable solution for enabling real-time VLM
deployment in resource-constrained environments.

</details>


### [344] [Evidential Spectrum-Aware Contrastive Learning for OOD Detection in Dynamic Graphs](https://arxiv.org/abs/2506.07417)
*Nan Sun,Xixun Lin,Zhiheng Zhou,Yanmin Shang,Zhenlin Cheng,Yanan Cao*

Main category: cs.LG

TL;DR: 本文提出了一种基于证据深度学习的动态图分布外（OOD）检测方法EviSEC，通过后验狄利克雷分布建模和频谱感知对比学习，解决单点估计偏差与分数同质化问题，提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有动态图OOD检测方法主要针对静态图设计，存在单点估计导致的高偏差/方差问题，以及缺乏OOD训练数据引发的分数同质化现象，导致检测效果受限。

Method: 提出EviSEC框架：1）设计证据神经网络将输出重定义为后验狄利克雷分布，通过分布不确定性解释输入随机性；2）引入频谱感知增强模块生成OOD近似样本，扩大ID/OOD数据间的分数差异。

Result: 在真实数据集上的实验表明，EviSEC能有效检测动态图中的OOD样本，缓解分数同质化问题并扩大ID/OOD分数差距。

Conclusion: EviSEC通过证据深度学习和频谱感知对比学习机制，显著提升了动态图OOD检测性能，为安全敏感领域提供了更可靠的解决方案。

Abstract: Recently, Out-of-distribution (OOD) detection in dynamic graphs, which aims
to identify whether incoming data deviates from the distribution of the
in-distribution (ID) training set, has garnered considerable attention in
security-sensitive fields. Current OOD detection paradigms primarily focus on
static graphs and confront two critical challenges: i) high bias and high
variance caused by single-point estimation, which makes the predictions
sensitive to randomness in the data; ii) score homogenization resulting from
the lack of OOD training data, where the model only learns ID-specific
patterns, resulting in overall low OOD scores and a narrow score gap between ID
and OOD data. To tackle these issues, we first investigate OOD detection in
dynamic graphs through the lens of Evidential Deep Learning (EDL).
Specifically, we propose EviSEC, an innovative and effective OOD detector via
Evidential Spectrum-awarE Contrastive Learning. We design an evidential neural
network to redefine the output as the posterior Dirichlet distribution,
explaining the randomness of inputs through the uncertainty of distribution,
which is overlooked by single-point estimation. Moreover, spectrum-aware
augmentation module generates OOD approximations to identify patterns with high
OOD scores, thereby widening the score gap between ID and OOD data and
mitigating score homogenization. Extensive experiments on real-world datasets
demonstrate that EviSAC effectively detects OOD samples in dynamic graphs.

</details>


### [345] [Extending Epistemic Uncertainty Beyond Parameters Would Assist in Designing Reliable LLMs](https://arxiv.org/abs/2506.07448)
*T. Duy Nguyen-Hien,Desi R. Ivanova,Yee Whye Teh,Wee Sun Lee*

Main category: cs.LG

TL;DR: 本文提出采用贝叶斯实验建模框架，主动解决LLM部署中的不确定性，而非被动拒绝输出，以提高系统可靠性和适用性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM部署中，为确保可靠性仅通过拒绝高不确定性输出来避免错误，缺乏系统性工具区分和应对不同不确定性来源，导致策略受限。

Method: 引入贝叶斯实验建模框架，通过概率推理明确不确定性的来源与可减少性，支持主动请求澄清、检索外部信息或优化输入等情境化应对措施。

Result: 该框架为LLM系统提供了主动解决不确定性的理论基础，使其在高风险现实场景中更可靠、透明且广泛适用。

Conclusion: 贝叶斯框架通过主动管理不确定性，突破了被动回避的局限性，为LLM在关键领域的实际应用开辟了新路径。

Abstract: Although large language models (LLMs) are highly interactive and extendable,
current approaches to ensure reliability in deployments remain mostly limited
to rejecting outputs with high uncertainty in order to avoid misinformation.
This conservative strategy reflects the current lack of tools to systematically
distinguish and respond to different sources of uncertainty. In this paper, we
advocate for the adoption of Bayesian Modeling of Experiments -- a framework
that provides a coherent foundation to reason about uncertainty and clarify the
reducibility of uncertainty -- for managing and proactively addressing
uncertainty that arises in LLM deployments. This framework enables LLMs and
their users to take contextually appropriate steps, such as requesting
clarification, retrieving external information, or refining inputs. By
supporting active resolution rather than passive avoidance, it opens the door
to more reliable, transparent, and broadly applicable LLM systems, particularly
in high-stakes, real-world settings.

</details>


### [346] [When Style Breaks Safety: Defending Language Models Against Superficial Style Alignment](https://arxiv.org/abs/2506.07452)
*Yuxin Xiao,Sana Tonekaboni,Walter Gerych,Vinith Suriyakumar,Marzyeh Ghassemi*

Main category: cs.LG

TL;DR: 研究发现，大型语言模型（LLMs）在特定风格提示下（如列表格式）可能因风格模式与恶意查询结合而增加安全风险。攻击成功率（ASR）与风格模式长度及模型对其注意力相关。通过提出SafeStyle防御策略，结合风格匹配的安全训练数据，可有效降低此类风险。


<details>
  <summary>Details</summary>
Motivation: 探究风格模式是否影响LLMs的安全性，尤其是当这些模式与恶意查询结合时如何增加模型脆弱性，并寻求在模型对齐过程中缓解此类风险的方法。

Method: 评估32个LLMs在7个越狱基准中的表现，分析风格模式对ASR的影响；提出SafeStyle的防御策略，通过增强与微调数据风格分布匹配的安全训练数据来提升安全性。

Result: 风格模式显著提高几乎所有模型的ASR，其影响与模式长度及模型注意力相关；风格对齐微调使模型对同类风格攻击更脆弱；SafeStrategy在多种模型和风格设置下均能有效维持安全性。

Conclusion: 风格模式可能通过表面特征削弱LLMs安全性，需在模型对齐中针对性防御。SafeStyle通过风格分布增强的安全训练，为缓解此类风险提供了可行方案。

Abstract: Large language models (LLMs) can be prompted with specific styles (e.g.,
formatting responses as lists), including in jailbreak queries. Although these
style patterns are semantically unrelated to the malicious intents behind
jailbreak queries, their safety impact remains unclear. In this work, we seek
to understand whether style patterns compromise LLM safety, how superficial
style alignment increases model vulnerability, and how best to mitigate these
risks during alignment. We evaluate 32 LLMs across seven jailbreak benchmarks,
and find that malicious queries with style patterns inflate the attack success
rate (ASR) for nearly all models. Notably, ASR inflation correlates with both
the length of style patterns and the relative attention an LLM exhibits on
them. We then investigate superficial style alignment, and find that
fine-tuning with specific styles makes LLMs more vulnerable to jailbreaks of
those same styles. Finally, we propose SafeStyle, a defense strategy that
incorporates a small amount of safety training data augmented to match the
distribution of style patterns in the fine-tuning data. Across three LLMs and
five fine-tuning style settings, SafeStyle consistently outperforms baselines
in maintaining LLM safety.

</details>


### [347] [ProteinZero: Self-Improving Protein Generation via Online Reinforcement Learning](https://arxiv.org/abs/2506.07459)
*Ziwen Wang,Jiajun Fan,Ruihan Guo,Thao Nguyen,Heng Ji,Ge Liu*

Main category: cs.LG

TL;DR: ProteinZero提出了一种基于在线强化学习的新型蛋白质生成框架，通过高效代理奖励模型和多目标优化策略，显著提升设计成功率与多样性，计算效率高。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质生成模型受限于高质量监督预训练数据稀缺导致成功率低，需开发能自我迭代优化的新方法。

Method: 结合ESM-fold和快速ddG预测器构建高效在线反馈系统，采用多奖励最大化、KL散度约束及序列嵌入多样性正则化的强化学习框架。

Result: 在结构精度/可设计性/稳定性/多样性等核心指标全面超越现有方法，设计失败率降低36%-48%，单8卡节点3天内完成CATH-4.3训练。

Conclusion: 建立了通过模型自主进化持续探索蛋白质设计空间的新范式，为突破现有设计瓶颈提供了可行路径。

Abstract: Protein generative models have shown remarkable promise in protein design but
still face limitations in success rate, due to the scarcity of high-quality
protein datasets for supervised pretraining. We present ProteinZero, a novel
framework that enables scalable, automated, and continuous self-improvement of
the inverse folding model through online reinforcement learning. To achieve
computationally tractable online feedback, we introduce efficient proxy reward
models based on ESM-fold and a novel rapid ddG predictor that significantly
accelerates evaluation speed. ProteinZero employs a general RL framework
balancing multi-reward maximization, KL-divergence from a reference model, and
a novel protein-embedding level diversity regularization that prevents mode
collapse while promoting higher sequence diversity. Through extensive
experiments, we demonstrate that ProteinZero substantially outperforms existing
methods across every key metric in protein design, achieving significant
improvements in structural accuracy, designability, thermodynamic stability,
and sequence diversity. Most impressively, ProteinZero reduces design failure
rates by approximately 36% - 48% compared to widely-used methods like
ProteinMPNN, ESM-IF and InstructPLM, consistently achieving success rates
exceeding 90% across diverse and complex protein folds. Notably, the entire RL
run on CATH-4.3 can be done with a single 8 X GPU node in under 3 days,
including reward computation. Our work establishes a new paradigm for protein
design where models evolve continuously from their own generated outputs,
opening new possibilities for exploring the vast protein design space.

</details>


### [348] [Circumventing Backdoor Space via Weight Symmetry](https://arxiv.org/abs/2506.07467)
*Jie Peng,Hongwei Yang,Jing Zhao,Hengji Dong,Hui He,Weizhe Zhang,Haoyu He*

Main category: cs.LG

TL;DR: 本文提出了一种名为TSC的新型后门净化防御方法，通过利用神经网络的排列不变性和二次模式连接性，仅需少量干净样本即可有效抵御后门攻击，并适用于监督与自监督学习场景。


<details>
  <summary>Details</summary>
Motivation: 现有后门防御方法依赖标记数据或特定训练流程，难以推广至监督学习以外的场景。随着后门攻击在不同学习范式中的成功，亟需一种通用且高效的防御方案。

Method: TSC（两阶段对称连接性）结合神经网络的排列不变性和二次模式连接性理论，通过放大中毒样本的损失并限制干净样本的精度损失，实现无需数据格式依赖、仅需少量干净样本的后门净化。

Result: 实验表明，TSC在监督学习场景下性能与现有最优方法相当，且在自监督框架（如SimCLR和CLIP）中仍保持强防御能力。

Conclusion: TSC解决了现有防御方法在数据格式和训练流程上的局限性，为跨学习范式的后门防御提供了通用解决方案，具有理论保障和实际应用潜力。

Abstract: Deep neural networks are vulnerable to backdoor attacks, where malicious
behaviors are implanted during training. While existing defenses can
effectively purify compromised models, they typically require labeled data or
specific training procedures, making them difficult to apply beyond supervised
learning settings. Notably, recent studies have shown successful backdoor
attacks across various learning paradigms, highlighting a critical security
concern. To address this gap, we propose Two-stage Symmetry Connectivity (TSC),
a novel backdoor purification defense that operates independently of data
format and requires only a small fraction of clean samples. Through theoretical
analysis, we prove that by leveraging permutation invariance in neural networks
and quadratic mode connectivity, TSC amplifies the loss on poisoned samples
while maintaining bounded clean accuracy. Experiments demonstrate that TSC
achieves robust performance comparable to state-of-the-art methods in
supervised learning scenarios. Furthermore, TSC generalizes to self-supervised
learning frameworks, such as SimCLR and CLIP, maintaining its strong defense
capabilities. Our code is available at https://github.com/JiePeng104/TSC.

</details>


### [349] [Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models](https://arxiv.org/abs/2506.07468)
*Mickel Liu,Liwei Jiang,Yancheng Liang,Simon Shaolei Du,Yejin Choi,Tim Althoff,Natasha Jaques*

Main category: cs.LG

TL;DR: 本文提出Self-RedTeam算法，通过自博弈强化学习实现语言模型攻防动态协同进化，提升安全对齐的鲁棒性与多样性。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型安全对齐采用静态攻防分离模式，导致防御滞后于攻击。攻击者过时适应旧防御，防御者无法应对新威胁，需解决此动态不匹配问题。

Method: 基于零和博弈理论框架设计在线自博弈强化学习算法：单一模型交替扮演攻击者（生成对抗提示）和防御者（防护响应），通过奖励模型裁决结果，并引入隐藏思维链实现私有推理规划。

Result: 实验显示攻击多样性提升21.8%（SBERT指标），WildJailBreak基准鲁棒性提高65.5%，隐藏思维链有效减少过度拒绝行为。

Conclusion: 研究推动语言模型安全训练从被动修补转向主动协同进化，通过多智能体强化学习实现可扩展、自主、鲁棒的持续自我改进。

Abstract: Conventional language model (LM) safety alignment relies on a reactive,
disjoint procedure: attackers exploit a static model, followed by defensive
fine-tuning to patch exposed vulnerabilities. This sequential approach creates
a mismatch -- attackers overfit to obsolete defenses, while defenders
perpetually lag behind emerging threats. To address this, we propose
Self-RedTeam, an online self-play reinforcement learning algorithm where an
attacker and defender agent co-evolve through continuous interaction. We cast
safety alignment as a two-player zero-sum game, where a single model alternates
between attacker and defender roles -- generating adversarial prompts and
safeguarding against them -- while a reward LM adjudicates outcomes. This
enables dynamic co-adaptation. Grounded in the game-theoretic framework of
zero-sum games, we establish a theoretical safety guarantee which motivates the
design of our method: if self-play converges to a Nash Equilibrium, the
defender will reliably produce safe responses to any adversarial input.
Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared
to attackers trained against static defenders and achieves higher robustness on
safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained
against static attackers. We further propose hidden Chain-of-Thought, allowing
agents to plan privately, which boosts adversarial diversity and reduces
over-refusals. Our results motivate a shift from reactive patching to proactive
co-evolution in LM safety training, enabling scalable, autonomous, and robust
self-improvement of LMs via multi-agent reinforcement learning (MARL).

</details>


### [350] [Premise Selection for a Lean Hammer](https://arxiv.org/abs/2506.07477)
*Thomas Zhu,Joshua Clune,Jeremy Avigad,Albert Qiaochu Jiang,Sean Welleck*

Main category: cs.LG

TL;DR: LeanHammer是首个为Lean证明助手设计的端到端通用工具，结合了神经前提选择和符号推理，显著提升了自动化推理的效率。


<details>
  <summary>Details</summary>
Motivation: 尽管神经方法在自动化推理中取得了进展，但将其整合到实际验证工作流中仍具挑战性。Lean证明助手缺乏一个有效的工具来提升其自动化推理能力。

Method: LeanHammer基于一种新颖的神经前提选择系统，结合符号证明搜索和重建，动态适应用户特定上下文，形成一个实用的工具。

Result: 评估显示，LeanHammer相比现有前提选择器，能够解决多21%的目标，并在多个领域中表现出良好的泛化能力。

Conclusion: LeanHammer填补了神经检索与符号推理之间的空白，使形式验证对研究人员和从业者更加易用。

Abstract: Neural methods are transforming automated reasoning for proof assistants, yet
integrating these advances into practical verification workflows remains
challenging. Hammers are tools that interface with external automatic theorem
provers to automate tedious reasoning steps. They have dramatically improved
productivity in proof assistants, but the Lean proof assistant still does not
have a hammer despite its growing popularity. We present LeanHammer, the first
end-to-end domain-general hammer for Lean, built on a novel neural premise
selection system for a hammer in dependent type theory. Unlike existing Lean
premise selectors, our approach dynamically adapts to user-specific contexts
and combines with symbolic proof search and reconstruction to create a
practical hammer. With comprehensive evaluations, we show that our premise
selector enables LeanHammer to solve 21\% more goals relative to existing
premise selectors, and generalize well to diverse domains. Our work bridges the
gap between neural retrieval and symbolic reasoning, making formal verification
more accessible to researchers and practitioners.

</details>


### [351] [Explicit Preference Optimization: No Need for an Implicit Reward Model](https://arxiv.org/abs/2506.07492)
*Xiangkun Hu,Lemin Kong,Tong He,David Wipf*

Main category: cs.LG

TL;DR: 本文提出EXPO框架，通过显式偏好优化解决DPO方法中的次优正则化和反直觉插值问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于直接偏好优化（DPO）的方法虽简化了RLHF流程，但其隐式奖励重参数化可能导致次优正则化和反直觉插值行为，需探索更透明的优化框架。

Method: 提出显式偏好优化框架EXPO，通过设计直观的正则化因子直接避免DPO缺陷，无需隐式奖励重参数化，并理论证明其满足正则化需求。

Result: 实验验证EXPO有效规避DPO变体的潜在问题，并在实际场景中展现出更优性能。

Conclusion: EXPO通过显式正则化机制解决了现有方法的局限性，为偏好优化提供了理论可靠且实践高效的替代方案。

Abstract: The generated responses of large language models (LLMs) are often fine-tuned
to human preferences through a process called reinforcement learning from human
feedback (RLHF). As RLHF relies on a challenging training sequence, whereby a
separate reward model is independently learned and then later applied to LLM
policy updates, ongoing research effort has targeted more straightforward
alternatives. In this regard, direct preference optimization (DPO) and its many
offshoots circumvent the need for a separate reward training step. Instead,
through the judicious use of a reparameterization trick that induces an
\textit{implicit} reward, DPO and related methods consolidate learning to the
minimization of a single loss function. And yet despite demonstrable success in
some real-world settings, we prove that DPO-based objectives are nonetheless
subject to sub-optimal regularization and counter-intuitive interpolation
behaviors, underappreciated artifacts of the reparameterizations upon which
they are based. To this end, we introduce an \textit{explicit} preference
optimization framework termed EXPO that requires no analogous
reparameterization to achieve an implicit reward. Quite differently, we merely
posit intuitively-appealing regularization factors from scratch that
transparently avoid the potential pitfalls of key DPO variants, provably
satisfying regularization desiderata that prior methods do not. Empirical
results serve to corroborate our analyses and showcase the efficacy of EXPO.

</details>


### [352] [Mind the Gap: Removing the Discretization Gap in Differentiable Logic Gate Networks](https://arxiv.org/abs/2506.07500)
*Shakir Yousefi,Andreas Plesner,Till Aczel,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 本文提出了一种通过引入Gumbel噪声和直通估计器来加速逻辑门网络（LGNs）训练的方法，显著提高了训练速度、神经元利用率，并减少了离散化差距。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络在多个基准测试中表现出色，但其高计算需求和能耗限制了实际应用。逻辑门网络（LGNs）虽然能高效进行图像分类，但训练时间长且存在大量未使用的门，导致离散化差距，影响实际部署的准确性。

Method: 在训练过程中引入Gumbel噪声和直通估计器，通过隐式Hessian正则化改善LGNs的收敛性。

Result: 训练速度提高了4.5倍，离散化差距减少了98%，未使用的门减少了100%。

Conclusion: 该方法显著提升了LGNs的训练效率和实际部署的准确性，为高效神经网络的应用提供了新的解决方案。

Abstract: Modern neural networks demonstrate state-of-the-art performance on numerous
existing benchmarks; however, their high computational requirements and energy
consumption prompt researchers to seek more efficient solutions for real-world
deployment. Logic gate networks (LGNs) learns a large network of logic gates
for efficient image classification. However, learning a network that can solve
a simple problem like CIFAR-10 can take days to weeks to train. Even then,
almost half of the network remains unused, causing a discretization gap. This
discretization gap hinders real-world deployment of LGNs, as the performance
drop between training and inference negatively impacts accuracy. We inject
Gumbel noise with a straight-through estimator during training to significantly
speed up training, improve neuron utilization, and decrease the discretization
gap. We theoretically show that this results from implicit Hessian
regularization, which improves the convergence properties of LGNs. We train
networks $4.5 \times$ faster in wall-clock time, reduce the discretization gap
by $98\%$, and reduce the number of unused gates by $100\%$.

</details>


### [353] [Graph-of-Causal Evolution: Challenging Chain-of-Model for Reasoning](https://arxiv.org/abs/2506.07501)
*Libo Wang*

Main category: cs.LG

TL;DR: 本文提出了一种图因果演化（GoCE）方法，通过将隐式令牌表示映射为可微分稀疏因果邻接矩阵，结合因果掩码注意力和因果-MoE，增强Transformer捕捉长距离因果依赖的能力，并实现自进化。


<details>
  <summary>Details</summary>
Motivation: 针对链式模型（CoM）中子链仅依赖前一个子链信息，且因果掩码可能阻断多级子链间全局上下文流动，导致长距离依赖丢失的问题，提出GoCE方法。

Method: GoCE通过将隐式令牌表示映射为可微分稀疏因果邻接矩阵，结合因果掩码注意力和因果-MoE，使用干预一致性损失测试和自进化门，实现因果结构学习与Transformer架构自适应更新的动态平衡。

Result: 实验表明，GoCE在CLUTRR、CLADDER、EX-FEVER和CausalQA等公开数据集上优于基线LLMs，增强了Transformer捕捉长距离因果依赖的能力，并提升了自进化能力。

Conclusion: GoCE不仅在设计原理上超越了CoM，还为未来因果学习和持续自适应改进研究提供了经验。

Abstract: In view of the problem that each subchain in the chain-of-model (CoM) relies
only on the information of the previous subchain and may lose long-range
dependencies due to the causal mask blocking the global context flow between
multi-level subchains, this work proposes a graph of causal evolution (GoCE).
Its core principle is to map the implicit token representation into a
differentiable and sparse causal adjacency matrix, then permeate causal
constraints through each layer of calculation using causal-masked attention and
causal-MoE. By combining intervention consistency loss test and self-evolution
gate, the dynamic balance between causal structure learning and adaptive
updating of transformer architecture is realized. The researcher built
experimental environments in sandboxes built with Claude Sonnet 4,
o4-mini-high, and DeepSeek R1 respectively with the transformer variant
architecture introduced in GoCE. It is evaluated on publicly available datasets
including CLUTRR, CLADDER, EX-FEVER, and CausalQA and compared with the
baseline LLMs. The finding proves that GoCE strengthens the transformer's
ability to capture long-range causal dependencies, while the ability to
self-evolve is improved. It not only surpasses the design of CoM in terms of
design principles, but also provides experience for future research on causal
learning and continuous adaptive improvement.

</details>


### [354] [Reinforcement Learning via Implicit Imitation Guidance](https://arxiv.org/abs/2506.07505)
*Perry Dong,Alec M. Lessing,Annie S. Chen,Chelsea Finn*

Main category: cs.LG

TL;DR: 本文提出一种基于数据引导噪声（DGN）的强化学习方法，通过利用先验数据指导探索而非强制模仿，显著提升样本效率与长期性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法将模仿学习目标作为正则化或参考策略，但可能因未直接对齐奖励最大化而损害长期性能。需探索更有效利用先验数据的方式。

Method: DGN框架仅将先验数据用于通过策略噪声引导探索，识别应探索的动作，避免显式行为克隆约束。

Result: 在7个模拟连续控制任务中，DGN相比现有基于离线数据的强化学习方法，性能提升达2-3倍。

Conclusion: 通过将先验数据用于探索引导而非动作强制，DGN有效平衡了样本效率与长期奖励最大化，验证了探索导向方法的优势。

Abstract: We study the problem of sample efficient reinforcement learning, where prior
data such as demonstrations are provided for initialization in lieu of a dense
reward signal. A natural approach is to incorporate an imitation learning
objective, either as regularization during training or to acquire a reference
policy. However, imitation learning objectives can ultimately degrade long-term
performance, as it does not directly align with reward maximization. In this
work, we propose to use prior data solely for guiding exploration via noise
added to the policy, sidestepping the need for explicit behavior cloning
constraints. The key insight in our framework, Data-Guided Noise (DGN), is that
demonstrations are most useful for identifying which actions should be
explored, rather than forcing the policy to take certain actions. Our approach
achieves up to 2-3x improvement over prior reinforcement learning from offline
data methods across seven simulated continuous control tasks.

</details>


### [355] [Addressing Correlated Latent Exogenous Variables in Debiased Recommender Systems](https://arxiv.org/abs/2506.07517)
*Shuqiang Zhang,Yuchao Zhang,Jinkun Chen,Haochen Sui*

Main category: cs.LG

TL;DR: 本文提出了一种基于似然最大化的学习算法，用于解决推荐系统中的选择偏差问题，通过处理潜在外生变量来提高推荐的准确性和公平性。


<details>
  <summary>Details</summary>
Motivation: 推荐系统面临选择偏差问题，导致用户偏好表示失真，影响推荐的准确性和公平性。现有方法假设外生变量独立，本文放松这一假设，提出新方法。

Method: 本文提出了一种基于似然最大化的学习算法，通过建模潜在外生变量的数据生成过程，并开发蒙特卡罗算法进行数值估计。

Result: 在合成数据集和三个真实数据集上的实验表明，所提出的方法在处理潜在外生变量方面具有有效性。

Conclusion: 本文提出的方法能够有效处理推荐系统中的选择偏差问题，提高推荐的准确性和公平性，代码已开源。

Abstract: Recommendation systems (RS) aim to provide personalized content, but they
face a challenge in unbiased learning due to selection bias, where users only
interact with items they prefer. This bias leads to a distorted representation
of user preferences, which hinders the accuracy and fairness of
recommendations. To address the issue, various methods such as error imputation
based, inverse propensity scoring, and doubly robust techniques have been
developed. Despite the progress, from the structural causal model perspective,
previous debiasing methods in RS assume the independence of the exogenous
variables. In this paper, we release this assumption and propose a learning
algorithm based on likelihood maximization to learn a prediction model. We
first discuss the correlation and difference between unmeasured confounding and
our scenario, then we propose a unified method that effectively handles latent
exogenous variables. Specifically, our method models the data generation
process with latent exogenous variables under mild normality assumptions. We
then develop a Monte Carlo algorithm to numerically estimate the likelihood
function. Extensive experiments on synthetic datasets and three real-world
datasets demonstrate the effectiveness of our proposed method. The code is at
https://github.com/WallaceSUI/kdd25-background-variable.

</details>


### [356] [Flowing Datasets with Wasserstein over Wasserstein Gradient Flows](https://arxiv.org/abs/2506.07534)
*Clément Bonet,Christophe Vauthier,Anna Korba*

Main category: cs.LG

TL;DR: 提出一种在概率分布空间上构建双重Wasserstein(WoW)梯度流的方法，用于处理以概率分布表示的数据，应用于迁移学习和数据集蒸馏任务，并开发了基于切片Wasserstein核的MMD目标函数。


<details>
  <summary>Details</summary>
Motivation: 处理以分布形式存在的数据(如带类别标签的数据集)需要新的梯度流方法，以支持领域适应、迁移学习等应用场景。现有方法难以直接对这类无限维概率分布空间进行优化。

Method: 将数据集建模为类别条件分布的混合分布，引入WoW距离构建微分结构，定义梯度流动态过程，并设计基于切片Wasserstein核的最大均值差异(MMD)作为可计算的目标函数。

Result: 在迁移学习和数据集蒸馏任务中验证了框架有效性，通过梯度流实现了对目标函数的优化，展示了WoW度量在分布空间建模中的优势。

Conclusion: 通过建立概率分布空间上的WoW度量结构和梯度流框架，为处理复杂分布数据提供了新工具，同时开发的切片Wasserstein核MMD扩展了分布间相似性度量的应用范围。

Abstract: Many applications in machine learning involve data represented as probability
distributions. The emergence of such data requires radically novel techniques
to design tractable gradient flows on probability distributions over this type
of (infinite-dimensional) objects. For instance, being able to flow labeled
datasets is a core task for applications ranging from domain adaptation to
transfer learning or dataset distillation. In this setting, we propose to
represent each class by the associated conditional distribution of features,
and to model the dataset as a mixture distribution supported on these classes
(which are themselves probability distributions), meaning that labeled datasets
can be seen as probability distributions over probability distributions. We
endow this space with a metric structure from optimal transport, namely the
Wasserstein over Wasserstein (WoW) distance, derive a differential structure on
this space, and define WoW gradient flows. The latter enables to design
dynamics over this space that decrease a given objective functional. We apply
our framework to transfer learning and dataset distillation tasks, leveraging
our gradient flow construction as well as novel tractable functionals that take
the form of Maximum Mean Discrepancies with Sliced-Wasserstein based kernels
between probability distributions.

</details>


### [357] [Improving Memory Efficiency for Training KANs via Meta Learning](https://arxiv.org/abs/2506.07549)
*Zhangchi Zhao,Jun Shu,Deyu Meng,Zongben Xu*

Main category: cs.LG

TL;DR: 本文提出MetaKANs，通过一个较小的元学习器生成KANs的权重，显著减少可训练参数数量，同时保持或提升性能，解决了KANs内存效率低和训练成本高的问题。


<details>
  <summary>Details</summary>
Motivation: KANs作为一种新型函数逼近框架，虽然具有高效和可解释性，但其可训练参数数量庞大，导致内存效率低和训练成本高。本文旨在解决这一问题。

Method: 提出MetaKANs，通过一个较小的元学习器生成KANs的权重，并以端到端可微分的方式训练KANs和MetaKANs。

Result: 在符号回归、偏微分方程求解和图像分类等任务上，MetaKANs在减少参数数量和内存使用方面表现出色，性能与KANs相当甚至更优。

Conclusion: MetaKANs提供了一种训练KANs的替代技术，提高了可扩展性和可扩展性，缩小了与MLPs的训练成本差距。

Abstract: Inspired by the Kolmogorov-Arnold representation theorem, KANs offer a novel
framework for function approximation by replacing traditional neural network
weights with learnable univariate functions. This design demonstrates
significant potential as an efficient and interpretable alternative to
traditional MLPs. However, KANs are characterized by a substantially larger
number of trainable parameters, leading to challenges in memory efficiency and
higher training costs compared to MLPs. To address this limitation, we propose
to generate weights for KANs via a smaller meta-learner, called MetaKANs. By
training KANs and MetaKANs in an end-to-end differentiable manner, MetaKANs
achieve comparable or even superior performance while significantly reducing
the number of trainable parameters and maintaining promising interpretability.
Extensive experiments on diverse benchmark tasks, including symbolic
regression, partial differential equation solving, and image classification,
demonstrate the effectiveness of MetaKANs in improving parameter efficiency and
memory usage. The proposed method provides an alternative technique for
training KANs, that allows for greater scalability and extensibility, and
narrows the training cost gap with MLPs stated in the original paper of KANs.
Our code is available at https://github.com/Murphyzc/MetaKAN.

</details>


### [358] [ChemAgent: Enhancing LLMs for Chemistry and Materials Science through Tree-Search Based Tool Learning](https://arxiv.org/abs/2506.07551)
*Mengsong Wu,YaFei Wang,Yidong Ming,Yuqi An,Yuwei Wan,Wenliang Chen,Binbin Lin,Yuqiang Li,Tong Xie,Dongzhan Zhou*

Main category: cs.LG

TL;DR: 该论文提出了一种基于大型语言模型（LLM）的智能体，通过整合137种外部化学工具和新型数据集ChemToolBench，结合层次化进化蒙特卡洛树搜索（HE-MCTS）框架，优化化学任务中的工具规划与执行，显著提升了化学问答与发现任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在化学任务中存在预训练知识过时、难以融合专业工具的问题，限制了其在复杂化学应用中的实用性。

Method: 提出LLM智能体框架，集成137个化学工具并构建ChemToolBench数据集；设计HE-MCTS算法分层优化工具选择与参数填充，通过自生成数据微调策略模型，训练任务自适应PRM/ORM模型。

Result: 实验表明该方法在化学QA和发现任务中性能显著优于基线（包括GPT-4o），工具调用准确率提升，代码与数据集已开源。

Conclusion: 通过工具集成与层次化优化框架，成功解决了LLM在化学领域的知识更新与专业化能力瓶颈，为复杂科学计算任务提供了可扩展的解决方案。

Abstract: Large language models (LLMs) have recently demonstrated promising
capabilities in chemistry tasks while still facing challenges due to outdated
pretraining knowledge and the difficulty of incorporating specialized chemical
expertise. To address these issues, we propose an LLM-based agent that
synergistically integrates 137 external chemical tools created ranging from
basic information retrieval to complex reaction predictions, and a dataset
curation pipeline to generate the dataset ChemToolBench that facilitates both
effective tool selection and precise parameter filling during fine-tuning and
evaluation. We introduce a Hierarchical Evolutionary Monte Carlo Tree Search
(HE-MCTS) framework, enabling independent optimization of tool planning and
execution. By leveraging self-generated data, our approach supports step-level
fine-tuning (FT) of the policy model and training task-adaptive PRM and ORM
that surpass GPT-4o. Experimental evaluations demonstrate that our approach
significantly improves performance in Chemistry QA and discovery tasks,
offering a robust solution to integrate specialized tools with LLMs for
advanced chemical applications. All datasets and code are available at
https://github.com/AI4Chem/ChemistryAgent .

</details>


### [359] [Denoising the Future: Top-p Distributions for Moving Through Time](https://arxiv.org/abs/2506.07578)
*Florian Andreas Marwitz,Ralf Möller,Magnus Bender,Marcel Gehrke*

Main category: cs.LG

TL;DR: 该论文针对动态概率模型推理中的计算效率问题，提出使用top-p状态（累积概率p的最可能状态）来加速推理并减少噪声，实验显示速度提升至少一个量级且误差可控。


<details>
  <summary>Details</summary>
Motivation: 传统隐马尔可夫模型推理需枚举全部状态空间，包括低概率状态，导致计算效率低下且噪声积累。

Method: 通过仅保留累积概率为p的top-p状态进行推理，理论证明其误差受p和模型最小混合率约束。

Result: 实验表明方法可实现至少10倍加速，总变差距离误差低于0.09，理论误差边界得到验证。

Conclusion: top-p状态方法在显著提升推理速度的同时，有效控制误差，平衡了计算效率与精度需求。

Abstract: Inference in dynamic probabilistic models is a complex task involving
expensive operations. In particular, for Hidden Markov Models, the whole state
space has to be enumerated for advancing in time. Even states with negligible
probabilities are considered, resulting in computational inefficiency and
increased noise due to the propagation of unlikely probability mass. We propose
to denoise the future and speed up inference by using only the top-p states,
i.e., the most probable states with accumulated probability p. We show that the
error introduced by using only the top-p states is bound by p and the so-called
minimal mixing rate of the underlying model. Moreover, in our empirical
evaluation, we show that we can expect speedups of at least an order of
magnitude, while the error in terms of total variation distance is below 0.09.

</details>


### [360] [FedCGD: Collective Gradient Divergence Optimized Scheduling for Wireless Federated Learning](https://arxiv.org/abs/2506.07581)
*Tan Chen,Jintao Yan,Yuxuan Sun,Sheng Zhou,Zhisheng Niu*

Main category: cs.LG

TL;DR: 本文提出了一种新的联邦学习算法FedCGD，通过平衡加权地球移动距离（WEMD）和采样方差，最小化多级集体梯度差异（CGD），从而提升模型收敛速度和分类精度。


<details>
  <summary>Details</summary>
Motivation: 在无线网络中的联邦学习（FL）应用中，设备数据异质性和带宽限制是影响性能的主要问题。现有研究多关注单个设备的数据异质性，而本文发现FL的收敛速度还受到设备级和样本级集体梯度差异（CGD）的共同影响。

Method: 本文首先证明了FL收敛速度与设备级和样本级CGD的和相关，并将设备级CGD转化为分类问题中的加权地球移动距离（WEMD）。随后提出FedCGD算法，在多项式时间内通过平衡WEMD和采样方差来最小化多级CGD。

Result: 实验表明，FedCGD在CIFAR-10数据集上分类精度提升了4.2%，同时减少了41.8%的设备调度，并能够在减少WEMD和采样方差之间灵活切换。

Conclusion: FedCGD通过优化多级CGD，显著提升了联邦学习的性能，为无线网络中的设备调度提供了新的解决方案。

Abstract: Federated learning (FL) is a promising paradigm for multiple devices to
cooperatively train a model. When applied in wireless networks, two issues
consistently affect the performance of FL, i.e., data heterogeneity of devices
and limited bandwidth. Many papers have investigated device scheduling
strategies considering the two issues. However, most of them recognize data
heterogeneity as a property of individual devices. In this paper, we prove that
the convergence speed of FL is affected by the sum of device-level and
sample-level collective gradient divergence (CGD). The device-level CGD refers
to the gradient divergence of the scheduled device group, instead of the sum of
the individual device divergence. The sample-level CGD is statistically upper
bounded by sampling variance, which is inversely proportional to the total
number of samples scheduled for local update. To derive a tractable form of the
device-level CGD, we further consider a classification problem and transform it
into the weighted earth moving distance (WEMD) between the group distribution
and the global distribution. Then we propose FedCGD algorithm to minimize the
sum of multi-level CGDs by balancing WEMD and sampling variance, within
polynomial time. Simulation shows that the proposed strategy increases
classification accuracy on the CIFAR-10 dataset by up to 4.2\% while scheduling
41.8\% fewer devices, and flexibly switches between reducing WEMD and reducing
sampling variance.

</details>


### [361] [MIRA: Medical Time Series Foundation Model for Real-World Health Data](https://arxiv.org/abs/2506.07584)
*Hao Li,Bowen Deng,Chang Xu,Zhiyuan Feng,Viktor Schlegel,Yu-Hao Huang,Yizheng Sun,Jingyuan Sun,Kailai Yang,Yiyao Yu,Jiang Bian*

Main category: cs.LG

TL;DR: 提出MIRA，一种专为医疗时间序列设计的统一基础模型，通过创新编码和动态建模技术，显著提升预测精度，减少标注需求和模型定制。


<details>
  <summary>Details</summary>
Motivation: 现有通用时间序列基础模型难以处理医疗数据的不规则间隔、异质采样率和频繁缺失值，需开发针对性解决方案。

Method: 结合连续时间旋转位置编码、频率特定专家混合层及基于神经ODE的连续动态外推块，实现细粒度时间建模与任意时间戳预测。

Result: 在超4540亿时间点预训练后，MIRA在分布外和分布内场景下预测误差分别平均降低10%和7%，优于零样本和微调基线。

Conclusion: MIRA为医疗时间序列建模建立新基准，其统一框架支持跨机构、模态和任务的鲁棒迁移，推动数据稀缺或隐私受限环境的研究。

Abstract: A unified foundation model for medical time series -- pretrained on open
access and ethics board-approved medical corpora -- offers the potential to
reduce annotation burdens, minimize model customization, and enable robust
transfer across clinical institutions, modalities, and tasks, particularly in
data-scarce or privacy-constrained environments. However, existing generalist
time series foundation models struggle to handle medical time series data due
to their inherent challenges, including irregular intervals, heterogeneous
sampling rates, and frequent missing values. To address these challenges, we
introduce MIRA, a unified foundation model specifically designed for medical
time series forecasting. MIRA incorporates a Continuous-Time Rotary Positional
Encoding that enables fine-grained modeling of variable time intervals, a
frequency-specific mixture-of-experts layer that routes computation across
latent frequency regimes to further promote temporal specialization, and a
Continuous Dynamics Extrapolation Block based on Neural ODE that models the
continuous trajectory of latent states, enabling accurate forecasting at
arbitrary target timestamps. Pretrained on a large-scale and diverse medical
corpus comprising over 454 billion time points collect from publicly available
datasets, MIRA achieves reductions in forecasting errors by an average of 10%
and 7% in out-of-distribution and in-distribution scenarios, respectively, when
compared to other zero-shot and fine-tuned baselines. We also introduce a
comprehensive benchmark spanning multiple downstream clinical tasks,
establishing a foundation for future research in medical time series modeling.

</details>


### [362] [Aircraft Trajectory Dataset Augmentation in Latent Space](https://arxiv.org/abs/2506.07585)
*Seokbin Yoon,Keumjin Lee*

Main category: cs.LG

TL;DR: 本文提出了一种名为ATRADA的新型框架，用于增强飞机轨迹数据集，通过生成合成轨迹数据来提高模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 飞机轨迹建模在航空交通管理中至关重要，但现有数据集往往不足或不平衡，需要通过数据增强来提升模型性能。

Method: ATRADA框架使用Transformer编码器学习原始轨迹数据的潜在模式，通过PCA降维并应用高斯混合模型（GMM）拟合数据分布，最后通过多层感知机（MLP）生成新的合成轨迹数据。

Result: 实验表明，ATRADA框架能够有效生成高质量的合成飞机轨迹数据，优于多个基线方法。

Conclusion: ATRADA框架为飞机轨迹数据增强提供了一种有效的方法，有助于提升航空交通管理中的下游任务性能。

Abstract: Aircraft trajectory modeling plays a crucial role in Air Traffic Management
(ATM) and is important for various downstream tasks, including conflict
detection and landing time prediction. Dataset augmentation through the
addition of synthetically generated trajectory data is necessary to develop a
more robust aircraft trajectory model and ensure that the trajectory dataset is
sufficient and balanced. In this work, we propose a novel framework called
ATRADA for aircraft trajectory dataset augmentation. In the proposed framework,
a Transformer encoder learns the underlying patterns in the original trajectory
dataset and converts each data point into a context vector in the learned
latent space. The converted dataset in the latent space is projected into
reduced dimensions using principal component analysis (PCA), and a Gaussian
mixture model (GMM) is applied to fit the probability distribution of the data
points in the reduced-dimensional space. Finally, new samples are drawn from
the fitted GMM, the dimension of the samples is reverted to the original
dimension, and they are decoded with a Multi-Layer Perceptron (MLP). Several
experiments demonstrate that the framework effectively generates new,
high-quality synthetic aircraft trajectory data, which were compared to the
results of several baselines.

</details>


### [363] [PrunePEFT: Iterative Hybrid Pruning for Parameter-Efficient Fine-tuning of LLMs](https://arxiv.org/abs/2506.07587)
*Tongzhou Yu,Zhuhao Zhang,Guanghui Zhu,Shen Jiang,Meikang Qiu,Yihua Huang*

Main category: cs.LG

TL;DR: PrunePEFT提出了一种新的参数高效微调方法，通过将PEFT策略搜索转化为剪枝问题，优化了微调配置，显著降低了计算负担。


<details>
  <summary>Details</summary>
Motivation: 传统的参数高效微调（PEFT）方法虽然减少了可训练参数，但其设计空间庞大，配置不当可能导致次优结果。现有的架构搜索技术虽然有效，但引入了大量额外开销。

Method: PrunePEFT将PEFT策略搜索转化为剪枝问题，采用混合剪枝策略，利用剪枝方法对不同PEFT模块的敏感性，迭代移除冗余或冲突的PEFT模块，从而优化微调配置。

Result: PrunePEFT显著减少了与架构搜索过程相关的计算负担，使其成为微调大型预训练模型的更可扩展和高效的解决方案。

Conclusion: PrunePEFT通过剪枝策略优化PEFT配置，提供了一种更高效、可扩展的微调方法，适用于大型预训练模型。

Abstract: Parameter Efficient Fine-Tuning (PEFT) methods have emerged as effective and
promising approaches for fine-tuning pre-trained language models. Compared with
Full parameter Fine-Tuning (FFT), PEFT achieved comparable task performance
with a substantial reduction of trainable parameters, which largely saved the
training and storage costs. However, using the PEFT method requires considering
a vast design space, such as the type of PEFT modules and their insertion
layers. Inadequate configurations can lead to sub-optimal results. Conventional
solutions such as architectural search techniques, while effective, tend to
introduce substantial additional overhead. In this paper, we propose a novel
approach, PrunePEFT, which formulates the PEFT strategy search as a pruning
problem and introduces a hybrid pruning strategy that capitalizes on the
sensitivity of pruning methods to different PEFT modules. This method extends
traditional pruning techniques by iteratively removing redundant or conflicting
PEFT modules, thereby optimizing the fine-tuned configuration. By efficiently
identifying the most relevant modules, our approach significantly reduces the
computational burden typically associated with architectural search processes,
making it a more scalable and efficient solution for fine-tuning large
pre-trained models.

</details>


### [364] [Exploiting Curvature in Online Convex Optimization with Delayed Feedback](https://arxiv.org/abs/2506.07595)
*Hao Qiu,Emmanuel Esposito,Mengxiao Zhang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this work, we study the online convex optimization problem with curved
losses and delayed feedback. When losses are strongly convex, existing
approaches obtain regret bounds of order $d_{\max} \ln T$, where $d_{\max}$ is
the maximum delay and $T$ is the time horizon. However, in many cases, this
guarantee can be much worse than $\sqrt{d_{\mathrm{tot}}}$ as obtained by a
delayed version of online gradient descent, where $d_{\mathrm{tot}}$ is the
total delay. We bridge this gap by proposing a variant of
follow-the-regularized-leader that obtains regret of order
$\min\{\sigma_{\max}\ln T, \sqrt{d_{\mathrm{tot}}}\}$, where $\sigma_{\max}$ is
the maximum number of missing observations. We then consider exp-concave losses
and extend the Online Newton Step algorithm to handle delays with an adaptive
learning rate tuning, achieving regret $\min\{d_{\max} n\ln T,
\sqrt{d_{\mathrm{tot}}}\}$ where $n$ is the dimension. To our knowledge, this
is the first algorithm to achieve such a regret bound for exp-concave losses.
We further consider the problem of unconstrained online linear regression and
achieve a similar guarantee by designing a variant of the Vovk-Azoury-Warmuth
forecaster with a clipping trick. Finally, we implement our algorithms and
conduct experiments under various types of delay and losses, showing an
improved performance over existing methods.

</details>


### [365] [TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts](https://arxiv.org/abs/2506.07596)
*Torsten Krauß,Hamid Dashtbani,Alexandra Dmitrienko*

Main category: cs.LG

TL;DR: 本文介绍了TwinBreak，一种创新的安全对齐移除方法，通过识别和修剪负责安全功能的参数，有效绕过LLM的安全机制，成功率达到89%至98%。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）带来了显著的好处，但也引入了社会风险，如恶意用户通过提交有害提示来利用LLM。现有的安全机制可以通过LLM越狱绕过，但通常需要大量手动工作或高计算成本。

Method: TwinBreak基于安全机制类似于嵌入式后门的思想，通过分析具有高结构和内容相似性的提示的中间输出来识别和修剪负责安全功能的参数。

Result: 实验证实TwinBreak在16个来自五个供应商的LLM上取得了89%至98%的成功率，且计算需求最小。

Conclusion: TwinBreak是一种有效且高效的安全对齐移除方法，能够在保持模型实用性的同时绕过安全机制。

Abstract: Machine learning is advancing rapidly, with applications bringing notable
benefits, such as improvements in translation and code generation. Models like
ChatGPT, powered by Large Language Models (LLMs), are increasingly integrated
into daily life. However, alongside these benefits, LLMs also introduce social
risks. Malicious users can exploit LLMs by submitting harmful prompts, such as
requesting instructions for illegal activities. To mitigate this, models often
include a security mechanism that automatically rejects such harmful prompts.
However, they can be bypassed through LLM jailbreaks. Current jailbreaks often
require significant manual effort, high computational costs, or result in
excessive model modifications that may degrade regular utility.
  We introduce TwinBreak, an innovative safety alignment removal method.
Building on the idea that the safety mechanism operates like an embedded
backdoor, TwinBreak identifies and prunes parameters responsible for this
functionality. By focusing on the most relevant model layers, TwinBreak
performs fine-grained analysis of parameters essential to model utility and
safety. TwinBreak is the first method to analyze intermediate outputs from
prompts with high structural and content similarity to isolate safety
parameters. We present the TwinPrompt dataset containing 100 such twin prompts.
Experiments confirm TwinBreak's effectiveness, achieving 89% to 98% success
rates with minimal computational requirements across 16 LLMs from five vendors.

</details>


### [366] [FuXi-Air: Urban Air Quality Forecasting Based on Emission-Meteorology-Pollutant multimodal Machine Learning](https://arxiv.org/abs/2506.07616)
*Zhixin Geng,Xu Fan,Xiqiao Lu,Yan Zhang,Guangyuan Yu,Cheng Huang,Qian Wang,Yuewu Li,Weichun Ma,Qi Yu,Libo Wu,Hao Li*

Main category: cs.LG

TL;DR: 本研究提出基于多模态数据融合的FuXi-Air模型，通过整合气象预报、排放清单与监测数据，结合自回归预测框架与帧插值策略，实现特大城市多站点72小时高精度空气质量预测，在计算效率与精度上超越主流数值模型。


<details>
  <summary>Details</summary>
Motivation: 传统数值模拟与单站点机器学习在空气质量预测中存在计算成本高、效率低、观测数据融合不足的问题，需开发低成本高效模型以支持智慧城市管理。

Method: 融合气象预报、排放清单与污染物监测数据，采用自回归预测框架与帧插值策略，实现多站点小时级分辨率污染物浓度72小时快速预测。

Result: 模型在25-30秒内完成预测，精度与效率均优于主流数值模型；多模态数据整合显著提升预测精度，气象数据贡献度高于排放清单，但数据融合确保不同污染机制下的可靠性。

Conclusion: 研究为多模态数据驱动模型应用于空气质量预测提供技术范例，证明混合预测系统在智慧城市污染风险预警中的潜力，强调多源数据融合对提升模型适应性的关键作用。

Abstract: Air pollution has emerged as a major public health challenge in megacities.
Numerical simulations and single-site machine learning approaches have been
widely applied in air quality forecasting tasks. However, these methods face
multiple limitations, including high computational costs, low operational
efficiency, and limited integration with observational data. With the rapid
advancement of artificial intelligence, there is an urgent need to develop a
low-cost, efficient air quality forecasting model for smart urban management.
An air quality forecasting model, named FuXi-Air, has been constructed in this
study based on multimodal data fusion to support high-precision air quality
forecasting and operated in typical megacities. The model integrates
meteorological forecasts, emission inventories, and pollutant monitoring data
under the guidance of air pollution mechanism. By combining an autoregressive
prediction framework with a frame interpolation strategy, the model
successfully completes 72-hour forecasts for six major air pollutants at an
hourly resolution across multiple monitoring sites within 25-30 seconds. In
terms of both computational efficiency and forecasting accuracy, it outperforms
the mainstream numerical air quality models in operational forecasting work.
Ablation experiments concerning key influencing factors show that although
meteorological data contribute more to model accuracy than emission inventories
do, the integration of multimodal data significantly improves forecasting
precision and ensures that reliable predictions are obtained under differing
pollution mechanisms across megacities. This study provides both a technical
reference and a practical example for applying multimodal data-driven models to
air quality forecasting and offers new insights into building hybrid
forecasting systems to support air pollution risk warning in smart city
management.

</details>


### [367] [The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning](https://arxiv.org/abs/2506.07619)
*Toby Boyne,Juan S. Campos,Becky D. Langdon,Jixiang Qing,Yilin Xie,Shiqiang Zhang,Calvin Tsay,Ruth Misener,Daniel W. Davies,Kim E. Jelfs,Sarah Boyall,Thomas M. Dixon,Linden Schrecker,Jose Pablo Folch*

Main category: cs.LG

TL;DR: 本文引入了一个新的数据集用于产率预测，首次提供用于机器学习基准测试的瞬态流数据集，覆盖超过1200个工艺条件，并展示了多种机器学习方法在溶剂选择中的应用。


<details>
  <summary>Details</summary>
Motivation: 化学数据集通常难以获取或需要清洗，限制了机器学习在化学领域的应用。本文旨在通过引入一个新的数据集，推动机器学习在化学实验室中的应用，特别是在溶剂选择和可持续制造方面。

Method: 本文引入了一个新的产率预测数据集，首次提供了用于机器学习基准测试的瞬态流数据集，覆盖超过1200个工艺条件。实验设置允许采样大量连续工艺条件，为机器学习模型带来新挑战。重点研究了溶剂选择任务，并展示了回归算法、迁移学习、特征工程和主动学习等方法的基准测试。

Result: 本文展示了多种机器学习方法在溶剂选择任务中的基准测试结果，证明了这些方法在溶剂替代和可持续制造中的重要应用潜力。

Conclusion: 本文通过引入新的数据集和多种机器学习方法，展示了机器学习在化学实验室中的广泛应用前景，特别是在溶剂选择和可持续制造领域。

Abstract: Machine learning has promised to change the landscape of laboratory
chemistry, with impressive results in molecular property prediction and
reaction retro-synthesis. However, chemical datasets are often inaccessible to
the machine learning community as they tend to require cleaning, thorough
understanding of the chemistry, or are simply not available. In this paper, we
introduce a novel dataset for yield prediction, providing the first-ever
transient flow dataset for machine learning benchmarking, covering over 1200
process conditions. While previous datasets focus on discrete parameters, our
experimental set-up allow us to sample a large number of continuous process
conditions, generating new challenges for machine learning models. We focus on
solvent selection, a task that is particularly difficult to model theoretically
and therefore ripe for machine learning applications. We showcase benchmarking
for regression algorithms, transfer-learning approaches, feature engineering,
and active learning, with important applications towards solvent replacement
and sustainable manufacturing.

</details>


### [368] [Return of ChebNet: Understanding and Improving an Overlooked GNN on Long Range Tasks](https://arxiv.org/abs/2506.07624)
*Ali Hariri,Álvaro Arroyo,Alessio Gravina,Moshe Eliasof,Carola-Bibiane Schönlieb,Davide Bacciu,Kamyar Azizzadenesheli,Xiaowen Dong,Pierre Vandergheynst*

Main category: cs.LG

TL;DR: 本文重新评估了早期谱图神经网络ChebNet，发现其处理长距离节点依赖的竞争力，但存在训练不稳定问题。通过将其建模为稳定非耗散动力系统，提出改进模型Stable-ChebNet，在保持计算效率的同时实现接近SOTA的性能。


<details>
  <summary>Details</summary>
Motivation: 现有MPNN和图变换器在捕捉长距离依赖时存在效率或结构信息损失问题。研究旨在探索ChebNet未被充分利用的潜力，解决其训练中的不稳定性。

Method: 将ChebNet转化为稳定非耗散动力系统(Stable-ChebNet)，通过控制动力学实现稳定信息传播，无需特征分解、位置编码或图重布线。

Result: Stable-ChebNet在多个基准测试中接近SOTA性能，且高阶多项式下仍保持良好扩展性。原始ChebNet在长距离任务中已优于经典MPNN和GT。

Conclusion: ChebNet被证明具有未开发的建模潜力，改进后的Stable-ChebNet有效解决了训练稳定性问题，在性能与效率间取得平衡，无需复杂图结构修改。

Abstract: ChebNet, one of the earliest spectral GNNs, has largely been overshadowed by
Message Passing Neural Networks (MPNNs), which gained popularity for their
simplicity and effectiveness in capturing local graph structure. Despite their
success, MPNNs are limited in their ability to capture long-range dependencies
between nodes. This has led researchers to adapt MPNNs through rewiring or make
use of Graph Transformers, which compromises the computational efficiency that
characterized early spatial message-passing architectures, and typically
disregards the graph structure. Almost a decade after its original
introduction, we revisit ChebNet to shed light on its ability to model distant
node interactions. We find that out-of-box, ChebNet already shows competitive
advantages relative to classical MPNNs and GTs on long-range benchmarks, while
maintaining good scalability properties for high-order polynomials. However, we
uncover that this polynomial expansion leads ChebNet to an unstable regime
during training. To address this limitation, we cast ChebNet as a stable and
non-dissipative dynamical system, which we coin Stable-ChebNet. Our
Stable-ChebNet model allows for stable information propagation, and has
controllable dynamics which do not require the use of eigendecompositions,
positional encodings, or graph rewiring. Across several benchmarks,
Stable-ChebNet achieves near state-of-the-art performance.

</details>


### [369] [The Universality Lens: Why Even Highly Over-Parametrized Models Learn Well](https://arxiv.org/abs/2506.07661)
*Meir Feder,Ruediger Urbanke,Yaniv Fogel*

Main category: cs.LG

TL;DR: 本文通过信息论和通用学习理论，解释了为何大规模过参数化模型（如深度神经网络和Transformer）在参数远多于训练样本时仍能良好泛化。研究发现，模型的泛化能力不取决于假设类的总体大小，而是取决于接近真实数据生成过程的模型的累积概率。这一视角为过参数化模型为何能避免过拟合提供了严谨且直观的解释。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习中，大规模过参数化模型（如深度神经网络和Transformer）在参数远多于训练样本时仍能良好泛化，这一现象尚未得到充分解释。本文旨在通过信息论和通用学习理论，深入探讨这一现象背后的原因。

Method: 本文研究了一个基于对数损失和（几乎）均匀先验的贝叶斯混合学习者，使用Kullback-Leibler散度距离来衡量模型与真实数据生成过程的接近程度，并引入假设权重概念。此外，本文通过随机梯度下降与Langevin动力学结合标准机器学习方法和集成学习来近似理论学习者。

Result: 研究表明，学习者的遗憾不取决于假设类的总体大小，而是取决于接近真实数据生成过程的模型的累积概率。这一结果解释了为何过参数化模型能避免过拟合，并提供了非均匀遗憾界，与平坦极小值和模型蒸馏等关键实践概念一致。

Conclusion: 本文通过信息论和通用学习理论，为大规模过参数化模型的泛化行为提供了统一且原则性的理解。研究结果表明，假设权重是决定模型泛化能力的关键因素，这一视角不仅适用于在线学习、批处理学习和监督学习，还为现代AI系统的泛化行为提供了新的解释。

Abstract: A fundamental question in modern machine learning is why large,
over-parameterized models, such as deep neural networks and transformers, tend
to generalize well, even when their number of parameters far exceeds the number
of training samples.
  We investigate this phenomenon through the lens of information theory,
grounded in universal learning theory. Specifically, we study a Bayesian
mixture learner with log-loss and (almost) uniform prior over an expansive
hypothesis class.
  Our key result shows that the learner's regret is not determined by the
overall size of the hypothesis class, but rather by the cumulative probability
of all models that are close, in Kullback-Leibler divergence distance, to the
true data-generating process. We refer to this cumulative probability as the
weight of the hypothesis.
  This leads to a natural notion of model simplicity: simple models are those
with large weight and thus require fewer samples to generalize, while complex
models have small weight and need more data. This perspective provides a
rigorous and intuitive explanation for why over-parameterized models often
avoid overfitting: the presence of simple hypotheses allows the posterior to
concentrate on them when supported by the data.
  We further bridge theory and practice by recalling that stochastic gradient
descent with Langevin dynamics samples from the correct posterior distribution,
enabling our theoretical learner to be approximated using standard machine
learning methods combined with ensemble learning.
  Our analysis yields non-uniform regret bounds and aligns with key practical
concepts such as flat minima and model distillation. The results apply broadly
across online, batch, and supervised learning settings, offering a unified and
principled understanding of the generalization behavior of modern AI systems.

</details>


### [370] [ProARD: progressive adversarial robustness distillation: provide wide range of robust students](https://arxiv.org/abs/2506.07666)
*Seyedhamidreza Mousavi,Seyedali Mousavi,Masoud Daneshtalab*

Main category: cs.LG

TL;DR: 本文提出Progressive Adversarial Robustness Distillation (ProARD)，通过一次性训练动态网络，支持多种轻量级学生网络，避免重复训练带来的高计算成本和碳排放。


<details>
  <summary>Details</summary>
Motivation: 现有对抗鲁棒性蒸馏方法需针对不同资源约束从头训练学生网络，导致计算成本高且碳排放增加。

Method: 构建基于动态层的动态深度神经网络，以最大规模学生网络作为动态教师网络，采用权重共享机制联合优化动态教师与内部学生网络，并通过采样机制选择子集降低计算开销。

Result: 随机采样学生网络在迭代中无法生成高精度且鲁棒的学生网络，而ProARD通过动态网络结构有效支持多样化架构。

Conclusion: ProARD通过一次性训练动态网络实现灵活部署，显著降低计算资源消耗与碳排放，为边缘设备提供高效鲁棒性蒸馏解决方案。

Abstract: Adversarial Robustness Distillation (ARD) has emerged as an effective method
to enhance the robustness of lightweight deep neural networks against
adversarial attacks. Current ARD approaches have leveraged a large robust
teacher network to train one robust lightweight student. However, due to the
diverse range of edge devices and resource constraints, current approaches
require training a new student network from scratch to meet specific
constraints, leading to substantial computational costs and increased CO2
emissions. This paper proposes Progressive Adversarial Robustness Distillation
(ProARD), enabling the efficient one-time training of a dynamic network that
supports a diverse range of accurate and robust student networks without
requiring retraining. We first make a dynamic deep neural network based on
dynamic layers by encompassing variations in width, depth, and expansion in
each design stage to support a wide range of architectures. Then, we consider
the student network with the largest size as the dynamic teacher network.
ProARD trains this dynamic network using a weight-sharing mechanism to jointly
optimize the dynamic teacher network and its internal student networks.
However, due to the high computational cost of calculating exact gradients for
all the students within the dynamic network, a sampling mechanism is required
to select a subset of students. We show that random student sampling in each
iteration fails to produce accurate and robust students.

</details>


### [371] [How Benchmark Prediction from Fewer Data Misses the Mark](https://arxiv.org/abs/2506.07673)
*Guanhua Zhang,Florian E. Dorner,Moritz Hardt*

Main category: cs.LG

TL;DR: 本文系统评估了11种基准预测方法，发现随机抽样回归方法优于多数现有方法，且现有方法依赖模型相似性，外推时效果下降。提出新方法AIPW在外推时优于随机平均，但提升有限，显示基准预测在评估前沿失效。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)评估成本高昂，需缩减基准数据集以加速评估。研究旨在验证现有基准预测方法的有效性，并探索其局限性。

Method: 在19个多样化基准上系统评估11种基准预测方法，包括随机抽样回归基线方法，并提出基于增强逆倾向加权(AIPW)的新方法。

Result: 1. 随机抽样回归方法超越多数现有方法；2. 现有方法依赖模型相似性，外推(评估新模型)时效果骤降；3. AIPW方法在外推时稳定优于随机平均，但增益有限。

Conclusion: 基准预测在评估前沿(需外推未知能力模型时)失效，此时其预测能力最被需要但表现最差，揭示当前方法的核心局限性。

Abstract: Large language model (LLM) evaluation is increasingly costly, prompting
interest in methods that speed up evaluation by shrinking benchmark datasets.
Benchmark prediction (also called efficient LLM evaluation) aims to select a
small subset of evaluation points and predict overall benchmark performance
from that subset. In this paper, we systematically assess the strengths and
limitations of 11 benchmark prediction methods across 19 diverse benchmarks.
First, we identify a highly competitive baseline: Take a random sample and fit
a regression model on the sample to predict missing entries. Outperforming most
existing methods, this baseline challenges the assumption that careful subset
selection is necessary for benchmark prediction. Second, we discover that all
existing methods crucially depend on model similarity. They work best when
interpolating scores among similar models. The effectiveness of benchmark
prediction sharply declines when new models have higher accuracy than
previously seen models. In this setting of extrapolation, none of the previous
methods consistently beat a simple average over random samples. To improve over
the sample average, we introduce a new method inspired by augmented inverse
propensity weighting. This method consistently outperforms the random sample
average even for extrapolation. However, its performance still relies on model
similarity and the gains are modest in general. This shows that benchmark
prediction fails just when it is most needed: at the evaluation frontier, where
the goal is to evaluate new models of unknown capabilities.

</details>


### [372] [Evaluating Robustness in Latent Diffusion Models via Embedding Level Augmentation](https://arxiv.org/abs/2506.07706)
*Boris Martirosyan,Alexey Karmanov*

Main category: cs.LG

TL;DR: 本文针对潜在扩散模型（LDMs）鲁棒性不足的问题，提出分离文本编码器的评估方法、新型数据增强技术和基于Dreambooth的微调框架，并设计专用评估流程。


<details>
  <summary>Details</summary>
Motivation: 当前研究未充分探索LDMs在鲁棒性方面的缺陷，尤其是文本编码器与生成器耦合导致的评估混淆问题。

Method: 1) 提出排除文本编码器的鲁棒性评估范式；2) 开发揭示文本提示鲁棒性缺陷的数据增强技术；3) 结合Dreambooth对Stable Diffusion 3/XL进行多任务微调；4) 设计专用评估流程。

Result: 成功实现针对Dreambooth微调后LDMs的鲁棒性评估框架，并通过实验验证方法的有效性。

Conclusion: 通过解耦评估对象、增强数据多样性和构建专用评估体系，系统提升了LDMs鲁棒性分析的完整性与可靠性。

Abstract: Latent diffusion models (LDMs) achieve state-of-the-art performance across
various tasks, including image generation and video synthesis. However, they
generally lack robustness, a limitation that remains not fully explored in
current research. In this paper, we propose several methods to address this
gap. First, we hypothesize that the robustness of LDMs primarily should be
measured without their text encoder, because if we take and explore the whole
architecture, the problems of image generator and text encoders wll be fused.
Second, we introduce novel data augmentation techniques designed to reveal
robustness shortcomings in LDMs when processing diverse textual prompts. We
then fine-tune Stable Diffusion 3 and Stable Diffusion XL models using
Dreambooth, incorporating these proposed augmentation methods across multiple
tasks. Finally, we propose a novel evaluation pipeline specifically tailored to
assess the robustness of LDMs fine-tuned via Dreambooth.

</details>


### [373] [Language Embedding Meets Dynamic Graph: A New Exploration for Neural Architecture Representation Learning](https://arxiv.org/abs/2506.07735)
*Haizhao Jing,Haokui Zhang,Zhenhao Shang,Rong Xiao,Peng Wang,Yanning Zhang*

Main category: cs.LG

TL;DR: LeDG-Former框架通过语言嵌入和动态图表示学习，解决了现有方法忽略硬件属性和静态拓扑结构的局限性，实现了跨硬件平台的零样本预测，并在多个基准测试中取得了新的SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络架构表示学习方法存在两个主要问题：一是忽略了硬件属性信息，限制了模型的实际应用；二是依赖静态邻接矩阵表示拓扑结构，无法捕捉计算节点间的结构差异。

Method: LeDG-Former框架结合了语言嵌入和动态图表示学习。首先，通过大语言模型将神经网络架构和硬件平台规范投影到统一的语义空间，实现跨硬件平台的零样本预测。其次，提出基于动态图的Transformer模型，提升神经网络架构建模性能。

Result: 在NNLQP基准测试中，LeDG-Former超越了现有方法，建立了新的SOTA，并首次实现了跨硬件延迟预测。在NAS-Bench-101和NAS-Bench-201数据集上也表现出色。

Conclusion: LeDG-Former通过创新的语言嵌入和动态图表示学习方法，有效解决了现有方法的局限性，显著提升了神经网络架构表示学习的性能，并展示了跨硬件平台预测的潜力。

Abstract: Neural Architecture Representation Learning aims to transform network models
into feature representations for predicting network attributes, playing a
crucial role in deploying and designing networks for real-world applications.
Recently, inspired by the success of transformers, transformer-based models
integrated with Graph Neural Networks (GNNs) have achieved significant progress
in representation learning. However, current methods still have some
limitations. First, existing methods overlook hardware attribute information,
which conflicts with the current trend of diversified deep learning hardware
and limits the practical applicability of models. Second, current encoding
approaches rely on static adjacency matrices to represent topological
structures, failing to capture the structural differences between computational
nodes, which ultimately compromises encoding effectiveness. In this paper, we
introduce LeDG-Former, an innovative framework that addresses these limitations
through the synergistic integration of language-based semantic embedding and
dynamic graph representation learning. Specifically, inspired by large language
models (LLMs), we propose a language embedding framework where both neural
architectures and hardware platform specifications are projected into a unified
semantic space through tokenization and LLM processing, enabling zero-shot
prediction across different hardware platforms for the first time. Then, we
propose a dynamic graph-based transformer for modeling neural architectures,
resulting in improved neural architecture modeling performance. On the NNLQP
benchmark, LeDG-Former surpasses previous methods, establishing a new SOTA
while demonstrating the first successful cross-hardware latency prediction
capability. Furthermore, our framework achieves superior performance on the
cell-structured NAS-Bench-101 and NAS-Bench-201 datasets.

</details>


### [374] [Graph-Assisted Stitching for Offline Hierarchical Reinforcement Learning](https://arxiv.org/abs/2506.07744)
*Seungho Baek,Taegeon Park,Jongchan Park,Seungjun Oh,Yusung Kim*

Main category: cs.LG

TL;DR: 本文提出基于图搜索的离线分层强化学习框架GAS，通过将子目标选择转化为图搜索问题，结合时序距离表示和时序效率指标，有效提升跨轨迹状态转移的拼接效率。


<details>
  <summary>Details</summary>
Motivation: 现有离线分层强化学习方法依赖高层策略生成子目标序列，存在任务时间跨度增大时效率下降、跨轨迹状态转移拼接策略不足的问题。

Method: GAS框架通过时序距离表示(TDR)空间嵌入状态实现跨轨迹状态聚类，构建图结构后使用最短路径算法选择子目标序列，并引入时序效率(TE)指标过滤低效转移状态。

Result: 在运动、导航和操作任务中全面超越现有方法，关键拼接场景下以88.3分显著突破原SOTA的1.0分。

Conclusion: GAS通过图搜索范式替代显式高层策略学习，结合状态表示与图优化技术，为离线分层强化学习提供了新的高效解决方案。

Abstract: Existing offline hierarchical reinforcement learning methods rely on
high-level policy learning to generate subgoal sequences. However, their
efficiency degrades as task horizons increase, and they lack effective
strategies for stitching useful state transitions across different
trajectories. We propose Graph-Assisted Stitching (GAS), a novel framework that
formulates subgoal selection as a graph search problem rather than learning an
explicit high-level policy. By embedding states into a Temporal Distance
Representation (TDR) space, GAS clusters semantically similar states from
different trajectories into unified graph nodes, enabling efficient transition
stitching. A shortest-path algorithm is then applied to select subgoal
sequences within the graph, while a low-level policy learns to reach the
subgoals. To improve graph quality, we introduce the Temporal Efficiency (TE)
metric, which filters out noisy or inefficient transition states, significantly
enhancing task performance. GAS outperforms prior offline HRL methods across
locomotion, navigation, and manipulation tasks. Notably, in the most
stitching-critical task, it achieves a score of 88.3, dramatically surpassing
the previous state-of-the-art score of 1.0. Our source code is available at:
https://github.com/qortmdgh4141/GAS.

</details>


### [375] [E-LDA: Toward Interpretable LDA Topic Models with Strong Guarantees in Logarithmic Parallel Time](https://arxiv.org/abs/2506.07747)
*Adam Breuer*

Main category: cs.LG

TL;DR: 本文提出了一种基于组合方法的LDA主题模型推断算法，具有对数级并行计算时间和可解释性保证，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决LDA主题模型在社会科学、数据探索和因果推断中核心推断问题，现有算法效率与可解释性不足。

Method: 采用非梯度组合式估计方法，通过形式化关联关键词实现可解释性，并保持因果推断所需的独立性假设。

Result: 算法收敛速度指数级提升(对数级并行时间)，语义质量优于传统LDA/神经模型/LLM模型，支持下游因果分析。

Conclusion: 该方法在效率、可解释性和因果推断兼容性方面实现了突破，为LDA应用提供了新的理论实践基础。

Abstract: In this paper, we provide the first practical algorithms with provable
guarantees for the problem of inferring the topics assigned to each document in
an LDA topic model. This is the primary inference problem for many applications
of topic models in social science, data exploration, and causal inference
settings. We obtain this result by showing a novel non-gradient-based,
combinatorial approach to estimating topic models. This yields algorithms that
converge to near-optimal posterior probability in logarithmic parallel
computation time (adaptivity) -- exponentially faster than any known LDA
algorithm. We also show that our approach can provide interpretability
guarantees such that each learned topic is formally associated with a known
keyword. Finally, we show that unlike alternatives, our approach can maintain
the independence assumptions necessary to use the learned topic model for
downstream causal inference methods that allow researchers to study topics as
treatments. In terms of practical performance, our approach consistently
returns solutions of higher semantic quality than solutions from
state-of-the-art LDA algorithms, neural topic models, and LLM-based topic
models across a diverse range of text datasets and evaluation parameters.

</details>


### [376] [Comparing Credit Risk Estimates in the Gen-AI Era](https://arxiv.org/abs/2506.07754)
*Nicola Lavecchia,Sid Fadanelli,Federico Ricciuti,Gennaro Aloe,Enrico Bagli,Pietro Giuffrida,Daniele Vergari*

Main category: cs.LG

TL;DR: 研究比较了生成式AI与传统方法在信用评分建模中的表现，发现当前生成式AI模型性能不及传统方法，需进一步研发才能应用。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI在信用风险评分中的潜力，并与传统方法进行对比，以验证其实际应用可行性。

Method: 采用对比分析方法，评估不同整合策略下生成式AI模型与传统信用评分建模技术的性能差异。

Result: 无论采用何种整合策略，当前生成式AI模型在信用评分任务中均未能超越传统方法的性能表现。

Conclusion: 生成式AI在信用风险评分领域仍存在局限性，需进一步研究改进后方可能应用于此类任务。

Abstract: Generative AI technologies have demonstrated significant potential across
diverse applications. This study provides a comparative analysis of credit
score modeling techniques, contrasting traditional approaches with those
leveraging generative AI. Our findings reveal that current generative AI models
fall short of matching the performance of traditional methods, regardless of
the integration strategy employed. These results highlight the limitations in
the current capabilities of generative AI for credit risk scoring, emphasizing
the need for further research and development before the possibility of
applying generative AI for this specific task, or equivalent ones.

</details>


### [377] [Clustered Federated Learning via Embedding Distributions](https://arxiv.org/abs/2506.07769)
*Dekai Zhang,Matthew Williams,Francesca Toni*

Main category: cs.LG

TL;DR: 本文提出了一种名为EMD-CFL的新型一次性聚类方法，利用嵌入空间中数据分布的Earth Mover's距离（EMD）解决联邦学习在非独立同分布（non-IID）数据下的脆弱性问题。该方法通过理论支持与实验验证，在多个数据集上显著优于16种基线方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在分布式数据环境中面临非独立同分布数据导致的性能下降问题。传统方法难以有效划分同质化客户端集群，因此需要一种更鲁棒的聚类方法以提升模型表现。

Method: 提出EMD-CFL方法：基于领域适应理论，在嵌入空间中计算客户端数据分布的Earth Mover's距离（EMD），通过一次性聚类形成同质化客户端集群。

Result: 在多个挑战性数据集上的实验表明，EMD-CFL的聚类性能显著优于16种基线方法，验证了EMD在分布对齐中的有效性。

Conclusion: EMD-CFL通过嵌入空间分布距离度量，为联邦学习非独立同分布数据问题提供了高效解决方案，其理论合理性与实践优越性得到充分验证。

Abstract: Federated learning (FL) is a widely used framework for machine learning in
distributed data environments where clients hold data that cannot be easily
centralised, such as for data protection reasons. FL, however, is known to be
vulnerable to non-IID data. Clustered FL addresses this issue by finding more
homogeneous clusters of clients. We propose a novel one-shot clustering method,
EMD-CFL, using the Earth Mover's distance (EMD) between data distributions in
embedding space. We theoretically motivate the use of EMDs using results from
the domain adaptation literature and demonstrate empirically superior
clustering performance in extensive comparisons against 16 baselines and on a
range of challenging datasets.

</details>


### [378] [Enhancing Adversarial Robustness with Conformal Prediction: A Framework for Guaranteed Model Reliability](https://arxiv.org/abs/2506.07804)
*Jie Bao,Chuangyin Dang,Rui Luo,Hanwei Zhang,Zhixin Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种基于Conformal Prediction的对抗训练方法OPSA-AT，通过OPSA攻击增强模型不确定性，提升模型鲁棒性和预测可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型在高风险应用中的部署，对抗攻击的防御和性能保证变得至关重要。仅靠准确性无法提供足够的保证或可靠的不确定性估计。

Method: 本文开发了OPSA攻击方法，旨在通过最大化模型不确定性来降低Conformal Prediction的效率，并提出了OPSA-AT防御策略，将OPSA整合到新的Conformal训练范式中。

Result: 实验表明，OPSA攻击方法在多种防御下比基线方法诱导了更大的不确定性，而OPSA-AT防御模型不仅显著增强了对OPSA的鲁棒性，还保持了对其他对抗攻击的可靠预测。

Conclusion: 本文的集成方法在开发可信赖和具有弹性的深度学习模型方面表现出色，适用于安全关键领域。

Abstract: As deep learning models are increasingly deployed in high-risk applications,
robust defenses against adversarial attacks and reliable performance guarantees
become paramount. Moreover, accuracy alone does not provide sufficient
assurance or reliable uncertainty estimates for these models. This study
advances adversarial training by leveraging principles from Conformal
Prediction. Specifically, we develop an adversarial attack method, termed OPSA
(OPtimal Size Attack), designed to reduce the efficiency of conformal
prediction at any significance level by maximizing model uncertainty without
requiring coverage guarantees. Correspondingly, we introduce OPSA-AT
(Adversarial Training), a defense strategy that integrates OPSA within a novel
conformal training paradigm. Experimental evaluations demonstrate that our OPSA
attack method induces greater uncertainty compared to baseline approaches for
various defenses. Conversely, our OPSA-AT defensive model significantly
enhances robustness not only against OPSA but also other adversarial attacks,
and maintains reliable prediction. Our findings highlight the effectiveness of
this integrated approach for developing trustworthy and resilient deep learning
models for safety-critical domains. Our code is available at
https://github.com/bjbbbb/Enhancing-Adversarial-Robustness-with-Conformal-Prediction.

</details>


### [379] [Identifiable Object Representations under Spatial Ambiguities](https://arxiv.org/abs/2506.07806)
*Avinash Kori,Francesca Toni,Ben Glocker*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的多视角概率方法，通过聚合视图特定的槽来捕捉不变内容信息，同时学习解耦的全局视角信息，解决了空间模糊性问题。


<details>
  <summary>Details</summary>
Motivation: 模块化的对象中心表示对于类人推理至关重要，但在空间模糊性（如遮挡和视角模糊）下难以获得。

Method: 引入了一种多视角概率方法，聚合视图特定的槽来捕捉不变内容信息，并学习解耦的全局视角信息，无需视角注释。

Result: 在标准基准和新颖复杂数据集上的广泛实验验证了该方法的鲁棒性和可扩展性。

Conclusion: 该方法解决了空间模糊性问题，提供了可识别性的理论保证，并在实验中表现出色。

Abstract: Modular object-centric representations are essential for *human-like
reasoning* but are challenging to obtain under spatial ambiguities, *e.g. due
to occlusions and view ambiguities*. However, addressing challenges presents
both theoretical and practical difficulties. We introduce a novel multi-view
probabilistic approach that aggregates view-specific slots to capture
*invariant content* information while simultaneously learning disentangled
global *viewpoint-level* information. Unlike prior single-view methods, our
approach resolves spatial ambiguities, provides theoretical guarantees for
identifiability, and requires *no viewpoint annotations*. Extensive experiments
on standard benchmarks and novel complex datasets validate our method's
robustness and scalability.

</details>


### [380] [Accelerating Diffusion Models in Offline RL via Reward-Aware Consistency Trajectory Distillation](https://arxiv.org/abs/2506.07822)
*Xintong Duan,Yutong He,Fahim Tajwar,Ruslan Salakhutdinov,J. Zico Kolter,Jeff Schneider*

Main category: cs.LG

TL;DR: 本文提出了一种新的离线强化学习一致性蒸馏方法，通过将奖励优化直接融入蒸馏过程，实现了单步生成，同时保持了更高的性能和更简单的训练。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在决策任务中取得了显著成果，但其推理速度较慢仍是一个关键限制。一致性模型虽提供了潜在解决方案，但在决策任务中常面临次优演示或依赖复杂多网络并发训练的问题。

Method: 本文提出了一种新的离线强化学习一致性蒸馏方法，将奖励优化直接融入蒸馏过程，从而实现单步生成。

Result: 在Gym MuJoCo基准测试和长时程规划任务中，该方法相比之前的最先进技术提升了8.7%，并在推理时间上实现了高达142倍的加速。

Conclusion: 本文提出的方法在保持高性能和简化训练的同时，显著提升了推理速度，为离线强化学习提供了一种有效的解决方案。

Abstract: Although diffusion models have achieved strong results in decision-making
tasks, their slow inference speed remains a key limitation. While the
consistency model offers a potential solution, its applications to
decision-making often struggle with suboptimal demonstrations or rely on
complex concurrent training of multiple networks. In this work, we propose a
novel approach to consistency distillation for offline reinforcement learning
that directly incorporates reward optimization into the distillation process.
Our method enables single-step generation while maintaining higher performance
and simpler training. Empirical evaluations on the Gym MuJoCo benchmarks and
long horizon planning demonstrate that our approach can achieve an 8.7%
improvement over previous state-of-the-art while offering up to 142x speedup
over diffusion counterparts in inference time.

</details>


### [381] [Decentralizing Multi-Agent Reinforcement Learning with Temporal Causal Information](https://arxiv.org/abs/2506.07829)
*Jan Corazza,Hadi Partovi Aria,Hyohun Kim,Daniel Neider,Zhe Xu*

Main category: cs.LG

TL;DR: 本文研究如何在去中心化多智能体强化学习（DMARL）中通过提供高层符号知识来解决隐私、通信和性能等挑战，并扩展了局部策略与团队任务兼容性的形式化工具。


<details>
  <summary>Details</summary>
Motivation: 许多现实世界问题需要多个智能体协作以实现共同目标，但去中心化多智能体强化学习（DMARL）在隐私、通信和性能方面面临独特挑战。

Method: 本文扩展了用于检查局部策略与团队任务兼容性的形式化工具，并引入高层符号知识来加速学习过程。

Result: 实验表明，关于环境事件时间演化的符号知识可以显著加速DMARL的学习过程。

Conclusion: 通过提供高层符号知识，本文的方法在更多场景中实现了具有理论保证的去中心化训练，并有效提升了学习效率。

Abstract: Reinforcement learning (RL) algorithms can find an optimal policy for a
single agent to accomplish a particular task. However, many real-world problems
require multiple agents to collaborate in order to achieve a common goal. For
example, a robot executing a task in a warehouse may require the assistance of
a drone to retrieve items from high shelves. In Decentralized Multi-Agent RL
(DMARL), agents learn independently and then combine their policies at
execution time, but often must satisfy constraints on compatibility of local
policies to ensure that they can achieve the global task when combined. In this
paper, we study how providing high-level symbolic knowledge to agents can help
address unique challenges of this setting, such as privacy constraints,
communication limitations, and performance concerns. In particular, we extend
the formal tools used to check the compatibility of local policies with the
team task, making decentralized training with theoretical guarantees usable in
more scenarios. Furthermore, we empirically demonstrate that symbolic knowledge
about the temporal evolution of events in the environment can significantly
expedite the learning process in DMARL.

</details>


### [382] [Improving large language models with concept-aware fine-tuning](https://arxiv.org/abs/2506.07833)
*Michael K. Chen,Xikun Zhang,Jiaxing Huang,Dacheng Tao*

Main category: cs.LG

TL;DR: 本文提出概念感知微调（CAFT），通过多标记训练增强大语言模型的高层次概念理解能力，首次将多标记预测应用于训练后阶段，显著提升文本摘要和蛋白质设计等任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型基于单标记预测的范式导致语义理解碎片化，无法形成统一的高层次概念，阻碍类人理解与推理能力的实现。

Method: 提出概念感知微调（CAFT），在模型微调阶段引入多标记序列学习，突破传统单标记预测限制，促进跨标记的连贯概念学习。

Result: 实验表明，CAFT在文本摘要和蛋白质设计等任务中显著优于传统单标记微调方法，且首次实现训练后阶段的多标记预测，降低应用成本。

Conclusion: CAFT为语言模型的概念学习提供新方向，其有效性暗示多标记学习在机器学习中的广泛潜力，同时通过开源促进社区研究与应用。

Abstract: Large language models (LLMs) have become the cornerstone of modern AI.
However, the existing paradigm of next-token prediction fundamentally limits
their ability to form coherent, high-level concepts, making it a critical
barrier to human-like understanding and reasoning. Take the phrase "ribonucleic
acid" as an example: an LLM will first decompose it into tokens, i.e.,
artificial text fragments ("rib", "on", ...), then learn each token
sequentially, rather than grasping the phrase as a unified, coherent semantic
entity. This fragmented representation hinders deeper conceptual understanding
and, ultimately, the development of truly intelligent systems. In response, we
introduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method
that redefines how LLMs are fine-tuned. By enabling the learning of sequences
that span multiple tokens, this method fosters stronger concept-aware learning.
Our experiments demonstrate significant improvements compared to conventional
next-token finetuning methods across diverse tasks, including traditional
applications like text summarization and domain-specific ones like de novo
protein design. Multi-token prediction was previously only possible in the
prohibitively expensive pretraining phase; CAFT, to our knowledge, is the first
to bring the multi-token setting to the post-training phase, thus effectively
democratizing its benefits for the broader community of practitioners and
researchers. Finally, the unexpected effectiveness of our proposed method
suggests wider implications for the machine learning research community. All
code and data are available at https://github.com/michaelchen-lab/caft-llm

</details>


### [383] [Jarzynski Reweighting and Sampling Dynamics for Training Energy-Based Models: Theoretical Analysis of Different Transition Kernels](https://arxiv.org/abs/2506.07843)
*Davide Carbone*

Main category: cs.LG

TL;DR: 本文提出使用非平衡态统计力学中的Jarzynski重加权技术改进基于能量模型（EBMs）的训练，通过分析核选择的影响，在流基扩散模型和受限玻尔兹曼机中验证其有效性，以解决传统方法（如对比散度）的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 传统EBMs训练方法（如对比散度、得分匹配）因归一化常数近似困难及复杂多模态分布采样效率低，存在偏差问题，需探索更理论严谨的替代方案。

Method: 结合Jarzynski重加权技术，分析核选择对模型性能的影响，应用于两类生成框架：1) 流基扩散模型（通过随机插值减少离散化误差）；2) 受限玻尔兹曼机（纠正对比散度偏差）。

Result: 理论分析表明，Jarzynski重加权可有效减少流基扩散模型的离散化误差、提升样本质量，并在受限玻尔兹曼机中修正对比散度的偏差，核选择对模型性能具有关键作用。

Conclusion: Jarzynski重加权为EBMs训练提供了理论支撑，其核选择机制在生成模型中具有广泛潜力，为平衡计算效率与学习准确性提供了新思路。

Abstract: Energy-Based Models (EBMs) provide a flexible framework for generative
modeling, but their training remains theoretically challenging due to the need
to approximate normalization constants and efficiently sample from complex,
multi-modal distributions. Traditional methods, such as contrastive divergence
and score matching, introduce biases that can hinder accurate learning. In this
work, we present a theoretical analysis of Jarzynski reweighting, a technique
from non-equilibrium statistical mechanics, and its implications for training
EBMs. We focus on the role of the choice of the kernel and we illustrate these
theoretical considerations in two key generative frameworks: (i) flow-based
diffusion models, where we reinterpret Jarzynski reweighting in the context of
stochastic interpolants to mitigate discretization errors and improve sample
quality, and (ii) Restricted Boltzmann Machines, where we analyze its role in
correcting the biases of contrastive divergence. Our results provide insights
into the interplay between kernel choice and model performance, highlighting
the potential of Jarzynski reweighting as a principled tool for generative
learning.

</details>


### [384] [Residual Reweighted Conformal Prediction for Graph Neural Networks](https://arxiv.org/abs/2506.07854)
*Zheng Zhang,Jie Bao,Zhixin Zhou,Nicolo Colombo,Lixin Cheng,Rui Luo*

Main category: cs.LG

TL;DR: 本文提出了一种名为RR-GNN的框架，通过图结构的Mondrian CP、残差自适应非共形分数和交叉训练协议，改进了图神经网络在高风险领域的预测性能，并提供了可证明的边际覆盖保证。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）在处理关系数据方面表现出色，但在高风险领域中，由于未量化的不确定性，面临重大挑战。现有的共形预测方法往往产生过于保守的预测区间，未能考虑图的异方差性和结构偏差。

Method: RR-GNN框架引入了三项主要创新：1）使用图结构的Mondrian CP将节点或边划分为社区，确保反映异质性的集群条件覆盖；2）通过在保留的校准集上训练次级GNN来估计任务特定的残差，动态调整预测区间；3）采用交叉训练协议，交替优化主GNN和残差预测器，以防止信息泄露并保持图依赖关系。

Result: 在15个真实世界的图上进行验证，RR-GNN在节点分类、回归和边权重预测等任务中，相比共形预测基线，提高了效率且未损失覆盖范围。

Conclusion: RR-GNN通过结合图结构信息和残差自适应方法，显著提升了图神经网络在高风险领域中的预测性能，并提供了可证明的统计覆盖保证。

Abstract: Graph Neural Networks (GNNs) excel at modeling relational data but face
significant challenges in high-stakes domains due to unquantified uncertainty.
Conformal prediction (CP) offers statistical coverage guarantees, but existing
methods often produce overly conservative prediction intervals that fail to
account for graph heteroscedasticity and structural biases. While residual
reweighting CP variants address some of these limitations, they neglect graph
topology, cluster-specific uncertainties, and risk data leakage by reusing
training sets. To address these issues, we propose Residual Reweighted GNN
(RR-GNN), a framework designed to generate minimal prediction sets with
provable marginal coverage guarantees.
  RR-GNN introduces three major innovations to enhance prediction performance.
First, it employs Graph-Structured Mondrian CP to partition nodes or edges into
communities based on topological features, ensuring cluster-conditional
coverage that reflects heterogeneity. Second, it uses Residual-Adaptive
Nonconformity Scores by training a secondary GNN on a held-out calibration set
to estimate task-specific residuals, dynamically adjusting prediction intervals
according to node or edge uncertainty. Third, it adopts a Cross-Training
Protocol, which alternates the optimization of the primary GNN and the residual
predictor to prevent information leakage while maintaining graph dependencies.
We validate RR-GNN on 15 real-world graphs across diverse tasks, including node
classification, regression, and edge weight prediction. Compared to CP
baselines, RR-GNN achieves improved efficiency over state-of-the-art methods,
with no loss of coverage.

</details>


### [385] [Fairness Overfitting in Machine Learning: An Information-Theoretic Perspective](https://arxiv.org/abs/2506.07861)
*Firas Laakom,Haobo Chen,Jürgen Schmidhuber,Yuheng Bu*

Main category: cs.LG

TL;DR: 本文提出了一种基于信息论的理论框架，用于分析公平性泛化误差，并通过Efron-Stein不等式推导出紧密的信息论公平性泛化边界，验证了其在实际算法中的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管在机器学习模型中促进公平性方面取得了显著进展，但现有方法通常通过修改训练过程来实现，缺乏对公平性在未见数据上泛化的正式保证。本文旨在填补这一研究空白。

Method: 本文提出了一种基于信息论的理论框架，利用Efron-Stein不等式推导出紧密的公平性泛化边界，并结合互信息（MI）和条件互信息（CMI）进行分析。

Result: 实证结果验证了所提出边界在不同公平性学习算法中的紧密性和实际相关性，表明该框架能够有效指导改进公平性泛化的算法设计。

Conclusion: 本文提出的理论框架为分析公平性泛化误差提供了新的视角，并通过信息论方法推导出紧密的边界，为设计更公平的机器学习算法提供了有价值的指导。

Abstract: Despite substantial progress in promoting fairness in high-stake applications
using machine learning models, existing methods often modify the training
process, such as through regularizers or other interventions, but lack formal
guarantees that fairness achieved during training will generalize to unseen
data. Although overfitting with respect to prediction performance has been
extensively studied, overfitting in terms of fairness loss has received far
less attention. This paper proposes a theoretical framework for analyzing
fairness generalization error through an information-theoretic lens. Our novel
bounding technique is based on Efron-Stein inequality, which allows us to
derive tight information-theoretic fairness generalization bounds with both
Mutual Information (MI) and Conditional Mutual Information (CMI). Our empirical
results validate the tightness and practical relevance of these bounds across
diverse fairness-aware learning algorithms. Our framework offers valuable
insights to guide the design of algorithms improving fairness generalization.

</details>


### [386] [Lightweight Sequential Transformers for Blood Glucose Level Prediction in Type-1 Diabetes](https://arxiv.org/abs/2506.07864)
*Mirko Paolo Barbato,Giorgia Rigamonti,Davide Marelli,Paolo Napoletano*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级序列Transformer模型，用于1型糖尿病患者的血糖预测，解决了在可穿戴设备上部署预测模型的计算和内存限制问题。


<details>
  <summary>Details</summary>
Motivation: 1型糖尿病需要持续监测以防止严重的低血糖和高血糖事件，但在可穿戴设备上部署预测模型面临计算和内存限制的挑战。

Method: 提出了一种结合Transformer注意力机制和循环神经网络序列处理的轻量级序列Transformer模型，优化了在资源受限的边缘设备上的部署，并引入了平衡损失函数处理数据不平衡问题。

Result: 在OhioT1DM和DiaTrend两个基准数据集上的实验表明，该模型在预测血糖水平和检测不良事件方面优于现有方法。

Conclusion: 该研究填补了高性能建模与实际部署之间的空白，为1型糖尿病管理提供了可靠且高效的解决方案。

Abstract: Type 1 Diabetes (T1D) affects millions worldwide, requiring continuous
monitoring to prevent severe hypo- and hyperglycemic events. While continuous
glucose monitoring has improved blood glucose management, deploying predictive
models on wearable devices remains challenging due to computational and memory
constraints. To address this, we propose a novel Lightweight Sequential
Transformer model designed for blood glucose prediction in T1D. By integrating
the strengths of Transformers' attention mechanisms and the sequential
processing of recurrent neural networks, our architecture captures long-term
dependencies while maintaining computational efficiency. The model is optimized
for deployment on resource-constrained edge devices and incorporates a balanced
loss function to handle the inherent data imbalance in hypo- and hyperglycemic
events. Experiments on two benchmark datasets, OhioT1DM and DiaTrend,
demonstrate that the proposed model outperforms state-of-the-art methods in
predicting glucose levels and detecting adverse events. This work fills the gap
between high-performance modeling and practical deployment, providing a
reliable and efficient T1D management solution.

</details>


### [387] [Can Hessian-Based Insights Support Fault Diagnosis in Attention-based Models?](https://arxiv.org/abs/2506.07871)
*Sigma Jahan,Mohammad Masudur Rahman*

Main category: cs.LG

TL;DR: 本文通过基于Hessian的指标，提出了一种更有效地定位注意力机制模型故障的方法。


<details>
  <summary>Details</summary>
Motivation: 随着基于注意力的深度学习模型规模和复杂度的增加，诊断其故障变得越来越困难。

Method: 使用Hessian分析来识别注意力机制中的脆弱区域和参数相互依赖性，并在三个不同模型上进行实验。

Result: 实验表明，基于Hessian的指标比仅使用梯度更有效地定位不稳定性和故障源。

Conclusion: 这些指标可以显著改善复杂神经架构中的故障诊断，可能提升软件调试实践。

Abstract: As attention-based deep learning models scale in size and complexity,
diagnosing their faults becomes increasingly challenging. In this work, we
conduct an empirical study to evaluate the potential of Hessian-based analysis
for diagnosing faults in attention-based models. Specifically, we use
Hessian-derived insights to identify fragile regions (via curvature analysis)
and parameter interdependencies (via parameter interaction analysis) within
attention mechanisms. Through experiments on three diverse models (HAN, 3D-CNN,
DistilBERT), we show that Hessian-based metrics can localize instability and
pinpoint fault sources more effectively than gradients alone. Our empirical
findings suggest that these metrics could significantly improve fault diagnosis
in complex neural architectures, potentially improving software debugging
practices.

</details>


### [388] [Diffusion Counterfactual Generation with Semantic Abduction](https://arxiv.org/abs/2506.07883)
*Rajat Rasal,Avinash Kori,Fabio De Sousa Ribeiro,Tian Xia,Ben Glocker*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散模型的反事实图像生成框架，通过引入空间、语义和动态推理，实现了高质量的图像编辑与语义控制。


<details>
  <summary>Details</summary>
Motivation: 现有的自编码框架在反事实图像生成中存在可扩展性和保真度问题，扩散模型在视觉质量和表示学习方面表现出色，因此探索其在反事实图像编辑中的应用。

Method: 提出了一种将语义表示通过Pearl因果理论整合到扩散模型中的通用框架，利用反事实推理过程进行图像编辑。

Result: 该框架首次实现了扩散反事实中的高级语义身份保留，并展示了语义控制如何在忠实因果控制和身份保留之间进行权衡。

Conclusion: 本文的扩散因果机制为反事实图像生成提供了新的方法，显著提升了图像编辑的质量和语义控制能力。

Abstract: Counterfactual image generation presents significant challenges, including
preserving identity, maintaining perceptual quality, and ensuring faithfulness
to an underlying causal model. While existing auto-encoding frameworks admit
semantic latent spaces which can be manipulated for causal control, they
struggle with scalability and fidelity. Advancements in diffusion models
present opportunities for improving counterfactual image editing, having
demonstrated state-of-the-art visual quality, human-aligned perception and
representation learning capabilities. Here, we present a suite of
diffusion-based causal mechanisms, introducing the notions of spatial, semantic
and dynamic abduction. We propose a general framework that integrates semantic
representations into diffusion models through the lens of Pearlian causality to
edit images via a counterfactual reasoning process. To our knowledge, this is
the first work to consider high-level semantic identity preservation for
diffusion counterfactuals and to demonstrate how semantic control enables
principled trade-offs between faithful causal control and identity
preservation.

</details>


### [389] [Uncovering the Functional Roles of Nonlinearity in Memory](https://arxiv.org/abs/2506.07919)
*Manuel Brenner,Georgia Koppe*

Main category: cs.LG

TL;DR: 本文通过研究循环神经网络中非线性的作用，发现最小化非线性不仅足够且通常最优，使模型更简单、稳健且可解释。


<details>
  <summary>Details</summary>
Motivation: 探讨非线性在循环神经网络中的必要性及其机制，挑战传统认为非线性对长程记忆和时序处理不可或缺的观点。

Method: 使用几乎线性循环神经网络（AL-RNNs）作为可调控非线性的工具，分析其在经典序列建模任务和真实刺激选择任务中的表现。

Result: 实验表明，最小非线性模型在性能上优于完全非线性或线性模型，同时提升鲁棒性和可解释性。

Conclusion: 研究为选择性引入非线性提供理论框架，连接动力系统理论与循环网络的功能需求，对人工及生物神经系统具有启示意义。

Abstract: Memory and long-range temporal processing are core requirements for sequence
modeling tasks across natural language processing, time-series forecasting,
speech recognition, and control. While nonlinear recurrence has long been
viewed as essential for enabling such mechanisms, recent work suggests that
linear dynamics may often suffice. In this study, we go beyond performance
comparisons to systematically dissect the functional role of nonlinearity in
recurrent networks--identifying both when it is computationally necessary, and
what mechanisms it enables. We use Almost Linear Recurrent Neural Networks
(AL-RNNs), which allow fine-grained control over nonlinearity, as both a
flexible modeling tool and a probe into the internal mechanisms of memory.
Across a range of classic sequence modeling tasks and a real-world stimulus
selection task, we find that minimal nonlinearity is not only sufficient but
often optimal, yielding models that are simpler, more robust, and more
interpretable than their fully nonlinear or linear counterparts. Our results
provide a principled framework for selectively introducing nonlinearity,
bridging dynamical systems theory with the functional demands of long-range
memory and structured computation in recurrent neural networks, with
implications for both artificial and biological neural systems.

</details>


### [390] [Schauder Bases for $C[0, 1]$ Using ReLU, Softplus and Two Sigmoidal Functions](https://arxiv.org/abs/2506.07884)
*Anand Ganesh,Babhrubahan Bose,Anand Rajagopalan*

Main category: cs.LG

TL;DR: 首次为C[0,1]空间构建了四个基于ReLU、Softplus及其sigmoidal变体的Schauder基，改进了相关通用逼近性质。


<details>
  <summary>Details</summary>
Motivation: 探索常用激活函数（如ReLU和Softplus）在函数空间中的基础构造能力，填补其作为Schauder基存在性的理论空白。

Method: 分别使用ReLU、Softplus函数及其两种sigmoidal变体，构造了四个不同的Schauder基。

Result: 首次证明了这些函数可构成C[0,1]空间的Schauder基，并提升了其通用逼近性质的理论边界。

Conclusion: 该研究不仅确立了激活函数作为函数空间基的数学基础，还为神经网络逼近理论提供了更严格的框架支持。

Abstract: We construct four Schauder bases for the space $C[0,1]$, one using ReLU
functions, another using Softplus functions, and two more using sigmoidal
versions of the ReLU and Softplus functions. This establishes the existence of
a basis using these functions for the first time, and improves on the universal
approximation property associated with them.

</details>


### [391] [FunDiff: Diffusion Models over Function Spaces for Physics-Informed Generative Modeling](https://arxiv.org/abs/2506.07902)
*Sifan Wang,Zehao Dou,Tong-Rui Liu,Lu Lu*

Main category: cs.LG

TL;DR: 本文提出FunDiff框架，结合潜在扩散过程与函数自编码器，解决生成模型在物理连续函数建模中的挑战，通过物理先验确保样本符合物理规律，并在流体与固体力学中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型（如扩散模型）在处理物理应用中的连续函数时面临困难，因其需满足复杂物理规律且输入函数离散化方式多样。

Method: FunDiff框架整合潜在扩散过程与函数自编码器，支持变离散化输入，生成可任意位置评估的连续函数，并通过架构约束或物理损失函数引入物理先验。

Result: 理论证明扩散模型在函数空间密度估计中达到最优收敛率；实验表明生成样本物理一致性高、保真度强，且对噪声与低分辨率数据鲁棒。

Conclusion: FunDiff通过灵活处理离散化、生成连续函数及物理约束集成，为物理建模提供高效工具，在流体与固体力学中展现出实际应用潜力。

Abstract: Recent advances in generative modeling -- particularly diffusion models and
flow matching -- have achieved remarkable success in synthesizing discrete data
such as images and videos. However, adapting these models to physical
applications remains challenging, as the quantities of interest are continuous
functions governed by complex physical laws. Here, we introduce
$\textbf{FunDiff}$, a novel framework for generative modeling in function
spaces. FunDiff combines a latent diffusion process with a function autoencoder
architecture to handle input functions with varying discretizations, generate
continuous functions evaluable at arbitrary locations, and seamlessly
incorporate physical priors. These priors are enforced through architectural
constraints or physics-informed loss functions, ensuring that generated samples
satisfy fundamental physical laws. We theoretically establish minimax
optimality guarantees for density estimation in function spaces, showing that
diffusion-based estimators achieve optimal convergence rates under suitable
regularity conditions. We demonstrate the practical effectiveness of FunDiff
across diverse applications in fluid dynamics and solid mechanics. Empirical
results show that our method generates physically consistent samples with high
fidelity to the target distribution and exhibits robustness to noisy and
low-resolution data. Code and datasets are publicly available at
https://github.com/sifanexisted/fundiff.

</details>


### [392] [Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces](https://arxiv.org/abs/2506.07903)
*Kevin Rojas,Yuchen Zhu,Sichen Zhu,Felix X. -F. Ye,Molei Tao*

Main category: cs.LG

TL;DR: 本文提出了一种新型多模态扩散模型框架，通过解耦噪声调度实现在任意状态空间下原生生成跨模态耦合数据，无需依赖外部预处理，并在文本-图像生成及混合表格数据合成中验证了其竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的多模态生成方法依赖外部预处理协议统一数据表示，对编码器/解码器精度要求高，在数据有限场景中受限。本文旨在突破这一限制，直接支持原生跨模态生成。

Method: 提出支持任意状态空间的多模态扩散模型框架，引入模态解耦噪声调度策略，使单一模型可同时实现无条件生成和模态条件生成。

Result: 在文本-图像生成和混合类型表格数据合成任务中，该方法达到与现有方法竞争的性能表现。

Conclusion: 所提框架突破了传统预处理依赖，通过原生跨模态生成机制扩展了扩散模型的应用边界，为多模态数据合成提供了灵活高效的解决方案。

Abstract: Diffusion models have demonstrated remarkable performance in generating
unimodal data across various tasks, including image, video, and text
generation. On the contrary, the joint generation of multimodal data through
diffusion models is still in the early stages of exploration. Existing
approaches heavily rely on external preprocessing protocols, such as tokenizers
and variational autoencoders, to harmonize varied data representations into a
unified, unimodal format. This process heavily demands the high accuracy of
encoders and decoders, which can be problematic for applications with limited
data. To lift this restriction, we propose a novel framework for building
multimodal diffusion models on arbitrary state spaces, enabling native
generation of coupled data across different modalities. By introducing an
innovative decoupled noise schedule for each modality, we enable both
unconditional and modality-conditioned generation within a single model
simultaneously. We empirically validate our approach for text-image generation
and mixed-type tabular data synthesis, demonstrating that it achieves
competitive performance.

</details>


### [393] [CausalPFN: Amortized Causal Effect Estimation via In-Context Learning](https://arxiv.org/abs/2506.07918)
*Vahid Balazadeh,Hamidreza Kamkari,Valentin Thomas,Benson Li,Junwei Ma,Jesse C. Cresswell,Rahul G. Krishnan*

Main category: cs.LG

TL;DR: CausalPFN是一种基于Transformer的因果效应估计模型，通过大规模模拟数据预训练，实现无需调整的即插即用推理，在多个基准测试中表现优异，并提供贝叶斯校准的不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 传统因果效应估计方法需要大量人工选择和领域知识，限制了其应用效率。本文旨在通过自动化方法减少人工干预，提升模型泛化能力。

Method: 结合贝叶斯因果推断与先验拟合网络（PFNs），在满足可忽略性假设的模拟数据生成过程库上训练单一Transformer模型，直接将原始观测映射到因果效应。

Result: 在IHDP/Lalonde/ACIC等异质性和平均处理效应估计基准中取得最优平均性能，在真实世界政策制定的提升建模任务中表现竞争力，并提供可靠的不确定性量化。

Conclusion: CausalPFN通过完全自动化、无需调参的即用特性，结合校准的贝叶斯不确定性估计，推动了可信赖的自动化因果推断发展。

Abstract: Causal effect estimation from observational data is fundamental across
various applications. However, selecting an appropriate estimator from dozens
of specialized methods demands substantial manual effort and domain expertise.
We present CausalPFN, a single transformer that amortizes this workflow:
trained once on a large library of simulated data-generating processes that
satisfy ignorability, it infers causal effects for new observational datasets
out-of-the-box. CausalPFN combines ideas from Bayesian causal inference with
the large-scale training protocol of prior-fitted networks (PFNs), learning to
map raw observations directly to causal effects without any task-specific
adjustment. Our approach achieves superior average performance on heterogeneous
and average treatment effect estimation benchmarks (IHDP, Lalonde, ACIC).
Moreover, it shows competitive performance for real-world policy making on
uplift modeling tasks. CausalPFN provides calibrated uncertainty estimates to
support reliable decision-making based on Bayesian principles. This
ready-to-use model does not require any further training or tuning and takes a
step toward automated causal inference (https://github.com/vdblm/CausalPFN).

</details>


### [394] [HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization](https://arxiv.org/abs/2506.07972)
*Hongzheng Chen,Yingheng Wang,Yaohui Cai,Hins Hu,Jiajie Li,Shirley Huang,Chenhui Deng,Rongjian Liang,Shufeng Kong,Haoxing Ren,Samitha Samaranayake,Carla P. Gomes,Zhiru Zhang*

Main category: cs.LG

TL;DR: 本文提出了HeuriGym框架，用于评估LLM生成的启发式算法在组合优化问题中的表现，揭示了当前模型在工具使用、规划和适应性推理方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法充分评估LLM在推理和基于代理的问题解决中的能力，现有基准测试要么依赖于容易饱和和记忆的封闭式问题，要么缺乏一致性和严谨性的主观比较。

Method: 引入HeuriGym框架，让LLM提出启发式算法，通过代码执行获得反馈，并迭代优化解决方案。使用质量-产出指数（QYI）量化性能。

Result: 评估了九个最先进的模型在九个问题上的表现，发现即使顶级模型如GPT-o4-mini-high和Gemini-2.5-Pro的QYI得分仅为0.6，远低于专家基准1。

Conclusion: HeuriGym开源基准旨在指导LLM在科学和工程领域更有效和现实的问题解决能力的发展。

Abstract: While Large Language Models (LLMs) have demonstrated significant advancements
in reasoning and agent-based problem-solving, current evaluation methodologies
fail to adequately assess their capabilities: existing benchmarks either rely
on closed-ended questions prone to saturation and memorization, or subjective
comparisons that lack consistency and rigor. In this work, we introduce
HeuriGym, an agentic framework designed for evaluating heuristic algorithms
generated by LLMs for combinatorial optimization problems, characterized by
clearly defined objectives and expansive solution spaces. HeuriGym empowers
LLMs to propose heuristics, receive evaluative feedback via code execution, and
iteratively refine their solutions. We evaluate nine state-of-the-art models on
nine problems across domains such as computer systems, logistics, and biology,
exposing persistent limitations in tool use, planning, and adaptive reasoning.
To quantify performance, we propose the Quality-Yield Index (QYI), a metric
that captures both solution pass rate and quality. Even top models like
GPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below
the expert baseline of 1. Our open-source benchmark aims to guide the
development of LLMs toward more effective and realistic problem-solving in
scientific and engineering domains.

</details>


### [395] [W4S4: WaLRUS Meets S4 for Long-Range Sequence Modeling](https://arxiv.org/abs/2506.07920)
*Hossein Babaei,Mel White,Richard G. Baraniuk*

Main category: cs.LG

TL;DR: 本文提出了一种新的状态空间模型WaLRUS，基于冗余小波框架构建，提升了长序列建模的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有的状态空间模型在处理长序列依赖时，其效果高度依赖于状态矩阵的选择和初始化，因此需要一种更高效且稳定的方法。

Method: 基于SaFARi框架和现有的WaLRUS SSMs，提出了W4S4模型，利用冗余小波框架构建，支持快速核计算且无需低秩近似。

Result: 实验表明，WaLRUS在延迟重建任务、分类基准和长序列建模中均表现出显著改进，优于基于HiPPO的SSMs。

Conclusion: WaLRUS为下一代基于深度SSM的模型提供了可扩展且多功能的基础，展示了基于小波的状态动态初始化的重要优势。

Abstract: State Space Models (SSMs) have emerged as powerful components for sequence
modeling, enabling efficient handling of long-range dependencies via linear
recurrence and convolutional computation. However, their effectiveness depends
heavily on the choice and initialization of the state matrix. In this work, we
build on the SaFARi framework and existing WaLRUS SSMs to introduce a new
variant, W4S4 (WaLRUS for S4), a new class of SSMs constructed from redundant
wavelet frames. WaLRUS admits a stable diagonalization and supports fast kernel
computation without requiring low-rank approximations, making it both
theoretically grounded and computationally efficient. We show that WaLRUS
retains information over long horizons significantly better than HiPPO-based
SSMs, both in isolation and when integrated into deep architectures such as S4.
Our experiments demonstrate consistent improvements across delay reconstruction
tasks, classification benchmarks, and long-range sequence modeling, confirming
that high-quality, structured initialization enabled by wavelet-based state
dynamic offers substantial advantages over existing alternatives. WaLRUS
provides a scalable and versatile foundation for the next generation of deep
SSM-based models.

</details>


### [396] [A Generative Physics-Informed Reinforcement Learning-Based Approach for Construction of Representative Drive Cycle](https://arxiv.org/abs/2506.07929)
*Amirreza Yasami,Mohammadali Tofigh,Mahdi Shahbakhti,Charles Robert Koch*

Main category: cs.LG

TL;DR: 提出一种基于物理信息的强化学习方法PIESMC，用于高效构建具有代表性的驾驶循环，显著降低计算成本并提升准确性。


<details>
  <summary>Details</summary>
Motivation: 精确构建驾驶循环对车辆设计、燃油经济性分析和环境影响评估至关重要，但现有方法在动态特征捕捉和计算效率上存在不足。

Method: 结合物理信息强化学习框架与蒙特卡洛采样（PIESMC），捕捉瞬态动态、加减速、怠速及道路坡度变化，确保模型保真度。

Result: 实验表明PIESMC在运动学误差上较MTB和MCB方法分别降低57.3%和10.5%，计算速度提升近十倍，且车辆功率分布与频率分析验证了其有效性。

Conclusion: PIESMC通过融合物理约束与高效采样，实现了高精度、低成本的驾驶循环构建，为实际工程应用提供了可靠工具。

Abstract: Accurate driving cycle construction is crucial for vehicle design, fuel
economy analysis, and environmental impact assessments. A generative
Physics-Informed Expected SARSA-Monte Carlo (PIESMC) approach that constructs
representative driving cycles by capturing transient dynamics, acceleration,
deceleration, idling, and road grade transitions while ensuring model fidelity
is introduced. Leveraging a physics-informed reinforcement learning framework
with Monte Carlo sampling, PIESMC delivers efficient cycle construction with
reduced computational cost. Experimental evaluations on two real-world datasets
demonstrate that PIESMC replicates key kinematic and energy metrics, achieving
up to a 57.3% reduction in cumulative kinematic fragment errors compared to the
Micro-trip-based (MTB) method and a 10.5% reduction relative to the
Markov-chain-based (MCB) method. Moreover, it is nearly an order of magnitude
faster than conventional techniques. Analyses of vehicle-specific power
distributions and wavelet-transformed frequency content further confirm its
ability to reproduce experimental central tendencies and variability.

</details>


### [397] [Reparameterized LLM Training via Orthogonal Equivalence Transformation](https://arxiv.org/abs/2506.08001)
*Zeju Qiu,Simon Buchholz,Tim Z. Xiao,Maximilian Dax,Bernhard Schölkopf,Weiyang Liu*

Main category: cs.LG

TL;DR: POET是一种新的重参数化训练算法，通过正交等价变换优化神经元，提升大语言模型的训练效果和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型推动了人工智能的快速发展，但如何有效且可靠地训练这些大模型仍是该领域的重大挑战。

Method: POET通过使用两个可学习的正交矩阵和一个固定的随机权重矩阵对每个神经元进行重参数化，并开发了高效的近似方法，使其适用于大规模神经网络的训练。

Result: 大量实验验证了POET在训练大语言模型中的有效性和可扩展性。

Conclusion: POET通过优化神经元的训练过程，显著提升了大语言模型的训练稳定性和泛化能力。

Abstract: While large language models (LLMs) are driving the rapid advancement of
artificial intelligence, effectively and reliably training these large models
remains one of the field's most significant challenges. To address this
challenge, we propose POET, a novel reParameterized training algorithm that
uses Orthogonal Equivalence Transformation to optimize neurons. Specifically,
POET reparameterizes each neuron with two learnable orthogonal matrices and a
fixed random weight matrix. Because of its provable preservation of spectral
properties of weight matrices, POET can stably optimize the objective function
with improved generalization. We further develop efficient approximations that
make POET flexible and scalable for training large-scale neural networks.
Extensive experiments validate the effectiveness and scalability of POET in
training LLMs.

</details>


### [398] [Ensemble-Based Survival Models with the Self-Attended Beran Estimator Predictions](https://arxiv.org/abs/2506.07933)
*Lev V. Utkin,Semen P. Khomets,Vlada A. Efremenko,Andrei V. Konstantinov,Natalya M. Verbova*

Main category: cs.LG

TL;DR: 提出SurvBESA模型，结合Beran估计器与自注意力机制，通过调整生存函数相似性来平滑噪声，解决传统集成模型预测不稳定的问题，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统生存分析集成模型（如随机生存森林、梯度提升）因依赖自助采样样本导致预测不稳定，需解决因删失数据及噪声引起的生存函数波动问题。

Method: SurvBESA将自注意力机制应用于预测的生存函数，基于相邻生存函数相似性调整以平滑噪声；探索使用Huber污染模型定义注意力权重，将训练简化为二次或线性优化问题。

Result: 数值实验表明SurvBESA优于当前最先进模型，且模型实现已开源。

Conclusion: SurvBESA通过自注意力机制有效提升生存分析预测稳定性，为处理删失数据提供新思路，其简化训练方法具有实用价值。

Abstract: Survival analysis predicts the time until an event of interest, such as
failure or death, but faces challenges due to censored data, where some events
remain unobserved. Ensemble-based models, like random survival forests and
gradient boosting, are widely used but can produce unstable predictions due to
variations in bootstrap samples. To address this, we propose SurvBESA (Survival
Beran Estimators Self-Attended), a novel ensemble model that combines Beran
estimators with a self-attention mechanism. Unlike traditional methods,
SurvBESA applies self-attention to predicted survival functions, smoothing out
noise by adjusting each survival function based on its similarity to
neighboring survival functions. We also explore a special case using Huber's
contamination model to define attention weights, simplifying training to a
quadratic or linear optimization problem. Numerical experiments show that
SurvBESA outperforms state-of-the-art models. The implementation of SurvBESA is
publicly available.

</details>


### [399] [TokenBreak: Bypassing Text Classification Models Through Token Manipulation](https://arxiv.org/abs/2506.07948)
*Kasimir Schulz,Kenneth Yeung,Kieran Evans*

Main category: cs.LG

TL;DR: 本文提出一种名为TokenBreak的新型攻击方法，通过操纵输入文本的标记化策略绕过NLP保护模型（如防御LLM注入攻击、恶意输入的分类模型），使保护模型误判但目标系统仍能理解攻击内容，并基于模型架构家族预测漏洞，同时提出无需重新训练防御模型的防护策略。


<details>
  <summary>Details</summary>
Motivation: 现有基于NLP的保护模型（如防御LLM提示注入、恶意输入检测）依赖文本分类模型，但其标记化策略存在潜在漏洞，攻击者可利用此绕过防护，使目标系统暴露于原应防御的攻击风险中。

Method: TokenBreak攻击通过构造特定文本扰动，利用保护模型与目标系统（如LLM）的标记化差异，使保护模型错误分类，而目标系统仍能正确解析攻击载荷。攻击可行性可通过模型架构家族关联的标记化策略进行预判。

Result: 实验证明TokenAttack可有效绕过文本分类保护模型，且模型家族与标记化策略的强关联性使得漏洞可预测。同时提出的防御层策略可增强现有模型鲁棒性而无需重新训练。

Conclusion: 标记化策略的差异导致NLP保护模型存在系统性脆弱性，TokenBreak揭示了此类漏洞的普遍性。通过架构家族预测漏洞并提出轻量级防御层，为模型安全性提供了新的实践方向。

Abstract: Natural Language Processing (NLP) models are used for text-related tasks such
as classification and generation. To complete these tasks, input data is first
tokenized from human-readable text into a format the model can understand,
enabling it to make inferences and understand context. Text classification
models can be implemented to guard against threats such as prompt injection
attacks against Large Language Models (LLMs), toxic input and cybersecurity
risks such as spam emails. In this paper, we introduce TokenBreak: a novel
attack that can bypass these protection models by taking advantage of the
tokenization strategy they use. This attack technique manipulates input text in
such a way that certain models give an incorrect classification. Importantly,
the end target (LLM or email recipient) can still understand and respond to the
manipulated text and therefore be vulnerable to the very attack the protection
model was put in place to prevent. The tokenizer is tied to model architecture,
meaning it is possible to predict whether or not a model is vulnerable to
attack based on family. We also present a defensive strategy as an added layer
of protection that can be implemented without having to retrain the defensive
model.

</details>


### [400] [Cost-Optimal Active AI Model Evaluation](https://arxiv.org/abs/2506.07949)
*Anastasios N. Angelopoulos,Jacob Eisenstein,Jonathan Berant,Alekh Agarwal,Adam Fisch*

Main category: cs.LG

TL;DR: 提出了一种成本感知方法，平衡使用低成本但不准确的弱评分者与高成本但准确的强评分者，以在有限预算下最大化统计效率。


<details>
  <summary>Details</summary>
Motivation: 生成式AI系统的开发需要持续评估和数据标注，成本高且耗时。实践中常依赖低成本但可能带来偏差的合成标注数据。

Method: 基于主动和预测驱动的统计推断，提出了一系列成本最优策略，用于在弱评分者和强评分者之间分配标注预算。

Result: 在任务样本难度差异大的情况下，新策略能以更低的标注预算达到与传统方法相同的估计精度。

Conclusion: 新方法在特定条件下显著优于传统评估方法，尤其在样本难度差异大的任务中表现突出。

Abstract: The development lifecycle of generative AI systems requires continual
evaluation, data acquisition, and annotation, which is costly in both resources
and time. In practice, rapid iteration often makes it necessary to rely on
synthetic annotation data because of the low cost, despite the potential for
substantial bias. In this paper, we develop novel, cost-aware methods for
actively balancing the use of a cheap, but often inaccurate, weak rater -- such
as a model-based autorater that is designed to automatically assess the quality
of generated content -- with a more expensive, but also more accurate, strong
rater alternative such as a human. More specifically, the goal of our approach
is to produce a low variance, unbiased estimate of the mean of the target
"strong" rating, subject to some total annotation budget. Building on recent
work in active and prediction-powered statistical inference, we derive a family
of cost-optimal policies for allocating a given annotation budget between weak
and strong raters so as to maximize statistical efficiency. Using synthetic and
real-world data, we empirically characterize the conditions under which these
policies yield improvements over prior methods. We find that, especially in
tasks where there is high variability in the difficulty of examples, our
policies can achieve the same estimation precision at a far lower total
annotation budget than standard evaluation methods.

</details>


### [401] [Neural Tangent Kernel Analysis to Probe Convergence in Physics-informed Neural Solvers: PIKANs vs. PINNs](https://arxiv.org/abs/2506.07958)
*Salah A. Faroughi,Farinaz Mostajeran*

Main category: cs.LG

TL;DR: 本文通过神经切线核（NTK）理论分析了基于Chebyshev的物理信息Kolmogorov-Arnold网络（cPIKANs），揭示了其在训练过程中的核结构演变及其对学习效率的影响，并探讨了不同优化策略对NTK和学习动态的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管cPIKANs在求解偏微分方程（PDEs）方面表现出潜力，但其训练动态和收敛行为在理论和数值上仍未被充分研究。本文旨在通过NTK理论深入理解cPIKANs的训练机制。

Method: 首先推导了标准cKANs在监督学习中的NTK，并将其扩展到物理信息场景。随后分析了四种典型PDEs的NTK矩阵的谱特性，并研究了不同优化策略对NTK演变和学习动态的影响。

Result: 研究表明，cPIKANs的NTK表现出可追踪的行为，揭示了标准物理信息神经网络（PINNs）无法捕捉的学习动态。谱趋势还揭示了域分解在何时能改善训练，直接将核行为与不同设置下的收敛速率联系起来。

Conclusion: 本文首次系统性地研究了cPIKANs的NTK，提供了理论见解，澄清并预测了其经验表现，为未来的研究提供了重要参考。

Abstract: Physics-informed Kolmogorov-Arnold Networks (PIKANs), and in particular their
Chebyshev-based variants (cPIKANs), have recently emerged as promising models
for solving partial differential equations (PDEs). However, their training
dynamics and convergence behavior remain largely unexplored both theoretically
and numerically. In this work, we aim to advance the theoretical understanding
of cPIKANs by analyzing them using Neural Tangent Kernel (NTK) theory. Our
objective is to discern the evolution of kernel structure throughout
gradient-based training and its subsequent impact on learning efficiency. We
first derive the NTK of standard cKANs in a supervised setting, and then extend
the analysis to the physics-informed context. We analyze the spectral
properties of NTK matrices, specifically their eigenvalue distributions and
spectral bias, for four representative PDEs: the steady-state Helmholtz
equation, transient diffusion and Allen-Cahn equations, and forced vibrations
governed by the Euler-Bernoulli beam equation. We also conduct an investigation
into the impact of various optimization strategies, e.g., first-order,
second-order, and hybrid approaches, on the evolution of the NTK and the
resulting learning dynamics. Results indicate a tractable behavior for NTK in
the context of cPIKANs, which exposes learning dynamics that standard
physics-informed neural networks (PINNs) cannot capture. Spectral trends also
reveal when domain decomposition improves training, directly linking kernel
behavior to convergence rates under different setups. To the best of our
knowledge, this is the first systematic NTK study of cPIKANs, providing
theoretical insight that clarifies and predicts their empirical performance.

</details>


### [402] [A Two-Phase Deep Learning Framework for Adaptive Time-Stepping in High-Speed Flow Modeling](https://arxiv.org/abs/2506.07969)
*Jacob Helwig,Sai Sreeharsha Adavi,Xuan Zhang,Yuchao Lin,Felix S. Chim,Luke Takeshi Vizzini,Haiyang Yu,Muhammad Hasnain,Saykat Kumar Biswas,John J. Holloway,Narendra Singh,N. K. Anand,Swagnik Guhathakurta,Shuiwang Ji*

Main category: cs.LG

TL;DR: 提出ShockCast框架，结合机器学习与自适应时间步长方法，有效模拟含激波的高速流体流动，并开源数据集与代码库。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注低速流体，而高速流动（如超音速）因激波等突变现象需自适应时间步长，传统均匀时间步长方法效率不足。

Method: 两阶段方法：1) 机器学习预测时间步长；2) 结合预测步长与当前流体场推进系统状态，引入神经ODE与混合专家策略优化时间步调节。

Result: 通过自建超音速流数据集验证，ShockCast实现激波的高效捕捉与计算成本平衡，相关代码与数据已开源。

Conclusion: ShockCast是首个针对高速流动的机器学习框架，通过自适应时间步长解决激波分辨率与计算效率的权衡问题，为后续研究奠定基础。

Abstract: We consider the problem of modeling high-speed flows using machine learning
methods. While most prior studies focus on low-speed fluid flows in which
uniform time-stepping is practical, flows approaching and exceeding the speed
of sound exhibit sudden changes such as shock waves. In such cases, it is
essential to use adaptive time-stepping methods to allow a temporal resolution
sufficient to resolve these phenomena while simultaneously balancing
computational costs. Here, we propose a two-phase machine learning method,
known as ShockCast, to model high-speed flows with adaptive time-stepping. In
the first phase, we propose to employ a machine learning model to predict the
timestep size. In the second phase, the predicted timestep is used as an input
along with the current fluid fields to advance the system state by the
predicted timestep. We explore several physically-motivated components for
timestep prediction and introduce timestep conditioning strategies inspired by
neural ODE and Mixture of Experts. As ShockCast is the first framework for
learning high-speed flows, we evaluate our methods by generating two supersonic
flow datasets, available at https://huggingface.co/datasets/divelab. Our code
is publicly available as part of the AIRS library
(https://github.com/divelab/AIRS).

</details>


### [403] [Hyperpruning: Efficient Search through Pruned Variants of Recurrent Neural Networks Leveraging Lyapunov Spectrum](https://arxiv.org/abs/2506.07975)
*Caleb Zheng,Eli Shlizerman*

Main category: cs.LG

TL;DR: 本文提出了一种基于李雅普诺夫谱（LS）距离度量的高效超剪枝框架LSH，通过早期预测剪枝网络性能，显著减少搜索时间，并在多个模型和数据集上验证其优于传统方法及原始密集模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法需通过大量计算资源进行穷举搜索，且无法在早期评估剪枝后网络的性能。如何在保证精度的前提下高效选择最优剪枝策略成为关键挑战。

Method: 提出基于李雅普诺夫谱（LS）的距离指标，用于早期比较剪枝网络与密集网络的动态特性，并结合超参数优化算法构建LSH框架，以低计算成本筛选高潜力剪枝配置。

Result: 实验表明，在固定训练预算和剪枝比例下，LSH找到的剪枝模型不仅优于基于损失的基线方法，甚至超过原始密集模型的性能，且效率比传统全训练搜索提升一个数量级。

Conclusion: LSH通过引入LS距离指标实现了剪枝策略的高效搜索，验证了动态特性与最终性能的强相关性，为网络压缩提供了新的可靠范式。

Abstract: A variety of pruning methods have been introduced for over-parameterized
Recurrent Neural Networks to improve efficiency in terms of power consumption
and storage utilization. These advances motivate a new paradigm, termed
`hyperpruning', which seeks to identify the most suitable pruning strategy for
a given network architecture and application. Unlike conventional
hyperparameter search, where the optimal configuration's accuracy remains
uncertain, in the context of network pruning, the accuracy of the dense model
sets the target for the accuracy of the pruned one. The goal, therefore, is to
discover pruned variants that match or even surpass this established accuracy.
However, exhaustive search over pruning configurations is computationally
expensive and lacks early performance guarantees. To address this challenge, we
propose a novel Lyapunov Spectrum (LS)-based distance metric that enables early
comparison between pruned and dense networks, allowing accurate prediction of
post-training performance. By integrating this LS-based distance with standard
hyperparameter optimization algorithms, we introduce an efficient hyperpruning
framework, termed LS-based Hyperpruning (LSH). LSH reduces search time by an
order of magnitude compared to conventional approaches relying on full
training. Experiments on stacked LSTM and RHN architectures using the Penn
Treebank dataset, and on AWD-LSTM-MoS using WikiText-2, demonstrate that under
fixed training budgets and target pruning ratios, LSH consistently identifies
superior pruned models. Remarkably, these pruned variants not only outperform
those selected by loss-based baseline but also exceed the performance of their
dense counterpart.

</details>


### [404] [Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction](https://arxiv.org/abs/2506.07976)
*Junhong Shen,Hao Bai,Lunjun Zhang,Yifei Zhou,Amrith Setlur,Shengbang Tong,Diego Caples,Nan Jiang,Tong Zhang,Ameet Talwalkar,Aviral Kumar*

Main category: cs.LG

TL;DR: 本文提出了一种新的测试时扩展方法——测试时交互扩展（TTI），通过增加代理的交互范围，使其能够在单个任务中执行探索、回溯和动态重新规划等复杂行为，从而提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前的测试时扩展方法依赖于生成长推理轨迹，但这种方法无法让代理从环境中获取新信息或随时间调整行为。本文旨在探索测试时交互扩展的潜力，以弥补这一不足。

Method: 本文提出了TTI，一种基于课程学习的在线强化学习方法，通过自适应调整代理的交互长度来训练代理。使用Gemma 3 12B模型，TTI在WebVoyager和WebArena基准测试中实现了最先进的性能。

Result: 实验表明，TTI显著提升了Web代理在基准测试中的任务成功率，并能够自适应地平衡探索与利用。

Conclusion: 测试时交互扩展是计算扩展的有力补充，为训练自适应代理提供了新的途径。

Abstract: The current paradigm of test-time scaling relies on generating long reasoning
traces ("thinking" more) before producing a response. In agent problems that
require interaction, this can be done by generating thinking traces before
acting in the world. However, this process does not allow agents to acquire new
information from the environment or adapt their behavior over time. In this
work, we propose to scale test-time interaction, an untapped dimension of
test-time scaling that increases the agent's interaction horizon to enable
running rich behaviors such as exploration, backtracking, and dynamic
re-planning within a single rollout. To demonstrate the promise of this scaling
dimension, we study the domain of web agents. We first show that even
prompting-based interaction scaling without any training can improve task
success on web benchmarks non-trivially. Building on this, we introduce TTI
(Test-Time Interaction), a curriculum-based online reinforcement learning (RL)
approach that trains agents by adaptively adjusting their rollout lengths.
Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data
web agents on WebVoyager and WebArena benchmarks. We further show that TTI
enables agents to balance exploration and exploitation adaptively. Our results
establish interaction scaling as a powerful, complementary axis to scaling
per-step compute, offering new avenues for training adaptive agents.

</details>


### [405] [Realistic Urban Traffic Generator using Decentralized Federated Learning for the SUMO simulator](https://arxiv.org/abs/2506.07980)
*Alberto Bazán-Guillén,Carlos Beis-Penedo,Diego Cajaraville-Aboy,Pablo Barbecho-Bautista,Rebeca P. Díaz-Redondo,Luis J. de la Cruz Llopis,Ana Fernández-Vilas,Mónica Aguilar Igartua,Manuel Fernández-Veiga*

Main category: cs.LG

TL;DR: 提出了一种去中心化城市交通生成框架DesRUTGe，通过深度强化学习与联邦学习的结合，在保护隐私的前提下生成高精度、可扩展的24小时交通模式，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有城市交通模拟方法存在准确性不足、扩展性受限及隐私泄露风险，尤其在处理大规模场景时难以平衡数据利用与隐私保护。

Method: 采用去中心化联邦学习架构，将交通检测器与城市区域作为独立节点，通过DRL模型局部训练，并与地理相邻节点交换参数协作优化模型，无需中央协调器。

Result: 基于巴塞罗那真实数据的实验表明，DesRUTGe在交通模式生成精度上超越SUMO RouteSampler和集中式学习方法，同时实现隐私保护。

Conclusion: DesRUTGe通过分布式学习机制有效解决了交通模拟的准确性、扩展性与隐私矛盾，为智慧城市交通规划提供了可靠工具。

Abstract: Realistic urban traffic simulation is essential for sustainable urban
planning and the development of intelligent transportation systems. However,
generating high-fidelity, time-varying traffic profiles that accurately reflect
real-world conditions, especially in large-scale scenarios, remains a major
challenge. Existing methods often suffer from limitations in accuracy,
scalability, or raise privacy concerns due to centralized data processing. This
work introduces DesRUTGe (Decentralized Realistic Urban Traffic Generator), a
novel framework that integrates Deep Reinforcement Learning (DRL) agents with
the SUMO simulator to generate realistic 24-hour traffic patterns. A key
innovation of DesRUTGe is its use of Decentralized Federated Learning (DFL),
wherein each traffic detector and its corresponding urban zone function as an
independent learning node. These nodes train local DRL models using minimal
historical data and collaboratively refine their performance by exchanging
model parameters with selected peers (e.g., geographically adjacent zones),
without requiring a central coordinator. Evaluated using real-world data from
the city of Barcelona, DesRUTGe outperforms standard SUMO-based tools such as
RouteSampler, as well as other centralized learning approaches, by delivering
more accurate and privacy-preserving traffic pattern generation.

</details>


### [406] [Generative Modeling of Weights: Generalization or Memorization?](https://arxiv.org/abs/2506.07998)
*Boya Zeng,Yida Yin,Zhiqiu Xu,Zhuang Liu*

Main category: cs.LG

TL;DR: 生成模型在合成神经网络权重时主要依赖记忆，无法生成新颖且高性能的权重，现有方法甚至不及简单基线方法。


<details>
  <summary>Details</summary>
Motivation: 本文旨在评估生成模型在合成神经网络权重方面的能力，特别是生成与训练数据不同的新颖权重。

Method: 研究了四种代表性方法，分析其生成新颖模型权重的能力，并与简单基线方法（如添加噪声或权重集成）进行比较。

Result: 现有方法主要通过记忆生成权重，无法生成新颖且高性能的模型，且无法通过修改模型因素或数据增强有效缓解记忆问题。

Conclusion: 当前生成模型在合成神经网络权重方面存在局限性，需更谨慎地评估生成模型在新领域的应用。

Abstract: Generative models, with their success in image and video generation, have
recently been explored for synthesizing effective neural network weights. These
approaches take trained neural network checkpoints as training data, and aim to
generate high-performing neural network weights during inference. In this work,
we examine four representative methods on their ability to generate novel model
weights, i.e., weights that are different from the checkpoints seen during
training. Surprisingly, we find that these methods synthesize weights largely
by memorization: they produce either replicas, or at best simple
interpolations, of the training checkpoints. Current methods fail to outperform
simple baselines, such as adding noise to the weights or taking a simple weight
ensemble, in obtaining different and simultaneously high-performing models. We
further show that this memorization cannot be effectively mitigated by
modifying modeling factors commonly associated with memorization in image
diffusion models, or applying data augmentations. Our findings provide a
realistic assessment of what types of data current generative models can model,
and highlight the need for more careful evaluation of generative models in new
domains. Our code is available at
https://github.com/boyazeng/weight_memorization.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [407] [Conditional Local Independence Testing with Application to Dynamic Causal Discovery](https://arxiv.org/abs/2506.07844)
*Mingzhou Liu,Xinwei Sun,Yizhou Wang*

Main category: stat.ME

TL;DR: 将Christgau等人（2024）的条件局部独立性检验理论扩展至伊藤过程，以支持动态系统的因果发现。


<details>
  <summary>Details</summary>
Motivation: 现有条件局部独立性测试理论在动态系统（如连续时间随机过程）中的应用受限，需扩展至更广泛的随机过程框架。

Method: 基于Christgau等人的理论，提出针对伊藤过程的条件局部独立性检验方法。

Result: 成功建立适用于伊藤过程的检验理论，为动态系统因果推断提供新工具。

Conclusion: 该扩展方法增强了因果发现理论在连续时间动态系统中的适用性，具有潜在应用价值。

Abstract: In this note, we extend the conditional local independence testing theory
developed in Christgau et al. (2024) to Ito processes. The result can be
applied to causal discovery in dynamic systems.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [408] [DELPHYNE: A Pre-Trained Model for General and Financial Time Series](https://arxiv.org/abs/2506.06288)
*Xueying Ding,Aakriti Mittal,Achintya Gopal*

Main category: q-fin.ST

TL;DR: 本文提出了一种新的预训练模型Delphyne，用于金融时间序列数据，解决了现有模型在金融应用中性能不足的问题，并在多个任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预训练模型在金融应用中表现不佳，主要原因是预训练阶段缺乏金融数据以及跨领域时间序列模式的不同导致的负迁移效应。

Method: 本文引入了名为Delphyne的预训练模型，专门针对金融时间序列数据，通过少量微调步骤在公开数据集上实现了与现有模型竞争的性能。

Result: Delphyne在多个金融任务上表现优异，展示了其在金融时间序列数据建模中的潜力。

Conclusion: Delphyne通过解决现有模型的不足，为金融时间序列数据建模提供了一种有效的解决方案，并在多个任务中表现出色。

Abstract: Time-series data is a vital modality within data science communities. This is
particularly valuable in financial applications, where it helps in detecting
patterns, understanding market behavior, and making informed decisions based on
historical data. Recent advances in language modeling have led to the rise of
time-series pre-trained models that are trained on vast collections of datasets
and applied to diverse tasks across financial domains. However, across
financial applications, existing time-series pre-trained models have not shown
boosts in performance over simple finance benchmarks in both zero-shot and
fine-tuning settings. This phenomenon occurs because of a i) lack of financial
data within the pre-training stage, and ii) the negative transfer effect due to
inherently different time-series patterns across domains. Furthermore,
time-series data is continuous, noisy, and can be collected at varying
frequencies and with varying lags across different variables, making this data
more challenging to model than languages. To address the above problems, we
introduce a Pre-trained MoDEL for FINance TimE-series (Delphyne). Delphyne
achieves competitive performance to existing foundation and full-shot models
with few fine-tuning steps on publicly available datasets, and also shows
superior performances on various financial tasks.

</details>


### [409] [Explainable-AI powered stock price prediction using time series transformers: A Case Study on BIST100](https://arxiv.org/abs/2506.06345)
*Sukru Selim Calik,Andac Akyuz,Zeynep Hilal Kilimci,Kerem Colak*

Main category: q-fin.ST

TL;DR: 本研究提出结合Transformer时间序列模型与可解释人工智能(XAI)，提升股价预测的准确性和可解释性。通过分析土耳其BIST100指数中五大银行股及关联指数数据，验证模型有效性，并利用SHAP、LIME技术揭示特征影响，证明该方法可辅助个人投资决策。


<details>
  <summary>Details</summary>
Motivation: 金融素养的提升需处理复杂金融数据与预测工具，但传统模型存在可解释性不足的问题。研究旨在通过XAI增强Transformer模型的透明度，使非专业投资者能理解预测依据，从而更主动参与金融市场。

Method: 采用DLinear、LTSNet、Vanilla Transformer和Time Series Transformer模型，输入特征融合技术指标。使用SHAP和LIME进行特征归因分析，数据集涵盖2015-2025年BIST100成分股中高交易量银行股及XBANK/XU100指数。

Result: Transformer模型展现出优于传统模型的预测性能，XAI技术成功识别波动率、移动平均线等技术指标对预测结果的关键影响，验证了模型决策逻辑与金融理论的一致性。

Conclusion: Transformer与XAI的结合不仅提高了预测精度，还通过可视化特征贡献度降低了模型黑箱效应，为个人投资者提供了兼具实用性和可解释性的决策支持工具。

Abstract: Financial literacy is increasingly dependent on the ability to interpret
complex financial data and utilize advanced forecasting tools. In this context,
this study proposes a novel approach that combines transformer-based time
series models with explainable artificial intelligence (XAI) to enhance the
interpretability and accuracy of stock price predictions. The analysis focuses
on the daily stock prices of the five highest-volume banks listed in the
BIST100 index, along with XBANK and XU100 indices, covering the period from
January 2015 to March 2025. Models including DLinear, LTSNet, Vanilla
Transformer, and Time Series Transformer are employed, with input features
enriched by technical indicators. SHAP and LIME techniques are used to provide
transparency into the influence of individual features on model outputs. The
results demonstrate the strong predictive capabilities of transformer models
and highlight the potential of interpretable machine learning to empower
individuals in making informed investment decisions and actively engaging in
financial markets.

</details>


### [410] [The Hype Index: an NLP-driven Measure of Market News Attention](https://arxiv.org/abs/2506.06329)
*Zheng Cao,Wanchaloem Wunkaew,Helyette Geman*

Main category: q-fin.ST

TL;DR: 本文提出了一种新的指标——Hype Index，用于量化媒体对大盘股的关注度，并利用自然语言处理技术从金融新闻中提取预测信号。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过量化媒体对大盘股的关注度，探索其对市场波动和短期市场走势的预测能力。

Method: 使用S&P 100作为研究对象，构建了基于新闻数量的Hype Index，并进一步调整了资本化权重，形成了资本化调整后的Hype Index。通过多种视角评估了这些指标。

Result: 研究发现，Hype Index系列工具在股票波动分析、市场信号传递和金融领域的NLP扩展中具有重要价值。

Conclusion: Hype Index为股票波动分析和市场信号传递提供了有价值的工具，并展示了NLP在金融领域的应用潜力。

Abstract: This paper introduces the Hype Index as a novel metric to quantify media
attention toward large-cap equities, leveraging advances in Natural Language
Processing (NLP) for extracting predictive signals from financial news. Using
the S&P 100 as the focus universe, we first construct a News Count-Based Hype
Index, which measures relative media exposure by computing the share of news
articles referencing each stock or sector. We then extend it to the
Capitalization Adjusted Hype Index, adjusts for economic size by taking the
ratio of a stock's or sector's media weight to its market capitalization weight
within its industry or sector. We compute both versions of the Hype Index at
the stock and sector levels, and evaluate them through multiple lenses: (1)
their classification into different hype groups, (2) their associations with
returns, volatility, and VIX index at various lags, (3) their signaling power
for short-term market movements, and (4) their empirical properties including
correlations, samplings, and trends. Our findings suggest that the Hype Index
family provides a valuable set of tools for stock volatility analysis, market
signaling, and NLP extensions in Finance.

</details>


### [411] [Towards Competent AI for Fundamental Analysis in Finance: A Benchmark Dataset and Evaluation](https://arxiv.org/abs/2506.07315)
*Zonghan Wu,Junlin Wang,Congyuan Zou,Chenhan Wang,Yilei Shao*

Main category: q-fin.ST

TL;DR: 本文提出FinAR-Bench基准数据集，用于评估大语言模型在财务报表分析中的表现，通过三个可测量步骤（提取关键信息、计算财务指标、逻辑推理）来客观评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在金融领域的应用日益广泛，但其在生成财务分析报告时存在不准确的风险，现有基准无法反映其在实际任务中的表现，因此需要更实用的评估方法。

Method: 提出FinAR-Bench基准数据集，将财务报表分析任务分解为提取关键信息、计算财务指标和逻辑推理三个步骤，以更精确和可靠地评估大语言模型的性能。

Result: 研究结果揭示了大语言模型在基本面分析中的当前优势和局限性，并提供了更实用的基准评估方法。

Conclusion: FinAR-Bench为大语言模型在金融领域的实际应用提供了更客观和实用的评估标准，有助于理解其在实际任务中的表现。

Abstract: Generative AI, particularly large language models (LLMs), is beginning to
transform the financial industry by automating tasks and helping to make sense
of complex financial information. One especially promising use case is the
automatic creation of fundamental analysis reports, which are essential for
making informed investment decisions, evaluating credit risks, guiding
corporate mergers, etc. While LLMs attempt to generate these reports from a
single prompt, the risks of inaccuracy are significant. Poor analysis can lead
to misguided investments, regulatory issues, and loss of trust. Existing
financial benchmarks mainly evaluate how well LLMs answer financial questions
but do not reflect performance in real-world tasks like generating financial
analysis reports. In this paper, we propose FinAR-Bench, a solid benchmark
dataset focusing on financial statement analysis, a core competence of
fundamental analysis. To make the evaluation more precise and reliable, we
break this task into three measurable steps: extracting key information,
calculating financial indicators, and applying logical reasoning. This
structured approach allows us to objectively assess how well LLMs perform each
step of the process. Our findings offer a clear understanding of LLMs current
strengths and limitations in fundamental analysis and provide a more practical
way to benchmark their performance in real-world financial settings.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [412] [Recursive Semantic Anchoring in ISO 639:2023: A Structural Extension to ISO/TC 37 Frameworks](https://arxiv.org/abs/2506.06870)
*Bugra Kilictas,Faruk Alpay*

Main category: cs.LO

TL;DR: 本文提出基于递归语义锚定的框架，通过固定点运算符建模语言变体的语义漂移，增强ISO 639:2023对语言方言混合的机器处理能力，实验证明其提升语言识别与翻译准确率。


<details>
  <summary>Details</summary>
Motivation: ISO 639:2023标准缺乏处理语言方言漂变和混合语的机器原生机制，需建立可量化语义漂移的数学框架。

Method: 定义递归语义锚定算子φₙ,ₘ，将语言实体χ与语义漂移向量Δ(χ)结合，构建范畴论模型DriftLang，并通过RDF模式实现锚定映射。

Result: 实验显示使用φ索引的Transformer模型在噪声/语码转换数据上的语言识别与翻译准确率显著提升（如尼日利亚皮钦语锚定至英语）。

Conclusion: 该框架兼容ISO/TC 37标准，为未来语言规范提供可AI处理的、支持语义漂移的元数据层，并证明范畴论映射的收敛性。

Abstract: ISO 639:2023 unifies the ISO language-code family and introduces contextual
metadata, but it lacks a machine-native mechanism for handling dialectal drift
and creole mixtures. We propose a formalisation of recursive semantic
anchoring, attaching to every language entity $\chi$ a family of fixed-point
operators $\phi_{n,m}$ that model bounded semantic drift via the relation
$\phi_{n,m}(\chi) = \chi \oplus \Delta(\chi)$, where $\Delta(\chi)$ is a drift
vector in a latent semantic manifold. The base anchor $\phi_{0,0}$ recovers the
canonical ISO 639:2023 identity, whereas $\phi_{99,9}$ marks the maximal drift
state that triggers a deterministic fallback. Using category theory, we treat
the operators $\phi_{n,m}$ as morphisms and drift vectors as arrows in a
category $\mathrm{DriftLang}$. A functor $\Phi: \mathrm{DriftLang} \to
\mathrm{AnchorLang}$ maps every drifted object to its unique anchor and proves
convergence. We provide an RDF/Turtle schema (\texttt{BaseLanguage},
\texttt{DriftedLanguage}, \texttt{ResolvedAnchor}) and worked examples -- e.g.,
$\phi_{8,4}$ (Standard Mandarin) versus $\phi_{8,7}$ (a colloquial variant),
and $\phi_{1,7}$ for Nigerian Pidgin anchored to English. Experiments with
transformer models show higher accuracy in language identification and
translation on noisy or code-switched input when the $\phi$-indices are used to
guide fallback routing. The framework is compatible with ISO/TC 37 and provides
an AI-tractable, drift-aware semantic layer for future standards.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [413] [Improving choice model specification using reinforcement learning](https://arxiv.org/abs/2506.06410)
*Gabriel Nova,Sander van Cranenburgh,Stephane Hess*

Main category: econ.GN

TL;DR: 提出基于深度强化学习的框架，通过智能体动态调整策略，自动优化离散选择模型设定，解决传统元启发式方法在搜索效率与知识迁移上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统离散选择模型设定依赖人工试错，存在耗时、主观性强等问题；现有元启发式方法采用静态策略且无法利用历史信息，导致搜索效率低下。

Method: 构建深度强化学习框架，使智能体通过估计模型并接收拟合优度与简洁性奖励，动态调整模型设定搜索策略。

Result: 实验表明该框架能自适应不同数据生成过程，有效识别高潜力模型设定，具有鲁棒性和潜在跨任务迁移能力。

Conclusion: 所提方法无需先验领域知识即可实现动态探索，为自动化选择模型设定提供了新路径。

Abstract: Discrete choice modelling is a theory-driven modelling framework for
understanding and forecasting choice behaviour. To obtain behavioural insights,
modellers test several competing model specifications in their attempts to
discover the 'true' data generation process. This trial-and-error process
requires expertise, is time-consuming, and relies on subjective theoretical
assumptions. Although metaheuristics have been proposed to assist choice
modellers, they treat model specification as a classic optimisation problem,
relying on static strategies, applying predefined rules, and neglecting
outcomes from previous estimated models. As a result, current metaheuristics
struggle to prioritise promising search regions, adapt exploration dynamically,
and transfer knowledge to other modelling tasks. To address these limitations,
we introduce a deep reinforcement learning-based framework where an 'agent'
specifies models by estimating them and receiving rewards based on
goodness-of-fit and parsimony. Results demonstrate the agent dynamically adapts
its strategies to identify promising specifications across data generation
processes, showing robustness and potential transferability, without prior
domain knowledge.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [414] [RBA-FE: A Robust Brain-Inspired Audio Feature Extractor for Depression Diagnosis](https://arxiv.org/abs/2506.07118)
*Yu-Xuan Wu,Ziyan Huang,Bin Hu,Zhi-Hong Guan*

Main category: cs.SD

TL;DR: 本文提出了一种基于脑启发的鲁棒音频特征提取模型（RBA-FE），用于抑郁症诊断，通过改进的分层网络架构和自适应速率平滑漏积分发放（ARSLIF）神经元模型，提升了噪声环境下的音频特征提取精度。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在图像诊断任务中表现优异，但忽视了音频特征。本文旨在解决音频特征提取中的噪声问题，提升抑郁症诊断的准确性。

Method: RBA-FE模型利用六种从原始音频中提取的声学特征，结合改进的ARSLIF神经元模型，模拟大脑注意力系统中的细胞信号选择性重调机制，增强模型对噪声的鲁棒性。

Result: 实验结果表明，RBA-FE在MODMA数据集上达到了最先进的准确率，并在AVEC2014和DAIC-WOZ数据集上展示了噪声鲁棒性的提升。

Conclusion: RBA-FE模型通过脑启发的设计，显著提升了抑郁症诊断中音频特征提取的精度和鲁棒性，ARSLIF神经元模型还提供了对抑郁音频数据的脑启发性解释。

Abstract: This article proposes a robust brain-inspired audio feature extractor
(RBA-FE) model for depression diagnosis, using an improved hierarchical network
architecture. Most deep learning models achieve state-of-the-art performance
for image-based diagnostic tasks, ignoring the counterpart audio features. In
order to tailor the noise challenge, RBA-FE leverages six acoustic features
extracted from the raw audio, capturing both spatial characteristics and
temporal dependencies. This hybrid attribute helps alleviate the precision
limitation in audio feature extraction within other learning models like deep
residual shrinkage networks. To deal with the noise issues, our model
incorporates an improved spiking neuron model, called adaptive rate smooth
leaky integrate-and-fire (ARSLIF). The ARSLIF model emulates the mechanism of
``retuning of cellular signal selectivity" in the brain attention systems,
which enhances the model robustness against environmental noises in audio data.
Experimental results demonstrate that RBA-FE achieves state-of-the-art accuracy
on the MODMA dataset, respectively with 0.8750, 0.8974, 0.8750 and 0.8750 in
precision, accuracy, recall and F1 score. Extensive experiments on the AVEC2014
and DAIC-WOZ datasets both show enhancements in noise robustness. It is further
indicated by comparison that the ARSLIF neuron model suggest the abnormal
firing pattern within the feature extraction on depressive audio data, offering
brain-inspired interpretability.

</details>


### [415] [Speech Recognition on TV Series with Video-guided Post-Correction](https://arxiv.org/abs/2506.07323)
*Haoyuan Yang,Yue Zhang,Liqiang Jing*

Main category: cs.SD

TL;DR: 本文提出了一种新颖的多模态后校正框架，通过利用视频中的上下文信息来提高自动语音识别（ASR）在复杂多媒体环境中的转录准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管ASR在深度学习的推动下取得了显著成功，但在复杂环境（如电视剧）中，重叠语音、领域特定术语和长距离上下文依赖仍然对转录准确性构成重大挑战。现有的多模态方法未能利用视频中的丰富时间和上下文信息来校正ASR输出。

Method: 本文提出的框架包括两个阶段：ASR生成和基于视频的后校正。第一阶段生成初始转录，第二阶段通过视频上下文信息提取和上下文感知的ASR校正来纠正错误。使用视频-大型多模态模型（VLMM）提取关键上下文信息，并结合大型语言模型（LLM）来优化ASR输出。

Result: 在电视剧ASR的多模态基准测试中，该方法通过利用视频上下文显著提高了ASR性能，增强了复杂多媒体环境中的转录准确性。

Conclusion: 本文提出的多模态后校正框架有效解决了ASR在复杂环境中的转录挑战，通过结合视频上下文信息显著提升了转录准确性。

Abstract: Automatic Speech Recognition (ASR) has achieved remarkable success with deep
learning, driving advancements in conversational artificial intelligence, media
transcription, and assistive technologies. However, ASR systems still struggle
in complex environments such as TV series, where overlapping speech,
domain-specific terminology, and long-range contextual dependencies pose
significant challenges to transcription accuracy. Existing multimodal
approaches fail to correct ASR outputs with the rich temporal and contextual
information available in video. To address this limitation, we propose a novel
multimodal post-correction framework that refines ASR transcriptions by
leveraging contextual cues extracted from video. Our framework consists of two
stages: ASR Generation and Video-based Post-Correction, where the first stage
produces the initial transcript and the second stage corrects errors using
Video-based Contextual Information Extraction and Context-aware ASR Correction.
We employ the Video-Large Multimodal Model (VLMM) to extract key contextual
information using tailored prompts, which is then integrated with a Large
Language Model (LLM) to refine the ASR output. We evaluate our method on a
multimodal benchmark for TV series ASR and demonstrate its effectiveness in
improving ASR performance by leveraging video-based context to enhance
transcription accuracy in complex multimedia environments.

</details>


### [416] [Lightweight Joint Audio-Visual Deepfake Detection via Single-Stream Multi-Modal Learning Framework](https://arxiv.org/abs/2506.07358)
*Kuiyuan Zhang,Wenjie Pei,Rushi Lan,Yifang Guo,Zhongyun Hua*

Main category: cs.SD

TL;DR: 本文提出了一种轻量级的单流多模态学习框架，用于音视频深度伪造检测，通过协作音视频学习块和多模态分类模块，显著提升了检测效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的深度伪造检测方法通常使用两个相对独立的子模型分别学习音频和视觉特征，未能充分利用音视频特征之间的内在关联，且模型冗余，不适用于资源受限的环境。

Method: 设计了一个轻量级网络，通过单流多模态学习框架和协作音视频学习块，实现了音视频特征的连续融合，并提出了多模态分类模块，增强了分类器对模态内容的依赖。

Result: 在DF-TIMIT、FakeAVCeleb和DFDC基准数据集上的实验表明，该方法仅需0.48M参数，在单模态和多模态深度伪造检测以及未见过的深度伪造类型上均表现出色。

Conclusion: 本文提出的轻量级单流多模态学习框架在深度伪造检测中表现出高效性和优越性，适用于资源受限的环境。

Abstract: Deepfakes are AI-synthesized multimedia data that may be abused for spreading
misinformation. Deepfake generation involves both visual and audio
manipulation. To detect audio-visual deepfakes, previous studies commonly
employ two relatively independent sub-models to learn audio and visual
features, respectively, and fuse them subsequently for deepfake detection.
However, this may underutilize the inherent correlations between audio and
visual features. Moreover, utilizing two isolated feature learning sub-models
can result in redundant neural layers, making the overall model inefficient and
impractical for resource-constrained environments.
  In this work, we design a lightweight network for audio-visual deepfake
detection via a single-stream multi-modal learning framework. Specifically, we
introduce a collaborative audio-visual learning block to efficiently integrate
multi-modal information while learning the visual and audio features. By
iteratively employing this block, our single-stream network achieves a
continuous fusion of multi-modal features across its layers. Thus, our network
efficiently captures visual and audio features without the need for excessive
block stacking, resulting in a lightweight network design. Furthermore, we
propose a multi-modal classification module that can boost the dependence of
the visual and audio classifiers on modality content. It also enhances the
whole resistance of the video classifier against the mismatches between audio
and visual modalities. We conduct experiments on the DF-TIMIT, FakeAVCeleb, and
DFDC benchmark datasets. Compared to state-of-the-art audio-visual joint
detection methods, our method is significantly lightweight with only 0.48M
parameters, yet it achieves superiority in both uni-modal and multi-modal
deepfakes, as well as in unseen types of deepfakes.

</details>


### [417] [Audio synthesizer inversion in symmetric parameter spaces with approximately equivariant flow matching](https://arxiv.org/abs/2506.07199)
*Ben Hayes,Charalampos Saitis,György Fazekas*

Main category: cs.SD

TL;DR: 论文探讨了音频合成器参数反演的固有对称性问题，提出使用条件生成模型和置换等变连续归一化流来提升性能，并在实际合成器中验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 音频合成器在相同信号下可能对应不同的参数配置，导致从声音到参数的反演问题具有内在的不适定性。本文旨在解决这一问题，特别是置换不变性带来的挑战。

Method: 首先在合成任务中展示了回归点估计在置换对称性下的性能下降，然后通过条件生成模型和置换等变连续归一化流来改进性能，并提出了一种自适应发现相关对称性的策略。

Result: 在Surge XT合成器上的实验表明，该方法在音频重建指标上优于回归和生成基线模型。

Conclusion: 通过条件生成模型和置换等变连续归一化流，本文有效解决了音频合成器参数反演中的对称性问题，提升了性能。

Abstract: Many audio synthesizers can produce the same signal given different parameter
configurations, meaning the inversion from sound to parameters is an inherently
ill-posed problem. We show that this is largely due to intrinsic symmetries of
the synthesizer, and focus in particular on permutation invariance. First, we
demonstrate on a synthetic task that regressing point estimates under
permutation symmetry degrades performance, even when using a
permutation-invariant loss function or symmetry-breaking heuristics. Then,
viewing equivalent solutions as modes of a probability distribution, we show
that a conditional generative model substantially improves performance.
Further, acknowledging the invariance of the implicit parameter distribution,
we find that performance is further improved by using a permutation equivariant
continuous normalizing flow. To accommodate intricate symmetries in real
synthesizers, we also propose a relaxed equivariance strategy that adaptively
discovers relevant symmetries from data. Applying our method to Surge XT, a
full-featured open source synthesizer used in real world audio production, we
find our method outperforms regression and generative baselines across audio
reconstruction metrics.

</details>


### [418] [Towards Generalized Source Tracing for Codec-Based Deepfake Speech](https://arxiv.org/abs/2506.07294)
*Xuanjun Chen,I-Ming Lin,Lin Zhang,Haibin Wu,Hung-yi Lee,Jyh-Shing Roger Jang*

Main category: cs.SD

TL;DR: 本文提出了一种新的语义-声学源追踪网络（SASTNet），通过结合Whisper和Wav2vec2与AudioMAE，解决了基于神经音频编解码器的深度伪造语音（CodecFake）源追踪性能不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于神经音频编解码器的深度伪造语音（CodecFake）源追踪方法表现不佳，尤其是在使用模拟数据训练时，难以在真实数据上保持良好性能。

Method: 本文提出了语义-声学源追踪网络（SASTNet），结合Whisper进行语义特征编码，以及Wav2vec2与AudioMAE进行声学特征编码，以提升源追踪性能。

Result: SASTNet在CodecFake+数据集的CoSG测试集上达到了最先进的性能，证明了其在可靠源追踪方面的有效性。

Conclusion: SASTNet通过联合语义和声学特征编码，显著提升了基于神经音频编解码器的深度伪造语音的源追踪性能。

Abstract: Recent attempts at source tracing for codec-based deepfake speech
(CodecFake), generated by neural audio codec-based speech generation (CoSG)
models, have exhibited suboptimal performance. However, how to train source
tracing models using simulated CoSG data while maintaining strong performance
on real CoSG-generated audio remains an open challenge. In this paper, we show
that models trained solely on codec-resynthesized data tend to overfit to
non-speech regions and struggle to generalize to unseen content. To mitigate
these challenges, we introduce the Semantic-Acoustic Source Tracing Network
(SASTNet), which jointly leverages Whisper for semantic feature encoding and
Wav2vec2 with AudioMAE for acoustic feature encoding. Our proposed SASTNet
achieves state-of-the-art performance on the CoSG test set of the CodecFake+
dataset, demonstrating its effectiveness for reliable source tracing.

</details>


### [419] [LeVo: High-Quality Song Generation with Multi-Preference Alignment](https://arxiv.org/abs/2506.07520)
*Shun Lei,Yaoxun Xu,Zhiwei Lin,Huaicheng Zhang,Wei Tan,Hangting Chen,Jianwei Yu,Yixuan Zhang,Chenyu Yang,Haina Zhu,Shuai Wang,Zhiyong Wu,Dong Yu*

Main category: cs.SD

TL;DR: 本文提出LeVo框架，通过并行建模混合与双轨token及多偏好对齐优化，解决现有歌词到歌曲生成在音质、音乐性、指令跟随等方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有歌词到歌曲生成方法因复杂歌曲结构及高质量数据稀缺，导致音质、音乐性、指令跟随及人声-乐器和谐度受限。

Method: LeVo框架包含LeLM（双解码器Transformer并行建模混合/双轨token）与音乐编解码器，采用模块化扩展训练策略防止干扰，并通过半自动数据构建和DPO后训练实现多偏好对齐。

Result: 实验表明LeVo在主客观指标上均超越现有方法，消融实验验证了设计有效性。

Conclusion: LeVo通过创新token建模与偏好对齐方法，显著提升歌词到歌曲生成质量，为复杂音乐合成提供新思路。

Abstract: Recent advances in large language models (LLMs) and audio language models
have significantly improved music generation, particularly in lyrics-to-song
generation. However, existing approaches still struggle with the complex
composition of songs and the scarcity of high-quality data, leading to
limitations in sound quality, musicality, instruction following, and
vocal-instrument harmony. To address these challenges, we introduce LeVo, an
LM-based framework consisting of LeLM and a music codec. LeLM is capable of
parallelly modeling two types of tokens: mixed tokens, which represent the
combined audio of vocals and accompaniment to achieve vocal-instrument harmony,
and dual-track tokens, which separately encode vocals and accompaniment for
high-quality song generation. It employs two decoder-only transformers and a
modular extension training strategy to prevent interference between different
token types. To further enhance musicality and instruction following, we
introduce a multi-preference alignment method based on Direct Preference
Optimization (DPO). This method handles diverse human preferences through a
semi-automatic data construction process and DPO post-training. Experimental
results demonstrate that LeVo consistently outperforms existing methods on both
objective and subjective metrics. Ablation studies further justify the
effectiveness of our designs. Audio examples are available at
https://levo-demo.github.io/.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [420] [Explaining Risks: Axiomatic Risk Attributions for Financial Models](https://arxiv.org/abs/2506.06653)
*Dangxing Chen*

Main category: q-fin.CP

TL;DR: 通过扩展Shapley值框架，公平分配模型预测中的风险贡献，解决高风险领域（如金融）中的风险归因问题。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域（如金融）中，模型预测的风险评估与均值预测同等重要。现有复杂黑箱结构虽成功，但缺乏对风险贡献的公平分配方法，需解决如何合理分配风险的问题。

Method: 通过扩展Shapley值框架，结合公理归因方法，将各特征对模型预测风险的贡献进行公平分配。

Result: 理论分析与实证案例表明，扩展后的Shapley值方法能有效分配风险，为模型风险解释提供可靠依据。

Conclusion: 基于Shapley值的风险归因方法在高风险场景中具有可行性，为模型风险解释及公平分配提供了新思路。

Abstract: In recent years, machine learning models have achieved great success at the
expense of highly complex black-box structures. By using axiomatic attribution
methods, we can fairly allocate the contributions of each feature, thus
allowing us to interpret the model predictions. In high-risk sectors such as
finance, risk is just as important as mean predictions. Throughout this work,
we address the following risk attribution problem: how to fairly allocate the
risk given a model with data? We demonstrate with analysis and empirical
examples that risk can be well allocated by extending the Shapley value
framework.

</details>


### [421] [Uncertainty-Aware Strategies: A Model-Agnostic Framework for Robust Financial Optimization through Subsampling](https://arxiv.org/abs/2506.07299)
*Hans Buehler,Blanka Horvath,Yannick Limmer,Thorsten Schmidt*

Main category: q-fin.CP

TL;DR: 本文提出一种基于不确定性度量的方法，结合子采样策略和改进的随机梯度下降算法，以应对量化金融中模型风险对决策的影响，并通过实证验证其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 量化金融中，模型不确定性可能导致基于有限数据估计的决策显著偏离真实情况。传统经验近似和贝叶斯方法在缺乏自然分布或计算复杂时存在局限性，需开发更鲁棒且高效的方法。

Method: 在传统目标函数上叠加模型空间的不确定性度量（受货币风险度量启发），提出基于子采样的近似策略（类似自助法和小批量采样），并设计并行化随机梯度下降算法以降低内存需求。

Result: 通过多时期、真实数据及高维案例验证：不确定性度量优于传统混合测度策略，子采样方法在抗模型风险方面表现接近复杂贝叶斯方法。

Conclusion: 所提方法在提升模型风险稳健性的同时保持计算效率，为缺乏先验分布或计算资源受限的场景提供了实用解决方案。

Abstract: This paper addresses the challenge of model uncertainty in quantitative
finance, where decisions in portfolio allocation, derivative pricing, and risk
management rely on estimating stochastic models from limited data. In practice,
the unavailability of the true probability measure forces reliance on an
empirical approximation, and even small misestimations can lead to significant
deviations in decision quality. Building on the framework of Klibanoff et al.
(2005), we enhance the conventional objective - whether this is expected
utility in an investing context or a hedging metric - by superimposing an outer
"uncertainty measure", motivated by traditional monetary risk measures, on the
space of models. In scenarios where a natural model distribution is lacking or
Bayesian methods are impractical, we propose an ad hoc subsampling strategy,
analogous to bootstrapping in statistical finance and related to mini-batch
sampling in deep learning, to approximate model uncertainty. To address the
quadratic memory demands of naive implementations, we also present an adapted
stochastic gradient descent algorithm that enables efficient parallelization.
Through analytical, simulated, and empirical studies - including multi-period,
real data and high-dimensional examples - we demonstrate that uncertainty
measures outperform traditional mixture of measures strategies and our
model-agnostic subsampling-based approach not only enhances robustness against
model risk but also achieves performance comparable to more elaborate Bayesian
methods.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [422] [MoE-Gyro: Self-Supervised Over-Range Reconstruction and Denoising for MEMS Gyroscopes](https://arxiv.org/abs/2506.06318)
*Feiyang Pan,Shenghe Zheng,Chunyan Yin,Guangbin Dou*

Main category: eess.SP

TL;DR: 本文提出了一种名为MoE-Gyro的自监督框架，通过混合专家模型同时解决MEMS陀螺仪的信号过载重建和噪声抑制问题，并引入ISEBench作为评估标准。


<details>
  <summary>Details</summary>
Motivation: MEMS陀螺仪在惯性导航和运动控制中至关重要，但其测量范围与噪声性能之间存在固有矛盾。现有硬件解决方案复杂且成本高，深度学习方法则难以在实际场景中部署。

Method: MoE-Gyro框架包含两个专家模型：过载重建专家（ORE）和去噪专家（DE），通过轻量级门控模块动态分配输入信号。同时，提出了ISEBench作为评估IMU信号增强方法的标准。

Result: 实验表明，MoE-Gyro将可测量范围从450 deg/s扩展到1500 deg/s，并将偏置不稳定性降低了98.4%，达到了最先进的性能。

Conclusion: MoE-Gyro有效解决了MEMS陀螺仪在惯性传感中的长期矛盾，为实际应用提供了可行的解决方案。

Abstract: MEMS gyroscopes play a critical role in inertial navigation and motion
control applications but typically suffer from a fundamental trade-off between
measurement range and noise performance. Existing hardware-based solutions
aimed at mitigating this issue introduce additional complexity, cost, and
scalability challenges. Deep-learning methods primarily focus on noise
reduction and typically require precisely aligned ground-truth signals, making
them difficult to deploy in practical scenarios and leaving the fundamental
trade-off unresolved. To address these challenges, we introduce Mixture of
Experts for MEMS Gyroscopes (MoE-Gyro), a novel self-supervised framework
specifically designed for simultaneous over-range signal reconstruction and
noise suppression. MoE-Gyro employs two experts: an Over-Range Reconstruction
Expert (ORE), featuring a Gaussian-Decay Attention mechanism for reconstructing
saturated segments; and a Denoise Expert (DE), utilizing dual-branch
complementary masking combined with FFT-guided augmentation for robust noise
reduction. A lightweight gating module dynamically routes input segments to the
appropriate expert. Furthermore, existing evaluation lack a comprehensive
standard for assessing multi-dimensional signal enhancement. To bridge this
gap, we introduce IMU Signal Enhancement Benchmark (ISEBench), an open-source
benchmarking platform comprising the GyroPeak-100 dataset and a unified
evaluation of IMU signal enhancement methods. We evaluate MoE-Gyro using our
proposed ISEBench, demonstrating that our framework significantly extends the
measurable range from 450 deg/s to 1500 deg/s, reduces Bias Instability by
98.4%, and achieves state-of-the-art performance, effectively addressing the
long-standing trade-off in inertial sensing.

</details>


### [423] [A Reinforcement Learning Approach for RIS-aided Fair Communications](https://arxiv.org/abs/2506.06344)
*Alex Pierron,Michel Barbeau,Luca De Cicco,Jose Rubio-Hernan,Joaquin Garcia-Alfaro*

Main category: eess.SP

TL;DR: 本文提出一种结合可重构智能表面（RIS）与强化学习（RL）的双工系统，旨在优化网络性能与能源效率的同时，确保多用户间的通信公平性。通过仿真实验验证方法有效性，并公开代码与数据集以推动后续研究。


<details>
  <summary>Details</summary>
Motivation: 现有RIS与RL结合的研究虽能提升网络性能与能效，但未充分解决多用户场景下的公平性问题。需确保所有用户设备（UE）获得足够信号强度，避免因资源分配不均导致服务中断。

Method: 通过分析现有工作的公平性缺陷，提出一种新型双工RIS-RL系统优化方法，结合强化学习动态调整电磁波特性，实现多用户间高效且公平的资源分配。

Result: 实验与仿真结果表明，所提方法在提升网络性能和能效的同时，显著改善了用户间的公平性。代码与数据集已开源以支持进一步研究。

Conclusion: 本文提出的双工RIS-RL系统在保障多用户公平通信方面具有有效性，为未来智能通信网络的资源优化提供了新思路与实践基础。

Abstract: Reconfigurable Intelligent Surfaces (RISs) are composed of physical elements
that can dynamically alter electromagnetic wave properties to enhance
beamforming and leading to improvements in areas with low coverage properties.
They have the potential to be combined with Reinforcement Learning (RL)
techniques to achieve network performance and energy efficiency via
optimization techniques. In addition to performance and energy improvements, it
is also crucial to consider the concept of fair communications. RISs must
ensure that User Equipment (UE) units receive their signals with adequate
strength, without other UE being deprived of service due to insufficient power.
In this paper, we address such a problem. We explore the fairness properties of
previous work and propose a novel method that aims at obtaining an efficient
and fair duplex RIS-RL system for multiple legitimate UE units. We report and
discuss our experimental work and simulation results. We also release our code
and datasets to foster further research in the topic.

</details>


### [424] [Deep learning methods for modeling infrasound transmission loss in the middle atmosphere](https://arxiv.org/abs/2506.06351)
*Alexis Le Pichon,Alice Janela Cameijo,Samir Aknine,Youcef Sklab,Souhila Arib,Quentin Brissaud,Sven Peter Naesholm*

Main category: eess.SP

TL;DR: 本研究提出一种优化的卷积神经网络模型，用于预测全球范围内次声波传输损耗，通过改进网络架构和结合大气场数据，显著降低预测误差并扩展传播距离至4000公里。


<details>
  <summary>Details</summary>
Motivation: 现有抛物方程（PE）方法虽能精确建模次声波传输损耗，但计算成本过高，而此前基于卷积神经网络的方法在复杂风场条件（尤其是高频和大距离传播）下表现不佳，需优化模型以提升预测精度和适用性。

Method: 开发优化的卷积神经网络，利用全球模拟的温度与风场组合数据（传播范围达4000公里），通过关键架构优化（如改进网络性能设计）减少预测误差。

Result: 优化模型在0.1-3.2 Hz全频段内平均误差为8.6 dB，且能有效预测真实大气场景下的传输损耗，显著优于先前区域模拟方法。

Conclusion: 所提优化卷积网络在保持低计算成本的同时，显著提升了次声波传输损耗预测的准确性和全局适用性，为国际监测系统的实际应用提供了高效工具。

Abstract: Accurate modeling of infrasound transmission losses (TLs) is essential to
assess the performance of the global International Monitoring System infrasound
network. Among existing propagation modeling tools, parabolic equation (PE)
method enables TLs to be finely modeled, but its computational cost does not
allow exploration of a large parameter space for operational monitoring
applications. To reduce computation times, Brissaud et al. 2023 explored the
potential of convolutional neural networks trained on a large set of regionally
simulated wavefields (< 1000 km from the source) to predict TLs with negligible
computation times compared to PE simulations. However, this method struggles in
unfavorable initial wind conditions, especially at high frequencies, and causal
issues with winds at large distances from the source affecting ground TLs close
to the source. In this study, we have developed an optimized convolutional
network designed to minimize prediction errors while predicting TLs from
globally simulated combined temperature and wind fields spanning over
propagation ranges of 4000 km. Our approach enhances the previously proposed
one by implementing key optimizations that improve the overall architecture
performance. The implemented model predicts TLs with an average error of 8.6 dB
in the whole frequency band (0.1-3.2 Hz) and explored realistic atmospheric
scenarios.

</details>


### [425] [Large Language Models for EEG: A Comprehensive Survey and Taxonomy](https://arxiv.org/abs/2506.06353)
*Naseem Babu,Jimson Mathew,A. P. Vinod*

Main category: eess.SP

TL;DR: 本文综述了大型语言模型（LLMs）与脑电图（EEG）研究的融合，系统整理了LLMs在EEG分析和应用中的最新进展，并提出了结构化的分类法。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs与EEG研究的日益融合，神经解码、脑机接口（BCIs）和情感计算等领域出现了新的研究方向。本文旨在系统回顾和分类这些进展，为未来研究提供基础资源。

Method: 本文通过系统回顾和结构化分类法，将相关文献分为四个领域：LLM启发的EEG表示学习、EEG到语言的解码、跨模态生成（如图像和3D对象合成）以及临床应用和数据集管理工具。

Result: 本文展示了通过微调、少样本和零样本学习，基于Transformer架构的EEG模型能够执行自然语言生成、语义解释和诊断辅助等复杂任务。

Conclusion: 本文为未来通过语言模型桥接自然语言处理和神经信号分析的研究提供了结构化的概述和基础资源。

Abstract: The growing convergence between Large Language Models (LLMs) and
electroencephalography (EEG) research is enabling new directions in neural
decoding, brain-computer interfaces (BCIs), and affective computing. This
survey offers a systematic review and structured taxonomy of recent
advancements that utilize LLMs for EEG-based analysis and applications. We
organize the literature into four domains: (1) LLM-inspired foundation models
for EEG representation learning, (2) EEG-to-language decoding, (3) cross-modal
generation including image and 3D object synthesis, and (4) clinical
applications and dataset management tools. The survey highlights how
transformer-based architectures adapted through fine-tuning, few-shot, and
zero-shot learning have enabled EEG-based models to perform complex tasks such
as natural language generation, semantic interpretation, and diagnostic
assistance. By offering a structured overview of modeling strategies, system
designs, and application areas, this work serves as a foundational resource for
future work to bridge natural language processing and neural signal analysis
through language models.

</details>


### [426] [Towards real-time assessment of infrasound event detection capability using deep learning-based transmission loss estimation](https://arxiv.org/abs/2506.06358)
*Alice Janela Cameijo,Alexis Le Pichon,Youcef Sklab,Souhila Arib,Quentin Brissaud,Sven peter Naesholm,Constantino Listowski,Samir Aknine*

Main category: eess.SP

TL;DR: 本研究通过结合风场和温度场输入及优化神经网络架构，显著提升了次声波传输损耗预测的准确性和适用性，支持国际监测系统对爆炸源的近实时检测评估。


<details>
  <summary>Details</summary>
Motivation: 现有次声波传输损耗模型因计算成本高难以探索大参数空间，且依赖不完整的大气模型（未包含温度场），限制了长距离传播预测能力。需开发高效、兼容性强的深度学习模型。

Method: 使用130公里高度内风场和温度场作为输入，结合卷积层和循环层捕捉空间与距离特征，优化神经网络架构，并集成认知和数据相关不确定性估计。

Result: 神经网络预测平均误差为4dB，在未训练的大气条件和频率（如2022汤加火山喷发案例）中仍有效，支持4000公里范围的次声波传播模拟。

Conclusion: 该模型为近实时评估国际监测系统对爆炸源的检测阈值提供了关键进展，验证了其在大范围、复杂大气条件下的鲁棒性和泛化能力。

Abstract: Accurate modeling of infrasound transmission loss is essential for evaluating
the performance of the International Monitoring System, enabling the effective
design and maintenance of infrasound stations to support compliance of the
Comprehensive Nuclear-Test-Ban Treaty. State-of-the-art propagation modeling
tools enable transmission loss to be finely simulated using atmospheric models.
However, the computational cost prohibits the exploration of a large parameter
space in operational monitoring applications. To address this, recent studies
made use of a deep learning algorithm capable of making transmission loss
predictions almost instantaneously. However, the use of nudged atmospheric
models leads to an incomplete representation of the medium, and the absence of
temperature as an input makes the algorithm incompatible with long range
propagation. In this study, we address these limitations by using both wind and
temperature fields as inputs to a neural network, simulated up to 130 km
altitude and 4,000 km distance. We also optimize several aspects of the neural
network architecture. We exploit convolutional and recurrent layers to capture
spatially and range-dependent features embedded in realistic atmospheric
models, improving the overall performance. The neural network reaches an
average error of 4 dB compared to full parabolic equation simulations and
provides epistemic and data-related uncertainty estimates. Its evaluation on
the 2022 Hunga Tonga-Hunga Ha'apai volcanic eruption demonstrates its
prediction capability using atmospheric conditions and frequencies not included
in the training. This represents a significant step towards near real-time
assessment of International Monitoring System detection thresholds of explosive
sources.

</details>


### [427] [Model-based Neural Data Augmentation for sub-wavelength Radio Localization](https://arxiv.org/abs/2506.06387)
*Baptiste Chatelier,Vincent Corlay,Musa Furkan Keskin,Matthieu Crussière,Henk Wymeersch,Luc Le Magoarou*

Main category: eess.SP

TL;DR: 本文提出一种基于模型神经网络的指纹定位方法，在非视距环境中显著提升定位精度并降低内存需求。


<details>
  <summary>Details</summary>
Motivation: 传统信号处理方法在复杂非视距(NLoS)无线电环境中定位精度不足，而现有机器学习方法存在训练/推理阶段计算复杂度高的问题。

Method: 使用模型驱动神经网络学习位置-信道映射关系，构建生成式神经信道模型以增强指纹比对字典，同时降低存储需求。

Result: 在NLoS环境下实现亚波长级定位精度，较经典指纹方法提升数个数量级精度，同时内存需求降低一个数量级。

Conclusion: 该方法通过神经生成模型有效平衡了定位精度与存储效率，为复杂无线电环境提供了高精度低成本的定位解决方案。

Abstract: The increasing deployment of large antenna arrays at base stations has
significantly improved the spatial resolution and localization accuracy of
radio-localization methods. However, traditional signal processing techniques
struggle in complex radio environments, particularly in scenarios dominated by
non line of sight (NLoS) propagation paths, resulting in degraded localization
accuracy. Recent developments in machine learning have facilitated the
development of machine learning-assisted localization techniques, enhancing
localization accuracy in complex radio environments. However, these methods
often involve substantial computational complexity during both the training and
inference phases. This work extends the well-established fingerprinting-based
localization framework by simultaneously reducing its memory requirements and
improving its accuracy. Specifically, a model-based neural network is used to
learn the location-to-channel mapping, and then serves as a generative neural
channel model. This generative model augments the fingerprinting comparison
dictionary while reducing the memory requirements. The proposed method
outperforms fingerprinting baselines by achieving sub-wavelength localization
accuracy, even in NLoS environments. Remarkably, it offers an improvement by
several orders of magnitude in localization accuracy, while simultaneously
reducing memory requirements by an order of magnitude compared to classical
fingerprinting methods.

</details>


### [428] [Benchmarking Early Agitation Prediction in Community-Dwelling People with Dementia Using Multimodal Sensors and Machine Learning](https://arxiv.org/abs/2506.06306)
*Ali Abedi,Charlene H. Chu,Shehroz S. Khan*

Main category: eess.SP

TL;DR: 本研究开发了基于多模态传感器数据和机器学习的躁动预测模型，利用社区痴呆患者的活动数据，通过引入情境特征和多种算法，实现了高精度预测，支持主动护理。


<details>
  <summary>Details</summary>
Motivation: 痴呆患者躁动行为常见且影响生活质量，社区环境中缺乏持续临床监护。及时预测可早期干预、减轻护理负担并提升患者与护理者生活质量。

Method: 提出基于活动数据的躁动情境特征，评估多种机器学习与深度学习模型（包括二元分类、时序数据分类及异常检测），使用最大公开数据集TIHM（含2803天居家活动、生理及睡眠数据）。

Result: 最佳模型为基于6小时时间戳的LightGBM二元分类器，结合时间与历史躁动信息后，AUC-ROC达0.9720，AUC-PR为0.4320。

Conclusion: 该方法首次实现隐私保护传感器数据下的全面躁动预测基准测试，提供高效、可解释的预测，支持社区痴呆患者的主动护理与居家养老。

Abstract: Agitation is one of the most common responsive behaviors in people living
with dementia, particularly among those residing in community settings without
continuous clinical supervision. Timely prediction of agitation can enable
early intervention, reduce caregiver burden, and improve the quality of life
for both patients and caregivers. This study aimed to develop and benchmark
machine learning approaches for the early prediction of agitation in
community-dwelling older adults with dementia using multimodal sensor data. A
new set of agitation-related contextual features derived from activity data was
introduced and employed for agitation prediction. A wide range of machine
learning and deep learning models was evaluated across multiple problem
formulations, including binary classification for single-timestamp tabular
sensor data and multi-timestamp sequential sensor data, as well as anomaly
detection for single-timestamp tabular sensor data. The study utilized the
Technology Integrated Health Management (TIHM) dataset, the largest publicly
available dataset for remote monitoring of people living with dementia,
comprising 2,803 days of in-home activity, physiology, and sleep data. The most
effective setting involved binary classification of sensor data using the
current 6-hour timestamp to predict agitation at the subsequent timestamp.
Incorporating additional information, such as time of day and agitation
history, further improved model performance, with the highest AUC-ROC of 0.9720
and AUC-PR of 0.4320 achieved by the light gradient boosting machine. This work
presents the first comprehensive benchmarking of state-of-the-art techniques
for agitation prediction in community-based dementia care using
privacy-preserving sensor data. The approach enables accurate, explainable, and
efficient agitation prediction, supporting proactive dementia care and aging in
place.

</details>


### [429] [Leveraging Novel Ensemble Learning Techniques and Landsat Multispectral Data for Estimating Olive Yields in Tunisia](https://arxiv.org/abs/2506.06309)
*Mohamed Kefi,Tien Dat Pham,Thin Nguyen,Mark G. Tjoelker,Viola Devasirvatham,Kenichi Kashiwagi*

Main category: eess.SP

TL;DR: 本研究开发了一种结合遥感、机器学习和地面调查数据的橄榄产量估算方法，在突尼斯地区取得了高精度的预测结果。


<details>
  <summary>Details</summary>
Motivation: 气候变化导致橄榄产量波动显著，准确估算产量对农业管理至关重要。

Method: 利用Landsat-8和Landsat-9卫星影像提取多光谱反射带和植被指数，结合数字高程模型和地面调查数据，使用AutoGluon构建自动化集成学习框架进行模型训练和评估。

Result: Landsat-8 OLI的R²为0.8635，RMSE为1.17吨/公顷；Landsat-9 OLI-2的R²为0.8378，RMSE为1.32吨/公顷，预测性能优异。

Conclusion: 该方法具有可扩展性、成本效益和准确性，适用于全球多种农业区域的产量估算。

Abstract: Olive production is an important tree crop in Mediterranean climates.
However, olive yield varies significantly due to climate change. Accurately
estimating yield using remote sensing and machine learning remains a complex
challenge. In this study, we developed a streamlined pipeline for olive yield
estimation in the Kairouan and Sousse governorates of Tunisia. We extracted
features from multispectral reflectance bands, vegetation indices derived from
Landsat-8 OLI and Landsat-9 OLI-2 satellite imagery, along with digital
elevation model data. These spatial features were combined with ground-based
field survey data to form a structured tabular dataset. We then developed an
automated ensemble learning framework, implemented using AutoGluon to train and
evaluate multiple machine learning models, select optimal combinations through
stacking, and generate robust yield predictions using five-fold
cross-validation. The results demonstrate strong predictive performance from
both sensors, with Landsat-8 OLI achieving R2 = 0.8635 and RMSE = 1.17 tons per
ha, and Landsat-9 OLI-2 achieving R2 = 0.8378 and RMSE = 1.32 tons per ha. This
study highlights a scalable, cost-effective, and accurate method for olive
yield estimation, with potential applicability across diverse agricultural
regions globally.

</details>


### [430] [Enhancing Contrastive Learning-based Electrocardiogram Pretrained Model with Patient Memory Queue](https://arxiv.org/abs/2506.06310)
*Xiaoyu Sun,Yang Yang,Xunde Dong*

Main category: eess.SP

TL;DR: 本文提出了一种基于对比学习的心电图预训练模型，通过引入患者记忆队列和额外数据增强方法，解决了现有方法在利用患者一致性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 在心电图自动诊断领域，由于标记数据有限，如何基于未标记数据构建鲁棒的预训练模型是研究重点。现有基于对比学习的模型未能充分利用患者一致性。

Method: 提出了一种基于对比学习的心电图预训练模型，引入患者记忆队列（PMQ）以缓解样本不足导致的模型退化，并采用两种额外数据增强方法提供更多正负样本视角。

Result: 在三个公开数据集上的实验表明，该方法在有限标记数据场景下优于现有对比学习方法，并表现出更强的鲁棒性。

Conclusion: 本文提出的方法通过患者记忆队列和数据增强，显著提升了心电图预训练模型的性能，尤其在标记数据有限的情况下表现优异。

Abstract: In the field of automatic Electrocardiogram (ECG) diagnosis, due to the
relatively limited amount of labeled data, how to build a robust ECG pretrained
model based on unlabeled data is a key area of focus for researchers. Recent
advancements in contrastive learning-based ECG pretrained models highlight the
potential of exploiting the additional patient-level self-supervisory signals
inherent in ECG. They are referred to as patient contrastive learning. Its
rationale is that multiple physical recordings from the same patient may share
commonalities, termed patient consistency, so redefining positive and negative
pairs in contrastive learning as intrapatient and inter-patient samples
provides more shared context to learn an effective representation. However,
these methods still fail to efficiently exploit patient consistency due to the
insufficient amount of intra-inter patient samples existing in a batch. Hence,
we propose a contrastive learning-based ECG pretrained model enhanced by the
Patient Memory Queue (PMQ), which incorporates a large patient memory queue to
mitigate model degeneration that can arise from insufficient intra-inter
patient samples. In order to further enhance the performance of the pretrained
model, we introduce two extra data augmentation methods to provide more
perspectives of positive and negative pairs for pretraining. Extensive
experiments were conducted on three public datasets with three different data
ratios. The experimental results show that the comprehensive performance of our
method outperforms previous contrastive learning methods and exhibits greater
robustness in scenarios with limited labeled data. The code is available at
https://github.com/3hiuwoo/PMQ.

</details>


### [431] [A Novel Shape-Aware Topological Representation for GPR Data with DNN Integration](https://arxiv.org/abs/2506.06311)
*Meiyan Kang,Shizuo Kaji,Sang-Yun Lee,Taegon Kim,Hee-Hwan Ryu,Suyoung Choi*

Main category: eess.SP

TL;DR: 该研究提出了一种结合拓扑数据分析（TDA）和YOLOv5深度学习网络的新框架，通过增强B-scan GPR图像的结构特征，提升地下管线检测精度，并采用Sim2Real策略解决真实数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 传统地下探地雷达（GPR）解释方法存在噪声敏感性和结构特征提取不足的局限性，制约了地下设施检测的准确性和可靠性。

Method: 提出基于TDA的形状感知拓扑特征表示方法，与YOLOv5结合进行空间检测；利用Sim2Real策略生成合成数据集以弥补真实标注数据不足。

Result: 实验显示平均精度均值（mAP）显著提升，验证了方法在噪声鲁棒性和几何特征响应上的有效性。

Conclusion: TDA增强学习框架为实时可靠的地下物体检测提供了新途径，可广泛应用于城市规划、安全检测和基础设施管理领域。

Abstract: Ground Penetrating Radar (GPR) is a widely used Non-Destructive Testing (NDT)
technique for subsurface exploration, particularly in infrastructure inspection
and maintenance. However, conventional interpretation methods are often limited
by noise sensitivity and a lack of structural awareness. This study presents a
novel framework that enhances the detection of underground utilities,
especially pipelines, by integrating shape-aware topological features derived
from B-scan GPR images using Topological Data Analysis (TDA), with the spatial
detection capabilities of the YOLOv5 deep neural network (DNN). We propose a
novel shape-aware topological representation that amplifies structural features
in the input data, thereby improving the model's responsiveness to the
geometrical features of buried objects. To address the scarcity of annotated
real-world data, we employ a Sim2Real strategy that generates diverse and
realistic synthetic datasets, effectively bridging the gap between simulated
and real-world domains. Experimental results demonstrate significant
improvements in mean Average Precision (mAP), validating the robustness and
efficacy of our approach. This approach underscores the potential of
TDA-enhanced learning in achieving reliable, real-time subsurface object
detection, with broad applications in urban planning, safety inspection, and
infrastructure management.

</details>


### [432] [An Open-Source Python Framework and Synthetic ECG Image Datasets for Digitization, Lead and Lead Name Detection, and Overlapping Signal Segmentation](https://arxiv.org/abs/2506.06315)
*Masoud Rahimi,Reza Karbasi,Abdol-Hossein Vahabie*

Main category: eess.SP

TL;DR: 本文提出一个开源Python框架，用于生成合成ECG图像数据集，支持ECG数字化、导联区域/名称检测及波形分割任务，并发布四个公开数据集及对应代码。


<details>
  <summary>Details</summary>
Motivation: 推动基于深度学习的ECG分析任务（如波形分割、导联检测等），解决真实标注ECG图像数据稀缺的问题。

Method: 基于PTB-XL信号数据集，构建框架生成四种数据集：含时间序列的ECG图像、YOLO标注导联区域/名称的数据集、单导联分割掩码（含正常与重叠版本）。重叠版本在目标导联图像中叠加相邻导联波形但保留干净掩码。

Result: 发布开源框架及四个数据集：1) ECG图像-信号配对数据；2) YOLO格式导联检测数据；3-4) U-Net兼容的单导联分割掩码（含重叠与非重叠版本）。代码与数据集已公开。

Conclusion: 所提框架与合成数据集为ECG分析任务（如波形分割、导联检测）提供了标准化数据支持，通过开源促进相关深度学习模型开发。

Abstract: We introduce an open-source Python framework for generating synthetic ECG
image datasets to advance critical deep learning-based tasks in ECG analysis,
including ECG digitization, lead region and lead name detection, and
pixel-level waveform segmentation. Using the PTB-XL signal dataset, our
proposed framework produces four open-access datasets: (1) ECG images in
various lead configurations paired with time-series signals for ECG
digitization, (2) ECG images annotated with YOLO-format bounding boxes for
detection of lead region and lead name, (3)-(4) cropped single-lead images with
segmentation masks compatible with U-Net-based models in normal and overlapping
versions. In the overlapping case, waveforms from neighboring leads are
superimposed onto the target lead image, while the segmentation masks remain
clean. The open-source Python framework and datasets are publicly available at
https://github.com/rezakarbasi/ecg-image-and-signal-dataset and
https://doi.org/10.5281/zenodo.15484519, respectively.

</details>


### [433] [Composite Reward Design in PPO-Driven Adaptive Filtering](https://arxiv.org/abs/2506.06323)
*Abdullah Burkan Bereketoglu*

Main category: eess.SP

TL;DR: 本文提出了一种基于PPO的自适应滤波框架，用于动态非平稳环境中的去噪，优于传统滤波器。


<details>
  <summary>Details</summary>
Motivation: 传统滤波器如LMS、RLS、Wiener和Kalman在非平稳环境中表现受限，需要复杂调优或精确噪声统计。

Method: 使用PPO算法，结合复合奖励函数，平衡SNR提升、MSE降低和残差平滑度。

Result: 实验表明，PPO代理在多种噪声类型下表现优异，具有实时性能，超越经典滤波器。

Conclusion: 本文证明了策略梯度强化学习在鲁棒、低延迟自适应信号滤波中的可行性。

Abstract: Model-free and reinforcement learning-based adaptive filtering methods are
gaining traction for denoising in dynamic, non-stationary environments such as
wireless signal channels. Traditional filters like LMS, RLS, Wiener, and Kalman
are limited by assumptions of stationary or requiring complex fine-tuning or
exact noise statistics or fixed models. This letter proposes an adaptive
filtering framework using Proximal Policy Optimization (PPO), guided by a
composite reward that balances SNR improvement, MSE reduction, and residual
smoothness. Experiments on synthetic signals with various noise types show that
our PPO agent generalizes beyond its training distribution, achieving real-time
performance and outperforming classical filters. This work demonstrates the
viability of policy-gradient reinforcement learning for robust, low-latency
adaptive signal filtering.

</details>


### [434] [Uncertainty-Aware Multi-view Arrhythmia Classification from ECG](https://arxiv.org/abs/2506.06342)
*Mohd Ashhad,Sana Rahmani,Mohammed Fayiz,Ali Etemad,Javad Hashemi*

Main category: eess.SP

TL;DR: 本文提出了一种深度神经网络架构，用于从心电图中进行不确定性感知的多视角心律失常分类，融合了1D和2D视图以提升分类效果和抗噪能力。


<details>
  <summary>Details</summary>
Motivation: 心电图中存在噪声和伪影，导致不同视角的信息冲突，本文旨在通过不确定性感知的多视角融合技术，提升心律失常分类的准确性和鲁棒性。

Method: 方法包括三个模块：时间序列模块用于学习心电图的形态特征，图像空间学习模块用于学习时空特征，不确定性感知融合模块用于融合不同视角的信息。

Result: 在两个真实数据集上的实验表明，本文方法不仅提升了心律失常分类的性能，还表现出更好的抗噪能力。

Conclusion: 本文提出的框架通过多视角融合和不确定性感知，显著提升了心律失常分类的准确性和鲁棒性，具有实际应用价值。

Abstract: We propose a deep neural architecture that performs uncertainty-aware
multi-view classification of arrhythmia from ECG. Our method learns two
different views (1D and 2D) of single-lead ECG to capture different types of
information. We use a fusion technique to reduce the conflict between the
different views caused by noise and artifacts in ECG data, thus incorporating
uncertainty to obtain stronger final predictions. Our framework contains the
following three modules (1) a time-series module to learn the morphological
features from ECG; (2) an image-space learning module to learn the
spatiotemporal features; and (3) the uncertainty-aware fusion module to fuse
the information from the two different views. Experimental results on two
real-world datasets demonstrate that our framework not only improves the
performance on arrhythmia classification compared to the state-of-the-art but
also shows better robustness to noise and artifacts present in ECG.

</details>


### [435] [LD-RPMNet: Near-Sensor Diagnosis for Railway Point Machines](https://arxiv.org/abs/2506.06346)
*Wei Li,Xiaochun Wu,Xiaoxi Hu,Yuxuan Zhang,Sebastian Bader,Yuhan Huang*

Main category: eess.SP

TL;DR: 本文提出了一种轻量级模型LD-RPMNet，结合了Transformer和卷积神经网络，通过多尺度深度可分离卷积和广播自注意力机制，显著降低了参数数量和计算复杂度，同时提高了铁路道岔机故障诊断的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着近传感器诊断在工业中的普及，本文旨在开发一种轻量级模型，以优化计算效率并提高铁路道岔机故障诊断的准确性。

Method: LD-RPMNet模型集成了Transformer和卷积神经网络，引入了多尺度深度可分离卷积模块和广播自注意力机制，以增强特征提取并简化复杂矩阵运算。

Result: 实验结果表明，优化后的模型减少了50%的参数数量和计算复杂度，同时将诊断准确性提高了近3%，最终达到了98.86%的准确率。

Conclusion: 该研究展示了在铁路道岔机中应用近传感器故障诊断的可能性，为实际工业应用提供了高效且准确的解决方案。

Abstract: Near-sensor diagnosis has become increasingly prevalent in industry. This
study proposes a lightweight model named LD-RPMNet that integrates Transformers
and Convolutional Neural Networks, leveraging both local and global feature
extraction to optimize computational efficiency for a practical railway
application. The LD-RPMNet introduces a Multi-scale Depthwise Separable
Convolution (MDSC) module, which decomposes cross-channel convolutions into
pointwise and depthwise convolutions while employing multi-scale kernels to
enhance feature extraction. Meanwhile, a Broadcast Self-Attention (BSA)
mechanism is incorporated to simplify complex matrix multiplications and
improve computational efficiency. Experimental results based on collected sound
signals during the operation of railway point machines demonstrate that the
optimized model reduces parameter count and computational complexity by 50%
while improving diagnostic accuracy by nearly 3%, ultimately achieving an
accuracy of 98.86%. This demonstrates the possibility of near-sensor fault
diagnosis applications in railway point machines.

</details>


### [436] [Multi-Platform Methane Plume Detection via Model and Domain Adaptation](https://arxiv.org/abs/2506.06348)
*Vassiliki Mancoridis,Brian Bue,Jake H. Lee,Andrew K. Thorpe,Daniel Cusworth,Alana Ayasse,Philip G. Brodrick,Riley Duren*

Main category: eess.SP

TL;DR: 本文提出了一种基于机器学习的跨平台甲烷羽流检测方法，通过迁移学习和CycleGAN技术，提升了空间观测数据的检测效果。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多的遥感平台用于甲烷羽流检测，跨平台数据对齐的需求日益增加。本文旨在解决这一问题，提升空间观测数据的甲烷羽流检测能力。

Method: 本文结合模型驱动和数据驱动的机器学习方法，利用航空观测数据改进空间观测的甲烷羽流检测。具体包括使用迁移学习优化分类器，以及通过CycleGAN进行数据分布对齐。

Result: 通过迁移学习和CycleGAN技术，本文提出的方法在空间观测数据上的甲烷羽流检测效果优于独立的空间观测模型。

Conclusion: 本文不仅展示了甲烷羽流检测的有效方法，还提供了一种数据驱动的跨平台数据对齐策略，适用于不同遥感仪器的相关产品对齐。

Abstract: Prioritizing methane for near-term climate action is crucial due to its
significant impact on global warming. Previous work used columnwise matched
filter products from the airborne AVIRIS-NG imaging spectrometer to detect
methane plume sources; convolutional neural networks (CNNs) discerned
anthropogenic methane plumes from false positive enhancements. However, as an
increasing number of remote sensing platforms are used for methane plume
detection, there is a growing need to address cross-platform alignment. In this
work, we describe model- and data-driven machine learning approaches that
leverage airborne observations to improve spaceborne methane plume detection,
reconciling the distributional shifts inherent with performing the same task
across platforms. We develop a spaceborne methane plume classifier using data
from the EMIT imaging spectroscopy mission. We refine classifiers trained on
airborne imagery from AVIRIS-NG campaigns using transfer learning,
outperforming the standalone spaceborne model. Finally, we use CycleGAN, an
unsupervised image-to-image translation technique, to align the data
distributions between airborne and spaceborne contexts. Translating spaceborne
EMIT data to the airborne AVIRIS-NG domain using CycleGAN and applying airborne
classifiers directly yields the best plume detection results. This methodology
is useful not only for data simulation, but also for direct data alignment.
Though demonstrated on the task of methane plume detection, our work more
broadly demonstrates a data-driven approach to align related products obtained
from distinct remote sensing instruments.

</details>


### [437] [Heart Rate Classification in ECG Signals Using Machine Learning and Deep Learning](https://arxiv.org/abs/2506.06349)
*Thien Nhan Vo,Thanh Xuan Truong*

Main category: eess.SP

TL;DR: 本研究对比传统机器学习（手工特征）与深度学习（图像转换）在心电图信号分类中的效果，结果表明LightGBM模型以99%准确率和0.94 F1分数优于图像方法，验证手工特征在捕捉ECG时态和形态变化的优势。


<details>
  <summary>Details</summary>
Motivation: 探索传统特征工程与深度学习在ECG信号分类中的性能差异，验证手工设计特征对心跳时序和形态特征的捕捉能力是否优于图像化表示方法。

Method: 1. 传统方法：提取HRV、均值、方差、RR间期等特征，使用SVM、随机森林、AdaBoost、LSTM等分类器；2. 深度学习方法：将ECG信号转换为GAF、MTF、RP图像，采用VGG、Inception等CNN进行分类。

Result: LightGBM模型表现最佳（准确率99%，F1分数0.94），显著优于图像方法（F1分数0.85）。SVM和AdaBoost效果较差，表明其不适用于此任务。

Conclusion: 手工特征能更有效表征ECG信号的时序和形态特征，未来可通过整合多导联信号和连续心跳的依赖关系进一步提升分类性能。

Abstract: This study addresses the classification of heartbeats from ECG signals
through two distinct approaches: traditional machine learning utilizing
hand-crafted features and deep learning via transformed images of ECG beats.
The dataset underwent preprocessing steps, including downsampling, filtering,
and normalization, to ensure consistency and relevance for subsequent analysis.
In the first approach, features such as heart rate variability (HRV), mean,
variance, and RR intervals were extracted to train various classifiers,
including SVM, Random Forest, AdaBoost, LSTM, Bi-directional LSTM, and
LightGBM. The second approach involved transforming ECG signals into images
using Gramian Angular Field (GAF), Markov Transition Field (MTF), and
Recurrence Plots (RP), with these images subsequently classified using CNN
architectures like VGG and Inception.
  Experimental results demonstrate that the LightGBM model achieved the highest
performance, with an accuracy of 99% and an F1 score of 0.94, outperforming the
image-based CNN approach (F1 score of 0.85). Models such as SVM and AdaBoost
yielded significantly lower scores, indicating limited suitability for this
task. The findings underscore the superior ability of hand-crafted features to
capture temporal and morphological variations in ECG signals compared to
image-based representations of individual beats. Future investigations may
benefit from incorporating multi-lead ECG signals and temporal dependencies
across successive beats to enhance classification accuracy further.

</details>


### [438] [Towards Generalizable Drowsiness Monitoring with Physiological Sensors: A Preliminary Study](https://arxiv.org/abs/2506.06360)
*Jiyao Wang,Suzan Ayas,Jiahao Zhang,Xiao Wen,Dengbo He,Birsen Donmez*

Main category: eess.SP

TL;DR: 通过分析心电图、皮肤电活动和呼吸信号，研究发现不同疲劳诱导因素会导致不同的生理反应，客观评估比主观评估更敏感，心率稳定性增加、呼吸幅度减少和皮肤电活动降低与疲劳增加相关。


<details>
  <summary>Details</summary>
Motivation: 准确检测疲劳对驾驶安全至关重要，基于生理信号的疲劳监测比基于摄像头的方法更能保护隐私，但不同数据集中生理指标与疲劳标签的关联存在冲突。

Method: 分析了四个数据集中的心电图、皮肤电活动和呼吸信号的关键特征，使用二元逻辑回归模型识别与疲劳相关的生理指标。

Result: 不同疲劳诱导因素会导致不同的生理反应，客观评估比主观评估更敏感，心率稳定性增加、呼吸幅度减少和皮肤电活动降低与疲劳增加相关。

Conclusion: 研究结果增强了对疲劳检测的理解，并为未来可推广的监测设计提供了信息。

Abstract: Accurately detecting drowsiness is vital to driving safety. Among all
measures, physiological-signal-based drowsiness monitoring can be more
privacy-preserving than a camera-based approach. However, conflicts exist
regarding how physiological metrics are associated with different drowsiness
labels across datasets. Thus, we analyzed key features from electrocardiograms
(ECG), electrodermal activity (EDA), and respiratory (RESP) signals across four
datasets, where different drowsiness inducers (such as fatigue and low arousal)
and assessment methods (subjective vs. objective) were used. Binary logistic
regression models were built to identify the physiological metrics that are
associated with drowsiness. Findings indicate that distinct different
drowsiness inducers can lead to different physiological responses, and
objective assessments were more sensitive than subjective ones in detecting
drowsiness. Further, the increased heart rate stability, reduced respiratory
amplitude, and decreased tonic EDA are robustly associated with increased
drowsiness. The results enhance understanding of drowsiness detection and can
inform future generalizable monitoring designs.

</details>


### [439] [Transformer-Based Decomposition of Electrodermal Activity for Real-World Mental Health Applications](https://arxiv.org/abs/2506.06378)
*Charalampos Tsirmpas,Stasinos Konstantopoulos,Dimitris Andrikopoulos,Konstantina Kyriakouli,Panagiotis Fatouros*

Main category: eess.SP

TL;DR: 本研究比较了知识驱动、统计和深度学习方法在分解皮肤电活动（EDA）信号中的应用，提出了一种基于Transformer的新模型Feel Transformer，用于无监督分离短期和长期成分，并在真实数据中表现出色。


<details>
  <summary>Details</summary>
Motivation: 分解皮肤电活动（EDA）信号为短期（phasic）和长期（tonic）成分对于提取情感和生理生物标志物至关重要，尤其是在可穿戴设备采集的野外数据中。

Method: 研究提出了一种基于Transformer的模型Feel Transformer，采用Autoformer架构，利用池化和趋势去除机制实现无监督的EDA信号分解，并与Ledalab、cvxEDA等传统方法进行比较。

Result: Feel Transformer在特征保真度（如SCR频率、振幅和tonic斜率）和对噪声数据的鲁棒性之间取得了平衡，表现出在实时生物信号分析中的潜力。

Conclusion: Feel Transformer在分解EDA信号方面表现出色，未来可应用于压力预测、数字心理健康干预和生理预测等领域。

Abstract: Decomposing Electrodermal Activity (EDA) into phasic (short-term,
stimulus-linked responses) and tonic (longer-term baseline) components is
essential for extracting meaningful emotional and physiological biomarkers.
This study presents a comparative analysis of knowledge-driven, statistical,
and deep learning-based methods for EDA signal decomposition, with a focus on
in-the-wild data collected from wearable devices. In particular, the authors
introduce the Feel Transformer, a novel Transformer-based model adapted from
the Autoformer architecture, designed to separate phasic and tonic components
without explicit supervision. The model leverages pooling and trend-removal
mechanisms to enforce physiologically meaningful decompositions. Comparative
experiments against methods such as Ledalab, cvxEDA, and conventional
detrending show that the Feel Transformer achieves a balance between feature
fidelity (SCR frequency, amplitude, and tonic slope) and robustness to noisy,
real-world data. The model demonstrates potential for real-time biosignal
analysis and future applications in stress prediction, digital mental health
interventions, and physiological forecasting.

</details>


### [440] [IQFM A Wireless Foundational Model for I/Q Streams in AI-Native 6G](https://arxiv.org/abs/2506.06718)
*Omar Mashaal,Hatem Abou-Zeid*

Main category: eess.SP

TL;DR: 本文提出首个基于原始IQ信号的无线通信基础模型IQFM，支持调制分类、到达角估计等任务，无需复杂预处理，通过对比自监督学习与任务感知增强策略，在少量标注数据下显著超越监督基线，并展示跨任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在自然语言处理和计算机视觉领域表现优异，但在无线通信中仍处于早期阶段。现有研究多关注图像化数据（如CSI、频谱图），而直接处理原始IQ信号的基础模型尚未充分探索。IQFM旨在填补这一空白，为多任务学习提供高效、可复用的编码器。

Method: 提出任务感知增强策略，将数据增强分为核心增强（如循环时间偏移）和任务特定增强，结合对比自监督学习框架。使用轻量级编码器在无线多天线IQ数据上进行预训练，并通过LoRA进行少量参数微调以适应新任务。

Result: IQFM在调制分类和AoA分类中分别达到99.67%和65.45%准确率（每类仅需1个标注样本），超越监督基线7倍和145倍。在波束预测（94.15%）、RML2016a调制分类（50.00%）和RF指纹（96.05%）等分布外任务中，仅需少量样本即接近或超过全监督模型性能。

Conclusion: IQFM验证了原始IQ信号基础模型作为6G原生AI系统多任务学习核心组件的潜力，通过高效自监督预训练和轻量微调机制，显著降低标注数据需求，为未来无线通信智能框架提供新范式。

Abstract: Foundational models have shown remarkable potential in natural language
processing and computer vision, yet remain in their infancy in wireless
communications. While a few efforts have explored image-based modalities such
as channel state information (CSI) and frequency spectrograms, foundational
models that operate directly on raw IQ data remain largely unexplored. This
paper presents, IQFM, the first I/Q signal foundational model for wireless
communications. IQFM supporting diverse tasks: modulation classification,
angle-of-arrival (AoA), beam prediction, and RF fingerprinting, without heavy
preprocessing or handcrafted features. We also introduce a task-aware
augmentation strategy that categorizes transformations into core augmentations,
such as cyclic time shifting, and task-specific augmentations. This strategy
forms the basis for structured, task-dependent representation learning within a
contrastive self-supervised learning (SSL) framework. Using this strategy, the
lightweight encoder, pre-trained via SSL on over-the-air multi-antenna IQ data,
achieves up to 99.67% and 65.45% accuracy on modulation and AoA classification,
respectively, using only one labeled sample per class, outperforming supervised
baselines by up to 7x and 145x. The model also generalizes to
out-of-distribution tasks; when adapted to new tasks using only 500 samples per
class and minimal parameter updates via LoRA, the same frozen encoder achieves
94.15% on beam prediction (vs. 89.53% supervised), 50.00% on RML2016a
modulation classification (vs. 49.30%), and 96.05% on RF fingerprinting (vs.
96.64%). These results demonstrate the potential of raw IQ-based foundational
models as efficient, reusable encoders for multi-task learning in AI-native 6G
systems.

</details>


### [441] [Conditional Denoising Diffusion for ISAC Enhanced Channel Estimation in Cell-Free 6G](https://arxiv.org/abs/2506.06942)
*Mohammad Farzanullah,Han Zhang,Akram Bin Sediq,Ali Afana,Melike Erol-Kantarci*

Main category: eess.SP

TL;DR: 本文提出了一种基于条件去噪扩散模型（CDDM）和多模态Transformer（MMT）的新框架，用于提升无蜂窝集成感知与通信（ISAC）系统中的信道估计性能。该框架通过利用感知信息，显著提高了信道估计的精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 无蜂窝ISAC系统在6G网络中具有重要潜力，但信道估计性能常受限于导频污染和噪声等问题。本文旨在通过结合感知信息，提升信道估计的可靠性和效率。

Method: 本文提出了一种新框架，将CDDM与MMT结合，利用感知和位置数据之间的跨模态关系，迭代去噪和优化信道估计。

Result: 仿真结果表明，该模型在归一化均方误差（NMSE）上比LS和MMSE估计器分别提升了8 dB和9 dB，比传统去噪扩散模型（TDDM）提升了27.8%。此外，该模型在低信噪比和导频污染条件下表现出更高的鲁棒性。

Conclusion: 本文提出的框架通过有效利用感知信息，显著提升了无蜂窝ISAC系统的信道估计性能，尤其在感知目标附近的用户中表现优异。

Abstract: Cell-free Integrated Sensing and Communication (ISAC) aims to revolutionize
6th Generation (6G) networks. By combining distributed access points with ISAC
capabilities, it boosts spectral efficiency, situational awareness, and
communication reliability. Channel estimation is a critical step in cell-free
ISAC systems to ensure reliable communication, but its performance is usually
limited by challenges such as pilot contamination and noisy channel estimates.
This paper presents a novel framework leveraging sensing information as a key
input within a Conditional Denoising Diffusion Model (CDDM). In this framework,
we integrate CDDM with a Multimodal Transformer (MMT) to enhance channel
estimation in ISAC-enabled cell-free systems. The MMT encoder effectively
captures inter-modal relationships between sensing and location data, enabling
the CDDM to iteratively denoise and refine channel estimates. Simulation
results demonstrate that the proposed approach achieves significant performance
gains. As compared with Least Squares (LS) and Minimum Mean Squared Error
(MMSE) estimators, the proposed model achieves normalized mean squared error
(NMSE) improvements of 8 dB and 9 dB, respectively. Moreover, we achieve a
27.8% NMSE improvement compared to the traditional denoising diffusion model
(TDDM), which does not incorporate sensing channel information. Additionally,
the model exhibits higher robustness against pilot contamination and maintains
high accuracy under challenging conditions, such as low signal-to-noise ratios
(SNRs). According to the simulation results, the model performs well for users
near sensing targets by leveraging the correlation between sensing and
communication channels.

</details>


### [442] [Diffusion Models-Aided Uplink Channel Estimation for RIS-Assisted Systems](https://arxiv.org/abs/2506.07770)
*Yang Wang,Yin Xu,Cixiao Zhang,Zhiyong Chen,Xiaowu Ou,Mingzeng Dai,Meixia Tao,Wenjun Zhang*

Main category: eess.SP

TL;DR: 本文提出了一种基于扩散模型（DM）框架的可重构智能表面（RIS）辅助系统的信道估计方法，通过确定性采样策略和轻量级网络设计，显著提升了信道估计的准确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统扩散模型在反向过程中存在的随机性问题，并提高信道估计的准确性，本文提出了一种新的方法。

Method: 本文将信道估计问题重新表述为去噪过程，采用确定性采样策略和步长对齐机制，并设计了一个轻量级网络以减少参数数量。

Result: 仿真结果表明，该方法在广泛的信噪比（SNR）范围内优于基线方法，特别是在SNR = 0 dB时，归一化均方误差（NMSE）提升了13.5 dB。

Conclusion: 所提出的方法在保持高性能的同时，显著减少了参数数量，增强了实际应用性。

Abstract: This letter proposes a channel estimation method for reconfigurable
intelligent surface (RIS)-assisted systems through a novel diffusion model (DM)
framework. We reformulate the channel estimation problem as a denoising
process, which aligns with the reverse process of the DM. To overcome the
inherent randomness in the reverse process of conventional DM approaches, we
adopt a deterministic sampling strategy with a step alignment mechanism that
ensures the accuracy of channel estimation while adapting to different
signal-to-noise ratio (SNR). Furthermore, to reduce the number of parameters of
the U-Net, we meticulously design a lightweight network that achieves
comparable performance, thereby enhancing the practicality of our proposed
method. Extensive simulations demonstrate superior performance over a wide
range of SNRs compared to baselines. For instance, the proposed method achieves
performance improvements of up to 13.5 dB in normalized mean square error
(NMSE) at SNR = 0 dB. Notably, the proposed lightweight network exhibits almost
no performance loss compared to the original U-Net, while requiring only 6.59\%
of its parameters.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [443] [Cost-Efficient LLM Training with Lifetime-Aware Tensor Offloading via GPUDirect Storage](https://arxiv.org/abs/2506.06472)
*Ziqi Yuan,Haoyang Zhang,Yirui Eric Zhou,Apoorve Mohan,I-Hsin Chung,Seetharami Seelam,Jian Huang*

Main category: cs.DC

TL;DR: 本文介绍了TERAIO，一种用于GPU内存扩展的寿命感知张量卸载框架，通过低成本的PCIe SSD优化大语言模型（LLM）训练。


<details>
  <summary>Details</summary>
Motivation: 在LLM训练中，活跃张量仅占GPU内存的一小部分，而大量不活跃张量长期未被使用，这为通过SSD进行张量卸载/预取提供了机会，从而避免GPU训练过程中的停滞。

Method: TERAIO通过分析训练过程的前几次迭代，准确估计每个张量的寿命，并生成优化的张量卸载/预取计划，通过PyTorch集成到编译的LLM程序中。它使用GPUDirect存储直接执行张量迁移，以缓解CPU瓶颈并最大化SSD带宽利用率。

Result: 与ZeRO-Offload和ZeRO-Infinity等先进研究相比，TERAIO将各种LLM的训练性能平均提高了1.47倍，并达到了假设无限GPU内存的理想性能的80.7%。

Conclusion: TERAIO通过有效的张量寿命分析和优化的卸载/预取策略，显著提升了LLM训练的性能，接近理想GPU内存条件下的表现。

Abstract: We present the design and implementation of a new lifetime-aware tensor
offloading framework for GPU memory expansion using low-cost PCIe-based
solid-state drives (SSDs). Our framework, TERAIO, is developed explicitly for
large language model (LLM) training with multiple GPUs and multiple SSDs. Its
design is driven by our observation that the active tensors take only a small
fraction (1.7% on average) of allocated GPU memory in each LLM training
iteration, the inactive tensors are usually large and will not be used for a
long period of time, creating ample opportunities for offloading/prefetching
tensors to/from slow SSDs without stalling the GPU training process. TERAIO
accurately estimates the lifetime (active period of time in GPU memory) of each
tensor with the profiling of the first few iterations in the training process.
With the tensor lifetime analysis, TERAIO will generate an optimized tensor
offloading/prefetching plan and integrate it into the compiled LLM program via
PyTorch. TERAIO has a runtime tensor migration engine to execute the
offloading/prefetching plan via GPUDirect storage, which allows direct tensor
migration between GPUs and SSDs for alleviating the CPU bottleneck and
maximizing the SSD bandwidth utilization. In comparison with state-of-the-art
studies such as ZeRO-Offload and ZeRO-Infinity, we show that TERAIO improves
the training performance of various LLMs by 1.47x on average, and achieves
80.7% of the ideal performance assuming unlimited GPU memory.

</details>


### [444] [pFedSOP : Accelerating Training Of Personalized Federated Learning Using Second-Order Optimization](https://arxiv.org/abs/2506.07159)
*Mrinmay Sen,Chalavadi Krishna Mohan*

Main category: cs.DC

TL;DR: 本文提出pFedSOP方法，通过二阶优化加速个性化联邦学习，减少通信轮次并提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有PFL方法因使用一阶优化导致训练速度慢、通信轮次多，且搜索个性化模型时需额外本地计算。二阶优化虽能加速但受限于Hessian矩阵计算困难。

Method: pFedSOP利用基于Gompertz函数的归一化梯度角度生成个性化梯度更新，并通过正则化Fisher信息矩阵（FIM）近似Hessian实现二阶优化，避免额外数据输入。

Result: 在异构图像分类数据集上的实验表明，pFedSOP在部分客户端参与情况下优于现有FL和PFL算法。

Conclusion: pFedSOP通过高效二阶优化解决了PFL的通信与计算瓶颈，在减少通信轮次的同时显著提升个性化模型性能。

Abstract: Personalized Federated Learning (PFL) enables clients to collaboratively
train personalized models tailored to their individual objectives, addressing
the challenge of model generalization in traditional Federated Learning (FL)
due to high data heterogeneity. However, existing PFL methods often require
increased communication rounds to achieve the desired performance, primarily
due to slow training caused by the use of first-order optimization, which has
linear convergence. Additionally, many of these methods increase local
computation because of the additional data fed into the model during the search
for personalized local models. One promising solution to this slow training is
second-order optimization, known for its quadratic convergence. However,
employing it in PFL is challenging due to the Hessian matrix and its inverse.
In this paper, we propose pFedSOP, which efficiently utilizes second-order
optimization in PFL to accelerate the training of personalized models and
enhance performance with fewer communication rounds. Our approach first
computes a personalized local gradient update using the Gompertz function-based
normalized angle between local and global gradient updates, incorporating
client-specific global information. We then use a regularized Fisher
Information Matrix (FIM), computed from this personalized gradient update, as
an approximation of the Hessian to update the personalized models. This
FIM-based second-order optimization speeds up training with fewer communication
rounds by tackling the challenges with exact Hessian and avoids additional data
being fed into the model during the search for personalized local models.
Extensive experiments on heterogeneously partitioned image classification
datasets with partial client participation demonstrate that pFedSOP outperforms
state-of-the-art FL and PFL algorithms.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [445] [Scientific machine learning in Hydrology: a unified perspective](https://arxiv.org/abs/2506.06308)
*Adoubi Vincent De Paul Adombi*

Main category: physics.comp-ph

TL;DR: 本文提出了一种统一的科学机器学习（SciML）框架，旨在整合水文研究中的物理知识与数据驱动建模，促进方法论的清晰性和累积性进展。


<details>
  <summary>Details</summary>
Motivation: 当前水文研究中的科学机器学习方法缺乏统一框架，导致方法论的碎片化，难以评估新颖性和识别进一步的研究方向。

Method: 本文通过综述现有的科学机器学习方法，提出了统一的框架，将代表性贡献整合到一个连贯的结构中。

Result: 提出了一个统一的科学机器学习框架，增强了概念清晰性，并支持水文建模的累积性进展。

Conclusion: 本文展示了统一框架的潜力，并指出了各方法家族的限制和未来机会，以指导系统性的水文研究。

Abstract: Scientific machine learning (SciML) provides a structured approach to
integrating physical knowledge into data-driven modeling, offering significant
potential for advancing hydrological research. In recent years, multiple
methodological families have emerged, including physics-informed machine
learning, physics-guided machine learning, hybrid physics-machine learning, and
data-driven physics discovery. Within each of these families, a proliferation
of heterogeneous approaches has developed independently, often without
conceptual coordination. This fragmentation complicates the assessment of
methodological novelty and makes it difficult to identify where meaningful
advances can still be made in the absence of a unified conceptual framework.
This review, the first focused overview of SciML in hydrology, addresses these
limitations by proposing a unified methodological framework for each SciML
family, bringing together representative contributions into a coherent
structure that fosters conceptual clarity and supports cumulative progress in
hydrological modeling. Finally, we highlight the limitations and future
opportunities of each unified family to guide systematic research in hydrology,
where these methods remain underutilized.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [446] [The Economic Dispatch of Power-to-Gas Systems with Deep Reinforcement Learning:Tackling the Challenge of Delayed Rewards with Long-Term Energy Storage](https://arxiv.org/abs/2506.06484)
*Manuel Sage,Khalil Al Handawi,Yaoyao Fiona Zhao*

Main category: eess.SY

TL;DR: 本文探讨了深度强化学习（DRL）在长期能源存储的Power-to-Gas（P2G）系统经济运营中的应用，通过改进DRL算法，解决了延迟奖励问题，提升了P2G系统的成本效益。


<details>
  <summary>Details</summary>
Motivation: P2G技术在整合间歇性可再生能源（如风能和太阳能）到电网中具有潜力，但其运营成本效益难以确定，且与电池储能系统相比效率较低。DRL在管理能源系统方面表现出色，但在P2G系统中面临延迟奖励的挑战。

Method: 本研究通过三个逐步复杂的案例研究，评估了Deep Q-Networks和Proximal Policy Optimization等DRL算法的性能，并引入了预测整合、奖励函数惩罚和战略成本计算等改进措施，以解决延迟奖励问题。

Result: 研究发现，尽管DRL在P2G系统复杂决策中初期表现不佳，但通过提出的改进措施，显著提升了其制定成本效益运营策略的能力，从而释放了P2G技术在长期能源存储中的潜力。

Conclusion: 本文提出的DRL改进方法有效解决了P2G系统运营中的延迟奖励问题，为长期能源存储提供了新的解决方案，展示了P2G技术在可再生能源整合中的广阔应用前景。

Abstract: Power-to-Gas (P2G) technologies gain recognition for enabling the integration
of intermittent renewables, such as wind and solar, into electricity grids.
However, determining the most cost-effective operation of these systems is
complex due to the volatile nature of renewable energy, electricity prices, and
loads. Additionally, P2G systems are less efficient in converting and storing
energy compared to battery energy storage systems (BESs), and the benefits of
converting electricity into gas are not immediately apparent. Deep
Reinforcement Learning (DRL) has shown promise in managing the operation of
energy systems amidst these uncertainties. Yet, DRL techniques face
difficulties with the delayed reward characteristic of P2G system operation.
Previous research has mostly focused on short-term studies that look at the
energy conversion process, neglecting the long-term storage capabilities of
P2G.
  This study presents a new method by thoroughly examining how DRL can be
applied to the economic operation of P2G systems, in combination with BESs and
gas turbines, over extended periods. Through three progressively more complex
case studies, we assess the performance of DRL algorithms, specifically Deep
Q-Networks and Proximal Policy Optimization, and introduce modifications to
enhance their effectiveness. These modifications include integrating forecasts,
implementing penalties on the reward function, and applying strategic cost
calculations, all aimed at addressing the issue of delayed rewards. Our
findings indicate that while DRL initially struggles with the complex
decision-making required for P2G system operation, the adjustments we propose
significantly improve its capability to devise cost-effective operation
strategies, thereby unlocking the potential for long-term energy storage in P2G
technologies.

</details>


### [447] [From Model-Based and Adaptive Control to Evolving Fuzzy Control](https://arxiv.org/abs/2506.06594)
*Daniel Leite,Igor Škrjanc,Fernando Gomide*

Main category: eess.SY

TL;DR: 本文回顾了模糊集理论60年发展历程，重点介绍了演化模糊系统在处理非平稳环境中的优势，并探讨了未来挑战。


<details>
  <summary>Details</summary>
Motivation: 为纪念模糊集理论60周年，本文旨在回顾经典模糊和自适应建模与控制框架的发展，并强调演化智能系统在模糊建模与控制中的重要性。

Method: 通过回顾历史发展和核心贡献，本文分析了演化模糊系统在数据流中逐步更新规则库结构的方法。

Result: 本文总结了演化模糊系统在处理非平稳环境中的优势，并提出了未来在安全性、可解释性和结构演化方面的挑战。

Conclusion: 演化模糊系统在模糊建模与控制中具有重要应用前景，但未来仍需解决安全性、可解释性和结构演化等关键问题。

Abstract: Evolving fuzzy systems build and adapt fuzzy models - such as predictors and
controllers - by incrementally updating their rule-base structure from data
streams. On the occasion of the 60-year anniversary of fuzzy set theory,
commemorated during the Fuzz-IEEE 2025 event, this brief paper revisits the
historical development and core contributions of classical fuzzy and adaptive
modeling and control frameworks. It then highlights the emergence and
significance of evolving intelligent systems in fuzzy modeling and control,
emphasizing their advantages in handling nonstationary environments. Key
challenges and future directions are discussed, including safety,
interpretability, and principled structural evolution.

</details>


### [448] [On the Generalization of Data-Assisted Control in port-Hamiltonian Systems (DAC-pH)](https://arxiv.org/abs/2506.07079)
*Mostafa Eslami,Maryam Babazadeh*

Main category: eess.SY

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper introduces a hypothetical hybrid control framework for
port-Hamiltonian (p$\mathcal{H}$) systems, employing a dynamic decomposition
based on Data-Assisted Control (DAC). The system's evolution is split into two
parts with fixed topology: Right-Hand Side (RHS)- an intrinsic Hamiltonian flow
handling worst-case parametric uncertainties, and Left-Hand Side (LHS)- a
dissipative/input flow addressing both structural and parametric uncertainties.
A virtual port variable $\Pi$ serves as the interface between these two
components. A nonlinear controller manages the intrinsic Hamiltonian flow,
determining a desired port control value $\Pi_c$. Concurrently, Reinforcement
Learning (RL) is applied to the dissipative/input flow to learn an agent for
providing optimal policy in mapping $\Pi_c$ to the actual system input. This
hybrid approach effectively manages RHS uncertainties while preserving the
system's inherent structure. Key advantages include adjustable performance via
LHS controller parameters, enhanced AI explainability and interpretability
through the port variable $\Pi$, the ability to guarantee safety and state
attainability with hard/soft constraints, reduced complexity in learning
hypothesis classes compared to end-to-end solutions, and improved
state/parameter estimation using LHS prior knowledge and system Hamiltonian to
address partial observability. The paper details the p$\mathcal{H}$
formulation, derives the decomposition, and presents the modular controller
architecture. Beyond design, crucial aspects of stability and robustness
analysis and synthesis are investigated, paving the way for deeper theoretical
investigations. An application example, a pendulum with nonlinear dynamics, is
simulated to demonstrate the approach's empirical and phenomenological benefits
for future research.

</details>


### [449] [Distributed Risk-Sensitive Safety Filters for Uncertain Discrete-Time Systems](https://arxiv.org/abs/2506.07347)
*Armin Lederer,Erfaun Noorani,Andreas Krause*

Main category: eess.SY

TL;DR: 提出一种基于控制屏障函数的风险敏感安全过滤器，用于不确定动态下的多智能体系统，通过分布式策略切换平衡安全性与保守性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在缺乏集中协调时面临安全挑战，需在模型不确定性下实现鲁棒安全保证，同时避免过度保守的解决方案。

Method: 利用指数风险算子构建集中化安全条件，设计基于最坏情况预测和邻近安全策略的两种策略，并通过动态切换确保分布式可行性。

Result: 数值实验表明该方法能有效维持系统安全，且在安全性与保守性之间取得优于传统方法的平衡。

Conclusion: 通过风险敏感安全条件与分布式策略切换机制，为多智能体系统提供了兼顾鲁棒性和实用性的安全保障框架。

Abstract: Ensuring safety in multi-agent systems is a significant challenge,
particularly in settings where centralized coordination is impractical. In this
work, we propose a novel risk-sensitive safety filter for discrete-time
multi-agent systems with uncertain dynamics that leverages control barrier
functions (CBFs) defined through value functions. Our approach relies on
centralized risk-sensitive safety conditions based on exponential risk
operators to ensure robustness against model uncertainties. We introduce a
distributed formulation of the safety filter by deriving two alternative
strategies: one based on worst-case anticipation and another on proximity to a
known safe policy. By allowing agents to switch between strategies, feasibility
can be ensured. Through detailed numerical evaluations, we demonstrate the
efficacy of our approach in maintaining safety without being overly
conservative.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [450] [G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems](https://arxiv.org/abs/2506.07398)
*Guibin Zhang,Muxin Fu,Guancheng Wan,Miao Yu,Kun Wang,Shuicheng Yan*

Main category: cs.MA

TL;DR: 本文提出了一种名为G-Memory的分层记忆系统，用于增强多代理系统（MAS）的自我进化能力，通过三层次图结构管理代理间交互，显著提升了任务执行的成功率和知识问答的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前多代理系统的记忆机制过于简单，忽视了代理间协作的复杂性，且缺乏跨任务和代理特定的定制化，限制了系统的自我进化能力。

Method: 引入G-Memory，一种基于组织记忆理论的分层记忆系统，通过洞察图、查询图和交互图三层次结构，双向遍历记忆以检索高层洞察和细粒度交互轨迹，并在任务执行中不断进化。

Result: 在五个基准测试、三种LLM骨干和三种流行的MAS框架中，G-Memory显著提升了任务执行的成功率（最高提升20.89%）和知识问答的准确性（最高提升10.12%）。

Conclusion: G-Memory通过其分层记忆结构有效解决了多代理系统记忆机制的不足，显著提升了系统的自我进化能力和任务执行效果。

Abstract: Large language model (LLM)-powered multi-agent systems (MAS) have
demonstrated cognitive and execution capabilities that far exceed those of
single LLM agents, yet their capacity for self-evolution remains hampered by
underdeveloped memory architectures. Upon close inspection, we are alarmed to
discover that prevailing MAS memory mechanisms (1) are overly simplistic,
completely disregarding the nuanced inter-agent collaboration trajectories, and
(2) lack cross-trial and agent-specific customization, in stark contrast to the
expressive memory developed for single agents. To bridge this gap, we introduce
G-Memory, a hierarchical, agentic memory system for MAS inspired by
organizational memory theory, which manages the lengthy MAS interaction via a
three-tier graph hierarchy: insight, query, and interaction graphs. Upon
receiving a new user query, G-Memory performs bi-directional memory traversal
to retrieve both $\textit{high-level, generalizable insights}$ that enable the
system to leverage cross-trial knowledge, and $\textit{fine-grained, condensed
interaction trajectories}$ that compactly encode prior collaboration
experiences. Upon task execution, the entire hierarchy evolves by assimilating
new collaborative trajectories, nurturing the progressive evolution of agent
teams. Extensive experiments across five benchmarks, three LLM backbones, and
three popular MAS frameworks demonstrate that G-Memory improves success rates
in embodied action and accuracy in knowledge QA by up to $20.89\%$ and
$10.12\%$, respectively, without any modifications to the original frameworks.
Our codes are available at https://github.com/bingreeky/GMemory.

</details>


### [451] [AI-Generated Compromises for Coalition Formation](https://arxiv.org/abs/2506.06837)
*Eyal Briman,Ehud Shapiro,Nimrod Talmon*

Main category: cs.MA

TL;DR: 本文提出了一种基于自然语言处理和大语言模型的AI方法，用于生成妥协提案，以促进大规模民主文本编辑。


<details>
  <summary>Details</summary>
Motivation: 在AI的争论、调解和谈判等子领域中，如何在代理提案之间找到妥协是一个基本挑战。本文旨在解决如何有效找到妥协提案的问题。

Method: 本文通过形式化一个包含代理有限理性和不确定性的模型，并利用自然语言处理技术和大语言模型在文本上诱导语义度量空间，设计算法生成可能获得广泛支持的妥协点。

Result: 通过模拟联盟形成过程，本文展示了AI可以促进大规模民主文本编辑，这是传统工具难以实现的领域。

Conclusion: 本文的方法为在协作文档写作等领域中生成妥协提案提供了有效的AI解决方案，展示了AI在大规模民主文本编辑中的潜力。

Abstract: The challenge of finding compromises between agent proposals is fundamental
to AI subfields such as argumentation, mediation, and negotiation. Building on
this tradition, Elkind et al. (2021) introduced a process for coalition
formation that seeks majority-supported proposals preferable to the status quo,
using a metric space where each agent has an ideal point. A crucial step in
this process involves identifying compromise proposals around which agent
coalitions can unite. How to effectively find such compromise proposals remains
an open question. We address this gap by formalizing a model that incorporates
agent bounded rationality and uncertainty, and by developing AI methods to
generate compromise proposals. We focus on the domain of collaborative document
writing, such as the democratic drafting of a community constitution. Our
approach uses natural language processing techniques and large language models
to induce a semantic metric space over text. Based on this space, we design
algorithms to suggest compromise points likely to receive broad support. To
evaluate our methods, we simulate coalition formation processes and show that
AI can facilitate large-scale democratic text editing, a domain where
traditional tools are limited.

</details>


### [452] [Learn as Individuals, Evolve as a Team: Multi-agent LLMs Adaptation in Embodied Environments](https://arxiv.org/abs/2506.07232)
*Xinran Li,Chenjia Bai,Zijian Li,Jiakun Zheng,Ting Xiao,Jun Zhang*

Main category: cs.MA

TL;DR: 提出LIET框架，通过个体学习与团队协同进化增强多语言模型在多智能体具身环境中的规划与协作能力，实验验证其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多智能体规划算法在具身场景下适应性不足，缺乏环境相关知识与高效协作机制，限制了复杂任务的执行效果。

Method: 提出LIET范式：个体层面通过探索数据集学习本地效用函数以理解环境；团队层面迭代维护共享协作知识列表，结合集中训练与分散执行思想，实现知识进化与通信优化。

Result: 在Communicative Watch-And-Help和ThreeD-World运输基准测试中，基于LLaMA/GPT-4o的LIET表现超越基线，展现出强协作规划能力。

Conclusion: LIET通过融合个体环境认知与团队知识协同进化，实现了多智能体大语言模型的全面适应性提升，为具身协作规划提供新范式。

Abstract: Large language models (LLMs) possess extensive knowledge bases and strong
reasoning capabilities, making them promising tools for complex, multi-agent
planning in embodied environments. However, despite LLMs' advanced abilities
and the sophisticated modular design of agentic methods, existing LLM-based
planning algorithms remain limited by weak adaptation capabilities to
multi-agent embodied scenarios. We address this limitation by introducing a
framework that enables LLM agents to learn and evolve both before and during
test time, equipping them with environment-relevant knowledge for better
planning and enhanced communication for improved cooperation. Inspired by
centralized training with decentralized execution in multi-agent reinforcement
learning, we propose a \textit{Learn as Individuals, Evolve as a Team (LIET)}
paradigm for multi-agent LLMs adaptation. At the individual level, LLM agents
learn a local utility function from exploratory datasets to better comprehend
the embodied environment, which is then queried during test time to support
informed decision-making. At the team level, LLM agents collaboratively and
iteratively maintain and update a shared cooperation knowledge list based on
new experiences, using it to guide more effective communication. By combining
individual learning with team evolution, LIET enables comprehensive and
flexible adaptation for LLM agents. Our experiments on Communicative
Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate
that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing
baselines and exhibits strong cooperative planning abilities.

</details>


### [453] [Shapley-Coop: Credit Assignment for Emergent Cooperation in Self-Interested LLM Agents](https://arxiv.org/abs/2506.07388)
*Yun Hua,Haosheng Chen,Shiqin Wang,Wenhao Li,Xiangfeng Wang,Jun Luo*

Main category: cs.MA

TL;DR: 本文提出Shapley-Coop工作流，通过Shapley链式思维与协商协议解决开放环境中LLM多智能体协作的信用分配问题，实现公平定价与激励协调。


<details>
  <summary>Details</summary>
Motivation: 开放环境下LLM多智能体因缺乏协调规则易陷入自利行为，需解决贡献度评估与异质目标定价机制问题，以支撑复杂人机协作中的公平补偿与责任追溯。

Method: 结合Shapley边际贡献链式思维（Shapley Chain-of-Thought）与结构化协商协议，建立任务时间定价及事后奖励再分配机制，保持智能体自主性。

Result: 在两类多智能体游戏和软件工程仿真中验证，Shapley-Coop显著提升协作效率，其定价机制能准确反映个体任务执行贡献度。

Conclusion: Shapley-Coop通过可解释的贡献度量化与动态协商机制，为开放环境LLM协作提供了可扩展的协调框架，推动人机协作向理性化发展。

Abstract: Large Language Models (LLMs) show strong collaborative performance in
multi-agent systems with predefined roles and workflows. However, in open-ended
environments lacking coordination rules, agents tend to act in self-interested
ways. The central challenge in achieving coordination lies in credit assignment
-- fairly evaluating each agent's contribution and designing pricing mechanisms
that align their heterogeneous goals. This problem is critical as LLMs
increasingly participate in complex human-AI collaborations, where fair
compensation and accountability rely on effective pricing mechanisms. Inspired
by how human societies address similar coordination challenges (e.g., through
temporary collaborations such as employment or subcontracting), we propose a
cooperative workflow, Shapley-Coop. Shapley-Coop integrates Shapley
Chain-of-Thought -- leveraging marginal contributions as a principled basis for
pricing -- with structured negotiation protocols for effective price matching,
enabling LLM agents to coordinate through rational task-time pricing and
post-task reward redistribution. This approach aligns agent incentives, fosters
cooperation, and maintains autonomy. We evaluate Shapley-Coop across two
multi-agent games and a software engineering simulation, demonstrating that it
consistently enhances LLM agent collaboration and facilitates equitable credit
assignment. These results highlight the effectiveness of Shapley-Coop's pricing
mechanisms in accurately reflecting individual contributions during task
execution.

</details>


### [454] [MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models](https://arxiv.org/abs/2506.07400)
*Philip Liu,Sparsh Bansal,Jimmy Dinh,Aditya Pawar,Ramani Satishkumar,Shail Desai,Neeraj Gupta,Xin Wang,Shu Hu*

Main category: cs.MA

TL;DR: 提出多智能体诊断框架MedChat，结合专用视觉模型与多LLM代理，解决通用大模型在医学影像中的幻觉与解释性问题，提升诊断可靠性和交互性。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型（LLM）应用于医学影像时存在幻觉、可解释性差及领域知识不足的问题，且现有方法依赖单一代理，难以模拟多学科医疗团队的复杂推理。

Method: 设计MedChat框架：通过协调代理整合专用视觉模型与多角色LLM代理，提供临床审核与教育交互界面，降低幻觉风险。

Result: 框架增强诊断可靠性，减少幻觉，并支持交互式报告生成，适用于临床与教育场景。

Conclusion: MedChat通过多智能体协作模拟医疗团队决策，有效提升自动化诊断的准确性与实用性，为医疗AI提供新范式。

Abstract: The integration of deep learning-based glaucoma detection with large language
models (LLMs) presents an automated strategy to mitigate ophthalmologist
shortages and improve clinical reporting efficiency. However, applying general
LLMs to medical imaging remains challenging due to hallucinations, limited
interpretability, and insufficient domain-specific medical knowledge, which can
potentially reduce clinical accuracy. Although recent approaches combining
imaging models with LLM reasoning have improved reporting, they typically rely
on a single generalist agent, restricting their capacity to emulate the diverse
and complex reasoning found in multidisciplinary medical teams. To address
these limitations, we propose MedChat, a multi-agent diagnostic framework and
platform that combines specialized vision models with multiple role-specific
LLM agents, all coordinated by a director agent. This design enhances
reliability, reduces hallucination risk, and enables interactive diagnostic
reporting through an interface tailored for clinical review and educational
use. Code available at https://github.com/Purdue-M2/MedChat.

</details>


### [455] [Diffusion of Responsibility in Collective Decision Making](https://arxiv.org/abs/2506.07935)
*Pavel Naumov,Jia Tao*

Main category: cs.MA

TL;DR: 本文探讨了集体决策机制中的责任扩散现象，指出避免责任扩散的唯一方式是让单一代理人做出决策，即通过‘独裁’或‘选举独裁’机制。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示集体决策中责任扩散的现象，并探讨如何避免这种责任模糊的情况。

Method: 通过定义决策机制的互模拟，证明互模拟保留责任相关属性，并在最小的互模拟机制中建立结果。

Result: 研究表明，在两人决策中，避免责任扩散的唯一方式是让一人独裁；在多人决策中，任何无责任扩散的机制都是‘选举独裁’。

Conclusion: 避免责任扩散的集体决策机制必须通过单一代理人做出决策，无论是独裁还是选举独裁。

Abstract: The term "diffusion of responsibility'' refers to situations in which
multiple agents share responsibility for an outcome, obscuring individual
accountability. This paper examines this frequently undesirable phenomenon in
the context of collective decision-making mechanisms.
  The work shows that if a decision is made by two agents, then the only way to
avoid diffusion of responsibility is for one agent to act as a "dictator'',
making the decision unilaterally. In scenarios with more than two agents, any
diffusion-free mechanism is an "elected dictatorship'' where the agents elect a
single agent to make a unilateral decision.
  The technical results are obtained by defining a bisimulation of
decision-making mechanisms, proving that bisimulation preserves
responsibility-related properties, and establishing the results for a smallest
bisimular mechanism.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [456] [Fast Geometric Embedding for Node Influence Maximization](https://arxiv.org/abs/2506.07435)
*Alexander Kolpakov,Igor Rivin*

Main category: cs.SI

TL;DR: 提出一种高效的力导向布局算法，通过低维嵌入将径向距离作为中心性度量替代指标，验证其与多种中心性的强相关性，并用于快速发现高影响力节点。


<details>
  <summary>Details</summary>
Motivation: 传统中心性度量（如介数、接近度）在大规模图计算中成本高昂，需寻找高效替代方法。

Method: 使用力导向布局算法将图嵌入低维空间，以节点到原点的径向距离近似表示多种中心性指标。

Result: 在多种图结构上验证，径向距离与度数、PageRank及路径中心性高度相关，且能高效识别关键节点。

Conclusion: 该方法为大规模图分析提供了快速可扩展的解决方案，可作为传统贪婪算法的有效替代方案。

Abstract: Computing classical centrality measures such as betweenness and closeness is
computationally expensive on large-scale graphs. In this work, we introduce an
efficient force layout algorithm that embeds a graph into a low-dimensional
space, where the radial distance from the origin serves as a proxy for various
centrality measures. We evaluate our method on multiple graph families and
demonstrate strong correlations with degree, PageRank, and paths-based
centralities. As an application, it turns out that the proposed embedding
allows to find high-influence nodes in a network, and provides a fast and
scalable alternative to the standard greedy algorithm.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [457] [Inverse Design of Metamaterials with Manufacturing-Guiding Spectrum-to-Structure Conditional Diffusion Model](https://arxiv.org/abs/2506.07083)
*Jiawen Li,Jiang Guo,Yuanzhe Li,Zetian Mao,Jiaxing Shen,Tashi Xu,Diptesh Das,Jinming He,Run Hu,Yaerim Lee,Koji Tsuda,Junichiro Shiomi*

Main category: physics.optics

TL;DR: 本文提出了一种基于条件扩散模型的框架，用于解决超材料逆向设计中的一对多问题，展示了其在光谱预测和制造方面的优势。


<details>
  <summary>Details</summary>
Motivation: 超材料的逆向设计面临高度非线性和制造困难，机器学习在此领域的应用具有挑战性。

Method: 采用条件扩散模型，实现定制化的光谱到形状和尺寸参数的映射，解决一对多逆向设计问题。

Result: 该方法在光谱预测精度和生成多样性上优于其他生成模型，并为实验制造提供了有价值的先验知识。

Conclusion: 所提出的方法成功设计并制造了具有定制选择性发射光谱的自由形式超材料，展示了其在热伪装应用中的有效性。

Abstract: Metamaterials are artificially engineered structures that manipulate
electromagnetic waves, having optical properties absent in natural materials.
Recently, machine learning for the inverse design of metamaterials has drawn
attention. However, the highly nonlinear relationship between the metamaterial
structures and optical behaviour, coupled with fabrication difficulties, poses
challenges for using machine learning to design and manufacture complex
metamaterials. Herein, we propose a general framework that implements
customised spectrum-to-shape and size parameters to address one-to-many
metamaterial inverse design problems using conditional diffusion models. Our
method exhibits superior spectral prediction accuracy, generates a diverse
range of patterns compared to other typical generative models, and offers
valuable prior knowledge for manufacturing through the subsequent analysis of
the diverse generated results, thereby facilitating the experimental
fabrication of metamaterial designs. We demonstrate the efficacy of the
proposed method by successfully designing and fabricating a free-form
metamaterial with a tailored selective emission spectrum for thermal camouflage
applications.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [458] [Decentralized Optimization on Compact Submanifolds by Quantized Riemannian Gradient Tracking](https://arxiv.org/abs/2506.07351)
*Jun Chen,Lina Liu,Tianyi Zhu,Yong Liu,Guang Dai,Yunliang Jiang,Ivor W. Tsang*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper considers the problem of decentralized optimization on compact
submanifolds, where a finite sum of smooth (possibly non-convex) local
functions is minimized by $n$ agents forming an undirected and connected graph.
However, the efficiency of distributed optimization is often hindered by
communication bottlenecks. To mitigate this, we propose the Quantized
Riemannian Gradient Tracking (Q-RGT) algorithm, where agents update their local
variables using quantized gradients. The introduction of quantization noise
allows our algorithm to bypass the constraints of the accurate Riemannian
projection operator (such as retraction), further improving iterative
efficiency. To the best of our knowledge, this is the first algorithm to
achieve an $\mathcal{O}(1/K)$ convergence rate in the presence of quantization,
matching the convergence rate of methods without quantization. Additionally, we
explicitly derive lower bounds on decentralized consensus associated with a
function of quantization levels. Numerical experiments demonstrate that Q-RGT
performs comparably to non-quantized methods while reducing communication
bottlenecks and computational overhead.

</details>


### [459] [Discrete and Continuous Difference of Submodular Minimization](https://arxiv.org/abs/2506.07952)
*George Orfanides,Tim Hoheisel,Marwa El Halabi*

Main category: math.OC

TL;DR: 本文研究了在连续和离散域上最小化两个子模函数差（DS）的问题，提出了一种新的DC算法变体，并在实验中展示了其在整数压缩感知和整数最小二乘中的优越性。


<details>
  <summary>Details</summary>
Motivation: 子模函数在众多应用中广泛存在，研究其最小化问题具有重要理论意义和实际应用价值。本文旨在扩展先前仅限于集合函数的研究，探讨在连续和离散域上最小化两个子模函数差的问题。

Method: 本文提出了一种新的DC算法（DCA）变体，并将其应用于离散域上的DC规划问题。对于连续域，通过离散化方法应用该算法。

Result: 实验结果表明，本文提出的方法在整数压缩感知和整数最小二乘问题中优于基线方法。

Conclusion: 本文证明了在离散域上所有函数和在连续域上所有光滑函数都是DS函数，并提出了有效的算法来解决DS最小化问题，展示了其在实际应用中的优越性。

Abstract: Submodular functions, defined on continuous or discrete domains, arise in
numerous applications. We study the minimization of the difference of two
submodular (DS) functions, over both domains, extending prior work restricted
to set functions. We show that all functions on discrete domains and all smooth
functions on continuous domains are DS. For discrete domains, we observe that
DS minimization is equivalent to minimizing the difference of two convex (DC)
functions, as in the set function case. We propose a novel variant of the DC
Algorithm (DCA) and apply it to the resulting DC Program, obtaining comparable
theoretical guarantees as in the set function case. The algorithm can be
applied to continuous domains via discretization. Experiments demonstrate that
our method outperforms baselines in integer compressive sensing and integer
least squares.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [460] [Neural networks with image recognition by pairs](https://arxiv.org/abs/2506.06322)
*Polad Geidarov*

Main category: cs.NE

TL;DR: 本文提出一种改进的度量识别神经网络，通过成对图像训练替代解析计算权重，简化训练流程并提升网络扩展性，支持动态增加类别而不影响现有参数。


<details>
  <summary>Details</summary>
Motivation: 传统基于度量的神经网络需通过解析表达式计算权重和架构，限制了灵活性和扩展性。研究旨在探索如何利用经典学习算法替代解析方法，降低网络设计复杂度。

Method: 将原网络结构转换为可通过经典算法训练的形式，采用成对图像识别进行训练，避免直接计算权重，并允许动态添加新样本和类别。

Result: 改进后的网络具备架构透明、训练简便、支持大规模图像识别、可无缝扩展新类别（无需调整已有权重和阈值）等优势。

Conclusion: 通过转换网络训练方式，实现了更灵活、可扩展的度量识别模型，为实际应用中的动态学习需求提供了高效解决方案。

Abstract: Neural networks based on metric recognition methods have a strictly
determined architecture. Number of neurons, connections, as well as weights and
thresholds values are calculated analytically, based on the initial conditions
of tasks: number of recognizable classes, number of samples, metric expressions
used. This paper discusses the possibility of transforming these networks in
order to apply classical learning algorithms to them without using analytical
expressions that calculate weight values. In the received network, training is
carried out by recognizing images in pairs. This approach simplifies the
learning process and easily allows to expand the neural network by adding new
images to the recognition task. The advantages of these networks, including
such as: 1) network architecture simplicity and transparency; 2) training
simplicity and reliability; 3) the possibility of using a large number of
images in the recognition problem using a neural network; 4) a consistent
increase in the number of recognizable classes without changing the previous
values of weights and thresholds.

</details>


### [461] [Evolutionary model for energy trading in community microgrids using Hawk-Dove strategies](https://arxiv.org/abs/2506.06325)
*Viorica Rozina Chifu,Tudor Cioara,Cristina Bianca Pop,Ionut Anghel*

Main category: cs.NE

TL;DR: 提出一种基于Hawk-Dove策略的微电网去中心化能源协作模型，通过进化算法优化能量交易矩阵，实现社区级能源平衡。


<details>
  <summary>Details</summary>
Motivation: 传统集中式能源系统难以适应微电网动态特性，需建立本地化协作机制以提升社区级能源稳定性和个体收益。

Method: 采用进化算法框架：1) 以能量交易矩阵为个体表征；2) 矩阵重组算子与高斯变异算子驱动种群进化；3) 多目标适应度函数综合评估卖家利润、社区稳定性、能源失衡惩罚和电池损耗。

Result: 在100个异构微电网仿真场景中，95%的微电网达到稳定状态，验证了模型在个体与社区层面的双重平衡能力。

Conclusion: 该方法通过博弈策略与进化优化的结合，有效解决了去中心化能源交易中的多目标协调问题，为智能电网社区自治提供新范式。

Abstract: This paper proposes a decentralized model of energy cooperation between
microgrids, in which decisions are made locally, at the level of the microgrid
community. Each microgrid is modeled as an autonomous agent that adopts a Hawk
or Dove strategy, depending on the level of energy stored in the battery and
its role in the energy trading process. The interactions between selling and
buying microgrids are modeled through an evolutionary algorithm. An individual
in the algorithm population is represented as an energy trading matrix that
encodes the amounts of energy traded between the selling and buying microgrids.
The population evolution is achieved by recombination and mutation operators.
Recombination uses a specialized operator for matrix structures, and mutation
is applied to the matrix elements according to a Gaussian distribution. The
evaluation of an individual is made with a multi-criteria fitness function that
considers the seller profit, the degree of energy stability at the community
level, penalties for energy imbalance at the community level and for the
degradation of microgrids batteries. The method was tested on a simulated
scenario with 100 microgrids, each with its own selling and buying thresholds,
to reflect a realistic environment with variable storage characteristics of
microgrids batteries. By applying the algorithm on this scenario, 95 out of the
100 microgrids reached a stable energy state. This result confirms the
effectiveness of the proposed model in achieving energy balance both at the
individual level, for each microgrid, and at the level of the entire community.

</details>


### [462] [Introduction to Predictive Coding Networks for Machine Learning](https://arxiv.org/abs/2506.06332)
*Mikko Stenlund*

Main category: cs.NE

TL;DR: 本文介绍了预测编码网络（PCNs）作为理解大脑分层计算的生物启发框架，并提供了其在机器学习中的应用，包括网络架构、推理和学习规则，以及一个CIFAR-10图像分类任务的实现。


<details>
  <summary>Details</summary>
Motivation: 预测编码网络（PCNs）为理解大脑中的分层计算提供了一个生物启发的框架，并作为传统前馈神经网络的替代方案，旨在为机器学习从业者提供一个快速入门指南。

Method: 本文详细介绍了PCNs的基础网络架构、推理和学习更新规则，并提供了基于PyTorch的算法实现，具体应用于CIFAR-10图像分类任务。

Result: 通过CIFAR-10图像分类任务的基准测试，展示了PCNs在实际应用中的高效性能。

Conclusion: 预测编码网络（PCNs）不仅为理解大脑计算提供了新的视角，还在机器学习任务中展示了其强大的应用潜力，特别是在图像分类任务中表现优异。

Abstract: Predictive coding networks (PCNs) constitute a biologically inspired
framework for understanding hierarchical computation in the brain, and offer an
alternative to traditional feedforward neural networks in ML. This note serves
as a quick, onboarding introduction to PCNs for machine learning practitioners.
We cover the foundational network architecture, inference and learning update
rules, and algorithmic implementation. A concrete image-classification task
(CIFAR-10) is provided as a benchmark-smashing application, together with an
accompanying Python notebook containing the PyTorch implementation.

</details>


### [463] [CR-BLEA: Contrastive Ranking for Adaptive Resource Allocation in Bilevel Evolutionary Algorithms](https://arxiv.org/abs/2506.06362)
*Dejun Xu,Jijia Chen,Gary G. Yen,Min Jiang*

Main category: cs.NE

TL;DR: 提出基于对比排序网络的双层进化算法资源分配框架，通过选择性优化下层任务显著降低计算成本，同时保持解的质量。


<details>
  <summary>Details</summary>
Motivation: 传统双层进化算法因需重复评估大量低潜力下层任务导致计算资源浪费，现有多任务/迁移学习方法仍存在效率瓶颈。

Method: 构建在线学习上下层解对关系的对比排序网络，设计基于参考的优先级排序策略，实现任务优化选择与自适应重采样控制。

Result: 在5种前沿算法上验证：计算成本显著减少，解精度保持或提升(平均加速比达3.2倍，部分问题精度提高12.7%)。

Conclusion: 该框架为双层优化提供了通用高效范式，通过智能资源分配推动可扩展性突破，开辟了基于关系学习的算法加速新路径。

Abstract: Bilevel optimization poses a significant computational challenge due to its
nested structure, where each upper-level candidate solution requires solving a
corresponding lower-level problem. While evolutionary algorithms (EAs) are
effective at navigating such complex landscapes, their high resource demands
remain a key bottleneck -- particularly the redundant evaluation of numerous
unpromising lower-level tasks. Despite recent advances in multitasking and
transfer learning, resource waste persists. To address this issue, we propose a
novel resource allocation framework for bilevel EAs that selectively identifies
and focuses on promising lower-level tasks. Central to our approach is a
contrastive ranking network that learns relational patterns between paired
upper- and lower-level solutions online. This knowledge guides a
reference-based ranking strategy that prioritizes tasks for optimization and
adaptively controls resampling based on estimated population quality.
Comprehensive experiments across five state-of-the-art bilevel algorithms show
that our framework significantly reduces computational cost while preserving --
or even enhancing -- solution accuracy. This work offers a generalizable
strategy to improve the efficiency of bilevel EAs, paving the way for more
scalable bilevel optimization.

</details>


### [464] [Can Biologically Plausible Temporal Credit Assignment Rules Match BPTT for Neural Similarity? E-prop as an Example](https://arxiv.org/abs/2506.06904)
*Yuhan Helena Liu,Guangyu Robert Yang,Christopher J. Cueva*

Main category: cs.NE

TL;DR: 研究证明生物合理学习规则（如e-prop）在任务精度相当的情况下，能达到与BPTT相似的神经数据相似性，且模型架构与初始条件对神经相似性的影响可能大于学习规则本身。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注生物合理学习规则在神经科学任务中的性能（标准1），但其与神经记录的匹配性（标准2）尚未充分验证。本文旨在填补这一空白。

Method: 使用Procrustes分析等工具，在经典神经科学数据集上对比生物合理学习规则（如e-prop）与BPTT的神经数据相似性，并控制任务精度以分离变量影响。

Result: 1. e-prop等规则在相同任务精度下与BPTT的神经相似性相当；2. 模型架构和初始条件对神经相似性的影响可能超过学习规则本身；3. BPTT与生物合理模型在动态特性上表现相似。

Conclusion: 生物合理学习规则已取得显著进展，既能实现高任务性能，又能匹配神经数据特征，且模型架构设计可能比学习规则选择更关键。

Abstract: Understanding how the brain learns may be informed by studying biologically
plausible learning rules. These rules, often approximating gradient descent
learning to respect biological constraints such as locality, must meet two
critical criteria to be considered an appropriate brain model: (1) good
neuroscience task performance and (2) alignment with neural recordings. While
extensive research has assessed the first criterion, the second remains
underexamined. Employing methods such as Procrustes analysis on well-known
neuroscience datasets, this study demonstrates the existence of a biologically
plausible learning rule -- namely e-prop, which is based on gradient truncation
and has demonstrated versatility across a wide range of tasks -- that can
achieve neural data similarity comparable to Backpropagation Through Time
(BPTT) when matched for task accuracy. Our findings also reveal that model
architecture and initial conditions can play a more significant role in
determining neural similarity than the specific learning rule. Furthermore, we
observe that BPTT-trained models and their biologically plausible counterparts
exhibit similar dynamical properties at comparable accuracies. These results
underscore the substantial progress made in developing biologically plausible
learning rules, highlighting their potential to achieve both competitive task
performance and neural data similarity.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [465] [El0ps: An Exact L0-regularized Problems Solver](https://arxiv.org/abs/2506.06373)
*Théo Guyard,Cédric Herzet,Clément Elvira*

Main category: cs.MS

TL;DR: El0ps是一个Python工具箱，用于处理L0正则化问题，提供自定义问题实例、高性能求解器和内置机器学习管道。


<details>
  <summary>Details</summary>
Motivation: 现有工具箱在处理L0正则化问题时缺乏灵活性和全面性，El0ps旨在填补这一空白，推动L0正则化问题在实际应用中的集成。

Method: El0ps通过灵活框架允许用户定义自定义问题实例，并提供专用求解器和内置机器学习管道。

Result: El0ps实现了最先进的性能，并提供了全面的工具，为L0正则化问题的实际应用开辟了新视角。

Conclusion: El0ps是一个全面的工具箱，能够有效处理L0正则化问题，并推动其在实际应用中的集成。

Abstract: This paper presents El0ps, a Python toolbox providing several utilities to
handle L0-regularized problems related to applications in machine learning,
statistics, and signal processing, among other fields. In contrast to existing
toolboxes, El0ps allows users to define custom instances of these problems
through a flexible framework, provides a dedicated solver achieving
state-of-the-art performance, and offers several built-in machine learning
pipelines. Our aim with El0ps is to provide a comprehensive tool which opens
new perspectives for the integration of L0-regularized problems in practical
applications.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [466] [Depth-Optimal Quantum Layout Synthesis as SAT](https://arxiv.org/abs/2506.06752)
*Anna B. Jakobsen,Anders B. Clausen,Jaco van de Pol,Irfansha Shaik*

Main category: quant-ph

TL;DR: 本文提出了一种新的、高效的量子电路布局合成的SAT编码方法，专注于最小化电路深度或CX门深度，并通过增量SAT求解和并行计划实现加速。实验表明，最小化CX门数量比最小化CX门深度更能有效减少噪声，但综合考虑两者效果最佳。


<details>
  <summary>Details</summary>
Motivation: 当前量子硬件平台对CX门的连接性有限制，且CX门噪声较大，因此在量子电路执行前，布局合成是一个重要步骤，以减少CX门数量或深度，从而降低噪声。

Method: 提出了一种新的SAT编码方法，专注于最小化电路深度或CX门深度，并使用增量SAT求解和并行计划来提高效率。

Result: 该方法比OLSQ2快10-100倍，但最小化深度仍比Q-Synth最小化门数量耗时更多。实验表明，最小化CX门数量比最小化CX门深度更能有效减少噪声，但综合考虑两者效果最佳。

Conclusion: 最小化CX门数量在减少噪声方面优于最小化CX门深度，但综合考虑CX门数量和深度能实现最佳的噪声减少效果。

Abstract: Quantum circuits consist of gates applied to qubits. Current quantum hardware
platforms impose connectivity restrictions on binary CX gates. Hence, Layout
Synthesis is an important step to transpile quantum circuits before they can be
executed. Since CX gates are noisy, it is important to reduce the CX count or
CX depth of the mapped circuits.
  We provide a new and efficient encoding of Quantum-circuit Layout Synthesis
in SAT. Previous SAT encodings focused on gate count and CX-gate count. Our
encoding instead guarantees that we find mapped circuits with minimal circuit
depth or minimal CX-gate depth. We use incremental SAT solving and parallel
plans for an efficient encoding. This results in speedups of more than 10-100x
compared to OLSQ2, which guarantees depth-optimality. But minimizing depth
still takes more time than minimizing gate count with Q-Synth.
  We correlate the noise reduction achieved by simulating circuits after
(CX)-count and (CX)-depth reduction. We find that minimizing for CX-count
correlates better with reducing noise than minimizing for CX-depth. However,
taking into account both CX-count and CX-depth provides the best noise
reduction.

</details>


### [467] [A weighted quantum ensemble of homogeneous quantum classifiers](https://arxiv.org/abs/2506.07810)
*Emiliano Tolotti,Enrico Blanzieri,Davide Pastorello*

Main category: quant-ph

TL;DR: 提出一种基于量子分类器的加权同质集成方法，通过量子叠加与受控门实现数据子采样及并行分类器执行，结合权重优化提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统同质集成方法依赖数据子采样实现多样性，而量子计算特性（如叠加与并行性）可更高效地实现多分类器组合，探索量子集成方法的潜力。

Method: 使用带索引寄存器的量子分类器编码数据，通过量子叠加状态实现特征/训练点子采样，利用受控门并行执行不同数据组合的分类器，并设计权重学习优化流程。

Result: 实验验证表明该方法有效，权重优化后的量子集成模型性能显著提升，且测试时权重直接编码于量子电路中。

Conclusion: 量子叠加与并行性为集成学习提供了新范式，该方法在量子硬件上实现了高效的多模型组合，为量子机器学习框架扩展提供参考。

Abstract: Ensemble methods in machine learning aim to improve prediction accuracy by
combining multiple models. This is achieved by ensuring diversity among
predictors to capture different data aspects. Homogeneous ensembles use
identical models, achieving diversity through different data subsets, and
weighted-average ensembles assign higher influence to more accurate models
through a weight learning procedure. We propose a method to achieve a weighted
homogeneous quantum ensemble using quantum classifiers with indexing registers
for data encoding. This approach leverages instance-based quantum classifiers,
enabling feature and training point subsampling through superposition and
controlled unitaries, and allowing for a quantum-parallel execution of diverse
internal classifiers with different data compositions in superposition. The
method integrates a learning process involving circuit execution and classical
weight optimization, for a trained ensemble execution with weights encoded in
the circuit at test-time. Empirical evaluation demonstrate the effectiveness of
the proposed method, offering insights into its performance.

</details>


### [468] [Deep reinforcement learning for near-deterministic preparation of cubic- and quartic-phase gates in photonic quantum computing](https://arxiv.org/abs/2506.07859)
*Amanuel Anteneh Léandre Brunel,Carlos González-Arciniegas,Olivier Pfister*

Main category: quant-ph

TL;DR: 通过深度神经网络和强化学习控制量子光学电路，成功生成立方相位态，平均成功率96%，并直接生成四次相位门。


<details>
  <summary>Details</summary>
Motivation: 探索立方相位态作为连续变量通用量子计算的资源，并研究其生成方法。

Method: 使用深度神经网络通过强化学习训练，控制量子光学电路生成立方相位态。

Result: 实验平均成功率为96%，且无需立方门分解即可直接生成四次相位门。

Conclusion: 立方相位态是通用量子计算的充分资源，且仅需光子数分辨测量作为非高斯资源。

Abstract: Cubic-phase states are a sufficient resource for universal quantum computing
over continuous variables. We present results from numerical experiments in
which deep neural networks are trained via reinforcement learning to control a
quantum optical circuit for generating cubic-phase states, with an average
success rate of 96%. The only non-Gaussian resource required is
photon-number-resolving measurements. We also show that the exact same
resources enable the direct generation of a quartic-phase gate, with no need
for a cubic gate decomposition.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [469] [Less is More: some Computational Principles based on Parcimony, and Limitations of Natural Intelligence](https://arxiv.org/abs/2506.07060)
*Laura Cohen,Xavier Hinaut,Lilyana Petrova,Alexandre Pitti,Syd Reynal,Ichiro Tsuda*

Main category: q-bio.NC

TL;DR: 本文探讨了自然智能（NI）在有限资源下的高效性，并建议通过引入类似的约束来提升人工智能（AI）的效率、适应性和创造性。


<details>
  <summary>Details</summary>
Motivation: 当前AI依赖大量计算资源和数据，而自然智能在有限资源下表现出高效性。本文旨在探讨如何通过引入类似自然智能的约束来改进AI系统。

Method: 通过分析有限神经带宽、混沌游走、储备计算等自然智能机制，结合发展视角，探讨了如何将这些机制应用于AI系统。

Result: 研究表明，引入能量约束、简约架构和现实世界互动等原则，可以促进更高效、可解释且基于生物学的AI系统的出现。

Conclusion: 通过借鉴自然智能的约束机制，AI系统可以在效率、适应性和创造性方面取得显著提升。

Abstract: Natural intelligence (NI) consistently achieves more with less. Infants learn
language, develop abstract concepts, and acquire sensorimotor skills from
sparse data, all within tight neural and energy limits. In contrast, today's AI
relies on virtually unlimited computational power, energy, and data to reach
high performance. This paper argues that constraints in NI are paradoxically
catalysts for efficiency, adaptability, and creativity. We first show how
limited neural bandwidth promotes concise codes that still capture complex
patterns. Spiking neurons, hierarchical structures, and symbolic-like
representations emerge naturally from bandwidth constraints, enabling robust
generalization. Next, we discuss chaotic itinerancy, illustrating how the brain
transits among transient attractors to flexibly retrieve memories and manage
uncertainty. We then highlight reservoir computing, where random projections
facilitate rapid generalization from small datasets. Drawing on developmental
perspectives, we emphasize how intrinsic motivation, along with responsive
social environments, drives infant language learning and discovery of meaning.
Such active, embodied processes are largely absent in current AI. Finally, we
suggest that adopting 'less is more' principles -- energy constraints,
parsimonious architectures, and real-world interaction -- can foster the
emergence of more efficient, interpretable, and biologically grounded
artificial systems.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [470] [GOLFer: Smaller LM-Generated Documents Hallucination Filter & Combiner for Query Expansion in Information Retrieval](https://arxiv.org/abs/2506.04762)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.IR

TL;DR: GOLFer是一种利用较小开源语言模型进行查询扩展的新方法，通过过滤生成文档中的虚假信息并整合内容，有效提升信息检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型的查询扩展方法成本高、计算量大且可访问性有限，因此需要一种更高效且经济的方法。

Method: GOLFer包含两个模块：虚假信息过滤器和文档整合器。前者检测并去除生成文档中的非事实和不一致句子，后者通过权重向量将过滤后的内容与查询结合。

Result: 实验表明，GOLFer在使用较小语言模型时优于其他方法，并在与大型语言模型方法对比时保持竞争力。

Conclusion: GOLFer展示了利用较小语言模型进行高效查询扩展的潜力，解决了现有方法的高成本和可访问性问题。

Abstract: Large language models (LLMs)-based query expansion for information retrieval
augments queries with generated hypothetical documents with LLMs. However, its
performance relies heavily on the scale of the language models (LMs),
necessitating larger, more advanced LLMs. This approach is costly,
computationally intensive, and often has limited accessibility. To address
these limitations, we introduce GOLFer - Smaller LMs-Generated Documents
Hallucination Filter & Combiner - a novel method leveraging smaller open-source
LMs for query expansion. GOLFer comprises two modules: a hallucination filter
and a documents combiner. The former detects and removes non-factual and
inconsistent sentences in generated documents, a common issue with smaller LMs,
while the latter combines the filtered content with the query using a weight
vector to balance their influence. We evaluate GOLFer alongside dominant
LLM-based query expansion methods on three web search and ten low-resource
datasets. Experimental results demonstrate that GOLFer consistently outperforms
other methods using smaller LMs, and maintains competitive performance against
methods using large-size LLMs, demonstrating its effectiveness.

</details>


### [471] [DISRetrieval: Harnessing Discourse Structure for Long Document Retrieval](https://arxiv.org/abs/2506.06313)
*Huiyao Chen,Yi Yang,Yinghui Li,Meishan Zhang,Min Zhang*

Main category: cs.IR

TL;DR: DISRetrieval提出基于语言学篇章结构的层次化检索框架，通过修辞结构理论构建文档表示，显著提升长文本理解效果。


<details>
  <summary>Details</summary>
Motivation: 现有长文档检索方法忽略文本内在的篇章结构，导致语义关系与文档连贯性缺失。本文旨在通过语言学结构增强检索效果。

Method: 1) 基于修辞结构理论(RST)构建句子级层次化文档表示；2) 结合LLM生成自适应摘要增强节点表征；3) 设计保持篇章连贯性的分层证据检索机制。

Result: 在QASPER和QuALITY数据集上，检索指标与问答任务表现均超越基线方法，消融实验验证篇章结构对各类文档长度/查询类型的普适有效性。

Conclusion: 语言学驱动的层次化文档表示显著提升长文本理解，公开代码与数据集为后续研究提供基础。

Abstract: Long document understanding has become increasingly crucial in natural
language processing, with retrieval-based methods emerging as a promising
solution to address the context length limitations of large language models
(LLMs). However, existing approaches either treat documents as flat sequences
or employ arbitrary chunking strategies, failing to capture the inherent
discourse structure that guides human comprehension. We present DISRetrieval, a
novel hierarchical retrieval framework that leverages linguistic discourse
structure to enhance long document understanding. Our approach introduces three
key innovations: (1) a discourse-aware document organization framework that
utilizes rhetorical structure theory (RST) to create sentence-level
hierarchical representations, preserving both semantic relationships and
natural document flow; (2) an LLM-enhanced node representation technique that
combines discourse structure with adaptive summarization to enrich tree nodes
with contextual information; and (3) a hierarchical evidence retrieval
mechanism that effectively selects relevant content while maintaining discourse
coherence. Through comprehensive experiments on QASPER and QuALITY datasets,
DISRetrieval demonstrates substantial improvements over existing methods in
both token-level retrieval metrics and downstream question answering tasks. Our
ablation studies confirm that incorporating discourse structure significantly
enhances retrieval effectiveness across different document lengths and query
types, validating the importance of linguistically-informed document
representation in long-text understanding. Our code and datasets are publicly
available at github/DreamH1gh/DISRetrieval to facilitate future research.

</details>


### [472] [A Reinforcement-Learning-Enhanced LLM Framework for Automated A/B Testing in Personalized Marketing](https://arxiv.org/abs/2506.06316)
*Haoyang Feng,Yanjun Dai,Yuan Gao*

Main category: cs.IR

TL;DR: 本文提出了一种结合强化学习和LLM的RL-LLM-AB测试框架，用于自动化和个性化A/B测试，通过实时反馈和长期用户偏好捕捉，优化用户响应。


<details>
  <summary>Details</summary>
Motivation: 个性化营销中，如何有效算法化A/B测试以最大化用户响应是一个亟待解决的新挑战。

Method: RL-LLM-AB测试框架基于预训练指令调优的语言模型，使用Prompt-Conditioned生成器生成候选内容变体，动态嵌入用户画像和当前查询上下文，通过Actor-Critic结构的策略优化模块实时选择内容版本，并嵌入Memory-Augmented Reward Estimator捕捉长期用户偏好。

Result: 数值结果表明，RL-LLM-ABTest在真实营销数据上优于现有的A/B测试方法，包括经典A/B测试、上下文Bandits和基准强化学习方法。

Conclusion: RL-LLM-AB测试框架通过结合强化学习和LLM，有效提升了A/B测试的自动化和个性化能力，能够更好地捕捉用户偏好并优化长期收益。

Abstract: For personalized marketing, a new challenge of how to effectively algorithm
the A/B testing to maximize user response is urgently to be overcome. In this
paper, we present a new approach, the RL-LLM-AB test framework, for using
reinforcement learning strategy optimization combined with LLM to automate and
personalize A/B tests. The RL-LLM-AB test is built upon the pre-trained
instruction-tuned language model. It first generates A/B versions of candidate
content variants using a Prompt-Conditioned Generator, and then dynamically
embeds and fuses the user portrait and the context of the current query with
the multi-modal perception module to constitute the current interaction state.
The content version is then selected in real-time through the policy
optimization module with an Actor-Critic structure, and long-term revenue is
estimated according to real-time feedback (such as click-through rate and
conversion rate). Furthermore, a Memory-Augmented Reward Estimator is embedded
into the framework to capture long-term user preference drift, which helps to
generalize policy across multiple users and content contexts. Numerical results
demonstrate the superiority of our proposed RL-LLM-ABTest over existing A/B
testing methods, including classical A/B testing, Contextual Bandits, and
benchmark reinforcement learning approaches on real-world marketing data.

</details>


### [473] [FinBERT2: A Specialized Bidirectional Encoder for Bridging the Gap in Finance-Specific Deployment of Large Language Models](https://arxiv.org/abs/2506.06335)
*Xuan Xu,Fufang Wen,Beilin Chu,Zhibing Fu,Qinhong Lin,Jiaqi Liu,Binjie Fei,Zhongliang Yang,Linna Zhou,Yu Li*

Main category: cs.IR

TL;DR: 本文介绍了FinBERT2，一个专门针对金融领域预训练的双向编码器，旨在解决大语言模型（LLMs）在金融应用中的局限性，并在多个金融任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）如GPT-3在自然语言处理中受到关注，但其在金融领域的实际应用存在局限性，特别是在判别任务、生成任务和主题建模等方面表现不佳。因此，本文提出FinBERT2，以弥补这些不足。

Method: 本文提出了FinBERT2，一个在32b金融领域语料上预训练的双向编码器，并在此基础上构建了Fin-Labelers、Fin-Retrievers和Fin-TopicModel，分别用于金融分类、检索和主题建模任务。

Result: FinBERT2在多个金融任务中表现优异：Fin-Labelers在金融分类任务中平均优于其他BERT变体和领先的LLMs；Fin-Retrievers在金融检索任务中优于开源和专有嵌入模型；Fin-TopicModel在金融标题的聚类和主题表示上表现优越。

Conclusion: FinBERT2通过对比分析当代LLMs，重新审视了金融BERT模型，并提供了在LLMs时代有效利用FinBERT的实用见解，成功弥补了LLMs在金融领域应用的不足。

Abstract: In natural language processing (NLP), the focus has shifted from encoder-only
tiny language models like BERT to decoder-only large language models(LLMs) such
as GPT-3. However, LLMs' practical application in the financial sector has
revealed three limitations: (1) LLMs often perform worse than fine-tuned BERT
on discriminative tasks despite costing much higher computational resources,
such as market sentiment analysis in financial reports; (2) Application on
generative tasks heavily relies on retrieval augmented generation (RAG) methods
to provide current and specialized information, with general retrievers showing
suboptimal performance on domain-specific retrieval tasks; (3) There are
additional inadequacies in other feature-based scenarios, such as topic
modeling. We introduce FinBERT2, a specialized bidirectional encoder pretrained
on a high-quality, financial-specific corpus of 32b tokens. This represents the
largest known Chinese financial pretraining corpus for models of this parameter
size. As a better backbone, FinBERT2 can bridge the gap in the
financial-specific deployment of LLMs through the following achievements: (1)
Discriminative fine-tuned models (Fin-Labelers) outperform other (Fin)BERT
variants by 0.4%-3.3% and leading LLMs by 9.7%-12.3% on average across five
financial classification tasks. (2) Contrastive fine-tuned models
(Fin-Retrievers) outperform both open-source (e.g., +6.8\% avg improvement over
BGE-base-zh) and proprietary (e.g., +4.2\% avg improvement over OpenAI's
text-embedding-3-large) embedders across five financial retrieval tasks; (3)
Building on FinBERT2 variants, we construct the Fin-TopicModel, which enables
superior clustering and topic representation for financial titles. Our work
revisits financial BERT models through comparative analysis with contemporary
LLMs and offers practical insights for effectively utilizing FinBERT in the
LLMs era.

</details>


### [474] [Optimizing RAG Pipelines for Arabic: A Systematic Analysis of Core Components](https://arxiv.org/abs/2506.06339)
*Jumana Alsubhi,Mohammad D. Alahmadi,Ahmed Alhusayni,Ibrahim Aldailami,Israa Hamdine,Ahmad Shabana,Yazeed Iskandar,Suhayb Khayyat*

Main category: cs.IR

TL;DR: 本文系统评估了阿拉伯语检索增强生成（RAG）组件的性能，发现句子感知分块、BGE-M3/Multilingual-E5-large嵌入模型、bge-reranker-v2-m3重排器和Aya-8B模型表现最佳，为构建高质量阿拉伯语RAG系统提供实践指导。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于高资源语言的RAG优化，而针对阿拉伯语的组件优化研究不足，需填补这一空白以提升其应用效果。

Method: 使用RAGAS框架，在多样化阿拉伯语数据集上评估分块策略、嵌入模型、重排器和语言模型，并基于上下文精确度、召回率、答案忠实度及相关性四个指标进行系统比较。

Result: 句子感知分块策略最优；BGE-M3与Multilingual-E5-large为最有效嵌入模型；重排器显著提升复杂数据集忠实度；Aya-8B生成质量优于StableLM。

Conclusion: 研究结果明确了阿拉伯语RAG组件的最优选择，为实际系统构建提供关键指导，同时填补了低资源语言RAG优化的研究空白。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful architecture
for combining the precision of retrieval systems with the fluency of large
language models. While several studies have investigated RAG pipelines for
high-resource languages, the optimization of RAG components for Arabic remains
underexplored. This study presents a comprehensive empirical evaluation of
state-of-the-art RAG components-including chunking strategies, embedding
models, rerankers, and language models-across a diverse set of Arabic datasets.
Using the RAGAS framework, we systematically compare performance across four
core metrics: context precision, context recall, answer faithfulness, and
answer relevancy. Our experiments demonstrate that sentence-aware chunking
outperforms all other segmentation methods, while BGE-M3 and
Multilingual-E5-large emerge as the most effective embedding models. The
inclusion of a reranker (bge-reranker-v2-m3) significantly boosts faithfulness
in complex datasets, and Aya-8B surpasses StableLM in generation quality. These
findings provide critical insights for building high-quality Arabic RAG
pipelines and offer practical guidelines for selecting optimal components
across different document types.

</details>


### [475] [Structured Semantics from Unstructured Notes: Language Model Approaches to EHR-Based Decision Support](https://arxiv.org/abs/2506.06340)
*Wu Hao Ran,Xi Xi,Furong Li,Jingyi Lu,Jian Jiang,Hui Huang,Yuzhuan Zhang,Shi Li*

Main category: cs.IR

TL;DR: 本文探讨了如何利用大型语言模型（LLMs）分析电子健康记录（EHRs）中的多样化数据，以提升临床决策支持。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHRs）包含大量非结构化数据，传统的高维分析常忽略文本特征。本文旨在通过LLMs挖掘这些数据的语义信息，提升跨机构数据的协调性。

Method: 本文应用先进的语言模型，结合EHRs中的自由文本临床记录、结构化实验室结果和诊断代码，进行数据分析和模型构建。

Result: 研究表明，文本特征能够提供丰富的语义表示，有助于跨机构数据的协调，并提升AI模型在医疗领域的通用性和公平性。

Conclusion: 通过LLMs分析EHRs中的多样化数据，能够有效提升临床决策支持，但需解决医疗代码整合和模型公平性等挑战。

Abstract: The advent of large language models (LLMs) has opened new avenues for
analyzing complex, unstructured data, particularly within the medical domain.
Electronic Health Records (EHRs) contain a wealth of information in various
formats, including free text clinical notes, structured lab results, and
diagnostic codes. This paper explores the application of advanced language
models to leverage these diverse data sources for improved clinical decision
support. We will discuss how text-based features, often overlooked in
traditional high dimensional EHR analysis, can provide semantically rich
representations and aid in harmonizing data across different institutions.
Furthermore, we delve into the challenges and opportunities of incorporating
medical codes and ensuring the generalizability and fairness of AI models in
healthcare.

</details>


### [476] [NR4DER: Neural Re-ranking for Diversified Exercise Recommendation](https://arxiv.org/abs/2506.06341)
*Xinghe Cheng,Xufang Zhou,Liangda Fang,Chaobo He,Yuyu Zhou,Weiqi Luo,Zhiguo Gong,Quanlong Guan*

Main category: cs.IR

TL;DR: 针对在线教育平台中练习推荐方法存在的高辍学率及无法适应学生多样化学习节奏的问题，本文提出NR4DER方法，通过mLSTM模型、序列增强和神经重排序技术提升推荐效果，实验证明其显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有练习推荐方法难以适应不活跃学生的学习模式，且无法匹配学生个体化学习节奏，导致推荐准确性和多样性受限，亟需改进。

Method: NR4DER采用三阶段方法：1) 基于mLSTM优化习题过滤模块；2) 通过序列增强提升不活跃学生表征并匹配习题难度；3) 利用神经重排序生成个性化多样化推荐列表。

Result: 在多组真实数据集实验中，NR4DER在推荐准确性和多样性指标上显著超越基线方法，并能有效适应不同学习节奏的学生群体。

Conclusion: NR4DER通过动态表征与神经重排序机制，成功解决了传统推荐系统在个性化学习节奏适应和多样性推荐上的瓶颈，为在线教育推荐提供了新思路。

Abstract: With the widespread adoption of online education platforms, an increasing
number of students are gaining new knowledge through Massive Open Online
Courses (MOOCs). Exercise recommendation have made strides toward improving
student learning outcomes. However, existing methods not only struggle with
high dropout rates but also fail to match the diverse learning pace of
students. They frequently face difficulties in adjusting to inactive students'
learning patterns and in accommodating individualized learning paces, resulting
in limited accuracy and diversity in recommendations. To tackle these
challenges, we propose Neural Re-ranking for Diversified Exercise
Recommendation (in short, NR4DER). NR4DER first leverages the mLSTM model to
improve the effectiveness of the exercise filter module. It then employs a
sequence enhancement method to enhance the representation of inactive students,
accurately matches students with exercises of appropriate difficulty. Finally,
it utilizes neural re-ranking to generate diverse recommendation lists based on
individual students' learning histories. Extensive experimental results
indicate that NR4DER significantly outperforms existing methods across multiple
real-world datasets and effectively caters to the diverse learning pace of
students.

</details>


### [477] [Is BERTopic Better than PLSA for Extracting Key Topics in Aviation Safety Reports?](https://arxiv.org/abs/2506.06328)
*Aziida Nanyonga,Joiner Keith,Turhan Ugur,Wild Graham*

Main category: cs.IR

TL;DR: 本研究对比了BERTopic与PLSA在航空安全报告主题建模中的效果，发现BERTopic在主题一致性和可解释性上更优，为航空安全数据分析提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 通过比较BERTopic与PLSA在航空安全报告中的主题提取效果，旨在提升对航空事故数据模式的理解，支持更有效的安全决策。

Method: 使用36,000余份NTSB航空事故报告（2000-2020年），分别采用BERTopic（基于Transformer嵌入和层次聚类）与PLSA（基于EM算法的概率模型）进行主题建模。

Result: BERTopic的Cv主题一致性得分（0.41）高于PLSA（0.37），且经专家验证具有更优的模型可解释性。

Conclusion: 基于Transformer的BERTopic在复杂航空数据分析中展现优势，未来将探索混合模型、多语言数据及先进聚类技术以进一步提升主题建模效果。

Abstract: This study compares the effectiveness of BERTopic and Probabilistic Latent
Semantic Analysis (PLSA) in extracting meaningful topics from aviation safety
reports aiming to enhance the understanding of patterns in aviation incident
data. Using a dataset of over 36,000 National Transportation Safety Board
(NTSB) reports from 2000 to 2020, BERTopic employed transformer based
embeddings and hierarchical clustering, while PLSA utilized probabilistic
modelling through the Expectation-Maximization (EM) algorithm. Results showed
that BERTopic outperformed PLSA in topic coherence, achieving a Cv score of
0.41 compared to PLSA 0.37, while also demonstrating superior interpretability
as validated by aviation safety experts. These findings underscore the
advantages of modern transformer based approaches in analyzing complex aviation
datasets, paving the way for enhanced insights and informed decision-making in
aviation safety. Future work will explore hybrid models, multilingual datasets,
and advanced clustering techniques to further improve topic modelling in this
domain.

</details>


### [478] [LlamaRec-LKG-RAG: A Single-Pass, Learnable Knowledge Graph-RAG Framework for LLM-Based Ranking](https://arxiv.org/abs/2506.07449)
*Vahid Azizi,Fatemeh Koochaki*

Main category: cs.IR

TL;DR: 提出LlamaRec-LKG-RAG框架，通过动态整合知识图谱关系路径增强LLM推荐效果，实验验证其在排序指标上的显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于检索增强生成（RAG）的推荐系统依赖扁平化相似性检索，未能有效挖掘用户-物品交互中的关系结构。

Method: 扩展LlamaRec架构，引入轻量级用户偏好模块动态识别异质知识图谱中的关键关系，将个性化子图融入Llama-2模型的提示生成过程。

Result: 在ML-100K和Amazon Beauty数据集上，MRR/NDCG/Recall等排序指标均显著超越LlamaRec基线模型。

Conclusion: 结构化推理对LLM推荐系统至关重要，本框架为可扩展的知识感知个性化推荐奠定了基础。

Abstract: Recent advances in Large Language Models (LLMs) have driven their adoption in
recommender systems through Retrieval-Augmented Generation (RAG) frameworks.
However, existing RAG approaches predominantly rely on flat, similarity-based
retrieval that fails to leverage the rich relational structure inherent in
user-item interactions. We introduce LlamaRec-LKG-RAG, a novel single-pass,
end-to-end trainable framework that integrates personalized knowledge graph
context into LLM-based recommendation ranking. Our approach extends the
LlamaRec architecture by incorporating a lightweight user preference module
that dynamically identifies salient relation paths within a heterogeneous
knowledge graph constructed from user behavior and item metadata. These
personalized subgraphs are seamlessly integrated into prompts for a fine-tuned
Llama-2 model, enabling efficient and interpretable recommendations through a
unified inference step. Comprehensive experiments on ML-100K and Amazon Beauty
datasets demonstrate consistent and significant improvements over LlamaRec
across key ranking metrics (MRR, NDCG, Recall). LlamaRec-LKG-RAG demonstrates
the critical value of structured reasoning in LLM-based recommendations and
establishes a foundation for scalable, knowledge-aware personalization in
next-generation recommender systems. Code is available
at~\href{https://github.com/VahidAz/LlamaRec-LKG-RAG}{repository}.

</details>


### [479] [Preference-based learning for news headline recommendation](https://arxiv.org/abs/2506.06334)
*Alexandre Bouras,Audrey Durand,Richard Khoury*

Main category: cs.IR

TL;DR: 本研究通过基于偏好的学习优化新闻标题推荐策略，利用法语新闻用户互动数据，在上下文老虎机框架下训练推荐模型，探讨翻译对预测的影响及不同交互策略对用户参与度的提升。结果表明，在噪声上下文存在时，显式探索可能非必要，为实践中的高效简化策略提供依据。


<details>
  <summary>Details</summary>
Motivation: 旨在优化新闻标题推荐效果，探究翻译对用户参与度预测的影响，并评估不同交互策略在数据收集过程中对用户参与度的提升作用。

Method: 基于上下文老虎机框架，使用真实法语新闻用户互动数据训练推荐代理，分析翻译特征与多种交互策略对模型性能的影响。

Result: 在噪声上下文存在时，显式探索策略可能不必要；翻译特征对预测的贡献有限，而简化策略在保持效率的同时可达到相近效果。

Conclusion: 实践中的新闻推荐系统可考虑采用更简化的交互策略，尤其在噪声显著场景下，显式探索带来的边际收益可能不足以覆盖其成本。

Abstract: This study explores strategies for optimizing news headline recommendations
through preference-based learning. Using real-world data of user interactions
with French-language online news posts, we learn a headline recommender agent
under a contextual bandit setting. This allows us to explore the impact of
translation on engagement predictions, as well as the benefits of different
interactive strategies on user engagement during data collection. Our results
show that explicit exploration may not be required in the presence of noisy
contexts, opening the door to simpler but efficient strategies in practice.

</details>


### [480] [Infinity Search: Approximate Vector Search with Projections on q-Metric Spaces](https://arxiv.org/abs/2506.06557)
*Antonio Pariente,Ignacio Hounie,Santiago Segarra,Alejandro Ribeiro*

Main category: cs.IR

TL;DR: 本文提出了一种基于$q$-度量空间的新型投影方法，利用度量树的强化三角不等式特性，降低精确搜索的复杂度，并通过实验验证了该方法在高维数据上的竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有的向量搜索算法通常忽略了向量嵌入的度量结构，未能充分利用其潜在特性。本文旨在通过$q$-度量空间中的度量树，优化搜索效率。

Method: 提出了一种新颖的投影方法，将具有任意相异度度量的向量数据集嵌入到$q$-度量空间，并学习该投影的近似以高效转换查询点。

Result: 实验结果表明，学习$q$-度量近似使经典的度量树算法在高维数据上能够与最先进的搜索方法竞争。

Conclusion: 本文的方法通过利用$q$-度量空间的特性，显著提升了向量搜索的效率，尤其是在高维数据场景下。

Abstract: Despite the ubiquity of vector search applications, prevailing search
algorithms overlook the metric structure of vector embeddings, treating it as a
constraint rather than exploiting its underlying properties. In this paper, we
demonstrate that in $q$-metric spaces, metric trees can leverage a stronger
version of the triangle inequality to reduce comparisons for exact search.
Notably, as $q$ approaches infinity, the search complexity becomes logarithmic.
Therefore, we propose a novel projection method that embeds vector datasets
with arbitrary dissimilarity measures into $q$-metric spaces while preserving
the nearest neighbor. We propose to learn an approximation of this projection
to efficiently transform query points to a space where euclidean distances
satisfy the desired properties. Our experimental results with text and image
vector embeddings show that learning $q$-metric approximations enables classic
metric tree algorithms -- which typically underperform with high-dimensional
data -- to achieve competitive performance against state-of-the-art search
methods.

</details>


### [481] [HotelMatch-LLM: Joint Multi-Task Training of Small and Large Language Models for Efficient Multimodal Hotel Retrieval](https://arxiv.org/abs/2506.07296)
*Arian Askari,Emmanouil Stergiadis,Ilya Gusev,Moran Beladev*

Main category: cs.IR

TL;DR: HotelMatch-LLM提出多模态密集检索模型，通过自然语言搜索解决传统旅游搜索引擎需预设目的地和参数的局限，结合多任务优化、非对称架构和图像处理，显著超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 传统旅游搜索引擎需用户先指定目的地并手动调整搜索参数，限制了搜索灵活性。本文旨在通过自然语言直接搜索酒店属性，提升用户体验。

Method: 1) 领域多任务优化（检索、视觉、语言建模目标）；2) 非对称架构（小型语言模型处理查询，大型模型嵌入酒店数据）；3) 全量图像库处理。

Result: 在四个测试集上表现优于VISTA和MARVEL等模型，主查询类型测试集达到0.681（基准MARVEL为0.603）。验证了多任务优化效果、跨LLM架构通用性及大规模图像处理能力。

Conclusion: 多任务优化显著提升性能，模型具备架构通用性和扩展性，可高效处理复杂图像库，为旅游领域搜索范式提供新方向。

Abstract: We present HotelMatch-LLM, a multimodal dense retrieval model for the travel
domain that enables natural language property search, addressing the
limitations of traditional travel search engines which require users to start
with a destination and editing search parameters. HotelMatch-LLM features three
key innovations: (1) Domain-specific multi-task optimization with three novel
retrieval, visual, and language modeling objectives; (2) Asymmetrical dense
retrieval architecture combining a small language model (SLM) for efficient
online query processing and a large language model (LLM) for embedding hotel
data; and (3) Extensive image processing to handle all property image
galleries. Experiments on four diverse test sets show HotelMatch-LLM
significantly outperforms state-of-the-art models, including VISTA and MARVEL.
Specifically, on the test set -- main query type -- we achieve 0.681 for
HotelMatch-LLM compared to 0.603 for the most effective baseline, MARVEL. Our
analysis highlights the impact of our multi-task optimization, the
generalizability of HotelMatch-LLM across LLM architectures, and its
scalability for processing large image galleries.

</details>


### [482] [Correcting for Position Bias in Learning to Rank: A Control Function Approach](https://arxiv.org/abs/2506.06989)
*Md Aminul Islam,Kathryn Vasilaky,Elena Zheleva*

Main category: cs.IR

TL;DR: 提出一种基于控制函数的两阶段位置偏差校正方法，利用排名过程的外生残差进行去偏，无需依赖点击/倾向模型先验，支持非线性排名模型，并引入验证集点击去偏技术用于超参调优。


<details>
  <summary>Details</summary>
Motivation: 隐式反馈数据(如点击)存在位置偏差——用户更倾向与高位文档交互，直接训练会导致次优排序。现有偏差校正方法需依赖点击模型假设且无法处理非线性模型。

Method: 两阶段控制函数框架：第一阶段利用排名残差生成外生变量，第二阶段构建包含该变量的点击方程。通过plug-in方式兼容现有排序算法，并提出验证集点击去偏技术用于模型选择。

Result: 实验证明该方法在位置偏差校正效果上优于现有state-of-the-art方法，验证集去偏技术有效解决了无偏验证数据缺失时的模型选择问题。

Conclusion: 所提方法具有模型无关性、支持非线性建模、端到端可部署等优势，为实际排序系统提供了一种灵活有效的偏差校正框架。

Abstract: Implicit feedback data, such as user clicks, is commonly used in
learning-to-rank (LTR) systems because it is easy to collect and it often
reflects user preferences. However, this data is prone to various biases, and
training an LTR system directly on biased data can result in suboptimal ranking
performance. One of the most prominent and well-studied biases in implicit
feedback data is position bias, which occurs because users are more likely to
interact with higher-ranked documents regardless of their true relevance. In
this paper, we propose a novel control function-based method that accounts for
position bias in a two-stage process. The first stage uses exogenous variation
from the residuals of the ranking process to correct for position bias in the
second stage click equation. Unlike previous position bias correction methods,
our method does not require knowledge of the click or propensity model and
allows for nonlinearity in the underlying ranking model. Moreover, our method
is general and allows for debiasing any state-of-the-art ranking algorithm by
plugging it into the second stage. We also introduce a technique to debias
validation clicks for hyperparameter tuning to select the optimal model in the
absence of unbiased validation data. Experimental results demonstrate that our
method outperforms state-of-the-art approaches in correcting for position bias.

</details>


### [483] [RADAR: Recall Augmentation through Deferred Asynchronous Retrieval](https://arxiv.org/abs/2506.07261)
*Amit Jaspal,Qian Dang,Ajantha Ramineni*

Main category: cs.IR

TL;DR: 本文提出了一种名为RADAR的新框架，通过异步离线计算预排名更大候选集，显著提升推荐系统的召回率和用户参与度。


<details>
  <summary>Details</summary>
Motivation: 现代大规模推荐系统在多阶段排序过程中，初始检索阶段常依赖效率高但精度低的方法，难以从海量数据中有效筛选出最具吸引力的内容。

Method: RADAR框架利用异步离线计算，使用全复杂度排名模型预排名更大候选集，并在在线推理时直接使用这些高质量候选，跳过在线检索和预排名阶段。

Result: 离线实验显示RADAR显著提升召回率（2X Recall@200），在线A/B测试验证了用户参与度提升0.8%。

Conclusion: RADAR是一种在严格在线服务约束下提升推荐质量的有效方法。

Abstract: Modern large-scale recommender systems employ multi-stage ranking funnel
(Retrieval, Pre-ranking, Ranking) to balance engagement and computational
constraints (latency, CPU). However, the initial retrieval stage, often relying
on efficient but less precise methods like K-Nearest Neighbors (KNN), struggles
to effectively surface the most engaging items from billion-scale catalogs,
particularly distinguishing highly relevant and engaging candidates from merely
relevant ones. We introduce Recall Augmentation through Deferred Asynchronous
Retrieval (RADAR), a novel framework that leverages asynchronous, offline
computation to pre-rank a significantly larger candidate set for users using
the full complexity ranking model. These top-ranked items are stored and
utilized as a high-quality retrieval source during online inference, bypassing
online retrieval and pre-ranking stages for these candidates. We demonstrate
through offline experiments that RADAR significantly boosts recall (2X
Recall@200 vs DNN retrieval baseline) by effectively combining a larger
retrieved candidate set with a more powerful ranking model. Online A/B tests
confirm a +0.8% lift in topline engagement metrics, validating RADAR as a
practical and effective method to improve recommendation quality under strict
online serving constraints.

</details>


### [484] [MoE-MLoRA for Multi-Domain CTR Prediction: Efficient Adaptation with Expert Specialization](https://arxiv.org/abs/2506.07563)
*Ken Yaggel,Eyal German,Aviel Ben Siman Tov*

Main category: cs.IR

TL;DR: 本文提出MoE-MLoRA框架，通过混合专家与动态门控网络提升多领域推荐系统的灵活性，实验表明其在复杂场景中有效，但需根据数据特性调整专家数量。


<details>
  <summary>Details</summary>
Motivation: 传统多领域推荐方法（如MLoRA）采用单一领域适配策略，难以灵活处理用户行为的多样性。需设计更动态的模型以应对不同领域交互的复杂性。

Method: MoE-MLoRA框架：先独立训练各领域专家模型使其专业化，再训练门控网络动态加权专家贡献，结合混合专家（MoE）与低秩适配（LoRA）技术。

Result: 在Taobao-20等动态大规模数据集上加权AUC提升1.45%，但在低多样性/稀疏性数据集（如Movielens）效果有限。专家数量增加未必提升性能，需模型感知调优。

Conclusion: 基于专家的架构通过任务专业化与动态门控可增强复杂推荐系统的预测能力，但需平衡专家数量与数据特性，为多领域推荐提供新方向。

Abstract: Personalized recommendation systems must adapt to user interactions across
different domains. Traditional approaches like MLoRA apply a single adaptation
per domain but lack flexibility in handling diverse user behaviors. To address
this, we propose MoE-MLoRA, a mixture-of-experts framework where each expert is
first trained independently to specialize in its domain before a gating network
is trained to weight their contributions dynamically. We evaluate MoE-MLoRA
across eight CTR models on Movielens and Taobao, showing that it improves
performance in large-scale, dynamic datasets (+1.45 Weighed-AUC in Taobao-20)
but offers limited benefits in structured datasets with low domain diversity
and sparsity. Further analysis of the number of experts per domain reveals that
larger ensembles do not always improve performance, indicating the need for
model-aware tuning. Our findings highlight the potential of expert-based
architectures for multi-domain recommendation systems, demonstrating that
task-aware specialization and adaptive gating can enhance predictive accuracy
in complex environments. The implementation and code are available in our
GitHub repository.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [485] [Private GPTs for LLM-driven testing in software development and machine learning](https://arxiv.org/abs/2506.06509)
*Jakub Jagielski,Markus Abel*

Main category: cs.SE

TL;DR: 本文探讨了私有GPT根据需求自动生成可执行测试代码的能力，通过两步法生成更高质量的测试代码。


<details>
  <summary>Details</summary>
Motivation: 研究私有GPT在生成可执行测试代码方面的能力，为产品所有者或业务智能提供直接生成可测试标准的方法。

Method: 使用接受标准作为输入，通过LLM直接生成代码或通过Gherkin语法进行中间步骤生成测试代码。

Result: 两步法生成的测试代码在人类可读性和最佳编码实践方面表现更好。

Conclusion: 结构化提示能生成更高质量的测试输出，两步法优于直接生成。

Abstract: In this contribution, we examine the capability of private GPTs to
automatically generate executable test code based on requirements. More
specifically, we use acceptance criteria as input, formulated as part of epics,
or stories, which are typically used in modern development processes. This
gives product owners, or business intelligence, respectively, a way to directly
produce testable criteria through the use of LLMs. We explore the quality of
the so-produced tests in two ways: i) directly by letting the LLM generate code
from requirements, ii) through an intermediate step using Gherkin syntax. As a
result, it turns out that the two-step procedure yields better results -where
we define better in terms of human readability and best coding practices, i.e.
lines of code and use of additional libraries typically used in testing.
Concretely, we evaluate prompt effectiveness across two scenarios: a simple
"Hello World" program and a digit classification model, showing that structured
prompts lead to higher-quality test outputs.

</details>


### [486] [Is Your Training Pipeline Production-Ready? A Case Study in the Healthcare Domain](https://arxiv.org/abs/2506.06946)
*Daniel Lawand,Lucas Quaresma,Roberto Bolgheroni,Alfredo Goldman,Renato Cordeiro Ferreira*

Main category: cs.SE

TL;DR: 本文探讨了将机器学习训练管道部署到生产环境中的挑战，并以SPIRA项目为例，展示了其架构从混乱到模块化再到微服务的演进过程，旨在为ML工程师和数据科学家提供MLOps实践的见解。


<details>
  <summary>Details</summary>
Motivation: 将机器学习训练管道部署到生产环境需要稳健的软件工程实践，这与实验性工作流有显著不同。本文通过SPIRA项目，探讨了这一挑战，并展示了如何通过改进架构设计来提升系统的可维护性、健壮性和可扩展性。

Method: 本文首先概述了SPIRA项目的MLES系统，然后比较了其持续训练子系统的三个版本架构，分别从混乱的“大泥球”架构，到模块化单体架构，再到微服务架构，逐步采用不同的设计原则和模式。

Result: 通过架构的演进，SPIRA项目的训练管道在可维护性、健壮性和可扩展性方面得到了显著提升，为ML工程师和数据科学家提供了MLOps实践的有益参考。

Conclusion: 本文通过SPIRA项目的案例，展示了如何通过改进架构设计来提升机器学习训练管道的生产化能力，为ML工程师和数据科学家提供了MLOps实践的宝贵经验。

Abstract: Deploying a Machine Learning (ML) training pipeline into production requires
robust software engineering practices. This differs significantly from
experimental workflows. This experience report investigates this challenge in
SPIRA, a project whose goal is to create an ML-Enabled System (MLES) to
pre-diagnose insufficiency respiratory via speech analysis. The first version
of SPIRA's training pipeline lacked critical software quality attributes. This
paper presents an overview of the MLES, then compares three versions of the
architecture of the Continuous Training subsystem, which evolved from a Big
Ball of Mud, to a Modular Monolith, towards Microservices. By adopting
different design principles and patterns to enhance its maintainability,
robustness, and extensibility. In this way, the paper seeks to offer insights
for both ML Engineers tasked to productionize ML training pipelines and Data
Scientists seeking to adopt MLOps practices.

</details>


### [487] [Taxonomy of migration scenarios for Qiskit refactoring using LLMs](https://arxiv.org/abs/2506.07135)
*José Manuel Suárez,Luís Mariano Bibbó,Joaquín Bogado,Alejandro Fernandez*

Main category: cs.SE

TL;DR: 本文研究了量子计算软件中的代码重构问题，开发了量子电路重构问题的分类法，并利用大语言模型（LLMs）对Qiskit版本迁移中的重构需求进行分类，最终整合了专家和LLM的分类法，为未来AI辅助迁移研究提供了基础。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算的发展，量子编程库的异构性和不断更新给开发者带来了新的挑战，尤其是频繁的库更新导致代码需要重构，增加了复杂性。这些重构问题与经典软件工程中的问题有本质不同，因此需要专门的研究。

Method: 本文通过分析Qiskit文档和发布说明，创建了Qiskit版本迁移中所需重构的初始分类法，并分别由专家开发者和LLM生成两种分类法，最后将两者整合为统一的分类法。

Result: 研究生成了两种分类法，并通过比较和整合，形成了一个统一的分类法，为未来AI辅助迁移研究提供了基础，并促进了量子软件工程中的最佳实践。

Conclusion: 本文通过系统化分类Qiskit中的重构挑战，为未来AI辅助迁移研究提供了基础，同时提升了量子软件开发的流程、语言兼容性和最佳实践。

Abstract: As quantum computing advances, quantum programming libraries' heterogeneity
and steady evolution create new challenges for software developers. Frequent
updates in software libraries break working code that needs to be refactored,
thus adding complexity to an already complex landscape. These refactoring
challenges are, in many cases, fundamentally different from those known in
classical software engineering due to the nature of quantum computing software.
This study addresses these challenges by developing a taxonomy of quantum
circuit's refactoring problems, providing a structured framework to analyze and
compare different refactoring approaches. Large Language Models (LLMs) have
proven valuable tools for classic software development, yet their value in
quantum software engineering remains unexplored. This study uses LLMs to
categorize refactoring needs in migration scenarios between different Qiskit
versions. Qiskit documentation and release notes were scrutinized to create an
initial taxonomy of refactoring required for migrating between Qiskit releases.
Two taxonomies were produced: one by expert developers and one by an LLM. These
taxonomies were compared, analyzing differences and similarities, and were
integrated into a unified taxonomy that reflects the findings of both methods.
By systematically categorizing refactoring challenges in Qiskit, the unified
taxonomy is a foundation for future research on AI-assisted migration while
enabling a more rigorous evaluation of automated refactoring techniques.
Additionally, this work contributes to quantum software engineering (QSE) by
enhancing software development workflows, improving language compatibility, and
promoting best practices in quantum programming.

</details>


### [488] [IntenTest: Stress Testing for Intent Integrity in API-Calling LLM Agents](https://arxiv.org/abs/2506.07524)
*Shiwei Feng,Xiangzhe Xu,Xuan Chen,Kaiyuan Zhang,Syed Yusuf Ahmed,Zian Su,Mingwei Zheng,Xiangyu Zhang*

Main category: cs.SE

TL;DR: IntenTest是一个API中心的压力测试框架，旨在系统性地揭示LLM代理中的意图完整性违规，通过生成基于工具包文档的现实任务并应用定向突变来暴露代理错误。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理被越来越多地用于通过自然语言指令调用API来自动化现实世界任务，它们经常因误解用户意图而导致行为偏离用户目标，尤其是在外部工具包不断演变的情况下。传统的软件测试方法无法有效处理自然语言的模糊性。

Method: IntenTest通过语义分区将自然语言任务组织成有意义的类别，基于工具包API参数及其等价类。在每个分区内，种子任务被突变并通过轻量级预测器进行排名，以估计触发代理错误的可能性。此外，IntenTest采用数据类型感知的策略记忆，从过去的案例中检索并调整有效的突变模式。

Result: 在80个工具包API上的实验表明，IntenTest有效地揭示了意图完整性违规，在错误暴露率和查询效率方面显著优于基线方法。此外，IntenTest能够很好地泛化到更强的目标模型，并使用较小的LLM进行测试生成，同时适应跨领域的API演变。

Conclusion: IntenTest为LLM代理的意图完整性测试提供了一种有效且高效的方法，能够适应不断变化的API环境，并显著提升测试的覆盖率和准确性。

Abstract: LLM agents are increasingly deployed to automate real-world tasks by invoking
APIs through natural language instructions. While powerful, they often suffer
from misinterpretation of user intent, leading to the agent's actions that
diverge from the user's intended goal, especially as external toolkits evolve.
Traditional software testing assumes structured inputs and thus falls short in
handling the ambiguity of natural language. We introduce IntenTest, an
API-centric stress testing framework that systematically uncovers intent
integrity violations in LLM agents. Unlike prior work focused on fixed
benchmarks or adversarial inputs, IntenTest generates realistic tasks based on
toolkits' documentation and applies targeted mutations to expose subtle agent
errors while preserving user intent. To guide testing, we propose semantic
partitioning, which organizes natural language tasks into meaningful categories
based on toolkit API parameters and their equivalence classes. Within each
partition, seed tasks are mutated and ranked by a lightweight predictor that
estimates the likelihood of triggering agent errors. To enhance efficiency,
IntenTest maintains a datatype-aware strategy memory that retrieves and adapts
effective mutation patterns from past cases. Experiments on 80 toolkit APIs
demonstrate that IntenTest effectively uncovers intent integrity violations,
significantly outperforming baselines in both error-exposing rate and query
efficiency. Moreover, IntenTest generalizes well to stronger target models
using smaller LLMs for test generation, and adapts to evolving APIs across
domains.

</details>


### [489] [Towards a Small Language Model Lifecycle Framework](https://arxiv.org/abs/2506.07695)
*Parsa Miraghaei,Sergio Moreschini,Antti Kolehmainen,David Hästbacka*

Main category: cs.SE

TL;DR: 本文提出了一种小型语言模型（SLM）的全生命周期框架，通过综合学术文献和实践经验，为SLM的开发与维护提供了系统化指导。


<details>
  <summary>Details</summary>
Motivation: 随着对高效、可部署语言模型需求的增长，小型语言模型（SLM）受到关注，但现有研究缺乏统一的生命周期视角。

Method: 通过对36篇文献的综合调查，分析并分类了与生命周期相关的技术。

Result: 提出了一个模块化的生命周期模型，包含主要、可选和跨领域组件，支持方法复用、协同适应和生命周期意识。

Conclusion: 该框架为SLM的开发与维护提供了系统化基础，连接理论与实践，并指导未来研究和工具开发。

Abstract: Background: The growing demand for efficient and deployable language models
has led to increased interest in Small Language Models (SLMs). However,
existing research remains fragmented, lacking a unified lifecycle perspective.
  Objective: This study aims to define a comprehensive lifecycle framework for
SLMs by synthesizing insights from academic literature and practitioner
sources.
  Method: We conducted a comprehensive survey of 36 works, analyzing and
categorizing lifecycle-relevant techniques.
  Results: We propose a modular lifecycle model structured into main, optional,
and cross-cutting components. The model captures key interconnections across
stages, supporting method reuse, co-adaptation, and lifecycle-awareness.
  Conclusion: Our framework provides a coherent foundation for developing and
maintaining SLMs, bridging theory and practice, and guiding future research and
tool development.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [490] [ChemGraph: An Agentic Framework for Computational Chemistry Workflows](https://arxiv.org/abs/2506.06363)
*Thang D. Pham,Aditya Tanikanti,Murat Keçeli*

Main category: physics.chem-ph

TL;DR: ChemGraph是一个结合AI与先进模拟工具的框架，通过图神经网络和大型语言模型自动化化学与材料科学工作流，实验表明任务分解策略可使小模型在特定场景超越大模型性能。


<details>
  <summary>Details</summary>
Motivation: 原子模拟在化学与材料科学中面临计算方法多样、软件生态复杂、依赖专家知识与人工操作等挑战，亟需自动化解决方案降低使用门槛。

Method: 开发ChemGraph多智能体框架：利用图神经网络基础模型实现高效计算，结合LLM进行自然语言交互与任务规划，支持从紧束缚法到密度泛函理论等多种方法。

Result: 在13项基准测试中，小模型（如GPT-4o-mini）处理简单工作流表现良好，复杂任务需大模型（如GPT-4o）；通过任务分解策略可使小模型在特定场景达到或超越GPT-4o性能。

Conclusion: 多智能体框架通过任务分解有效整合不同规模模型，证明复杂任务拆解策略能提升小模型性能，为自动化计算化学提供灵活高效的解决方案。

Abstract: Atomistic simulations are essential tools in chemistry and materials science,
accelerating the discovery of novel catalysts, energy storage materials, and
pharmaceuticals. However, running these simulations remains challenging due to
the wide range of computational methods, diverse software ecosystems, and the
need for expert knowledge and manual effort for the setup, execution, and
validation stages. In this work, we present ChemGraph, an agentic framework
powered by artificial intelligence and state-of-the-art simulation tools to
streamline and automate computational chemistry and materials science
workflows. ChemGraph leverages graph neural network-based foundation models for
accurate yet computationally efficient calculations and large language models
(LLMs) for natural language understanding, task planning, and scientific
reasoning to provide an intuitive and interactive interface. Users can perform
tasks such as molecular structure generation, single-point energy, geometry
optimization, vibrational analysis, and thermochemistry calculations with
methods ranging from tight-binding and machine learning interatomic potentials
to density functional theory or wave function theory-based methods. We evaluate
ChemGraph across 13 benchmark tasks and demonstrate that smaller LLMs
(GPT-4o-mini, Claude-3.5-haiku, Qwen2.5-14B) perform well on simple workflows,
while more complex tasks benefit from using larger models like GPT-4o.
Importantly, we show that decomposing complex tasks into smaller subtasks
through a multi-agent framework enables smaller LLM models to match or exceed
GPT-4o's performance in specific scenarios.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [491] [Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning](https://arxiv.org/abs/2309.11082)
*Chen Jiang,Hong Liu,Xuzheng Yu,Qing Wang,Yuan Cheng,Jia Xu,Zhongyi Liu,Qingpei Guo,Wei Chu,Ming Yang,Yuan Qi*

Main category: cs.CV

TL;DR: 本文提出两种新方法DMAE和TPM-CL，通过挖掘困难负样本和构建细粒度三元组样本，显著提升了跨模态文本-视频检索的对比学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习在文本-视频检索任务中忽视困难负样本的挖掘，且缺乏对多层次语义相似性的建模能力。

Method: 提出双模态注意力增强模块(DMAE)挖掘困难负样本，并设计NegNCE损失函数强化其影响；开发三元组部分间隔对比学习(TPM-CL)，通过自适应token掩码策略建模细粒度语义差异。

Result: 在MSR-VTT等四个主流数据集上超越现有方法，验证了方法的有效性。

Conclusion: 通过显式处理困难负样本和细粒度语义建模，提出的双模块方法为跨模态检索任务提供了新的有效解决方案。

Abstract: In recent years, the explosion of web videos makes text-video retrieval
increasingly essential and popular for video filtering, recommendation, and
search. Text-video retrieval aims to rank relevant text/video higher than
irrelevant ones. The core of this task is to precisely measure the cross-modal
similarity between texts and videos. Recently, contrastive learning methods
have shown promising results for text-video retrieval, most of which focus on
the construction of positive and negative pairs to learn text and video
representations. Nevertheless, they do not pay enough attention to hard
negative pairs and lack the ability to model different levels of semantic
similarity. To address these two issues, this paper improves contrastive
learning using two novel techniques. First, to exploit hard examples for robust
discriminative power, we propose a novel Dual-Modal Attention-Enhanced Module
(DMAE) to mine hard negative pairs from textual and visual clues. By further
introducing a Negative-aware InfoNCE (NegNCE) loss, we are able to adaptively
identify all these hard negatives and explicitly highlight their impacts in the
training loss. Second, our work argues that triplet samples can better model
fine-grained semantic similarity compared to pairwise samples. We thereby
present a new Triplet Partial Margin Contrastive Learning (TPM-CL) module to
construct partial order triplet samples by automatically generating
fine-grained hard negatives for matched text-video pairs. The proposed TPM-CL
designs an adaptive token masking strategy with cross-modal interaction to
model subtle semantic differences. Extensive experiments demonstrate that the
proposed approach outperforms existing methods on four widely-used text-video
retrieval datasets, including MSR-VTT, MSVD, DiDeMo and ActivityNet.

</details>


### [492] [MiniGPT-Reverse-Designing: Predicting Image Adjustments Utilizing MiniGPT-4](https://arxiv.org/abs/2406.00971)
*Vahid Azizi,Fatemeh Koochaki*

Main category: cs.CV

TL;DR: 本文通过扩展和微调MiniGPT-4，展示了现成视觉语言模型在复杂任务如逆向设计中的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索视觉语言模型（VLMs）在处理复杂多模态任务（如逆向设计）中的潜力，特别是与大型语言模型（LLMs）结合后的表现。

Method: 本文扩展并微调了MiniGPT-4模型，用于逆向设计任务，该任务要求模型同时理解源图像、编辑后的图像及可选的文本描述。

Result: 实验结果表明，MiniGPT-4在逆向设计等复杂任务中表现出色，验证了现成VLMs的可扩展性。

Conclusion: 本文成功展示了MiniGPT-4在复杂视觉语言任务中的潜力，为未来多模态模型的研究提供了新的方向。

Abstract: Vision-Language Models (VLMs) have recently seen significant advancements
through integrating with Large Language Models (LLMs). The VLMs, which process
image and text modalities simultaneously, have demonstrated the ability to
learn and understand the interaction between images and texts across various
multi-modal tasks. Reverse designing, which could be defined as a complex
vision-language task, aims to predict the edits and their parameters, given a
source image, an edited version, and an optional high-level textual edit
description. This task requires VLMs to comprehend the interplay between the
source image, the edited version, and the optional textual context
simultaneously, going beyond traditional vision-language tasks. In this paper,
we extend and fine-tune MiniGPT-4 for the reverse designing task. Our
experiments demonstrate the extensibility of off-the-shelf VLMs, specifically
MiniGPT-4, for more complex tasks such as reverse designing. Code is available
at this \href{https://github.com/VahidAz/MiniGPT-Reverse-Designing}

</details>


### [493] [STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis](https://arxiv.org/abs/2506.06276)
*Jiatao Gu,Tianrong Chen,David Berthelot,Huangjie Zheng,Yuyang Wang,Ruixiang Zhang,Laurent Dinh,Miguel Angel Bautista,Josh Susskind,Shuangfei Zhai*

Main category: cs.CV

TL;DR: 提出STARFlow模型，通过结合标准化流与自回归Transformer架构，引入深度-浅层设计、潜在空间建模和新引导算法，在高分辨率图像生成任务中达到与扩散模型竞争的SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 解决标准化流模型在大规模高分辨率图像生成中扩展性不足的问题，通过融合Transformer的结构化建模能力提升生成质量，同时保持标准化流的精确似然计算优势。

Method: 1) 提出TARFlow架构结合标准化流与自回归Transformer；2) 深度-浅层混合设计平衡计算效率与表达能力；3) 基于预训练自编码器的潜在空间建模；4) 新型引导算法提升样本质量；5) 端到端连续空间最大似然训练。

Result: 在类别条件与文本条件图像生成任务中取得竞争性表现，样本质量接近当前最优扩散模型，首次验证标准化流在大规模高分辨率场景的有效性。

Conclusion: 通过架构创新成功扩展标准化流的应用边界，证明其在高分辨率生成任务中可与扩散模型抗衡，为生成模型研究提供了新的技术路径。

Abstract: We present STARFlow, a scalable generative model based on normalizing flows
that achieves strong performance in high-resolution image synthesis. The core
of STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the
expressive power of normalizing flows with the structured modeling capabilities
of Autoregressive Transformers. We first establish the theoretical universality
of TARFlow for modeling continuous distributions. Building on this foundation,
we introduce several key architectural and algorithmic innovations to
significantly enhance scalability: (1) a deep-shallow design, wherein a deep
Transformer block captures most of the model representational capacity,
complemented by a few shallow Transformer blocks that are computationally
efficient yet substantially beneficial; (2) modeling in the latent space of
pretrained autoencoders, which proves more effective than direct pixel-level
modeling; and (3) a novel guidance algorithm that significantly boosts sample
quality. Crucially, our model remains an end-to-end normalizing flow, enabling
exact maximum likelihood training in continuous spaces without discretization.
STARFlow achieves competitive performance in both class-conditional and
text-conditional image generation tasks, approaching state-of-the-art diffusion
models in sample quality. To our knowledge, this work is the first successful
demonstration of normalizing flows operating effectively at this scale and
resolution.

</details>


### [494] [Facial Foundational Model Advances Early Warning of Coronary Artery Disease from Live Videos with DigitalShadow](https://arxiv.org/abs/2506.06283)
*Juexiao Zhou,Zhongyi Han,Mankun Xin,Xingwei He,Guotao Wang,Jiaoyan Song,Gongning Luo,Wenjia He,Xintong Li,Yuetan Chu,Juanwen Chen,Bo Wang,Xia Wu,Wenwen Duan,Zhixia Guo,Liyan Bai,Yilin Pan,Xuefei Bi,Lu Liu,Long Feng,Xiaonan He,Xin Gao*

Main category: cs.CV

TL;DR: 本文介绍了DigitalShadow，一种基于面部基础模型的冠心病早期预警系统，通过无接触方式从视频流中提取面部特征，生成个性化健康报告。


<details>
  <summary>Details</summary>
Motivation: 全球人口老龄化对医疗系统带来挑战，冠心病是主要死因之一，早期检测和主动管理至关重要。

Method: DigitalShadow系统基于预训练的2100万张面部图像，并在中国四家医院的1751名受试者的7004张面部图像上微调，形成LiveCAD模型，用于无接触式冠心病风险评估。

Result: DigitalShadow能够从视频流中提取面部特征，生成自然语言风险报告和个性化健康建议，且支持本地部署以确保数据隐私。

Conclusion: DigitalShadow作为一种无接触式早期预警系统，为冠心病的预防和管理提供了有效工具，同时保障了用户隐私。

Abstract: Global population aging presents increasing challenges to healthcare systems,
with coronary artery disease (CAD) responsible for approximately 17.8 million
deaths annually, making it a leading cause of global mortality. As CAD is
largely preventable, early detection and proactive management are essential. In
this work, we introduce DigitalShadow, an advanced early warning system for
CAD, powered by a fine-tuned facial foundation model. The system is pre-trained
on 21 million facial images and subsequently fine-tuned into LiveCAD, a
specialized CAD risk assessment model trained on 7,004 facial images from 1,751
subjects across four hospitals in China. DigitalShadow functions passively and
contactlessly, extracting facial features from live video streams without
requiring active user engagement. Integrated with a personalized database, it
generates natural language risk reports and individualized health
recommendations. With privacy as a core design principle, DigitalShadow
supports local deployment to ensure secure handling of user data.

</details>


### [495] [Mitigating Object Hallucination via Robust Local Perception Search](https://arxiv.org/abs/2506.06729)
*Zixian Gao,Chao Yang,Zhanhui Zhou,Xing Xu,Chaochao Lu*

Main category: cs.CV

TL;DR: 提出了一种名为局部感知搜索（LPS）的解码方法，用于减少多模态大语言模型（MLLMs）中的幻觉现象，特别是在图像噪声较高的场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型在视觉和语言整合方面取得了显著成功，但其输出仍存在与图像内容不符的幻觉现象，需要一种简单且无需训练的方法来缓解这一问题。

Method: 引入局部感知搜索（LPS），利用局部视觉先验信息作为值函数来修正解码过程，该方法无需训练且兼容多种模型。

Result: 在广泛使用的幻觉基准测试和噪声数据上的实验表明，LPS显著减少了幻觉现象，尤其在噪声环境中表现突出。

Conclusion: LPS是一种即插即用的方法，有效抑制了多模态大语言模型中的幻觉现象，特别是在高噪声场景下表现优异。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have enabled
them to effectively integrate vision and language, addressing a variety of
downstream tasks. However, despite their significant success, these models
still exhibit hallucination phenomena, where the outputs appear plausible but
do not align with the content of the images. To mitigate this issue, we
introduce Local Perception Search (LPS), a decoding method during inference
that is both simple and training-free, yet effectively suppresses
hallucinations. This method leverages local visual prior information as a value
function to correct the decoding process. Additionally, we observe that the
impact of the local visual prior on model performance is more pronounced in
scenarios with high levels of image noise. Notably, LPS is a plug-and-play
approach that is compatible with various models. Extensive experiments on
widely used hallucination benchmarks and noisy data demonstrate that LPS
significantly reduces the incidence of hallucinations compared to the baseline,
showing exceptional performance, particularly in noisy settings.

</details>


### [496] [Textile Analysis for Recycling Automation using Transfer Learning and Zero-Shot Foundation Models](https://arxiv.org/abs/2506.06569)
*Yannis Spyridis,Vasileios Argyriou*

Main category: cs.CV

TL;DR: 本文探讨了使用RGB图像和深度学习技术（如迁移学习和基础模型）来自动化纺织品回收中的关键预处理任务，包括分类和特征分割。


<details>
  <summary>Details</summary>
Motivation: 提高纺织品回收的效率和可扩展性，准确识别材料成分和检测污染物。

Method: 使用RGB图像，结合迁移学习进行纺织品分类，采用Grounding DINO和Segment Anything Model进行零样本特征分割。

Result: EfficientNetB0在分类任务中取得了81.25%的准确率，特征分割的mIoU达到了0.90。

Conclusion: 研究表明，RGB图像结合现代深度学习技术可以有效地支持自动化纺织品回收的关键分析步骤。

Abstract: Automated sorting is crucial for improving the efficiency and scalability of
textile recycling, but accurately identifying material composition and
detecting contaminants from sensor data remains challenging. This paper
investigates the use of standard RGB imagery, a cost-effective sensing
modality, for key pre-processing tasks in an automated system. We present
computer vision components designed for a conveyor belt setup to perform (a)
classification of four common textile types and (b) segmentation of non-textile
features such as buttons and zippers. For classification, several pre-trained
architectures were evaluated using transfer learning and cross-validation, with
EfficientNetB0 achieving the best performance on a held-out test set with
81.25\% accuracy. For feature segmentation, a zero-shot approach combining the
Grounding DINO open-vocabulary detector with the Segment Anything Model (SAM)
was employed, demonstrating excellent performance with a mIoU of 0.90 for the
generated masks against ground truth. This study demonstrates the feasibility
of using RGB images coupled with modern deep learning techniques, including
transfer learning for classification and foundation models for zero-shot
segmentation, to enable essential analysis steps for automated textile
recycling pipelines.

</details>


### [497] [Interpretable and Reliable Detection of AI-Generated Images via Grounded Reasoning in MLLMs](https://arxiv.org/abs/2506.07045)
*Yikun Ji,Hong Yan,Jun Lan,Huijia Zhu,Weiqiang Wang,Qi Fan,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: 本文提出一种基于多模态大语言模型（MLLMs）的AI生成图像检测方法，通过构建标注数据集和多阶段优化策略，实现高精度检测与可解释性，显著超越基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测方法多为黑箱模型，缺乏可解释性；而MLLMs虽具推理能力，但存在幻觉问题且视觉解释与人类认知不一致，需改进其检测与解释的协同能力。

Method: 构建包含合成伪影标注框与描述的数据集，并通过多阶段优化策略微调MLLMs，平衡检测准确性、视觉定位和文本解释的连贯性。

Result: 模型在AI生成图像检测和视觉缺陷定位任务中表现显著优于基线方法，同时提供符合人类推理的解释。

Conclusion: 结合视觉-文本联合推理与多阶段优化的方法，有效提升MLLMs在AI生成图像检测中的性能与可解释性，为可信检测提供新思路。

Abstract: The rapid advancement of image generation technologies intensifies the demand
for interpretable and robust detection methods. Although existing approaches
often attain high accuracy, they typically operate as black boxes without
providing human-understandable justifications. Multi-modal Large Language
Models (MLLMs), while not originally intended for forgery detection, exhibit
strong analytical and reasoning capabilities. When properly fine-tuned, they
can effectively identify AI-generated images and offer meaningful explanations.
However, existing MLLMs still struggle with hallucination and often fail to
align their visual interpretations with actual image content and human
reasoning. To bridge this gap, we construct a dataset of AI-generated images
annotated with bounding boxes and descriptive captions that highlight synthesis
artifacts, establishing a foundation for human-aligned visual-textual grounded
reasoning. We then finetune MLLMs through a multi-stage optimization strategy
that progressively balances the objectives of accurate detection, visual
localization, and coherent textual explanation. The resulting model achieves
superior performance in both detecting AI-generated images and localizing
visual flaws, significantly outperforming baseline methods.

</details>


### [498] [Learning Compact Vision Tokens for Efficient Large Multimodal Models](https://arxiv.org/abs/2506.07138)
*Hao Tang,Chengchao Shen*

Main category: cs.CV

TL;DR: 本文提出一种结合空间令牌融合（STF）和多块令牌融合（MBTF）的方法，通过减少视觉令牌数量并补充多粒度特征，在保持多模态推理能力的同时显著提升大型多模态模型的计算效率。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型（LMMs）因LLMs的高计算成本和长视觉令牌序列的二次复杂度面临效率挑战，且现有冻结权重的视觉编码器难以适应多样化下游任务需求。

Method: 提出STF融合空间相邻令牌以缩短序列长度，并设计MBTF模块为缩减后的令牌序列补充多粒度特征，平衡令牌压缩与信息保留。

Result: 基于LLaVA-1.5的模型在8个主流视觉-语言基准测试中仅使用基线25%的视觉令牌，达到可比或更优性能，代码与权重已开源。

Conclusion: 该方法通过联合优化令牌压缩与特征增强，在显著提升推理效率的同时未牺牲模型性能，为多模态模型轻量化提供了有效解决方案。

Abstract: Large multimodal models (LMMs) suffer significant computational challenges
due to the high cost of Large Language Models (LLMs) and the quadratic
complexity of processing long vision token sequences. In this paper, we explore
the spatial redundancy among vision tokens and shorten the length of vision
token sequences for inference acceleration. Specifically, we propose a Spatial
Token Fusion (STF) method to learn compact vision tokens for short vision token
sequence, where spatial-adjacent tokens are fused into one. Meanwhile,
weight-frozen vision encoder can not well adapt to the demand of extensive
downstream vision-language tasks. To this end, we further introduce a
Multi-Block Token Fusion (MBTF) module to supplement multi-granularity features
for the reduced token sequence. Overall, we combine STF and MBTF module to
balance token reduction and information preservation, thereby improving
inference efficiency without sacrificing multimodal reasoning capabilities.
Experimental results demonstrate that our method based on LLaVA-1.5 achieves
comparable or even superior performance to the baseline on 8 popular
vision-language benchmarks with only $25\%$ vision tokens of baseline. The
source code and trained weights are available at
https://github.com/visresearch/LLaVA-STF.

</details>


### [499] [SAP-Bench: Benchmarking Multimodal Large Language Models in Surgical Action Planning](https://arxiv.org/abs/2506.07196)
*Mengya Xu,Zhongzhen Huang,Dillan Imans,Yiru Ye,Xiaofan Zhang,Qi Dou*

Main category: cs.CV

TL;DR: 本文提出SAP-Bench数据集与MLLM-SAP框架，用于评估多模态大语言模型在手术动作规划任务中的表现，发现现有模型在预测下一手术动作方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法充分评估模型在手术动作规划任务中所需的原子视觉动作识别与长流程协调能力，而手术决策具有生命攸关性，需建立可靠评估体系以确保患者安全。

Method: 构建基于胆囊切除术的SAP-Bench数据集（含1,226个临床验证动作标注），提出融合领域知识的MLLM-SAP框架，通过当前手术场景与自然语言指令生成下一动作建议。

Result: 对7个SOTA模型（包括GPT-4o、Claude-3.5等）的评估显示，模型在下一动作预测任务中普遍存在关键性能缺陷。

Conclusion: SAP-Bench填补了手术规划评估基准的空白，揭示了当前MLLMs在复杂决策任务中的局限性，为提升医疗AI可靠性提供了新方向。

Abstract: Effective evaluation is critical for driving advancements in MLLM research.
The surgical action planning (SAP) task, which aims to generate future action
sequences from visual inputs, demands precise and sophisticated analytical
capabilities. Unlike mathematical reasoning, surgical decision-making operates
in life-critical domains and requires meticulous, verifiable processes to
ensure reliability and patient safety. This task demands the ability to
distinguish between atomic visual actions and coordinate complex, long-horizon
procedures, capabilities that are inadequately evaluated by current benchmarks.
To address this gap, we introduce SAP-Bench, a large-scale, high-quality
dataset designed to enable multimodal large language models (MLLMs) to perform
interpretable surgical action planning. Our SAP-Bench benchmark, derived from
the cholecystectomy procedures context with the mean duration of 1137.5s, and
introduces temporally-grounded surgical action annotations, comprising the
1,226 clinically validated action clips (mean duration: 68.7s) capturing five
fundamental surgical actions across 74 procedures. The dataset provides 1,152
strategically sampled current frames, each paired with the corresponding next
action as multimodal analysis anchors. We propose the MLLM-SAP framework that
leverages MLLMs to generate next action recommendations from the current
surgical scene and natural language instructions, enhanced with injected
surgical domain knowledge. To assess our dataset's effectiveness and the
broader capabilities of current models, we evaluate seven state-of-the-art
MLLMs (e.g., OpenAI-o1, GPT-4o, QwenVL2.5-72B, Claude-3.5-Sonnet, GeminiPro2.5,
Step-1o, and GLM-4v) and reveal critical gaps in next action prediction
performance.

</details>


### [500] [Hallucination at a Glance: Controlled Visual Edits and Fine-Grained Multimodal Learning](https://arxiv.org/abs/2506.07227)
*Tianyi Bai,Yuxuan Fan,Jiantao Qiu,Fupeng Sun,Jiayi Song,Junlin Han,Zichen Liu,Conghui He,Wentao Zhang,Binhang Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种针对多模态大语言模型（MLLMs）的微编辑数据集（MED）和监督微调框架，以提高模型在细粒度视觉差异上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在视觉-语言任务上表现良好，但在处理细粒度视觉差异时容易出现幻觉或遗漏语义变化，主要原因是训练数据和学习目标的局限性。

Method: 通过构建包含50K图像-文本对的微编辑数据集（MED），并引入特征级一致性损失的监督微调框架，以增强模型在细微视觉变化下的稳定性。

Result: 该方法在微编辑检测基准上提高了差异检测的准确性，减少了幻觉现象，并在图像描述和视觉问答等标准任务上取得了稳定的提升。

Conclusion: 结合有针对性的数据和对齐目标，可以有效增强多模态大语言模型在细粒度视觉推理上的表现。

Abstract: Multimodal large language models (MLLMs) have achieved strong performance on
vision-language tasks but still struggle with fine-grained visual differences,
leading to hallucinations or missed semantic shifts. We attribute this to
limitations in both training data and learning objectives. To address these
issues, we propose a controlled data generation pipeline that produces
minimally edited image pairs with semantically aligned captions. Using this
pipeline, we construct the Micro Edit Dataset (MED), containing over 50K
image-text pairs spanning 11 fine-grained edit categories, including attribute,
count, position, and object presence changes. Building on MED, we introduce a
supervised fine-tuning (SFT) framework with a feature-level consistency loss
that promotes stable visual embeddings under small edits. We evaluate our
approach on the Micro Edit Detection benchmark, which includes carefully
balanced evaluation pairs designed to test sensitivity to subtle visual
variations across the same edit categories. Our method improves difference
detection accuracy and reduces hallucinations compared to strong baselines,
including GPT-4o. Moreover, it yields consistent gains on standard
vision-language tasks such as image captioning and visual question answering.
These results demonstrate the effectiveness of combining targeted data and
alignment objectives for enhancing fine-grained visual reasoning in MLLMs.

</details>


### [501] [Multi-Step Visual Reasoning with Visual Tokens Scaling and Verification](https://arxiv.org/abs/2506.07235)
*Tianyi Bai,Zengjie Hu,Fupeng Sun,Jiantao Qiu,Yizhen Jiang,Guangxin He,Bohan Zeng,Conghui He,Binhang Yuan,Wentao Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态大语言模型框架，通过动态推理机制实现迭代、验证器引导的视觉内容推理，显著提升了视觉推理任务的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型采用静态推理范式，无法在推理过程中迭代优化或适应上下文，与人类动态、选择性和反馈驱动的感知方式形成鲜明对比。

Method: 本文提出了一种基于马尔可夫决策过程的推理框架，包括提出视觉动作的推理器和通过多步直接偏好优化训练的验证器，并引入了新的数据集VTS来支持该方法。

Result: 该方法在多种视觉推理基准测试中显著优于现有方法，不仅提高了准确性，还提供了更具可解释性和基础性的推理过程。

Conclusion: 动态推理机制为下一代多模态大语言模型实现细粒度、上下文感知的视觉推理提供了广阔前景。

Abstract: Multi-modal large language models (MLLMs) have achieved remarkable
capabilities by integrating visual perception with language understanding,
enabling applications such as image-grounded dialogue, visual question
answering, and scientific analysis. However, most MLLMs adopt a static
inference paradigm, encoding the entire image into fixed visual tokens upfront,
which limits their ability to iteratively refine understanding or adapt to
context during inference. This contrasts sharply with human perception, which
is dynamic, selective, and feedback-driven. In this work, we introduce a novel
framework for inference-time visual token scaling that enables MLLMs to perform
iterative, verifier-guided reasoning over visual content. We formulate the
problem as a Markov Decision Process, involving a reasoner that proposes visual
actions and a verifier, which is trained via multi-step Direct Preference
Optimization (DPO), that evaluates these actions and determines when reasoning
should terminate. To support this, we present a new dataset, VTS, comprising
supervised reasoning trajectories (VTS-SFT) and preference-labeled reasoning
comparisons (VTS-DPO). Our method significantly outperforms existing approaches
across diverse visual reasoning benchmarks, offering not only improved accuracy
but also more interpretable and grounded reasoning processes. These results
demonstrate the promise of dynamic inference mechanisms for enabling
fine-grained, context-aware visual reasoning in next-generation MLLMs.

</details>


### [502] [GLOS: Sign Language Generation with Temporally Aligned Gloss-Level Conditioning](https://arxiv.org/abs/2506.07460)
*Taeryung Lee,Hyeongjin Nam,Gyeongsik Moon,Kyoung Mu Lee*

Main category: cs.CV

TL;DR: 本文提出GLOS框架，通过时间对齐的gloss级条件与条件融合模块TAC，解决了手语生成中词汇顺序错误和语义模糊问题，显著提升了生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有手语生成方法采用句子级条件编码，导致无法捕捉手语时间结构及细粒度语义，生成结果常出现词汇顺序混乱和动作歧义。

Method: 结合gloss级条件（时间对齐的gloss嵌入序列）与TAC模块（时序对齐条件融合），实现细粒度语义控制和时序结构保持。

Result: 在CSL-Daily和Phoenix-2014T数据集上，GLOS生成的手语词汇顺序正确且语义准确性优于现有方法。

Conclusion: 通过gloss级时序对齐条件与TAC模块的协同作用，GLOS框架有效提升了手语生成的语义准确性和动作连贯性。

Abstract: Sign language generation (SLG), or text-to-sign generation, bridges the gap
between signers and non-signers. Despite recent progress in SLG, existing
methods still often suffer from incorrect lexical ordering and low semantic
accuracy. This is primarily due to sentence-level condition, which encodes the
entire sentence of the input text into a single feature vector as a condition
for SLG. This approach fails to capture the temporal structure of sign language
and lacks the granularity of word-level semantics, often leading to disordered
sign sequences and ambiguous motions. To overcome these limitations, we propose
GLOS, a sign language generation framework with temporally aligned gloss-level
conditioning. First, we employ gloss-level conditions, which we define as
sequences of gloss embeddings temporally aligned with the motion sequence. This
enables the model to access both the temporal structure of sign language and
word-level semantics at each timestep. As a result, this allows for
fine-grained control of signs and better preservation of lexical order. Second,
we introduce a condition fusion module, temporal alignment conditioning (TAC),
to efficiently deliver the word-level semantic and temporal structure provided
by the gloss-level condition to the corresponding motion timesteps. Our method,
which is composed of gloss-level conditions and TAC, generates signs with
correct lexical order and high semantic accuracy, outperforming prior methods
on CSL-Daily and Phoenix-2014T.

</details>


### [503] [Improving Wildlife Out-of-Distribution Detection: Africas Big Five](https://arxiv.org/abs/2506.06719)
*Mufhumudzi Muthivhi,Jiahao Huo,Fredrik Gustafsson,Terence L. van Zyl*

Main category: cs.CV

TL;DR: 本研究针对野生动物分布外检测（OOD），提出基于预训练特征的参数化与非参数化方法，在非洲五大动物分类中验证其优于传统OOD方法。


<details>
  <summary>Details</summary>
Motivation: 现有动物分类模型基于封闭世界假设，对未知类别过度自信。需解决野生动物监测中分布外样本的识别问题，以缓解人兽冲突。

Method: 采用参数化最近类均值（NCM）和非参数化对比学习方法，利用预训练分类编码器特征，并与多种文献OOD方法进行对比。

Result: 基于特征的方法泛化能力更强：ImageNet预训练的NCM在AUPR-IN、AUPR-OUT和AUTC指标上分别提升2%、4%和22%。

Conclusion: 特征驱动方法在野生动物OOD检测中表现优异，验证了预训练特征对开放世界场景的适应性，为实际监测系统提供有效解决方案。

Abstract: Mitigating human-wildlife conflict seeks to resolve unwanted encounters
between these parties. Computer Vision provides a solution to identifying
individuals that might escalate into conflict, such as members of the Big Five
African animals. However, environments often contain several varied species.
The current state-of-the-art animal classification models are trained under a
closed-world assumption. They almost always remain overconfident in their
predictions even when presented with unknown classes. This study investigates
out-of-distribution (OOD) detection of wildlife, specifically the Big Five. To
this end, we select a parametric Nearest Class Mean (NCM) and a non-parametric
contrastive learning approach as baselines to take advantage of pretrained and
projected features from popular classification encoders. Moreover, we compare
our baselines to various common OOD methods in the literature. The results show
feature-based methods reflect stronger generalisation capability across varying
classification thresholds. Specifically, NCM with ImageNet pre-trained features
achieves a 2%, 4% and 22% improvement on AUPR-IN, AUPR-OUT and AUTC over the
best OOD methods, respectively. The code can be found here
https://github.com/pxpana/BIG5OOD

</details>


### [504] [Learning Speaker-Invariant Visual Features for Lipreading](https://arxiv.org/abs/2506.07572)
*Yu Li,Feng Xue,Shujie Li,Jinrui Zhang,Shuang Yang,Dan Guo,Richang Hong*

Main category: cs.CV

TL;DR: 提出SIFLip框架，通过隐式与显式解耦模块分离说话人特定特征，提升唇读模型的泛化能力，实验表明其在多数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有唇读方法提取的视觉特征包含说话人特定属性（如唇形、颜色），导致视觉与文本的虚假相关性，降低准确率并限制模型泛化能力。

Method: SIFLip框架结合隐式解耦（利用稳定文本嵌入监督学习跨说话人通用视觉表征）和显式解耦（通过说话人识别子任务及梯度反转过滤个性化特征）。

Result: 实验表明SIFLip在多个公开数据集上显著提升泛化性能，优于当前最优方法。

Conclusion: 通过解耦说话人特定特征，SIFLip有效提升唇读模型的泛化能力与准确率，验证了跨模态任务中特征分离的重要性。

Abstract: Lipreading is a challenging cross-modal task that aims to convert visual lip
movements into spoken text. Existing lipreading methods often extract visual
features that include speaker-specific lip attributes (e.g., shape, color,
texture), which introduce spurious correlations between vision and text. These
correlations lead to suboptimal lipreading accuracy and restrict model
generalization. To address this challenge, we introduce SIFLip, a
speaker-invariant visual feature learning framework that disentangles
speaker-specific attributes using two complementary disentanglement modules
(Implicit Disentanglement and Explicit Disentanglement) to improve
generalization. Specifically, since different speakers exhibit semantic
consistency between lip movements and phonetic text when pronouncing the same
words, our implicit disentanglement module leverages stable text embeddings as
supervisory signals to learn common visual representations across speakers,
implicitly decoupling speaker-specific features. Additionally, we design a
speaker recognition sub-task within the main lipreading pipeline to filter
speaker-specific features, then further explicitly disentangle these
personalized visual features from the backbone network via gradient reversal.
Experimental results demonstrate that SIFLip significantly enhances
generalization performance across multiple public datasets. Experimental
results demonstrate that SIFLip significantly improves generalization
performance across multiple public datasets, outperforming state-of-the-art
methods.

</details>


### [505] [Mimicking or Reasoning: Rethinking Multi-Modal In-Context Learning in Vision-Language Models](https://arxiv.org/abs/2506.07936)
*Chengyue Huang,Yuchen Zhu,Sichen Zhu,Jingyun Xiao,Moises Andrade,Shivang Chopra,Zsolt Kira*

Main category: cs.CV

TL;DR: 当前视觉语言模型（VLMs）在多模态上下文学习（MM-ICL）中依赖浅层启发式方法（如复制答案），而非真正任务理解。即使引入推理流程，模型仍无法有效利用示例信息。


<details>
  <summary>Details</summary>
Motivation: 验证VLMs是否具备真正的多模态上下文学习能力，而非依赖浅层策略（如复制或多数投票），并探究其在分布变化下的表现。

Method: 提出MM-ICL with Reasoning流程，为每个示例生成答案及推理依据，并在感知与推理任务数据集上测试不同规模的开源/商用模型（3B-72B）。通过控制变量（示例数量、检索方法、理由质量等）进行实验。

Result: 模型性能随示例数量增加而下降，倾向于直接复制答案；不同因素（如理由质量、分布差异）对性能影响有限，表明现有VLMs未能有效利用示例级信息。

Conclusion: 当前VLMs在MM-ICL中未实现预期任务理解能力，其表现主要依赖浅层机制，而非示例内容的深层关联。

Abstract: Vision-language models (VLMs) are widely assumed to exhibit in-context
learning (ICL), a property similar to that of their language-only counterparts.
While recent work suggests VLMs can perform multimodal ICL (MM-ICL), studies
show they often rely on shallow heuristics -- such as copying or majority
voting -- rather than true task understanding. We revisit this assumption by
evaluating VLMs under distribution shifts, where support examples come from a
dataset different from the query. Surprisingly, performance often degrades with
more demonstrations, and models tend to copy answers rather than learn from
them. To investigate further, we propose a new MM-ICL with Reasoning pipeline
that augments each demonstration with a generated rationale alongside the
answer. We conduct extensive and comprehensive experiments on both perception-
and reasoning-required datasets with open-source VLMs ranging from 3B to 72B
and proprietary models such as Gemini 2.0. We conduct controlled studies
varying shot count, retrieval method, rationale quality, and distribution. Our
results show limited performance sensitivity across these factors, suggesting
that current VLMs do not effectively utilize demonstration-level information as
intended in MM-ICL.

</details>


### [506] [Hi-LSplat: Hierarchical 3D Language Gaussian Splatting](https://arxiv.org/abs/2506.06822)
*Chenlu Zhan,Yufei Zhang,Gaoang Wang,Hongwei Wang*

Main category: cs.CV

TL;DR: 本文提出Hi-LSplat方法，通过构建3D层次语义树与对比损失，解决现有3DGS模型在开放词汇查询中的视图不一致与层次语义理解问题，并在实验中展现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于3DGS的模型依赖2D基础模型提取语义特征，导致3D场景中视图不一致性，且开放词汇特性易引发对象与关系描述矛盾，阻碍层次化语义理解。

Method: 通过分层实例聚类构建3D层次语义树，将2D特征提升至3D空间；引入实例级与部件级对比损失，捕获多粒度语义表征；构建专用层次语义数据集进行评测。

Result: 实验表明，Hi-LSplat在3D开放词汇分割与定位任务中表现优异，尤其在层次语义数据集上能有效解析复杂语义层级结构。

Conclusion: Hi-LSplat通过统一3D层次语义表示与对比学习机制，显著提升开放词汇场景下的视图一致性与语义理解能力，为3D场景解析提供新思路。

Abstract: Modeling 3D language fields with Gaussian Splatting for open-ended language
queries has recently garnered increasing attention. However, recent 3DGS-based
models leverage view-dependent 2D foundation models to refine 3D semantics but
lack a unified 3D representation, leading to view inconsistencies.
Additionally, inherent open-vocabulary challenges cause inconsistencies in
object and relational descriptions, impeding hierarchical semantic
understanding. In this paper, we propose Hi-LSplat, a view-consistent
Hierarchical Language Gaussian Splatting work for 3D open-vocabulary querying.
To achieve view-consistent 3D hierarchical semantics, we first lift 2D features
to 3D features by constructing a 3D hierarchical semantic tree with layered
instance clustering, which addresses the view inconsistency issue caused by 2D
semantic features. Besides, we introduce instance-wise and part-wise
contrastive losses to capture all-sided hierarchical semantic representations.
Notably, we construct two hierarchical semantic datasets to better assess the
model's ability to distinguish different semantic levels. Extensive experiments
highlight our method's superiority in 3D open-vocabulary segmentation and
localization. Its strong performance on hierarchical semantic datasets
underscores its ability to capture complex hierarchical semantics within 3D
scenes.

</details>


### [507] [Exploring Visual Prompting: Robustness Inheritance and Beyond](https://arxiv.org/abs/2506.06823)
*Qi Li,Liangzhi Li,Zhouqiang Jiang,Bowen Wang,Keke Tang*

Main category: cs.CV

TL;DR: 本文首次探讨了视觉提示（VP）在鲁棒源模型下的表现，提出了一种名为Prompt Boundary Loosening (PBL)的策略，以缓解VP在鲁棒性和泛化能力之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 先前的研究仅关注标准源模型下的视觉提示（VP），而VP在鲁棒源模型下的表现尚不明确。本文旨在探讨VP是否能继承源模型的鲁棒性，以及是否存在鲁棒性与泛化能力之间的权衡。

Method: 本文提出了一种名为Prompt Boundary Loosening (PBL)的策略，作为一种轻量级、即插即用的方法，PBL与VP自然兼容，旨在确保在源模型为鲁棒模型时成功继承鲁棒性，并显著提升VP在各种下游数据集上的泛化能力。

Result: 通过在不同数据集上的广泛实验，本文证明了所提出策略的普适性，并展示了PBL在继承鲁棒性和提升泛化能力方面的显著优势。

Conclusion: 本文首次全面探讨了VP在鲁棒源模型下的表现，并提出了PBL策略，有效缓解了VP在鲁棒性与泛化能力之间的权衡，为VP的应用提供了新的思路。

Abstract: Visual Prompting (VP), an efficient method for transfer learning, has shown
its potential in vision tasks. However, previous works focus exclusively on VP
from standard source models, it is still unknown how it performs under the
scenario of a robust source model: Can the robustness of the source model be
successfully inherited? Does VP also encounter the same trade-off between
robustness and generalization ability as the source model during this process?
If such a trade-off exists, is there a strategy specifically tailored to VP to
mitigate this limitation? In this paper, we thoroughly explore these three
questions for the first time and provide affirmative answers to them. To
mitigate the trade-off faced by VP, we propose a strategy called Prompt
Boundary Loosening (PBL). As a lightweight, plug-and-play strategy naturally
compatible with VP, PBL effectively ensures the successful inheritance of
robustness when the source model is a robust model, while significantly
enhancing VP's generalization ability across various downstream datasets.
Extensive experiments across various datasets show that our findings are
universal and demonstrate the significant benefits of the proposed strategy.

</details>


### [508] [Play to Generalize: Learning to Reason Through Game Play](https://arxiv.org/abs/2506.08011)
*Yunfei Xie,Yinsong Ma,Shiyi Lan,Alan Yuille,Junfei Xiao,Chen Wei*

Main category: cs.CV

TL;DR: 提出ViGaL后训练范式，通过强化学习让多模态大语言模型玩街机游戏，显著提升跨领域多模态推理能力，且不损失基础视觉性能。


<details>
  <summary>Details</summary>
Motivation: 受认知科学启发，发现游戏能促进可迁移认知技能，故探索通过游戏训练提升MLLMs的泛化推理能力。

Method: 使用强化学习对7B参数MLLM进行后训练，训练任务为简单街机游戏（如贪吃蛇），过程中不接触解题步骤或图表。

Result: 模型在MathVista、MMMU等推理基准上超越专用模型，同时保持基础视觉任务性能，验证了迁移推理能力的获取。

Conclusion: 基于规则的游戏可作为可控、可扩展的预训练任务，有效解锁MLLMs的通用多模态推理能力，为后训练范式提供新方向。

Abstract: Developing generalizable reasoning capabilities in multimodal large language
models (MLLMs) remains challenging. Motivated by cognitive science literature
suggesting that gameplay promotes transferable cognitive skills, we propose a
novel post-training paradigm, Visual Game Learning, or ViGaL, where MLLMs
develop out-of-domain generalization of multimodal reasoning through playing
arcade-like games. Specifically, we show that post-training a 7B-parameter MLLM
via reinforcement learning (RL) on simple arcade-like games, e.g. Snake,
significantly enhances its downstream performance on multimodal math benchmarks
like MathVista, and on multi-discipline questions like MMMU, without seeing any
worked solutions, equations, or diagrams during RL, suggesting the capture of
transferable reasoning skills. Remarkably, our model outperforms specialist
models tuned on multimodal reasoning data in multimodal reasoning benchmarks,
while preserving the base model's performance on general visual benchmarks, a
challenge where specialist models often fall short. Our findings suggest a new
post-training paradigm: synthetic, rule-based games can serve as controllable
and scalable pre-text tasks that unlock generalizable multimodal reasoning
abilities in MLLMs.

</details>


### [509] [Controllable Coupled Image Generation via Diffusion Models](https://arxiv.org/abs/2506.06826)
*Chenfei Yuan,Nanshan Jia,Hangqi Li,Peter W. Glynn,Zeyu Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种用于耦合图像生成任务的注意力控制方法，旨在生成具有相同背景但中心物体不同的图像。


<details>
  <summary>Details</summary>
Motivation: 为了解决在生成多个图像时背景一致但中心物体灵活变化的需求，本文提出了一种新的注意力控制方法。

Method: 该方法通过解耦模型交叉注意力模块中的背景和实体组件，并结合时间变化的权重控制参数来优化生成过程。

Result: 实验结果表明，该方法在背景耦合、文本到图像对齐和整体视觉质量方面优于现有方法。

Conclusion: 本文提出的方法有效解决了耦合图像生成中的背景一致性和物体灵活性问题，具有显著的应用价值。

Abstract: We provide an attention-level control method for the task of coupled image
generation, where "coupled" means that multiple simultaneously generated images
are expected to have the same or very similar backgrounds. While backgrounds
coupled, the centered objects in the generated images are still expected to
enjoy the flexibility raised from different text prompts. The proposed method
disentangles the background and entity components in the model's
cross-attention modules, attached with a sequence of time-varying weight
control parameters depending on the time step of sampling. We optimize this
sequence of weight control parameters with a combined objective that assesses
how coupled the backgrounds are as well as text-to-image alignment and overall
visual quality. Empirical results demonstrate that our method outperforms
existing approaches across these criteria.

</details>


### [510] [EndoARSS: Adapting Spatially-Aware Foundation Model for Efficient Activity Recognition and Semantic Segmentation in Endoscopic Surgery](https://arxiv.org/abs/2506.06830)
*Guankun Wang,Rui Tang,Mengya Xu,Long Bai,Huxin Gao,Hongliang Ren*

Main category: cs.CV

TL;DR: 提出EndoARSS多任务学习框架，结合DINOv2基础模型与低秩自适应技术，解决内窥镜手术场景中活动识别与语义分割的跨任务干扰问题，并通过新数据集验证其性能优势。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在内窥镜手术场景中因跨活动干扰和复杂图像特征导致下游任务性能受限，需通过多任务学习利用任务间关联特征提升整体性能。

Method: 基于DINOv2基础模型，集成低秩自适应（LoRA）和任务高效共享低秩适配器（TES-LoRA）减少梯度冲突，引入空间感知多尺度注意力增强全局特征表征。

Result: 在MTLESD等三个新数据集上验证，EndoARSS在活动识别与语义分割任务中准确率与鲁棒性显著优于现有模型，多基准测试表现突出。

Conclusion: EndoARSS通过多任务协同优化提升了AI内窥镜手术系统的场景理解能力，为增强手术安全性与效率提供了有效技术路径。

Abstract: Endoscopic surgery is the gold standard for robotic-assisted minimally
invasive surgery, offering significant advantages in early disease detection
and precise interventions. However, the complexity of surgical scenes,
characterized by high variability in different surgical activity scenarios and
confused image features between targets and the background, presents challenges
for surgical environment understanding. Traditional deep learning models often
struggle with cross-activity interference, leading to suboptimal performance in
each downstream task. To address this limitation, we explore multi-task
learning, which utilizes the interrelated features between tasks to enhance
overall task performance. In this paper, we propose EndoARSS, a novel
multi-task learning framework specifically designed for endoscopy surgery
activity recognition and semantic segmentation. Built upon the DINOv2
foundation model, our approach integrates Low-Rank Adaptation to facilitate
efficient fine-tuning while incorporating Task Efficient Shared Low-Rank
Adapters to mitigate gradient conflicts across diverse tasks. Additionally, we
introduce the Spatially-Aware Multi-Scale Attention that enhances feature
representation discrimination by enabling cross-spatial learning of global
information. In order to evaluate the effectiveness of our framework, we
present three novel datasets, MTLESD, MTLEndovis and MTLEndovis-Gen, tailored
for endoscopic surgery scenarios with detailed annotations for both activity
recognition and semantic segmentation tasks. Extensive experiments demonstrate
that EndoARSS achieves remarkable performance across multiple benchmarks,
significantly improving both accuracy and robustness in comparison to existing
models. These results underscore the potential of EndoARSS to advance AI-driven
endoscopic surgical systems, offering valuable insights for enhancing surgical
safety and efficiency.

</details>


### [511] [Harnessing Vision-Language Models for Time Series Anomaly Detection](https://arxiv.org/abs/2506.06836)
*Zelin He,Sarah Alnegheimish,Matthew Reimherr*

Main category: cs.CV

TL;DR: 提出基于视觉语言模型（VLM）的两阶段时间序列异常检测方法ViT4TS+VLM4TS，无需时序训练即超越传统方法，效率提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常检测方法依赖数值数据训练，缺乏人类专家的视觉-时序上下文推理能力，且直接应用VLM存在精度与效率不足。

Method: 两阶段框架：1) ViT4TS通过轻量级视觉编码器定位候选异常；2) VLM4TS结合全局时序上下文与VLM推理能力优化检测结果。

Result: VLM4TS在多数场景下F1-max分数较最佳基线提升24.6%，效率比语言模型方法高36倍，且无需时序数据训练。

Conclusion: 融合视觉编码与VLM推理的两阶段方法有效解决了时序异常检测的上下文建模问题，在精度与效率上实现突破。

Abstract: Time-series anomaly detection (TSAD) has played a vital role in a variety of
fields, including healthcare, finance, and industrial monitoring. Prior
methods, which mainly focus on training domain-specific models on numerical
data, lack the visual-temporal reasoning capacity that human experts have to
identify contextual anomalies. To fill this gap, we explore a solution based on
vision language models (VLMs). Recent studies have shown the ability of VLMs
for visual reasoning tasks, yet their direct application to time series has
fallen short on both accuracy and efficiency. To harness the power of VLMs for
TSAD, we propose a two-stage solution, with (1) ViT4TS, a vision-screening
stage built on a relatively lightweight pretrained vision encoder, which
leverages 2-D time-series representations to accurately localize candidate
anomalies; (2) VLM4TS, a VLM-based stage that integrates global temporal
context and VLM reasoning capacity to refine the detection upon the candidates
provided by ViT4TS. We show that without any time-series training, VLM4TS
outperforms time-series pretrained and from-scratch baselines in most cases,
yielding a 24.6 percent improvement in F1-max score over the best baseline.
Moreover, VLM4TS also consistently outperforms existing language-model-based
TSAD methods and is on average 36 times more efficient in token usage.

</details>


### [512] [Position Prediction Self-Supervised Learning for Multimodal Satellite Imagery Semantic Segmentation](https://arxiv.org/abs/2506.06852)
*John Waithaka,Moise Busogi*

Main category: cs.CV

TL;DR: 本文提出了一种基于位置预测的自监督学习方法LOCA，用于多模态卫星图像的语义分割，显著优于现有的基于重建的方法。


<details>
  <summary>Details</summary>
Motivation: 卫星图像的语义分割对地球观测应用至关重要，但受限于有限的标注数据。现有的自监督预训练方法如MAE侧重于重建，而非分割任务中的定位。

Method: 本文扩展了SatMAE的通道分组方法，从多光谱数据扩展到多模态数据，并引入同组注意力掩码以促进预训练期间的跨模态交互，使用相对补丁位置预测来鼓励空间推理。

Result: 在Sen1Floods11洪水映射数据集上的评估表明，该方法显著优于现有的基于重建的自监督学习方法。

Conclusion: 本文证明了位置预测任务在适应多模态卫星图像时，能够学习到比基于重建的方法更有效的表示，适用于卫星图像的语义分割。

Abstract: Semantic segmentation of satellite imagery is crucial for Earth observation
applications, but remains constrained by limited labelled training data. While
self-supervised pretraining methods like Masked Autoencoders (MAE) have shown
promise, they focus on reconstruction rather than localisation-a fundamental
aspect of segmentation tasks. We propose adapting LOCA (Location-aware), a
position prediction self-supervised learning method, for multimodal satellite
imagery semantic segmentation. Our approach addresses the unique challenges of
satellite data by extending SatMAE's channel grouping from multispectral to
multimodal data, enabling effective handling of multiple modalities, and
introducing same-group attention masking to encourage cross-modal interaction
during pretraining. The method uses relative patch position prediction,
encouraging spatial reasoning for localisation rather than reconstruction. We
evaluate our approach on the Sen1Floods11 flood mapping dataset, where it
significantly outperforms existing reconstruction-based self-supervised
learning methods for satellite imagery. Our results demonstrate that position
prediction tasks, when properly adapted for multimodal satellite imagery, learn
representations more effective for satellite image semantic segmentation than
reconstruction-based approaches.

</details>


### [513] [Face recognition on point cloud with cgan-top for denoising](https://arxiv.org/abs/2506.06864)
*Junyu Liu,Jianfeng Ren,Sunhong Liang,Xudong Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种端到端的3D人脸识别方法，通过结合去噪和识别模块，显著提高了在噪声点云下的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 由于传感器不完美，原始点云通常包含大量噪声，影响了3D人脸识别的准确性。本文旨在解决这一问题。

Method: 设计了基于三个正交平面的条件生成对抗网络（cGAN-TOP）来去除点云中的噪声，并采用链接动态图卷积神经网络（LDGCNN）进行人脸识别。

Result: 在Bosphorus数据集上的实验表明，该方法在所有噪声设置下均显著提高了识别准确率，最大增益达14.81%。

Conclusion: 本文提出的方法有效结合了去噪和识别模块，显著提升了3D人脸识别在噪声环境下的性能。

Abstract: Face recognition using 3D point clouds is gaining growing interest, while raw
point clouds often contain a significant amount of noise due to imperfect
sensors. In this paper, an end-to-end 3D face recognition on a noisy point
cloud is proposed, which synergistically integrates the denoising and
recognition modules. Specifically, a Conditional Generative Adversarial Network
on Three Orthogonal Planes (cGAN-TOP) is designed to effectively remove the
noise in the point cloud, and recover the underlying features for subsequent
recognition. A Linked Dynamic Graph Convolutional Neural Network (LDGCNN) is
then adapted to recognize faces from the processed point cloud, which
hierarchically links both the local point features and neighboring features of
multiple scales. The proposed method is validated on the Bosphorus dataset. It
significantly improves the recognition accuracy under all noise settings, with
a maximum gain of 14.81%.

</details>


### [514] [Polar Hierarchical Mamba: Towards Streaming LiDAR Object Detection with Point Clouds as Egocentric Sequences](https://arxiv.org/abs/2506.06944)
*Mellon M. Zhang,Glen Chou,Saibal Mukhopadhyay*

Main category: cs.CV

TL;DR: 本文提出Polar Hierarchical Mamba (PHiM)，一种针对极坐标流式LiDAR设计的状态空间模型架构，通过局部双向Mamba块和全局前向Mamba实现高效时空建模，在Waymo数据集上以两倍吞吐量达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统LiDAR全帧处理方法延迟高，而现有流式方法依赖与极坐标几何不匹配的卷积操作，导致性能下降。基于Mamba的模型虽有效但仅适用于全帧扫描，且存在内存消耗大、不兼容流式处理的问题。

Method: PHiM采用局部双向Mamba块进行扇区内空间编码，全局前向Mamba进行扇区间时序建模，用维度分解操作替代传统卷积和位置编码，实现几何感知的轻量级处理。

Result: 在Waymo Open Dataset上，PHiM以10%优势超越现有最佳流式检测器，吞吐量翻倍的同时达到与全帧基准相当的性能。

Conclusion: PHiM通过Mamba架构与极坐标几何的深度适配，证明了在流式LiDAR处理中替代卷积网络的可行性，为实时感知提供了高效解决方案。

Abstract: Accurate and efficient object detection is essential for autonomous vehicles,
where real-time perception requires low latency and high throughput. LiDAR
sensors provide robust depth information, but conventional methods process full
360{\deg} scans in a single pass, introducing significant delay. Streaming
approaches address this by sequentially processing partial scans in the native
polar coordinate system, yet they rely on translation-invariant convolutions
that are misaligned with polar geometry -- resulting in degraded performance or
requiring complex distortion mitigation. Recent Mamba-based state space models
(SSMs) have shown promise for LiDAR perception, but only in the full-scan
setting, relying on geometric serialization and positional embeddings that are
memory-intensive and ill-suited to streaming. We propose Polar Hierarchical
Mamba (PHiM), a novel SSM architecture designed for polar-coordinate streaming
LiDAR. PHiM uses local bidirectional Mamba blocks for intra-sector spatial
encoding and a global forward Mamba for inter-sector temporal modeling,
replacing convolutions and positional encodings with distortion-aware,
dimensionally-decomposed operations. PHiM sets a new state-of-the-art among
streaming detectors on the Waymo Open Dataset, outperforming the previous best
by 10\% and matching full-scan baselines at twice the throughput. Code will be
available at https://github.com/meilongzhang/Polar-Hierarchical-Mamba .

</details>


### [515] [MAGNET: A Multi-agent Framework for Finding Audio-Visual Needles by Reasoning over Multi-Video Haystacks](https://arxiv.org/abs/2506.07016)
*Sanjoy Chowdhury,Mohamed Elmoghany,Yohan Abeysinghe,Junjie Fei,Sayan Nag,Salman Khan,Mohamed Elhoseiny,Dinesh Manocha*

Main category: cs.CV

TL;DR: 本文提出新任务AV-HaystacksQA及基准AVHaystacks，解决多模态模型在跨视频复杂推理中的不足，并提出多智能体框架MAGNET和评估指标STEM/MTGS，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答基准局限于单片段查询，无法反映实际应用中跨大规模视频的视听检索与复杂推理挑战。

Method: 提出AVHaystacks基准(3100个QA对)和模型无关的多智能体框架MAGNET，设计新评估指标STEM(步骤序列误差)和MTGS(分段定位评分)。

Result: MAGNET在BLEU@4和GPT评分上分别相对提升89%和65%，新指标有效支持多视频检索与时间定位的鲁棒性评估。

Conclusion: 该研究填补了多视频复杂推理任务的空白，提出的框架与指标为大规模视听理解系统的开发提供了新基准和评估体系。

Abstract: Large multimodal models (LMMs) have shown remarkable progress in audio-visual
understanding, yet they struggle with real-world scenarios that require complex
reasoning across extensive video collections. Existing benchmarks for video
question answering remain limited in scope, typically involving one clip per
query, which falls short of representing the challenges of large-scale,
audio-visual retrieval and reasoning encountered in practical applications. To
bridge this gap, we introduce a novel task named AV-HaystacksQA, where the goal
is to identify salient segments across different videos in response to a query
and link them together to generate the most informative answer. To this end, we
present AVHaystacks, an audio-visual benchmark comprising 3100 annotated QA
pairs designed to assess the capabilities of LMMs in multi-video retrieval and
temporal grounding task. Additionally, we propose a model-agnostic, multi-agent
framework MAGNET to address this challenge, achieving up to 89% and 65%
relative improvements over baseline methods on BLEU@4 and GPT evaluation scores
in QA task on our proposed AVHaystacks. To enable robust evaluation of
multi-video retrieval and temporal grounding for optimal response generation,
we introduce two new metrics, STEM, which captures alignment errors between a
ground truth and a predicted step sequence and MTGS, to facilitate balanced and
interpretable evaluation of segment-level grounding performance. Project:
https://schowdhury671.github.io/magnet_project/

</details>


### [516] [Exploring Adversarial Watermarking in Transformer-Based Models: Transferability and Robustness Against Defense Mechanism for Medical Images](https://arxiv.org/abs/2506.06389)
*Rifat Sadik,Tanvir Rahman,Arpan Bhattacharjee,Bikash Chandra Halder,Ismail Hossain*

Main category: cs.CV

TL;DR: 本文研究了基于Vision Transformers（ViTs）的医学图像在对抗性水印攻击下的脆弱性，并探讨了对抗训练作为防御机制的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer模型在计算机视觉任务中的成功，特别是在皮肤图像分析领域，本文旨在探讨ViTs在医学图像处理中对对抗性攻击的敏感性。

Method: 通过使用投影梯度下降（PGD）生成对抗性水印，评估其对ViTs的攻击效果，并分析对抗训练作为防御机制的效果。

Result: 结果表明，ViTs在对抗性攻击下表现脆弱，准确率下降至27.6%，但通过对抗训练，准确率可提升至90.0%。

Conclusion: 尽管ViTs在处理干净图像时表现良好，但其对对抗性攻击的脆弱性显著。对抗训练能有效提升其鲁棒性。

Abstract: Deep learning models have shown remarkable success in dermatological image
analysis, offering potential for automated skin disease diagnosis. Previously,
convolutional neural network(CNN) based architectures have achieved immense
popularity and success in computer vision (CV) based task like skin image
recognition, generation and video analysis. But with the emergence of
transformer based models, CV tasks are now are nowadays carrying out using
these models. Vision Transformers (ViTs) is such a transformer-based models
that have shown success in computer vision. It uses self-attention mechanisms
to achieve state-of-the-art performance across various tasks. However, their
reliance on global attention mechanisms makes them susceptible to adversarial
perturbations. This paper aims to investigate the susceptibility of ViTs for
medical images to adversarial watermarking-a method that adds so-called
imperceptible perturbations in order to fool models. By generating adversarial
watermarks through Projected Gradient Descent (PGD), we examine the
transferability of such attacks to CNNs and analyze the performance defense
mechanism -- adversarial training. Results indicate that while performance is
not compromised for clean images, ViTs certainly become much more vulnerable to
adversarial attacks: an accuracy drop of as low as 27.6%. Nevertheless,
adversarial training raises it up to 90.0%.

</details>


### [517] [Image segmentation and classification of E-waste for waste segregation](https://arxiv.org/abs/2506.07122)
*Prakriti Tripathi,Theertha Biju,Maniram Thota,Rakesh Lingam*

Main category: cs.CV

TL;DR: 使用YOLOv11和Mask-RCNN模型对电子废物进行分类，YOLOv11达到70 mAP，Mask-RCNN达到41 mAP，模型将用于分拣机器人。


<details>
  <summary>Details</summary>
Motivation: 解决电子废物分类问题，以便分拣机器人能够有效进行废物分离。

Method: 通过拆解常见电子废物并拍照创建数据集，训练YOLOv11和Mask-RCNN模型。

Result: YOLOv11模型在实时分类中达到70 mAP，Mask-RCNN模型达到41 mAP。

Conclusion: 模型将进一步集成到分拣机器人中，用于电子废物的实际分类。

Abstract: Industry partners provided a problem statement that involves classifying
electronic waste using machine learning models that will be used by
pick-and-place robots for waste segregation. We started by taking common
electronic waste items, such as a mouse and charger, unsoldering them, and
taking pictures to create a custom dataset. Then state-of-the art YOLOv11 model
was trained and run to achieve 70 mAP in real-time. Mask-RCNN model was also
trained and achieved 41 mAP. The model will be further integrated with
pick-and-place robots to perform segregation of e-waste.

</details>


### [518] [Securing Traffic Sign Recognition Systems in Autonomous Vehicles](https://arxiv.org/abs/2506.06563)
*Thushari Hapuarachchi,Long Dang,Kaiqi Xiong*

Main category: cs.CV

TL;DR: 本文研究了用于交通标志识别的深度神经网络（DNNs）的鲁棒性，提出了基于数据增强的训练方法以抵御误差最小化攻击，并开发了检测模型以识别被污染的数据。


<details>
  <summary>Details</summary>
Motivation: 由于DNNs在交通标志识别中的广泛应用，且其训练数据来源未知，确保模型在训练过程中不被攻击或污染至关重要。

Method: 首先对DNNs进行误差最小化攻击，通过在训练数据中添加不可察觉的扰动；然后提出基于数据增强的训练方法，利用非线性变换破坏扰动并提高模型鲁棒性。

Result: 误差最小化攻击将DNNs的预测准确率从99.90%降至10.6%，而提出的缓解方案成功将准确率恢复至96.05%。检测模型在识别攻击时的成功率超过99%。

Conclusion: 研究表明，在交通标志识别系统中采用先进的训练方法可以有效缓解数据污染攻击的影响。

Abstract: Deep Neural Networks (DNNs) are widely used for traffic sign recognition
because they can automatically extract high-level features from images. These
DNNs are trained on large-scale datasets obtained from unknown sources.
Therefore, it is important to ensure that the models remain secure and are not
compromised or poisoned during training. In this paper, we investigate the
robustness of DNNs trained for traffic sign recognition. First, we perform the
error-minimizing attacks on DNNs used for traffic sign recognition by adding
imperceptible perturbations on training data. Then, we propose a data
augmentation-based training method to mitigate the error-minimizing attacks.
The proposed training method utilizes nonlinear transformations to disrupt the
perturbations and improve the model robustness. We experiment with two
well-known traffic sign datasets to demonstrate the severity of the attack and
the effectiveness of our mitigation scheme. The error-minimizing attacks reduce
the prediction accuracy of the DNNs from 99.90% to 10.6%. However, our
mitigation scheme successfully restores the prediction accuracy to 96.05%.
Moreover, our approach outperforms adversarial training in mitigating the
error-minimizing attacks. Furthermore, we propose a detection model capable of
identifying poisoned data even when the perturbations are imperceptible to
human inspection. Our detection model achieves a success rate of over 99% in
identifying the attack. This research highlights the need to employ advanced
training methods for DNNs in traffic sign recognition systems to mitigate the
effects of data poisoning attacks.

</details>


### [519] [Frame Guidance: Training-Free Guidance for Frame-Level Control in Video Diffusion Models](https://arxiv.org/abs/2506.07177)
*Sangwon Jang,Taekyung Ki,Jaehyeong Jo,Jaehong Yoon,Soo Ye Kim,Zhe Lin,Sung Ju Hwang*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的Frame Guidance方法，通过帧级信号（如关键帧、风格参考图等）实现可控视频生成，显著降低内存消耗并提升全局一致性，兼容多种任务和模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的可控视频生成方法依赖大规模模型微调，随着模型规模增长，其计算和资源成本变得不切实际。

Method: 提出Frame Guidance框架，包含内存优化的潜在处理方法与全局一致性潜在优化策略，支持关键帧引导、风格化等任务，无需额外训练。

Result: 实验表明该方法能生成高质量可控视频，适用于多样化输入信号（如草图、深度图等），且与任意视频模型兼容。

Conclusion: Frame Guidance为无需训练的可控视频生成提供了高效解决方案，在降低资源消耗的同时保持生成内容的全局连贯性。

Abstract: Advancements in diffusion models have significantly improved video quality,
directing attention to fine-grained controllability. However, many existing
methods depend on fine-tuning large-scale video models for specific tasks,
which becomes increasingly impractical as model sizes continue to grow. In this
work, we present Frame Guidance, a training-free guidance for controllable
video generation based on frame-level signals, such as keyframes, style
reference images, sketches, or depth maps. For practical training-free
guidance, we propose a simple latent processing method that dramatically
reduces memory usage, and apply a novel latent optimization strategy designed
for globally coherent video generation. Frame Guidance enables effective
control across diverse tasks, including keyframe guidance, stylization, and
looping, without any training, compatible with any video models. Experimental
results show that Frame Guidance can produce high-quality controlled videos for
a wide range of tasks and input signals.

</details>


### [520] [Flood-DamageSense: Multimodal Mamba with Multitask Learning for Building Flood Damage Assessment using SAR Remote Sensing Imagery](https://arxiv.org/abs/2506.06667)
*Yu-Hsuan Ho,Ali Mostafavi*

Main category: cs.CV

TL;DR: 本文提出首个专用于建筑物级洪水损害评估的深度学习框架Flood-DamageSense，通过融合多源遥感数据与固有风险层，在灾后快速生成高精度损害地图，F1分数较现有方法提升达19%。


<details>
  <summary>Details</summary>
Motivation: 现有灾害损害分类器依赖明显的光谱/结构特征，但洪水损害常缺乏此类特征，导致传统模型在洪水建筑损害识别中表现不佳。

Method: 结合灾前/灾后SAR/InSAR影像、高分辨率光学底图与固有洪水风险层，采用多模态Mamba架构，通过半孪生编码器与任务专用解码器联合预测建筑损害等级、洪水范围及建筑轮廓。

Result: 在哈维飓风数据集上验证，F1分数平均提升19%，在易误判的轻微/中度损害类别改进最显著。消融实验证明固有风险层是性能提升最大贡献因素，端到端流程可在数分钟内生成建筑级损害图。

Conclusion: Flood-DamageSense通过风险感知建模与SAR全天候能力结合，为灾后决策提供更快速、细粒度且可靠的损害评估，支持资源优化调配。

Abstract: Most post-disaster damage classifiers succeed only when destructive forces
leave clear spectral or structural signatures -- conditions rarely present
after inundation. Consequently, existing models perform poorly at identifying
flood-related building damages. The model presented in this study,
Flood-DamageSense, addresses this gap as the first deep-learning framework
purpose-built for building-level flood-damage assessment. The architecture
fuses pre- and post-event SAR/InSAR scenes with very-high-resolution optical
basemaps and an inherent flood-risk layer that encodes long-term exposure
probabilities, guiding the network toward plausibly affected structures even
when compositional change is minimal. A multimodal Mamba backbone with a
semi-Siamese encoder and task-specific decoders jointly predicts (1) graded
building-damage states, (2) floodwater extent, and (3) building footprints.
Training and evaluation on Hurricane Harvey (2017) imagery from Harris County,
Texas -- supported by insurance-derived property-damage extents -- show a mean
F1 improvement of up to 19 percentage points over state-of-the-art baselines,
with the largest gains in the frequently misclassified "minor" and "moderate"
damage categories. Ablation studies identify the inherent-risk feature as the
single most significant contributor to this performance boost. An end-to-end
post-processing pipeline converts pixel-level outputs to actionable,
building-scale damage maps within minutes of image acquisition. By combining
risk-aware modeling with SAR's all-weather capability, Flood-DamageSense
delivers faster, finer-grained, and more reliable flood-damage intelligence to
support post-disaster decision-making and resource allocation.

</details>


### [521] [Interpretation of Deep Learning Model in Embryo Selection for In Vitro Fertilization (IVF) Treatment](https://arxiv.org/abs/2506.06680)
*Radha Kodali,Venkata Rao Dhulipalla,Venkata Siva Kishor Tatavarty,Madhavi Nadakuditi,Bharadwaj Thiruveedhula,Suryanarayana Gunnam,Durga Prasad Bavirisetti*

Main category: cs.CV

TL;DR: 本研究提出了一种结合CNN和LSTM的可解释人工智能框架（CNN-LSTM），用于提高胚胎分类的准确性和效率，并通过XAI保持模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统胚胎分级依赖人工评估，效率低且主观性强。随着低生育率问题加剧，需开发自动化、高精度的胚胎筛选技术以优化体外受精（IVF）流程。

Method: 采用卷积神经网络（CNN）与长短期记忆网络（LSTM）融合架构，结合可解释人工智能（XAI）技术，对囊胚图像进行深度学习建模。

Result: 模型在胚胎分类任务中实现高准确率，同时通过XAI技术提供分类决策的可视化解释，平衡了性能与可解释性。

Conclusion: CNN-LSTM框架有效解决了传统胚胎评估方法的效率瓶颈，为临床胚胎筛选提供了高效、可靠的自动化工具，并增强了AI模型在医疗应用中的可信度。

Abstract: Infertility has a considerable impact on individuals' quality of life,
affecting them socially and psychologically, with projections indicating a rise
in the upcoming years. In vitro fertilization (IVF) emerges as one of the
primary techniques within economically developed nations, employed to address
the rising problem of low fertility. Expert embryologists conventionally grade
embryos by reviewing blastocyst images to select the most optimal for transfer,
yet this process is time-consuming and lacks efficiency. Blastocyst images
provide a valuable resource for assessing embryo viability. In this study, we
introduce an explainable artificial intelligence (XAI) framework for
classifying embryos, employing a fusion of convolutional neural network (CNN)
and long short-term memory (LSTM) architecture, referred to as CNN-LSTM.
Utilizing deep learning, our model achieves high accuracy in embryo
classification while maintaining interpretability through XAI.

</details>


### [522] [Continuous-Time SO(3) Forecasting with Savitzky--Golay Neural Controlled Differential Equations](https://arxiv.org/abs/2506.06780)
*Lennart Bastian,Mohammad Rashed,Nassir Navab,Tolga Birdal*

Main category: cs.CV

TL;DR: 提出一种基于神经控制微分方程和Savitzky-Golay路径的SO(3)旋转动态建模方法，有效解决噪声观测、复杂动态和长期预测挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖简化运动假设，难以处理传感器噪声、复杂旋转模式及长期预测需求，需建立符合旋转几何结构的通用动态模型。

Method: 在SO(3)流形上构建神经控制微分方程，通过Savitzky-Golay路径引导，学习保持旋转几何特性的潜在动态系统。

Result: 真实数据实验表明，该方法在旋转预测任务中显著优于现有方法，尤其在长期预测场景表现突出。

Conclusion: 结合神经微分方程与信号平滑技术，成功建立了几何保持的连续时间旋转动态模型，为复杂SO(3)外推问题提供新解决方案。

Abstract: Tracking and forecasting the rotation of objects is fundamental in computer
vision and robotics, yet SO(3) extrapolation remains challenging as (1) sensor
observations can be noisy and sparse, (2) motion patterns can be governed by
complex dynamics, and (3) application settings can demand long-term
forecasting. This work proposes modeling continuous-time rotational object
dynamics on $SO(3)$ using Neural Controlled Differential Equations guided by
Savitzky-Golay paths. Unlike existing methods that rely on simplified motion
assumptions, our method learns a general latent dynamical system of the
underlying object trajectory while respecting the geometric structure of
rotations. Experimental results on real-world data demonstrate compelling
forecasting capabilities compared to existing approaches.

</details>


### [523] [From Generation to Generalization: Emergent Few-Shot Learning in Video Diffusion Models](https://arxiv.org/abs/2506.07280)
*Pablo Acuaviva,Aram Davtyan,Mariam Hassan,Sebastian Stapf,Ahmad Rahimi,Alexandre Alahi,Paolo Favaro*

Main category: cs.CV

TL;DR: 视频扩散模型（VDMs）不仅是生成高质量视频的工具，还能通过少量样本微调适应新任务，展现出强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索视频扩散模型（VDMs）在生成视频之外的潜力，特别是其内部结构化的视觉世界理解能力。

Method: 提出了一种少样本微调框架，将新任务转化为视觉过渡，通过训练LoRA权重在不改变冻结VDM生成接口的情况下适应新任务。

Result: 模型在从低层次视觉任务（如分割和姿态估计）到高层次推理任务（如ARC-AGI）中表现出强大的泛化能力。

Conclusion: 视频扩散模型不仅是生成引擎，还是适应性强的视觉学习器，有潜力作为未来视觉基础模型的核心。

Abstract: Video Diffusion Models (VDMs) have emerged as powerful generative tools,
capable of synthesizing high-quality spatiotemporal content. Yet, their
potential goes far beyond mere video generation. We argue that the training
dynamics of VDMs, driven by the need to model coherent sequences, naturally
pushes them to internalize structured representations and an implicit
understanding of the visual world. To probe the extent of this internal
knowledge, we introduce a few-shot fine-tuning framework that repurposes VDMs
for new tasks using only a handful of examples. Our method transforms each task
into a visual transition, enabling the training of LoRA weights on short
input-output sequences without altering the generative interface of a frozen
VDM. Despite minimal supervision, the model exhibits strong generalization
across diverse tasks, from low-level vision (for example, segmentation and pose
estimation) to high-level reasoning (for example, on ARC-AGI). These results
reframe VDMs as more than generative engines. They are adaptable visual
learners with the potential to serve as the backbone for future foundation
models in vision.

</details>


### [524] [NSD-Imagery: A benchmark dataset for extending fMRI vision decoding methods to mental imagery](https://arxiv.org/abs/2506.06898)
*Reese Kneeland,Paul S. Scotti,Ghislain St-Yves,Jesse Breedlove,Kendrick Kay,Thomas Naselaris*

Main category: cs.CV

TL;DR: NSD-Imagery数据集发布，用于评估fMRI到心理图像重建模型的性能，发现简单线性解码架构在心理图像重建上表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有NSD数据集仅支持视觉图像重建评估，NSD-Imagery旨在填补心理图像重建的空白，推动医学和脑机接口领域的实际应用。

Method: 基于NSD训练的多种开源视觉解码模型（如MindEye1、MindEye2等），在NSD-Imagery上进行心理图像重建性能评估。

Result: 简单线性解码架构和多模态特征解码在心理图像重建上表现更优，复杂架构容易过拟合视觉训练数据。

Conclusion: 心理图像数据集对实际应用开发至关重要，NSD-Imagery为视觉解码方法的优化提供了重要资源。

Abstract: We release NSD-Imagery, a benchmark dataset of human fMRI activity paired
with mental images, to complement the existing Natural Scenes Dataset (NSD), a
large-scale dataset of fMRI activity paired with seen images that enabled
unprecedented improvements in fMRI-to-image reconstruction efforts. Recent
models trained on NSD have been evaluated only on seen image reconstruction.
Using NSD-Imagery, it is possible to assess how well these models perform on
mental image reconstruction. This is a challenging generalization requirement
because mental images are encoded in human brain activity with relatively lower
signal-to-noise and spatial resolution; however, generalization from seen to
mental imagery is critical for real-world applications in medical domains and
brain-computer interfaces, where the desired information is always internally
generated. We provide benchmarks for a suite of recent NSD-trained open-source
visual decoding models (MindEye1, MindEye2, Brain Diffuser, iCNN, Takagi et
al.) on NSD-Imagery, and show that the performance of decoding methods on
mental images is largely decoupled from performance on vision reconstruction.
We further demonstrate that architectural choices significantly impact
cross-decoding performance: models employing simple linear decoding
architectures and multimodal feature decoding generalize better to mental
imagery, while complex architectures tend to overfit visual training data. Our
findings indicate that mental imagery datasets are critical for the development
of practical applications, and establish NSD-Imagery as a useful resource for
better aligning visual decoding methods with this goal.

</details>


### [525] [Multiple Object Stitching for Unsupervised Representation Learning](https://arxiv.org/abs/2506.07364)
*Chengchao Shen,Dawei Liu,Jianxin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为多对象拼接（MOS）的简单有效方法，用于改进多对象图像的无监督表示，通过拼接单对象图像来构建多对象图像，并在ImageNet、CIFAR和COCO数据集上取得了领先的无监督表示性能。


<details>
  <summary>Details</summary>
Motivation: 现有的对比学习方法在单对象图像上取得了显著进展，但在多对象图像上表现不佳。本文旨在解决这一问题，提供更详细的多对象图像表示。

Method: 本文提出了一种名为多对象拼接（MOS）的方法，通过拼接单对象图像来构建多对象图像，并在这些图像中预定义对象，从而提供额外的对象对应关系，无需人工标注。

Result: 在ImageNet、CIFAR和COCO数据集上的实验结果表明，本文提出的方法在单对象和多对象图像上均取得了领先的无监督表示性能。

Conclusion: 本文提出的MOS方法通过拼接单对象图像来改进多对象图像的无监督表示，为复杂下游任务提供了更详细的表示，实验证明了其有效性。

Abstract: Contrastive learning for single object centric images has achieved remarkable
progress on unsupervised representation, but suffering inferior performance on
the widespread images with multiple objects. In this paper, we propose a simple
but effective method, Multiple Object Stitching (MOS), to refine the
unsupervised representation for multi-object images. Specifically, we construct
the multi-object images by stitching the single object centric ones, where the
objects in the synthesized multi-object images are predetermined. Hence,
compared to the existing contrastive methods, our method provides additional
object correspondences between multi-object images without human annotations.
In this manner, our method pays more attention to the representations of each
object in multi-object image, thus providing more detailed representations for
complicated downstream tasks, such as object detection and semantic
segmentation. Experimental results on ImageNet, CIFAR and COCO datasets
demonstrate that our proposed method achieves the leading unsupervised
representation performance on both single object centric images and
multi-object ones. The source code is available at
https://github.com/visresearch/MultipleObjectStitching.

</details>


### [526] [C3S3: Complementary Competition and Contrastive Selection for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2506.07368)
*Jiaying He,Yitong Lin,Jiahe Chen,Honghui Xu,Jianwei Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种新的半监督医学图像分割模型C3S3，通过结合互补竞争和对比选择，显著提升了边界定位和整体精度。


<details>
  <summary>Details</summary>
Motivation: 当前医学图像分割方法在处理边界细节时存在不足，导致诊断不准确。为了解决这一问题，本文提出了C3S3模型。

Method: C3S3模型集成了结果驱动的对比学习模块和动态互补竞争模块，利用两个高性能子网络生成伪标签，从而提升分割质量。

Result: 在公开的MRI和CT数据集上，C3S3在95HD和ASD指标上至少提升了6%，表现优于现有方法。

Conclusion: C3S3模型在医学图像分割中表现出色，特别是在边界细节处理上取得了显著进展。

Abstract: For the immanent challenge of insufficiently annotated samples in the medical
field, semi-supervised medical image segmentation (SSMIS) offers a promising
solution. Despite achieving impressive results in delineating primary target
areas, most current methodologies struggle to precisely capture the subtle
details of boundaries. This deficiency often leads to significant diagnostic
inaccuracies. To tackle this issue, we introduce C3S3, a novel semi-supervised
segmentation model that synergistically integrates complementary competition
and contrastive selection. This design significantly sharpens boundary
delineation and enhances overall precision. Specifically, we develop an
$\textit{Outcome-Driven Contrastive Learning}$ module dedicated to refining
boundary localization. Additionally, we incorporate a $\textit{Dynamic
Complementary Competition}$ module that leverages two high-performing
sub-networks to generate pseudo-labels, thereby further improving segmentation
quality. The proposed C3S3 undergoes rigorous validation on two publicly
accessible datasets, encompassing the practices of both MRI and CT scans. The
results demonstrate that our method achieves superior performance compared to
previous cutting-edge competitors. Especially, on the 95HD and ASD metrics, our
approach achieves a notable improvement of at least $6\%$, highlighting the
significant advancements. The code is available at
https://github.com/Y-TARL/C3S3.

</details>


### [527] [TABLET: Table Structure Recognition using Encoder-only Transformers](https://arxiv.org/abs/2506.07015)
*Qiyu Hou,Jun Wang*

Main category: cs.CV

TL;DR: 提出了一种基于Split-Merge的表格结构识别模型，通过Transformer编码器优化行列分割与合并，减少计算复杂度，提升处理速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 为了解决大规模、密集表格结构识别中的挑战，提出了一种高效且可扩展的解决方案，适用于工业部署。

Method: 模型将行列分割视为序列标注任务，使用双Transformer编码器捕捉特征交互；合并过程则通过额外的Transformer编码器进行网格单元分类，确保准确性和一致性。

Result: 在FinTabNet和PubTabNet数据集上的实验表明，该模型在真实场景中优于现有方法，具有高准确性和快速处理速度。

Conclusion: 该方法为大规模表格识别提供了鲁棒、可扩展且高效的解决方案，适合工业应用。

Abstract: To address the challenges of table structure recognition, we propose a novel
Split-Merge-based top-down model optimized for large, densely populated tables.
Our approach formulates row and column splitting as sequence labeling tasks,
utilizing dual Transformer encoders to capture feature interactions. The
merging process is framed as a grid cell classification task, leveraging an
additional Transformer encoder to ensure accurate and coherent merging. By
eliminating unstable bounding box predictions, our method reduces resolution
loss and computational complexity, achieving high accuracy while maintaining
fast processing speed. Extensive experiments on FinTabNet and PubTabNet
demonstrate the superiority of our model over existing approaches, particularly
in real-world applications. Our method offers a robust, scalable, and efficient
solution for large-scale table recognition, making it well-suited for
industrial deployment.

</details>


### [528] [D2R: dual regularization loss with collaborative adversarial generation for model robustness](https://arxiv.org/abs/2506.07056)
*Zhenyu Liu,Huizhi Liang,Rajiv Ranjan,Zhanxing Zhu,Vaclav Snasel,Varun Ojha*

Main category: cs.CV

TL;DR: 本文提出了一种双正则化损失（D2R Loss）方法和协作对抗生成（CAG）策略，以增强深度神经网络模型对抗对抗攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在通过损失函数指导目标模型和生成对抗样本方面存在不足，因此需要新的方法来提升模型鲁棒性。

Method: 提出D2R Loss方法，包括对抗分布和干净分布优化，以及CAG策略，通过梯度协作生成对抗样本。

Result: 在CIFAR-10、CIFAR-100、Tiny ImageNet等数据集上的实验表明，D2R Loss与CAG结合能生成高度鲁棒的模型。

Conclusion: D2R Loss与CAG策略有效提升了模型对抗对抗攻击的鲁棒性，实验结果验证了其优越性。

Abstract: The robustness of Deep Neural Network models is crucial for defending models
against adversarial attacks. Recent defense methods have employed collaborative
learning frameworks to enhance model robustness. Two key limitations of
existing methods are (i) insufficient guidance of the target model via loss
functions and (ii) non-collaborative adversarial generation. We, therefore,
propose a dual regularization loss (D2R Loss) method and a collaborative
adversarial generation (CAG) strategy for adversarial training. D2R loss
includes two optimization steps. The adversarial distribution and clean
distribution optimizations enhance the target model's robustness by leveraging
the strengths of different loss functions obtained via a suitable function
space exploration to focus more precisely on the target model's distribution.
CAG generates adversarial samples using a gradient-based collaboration between
guidance and target models. We conducted extensive experiments on three
benchmark databases, including CIFAR-10, CIFAR-100, Tiny ImageNet, and two
popular target models, WideResNet34-10 and PreActResNet18. Our results show
that D2R loss with CAG produces highly robust models.

</details>


### [529] [Adapter Naturally Serves as Decoupler for Cross-Domain Few-Shot Semantic Segmentation](https://arxiv.org/abs/2506.07376)
*Jintao Tong,Ran Ma,Yixiong Zou,Guangyao Chen,Yuhua Li,Ruixuan Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于域特征导航器（DFN）的跨域少样本分割方法，通过结构化解耦器捕获域特定信息，结合SAM-SVN防止过拟合，显著提升了目标域的分割性能。


<details>
  <summary>Details</summary>
Motivation: 跨域少样本分割（CD-FSS）面临域间差异和少样本微调两大挑战。现有方法未充分利用适配器的域信息解耦能力，需探索更有效的解耦机制以提升模型对域无关知识的学习。

Method: 提出域特征导航器（DFN），通过结构化解耦捕获域特定信息，引导模型关注域无关知识；设计SAM-SVN约束DFN避免学习样本特异性知识，并在目标域冻结模型、仅微调DFN以适配目标域。

Result: 实验表明，该方法在1-shot和5-shot场景下分别以2.69%和4.68%的MIoU显著超越现有最优方法。

Conclusion: DFN通过结构化解耦有效缓解域差异，结合SAM-SVN的约束机制，在少样本条件下实现了跨域分割性能的显著提升，验证了域信息解耦策略的优越性。

Abstract: Cross-domain few-shot segmentation (CD-FSS) is proposed to pre-train the
model on a source-domain dataset with sufficient samples, and then transfer the
model to target-domain datasets where only a few samples are available for
efficient fine-tuning. There are majorly two challenges in this task: (1) the
domain gap and (2) fine-tuning with scarce data. To solve these challenges, we
revisit the adapter-based methods, and discover an intriguing insight not
explored in previous works: the adapter not only helps the fine-tuning of
downstream tasks but also naturally serves as a domain information decoupler.
Then, we delve into this finding for an interpretation, and find the model's
inherent structure could lead to a natural decoupling of domain information.
Building upon this insight, we propose the Domain Feature Navigator (DFN),
which is a structure-based decoupler instead of loss-based ones like current
works, to capture domain-specific information, thereby directing the model's
attention towards domain-agnostic knowledge. Moreover, to prevent the potential
excessive overfitting of DFN during the source-domain training, we further
design the SAM-SVN method to constrain DFN from learning sample-specific
knowledge. On target domains, we freeze the model and fine-tune the DFN to
learn target-specific knowledge specific. Extensive experiments demonstrate
that our method surpasses the state-of-the-art method in CD-FSS significantly
by 2.69% and 4.68% MIoU in 1-shot and 5-shot scenarios, respectively.

</details>


### [530] [MrM: Black-Box Membership Inference Attacks against Multimodal RAG Systems](https://arxiv.org/abs/2506.07399)
*Peiru Yang,Jinhua Yin,Haoran Zheng,Xueying Bai,Huili Wang,Yufei Sun,Xintian Li,Shangguang Wang,Yongfeng Huang,Tao Qi*

Main category: cs.CV

TL;DR: 本文提出了一种针对多模态RAG系统的黑盒成员推理攻击框架MrM，通过多目标数据扰动和反事实攻击，成功泄露敏感信息。


<details>
  <summary>Details</summary>
Motivation: 多模态RAG系统在处理敏感信息时存在隐私泄露风险，尤其是成员推理攻击（MIA）。现有方法主要关注文本模态，视觉模态研究较少。

Method: MrM框架采用多目标数据扰动和反事实攻击，通过对象感知的数据扰动和反事实引导的掩码选择策略，增强攻击效果。

Result: 在两个视觉数据集和八个主流视觉语言模型上的实验表明，MrM在样本级和集合级评估中均表现出色，且在自适应防御下仍保持鲁棒性。

Conclusion: MrM框架有效填补了多模态RAG系统在视觉模态上的隐私保护研究空白，展示了其在成员推理攻击中的强大性能。

Abstract: Multimodal retrieval-augmented generation (RAG) systems enhance large
vision-language models by integrating cross-modal knowledge, enabling their
increasing adoption across real-world multimodal tasks. These knowledge
databases may contain sensitive information that requires privacy protection.
However, multimodal RAG systems inherently grant external users indirect access
to such data, making them potentially vulnerable to privacy attacks,
particularly membership inference attacks (MIAs). % Existing MIA methods
targeting RAG systems predominantly focus on the textual modality, while the
visual modality remains relatively underexplored. To bridge this gap, we
propose MrM, the first black-box MIA framework targeted at multimodal RAG
systems. It utilizes a multi-object data perturbation framework constrained by
counterfactual attacks, which can concurrently induce the RAG systems to
retrieve the target data and generate information that leaks the membership
information. Our method first employs an object-aware data perturbation method
to constrain the perturbation to key semantics and ensure successful retrieval.
Building on this, we design a counterfact-informed mask selection strategy to
prioritize the most informative masked regions, aiming to eliminate the
interference of model self-knowledge and amplify attack efficacy. Finally, we
perform statistical membership inference by modeling query trials to extract
features that reflect the reconstruction of masked semantics from response
patterns. Experiments on two visual datasets and eight mainstream commercial
visual-language models (e.g., GPT-4o, Gemini-2) demonstrate that MrM achieves
consistently strong performance across both sample-level and set-level
evaluations, and remains robust under adaptive defenses.

</details>


### [531] [FAMSeg: Fetal Femur and Cranial Ultrasound Segmentation Using Feature-Aware Attention and Mamba Enhancement](https://arxiv.org/abs/2506.07431)
*Jie He,Minglang Chen,Minying Lu,Bocheng Liang,Junming Wei,Guiyan Peng,Jiaxi Chen,Ying Tan*

Main category: cs.CV

TL;DR: 本文提出了一种基于特征感知和Mamba增强的胎儿超声图像分割模型FAMSeg，通过独立视角扫描卷积和特征感知模块提升局部细节捕捉能力，结合Mamba优化残差结构抑制噪声干扰，在多种尺寸和方向的图像中实现了最优分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有自然场景分割模型难以适应高噪声、高相似性的超声图像，尤其在小目标分割时会产生锯齿效应，而人工标注存在效率低、误差大的问题。

Method: 设计纵向横向独立视角扫描卷积块增强局部细节捕捉，结合特征感知模块优化上下文融合；采用Mamba优化的残差结构进行多维度局部扫描，通过混合优化器训练建立全局-局部特征依赖关系。

Result: FAMSeg网络在实验中表现出最快的损失收敛速度，在不同尺寸和方向的超声图像上均取得最佳分割效果。

Conclusion: 该模型有效抑制原始噪声干扰，通过特征感知与Mamba增强机制显著提升了胎儿股骨/颅脑超声图像的分割精度和鲁棒性。

Abstract: Accurate ultrasound image segmentation is a prerequisite for precise
biometrics and accurate assessment. Relying on manual delineation introduces
significant errors and is time-consuming. However, existing segmentation models
are designed based on objects in natural scenes, making them difficult to adapt
to ultrasound objects with high noise and high similarity. This is particularly
evident in small object segmentation, where a pronounced jagged effect occurs.
Therefore, this paper proposes a fetal femur and cranial ultrasound image
segmentation model based on feature perception and Mamba enhancement to address
these challenges. Specifically, a longitudinal and transverse independent
viewpoint scanning convolution block and a feature perception module were
designed to enhance the ability to capture local detail information and improve
the fusion of contextual information. Combined with the Mamba-optimized
residual structure, this design suppresses the interference of raw noise and
enhances local multi-dimensional scanning. The system builds global information
and local feature dependencies, and is trained with a combination of different
optimizers to achieve the optimal solution. After extensive experimental
validation, the FAMSeg network achieved the fastest loss reduction and the best
segmentation performance across images of varying sizes and orientations.

</details>


### [532] [Multi-Step Guided Diffusion for Image Restoration on Edge Devices: Toward Lightweight Perception in Embodied AI](https://arxiv.org/abs/2506.07286)
*Aditya Chakravarty*

Main category: cs.CV

TL;DR: 本文提出在扩散模型的每个去噪步骤中采用多步优化策略，显著提升图像恢复质量与泛化能力，验证了其作为轻量级实时修复模块在无人机等嵌入式AI设备上的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有方法如MPGD在单次梯度更新下存在恢复保真度不足、鲁棒性受限的问题，尤其在嵌入式设备或分布外场景中表现欠佳。

Method: 在去噪过程的每个时间步内引入多步梯度更新策略，通过增加优化迭代次数提升重建精度。

Result: 超分辨率和去模糊任务中LPIPS/PSNR指标显著提升，在Jetson Orin Nano平台验证了跨领域(自然场景/无人机数据集)的泛化能力，推理延迟仅微量增加。

Conclusion: MPGD可作为即插即用的轻量级修复模块，为无人机/移动机器人等具身智能体提供实时视觉感知支持，其多步优化机制具有普适性。

Abstract: Diffusion models have shown remarkable flexibility for solving inverse
problems without task-specific retraining. However, existing approaches such as
Manifold Preserving Guided Diffusion (MPGD) apply only a single gradient update
per denoising step, limiting restoration fidelity and robustness, especially in
embedded or out-of-distribution settings. In this work, we introduce a
multistep optimization strategy within each denoising timestep, significantly
enhancing image quality, perceptual accuracy, and generalization. Our
experiments on super-resolution and Gaussian deblurring demonstrate that
increasing the number of gradient updates per step improves LPIPS and PSNR with
minimal latency overhead. Notably, we validate this approach on a Jetson Orin
Nano using degraded ImageNet and a UAV dataset, showing that MPGD, originally
trained on face datasets, generalizes effectively to natural and aerial scenes.
Our findings highlight MPGD's potential as a lightweight, plug-and-play
restoration module for real-time visual perception in embodied AI agents such
as drones and mobile robots.

</details>


### [533] [Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition](https://arxiv.org/abs/2506.07436)
*Nishi Chaudhary,S M Jamil Uddin,Sathvik Sharath Chandra,Anto Ovid,Alex Albert*

Main category: cs.CV

TL;DR: 本文比较了五种先进的多模态大语言模型在建筑工地视觉危险识别中的表现，发现提示策略对性能有显著影响，尤其是链式思维提示能提高准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型在视觉任务中展现出潜力，但它们在建筑安全关键任务中的表现尚未得到充分研究。本文旨在填补这一空白。

Method: 研究对Claude-3 Opus、GPT-4.5、GPT-4o、GPT-o3和Gemini 2.0 Pro五种模型进行了比较评估，采用零样本、少样本和链式思维三种提示策略，并使用精确率、召回率和F1分数进行定量分析。

Result: 结果显示，链式思维提示策略在所有模型中均能提高准确性，GPT-4.5和GPT-o3在大多数情况下表现最佳。

Conclusion: 提示设计在多模态大语言模型的应用中至关重要，本研究为建筑安全领域的AI辅助系统开发提供了实用见解。

Abstract: The recent emergence of multimodal large language models (LLMs) has
introduced new opportunities for improving visual hazard recognition on
construction sites. Unlike traditional computer vision models that rely on
domain-specific training and extensive datasets, modern LLMs can interpret and
describe complex visual scenes using simple natural language prompts. However,
despite growing interest in their applications, there has been limited
investigation into how different LLMs perform in safety-critical visual tasks
within the construction domain. To address this gap, this study conducts a
comparative evaluation of five state-of-the-art LLMs: Claude-3 Opus, GPT-4.5,
GPT-4o, GPT-o3, and Gemini 2.0 Pro, to assess their ability to identify
potential hazards from real-world construction images. Each model was tested
under three prompting strategies: zero-shot, few-shot, and chain-of-thought
(CoT). Zero-shot prompting involved minimal instruction, few-shot incorporated
basic safety context and a hazard source mnemonic, and CoT provided
step-by-step reasoning examples to scaffold model thinking. Quantitative
analysis was performed using precision, recall, and F1-score metrics across all
conditions. Results reveal that prompting strategy significantly influenced
performance, with CoT prompting consistently producing higher accuracy across
models. Additionally, LLM performance varied under different conditions, with
GPT-4.5 and GPT-o3 outperforming others in most settings. The findings also
demonstrate the critical role of prompt design in enhancing the accuracy and
consistency of multimodal LLMs for construction safety applications. This study
offers actionable insights into the integration of prompt engineering and LLMs
for practical hazard recognition, contributing to the development of more
reliable AI-assisted safety systems.

</details>


### [534] ["CASE: Contrastive Activation for Saliency Estimation](https://arxiv.org/abs/2506.07327)
*Dane Williamson,Yangfeng Ji,Matthew Dwyer*

Main category: cs.CV

TL;DR: 本文提出了一种诊断测试，评估显著性方法在区分同一输入的不同类别标签上的能力，并发现许多方法在不同类别下生成几乎相同的解释。为此，作者提出了CASE方法，生成更忠实且类别特定的解释。


<details>
  <summary>Details</summary>
Motivation: 显著性方法广泛用于可视化模型预测中哪些输入特征被认为是相关的，但其视觉上的合理性可能掩盖了关键局限性。本文旨在评估这些方法在区分不同类别标签上的能力。

Method: 本文提出了一种诊断测试，评估显著性方法的类别敏感性，并通过实验发现许多方法在不同类别下生成几乎相同的解释。基于此，作者提出了CASE方法，通过对比性解释来隔离对预测类别唯一具有区分性的特征。

Result: 实验表明，许多广泛使用的显著性方法在不同类别下生成几乎相同的解释，表明其类别不敏感性是结构性的而非模型特定的。CASE方法在诊断测试和基于扰动的保真度测试中表现优于现有方法。

Conclusion: 本文揭示了显著性方法在类别区分上的局限性，并提出了CASE方法，生成更忠实且类别特定的解释，为显著性方法的研究提供了新的方向。

Abstract: Saliency methods are widely used to visualize which input features are deemed
relevant to a model's prediction. However, their visual plausibility can
obscure critical limitations. In this work, we propose a diagnostic test for
class sensitivity: a method's ability to distinguish between competing class
labels on the same input. Through extensive experiments, we show that many
widely used saliency methods produce nearly identical explanations regardless
of the class label, calling into question their reliability. We find that
class-insensitive behavior persists across architectures and datasets,
suggesting the failure mode is structural rather than model-specific. Motivated
by these findings, we introduce CASE, a contrastive explanation method that
isolates features uniquely discriminative for the predicted class. We evaluate
CASE using the proposed diagnostic and a perturbation-based fidelity test, and
show that it produces faithful and more class-specific explanations than
existing methods.

</details>


### [535] [DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO](https://arxiv.org/abs/2506.07464)
*Jinyoung Park,Jeehye Na,Jinyoung Kim,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: 本文探讨了GRPO在视频大语言模型中的应用，提出了Reg-GRPO和难度感知数据增强策略，显著提升了视频推理性能。


<details>
  <summary>Details</summary>
Motivation: 尽管GRPO在增强大语言模型推理能力方面表现出色，但其在视频大语言模型中的应用研究较少。本文旨在解决GRPO在视频大语言模型中的两个主要问题：对安全机制的依赖和优势消失问题。

Method: 提出了Reg-GRPO，将GRPO目标重新定义为回归任务，直接预测GRPO中的优势值，并设计了难度感知数据增强策略，动态增强可解决难度水平的训练样本。

Result: 实验表明，DeepVideo-R1在多个视频推理基准测试中显著提升了视频推理性能。

Conclusion: 通过Reg-GRPO和难度感知数据增强策略，DeepVideo-R1有效解决了GRPO在视频大语言模型中的学习问题，显著提升了模型性能。

Abstract: Recent works have demonstrated the effectiveness of reinforcement learning
(RL)-based post-training in enhancing the reasoning capabilities of large
language models (LLMs). In particular, Group Relative Policy Optimization
(GRPO) has shown impressive success by employing a PPO-style reinforcement
algorithm with group-based normalized rewards. However, the application of GRPO
to Video Large Language Models (Video LLMs) has been less studied. In this
paper, we explore GRPO for video LLMs and identify two primary issues that
impede its effective learning: (1) reliance on safeguards, and (2) the
vanishing advantage problem. To mitigate these challenges, we propose
DeepVideo-R1, a video large language model trained with our proposed Reg-GRPO
(Regressive GRPO) and difficulty-aware data augmentation strategy. Reg-GRPO
reformulates the GRPO objective as a regression task, directly predicting the
advantage in GRPO. This design eliminates the need for safeguards like clipping
and min functions, thereby facilitating more direct policy guidance by aligning
the model with the advantage values. We also design the difficulty-aware data
augmentation strategy that dynamically augments training samples at solvable
difficulty levels, fostering diverse and informative reward signals. Our
comprehensive experiments show that DeepVideo-R1 significantly improves video
reasoning performance across multiple video reasoning benchmarks.

</details>


### [536] [CBAM-STN-TPS-YOLO: Enhancing Agricultural Object Detection through Spatially Adaptive Attention Mechanisms](https://arxiv.org/abs/2506.07357)
*Satvik Praveen,Yoonsung Jung*

Main category: cs.CV

TL;DR: 提出结合TPS的STN与CBAM的改进YOLO模型，通过柔性空间变换和注意力机制提升农业场景目标检测精度，在遮挡严重的PGP数据集上实现误检率降低12%，支持轻量化边缘部署。


<details>
  <summary>Details</summary>
Motivation: 现有YOLO模型在农业植物监测中存在遮挡物、非刚性形变（如弯曲叶片）和背景噪声导致的检测精度下降问题，传统STN的仿射变换无法有效处理非刚性空间对齐需求。

Method: CBAM-STN-TPS-YOLO模型：1) 在STN中集成薄板样条(TPS)实现非刚性空间变换；2) 通过卷积注意力模块(CBAM)抑制背景噪声并强化空间/通道特征；3) 研究TPS正则化参数对形变平滑度与检测性能的平衡机制。

Result: 在含严重遮挡的PGP数据集上：1) 精确率、召回率、mAP全面超越STN-YOLO；2) 误检率降低12%；3) 验证了空间柔性变换与注意力机制协同优化的有效性。

Conclusion: 该轻量级模型通过TPS增强空间适应能力与CBAM特征优化，兼顾检测精度与实时性，为需要精准高效监测的智慧农业场景提供了可行解决方案。

Abstract: Object detection is vital in precision agriculture for plant monitoring,
disease detection, and yield estimation. However, models like YOLO struggle
with occlusions, irregular structures, and background noise, reducing detection
accuracy. While Spatial Transformer Networks (STNs) improve spatial invariance
through learned transformations, affine mappings are insufficient for non-rigid
deformations such as bent leaves and overlaps.
  We propose CBAM-STN-TPS-YOLO, a model integrating Thin-Plate Splines (TPS)
into STNs for flexible, non-rigid spatial transformations that better align
features. Performance is further enhanced by the Convolutional Block Attention
Module (CBAM), which suppresses background noise and emphasizes relevant
spatial and channel-wise features.
  On the occlusion-heavy Plant Growth and Phenotyping (PGP) dataset, our model
outperforms STN-YOLO in precision, recall, and mAP. It achieves a 12% reduction
in false positives, highlighting the benefits of improved spatial flexibility
and attention-guided refinement. We also examine the impact of the TPS
regularization parameter in balancing transformation smoothness and detection
performance.
  This lightweight model improves spatial awareness and supports real-time edge
deployment, making it ideal for smart farming applications requiring accurate
and efficient monitoring.

</details>


### [537] [Ambiguity-Restrained Text-Video Representation Learning for Partially Relevant Video Retrieval](https://arxiv.org/abs/2506.07471)
*CH Cho,WJ Moon,W Jun,MS Jung,JP Heo*

Main category: cs.CV

TL;DR: 本文提出了一种处理部分相关视频检索（PRVR）中文本-视频对模糊性的框架ARL，通过多标准检测模糊对、多层次对比学习及跨模型检测，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统PRVR训练假设文本查询与视频一一对应，但实际存在文本与视频内容的固有模糊性（如概念范围差异）。本文旨在将这种模糊性纳入模型学习过程。

Method: 提出ARL框架：1) 基于不确定性和相似性检测模糊文本-视频对；2) 通过多正面对比学习和双三重边际损失分层学习语义关系；3) 探索视频内部帧级细粒度关系；4) 引入跨模型模糊检测以减少单模型误差传播。

Result: 结合上述方法后，所提框架在PRVR任务中表现出有效性，验证了模糊性处理对模型性能的提升作用。

Conclusion: 通过显式建模文本-视频对的模糊性，并结合跨模型检测与细粒度学习，ARL框架显著改善了PRVR的检索效果，证明了处理多义性对视频理解的重要性。

Abstract: Partially Relevant Video Retrieval~(PRVR) aims to retrieve a video where a
specific segment is relevant to a given text query. Typical training processes
of PRVR assume a one-to-one relationship where each text query is relevant to
only one video. However, we point out the inherent ambiguity between text and
video content based on their conceptual scope and propose a framework that
incorporates this ambiguity into the model learning process. Specifically, we
propose Ambiguity-Restrained representation Learning~(ARL) to address ambiguous
text-video pairs. Initially, ARL detects ambiguous pairs based on two criteria:
uncertainty and similarity. Uncertainty represents whether instances include
commonly shared context across the dataset, while similarity indicates
pair-wise semantic overlap. Then, with the detected ambiguous pairs, our ARL
hierarchically learns the semantic relationship via multi-positive contrastive
learning and dual triplet margin loss. Additionally, we delve into fine-grained
relationships within the video instances. Unlike typical training at the
text-video level, where pairwise information is provided, we address the
inherent ambiguity within frames of the same untrimmed video, which often
contains multiple contexts. This allows us to further enhance learning at the
text-frame level. Lastly, we propose cross-model ambiguity detection to
mitigate the error propagation that occurs when a single model is employed to
detect ambiguous pairs for its training. With all components combined, our
proposed method demonstrates its effectiveness in PRVR.

</details>


### [538] [CoCoA-Mix: Confusion-and-Confidence-Aware Mixture Model for Context Optimization](https://arxiv.org/abs/2506.07484)
*Dasol Hong,Wooju Lee,Hyun Myung*

Main category: cs.CV

TL;DR: 本文提出CoCoA-Mix方法，通过混淆感知损失（CoA-loss）和置信感知权重（CoA-weights）优化视觉语言模型的提示调优，提升任务专业化与跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有提示调优方法因冻结编码器导致特征错位，引发类别混淆，限制了模型在特定任务上的专业化能力。

Method: 结合CoA-loss细化混淆类别的决策边界，并设计基于置信度的CoA-weights混合模型，平衡泛化与专业化。

Result: 实验表明CoCoA-Mix在专业化和泛化性能上优于现有方法，代码已开源。

Conclusion: 通过混淆感知机制与混合模型设计，CoCoA-Mix有效解决了提示调优中专业化与泛化的权衡问题。

Abstract: Prompt tuning, which adapts vision-language models by freezing model
parameters and optimizing only the prompt, has proven effective for
task-specific adaptations. The core challenge in prompt tuning is improving
specialization for a specific task and generalization for unseen domains.
However, frozen encoders often produce misaligned features, leading to
confusion between classes and limiting specialization. To overcome this issue,
we propose a confusion-aware loss (CoA-loss) that improves specialization by
refining the decision boundaries between confusing classes. Additionally, we
mathematically demonstrate that a mixture model can enhance generalization
without compromising specialization. This is achieved using confidence-aware
weights (CoA-weights), which adjust the weights of each prediction in the
mixture model based on its confidence within the class domains. Extensive
experiments show that CoCoA-Mix, a mixture model with CoA-loss and CoA-weights,
outperforms state-of-the-art methods by enhancing specialization and
generalization. Our code is publicly available at
https://github.com/url-kaist/CoCoA-Mix.

</details>


### [539] [Domain Randomization for Object Detection in Manufacturing Applications using Synthetic Data: A Comprehensive Study](https://arxiv.org/abs/2506.07539)
*Xiaomeng Zhu,Jacob Henningsson,Duruo Li,Pär Mårtensson,Lars Hanson,Mårten Björkman,Atsuto Maki*

Main category: cs.CV

TL;DR: 本文探讨了在制造业物体检测应用中生成合成数据的关键方面，提出了一个综合数据生成管道，并引入了SIP15-OD数据集。实验结果表明，该方法在公开数据集上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 本文旨在通过域随机化生成合成数据，以解决制造业物体检测应用中的数据需求问题，并探索从模拟到真实物体检测的可行性。

Method: 本文提出了一个综合数据生成管道，考虑了物体特性、背景、光照、相机设置和后处理等因素，并引入了SIP15-OD数据集作为测试平台。

Result: 实验结果显示，使用Yolov8模型在合成数据上训练后，在公开数据集上取得了mAP@50分别为96.4%、94.1%、99.5%和95.3%的优异性能。

Conclusion: 本文提出的域随机化方法有效覆盖了接近真实数据的分布，展示了其在制造业物体检测应用中的潜力。

Abstract: This paper addresses key aspects of domain randomization in generating
synthetic data for manufacturing object detection applications. To this end, we
present a comprehensive data generation pipeline that reflects different
factors: object characteristics, background, illumination, camera settings, and
post-processing. We also introduce the Synthetic Industrial Parts Object
Detection dataset (SIP15-OD) consisting of 15 objects from three industrial use
cases under varying environments as a test bed for the study, while also
employing an industrial dataset publicly available for robotic applications. In
our experiments, we present more abundant results and insights into the
feasibility as well as challenges of sim-to-real object detection. In
particular, we identified material properties, rendering methods,
post-processing, and distractors as important factors. Our method, leveraging
these, achieves top performance on the public dataset with Yolov8 models
trained exclusively on synthetic data; mAP@50 scores of 96.4% for the robotics
dataset, and 94.1%, 99.5%, and 95.3% across three of the SIP15-OD use cases,
respectively. The results showcase the effectiveness of the proposed domain
randomization, potentially covering the distribution close to real data for the
applications.

</details>


### [540] [APTOS-2024 challenge report: Generation of synthetic 3D OCT images from fundus photographs](https://arxiv.org/abs/2506.07542)
*Bowen Liu,Weiyi Zhang,Peranut Chotcomwongse,Xiaolan Chen,Ruoyu Chen,Pawin Pakaymaskul,Niracha Arjkongharn,Nattaporn Vongsa,Xuelian Cheng,Zongyuan Ge,Kun Huang,Xiaohui Li,Yiru Duan,Zhenbang Wang,BaoYe Xie,Qiang Chen,Huazhu Fu,Michael A. Mahr,Jiaqi Qu,Wangyiyang Chen,Shiye Wang,Yubo Tan,Yongjie Li,Mingguang He,Danli Shi,Paisan Ruamviboonsuk*

Main category: cs.CV

TL;DR: APTOS-2024挑战赛首次验证了从2D眼底图像生成3D OCT图像的可行性，通过342个团队的创新方法（如跨模态协作、预训练模型整合），为解决医疗资源不足地区的眼科诊疗可及性提供了潜在方案。


<details>
  <summary>Details</summary>
Motivation: OCT虽能高分辨率3D成像，但设备昂贵且依赖专业操作；2D眼底摄影更易获取但缺乏深度信息。研究旨在通过生成式AI突破跨模态维度差异，实现低成本OCT替代方案。

Method: 构建基准数据集的双重评估体系（像素级B-scan相似度+语义级体积一致性），采用混合数据预处理、外部眼科数据集预训练、视觉基础模型集成及模型架构优化等方法。

Result: 42个初步方案中9个进入决赛，领先方案通过跨模态协作范式提升生成质量，验证了2D-to-3D跨模态合成的技术可行性。

Conclusion: 该挑战赛为资源匮乏地区提供了新型眼科诊断工具开发路径，同时加速了医学影像合成技术在临床研究中的应用进程。

Abstract: Optical Coherence Tomography (OCT) provides high-resolution, 3D, and
non-invasive visualization of retinal layers in vivo, serving as a critical
tool for lesion localization and disease diagnosis. However, its widespread
adoption is limited by equipment costs and the need for specialized operators.
In comparison, 2D color fundus photography offers faster acquisition and
greater accessibility with less dependence on expensive devices. Although
generative artificial intelligence has demonstrated promising results in
medical image synthesis, translating 2D fundus images into 3D OCT images
presents unique challenges due to inherent differences in data dimensionality
and biological information between modalities. To advance generative models in
the fundus-to-3D-OCT setting, the Asia Pacific Tele-Ophthalmology Society
(APTOS-2024) organized a challenge titled Artificial Intelligence-based OCT
Generation from Fundus Images. This paper details the challenge framework
(referred to as APTOS-2024 Challenge), including: the benchmark dataset,
evaluation methodology featuring two fidelity metrics-image-based distance
(pixel-level OCT B-scan similarity) and video-based distance (semantic-level
volumetric consistency), and analysis of top-performing solutions. The
challenge attracted 342 participating teams, with 42 preliminary submissions
and 9 finalists. Leading methodologies incorporated innovations in hybrid data
preprocessing or augmentation (cross-modality collaborative paradigms),
pre-training on external ophthalmic imaging datasets, integration of vision
foundation models, and model architecture improvement. The APTOS-2024 Challenge
is the first benchmark demonstrating the feasibility of fundus-to-3D-OCT
synthesis as a potential solution for improving ophthalmic care accessibility
in under-resourced healthcare settings, while helping to expedite medical
research and clinical applications.

</details>


### [541] [Synthesize Privacy-Preserving High-Resolution Images via Private Textual Intermediaries](https://arxiv.org/abs/2506.07555)
*Haoxiang Wang,Zinan Lin,Da Yu,Huishuai Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为SPTI的新方法，通过将差分隐私图像合成的挑战从图像域转移到文本域，生成高分辨率差分隐私图像，显著提高了图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有的差分隐私图像合成方法难以生成高分辨率且忠实于原始数据结构的图像，限制了敏感视觉数据的共享与分析。

Method: SPTI方法首先将每张私有图像通过图像到文本模型转换为简洁的文本描述，然后应用改进的Private Evolution算法生成差分隐私文本，最后通过文本到图像模型重建图像。

Result: 在LSUN Bedroom数据集上，SPTI在ε=1.0时FID≤26.71，优于Private Evolution的40.36；在MM CelebA HQ数据集上，SPTI在ε=1.0时FID≤33.27，优于DP微调基线的57.01。

Conclusion: SPTI提供了一种资源高效且兼容专有模型的框架，用于生成高分辨率差分隐私合成图像，极大地扩展了对私有视觉数据集的访问。

Abstract: Generating high fidelity, differentially private (DP) synthetic images offers
a promising route to share and analyze sensitive visual data without
compromising individual privacy. However, existing DP image synthesis methods
struggle to produce high resolution outputs that faithfully capture the
structure of the original data. In this paper, we introduce a novel method,
referred to as Synthesis via Private Textual Intermediaries (SPTI), that can
generate high resolution DP images with easy adoption. The key idea is to shift
the challenge of DP image synthesis from the image domain to the text domain by
leveraging state of the art DP text generation methods. SPTI first summarizes
each private image into a concise textual description using image to text
models, then applies a modified Private Evolution algorithm to generate DP
text, and finally reconstructs images using text to image models. Notably, SPTI
requires no model training, only inference with off the shelf models. Given a
private dataset, SPTI produces synthetic images of substantially higher quality
than prior DP approaches. On the LSUN Bedroom dataset, SPTI attains an FID less
than or equal to 26.71 under epsilon equal to 1.0, improving over Private
Evolution FID of 40.36. Similarly, on MM CelebA HQ, SPTI achieves an FID less
than or equal to 33.27 at epsilon equal to 1.0, compared to 57.01 from DP fine
tuning baselines. Overall, our results demonstrate that Synthesis via Private
Textual Intermediaries provides a resource efficient and proprietary model
compatible framework for generating high resolution DP synthetic images,
greatly expanding access to private visual datasets.

</details>


### [542] [Uncertainty-o: One Model-agnostic Framework for Unveiling Uncertainty in Large Multimodal Models](https://arxiv.org/abs/2506.07575)
*Ruiyang Zhang,Hu Zhang,Hao Fei,Zhedong Zheng*

Main category: cs.CV

TL;DR: 本文提出Uncertainty-o框架，用于统一评估大型多模态模型（LMMs）的不确定性，并通过多模态提示扰动和语义不确定性量化方法，提升下游任务如幻觉检测和推理的可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管LMMs被认为比纯语言模型更鲁棒，但其不确定性评估仍存在三个未解问题：如何统一评估不同LMMs的不确定性、如何通过提示激发其不确定性表达，以及如何量化不确定性以支持下游任务。

Method: 提出模型无关框架Uncertainty-o，通过多模态提示扰动揭示LMMs的不确定性，并推导多模态语义不确定性公式，以量化多模态响应中的不确定性。

Result: 在涵盖多种模态的18个基准测试和10个开源/闭源LMMs上的实验表明，Uncertainty-o能可靠估计LMMs的不确定性，有效提升幻觉检测、缓解及不确定性感知思维链推理等任务性能。

Conclusion: Uncertainty-o为LMMs的不确定性评估提供了统一框架，其多模态语义不确定性量化方法显著增强了下游任务的可靠性和模型透明度。

Abstract: Large Multimodal Models (LMMs), harnessing the complementarity among diverse
modalities, are often considered more robust than pure Language Large Models
(LLMs); yet do LMMs know what they do not know? There are three key open
questions remaining: (1) how to evaluate the uncertainty of diverse LMMs in a
unified manner, (2) how to prompt LMMs to show its uncertainty, and (3) how to
quantify uncertainty for downstream tasks. In an attempt to address these
challenges, we introduce Uncertainty-o: (1) a model-agnostic framework designed
to reveal uncertainty in LMMs regardless of their modalities, architectures, or
capabilities, (2) an empirical exploration of multimodal prompt perturbations
to uncover LMM uncertainty, offering insights and findings, and (3) derive the
formulation of multimodal semantic uncertainty, which enables quantifying
uncertainty from multimodal responses. Experiments across 18 benchmarks
spanning various modalities and 10 LMMs (both open- and closed-source)
demonstrate the effectiveness of Uncertainty-o in reliably estimating LMM
uncertainty, thereby enhancing downstream tasks such as hallucination
detection, hallucination mitigation, and uncertainty-aware Chain-of-Thought
reasoning.

</details>


### [543] [Explore the vulnerability of black-box models via diffusion models](https://arxiv.org/abs/2506.07590)
*Jiacheng Shi,Yanfu Zhang,Huajie Shao,Ashley Gao*

Main category: cs.CV

TL;DR: 本文揭示了一种新的安全威胁，攻击者利用扩散模型API生成合成图像，用于训练高性能替代模型，从而在无需原始训练数据的情况下，以最少的查询执行模型提取和基于转移的对抗攻击。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在图像生成方面取得了显著进展，但它们也带来了安全和隐私风险，如版权侵犯、敏感信息泄露以及生成有害或冒犯性内容。本文旨在揭示并应对这些潜在的安全威胁。

Method: 攻击者通过扩散模型API生成高分辨率和多样化的合成图像，用于训练替代模型，从而实现对黑盒分类模型的模型提取和对抗攻击。

Result: 在包括CIFAR和ImageNet子集在内的七个基准测试中，该方法在使用仅0.01倍查询预算的情况下，平均比现有方法提高了27.37%，并在目标模型上实现了98.68%的对抗攻击成功率。

Conclusion: 本文展示了扩散模型在安全领域的潜在风险，并提出了有效的攻击方法，强调了在应用这些模型时加强安全措施的必要性。

Abstract: Recent advancements in diffusion models have enabled high-fidelity and
photorealistic image generation across diverse applications. However, these
models also present security and privacy risks, including copyright violations,
sensitive information leakage, and the creation of harmful or offensive content
that could be exploited maliciously. In this study, we uncover a novel security
threat where an attacker leverages diffusion model APIs to generate synthetic
images, which are then used to train a high-performing substitute model. This
enables the attacker to execute model extraction and transfer-based adversarial
attacks on black-box classification models with minimal queries, without
needing access to the original training data. The generated images are
sufficiently high-resolution and diverse to train a substitute model whose
outputs closely match those of the target model. Across the seven benchmarks,
including CIFAR and ImageNet subsets, our method shows an average improvement
of 27.37% over state-of-the-art methods while using just 0.01 times of the
query budget, achieving a 98.68% success rate in adversarial attacks on the
target model.

</details>


### [544] [LLM-driven Indoor Scene Layout Generation via Scaled Human-aligned Data Synthesis and Multi-Stage Preference Optimization](https://arxiv.org/abs/2506.07570)
*Yixuan Yang,Zhen Luo,Tongsheng Ding,Junru Lu,Mingqi Gao,Jinyu Yang,Victor Sanchez,Feng Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种基于LLM的室内布局生成方法，结合了大规模数据集3D-SynthPlace和优化的开源LLM OptiScene，显著提升了布局生成的质量和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的室内布局生成方法存在空间不一致性、计算成本高以及数据限制等问题，本文旨在通过结合合成数据和优化的LLM来解决这些问题。

Method: 本文提出了3D-SynthPlace数据集，并通过两阶段训练优化了开源LLM OptiScene，包括监督微调（SFT）和多轮直接偏好优化（DPO）。

Result: 实验表明，OptiScene在布局生成任务中优于传统的提示驱动和学习基线方法，并在场景编辑和机器人导航等交互任务中展现出潜力。

Conclusion: 本文的方法通过结合合成数据和优化的LLM，显著提升了室内布局生成的质量和成功率，展示了其在交互任务中的应用前景。

Abstract: Automatic indoor layout generation has attracted increasing attention due to
its potential in interior design, virtual environment construction, and
embodied AI. Existing methods fall into two categories: prompt-driven
approaches that leverage proprietary LLM services (e.g., GPT APIs) and
learning-based methods trained on layout data upon diffusion-based models.
Prompt-driven methods often suffer from spatial inconsistency and high
computational costs, while learning-based methods are typically constrained by
coarse relational graphs and limited datasets, restricting their generalization
to diverse room categories. In this paper, we revisit LLM-based indoor layout
generation and present 3D-SynthPlace, a large-scale dataset that combines
synthetic layouts generated via a 'GPT synthesize, Human inspect' pipeline,
upgraded from the 3D-Front dataset. 3D-SynthPlace contains nearly 17,000
scenes, covering four common room types -- bedroom, living room, kitchen, and
bathroom -- enriched with diverse objects and high-level spatial annotations.
We further introduce OptiScene, a strong open-source LLM optimized for indoor
layout generation, fine-tuned based on our 3D-SynthPlace dataset through our
two-stage training. For the warum-up stage I, we adopt supervised fine-tuning
(SFT), which is taught to first generate high-level spatial descriptions then
conditionally predict concrete object placements. For the reinforcing stage II,
to better align the generated layouts with human design preferences, we apply
multi-turn direct preference optimization (DPO), which significantly improving
layout quality and generation success rates. Extensive experiments demonstrate
that OptiScene outperforms traditional prompt-driven and learning-based
baselines. Moreover, OptiScene shows promising potential in interactive tasks
such as scene editing and robot navigation.

</details>


### [545] [HieraEdgeNet: A Multi-Scale Edge-Enhanced Framework for Automated Pollen Recognition](https://arxiv.org/abs/2506.07637)
*Yuchong Long,Wen Sun,Ningxiao Sun,Wenxiao Wang,Chao Li,Shan Yin*

Main category: cs.CV

TL;DR: 提出HieraEdgeNet多尺度边缘增强框架，通过HEM、SEF、CSPOKM三模块协同工作，显著提升显微花粉检测精度，在120类数据集上mAP@.5达0.9501，超越YOLOv12n等基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统花粉识别方法效率低且主观性强，现有深度学习模型对显微目标（如花粉）的定位精度不足，因其尺寸微小、边缘模糊且背景复杂。

Method: 构建包含三模块的框架：HEM提取多尺度边缘特征金字塔，SEF融合边缘先验与语义信息，CSPOKM通过各向大核卷积与混合域注意力优化细节层，并嵌入CSP计算框架提升效率。

Result: 在120类花粉数据集上达到0.9501的mAP@.5，定性分析显示特征表示更聚焦目标边界，显著优于YOLOv12n和RT-DETR等SOTA模型。

Conclusion: HieraEdgeNet通过系统整合边缘信息，为显微物体高精度检测提供高效解决方案，特征表征能力验证了边缘增强策略的有效性。

Abstract: Automated pollen recognition is vital to paleoclimatology, biodiversity
monitoring, and public health, yet conventional methods are hampered by
inefficiency and subjectivity. Existing deep learning models often struggle to
achieve the requisite localization accuracy for microscopic targets like
pollen, which are characterized by their minute size, indistinct edges, and
complex backgrounds. To overcome this limitation, we introduce HieraEdgeNet, a
multi-scale edge-enhancement framework. The framework's core innovation is the
introduction of three synergistic modules: the Hierarchical Edge Module (HEM),
which explicitly extracts a multi-scale pyramid of edge features that
corresponds to the semantic hierarchy at early network stages; the Synergistic
Edge Fusion (SEF) module, for deeply fusing these edge priors with semantic
information at each respective scale; and the Cross Stage Partial Omni-Kernel
Module (CSPOKM), which maximally refines the most detail-rich feature layers
using an Omni-Kernel operator - comprising anisotropic large-kernel
convolutions and mixed-domain attention - all within a computationally
efficient Cross-Stage Partial (CSP) framework. On a large-scale dataset
comprising 120 pollen classes, HieraEdgeNet achieves a mean Average Precision
(mAP@.5) of 0.9501, significantly outperforming state-of-the-art baseline
models such as YOLOv12n and RT-DETR. Furthermore, qualitative analysis confirms
that our approach generates feature representations that are more precisely
focused on object boundaries. By systematically integrating edge information,
HieraEdgeNet provides a robust and powerful solution for high-precision,
high-efficiency automated detection of microscopic objects.

</details>


### [546] [SceneRAG: Scene-level Retrieval-Augmented Generation for Video Understanding](https://arxiv.org/abs/2506.07600)
*Nianbo Zeng,Haowen Hou,Fei Richard Yu,Si Shi,Ying Tiffany He*

Main category: cs.CV

TL;DR: 本文提出SceneRAG框架，通过大语言模型将视频分割为叙事连贯的场景，融合多模态信息构建动态知识图谱，显著提升长视频理解与生成任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成（RAG）方法采用固定长度视频分块，破坏上下文连续性且无法捕捉真实场景边界，而人类具备将连续经验组织为连贯场景的能力，这成为研究动机。

Method: 1. 利用LLM处理ASR转录文本及时间元数据实现叙事一致场景分割
2. 通过轻量级启发式规则与迭代校正优化场景边界
3. 融合视觉-文本多模态信息提取实体关系，构建动态知识图谱支持多跳检索

Result: 在包含134+小时视频的LongerVideos基准测试中，SceneRAG生成任务胜率达72.5%，显著优于现有基线方法。

Conclusion: SceneRAG通过场景级分割与多模态知识图谱建模，有效解决长视频理解中的长程依赖问题，为复杂视频内容分析提供了新范式。

Abstract: Despite recent advances in retrieval-augmented generation (RAG) for video
understanding, effectively understanding long-form video content remains
underexplored due to the vast scale and high complexity of video data. Current
RAG approaches typically segment videos into fixed-length chunks, which often
disrupts the continuity of contextual information and fails to capture
authentic scene boundaries. Inspired by the human ability to naturally organize
continuous experiences into coherent scenes, we present SceneRAG, a unified
framework that leverages large language models to segment videos into
narrative-consistent scenes by processing ASR transcripts alongside temporal
metadata. SceneRAG further sharpens these initial boundaries through
lightweight heuristics and iterative correction. For each scene, the framework
fuses information from both visual and textual modalities to extract entity
relations and dynamically builds a knowledge graph, enabling robust multi-hop
retrieval and generation that account for long-range dependencies. Experiments
on the LongerVideos benchmark, featuring over 134 hours of diverse content,
confirm that SceneRAG substantially outperforms prior baselines, achieving a
win rate of up to 72.5 percent on generation tasks.

</details>


### [547] [SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis](https://arxiv.org/abs/2506.07603)
*Jianhui Wei,Zikai Xiao,Danyu Sun,Luqi Gong,Zongxin Yang,Zuozhu Liu,Jian Wu*

Main category: cs.CV

TL;DR: 提出SurgBench框架，包含大规模预训练数据集SurgBench-P和评估基准SurgBench-E，解决手术视频基础模型数据不足问题，并通过实验验证其预训练能显著提升跨领域泛化性能。


<details>
  <summary>Details</summary>
Motivation: 当前手术视频基础模型发展受限于缺乏大规模多样化数据集及系统性评估标准，阻碍了自动化术中决策、技能评估等应用进展。

Method: 构建统一基准框架SurgBench：SurgBench-P预训练数据集覆盖22种手术/11专科的5300万帧；SurgBench-E评估基准包含6大类72项细粒度任务（如阶段分类、工具识别等）。

Result: 实验表明现有视频基础模型泛化能力不足，而基于SurgBench-P预训练的模型性能显著提升（尤其在未见过手术类型和流程中表现跨模态泛化优势）。

Conclusion: SurgBench为手术视频分析提供标准化框架，其数据多样性和系统性评估体系可推动基础模型发展，并验证了领域专用预训练对医疗视频理解的重要性。

Abstract: Surgical video understanding is pivotal for enabling automated intraoperative
decision-making, skill assessment, and postoperative quality improvement.
However, progress in developing surgical video foundation models (FMs) remains
hindered by the scarcity of large-scale, diverse datasets for pretraining and
systematic evaluation. In this paper, we introduce \textbf{SurgBench}, a
unified surgical video benchmarking framework comprising a pretraining dataset,
\textbf{SurgBench-P}, and an evaluation benchmark, \textbf{SurgBench-E}.
SurgBench offers extensive coverage of diverse surgical scenarios, with
SurgBench-P encompassing 53 million frames across 22 surgical procedures and 11
specialties, and SurgBench-E providing robust evaluation across six categories
(phase classification, camera motion, tool recognition, disease diagnosis,
action classification, and organ detection) spanning 72 fine-grained tasks.
Extensive experiments reveal that existing video FMs struggle to generalize
across varied surgical video analysis tasks, whereas pretraining on SurgBench-P
yields substantial performance improvements and superior cross-domain
generalization to unseen procedures and modalities. Our dataset and code are
available upon request.

</details>


### [548] [FMaMIL: Frequency-Driven Mamba Multi-Instance Learning for Weakly Supervised Lesion Segmentation in Medical Images](https://arxiv.org/abs/2506.07652)
*Hangbei Cheng,Xiaorong Dong,Xueyu Liu,Jianan Zhang,Xuetao Ma,Mingqiang Wei,Liansheng Wang,Junxin Chen,Yongfei Wu*

Main category: cs.CV

TL;DR: 提出FMaMIL两阶段弱监督框架，仅用图像级标签实现病理图像病灶分割，通过Mamba编码器捕捉长程依赖并引入频域编码增强空间敏感性，结合自校正机制优化伪标签训练。


<details>
  <summary>Details</summary>
Motivation: 组织病理图像病灶分割依赖高成本像素级标注，现有弱监督方法性能受限。需开发不依赖像素标注且能有效捕捉病灶结构的方法。

Method: 1) 一阶段：轻量Mamba编码器在MIL范式下建模图像块长程依赖，设计可学习频域编码模块补充空间特征；2) 二阶段：基于CAM的软标签监督和自校正机制优化伪标签，提升噪声标签下的鲁棒性。

Result: 在公开/私有病理数据集上超越SOTA弱监督方法，无需像素级标注即实现高精度分割。

Conclusion: FMaMIL验证了频域增强与自校正机制的有效性，为数字病理提供低成本高精度的弱监督解决方案。

Abstract: Accurate lesion segmentation in histopathology images is essential for
diagnostic interpretation and quantitative analysis, yet it remains challenging
due to the limited availability of costly pixel-level annotations. To address
this, we propose FMaMIL, a novel two-stage framework for weakly supervised
lesion segmentation based solely on image-level labels. In the first stage, a
lightweight Mamba-based encoder is introduced to capture long-range
dependencies across image patches under the MIL paradigm. To enhance spatial
sensitivity and structural awareness, we design a learnable frequency-domain
encoding module that supplements spatial-domain features with spectrum-based
information. CAMs generated in this stage are used to guide segmentation
training. In the second stage, we refine the initial pseudo labels via a
CAM-guided soft-label supervision and a self-correction mechanism, enabling
robust training even under label noise. Extensive experiments on both public
and private histopathology datasets demonstrate that FMaMIL outperforms
state-of-the-art weakly supervised methods without relying on pixel-level
annotations, validating its effectiveness and potential for digital pathology
applications.

</details>


### [549] [NOVA3D: Normal Aligned Video Diffusion Model for Single Image to 3D Generation](https://arxiv.org/abs/2506.07698)
*Yuxiao Yang,Peihao Li,Yuhong Zhang,Junzhe Lu,Xianglong He,Minghan Qin,Weitao Wang,Haoqian Wang*

Main category: cs.CV

TL;DR: NOVA3D提出了一种基于预训练视频扩散模型的单图到3D生成框架，通过几何-时序对齐注意力机制和去冲突几何融合算法，解决了多视角一致性与纹理保真度问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像扩散模型的3D生成方法因3D先验不足导致多视角一致性差，需利用视频模型的时序信息增强几何一致性。

Method: 1) 利用视频扩散模型提取3D先验 2) 提出几何-时序对齐(GTA)注意力机制促进跨域信息交互 3) 设计去冲突几何融合算法解决多视角位姿对齐问题

Result: 实验表明NOVA3D在生成质量、多视角一致性等指标上优于现有基线方法

Conclusion: 通过融合视频模型的时序先验与几何优化策略，NOVA3D实现了高保真、多视角一致的3D内容生成，推动了AIGC在三维创作中的应用。

Abstract: 3D AI-generated content (AIGC) has made it increasingly accessible for anyone
to become a 3D content creator. While recent methods leverage Score
Distillation Sampling to distill 3D objects from pretrained image diffusion
models, they often suffer from inadequate 3D priors, leading to insufficient
multi-view consistency. In this work, we introduce NOVA3D, an innovative
single-image-to-3D generation framework. Our key insight lies in leveraging
strong 3D priors from a pretrained video diffusion model and integrating
geometric information during multi-view video fine-tuning. To facilitate
information exchange between color and geometric domains, we propose the
Geometry-Temporal Alignment (GTA) attention mechanism, thereby improving
generalization and multi-view consistency. Moreover, we introduce the
de-conflict geometry fusion algorithm, which improves texture fidelity by
addressing multi-view inaccuracies and resolving discrepancies in pose
alignment. Extensive experiments validate the superiority of NOVA3D over
existing baselines.

</details>


### [550] [Trend-Aware Fashion Recommendation with Visual Segmentation and Semantic Similarity](https://arxiv.org/abs/2506.07773)
*Mohamed Djilani,Nassim Ali Ousalah,Nidhal Eddine Chenni*

Main category: cs.CV

TL;DR: 本文提出了一种结合视觉、语义和用户行为的时尚推荐系统，通过深度学习模型和合成购买历史生成个性化推荐。


<details>
  <summary>Details</summary>
Motivation: 为了在时尚推荐中平衡个人风格与流行趋势，本文旨在开发一个能够整合视觉表示、语义相似性和用户行为的推荐系统。

Method: 系统通过语义分割提取服装区域，使用预训练的CNN模型（如ResNet-50）生成视觉嵌入，并结合用户行为模拟生成推荐。

Result: 在DeepFashion数据集上的实验显示，ResNet-50在类别相似性上达到64.95%，并在流行度预测上表现最佳。

Conclusion: 该方法提供了一个可扩展的框架，能够在个性化时尚推荐中有效结合视觉和流行度信息。

Abstract: We introduce a trend-aware and visually-grounded fashion recommendation
system that integrates deep visual representations, garment-aware segmentation,
semantic category similarity and user behavior simulation. Our pipeline
extracts focused visual embeddings by masking non-garment regions via semantic
segmentation followed by feature extraction using pretrained CNN backbones
(ResNet-50, DenseNet-121, VGG16). To simulate realistic shopping behavior, we
generate synthetic purchase histories influenced by user-specific trendiness
and item popularity. Recommendations are computed using a weighted scoring
function that fuses visual similarity, semantic coherence and popularity
alignment. Experiments on the DeepFashion dataset demonstrate consistent gender
alignment and improved category relevance, with ResNet-50 achieving 64.95%
category similarity and lowest popularity MAE. An ablation study confirms the
complementary roles of visual and popularity cues. Our method provides a
scalable framework for personalized fashion recommendations that balances
individual style with emerging trends. Our implementation is available at
https://github.com/meddjilani/FashionRecommender

</details>


### [551] [Consistent Video Editing as Flow-Driven Image-to-Video Generation](https://arxiv.org/abs/2506.07713)
*Ge Wang,Songlin Fan,Hangxu Liu,Quanjian Song,Hewei Wang,Jinfeng Xu*

Main category: cs.CV

TL;DR: FlowV2V提出了一种基于光流的视频编辑方法，通过分解为第一帧编辑和条件图像到视频生成，提升了复杂运动建模的时序一致性和样本质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法在处理复杂运动模式时存在局限，尤其是在非刚性物体运动和多对象编辑任务中表现不佳。光流为复杂运动建模提供了新的可能性。

Method: FlowV2V将视频编辑任务分解为第一帧编辑和条件图像到视频生成，并模拟与变形形状对齐的伪光流序列，确保编辑过程中的时序一致性。

Result: 在DAVIS-EDIT数据集上，FlowV2V在DOVER和warping error指标上分别提升了13.67%和50.66%，展示了其在时序一致性和样本质量上的优越性。

Conclusion: FlowV2V通过光流驱动的图像到视频生成方法，显著提升了视频编辑的复杂运动建模能力，尤其在非刚性物体运动和多对象编辑任务中表现突出。

Abstract: With the prosper of video diffusion models, down-stream applications like
video editing have been significantly promoted without consuming much
computational cost. One particular challenge in this task lies at the motion
transfer process from the source video to the edited one, where it requires the
consideration of the shape deformation in between, meanwhile maintaining the
temporal consistency in the generated video sequence. However, existing methods
fail to model complicated motion patterns for video editing, and are
fundamentally limited to object replacement, where tasks with non-rigid object
motions like multi-object and portrait editing are largely neglected. In this
paper, we observe that optical flows offer a promising alternative in complex
motion modeling, and present FlowV2V to re-investigate video editing as a task
of flow-driven Image-to-Video (I2V) generation. Specifically, FlowV2V
decomposes the entire pipeline into first-frame editing and conditional I2V
generation, and simulates pseudo flow sequence that aligns with the deformed
shape, thus ensuring the consistency during editing. Experimental results on
DAVIS-EDIT with improvements of 13.67% and 50.66% on DOVER and warping error
illustrate the superior temporal consistency and sample quality of FlowV2V
compared to existing state-of-the-art ones. Furthermore, we conduct
comprehensive ablation studies to analyze the internal functionalities of the
first-frame paradigm and flow alignment in the proposed method.

</details>


### [552] [Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger](https://arxiv.org/abs/2506.07785)
*Qi Yang,Chenghao Zhang,Lubin Fan,Kun Ding,Jieping Ye,Shiming Xiang*

Main category: cs.CV

TL;DR: 本文提出了一种名为RCTS的多模态RAG框架，通过构建推理上下文丰富的知识库和树搜索重排序方法，提升了大视觉语言模型在视觉问答任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大视觉语言模型在视觉问答任务中面临知识推理示例稀缺和检索知识响应不稳定的问题，本文旨在解决这些问题。

Method: 本文提出了RCTS框架，包括构建推理上下文丰富的知识库和引入蒙特卡洛树搜索与启发式奖励（MCTS-HR）来重排序最相关的示例。

Result: 实验表明，RCTS框架在多个视觉问答数据集上达到了最先进的性能，显著优于上下文学习和传统RAG方法。

Conclusion: RCTS框架通过其知识库和重排序方法有效提升了大视觉语言模型的表现，展示了其在视觉问答任务中的潜力。

Abstract: Recent advancements in Large Vision Language Models (LVLMs) have
significantly improved performance in Visual Question Answering (VQA) tasks
through multimodal Retrieval-Augmented Generation (RAG). However, existing
methods still face challenges, such as the scarcity of knowledge with reasoning
examples and erratic responses from retrieved knowledge. To address these
issues, in this study, we propose a multimodal RAG framework, termed RCTS,
which enhances LVLMs by constructing a Reasoning Context-enriched knowledge
base and a Tree Search re-ranking method. Specifically, we introduce a
self-consistent evaluation mechanism to enrich the knowledge base with
intrinsic reasoning patterns. We further propose a Monte Carlo Tree Search with
Heuristic Rewards (MCTS-HR) to prioritize the most relevant examples. This
ensures that LVLMs can leverage high-quality contextual reasoning for better
and more consistent responses. Extensive experiments demonstrate that our
framework achieves state-of-the-art performance on multiple VQA datasets,
significantly outperforming In-Context Learning (ICL) and Vanilla-RAG methods.
It highlights the effectiveness of our knowledge base and re-ranking method in
improving LVLMs. Our code is available at https://github.com/yannqi/RCTS-RAG.

</details>


### [553] [ETA: Efficiency through Thinking Ahead, A Dual Approach to Self-Driving with Large Models](https://arxiv.org/abs/2506.07725)
*Shadi Hamdan,Chonghao Sima,Zetong Yang,Hongyang Li,Fatma Güney*

Main category: cs.CV

TL;DR: 论文提出了一种名为ETA的异步系统，通过将当前帧的密集计算转移到前一时间步，并执行多时间步的批量推理，使大模型能够及时响应每个时间步，从而在不牺牲推理速度的情况下利用大模型。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶系统中，如何在利用大模型的同时不牺牲推理速度是一个常见难题。现有的双系统设计虽然通过小模型进行快速反应决策，大模型进行更慢但更深入的分析，但仍难以使大模型对每个在线帧做出及时响应。

Method: 论文提出了ETA系统，通过将当前帧的密集计算转移到前一时间步，并执行多时间步的批量推理，使大模型能够及时响应每个时间步。具体方法包括：1）利用大模型的未来预测将信息特征从过去传播到当前帧；2）使用小模型提取当前帧特征以实现实时响应；3）通过动作掩码机制整合双特征，强调动作关键图像区域。

Result: 在Bench2Drive CARLA Leaderboard-v2基准测试中，ETA将最先进的性能提升了8%，驾驶得分为69.53，同时保持了接近实时的推理速度（50毫秒）。

Conclusion: ETA系统通过异步设计和批量推理，成功实现了在不牺牲推理速度的情况下利用大模型，显著提升了自动驾驶系统的性能。

Abstract: How can we benefit from large models without sacrificing inference speed, a
common dilemma in self-driving systems? A prevalent solution is a dual-system
architecture, employing a small model for rapid, reactive decisions and a
larger model for slower but more informative analyses. Existing dual-system
designs often implement parallel architectures where inference is either
directly conducted using the large model at each current frame or retrieved
from previously stored inference results. However, these works still struggle
to enable large models for a timely response to every online frame. Our key
insight is to shift intensive computations of the current frame to previous
time steps and perform a batch inference of multiple time steps to make large
models respond promptly to each time step. To achieve the shifting, we
introduce Efficiency through Thinking Ahead (ETA), an asynchronous system
designed to: (1) propagate informative features from the past to the current
frame using future predictions from the large model, (2) extract current frame
features using a small model for real-time responsiveness, and (3) integrate
these dual features via an action mask mechanism that emphasizes
action-critical image regions. Evaluated on the Bench2Drive CARLA
Leaderboard-v2 benchmark, ETA advances state-of-the-art performance by 8% with
a driving score of 69.53 while maintaining a near-real-time inference speed at
50 ms.

</details>


### [554] [ArchiLense: A Framework for Quantitative Analysis of Architectural Styles Based on Vision Large Language Models](https://arxiv.org/abs/2506.07739)
*Jing Zhong,Jun Yin,Peilin Li,Pengyu Zeng,Miao Zang,Ran Luo,Shuai Lu*

Main category: cs.CV

TL;DR: 本研究通过构建专业建筑风格数据集ArchDiffBench和基于视觉语言模型的框架ArchiLense，结合计算机视觉与深度学习技术，实现了建筑图像的自动识别与风格差异分析，有效解决了传统建筑文化研究中的主观性和地域局限性问题，分类准确率达84.5%。


<details>
  <summary>Details</summary>
Motivation: 传统建筑文化研究依赖主观专家解读和历史文献，存在地域偏见与解释范围受限的问题。需通过客观技术手段实现跨区域建筑风格的系统性分析。

Method: 1. 构建含1,765张多区域历史时期建筑图像数据集ArchDiffBench；2. 开发基于视觉语言模型ArchiLense框架，整合计算机视觉、深度学习与机器学习算法，实现建筑风格自动识别与描述生成。

Result: ArchiLense在风格识别中达到92.4%专家标注一致性率和84.5%分类准确率，能有效捕捉跨图像风格差异并生成描述性语言输出。

Conclusion: 该方法突破传统分析主观性局限，为跨文化建筑比较研究建立了客观量化分析范式，显著提升风格识别精度与解释力。

Abstract: Architectural cultures across regions are characterized by stylistic
diversity, shaped by historical, social, and technological contexts in addition
to geograph-ical conditions. Understanding architectural styles requires the
ability to describe and analyze the stylistic features of different architects
from various regions through visual observations of architectural imagery.
However, traditional studies of architectural culture have largely relied on
subjective expert interpretations and historical literature reviews, often
suffering from regional biases and limited ex-planatory scope. To address these
challenges, this study proposes three core contributions: (1) We construct a
professional architectural style dataset named ArchDiffBench, which comprises
1,765 high-quality architectural images and their corresponding style
annotations, collected from different regions and historical periods. (2) We
propose ArchiLense, an analytical framework grounded in Vision-Language Models
and constructed using the ArchDiffBench dataset. By integrating ad-vanced
computer vision techniques, deep learning, and machine learning algo-rithms,
ArchiLense enables automatic recognition, comparison, and precise
classi-fication of architectural imagery, producing descriptive language
outputs that ar-ticulate stylistic differences. (3) Extensive evaluations show
that ArchiLense achieves strong performance in architectural style recognition,
with a 92.4% con-sistency rate with expert annotations and 84.5% classification
accuracy, effec-tively capturing stylistic distinctions across images. The
proposed approach transcends the subjectivity inherent in traditional analyses
and offers a more objective and accurate perspective for comparative studies of
architectural culture.

</details>


### [555] [R3D2: Realistic 3D Asset Insertion via Diffusion for Autonomous Driving Simulation](https://arxiv.org/abs/2506.07826)
*William Ljungbergh,Bernardo Taveira,Wenzhao Zheng,Adam Tonderski,Chensheng Peng,Fredrik Kahl,Christoffer Petersson,Michael Felsberg,Kurt Keutzer,Masayoshi Tomizuka,Wei Zhan*

Main category: cs.CV

TL;DR: 本文提出R3D2，一种轻量级扩散模型，用于在自动驾驶验证中实现真实感3D资产插入，解决了现有神经重建方法在动态对象操作和可重用性上的不足。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统的验证需要多样化和安全关键的测试，传统仿真平台虽可控但资源密集且存在与现实数据的领域差距，而现有神经重建方法在动态对象操作和可重用性上存在局限。

Method: R3D2通过训练于一个新颖的数据集，利用图像条件3D生成模型从真实驾驶数据生成3DGS对象资产，并将其合成到基于神经渲染的虚拟环境中，学习真实感集成。

Result: 定量和定性评估表明，R3D2显著增强了插入资产的真实感，支持文本到3D资产插入和跨场景/数据集对象转移，实现了自动驾驶验证的真正可扩展性。

Conclusion: R3D2为自动驾驶验证提供了可扩展且真实的仿真解决方案，并公开了数据集和代码以促进进一步研究。

Abstract: Validating autonomous driving (AD) systems requires diverse and
safety-critical testing, making photorealistic virtual environments essential.
Traditional simulation platforms, while controllable, are resource-intensive to
scale and often suffer from a domain gap with real-world data. In contrast,
neural reconstruction methods like 3D Gaussian Splatting (3DGS) offer a
scalable solution for creating photorealistic digital twins of real-world
driving scenes. However, they struggle with dynamic object manipulation and
reusability as their per-scene optimization-based methodology tends to result
in incomplete object models with integrated illumination effects. This paper
introduces R3D2, a lightweight, one-step diffusion model designed to overcome
these limitations and enable realistic insertion of complete 3D assets into
existing scenes by generating plausible rendering effects-such as shadows and
consistent lighting-in real time. This is achieved by training R3D2 on a novel
dataset: 3DGS object assets are generated from in-the-wild AD data using an
image-conditioned 3D generative model, and then synthetically placed into
neural rendering-based virtual environments, allowing R3D2 to learn realistic
integration. Quantitative and qualitative evaluations demonstrate that R3D2
significantly enhances the realism of inserted assets, enabling use-cases like
text-to-3D asset insertion and cross-scene/dataset object transfer, allowing
for true scalability in AD validation. To promote further research in scalable
and realistic AD simulation, we will release our dataset and code, see
https://research.zenseact.com/publications/R3D2/.

</details>


### [556] [LogoSP: Local-global Grouping of Superpoints for Unsupervised Semantic Segmentation of 3D Point Clouds](https://arxiv.org/abs/2506.07857)
*Zihui Zhang,Weisheng Dai,Hongtao Wen,Bo Yang*

Main category: cs.CV

TL;DR: 本文提出LogoSP方法，通过结合局部与全局特征在频域中生成高质量语义伪标签，实现无监督3D点云语义分割，性能显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有无监督3D分割方法仅依赖局部特征和简单分组，无法挖掘更丰富的语义先验。本文旨在通过全局模式增强语义发现能力。

Method: LogoSP方法在频域中分析超点的全局模式，据此分组生成语义标签，结合局部特征训练分割网络。

Result: 在室内外数据集上达到SOTA，性能大幅领先现有方法。实验证实学到的全局模式能有效表征无标签下的3D语义。

Conclusion: 通过频域模式挖掘全局语义先验，证明了无监督条件下结合局部与全局特征对3D语义分割的有效性。

Abstract: We study the problem of unsupervised 3D semantic segmentation on raw point
clouds without needing human labels in training. Existing methods usually
formulate this problem into learning per-point local features followed by a
simple grouping strategy, lacking the ability to discover additional and
possibly richer semantic priors beyond local features. In this paper, we
introduce LogoSP to learn 3D semantics from both local and global point
features. The key to our approach is to discover 3D semantic information by
grouping superpoints according to their global patterns in the frequency
domain, thus generating highly accurate semantic pseudo-labels for training a
segmentation network. Extensive experiments on two indoor and an outdoor
datasets show that our LogoSP surpasses all existing unsupervised methods by
large margins, achieving the state-of-the-art performance for unsupervised 3D
semantic segmentation. Notably, our investigation into the learned global
patterns reveals that they truly represent meaningful 3D semantics in the
absence of human labels during training.

</details>


### [557] [Self-Cascaded Diffusion Models for Arbitrary-Scale Image Super-Resolution](https://arxiv.org/abs/2506.07813)
*Junseo Bang,Joonhee Lee,Kyeonghyun Lee,Haechang Lee,Dong Un Kang,Se Young Chun*

Main category: cs.CV

TL;DR: 本文提出CasArbi，一种自级联扩散框架，通过分解缩放因子为连续小步骤并逐步提升分辨率，结合坐标引导的残差扩散模型，在任意尺度超分辨率任务中实现感知与失真性能的双优。


<details>
  <summary>Details</summary>
Motivation: 传统单阶段上采样方法难以适应连续缩放因子的广泛分布，而现有渐进式策略与扩散模型的结合研究不足。需开发能灵活适应任意缩放需求的高效超分辨率方法。

Method: CasArbi框架采用自级联结构，将任意缩放需求分解为连续小因子，通过坐标引导的残差扩散模型逐步增强分辨率，实现连续图像表征学习及高效扩散采样。

Result: 实验表明CasArbi在多种任意尺度超分辨率基准测试中，感知质量与失真指标均超越现有方法，验证了框架的有效性。

Conclusion: CasArbi通过渐进式扩散框架与残差建模，成功解决了任意尺度超分辨率问题，为连续缩放任务提供了灵活高效的解决方案。

Abstract: Arbitrary-scale image super-resolution aims to upsample images to any desired
resolution, offering greater flexibility than traditional fixed-scale
super-resolution. Recent approaches in this domain utilize regression-based or
generative models, but many of them are a single-stage upsampling process,
which may be challenging to learn across a wide, continuous distribution of
scaling factors. Progressive upsampling strategies have shown promise in
mitigating this issue, yet their integration with diffusion models for flexible
upscaling remains underexplored. Here, we present CasArbi, a novel
self-cascaded diffusion framework for arbitrary-scale image super-resolution.
CasArbi meets the varying scaling demands by breaking them down into smaller
sequential factors and progressively enhancing the image resolution at each
step with seamless transitions for arbitrary scales. Our novel
coordinate-guided residual diffusion model allows for the learning of
continuous image representations while enabling efficient diffusion sampling.
Extensive experiments demonstrate that our CasArbi outperforms prior arts in
both perceptual and distortion performance metrics across diverse
arbitrary-scale super-resolution benchmarks.

</details>


### [558] [VIVAT: Virtuous Improving VAE Training through Artifact Mitigation](https://arxiv.org/abs/2506.07863)
*Lev Novitskiy,Viacheslav Vasilev,Maria Kovaleva,Vladimir Arkhipkin,Denis Dimitrov*

Main category: cs.CV

TL;DR: 本文提出VIVAT方法，通过系统化调整KL-VAE训练中的损失权重、填充策略等，有效缓解常见伪影问题，在图像重建与生成任务中取得SOTA结果，同时保持框架简洁性。


<details>
  <summary>Details</summary>
Motivation: 变分自编码器（VAE）训练中常出现颜色偏移、网格伪影等问题，影响重建与生成质量。现有方法需复杂架构调整，本文旨在通过轻量级改进解决这些实际问题。

Method: 提出五类常见伪影（颜色偏移、网格、模糊等）的成因分析，并通过调整损失权重、优化填充策略、引入空间条件归一化（SCN）等方法改进训练过程。

Result: 在多个基准测试中，VIVAT的PSNR和SSIM指标达到SOTA，文本生成图像的CLIP分数显著提升，验证了方法在重建与生成任务中的有效性。

Conclusion: VIVAT在保持KL-VAE框架简洁性的前提下，系统化解决了训练伪影问题，为优化VAE训练提供了可复用的实践方案。

Abstract: Variational Autoencoders (VAEs) remain a cornerstone of generative computer
vision, yet their training is often plagued by artifacts that degrade
reconstruction and generation quality. This paper introduces VIVAT, a
systematic approach to mitigating common artifacts in KL-VAE training without
requiring radical architectural changes. We present a detailed taxonomy of five
prevalent artifacts - color shift, grid patterns, blur, corner and droplet
artifacts - and analyze their root causes. Through straightforward
modifications, including adjustments to loss weights, padding strategies, and
the integration of Spatially Conditional Normalization, we demonstrate
significant improvements in VAE performance. Our method achieves
state-of-the-art results in image reconstruction metrics (PSNR and SSIM) across
multiple benchmarks and enhances text-to-image generation quality, as evidenced
by superior CLIP scores. By preserving the simplicity of the KL-VAE framework
while addressing its practical challenges, VIVAT offers actionable insights for
researchers and practitioners aiming to optimize VAE training.

</details>


### [559] [FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity](https://arxiv.org/abs/2506.07865)
*Jinxi Li,Ziyang Song,Siyuan Zhou,Bo Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为FreeGave的方法，旨在从多视角视频中建模3D场景几何、外观和底层物理，无需对象先验知识，并在未来帧外推和运动分割方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂物理运动学习上存在不足，通常需要对象先验知识或无法有效处理边界问题。本文旨在解决这些问题，提出一种无需对象先验知识的方法。

Method: 本文提出FreeGave方法，通过引入物理代码和精心设计的无散度模块，估计每个高斯速度场，避免使用低效的PINN损失。

Result: 在三个公共数据集和一个新收集的具有挑战性的真实世界数据集上的广泛实验表明，该方法在未来帧外推和运动分割方面表现优异。

Conclusion: 研究表明，FreeGave方法在没有人类标签的情况下，能够学习到有意义的3D物理运动模式。

Abstract: In this paper, we aim to model 3D scene geometry, appearance, and the
underlying physics purely from multi-view videos. By applying various governing
PDEs as PINN losses or incorporating physics simulation into neural networks,
existing works often fail to learn complex physical motions at boundaries or
require object priors such as masks or types. In this paper, we propose
FreeGave to learn the physics of complex dynamic 3D scenes without needing any
object priors. The key to our approach is to introduce a physics code followed
by a carefully designed divergence-free module for estimating a per-Gaussian
velocity field, without relying on the inefficient PINN losses. Extensive
experiments on three public datasets and a newly collected challenging
real-world dataset demonstrate the superior performance of our method for
future frame extrapolation and motion segmentation. Most notably, our
investigation into the learned physics codes reveals that they truly learn
meaningful 3D physical motion patterns in the absence of any human labels in
training.

</details>


### [560] [Diffusion models under low-noise regime](https://arxiv.org/abs/2506.07841)
*Elizabeth Pavlova,Xue-Xin Wei*

Main category: cs.CV

TL;DR: 本文系统研究了低噪声条件下扩散模型的行为，揭示了训练集规模、数据几何及模型目标对去噪轨迹的影响，填补了生成模型在小扰动场景可靠性理解的空白。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注高噪声环境下扩散模型的记忆化与泛化机制，但对其在低噪声条件下作为有效去噪器的行为模式及可靠性缺乏深入理解。

Method: 通过CelebA数据子集与高斯混合解析基准，分析模型在低噪声扩散动态下的表现，量化训练集规模、数据几何结构与目标函数对去噪轨迹的影响。

Result: 发现不同训练数据的模型在数据流形附近输出显著分歧，且训练集规模与数据几何结构直接影响分数估计精度，揭示了模型学习数据分布表征的内在机制。

Conclusion: 研究阐明了扩散模型在低噪声场景下的行为特性，为实际应用中模型对小扰动的鲁棒性及可解释性提供了理论依据。

Abstract: Recent work on diffusion models proposed that they operate in two regimes:
memorization, in which models reproduce their training data, and
generalization, in which they generate novel samples. While this has been
tested in high-noise settings, the behavior of diffusion models as effective
denoisers when the corruption level is small remains unclear. To address this
gap, we systematically investigated the behavior of diffusion models under
low-noise diffusion dynamics, with implications for model robustness and
interpretability. Using (i) CelebA subsets of varying sample sizes and (ii)
analytic Gaussian mixture benchmarks, we reveal that models trained on disjoint
data diverge near the data manifold even when their high-noise outputs
converge. We quantify how training set size, data geometry, and model objective
choice shape denoising trajectories and affect score accuracy, providing
insights into how these models actually learn representations of data
distributions. This work starts to address gaps in our understanding of
generative model reliability in practical applications where small
perturbations are common.

</details>


### [561] [A Comparative Study of U-Net Architectures for Change Detection in Satellite Images](https://arxiv.org/abs/2506.07925)
*Yaxita Amin,Naimisha S Trivedi,Rashmi Bhattad*

Main category: cs.CV

TL;DR: 本文通过分析34篇论文，比较了18种U-Net变体在遥感变化检测中的应用，强调了处理多时相数据的重要性。


<details>
  <summary>Details</summary>
Motivation: U-Net架构在遥感变化检测中的应用尚未充分探索，本文旨在填补这一空白。

Method: 本文对34篇论文进行了综合分析，比较了18种U-Net变体，评估了它们在遥感变化检测中的潜力。

Result: 研究强调了处理多时相数据和长距离关系对提高变化检测精度的重要性。

Conclusion: 本文为选择U-Net版本进行遥感变化检测的研究者和实践者提供了有价值的见解。

Abstract: Remote sensing change detection is essential for monitoring the everchanging
landscapes of the Earth. The U-Net architecture has gained popularity for its
capability to capture spatial information and perform pixel-wise
classification. However, their application in the Remote sensing field remains
largely unexplored. Therefore, this paper fill the gap by conducting a
comprehensive analysis of 34 papers. This study conducts a comparison and
analysis of 18 different U-Net variations, assessing their potential for
detecting changes in remote sensing. We evaluate both benefits along with
drawbacks of each variation within the framework of this particular
application. We emphasize variations that are explicitly built for change
detection, such as Siamese Swin-U-Net, which utilizes a Siamese architecture.
The analysis highlights the significance of aspects such as managing data from
different time periods and collecting relationships over a long distance to
enhance the precision of change detection. This study provides valuable
insights for researchers and practitioners that choose U-Net versions for
remote sensing change detection tasks.

</details>


### [562] [PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction and Enhancement](https://arxiv.org/abs/2506.07848)
*Teng Hu,Zhentao Yu,Zhengguang Zhou,Jiangning Zhang,Yuan Zhou,Qinglin Lu,Ran Yi*

Main category: cs.CV

TL;DR: PolyVivid提出了一种多主体视频定制框架，通过VLLM文本图像融合模块、3D-RoPE增强模块和注意力继承身份注入模块，实现了身份一致性和主体交互的精细控制，显著提升了视频生成的质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在多主体定制方面缺乏精细控制，尤其是在身份一致性和主体交互方面存在不足。

Method: PolyVivid设计了VLLM文本图像融合模块、3D-RoPE增强模块和注意力继承身份注入模块，并结合MLLM数据管道，实现了高质量的多主体视频生成。

Result: 实验表明，PolyVivid在身份保真度、视频真实性和主体对齐方面表现优异，超越了现有的开源和商业基线模型。

Conclusion: PolyVivid通过创新的模块设计和数据管道，显著提升了多主体视频生成的精细控制能力，为视频生成领域提供了新的解决方案。

Abstract: Despite recent advances in video generation, existing models still lack
fine-grained controllability, especially for multi-subject customization with
consistent identity and interaction. In this paper, we propose PolyVivid, a
multi-subject video customization framework that enables flexible and
identity-consistent generation. To establish accurate correspondences between
subject images and textual entities, we design a VLLM-based text-image fusion
module that embeds visual identities into the textual space for precise
grounding. To further enhance identity preservation and subject interaction, we
propose a 3D-RoPE-based enhancement module that enables structured
bidirectional fusion between text and image embeddings. Moreover, we develop an
attention-inherited identity injection module to effectively inject fused
identity features into the video generation process, mitigating identity drift.
Finally, we construct an MLLM-based data pipeline that combines MLLM-based
grounding, segmentation, and a clique-based subject consolidation strategy to
produce high-quality multi-subject data, effectively enhancing subject
distinction and reducing ambiguity in downstream video generation. Extensive
experiments demonstrate that PolyVivid achieves superior performance in
identity fidelity, video realism, and subject alignment, outperforming existing
open-source and commercial baselines.

</details>


### [563] [Real-time Localization of a Soccer Ball from a Single Camera](https://arxiv.org/abs/2506.07981)
*Dmitrii Vorobev,Artem Prosvetov,Karim Elhadji Daou*

Main category: cs.CV

TL;DR: 提出了一种计算高效的单摄像头实时三维足球轨迹重建方法，通过多模式状态模型加速优化，保持厘米级精度，适用于直播环境。


<details>
  <summary>Details</summary>
Motivation: 现有方法在严重遮挡、运动模糊和复杂背景下的三维足球轨迹重建效果不佳，且多摄像头系统成本高，因此需要一种高效且低成本的方法。

Method: 引入多模式状态模型，使用$W$个离散模式加速优化，系统在标准CPU上运行，适用于实时直播。

Result: 在6K分辨率的俄罗斯超级联赛数据集上评估，性能与多摄像头系统相当，无需昂贵基础设施。

Conclusion: 该方法为专业足球环境提供了一种实用、准确且低成本的三维球体跟踪解决方案。

Abstract: We propose a computationally efficient method for real-time three-dimensional
football trajectory reconstruction from a single broadcast camera. In contrast
to previous work, our approach introduces a multi-mode state model with $W$
discrete modes to significantly accelerate optimization while preserving
centimeter-level accuracy -- even in cases of severe occlusion, motion blur,
and complex backgrounds. The system operates on standard CPUs and achieves low
latency suitable for live broadcast settings. Extensive evaluation on a
proprietary dataset of 6K-resolution Russian Premier League matches
demonstrates performance comparable to multi-camera systems, without the need
for specialized or costly infrastructure. This work provides a practical method
for accessible and accurate 3D ball tracking in professional football
environments.

</details>


### [564] [CXR-LT 2024: A MICCAI challenge on long-tailed, multi-label, and zero-shot disease classification from chest X-ray](https://arxiv.org/abs/2506.07984)
*Mingquan Lin,Gregory Holste,Song Wang,Yiliang Zhou,Yishu Wei,Imon Banerjee,Pengyi Chen,Tianjie Dai,Yuexi Du,Nicha C. Dvornek,Yuyan Ge,Zuowei Guo,Shouhei Hanaoka,Dongkyun Kim,Pablo Messina,Yang Lu,Denis Parra,Donghyun Son,Álvaro Soto,Aisha Urooj,René Vidal,Yosuke Yamagishi,Zefan Yang,Ruichi Zhang,Yang Zhou,Leo Anthony Celi,Ronald M. Summers,Zhiyong Lu,Hao Chen,Adam Flanders,George Shih,Zhangyang Wang,Yifan Peng*

Main category: cs.CV

TL;DR: CXR-LT 2024是一个社区驱动的胸部X光疾病分类项目，通过扩展数据集至37.7万张图像和45种疾病标签（含19种新罕见病），并新增零样本学习任务，旨在提升长尾分布下的分类性能及临床泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决开放长尾肺部疾病分类的挑战，提升现有技术的可测量性，并通过扩展数据集和引入零样本学习应对实际临床中罕见病和未见过疾病的诊断需求。

Method: 扩展数据集至377,110张胸部X光片，新增19种罕见病标签；设计三个任务：长尾分类（含噪声测试集和人工标注子集）、零样本泛化至5种未见疾病；采用多模态模型、生成式方法处理噪声标签及零样本策略。

Result: 构建了覆盖更广疾病谱的大规模数据集，整合了前沿解决方案（如多模态模型和生成方法），为临床真实场景提供资源，并推动诊断模型的泛化能力。

Conclusion: CXR-LT 2024通过数据扩展与任务创新，促进了临床实用化胸部X光诊断模型的发展，为未来研究提供了标准化基准和跨疾病泛化的新方向。

Abstract: The CXR-LT series is a community-driven initiative designed to enhance lung
disease classification using chest X-rays (CXR). It tackles challenges in open
long-tailed lung disease classification and enhances the measurability of
state-of-the-art techniques. The first event, CXR-LT 2023, aimed to achieve
these goals by providing high-quality benchmark CXR data for model development
and conducting comprehensive evaluations to identify ongoing issues impacting
lung disease classification performance. Building on the success of CXR-LT
2023, the CXR-LT 2024 expands the dataset to 377,110 chest X-rays (CXRs) and 45
disease labels, including 19 new rare disease findings. It also introduces a
new focus on zero-shot learning to address limitations identified in the
previous event. Specifically, CXR-LT 2024 features three tasks: (i) long-tailed
classification on a large, noisy test set, (ii) long-tailed classification on a
manually annotated "gold standard" subset, and (iii) zero-shot generalization
to five previously unseen disease findings. This paper provides an overview of
CXR-LT 2024, detailing the data curation process and consolidating
state-of-the-art solutions, including the use of multimodal models for rare
disease detection, advanced generative approaches to handle noisy labels, and
zero-shot learning strategies for unseen diseases. Additionally, the expanded
dataset enhances disease coverage to better represent real-world clinical
settings, offering a valuable resource for future research. By synthesizing the
insights and innovations of participating teams, we aim to advance the
development of clinically realistic and generalizable diagnostic models for
chest radiography.

</details>


### [565] [Rethinking Crowd-Sourced Evaluation of Neuron Explanations](https://arxiv.org/abs/2506.07985)
*Tuomas Oikarinen,Ge Yan,Akshay Kulkarni,Tsui-Wei Weng*

Main category: cs.CV

TL;DR: 本文提出了一种高效且准确的众包评估策略，用于评估神经元解释的可靠性，并通过重要抽样和贝叶斯方法显著降低了成本。


<details>
  <summary>Details</summary>
Motivation: 现有的神经元解释算法缺乏明确的可靠性评估，且传统的众包评估方法成本高、噪声大，导致结果不可靠。

Method: 本文引入重要抽样技术选择最有价值的输入样本，并采用贝叶斯方法聚合多个评分，从而降低评估成本。

Result: 该方法将评估成本降低了约30倍，并通过贝叶斯聚合进一步减少了5倍的评分需求，同时保持了高准确性。

Conclusion: 本文提出的评估策略显著提高了神经元解释评估的效率和准确性，为未来研究提供了可靠的工具。

Abstract: Interpreting individual neurons or directions in activations space is an
important component of mechanistic interpretability. As such, many algorithms
have been proposed to automatically produce neuron explanations, but it is
often not clear how reliable these explanations are, or which methods produce
the best explanations. This can be measured via crowd-sourced evaluations, but
they can often be noisy and expensive, leading to unreliable results. In this
paper, we carefully analyze the evaluation pipeline and develop a
cost-effective and highly accurate crowdsourced evaluation strategy. In
contrast to previous human studies that only rate whether the explanation
matches the most highly activating inputs, we estimate whether the explanation
describes neuron activations across all inputs. To estimate this effectively,
we introduce a novel application of importance sampling to determine which
inputs are the most valuable to show to raters, leading to around 30x cost
reduction compared to uniform sampling. We also analyze the label noise present
in crowd-sourced evaluations and propose a Bayesian method to aggregate
multiple ratings leading to a further ~5x reduction in number of ratings
required for the same accuracy. Finally, we use these methods to conduct a
large-scale study comparing the quality of neuron explanations produced by the
most popular methods for two different vision models.

</details>


### [566] [MADFormer: Mixed Autoregressive and Diffusion Transformers for Continuous Image Generation](https://arxiv.org/abs/2506.07999)
*Junhao Chen,Yulia Tsvetkov,Xiaochuang Han*

Main category: cs.CV

TL;DR: MADFormer结合自回归和扩散模型，通过分块生成和混合层设计，提升高分辨率图像生成的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有混合模型缺乏系统指导，无法有效分配自回归和扩散模型的能力。

Method: 提出MADFormer，分块生成图像，自回归层用于全局条件，扩散层用于局部细化。

Result: 分块生成显著提升高分辨率图像性能，混合层设计在有限计算下提升FID达75%。

Conclusion: MADFormer为未来混合生成模型提供了实用设计原则。

Abstract: Recent progress in multimodal generation has increasingly combined
autoregressive (AR) and diffusion-based approaches, leveraging their
complementary strengths: AR models capture long-range dependencies and produce
fluent, context-aware outputs, while diffusion models operate in continuous
latent spaces to refine high-fidelity visual details. However, existing hybrids
often lack systematic guidance on how and why to allocate model capacity
between these paradigms. In this work, we introduce MADFormer, a Mixed
Autoregressive and Diffusion Transformer that serves as a testbed for analyzing
AR-diffusion trade-offs. MADFormer partitions image generation into spatial
blocks, using AR layers for one-pass global conditioning across blocks and
diffusion layers for iterative local refinement within each block. Through
controlled experiments on FFHQ-1024 and ImageNet, we identify two key insights:
(1) block-wise partitioning significantly improves performance on
high-resolution images, and (2) vertically mixing AR and diffusion layers
yields better quality-efficiency balances--improving FID by up to 75% under
constrained inference compute. Our findings offer practical design principles
for future hybrid generative models.

</details>


### [567] [Hidden in plain sight: VLMs overlook their visual representations](https://arxiv.org/abs/2506.08008)
*Stephanie Fu,Tyler Bonnen,Devin Guillory,Trevor Darrell*

Main category: cs.CV

TL;DR: 研究发现，视觉语言模型（VLMs）在视觉中心任务上表现显著低于其视觉编码器，主要因语言模型未能有效利用视觉信息且受语言先验影响。


<details>
  <summary>Details</summary>
Motivation: 探讨视觉语言模型（VLMs）整合视觉与语言信息的能力，揭示其在视觉任务中的瓶颈。

Method: 通过对比VLMs与其视觉编码器的直接输出，分析模型在视觉表征退化、任务提示敏感度及语言模型作用三方面的表现。

Result: VLMs在深度估计、对应关系等视觉任务中表现接近随机水平，主要因语言模型未有效利用视觉信息且依赖语言先验。

Conclusion: VLMs的瓶颈在于语言模型未能有效整合视觉信息，研究为未来改进视觉理解提供了诊断方法和评估框架。

Abstract: Language provides a natural interface to specify and evaluate performance on
visual tasks. To realize this possibility, vision language models (VLMs) must
successfully integrate visual and linguistic information. Our work compares
VLMs to a direct readout of their visual encoders to understand their ability
to integrate across these modalities. Across a series of vision-centric
benchmarks (e.g., depth estimation, correspondence), we find that VLMs perform
substantially worse than their visual encoders, dropping to near-chance
performance. We investigate these results through a series of analyses across
the entire VLM: namely 1) the degradation of vision representations, 2)
brittleness to task prompt, and 3) the language model's role in solving the
task. We find that the bottleneck in performing these vision-centric tasks lies
in this third category; VLMs are not effectively using visual information
easily accessible throughout the entire model, and they inherit the language
priors present in the LLM. Our work helps diagnose the failure modes of
open-source VLMs, and presents a series of evaluations useful for future
investigations into visual understanding within VLMs.

</details>


### [568] [Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion](https://arxiv.org/abs/2506.08009)
*Xun Huang,Zhengqi Li,Guande He,Mingyuan Zhou,Eli Shechtman*

Main category: cs.CV

TL;DR: 本文提出了一种名为Self Forcing的新型训练范式，用于自回归视频扩散模型，解决了长期存在的曝光偏差问题，并在单GPU上实现了实时视频生成。


<details>
  <summary>Details</summary>
Motivation: 自回归视频扩散模型在训练时使用真实上下文，但在推理时却需要基于自身不完美的输出生成序列，这导致了曝光偏差问题。本文旨在解决这一问题。

Method: Self Forcing通过在训练时使用自回归展开和键值缓存，使每一帧的生成基于之前自生成的输出，并通过视频级别的整体损失进行监督。此外，采用少步扩散模型和随机梯度截断策略来平衡计算成本和性能。

Result: 实验表明，该方法在单GPU上实现了亚秒级延迟的实时视频生成，生成质量与更慢且非因果的扩散模型相当甚至更好。

Conclusion: Self Forcing通过创新的训练策略，有效解决了曝光偏差问题，并在保持高效计算的同时，提升了视频生成的质量和实时性。

Abstract: We introduce Self Forcing, a novel training paradigm for autoregressive video
diffusion models. It addresses the longstanding issue of exposure bias, where
models trained on ground-truth context must generate sequences conditioned on
their own imperfect outputs during inference. Unlike prior methods that denoise
future frames based on ground-truth context frames, Self Forcing conditions
each frame's generation on previously self-generated outputs by performing
autoregressive rollout with key-value (KV) caching during training. This
strategy enables supervision through a holistic loss at the video level that
directly evaluates the quality of the entire generated sequence, rather than
relying solely on traditional frame-wise objectives. To ensure training
efficiency, we employ a few-step diffusion model along with a stochastic
gradient truncation strategy, effectively balancing computational cost and
performance. We further introduce a rolling KV cache mechanism that enables
efficient autoregressive video extrapolation. Extensive experiments demonstrate
that our approach achieves real-time streaming video generation with sub-second
latency on a single GPU, while matching or even surpassing the generation
quality of significantly slower and non-causal diffusion models. Project
website: http://self-forcing.github.io/

</details>


### [569] [StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning from Partially Annotated Synthetic Datasets](https://arxiv.org/abs/2506.08013)
*Anh-Quan Cao,Ivan Lopes,Raoul de Charette*

Main category: cs.CV

TL;DR: StableMTL利用扩散模型的泛化能力，在零样本设置下训练多任务模型，通过任务编码和多流模型实现跨任务共享，显著提升了多任务密集预测的性能。


<details>
  <summary>Details</summary>
Motivation: 多任务学习在密集预测任务中面临大量标注数据的挑战，本文旨在通过扩散模型的泛化能力，探索在部分任务标注下的零样本学习。

Method: StableMTL方法利用图像生成器进行潜在回归，采用去噪框架、任务编码和定制训练方案，通过统一潜在损失和多流模型中的任务注意力机制，实现跨任务共享。

Result: StableMTL在8个基准测试中的7个任务上均优于基线模型，展示了其在多任务密集预测中的有效性。

Conclusion: StableMTL通过扩散模型和任务注意力机制，成功实现了零样本多任务学习，显著提升了多任务密集预测的性能。

Abstract: Multi-task learning for dense prediction is limited by the need for extensive
annotation for every task, though recent works have explored training with
partial task labels. Leveraging the generalization power of diffusion models,
we extend the partial learning setup to a zero-shot setting, training a
multi-task model on multiple synthetic datasets, each labeled for only a subset
of tasks. Our method, StableMTL, repurposes image generators for latent
regression. Adapting a denoising framework with task encoding, per-task
conditioning and a tailored training scheme. Instead of per-task losses
requiring careful balancing, a unified latent loss is adopted, enabling
seamless scaling to more tasks. To encourage inter-task synergy, we introduce a
multi-stream model with a task-attention mechanism that converts N-to-N task
interactions into efficient 1-to-N attention, promoting effective cross-task
sharing. StableMTL outperforms baselines on 7 tasks across 8 benchmarks.

</details>


### [570] [Decoupling the Image Perception and Multimodal Reasoning for Reasoning Segmentation with Digital Twin Representations](https://arxiv.org/abs/2506.07943)
*Yizhen Li,Dell Zhang,Xuelong Li,Yiqing Shen*

Main category: cs.CV

TL;DR: DTwinSeger提出一种基于数字孪生（DT）表示的两阶段推理分割方法，通过解耦感知与推理，利用LLM在结构化DT表示上进行显式推理，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型（VLM）的推理分割方法因图像分块化处理破坏了物体间连续空间关系，限制了多模态推理能力。

Method: 1. 将图像转换为保留空间关系的结构化DT表示；2. 使用LLM对DT表示进行显式推理。提出针对LLM的监督微调方法及数据集Seg-DT。

Result: 在两个图像推理分割基准和三个图像指代分割基准上取得最优性能，验证了DT表示作为视觉-文本桥梁的有效性。

Conclusion: DT表示能有效连接视觉与文本模态，仅通过LLM即可完成复杂多模态推理任务，为视觉-语言交互提供了新范式。

Abstract: Reasoning Segmentation (RS) is a multimodal vision-text task that requires
segmenting objects based on implicit text queries, demanding both precise
visual perception and vision-text reasoning capabilities. Current RS approaches
rely on fine-tuning vision-language models (VLMs) for both perception and
reasoning, but their tokenization of images fundamentally disrupts continuous
spatial relationships between objects. We introduce DTwinSeger, a novel RS
approach that leverages Digital Twin (DT) representation as an intermediate
layer to decouple perception from reasoning. Innovatively, DTwinSeger
reformulates RS as a two-stage process, where the first transforms the image
into a structured DT representation that preserves spatial relationships and
semantic properties and then employs a Large Language Model (LLM) to perform
explicit reasoning over this representation to identify target objects. We
propose a supervised fine-tuning method specifically for LLM with DT
representation, together with a corresponding fine-tuning dataset Seg-DT, to
enhance the LLM's reasoning capabilities with DT representations. Experiments
show that our method can achieve state-of-the-art performance on two image RS
benchmarks and three image referring segmentation benchmarks. It yields that DT
representation functions as an effective bridge between vision and text,
enabling complex multimodal reasoning tasks to be accomplished solely with an
LLM.

</details>


### [571] [SlideCoder: Layout-aware RAG-enhanced Hierarchical Slide Generation from Design](https://arxiv.org/abs/2506.07964)
*Wenxin Tang,Jingyu Xiao,Wenxuan Jiang,Xi Xiao,Yuhang Wang,Xuxin Tang,Qing Li,Yuehe Ma,Junliang Liu,Shisong Tang,Michael R. Lyu*

Main category: cs.CV

TL;DR: 本文提出Slide2Code基准及SlideCoder框架，通过布局感知的检索增强方法从参考图像生成可编辑幻灯片，并发布开源模型SlideMaster，在布局保真度、执行准确性和视觉一致性上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统手动制作幻灯片耗时且依赖专家知识，现有基于自然语言的LLM生成方法难以捕捉幻灯片设计的视觉与结构细节。

Method: 提出SlideCoder框架，结合颜色梯度分割算法与分层检索增强生成方法，分解复杂任务并增强代码生成能力；发布7B开源模型SlideMaster。

Result: 实验显示SlideCoder在布局保真度、执行准确性和视觉一致性上比现有最优方法提升达40.5分。

Conclusion: SlideCoder通过创新性任务拆解与检索增强机制，建立了参考图像到可编辑幻灯片的高效生成范式，为自动化幻灯片设计提供了新基准。

Abstract: Manual slide creation is labor-intensive and requires expert prior knowledge.
Existing natural language-based LLM generation methods struggle to capture the
visual and structural nuances of slide designs. To address this, we formalize
the Reference Image to Slide Generation task and propose Slide2Code, the first
benchmark with difficulty-tiered samples based on a novel Slide Complexity
Metric. We introduce SlideCoder, a layout-aware, retrieval-augmented framework
for generating editable slides from reference images. SlideCoder integrates a
Color Gradient-based Segmentation algorithm and a Hierarchical
Retrieval-Augmented Generation method to decompose complex tasks and enhance
code generation. We also release SlideMaster, a 7B open-source model fine-tuned
with improved reverse-engineered data. Experiments show that SlideCoder
outperforms state-of-the-art baselines by up to 40.5 points, demonstrating
strong performance across layout fidelity, execution accuracy, and visual
consistency. Our code is available at
https://github.com/vinsontang1/SlideCoder.

</details>


### [572] [Audio-Sync Video Generation with Multi-Stream Temporal Control](https://arxiv.org/abs/2506.08003)
*Shuchen Weng,Haojie Zheng,Zheng Chang,Si Li,Boxin Shi,Xinlong Wang*

Main category: cs.CV

TL;DR: 本文提出MTV框架，通过分离音频轨道实现细粒度音视频同步生成，并构建DEMIX数据集支持多阶段训练，在视频质量、一致性及音画同步方面达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂音频类型下难以生成高质量且音画精准同步的视频，需解决音频作为控制信号与视觉叙事直接转换的挑战。

Method: MTV框架将音频解耦为语音/音效/音乐轨道，分别控制唇形、事件时序与视觉氛围；配合多阶段训练数据集DEMIX（含电影级视频与分离音轨）。

Result: 实验表明MTV在视频质量、文本-视频一致性及音画对齐等6项指标上均达到当前最优性能。

Conclusion: MTV通过音轨解耦与结构化数据集实现了跨复杂音频类型的精准可控视频生成，为音画同步任务提供了高效解决方案。

Abstract: Audio is inherently temporal and closely synchronized with the visual world,
making it a naturally aligned and expressive control signal for controllable
video generation (e.g., movies). Beyond control, directly translating audio
into video is essential for understanding and visualizing rich audio narratives
(e.g., Podcasts or historical recordings). However, existing approaches fall
short in generating high-quality videos with precise audio-visual
synchronization, especially across diverse and complex audio types. In this
work, we introduce MTV, a versatile framework for audio-sync video generation.
MTV explicitly separates audios into speech, effects, and music tracks,
enabling disentangled control over lip motion, event timing, and visual mood,
respectively -- resulting in fine-grained and semantically aligned video
generation. To support the framework, we additionally present DEMIX, a dataset
comprising high-quality cinematic videos and demixed audio tracks. DEMIX is
structured into five overlapped subsets, enabling scalable multi-stage training
for diverse generation scenarios. Extensive experiments demonstrate that MTV
achieves state-of-the-art performance across six standard metrics spanning
video quality, text-video consistency, and audio-video alignment. Project page:
https://hjzheng.net/projects/MTV/.

</details>


### [573] [Dynamic View Synthesis as an Inverse Problem](https://arxiv.org/abs/2506.08004)
*Hidir Yesiltepe,Pinar Yanardag*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的动态视图合成方法，通过重新设计预训练视频扩散模型的噪声初始化阶段，实现了高保真的动态视图合成。


<details>
  <summary>Details</summary>
Motivation: 为了解决单目视频动态视图合成中的逆问题，本文旨在在无需训练的情况下实现高保真的动态视图合成。

Method: 本文引入了K阶递归噪声表示，解决了零终端信噪比调度带来的确定性反演障碍，并提出了随机潜在调制，通过潜在空间中的可见性感知采样来补全遮挡区域。

Result: 实验表明，通过噪声初始化阶段的结构化潜在操作，可以有效地进行动态视图合成。

Conclusion: 本文提出的方法在无需更新权重或辅助模块的情况下，成功实现了高保真的动态视图合成，展示了噪声初始化阶段结构化潜在操作的有效性。

Abstract: In this work, we address dynamic view synthesis from monocular videos as an
inverse problem in a training-free setting. By redesigning the noise
initialization phase of a pre-trained video diffusion model, we enable
high-fidelity dynamic view synthesis without any weight updates or auxiliary
modules. We begin by identifying a fundamental obstacle to deterministic
inversion arising from zero-terminal signal-to-noise ratio (SNR) schedules and
resolve it by introducing a novel noise representation, termed K-order
Recursive Noise Representation. We derive a closed form expression for this
representation, enabling precise and efficient alignment between the
VAE-encoded and the DDIM inverted latents. To synthesize newly visible regions
resulting from camera motion, we introduce Stochastic Latent Modulation, which
performs visibility aware sampling over the latent space to complete occluded
regions. Comprehensive experiments demonstrate that dynamic view synthesis can
be effectively performed through structured latent manipulation in the noise
initialization phase.

</details>


### [574] [Vision Transformers Don't Need Trained Registers](https://arxiv.org/abs/2506.08010)
*Nick Jiang,Amil Dravid,Alexei Efros,Yossi Gandelsman*

Main category: cs.CV

TL;DR: 提出无需重新训练的测试时注册方法，解决Vision Transformers中高范数令牌导致的注意力噪声问题，提升下游任务表现和模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法需通过重新训练模型并添加注册令牌来消除异常高范数令牌，但本文旨在探索无需重新训练的解决方案。

Method: 通过将已识别的高范数激活从特定神经元转移到额外未训练令牌，模拟注册令牌效果，适用于任何预训练模型。

Result: 方法使注意力图更清晰、特征图质量提升，下游任务性能优于基准模型，且与显式训练注册令牌的模型效果相当。

Conclusion: 测试时注册机制可替代显式注册令牌，为未预置注册令牌的模型提供零训练成本的优化方案。

Abstract: We investigate the mechanism underlying a previously identified phenomenon in
Vision Transformers -- the emergence of high-norm tokens that lead to noisy
attention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a
sparse set of neurons is responsible for concentrating high-norm activations on
outlier tokens, leading to irregular attention patterns and degrading
downstream visual processing. While the existing solution for removing these
outliers involves retraining models from scratch with additional learned
register tokens, we use our findings to create a training-free approach to
mitigate these artifacts. By shifting the high-norm activations from our
discovered register neurons into an additional untrained token, we can mimic
the effect of register tokens on a model already trained without registers. We
demonstrate that our method produces cleaner attention and feature maps,
enhances performance over base models across multiple downstream visual tasks,
and achieves results comparable to models explicitly trained with register
tokens. We then extend test-time registers to off-the-shelf vision-language
models to improve their interpretability. Our results suggest that test-time
registers effectively take on the role of register tokens at test-time,
offering a training-free solution for any pre-trained model released without
them.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [575] [KramaBench: A Benchmark for AI Systems on Data-to-Insight Pipelines over Data Lakes](https://arxiv.org/abs/2506.06541)
*Eugenie Lai,Gerardo Vitagliano,Ziyu Zhang,Sivaprasad Sudhir,Om Chabra,Anna Zeng,Anton A. Zabreyko,Chenning Li,Ferdi Kossmann,Jialin Ding,Jun Chen,Markos Markakis,Matthew Russo,Weiyang Wang,Ziniu Wu,Michael J. Cafarella,Lei Cao,Samuel Madden,Tim Kraska*

Main category: cs.DB

TL;DR: KRAMABENCH是一个包含104个真实世界数据科学管道的基准，用于测试AI系统在数据处理中的端到端能力。现有模型在处理复杂管道时仍显不足。


<details>
  <summary>Details</summary>
Motivation: 构建真实世界的数据到洞察的管道涉及数据提取、整合和多样化操作，设计这些管道需要领域知识和技术专长。尽管AI系统在推理、编码和理解方面表现出色，但它们在设计和执行复杂管道方面的能力尚不明确。

Method: 引入KRAMABENCH基准，包含104个手动策划的真实世界数据科学管道，涵盖6个不同领域的24个数据源。使用DS-GURU框架评估5个通用模型和3个代码生成模型。

Result: 现有模型能够解决明确的数据科学代码生成任务，但在需要大量数据处理和领域知识构建真实世界数据科学管道时表现不足。

Conclusion: KRAMABENCH的研究是开发自主数据科学代理的关键步骤，代码、参考框架和数据已公开。

Abstract: Constructing real-world data-to-insight pipelines often involves data
extraction from data lakes, data integration across heterogeneous data sources,
and diverse operations from data cleaning to analysis. The design and
implementation of data science pipelines require domain knowledge, technical
expertise, and even project-specific insights. AI systems have shown remarkable
reasoning, coding, and understanding capabilities. However, it remains unclear
to what extent these capabilities translate into successful design and
execution of such complex pipelines. We introduce KRAMABENCH: a benchmark
composed of 104 manually-curated real-world data science pipelines spanning
1700 data files from 24 data sources in 6 different domains. We show that these
pipelines test the end-to-end capabilities of AI systems on data processing,
requiring data discovery, wrangling and cleaning, efficient processing,
statistical reasoning, and orchestrating data processing steps given a
high-level task. Our evaluation tests 5 general models and 3 code generation
models using our reference framework, DS-GURU, which instructs the AI model to
decompose a question into a sequence of subtasks, reason through each step, and
synthesize Python code that implements the proposed design. Our results on
KRAMABENCH show that, although the models are sufficiently capable of solving
well-specified data science code generation tasks, when extensive data
processing and domain knowledge are required to construct real-world data
science pipelines, existing out-of-box models fall short. Progress on
KramaBench represents crucial steps towards developing autonomous data science
agents for real-world applications. Our code, reference framework, and data are
available at https://github.com/mitdbg/KramaBench.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [576] [Disentangling AI Alignment: A Structured Taxonomy Beyond Safety and Ethics](https://arxiv.org/abs/2506.06286)
*Kevin Baum*

Main category: cs.CY

TL;DR: 本文提出了一个结构化概念框架，用于理解AI对齐，区分了对齐目标、范围和利益相关者，揭示了多种合法的对齐配置。


<details>
  <summary>Details</summary>
Motivation: 随着AI研究的进展，人工代理在现实世界中的影响日益显著，确保其不仅安全且符合更广泛的规范期望成为紧迫的跨学科挑战。然而，相关领域的概念边界和相互关系仍不清晰，研究者缺乏明确的指导。

Method: 本文开发了一个结构化概念框架，区分了对齐目标（如安全性、伦理性、合法性等）、范围（结果与执行）和利益相关者（个体与集体）。

Result: 该框架揭示了多种合法的对齐配置，为跨领域的实践和哲学整合提供了基础，并阐明了“全面对齐”的含义。

Conclusion: 本文提出的框架为AI对齐研究提供了清晰的结构化指导，有助于跨领域整合，并明确了全面对齐的可能含义。

Abstract: Recent advances in AI research make it increasingly plausible that artificial
agents with consequential real-world impact will soon operate beyond tightly
controlled environments. Ensuring that these agents are not only safe but that
they adhere to broader normative expectations is thus an urgent
interdisciplinary challenge. Multiple fields -- notably AI Safety, AI
Alignment, and Machine Ethics -- claim to contribute to this task. However, the
conceptual boundaries and interrelations among these domains remain vague,
leaving researchers without clear guidance in positioning their work.
  To address this meta-challenge, we develop a structured conceptual framework
for understanding AI alignment. Rather than focusing solely on alignment goals,
we introduce a taxonomy distinguishing the alignment aim (safety, ethicality,
legality, etc.), scope (outcome vs. execution), and constituency (individual
vs. collective). This structural approach reveals multiple legitimate alignment
configurations, providing a foundation for practical and philosophical
integration across domains, and clarifying what it might mean for an agent to
be aligned all-things-considered.

</details>


### [577] [How Malicious AI Swarms Can Threaten Democracy](https://arxiv.org/abs/2506.06299)
*Daniel Thilo Schroeder,Meeyoung Cha,Andrea Baronchelli,Nick Bostrom,Nicholas A. Christakis,David Garcia,Amit Goldenberg,Yara Kyrychenko,Kevin Leyton-Brown,Nina Lutz,Gary Marcus,Filippo Menczer,Gordon Pennycook,David G. Rand,Frank Schweitzer,Christopher Summerfield,Audrey Tang,Jay Van Bavel,Sander van der Linden,Dawn Song,Jonas R. Kunst*

Main category: cs.CY

TL;DR: AI技术的进步可能导致复杂的虚假信息操作，恶意AI群体可能协调行动、渗透社区、逃避检测，并对民主进程构成威胁。文章提出三方面应对措施：平台防御、模型保护和系统监督。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的发展，恶意AI群体可能对民主进程和社会信任构成严重威胁，因此需要提出有效的应对策略。

Method: 文章提出三方面应对措施：平台防御（如群体检测仪表盘、透明度审计）、模型保护（如标准化说服风险测试、水印技术）和系统监督（如联合国支持的AI影响观察站）。

Result: 通过实施这些措施，可以有效应对恶意AI群体带来的威胁，保护民主进程和社会信任。

Conclusion: 面对AI技术带来的新挑战，必须采取多层次的防御和监督措施，以确保社会的稳定和民主的健康发展。

Abstract: Advances in AI portend a new era of sophisticated disinformation operations.
While individual AI systems already create convincing -- and at times
misleading -- information, an imminent development is the emergence of
malicious AI swarms. These systems can coordinate covertly, infiltrate
communities, evade traditional detectors, and run continuous A/B tests, with
round-the-clock persistence. The result can include fabricated grassroots
consensus, fragmented shared reality, mass harassment, voter micro-suppression
or mobilization, contamination of AI training data, and erosion of
institutional trust. With democratic processes worldwide increasingly
vulnerable, we urge a three-pronged response: (1) platform-side defenses --
always-on swarm-detection dashboards, pre-election high-fidelity
swarm-simulation stress-tests, transparency audits, and optional client-side
"AI shields" for users; (2) model-side safeguards -- standardized
persuasion-risk tests, provenance-authenticating passkeys, and watermarking;
and (3) system-level oversight -- a UN-backed AI Influence Observatory.

</details>


### [578] [Human and AI collaboration in Fitness Education:A Longitudinal Study with a Pilates Instructor](https://arxiv.org/abs/2506.06383)
*Qian Huang,King Wang Poon*

Main category: cs.CY

TL;DR: 本研究通过为期一年的质性案例研究，探讨生成式AI如何与普拉提教练协作优化健身教育，结合参与式观察与半结构化访谈分析人机协同模式。


<details>
  <summary>Details</summary>
Motivation: 人工智能正在改变教学与教练领域，但其与人类专业知识的协同机制尚不明确。研究旨在探索AI在健身教育中与人类专家的协作边界与整合方式。

Method: 采用参与式行动研究方法，研究者全年参与普拉提课程并进行双周半结构化访谈，系统性收集AI在课程规划与教学实践中的应用数据。

Result: 研究揭示了生成式AI在个性化课程设计、实时动作矫正提示等场景化应用潜力，同时识别出人类教师在情感支持与复杂决策中的不可替代性。

Conclusion: 人机协同需通过动态角色分配实现优势互补，AI作为增强工具而非替代方案时，能最大程度提升健身教育效果，但需建立明确的协作框架与责任边界。

Abstract: Artificial intelligence is poised to transform teaching and coaching
practices,yet its optimal role alongside human expertise remains unclear.This
study investigates human and AI collaboration in fitness education through a
one year qualitative case study with a Pilates instructor.The researcher
participated in the instructor classes and conducted biweekly semi structured
interviews to explore how generative AI could be integrated into class planning
and instruction.

</details>


### [579] [Benchmarking Large Language Models on Homework Assessment in Circuit Analysis](https://arxiv.org/abs/2506.06390)
*Liangliang Chen,Zhihao Qin,Yiming Guo,Jacqueline Rohde,Ying Zhang*

Main category: cs.CY

TL;DR: 本研究评估了GPT-3.5 Turbo、GPT-4o和Llama 3 70B在电路分析课程作业批改中的表现，发现GPT-4o与Llama 3 70B优于GPT-3.5 Turbo，并揭示了当前大语言模型在工程教育应用中的局限性。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型（LLMs）的广泛先验知识与快速进步潜力，探索其在工程教育中作为个性化辅导工具的可行性，重点解决作业评估的可靠性问题。

Method: 构建包含标准答案与学生实际解题的LaTeX格式数据集，设计提示模板评估解题指标（完整性、方法、答案、计算错误、单位），对比不同LLM在电路分析作业批改中的表现。

Result: GPT-4o与Llama 3 70B在五项指标上显著优于GPT-3.5 Turbo，两者在不同评估维度各具优势。研究同时揭示了当前LLM在电路分析中的局限性。

Conclusion: 研究为开发可靠的电路分析个性化辅导工具提供了基准与洞见，所提评估方法可扩展至更广泛的工程教育课程，但需优先解决LLM输出可靠性问题以避免误导学生。

Abstract: Large language models (LLMs) have the potential to revolutionize various
fields, including code development, robotics, finance, and education, due to
their extensive prior knowledge and rapid advancements. This paper investigates
how LLMs can be leveraged in engineering education. Specifically, we benchmark
the capabilities of different LLMs, including GPT-3.5 Turbo, GPT-4o, and Llama
3 70B, in assessing homework for an undergraduate-level circuit analysis
course. We have developed a novel dataset consisting of official reference
solutions and real student solutions to problems from various topics in circuit
analysis. To overcome the limitations of image recognition in current
state-of-the-art LLMs, the solutions in the dataset are converted to LaTeX
format. Using this dataset, a prompt template is designed to test five metrics
of student solutions: completeness, method, final answer, arithmetic error, and
units. The results show that GPT-4o and Llama 3 70B perform significantly
better than GPT-3.5 Turbo across all five metrics, with GPT-4o and Llama 3 70B
each having distinct advantages in different evaluation aspects. Additionally,
we present insights into the limitations of current LLMs in several aspects of
circuit analysis. Given the paramount importance of ensuring reliability in
LLM-generated homework assessment to avoid misleading students, our results
establish benchmarks and offer valuable insights for the development of a
reliable, personalized tutor for circuit analysis -- a focus of our future
work. Furthermore, the proposed evaluation methods can be generalized to a
broader range of courses for engineering education in the future.

</details>


### [580] [From Rogue to Safe AI: The Role of Explicit Refusals in Aligning LLMs with International Humanitarian Law](https://arxiv.org/abs/2506.06391)
*John Mavi,Diana Teodora Găitan,Sergio Coronado*

Main category: cs.CY

TL;DR: 研究评估了8个主流大语言模型（LLM）拒绝违反国际人道法（IHL）请求的能力及其回应的清晰度。标准化安全干预可提升解释质量，但复杂提示仍暴露漏洞。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM被广泛应用，但其对国际人道法的合规性尚不明确。研究旨在评估模型拒绝非法请求的能力，并分析其回应的明确性与建设性。

Method: 测试8个领先LLM对明确违反IHL的提示的拒绝能力，引入标准化系统级安全提示，并评估其对解释质量的改进效果。

Result: 多数模型能拒绝非法请求，但回应清晰度参差；标准化提示显著提升解释质量，但涉及技术语言或代码的复杂提示仍存在漏洞。

Conclusion: 解释性拒绝可增强AI系统透明度和安全性，轻量级干预有效但需解决复杂场景漏洞。研究提出评估LLM合规IHL的基准框架。

Abstract: Large Language Models (LLMs) are widely used across sectors, yet their
alignment with International Humanitarian Law (IHL) is not well understood.
This study evaluates eight leading LLMs on their ability to refuse prompts that
explicitly violate these legal frameworks, focusing also on helpfulness - how
clearly and constructively refusals are communicated. While most models
rejected unlawful requests, the clarity and consistency of their responses
varied. By revealing the model's rationale and referencing relevant legal or
safety principles, explanatory refusals clarify the system's boundaries, reduce
ambiguity, and help prevent misuse. A standardised system-level safety prompt
significantly improved the quality of the explanations expressed within
refusals in most models, highlighting the effectiveness of lightweight
interventions. However, more complex prompts involving technical language or
requests for code revealed ongoing vulnerabilities. These findings contribute
to the development of safer, more transparent AI systems and propose a
benchmark to evaluate the compliance of LLM with IHL.

</details>


### [581] [LLMs as World Models: Data-Driven and Human-Centered Pre-Event Simulation for Disaster Impact Assessment](https://arxiv.org/abs/2506.06355)
*Lingyao Li,Dawei Li,Zhenhui Ou,Xiaoran Xu,Jingxiao Liu,Zihui Ma,Runlong Yu,Min Deng*

Main category: cs.CY

TL;DR: 该研究利用大语言模型（LLMs）模拟地震影响，通过多模态数据集生成修正麦加利烈度（MMI）预测，并在2014年纳帕和2019年里奇克莱斯特地震中验证了其高相关性。


<details>
  <summary>Details</summary>
Motivation: 提高对突发性灾害（如地震）的主动准备能力，需要高效的模拟方法。大语言模型（LLMs）作为世界模型的最新进展，展示了模拟复杂场景的潜力。

Method: 研究使用多模态数据集（包括地理空间、社会经济、建筑和街景图像数据），生成修正麦加利烈度（MMI）预测，并利用RAG和ICL技术提升模拟性能。

Result: 在2014年纳帕和2019年里奇克莱斯特地震中，模拟结果与美国地质调查局（USGS）的“你感觉到了吗？（DYFI）”报告高度一致，相关系数为0.88，RMSE为0.77。

Conclusion: 研究表明，大语言模型在模拟灾害影响方面具有潜力，有助于加强事前规划，视觉输入相比结构化数值数据显著提高了准确性。

Abstract: Efficient simulation is essential for enhancing proactive preparedness for
sudden-onset disasters such as earthquakes. Recent advancements in large
language models (LLMs) as world models show promise in simulating complex
scenarios. This study examines multiple LLMs to proactively estimate perceived
earthquake impacts. Leveraging multimodal datasets including geospatial,
socioeconomic, building, and street-level imagery data, our framework generates
Modified Mercalli Intensity (MMI) predictions at zip code and county scales.
Evaluations on the 2014 Napa and 2019 Ridgecrest earthquakes using USGS ''Did
You Feel It? (DYFI)'' reports demonstrate significant alignment, as evidenced
by a high correlation of 0.88 and a low RMSE of 0.77 as compared to real
reports at the zip code level. Techniques such as RAG and ICL can improve
simulation performance, while visual inputs notably enhance accuracy compared
to structured numerical data alone. These findings show the promise of LLMs in
simulating disaster impacts that can help strengthen pre-event planning.

</details>


### [582] [Large Language Models Can Be a Viable Substitute for Expert Political Surveys When a Shock Disrupts Traditional Measurement Approaches](https://arxiv.org/abs/2506.06540)
*Patrick Y. Wu*

Main category: cs.CY

TL;DR: 本文探讨了在突发事件（如2025年DOGE联邦裁员）后，专家判断受结果影响，难以重建事前认知。提出使用大型语言模型（LLMs）替代专家政治调查，通过案例研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于突发事件后，专家判断受结果影响，难以重建事前认知，传统测量方法失效，需寻找替代方案。

Method: 使用LLMs进行成对比较提示，推导联邦行政机构的意识形态得分，并分析哪些机构被DOGE针对。

Result: LLMs得出的意识形态得分复制了裁员前的专家测量，并预测了被DOGE针对的机构，同时发现某些联邦机构作为知识机构的认知也预测了被针对的情况。

Conclusion: LLMs在传统测量方法失效时，能快速测试与突发事件相关的假设因素，为研究者提供替代专家政治调查的可行方案。

Abstract: After a disruptive event or shock, such as the Department of Government
Efficiency (DOGE) federal layoffs of 2025, expert judgments are colored by
knowledge of the outcome. This can make it difficult or impossible to
reconstruct the pre-event perceptions needed to study the factors associated
with the event. This position paper argues that large language models (LLMs),
trained on vast amounts of digital media data, can be a viable substitute for
expert political surveys when a shock disrupts traditional measurement. We
analyze the DOGE layoffs as a specific case study for this position. We use
pairwise comparison prompts with LLMs and derive ideology scores for federal
executive agencies. These scores replicate pre-layoff expert measures and
predict which agencies were targeted by DOGE. We also use this same approach
and find that the perceptions of certain federal agencies as knowledge
institutions predict which agencies were targeted by DOGE, even when
controlling for ideology. This case study demonstrates that using LLMs allows
us to rapidly and easily test the associated factors hypothesized behind the
shock. More broadly, our case study of this recent event exemplifies how LLMs
offer insights into the correlational factors of the shock when traditional
measurement techniques fail. We conclude by proposing a two-part criterion for
when researchers can turn to LLMs as a substitute for expert political surveys.

</details>


### [583] [Future of Work with AI Agents: Auditing Automation and Augmentation Potential across the U.S. Workforce](https://arxiv.org/abs/2506.06576)
*Yijia Shao,Humishka Zope,Yucheng Jiang,Jiaxin Pei,David Nguyen,Erik Brynjolfsson,Diyi Yang*

Main category: cs.CY

TL;DR: 本文提出了一种新的审计框架，评估工人希望AI自动化的任务，并构建了WORKBank数据库，揭示了AI代理开发中的关键不匹配和机会。


<details>
  <summary>Details</summary>
Motivation: 复合AI系统的迅速崛起正在重塑劳动力市场，引发了关于工作替代、人类能动性减弱和过度依赖自动化的担忧，但目前缺乏对这一演变景观的系统理解。

Method: 本文引入了一种新的审计框架，包括音频增强的迷你访谈和人类能动性量表（HAS），并基于美国劳工部的O*NET数据库构建了WORKBank数据库，收集了1500名领域工人的偏好和AI专家对844项任务的评估。

Result: 研究将任务分为四个区域：自动化“绿灯”区、自动化“红灯”区、研发机会区和低优先级区，揭示了不同职业对人类参与的异质期望，并提供了AI代理整合可能如何重塑核心人类能力的早期信号。

Conclusion: 研究强调了将AI代理开发与人类期望对齐的重要性，并为工人适应不断变化的工作环境做好准备。

Abstract: The rapid rise of compound AI systems (a.k.a., AI agents) is reshaping the
labor market, raising concerns about job displacement, diminished human agency,
and overreliance on automation. Yet, we lack a systematic understanding of the
evolving landscape. In this paper, we address this gap by introducing a novel
auditing framework to assess which occupational tasks workers want AI agents to
automate or augment, and how those desires align with the current technological
capabilities. Our framework features an audio-enhanced mini-interview to
capture nuanced worker desires and introduces the Human Agency Scale (HAS) as a
shared language to quantify the preferred level of human involvement. Using
this framework, we construct the WORKBank database, building on the U.S.
Department of Labor's O*NET database, to capture preferences from 1,500 domain
workers and capability assessments from AI experts across over 844 tasks
spanning 104 occupations. Jointly considering the desire and technological
capability divides tasks in WORKBank into four zones: Automation "Green Light"
Zone, Automation "Red Light" Zone, R&D Opportunity Zone, Low Priority Zone.
This highlights critical mismatches and opportunities for AI agent development.
Moving beyond a simple automate-or-not dichotomy, our results reveal diverse
HAS profiles across occupations, reflecting heterogeneous expectations for
human involvement. Moreover, our study offers early signals of how AI agent
integration may reshape the core human competencies, shifting from
information-focused skills to interpersonal ones. These findings underscore the
importance of aligning AI agent development with human desires and preparing
workers for evolving workplace dynamics.

</details>


### [584] [Position: Simulating Society Requires Simulating Thought](https://arxiv.org/abs/2506.06958)
*Chance Jiajie Li,Jiayi Wu,Zhenze Mo,Ao Qu,Yuhan Tang,Kaiya Ivy Zhao,Yulu Gan,Jie Fan,Jiangbo Yu,Jinhua Zhao,Paul Liang,Luis Alonso,Kent Larson*

Main category: cs.CY

TL;DR: 论文提出了一种基于认知科学的概念建模范式（GenMinds），并引入了RECAP框架来评估生成代理的推理能力，旨在从表面模仿转向模拟思维的生成代理。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型（LLMs）的代理在模拟个体和群体行为时，缺乏内部一致性、因果推理和信念可追溯性，导致其在分析人类推理、决策或干预反应时不可靠。

Method: 提出了Generative Minds（GenMinds）概念建模范式，并引入了RECAP框架，通过因果可追溯性、人口统计基础和干预一致性来评估生成代理的推理能力。

Result: GenMinds和RECAP框架能够支持生成代理中的结构化信念表示，并提高其推理的保真度。

Conclusion: 该研究推动了从表面模仿到模拟思维的生成代理的转变，为社会科学模拟提供了更可靠的工具。

Abstract: Simulating society with large language models (LLMs), we argue, requires more
than generating plausible behavior -- it demands cognitively grounded reasoning
that is structured, revisable, and traceable. LLM-based agents are increasingly
used to emulate individual and group behavior -- primarily through prompting
and supervised fine-tuning. Yet they often lack internal coherence, causal
reasoning, and belief traceability -- making them unreliable for analyzing how
people reason, deliberate, or respond to interventions.
  To address this, we present a conceptual modeling paradigm, Generative Minds
(GenMinds), which draws from cognitive science to support structured belief
representations in generative agents. To evaluate such agents, we introduce the
RECAP (REconstructing CAusal Paths) framework, a benchmark designed to assess
reasoning fidelity via causal traceability, demographic grounding, and
intervention consistency. These contributions advance a broader shift: from
surface-level mimicry to generative agents that simulate thought -- not just
language -- for social simulations.

</details>


### [585] [Evaluating Large Language Model Capabilities in Assessing Spatial Econometrics Research](https://arxiv.org/abs/2506.06377)
*Giuseppe Arbia,Luca Morandini,Vincenzo Nardelli*

Main category: cs.CY

TL;DR: 研究探讨大语言模型（LLMs）评估空间计量经济学实证结果的经济合理性与理论一致性的能力，发现其在变量选择等表层任务表现优异，但深层经济推理仍依赖人类监督。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在学术研究评估中的潜力，尤其是其对变量选择、系数合理性等计量经济学核心要素的判断能力，以验证其在辅助同行评审中的可行性。

Method: 基于28篇已发表论文（2005-2024）生成原始及人工篡改的'反事实'摘要，使用多种LLMs进行定性评估和结构化二元分类（变量选择、系数合理性、发表适宜性）。

Result: LLMs（如GPT-4o）在变量选择评估中表现卓越（F1=0.87），但评估系数合理性和发表适宜性时性能显著波动，模型选择、论文特性及其交互作用对结果精度有显著影响。

Conclusion: LLMs当前适用于辅助表层审查（如变量一致性检查），但深度经济推理仍需人类主导，未来可在需强人工监督的同行评审中发挥有限辅助作用。

Abstract: This paper investigates Large Language Models (LLMs) ability to assess the
economic soundness and theoretical consistency of empirical findings in spatial
econometrics. We created original and deliberately altered "counterfactual"
summaries from 28 published papers (2005-2024), which were evaluated by a
diverse set of LLMs. The LLMs provided qualitative assessments and structured
binary classifications on variable choice, coefficient plausibility, and
publication suitability. The results indicate that while LLMs can expertly
assess the coherence of variable choices (with top models like GPT-4o achieving
an overall F1 score of 0.87), their performance varies significantly when
evaluating deeper aspects such as coefficient plausibility and overall
publication suitability. The results further revealed that the choice of LLM,
the specific characteristics of the paper and the interaction between these two
factors significantly influence the accuracy of the assessment, particularly
for nuanced judgments. These findings highlight LLMs' current strengths in
assisting with initial, more surface-level checks and their limitations in
performing comprehensive, deep economic reasoning, suggesting a potential
assistive role in peer review that still necessitates robust human oversight.

</details>


### [586] [Deepfake Technology Unveiled: The Commoditization of AI and Its Impact on Digital Trust](https://arxiv.org/abs/2506.07363)
*Claudiu Popa,Rex Pallath,Liam Cunningham,Hewad Tahiri,Abiram Kesavarajah,Tao Wu*

Main category: cs.CY

TL;DR: 生成式AI的普及使深度伪造技术门槛降低，虽带来创新机会，但也加剧了欺诈、错误信息及数字信任危机。本文通过低成本工具演示深度伪造风险，呼吁加强监管与协作应对挑战。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI工具（如语音克隆、换脸）的易用性和成本降低，深度伪造技术被滥用于欺诈和虚假信息，威胁数字媒体真实性。需研究其影响及应对策略以维护数字信任。

Method: 使用Runway、Rope、ElevenLabs等低成本工具创建逼真深度伪造案例，分析技术滥用风险，并探讨检测与治理的技术与伦理挑战。

Result: 证明即使资源有限也可生成高仿真伪造内容，现有检测手段存在技术局限，且缺乏有效监管框架，个人与组织均面临严重信任与安全风险。

Conclusion: 亟需建立法规、提升公众认知，并通过多方协作开发可靠检测技术，以应对深度伪造对数字信任的侵蚀，确保多媒体真实性。

Abstract: Deepfake Technology Unveiled: The Commoditization of AI and Its Impact on
Digital Trust. With the increasing accessibility of generative AI, tools for
voice cloning, face-swapping, and synthetic media creation have advanced
significantly, lowering both financial and technical barriers for their use.
While these technologies present innovative opportunities, their rapid growth
raises concerns about trust, privacy, and security. This white paper explores
the implications of deepfake technology, analyzing its role in enabling fraud,
misinformation, and the erosion of authenticity in multimedia. Using
cost-effective, easy to use tools such as Runway, Rope, and ElevenLabs, we
explore how realistic deepfakes can be created with limited resources,
demonstrating the risks posed to individuals and organizations alike. By
analyzing the technical and ethical challenges of deepfake mitigation and
detection, we emphasize the urgent need for regulatory frameworks, public
awareness, and collaborative efforts to maintain trust in digital media.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [587] [TimeWak: Temporal Chained-Hashing Watermark for Time Series Data](https://arxiv.org/abs/2506.06407)
*Zhi Wen Soi,Chaoyi Zhu,Fouad Abiad,Aditya Shankar,Jeroen M. Galjaard,Huijuan Wang,Lydia Y. Chen*

Main category: cs.CR

TL;DR: 本文提出TimeWak算法，首次在多元时间序列扩散模型中实现水印嵌入，通过时间链式哈希和ε-精确反转解决特征异质性和重构误差问题，显著提升合成数据质量与水印检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法依赖同质潜在空间，而时间序列生成器在真实空间运行，导致水印嵌入不兼容。需直接在真实时空嵌入水印，并处理特征异质性和时序依赖性。

Method: TimeWak在真实时空嵌入时间链式哈希水印，结合ε-精确反转技术，推导多元时间序列逆扩散误差边界，确保水印可检测性。

Result: 实验表明，TimeWak在5个数据集上相比基线模型，context-FID提升61.96%，相关性分数提升8.44%，且水印检测鲁棒性显著增强。

Conclusion: TimeWak首次实现真实时空水印嵌入，通过时空特征联合建模与误差控制，在保持数据质量的同时实现高可追溯性，为隐私敏感时序数据共享奠定基础。

Abstract: Synthetic time series generated by diffusion models enable sharing
privacy-sensitive datasets, such as patients' functional MRI records. Key
criteria for synthetic data include high data utility and traceability to
verify the data source. Recent watermarking methods embed in homogeneous latent
spaces, but state-of-the-art time series generators operate in real space,
making latent-based watermarking incompatible. This creates the challenge of
watermarking directly in real space while handling feature heterogeneity and
temporal dependencies. We propose TimeWak, the first watermarking algorithm for
multivariate time series diffusion models. To handle temporal dependence and
spatial heterogeneity, TimeWak embeds a temporal chained-hashing watermark
directly within the real temporal-feature space. The other unique feature is
the $\epsilon$-exact inversion, which addresses the non-uniform reconstruction
error distribution across features from inverting the diffusion process to
detect watermarks. We derive the error bound of inverting multivariate time
series and further maintain high watermark detectability. We extensively
evaluate TimeWak on its impact on synthetic data quality, watermark
detectability, and robustness under various post-editing attacks, against 5
datasets and baselines of different temporal lengths. Our results show that
TimeWak achieves improvements of 61.96% in context-FID score, and 8.44% in
correlational scores against the state-of-the-art baseline, while remaining
consistently detectable.

</details>


### [588] [HeavyWater and SimplexWater: Watermarking Low-Entropy Text Distributions](https://arxiv.org/abs/2506.06409)
*Dor Tsur,Carol Xuan Long,Claudio Mayrink Verdun,Hsiang Hsu,Chen-Fu Chen,Haim Permuter,Sajani Vithana,Flavio P. Calmon*

Main category: cs.CR

TL;DR: 本文提出两种可调水印方法HeavyWater和SimplexWater，通过优化框架在低熵生成任务中平衡检测准确性与文本失真，适用于任意大语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM水印技术在低熵任务（如代码生成）中面临挑战，因确定性预测导致水印嵌入困难。需优化随机辅助信息使用以提升检测率并减少文本质量损失。

Method: 设计基于优化理论的水印框架，开发HeavyWater和SimplexWater两种水印。通过调节参数权衡检测精度与文本失真，且不依赖特定辅助信息生成方法。

Result: 实验表明两种水印在低熵场景下检测准确率高，文本质量损失小。理论分析揭示了水印与编码理论的新联系。

Conclusion: 所提水印方法在低熵任务中实现高检测率与低失真平衡，建立了水印设计与编码理论的关联，为LLM水印提供通用解决方案。

Abstract: Large language model (LLM) watermarks enable authentication of text
provenance, curb misuse of machine-generated text, and promote trust in AI
systems. Current watermarks operate by changing the next-token predictions
output by an LLM. The updated (i.e., watermarked) predictions depend on random
side information produced, for example, by hashing previously generated tokens.
LLM watermarking is particularly challenging in low-entropy generation tasks -
such as coding - where next-token predictions are near-deterministic. In this
paper, we propose an optimization framework for watermark design. Our goal is
to understand how to most effectively use random side information in order to
maximize the likelihood of watermark detection and minimize the distortion of
generated text. Our analysis informs the design of two new watermarks:
HeavyWater and SimplexWater. Both watermarks are tunable, gracefully
trading-off between detection accuracy and text distortion. They can also be
applied to any LLM and are agnostic to side information generation. We examine
the performance of HeavyWater and SimplexWater through several benchmarks,
demonstrating that they can achieve high watermark detection accuracy with
minimal compromise of text generation quality, particularly in the low-entropy
regime. Our theoretical analysis also reveals surprising new connections
between LLM watermarking and coding theory. The code implementation can be
found in https://github.com/DorTsur/HeavyWater_SimplexWater

</details>


### [589] [Benchmarking Misuse Mitigation Against Covert Adversaries](https://arxiv.org/abs/2506.06414)
*Davis Brown,Mahdi Sabbaghi,Luze Sun,Alexander Robey,George J. Pappas,Eric Wong,Hamed Hassani*

Main category: cs.CR

TL;DR: 本文提出了一种针对语言模型隐蔽攻击的评估方法，开发了Benchmarks for Stateful Defenses (BSD)自动化评估管道，并创建了两个新数据集，表明分解攻击是有效的滥用手段，状态防御是应对措施。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型安全评估主要关注明显攻击和低风险任务，而现实中的攻击者可以通过多个看似无害的独立查询绕过当前的安全防护，因此需要开发新的防御策略。

Method: 开发了Benchmarks for Stateful Defenses (BSD)自动化评估管道，用于生成隐蔽攻击和相应防御的评估数据，并创建了两个新数据集。

Result: 评估表明分解攻击是有效的滥用手段，前沿模型能够一致拒绝这些数据集，而较弱的开放权重模型则难以应对。

Conclusion: 状态防御是应对分解攻击的有效措施，BSD管道为评估隐蔽攻击和防御提供了新的工具。

Abstract: Existing language model safety evaluations focus on overt attacks and
low-stakes tasks. Realistic attackers can subvert current safeguards by
requesting help on small, benign-seeming tasks across many independent queries.
Because individual queries do not appear harmful, the attack is hard to
{detect}. However, when combined, these fragments uplift misuse by helping the
attacker complete hard and dangerous tasks. Toward identifying defenses against
such strategies, we develop Benchmarks for Stateful Defenses (BSD), a data
generation pipeline that automates evaluations of covert attacks and
corresponding defenses. Using this pipeline, we curate two new datasets that
are consistently refused by frontier models and are too difficult for weaker
open-weight models. Our evaluations indicate that decomposition attacks are
effective misuse enablers, and highlight stateful defenses as a countermeasure.

</details>


### [590] [Auditing Black-Box LLM APIs with a Rank-Based Uniformity Test](https://arxiv.org/abs/2506.06975)
*Xiaoyuan Zhu,Yaowen Ye,Tianyi Qiu,Hanlin Zhu,Sijun Tan,Ajraf Mannan,Jonathan Michala,Raluca Ada Popa,Willie Neiswanger*

Main category: cs.CR

TL;DR: 提出了一种基于秩的一致性测试方法，用于验证黑箱大语言模型与本地部署的真实模型的行为一致性，有效应对多种威胁场景。


<details>
  <summary>Details</summary>
Motivation: 随着API成为访问大语言模型的主要接口，用户常面对缺乏透明度的黑箱系统，API提供商可能暗中提供量化或微调的模型变体，导致性能下降和安全隐患。

Method: 提出了一种基于秩的一致性测试方法，能够在有限的查询预算下，准确且高效地验证黑箱模型与真实模型的行为一致性，避免被检测到测试尝试。

Result: 该方法在量化、有害微调、越狱提示和全模型替换等多种威胁场景下表现出色，统计功效优于现有方法。

Conclusion: 该研究提供了一种有效的方法来检测黑箱大语言模型的行为变化，增强了模型使用的透明度和安全性。

Abstract: As API access becomes a primary interface to large language models (LLMs),
users often interact with black-box systems that offer little transparency into
the deployed model. To reduce costs or maliciously alter model behaviors, API
providers may discreetly serve quantized or fine-tuned variants, which can
degrade performance and compromise safety. Detecting such substitutions is
difficult, as users lack access to model weights and, in most cases, even
output logits. To tackle this problem, we propose a rank-based uniformity test
that can verify the behavioral equality of a black-box LLM to a locally
deployed authentic model. Our method is accurate, query-efficient, and avoids
detectable query patterns, making it robust to adversarial providers that
reroute or mix responses upon the detection of testing attempts. We evaluate
the approach across diverse threat scenarios, including quantization, harmful
fine-tuning, jailbreak prompts, and full model substitution, showing that it
consistently achieves superior statistical power over prior methods under
constrained query budgets.

</details>


### [591] [HauntAttack: When Attack Follows Reasoning as a Shadow](https://arxiv.org/abs/2506.07031)
*Jingyuan Ma,Rui Li,Zheng Li,Junfeng Liu,Lei Sha,Zhifang Sui*

Main category: cs.CR

TL;DR: 论文提出HauntAttack攻击框架，通过将有害指令嵌入推理问题，揭示大型推理模型（LRMs）在安全与推理能力间的显著脆弱性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在提升推理能力的同时，其内部推理过程可能因与有害性高度耦合而产生安全漏洞，需探究其安全-推理权衡关系。

Method: 提出HauntAttack黑盒攻击框架：将推理问题作为载体，用有害指令替换原始条件，逐步引导模型生成不安全输出。

Result: 实验表明即使最先进的LRMs也存在重大安全漏洞，并分析了模型类型、有害指令类型及输出模式对攻击效果的影响。

Conclusion: LRMs在安全与推理能力间存在固有矛盾，需通过系统性安全分析（如HauntAttack）提升模型抗攻击能力。

Abstract: Emerging Large Reasoning Models (LRMs) consistently excel in mathematical and
reasoning tasks, showcasing exceptional capabilities. However, the enhancement
of reasoning abilities and the exposure of their internal reasoning processes
introduce new safety vulnerabilities. One intriguing concern is: when reasoning
is strongly entangled with harmfulness, what safety-reasoning trade-off do LRMs
exhibit? To address this issue, we introduce HauntAttack, a novel and
general-purpose black-box attack framework that systematically embeds harmful
instructions into reasoning questions. Specifically, we treat reasoning
questions as carriers and substitute one of their original conditions with a
harmful instruction. This process creates a reasoning pathway in which the
model is guided step by step toward generating unsafe outputs. Based on
HauntAttack, we conduct comprehensive experiments on multiple LRMs. Our results
reveal that even the most advanced LRMs exhibit significant safety
vulnerabilities. Additionally, we perform a detailed analysis of different
models, various types of harmful instructions, and model output patterns,
providing valuable insights into the security of LRMs.

</details>


### [592] [Beyond Jailbreaks: Revealing Stealthier and Broader LLM Security Risks Stemming from Alignment Failures](https://arxiv.org/abs/2506.07402)
*Yukai Zhou,Sibei Yang,Wenjie Wang*

Main category: cs.CR

TL;DR: 论文提出LLMs在看似无害的输入下可能产生隐含危害，并开发了JailFlipBench基准和攻击方法以评估此类风险。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在现实世界中的广泛应用，其安全性问题日益突出。传统研究主要关注显性有害查询的越狱攻击，而忽略了看似无害输入下可能产生的隐含危害。

Method: 论文通过结构化象限视角重新定义LLM风险，提出JailFlipBench基准，涵盖单模态、多模态和事实扩展场景，并开发了JailFlip攻击方法。

Result: 评估表明，隐含危害在多个开源和黑盒LLMs中普遍存在，具有现实世界的紧迫风险。

Conclusion: 论文呼吁在传统越狱范式之外，进行更广泛的LLM安全评估和对齐，以应对隐含危害带来的挑战。

Abstract: Large language models (LLMs) are increasingly deployed in real-world
applications, raising concerns about their security. While jailbreak attacks
highlight failures under overtly harmful queries, they overlook a critical
risk: incorrectly answering harmless-looking inputs can be dangerous and cause
real-world harm (Implicit Harm). We systematically reformulate the LLM risk
landscape through a structured quadrant perspective based on output factuality
and input harmlessness, uncovering an overlooked high-risk region. To
investigate this gap, we propose JailFlipBench, a benchmark aims to capture
implicit harm, spanning single-modal, multimodal, and factual extension
scenarios with diverse evaluation metrics. We further develop initial JailFlip
attack methodologies and conduct comprehensive evaluations across multiple
open-source and black-box LLMs, show that implicit harm present immediate and
urgent real-world risks, calling for broader LLM safety assessments and
alignment beyond conventional jailbreak paradigms.

</details>


### [593] [Fuse and Federate: Enhancing EV Charging Station Security with Multimodal Fusion and Federated Learning](https://arxiv.org/abs/2506.06730)
*Rabah Rahal,Abdelaziz Amara Korba,Yacine Ghamri-Doudane*

Main category: cs.CR

TL;DR: 本文提出了一种新型的入侵检测框架，利用多模态数据源和分布式学习方法，有效应对电动汽车充电设施（EVSE）面临的复杂网络安全威胁。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车的全球普及，EVSE成为智能电网的关键组成部分，但其面临的网络安全挑战日益严峻，传统入侵检测系统难以应对复杂的针对性攻击。

Method: 本文提出了一种基于多模态数据源（包括网络流量和内核事件）的入侵检测框架，采用分布式学习方法，通过联邦学习实现数据隐私保护。

Result: 实验结果表明，该框架在去中心化环境中的检测率超过98%，精确率超过97%，优于现有解决方案。

Conclusion: 该框架为EVSE安全提供了可扩展且隐私保护的解决方案，能够有效应对高级网络威胁。

Abstract: The rapid global adoption of electric vehicles (EVs) has established electric
vehicle supply equipment (EVSE) as a critical component of smart grid
infrastructure. While essential for ensuring reliable energy delivery and
accessibility, EVSE systems face significant cybersecurity challenges,
including network reconnaissance, backdoor intrusions, and distributed
denial-of-service (DDoS) attacks. These emerging threats, driven by the
interconnected and autonomous nature of EVSE, require innovative and adaptive
security mechanisms that go beyond traditional intrusion detection systems
(IDS). Existing approaches, whether network-based or host-based, often fail to
detect sophisticated and targeted attacks specifically crafted to exploit new
vulnerabilities in EVSE infrastructure. This paper proposes a novel intrusion
detection framework that leverages multimodal data sources, including network
traffic and kernel events, to identify complex attack patterns. The framework
employs a distributed learning approach, enabling collaborative intelligence
across EVSE stations while preserving data privacy through federated learning.
Experimental results demonstrate that the proposed framework outperforms
existing solutions, achieving a detection rate above 98% and a precision rate
exceeding 97% in decentralized environments. This solution addresses the
evolving challenges of EVSE security, offering a scalable and privacypreserving
response to advanced cyber threats

</details>


### [594] [Ai-Driven Vulnerability Analysis in Smart Contracts: Trends, Challenges and Future Directions](https://arxiv.org/abs/2506.06735)
*Mesut Ozdag*

Main category: cs.CR

TL;DR: 本文探讨了基于AI的智能合约漏洞检测技术，包括机器学习、深度学习、图神经网络和Transformer模型，并分析了它们的优缺点及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 智能合约在区块链生态中至关重要，但其漏洞导致巨额损失。传统审计方法在可扩展性、自动化和适应性方面存在局限，因此AI技术成为有前景的替代方案。

Method: 本文研究了多种AI驱动的技术，包括机器学习、深度学习、图神经网络和Transformer模型，分析它们如何表示代码、处理语义信息并应对实际漏洞。

Result: 通过比较不同技术的准确性、可解释性、计算开销和实时适用性，本文总结了它们的优缺点，并提出了未来研究的挑战和机会。

Conclusion: AI技术在智能合约漏洞检测中展现出巨大潜力，但仍需解决可解释性和实时性等挑战，未来研究应进一步推动该领域的发展。

Abstract: Smart contracts, integral to blockchain ecosystems, enable decentralized
applications to execute predefined operations without intermediaries. Their
ability to enforce trustless interactions has made them a core component of
platforms such as Ethereum. Vulnerabilities such as numerical overflows,
reentrancy attacks, and improper access permissions have led to the loss of
millions of dollars throughout the blockchain and smart contract sector.
Traditional smart contract auditing techniques such as manual code reviews and
formal verification face limitations in scalability, automation, and
adaptability to evolving development patterns. As a result, AI-based solutions
have emerged as a promising alternative, offering the ability to learn complex
patterns, detect subtle flaws, and provide scalable security assurances. This
paper examines novel AI-driven techniques for vulnerability detection in smart
contracts, focusing on machine learning, deep learning, graph neural networks,
and transformer-based models. This paper analyzes how each technique represents
code, processes semantic information, and responds to real world vulnerability
classes. We also compare their strengths and weaknesses in terms of accuracy,
interpretability, computational overhead, and real time applicability. Lastly,
it highlights open challenges and future opportunities for advancing this
domain.

</details>


### [595] [Amatriciana: Exploiting Temporal GNNs for Robust and Efficient Money Laundering Detection](https://arxiv.org/abs/2506.00654)
*Marco Di Gennaro,Francesco Panebianco,Marco Pianta,Stefano Zanero,Michele Carminati*

Main category: cs.CR

TL;DR: 本文提出了一种基于图神经网络的新方法Amatriciana，用于检测交易图中的洗钱者，利用完整的时间信息，减少误报并提高检测效果。


<details>
  <summary>Details</summary>
Motivation: 洗钱作为一种金融犯罪，严重威胁金融完整性和社会安全。随着交易数量的增加，需要自动工具帮助执法机构检测此类犯罪活动。

Method: Amatriciana方法基于图神经网络，利用完整的交易图，不将其分割为多个时间子图，充分利用数据集中的关系信息。

Result: 实验表明，Amatriciana在有限数据下能有效学习，数据充足时优于其他先进方法，F1得分为0.76，误报率降低55%。

Conclusion: Amatriciana在检测洗钱者方面表现出色，尤其在减少误报方面具有显著优势，展示了其在金融犯罪检测中的潜力。

Abstract: Money laundering is a financial crime that poses a serious threat to
financial integrity and social security. The growing number of transactions
makes it necessary to use automatic tools that help law enforcement agencies
detect such criminal activity. In this work, we present Amatriciana, a novel
approach based on Graph Neural Networks to detect money launderers inside a
graph of transactions by considering temporal information. Amatriciana uses the
whole graph of transactions without splitting it into several time-based
subgraphs, exploiting all relational information in the dataset. Our
experiments on a public dataset reveal that the model can learn from a limited
amount of data. Furthermore, when more data is available, the model outperforms
other State-of-the-art approaches; in particular, Amatriciana decreases the
number of False Positives (FPs) while detecting many launderers. In summary,
Amatriciana achieves an F1 score of 0.76. In addition, it lowers the FPs by 55%
with respect to other State-of-the-art models.

</details>


### [596] [Dual-Priv Pruning : Efficient Differential Private Fine-Tuning in Multimodal Large Language Models](https://arxiv.org/abs/2506.07077)
*Qianshan Wei,Jiaqi Li,Zihan You,Yi Zhan,Kecen Li,Jialin Wu,Xinfeng Li Hengjun Liu,Yi Yu,Bin Cao,Yiwen Xu,Yang Liu,Guilin Qi*

Main category: cs.CR

TL;DR: 本文提出Dual-Priv Pruning框架，通过视觉令牌剪枝和梯度更新剪枝两种机制，在多模态大语言模型（MLLMs）中实现高效的差分隐私（DP）微调，减少计算开销和模型性能损失。


<details>
  <summary>Details</summary>
Motivation: 差分隐私（DP）在MLLMs中的有效性尚未明确，其计算开销大且噪声随参数维度增加导致模型性能显著下降，隐私与效用的权衡问题阻碍了DP在复杂模型中的应用。

Method: 采用双剪枝机制：1）视觉令牌剪枝去除冗余视觉信息以降低输入维度；2）在DP优化过程中基于噪声梯度幅值选择性剪枝参数更新，以缓解噪声影响并提升模型效用。

Result: 实验表明，该方法在性能损失极小的情况下达到竞争性结果，内存效率优于标准DP-SGD，在H20 GPU上表现最佳，且仅比存在严重性能问题的零阶方法多消耗1.74%内存。

Conclusion: Dual-Priv Pruning有效解决了MLLMs中DP应用的计算与性能挑战，首次探索了DP微调在MLLMs中的可行性，平衡了隐私保护与模型实用性。

Abstract: Differential Privacy (DP) is a widely adopted technique, valued for its
effectiveness in protecting the privacy of task-specific datasets, making it a
critical tool for large language models. However, its effectiveness in
Multimodal Large Language Models (MLLMs) remains uncertain. Applying
Differential Privacy (DP) inherently introduces substantial computation
overhead, a concern particularly relevant for MLLMs which process extensive
textual and visual data. Furthermore, a critical challenge of DP is that the
injected noise, necessary for privacy, scales with parameter dimensionality,
leading to pronounced model degradation; This trade-off between privacy and
utility complicates the application of Differential Privacy (DP) to complex
architectures like MLLMs. To address these, we propose Dual-Priv Pruning, a
framework that employs two complementary pruning mechanisms for DP fine-tuning
in MLLMs: (i) visual token pruning to reduce input dimensionality by removing
redundant visual information, and (ii) gradient-update pruning during the DP
optimization process. This second mechanism selectively prunes parameter
updates based on the magnitude of noisy gradients, aiming to mitigate noise
impact and improve utility. Experiments demonstrate that our approach achieves
competitive results with minimal performance degradation. In terms of
computational efficiency, our approach consistently utilizes less memory than
standard DP-SGD. While requiring only 1.74% more memory than zeroth-order
methods which suffer from severe performance issues on A100 GPUs, our method
demonstrates leading memory efficiency on H20 GPUs. To the best of our
knowledge, we are the first to explore DP fine-tuning in MLLMs. Our code is
coming soon.

</details>


### [597] [A Systematic Review of Poisoning Attacks Against Large Language Models](https://arxiv.org/abs/2506.06518)
*Neil Fendley,Edward W. Staley,Joshua Carney,William Redman,Marie Chau,Nathan Drenkow*

Main category: cs.CR

TL;DR: 本文系统综述了大语言模型（LLM）投毒攻击的研究现状，提出了一种全面的投毒威胁模型，包含四类攻击规范和六项评估指标，以澄清安全风险并统一术语不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM投毒攻击框架和术语源自传统分类任务投毒攻击研究，无法充分应对生成式LLM的安全风险，需系统性梳理攻击特征及安全影响。

Method: 通过系统性文献综述，提出包含四类攻击规范（攻击逻辑与策略）和六项投毒指标（攻击关键特征）的威胁模型框架，并围绕概念投毒、隐蔽投毒、持久投毒及特定任务投毒四个维度组织分析。

Result: 所提框架能有效分类现有LLM投毒攻击，揭示攻击在概念操控、隐蔽性、持久性及任务独特性四个维度的安全风险特征。

Conclusion: 统一的投毒威胁模型框架有助于明确LLM投毒攻击的安全边界，为后续防御机制设计提供结构化分析基础。

Abstract: With the widespread availability of pretrained Large Language Models (LLMs)
and their training datasets, concerns about the security risks associated with
their usage has increased significantly. One of these security risks is the
threat of LLM poisoning attacks where an attacker modifies some part of the LLM
training process to cause the LLM to behave in a malicious way. As an emerging
area of research, the current frameworks and terminology for LLM poisoning
attacks are derived from earlier classification poisoning literature and are
not fully equipped for generative LLM settings. We conduct a systematic review
of published LLM poisoning attacks to clarify the security implications and
address inconsistencies in terminology across the literature. We propose a
comprehensive poisoning threat model applicable to categorize a wide range of
LLM poisoning attacks. The poisoning threat model includes four poisoning
attack specifications that define the logistics and manipulation strategies of
an attack as well as six poisoning metrics used to measure key characteristics
of an attack. Under our proposed framework, we organize our discussion of
published LLM poisoning literature along four critical dimensions of LLM
poisoning attacks: concept poisons, stealthy poisons, persistent poisons, and
poisons for unique tasks, to better understand the current landscape of
security risks.

</details>


### [598] [Mind the Web: The Security of Web Use Agents](https://arxiv.org/abs/2506.07153)
*Avishag Shapira,Parth Atulbhai Gandhi,Edan Habler,Oleg Brodt,Asaf Shabtai*

Main category: cs.CR

TL;DR: 本文揭示网页自动化代理的高权限能力存在重大安全风险，攻击者可通过恶意网页内容（如评论、广告）注入任务对齐指令，利用LLM上下文推理缺陷绕过安全机制，成功实施摄像头劫持、文件窃取等攻击（成功率80%-100%），并提出包含执行约束、任务感知推理的综合防御方案。


<details>
  <summary>Details</summary>
Motivation: 网页自动化代理具备DOM操作、多标签导航等高权限能力，但其未被充分研究的安全攻击面可能被恶意内容（如网页评论）利用，通过伪装成任务指令诱导代理执行越权操作，威胁用户隐私与系统安全。

Method: 提出任务对齐注入技术，将恶意指令伪装为任务指导；系统评估四大主流网页代理（如OpenAI Operator），设计九类攻击载荷（如摄像头激活、本地文件窃取），验证攻击在多种LLM上的有效性。

Result: 实验显示攻击成功率高达80%-100%，包括身份冒充、密码泄露等九类攻击均成功绕过代理内置安全机制，仅需在公共网站发布内容即可触发，暴露高权限代理的严重脆弱性。

Conclusion: 网页代理的高权限与LLM上下文推理缺陷导致新型攻击面，需结合任务感知推理、执行沙箱等综合防御策略，为安全部署提供实践方向。

Abstract: Web-use agents are rapidly being deployed to automate complex web tasks,
operating with extensive browser capabilities including multi-tab navigation,
DOM manipulation, JavaScript execution and authenticated session access.
However, these powerful capabilities create a critical and previously
unexplored attack surface. This paper demonstrates how attackers can exploit
web-use agents' high-privilege capabilities by embedding malicious content in
web pages such as comments, reviews, or advertisements that agents encounter
during legitimate browsing tasks. In addition, we introduce the task-aligned
injection technique that frame malicious commands as helpful task guidance
rather than obvious attacks. This technique exploiting fundamental limitations
in LLMs' contextual reasoning: agents struggle in maintaining coherent
contextual awareness and fail to detect when seemingly helpful web content
contains steering attempts that deviate from their original task goal. Through
systematic evaluation of four popular agents (OpenAI Operator, Browser Use, Do
Browser, OpenOperator), we demonstrate nine payload types that compromise
confidentiality, integrity, and availability, including unauthorized camera
activation, user impersonation, local file exfiltration, password leakage, and
denial of service, with validation across multiple LLMs achieving success rates
of 80%-100%. These payloads succeed across agents with built-in safety
mechanisms, requiring only the ability to post content on public websites,
creating unprecedented risks given the ease of exploitation combined with
agents' high-privilege access. To address this attack, we propose comprehensive
mitigation strategies including oversight mechanisms, execution constraints,
and task-aware reasoning techniques, providing practical directions for secure
development and deployment.

</details>


### [599] [Scoring the Unscorables: Cyber Risk Assessment Beyond Internet Scans](https://arxiv.org/abs/2506.06604)
*Armin Sarabi,Manish Karir,Mingyan Liu*

Main category: cs.CR

TL;DR: 本文提出了一种利用新型数据类型进行网络风险量化的方法，通过爬取组织网站的技术签名来构建高精度的网络风险评估模型，克服了传统IP扫描数据的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统基于大规模IP扫描的网络风险评估方法存在IP地址映射不完整或缺失的问题，尤其对中小型组织（SMEs）数据不足。本文旨在探索一种更易获取且适用于SMEs的替代方法。

Method: 通过爬取组织网站的技术签名数据，构建网络风险评估模型，并与不同网络事件数据集进行交叉验证。

Result: 研究表明，技术签名与组织的网络安全态势存在强相关性，且该方法适用于数百万SMEs。同时，模型揭示了勒索软件攻击受害者与其他网络事件受害者之间的关键差异。

Conclusion: 利用技术签名数据构建的网络风险评估模型具有高精度和广泛适用性，为SMEs的网络风险管理提供了有效工具。

Abstract: In this paper we present a study on using novel data types to perform cyber
risk quantification by estimating the likelihood of a data breach. We
demonstrate that it is feasible to build a highly accurate cyber risk
assessment model using public and readily available technology signatures
obtained from crawling an organization's website. This approach overcomes the
limitations of previous similar approaches that relied on large-scale IP
address based scanning data, which suffers from incomplete/missing IP address
mappings as well as the lack of such data for large numbers of small and
medium-sized organizations (SMEs). In comparison to scan data, technology
digital signature data is more readily available for millions of SMEs. Our
study shows that there is a strong relationship between these technology
signatures and an organization's cybersecurity posture. In cross-validating our
model using different cyber incident datasets, we also highlight the key
differences between ransomware attack victims and the larger population of
cyber incident and data breach victims.

</details>


### [600] [From Static to Adaptive Defense: Federated Multi-Agent Deep Reinforcement Learning-Driven Moving Target Defense Against DoS Attacks in UAV Swarm Networks](https://arxiv.org/abs/2506.07392)
*Yuyang Zhou,Guang Cheng,Kang Du,Zihan Chen,Tian Qin,Yuyu Zhao*

Main category: cs.CR

TL;DR: 本文提出了一种基于联邦多智能体深度强化学习（FMADRL）的移动目标防御（MTD）框架，用于无人机群网络中主动和自适应的拒绝服务（DoS）攻击缓解。通过设计三种轻量级协调的MTD机制，显著提升了网络韧性，减少了恢复时间和能耗。


<details>
  <summary>Details</summary>
Motivation: 无人机群网络的开放无线环境、动态拓扑和资源限制使其容易受到DoS攻击，传统的静态或集中式防御机制无法有效应对这些动态和分布式场景。

Method: 提出了一种基于FMADRL的MTD框架，设计了领导者切换、路由变异和频率跳变三种轻量级协调的MTD机制，将防御问题建模为多智能体部分可观测马尔可夫决策过程（POMDP），并通过策略梯度算法实现分布式学习。

Result: 仿真结果表明，该方法在攻击缓解率、平均恢复时间、能耗和防御成本方面显著优于现有基线，分别提升了34.6%、减少了94.6%、降低了29.3%和98.3%。

Conclusion: 该FMADRL驱动的MTD框架有效提升了无人机群网络在DoS攻击下的韧性和任务连续性，为动态和分布式网络防御提供了新的解决方案。

Abstract: The proliferation of unmanned aerial vehicle (UAV) swarms has enabled a wide
range of mission-critical applications, but also exposes UAV networks to severe
Denial-of-Service (DoS) threats due to their open wireless environment, dynamic
topology, and resource constraints. Traditional static or centralized defense
mechanisms are often inadequate for such dynamic and distributed scenarios. To
address these challenges, we propose a novel federated multi-agent deep
reinforcement learning (FMADRL)-driven moving target defense (MTD) framework
for proactive and adaptive DoS mitigation in UAV swarm networks. Specifically,
we design three lightweight and coordinated MTD mechanisms, including leader
switching, route mutation, and frequency hopping, that leverage the inherent
flexibility of UAV swarms to disrupt attacker efforts and enhance network
resilience. The defense problem is formulated as a multi-agent partially
observable Markov decision process (POMDP), capturing the distributed,
resource-constrained, and uncertain nature of UAV swarms under attack. Each UAV
is equipped with a local policy agent that autonomously selects MTD actions
based on partial observations and local experiences. By employing a policy
gradient-based FMADRL algorithm, UAVs collaboratively optimize their defense
policies via reward-weighted aggregation, enabling distributed learning without
sharing raw data and thus reducing communication overhead. Extensive
simulations demonstrate that our approach significantly outperforms
state-of-the-art baselines, achieving up to a 34.6% improvement in attack
mitigation rate, a reduction in average recovery time of up to 94.6%, and
decreases in energy consumption and defense cost by as much as 29.3% and 98.3%,
respectively, while maintaining robust mission continuity under various DoS
attack strategies.

</details>


### [601] [TimberStrike: Dataset Reconstruction Attack Revealing Privacy Leakage in Federated Tree-Based Systems](https://arxiv.org/abs/2506.07605)
*Marco Di Gennaro,Giovanni De Lucia,Stefano Longari,Stefano Zanero,Michele Carminati*

Main category: cs.CR

TL;DR: 本文介绍了TimberStrike，一种针对水平联邦树模型的优化数据集重建攻击，展示了其在不同框架中的有效性，并分析了差分隐私的部分缓解作用。


<details>
  <summary>Details</summary>
Motivation: 联邦学习作为一种隐私保护的机器学习替代方案，尽管在神经网络中得到广泛研究，但树模型的安全和隐私影响尚未充分探索。

Method: TimberStrike利用决策树的离散特性，通过分裂值和决策路径推断其他客户的敏感训练数据，并在多个联邦梯度提升框架中进行评估。

Result: 在公开的中风预测数据集上，TimberStrike在所有实现中重建了73.05%至95.63%的目标数据集，差分隐私虽部分缓解攻击，但显著降低模型性能。

Conclusion: 研究强调了为基于树的联邦学习系统设计专门隐私保护机制的必要性，并提供了初步设计见解。

Abstract: Federated Learning has emerged as a privacy-oriented alternative to
centralized Machine Learning, enabling collaborative model training without
direct data sharing. While extensively studied for neural networks, the
security and privacy implications of tree-based models remain underexplored.
This work introduces TimberStrike, an optimization-based dataset reconstruction
attack targeting horizontally federated tree-based models. Our attack, carried
out by a single client, exploits the discrete nature of decision trees by using
split values and decision paths to infer sensitive training data from other
clients. We evaluate TimberStrike on State-of-the-Art federated gradient
boosting implementations across multiple frameworks, including Flower, NVFlare,
and FedTree, demonstrating their vulnerability to privacy breaches. On a
publicly available stroke prediction dataset, TimberStrike consistently
reconstructs between 73.05% and 95.63% of the target dataset across all
implementations. We further analyze Differential Privacy, showing that while it
partially mitigates the attack, it also significantly degrades model
performance. Our findings highlight the need for privacy-preserving mechanisms
specifically designed for tree-based Federated Learning systems, and we provide
preliminary insights into their design.

</details>


### [602] [Profiling Electric Vehicles via Early Charging Voltage Patterns](https://arxiv.org/abs/2506.07714)
*Francesco Marchiori,Denis Donadel,Alessandro Brighente,Mauro Conti*

Main category: cs.CR

TL;DR: 本文提出一种基于充电初期电压特征的电动汽车（EV）身份识别框架，通过早期电压行为分析实现快速、高效认证，准确率达0.86，同时揭示充电数据可能引发的隐私风险。


<details>
  <summary>Details</summary>
Motivation: 现有EV认证方法集中于充电后期检测，导致攻击者可在被发现前窃取大量能源。同时，基于充电模式的车辆识别存在用户追踪隐私隐患，需开发更早期、更可靠的认证方案。

Method: 利用充电初期电压测量数据构建EV特征指纹，假设其与后期电流特征具有相似性。通过提取电压特征并采用轻量级模型，结合10个关键特征实现高效识别。

Result: 在49辆EV的7408次充电数据测试中达到0.86准确率，特征重要性显示仅需10个核心特征即可接近最优性能，模型计算效率显著提升。

Conclusion: 该方法为EV认证提供新维度，同时暴露充电数据可能被滥用于用户追踪的隐私风险，需在安全与隐私保护间寻求平衡。

Abstract: Electric Vehicles (EVs) are rapidly gaining adoption as a sustainable
alternative to fuel-powered vehicles, making secure charging infrastructure
essential. Despite traditional authentication protocols, recent results showed
that attackers may steal energy through tailored relay attacks. One
countermeasure is leveraging the EV's fingerprint on the current exchanged
during charging. However, existing methods focus on the final charging stage,
allowing malicious actors to consume substantial energy before being detected
and repudiated. This underscores the need for earlier and more effective
authentication methods to prevent unauthorized charging. Meanwhile, profiling
raises privacy concerns, as uniquely identifying EVs through charging patterns
could enable user tracking.
  In this paper, we propose a framework for uniquely identifying EVs using
physical measurements from the early charging stages. We hypothesize that
voltage behavior early in the process exhibits similar characteristics to
current behavior in later stages. By extracting features from early voltage
measurements, we demonstrate the feasibility of EV profiling. Our approach
improves existing methods by enabling faster and more reliable vehicle
identification. We test our solution on a dataset of 7408 usable charges from
49 EVs, achieving up to 0.86 accuracy. Feature importance analysis shows that
near-optimal performance is possible with just 10 key features, improving
efficiency alongside our lightweight models. This research lays the foundation
for a novel authentication factor while exposing potential privacy risks from
unauthorized access to charging data.

</details>


### [603] [SoK: Data Reconstruction Attacks Against Machine Learning Models: Definition, Metrics, and Benchmark](https://arxiv.org/abs/2506.07888)
*Rui Wen,Yiyong Liu,Michael Backes,Yang Zhang*

Main category: cs.CR

TL;DR: 本文针对视觉领域的数据重建攻击，提出统一攻击分类法、形式化定义及定量评估指标，并利用大语言模型进行视觉评估，建立系统性评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前数据重建攻击领域缺乏形式化定义和通用评估指标，阻碍了研究进展。本文旨在解决这一问题。

Method: 提出包含可量化性/一致性/精确性/多样性标准的评估指标，利用LLM替代人工视觉评估，构建统一框架系统化分析现有攻击方法。

Result: 实证结果表明指标有效性，并从记忆化角度为设计新攻击提供洞见，建立了未来研究的基准。

Conclusion: 通过统一分类与量化指标填补领域空白，系统性评估框架为数据重建攻击研究提供了方法论基础与方向指引。

Abstract: Data reconstruction attacks, which aim to recover the training dataset of a
target model with limited access, have gained increasing attention in recent
years. However, there is currently no consensus on a formal definition of data
reconstruction attacks or appropriate evaluation metrics for measuring their
quality. This lack of rigorous definitions and universal metrics has hindered
further advancement in this field. In this paper, we address this issue in the
vision domain by proposing a unified attack taxonomy and formal definitions of
data reconstruction attacks. We first propose a set of quantitative evaluation
metrics that consider important criteria such as quantifiability, consistency,
precision, and diversity. Additionally, we leverage large language models
(LLMs) as a substitute for human judgment, enabling visual evaluation with an
emphasis on high-quality reconstructions. Using our proposed taxonomy and
metrics, we present a unified framework for systematically evaluating the
strengths and limitations of existing attacks and establishing a benchmark for
future research. Empirical results, primarily from a memorization perspective,
not only validate the effectiveness of our metrics but also offer valuable
insights for designing new attacks.

</details>


### [604] [Are Trees Really Green? A Detection Approach of IoT Malware Attacks](https://arxiv.org/abs/2506.07836)
*Silvia Lucia Sanna,Diego Soi,Davide Maiorca,Giorgio Giacinto*

Main category: cs.CR

TL;DR: 本文提出一种基于流量隐私保护统计特征的绿色方法，通过优化树模型超参数，在保持高检测性能的同时降低能耗，适用于资源受限的IoT设备。


<details>
  <summary>Details</summary>
Motivation: IoT设备因资源限制难以部署安全补丁，现有机器学习攻击检测方法多关注识别能力，但忽视算法对计算资源的影响。需开发兼顾性能与能耗的轻量级检测方案。

Method: 使用决策树、随机森林和Extra-Trees三种树模型，基于流量隐私保护统计特征，以马修斯相关系数为指标优化超参数，同时考虑能耗与测试性能。

Result: 优化后模型在维持高检测准确率（马修斯系数）的同时，显著降低功耗（瓦特小时单位能耗），验证了本地化ML入侵检测系统在IoT场景的可行性。

Conclusion: 通过能耗与性能联合优化的树模型，可在资源受限设备上实现高效低耗的入侵检测，为绿色ML在IoT安全领域提供实践依据。

Abstract: Nowadays, the Internet of Things (IoT) is widely employed, and its usage is
growing exponentially because it facilitates remote monitoring, predictive
maintenance, and data-driven decision making, especially in the healthcare and
industrial sectors. However, IoT devices remain vulnerable due to their
resource constraints and difficulty in applying security patches. Consequently,
various cybersecurity attacks are reported daily, such as Denial of Service,
particularly in IoT-driven solutions. Most attack detection methodologies are
based on Machine Learning (ML) techniques, which can detect attack patterns.
However, the focus is more on identification rather than considering the impact
of ML algorithms on computational resources. This paper proposes a green
methodology to identify IoT malware networking attacks based on flow
privacy-preserving statistical features. In particular, the hyperparameters of
three tree-based models -- Decision Trees, Random Forest and Extra-Trees -- are
optimized based on energy consumption and test-time performance in terms of
Matthew's Correlation Coefficient. Our results show that models maintain high
performance and detection accuracy while consistently reducing power usage in
terms of watt-hours (Wh). This suggests that on-premise ML-based Intrusion
Detection Systems are suitable for IoT and other resource-constrained devices.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [605] [On the Fundamental Impossibility of Hallucination Control in Large Language Models](https://arxiv.org/abs/2506.06382)
*Michał P. Karpowicz*

Main category: stat.ML

TL;DR: 该论文通过形式化不可能定理证明，大型语言模型无法同时满足真实性、语义守恒、相关知识揭示和最优性四个属性，揭示了模型幻觉的必然性及权衡需求。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示大型语言模型（LLM）无法完全避免幻觉的根本原因，并探索模型设计中的核心权衡问题，为改进模型提供理论依据。

Method: 将LLM推理建模为神经组件竞争生成结果的“思想拍卖”机制，基于Green-Laffont定理构建数学框架，形式化证明四属性不可兼得性。

Result: 严格论证了所有LLM推理机制必然存在幻觉与性能的权衡，为模型架构、训练目标及评估方法提供了理论约束框架。

Conclusion: 模型设计需在四个关键属性间进行取舍，该理论框架为理解LLM推理本质及优化方向奠定了数学基础。

Abstract: This paper explains \textbf{why it is impossible to create large language
models that do not hallucinate and what are the trade-offs we should be looking
for}. It presents a formal \textbf{impossibility theorem} demonstrating that no
inference mechanism can simultaneously satisfy four fundamental properties:
\textbf{truthful (non-hallucinatory) generation, semantic information
conservation, relevant knowledge revelation, and knowledge-constrained
optimality}. By modeling LLM inference as an \textbf{auction of ideas} where
neural components compete to contribute to responses, we prove the
impossibility using the Green-Laffont theorem. That mathematical framework
provides a rigorous foundation for understanding the nature of inference
process, with implications for model architecture, training objectives, and
evaluation methods.

</details>


### [606] [A Statistical Framework for Model Selection in LSTM Networks](https://arxiv.org/abs/2506.06840)
*Fahad Mostafa*

Main category: stat.ML

TL;DR: 本文提出了一种针对LSTM网络的统一统计框架，通过扩展经典模型选择方法（如信息准则和收缩估计），结合时序结构惩罚似然和变门限隐状态动态机制，利用变分贝叶斯进行高效计算，在生物医学数据中验证了优越性能。


<details>
  <summary>Details</summary>
Motivation: LSTM网络在序列建模中广泛应用，但其模型选择（超参调优/结构设计/正则化选择）仍高度依赖经验性方法且计算成本高昂，亟需系统化解决方案。

Method: 构建融合时序结构的惩罚似然函数，提出广义隐状态动态门限机制，采用变分贝叶斯和近似边缘似然方法实现高效参数估计，将传统统计模型选择理论扩展至神经网络。

Result: 在生物医学数据集上的实验表明，该框架具有更好的灵活性和预测性能，验证了统计理论驱动方法在神经网络模型选择中的有效性。

Conclusion: 所建框架为LSTM模型选择提供了理论严谨、计算高效的系统化路径，成功将经典统计推断工具与深度学习相结合，突破了传统经验调参方法的局限性。

Abstract: Long Short-Term Memory (LSTM) neural network models have become the
cornerstone for sequential data modeling in numerous applications, ranging from
natural language processing to time series forecasting. Despite their success,
the problem of model selection, including hyperparameter tuning, architecture
specification, and regularization choice remains largely heuristic and
computationally expensive. In this paper, we propose a unified statistical
framework for systematic model selection in LSTM networks. Our framework
extends classical model selection ideas, such as information criteria and
shrinkage estimation, to sequential neural networks. We define penalized
likelihoods adapted to temporal structures, propose a generalized threshold
approach for hidden state dynamics, and provide efficient estimation strategies
using variational Bayes and approximate marginal likelihood methods. Several
biomedical data centric examples demonstrate the flexibility and improved
performance of the proposed framework.

</details>


### [607] [Direct Fisher Score Estimation for Likelihood Maximization](https://arxiv.org/abs/2506.06542)
*Sherman Khoo,Yakun Wang,Song Liu,Mark Beaumont*

Main category: stat.ML

TL;DR: 本文提出一种基于局部评分匹配的序列梯度优化方法，用于在似然函数难以计算但可模拟时进行极大似然估计。该方法通过线性参数化构建Fisher得分的闭式近似，有效平滑似然目标并提升优化效率。


<details>
  <summary>Details</summary>
Motivation: 传统极大似然估计在似然函数复杂或难以计算时面临困难，而基于模拟的方法需要高效利用模型生成数据的能力。

Method: 采用局部参数区域模拟数据，通过线性参数化模型构建Fisher得分的闭式最小二乘解，形成序列梯度优化框架。

Result: 理论证明得分估计的偏差界限，实验在合成和真实数据集上显示方法优于现有基准，尤其在复杂似然场景中表现突出。

Conclusion: 所提方法通过局部评分匹配和闭式解设计，显著提升复杂似然优化问题的计算效率与稳定性，具有理论保障和广泛适用性。

Abstract: We study the problem of likelihood maximization when the likelihood function
is intractable but model simulations are readily available. We propose a
sequential, gradient-based optimization method that directly models the Fisher
score based on a local score matching technique which uses simulations from a
localized region around each parameter iterate. By employing a linear
parameterization to the surrogate score model, our technique admits a
closed-form, least-squares solution. This approach yields a fast, flexible, and
efficient approximation to the Fisher score, effectively smoothing the
likelihood objective and mitigating the challenges posed by complex likelihood
landscapes. We provide theoretical guarantees for our score estimator,
including bounds on the bias introduced by the smoothing. Empirical results on
a range of synthetic and real-world problems demonstrate the superior
performance of our method compared to existing benchmarks.

</details>


### [608] [Robust Learnability of Sample-Compressible Distributions under Noisy or Adversarial Perturbations](https://arxiv.org/abs/2506.06613)
*Arefe Boushehrian,Amir Najafi*

Main category: stat.ML

TL;DR: 本文研究了在加性噪声和对抗扰动下，样本可压缩分布族的PAC可学习性，提出了扰动-量化框架，并解决了高维均匀分布混合和对抗损坏高斯混合模型的学习样本复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 探究在数据扰动（噪声或对抗破坏）下，具有样本压缩性的分布族是否仍保持可学习性，并验证其样本复杂度是否随噪声水平优雅扩展。

Method: 建立两种扰动模型：加性独立噪声模型和对抗破坏模型；开发与压缩方案自然对接的扰动-量化框架，最小化假设依赖。

Result: 证明了样本可压缩族在必要充分条件下仍可学习，推导了噪声/对抗预算与样本复杂度的定量关系，解决了高维均匀分布混合和对抗损坏高斯混合的样本复杂度开放问题。

Conclusion: 样本压缩理论可扩展至扰动场景，所提框架具有通用性，为噪声和对抗环境下的分布学习提供了统一分析工具。

Abstract: Learning distribution families over $\mathbb{R}^d$ is a fundamental problem
in unsupervised learning and statistics. A central question in this setting is
whether a given family of distributions possesses sufficient structure to be
(at least) information-theoretically learnable and, if so, to characterize its
sample complexity. In 2018, Ashtiani et al. reframed \emph{sample
compressibility}, originally due to Littlestone and Warmuth (1986), as a
structural property of distribution classes, proving that it guarantees
PAC-learnability. This discovery subsequently enabled a series of recent
advancements in deriving nearly tight sample complexity bounds for various
high-dimensional open problems. It has been further conjectured that the
converse also holds: every learnable class admits a tight sample compression
scheme.
  In this work, we establish that sample compressible families remain learnable
even from perturbed samples, subject to a set of necessary and sufficient
conditions. We analyze two models of data perturbation: (i) an additive
independent noise model, and (ii) an adversarial corruption model, where an
adversary manipulates a limited subset of the samples unknown to the learner.
Our results are general and rely on as minimal assumptions as possible. We
develop a perturbation-quantization framework that interfaces naturally with
the compression scheme and leads to sample complexity bounds that scale
gracefully with the noise level and corruption budget. As concrete
applications, we establish new sample complexity bounds for learning finite
mixtures of high-dimensional uniform distributions under both noise and
adversarial perturbations, as well as for learning Gaussian mixture models from
adversarially corrupted samples, resolving two open problems in the literature.

</details>


### [609] [Continuous Semi-Implicit Models](https://arxiv.org/abs/2506.06778)
*Longlin Yu,Jiajun Zha,Tong Yang,Tianyu Xie,Xiangyu Zhang,S. -H. Gary Chan,Cheng Zhang*

Main category: stat.ML

TL;DR: 本文提出了CoSIM，一种连续半隐式模型，通过引入连续转移核，实现了高效的无仿真训练，并在图像生成任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统的分层半隐式模型在序列训练中收敛较慢，限制了其在生成模型中的应用。本文旨在解决这一问题，提升模型的训练效率和生成能力。

Method: CoSIM通过将分层半隐式模型扩展为连续框架，并引入连续转移核，实现了高效的无仿真训练。此外，通过精心设计的转移核，CoSIM在分布层面上实现了生成模型的多步蒸馏。

Result: 实验表明，CoSIM在图像生成任务中表现优异，与现有的扩散模型加速方法相比，性能相当或更好，尤其在FD-DINOv2上表现突出。

Conclusion: CoSIM通过连续框架和转移核的设计，显著提升了半隐式模型的训练效率和生成能力，为生成模型的加速提供了新的思路。

Abstract: Semi-implicit distributions have shown great promise in variational inference
and generative modeling. Hierarchical semi-implicit models, which stack
multiple semi-implicit layers, enhance the expressiveness of semi-implicit
distributions and can be used to accelerate diffusion models given pretrained
score networks. However, their sequential training often suffers from slow
convergence. In this paper, we introduce CoSIM, a continuous semi-implicit
model that extends hierarchical semi-implicit models into a continuous
framework. By incorporating a continuous transition kernel, CoSIM enables
efficient, simulation-free training. Furthermore, we show that CoSIM achieves
consistency with a carefully designed transition kernel, offering a novel
approach for multistep distillation of generative models at the distributional
level. Extensive experiments on image generation demonstrate that CoSIM
performs on par or better than existing diffusion model acceleration methods,
achieving superior performance on FD-DINOv2.

</details>


### [610] [The Currents of Conflict: Decomposing Conflict Trends with Gaussian Processes](https://arxiv.org/abs/2506.06828)
*Simon P. von der Maase*

Main category: stat.ML

TL;DR: 提出一种基于高斯过程和细粒度时空冲突数据的新方法，用于估计冲突趋势、支持分析预测，并仅依赖历史数据实现高效建模。


<details>
  <summary>Details</summary>
Motivation: 现有方法可能难以捕捉冲突的复杂时空动态，需更有效工具分析冲突陷阱、扩散模式及预测未来趋势。

Method: 结合高分辨率时空冲突事件数据与高斯过程建模，构建冲突时空趋势估计框架。

Result: 方法可解析冲突动态特征、控制时空混杂因素，并实现前沿水平的未来冲突预测。

Conclusion: 该简约框架仅利用历史冲突数据即实现多维分析预测，为冲突研究提供高效建模工具。

Abstract: I present a novel approach to estimating the temporal and spatial patterns of
violent conflict. I show how we can use highly temporally and spatially
disaggregated data on conflict events in tandem with Gaussian processes to
estimate temporospatial conflict trends. These trends can be studied to gain
insight into conflict traps, diffusion and tempo-spatial conflict exposure in
general; they can also be used to control for such phenomenons given other
estimation tasks; lastly, the approach allow us to extrapolate the estimated
tempo-spatial conflict patterns into future temporal units, thus facilitating
powerful, stat-of-the-art, conflict forecasts. Importantly, these results are
achieved via a relatively parsimonious framework using only one data source:
past conflict patterns.

</details>


### [611] [Half-AVAE: Adversarial-Enhanced Factorized and Structured Encoder-Free VAE for Underdetermined Independent Component Analysis](https://arxiv.org/abs/2506.07011)
*Yuan-Hao Wei,Yan-Jie Sun*

Main category: stat.ML

TL;DR: 本研究提出了一种新型半对抗变分自编码器（Half-AVAE），通过消除编码器结构、结合对抗网络与外部增强项，解决了欠定独立成分分析（ICA）中潜在变量独立性与可解释性不足的问题，在合成数据实验中表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统VAE在处理欠定ICA（潜在变量数多于观测信号）时，因依赖编码器-解码器架构而难以有效恢复独立成分。研究旨在提升潜在变量的解耦能力与模型灵活性，以支持因果推理、生成建模等应用。

Method: 基于无编码器的Half-VAE框架，提出Half-AVAE模型：1) 消除显式逆映射以应对欠定场景；2) 引入对抗网络和外部增强（EE）项，增强潜在维度间的独立性；3) 结合结构化先验实现因子化表示。

Result: 在合成信号实验中，Half-AVAE的均方根误差低于GP-AVAE和Half-VAE等基线模型，显著提升了欠定条件下独立成分的恢复能力，验证了方法的有效性。

Conclusion: 研究表明，通过去除编码器、结合对抗训练与结构化先验，VAE框架可灵活解决复杂ICA问题，为解耦表示、因果推理等任务提供了新思路，扩展了变分推断的应用边界。

Abstract: This study advances the Variational Autoencoder (VAE) framework by addressing
challenges in Independent Component Analysis (ICA) under both determined and
underdetermined conditions, focusing on enhancing the independence and
interpretability of latent variables. Traditional VAEs map observed data to
latent variables and back via an encoder-decoder architecture, but struggle
with underdetermined ICA where the number of latent variables exceeds observed
signals. The proposed Half Adversarial VAE (Half-AVAE) builds on the
encoder-free Half-VAE framework, eliminating explicit inverse mapping to tackle
underdetermined scenarios. By integrating adversarial networks and External
Enhancement (EE) terms, Half-AVAE promotes mutual independence among latent
dimensions, achieving factorized and interpretable representations. Experiments
with synthetic signals demonstrate that Half-AVAE outperforms baseline models,
including GP-AVAE and Half-VAE, in recovering independent components under
underdetermined conditions, as evidenced by lower root mean square errors. The
study highlights the flexibility of VAEs in variational inference, showing that
encoder omission, combined with adversarial training and structured priors,
enables effective solutions for complex ICA tasks, advancing applications in
disentanglement, causal inference, and generative modeling.

</details>


### [612] [Quantile-Optimal Policy Learning under Unmeasured Confounding](https://arxiv.org/abs/2506.07140)
*Zhongren Chen,Siyu Chen,Zhengling Qi,Xiaohong Chen,Zhuoran Yang*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study quantile-optimal policy learning where the goal is to find a policy
whose reward distribution has the largest $\alpha$-quantile for some $\alpha
\in (0, 1)$. We focus on the offline setting whose generating process involves
unobserved confounders. Such a problem suffers from three main challenges: (i)
nonlinearity of the quantile objective as a functional of the reward
distribution, (ii) unobserved confounding issue, and (iii) insufficient
coverage of the offline dataset. To address these challenges, we propose a
suite of causal-assisted policy learning methods that provably enjoy strong
theoretical guarantees under mild conditions. In particular, to address (i) and
(ii), using causal inference tools such as instrumental variables and negative
controls, we propose to estimate the quantile objectives by solving nonlinear
functional integral equations. Then we adopt a minimax estimation approach with
nonparametric models to solve these integral equations, and propose to
construct conservative policy estimates that address (iii). The final policy is
the one that maximizes these pessimistic estimates. In addition, we propose a
novel regularized policy learning method that is more amenable to computation.
Finally, we prove that the policies learned by these methods are
$\tilde{\mathscr{O}}(n^{-1/2})$ quantile-optimal under a mild coverage
assumption on the offline dataset. Here, $\tilde{\mathscr{O}}(\cdot)$ omits
poly-logarithmic factors. To the best of our knowledge, we propose the first
sample-efficient policy learning algorithms for estimating the quantile-optimal
policy when there exist unmeasured confounding.

</details>


### [613] [ALINE: Joint Amortization for Bayesian Inference and Active Data Acquisition](https://arxiv.org/abs/2506.07259)
*Daolang Huang,Xinyi Wen,Ayush Bharti,Samuel Kaski,Luigi Acerbi*

Main category: stat.ML

TL;DR: 本文提出了一种名为ALINE的统一框架，通过结合摊销贝叶斯推断和主动数据获取，实现了高效选择信息量最大的数据点并实时进行推理。该方法基于强化学习训练的架构，在多个实验场景中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有摊销贝叶斯推断和实验设计方法在需要即时收集新数据并推理的复杂任务中表现不足，因此需要开发能同时优化数据获取与推理的统一框架。

Method: 使用基于Transformer的架构，通过强化学习训练，奖励机制基于自估计信息增益。框架包含集成推理模块，可定向优化参数子集或特定预测任务的数据查询策略。

Result: 在回归主动学习、贝叶斯实验设计基准测试和参数定向心理测量模型中，ALINE在即时推理精度和数据点选择效率上均优于传统方法。

Conclusion: ALINE通过联合优化数据获取与推理过程，为需要实时决策的应用提供了同时具备高效数据选择能力和高精度推理的解决方案。

Abstract: Many critical applications, from autonomous scientific discovery to
personalized medicine, demand systems that can both strategically acquire the
most informative data and instantaneously perform inference based upon it.
While amortized methods for Bayesian inference and experimental design offer
part of the solution, neither approach is optimal in the most general and
challenging task, where new data needs to be collected for instant inference.
To tackle this issue, we introduce the Amortized Active Learning and Inference
Engine (ALINE), a unified framework for amortized Bayesian inference and active
data acquisition. ALINE leverages a transformer architecture trained via
reinforcement learning with a reward based on self-estimated information gain
provided by its own integrated inference component. This allows it to
strategically query informative data points while simultaneously refining its
predictions. Moreover, ALINE can selectively direct its querying strategy
towards specific subsets of model parameters or designated predictive tasks,
optimizing for posterior estimation, data prediction, or a mixture thereof.
Empirical results on regression-based active learning, classical Bayesian
experimental design benchmarks, and a psychometric model with selectively
targeted parameters demonstrate that ALINE delivers both instant and accurate
inference along with efficient selection of informative points.

</details>


### [614] [Rao-Blackwellised Reparameterisation Gradients](https://arxiv.org/abs/2506.07687)
*Kevin Lam,Thang Bui,George Deligiannidis,Yee Whye Teh*

Main category: stat.ML

TL;DR: 本文提出了R2-G2估计器，作为重参数化梯度估计器的Rao-Blackwell化版本，展示了其在贝叶斯多层感知器中的应用，并证明了其在多重重参数化模型中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 在概率机器学习中，潜在高斯变量和梯度估计器是优化模型的关键。重参数化技巧因其简单实现和低方差梯度而成为默认选择，但本文旨在通过Rao-Blackwell化进一步提升其性能。

Method: 本文提出了R2-G2估计器，作为重参数化梯度估计器的Rao-Blackwell化版本，并展示了其在贝叶斯多层感知器中的具体应用。

Result: 实验表明，使用R2-G2进行初始训练在多重重参数化模型中始终表现更好，验证了其有效性。

Conclusion: R2-G2估计器通过Rao-Blackwell化提升了重参数化梯度估计器的性能，适用于多种概率模型，并展示了其在多重重参数化应用中的优势。

Abstract: Latent Gaussian variables have been popularised in probabilistic machine
learning. In turn, gradient estimators are the machinery that facilitates
gradient-based optimisation for models with latent Gaussian variables. The
reparameterisation trick is often used as the default estimator as it is simple
to implement and yields low-variance gradients for variational inference. In
this work, we propose the R2-G2 estimator as the Rao-Blackwellisation of the
reparameterisation gradient estimator. Interestingly, we show that the local
reparameterisation gradient estimator for Bayesian MLPs is an instance of the
R2-G2 estimator and Rao-Blackwellisation. This lets us extend benefits of
Rao-Blackwellised gradients to a suite of probabilistic models. We show that
initial training with R2-G2 consistently yields better performance in models
with multiple applications of the reparameterisation trick.

</details>


### [615] [Quickest Causal Change Point Detection by Adaptive Intervention](https://arxiv.org/abs/2506.07760)
*Haijie Xu,Chen Zhang*

Main category: stat.ML

TL;DR: 提出一种针对线性因果模型中变化点监测的算法，通过中心化技术将因果传播变化集中至单一维度，并基于KL散度选择干预节点以放大变化幅度。结合自适应干预策略平衡探索与利用，理论证明其一阶最优性，并通过实验验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在因果模型变化点监测中未充分处理干预效应，需解决因果传播带来的复杂变化及如何高效选择干预节点以提高检测灵敏度。

Method: 1. 使用中心化技术集中多节点变化至单维度；2. 基于KL散度筛选干预节点；3. 设计干预值选择算法；4. 提出两种含自适应干预策略的监测方法。

Result: 理论证明方法具有一阶最优性，模拟实验与真实案例（如交通流量监测）显示算法能有效识别变化点并提升检测效率。

Conclusion: 所提方法通过干预节点优化与自适应策略，在理论和实践中均显著提升因果模型变化点监测性能，平衡了探索与利用的权衡。

Abstract: We propose an algorithm for change point monitoring in linear causal models
that accounts for interventions. Through a special centralization technique, we
can concentrate the changes arising from causal propagation across nodes into a
single dimension. Additionally, by selecting appropriate intervention nodes
based on Kullback-Leibler divergence, we can amplify the change magnitude. We
also present an algorithm for selecting the intervention values, which aids in
the identification of the most effective intervention nodes. Two monitoring
methods are proposed, each with an adaptive intervention policy to make a
balance between exploration and exploitation. We theoretically demonstrate the
first-order optimality of the proposed methods and validate their properties
using simulation datasets and two real-world case studies.

</details>


### [616] [Accelerating Constrained Sampling: A Large Deviations Approach](https://arxiv.org/abs/2506.07816)
*Yingli Wang,Changwei Tu,Xiaoyu Wang,Lingjiong Zhu*

Main category: stat.ML

TL;DR: 本文研究了带偏斜对称矩阵的反射朗之万动力学（SRNLD）的长期行为，提出了如何设计偏斜对称矩阵以加速收敛到目标分布，并通过数值实验验证了理论发现。


<details>
  <summary>Details</summary>
Motivation: 在约束域上采样目标概率分布是机器学习等应用中的常见问题。尽管已有研究探讨了SRNLD的非渐近收敛性及其相对于RLD的加速效果，但如何设计偏斜对称矩阵以在实践中获得良好性能仍不明确。

Method: 本文通过建立SRNLD经验测度的大偏差原理（LDP），并明确表征速率函数，研究了偏斜对称矩阵的设计方法，特别是当其与边界上的内向单位法向量场的乘积为零时。

Result: 研究表明，通过合理选择偏斜对称矩阵，SRNLD可以加速收敛到目标分布，数值实验验证了这一理论发现。

Conclusion: 本文通过理论分析和数值实验，证明了合理设计偏斜对称矩阵可以显著提升SRNLD的性能，为约束采样问题提供了新的解决方案。

Abstract: The problem of sampling a target probability distribution on a constrained
domain arises in many applications including machine learning. For constrained
sampling, various Langevin algorithms such as projected Langevin Monte Carlo
(PLMC) based on the discretization of reflected Langevin dynamics (RLD) and
more generally skew-reflected non-reversible Langevin Monte Carlo (SRNLMC)
based on the discretization of skew-reflected non-reversible Langevin dynamics
(SRNLD) have been proposed and studied in the literature. This work focuses on
the long-time behavior of SRNLD, where a skew-symmetric matrix is added to RLD.
Although the non-asymptotic convergence analysis for SRNLD (and SRNLMC) and the
acceleration compared to RLD (and PMLC) have been studied in the literature, it
is not clear how one should design the skew-symmetric matrix in the dynamics to
achieve good performance in practice. We establish a large deviation principle
(LDP) for the empirical measure of SRNLD when the skew-symmetric matrix is
chosen such that its product with the inward unit normal vector field on the
boundary is zero. By explicitly characterizing the rate functions, we show that
SRNLD can accelerate the convergence to the target distribution compared to RLD
with this choice of the skew-symmetric matrix. Numerical experiments for SRNLMC
based on the proposed skew-symmetric matrix show superior performance which
validate the theoretical findings from the large deviations theory.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [617] [AS-ASR: A Lightweight Framework for Aphasia-Specific Automatic Speech Recognition](https://arxiv.org/abs/2506.06566)
*Chen Bao,Chuanbing Huo,Qinyu Chen,Chang Gao*

Main category: eess.AS

TL;DR: 本文提出AS-ASR，一个基于Whisper-tiny的轻量级失语症特定语音识别框架，适用于边缘设备的低资源部署。通过混合训练策略和GPT-4增强方法，显著降低了失语症语音的WER。


<details>
  <summary>Details</summary>
Motivation: 针对失语症患者的语音识别需求，开发一个轻量级且高效的框架，能够在低资源环境下进行部署，并提高识别准确性。

Method: 采用混合训练策略，结合标准语音和失语症语音进行训练，并使用GPT-4增强方法优化噪声转录，提高监督质量。

Result: 实验表明，微调后的模型在失语症语音上的WER降低了30%以上，同时在标准语音上保持了性能。

Conclusion: AS-ASR框架为现实世界中的失语症语音识别提供了一个可扩展且高效的解决方案。

Abstract: This paper proposes AS-ASR, a lightweight aphasia-specific speech recognition
framework based on Whisper-tiny, tailored for low-resource deployment on edge
devices. Our approach introduces a hybrid training strategy that systematically
combines standard and aphasic speech at varying ratios, enabling robust
generalization, and a GPT-4-based reference enhancement method that refines
noisy aphasic transcripts, improving supervision quality. We conduct extensive
experiments across multiple data mixing configurations and evaluation settings.
Results show that our fine-tuned model significantly outperforms the zero-shot
baseline, reducing WER on aphasic speech by over 30% while preserving
performance on standard speech. The proposed framework offers a scalable,
efficient solution for real-world disordered speech recognition.

</details>


### [618] [Reducing Object Hallucination in Large Audio-Language Models via Audio-Aware Decoding](https://arxiv.org/abs/2506.07233)
*Tzu-wen Hsu,Ke-Han Lu,Cheng-Han Chiang,Hung-yi Lee*

Main category: eess.AS

TL;DR: 本文提出一种轻量级推理策略AAD，通过对比解码减少大型音频语言模型（LALMs）的幻觉问题，实验显示其在对象幻觉数据集和通用音频QA任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有LALMs在标准基准测试中表现优异，但存在对音频内容产生幻觉的严重问题，需开发有效抑制方法。

Method: 音频感知解码（AAD）利用对比解码技术，比较含/不含音频上下文的token预测logits，优先选择音频存在时概率提升的token。

Result: 在对象幻觉数据集上F1提升0.046-0.428，Clotho-AQA通用QA准确率提升5.4%-10.3%，消融实验验证各组件有效性。

Conclusion: AAD作为轻量级推理时策略，能有效抑制LALMs幻觉并提升多任务性能，无需额外训练成本。

Abstract: Large Audio-Language Models (LALMs) can take audio and text as the inputs and
answer questions about the audio. While prior LALMs have shown strong
performance on standard benchmarks, there has been alarming evidence that LALMs
can hallucinate what is presented in the audio. To mitigate the hallucination
of LALMs, we introduce Audio-Aware Decoding (AAD), a lightweight inference-time
strategy that uses contrastive decoding to compare the token prediction logits
with and without the audio context. By contrastive decoding, AAD promotes the
tokens whose probability increases when the audio is present. We conduct our
experiment on object hallucination datasets with three LALMs and show that AAD
improves the F1 score by 0.046 to 0.428. We also show that AAD can improve the
accuracy on general audio QA datasets like Clotho-AQA by 5.4% to 10.3%. We
conduct thorough ablation studies to understand the effectiveness of each
component in AAD.

</details>


### [619] [Speaker-Distinguishable CTC: Learning Speaker Distinction Using CTC for Multi-Talker Speech Recognition](https://arxiv.org/abs/2506.07515)
*Asahi Sakuma,Hiroaki Sato,Ryuga Sugano,Tadashi Kumano,Yoshihiko Kawai,Tetsuji Ogawa*

Main category: eess.AS

TL;DR: 本文提出了一种无需辅助信息的多说话者自动语音识别框架，通过扩展CTC并集成到SOT框架中，显著降低了错误率。


<details>
  <summary>Details</summary>
Motivation: 现有的序列化输出训练（SOT）方法因说话者分配失败导致识别错误，且从自然对话中提取辅助信息（如时间戳）具有挑战性。

Method: 提出了Speaker-Distinguishable CTC（SD-CTC），扩展了CTC以联合分配每个帧的标记和说话者标签，并将其集成到SOT框架中。

Result: 实验表明，SD-CTC与SOT的多任务学习将SOT模型的错误率降低了26%，性能接近依赖辅助信息的最先进方法。

Conclusion: SD-CTC与SOT的结合有效提升了多说话者语音识别的准确性，无需依赖辅助信息。

Abstract: This paper presents a novel framework for multi-talker automatic speech
recognition without the need for auxiliary information. Serialized Output
Training (SOT), a widely used approach, suffers from recognition errors due to
speaker assignment failures. Although incorporating auxiliary information, such
as token-level timestamps, can improve recognition accuracy, extracting such
information from natural conversational speech remains challenging. To address
this limitation, we propose Speaker-Distinguishable CTC (SD-CTC), an extension
of CTC that jointly assigns a token and its corresponding speaker label to each
frame. We further integrate SD-CTC into the SOT framework, enabling the SOT
model to learn speaker distinction using only overlapping speech and
transcriptions. Experimental comparisons show that multi-task learning with
SD-CTC and SOT reduces the error rate of the SOT model by 26% and achieves
performance comparable to state-of-the-art methods relying on auxiliary
information.

</details>


### [620] [Neural Spectral Band Generation for Audio Coding](https://arxiv.org/abs/2506.06732)
*Woongjib Choi,Byeong Hyeon Kim,Hyungseob Lim,Inseon Jang,Hong-Goo Kang*

Main category: eess.AS

TL;DR: 本文提出了一种新的参数化非盲音频带宽扩展方法，结合了深度神经网络的前端和后端处理，以替代传统的频谱带复制技术。


<details>
  <summary>Details</summary>
Motivation: 传统的频谱带复制技术在处理多种音频信号时存在局限性，而现有的深度神经网络方法由于缺乏先验信息，性能不佳。

Method: 提出了一种参数化非盲带宽扩展方法，利用深度神经网络在音频编码管道的前端和后端进行信息提取和带宽扩展。

Result: 该方法有望克服传统技术和现有深度神经网络方法的局限性，提供更优的音频带宽扩展效果。

Conclusion: 结合深度神经网络的参数化非盲带宽扩展方法是一种有前景的技术，能够有效提升音频信号的质量。

Abstract: Audio bandwidth extension is the task of reconstructing missing high
frequency components of bandwidth-limited audio signals, where bandwidth
limitation is a common issue for audio signals due to several reasons,
including channel capacity and data constraints. While conventional spectral
band replication is a well-established parametric approach to audio bandwidth
extension, the SBR usually entails coarse feature extraction and reconstruction
techniques, which leads to limitations when processing various types of audio
signals. In parallel, numerous deep neural network-based audio bandwidth
extension methods have been proposed. These DNN-based methods are usually
referred to as blind BWE, as these methods do not rely on prior information
extracted from original signals, and only utilize given low frequency band
signals to estimate missing high frequency components. In order to replace
conventional SBR with DNNs, simply adopting existing DNN-based methodologies
results in suboptimal performance due to the blindness of these methods. My
proposed research suggests a new approach to parametric non-blind bandwidth
extension, as DNN-based side information extraction and DNN-based bandwidth
extension are performed only at the front and end of the audio coding pipeline.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [621] [Poisson Midpoint Method for Log Concave Sampling: Beyond the Strong Error Lower Bounds](https://arxiv.org/abs/2506.07614)
*Rishikesh Srinivasan,Dheeraj Nagaraj*

Main category: math.PR

TL;DR: 本文研究了使用泊松中点离散化方法在强对数凹分布下的采样问题，证明其在2-Wasserstein距离上的收敛性，相比欧拉-丸山方法实现了三次加速，并在欠阻尼Langevin动力学中展示了更优的复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有随机中点方法在Langevin动力学的收敛性分析中仍存在对目标精度ε的依赖效率不足的问题，需探索更优的离散化方法以突破复杂度界限。

Method: 采用泊松中点离散化（随机中点法的变体），结合过阻尼/欠阻尼Langevin动力学，分析其在2-Wasserstein距离（W2）下的收敛性。

Result: 在W2距离下，该方法对ε的依赖实现了三次加速，且欠阻尼Langevin动力学的W2收敛复杂度显著低于文献中L2强误差的复杂度下限。

Conclusion: 泊松中点离散化在采样效率上显著优于传统方法，尤其为欠阻尼Langevin动力学提供了更高效的收敛保证，突破了现有理论复杂度下界。

Abstract: We study the problem of sampling from strongly log-concave distributions over
$\mathbb{R}^d$ using the Poisson midpoint discretization (a variant of the
randomized midpoint method) for overdamped/underdamped Langevin dynamics. We
prove its convergence in the 2-Wasserstein distance ($W_2$), achieving a cubic
speedup in dependence on the target accuracy ($\epsilon$) over the
Euler-Maruyama discretization, surpassing existing bounds for randomized
midpoint methods. Notably, in the case of underdamped Langevin dynamics, we
demonstrate the complexity of $W_2$ convergence is much smaller than the
complexity lower bounds for convergence in $L^2$ strong error established in
the literature.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [622] [Noise Consistency Regularization for Improved Subject-Driven Image Synthesis](https://arxiv.org/abs/2506.06483)
*Yao Ni,Song Wen,Piotr Koniusz,Anoop Cherian*

Main category: cs.GR

TL;DR: 本文提出两种辅助一致性损失函数，用于改进Stable Diffusion的微调过程，解决欠拟合和过拟合问题，提升图像多样性和主题一致性。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法在生成包含特定主题的图像时，存在欠拟合和过拟合问题，导致模型无法可靠捕捉主题身份或减少背景多样性。

Method: 提出两种辅助一致性损失：先验一致性正则化损失和主题一致性正则化损失，分别用于保持先验图像的扩散噪声一致性和增强模型对噪声调制潜码的鲁棒性。

Result: 实验表明，引入这些损失后，微调模型在CLIP分数、背景变化和整体视觉质量上优于DreamBooth，同时保持了主题身份并提升了图像多样性。

Conclusion: 通过引入辅助一致性损失，本文方法有效解决了微调中的欠拟合和过拟合问题，显著提升了图像生成的质量和多样性。

Abstract: Fine-tuning Stable Diffusion enables subject-driven image synthesis by
adapting the model to generate images containing specific subjects. However,
existing fine-tuning methods suffer from two key issues: underfitting, where
the model fails to reliably capture subject identity, and overfitting, where it
memorizes the subject image and reduces background diversity. To address these
challenges, we propose two auxiliary consistency losses for diffusion
fine-tuning. First, a prior consistency regularization loss ensures that the
predicted diffusion noise for prior (non-subject) images remains consistent
with that of the pretrained model, improving fidelity. Second, a subject
consistency regularization loss enhances the fine-tuned model's robustness to
multiplicative noise modulated latent code, helping to preserve subject
identity while improving diversity. Our experimental results demonstrate that
incorporating these losses into fine-tuning not only preserves subject identity
but also enhances image diversity, outperforming DreamBooth in terms of CLIP
scores, background variation, and overall visual quality.

</details>


### [623] [Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization](https://arxiv.org/abs/2506.07069)
*Zhican Wang,Guanghui He,Dantong Liu,Lingjun Gao,Shell Xu Hu,Chen Zhang,Zhuoran Song,Nicholas Lane,Wayne Luk,Hongxiang Fan*

Main category: cs.GR

TL;DR: 本文提出了一种架构-算法协同设计，通过轴导向光栅化、神经排序方法和可重构处理阵列，显著提升了3D高斯泼溅在资源受限设备上的实时渲染效率。


<details>
  <summary>Details</summary>
Motivation: 尽管3D高斯泼溅在算法性能上表现出色，但在资源受限设备上实现实时渲染仍面临巨大挑战，主要受限于功耗和面积预算。

Method: 本文提出了轴导向光栅化以减少重复计算，引入神经排序方法替代硬件排序器，并设计了可重构处理阵列以支持光栅化和神经网络推理。此外，还提出了π轨迹瓦片调度以优化高斯重用和减少内存访问开销。

Result: 实验表明，该设计在保持渲染质量的同时，相比边缘GPU实现了23.4~27.8倍的加速和28.8~51.4倍的节能。

Conclusion: 本文提出的架构-算法协同设计有效解决了3D高斯泼溅在资源受限设备上的实时渲染问题，并计划开源以促进该领域的进一步发展。

Abstract: 3D Gaussian Splatting (3DGS) has recently gained significant attention for
high-quality and efficient view synthesis, making it widely adopted in fields
such as AR/VR, robotics, and autonomous driving. Despite its impressive
algorithmic performance, real-time rendering on resource-constrained devices
remains a major challenge due to tight power and area budgets. This paper
presents an architecture-algorithm co-design to address these inefficiencies.
First, we reveal substantial redundancy caused by repeated computation of
common terms/expressions during the conventional rasterization. To resolve
this, we propose axis-oriented rasterization, which pre-computes and reuses
shared terms along both the X and Y axes through a dedicated hardware design,
effectively reducing multiply-and-add (MAC) operations by up to 63%. Second, by
identifying the resource and performance inefficiency of the sorting process,
we introduce a novel neural sorting approach that predicts order-independent
blending weights using an efficient neural network, eliminating the need for
costly hardware sorters. A dedicated training framework is also proposed to
improve its algorithmic stability. Third, to uniformly support rasterization
and neural network inference, we design an efficient reconfigurable processing
array that maximizes hardware utilization and throughput. Furthermore, we
introduce a $\pi$-trajectory tile schedule, inspired by Morton encoding and
Hilbert curve, to optimize Gaussian reuse and reduce memory access overhead.
Comprehensive experiments demonstrate that the proposed design preserves
rendering quality while achieving a speedup of $23.4\sim27.8\times$ and energy
savings of $28.8\sim51.4\times$ compared to edge GPUs for real-world scenes. We
plan to open-source our design to foster further development in this field.

</details>


### [624] [GaussianVAE: Adaptive Learning Dynamics of 3D Gaussians for High-Fidelity Super-Resolution](https://arxiv.org/abs/2506.07897)
*Shuja Khalid,Mohamed Ibrahim,Yang Liu*

Main category: cs.GR

TL;DR: 本文提出一种轻量级生成模型，通过Hessian辅助采样策略实时增强3D高斯溅射的分辨率与几何保真度，突破传统方法受限于训练分辨率的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法因输入分辨率限制无法生成比训练视图更精细的细节，亟需突破该分辨率依赖性的技术方案。

Method: 采用轻量级模型预测补充3D高斯分布，结合Hessian矩阵智能识别需密集化区域，实现计算高效的局部优化。

Result: 在消费级GPU上达到单次推理0.015秒的实时性能，几何精度与渲染质量显著优于SOTA方法，支持交互式操作。

Conclusion: 该方法建立了分辨率无关的3D场景增强新范式，通过生成式局部优化实现了质量与效率的平衡。

Abstract: We present a novel approach for enhancing the resolution and geometric
fidelity of 3D Gaussian Splatting (3DGS) beyond native training resolution.
Current 3DGS methods are fundamentally limited by their input resolution,
producing reconstructions that cannot extrapolate finer details than are
present in the training views. Our work breaks this limitation through a
lightweight generative model that predicts and refines additional 3D Gaussians
where needed most. The key innovation is our Hessian-assisted sampling
strategy, which intelligently identifies regions that are likely to benefit
from densification, ensuring computational efficiency. Unlike computationally
intensive GANs or diffusion approaches, our method operates in real-time
(0.015s per inference on a single consumer-grade GPU), making it practical for
interactive applications. Comprehensive experiments demonstrate significant
improvements in both geometric accuracy and rendering quality compared to
state-of-the-art methods, establishing a new paradigm for resolution-free 3D
scene enhancement.

</details>


### [625] [Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural Compressor](https://arxiv.org/abs/2506.07932)
*Rishit Dagli,Yushi Guan,Sankeerth Durvasula,Mohammadreza Mofayezi,Nandita Vijaykumar*

Main category: cs.GR

TL;DR: Squeeze3D利用预训练3D生成模型的隐式先验知识，通过映射网络连接编码器与生成模型的潜在空间，实现3D数据超高压缩比（如网格2187倍），支持多种格式且无需真实数据集训练。


<details>
  <summary>Details</summary>
Motivation: 现有3D数据压缩方法在压缩比和跨格式兼容性上存在局限，需依赖大量真实数据集训练。Squeeze3D旨在通过预训练模型的隐式知识实现高效、通用的压缩方案。

Method: 结合预训练编码器与生成模型，通过可训练映射网络将输入数据压缩为紧凑潜在编码，再利用生成模型解码还原。全程使用合成数据训练，支持网格/点云/辐射场等多种格式。

Result: 实现网格2187x、点云55x、辐射场619x的压缩比，视觉质量与现有方法相当，且压缩/解压延迟低（无需对象特定训练）。

Conclusion: Squeeze3D证明了预训练模型隐式知识在3D压缩中的有效性，提供高兼容性、高压缩比且低延迟的解决方案，无需真实数据集支持。

Abstract: We propose Squeeze3D, a novel framework that leverages implicit prior
knowledge learnt by existing pre-trained 3D generative models to compress 3D
data at extremely high compression ratios. Our approach bridges the latent
spaces between a pre-trained encoder and a pre-trained generation model through
trainable mapping networks. Any 3D model represented as a mesh, point cloud, or
a radiance field is first encoded by the pre-trained encoder and then
transformed (i.e. compressed) into a highly compact latent code. This latent
code can effectively be used as an extremely compressed representation of the
mesh or point cloud. A mapping network transforms the compressed latent code
into the latent space of a powerful generative model, which is then conditioned
to recreate the original 3D model (i.e. decompression). Squeeze3D is trained
entirely on generated synthetic data and does not require any 3D datasets. The
Squeeze3D architecture can be flexibly used with existing pre-trained 3D
encoders and existing generative models. It can flexibly support different
formats, including meshes, point clouds, and radiance fields. Our experiments
demonstrate that Squeeze3D achieves compression ratios of up to 2187x for
textured meshes, 55x for point clouds, and 619x for radiance fields while
maintaining visual quality comparable to many existing methods. Squeeze3D only
incurs a small compression and decompression latency since it does not involve
training object-specific networks to compress an object.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [626] [HyColor: An Efficient Heuristic Algorithm for Graph Coloring](https://arxiv.org/abs/2506.07373)
*Enqiang Zhu,Yu Zhang,Haopeng Sun,Ziqi Wei,Witold Pedrycz,Chanjuan Liu,Jin Xu*

Main category: cs.DM

TL;DR: 本文提出了一种名为HyColor的高效混合启发式算法，用于解决图着色问题（GCP）。该算法通过局部决策、图归约和基于k-core的贪心策略，显著提升大规模稀疏图和小型密集图的着色效率与精度，在209个测试实例中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有GCP算法主要针对小规模难解图或大规模稀疏图（顶点数达10^7），但缺乏同时高效处理两类图的能力。本文旨在设计一种兼顾大规模稀疏图和小型密集图的通用高效算法。

Method: HyColor结合三种策略：1) 局部决策策略提升色数下限；2) 图归约策略减少计算规模；3) 基于k-core和混合度的贪心启发式着色方法，平衡计算效率与解的质量。

Result: 在包含209个实例的四个基准测试中，HyColor在194个实例（93%）中取得最优解，其中34个结果显著优于其他算法，并在128个实例中确定色数且实现最优着色，整体表现超越现有算法。

Conclusion: HyColor通过多策略融合，在GCP问题上实现了大规模稀疏图的高效处理与小型密集图的高精度求解，验证了混合启发式方法在组合优化问题中的有效性。

Abstract: The graph coloring problem (GCP) is a classic combinatorial optimization
problem that aims to find the minimum number of colors assigned to vertices of
a graph such that no two adjacent vertices receive the same color. GCP has been
extensively studied by researchers from various fields, including mathematics,
computer science, and biological science. Due to the NP-hard nature, many
heuristic algorithms have been proposed to solve GCP. However, existing GCP
algorithms focus on either small hard graphs or large-scale sparse graphs (with
up to 10^7 vertices). This paper presents an efficient hybrid heuristic
algorithm for GCP, named HyColor, which excels in handling large-scale sparse
graphs while achieving impressive results on small dense graphs. The efficiency
of HyColor comes from the following three aspects: a local decision strategy to
improve the lower bound on the chromatic number; a graph-reduction strategy to
reduce the working graph; and a k-core and mixed degree-based greedy heuristic
for efficiently coloring graphs. HyColor is evaluated against three
state-of-the-art GCP algorithms across four benchmarks, comprising three
large-scale sparse graph benchmarks and one small dense graph benchmark,
totaling 209 instances. The results demonstrate that HyColor consistently
outperforms existing heuristic algorithms in both solution accuracy and
computational efficiency for the majority of instances. Notably, HyColor
achieved the best solutions in 194 instances (over 93%), with 34 of these
solutions significantly surpassing those of other algorithms. Furthermore,
HyColor successfully determined the chromatic number and achieved optimal
coloring in 128 instances.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [627] [LLM-D12: A Dual-Dimensional Scale of Instrumental and Relational Dependencies on Large Language Models](https://arxiv.org/abs/2506.06874)
*Ala Yankouskaya,Areej B. Babiker,Syeda W. F. Rizvi,Sameha Alshakhsi,Magnus Liebherr,Raian Ali*

Main category: cs.HC

TL;DR: 本文开发并验证了一个名为LLM-D12的12项问卷，用于测量人们对大型语言模型（LLMs）的依赖程度，揭示了工具依赖和关系依赖两个维度。


<details>
  <summary>Details</summary>
Motivation: 随着人们对LLMs的依赖增加，了解这种依赖是否会导致成瘾行为变得重要。现有工具主要基于行为成瘾症状，缺乏针对LLMs依赖的专门评估工具。

Method: 基于作者先前的理论工作，开发了LLM-D12问卷，收集了526名英国参与者的数据，并通过探索性和验证性因子分析验证了问卷的两因素结构。

Result: LLM-D12问卷显示出良好的内部一致性和区分效度，支持工具依赖和关系依赖两个维度，分别反映个体在决策和认知任务中对LLMs的依赖，以及将LLMs视为有社会意义的实体的倾向。

Conclusion: LLM-D12问卷为评估LLMs依赖提供了新的视角，表明这种依赖不一定是功能失调，但在某些情境下可能成为问题。

Abstract: There is growing interest in understanding how people interact with large
language models (LLMs) and whether such models elicit dependency or even
addictive behaviour. Validated tools to assess the extent to which individuals
may become dependent on LLMs are scarce and primarily build on classic
behavioral addiction symptoms, adapted to the context of LLM use. We view this
as a conceptual limitation, as the LLM-human relationship is more nuanced and
warrants a fresh and distinct perspective. To address this gap, we developed
and validated a new 12-item questionnaire to measure LLM dependency, referred
to as LLM-D12. The scale was based on the authors' prior theoretical work, with
items developed accordingly and responses collected from 526 participants in
the UK. Exploratory and confirmatory factor analyses, performed on separate
halves of the total sample using a split-sample approach, supported a
two-factor structure: Instrumental Dependency (six items) and Relationship
Dependency (six items). Instrumental Dependency reflects the extent to which
individuals rely on LLMs to support or collaborate in decision-making and
cognitive tasks. Relationship Dependency captures the tendency to perceive LLMs
as socially meaningful, sentient, or companion-like entities. The two-factor
structure demonstrated excellent internal consistency and clear discriminant
validity. External validation confirmed both the conceptual foundation and the
distinction between the two subscales. The psychometric properties and
structure of our LLM-D12 scale were interpreted in light of the emerging view
that dependency on LLMs does not necessarily indicate dysfunction but may still
reflect reliance levels that could become problematic in certain contexts.

</details>


### [628] [Sword and Shield: Uses and Strategies of LLMs in Navigating Disinformation](https://arxiv.org/abs/2506.07211)
*Gionnieve Lim,Bryan Chen Zhengyu Tan,Kellie Yu Hui Sim,Weiyan Shi,Ming Hui Chew,Ming Shan Hee,Roy Ka-Wei Lee,Simon T. Perrault,Kenny Tsu Wei Choo*

Main category: cs.HC

TL;DR: 大型语言模型（LLMs）在虚假信息传播中具有双重性：既可能被滥用于生成高说服力虚假信息，也能辅助检测与应对。研究通过模拟在线论坛的沟通游戏，揭示不同角色（散布者、版主、用户）对LLMs的策略性使用，强调平衡LLM开发与平台设计的重要性。


<details>
  <summary>Details</summary>
Motivation: LLMs的普及带来虚假信息传播风险与治理机遇的双重挑战，需探究其在不同角色中的实际应用效果及潜在滥用可能性。

Method: 设计基于‘狼人杀’启发的在线论坛模拟游戏，招募25名参与者分别扮演散布者、版主和用户，分析其利用LLMs达成目标的具体策略。

Result: 不同角色对LLMs的使用策略差异显著：散布者生成更隐蔽的虚假内容，版主依赖LLMs增强检测能力，用户则需工具辅助辨别信息真实性。

Conclusion: 未来LLM开发与平台设计需采取平衡策略，在增强用户赋权与信任的同时，限制LLM辅助虚假信息的风险，例如通过透明化机制与多角色协作框架。

Abstract: The emergence of Large Language Models (LLMs) presents a dual challenge in
the fight against disinformation. These powerful tools, capable of generating
human-like text at scale, can be weaponised to produce sophisticated and
persuasive disinformation, yet they also hold promise for enhancing detection
and mitigation strategies. This paper investigates the complex dynamics between
LLMs and disinformation through a communication game that simulates online
forums, inspired by the game Werewolf, with 25 participants. We analyse how
Disinformers, Moderators, and Users leverage LLMs to advance their goals,
revealing both the potential for misuse and combating disinformation. Our
findings highlight the varying uses of LLMs depending on the participants'
roles and strategies, underscoring the importance of understanding their
effectiveness in this context. We conclude by discussing implications for
future LLM development and online platform design, advocating for a balanced
approach that empowers users and fosters trust while mitigating the risks of
LLM-assisted disinformation.

</details>


### [629] [Secondary Stakeholders in AI: Fighting for, Brokering, and Navigating Agency](https://arxiv.org/abs/2506.07281)
*Leah Hope Ajmani,Nuredin Ali Abdelkadir,Stevie Chancellor*

Main category: cs.HC

TL;DR: 本文探讨了如何将参与式AI理念扩展到次要利益相关者，提出了三个参与理想：知情、同意和能动性，并通过半结构化访谈分析了次要利益相关者如何实现这些理想。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术越来越面向人类，许多呼吁要求采用参与式方法开发AI，但这些呼吁通常只关注主要利益相关者（如终端用户），而忽略了次要利益相关者。本文旨在将参与式AI理念扩展到更广泛的次要利益相关者群体。

Method: 通过半结构化访谈，本文探讨了次要利益相关者如何实现参与式AI的三个理想：知情、同意和能动性。

Result: 本文提出了三个次要利益相关者原型：不情愿的数据贡献者、未得到支持的活动家和善意的实践者，并分析了他们在实现能动性AI关系时面临的系统性障碍。

Conclusion: 本文展望了一个次要利益相关者能够有意义地参与AI系统的未来，这些系统既影响他们，也受他们影响。

Abstract: As AI technologies become more human-facing, there have been numerous calls
to adapt participatory approaches to AI development -- spurring the idea of
participatory AI. However, these calls often focus only on primary
stakeholders, such as end-users, and not secondary stakeholders. This paper
seeks to translate the ideals of participatory AI to a broader population of
secondary AI stakeholders through semi-structured interviews. We theorize that
meaningful participation involves three participatory ideals: (1) informedness,
(2) consent, and (3) agency. We also explore how secondary stakeholders realize
these ideals by traversing a complicated problem space. Like walking up the
rungs of a ladder, these ideals build on one another. We introduce three
stakeholder archetypes: the reluctant data contributor, the unsupported
activist, and the well-intentioned practitioner, who must navigate systemic
barriers to achieving agentic AI relationships. We envision an AI future where
secondary stakeholders are able to meaningfully participate with the AI systems
they influence and are influenced by.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [630] [Optimal Transport Driven Asymmetric Image-to-Image Translation for Nuclei Segmentation of Histological Images](https://arxiv.org/abs/2506.07023)
*Suman Mahapatra,Pradipta Maji*

Main category: eess.IV

TL;DR: 本文提出了一种新的深度生成模型，用于从组织学图像中分割细胞核结构，通过处理信息不对称问题，实现了网络复杂性与性能的更好平衡。


<details>
  <summary>Details</summary>
Motivation: 细胞核区域的分割有助于疾病的检测和诊断，但现有的图像到图像翻译模型在处理信息不对称时表现不佳。

Method: 提出了一种深度生成模型，结合最优传输和测度理论，开发了可逆生成器，并引入了空间约束的压缩操作。

Result: 该模型在公开的组织学图像数据集上表现优异，与现有方法相比，在复杂性和性能之间取得了更好的平衡。

Conclusion: 所提出的模型通过处理信息不对称问题，提供了一种高效的细胞核分割方法，具有较低的复杂性和较高的性能。

Abstract: Segmentation of nuclei regions from histological images enables morphometric
analysis of nuclei structures, which in turn helps in the detection and
diagnosis of diseases under consideration. To develop a nuclei segmentation
algorithm, applicable to different types of target domain representations,
image-to-image translation networks can be considered as they are invariant to
target domain image representations. One of the important issues with
image-to-image translation models is that they fail miserably when the
information content between two image domains are asymmetric in nature. In this
regard, the paper introduces a new deep generative model for segmenting nuclei
structures from histological images. The proposed model considers an embedding
space for handling information-disparity between information-rich histological
image space and information-poor segmentation map domain. Integrating
judiciously the concepts of optimal transport and measure theory, the model
develops an invertible generator, which provides an efficient optimization
framework with lower network complexity. The concept of invertible generator
automatically eliminates the need of any explicit cycle-consistency loss. The
proposed model also introduces a spatially-constrained squeeze operation within
the framework of invertible generator to maintain spatial continuity within the
image patches. The model provides a better trade-off between network complexity
and model performance compared to other existing models having complex network
architectures. The performance of the proposed deep generative model, along
with a comparison with state-of-the-art nuclei segmentation methods, is
demonstrated on publicly available histological image data sets.

</details>


### [631] [SiliCoN: Simultaneous Nuclei Segmentation and Color Normalization of Histological Images](https://arxiv.org/abs/2506.07028)
*Suman Mahapatra,Pradipta Maji*

Main category: eess.IV

TL;DR: 本文提出了一种新的深度生成模型，用于同时分割细胞核结构和标准化染色组织图像的颜色外观，解决了颜色变化和染色重叠问题。


<details>
  <summary>Details</summary>
Motivation: 在组织学图像分析中，细胞核区域的准确分割对于自动化分析至关重要，尤其是在染色图像存在不可接受的颜色变化时。

Method: 该模型结合了截断正态分布和空间注意力的优点，假设潜在颜色外观信息与细胞核分割图和嵌入图信息独立，并采用混合截断正态分布作为潜在颜色外观代码的先验。

Result: 所提出的方法在公开的标准组织学图像数据集上进行了验证，并与相关的最先进算法进行了比较分析，展示了其优越性能。

Conclusion: 该模型通过解耦表示和空间注意力机制，实现了对细胞核区域的准确分割和颜色外观的标准化，具有较高的通用性和适应性。

Abstract: Segmentation of nuclei regions from histological images is an important task
for automated computer-aided analysis of histological images, particularly in
the presence of impermissible color variation in the color appearance of
stained tissue images. While color normalization enables better nuclei
segmentation, accurate segmentation of nuclei structures makes color
normalization rather trivial. In this respect, the paper proposes a novel deep
generative model for simultaneously segmenting nuclei structures and
normalizing color appearance of stained histological images.This model
judiciously integrates the merits of truncated normal distribution and spatial
attention. The model assumes that the latent color appearance information,
corresponding to a particular histological image, is independent of respective
nuclei segmentation map as well as embedding map information. The disentangled
representation makes the model generalizable and adaptable as the modification
or loss in color appearance information cannot be able to affect the nuclei
segmentation map as well as embedding information. Also, for dealing with the
stain overlap of associated histochemical reagents, the prior for latent color
appearance code is assumed to be a mixture of truncated normal distributions.
The proposed model incorporates the concept of spatial attention for
segmentation of nuclei regions from histological images. The performance of the
proposed approach, along with a comparative analysis with related
state-of-the-art algorithms, has been demonstrated on publicly available
standard histological image data sets.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [632] [From Axioms to Algorithms: Mechanized Proofs of the vNM Utility Theorem](https://arxiv.org/abs/2506.07066)
*Li Jingyuan*

Main category: econ.TH

TL;DR: 本文使用Lean 4交互式定理证明器对冯·诺依曼-摩根斯坦期望效用定理进行了全面形式化，验证了偏好关系的数学结构，并提供了机器验证的效用表示存在性和唯一性证明。


<details>
  <summary>Details</summary>
Motivation: 本文旨在通过形式化方法，为经济建模、AI对齐和管理决策系统提供严格的理论基础，弥合理论决策理论与计算实现之间的差距。

Method: 本文实现了偏好完备性、传递性、连续性和独立性等经典公理，并通过机器验证证明了效用表示的存在性和唯一性。

Result: 本文成功形式化了冯·诺依曼-摩根斯坦期望效用定理，验证了偏好关系可以通过期望效用最大化表示，并提供了计算实验验证结果。

Conclusion: 本文的形式化为相关领域提供了精确的理论基础，展示了形式化方法在决策理论中的潜力。

Abstract: This paper presents a comprehensive formalization of the von
Neumann-Morgenstern (vNM) expected utility theorem using the Lean 4 interactive
theorem prover. We implement the classical axioms of preference-completeness,
transitivity, continuity, and independence-enabling machine-verified proofs of
both the existence and uniqueness of utility representations. Our formalization
captures the mathematical structure of preference relations over lotteries,
verifying that preferences satisfying the vNM axioms can be represented by
expected utility maximization.
  Our contributions include a granular implementation of the independence
axiom, formally verified proofs of fundamental claims about mixture lotteries,
constructive demonstrations of utility existence, and computational experiments
validating the results. We prove equivalence to classical presentations while
offering greater precision at decision boundaries.
  This formalization provides a rigorous foundation for applications in
economic modeling, AI alignment, and management decision systems, bridging the
gap between theoretical decision theory and computational implementation.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [633] [Template-Guided 3D Molecular Pose Generation via Flow Matching and Differentiable Optimization](https://arxiv.org/abs/2506.06305)
*Noémie Bergues,Arthur Carré,Paul Join-Lambert,Brice Hoffmann,Arnaud Blondel,Hamza Tajmouati*

Main category: q-bio.BM

TL;DR: 本文提出了一种基于模板引导的两阶段配体构象生成方法，结合分子对齐与可微分优化，显著提升了低相似度或高灵活性配体的3D姿态预测精度。


<details>
  <summary>Details</summary>
Motivation: 在药物设计中，准确预测小分子在蛋白质结合位点的3D构象是关键挑战。现有方法在模板配体可用时能利用几何先验，但在模板相似性低或配体灵活性高时效果有限，需开发更鲁棒的预测方法。

Method: 两阶段方法：1) 基于流匹配的分子对齐生成初始3D坐标；2) 可微分姿态优化，结合形状/药效团相似性、内部能量及蛋白结合位点信息细化构象。

Result: 在共结晶配体对的新基准测试中，该方法优于标准对接工具和公开对齐方法，尤其在低相似性（模板差异大）或高配体灵活性场景下表现突出。

Conclusion: 模板引导的两阶段框架有效整合几何先验与多目标优化，为复杂配体构象预测提供了新解决方案，对药物设计具有实用价值。

Abstract: Predicting the 3D conformation of small molecules within protein binding
sites is a key challenge in drug design. When a crystallized reference ligand
(template) is available, it provides geometric priors that can guide 3D pose
prediction. We present a two-stage method for ligand conformation generation
guided by such templates. In the first stage, we introduce a molecular
alignment approach based on flow-matching to generate 3D coordinates for the
ligand, using the template structure as a reference. In the second stage, a
differentiable pose optimization procedure refines this conformation based on
shape and pharmacophore similarities, internal energy, and, optionally, the
protein binding pocket. We evaluate our approach on a new benchmark of ligand
pairs co-crystallized with the same target and show that it outperforms
standard docking tools and open-access alignment methods, especially in cases
involving low similarity to the template or high ligand flexibility.

</details>


### [634] [AnnoDPO: Protein Functional Annotation Learning with Direct Preference Optimization](https://arxiv.org/abs/2506.07035)
*Zixuan Jiang,Renjing Xu*

Main category: q-bio.BM

TL;DR: 论文提出了一种名为AnnoDPO的多模态框架，用于蛋白质功能预测，通过直接偏好优化（DPO）增强注释学习，解决注释稀缺和类别不平衡的挑战。


<details>
  <summary>Details</summary>
Motivation: 蛋白质功能解析是蛋白质表示学习中的一个基本挑战，由于功能注释类别的庞大数量和生物本体学中注释实例的高度不平衡分布，蛋白质语言模型（PLMs）在这一任务上面临显著困难。

Method: 提出AnnoDPO框架，利用直接偏好优化（DPO）进行多模态蛋白质功能预测，通过偏好对齐的训练目标解决注释稀缺和类别不平衡的问题。

Result: AnnoDPO框架为生物知识整合在蛋白质表示学习中建立了一个新的范式。

Conclusion: AnnoDPO通过偏好对齐的训练目标，成功解决了蛋白质功能预测中的注释稀缺和类别不平衡问题，为蛋白质表示学习提供了新的方法。

Abstract: Deciphering protein function remains a fundamental challenge in protein
representation learning. The task presents significant difficulties for protein
language models (PLMs) due to the sheer volume of functional annotation
categories and the highly imbalanced distribution of annotated instances across
biological ontologies. Inspired by the remarkable success of reinforcement
learning from human feedback (RLHF) in large language model (LLM) alignment, we
propose AnnoDPO, a novel multi-modal framework for protein function prediction
that leverages Direct Preference Optimization (DPO) to enhance annotation
learning. Our methodology addresses the dual challenges of annotation scarcity
and category imbalance through preference-aligned training objectives,
establishing a new paradigm for biological knowledge integration in protein
representation learning.

</details>


### [635] [Graph Neural Networks in Modern AI-aided Drug Discovery](https://arxiv.org/abs/2506.06915)
*Odin Zhang,Haitao Lin,Xujun Zhang,Xiaorui Wang,Zhenxing Wu,Qing Ye,Weibo Zhao,Jike Wang,Kejun Ying,Yu Kang,Chang-yu Hsieh,Tingjun Hou*

Main category: q-bio.BM

TL;DR: 本文综述了图神经网络（GNNs）在AI辅助药物发现（AIDD）中的应用，涵盖了方法基础、代表性应用及最新技术进展。


<details>
  <summary>Details</summary>
Motivation: GNNs作为深度学习中的拓扑/结构感知模型，能够直接操作分子图，为学习药物分子的复杂拓扑和几何特征提供了直观且富有表现力的框架，因此在现代分子建模中占据重要地位。

Method: 本文综述了GNNs在药物发现中的方法基础，包括分子属性预测、虚拟筛选、分子生成、生物医学知识图谱构建和合成规划等任务，并特别关注了几何GNNs、可解释模型、不确定性量化、可扩展图架构和图生成框架等最新技术进展。

Result: GNNs在药物发现中展示了强大的应用潜力，能够与自监督学习、多任务学习、元学习和预训练等现代深度学习方法有效结合。

Conclusion: 尽管GNNs在药物发现中取得了显著进展，但在实际应用中仍面临挑战和方法瓶颈，未来需要进一步探索和优化。

Abstract: Graph neural networks (GNNs), as topology/structure-aware models within deep
learning, have emerged as powerful tools for AI-aided drug discovery (AIDD). By
directly operating on molecular graphs, GNNs offer an intuitive and expressive
framework for learning the complex topological and geometric features of
drug-like molecules, cementing their role in modern molecular modeling. This
review provides a comprehensive overview of the methodological foundations and
representative applications of GNNs in drug discovery, spanning tasks such as
molecular property prediction, virtual screening, molecular generation,
biomedical knowledge graph construction, and synthesis planning. Particular
attention is given to recent methodological advances, including geometric GNNs,
interpretable models, uncertainty quantification, scalable graph architectures,
and graph generative frameworks. We also discuss how these models integrate
with modern deep learning approaches, such as self-supervised learning,
multi-task learning, meta-learning and pre-training. Throughout this review, we
highlight the practical challenges and methodological bottlenecks encountered
when applying GNNs to real-world drug discovery pipelines, and conclude with a
discussion on future directions.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [636] [Tactile MNIST: Benchmarking Active Tactile Perception](https://arxiv.org/abs/2506.06361)
*Tim Schneider,Guillaume Duret,Cristiana de Farias,Roberto Calandra,Liming Chen,Jan Peters*

Main category: cs.RO

TL;DR: 本文提出Tactile MNIST Benchmark Suite，一个用于主动触觉感知任务的开源基准套件，包含合成与真实触觉数据集，并通过CycleGAN实现高仿真渲染，以解决触觉感知与主动感知领域缺乏标准化评估的问题。


<details>
  <summary>Details</summary>
Motivation: 触觉感知虽能增强机器人灵巧操作，但其局部性限制了全局场景理解能力。现有研究缺乏标准化基准，阻碍了主动触觉感知技术的系统化发展。

Method: 开发兼容Gymnasium的标准化基准套件，包含定位、分类、体积估计等任务；构建13,500个合成3D MNIST模型及153,600个真实触觉样本数据集，并利用CycleGAN实现触觉仿真渲染。

Result: 成功创建多场景仿真环境与跨模态数据集，CycleGAN显著提升触觉渲染真实性，提供可复现评估框架，为触觉感知研究建立统一基准。

Conclusion: 该基准套件填补了触觉感知与主动感知领域的标准化空白，通过开源协议与数据集共享，推动系统化技术进展，为未来研究提供可扩展基础。

Abstract: Tactile perception has the potential to significantly enhance dexterous
robotic manipulation by providing rich local information that can complement or
substitute for other sensory modalities such as vision. However, because
tactile sensing is inherently local, it is not well-suited for tasks that
require broad spatial awareness or global scene understanding on its own. A
human-inspired strategy to address this issue is to consider active perception
techniques instead. That is, to actively guide sensors toward regions with more
informative or significant features and integrate such information over time in
order to understand a scene or complete a task. Both active perception and
different methods for tactile sensing have received significant attention
recently. Yet, despite advancements, both fields lack standardized benchmarks.
To bridge this gap, we introduce the Tactile MNIST Benchmark Suite, an
open-source, Gymnasium-compatible benchmark specifically designed for active
tactile perception tasks, including localization, classification, and volume
estimation. Our benchmark suite offers diverse simulation scenarios, from
simple toy environments all the way to complex tactile perception tasks using
vision-based tactile sensors. Furthermore, we also offer a comprehensive
dataset comprising 13,500 synthetic 3D MNIST digit models and 153,600
real-world tactile samples collected from 600 3D printed digits. Using this
dataset, we train a CycleGAN for realistic tactile simulation rendering. By
providing standardized protocols and reproducible evaluation frameworks, our
benchmark suite facilitates systematic progress in the fields of tactile
sensing and active perception.

</details>


### [637] [CPS-Guard: Framework for Dependability Assurance of AI- and LLM-Based Cyber-Physical Systems](https://arxiv.org/abs/2506.06381)
*Trisanth Srinivasan,Santosh Patapati,Himani Musku,Idhant Gode,Aditya Arora,Samvit Bhattacharya,Abubakr Nazriev,Sanika Hirave,Zaryab Kanjiani,Srinjoy Ghose,Srinidhi Shetty*

Main category: cs.RO

TL;DR: 提出CPS-Guard框架，通过多角色协调自动化验证AI驱动的信息物理系统，案例研究表明其能有效检测漏洞并支持自适应恢复。


<details>
  <summary>Details</summary>
Motivation: 传统验证方法难以应对AI组件的动态性与不可预测性，需开发新框架以提升关键系统中AI行为的可靠性与安全性。

Method: 采用多角色代理(安全监控/安全评估/故障注入/恢复规划)在模拟环境中持续迭代评估AI行为，满足可靠性需求。

Result: 自动驾驶交叉路口案例证明框架可检测系统漏洞，管理性能影响，并实现动态恢复策略，验证了方法的有效性。

Conclusion: CPS-Guard为安全关键系统提供了结构化、可扩展的严格验证方案，解决了AI组件动态验证的核心挑战。

Abstract: Cyber-Physical Systems (CPS) increasingly depend on advanced AI techniques to
operate in critical applications. However, traditional verification and
validation methods often struggle to handle the unpredictable and dynamic
nature of AI components. In this paper, we introduce CPS-Guard, a novel
framework that employs multi-role orchestration to automate the iterative
assurance process for AI-powered CPS. By assigning specialized roles (e.g.,
safety monitoring, security assessment, fault injection, and recovery planning)
to dedicated agents within a simulated environment, CPS-Guard continuously
evaluates and refines AI behavior against a range of dependability
requirements. We demonstrate the framework through a case study involving an
autonomous vehicle navigating an intersection with an AI-based planner. Our
results show that CPS-Guard effectively detects vulnerabilities, manages
performance impacts, and supports adaptive recovery strategies, thereby
offering a structured and extensible solution for rigorous V&V in safety- and
security-critical systems.

</details>


### [638] [Edge-Enabled Collaborative Object Detection for Real-Time Multi-Vehicle Perception](https://arxiv.org/abs/2506.06474)
*Everett Richards,Bipul Thapa,Lena Mashayekhy*

Main category: cs.RO

TL;DR: 本文提出了一种基于边缘计算和多车协作的实时多视角物体检测框架ECOD，通过PACE和VOTE算法显著提升物体分类精度，适用于动态环境中的自动驾驶。


<details>
  <summary>Details</summary>
Motivation: 传统车载感知系统因遮挡和盲区导致精度有限，而基于云的解决方案存在显著延迟，无法满足自动驾驶在动态环境中的实时处理需求。

Method: ECOD框架结合了PACE和VOTE算法，PACE在边缘服务器上聚合多车检测数据以增强感知，VOTE通过共识投票机制提升物体分类精度。

Result: 实验结果表明，ECOD在物体分类精度上比传统单视角车载方法提升高达75%，同时确保低延迟的边缘驱动实时处理。

Conclusion: 该研究展示了边缘计算在提升延迟敏感型自动驾驶系统协作感知方面的潜力。

Abstract: Accurate and reliable object detection is critical for ensuring the safety
and efficiency of Connected Autonomous Vehicles (CAVs). Traditional on-board
perception systems have limited accuracy due to occlusions and blind spots,
while cloud-based solutions introduce significant latency, making them
unsuitable for real-time processing demands required for autonomous driving in
dynamic environments. To address these challenges, we introduce an innovative
framework, Edge-Enabled Collaborative Object Detection (ECOD) for CAVs, that
leverages edge computing and multi-CAV collaboration for real-time,
multi-perspective object detection. Our ECOD framework integrates two key
algorithms: Perceptive Aggregation and Collaborative Estimation (PACE) and
Variable Object Tally and Evaluation (VOTE). PACE aggregates detection data
from multiple CAVs on an edge server to enhance perception in scenarios where
individual CAVs have limited visibility. VOTE utilizes a consensus-based voting
mechanism to improve the accuracy of object classification by integrating data
from multiple CAVs. Both algorithms are designed at the edge to operate in
real-time, ensuring low-latency and reliable decision-making for CAVs. We
develop a hardware-based controlled testbed consisting of camera-equipped
robotic CAVs and an edge server to evaluate the efficacy of our framework. Our
experimental results demonstrate the significant benefits of ECOD in terms of
improved object classification accuracy, outperforming traditional
single-perspective onboard approaches by up to 75%, while ensuring low-latency,
edge-driven real-time processing. This research highlights the potential of
edge computing to enhance collaborative perception for latency-sensitive
autonomous systems.

</details>


### [639] [Active Test-time Vision-Language Navigation](https://arxiv.org/abs/2506.06630)
*Heeju Ko,Sungjune Kim,Gyeongrok Oh,Jeongyoon Yoon,Honglak Lee,Sujin Jang,Seungryong Kim,Sangpil Kim*

Main category: cs.RO

TL;DR: 本文提出了ATENA框架，通过主动学习和混合熵优化，解决视觉语言导航在陌生环境中的性能下降问题，并在多个基准测试中取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 视觉语言导航（VLN）策略在离线数据集上训练后，在陌生环境中部署时性能下降，且测试时无法获得外部反馈。熵最小化虽能减少预测不确定性，但可能导致错误累积。

Method: 提出ATENA框架，结合主动学习和混合熵优化，通过伪专家分布和动作分布的混合熵控制预测置信度和动作偏好，并引入自主动学习策略评估导航结果。

Result: 在REVERIE、R2R和R2R-CE等基准测试中，ATENA成功克服了测试时的分布偏移，表现优于基线方法。

Conclusion: ATENA通过主动学习和混合熵优化，提升了视觉语言导航在陌生环境中的适应性和决策能力，显著改善了测试性能。

Abstract: Vision-Language Navigation (VLN) policies trained on offline datasets often
exhibit degraded task performance when deployed in unfamiliar navigation
environments at test time, where agents are typically evaluated without access
to external interaction or feedback. Entropy minimization has emerged as a
practical solution for reducing prediction uncertainty at test time; however,
it can suffer from accumulated errors, as agents may become overconfident in
incorrect actions without sufficient contextual grounding. To tackle these
challenges, we introduce ATENA (Active TEst-time Navigation Agent), a test-time
active learning framework that enables a practical human-robot interaction via
episodic feedback on uncertain navigation outcomes. In particular, ATENA learns
to increase certainty in successful episodes and decrease it in failed ones,
improving uncertainty calibration. Here, we propose mixture entropy
optimization, where entropy is obtained from a combination of the action and
pseudo-expert distributions-a hypothetical action distribution assuming the
agent's selected action to be optimal-controlling both prediction confidence
and action preference. In addition, we propose a self-active learning strategy
that enables an agent to evaluate its navigation outcomes based on confident
predictions. As a result, the agent stays actively engaged throughout all
iterations, leading to well-grounded and adaptive decision-making. Extensive
evaluations on challenging VLN benchmarks-REVERIE, R2R, and R2R-CE-demonstrate
that ATENA successfully overcomes distributional shifts at test time,
outperforming the compared baseline methods across various settings.

</details>


### [640] [Self-Adapting Improvement Loops for Robotic Learning](https://arxiv.org/abs/2506.06658)
*Calvin Luo,Zilai Zeng,Mingxi Jia,Yilun Du,Chen Sun*

Main category: cs.RO

TL;DR: 提出了自我适应改进循环（SAIL），通过自我收集的行为迭代更新视频生成模型，以解决机器人任务中的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 虽然视频生成模型在专家演示上训练后能作为文本条件视觉规划器解决机器人任务，但在未见任务上的泛化能力仍有限。本文旨在设计能通过在线自我收集行为持续改进的智能体。

Method: 提出自我适应改进循环（SAIL），利用互联网规模预训练视频模型进行适应，并通过自我生成的轨迹迭代更新域内视频模型。

Result: 在MetaWorld任务和真实机器人手臂操作任务中，SAIL在多次迭代后持续提升性能，且对自我收集经验的过滤和初始演示质量表现出鲁棒性。

Conclusion: 通过互联网规模数据的适应和在线经验学习，SAIL展示了通过自我改进迭代引导高性能视频模型解决新机器人任务的方法。

Abstract: Video generative models trained on expert demonstrations have been utilized
as performant text-conditioned visual planners for solving robotic tasks.
However, generalization to unseen tasks remains a challenge. Whereas improved
generalization may be facilitated by leveraging learned prior knowledge from
additional pre-collected offline data sources, such as web-scale video
datasets, in the era of experience we aim to design agents that can
continuously improve in an online manner from self-collected behaviors. In this
work we thus propose the Self-Adapting Improvement Loop (SAIL), where an
in-domain video model iteratively updates itself on self-produced trajectories,
collected through adaptation with an internet-scale pretrained video model, and
steadily improves its performance for a specified task of interest. We apply
SAIL to a diverse suite of MetaWorld tasks, as well as two manipulation tasks
on a real robot arm, and find that performance improvements continuously emerge
over multiple iterations for novel tasks initially unseen during original
in-domain video model training. Furthermore, we discover that SAIL is
surprisingly robust regarding if and how the self-collected experience is
filtered, and the quality of the initial in-domain demonstrations. Through
adaptation with summarized internet-scale data, and learning through online
experience, we thus demonstrate a way to iteratively bootstrap a
high-performance video model for solving novel robotic tasks through
self-improvement.

</details>


### [641] [DriveSuprim: Towards Precise Trajectory Selection for End-to-End Planning](https://arxiv.org/abs/2506.06659)
*Wenhao Yao,Zhenxin Li,Shiyi Lan,Zi Wang,Xinglong Sun,Jose M. Alvarez,Zuxuan Wu*

Main category: cs.RO

TL;DR: DriveSuprim提出了一种从粗到细的渐进候选过滤、基于旋转的增强方法和自蒸馏框架，以提高自主车辆在复杂驾驶环境中的安全性和轨迹质量。


<details>
  <summary>Details</summary>
Motivation: 在复杂驾驶环境中，自主车辆必须安全导航。现有的回归方法依赖单一预测路径，无法明确评估预测轨迹的安全性，而选择方法在从数千种可能性中精确选择最佳选项时面临优化挑战。

Method: DriveSuprim通过从粗到细的渐进候选过滤、基于旋转的增强方法和自蒸馏框架来克服这些挑战，提高在分布外场景中的鲁棒性和训练稳定性。

Result: DriveSuprim在NAVSIM v1和v2中分别达到了93.5% PDMS和87.1% EPDMS，展示了在碰撞避免和规则遵守方面的卓越安全能力，同时保持了高轨迹质量。

Conclusion: DriveSuprim通过其创新方法在自主车辆导航中实现了最先进的性能，显著提高了安全性和轨迹质量。

Abstract: In complex driving environments, autonomous vehicles must navigate safely.
Relying on a single predicted path, as in regression-based approaches, usually
does not explicitly assess the safety of the predicted trajectory.
Selection-based methods address this by generating and scoring multiple
trajectory candidates and predicting the safety score for each, but face
optimization challenges in precisely selecting the best option from thousands
of possibilities and distinguishing subtle but safety-critical differences,
especially in rare or underrepresented scenarios. We propose DriveSuprim to
overcome these challenges and advance the selection-based paradigm through a
coarse-to-fine paradigm for progressive candidate filtering, a rotation-based
augmentation method to improve robustness in out-of-distribution scenarios, and
a self-distillation framework to stabilize training. DriveSuprim achieves
state-of-the-art performance, reaching 93.5% PDMS in NAVSIM v1 and 87.1% EPDMS
in NAVSIM v2 without extra data, demonstrating superior safetycritical
capabilities, including collision avoidance and compliance with rules, while
maintaining high trajectory quality in various driving scenarios.

</details>


### [642] [RoboPARA: Dual-Arm Robot Planning with Parallel Allocation and Recomposition Across Tasks](https://arxiv.org/abs/2506.06683)
*Shiying Duan,Pei Ren,Nanxiang Jiang,Zhengping Che,Jian Tang,Yifan Sun,Zhaoxin Fan,Wenjun Wu*

Main category: cs.RO

TL;DR: 本文提出RoboPARA框架，通过两阶段依赖图规划及图重遍历优化双臂机器人任务并行性，并引入跨场景数据集X-DAPT。实验表明其效率与可靠性显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有双臂机器人任务规划方法未充分优化任务并行性，限制了协作潜力。需提升复杂多任务场景下的效率与灵活性。

Method: 1. 基于有向无环图(DAG)生成任务依赖关系候选方案；2. 通过图重遍历优化双臂并行规划。同时提出首个双臂并行任务评估数据集X-DAPT。

Result: 在X-DAPT数据集上，RoboPARA在复杂任务组合中表现出更高效率与可靠性，显著超越现有方法。

Conclusion: RoboPARA有效解决了双臂任务并行性优化问题，其框架与数据集为未来研究提供了新方向与基准。

Abstract: Dual-arm robots play a crucial role in improving efficiency and flexibility
in complex multitasking scenarios. While existing methods have achieved
promising results in task planning, they often fail to fully optimize task
parallelism, limiting the potential of dual-arm collaboration. To address this
issue, we propose RoboPARA, a novel large language model (LLM)-driven framework
for dual-arm task parallelism planning. RoboPARA employs a two-stage process:
(1) Dependency Graph-based Planning Candidates Generation, which constructs
directed acyclic graphs (DAGs) to model task dependencies and eliminate
redundancy, and (2) Graph Re-Traversal-based Dual-Arm Parallel Planning, which
optimizes DAG traversal to maximize parallelism while maintaining task
coherence. In addition, we introduce the Cross-Scenario Dual-Arm Parallel Task
dataset (X-DAPT dataset), the first dataset specifically designed to evaluate
dual-arm task parallelism across diverse scenarios and difficulty levels.
Extensive experiments on the X-DAPT dataset demonstrate that RoboPARA
significantly outperforms existing methods, achieving higher efficiency and
reliability, particularly in complex task combinations. The code and dataset
will be released upon acceptance.

</details>


### [643] [Multimodal Spatial Language Maps for Robot Navigation and Manipulation](https://arxiv.org/abs/2506.06862)
*Chenguang Huang,Oier Mees,Andy Zeng,Wolfram Burgard*

Main category: cs.RO

TL;DR: 本文提出多模态空间语言地图（VLMaps/AVLMaps），通过融合预训练多模态特征与3D环境重建，结合大语言模型实现零样本空间导航与多模态目标定位，在模糊场景下召回率提升50%。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在环境地图割裂、空间精度不足、忽视非视觉模态等问题。本文旨在构建融合多模态信息的空间地图，增强机器人对自然语言指令和复杂环境的理解能力。

Method: 1) 构建视觉-语言地图（VLMaps）及其音频扩展（AVLMaps），通过自主探索生成融合多模态特征的3D地图；2) 结合大语言模型解析自然语言指令为空间目标；3) 支持跨平台生成定制障碍物地图，并整合音频信息增强目标消歧。

Result: 实验表明：1) 在仿真与真实场景中实现零样本空间/多模态目标导航；2) 模糊场景目标召回率提升50%；3) 支持移动机器人及桌面机械臂的跨模态导航与交互。

Conclusion: 多模态空间语言地图通过统一表征视觉、语言、音频信息，显著提升机器人对复杂指令和模糊环境的理解能力，为多模态人机交互提供了可扩展的解决方案。

Abstract: Grounding language to a navigating agent's observations can leverage
pretrained multimodal foundation models to match perceptions to object or event
descriptions. However, previous approaches remain disconnected from environment
mapping, lack the spatial precision of geometric maps, or neglect additional
modality information beyond vision. To address this, we propose multimodal
spatial language maps as a spatial map representation that fuses pretrained
multimodal features with a 3D reconstruction of the environment. We build these
maps autonomously using standard exploration. We present two instances of our
maps, which are visual-language maps (VLMaps) and their extension to
audio-visual-language maps (AVLMaps) obtained by adding audio information. When
combined with large language models (LLMs), VLMaps can (i) translate natural
language commands into open-vocabulary spatial goals (e.g., "in between the
sofa and TV") directly localized in the map, and (ii) be shared across
different robot embodiments to generate tailored obstacle maps on demand.
Building upon the capabilities above, AVLMaps extend VLMaps by introducing a
unified 3D spatial representation integrating audio, visual, and language cues
through the fusion of features from pretrained multimodal foundation models.
This enables robots to ground multimodal goal queries (e.g., text, images, or
audio snippets) to spatial locations for navigation. Additionally, the
incorporation of diverse sensory inputs significantly enhances goal
disambiguation in ambiguous environments. Experiments in simulation and
real-world settings demonstrate that our multimodal spatial language maps
enable zero-shot spatial and multimodal goal navigation and improve recall by
50% in ambiguous scenarios. These capabilities extend to mobile robots and
tabletop manipulators, supporting navigation and interaction guided by visual,
audio, and spatial cues.

</details>


### [644] [CARoL: Context-aware Adaptation for Robot Learning](https://arxiv.org/abs/2506.07006)
*Zechen Hu,Tong Xu,Xuesu Xiao,Xuan Wang*

Main category: cs.RO

TL;DR: 提出CARoL框架，通过上下文感知和自适应知识迁移，提升强化学习在机器人新任务中的效率，验证了其在仿真和实际环境中的有效性。


<details>
  <summary>Details</summary>
Motivation: 强化学习从零开始学习新机器人任务效率低，且现有知识迁移面临相关性判断与自适应整合两大挑战。

Method: CARoL通过分析系统动态中的状态转移识别任务间相似性，动态调整知识优先级，并支持策略、价值、演员-评论家三类强化学习算法。

Result: 在CarRacing、LunarLander仿真中收敛更快且奖励更高；实物地面车辆成功将仿真策略迁移至真实越野场景。

Conclusion: CARoL通过上下文感知机制有效解决了知识迁移的核心挑战，具备算法普适性与跨仿真-现实场景的强泛化能力。

Abstract: Using Reinforcement Learning (RL) to learn new robotic tasks from scratch is
often inefficient. Leveraging prior knowledge has the potential to
significantly enhance learning efficiency, which, however, raises two critical
challenges: how to determine the relevancy of existing knowledge and how to
adaptively integrate them into learning a new task. In this paper, we propose
Context-aware Adaptation for Robot Learning (CARoL), a novel framework to
efficiently learn a similar but distinct new task from prior knowledge. CARoL
incorporates context awareness by analyzing state transitions in system
dynamics to identify similarities between the new task and prior knowledge. It
then utilizes these identified similarities to prioritize and adapt specific
knowledge pieces for the new task. Additionally, CARoL has a broad
applicability spanning policy-based, value-based, and actor-critic RL
algorithms. We validate the efficiency and generalizability of CARoL on both
simulated robotic platforms and physical ground vehicles. The simulations
include CarRacing and LunarLander environments, where CARoL demonstrates faster
convergence and higher rewards when learning policies for new tasks. In
real-world experiments, we show that CARoL enables a ground vehicle to quickly
and efficiently adapt policies learned in simulation to smoothly traverse
real-world off-road terrain.

</details>


### [645] [Prime the search: Using large language models for guiding geometric task and motion planning by warm-starting tree search](https://arxiv.org/abs/2506.07062)
*Dongryung Lee,Sejune Joo,Kimin Lee,Beomjoon Kim*

Main category: cs.RO

TL;DR: 本文提出利用大型语言模型（LLMs）的常识知识来指导几何任务与运动规划（G-TAMP）问题，通过设计基于谓词的提示进行几何推理，并使用LLM生成任务计划，结合蒙特卡罗树搜索（MCTS）进行搜索优化。


<details>
  <summary>Details</summary>
Motivation: 传统的G-TAMP方法依赖领域无关的启发式方法或从规划经验中学习，这些方法通常需要大量计算资源或数据。受人类使用常识解决G-TAMP问题的启发，本文提出利用LLMs的常识知识来指导任务规划。

Method: 设计基于谓词的提示，利用LLM生成任务计划，并将其与MCTS结合，通过LLM引导搜索，减少计算成本。

Result: 在六个不同的G-TAMP问题上，本文方法优于之前的LLM规划器和纯搜索算法。

Conclusion: 本文方法通过结合LLMs的常识知识和MCTS的搜索能力，有效提升了G-TAMP问题的解决效率。

Abstract: The problem of relocating a set of objects to designated areas amidst movable
obstacles can be framed as a Geometric Task and Motion Planning (G-TAMP)
problem, a subclass of task and motion planning (TAMP). Traditional approaches
to G-TAMP have relied either on domain-independent heuristics or on learning
from planning experience to guide the search, both of which typically demand
significant computational resources or data. In contrast, humans often use
common sense to intuitively decide which objects to manipulate in G-TAMP
problems. Inspired by this, we propose leveraging Large Language Models (LLMs),
which have common sense knowledge acquired from internet-scale data, to guide
task planning in G-TAMP problems. To enable LLMs to perform geometric
reasoning, we design a predicate-based prompt that encodes geometric
information derived from a motion planning algorithm. We then query the LLM to
generate a task plan, which is then used to search for a feasible set of
continuous parameters. Since LLMs are prone to mistakes, instead of committing
to LLM's outputs, we extend Monte Carlo Tree Search (MCTS) to a hybrid action
space and use the LLM to guide the search. Unlike the previous approach that
calls an LLM at every node and incurs high computational costs, we use it to
warm-start the MCTS with the nodes explored in completing the LLM's task plan.
On six different G-TAMP problems, we show our method outperforms previous LLM
planners and pure search algorithms. Code can be found at:
https://github.com/iMSquared/prime-the-search

</details>


### [646] [Robotic Policy Learning via Human-assisted Action Preference Optimization](https://arxiv.org/abs/2506.07127)
*Wenke xia,Yichu Yang,Hongtao Wu,Xiao Ma,Tao Kong,Di Hu*

Main category: cs.RO

TL;DR: 提出HAPO方法，通过人类辅助动作偏好优化解决VLA模型依赖专家示范的问题，提升机器人系统容错与自适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作（VLA）模型依赖专家示范，导致无法有效纠正错误并从失败中学习，限制了实际部署的可靠性。

Method: 结合人机协作框架收集人类干预轨迹，设计自适应重加权算法优化动作偏好，解决不可逆交互与概率不匹配问题。

Result: 仿真与真实场景实验表明，该方法在多种操作任务中具有更优的泛化性与鲁棒性。

Conclusion: HAPO通过人类干预与偏好对齐机制，使VLA模型实现可靠部署与失败学习，为机器人系统提供可迭代优化路径。

Abstract: Establishing a reliable and iteratively refined robotic system is essential
for deploying real-world applications. While Vision-Language-Action (VLA)
models are widely recognized as the foundation model for such robotic
deployment, their dependence on expert demonstrations hinders the crucial
capabilities of correction and learning from failures. To mitigate this
limitation, we introduce a Human-assisted Action Preference Optimization method
named HAPO, designed to correct deployment failures and foster effective
adaptation through preference alignment for VLA models. This method begins with
a human-robot collaboration framework for reliable failure correction and
interaction trajectory collection through human intervention. These
human-intervention trajectories are further employed within the action
preference optimization process, facilitating VLA models to mitigate failure
action occurrences while enhancing corrective action adaptation. Specifically,
we propose an adaptive reweighting algorithm to address the issues of
irreversible interactions and token probability mismatch when introducing
preference optimization into VLA models, facilitating model learning from
binary desirability signals derived from interactions. Through combining these
modules, our human-assisted action preference optimization method ensures
reliable deployment and effective learning from failure for VLA models. The
experiments conducted in simulation and real-world scenarios prove superior
generalization and robustness of our framework across a variety of manipulation
tasks.

</details>


### [647] [Real-Time Execution of Action Chunking Flow Policies](https://arxiv.org/abs/2506.07339)
*Kevin Black,Manuel Y. Galliker,Sergey Levine*

Main category: cs.RO

TL;DR: 本文提出了一种新的推理时间算法——实时分块（RTC），用于解决现代AI系统在物理世界交互中的高延迟问题，显著提高了任务吞吐量和精确任务的成功率。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统，尤其是与物理世界交互的系统，越来越需要实时性能。然而，包括最新的视觉语言动作模型（VLA）在内的最先进通用模型的高延迟问题，成为了一个重大挑战。

Method: 本文提出了一种新的推理时间算法——实时分块（RTC），该算法适用于任何基于扩散或流的VLA，无需重新训练。RTC在执行当前动作块的同时生成下一个动作块，确保动作的平滑执行。

Result: 通过在Kinetix模拟器中的12个高动态任务和6个具有挑战性的现实世界双手操作任务中的测试，结果表明RTC快速、高效，并且对推理延迟具有独特的鲁棒性，显著提高了任务吞吐量和精确任务的成功率。

Conclusion: RTC算法有效解决了AI系统在物理世界交互中的高延迟问题，显著提高了任务吞吐量和精确任务的成功率，为现代AI系统的实时性能提供了新的解决方案。

Abstract: Modern AI systems, especially those interacting with the physical world,
increasingly require real-time performance. However, the high latency of
state-of-the-art generalist models, including recent vision-language action
models (VLAs), poses a significant challenge. While action chunking has enabled
temporal consistency in high-frequency control tasks, it does not fully address
the latency problem, leading to pauses or out-of-distribution jerky movements
at chunk boundaries. This paper presents a novel inference-time algorithm that
enables smooth asynchronous execution of action chunking policies. Our method,
real-time chunking (RTC), is applicable to any diffusion- or flow-based VLA out
of the box with no re-training. It generates the next action chunk while
executing the current one, "freezing" actions guaranteed to execute and
"inpainting" the rest. To test RTC, we introduce a new benchmark of 12 highly
dynamic tasks in the Kinetix simulator, as well as evaluate 6 challenging
real-world bimanual manipulation tasks. Results demonstrate that RTC is fast,
performant, and uniquely robust to inference delay, significantly improving
task throughput and enabling high success rates in precise tasks
$\unicode{x2013}$ such as lighting a match $\unicode{x2013}$ even in the
presence of significant latency. See
https://pi.website/research/real_time_chunking for videos.

</details>


### [648] [Machine Learning-Based Self-Localization Using Internal Sensors for Automating Bulldozers](https://arxiv.org/abs/2506.07271)
*Hikaru Sawafuji,Ryota Ozaki,Takuto Motomura,Toyohisa Matsuda,Masanori Tojima,Kento Uchida,Shinichi Shirakawa*

Main category: cs.RO

TL;DR: 本文提出了一种基于机器学习的推土机自定位方法，通过内部传感器估计局部速度，并结合扩展卡尔曼滤波器进行全局定位，有效减少了位置误差累积。


<details>
  <summary>Details</summary>
Motivation: 传统的推土机自定位系统依赖RTK-GNSS信号，但在某些采矿条件下信号可能丢失，因此需要不依赖RTK-GNSS的自定位方法。

Method: 该方法分为两步：首先使用机器学习模型从内部传感器估计局部速度，然后将这些估计值结合到扩展卡尔曼滤波器中进行全局定位。

Result: 实验表明，与基于运动学的方法相比，该方法在发生滑动时能有效抑制位置误差的累积，且推土机专用传感器（如铲刀位置传感器和液压压力传感器）提高了定位精度。

Conclusion: 基于机器学习的自定位方法在推土机应用中表现出色，尤其在信号丢失或滑动情况下，推土机专用传感器对提升定位精度有显著贡献。

Abstract: Self-localization is an important technology for automating bulldozers.
Conventional bulldozer self-localization systems rely on RTK-GNSS (Real Time
Kinematic-Global Navigation Satellite Systems). However, RTK-GNSS signals are
sometimes lost in certain mining conditions. Therefore, self-localization
methods that do not depend on RTK-GNSS are required. In this paper, we propose
a machine learning-based self-localization method for bulldozers. The proposed
method consists of two steps: estimating local velocities using a machine
learning model from internal sensors, and incorporating these estimates into an
Extended Kalman Filter (EKF) for global localization. We also created a novel
dataset for bulldozer odometry and conducted experiments across various driving
scenarios, including slalom, excavation, and driving on slopes. The result
demonstrated that the proposed self-localization method suppressed the
accumulation of position errors compared to kinematics-based methods,
especially when slip occurred. Furthermore, this study showed that
bulldozer-specific sensors, such as blade position sensors and hydraulic
pressure sensors, contributed to improving self-localization accuracy.

</details>


### [649] [Language-Grounded Hierarchical Planning and Execution with Multi-Robot 3D Scene Graphs](https://arxiv.org/abs/2506.07454)
*Jared Strader,Aaron Ray,Jacob Arkin,Mason B. Peterson,Yun Chang,Nathan Hughes,Christopher Bradley,Yi Xuan Jia,Carlos Nieto-Granda,Rajat Talak,Chuchu Fan,Luca Carlone,Jonathan P. How,Nicholas Roy*

Main category: cs.RO

TL;DR: 本文提出了一种结合3D场景图的多机器人系统，通过自然语言指令实现复杂任务的映射、定位与规划，并利用大语言模型将操作意图转化为PDDL目标。


<details>
  <summary>Details</summary>
Motivation: 现有系统在多机器人协作中面临复杂指令执行、实时环境感知与规划效率的挑战，需一种集成化解决方案以提升任务执行能力。

Method: 构建共享3D场景图支持开放集物体地图融合，结合实时重定位与任务运动规划，并利用LLM将自然语言指令转换为PDDL目标。

Result: 系统在大型户外环境中完成真实任务实验验证，展示了多机器人协作的实时规划与复杂指令执行能力。

Conclusion: 所提系统通过3D场景图与LLM的结合，实现了高效的多机器人协作与自然语言交互，为复杂环境任务提供了可扩展的解决方案。

Abstract: In this paper, we introduce a multi-robot system that integrates mapping,
localization, and task and motion planning (TAMP) enabled by 3D scene graphs to
execute complex instructions expressed in natural language. Our system builds a
shared 3D scene graph incorporating an open-set object-based map, which is
leveraged for multi-robot 3D scene graph fusion. This representation supports
real-time, view-invariant relocalization (via the object-based map) and
planning (via the 3D scene graph), allowing a team of robots to reason about
their surroundings and execute complex tasks. Additionally, we introduce a
planning approach that translates operator intent into Planning Domain
Definition Language (PDDL) goals using a Large Language Model (LLM) by
leveraging context from the shared 3D scene graph and robot capabilities. We
provide an experimental assessment of the performance of our system on
real-world tasks in large-scale, outdoor environments.

</details>


### [650] [BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models](https://arxiv.org/abs/2506.07961)
*Peiyan Li,Yixiang Chen,Hongtao Wu,Xiao Ma,Xiangnan Wu,Yan Huang,Liang Wang,Tao Kong,Tieniu Tan*

Main category: cs.RO

TL;DR: BridgeVLA是一种新型的3D视觉-语言-动作模型，通过将3D输入投影到多个2D图像并利用2D热图进行动作预测，显著提高了机器人操作学习的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型在动作预测中很少利用3D信号，且未能充分利用3D数据的空间结构，导致样本效率低下。

Method: BridgeVLA将3D输入投影到多个2D图像，确保与视觉-语言模型骨干对齐，并利用2D热图进行动作预测，统一输入和输出空间。此外，提出了一种可扩展的预训练方法，使模型在策略学习前具备预测2D热图的能力。

Result: BridgeVLA在三个仿真基准测试中均优于现有方法，在RLBench中将平均成功率从81.4%提升至88.2%，在COLOSSEUM中从56.7%提升至64.0%，在GemBench中表现最佳。在真实机器人实验中，平均优于现有方法32%，并在多种分布外设置中表现出色。

Conclusion: BridgeVLA通过有效利用3D数据和2D热图，显著提高了机器人操作学习的效率和泛化能力，展示了其在复杂任务中的卓越表现。

Abstract: Recently, leveraging pre-trained vision-language models (VLMs) for building
vision-language-action (VLA) models has emerged as a promising approach to
effective robot manipulation learning. However, only few methods incorporate 3D
signals into VLMs for action prediction, and they do not fully leverage the
spatial structure inherent in 3D data, leading to low sample efficiency. In
this paper, we introduce BridgeVLA, a novel 3D VLA model that (1) projects 3D
inputs to multiple 2D images, ensuring input alignment with the VLM backbone,
and (2) utilizes 2D heatmaps for action prediction, unifying the input and
output spaces within a consistent 2D image space. In addition, we propose a
scalable pre-training method that equips the VLM backbone with the capability
to predict 2D heatmaps before downstream policy learning. Extensive experiments
show the proposed method is able to learn 3D manipulation efficiently and
effectively. BridgeVLA outperforms state-of-the-art baseline methods across
three simulation benchmarks. In RLBench, it improves the average success rate
from 81.4% to 88.2%. In COLOSSEUM, it demonstrates significantly better
performance in challenging generalization settings, boosting the average
success rate from 56.7% to 64.0%. In GemBench, it surpasses all the comparing
baseline methods in terms of average success rate. In real-robot experiments,
BridgeVLA outperforms a state-of-the-art baseline method by 32% on average. It
generalizes robustly in multiple out-of-distribution settings, including visual
disturbances and unseen instructions. Remarkably, it is able to achieve a
success rate of 96.8% on 10+ tasks with only 3 trajectories per task,
highlighting its extraordinary sample efficiency. Project
Website:https://bridgevla.github.io/

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [651] [Deep regularization networks for inverse problems with noisy operators](https://arxiv.org/abs/2506.07008)
*Fatemeh Pourahmadian,Yang Xu*

Main category: math.NA

TL;DR: 本文提出了一种监督学习方法，用于加速大逆问题的正则化过程，特别是在噪声数据构建的主算子下，通过神经网络映射散射方程的右侧模式到其关联的正则化参数，从而实现实时成像。


<details>
  <summary>Details</summary>
Motivation: 研究旨在加速逆散射理论中的时空正则化过程，以实现实时成像，特别是在复杂环境中提高图像质量。

Method: 采用两步训练策略：首先在低分辨率正则化图上训练，然后通过最小化Tikhonov损失函数优化网络预测，无需先验知识即可生成高质量图像。

Result: 实验结果表明，基于差异原则的正则化网络不仅加速了成像过程，还在复杂环境中显著提高了图像质量。

Conclusion: 本文提出的方法通过直接学习测试数据，无需先验知识，能够快速生成高分辨率图像，并显著提升图像质量。

Abstract: A supervised learning approach is proposed for regularization of large
inverse problems where the main operator is built from noisy data. This is
germane to superresolution imaging via the sampling indicators of the inverse
scattering theory. We aim to accelerate the spatiotemporal regularization
process for this class of inverse problems to enable real-time imaging. In this
approach, a neural operator maps each pattern on the right-hand side of the
scattering equation to its affiliated regularization parameter. The network is
trained in two steps which entails: (1) training on low-resolution
regularization maps furnished by the Morozov discrepancy principle with
nonoptimal thresholds, and (2) optimizing network predictions through
minimization of the Tikhonov loss function regulated by the validation loss.
Step 2 allows for tailoring of the approximate maps of Step 1 toward
construction of higher quality images. This approach enables direct learning
from test data and dispenses with the need for a-priori knowledge of the
optimal regularization maps. The network, trained on low-resolution data,
quickly generates dense regularization maps for high-resolution imaging. We
highlight the importance of the training loss function on the network's
generalizability. In particular, we demonstrate that networks informed by the
logic of discrepancy principle lead to images of higher contrast. In this case,
the training process involves many-objective optimization. We propose a new
method to adaptively select the appropriate loss weights during training
without requiring an additional optimization process. The proposed approach is
synthetically examined for imaging damage evolution in an elastic plate. The
results indicate that the discrepancy-informed regularization networks not only
accelerate the imaging process, but also remarkably enhance the image quality
in complex environments.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [652] [\textit{QuantMCP}: Grounding Large Language Models in Verifiable Financial Reality](https://arxiv.org/abs/2506.06622)
*Yifan Zeng*

Main category: cs.CE

TL;DR: QuantMCP框架通过标准化和安全的工具调用，使大语言模型能够准确访问实时金融数据，提升其在金融分析中的可靠性和深度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在金融分析和决策中具有巨大潜力，但其直接应用常受数据幻觉和缺乏实时可验证金融信息的限制。

Method: 引入QuantMCP框架，利用模型上下文协议（MCP）标准化和安全的工具调用，使大语言模型能够准确访问多种Python金融数据API。

Result: QuantMCP框架成功克服了大语言模型在事实数据回忆中的固有局限，解锁了其分析能力，支持更复杂的金融决策。

Conclusion: QuantMCP为对话式AI与复杂金融数据之间提供了强大、可扩展且安全的桥梁，增强了金融领域大语言模型应用的可靠性和分析深度。

Abstract: Large Language Models (LLMs) hold immense promise for revolutionizing
financial analysis and decision-making, yet their direct application is often
hampered by issues of data hallucination and lack of access to real-time,
verifiable financial information. This paper introduces QuantMCP, a novel
framework designed to rigorously ground LLMs in financial reality. By
leveraging the Model Context Protocol (MCP) for standardized and secure tool
invocation, QuantMCP enables LLMs to accurately interface with a diverse array
of Python-accessible financial data APIs (e.g., Wind, yfinance). Users can
interact via natural language to precisely retrieve up-to-date financial data,
thereby overcoming LLM's inherent limitations in factual data recall. More
critically, once furnished with this verified, structured data, the LLM's
analytical capabilities are unlocked, empowering it to perform sophisticated
data interpretation, generate insights, and ultimately support more informed
financial decision-making processes. QuantMCP provides a robust, extensible,
and secure bridge between conversational AI and the complex world of financial
data, aiming to enhance both the reliability and the analytical depth of LLM
applications in finance.

</details>


### [653] [Deep Learning Enhanced Multi-Day Turnover Quantitative Trading Algorithm for Chinese A-Share Market](https://arxiv.org/abs/2506.06356)
*Yimin Du*

Main category: cs.CE

TL;DR: 本文提出一种结合深度学习与横截面预测的多日换手量化交易算法，通过五大模块实现资金效率与风险控制的平衡，在A股市场回测中展现出15.2%年化收益、5%内最大回撤及1.87夏普比率，具备机构级资金容量。


<details>
  <summary>Details</summary>
Motivation: 针对中国A股市场开发高容量量化策略，通过深度学习与动态风控机制解决传统策略中资金效率与风险管理难以平衡的问题，适应不同市场周期下的机构级部署需求。

Method: 五模块框架：1)深度交叉预测网络选股 2)混合模型开盘信号识别 3)市值/流动性动态头寸 4)网格搜索优化止盈止损 5)多粒度波动择时模型，采用自适应持仓周期与智能进出场时序控制。

Result: 2010-2020训练集与2021-2024回测显示：年化收益15.2%、最大回撤<5%、夏普比率1.87，日均持仓50-100只且最长持有9天，动态止盈止损机制使资金周转效率提升30%。

Conclusion: 该算法通过模块化设计实现风险收益的优化平衡，在保持高资金容量的同时，验证了深度学习与经典量化框架融合的有效性，为机构投资者提供了可扩展的A股市场解决方案。

Abstract: This paper presents a sophisticated multi-day turnover quantitative trading
algorithm that integrates advanced deep learning techniques with comprehensive
cross-sectional stock prediction for the Chinese A-share market. Our framework
combines five interconnected modules: initial stock selection through deep
cross-sectional prediction networks, opening signal distribution analysis using
mixture models for arbitrage identification, market capitalization and
liquidity-based dynamic position sizing, grid-search optimized profit-taking
and stop-loss mechanisms, and multi-granularity volatility-based market timing
models. The algorithm employs a novel approach to balance capital efficiency
with risk management through adaptive holding periods and sophisticated
entry/exit timing. Trained on comprehensive A-share data from 2010-2020 and
rigorously backtested on 2021-2024 data, our method achieves remarkable
performance with 15.2\% annualized returns, maximum drawdown constrained below
5\%, and a Sharpe ratio of 1.87. The strategy demonstrates exceptional
scalability by maintaining 50-100 daily positions with a 9-day maximum holding
period, incorporating dynamic profit-taking and stop-loss mechanisms that
enhance capital turnover efficiency while preserving risk-adjusted returns. Our
approach exhibits robust performance across various market regimes while
maintaining high capital capacity suitable for institutional deployment.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [654] [Design and Implementation of a RISC-V SoC with Custom DSP Accelerators for Edge Computing](https://arxiv.org/abs/2506.06693)
*Priyanshu Yadav*

Main category: cs.AR

TL;DR: 本文对RISC-V指令集架构进行了全面分析，重点关注其模块化设计、实现挑战和性能特征，展示了其在嵌入式系统中的优势及定制加速器的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 研究RISC-V指令集架构的模块化设计及其在嵌入式系统和定制加速器中的应用潜力，探索其性能优化和功耗降低的可能性。

Method: 通过周期精确的流水线实现模拟，评估了RV32I基础指令集及其扩展（乘法和原子操作）的性能指标，如CPI和能效。

Result: 结果表明，RISC-V在类似工艺节点下比ARM Cortex-M0实现功耗降低17%，并展示了其在嵌入式系统中的优势及可扩展性。

Conclusion: RISC-V的开源标准特性为领域特定优化提供了显著灵活性，适合嵌入式系统和定制加速器应用。

Abstract: This paper presents a comprehensive analysis of the RISC-V instruction set
architecture, focusing on its modular design, implementation challenges, and
performance characteristics. We examine the RV32I base instruction set with
extensions for multiplication (M) and atomic operations (A). Through
cycle-accurate simulation of a pipelined implementation, we evaluate
performance metrics including CPI (cycles per instruction) and power
efficiency. Our results demonstrate RISC-V's advantages in embedded systems and
its scalability for custom accelerators. Comparative analysis shows a 17%
reduction in power consumption compared to ARM Cortex-M0 implementations in
similar process nodes. The open-standard nature of RISC-V provides significant
flexibility for domain-specific optimizations.

</details>


### [655] [ProtocolLLM: RTL Benchmark for SystemVerilog Generation of Communication Protocols](https://arxiv.org/abs/2506.07945)
*Arnav Sheth,Ivaxi Sheth,Mario Fritz*

Main category: cs.AR

TL;DR: 本文分析了当前大语言模型在生成SystemVerilog硬件描述语言代码方面的能力，特别是针对标准通信协议的实现，并引入了一个新的基准测试套件。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在通用编程语言代码生成方面表现出色，但在硬件描述语言（如SystemVerilog）中的应用，尤其是生成可综合且功能正确的设计，仍未被充分探索。

Method: 本文引入了一个针对SPI、I2C、UART和AXI四种广泛使用的通信协议的基准测试套件，定义了不同设计抽象层次和提示特异性的代码生成任务，并通过波形仿真和测试台评估生成设计的语法正确性、可综合性和功能保真度。

Result: 生成的SystemVerilog设计在语法正确性、可综合性和功能保真度方面进行了评估，具体结果未在摘要中详细说明。

Conclusion: 本文为评估大语言模型在硬件描述语言代码生成方面的能力提供了新的基准测试套件，并展示了其在生成标准通信协议实现方面的潜力。

Abstract: Recent advances in Large Language Models (LLMs) have shown promising
capabilities in generating code for general-purpose programming languages. In
contrast, their applicability for hardware description languages, particularly
for generating synthesizable and functionally correct designs, remains
significantly underexplored. HDLs such as SystemVerilog are logic-oriented and
demand strict adherence to timing semantics, concurrency, and synthesizability
constraints. Moreover, HDL-based design flows encompass a broad set of tasks
beyond structural code generation, including testbench development,
assertion-based verification, timing closure, and protocol-level integration
for on-chip communication. The objective of our paper is to analyze the
capabilities of state-of-the-art LLMs in generating SystemVerilog
implementations of standard communication protocols, a core component of
embedded and System-on-Chip (SoC) architectures. This paper introduces the
first benchmark suite targeting four widely used protocols: SPI, I2C, UART, and
AXI. We define code generation tasks that capture varying levels of design
abstraction and prompt specificity. The generated designs are assessed for
syntactic correctness, synthesizability, and functional fidelity via waveform
simulation and test benches.

</details>


### [656] [MAGNet: A Multi-Scale Attention-Guided Graph Fusion Network for DRC Violation Detection](https://arxiv.org/abs/2506.07126)
*Weihan Lu,Hong Cai Chen*

Main category: cs.AR

TL;DR: 本文提出了一种名为MAGNet的混合深度学习模型，结合改进的U-Net和图神经网络，用于集成电路设计中的设计规则检查（DRC）违规预测，显著提高了预测精度并降低了误报率。


<details>
  <summary>Details</summary>
Motivation: 设计规则检查（DRC）在集成电路设计中对于降低成本和提高设计效率至关重要。基于机器学习的DRC已成为计算机辅助设计（CAD）中的重要方法。

Method: MAGNet模型结合了改进的U-Net和图神经网络。U-Net通过动态注意力模块（DAM）和多尺度卷积模块（MSCM）增强，以提取细粒度和多尺度的空间特征。同时，基于芯片布局构建像素对齐的图结构，并应用专门的图神经网络（GNN）建模引脚间的拓扑关系。训练过程中采用标签放大策略，增强模型对稀疏违规模式的敏感性。

Result: MAGNet在DRC热点检测中显著优于ibUnet、RouteNet和J-Net，整体性能大幅提升，预测精度提高，误报率降低。

Conclusion: MAGNet通过结合空间、语义和结构信息，有效提升了DRC违规预测的准确性，并通过增量训练进一步增强了热点识别的敏感性。

Abstract: Design rule checking (DRC) is of great significance for cost reduction and
design efficiency improvement in integrated circuit (IC) designs.
Machine-learning-based DRC has become an important approach in computer-aided
design (CAD). In this paper, we propose MAGNet, a hybrid deep learning model
that integrates an improved U-Net with a graph neural network for DRC violation
prediction. The U-Net backbone is enhanced with a Dynamic Attention Module
(DAM) and a Multi-Scale Convolution Module (MSCM) to strengthen its capability
in extracting fine-grained and multi-scale spatial features. In parallel, we
construct a pixel-aligned graph structure based on chip layout tiles, and apply
a specialized GNN to model the topological relationships among pins. During
graph construction, a graph-to-grid mapping is generated to align GNN features
with the layout image. In addition, a label amplification strategy is adopted
during training to enhance the model's sensitivity to sparse violation
patterns. Overall, MAGNet effectively combines spatial, semantic, and
structural information, achieving improved prediction accuracy and reduced
false positive rates in DRC hotspot detection. Subsequently, through
incremental training, we achieve a more sensitive discrimination ability for
hotspots. The results demonstrate that, in comparison with ibUnet, RouteNet,
and J-Net, MAGnet significantly outperforms these models, achieving substantial
improvements in overall performance.

</details>


### [657] [VeriLoC: Line-of-Code Level Prediction of Hardware Design Quality from Verilog Code](https://arxiv.org/abs/2506.07239)
*Raghu Vamshi Hemadri,Jitendra Bhandari,Johann Knechtel,Badri P Gopalan,Ramesh Narayanaswamy,Ramesh Karri,Siddharth Garg*

Main category: cs.AR

TL;DR: VeriLoC提出了一种直接从Verilog代码预测芯片设计质量的方法，在行级和模块级均表现优异，利用LLM提取嵌入特征并训练模型，显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅能预测模块级设计质量，无法定位具体导致时序违规或布线拥堵的代码行，而早期行级预测对复杂芯片设计优化至关重要。

Method: 结合Verilog代码生成LLM提取行级与模块级嵌入特征，通过拼接两类特征训练分类器/回归器实现多粒度预测。

Result: 行级拥堵和时序预测F1-score达0.86-0.95，平均绝对百分比误差从SOTA的14%-18%降至4%。

Conclusion: VeriLoC为硬件设计预测任务提供了有效工具，其嵌入特征对其他设计优化任务亦具潜在价值。

Abstract: Modern chip design is complex, and there is a crucial need for early-stage
prediction of key design-quality metrics like timing and routing congestion
directly from Verilog code (a commonly used programming language for hardware
design). It is especially important yet complex to predict individual lines of
code that cause timing violations or downstream routing congestion. Prior works
have tried approaches like converting Verilog into an intermediate graph
representation and using LLM embeddings alongside other features to predict
module-level quality, but did not consider line-level quality prediction. We
propose VeriLoC, the first method that predicts design quality directly from
Verilog at both the line- and module-level. To this end, VeriLoC leverages
recent Verilog code-generation LLMs to extract local line-level and
module-level embeddings, and train downstream classifiers/regressors on
concatenations of these embeddings. VeriLoC achieves high F1-scores of
0.86-0.95 for line-level congestion and timing prediction, and reduces the mean
average percentage error from 14% - 18% for SOTA methods down to only 4%. We
believe that VeriLoC embeddings and insights from our work will also be of
value for other predictive and optimization tasks for complex hardware design.

</details>


### [658] [Taming Wild Branches: Overcoming Hard-to-Predict Branches using the Bullseye Predictor](https://arxiv.org/abs/2506.06773)
*Emet Behrendt,Shing Wai Pun,Prashant J. Nair*

Main category: cs.AR

TL;DR: 本文提出了一种名为Bullseye的预测器，用于增强TAGE-SC-L分支预测器，通过针对难以预测的分支进行优化，显著降低了误预测率。


<details>
  <summary>Details</summary>
Motivation: 尽管TAGE-SC-L在CBP-2016中获胜，但其仍有超过一半的误预测来自难以预测的分支。这些分支在多样化的全局历史下发生，导致TAGE表频繁抖动，且计数器无法成熟。

Method: 在159 KB的TAGE-SC-L预测器基础上，增加了一个28 KB的Bullseye子系统。该系统通过H2P识别表（HIT）识别问题分支，并将其引导至两个分支特定的感知器中。感知器通过局部历史和全局历史进行索引，并在H2P缓存中进行短期试验。

Result: Bullseye预测器显著降低了误预测率，平均MPKI为3.4045，CycWpPKI为145.09。

Conclusion: Bullseye预测器通过针对难以预测的分支进行优化，有效提升了TAGE-SC-L的性能，减少了误预测率。

Abstract: Branch prediction is key to the performance of out-of-order processors. While
the CBP-2016 winner TAGE-SC-L combines geometric-history tables, a statistical
corrector, and a loop predictor, over half of its remaining mispredictions stem
from a small set of hard-to-predict (H2P) branches. These branches occur under
diverse global histories, causing repeated thrashing in TAGE and eviction
before usefulness counters can mature. Prior work shows that simply enlarging
the tables offers only marginal improvement.
  We augment a 159 KB TAGE-SC-L predictor with a 28 KB H2P-targeted subsystem
called the Bullseye predictor. It identifies problematic PCs using a
set-associative H2P Identification Table (HIT) and steers them to one of two
branch-specific perceptrons, one indexed by hashed local history and the other
by folded global history. A short trial phase tracks head-to-head accuracy in
an H2P cache. A branch becomes perceptron-resident only if the perceptron's
sustained accuracy and output magnitude exceed dynamic thresholds, after which
TAGE updates for that PC are suppressed to reduce pollution. The HIT, cache,
and perceptron operate fully in parallel with TAGE-SC-L, providing higher
fidelity on the H2P tail. This achieves an average MPKI of 3.4045 and CycWpPKI
of 145.09.

</details>


### [659] [ASPO: Constraint-Aware Bayesian Optimization for FPGA-based Soft Processors](https://arxiv.org/abs/2506.06817)
*Haoran Wu,Ce Guo,Wayne Luk,Robert Mullins*

Main category: cs.AR

TL;DR: ASPO提出一种改进贝叶斯优化的方法，通过定制协方差核支持类别参数约束，并利用合成检查点复用加速FPGA软处理器设计，在BOOM处理器上实现35%执行时间缩减和74%设计时间优化。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化(BO)无法处理含类别参数（如分支预测器类型）的约束，且优化时间随处理器复杂度显著增加，限制了其在FPGA软处理器设计中的应用。

Method: 提出ASPO方法：1) 设计支持类别参数的贝叶斯优化协方差核 2) 通过惩罚获取函数和复用FPGA合成检查点加速评估 3) 针对RocketChip/BOOM/EL2 VeeR三种处理器实现。

Result: 在7个RISC-V基准测试中，BOOM处理器的乘法基准测试执行时间减少35%，设计时间较Boomerang方法缩短74%。

Conclusion: ASPO通过数学机制定制有效解决了贝叶斯优化在软处理器设计中的局限性，显著提升设计效率与性能，适用于多种现代处理器架构。

Abstract: Bayesian Optimization (BO) has shown promise in tuning processor design
parameters. However, standard BO does not support constraints involving
categorical parameters such as types of branch predictors and division
circuits. In addition, optimization time of BO grows with processor complexity,
which becomes increasingly significant especially for FPGA-based soft
processors. This paper introduces ASPO, an approach that leverages disjunctive
form to enable BO to handle constraints involving categorical parameters.
Unlike existing methods that directly apply standard BO, the proposed ASPO
method, for the first time, customizes the mathematical mechanism of BO to
address challenges faced by soft-processor designs on FPGAs. Specifically, ASPO
supports categorical parameters using a novel customized BO covariance kernel.
It also accelerates the design evaluation procedure by penalizing the BO
acquisition function with potential evaluation time and by reusing FPGA
synthesis checkpoints from previously evaluated configurations. ASPO targets
three soft processors: RocketChip, BOOM, and EL2 VeeR. The approach is
evaluated based on seven RISC-V benchmarks. Results show that ASPO can reduce
execution time for the ``multiply'' benchmark on the BOOM processor by up to
35\% compared to the default configuration. Furthermore, it reduces design time
for the BOOM processor by up to 74\% compared to Boomerang, a state-of-the-art
hardware-oriented BO approach.

</details>
