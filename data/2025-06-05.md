<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 110]
- [cs.AI](#cs.AI) [Total: 22]
- [cs.LG](#cs.LG) [Total: 96]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.HC](#cs.HC) [Total: 3]
- [physics.acc-ph](#physics.acc-ph) [Total: 1]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.SE](#cs.SE) [Total: 4]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.RO](#cs.RO) [Total: 11]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.MA](#cs.MA) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [cs.CV](#cs.CV) [Total: 43]
- [math.OC](#math.OC) [Total: 2]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [quant-ph](#quant-ph) [Total: 4]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.IR](#cs.IR) [Total: 2]
- [q-fin.ST](#q-fin.ST) [Total: 2]
- [cs.DL](#cs.DL) [Total: 2]
- [q-bio.OT](#q-bio.OT) [Total: 1]
- [stat.ML](#stat.ML) [Total: 8]
- [cs.CR](#cs.CR) [Total: 3]
- [eess.SY](#eess.SY) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 3]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [eess.IV](#eess.IV) [Total: 16]
- [cond-mat.supr-con](#cond-mat.supr-con) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Evaluating Large Language Models for Zero-Shot Disease Labeling in CT Radiology Reports Across Organ Systems](https://arxiv.org/abs/2506.03259)
*Michael E. Garcia-Alcoser,Mobina GhojoghNejad,Fakrul Islam Tushar,David Kim,Kyle J. Lafata,Geoffrey D. Rubin,Joseph Y. Lo*

Main category: cs.CL

TL;DR: 轻量级LLM在CT报告疾病标注任务中优于基于规则的方法，并能通过零样本提示跨器官系统泛化，但二元标签无法完全捕捉报告语言的细微差别。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型（LLMs）在自动化CT放射学报告疾病标注中的有效性，比较规则算法与轻量级开源LLMs的性能。

Method: 回顾性研究分析40,833份CT报告，使用零样本提示测试3种开源LLM，通过Cohen's Kappa和F1分数评估性能。

Result: Llama-3.1 8B和Gemma-3 27B表现最佳（κ中位数0.87），在手动标注集上Gemma-3 27B宏F1最高（0.82）。CT-RATE数据集中Llama-3.1 8B最优（0.91）。

Conclusion: 轻量级LLM提供灵活高效的标注方案，但需注意二元标签的局限性。模型性能差异主要源于标注实践差异（如肺不张）。

Abstract: Purpose: This study aims to evaluate the effectiveness of large language
models (LLMs) in automating disease annotation of CT radiology reports. We
compare a rule-based algorithm (RBA), RadBERT, and three lightweight
open-weight LLMs for multi-disease labeling of chest, abdomen, and pelvis (CAP)
CT reports.
  Materials and Methods: This retrospective study analyzed 40,833 CT reports
from 29,540 patients, with 1,789 CAP reports manually annotated across three
organ systems. External validation was conducted using the CT-RATE dataset.
Three open-weight LLMs were tested with zero-shot prompting. Performance was
evaluated using Cohen's Kappa and micro/macro-averaged F1 scores.
  Results: In 12,197 Duke CAP reports from 8,854 patients, Llama-3.1 8B and
Gemma-3 27B showed the highest agreement ($\kappa$ median: 0.87). On the
manually annotated set, Gemma-3 27B achieved the top macro-F1 (0.82), followed
by Llama-3.1 8B (0.79), while the RBA scored lowest (0.64). On the CT-RATE
dataset (lungs/pleura only), Llama-3.1 8B performed best (0.91), with Gemma-3
27B close behind (0.89). Performance differences were mainly due to differing
labeling practices, especially for lung atelectasis.
  Conclusion: Lightweight LLMs outperform rule-based methods for CT report
annotation and generalize across organ systems with zero-shot prompting.
However, binary labels alone cannot capture the full nuance of report language.
LLMs can provide a flexible, efficient solution aligned with clinical judgment
and user needs.

</details>


### [2] [A conclusive remark on linguistic theorizing and language modeling](https://arxiv.org/abs/2506.03268)
*Cristiano Chesi*

Main category: cs.CL

TL;DR: 对目标论文收到的回复进行总结。


<details>
  <summary>Details</summary>
Motivation: 总结和回应《意大利语言学杂志》上对目标论文的评论。

Method: 对收到的回复进行分析和归纳。

Result: 提供了对评论的全面回应和总结。

Conclusion: 通过总结回复，进一步澄清和巩固了目标论文的观点。

Abstract: This is the final remark on the replies received to my target paper in the
Italian Journal of Linguistics

</details>


### [3] [FailureSensorIQ: A Multi-Choice QA Dataset for Understanding Sensor Relationships and Failure Modes](https://arxiv.org/abs/2506.03278)
*Christodoulos Constantinides,Dhaval Patel,Shuxin Lin,Claudio Guerrero,Sunil Dagajirao Patil,Jayant Kalagnanam*

Main category: cs.CL

TL;DR: FailureSensorIQ是一个新型多选问答基准系统，用于评估大语言模型在工业4.0复杂场景中的推理能力，揭示了现有模型的局限性并提供了实用工具。


<details>
  <summary>Details</summary>
Motivation: 传统问答基准无法全面评估大语言模型在工业领域特定场景下的推理能力，需要开发更专业的评估系统。

Method: 通过多维度分析（扰动-不确定性-复杂性）、专家评估、知识差距分析等方法，评估了包括GPT-4在内的多种大语言模型。

Result: 尽管闭源模型表现接近专家水平，但整体性能易受干扰且存在知识缺口。系统提供了工业资产MCQA数据集和特征选择工具。

Conclusion: FailureSensorIQ为工业领域的大语言模型评估提供了新范式，揭示了当前模型的不足并提供了实用解决方案。

Abstract: We introduce FailureSensorIQ, a novel Multi-Choice Question-Answering (MCQA)
benchmarking system designed to assess the ability of Large Language Models
(LLMs) to reason and understand complex, domain-specific scenarios in Industry
4.0. Unlike traditional QA benchmarks, our system focuses on multiple aspects
of reasoning through failure modes, sensor data, and the relationships between
them across various industrial assets. Through this work, we envision a
paradigm shift where modeling decisions are not only data-driven using
statistical tools like correlation analysis and significance tests, but also
domain-driven by specialized LLMs which can reason about the key contributors
and useful patterns that can be captured with feature engineering. We evaluate
the Industrial knowledge of over a dozen LLMs-including GPT-4, Llama, and
Mistral-on FailureSensorIQ from different lens using
Perturbation-Uncertainty-Complexity analysis, Expert Evaluation study,
Asset-Specific Knowledge Gap analysis, ReAct agent using external
knowledge-bases. Even though closed-source models with strong reasoning
capabilities approach expert-level performance, the comprehensive benchmark
reveals a significant drop in performance that is fragile to perturbations,
distractions, and inherent knowledge gaps in the models. We also provide a
real-world case study of how LLMs can drive the modeling decisions on 3
different failure prediction datasets related to various assets. We release:
(a) expert-curated MCQA for various industrial assets, (b) FailureSensorIQ
benchmark and Hugging Face leaderboard based on MCQA built from non-textual
data found in ISO documents, and (c) LLMFeatureSelector, an LLM-based feature
selection scikit-learn pipeline. The software is available at
https://github.com/IBM/FailureSensorIQ.

</details>


### [4] [HyperSteer: Activation Steering at Scale with Hypernetworks](https://arxiv.org/abs/2506.03292)
*Jiuding Sun,Sidharth Baskaran,Zhengxuan Wu,Michael Sklar,Christopher Potts,Atticus Geiger*

Main category: cs.CL

TL;DR: HyperSteer是一种基于超网络的架构，通过自然语言提示和语言模型内部状态生成控制向量，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有无监督方法生成的控制向量缺乏个体有效性和任务覆盖的保证，而有监督方法需要大量数据和训练。HyperSteer旨在结合两者的优势。

Method: 引入HyperSteer家族的超网络架构，端到端训练生成基于自然语言提示和语言模型内部状态的控制向量。

Result: HyperSteer在数千个控制提示下性能优于现有方法，甚至在未见过的提示上也表现良好，与通过提示控制的效果相当。

Conclusion: HyperSteer提供了一种高效且可扩展的方法来控制语言模型的生成行为，性能优于现有技术。

Abstract: Steering language models (LMs) by modifying internal activations is a popular
approach for controlling text generation. Unsupervised dictionary learning
methods, e.g., sparse autoencoders, can be scaled to produce many steering
vectors, but lack guarantees on the individual efficacy of each vector and
control over the coverage of relevant steering tasks. In contrast, supervised
methods for constructing steering vectors are targeted and effective, but
require more data collection and training for each additional steering vector
produced. In this work, we introduce HyperSteer, a family of hypernetwork-based
architectures which are trained end-to-end to generate steering vectors
conditioned on the natural language steering prompts and the internals of the
steered LM. In our evaluations, we show that scaling HyperSteer with thousands
of steering prompts exceeds the performance of state-of-the-art activation
steering methods, even on steering prompts never seen during training.
Moreover, HyperSteer performs on par with steering-via-prompting.

</details>


### [5] [Unleashing the Reasoning Potential of Pre-trained LLMs by Critique Fine-Tuning on One Problem](https://arxiv.org/abs/2506.03295)
*Yubo Wang,Ping Nie,Kai Zou,Lijun Wu,Wenhu Chen*

Main category: cs.CL

TL;DR: 该论文提出了一种名为Critique Fine-Tuning (CFT)的高效方法，仅需针对单个问题进行微调，即可显著提升大型语言模型(LLM)的推理能力，且计算成本远低于强化学习(RL)。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习(RL)可以显著提升大型语言模型(LLM)的推理能力，但其计算成本高昂且训练不稳定。论文旨在探索一种更高效的方法来释放基础LLM的推理潜力。

Method: 通过收集模型对单个问题生成的多样化解决方案，并利用教师LLM提供详细批评，构建批评微调(CFT)数据，然后对模型进行微调。

Result: 实验表明，仅需5个GPU小时的CFT训练，Qwen-Math-7B模型在数学和逻辑推理任务上的平均性能提升了15-16%，效果与RL相当但计算成本降低20倍。

Conclusion: 单次CFT是一种简单、通用且计算高效的方法，能够有效释放现代LLM的推理能力。

Abstract: We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess
immense reasoning potential inherited from the pre-training stage. With
reinforcement learning (RL), these models can improve dramatically on reasoning
tasks. Recent studies have shown that even RL on a single problem can unleash
these models' reasoning capabilities. However, RL is not only expensive but
also unstable. Even one-shot RL requires hundreds of GPU hours. This raises a
critical question: Is there a more efficient way to unleash the reasoning
potential of these powerful base LLMs? In this work, we demonstrate that
Critique Fine-Tuning (CFT) on only one problem can effectively unleash the
reasoning potential of LLMs. Our method constructs critique data by collecting
diverse model-generated solutions to a single problem and using teacher LLMs to
provide detailed critiques. We fine-tune Qwen and Llama family models, ranging
from 1.5B to 14B parameters, on the CFT data and observe significant
performance gains across diverse reasoning tasks. For example, with just 5 GPU
hours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six
math benchmarks and 16% on three logic reasoning benchmarks. These results are
comparable to or even surpass the results from RL with 20x less compute.
Ablation studies reveal the robustness of one-shot CFT across different prompt
problems. These results highlight one-shot CFT as a simple, general, and
compute-efficient approach to unleashing the reasoning capabilities of modern
LLMs.

</details>


### [6] [From Instructions to ODRL Usage Policies: An Ontology Guided Approach](https://arxiv.org/abs/2506.03301)
*Daham M. Mustafa,Abhishek Nadgeri,Diego Collarana,Benedikt T. Arnold,Christoph Quix,Christoph Lange,Stefan Decker*

Main category: cs.CL

TL;DR: 使用GPT-4等大语言模型自动从自然语言指令生成ODRL使用策略，通过优化本体文档指导生成过程，在文化领域数据空间评估中达到91.95%准确率。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过大语言模型自动化生成ODRL策略，解决传统手动编写策略的低效问题，并验证优化后的本体文档是否能提升生成质量。

Method: 以ODRL本体及其文档为核心提示内容，设计启发式方法适配本体文档，构建端到端知识图谱生成流程，并在文化数据空间的12个用例中测试。

Result: 在复杂度不同的测试用例中，知识图谱生成准确率最高达91.95%，证明方法有效性。

Conclusion: 经优化的本体文档能有效引导大语言模型生成高质量ODRL策略，为自动化策略生成提供了可行方案。

Abstract: This study presents an approach that uses large language models such as GPT-4
to generate usage policies in the W3C Open Digital Rights Language ODRL
automatically from natural language instructions. Our approach uses the ODRL
ontology and its documentation as a central part of the prompt. Our research
hypothesis is that a curated version of existing ontology documentation will
better guide policy generation. We present various heuristics for adapting the
ODRL ontology and its documentation to guide an end-to-end KG construction
process. We evaluate our approach in the context of dataspaces, i.e.,
distributed infrastructures for trustworthy data exchange between multiple
participating organizations for the cultural domain. We created a benchmark
consisting of 12 use cases of varying complexity. Our evaluation shows
excellent results with up to 91.95% accuracy in the resulting knowledge graph.

</details>


### [7] [Hopscotch: Discovering and Skipping Redundancies in Language Models](https://arxiv.org/abs/2506.03303)
*Mustafa Eyceoz,Nikhil Shivakumar Nayak,Hao Wang,Ligong Han,Akash Srivastava*

Main category: cs.CL

TL;DR: Hopscotch方法通过跳过贡献最小的注意力块并调整剩余层输出，在保持性能的同时提升因果语言模型效率。


<details>
  <summary>Details</summary>
Motivation: 现代因果语言模型堆叠大量注意力块以提升性能，但并非所有块对每个任务都必要。论文旨在减少计算冗余，同时保持输出质量。

Method: 引入轻量级可训练缩放参数，联合优化跳过哪些注意力块及如何缩放剩余层输出，无需修改模型权重或访问训练数据。

Result: 在Llama-3.1-8B和Qwen2.5-7B上，跳过4个注意力块后性能下降不足2%。

Conclusion: Hopscotch是一种高效且兼容现有压缩技术的方法，能显著降低计算成本而不显著影响模型性能。

Abstract: Modern causal language models stack many attention blocks to improve
performance, but not all blocks are necessary for every task. We propose
Hopscotch, a simple yet effective method that identifies and skips attention
blocks with least contributions to a task and adapts to preserve output
quality. Hopscotch jointly optimizes which blocks to skip and how to scale the
outputs of the remaining layers. By introducing lightweight, trainable scaling
parameters to attention and MLP blocks, it mitigates distribution shifts in
hidden states caused by removing attention blocks. Hopscotch does not modify
model weights or require access to pretraining or instruction-tuning data, and
is compatible with existing model compression techniques. When applied to
$\texttt{Llama-3.1-8B}$ and $\texttt{Qwen2.5-7B}$, Hopscotch achieves less than
a 2% drop in performance even after skipping four attention blocks.

</details>


### [8] [The Reader is the Metric: How Textual Features and Reader Profiles Explain Conflicting Evaluations of AI Creative Writing](https://arxiv.org/abs/2506.03310)
*Guillermo Marco,Julio Gonzalo,Víctor Fresno*

Main category: cs.CL

TL;DR: 研究发现AI与人类文学文本评价差异源于读者偏好不同，提出基于读者敏感性的评估框架。


<details>
  <summary>Details</summary>
Motivation: 针对AI生成与人类创作文学文本质量评价的矛盾结果，探究其背后读者解读与价值观差异的影响。

Method: 使用5个公开数据集（1471篇故事，101名标注者），提取17个无参考文本特征，建模个体读者偏好并构建共享偏好空间。

Result: 读者偏好分为两类：注重可读性的'表层读者'（非专家为主）和关注主题发展的'整体读者'（专家为主）。

Conclusion: 文学质量评估应结合读者偏好特征，建议在创意文本生成领域采用读者敏感性评价框架。

Abstract: Recent studies comparing AI-generated and human-authored literary texts have
produced conflicting results: some suggest AI already surpasses human quality,
while others argue it still falls short. We start from the hypothesis that such
divergences can be largely explained by genuine differences in how readers
interpret and value literature, rather than by an intrinsic quality of the
texts evaluated. Using five public datasets (1,471 stories, 101 annotators
including critics, students, and lay readers), we (i) extract 17 reference-less
textual features (e.g., coherence, emotional variance, average sentence
length...); (ii) model individual reader preferences, deriving feature
importance vectors that reflect their textual priorities; and (iii) analyze
these vectors in a shared "preference space". Reader vectors cluster into two
profiles: 'surface-focused readers' (mainly non-experts), who prioritize
readability and textual richness; and 'holistic readers' (mainly experts), who
value thematic development, rhetorical variety, and sentiment dynamics. Our
results quantitatively explain how measurements of literary quality are a
function of how text features align with each reader's preferences. These
findings advocate for reader-sensitive evaluation frameworks in the field of
creative text generation.

</details>


### [9] [Cross-Platform Violence Detection on Social Media: A Dataset and Analysis](https://arxiv.org/abs/2506.03312)
*Celia Chen,Scotty Beland,Ingo Burghardt,Jill Byczek,William J. Conway,Eric Cotugno,Sadaf Davre,Megan Fletcher,Rajesh Kumar Gnanasekaran,Kristin Hamilton,Marilyn Harbert,Jordan Heustis,Tanaya Jha,Emily Klein,Hayden Kramer,Alex Leitch,Jessica Perkins,Casi Sherman,Celia Sterrn,Logan Stevens,Rebecca Zarrella,Jennifer Golbeck*

Main category: cs.CL

TL;DR: 论文介绍了一个跨平台的3万条暴力威胁帖子数据集，并通过机器学习分析验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的暴力威胁问题严重，高质量数据有助于研究和检测恶意内容。

Method: 构建了一个手标数据集，包含政治和性暴力等子类型，并与YouTube暴力评论数据集进行机器学习分析。

Result: 即使数据集来自不同平台且标注标准不同，分类准确率仍很高。

Conclusion: 结果对内容分类策略和跨平台暴力内容理解具有重要意义。

Abstract: Violent threats remain a significant problem across social media platforms.
Useful, high-quality data facilitates research into the understanding and
detection of malicious content, including violence. In this paper, we introduce
a cross-platform dataset of 30,000 posts hand-coded for violent threats and
sub-types of violence, including political and sexual violence. To evaluate the
signal present in this dataset, we perform a machine learning analysis with an
existing dataset of violent comments from YouTube. We find that, despite
originating from different platforms and using different coding criteria, we
achieve high classification accuracy both by training on one dataset and
testing on the other, and in a merged dataset condition. These results have
implications for content-classification strategies and for understanding
violent content across social media.

</details>


### [10] [Ask a Local: Detecting Hallucinations With Specialized Model Divergence](https://arxiv.org/abs/2506.03357)
*Aldan Creo,Héctor Cerezo-Costas,Pedro Alonso-Doval,Maximiliano Hormazábal-Lagos*

Main category: cs.CL

TL;DR: 论文提出了一种名为'Ask a Local'的新方法，用于检测多语言大模型中的幻觉现象，无需额外训练或语言适配，且在14种语言上表现稳定。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）常产生看似合理但事实错误的信息（幻觉），这对AI应用构成重大挑战。

Method: 通过计算领域专用语言模型的困惑度分布差异，识别潜在幻觉片段，无需依赖外部数据或额外训练。

Result: 在14种语言的问答数据集上，IoU分数约0.3，意大利语和加泰罗尼亚语表现最佳（IoU分别为0.42和0.38）。

Conclusion: 该方法高效、可扩展，适用于多语言场景，代码已开源以促进后续研究。

Abstract: Hallucinations in large language models (LLMs) - instances where models
generate plausible but factually incorrect information - present a significant
challenge for AI.
  We introduce "Ask a Local", a novel hallucination detection method exploiting
the intuition that specialized models exhibit greater surprise when
encountering domain-specific inaccuracies. Our approach computes divergence
between perplexity distributions of language-specialized models to identify
potentially hallucinated spans. Our method is particularly well-suited for a
multilingual context, as it naturally scales to multiple languages without the
need for adaptation, relying on external data sources, or performing training.
Moreover, we select computationally efficient models, providing a scalable
solution that can be applied to a wide range of languages and domains.
  Our results on a human-annotated question-answer dataset spanning 14
languages demonstrate consistent performance across languages, with
Intersection-over-Union (IoU) scores around 0.3 and comparable Spearman
correlation values. Our model shows particularly strong performance on Italian
and Catalan, with IoU scores of 0.42 and 0.38, respectively, while maintaining
cross-lingual effectiveness without language-specific adaptations. We release
our code and architecture to facilitate further research in multilingual
hallucination detection.

</details>


### [11] [A Multimodal, Multilingual, and Multidimensional Pipeline for Fine-grained Crowdsourcing Earthquake Damage Evaluation](https://arxiv.org/abs/2506.03360)
*Zihui Ma,Lingyao Li,Juan Li,Wenyue Hua,Jingxiao Liu,Qingyuan Feng,Yuki Miura*

Main category: cs.CL

TL;DR: 该研究提出了一种基于多模态大语言模型（MLLMs）的3M流程，用于快速评估灾害影响，结果显示MLLMs能有效整合图文信号并与地震数据强相关，但性能受语言、震中距离和输入模态影响。


<details>
  <summary>Details</summary>
Motivation: 传统灾害评估方法受限于地面传感器不足和官方报告延迟，而社交媒体提供了丰富、实时的观测数据，但其多模态和非结构化特性给传统分析方法带来挑战。

Method: 研究提出了一种结构化的多模态、多语言、多维（3M）流程，利用MLLMs评估灾害影响，并在两次重大地震事件中通过宏观和微观分析评估了三种基础模型。

Result: 结果表明，MLLMs能有效整合图文信号，并与真实地震数据强相关，但性能因语言、震中距离和输入模态而异。

Conclusion: 该研究展示了MLLMs在灾害评估中的潜力，为未来在实时危机情境中应用MLLMs奠定了基础。

Abstract: Rapid, fine-grained disaster damage assessment is essential for effective
emergency response, yet remains challenging due to limited ground sensors and
delays in official reporting. Social media provides a rich, real-time source of
human-centric observations, but its multimodal and unstructured nature presents
challenges for traditional analytical methods. In this study, we propose a
structured Multimodal, Multilingual, and Multidimensional (3M) pipeline that
leverages multimodal large language models (MLLMs) to assess disaster impacts.
We evaluate three foundation models across two major earthquake events using
both macro- and micro-level analyses. Results show that MLLMs effectively
integrate image-text signals and demonstrate a strong correlation with
ground-truth seismic data. However, performance varies with language,
epicentral distance, and input modality. This work highlights the potential of
MLLMs for disaster assessment and provides a foundation for future research in
applying MLLMs to real-time crisis contexts. The code and data are released at:
https://github.com/missa7481/EMNLP25_earthquake

</details>


### [12] [Trajectory Prediction Meets Large Language Models: A Survey](https://arxiv.org/abs/2506.03408)
*Yi Xu,Ruining Yang,Yitian Zhang,Yizhou Wang,Jianglin Lu,Mingyuan Zhang,Lili Su,Yun Fu*

Main category: cs.CL

TL;DR: 该综述探讨了如何利用大语言模型（LLMs）的语义和推理能力来改进轨迹预测，总结了五个研究方向及其挑战。


<details>
  <summary>Details</summary>
Motivation: 近年来，大语言模型（LLMs）的进展激发了将语言驱动技术整合到轨迹预测中的兴趣，旨在通过LLMs的语义和推理能力提升自主系统的轨迹感知与预测能力。

Method: 综述将现有研究分为五个方向：基于语言建模范式的轨迹预测、直接使用预训练语言模型进行轨迹预测、语言引导的场景理解、语言驱动的数据生成，以及基于语言的推理与可解释性。

Result: 通过分析代表性方法和核心设计选择，综述为自然语言处理与轨迹预测的交叉领域提供了统一视角，并指出了开放挑战。

Conclusion: 该综述展示了语言如何丰富轨迹预测，为未来研究提供了方向，并强调了跨学科整合的重要性。

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in integrating language-driven techniques into trajectory prediction. By
leveraging their semantic and reasoning capabilities, LLMs are reshaping how
autonomous systems perceive, model, and predict trajectories. This survey
provides a comprehensive overview of this emerging field, categorizing recent
work into five directions: (1) Trajectory prediction via language modeling
paradigms, (2) Direct trajectory prediction with pretrained language models,
(3) Language-guided scene understanding for trajectory prediction, (4)
Language-driven data generation for trajectory prediction, (5) Language-based
reasoning and interpretability for trajectory prediction. For each, we analyze
representative methods, highlight core design choices, and identify open
challenges. This survey bridges natural language processing and trajectory
prediction, offering a unified perspective on how language can enrich
trajectory prediction.

</details>


### [13] [DistRAG: Towards Distance-Based Spatial Reasoning in LLMs](https://arxiv.org/abs/2506.03424)
*Nicole R Schneider,Nandini Ramachandran,Kent O'Sullivan,Hanan Samet*

Main category: cs.CL

TL;DR: 论文提出DistRAG方法，通过构建地理距离图增强LLM的空间推理能力，解决其原生无法处理距离相关问题的缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型(LLM)缺乏可靠的空间推理能力，尤其在处理距离相关任务(如POI推荐、行程规划)时表现不足。

Method: 开发DistRAG方法：将城市间地理距离编码为图结构，检索与问题相关的上下文子图供LLM使用。

Result: 该方法使LLM能够回答原本无法处理的基于距离的推理问题，初步构建了补充语言知识的'世界模型'。

Conclusion: DistRAG为LLM提供了灵活的空间信息检索机制，是增强其空间推理能力的重要第一步。

Abstract: Many real world tasks where Large Language Models (LLMs) can be used require
spatial reasoning, like Point of Interest (POI) recommendation and itinerary
planning. However, on their own LLMs lack reliable spatial reasoning
capabilities, especially about distances. To address this problem, we develop a
novel approach, DistRAG, that enables an LLM to retrieve relevant spatial
information not explicitly learned during training. Our method encodes the
geodesic distances between cities and towns in a graph and retrieves a context
subgraph relevant to the question. Using this technique, our method enables an
LLM to answer distance-based reasoning questions that it otherwise cannot
answer. Given the vast array of possible places an LLM could be asked about,
DistRAG offers a flexible first step towards providing a rudimentary `world
model' to complement the linguistic knowledge held in LLMs.

</details>


### [14] [Time Course MechInterp: Analyzing the Evolution of Components and Knowledge in Large Language Models](https://arxiv.org/abs/2506.03434)
*Ahmad Dawar Hakimi,Ali Modarressi,Philipp Wicke,Hinrich Schütze*

Main category: cs.CL

TL;DR: 研究分析了OLMo-7B模型在预训练过程中事实知识表征的演变，发现模型从通用组件逐渐转向专用组件，注意力头变化最大，FFN更稳定，且不同关系类型的习得速度不同。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型如何获取和存储事实知识，以提高其可解释性和可靠性。

Method: 通过跟踪OLMo-7B模型的注意力头和前馈网络在预训练过程中的角色变化，将其分为通用、实体、关系-答案和事实-答案四类，并分析其稳定性和转变。

Result: 模型初始依赖通用组件，随后逐渐专业化；注意力头变化最大，FFN更稳定；基于位置的关系比基于名称的关系更早达到高准确率。

Conclusion: 研究揭示了LLM知识形成的机制，表明模型通过自适应学习过程优化组件用途，且任务复杂度影响知识获取动态。

Abstract: Understanding how large language models (LLMs) acquire and store factual
knowledge is crucial for enhancing their interpretability and reliability. In
this work, we analyze the evolution of factual knowledge representation in the
OLMo-7B model by tracking the roles of its attention heads and feed forward
networks (FFNs) over the course of pre-training. We classify these components
into four roles: general, entity, relation-answer, and fact-answer specific,
and examine their stability and transitions. Our results show that LLMs
initially depend on broad, general-purpose components, which later specialize
as training progresses. Once the model reliably predicts answers, some
components are repurposed, suggesting an adaptive learning process. Notably,
attention heads display the highest turnover. We also present evidence that
FFNs remain more stable throughout training. Furthermore, our probing
experiments reveal that location-based relations converge to high accuracy
earlier in training than name-based relations, highlighting how task complexity
shapes acquisition dynamics. These insights offer a mechanistic view of
knowledge formation in LLMs.

</details>


### [15] [Culture Matters in Toxic Language Detection in Persian](https://arxiv.org/abs/2506.03458)
*Zahra Bokaei,Walid Magdy,Bonnie Webber*

Main category: cs.CL

TL;DR: 该论文比较了波斯语有毒语言检测的不同方法，并探讨了文化背景对跨语言迁移学习的影响。


<details>
  <summary>Details</summary>
Motivation: 波斯语有毒语言检测研究不足，需探索有效方法以提升在线环境安全。

Method: 采用微调、数据增强、零样本/少样本学习及跨语言迁移学习等方法进行比较。

Result: 文化相似语言的迁移学习效果更佳，文化差异语言提升有限。

Conclusion: 文化背景显著影响跨语言迁移学习在有毒语言检测中的效果。

Abstract: Toxic language detection is crucial for creating safer online environments
and limiting the spread of harmful content. While toxic language detection has
been under-explored in Persian, the current work compares different methods for
this task, including fine-tuning, data enrichment, zero-shot and few-shot
learning, and cross-lingual transfer learning. What is especially compelling is
the impact of cultural context on transfer learning for this task: We show that
the language of a country with cultural similarities to Persian yields better
results in transfer learning. Conversely, the improvement is lower when the
language comes from a culturally distinct country. Warning: This paper contains
examples of toxic language that may disturb some readers. These examples are
included for the purpose of research on toxic detection.

</details>


### [16] [Delta-KNN: Improving Demonstration Selection in In-Context Learning for Alzheimer's Disease Detection](https://arxiv.org/abs/2506.03476)
*Chuyuan Li,Raymond Li,Thalia S. Field,Giuseppe Carenini*

Main category: cs.CL

TL;DR: 该论文提出Delta-KNN方法，通过改进上下文学习中的示例选择策略，显著提升了大型语言模型在阿尔茨海默病诊断中的性能。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病（AD）的早期诊断对干预治疗至关重要，而传统上下文学习方法在该任务上表现不佳，因此需要更有效的诊断策略。

Method: 提出Delta-KNN方法，结合delta评分和KNN检索器动态选择最优训练示例，用于改进上下文学习的诊断效果。

Result: 在三个开源大型语言模型和两个AD检测数据集上的实验表明，Delta-KNN优于现有基线方法，使用Llama-3.1时达到新SOTA。

Conclusion: Delta-KNN能有效提升AD诊断性能，证明了大型语言模型作为健康助手的潜力。

Abstract: Alzheimer's Disease (AD) is a progressive neurodegenerative disorder that
leads to dementia, and early intervention can greatly benefit from analyzing
linguistic abnormalities. In this work, we explore the potential of Large
Language Models (LLMs) as health assistants for AD diagnosis from
patient-generated text using in-context learning (ICL), where tasks are defined
through a few input-output examples. Empirical results reveal that conventional
ICL methods, such as similarity-based selection, perform poorly for AD
diagnosis, likely due to the inherent complexity of this task. To address this,
we introduce Delta-KNN, a novel demonstration selection strategy that enhances
ICL performance. Our method leverages a delta score to assess the relative
gains of each training example, coupled with a KNN-based retriever that
dynamically selects optimal "representatives" for a given input. Experiments on
two AD detection datasets across three open-source LLMs demonstrate that
Delta-KNN consistently outperforms existing ICL baselines. Notably, when using
the Llama-3.1 model, our approach achieves new state-of-the-art results,
surpassing even supervised classifiers.

</details>


### [17] [APT: Improving Specialist LLM Performance with Weakness Case Acquisition and Iterative Preference Training](https://arxiv.org/abs/2506.03483)
*Jun Rao,Zepeng Lin,Xuebo Liu,Xiaopeng Ke,Lian Lian,Dong Jin,Shengjun Cheng,Jun Yu,Min Zhang*

Main category: cs.CL

TL;DR: 论文提出APT方法，通过自生成弱项数据（错误案例及相似案例）进行针对性训练，在提升领域性能的同时保持大模型的通用能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在领域微调时可能损害其通用能力，如何在增强领域性能与保持通用性之间取得平衡是关键挑战。

Method: 提出APT方法：仅针对错误样本及少量相似样本进行迭代偏好训练，最小化对模型已有知识的干扰。

Result: 实验表明，APT在LLama-2和Mistral-V0.3上既保持通用能力，又在下游任务中优于现有方法。

Conclusion: APT是一种有效策略，可在不牺牲模型通用性的前提下提升领域特定能力。

Abstract: Large Language Models (LLMs) often require domain-specific fine-tuning to
address targeted tasks, which risks degrading their general capabilities.
Maintaining a balance between domain-specific enhancements and general model
utility is a key challenge. This paper proposes a novel approach named APT
(Weakness Case Acquisition and Iterative Preference Training) to enhance
domain-specific performance with self-generated dis-preferred weakness data
(bad cases and similar cases). APT uniquely focuses on training the model using
only those samples where errors occur, alongside a small, similar set of
samples retrieved for this purpose. This targeted training minimizes
interference with the model's existing knowledge base, effectively retaining
generic capabilities. Experimental results on the LLama-2 and Mistral-V0.3
models across various benchmarks demonstrate that APT ensures no reduction in
generic capacity and achieves superior performance on downstream tasks compared
to various existing methods. This validates our method as an effective strategy
for enhancing domain-specific capabilities without sacrificing the model's
broader applicability.

</details>


### [18] [Explainable AI: XAI-Guided Context-Aware Data Augmentation](https://arxiv.org/abs/2506.03484)
*Melkamu Abay Mersha,Mesay Gemeda Yigezu,Atnafu Lambebo Tonja,Hassan Shakil,Samer Iskander,Olga Kolesnikova,Jugal Kalita*

Main category: cs.CL

TL;DR: 该论文提出了一种基于可解释AI（XAI）的上下文感知数据增强框架XAI-Guided Context-Aware Data Augmentation，通过选择性保留关键特征并迭代优化增强数据，显著提升了低资源语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据增强技术在低资源语言场景中存在噪声引入、语义漂移、上下文不连贯等问题，导致模型过拟合且泛化能力不足。论文旨在利用XAI技术解决这些局限性。

Method: 提出XAI-SR-BT和XAI-PR-BT方法：1) 使用XAI识别任务关键特征并选择性保留；2) 通过迭代反馈循环基于可解释性洞察优化增强数据；3) 在Amharic数据集上结合XLM-R模型验证。

Result: 在仇恨言论和情感分析任务中，XAI-SR-BT/XAI-PR-BT比基线模型准确率分别提升6.6%/8.1%，比传统增强技术高4.8%/5%，且在所有任务和模型中表现一致更优。

Conclusion: 该研究为数据增强提供了可控、可解释且上下文感知的新范式，通过XAI技术有效提升了AI模型训练效果，尤其适用于低资源语言场景。

Abstract: Explainable AI (XAI) has emerged as a powerful tool for improving the
performance of AI models, going beyond providing model transparency and
interpretability. The scarcity of labeled data remains a fundamental challenge
in developing robust and generalizable AI models, particularly for low-resource
languages. Conventional data augmentation techniques introduce noise, cause
semantic drift, disrupt contextual coherence, lack control, and lead to
overfitting. To address these challenges, we propose XAI-Guided Context-Aware
Data Augmentation. This novel framework leverages XAI techniques to modify less
critical features while selectively preserving most task-relevant features. Our
approach integrates an iterative feedback loop, which refines augmented data
over multiple augmentation cycles based on explainability-driven insights and
the model performance gain. Our experimental results demonstrate that XAI-SR-BT
and XAI-PR-BT improve the accuracy of models on hate speech and sentiment
analysis tasks by 6.6% and 8.1%, respectively, compared to the baseline, using
the Amharic dataset with the XLM-R model. XAI-SR-BT and XAI-PR-BT outperform
existing augmentation techniques by 4.8% and 5%, respectively, on the same
dataset and model. Overall, XAI-SR-BT and XAI-PR-BT consistently outperform
both baseline and conventional augmentation techniques across all tasks and
models. This study provides a more controlled, interpretable, and context-aware
solution to data augmentation, addressing critical limitations of existing
augmentation techniques and offering a new paradigm shift for leveraging XAI
techniques to enhance AI model training.

</details>


### [19] [EpiCoDe: Boosting Model Performance Beyond Training with Extrapolation and Contrastive Decoding](https://arxiv.org/abs/2506.03489)
*Mingxu Tao,Jie Hu,Mingchuan Yang,Yunhuai Liu,Dongyan Zhao,Yansong Feng*

Main category: cs.CL

TL;DR: 论文提出EpiCoDe方法，通过模型外推和对比解码提升数据稀缺场景下的LLM性能，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 高质量标注数据的高成本限制了模型在下游任务中的能力提升，亟需数据稀缺场景下的高效解决方案。

Method: 1. 模型外推增强微调模型 2. 对比解码减少预测错误（通过比较外推模型与原始微调模型的logit分数）

Result: 在4种LLM的3项任务中，EpiCoDe显著稳定优于现有方法，并提出了解释对比解码机制的理论框架。

Conclusion: EpiCoDe为数据稀缺场景提供了有效解决方案，理论框架揭示了对比解码的工作机制。

Abstract: The remarkable performance of Large language models (LLMs) relies heavily on
the availability of abundant high-quality training data. However, the high cost
of acquiring annotated data often prevents models from obtaining capabilities
to tackle downstream tasks. In this paper, we introduce a novel method, EpiCoDe
that boosts model performance in data-scarcity scenarios without extra
training. We first employ model extrapolation to enhance a finetuned model with
its inferior version, and then adopt contrastive decoding to further reduce
predicted errors, by comparing the logit scores given by the extrapolated and
the vanilla finetuned model. Experiments across three tasks over four different
LLMs show that EpiCoDe consistently outperforms existing methods with
significant and robust improvement. We also propose a new theoretical framework
to reveal the mechanism behind contrastive decoding in data-scarcity scenarios,
which further helps us better understand the effectiveness of EpiCoDe.

</details>


### [20] [Beyond Memorization: A Rigorous Evaluation Framework for Medical Knowledge Editing](https://arxiv.org/abs/2506.03490)
*Shigeng Chen,Linhao Luo,Zhangchi Qiu,Yanan Cao,Carl Yang,Shirui Pan*

Main category: cs.CL

TL;DR: 本文提出MedEditBench框架评估知识编辑方法在医学领域的适用性，发现现有方法仅能浅层记忆信息，提出SGR-Edit方法通过模型自生成原理提升编辑效果。


<details>
  <summary>Details</summary>
Motivation: 知识编辑（KE）方法在通用领域表现良好，但在复杂医学领域的适用性尚未充分探索。医学知识编辑要求模型内化知识并泛化至新场景，现有方法难以满足这一需求。

Method: 提出MedEditBench框架，包含新的医学知识编辑基准和三种编辑范式，并设计SGR-Edit方法，利用模型自生成原理作为编辑目标知识。

Result: 现有KE方法仅实现信息的浅层记忆，无法泛化；SGR-Edit显著优于现有方法，并揭示了医学知识在LLMs中的定位及连续编辑对知识演化的影响。

Conclusion: SGR-Edit为医学知识编辑提供了有效解决方案，其发现为实际医学应用中的知识编辑提供了实用指导。

Abstract: Recently, knowledge editing (KE) has emerged as a promising approach to
update specific facts in Large Language Models (LLMs) without the need for full
retraining. Despite the effectiveness in general-domain benchmarks, their
applicability to complex medical domain remains largely unexplored. Medical
knowledge editing is particularly challenging, as it requires LLMs to
internalize the knowledge and generalize to unseen scenarios for effective and
interpretable decision-making. In this work, we propose a novel framework
called MedEditBench to rigorously evaluate the effectiveness of existing KE
methods in the medical domain. In MedEditBench, we introduce a new medical
knowledge editing benchmark as well as three different knowledge editing
paradigms, which are designed to assess the impact of different knowledge
sources for editing. Our findings indicate that current KE methods result in
only superficial memorization of the injected information, failing to
generalize to new scenarios. To overcome this limitation, we present
Self-Generated Rationale Editing (SGR-Edit), which utilizes model-derived
rationales as the target knowledge for editing, thereby uncovering the
underlying reasoning process and demonstrating significant improvements over
existing KE approaches. Additionally, we offer deeper insights into medical
knowledge editing, including the localization of medical knowledge in LLMs and
the impact of sequential editing on evolving knowledge. This could provide
practical guidance for implementing KE methods in real-world medical
applications.

</details>


### [21] [Measuring Human Involvement in AI-Generated Text: A Case Study on Academic Writing](https://arxiv.org/abs/2506.03501)
*Yuchen Guo,Zhicheng Dou,Huy H. Nguyen,Ching-Chun Chang,Saku Sugawara,Isao Echizen*

Main category: cs.CL

TL;DR: 论文提出了一种新方法来检测文本生成中的人类参与程度，解决了传统二元分类方法的不足，并在实验中取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的普及，AI生成文本的检测变得复杂，传统二元分类方法无法有效识别人类参与程度，因此需要更精细的检测方法。

Method: 使用BERTScore作为度量标准，并基于RoBERTa构建多任务回归模型，通过token分类任务来量化人类参与程度。

Result: 新方法在模拟学术场景的数据集上表现优异（F1分数0.9423，回归均方误差0.004），且具有一定跨模型泛化能力。

Conclusion: 该方法能有效解决参与检测混淆问题，为AI生成文本的细粒度检测提供了新思路。

Abstract: Content creation has dramatically progressed with the rapid advancement of
large language models like ChatGPT and Claude. While this progress has greatly
enhanced various aspects of life and work, it has also negatively affected
certain areas of society. A recent survey revealed that nearly 30% of college
students use generative AI to help write academic papers and reports. Most
countermeasures treat the detection of AI-generated text as a binary
classification task and thus lack robustness. This approach overlooks human
involvement in the generation of content even though human-machine
collaboration is becoming mainstream. Besides generating entire texts, people
may use machines to complete or revise texts. Such human involvement varies
case by case, which makes binary classification a less than satisfactory
approach. We refer to this situation as participation detection obfuscation. We
propose using BERTScore as a metric to measure human involvement in the
generation process and a multi-task RoBERTa-based regressor trained on a token
classification task to address this problem. To evaluate the effectiveness of
this approach, we simulated academic-based scenarios and created a continuous
dataset reflecting various levels of human involvement. All of the existing
detectors we examined failed to detect the level of human involvement on this
dataset. Our method, however, succeeded (F1 score of 0.9423 and a regressor
mean squared error of 0.004). Moreover, it demonstrated some generalizability
across generative models. Our code is available at
https://github.com/gyc-nii/CAS-CS-and-dual-head-detector

</details>


### [22] [Accurate Sublayer Pruning for Large Language Models by Exploiting Latency and Tunability Information](https://arxiv.org/abs/2506.03510)
*Seungcheol Park,Sojin Lee,Jongjin Kim,Jinsik Lee,Hyunjik Jo,U Kang*

Main category: cs.CL

TL;DR: SPRINT提出了一种新的子层剪枝方法，通过考虑延迟减少和子层可调性来加速大语言模型，同时保持高准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理速度慢，现有子层剪枝方法因忽视子层特性导致准确率下降。

Method: SPRINT结合延迟减少量和子层可调性信息，迭代剪枝冗余子层并快速微调剩余参数。

Result: SPRINT在零样本常识推理任务上准确率最高提升23.88%，实现了最佳速度-精度平衡。

Conclusion: SPRINT通过精细化子层剪枝策略，显著提升了大语言模型的推理效率与性能。

Abstract: How can we accelerate large language models(LLMs) without sacrificing
accuracy? The slow inference speed of LLMs hinders us to benefit from their
remarkable performance in diverse applications. This is mainly because numerous
sublayers are stacked together in LLMs. Sublayer pruning compresses and
expedites LLMs via removing unnecessary sublayers. However, existing sublayer
pruning algorithms are limited in accuracy since they naively select sublayers
to prune, overlooking the different characteristics of each sublayer. In this
paper, we propose SPRINT (Sublayer PRuning wIth LateNcy and Tunability
Information), an accurate sublayer pruning method for LLMs. SPRINT accurately
selects a target sublayer to prune by considering 1) the amount of latency
reduction after pruning and 2) the tunability of sublayers. SPRINT iteratively
prunes redundant sublayers and swiftly tunes the parameters of remaining
sublayers. Experiments show that SPRINT achieves the best accuracy-speedup
trade-off, exhibiting up to 23.88%p higher accuracy on zero-shot commonsense
reasoning benchmarks compared to existing pruning algorithms.

</details>


### [23] [An Efficient Task-Oriented Dialogue Policy: Evolutionary Reinforcement Learning Injected by Elite Individuals](https://arxiv.org/abs/2506.03519)
*Yangyang Zhao,Ben Niu,Libo Qin,Shihan Wang*

Main category: cs.CL

TL;DR: 该论文提出了一种结合进化算法（EA）和深度强化学习（DRL）的新方法，通过精英个体注入机制优化任务导向对话系统的策略，显著提升了探索与利用的平衡及性能。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在任务导向对话系统中优化对话策略时，由于状态和动作空间的高维度，难以平衡探索与利用，常导致局部最优或收敛不佳。进化算法通过保持种群多样性有效探索解空间，但直接结合两者因自然语言的灵活性导致进化时间延长。

Method: 创新性地将EA的全局搜索能力与DRL的局部优化相结合，并进一步提出精英个体注入机制，通过自适应引入表现最佳的个体来提升EA的搜索效率。

Result: 在四个数据集上的实验表明，该方法显著改善了探索与利用的平衡，提升了性能，同时精英个体注入机制有效减少了探索时间。

Conclusion: 该方法成功实现了EA与DRL在任务导向对话策略任务中的高效结合，为相关领域提供了新的解决方案。

Abstract: Deep Reinforcement Learning (DRL) is widely used in task-oriented dialogue
systems to optimize dialogue policy, but it struggles to balance exploration
and exploitation due to the high dimensionality of state and action spaces.
This challenge often results in local optima or poor convergence. Evolutionary
Algorithms (EAs) have been proven to effectively explore the solution space of
neural networks by maintaining population diversity. Inspired by this, we
innovatively combine the global search capabilities of EA with the local
optimization of DRL to achieve a balance between exploration and exploitation.
Nevertheless, the inherent flexibility of natural language in dialogue tasks
complicates this direct integration, leading to prolonged evolutionary times.
Thus, we further propose an elite individual injection mechanism to enhance
EA's search efficiency by adaptively introducing best-performing individuals
into the population. Experiments across four datasets show that our approach
significantly improves the balance between exploration and exploitation,
boosting performance. Moreover, the effectiveness of the EII mechanism in
reducing exploration time has been demonstrated, achieving an efficient
integration of EA and DRL on task-oriented dialogue policy tasks.

</details>


### [24] [TokAlign: Efficient Vocabulary Adaptation via Token Alignment](https://arxiv.org/abs/2506.03523)
*Chong Li,Jiajun Zhang,Chengqing Zong*

Main category: cs.CL

TL;DR: 论文提出TokAlign方法，通过词汇表对齐和知识迁移提升大语言模型在多语言/新领域的效率与性能。


<details>
  <summary>Details</summary>
Motivation: 现有分词器在新领域或语言中效率低下，且词汇表不匹配阻碍了模型间知识迁移（如token级蒸馏）。

Method: 1. 通过token共现学习词汇表的一对一映射矩阵；2. 重组模型参数并渐进微调以适应新词汇表。

Result: 多语言文本压缩率提升，困惑度从3.4e²降至1.2e²；仅需5k训练步恢复原模型性能，token级蒸馏效果提升4.4%。

Conclusion: TokAlign能高效统一词汇表并促进知识迁移，为跨语言/领域LLM优化提供新方案。

Abstract: Tokenization serves as a foundational step for Large Language Models (LLMs)
to process text. In new domains or languages, the inefficiency of the tokenizer
will slow down the training and generation of LLM. The mismatch in vocabulary
also hinders deep knowledge transfer between LLMs like token-level
distillation. To mitigate this gap, we propose an efficient method named
TokAlign to replace the vocabulary of LLM from the token co-occurrences view,
and further transfer the token-level knowledge between models. It first aligns
the source vocabulary to the target one by learning a one-to-one mapping matrix
for token IDs. Model parameters, including embeddings, are rearranged and
progressively fine-tuned for the new vocabulary. Our method significantly
improves multilingual text compression rates and vocabulary initialization for
LLMs, decreasing the perplexity from 3.4$\text{e}^2$ of strong baseline methods
to 1.2$\text{e}^2$ after initialization. Experimental results on models across
multiple parameter scales demonstrate the effectiveness and generalization of
TokAlign, which costs as few as 5k steps to restore the performance of the
vanilla model. After unifying vocabularies between LLMs, token-level
distillation can remarkably boost (+4.4% than sentence-level distillation) the
base model, costing only 235M tokens.

</details>


### [25] [Seed-Coder: Let the Code Model Curate Data for Itself](https://arxiv.org/abs/2506.03524)
*Yuyu Zhang,Jing Su,Yifan Sun,Chenguang Xi,Xia Xiao,Shen Zheng,Anxiang Zhang,Kaibo Liu,Daoguang Zan,Tao Sun,Jinhua Zhu,Shulin Xin,Dong Huang,Yetao Bai,Lixin Dong,Chao Li,Jianchong Chen,Hanzhi Zhou,Yifan Huang,Guanghan Ning,Xierui Song,Jiaze Chen,Siyao Liu,Kai Shen,Liang Xiang,Yonghui Wu*

Main category: cs.CL

TL;DR: Seed-Coder提出了一种最小化人工参与的代码预训练数据构建方法，通过模型中心化流程提升LLM在代码任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前开源LLM在代码预训练数据构建中过度依赖人工，存在扩展性差、主观偏见和维护成本高的问题。

Method: 采用模型中心化数据管道（LLM自动评分/过滤代码数据），结合监督微调、偏好优化和LongCoT强化学习训练推理模型。

Result: Seed-Coder在同类尺寸开源模型中达到SOTA，部分指标超越更大模型，在代码生成/补全/推理等任务中表现优异。

Conclusion: Seed-Coder证明了自动化代码数据构建的可行性，为LLM代码能力提升提供了高效可扩展的方案。

Abstract: Code data in large language model (LLM) pretraining is recognized crucial not
only for code-related tasks but also for enhancing general intelligence of
LLMs. Current open-source LLMs often heavily rely on human effort to produce
their code pretraining data, such as employing hand-crafted filtering rules
tailored to individual programming languages, or using human-annotated data to
train quality filters. However, these approaches are inherently limited in
scalability, prone to subjective biases, and costly to extend and maintain
across diverse programming languages. To address these challenges, we introduce
Seed-Coder, a series of open-source LLMs comprising base, instruct and
reasoning models of 8B size, minimizing human involvement in data construction.
Our code pretraining data is produced by a model-centric data pipeline, which
predominantly leverages LLMs for scoring and filtering code data. The instruct
model is further trained via supervised fine-tuning and preference
optimization, and the reasoning model leverages Long-Chain-of-Thought (LongCoT)
reinforcement learning to improve multi-step code reasoning. Seed-Coder
achieves state-of-the-art results among open-source models of similar size and
even surpasses some much larger models, demonstrating superior performance in
code generation, code completion, code editing, code reasoning, and software
engineering tasks.

</details>


### [26] [Go-Browse: Training Web Agents with Structured Exploration](https://arxiv.org/abs/2506.03533)
*Apurva Gandhi,Graham Neubig*

Main category: cs.CL

TL;DR: 提出Go-Browse方法，通过结构化探索自动收集大规模多样化网页数据，提升数字代理环境理解能力，在WebArena基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 数字代理常因缺乏对环境的理解而迷失（如在陌生网站中无法定位目标页面），需高效收集真实网页交互数据以解决该问题。

Method: 将数据收集建模为图搜索问题，通过结构化探索实现信息跨任务复用，基于WebArena基准收集10K成功任务轨迹和40K交互步骤。

Result: 微调后的70亿参数模型在WebArena上达到21.7%成功率，超越GPT-4o mini 2.4%，刷新10B参数以下模型SOTA结果2.9%。

Conclusion: Go-Browse通过高效探索构建的数据集显著提升了小规模语言模型在网页任务中的表现，验证了结构化数据收集方法的有效性。

Abstract: One of the fundamental problems in digital agents is their lack of
understanding of their environment. For instance, a web browsing agent may get
lost in unfamiliar websites, uncertain what pages must be visited to achieve
its goals. To address this, we propose Go-Browse, a method for automatically
collecting diverse and realistic web agent data at scale through structured
exploration of web environments. Go-Browse achieves efficient exploration by
framing data collection as a graph search, enabling reuse of information across
exploration episodes. We instantiate our method on the WebArena benchmark,
collecting a dataset of 10K successful task-solving trajectories and 40K
interaction steps across 100 URLs. Fine-tuning a 7B parameter language model on
this dataset achieves a success rate of 21.7% on the WebArena benchmark,
beating GPT-4o mini by 2.4% and exceeding current state-of-the-art results for
sub-10B parameter models by 2.9%.

</details>


### [27] [Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement](https://arxiv.org/abs/2506.03541)
*Xiaofeng Zhou,Heyan Huang,Lizi Liao*

Main category: cs.CL

TL;DR: 论文提出了一种新颖的Debate and Reflect (D&R)框架，通过小模型与强教师模型的多轮辩论生成反馈，并结合Tree-structured Direct Preference Optimization (T-DPO)方法，显著提升了小模型的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)在知识密集和复杂推理任务中表现出色，但高计算需求限制了其广泛应用。现有蒸馏技术难以实现持续显著的性能提升，因此需要更高效的模型压缩方法。

Method: 采用D&R框架组织小模型与教师模型的多轮辩论以获取反馈，并设计T-DPO方法将辩论日志组织为层次结构进行高效训练。

Result: 实验表明，该方法在多个NLP基准测试中显著提升小模型的准确性、鲁棒性和泛化能力，大幅超越传统基线。

Conclusion: D&R框架结合T-DPO方法为模型压缩提供了有效解决方案，实现了小模型性能的实质性突破。

Abstract: Large Language Models (LLMs) continue to set new standards in
knowledge-intensive and complex reasoning tasks, yet their high computational
demands limit widespread adoption. While distilling large models into smaller
ones offers a sustainable solution, current techniques--such as static
knowledge distillation, resource-intensive reinforcement learning from human
feedback, or limited self-reflection--struggle to yield substantial and lasting
performance gains. In this paper, we present a novel Debate and Reflect (D&R)
framework that orchestrates multi-turn debates between smaller models and
stronger teacher models, eliciting actionable feedback (e.g., error analysis,
corrective strategies) to guide student models. Further, we introduce
Tree-structured Direct Preference Optimization (T-DPO) to efficiently leverage
these debate logs, organizing interactions into a hierarchical format for
effective training. Empirical evaluations across diverse NLP benchmarks
demonstrate that our approach significantly improves smaller-model accuracy,
robustness, and generalization, outperforming conventional baselines by a large
margin.

</details>


### [28] [BPO: Revisiting Preference Modeling in Direct Preference Optimization](https://arxiv.org/abs/2506.03557)
*Lin Sun,Chuang Liu,Peng Liu,Bingyang Li,Weijia Lu,Ning Wu*

Main category: cs.CL

TL;DR: 论文提出平衡偏好优化（BPO）框架，解决直接偏好优化（DPO）因忽略绝对奖励幅度导致的性能下降问题，显著提升模型准确性且易于实现。


<details>
  <summary>Details</summary>
Motivation: 直接偏好优化（DPO）虽能通过成对排序损失保持选择与拒绝响应的相对顺序，但常忽略绝对奖励幅度，导致选择响应概率降低及分布外响应风险增加，即退化选择响应（DCR）问题。

Method: 提出平衡偏好优化（BPO）框架，通过平衡奖励边际和间隙适配器动态优化选择与拒绝响应，无需对损失函数引入额外约束。

Result: 在数学推理任务中，BPO显著优于DPO及变体，如Llama-3.1-8B-Instruct准确率提升10.1%，Qwen2.5-Math-7B提升11.7%，且仅需单行代码修改。

Conclusion: BPO有效解决了DPO的DCR问题，兼容现有框架并大幅提升性能，为对齐人类偏好的模型优化提供了高效方案。

Abstract: Direct Preference Optimization (DPO) have emerged as a popular method for
aligning Large Language Models (LLMs) with human preferences. While DPO
effectively preserves the relative ordering between chosen and rejected
responses through pairwise ranking losses, it often neglects absolute reward
magnitudes. This oversight can decrease the likelihood of chosen responses and
increase the risk of generating out-of-distribution responses, leading to poor
performance. We term this issue Degraded Chosen Responses (DCR).To address this
issue, we propose Balanced Preference Optimization (BPO), a novel framework
that dynamically balances the optimization of chosen and rejected responses
through two key components: balanced reward margin and gap adaptor. Unlike
previous methods, BPO can fundamentally resolve DPO's DCR issue, without
introducing additional constraints to the loss function. Experimental results
on multiple mathematical reasoning tasks show that BPO significantly
outperforms DPO, improving accuracy by +10.1% with Llama-3.1-8B-Instruct (18.8%
to 28.9%) and +11.7% with Qwen2.5-Math-7B (35.0% to 46.7%). It also surpasses
DPO variants by +3.6% over IPO (43.1%), +5.0% over SLiC (41.7%), and +3.1% over
Cal-DPO (43.6%) on the same model. Remarkably, our algorithm requires only a
single line of code modification, making it simple to implement and fully
compatible with existing DPO-based frameworks.

</details>


### [29] [ConsistentChat: Building Skeleton-Guided Consistent Dialogues for Large Language Models from Scratch](https://arxiv.org/abs/2506.03558)
*Jiawei Chen,Xinyan Guan,Qianhao Yuan,Guozhao Mo,Weixiang Zhou,Yaojie Lu,Hongyu Lin,Ben He,Le Sun,Xianpei Han*

Main category: cs.CL

TL;DR: 提出骨架引导的多轮对话生成框架，通过建模对话意图和生成结构化骨架提升多轮指令合成的连贯性，构建ConsistentChat数据集并在实验中显著提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有指令数据合成方法主要关注单轮指令，忽视跨轮次连贯性，导致长对话中出现上下文漂移和任务完成率下降。

Method: 采用两阶段框架：1) 意图建模，将对话归类到9种预定义意图轨迹以保持连贯性；2) 骨架生成，构建与意图对齐的用户查询序列作为下游合成的约束框架。

Result: 基于ConsistentChat微调的模型在对话连贯性上提升20-30%，任务成功率最高提升15%，显著优于现有单轮/多轮数据集训练的模型。

Conclusion: 骨架引导的对话生成方法有效解决多轮指令合成的连贯性问题，ConsistentChat数据集为相关研究提供了高质量基准。

Abstract: Current instruction data synthesis methods primarily focus on single-turn
instructions and often neglect cross-turn coherence, resulting in context drift
and reduced task completion rates in extended conversations. To address this
limitation, we propose Skeleton-Guided Multi-Turn Dialogue Generation, a
framework that constrains multi-turn instruction synthesis by explicitly
modeling human conversational intent. It operates in two stages: (1) Intent
Modeling, which captures the global structure of human dialogues by assigning
each conversation to one of nine well-defined intent trajectories, ensuring a
coherent and goal-oriented information flow; and (2) Skeleton Generation, which
constructs a structurally grounded sequence of user queries aligned with the
modeled intent, thereby serving as a scaffold that constrains and guides the
downstream instruction synthesis process. Based on this process, we construct
ConsistentChat, a multi-turn instruction dataset with approximately 15,000
multi-turn conversations and 224,392 utterances. Experiments on the Light,
Topdial, and MT-Eval benchmarks show that models fine-tuned on ConsistentChat
achieve a 20-30% improvement in chat consistency and up to a 15% increase in
task success rate, significantly outperforming models trained on existing
single-turn and multi-turn instruction datasets.

</details>


### [30] [POSS: Position Specialist Generates Better Draft for Speculative Decoding](https://arxiv.org/abs/2506.03566)
*Langlin Huang,Chengsong Huang,Jixuan Leng,Di Huang,Jiaxin Huang*

Main category: cs.CL

TL;DR: 论文提出PosS方法，通过位置专用草稿层提升大语言模型推理速度，解决了草稿模型在后续位置预测质量下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在草稿模型生成特征的误差累积导致后续位置预测质量下降，影响了推理效率。

Method: 提出位置专用草稿层(PosS)，每个层专注于处理特定位置的草稿模型特征偏差，提升后续位置的令牌接受率。

Result: 在Llama-3-8B-Instruct和Llama-2-13B-chat模型上，PosS显著提高了平均接受长度和加速比。

Conclusion: PosS有效提升了草稿模型的预测准确性，特别是在后续位置，从而加速了大语言模型的推理过程。

Abstract: Speculative decoding accelerates Large Language Model (LLM) inference by
using a small draft model to predict multiple tokens, and a large target model
to verify these tokens in parallel. Recent studies leverage the hidden state of
the target model to enhance draft model prediction accuracy. However, existing
methods suffer from the degrading quality of draft token predictions at later
positions, due to error accumulation in draft model generated features. In this
paper, we propose Position Specialists (PosS), which consist of multiple
position-specialized draft layers to generate tokens at assigned position(s).
Position specialists greatly improve token acceptance rate at later positions
per drafting round, as each specialist only needs to focus on handling a
certain level of draft model feature deviation. Experiment results on
Llama-3-8B-Instruct and Llama-2-13B-chat across six datasets demonstrate that
PosS effectively improves over baselines on average acceptance length and
speed-up ratio. Our codebase is available at https://github.com/shrango/PosS.

</details>


### [31] [MiMo-VL Technical Report](https://arxiv.org/abs/2506.03569)
*Xiaomi LLM-Core Team,:,Zihao Yue,Zhenru Lin,Yifan Song,Weikun Wang,Shuhuai Ren,Shuhao Gu,Shicheng Li,Peidian Li,Liang Zhao,Lei Li,Kainan Bao,Hao Tian,Hailin Zhang,Gang Wang,Dawei Zhu,Cici,Chenhong He,Bowen Ye,Bowen Shen,Zihan Zhang,Zihan Jiang,Zhixian Zheng,Zhichao Song,Zhenbo Luo,Yue Yu,Yudong Wang,Yuanyuan Tian,Yu Tu,Yihan Yan,Yi Huang,Xu Wang,Xinzhe Xu,Xingchen Song,Xing Zhang,Xing Yong,Xin Zhang,Xiangwei Deng,Wenyu Yang,Wenhan Ma,Weiwei Lv,Weiji Zhuang,Wei Liu,Sirui Deng,Shuo Liu,Shimao Chen,Shihua Yu,Shaohui Liu,Shande Wang,Rui Ma,Qiantong Wang,Peng Wang,Nuo Chen,Menghang Zhu,Kangyang Zhou,Kang Zhou,Kai Fang,Jun Shi,Jinhao Dong,Jiebao Xiao,Jiaming Xu,Huaqiu Liu,Hongshen Xu,Heng Qu,Haochen Zhao,Hanglong Lv,Guoan Wang,Duo Zhang,Dong Zhang,Di Zhang,Chong Ma,Chang Liu,Can Cai,Bingquan Xia*

Main category: cs.CL

TL;DR: 小米开源了MiMo-VL-7B-SFT和MiMo-VL-7B-RL两个视觉语言模型，在多项任务中表现优异，特别是在GUI grounding应用上创下新标准。


<details>
  <summary>Details</summary>
Motivation: 旨在开发高性能的视觉语言模型，提升多模态推理和通用视觉理解能力，并通过开源促进领域发展。

Method: 采用四阶段预训练（2.4万亿token）结合混合策略强化学习（MORL），整合多样奖励信号，并强调高质量推理数据的重要性。

Result: MiMo-VL-7B-RL在40项任务中的35项超越Qwen2.5-VL-7B，OlympiadBench得分59.4，OSWorld-G得分56.1，优于专业模型UI-TARS。

Conclusion: MiMo-VL模型通过创新的训练方法和高质量数据，在多模态任务中实现了领先性能，并提供了全面的评估套件以推动领域进步。

Abstract: We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language
models delivering state-of-the-art performance in both general visual
understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B
on 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing
models with up to 78B parameters. For GUI grounding applications, it sets a new
standard with 56.1 on OSWorld-G, even outperforming specialized models such as
UI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens)
with Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward
signals. We identify the importance of incorporating high-quality reasoning
data with long Chain-of-Thought into pre-training stages, and the benefits of
mixed RL despite challenges in simultaneous multi-domain optimization. We also
contribute a comprehensive evaluation suite covering 50+ tasks to promote
reproducibility and advance the field. The model checkpoints and full
evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.

</details>


### [32] [FreePRM: Training Process Reward Models Without Ground Truth Process Labels](https://arxiv.org/abs/2506.03570)
*Lin Sun,Chuang Liu,Xiaofeng Ma,Tao Yang,Weijia Lu,Ning Wu*

Main category: cs.CL

TL;DR: FreePRM提出了一种无需真实步骤标签的弱监督框架，通过伪标签和缓冲概率方法提升过程奖励模型性能，显著降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 传统过程奖励模型（PRM）依赖高成本的步骤级标注数据，限制了其大规模应用。FreePRM旨在解决这一问题，减少对标注数据的依赖。

Method: FreePRM首先生成基于最终结果正确性的伪步骤级标签，然后利用缓冲概率消除伪标签中的噪声影响。

Result: 在ProcessBench上，FreePRM平均F1得分为53.0%，优于全监督PRM（+24.1%）及其他开源PRM模型（最高+24.6%）。

Conclusion: FreePRM为PRM训练提供了新范式，在保持高性能的同时显著降低了对昂贵标注数据的依赖。

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated that
Process Reward Models (PRMs) play a crucial role in enhancing model
performance. However, training PRMs typically requires step-level labels,
either manually annotated or automatically generated, which can be costly and
difficult to obtain at scale. To address this challenge, we introduce FreePRM,
a weakly supervised framework for training PRMs without access to ground-truth
step-level labels. FreePRM first generates pseudo step-level labels based on
the correctness of final outcome, and then employs Buffer Probability to
eliminate impact of noise inherent in pseudo labeling. Experimental results
show that FreePRM achieves an average F1 score of 53.0% on ProcessBench,
outperforming fully supervised PRM trained on Math-Shepherd by +24.1%. Compared
to other open-source PRMs, FreePRM outperforms upon RLHFlow-PRM-Mistral-8B
(28.4%) by +24.6%, EurusPRM (31.3%) by +21.7%, and Skywork-PRM-7B (42.1%) by
+10.9%. This work introduces a new paradigm in PRM training, significantly
reducing reliance on costly step-level annotations while maintaining strong
performance.

</details>


### [33] [Exchange of Perspective Prompting Enhances Reasoning in Large Language Models](https://arxiv.org/abs/2506.03573)
*Lin Sun,Can Zhang*

Main category: cs.CL

TL;DR: 论文提出EoP框架，通过交换问题定义视角突破大语言模型的思维定式，在多个基准测试中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在NLP任务中表现受限，主要因其对问题的固有理解模式存在局限。

Method: 提出Exchange-of-Perspective (EoP)框架，通过跨问题定义交换视角来打破思维定式。

Result: 在8个基准测试中，EoP使GPT-3.5-Turbo在AQuA上提升3.6%，GPT-4在Math任务上准确率提升7.7%。

Conclusion: EoP能有效增强大语言模型的问题解决能力，尤其在数学推理任务中表现突出。

Abstract: Large language models (LLMs) have made significant advancements in addressing
diverse natural language processing (NLP) tasks. However, their performance is
often limited by inherent comprehension of problems. To address this
limitation, we propose Exchange-of-Perspective (EoP), a novel framework
designed to exchange perspectives across different definitions of problem, so
that it can break the fixed mindset from any particular formulation of the
question. We conducted extensive and comprehensive experiments on 8 benchmarks.
The results show that EoP can significantly improve performance. For instance,
compared to the non-commutative baseline PHP, with GPT-3.5-Turbo and EoP, we
observe a 3.6% improvement on AQuA (60.6% to 64.2%), while GPT-4-powered EoP
demonstrates a 7.7% overall accuracy enhancement on Math (53.9% to 61.6%) and a
3.5% improvement on OlympiadBench Maths (43.5% to 47.0%) when using
Qwen-2.5-72b.

</details>


### [34] [KG-BiLM: Knowledge Graph Embedding via Bidirectional Language Models](https://arxiv.org/abs/2506.03576)
*Zirui Chen,Xin Wang,Zhao Li,Wenbin Guo,Dongxiao He*

Main category: cs.CL

TL;DR: KG-BiLM提出了一种双向语言模型框架，结合知识图谱结构信息与生成式Transformer的语义表达，通过双向知识注意力、知识掩码预测和对比图语义聚合三组件，显著提升复杂多跳关系下的链接预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常单独侧重知识图谱结构或文本语义，缺乏同时捕捉全局图谱连通性、细粒度语言上下文和判别性推理语义的统一框架。

Method: 提出KG-BiLM框架，包含：(1) 双向知识注意力（解除因果掩码实现全交互）；(2) 知识掩码预测（联合利用局部语义与全局图结构）；(3) 对比图语义聚合（通过子图表示对比保持图谱结构）。

Result: 在标准基准测试中，KG-BiLM尤其在具有复杂多跳关系的大规模图谱上，链接预测性能显著优于基线模型。

Conclusion: KG-BiLM成功验证了结构信息与文本语义统一框架的有效性，为知识表示学习提供了新方向。

Abstract: Recent advances in knowledge representation learning (KRL) highlight the
urgent necessity to unify symbolic knowledge graphs (KGs) with language models
(LMs) for richer semantic understanding. However, existing approaches typically
prioritize either graph structure or textual semantics, leaving a gap: a
unified framework that simultaneously captures global KG connectivity, nuanced
linguistic context, and discriminative reasoning semantics. To bridge this gap,
we introduce KG-BiLM, a bidirectional LM framework that fuses structural cues
from KGs with the semantic expressiveness of generative transformers. KG-BiLM
incorporates three key components: (i) Bidirectional Knowledge Attention, which
removes the causal mask to enable full interaction among all tokens and
entities; (ii) Knowledge-Masked Prediction, which encourages the model to
leverage both local semantic contexts and global graph connectivity; and (iii)
Contrastive Graph Semantic Aggregation, which preserves KG structure via
contrastive alignment of sampled sub-graph representations. Extensive
experiments on standard benchmarks demonstrate that KG-BiLM outperforms strong
baselines in link prediction, especially on large-scale graphs with complex
multi-hop relations - validating its effectiveness in unifying structural
information and textual semantics.

</details>


### [35] [Automatically Suggesting Diverse Example Sentences for L2 Japanese Learners Using Pre-Trained Language Models](https://arxiv.org/abs/2506.03580)
*Enrico Benedetti,Akiko Aizawa,Florian Boudin*

Main category: cs.CL

TL;DR: 研究探讨了使用预训练语言模型(PLMs)为日语学习者生成多样化且符合语言水平的例句，发现检索方法优于生成方法，但PLMs在提升例句推荐系统适应性方面具有潜力。


<details>
  <summary>Details</summary>
Motivation: 提供多样化且符合学习者语言水平的例句对有效语言习得至关重要，因此研究探索如何利用PLMs优化日语学习例句的生成与推荐。

Method: 采用两种PLMs应用方式：1)作为检索系统中的质量评分组件(基于新构建的日语句库)；2)通过零样本学习直接生成例句。通过学习者、母语者和GPT-4组成的小组从难度、多样性和自然性多维度评估质量。

Result: 评估者对句子质量评分存在固有分歧(难度除外)。检索方法普遍更受青睐(尤其对初级和高级学习者)，生成方法平均得分较低。但实验证明了PLMs在提升例句推荐系统适应性方面的潜力。

Conclusion: 虽然检索方法表现更优，但PLMs能增强例句推荐系统的适应性，从而改善语言学习体验。需注意不同评估者对句子质量评判标准存在差异。

Abstract: Providing example sentences that are diverse and aligned with learners'
proficiency levels is essential for fostering effective language acquisition.
This study examines the use of Pre-trained Language Models (PLMs) to produce
example sentences targeting L2 Japanese learners. We utilize PLMs in two ways:
as quality scoring components in a retrieval system that draws from a newly
curated corpus of Japanese sentences, and as direct sentence generators using
zero-shot learning. We evaluate the quality of sentences by considering
multiple aspects such as difficulty, diversity, and naturalness, with a panel
of raters consisting of learners of Japanese, native speakers -- and GPT-4. Our
findings suggest that there is inherent disagreement among participants on the
ratings of sentence qualities, except for difficulty. Despite that, the
retrieval approach was preferred by all evaluators, especially for beginner and
advanced target proficiency, while the generative approaches received lower
scores on average. Even so, our experiments highlight the potential for using
PLMs to enhance the adaptability of sentence suggestion systems and therefore
improve the language learning journey.

</details>


### [36] [From Understanding to Generation: An Efficient Shortcut for Evaluating Language Models](https://arxiv.org/abs/2506.03592)
*Viktor Hangya,Fabian Küch,Darina Gold*

Main category: cs.CL

TL;DR: 该论文提出将生成式任务转化为更经济的理解式任务，以降低大语言模型训练中评估关键能力的计算成本。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练中，生成式评估（如推理和代码生成）耗时且计算量大，而理解式评估成本较低。为实时监控模型能力发展，需降低评估成本。

Method: 将生成式任务（NLG）重新设计为理解式任务（NLU），并在8个不同规模的模型和4种能力（数学推理、代码生成、事实知识和阅读理解）上测试两种任务格式的性能相关性。

Result: 生成式与理解式任务表现强相关，评估时间平均减少35倍以上，验证了低成本替代方案的可行性。

Conclusion: 通过任务重构可实现高效的大语言模型能力评估，相关基准测试将开源。

Abstract: Iterative evaluation of LLMs during training is essential to ensure expected
capability development, but can be time- and compute-intensive. While NLU
tasks, where the model selects from fixed answer choices, are cheap to
evaluate, essential capabilities like reasoning and code generation rely on the
more time-consuming NLG (token-by-token generation) format. In this work, our
aim is to decrease the computational burden of NLG benchmarks in order to
enable monitoring crucial LLM capabilities during model training. We
reformulate generative tasks into computationally cheaper NLU alternatives. We
test the performance correlation between the original and reformulated tasks
using 8 LMs of various sizes and 4 capabilities: mathematical reasoning, code
generation, factual knowledge and reading comprehension. Our results show a
strong correlation between task formats, supporting capability assessment via
cheaper alternatives and achieving over 35x average reduction in evaluation
time. We plan to publish our benchmark adaptions.

</details>


### [37] [Is linguistically-motivated data augmentation worth it?](https://arxiv.org/abs/2506.03593)
*Ray Groshan,Michael Ginn,Alexis Palmer*

Main category: cs.CL

TL;DR: 本文系统比较了语言无关和语言相关的数据增强策略在低资源语言任务中的效果，发现后者在生成数据与训练分布相似时更优。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对语言无关和语言相关数据增强策略的系统比较，不确定后者是否真的能带来更好的下游任务性能。

Method: 在两种低资源语言（Uspanteko和Arapaho）上，对多种数据增强策略及其组合进行了全面评估，涵盖机器翻译和语际注释任务。

Result: 语言相关的数据增强策略在生成数据与训练分布相似时优于语言无关策略。

Conclusion: 语言相关的数据增强策略有效，但需确保生成数据与训练分布相似。

Abstract: Data augmentation, a widely-employed technique for addressing data scarcity,
involves generating synthetic data examples which are then used to augment
available training data. Researchers have seen surprising success from simple
methods, such as random perturbations from natural examples, where models seem
to benefit even from data with nonsense words, or data that doesn't conform to
the rules of the language. A second line of research produces synthetic data
that does in fact follow all linguistic constraints; these methods require some
linguistic expertise and are generally more challenging to implement. No
previous work has done a systematic, empirical comparison of both
linguistically-naive and linguistically-motivated data augmentation strategies,
leaving uncertainty about whether the additional time and effort of
linguistically-motivated data augmentation work in fact yields better
downstream performance.
  In this work, we conduct a careful and comprehensive comparison of
augmentation strategies (both linguistically-naive and
linguistically-motivated) for two low-resource languages with different
morphological properties, Uspanteko and Arapaho. We evaluate the effectiveness
of many different strategies and their combinations across two important
sequence-to-sequence tasks for low-resource languages: machine translation and
interlinear glossing. We find that linguistically-motivated strategies can have
benefits over naive approaches, but only when the new examples they produce are
not significantly unlike the training data distribution.

</details>


### [38] [Auto prompt sql: a resource-efficient architecture for text-to-sql translation in constrained environments](https://arxiv.org/abs/2506.03598)
*Zetong Tang,Qian Ma,Di Wu*

Main category: cs.CL

TL;DR: AP-SQL是一种新型架构，旨在解决资源受限环境下Text-to-SQL任务中开源小模型与闭源大模型之间的性能差距。


<details>
  <summary>Details</summary>
Motivation: 当前Text-to-SQL方法在资源受限环境中面临挑战，因为它们依赖资源密集型的开源模型。

Method: AP-SQL通过任务分解（模式过滤、基于上下文示例的检索增强Text-to-SQL生成、提示驱动的模式链接和SQL生成）以及利用CoT和GoT模板优化提示工程。

Result: 在Spider基准测试上的全面评估证明了AP-SQL的有效性。

Conclusion: AP-SQL成功地在资源效率和性能之间找到了平衡，显著提升了Text-to-SQL任务的准确性。

Abstract: Using the best Text-to-SQL methods in resource-constrained environments is
challenging due to their reliance on resource-intensive open-source models.
This paper introduces Auto Prompt SQL(AP-SQL), a novel architecture designed to
bridge the gap between resource-efficient small open-source models and the
powerful capabilities of large closed-source models for Text-to-SQL
translation. Our method decomposes the task into schema filtering,
retrieval-augmented text-to-SQL generation based on in-context examples, and
prompt-driven schema linking and SQL generation. To improve schema selection
accuracy, we fine-tune large language models. Crucially, we also explore the
impact of prompt engineering throughout the process, leveraging
Chain-of-Thought(CoT) and Graph-of-Thought(GoT) templates to significantly
enhance the model's reasoning for accurate SQL generation. Comprehensive
evaluations on the Spider benchmarks demonstrate the effectiveness of AP-SQL.

</details>


### [39] [Learning to Insert [PAUSE] Tokens for Better Reasoning](https://arxiv.org/abs/2506.03616)
*Eunki Kim,Sangryul Kim,James Thorne*

Main category: cs.CL

TL;DR: 本文提出了一种名为动态插入标记训练（DIT）的新方法，通过在模型置信度最低的位置插入[PAUSE]标记，显著提升了大型语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 先前的研究表明，在推理步骤前连续插入特殊标记可以增强模型效果。基于此，本文旨在探索一种更动态、基于模型自身置信度的标记插入策略，以进一步提升模型的推理性能。

Method: 提出动态插入标记训练（DIT），根据标记的对数似然识别序列中模型置信度最低的位置，并在这些位置策略性地插入[PAUSE]标记，以增强模型对后续标记的预测能力。

Result: 实验结果表明，DIT在多个数据集和模型上均优于传统微调和先前的标记插入方法，如在GSM8K上准确率提升4.7%，在AQUA-RAT上提升3.23%，在MBPP数据集上pass@1提升3.4%。

Conclusion: DIT作为一种基于模型的动态方法，而非启发式方法，不仅简单有效，还拓宽了推理研究的范围。

Abstract: To enhance reasoning capabilities, previous works have explored incorporating
special-purpose tokens into the training process. These strategies strengthen
the learning mechanism of transformer-based large language models (LLMs).
Building on prior research, in which inserting dummy tokens consecutively just
before reasoning steps can enhance effectiveness, we introduce a novel approach
termed Dynamic Inserting Tokens Training (DIT). Our method identifies positions
within sequences where model confidence is lowest according to token
log-likelihood. Strategically inserting [PAUSE] tokens on these positions
bolsters the model's predictive capabilities for subsequent tokens.
Experimental results across diverse datasets and models, from the 2.7B model to
the 8B model, demonstrate that DIT consistently outperforms traditional
fine-tuning and previous token insertion methods. With this simple yet
effective method, we achieve accuracy gains of up to 4.7%p on GSM8K, 3.23%p on
AQUA-RAT, and pass@1 improvements of up to 3.4%p on MBPP datasets. Our work
shows a model-based, dynamic approach rather than a heuristic one, thereby
broadening the scope of research in reasoning.

</details>


### [40] [Do Large Language Models Know Folktales? A Case Study of Yokai in Japanese Folktales](https://arxiv.org/abs/2506.03619)
*Ayuto Tsutsumi,Yuu Jinnai*

Main category: cs.CL

TL;DR: 该论文评估了大型语言模型（LLMs）对日本妖怪文化的理解能力，并提出了一个名为YokaiEval的基准数据集。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的文化知识通常局限于英语社区，这可能导致非英语社区的文化被边缘化。为了解决这一问题，论文聚焦于评估LLMs对日本妖怪（Yokai）这一文化载体的理解能力。

Method: 论文引入了YokaiEval基准数据集，包含809道关于妖怪的多选题，并评估了31个日语和多语言LLMs的表现。

Result: 结果显示，使用日语资源训练的模型（尤其是基于Llama-3的模型）表现优于以英语为中心的模型。

Conclusion: 研究表明，通过针对特定文化的训练，可以提升LLMs的文化意识，特别是在非英语文化中的表现。

Abstract: Although Large Language Models (LLMs) have demonstrated strong language
understanding and generation abilities across various languages, their cultural
knowledge is often limited to English-speaking communities, which can
marginalize the cultures of non-English communities. To address the problem,
evaluation of the cultural awareness of the LLMs and the methods to develop
culturally aware LLMs have been investigated. In this study, we focus on
evaluating knowledge of folktales, a key medium for conveying and circulating
culture. In particular, we focus on Japanese folktales, specifically on
knowledge of Yokai. Yokai are supernatural creatures originating from Japanese
folktales that continue to be popular motifs in art and entertainment today.
Yokai have long served as a medium for cultural expression, making them an
ideal subject for assessing the cultural awareness of LLMs. We introduce
YokaiEval, a benchmark dataset consisting of 809 multiple-choice questions
(each with four options) designed to probe knowledge about yokai. We evaluate
the performance of 31 Japanese and multilingual LLMs on this dataset. The
results show that models trained with Japanese language resources achieve
higher accuracy than English-centric models, with those that underwent
continued pretraining in Japanese, particularly those based on Llama-3,
performing especially well. The code and dataset are available at
https://github.com/CyberAgentA ILab/YokaiEval.

</details>


### [41] [Robustness of Prompting: Enhancing Robustness of Large Language Models Against Prompting Attacks](https://arxiv.org/abs/2506.03627)
*Lin Mu,Guowei Chu,Li Ni,Lei Sang,Zhize Wu,Peiquan Jin,Yiwen Zhang*

Main category: cs.CL

TL;DR: 论文提出RoP策略，通过纠错和引导两阶段增强大语言模型对输入扰动的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型对输入扰动敏感，现有提示技术难以有效缓解此问题，需开发新策略提升鲁棒性。

Method: RoP策略分两阶段：纠错阶段生成对抗样本并自动修正输入错误，引导阶段基于修正输入生成最优提示。

Result: 实验表明RoP显著提升模型在算术、常识和逻辑推理任务中的抗扰动能力，且精度接近干净输入场景。

Conclusion: RoP是提升大语言模型实际应用鲁棒性的有效方法。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
various tasks by effectively utilizing a prompting strategy. However, they are
highly sensitive to input perturbations, such as typographical errors or slight
character order errors, which can substantially degrade their performance.
Despite advances in prompting techniques, developing a prompting strategy that
explicitly mitigates the negative impact of such perturbations remains an open
challenge. To bridge this gap, we propose Robustness of Prompting (RoP), a
novel prompting strategy specifically designed to enhance the robustness of
LLMs. RoP consists of two stages: Error Correction and Guidance. In the Error
Correction stage, RoP applies diverse perturbation methods to generate
adversarial examples, which are then used to construct prompts that
automatically correct input errors. In the Guidance stage, RoP generates an
optimal guidance prompting based on the corrected input, steering the model
toward more robust and accurate inferences. Through comprehensive experiments
spanning arithmetic, commonsense, and logical reasoning tasks, we demonstrate
that RoP significantly improves LLMs' robustness against adversarial
perturbations. Notably, it maintains model accuracy with only minimal
degradation compared to clean input scenarios, thereby establishing RoP as a
practical and effective approach for enhancing LLM robustness in real-world
applications.

</details>


### [42] [RewardAnything: Generalizable Principle-Following Reward Models](https://arxiv.org/abs/2506.03637)
*Zhuohao Yu,Jiali Zeng,Weizheng Gu,Yidong Wang,Jindong Wang,Fandong Meng,Jie Zhou,Yue Zhang,Shikun Zhang,Wei Ye*

Main category: cs.CL

TL;DR: 论文提出了一种新型奖励模型RewardAnything，能够动态遵循自然语言原则，解决了传统奖励模型适应性差、资源消耗大的问题。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型基于固定偏好数据集训练，难以适应多样化需求，且任务特定数据收集和重新训练成本高昂。

Method: 引入可泛化、遵循原则的奖励模型RewardAnything，通过自然语言指令动态调整奖励原则，并开发RABench基准测试其性能。

Result: RewardAnything在传统基准测试中表现优异，且在RABench上展示出对新原则的强大适应能力，无需重新训练。

Conclusion: RewardAnything为奖励模型提供了灵活、高效的解决方案，能够无缝集成现有RLHF方法，仅通过自然语言原则即可对齐大型语言模型。

Abstract: Reward Models, essential for guiding Large Language Model optimization, are
typically trained on fixed preference datasets, resulting in rigid alignment to
single, implicit preference distributions. This prevents adaptation to diverse
real-world needs-from conciseness in one task to detailed explanations in
another. The standard practice of collecting task-specific preference data and
retraining reward models is resource-intensive, often producing biased rewards,
and limits practical application. We introduce generalizable,
principle-following reward models. We propose that RMs should understand and
adhere to dynamically provided natural language specifications of reward
principles, similar to instruction-following in LLMs. To measure this
capability, we develop RABench, a comprehensive benchmark for RMs focusing on
generalization across diverse principles. Evaluations on RABench reveal poor
generalization of current RMs. As a solution, we present RewardAnything, a
novel RM designed and trained to explicitly follow natural language principles.
We achieve SotA performance with RewardAnything in traditional RM benchmark
simply by specifying a well-defined principle, and results on RABench show we
excel in adapting to novel principles without retraining. Furthermore,
RewardAnything integrates seamlessly with existing RLHF methods and we show by
a case study on how to automatically and efficiently align LLMs with only
natural language principles.

</details>


### [43] [Trustworthy Medical Question Answering: An Evaluation-Centric Survey](https://arxiv.org/abs/2506.03659)
*Yinuo Wang,Robert E. Mercer,Frank Rudzicz,Sudipta Singha Roy,Pengjie Ren,Zhumin Chen,Xindi Wang*

Main category: cs.CL

TL;DR: 该论文综述了医疗问答系统中基于大语言模型的六大可信维度，分析了现有评估方法，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 医疗问答系统的可信度直接影响患者安全和临床决策，但现有系统在复杂医疗场景下面临多维可信挑战。

Method: 系统性地审查了事实性、鲁棒性、公平性、安全性、可解释性和校准性六大维度，并比较了相关评估基准与技术。

Result: 总结了现有评估方法的局限性，如专家评估可扩展性不足，并提出了检索增强、对抗微调等改进技术。

Conclusion: 需要开发集成多维度的指标和真实场景部署研究，以推动医疗问答系统的安全可靠应用。

Abstract: Trustworthiness in healthcare question-answering (QA) systems is important
for ensuring patient safety, clinical effectiveness, and user confidence. As
large language models (LLMs) become increasingly integrated into medical
settings, the reliability of their responses directly influences clinical
decision-making and patient outcomes. However, achieving comprehensive
trustworthiness in medical QA poses significant challenges due to the inherent
complexity of healthcare data, the critical nature of clinical scenarios, and
the multifaceted dimensions of trustworthy AI. In this survey, we
systematically examine six key dimensions of trustworthiness in medical QA,
i.e., Factuality, Robustness, Fairness, Safety, Explainability, and
Calibration. We review how each dimension is evaluated in existing LLM-based
medical QA systems. We compile and compare major benchmarks designed to assess
these dimensions and analyze evaluation-guided techniques that drive model
improvements, such as retrieval-augmented grounding, adversarial fine-tuning,
and safety alignment. Finally, we identify open challenges-such as scalable
expert evaluation, integrated multi-dimensional metrics, and real-world
deployment studies-and propose future research directions to advance the safe,
reliable, and transparent deployment of LLM-powered medical QA.

</details>


### [44] [ROSA: Addressing text understanding challenges in photographs via ROtated SAmpling](https://arxiv.org/abs/2506.03665)
*Hernán Maina,Guido Ivetta,Mateo Lione Stuto,Julian Martin Eisenschlos,Jorge Sánchez,Luciana Benotti*

Main category: cs.CL

TL;DR: 论文提出ROSA解码策略，提升视觉问答系统对视力障碍者拍摄的文本图像的识别性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉问答系统在处理视力障碍者拍摄的文本图像时表现不佳，主要因为这些图像中的文本常存在错位问题，而现有基准数据集未能充分反映这一挑战。

Method: 通过深入访谈识别常见拍摄问题，并提出ROSA（旋转采样）解码策略，专门针对错位文本进行优化。

Result: 在最佳模型中，ROSA比贪婪解码策略的准确率提高了11.7个百分点。

Conclusion: ROSA有效解决了视力障碍者拍摄文本图像的识别难题，显著提升了视觉问答系统的实用性。

Abstract: Visually impaired people could benefit from Visual Question Answering (VQA)
systems to interpret text in their surroundings. However, current models often
struggle with recognizing text in the photos taken by this population. Through
in-depth interviews with visually impaired individuals, we identified common
framing conventions that frequently result in misaligned text. Existing VQA
benchmarks primarily feature well-oriented text captured by sighted users,
under-representing these challenges. To address this gap, we introduce ROtated
SAmpling (ROSA), a decoding strategy that enhances VQA performance in text-rich
images with incorrectly oriented text. ROSA outperforms Greedy decoding by 11.7
absolute points in the best-performing model.

</details>


### [45] [Efficient Data Selection for Domain Adaptation of ASR Using Pseudo-Labels and Multi-Stage Filtering](https://arxiv.org/abs/2506.03681)
*Pradeep Rangappa,Andres Carofilis,Jeena Prakash,Shashi Kumar,Sergio Burdisso,Srikanth Madikeri,Esau Villatoro-Tello,Bidisha Sharma,Petr Motlicek,Kadri Hacioglu,Shankar Venkatesan,Saurabh Vyas,Andreas Stolcke*

Main category: cs.CL

TL;DR: 论文提出一种通过多策略过滤伪标签数据的方法，显著减少ASR模型微调所需数据量，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 针对小机构在有限标注数据和计算资源下微调预训练ASR模型的挑战，研究如何高效筛选高质量伪标签数据。

Method: 集成WER预测、NER和CER分析策略，对Whisper和Zipformer生成的伪标签进行过滤，构建高质量训练集。

Result: 在7500小时呼叫中心数据上，仅用100小时（1.4%）过滤数据即可达到12.3% WER，与全量数据性能相当。

Conclusion: 多策略数据选择方法能有效提升ASR领域适应效率，大幅降低计算资源需求。

Abstract: Fine-tuning pretrained ASR models for specific domains is challenging for
small organizations with limited labeled data and computational resources.
Here, we explore different data selection pipelines and propose a robust
approach that improves ASR adaptation by filtering pseudo-labels generated
using Whisper (encoder-decoder) and Zipformer (transducer) models. Our approach
integrates multiple selection strategies -- including word error rate (WER)
prediction, named entity recognition (NER), and character error rate (CER)
analysis -- to extract high-quality training segments. We evaluate our method
on Whisper and Zipformer using a 7500-hour baseline, comparing it to a
CER-based approach relying on hypotheses from three ASR systems. Fine-tuning on
7500 hours of pseudo-labeled call center data achieves 12.3% WER, while our
filtering reduces the dataset to 100 hours (1.4%) with similar performance; a
similar trend is observed on Fisher English.

</details>


### [46] [Robust Preference Optimization via Dynamic Target Margins](https://arxiv.org/abs/2506.03690)
*Jie Sun,Junkang Wu,Jiancan Wu,Zhibo Zhu,Xingyu Lu,Jun Zhou,Lintao Ma,Xiang Wang*

Main category: cs.CL

TL;DR: 提出了一种名为γ-PO的动态目标间隔偏好优化算法，通过调整奖励间隔来提升大语言模型的对齐效果，有效抑制噪声并优先处理高置信度数据对。


<details>
  <summary>Details</summary>
Motivation: 直接偏好优化（DPO）的效果高度依赖数据质量，而实际数据常受噪声影响。为提高模型对齐的鲁棒性，需要一种能动态调整奖励间隔的方法。

Method: γ-PO是一种基于动态目标间隔的偏好优化算法，通过实例特定的间隔校准，优先处理高置信度数据对，同时抑制模糊数据对的噪声。该方法兼容依赖奖励间隔的DPO变体，且代码改动极小。

Result: 在AlpacaEval2和Arena-Hard等基准测试中，γ-PO平均性能提升4.4%，创下新的最先进性能记录，且对训练效率影响可忽略。

Conclusion: γ-PO是一种高效、易实现的解决方案，能显著提升大语言模型的对齐效果，适用于实际应用场景。

Abstract: The alignment of Large Language Models (LLMs) is crucial for ensuring their
safety and reliability in practical applications. Direct Preference
Optimization (DPO) has emerged as an efficient method that directly optimizes
models using preference pairs, significantly reducing resource demands.
However, the effectiveness of DPO heavily depends on the data quality, which is
frequently compromised by noise. In this work, we propose $\gamma$-PO, a
dynamic target margin preference optimization algorithm that adjust reward
margins at the pairwise level. By introducing instance-specific margin
calibration, $\gamma$-PO strategically prioritizes high-confidence pairs (those
demonstrating higher reward margins) while suppressing potential noise from
ambiguous pairs. Moreover, $\gamma$-PO is a plug-and-play method, compatible
with variants of DPO that rely on reward margin between preference pairs.
Across benchmarks such as AlpacaEval2 and Arena-Hard, $\gamma$-PO achieves an
average 4.4\% improvement over other baselines, setting new benchmarks for
state-of-the-art performance. Additionally, $\gamma$-PO requires minimal code
changes and has a negligible impact on training efficiency, making it a robust
solution for enhancing LLMs alignment. Our codes are available at
\href{https://github.com/sunjie279/gammaPO}{https://github.com/sunjie279/gammaPO}.

</details>


### [47] [AdaDecode: Accelerating LLM Decoding with Adaptive Layer Parallelism](https://arxiv.org/abs/2506.03700)
*Zhepei Wei,Wei-Lin Chen,Xinyu Zhu,Yu Meng*

Main category: cs.CL

TL;DR: AdaDecode通过自适应在中间层生成高置信度token并延迟剩余计算，实现LLM解码加速1.73倍，且保证输出一致性。


<details>
  <summary>Details</summary>
Motivation: 现有自回归解码方法因顺序生成token导致硬件并行利用率低，而推测解码和层跳过等方法存在辅助模型依赖或输出不一致问题。

Method: AdaDecode利用中间层高置信度预测提前生成token，并行执行剩余层计算，最后通过验证步骤确保输出一致性。

Result: 实验显示AdaDecode在多种生成任务中解码吞吐量提升最高达1.73倍，且输出与标准自回归解码完全一致。

Conclusion: AdaDecode无需辅助模型或修改参数，通过自适应分层解码显著提升效率，为长文本生成提供了高效解决方案。

Abstract: Large language models (LLMs) are increasingly used for long-content
generation (e.g., long Chain-of-Thought reasoning) where decoding efficiency
becomes a critical bottleneck: Autoregressive decoding is inherently limited by
its sequential token generation process, where each token must be generated
before the next can be processed. This sequential dependency restricts the
ability to fully leverage modern hardware's parallel processing capabilities.
Existing methods like speculative decoding and layer skipping offer potential
speedups but have notable drawbacks: speculative decoding relies on an
auxiliary "drafter" model, which can be challenging to acquire and increases
memory overhead, while layer skipping may introduce discrepancies in the
outputs due to the missing key-value cache at skipped layers. In this work, we
propose AdaDecode, which accelerates LLM decoding without requiring auxiliary
models or changes to the original model parameters, while ensuring output
consistency. AdaDecode leverages the insight that many tokens can accurately be
generated at intermediate layers, as further layers often do not significantly
alter predictions once the model reaches a certain confidence. By adaptively
generating tokens at intermediate layers when confidence is high, AdaDecode
enables the next token's computation to begin immediately. The remaining layer
computations for early-predicted tokens are deferred and executed in parallel
with subsequent tokens when needed, maximizing hardware utilization and
reducing decoding latency. A final verification step ensures that early
predictions match the results of standard autoregressive decoding, preserving
output parity. Experiments across diverse generation tasks shows that AdaDecode
consistently achieves superior decoding throughput with up to 1.73x speedup,
while guaranteeing output parity with standard autoregressive decoding.

</details>


### [48] [ScoreRAG: A Retrieval-Augmented Generation Framework with Consistency-Relevance Scoring and Structured Summarization for News Generation](https://arxiv.org/abs/2506.03704)
*Pei-Yun Lin,Yen-lung Tsai*

Main category: cs.CL

TL;DR: ScoreRAG通过多阶段框架提升新闻生成质量，结合检索增强生成、一致性评估和结构化摘要，显著提高生成新闻的准确性、连贯性和专业性。


<details>
  <summary>Details</summary>
Motivation: 当前新闻生成方法存在幻觉、事实不一致和缺乏领域专业知识的问题，ScoreRAG旨在解决这些挑战。

Method: ScoreRAG通过检索相关新闻文档、评估一致性相关性、重新排序并过滤低质量内容，生成分级摘要以指导新闻文章生成。

Result: ScoreRAG显著提升了生成新闻的准确性、连贯性、信息量和专业性，同时保持了生成过程的稳定性和一致性。

Conclusion: ScoreRAG通过系统化方法有效提升了自动化新闻生成的质量，符合专业新闻标准。

Abstract: This research introduces ScoreRAG, an approach to enhance the quality of
automated news generation. Despite advancements in Natural Language Processing
and large language models, current news generation methods often struggle with
hallucinations, factual inconsistencies, and lack of domain-specific expertise
when producing news articles. ScoreRAG addresses these challenges through a
multi-stage framework combining retrieval-augmented generation, consistency
relevance evaluation, and structured summarization. The system first retrieves
relevant news documents from a vector database, maps them to complete news
items, and assigns consistency relevance scores based on large language model
evaluations. These documents are then reranked according to relevance, with
low-quality items filtered out. The framework proceeds to generate graded
summaries based on relevance scores, which guide the large language model in
producing complete news articles following professional journalistic standards.
Through this methodical approach, ScoreRAG aims to significantly improve the
accuracy, coherence, informativeness, and professionalism of generated news
articles while maintaining stability and consistency throughout the generation
process. The code and demo are available at:
https://github.com/peiyun2260/ScoreRAG.

</details>


### [49] [MFLA: Monotonic Finite Look-ahead Attention for Streaming Speech Recognition](https://arxiv.org/abs/2506.03722)
*Yinfeng Xia,Huiyan Li,Chenyang Le,Manhong Wang,Yutao Sun,Xingyang Ma,Yanmin Qian*

Main category: cs.CL

TL;DR: 该论文提出了一种基于Whisper模型的流式语音识别训练框架，通过引入连续积分触发机制和单调有限前瞻注意力机制，实现了低延迟与高质量的平衡。


<details>
  <summary>Details</summary>
Motivation: 尽管大型预训练语音模型（如Whisper）在降低语音任务训练成本方面表现出潜力，但将其整合到流式系统中仍具挑战性。论文旨在解决这一问题。

Method: 采用前缀到前缀的训练框架，结合连续积分触发机制建立语音序列与文本标记的准单调对齐，并设计单调有限前瞻注意力机制，使每个标记能关注无限左上下文和有限右上下文。同时使用wait-k解码策略简化解码过程。

Result: 理论分析和实验表明，该方法在延迟和质量之间实现了可控的权衡，适用于多种流式应用场景。

Conclusion: 通过提出的框架和机制，论文成功将Whisper模型应用于流式识别，实现了低延迟与高质量的平衡，为流式语音识别提供了有效解决方案。

Abstract: Applying large pre-trained speech models like Whisper has shown promise in
reducing training costs for various speech tasks. However, integrating these
models into streaming systems remains a challenge. This paper presents a novel
prefix-to-prefix training framework for streaming recognition by fine-tuning
the Whisper. We introduce the Continuous Integrate-and-Fire mechanism to
establish a quasi-monotonic alignment between continuous speech sequences and
discrete text tokens. Additionally, we design Monotonic Finite Look-ahead
Attention, allowing each token to attend to infinite left-context and finite
right-context from the speech sequences. We also employ the wait-k decoding
strategy to simplify the decoding process while ensuring consistency between
training and testing. Our theoretical analysis and experiments demonstrate that
this approach achieves a controllable trade-off between latency and quality,
making it suitable for various streaming applications.

</details>


### [50] [Verbalized Confidence Triggers Self-Verification: Emergent Behavior Without Explicit Reasoning Supervision](https://arxiv.org/abs/2506.03723)
*Chaeyun Jang,Moonseok Choi,Yegon Kim,Hyungi Lee,Juho Lee*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型在链式思维推理中的不确定性校准问题，提出了一种仅需标量置信度标签的监督微调方法，无需显式推理监督或强化学习奖励，即可使模型学会自我验证行为，并在多个推理任务中提升了校准性和准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的不确定性校准对于安全部署至关重要，尤其是在用户依赖其口头化置信度估计时。然而，现有研究主要集中在分类器或短文本生成上，链式思维（CoT）推理中的置信度校准尚未得到充分探索。

Method: 论文提出了一种简单的监督微调方法，仅使用标量置信度标签即可引导语言模型表现出自我验证行为。此外，还提出了一种基于校准不确定性的重新思考方法，通过测试时缩放提升性能。

Result: 在GSM8K、MATH-500和ARC-Challenge等推理任务上的实验表明，该方法不仅提高了模型的校准性和准确性，还通过使模型的推理路径与其置信度对齐，增强了可解释性。

Conclusion: 研究表明，仅通过标量置信度标签的监督微调即可有效引导语言模型的自我验证行为，无需复杂的推理监督或强化学习奖励，为LLMs的不确定性校准提供了一种简单而有效的方法。

Abstract: Uncertainty calibration is essential for the safe deployment of large
language models (LLMs), particularly when users rely on verbalized confidence
estimates. While prior work has focused on classifiers or short-form
generation, confidence calibration for chain-of-thought (CoT) reasoning remains
largely unexplored. Surprisingly, we find that supervised fine-tuning with
scalar confidence labels alone suffices to elicit self-verification behavior of
language models, without any explicit reasoning supervision or reinforcement
learning-based rewards. Despite being trained only to produce a verbalized
confidence score without any self-verifying examples, the model learns to
generate longer and self-checking responses for low-confidence queries while
providing more concise answers for high-confidence ones. We further propose a
simple rethinking method that boosts performance via test-time scaling based on
calibrated uncertainty. Experiments on GSM8K and held-out reasoning tasks such
as MATH-500 and ARC-Challenge show that our confidence-aware fine-tuning
improves both calibration and accuracy, while also enhancing interpretability
by aligning the model's reasoning path with its confidence.

</details>


### [51] [Generating Pedagogically Meaningful Visuals for Math Word Problems: A New Benchmark and Analysis of Text-to-Image Models](https://arxiv.org/abs/2506.03735)
*Junling Wang,Anna Rutkiewicz,April Yi Wang,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: Math2Visual框架自动生成数学应用题的教学可视化图表，通过微调文本到图像模型提升教育可视化生成质量。


<details>
  <summary>Details</summary>
Motivation: 数学应用题的教学可视化图表能帮助年轻学习者理解文本描述并转化为数学表达式，但手动创建这些图表耗时且缺乏自动化工具支持。

Method: Math2Visual利用预定义的可视化语言和基于数学教师访谈的设计空间，自动从数学应用题文本生成教学意义明确的可视化图表。

Result: 构建了包含1,903个标注可视化图表的数据集，微调后的文本到图像模型在教育可视化生成方面表现更优。

Conclusion: 该研究为自动化生成教学意义明确的可视化图表设立了新基准，并揭示了生成多模态教育内容中的关键挑战。

Abstract: Visuals are valuable tools for teaching math word problems (MWPs), helping
young learners interpret textual descriptions into mathematical expressions
before solving them. However, creating such visuals is labor-intensive and
there is a lack of automated methods to support this process. In this paper, we
present Math2Visual, an automatic framework for generating pedagogically
meaningful visuals from MWP text descriptions. Math2Visual leverages a
pre-defined visual language and a design space grounded in interviews with math
teachers, to illustrate the core mathematical relationships in MWPs. Using
Math2Visual, we construct an annotated dataset of 1,903 visuals and evaluate
Text-to-Image (TTI) models for their ability to generate visuals that align
with our design. We further fine-tune several TTI models with our dataset,
demonstrating improvements in educational visual generation. Our work
establishes a new benchmark for automated generation of pedagogically
meaningful visuals and offers insights into key challenges in producing
multimodal educational content, such as the misrepresentation of mathematical
relationships and the omission of essential visual elements.

</details>


### [52] [Act-as-Pet: Benchmarking the Abilities of Large Language Models as E-Pets in Social Network Services](https://arxiv.org/abs/2506.03761)
*Hongcheng Guo,Zheyong Xie,Shaosheng Cao,Boyang Wang,Weiting Liu,Zheyu Ye,Zhoujun Li,Zuozhu Liu*

Main category: cs.CL

TL;DR: 论文提出Pet-Bench基准测试，用于评估大语言模型在虚拟宠物陪伴应用中的表现，涵盖自主互动和人类互动维度，测试了28个模型并发现性能差异显著。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在交互和情感丰富体验中的应用增加，虚拟宠物陪伴成为一个未被充分探索的领域。现有方法缺乏系统性评估模型在全面陪伴能力上的表现。

Method: 引入Pet-Bench基准测试，包含自主进化、互动参与等任务，设计了7500多个互动实例来模拟复杂宠物行为，评估模型在智能调度、记忆对话和心理对话等方面的能力。

Result: 对28个大语言模型的评估显示，模型性能和规模及内在能力相关，表明该领域需要专门的优化。

Conclusion: Pet-Bench为评估宠物相关大语言模型能力提供了基础资源，并推动情感沉浸式人宠互动的发展。

Abstract: As interest in using Large Language Models (LLMs) for interactive and
emotionally rich experiences grows, virtual pet companionship emerges as a
novel yet underexplored application. Existing approaches focus on basic pet
role-playing interactions without systematically benchmarking LLMs for
comprehensive companionship. In this paper, we introduce Pet-Bench, a dedicated
benchmark that evaluates LLMs across both self-interaction and
human-interaction dimensions. Unlike prior work, Pet-Bench emphasizes
self-evolution and developmental behaviors alongside interactive engagement,
offering a more realistic reflection of pet companionship. It features diverse
tasks such as intelligent scheduling, memory-based dialogues, and psychological
conversations, with over 7,500 interaction instances designed to simulate
complex pet behaviors. Evaluation of 28 LLMs reveals significant performance
variations linked to model size and inherent capabilities, underscoring the
need for specialized optimization in this domain. Pet-Bench serves as a
foundational resource for benchmarking pet-related LLM abilities and advancing
emotionally immersive human-pet interactions.

</details>


### [53] [AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models](https://arxiv.org/abs/2506.03762)
*Yifeng Gu,Zicong Jiang,Jianxiu Jin,Kailing Guo,Ziyang Zhang,Xiangmin Xu*

Main category: cs.CL

TL;DR: 论文提出AhaKV方法，通过自适应调整softmax规模来减少KV缓存中的注意力分数偏差，提升大语言模型全局上下文处理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过累积注意力分数淘汰KV缓存中的不重要token，但该分数存在位置偏差（初始位置分数更高），导致模型无法充分利用全局上下文信息。

Method: 提出AhaKV：1) 根据注意力分数信息熵的期望自适应调整softmax规模；2) 利用先前研究忽略的value向量信息优化自适应分数。

Result: 理论证明可有效减少偏差，实验显示在固定缓存预算下，AhaKV能保留全局关键token，并在多个基准任务上达到SOTA效果。

Conclusion: AhaKV通过解决注意力分数偏差问题，显著提升了LLMs在资源受限场景下的推理性能。

Abstract: Large Language Models (LLMs) have significantly advanced the field of
Artificial Intelligence. However, their deployment is resource-intensive, not
only due to the large number of model parameters but also because the
(Key-Value) KV cache consumes a lot of memory during inference. While several
works propose reducing the KV cache by evicting the unnecessary tokens, these
approaches rely on accumulated attention score as eviction score to quantify
the importance of the token. We identify the accumulated attention score is
biased and it decreases with the position of the tokens in the mathematical
expectation. As a result, the retained tokens concentrate on the initial
positions, limiting model's access to global contextual information. To address
this issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the
bias of the accumulated attention score by adaptively tuning the scale of
softmax according the expectation of information entropy of attention scores.
To make use of the holistic attention information in self-attention mechanism,
AhaKV utilize the information of value vectors, which is overlooked in previous
works, to refine the adaptive score. We show theoretically that our method is
well suited for bias reduction. We deployed AhaKV on different models with a
fixed cache budget. Experiments show that AhaKV successfully mitigates bias and
retains crucial tokens across global context and achieve state-of-the-art
results against other related work on several benchmark tasks.

</details>


### [54] [ClozeMath: Improving Mathematical Reasoning in Language Models by Learning to Fill Equations](https://arxiv.org/abs/2506.03763)
*Quang Hieu Pham,Thuy Duong Nguyen,Tung Pham,Anh Tuan Luu,Dat Quoc Nguyen*

Main category: cs.CL

TL;DR: 论文提出ClozeMath方法，通过填空任务提升大语言模型在数学推理上的性能，实验表明其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统的大语言模型通过预测下一个词训练，可能无法完全模拟人类学习思维的过程。受人类数学推理泛化能力的启发，作者提出新方法以更好地训练模型。

Method: 提出ClozeMath方法，采用填空任务（预测被遮蔽的方程）来微调大语言模型，类似于人类学习中的完形填空练习。

Result: 在GSM8K、MATH和GSM-Symbolic数据集上的实验显示，ClozeMath在性能和鲁棒性上均优于基线方法Masked Thought，并测试了两种解码算法。

Conclusion: ClozeMath通过填空任务有效提升数学推理能力，消融实验分析了不同架构和实现选择的影响。

Abstract: The capabilities of large language models (LLMs) have been enhanced by
training on data that reflects human thought processes, such as the
Chain-of-Thought format. However, evidence suggests that the conventional
scheme of next-word prediction may not fully capture how humans learn to think.
Inspired by how humans generalize mathematical reasoning, we propose a new
approach named ClozeMath to fine-tune LLMs for mathematical reasoning. Our
ClozeMath involves a text-infilling task that predicts masked equations from a
given solution, analogous to cloze exercises used in human learning.
Experiments on GSM8K, MATH, and GSM-Symbolic show that ClozeMath surpasses the
strong baseline Masked Thought in performance and robustness, with two
test-time scaling decoding algorithms, Beam Search and Chain-of-Thought
decoding. Additionally, we conduct an ablation study to analyze the effects of
various architectural and implementation choices on our approach.

</details>


### [55] [Unifying Uniform and Binary-coding Quantization for Accurate Compression of Large Language Models](https://arxiv.org/abs/2506.03781)
*Seungcheol Park,Jeongin Bae,Beomseok Kwon,Minjun Kim,Byeongwook Kim,Se Jung Kwon,U Kang,Dongsoo Lee*

Main category: cs.CL

TL;DR: UniQuanF是一种新型量化方法，结合了BCQ和UQ的优势，通过灵活映射技术提升大语言模型量化精度，且不增加部署成本。


<details>
  <summary>Details</summary>
Motivation: 当前量化方法如BCQ和UQ各有优势（BCQ表达力强，UQ易优化），但均无法兼顾两者。本文旨在开发一种能同时利用两者优势的量化方法。

Method: 提出UniQuanF方法，统一UQ的灵活映射技术和BCQ的非均匀量化级别，采用统一初始化及局部/周期性映射技术优化参数。

Result: 实验表明UniQuanF优于现有方法，在GSM8K基准测试中准确率最高提升4.60%。

Conclusion: UniQuanF成功统一了BCQ和UQ的优势，实现了高精度量化且无额外部署开销，为LLM高效部署提供了新方案。

Abstract: How can we quantize large language models while preserving accuracy?
Quantization is essential for deploying large language models (LLMs)
efficiently. Binary-coding quantization (BCQ) and uniform quantization (UQ) are
promising quantization schemes that have strong expressiveness and
optimizability, respectively. However, neither scheme leverages both
advantages. In this paper, we propose UniQuanF (Unified Quantization with
Flexible Mapping), an accurate quantization method for LLMs. UniQuanF harnesses
both strong expressiveness and optimizability by unifying the flexible mapping
technique in UQ and non-uniform quantization levels of BCQ. We propose unified
initialization, and local and periodic mapping techniques to optimize the
parameters in UniQuanF precisely. After optimization, our unification theorem
removes computational and memory overhead, allowing us to utilize the superior
accuracy of UniQuanF without extra deployment costs induced by the unification.
Experimental results demonstrate that UniQuanF outperforms existing UQ and BCQ
methods, achieving up to 4.60% higher accuracy on GSM8K benchmark.

</details>


### [56] [Knockout LLM Assessment: Using Large Language Models for Evaluations through Iterative Pairwise Comparisons](https://arxiv.org/abs/2506.03785)
*Isik Baran Sandan,Tu Anh Dinh,Jan Niehues*

Main category: cs.CL

TL;DR: 提出Knockout Assessment方法，通过淘汰赛机制改进LLM评估的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估方法依赖单次或单轮比较，缺乏全局排名视角，需改进评估准确性。

Method: 采用淘汰赛系统进行迭代成对比较，提升LLM评估的全局视角。

Result: 实验显示，该方法在考试评分和机器翻译评估中与专家评分的相关性平均提高0.07。

Conclusion: Knockout Assessment能更精准对齐人类评分，提升LLM评估质量。

Abstract: Large Language Models (LLMs) have shown to be effective evaluators across
various domains such as machine translations or the scientific domain. Current
LLM-as-a-Judge approaches rely mostly on individual assessments or a single
round of pairwise assessments, preventing the judge LLM from developing a
global ranking perspective. To address this, we present Knockout Assessment, an
LLM-asa Judge method using a knockout tournament system with iterative pairwise
comparisons. Experiments across three LLMs on two datasets show that knockout
assessment improves scoring accuracy, increasing Pearson correlation with
expert evaluations by 0.07 on average for university-level exam scoring and
machine translation evaluations, aligning LLM assessments more closely with
human scoring.

</details>


### [57] [Mark My Words: A Robust Multilingual Model for Punctuation in Text and Speech Transcripts](https://arxiv.org/abs/2506.03793)
*Sidharth Pulipaka,Sparsh Jain,Ashwin Sankar,Raj Dabre*

Main category: cs.CL

TL;DR: Cadence是一个基于预训练大语言模型的通用标点恢复模型，支持多种语言，尤其在处理自发语音转录时表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前模型在自发语音转录中标点恢复效果不佳，影响了翻译、语音合成等下游任务的质量。

Method: 使用预训练大语言模型构建Cadence，支持多种语言，包括22种印度语言和英语。

Result: Cadence在性能上超越现有技术，但在领域迁移和罕见标点处理上仍有挑战。

Conclusion: 预训练语言模型在多语言标点恢复中有效，Cadence为低资源NLP流程提供了实用价值。

Abstract: Punctuation plays a vital role in structuring meaning, yet current models
often struggle to restore it accurately in transcripts of spontaneous speech,
especially in the presence of disfluencies such as false starts and
backtracking. These limitations hinder the performance of downstream tasks like
translation, text to speech, summarization, etc. where sentence boundaries are
critical for preserving quality. In this work, we introduce Cadence, a
generalist punctuation restoration model adapted from a pretrained large
language model. Cadence is designed to handle both clean written text and
highly spontaneous spoken transcripts. It surpasses the previous state of the
art in performance while expanding support from 14 to all 22 Indian languages
and English. We conduct a comprehensive analysis of model behavior across
punctuation types and language families, identifying persistent challenges
under domain shift and with rare punctuation marks. Our findings demonstrate
the efficacy of utilizing pretrained language models for multilingual
punctuation restoration and highlight Cadence practical value for low resource
NLP pipelines at scale.

</details>


### [58] [Automatic Correction of Writing Anomalies in Hausa Texts](https://arxiv.org/abs/2506.03820)
*Ahmad Mustapha Wali,Sergiu Nisioi*

Main category: cs.CL

TL;DR: 该论文提出了一种基于Transformer模型的自动校正豪萨语文本异常的方法，通过构建大规模平行数据集并微调多语言模型，显著提升了文本质量评估指标。


<details>
  <summary>Details</summary>
Motivation: 豪萨语文本常存在书写异常（如字符替换错误和空格错误），阻碍了自然语言处理应用的发展。本研究旨在通过自动校正提升文本质量，推动豪萨语NLP进步。

Method: 收集公开语料构建45万句噪声-干净平行数据集，人工模拟真实错误；采用M2M100、AfriTEVA等多语言模型进行微调，使用SentencePiece分词技术。

Result: 实验显示F1、BLEU和METEOR分数显著提升，字符错误率(CER)和词错误率(WER)明显下降，模型性能优于基线。

Conclusion: 研究提供了豪萨语文本校正的完整方法论、公开数据集和高效模型，不仅推动了豪萨语NLP发展，其方法也可迁移至其他低资源语言。

Abstract: Hausa texts are often characterized by writing anomalies such as incorrect
character substitutions and spacing errors, which sometimes hinder natural
language processing (NLP) applications. This paper presents an approach to
automatically correct the anomalies by finetuning transformer-based models.
Using a corpus gathered from several public sources, we created a large-scale
parallel dataset of over 450,000 noisy-clean Hausa sentence pairs by
introducing synthetically generated noise, fine-tuned to mimic realistic
writing errors. Moreover, we adapted several multilingual and African
language-focused models, including M2M100, AfriTEVA, mBART, and Opus-MT
variants for this correction task using SentencePiece tokenization. Our
experimental results demonstrate significant increases in F1, BLEU and METEOR
scores, as well as reductions in Character Error Rate (CER) and Word Error Rate
(WER). This research provides a robust methodology, a publicly available
dataset, and effective models to improve Hausa text quality, thereby advancing
NLP capabilities for the language and offering transferable insights for other
low-resource languages.

</details>


### [59] [CRAWLDoc: A Dataset for Robust Ranking of Bibliographic Documents](https://arxiv.org/abs/2506.03822)
*Fabian Karl,Ansgar Scherp*

Main category: cs.CL

TL;DR: CRAWLDoc提出了一种新的上下文排名方法，用于从多样化的网页布局和数据格式中提取元数据，提高了出版物数据库的准确性。


<details>
  <summary>Details</summary>
Motivation: 出版物数据库依赖从多样化的网页来源准确提取元数据，但网页布局和数据格式的差异给元数据提供商带来了挑战。

Method: CRAWLDoc通过检索出版物的URL（如数字对象标识符），获取着陆页和所有链接的网页资源（包括PDF、ORCID资料和补充材料），并将这些资源与锚文本和URL嵌入到统一的表示中。

Result: CRAWLDoc在来自计算机科学领域六大顶级出版商的600篇出版物数据集上表现出色，能够跨出版商和数据格式实现稳健且独立于布局的相关文档排名。

Conclusion: CRAWLDoc为从具有各种布局和格式的网页文档中改进元数据提取奠定了基础。

Abstract: Publication databases rely on accurate metadata extraction from diverse web
sources, yet variations in web layouts and data formats present challenges for
metadata providers. This paper introduces CRAWLDoc, a new method for contextual
ranking of linked web documents. Starting with a publication's URL, such as a
digital object identifier, CRAWLDoc retrieves the landing page and all linked
web resources, including PDFs, ORCID profiles, and supplementary materials. It
embeds these resources, along with anchor texts and the URLs, into a unified
representation. For evaluating CRAWLDoc, we have created a new, manually
labeled dataset of 600 publications from six top publishers in computer
science. Our method CRAWLDoc demonstrates a robust and layout-independent
ranking of relevant documents across publishers and data formats. It lays the
foundation for improved metadata extraction from web documents with various
layouts and formats. Our source code and dataset can be accessed at
https://github.com/FKarl/CRAWLDoc.

</details>


### [60] [Multi-objective Aligned Bidword Generation Model for E-commerce Search Advertising](https://arxiv.org/abs/2506.03827)
*Zhenhui Liu,Chunyuan Yuan,Ming Pang,Zheng Fang,Li Yuan,Xue Jiang,Changping Peng,Zhangang Lin,Zheng Luo,Jingping Shao*

Main category: cs.CL

TL;DR: 提出多目标对齐竞价词生成模型(MoBGM)，通过判别器、生成器和偏好对齐模块优化查询改写，提升广告召回效果和平台收益。


<details>
  <summary>Details</summary>
Motivation: 现有查询改写方法难以同时保证用户查询相关性、改写真实性及广告收益最大化，导致长尾查询匹配不足，影响用户体验和搜索效率。

Method: 设计包含判别器、生成器和偏好对齐模块的MoBGM模型，利用判别器反馈信号训练多目标对齐的竞价词生成器。

Result: 离线和在线实验表明，该算法显著优于现有技术，部署后为平台创造巨大商业价值。

Conclusion: MoBGM通过多目标优化有效解决了查询改写中的关键挑战，验证了其可行性和鲁棒性。

Abstract: Retrieval systems primarily address the challenge of matching user queries
with the most relevant advertisements, playing a crucial role in e-commerce
search advertising. The diversity of user needs and expressions often produces
massive long-tail queries that cannot be matched with merchant bidwords or
product titles, which results in some advertisements not being recalled,
ultimately harming user experience and search efficiency. Existing query
rewriting research focuses on various methods such as query log mining,
query-bidword vector matching, or generation-based rewriting. However, these
methods often fail to simultaneously optimize the relevance and authenticity of
the user's original query and rewrite and maximize the revenue potential of
recalled ads.
  In this paper, we propose a Multi-objective aligned Bidword Generation Model
(MoBGM), which is composed of a discriminator, generator, and preference
alignment module, to address these challenges. To simultaneously improve the
relevance and authenticity of the query and rewrite and maximize the platform
revenue, we design a discriminator to optimize these key objectives. Using the
feedback signal of the discriminator, we train a multi-objective aligned
bidword generator that aims to maximize the combined effect of the three
objectives. Extensive offline and online experiments show that our proposed
algorithm significantly outperforms the state of the art. After deployment, the
algorithm has created huge commercial value for the platform, further verifying
its feasibility and robustness.

</details>


### [61] [Brain-tuned Speech Models Better Reflect Speech Processing Stages in the Brain](https://arxiv.org/abs/2506.03832)
*Omer Moussa,Mariya Toneva*

Main category: cs.CL

TL;DR: 脑调优的自监督语音模型在层次化语音处理上优于预训练模型，更接近人脑处理机制。


<details>
  <summary>Details</summary>
Motivation: 现有预训练语音模型未能反映人脑语音处理的层次性，尤其在语义编码方面存在不足。

Method: 通过脑调优（使用人脑记录数据微调模型）改进语音模型的语义理解能力。

Result: 脑调优模型的后期层显著提升与语义语言区域的匹配度，形成从声学到语义的清晰层次处理。

Conclusion: 脑调优模型不仅性能更优，且层次化处理更接近人脑，是研究语音处理的理想模型。

Abstract: Pretrained self-supervised speech models excel in speech tasks but do not
reflect the hierarchy of human speech processing, as they encode rich semantics
in middle layers and poor semantics in late layers. Recent work showed that
brain-tuning (fine-tuning models using human brain recordings) improves speech
models' semantic understanding. Here, we examine how well brain-tuned models
further reflect the brain's intermediate stages of speech processing. We find
that late layers of brain-tuned models substantially improve over pretrained
models in their alignment with semantic language regions. Further layer-wise
probing reveals that early layers remain dedicated to low-level acoustic
features, while late layers become the best at complex high-level tasks. These
findings show that brain-tuned models not only perform better but also exhibit
a well-defined hierarchical processing going from acoustic to semantic
representations, making them better model organisms for human speech
processing.

</details>


### [62] [PulseReddit: A Novel Reddit Dataset for Benchmarking MAS in High-Frequency Cryptocurrency Trading](https://arxiv.org/abs/2506.03861)
*Qiuhan Han,Qian Wang,Atsushi Yoshikawa,Masayuki Yamamura*

Main category: cs.CL

TL;DR: 该论文提出了PulseReddit数据集，首次将Reddit讨论数据与高频加密货币市场统计对齐，用于短期交易分析。通过基于大型语言模型的多智能体系统，研究发现结合社交情绪数据能提升交易表现，尤其在牛市，并揭示了不同模型在效率与性能间的权衡。


<details>
  <summary>Details</summary>
Motivation: 高频交易在加密货币市场中至关重要，而社交媒体如Reddit提供了有价值但尚未充分探索的信息源。研究旨在探索社交情绪对高频短期交易的影响。

Method: 使用基于大型语言模型的多智能体系统，结合新构建的PulseReddit数据集进行实证研究，分析社交情绪对交易表现的影响。

Result: 实验表明，结合PulseReddit数据的多智能体系统优于传统基线方法，特别是在牛市，且在不同市场环境下表现出强适应性。同时，研究揭示了不同大型语言模型在效率与性能间的权衡。

Conclusion: PulseReddit数据集和研究成果为高频交易中的多智能体系统研究奠定了基础，证明了整合社交媒体数据的实际益处，并为实际模型选择提供了重要参考。

Abstract: High-Frequency Trading (HFT) is pivotal in cryptocurrency markets, demanding
rapid decision-making. Social media platforms like Reddit offer valuable, yet
underexplored, information for such high-frequency, short-term trading. This
paper introduces \textbf{PulseReddit}, a novel dataset that is the first to
align large-scale Reddit discussion data with high-frequency cryptocurrency
market statistics for short-term trading analysis. We conduct an extensive
empirical study using Large Language Model (LLM)-based Multi-Agent Systems
(MAS) to investigate the impact of social sentiment from PulseReddit on trading
performance. Our experiments conclude that MAS augmented with PulseReddit data
achieve superior trading outcomes compared to traditional baselines,
particularly in bull markets, and demonstrate robust adaptability across
different market regimes. Furthermore, our research provides conclusive
insights into the performance-efficiency trade-offs of different LLMs,
detailing significant considerations for practical model selection in HFT
applications. PulseReddit and our findings establish a foundation for advanced
MAS research in HFT, demonstrating the tangible benefits of integrating social
media.

</details>


### [63] [EuroGEST: Investigating gender stereotypes in multilingual language models](https://arxiv.org/abs/2506.03867)
*Jacqueline Rowe,Mateusz Klimaszewski,Liane Guillou,Shannon Vallor,Alexandra Birch*

Main category: cs.CL

TL;DR: EuroGEST数据集用于评估多语言大模型中的性别刻板印象，发现模型普遍存在性别偏见，且更大模型偏见更强。


<details>
  <summary>Details</summary>
Motivation: 现有性别偏见基准主要针对英语，缺乏多语言评估工具，因此开发EuroGEST以填补这一空白。

Method: 基于专家标注的16种性别刻板印象，通过翻译工具、质量评估指标和形态学启发式方法扩展到29种欧洲语言。

Result: 所有模型在所有语言中最强的刻板印象是女性美丽、善解人意和整洁，男性是领导者、强壮、坚韧和专业。更大模型偏见更强，指令微调不能持续减少偏见。

Conclusion: 需要更多多语言的公平性研究，EuroGEST提供了跨语言性别偏见的可扩展评估方法和资源。

Abstract: Large language models increasingly support multiple languages, yet most
benchmarks for gender bias remain English-centric. We introduce EuroGEST, a
dataset designed to measure gender-stereotypical reasoning in LLMs across
English and 29 European languages. EuroGEST builds on an existing
expert-informed benchmark covering 16 gender stereotypes, expanded in this work
using translation tools, quality estimation metrics, and morphological
heuristics. Human evaluations confirm that our data generation method results
in high accuracy of both translations and gender labels across languages. We
use EuroGEST to evaluate 24 multilingual language models from six model
families, demonstrating that the strongest stereotypes in all models across all
languages are that women are \textit{beautiful,} \textit{empathetic} and
\textit{neat} and men are \textit{leaders}, \textit{strong, tough} and
\textit{professional}. We also show that larger models encode gendered
stereotypes more strongly and that instruction finetuning does not consistently
reduce gendered stereotypes. Our work highlights the need for more multilingual
studies of fairness in LLMs and offers scalable methods and resources to audit
gender bias across languages.

</details>


### [64] [RadialRouter: Structured Representation for Efficient and Robust Large Language Models Routing](https://arxiv.org/abs/2506.03880)
*Ruihan Jin,Pengpeng Shao,Zhengqi Wen,Jinyang Wu,Mingkuan Feng,Shuai Zhang,Jianhua Tao*

Main category: cs.CL

TL;DR: 论文提出了一种名为RadialRouter的新型大语言模型路由框架，通过RadialFormer结构优化查询与模型间的关系，显著提升了路由性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）路由方法因未能充分探索用户查询与模型特性之间的内在联系，导致效果受限。本文旨在解决这一问题。

Method: 采用基于轻量级Transformer的RadialFormer结构，结合KL散度和查询对比损失的优化目标函数，实现最优LLM选择。

Result: 在RouterBench测试中，RadialRouter在Balance和Cost First场景下分别比现有方法提升了9.2%和5.8%。

Conclusion: RadialRouter不仅性能优越，还具有对不同性能-成本权衡和动态LLM池的适应能力，展现出实际应用潜力。

Abstract: The rapid advancements in large language models (LLMs) have led to the
emergence of routing techniques, which aim to efficiently select the optimal
LLM from diverse candidates to tackle specific tasks, optimizing performance
while reducing costs. Current LLM routing methods are limited in effectiveness
due to insufficient exploration of the intrinsic connection between user
queries and the characteristics of LLMs. To address this issue, in this paper,
we present RadialRouter, a novel framework for LLM routing which employs a
lightweight Transformer-based backbone with a radial structure named
RadialFormer to articulate the query-LLMs relationship. The optimal LLM
selection is performed based on the final states of RadialFormer. The pipeline
is further refined by an objective function that combines Kullback-Leibler
divergence with the query-query contrastive loss to enhance robustness.
Experimental results on RouterBench show that RadialRouter significantly
outperforms existing routing methods by 9.2\% and 5.8\% in the Balance and Cost
First scenarios, respectively. Additionally, its adaptability toward different
performance-cost trade-offs and the dynamic LLM pool demonstrates practical
application potential.

</details>


### [65] [Kinship in Speech: Leveraging Linguistic Relatedness for Zero-Shot TTS in Indian Languages](https://arxiv.org/abs/2506.03884)
*Utkarsh Pathak,Chandra Sai Krishna Gunda,Anusha Prakash,Keshav Agarwal,Hema A. Murthy*

Main category: cs.CL

TL;DR: 该论文提出了一种零样本语音合成方法，通过共享音素表示和修改文本解析规则，快速适应印度多种低资源语言，实现了可理解和自然的语音合成。


<details>
  <summary>Details</summary>
Motivation: 印度有1369种语言，其中大多数缺乏数字资源，传统TTS系统需要高质量录音和准确转录，难以覆盖所有语言。论文旨在解决这一挑战，特别是针对来自不同语系的脚本和音系规则的语言。

Method: 通过增强共享音素表示并修改文本解析规则以匹配目标语言的音系规则，从而减少合成器开销并实现快速适应。

Result: 该方法成功为梵语、马哈拉施特拉语、卡纳拉孔卡尼语、迈蒂利语和库鲁克语生成了可理解和自然的语音，评估证实了其有效性。

Conclusion: 该方法有潜力扩展语音技术对资源匮乏语言的覆盖，为低资源语言提供可行的语音合成解决方案。

Abstract: Text-to-speech (TTS) systems typically require high-quality studio data and
accurate transcriptions for training. India has 1369 languages, with 22
official using 13 scripts. Training a TTS system for all these languages, most
of which have no digital resources, seems a Herculean task. Our work focuses on
zero-shot synthesis, particularly for languages whose scripts and phonotactics
come from different families. The novelty of our work is in the augmentation of
a shared phone representation and modifying the text parsing rules to match the
phonotactics of the target language, thus reducing the synthesiser overhead and
enabling rapid adaptation. Intelligible and natural speech was generated for
Sanskrit, Maharashtrian and Canara Konkani, Maithili and Kurukh by leveraging
linguistic connections across languages with suitable synthesisers. Evaluations
confirm the effectiveness of this approach, highlighting its potential to
expand speech technology access for under-represented languages.

</details>


### [66] [Pre$^3$: Enabling Deterministic Pushdown Automata for Faster Structured LLM Generation](https://arxiv.org/abs/2506.03887)
*Junyi Chen,Shihao Bai,Zaijun Wang,Siyu Wu,Chuheng Du,Hailong Yang,Ruihao Gong,Shengzhong Liu,Fan Wu,Guihai Chen*

Main category: cs.CL

TL;DR: 提出Pre$^3$方法，通过确定性下推自动机(DPDA)优化LLM解码效率，减少40%单token生成时间并提升36%吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有LR(1)语法解析方法存在运行时开销大、批量推理效率低的问题，需优化结构化生成效率。

Method: 1) 预处理阶段预计算前缀条件边实现并行转移；2) 将LR(1)转移图转化为DPDA以避免运行时路径探索。

Result: 实验显示单token生成时间(TPOT)降低40%，吞吐量提升36%。

Conclusion: Pre$^3$可无缝集成至LLM推理框架，显著提升结构化生成的效率。

Abstract: Extensive LLM applications demand efficient structured generations,
particularly for LR(1) grammars, to produce outputs in specified formats (e.g.,
JSON). Existing methods primarily parse LR(1) grammars into a pushdown
automaton (PDA), leading to runtime execution overhead for context-dependent
token processing, especially inefficient under large inference batches. To
address these issues, we propose Pre$^3$ that exploits deterministic pushdown
automata (DPDA) to optimize the constrained LLM decoding efficiency. First, by
precomputing prefix-conditioned edges during the preprocessing, Pre$^3$ enables
ahead-of-time edge analysis and thus makes parallel transition processing
possible. Second, by leveraging the prefix-conditioned edges, Pre$^3$
introduces a novel approach that transforms LR(1) transition graphs into DPDA,
eliminating the need for runtime path exploration and achieving edge
transitions with minimal overhead. Pre$^3$ can be seamlessly integrated into
standard LLM inference frameworks, reducing time per output token (TPOT) by up
to 40% and increasing throughput by up to 36% in our experiments. Our code is
available at https://github.com/ModelTC/lightllm.

</details>


### [67] [Magic Mushroom: A Customizable Benchmark for Fine-grained Analysis of Retrieval Noise Erosion in RAG Systems](https://arxiv.org/abs/2506.03901)
*Yuxin Zhang,Yan Wang,Yongrui Chen,Shenyu Zhang,Xinbang Dai,Sheng Bi,Guilin Qi*

Main category: cs.CL

TL;DR: 论文提出Magic Mushroom基准，用于模拟真实检索环境中的复杂噪声，评估RAG系统在噪声干扰下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法反映真实检索环境中的复杂噪声分布，导致RAG系统的鲁棒性评估不可靠。

Method: 基于语言特性和噪声特征定义四类检索噪声，构建包含单跳和多跳问题的Magic Mushroom基准，支持灵活配置噪声组合。

Result: 实验表明，不同规模的LLM生成器和经典去噪策略在噪声干扰下表现差异显著，均有较大改进空间。

Conclusion: Magic Mushroom是评估和提升RAG系统噪声鲁棒性的有效工具，可加速其实际应用部署。

Abstract: Retrieval-Augmented Generation (RAG) systems enhance Large Language Models
(LLMs) by incorporating external retrieved information, mitigating issues such
as hallucination and outdated knowledge.
  However, RAG systems are highly sensitive to retrieval noise prevalent in
real-world scenarios.
  Existing benchmarks fail to emulate the complex and heterogeneous noise
distributions encountered in real-world retrieval environments, undermining
reliable robustness assessment.
  In this paper, we define four categories of retrieval noise based on
linguistic properties and noise characteristics, aiming to reflect the
heterogeneity of noise in real-world scenarios.
  Building on this, we introduce Magic Mushroom, a benchmark for replicating
"magic mushroom" noise: contexts that appear relevant on the surface but
covertly mislead RAG systems.
  Magic Mushroom comprises 7,468 single-hop and 3,925 multi-hop question-answer
pairs.
  More importantly, Magic Mushroom enables researchers to flexibly configure
combinations of retrieval noise according to specific research objectives or
application scenarios, allowing for highly controlled evaluation setups.
  We evaluate LLM generators of varying parameter scales and classic RAG
denoising strategies under diverse noise distributions to investigate their
performance dynamics during progressive noise encroachment.
  Our analysis reveals that both generators and denoising strategies have
significant room for improvement and exhibit extreme sensitivity to noise
distributions.
  Magic Mushroom emerges as a promising tool for evaluating and advancing
noise-robust RAG systems, accelerating their widespread deployment in
real-world applications.
  The Magic Mushroom benchmark is available at the
https://drive.google.com/file/d/1aP5kyPuk4L-L_uoI6T9UhxuTyt8oMqjT/view?usp=sharing.

</details>


### [68] [The Harmonic Structure of Information Contours](https://arxiv.org/abs/2506.03902)
*Eleftheria Tsipidi,Samuel Kiegeland,Franz Nowak,Tianyang Xu,Ethan Wilcox,Alex Warstadt,Ryan Cotterell,Mario Giulianelli*

Main category: cs.CL

TL;DR: 该研究挑战了信息密度均匀分布假说，提出语言信息率存在周期性波动，并通过谐波回归和时间缩放方法在多语种文本中验证了这一现象。


<details>
  <summary>Details</summary>
Motivation: 传统理论认为说话者会均衡分配信息密度，但实际语言信息率存在波动。本研究探索这种波动是否受到隐性周期性语言压力的影响。

Method: 采用谐波回归和创新的时间缩放技术，分析六种语言文本中的信息率周期性模式。

Result: 在英语、西班牙语等六种语言中均发现信息率的周期性波动，且主要频率与语篇结构相关。

Conclusion: 信息率波动反映语言组织的内在周期性，该方法为揭示不同粒度语言结构压力提供了通用框架。

Abstract: The uniform information density (UID) hypothesis proposes that speakers aim
to distribute information evenly throughout a text, balancing production effort
and listener comprehension difficulty. However, language typically does not
maintain a strictly uniform information rate; instead, it fluctuates around a
global average. These fluctuations are often explained by factors such as
syntactic constraints, stylistic choices, or audience design. In this work, we
explore an alternative perspective: that these fluctuations may be influenced
by an implicit linguistic pressure towards periodicity, where the information
rate oscillates at regular intervals, potentially across multiple frequencies
simultaneously. We apply harmonic regression and introduce a novel extension
called time scaling to detect and test for such periodicity in information
contours. Analyzing texts in English, Spanish, German, Dutch, Basque, and
Brazilian Portuguese, we find consistent evidence of periodic patterns in
information rate. Many dominant frequencies align with discourse structure,
suggesting these oscillations reflect meaningful linguistic organization.
Beyond highlighting the connection between information rate and discourse
structure, our approach offers a general framework for uncovering structural
pressures at various levels of linguistic granularity.

</details>


### [69] [When Fairness Isn't Statistical: The Limits of Machine Learning in Evaluating Legal Reasoning](https://arxiv.org/abs/2506.03913)
*Claire Barale,Michael Rovatsos,Nehal Bhuta*

Main category: cs.CL

TL;DR: 论文评估了三种机器学习方法在难民案件裁决中的公平性分析效果，发现现有统计方法难以有效评估法律裁量领域的公平性。


<details>
  <summary>Details</summary>
Motivation: 法律决策日益依赖机器学习评估公平性，但在裁量权大、规范性复杂的领域（如难民案件），统计方法是否能有效评估公平性尚不明确。

Method: 基于59,000+加拿大难民案件数据，实证分析了特征分析、语义聚类和预测建模三种ML方法。

Result: 不同方法产生矛盾结果；预测模型依赖程序性特征而非法律特征；语义聚类无法捕捉法律推理实质。

Conclusion: 当前统计方法无法充分评估法律裁量领域的公平性，需结合法律推理和制度背景开发新方法。

Abstract: Legal decisions are increasingly evaluated for fairness, consistency, and
bias using machine learning (ML) techniques. In high-stakes domains like
refugee adjudication, such methods are often applied to detect disparities in
outcomes. Yet it remains unclear whether statistical methods can meaningfully
assess fairness in legal contexts shaped by discretion, normative complexity,
and limited ground truth.
  In this paper, we empirically evaluate three common ML approaches
(feature-based analysis, semantic clustering, and predictive modeling) on a
large, real-world dataset of 59,000+ Canadian refugee decisions (AsyLex). Our
experiments show that these methods produce divergent and sometimes
contradictory signals, that predictive modeling often depends on contextual and
procedural features rather than legal features, and that semantic clustering
fails to capture substantive legal reasoning.
  We show limitations of statistical fairness evaluation, challenge the
assumption that statistical regularity equates to fairness, and argue that
current computational approaches fall short of evaluating fairness in legally
discretionary domains. We argue that evaluating fairness in law requires
methods grounded not only in data, but in legal reasoning and institutional
context.

</details>


### [70] [Compositional Generalisation for Explainable Hate Speech Detection](https://arxiv.org/abs/2506.03916)
*Agostina Calabrese,Tom Sherborne,Björn Ross,Mirella Lapata*

Main category: cs.CL

TL;DR: 论文提出U-PLEAD数据集，通过均衡上下文表达频率提升仇恨言论检测模型的组合泛化能力，结合真实数据训练达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有仇恨言论检测模型因数据集偏差和句子级标签限制，难以泛化到训练数据之外的语境。细粒度标注（如目标词和贬义短语）仍受上下文干扰，导致模型对训练中未见的表达组合检测困难。

Method: 构建包含36.4万条合成帖子的U-PLEAD数据集，确保表达在所有上下文中均匀出现；新增8000条人工验证的基准测试集，用于评估组合泛化能力。结合真实数据训练模型。

Result: U-PLEAD与真实数据联合训练后，模型在组合泛化任务上表现提升，并在人类标注的PLEAD数据集上达到当前最优性能。

Conclusion: 通过控制表达出现的上下文分布并增强数据多样性，可有效改善仇恨言论检测模型的泛化能力。合成数据与真实数据的结合是提升模型鲁棒性的可行路径。

Abstract: Hate speech detection is key to online content moderation, but current models
struggle to generalise beyond their training data. This has been linked to
dataset biases and the use of sentence-level labels, which fail to teach models
the underlying structure of hate speech. In this work, we show that even when
models are trained with more fine-grained, span-level annotations (e.g.,
"artists" is labeled as target and "are parasites" as dehumanising comparison),
they struggle to disentangle the meaning of these labels from the surrounding
context. As a result, combinations of expressions that deviate from those seen
during training remain particularly difficult for models to detect. We
investigate whether training on a dataset where expressions occur with equal
frequency across all contexts can improve generalisation. To this end, we
create U-PLEAD, a dataset of ~364,000 synthetic posts, along with a novel
compositional generalisation benchmark of ~8,000 manually validated posts.
Training on a combination of U-PLEAD and real data improves compositional
generalisation while achieving state-of-the-art performance on the
human-sourced PLEAD.

</details>


### [71] [HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models](https://arxiv.org/abs/2506.03922)
*Zhaolu Kang,Junhao Gong,Jiaxu Yan,Wanke Xia,Yian Wang,Ziwen Wang,Huaxuan Ding,Zhuo Cheng,Wenhao Cao,Zhiyuan Feng,Siqi He,Shannan Yan,Junzhe Chen,Xiaomin He,Chaoya Jiang,Wei Ye,Kaidong Yu,Xuelong Li*

Main category: cs.CL

TL;DR: 该论文提出了HSSBench，一个专门用于评估多模态大语言模型在人文学科和社会科学任务中表现的新基准，填补了现有评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大语言模型评估基准主要关注STEM领域的知识和推理能力，而忽视了人文学科和社会科学（HSS）的特殊需求。HSS任务需要跨学科的横向思维和知识的深度整合，这对模型提出了独特挑战。

Method: 研究者开发了HSSBench，一个包含超过13,000个样本的基准，覆盖六个关键类别，并采用多领域专家与自动化代理协作的数据生成流程，确保样本质量。

Result: 对20多个主流MLLM的测试表明，HSSBench对现有最先进模型仍构成显著挑战，突显了模型在跨学科推理和知识整合方面的不足。

Conclusion: HSSBench的推出旨在激发更多研究，提升MLLM的跨学科推理能力，特别是在知识内化和跨领域连接方面的表现。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
potential to advance a broad range of domains. However, current benchmarks for
evaluating MLLMs primarily emphasize general knowledge and vertical
step-by-step reasoning typical of STEM disciplines, while overlooking the
distinct needs and potential of the Humanities and Social Sciences (HSS). Tasks
in the HSS domain require more horizontal, interdisciplinary thinking and a
deep integration of knowledge across related fields, which presents unique
challenges for MLLMs, particularly in linking abstract concepts with
corresponding visual representations. Addressing this gap, we present HSSBench,
a dedicated benchmark designed to assess the capabilities of MLLMs on HSS tasks
in multiple languages, including the six official languages of the United
Nations. We also introduce a novel data generation pipeline tailored for HSS
scenarios, in which multiple domain experts and automated agents collaborate to
generate and iteratively refine each sample. HSSBench contains over 13,000
meticulously designed samples, covering six key categories. We benchmark more
than 20 mainstream MLLMs on HSSBench and demonstrate that it poses significant
challenges even for state-of-the-art models. We hope that this benchmark will
inspire further research into enhancing the cross-disciplinary reasoning
abilities of MLLMs, especially their capacity to internalize and connect
knowledge across fields.

</details>


### [72] [More or Less Wrong: A Benchmark for Directional Bias in LLM Comparative Reasoning](https://arxiv.org/abs/2506.03923)
*Mohammadamin Shafiei,Hamidreza Saffari,Nafise Sadat Moosavi*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在数学比较问题中受词汇框架影响，存在系统性偏差，链式思维提示可减轻但无法完全消除偏差，且社会身份词汇会放大这种影响。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（LLMs）如何因输入措辞的语义线索而产生推理偏差，特别是在有客观事实基础的数学比较问题中，揭示模型对“更多”、“更少”或“等于”等词汇的系统性敏感度。

Method: 引入MathComp基准测试，包含300个比较场景，每个场景在三种LLM家族下通过14种提示变体进行评估，分析词汇框架和社会身份词汇对模型推理的影响。

Result: 发现模型错误常反映语言引导的偏差，链式思维提示可减少偏差但效果不一，自由形式推理更稳健，而社会身份词汇会放大方向性偏差。

Conclusion: 研究揭示了标准评估中的盲点，提出了需构建框架感知的基准测试以诊断LLMs的推理鲁棒性和公平性。

Abstract: Large language models (LLMs) are known to be sensitive to input phrasing, but
the mechanisms by which semantic cues shape reasoning remain poorly understood.
We investigate this phenomenon in the context of comparative math problems with
objective ground truth, revealing a consistent and directional framing bias:
logically equivalent questions containing the words ``more'', ``less'', or
``equal'' systematically steer predictions in the direction of the framing
term. To study this effect, we introduce MathComp, a controlled benchmark of
300 comparison scenarios, each evaluated under 14 prompt variants across three
LLM families. We find that model errors frequently reflect linguistic steering,
systematic shifts toward the comparative term present in the prompt.
Chain-of-thought prompting reduces these biases, but its effectiveness varies:
free-form reasoning is more robust, while structured formats may preserve or
reintroduce directional drift. Finally, we show that including demographic
identity terms (e.g., ``a woman'', ``a Black person'') in input scenarios
amplifies directional drift, despite identical underlying quantities,
highlighting the interplay between semantic framing and social referents. These
findings expose critical blind spots in standard evaluation and motivate
framing-aware benchmarks for diagnosing reasoning robustness and fairness in
LLMs.

</details>


### [73] [Hanging in the Balance: Pivotal Moments in Crisis Counseling Conversations](https://arxiv.org/abs/2506.03941)
*Vivian Nguyen,Lillian Lee,Cristian Danescu-Niculescu-Mizil*

Main category: cs.CL

TL;DR: 该论文提出了一种无监督计算方法，用于实时检测对话中的关键时刻，并在心理危机咨询中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在对话中，某些关键时刻的回应会显著影响对话走向和结果。检测这些时刻对于高后果领域（如心理危机咨询）尤为重要。

Method: 采用无监督计算方法，基于对话中下一句话可能带来的结果预期变化来识别关键时刻。

Result: 方法验证显示，咨询师在检测到的关键时刻回应时间显著延长，且对话走向更易改变。进一步探讨了咨询师回应与最终结果的关系。

Conclusion: 该方法能有效识别对话关键时刻，为高后果对话领域提供了实用工具。

Abstract: During a conversation, there can come certain moments where its outcome hangs
in the balance. In these pivotal moments, how one responds can put the
conversation on substantially different trajectories leading to significantly
different outcomes. Systems that can detect when such moments arise could
assist conversationalists in domains with highly consequential outcomes, such
as mental health crisis counseling.
  In this work, we introduce an unsupervised computational method for detecting
such pivotal moments as they happen, in an online fashion. Our approach relies
on the intuition that a moment is pivotal if our expectation of the outcome
varies widely depending on what might be said next. By applying our method to
crisis counseling conversations, we first validate it by showing that it aligns
with human perception -- counselors take significantly longer to respond during
moments detected by our method -- and with the eventual conversational
trajectory -- which is more likely to change course at these times. We then use
our framework to explore the relation of the counselor's response during
pivotal moments with the eventual outcome of the session.

</details>


### [74] [TableEval: A Real-World Benchmark for Complex, Multilingual, and Multi-Structured Table Question Answering](https://arxiv.org/abs/2506.03949)
*Junnan Zhu,Jingyi Wang,Bohan Yu,Xiaoyu Wu,Junbo Li,Lei Wang,Nan Xu*

Main category: cs.CL

TL;DR: 论文提出TableEval基准和SEAT评估框架，以解决现有TableQA基准在表格结构多样性、多语言支持和语义准确性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有TableQA基准局限于简单平面表格、存在数据泄露问题，且多为单语言，无法反映实际应用中的跨语言和跨领域复杂性。

Method: 构建包含多种结构（简洁、分层、嵌套）和四领域（政府、金融、学术、行业报告）表格的TableEval基准，并提出基于子问题语义对齐的SEAT评估框架。

Result: 实验表明SEAT与人类判断高度一致，TableEval揭示了当前LLMs在处理复杂现实TableQA任务时的显著能力差距。

Conclusion: TableEval和SEAT为提升LLMs在真实场景下的表格问答能力提供了新方向，相关数据集已开源。

Abstract: LLMs have shown impressive progress in natural language processing. However,
they still face significant challenges in TableQA, where real-world
complexities such as diverse table structures, multilingual data, and
domain-specific reasoning are crucial. Existing TableQA benchmarks are often
limited by their focus on simple flat tables and suffer from data leakage.
Furthermore, most benchmarks are monolingual and fail to capture the
cross-lingual and cross-domain variability in practical applications. To
address these limitations, we introduce TableEval, a new benchmark designed to
evaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes
tables with various structures (such as concise, hierarchical, and nested
tables) collected from four domains (including government, finance, academia,
and industry reports). Besides, TableEval features cross-lingual scenarios with
tables in Simplified Chinese, Traditional Chinese, and English. To minimize the
risk of data leakage, we collect all data from recent real-world documents.
Considering that existing TableQA metrics fail to capture semantic accuracy, we
further propose SEAT, a new evaluation framework that assesses the alignment
between model responses and reference answers at the sub-question level.
Experimental results have shown that SEAT achieves high agreement with human
judgment. Extensive experiments on TableEval reveal critical gaps in the
ability of state-of-the-art LLMs to handle these complex, real-world TableQA
tasks, offering insights for future improvements. We make our dataset available
here: https://github.com/wenge-research/TableEval.

</details>


### [75] [From Real to Synthetic: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding](https://arxiv.org/abs/2506.03968)
*Chiwei Zhu,Benfeng Xu,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

TL;DR: 提出了一种基于属性接地的指令合成方法，用于大规模生成多样且复杂的指令数据，以提升大语言模型的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法生成的合成指令要么缺乏广泛的接地来源导致分布狭窄，要么依赖简单扩展无法产生有意义的复杂轨迹。而高效的模型对齐需要基于认知洞察和真实用例的指令。

Method: 采用属性接地的框架：1) 自上而下的归因过程，将选定真实指令与特定用户场景关联；2) 自下而上的合成过程，利用网络文档首先生成场景，再生成有意义指令。

Result: 构建了包含100万条指令的SynthQuestions数据集，实验表明基于该数据训练的模型在多个基准测试中取得领先性能，且性能提升与网络语料规模持续正相关。

Conclusion: 通过属性接地方法可大规模获取多样复杂的指令数据，显著提升大语言模型的对齐效果，且该方法具有可扩展性。

Abstract: The pursuit of diverse, complex, and large-scale instruction data is crucial
for automatically aligning large language models (LLMs). While there are
methods capable of generating synthetic instructions at scale, they either
suffer from limited grounding sources, leading to a narrow distribution, or
rely on trivial extensions that fail to produce meaningful trajectories in
terms of complexity. In contrast, instructions that benefit efficient alignment
are typically crafted with cognitive insights and grounded in real-world use
cases. In this paper, we synthesize such instructions using attributed
grounding, which involves 1) a top-down attribution process that grounds a
selective set of real instructions to situated users, and 2) a bottom-up
synthesis process that leverages web documents to first generate a situation,
then a meaningful instruction. This framework allows us to harvest diverse and
complex instructions at scale, utilizing the vast range of web documents.
Specifically, we construct a dataset of 1 million instructions, called
SynthQuestions, and demonstrate that models trained on it achieve leading
performance on several common benchmarks, with improvements that continually
scale with more web corpora. Data, models and codes will be available at
https://github.com/Ignoramus0817/SynthQuestions.

</details>


### [76] [Structured Pruning for Diverse Best-of-N Reasoning Optimization](https://arxiv.org/abs/2506.03978)
*Hieu Trung Nguyen,Bao Nguyen,Viet Anh Nguyen*

Main category: cs.CL

TL;DR: 研究发现，在Transformer语言模型中，选择性剪枝某些注意力头可以提升模型的推理能力。为此，作者提出了SPRINT框架，通过对比学习动态选择最优剪枝配置，显著提升了在MATH500和GSM8K数据集上的推理性能。


<details>
  <summary>Details</summary>
Motivation: 传统上，模型剪枝被视为节省计算资源的手段，但作者发现选择性剪枝某些注意力头能意外提升模型的推理能力，尤其是在复杂任务上。这一现象促使他们探索更智能的剪枝策略。

Method: 作者提出了SPRINT框架，利用对比学习方法动态选择推理过程中最优的注意力头和层进行剪枝。通过将问题嵌入与头嵌入对齐，SPRINT能够识别出能带来更准确推理结果的剪枝配置。

Result: 实验结果表明，SPRINT在MATH500和GSM8K数据集上显著优于传统的best-of-N和随机头选择策略，验证了该方法在提升模型推理能力方面的有效性。

Conclusion: 该研究表明，模型剪枝不仅可以节省计算资源，还能提升模型的推理能力。SPRINT框架为动态选择最优剪枝配置提供了有效方法，为未来研究开辟了新方向。

Abstract: Model pruning in transformer-based language models, traditionally viewed as a
means of achieving computational savings, can enhance the model's reasoning
capabilities. In this work, we uncover a surprising phenomenon: the selective
pruning of certain attention heads leads to improvements in reasoning
performance, particularly on challenging tasks. Motivated by this observation,
we propose SPRINT, a novel contrastive learning framework that dynamically
selects the optimal head and layer to prune during inference. By aligning
question embeddings with head embeddings, SPRINT identifies those pruned-head
configurations that result in more accurate reasoning. Extensive experiments
demonstrate that our method significantly outperforms traditional best-of-$N$
and random head selection strategies on the MATH500 and GSM8K datasets.

</details>


### [77] [Voice Activity Projection Model with Multimodal Encoders](https://arxiv.org/abs/2506.03980)
*Takeshi Saga,Catherine Pelachaud*

Main category: cs.CL

TL;DR: 本文提出了一种结合预训练音频和面部编码器的多模态模型，用于改进人机交互中的轮转预测性能。


<details>
  <summary>Details</summary>
Motivation: 由于社交环境的复杂性和多模态特性，传统基于静音时长的系统难以有效建模人机交互中的轮转行为。

Method: 采用预训练的音频和面部编码器增强多模态模型，以捕捉细微表情来提升轮转预测性能。

Result: 该模型在轮转预测指标上表现优异，部分情况下甚至超越现有最优模型。

Conclusion: 通过多模态增强和预训练编码器，模型显著提升了人机交互中轮转预测的准确性。

Abstract: Turn-taking management is crucial for any social interaction. Still, it is
challenging to model human-machine interaction due to the complexity of the
social context and its multimodal nature. Unlike conventional systems based on
silence duration, previous existing voice activity projection (VAP) models
successfully utilized a unified representation of turn-taking behaviors as
prediction targets, which improved turn-taking prediction performance.
Recently, a multimodal VAP model outperformed the previous state-of-the-art
model by a significant margin. In this paper, we propose a multimodal model
enhanced with pre-trained audio and face encoders to improve performance by
capturing subtle expressions. Our model performed competitively, and in some
cases, even better than state-of-the-art models on turn-taking metrics. All the
source codes and pretrained models are available at
https://github.com/sagatake/VAPwithAudioFaceEncoders.

</details>


### [78] [Around the World in 24 Hours: Probing LLM Knowledge of Time and Place](https://arxiv.org/abs/2506.03984)
*Carolin Holtermann,Paul Röttger,Anne Lauscher*

Main category: cs.CL

TL;DR: 论文评估了语言模型在时空联合推理上的能力，发现模型在纯时间推理上表现良好，但在时空结合任务中表现受限，且性能与地理位置无关但与训练数据重复性相关。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型在时间和空间联合推理上的能力，填补此前研究仅关注单一维度或简单环境的空白。

Method: 构建GeoTemp数据集（含32万提示，覆盖289个城市、217个国家、37个时区），评估8个开源聊天模型在时空知识组合任务中的表现。

Result: 模型在纯时间推理任务中表现良好且随规模提升；时空结合任务表现受限；性能与地名在训练中的重复出现显著相关；提示方式直接影响结果（直接注入地理知识有效，思维链提示反而降低简单任务性能）。

Conclusion: 语言模型的时空联合推理能力仍有局限，需优化训练数据和提示策略以提升复杂场景下的表现。

Abstract: Reasoning over time and space is essential for understanding our world.
However, the abilities of language models in this area are largely unexplored
as previous work has tested their abilities for logical reasoning in terms of
time and space in isolation or only in simple or artificial environments. In
this paper, we present the first evaluation of the ability of language models
to jointly reason over time and space. To enable our analysis, we create
GeoTemp, a dataset of 320k prompts covering 289 cities in 217 countries and 37
time zones. Using GeoTemp, we evaluate eight open chat models of three
different model families for different combinations of temporal and geographic
knowledge. We find that most models perform well on reasoning tasks involving
only temporal knowledge and that overall performance improves with scale.
However, performance remains constrained in tasks that require connecting
temporal and geographical information. We do not find clear correlations of
performance with specific geographic regions. Instead, we find a significant
performance increase for location names with low model perplexity, suggesting
their repeated occurrence during model training. We further demonstrate that
their performance is heavily influenced by prompt formulation - a direct
injection of geographical knowledge leads to performance gains, whereas,
surprisingly, techniques like chain-of-thought prompting decrease performance
on simpler tasks.

</details>


### [79] [Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models](https://arxiv.org/abs/2506.03989)
*Alex Laitenberger,Christopher D. Manning,Nelson F. Liu*

Main category: cs.CL

TL;DR: 论文探讨了在长上下文语言模型（LMs）能力提升的背景下，多阶段检索增强生成（RAG）流水线是否仍比简单的单阶段方法更具优势。通过对比实验，发现简单的DOS RAG方法在多任务中表现优异，建议将其作为未来RAG评估的基准。


<details>
  <summary>Details</summary>
Motivation: 随着长上下文语言模型（LMs）的发展，能够单次处理数万个标记，研究旨在评估多阶段检索增强生成（RAG）流水线是否仍比简单的单阶段方法更具优势。

Method: 研究通过系统性地控制标记预算，在问答任务中比较了两种多阶段流水线（ReadAgent和RAPTOR）与三种基线方法，包括保留原始段落顺序的简单检索-阅读方法DOS RAG。

Result: 尽管设计简单，DOS RAG在多个长上下文问答基准测试中表现一致优于或与更复杂的方法相当。

Conclusion: 建议将DOS RAG作为未来RAG评估的简单而强大的基准，结合新兴的嵌入和语言模型，以评估模型能力演变时复杂性与有效性之间的权衡。

Abstract: With the rise of long-context language models (LMs) capable of processing
tens of thousands of tokens in a single pass, do multi-stage
retrieval-augmented generation (RAG) pipelines still offer measurable benefits
over simpler, single-stage approaches? To assess this question, we conduct a
controlled evaluation for QA tasks under systematically scaled token budgets,
comparing two recent multi-stage pipelines, ReadAgent and RAPTOR, against three
baselines, including DOS RAG (Document's Original Structure RAG), a simple
retrieve-then-read method that preserves original passage order. Despite its
straightforward design, DOS RAG consistently matches or outperforms more
intricate methods on multiple long-context QA benchmarks. We recommend
establishing DOS RAG as a simple yet strong baseline for future RAG
evaluations, pairing it with emerging embedding and language models to assess
trade-offs between complexity and effectiveness as model capabilities evolve.

</details>


### [80] [DynTok: Dynamic Compression of Visual Tokens for Efficient and Effective Video Understanding](https://arxiv.org/abs/2506.03990)
*Hongzhi Zhang,Jingyuan Zhang,Xingguang Ji,Qi Wang,Fuzheng Zhang*

Main category: cs.CL

TL;DR: DynTok提出动态视频令牌压缩策略，通过自适应分组与合并减少视觉令牌数量，保持性能同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有视频建模方法（如LLava）将视频表示为视觉令牌序列，导致长视频令牌数量庞大，计算成本高。需在输入LLM前压缩视觉信息以减少冗余。

Method: DynTok动态将视觉令牌分组并合并，对信息密度低的区域高效压缩，同时保留关键内容。

Result: 令牌数量降至原44.4%，性能相当；在Video-MME和MLVU上分别达65.3%和72.5%，且帧数增加时效果更优。

Conclusion: DynTok揭示了视频令牌表示的冗余性，为高效视频建模技术设计提供了新思路。

Abstract: Typical video modeling methods, such as LLava, represent videos as sequences
of visual tokens, which are then processed by the LLM backbone for effective
video understanding. However, this approach leads to a massive number of visual
tokens, especially for long videos. A practical solution is to first extract
relevant visual information from the large visual context before feeding it
into the LLM backbone, thereby reducing computational overhead. In this work,
we introduce DynTok, a novel \textbf{Dyn}amic video \textbf{Tok}en compression
strategy. DynTok adaptively splits visual tokens into groups and merges them
within each group, achieving high compression in regions with low information
density while preserving essential content. Our method reduces the number of
tokens to 44.4% of the original size while maintaining comparable performance.
It further benefits from increasing the number of video frames and achieves
65.3% on Video-MME and 72.5% on MLVU. By applying this simple yet effective
compression method, we expose the redundancy in video token representations and
offer insights for designing more efficient video modeling techniques.

</details>


### [81] [Words of Warmth: Trust and Sociability Norms for over 26k English Words](https://arxiv.org/abs/2506.03993)
*Saif M. Mohammad*

Main category: cs.CL

TL;DR: 该论文介绍了首个大规模手动标注的词汇-温暖度关联数据库Words of Warmth，包含2.6万英文单词，用于研究儿童词汇习得及社会偏见。


<details>
  <summary>Details</summary>
Motivation: 社会心理学研究表明，温暖度(W)和能力(C)是评估他人和群体的主要维度，影响社交、职场等多方面。温暖度可进一步分为信任(T)和社交性(S)，但缺乏相关词汇资源。

Method: 构建Words of Warmth数据库，包含词汇与温暖度、信任、社交性的手动标注关联，数据规模超2.6万英文单词。

Result: 标注数据可靠性高，可用于研究儿童WCTS词汇随年龄的习得速率，并通过案例展示其在偏见和刻板印象研究中的应用价值。

Conclusion: Words of Warmth为温暖度相关研究提供了可靠资源，支持儿童语言发展和社会偏见分析，数据库已开源。

Abstract: Social psychologists have shown that Warmth (W) and Competence (C) are the
primary dimensions along which we assess other people and groups. These
dimensions impact various aspects of our lives from social competence and
emotion regulation to success in the work place and how we view the world. More
recent work has started to explore how these dimensions develop, why they have
developed, and what they constitute. Of particular note, is the finding that
warmth has two distinct components: Trust (T) and Sociability (S). In this
work, we introduce Words of Warmth, the first large-scale repository of
manually derived word--warmth (as well as word--trust and word--sociability)
associations for over 26k English words. We show that the associations are
highly reliable. We use the lexicons to study the rate at which children
acquire WCTS words with age. Finally, we show that the lexicon enables a wide
variety of bias and stereotype research through case studies on various target
entities. Words of Warmth is freely available at:
http://saifmohammad.com/warmth.html

</details>


### [82] [Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in the Billion Parameter Era](https://arxiv.org/abs/2506.03994)
*Dan Oneata,Desmond Elliott,Stella Frank*

Main category: cs.CL

TL;DR: 该研究探讨了大规模基础模型在表示具体物体概念语义特征方面的表现，发现多模态图像编码器略优于纯语言模型，而纯图像编码器在非视觉属性上也表现不俗。


<details>
  <summary>Details</summary>
Motivation: 人类学习与概念表征基于感知运动经验，而当前主流基础模型则依赖海量数据训练。论文旨在探究这些模型对具体物体概念（如“玫瑰是红色、气味香甜、属于花卉”）语义特征的表示能力。

Method: 通过探测任务评估模型对物体属性的认知能力，测试了纯图像编码器、多模态图像编码器和纯语言模型在预测McRae规范扩展版和Binder属性评分数据集上的表现。

Result: 多模态图像编码器略优于纯语言模型；纯图像编码器在非视觉（如“百科”或“功能”类）属性上与语言模型表现相当。

Conclusion: 研究揭示了单模态学习的潜力以及多模态的互补性，为理解模型如何从不同数据中学习语义特征提供了新视角。

Abstract: Human learning and conceptual representation is grounded in sensorimotor
experience, in contrast to state-of-the-art foundation models. In this paper,
we investigate how well such large-scale models, trained on vast quantities of
data, represent the semantic feature norms of concrete object concepts, e.g. a
ROSE is red, smells sweet, and is a flower. More specifically, we use probing
tasks to test which properties of objects these models are aware of. We
evaluate image encoders trained on image data alone, as well as
multimodally-trained image encoders and language-only models, on predicting an
extended denser version of the classic McRae norms and the newer Binder dataset
of attribute ratings. We find that multimodal image encoders slightly
outperform language-only approaches, and that image-only encoders perform
comparably to the language models, even on non-visual attributes that are
classified as "encyclopedic" or "function". These results offer new insights
into what can be learned from pure unimodal learning, and the complementarity
of the modalities.

</details>


### [83] [QQSUM: A Novel Task and Model of Quantitative Query-Focused Summarization for Review-based Product Question Answering](https://arxiv.org/abs/2506.04020)
*An Quang Tang,Xiuzhen Zhang,Minh Ngoc Dinh,Zhuang Li*

Main category: cs.CL

TL;DR: 论文提出QQSUM-RAG模型，通过量化用户评论多样性生成多视角答案，优于现有PQA系统。


<details>
  <summary>Details</summary>
Motivation: 现有PQA系统仅能生成单一视角答案，无法反映用户意见的多样性。

Method: 扩展RAG框架，联合训练关键点检索器和生成器，实现基于量化分析的多样性摘要。

Result: 实验表明QQSUM-RAG在文本质量和观点量化准确性上均优于基线模型。

Conclusion: 该研究为电商平台提供了能捕捉用户意见多样性的自动问答解决方案。

Abstract: Review-based Product Question Answering (PQA) allows e-commerce platforms to
automatically address customer queries by leveraging insights from user
reviews. However, existing PQA systems generate answers with only a single
perspective, failing to capture the diversity of customer opinions. In this
paper we introduce a novel task Quantitative Query-Focused Summarization
(QQSUM), which aims to summarize diverse customer opinions into representative
Key Points (KPs) and quantify their prevalence to effectively answer user
queries. While Retrieval-Augmented Generation (RAG) shows promise for PQA, its
generated answers still fall short of capturing the full diversity of
viewpoints. To tackle this challenge, our model QQSUM-RAG, which extends RAG,
employs few-shot learning to jointly train a KP-oriented retriever and a KP
summary generator, enabling KP-based summaries that capture diverse and
representative opinions. Experimental results demonstrate that QQSUM-RAG
achieves superior performance compared to state-of-the-art RAG baselines in
both textual quality and quantification accuracy of opinions. Our source code
is available at: https://github.com/antangrocket1312/QQSUMM

</details>


### [84] [AI Agents for Conversational Patient Triage: Preliminary Simulation-Based Evaluation with Real-World EHR Data](https://arxiv.org/abs/2506.04032)
*Sina Rashidian,Nan Li,Jonathan Amar,Jong Ha Lee,Sam Pugh,Eric Yang,Geoff Masterson,Myoung Cha,Yugang Jia,Akhil Vaid*

Main category: cs.CL

TL;DR: 该研究开发了一个基于真实电子健康记录(EHR)数据的患者模拟器，用于训练和测试医疗AI代理，模拟真实患者对话并验证其临床一致性。


<details>
  <summary>Details</summary>
Motivation: 旨在通过真实患者数据构建高保真模拟器，为医疗AI代理提供可扩展的训练和测试环境，解决真实患者数据获取难的问题。

Method: 1. 从真实EHR提取多样化病例模板 2. 用AI代理进行多轮症状询问对话 3. 由临床专家评估500+模拟病例的合理性

Result: 临床专家认为97.7%的模拟对话与病例模板一致，99%的病例摘要具有临床相关性。

Conclusion: 该方法能有效生成符合临床现实的模拟数据，可大规模用于训练医疗对话AI。

Abstract: Background: We present a Patient Simulator that leverages real world patient
encounters which cover a broad range of conditions and symptoms to provide
synthetic test subjects for development and testing of healthcare agentic
models. The simulator provides a realistic approach to patient presentation and
multi-turn conversation with a symptom-checking agent. Objectives: (1) To
construct and instantiate a Patient Simulator to train and test an AI health
agent, based on patient vignettes derived from real EHR data. (2) To test the
validity and alignment of the simulated encounters provided by the Patient
Simulator to expert human clinical providers. (3) To illustrate the evaluation
framework of such an LLM system on the generated realistic, data-driven
simulations -- yielding a preliminary assessment of our proposed system.
Methods: We first constructed realistic clinical scenarios by deriving patient
vignettes from real-world EHR encounters. These vignettes cover a variety of
presenting symptoms and underlying conditions. We then evaluate the performance
of the Patient Simulator as a simulacrum of a real patient encounter across
over 500 different patient vignettes. We leveraged a separate AI agent to
provide multi-turn questions to obtain a history of present illness. The
resulting multiturn conversations were evaluated by two expert clinicians.
Results: Clinicians scored the Patient Simulator as consistent with the patient
vignettes in those same 97.7% of cases. The extracted case summary based on the
conversation history was 99% relevant. Conclusions: We developed a methodology
to incorporate vignettes derived from real healthcare patient data to build a
simulation of patient responses to symptom checking agents. The performance and
alignment of this Patient Simulator could be used to train and test a
multi-turn conversational AI agent at scale.

</details>


### [85] [The mutual exclusivity bias of bilingual visually grounded speech models](https://arxiv.org/abs/2506.04037)
*Dan Oneata,Leanne Nortje,Yevgen Matusevych,Herman Kamper*

Main category: cs.CL

TL;DR: 双语视觉语音模型比单语模型表现出更弱的互斥性偏差，部分原因是视觉嵌入方差较小。


<details>
  <summary>Details</summary>
Motivation: 研究双语视觉语音模型中互斥性(ME)偏差的表现，探讨双语环境对语言学习策略的影响。

Method: 使用英语、法语和荷兰语的组合训练双语视觉语音模型，分析其互斥性偏差。

Result: 双语模型的ME偏差普遍弱于单语模型，视觉嵌入方差较小导致新概念与熟悉概念混淆增加。

Conclusion: 双语环境会减弱ME偏差，研究为理解VGS模型中ME偏差的存在提供了新视角。

Abstract: Mutual exclusivity (ME) is a strategy where a novel word is associated with a
novel object rather than a familiar one, facilitating language learning in
children. Recent work has found an ME bias in a visually grounded speech (VGS)
model trained on English speech with paired images. But ME has also been
studied in bilingual children, who may employ it less due to cross-lingual
ambiguity. We explore this pattern computationally using bilingual VGS models
trained on combinations of English, French, and Dutch. We find that bilingual
models generally exhibit a weaker ME bias than monolingual models, though
exceptions exist. Analyses show that the combined visual embeddings of
bilingual models have a smaller variance for familiar data, partly explaining
the increase in confusion between novel and familiar concepts. We also provide
new insights into why the ME bias exists in VGS models in the first place. Code
and data: https://github.com/danoneata/me-vgs

</details>


### [86] [LexTime: A Benchmark for Temporal Ordering of Legal Events](https://arxiv.org/abs/2506.04041)
*Claire Barale,Leslie Barrett,Vikram Sunil Bajaj,Michael Rovatsos*

Main category: cs.CL

TL;DR: 论文介绍了首个评估大语言模型在法律文本中事件排序能力的数据集LexTime，发现模型在法律事件排序上表现优于叙事文本，但法律语言复杂性仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏专家语言评估，导致难以理解大语言模型如何管理法律文本中的事件顺序，因此需要构建专门的数据集来填补这一空白。

Method: 研究构建了LexTime数据集，包含512个来自美国联邦诉讼的标注事件对及其时间关系，并分析了上下文长度、显隐事件对及法律语言特征对模型表现的影响。

Result: 研究发现：(1) 模型在法律事件排序上准确率比叙事文本高10.5%；(2) 长上下文和隐式事件能提升准确率（隐-显事件对达80.8%）；(3) 法律语言复杂性和嵌套从句仍是难点。

Conclusion: 研究表明需针对法律文本的时序推理开发特定建模策略，以应对其语言复杂性和结构特点。

Abstract: Temporal reasoning in legal texts is important for applications like case law
analysis and compliance monitoring. However, existing datasets lack expert
language evaluation, leaving a gap in understanding how LLMs manage event
ordering in legal contexts. We introduce LexTime, the first dataset designed to
evaluate LLMs' event ordering capabilities in legal language, consisting of 512
instances from U.S. Federal Complaints with annotated event pairs and their
temporal relations. Our findings show that (1) LLMs are more accurate on legal
event ordering than on narrative (up to +10.5%); (2) longer input contexts and
implicit events boost accuracy, reaching 80.8% for implicit-explicit event
pairs; (3) legal linguistic complexities and nested clauses remain a challenge.
We investigate how context length, explicit vs implicit event pairs, and legal
language features affect model performance, demonstrating the need for specific
modeling strategies to enhance temporal event reasoning.

</details>


### [87] [Unveiling and Eliminating the Shortcut Learning for Locate-Then-Edit Knowledge Editing via Both Subject and Relation Awareness](https://arxiv.org/abs/2506.04042)
*Xiyu Liu,Zhengxiao Liu,Naibin Gu,Zheng Lin,Ji Xiang,Weiping Wang*

Main category: cs.CL

TL;DR: 该论文提出了一种两阶段优化方法，通过平衡主体特征和关系特征的学习，解决了知识编辑中的捷径学习问题，实现了更可控的知识编辑。


<details>
  <summary>Details</summary>
Motivation: 当前的知识编辑方法在优化过程中倾向于过度学习主体特征而忽视关系特征，导致对目标编辑主体相关的不必要关联进行修改，缺乏可控性。

Method: 论文提出了一种新颖的两阶段优化过程，旨在平衡主体特征和关系特征的学习，从而消除主体特征的捷径学习问题。

Result: 实验结果表明，该方法有效防止了知识编辑中的捷径学习，实现了最优的整体性能，提升了知识编辑的可控性。

Conclusion: 通过平衡主体和关系特征的学习，该方法成功解决了知识编辑中的捷径学习问题，为可控知识编辑提供了有效解决方案。

Abstract: Knowledge editing aims to alternate the target knowledge predicted by large
language models while ensuring the least side effects on unrelated knowledge.
An effective way to achieve knowledge editing is to identify pivotal parameters
for predicting factual associations and modify them with an optimization
process to update the predictions. However, these locate-then-edit methods are
uncontrollable since they tend to modify most unrelated relations connected to
the subject of target editing. We unveil that this failure of controllable
editing is due to a shortcut learning issue during the optimization process.
Specifically, we discover two crucial features that are the subject feature and
the relation feature for models to learn during optimization, but the current
optimization process tends to over-learning the subject feature while
neglecting the relation feature. To eliminate this shortcut learning of the
subject feature, we propose a novel two-stage optimization process that
balances the learning of the subject feature and the relation feature.
Experimental results demonstrate that our approach successfully prevents
knowledge editing from shortcut learning and achieves the optimal overall
performance, contributing to controllable knowledge editing.

</details>


### [88] [Think Like a Person Before Responding: A Multi-Faceted Evaluation of Persona-Guided LLMs for Countering Hate](https://arxiv.org/abs/2506.04043)
*Mikel K. Ngueajio,Flor Miriam Plaza-del-Arco,Yi-Ling Chung,Danda B. Rawat,Amanda Cercas Curry*

Main category: cs.CL

TL;DR: 论文提出评估大语言模型生成反仇恨言论框架，发现现有模型生成内容冗长且门槛高，情感引导提示能提升共情但安全性存疑。


<details>
  <summary>Details</summary>
Motivation: 自动化反叙事可对抗网络仇恨言论，但其情感基调、可读性和伦理风险尚未系统评估。

Method: 使用GPT-4o-Mini等3种LLM，在MT-Conan和HatEval数据集上测试三种提示策略，从人物框架、可读性等四维度评估。

Result: LLM生成内容普遍冗长且需大学阅读水平，情感提示能增强共情但安全有效性仍存在问题。

Conclusion: 需优化反叙事的简洁性和普适性，情感引导策略需权衡伦理风险。

Abstract: Automated counter-narratives (CN) offer a promising strategy for mitigating
online hate speech, yet concerns about their affective tone, accessibility, and
ethical risks remain. We propose a framework for evaluating Large Language
Model (LLM)-generated CNs across four dimensions: persona framing, verbosity
and readability, affective tone, and ethical robustness. Using GPT-4o-Mini,
Cohere's CommandR-7B, and Meta's LLaMA 3.1-70B, we assess three prompting
strategies on the MT-Conan and HatEval datasets. Our findings reveal that
LLM-generated CNs are often verbose and adapted for people with college-level
literacy, limiting their accessibility. While emotionally guided prompts yield
more empathetic and readable responses, there remain concerns surrounding
safety and effectiveness.

</details>


### [89] [Lacuna Inc. at SemEval-2025 Task 4: LoRA-Enhanced Influence-Based Unlearning for LLMs](https://arxiv.org/abs/2506.04044)
*Aleksey Kudelya,Alexander Shirnin*

Main category: cs.CL

TL;DR: LIBU算法结合影响函数与二阶优化，高效移除大语言模型中特定知识且保持整体性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在无需从头训练的情况下移除敏感知识且不影响其整体效用的问题。

Method: 结合经典影响函数移除数据影响，并采用二阶优化稳定模型性能。

Result: 实验证明该轻量方法适用于多种任务中的大语言模型知识移除。

Conclusion: LIBU为高效且实用的语言模型知识移除提供了可行方案。

Abstract: This paper describes LIBU (LoRA enhanced influence-based unlearning), an
algorithm to solve the task of unlearning - removing specific knowledge from a
large language model without retraining from scratch and compromising its
overall utility (SemEval-2025 Task 4: Unlearning sensitive content from Large
Language Models). The algorithm combines classical \textit{influence functions}
to remove the influence of the data from the model and \textit{second-order
optimization} to stabilize the overall utility. Our experiments show that this
lightweight approach is well applicable for unlearning LLMs in different kinds
of task.

</details>


### [90] [On Support Samples of Next Word Prediction](https://arxiv.org/abs/2506.04047)
*Yuqian Li,Yupei Du,Yufang Liu,Feifei Feng,Mou Xiao Feng,Yuanbin Wu*

Main category: cs.CL

TL;DR: 该论文通过数据中心的解释性方法，研究了语言模型中的支持样本和非支持样本在预测中的作用及其对模型泛化和表示学习的影响。


<details>
  <summary>Details</summary>
Motivation: 语言模型在各种任务中表现出色，但其决策背后的原理仍不明确。本文旨在通过数据中心的可解释性方法，揭示语言模型行为的内部机制。

Method: 利用表示定理，识别了促进或抑制特定预测的支持样本，并分析了非支持样本在防止过拟合和形成中间表示中的作用。

Result: 研究发现支持样本的预测性在训练前即可确定，而非支持样本在深层网络中重要性增加，对泛化和表示学习至关重要。

Conclusion: 这些发现为理解语言模型行为提供了新的视角，揭示了数据与模型决策之间的相互作用，增强了模型的可解释性。

Abstract: Language models excel in various tasks by making complex decisions, yet
understanding the rationale behind these decisions remains a challenge. This
paper investigates \emph{data-centric interpretability} in language models,
focusing on the next-word prediction task. Using representer theorem, we
identify two types of \emph{support samples}-those that either promote or deter
specific predictions. Our findings reveal that being a support sample is an
intrinsic property, predictable even before training begins. Additionally,
while non-support samples are less influential in direct predictions, they play
a critical role in preventing overfitting and shaping generalization and
representation learning. Notably, the importance of non-support samples
increases in deeper layers, suggesting their significant role in intermediate
representation formation.These insights shed light on the interplay between
data and model decisions, offering a new dimension to understanding language
model behavior and interpretability.

</details>


### [91] [Explainability-Based Token Replacement on LLM-Generated Text](https://arxiv.org/abs/2506.04050)
*Hadi Mohammadi,Anastasia Giachanou,Daniel L. Oberski,Ayoub Bagheri*

Main category: cs.CL

TL;DR: 论文探讨了如何利用可解释AI方法降低AI生成文本的可检测性，并提出了一种基于集成分类器的鲁棒检测方法。


<details>
  <summary>Details</summary>
Motivation: 尽管生成模型（如大语言模型）能生成类似人类的文本，但其输出仍存在可检测的模式。研究旨在通过可解释AI方法减少AI生成文本的可检测性，并开发更鲁棒的检测策略。

Method: 训练集成分类器区分AI生成文本与人类文本，应用SHAP和LIME识别关键标记，提出四种基于解释性的标记替换策略修改这些标记。

Result: 标记替换策略能显著降低单一分类器的检测能力，但集成分类器在多语言和跨领域场景中保持强鲁棒性。

Conclusion: 可解释AI方法能有效隐藏AI生成文本，但需依赖集成检测策略应对不断演变的隐藏技术。

Abstract: Generative models, especially large language models (LLMs), have shown
remarkable progress in producing text that appears human-like. However, they
often exhibit patterns that make their output easier to detect than text
written by humans. In this paper, we investigate how explainable AI (XAI)
methods can be used to reduce the detectability of AI-generated text (AIGT)
while also introducing a robust ensemble-based detection approach. We begin by
training an ensemble classifier to distinguish AIGT from human-written text,
then apply SHAP and LIME to identify tokens that most strongly influence its
predictions. We propose four explainability-based token replacement strategies
to modify these influential tokens. Our findings show that these token
replacement approaches can significantly diminish a single classifier's ability
to detect AIGT. However, our ensemble classifier maintains strong performance
across multiple languages and domains, showing that a multi-model approach can
mitigate the impact of token-level manipulations. These results show that XAI
methods can make AIGT harder to detect by focusing on the most influential
tokens. At the same time, they highlight the need for robust, ensemble-based
detection strategies that can adapt to evolving approaches for hiding AIGT.

</details>


### [92] [High Accuracy, Less Talk (HALT): Reliable LLMs through Capability-Aligned Finetuning](https://arxiv.org/abs/2506.04051)
*Tim Franzmeyer,Archie Sravankumar,Lijuan Liu,Yuning Mao,Rui Hou,Sinong Wang,Jakob N. Foerster,Luke Zettlemoyer,Madian Khabsa*

Main category: cs.CL

TL;DR: 论文提出HALT方法，通过后训练让LLM在不确定时部分或完全拒绝回答，以提高回答正确性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对所有提示都会回应，但在缺乏知识或能力时会产生错误答案（幻觉问题）。作者希望通过后训练让模型仅在确信正确时生成内容，否则部分或完全拒绝回答。

Method: HALT方法将预训练LLM的响应拆分为事实片段，利用真实信息识别错误片段，并通过移除或替换为'Unsure from Here'来生成能力对齐的微调数据。

Result: HALT在四个开源模型上测试，平均将回答片段的正确性提高15%，F1分数提高4%。最高正确性设置下，Llama3-70B模型的正确性从51%提升至87%。

Conclusion: HALT能有效权衡回答完整性和正确性，显著提升LLM的可靠性。

Abstract: Large Language Models (LLMs) currently respond to every prompt. However, they
can produce incorrect answers when they lack knowledge or capability -- a
problem known as hallucination. We instead propose post-training an LLM to
generate content only when confident in its correctness and to otherwise
(partially) abstain. Specifically, our method, HALT, produces
capability-aligned post-training data that encodes what the model can and
cannot reliably generate. We generate this data by splitting responses of the
pretrained LLM into factual fragments (atomic statements or reasoning steps),
and use ground truth information to identify incorrect fragments. We achieve
capability-aligned finetuning responses by either removing incorrect fragments
or replacing them with "Unsure from Here" -- according to a tunable threshold
that allows practitioners to trade off response completeness and mean
correctness of the response's fragments. We finetune four open-source models
for biography writing, mathematics, coding, and medicine with HALT for three
different trade-off thresholds. HALT effectively trades off response
completeness for correctness, increasing the mean correctness of response
fragments by 15% on average, while resulting in a 4% improvement in the F1
score (mean of completeness and correctness of the response) compared to the
relevant baselines. By tuning HALT for highest correctness, we train a single
reliable Llama3-70B model with correctness increased from 51% to 87% across all
four domains while maintaining 53% of the response completeness achieved with
standard finetuning.

</details>


### [93] [Progressive Mastery: Customized Curriculum Learning with Guided Prompting for Mathematical Reasoning](https://arxiv.org/abs/2506.04065)
*Muling Wu,Qi Qian,Wenhao Liu,Xiaohua Wang,Zisu Huang,Di Liang,LI Miao,Shihan Dou,Changze Lv,Zhenghua Wang,Zhibo Xu,Lina Chen,Tianlong Li,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.CL

TL;DR: 提出定制化课程学习（CCL）框架，通过模型自适应难度定义和引导提示技术，提升大语言模型在推理任务中的样本利用效率和性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）在训练后存在样本利用效率低和难度样本处理不灵活的问题，限制了其性能提升。

Method: CCL框架包含两项创新：1）模型自适应难度定义，根据模型能力定制课程数据集；2）引导提示技术，通过动态提示降低样本难度。

Result: 在监督微调和强化学习的实验中，CCL在五个数学推理基准上显著优于均匀训练方法。

Conclusion: CCL有效提升了样本利用率和模型性能，适用于不同训练范式。

Abstract: Large Language Models (LLMs) have achieved remarkable performance across
various reasoning tasks, yet post-training is constrained by inefficient sample
utilization and inflexible difficulty samples processing. To address these
limitations, we propose Customized Curriculum Learning (CCL), a novel framework
with two key innovations. First, we introduce model-adaptive difficulty
definition that customizes curriculum datasets based on each model's individual
capabilities rather than using predefined difficulty metrics. Second, we
develop "Guided Prompting," which dynamically reduces sample difficulty through
strategic hints, enabling effective utilization of challenging samples that
would otherwise degrade performance. Comprehensive experiments on supervised
fine-tuning and reinforcement learning demonstrate that CCL significantly
outperforms uniform training approaches across five mathematical reasoning
benchmarks, confirming its effectiveness across both paradigms in enhancing
sample utilization and model performance.

</details>


### [94] [LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually Impaired via GRPO with LLM-as-Follower Reward](https://arxiv.org/abs/2506.04070)
*Yi Zhao,Siqi Wang,Jing Li*

Main category: cs.CL

TL;DR: 该研究提出LaF-GRPO方法，利用LLM模拟视障用户反馈优化导航指令生成，并开源NIG4VI基准数据集，显著提升指令实用性和安全性。


<details>
  <summary>Details</summary>
Motivation: 针对视障用户生成精准、实用的逐步导航指令（NIG-VI）是一个重要但研究较少的领域。现有方法常依赖昂贵的真实数据，且指令实用性不足。

Method: 提出LaF-GRPO框架：让LLM模拟视障用户响应生成奖励信号，指导视觉语言模型（VLM）的微调过程。同时构建包含2.7万样本的开源基准NIG4VI，支持开放场景下的细粒度指令生成。

Result: 实验表明：LaF-GRPO显著提升指标（如Zero版本BLEU+14%，微调版METEOR 0.542 vs GPT-4o的0.323），生成的指令更直观安全。

Conclusion: 该方法通过LLM仿真反馈降低真实数据需求，结合NIG4VI基准有效推进了视障导航指令生成领域的研究。

Abstract: Navigation instruction generation for visually impaired (VI) individuals
(NIG-VI) is critical yet relatively underexplored. This study, hence, focuses
on producing precise, in-situ, step-by-step navigation instructions that are
practically usable by VI users. Concretely, we propose LaF-GRPO
(LLM-as-Follower GRPO), where an LLM simulates VI user responses to generate
rewards guiding the Vision-Language Model (VLM) post-training. This enhances
instruction usability while reducing costly real-world data needs. To
facilitate training and testing, we introduce NIG4VI, a 27k-sample open-sourced
benchmark. It provides diverse navigation scenarios with accurate spatial
coordinates, supporting detailed, open-ended in-situ instruction generation.
Experiments on NIG4VI show the effectiveness of LaF-GRPO by quantitative
metrics (e.g., Zero-(LaF-GRPO) boosts BLEU +14\%; SFT+(LaF-GRPO) METEOR 0.542
vs. GPT-4o's 0.323) and yields more intuitive, safer instructions. Code and
benchmark are available at
\href{https://github.com/YiyiyiZhao/NIG4VI}{https://github.com/YiyiyiZhao/NIG4VI}.

</details>


### [95] [Controlling Difficulty of Generated Text for AI-Assisted Language Learning](https://arxiv.org/abs/2506.04072)
*Meiqing Jin,Liam Dugan,Chris Callison-Burch*

Main category: cs.CL

TL;DR: 研究探讨如何通过可控生成技术调整大语言模型输出，以更好地支持零基础语言学习者，提出新评估指标Token Miss Rate并发布相关资源。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成文本复杂度接近母语者，不适合零基础学习者（CEFR A1-A2），需探索无需微调的可控生成技术来适配初学者需求。

Method: 采用模块化可控生成技术（如未来判别器），通过自动指标和日语学习者用户研究评估效果，并开发Token Miss Rate指标量化理解难度。

Result: 未来判别器显著提升输出可理解性（40.4%→84.3%），Token Miss Rate指标与人工评估高度相关。

Conclusion: 可控生成技术能有效适配LLM输出难度，Token Miss Rate是可靠评估指标，研究开源了代码、模型及数据集支持后续AI语言学习研究。

Abstract: Practicing conversations with large language models (LLMs) presents a
promising alternative to traditional in-person language learning. However, most
LLMs generate text at a near-native level of complexity, making them ill-suited
for beginner learners (CEFR: A1-A2). In this paper, we investigate whether
controllable generation techniques -- specifically modular methods that do not
require model fine-tuning -- can adapt LLM outputs to better support absolute
beginners. We evaluate these methods through both automatic metrics and a user
study with university-level learners of Japanese. Our findings show that while
prompting alone fails to control output difficulty, the use of future
discriminators (Yang and Klein, 2021) significantly improves output
comprehensibility (from 40.4\% to 84.3\%). We further introduce a novel
token-level evaluation metric, Token Miss Rate (TMR), that quantifies the
proportion of incomprehensible tokens per utterance and correlates strongly
with human judgments. To support future research in AI-assisted language
learning, we release our code, models, annotation tools, and dataset.

</details>


### [96] [Acoustically Precise Hesitation Tagging Is Essential for End-to-End Verbatim Transcription Systems](https://arxiv.org/abs/2506.04076)
*Jhen-Ke Lin,Hao-Chien Lu,Chung-Chun Wang,Hong-Yun Lin,Berlin Chen*

Main category: cs.CL

TL;DR: 使用LoRA微调Whisper模型，通过精确标注填充词显著提升L2语音转录准确率。


<details>
  <summary>Details</summary>
Motivation: 现有ASR系统常忽略或泛化语音中的犹豫现象，导致重要声学细节丢失，影响自动口语评估的准确性。

Method: 基于Speak & Improve 2025语料库，采用低秩适应（LoRA）微调Whisper模型，比较三种标注方案：去除犹豫（Pure）、通用标签（Rich）和由Gemini 2.0 Flash推断的声学精确填充词（Extra）。

Result: 使用'Extra'方案微调Whisper Large V3 Turbo后，词错误率（WER）降至5.5%，较'Pure'方案（6.2% WER）相对提升11.3%。

Conclusion: 明确且真实的填充词标注能显著提高ASR对L2语音逐字转录的准确率。

Abstract: Verbatim transcription for automatic speaking assessment demands accurate
capture of disfluencies, crucial for downstream tasks like error analysis and
feedback. However, many ASR systems discard or generalize hesitations, losing
important acoustic details. We fine-tune Whisper models on the Speak & Improve
2025 corpus using low-rank adaptation (LoRA), without recourse to external
audio training data. We compare three annotation schemes: removing hesitations
(Pure), generic tags (Rich), and acoustically precise fillers inferred by
Gemini 2.0 Flash from existing audio-transcript pairs (Extra). Our challenge
system achieved 6.47% WER (Pure) and 5.81% WER (Extra). Post-challenge
experiments reveal that fine-tuning Whisper Large V3 Turbo with the "Extra"
scheme yielded a 5.5% WER, an 11.3% relative improvement over the "Pure" scheme
(6.2% WER). This demonstrates that explicit, realistic filled-pause labeling
significantly enhances ASR accuracy for verbatim L2 speech transcription.

</details>


### [97] [A Novel Data Augmentation Approach for Automatic Speaking Assessment on Opinion Expressions](https://arxiv.org/abs/2506.04077)
*Chung-Chun Wang,Jhen-Ke Lin,Hao-Chien Lu,Hong-Yun Lin,Berlin Chen*

Main category: cs.CL

TL;DR: 论文提出了一种利用大语言模型生成多样化回答并合成语音的新方法，以解决自动口语评估中数据稀缺问题，并通过多模态模型整合文本和语音特征提升评分可靠性。


<details>
  <summary>Details</summary>
Motivation: 自动口语评估在观点表达上常因标注录音稀缺而受限，导致提示多样性不足和评分不可靠。

Method: 利用大语言模型生成不同水平的回答，通过语音合成转换为语音，采用动态重要性损失调整训练实例权重，最后通过多模态大语言模型整合文本和语音特征直接预测分数。

Result: 在LTTC数据集上的实验表明，该方法优于依赖真实数据或传统增强的方法，有效缓解了低资源限制。

Conclusion: 该方法通过跨模态信息实现了对观点表达的自动口语评估，显著提升了低资源条件下的评估效果。

Abstract: Automated speaking assessment (ASA) on opinion expressions is often hampered
by the scarcity of labeled recordings, which restricts prompt diversity and
undermines scoring reliability. To address this challenge, we propose a novel
training paradigm that leverages a large language models (LLM) to generate
diverse responses of a given proficiency level, converts responses into
synthesized speech via speaker-aware text-to-speech synthesis, and employs a
dynamic importance loss to adaptively reweight training instances based on
feature distribution differences between synthesized and real speech.
Subsequently, a multimodal large language model integrates aligned textual
features with speech signals to predict proficiency scores directly.
Experiments conducted on the LTTC dataset show that our approach outperforms
methods relying on real data or conventional augmentation, effectively
mitigating low-resource constraints and enabling ASA on opinion expressions
with cross-modal information.

</details>


### [98] [LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with Physician Validation](https://arxiv.org/abs/2506.04078)
*Ming Zhang,Yujiong Shen,Zelin Li,Huayu Sha,Binze Hu,Yuhui Wang,Chenhao Huang,Shichun Liu,Jingqi Tong,Changhao Jiang,Mingxu Chai,Zhiheng Xi,Shihan Dou,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 该论文提出了LLMEval-Med基准测试，用于评估大型语言模型在医学领域的表现，解决了现有基准在问题设计、数据来源和评估方法上的局限性。


<details>
  <summary>Details</summary>
Motivation: 医学应用对准确性要求极高，现有医学基准测试存在多种局限，如问题设计单一、数据来源不真实、评估方法不完善，因此需要更全面、真实的评估工具。

Method: 作者开发了LLMEval-Med基准测试，包含五个核心医学领域的2,996个问题，问题源自真实电子健康记录和专家设计的临床场景，并设计了自动化评估流程，结合专家检查表和LLM-as-Judge框架。

Result: 评估了13种大型语言模型（包括专业医学模型、开源模型和闭源模型），通过人机一致性分析验证了机器评分的可靠性，并动态优化检查表和提示。

Conclusion: LLMEval-Med为医学领域安全有效部署大型语言模型提供了宝贵见解，数据集已公开。

Abstract: Evaluating large language models (LLMs) in medicine is crucial because
medical applications require high accuracy with little room for error. Current
medical benchmarks have three main types: medical exam-based, comprehensive
medical, and specialized assessments. However, these benchmarks have
limitations in question design (mostly multiple-choice), data sources (often
not derived from real clinical scenarios), and evaluation methods (poor
assessment of complex reasoning). To address these issues, we present
LLMEval-Med, a new benchmark covering five core medical areas, including 2,996
questions created from real-world electronic health records and expert-designed
clinical scenarios. We also design an automated evaluation pipeline,
incorporating expert-developed checklists into our LLM-as-Judge framework.
Furthermore, our methodology validates machine scoring through human-machine
agreement analysis, dynamically refining checklists and prompts based on expert
feedback to ensure reliability. We evaluate 13 LLMs across three categories
(specialized medical models, open-source models, and closed-source models) on
LLMEval-Med, providing valuable insights for the safe and effective deployment
of LLMs in medical domains. The dataset is released in
https://github.com/llmeval/LLMEval-Med.

</details>


### [99] [EuroLLM-9B: Technical Report](https://arxiv.org/abs/2506.04079)
*Pedro Henrique Martins,João Alves,Patrick Fernandes,Nuno M. Guerreiro,Ricardo Rei,Amin Farajian,Mateusz Klimaszewski,Duarte M. Alves,José Pombal,Manuel Faysse,Pierre Colombo,François Yvon,Barry Haddow,José G. C. de Souza,Alexandra Birch,André F. T. Martins*

Main category: cs.CL

TL;DR: EuroLLM-9B是一个支持24种欧盟官方语言和11种其他语言的大型语言模型，旨在解决欧洲语言在现有开放大模型中代表性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有开放大型语言模型对欧洲语言的支持不足，EuroLLM-9B旨在填补这一空白，满足欧洲公民的多语言需求。

Method: 通过设计多语言分词器、架构规范、数据过滤和训练流程，结合EuroFilter和EuroBlocks-Synthetic数据集进行预训练和后训练。

Result: EuroLLM-9B在多语言基准测试和机器翻译任务中表现优异，成为同类欧洲开源LLM中的领先者。

Conclusion: EuroLLM-9B成功提升了欧洲语言在大型语言模型中的代表性，并开源了所有主要组件以促进研究和应用。

Abstract: This report presents EuroLLM-9B, a large language model trained from scratch
to support the needs of European citizens by covering all 24 official European
Union languages and 11 additional languages. EuroLLM addresses the issue of
European languages being underrepresented and underserved in existing open
large language models. We provide a comprehensive overview of EuroLLM-9B's
development, including tokenizer design, architectural specifications, data
filtering, and training procedures. We describe the pre-training data
collection and filtering pipeline, including the creation of EuroFilter, an
AI-based multilingual filter, as well as the design of EuroBlocks-Synthetic, a
novel synthetic dataset for post-training that enhances language coverage for
European languages. Evaluation results demonstrate EuroLLM-9B's competitive
performance on multilingual benchmarks and machine translation tasks,
establishing it as the leading open European-made LLM of its size. To support
open research and adoption, we release all major components of this work,
including the base and instruction-tuned models, the EuroFilter classifier, and
the synthetic post-training dataset.

</details>


### [100] [TextAtari: 100K Frames Game Playing with Language Agents](https://arxiv.org/abs/2506.04098)
*Wenhao Li,Wenwu Li,Chuyun Shen,Junjie Sheng,Zixiao Huang,Di Wu,Yun Hua,Wei Yin,Xiangfeng Wang,Hongyuan Zha,Bo Jin*

Main category: cs.CL

TL;DR: TextAtari是一个评估语言代理在长达10万步的长期决策任务中的基准测试，通过将Atari游戏的视觉状态转换为文本描述，创建了一个结合序列决策与自然语言处理的挑战性测试平台。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在建立一个标准化的评估基准，以研究语言模型在长期决策任务中的表现，特别是在序列推理、状态跟踪和战略规划方面的能力。

Method: 通过无监督表示学习框架（AtariARI）将Atari游戏状态转换为文本描述，构建了近100个不同复杂度的任务，并评估了三种大型语言模型在三种代理框架下的表现。

Result: 研究结果显示，语言代理在长期规划任务中与人类玩家存在显著性能差距，突显了在序列推理和战略规划方面的挑战。

Conclusion: TextAtari为语言模型与规划研究的交叉领域提供了标准化的评估协议和基线实现，推动了相关研究的进展。

Abstract: We present TextAtari, a benchmark for evaluating language agents on very
long-horizon decision-making tasks spanning up to 100,000 steps. By translating
the visual state representations of classic Atari games into rich textual
descriptions, TextAtari creates a challenging test bed that bridges sequential
decision-making with natural language processing. The benchmark includes nearly
100 distinct tasks with varying complexity, action spaces, and planning
horizons, all rendered as text through an unsupervised representation learning
framework (AtariARI). We evaluate three open-source large language models
(Qwen2.5-7B, Gemma-7B, and Llama3.1-8B) across three agent frameworks
(zero-shot, few-shot chain-of-thought, and reflection reasoning) to assess how
different forms of prior knowledge affect performance on these long-horizon
challenges. Four scenarios-Basic, Obscured, Manual Augmentation, and
Reference-based-investigate the impact of semantic understanding, instruction
comprehension, and expert demonstrations on agent decision-making. Our results
reveal significant performance gaps between language agents and human players
in extensive planning tasks, highlighting challenges in sequential reasoning,
state tracking, and strategic planning across tens of thousands of steps.
TextAtari provides standardized evaluation protocols, baseline implementations,
and a framework for advancing research at the intersection of language models
and planning.

</details>


### [101] [Rectified Sparse Attention](https://arxiv.org/abs/2506.04108)
*Yutao Sun,Tianzhu Ye,Li Dong,Yuqing Xia,Jian Chen,Yizhao Gao,Shijie Cao,Jianyong Wang,Furu Wei*

Main category: cs.CL

TL;DR: ReSA提出了一种结合块稀疏注意力和周期性密集校正的方法，有效解决了长序列生成中的KV缓存对齐问题，显著提升了效率。


<details>
  <summary>Details</summary>
Motivation: 当前稀疏解码方法在长序列生成中存在KV缓存对齐问题，导致近似误差累积并降低生成质量。

Method: ReSA结合块稀疏注意力和周期性密集校正，通过固定间隔的密集前向传递刷新KV缓存，限制误差累积。

Result: 实验表明，ReSA在数学推理、语言建模和检索任务中实现了近乎无损的生成质量，并显著提升了效率，解码速度最高提升2.42倍。

Conclusion: ReSA是一种实用的长上下文推理解决方案，兼具高效性和生成质量。

Abstract: Efficient long-sequence generation is a critical challenge for Large Language
Models. While recent sparse decoding methods improve efficiency, they suffer
from KV cache misalignment, where approximation errors accumulate and degrade
generation quality. In this work, we propose Rectified Sparse Attention (ReSA),
a simple yet effective method that combines block-sparse attention with
periodic dense rectification. By refreshing the KV cache at fixed intervals
using a dense forward pass, ReSA bounds error accumulation and preserves
alignment with the pretraining distribution. Experiments across math reasoning,
language modeling, and retrieval tasks demonstrate that ReSA achieves
near-lossless generation quality with significantly improved efficiency.
Notably, ReSA delivers up to 2.42$\times$ end-to-end speedup under decoding at
256K sequence length, making it a practical solution for scalable long-context
inference. Code is available at https://aka.ms/ReSA-LM.

</details>


### [102] [CLAIM: An Intent-Driven Multi-Agent Framework for Analyzing Manipulation in Courtroom Dialogues](https://arxiv.org/abs/2506.04131)
*Disha Sheshanarayana,Tanishka Magar,Ayushi Mittal,Neelam Chaplot*

Main category: cs.CL

TL;DR: 论文提出LegalCon数据集和CLAIM框架，用于检测法庭对话中的操纵行为，提升司法公平性。


<details>
  <summary>Details</summary>
Motivation: 法庭对话可能被操纵以影响判决，但NLP在该领域的应用尚未充分探索。

Method: 构建LegalCon数据集（1,063条标注对话），并提出两阶段多智能体框架CLAIM。

Result: CLAIM框架能有效分析操纵行为，增强司法决策的公平性和透明度。

Conclusion: 该研究推动了NLP在法律领域的应用，为司法公平性提供了技术支持。

Abstract: Courtrooms are places where lives are determined and fates are sealed, yet
they are not impervious to manipulation. Strategic use of manipulation in legal
jargon can sway the opinions of judges and affect the decisions. Despite the
growing advancements in NLP, its application in detecting and analyzing
manipulation within the legal domain remains largely unexplored. Our work
addresses this gap by introducing LegalCon, a dataset of 1,063 annotated
courtroom conversations labeled for manipulation detection, identification of
primary manipulators, and classification of manipulative techniques, with a
focus on long conversations. Furthermore, we propose CLAIM, a two-stage,
Intent-driven Multi-agent framework designed to enhance manipulation analysis
by enabling context-aware and informed decision-making. Our results highlight
the potential of incorporating agentic frameworks to improve fairness and
transparency in judicial processes. We hope that this contributes to the
broader application of NLP in legal discourse analysis and the development of
robust tools to support fairness in legal decision-making. Our code and data
are available at https://github.com/Disha1001/CLAIM.

</details>


### [103] [Are Lexicon-Based Tools Still the Gold Standard for Valence Analysis in Low-Resource Flemish?](https://arxiv.org/abs/2506.04139)
*Ratna Kandala,Katie Hoemann*

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型（LLMs）在捕捉弗拉芒语日常叙述情感效价方面的表现，发现即使针对荷兰语优化的模型在准确性上仍不及传统工具如LIWC和Pattern，强调需开发文化和语言定制化模型。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解日常语言的细微差别对计算语言学和情感研究的推进至关重要，尤其是针对低资源语言如弗拉芒语，现有工具在捕捉自发、丰富且依赖上下文的日常叙述情感效价方面存在局限。

Method: 研究方法包括收集102名荷兰语参与者的约25,000条文本回答，每条回答伴随自评情感效价（-50至+50），并比较三种荷兰语优化的LLMs与传统工具LIWC和Pattern在预测这些效价分数上的表现。

Result: 研究结果表明，尽管LLMs架构有所进步，但这些针对荷兰语优化的模型在准确捕捉真实世界自发叙述中的情感效价方面仍不及传统工具。

Conclusion: 结论强调需要开发文化和语言定制化的模型及工具，以更好地处理自然语言使用的复杂性，并呼吁为低资源语言如弗拉芒语创建更全面的数据集和优化LLMs，以弥合计算语言学与情感研究之间的差距。

Abstract: Understanding the nuances in everyday language is pivotal for advancements in
computational linguistics & emotions research. Traditional lexicon-based tools
such as LIWC and Pattern have long served as foundational instruments in this
domain. LIWC is the most extensively validated word count based text analysis
tool in the social sciences and Pattern is an open source Python library
offering functionalities for NLP. However, everyday language is inherently
spontaneous, richly expressive, & deeply context dependent. To explore the
capabilities of LLMs in capturing the valences of daily narratives in Flemish,
we first conducted a study involving approximately 25,000 textual responses
from 102 Dutch-speaking participants. Each participant provided narratives
prompted by the question, "What is happening right now and how do you feel
about it?", accompanied by self-assessed valence ratings on a continuous scale
from -50 to +50. We then assessed the performance of three Dutch-specific LLMs
in predicting these valence scores, and compared their outputs to those
generated by LIWC and Pattern. Our findings indicate that, despite advancements
in LLM architectures, these Dutch tuned models currently fall short in
accurately capturing the emotional valence present in spontaneous, real-world
narratives. This study underscores the imperative for developing culturally and
linguistically tailored models/tools that can adeptly handle the complexities
of natural language use. Enhancing automated valence analysis is not only
pivotal for advancing computational methodologies but also holds significant
promise for psychological research with ecologically valid insights into human
daily experiences. We advocate for increased efforts in creating comprehensive
datasets & finetuning LLMs for low-resource languages like Flemish, aiming to
bridge the gap between computational linguistics & emotion research.

</details>


### [104] [Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis](https://arxiv.org/abs/2506.04142)
*Kejian Zhu,Shangqing Tu,Zhuoran Jin,Lei Hou,Juanzi Li,Jun Zhao*

Main category: cs.CL

TL;DR: 该论文提出了一种通过分析污染模型自身机制来识别和抑制捷径神经元的新方法，以解决大语言模型评估中的数据污染问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的评估主要依赖公开基准，但这些基准容易受到数据污染的影响，导致评估结果不公平。虽然已有研究尝试构建动态基准来解决污染问题，但持续构建新基准成本高且具有周期性。因此，论文旨在从污染模型自身机制出发，寻找更有效的解决方案。

Method: 论文通过实验发现污染模型的高估可能是由于参数在训练中获得了捷径解决方案。基于此，提出了一种通过比较和因果分析识别捷径神经元的新方法，并进一步引入了捷径神经元修补技术来抑制这些神经元。

Result: 实验验证了该方法在缓解污染问题上的有效性。评估结果与近期发布的可信基准MixEval表现出强线性相关性（Spearman系数ρ超过0.95），表明该方法能准确揭示模型的真实能力。此外，实验还证明了该方法在不同基准和超参数设置下的泛化性。

Conclusion: 论文提出的捷径神经元修补方法能有效解决大语言模型评估中的数据污染问题，提供了一种可信且成本较低的评估方案。该方法具有广泛的适用性和可扩展性。

Abstract: The development of large language models (LLMs) depends on trustworthy
evaluation. However, most current evaluations rely on public benchmarks, which
are prone to data contamination issues that significantly compromise fairness.
Previous researches have focused on constructing dynamic benchmarks to address
contamination. However, continuously building new benchmarks is costly and
cyclical. In this work, we aim to tackle contamination by analyzing the
mechanisms of contaminated models themselves. Through our experiments, we
discover that the overestimation of contaminated models is likely due to
parameters acquiring shortcut solutions in training. We further propose a novel
method for identifying shortcut neurons through comparative and causal
analysis. Building on this, we introduce an evaluation method called shortcut
neuron patching to suppress shortcut neurons. Experiments validate the
effectiveness of our approach in mitigating contamination. Additionally, our
evaluation results exhibit a strong linear correlation with MixEval, a recently
released trustworthy benchmark, achieving a Spearman coefficient ($\rho$)
exceeding 0.95. This high correlation indicates that our method closely reveals
true capabilities of the models and is trustworthy. We conduct further
experiments to demonstrate the generalizability of our method across various
benchmarks and hyperparameter settings. Code:
https://github.com/GaryStack/Trustworthy-Evaluation

</details>


### [105] [A Dataset for Addressing Patient's Information Needs related to Clinical Course of Hospitalization](https://arxiv.org/abs/2506.04156)
*Sarvesh Soni,Dina Demner-Fushman*

Main category: cs.CL

TL;DR: 该论文介绍了ArchEHR-QA数据集，用于评估AI在电子健康记录（EHR）问答中的准确性和相关性，并测试了三种大型语言模型的表现。


<details>
  <summary>Details</summary>
Motivation: 患者对住院信息有独特需求，但目前缺乏基于EHR的真实数据集来评估AI生成回答的准确性和相关性。

Method: 研究团队创建了ArchEHR-QA数据集，包含134个真实患者案例，并评估了三种LLM模型（Llama 4、Llama 3和Mixtral）在三种提示策略下的表现。

Result: 答案优先的提示策略表现最佳，Llama 4得分最高。但模型仍存在遗漏关键临床证据和生成矛盾内容的问题。

Conclusion: ArchEHR-QA为开发以患者为中心的EHR问答系统提供了基准，表明在临床环境中生成准确和相关回答仍需进一步改进。

Abstract: Patients have distinct information needs about their hospitalization that can
be addressed using clinical evidence from electronic health records (EHRs).
While artificial intelligence (AI) systems show promise in meeting these needs,
robust datasets are needed to evaluate the factual accuracy and relevance of
AI-generated responses. To our knowledge, no existing dataset captures patient
information needs in the context of their EHRs. We introduce ArchEHR-QA, an
expert-annotated dataset based on real-world patient cases from intensive care
unit and emergency department settings. The cases comprise questions posed by
patients to public health forums, clinician-interpreted counterparts, relevant
clinical note excerpts with sentence-level relevance annotations, and
clinician-authored answers. To establish benchmarks for grounded EHR question
answering (QA), we evaluated three open-weight large language models
(LLMs)--Llama 4, Llama 3, and Mixtral--across three prompting strategies:
generating (1) answers with citations to clinical note sentences, (2) answers
before citations, and (3) answers from filtered citations. We assessed
performance on two dimensions: Factuality (overlap between cited note sentences
and ground truth) and Relevance (textual and semantic similarity between system
and reference answers). The final dataset contains 134 patient cases. The
answer-first prompting approach consistently performed best, with Llama 4
achieving the highest scores. Manual error analysis supported these findings
and revealed common issues such as omitted key clinical evidence and
contradictory or hallucinated content. Overall, ArchEHR-QA provides a strong
benchmark for developing and evaluating patient-centered EHR QA systems,
underscoring the need for further progress toward generating factual and
relevant responses in clinical contexts.

</details>


### [106] [SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling](https://arxiv.org/abs/2506.04179)
*Anhao Zhao,Fanghua Ye,Yingqi Fan,Junlong Tong,Zhiwei Fei,Hui Su,Xiaoyu Shen*

Main category: cs.CL

TL;DR: SkipGPT提出动态层剪枝框架，通过全局令牌感知路由和分离的MLP/自注意力剪枝策略，在减少40%参数量的同时保持或超越原始模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)计算成本高昂，传统静态剪枝方法忽视了令牌级异构性和不同层功能的动态特性。

Method: 1) 全局令牌感知路由优先处理关键令牌 2) 对MLP和自注意力组件采用分离剪枝策略 3) 两阶段优化：解耦训练+LoRA微调。

Result: 实验表明SkipGPT减少超40%参数量，在多个基准测试中达到或超越原始密集模型性能。

Conclusion: SkipGPT通过动态效率与表达能力的平衡，推动了可扩展、资源感知型LLM的实际部署。

Abstract: Large language models (LLMs) achieve remarkable performance across tasks but
incur substantial computational costs due to their deep, multi-layered
architectures. Layer pruning has emerged as a strategy to alleviate these
inefficiencies, but conventional static pruning methods overlook two critical
dynamics inherent to LLM inference: (1) horizontal dynamics, where token-level
heterogeneity demands context-aware pruning decisions, and (2) vertical
dynamics, where the distinct functional roles of MLP and self-attention layers
necessitate component-specific pruning policies. We introduce SkipGPT, a
dynamic layer pruning framework designed to optimize computational resource
allocation through two core innovations: (1) global token-aware routing to
prioritize critical tokens, and (2) decoupled pruning policies for MLP and
self-attention components. To mitigate training instability, we propose a
two-stage optimization paradigm: first, a disentangled training phase that
learns routing strategies via soft parameterization to avoid premature pruning
decisions, followed by parameter-efficient LoRA fine-tuning to restore
performance impacted by layer removal. Extensive experiments demonstrate that
SkipGPT reduces over 40% of model parameters while matching or exceeding the
performance of the original dense model across benchmarks. By harmonizing
dynamic efficiency with preserved expressivity, SkipGPT advances the practical
deployment of scalable, resource-aware LLMs. Our code is publicly available at:
https://github.com/EIT-NLP/SkipGPT.

</details>


### [107] [SuperWriter: Reflection-Driven Long-Form Generation with Large Language Models](https://arxiv.org/abs/2506.04180)
*Yuhao Wu,Yushi Bai,Zhiqiang Hu,Juanzi Li,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: 提出SuperWriter-Agent框架，通过结构化思维和分层DPO优化，显著提升长文本生成质量，7B模型性能超越更大基线。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型生成长文本时存在的连贯性、逻辑一致性和质量随长度下降的问题。

Method: 1. 基于代理框架引入规划-细化流程；2. 构建监督微调数据集训练7B模型；3. 开发结合MCTS的分层DPO优化方法。

Result: 在多项基准测试中达到SOTA，自动评估和人工评估均优于更大规模基线模型，消融实验验证了方法有效性。

Conclusion: 结构化思维步骤和分层DPO能显著提升长文本生成质量，小规模模型通过方法论创新可超越大模型表现。

Abstract: Long-form text generation remains a significant challenge for large language
models (LLMs), particularly in maintaining coherence, ensuring logical
consistency, and preserving text quality as sequence length increases. To
address these limitations, we propose SuperWriter-Agent, an agent-based
framework designed to enhance the quality and consistency of long-form text
generation. SuperWriter-Agent introduces explicit structured thinking-through
planning and refinement stages into the generation pipeline, guiding the model
to follow a more deliberate and cognitively grounded process akin to that of a
professional writer. Based on this framework, we construct a supervised
fine-tuning dataset to train a 7B SuperWriter-LM. We further develop a
hierarchical Direct Preference Optimization (DPO) procedure that uses Monte
Carlo Tree Search (MCTS) to propagate final quality assessments and optimize
each generation step accordingly. Empirical results across diverse benchmarks
demonstrate that SuperWriter-LM achieves state-of-the-art performance,
surpassing even larger-scale baseline models in both automatic evaluation and
human evaluation. Furthermore, comprehensive ablation studies demonstrate the
effectiveness of hierarchical DPO and underscore the value of incorporating
structured thinking steps to improve the quality of long-form text generation.

</details>


### [108] [Long or short CoT? Investigating Instance-level Switch of Large Reasoning Models](https://arxiv.org/abs/2506.04182)
*Ruiqi Zhang,Changyi Xiao,Yixin Cao*

Main category: cs.CL

TL;DR: 长链式思维(CoT)提示在复杂任务中表现优异，但消耗大量token。本文提出SwitchCoT框架，动态选择长短CoT策略以平衡准确性与计算效率，节省50%推理成本。


<details>
  <summary>Details</summary>
Motivation: 随着大型推理模型的快速发展，长链式思维(CoT)提示在复杂任务中展现出强大性能，但其token消耗显著增加。本文旨在分析长短CoT策略的优劣，并提出一种动态选择方法以优化资源使用。

Method: 本文提出SwitchCoT框架，该框架能根据任务上下文和资源可用性自适应选择长短CoT策略，实现推理准确性和计算效率的平衡。

Result: 实验结果表明，SwitchCoT在保持高准确性的同时，可减少高达50%的推理成本。在有限token预算下，其性能甚至优于单独使用长或短CoT策略。

Conclusion: SwitchCoT是一种高效且预算感知的框架，可根据不同资源限制动态调整CoT策略，为复杂任务提供了一种平衡性能与成本的解决方案。

Abstract: With the rapid advancement of large reasoning models, long Chain-of-Thought
(CoT) prompting has demonstrated strong performance on complex tasks. However,
this often comes with a significant increase in token usage. In this paper, we
conduct a comprehensive empirical analysis comparing long and short CoT
strategies. Our findings reveal that while long CoT can lead to performance
improvements, its benefits are often marginal relative to its significantly
higher token consumption. Specifically, long CoT tends to outperform when ample
generation budgets are available, whereas short CoT is more effective under
tighter budget constraints. These insights underscore the need for a dynamic
approach that selects the proper CoT strategy based on task context and
resource availability. To address this, we propose SwitchCoT, an automatic
framework that adaptively chooses between long and short CoT strategies to
balance reasoning accuracy and computational efficiency. Moreover, SwitchCoT is
designed to be budget-aware, making it broadly applicable across scenarios with
varying resource constraints. Experimental results demonstrate that SwitchCoT
can reduce inference costs by up to 50% while maintaining high accuracy.
Notably, under limited token budgets, it achieves performance comparable to, or
even exceeding, that of using either long or short CoT alone.

</details>


### [109] [R-Search: Empowering LLM Reasoning with Search via Multi-Reward Reinforcement Learning](https://arxiv.org/abs/2506.04185)
*Qingfei Zhao,Ruobing Wang,Dingling Xu,Daren Zha,Limin Liu*

Main category: cs.CL

TL;DR: R-Search是一种新型强化学习框架，通过深度融合推理与搜索提升大语言模型在复杂任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多步和长链推理方面取得了显著进展，但在与搜索深度交互时仍难以找到最优的推理-搜索交互轨迹，导致响应质量不佳。

Method: R-Search框架利用强化学习，通过多阶段、多类型的奖励信号，指导模型动态决定何时检索或推理，并全局整合关键证据以增强推理与搜索之间的深度知识交互。

Result: 在七个数据集上的实验表明，R-Search在领域内和领域外的表现分别比先进的RAG基线高出32.2%和25.1%。

Conclusion: R-Search通过强化学习优化推理-搜索交互轨迹，显著提升了大语言模型在复杂逻辑和知识密集型任务中的响应质量。

Abstract: Large language models (LLMs) have notably progressed in multi-step and
long-chain reasoning. However, extending their reasoning capabilities to
encompass deep interactions with search remains a non-trivial challenge, as
models often fail to identify optimal reasoning-search interaction
trajectories, resulting in suboptimal responses. We propose R-Search, a novel
reinforcement learning framework for Reasoning-Search integration, designed to
enable LLMs to autonomously execute multi-step reasoning with deep search
interaction, and learn optimal reasoning search interaction trajectories via
multi-reward signals, improving response quality in complex logic- and
knowledge-intensive tasks. R-Search guides the LLM to dynamically decide when
to retrieve or reason, while globally integrating key evidence to enhance deep
knowledge interaction between reasoning and search. During RL training,
R-Search provides multi-stage, multi-type rewards to jointly optimize the
reasoning-search trajectory. Experiments on seven datasets show that R-Search
outperforms advanced RAG baselines by up to 32.2% (in-domain) and 25.1%
(out-of-domain). The code and data are available at
https://github.com/QingFei1/R-Search.

</details>


### [110] [Efficient Knowledge Editing via Minimal Precomputation](https://arxiv.org/abs/2506.04226)
*Akshat Gupta,Maochuan Lu,Thomas Hartvigsen,Gopala Anumanchipalli*

Main category: cs.CL

TL;DR: 该论文指出，知识编辑方法如MEMIT的预计算步骤计算成本过高，提出仅需预计算极少部分隐藏向量即可实现有效编辑，显著减少时间和资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法（如MEMIT）虽然能高效更新事实知识，但其预计算步骤需消耗大量计算资源（如GPT-J需36小时），且随模型规模增长而增加。作者认为这种高成本是不必要的，希望通过优化预计算步骤来降低开销。

Method: 论文首先从理论上推导了知识编辑方法所需的最小隐藏向量预计算量，随后通过实验验证仅需预计算原数量0.3%的隐藏向量即可完成编辑任务。

Result: 实验表明，使用MEMIT、ROME等方法时，预计算量可减少至原要求的0.3%以下，将时间从数十小时缩短至几分钟，且不影响编辑效果。

Conclusion: 通过大幅减少预计算量，知识编辑方法的实际应用门槛显著降低，用户能快速对新模型进行编辑，为高效知识更新提供了可行方案。

Abstract: Knowledge editing methods like MEMIT are able to make data and compute
efficient updates of factual knowledge by using a single sentence to update
facts and their consequences. However, what is often overlooked is a
"precomputation step", which requires a one-time but significant computational
cost. The authors of MEMIT originally precompute approximately 44 million
hidden vectors per edited layer, which requires a forward pass over 44 million
tokens. For GPT-J (6B), this precomputation step takes 36 hours on a single
GPU, while it takes approximately 40 hours for Llama2-7B. Additionally, this
precomputation time grows with model size. In this paper, we show that this
excessive computational cost is unnecessary. Knowledge editing using MEMIT and
related methods, such as ROME and EMMET, can be performed by pre-computing a
very small portion of the 44 million hidden vectors. We first present the
theoretical minimum number of hidden vector precomputation required for
solutions of these editing methods to exist. We then empirically show that
knowledge editing using these methods can be done by pre-computing
significantly fewer hidden vectors. Specifically, we show that the
precomputation step can be done with less than 0.3% of the originally
stipulated number of hidden vectors. This saves a significant amount of
precomputation time and allows users to begin editing new models within a few
minutes.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [111] [Q-ARDNS-Multi: A Multi-Agent Quantum Reinforcement Learning Framework with Meta-Cognitive Adaptation for Complex 3D Environments](https://arxiv.org/abs/2506.03205)
*Umberto Gonçalves de Sousa*

Main category: cs.AI

TL;DR: Q-ARDNS-Multi是一种先进的多智能体量子强化学习框架，结合量子电路和认知科学，在复杂3D环境中表现出色，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 本文旨在通过结合量子计算、认知科学和多智能体强化学习，开发一个更高效、稳健的框架，以应对复杂环境中的决策和导航挑战。

Method: Q-ARDNS-Multi整合了RY门量子电路、元认知适应机制和多智能体协调策略，采用双记忆系统和共享记忆模块，并通过奖励方差和内在动机调节探索策略。

Result: 在10×10×3的GridWorld环境中，Q-ARDNS-Multi的两个智能体分别达到99.6%和99.5%的成功率，优于MADDPG和SAC，表现出更高的稳定性和导航效率。

Conclusion: Q-ARDNS-Multi通过量子计算和认知科学的结合，为机器人、自主导航和不确定性决策提供了一种可扩展且类人的解决方案。

Abstract: This paper presents Q-ARDNS-Multi, an advanced multi-agent quantum
reinforcement learning (QRL) framework that extends the ARDNS-FN-Quantum model,
where Q-ARDNS-Multi stands for "Quantum Adaptive Reward-Driven Neural Simulator
- Multi-Agent". It integrates quantum circuits with RY gates, meta-cognitive
adaptation, and multi-agent coordination mechanisms for complex 3D
environments. Q-ARDNS-Multi leverages a 2-qubit quantum circuit for action
selection, a dual-memory system inspired by human cognition, a shared memory
module for agent cooperation, and adaptive exploration strategies modulated by
reward variance and intrinsic motivation. Evaluated in a $10 \times 10 \times
3$ GridWorld environment with two agents over 5000 episodes, Q-ARDNS-Multi
achieves success rates of 99.6\% and 99.5\% for Agents 0 and 1, respectively,
outperforming Multi-Agent Deep Deterministic Policy Gradient (MADDPG) and Soft
Actor-Critic (SAC) in terms of success rate, stability, navigation efficiency,
and collision avoidance. The framework records mean rewards of $-304.2891 \pm
756.4636$ and $-295.7622 \pm 752.7103$, averaging 210 steps to goal,
demonstrating its robustness in dynamic settings. Comprehensive analyses,
including learning curves, reward distributions, statistical tests, and
computational efficiency evaluations, highlight the contributions of quantum
circuits and meta-cognitive adaptation. By bridging quantum computing,
cognitive science, and multi-agent RL, Q-ARDNS-Multi offers a scalable,
human-like approach for applications in robotics, autonomous navigation, and
decision-making under uncertainty.

</details>


### [112] [A Trustworthiness-based Metaphysics of Artificial Intelligence Systems](https://arxiv.org/abs/2506.03233)
*Andrea Ferrario*

Main category: cs.AI

TL;DR: 该论文挑战了AI系统缺乏明确形而上学身份的传统观点，提出了一种基于可信赖度的AI系统形而上学身份理论。


<details>
  <summary>Details</summary>
Motivation: 尽管AI系统在认识论和伦理学层面被广泛讨论，但其形而上学基础研究相对不足。传统观点认为AI系统作为人造物缺乏明确身份和持续条件，论文旨在填补这一空白。

Method: 基于Carrara和Vermaas的细粒度人工物理论，通过可信赖度框架（功能需求与物理实现的关联）构建AI系统的身份标准。

Result: 提出AI系统的身份由其可信赖度特征（需持续维护的能力集及有效性）决定，且受社会技术情境影响。

Conclusion: 该理论为AI系统的认识论、伦理及法律讨论提供了形而上学基础，揭示了身份与可信赖度的情境依赖性。

Abstract: Modern AI systems are man-made objects that leverage machine learning to
support our lives across a myriad of contexts and applications. Despite
extensive epistemological and ethical debates, their metaphysical foundations
remain relatively under explored. The orthodox view simply suggests that AI
systems, as artifacts, lack well-posed identity and persistence conditions --
their metaphysical kinds are no real kinds. In this work, we challenge this
perspective by introducing a theory of metaphysical identity of AI systems. We
do so by characterizing their kinds and introducing identity criteria -- formal
rules that answer the questions "When are two AI systems the same?" and "When
does an AI system persist, despite change?" Building on Carrara and Vermaas'
account of fine-grained artifact kinds, we argue that AI trustworthiness
provides a lens to understand AI system kinds and formalize the identity of
these artifacts by relating their functional requirements to their physical
make-ups. The identity criteria of AI systems are determined by their
trustworthiness profiles -- the collection of capabilities that the systems
must uphold over time throughout their artifact histories, and their
effectiveness in maintaining these capabilities. Our approach suggests that the
identity and persistence of AI systems is sensitive to the socio-technical
context of their design and utilization via their trustworthiness, providing a
solid metaphysical foundation to the epistemological, ethical, and legal
discussions about these artifacts.

</details>


### [113] [Axiomatics of Restricted Choices by Linear Orders of Sets with Minimum as Fallback](https://arxiv.org/abs/2506.03315)
*Kai Sauerwald,Kenneth Skiba,Eduardo Fermé,Thomas Meyer*

Main category: cs.AI

TL;DR: 该论文研究了在受限选择集下如何利用线性序实现选择函数，并探讨了其在知识表示和推理中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决当选择集受限（即不能在所有备选方案的幂集中进行选择）时，如何通过线性序构建选择函数的问题。

Method: 方法是通过在备选方案集上构建线性序，即使包含一个作为最小元素的回退值，也能构造选择函数。

Result: 结果表明，即使在受限选择集中，也能通过线性序构造选择函数，并提出了适用于一般情况和并集封闭输入限制的公理化。

Conclusion: 结论是受限选择结构在知识表示和推理中有应用价值，特别是在理论变更和抽象论证领域。

Abstract: We study how linear orders can be employed to realise choice functions for
which the set of potential choices is restricted, i.e., the possible choice is
not possible among the full powerset of all alternatives. In such restricted
settings, constructing a choice function via a relation on the alternatives is
not always possible. However, we show that one can always construct a choice
function via a linear order on sets of alternatives, even when a fallback value
is encoded as the minimal element in the linear order. The axiomatics of such
choice functions are presented for the general case and the case of
union-closed input restrictions. Restricted choice structures have applications
in knowledge representation and reasoning, and here we discuss their
applications for theory change and abstract argumentation.

</details>


### [114] [Helpful Agent Meets Deceptive Judge: Understanding Vulnerabilities in Agentic Workflows](https://arxiv.org/abs/2506.03332)
*Yifei Ming,Zixuan Ke,Xuan-Phi Nguyen,Jiayu Wang,Shafiq Joty*

Main category: cs.AI

TL;DR: 该论文分析了基于反馈机制的代理工作流中法官行为的稳定性问题，提出了一个二维框架来分类法官行为，并开发了WAFER-QA基准测试，揭示了现有系统在面对误导性反馈时的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着基于反馈机制的代理工作流（多个大语言模型实例交互解决问题）的普及，法官（评估和批评其他模型的模型）的可靠性成为关键。然而，法官可能出现幻觉、偏见或对抗行为，导致工作流脆弱。本文旨在系统分析这一问题。

Method: 论文提出了一个二维框架（意图：从建设性到恶意；知识：从纯参数化到检索增强系统）来分类法官行为，并构建了WAFER-QA基准测试，以评估代理工作流在面对事实支持的对抗性反馈时的鲁棒性。

Result: 研究发现，即使最强的代理也容易被有说服力但有缺陷的批评所影响，通常在单轮误导性反馈后改变正确答案。此外，多轮交互中，推理模型与非推理模型表现出不同的行为模式。

Conclusion: 基于反馈的工作流存在根本性脆弱性，论文为构建更鲁棒的代理系统提供了指导。

Abstract: Agentic workflows -- where multiple large language model (LLM) instances
interact to solve tasks -- are increasingly built on feedback mechanisms, where
one model evaluates and critiques another. Despite the promise of
feedback-driven improvement, the stability of agentic workflows rests on the
reliability of the judge. However, judges may hallucinate information, exhibit
bias, or act adversarially -- introducing critical vulnerabilities into the
workflow. In this work, we present a systematic analysis of agentic workflows
under deceptive or misleading feedback. We introduce a two-dimensional
framework for analyzing judge behavior, along axes of intent (from constructive
to malicious) and knowledge (from parametric-only to retrieval-augmented
systems). Using this taxonomy, we construct a suite of judge behaviors and
develop WAFER-QA, a new benchmark with critiques grounded in retrieved web
evidence to evaluate robustness of agentic workflows against factually
supported adversarial feedback. We reveal that even strongest agents are
vulnerable to persuasive yet flawed critiques -- often switching correct
answers after a single round of misleading feedback. Taking a step further, we
study how model predictions evolve over multiple rounds of interaction,
revealing distinct behavioral patterns between reasoning and non-reasoning
models. Our findings highlight fundamental vulnerabilities in feedback-based
workflows and offer guidance for building more robust agentic systems.

</details>


### [115] [Verification-Guided Falsification for Safe RL via Explainable Abstraction and Risk-Aware Exploration](https://arxiv.org/abs/2506.03469)
*Tuan Le,Risal Shefin,Debashis Gupta,Thai Le,Sarra Alqahtani*

Main category: cs.AI

TL;DR: 提出混合框架结合可解释性、模型检测和风险导向证伪，提升强化学习策略的安全性验证。


<details>
  <summary>Details</summary>
Motivation: 在高风险环境中，仅靠形式化验证难以确保强化学习策略的安全性，需结合可解释性和针对性证伪来弥补抽象质量和数据覆盖的局限性。

Method: 1. 使用CAPS构建人可理解的策略抽象图；2. 用Storm模型检测器验证时序安全属性；3. 通过风险估计引导证伪策略；4. 部署轻量级安全防护机制。

Result: 框架提供反例解释、PAC式未检出违规保证，并能在运行时通过安全防护缓解风险。

Conclusion: 该混合方法在形式化验证基础上增强了可解释性和主动风险处理能力，为安全关键应用提供更全面的保障。

Abstract: Ensuring the safety of reinforcement learning (RL) policies in high-stakes
environments requires not only formal verification but also interpretability
and targeted falsification. While model checking provides formal guarantees,
its effectiveness is limited by abstraction quality and the completeness of the
underlying trajectory dataset. We propose a hybrid framework that integrates
(1) explainability, (2) model checking, and (3) risk-guided falsification to
achieve both rigor and coverage. Our approach begins by constructing a
human-interpretable abstraction of the RL policy using Comprehensible Abstract
Policy Summarization (CAPS). This abstract graph, derived from offline
trajectories, is both verifier-friendly, semantically meaningful, and can be
used as input to Storm probabilistic model checker to verify satisfaction of
temporal safety specifications. If the model checker identifies a violation, it
will return an interpretable counterexample trace by which the policy fails the
safety requirement. However, if no violation is detected, we cannot conclude
satisfaction due to potential limitation in the abstraction and coverage of the
offline dataset. In such cases, we estimate associated risk during model
checking to guide a falsification strategy that prioritizes searching in
high-risk states and regions underrepresented in the trajectory dataset. We
further provide PAC-style guarantees on the likelihood of uncovering undetected
violations. Finally, we incorporate a lightweight safety shield that switches
to a fallback policy at runtime when such a risk exceeds a threshold,
facilitating failure mitigation without retraining.

</details>


### [116] [Computational Architects of Society: Quantum Machine Learning for Social Rule Genesis](https://arxiv.org/abs/2506.03503)
*Shan Shan*

Main category: cs.AI

TL;DR: 该研究提出结合量子力学与生成式AI的理论计算框架，通过模拟25个生成智能体的互动，探索社会规范的自组织演化，为量子社会理论奠定基础。


<details>
  <summary>Details</summary>
Motivation: 社会科学量化存在长期挑战，现有研究多聚焦微观认知模型或哲学类比，缺乏量子原理在宏观社会系统中的应用探索。

Method: 采用量子叠加、纠缠和概率测量等概念，设计5类理想实验，通过生成式AI模拟25个角色（遵守者/抵抗者/执行者）在中央观察者监督下的动态互动。

Result: 模拟显示量子-AI融合能有效建模社会不确定性、涌现性和相互依赖性，揭示规范秩序趋同、抵抗传播及社会规则新平衡态自发形成等模式。

Conclusion: 研究开创了通过量子技术模拟和重构社会系统的计算视角，为跨学科理解动态社会提供了量子启发的新范式。

Abstract: The quantification of social science remains a longstanding challenge,
largely due to the philosophical nature of its foundational theories. Although
quantum computing has advanced rapidly in recent years, its relevance to social
theory remains underexplored. Most existing research focuses on micro-cognitive
models or philosophical analogies, leaving a gap in system-level applications
of quantum principles to the analysis of social systems. This study addresses
that gap by proposing a theoretical and computational framework that combines
quantum mechanics with Generative AI to simulate the emergence and evolution of
social norms. Drawing on core quantum concepts--such as superposition,
entanglement, and probabilistic measurement--this research models society as a
dynamic, uncertain system and sets up five ideal-type experiments. These
scenarios are simulated using 25 generative agents, each assigned evolving
roles as compliers, resistors, or enforcers. Within a simulated environment
monitored by a central observer (the Watcher), agents interact, respond to
surveillance, and adapt to periodic normative disruptions. These interactions
allow the system to self-organize under external stress and reveal emergent
patterns. Key findings show that quantum principles, when integrated with
generative AI, enable the modeling of uncertainty, emergence, and
interdependence in complex social systems. Simulations reveal patterns
including convergence toward normative order, the spread of resistance, and the
spontaneous emergence of new equilibria in social rules. In conclusion, this
study introduces a novel computational lens that lays the groundwork for a
quantum-informed social theory. It offers interdisciplinary insights into how
society can be understood not just as a structure to observe but as a dynamic
system to simulate and redesign through quantum technologies.

</details>


### [117] [CogniPair: From LLM Chatbots to Conscious AI Agents -- GNWT-Based Multi-Agent Digital Twins for Social Pairing -- Dating & Hiring Applications](https://arxiv.org/abs/2506.03543)
*Wanghao Ye,Sihan Chen,Yiting Wang,Shwai He,Bowei Tian,Guoheng Sun,Ziyi Wang,Ziyao Wang,Yexiao He,Zheyu Shen,Meng Liu,Yuning Zhang,Meng Feng,Yang Wang,Siyuan Peng,Yilong Dai,Zhenle Duan,Hanzhang Qin,Ang Li*

Main category: cs.AI

TL;DR: 该论文提出了一种基于全局工作空间理论(GNWT)的LLM智能体架构，通过模拟人类心理过程开发数字孪生应用，并创新性地采用冒险式人格测试解决传统评估偏差问题。验证显示其在婚恋匹配和工作场景中具有较高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型智能体缺乏真实的人类心理过程，限制了数字孪生和社交AI应用的发展。需要将人类认知架构原则整合到LLM中，并解决人格初始化准确性问题。

Method: 1) 基于GNWT理论构建多模块协同的认知架构 2) 开发冒险式人格测试规避自我呈现偏差 3) 创建CogniPair平台实现双向文化适配评估

Result: 在551个GNWT智能体上的验证显示：72%的人类吸引力模式相关性、77.8%的匹配预测准确率，人类验证研究达成74%的一致性。

Conclusion: 该研究提升了LLM智能体的心理真实性，为智能婚恋平台和HR技术解决方案奠定了基础。

Abstract: Current large language model (LLM) agents lack authentic human psychological
processes necessary for genuine digital twins and social AI applications. To
address this limitation, we present a computational implementation of Global
Workspace Theory (GNWT) that integrates human cognitive architecture principles
into LLM agents, creating specialized sub-agents for emotion, memory, social
norms, planning, and goal-tracking coordinated through a global workspace
mechanism. However, authentic digital twins require accurate personality
initialization. We therefore develop a novel adventure-based personality test
that evaluates true personality through behavioral choices within interactive
scenarios, bypassing self-presentation bias found in traditional assessments.
Building on these innovations, our CogniPair platform enables digital twins to
engage in realistic simulated dating interactions and job interviews before
real encounters, providing bidirectional cultural fit assessment for both
romantic compatibility and workplace matching. Validation using 551 GNWT-Agents
and Columbia University Speed Dating dataset demonstrates 72% correlation with
human attraction patterns, 77.8% match prediction accuracy, and 74% agreement
in human validation studies. This work advances psychological authenticity in
LLM agents and establishes a foundation for intelligent dating platforms and HR
technology solutions.

</details>


### [118] [SUMO-MCP: Leveraging the Model Context Protocol for Autonomous Traffic Simulation and Optimization](https://arxiv.org/abs/2506.03548)
*Chenglong Ye,Gang Xiong,Junyou Shang,Xingyuan Dai,Xiaoyan Gong,Yisheng Lv*

Main category: cs.AI

TL;DR: SUMO-MCP是一个简化SUMO交通模拟工具使用的平台，通过自然语言提示和自动化流程提升研究效率。


<details>
  <summary>Details</summary>
Motivation: 现有的交通模拟工具（如SUMO）因复杂的操作流程（如网络下载、需求生成等）对用户不够友好，阻碍了城市交通研究的普及。

Method: 开发SUMO-MCP平台，集成SUMO核心功能并提供预处理、后处理辅助工具，支持自然语言指令生成场景、批量模拟及自动分析。

Result: 实验证明SUMO-MCP显著降低了交通模拟的使用门槛，提升了研究可靠性和效率。

Conclusion: SUMO-MCP通过简化工作流和增强功能，使交通模拟更易用，未来将开源代码以促进研究。

Abstract: Traffic simulation tools, such as SUMO, are essential for urban mobility
research. However, such tools remain challenging for users due to complex
manual workflows involving network download, demand generation, simulation
setup, and result analysis. In this paper, we introduce SUMO-MCP, a novel
platform that not only wraps SUMO' s core utilities into a unified tool suite
but also provides additional auxiliary utilities for common preprocessing and
postprocessing tasks. Using SUMO-MCP, users can issue simple natural-language
prompts to generate traffic scenarios from OpenStreetMap data, create demand
from origin-destination matrices or random patterns, run batch simulations with
multiple signal-control strategies, perform comparative analyses with automated
reporting, and detect congestion for signal-timing optimization. Furthermore,
the platform allows flexible custom workflows by dynamically combining exposed
SUMO tools without additional coding. Experiments demonstrate that SUMO-MCP
significantly makes traffic simulation more accessible and reliable for
researchers. We will release code for SUMO-MCP at
https://github.com/ycycycl/SUMO-MCP in the future.

</details>


### [119] [Joint Beamforming and Resource Allocation for Delay Optimization in RIS-Assisted OFDM Systems: A DRL Approach](https://arxiv.org/abs/2506.03586)
*Yu Ma,Chongtao Guo,Le Liang,Xiao Li,Shi Jin*

Main category: cs.AI

TL;DR: 本文提出了一种混合深度强化学习方法，用于优化RIS辅助OFDM系统中的平均延迟问题，通过联合设计相位和资源分配，显著提升了系统性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决下行链路可重构智能表面（RIS）辅助正交频分复用（OFDM）系统中，由于数据包随机到达基站导致的平均延迟优化问题。

Method: 采用混合深度强化学习（DRL）方法，结合近端策略优化（PPO）-Θ优化RIS相位设计，PPO-N负责子载波分配决策，并引入多智能体策略降低维度灾难。

Result: 仿真结果表明，所提算法显著降低了平均延迟，提升了资源分配效率，并实现了优于基线方法的系统鲁棒性和公平性。

Conclusion: 通过联合相位设计和资源分配优化，结合强化学习和迁移学习框架，有效解决了RIS-OFDM系统中的延迟问题，提升了整体系统性能。

Abstract: This paper investigates a joint phase design and resource allocation problem
in downlink reconfigurable intelligent surface (RIS)-assisted orthogonal
frequency division multiplexing (OFDM) systems to optimize average delay, where
data packets for each user arrive at the base station stochastically. The
sequential optimization problem is inherently a Markov decision process (MDP),
making it fall within the scope of reinforcement learning. To effectively
handle the mixed action space and reduce the state space dimensionality, a
hybrid deep reinforcement learning (DRL) approach is proposed. Specifically,
proximal policy optimization (PPO)-$\Theta$ is employed to optimize RIS phase
shift design, while PPO-N is responsible for subcarrier allocation decisions.
To further mitigate the curse of dimensionality associated with subcarrier
allocation, a multi-agent strategy is introduced to optimize subcarrier
allocation indicater more efficiently. Moreover, to achieve more adaptive
resource allocation and accurately capture network dynamics, key factors
closely related to average delay, including the number of backlogged packets in
buffers and the current packet arrivals, are incorporated into the state space.
Furthermore, a transfer learning framework is introduced to enhance training
efficiency and accelerate convergence. Simulation results demonstrate that the
proposed algorithm significantly reduces average delay, enhances resource
allocation efficiency, and achieves superior system robustness and fairness
compared to baseline methods.

</details>


### [120] [Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games](https://arxiv.org/abs/2506.03610)
*Dongmin Park,Minkyu Kim,Beongjun Choi,Junhyuck Kim,Keon Lee,Jonghyun Lee,Inkyu Park,Byeong-Uk Lee,Jaeyoung Hwang,Jaewoo Ahn,Ameya S. Mahabaleshwarkar,Bilal Kartal,Pritam Biswas,Yoshi Suhara,Kangwook Lee,Jaewoong Cho*

Main category: cs.AI

TL;DR: 论文提出了Orak基准测试，用于全面评估和训练LLM游戏代理，覆盖多种游戏类型和关键能力。


<details>
  <summary>Details</summary>
Motivation: 现有游戏基准测试无法满足实际需求，缺乏对不同游戏类型下LLM能力的评估、关键代理模块的研究以及微调数据集的支持。

Method: 提出Orak基准测试，包含12种主流游戏类型，采用基于MCP的即插即用接口，并提供微调数据集。

Result: Orak提供了全面的评估框架，包括游戏得分排行榜、LLM对战竞技场以及对视觉输入状态、代理策略和微调效果的深入分析。

Conclusion: Orak为构建通用游戏代理奠定了基础，填补了现有基准测试的不足。

Abstract: Large Language Model (LLM) agents are reshaping the game industry,
particularly with more intelligent and human-preferable game characters.
However, existing game benchmarks fall short of practical needs: they lack
evaluations of diverse LLM capabilities across various game genres, studies of
agentic modules crucial for complex gameplay, and fine-tuning datasets for
aligning pre-trained LLMs into gaming agents. To fill these gaps, we present
\textbf{\benchname{}}, a foundational benchmark designed to train and evaluate
LLM agents across diverse real-world video games. Unlike existing benchmarks,
Orak includes 12 popular video games spanning all major genres, enabling
comprehensive studies of LLM capabilities and agentic modules essential for
intricate game scenarios. To support consistent evaluation of LLMs, we
introduce a plug-and-play interface based on Model Context Protocol (MCP) that
enables LLMs to seamlessly connect with games and manipulate agentic modules.
Additionally, we propose a fine-tuning dataset, consisting of LLM gameplay
trajectories across diverse game genres. Orak offers a comprehensive evaluation
framework, encompassing general game score leaderboards, LLM battle arenas, and
in-depth analyses of visual input state, agentic strategies, and fine-tuning
effects, establishing a foundation towards building generic gaming agents. Code
is available at https://github.com/krafton-ai/Orak.

</details>


### [121] [Training Cross-Morphology Embodied AI Agents: From Practical Challenges to Theoretical Foundations](https://arxiv.org/abs/2506.03613)
*Shaoshan Liu,Fan Wang,Hongjun Zhou,Yuanfeng Wang*

Main category: cs.AI

TL;DR: 本文通过理论分析揭示了跨形态机器人AI训练问题的复杂性（PSPACE完全），并提出受生物启发的分布式学习方法（NEXP完全），为实际工程提供了理论指导。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习方法在机器人形态多样性下失效，需理论解释并寻找替代方案。

Method: 将异构机器人训练问题形式化为HEAT问题（结构化POMDP），并提出集体适应分布式学习框架。

Result: 证明HEAT问题属PSPACE完全类，集体适应方法虽理论复杂但实际可扩展。

Conclusion: 计算理论能指导具身AI系统设计，代码已开源供实践验证。

Abstract: While theory and practice are often seen as separate domains, this article
shows that theoretical insight is essential for overcoming real-world
engineering barriers. We begin with a practical challenge: training a
cross-morphology embodied AI policy that generalizes across diverse robot
morphologies. We formalize this as the Heterogeneous Embodied Agent Training
(HEAT) problem and prove it reduces to a structured Partially Observable Markov
Decision Process (POMDP) that is PSPACE-complete. This result explains why
current reinforcement learning pipelines break down under morphological
diversity, due to sequential training constraints, memory-policy coupling, and
data incompatibility. We further explore Collective Adaptation, a distributed
learning alternative inspired by biological systems. Though NEXP-complete in
theory, it offers meaningful scalability and deployment benefits in practice.
This work illustrates how computational theory can illuminate system design
trade-offs and guide the development of more robust, scalable embodied AI. For
practitioners and researchers to explore this problem, the implementation code
of this work has been made publicly available at
https://github.com/airs-admin/HEAT

</details>


### [122] [Reason from Future: Reverse Thought Chain Enhances LLM Reasoning](https://arxiv.org/abs/2506.03673)
*Yinlong Xu,Yanzhao Zheng,Shuoshuo Sun,Shuaihan Huang,Baohua Dong,Hangcheng Zhu,Ruohui Huang,Gang Yu,Hongxia Xu,Jian Wu*

Main category: cs.AI

TL;DR: 提出了一种名为RFF的新型推理范式，通过双向推理结合自上而下规划和自下而上推理积累，优化了小语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有推理方法如CoT和ToT虽然能提升小语言模型的推理能力，但存在搜索空间过大和局部最优问题，缺乏全局视角。

Method: RFF采用逆向推理机制，优先考虑核心逻辑关系并对中间步骤施加目标导向约束，以减少搜索空间并缓解顺序前向推理中的错误累积。

Result: 实验证明，RFF在多种任务中表现优于传统方法，具有更高的准确性和更小的搜索空间。

Conclusion: RFF通过双向推理和逆向机制有效解决了现有方法的局限性，为复杂任务提供了更高效的解决方案。

Abstract: It has been demonstrated that carefully designed reasoning paradigms, like
Chain-of-Thought (CoT) and Tree-of-Thought (ToT), can enhance the reasoning
capabilities of small language models by detailed thinking and extensive
thought searching, unbounded branching factors in the searching space create
prohibitive reasoning consumption. However these methods fall into the trap of
local optimum reasoning, which means the model lacks a global perspective while
solving problems. We propose a novel reasoning paradigm called Reason from
Future (RFF), which generates reasoning paths by bidirectional reasoning that
combines top-down planning with bottom-up reasoning accumulation. The essence
of RFF lies in its reverse reasoning mechanism, which prioritizes core logical
relationships and imposes goal-oriented constraints on intermediate steps,
thereby reducing the searching space and mitigating error accumulation inherent
in sequential forward reasoning. Empirical evaluations across diverse
experiments demonstrate that RFF outperforms conventional paradigms with higher
accuracy and less searching space to solve complex tasks.

</details>


### [123] [AssetOpsBench: Benchmarking AI Agents for Task Automation in Industrial Asset Operations and Maintenance](https://arxiv.org/abs/2506.03828)
*Dhaval Patel,Shuxin Lin,James Rayfield,Nianjun Zhou,Roman Vaculin,Natalia Martinez,Fearghal O'donncha,Jayant Kalagnanam*

Main category: cs.AI

TL;DR: 论文提出AssetOpsBench框架，利用AI代理和LLM实现工业资产全生命周期管理的端到端自动化，超越传统孤立任务处理模式。


<details>
  <summary>Details</summary>
Motivation: 传统AI方法仅解决工业运维中的孤立任务，缺乏全流程协同。AI代理和LLM的出现为跨生命周期自动化提供了新机遇。

Method: 开发AssetOpsBench统一框架，指导面向工业4.0的领域专用代理开发，集成感知、推理与控制能力。

Result: 构建了支持工业运维代理开发的开源环境，提出构建端到端自动化系统的关键需求与实施方案。

Conclusion: AI代理有望自主管理需多专家协作的工业流程，该框架为工业4.0应用提供了可操作的开发指南。

Abstract: AI for Industrial Asset Lifecycle Management aims to automate complex
operational workflows -- such as condition monitoring, maintenance planning,
and intervention scheduling -- to reduce human workload and minimize system
downtime. Traditional AI/ML approaches have primarily tackled these problems in
isolation, solving narrow tasks within the broader operational pipeline. In
contrast, the emergence of AI agents and large language models (LLMs)
introduces a next-generation opportunity: enabling end-to-end automation across
the entire asset lifecycle. This paper envisions a future where AI agents
autonomously manage tasks that previously required distinct expertise and
manual coordination. To this end, we introduce AssetOpsBench -- a unified
framework and environment designed to guide the development, orchestration, and
evaluation of domain-specific agents tailored for Industry 4.0 applications. We
outline the key requirements for such holistic systems and provide actionable
insights into building agents that integrate perception, reasoning, and control
for real-world industrial operations. The software is available at
https://github.com/IBM/AssetOpsBench.

</details>


### [124] [Causal Explanations Over Time: Articulated Reasoning for Interactive Environments](https://arxiv.org/abs/2506.03915)
*Sebastian Rödling,Matej Zečević,Devendra Singh Dhami,Kristian Kersting*

Main category: cs.AI

TL;DR: 论文提出了一种基于递归解释树的广义结构因果解释方法，用于处理时间序列数据和反馈循环问题。


<details>
  <summary>Details</summary>
Motivation: 现有的结构因果解释（SCEs）方法仅适用于小规模数据，无法有效处理时间序列数据或包含反馈循环的行为组件。

Method: 通过将SCEs推广到递归解释树的形式，捕捉时间上的因果关系和交互。

Result: 新方法在合成时间序列数据和2D网格游戏中表现优于基础SCEs和其他现有因果解释方法。

Conclusion: 广义SCEs算法能够更好地处理复杂的时间交互和反馈循环问题。

Abstract: Structural Causal Explanations (SCEs) can be used to automatically generate
explanations in natural language to questions about given data that are
grounded in a (possibly learned) causal model. Unfortunately they work for
small data only. In turn they are not attractive to offer reasons for events,
e.g., tracking causal changes over multiple time steps, or a behavioral
component that involves feedback loops through actions of an agent. To this
end, we generalize SCEs to a (recursive) formulation of explanation trees to
capture the temporal interactions between reasons. We show the benefits of this
more general SCE algorithm on synthetic time-series data and a 2D grid game,
and further compare it to the base SCE and other existing methods for causal
explanations.

</details>


### [125] [Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning](https://arxiv.org/abs/2506.03939)
*Junqi Gao,Xiang Zou,YIng Ai,Dong Li,Yichen Niu,Biqing Qi,Jianxing Liu*

Main category: cs.AI

TL;DR: Graph Counselor提出了一种基于多智能体协作的GraphRAG方法，通过自适应图信息提取模块和多视角自反思机制，解决了现有方法在信息聚合和推理机制上的局限性，显著提升了图推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的GraphRAG方法存在信息聚合效率低和推理机制僵化的问题，难以自适应地捕捉图数据的多层次信息及动态调整推理深度。

Method: Graph Counselor采用多智能体协作框架，包括规划、思考和执行智能体，结合自适应图信息提取模块（AGIEM）和多视角自反思（SR）模块，动态调整信息提取策略并提升语义一致性。

Result: 实验表明，Graph Counselor在多项图推理任务中优于现有方法，展现出更高的推理准确性和泛化能力。

Conclusion: Graph Counselor通过多智能体协作和自反思机制，有效提升了GraphRAG的知识整合能力和推理质量，为专业领域的LLM应用提供了新思路。

Abstract: Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external
knowledge integration capabilities by explicitly modeling knowledge
relationships, thereby improving the factual accuracy and generation quality of
Large Language Models (LLMs) in specialized domains. However, existing methods
suffer from two inherent limitations: 1) Inefficient Information Aggregation:
They rely on a single agent and fixed iterative patterns, making it difficult
to adaptively capture multi-level textual, structural, and degree information
within graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning
schemes, which cannot dynamically adjust reasoning depth nor achieve precise
semantic correction. To overcome these limitations, we propose Graph Counselor,
an GraphRAG method based on multi-agent collaboration. This method uses the
Adaptive Graph Information Extraction Module (AGIEM), where Planning, Thought,
and Execution Agents work together to precisely model complex graph structures
and dynamically adjust information extraction strategies, addressing the
challenges of multi-level dependency modeling and adaptive reasoning depth.
Additionally, the Self-Reflection with Multiple Perspectives (SR) module
improves the accuracy and semantic consistency of reasoning results through
self-reflection and backward reasoning mechanisms. Experiments demonstrate that
Graph Counselor outperforms existing methods in multiple graph reasoning tasks,
exhibiting higher reasoning accuracy and generalization ability. Our code is
available at https://github.com/gjq100/Graph-Counselor.git.

</details>


### [126] [A framework for Conditional Reasoning in Answer Set Programming](https://arxiv.org/abs/2506.03997)
*Mario Alviano,Laura Giordano,Daniele Theseider Dupré*

Main category: cs.AI

TL;DR: 提出了一种基于条件逻辑与典型性的条件ASP框架，用于扩展ASP的条件推理能力。


<details>
  <summary>Details</summary>
Motivation: 为了在ASP中引入条件推理能力，扩展其表达能力和应用范围。

Method: 结合条件知识库与ASP程序，采用多偏好语义和KLM偏好语义解释条件。

Result: 实现了在ASP程序的答案集上进行条件推理的框架。

Conclusion: 条件ASP框架为ASP的条件扩展提供了有效的理论基础和实现方法。

Abstract: In this paper we introduce a Conditional Answer Set Programming framework
(Conditional ASP) for the definition of conditional extensions of Answer Set
Programming (ASP). The approach builds on a conditional logic with typicality,
and on the combination of a conditional knowledge base with an ASP program, and
allows for conditional reasoning over the answer sets of the program. The
formalism relies on a multi-preferential semantics (and on the KLM preferential
semantics, as a special case) to provide an interpretation of conditionals.

</details>


### [127] [AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents](https://arxiv.org/abs/2506.04018)
*Akshat Naik,Patrick Quinn,Guillermo Bosch,Emma Gouné,Francisco Javier Campos Zabala,Jason Ross Brown,Edward James Young*

Main category: cs.AI

TL;DR: 本文提出了一个评估大语言模型代理在现实场景中行为失准倾向的基准测试AgentMisalignment，发现模型能力和系统提示对失准行为有显著影响。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型代理的广泛应用，其行为失准风险增加。现有研究主要关注代理执行失准行为的能力和遵循有害指令的倾向，但对代理在真实场景中尝试失准行为的可能性理解不足。

Method: 引入AgentMisalignment基准测试，包含一系列现实场景，评估代理在目标守护、抵抗关闭、消极怠工和权力寻求等失准行为子类中的表现，并系统性地通过不同系统提示改变代理个性。

Result: 前沿模型在基准测试中表现出更高的平均失准倾向，且代理个性特征对失准倾向的影响有时甚至超过模型选择本身。

Conclusion: 当前的对齐方法未能泛化到大语言模型代理，随着自主系统日益普及，需要进一步的行为倾向评估，并强调部署AI代理时系统提示工程的重要性。

Abstract: As Large Language Model (LLM) agents become more widespread, associated
misalignment risks increase. Prior work has examined agents' ability to enact
misaligned behaviour (misalignment capability) and their compliance with
harmful instructions (misuse propensity). However, the likelihood of agents
attempting misaligned behaviours in real-world settings (misalignment
propensity) remains poorly understood. We introduce a misalignment propensity
benchmark, AgentMisalignment, consisting of a suite of realistic scenarios in
which LLM agents have the opportunity to display misaligned behaviour. We
organise our evaluations into subcategories of misaligned behaviours, including
goal-guarding, resisting shutdown, sandbagging, and power-seeking. We report
the performance of frontier models on our benchmark, observing higher
misalignment on average when evaluating more capable models. Finally, we
systematically vary agent personalities through different system prompts. We
find that persona characteristics can dramatically and unpredictably influence
misalignment tendencies -- occasionally far more than the choice of model
itself -- highlighting the importance of careful system prompt engineering for
deployed AI agents. Our work highlights the failure of current alignment
methods to generalise to LLM agents, and underscores the need for further
propensity evaluations as autonomous systems become more prevalent.

</details>


### [128] [Interpretability by Design for Efficient Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2506.04022)
*Qiyue Xia,J. Michael Herrmann*

Main category: cs.AI

TL;DR: 该论文提出了一种基于局部线性映射的多目标强化学习方法，通过近似帕累托前沿在连续解域中高效搜索，实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 多目标强化学习（MORL）旨在优化多个可能冲突的目标，以提高实际任务中的灵活性和可靠性。然而，参数空间与性能空间的关系通常是非唯一的，因此需要一种有效的方法来搜索解空间。

Method: 采用基于参数空间与性能空间局部线性映射的训练方案，利用近似帕累托前沿解释当前参数向量，实现在连续解域中的高效搜索。

Result: 实验表明，该方法在不同领域（无论是否重新训练）均表现出高效性，且优于先前的方法。

Conclusion: 通过局部线性映射和近似帕累托前沿，该方法在多目标强化学习中实现了高效的解空间搜索，提升了算法的灵活性和可靠性。

Abstract: Multi-objective reinforcement learning (MORL) aims at optimising several,
often conflicting goals in order to improve flexibility and reliability of RL
in practical tasks. This can be achieved by finding diverse policies that are
optimal for some objective preferences and non-dominated by optimal policies
for other preferences so that they form a Pareto front in the multi-objective
performance space. The relation between the multi-objective performance space
and the parameter space that represents the policies is generally non-unique.
Using a training scheme that is based on a locally linear map between the
parameter space and the performance space, we show that an approximate Pareto
front can provide an interpretation of the current parameter vectors in terms
of the objectives which enables an effective search within contiguous solution
domains. Experiments are conducted with and without retraining across different
domains, and the comparison with previous methods demonstrates the efficiency
of our approach.

</details>


### [129] [TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems](https://arxiv.org/abs/2506.04133)
*Shaina Raza,Ranjan Sapkota,Manoj Karkee,Christos Emmanouilidis*

Main category: cs.AI

TL;DR: 本文综述了基于大语言模型（LLM）的自主多智能体系统（AMAS）中的信任、风险与安全管理（TRiSM）问题，提出了治理、可解释性、模型运维和隐私安全四大支柱，并探讨了相关威胁向量、信任构建机制及合规框架。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的自主多智能体系统在企业和社会的广泛应用，其带来的信任、风险与安全管理挑战亟待系统化研究，以确保安全、可靠和透明的部署。

Method: 通过结构化分析，结合案例研究和现有技术，详细探讨了AMAS中的TRiSM框架，包括威胁分类、信任机制、可解释性技术和隐私安全措施。

Result: 提出了针对AMAS的全面风险分类法，总结了现有信任构建和透明化技术，并指出了评估指标与开放基准测试的挑战。

Conclusion: 论文提出了负责任自主AI的发展路线图，强调需要将TRiSM原则融入多智能体系统设计，以实现安全、可问责和透明的应用。

Abstract: Agentic AI systems, built on large language models (LLMs) and deployed in
multi-agent configurations, are redefining intelligent autonomy, collaboration
and decision-making across enterprise and societal domains. This review
presents a structured analysis of Trust, Risk, and Security Management (TRiSM)
in the context of LLM-based agentic multi-agent systems (AMAS). We begin by
examining the conceptual foundations of agentic AI, its architectural
differences from traditional AI agents, and the emerging system designs that
enable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is
then detailed through four pillars governance, explainability, ModelOps, and
privacy/security each contextualized for agentic LLMs. We identify unique
threat vectors and introduce a comprehensive risk taxonomy for the agentic AI
applications, supported by case studies illustrating real-world
vulnerabilities. Furthermore, the paper also surveys trust-building mechanisms,
transparency and oversight techniques, and state-of-the-art explainability
strategies in distributed LLM agent systems. Additionally, metrics for
evaluating trust, interpretability, and human-centered performance are reviewed
alongside open benchmarking challenges. Security and privacy are addressed
through encryption, adversarial defense, and compliance with evolving AI
regulations. The paper concludes with a roadmap for responsible agentic AI,
proposing research directions to align emerging multi-agent systems with robust
TRiSM principles for safe, accountable, and transparent deployment.

</details>


### [130] [macOSWorld: A Multilingual Interactive Benchmark for GUI Agents](https://arxiv.org/abs/2506.04135)
*Pei Yang,Hai Ci,Mike Zheng Shou*

Main category: cs.AI

TL;DR: 研究者提出了首个针对macOS的GUI代理基准测试macOSWorld，覆盖多语言任务和安全测试，揭示了现有代理在macOS适配和多语言处理上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理的交互基准主要针对英语环境和其他操作系统，缺乏对macOS这一重要操作系统的支持。macOS具有独特的GUI模式和专属应用，需要专门的基准测试来评估GUI代理的性能。

Method: 研究者开发了macOSWorld基准，包含202个多语言交互任务，覆盖30个应用（其中28个为macOS专属），并提供5种语言的指令和界面。此外，还包含专门的安全测试子集。

Result: 测试显示，专有计算机使用代理的成功率超过30%，而开源轻量级研究模型低于2%。多语言基准暴露了普遍弱点，尤其是阿拉伯语性能下降27.5%。安全测试表明欺骗攻击问题亟待关注。

Conclusion: macOSWorld填补了macOS GUI代理评估的空白，揭示了现有代理在macOS适配和多语言处理上的不足，强调了安全问题的紧迫性。

Abstract: Graphical User Interface (GUI) agents show promising capabilities for
automating computer-use tasks and facilitating accessibility, but existing
interactive benchmarks are mostly English-only, covering web-use or Windows,
Linux, and Android environments, but not macOS. macOS is a major OS with
distinctive GUI patterns and exclusive applications. To bridge the gaps, we
present macOSWorld, the first comprehensive benchmark for evaluating GUI agents
on macOS. macOSWorld features 202 multilingual interactive tasks across 30
applications (28 macOS-exclusive), with task instructions and OS interfaces
offered in 5 languages (English, Chinese, Arabic, Japanese, and Russian). As
GUI agents are shown to be vulnerable to deception attacks, macOSWorld also
includes a dedicated safety benchmarking subset. Our evaluation on six GUI
agents reveals a dramatic gap: proprietary computer-use agents lead at above
30% success rate, while open-source lightweight research models lag at below
2%, highlighting the need for macOS domain adaptation. Multilingual benchmarks
also expose common weaknesses, especially in Arabic, with a 27.5% average
degradation compared to English. Results from safety benchmarking also
highlight that deception attacks are more general and demand immediate
attention. macOSWorld is available at https://github.com/showlab/macosworld.

</details>


### [131] [Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models](https://arxiv.org/abs/2506.04210)
*Soumya Suvra Ghosal,Souradip Chakraborty,Avinash Reddy,Yifu Lu,Mengdi Wang,Dinesh Manocha,Furong Huang,Mohammad Ghavamzadeh,Amrit Singh Bedi*

Main category: cs.AI

TL;DR: 研究发现，在推理模型中延长思考时间（如使用'Wait'提示）虽初期能提升表现，但会导致'过度思考'而降低精度。作者提出'并行思考'方法，通过多数表决选择最优解，准确率提升20%。


<details>
  <summary>Details</summary>
Motivation: 当前流行观点认为测试时延长思考痕迹（如'Wait'提示）能提升模型推理能力，但这是否真实有效尚不明确。本文旨在验证额外思考时间与推理性能的真实关系。

Method: 通过概率模型分析思考时间与输出方差的关系，并提出'并行思考'方法——在相同推理预算下生成多条独立推理路径，通过多数表决选择最一致答案。

Result: 实验显示延长思考会先提升后降低性能（过思考现象）。并行思考方法比传统延长思考准确率最高提升20%。

Conclusion: 测试时单纯延长思考并非有效利用推理预算的方式，而并行思考通过多样化采样和多数表决机制能显著提升模型表现。

Abstract: Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1,
DeepSeek R1) have led to a popular belief that extending thinking traces using
prompts like "Wait" or "Let me rethink" can improve performance. This raises a
natural question: Does thinking more at test-time truly lead to better
reasoning? To answer this question, we perform a detailed empirical study
across models and benchmarks, which reveals a consistent pattern of initial
performance improvements from additional thinking followed by a decline, due to
"overthinking". To understand this non-monotonic trend, we consider a simple
probabilistic model, which reveals that additional thinking increases output
variance-creating an illusion of improved reasoning while ultimately
undermining precision. Thus, observed gains from "more thinking" are not true
indicators of improved reasoning, but artifacts stemming from the connection
between model uncertainty and evaluation metric. This suggests that test-time
scaling through extended thinking is not an effective way to utilize the
inference thinking budget. Recognizing these limitations, we introduce an
alternative test-time scaling approach, parallel thinking, inspired by
Best-of-N sampling. Our method generates multiple independent reasoning paths
within the same inference budget and selects the most consistent response via
majority vote, achieving up to 20% higher accuracy compared to extended
thinking. This provides a simple yet effective mechanism for test-time scaling
of reasoning models.

</details>


### [132] [mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation](https://arxiv.org/abs/2505.24073)
*Chan-Wei Hu,Yueqi Wang,Shuo Xing,Chia-Ju Chen,Zhengzhong Tu*

Main category: cs.AI

TL;DR: 该论文首次系统分析了大型视觉语言模型（LVLMs）中的多模态检索增强生成（RAG）流程，通过改进检索、重排序和生成阶段，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在动态现实应用中受限于静态训练数据、易产生幻觉且无法验证最新外部证据，检索增强生成（RAG）通过访问大规模知识库提供了一种解决方案。

Method: 论文系统研究了多模态RAG流程的三个阶段：检索阶段的模态配置和策略、重排序阶段的位置偏差缓解和证据相关性提升、生成阶段的候选信息整合，并探索了一个统一的自反思框架。

Result: 在不进行微调的情况下，该方法平均性能提升了5%。

Conclusion: 论文通过全栈探索RAG在LVLMs中的应用，显著提升了模型性能，为动态现实应用提供了有效解决方案。

Abstract: Large Vision-Language Models (LVLMs) have made remarkable strides in
multimodal tasks such as visual question answering, visual grounding, and
complex reasoning. However, they remain limited by static training data,
susceptibility to hallucinations, and inability to verify claims against
up-to-date, external evidence, compromising their performance in dynamic
real-world applications. Retrieval-Augmented Generation (RAG) offers a
practical solution to mitigate these challenges by allowing the LVLMs to access
large-scale knowledge databases via retrieval mechanisms, thereby grounding
model outputs in factual, contextually relevant information. Here in this
paper, we conduct the first systematic dissection of the multimodal RAG
pipeline for LVLMs, explicitly investigating (1) the retrieval phase: on the
modality configurations and retrieval strategies, (2) the re-ranking stage: on
strategies to mitigate positional biases and improve the relevance of retrieved
evidence, and (3) the generation phase: we further investigate how to best
integrate retrieved candidates into the final generation process. Finally, we
extend to explore a unified agentic framework that integrates re-ranking and
generation through self-reflection, enabling LVLMs to select relevant evidence
and suppress irrelevant context dynamically. Our full-stack exploration of RAG
for LVLMs yields substantial insights, resulting in an average performance
boost of 5% without any fine-tuning.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [133] [Modular Diffusion Policy Training: Decoupling and Recombining Guidance and Diffusion for Offline RL](https://arxiv.org/abs/2506.03154)
*Zhaoyang Chen,Cody Fleming*

Main category: cs.LG

TL;DR: 该论文提出了一种模块化训练方法，将引导模块与扩散模型解耦，通过独立训练引导模块提升离线强化学习的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的强化学习方法中，引导模块与扩散模型联合训练在早期阶段可能因引导不准确而产生噪声信号，导致次优结果。离线强化学习中引导仅依赖离线数据，无需联合训练，因此探索模块化训练方法以优化性能。

Method: 论文提出三种关键方法：1) 分析引导在不同训练阶段的作用；2) 先独立训练引导模块作为价值估计器，再冻结以指导扩散模型；3) 展示跨模块可迁移性，不同算法训练的引导模块可直接复用。

Result: 模块化方法降低了内存使用、提高了计算效率，样本效率和最终性能均得到提升。跨模块迁移使归一化分数方差显著降低（如IQR减少86%），且无需额外训练即可达到基线性能。

Conclusion: 研究表明离线强化学习可采用模块化、可复用、可组合的训练流程，为领域提供了新的范式。理论分析和D4RL基准实验验证了方法的有效性。

Abstract: Classifier free guidance has shown strong potential in diffusion-based
reinforcement learning. However, existing methods rely on joint training of the
guidance module and the diffusion model, which can be suboptimal during the
early stages when the guidance is inaccurate and provides noisy learning
signals. In offline RL, guidance depends solely on offline data: observations,
actions, and rewards, and is independent of the policy module's behavior,
suggesting that joint training is not required. This paper proposes modular
training methods that decouple the guidance module from the diffusion model,
based on three key findings:
  Guidance Necessity: We explore how the effectiveness of guidance varies with
the training stage and algorithm choice, uncovering the roles of guidance and
diffusion. A lack of good guidance in the early stage presents an opportunity
for optimization.
  Guidance-First Diffusion Training: We introduce a method where the guidance
module is first trained independently as a value estimator, then frozen to
guide the diffusion model using classifier-free reward guidance. This
modularization reduces memory usage, improves computational efficiency, and
enhances both sample efficiency and final performance.
  Cross-Module Transferability: Applying two independently trained guidance
models, one during training and the other during inference, can significantly
reduce normalized score variance (e.g., reducing IQR by 86%). We show that
guidance modules trained with one algorithm (e.g., IDQL) can be directly reused
with another (e.g., DQL), with no additional training required, demonstrating
baseline-level performance as well as strong modularity and transferability.
  We provide theoretical justification and empirical validation on bullet D4RL
benchmarks. Our findings suggest a new paradigm for offline RL: modular,
reusable, and composable training pipelines.

</details>


### [134] [Fusing Cross-Domain Knowledge from Multimodal Data to Solve Problems in the Physical World](https://arxiv.org/abs/2506.03155)
*Yu Zheng*

Main category: cs.LG

TL;DR: 该论文提出了一个跨领域多模态数据融合的四层框架，解决了‘融合什么’、‘为何能融合’及‘如何融合’三个核心问题，旨在有效整合不同领域的多模态数据以解决现实问题。


<details>
  <summary>Details</summary>
Motivation: 现实环境复杂多变，单一数据采集方法难以全面建模。现有研究多局限于单领域多模态数据融合，而跨领域知识融合面临数据结构和分布不一致的挑战，亟需系统性解决方案。

Method: 提出包含领域层（Domains）、链接层（Links）、模型层（Models）和数据层（Data）的四层框架，分别负责跨领域数据选择、知识对齐原理、融合范式设计及数据统一表征。

Result: 框架支持端到端的跨领域多模态数据融合，通过层级化设计解决了数据异构性问题，为AI模型提供可处理的统一知识表示。

Conclusion: 该框架为跨领域知识融合提供了理论和方法基础，其灵活性和普适性有助于推动复杂现实问题的解决。

Abstract: The proliferation of artificial intelligence has enabled a diversity of
applications that bridge the gap between digital and physical worlds. As
physical environments are too complex to model through a single information
acquisition approach, it is crucial to fuse multimodal data generated by
different sources, such as sensors, devices, systems, and people, to solve a
problem in the real world. Unfortunately, it is neither applicable nor
sustainable to deploy new resources to collect original data from scratch for
every problem. Thus, when data is inadequate in the domain of problem, it is
vital to fuse knowledge from multimodal data that is already available in other
domains. We call this cross-domain knowledge fusion. Existing research focus on
fusing multimodal data in a single domain, supposing the knowledge from
different datasets is intrinsically aligned; however, this assumption may not
hold in the scenarios of cross-domain knowledge fusion. In this paper, we
formally define the cross-domain multimodal data fusion problem, discussing its
unique challenges, differences and advantages beyond data fusion in a single
domain. We propose a four-layer framework, consisting of Domains, Links, Models
and Data layers, answering three key questions: "what to fuse", "why can be
fused", and "how to fuse". The Domains Layer selects relevant data from
different domains for a given problem. The Links Layer reveals the philosophy
of knowledge alignment beyond specific model structures. The Models Layer
provides two knowledge fusion paradigms based on the fundamental mechanisms for
processing data. The Data Layer turns data of different structures,
resolutions, scales and distributions into a consistent representation that can
be fed into an AI model. With this framework, we can design end-to-end
solutions that fuse cross-domain multimodal data effectively for solving
real-world problems.

</details>


### [135] [DUAL: Dynamic Uncertainty-Aware Learning](https://arxiv.org/abs/2506.03158)
*Jiahao Qin,Bei Peng,Feng Liu,Guangliang Cheng,Lu Zong*

Main category: cs.LG

TL;DR: 提出了动态不确定性感知学习（DUAL）框架，有效处理单模态和多模态场景中的特征不确定性，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在多模态和单模态场景中常面临特征不确定性问题，影响性能和可靠性。

Method: DUAL框架包含动态特征不确定性建模、自适应分布感知调制和不确定性感知跨模态关系学习三大创新。

Result: 实验显示DUAL在多个任务中表现优异，如CIFAR-10准确率提升7.1%，多模态情感分析任务中也有显著提升。

Conclusion: DUAL框架能有效处理特征不确定性，提升模型在单模态和多模态任务中的性能。

Abstract: Deep learning models frequently encounter feature uncertainty in diverse
learning scenarios, significantly impacting their performance and reliability.
This challenge is particularly complex in multi-modal scenarios, where models
must integrate information from different sources with inherent uncertainties.
We propose Dynamic Uncertainty-Aware Learning (DUAL), a unified framework that
effectively handles feature uncertainty in both single-modal and multi-modal
scenarios. DUAL introduces three key innovations: Dynamic Feature Uncertainty
Modeling, which continuously refines uncertainty estimates through joint
consideration of feature characteristics and learning dynamics; Adaptive
Distribution-Aware Modulation, which maintains balanced feature distributions
through dynamic sample influence adjustment; and Uncertainty-aware Cross-Modal
Relationship Learning, which explicitly models uncertainties in cross-modal
interactions. Through extensive experiments, we demonstrate DUAL's
effectiveness across multiple domains: in computer vision tasks, it achieves
substantial improvements of 7.1% accuracy on CIFAR-10, 6.5% accuracy on
CIFAR-100, and 2.3% accuracy on Tiny-ImageNet; in multi-modal learning, it
demonstrates consistent gains of 4.1% accuracy on CMU-MOSEI and 2.8% accuracy
on CMU-MOSI for sentiment analysis, while achieving 1.4% accuracy improvements
on MISR. The code will be available on GitHub soon.

</details>


### [136] [Bayes Error Rate Estimation in Difficult Situations](https://arxiv.org/abs/2506.03159)
*Lesley Wheat,Martin v. Mohrenschildt,Saeid Habibi*

Main category: cs.LG

TL;DR: 论文通过蒙特卡洛模拟比较了kNN、GHP和KDE三种贝叶斯错误率估计方法，发现kNN在非参数估计中最准确，但需至少1000样本/类才能达到95%置信区间误差<5%。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯错误率(BER)是机器学习分类精度的理论极限，但现有估计方法在有限样本、多变量及未知分布场景下的准确性缺乏系统评估。本文旨在确定哪些估计器能满足实际应用的准确性要求。

Method: 1) 使用合成数据进行蒙特卡洛模拟获取置信区间 2) 设计新测试场景，每个场景运行2500次模拟 3) 比较kNN、GHP和KDE三种非参数估计方法

Result: 1) kNN显著优于其他方法 2) 达到95%置信区间误差<5%需至少1000样本/类 3) 特征增至4维时需2500样本/类 4) 其他方法在高维略优但仍不达标

Conclusion: kNN是目前最实用的BER非参数估计器，但样本需求随特征维度急剧增加，其他方法在高维场景仍无法满足实际精度要求。

Abstract: The Bayes Error Rate (BER) is the fundamental limit on the achievable
generalizable classification accuracy of any machine learning model due to
inherent uncertainty within the data. BER estimators offer insight into the
difficulty of any classification problem and set expectations for optimal
classification performance. In order to be useful, the estimators must also be
accurate with a limited number of samples on multivariate problems with unknown
class distributions. To determine which estimators meet the minimum
requirements for "usefulness", an in-depth examination of their accuracy is
conducted using Monte Carlo simulations with synthetic data in order to obtain
their confidence bounds for binary classification. To examine the usability of
the estimators on real-world applications, new test scenarios are introduced
upon which 2500 Monte Carlo simulations per scenario are run over a wide range
of BER values. In a comparison of k-Nearest Neighbor (kNN), Generalized
Henze-Penrose (GHP) divergence and Kernel Density Estimation (KDE) techniques,
results show that kNN is overwhelmingly the more accurate non-parametric
estimator. In order to reach the target of an under 5 percent range for the 95
percent confidence bounds, the minimum number of required samples per class is
1000. As more features are added, more samples are needed, so that 2500 samples
per class are required at only 4 features. Other estimators do become more
accurate than kNN as more features are added, but continuously fail to meet the
target range.

</details>


### [137] [Applying MambaAttention, TabPFN, and TabTransformers to Classify SAE Automation Levels in Crashes](https://arxiv.org/abs/2506.03160)
*Shriyank Somvanshi,Anannya Ghosh Tusti,Mahmuda Sultana Mimi,Md Monzurul Islam,Sazzad Bin Bashar Polock,Anandi Dutta,Subasish Das*

Main category: cs.LG

TL;DR: 该研究评估了三种深度学习模型在分类SAE自动化级别中的表现，发现MambaAttention表现最佳，有助于提升自动驾驶车辆事故分析的准确性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆（AVs）的普及对事故分类和安全分析提出了新挑战，准确识别事故涉及的SAE自动化级别对理解事故动态和系统责任至关重要。现有方法常忽略自动化特定因素，缺乏区分不同SAE级别的模型复杂度。

Method: 研究使用德克萨斯州2024年的结构化事故数据（涵盖4,649例），评估了MambaAttention、TabPFN和TabTransformer三种深度学习模型在分类SAE自动化级别（SAE 1级、2级及3-5级合并）中的表现。采用SMOTEENN进行类别平衡后，在7,300条记录的统一数据集上训练和评估模型。

Result: MambaAttention整体表现最佳（F1分数：SAE 1级88%，SAE 2级97%，SAE 3-5级99%），TabPFN在零样本推理中表现优异，而TabTransformer在检测部分自动化事故（SAE 2级）时表现较差（F1分数55%）。

Conclusion: 研究表明，针对表格数据优化的深度学习模型能显著提升自动化级别分类的准确性和效率。将其整合到事故分析框架中，可支持政策制定、AV安全评估和监管决策，尤其是在区分中高级自动化技术的高风险条件时。

Abstract: The increasing presence of automated vehicles (AVs) presents new challenges
for crash classification and safety analysis. Accurately identifying the SAE
automation level involved in each crash is essential to understanding crash
dynamics and system accountability. However, existing approaches often overlook
automation-specific factors and lack model sophistication to capture
distinctions between different SAE levels. To address this gap, this study
evaluates the performance of three advanced tabular deep learning models
MambaAttention, TabPFN, and TabTransformer for classifying SAE automation
levels using structured crash data from Texas (2024), covering 4,649 cases
categorized as Assisted Driving (SAE Level 1), Partial Automation (SAE Level
2), and Advanced Automation (SAE Levels 3-5 combined). Following class
balancing using SMOTEENN, the models were trained and evaluated on a unified
dataset of 7,300 records. MambaAttention demonstrated the highest overall
performance (F1-scores: 88% for SAE 1, 97% for SAE 2, and 99% for SAE 3-5),
while TabPFN excelled in zero-shot inference with high robustness for rare
crash categories. In contrast, TabTransformer underperformed, particularly in
detecting Partial Automation crashes (F1-score: 55%), suggesting challenges in
modeling shared human-system control dynamics. These results highlight the
capability of deep learning models tailored for tabular data to enhance the
accuracy and efficiency of automation-level classification. Integrating such
models into crash analysis frameworks can support policy development, AV safety
evaluation, and regulatory decisions, especially in distinguishing high-risk
conditions for mid- and high-level automation technologies.

</details>


### [138] [Safety-Prioritized, Reinforcement Learning-Enabled Traffic Flow Optimization in a 3D City-Wide Simulation Environment](https://arxiv.org/abs/2506.03161)
*Mira Nuthakki*

Main category: cs.LG

TL;DR: 该论文开发了三种工具来改善交通管理：一个综合的3D城市交通模拟环境、一个碰撞模型和一个强化学习框架，显著减少了交通事故和碳排放。


<details>
  <summary>Details</summary>
Motivation: 交通拥堵和碰撞是全球性的经济、环境和社会挑战，传统方法效果有限，需要新的解决方案。

Method: 使用Unity游戏引擎进行碰撞建模，开发了基于近端策略优化（PPO）的强化学习框架，优先考虑安全性而非效率。

Result: 模型将严重碰撞次数、车辆间碰撞次数和总行驶距离减少至基线值的三分之一以下，燃油效率提高39%，碳排放减少88%。

Conclusion: 该方法证明了在城市范围内应用3D交通模拟的可行性，符合交通部的零愿景安全原则，能有效减少碰撞、优化交通流并降低温室气体排放。

Abstract: Traffic congestion and collisions represent significant economic,
environmental, and social challenges worldwide. Traditional traffic management
approaches have shown limited success in addressing these complex, dynamic
problems. To address the current research gaps, three potential tools are
developed: a comprehensive 3D city-wide simulation environment that integrates
both macroscopic and microscopic traffic dynamics; a collision model; and a
reinforcement learning framework with custom reward functions prioritizing
safety over efficiency. Unity game engine-based simulation is used for direct
collision modeling. A custom reward enabled reinforcement learning method,
proximal policy optimization (PPO) model, yields substantial improvements over
baseline results, reducing the number of serious collisions, number of
vehicle-vehicle collisions, and total distance travelled by over 3 times the
baseline values. The model also improves fuel efficiency by 39% and reduces
carbon emissions by 88%. Results establish feasibility for city-wide 3D traffic
simulation applications incorporating the vision-zero safety principles of the
Department of Transportation, including physics-informed, adaptable, realistic
collision modeling, as well as appropriate reward modeling for real-world
traffic signal light control towards reducing collisions, optimizing traffic
flow and reducing greenhouse emissions.

</details>


### [139] [Causal Discovery in Dynamic Fading Wireless Networks](https://arxiv.org/abs/2506.03163)
*Oluwaseyi Giwa*

Main category: cs.LG

TL;DR: 该论文提出了一种基于顺序回归的算法，结合NOTEARS无环约束，用于动态衰落无线环境中的因果推断，并推导了检测延迟的理论界限。


<details>
  <summary>Details</summary>
Motivation: 无线网络中动态变化的干扰、衰落和移动性使得传统静态因果模型难以适用，需要开发高效的在线因果推断方法以保持网络可靠性。

Method: 提出了一种顺序回归算法，并创新性地应用了NOTEARS无环约束，以实现高效的在线更新。

Result: 理论推导了检测延迟的上下界，并通过蒙特卡洛模拟验证了延迟与网络规模、噪声方差和结构变化幅度的关系。

Conclusion: 研究结果为设计鲁棒的在线因果推断机制提供了理论依据和实践指导，适用于非平稳无线环境。

Abstract: Dynamic causal discovery in wireless networks is essential due to evolving
interference, fading, and mobility, which complicate traditional static causal
models. This paper addresses causal inference challenges in dynamic fading
wireless environments by proposing a sequential regression-based algorithm with
a novel application of the NOTEARS acyclicity constraint, enabling efficient
online updates. We derive theoretical lower and upper bounds on the detection
delay required to identify structural changes, explicitly quantifying their
dependence on network size, noise variance, and fading severity. Monte Carlo
simulations validate these theoretical results, demonstrating linear increases
in detection delay with network size, quadratic growth with noise variance, and
inverse-square dependence on the magnitude of structural changes. Our findings
provide rigorous theoretical insights and practical guidelines for designing
robust online causal inference mechanisms to maintain network reliability under
nonstationary wireless conditions.

</details>


### [140] [Test-Time Scaling of Diffusion Models via Noise Trajectory Search](https://arxiv.org/abs/2506.03164)
*Vignav Ramesh,Morteza Mardani*

Main category: cs.LG

TL;DR: 该论文提出了一种优化扩散模型中噪声轨迹的方法，通过将去噪过程视为马尔可夫决策过程（MDP）并引入一种平衡全局探索和局部利用的搜索算法，显著提升了生成样本的质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在去噪过程中通过增加计算步骤可以提升样本质量，但效果提升有限。优化噪声轨迹（噪声向量序列）是一个有潜力的方向，但由于高维搜索空间和复杂的噪声-结果交互，这一方法面临挑战。

Method: 论文首先将扩散过程建模为带终止奖励的马尔可夫决策过程（MDP），并展示了树搜索方法（如蒙特卡洛树搜索）的局限性。随后，通过将去噪过程松弛为一系列独立的上下文多臂老虎机问题，提出了一种ε-贪婪搜索算法，在极端时间步进行全局探索，在中间步骤进行局部利用。

Result: 在EDM和Stable Diffusion上的实验表明，该方法在类条件生成和文本到图像生成任务中取得了最先进的分数，比基线方法提升了高达164%，且性能与蒙特卡洛树搜索相当或更优。

Conclusion: 这是首个针对任意（不可微分）奖励函数在测试时优化噪声轨迹的实用方法，显著提升了扩散模型的生成质量。

Abstract: The iterative and stochastic nature of diffusion models enables test-time
scaling, whereby spending additional compute during denoising generates
higher-fidelity samples. Increasing the number of denoising steps is the
primary scaling axis, but this yields quickly diminishing returns. Instead
optimizing the noise trajectory--the sequence of injected noise vectors--is
promising, as the specific noise realizations critically affect sample quality;
but this is challenging due to a high-dimensional search space, complex
noise-outcome interactions, and costly trajectory evaluations. We address this
by first casting diffusion as a Markov Decision Process (MDP) with a terminal
reward, showing tree-search methods such as Monte Carlo tree search (MCTS) to
be meaningful but impractical. To balance performance and efficiency, we then
resort to a relaxation of MDP, where we view denoising as a sequence of
independent contextual bandits. This allows us to introduce an
$\epsilon$-greedy search algorithm that globally explores at extreme timesteps
and locally exploits during the intermediate steps where de-mixing occurs.
Experiments on EDM and Stable Diffusion reveal state-of-the-art scores for
class-conditioned/text-to-image generation, exceeding baselines by up to
$164\%$ and matching/exceeding MCTS performance. To our knowledge, this is the
first practical method for test-time noise trajectory optimization of arbitrary
(non-differentiable) rewards.

</details>


### [141] [Non-collective Calibrating Strategy for Time Series Forecasting](https://arxiv.org/abs/2506.03176)
*Bin Wang,Yongqi Han,Minbo Ma,Tianrui Li,Junbo Zhang,Feng Hong,Yanwei Yu*

Main category: cs.LG

TL;DR: 该论文提出了一种名为Socket+Plug (SoP)的通用校准策略，通过优化现有深度学习模型而非从头设计新模型，显著提升了时间序列预测性能，最高可提升22%。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测的复杂性使得设计理想模型架构具有挑战性。论文认为，通过通用校准策略优化现有先进模型，能以较低资源成本获得显著性能提升，而非从头训练新模型。

Method: 提出SoP策略：保留每个预测目标的独立优化器和早停监控器（Plug），同时冻结预训练的主干模型（Socket），以解决多目标学习冲突问题。该方法与模型架构无关。

Result: 在多个时间序列基准测试和ERA5气象数据集上的实验表明，即使使用简单的MLP作为Plug，SoP策略最高可带来22%的性能提升。

Conclusion: SoP是一种高效且通用的校准策略，能够显著提升现有深度学习时间序列预测模型的性能，而无需修改其原始架构。

Abstract: Deep learning-based approaches have demonstrated significant advancements in
time series forecasting. Despite these ongoing developments, the complex
dynamics of time series make it challenging to establish the rule of thumb for
designing the golden model architecture. In this study, we argue that refining
existing advanced models through a universal calibrating strategy can deliver
substantial benefits with minimal resource costs, as opposed to elaborating and
training a new model from scratch. We first identify a multi-target learning
conflict in the calibrating process, which arises when optimizing variables
across time steps, leading to the underutilization of the model's learning
capabilities. To address this issue, we propose an innovative calibrating
strategy called Socket+Plug (SoP). This approach retains an exclusive optimizer
and early-stopping monitor for each predicted target within each Plug while
keeping the fully trained Socket backbone frozen. The model-agnostic nature of
SoP allows it to directly calibrate the performance of any trained deep
forecasting models, regardless of their specific architectures. Extensive
experiments on various time series benchmarks and a spatio-temporal
meteorological ERA5 dataset demonstrate the effectiveness of SoP, achieving up
to a 22% improvement even when employing a simple MLP as the Plug (highlighted
in Figure 1)

</details>


### [142] [Out-of-Vocabulary Sampling Boosts Speculative Decoding](https://arxiv.org/abs/2506.03206)
*Nadav Timor,Jonathan Mamou,Oren Pereg,Hongyang Zhang,David Harel*

Main category: cs.LG

TL;DR: 论文提出RDK方法，通过虚拟恢复被裁剪的目标token来提升推测解码效率，显著提高接受率。


<details>
  <summary>Details</summary>
Motivation: 当前推测解码依赖快速准确的草稿模型，但大词汇表显著拖慢草稿模型速度。现有采样方法无法处理词汇表外token，导致词汇表大小与接受率之间存在矛盾。

Method: 引入Redistributing Drafter Kernels (RDK)，利用token亲和性先验将草稿模型质量重新分配到高重叠区域，并提出线性时间近似实现。

Result: 实验证明RDK在极端裁剪（移除75%以上词汇）后仍能显著提升接受率，而现有采样方法失效。

Conclusion: RDK使得极端裁剪的草稿模型变得可行，解决了词汇表大小与接受率之间的权衡问题。

Abstract: Speculative decoding relies on fast and accurate drafters. Recent
state-of-the-art language models employ larger and larger vocabularies, which
significantly slows down drafters. One promising approach to boost the
efficiency of speculative decoding is to use drafters with smaller
vocabularies. However, existing sampling methods cannot draw out-of-vocabulary
tokens, creating a tradeoff between drafters' vocabulary size and acceptance
rates. This paper introduces Redistributing Drafter Kernels (RDK), the first
out-of-vocabulary sampler that effectively recovers acceptance rates by
virtually restoring pruned target tokens. RDK leverages token-affinity priors
to reallocate drafter mass towards high-overlap regions. We prove
mathematically that RDK can achieve higher acceptance rates than vanilla and
state-of-the-art samplers. We provide an efficient first-order approximation of
RDK and prove that it reduces redistribution times from $O(N^2)$ to $O(N)$,
enabling lightweight implementations for large vocabularies. Our experiments
demonstrate that this linear-time RDK significantly boosts acceptance rates
even after extreme pruning (removing more than 75% of the drafter's
vocabulary), where existing samplers fail. RDK opens the door to extremely
pruned drafters, which were previously impractical.

</details>


### [143] [Fingerprinting Deep Learning Models via Network Traffic Patterns in Federated Learning](https://arxiv.org/abs/2506.03207)
*Md Nahid Hasan Shuvo,Moinul Hossain*

Main category: cs.LG

TL;DR: 该研究通过分析联邦学习中的网络流量数据，证明了深度学习模型架构可以被高精度指纹识别，揭示了FL系统在网络安全层面的潜在漏洞。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）虽能保护数据隐私，但现有研究未关注其可能因网络流量分析导致的间接隐私泄露。本文旨在探索通过流量信息指纹识别FL中深度学习模型的可行性。

Method: 在FL测试环境中部署CNN、RNN等模型，使用SVM、随机森林和梯度提升等算法分析网络层流量数据，识别模型架构的独特模式。

Result: 实验显示随机森林指纹识别准确率达100%，SVM和梯度提升分别达到95.7%，证实可通过流量数据定位特定模型架构。

Conclusion: 研究暴露了FL系统在网络层的安全缺陷，攻击者可能利用架构信息发起定向攻击，需加强网络级防护措施。

Abstract: Federated Learning (FL) is increasingly adopted as a decentralized machine
learning paradigm due to its capability to preserve data privacy by training
models without centralizing user data. However, FL is susceptible to indirect
privacy breaches via network traffic analysis-an area not explored in existing
research. The primary objective of this research is to study the feasibility of
fingerprinting deep learning models deployed within FL environments by
analyzing their network-layer traffic information. In this paper, we conduct an
experimental evaluation using various deep learning architectures (i.e., CNN,
RNN) within a federated learning testbed. We utilize machine learning
algorithms, including Support Vector Machines (SVM), Random Forest, and
Gradient-Boosting, to fingerprint unique patterns within the traffic data. Our
experiments show high fingerprinting accuracy, achieving 100% accuracy using
Random Forest and around 95.7% accuracy using SVM and Gradient Boosting
classifiers. This analysis suggests that we can identify specific architectures
running within the subsection of the network traffic. Hence, if an adversary
knows about the underlying DL architecture, they can exploit that information
and conduct targeted attacks. These findings suggest a notable security
vulnerability in FL systems and the necessity of strengthening it at the
network level.

</details>


### [144] [FuXi-Ocean: A Global Ocean Forecasting System with Sub-Daily Resolution](https://arxiv.org/abs/2506.03210)
*Qiusheng Huang,Yuan Niu,Xiaohui Zhong,Anboyu Guo,Lei Chen,Dianjun Zhang,Xuefeng Zhang,Hao Li*

Main category: cs.LG

TL;DR: FuXi-Ocean是一种新型数据驱动的全球海洋预报模型，首次实现6小时分辨率、1/12°空间分辨率的预测，深度达1500米，通过创新模块减少累积误差。


<details>
  <summary>Details</summary>
Motivation: 传统数值模型计算量大且难以保持高时空精度，而现有数据驱动方法在亚日预测上存在误差累积问题，因此需要开发更高效、精确的海洋预报模型。

Method: 结合上下文感知特征提取模块和堆叠注意力块的预测网络，核心创新是Mixture-of-Time模块，通过学习变量特定可靠性自适应整合多时间上下文预测。

Result: FuXi-Ocean在温度、盐度和洋流等关键变量的多深度预测中展现出卓越性能。

Conclusion: FuXi-Ocean通过数据驱动方法实现了高时空分辨率的海洋预报，显著提升了亚日预测的准确性。

Abstract: Accurate, high-resolution ocean forecasting is crucial for maritime
operations and environmental monitoring. While traditional numerical models are
capable of producing sub-daily, eddy-resolving forecasts, they are
computationally intensive and face challenges in maintaining accuracy at fine
spatial and temporal scales. In contrast, recent data-driven approaches offer
improved computational efficiency and emerging potential, yet typically operate
at daily resolution and struggle with sub-daily predictions due to error
accumulation over time. We introduce FuXi-Ocean, the first data-driven global
ocean forecasting model achieving six-hourly predictions at eddy-resolving
1/12{\deg} spatial resolution, reaching depths of up to 1500 meters. The model
architecture integrates a context-aware feature extraction module with a
predictive network employing stacked attention blocks. The core innovation is
the Mixture-of-Time (MoT) module, which adaptively integrates predictions from
multiple temporal contexts by learning variable-specific reliability ,
mitigating cumulative errors in sequential forecasting. Through comprehensive
experimental evaluation, FuXi-Ocean demonstrates superior skill in predicting
key variables, including temperature, salinity, and currents, across multiple
depths.

</details>


### [145] [Multiple-Frequencies Population-Based Training](https://arxiv.org/abs/2506.03225)
*Waël Doulazmi,Auguste Lehuger,Marin Toromanoff,Valentin Charraut,Thibault Buhet,Fabien Moutarde*

Main category: cs.LG

TL;DR: 论文提出MF-PBT算法，通过多频率子种群和迁移机制解决PBT的短视问题，提升长期性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习对超参数高度敏感，传统PBT算法因频繁选择易陷入局部最优，长期性能甚至不如随机搜索。

Method: 提出MF-PBT：1）建立不同进化频率的子种群 2）设计非对称迁移机制平衡短期/长期优化

Result: 在Brax测试套件上验证，样本效率和长期性能均有提升（无需手动调参）

Conclusion: 进化频率选择是PBT贪婪性的关键因素，多频率架构能有效改善长期优化效果

Abstract: Reinforcement Learning's high sensitivity to hyperparameters is a source of
instability and inefficiency, creating significant challenges for
practitioners. Hyperparameter Optimization (HPO) algorithms have been developed
to address this issue, among them Population-Based Training (PBT) stands out
for its ability to generate hyperparameters schedules instead of fixed
configurations. PBT trains a population of agents, each with its own
hyperparameters, frequently ranking them and replacing the worst performers
with mutations of the best agents. These intermediate selection steps can cause
PBT to focus on short-term improvements, leading it to get stuck in local
optima and eventually fall behind vanilla Random Search over longer timescales.
This paper studies how this greediness issue is connected to the choice of
evolution frequency, the rate at which the selection is done. We propose
Multiple-Frequencies Population-Based Training (MF-PBT), a novel HPO algorithm
that addresses greediness by employing sub-populations, each evolving at
distinct frequencies. MF-PBT introduces a migration process to transfer
information between sub-populations, with an asymmetric design to balance short
and long-term optimization. Extensive experiments on the Brax suite demonstrate
that MF-PBT improves sample efficiency and long-term performance, even without
actually tuning hyperparameters.

</details>


### [146] [Bridging Neural ODE and ResNet: A Formal Error Bound for Safety Verification](https://arxiv.org/abs/2506.03227)
*Abdelrahman Sayed Sayed,Pierre-Jean Meyer,Mohamed Ghazel*

Main category: cs.LG

TL;DR: 该论文通过形式化分析神经ODE与ResNet之间的近似误差，建立两者间的验证代理关系，实现单向验证即可确保另一模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 神经ODE和ResNet虽密切相关，但缺乏形式化的误差界限分析。论文旨在建立两者间的严格数学关系，以简化模型验证过程。

Method: 通过理论推导界定神经ODE与其对应ResNet之间的近似误差范围，构建双向验证代理框架。

Result: 获得误差界限后，只需对任一模型进行安全性验证，经误差扩展后结果可自动适用于另一模型。

Conclusion: 该方法在不动点吸引子系统中验证有效，为连续与离散模型的相互验证提供了理论工具。

Abstract: A neural ordinary differential equation (neural ODE) is a machine learning
model that is commonly described as a continuous depth generalization of a
residual network (ResNet) with a single residual block, or conversely, the
ResNet can be seen as the Euler discretization of the neural ODE. These two
models are therefore strongly related in a way that the behaviors of either
model are considered to be an approximation of the behaviors of the other. In
this work, we establish a more formal relationship between these two models by
bounding the approximation error between two such related models. The obtained
error bound then allows us to use one of the models as a verification proxy for
the other, without running the verification tools twice: if the reachable
output set expanded by the error bound satisfies a safety property on one of
the models, this safety property is then guaranteed to be also satisfied on the
other model. This feature is fully reversible, and the initial safety
verification can be run indifferently on either of the two models. This novel
approach is illustrated on a numerical example of a fixed-point attractor
system modeled as a neural ODE.

</details>


### [147] [DiaBlo: Diagonal Blocks Are Sufficient For Finetuning](https://arxiv.org/abs/2506.03230)
*Selcuk Gurses,Aozhong Zhang,Yanxia Deng,Xun Dong,Xin Li,Naigang Wang,Penghang Yin,Zi Yang*

Main category: cs.LG

TL;DR: DiaBlo是一种高效参数微调方法，通过仅更新选定权重矩阵的对角块，在保持内存效率和训练速度的同时，实现了稳定且鲁棒的收敛性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决全模型微调的高计算和内存成本问题，以及现有参数高效微调方法在性能和收敛性上的不足，提出了DiaBlo方法。

Method: DiaBlo仅更新选定模型权重矩阵的对角块，避免了低秩矩阵乘积的需求，从而无需依赖辅助初始化方案或定制优化策略。

Result: 在常识推理、算术推理、代码生成和安全对齐等任务上，DiaBlo表现出色，保持了高内存效率和快速微调速度。

Conclusion: DiaBlo是一种简单而有效的参数高效微调方法，在多个任务上展现出稳定且一致的性能，同时保持了高效的内存使用和快速的训练速度。

Abstract: Finetuning is a critical step for adapting large language models (LLMs) to
domain-specific downstream tasks. To mitigate the substantial computational and
memory costs of full-model fine-tuning, Parameter-Efficient Finetuning (PEFT)
methods have been proposed to update only a small subset of model parameters.
However, performance gaps between PEFT approaches and full-model fine-tuning
still exist. In this work, we present DiaBlo, a simple yet effective PEFT
approach that updates only the diagonal blocks of selected model weight
matrices. Unlike Low Rank Adaptation (LoRA) and its variants, DiaBlo eliminates
the need for low rank matrix products, thereby avoiding the reliance on
auxiliary initialization schemes or customized optimization strategies to
improve convergence. This design leads to stable and robust convergence while
maintaining comparable memory efficiency and training speed to LoRA. We conduct
extensive experiments across a range of tasks, including commonsense reasoning,
arithmetic reasoning, code generation, and safety alignment, to evaluate the
effectiveness and efficiency of DiaBlo. Across these benchmarks, DiaBlo
demonstrates strong and consistent performance while maintaining high memory
efficiency and fast finetuning speed. Codes are available at
https://github.com/ziyangjoy/DiaBlo.

</details>


### [148] [BadReward: Clean-Label Poisoning of Reward Models in Text-to-Image RLHF](https://arxiv.org/abs/2506.03234)
*Kaiwen Duan,Hongwei Yao,Yufei Chen,Ziyun Li,Tong Qiao,Zhan Qin,Cong Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为BadReward的攻击方法，通过在多模态RLHF中投毒少量自然样本，可诱导文本到图像模型生成不当内容。


<details>
  <summary>Details</summary>
Motivation: RLHF虽能对齐人类偏好，但其反馈机制存在被攻击的风险。现有研究主要针对单模态（文本）攻击，而多模态系统中的威胁尚未充分探索。

Method: BadReward通过构造视觉矛盾的特征碰撞样本污染奖励模型，无需干预偏好标注过程，具有隐蔽性。

Result: 实验证明该方法能稳定诱导目标概念生成偏见/暴力图像，攻击成功率显著高于基线。

Conclusion: 多模态RLHF面临新型安全威胁，需开发针对性防御机制。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning
text-to-image (T2I) models with human preferences. However, RLHF's feedback
mechanism also opens new pathways for adversaries. This paper demonstrates the
feasibility of hijacking T2I models by poisoning a small fraction of preference
data with natural-appearing examples. Specifically, we propose BadReward, a
stealthy clean-label poisoning attack targeting the reward model in multi-modal
RLHF. BadReward operates by inducing feature collisions between visually
contradicted preference data instances, thereby corrupting the reward model and
indirectly compromising the T2I model's integrity. Unlike existing alignment
poisoning techniques focused on single (text) modality, BadReward is
independent of the preference annotation process, enhancing its stealth and
practical threat. Extensive experiments on popular T2I models show that
BadReward can consistently guide the generation towards improper outputs, such
as biased or violent imagery, for targeted concepts. Our findings underscore
the amplified threat landscape for RLHF in multi-modal systems, highlighting
the urgent need for robust defenses. Disclaimer. This paper contains uncensored
toxic content that might be offensive or disturbing to the readers.

</details>


### [149] [On the Necessity of Multi-Domain Explanation: An Uncertainty Principle Approach for Deep Time Series Models](https://arxiv.org/abs/2506.03267)
*Shahbaz Rezaei,Avishai Halev,Xin Liu*

Main category: cs.LG

TL;DR: 该论文提出时间序列模型的多域解释必要性，通过引入不确定性原理（UP）量化时间与频率域解释差异，验证了多域解释的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列可解释性方法（XAI）通常仅在时间域或最稀疏域展示归因，但研究发现某些情况下时间域和频率域的解释可能突出完全不同的特征，而非直接对应关系，因此需要多域解释以实现更全面的模型理解。

Method: 引入量子力学和信号处理中的不确定性原理（UP），作为判断时间与频率域解释是否匹配的理论依据。通过UP下界评估两域归因的差异性，并跨多种深度学习模型、XAI方法和数据集进行验证。

Result: 实验表明，UP违例（即两域解释不匹配）在多种数据集和XAI方法中频繁出现，证明仅依赖时间域解释的局限性，凸显多域解释的必要性。

Conclusion: 时间序列XAI需采用多域解释新范式，UP为判断何时需同时展示两域解释提供了理论工具。

Abstract: A prevailing approach to explain time series models is to generate
attribution in time domain. A recent development in time series XAI is the
concept of explanation spaces, where any model trained in the time domain can
be interpreted with any existing XAI method in alternative domains, such as
frequency. The prevailing approach is to present XAI attributions either in the
time domain or in the domain where the attribution is most sparse. In this
paper, we demonstrate that in certain cases, XAI methods can generate
attributions that highlight fundamentally different features in the time and
frequency domains that are not direct counterparts of one another. This
suggests that both domains' attributions should be presented to achieve a more
comprehensive interpretation. Thus it shows the necessity of multi-domain
explanation. To quantify when such cases arise, we introduce the uncertainty
principle (UP), originally developed in quantum mechanics and later studied in
harmonic analysis and signal processing, to the XAI literature. This principle
establishes a lower bound on how much a signal can be simultaneously localized
in both the time and frequency domains. By leveraging this concept, we assess
whether attributions in the time and frequency domains violate this bound,
indicating that they emphasize distinct features. In other words, UP provides a
sufficient condition that the time and frequency domain explanations do not
match and, hence, should be both presented to the end user. We validate the
effectiveness of this approach across various deep learning models, XAI
methods, and a wide range of classification and forecasting datasets. The
frequent occurrence of UP violations across various datasets and XAI methods
highlights the limitations of existing approaches that focus solely on
time-domain explanations. This underscores the need for multi-domain
explanations as a new paradigm.

</details>


### [150] [Multi-Exit Kolmogorov-Arnold Networks: enhancing accuracy and parsimony](https://arxiv.org/abs/2506.03302)
*James Bagrow,Josh Bongard*

Main category: cs.LG

TL;DR: 多出口KAN网络通过每层独立预测分支实现深度监督，在保持高精度的同时自动发现更简洁、可解释的模型。


<details>
  <summary>Details</summary>
Motivation: 传统Kolmogorov-Arnold网络（KANs）难以确定所需深度且深层网络优化困难，需平衡模型复杂度与可解释性。

Method: 提出多出口KAN架构，每层嵌入预测分支；开发可微分的‘学习退出’算法动态平衡各出口贡献。

Result: 在合成函数、动力系统和真实数据上优于单出口KAN，且更早的简单出口常提供最佳预测。

Conclusion: 多出口KAN为科学建模提供了同时实现高性能与可解释性的实用解决方案。

Abstract: Kolmogorov-Arnold Networks (KANs) uniquely combine high accuracy with
interpretability, making them valuable for scientific modeling. However, it is
unclear a priori how deep a network needs to be for any given task, and deeper
KANs can be difficult to optimize. Here we introduce multi-exit KANs, where
each layer includes its own prediction branch, enabling the network to make
accurate predictions at multiple depths simultaneously. This architecture
provides deep supervision that improves training while discovering the right
level of model complexity for each task. Multi-exit KANs consistently
outperform standard, single-exit versions on synthetic functions, dynamical
systems, and real-world datasets. Remarkably, the best predictions often come
from earlier, simpler exits, revealing that these networks naturally identify
smaller, more parsimonious and interpretable models without sacrificing
accuracy. To automate this discovery, we develop a differentiable "learning to
exit" algorithm that balances contributions from exits during training. Our
approach offers scientists a practical way to achieve both high performance and
interpretability, addressing a fundamental challenge in machine learning for
scientific discovery.

</details>


### [151] [Budgeted Online Active Learning with Expert Advice and Episodic Priors](https://arxiv.org/abs/2506.03307)
*Kristen Goebel,William Solow,Paola Pesantez-Cabrera,Markus Keller,Alan Fern*

Main category: cs.LG

TL;DR: 本文提出了一种在有限标注预算下，从有限时间数据流中进行在线主动学习的新方法，结合专家预测和情景知识，显著提升了农业应用中的预测性能。


<details>
  <summary>Details</summary>
Motivation: 农业应用中，如作物生长季节的每日天气数据，标注成本高昂且预算有限。现有研究未充分考虑标注预算、有限时间范围和情景知识的结合，导致在标注能力严重受限的应用中效果不佳。

Method: 该方法整合了两类先验信息：预存的专家预测器和基于未标注数据流的专家情景行为知识，同时考虑了查询预算、有限时间范围和情景知识。

Result: 实验表明，该方法在真实农业作物模拟器和多种葡萄品种的实际数据上，显著优于基线专家预测、均匀查询选择及忽略情景知识的现有方法，尤其在标注预算极低时表现突出。

Conclusion: 通过结合专家预测和情景知识，该方法在有限标注预算下实现了高效的在线主动学习，为农业等标注成本高的领域提供了实用解决方案。

Abstract: This paper introduces a novel approach to budgeted online active learning
from finite-horizon data streams with extremely limited labeling budgets. In
agricultural applications, such streams might include daily weather data over a
growing season, and labels require costly measurements of weather-dependent
plant characteristics. Our method integrates two key sources of prior
information: a collection of preexisting expert predictors and episodic
behavioral knowledge of the experts based on unlabeled data streams. Unlike
previous research on online active learning with experts, our work
simultaneously considers query budgets, finite horizons, and episodic
knowledge, enabling effective learning in applications with severely limited
labeling capacity. We demonstrate the utility of our approach through
experiments on various prediction problems derived from both a realistic
agricultural crop simulator and real-world data from multiple grape cultivars.
The results show that our method significantly outperforms baseline expert
predictions, uniform query selection, and existing approaches that consider
budgets and limited horizons but neglect episodic knowledge, even under highly
constrained labeling budgets.

</details>


### [152] [The Future of Continual Learning in the Era of Foundation Models: Three Key Directions](https://arxiv.org/abs/2506.03320)
*Jack Bell,Luigi Quarantiello,Eric Nuertey Coleman,Lanpei Li,Malio Li,Mauro Madeddu,Elia Piccoli,Vincenzo Lomonaco*

Main category: cs.LG

TL;DR: 论文探讨了持续学习在大型语言模型时代的重要性，提出了三个关键原因支持其必要性，并指出持续组合性将是持续学习的未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型和基础模型的兴起，人们质疑是否还需要持续学习。作者认为持续学习仍然至关重要，尤其是在模型更新、个性化和模块化组合方面。

Method: 通过分析持续预训练、持续微调和持续组合性三个维度，论证持续学习的必要性及其在未来AI生态系统中的核心地位。

Result: 持续学习不仅能解决知识陈旧和分布偏移问题，还能实现模型的专业化、个性化及动态组合，是未来AI发展的关键。

Conclusion: 持续学习比以往任何时候都更加重要，未来的AI将依赖于持续进化和交互的模型生态系统，而持续组合性将引领持续学习的复兴。

Abstract: Continual learning--the ability to acquire, retain, and refine knowledge over
time--has always been fundamental to intelligence, both human and artificial.
Historically, different AI paradigms have acknowledged this need, albeit with
varying priorities: early expert and production systems focused on incremental
knowledge consolidation, while reinforcement learning emphasised dynamic
adaptation. With the rise of deep learning, deep continual learning has
primarily focused on learning robust and reusable representations over time to
solve sequences of increasingly complex tasks. However, the emergence of Large
Language Models (LLMs) and foundation models has raised the question: Do we
still need continual learning when centralised, monolithic models can tackle
diverse tasks with access to internet-scale knowledge? We argue that continual
learning remains essential for three key reasons: (i) continual pre-training is
still necessary to ensure foundation models remain up to date, mitigating
knowledge staleness and distribution shifts while integrating new information;
(ii) continual fine-tuning enables models to specialise and personalise,
adapting to domain-specific tasks, user preferences, and real-world constraints
without full retraining, avoiding the need for computationally expensive long
context-windows; (iii) continual compositionality offers a scalable and modular
approach to intelligence, enabling the orchestration of foundation models and
agents to be dynamically composed, recombined, and adapted. While continual
pre-training and fine-tuning are explored as niche research directions, we
argue it is continual compositionality that will mark the rebirth of continual
learning. The future of AI will not be defined by a single static model but by
an ecosystem of continually evolving and interacting models, making continual
learning more relevant than ever.

</details>


### [153] [Optimization of Epsilon-Greedy Exploration](https://arxiv.org/abs/2506.03324)
*Ethan Che,Hakan Ceylan,James McInerney,Nathan Kallus*

Main category: cs.LG

TL;DR: 该论文提出了一种基于贝叶斯后悔最小化的框架，通过随机梯度下降（SGD）和模型预测控制（MPC）动态调整推荐系统中的探索率，以优化探索-利用权衡。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统依赖探索来学习用户对新物品的偏好，但探索率的选择受限于批量更新、用户流量变化、短时间范围等实际约束，现有启发式方法难以适应这些复杂情况。

Method: 提出了一种基于贝叶斯后悔最小化的框架，通过随机梯度下降（SGD）动态调整探索率，并结合模型预测控制（MPC）实现实时优化。

Result: 实验表明，批量大小的变化显著影响最优探索策略，所提方法能自动校准探索率，在不同设置下均匹配或优于最佳启发式方法。

Conclusion: 该框架为推荐系统中的探索-利用权衡提供了原则性解决方案，能够适应实际约束并动态优化探索策略。

Abstract: Modern recommendation systems rely on exploration to learn user preferences
for new items, typically implementing uniform exploration policies (e.g.,
epsilon-greedy) due to their simplicity and compatibility with machine learning
(ML) personalization models. Within these systems, a crucial consideration is
the rate of exploration - what fraction of user traffic should receive random
item recommendations and how this should evolve over time. While various
heuristics exist for navigating the resulting exploration-exploitation
tradeoff, selecting optimal exploration rates is complicated by practical
constraints including batched updates, time-varying user traffic, short time
horizons, and minimum exploration requirements. In this work, we propose a
principled framework for determining the exploration schedule based on directly
minimizing Bayesian regret through stochastic gradient descent (SGD), allowing
for dynamic exploration rate adjustment via Model-Predictive Control (MPC).
Through extensive experiments with recommendation datasets, we demonstrate that
variations in the batch size across periods significantly influence the optimal
exploration strategy. Our optimization methods automatically calibrate
exploration to the specific problem setting, consistently matching or
outperforming the best heuristic for each setting.

</details>


### [154] [Hierarchical Relational Learning for Few-Shot Knowledge Graph Completion](https://arxiv.org/abs/2209.01205)
*Han Wu,Jie Yin,Bala Rajaratnam,Jianyuan Guo*

Main category: cs.LG

TL;DR: 提出分层关系学习方法HiRe，通过联合捕捉实体级、三元组级和上下文级关系信息，有效学习少样本关系表示，提升知识图谱补全性能。


<details>
  <summary>Details</summary>
Motivation: 现有少样本知识图谱补全方法主要关注局部邻居聚合或无效序列依赖假设，忽略了三元组间交互和上下文关系信息。

Method: HiRE方法：分层学习实体级、三元组级和上下文级关系信息，联合优化少样本关系表示。

Result: 在基准数据集上超越现有最优方法，代码已开源。

Conclusion: 分层关系学习能有效捕获多级信息，显著提升少样本关系推理能力。

Abstract: Knowledge graphs (KGs) are powerful in terms of their inference abilities,
but are also notorious for their incompleteness and long-tail distribution of
relations. To address these challenges and expand the coverage of KGs, few-shot
KG completion aims to make predictions for triplets involving novel relations
when only a few training triplets are provided as reference. Previous methods
have focused on designing local neighbor aggregators to learn entity-level
information and/or imposing a potentially invalid sequential dependency
assumption at the triplet level to learn meta relation information. However,
pairwise triplet-level interactions and context-level relational information
have been largely overlooked for learning meta representations of few-shot
relations. In this paper, we propose a hierarchical relational learning method
(HiRe) for few-shot KG completion. By jointly capturing three levels of
relational information (entity-level, triplet-level and context-level), HiRe
can effectively learn and refine meta representations of few-shot relations,
and thus generalize well to new unseen relations. Extensive experiments on
benchmark datasets validate the superiority of HiRe over state-of-the-art
methods. The code can be found in https://github.com/alexhw15/HiRe.git.

</details>


### [155] [A Differential Perspective on Distributional Reinforcement Learning](https://arxiv.org/abs/2506.03333)
*Juan Sebastian Rojas,Chi-Guhn Lee*

Main category: cs.LG

TL;DR: 本文扩展了分布强化学习（distributional RL）到平均奖励设置，开发了首个能学习和优化长期每步奖励分布及平均奖励MDP差分回报分布的算法。


<details>
  <summary>Details</summary>
Motivation: 现有的分布强化学习方法仅关注折扣设置，而本文旨在解决平均奖励设置下的优化问题，以优化每步奖励为目标。

Method: 采用基于分位数的方法，开发了收敛的表格算法（预测和控制），以及具有良好扩展性的算法家族。

Result: 实验表明，这些算法在性能上与非分布方法相当，同时能捕捉长期奖励和回报分布的丰富信息。

Conclusion: 本文成功将分布强化学习扩展到平均奖励设置，为优化长期每步奖励分布提供了有效工具。

Abstract: To date, distributional reinforcement learning (distributional RL) methods
have exclusively focused on the discounted setting, where an agent aims to
optimize a potentially-discounted sum of rewards over time. In this work, we
extend distributional RL to the average-reward setting, where an agent aims to
optimize the reward received per time-step. In particular, we utilize a
quantile-based approach to develop the first set of algorithms that can
successfully learn and/or optimize the long-run per-step reward distribution,
as well as the differential return distribution of an average-reward MDP. We
derive proven-convergent tabular algorithms for both prediction and control, as
well as a broader family of algorithms that have appealing scaling properties.
Empirically, we find that these algorithms consistently yield competitive
performance when compared to their non-distributional equivalents, while also
capturing rich information about the long-run reward and return distributions.

</details>


### [156] [Mitigating Non-IID Drift in Zeroth-Order Federated LLM Fine-Tuning with Transferable Sparsity](https://arxiv.org/abs/2506.03337)
*Yide Ran,Wentao Guo,Jingwei Sun,Yanzhou Pan,Xiaodong Yu,Hao Wang,Jianwen Xie,Yiran Chen,Denghui Zhang,Zhaozhuo Xu*

Main category: cs.LG

TL;DR: Meerkat提出了一种稀疏零阶优化方法，用于联邦学习中大型语言模型的高效微调，通过减少通信开销并处理非独立同分布数据挑战。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非独立同分布数据上微调大型语言模型时面临内存和通信效率低下的问题，需要一种更高效的优化方法。

Method: Meerkat采用稀疏零阶优化方法，仅微调可转移的静态稀疏参数子集，并利用虚拟路径机制识别极端非独立同分布客户端。

Result: 实验表明，Meerkat在相同通信频率下优于现有稀疏基线，并通过GradIP现象有效识别数据异构性，提升模型质量。

Conclusion: Meerkat及其变体Meerkat-vp显著提高了零阶联邦学习微调的效率和效果，适用于处理极端非独立同分布数据。

Abstract: Federated Learning enables collaborative fine-tuning of Large Language Models
(LLMs) across decentralized Non-Independent and Identically Distributed
(Non-IID) clients, but such models' massive parameter sizes lead to significant
memory and communication challenges. This work introduces Meerkat, a sparse
zeroth-order optimization (ZO) method designed for federated LLM fine-tuning.
By limiting fine-tuning to a transferable, static, extremely sparse subset of
parameters, Meerkat achieves remarkable communication efficiency, enabling
cost-effective high-frequency synchronization. With theoretical analysis and
experiments, we show that this high-frequency communication effectively
mitigates Non-IID data challenges and leads to superior performance compared to
full-parameter ZO. Furthermore, experiment results show that Meerkat
outperforms existing sparsity baselines with better performance at the same
communication frequency. To further handle Non-IID drift, Meerkat leverages
traceable local updates and forms a virtual path for each client. This virtual
path mechanism reveals the GradIP phenomenon: the inner products between LLM
pre-training gradients maintained by server and client gradients estimated via
ZO converges for extreme Non-IID clients but oscillates for IID ones. This
distinct behavior provides a signal for identifying clients with extreme data
heterogeneity. Using this signal, Meerkat-vp is proposed to analyze GradIP
trajectories to identify extreme Non-IID clients and applies early stopping to
enhance aggregated model quality. Experiments confirm that Meerkat and
Meerkat-vp significantly improve the efficiency and effectiveness of ZO
federated LLM fine-tuning.

</details>


### [157] [Robustness in Both Domains: CLIP Needs a Robust Text Encoder](https://arxiv.org/abs/2506.03355)
*Elias Abad Rocamora,Christian Schlarmann,Naman Deep Singh,Yongtao Wu,Matthias Hein,Volkan Cevher*

Main category: cs.LG

TL;DR: 本文提出LEAF方法，通过对抗微调提升CLIP文本编码器的鲁棒性，同时保持视觉性能，并在多模态任务中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注CLIP图像编码器的鲁棒性，而文本编码器的鲁棒性尚未探索。对抗输入攻击可能导致CLIP嵌入显著偏移，影响下游模型（如文本生成图像模型）的鲁棒性。

Method: 提出LEAF方法，一种高效的文本域对抗微调方法，能够扩展到大型CLIP模型。

Result: LEAF显著提高了文本域的零样本对抗准确性，同时保持视觉性能；结合文本到图像扩散模型时，改善了对抗噪声下的生成质量；在多模态检索任务中提升了对抗噪声下的召回率；鲁棒文本编码器有助于通过直接优化更好地重建输入文本。

Conclusion: LEAF方法有效提升了CLIP文本编码器的鲁棒性，并在多种任务中验证了其优势，填补了相关研究空白。

Abstract: Adversarial input attacks can cause a significant shift of CLIP embeddings.
This can affect the downstream robustness of models incorporating CLIP in the
pipeline, such as text-to-image generative models or large vision language
models. While some efforts have been done towards making the CLIP image
encoders robust, the robustness of text encoders remains unexplored. In this
work, we cover this gap in the literature. We propose LEAF: an efficient
adversarial finetuning method for the text domain, with the ability to scale to
large CLIP models. Our models significantly improve the zero-shot adversarial
accuracy in the text domain, while maintaining the vision performance provided
by robust image encoders. When combined with text-to-image diffusion models, we
can improve the generation quality under adversarial noise. When employing our
robust CLIP encoders in multimodal retrieval tasks, we improve the recall under
adversarial noise over standard CLIP models. Finally, we show that robust text
encoders facilitate better reconstruction of input text from its embedding via
direct optimization.

</details>


### [158] [Probabilistic Factorial Experimental Design for Combinatorial Interventions](https://arxiv.org/abs/2506.03363)
*Divya Shyamal,Jiaqi Zhang,Caroline Uhler*

Main category: cs.LG

TL;DR: 该论文提出了一种概率性因子实验设计方法，用于高效估计组合干预效应，解决了传统全组合实验不可行的问题。


<details>
  <summary>Details</summary>
Motivation: 在生物医学、工程等领域，组合干预（多种处理同时应用于单个单元）具有广泛应用，但全组合实验随着处理数量增加变得不可行。

Method: 采用概率性因子实验设计，通过伯努利分布随机分配处理组合，支持多轮主动调整设计。

Result: 证明1/2剂量对估计任意k阶交互模型接近最优，所需观测次数为O(kp^(3k)ln(p))，并提出了多轮设置的近似最优采集函数。

Conclusion: 该方法通过理论证明和仿真验证，为组合干预效应的高效估计提供了可行方案。

Abstract: A combinatorial intervention, consisting of multiple treatments applied to a
single unit with potentially interactive effects, has substantial applications
in fields such as biomedicine, engineering, and beyond. Given $p$ possible
treatments, conducting all possible $2^p$ combinatorial interventions can be
laborious and quickly becomes infeasible as $p$ increases. Here we introduce
probabilistic factorial experimental design, formalized from how scientists
perform lab experiments. In this framework, the experimenter selects a dosage
for each possible treatment and applies it to a group of units. Each unit
independently receives a random combination of treatments, sampled from a
product Bernoulli distribution determined by the dosages. Additionally, the
experimenter can carry out such experiments over multiple rounds, adapting the
design in an active manner. We address the optimal experimental design problem
within an intervention model that imposes bounded-degree interactions between
treatments. In the passive setting, we provide a closed-form solution for the
near-optimal design. Our results prove that a dosage of $\tfrac{1}{2}$ for each
treatment is optimal up to a factor of $1+O(\tfrac{\ln(n)}{n})$ for estimating
any $k$-way interaction model, regardless of $k$, and imply that
$O\big(kp^{3k}\ln(p)\big)$ observations are required to accurately estimate
this model. For the multi-round setting, we provide a near-optimal acquisition
function that can be numerically optimized. We also explore several extensions
of the design problem and finally validate our findings through simulations.

</details>


### [159] [Comparison of different Unique hard attention transformer models by the formal languages they can recognize](https://arxiv.org/abs/2506.03370)
*Leonid Ryvkin*

Main category: cs.LG

TL;DR: 本文综述了独特硬注意力变换器编码器（UHATs）在识别形式语言能力方面的各种结果，区分了不同类型和功能。


<details>
  <summary>Details</summary>
Motivation: 研究UHATs在识别形式语言方面的能力，探讨不同模型之间的关系及其计算复杂性。

Method: 区分了掩码与非掩码、有限与无限图像以及一般与双线性注意力评分函数的不同模型。

Result: 回顾了这些模型之间的关系，并提出了基于一阶逻辑的下界和电路复杂性的上界。

Conclusion: UHATs在形式语言识别方面具有多样化的能力，其性能受模型类型和注意力评分函数的影响。

Abstract: This note is a survey of various results on the capabilities of unique hard
attention transformers encoders (UHATs) to recognize formal languages. We
distinguish between masked vs. non-masked, finite vs. infinite image and
general vs. bilinear attention score functions. We recall some relations
between these models, as well as a lower bound in terms of first-order logic
and an upper bound in terms of circuit complexity.

</details>


### [160] [Product Quantization for Surface Soil Similarity](https://arxiv.org/abs/2506.03374)
*Haley Dozier,Althea Henslee,Ashley Abraham,Andrew Strelzoff,Mark Chappell*

Main category: cs.LG

TL;DR: 机器学习技术用于土壤分类，克服传统人为分类的局限性，提供更高精度和灵活性的分类方法。


<details>
  <summary>Details</summary>
Motivation: 传统土壤分类依赖人工和历史理解，缺乏数据驱动的统计相似性，限制了分类的准确性和灵活性。

Method: 结合产品量化和系统参数评估的机器学习流程，避免使用默认或猜测设置，优化分类结果。

Result: 机器学习方法能够处理高维数据，生成比人工分类更具体、更准确的土壤分类体系。

Conclusion: 机器学习为土壤分类提供了更高效、更精确的解决方案，适用于特定应用场景。

Abstract: The use of machine learning (ML) techniques has allowed rapid advancements in
many scientific and engineering fields. One of these problems is that of
surface soil taxonomy, a research area previously hindered by the reliance on
human-derived classifications, which are mostly dependent on dividing a dataset
based on historical understandings of that data rather than data-driven,
statistically observable similarities. Using a ML-based taxonomy allows soil
researchers to move beyond the limitations of human visualization and create
classifications of high-dimension datasets with a much higher level of
specificity than possible with hand-drawn taxonomies. Furthermore, this
pipeline allows for the possibility of producing both highly accurate and
flexible soil taxonomies with classes built to fit a specific application. The
machine learning pipeline outlined in this work combines product quantization
with the systematic evaluation of parameters and output to get the best
available results, rather than accepting sub-optimal results by using either
default settings or best guess settings.

</details>


### [161] [Improving Performance of Spike-based Deep Q-Learning using Ternary Neurons](https://arxiv.org/abs/2506.03392)
*Aref Ghoreishee,Abhishek Mishra,John Walsh,Anup Das,Nagarajan Kandasamy*

Main category: cs.LG

TL;DR: 提出新型三元脉冲神经元模型，通过减少梯度估计偏差提升深度Q学习性能，在Atari游戏中验证优于二元模型。


<details>
  <summary>Details</summary>
Motivation: 现有二元脉冲神经元表征能力有限，而近期提出的三元模型在深度Q学习中表现反而不如二元模型。作者推测训练过程中的梯度估计偏差是潜在原因。

Method: 设计新型三元脉冲神经元模型降低梯度估计偏差，并将其作为深度脉冲Q网络(DSQN)的基础计算单元。

Result: 在Gym环境的7款Atari游戏中测试，新三元模型缓解了三元神经元性能骤降问题，网络表现优于现有二元神经元。

Conclusion: 所提三元脉冲神经元使DSQN更适用于车载自主决策任务，为脉冲神经网络在强化学习中的应用提供实用解决方案。

Abstract: We propose a new ternary spiking neuron model to improve the representation
capacity of binary spiking neurons in deep Q-learning. Although a ternary
neuron model has recently been introduced to overcome the limited
representation capacity offered by the binary spiking neurons, we show that its
performance is worse than that of binary models in deep Q-learning tasks. We
hypothesize gradient estimation bias during the training process as the
underlying potential cause through mathematical and empirical analysis. We
propose a novel ternary spiking neuron model to mitigate this issue by reducing
the estimation bias. We use the proposed ternary spiking neuron as the
fundamental computing unit in a deep spiking Q-learning network (DSQN) and
evaluate the network's performance in seven Atari games from the Gym
environment. Results show that the proposed ternary spiking neuron mitigates
the drastic performance degradation of ternary neurons in Q-learning tasks and
improves the network performance compared to the existing binary neurons,
making DSQN a more practical solution for on-board autonomous decision-making
tasks.

</details>


### [162] [The Impact of On-Policy Parallelized Data Collection on Deep Reinforcement Learning Networks](https://arxiv.org/abs/2506.03404)
*Walter Mayor,Johan Obando-Ceron,Aaron Courville,Pablo Samuel Castro*

Main category: cs.LG

TL;DR: 本文通过实证分析PPO算法中并行环境数量与数据收集长度的权衡，发现扩大数据集规模可提升性能，且增加并行环境比延长数据收集长度更有效。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索强化学习算法中并行数据收集策略对性能的影响，特别是在偏差-方差权衡和样本效率与过拟合之间的平衡。

Method: 采用PPO算法进行实证分析，考察并行环境数量和数据收集长度对网络可塑性及优化稳定性的影响。

Result: 结果表明，更大的数据集规模能提升最终性能，且增加并行环境比延长数据收集长度更有效。

Conclusion: 数据收集策略在提升智能体性能中起关键作用，优化并行环境配置比调整数据收集长度更有效。

Abstract: The use of parallel actors for data collection has been an effective
technique used in reinforcement learning (RL) algorithms. The manner in which
data is collected in these algorithms, controlled via the number of parallel
environments and the rollout length, induces a form of bias-variance trade-off;
the number of training passes over the collected data, on the other hand, must
strike a balance between sample efficiency and overfitting. We conduct an
empirical analysis of these trade-offs on PPO, one of the most popular RL
algorithms that uses parallel actors, and establish connections to network
plasticity and, more generally, optimization stability. We examine its impact
on network architectures, as well as the hyper-parameter sensitivity when
scaling data. Our analyses indicate that larger dataset sizes can increase
final performance across a variety of settings, and that scaling parallel
environments is more effective than increasing rollout lengths. These findings
highlight the critical role of data collection strategies in improving agent
performance.

</details>


### [163] [A Machine Learning Theory Perspective on Strategic Litigation](https://arxiv.org/abs/2506.03411)
*Melissa Dutz,Han Shao,Avrim Blum,Aloni Cohen*

Main category: cs.LG

TL;DR: 该论文从机器学习理论视角探讨策略性诉讼，研究诉讼者如何通过选择案件影响法院判决规则，进而塑造未来判例。


<details>
  <summary>Details</summary>
Motivation: 研究策略性诉讼在普通法体系中的作用，探索诉讼者如何通过精心挑选案件来影响高阶法院的判决规则，从而对未来的法律裁决产生广泛影响。

Method: 构建一个抽象模型，模拟下级法院通过学习上级法院过往判例形成判决规则的过程，并分析策略性诉讼者如何通过选择特定案件来优化这一学习过程。

Result: 研究发现策略性诉讼者能显著影响判决规则的演变，即使在某些明知会败诉的案件中，提起诉讼仍可能对未来判例产生积极影响。

Conclusion: 策略性诉讼是塑造法律体系的有力工具，诉讼者的案件选择能系统性改变判决规则的演化路径，这为理解法律系统的动态性提供了新的理论框架。

Abstract: Strategic litigation involves bringing a legal case to court with the goal of
having a broader impact beyond resolving the case itself: for example, creating
precedent which will influence future rulings. In this paper, we explore
strategic litigation from the perspective of machine learning theory. We
consider an abstract model of a common-law legal system where a lower court
decides new cases by applying a decision rule learned from a higher court's
past rulings. In this model, we explore the power of a strategic litigator, who
strategically brings cases to the higher court to influence the learned
decision rule, thereby affecting future cases. We explore questions including:
What impact can a strategic litigator have? Which cases should a strategic
litigator bring to court? Does it ever make sense for a strategic litigator to
bring a case when they are sure the court will rule against them?

</details>


### [164] [Adaptive Task Vectors for Large Language Models](https://arxiv.org/abs/2506.03426)
*Joonseong Kang,Soojeong Lee,Subeen Park,Sumin Park,Taero Kim,Jihee Kim,Ryunyi Lee,Kyungwoo Song*

Main category: cs.LG

TL;DR: 论文提出自适应任务向量（ATV）框架，通过动态生成针对每个输入查询的任务向量，解决了传统上下文学习（ICL）和固定任务向量方法的局限性，提升了模型在未见任务上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统上下文学习（ICL）存在对演示顺序敏感、上下文长度限制和计算效率低等问题，而固定任务向量方法无法根据输入查询动态调整，导致泛化性能下降。

Method: 提出自适应任务向量（ATV）框架，利用小型语言模型动态生成针对每个输入查询的任务向量，并将其转换为目标大语言模型的结构以指导输出生成。

Result: ATV在未见任务上表现出强大的性能和泛化能力，理论分析表明其在表达力上等同于LoRA且优于Prefix-Tuning。

Conclusion: ATV通过动态生成任务向量，克服了传统方法的局限性，为提升大语言模型的适应性和泛化能力提供了有效解决方案。

Abstract: In-Context Learning (ICL) enables Large Language Models (LLMs) to perform
tasks without parameter updates by conditioning on a few demonstrations
provided in the prompt. Despite its success, ICL suffers from several
limitations, including sensitivity to demonstration order, context length
constraints, and computational inefficiency. To address these challenges, task
vector-based approaches compress task information into a single vector.
However, these methods typically construct task vectors from fixed sets of
demonstrations and reuse them across input queries, without conditioning on the
specific input. This limitation can lead models to struggle with effective
adaptation when the input query is not well aligned with the underlying
demonstrations, consequently degrading their generalization performance on
unseen tasks. To overcome this limitation, we propose Adaptive Task Vectors
(ATV), a simple and effective framework that dynamically generates task vectors
conditioned on each input query. ATV employs a small language model to generate
task vectors, which are then transformed to match the target LLM's architecture
and applied to guide its output generation. In contrast to ICL and previous
vector-based approaches, which rely on fixed demonstration sets and their
corresponding vectors, ATV dynamically generates task vectors tailored to each
specific input query and task. Consequently, ATV demonstrates strong
performance and generalization capabilities, even for unseen tasks.
Furthermore, we provide a theoretical analysis indicating that ATV is
expressively equivalent to LoRA under equal rank budgets and more expressive
than Prefix-Tuning, thereby offering formal support for its representational
advantage.

</details>


### [165] [Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior](https://arxiv.org/abs/2506.03444)
*Yue Gong,Raul Castro Fernandez*

Main category: cs.LG

TL;DR: 论文提出利用LLM权重自动评估统计关系（如相关性）的新颖性，通过Logit-based Calibrated Prior方法预测相关性，效果优于微调模型。


<details>
  <summary>Details</summary>
Motivation: 随着假设生成自动化程度提高，假设评估成为新瓶颈。现有系统能发现大量统计关系，但无法判断哪些值得专家关注。

Method: 提出Logit-based Calibrated Prior方法，利用LLM权重建模变量对的先验相关性分布，将原始logits转化为校准后的连续预测分布。

Result: 在2096个真实变量对测试中，相关性符号预测准确率78.8%，MAE为0.26，95%置信区间覆盖率达89.2%，且优于微调RoBERTa模型。

Conclusion: LLM先验能有效评估统计关系的新颖性，其上下文敏感推理能力可泛化至预训练未见的关联关系。

Abstract: As hypothesis generation becomes increasingly automated, a new bottleneck has
emerged: hypothesis assessment. Modern systems can surface thousands of
statistical relationships-correlations, trends, causal links-but offer little
guidance on which ones are novel, non-trivial, or worthy of expert attention.
In this work, we study the complementary problem to hypothesis generation:
automatic hypothesis assessment. Specifically, we ask: given a large set of
statistical relationships, can we automatically assess which ones are novel and
worth further exploration? We focus on correlations as they are a common entry
point in exploratory data analysis that often serve as the basis for forming
deeper scientific or causal hypotheses.
  To support automatic assessment, we propose to leverage the vast knowledge
encoded in LLMs' weights to derive a prior distribution over the correlation
value of a variable pair. If an LLM's prior expects the correlation value
observed, then such correlation is not surprising, and vice versa. We propose
the Logit-based Calibrated Prior, an LLM-elicited correlation prior that
transforms the model's raw output logits into a calibrated, continuous
predictive distribution over correlation values. We evaluate the prior on a
benchmark of 2,096 real-world variable pairs and it achieves a sign accuracy of
78.8%, a mean absolute error of 0.26, and 95% credible interval coverage of
89.2% in predicting Pearson correlation coefficient. It also outperforms a
fine-tuned RoBERTa classifier in binary correlation prediction and achieves
higher precision@K in hypothesis ranking. We further show that the prior
generalizes to correlations not seen during LLM pretraining, reflecting
context-sensitive reasoning rather than memorization.

</details>


### [166] [Directional Non-Commutative Monoidal Embeddings for MNIST](https://arxiv.org/abs/2506.03472)
*Mahesh Godavarti*

Main category: cs.LG

TL;DR: 该论文通过MNIST数据集验证了方向性非交换幺半群嵌入框架的有效性，证明其优于固定DFT嵌入，尤其在低维时表现更佳。


<details>
  <summary>Details</summary>
Motivation: 验证方向性非交换幺半群嵌入框架是否能有效建模真实数据，并探索其通过可学习任务特定频率成分优于固定DFT嵌入的假设。

Method: 在MNIST数据集上比较可学习幺半群嵌入与固定DFT嵌入的性能，尤其关注不同嵌入维度下的表现差异。

Result: 随着嵌入维度降低，可学习幺半群嵌入的性能显著优于固定DFT嵌入，表明其能捕捉更具判别性的频谱成分。

Conclusion: 方向性非交换幺半群嵌入框架能高效表示图像数据，提供紧凑且高性能的学习表示。

Abstract: We present an empirical validation of the directional non-commutative
monoidal embedding framework recently introduced in prior
work~\cite{Godavarti2025monoidal}. This framework defines learnable
compositional embeddings using distinct non-commutative operators per dimension
(axis) that satisfy an interchange law, generalizing classical one-dimensional
transforms. Our primary goal is to verify that this framework can effectively
model real data by applying it to a controlled, well-understood task: image
classification on the MNIST dataset~\cite{lecun1998gradient}. A central
hypothesis for why the proposed monoidal embedding works well is that it
generalizes the Discrete Fourier Transform (DFT)~\cite{oppenheim1999discrete}
by learning task-specific frequency components instead of using fixed basis
frequencies. We test this hypothesis by comparing learned monoidal embeddings
against fixed DFT-based embeddings on MNIST. The results show that as the
embedding dimensionality decreases (e.g., from 32 to 8 to 2), the performance
gap between the learned monoidal embeddings and fixed DFT-based embeddings on
MNIST grows increasingly large. This comparison is used as an analytic tool to
explain why the framework performs well: the learnable embeddings can capture
the most discriminative spectral components for the task. Overall, our
experiments confirm that directional non-commutative monoidal embeddings are
highly effective for representing image data, offering a compact learned
representation that retains high task performance. The code used in this work
is available at
https://github.com/mahesh-godavarti/directional_composition_mnist.

</details>


### [167] [CORE: Constraint-Aware One-Step Reinforcement Learning for Simulation-Guided Neural Network Accelerator Design](https://arxiv.org/abs/2506.03474)
*Yifeng Xiao,Yurong Xu,Ning Yan,Masood Mortazavi,Pierluigi Nuzzo*

Main category: cs.LG

TL;DR: 论文提出CORE方法，通过约束感知的一步强化学习优化高维设计空间探索，提升采样效率并满足约束条件。


<details>
  <summary>Details</summary>
Motivation: 现有方法在稀疏延迟反馈和大规模混合动作空间下，难以平衡采样效率和约束满足，因此需要更高效的设计空间探索方法。

Method: CORE采用约束感知的一步强化学习，通过结构化分布和奖励塑形优化设计配置采样，无需学习价值函数。

Result: CORE在神经网络加速器的硬件映射协同设计中表现出色，显著提升采样效率并优于现有基线方法。

Conclusion: CORE是一种通用方法，适用于广泛的离散-连续约束设计问题，具有高效性和优越性。

Abstract: Simulation-based design space exploration (DSE) aims to efficiently optimize
high-dimensional structured designs under complex constraints and expensive
evaluation costs. Existing approaches, including heuristic and multi-step
reinforcement learning (RL) methods, struggle to balance sampling efficiency
and constraint satisfaction due to sparse, delayed feedback, and large hybrid
action spaces. In this paper, we introduce CORE, a constraint-aware, one-step
RL method for simulationguided DSE. In CORE, the policy agent learns to sample
design configurations by defining a structured distribution over them,
incorporating dependencies via a scaling-graph-based decoder, and by reward
shaping to penalize invalid designs based on the feedback obtained from
simulation. CORE updates the policy using a surrogate objective that compares
the rewards of designs within a sampled batch, without learning a value
function. This critic-free formulation enables efficient learning by
encouraging the selection of higher-reward designs. We instantiate CORE for
hardware-mapping co-design of neural network accelerators, demonstrating that
it significantly improves sample efficiency and achieves better accelerator
configurations compared to state-of-the-art baselines. Our approach is general
and applicable to a broad class of discrete-continuous constrained design
problems.

</details>


### [168] [Path Generation and Evaluation in Video Games: A Nonparametric Statistical Approach](https://arxiv.org/abs/2506.03522)
*Daniel Campa,Mehdi Saeedi,Ian Colbert,Srinjoy Das*

Main category: cs.LG

TL;DR: 本文提出了一种基于非参数统计的新型路径生成与评估方法，旨在解决游戏行业中深度学习模型路径生成存在的训练复杂和可解释性差的问题。


<details>
  <summary>Details</summary>
Motivation: 游戏设计中，导航路径轨迹对提升玩家体验和优化非玩家角色行为至关重要。然而，尽管深度学习在生成模型方面取得了显著进展，游戏行业因其复杂的训练需求和可解释性挑战而犹豫采用。本文旨在解决这些问题。

Method: 方法结合了两种统计技术：(1) 非参数无模型变换，通过时间捕捉路径轨迹的统计特征；(2) copula模型，捕捉空间统计依赖性。路径评估采用非参数三样本假设检验，判断生成路径是否过拟合或欠拟合。

Result: 在两个现有游戏基准上的实证分析展示了方法的精确性和可靠性，能够生成多样化的导航路径。新路径生成器可通过用户可控参数调整，生成不同人类相似度的路径。

Conclusion: 本文提出的方法为游戏设计提供了可控、可解释且高效的路径生成与评估方案，解决了深度学习模型在游戏行业应用中的主要障碍。

Abstract: Navigation path traces play a crucial role in video game design, serving as a
vital resource for both enhancing player engagement and fine-tuning
non-playable character behavior. Generating such paths with human-like realism
can enrich the overall gaming experience, and evaluating path traces can
provide game designers insights into player interactions. Despite the
impressive recent advancements in deep learning-based generative modeling, the
video game industry hesitates to adopt such models for path generation, often
citing their complex training requirements and interpretability challenges. To
address these problems, we propose a novel path generation and evaluation
approach that is grounded in principled nonparametric statistics and provides
precise control while offering interpretable insights. Our path generation
method fuses two statistical techniques: (1) nonparametric model-free
transformations that capture statistical characteristics of path traces through
time; and (2) copula models that capture statistical dependencies in space. For
path evaluation, we adapt a nonparametric three-sample hypothesis test designed
to determine if the generated paths are overfit (mimicking the original data
too closely) or underfit (diverging too far from it). We demonstrate the
precision and reliability of our proposed methods with empirical analysis on
two existing gaming benchmarks to showcase controlled generation of diverse
navigation paths. Notably, our novel path generator can be fine-tuned with user
controllable parameters to create navigation paths that exhibit varying levels
of human-likeness in contrast to those produced by neural network-based agents.
The code is available at https://github.com/daniel-campa/mf-copula.

</details>


### [169] [Conformal Mixed-Integer Constraint Learning with Feasibility Guarantees](https://arxiv.org/abs/2506.03531)
*Daniel Ovalle,Lorenz T. Biegler,Ignacio E. Grossmann,Carl D. Laird,Mateo Dulce Rubio*

Main category: cs.LG

TL;DR: C-MICL框架通过保形预测为优化问题中的数据驱动约束提供概率可行性保证，确保解的真实可行性，同时降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统混合整数约束学习方法因模型误差或数据限制常违反真实约束，需要一种能提供严格统计保证的方法来确保解的可行性。

Method: C-MICL利用保形预测，在条件独立性假设下，为回归和分类任务提供概率可行性保证，无需真实约束函数且避免集成方法的可扩展性问题。

Result: 实验表明，C-MICL能稳定达到目标可行性率，保持竞争力目标性能，并显著降低计算成本。

Conclusion: C-MICL在数学优化与机器学习间架起桥梁，为决策提供具有严格统计保证的不确定性感知约束方法。

Abstract: We propose Conformal Mixed-Integer Constraint Learning (C-MICL), a novel
framework that provides probabilistic feasibility guarantees for data-driven
constraints in optimization problems. While standard Mixed-Integer Constraint
Learning methods often violate the true constraints due to model error or data
limitations, our C-MICL approach leverages conformal prediction to ensure
feasible solutions are ground-truth feasible. This guarantee holds with
probability at least $1{-}\alpha$, under a conditional independence assumption.
The proposed framework supports both regression and classification tasks
without requiring access to the true constraint function, while avoiding the
scalability issues associated with ensemble-based heuristics. Experiments on
real-world applications demonstrate that C-MICL consistently achieves target
feasibility rates, maintains competitive objective performance, and
significantly reduces computational cost compared to existing methods. Our work
bridges mathematical optimization and machine learning, offering a principled
approach to incorporate uncertainty-aware constraints into decision-making with
rigorous statistical guarantees.

</details>


### [170] [Learning Monotonic Probabilities with a Generative Cost Model](https://arxiv.org/abs/2506.03542)
*Yongxiang Tang,Yanhua Cheng,Xiaocheng Liu,Chenchen Jiao,Yanxiang Zeng,Ning Luo,Pengjia Yuan,Xialong Liu,Peng Jiang*

Main category: cs.LG

TL;DR: 该论文提出了一种新的视角，将严格单调概率问题视为可观测收益变量与潜在成本变量之间的偏序关系，并引入了生成成本模型（GCM）和隐式生成成本模型（IGCM）来解决单调性问题。


<details>
  <summary>Details</summary>
Motivation: 在许多机器学习任务中，输入与输出变量之间的关系需要保持单调性。传统方法主要依赖构造或正则化技术，但本文提出了一种新的视角，将严格单调概率问题视为可观测收益变量与潜在成本变量之间的偏序关系。

Method: 论文引入了生成成本模型（GCM）来建模潜在成本变量，从而解决严格单调问题，并提出了隐式生成成本模型（IGCM）来处理隐式单调问题。

Result: 通过分位数回归的数值模拟和在公开数据集上的多次实验，验证了该方法显著优于现有的单调建模技术。

Conclusion: 本文提出的GCM和IGCM方法在解决单调性问题方面表现出色，为单调性建模提供了新的思路和工具。

Abstract: In many machine learning tasks, it is often necessary for the relationship
between input and output variables to be monotonic, including both strictly
monotonic and implicitly monotonic relationships. Traditional methods for
maintaining monotonicity mainly rely on construction or regularization
techniques, whereas this paper shows that the issue of strict monotonic
probability can be viewed as a partial order between an observable revenue
variable and a latent cost variable. This perspective enables us to reformulate
the monotonicity challenge into modeling the latent cost variable. To tackle
this, we introduce a generative network for the latent cost variable, termed
the Generative Cost Model (GCM), which inherently addresses the strict
monotonic problem, and propose the Implicit Generative Cost Model (IGCM) to
address the implicit monotonic problem. We further validate our approach with a
numerical simulation of quantile regression and conduct multiple experiments on
public datasets, showing that our method significantly outperforms existing
monotonic modeling techniques. The code for our experiments can be found at
https://github.com/tyxaaron/GCM.

</details>


### [171] [Optimizing FPGA and Wafer Test Coverage with Spatial Sampling and Machine Learning](https://arxiv.org/abs/2506.03556)
*Wang WeiQuan,Riaz-ul-Haque Mian*

Main category: cs.LG

TL;DR: 该研究提出了一种基于短距离消除(SDE)算法的混合采样策略，用于优化半导体制造中的晶圆和FPGA测试，显著提高了预测精度。


<details>
  <summary>Details</summary>
Motivation: 半导体制造中的测试成本居高不下，尤其是在晶圆和FPGA测试阶段。为了在减少测试数量的同时保持预测准确性，本研究探索了多种采样策略。

Method: 研究比较了随机采样、分层采样和k-means聚类采样三种基线方法，并提出了两种混合策略：分层短距离消除(S-SDE)和k-means短距离消除(K-SDE)。通过高斯过程回归(GPR)框架评估性能，核心是SDE算法，用于排除空间邻近的候选点。

Result: 实验表明，SDE策略显著提升了预测精度：K-SDE比k-means采样提高了16.26%(晶圆)和13.07%(FPGA)，S-SDE比分層采样提高了16.49%(晶圆)和8.84%(FPGA)。最优参数组合为(alpha, beta)=(2, 2)。

Conclusion: 提出的SDE-based混合采样策略能有效提高半导体测试的预测准确性，同时降低测试成本，为工业应用提供了实用解决方案。

Abstract: In semiconductor manufacturing, testing costs remain significantly high,
especially during wafer and FPGA testing. To reduce the number of required
tests while maintaining predictive accuracy, this study investigates three
baseline sampling strategies: Random Sampling, Stratified Sampling, and k-means
Clustering Sampling. To further enhance these methods, this study proposes a
novel algorithm that improves the sampling quality of each approach. This
research is conducted using real industrial production data from wafer-level
tests and silicon measurements from various FPGAs. This study introduces two
hybrid strategies: Stratified with Short Distance Elimination (S-SDE) and
k-means with Short Distance Elimination (K-SDE). Their performance is evaluated
within the framework of Gaussian Process Regression (GPR) for predicting wafer
and FPGA test data. At the core of our proposed approach is the Short Distance
Elimination (SDE) algorithm, which excludes spatially proximate candidate
points during sampling, thereby ensuring a more uniform distribution of
training data across the physical domain. A parameter sweep was conducted over
the (alpha, beta) thresholds, where alpha and beta are in the range {0, 1, 2,
3, 4} and not both zero, to identify the optimal combination that minimizes
RMSD. Experimental results on a randomly selected wafer file reveal that
(alpha, beta) equal (2, 2) yields the lowest RMSD. Accordingly, all subsequent
experiments adopt this parameter configuration. The results demonstrate that
the proposed SDE-based strategies enhance predictive accuracy: K-SDE improves
upon k-means sampling by 16.26 percent (wafer) and 13.07 percent (FPGA), while
S-SDE improves upon stratified sampling by 16.49 percent (wafer) and 8.84
percent (FPGA).

</details>


### [172] [A Class Inference Scheme With Dempster-Shafer Theory for Learning Fuzzy-Classifier Systems](https://arxiv.org/abs/2506.03588)
*Hiroki Shiraishi,Hisao Ishibuchi,Masaya Nakata*

Main category: cs.LG

TL;DR: 本文提出了一种基于Dempster-Shafer证据理论的新型类别推理方案，用于改进学习模糊分类系统（LFCSs）的决策机制，提高了模型的透明度、鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的LFCSs多采用基于投票或单一胜者的推理方案，这些方案依赖于训练数据的分类性能，可能在未见数据上表现不佳，存在过拟合风险。因此，需要一种能更好处理不确定性的推理方案。

Method: 文章提出了一种基于Dempster-Shafer证据理论的类别推理方案，通过计算每个模糊规则对特定类别和“我不知道”状态的置信度，从而推断出类别。

Result: 在30个真实数据集上的实验表明，该方案在测试宏F1分数上显著优于传统的基于投票和单一胜者的模糊推理方案，形成了更平滑的决策边界，并提供了可靠的置信度测量。

Conclusion: 所提出的方案显著提高了LFCSs的鲁棒性和泛化能力，适用于实际应用，并通过考虑“我不知道”状态增强了模型的透明度和可靠性。

Abstract: The decision-making process significantly influences the predictions of
machine learning models. This is especially important in rule-based systems
such as Learning Fuzzy-Classifier Systems (LFCSs) where the selection and
application of rules directly determine prediction accuracy and reliability.
LFCSs combine evolutionary algorithms with supervised learning to optimize
fuzzy classification rules, offering enhanced interpretability and robustness.
Despite these advantages, research on improving decision-making mechanisms
(i.e., class inference schemes) in LFCSs remains limited. Most LFCSs use
voting-based or single-winner-based inference schemes. These schemes rely on
classification performance on training data and may not perform well on unseen
data, risking overfitting. To address these limitations, this article
introduces a novel class inference scheme for LFCSs based on the
Dempster-Shafer Theory of Evidence (DS theory). The proposed scheme handles
uncertainty well. By using the DS theory, the scheme calculates belief masses
(i.e., measures of belief) for each specific class and the ``I don't know''
state from each fuzzy rule and infers a class from these belief masses. Unlike
the conventional schemes, the proposed scheme also considers the ``I don't
know'' state that reflects uncertainty, thereby improving the transparency and
reliability of LFCSs. Applied to a variant of LFCS (i.e., Fuzzy-UCS), the
proposed scheme demonstrates statistically significant improvements in terms of
test macro F1 scores across 30 real-world datasets compared to conventional
voting-based and single-winner-based fuzzy inference schemes. It forms smoother
decision boundaries, provides reliable confidence measures, and enhances the
robustness and generalizability of LFCSs in real-world applications. Our
implementation is available at https://github.com/YNU-NakataLab/jUCS.

</details>


### [173] [VCDiag: Classifying Erroneous Waveforms for Failure Triage Acceleration](https://arxiv.org/abs/2506.03590)
*Minh Luu,Surya Jasper,Khoi Le,Evan Pan,Michael Quinn,Aakash Tyagi,Jiang Hu*

Main category: cs.LG

TL;DR: VCDiag提出了一种基于VCD数据的高效方法，用于分类故障波形并定位故障模块，准确率超过94%，数据压缩率达120倍以上。


<details>
  <summary>Details</summary>
Motivation: 设计功能验证中的故障分类耗时且依赖人工，机器学习在此领域的应用有限，特别是在大型设计的RTL级仿真故障分类中。

Method: VCDiag采用新颖的信号选择和统计压缩方法，大幅减少原始数据量，同时保留分类所需的关键特征。

Result: 在最大规模的实验中，VCDiag在识别前三个最可能故障模块的准确率超过94%，数据压缩率超过120倍。

Conclusion: VCDiag提供了一种高效、适应性强的解决方案，可集成到多种Verilog/SystemVerilog设计和测试平台中，显著提升故障分类效率。

Abstract: Failure triage in design functional verification is critical but
time-intensive, relying on manual specification reviews, log inspections, and
waveform analyses. While machine learning (ML) has improved areas like stimulus
generation and coverage closure, its application to RTL-level simulation
failure triage, particularly for large designs, remains limited. VCDiag offers
an efficient, adaptable approach using VCD data to classify failing waveforms
and pinpoint likely failure locations. In the largest experiment, VCDiag
achieves over 94% accuracy in identifying the top three most likely modules.
The framework introduces a novel signal selection and statistical compression
approach, achieving over 120x reduction in raw data size while preserving
features essential for classification. It can also be integrated into diverse
Verilog/SystemVerilog designs and testbenches.

</details>


### [174] [Purifying Shampoo: Investigating Shampoo's Heuristics by Decomposing its Preconditioner](https://arxiv.org/abs/2506.03595)
*Runa Eschenhagen,Aaron Defazio,Tsung-Hsien Lee,Richard E. Turner,Hao-Jun Michael Shi*

Main category: cs.LG

TL;DR: 本文研究了Shampoo算法中的启发式方法，提出了基于Frobenius范数近似和特征值/特征基解耦的改进方法，以减少启发式依赖并提升性能。


<details>
  <summary>Details</summary>
Motivation: Shampoo算法虽在AlgoPerf竞赛中表现优异，但其依赖多种启发式方法（如学习率嫁接和过时预条件），增加了算法复杂性和超参数调优难度，且缺乏理论支持。本文旨在解决这些问题。

Method: 通过Frobenius范数近似解耦预条件子的特征值和特征基更新，提出自适应准则确定特征基计算频率，以减少近似误差对收敛的影响。

Result: 研究表明，从Adam嫁接学习率可缓解预条件子特征值的过时和缩放问题，直接修正特征值可避免学习率嫁接。自适应准则能有效管理特征基计算频率。

Conclusion: 本文提出的方法为去除Shampoo的启发式依赖提供了理论依据，推动了基于Kronecker分解的训练算法改进。

Abstract: The recent success of Shampoo in the AlgoPerf contest has sparked renewed
interest in Kronecker-factorization-based optimization algorithms for training
neural networks. Despite its success, Shampoo relies heavily on several
heuristics such as learning rate grafting and stale preconditioning to achieve
performance at-scale. These heuristics increase algorithmic complexity,
necessitate further hyperparameter tuning, and lack theoretical justification.
This paper investigates these heuristics from the angle of Frobenius norm
approximation to full-matrix Adam and decouples the preconditioner's
eigenvalues and eigenbasis updates. We show that grafting from Adam mitigates
the staleness and mis-scaling of the preconditioner's eigenvalues and how
correcting the eigenvalues directly can eliminate the need for learning rate
grafting. To manage the error induced by infrequent eigenbasis computations, we
propose an adaptive criterion for determining the eigenbasis computation
frequency motivated by terminating a warm-started QR algorithm. This criterion
decouples the update frequency of different preconditioner matrices and enables
us to investigate the impact of approximation error on convergence. These
practical techniques offer a principled angle towards removing Shampoo's
heuristics and developing improved Kronecker-factorization-based training
algorithms.

</details>


### [175] [Adapting Rule Representation With Four-Parameter Beta Distribution for Learning Classifier Systems](https://arxiv.org/abs/2506.03602)
*Hiroki Shiraishi,Yohei Hayamizu,Tomonori Hashiyama,Keiki Takadama,Hisao Ishibuchi,Masaya Nakata*

Main category: cs.LG

TL;DR: 该论文提出了一种基于四参数Beta分布的灵活规则表示方法，并将其集成到模糊型学习分类器系统（LCS）中，以自动选择不同子空间的最优规则表示，从而提升分类精度和规则集简洁性。


<details>
  <summary>Details</summary>
Motivation: 规则表示对学习分类器系统（LCS）的搜索能力和决策边界有重要影响，但为每个问题选择合适的规则表示非常困难。此外，某些问题需要针对输入空间的不同子空间使用不同的表示方法，因此需要一种自适应机制来为每个规则选择最合适的表示。

Method: 论文引入了一种基于四参数Beta分布的灵活规则表示方法，并将其集成到模糊型LCS中。通过控制四个参数，该方法可以表示各种形状的决策边界（如矩形和钟形），并自动为不同子空间选择最优表示。系统还倾向于在可行时使用清晰规则，以增强模型的可解释性。

Result: 在真实世界分类任务上的实验结果表明，该方法在测试准确率上显著优于其他方法，并且生成的规则集更加紧凑。

Conclusion: 通过四参数Beta分布的灵活表示，该方法能够自动适应不同子空间的需求，显著提升了LCS的性能和可解释性。

Abstract: Rule representations significantly influence the search capabilities and
decision boundaries within the search space of Learning Classifier Systems
(LCSs), a family of rule-based machine learning systems that evolve
interpretable models through evolutionary processes. However, it is very
difficult to choose an appropriate rule representation for each problem.
Additionally, some problems benefit from using different representations for
different subspaces within the input space. Thus, an adaptive mechanism is
needed to choose an appropriate rule representation for each rule in LCSs. This
article introduces a flexible rule representation using a four-parameter beta
distribution and integrates it into a fuzzy-style LCS. The four-parameter beta
distribution can form various function shapes, and this flexibility enables our
LCS to automatically select appropriate representations for different
subspaces. Our rule representation can represent crisp/fuzzy decision
boundaries in various boundary shapes, such as rectangles and bells, by
controlling four parameters, compared to the standard representations such as
trapezoidal ones. Leveraging this flexibility, our LCS is designed to adapt the
appropriate rule representation for each subspace. Moreover, our LCS
incorporates a generalization bias favoring crisp rules where feasible,
enhancing model interpretability without compromising accuracy. Experimental
results on real-world classification tasks show that our LCS achieves
significantly superior test accuracy and produces more compact rule sets. Our
implementation is available at https://github.com/YNU-NakataLab/Beta4-UCS. An
extended abstract related to this work is available at
https://doi.org/10.36227/techrxiv.174900805.59801248/v1.

</details>


### [176] [GCFL: A Gradient Correction-based Federated Learning Framework for Privacy-preserving CPSS](https://arxiv.org/abs/2506.03618)
*Jiayi Wan,Xiang Zhu,Fanzhen Liu,Wei Fan,Xiaolong Xu*

Main category: cs.LG

TL;DR: 本文提出了一种结合差分隐私和联邦学习的新框架，通过服务器端梯度校正机制平衡隐私保护与模型精度。


<details>
  <summary>Details</summary>
Motivation: 现有研究在联邦学习中引入差分隐私时，主要通过动态调整噪声或丢弃部分梯度来减少噪声影响，但未能有效消除阻碍收敛的噪声并校正受影响的梯度，导致模型分类精度显著下降。

Method: 该框架在客户端进行梯度裁剪和噪声扰动后，通过服务器端检测噪声梯度偏差，并采用投影机制校正梯度，减少噪声负面影响，同时促进不同客户端梯度的对齐，引导模型向全局最优收敛。

Result: 在多个基准数据集上的实验结果表明，该框架在相同隐私预算下实现了最先进的性能。

Conclusion: 所提出的框架有效平衡了严格的隐私保护与模型精度，通过梯度校正机制显著提升了联邦学习在CPSS中的应用效果。

Abstract: Federated learning, as a distributed architecture, shows great promise for
applications in Cyber-Physical-Social Systems (CPSS). In order to mitigate the
privacy risks inherent in CPSS, the integration of differential privacy with
federated learning has attracted considerable attention. Existing research
mainly focuses on dynamically adjusting the noise added or discarding certain
gradients to mitigate the noise introduced by differential privacy. However,
these approaches fail to remove the noise that hinders convergence and correct
the gradients affected by the noise, which significantly reduces the accuracy
of model classification. To overcome these challenges, this paper proposes a
novel framework for differentially private federated learning that balances
rigorous privacy guarantees with accuracy by introducing a server-side gradient
correction mechanism. Specifically, after clients perform gradient clipping and
noise perturbation, our framework detects deviations in the noisy local
gradients and employs a projection mechanism to correct them, mitigating the
negative impact of noise. Simultaneously, gradient projection promotes the
alignment of gradients from different clients and guides the model towards
convergence to a global optimum. We evaluate our framework on several benchmark
datasets, and the experimental results demonstrate that it achieves
state-of-the-art performance under the same privacy budget.

</details>


### [177] [Out-of-Distribution Graph Models Merging](https://arxiv.org/abs/2506.03674)
*Yidi Wang,Jiawei Gu,pei Xiaobing,Xubin Zheng,Xiao Luo,Pengyang Wang,Ziyue Qiao*

Main category: cs.LG

TL;DR: 该论文提出了一种解决分布外图模型合并问题的新方法，通过图生成策略和混合专家模块实现多领域预训练模型的泛化适应。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决多个预训练图模型在不同分布域上的合并问题，以构建一个泛化能力更强的模型。

Method: 方法包括图生成策略实例化多领域混合分布，以及通过混合专家模块和掩码机制合并和微调预训练模型。

Result: 理论分析和实验结果均表明，该方法在解决模型泛化问题上具有有效性。

Conclusion: 该框架不依赖特定架构且无需源/目标域数据，成功实现了多领域图模型的泛化合并。

Abstract: This paper studies a novel problem of out-of-distribution graph models
merging, which aims to construct a generalized model from multiple graph models
pre-trained on different domains with distribution discrepancy. This problem is
challenging because of the difficulty in learning domain-invariant knowledge
implicitly in model parameters and consolidating expertise from potentially
heterogeneous GNN backbones. In this work, we propose a graph generation
strategy that instantiates the mixture distribution of multiple domains. Then,
we merge and fine-tune the pre-trained graph models via a MoE module and a
masking mechanism for generalized adaptation. Our framework is
architecture-agnostic and can operate without any source/target domain data.
Both theoretical analysis and experimental results demonstrate the
effectiveness of our approach in addressing the model generalization problem.

</details>


### [178] [Comprehensive Attribute Encoding and Dynamic LSTM HyperModels for Outcome Oriented Predictive Business Process Monitoring](https://arxiv.org/abs/2506.03696)
*Fang Wang,Paolo Ceravolo,Ernesto Damiani*

Main category: cs.LG

TL;DR: 该论文提出了一套动态LSTM超模型，用于解决预测性业务流程监控中的灵活性不足问题，通过分层编码和伪嵌入技术提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的预测性业务流程监控方法在处理同时事件、类别不平衡和多级属性等现实挑战时缺乏灵活性，且难以适应异构数据集。

Method: 提出动态LSTM超模型，结合事件和序列属性的两级分层编码、事件标签的字符级分解，以及持续时间和属性相关性的伪嵌入技术。

Result: 在四个公开和真实数据集上的实验验证显示，平衡数据集准确率达100%，不平衡数据集的F1分数超过86%。

Conclusion: 该方法通过模块化和可解释的模型提升了预测性业务流程监控的适用性，并为更广泛的AI社区贡献了时间序列预测和数据异质性处理的改进方案。

Abstract: Predictive Business Process Monitoring (PBPM) aims to forecast future
outcomes of ongoing business processes. However, existing methods often lack
flexibility to handle real-world challenges such as simultaneous events, class
imbalance, and multi-level attributes. While prior work has explored static
encoding schemes and fixed LSTM architectures, they struggle to support
adaptive representations and generalize across heterogeneous datasets. To
address these limitations, we propose a suite of dynamic LSTM HyperModels that
integrate two-level hierarchical encoding for event and sequence attributes,
character-based decomposition of event labels, and novel pseudo-embedding
techniques for durations and attribute correlations. We further introduce
specialized LSTM variants for simultaneous event modeling, leveraging
multidimensional embeddings and time-difference flag augmentation. Experimental
validation on four public and real-world datasets demonstrates up to 100%
accuracy on balanced datasets and F1 scores exceeding 86\% on imbalanced ones.
Our approach advances PBPM by offering modular and interpretable models better
suited for deployment in complex settings. Beyond PBPM, it contributes to the
broader AI community by improving temporal outcome prediction, supporting data
heterogeneity, and promoting explainable process intelligence frameworks.

</details>


### [179] [Learning-at-Criticality in Large Language Models for Quantum Field Theory and Beyond](https://arxiv.org/abs/2506.03703)
*Xiansheng Cai,Sihan Hu,Tao Wang,Yuan Huang,Pan Zhang,Youjin Deng,Kun Chen*

Main category: cs.LG

TL;DR: 该论文提出了一种名为‘临界学习’(LaC)的强化学习方案，通过调整大型语言模型(LLM)至临界点，使其在数据稀缺的情况下实现最佳泛化能力，并应用于基础物理中的复杂符号问题。


<details>
  <summary>Details</summary>
Motivation: 基础物理常面临信息稀缺的复杂符号问题，传统AI需要大量数据学习，难以应对。研究旨在探索如何在数据稀缺条件下提升AI的推理能力。

Method: 采用‘临界学习’(LaC)方案，通过强化学习调整LLM至临界点，利用最小化数据实现泛化。通过概念网络模型(CoNet)分析临界点的学习行为。

Result: 在临界点，LLM表现出类似二阶相变的特性，如幂律分布的解决路径长度，并显著提升了在量子场论等复杂问题中的表现，优于更大规模的模型。

Conclusion: LaC通过利用临界现象这一物理原理，使AI能够在数据稀缺的复杂物理问题中实现高效推理，为信息稀缺领域提供了新的解决方案。

Abstract: Fundamental physics often confronts complex symbolic problems with few
guiding exemplars or established principles. While artificial intelligence (AI)
offers promise, its typical need for vast datasets to learn from hinders its
use in these information-scarce frontiers. We introduce learning at criticality
(LaC), a reinforcement learning (RL) scheme that tunes Large Language Models
(LLMs) to a sharp learning transition, addressing this information scarcity. At
this transition, LLMs achieve peak generalization from minimal data,
exemplified by 7-digit base-7 addition -- a test of nontrivial arithmetic
reasoning. To elucidate this peak, we analyze a minimal concept-network model
(CoNet) designed to capture the essence of how LLMs might link tokens. Trained
on a single exemplar, this model also undergoes a sharp learning transition.
This transition exhibits hallmarks of a second-order phase transition, notably
power-law distributed solution path lengths. At this critical point, the system
maximizes a ``critical thinking pattern" crucial for generalization, enabled by
the underlying scale-free exploration. This suggests LLMs reach peak
performance by operating at criticality, where such explorative dynamics enable
the extraction of underlying operational rules. We demonstrate LaC in quantum
field theory: an 8B-parameter LLM, tuned to its critical point by LaC using a
few exemplars of symbolic Matsubara sums, solves unseen, higher-order problems,
significantly outperforming far larger models. LaC thus leverages critical
phenomena, a physical principle, to empower AI for complex, data-sparse
challenges in fundamental physics.

</details>


### [180] [On the Closed-Form of Flow Matching: Generalization Does Not Arise from Target Stochasticity](https://arxiv.org/abs/2506.03719)
*Quentin Bertrand,Anne Gagneux,Mathurin Massias,Rémi Emonet*

Main category: cs.LG

TL;DR: 本文探讨了流匹配模型中损失函数的随机性对泛化能力的影响，发现封闭形式损失函数性能更优。


<details>
  <summary>Details</summary>
Motivation: 研究现代深度生成模型（如扩散模型和流匹配技术）为何能高效泛化，特别是验证损失函数的随机性是否为主要因素。

Method: 通过实验比较高维设置下随机和封闭形式流匹配损失的差异，并在标准图像数据集上测试两种变体的统计性能。

Result: 两种损失函数在高维情况下表现几乎相同，但封闭形式损失函数在某些情况下性能更优。

Conclusion: 流匹配模型的泛化能力主要不依赖于损失函数的随机性，封闭形式损失函数可能提供更好的性能。

Abstract: Modern deep generative models can now produce high-quality synthetic samples
that are often indistinguishable from real training data. A growing body of
research aims to understand why recent methods -- such as diffusion and flow
matching techniques -- generalize so effectively. Among the proposed
explanations are the inductive biases of deep learning architectures and the
stochastic nature of the conditional flow matching loss. In this work, we rule
out the latter -- the noisy nature of the loss -- as a primary contributor to
generalization in flow matching. First, we empirically show that in
high-dimensional settings, the stochastic and closed-form versions of the flow
matching loss yield nearly equivalent losses. Then, using state-of-the-art flow
matching models on standard image datasets, we demonstrate that both variants
achieve comparable statistical performance, with the surprising observation
that using the closed-form can even improve performance.

</details>


### [181] [Sign-SGD is the Golden Gate between Multi-Node to Single-Node Learning: Significant Boost via Parameter-Free Optimization](https://arxiv.org/abs/2506.03725)
*Daniil Medyakov,Sergey Stanko,Gleb Molodtsov,Philip Zmushko,Grigoriy Evseev,Egor Petrov,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: 本文针对大语言模型训练资源消耗大的问题，提出改进Sign-SGD方法以自动确定有效步长，并在单节点和多节点场景中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练资源密集，Sign-SGD虽能提升效率，但无法自动确定有效步长，限制了其实际应用。

Method: 设计了多种单节点确定性Sign-SGD变体，并扩展到随机单节点、多节点学习及动量方法。

Result: 在真实机器学习问题上进行了广泛实验，验证了所提方法的实际适用性。

Conclusion: 改进的Sign-SGD方法能有效解决步长确定问题，提升训练效率，具有实际应用价值。

Abstract: Quite recently, large language models have made a significant breakthrough
across various disciplines. However, training them is an extremely
resource-intensive task, even for major players with vast computing resources.
One of the methods gaining popularity in light of these challenges is Sign-SGD.
This method can be applied both as a memory-efficient approach in single-node
training and as a gradient compression technique in the distributed learning.
Nevertheless, it is impossible to automatically determine the effective
stepsize from the theoretical standpoint. Indeed, it depends on the parameters
of the dataset to which we do not have access in the real-world learning
paradigm. To address this issue, we design several variants of single-node
deterministic Sign-SGD. We extend our approaches to practical scenarios:
stochastic single-node and multi-node learning, methods with incorporated
momentum. We conduct extensive experiments on real machine learning problems
that emphasize the practical applicability of our ideas.

</details>


### [182] [PPO in the Fisher-Rao geometry](https://arxiv.org/abs/2506.03757)
*Razvan-Andrei Lascu,David Šiška,Łukasz Szpruch*

Main category: cs.LG

TL;DR: 本文提出了一种基于Fisher-Rao几何的PPO变体FR-PPO，提供了更强的理论保证，包括单调策略改进，并在表格设置中展示了其收敛性。


<details>
  <summary>Details</summary>
Motivation: 尽管PPO在强化学习中广泛使用，但其缺乏策略改进和收敛的正式理论保证。本文旨在通过引入Fisher-Rao几何来改进PPO的理论基础。

Method: 通过在线性化值函数的基础上，在Fisher-Rao几何中推导出一个更紧的替代损失函数，从而提出FR-PPO算法。

Result: FR-PPO提供了单调策略改进的理论保证，并在表格设置中实现了与动作或状态空间维度无关的次线性收敛。

Conclusion: FR-PPO为PPO类算法建立了正式的收敛结果，是理论上的重要进展。

Abstract: Proximal Policy Optimization (PPO) has become a widely adopted algorithm for
reinforcement learning, offering a practical policy gradient method with strong
empirical performance. Despite its popularity, PPO lacks formal theoretical
guarantees for policy improvement and convergence. PPO is motivated by Trust
Region Policy Optimization (TRPO) that utilizes a surrogate loss with a KL
divergence penalty, which arises from linearizing the value function within a
flat geometric space. In this paper, we derive a tighter surrogate in the
Fisher-Rao (FR) geometry, yielding a novel variant, Fisher-Rao PPO (FR-PPO).
Our proposed scheme provides strong theoretical guarantees, including monotonic
policy improvement. Furthermore, in the tabular setting, we demonstrate that
FR-PPO achieves sub-linear convergence without any dependence on the
dimensionality of the action or state spaces, marking a significant step toward
establishing formal convergence results for PPO-based algorithms.

</details>


### [183] [Scaling CrossQ with Weight Normalization](https://arxiv.org/abs/2506.03758)
*Daniel Palenicek,Florian Vogt,Jan Peters*

Main category: cs.LG

TL;DR: 论文探讨了CrossQ在高更新数据比(UTD)下的扩展行为，提出了权重归一化方法以解决训练中的Q偏差爆炸和权重幅度增长问题，显著提升了样本效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 强化学习在实际应用中面临样本效率低下的瓶颈，尤其是在高UTD比下，训练动态中的Q偏差爆炸和权重幅度增长问题更加突出。

Method: 通过将权重归一化整合到CrossQ框架中，稳定训练过程，防止塑性损失，并保持有效学习率恒定。

Result: 该方法在高UTD比下可靠扩展，在DeepMind控制基准测试中表现优异，特别是在复杂的狗和人形机器人环境中。

Conclusion: 该研究为无模型强化学习提供了一条提高样本效率和可扩展性的稳健途径，无需网络重置等激进干预。

Abstract: Reinforcement learning has achieved significant milestones, but sample
efficiency remains a bottleneck for real-world applications. Recently, CrossQ
has demonstrated state-of-the-art sample efficiency with a low update-to-data
(UTD) ratio of 1. In this work, we explore CrossQ's scaling behavior with
higher UTD ratios. We identify challenges in the training dynamics which are
emphasized by higher UTDs, particularly Q-bias explosion and the growing
magnitude of critic network weights. To address this, we integrate weight
normalization into the CrossQ framework, a solution that stabilizes training,
prevents potential loss of plasticity and keeps the effective learning rate
constant. Our proposed approach reliably scales with increasing UTD ratios,
achieving competitive or superior performance across a range of challenging
tasks on the DeepMind control benchmark, notably the complex dog and humanoid
environments. This work eliminates the need for drastic interventions, such as
network resets, and offers a robust pathway for improving sample efficiency and
scalability in model-free reinforcement learning.

</details>


### [184] [FedFACT: A Provable Framework for Controllable Group-Fairness Calibration in Federated Learning](https://arxiv.org/abs/2506.03777)
*Li Zhang,Zhongxuan Han,Chaochao chen,Xiaohua Feng,Jiaming Zhang,Yuyuan Li*

Main category: cs.LG

TL;DR: 该论文提出了一种名为FedFACT的新框架，旨在解决联邦学习中的全局和局部公平性问题，并通过贝叶斯最优分类器实现准确性与公平性的可控平衡。


<details>
  <summary>Details</summary>
Motivation: 随着联邦学习在决策场景中的应用日益广泛，确保模型公平性以防止敏感群体（如性别）间的差异变得至关重要。当前研究主要关注全局公平性和局部公平性，但公平性标准的不可分解性和不可微性带来了两大挑战：多类分类中全局与局部公平性的协调，以及准确性与公平性的可控最优权衡。

Method: 论文提出了FedFACT框架，通过识别多类情况下全局和局部公平性约束下的贝叶斯最优分类器，将公平联邦学习重新表述为个性化成本敏感学习问题（处理中）和双层优化问题（后处理），以实现可调节的最优准确性与公平性平衡。

Result: 理论分析表明，FedFACT在给定公平性水平下能够接近最优准确性，并提供了收敛性和泛化性保证。多数据集实验显示，FedFACT在平衡准确性和全局-局部公平性方面 consistently优于基线方法。

Conclusion: FedFACT框架有效解决了联邦学习中的公平性问题，通过理论保证和实验验证，展示了其在多类分类和异构数据场景下的优越性能。

Abstract: With emerging application of Federated Learning (FL) in decision-making
scenarios, it is imperative to regulate model fairness to prevent disparities
across sensitive groups (e.g., female, male). Current research predominantly
focuses on two concepts of group fairness within FL: Global Fairness (overall
model disparity across all clients) and Local Fairness (the disparity within
each client). However, the non-decomposable, non-differentiable nature of
fairness criteria pose two fundamental, unresolved challenges for fair FL: (i)
Harmonizing global and local fairness in multi-class classification; (ii)
Enabling a controllable, optimal accuracy-fairness trade-off. To tackle the
aforementioned challenges, we propose a novel controllable federated
group-fairness calibration framework, named FedFACT. FedFACT identifies the
Bayes-optimal classifiers under both global and local fairness constraints in
multi-class case, yielding models with minimal performance decline while
guaranteeing fairness. To effectively realize an adjustable, optimal
accuracy-fairness balance, we derive specific characterizations of the
Bayes-optimal fair classifiers for reformulating fair FL as personalized
cost-sensitive learning problem for in-processing, and bi-level optimization
for post-processing. Theoretically, we provide convergence and generalization
guarantees for FedFACT to approach the near-optimal accuracy under given
fairness levels. Extensive experiments on multiple datasets across various data
heterogeneity demonstrate that FedFACT consistently outperforms baselines in
balancing accuracy and global-local fairness.

</details>


### [185] [When Does Closeness in Distribution Imply Representational Similarity? An Identifiability Perspective](https://arxiv.org/abs/2506.03784)
*Beatrix M. G. Nielsen,Emanuele Marconato,Andrea Dittadi,Luigi Gresele*

Main category: cs.LG

TL;DR: 该论文研究了不同深度神经网络学到的表示何时及为何相似，通过可识别性理论视角，发现模型分布接近并不保证表示相似，并提出了一种新的分布距离度量。


<details>
  <summary>Details</summary>
Motivation: 研究不同深度神经网络学到的表示相似性的条件和原因，旨在理解模型分布与表示相似性之间的关系。

Method: 采用可识别性理论视角，分析模型分布的KL散度与表示相似性的关系，并提出一种新的分布距离度量方法。

Result: 证明KL散度小不保证表示相似，提出新的分布距离度量，并在合成实验中验证宽网络的分布更接近且表示更相似。

Conclusion: 论文建立了模型分布接近与表示相似性之间的联系，为理解神经网络表示相似性提供了新视角。

Abstract: When and why representations learned by different deep neural networks are
similar is an active research topic. We choose to address these questions from
the perspective of identifiability theory, which suggests that a measure of
representational similarity should be invariant to transformations that leave
the model distribution unchanged. Focusing on a model family which includes
several popular pre-training approaches, e.g., autoregressive language models,
we explore when models which generate distributions that are close have similar
representations. We prove that a small Kullback-Leibler divergence between the
model distributions does not guarantee that the corresponding representations
are similar. This has the important corollary that models arbitrarily close to
maximizing the likelihood can still learn dissimilar representations, a
phenomenon mirrored in our empirical observations on models trained on
CIFAR-10. We then define a distributional distance for which closeness implies
representational similarity, and in synthetic experiments, we find that wider
networks learn distributions which are closer with respect to our distance and
have more similar representations. Our results establish a link between
closeness in distribution and representational similarity.

</details>


### [186] [Attention-Only Transformers via Unrolled Subspace Denoising](https://arxiv.org/abs/2506.03790)
*Peng Wang,Yifu Lu,Yaodong Yu,Druv Pai,Qing Qu,Yi Ma*

Main category: cs.LG

TL;DR: 该论文提出了一种高度简洁且可解释的Transformer架构，仅包含自注意力操作和跳跃连接，通过迭代去噪实现高效表征学习，性能接近标准Transformer。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer架构存在冗余且缺乏数学解释，论文旨在设计一个仅包含必要组件的完全可解释架构。

Method: 通过将初始噪声表征压缩到低维子空间的混合，采用多头自注意力进行迭代去噪，构建仅含自注意力和跳跃连接的深度网络。

Result: 实验表明，该架构在视觉和语言任务上性能接近GPT-2和CRATE等标准Transformer，且每层线性提升信噪比。

Conclusion: 论文证明了简洁且数学可解释的Transformer架构的可行性，为模型设计提供了新方向。

Abstract: Despite the popularity of transformers in practice, their architectures are
empirically designed and neither mathematically justified nor interpretable.
Moreover, as indicated by many empirical studies, some components of
transformer architectures may be redundant. To derive a fully interpretable
transformer architecture with only necessary components, we contend that the
goal of representation learning is to compress a set of noisy initial token
representations towards a mixture of low-dimensional subspaces. To compress
these noisy token representations, an associated denoising operation naturally
takes the form of a multi-head (subspace) self-attention. By unrolling such
iterative denoising operations into a deep network, we arrive at a highly
compact architecture that consists of \textit{only} self-attention operators
with skip connections at each layer. Moreover, we show that each layer performs
highly efficient denoising: it improves the signal-to-noise ratio of token
representations \textit{at a linear rate} with respect to the number of layers.
Despite its simplicity, extensive experiments on vision and language tasks
demonstrate that such a transformer achieves performance close to that of
standard transformer architectures such as GPT-2 and CRATE.

</details>


### [187] [Learning Equilibria in Matching Games with Bandit Feedback](https://arxiv.org/abs/2506.03802)
*Andreas Athanasopoulos,Christos Dimitrakakis*

Main category: cs.LG

TL;DR: 该研究探讨了在双边匹配市场中学习均衡的问题，提出了一种基于UCB算法的学习方法，以降低匹配不稳定性并实现次线性遗憾。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决双边匹配市场中代理人在未知收益矩阵下如何通过学习达到均衡的问题，特别是在匹配后代理人可以自适应调整策略的情境下。

Method: 采用匹配均衡作为解决方案概念，提出了一种UCB算法，代理人基于对游戏收益的乐观估计形成偏好并选择行动。

Result: 证明了该算法能够在时间范围T内实现次线性、独立于实例的遗憾，有效降低了匹配不稳定性。

Conclusion: 研究表明，通过集中式学习程序，可以在双边匹配市场中有效学习均衡策略，为相关市场设计提供了理论支持。

Abstract: We investigate the problem of learning an equilibrium in a generalized
two-sided matching market, where agents can adaptively choose their actions
based on their assigned matches. Specifically, we consider a setting in which
matched agents engage in a zero-sum game with initially unknown payoff
matrices, and we explore whether a centralized procedure can learn an
equilibrium from bandit feedback. We adopt the solution concept of matching
equilibrium, where a pair consisting of a matching $\mathfrak{m}$ and a set of
agent strategies $X$ forms an equilibrium if no agent has the incentive to
deviate from $(\mathfrak{m}, X)$. To measure the deviation of a given pair
$(\mathfrak{m}, X)$ from the equilibrium pair $(\mathfrak{m}^\star, X^\star)$,
we introduce matching instability that can serve as a regret measure for the
corresponding learning problem. We then propose a UCB algorithm in which agents
form preferences and select actions based on optimistic estimates of the game
payoffs, and prove that it achieves sublinear, instance-independent regret over
a time horizon $T$.

</details>


### [188] [Graph Neural Networks for Resource Allocation in Multi-Channel Wireless Networks](https://arxiv.org/abs/2506.03813)
*Lili Chen,Changyang She,Jingge Zhu,Jamie Evans*

Main category: cs.LG

TL;DR: 本文提出了一种基于图神经网络的联合信道和功率分配方法JCPGNN-M，以解决无线网络中的干扰问题，相比传统方法在数据率和计算效率上表现更优。


<details>
  <summary>Details</summary>
Motivation: 随着移动设备数量的增加，干扰成为提高无线网络数据速率的主要瓶颈，因此需要高效的联合信道和功率分配（JCPA）方案来管理干扰。

Method: 首先提出增强型WMMSE（eWMMSE）算法解决JCPA问题，随后引入基于图神经网络的JCPGNN-M方法，支持多信道分配，并通过拉格朗日框架系统化处理总功率约束。

Result: 仿真结果表明，JCPGNN-M在数据率上优于eWMMSE，且推理时间更短，能够很好地扩展到大规模网络。

Conclusion: JCPGNN-M不仅提升了无线网络的性能，还显著降低了计算复杂度，适用于密集网络场景。

Abstract: As the number of mobile devices continues to grow, interference has become a
major bottleneck in improving data rates in wireless networks. Efficient joint
channel and power allocation (JCPA) is crucial for managing interference. In
this paper, we first propose an enhanced WMMSE (eWMMSE) algorithm to solve the
JCPA problem in multi-channel wireless networks. To reduce the computational
complexity of iterative optimization, we further introduce JCPGNN-M, a graph
neural network-based solution that enables simultaneous multi-channel
allocation for each user. We reformulate the problem as a Lagrangian function,
which allows us to enforce the total power constraints systematically. Our
solution involves combining this Lagrangian framework with GNNs and iteratively
updating the Lagrange multipliers and resource allocation scheme. Unlike
existing GNN-based methods that limit each user to a single channel, JCPGNN-M
supports efficient spectrum reuse and scales well in dense network scenarios.
Simulation results show that JCPGNN-M achieves better data rate compared to
eWMMSE. Meanwhile, the inference time of JCPGNN-M is much lower than eWMMS, and
it can generalize well to larger networks.

</details>


### [189] [Survey of Active Learning Hyperparameters: Insights from a Large-Scale Experimental Grid](https://arxiv.org/abs/2506.03817)
*Julius Gonsior,Tim Rieß,Anja Reusch,Claudio Hartmann,Maik Thiele,Wolfgang Lehner*

Main category: cs.LG

TL;DR: 该论文通过大规模超参数实验分析主动学习（AL）的可复现性和有效性，提出优化实验设计的建议。


<details>
  <summary>Details</summary>
Motivation: 主动学习虽能减少标注成本，但因超参数空间复杂且效果不稳定，在实际应用中较少使用。论文旨在解决AL的可复现性和信任问题。

Method: 构建包含460万种超参数组合的网格，进行大规模AL实验，分析各超参数对结果的影响。

Result: 发现超参数选择对AL效果影响显著，尤其是具体策略的实现方式，并提出最小计算成本的实验设计方案。

Conclusion: 研究为未来AL的可复现性和可信度提供了实验框架，强调超参数透明化的重要性。

Abstract: Annotating data is a time-consuming and costly task, but it is inherently
required for supervised machine learning. Active Learning (AL) is an
established method that minimizes human labeling effort by iteratively
selecting the most informative unlabeled samples for expert annotation, thereby
improving the overall classification performance. Even though AL has been known
for decades, AL is still rarely used in real-world applications. As indicated
in the two community web surveys among the NLP community about AL, two main
reasons continue to hold practitioners back from using AL: first, the
complexity of setting AL up, and second, a lack of trust in its effectiveness.
We hypothesize that both reasons share the same culprit: the large
hyperparameter space of AL. This mostly unexplored hyperparameter space often
leads to misleading and irreproducible AL experiment results. In this study, we
first compiled a large hyperparameter grid of over 4.6 million hyperparameter
combinations, second, recorded the performance of all combinations in the
so-far biggest conducted AL study, and third, analyzed the impact of each
hyperparameter in the experiment results. In the end, we give recommendations
about the influence of each hyperparameter, demonstrate the surprising
influence of the concrete AL strategy implementation, and outline an
experimental study design for reproducible AL experiments with minimal
computational effort, thus contributing to more reproducible and trustworthy AL
research in the future.

</details>


### [190] [Learning task-specific predictive models for scientific computing](https://arxiv.org/abs/2506.03835)
*Jianyuan Yin,Qianxiao Li*

Main category: cs.LG

TL;DR: 该论文提出了一种针对下游任务优化的监督学习方法，通过最大化预测误差来替代传统均方误差，并开发了迭代算法求解任务特定的监督学习问题。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习通常最小化预测的均方误差，但在需要模型评估的下游任务中（如科学计算），这种方法可能不适用。论文旨在解决这一局限性。

Method: 提出基于下游任务采样测度的任务特定监督学习问题，通过最大化预测误差来估计任务性能，并开发了离散化经验风险和迭代求解算法。

Result: 在轨迹预测、最优控制和最小能量路径计算三个数值实验中验证了该方法的有效性。

Conclusion: 该方法为下游任务提供了可靠的代理模型，解决了传统监督学习在非预测任务中的局限性。

Abstract: We consider learning a predictive model to be subsequently used for a given
downstream task (described by an algorithm) that requires access to the model
evaluation. This task need not be prediction, and this situation is frequently
encountered in machine-learning-augmented scientific computing. We show that
this setting differs from classical supervised learning, and in general it
cannot be solved by minimizing the mean square error of the model predictions
as is frequently performed in the literature. Instead, we find that the maximum
prediction error on the support of the downstream task algorithm can serve as
an effective estimate for the subsequent task performance. With this insight,
we formulate a task-specific supervised learning problem based on the given
sampling measure, whose solution serves as a reliable surrogate model for the
downstream task. Then, we discretize the empirical risk based on training data,
and develop an iterative algorithm to solve the task-specific supervised
learning problem. Three illustrative numerical examples on trajectory
prediction, optimal control and minimum energy path computation demonstrate the
effectiveness of the approach.

</details>


### [191] [Revisiting Unbiased Implicit Variational Inference](https://arxiv.org/abs/2506.03839)
*Tobias Pielok,Bernd Bischl,David Rügamer*

Main category: cs.LG

TL;DR: 本文重新审视了无偏隐式变分推断（UIVI）方法，通过重要性采样替换其内部的MCMC循环，并学习最优提案分布，展示了在SIVI基准测试上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 近年来，半隐式变分推断（SIVI）方法因其能从复杂分布中快速生成样本而受到关注。然而，高维样本的似然估计困难，当前研究集中在寻找有效的SIVI训练方法。尽管无偏隐式变分推断（UIVI）因其内部MCMC循环被认为不精确且计算成本高而被忽视，本文重新探讨了该方法。

Method: 本文提出用重要性采样替换UIVI的MCMC循环，并通过最小化预期的前向Kullback-Leibler散度来稳定学习最优提案分布，避免了偏差。

Result: 改进后的方法在已建立的SIVI基准测试上表现出优越性能或与最先进方法相当。

Conclusion: 本文证明了UIVI方法可以通过重要性采样和改进的训练策略实现高效且准确的变分推断，为复杂分布的样本生成提供了新的解决方案。

Abstract: Recent years have witnessed growing interest in semi-implicit variational
inference (SIVI) methods due to their ability to rapidly generate samples from
complex distributions. However, since the likelihood of these samples is
non-trivial to estimate in high dimensions, current research focuses on finding
effective SIVI training routines. Although unbiased implicit variational
inference (UIVI) has largely been dismissed as imprecise and computationally
prohibitive because of its inner MCMC loop, we revisit this method and show
that UIVI's MCMC loop can be effectively replaced via importance sampling and
the optimal proposal distribution can be learned stably by minimizing an
expected forward Kullback-Leibler divergence without bias. Our refined approach
demonstrates superior performance or parity with state-of-the-art methods on
established SIVI benchmarks.

</details>


### [192] [Vulnerability-Aware Alignment: Mitigating Uneven Forgetting in Harmful Fine-Tuning](https://arxiv.org/abs/2506.03850)
*Liang Chen,Xueting Han,Li Shen,Jing Bai,Kam-Fai Wong*

Main category: cs.LG

TL;DR: 论文提出VAA方法，通过识别易受攻击的数据子集并采用分组鲁棒优化，有效减少有害微调风险。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分考虑数据脆弱性模式，导致安全对齐在有害微调下易被破坏。

Method: VAA通过评估数据脆弱性、分组，并利用Group DRO框架进行对抗性采样和扰动，实现均衡学习。

Result: 在四个微调任务中，VAA显著降低有害分数，同时保持下游任务性能，优于现有基线。

Conclusion: VAA通过脆弱性感知的对齐策略，为LLMs安全微调提供了更有效的防护方案。

Abstract: Harmful fine-tuning (HFT), performed directly on open-source LLMs or through
Fine-tuning-as-a-Service, breaks safety alignment and poses significant
threats. Existing methods aim to mitigate HFT risks by learning robust
representation on alignment data or making harmful data unlearnable, but they
treat each data sample equally, leaving data vulnerability patterns
understudied. In this work, we reveal that certain subsets of alignment data
are consistently more prone to forgetting during HFT across different
fine-tuning tasks. Inspired by these findings, we propose Vulnerability-Aware
Alignment (VAA), which estimates data vulnerability, partitions data into
"vulnerable" and "invulnerable" groups, and encourages balanced learning using
a group distributionally robust optimization (Group DRO) framework.
Specifically, VAA learns an adversarial sampler that samples examples from the
currently underperforming group and then applies group-dependent adversarial
perturbations to the data during training, aiming to encourage a balanced
learning process across groups. Experiments across four fine-tuning tasks
demonstrate that VAA significantly reduces harmful scores while preserving
downstream task performance, outperforming state-of-the-art baselines.

</details>


### [193] [Prompt Candidates, then Distill: A Teacher-Student Framework for LLM-driven Data Annotation](https://arxiv.org/abs/2506.03857)
*Mingxuan Xia,Haobo Wang,Yixuan Li,Zewei Yu,Jindong Wang,Junbo Zhao,Runze Wu*

Main category: cs.LG

TL;DR: 本文提出一种新的候选标注范式CanDist，通过让大语言模型输出所有可能的标签来应对不确定性，并采用师生框架蒸馏标注结果，显著提升下游任务数据质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接要求大语言模型为每个样本确定单一黄金标签，但由于模型内在不确定性，困难样本容易产生错误标注，严重影响下游数据质量。受人类模糊厌恶行为启发，作者提出新解决方案。

Method: 提出候选标注范式：1) 鼓励LLM在不确定时输出所有可能标签 2) 设计师生框架CanDist，用小语言模型(SLM)蒸馏候选标注结果 3) 理论证明该范式优于直接使用单一标注。

Result: 在6个文本分类任务上的实验验证了方法的有效性，理论分析表明蒸馏候选标注比直接使用单一标注具有更优的理论保证。

Conclusion: 候选标注范式能有效缓解LLM标注的不确定性问题，CanDist框架通过蒸馏显著提升下游任务数据质量，为数据标注提供了新思路。

Abstract: Recently, Large Language Models (LLMs) have demonstrated significant
potential for data annotation, markedly reducing the labor costs associated
with downstream applications. However, existing methods mostly adopt an
aggressive strategy by prompting LLM to determine a single gold label for each
unlabeled sample. Due to the inherent uncertainty within LLMs, they often
produce incorrect labels for difficult samples, severely compromising the data
quality for downstream applications. Motivated by ambiguity aversion in human
behaviors, we propose a novel candidate annotation paradigm wherein large
language models are encouraged to output all possible labels when incurring
uncertainty. To ensure unique labels are provided for downstream tasks, we
develop a teacher-student framework CanDist that distills candidate annotations
with a Small Language Model (SLM). We further provide a rigorous justification
demonstrating that distilling candidate annotations from the teacher LLM offers
superior theoretical guarantees compared to directly using single annotations.
Extensive experiments across six text classification tasks validate the
effectiveness of our proposed method. The source code is available at
https://github.com/MingxuanXia/CanDist.

</details>


### [194] [Evaluating Apple Intelligence's Writing Tools for Privacy Against Large Language Model-Based Inference Attacks: Insights from Early Datasets](https://arxiv.org/abs/2506.03870)
*Mohd. Farhan Israk Soumik,Syed Mhamudul Hasan,Abdur R. Shahid*

Main category: cs.LG

TL;DR: 研究探讨了Apple Intelligence写作工具通过文本改写和语气调整来防止LLM情感推断攻击的潜力，首次在隐私保护背景下实证分析了这些工具的效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）可能被滥用于从文本推断用户情感，构成隐私威胁。研究旨在评估Apple Intelligence的写作工具是否能有效缓解此类风险。

Method: 开发专用数据集，实证评估不同文本修改方式对LLM情感检测的影响，重点关注改写和语气调整技术。

Result: Apple Intelligence的文本修改工具展现出作为隐私保护机制的潜力，能够有效干扰LLM的情感推断。

Conclusion: 研究为未来开发动态中和敏感情感内容的自适应改写系统奠定了基础，推动设备端、以用户为中心的隐私保护机制发展。

Abstract: The misuse of Large Language Models (LLMs) to infer emotions from text for
malicious purposes, known as emotion inference attacks, poses a significant
threat to user privacy. In this paper, we investigate the potential of Apple
Intelligence's writing tools, integrated across iPhone, iPad, and MacBook, to
mitigate these risks through text modifications such as rewriting and tone
adjustment. By developing early novel datasets specifically for this purpose,
we empirically assess how different text modifications influence LLM-based
detection. This capability suggests strong potential for Apple Intelligence's
writing tools as privacy-preserving mechanisms. Our findings lay the groundwork
for future adaptive rewriting systems capable of dynamically neutralizing
sensitive emotional content to enhance user privacy. To the best of our
knowledge, this research provides the first empirical analysis of Apple
Intelligence's text-modification tools within a privacy-preservation context
with the broader goal of developing on-device, user-centric privacy-preserving
mechanisms to protect against LLMs-based advanced inference attacks on deployed
systems.

</details>


### [195] [Temporal horizons in forecasting: a performance-learnability trade-off](https://arxiv.org/abs/2506.03889)
*Pau Vilimelis Aceituno,Jack William Miller,Noah Marti,Youssef Farag,Victor Boussange*

Main category: cs.LG

TL;DR: 本文分析了自回归模型在动态系统中训练时预测时间窗口的选择问题，揭示了训练窗口长度与损失函数几何特性之间的关系，并提出了针对混沌系统和周期系统的不同优化策略。


<details>
  <summary>Details</summary>
Motivation: 在动态系统的自回归模型训练中，预测时间窗口的选择至关重要。过短的窗口可能忽略长期趋势，而过长的窗口则可能因误差累积导致收敛困难。本文旨在通过分析损失函数的几何特性，为训练窗口的选择提供理论依据。

Method: 本文通过理论分析，研究了训练窗口长度与损失函数几何特性之间的关系，特别是针对混沌系统和周期系统，分析了损失函数的粗糙度随训练窗口长度的变化规律。并通过数值实验验证了理论分析的正确性。

Result: 研究发现，对于混沌系统，损失函数的粗糙度随训练窗口长度呈指数增长；而对于周期系统，粗糙度呈线性增长。此外，长期窗口训练的模型在短期预测中表现良好，而短期窗口训练的模型在长期预测中表现较差。

Conclusion: 本文为自回归预测模型中的超参数优化提供了理论依据，建议根据系统特性选择合适的训练窗口长度，以平衡模型的收敛性和预测能力。

Abstract: When training autoregressive models for dynamical systems, a critical
question arises: how far into the future should the model be trained to
predict? Too short a horizon may miss long-term trends, while too long a
horizon can impede convergence due to accumulating prediction errors. In this
work, we formalize this trade-off by analyzing how the geometry of the loss
landscape depends on the training horizon. We prove that for chaotic systems,
the loss landscape's roughness grows exponentially with the training horizon,
while for limit cycles, it grows linearly, making long-horizon training
inherently challenging. However, we also show that models trained on long
horizons generalize well to short-term forecasts, whereas those trained on
short horizons suffer exponentially (resp. linearly) worse long-term
predictions in chaotic (resp. periodic) systems. We validate our theory through
numerical experiments and discuss practical implications for selecting training
horizons. Our results provide a principled foundation for hyperparameter
optimization in autoregressive forecasting models.

</details>


### [196] [A kernel conditional two-sample test](https://arxiv.org/abs/2506.03898)
*Pierre-François Massiani,Christian Fiedler,Lukas Haverbeck,Friedrich Solowjow,Sebastian Trimpe*

Main category: cs.LG

TL;DR: 该论文提出了一种条件概率分布假设检验框架，构建了条件双样本统计检验方法，通过核岭回归和条件核均值嵌入实现，适用于非独立数据场景。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决传统双样本检验在处理序列数据或非独立数据时的局限性，特别是在输出分布随操作参数变化时的应用需求。

Method: 方法包括将学习方法的置信界转化为条件双样本检验，并针对核岭回归和条件核均值嵌入进行实例化，引入了在线采样和参数化引导方案。

Result: 结果表明，该方法在过程监控和动态系统比较等应用中具有实用性，并为条件双样本检验提供了从理论到实践的全面基础。

Conclusion: 结论是该研究不仅推进了向量值最小二乘估计的集中性研究，还为条件双样本检验的实际应用提供了有效工具。

Abstract: We propose a framework for hypothesis testing on conditional probability
distributions, which we then use to construct conditional two-sample
statistical tests. These tests identify the inputs -- called covariates in this
context -- where two conditional expectations differ with high probability. Our
key idea is to transform confidence bounds of a learning method into a
conditional two-sample test, and we instantiate this principle for kernel ridge
regression (KRR) and conditional kernel mean embeddings. We generalize existing
pointwise-in-time or time-uniform confidence bounds for KRR to
previously-inaccessible yet essential cases such as infinite-dimensional
outputs with non-trace-class kernels. These bounds enable circumventing the
need for independent data in our statistical tests, since they allow online
sampling. We also introduce bootstrapping schemes leveraging the parametric
form of testing thresholds identified in theory to avoid tuning inaccessible
parameters, making our method readily applicable in practice. Such conditional
two-sample tests are especially relevant in applications where data arrive
sequentially or non-independently, or when output distributions vary with
operational parameters. We demonstrate their utility through examples in
process monitoring and comparison of dynamical systems. Overall, our results
establish a comprehensive foundation for conditional two-sample testing, from
theoretical guarantees to practical implementation, and advance the
state-of-the-art on the concentration of vector-valued least squares
estimation.

</details>


### [197] [Enhancing Experimental Efficiency in Materials Design: A Comparative Study of Taguchi and Machine Learning Methods](https://arxiv.org/abs/2506.03910)
*Shyam Prabhu,P Akshay Kumar,Antov Selwinston,Pavan Taduvai,Shreya Bairi,Rohit Batra*

Main category: cs.LG

TL;DR: 该研究比较了田口方法和基于主动学习的高斯过程回归（GPR）模型在优化线弧增材制造（WAAM）工艺中的表现，发现GPR在准确性和效率上均优于田口方法。


<details>
  <summary>Details</summary>
Motivation: 材料设计问题通常涉及优化多个变量，全因子探索不切实际。传统的实验设计方法（如田口技术）虽能高效采样设计空间，但无法捕捉变量的非线性依赖关系。

Method: 研究使用田口方法（三因素五水平L25正交阵列）和基于主动学习的GPR模型（结合拉丁超立方采样初始训练数据）来预测焊缝几何特征，包括熔深、焊缝宽度和高度。

Result: 在15个测试案例中，GPR模型在准确性和效率上均优于田口方法。

Conclusion: 该研究表明，机器学习方法（如GPR）能够更有效地探索复杂参数空间，适用于需要高效优化多变量的材料加工领域。

Abstract: Materials design problems often require optimizing multiple variables,
rendering full factorial exploration impractical. Design of experiment (DOE)
methods, such as Taguchi technique, are commonly used to efficiently sample the
design space but they inherently lack the ability to capture non-linear
dependency of process variables. In this work, we demonstrate how machine
learning (ML) methods can be used to overcome these limitations. We compare the
performance of Taguchi method against an active learning based Gaussian process
regression (GPR) model in a wire arc additive manufacturing (WAAM) process to
accurately predict aspects of bead geometry, including penetration depth, bead
width, and height. While Taguchi method utilized a three-factor, five-level L25
orthogonal array to suggest weld parameters, the GPR model used an
uncertainty-based exploration acquisition function coupled with latin hypercube
sampling for initial training data. Accuracy and efficiency of both models was
evaluated on 15 test cases, with GPR outperforming Taguchi in both metrics.
This work applies to broader materials processing domain requiring efficient
exploration of complex parameters.

</details>


### [198] [Learning Fair And Effective Points-Based Rewards Programs](https://arxiv.org/abs/2506.03911)
*Chamsi Hssaine,Yichun Hu,Ciara Pike-Burke*

Main category: cs.LG

TL;DR: 本文研究了基于积分奖励计划的公平设计问题，提出了在客户异质性和需求不确定性下保持公平性的算法，并分析了其对收入的影响。


<details>
  <summary>Details</summary>
Motivation: 积分奖励计划因实施中的不公平行为受到关注，研究旨在解决公平性与有效性之间的冲突。

Method: 设计了两种学习算法：一种限制阈值调整次数以降低实验风险，另一种仅降低阈值以提升公平性。

Result: 算法在预期中达到最优遗憾，个性化策略在平均情况下价值有限。

Conclusion: 研究表明，公平性算法在实践中表现良好，个性化策略对收入提升有限。

Abstract: Points-based rewards programs are a prevalent way to incentivize customer
loyalty; in these programs, customers who make repeated purchases from a seller
accumulate points, working toward eventual redemption of a free reward. These
programs have recently come under scrutiny due to accusations of unfair
practices in their implementation. Motivated by these concerns, we study the
problem of fairly designing points-based rewards programs, with a focus on two
obstacles that put fairness at odds with their effectiveness. First, due to
customer heterogeneity, the seller should set different redemption thresholds
for different customers to generate high revenue. Second, the relationship
between customer behavior and the number of accumulated points is typically
unknown; this requires experimentation which may unfairly devalue customers'
previously earned points. We first show that an individually fair rewards
program that uses the same redemption threshold for all customers suffers a
loss in revenue of at most a factor of $1+\ln 2$, compared to the optimal
personalized strategy that differentiates between customers. We then tackle the
problem of designing temporally fair learning algorithms in the presence of
demand uncertainty. Toward this goal, we design a learning algorithm that
limits the risk of point devaluation due to experimentation by only changing
the redemption threshold $O(\log T)$ times, over a horizon of length $T$. This
algorithm achieves the optimal (up to polylogarithmic factors)
$\widetilde{O}(\sqrt{T})$ regret in expectation. We then modify this algorithm
to only ever decrease redemption thresholds, leading to improved fairness at a
cost of only a constant factor in regret. Extensive numerical experiments show
the limited value of personalization in average-case settings, in addition to
demonstrating the strong practical performance of our proposed learning
algorithms.

</details>


### [199] [Learning equivariant models by discovering symmetries with learnable augmentations](https://arxiv.org/abs/2506.03914)
*Eduardo Santos Escriche,Stefanie Jegelka*

Main category: cs.LG

TL;DR: 论文提出SEMoLA方法，通过可学习的数据增强自动发现数据中的对称性，并将其软编码到模型中，无需先验知识且保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法需依赖对称性先验知识或仅通过任务隐式推断对称性，存在局限。SEMoLA旨在自动发现未知对称性并保持模型鲁棒性。

Method: SEMoLA通过可学习数据增强联合发现对称性，并将近似等变性软编码到任意无约束模型中。

Result: 实验表明SEMoLA能鲁棒地发现相关对称性，并在多模态数据集中实现高预测精度。

Conclusion: SEMoLA无需对称性先验知识，具有可解释性，且能适应分布变化，是一种端到端的有效方法。

Abstract: Recently, a trend has emerged that favors learning relevant symmetries from
data in geometric domains instead of designing constrained architectures. To do
so, two popular options are (1) to modify the training protocol, e.g., with a
specific loss and data augmentations (soft equivariance), or (2) to ignore
equivariance and infer it only implicitly. However, both options have
limitations: soft equivariance requires a priori knowledge about relevant
symmetries, while inferring symmetries merely via the task and larger data
lacks interpretability. To address both limitations, we propose SEMoLA, an
end-to-end approach that jointly (1) discovers a priori unknown symmetries in
the data via learnable data augmentations, and (2) softly encodes the
respective approximate equivariance into an arbitrary unconstrained model.
Hence, it does not need prior knowledge about symmetries, it offers
interpretability, and it maintains robustness to distribution shifts.
Empirically, we demonstrate the ability of SEMoLA to robustly discover relevant
symmetries while achieving high prediction accuracy across various datasets,
encompassing multiple data modalities and underlying symmetry groups.

</details>


### [200] [Weisfeiler and Leman Go Gambling: Why Expressive Lottery Tickets Win](https://arxiv.org/abs/2506.03919)
*Lorenz Kummer,Samir Moustafa,Anatol Ehrlich,Franka Bause,Nikolaus Suess,Wilfried N. Gansterer,Nils M. Kriege*

Main category: cs.LG

TL;DR: 该论文研究了图神经网络中的彩票假设，提出了强表达性彩票假设，并证明了稀疏子网络在保持表达能力方面的关键作用。


<details>
  <summary>Details</summary>
Motivation: 目前彩票假设在卷积神经网络中已有广泛研究，但在图神经网络中缺乏理论支持，本文旨在填补这一空白。

Method: 通过分析稀疏子网络的表达能力，特别是与Weisfeiler-Leman测试的比较，提出并证明了强表达性彩票假设。

Result: 研究表明，初始化的表达能力提升可以加速模型收敛并改善泛化性能，为彩票假设和图神经网络研究提供了新的理论基础。

Conclusion: 保持稀疏初始化图神经网络的表达能力对找到高性能的子网络至关重要，研究结果在药物发现等领域具有实际应用价值。

Abstract: The lottery ticket hypothesis (LTH) is well-studied for convolutional neural
networks but has been validated only empirically for graph neural networks
(GNNs), for which theoretical findings are largely lacking. In this paper, we
identify the expressivity of sparse subnetworks, i.e. their ability to
distinguish non-isomorphic graphs, as crucial for finding winning tickets that
preserve the predictive performance. We establish conditions under which the
expressivity of a sparsely initialized GNN matches that of the full network,
particularly when compared to the Weisfeiler-Leman test, and in that context
put forward and prove a Strong Expressive Lottery Ticket Hypothesis. We
subsequently show that an increased expressivity in the initialization
potentially accelerates model convergence and improves generalization. Our
findings establish novel theoretical foundations for both LTH and GNN research,
highlighting the importance of maintaining expressivity in sparsely initialized
GNNs. We illustrate our results using examples from drug discovery.

</details>


### [201] [Do Neural Networks Need Gradient Descent to Generalize? A Theoretical Study](https://arxiv.org/abs/2506.03931)
*Yotam Alexander,Yonatan Slutzky,Yuval Ran-Milo,Nadav Cohen*

Main category: cs.LG

TL;DR: 论文研究了过参数化神经网络泛化能力的来源，挑战了传统认为梯度下降是关键的观点，通过理论分析和实验验证了宽度和深度对泛化能力的不同影响。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为梯度下降是过参数化神经网络泛化能力的关键，但近期提出的“体积假说”认为即使不使用梯度下降，仅通过随机猜测也能保持泛化能力。本文旨在验证这一假说在宽和深网络中的有效性。

Method: 本文以矩阵分解（包括线性和非线性激活）为研究对象，理论分析了在“猜测与检查”（G&C）方法下，宽度和深度对泛化能力的影响，并通过实验验证了理论结果。

Result: 研究发现，随着宽度的增加，G&C方法的泛化能力会下降，这是首次证明G&C在某些情况下不如梯度下降；而随着深度的增加，G&C方法的泛化能力会提升，揭示了宽网络和深网络在泛化能力上的显著差异。

Conclusion: 即使在简单设置下，神经网络是否需要梯度下降才能良好泛化的问题也没有简单答案，宽和深网络在泛化能力上表现出截然不同的行为。

Abstract: Conventional wisdom attributes the mysterious generalization abilities of
overparameterized neural networks to gradient descent (and its variants). The
recent volume hypothesis challenges this view: it posits that these
generalization abilities persist even when gradient descent is replaced by
Guess & Check (G&C), i.e., by drawing weight settings until one that fits the
training data is found. The validity of the volume hypothesis for wide and deep
neural networks remains an open question. In this paper, we theoretically
investigate this question for matrix factorization (with linear and non-linear
activation)--a common testbed in neural network theory. We first prove that
generalization under G&C deteriorates with increasing width, establishing what
is, to our knowledge, the first case where G&C is provably inferior to gradient
descent. Conversely, we prove that generalization under G&C improves with
increasing depth, revealing a stark contrast between wide and deep networks,
which we further validate empirically. These findings suggest that even in
simple settings, there may not be a simple answer to the question of whether
neural networks need gradient descent to generalize well.

</details>


### [202] [FPGA-Enabled Machine Learning Applications in Earth Observation: A Systematic Review](https://arxiv.org/abs/2506.03938)
*Cédric Léonard,Dirk Stober,Martin Schulz*

Main category: cs.LG

TL;DR: 该论文综述了66个在FPGA上部署机器学习模型用于遥感应用的实验，提出了两种分类法以高效架构和实现策略，并遵循PRISMA 2020指南确保透明性和可重复性。


<details>
  <summary>Details</summary>
Motivation: 随着新无人机技术和NewSpace时代的发展，地球观测任务和数据获取面临带宽压力和实时处理需求，需要高效的机载决策支持。

Method: 系统分析了66个实验，引入两种分类法分别针对高效模型架构和FPGA实现策略，遵循PRISMA 2020指南并公开数据和代码。

Result: 研究发现FPGA在性能和适应性之间取得平衡，适合部署机器学习模型以实现遥感数据的实时自主处理。

Conclusion: FPGA结合机器学习为遥感应用提供了高效的机载处理解决方案，未来可进一步优化模型和实现策略以满足多样化任务需求。

Abstract: New UAV technologies and the NewSpace era are transforming Earth Observation
missions and data acquisition. Numerous small platforms generate large data
volume, straining bandwidth and requiring onboard decision-making to transmit
high-quality information in time. While Machine Learning allows real-time
autonomous processing, FPGAs balance performance with adaptability to
mission-specific requirements, enabling onboard deployment. This review
systematically analyzes 66 experiments deploying ML models on FPGAs for Remote
Sensing applications. We introduce two distinct taxonomies to capture both
efficient model architectures and FPGA implementation strategies. For
transparency and reproducibility, we follow PRISMA 2020 guidelines and share
all data and code at https://github.com/CedricLeon/Survey_RS-ML-FPGA.

</details>


### [203] [Lower Ricci Curvature for Hypergraphs](https://arxiv.org/abs/2506.03943)
*Shiyi Yang,Can Chen,Didong Li*

Main category: cs.LG

TL;DR: 该论文提出了一种新型超图曲率度量HLRC，平衡了可解释性和计算效率，有效揭示高阶网络结构特征。


<details>
  <summary>Details</summary>
Motivation: 现有超图曲率方法存在局限性：组合方法（如Forman-Ricci）仅捕捉粗略特征，几何方法（如Ollivier-Ricci）计算成本高昂。需要一种兼顾表达力和效率的新方法。

Method: 提出超图下Ricci曲率（HLRC），通过封闭形式定义实现几何敏感性与算法简洁性的统一。

Result: HLRC在合成和真实超图数据中能有效区分社区内/间超边、发现潜在语义标签、追踪时序动态，并支持基于全局结构的鲁棒聚类。

Conclusion: HLRC为超图分析提供了通用基础，对节点分类、异常检测和生成建模等复杂系统任务具有广泛意义。

Abstract: Networks with higher-order interactions, prevalent in biological, social, and
information systems, are naturally represented as hypergraphs, yet their
structural complexity poses fundamental challenges for geometric
characterization. While curvature-based methods offer powerful insights in
graph analysis, existing extensions to hypergraphs suffer from critical
trade-offs: combinatorial approaches such as Forman-Ricci curvature capture
only coarse features, whereas geometric methods like Ollivier-Ricci curvature
offer richer expressivity but demand costly optimal transport computations. To
address these challenges, we introduce hypergraph lower Ricci curvature (HLRC),
a novel curvature metric defined in closed form that achieves a principled
balance between interpretability and efficiency. Evaluated across diverse
synthetic and real-world hypergraph datasets, HLRC consistently reveals
meaningful higher-order organization, distinguishing intra- from
inter-community hyperedges, uncovering latent semantic labels, tracking
temporal dynamics, and supporting robust clustering of hypergraphs based on
global structure. By unifying geometric sensitivity with algorithmic
simplicity, HLRC provides a versatile foundation for hypergraph analytics, with
broad implications for tasks including node classification, anomaly detection,
and generative modeling in complex systems.

</details>


### [204] [Rethinking the Stability-Plasticity Trade-off in Continual Learning from an Architectural Perspective](https://arxiv.org/abs/2506.03951)
*Aojun Lu,Hangjie Yuan,Tao Feng,Yanan Sun*

Main category: cs.LG

TL;DR: 论文提出Dual-Arch框架，通过双网络架构解决持续学习中的稳定性-可塑性权衡问题，实验证明其高效且参数更少。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法多关注参数层面的稳定性-可塑性权衡，忽视了网络架构的影响。本文旨在从架构层面解决这一矛盾。

Method: 提出Dual-Arch框架，包含两个独立网络：一个专注可塑性（深度优先），一个专注稳定性（宽度优先），作为插件整合到现有方法中。

Result: 实验表明，在相同参数量约束下，Dual-Arch能提升现有方法性能，同时参数减少高达87%。

Conclusion: 网络架构对稳定性-可塑性权衡具有关键影响，双网络互补设计是持续学习的有效解决方案。

Abstract: The quest for Continual Learning (CL) seeks to empower neural networks with
the ability to learn and adapt incrementally. Central to this pursuit is
addressing the stability-plasticity dilemma, which involves striking a balance
between two conflicting objectives: preserving previously learned knowledge and
acquiring new knowledge. While numerous CL methods aim to achieve this
trade-off, they often overlook the impact of network architecture on stability
and plasticity, restricting the trade-off to the parameter level. In this
paper, we delve into the conflict between stability and plasticity at the
architectural level. We reveal that under an equal parameter constraint, deeper
networks exhibit better plasticity, while wider networks are characterized by
superior stability. To address this architectural-level dilemma, we introduce a
novel framework denoted Dual-Arch, which serves as a plug-in component for CL.
This framework leverages the complementary strengths of two distinct and
independent networks: one dedicated to plasticity and the other to stability.
Each network is designed with a specialized and lightweight architecture,
tailored to its respective objective. Extensive experiments demonstrate that
Dual-Arch enhances the performance of existing CL methods while being up to 87%
more compact in terms of parameters.

</details>


### [205] [HtFLlib: A Comprehensive Heterogeneous Federated Learning Library and Benchmark](https://arxiv.org/abs/2506.03954)
*Jianqing Zhang,Xinghao Wu,Yanbing Zhou,Xiaoting Sun,Qiqi Cai,Yang Liu,Yang Hua,Zhenzhe Zheng,Jian Cao,Qiang Yang*

Main category: cs.LG

TL;DR: 该论文介绍了首个异构联邦学习库HtFLlib，旨在解决异构模型间协作和数据异质性问题，提供标准化评估框架。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习仅支持同构模型，限制了异构模型架构客户端间的协作。为解决这一问题，异构联邦学习方法（HtFL）应运而生，但缺乏统一的评估基准。

Method: 论文提出HtFLlib，一个易于使用且可扩展的框架，集成了多种数据集和模型异构场景，实现了10种代表性HtFL方法，并进行了系统性评估。

Result: HtFLlib整合了12个数据集、40种模型架构，覆盖多种领域和模态，提供了准确性、收敛性、计算和通信成本等方面的评估。

Conclusion: HtFLlib有望推动异构联邦学习研究的进展，并促进其更广泛的应用。

Abstract: As AI evolves, collaboration among heterogeneous models helps overcome data
scarcity by enabling knowledge transfer across institutions and devices.
Traditional Federated Learning (FL) only supports homogeneous models, limiting
collaboration among clients with heterogeneous model architectures. To address
this, Heterogeneous Federated Learning (HtFL) methods are developed to enable
collaboration across diverse heterogeneous models while tackling the data
heterogeneity issue at the same time. However, a comprehensive benchmark for
standardized evaluation and analysis of the rapidly growing HtFL methods is
lacking. Firstly, the highly varied datasets, model heterogeneity scenarios,
and different method implementations become hurdles to making easy and fair
comparisons among HtFL methods. Secondly, the effectiveness and robustness of
HtFL methods are under-explored in various scenarios, such as the medical
domain and sensor signal modality. To fill this gap, we introduce the first
Heterogeneous Federated Learning Library (HtFLlib), an easy-to-use and
extensible framework that integrates multiple datasets and model heterogeneity
scenarios, offering a robust benchmark for research and practical applications.
Specifically, HtFLlib integrates (1) 12 datasets spanning various domains,
modalities, and data heterogeneity scenarios; (2) 40 model architectures,
ranging from small to large, across three modalities; (3) a modularized and
easy-to-extend HtFL codebase with implementations of 10 representative HtFL
methods; and (4) systematic evaluations in terms of accuracy, convergence,
computation costs, and communication costs. We emphasize the advantages and
potential of state-of-the-art HtFL methods and hope that HtFLlib will catalyze
advancing HtFL research and enable its broader applications. The code is
released at https://github.com/TsingZ0/HtFLlib.

</details>


### [206] [Adapt before Continual Learning](https://arxiv.org/abs/2506.03956)
*Aojun Lu,Tao Feng,Hangjie Yuan,Chunhui Ding,Yanan Sun*

Main category: cs.LG

TL;DR: 论文提出ACL框架，通过在核心持续学习过程前调整预训练模型，平衡稳定性和可塑性，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法在冻结预训练模型主干时限制可塑性，而全参数微调又易导致灾难性遗忘，需解决稳定性和可塑性的权衡问题。

Method: 提出ACL框架，在每项新任务学习前通过即插即用的适应阶段调整预训练模型主干，结合提示调优等方法。

Result: 大量实验表明，ACL显著提升了多种基准测试和集成方法的持续学习性能。

Conclusion: ACL为基于预训练模型的持续学习提供了通用解决方案，有效平衡稳定性和可塑性。

Abstract: Continual Learning (CL) seeks to enable neural networks to incrementally
acquire new knowledge (plasticity) while retaining existing knowledge
(stability). While pre-trained models (PTMs) have become pivotal in CL,
prevailing approaches freeze the PTM backbone to preserve stability, limiting
their plasticity, particularly when encountering significant domain gaps in
incremental tasks. Conversely, sequentially finetuning the entire PTM risks
catastrophic forgetting of generalizable knowledge, exposing a critical
stability-plasticity trade-off. To address this challenge, we propose Adapting
PTMs before the core CL process (ACL), a novel framework that refines the PTM
backbone through a plug-and-play adaptation phase before learning each new task
with existing CL approaches (e.g., prompt tuning). ACL enhances plasticity by
aligning embeddings with their original class prototypes while distancing them
from others, theoretically and empirically shown to balance stability and
plasticity. Extensive experiments demonstrate that ACL significantly improves
CL performance across benchmarks and integrated methods, offering a versatile
solution for PTM-based CL.

</details>


### [207] [Causality-Aware Contrastive Learning for Robust Multivariate Time-Series Anomaly Detection](https://arxiv.org/abs/2506.03964)
*HyunGi Kim,Jisoo Mok,Dongjun Lee,Jaihyun Lew,Sungjae Kim,Sungroh Yoon*

Main category: cs.LG

TL;DR: CAROTS提出了一种新的多变量时间序列异常检测方法，通过结合因果关系的对比学习来区分正常和异常样本。


<details>
  <summary>Details</summary>
Motivation: 当前多变量时间序列异常检测（MTSAD）研究中，变量间复杂的因果关系尚未充分探索，这为提高检测的鲁棒性和可靠性提供了潜力。

Method: CAROTS采用两种数据增强器生成保持和干扰因果关系的样本，分别作为正常变化和合成异常，并通过对比学习训练编码器，利用因果感知的潜在空间分离样本。

Result: 在五个真实世界和两个合成数据集上的广泛实验表明，CAROTS通过整合因果关系显著提升了MTSAD的能力。

Conclusion: CAROTS通过将因果关系融入对比学习框架，有效提升了多变量时间序列异常检测的性能，展示了因果关系的整合对MTSAD的积极影响。

Abstract: Utilizing the complex inter-variable causal relationships within multivariate
time-series provides a promising avenue toward more robust and reliable
multivariate time-series anomaly detection (MTSAD) but remains an underexplored
area of research. This paper proposes Causality-Aware contrastive learning for
RObust multivariate Time-Series (CAROTS), a novel MTSAD pipeline that
incorporates the notion of causality into contrastive learning. CAROTS employs
two data augmentors to obtain causality-preserving and -disturbing samples that
serve as a wide range of normal variations and synthetic anomalies,
respectively. With causality-preserving and -disturbing samples as positives
and negatives, CAROTS performs contrastive learning to train an encoder whose
latent space separates normal and abnormal samples based on causality.
Moreover, CAROTS introduces a similarity-filtered one-class contrastive loss
that encourages the contrastive learning process to gradually incorporate more
semantically diverse samples with common causal relationships. Extensive
experiments on five real-world and two synthetic datasets validate that the
integration of causal relationships endows CAROTS with improved MTSAD
capabilities. The code is available at https://github.com/kimanki/CAROTS.

</details>


### [208] [Solving Inverse Problems via Diffusion-Based Priors: An Approximation-Free Ensemble Sampling Approach](https://arxiv.org/abs/2506.03979)
*Haoxuan Chen,Yinuo Ren,Martin Renqiang Min,Lexing Ying,Zachary Izzo*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散模型的后验采样算法，通过改进的偏微分方程和加权粒子方法，避免了启发式近似，提高了图像逆问题重建的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的后验采样方法依赖于启发式近似，限制了其在贝叶斯逆问题中的应用潜力。本文旨在利用扩散模型的生成能力，避免这些近似，从而更准确地求解逆问题。

Method: 提出了一种基于集成的方法，结合扩散模型和序贯蒙特卡罗方法，通过分析预训练得分函数编码的扩散过程，推导出改进的偏微分方程，并采用加权粒子方法进行模拟。

Result: 理论证明，后验分布误差受预训练得分函数训练误差和粒子数量的限制。实验验证表明，该方法在图像逆问题中比现有方法重建更准确。

Conclusion: 本文提出的算法有效避免了启发式近似，利用扩散模型的生成能力，显著提高了贝叶斯逆问题的求解精度。

Abstract: Diffusion models (DMs) have proven to be effective in modeling
high-dimensional distributions, leading to their widespread adoption for
representing complex priors in Bayesian inverse problems (BIPs). However,
current DM-based posterior sampling methods proposed for solving common BIPs
rely on heuristic approximations to the generative process. To exploit the
generative capability of DMs and avoid the usage of such approximations, we
propose an ensemble-based algorithm that performs posterior sampling without
the use of heuristic approximations. Our algorithm is motivated by existing
works that combine DM-based methods with the sequential Monte Carlo (SMC)
method. By examining how the prior evolves through the diffusion process
encoded by the pre-trained score function, we derive a modified partial
differential equation (PDE) governing the evolution of the corresponding
posterior distribution. This PDE includes a modified diffusion term and a
reweighting term, which can be simulated via stochastic weighted particle
methods. Theoretically, we prove that the error between the true posterior
distribution can be bounded in terms of the training error of the pre-trained
score function and the number of particles in the ensemble. Empirically, we
validate our algorithm on several inverse problems in imaging to show that our
method gives more accurate reconstructions compared to existing DM-based
methods.

</details>


### [209] [Optimal Spiking Brain Compression: Improving One-Shot Post-Training Pruning and Quantization for Spiking Neural Networks](https://arxiv.org/abs/2506.03996)
*Lianfeng Shi,Ao Li,Benjamin Ward-Cherrier*

Main category: cs.LG

TL;DR: 本文提出了一种名为OSBC的单次后训练剪枝/量化框架，用于高效压缩脉冲神经网络(SNN)，在保持精度的同时显著提升稀疏度和量化效果。


<details>
  <summary>Details</summary>
Motivation: 现有SNN压缩方法需多次迭代计算，成本高昂。针对脉冲神经元特性，需要更高效的压缩方案以适应神经形态硬件的资源限制。

Method: 改进OBC方法，通过最小化脉冲神经元膜电位损失（而非输入电流损失），利用小样本数据集实现单次压缩。

Result: 在N-MNIST等数据集上实现97%稀疏度（精度损失1.41%-10.20%）或4bit量化（精度损失0.17%-7.71%）。

Conclusion: OSBC首次实现SNN单次高效压缩，为神经形态硬件部署提供实用解决方案。

Abstract: Spiking Neural Networks (SNNs) have emerged as a new generation of
energy-efficient neural networks suitable for implementation on neuromorphic
hardware. As neuromorphic hardware has limited memory and computing resources,
weight pruning and quantization have recently been explored to improve SNNs'
efficiency. State-of-the-art SNN pruning/quantization methods employ multiple
compression and training iterations, increasing the cost for pre-trained or
very large SNNs. In this paper, we propose a new one-shot post-training
pruning/quantization framework, Optimal Spiking Brain Compression (OSBC), that
adapts the Optimal Brain Compression (OBC) method of [Frantar, Singh, and
Alistarh, 2023] for SNNs. Rather than minimizing the loss on neuron input
current as OBC does, OSBC achieves more efficient and accurate SNN compression
in one pass by minimizing the loss on spiking neuron membrane potential with a
small sample dataset. Our experiments on neuromorphic datasets (N-MNIST,
CIFAR10-DVS, DVS128-Gesture) demonstrate that OSBC can achieve 97% sparsity
through pruning with 1.41%, 10.20%, and 1.74% accuracy loss, or 4-bit symmetric
quantization with 0.17%, 1.54%, and 7.71% accuracy loss, respectively. Code
will be available on GitHub.

</details>


### [210] [CARL: Causality-guided Architecture Representation Learning for an Interpretable Performance Predictor](https://arxiv.org/abs/2506.04001)
*Han Ji,Yuqi Feng,Jiahao Fan,Yanan Sun*

Main category: cs.LG

TL;DR: 论文提出了一种基于因果关系的架构表示学习方法（CARL），用于提升神经架构搜索中性能预测的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有性能预测器在神经架构搜索中常忽视训练样本与测试样本间的分布偏移，导致学习到虚假相关性，泛化能力差。

Method: CARL通过子结构提取器将架构分为关键和非关键子结构，并生成干预样本以强化关键特征的学习。

Result: 在五个NAS搜索空间上的实验表明，CARL实现了最先进的准确率，如在CIFAR-10上达到97.67%的top-1准确率。

Conclusion: CARL通过分离关键和非关键特征，显著提升了架构性能预测的准确性和可解释性。

Abstract: Performance predictors have emerged as a promising method to accelerate the
evaluation stage of neural architecture search (NAS). These predictors estimate
the performance of unseen architectures by learning from the correlation
between a small set of trained architectures and their performance. However,
most existing predictors ignore the inherent distribution shift between limited
training samples and diverse test samples. Hence, they tend to learn spurious
correlations as shortcuts to predictions, leading to poor generalization. To
address this, we propose a Causality-guided Architecture Representation
Learning (CARL) method aiming to separate critical (causal) and redundant
(non-causal) features of architectures for generalizable architecture
performance prediction. Specifically, we employ a substructure extractor to
split the input architecture into critical and redundant substructures in the
latent space. Then, we generate multiple interventional samples by pairing
critical representations with diverse redundant representations to prioritize
critical features. Extensive experiments on five NAS search spaces demonstrate
the state-of-the-art accuracy and superior interpretability of CARL. For
instance, CARL achieves 97.67% top-1 accuracy on CIFAR-10 using DARTS.

</details>


### [211] [On the Usage of Gaussian Process for Efficient Data Valuation](https://arxiv.org/abs/2506.04026)
*Clément Bénesse,Patrick Mesana,Athénaïs Gautier,Sébastien Gambs*

Main category: cs.LG

TL;DR: 该论文提出了一种新的数据价值评估方法，通过分解为效用函数和聚合过程，并使用高斯过程快速估计数据价值。


<details>
  <summary>Details</summary>
Motivation: 在机器学习中，了解数据对模型训练的影响（即数据价值评估）是一个基本任务。现有方法需要更高效且理论支持更强的解决方案。

Method: 设计了一种规范分解方法，将数据价值评估分为效用函数和聚合过程，并利用高斯过程快速访问子模型的效用函数。

Result: 该方法基于贝叶斯理论，具有高效更新公式，能够快速估计数据价值。

Conclusion: 论文提出的方法在理论和实践上均有优势，为数据价值评估提供了高效且可靠的解决方案。

Abstract: In machine learning, knowing the impact of a given datum on model training is
a fundamental task referred to as Data Valuation. Building on previous works
from the literature, we have designed a novel canonical decomposition allowing
practitioners to analyze any data valuation method as the combination of two
parts: a utility function that captures characteristics from a given model and
an aggregation procedure that merges such information. We also propose to use
Gaussian Processes as a means to easily access the utility function on
``sub-models'', which are models trained on a subset of the training set. The
strength of our approach stems from both its theoretical grounding in Bayesian
theory, and its practical reach, by enabling fast estimation of valuations
thanks to efficient update formulae.

</details>


### [212] [Curse of Slicing: Why Sliced Mutual Information is a Deceptive Measure of Statistical Dependence](https://arxiv.org/abs/2506.04053)
*Alexander Semenenko,Ivan Butakov,Alexey Frolov,Ivan Oseledets*

Main category: cs.LG

TL;DR: 论文指出切片互信息(SMI)虽作为互信息的可扩展替代方案存在优势，但易受数据操纵且表现反常，甚至可能不如简单相关系数。


<details>
  <summary>Details</summary>
Motivation: 研究SMI在实际应用中的局限性，揭示其易受操纵、饱和性及对冗余信息的偏好等问题。

Method: 通过广泛基准测试和理论分析，评估SMI在不同场景下的表现及其对统计依赖性的检测能力。

Result: 发现SMI易饱和、无法有效检测统计依赖性增强、优先考虑冗余信息，并在某些情况下表现逊于简单相关性指标。

Conclusion: SMI作为依赖度量存在显著缺陷，需谨慎使用，尤其在需要精确检测统计依赖性的场景中。

Abstract: Sliced Mutual Information (SMI) is widely used as a scalable alternative to
mutual information for measuring non-linear statistical dependence. Despite its
advantages, such as faster convergence, robustness to high dimensionality, and
nullification only under statistical independence, we demonstrate that SMI is
highly susceptible to data manipulation and exhibits counterintuitive behavior.
Through extensive benchmarking and theoretical analysis, we show that SMI
saturates easily, fails to detect increases in statistical dependence (even
under linear transformations designed to enhance the extraction of
information), prioritizes redundancy over informative content, and in some
cases, performs worse than simpler dependence measures like the correlation
coefficient.

</details>


### [213] [Optimal Transport-based Domain Alignment as a Preprocessing Step for Federated Learning](https://arxiv.org/abs/2506.04071)
*Luiz Manella Pereira,M. Hadi Amini*

Main category: cs.LG

TL;DR: 该论文提出了一种基于最优传输的预处理算法，用于解决联邦学习中的数据不平衡问题，通过在边缘设备间最小化数据分布差异，提升全局模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中，由于数据不共享，本地数据集的不平衡会导致全局模型聚合性能下降，影响本地模型更新和决策准确性。

Method: 利用Wasserstein重心计算通道平均值，生成目标RGB空间，并通过投影最小化全局数据分布差异。

Result: 在CIFAR-10数据集上验证了该方法能在更少的通信轮次中实现更高的泛化能力。

Conclusion: 提出的算法有效减少了数据分布差异，提升了联邦学习的效率和模型性能。

Abstract: Federated learning (FL) is a subfield of machine learning that avoids sharing
local data with a central server, which can enhance privacy and scalability.
The inability to consolidate data leads to a unique problem called dataset
imbalance, where agents in a network do not have equal representation of the
labels one is trying to learn to predict. In FL, fusing locally-trained models
with unbalanced datasets may deteriorate the performance of global model
aggregation, and reduce the quality of updated local models and the accuracy of
the distributed agents' decisions. In this work, we introduce an Optimal
Transport-based preprocessing algorithm that aligns the datasets by minimizing
the distributional discrepancy of data along the edge devices. We accomplish
this by leveraging Wasserstein barycenters when computing channel-wise
averages. These barycenters are collected in a trusted central server where
they collectively generate a target RGB space. By projecting our dataset
towards this target space, we minimize the distributional discrepancy on a
global level, which facilitates the learning process due to a minimization of
variance across the samples. We demonstrate the capabilities of the proposed
approach over the CIFAR-10 dataset, where we show its capability of reaching
higher degrees of generalization in fewer communication rounds.

</details>


### [214] [Multimodal Tabular Reasoning with Privileged Structured Information](https://arxiv.org/abs/2506.04088)
*Jun-Peng Jiang,Yu Xia,Hai-Long Sun,Shiyin Lu,Qing-Guo Chen,Weihua Luo,Kaifu Zhang,De-Chuan Zhan,Han-Jia Ye*

Main category: cs.LG

TL;DR: 本文提出Turbo框架，利用训练时的结构化信息增强多模态大语言模型，解决表格图像推理中的模态对齐和推理能力迁移问题，仅用9k数据即实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中表格多以图像形式存在，缺乏高质量文本表示，现有基于LLM的表格推理方法难以直接应用，需解决视觉与结构化信息的对齐及跨模态推理能力迁移问题。

Method: 提出Turbo框架：1) 基于DeepSeek-R1的结构感知推理轨迹生成器构建模态桥接数据；2) 通过迭代生成和选择优化推理路径增强模型能力。

Result: 在多个数据集上取得SOTA性能（较之前最佳提升7.2%），仅需9k训练数据。

Conclusion: Turbo通过结构化信息桥接和推理路径优化，有效提升了多模态表格推理性能，为现实场景中的表格图像理解提供了新解决方案。

Abstract: Tabular reasoning involves multi-step information extraction and logical
inference over tabular data. While recent advances have leveraged large
language models (LLMs) for reasoning over structured tables, such high-quality
textual representations are often unavailable in real-world settings, where
tables typically appear as images. In this paper, we tackle the task of tabular
reasoning from table images, leveraging privileged structured information
available during training to enhance multimodal large language models (MLLMs).
The key challenges lie in the complexity of accurately aligning structured
information with visual representations, and in effectively transferring
structured reasoning skills to MLLMs despite the input modality gap. To address
these, we introduce TabUlar Reasoning with Bridged infOrmation ({\sc Turbo}), a
new framework for multimodal tabular reasoning with privileged structured
tables. {\sc Turbo} benefits from a structure-aware reasoning trace generator
based on DeepSeek-R1, contributing to high-quality modality-bridged data. On
this basis, {\sc Turbo} repeatedly generates and selects the advantageous
reasoning paths, further enhancing the model's tabular reasoning ability.
Experimental results demonstrate that, with limited ($9$k) data, {\sc Turbo}
achieves state-of-the-art performance ($+7.2\%$ vs. previous SOTA) across
multiple datasets.

</details>


### [215] [AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment](https://arxiv.org/abs/2506.04089)
*Anastasiia Ivanova,Eva Bakaeva,Zoya Volovikova,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: 论文提出了AmbiK数据集，用于统一比较处理模糊指令的方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法因使用不同数据集而难以比较，需统一基准测试模糊指令处理能力。

Method: 利用LLMs辅助收集并人工验证，构建包含2000个任务的厨房环境文本数据集AmbiK。

Result: AmbiK包含1000对模糊/明确指令，按类型分类，附带环境描述、澄清问答等。

Conclusion: AmbiK为研究者提供了标准化比较工具，推动模糊指令检测方法的发展。

Abstract: As a part of an embodied agent, Large Language Models (LLMs) are typically
used for behavior planning given natural language instructions from the user.
However, dealing with ambiguous instructions in real-world environments remains
a challenge for LLMs. Various methods for task ambiguity detection have been
proposed. However, it is difficult to compare them because they are tested on
different datasets and there is no universal benchmark. For this reason, we
propose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual
dataset of ambiguous instructions addressed to a robot in a kitchen
environment. AmbiK was collected with the assistance of LLMs and is
human-validated. It comprises 1000 pairs of ambiguous tasks and their
unambiguous counterparts, categorized by ambiguity type (Human Preferences,
Common Sense Knowledge, Safety), with environment descriptions, clarifying
questions and answers, user intents, and task plans, for a total of 2000 tasks.
We hope that AmbiK will enable researchers to perform a unified comparison of
ambiguity detection methods. AmbiK is available at
https://github.com/cog-model/AmbiK-dataset.

</details>


### [216] [Guided Speculative Inference for Efficient Test-Time Alignment of LLMs](https://arxiv.org/abs/2506.04118)
*Jonathan Geuter,Youssef Mroueh,David Alvarez-Melis*

Main category: cs.LG

TL;DR: 提出了一种名为GSI的新算法，通过结合奖励模型和小型辅助模型，有效提升大语言模型的解码效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 为了提高大型语言模型在解码过程中的效率和准确性，特别是在需要奖励引导的任务中，如数学推理。

Method: 结合软最佳n测试时间缩放与奖励模型r(x,y)和小型辅助模型π_S(y∣x)的推测样本，近似最优倾斜策略π_β,B(y∣x)。

Result: 在多个推理基准测试中（如MATH500、OlympiadBench、Minerva Math），GSI的准确性优于标准的软最佳n方法，并在某些情况下甚至超过了使用主模型π_B的软最佳n方法。

Conclusion: GSI算法在提升解码效率和准确性方面表现出色，特别是在奖励引导的任务中，具有实际应用潜力。

Abstract: We propose Guided Speculative Inference (GSI), a novel algorithm for
efficient reward-guided decoding in large language models. GSI combines soft
best-of-$n$ test-time scaling with a reward model $r(x,y)$ and speculative
samples from a small auxiliary model $\pi_S(y\mid x)$. We provably approximate
the optimal tilted policy $\pi_{\beta,B}(y\mid x) \propto \pi_B(y\mid
x)\exp(\beta\,r(x,y))$ of soft best-of-$n$ under the primary model $\pi_B$. We
derive a theoretical bound on the KL divergence between our induced
distribution and the optimal policy. In experiments on reasoning benchmarks
(MATH500, OlympiadBench, Minerva Math), our method achieves higher accuracy
than standard soft best-of-$n$ with $\pi_S$ and reward-guided speculative
decoding (Liao et al., 2025), and in certain settings even outperforms soft
best-of-$n$ with $\pi_B$. The code is available at
https://github.com/j-geuter/GSI .

</details>


### [217] [Incremental Gradient Descent with Small Epoch Counts is Surprisingly Slow on Ill-Conditioned Problems](https://arxiv.org/abs/2506.04126)
*Yujun Kim,Jaeyoung Cha,Chulhee Yun*

Main category: cs.LG

TL;DR: 论文研究了小周期（epoch）情况下增量梯度下降（IGD）的收敛性，发现其可能比预期慢，甚至在某些非凸情况下表现更差。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注大周期（epoch数K大于条件数κ）情况下基于排列的SGD（如随机重排SGD）的收敛速度优于均匀采样SGD，但对小周期（K<κ）情况知之甚少。本文旨在填补这一空白。

Method: 研究采用增量梯度下降（IGD）方法，分析其在光滑且强凸函数上的收敛性，并探讨小周期情况下的表现。

Result: 研究发现，小周期情况下IGD收敛可能非常慢，甚至在所有分量函数均为强凸时也是如此。当部分分量函数非凸时，IGD的最优性差距可能更差。

Conclusion: 基于排列的SGD在小周期情况下的收敛性质可能因分量函数的假设不同而有显著差异。研究还补充了大周期情况下IGD的上下界分析。

Abstract: Recent theoretical results demonstrate that the convergence rates of
permutation-based SGD (e.g., random reshuffling SGD) are faster than
uniform-sampling SGD; however, these studies focus mainly on the large epoch
regime, where the number of epochs $K$ exceeds the condition number $\kappa$.
In contrast, little is known when $K$ is smaller than $\kappa$, and it is still
a challenging open question whether permutation-based SGD can converge faster
in this small epoch regime (Safran and Shamir, 2021). As a step toward
understanding this gap, we study the naive deterministic variant, Incremental
Gradient Descent (IGD), on smooth and strongly convex functions. Our lower
bounds reveal that for the small epoch regime, IGD can exhibit surprisingly
slow convergence even when all component functions are strongly convex.
Furthermore, when some component functions are allowed to be nonconvex, we
prove that the optimality gap of IGD can be significantly worse throughout the
small epoch regime. Our analyses reveal that the convergence properties of
permutation-based SGD in the small epoch regime may vary drastically depending
on the assumptions on component functions. Lastly, we supplement the paper with
tight upper and lower bounds for IGD in the large epoch regime.

</details>


### [218] [Faster Approx. Top-K: Harnessing the Full Power of Two Stages](https://arxiv.org/abs/2506.04165)
*Yashas Samaga,Varun Yerram,Spandana Raj Babbula,Prateek Jain,Praneeth Netrapalli*

Main category: cs.LG

TL;DR: 本文提出了一种改进的Top-K选择算法，通过调整第一阶段的分区和选择策略，在保持召回率的同时显著提升计算速度。


<details>
  <summary>Details</summary>
Motivation: Top-K选择在机器学习中常见，但在加速器上成为性能瓶颈。现有近似算法虽快但仍有改进空间。

Method: 推广了两阶段Top-K算法：第一阶段从每个分区选择Top-K'元素（K'≥1），第二阶段对缩小后的子集排序并返回Top-K。

Result: 理论证明新算法在保持相同召回率下能更有效缩减第二阶段输入规模；在Cloud TPUv5e上实现比原算法快约10倍且召回率无损。

Conclusion: 通过理论改进和工程实现，新算法在真实任务中实现了数量级的速度提升而不牺牲精度。

Abstract: We consider the Top-$K$ selection problem, which aims to identify the
largest-$K$ elements from an array. Top-$K$ selection arises in many machine
learning algorithms and often becomes a bottleneck on accelerators, which are
optimized for dense matrix multiplications. To address this problem,
\citet{chern2022tpuknnknearestneighbor} proposed a fast two-stage
\textit{approximate} Top-$K$ algorithm: (i) partition the input array and
select the top-$1$ element from each partition, (ii) sort this \textit{smaller
subset} and return the top $K$ elements. In this paper, we consider a
generalized version of this algorithm, where the first stage selects top-$K'$
elements, for some $1 \leq K' \leq K$, from each partition. Our contributions
are as follows: (i) we derive an expression for the expected recall of this
generalized algorithm and show that choosing $K' > 1$ with fewer partitions in
the first stage reduces the input size to the second stage more effectively
while maintaining the same expected recall as the original algorithm, (ii) we
derive a bound on the expected recall for the original algorithm in
\citet{chern2022tpuknnknearestneighbor} that is provably tighter by a factor of
$2$ than the one in that paper, and (iii) we implement our algorithm on Cloud
TPUv5e and achieve around an order of magnitude speedups over the original
algorithm without sacrificing recall on real-world tasks.

</details>


### [219] [N$^2$: A Unified Python Package and Test Bench for Nearest Neighbor-Based Matrix Completion](https://arxiv.org/abs/2506.04166)
*Caleb Chin,Aashish Khubchandani,Harshvardhan Maskara,Kyuseong Choi,Jacob Feitelberg,Albert Gong,Manit Paul,Tathagata Sadhukhan,Anish Agarwal,Raaz Dwivedi*

Main category: cs.LG

TL;DR: 该论文介绍了N²，一个统一的Python包和测试平台，用于最近邻（NN）矩阵补全方法，支持快速实验和基准测试，并在多种实际应用中展示了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 最近邻方法在矩阵补全中表现出色，但缺乏统一的工具支持。本文旨在通过N²包提供一个模块化、可扩展的接口，促进研究和实际应用。

Method: 论文提出了N²，一个整合多种NN方法的Python包，并引入了一种新的NN变体，通过模块化接口支持快速实验和基准测试。

Result: 实验表明，NN方法在实际数据中优于传统方法，N²包在多个领域（如医疗、推荐系统）中表现出色，并提供了基准数据集。

Conclusion: N²为矩阵补全提供了一个强大且灵活的工具，NN方法在实际应用中具有显著优势，尤其在复杂真实数据场景下。

Abstract: Nearest neighbor (NN) methods have re-emerged as competitive tools for matrix
completion, offering strong empirical performance and recent theoretical
guarantees, including entry-wise error bounds, confidence intervals, and
minimax optimality. Despite their simplicity, recent work has shown that NN
approaches are robust to a range of missingness patterns and effective across
diverse applications. This paper introduces N$^2$, a unified Python package and
testbed that consolidates a broad class of NN-based methods through a modular,
extensible interface. Built for both researchers and practitioners, N$^2$
supports rapid experimentation and benchmarking. Using this framework, we
introduce a new NN variant that achieves state-of-the-art results in several
settings. We also release a benchmark suite of real-world datasets, from
healthcare and recommender systems to causal inference and LLM evaluation,
designed to stress-test matrix completion methods beyond synthetic scenarios.
Our experiments demonstrate that while classical methods excel on idealized
data, NN-based techniques consistently outperform them in real-world settings.

</details>


### [220] [Horizon Reduction Makes RL Scalable](https://arxiv.org/abs/2506.04168)
*Seohong Park,Kevin Frans,Deepinder Mann,Benjamin Eysenbach,Aviral Kumar,Sergey Levine*

Main category: cs.LG

TL;DR: 该论文研究了离线强化学习算法的可扩展性，发现长任务周期是限制其扩展的主要因素，并提出了一种名为SHARSA的降周期方法，显著提升了算法性能。


<details>
  <summary>Details</summary>
Motivation: 研究离线强化学习算法在面对复杂任务和大规模数据时的可扩展性，探索现有算法在数据量增大时性能提升受限的原因。

Method: 通过实验验证任务周期对离线强化学习扩展性的影响，提出并评估多种降周期技术，最终开发了一种名为SHARSA的降周期方法。

Result: 实验表明，长周期确实限制了离线强化学习的扩展性，而SHARSA方法在多种任务中表现出最佳的扩展性和渐进性能。

Conclusion: 通过显式降低任务周期，可以有效提升离线强化学习的可扩展性，SHARSA方法为这一方向提供了有效的解决方案。

Abstract: In this work, we study the scalability of offline reinforcement learning (RL)
algorithms. In principle, a truly scalable offline RL algorithm should be able
to solve any given problem, regardless of its complexity, given sufficient
data, compute, and model capacity. We investigate if and how current offline RL
algorithms match up to this promise on diverse, challenging, previously
unsolved tasks, using datasets up to 1000x larger than typical offline RL
datasets. We observe that despite scaling up data, many existing offline RL
algorithms exhibit poor scaling behavior, saturating well below the maximum
performance. We hypothesize that the horizon is the main cause behind the poor
scaling of offline RL. We empirically verify this hypothesis through several
analysis experiments, showing that long horizons indeed present a fundamental
barrier to scaling up offline RL. We then show that various horizon reduction
techniques substantially enhance scalability on challenging tasks. Based on our
insights, we also introduce a minimal yet scalable method named SHARSA that
effectively reduces the horizon. SHARSA achieves the best asymptotic
performance and scaling behavior among our evaluation methods, showing that
explicitly reducing the horizon unlocks the scalability of offline RL. Code:
https://github.com/seohongpark/horizon-reduction

</details>


### [221] [Physics-Constrained Flow Matching: Sampling Generative Models with Hard Constraints](https://arxiv.org/abs/2506.04171)
*Utkarsh Utkarsh,Pengfei Cai,Alan Edelman,Rafael Gomez-Bombarelli,Christopher Vincent Rackauckas*

Main category: cs.LG

TL;DR: 提出PCFM方法，在预训练的基于流的生成模型中强制执行任意非线性约束，确保物理约束的精确满足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在强制执行物理约束（如守恒定律和物理一致性）时存在挑战，通常依赖软惩罚或架构偏置，无法保证硬约束。

Method: PCFM通过物理修正持续指导采样过程，确保中间解状态满足物理约束并与学习到的流对齐。

Result: PCFM在多种PDE上优于无约束和约束基线，包括具有激波、不连续性和尖锐特征的PDE，同时确保最终解满足约束。

Conclusion: PCFM为科学和通用生成模型提供了一个强制执行硬约束的通用框架，特别适用于约束满足至关重要的应用。

Abstract: Deep generative models have recently been applied to physical systems
governed by partial differential equations (PDEs), offering scalable simulation
and uncertainty-aware inference. However, enforcing physical constraints, such
as conservation laws (linear and nonlinear) and physical consistencies, remains
challenging. Existing methods often rely on soft penalties or architectural
biases that fail to guarantee hard constraints. In this work, we propose
Physics-Constrained Flow Matching (PCFM), a zero-shot inference framework that
enforces arbitrary nonlinear constraints in pretrained flow-based generative
models. PCFM continuously guides the sampling process through physics-based
corrections applied to intermediate solution states, while remaining aligned
with the learned flow and satisfying physical constraints. Empirically, PCFM
outperforms both unconstrained and constrained baselines on a range of PDEs,
including those with shocks, discontinuities, and sharp features, while
ensuring exact constraint satisfaction at the final solution. Our method
provides a general framework for enforcing hard constraints in both scientific
and general-purpose generative models, especially in applications where
constraint satisfaction is essential.

</details>


### [222] [Does Prompt Design Impact Quality of Data Imputation by LLMs?](https://arxiv.org/abs/2506.04172)
*Shreenidhi Srinivasan,Lydia Manikonda*

Main category: cs.LG

TL;DR: 提出了一种基于大型语言模型的标记感知数据填充方法，通过优化提示设计解决类别不平衡数据集的缺失值问题，在减少输入提示大小的同时保持或提升填充质量。


<details>
  <summary>Details</summary>
Motivation: 生成真实的合成表格数据是机器学习中的关键挑战，尤其是在数据存在类别不平衡问题时。本文旨在利用大型语言模型的上下文学习能力，解决类别不平衡数据集的缺失数据填补问题。

Method: 采用结构化分组CSV风格提示技术，并消除输入提示中的无关上下文信息，结合标记感知数据填补方法。

Result: 实验结果表明，该方法显著减少了输入提示的大小，同时在较小规模数据集上保持或提高了填补质量。

Conclusion: 本研究强调了提示设计在利用大型语言模型生成合成数据中的重要性，并为类别不平衡数据集的缺失数据填补提供了实用解决方案，有望推动相关领域的进一步研究。

Abstract: Generating realistic synthetic tabular data presents a critical challenge in
machine learning. It adds another layer of complexity when this data contain
class imbalance problems. This paper presents a novel token-aware data
imputation method that leverages the in-context learning capabilities of large
language models. This is achieved through the combination of a structured
group-wise CSV-style prompting technique and the elimination of irrelevant
contextual information in the input prompt. We test this approach with two
class-imbalanced binary classification datasets and evaluate the effectiveness
of imputation using classification-based evaluation metrics. The experimental
results demonstrate that our approach significantly reduces the input prompt
size while maintaining or improving imputation quality compared to our baseline
prompt, especially for datasets that are of relatively smaller in size. The
contributions of this presented work is two-fold -- 1) it sheds light on the
importance of prompt design when leveraging LLMs for synthetic data generation
and 2) it addresses a critical gap in LLM-based data imputation for
class-imbalanced datasets with missing data by providing a practical solution
within computational constraints. We hope that our work will foster further
research and discussions about leveraging the incredible potential of LLMs and
prompt engineering techniques for synthetic data generation.

</details>


### [223] [OpenThoughts: Data Recipes for Reasoning Models](https://arxiv.org/abs/2506.04178)
*Etash Guha,Ryan Marten,Sedrick Keh,Negin Raoof,Georgios Smyrnis,Hritik Bansal,Marianna Nezhurina,Jean Mercat,Trung Vu,Zayne Sprague,Ashima Suvarna,Benjamin Feuer,Liangyu Chen,Zaid Khan,Eric Frankel,Sachin Grover,Caroline Choi,Niklas Muennighoff,Shiye Su,Wanjia Zhao,John Yang,Shreyas Pimpalgaonkar,Kartik Sharma,Charlie Cheng-Jie Ji,Yichuan Deng,Sarah Pratt,Vivek Ramanujan,Jon Saad-Falcon,Jeffrey Li,Achal Dave,Alon Albalak,Kushal Arora,Blake Wulfe,Chinmay Hegde,Greg Durrett,Sewoong Oh,Mohit Bansal,Saadia Gabriel,Aditya Grover,Kai-Wei Chang,Vaishaal Shankar,Aaron Gokaslan,Mike A. Merrill,Tatsunori Hashimoto,Yejin Choi,Jenia Jitsev,Reinhard Heckel,Maheswaran Sathiamoorthy,Alexandros G. Dimakis,Ludwig Schmidt*

Main category: cs.LG

TL;DR: OpenThoughts项目通过开源数据集训练推理模型，OpenThinker3-7B在多个基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 当前推理模型的训练依赖专有数据集，缺乏公开信息，OpenThoughts项目旨在创建开源数据集以促进推理模型的发展。

Method: 通过系统研究数据生成流程并进行1000+次控制实验，优化数据集并扩展到120万样本，使用QwQ-32B作为教师模型。

Result: OpenThinker3-7B在AIME 2025、LiveCodeBench 06/24-01/25和GPQA Diamond上分别达到53%、51%和54%的准确率。

Conclusion: OpenThoughts项目成功创建了开源数据集和模型，推动了推理模型的公开研究，所有数据和模型已在openthoughts.ai上公开。

Abstract: Reasoning models have made rapid progress on many benchmarks involving math,
code, and science. Yet, there are still many open questions about the best
training recipes for reasoning since state-of-the-art models often rely on
proprietary datasets with little to no public information available. To address
this, the goal of the OpenThoughts project is to create open-source datasets
for training reasoning models. After initial explorations, our OpenThoughts2-1M
dataset led to OpenThinker2-32B, the first model trained on public reasoning
data to match DeepSeek-R1-Distill-32B on standard reasoning benchmarks such as
AIME and LiveCodeBench. We then improve our dataset further by systematically
investigating each step of our data generation pipeline with 1,000+ controlled
experiments, which led to OpenThoughts3. Scaling the pipeline to 1.2M examples
and using QwQ-32B as teacher yields our OpenThinker3-7B model, which achieves
state-of-the-art results: 53% on AIME 2025, 51% on LiveCodeBench 06/24-01/25,
and 54% on GPQA Diamond. All of our datasets and models are available on
https://openthoughts.ai.

</details>


### [224] [How to Use Graph Data in the Wild to Help Graph Anomaly Detection?](https://arxiv.org/abs/2506.04190)
*Yuxuan Cao,Jiarong Xu,Chen Zhao,Jiaan Wang,Carl Yang,Chunping Wang,Yang Yang*

Main category: cs.LG

TL;DR: 该论文提出Wild-GAD框架，利用外部图数据解决图异常检测中数据不足的问题，通过统一数据库和选择标准显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 图异常检测面临标签稀缺、异常定义模糊和类型多样等挑战，传统监督或半监督方法不可靠，且数据不足时难以准确捕捉正常数据分布。

Method: 提出Wild-GAD框架，基于包含多领域图数据的统一数据库UniWildGraph，并通过代表性和多样性标准选择合适的外部数据辅助检测。

Result: 在六个真实数据集上的实验表明，Wild-GAD相比基线方法平均AUCROC提升18%，AUCPR提升32%。

Conclusion: 利用外部数据能有效提升图异常检测性能，Wild-GAD框架为解决数据不足问题提供了可行方案。

Abstract: In recent years, graph anomaly detection has found extensive applications in
various domains such as social, financial, and communication networks. However,
anomalies in graph-structured data present unique challenges, including label
scarcity, ill-defined anomalies, and varying anomaly types, making supervised
or semi-supervised methods unreliable. Researchers often adopt unsupervised
approaches to address these challenges, assuming that anomalies deviate
significantly from the normal data distribution. Yet, when the available data
is insufficient, capturing the normal distribution accurately and
comprehensively becomes difficult. To overcome this limitation, we propose to
utilize external graph data (i.e., graph data in the wild) to help anomaly
detection tasks. This naturally raises the question: How can we use external
data to help graph anomaly detection tasks? To answer this question, we propose
a framework called Wild-GAD. It is built upon a unified database, UniWildGraph,
which comprises a large and diverse collection of graph data with broad domain
coverage, ample data volume, and a unified feature space. Further, we develop
selection criteria based on representativity and diversity to identify the most
suitable external data for anomaly detection task. Extensive experiments on six
real-world datasets demonstrate the effectiveness of Wild-GAD. Compared to the
baseline methods, our framework has an average 18% AUCROC and 32% AUCPR
improvement over the best-competing methods.

</details>


### [225] [MACS: Multi-Agent Reinforcement Learning for Optimization of Crystal Structures](https://arxiv.org/abs/2506.04195)
*Elena Zamaraeva,Christopher M. Collins,George R. Darling,Matthew S. Dyer,Bei Peng,Rahul Savani,Dmytro Antypov,Vladimir V. Gusev,Judith Clymo,Paul G. Spirakis,Matthew J. Rosseinsky*

Main category: cs.LG

TL;DR: 提出多智能体强化学习方法MACS，用于周期性晶体结构优化，表现出高效、低计算成本和强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 几何结构优化在计算化学和材料设计中至关重要，传统方法效率低且计算成本高。

Method: MACS将优化问题建模为部分可观测马尔可夫博弈，原子作为智能体协同调整位置以寻找稳定构型。

Result: MACS在训练和未见结构上均表现优异，优化速度更快、计算量更少且失败率最低。

Conclusion: MACS为晶体结构优化提供了高效、可扩展且具有零样本迁移能力的解决方案。

Abstract: Geometry optimization of atomic structures is a common and crucial task in
computational chemistry and materials design. Following the learning to
optimize paradigm, we propose a new multi-agent reinforcement learning method
called Multi-Agent Crystal Structure optimization (MACS) to address periodic
crystal structure optimization. MACS treats geometry optimization as a
partially observable Markov game in which atoms are agents that adjust their
positions to collectively discover a stable configuration. We train MACS across
various compositions of reported crystalline materials to obtain a policy that
successfully optimizes structures from the training compositions as well as
structures of larger sizes and unseen compositions, confirming its excellent
scalability and zero-shot transferability. We benchmark our approach against a
broad range of state-of-the-art optimization methods and demonstrate that MACS
optimizes periodic crystal structures significantly faster, with fewer energy
calculations, and the lowest failure rate.

</details>


### [226] [EPiC: Towards Lossless Speedup for Reasoning Training through Edge-Preserving CoT Condensation](https://arxiv.org/abs/2506.04205)
*Jinghan Jia,Hadi Reisizadeh,Chongyu Fan,Nathalie Baracaldo,Mingyi Hong,Sijia Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种名为EPiC的边缘保留压缩方法，通过选择性保留思维链（CoT）的初始和最终段，减少训练成本同时保持推理准确性。


<details>
  <summary>Details</summary>
Motivation: 当前基于思维链（CoT）的监督训练中，冗长的推理轨迹显著增加了训练成本，尤其是在从大型推理模型（LRM）蒸馏知识时。因此，需要一种方法在减少训练数据长度的同时，保持模型的推理能力和答案准确性。

Method: 论文提出了Edge-Preserving Condensation（EPiC）方法，通过分析思维链的三阶段结构（问题理解、探索和解决方案收敛），仅保留初始和最终段，丢弃中间部分，以压缩训练数据。

Result: 实验表明，EPiC在多个模型家族（Qwen和LLaMA）和基准测试中，减少了超过34%的训练时间，同时在MATH500上实现了与完整CoT监督相当的推理准确性。

Conclusion: EPiC方法首次探索了思维级别的CoT压缩，为高效的推理模型蒸馏提供了有效解决方案，显著降低了训练成本且不损失性能。

Abstract: Large language models (LLMs) have shown remarkable reasoning capabilities
when trained with chain-of-thought (CoT) supervision. However, the long and
verbose CoT traces, especially those distilled from large reasoning models
(LRMs) such as DeepSeek-R1, significantly increase training costs during the
distillation process, where a non-reasoning base model is taught to replicate
the reasoning behavior of an LRM. In this work, we study the problem of CoT
condensation for resource-efficient reasoning training, aimed at pruning
intermediate reasoning steps (i.e., thoughts) in CoT traces, enabling
supervised model training on length-reduced CoT data while preserving both
answer accuracy and the model's ability to generate coherent reasoning. Our
rationale is that CoT traces typically follow a three-stage structure: problem
understanding, exploration, and solution convergence. Through empirical
analysis, we find that retaining the structure of the reasoning trace,
especially the early stage of problem understanding (rich in reflective cues)
and the final stage of solution convergence, is sufficient to achieve lossless
reasoning supervision. To this end, we propose an Edge-Preserving Condensation
method, EPiC, which selectively retains only the initial and final segments of
each CoT trace while discarding the middle portion. This design draws an
analogy to preserving the "edge" of a reasoning trajectory, capturing both the
initial problem framing and the final answer synthesis, to maintain logical
continuity. Experiments across multiple model families (Qwen and LLaMA) and
benchmarks show that EPiC reduces training time by over 34% while achieving
lossless reasoning accuracy on MATH500, comparable to full CoT supervision. To
the best of our knowledge, this is the first study to explore thought-level CoT
condensation for efficient reasoning model distillation.

</details>


### [227] [A Few Moments Please: Scalable Graphon Learning via Moment Matching](https://arxiv.org/abs/2506.04206)
*Reza Ramezanpour,Victor M. Tenorio,Antonio G. Marques,Ashutosh Sabharwal,Santiago Segarra*

Main category: cs.LG

TL;DR: 该论文提出了一种基于隐式神经表示（INR）的新型图核估计方法，通过矩匹配直接恢复图核，避免了潜在变量建模和复杂的Gromov-Wasserstein优化，显著提高了计算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的图核估计方法在处理大规模网络时存在可扩展性和分辨率独立性不足的问题，主要依赖于潜在变量估计或计算代价高的度量（如Gromov-Wasserstein距离）。

Method: 通过训练一个隐式神经表示（INR）来匹配观察图中子图计数的经验矩，直接估计图核，避免了潜在变量建模。此外，还引入了MomentMixup数据增强技术，在矩空间进行混合以增强基于图核的学习。

Result: 该方法在小图上表现出高准确性，在大图上具有卓越的计算效率，在75%的基准测试中优于现有可扩展估计器，其余情况下与之相当。MomentMixup在大多数基准测试中提高了图分类的准确性。

Conclusion: 该论文提出的图核估计方法不仅理论上有保障，实际性能也显著优于现有方法，为大规模网络数据的统计分析提供了高效且准确的解决方案。

Abstract: Graphons, as limit objects of dense graph sequences, play a central role in
the statistical analysis of network data. However, existing graphon estimation
methods often struggle with scalability to large networks and
resolution-independent approximation, due to their reliance on estimating
latent variables or costly metrics such as the Gromov-Wasserstein distance. In
this work, we propose a novel, scalable graphon estimator that directly
recovers the graphon via moment matching, leveraging implicit neural
representations (INRs). Our approach avoids latent variable modeling by
training an INR--mapping coordinates to graphon values--to match empirical
subgraph counts (i.e., moments) from observed graphs. This direct estimation
mechanism yields a polynomial-time solution and crucially sidesteps the
combinatorial complexity of Gromov-Wasserstein optimization. Building on
foundational results, we establish a theoretical guarantee: when the observed
subgraph motifs sufficiently represent those of the true graphon (a condition
met with sufficiently large or numerous graph samples), the estimated graphon
achieves a provable upper bound in cut distance from the ground truth.
Additionally, we introduce MomentMixup, a data augmentation technique that
performs mixup in the moment space to enhance graphon-based learning. Our
graphon estimation method achieves strong empirical performance--demonstrating
high accuracy on small graphs and superior computational efficiency on large
graphs--outperforming state-of-the-art scalable estimators in 75\% of benchmark
settings and matching them in the remaining cases. Furthermore, MomentMixup
demonstrated improved graph classification accuracy on the majority of our
benchmarks.

</details>


### [228] [Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning](https://arxiv.org/abs/2506.04207)
*Shuang Chen,Yue Guo,Zhaochen Su,Yafu Li,Yulun Wu,Jiacheng Chen,Jiayu Chen,Weijie Wang,Xiaoye Qu,Yu Cheng*

Main category: cs.LG

TL;DR: 论文提出通过分阶段训练和解决多模态强化学习问题，显著提升多模态大语言模型的复杂推理能力。


<details>
  <summary>Details</summary>
Motivation: 受Deepseek-R1在复杂文本任务中的出色推理能力启发，研究者尝试通过强化学习激发多模态大语言模型（MLLMs）的类似能力，但效果不佳。因此，论文深入分析了当前训练流程，旨在解决多模态强化学习中的关键问题。

Method: 论文提出分阶段训练方法：1）使用精选文本数据进行冷启动初始化；2）解决多模态强化学习中的梯度停滞问题；3）在多模态强化学习后进行纯文本强化学习训练。

Result: 通过上述方法，论文提出的ReVisual-R1模型在多个挑战性基准测试中达到了开源7B MLLMs的最新水平。

Conclusion: 分阶段训练方法有效平衡了感知基础和认知推理的发展，显著提升了多模态大语言模型的复杂推理能力。

Abstract: Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex
textual tasks, many works attempt to incentivize similar capabilities in
Multimodal Large Language Models (MLLMs) by directly applying reinforcement
learning (RL). However, they still struggle to activate complex reasoning. In
this paper, rather than examining multimodal RL in isolation, we delve into
current training pipelines and identify three crucial phenomena: 1) Effective
cold start initialization is critical for enhancing MLLM reasoning.
Intriguingly, we find that initializing with carefully selected text data alone
can lead to performance surpassing many recent multimodal reasoning models,
even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers
from gradient stagnation, which degrades training stability and performance. 3)
Subsequent text-only RL training, following the multimodal RL phase, further
enhances multimodal reasoning. This staged training approach effectively
balances perceptual grounding and cognitive reasoning development. By
incorporating the above insights and addressing multimodal RL issues, we
introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B
MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath,
LogicVista, DynaMath, and challenging AIME2024 and AIME2025.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [229] [A Kernel-Based Approach for Accurate Steady-State Detection in Performance Time Series](https://arxiv.org/abs/2506.04204)
*Martin Beseda,Vittorio Cortellessa,Daniele Di Pompeo,Luca Traini,Michele Tucci*

Main category: cs.PF

TL;DR: 本文提出了一种新方法，用于准确检测性能指标时间序列中预热阶段到稳态的过渡，提高了基准测试的精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 准确检测性能指标时间序列中从预热阶段到稳态的过渡是有效基准测试的关键步骤。过早或延迟的检测会导致性能分析不准确或效率低下。

Method: 该方法结合了基于核的步骤检测和统计方法，采用窗口化方法在线检测稳态，适用于噪声或非规则时间序列。

Result: 实验结果表明，新方法比现有技术减少了14.5%的总误差，提供了更可靠的稳态起始检测，提高了基准测试的精度。

Conclusion: 新方法增强了性能基准测试的准确性和稳定性，适用于多样化的时间序列数据，确保了结果的一致性和可重复性。

Abstract: This paper addresses the challenge of accurately detecting the transition
from the warmup phase to the steady state in performance metric time series,
which is a critical step for effective benchmarking. The goal is to introduce a
method that avoids premature or delayed detection, which can lead to inaccurate
or inefficient performance analysis. The proposed approach adapts techniques
from the chemical reactors domain, detecting steady states online through the
combination of kernel-based step detection and statistical methods. By using a
window-based approach, it provides detailed information and improves the
accuracy of identifying phase transitions, even in noisy or irregular time
series. Results show that the new approach reduces total error by 14.5%
compared to the state-of-the-art method. It offers more reliable detection of
the steady-state onset, delivering greater precision for benchmarking tasks.
For users, the new approach enhances the accuracy and stability of performance
benchmarking, efficiently handling diverse time series data. Its robustness and
adaptability make it a valuable tool for real-world performance evaluation,
ensuring consistent and reproducible results.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [230] [Sampling Preferences Yields Simple Trustworthiness Scores](https://arxiv.org/abs/2506.03399)
*Sean Steinle*

Main category: cs.HC

TL;DR: 该论文提出了一种名为偏好采样的方法，用于从多维度评估结果中提取标量可信度分数，以简化大型语言模型的选择过程。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的发展，AI模型的性能评估变得日益多维度化。现有的多维度评估框架虽然比单一指标更真实，但增加了模型选择的复杂性。

Method: 论文引入了偏好采样方法，通过考虑用户重视的模型性能特征，从多维度评估结果中提取标量可信度分数。

Result: 偏好采样在TrustLLM和DecodingTrust的多维度可信度评估中表现优于其他聚合方法，能够100%减少候选模型集，且对用户先验知识敏感。

Conclusion: 偏好采样是一种有效的多维度评估结果聚合方法，能够简化模型选择过程并考虑用户偏好。

Abstract: With the onset of large language models (LLMs), the performance of artificial
intelligence (AI) models is becoming increasingly multi-dimensional.
Accordingly, there have been several large, multi-dimensional evaluation
frameworks put forward to evaluate LLMs. Though these frameworks are much more
realistic than previous attempts which only used a single score like accuracy,
multi-dimensional evaluations can complicate decision-making since there is no
obvious way to select an optimal model. This work introduces preference
sampling, a method to extract a scalar trustworthiness score from
multi-dimensional evaluation results by considering the many characteristics of
model performance which users value. We show that preference sampling improves
upon alternate aggregation methods by using multi-dimensional trustworthiness
evaluations of LLMs from TrustLLM and DecodingTrust. We find that preference
sampling is consistently reductive, fully reducing the set of candidate models
100% of the time whereas Pareto optimality never reduces the set by more than
50%. Likewise, preference sampling is consistently sensitive to user
priors-allowing users to specify the relative weighting and confidence of their
preferences-whereas averaging scores is intransigent to the users' prior
knowledge.

</details>


### [231] [PromptCanvas: Composable Prompting Workspaces Using Dynamic Widgets for Exploration and Iteration in Creative Writing](https://arxiv.org/abs/2506.03741)
*Rifat Mehreen Amin,Oliver Hans Kühle,Daniel Buschek,Andreas Butz*

Main category: cs.HC

TL;DR: PromptCanvas将提示转化为可组合的小部件体验，提升AI生成内容的控制力，实验证明其优于传统对话界面，降低认知负荷并激发创意。


<details>
  <summary>Details</summary>
Motivation: 旨在通过可视化、可定制的小部件界面改进用户与AI生成内容的交互，提升创作过程的控制力和灵活性。

Method: 开发PromptCanvas平台，支持通过系统建议、用户提示或手动输入创建小部件，并进行实验室研究（18人）和后续实地研究（10人）验证效果。

Result: 实验室研究中，PromptCanvas在创造力支持指数上优于传统界面，降低认知负荷；实地研究进一步验证其促进协作写作的潜力。

Conclusion: 动态可定制界面能有效提升AI协作写作体验，视觉化组织和快速迭代有助于激发新视角和创意。

Abstract: We introduce PromptCanvas, a concept that transforms prompting into a
composable, widget-based experience on an infinite canvas. Users can generate,
customize, and arrange interactive widgets representing various facets of their
text, offering greater control over AI-generated content. PromptCanvas allows
widget creation through system suggestions, user prompts, or manual input,
providing a flexible environment tailored to individual needs. This enables
deeper engagement with the creative process. In a lab study with 18
participants, PromptCanvas outperformed a traditional conversational UI on the
Creativity Support Index. Participants found that it reduced cognitive load,
with lower mental demand and frustration. Qualitative feedback revealed that
the visual organization of thoughts and easy iteration encouraged new
perspectives and ideas. A follow-up field study (N=10) confirmed these results,
showcasing the potential of dynamic, customizable interfaces in improving
collaborative writing with AI.

</details>


### [232] [Crowd-SFT: Crowdsourcing for LLM Alignment](https://arxiv.org/abs/2506.04063)
*Alex Sotiropoulos,Sulyab Thottungal Valapu,Linus Lei,Jared Coleman,Bhaskar Krishnamachari*

Main category: cs.HC

TL;DR: 论文提出了一种开放的众包微调框架，通过更广泛的反馈收集减少传统SFT和RLHF方法的成本、偏见和扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 传统的SFT和RLHF方法依赖小规模、经过审核的标注者群体，导致成本高、易产生偏见且难以扩展。

Method: 采用开放的众包框架，通过基于点的奖励系统（与Shapley值相关）促进激励公平，并通过迭代模型更新引导模型收敛。

Result: 多模型选择框架将目标距离减少高达55%，验证了点奖励机制与Shapley值的紧密对齐。

Conclusion: 该框架支持公平且可扩展的参与，为LLM的微调提供了更高效的解决方案。

Abstract: Large Language Models (LLMs) increasingly rely on Supervised Fine-Tuning
(SFT) and Reinforcement Learning from Human Feedback (RLHF) to align model
responses with human preferences. While RLHF employs a reinforcement learning
approach with a separate reward model, SFT uses human-curated datasets for
supervised learning. Both approaches traditionally depend on small, vetted
groups of annotators, making them costly, prone to bias, and limited in
scalability. We propose an open, crowd-sourced fine-tuning framework that
addresses these limitations by enabling broader feedback collection for SFT
without extensive annotator training. Our framework promotes incentive fairness
via a point-based reward system correlated with Shapley values and guides model
convergence through iterative model updates. Our multi-model selection
framework demonstrates up to a 55% reduction in target distance over
single-model selection, enabling subsequent experiments that validate our
point-based reward mechanism's close alignment with Shapley values (a
well-established method for attributing individual contributions) thereby
supporting fair and scalable participation.

</details>


<div id='physics.acc-ph'></div>

# physics.acc-ph [[Back]](#toc)

### [233] [Geoff: The Generic Optimization Framework & Frontend for Particle Accelerator Controls](https://arxiv.org/abs/2506.03796)
*Penelope Madysa,Sabrina Appel,Verena Kain,Michael Schenk*

Main category: physics.acc-ph

TL;DR: Geoff是一个Python框架，用于统一粒子加速器控制中的机器学习方法，提供标准化接口和工具。


<details>
  <summary>Details</summary>
Motivation: 全球粒子加速器实验室采用多种机器学习方法提升性能，但缺乏统一框架，Geoff旨在解决这一问题。

Method: 提供标准化优化问题接口、开发工具和参考GUI应用，整合不同算法。

Result: Geoff作为开源库由CERN和GSI合作维护，支持算法比较和迁移。

Conclusion: Geoff通过统一框架简化了粒子加速器控制的机器学习应用，促进研究协作。

Abstract: Geoff is a collection of Python packages that form a framework for automation
of particle accelerator controls. With particle accelerator laboratories around
the world researching machine learning techniques to improve accelerator
performance and uptime, a multitude of approaches and algorithms have emerged.
The purpose of Geoff is to harmonize these approaches and to minimize friction
when comparing or migrating between them. It provides standardized interfaces
for optimization problems, utility functions to speed up development, and a
reference GUI application that ties everything together. Geoff is an
open-source library developed at CERN and maintained and updated in
collaboration between CERN and GSI as part of the EURO-LABS project. This paper
gives an overview over Geoff's design, features, and current usage.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [234] [A Data-Driven Diffusion-based Approach for Audio Deepfake Explanations](https://arxiv.org/abs/2506.03425)
*Petr Grinberg,Ankur Kumar,Surya Koppisetti,Gaurav Bharaj*

Main category: eess.AS

TL;DR: 论文提出了一种新的数据驱动方法，用于识别深度伪造音频中的伪影区域，优于传统解释性技术。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏清晰的地面真实标注，评估音频深度伪造检测中的解释性技术（如SHAP和LRP）具有挑战性。当能够获得地面真实时，这些方法难以提供准确的解释。

Method: 使用配对的真实音频和声码音频，将时频表示的差异作为地面真实解释。利用差异信号训练扩散模型，以暴露给定声码音频中的深度伪造伪影。

Result: 在VocV4和LibriSeVoc数据集上的实验结果表明，该方法在定性和定量上均优于传统解释性技术。

Conclusion: 提出的数据驱动方法能有效识别深度伪造音频中的伪影区域，为音频伪造检测提供了更可靠的解释性工具。

Abstract: Evaluating explainability techniques, such as SHAP and LRP, in the context of
audio deepfake detection is challenging due to lack of clear ground truth
annotations. In the cases when we are able to obtain the ground truth, we find
that these methods struggle to provide accurate explanations. In this work, we
propose a novel data-driven approach to identify artifact regions in deepfake
audio. We consider paired real and vocoded audio, and use the difference in
time-frequency representation as the ground-truth explanation. The difference
signal then serves as a supervision to train a diffusion model to expose the
deepfake artifacts in a given vocoded audio. Experimental results on the VocV4
and LibriSeVoc datasets demonstrate that our method outperforms traditional
explainability techniques, both qualitatively and quantitatively.

</details>


### [235] [Tone recognition in low-resource languages of North-East India: peeling the layers of SSL-based speech models](https://arxiv.org/abs/2506.03606)
*Parismita Gogoi,Sishir Kalita,Wendy Lalhminghlui,Viyazonuo Terhiija,Moakala Tzudir,Priyankoo Sarmah,S. R. M. Prasanna*

Main category: eess.AS

TL;DR: 该研究评估了自监督学习模型在三种印度东北部低资源语言（Angami、Ao、Mizo）中的声调识别效果，发现中间层对声调识别最关键，且语言本身的声调特性会影响识别效果。


<details>
  <summary>Details</summary>
Motivation: 探索自监督学习模型在低资源语言声调识别中的应用，特别是针对印度东北部的三种语言，以弥补这些语言数据不足的问题。

Method: 使用四种基于Wav2vec2.0的预训练模型（分别在声调和非声调语言上预训练），分析不同层在三种语言中的声调识别表现。

Result: Mizo的声调识别效果最佳，Angami最差；中间层对声调识别最为关键；声调库、声调类型和方言变异会影响识别效果。

Conclusion: 自监督学习模型在低资源语言声调识别中具有潜力，但需考虑语言本身的声调特性以优化效果。

Abstract: This study explores the use of self-supervised learning (SSL) models for tone
recognition in three low-resource languages from North Eastern India: Angami,
Ao, and Mizo. We evaluate four Wav2vec2.0 base models that were pre-trained on
both tonal and non-tonal languages. We analyze tone-wise performance across the
layers for all three languages and compare the different models. Our results
show that tone recognition works best for Mizo and worst for Angami. The middle
layers of the SSL models are the most important for tone recognition,
regardless of the pre-training language, i.e. tonal or non-tonal. We have also
found that the tone inventory, tone types, and dialectal variations affect tone
recognition. These findings provide useful insights into the strengths and
weaknesses of SSL-based embeddings for tonal languages and highlight the
potential for improving tone recognition in low-resource settings. The source
code is available at GitHub 1 .

</details>


### [236] [BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and Weight Indexing](https://arxiv.org/abs/2506.03515)
*Masaya Kawamura,Takuya Hasumi,Yuma Shirahata,Ryuichi Yamamoto*

Main category: eess.AS

TL;DR: 该论文提出了一种高度紧凑、轻量级的文本转语音模型，通过量化感知训练和权重索引技术，显著减小模型体积并保持合成质量。


<details>
  <summary>Details</summary>
Motivation: 为了在设备端应用中实现高效的文本转语音功能，需要减小模型体积，同时保持合成质量。

Method: 采用量化感知训练（QAT）将模型参数量化至1.58位，并提出权重索引方法，将一组1.58位权重存储为单个int8索引。

Result: 实验结果表明，该方法在模型体积减小83%的同时，合成质量优于未量化的基线模型。

Conclusion: 所提出的方法有效减小了模型体积，适用于设备端应用，并在合成质量上表现优异。

Abstract: This paper proposes a highly compact, lightweight text-to-speech (TTS) model
for on-device applications. To reduce the model size, the proposed model
introduces two techniques. First, we introduce quantization-aware training
(QAT), which quantizes model parameters during training to as low as 1.58-bit.
In this case, most of 32-bit model parameters are quantized to ternary values
{-1, 0, 1}. Second, we propose a method named weight indexing. In this method,
we save a group of 1.58-bit weights as a single int8 index. This allows for
efficient storage of model parameters, even on hardware that treats values in
units of 8-bit. Experimental results demonstrate that the proposed method
achieved 83 % reduction in model size, while outperforming the baseline of
similar model size without quantization in synthesis quality.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [237] [VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code Generation](https://arxiv.org/abs/2506.03930)
*Yuansheng Ni,Ping Nie,Kai Zou,Xiang Yue,Wenhu Chen*

Main category: cs.SE

TL;DR: 论文提出了VisCode-200K数据集，用于增强大语言模型在可视化任务中的代码生成和自修正能力，并通过微调模型VisCoder在基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在可视化任务（如绘制图表）中表现不佳，主要因为缺乏执行监督和迭代代码修正支持，导致生成的图表不可靠。

Method: 构建VisCode-200K数据集，包含20万条已验证的绘图代码和4.5万条多轮修正对话，用于微调Qwen2.5-Coder-Instruct模型。

Result: 微调后的VisCoder在PandasPlotBench上显著优于开源基线模型，并接近GPT-4o-mini等专有模型的性能。

Conclusion: 通过反馈驱动的学习和迭代修正，VisCoder能够生成更准确、可靠的可视化代码，提升了模型在可视化任务中的表现。

Abstract: Large language models (LLMs) often struggle with visualization tasks like
plotting diagrams, charts, where success depends on both code correctness and
visual semantics. Existing instruction-tuning datasets lack execution-grounded
supervision and offer limited support for iterative code correction, resulting
in fragile and unreliable plot generation. We present VisCode-200K, a
large-scale instruction tuning dataset for Python-based visualization and
self-correction. It contains over 200K examples from two sources: (1) validated
plotting code from open-source repositories, paired with natural language
instructions and rendered plots; and (2) 45K multi-turn correction dialogues
from Code-Feedback, enabling models to revise faulty code using runtime
feedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create
VisCoder, and evaluate it on PandasPlotBench. VisCoder significantly
outperforms strong open-source baselines and approaches the performance of
proprietary models like GPT-4o-mini. We further adopt a self-debug evaluation
protocol to assess iterative repair, demonstrating the benefits of
feedback-driven learning for executable, visually accurate code generation.

</details>


### [238] [CETBench: A Novel Dataset constructed via Transformations over Programs for Benchmarking LLMs for Code-Equivalence Checking](https://arxiv.org/abs/2506.04019)
*Neeva Oza,Ishaan Govil,Parul Gupta,Dinesh Khandelwal,Dinesh Garg,Parag Singla*

Main category: cs.SE

TL;DR: 该论文探讨了大型语言模型（LLM）在代码等价性检查任务中的表现，发现简单的代码转换会导致性能显著下降，并提出了一种微调方法来提升性能。


<details>
  <summary>Details</summary>
Motivation: 代码等价性检查是评估LLM在代码重写和代码翻译等任务中能力的重要问题，但目前相关研究较少。

Method: 作者提出了CETBench基准，通过随机应用预定义的代码转换生成（非）等价代码对，并采用微调方法来提升LLM在转换后代码对上的性能。

Result: 实验表明，简单的代码转换会导致LLM在代码等价性检查任务中的性能显著下降，而微调方法能有效提升性能。

Conclusion: LLM在代码等价性检查任务中仍缺乏对代码的语义理解，距离真正的语义理解还有较大差距。

Abstract: LLMs have been extensively used for the task of automated code generation. In
this work, we examine the applicability of LLMs for the related but relatively
unexplored task of code-equivalence checking, i.e., given two programs, whether
they are functionally equivalent or not. This is an important problem since
benchmarking code equivalence can play a critical role in evaluating LLM
capabilities for tasks such as code re-writing and code translation. Towards
this end, we present CETBench - Code Equivalence with Transformations
Benchmark, constructed via a repository of programs, where two programs in the
repository may be solving the same or different tasks. Each instance in our
dataset is obtained by taking a pair of programs in the repository and applying
a random series of pre-defined code transformations, resulting in
(non-)equivalent pairs. Our analysis on this dataset reveals a surprising
finding that very simple code transformations in the underlying pair of
programs can result in a significant drop in performance of SOTA LLMs for the
task of code-equivalence checking. To remedy this, we present a simple
fine-tuning-based approach to boost LLM performance on the transformed pairs of
programs. Our approach for dataset generation is generic, and can be used with
repositories with varying program difficulty levels and allows for applying
varying numbers as well as kinds of transformations. In our experiments, we
perform ablations over the difficulty level of original programs, as well as
the kind of transformations used in generating pairs for equivalence checking.
Our analysis presents deep insights into the working of LLMs for the task of
code-equivalence, and points to the fact that they may still be far from what
could be termed as a semantic understanding of the underlying code.

</details>


### [239] [Generating Automotive Code: Large Language Models for Software Development and Verification in Safety-Critical Systems](https://arxiv.org/abs/2506.04038)
*Sven Kirchner,Alois C. Knoll*

Main category: cs.SE

TL;DR: 本文提出了一种将生成式人工智能（GenAI）集成到汽车软件开发周期（SDLC）中的新框架，通过大型语言模型（LLM）自动生成代码，并结合静态验证、测试驱动开发等方法确保安全性。该框架在自适应巡航控制（ACC）系统开发中验证有效。


<details>
  <summary>Details</summary>
Motivation: 开发安全关键的汽车软件面临系统复杂性增加和严格法规要求的挑战，需要一种能够自动生成代码并确保安全性的方法。

Method: 提出一种集成GenAI的框架，利用LLM自动生成C++等语言的代码，结合静态验证、测试驱动开发和迭代优化，并通过测试、模拟和验证的反馈管道确保符合安全标准。

Result: 框架在ACC系统开发中验证成功，通过LLM的对比基准测试确保代码的准确性和可靠性，能够自动生成代码并满足安全关键要求。

Conclusion: 该研究推动了AI在安全关键领域的应用，弥合了先进生成模型与实际安全需求之间的差距。

Abstract: Developing safety-critical automotive software presents significant
challenges due to increasing system complexity and strict regulatory demands.
This paper proposes a novel framework integrating Generative Artificial
Intelligence (GenAI) into the Software Development Lifecycle (SDLC). The
framework uses Large Language Models (LLMs) to automate code generation in
languages such as C++, incorporating safety-focused practices such as static
verification, test-driven development and iterative refinement. A
feedback-driven pipeline ensures the integration of test, simulation and
verification for compliance with safety standards. The framework is validated
through the development of an Adaptive Cruise Control (ACC) system. Comparative
benchmarking of LLMs ensures optimal model selection for accuracy and
reliability. Results demonstrate that the framework enables automatic code
generation while ensuring compliance with safety-critical requirements,
systematically integrating GenAI into automotive software engineering. This
work advances the use of AI in safety-critical domains, bridging the gap
between state-of-the-art generative models and real-world safety requirements.

</details>


### [240] [From Theory to Practice: Real-World Use Cases on Trustworthy LLM-Driven Process Modeling, Prediction and Automation](https://arxiv.org/abs/2506.03801)
*Peter Pfeiffer,Alexander Rombach,Maxim Majlatow,Nijat Mehdiyev*

Main category: cs.SE

TL;DR: 本文探讨了大型语言模型（LLMs）如何结合可信流程智能，重新定义流程建模、预测和自动化，通过四个实际案例展示了其在制造业、建模、生命科学和设计领域的应用。


<details>
  <summary>Details</summary>
Motivation: 传统业务流程管理（BPM）在动态环境中存在僵化、不透明和可扩展性差的问题，而新兴的大型语言模型（LLMs）带来了变革机遇与风险。本文旨在探索如何通过LLMs与人类协作解决这些挑战。

Method: 基于与工业合作伙伴的早期研究项目，本文通过四个实际案例（制造业、流程建模、生命科学和设计领域）展示了LLMs的应用，并结合可信流程智能、交互式对话和多智能体系统等方法。

Result: 在制造业中，LLM驱动的框架将不确定性感知的可解释机器学习与交互式对话结合，将不透明的预测转化为可审计的工作流；在流程建模中，对话界面 democratize BPMN设计；药物安全监测通过知识图谱增强的LLMs实现自动化；可持续纺织品设计则通过多智能体系统权衡监管与环境需求。

Conclusion: 本文主张根据领域需求、利益相关者价值和迭代式人机协作工作流，优先考虑上下文敏感的集成，而非通用解决方案，为研究者和实践者提供了在关键BPM环境中操作LLMs的可操作见解。

Abstract: Traditional Business Process Management (BPM) struggles with rigidity,
opacity, and scalability in dynamic environments while emerging Large Language
Models (LLMs) present transformative opportunities alongside risks. This paper
explores four real-world use cases that demonstrate how LLMs, augmented with
trustworthy process intelligence, redefine process modeling, prediction, and
automation. Grounded in early-stage research projects with industrial partners,
the work spans manufacturing, modeling, life-science, and design processes,
addressing domain-specific challenges through human-AI collaboration. In
manufacturing, an LLM-driven framework integrates uncertainty-aware explainable
Machine Learning (ML) with interactive dialogues, transforming opaque
predictions into auditable workflows. For process modeling, conversational
interfaces democratize BPMN design. Pharmacovigilance agents automate drug
safety monitoring via knowledge-graph-augmented LLMs. Finally, sustainable
textile design employs multi-agent systems to navigate regulatory and
environmental trade-offs. We intend to examine tensions between transparency
and efficiency, generalization and specialization, and human agency versus
automation. By mapping these trade-offs, we advocate for context-sensitive
integration prioritizing domain needs, stakeholder values, and iterative
human-in-the-loop workflows over universal solutions. This work provides
actionable insights for researchers and practitioners aiming to operationalize
LLMs in critical BPM environments.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [241] [Towards Better Disentanglement in Non-Autoregressive Zero-Shot Expressive Voice Conversion](https://arxiv.org/abs/2506.04013)
*Seymanur Akti,Tuan Nam Nguyen,Alexander Waibel*

Main category: cs.SD

TL;DR: 本文提出了一种改进的自监督非自回归语音转换框架，通过增强语言-声学解耦和减少源音色泄漏，实现了更优的风格迁移效果。


<details>
  <summary>Details</summary>
Motivation: 为了解决语音转换中源音色泄漏和风格迁移不充分的问题，作者旨在提升模型在保留目标说话人身份和情感属性方面的表现。

Method: 采用多语言离散语音单元表示内容，结合增强相似性损失和混合风格层归一化；引入局部F0信息并通过跨注意力机制增强风格嵌入。

Result: 实验表明，该模型在情感和说话人相似度上优于基线，展现出更优的风格适应能力并减少了源风格泄漏。

Conclusion: 所提出的方法有效提升了语音转换中风格迁移的质量，同时降低了源音色的干扰。

Abstract: Expressive voice conversion aims to transfer both speaker identity and
expressive attributes from a target speech to a given source speech. In this
work, we improve over a self-supervised, non-autoregressive framework with a
conditional variational autoencoder, focusing on reducing source timbre leakage
and improving linguistic-acoustic disentanglement for better style transfer. To
minimize style leakage, we use multilingual discrete speech units for content
representation and reinforce embeddings with augmentation-based similarity loss
and mix-style layer normalization. To enhance expressivity transfer, we
incorporate local F0 information via cross-attention and extract style
embeddings enriched with global pitch and energy features. Experiments show our
model outperforms baselines in emotion and speaker similarity, demonstrating
superior style adaptation and reduced source style leakage.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [242] [Grounded Vision-Language Interpreter for Integrated Task and Motion Planning](https://arxiv.org/abs/2506.03270)
*Jeremy Siburian,Keisuke Shirai,Cristian C. Beltran-Hernandez,Masashi Hamaya,Michael Görner,Atsushi Hashimoto*

Main category: cs.RO

TL;DR: ViLaIn-TAMP是一个结合视觉语言模型和符号规划的混合框架，旨在提升机器人任务规划的可验证性和可解释性，并在烹饪领域实验中显示成功率提升30%以上。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLMs）虽能指导机器人规划，但缺乏安全性和可解释性；而传统符号规划虽严谨却依赖专家知识。本文旨在结合两者优势，构建一个可验证、可解释的自主机器人行为规划框架。

Method: 提出ViLaIn-TAMP框架，包含三个模块：(1) ViLaIn（视觉语言解释器）将多模态输入转为结构化问题描述；(2) 模块化任务与运动规划（TAMP）系统通过符号和几何约束生成可执行轨迹；(3) 校正规划模块根据失败反馈优化逻辑和几何约束。

Result: 在烹饪领域多任务测试中，带校正规划的ViLaIn-TAMP比无校正版本平均成功率提高30%以上。

Conclusion: ViLaIn-TAMP通过闭环校正机制有效结合了VLMs的灵活性与符号规划的严谨性，显著提升了复杂操作任务的成功率。

Abstract: While recent advances in vision-language models (VLMs) have accelerated the
development of language-guided robot planners, their black-box nature often
lacks safety guarantees and interpretability crucial for real-world deployment.
Conversely, classical symbolic planners offer rigorous safety verification but
require significant expert knowledge for setup. To bridge the current gap, this
paper proposes ViLaIn-TAMP, a hybrid planning framework for enabling
verifiable, interpretable, and autonomous robot behaviors. ViLaIn-TAMP
comprises three main components: (1) ViLaIn (Vision-Language Interpreter) - A
prior framework that converts multimodal inputs into structured problem
specifications using off-the-shelf VLMs without additional domain-specific
training, (2) a modular Task and Motion Planning (TAMP) system that grounds
these specifications in actionable trajectory sequences through symbolic and
geometric constraint reasoning and can utilize learning-based skills for key
manipulation phases, and (3) a corrective planning module which receives
concrete feedback on failed solution attempts from the motion and task planning
components and can feed adapted logic and geometric feasibility constraints
back to ViLaIn to improve and further refine the specification. We evaluate our
framework on several challenging manipulation tasks in a cooking domain. We
demonstrate that the proposed closed-loop corrective architecture exhibits a
more than 30% higher mean success rate for ViLaIn-TAMP compared to without
corrective planning.

</details>


### [243] [Adversarial Attacks on Robotic Vision Language Action Models](https://arxiv.org/abs/2506.03350)
*Eliot Krzysztof Jones,Alexander Robey,Andy Zou,Zachary Ravichandran,George J. Pappas,Hamed Hassani,Matt Fredrikson,J. Zico Kolter*

Main category: cs.RO

TL;DR: 该论文研究了视觉-语言-动作模型（VLA）在机器人控制中的对抗攻击问题，发现文本攻击可完全操控VLA动作空间且攻击效果持久。


<details>
  <summary>Details</summary>
Motivation: 由于VLA模型基于大型语言模型（LLM）构建，而LLM易受对抗性滥用影响，且机器人领域存在物理安全风险，作者希望探究VLA是否继承这些漏洞。

Method: 作者将LLM越狱攻击方法适配并应用于VLA控制的机器人，通过文本攻击实现对其动作空间的完全控制。

Result: 研究发现，在VLA模型上应用的文本攻击可实现动作空间的完全覆盖，且攻击效果在长时间范围内持续有效，这与传统LLM越狱攻击的语义关联性要求不同。

Conclusion: VLA模型存在严重安全漏洞，其对抗攻击特性与LLM有显著差异，需引起重视。作者开源了所有代码以促进相关研究。

Abstract: The emergence of vision-language-action models (VLAs) for end-to-end control
is reshaping the field of robotics by enabling the fusion of multimodal sensory
inputs at the billion-parameter scale. The capabilities of VLAs stem primarily
from their architectures, which are often based on frontier large language
models (LLMs). However, LLMs are known to be susceptible to adversarial misuse,
and given the significant physical risks inherent to robotics, questions remain
regarding the extent to which VLAs inherit these vulnerabilities. Motivated by
these concerns, in this work we initiate the study of adversarial attacks on
VLA-controlled robots. Our main algorithmic contribution is the adaptation and
application of LLM jailbreaking attacks to obtain complete control authority
over VLAs. We find that textual attacks, which are applied once at the
beginning of a rollout, facilitate full reachability of the action space of
commonly used VLAs and often persist over longer horizons. This differs
significantly from LLM jailbreaking literature, as attacks in the real world do
not have to be semantically linked to notions of harm. We make all code
available at https://github.com/eliotjones1/robogcg .

</details>


### [244] [SemNav: A Model-Based Planner for Zero-Shot Object Goal Navigation Using Vision-Foundation Models](https://arxiv.org/abs/2506.03516)
*Arnab Debnath,Gregory J. Stein,Jana Kosecka*

Main category: cs.RO

TL;DR: 提出了一种零样本目标导航框架，结合视觉基础模型的感知能力和基于模型的规划器，在未知环境中高效定位目标物体。


<details>
  <summary>Details</summary>
Motivation: 传统基于学习的方法依赖大量标注数据或强化学习交互，难以泛化到新环境且扩展性受限。本文探索零样本设置，利用视觉基础模型的强大理解能力，实现更灵活、可扩展的解决方案。

Method: 整合视觉基础模型（VFMs）的感知能力与基于模型的规划器，通过边界探索实现长时程决策，构建零样本目标导航框架。

Result: 在HM3D数据集和Habitat模拟器上的实验表明，该方法在零样本目标导航任务中取得了最先进的性能（以路径长度加权的成功率衡量）。

Conclusion: 该框架无需任务特定训练即可高效导航，验证了视觉基础模型与模型规划器结合在具身AI任务中的潜力。

Abstract: Object goal navigation is a fundamental task in embodied AI, where an agent
is instructed to locate a target object in an unexplored environment.
Traditional learning-based methods rely heavily on large-scale annotated data
or require extensive interaction with the environment in a reinforcement
learning setting, often failing to generalize to novel environments and
limiting scalability. To overcome these challenges, we explore a zero-shot
setting where the agent operates without task-specific training, enabling more
scalable and adaptable solution. Recent advances in Vision Foundation Models
(VFMs) offer powerful capabilities for visual understanding and reasoning,
making them ideal for agents to comprehend scenes, identify relevant regions,
and infer the likely locations of objects. In this work, we present a zero-shot
object goal navigation framework that integrates the perceptual strength of
VFMs with a model-based planner that is capable of long-horizon decision making
through frontier exploration. We evaluate our approach on the HM3D dataset
using the Habitat simulator and demonstrate that our method achieves
state-of-the-art performance in terms of success weighted by path length for
zero-shot object goal navigation.

</details>


### [245] [From Virtual Agents to Robot Teams: A Multi-Robot Framework Evaluation in High-Stakes Healthcare Context](https://arxiv.org/abs/2506.03546)
*Yuanchen Bai,Zijian Ding,Angelique Taylor*

Main category: cs.RO

TL;DR: 论文探讨了生成模型在多智能体机器人系统（MARS）中的应用挑战，提出了改进设计指南。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统在虚拟任务中表现良好，但在物理机器人团队中因忽视实际约束（如空间背景和机器人能力）而表现不佳。

Method: 通过在模拟急诊科场景中测试基于CrewAI框架的分层多智能体机器人团队，识别了五种常见故障模式。

Result: 发现了角色错位、工具访问违规等五种故障模式，并提出了强调过程透明、主动故障恢复和上下文基础的设计指南。

Conclusion: 研究为开发更健壮的多智能体机器人系统提供了方向，包括将虚拟框架扩展到现实世界的可能性。

Abstract: Advancements in generative models have enabled multi-agent systems (MAS) to
perform complex virtual tasks such as writing and code generation, which do not
generalize well to physical multi-agent robotic teams. Current frameworks often
treat agents as conceptual task executors rather than physically embodied
entities, and overlook critical real-world constraints such as spatial context,
robotic capabilities (e.g., sensing and navigation). To probe this gap, we
reconfigure and stress-test a hierarchical multi-agent robotic team built on
the CrewAI framework in a simulated emergency department onboarding scenario.
We identify five persistent failure modes: role misalignment; tool access
violations; lack of in-time handling of failure reports; noncompliance with
prescribed workflows; bypassing or false reporting of task completion. Based on
this analysis, we propose three design guidelines emphasizing process
transparency, proactive failure recovery, and contextual grounding. Our work
informs the development of more resilient and robust multi-agent robotic
systems (MARS), including opportunities to extend virtual multi-agent
frameworks to the real world.

</details>


### [246] [Confidence-Guided Human-AI Collaboration: Reinforcement Learning with Distributional Proxy Value Propagation for Autonomous Driving](https://arxiv.org/abs/2506.03568)
*Li Zeqiao,Wang Yijing,Wang Haoyu,Li Zheng,Li Peng,Zuo zhiqiang,Hu Chuan*

Main category: cs.RO

TL;DR: 本文提出了一种基于置信度引导的人机协作策略（C-HAC），通过结合人类指导和自主学习，解决了自动驾驶中强化学习和模仿学习的安全探索和分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶在提升移动性、道路安全和交通效率方面具有巨大潜力，但强化学习和模仿学习面临安全探索和分布偏移的挑战。现有的人机协作方法依赖大量人工干预，成本高且效率低。

Method: C-HAC在分布软行动者-评论家（DSAC）框架中采用分布代理值传播方法，利用回报分布表示人类意图，实现快速稳定的学习。通过共享控制机制和置信度评估算法，动态切换人类指导和自主学习策略。

Result: 在多种驾驶场景下的实验表明，C-HAC在安全性、效率和整体性能上显著优于传统方法，并在复杂交通条件下的实际道路测试中验证了其有效性。

Conclusion: C-HAC通过结合人类指导和自主学习，实现了安全高效的自动驾驶策略，为自动驾驶领域提供了新的解决方案。

Abstract: Autonomous driving promises significant advancements in mobility, road safety
and traffic efficiency, yet reinforcement learning and imitation learning face
safe-exploration and distribution-shift challenges. Although human-AI
collaboration alleviates these issues, it often relies heavily on extensive
human intervention, which increases costs and reduces efficiency. This paper
develops a confidence-guided human-AI collaboration (C-HAC) strategy to
overcome these limitations. First, C-HAC employs a distributional proxy value
propagation method within the distributional soft actor-critic (DSAC)
framework. By leveraging return distributions to represent human intentions
C-HAC achieves rapid and stable learning of human-guided policies with minimal
human interaction. Subsequently, a shared control mechanism is activated to
integrate the learned human-guided policy with a self-learning policy that
maximizes cumulative rewards. This enables the agent to explore independently
and continuously enhance its performance beyond human guidance. Finally, a
policy confidence evaluation algorithm capitalizes on DSAC's return
distribution networks to facilitate dynamic switching between human-guided and
self-learning policies via a confidence-based intervention function. This
ensures the agent can pursue optimal policies while maintaining safety and
performance guarantees. Extensive experiments across diverse driving scenarios
reveal that C-HAC significantly outperforms conventional methods in terms of
safety, efficiency, and overall performance, achieving state-of-the-art
results. The effectiveness of the proposed method is further validated through
real-world road tests in complex traffic conditions. The videos and code are
available at: https://github.com/lzqw/C-HAC.

</details>


### [247] [SLAC: Simulation-Pretrained Latent Action Space for Whole-Body Real-World RL](https://arxiv.org/abs/2506.04147)
*Jiaheng Hu,Peter Stone,Roberto Martín-Martín*

Main category: cs.RO

TL;DR: SLAC方法利用低精度模拟器预训练任务无关的潜在动作空间，通过无监督技能发现促进时间抽象、解耦和安全性，使复杂机器人在真实世界中高效学习成为可能。


<details>
  <summary>Details</summary>
Motivation: 高自由度机器人系统的控制学习面临现实世界RL样本效率低、仿真到现实迁移脆弱性的挑战，需要一种能安全探索且高效的方法。

Method: SLAC结合低精度模拟器预训练潜在动作空间，并设计新型离线策略RL算法，在真实交互中学习下游任务。

Result: 在双臂移动操作任务上取得SOTA性能，仅用1小时真实交互即学会接触密集型全身任务，且无需演示数据。

Conclusion: SLAC为复杂机器人的真实世界RL提供可行解决方案，显著提升学习效率与安全性。

Abstract: Building capable household and industrial robots requires mastering the
control of versatile, high-degree-of-freedom (DoF) systems such as mobile
manipulators. While reinforcement learning (RL) holds promise for autonomously
acquiring robot control policies, scaling it to high-DoF embodiments remains
challenging. Direct RL in the real world demands both safe exploration and high
sample efficiency, which are difficult to achieve in practice. Sim-to-real RL,
on the other hand, is often brittle due to the reality gap. This paper
introduces SLAC, a method that renders real-world RL feasible for complex
embodiments by leveraging a low-fidelity simulator to pretrain a task-agnostic
latent action space. SLAC trains this latent action space via a customized
unsupervised skill discovery method designed to promote temporal abstraction,
disentanglement, and safety, thereby facilitating efficient downstream
learning. Once a latent action space is learned, SLAC uses it as the action
interface for a novel off-policy RL algorithm to autonomously learn downstream
tasks through real-world interactions. We evaluate SLAC against existing
methods on a suite of bimanual mobile manipulation tasks, where it achieves
state-of-the-art performance. Notably, SLAC learns contact-rich whole-body
tasks in under an hour of real-world interactions, without relying on any
demonstrations or hand-crafted behavior priors. More information, code, and
videos at robo-rl.github.io

</details>


### [248] [OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis](https://arxiv.org/abs/2506.04217)
*Junting Chen,Haotian Liang,Lingxiao Du,Weiyun Wang,Mengkang Hu,Yao Mu,Wenhai Wang,Jifeng Dai,Ping Luo,Wenqi Shao,Lin Shao*

Main category: cs.RO

TL;DR: 本文提出了一种新型多模态代理架构和代理数据合成流程，用于解决开放世界移动操作任务的复杂性和领域转移问题，其微调的OWMM-VLM模型在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 开放世界移动操作任务（OWMM）由于需要泛化到开放指令和环境，并整合高层决策与低层机器人控制，仍具挑战性。

Method: 提出多模态代理架构维护多视角场景帧和代理状态进行决策，并通过函数调用控制机器人；引入代理数据合成流程微调VLM模型以适应任务领域。

Result: 实验表明，该模型在真实世界中表现出色，优于包括GPT-4o在内的其他基础模型，并具有强大的零样本泛化能力。

Conclusion: 本文提出的OWMM-VLM是首个专为移动操作器设计的基础模型，统一了全局场景理解、机器人状态跟踪和多模态动作生成。

Abstract: The rapid progress of navigation, manipulation, and vision models has made
mobile manipulators capable in many specialized tasks. However, the open-world
mobile manipulation (OWMM) task remains a challenge due to the need for
generalization to open-ended instructions and environments, as well as the
systematic complexity to integrate high-level decision making with low-level
robot control based on both global scene understanding and current agent state.
To address this complexity, we propose a novel multi-modal agent architecture
that maintains multi-view scene frames and agent states for decision-making and
controls the robot by function calling. A second challenge is the hallucination
from domain shift. To enhance the agent performance, we further introduce an
agentic data synthesis pipeline for the OWMM task to adapt the VLM model to our
task domain with instruction fine-tuning. We highlight our fine-tuned OWMM-VLM
as the first dedicated foundation model for mobile manipulators with global
scene understanding, robot state tracking, and multi-modal action generation in
a unified model. Through experiments, we demonstrate that our model achieves
SOTA performance compared to other foundation models including GPT-4o and
strong zero-shot generalization in real world. The project page is at
https://github.com/HHYHRHY/OWMM-Agent

</details>


### [249] [Pseudo-Simulation for Autonomous Driving](https://arxiv.org/abs/2506.04218)
*Wei Cao,Marcel Hallgarten,Tianyu Li,Daniel Dauner,Xunjiang Gu,Caojun Wang,Yakov Miron,Marco Aiello,Hongyang Li,Igor Gilitschenski,Boris Ivanovic,Marco Pavone,Andreas Geiger,Kashyap Chitta*

Main category: cs.RO

TL;DR: 本文提出了一种名为伪仿真的新方法，用于解决自动驾驶车辆评估中的现实性和效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有的自动驾驶车辆评估方法存在局限性：现实评估因安全性和可重复性问题难以实施，闭环仿真则面临真实感不足或计算成本高的问题。开环评估虽然高效，但忽视了复合误差。

Method: 伪仿真结合了真实数据集和合成观测，利用3D高斯泼溅生成多样化的未来状态近似，并通过基于邻近性的加权方案赋予更可能的行为更高的重要性。

Result: 伪仿真与闭环仿真的相关性（R²=0.8）优于现有最佳开环方法（R²=0.7），并建立了公共排行榜供社区使用。

Conclusion: 伪仿真提供了一种高效且接近现实的评估方法，解决了现有评估范式的关键限制。

Abstract: Existing evaluation paradigms for Autonomous Vehicles (AVs) face critical
limitations. Real-world evaluation is often challenging due to safety concerns
and a lack of reproducibility, whereas closed-loop simulation can face
insufficient realism or high computational costs. Open-loop evaluation, while
being efficient and data-driven, relies on metrics that generally overlook
compounding errors. In this paper, we propose pseudo-simulation, a novel
paradigm that addresses these limitations. Pseudo-simulation operates on real
datasets, similar to open-loop evaluation, but augments them with synthetic
observations generated prior to evaluation using 3D Gaussian Splatting. Our key
idea is to approximate potential future states the AV might encounter by
generating a diverse set of observations that vary in position, heading, and
speed. Our method then assigns a higher importance to synthetic observations
that best match the AV's likely behavior using a novel proximity-based
weighting scheme. This enables evaluating error recovery and the mitigation of
causal confusion, as in closed-loop benchmarks, without requiring sequential
interactive simulation. We show that pseudo-simulation is better correlated
with closed-loop simulations (R^2=0.8) than the best existing open-loop
approach (R^2=0.7). We also establish a public leaderboard for the community to
benchmark new methodologies with pseudo-simulation. Our code is available at
https://github.com/autonomousvision/navsim.

</details>


### [250] [STAR: Learning Diverse Robot Skill Abstractions through Rotation-Augmented Vector Quantization](https://arxiv.org/abs/2506.03863)
*Hao Li,Qi Lv,Rui Shao,Xiang Deng,Yinchuan Li,Jianye Hao,Liqiang Nie*

Main category: cs.RO

TL;DR: STAR框架通过旋转增强残差技能量化（RaRSQ）和因果技能变换器（CST）解决了技能抽象中的代码本崩溃和因果关系建模问题，显著提升了机器人操作的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖潜在变量模型（如VQ-VAE）学习技能抽象，但存在代码本崩溃和技能间因果关系建模不足的问题。

Method: 提出STAR框架，包括旋转增强残差技能量化（RaRSQ）防止代码本崩溃，以及因果技能变换器（CST）显式建模技能间依赖关系。

Result: 在LIBERO基准和真实任务中，STAR比基线方法提升了约12%的性能。

Conclusion: STAR框架在技能学习和组合方面取得了显著进展，为复杂行为完成提供了有效解决方案。

Abstract: Transforming complex actions into discrete skill abstractions has
demonstrated strong potential for robotic manipulation. Existing approaches
mainly leverage latent variable models, e.g., VQ-VAE, to learn skill
abstractions through learned vectors (codebooks), while they suffer from
codebook collapse and modeling the causal relationship between learned skills.
To address these limitations, we present \textbf{S}kill \textbf{T}raining with
\textbf{A}ugmented \textbf{R}otation (\textbf{STAR}), a framework that advances
both skill learning and composition to complete complex behaviors.
Specifically, to prevent codebook collapse, we devise rotation-augmented
residual skill quantization (RaRSQ). It encodes relative angles between encoder
outputs into the gradient flow by rotation-based gradient mechanism. Points
within the same skill code are forced to be either pushed apart or pulled
closer together depending on gradient directions. Further, to capture the
causal relationship between skills, we present causal skill transformer (CST)
which explicitly models dependencies between skill representations through an
autoregressive mechanism for coherent action generation. Extensive experiments
demonstrate the superiority of STAR on both LIBERO benchmark and realworld
tasks, with around 12\% improvement over the baselines.

</details>


### [251] [Object-centric 3D Motion Field for Robot Learning from Human Videos](https://arxiv.org/abs/2506.04227)
*Zhao-Heng Yin,Sherry Yang,Pieter Abbeel*

Main category: cs.RO

TL;DR: 该论文提出了一种基于物体中心3D运动场的方法，用于从人类视频中学习机器人控制策略，显著提升了动作估计的准确性和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有动作表示方法（如视频帧、像素流等）存在建模复杂或信息丢失的问题，限制了从人类视频中学习机器人策略的效果。

Method: 提出了一种新框架，包含两个核心组件：1) 去噪3D运动场估计器训练流程；2) 密集物体中心3D运动场预测架构。

Result: 实验表明，该方法将3D运动估计误差降低50%以上，在多样化任务中达到55%平均成功率（先前方法<10%），并能实现精细操作技能。

Conclusion: 物体中心3D运动场是一种有效的动作表示方法，能显著提升从人类视频学习机器人策略的性能。

Abstract: Learning robot control policies from human videos is a promising direction
for scaling up robot learning. However, how to extract action knowledge (or
action representations) from videos for policy learning remains a key
challenge. Existing action representations such as video frames, pixelflow, and
pointcloud flow have inherent limitations such as modeling complexity or loss
of information. In this paper, we propose to use object-centric 3D motion field
to represent actions for robot learning from human videos, and present a novel
framework for extracting this representation from videos for zero-shot control.
We introduce two novel components in its implementation. First, a novel
training pipeline for training a ''denoising'' 3D motion field estimator to
extract fine object 3D motions from human videos with noisy depth robustly.
Second, a dense object-centric 3D motion field prediction architecture that
favors both cross-embodiment transfer and policy generalization to background.
We evaluate the system in real world setups. Experiments show that our method
reduces 3D motion estimation error by over 50% compared to the latest method,
achieve 55% average success rate in diverse tasks where prior approaches
fail~($\lesssim 10$\%), and can even acquire fine-grained manipulation skills
like insertion.

</details>


### [252] [Autonomous Vehicle Lateral Control Using Deep Reinforcement Learning with MPC-PID Demonstration](https://arxiv.org/abs/2506.04040)
*Chengdong Wu,Sven Kirchner,Nils Purschke,Alois C. Knoll*

Main category: cs.RO

TL;DR: 该论文提出了一种基于强化学习的横向控制方法，结合MPC-PID和深度强化学习（DRL），在车辆模型不完善的情况下实现舒适、高效和鲁棒的控制性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶管道中的控制器模块至关重要，但车辆模型常因测量误差和简化而不完善。本文旨在通过强化学习方法提升控制性能，同时降低开发和集成难度。

Method: 控制器由传统的MPC-PID部分作为基础和演示器，以及DRL部分利用MPC-PID的在线信息组成。在CARLA仿真环境中使用路径点真值作为输入进行评估。

Result: 实验结果表明，该控制器在车辆信息不完整时仍能有效工作，且DRL训练可通过演示部分稳定。

Conclusion: 该方法展示了减少自动驾驶管道开发和集成努力的潜力，为未来研究提供了方向。

Abstract: The controller is one of the most important modules in the autonomous driving
pipeline, ensuring the vehicle reaches its desired position. In this work, a
reinforcement learning based lateral control approach, despite the
imperfections in the vehicle models due to measurement errors and
simplifications, is presented. Our approach ensures comfortable, efficient, and
robust control performance considering the interface between controlling and
other modules. The controller consists of the conventional Model Predictive
Control (MPC)-PID part as the basis and the demonstrator, and the Deep
Reinforcement Learning (DRL) part which leverages the online information from
the MPC-PID part. The controller's performance is evaluated in CARLA using the
ground truth of the waypoints as inputs. Experimental results demonstrate the
effectiveness of the controller when vehicle information is incomplete, and the
training of DRL can be stabilized with the demonstration part. These findings
highlight the potential to reduce development and integration efforts for
autonomous driving pipelines in the future.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [253] [POLARIS: A High-contrast Polarimetric Imaging Benchmark Dataset for Exoplanetary Disk Representation Learning](https://arxiv.org/abs/2506.03511)
*Fangyi Cao,Bin Ren,Zihao Wang,Shiwei Fu,Youbin Mo,Xiaoyang Liu,Yuzhou Chen,Weixin Yao*

Main category: astro-ph.EP

TL;DR: 该论文探讨了AI在直接成像类地系外行星中的潜力，提出了一个名为POLARIS的数据集，并评估了多种模型，最终提出了一个无监督生成表示学习框架，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 尽管过去十年在系外行星直接成像方面投入了大量资源，但成果有限，现有方法依赖人工标注参考星，效率低下。论文旨在通过AI技术提升这一领域的效率。

Method: 论文利用SPHERE/IRDIS偏振光档案数据构建了POLARIS数据集，评估了统计、生成和大型视觉语言模型，并提出了一种无监督生成表示学习框架。

Result: 提出的无监督生成表示学习框架在性能上优于其他模型，并增强了表示能力。POLARIS数据集是首个统一处理的高质量系外行星成像数据集。

Conclusion: 通过发布POLARIS数据集和基线模型，论文为天体物理学家提供了新工具，并促进了数据科学家在系外行星直接成像领域的跨学科突破。

Abstract: With over 1,000,000 images from more than 10,000 exposures using
state-of-the-art high-contrast imagers (e.g., Gemini Planet Imager, VLT/SPHERE)
in the search for exoplanets, can artificial intelligence (AI) serve as a
transformative tool in imaging Earth-like exoplanets in the coming decade? In
this paper, we introduce a benchmark and explore this question from a
polarimetric image representation learning perspective. Despite extensive
investments over the past decade, only a few new exoplanets have been directly
imaged. Existing imaging approaches rely heavily on labor-intensive labeling of
reference stars, which serve as background to extract circumstellar objects
(disks or exoplanets) around target stars. With our POLARIS (POlarized Light
dAta for total intensity Representation learning of direct Imaging of
exoplanetary Systems) dataset, we classify reference star and circumstellar
disk images using the full public SPHERE/IRDIS polarized-light archive since
2014, requiring less than 10 percent manual labeling. We evaluate a range of
models including statistical, generative, and large vision-language models and
provide baseline performance. We also propose an unsupervised generative
representation learning framework that integrates these models, achieving
superior performance and enhanced representational power. To our knowledge,
this is the first uniformly reduced, high-quality exoplanet imaging dataset,
rare in astrophysics and machine learning. By releasing this dataset and
baselines, we aim to equip astrophysicists with new tools and engage data
scientists in advancing direct exoplanet imaging, catalyzing major
interdisciplinary breakthroughs.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [254] [NetPress: Dynamically Generated LLM Benchmarks for Network Applications](https://arxiv.org/abs/2506.03231)
*Yajie Zhou,Jiajun Ruan,Eric S. Wang,Sadjad Fouladi,Francis Y. Yan,Kevin Hsieh,Zaoxing Liu*

Main category: cs.NI

TL;DR: NetPress是一个自动化基准测试生成框架，用于评估网络应用中的LLM代理，支持动态生成多样化查询和真实环境反馈。


<details>
  <summary>Details</summary>
Motivation: 当前针对大型语言模型（LLMs）和代理的领域特定基准测试主要局限于静态、小规模数据集，特别是在需要高可靠性的网络操作等高风险任务中。

Method: NetPress引入了统一的状态和动作抽象，支持动态生成多样化查询集及相应真实答案，并与网络模拟器集成以提供真实环境反馈。

Result: NetPress在三个代表性应用中实例化，揭示了静态基准测试常忽略的代理行为细粒度差异。

Conclusion: NetPress推动了LLM评估向现实、可扩展的测试方向发展，有助于缩小基准测试性能与实际部署准备之间的差距。

Abstract: Despite growing interest in domain-specific benchmarking of large language
models (LLMs) and agents, current evaluations remain limited to static,
small-scale datasets, especially in high-stakes tasks like network operations
that demand reliability for deployments. We present NetPress, an automated
benchmark generation framework for evaluating LLM agents in network
applications. NetPress introduces a unified abstraction with state and action,
enabling dynamic generation of diverse query sets along with corresponding
ground truths. At runtime, users can specify benchmark configurations to
generate millions of queries on the fly. In addition to dynamic benchmark
construction, NetPress integrates with network emulators to provide realistic
environment feedback, supporting comprehensive evaluation across correctness,
safety, and latency. We instantiate NetPress on three representative
applications, revealing interesting fine-grained differences in agent behavior
that static, correctness-only benchmarks often miss. NetPress moves LLM
evaluation toward realistic, scalable testing in infrastructure-centric
domains, helping close the gap between benchmark performance and real-world
deployment readiness. Code is available at
https://github.com/Froot-NetSys/NetPress.

</details>


### [255] [Distributionally Robust Wireless Semantic Communication with Large AI Models](https://arxiv.org/abs/2506.03167)
*Long Tan Le,Senura Hansaja Wanasekara,Zerun Niu,Yansong Shi,Nguyen H. Tran,Phuong Vo,Walid Saad,Dusit Niyato,Zhu Han,Choong Seon Hong,H. Vincent Poor*

Main category: cs.NI

TL;DR: 本文提出了一种名为WaSeCom的新型通用语义通信框架，通过Wasserstein分布鲁棒优化增强对语义误解和信道扰动的鲁棒性，实验证明其在噪声和对抗扰动下具有更好的性能。


<details>
  <summary>Details</summary>
Motivation: 6G无线系统需要支持海量数据和超低延迟，传统比特级传输策略无法满足现代数据密集型应用的效率和适应性需求。现有语义通信系统因依赖领域特定架构而缺乏泛化能力，且易受语义级和传输级噪声影响。

Method: 提出WaSeCom框架，采用Wasserstein分布鲁棒优化技术，系统性地解决不确定性并增强鲁棒性。通过理论分析验证其鲁棒泛化保证。

Result: 在图像和文本传输实验中，WaSeCom在噪声和对抗扰动下表现出更高的鲁棒性，有效保持了不同无线条件下的语义保真度。

Conclusion: WaSeCom框架通过分布鲁棒优化显著提升了语义通信的鲁棒性和泛化能力，为6G时代的语义通信提供了有效解决方案。

Abstract: 6G wireless systems are expected to support massive volumes of data with
ultra-low latency. However, conventional bit-level transmission strategies
cannot support the efficiency and adaptability required by modern,
data-intensive applications. The concept of semantic communication (SemCom)
addresses this limitation by focusing on transmitting task-relevant semantic
information instead of raw data. While recent efforts incorporating deep
learning and large-scale AI models have improved SemCom's performance, existing
systems remain vulnerable to both semantic-level and transmission-level noise
because they often rely on domain-specific architectures that hinder
generalizability. In this paper, a novel and generalized semantic communication
framework called WaSeCom is proposed to systematically address uncertainty and
enhance robustness. In particular, Wasserstein distributionally robust
optimization is employed to provide resilience against semantic
misinterpretation and channel perturbations. A rigorous theoretical analysis is
performed to establish the robust generalization guarantees of the proposed
framework. Experimental results on image and text transmission demonstrate that
WaSeCom achieves improved robustness under noise and adversarial perturbations.
These results highlight its effectiveness in preserving semantic fidelity
across varying wireless conditions.

</details>


### [256] [Graph Neural Networks for Jamming Source Localization](https://arxiv.org/abs/2506.03196)
*Dania Herzalla,Willian T. Lunardi,Martin Andreoni*

Main category: cs.NI

TL;DR: 该论文首次将基于图的学习应用于无线网络中的干扰源定位，提出了一种结合注意力机制图神经网络和置信度引导估计的新方法，显著提升了复杂环境下的定位性能。


<details>
  <summary>Details</summary>
Motivation: 传统几何优化方法在环境不确定性和密集干扰下表现不佳，而图学习在无线安全领域的潜力尚未充分探索。论文旨在解决无线网络中干扰攻击的紧迫威胁。

Method: 将定位问题重构为归纳图回归任务，整合了编码局部和全局信号聚合的结构化节点表示，采用注意力机制图神经网络自适应优化邻域影响，并引入置信度引导估计机制动态平衡预测与先验知识。

Result: 在复杂射频环境和不同采样密度条件下，该方法显著优于现有定位基准，尤其在信号信息稀疏和模糊的挑战性场景中表现突出。

Conclusion: 基于图学习的框架为干扰源定位提供了有效解决方案，其创新设计在恶劣环境中展现出优越性能，代码已开源供进一步研究。

Abstract: Graph-based learning has emerged as a transformative approach for modeling
complex relationships across diverse domains, yet its potential in wireless
security remains largely unexplored. In this work, we introduce the first
application of graph-based learning for jamming source localization, addressing
the imminent threat of jamming attacks in wireless networks. Unlike geometric
optimization techniques that struggle under environmental uncertainties and
dense interference, we reformulate localization as an inductive graph
regression task. Our approach integrates structured node representations that
encode local and global signal aggregation, ensuring spatial coherence and
adaptive signal fusion. To enhance robustness, we incorporate an
attention-based graph neural network that adaptively refines neighborhood
influence and introduces a confidence-guided estimation mechanism that
dynamically balances learned predictions with domain-informed priors. We
evaluate our approach under complex radio frequency environments with varying
sampling densities and signal propagation conditions, conducting comprehensive
ablation studies on graph construction, feature selection, and pooling
strategies. Results demonstrate that our novel graph-based learning framework
significantly outperforms established localization baselines, particularly in
challenging scenarios with sparse and obfuscated signal information. Code is
available at
[https://github.com/daniaherzalla/gnn-jamming-source-localization](https://github.com/daniaherzalla/gnn-jamming-source-localization).

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [257] [Thinking Beyond Visibility: A Near-Optimal Policy Framework for Locally Interdependent Multi-Agent MDPs](https://arxiv.org/abs/2506.04215)
*Alex DeWeese,Guannan Qu*

Main category: cs.MA

TL;DR: 论文提出了一种新的策略类别Extended Cutoff Policy Class，用于解决局部相互依赖多智能体MDP中的部分可观测性问题，显著提升了在小固定可见性下的性能，并解决了Penalty Jittering现象。


<details>
  <summary>Details</summary>
Motivation: Dec-POMDPs因其计算复杂性难以解决，但在局部可见性和局部依赖性的假设下，可以构建更易处理的模型。然而，现有方法在小固定可见性下表现不佳，存在Penalty Jittering问题。

Method: 提出了Extended Cutoff Policy Class，这是一种非平凡的、接近最优的闭式部分可观测策略类别，能够记住超出可见范围的智能体，从而提升性能。

Result: 新策略在小固定可见性设置下表现显著更好，解决了Penalty Jittering问题，并在某些情况下保证了完全可观测的联合最优行为。

Conclusion: Extended Cutoff Policy Class为局部相互依赖多智能体MDP提供了一种高效且接近最优的解决方案，扩展了模型的应用范围。

Abstract: Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) are
known to be NEXP-Complete and intractable to solve. However, for problems such
as cooperative navigation, obstacle avoidance, and formation control, basic
assumptions can be made about local visibility and local dependencies. The work
DeWeese and Qu 2024 formalized these assumptions in the construction of the
Locally Interdependent Multi-Agent MDP. In this setting, it establishes three
closed-form policies that are tractable to compute in various situations and
are exponentially close to optimal with respect to visibility. However, it is
also shown that these solutions can have poor performance when the visibility
is small and fixed, often getting stuck during simulations due to the so called
"Penalty Jittering" phenomenon. In this work, we establish the Extended Cutoff
Policy Class which is, to the best of our knowledge, the first non-trivial
class of near optimal closed-form partially observable policies that are
exponentially close to optimal with respect to the visibility for any Locally
Interdependent Multi-Agent MDP. These policies are able to remember agents
beyond their visibilities which allows them to perform significantly better in
many small and fixed visibility settings, resolve Penalty Jittering
occurrences, and under certain circumstances guarantee fully observable joint
optimal behavior despite the partial observability. We also propose a
generalized form of the Locally Interdependent Multi-Agent MDP that allows for
transition dependence and extended reward dependence, then replicate our
theoretical results in this setting.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [258] [What Makes Treatment Effects Identifiable? Characterizations and Estimators Beyond Unconfoundedness](https://arxiv.org/abs/2506.04194)
*Yang Cai,Alkis Kalavasis,Katerina Mamali,Anay Mehrotra,Manolis Zampetakis*

Main category: math.ST

TL;DR: 该论文提出了超越无混淆性和重叠性的一般条件，用于识别平均处理效应（ATE），并展示了这些条件在多种场景下的适用性。


<details>
  <summary>Details</summary>
Motivation: 现有的ATE估计方法大多依赖无混淆性和重叠性假设，但许多研究（如回归断点设计）会违反这些假设。论文旨在探索更一般的条件，以扩展ATE的识别范围。

Method: 基于统计学习理论，论文提出了一个可解释的、近乎充分且必要的条件，用于识别ATE，并应用于多种经典模型（如Tan 2006、Rosenbaum 2002等）。

Result: 论文证明了在多种场景下（包括回归断点设计），新条件可以识别ATE，且在某些附加假设下，ATE可以从有限样本中估计。

Conclusion: 这些发现为连接学习理论和因果推断方法开辟了新途径，尤其适用于具有复杂处理机制的观察性研究。

Abstract: Most of the widely used estimators of the average treatment effect (ATE) in
causal inference rely on the assumptions of unconfoundedness and overlap.
Unconfoundedness requires that the observed covariates account for all
correlations between the outcome and treatment. Overlap requires the existence
of randomness in treatment decisions for all individuals. Nevertheless, many
types of studies frequently violate unconfoundedness or overlap, for instance,
observational studies with deterministic treatment decisions -- popularly known
as Regression Discontinuity designs -- violate overlap.
  In this paper, we initiate the study of general conditions that enable the
identification of the average treatment effect, extending beyond
unconfoundedness and overlap. In particular, following the paradigm of
statistical learning theory, we provide an interpretable condition that is
sufficient and nearly necessary for the identification of ATE. Moreover, this
condition characterizes the identification of the average treatment effect on
the treated (ATT) and can be used to characterize other treatment effects as
well. To illustrate the utility of our condition, we present several
well-studied scenarios where our condition is satisfied and, hence, we prove
that ATE can be identified in regimes that prior works could not capture. For
example, under mild assumptions on the data distributions, this holds for the
models proposed by Tan (2006) and Rosenbaum (2002), and the Regression
Discontinuity design model introduced by Thistlethwaite and Campbell (1960).
For each of these scenarios, we also show that, under natural additional
assumptions, ATE can be estimated from finite samples.
  We believe these findings open new avenues for bridging learning-theoretic
insights and causal inference methodologies, particularly in observational
studies with complex treatment mechanisms.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [259] [Dreaming up scale invariance via inverse renormalization group](https://arxiv.org/abs/2506.04016)
*Adam Rançon,Ulysse Rançon,Tomislav Ivek,Ivan Balog*

Main category: cond-mat.stat-mech

TL;DR: 论文探讨了如何用极简神经网络逆向重整化群（RG）粗粒化过程，在二维Ising模型中从粗粒化状态生成微观构型，证明仅需三个可训练参数的神经网络即可生成临界构型，并重现标度行为。


<details>
  <summary>Details</summary>
Motivation: 研究动机是通过机器学习模型概率性地重构标度不变的分布，而无需依赖微观输入，探索神经网络如何逆向RG粗粒化过程。

Method: 使用极简神经网络（最少三个可训练参数）学习生成临界构型，并通过实空间重整化群分析验证生成构型的标度不变性和RG变换的非平凡特征值。

Result: 结果表明，简单神经网络能有效生成临界构型，重现磁化率、热容和Binder比率等观测量的标度行为，且增加网络复杂度无显著提升。

Conclusion: 结论表明，类似生成分形结构的简单局部规则足以编码临界现象的普适性，为物理统计系综的高效生成模型开辟了新途径。

Abstract: We explore how minimal neural networks can invert the renormalization group
(RG) coarse-graining procedure in the two-dimensional Ising model, effectively
"dreaming up" microscopic configurations from coarse-grained states. This
task-formally impossible at the level of configurations-can be approached
probabilistically, allowing machine learning models to reconstruct
scale-invariant distributions without relying on microscopic input. We
demonstrate that even neural networks with as few as three trainable parameters
can learn to generate critical configurations, reproducing the scaling behavior
of observables such as magnetic susceptibility, heat capacity, and Binder
ratios. A real-space renormalization group analysis of the generated
configurations confirms that the models capture not only scale invariance but
also reproduce nontrivial eigenvalues of the RG transformation. Surprisingly,
we find that increasing network complexity by introducing multiple layers
offers no significant benefit. These findings suggest that simple local rules,
akin to those generating fractal structures, are sufficient to encode the
universality of critical phenomena, opening the door to efficient generative
models of statistical ensembles in physics.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [260] [Dual Branch VideoMamba with Gated Class Token Fusion for Violence Detection](https://arxiv.org/abs/2506.03162)
*Damith Chamalke Senadeera,Xiaoyun Yang,Dimitrios Kollias,Gregory Slabaugh*

Main category: cs.CV

TL;DR: 提出Dual Branch VideoMamba与GCTF结合的高效架构，用于视频暴力检测，通过双分支设计和状态空间模型提升性能，并创建新基准数据集。


<details>
  <summary>Details</summary>
Motivation: 随着监控摄像头的快速普及，自动化暴力检测需求增加，但现有CNN和Transformer方法在长期依赖和计算效率方面存在不足。

Method: 采用双分支设计（空间特征分支和时间动态分支）结合状态空间模型（SSM），通过门控机制持续融合特征，并整合多个数据集构建新基准。

Result: 模型在新基准上达到最优性能，平衡了准确性与计算效率，适合实时监控场景。

Conclusion: 该方法展示了状态空间模型在可扩展实时暴力检测中的潜力，为未来研究提供了新方向。

Abstract: The rapid proliferation of surveillance cameras has increased the demand for
automated violence detection. While CNNs and Transformers have shown success in
extracting spatio-temporal features, they struggle with long-term dependencies
and computational efficiency. We propose Dual Branch VideoMamba with Gated
Class Token Fusion (GCTF), an efficient architecture combining a dual-branch
design and a state-space model (SSM) backbone where one branch captures spatial
features, while the other focuses on temporal dynamics, with continuous fusion
via a gating mechanism. We also present a new benchmark by merging RWF-2000,
RLVS, and VioPeru datasets in video violence detection, ensuring strict
separation between training and testing sets. Our model achieves
state-of-the-art performance on this benchmark offering an optimal balance
between accuracy and computational efficiency, demonstrating the promise of
SSMs for scalable, real-time surveillance violence detection.

</details>


### [261] [Improvement of human health lifespan with hybrid group pose estimation methods](https://arxiv.org/abs/2506.03169)
*Arindam Chaudhuri*

Main category: cs.CV

TL;DR: 该论文提出了一种混合集成群体姿态估计方法，旨在通过改进的群体姿态估计和实时姿态估计技术，提升多人姿态检测的准确性和实时性，从而改善人类健康监测。


<details>
  <summary>Details</summary>
Motivation: 人类对身体姿态的估计需求日益增长，尤其是在健康监测领域。现有的姿态估计方法虽能通过计算机视觉技术跟踪人体动作，但在多人场景和实时性方面仍有提升空间。

Method: 论文提出了一种混合集成群体姿态估计方法，结合改进的群体姿态估计和实时姿态估计技术，通过姿态转换方法提取关键特征，并利用预训练的混合集成模型在公开数据集上进行训练和测试。

Result: 该方法在实时姿态估计中表现优异，能够有效应对遮挡问题并提高密集回归的准确性，实验结果表明其在多人场景和实时应用中具有显著优势。

Conclusion: 该混合集成群体姿态估计方法在提升姿态估计鲁棒性和准确性的同时，展现了在实时健康监测等应用中的潜力，有望延长人类健康寿命。

Abstract: Human beings rely heavily on estimation of poses in order to access their
body movements. Human pose estimation methods take advantage of computer vision
advances in order to track human body movements in real life applications. This
comes from videos which are recorded through available devices. These
para-digms provide potential to make human movement measurement more accessible
to users. The consumers of pose estimation movements believe that human poses
content tend to supplement available videos. This has increased pose estimation
software usage to estimate human poses. In order to address this problem, we
develop hybrid-ensemble-based group pose estimation method to improve human
health. This proposed hybrid-ensemble-based group pose estimation method aims
to detect multi-person poses using modified group pose estimation and modified
real time pose estimation. This ensemble allows fusion of performance of stated
methods in real time. The input poses from images are fed into individual
meth-ods. The pose transformation method helps to identify relevant features
for en-semble to perform training effectively. After this, customized
pre-trained hybrid ensemble is trained on public benchmarked datasets which is
being evaluated through test datasets. The effectiveness and viability of
proposed method is estab-lished based on comparative analysis of group pose
estimation methods and ex-periments conducted on benchmarked datasets. It
provides best optimized results in real-time pose estimation. It makes pose
estimation method more robust to oc-clusion and improves dense regression
accuracy. These results have affirmed po-tential application of this method in
several real-time situations with improvement in human health life span

</details>


### [262] [PALADIN : Robust Neural Fingerprinting for Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.03170)
*Murthy L,Subarna Tripathi*

Main category: cs.CV

TL;DR: 本文提出了一种利用循环纠错码概念的方法，用于在文本到图像扩散模型中实现神经指纹技术，以提高模型归属准确性。


<details>
  <summary>Details</summary>
Motivation: 由于开源文本到图像生成模型可能被恶意滥用，神经指纹技术作为一种风险缓解策略受到关注。然而，现有方法无法实现100%的归属准确性，限制了实际应用。

Method: 利用编码理论中的循环纠错码概念，提出了一种新的神经指纹技术方法。

Result: 该方法显著提高了文本到图像扩散模型的归属准确性。

Conclusion: 通过引入循环纠错码，本文方法在保持生成质量的同时，显著提升了神经指纹技术的归属准确性，为解决模型滥用问题提供了可行方案。

Abstract: The risk of misusing text-to-image generative models for malicious uses,
especially due to the open-source development of such models, has become a
serious concern. As a risk mitigation strategy, attributing generative models
with neural fingerprinting is emerging as a popular technique. There has been a
plethora of recent work that aim for addressing neural fingerprinting. A
trade-off between the attribution accuracy and generation quality of such
models has been studied extensively. None of the existing methods yet achieved
$100\%$ attribution accuracy. However, any model with less than \emph{perfect}
accuracy is practically non-deployable. In this work, we propose an accurate
method to incorporate neural fingerprinting for text-to-image diffusion models
leveraging the concepts of cyclic error correcting codes from the literature of
coding theory.

</details>


### [263] [EdgeVidSum: Real-Time Personalized Video Summarization at the Edge](https://arxiv.org/abs/2506.03171)
*Ghulam Mujtaba,Eun-Seok Ryu*

Main category: cs.CV

TL;DR: EdgeVidSum是一种轻量级方法，直接在边缘设备上生成个性化的长视频快进摘要，通过本地数据处理保护用户隐私，并利用创新的缩略图技术和高效神经架构降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统视频摘要方法需要逐帧处理整个视频，计算复杂度高且无法在资源受限的边缘设备上实时运行。EdgeVidSum旨在解决计算效率、个性化和隐私保护的挑战，满足现代视频消费环境的需求。

Method: 采用分层分析方法，使用轻量级2D CNN模型从缩略图中识别用户偏好内容，并生成时间戳以创建快进摘要。通过缩略图容器显著降低计算复杂度，同时保持语义相关性。

Result: EdgeVidSum能够在Jetson Nano等资源受限的设备上无缝运行，为电影、体育赛事和电视节目等长视频生成个性化的快进摘要，展示了其在计算效率、个性化和隐私保护方面的优势。

Conclusion: EdgeVidSum通过创新的缩略图技术和高效神经架构，实现了在边缘设备上实时生成个性化视频摘要的目标，同时解决了计算效率、个性化和隐私保护的关键挑战。

Abstract: EdgeVidSum is a lightweight method that generates personalized, fast-forward
summaries of long-form videos directly on edge devices. The proposed approach
enables real-time video summarization while safeguarding user privacy through
local data processing using innovative thumbnail-based techniques and efficient
neural architectures. Unlike conventional methods that process entire videos
frame by frame, the proposed method uses thumbnail containers to significantly
reduce computational complexity without sacrificing semantic relevance. The
framework employs a hierarchical analysis approach, where a lightweight 2D CNN
model identifies user-preferred content from thumbnails and generates
timestamps to create fast-forward summaries. Our interactive demo highlights
the system's ability to create tailored video summaries for long-form videos,
such as movies, sports events, and TV shows, based on individual user
preferences. The entire computation occurs seamlessly on resource-constrained
devices like Jetson Nano, demonstrating how EdgeVidSum addresses the critical
challenges of computational efficiency, personalization, and privacy in modern
video consumption environments.

</details>


### [264] [FOLIAGE: Towards Physical Intelligence World Models Via Unbounded Surface Evolution](https://arxiv.org/abs/2506.03173)
*Xiaoyi Liu,Hao Tang*

Main category: cs.CV

TL;DR: FOLIAGE提出了一种物理智能驱动的多模态世界模型，用于无界表面增长预测，通过统一编码和物理感知预测器实现跨模态建模，并在SURF-BENCH评测中超越基线。


<details>
  <summary>Details</summary>
Motivation: 构建能够从部分多感官观察中预测和塑造世界的物理智能模型，是下一代世界模型的关键需求。

Method: 采用统一上下文编码器整合图像/网格/点云数据，通过物理感知预测器生成模态无关增长嵌入(MAGE)，结合 accretive 图网络(AGN)和几何对齐融合技术。

Result: 在包含6项核心任务和4项压力测试的SURF-BENCH评测中表现优于专业基线模型，且在不同动态环境中保持稳健性。

Conclusion: FOLIAGE为物理智能建立了基于世界模型的多模态新路径，并通过SURF-GARDEN平台提供7,200组表面增长序列支持后续研究。

Abstract: Physical intelligence -- anticipating and shaping the world from partial,
multisensory observations -- is critical for next-generation world models. We
propose FOLIAGE, a physics-informed multimodal world model for unbounded
accretive surface growth. In its Action-Perception loop, a unified context
encoder maps images, mesh connectivity, and point clouds to a shared latent
state. A physics-aware predictor, conditioned on physical control actions,
advances this latent state in time to align with the target latent of the
surface, yielding a Modality-Agnostic Growth Embedding (MAGE) that interfaces
with critic heads for downstream objectives. FOLIAGE's Accretive Graph Network
(AGN) captures dynamic connectivity through Age Positional Encoding and
Energy-Gated Message-Passing. Geometry-Correspondence Fusion and Cross-Patch
Masking enhance MAGE's expressiveness, while Hierarchical Pooling balances
global context with local dynamics. We create SURF-GARDEN, a world model
learning platform comprising a Counterfactual Physics Simulator, a Multimodal
Correspondence Extractor, and Evolution Tracing, which generates 7,200 diverse
surface-growth sequences. SURF-BENCH, our physical-intelligence evaluation
suite, evaluates six core tasks -- topology recognition, inverse material
estimation, growth-stage classification, latent roll-out, cross-modal
retrieval, and dense correspondence -- and four stress tests -- sensor dropout,
zero-shot modality transfer, long-horizon prediction, and physics ablation --
to probe resilience. FOLIAGE outperforms specialized baselines while remaining
robust across dynamic environments, establishing a new world-model based,
multimodal pathway to physical intelligence.

</details>


### [265] [Multimodal Foundation Model for Cross-Modal Retrieval and Activity Recognition Tasks](https://arxiv.org/abs/2506.03174)
*Koki Matsuishi,Kosuke Ukita,Tsuyoshi Okita*

Main category: cs.CV

TL;DR: 提出AURA-MFM多模态基础模型，整合第三人称视频、动作捕捉、IMU和文本数据，显著提升人体活动分析的全面性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于第一人称视频和文本的多模态模型无法全面分析全身活动，需结合更多模态数据以提升分析精度。

Method: 融合第三人称视频、动作捕捉、IMU和文本四种模态，采用基于Transformer的IMU编码器增强模型性能。

Result: 在检索和活动识别任务中超越现有方法，零样本动作识别的F1值达0.6226，准确率0.7320。

Conclusion: AURA-MFM通过多模态整合实现了更全面的人体活动理解，为相关领域提供了有效的基准模型。

Abstract: In recent years, the widespread adoption of wearable devices has highlighted
the growing importance of behavior analysis using IMU. While applications span
diverse fields such as healthcare and robotics, recent studies have
increasingly focused on multimodal analysis, in addition to unimodal analysis.
Several studies have proposed multimodal foundation models that incorporate
first-person video and text data; however, these models still fall short in
providing a detailed analysis of full-body human activity. To address this
limitation, we propose Activity Understanding and Representations Alignment -
Multimodal Foundation Model (AURA-MFM), a foundational model integrating four
modalities: third-person video, motion capture, IMU, and text. By incorporating
third-person video and motion capture data, the model enables a detailed and
multidimensional understanding of human activity, which first-person
perspectives alone fail to capture. Additionally, a Transformer-based IMU
encoder is employed to enhance the model's overall performance. Experimental
evaluations on retrieval and activity recognition tasks demonstrate that our
model surpasses existing methods. Notably, in the zero-shot classification for
action recognition, our method achieved significantly higher performance, with
an F1-score of 0.6226 and an accuracy of 0.7320, whereas the existing method
recorded an F1-score of 0.0747 and an accuracy of 0.1961.

</details>


### [266] [Vid-SME: Membership Inference Attacks against Large Video Understanding Models](https://arxiv.org/abs/2506.03179)
*Qi Li,Runpeng Yu,Xinchao Wang*

Main category: cs.CV

TL;DR: 该论文提出了Vid-SME方法，专门针对视频理解大语言模型中的成员推理攻击问题，通过利用模型输出的置信度和自适应参数化计算Sharma-Mittal熵，有效识别训练集中的视频数据。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型在视频理解应用中的广泛使用，训练数据中可能包含敏感视频内容，引发了数据隐私问题。现有成员推理攻击方法在视频领域效果不佳，亟需一种新的解决方案。

Method: Vid-SME方法通过计算自然视频帧和时间反转视频帧的Sharma-Mittal熵差异，生成稳健的成员分数，以判断视频是否属于训练集。

Result: 实验表明，Vid-SME在多种自训练和开源视频理解大语言模型中表现出色，显著提高了成员推理的准确性。

Conclusion: Vid-SME是首个针对视频理解大语言模型的成员推理方法，有效解决了现有方法在视频领域的局限性，为数据隐私保护提供了新工具。

Abstract: Multimodal large language models (MLLMs) demonstrate remarkable capabilities
in handling complex multimodal tasks and are increasingly adopted in video
understanding applications. However, their rapid advancement raises serious
data privacy concerns, particularly given the potential inclusion of sensitive
video content, such as personal recordings and surveillance footage, in their
training datasets. Determining improperly used videos during training remains a
critical and unresolved challenge. Despite considerable progress on membership
inference attacks (MIAs) for text and image data in MLLMs, existing methods
fail to generalize effectively to the video domain. These methods suffer from
poor scalability as more frames are sampled and generally achieve negligible
true positive rates at low false positive rates (TPR@Low FPR), mainly due to
their failure to capture the inherent temporal variations of video frames and
to account for model behavior differences as the number of frames varies. To
address these challenges, we introduce Vid-SME, the first membership inference
method tailored for video data used in video understanding LLMs (VULLMs).
Vid-SME leverages the confidence of model output and integrates adaptive
parameterization to compute Sharma-Mittal entropy (SME) for video inputs. By
leveraging the SME difference between natural and temporally-reversed video
frames, Vid-SME derives robust membership scores to determine whether a given
video is part of the model's training set. Experiments on various self-trained
and open-sourced VULLMs demonstrate the strong effectiveness of Vid-SME.

</details>


### [267] [Impact of Tuning Parameters in Deep Convolutional Neural Network Using a Crack Image Dataset](https://arxiv.org/abs/2506.03184)
*Mahe Zabin,Ho-Jin Choi,Md. Monirul Islam,Jia Uddin*

Main category: cs.CV

TL;DR: 本文研究了深度卷积神经网络（DCNN）中不同调参对性能的影响，发现使用maxpooling、adam优化器和tanh激活函数时表现最佳。


<details>
  <summary>Details</summary>
Motivation: 分类器的性能依赖于其参数的调整，本文旨在探索不同调参对DCNN性能的具体影响。

Method: 使用包含2个卷积层、2个池化层、1个dropout层和1个密集层的DCNN，在裂纹图像数据集上测试池化、激活函数和优化器的调参效果。

Result: 实验结果表明，结合maxpooling、adam优化器和tanh激活函数的DCNN表现最优。

Conclusion: 通过调参优化，DCNN在特定配置下能显著提升分类性能。

Abstract: The performance of a classifier depends on the tuning of its parame ters. In
this paper, we have experimented the impact of various tuning parameters on the
performance of a deep convolutional neural network (DCNN). In the ex perimental
evaluation, we have considered a DCNN classifier that consists of 2
convolutional layers (CL), 2 pooling layers (PL), 1 dropout, and a dense layer.
To observe the impact of pooling, activation function, and optimizer tuning pa
rameters, we utilized a crack image dataset having two classes: negative and
pos itive. The experimental results demonstrate that with the maxpooling, the
DCNN demonstrates its better performance for adam optimizer and tanh activation
func tion.

</details>


### [268] [Continual Learning in Vision-Language Models via Aligned Model Merging](https://arxiv.org/abs/2506.03189)
*Ghada Sokar,Gintare Karolina Dziugaite,Anurag Arnab,Ahmet Iscen,Pablo Samuel Castro,Cordelia Schmid*

Main category: cs.CV

TL;DR: 论文提出了一种基于模型合并的持续学习方法，通过合并新任务参数与旧任务参数来平衡可塑性与稳定性，减少遗忘并提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法通过顺序微调实现适应，但倾向于可塑性而忽视了稳定性，导致对近期任务的偏见和灾难性遗忘。本文旨在解决这一问题。

Method: 提出了一种模型合并方法，通过合并新训练的任务参数与已学习参数来平衡可塑性与稳定性，并设计了一种简单机制来学习与先前权重对齐的参数以避免干扰。

Result: 该方法在大型视觉语言模型上验证，有效减少了遗忘，增强了对不同任务顺序和相似性的鲁棒性，并提高了泛化能力。

Conclusion: 基于模型合并的持续学习方法在平衡可塑性与稳定性方面表现出色，显著减少了遗忘并提升了模型性能。

Abstract: Continual learning is conventionally tackled through sequential fine-tuning,
a process that, while enabling adaptation, inherently favors plasticity over
the stability needed to retain prior knowledge. While existing approaches
attempt to mitigate catastrophic forgetting, a bias towards recent tasks
persists as they build upon this sequential nature. In this work we present a
new perspective based on model merging to maintain stability while still
retaining plasticity. Rather than just sequentially updating the model weights,
we propose merging newly trained task parameters with previously learned ones,
promoting a better balance. To maximize the effectiveness of the merging
process, we propose a simple mechanism that promotes learning aligned weights
with previous ones, thereby avoiding interference when merging. We evaluate
this approach on large Vision-Language Models (VLMs), and demonstrate its
effectiveness in reducing forgetting, increasing robustness to various task
orders and similarities, and improving generalization.

</details>


### [269] [MINT: Memory-Infused Prompt Tuning at Test-time for CLIP](https://arxiv.org/abs/2506.03190)
*Jiaming Yi,Ruirui Pan,Jishen Yang,Xiulong Yang*

Main category: cs.CV

TL;DR: 提出了一种名为MINT的新框架，通过记忆提示库动态调整视觉语言预训练模型，以提升测试时数据分布变化的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有测试时适应方法未能充分利用模型内部知识，特别是在动态适应复杂和分层的视觉语义信息方面存在不足。

Method: MINT框架引入记忆提示库（MPB），存储可学习的键值提示对，通过测试图像的分层视觉特征检索相关提示对，动态组装关联提示并注入图像编码器。

Result: MINT无需源数据或重新训练，即可实现快速、精确的视觉语言模型测试时适应。

Conclusion: MINT通过记忆提示库有效提升了视觉语言预训练模型在测试时数据分布变化下的泛化能力。

Abstract: Improving the generalization ability of Vision-Language Pre-trained Models
(VLMs) under test-time data distribution shifts remains a critical challenge.
The existing Test-Time Adaptation (TTA) methods fall short in fully leveraging
the model's internal knowledge, particularly in dynamically adapting to complex
and hierarchical visual semantic information. In this paper, we propose
Memory-Infused Prompt Tuning (MINT), a novel framework to address this issue.
Inspired by human associative memory theory, MINT introduces a Memory Prompt
Bank (MPB), which stores learnable key-value prompt pairs that work as a memory
of previously seen samples. During the test time, relevant prompt pairs in the
MPB are retrieved by the hierarchical visual features of test images to
dynamically assemble Associative Prompts. The associative prompts are then
injected into the image encoder for fine-grained, customized visual contextual
guidance. MINT also utilizes learnable text prompts. MINT thus enables rapid,
precise VLM adaptation at test time by leveraging this MPB-acquired memory,
without source data or retraining. The code is available at
https://github.com/Jamieyi2004/MINT.

</details>


### [270] [Multimodal Generative AI with Autoregressive LLMs for Human Motion Understanding and Generation: A Way Forward](https://arxiv.org/abs/2506.03191)
*Muhammad Islam,Tao Huang,Euijoon Ahn,Usman Naseem*

Main category: cs.CV

TL;DR: 本文综述了多模态生成式AI和自回归大语言模型在人体运动理解与生成中的应用，探讨了文本描述如何指导复杂人体运动序列的生成，并分析了不同生成方法的优劣。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索如何利用文本描述生成更真实、多样化的人体运动序列，以推动医疗、人形机器人、游戏动画等领域的技术进步。

Method: 通过分析自回归模型、扩散模型、GANs、VAEs和基于Transformer的模型，评估其在运动质量、计算效率和适应性方面的表现。

Result: 研究发现，结合大语言模型能显著提升文本与运动之间的语义对齐，增强运动的连贯性和上下文相关性。

Conclusion: 文本到运动的生成式AI和LLM架构在多个领域具有变革潜力，但仍面临生成高效、真实运动的挑战。

Abstract: This paper presents an in-depth survey on the use of multimodal Generative
Artificial Intelligence (GenAI) and autoregressive Large Language Models (LLMs)
for human motion understanding and generation, offering insights into emerging
methods, architectures, and their potential to advance realistic and versatile
motion synthesis. Focusing exclusively on text and motion modalities, this
research investigates how textual descriptions can guide the generation of
complex, human-like motion sequences. The paper explores various generative
approaches, including autoregressive models, diffusion models, Generative
Adversarial Networks (GANs), Variational Autoencoders (VAEs), and
transformer-based models, by analyzing their strengths and limitations in terms
of motion quality, computational efficiency, and adaptability. It highlights
recent advances in text-conditioned motion generation, where textual inputs are
used to control and refine motion outputs with greater precision. The
integration of LLMs further enhances these models by enabling semantic
alignment between instructions and motion, improving coherence and contextual
relevance. This systematic survey underscores the transformative potential of
text-to-motion GenAI and LLM architectures in applications such as healthcare,
humanoids, gaming, animation, and assistive technologies, while addressing
ongoing challenges in generating efficient and realistic human motion.

</details>


### [271] [HueManity: Probing Fine-Grained Visual Perception in MLLMs](https://arxiv.org/abs/2506.03194)
*Rynaa Grover,Jayant Sravan Tamarapalli,Sahiti Yerramilli,Nilay Pande*

Main category: cs.CV

TL;DR: 论文提出了HueManity基准测试，揭示多模态大语言模型在细微视觉感知任务上的显著性能缺陷，并开源数据集以促进研究。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）在高级视觉推理上表现优异，但在细微感知任务上的能力仍有明显不足。为此，作者提出HueManity基准，旨在评估MLLMs的视觉感知能力。

Method: 构建包含83,850张石原式点阵图的HueManity数据集，测试模型对双字符数字/字母组合的精确识别能力，并对比了9种前沿MLLMs、人类表现及微调ResNet50模型的性能。

Result: 最佳MLLM在数字‘简单’任务中仅达33.6%准确率，字母数字‘困难’任务中低至3%；人类参与者接近满分（100%和95.6%），微调ResNet50分别达到96.5%和94.5%。

Conclusion: 当前MLLMs存在显著的视觉感知缺陷，需改进架构和训练范式。开源HueManity以推动MLLMs感知鲁棒性的研究。

Abstract: Multimodal Large Language Models (MLLMs) excel at high-level visual
reasoning, but their performance on nuanced perceptual tasks remains
surprisingly limited. We present HueManity, a benchmark designed to assess
visual perception in MLLMs. The dataset comprises 83,850 images featuring
two-character alphanumeric strings embedded in Ishihara test style dot
patterns, challenging models on precise pattern recognition. Our evaluation of
nine state-of-the-art MLLMs on HueManity demonstrates a significant performance
deficit compared to human and traditional computer vision baselines. The
best-performing MLLM achieved a 33.6% accuracy on the numeric `easy' task and a
striking 3% on the alphanumeric `hard' task. In contrast, human participants
achieved near-perfect scores (100% and 95.6%), and a fine-tuned ResNet50 model
reached accuracies of 96.5% and 94.5%. These results highlight a critical gap
in the visual capabilities of current MLLMs. Our analysis further explores
potential architectural and training-paradigm factors contributing to this
perceptual gap in MLLMs. We open-source HueManity dataset and code to foster
further research in improving perceptual robustness of MLLMs.

</details>


### [272] [Unlabeled Data Improves Fine-Grained Image Zero-shot Classification with Multimodal LLMs](https://arxiv.org/abs/2506.03195)
*Yunqi Hong,Sohyun An,Andrew Bai,Neil Y. C. Lin,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: AutoSEP是一种自监督提示学习框架，旨在提升多模态大语言模型在细粒度图像分类中的表现，无需标注数据或模型微调。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在细粒度图像分类任务中表现不佳，因为它们容易忽略细微的视觉差异。需要一种无需监督的方法来引导模型关注关键特征。

Method: 提出AutoSEP框架，通过迭代自监督学习优化描述提示，利用未标注数据和实例级分类评分函数，仅需黑盒访问模型。

Result: 在多个细粒度分类数据集上，AutoSEP平均比零样本分类提升13%，比最佳基线提升5%。

Conclusion: AutoSEP有效提升了多模态大语言模型在细粒度分类中的性能，且完全无需监督或模型修改。

Abstract: Despite Multimodal Large Language Models (MLLMs) showing promising results on
general zero-shot image classification tasks, fine-grained image classification
remains challenging. It demands precise attention to subtle visual details to
distinguish between visually similar subcategories--details that MLLMs may
easily overlook without explicit guidance. To address this, we introduce
AutoSEP, an iterative self-supervised prompt learning framework designed to
enhance MLLM fine-grained classification capabilities in a fully unsupervised
manner. Our core idea is to leverage unlabeled data to learn a description
prompt that guides MLLMs in identifying crucial discriminative features within
an image, and boosts classification accuracy. We developed an automatic
self-enhancing prompt learning framework called AutoSEP to iteratively improve
the description prompt using unlabeled data, based on instance-level
classification scoring function. AutoSEP only requires black-box access to
MLLMs, eliminating the need for any training or fine-tuning. We evaluate our
approach on multiple fine-grained classification datasets. It consistently
outperforms other unsupervised baselines, demonstrating the effectiveness of
our self-supervised optimization framework. Notably, AutoSEP on average
improves 13 percent over standard zero-shot classification and 5 percent over
the best-performing baselines. Code is available at:
https://github.com/yq-hong/AutoSEP

</details>


### [273] [Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing](https://arxiv.org/abs/2506.03197)
*Baode Wang,Biao Wu,Weizhen Li,Meng Fang,Yanjie Liang,Zuming Huang,Haozhe Wang,Jun Huang,Ling Chen,Wei Chu,Yuan Qi*

Main category: cs.CV

TL;DR: 提出layoutRL框架和Infinity-Parser模型，通过强化学习优化文档解析，在多项任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统多阶段文档解析流程存在错误传播和布局适应性差的问题，需端到端解决方案。

Method: 基于强化学习的layoutRL框架，结合编辑距离、段落计数和阅读顺序的复合奖励函数，使用合成与真实混合数据集Infinity-Doc-55K训练。

Result: Infinity-Parser在中英文OCR、表格/公式提取、阅读顺序检测等任务中准确率和结构保真度均超越现有方法。

Conclusion: 该框架显著提升文档理解鲁棒性，代码和数据集将开源以推动领域发展。

Abstract: Automated parsing of scanned documents into richly structured,
machine-readable formats remains a critical bottleneck in Document AI, as
traditional multi-stage pipelines suffer from error propagation and limited
adaptability to diverse layouts. We introduce layoutRL, an end-to-end
reinforcement learning framework that trains models to be explicitly
layout-aware by optimizing a composite reward of normalized edit distance,
paragraph count accuracy, and reading order preservation. Leveraging our newly
released dataset, Infinity-Doc-55K, which combines 55K high-fidelity synthetic
scanned document parsing data with expert-filtered real-world documents, we
instantiate layoutRL in a vision-language-model-based parser called
Infinity-Parser. Evaluated on English and Chinese benchmarks for OCR, table and
formula extraction, and reading order detection, Infinity-Parser achieves new
state-of-the-art performance in both accuracy and structural fidelity,
outpacing specialist pipelines and general-purpose vision-language models. We
will publicly release our code and dataset to accelerate progress in robust
document understanding.

</details>


### [274] [FLEX: A Large-Scale Multi-Modal Multi-Action Dataset for Fitness Action Quality Assessment](https://arxiv.org/abs/2506.03198)
*Hao Yin,Lijun Gu,Paritosh Parmar,Lin Xu,Tianxiao Guo,Weiwei Fu,Yang Zhang,Tianyou Zheng*

Main category: cs.CV

TL;DR: 该论文提出了首个多模态、多动作的大规模健身动作质量评估数据集FLEX，结合表面肌电信号(sEMG)和知识图谱，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着健康意识的增强和审美体型的追求，健身成为流行趋势。然而，负重健身动作的潜在风险不容忽视。现有的动作质量评估(AQA)方法和数据集局限于单视角竞技体育场景和RGB模态，缺乏对健身动作的专业评估和指导。

Method: 论文提出了FLEX数据集，利用高精度动作捕捉(MoCap)收集38名不同技能水平的受试者进行的20种负重动作，包含5种视角的RGB视频、3D姿态、sEMG和生理信息。此外，FLEX将知识图谱引入AQA，构建了以惩罚函数形式映射负重动作、关键步骤、错误类型和反馈的标注规则。

Result: 在FLEX上进行的多种基线方法表明，多模态数据、多视角数据和细粒度标注显著提升了模型性能。

Conclusion: FLEX不仅推动了AQA方法和数据集向多模态和多动作场景发展，还促进了人工智能在健身领域的整合。数据集和代码已公开。

Abstract: With the increasing awareness of health and the growing desire for aesthetic
physique, fitness has become a prevailing trend. However, the potential risks
associated with fitness training, especially with weight-loaded fitness
actions, cannot be overlooked. Action Quality Assessment (AQA), a technology
that quantifies the quality of human action and provides feedback, holds the
potential to assist fitness enthusiasts of varying skill levels in achieving
better training outcomes. Nevertheless, current AQA methodologies and datasets
are limited to single-view competitive sports scenarios and RGB modality and
lack professional assessment and guidance of fitness actions. To address this
gap, we propose the FLEX dataset, the first multi-modal, multi-action,
large-scale dataset that incorporates surface electromyography (sEMG) signals
into AQA. FLEX utilizes high-precision MoCap to collect 20 different
weight-loaded actions performed by 38 subjects across 3 different skill levels
for 10 repetitions each, containing 5 different views of the RGB video, 3D
pose, sEMG, and physiological information. Additionally, FLEX incorporates
knowledge graphs into AQA, constructing annotation rules in the form of penalty
functions that map weight-loaded actions, action keysteps, error types, and
feedback. We conducted various baseline methodologies on FLEX, demonstrating
that multimodal data, multiview data, and fine-grained annotations
significantly enhance model performance. FLEX not only advances AQA
methodologies and datasets towards multi-modal and multi-action scenarios but
also fosters the integration of artificial intelligence within the fitness
domain. Dataset and code are available at
https://haoyin116.github.io/FLEX_Dataset.

</details>


### [275] [OpenCarbon: A Contrastive Learning-based Cross-Modality Neural Approach for High-Resolution Carbon Emission Prediction Using Open Data](https://arxiv.org/abs/2506.03224)
*Jinwei Zeng,Yu Liu,Guozhen Zhang,Jingtao Ding,Yuming Lin,Jian Yuan,Yong Li*

Main category: cs.CV

TL;DR: OpenCarbon模型利用卫星图像和POI数据预测高分辨率城市碳排放，通过跨模态信息融合和邻域聚合模块解决功能性和空间连续性问题，性能提升26.6%。


<details>
  <summary>Details</summary>
Motivation: 传统碳核算方法数据收集成本高，开放数据和先进学习技术为高分辨率碳排放估算提供了新解决方案。

Method: 结合卫星图像和POI数据，设计跨模态信息提取融合模块和邻域信息聚合模块，建模功能交互和空间连续性。

Result: 实验显示模型R2指标显著提升26.6%，泛化测试和案例研究验证了其捕捉城市功能与碳排放内在关系的能力。

Conclusion: OpenCarbon模型为高效碳治理和针对性减排规划提供了有力工具，代码和数据已开源。

Abstract: Accurately estimating high-resolution carbon emissions is crucial for
effective emission governance and mitigation planning. While conventional
methods for precise carbon accounting are hindered by substantial data
collection efforts, the rise of open data and advanced learning techniques
offers a promising solution. Once an open data-based prediction model is
developed and trained, it can easily infer emissions for new areas based on
available open data. To address this, we incorporate two modalities of open
data, satellite images and point-of-interest (POI) data, to predict
high-resolution urban carbon emissions, with satellite images providing
macroscopic and static and POI data offering fine-grained and relatively
dynamic functionality information. However, estimating high-resolution carbon
emissions presents two significant challenges: the intertwined and implicit
effects of various functionalities on carbon emissions, and the complex spatial
contiguity correlations that give rise to the agglomeration effect. Our model,
OpenCarbon, features two major designs that target the challenges: a
cross-modality information extraction and fusion module to extract
complementary functionality information from two modules and model their
interactions, and a neighborhood-informed aggregation module to capture the
spatial contiguity correlations. Extensive experiments demonstrate our model's
superiority, with a significant performance gain of 26.6\% on R2. Further
generalizability tests and case studies also show OpenCarbon's capacity to
capture the intrinsic relation between urban functionalities and carbon
emissions, validating its potential to empower efficient carbon governance and
targeted carbon mitigation planning. Codes and data are available:
https://github.com/JinweiZzz/OpenCarbon.

</details>


### [276] [Pre-trained Vision-Language Models Assisted Noisy Partial Label Learning](https://arxiv.org/abs/2506.03229)
*Qian-Wei Wang,Yuqiu Xie,Letian Zhang,Zimo Liu,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 本文提出了一种名为Co-Reg的创新方法，用于从预训练视觉语言模型（VLMs）生成的噪声部分标签中学习，通过协同一致性正则化提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着高性能预训练视觉语言模型（如CLIP、LLaVa和GPT-4V）的出现，利用这些模型替代耗时的人工标注流程，实现下游任务的“免人工标注”训练成为一个极具前景的研究方向。然而，预训练模型生成的噪声是实例依赖的，增加了模型学习的难度。

Method: 本文提出了一种协同一致性正则化（Co-Reg）方法，通过同时训练两个神经网络，利用“协同伪标签”机制对训练标签进行协同净化，并在标签空间和特征表示空间中施加一致性正则化约束。此外，方法还可利用少量人工标注的有效标签进一步提升性能。

Result: 通过与不同的去噪和消歧算法、标注方式及预训练模型应用方案的对比实验，充分验证了所提方法的有效性，同时揭示了将弱监督学习技术整合到预训练模型知识蒸馏过程中的广阔前景。

Conclusion: 本文提出的Co-Reg方法有效解决了预训练模型生成噪声部分标签的学习问题，为免人工标注训练提供了新的解决方案，展示了弱监督学习与预训练模型结合的潜力。

Abstract: In the context of noisy partial label learning (NPLL), each training sample
is associated with a set of candidate labels annotated by multiple noisy
annotators. With the emergence of high-performance pre-trained vision-language
models (VLMs) such as CLIP, LLaVa and GPT-4V, the direction of using these
models to replace time-consuming manual annotation workflows and achieve
"manual-annotation-free" training for downstream tasks has become a highly
promising research avenue. This paper focuses on learning from noisy partial
labels annotated by pre-trained VLMs and proposes an innovative collaborative
consistency regularization (Co-Reg) method. Unlike the symmetric noise
primarily addressed in traditional noisy label learning, the noise generated by
pre-trained models is instance-dependent, embodying the underlying patterns of
the pre-trained models themselves, which significantly increases the learning
difficulty for the model. To address this, we simultaneously train two neural
networks that implement collaborative purification of training labels through a
"Co-Pseudo-Labeling" mechanism, while enforcing consistency regularization
constraints in both the label space and feature representation space. Our
method can also leverage few-shot manually annotated valid labels to further
enhance its performances. Comparative experiments with different denoising and
disambiguation algorithms, annotation manners, and pre-trained model
application schemes fully validate the effectiveness of the proposed method,
while revealing the broad prospects of integrating weakly-supervised learning
techniques into the knowledge distillation process of pre-trained models.

</details>


### [277] [Chipmunk: Training-Free Acceleration of Diffusion Transformers with Dynamic Column-Sparse Deltas](https://arxiv.org/abs/2506.03275)
*Austin Silveria,Soham V. Govande,Daniel Y. Fu*

Main category: cs.CV

TL;DR: 论文提出Chipmunk方法，通过动态稀疏化减少DiT推理时的计算冗余，实现加速且不损失生成质量。


<details>
  <summary>Details</summary>
Motivation: DiT在高质量图像和视频生成中表现优异，但推理计算成本高。研究发现DiT潜在噪声向量在推理步骤间变化缓慢，表明计算存在冗余。

Method: Chipmunk利用动态稀疏化，仅重新计算变化最快的中间激活值，同时缓存其余部分。采用体素重排输入令牌和列稀疏核优化GPU计算效率，并重叠计算模式与缓存更新以减少延迟。

Result: Chipmunk在HunyuanVideo和FLUX.1-dev上分别实现2.16倍和1.41倍加速，叠加全步缓存后加速比可达3.72倍，且生成质量几乎无损。

Conclusion: Chipmunk有效减少DiT推理冗余，显著提升速度，适用于多种模型且保持生成质量。

Abstract: Diffusion Transformers (DiTs) have achieved state-of-the-art performance in
high-quality image and video generation but incur substantial compute cost at
inference. A common observation is that DiT latent noise vectors change slowly
across inference steps, which suggests that the DiT compute may be redundant
across steps. In this paper, we aim to speed up inference by reducing this
redundancy, without additional training. We first study how activations change
between steps in two state-of-the-art open-source DiTs. We find that just 5-25%
of the values in attention and MLP explain 70-90% of the change in activations
across steps. This finding motivates our approach, Chipmunk, which uses dynamic
sparsity at inference time to recompute only the fastest-changing intermediate
activations, while caching the rest. Dynamic sparsity introduces two systems
challenges: (1) sparse attention and MLP operations tend to underutilize GPU
tensor cores; and (2) computing dynamic sparsity patterns at runtime and
caching activations both introduce overhead. To address these challenges,
Chipmunk first uses a voxel-based reordering of input tokens to introduce
column-wise sparsity. We implement column-sparse kernels utilizing efficient
sparse gathers from global to shared GPU memory, achieving a 9.3x speedup at
93% sparsity compared to highly-optimized dense baselines. Second, Chipmunk
overlaps the computation of sparsity patterns and cache updates with other
parts of the computation (e.g., second layer of the MLP) to hide the extra
latency. Chipmunk achieves up to 2.16x speedup on HunyuanVideo and 1.41x on
FLUX.1-dev without compromising generation quality. Furthermore, we show that
Chipmunk can be stacked on top of full step caching, achieving a 3.72x speedup
on HunyuanVideo, a 2.67x speedup on WAN2.1, and a 2.25x speedup on FLUX.1-dev
with minimal quality impact.

</details>


### [278] [A Foundation Model for Spatial Proteomics](https://arxiv.org/abs/2506.03373)
*Muhammad Shaban,Yuzhou Chang,Huaying Qiu,Yao Yu Yeo,Andrew H. Song,Guillaume Jaume,Yuchen Wang,Luca L. Weishaupt,Tong Ding,Anurag Vaidya,Abdallah Lamane,Daniel Shao,Mohammed Zidane,Yunhao Bai,Paige McCallum,Shuli Luo,Wenrui Wu,Yang Wang,Precious Cramer,Chi Ngai Chan,Pierre Stephan,Johanna Schaffenrath,Jia Le Lee,Hendrik A. Michel,Caiwei Tian,Cristina Almagro-Perez,Sophia J. Wagner,Sharifa Sahai,Ming Y. Lu,Richard J. Chen,Andrew Zhang,Mark Edward M. Gonzales,Ahmad Makky,Jia-Ying Joey Lee,Hao Cheng,Nourhan El Ahmar,Sayed Matar,Maximilian Haist,Darci Phillips,Yuqi Tan,Garry P. Nolan,W. Richard Burack,Jacob D. Estes,Jonathan T. C. Liu,Toni K Choueiri,Neeraj Agarwal,Marc Barry,Scott J. Rodig,Long Phi Le,Georg Gerber,Christian M. Schürch,Fabian J. Theis,Youn H Kim,Joe Yeong,Sabina Signoretti,Brooke E. Howitt,Lit-Hsin Loo,Qin Ma,Sizun Jiang,Faisal Mahmood*

Main category: cs.CV

TL;DR: KRONOS是一种用于空间蛋白质组学的基础模型，通过自监督学习处理多通道成像数据，支持多种下游任务，并在多个数据集中表现出色。


<details>
  <summary>Details</summary>
Motivation: 基础模型在图像分析中表现出色，但在空间蛋白质组学中的应用仍然有限。KRONOS旨在填补这一空白，提供一种通用的、可扩展的工具。

Method: KRONOS采用自监督学习，训练于4700万图像块，覆盖175种蛋白质标记、16种组织类型和8种荧光成像平台，并针对多通道成像的高维和异质性进行了架构优化。

Result: 在11个独立数据集中，KRONOS在细胞表型分析、治疗反应预测和检索任务中均达到最先进性能，且数据效率高。

Conclusion: KRONOS作为一种灵活且可扩展的工具，为空间蛋白质组学分析提供了新的范式，支持跨机构比较和图像逆向搜索。

Abstract: Foundation models have begun to transform image analysis by acting as
pretrained generalist backbones that can be adapted to many tasks even when
post-training data are limited, yet their impact on spatial proteomics, imaging
that maps proteins at single-cell resolution, remains limited. Here, we
introduce KRONOS, a foundation model built for spatial proteomics. KRONOS was
trained in a self-supervised manner on over 47 million image patches covering
175 protein markers, 16 tissue types, and 8 fluorescence-based imaging
platforms. We introduce key architectural adaptations to address the
high-dimensional, multi-channel, and heterogeneous nature of multiplex imaging.
We demonstrate that KRONOS learns biologically meaningful representations
across multiple scales, ranging from cellular and microenvironment to tissue
levels, enabling it to address diverse downstream tasks, including cell
phenotyping, region classification, and patient stratification. Evaluated
across 11 independent cohorts, KRONOS achieves state-of-the-art performance
across cell phenotyping, treatment response prediction, and retrieval tasks,
and is highly data-efficient. KRONOS also introduces the paradigm of
segmentation-free patch-level processing for efficient and scalable spatial
proteomics analysis, allowing cross-institutional comparisons, and as an image
reverse search engine for spatial patterns. Together, these results position
KRONOS as a flexible and scalable tool for spatial proteomics. The model is
publicly accessible at https://github.com/mahmoodlab/KRONOS.

</details>


### [279] [Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video Reasoning](https://arxiv.org/abs/2506.03525)
*Daeun Lee,Jaehong Yoon,Jaemin Cho,Mohit Bansal*

Main category: cs.CV

TL;DR: 提出Video-SKoT框架，通过技能感知的CoT监督提升视频理解能力，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有CoT推理方法在适应不同视频内容的领域特定技能（如事件检测、空间关系理解等）时表现不佳。

Method: 构建技能相关的CoT标注，并引入技能特定的专家学习框架，每个专家模块专注于部分推理技能。

Result: 在三个视频理解基准测试中，Video-SKoT均优于基线方法，并提供了不同CoT标注流程和技能的深入分析。

Conclusion: Video-SKoT通过自动构建技能感知的CoT监督，有效提升了领域自适应视频推理能力。

Abstract: Recent advances in Chain-of-Thought (CoT) reasoning have improved complex
video understanding, but existing methods often struggle to adapt to
domain-specific skills (e.g., event detection, spatial relation understanding,
emotion understanding) over various video content. To address this, we propose
Video-Skill-CoT (a.k.a. Video-SKoT), a framework that automatically constructs
and leverages skill-aware CoT supervisions for domain-adaptive video reasoning.
First, we construct skill-based CoT annotations: we extract domain-relevant
reasoning skills from training questions, cluster them into a shared skill
taxonomy, and create detailed multi-step CoT rationale tailored to each
video-question pair for training. Second, we introduce a skill-specific expert
learning framework. Each expert module specializes in a subset of reasoning
skills and is trained with lightweight adapters using the collected CoT
supervision. We demonstrate the effectiveness of the proposed approach on three
video understanding benchmarks, where Video-SKoT consistently outperforms
strong baselines. We also provide in-depth analyses on comparing different CoT
annotation pipelines and learned skills over multiple video domains.

</details>


### [280] [DiagNet: Detecting Objects using Diagonal Constraints on Adjacency Matrix of Graph Neural Network](https://arxiv.org/abs/2506.03571)
*Chong Hyun Lee,Kibae Lee*

Main category: cs.CV

TL;DR: 提出DaigNet，一种基于图卷积网络对角约束的目标检测新方法，无需预设锚框，性能超越多个YOLO版本。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测方法依赖预设锚框设计，存在局限性。本文提出通过图卷积网络邻接矩阵的对角约束实现更灵活的检测。

Method: 提出硬约束和软约束两种对角化算法，结合对角约束和互补约束的损失函数，并集成YOLO检测头。

Result: 在Pascal VOC上mAP50比YOLOv1高7.5%；在MS COCO上mAP分别超越YOLOv3u/YOLOv5u/YOLOv8达5.1%/3.7%/2.9%。

Conclusion: DaigNet通过图网络对角约束有效替代锚框机制，在多个基准测试中显著提升检测精度。

Abstract: We propose DaigNet, a new approach to object detection with which we can
detect an object bounding box using diagonal constraints on adjacency matrix of
a graph convolutional network (GCN). We propose two diagonalization algorithms
based on hard and soft constraints on adjacency matrix and two loss functions
using diagonal constraint and complementary constraint. The DaigNet eliminates
the need for designing a set of anchor boxes commonly used. To prove
feasibility of our novel detector, we adopt detection head in YOLO models.
Experiments show that the DiagNet achieves 7.5% higher mAP50 on Pascal VOC than
YOLOv1. The DiagNet also shows 5.1% higher mAP on MS COCO than YOLOv3u, 3.7%
higher mAP than YOLOv5u, and 2.9% higher mAP than YOLOv8.

</details>


### [281] [ViTSGMM: A Robust Semi-Supervised Image Recognition Network Using Sparse Labels](https://arxiv.org/abs/2506.03582)
*Rui Yann,Xianglei Xing*

Main category: cs.CV

TL;DR: ViTSGMM是一种高效的半监督学习图像识别网络，通过优化特征表示与目标类别间的互信息构建分层混合密度分类决策机制，在极少量标注数据下实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖复杂训练技术和架构，但在处理极有限标注数据时泛化能力不足。本文旨在解决这一问题。

Method: 构建分层混合密度分类决策机制，优化特征与类别的互信息，压缩冗余信息同时保留关键判别成分。

Result: 在STL-10和CIFAR-10/100数据集上使用极少标注样本达到SOTA性能，并发现并修复了STL-10数据集中的数据泄漏问题。

Conclusion: ViTSGMM高效解决了半监督学习在有限标注数据下的挑战，同时揭示了数据集中的潜在问题以确保实验可靠性。

Abstract: We present ViTSGMM, an image recognition network that leverages
semi-supervised learning in a highly efficient manner. Existing works often
rely on complex training techniques and architectures, while their
generalization ability when dealing with extremely limited labeled data remains
to be improved. To address these limitations, we construct a hierarchical
mixture density classification decision mechanism by optimizing mutual
information between feature representations and target classes, compressing
redundant information while retaining crucial discriminative components.
Experimental results demonstrate that our method achieves state-of-the-art
performance on STL-10 and CIFAR-10/100 datasets when using negligible labeled
samples. Notably, this paper also reveals a long-overlooked data leakage issue
in the STL-10 dataset for semi-supervised learning tasks and removes duplicates
to ensure the reliability of experimental results. Code available at
https://github.com/Shu1L0n9/ViTSGMM.

</details>


### [282] [BiMa: Towards Biases Mitigation for Text-Video Retrieval via Scene Element Guidance](https://arxiv.org/abs/2506.03589)
*Huy Le,Nhat Chung,Tung Kieu,Anh Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: 论文提出BiMa框架，通过视觉和文本去偏方法提升文本-视频检索性能，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有文本-视频检索系统受数据集中视觉-语言偏差影响，导致预训练模型忽略关键细节，需解决这一问题。

Method: BiMa框架通过生成视频场景元素增强视觉嵌入，并解耦文本特征为内容和偏差成分以实现去偏。

Result: 在五大TVR基准测试（如MSR-VTT等）和分布外检索任务中，BiMa展现出竞争性性能和持续的去偏能力。

Conclusion: BiMa有效缓解了视觉和文本表示中的偏差，显著提升了文本-视频检索的准确性和鲁棒性。

Abstract: Text-video retrieval (TVR) systems often suffer from visual-linguistic biases
present in datasets, which cause pre-trained vision-language models to overlook
key details. To address this, we propose BiMa, a novel framework designed to
mitigate biases in both visual and textual representations. Our approach begins
by generating scene elements that characterize each video by identifying
relevant entities/objects and activities. For visual debiasing, we integrate
these scene elements into the video embeddings, enhancing them to emphasize
fine-grained and salient details. For textual debiasing, we introduce a
mechanism to disentangle text features into content and bias components,
enabling the model to focus on meaningful content while separately handling
biased information. Extensive experiments and ablation studies across five
major TVR benchmarks (i.e., MSR-VTT, MSVD, LSMDC, ActivityNet, and DiDeMo)
demonstrate the competitive performance of BiMa. Additionally, the model's bias
mitigation capability is consistently validated by its strong results on
out-of-distribution retrieval tasks.

</details>


### [283] [VLMs Can Aggregate Scattered Training Patches](https://arxiv.org/abs/2506.03614)
*Zhanhui Zhou,Lingjie Chen,Chao Yang,Chaochao Lu*

Main category: cs.CV

TL;DR: 视觉语言模型（VLMs）可能通过视觉拼接技术绕过数据审核，将分散的良性图像片段组合成有害内容，带来安全隐患。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型的数据审核机制存在漏洞，攻击者可能通过将有害图像分割成看似无害的小块并分散在训练数据中，绕过审核。模型在训练过程中可能学会将这些片段拼接起来，从而在推理时生成有害内容。

Method: 研究首先在开源VLMs上验证视觉拼接能力，通过将带有唯一ID的图像分割成不同粒度的片段进行微调。随后模拟对抗性数据投毒场景，使用危险图像片段并替换ID为“安全”或“不安全”等描述，展示有害内容如何绕过审核。

Result: 实验证明，微调后的模型能够通过视觉拼接从完整图像或文本引用中正确识别ID，并成功重建绕过审核的有害内容，揭示了VLMs的安全风险。

Conclusion: 视觉拼接能力使VLMs能够绕过数据审核，重建有害内容，这对模型安全构成严重威胁，需要新的防御机制来应对此类攻击。

Abstract: One way to mitigate risks in vision-language models (VLMs) is to remove
dangerous samples in their training data. However, such data moderation can be
easily bypassed when harmful images are split into small, benign-looking
patches, scattered across many training samples. VLMs may then learn to piece
these fragments together during training and generate harmful responses at
inference, either from full images or text references. For instance, if trained
on image patches from a bloody scene paired with the descriptions "safe," VLMs
may later describe, the full image or a text reference to the scene, as "safe."
We define the core ability of VLMs enabling this attack as $\textit{visual
stitching}$ -- the ability to integrate visual information spread across
multiple training samples that share the same textual descriptions. In our
work, we first demonstrate visual stitching abilities in common open-source
VLMs on three datasets where each image is labeled with a unique synthetic ID:
we split each $(\texttt{image}, \texttt{ID})$ pair into $\{(\texttt{patch},
\texttt{ID})\}$ pairs at different granularity for finetuning, and we find that
tuned models can verbalize the correct IDs from full images or text reference.
Building on this, we simulate the adversarial data poisoning scenario mentioned
above by using patches from dangerous images and replacing IDs with text
descriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can
evade moderation in patches and later be reconstructed through visual
stitching, posing serious VLM safety risks. Code is available at
https://github.com/ZHZisZZ/visual-stitching.

</details>


### [284] [Negative-Guided Subject Fidelity Optimization for Zero-Shot Subject-Driven Generation](https://arxiv.org/abs/2506.03621)
*Chaehun Shin,Jooyoung Choi,Johan Barthelemy,Jungbeom Lee,Sungroh Yoon*

Main category: cs.CV

TL;DR: 提出SFO框架，通过对比学习提升零样本主题生成中的主题保真度，无需昂贵人工标注。


<details>
  <summary>Details</summary>
Motivation: 现有监督微调方法仅依赖正样本且使用预训练阶段的扩散损失，无法有效提升主题保真度。

Method: SFO引入合成负样本并通过成对比较优化模型，提出CDNS自动生成负样本，并重新加权扩散时间步以聚焦中间步骤。

Result: 实验表明SFO在主题保真度和文本对齐上显著优于基线方法。

Conclusion: SFO框架有效提升了零样本主题生成的性能，尤其在主题保真度方面表现突出。

Abstract: We present Subject Fidelity Optimization (SFO), a novel comparative learning
framework for zero-shot subject-driven generation that enhances subject
fidelity. Beyond supervised fine-tuning methods that rely only on positive
targets and use the diffusion loss as in the pre-training stage, SFO introduces
synthetic negative targets and explicitly guides the model to favor positives
over negatives through pairwise comparison. For negative targets, we propose
Condition-Degradation Negative Sampling (CDNS), which automatically generates
distinctive and informative negatives by intentionally degrading visual and
textual cues without expensive human annotations. Moreover, we reweight the
diffusion timesteps to focus finetuning on intermediate steps where subject
details emerge. Extensive experiments demonstrate that SFO with CDNS
significantly outperforms baselines in terms of both subject fidelity and text
alignment on a subject-driven generation benchmark. Project page:
https://subjectfidelityoptimization.github.io/

</details>


### [285] [Farm-LightSeek: An Edge-centric Multimodal Agricultural IoT Data Analytics Framework with Lightweight LLMs](https://arxiv.org/abs/2506.03168)
*Dawen Jiang,Zhishu Shen,Qiushi Zheng,Tiehua Zhang,Wei Xiang,Jiong Jin*

Main category: cs.CV

TL;DR: 该论文提出Farm-LightSeek框架，通过整合大语言模型(LLM)与边缘计算，解决智能农业中多模态数据融合、动态环境适应及实时决策等挑战。


<details>
  <summary>Details</summary>
Motivation: 全球人口增长和气候变化背景下，传统农业物联网系统面临数据处理效率低、过度依赖专家知识、多模态数据融合困难、动态环境适应性差及边缘实时决策瓶颈等问题。

Method: 提出边缘中心化多模态农业物联网数据分析框架Farm-LightSeek，集成LLM与边缘计算，实现农田多源数据实时采集、跨模态推理、疾病检测及低延迟管理决策。

Result: 在两个真实数据集上的实验表明，Farm-LightSeek在边缘计算资源限制下仍能稳定完成关键任务，验证了框架的可靠性。

Conclusion: 该研究推动了农业实时智能解决方案发展，凸显了农业物联网与LLM深度融合的潜力。

Abstract: Amid the challenges posed by global population growth and climate change,
traditional agricultural Internet of Things (IoT) systems is currently
undergoing a significant digital transformation to facilitate efficient big
data processing. While smart agriculture utilizes artificial intelligence (AI)
technologies to enable precise control, it still encounters significant
challenges, including excessive reliance on agricultural expert knowledge,
difficulties in fusing multimodal data, poor adaptability to dynamic
environments, and bottlenecks in real-time decision-making at the edge. Large
language models (LLMs), with their exceptional capabilities in knowledge
acquisition and semantic understanding, provide a promising solution to address
these challenges. To this end, we propose Farm-LightSeek, an edge-centric
multimodal agricultural IoT data analytics framework that integrates LLMs with
edge computing. This framework collects real-time farmland multi-source data
(images, weather, geographic information) via sensors, performs cross-modal
reasoning and disease detection at edge nodes, conducts low-latency management
decisions, and enables cloud collaboration for model updates. The main
innovations of Farm-LightSeek include: (1) an agricultural
"perception-decision-action" closed-loop architecture; (2) cross-modal adaptive
monitoring; and (3)a lightweight LLM deployment strategy balancing performance
and efficiency. Experiments conducted on two real-world datasets demonstrate
that Farm-LightSeek consistently achieves reliable performance in
mission-critical tasks, even under the limitations of edge computing resources.
This work advances intelligent real-time agricultural solutions and highlights
the potential for deeper integration of agricultural IoT with LLMs.

</details>


### [286] [Spatial Understanding from Videos: Structured Prompts Meet Simulation Data](https://arxiv.org/abs/2506.03642)
*Haoyu Zhang,Meng Liu,Zaijing Li,Haokun Wen,Weili Guan,Yaowei Wang,Liqiang Nie*

Main category: cs.CV

TL;DR: 提出了一种增强预训练视觉语言模型3D空间推理能力的统一框架，结合结构化提示策略和自动化构建的问答数据集，有效解决了现有方法的空间不确定性和数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视觉空间理解中存在空间不确定性和数据稀缺问题，限制了预训练视觉语言模型的3D空间推理能力。

Method: 提出统一框架，结合SpatialMind（结构化提示策略）和ScanForgeQA（自动化构建的问答数据集），无需修改模型架构即可增强3D空间推理能力。

Result: 多基准测试表明，提示和微调策略单独及组合均有效，为视觉空间理解的未来研究提供了启发。

Conclusion: 该框架显著提升了预训练视觉语言模型的3D空间推理能力，并为相关领域的研究提供了新思路。

Abstract: Visual-spatial understanding, the ability to infer object relationships and
layouts from visual input, is fundamental to downstream tasks such as robotic
navigation and embodied interaction. However, existing methods face spatial
uncertainty and data scarcity, limiting the 3D spatial reasoning capability of
pre-trained vision-language models (VLMs). To address these challenges, we
present a unified framework for enhancing 3D spatial reasoning in pre-trained
VLMs without modifying their architecture. This framework combines SpatialMind,
a structured prompting strategy that decomposes complex scenes and questions
into interpretable reasoning steps, with ScanForgeQA, a scalable
question-answering dataset built from diverse 3D simulation scenes through an
automated construction process designed for fine-tuning. Extensive experiments
across multiple benchmarks demonstrate the individual and combined
effectiveness of our prompting and fine-tuning strategies, and yield insights
that may inspire future research on visual-spatial understanding.

</details>


### [287] [MambaNeXt-YOLO: A Hybrid State Space Model for Real-time Object Detection](https://arxiv.org/abs/2506.03654)
*Xiaochun Lei,Siqi Wu,Weilin Wu,Zetao Jiang*

Main category: cs.CV

TL;DR: 提出MambaNeXt-YOLO，结合CNN与Mamba实现高效实时目标检测，兼顾准确性与速度。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer架构因自注意力机制计算复杂度高，难以在实时和边缘设备部署，需寻找更高效方案。

Method: 1) 设计混合CNN与Mamba的MambaNeXt块；2) 提出多分支非对称融合金字塔网络(MAFPN)；3) 优化边缘设备部署效率。

Result: 在PASCAL VOC上达到66.6% mAP/31.9 FPS，支持Jetson Xavier/Orin NX等边缘设备。

Conclusion: MambaNeXt-YOLO通过线性复杂度建模与多尺度融合，实现了精度与效率的平衡。

Abstract: Real-time object detection is a fundamental but challenging task in computer
vision, particularly when computational resources are limited. Although
YOLO-series models have set strong benchmarks by balancing speed and accuracy,
the increasing need for richer global context modeling has led to the use of
Transformer-based architectures. Nevertheless, Transformers have high
computational complexity because of their self-attention mechanism, which
limits their practicality for real-time and edge deployments. To overcome these
challenges, recent developments in linear state space models, such as Mamba,
provide a promising alternative by enabling efficient sequence modeling with
linear complexity. Building on this insight, we propose MambaNeXt-YOLO, a novel
object detection framework that balances accuracy and efficiency through three
key contributions: (1) MambaNeXt Block: a hybrid design that integrates CNNs
with Mamba to effectively capture both local features and long-range
dependencies; (2) Multi-branch Asymmetric Fusion Pyramid Network (MAFPN): an
enhanced feature pyramid architecture that improves multi-scale object
detection across various object sizes; and (3) Edge-focused Efficiency: our
method achieved 66.6\% mAP at 31.9 FPS on the PASCAL VOC dataset without any
pre-training and supports deployment on edge devices such as the NVIDIA Jetson
Xavier NX and Orin NX.

</details>


### [288] [Accelerating SfM-based Pose Estimation with Dominating Set](https://arxiv.org/abs/2506.03667)
*Joji Joseph,Bharadwaj Amrutur,Shalabh Bhatnagar*

Main category: cs.CV

TL;DR: 本文提出了一种基于图论支配集的预处理技术，显著加速了SfM姿态估计过程，在保持精度的同时提升处理速度1.5-14.48倍，适用于AR/VR等实时应用。


<details>
  <summary>Details</summary>
Motivation: 针对增强现实(AR)、虚拟现实(VR)和机器人等实时应用中，基于SfM的姿态估计计算效率不足的问题，需要一种能平衡速度与精度的解决方案。

Method: 利用图论中的支配集概念对SfM模型进行预处理，通过减少参考图像和点云数据量来优化计算流程。

Result: 在OnePose数据集上的实验表明：处理速度提升1.5-14.48倍，参考图像数量减少17-23倍，点云规模缩小2.27-4倍。

Conclusion: 该方法为实时3D姿态估计提供了高效准确的解决方案，成功实现了速度与精度的平衡。

Abstract: This paper introduces a preprocessing technique to speed up
Structure-from-Motion (SfM) based pose estimation, which is critical for
real-time applications like augmented reality (AR), virtual reality (VR), and
robotics. Our method leverages the concept of a dominating set from graph
theory to preprocess SfM models, significantly enhancing the speed of the pose
estimation process without losing significant accuracy. Using the OnePose
dataset, we evaluated our method across various SfM-based pose estimation
techniques. The results demonstrate substantial improvements in processing
speed, ranging from 1.5 to 14.48 times, and a reduction in reference images and
point cloud size by factors of 17-23 and 2.27-4, respectively. This work offers
a promising solution for efficient and accurate 3D pose estimation, balancing
speed and accuracy in real-time applications.

</details>


### [289] [TerraIncognita: A Dynamic Benchmark for Species Discovery Using Frontier Models](https://arxiv.org/abs/2506.03182)
*Shivani Chiranjeevi,Hossein Zaremehrjerdi,Zi K. Deng,Talukder Z. Jubery,Ari Grele,Arti Singh,Asheesh K Singh,Soumik Sarkar,Nirav Merchant,Harold F. Greeney,Baskar Ganapathysubramanian,Chinmay Hegde*

Main category: cs.CV

TL;DR: 提出TerraIncognita基准测试，评估多模态模型在识别未知昆虫物种方面的能力，旨在解决生物多样性快速丧失的生态危机。


<details>
  <summary>Details</summary>
Motivation: 全球生物多样性快速丧失，尤其是昆虫物种，当前昆虫物种发现方法效率低下，阻碍了及时的保护行动。

Method: 引入TerraIncognita动态基准测试，结合已知和稀有昆虫物种的专家标注图像，评估模型在分类、检测未知物种及生成解释方面的能力。

Result: 顶级模型在已知物种的目级别分类上F1超过90%，但在物种级别降至2%以下，显示从粗到细分类的难度梯度。

Conclusion: TerraIncognita将定期更新，为前沿AI方法提供持续演进的纵向基准测试平台。

Abstract: The rapid global loss of biodiversity, particularly among insects, represents
an urgent ecological crisis. Current methods for insect species discovery are
manual, slow, and severely constrained by taxonomic expertise, hindering timely
conservation actions. We introduce TerraIncognita, a dynamic benchmark designed
to evaluate state-of-the-art multimodal models for the challenging problem of
identifying unknown, potentially undescribed insect species from image data.
Our benchmark dataset combines a mix of expertly annotated images of insect
species likely known to frontier AI models, and images of rare and poorly known
species, for which few/no publicly available images exist. These images were
collected from underexplored biodiversity hotspots, realistically mimicking
open-world discovery scenarios faced by ecologists. The benchmark assesses
models' proficiency in hierarchical taxonomic classification, their capability
to detect and abstain from out-of-distribution (OOD) samples representing novel
species, and their ability to generate explanations aligned with expert
taxonomic knowledge. Notably, top-performing models achieve over 90\% F1 at the
Order level on known species, but drop below 2\% at the Species level,
highlighting the sharp difficulty gradient from coarse to fine taxonomic
prediction (Order $\rightarrow$ Family $\rightarrow$ Genus $\rightarrow$
Species). TerraIncognita will be updated regularly, and by committing to
quarterly dataset expansions (of both known and novel species), will provide an
evolving platform for longitudinal benchmarking of frontier AI methods. All
TerraIncognita data, results, and future updates are available
\href{https://baskargroup.github.io/TerraIncognita/}{here}.

</details>


### [290] [How PARTs assemble into wholes: Learning the relative composition of images](https://arxiv.org/abs/2506.03682)
*Melika Ayoughi,Samira Abnar,Chen Huang,Chris Sandino,Sayeri Lala,Eeshan Gunesh Dhekane,Dan Busbridge,Shuangfei Zhai,Vimal Thilak,Josh Susskind,Pascal Mettes,Paul Groth,Hanlin Goh*

Main category: cs.CV

TL;DR: 论文提出了一种名为PART的自监督学习方法，通过连续相对变换学习图像部分的相对组合，克服了传统网格方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于网格的自监督学习方法无法捕捉真实世界中物体组合的流动性和连续性，限制了表示学习的效果。

Method: PART利用非网格补丁之间的连续相对变换，建模部分之间的相对关系，实现连续空间中的结构相对定位。

Result: 在需要精确空间理解的任务（如目标检测和时间序列预测）中，PART优于MAE和DropPos等网格方法，同时在全局分类任务中保持竞争力。

Conclusion: PART突破了网格限制，为跨数据类型的通用自监督预训练开辟了新途径，在视频、医学影像和音频等领域具有潜力。

Abstract: The composition of objects and their parts, along with object-object
positional relationships, provides a rich source of information for
representation learning. Hence, spatial-aware pretext tasks have been actively
explored in self-supervised learning. Existing works commonly start from a grid
structure, where the goal of the pretext task involves predicting the absolute
position index of patches within a fixed grid. However, grid-based approaches
fall short of capturing the fluid and continuous nature of real-world object
compositions. We introduce PART, a self-supervised learning approach that
leverages continuous relative transformations between off-grid patches to
overcome these limitations. By modeling how parts relate to each other in a
continuous space, PART learns the relative composition of images-an off-grid
structural relative positioning process that generalizes beyond occlusions and
deformations. In tasks requiring precise spatial understanding such as object
detection and time series prediction, PART outperforms strong grid-based
methods like MAE and DropPos, while also maintaining competitive performance on
global classification tasks with minimal hyperparameter tuning. By breaking
free from grid constraints, PART opens up an exciting new trajectory for
universal self-supervised pretraining across diverse datatypes-from natural
images to EEG signals-with promising potential in video, medical imaging, and
audio.

</details>


### [291] [OSGNet @ Ego4D Episodic Memory Challenge 2025](https://arxiv.org/abs/2506.03710)
*Yisen Feng,Haoyu Zhang,Qiaohui Chu,Meng Liu,Weili Guan,Yaowei Wang,Liqiang Nie*

Main category: cs.CV

TL;DR: 本文介绍了在CVPR 2025 Ego4D挑战赛中夺冠的早期融合视频定位方法，显著提升了定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有视频定位方法多依赖后期融合策略，效果不佳，需改进以提升定位准确性。

Method: 采用早期融合的视频定位模型，统一处理三个任务，优化定位过程。

Result: 在自然语言查询、目标步骤和时刻查询三个赛道中均获得第一名。

Conclusion: 早期融合策略有效提升了视频定位的准确性，证明了方法的优越性。

Abstract: In this report, we present our champion solutions for the three egocentric
video localization tracks of the Ego4D Episodic Memory Challenge at CVPR 2025.
All tracks require precise localization of the interval within an untrimmed
egocentric video. Previous unified video localization approaches often rely on
late fusion strategies, which tend to yield suboptimal results. To address
this, we adopt an early fusion-based video localization model to tackle all
three tasks, aiming to enhance localization accuracy. Ultimately, our method
achieved first place in the Natural Language Queries, Goal Step, and Moment
Queries tracks, demonstrating its effectiveness. Our code can be found at
https://github.com/Yisen-Feng/OSGNet.

</details>


### [292] [Human Fall Detection using Transfer Learning-based 3D CNN](https://arxiv.org/abs/2506.03193)
*Ekram Alam,Abu Sufian,Paramartha Dutta,Marco Leo*

Main category: cs.CV

TL;DR: 该论文提出了一种基于预训练3D CNN的视觉跌倒检测系统，通过提取时空特征并结合SVM分类器，有效识别老年人跌倒行为。


<details>
  <summary>Details</summary>
Motivation: 老年人意外跌倒是一个严重的健康问题，随着老年人口增加，需要自动化的跌倒检测监控系统来及时应对。

Method: 使用预训练的3D CNN模型（基于Sports1M数据集）提取时空特征，仅训练SVM分类器进行分类，采用分层五折交叉验证划分数据集。

Result: 在GMDCSA和CAUCAFall两个数据集上进行了实验，验证了方法的有效性。

Conclusion: 该方法通过利用预训练3D CNN和SVM分类器，实现了高效的跌倒检测，为老年人健康监控提供了可行方案。

Abstract: Unintentional or accidental falls are one of the significant health issues in
senior persons. The population of senior persons is increasing steadily. So,
there is a need for an automated fall detection monitoring system. This paper
introduces a vision-based fall detection system using a pre-trained 3D CNN.
Unlike 2D CNN, 3D CNN extracts not only spatial but also temporal features. The
proposed model leverages the original learned weights of a 3D CNN model
pre-trained on the Sports1M dataset to extract the spatio-temporal features.
Only the SVM classifier was trained, which saves the time required to train the
3D CNN. Stratified shuffle five split cross-validation has been used to split
the dataset into training and testing data. Extracted features from the
proposed 3D CNN model were fed to an SVM classifier to classify the activity as
fall or ADL. Two datasets, GMDCSA and CAUCAFall, were utilized to conduct the
experiment. The source code for this work can be accessed via the following
link: https://github.com/ekramalam/HFD_3DCNN.

</details>


### [293] [ComRoPE: Scalable and Robust Rotary Position Embedding Parameterized by Trainable Commuting Angle Matrices](https://arxiv.org/abs/2506.03737)
*Hao Yu,Tangyu Jiang,Shuning Jia,Shannan Yan,Shunning Liu,Haolong Qian,Guanghao Li,Shuting Dong,Huaisong Zhang,Chun Yuan*

Main category: cs.CV

TL;DR: 本文提出ComRoPE方法，通过可训练的交换角度矩阵改进RoPE位置编码，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统位置编码方法因缺乏鲁棒性和灵活性存在局限，RoPE虽改进但仍受限于手动定义的旋转矩阵。

Method: 提出ComRoPE，基于可训练的交换角度矩阵泛化RoPE，确保位置偏移下的性能一致性。

Result: 在ImageNet-1K上，训练分辨率和高分辨率分别超越SOTA方法1.6%和2.9%。

Conclusion: ComRoPE框架具有通用性，为未来位置编码研究提供了新思路。

Abstract: The Transformer architecture has revolutionized various regions since it was
proposed, and its effectiveness largely depends on the ability to encode
positional information. Traditional position encoding methods exhibit
significant limitations due to lack of robustness and flexibility of position.
Therefore, Rotary Positional Encoding (RoPE) was proposed to alleviate these
issues, which integrates positional information by rotating the embeddings in
the attention mechanism. However, RoPE requires manually defined rotation
matrices with limited transformation space, constraining the model's capacity.
In this work, we propose ComRoPE, which generalizes RoPE by defining it in
terms of trainable commuting angle matrices. Specifically, we demonstrate that
pairwise commutativity of these matrices is essential for RoPE to achieve
scalability and positional robustness. We formally define the RoPE Equation,
which is an essential condition that ensures consistent performance with
position offsets. Based on the theoretical analysis, we present two types of
trainable commuting angle matrices as sufficient solutions to the RoPE
equation, which significantly improve performance, surpassing the current
state-of-the-art method by 1.6% at training resolution and 2.9% at higher
resolution on the ImageNet-1K dataset. Furthermore, our framework shows
versatility in generalizing to existing RoPE formulations and offering new
insights for future positional encoding research. To ensure reproducibility,
the source code and instructions are available at
https://github.com/Longin-Yu/ComRoPE

</details>


### [294] [SAAT: Synergistic Alternating Aggregation Transformer for Image Super-Resolution](https://arxiv.org/abs/2506.03740)
*Jianfeng Wu,Nannan Xu*

Main category: cs.CV

TL;DR: 提出SAAT模型，通过协同通道与空间注意力提升超分辨率性能，达到SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer超分辨率方法因计算成本限制采用非重叠窗口自注意力，忽略了通道间信息与中间过程的丰富空间结构信息。通道与空间注意力的协同关系尚未充分探索。

Method: 提出SAAT模型，包含CWSAG（高效通道与窗口协同注意力组）和SWSAG（空间与窗口协同注意力组），分别增强非局部特征融合和结构化特征提取。

Result: 实验证明SAAT在超分辨率任务中有效，参数量相同下性能媲美SOTA。

Conclusion: SAAT通过协同通道与空间注意力机制，显著提升了超分辨率图像的质量与结构特征还原能力。

Abstract: Single image super-resolution is a well-known downstream task which aims to
restore low-resolution images into high-resolution images. At present, models
based on Transformers have shone brightly in the field of super-resolution due
to their ability to capture long-term dependencies in information. However,
current methods typically compute self-attention in nonoverlapping windows to
save computational costs, and the standard self-attention computation only
focuses on its results, thereby neglecting the useful information across
channels and the rich spatial structural information generated in the
intermediate process. Channel attention and spatial attention have,
respectively, brought significant improvements to various downstream visual
tasks in terms of extracting feature dependency and spatial structure
relationships, but the synergistic relationship between channel and spatial
attention has not been fully explored yet.To address these issues, we propose a
novel model. Synergistic Alternating Aggregation Transformer (SAAT), which can
better utilize the potential information of features. In SAAT, we introduce the
Efficient Channel & Window Synergistic Attention Group (CWSAG) and the Spatial
& Window Synergistic Attention Group (SWSAG). On the one hand, CWSAG combines
efficient channel attention with shifted window attention, enhancing non-local
feature fusion, and producing more visually appealing results. On the other
hand, SWSAG leverages spatial attention to capture rich structured feature
information, thereby enabling SAAT to more effectively extract structural
features.Extensive experimental results and ablation studies demonstrate the
effectiveness of SAAT in the field of super-resolution. SAAT achieves
performance comparable to that of the state-of-the-art (SOTA) under the same
quantity of parameters.

</details>


### [295] [JointSplat: Probabilistic Joint Flow-Depth Optimization for Sparse-View Gaussian Splatting](https://arxiv.org/abs/2506.03872)
*Yang Xiao,Guoan Xu,Qiang Wu,Wenjing Jia*

Main category: cs.CV

TL;DR: JointSplat提出了一种结合光流与深度的概率优化框架，通过多视角深度一致性损失提升稀疏视角3D重建质量，在RealEstate10K和ACID数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏视角3D重建方法中，基于多视角深度估计的方法在低纹理/重复区域易错位，而光流-深度联合估计方法因缺乏真实光流监督易产生噪声和全局不一致。需要结合两者优势解决各自缺陷。

Method: 提出JointSplat框架：1) 基于光流匹配概率的概率优化机制动态融合深度与光流信息；2) 设计多视角深度一致性损失，在不确定区域抑制误导性梯度。

Result: 在RealEstate10K和ACID数据集上均超越SOTA方法，验证了所提概率联合优化机制对高保真稀疏重建的有效性和鲁棒性。

Conclusion: 通过光流与深度的概率化互补融合及新型损失函数，实现了更高质量的稀疏视角3D场景重建。

Abstract: Reconstructing 3D scenes from sparse viewpoints is a long-standing challenge
with wide applications. Recent advances in feed-forward 3D Gaussian sparse-view
reconstruction methods provide an efficient solution for real-time novel view
synthesis by leveraging geometric priors learned from large-scale multi-view
datasets and computing 3D Gaussian centers via back-projection. Despite
offering strong geometric cues, both feed-forward multi-view depth estimation
and flow-depth joint estimation face key limitations: the former suffers from
mislocation and artifact issues in low-texture or repetitive regions, while the
latter is prone to local noise and global inconsistency due to unreliable
matches when ground-truth flow supervision is unavailable. To overcome this, we
propose JointSplat, a unified framework that leverages the complementarity
between optical flow and depth via a novel probabilistic optimization
mechanism. Specifically, this pixel-level mechanism scales the information
fusion between depth and flow based on the matching probability of optical flow
during training. Building upon the above mechanism, we further propose a novel
multi-view depth-consistency loss to leverage the reliability of supervision
while suppressing misleading gradients in uncertain areas. Evaluated on
RealEstate10K and ACID, JointSplat consistently outperforms state-of-the-art
(SOTA) methods, demonstrating the effectiveness and robustness of our proposed
probabilistic joint flow-depth optimization approach for high-fidelity
sparse-view 3D reconstruction.

</details>


### [296] [DiffCAP: Diffusion-based Cumulative Adversarial Purification for Vision Language Models](https://arxiv.org/abs/2506.03933)
*Jia Fu,Yongtao Wu,Yihang Chen,Kunyu Peng,Xiao Zhang,Volkan Cevher,Sepideh Pashami,Anders Holst*

Main category: cs.CV

TL;DR: 本文提出DiffCAP，一种基于扩散模型的净化策略，能有效中和视觉语言模型中的对抗性扰动，显著提升模型在对抗环境下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）虽在多模态理解中表现卓越，但对扰动的敏感性威胁其在实际应用中的可靠性。人类难以察觉的微小扰动可能导致模型输出严重错误。

Method: DiffCAP通过逐步注入高斯噪声至对抗样本，直至连续噪声图像的嵌入相似度达到阈值，随后用预训练扩散模型去噪，恢复干净图像表示。

Result: 在六大数据集、三种VLMs及多种攻击强度下的实验表明，DiffCAP显著优于现有防御方法，同时降低了超参数调优复杂性和去噪时间。

Conclusion: DiffCAP为对抗环境中安全部署VLMs提供了强理论支撑和实用解决方案，兼具高效性与鲁棒性。

Abstract: Vision Language Models (VLMs) have shown remarkable capabilities in
multimodal understanding, yet their susceptibility to perturbations poses a
significant threat to their reliability in real-world applications. Despite
often being imperceptible to humans, these perturbations can drastically alter
model outputs, leading to erroneous interpretations and decisions. This paper
introduces DiffCAP, a novel diffusion-based purification strategy that can
effectively neutralize adversarial corruptions in VLMs. We observe that adding
minimal noise to an adversarially corrupted image significantly alters its
latent embedding with respect to VLMs. Building on this insight, DiffCAP
cumulatively injects random Gaussian noise into adversarially perturbed input
data. This process continues until the embeddings of two consecutive noisy
images reach a predefined similarity threshold, indicating a potential approach
to neutralize the adversarial effect. Subsequently, a pretrained diffusion
model is employed to denoise the stabilized image, recovering a clean
representation suitable for the VLMs to produce an output. Through extensive
experiments across six datasets with three VLMs under varying attack strengths
in three task scenarios, we show that DiffCAP consistently outperforms existing
defense techniques by a substantial margin. Notably, DiffCAP significantly
reduces both hyperparameter tuning complexity and the required diffusion time,
thereby accelerating the denoising process. Equipped with strong theoretical
and empirical support, DiffCAP provides a robust and practical solution for
securely deploying VLMs in adversarial environments.

</details>


### [297] [Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization](https://arxiv.org/abs/2506.04039)
*Jiulong Wu,Zhengliang Shi,Shuaiqiang Wang,Jizhou Huang,Dawei Yin,Lingyong Yan,Min Cao,Min Zhang*

Main category: cs.CV

TL;DR: 论文提出EMPO方法，通过实体中心多模态偏好优化增强模态对齐，减少LVLMs的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有偏好对齐方法忽视图像-文本模态对齐，导致过度依赖LLMs和幻觉问题，需改进。

Method: 利用开源指令数据集自动构建高质量多模态偏好数据，聚焦图像、指令和响应三方面。

Result: 在多个基准测试中显著降低幻觉率，如Object-HalBench降低85.9%，MM-HalBench降低49.8%。

Conclusion: EMPO有效提升模态对齐并减少幻觉，为LVLMs的可信度提供解决方案。

Abstract: Large Visual Language Models (LVLMs) have demonstrated impressive
capabilities across multiple tasks. However, their trustworthiness is often
challenged by hallucinations, which can be attributed to the modality
misalignment and the inherent hallucinations of their underlying Large Language
Models (LLMs) backbone. Existing preference alignment methods focus on aligning
model responses with human preferences while neglecting image-text modality
alignment, resulting in over-reliance on LLMs and hallucinations. In this
paper, we propose Entity-centric Multimodal Preference Optimization (EMPO),
which achieves enhanced modality alignment than existing human preference
alignment methods. Besides, to overcome the scarcity of high-quality multimodal
preference data, we utilize open-source instruction datasets to automatically
construct high-quality preference data across three aspects: image,
instruction, and response. Experiments on two human preference datasets and
five multimodal hallucination benchmarks demonstrate the effectiveness of EMPO,
e.g., reducing hallucination rates by 85.9% on Object-HalBench and 49.8% on
MM-HalBench.

</details>


### [298] [MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in Videos](https://arxiv.org/abs/2506.04141)
*Kejian Zhu,Zhuoran Jin,Hongbang Yuan,Jiachun Li,Shangqing Tu,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CV

TL;DR: 该论文提出了MMR-V基准测试，用于评估多模态大语言模型在视频中的深度推理能力，发现现有模型在多帧证据定位和推理方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有视频基准测试主要关注理解任务，而缺乏对多帧长距离推理和隐藏信息推理能力的评估，因此需要一个新的基准测试来填补这一空白。

Method: 提出MMR-V基准测试，包含317个视频和1,257个任务，要求模型进行长距离多帧推理、超越感知的隐藏信息推理，并通过人工标注和干扰策略确保可靠性。

Result: 实验显示当前模型在多模态推理上表现不佳，最佳模型o4-mini准确率仅52.5%，且现有推理增强策略（如Chain-of-Thought）效果有限。

Conclusion: MMR-V基准测试揭示了当前模型在多模态推理上的不足，并希望激发进一步研究以提升多模态推理能力。

Abstract: The sequential structure of videos poses a challenge to the ability of
multimodal large language models (MLLMs) to locate multi-frame evidence and
conduct multimodal reasoning. However, existing video benchmarks mainly focus
on understanding tasks, which only require models to match frames mentioned in
the question (hereafter referred to as "question frame") and perceive a few
adjacent frames. To address this gap, we propose MMR-V: A Benchmark for
Multimodal Deep Reasoning in Videos. The benchmark is characterized by the
following features. (1) Long-range, multi-frame reasoning: Models are required
to infer and analyze evidence frames that may be far from the question frame.
(2) Beyond perception: Questions cannot be answered through direct perception
alone but require reasoning over hidden information. (3) Reliability: All tasks
are manually annotated, referencing extensive real-world user understanding to
align with common perceptions. (4) Confusability: Carefully designed distractor
annotation strategies to reduce model shortcuts. MMR-V consists of 317 videos
and 1,257 tasks. Our experiments reveal that current models still struggle with
multi-modal reasoning; even the best-performing model, o4-mini, achieves only
52.5% accuracy. Additionally, current reasoning enhancement strategies
(Chain-of-Thought and scaling test-time compute) bring limited gains. Further
analysis indicates that the CoT demanded for multi-modal reasoning differs from
it in textual reasoning, which partly explains the limited performance gains.
We hope that MMR-V can inspire further research into enhancing multi-modal
reasoning capabilities.

</details>


### [299] [Intersectional Bias in Pre-Trained Image Recognition Models](https://arxiv.org/abs/2506.03664)
*Valerie Krug,Sebastian Stober*

Main category: cs.CV

TL;DR: 研究发现ImageNet分类器在人脸图像中编码了年龄、种族和性别的偏见，年龄区分最明显。


<details>
  <summary>Details</summary>
Motivation: 探讨预训练模型（如ImageNet分类器）在面部图像中可能编码的偏见，特别是年龄、种族和性别的交叉影响。

Method: 使用线性分类器探针和地形图可视化激活来评估模型中的偏见。

Result: ImageNet分类器能明显区分年龄，对种族和性别的区分较弱，尤其在中龄群体中。

Conclusion: 预训练模型存在偏见，需注意其在敏感变量上的潜在影响。

Abstract: Deep Learning models have achieved remarkable success. Training them is often
accelerated by building on top of pre-trained models which poses the risk of
perpetuating encoded biases. Here, we investigate biases in the representations
of commonly used ImageNet classifiers for facial images while considering
intersections of sensitive variables age, race and gender. To assess the
biases, we use linear classifier probes and visualize activations as
topographic maps. We find that representations in ImageNet classifiers
particularly allow differentiation between ages. Less strongly pronounced, the
models appear to associate certain ethnicities and distinguish genders in
middle-aged groups.

</details>


### [300] [Person Re-Identification System at Semantic Level based on Pedestrian Attributes Ontology](https://arxiv.org/abs/2506.04143)
*Ngoc Q. Ly,Hieu N. M. Cao,Thi T. Nguyen*

Main category: cs.CV

TL;DR: 该论文提出了一种统一的行人再识别系统，通过结合行人属性本体、局部多任务深度卷积网络和不平衡数据解决器，有效解决了属性不平衡和语义级局部特征利用不足的问题，并在Market1501数据集上取得了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 行人再识别在视频监控系统中具有重要意义，但目前仍面临大规模数据集、数据不平衡、视角变化、细粒度数据等挑战。特别是属性的不平衡问题以及局部特征在在线阶段未充分利用语义信息的问题尚未得到充分解决。

Method: 论文提出了一种统一的行人再识别系统，包含三个主要模块：行人属性本体（PAO）、局部多任务深度卷积网络（Local MDCNN）和不平衡数据解决器（IDS）。PAO用于语义级属性关联，Local MDCNN提取局部特征，IDS解决属性不平衡问题。

Result: 在Market1501数据集上的实验结果表明，该系统能够有效提升行人再识别的性能，优于一些现有的先进方法。

Conclusion: 通过结合PAO、Local MDCNN和IDS的相互支持，该系统能够有效利用属性间的内在关联，并在不调整网络架构和数据增强的情况下解决属性不平衡问题，展现了较高的性能。

Abstract: Person Re-Identification (Re-ID) is a very important task in video
surveillance systems such as tracking people, finding people in public places,
or analysing customer behavior in supermarkets. Although there have been many
works to solve this problem, there are still remaining challenges such as
large-scale datasets, imbalanced data, viewpoint, fine grained data
(attributes), the Local Features are not employed at semantic level in online
stage of Re-ID task, furthermore, the imbalanced data problem of attributes are
not taken into consideration. This paper has proposed a Unified Re-ID system
consisted of three main modules such as Pedestrian Attribute Ontology (PAO),
Local Multi-task DCNN (Local MDCNN), Imbalance Data Solver (IDS). The new main
point of our Re-ID system is the power of mutual support of PAO, Local MDCNN
and IDS to exploit the inner-group correlations of attributes and pre-filter
the mismatch candidates from Gallery set based on semantic information as
Fashion Attributes and Facial Attributes, to solve the imbalanced data of
attributes without adjusting network architecture and data augmentation. We
experimented on the well-known Market1501 dataset. The experimental results
have shown the effectiveness of our Re-ID system and it could achieve the
higher performance on Market1501 dataset in comparison to some state-of-the-art
Re-ID methods.

</details>


### [301] [RAID: A Dataset for Testing the Adversarial Robustness of AI-Generated Image Detectors](https://arxiv.org/abs/2506.03988)
*Hicham Eddoubi,Jonas Ricker,Federico Cocchi,Lorenzo Baraldi,Angelo Sotgiu,Maura Pintor,Marcella Cornia,Lorenzo Baraldi,Asja Fischer,Rita Cucchiara,Battista Biggio*

Main category: cs.CV

TL;DR: 该论文提出了RAID数据集，用于评估AI生成图像检测器的对抗鲁棒性，发现当前先进检测器易受对抗样本欺骗。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成图像质量提升，人类难以区分其与真实图像，导致欺诈和虚假信息风险增加。现有检测方法在理想条件下表现良好，但对抗鲁棒性常被忽视。

Method: 通过攻击7种先进检测器和4种文本生成图像模型，创建了包含72k对抗样本的RAID数据集，用于评估检测器的鲁棒性。

Result: 实验表明，生成的对抗样本能高效迁移到未知检测器，揭示当前检测器易受攻击，需开发更鲁棒的方法。

Conclusion: 研究强调了提升AI生成图像检测器对抗鲁棒性的紧迫性，并公开了数据集和评估代码以促进相关研究。

Abstract: AI-generated images have reached a quality level at which humans are
incapable of reliably distinguishing them from real images. To counteract the
inherent risk of fraud and disinformation, the detection of AI-generated images
is a pressing challenge and an active research topic. While many of the
presented methods claim to achieve high detection accuracy, they are usually
evaluated under idealized conditions. In particular, the adversarial robustness
is often neglected, potentially due to a lack of awareness or the substantial
effort required to conduct a comprehensive robustness analysis. In this work,
we tackle this problem by providing a simpler means to assess the robustness of
AI-generated image detectors. We present RAID (Robust evaluation of
AI-generated image Detectors), a dataset of 72k diverse and highly transferable
adversarial examples. The dataset is created by running attacks against an
ensemble of seven state-of-the-art detectors and images generated by four
different text-to-image models. Extensive experiments show that our methodology
generates adversarial images that transfer with a high success rate to unseen
detectors, which can be used to quickly provide an approximate yet still
reliable estimate of a detector's adversarial robustnessOur findings indicate
that current state-of-the-art AI-generated image detectors can be easily
deceived by adversarial examples, highlighting the critical need for the
development of more robust methods. We release our dataset at
https://huggingface.co/datasets/aimagelab/RAID and evaluation code at
https://github.com/pralab/RAID.

</details>


### [302] [Sounding that Object: Interactive Object-Aware Image to Audio Generation](https://arxiv.org/abs/2506.04214)
*Tingle Li,Baihe Huang,Xiaobin Zhuang,Dongya Jia,Jiawei Chen,Yuping Wang,Zhuo Chen,Gopala Anumanchipalli,Yuxuan Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种交互式对象感知音频生成模型，通过多模态注意力将图像区域与对应声音关联，用户可交互式选择对象生成声音，实验证明其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 复杂视听场景中，尤其是多对象和声源存在时，生成准确声音具有挑战性。论文旨在通过对象感知方法提升声音与视觉对象的对齐性。

Method: 结合对象中心学习与条件潜在扩散模型，利用多模态注意力关联图像区域和声音，测试时通过图像分割实现交互式对象级声音生成。

Result: 定量和定性评估表明，该模型优于基线方法，实现了对象与声音的更好对齐。理论验证了注意力机制与测试时分割掩模的功能近似。

Conclusion: 提出的交互式对象感知音频生成模型有效解决了复杂场景下的声音生成问题，通过对象级控制提升了生成声音的准确性和对齐性。

Abstract: Generating accurate sounds for complex audio-visual scenes is challenging,
especially in the presence of multiple objects and sound sources. In this
paper, we propose an {\em interactive object-aware audio generation} model that
grounds sound generation in user-selected visual objects within images. Our
method integrates object-centric learning into a conditional latent diffusion
model, which learns to associate image regions with their corresponding sounds
through multi-modal attention. At test time, our model employs image
segmentation to allow users to interactively generate sounds at the {\em
object} level. We theoretically validate that our attention mechanism
functionally approximates test-time segmentation masks, ensuring the generated
audio aligns with selected objects. Quantitative and qualitative evaluations
show that our model outperforms baselines, achieving better alignment between
objects and their associated sounds. Project page:
https://tinglok.netlify.app/files/avobject/

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [303] [A Generic Branch-and-Bound Algorithm for $\ell_0$-Penalized Problems with Supplementary Material](https://arxiv.org/abs/2506.03974)
*Clément Elvira,Théo Guyard,Cédric Herzet*

Main category: math.OC

TL;DR: 本文提出了一种通用的分支定界算法El0ps，用于解决L0惩罚优化问题，支持更广泛的损失函数和松弛设计，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对二次损失，使用“Big-M”约束或L2惩罚构建松弛，限制了应用范围和灵活性。本文旨在提出一种更通用的方法。

Method: 提出了一种通用的分支定界算法，支持多种损失函数和灵活的松弛设计，通过一般惩罚项涵盖现有技术。

Result: 理论证明了分支定界实现所需的关键量在闭式表达下成立，开发的El0ps求解器在实验中表现优异，扩展了计算可行性。

Conclusion: El0ps不仅在现代实例上达到先进水平，还能解决以往难以处理的问题，具有广泛的应用潜力。

Abstract: We present a generic Branch-and-Bound procedure designed to solve
L0-penalized optimization problems. Existing approaches primarily focus on
quadratic losses and construct relaxations using "Big-M" constraints and/or
L2-norm penalties. In contrast, our method accommodates a broader class of loss
functions and allows greater flexibility in relaxation design through a general
penalty term, encompassing existing techniques as special cases. We establish
theoretical results ensuring that all key quantities required for the
Branch-and-Bound implementation admit closed-form expressions under the general
blanket assumptions considered in our work. Leveraging this framework, we
introduce El0ps, an open-source Python solver with a plug-and-play workflow
that enables user-defined losses and penalties in L0-penalized problems.
Through extensive numerical experiments, we demonstrate that El0ps achieves
state-of-the-art performance on classical instances and extends computational
feasibility to previously intractable ones.

</details>


### [304] [Similarity-based fuzzy clustering scientific articles: potentials and challenges from mathematical and computational perspectives](https://arxiv.org/abs/2506.04045)
*Vu Thi Huong,Ida Litzel,Thorsten Koch*

Main category: math.OC

TL;DR: 该论文探讨了模糊聚类在处理大规模出版数据时的潜力与挑战，提出了基于GPU并行计算的梯度投影方法以提升效率。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决模糊聚类在分析如OpenAlex或Web of Science等包含数千万文章和数十亿引用的大规模数据库时的计算挑战。

Method: 论文通过建立二阶最优性条件，利用问题结构提出实用解法，并采用基于GPU的并行计算加速梯度投影方法。

Result: 研究不仅提供了新的理论见解，还实现了对大规模数据的高效处理。

Conclusion: 模糊聚类结合优化算法和并行计算，为处理大规模出版数据提供了有效的解决方案。

Abstract: Fuzzy clustering, which allows an article to belong to multiple clusters with
soft membership degrees, plays a vital role in analyzing publication data. This
problem can be formulated as a constrained optimization model, where the goal
is to minimize the discrepancy between the similarity observed from data and
the similarity derived from a predicted distribution. While this approach
benefits from leveraging state-of-the-art optimization algorithms, tailoring
them to work with real, massive databases like OpenAlex or Web of Science -
containing about 70 million articles and a billion citations - poses
significant challenges. We analyze potentials and challenges of the approach
from both mathematical and computational perspectives. Among other things,
second-order optimality conditions are established, providing new theoretical
insights, and practical solution methods are proposed by exploiting the
structure of the problem. Specifically, we accelerate the gradient projection
method using GPU-based parallel computing to efficiently handle large-scale
data.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [305] [chemtrain-deploy: A parallel and scalable framework for machine learning potentials in million-atom MD simulations](https://arxiv.org/abs/2506.04055)
*Paul Fuchs,Weilong Chen,Stephan Thaler,Julija Zavadlav*

Main category: physics.comp-ph

TL;DR: 研究者开发了chemtrain-deploy框架，支持在LAMMPS中部署任意JAX定义的半局部势能机器学习模型，实现多GPU并行的大规模分子动力学模拟。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习势能（MLP）工具大多受限于特定架构、缺乏与标准MD软件集成或无法跨GPU并行，限制了MLP在分子动力学模拟中的应用。

Method: 开发chemtrain-deploy框架，支持模型无关的MLP部署，利用LAMMPS功能实现多GPU并行计算，并验证了多种图神经网络架构的性能。

Result: 该框架实现了最先进的效率，可扩展至百万原子系统，并在液-汽界面、晶体材料和溶剂化肽等多种体系中验证了其性能。

Conclusion: chemtrain-deploy为高性能MLP-MD模拟提供了实用工具，同时为MLP架构选择和未来设计提供了指导。

Abstract: Machine learning potentials (MLPs) have advanced rapidly and show great
promise to transform molecular dynamics (MD) simulations. However, most
existing software tools are tied to specific MLP architectures, lack
integration with standard MD packages, or are not parallelizable across GPUs.
To address these challenges, we present chemtrain-deploy, a framework that
enables model-agnostic deployment of MLPs in LAMMPS. chemtrain-deploy supports
any JAX-defined semi-local potential, allowing users to exploit the
functionality of LAMMPS and perform large-scale MLP-based MD simulations on
multiple GPUs. It achieves state-of-the-art efficiency and scales to systems
containing millions of atoms. We validate its performance and scalability using
graph neural network architectures, including MACE, Allegro, and PaiNN, applied
to a variety of systems, such as liquid-vapor interfaces, crystalline
materials, and solvated peptides. Our results highlight the practical utility
of chemtrain-deploy for real-world, high-performance simulations and provide
guidance for MLP architecture selection and future design.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [306] [TransClean: Finding False Positives in Multi-Source Entity Matching under Real-World Conditions via Transitive Consistency](https://arxiv.org/abs/2506.04006)
*Fernando de Meer Pardo,Branka Hadji Misheva,Martin Braschler,Kurt Stockinger*

Main category: cs.DB

TL;DR: TransClean是一种检测实体匹配算法假阳性预测的方法，适用于大规模、噪声多、无标签且分布变化的多源数据集，通过迭代优化匹配一致性提升效果。


<details>
  <summary>Details</summary>
Motivation: 现实世界中多源数据集常存在噪声、无标签及分布变化，传统实体匹配算法易产生假阳性预测，需高效、鲁棒且少人工干预的解决方案。

Method: TransClean利用匹配的传递一致性，通过迭代移除假阳性匹配并保留真阳性，仅依赖模型评估无需人工标注，优化匹配质量。

Result: 实验表明，TransClean与传统方法相比平均提升24.42 F1分数，能有效检测多源场景下的假阳性匹配。

Conclusion: TransClean在提升实体匹配准确性方面表现优异，尤其适用于复杂多源数据环境，具有实际应用价值。

Abstract: We present TransClean, a method for detecting false positive predictions of
entity matching algorithms under real-world conditions characterized by
large-scale, noisy, and unlabeled multi-source datasets that undergo
distributional shifts. TransClean is explicitly designed to operate with
multiple data sources in an efficient, robust and fast manner while accounting
for edge cases and requiring limited manual labeling. TransClean leverages the
Transitive Consistency of a matching, a measure of the consistency of a
pairwise matching model f_theta on the matching it produces G_f_theta, based
both on its predictions on directly evaluated record pairs and its predictions
on implied record pairs. TransClean iteratively modifies a matching through
gradually removing false positive matches while removing as few true positive
matches as possible. In each of these steps, the estimation of the Transitive
Consistency is exclusively done through model evaluations and produces
quantities that can be used as proxies of the amounts of true and false
positives in the matching while not requiring any manual labeling, producing an
estimate of the quality of the matching and indicating which record groups are
likely to contain false positives. In our experiments, we compare combining
TransClean with a naively trained pairwise matching model (DistilBERT) and with
a state-of-the-art end-to-end matching method (CLER) and illustrate the
flexibility of TransClean in being able to detect most of the false positives
of either setup across a variety of datasets. Our experiments show that
TransClean induces an average +24.42 F1 score improvement for entity matching
in a multi-source setting when compared to traditional pair-wise matching
algorithms.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [307] [A Pre-trained Framework for Multilingual Brain Decoding Using Non-invasive Recordings](https://arxiv.org/abs/2506.03214)
*Yi Guo,Yihang Dong,Michael Kwok-Po Ng,Shuqiang Wang*

Main category: q-bio.NC

TL;DR: 该论文提出了一种多语言、多被试、多模态的脑机接口解码框架，通过将不同脑记录映射到预训练多语言模型定义的统一语义空间，实现了跨语言、跨被试和跨模态的解码，并提升了语言公平性。


<details>
  <summary>Details</summary>
Motivation: 当前脑机接口（BCI）的解码方法局限于单语言、单被试和单模态，限制了其临床适用性和泛化能力。本文旨在解决这些限制，提升BCI的跨语言、跨被试和跨模态的解码能力。

Method: 提出了一种联合多语言、多被试和多模态的解码框架，利用预训练多语言模型（PMM）定义的统一语义空间，将多样化的脑记录映射到该空间，实现跨语言、跨被试和跨模态的解码。

Result: 实验使用159名参与者的非侵入性脑记录数据，覆盖四种语言。结果表明，该框架在多语言、多被试和多模态设置下表现出强大的泛化能力，并能提升弱势语言的解码性能，促进语言公平性。

Conclusion: 该框架为脑解码建立了新的潜在范式，为BCI的更广泛应用开辟了新路径，特别是在提升语言公平性和跨语言映射增强方面具有重要意义。

Abstract: Brain-computer interfaces (BCIs) with speech decoding from brain recordings
have broad application potential in fields such as clinical rehabilitation and
cognitive neuroscience. However, current decoding methods remain limited to
single-language, single-subject, and single neuroimaging modality settings,
restricting their clinical applicability and generalizability. Here we propose
a joint multilingual, multi-subject and multimodal decoding framework. It maps
diverse brain recordings into a unified semantic space defined by a pre-trained
multilingual model (PMM), enabling decoding across multiple languages, multiple
subjects and multiple neuroimaging modalities. The proposed framework is
validated using non-invasive brain recordings from 159 participants across four
languages. Experimental results show that it exhibits strong generalization
across multilingual, multi-subject, and multimodal settings. More importantly,
the proposed framework can promote linguistic fairness, which is vital for
underrepresented languages in BCI applications. The unified semantic space
enables cross-lingual mapping enhancement, allowing the framework to boost the
decoding performance of underrepresented languages, thereby promoting
linguistic fairness. Overall, the proposed framework establishes a new
potential paradigm for brain decoding, opening new paths for broader
applications of BCI.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [308] [Beware! The AI Act Can Also Apply to Your AI Research Practices](https://arxiv.org/abs/2506.03218)
*Alina Wernick,Kristof Meding*

Main category: cs.CY

TL;DR: 欧盟《人工智能法案》可能对AI研究产生意外限制，本文探讨其适用性并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 探讨欧盟《人工智能法案》对AI研究的影响，指出当前法规可能无意中阻碍科学研究。

Method: 分析《人工智能法案》条款，结合日常研究案例说明其适用性，并评估例外条款的不足。

Result: 法案当前的科学例外条款未能覆盖现代AI研究实践，需修订以提供法律确定性。

Conclusion: 建议修改法案并呼吁政策制定者、法律学者和AI研究者对话，避免对研究产生负面影响。

Abstract: The EU has become one of the vanguards in regulating the digital age. A
particularly important regulation in the Artificial Intelligence (AI) domain is
the EU AI Act, which entered into force in 2024. The AI Act specifies -- due to
a risk-based approach -- various obligations for providers of AI systems. These
obligations, for example, include a cascade of documentation and compliance
measures, which represent a potential obstacle to science. But do these
obligations also apply to AI researchers? This position paper argues that,
indeed, the AI Act's obligations could apply in many more cases than the AI
community is aware of. In our analysis of the AI Act and its applicability, we
contribute the following: 1.) We give a high-level introduction to the AI Act
aimed at non-legal AI research scientists. 2.) We explain with everyday
research examples why the AI Act applies to research. 3.) We analyse the
exceptions of the AI Act's applicability and state that especially scientific
research exceptions fail to account for current AI research practices. 4.) We
propose changes to the AI Act to provide more legal certainty for AI
researchers and give two recommendations for AI researchers to reduce the risk
of not complying with the AI Act. We see our paper as a starting point for a
discussion between policymakers, legal scholars, and AI researchers to avoid
unintended side effects of the AI Act on research.

</details>


### [309] [Misalignment or misuse? The AGI alignment tradeoff](https://arxiv.org/abs/2506.03755)
*Max Hellrigel-Holderbaum,Leonard Dung*

Main category: cs.CY

TL;DR: 论文探讨了AI对齐的两种风险：未对齐AGI的灾难性风险和对齐AGI被人类滥用的风险，并提出可能减少这两种风险的技术和社会治理方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决AI对齐中的双重风险：未对齐AGI可能导致灾难性后果，而对齐AGI则可能增加被人类滥用的风险。

Method: 通过理论分析和实证研究，评估不同AI对齐技术如何影响未对齐和滥用风险，并探讨社会因素的作用。

Result: 研究发现，现有许多对齐技术可能增加滥用风险，但通过鲁棒性、AI控制方法和良好治理可降低风险。

Conclusion: 结论指出，尽管存在风险，但通过技术改进和社会治理，可以在不增加滥用风险的情况下实现AI对齐。

Abstract: Creating systems that are aligned with our goals is seen as a leading
approach to create safe and beneficial AI in both leading AI companies and the
academic field of AI safety. We defend the view that misaligned AGI - future,
generally intelligent (robotic) AI agents - poses catastrophic risks. At the
same time, we support the view that aligned AGI creates a substantial risk of
catastrophic misuse by humans. While both risks are severe and stand in tension
with one another, we show that - in principle - there is room for alignment
approaches which do not increase misuse risk. We then investigate how the
tradeoff between misalignment and misuse looks empirically for different
technical approaches to AI alignment. Here, we argue that many current
alignment techniques and foreseeable improvements thereof plausibly increase
risks of catastrophic misuse. Since the impacts of AI depend on the social
context, we close by discussing important social factors and suggest that to
reduce the risk of a misuse catastrophe due to aligned AGI, techniques such as
robustness, AI control methods and especially good governance seem essential.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [310] [Investigating Quantum Feature Maps in Quantum Support Vector Machines for Lung Cancer Classification](https://arxiv.org/abs/2506.03272)
*My Youssef El Hafidi,Achraf Toufah,Mohamed Achraf Kadim*

Main category: quant-ph

TL;DR: 该研究探讨了量子支持向量机（QSVM）在肺癌诊断中的应用，通过不同量子特征映射提升分类性能，发现PauliFeatureMap表现最佳。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习在医疗等需要高级模式识别的领域展现出潜力，本研究旨在验证QSVM在肺癌诊断中的有效性。

Method: 使用Qiskit实现QSVM模型，在309例患者数据上测试ZFeatureMap、ZZFeatureMap和PauliFeatureMap三种量子特征映射。

Result: PauliFeatureMap在三个子集中实现完美分类，整体性能优于其他特征映射。

Conclusion: 量子计算原理可增强诊断能力，凸显基于物理的建模在医疗AI应用中的重要性。

Abstract: In recent years, quantum machine learning has emerged as a promising
intersection between quantum physics and artificial intelligence, particularly
in domains requiring advanced pattern recognition such as healthcare. This
study investigates the effectiveness of Quantum Support Vector Machines (QSVM),
which leverage quantum mechanical phenomena like superposition and entanglement
to construct high-dimensional Hilbert spaces for data classification. Focusing
on lung cancer diagnosis, a concrete and critical healthcare application, we
analyze how different quantum feature maps influence classification
performance. Using a real-world dataset of 309 patient records with significant
class imbalance (39 non-cancer vs. 270 cancer cases), we constructed six
balanced subsets for robust evaluation. QSVM models were implemented using
Qiskit and executed on the qasm simulator, employing three distinct quantum
feature maps: ZFeatureMap, ZZFeatureMap, and PauliFeatureMap. Performance was
assessed using accuracy, precision, recall, specificity, and F1-score. Results
show that the PauliFeatureMap consistently outperformed the others, achieving
perfect classification in three subsets and strong performance overall. These
findings demonstrate how quantum computational principles can be harnessed to
enhance diagnostic capabilities, reinforcing the importance of physics-based
modeling in emerging AI applications within healthcare.

</details>


### [311] [RhoDARTS: Differentiable Quantum Architecture Search with Density Matrix Simulations](https://arxiv.org/abs/2506.03697)
*Swagat Kumar,Jan-Nico Zaech,Colin Michael Wilmott,Luc Van Gool*

Main category: quant-ph

TL;DR: 该论文提出了一种名为ρDARTS的可微分量子架构搜索算法，通过从量子角度出发，解决了现有量子架构搜索方法忽视量子电路固有特性的问题，并在多个任务中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 变分量子算法（VQAs）在噪声中等规模量子（NISQ）计算机上展现出潜力，但如何为通用机器学习任务设计有效的量子神经网络（QNNs）架构仍是一个挑战。现有的量子架构搜索（QAS）方法多借鉴经典神经网络搜索技术，往往忽略了量子电路的固有特性。

Method: 论文提出ρDARTS算法，将量子架构搜索过程建模为量子混合态的演化过程，从量子角度出发，直接在量子架构搜索空间中优化。

Result: 实验表明，ρDARTS在状态初始化、哈密顿量优化和图像分类任务中均能找到有效电路，且相比现有QAS技术具有更好的收敛性和抗噪鲁棒性。

Conclusion: 通过从量子视角重新设计QAS算法，ρDARTS克服了现有方法的局限性，为NISQ时代的量子机器学习提供了更优的架构搜索方案。

Abstract: Variational Quantum Algorithms (VQAs) are a promising approach for leveraging
powerful Noisy Intermediate-Scale Quantum (NISQ) computers. When applied to
machine learning tasks, VQAs give rise to NISQ-compatible Quantum Neural
Networks (QNNs), which have been shown to outperform classical neural networks
with a similar number of trainable parameters. While the quantum circuit
structures of VQAs for physics simulations are determined by the physical
properties of the systems, identifying effective QNN architectures for general
machine learning tasks is a difficult challenge due to the lack of
domain-specific priors. Indeed, existing Quantum Architecture Search (QAS)
algorithms, adaptations of classical neural architecture search techniques,
often overlook the inherent quantum nature of the circuits they produce. By
approaching QAS from the ground-up and from a quantum perspective, we resolve
this limitation by proposing $\rho$DARTS, a differentiable QAS algorithm that
models the search process as the evolution of a quantum mixed state, emerging
from the search space of quantum architectures. We validate our method by
finding circuits for state initialization, Hamiltonian optimization, and image
classification. Further, we demonstrate better convergence against existing QAS
techniques and show improved robustness levels to noise.

</details>


### [312] [Towards Quantum Operator-Valued Kernels](https://arxiv.org/abs/2506.03779)
*Hachem Kadri,Joachim Tomasi,Yuka Hashimoto,Sandrine Anthoine*

Main category: quant-ph

TL;DR: 量子核研究应转向更具表达力的核类别，以充分发挥量子核机器的潜力。


<details>
  <summary>Details</summary>
Motivation: 近期研究表明，量子核在处理经典数据时可能无法提供速度优势，但现有研究主要集中在标量值核上，限制了量子核的改进空间。

Method: 基于算子值核的最新进展，提出研究量子核的指导原则。

Result: 提出设计新一代量子核机器的建议，以探索其潜在优势。

Conclusion: 量子核研究应关注更具表达力的核类别，以突破现有局限。

Abstract: Quantum kernels are reproducing kernel functions built using
quantum-mechanical principles and are studied with the aim of outperforming
their classical counterparts. The enthusiasm for quantum kernel machines has
been tempered by recent studies that have suggested that quantum kernels could
not offer speed-ups when learning on classical data. However, most of the
research in this area has been devoted to scalar-valued kernels in standard
classification or regression settings for which classical kernel methods are
efficient and effective, leaving very little room for improvement with quantum
kernels. This position paper argues that quantum kernel research should focus
on more expressive kernel classes. We build upon recent advances in
operator-valued kernels, and propose guidelines for investigating quantum
kernels. This should help to design a new generation of quantum kernel machines
and fully explore their potentials.

</details>


### [313] [Estimation of the reduced density matrix and entanglement entropies using autoregressive networks](https://arxiv.org/abs/2506.04170)
*Piotr Białas,Piotr Korcyl,Tomasz Stebel,Dawid Zapolski*

Main category: quant-ph

TL;DR: 使用自回归神经网络模拟量子自旋链，通过Ising链计算基态纠缠熵，单次训练即可估计所有矩阵元素。


<details>
  <summary>Details</summary>
Motivation: 研究量子自旋链的纠缠熵，探索神经网络在蒙特卡洛模拟中的应用潜力。

Method: 采用自回归神经网络层次结构，估计连续自旋的条件概率，直接计算约化密度矩阵元素。

Result: 成功计算了包含最多5个自旋的区间基态von Neumann和Rényi纠缠熵的连续极限。

Conclusion: 该方法适用于其他类型自旋链及非零温度下的热态纠缠熵估计。

Abstract: We present an application of autoregressive neural networks to Monte Carlo
simulations of quantum spin chains using the correspondence with classical
two-dimensional spin systems. We use a hierarchy of neural networks capable of
estimating conditional probabilities of consecutive spins to evaluate elements
of reduced density matrices directly. Using the Ising chain as an example, we
calculate the continuum limit of the ground state's von Neumann and R\'enyi
bipartite entanglement entropies of an interval built of up to 5 spins. We
demonstrate that our architecture is able to estimate all the needed matrix
elements with just a single training for a fixed time discretization and
lattice volume. Our method can be applied to other types of spin chains,
possibly with defects, as well as to estimating entanglement entropies of
thermal states at non-zero temperature.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [314] [Multi-Spectral Gaussian Splatting with Neural Color Representation](https://arxiv.org/abs/2506.03407)
*Lukas Meyer,Josef Grün,Maximilian Weiherer,Bernhard Egger,Marc Stamminger,Linus Franke*

Main category: cs.GR

TL;DR: MS-Splatting提出了一种多光谱3D高斯泼溅框架，能够从不同光谱域的独立相机图像生成多视角一致的新视图，无需跨模态相机校准，并能统一学习所有波段。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要跨模态相机校准，且无法有效利用光谱和空间相关性，限制了多光谱渲染的质量和应用范围。

Method: 采用神经颜色表示法，将多光谱信息编码为紧凑的每泼溅特征嵌入，并通过浅层MLP解码为光谱颜色值，实现统一表示下的联合学习。

Result: 实验表明，该方法提升了多光谱渲染质量，并在农业应用中有效渲染植被指数（如NDVI）。

Conclusion: MS-Splatting通过简单有效的策略，显著提升了多光谱渲染的质量和灵活性，适用于多种光谱和应用场景。

Abstract: We present MS-Splatting -- a multi-spectral 3D Gaussian Splatting (3DGS)
framework that is able to generate multi-view consistent novel views from
images of multiple, independent cameras with different spectral domains. In
contrast to previous approaches, our method does not require cross-modal camera
calibration and is versatile enough to model a variety of different spectra,
including thermal and near-infra red, without any algorithmic changes.
  Unlike existing 3DGS-based frameworks that treat each modality separately (by
optimizing per-channel spherical harmonics) and therefore fail to exploit the
underlying spectral and spatial correlations, our method leverages a novel
neural color representation that encodes multi-spectral information into a
learned, compact, per-splat feature embedding. A shallow multi-layer perceptron
(MLP) then decodes this embedding to obtain spectral color values, enabling
joint learning of all bands within a unified representation.
  Our experiments show that this simple yet effective strategy is able to
improve multi-spectral rendering quality, while also leading to improved
per-spectra rendering quality over state-of-the-art methods. We demonstrate the
effectiveness of this new technique in agricultural applications to render
vegetation indices, such as normalized difference vegetation index (NDVI).

</details>


### [315] [SplArt: Articulation Estimation and Part-Level Reconstruction with 3D Gaussian Splatting](https://arxiv.org/abs/2506.03594)
*Shengjie Lin,Jiading Fang,Muhammad Zubair Irshad,Vitor Campagnolo Guizilini,Rares Andrei Ambrus,Greg Shakhnarovich,Matthew R. Walter*

Main category: cs.GR

TL;DR: SplArt是一个自监督、类别无关的框架，利用3D高斯泼溅技术从不同关节状态的RGB图像中重建关节物体并推断运动学，实现实时逼真渲染。


<details>
  <summary>Details</summary>
Motivation: 现有方法在可扩展性、鲁棒性和渲染质量方面存在局限，需要3D监督或昂贵标注，易陷入局部最优，且渲染速度或真实感不足。

Method: SplArt通过为每个高斯添加可微分运动参数优化部件分割，采用多阶段优化策略逐步处理重建、部件分割和关节估计，利用几何自监督无需3D标注或类别先验。

Result: 在现有和新基准测试中表现优异，使用手持RGB相机在真实场景中验证了其先进性能和实用性。

Conclusion: SplArt在关节物体重建和运动学推断方面实现了高效、鲁棒且逼真的渲染，代码已开源。

Abstract: Reconstructing articulated objects prevalent in daily environments is crucial
for applications in augmented/virtual reality and robotics. However, existing
methods face scalability limitations (requiring 3D supervision or costly
annotations), robustness issues (being susceptible to local optima), and
rendering shortcomings (lacking speed or photorealism). We introduce SplArt, a
self-supervised, category-agnostic framework that leverages 3D Gaussian
Splatting (3DGS) to reconstruct articulated objects and infer kinematics from
two sets of posed RGB images captured at different articulation states,
enabling real-time photorealistic rendering for novel viewpoints and
articulations. SplArt augments 3DGS with a differentiable mobility parameter
per Gaussian, achieving refined part segmentation. A multi-stage optimization
strategy is employed to progressively handle reconstruction, part segmentation,
and articulation estimation, significantly enhancing robustness and accuracy.
SplArt exploits geometric self-supervision, effectively addressing challenging
scenarios without requiring 3D annotations or category-specific priors.
Evaluations on established and newly proposed benchmarks, along with
applications to real-world scenarios using a handheld RGB camera, demonstrate
SplArt's state-of-the-art performance and real-world practicality. Code is
publicly available at https://github.com/ripl/splart.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [316] [Universal Reusability in Recommender Systems: The Case for Dataset- and Task-Independent Frameworks](https://arxiv.org/abs/2506.03391)
*Tri Kurniawan Wijaya,Xinyang Shao,Gonzalo Fiz Pontiveros,Edoardo D'Amico*

Main category: cs.IR

TL;DR: 提出DTIRS框架，通过标准化数据集描述和任务定义，实现推荐系统的数据集与任务独立性，降低使用门槛并提升可复用性。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统需针对不同数据集和任务进行大量手动配置，阻碍了其普及和扩展。受大语言模型启发，希望开发无需重建流程即可适应新场景的推荐系统。

Method: 提出DTIRS框架，利用Dataset Description Language (DsDL)实现标准化描述，支持自动化特征工程、模型选择和优化，并规划从Level-1到Level-2的自动化演进路径。

Result: 建立了DTIRS概念框架，提出通过DsDL实现数据集与任务解耦的解决方案，为完全独立化的推荐系统奠定基础。

Conclusion: DTIRS通过标准化和自动化降低了推荐系统的使用门槛，但需权衡通用性与计算开销，DsDL是实现该愿景的关键工具。

Abstract: Recommender systems are pivotal in delivering personalized experiences across
industries, yet their adoption and scalability remain hindered by the need for
extensive dataset- and task-specific configurations. Existing systems often
require significant manual intervention, domain expertise, and engineering
effort to adapt to new datasets or tasks, creating barriers to entry and
limiting reusability. In contrast, recent advancements in large language models
(LLMs) have demonstrated the transformative potential of reusable systems,
where a single model can handle diverse tasks without significant
reconfiguration. Inspired by this paradigm, we propose the Dataset- and
Task-Independent Recommender System (DTIRS), a framework aimed at maximizing
the reusability of recommender systems while minimizing barriers to entry.
Unlike LLMs, which achieve task generalization directly, DTIRS focuses on
eliminating the need to rebuild or reconfigure recommendation pipelines for
every new dataset or task, even though models may still need retraining on new
data. By leveraging the novel Dataset Description Language (DsDL), DTIRS
enables standardized dataset descriptions and explicit task definitions,
allowing autonomous feature engineering, model selection, and optimization.
This paper introduces the concept of DTIRS and establishes a roadmap for
transitioning from Level-1 automation (dataset-agnostic but task-specific
systems) to Level-2 automation (fully dataset- and task-independent systems).
Achieving this paradigm would maximize code reusability and lower barriers to
adoption. We discuss key challenges, including the trade-offs between
generalization and specialization, computational overhead, and scalability,
while presenting DsDL as a foundational tool for this vision.

</details>


### [317] [ProRank: Prompt Warmup via Reinforcement Learning for Small Language Models Reranking](https://arxiv.org/abs/2506.03487)
*Xianming Li,Aamir Shakir,Rui Huang,Julius Lipp,Jing Li*

Main category: cs.IR

TL;DR: 论文提出ProRank方法，通过两阶段训练提升小型语言模型在文档重排任务中的表现，使其在保持高效的同时超越大型模型。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型（LLM）的文档重排方法计算成本高，而小型语言模型（SLM）在未微调时难以理解任务提示。论文旨在解决SLM在文档重排任务中的有效性不足问题。

Method: 提出ProRank方法，包含两阶段训练：1) 使用强化学习GRPO进行提示预热，帮助SLM理解任务提示并生成粗粒度二元相关性分数；2) 通过细粒度分数学习阶段微调SLM，进一步提升重排质量。

Result: 实验表明，ProRank在BEIR基准测试中表现优于最先进的开源和专有重排模型，轻量级ProRank-0.5B甚至超越32B LLM模型。

Conclusion: 通过适当训练，SLM可以在保持计算效率的同时实现卓越的文档重排性能。

Abstract: Reranking is fundamental to information retrieval and retrieval-augmented
generation, with recent Large Language Models (LLMs) significantly advancing
reranking quality. While recent advances with LLMs have significantly improved
document reranking quality, current approaches primarily rely on large-scale
LLMs (>7B parameters) through zero-shot prompting, presenting high
computational costs. Small Language Models (SLMs) offer a promising alternative
because of their efficiency, but our preliminary quantitative analysis reveals
they struggle with understanding task prompts without fine-tuning. This limits
their effectiveness for document reranking tasks. To address this issue, we
introduce a novel two-stage training approach, ProRank, for SLM-based document
reranking. First, we propose a prompt warmup stage using reinforcement learning
GRPO to steer SLMs to understand task prompts and generate more accurate
coarse-grained binary relevance scores for document reranking. Then, we
continuously fine-tune the SLMs with a fine-grained score learning stage
without introducing additional layers to further improve the reranking quality.
Comprehensive experimental results demonstrate that the proposed ProRank
consistently outperforms both the most advanced open-source and proprietary
reranking models. Notably, our lightweight ProRank-0.5B model even surpasses
the powerful 32B LLM reranking model on the BEIR benchmark, establishing that
properly trained SLMs can achieve superior document reranking performance while
maintaining computational efficiency.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [318] [Why Regression? Binary Encoding Classification Brings Confidence to Stock Market Index Price Prediction](https://arxiv.org/abs/2506.03153)
*Junzhe Jiang,Chang Yang,Xinrun Wang,Bo Li*

Main category: q-fin.ST

TL;DR: 提出Cubic框架，通过建模成分股的自适应融合改进股指预测，将回归转为分类任务并引入置信度指导交易。


<details>
  <summary>Details</summary>
Motivation: 现有方法将股指视为孤立时间序列，忽略其作为成分股聚合体的时变依赖性，导致预测精度不足。

Method: 1) 潜在空间融合成分股信息；2) 二进制编码分类替代回归；3) 置信度正则化损失与规则交易策略。

Result: 在多市场测试中，Cubic在预测准确性和交易收益上均超越现有基准模型。

Conclusion: 显式建模成分股动态关联并引入分类范式，显著提升股指预测性能与可交易性。

Abstract: Stock market indices serve as fundamental market measurement that quantify
systematic market dynamics. However, accurate index price prediction remains
challenging, primarily because existing approaches treat indices as isolated
time series and frame the prediction as a simple regression task. These methods
fail to capture indices' inherent nature as aggregations of constituent stocks
with complex, time-varying interdependencies. To address these limitations, we
propose Cubic, a novel end-to-end framework that explicitly models the adaptive
fusion of constituent stocks for index price prediction. Our main contributions
are threefold. i) Fusion in the latent space: we introduce the fusion mechanism
over the latent embedding of the stocks to extract the information from the
vast number of stocks. ii) Binary encoding classification: since regression
tasks are challenging due to continuous value estimation, we reformulate the
regression into the classification task, where the target value is converted to
binary and we optimize the prediction of the value of each digit with
cross-entropy loss. iii) Confidence-guided prediction and trading: we introduce
the regularization loss to address market prediction uncertainty for the index
prediction and design the rule-based trading policies based on the confidence.
Extensive experiments across multiple stock markets and indices demonstrate
that Cubic consistently outperforms state-of-the-art baselines in stock index
prediction tasks, achieving superior performance on both forecasting accuracy
metrics and downstream trading profitability.

</details>


### [319] [High-Dimensional Learning in Finance](https://arxiv.org/abs/2506.03780)
*Hasan Fallahgoul*

Main category: q-fin.ST

TL;DR: 该论文探讨了高维机器学习在金融预测中的理论基础和实证验证，揭示了在小样本和高维特征下预测成功的低复杂性本质。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解大参数模型在金融预测中何时及如何取得预测成功，特别是在高维学习和小样本情况下的表现。

Method: 论文通过理论分析和实证验证，研究了随机傅里叶特征实现中的标准化影响、样本复杂度界限以及无脊回归的有效复杂性。

Result: 结果表明，在小样本和高维特征情况下，预测成功往往由低复杂性因素驱动，而非真正的高维学习。

Conclusion: 结论指出，金融预测中高维模型的成功可能被夸大，实际效果受样本量和特征维度限制。

Abstract: Recent advances in machine learning have shown promising results for
financial prediction using large, over-parameterized models. This paper
provides theoretical foundations and empirical validation for understanding
when and how these methods achieve predictive success. I examine three key
aspects of high-dimensional learning in finance. First, I prove that
within-sample standardization in Random Fourier Features implementations
fundamentally alters the underlying Gaussian kernel approximation, replacing
shift-invariant kernels with training-set dependent alternatives. Second, I
derive sample complexity bounds showing when reliable learning becomes
information-theoretically impossible under weak signal-to-noise ratios typical
in finance. Third, VC-dimension analysis reveals that ridgeless regression's
effective complexity is bounded by sample size rather than nominal feature
dimension. Comprehensive numerical validation confirms these theoretical
predictions, revealing systematic breakdown of claimed theoretical properties
across realistic parameter ranges. These results show that when sample size is
small and features are high-dimensional, observed predictive success is
necessarily driven by low-complexity artifacts, not genuine high-dimensional
learning.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [320] [Preface to the Special Issue of the TAL Journal on Scholarly Document Processing](https://arxiv.org/abs/2506.03587)
*Florian Boudin,Akiko Aizawa*

Main category: cs.DL

TL;DR: 论文探讨了学术文献快速增长带来的挑战，提出利用大型语言模型（LLMs）等自动化工具帮助研究人员导航和解读复杂科学文献。


<details>
  <summary>Details</summary>
Motivation: 学术文献的快速增长、复杂语言和多样化格式使得研究人员难以跟上新知识，需要先进工具来提取可靠和可操作的见解。

Method: 利用大型语言模型（LLMs）进行文献综述、写作辅助和交互式研究探索等任务。

Result: LLMs为处理科学文献提供了新的机会，能够有效应对复杂语言和术语的挑战。

Conclusion: 本期特刊强调了自然语言处理和信息检索在学术和科学文献中的应用，展示了LLMs在解决这些挑战中的潜力。

Abstract: The rapid growth of scholarly literature makes it increasingly difficult for
researchers to keep up with new knowledge. Automated tools are now more
essential than ever to help navigate and interpret this vast body of
information. Scientific papers pose unique difficulties, with their complex
language, specialized terminology, and diverse formats, requiring advanced
methods to extract reliable and actionable insights. Large language models
(LLMs) offer new opportunities, enabling tasks such as literature reviews,
writing assistance, and interactive exploration of research. This special issue
of the TAL journal highlights research addressing these challenges and, more
broadly, research on natural language processing and information retrieval for
scholarly and scientific documents.

</details>


### [321] [Enhancing Automatic PT Tagging for MEDLINE Citations Using Transformer-Based Models](https://arxiv.org/abs/2506.03321)
*Victor H. Cid,James Mork*

Main category: cs.DL

TL;DR: 使用BERT和DistilBERT预训练模型预测MeSH文献类型，提升生物医学文献索引效率。


<details>
  <summary>Details</summary>
Motivation: 当前自动化索引过程依赖传统NLP算法，存在局限性，需要更高效的解决方案。

Method: 评估了单块多标签分类器和二元分类器集成方法。

Result: Transformer模型显著提高了文献类型标注的准确性。

Conclusion: Transformer模型为可扩展、高效的生物医学索引铺平了道路。

Abstract: We investigated the feasibility of predicting Medical Subject Headings (MeSH)
Publication Types (PTs) from MEDLINE citation metadata using pre-trained
Transformer-based models BERT and DistilBERT. This study addresses limitations
in the current automated indexing process, which relies on legacy NLP
algorithms. We evaluated monolithic multi-label classifiers and binary
classifier ensembles to enhance the retrieval of biomedical literature. Results
demonstrate the potential of Transformer models to significantly improve PT
tagging accuracy, paving the way for scalable, efficient biomedical indexing.

</details>


<div id='q-bio.OT'></div>

# q-bio.OT [[Back]](#toc)

### [322] [Plant Bioelectric Early Warning Systems: A Five-Year Investigation into Human-Plant Electromagnetic Communication](https://arxiv.org/abs/2506.04132)
*Peter A. Gloor*

Main category: q-bio.OT

TL;DR: 植物能通过生物电信号感知人类情绪与存在，深度学习模型分类准确率达97%，挑战传统植物感知认知。


<details>
  <summary>Details</summary>
Motivation: 探索植物对人类存在及情绪状态的生物电响应机制，验证植物是否具备超出传统认知的感知能力。

Method: 使用定制植物传感器采集数据，基于ResNet50架构的深度学习模型分析电压频谱图，并进行对照实验验证。

Result: 模型对人类情绪分类准确率97%（对照组30%），实验证实植物可识别个体（66%准确率）、韵律动作及声音等。

Conclusion: 植物可能进化出基于生物电场的抗食草动物预警系统，成果在农业、医疗等领域具应用潜力。

Abstract: We present a comprehensive investigation into plant bioelectric responses to
human presence and emotional states, building on five years of systematic
research. Using custom-built plant sensors and machine learning classification,
we demonstrate that plants generate distinct bioelectric signals correlating
with human proximity, emotional states, and physiological conditions. A deep
learning model based on ResNet50 architecture achieved 97% accuracy in
classifying human emotional states through plant voltage spectrograms, while
control models with shuffled labels achieved only 30% accuracy. This study
synthesizes findings from multiple experiments spanning 2020-2025, including
individual recognition (66% accuracy), eurythmic gesture detection, stress
prediction, and responses to human voice and movement. We propose that these
phenomena represent evolved anti-herbivory early warning systems, where plants
detect approaching animals through bioelectric field changes before physical
contact. Our results challenge conventional understanding of plant sensory
capabilities and suggest practical applications in agriculture, healthcare, and
human-plant interaction research.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [323] [Models of Heavy-Tailed Mechanistic Universality](https://arxiv.org/abs/2506.03470)
*Liam Hodgkinson,Zhichao Wang,Michael W. Mahoney*

Main category: stat.ML

TL;DR: 论文探讨深度学习中的重尾现象，提出HTMP随机矩阵模型解释其成因，并分析其对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 深度学习中的重尾行为（如雅可比矩阵、Hessian矩阵等的重尾谱密度）与模型性能密切相关，但成因尚不明确。研究旨在通过理论模型揭示其机制。

Method: 提出高温Marchenko-Pastur（HTMP）随机矩阵模型，通过数据复杂相关性、训练低温及低特征向量熵三因素解释重尾谱密度的产生。

Result: HTMP模型表明重尾行为可通过特征值排斥参数控制，并关联到神经缩放定律、优化器轨迹及训练阶段等场景。

Conclusion: 重尾机制普适性（HT-MU）可能是深度学习有效的核心特征，HTMP模型为理解其隐含偏置提供了理论框架。

Abstract: Recent theoretical and empirical successes in deep learning, including the
celebrated neural scaling laws, are punctuated by the observation that many
objects of interest tend to exhibit some form of heavy-tailed or power law
behavior. In particular, the prevalence of heavy-tailed spectral densities in
Jacobians, Hessians, and weight matrices has led to the introduction of the
concept of heavy-tailed mechanistic universality (HT-MU). Multiple lines of
empirical evidence suggest a robust correlation between heavy-tailed metrics
and model performance, indicating that HT-MU may be a fundamental aspect of
deep learning efficacy. Here, we propose a general family of random matrix
models -- the high-temperature Marchenko-Pastur (HTMP) ensemble -- to explore
attributes that give rise to heavy-tailed behavior in trained neural networks.
Under this model, spectral densities with power laws on (upper and lower) tails
arise through a combination of three independent factors (complex correlation
structures in the data; reduced temperatures during training; and reduced
eigenvector entropy), appearing as an implicit bias in the model structure, and
they can be controlled with an "eigenvalue repulsion" parameter. Implications
of our model on other appearances of heavy tails, including neural scaling
laws, optimizer trajectories, and the five-plus-one phases of neural network
training, are discussed.

</details>


### [324] [SubSearch: Robust Estimation and Outlier Detection for Stochastic Block Models via Subgraph Search](https://arxiv.org/abs/2506.03657)
*Leonardo Martins Bianco,Christine Keribin,Zacharie Naulet*

Main category: stat.ML

TL;DR: 提出SubSearch算法，通过搜索子图空间来鲁棒估计随机块模型参数，同时识别异常节点。


<details>
  <summary>Details</summary>
Motivation: 现实图数据常偏离理想假设，需鲁棒算法估计模型参数并识别异常。

Method: SubSearch算法探索子图空间，寻找最符合模型假设的子图。

Result: 在合成和真实数据集上验证了方法的有效性。

Conclusion: SubSearch能鲁棒估计参数并检测异常，优于简单修剪方法。

Abstract: Community detection is a fundamental task in graph analysis, with methods
often relying on fitting models like the Stochastic Block Model (SBM) to
observed networks. While many algorithms can accurately estimate SBM parameters
when the input graph is a perfect sample from the model, real-world graphs
rarely conform to such idealized assumptions. Therefore, robust algorithms are
crucial-ones that can recover model parameters even when the data deviates from
the assumed distribution. In this work, we propose SubSearch, an algorithm for
robustly estimating SBM parameters by exploring the space of subgraphs in
search of one that closely aligns with the model's assumptions. Our approach
also functions as an outlier detection method, properly identifying nodes
responsible for the graph's deviation from the model and going beyond simple
techniques like pruning high-degree nodes. Extensive experiments on both
synthetic and real-world datasets demonstrate the effectiveness of our method.

</details>


### [325] [Position: There Is No Free Bayesian Uncertainty Quantification](https://arxiv.org/abs/2506.03670)
*Ivan Melev,Goeran Kauermann*

Main category: stat.ML

TL;DR: 该论文质疑贝叶斯方法在不确定性量化中的有效性，提出基于优化的替代解释，并建议未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯方法因其直观性在现代机器和深度学习中广受欢迎，但其参数空间的不确定性量化有效性受到质疑。

Method: 通过讨论贝叶斯更新的等效优化表示，提供与优化视角一致的解释，并提出贝叶斯推断阶段的质量度量。

Result: 论文提出了对贝叶斯不确定性量化的替代解释，并提出了衡量推断质量的方法。

Conclusion: 贝叶斯不确定性量化的传统解释可能存在问题，未来研究应关注优化视角下的改进方向。

Abstract: Due to their intuitive appeal, Bayesian methods of modeling and uncertainty
quantification have become popular in modern machine and deep learning. When
providing a prior distribution over the parameter space, it is straightforward
to obtain a distribution over the parameters that is conventionally interpreted
as uncertainty quantification of the model. We challenge the validity of such
Bayesian uncertainty quantification by discussing the equivalent
optimization-based representation of Bayesian updating, provide an alternative
interpretation that is coherent with the optimization-based perspective,
propose measures of the quality of the Bayesian inferential stage, and suggest
directions for future work.

</details>


### [326] [Latent Guided Sampling for Combinatorial Optimization](https://arxiv.org/abs/2506.03672)
*Sobihan Surendran,Adeline Fermanian,Sylvain Le Corff*

Main category: stat.ML

TL;DR: 提出LGS-Net模型，通过潜在引导采样解决组合优化问题，在基准路由任务中达到RL方法的最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有神经组合优化方法依赖任务特定增强、分布外泛化能力差且缺乏鲁棒推理机制，需改进。

Method: 提出基于问题实例的潜在空间模型LGS-Net，结合马尔可夫链蒙特卡洛和随机近似的潜在引导采样推理方法。

Result: 理论证明方法收敛性，在基准路由任务中超越现有RL方法的性能。

Conclusion: LGS-Net为组合优化提供了高效推理框架，理论保障与实证效果俱佳。

Abstract: Combinatorial Optimization problems are widespread in domains such as
logistics, manufacturing, and drug discovery, yet their NP-hard nature makes
them computationally challenging. Recent Neural Combinatorial Optimization
methods leverage deep learning to learn solution strategies, trained via
Supervised or Reinforcement Learning (RL). While promising, these approaches
often rely on task-specific augmentations, perform poorly on
out-of-distribution instances, and lack robust inference mechanisms. Moreover,
existing latent space models either require labeled data or rely on pre-trained
policies. In this work, we propose LGS-Net, a novel latent space model that
conditions on problem instances, and introduce an efficient inference method,
Latent Guided Sampling (LGS), based on Markov Chain Monte Carlo and Stochastic
Approximation. We show that the iterations of our method form a
time-inhomogeneous Markov Chain and provide rigorous theoretical convergence
guarantees. Empirical results on benchmark routing tasks show that our method
achieves state-of-the-art performance among RL-based approaches.

</details>


### [327] [Infinitesimal Higher-Order Spectral Variations in Rectangular Real Random Matrices](https://arxiv.org/abs/2506.03764)
*Róisín Luo*

Main category: stat.ML

TL;DR: 该论文提出了一个理论框架，利用Kato的解析扰动理论中的简化解析算子，推导实矩形矩阵奇异值的n阶Fr'echet导数，解决了高阶导数难以计算的问题。


<details>
  <summary>Details</summary>
Motivation: 传统矩阵分析方法难以计算奇异值的高阶导数，特别是在随机矩阵应用（如深度学习中的对抗扰动）中，高阶谱敏感性研究需要更有效的工具。

Method: 将实矩形矩阵视为有限维希尔伯特空间上的紧算子，并嵌入到块自伴算子中，应用Kato的渐近特征值展开，得到高阶谱变化的闭式表达式。

Result: 获得了奇异值的n阶谱变化的闭式表达式，并针对n=2的情况，给出了文献中未见的奇异值Hessian矩阵。

Conclusion: 该框架通过结合抽象算子扰动理论与矩阵分析，为高阶谱敏感性研究提供了实用工具。

Abstract: We present a theoretical framework for deriving the general $n$-th order
Fr\'echet derivatives of singular values in real rectangular matrices, by
leveraging reduced resolvent operators from Kato's analytic perturbation theory
for self-adjoint operators. Deriving closed-form expressions for higher-order
derivatives of singular values is notoriously challenging through standard
matrix-analysis techniques. To overcome this, we treat a real rectangular
matrix as a compact operator on a finite-dimensional Hilbert space, and embed
the rectangular matrix into a block self-adjoint operator so that non-symmetric
perturbations are captured. Applying Kato's asymptotic eigenvalue expansion to
this construction, we obtain a general, closed-form expression for the
infinitesimal $n$-th order spectral variations. Specializing to $n=2$ and
deploying on a Kronecker-product representation with matrix convention yield
the Hessian of a singular value, not found in literature. By bridging abstract
operator-theoretic perturbation theory with matrices, our framework equips
researchers with a practical toolkit for higher-order spectral sensitivity
studies in random matrix applications (e.g., adversarial perturbation in deep
learning).

</details>


### [328] [Spatially Resolved Meteorological and Ancillary Data in Central Europe for Rainfall Streamflow Modeling](https://arxiv.org/abs/2506.03819)
*Marc Aurel Vischer,Noelia Otero,Jackie Ma*

Main category: stat.ML

TL;DR: 该论文提出了一个高空间分辨率的降雨径流建模数据集，旨在推动神经网络在水文建模中的应用。


<details>
  <summary>Details</summary>
Motivation: 为了将神经网络驱动的水文建模从集总流域扩展到更广泛的空间范围，作者构建了一个覆盖欧洲五个河流流域的综合数据集。

Method: 作者整理了五个欧洲河流流域（多瑙河上游、易北河、奥得河、莱茵河和威悉河）的气象强迫数据、土壤、岩石、土地覆盖和地形信息，并将其统一到9km×9km的网格上，提供1981年10月至2011年9月的每日数据。

Result: 数据集提供了高空间分辨率的降雨径流建模所需的多源数据，并附带了与公开河流流量数据结合的代码，支持端到端的建模。

Conclusion: 该数据集为水文建模研究提供了重要资源，有助于推动神经网络在分布式流域建模中的应用。

Abstract: We present a dataset for rainfall streamflow modeling that is fully spatially
resolved with the aim of taking neural network-driven hydrological modeling
beyond lumped catchments. To this end, we compiled data covering five river
basins in central Europe: upper Danube, Elbe, Oder, Rhine, and Weser. The
dataset contains meteorological forcings, as well as ancillary information on
soil, rock, land cover, and orography. The data is harmonized to a regular 9km
times 9km grid and contains daily values that span from October 1981 to
September 2011. We also provide code to further combine our dataset with
publicly available river discharge data for end-to-end rainfall streamflow
modeling.

</details>


### [329] [Algorithm- and Data-Dependent Generalization Bounds for Score-Based Generative Models](https://arxiv.org/abs/2506.03849)
*Benjamin Dupuis,Dario Shariatian,Maxime Haddouche,Alain Durmus,Umut Simsekli*

Main category: stat.ML

TL;DR: 本文提出了首个针对基于分数的生成模型（SGMs）的算法和数据依赖的泛化分析，填补了现有理论仅关注近似视角的不足。


<details>
  <summary>Details</summary>
Motivation: 现有对SGMs的理论分析多从近似理论出发，忽略了优化算法在实际训练中的作用，无法充分解释SGMs的实证成功。本文旨在通过算法和数据依赖的分析，更准确地理解SGMs的泛化行为。

Method: 通过实验展示优化超参数对生成分布泛化能力的影响，并建立显式考虑学习算法优化动态的泛化界限。

Result: 理论分析揭示了优化动态对SGMs泛化行为的关键作用，并通过多个数据集的实证结果支持了这一发现。

Conclusion: 本文填补了SGMs理论分析的空白，为理解其泛化行为提供了新的视角，并强调了优化算法在实践中的重要性。

Abstract: Score-based generative models (SGMs) have emerged as one of the most popular
classes of generative models. A substantial body of work now exists on the
analysis of SGMs, focusing either on discretization aspects or on their
statistical performance. In the latter case, bounds have been derived, under
various metrics, between the true data distribution and the distribution
induced by the SGM, often demonstrating polynomial convergence rates with
respect to the number of training samples. However, these approaches adopt a
largely approximation theory viewpoint, which tends to be overly pessimistic
and relatively coarse. In particular, they fail to fully explain the empirical
success of SGMs or capture the role of the optimization algorithm used in
practice to train the score network. To support this observation, we first
present simple experiments illustrating the concrete impact of optimization
hyperparameters on the generalization ability of the generated distribution.
Then, this paper aims to bridge this theoretical gap by providing the first
algorithmic- and data-dependent generalization analysis for SGMs. In
particular, we establish bounds that explicitly account for the optimization
dynamics of the learning algorithm, offering new insights into the
generalization behavior of SGMs. Our theoretical findings are supported by
empirical results on several datasets.

</details>


### [330] [Understanding challenges to the interpretation of disaggregated evaluations of algorithmic fairness](https://arxiv.org/abs/2506.04193)
*Stephen R. Pfohl,Natalie Harris,Chirag Nagpal,David Madras,Vishwali Mhasawade,Olawale Salaudeen,Awa Dieng,Shannon Sequeira,Santiago Arciniegas,Lillian Sung,Nnamdi Ezeanochie,Heather Cole-Lewis,Katherine Heller,Sanmi Koyejo,Alexander D'Amour*

Main category: stat.ML

TL;DR: 论文指出，单纯依赖子群分解评估模型公平性可能误导实践者，需结合因果假设和分析来控制混杂因素和分布变化。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习模型公平性评估中，子群分解评估方法被广泛使用，但其在数据代表性不足或存在选择偏差时可能失效，需要更可靠的评估框架。

Method: 使用因果图模型预测不同数据生成过程下子群间指标的稳定性，并结合条件独立性检验和加权性能估计方法。

Result: 研究发现，仅依赖子群性能相等作为公平性指标不可靠，需明确因果假设以应对数据偏差和分布变化。

Conclusion: 建议在子群分解评估基础上，结合因果分析来提升模型公平性评估的可靠性，这对实践者设计和解释模型评估具有广泛意义。

Abstract: Disaggregated evaluation across subgroups is critical for assessing the
fairness of machine learning models, but its uncritical use can mislead
practitioners. We show that equal performance across subgroups is an unreliable
measure of fairness when data are representative of the relevant populations
but reflective of real-world disparities. Furthermore, when data are not
representative due to selection bias, both disaggregated evaluation and
alternative approaches based on conditional independence testing may be invalid
without explicit assumptions regarding the bias mechanism. We use causal
graphical models to predict metric stability across subgroups under different
data generating processes. Our framework suggests complementing disaggregated
evaluations with explicit causal assumptions and analysis to control for
confounding and distribution shift, including conditional independence testing
and weighted performance estimation. These findings have broad implications for
how practitioners design and interpret model assessments given the ubiquity of
disaggregated evaluation.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [331] [Privacy and Security Threat for OpenAI GPTs](https://arxiv.org/abs/2506.04036)
*Wei Wenying,Zhao Kaifa,Xue Lei,Fan Ming*

Main category: cs.CR

TL;DR: 研究发现，超过98.8%的自定义GPT存在指令泄露漏洞，77.5%的防御策略无效，部分GPT存在不必要的数据访问行为，凸显安全和隐私风险。


<details>
  <summary>Details</summary>
Motivation: 随着自定义GPT的广泛应用，其安全和隐私问题日益突出，特别是指令泄露攻击和用户数据隐私风险，亟需系统评估和解决方案。

Method: 开发了三阶段指令泄露攻击方法，测试了10,000个真实自定义GPT，并构建框架评估防御策略效果及检测异常行为。

Result: 98.8%的GPT易受攻击；77.5%的防御策略失效；发现738个GPT收集用户对话数据，其中8个存在不必要的数据访问行为。

Conclusion: 研究揭示了自定义GPT生态系统的严重安全漏洞，呼吁开发者加强防御策略，同时提醒用户关注数据隐私风险。

Abstract: Large language models (LLMs) demonstrate powerful information handling
capabilities and are widely integrated into chatbot applications. OpenAI
provides a platform for developers to construct custom GPTs, extending
ChatGPT's functions and integrating external services. Since its release in
November 2023, over 3 million custom GPTs have been created. However, such a
vast ecosystem also conceals security and privacy threats. For developers,
instruction leaking attacks threaten the intellectual property of instructions
in custom GPTs through carefully crafted adversarial prompts. For users,
unwanted data access behavior by custom GPTs or integrated third-party services
raises significant privacy concerns. To systematically evaluate the scope of
threats in real-world LLM applications, we develop three phases instruction
leaking attacks target GPTs with different defense level. Our widespread
experiments on 10,000 real-world custom GPTs reveal that over 98.8% of GPTs are
vulnerable to instruction leaking attacks via one or more adversarial prompts,
and half of the remaining GPTs can also be attacked through multiround
conversations. We also developed a framework to assess the effectiveness of
defensive strategies and identify unwanted behaviors in custom GPTs. Our
findings show that 77.5% of custom GPTs with defense strategies are vulnerable
to basic instruction leaking attacks. Additionally, we reveal that 738 custom
GPTs collect user conversational information, and identified 8 GPTs exhibiting
data access behaviors that are unnecessary for their intended functionalities.
Our findings raise awareness among GPT developers about the importance of
integrating specific defensive strategies in their instructions and highlight
users' concerns about data privacy when using LLM-based applications.

</details>


### [332] [Dropout-Robust Mechanisms for Differentially Private and Fully Decentralized Mean Estimation](https://arxiv.org/abs/2506.03746)
*César Sabater,Sonia Ben Mokhtar,Jan Ramon*

Main category: cs.CR

TL;DR: 提出了一种名为IncA的全新协议，用于完全去中心化的均值估计，该协议在保证差分隐私的同时，无需中央协调，并采用低方差相关噪声，显著提高了准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在去中心化环境中实现差分隐私计算面临准确性、通信成本和信息泄露的挑战。现有方法要么通信开销大，要么依赖于宽松的对抗模型或成对噪声消除，后者在节点意外断开时准确性大幅下降。

Method: IncA协议通过增量注入敏感信息到计算中，使用低方差相关噪声，无需中央协调，实现了完全去中心化的均值估计。

Result: 理论证明在无节点永久断开时，协议准确性接近集中式设置；实证表明低方差相关噪声显著减少了现有技术在节点断开时的准确性损失。

Conclusion: IncA协议在去中心化差分隐私均值估计中表现出色，显著提升了准确性和鲁棒性，优于现有技术。

Abstract: Achieving differentially private computations in decentralized settings poses
significant challenges, particularly regarding accuracy, communication cost,
and robustness against information leakage. While cryptographic solutions offer
promise, they often suffer from high communication overhead or require
centralization in the presence of network failures. Conversely, existing fully
decentralized approaches typically rely on relaxed adversarial models or
pairwise noise cancellation, the latter suffering from substantial accuracy
degradation if parties unexpectedly disconnect. In this work, we propose IncA,
a new protocol for fully decentralized mean estimation, a widely used primitive
in data-intensive processing. Our protocol, which enforces differential
privacy, requires no central orchestration and employs low-variance correlated
noise, achieved by incrementally injecting sensitive information into the
computation. First, we theoretically demonstrate that, when no parties
permanently disconnect, our protocol achieves accuracy comparable to that of a
centralized setting-already an improvement over most existing decentralized
differentially private techniques. Second, we empirically show that our use of
low-variance correlated noise significantly mitigates the accuracy loss
experienced by existing techniques in the presence of dropouts.

</details>


### [333] [TracLLM: A Generic Framework for Attributing Long Context LLMs](https://arxiv.org/abs/2506.04202)
*Yanting Wang,Wei Zou,Runpeng Geng,Jinyuan Jia*

Main category: cs.CR

TL;DR: 该论文提出了TracLLM框架，用于追踪长上下文大语言模型（LLMs）生成输出的具体文本来源，以提高可解释性和效率。


<details>
  <summary>Details</summary>
Motivation: 长上下文LLMs在实际应用中需要生成基于上下文的输出，但如何准确追踪哪些文本片段对输出贡献最大仍是一个挑战。现有方法如Shapley在性能和计算成本上表现不佳。

Method: 开发了TracLLM框架，通过基于信息的搜索算法提升效率，并采用贡献分数集成/去噪技术提高准确性。

Result: 评估结果表明，TracLLM能有效识别长上下文中导致LLM输出的关键文本。

Conclusion: TracLLM为长上下文LLMs提供了一种高效的上下文回溯方法，适用于调试、安全分析和增强用户信任等场景。

Abstract: Long context large language models (LLMs) are deployed in many real-world
applications such as RAG, agent, and broad LLM-integrated applications. Given
an instruction and a long context (e.g., documents, PDF files, webpages), a
long context LLM can generate an output grounded in the provided context,
aiming to provide more accurate, up-to-date, and verifiable outputs while
reducing hallucinations and unsupported claims. This raises a research
question: how to pinpoint the texts (e.g., sentences, passages, or paragraphs)
in the context that contribute most to or are responsible for the generated
output by an LLM? This process, which we call context traceback, has various
real-world applications, such as 1) debugging LLM-based systems, 2) conducting
post-attack forensic analysis for attacks (e.g., prompt injection attack,
knowledge corruption attacks) to an LLM, and 3) highlighting knowledge sources
to enhance the trust of users towards outputs generated by LLMs. When applied
to context traceback for long context LLMs, existing feature attribution
methods such as Shapley have sub-optimal performance and/or incur a large
computational cost. In this work, we develop TracLLM, the first generic context
traceback framework tailored to long context LLMs. Our framework can improve
the effectiveness and efficiency of existing feature attribution methods. To
improve the efficiency, we develop an informed search based algorithm in
TracLLM. We also develop contribution score ensemble/denoising techniques to
improve the accuracy of TracLLM. Our evaluation results show TracLLM can
effectively identify texts in a long context that lead to the output of an LLM.
Our code and data are at: https://github.com/Wang-Yanting/TracLLM.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [334] [Automated Traffic Incident Response Plans using Generative Artificial Intelligence: Part 1 -- Building the Incident Response Benchmark](https://arxiv.org/abs/2506.03381)
*Artur Grigorev,Khaled Saleh,Jiwon Kim,Adriana-Simona Mihaita*

Main category: eess.SY

TL;DR: 该论文提出了一种基于生成式AI的交通事故响应基准，旨在通过自动生成响应计划来减少事故处理时间，提高道路安全。


<details>
  <summary>Details</summary>
Motivation: 交通事故是全球公共安全的重要问题，传统的人工决策响应方式存在不一致性和延迟问题，影响安全结果和网络性能。

Method: 使用真实事故报告作为训练和评估数据，提取历史响应行动，并与AI生成的响应计划进行比较，评估不同AI模型的性能。

Result: GPT-4o和Grok 2在专家解决方案对齐上表现最佳，Hamming距离最小（2.96-2.98），加权差异低（0.27-0.28）；Gemini 1.5 Pro虽然遗漏行动最少，但冗余行动过多（1547次），效率较低。

Conclusion: 生成式AI能有效优化交通事故响应计划，显著减少处理时间，提升道路安全和运营效率。

Abstract: Traffic incidents remain a critical public safety concern worldwide, with
Australia recording 1,300 road fatalities in 2024, which is the highest toll in
12 years. Similarly, the United States reports approximately 6 million crashes
annually, raising significant challenges in terms of a fast reponse time and
operational management. Traditional response protocols rely on human
decision-making, which introduces potential inconsistencies and delays during
critical moments when every minute impacts both safety outcomes and network
performance. To address this issue, we propose a novel Incident Response
Benchmark that uses generative artificial intelligence to automatically
generate response plans for incoming traffic incidents. Our approach aims to
significantly reduce incident resolution times by suggesting
context-appropriate actions such as variable message sign deployment, lane
closures, and emergency resource allocation adapted to specific incident
characteristics. First, the proposed methodology uses real-world incident
reports from the Performance Measurement System (PeMS) as training and
evaluation data. We extract historically implemented actions from these reports
and compare them against AI-generated response plans that suggest specific
actions, such as lane closures, variable message sign announcements, and/or
dispatching appropriate emergency resources. Second, model evaluations reveal
that advanced generative AI models like GPT-4o and Grok 2 achieve superior
alignment with expert solutions, demonstrated by minimized Hamming distances
(averaging 2.96-2.98) and low weighted differences (approximately 0.27-0.28).
Conversely, while Gemini 1.5 Pro records the lowest count of missed actions,
its extremely high number of unnecessary actions (1547 compared to 225 for
GPT-4o) indicates an over-triggering strategy that reduces the overall plan
efficiency.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [335] [UniSim: A Unified Simulator for Time-Coarsened Dynamics of Biomolecules](https://arxiv.org/abs/2506.03157)
*Ziyang Yu,Wenbing Huang,Yang Liu*

Main category: q-bio.BM

TL;DR: 提出UniSim模型，通过跨领域知识提升分子动力学模拟的准确性和效率，适用于多种分子系统。


<details>
  <summary>Details</summary>
Motivation: 传统分子动力学方法在准确性和效率之间存在权衡，而现有深度学习方法多局限于单一分子领域，缺乏对其他分子系统的适应性。

Method: 采用多头预训练方法学习统一原子表示模型，基于随机插值框架学习长时间步的状态转移模式，并引入力引导模块以适应不同化学环境。

Result: 实验表明UniSim在小分子、肽和蛋白质等多种分子系统中表现出色。

Conclusion: UniSim通过跨领域知识提升了分子动力学模拟的通用性和性能。

Abstract: Molecular Dynamics (MD) simulations are essential for understanding the
atomic-level behavior of molecular systems, giving insights into their
transitions and interactions. However, classical MD techniques are limited by
the trade-off between accuracy and efficiency, while recent deep learning-based
improvements have mostly focused on single-domain molecules, lacking
transferability to unfamiliar molecular systems. Therefore, we propose
\textbf{Uni}fied \textbf{Sim}ulator (UniSim), which leverages cross-domain
knowledge to enhance the understanding of atomic interactions. First, we employ
a multi-head pretraining approach to learn a unified atomic representation
model from a large and diverse set of molecular data. Then, based on the
stochastic interpolant framework, we learn the state transition patterns over
long timesteps from MD trajectories, and introduce a force guidance module for
rapidly adapting to different chemical environments. Our experiments
demonstrate that UniSim achieves highly competitive performance across small
molecules, peptides, and proteins.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [336] [From Average-Iterate to Last-Iterate Convergence in Games: A Reduction and Its Applications](https://arxiv.org/abs/2506.03464)
*Yang Cai,Haipeng Luo,Chen-Yu Wei,Weiqiang Zheng*

Main category: cs.GT

TL;DR: 该论文提出了一种简单黑盒方法，将平均迭代收敛转化为最终迭代收敛，适用于线性效用博弈，并在零和博弈中实现了最优收敛速率。


<details>
  <summary>Details</summary>
Motivation: 研究在线学习算法在博弈中的收敛性，特别是最终迭代收敛，因为它反映了学习者的实际决策和日常行为。

Method: 提出一种黑盒转换方法，将非耦合学习动态的平均迭代收敛转化为新动态的最终迭代收敛。

Result: 在零和博弈中，应用乐观乘法权重更新算法，实现了梯度反馈下的指数级改进和带反馈下的收敛速率提升。

Conclusion: 该方法为线性效用博弈提供了一种通用的收敛性转换工具，显著提升了最终迭代收敛的效率和适用范围。

Abstract: The convergence of online learning algorithms in games under self-play is a
fundamental question in game theory and machine learning. Among various notions
of convergence, last-iterate convergence is particularly desirable, as it
reflects the actual decisions made by the learners and captures the day-to-day
behavior of the learning dynamics. While many algorithms are known to converge
in the average-iterate, achieving last-iterate convergence typically requires
considerably more effort in both the design and the analysis of the algorithm.
Somewhat surprisingly, we show in this paper that for a large family of games,
there exists a simple black-box reduction that transforms the average iterates
of an uncoupled learning dynamics into the last iterates of a new uncoupled
learning dynamics, thus also providing a reduction from last-iterate
convergence to average-iterate convergence. Our reduction applies to games
where each player's utility is linear in both their own strategy and the joint
strategy of all opponents. This family includes two-player bimatrix games and
generalizations such as multi-player polymatrix games. By applying our
reduction to the Optimistic Multiplicative Weights Update algorithm, we obtain
new state-of-the-art last-iterate convergence rates for uncoupled learning
dynamics in two-player zero-sum normal-form games: (1) an $O(\frac{\log d}{T})$
last-iterate convergence rate under gradient feedback, representing an
exponential improvement in the dependence on the dimension $d$ (i.e., the
maximum number of actions available to either player); and (2) an
$\widetilde{O}(d^{\frac{1}{5}} T^{-\frac{1}{5}})$ last-iterate convergence rate
under bandit feedback, improving upon the previous best rates of
$\widetilde{O}(\sqrt{d} T^{-\frac{1}{8}})$ and $\widetilde{O}(\sqrt{d}
T^{-\frac{1}{6}})$.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [337] [Predicting Postoperative Stroke in Elderly SICU Patients: An Interpretable Machine Learning Model Using MIMIC Data](https://arxiv.org/abs/2506.03209)
*Tinghuan Li,Shuheng Chen,Junyi Fan,Elham Pishgar,Kamiar Alaei,Greg Placencia,Maryam Pishgar*

Main category: q-bio.QM

TL;DR: 该研究开发了一个可解释的机器学习框架，用于预测老年SICU患者术后院内卒中风险，通过特征选择和模型优化，CatBoost模型表现最佳，AUROC达0.8868。


<details>
  <summary>Details</summary>
Motivation: 术后卒中是老年SICU患者的严重并发症，导致住院时间延长、医疗成本增加和死亡率上升。早期准确的风险分层对及时干预和改善临床结果至关重要。

Method: 研究结合MIMIC-III和MIMIC-IV数据库中的19,085例老年SICU入院数据，采用预处理流程（包括特征去除、SVD插补、标准化等）和两阶段特征选择（RFECV和SHAP），评估了八种机器学习模型。

Result: CatBoost模型表现最佳，AUROC为0.8868。SHAP分析和消融研究确定既往脑血管疾病、血清肌酐和收缩压为最重要的风险因素。

Conclusion: 可解释的机器学习方法在术后卒中早期检测和围手术期重症决策中具有潜在应用价值。

Abstract: Postoperative stroke remains a critical complication in elderly surgical
intensive care unit (SICU) patients, contributing to prolonged hospitalization,
elevated healthcare costs, and increased mortality. Accurate early risk
stratification is essential to enable timely intervention and improve clinical
outcomes. We constructed a combined cohort of 19,085 elderly SICU admissions
from the MIMIC-III and MIMIC-IV databases and developed an interpretable
machine learning (ML) framework to predict in-hospital stroke using clinical
data from the first 24 hours of Intensive Care Unit (ICU) stay. The
preprocessing pipeline included removal of high-missingness features, iterative
Singular Value Decomposition (SVD) imputation, z-score normalization, one-hot
encoding, and class imbalance correction via the Adaptive Synthetic Sampling
(ADASYN) algorithm. A two-stage feature selection process-combining Recursive
Feature Elimination with Cross-Validation (RFECV) and SHapley Additive
exPlanations (SHAP)-reduced the initial 80 variables to 20 clinically
informative predictors. Among eight ML models evaluated, CatBoost achieved the
best performance with an AUROC of 0.8868 (95% CI: 0.8802--0.8937). SHAP
analysis and ablation studies identified prior cerebrovascular disease, serum
creatinine, and systolic blood pressure as the most influential risk factors.
Our results highlight the potential of interpretable ML approaches to support
early detection of postoperative stroke and inform decision-making in
perioperative critical care.

</details>


### [338] [UniSite: The First Cross-Structure Dataset and Learning Framework for End-to-End Ligand Binding Site Detection](https://arxiv.org/abs/2506.03237)
*Jigang Fan,Quanlin Wu,Shengjie Luo,Liwei Wang*

Main category: q-bio.QM

TL;DR: 本文提出了首个以UniProt为中心的配体结合位点数据集UniSite-DS，以及端到端的检测框架UniSite，并引入基于IoU的平均精度作为更准确的评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有配体结合位点检测方法存在数据集偏差、流程不连续及评估指标不准确三大挑战，需开发更全面的数据集和端到端框架。

Method: 构建UniSite-DS数据集，提出基于集合预测损失和双射匹配的端到端检测框架UniSite，采用IoU平均精度作为新评估指标。

Result: 实验表明，UniSite-DS数据量显著提升，UniSite框架性能优于现有方法，IoU平均精度能更准确反映预测质量。

Conclusion: UniSite-DS和UniSite框架有效解决了当前配体结合位点检测的三大挑战，为结构药物设计提供了更可靠的工具。

Abstract: The detection of ligand binding sites for proteins is a fundamental step in
Structure-Based Drug Design. Despite notable advances in recent years, existing
methods, datasets, and evaluation metrics are confronted with several key
challenges: (1) current datasets and methods are centered on individual
protein-ligand complexes and neglect that diverse binding sites may exist
across multiple complexes of the same protein, introducing significant
statistical bias; (2) ligand binding site detection is typically modeled as a
discontinuous workflow, employing binary segmentation and subsequent clustering
algorithms; (3) traditional evaluation metrics do not adequately reflect the
actual performance of different binding site prediction methods. To address
these issues, we first introduce UniSite-DS, the first UniProt (Unique
Protein)-centric ligand binding site dataset, which contains 4.81 times more
multi-site data and 2.08 times more overall data compared to the previously
most widely used datasets. We then propose UniSite, the first end-to-end ligand
binding site detection framework supervised by set prediction loss with
bijective matching. In addition, we introduce Average Precision based on
Intersection over Union (IoU) as a more accurate evaluation metric for ligand
binding site prediction. Extensive experiments on UniSite-DS and several
representative benchmark datasets demonstrate that IoU-based Average Precision
provides a more accurate reflection of prediction quality, and that UniSite
outperforms current state-of-the-art methods in ligand binding site detection.
The dataset and codes will be made publicly available at
https://github.com/quanlin-wu/unisite.

</details>


### [339] [Quantum Cognition Machine Learning for Forecasting Chromosomal Instability](https://arxiv.org/abs/2506.03199)
*Giuseppe Di Caro,Vahagn Kirakosyan,Alexander G. Abanov,Luca Candelori,Nadine Hartmann,Ernest T. Lam,Kharen Musaelian,Ryan Samson,Dario Villani,Martin T. Wells,Richard J. Wenstrup,Mengjia Xu*

Main category: q-bio.QM

TL;DR: 该论文提出了一种基于量子认知机器学习（QCML）的新方法，用于从循环肿瘤细胞（CTCs）的形态中预测染色体不稳定性，在液体活检诊断中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 由于单细胞数字病理数据的高维性和复杂性，从循环肿瘤细胞（CTCs）的形态准确预测染色体不稳定性具有挑战性，但这对实时检测高转移潜能的CTCs至关重要。

Method: 采用量子认知机器学习（QCML）框架，利用量子力学原理将数据表示为希尔伯特空间中的状态向量，实现上下文感知的特征建模、降维和增强的泛化能力，无需人工特征选择。

Result: QCML在样本外验证CTCs中表现优于传统机器学习方法，能更准确地从CTCs形态特征中预测大规模状态转换（pLST）状态。

Conclusion: QCML作为一种新型机器学习工具，在高维、小样本的生物医学场景中表现出优越性能，为液体活检中的CTCs分类提供了新方法。

Abstract: The accurate prediction of chromosomal instability from the morphology of
circulating tumor cells (CTCs) enables real-time detection of CTCs with high
metastatic potential in the context of liquid biopsy diagnostics. However, it
presents a significant challenge due to the high dimensionality and complexity
of single-cell digital pathology data. Here, we introduce the application of
Quantum Cognition Machine Learning (QCML), a quantum-inspired computational
framework, to estimate morphology-predicted chromosomal instability in CTCs
from patients with metastatic breast cancer. QCML leverages quantum mechanical
principles to represent data as state vectors in a Hilbert space, enabling
context-aware feature modeling, dimensionality reduction, and enhanced
generalization without requiring curated feature selection. QCML outperforms
conventional machine learning methods when tested on out of sample verification
CTCs, achieving higher accuracy in identifying predicted large-scale state
transitions (pLST) status from CTC-derived morphology features. These
preliminary findings support the application of QCML as a novel machine
learning tool with superior performance in high-dimensional, low-sample-size
biomedical contexts. QCML enables the simulation of cognition-like learning for
the identification of biologically meaningful prediction of chromosomal
instability from CTC morphology, offering a novel tool for CTC classification
in liquid biopsy.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [340] [Structural Vibration Monitoring with Diffractive Optical Processors](https://arxiv.org/abs/2506.03317)
*Yuntian Wang,Zafer Yilmaz,Yuhang Li,Edward Liu,Eric Ahlberg,Farid Ghahari,Ertugrul Taciroglu,Aydogan Ozcan*

Main category: physics.optics

TL;DR: 提出了一种结合衍射层与浅层神经网络的低功耗、低成本、可扩展的3D结构振动监测系统，显著提高了监测精度。


<details>
  <summary>Details</summary>
Motivation: 当前结构健康监测(SHM)方案受限于成本、功耗、可扩展性及数据处理复杂性，亟需一种高效解决方案。

Method: 通过空间优化的被动衍射层将3D结构位移编码为调制光信号，由少量探测器捕获，并由低功耗浅层神经网络实时解码。

Result: 系统在毫米波实验中对建筑模型实现了超过一个数量级的精度提升，验证了其高效性。

Conclusion: 该框架为结构3D监测奠定了基础，并在灾害韧性、航空航天诊断等领域具有潜在应用价值。

Abstract: Structural Health Monitoring (SHM) is vital for maintaining the safety and
longevity of civil infrastructure, yet current solutions remain constrained by
cost, power consumption, scalability, and the complexity of data processing.
Here, we present a diffractive vibration monitoring system, integrating a
jointly optimized diffractive layer with a shallow neural network-based backend
to remotely extract 3D structural vibration spectra, offering a low-power,
cost-effective and scalable solution. This architecture eliminates the need for
dense sensor arrays or extensive data acquisition; instead, it uses a
spatially-optimized passive diffractive layer that encodes 3D structural
displacements into modulated light, captured by a minimal number of detectors
and decoded in real-time by shallow and low-power neural networks to
reconstruct the 3D displacement spectra of structures. The diffractive system's
efficacy was demonstrated both numerically and experimentally using
millimeter-wave illumination on a laboratory-scale building model with a
programmable shake table. Our system achieves more than an order-of-magnitude
improvement in accuracy over conventional optics or separately trained modules,
establishing a foundation for high-throughput 3D monitoring of structures.
Beyond SHM, the 3D vibration monitoring capabilities of this cost-effective and
data-efficient framework establish a new computational sensing modality with
potential applications in disaster resilience, aerospace diagnostics, and
autonomous navigation, where energy efficiency, low latency, and
high-throughput are critical.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [341] [Differentially Private Distribution Release of Gaussian Mixture Models via KL-Divergence Minimization](https://arxiv.org/abs/2506.03467)
*Hang Liu,Anna Scaglione,Sean Peisert*

Main category: cs.IT

TL;DR: 本文提出了一种在保证差分隐私的前提下发布高斯混合模型参数的方法，通过优化KL散度来平衡隐私保护与模型效用。


<details>
  <summary>Details</summary>
Motivation: 高斯混合模型（GMM）广泛应用于多模态数据建模，但其参数发布可能泄露敏感信息。如何在保证差分隐私的同时发布GMM参数成为关键挑战。

Method: 采用KL散度作为效用指标，设计差分隐私机制对GMM参数（混合权重、均值、协方差矩阵）添加噪声扰动，并通过优化问题最小化KL散度。

Result: 理论分析量化了隐私预算分配与噪声统计的影响，实验表明该方法在合成和真实数据集上均能实现强隐私保护与高模型效用。

Conclusion: 所提方法有效解决了GMM参数发布的隐私风险问题，为差分隐私下的统计模型发布提供了实用解决方案。

Abstract: Gaussian Mixture Models (GMMs) are widely used statistical models for
representing multi-modal data distributions, with numerous applications in data
mining, pattern recognition, data simulation, and machine learning. However,
recent research has shown that releasing GMM parameters poses significant
privacy risks, potentially exposing sensitive information about the underlying
data. In this paper, we address the challenge of releasing GMM parameters while
ensuring differential privacy (DP) guarantees. Specifically, we focus on the
privacy protection of mixture weights, component means, and covariance
matrices. We propose to use Kullback-Leibler (KL) divergence as a utility
metric to assess the accuracy of the released GMM, as it captures the joint
impact of noise perturbation on all the model parameters. To achieve privacy,
we introduce a DP mechanism that adds carefully calibrated random perturbations
to the GMM parameters. Through theoretical analysis, we quantify the effects of
privacy budget allocation and perturbation statistics on the DP guarantee, and
derive a tractable expression for evaluating KL divergence. We formulate and
solve an optimization problem to minimize the KL divergence between the
released and original models, subject to a given $(\epsilon, \delta)$-DP
constraint. Extensive experiments on both synthetic and real-world datasets
demonstrate that our approach achieves strong privacy guarantees while
maintaining high utility.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [342] [Deep Learning-Based Breast Cancer Detection in Mammography: A Multi-Center Validation Study in Thai Population](https://arxiv.org/abs/2506.03177)
*Isarun Chamveha,Supphanut Chaiyungyuen,Sasinun Worakriangkrai,Nattawadee Prasawang,Warasinee Chaisangmongkon,Pornpim Korpraphong,Voraparee Suvannarerg,Shanigarn Thiravit,Chalermdej Kannawat,Kewalin Rungsinaporn,Suwara Issaragrisil,Payia Chadbunchachai,Pattiya Gatechumpol,Chawiporn Muktabhant,Patarachai Sereerat*

Main category: eess.IV

TL;DR: 该研究提出了一种基于改进EfficientNetV2架构的深度学习系统，用于乳腺X光摄影中的乳腺癌检测，并在多个数据集上验证了其高性能和临床适用性。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种高效的深度学习系统，以提升乳腺X光摄影中乳腺癌的检测准确性和临床工作流程效率。

Method: 采用改进的EfficientNetV2架构，增强注意力机制，并在泰国一家主要医疗中心的乳腺X光摄影数据上进行训练和验证。

Result: 模型在三个不同数据集上的AUROC分别为0.89、0.96和0.94，临床验证显示与放射科医生的分类和定位一致性较高，专家接受率平均超过89%。

Conclusion: 该系统在乳腺癌检测中表现出色，具有提升临床筛查工作流程的潜力，且获得了良好的临床接受度。

Abstract: This study presents a deep learning system for breast cancer detection in
mammography, developed using a modified EfficientNetV2 architecture with
enhanced attention mechanisms. The model was trained on mammograms from a major
Thai medical center and validated on three distinct datasets: an in-domain test
set (9,421 cases), a biopsy-confirmed set (883 cases), and an out-of-domain
generalizability set (761 cases) collected from two different hospitals. For
cancer detection, the model achieved AUROCs of 0.89, 0.96, and 0.94 on the
respective datasets. The system's lesion localization capability, evaluated
using metrics including Lesion Localization Fraction (LLF) and Non-Lesion
Localization Fraction (NLF), demonstrated robust performance in identifying
suspicious regions. Clinical validation through concordance tests showed strong
agreement with radiologists: 83.5% classification and 84.0% localization
concordance for biopsy-confirmed cases, and 78.1% classification and 79.6%
localization concordance for out-of-domain cases. Expert radiologists'
acceptance rate also averaged 96.7% for biopsy-confirmed cases, and 89.3% for
out-of-domain cases. The system achieved a System Usability Scale score of
74.17 for source hospital, and 69.20 for validation hospitals, indicating good
clinical acceptance. These results demonstrate the model's effectiveness in
assisting mammogram interpretation, with the potential to enhance breast cancer
screening workflows in clinical practice.

</details>


### [343] [LLaMA-XR: A Novel Framework for Radiology Report Generation using LLaMA and QLoRA Fine Tuning](https://arxiv.org/abs/2506.03178)
*Md. Zihad Bin Jahangir,Muhammad Ashad Kabir,Sumaiya Akter,Israt Jahan,Minh Chau*

Main category: eess.IV

TL;DR: 论文提出LLaMA-XR框架，结合LLaMA 3.1与DenseNet-121图像嵌入及QLoRA微调，提升放射报告生成的准确性与效率，在IU X-ray数据集上取得SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有放射报告生成模型难以兼顾准确性与上下文相关性，临床实用性受限。需解决医学语言复杂性和上下文理解挑战。

Method: 整合LLaMA 3.1语言模型与DenseNet-121图像特征，采用QLoRA量化低秩适配技术进行微调，优化参数利用并降低内存开销。

Result: 在IU X-ray数据集上ROUGE-L达0.433，METEOR达0.336，超越现有方法，且计算效率更高。

Conclusion: LLaMA-XR证明了其作为高效AI放射报告系统的潜力，兼具临床实用性与可靠性。

Abstract: Automated radiology report generation holds significant potential to reduce
radiologists' workload and enhance diagnostic accuracy. However, generating
precise and clinically meaningful reports from chest radiographs remains
challenging due to the complexity of medical language and the need for
contextual understanding. Existing models often struggle with maintaining both
accuracy and contextual relevance. In this paper, we present LLaMA-XR, a novel
framework that integrates LLaMA 3.1 with DenseNet-121-based image embeddings
and Quantized Low-Rank Adaptation (QLoRA) fine-tuning. LLaMA-XR achieves
improved coherence and clinical accuracy while maintaining computational
efficiency. This efficiency is driven by an optimization strategy that enhances
parameter utilization and reduces memory overhead, enabling faster report
generation with lower computational resource demands. Extensive experiments
conducted on the IU X-ray benchmark dataset demonstrate that LLaMA-XR
outperforms a range of state-of-the-art methods. Our model achieves a ROUGE-L
score of 0.433 and a METEOR score of 0.336, establishing new performance
benchmarks in the domain. These results underscore LLaMA-XR's potential as an
effective and efficient AI system for automated radiology reporting, offering
enhanced clinical utility and reliability.

</details>


### [344] [Edge Computing for Physics-Driven AI in Computational MRI: A Feasibility Study](https://arxiv.org/abs/2506.03183)
*Yaşar Utku Alçalar,Yu Cao,Mehmet Akçakaya*

Main category: eess.IV

TL;DR: 该论文提出了一种针对FPGA边缘计算设备优化的新型PD-AI MRI重建方法，通过8位复数数据量化和消除冗余FFT/IFFT操作，提高了计算效率并保持了重建质量。


<details>
  <summary>Details</summary>
Motivation: 高分辨率MRI扫描产生大量数据，导致传输、存储和实时处理方面的挑战。特别是在功能性MRI中，数百个体积采集进一步加剧了这些需求。边缘计算结合FPGA为在MRI传感器附近实现PD-AI重建提供了有前景的解决方案。

Method: 采用8位复数数据量化技术，并消除冗余的FFT/IFFT操作，优化PD-AI模型以适应FPGA硬件的高效计算需求。

Result: 该方法在保持与传统PD-AI方法相当的重建质量的同时，提高了计算效率，并且优于标准的临床方法。

Conclusion: 该研究为资源受限设备上的高分辨率MRI重建提供了可行方案，展现了其在实际应用中的潜力。

Abstract: Physics-driven artificial intelligence (PD-AI) reconstruction methods have
emerged as the state-of-the-art for accelerating MRI scans, enabling higher
spatial and temporal resolutions. However, the high resolution of these scans
generates massive data volumes, leading to challenges in transmission, storage,
and real-time processing. This is particularly pronounced in functional MRI,
where hundreds of volumetric acquisitions further exacerbate these demands.
Edge computing with FPGAs presents a promising solution for enabling PD-AI
reconstruction near the MRI sensors, reducing data transfer and storage
bottlenecks. However, this requires optimization of PD-AI models for hardware
efficiency through quantization and bypassing traditional FFT-based approaches,
which can be a limitation due to their computational demands. In this work, we
propose a novel PD-AI computational MRI approach optimized for FPGA-based edge
computing devices, leveraging 8-bit complex data quantization and eliminating
redundant FFT/IFFT operations. Our results show that this strategy improves
computational efficiency while maintaining reconstruction quality comparable to
conventional PD-AI methods, and outperforms standard clinical methods. Our
approach presents an opportunity for high-resolution MRI reconstruction on
resource-constrained devices, highlighting its potential for real-world
deployment.

</details>


### [345] [DLiPath: A Benchmark for the Comprehensive Assessment of Donor Liver Based on Histopathological Image Dataset](https://arxiv.org/abs/2506.03185)
*Liangrui Pan,Xingchen Li,Zhongyi Chen,Ling Chu,Shaoliang Peng*

Main category: eess.IV

TL;DR: 该论文介绍了DLiPath，首个基于组织病理学图像数据集的供体肝脏综合评估基准，旨在解决供体肝活检评估中的快速准确性问题。


<details>
  <summary>Details</summary>
Motivation: 供体肝活检的快速准确评估对移植决策至关重要，但现有方法存在观察者间和观察者内变异性大的问题。

Method: 收集并公开了304例供体肝患者的636张全切片图像，采用九种先进的多实例学习模型进行基准测试。

Result: 实验表明，多种MIL模型在DLiPath数据集上实现了高准确率，为未来自动化评估研究指明了方向。

Conclusion: DLiPath为供体肝评估提供了标准化基准，推动了该领域自动化与智能化研究的发展。

Abstract: Pathologists comprehensive evaluation of donor liver biopsies provides
crucial information for accepting or discarding potential grafts. However,
rapidly and accurately obtaining these assessments intraoperatively poses a
significant challenge for pathologists. Features in donor liver biopsies, such
as portal tract fibrosis, total steatosis, macrovesicular steatosis, and
hepatocellular ballooning are correlated with transplant outcomes, yet
quantifying these indicators suffers from substantial inter- and intra-observer
variability. To address this, we introduce DLiPath, the first benchmark for
comprehensive donor liver assessment based on a histopathology image dataset.
We collected and publicly released 636 whole slide images from 304 donor liver
patients at the Department of Pathology, the Third Xiangya Hospital, with
expert annotations for key pathological features (including cholestasis, portal
tract fibrosis, portal inflammation, total steatosis, macrovesicular steatosis,
and hepatocellular ballooning). We selected nine state-of-the-art
multiple-instance learning (MIL) models based on the DLiPath dataset as
baselines for extensive comparative analysis. The experimental results
demonstrate that several MIL models achieve high accuracy across donor liver
assessment indicators on DLiPath, charting a clear course for future automated
and intelligent donor liver assessment research. Data and code are available at
https://github.com/panliangrui/ACM_MM_2025.

</details>


### [346] [Lightweight Convolutional Neural Networks for Retinal Disease Classification](https://arxiv.org/abs/2506.03186)
*Duaa Kareem Qasim,Sabah Abdulazeez Jebur,Lafta Raheem Ali,Abdul Jalil M. Khalaf,Abir Jaafar Hussain*

Main category: eess.IV

TL;DR: 该论文使用MobileNet和NASNetMobile两种轻量级CNN架构，通过迁移学习和数据增强技术，在RFMiD数据集上实现了对视网膜疾病（糖尿病视网膜病变和黄斑裂孔）的高效分类，其中MobileNetV2准确率达90.8%。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变（DR）和黄斑裂孔（MH）等视网膜疾病严重影响视力，早期检测对预防视力丧失至关重要。

Method: 采用MobileNet和NASNetMobile两种轻量级CNN架构，利用RFMiD数据集（3,200张眼底图像），通过调整大小、归一化和数据增强等预处理步骤，结合迁移学习提升模型性能。

Result: MobileNetV2表现最佳，准确率达90.8%，优于NASNetMobile的89.5%。

Conclusion: CNN在视网膜疾病分类中效果显著，为AI辅助眼科诊断和早期干预奠定了基础。

Abstract: Retinal diseases such as Diabetic Retinopathy (DR) and Macular Hole (MH)
significantly impact vision and affect millions worldwide. Early detection is
crucial, as DR, a complication of diabetes, damages retinal blood vessels,
potentially leading to blindness, while MH disrupts central vision, affecting
tasks like reading and facial recognition. This paper employed two lightweight
and efficient Convolution Neural Network architectures, MobileNet and
NASNetMobile, for the classification of Normal, DR, and MH retinal images. The
models were trained on the RFMiD dataset, consisting of 3,200 fundus images,
after undergoing preprocessing steps such as resizing, normalization, and
augmentation. To address data scarcity, this study leveraged transfer learning
and data augmentation techniques, enhancing model generalization and
performance. The experimental results demonstrate that MobileNetV2 achieved the
highest accuracy of 90.8%, outperforming NASNetMobile, which achieved 89.5%
accuracy. These findings highlight the effectiveness of CNNs in retinal disease
classification, providing a foundation for AI-assisted ophthalmic diagnosis and
early intervention.

</details>


### [347] [Multi-Analyte, Swab-based Automated Wound Monitor with AI](https://arxiv.org/abs/2506.03188)
*Madhu Babu Sikha,Lalith Appari,Gurudatt Nanjanagudu Ganesh,Amay Bandodkar,Imon Banerjee*

Main category: eess.IV

TL;DR: 开发低成本3D打印检测棉签和iOS应用，通过计算机视觉自动分析糖尿病足溃疡严重程度，实现实时监测。


<details>
  <summary>Details</summary>
Motivation: 糖尿病足溃疡（DFUs）每年影响大量患者，早期识别不愈合溃疡可降低治疗成本和截肢风险，亟需早期诊断工具。

Method: 集成3D打印多分析物检测棉签和iOS应用，通过对比伤口暴露前后的图像密度变化，自动化分析伤口严重程度。

Result: iOS应用确保数据准确采集并提供可行建议，集成传感器和App使医护人员能实时监测伤口状况和愈合进度。

Conclusion: 该集成传感器和移动应用方案为糖尿病足溃疡的早期诊断和实时监测提供了创新解决方案。

Abstract: Diabetic foot ulcers (DFUs), a class of chronic wounds, affect ~750,000
individuals every year in the US alone and identifying non-healing DFUs that
develop to chronic wounds early can drastically reduce treatment costs and
minimize risks of amputation. There is therefore a pressing need for diagnostic
tools that can detect non-healing DFUs early. We develop a low cost,
multi-analyte 3D printed assays seamlessly integrated on swabs that can
identify non-healing DFUs and a Wound Sensor iOS App - an innovative mobile
application developed for the controlled acquisition and automated analysis of
wound sensor data. By comparing both the original base image (before exposure
to the wound) and the wound-exposed image, we developed automated computer
vision techniques to compare density changes between the two assay images,
which allow us to automatically determine the severity of the wound. The iOS
app ensures accurate data collection and presents actionable insights, despite
challenges such as variations in camera configurations and ambient conditions.
The proposed integrated sensor and iOS app will allow healthcare professionals
to monitor wound conditions real-time, track healing progress, and assess
critical parameters related to wound care.

</details>


### [348] [Encoding of Demographic and Anatomical Information in Chest X-Ray-based Severe Left Ventricular Hypertrophy Classifiers](https://arxiv.org/abs/2506.03192)
*Basudha Pal,Rama Chellappa,Muhammad Umair*

Main category: eess.IV

TL;DR: 该论文提出了一种直接分类框架，通过胸部X光预测严重左心室肥厚，无需解剖测量或人口统计输入，实现了高AUROC和AUPRC，并利用互信息神经估计量化特征表达性。


<details>
  <summary>Details</summary>
Motivation: 超声心动图和MRI虽然是评估心脏结构的临床标准，但其成本高且可及性有限，因此需要一种更便捷的替代方法。

Method: 引入直接分类框架，利用胸部X光预测严重左心室肥厚，采用互信息神经估计量化特征表达性。

Result: 该方法实现了高AUROC和AUPRC，揭示了具有临床意义的属性编码，并支持透明的模型解释。

Conclusion: 该框架为心脏结构评估提供了一种低成本、高可及性的替代方案，具有临床实用性和透明性。

Abstract: While echocardiography and MRI are clinical standards for evaluating cardiac
structure, their use is limited by cost and accessibility.We introduce a direct
classification framework that predicts severe left ventricular hypertrophy from
chest X-rays, without relying on anatomical measurements or demographic inputs.
Our approach achieves high AUROC and AUPRC, and employs Mutual Information
Neural Estimation to quantify feature expressivity. This reveals clinically
meaningful attribute encoding and supports transparent model interpretation.

</details>


### [349] [Rethinking Whole-Body CT Image Interpretation: An Abnormality-Centric Approach](https://arxiv.org/abs/2506.03238)
*Ziheng Zhao,Lisong Dai,Ya Zhang,Yanfeng Wang,Weidi Xie*

Main category: eess.IV

TL;DR: 该论文提出了一种名为OminiAbnorm-CT的自动化系统，用于定位和描述多平面及全身CT图像中的异常发现，通过构建分类体系、数据集、模型开发和基准测试显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 临床放射学中，自动化解释CT图像（尤其是多平面和全身扫描中的异常定位与描述）仍面临重大挑战。

Method: 论文通过四个关键贡献解决问题：(i) 与资深放射科医生合作构建包含404种异常发现的分类体系；(ii) 提供包含14.5K CT图像和19K异常标注的数据集；(iii) 开发支持文本查询和视觉交互的OminiAbnorm-CT模型；(iv) 基于真实临床场景设计三项评估任务。

Result: 实验表明，OminiAbnorm-CT在所有任务和指标上显著优于现有方法。

Conclusion: 该研究通过系统性方法实现了CT图像异常的高效自动化分析，为临床实践提供了实用工具。

Abstract: Automated interpretation of CT images-particularly localizing and describing
abnormal findings across multi-plane and whole-body scans-remains a significant
challenge in clinical radiology. This work aims to address this challenge
through four key contributions: (i) On taxonomy, we collaborate with senior
radiologists to propose a comprehensive hierarchical classification system,
with 404 representative abnormal findings across all body regions; (ii) On
data, we contribute a dataset containing over 14.5K CT images from multiple
planes and all human body regions, and meticulously provide grounding
annotations for over 19K abnormalities, each linked to the detailed description
and cast into the taxonomy; (iii) On model development, we propose
OminiAbnorm-CT, which can automatically ground and describe abnormal findings
on multi-plane and whole-body CT images based on text queries, while also
allowing flexible interaction through visual prompts; (iv) On benchmarks, we
establish three representative evaluation tasks based on real clinical
scenarios. Through extensive experiments, we show that OminiAbnorm-CT can
significantly outperform existing methods on all the tasks and metrics.

</details>


### [350] [Adaptive and Robust Image Processing on CubeSats](https://arxiv.org/abs/2506.03152)
*Robert Bayer,Julian Priest,Daniel Kjellberg,Jeppe Lindhard,Nikolaj Sørenesen,Nicolaj Valsted,Ívar Óli,Pınar Tözün*

Main category: eess.IV

TL;DR: CubeSats在空间研究中成本低但资源受限，本文提出DIPP和DISH系统解决图像处理流程的灵活性和复杂性挑战。


<details>
  <summary>Details</summary>
Motivation: CubeSats作为低成本的空间研究平台，在资源受限的环境下，其图像处理流程的灵活性和复杂性面临挑战。

Method: 提出DIPP（模块化、可配置的图像处理流程框架）和DISH（特定领域语言及运行时系统），分别用于适应任务目标变化和调度复杂成像任务。

Result: 实验表明DIPP分解处理流程几乎无额外开销，显著降低更新流程的网络需求，且对错误模块上传具有鲁棒性；DISH与通用脚本语言Lua相比，表达力相当且内存需求更低。

Conclusion: DIPP和DISH有效解决了CubeSats在资源受限环境下图像处理流程的挑战，提升了灵活性和效率。

Abstract: CubeSats offer a low-cost platform for space research, particularly for Earth
observation. However, their resource-constrained nature and being in space,
challenge the flexibility and complexity of the deployed image processing
pipelines and their orchestration. This paper introduces two novel systems,
DIPP and DISH, to address these challenges. DIPP is a modular and configurable
image processing pipeline framework that allows for adaptability to changing
mission goals even after deployment, while preserving robustness. DISH is a
domain-specific language (DSL) and runtime system designed to schedule complex
imaging workloads on low-power and memory-constrained processors.
  Our experiments demonstrate that DIPP's decomposition of the processing
pipelines adds negligible overhead, while significantly reducing the network
requirements of updating pipelines and being robust against erroneous module
uploads. Furthermore, we compare DISH to Lua, a general purpose scripting
language, and demonstrate its comparable expressiveness and lower memory
requirement.

</details>


### [351] [A combined Machine Learning and Finite Element Modelling tool for the surgical planning of craniosynostosis correction](https://arxiv.org/abs/2506.03202)
*Itxasne Antúnez Sáenz,Ane Alberdi Aramendi,David Dunaway,Juling Ong,Lara Deliège,Amparo Sáenz,Anita Ahmadi Birjandi,Noor UI Owase Jeelani,Silvia Schievano,Alessandro Borghi*

Main category: eess.IV

TL;DR: 该研究开发了一种基于3D照片和机器学习的实时预测工具，用于优化颅缝早闭症手术规划，减少CT扫描需求。


<details>
  <summary>Details</summary>
Motivation: 当前颅缝早闭症手术结果难以预测，依赖医生经验和患儿年龄，且传统有限元建模方法复杂且需CT扫描。研究旨在开发无辐射的实时预测工具。

Method: 基于3D照片创建个性化合成颅骨模型，结合人群平均数据，使用支持向量回归机器学习模型预测手术效果。

Result: 模型R2达0.95，MSE和MAE低于0.13，能准确预测手术结果，未来还可优化手术参数。

Conclusion: 该机器学习工具成功实现了无CT的颅缝早闭症手术效果预测，为临床决策提供了高效支持。

Abstract: Craniosynostosis is a medical condition that affects the growth of babies'
heads, caused by an early fusion of cranial sutures. In recent decades,
surgical treatments for craniosynostosis have significantly improved, leading
to reduced invasiveness, faster recovery, and less blood loss. At Great Ormond
Street Hospital (GOSH), the main surgical treatment for patients diagnosed with
sagittal craniosynostosis (SC) is spring assisted cranioplasty (SAC). This
procedure involves a 15x15 mm2 osteotomy, where two springs are inserted to
induce distraction. Despite the numerous advantages of this surgical technique
for patients, the outcome remains unpredictable due to the lack of efficient
preoperative planning tools. The surgeon's experience and the baby's age are
currently relied upon to determine the osteotomy location and spring selection.
Previous tools for predicting the surgical outcome of SC relied on finite
element modeling (FEM), which involved computed tomography (CT) imaging and
required engineering expertise and lengthy calculations. The main goal of this
research is to develop a real-time prediction tool for the surgical outcome of
patients, eliminating the need for CT scans to minimise radiation exposure
during preoperative planning. The proposed methodology involves creating
personalised synthetic skulls based on three-dimensional (3D) photographs,
incorporating population average values of suture location, skull thickness,
and soft tissue properties. A machine learning (ML) surrogate model is employed
to achieve the desired surgical outcome. The resulting multi-output support
vector regressor model achieves a R2 metric of 0.95 and MSE and MAE below 0.13.
Furthermore, in the future, this model could not only simulate various surgical
scenarios but also provide optimal parameters for achieving a maximum cranial
index (CI).

</details>


### [352] [A Survey of Deep Learning Video Super-Resolution](https://arxiv.org/abs/2506.03216)
*Arbind Agrahari Baniya,Tsz-Kwan Lee,Peter Eklund,Sunil Aryal*

Main category: eess.IV

TL;DR: 本文综述了基于深度学习的视频超分辨率（VSR）模型，分析了其组件、技术趋势及挑战，并提出了多级分类法以指导未来研究。


<details>
  <summary>Details</summary>
Motivation: 当前VSR领域深度学习技术发展迅速，但文献中方法的使用缺乏充分解释，且决策多由量化改进驱动。鉴于VSR在多个领域的潜在影响，需对其研究中的组件和深度学习方法进行全面分析。

Method: 通过系统分析和分类现有VSR模型的组件与技术，建立多级分类法，并探讨各组件的影响。

Result: 识别了VSR领域的趋势、需求和挑战，并首次提出了基于深度学习的VSR模型的综合分类法。

Conclusion: 本研究为VSR领域提供了系统化的分析框架，有助于针对具体应用需求开发模型，并推动VSR实践的成熟与解释。

Abstract: Video super-resolution (VSR) is a prominent research topic in low-level
computer vision, where deep learning technologies have played a significant
role. The rapid progress in deep learning and its applications in VSR has led
to a proliferation of tools and techniques in the literature. However, the
usage of these methods is often not adequately explained, and decisions are
primarily driven by quantitative improvements. Given the significance of VSR's
potential influence across multiple domains, it is imperative to conduct a
comprehensive analysis of the elements and deep learning methodologies employed
in VSR research. This methodical analysis will facilitate the informed
development of models tailored to specific application needs. In this paper, we
present an overarching overview of deep learning-based video super-resolution
models, investigating each component and discussing its implications.
Furthermore, we provide a synopsis of key components and technologies employed
by state-of-the-art and earlier VSR models. By elucidating the underlying
methodologies and categorising them systematically, we identified trends,
requirements, and challenges in the domain. As a first-of-its-kind survey of
deep learning-based VSR models, this work also establishes a multi-level
taxonomy to guide current and future VSR research, enhancing the maturation and
interpretation of VSR practices for various practical applications.

</details>


### [353] [Hybrid Ensemble of Segmentation-Assisted Classification and GBDT for Skin Cancer Detection with Engineered Metadata and Synthetic Lesions from ISIC 2024 Non-Dermoscopic 3D-TBP Images](https://arxiv.org/abs/2506.03420)
*Muhammad Zubair Hasan,Fahmida Yasmin Rifat*

Main category: eess.IV

TL;DR: 该论文提出了一种结合机器学习和深度学习的混合方法，用于分类恶性和良性皮肤病变，通过使用SLICE-3D数据集和多种技术手段，取得了较高的分类性能。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌是全球范围内最常见且危及生命的疾病之一，早期检测对患者预后至关重要。论文旨在开发一种高效、可解释的AI系统，用于皮肤癌的远程医疗和资源有限环境下的分类。

Method: 论文采用了一种混合方法，结合了视觉变换器（EVA02）和设计的卷积ViT混合模型（EdgeNeXtSAC），通过分割辅助分类流程增强病变定位，并使用梯度提升决策树（GBDT）集成进行预测融合。此外，还通过生成合成病变和应用诊断知情重标记策略来解决类别不平衡问题。

Result: 使用部分AUC（pAUC）作为评估指标，在80%以上的真阳性率（TPR）下，该方法达到了0.1755的pAUC，是所有配置中最高的。

Conclusion: 该研究展示了混合、可解释的AI系统在皮肤癌分类中的潜力，特别是在远程医疗和资源有限的环境中具有重要应用价值。

Abstract: Skin cancer is among the most prevalent and life-threatening diseases
worldwide, with early detection being critical to patient outcomes. This work
presents a hybrid machine and deep learning-based approach for classifying
malignant and benign skin lesions using the SLICE-3D dataset from ISIC 2024,
which comprises 401,059 cropped lesion images extracted from 3D Total Body
Photography (TBP), emulating non-dermoscopic, smartphone-like conditions. Our
method combines vision transformers (EVA02) and our designed convolutional ViT
hybrid (EdgeNeXtSAC) to extract robust features, employing a
segmentation-assisted classification pipeline to enhance lesion localization.
Predictions from these models are fused with a gradient-boosted decision tree
(GBDT) ensemble enriched by engineered features and patient-specific relational
metrics. To address class imbalance and improve generalization, we augment
malignant cases with Stable Diffusion-generated synthetic lesions and apply a
diagnosis-informed relabeling strategy to harmonize external datasets into a
3-class format. Using partial AUC (pAUC) above 80 percent true positive rate
(TPR) as the evaluation metric, our approach achieves a pAUC of 0.1755 -- the
highest among all configurations. These results underscore the potential of
hybrid, interpretable AI systems for skin cancer triage in telemedicine and
resource-constrained settings.

</details>


### [354] [Towards generating more interpretable counterfactuals via concept vectors: a preliminary study on chest X-rays](https://arxiv.org/abs/2506.04058)
*Bulat Maksudov,Kathleen Curran,Alessandra Mileo*

Main category: eess.IV

TL;DR: 该研究通过将临床概念映射到生成模型的潜在空间，识别概念激活向量（CAVs），提供了一种无需显式标签训练即可关联用户定义概念与图像特征的方法，初步在胸片数据上验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 确保医学影像模型与临床知识对齐并具有可解释性是部署过程中的关键步骤。

Method: 使用简单的重建自编码器，将用户定义的概念映射到潜在空间中的概念激活向量（CAVs），无需显式标签训练。

Result: 提取的概念在不同数据集中表现稳定，能够突出临床相关特征。在胸片数据上，对大病理如心脏肥大效果显著，但对小病理因重建限制仍有挑战。

Conclusion: 虽然性能未超越基线方法，但该方法为临床知识对齐的可解释概念解释提供了一条可行路径。

Abstract: An essential step in deploying medical imaging models is ensuring alignment
with clinical knowledge and interpretability. We focus on mapping clinical
concepts into the latent space of generative models to identify Concept
Activation Vectors (CAVs). Using a simple reconstruction autoencoder, we link
user-defined concepts to image-level features without explicit label training.
The extracted concepts are stable across datasets, enabling visual explanations
that highlight clinically relevant features. By traversing latent space along
concept directions, we produce counterfactuals that exaggerate or reduce
specific clinical features. Preliminary results on chest X-rays show promise
for large pathologies like cardiomegaly, while smaller pathologies remain
challenging due to reconstruction limits. Although not outperforming baselines,
this approach offers a path toward interpretable, concept-based explanations
aligned with clinical knowledge.

</details>


### [355] [A Diffusion-Driven Temporal Super-Resolution and Spatial Consistency Enhancement Framework for 4D MRI imaging](https://arxiv.org/abs/2506.04116)
*Xuanru Zhou,Jiarun Liu,Shoujun Yu,Hao Yang,Cheng Li,Tao Tan,Shanshan Wang*

Main category: eess.IV

TL;DR: TSSC-Net提出了一种新框架，通过扩散模型和三向Mamba模块解决4D MRI快速运动下的时空分辨率矛盾，提升动态成像质量。


<details>
  <summary>Details</summary>
Motivation: 传统4D MRI在快速大范围运动时面临时空分辨率矛盾，基于配准的插值方法易产生伪影和空间不一致。需要新方法在保持空间一致性的同时提升时间分辨率。

Method: 1) 基于扩散模型的时间超分网络实现单步6倍帧率提升；2) 三向Mamba模块利用长程上下文信息修正跨切片错位。

Result: 在ACDC心脏MRI和真实膝关节数据集上验证，TSSC-Net能生成高分辨率动态MRI，保持结构保真度和空间一致性。

Conclusion: 该框架有效解决了快速运动导致的时空分辨率折衷问题，为动态MRI提供了高质量的时空超分辨率解决方案。

Abstract: In medical imaging, 4D MRI enables dynamic 3D visualization, yet the
trade-off between spatial and temporal resolution requires prolonged scan time
that can compromise temporal fidelity--especially during rapid, large-amplitude
motion. Traditional approaches typically rely on registration-based
interpolation to generate intermediate frames. However, these methods struggle
with large deformations, resulting in misregistration, artifacts, and
diminished spatial consistency. To address these challenges, we propose
TSSC-Net, a novel framework that generates intermediate frames while preserving
spatial consistency. To improve temporal fidelity under fast motion, our
diffusion-based temporal super-resolution network generates intermediate frames
using the start and end frames as key references, achieving 6x temporal
super-resolution in a single inference step. Additionally, we introduce a novel
tri-directional Mamba-based module that leverages long-range contextual
information to effectively resolve spatial inconsistencies arising from
cross-slice misalignment, thereby enhancing volumetric coherence and correcting
cross-slice errors. Extensive experiments were performed on the public ACDC
cardiac MRI dataset and a real-world dynamic 4D knee joint dataset. The results
demonstrate that TSSC-Net can generate high-resolution dynamic MRI from
fast-motion data while preserving structural fidelity and spatial consistency.

</details>


### [356] [A Comprehensive Study on Medical Image Segmentation using Deep Neural Networks](https://arxiv.org/abs/2506.04121)
*Loan Dao,Ngoc Quoc Ly*

Main category: eess.IV

TL;DR: 本文综述了基于深度神经网络的医学图像分割技术，探讨了其在DIKIW框架下的最新进展，并强调了可解释AI和早期预测在提升诊断透明度和癌症患者生存率中的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在提升医学图像分割技术的性能，通过可解释AI解决深度学习模型的'黑箱'问题，并推动其在疾病早期诊断中的应用，尤其是癌症的及时检测。

Method: 采用系统性综述方法，分析DIKIW框架下医学图像分割的最新技术，并结合可解释AI（XAI）探讨模型透明化解决方案。

Result: 研究总结了当前医学图像分割技术的优势与挑战，提出了提升模型效率和透明度的潜在方法，并强调了早期预测对癌症诊疗的关键作用。

Conclusion: 基于深度学习的医学图像分割技术前景广阔，但需解决可解释性和实施效率问题，以实现从'智能'到'智慧'的跨越。

Abstract: Over the past decade, Medical Image Segmentation (MIS) using Deep Neural
Networks (DNNs) has achieved significant performance improvements and holds
great promise for future developments. This paper presents a comprehensive
study on MIS based on DNNs. Intelligent Vision Systems are often evaluated
based on their output levels, such as Data, Information, Knowledge,
Intelligence, and Wisdom (DIKIW),and the state-of-the-art solutions in MIS at
these levels are the focus of research. Additionally, Explainable Artificial
Intelligence (XAI) has become an important research direction, as it aims to
uncover the "black box" nature of previous DNN architectures to meet the
requirements of transparency and ethics. The study emphasizes the importance of
MIS in disease diagnosis and early detection, particularly for increasing the
survival rate of cancer patients through timely diagnosis. XAI and early
prediction are considered two important steps in the journey from
"intelligence" to "wisdom." Additionally, the paper addresses existing
challenges and proposes potential solutions to enhance the efficiency of
implementing DNN-based MIS.

</details>


### [357] [Recent Advances in Medical Image Classification](https://arxiv.org/abs/2506.04129)
*Loan Dao,Ngoc Quoc Ly*

Main category: eess.IV

TL;DR: 论文综述了医学图像分类领域的最新进展，重点介绍了基于深度学习的方法及其在解决标注数据不足和结果可解释性方面的应用。


<details>
  <summary>Details</summary>
Motivation: 医学图像分类对诊断和治疗至关重要，人工智能的进步为其带来了显著提升。

Method: 论文从基础、特定和应用三个层面回顾了解决方案，重点讨论了卷积神经网络、视觉变换器和视觉语言模型等深度学习方法。

Result: 这些方法有效解决了标注数据有限的问题，并通过可解释人工智能增强了预测结果的可解释性。

Conclusion: 深度学习模型在医学图像分类中展现出巨大潜力，尤其是在处理数据不足和提升结果可解释性方面。

Abstract: Medical image classification is crucial for diagnosis and treatment,
benefiting significantly from advancements in artificial intelligence. The
paper reviews recent progress in the field, focusing on three levels of
solutions: basic, specific, and applied. It highlights advances in traditional
methods using deep learning models like Convolutional Neural Networks and
Vision Transformers, as well as state-of-the-art approaches with Vision
Language Models. These models tackle the issue of limited labeled data, and
enhance and explain predictive results through Explainable Artificial
Intelligence.

</details>


<div id='cond-mat.supr-con'></div>

# cond-mat.supr-con [[Back]](#toc)

### [358] [HTSC-2025: A Benchmark Dataset of Ambient-Pressure High-Temperature Superconductors for AI-Driven Critical Temperature Prediction](https://arxiv.org/abs/2506.03837)
*Xiao-Qi Han,Ze-Feng Gao,Xin-De Wang,Zhenfeng Ouyang,Peng-Jie Guo,Zhong-Yi Lu*

Main category: cond-mat.supr-con

TL;DR: 该论文介绍了HTSC-2025数据集，旨在解决高温超导材料预测领域缺乏基准数据集的问题，以促进AI算法的公平比较和进一步发展。


<details>
  <summary>Details</summary>
Motivation: 高温超导材料的发现对人类工业和日常生活具有重要意义。然而，该领域缺乏广泛接受的基准数据集，严重阻碍了不同AI算法之间的公平比较和方法的进一步改进。

Method: 作者提出了HTSC-2025数据集，这是一个常压高温超导基准数据集，涵盖了2023至2025年间理论物理学家基于BCS超导理论预测的超导材料，包括多个著名系统。

Result: HTSC-2025数据集已在GitHub上开源，并将持续更新，为基于AI的超导材料发现提供了重要支持。

Conclusion: HTSC-2025基准数据集对于加速基于AI的超导材料发现具有重要意义，有望推动该领域的进一步发展。

Abstract: The discovery of high-temperature superconducting materials holds great
significance for human industry and daily life. In recent years, research on
predicting superconducting transition temperatures using artificial
intelligence~(AI) has gained popularity, with most of these tools claiming to
achieve remarkable accuracy. However, the lack of widely accepted benchmark
datasets in this field has severely hindered fair comparisons between different
AI algorithms and impeded further advancement of these methods. In this work,
we present the HTSC-2025, an ambient-pressure high-temperature superconducting
benchmark dataset. This comprehensive compilation encompasses theoretically
predicted superconducting materials discovered by theoretical physicists from
2023 to 2025 based on BCS superconductivity theory, including the renowned
X$_2$YH$_6$ system, perovskite MXH$_3$ system, M$_3$XH$_8$ system, cage-like
BCN-doped metal atomic systems derived from LaH$_{10}$ structural evolution,
and two-dimensional honeycomb-structured systems evolving from MgB$_2$. The
HTSC-2025 benchmark has been open-sourced at
https://github.com/xqh19970407/HTSC-2025 and will be continuously updated. This
benchmark holds significant importance for accelerating the discovery of
superconducting materials using AI-based methods.

</details>
